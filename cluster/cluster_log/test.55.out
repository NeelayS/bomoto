Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=55, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 3080-3135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855103
Iteration 2/25 | Loss: 0.00136822
Iteration 3/25 | Loss: 0.00128389
Iteration 4/25 | Loss: 0.00127262
Iteration 5/25 | Loss: 0.00127063
Iteration 6/25 | Loss: 0.00127063
Iteration 7/25 | Loss: 0.00127063
Iteration 8/25 | Loss: 0.00127063
Iteration 9/25 | Loss: 0.00127063
Iteration 10/25 | Loss: 0.00127063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012706328416243196, 0.0012706328416243196, 0.0012706328416243196, 0.0012706328416243196, 0.0012706328416243196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012706328416243196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.56010294
Iteration 2/25 | Loss: 0.00082946
Iteration 3/25 | Loss: 0.00082945
Iteration 4/25 | Loss: 0.00082945
Iteration 5/25 | Loss: 0.00082945
Iteration 6/25 | Loss: 0.00082945
Iteration 7/25 | Loss: 0.00082945
Iteration 8/25 | Loss: 0.00082945
Iteration 9/25 | Loss: 0.00082945
Iteration 10/25 | Loss: 0.00082945
Iteration 11/25 | Loss: 0.00082945
Iteration 12/25 | Loss: 0.00082945
Iteration 13/25 | Loss: 0.00082945
Iteration 14/25 | Loss: 0.00082945
Iteration 15/25 | Loss: 0.00082945
Iteration 16/25 | Loss: 0.00082945
Iteration 17/25 | Loss: 0.00082945
Iteration 18/25 | Loss: 0.00082945
Iteration 19/25 | Loss: 0.00082945
Iteration 20/25 | Loss: 0.00082945
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008294503204524517, 0.0008294503204524517, 0.0008294503204524517, 0.0008294503204524517, 0.0008294503204524517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008294503204524517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082945
Iteration 2/1000 | Loss: 0.00002644
Iteration 3/1000 | Loss: 0.00002101
Iteration 4/1000 | Loss: 0.00001964
Iteration 5/1000 | Loss: 0.00001872
Iteration 6/1000 | Loss: 0.00001823
Iteration 7/1000 | Loss: 0.00001782
Iteration 8/1000 | Loss: 0.00001753
Iteration 9/1000 | Loss: 0.00001727
Iteration 10/1000 | Loss: 0.00001698
Iteration 11/1000 | Loss: 0.00001689
Iteration 12/1000 | Loss: 0.00001675
Iteration 13/1000 | Loss: 0.00001669
Iteration 14/1000 | Loss: 0.00001654
Iteration 15/1000 | Loss: 0.00001648
Iteration 16/1000 | Loss: 0.00001641
Iteration 17/1000 | Loss: 0.00001640
Iteration 18/1000 | Loss: 0.00001637
Iteration 19/1000 | Loss: 0.00001636
Iteration 20/1000 | Loss: 0.00001634
Iteration 21/1000 | Loss: 0.00001633
Iteration 22/1000 | Loss: 0.00001633
Iteration 23/1000 | Loss: 0.00001627
Iteration 24/1000 | Loss: 0.00001623
Iteration 25/1000 | Loss: 0.00001623
Iteration 26/1000 | Loss: 0.00001621
Iteration 27/1000 | Loss: 0.00001615
Iteration 28/1000 | Loss: 0.00001611
Iteration 29/1000 | Loss: 0.00001610
Iteration 30/1000 | Loss: 0.00001609
Iteration 31/1000 | Loss: 0.00001609
Iteration 32/1000 | Loss: 0.00001608
Iteration 33/1000 | Loss: 0.00001606
Iteration 34/1000 | Loss: 0.00001605
Iteration 35/1000 | Loss: 0.00001604
Iteration 36/1000 | Loss: 0.00001604
Iteration 37/1000 | Loss: 0.00001604
Iteration 38/1000 | Loss: 0.00001603
Iteration 39/1000 | Loss: 0.00001602
Iteration 40/1000 | Loss: 0.00001601
Iteration 41/1000 | Loss: 0.00001600
Iteration 42/1000 | Loss: 0.00001600
Iteration 43/1000 | Loss: 0.00001600
Iteration 44/1000 | Loss: 0.00001600
Iteration 45/1000 | Loss: 0.00001600
Iteration 46/1000 | Loss: 0.00001600
Iteration 47/1000 | Loss: 0.00001600
Iteration 48/1000 | Loss: 0.00001600
Iteration 49/1000 | Loss: 0.00001596
Iteration 50/1000 | Loss: 0.00001594
Iteration 51/1000 | Loss: 0.00001594
Iteration 52/1000 | Loss: 0.00001592
Iteration 53/1000 | Loss: 0.00001592
Iteration 54/1000 | Loss: 0.00001592
Iteration 55/1000 | Loss: 0.00001592
Iteration 56/1000 | Loss: 0.00001592
Iteration 57/1000 | Loss: 0.00001591
Iteration 58/1000 | Loss: 0.00001591
Iteration 59/1000 | Loss: 0.00001591
Iteration 60/1000 | Loss: 0.00001591
Iteration 61/1000 | Loss: 0.00001591
Iteration 62/1000 | Loss: 0.00001590
Iteration 63/1000 | Loss: 0.00001590
Iteration 64/1000 | Loss: 0.00001590
Iteration 65/1000 | Loss: 0.00001589
Iteration 66/1000 | Loss: 0.00001589
Iteration 67/1000 | Loss: 0.00001589
Iteration 68/1000 | Loss: 0.00001589
Iteration 69/1000 | Loss: 0.00001589
Iteration 70/1000 | Loss: 0.00001589
Iteration 71/1000 | Loss: 0.00001589
Iteration 72/1000 | Loss: 0.00001589
Iteration 73/1000 | Loss: 0.00001589
Iteration 74/1000 | Loss: 0.00001589
Iteration 75/1000 | Loss: 0.00001589
Iteration 76/1000 | Loss: 0.00001589
Iteration 77/1000 | Loss: 0.00001589
Iteration 78/1000 | Loss: 0.00001589
Iteration 79/1000 | Loss: 0.00001589
Iteration 80/1000 | Loss: 0.00001589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.5887007975834422e-05, 1.5887007975834422e-05, 1.5887007975834422e-05, 1.5887007975834422e-05, 1.5887007975834422e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5887007975834422e-05

Optimization complete. Final v2v error: 3.419562816619873 mm

Highest mean error: 3.803553819656372 mm for frame 180

Lowest mean error: 3.1996500492095947 mm for frame 58

Saving results

Total time: 37.857338666915894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00509706
Iteration 2/25 | Loss: 0.00143166
Iteration 3/25 | Loss: 0.00134509
Iteration 4/25 | Loss: 0.00133545
Iteration 5/25 | Loss: 0.00133300
Iteration 6/25 | Loss: 0.00133230
Iteration 7/25 | Loss: 0.00133230
Iteration 8/25 | Loss: 0.00133230
Iteration 9/25 | Loss: 0.00133230
Iteration 10/25 | Loss: 0.00133230
Iteration 11/25 | Loss: 0.00133230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013322960585355759, 0.0013322960585355759, 0.0013322960585355759, 0.0013322960585355759, 0.0013322960585355759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013322960585355759

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44642484
Iteration 2/25 | Loss: 0.00089698
Iteration 3/25 | Loss: 0.00089693
Iteration 4/25 | Loss: 0.00089693
Iteration 5/25 | Loss: 0.00089693
Iteration 6/25 | Loss: 0.00089693
Iteration 7/25 | Loss: 0.00089693
Iteration 8/25 | Loss: 0.00089693
Iteration 9/25 | Loss: 0.00089693
Iteration 10/25 | Loss: 0.00089693
Iteration 11/25 | Loss: 0.00089693
Iteration 12/25 | Loss: 0.00089693
Iteration 13/25 | Loss: 0.00089693
Iteration 14/25 | Loss: 0.00089693
Iteration 15/25 | Loss: 0.00089693
Iteration 16/25 | Loss: 0.00089693
Iteration 17/25 | Loss: 0.00089693
Iteration 18/25 | Loss: 0.00089693
Iteration 19/25 | Loss: 0.00089693
Iteration 20/25 | Loss: 0.00089693
Iteration 21/25 | Loss: 0.00089693
Iteration 22/25 | Loss: 0.00089693
Iteration 23/25 | Loss: 0.00089693
Iteration 24/25 | Loss: 0.00089693
Iteration 25/25 | Loss: 0.00089693

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089693
Iteration 2/1000 | Loss: 0.00005922
Iteration 3/1000 | Loss: 0.00003290
Iteration 4/1000 | Loss: 0.00002521
Iteration 5/1000 | Loss: 0.00002303
Iteration 6/1000 | Loss: 0.00002185
Iteration 7/1000 | Loss: 0.00002123
Iteration 8/1000 | Loss: 0.00002072
Iteration 9/1000 | Loss: 0.00002021
Iteration 10/1000 | Loss: 0.00001979
Iteration 11/1000 | Loss: 0.00001969
Iteration 12/1000 | Loss: 0.00001959
Iteration 13/1000 | Loss: 0.00001953
Iteration 14/1000 | Loss: 0.00001944
Iteration 15/1000 | Loss: 0.00001930
Iteration 16/1000 | Loss: 0.00001927
Iteration 17/1000 | Loss: 0.00001922
Iteration 18/1000 | Loss: 0.00001920
Iteration 19/1000 | Loss: 0.00001920
Iteration 20/1000 | Loss: 0.00001917
Iteration 21/1000 | Loss: 0.00001914
Iteration 22/1000 | Loss: 0.00001913
Iteration 23/1000 | Loss: 0.00001911
Iteration 24/1000 | Loss: 0.00001911
Iteration 25/1000 | Loss: 0.00001910
Iteration 26/1000 | Loss: 0.00001910
Iteration 27/1000 | Loss: 0.00001909
Iteration 28/1000 | Loss: 0.00001909
Iteration 29/1000 | Loss: 0.00001908
Iteration 30/1000 | Loss: 0.00001908
Iteration 31/1000 | Loss: 0.00001906
Iteration 32/1000 | Loss: 0.00001903
Iteration 33/1000 | Loss: 0.00001900
Iteration 34/1000 | Loss: 0.00001900
Iteration 35/1000 | Loss: 0.00001897
Iteration 36/1000 | Loss: 0.00001896
Iteration 37/1000 | Loss: 0.00001895
Iteration 38/1000 | Loss: 0.00001894
Iteration 39/1000 | Loss: 0.00001894
Iteration 40/1000 | Loss: 0.00001894
Iteration 41/1000 | Loss: 0.00001893
Iteration 42/1000 | Loss: 0.00001892
Iteration 43/1000 | Loss: 0.00001891
Iteration 44/1000 | Loss: 0.00001890
Iteration 45/1000 | Loss: 0.00001889
Iteration 46/1000 | Loss: 0.00001887
Iteration 47/1000 | Loss: 0.00001883
Iteration 48/1000 | Loss: 0.00001883
Iteration 49/1000 | Loss: 0.00001882
Iteration 50/1000 | Loss: 0.00001881
Iteration 51/1000 | Loss: 0.00001880
Iteration 52/1000 | Loss: 0.00001879
Iteration 53/1000 | Loss: 0.00001879
Iteration 54/1000 | Loss: 0.00001878
Iteration 55/1000 | Loss: 0.00001878
Iteration 56/1000 | Loss: 0.00001877
Iteration 57/1000 | Loss: 0.00001876
Iteration 58/1000 | Loss: 0.00001876
Iteration 59/1000 | Loss: 0.00001876
Iteration 60/1000 | Loss: 0.00001875
Iteration 61/1000 | Loss: 0.00001875
Iteration 62/1000 | Loss: 0.00001875
Iteration 63/1000 | Loss: 0.00001874
Iteration 64/1000 | Loss: 0.00001874
Iteration 65/1000 | Loss: 0.00001874
Iteration 66/1000 | Loss: 0.00001873
Iteration 67/1000 | Loss: 0.00001873
Iteration 68/1000 | Loss: 0.00001872
Iteration 69/1000 | Loss: 0.00001872
Iteration 70/1000 | Loss: 0.00001871
Iteration 71/1000 | Loss: 0.00001871
Iteration 72/1000 | Loss: 0.00001871
Iteration 73/1000 | Loss: 0.00001870
Iteration 74/1000 | Loss: 0.00001870
Iteration 75/1000 | Loss: 0.00001870
Iteration 76/1000 | Loss: 0.00001870
Iteration 77/1000 | Loss: 0.00001869
Iteration 78/1000 | Loss: 0.00001869
Iteration 79/1000 | Loss: 0.00001869
Iteration 80/1000 | Loss: 0.00001869
Iteration 81/1000 | Loss: 0.00001869
Iteration 82/1000 | Loss: 0.00001869
Iteration 83/1000 | Loss: 0.00001869
Iteration 84/1000 | Loss: 0.00001868
Iteration 85/1000 | Loss: 0.00001868
Iteration 86/1000 | Loss: 0.00001868
Iteration 87/1000 | Loss: 0.00001867
Iteration 88/1000 | Loss: 0.00001867
Iteration 89/1000 | Loss: 0.00001867
Iteration 90/1000 | Loss: 0.00001866
Iteration 91/1000 | Loss: 0.00001866
Iteration 92/1000 | Loss: 0.00001866
Iteration 93/1000 | Loss: 0.00001866
Iteration 94/1000 | Loss: 0.00001866
Iteration 95/1000 | Loss: 0.00001865
Iteration 96/1000 | Loss: 0.00001865
Iteration 97/1000 | Loss: 0.00001865
Iteration 98/1000 | Loss: 0.00001865
Iteration 99/1000 | Loss: 0.00001864
Iteration 100/1000 | Loss: 0.00001864
Iteration 101/1000 | Loss: 0.00001864
Iteration 102/1000 | Loss: 0.00001863
Iteration 103/1000 | Loss: 0.00001863
Iteration 104/1000 | Loss: 0.00001863
Iteration 105/1000 | Loss: 0.00001863
Iteration 106/1000 | Loss: 0.00001863
Iteration 107/1000 | Loss: 0.00001862
Iteration 108/1000 | Loss: 0.00001862
Iteration 109/1000 | Loss: 0.00001862
Iteration 110/1000 | Loss: 0.00001862
Iteration 111/1000 | Loss: 0.00001862
Iteration 112/1000 | Loss: 0.00001862
Iteration 113/1000 | Loss: 0.00001862
Iteration 114/1000 | Loss: 0.00001862
Iteration 115/1000 | Loss: 0.00001862
Iteration 116/1000 | Loss: 0.00001861
Iteration 117/1000 | Loss: 0.00001861
Iteration 118/1000 | Loss: 0.00001861
Iteration 119/1000 | Loss: 0.00001861
Iteration 120/1000 | Loss: 0.00001860
Iteration 121/1000 | Loss: 0.00001860
Iteration 122/1000 | Loss: 0.00001860
Iteration 123/1000 | Loss: 0.00001860
Iteration 124/1000 | Loss: 0.00001860
Iteration 125/1000 | Loss: 0.00001860
Iteration 126/1000 | Loss: 0.00001860
Iteration 127/1000 | Loss: 0.00001860
Iteration 128/1000 | Loss: 0.00001860
Iteration 129/1000 | Loss: 0.00001860
Iteration 130/1000 | Loss: 0.00001860
Iteration 131/1000 | Loss: 0.00001859
Iteration 132/1000 | Loss: 0.00001859
Iteration 133/1000 | Loss: 0.00001859
Iteration 134/1000 | Loss: 0.00001859
Iteration 135/1000 | Loss: 0.00001859
Iteration 136/1000 | Loss: 0.00001859
Iteration 137/1000 | Loss: 0.00001859
Iteration 138/1000 | Loss: 0.00001859
Iteration 139/1000 | Loss: 0.00001859
Iteration 140/1000 | Loss: 0.00001859
Iteration 141/1000 | Loss: 0.00001859
Iteration 142/1000 | Loss: 0.00001859
Iteration 143/1000 | Loss: 0.00001859
Iteration 144/1000 | Loss: 0.00001859
Iteration 145/1000 | Loss: 0.00001859
Iteration 146/1000 | Loss: 0.00001859
Iteration 147/1000 | Loss: 0.00001859
Iteration 148/1000 | Loss: 0.00001859
Iteration 149/1000 | Loss: 0.00001859
Iteration 150/1000 | Loss: 0.00001859
Iteration 151/1000 | Loss: 0.00001859
Iteration 152/1000 | Loss: 0.00001859
Iteration 153/1000 | Loss: 0.00001859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.8590473700896837e-05, 1.8590473700896837e-05, 1.8590473700896837e-05, 1.8590473700896837e-05, 1.8590473700896837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8590473700896837e-05

Optimization complete. Final v2v error: 3.4606237411499023 mm

Highest mean error: 5.236663341522217 mm for frame 59

Lowest mean error: 2.873699426651001 mm for frame 87

Saving results

Total time: 41.610652923583984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026384
Iteration 2/25 | Loss: 0.00345509
Iteration 3/25 | Loss: 0.00198975
Iteration 4/25 | Loss: 0.00177883
Iteration 5/25 | Loss: 0.00173367
Iteration 6/25 | Loss: 0.00173503
Iteration 7/25 | Loss: 0.00159494
Iteration 8/25 | Loss: 0.00146446
Iteration 9/25 | Loss: 0.00140134
Iteration 10/25 | Loss: 0.00138545
Iteration 11/25 | Loss: 0.00137539
Iteration 12/25 | Loss: 0.00137332
Iteration 13/25 | Loss: 0.00137729
Iteration 14/25 | Loss: 0.00137104
Iteration 15/25 | Loss: 0.00136753
Iteration 16/25 | Loss: 0.00136332
Iteration 17/25 | Loss: 0.00136810
Iteration 18/25 | Loss: 0.00136463
Iteration 19/25 | Loss: 0.00136257
Iteration 20/25 | Loss: 0.00136239
Iteration 21/25 | Loss: 0.00136033
Iteration 22/25 | Loss: 0.00136573
Iteration 23/25 | Loss: 0.00136507
Iteration 24/25 | Loss: 0.00136163
Iteration 25/25 | Loss: 0.00136071

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76917326
Iteration 2/25 | Loss: 0.00123926
Iteration 3/25 | Loss: 0.00122382
Iteration 4/25 | Loss: 0.00122382
Iteration 5/25 | Loss: 0.00122382
Iteration 6/25 | Loss: 0.00122382
Iteration 7/25 | Loss: 0.00122382
Iteration 8/25 | Loss: 0.00122382
Iteration 9/25 | Loss: 0.00122382
Iteration 10/25 | Loss: 0.00122382
Iteration 11/25 | Loss: 0.00122382
Iteration 12/25 | Loss: 0.00122382
Iteration 13/25 | Loss: 0.00122382
Iteration 14/25 | Loss: 0.00122382
Iteration 15/25 | Loss: 0.00122382
Iteration 16/25 | Loss: 0.00122382
Iteration 17/25 | Loss: 0.00122382
Iteration 18/25 | Loss: 0.00122382
Iteration 19/25 | Loss: 0.00122382
Iteration 20/25 | Loss: 0.00122382
Iteration 21/25 | Loss: 0.00122382
Iteration 22/25 | Loss: 0.00122382
Iteration 23/25 | Loss: 0.00122382
Iteration 24/25 | Loss: 0.00122382
Iteration 25/25 | Loss: 0.00122382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122382
Iteration 2/1000 | Loss: 0.00011326
Iteration 3/1000 | Loss: 0.00014055
Iteration 4/1000 | Loss: 0.00009351
Iteration 5/1000 | Loss: 0.00008344
Iteration 6/1000 | Loss: 0.00017777
Iteration 7/1000 | Loss: 0.00007597
Iteration 8/1000 | Loss: 0.00008917
Iteration 9/1000 | Loss: 0.00015265
Iteration 10/1000 | Loss: 0.00010339
Iteration 11/1000 | Loss: 0.00007988
Iteration 12/1000 | Loss: 0.00006908
Iteration 13/1000 | Loss: 0.00008385
Iteration 14/1000 | Loss: 0.00006546
Iteration 15/1000 | Loss: 0.00009296
Iteration 16/1000 | Loss: 0.00010696
Iteration 17/1000 | Loss: 0.00370333
Iteration 18/1000 | Loss: 0.00138248
Iteration 19/1000 | Loss: 0.00017130
Iteration 20/1000 | Loss: 0.00007141
Iteration 21/1000 | Loss: 0.00081051
Iteration 22/1000 | Loss: 0.00029502
Iteration 23/1000 | Loss: 0.00006484
Iteration 24/1000 | Loss: 0.00005412
Iteration 25/1000 | Loss: 0.00003175
Iteration 26/1000 | Loss: 0.00019559
Iteration 27/1000 | Loss: 0.00003085
Iteration 28/1000 | Loss: 0.00002277
Iteration 29/1000 | Loss: 0.00002255
Iteration 30/1000 | Loss: 0.00005770
Iteration 31/1000 | Loss: 0.00009976
Iteration 32/1000 | Loss: 0.00002192
Iteration 33/1000 | Loss: 0.00002550
Iteration 34/1000 | Loss: 0.00006409
Iteration 35/1000 | Loss: 0.00001915
Iteration 36/1000 | Loss: 0.00001762
Iteration 37/1000 | Loss: 0.00001764
Iteration 38/1000 | Loss: 0.00001684
Iteration 39/1000 | Loss: 0.00001666
Iteration 40/1000 | Loss: 0.00001665
Iteration 41/1000 | Loss: 0.00001647
Iteration 42/1000 | Loss: 0.00001644
Iteration 43/1000 | Loss: 0.00001642
Iteration 44/1000 | Loss: 0.00001637
Iteration 45/1000 | Loss: 0.00001637
Iteration 46/1000 | Loss: 0.00001636
Iteration 47/1000 | Loss: 0.00001628
Iteration 48/1000 | Loss: 0.00001627
Iteration 49/1000 | Loss: 0.00001627
Iteration 50/1000 | Loss: 0.00001626
Iteration 51/1000 | Loss: 0.00001625
Iteration 52/1000 | Loss: 0.00001624
Iteration 53/1000 | Loss: 0.00001618
Iteration 54/1000 | Loss: 0.00001618
Iteration 55/1000 | Loss: 0.00001618
Iteration 56/1000 | Loss: 0.00001617
Iteration 57/1000 | Loss: 0.00001617
Iteration 58/1000 | Loss: 0.00001616
Iteration 59/1000 | Loss: 0.00001616
Iteration 60/1000 | Loss: 0.00001616
Iteration 61/1000 | Loss: 0.00001616
Iteration 62/1000 | Loss: 0.00001615
Iteration 63/1000 | Loss: 0.00001615
Iteration 64/1000 | Loss: 0.00001615
Iteration 65/1000 | Loss: 0.00001614
Iteration 66/1000 | Loss: 0.00001614
Iteration 67/1000 | Loss: 0.00001614
Iteration 68/1000 | Loss: 0.00001614
Iteration 69/1000 | Loss: 0.00001613
Iteration 70/1000 | Loss: 0.00001613
Iteration 71/1000 | Loss: 0.00001613
Iteration 72/1000 | Loss: 0.00001613
Iteration 73/1000 | Loss: 0.00001613
Iteration 74/1000 | Loss: 0.00001613
Iteration 75/1000 | Loss: 0.00001613
Iteration 76/1000 | Loss: 0.00001613
Iteration 77/1000 | Loss: 0.00001613
Iteration 78/1000 | Loss: 0.00001613
Iteration 79/1000 | Loss: 0.00001613
Iteration 80/1000 | Loss: 0.00001613
Iteration 81/1000 | Loss: 0.00001613
Iteration 82/1000 | Loss: 0.00001613
Iteration 83/1000 | Loss: 0.00001613
Iteration 84/1000 | Loss: 0.00001613
Iteration 85/1000 | Loss: 0.00001613
Iteration 86/1000 | Loss: 0.00001613
Iteration 87/1000 | Loss: 0.00001613
Iteration 88/1000 | Loss: 0.00001613
Iteration 89/1000 | Loss: 0.00001613
Iteration 90/1000 | Loss: 0.00001613
Iteration 91/1000 | Loss: 0.00001613
Iteration 92/1000 | Loss: 0.00001613
Iteration 93/1000 | Loss: 0.00001613
Iteration 94/1000 | Loss: 0.00001613
Iteration 95/1000 | Loss: 0.00001613
Iteration 96/1000 | Loss: 0.00001613
Iteration 97/1000 | Loss: 0.00001613
Iteration 98/1000 | Loss: 0.00001613
Iteration 99/1000 | Loss: 0.00001613
Iteration 100/1000 | Loss: 0.00001613
Iteration 101/1000 | Loss: 0.00001613
Iteration 102/1000 | Loss: 0.00001613
Iteration 103/1000 | Loss: 0.00001613
Iteration 104/1000 | Loss: 0.00001613
Iteration 105/1000 | Loss: 0.00001613
Iteration 106/1000 | Loss: 0.00001613
Iteration 107/1000 | Loss: 0.00001613
Iteration 108/1000 | Loss: 0.00001613
Iteration 109/1000 | Loss: 0.00001613
Iteration 110/1000 | Loss: 0.00001613
Iteration 111/1000 | Loss: 0.00001613
Iteration 112/1000 | Loss: 0.00001613
Iteration 113/1000 | Loss: 0.00001613
Iteration 114/1000 | Loss: 0.00001613
Iteration 115/1000 | Loss: 0.00001613
Iteration 116/1000 | Loss: 0.00001613
Iteration 117/1000 | Loss: 0.00001613
Iteration 118/1000 | Loss: 0.00001613
Iteration 119/1000 | Loss: 0.00001613
Iteration 120/1000 | Loss: 0.00001613
Iteration 121/1000 | Loss: 0.00001613
Iteration 122/1000 | Loss: 0.00001613
Iteration 123/1000 | Loss: 0.00001613
Iteration 124/1000 | Loss: 0.00001613
Iteration 125/1000 | Loss: 0.00001613
Iteration 126/1000 | Loss: 0.00001613
Iteration 127/1000 | Loss: 0.00001613
Iteration 128/1000 | Loss: 0.00001613
Iteration 129/1000 | Loss: 0.00001613
Iteration 130/1000 | Loss: 0.00001613
Iteration 131/1000 | Loss: 0.00001613
Iteration 132/1000 | Loss: 0.00001613
Iteration 133/1000 | Loss: 0.00001613
Iteration 134/1000 | Loss: 0.00001613
Iteration 135/1000 | Loss: 0.00001613
Iteration 136/1000 | Loss: 0.00001613
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.6126972695928998e-05, 1.6126972695928998e-05, 1.6126972695928998e-05, 1.6126972695928998e-05, 1.6126972695928998e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6126972695928998e-05

Optimization complete. Final v2v error: 3.391664981842041 mm

Highest mean error: 4.540515422821045 mm for frame 78

Lowest mean error: 3.066086530685425 mm for frame 22

Saving results

Total time: 108.94642972946167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428218
Iteration 2/25 | Loss: 0.00133895
Iteration 3/25 | Loss: 0.00127791
Iteration 4/25 | Loss: 0.00126360
Iteration 5/25 | Loss: 0.00125951
Iteration 6/25 | Loss: 0.00125862
Iteration 7/25 | Loss: 0.00125839
Iteration 8/25 | Loss: 0.00125839
Iteration 9/25 | Loss: 0.00125839
Iteration 10/25 | Loss: 0.00125839
Iteration 11/25 | Loss: 0.00125839
Iteration 12/25 | Loss: 0.00125839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012583908392116427, 0.0012583908392116427, 0.0012583908392116427, 0.0012583908392116427, 0.0012583908392116427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012583908392116427

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59250700
Iteration 2/25 | Loss: 0.00082419
Iteration 3/25 | Loss: 0.00082419
Iteration 4/25 | Loss: 0.00082419
Iteration 5/25 | Loss: 0.00082419
Iteration 6/25 | Loss: 0.00082419
Iteration 7/25 | Loss: 0.00082419
Iteration 8/25 | Loss: 0.00082419
Iteration 9/25 | Loss: 0.00082419
Iteration 10/25 | Loss: 0.00082419
Iteration 11/25 | Loss: 0.00082419
Iteration 12/25 | Loss: 0.00082419
Iteration 13/25 | Loss: 0.00082419
Iteration 14/25 | Loss: 0.00082419
Iteration 15/25 | Loss: 0.00082419
Iteration 16/25 | Loss: 0.00082419
Iteration 17/25 | Loss: 0.00082419
Iteration 18/25 | Loss: 0.00082419
Iteration 19/25 | Loss: 0.00082419
Iteration 20/25 | Loss: 0.00082419
Iteration 21/25 | Loss: 0.00082419
Iteration 22/25 | Loss: 0.00082419
Iteration 23/25 | Loss: 0.00082419
Iteration 24/25 | Loss: 0.00082419
Iteration 25/25 | Loss: 0.00082419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082419
Iteration 2/1000 | Loss: 0.00002788
Iteration 3/1000 | Loss: 0.00001968
Iteration 4/1000 | Loss: 0.00001810
Iteration 5/1000 | Loss: 0.00001743
Iteration 6/1000 | Loss: 0.00001688
Iteration 7/1000 | Loss: 0.00001640
Iteration 8/1000 | Loss: 0.00001605
Iteration 9/1000 | Loss: 0.00001584
Iteration 10/1000 | Loss: 0.00001553
Iteration 11/1000 | Loss: 0.00001546
Iteration 12/1000 | Loss: 0.00001546
Iteration 13/1000 | Loss: 0.00001542
Iteration 14/1000 | Loss: 0.00001532
Iteration 15/1000 | Loss: 0.00001526
Iteration 16/1000 | Loss: 0.00001524
Iteration 17/1000 | Loss: 0.00001522
Iteration 18/1000 | Loss: 0.00001516
Iteration 19/1000 | Loss: 0.00001513
Iteration 20/1000 | Loss: 0.00001512
Iteration 21/1000 | Loss: 0.00001512
Iteration 22/1000 | Loss: 0.00001508
Iteration 23/1000 | Loss: 0.00001505
Iteration 24/1000 | Loss: 0.00001504
Iteration 25/1000 | Loss: 0.00001504
Iteration 26/1000 | Loss: 0.00001504
Iteration 27/1000 | Loss: 0.00001502
Iteration 28/1000 | Loss: 0.00001502
Iteration 29/1000 | Loss: 0.00001501
Iteration 30/1000 | Loss: 0.00001501
Iteration 31/1000 | Loss: 0.00001501
Iteration 32/1000 | Loss: 0.00001500
Iteration 33/1000 | Loss: 0.00001500
Iteration 34/1000 | Loss: 0.00001500
Iteration 35/1000 | Loss: 0.00001500
Iteration 36/1000 | Loss: 0.00001500
Iteration 37/1000 | Loss: 0.00001499
Iteration 38/1000 | Loss: 0.00001499
Iteration 39/1000 | Loss: 0.00001498
Iteration 40/1000 | Loss: 0.00001498
Iteration 41/1000 | Loss: 0.00001497
Iteration 42/1000 | Loss: 0.00001497
Iteration 43/1000 | Loss: 0.00001497
Iteration 44/1000 | Loss: 0.00001496
Iteration 45/1000 | Loss: 0.00001496
Iteration 46/1000 | Loss: 0.00001496
Iteration 47/1000 | Loss: 0.00001495
Iteration 48/1000 | Loss: 0.00001495
Iteration 49/1000 | Loss: 0.00001495
Iteration 50/1000 | Loss: 0.00001495
Iteration 51/1000 | Loss: 0.00001494
Iteration 52/1000 | Loss: 0.00001494
Iteration 53/1000 | Loss: 0.00001493
Iteration 54/1000 | Loss: 0.00001493
Iteration 55/1000 | Loss: 0.00001491
Iteration 56/1000 | Loss: 0.00001491
Iteration 57/1000 | Loss: 0.00001490
Iteration 58/1000 | Loss: 0.00001490
Iteration 59/1000 | Loss: 0.00001490
Iteration 60/1000 | Loss: 0.00001490
Iteration 61/1000 | Loss: 0.00001489
Iteration 62/1000 | Loss: 0.00001489
Iteration 63/1000 | Loss: 0.00001488
Iteration 64/1000 | Loss: 0.00001486
Iteration 65/1000 | Loss: 0.00001484
Iteration 66/1000 | Loss: 0.00001484
Iteration 67/1000 | Loss: 0.00001483
Iteration 68/1000 | Loss: 0.00001483
Iteration 69/1000 | Loss: 0.00001483
Iteration 70/1000 | Loss: 0.00001483
Iteration 71/1000 | Loss: 0.00001483
Iteration 72/1000 | Loss: 0.00001483
Iteration 73/1000 | Loss: 0.00001483
Iteration 74/1000 | Loss: 0.00001483
Iteration 75/1000 | Loss: 0.00001482
Iteration 76/1000 | Loss: 0.00001482
Iteration 77/1000 | Loss: 0.00001481
Iteration 78/1000 | Loss: 0.00001480
Iteration 79/1000 | Loss: 0.00001480
Iteration 80/1000 | Loss: 0.00001480
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001479
Iteration 85/1000 | Loss: 0.00001479
Iteration 86/1000 | Loss: 0.00001479
Iteration 87/1000 | Loss: 0.00001479
Iteration 88/1000 | Loss: 0.00001479
Iteration 89/1000 | Loss: 0.00001479
Iteration 90/1000 | Loss: 0.00001479
Iteration 91/1000 | Loss: 0.00001478
Iteration 92/1000 | Loss: 0.00001478
Iteration 93/1000 | Loss: 0.00001478
Iteration 94/1000 | Loss: 0.00001477
Iteration 95/1000 | Loss: 0.00001477
Iteration 96/1000 | Loss: 0.00001477
Iteration 97/1000 | Loss: 0.00001476
Iteration 98/1000 | Loss: 0.00001476
Iteration 99/1000 | Loss: 0.00001476
Iteration 100/1000 | Loss: 0.00001475
Iteration 101/1000 | Loss: 0.00001475
Iteration 102/1000 | Loss: 0.00001474
Iteration 103/1000 | Loss: 0.00001474
Iteration 104/1000 | Loss: 0.00001474
Iteration 105/1000 | Loss: 0.00001473
Iteration 106/1000 | Loss: 0.00001472
Iteration 107/1000 | Loss: 0.00001472
Iteration 108/1000 | Loss: 0.00001472
Iteration 109/1000 | Loss: 0.00001472
Iteration 110/1000 | Loss: 0.00001472
Iteration 111/1000 | Loss: 0.00001471
Iteration 112/1000 | Loss: 0.00001471
Iteration 113/1000 | Loss: 0.00001471
Iteration 114/1000 | Loss: 0.00001471
Iteration 115/1000 | Loss: 0.00001470
Iteration 116/1000 | Loss: 0.00001470
Iteration 117/1000 | Loss: 0.00001470
Iteration 118/1000 | Loss: 0.00001470
Iteration 119/1000 | Loss: 0.00001470
Iteration 120/1000 | Loss: 0.00001470
Iteration 121/1000 | Loss: 0.00001470
Iteration 122/1000 | Loss: 0.00001469
Iteration 123/1000 | Loss: 0.00001469
Iteration 124/1000 | Loss: 0.00001469
Iteration 125/1000 | Loss: 0.00001469
Iteration 126/1000 | Loss: 0.00001469
Iteration 127/1000 | Loss: 0.00001469
Iteration 128/1000 | Loss: 0.00001469
Iteration 129/1000 | Loss: 0.00001469
Iteration 130/1000 | Loss: 0.00001469
Iteration 131/1000 | Loss: 0.00001469
Iteration 132/1000 | Loss: 0.00001469
Iteration 133/1000 | Loss: 0.00001469
Iteration 134/1000 | Loss: 0.00001468
Iteration 135/1000 | Loss: 0.00001468
Iteration 136/1000 | Loss: 0.00001468
Iteration 137/1000 | Loss: 0.00001468
Iteration 138/1000 | Loss: 0.00001468
Iteration 139/1000 | Loss: 0.00001468
Iteration 140/1000 | Loss: 0.00001468
Iteration 141/1000 | Loss: 0.00001467
Iteration 142/1000 | Loss: 0.00001467
Iteration 143/1000 | Loss: 0.00001467
Iteration 144/1000 | Loss: 0.00001467
Iteration 145/1000 | Loss: 0.00001467
Iteration 146/1000 | Loss: 0.00001467
Iteration 147/1000 | Loss: 0.00001467
Iteration 148/1000 | Loss: 0.00001467
Iteration 149/1000 | Loss: 0.00001467
Iteration 150/1000 | Loss: 0.00001467
Iteration 151/1000 | Loss: 0.00001467
Iteration 152/1000 | Loss: 0.00001467
Iteration 153/1000 | Loss: 0.00001467
Iteration 154/1000 | Loss: 0.00001467
Iteration 155/1000 | Loss: 0.00001467
Iteration 156/1000 | Loss: 0.00001467
Iteration 157/1000 | Loss: 0.00001467
Iteration 158/1000 | Loss: 0.00001467
Iteration 159/1000 | Loss: 0.00001467
Iteration 160/1000 | Loss: 0.00001467
Iteration 161/1000 | Loss: 0.00001467
Iteration 162/1000 | Loss: 0.00001467
Iteration 163/1000 | Loss: 0.00001467
Iteration 164/1000 | Loss: 0.00001467
Iteration 165/1000 | Loss: 0.00001467
Iteration 166/1000 | Loss: 0.00001467
Iteration 167/1000 | Loss: 0.00001467
Iteration 168/1000 | Loss: 0.00001467
Iteration 169/1000 | Loss: 0.00001467
Iteration 170/1000 | Loss: 0.00001467
Iteration 171/1000 | Loss: 0.00001467
Iteration 172/1000 | Loss: 0.00001467
Iteration 173/1000 | Loss: 0.00001467
Iteration 174/1000 | Loss: 0.00001467
Iteration 175/1000 | Loss: 0.00001467
Iteration 176/1000 | Loss: 0.00001467
Iteration 177/1000 | Loss: 0.00001467
Iteration 178/1000 | Loss: 0.00001467
Iteration 179/1000 | Loss: 0.00001467
Iteration 180/1000 | Loss: 0.00001467
Iteration 181/1000 | Loss: 0.00001467
Iteration 182/1000 | Loss: 0.00001467
Iteration 183/1000 | Loss: 0.00001467
Iteration 184/1000 | Loss: 0.00001467
Iteration 185/1000 | Loss: 0.00001467
Iteration 186/1000 | Loss: 0.00001467
Iteration 187/1000 | Loss: 0.00001467
Iteration 188/1000 | Loss: 0.00001467
Iteration 189/1000 | Loss: 0.00001467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.467227230023127e-05, 1.467227230023127e-05, 1.467227230023127e-05, 1.467227230023127e-05, 1.467227230023127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.467227230023127e-05

Optimization complete. Final v2v error: 3.2894744873046875 mm

Highest mean error: 3.7075297832489014 mm for frame 89

Lowest mean error: 3.1586501598358154 mm for frame 124

Saving results

Total time: 41.44476056098938
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862853
Iteration 2/25 | Loss: 0.00181252
Iteration 3/25 | Loss: 0.00150046
Iteration 4/25 | Loss: 0.00146340
Iteration 5/25 | Loss: 0.00145793
Iteration 6/25 | Loss: 0.00145643
Iteration 7/25 | Loss: 0.00139228
Iteration 8/25 | Loss: 0.00137143
Iteration 9/25 | Loss: 0.00136510
Iteration 10/25 | Loss: 0.00136284
Iteration 11/25 | Loss: 0.00136054
Iteration 12/25 | Loss: 0.00134888
Iteration 13/25 | Loss: 0.00134480
Iteration 14/25 | Loss: 0.00134114
Iteration 15/25 | Loss: 0.00133623
Iteration 16/25 | Loss: 0.00133410
Iteration 17/25 | Loss: 0.00133362
Iteration 18/25 | Loss: 0.00133350
Iteration 19/25 | Loss: 0.00133347
Iteration 20/25 | Loss: 0.00133347
Iteration 21/25 | Loss: 0.00133347
Iteration 22/25 | Loss: 0.00133346
Iteration 23/25 | Loss: 0.00133346
Iteration 24/25 | Loss: 0.00133346
Iteration 25/25 | Loss: 0.00133346

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96367455
Iteration 2/25 | Loss: 0.00054548
Iteration 3/25 | Loss: 0.00054547
Iteration 4/25 | Loss: 0.00054547
Iteration 5/25 | Loss: 0.00054547
Iteration 6/25 | Loss: 0.00054547
Iteration 7/25 | Loss: 0.00054547
Iteration 8/25 | Loss: 0.00054547
Iteration 9/25 | Loss: 0.00054547
Iteration 10/25 | Loss: 0.00054547
Iteration 11/25 | Loss: 0.00054547
Iteration 12/25 | Loss: 0.00054547
Iteration 13/25 | Loss: 0.00054546
Iteration 14/25 | Loss: 0.00054546
Iteration 15/25 | Loss: 0.00054546
Iteration 16/25 | Loss: 0.00054546
Iteration 17/25 | Loss: 0.00054546
Iteration 18/25 | Loss: 0.00054546
Iteration 19/25 | Loss: 0.00054546
Iteration 20/25 | Loss: 0.00054546
Iteration 21/25 | Loss: 0.00054546
Iteration 22/25 | Loss: 0.00054546
Iteration 23/25 | Loss: 0.00054546
Iteration 24/25 | Loss: 0.00054546
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000545464747119695, 0.000545464747119695, 0.000545464747119695, 0.000545464747119695, 0.000545464747119695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000545464747119695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054546
Iteration 2/1000 | Loss: 0.00004442
Iteration 3/1000 | Loss: 0.00003603
Iteration 4/1000 | Loss: 0.00003306
Iteration 5/1000 | Loss: 0.00003158
Iteration 6/1000 | Loss: 0.00003049
Iteration 7/1000 | Loss: 0.00002970
Iteration 8/1000 | Loss: 0.00002917
Iteration 9/1000 | Loss: 0.00002871
Iteration 10/1000 | Loss: 0.00002840
Iteration 11/1000 | Loss: 0.00002837
Iteration 12/1000 | Loss: 0.00002821
Iteration 13/1000 | Loss: 0.00002804
Iteration 14/1000 | Loss: 0.00002804
Iteration 15/1000 | Loss: 0.00002795
Iteration 16/1000 | Loss: 0.00002795
Iteration 17/1000 | Loss: 0.00002795
Iteration 18/1000 | Loss: 0.00002787
Iteration 19/1000 | Loss: 0.00002787
Iteration 20/1000 | Loss: 0.00002787
Iteration 21/1000 | Loss: 0.00002787
Iteration 22/1000 | Loss: 0.00002787
Iteration 23/1000 | Loss: 0.00002787
Iteration 24/1000 | Loss: 0.00002787
Iteration 25/1000 | Loss: 0.00002787
Iteration 26/1000 | Loss: 0.00002787
Iteration 27/1000 | Loss: 0.00002786
Iteration 28/1000 | Loss: 0.00002786
Iteration 29/1000 | Loss: 0.00002786
Iteration 30/1000 | Loss: 0.00002786
Iteration 31/1000 | Loss: 0.00002785
Iteration 32/1000 | Loss: 0.00002785
Iteration 33/1000 | Loss: 0.00002785
Iteration 34/1000 | Loss: 0.00002785
Iteration 35/1000 | Loss: 0.00002785
Iteration 36/1000 | Loss: 0.00002785
Iteration 37/1000 | Loss: 0.00002785
Iteration 38/1000 | Loss: 0.00002784
Iteration 39/1000 | Loss: 0.00002784
Iteration 40/1000 | Loss: 0.00002784
Iteration 41/1000 | Loss: 0.00002784
Iteration 42/1000 | Loss: 0.00002784
Iteration 43/1000 | Loss: 0.00002783
Iteration 44/1000 | Loss: 0.00002783
Iteration 45/1000 | Loss: 0.00002783
Iteration 46/1000 | Loss: 0.00002783
Iteration 47/1000 | Loss: 0.00002783
Iteration 48/1000 | Loss: 0.00002783
Iteration 49/1000 | Loss: 0.00002783
Iteration 50/1000 | Loss: 0.00002783
Iteration 51/1000 | Loss: 0.00002782
Iteration 52/1000 | Loss: 0.00002782
Iteration 53/1000 | Loss: 0.00002782
Iteration 54/1000 | Loss: 0.00002781
Iteration 55/1000 | Loss: 0.00002780
Iteration 56/1000 | Loss: 0.00002780
Iteration 57/1000 | Loss: 0.00002780
Iteration 58/1000 | Loss: 0.00002780
Iteration 59/1000 | Loss: 0.00002780
Iteration 60/1000 | Loss: 0.00002780
Iteration 61/1000 | Loss: 0.00002779
Iteration 62/1000 | Loss: 0.00002779
Iteration 63/1000 | Loss: 0.00002779
Iteration 64/1000 | Loss: 0.00002779
Iteration 65/1000 | Loss: 0.00002779
Iteration 66/1000 | Loss: 0.00002779
Iteration 67/1000 | Loss: 0.00002779
Iteration 68/1000 | Loss: 0.00002779
Iteration 69/1000 | Loss: 0.00002779
Iteration 70/1000 | Loss: 0.00002779
Iteration 71/1000 | Loss: 0.00002779
Iteration 72/1000 | Loss: 0.00002779
Iteration 73/1000 | Loss: 0.00002779
Iteration 74/1000 | Loss: 0.00002779
Iteration 75/1000 | Loss: 0.00002779
Iteration 76/1000 | Loss: 0.00002779
Iteration 77/1000 | Loss: 0.00002779
Iteration 78/1000 | Loss: 0.00002779
Iteration 79/1000 | Loss: 0.00002778
Iteration 80/1000 | Loss: 0.00002778
Iteration 81/1000 | Loss: 0.00002778
Iteration 82/1000 | Loss: 0.00002778
Iteration 83/1000 | Loss: 0.00002778
Iteration 84/1000 | Loss: 0.00002777
Iteration 85/1000 | Loss: 0.00002777
Iteration 86/1000 | Loss: 0.00002776
Iteration 87/1000 | Loss: 0.00002776
Iteration 88/1000 | Loss: 0.00002776
Iteration 89/1000 | Loss: 0.00002776
Iteration 90/1000 | Loss: 0.00002776
Iteration 91/1000 | Loss: 0.00002776
Iteration 92/1000 | Loss: 0.00002776
Iteration 93/1000 | Loss: 0.00002776
Iteration 94/1000 | Loss: 0.00002776
Iteration 95/1000 | Loss: 0.00002776
Iteration 96/1000 | Loss: 0.00002776
Iteration 97/1000 | Loss: 0.00002776
Iteration 98/1000 | Loss: 0.00002776
Iteration 99/1000 | Loss: 0.00002776
Iteration 100/1000 | Loss: 0.00002776
Iteration 101/1000 | Loss: 0.00002776
Iteration 102/1000 | Loss: 0.00002776
Iteration 103/1000 | Loss: 0.00002776
Iteration 104/1000 | Loss: 0.00002776
Iteration 105/1000 | Loss: 0.00002776
Iteration 106/1000 | Loss: 0.00002776
Iteration 107/1000 | Loss: 0.00002776
Iteration 108/1000 | Loss: 0.00002776
Iteration 109/1000 | Loss: 0.00002776
Iteration 110/1000 | Loss: 0.00002776
Iteration 111/1000 | Loss: 0.00002776
Iteration 112/1000 | Loss: 0.00002776
Iteration 113/1000 | Loss: 0.00002776
Iteration 114/1000 | Loss: 0.00002776
Iteration 115/1000 | Loss: 0.00002776
Iteration 116/1000 | Loss: 0.00002776
Iteration 117/1000 | Loss: 0.00002776
Iteration 118/1000 | Loss: 0.00002776
Iteration 119/1000 | Loss: 0.00002776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.775505527097266e-05, 2.775505527097266e-05, 2.775505527097266e-05, 2.775505527097266e-05, 2.775505527097266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.775505527097266e-05

Optimization complete. Final v2v error: 4.399016380310059 mm

Highest mean error: 4.495062828063965 mm for frame 47

Lowest mean error: 4.329190731048584 mm for frame 15

Saving results

Total time: 57.7904269695282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01126927
Iteration 2/25 | Loss: 0.01126927
Iteration 3/25 | Loss: 0.01126927
Iteration 4/25 | Loss: 0.01126927
Iteration 5/25 | Loss: 0.01126927
Iteration 6/25 | Loss: 0.01126927
Iteration 7/25 | Loss: 0.01126927
Iteration 8/25 | Loss: 0.01126927
Iteration 9/25 | Loss: 0.01126927
Iteration 10/25 | Loss: 0.01126927
Iteration 11/25 | Loss: 0.01126927
Iteration 12/25 | Loss: 0.01126927
Iteration 13/25 | Loss: 0.01126927
Iteration 14/25 | Loss: 0.01126927
Iteration 15/25 | Loss: 0.01126927
Iteration 16/25 | Loss: 0.01126927
Iteration 17/25 | Loss: 0.01126927
Iteration 18/25 | Loss: 0.01126926
Iteration 19/25 | Loss: 0.01126926
Iteration 20/25 | Loss: 0.01126926
Iteration 21/25 | Loss: 0.01126926
Iteration 22/25 | Loss: 0.01126926
Iteration 23/25 | Loss: 0.01126926
Iteration 24/25 | Loss: 0.01126926
Iteration 25/25 | Loss: 0.01126926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.68002987
Iteration 2/25 | Loss: 0.18487038
Iteration 3/25 | Loss: 0.18410794
Iteration 4/25 | Loss: 0.18410788
Iteration 5/25 | Loss: 0.18410787
Iteration 6/25 | Loss: 0.18410784
Iteration 7/25 | Loss: 0.18410784
Iteration 8/25 | Loss: 0.18410784
Iteration 9/25 | Loss: 0.18410784
Iteration 10/25 | Loss: 0.18410784
Iteration 11/25 | Loss: 0.18410784
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.18410784006118774, 0.18410784006118774, 0.18410784006118774, 0.18410784006118774, 0.18410784006118774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18410784006118774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18410784
Iteration 2/1000 | Loss: 0.00758910
Iteration 3/1000 | Loss: 0.00217587
Iteration 4/1000 | Loss: 0.00050681
Iteration 5/1000 | Loss: 0.00015993
Iteration 6/1000 | Loss: 0.00024902
Iteration 7/1000 | Loss: 0.00021196
Iteration 8/1000 | Loss: 0.00013650
Iteration 9/1000 | Loss: 0.00034723
Iteration 10/1000 | Loss: 0.00004325
Iteration 11/1000 | Loss: 0.00005819
Iteration 12/1000 | Loss: 0.00012265
Iteration 13/1000 | Loss: 0.00003049
Iteration 14/1000 | Loss: 0.00009368
Iteration 15/1000 | Loss: 0.00002723
Iteration 16/1000 | Loss: 0.00007124
Iteration 17/1000 | Loss: 0.00015691
Iteration 18/1000 | Loss: 0.00007593
Iteration 19/1000 | Loss: 0.00003090
Iteration 20/1000 | Loss: 0.00004559
Iteration 21/1000 | Loss: 0.00002429
Iteration 22/1000 | Loss: 0.00005338
Iteration 23/1000 | Loss: 0.00002341
Iteration 24/1000 | Loss: 0.00012073
Iteration 25/1000 | Loss: 0.00007618
Iteration 26/1000 | Loss: 0.00011519
Iteration 27/1000 | Loss: 0.00002207
Iteration 28/1000 | Loss: 0.00002391
Iteration 29/1000 | Loss: 0.00009480
Iteration 30/1000 | Loss: 0.00006740
Iteration 31/1000 | Loss: 0.00002082
Iteration 32/1000 | Loss: 0.00009016
Iteration 33/1000 | Loss: 0.00004142
Iteration 34/1000 | Loss: 0.00005224
Iteration 35/1000 | Loss: 0.00002919
Iteration 36/1000 | Loss: 0.00001995
Iteration 37/1000 | Loss: 0.00002641
Iteration 38/1000 | Loss: 0.00001960
Iteration 39/1000 | Loss: 0.00001951
Iteration 40/1000 | Loss: 0.00001951
Iteration 41/1000 | Loss: 0.00005579
Iteration 42/1000 | Loss: 0.00001940
Iteration 43/1000 | Loss: 0.00001925
Iteration 44/1000 | Loss: 0.00001916
Iteration 45/1000 | Loss: 0.00001916
Iteration 46/1000 | Loss: 0.00001916
Iteration 47/1000 | Loss: 0.00005445
Iteration 48/1000 | Loss: 0.00001915
Iteration 49/1000 | Loss: 0.00006319
Iteration 50/1000 | Loss: 0.00001900
Iteration 51/1000 | Loss: 0.00001897
Iteration 52/1000 | Loss: 0.00001897
Iteration 53/1000 | Loss: 0.00001897
Iteration 54/1000 | Loss: 0.00001896
Iteration 55/1000 | Loss: 0.00001896
Iteration 56/1000 | Loss: 0.00001896
Iteration 57/1000 | Loss: 0.00001895
Iteration 58/1000 | Loss: 0.00001895
Iteration 59/1000 | Loss: 0.00001891
Iteration 60/1000 | Loss: 0.00001889
Iteration 61/1000 | Loss: 0.00001889
Iteration 62/1000 | Loss: 0.00001885
Iteration 63/1000 | Loss: 0.00001885
Iteration 64/1000 | Loss: 0.00001885
Iteration 65/1000 | Loss: 0.00001885
Iteration 66/1000 | Loss: 0.00001885
Iteration 67/1000 | Loss: 0.00001885
Iteration 68/1000 | Loss: 0.00001885
Iteration 69/1000 | Loss: 0.00001885
Iteration 70/1000 | Loss: 0.00001885
Iteration 71/1000 | Loss: 0.00001884
Iteration 72/1000 | Loss: 0.00001884
Iteration 73/1000 | Loss: 0.00001884
Iteration 74/1000 | Loss: 0.00005469
Iteration 75/1000 | Loss: 0.00001881
Iteration 76/1000 | Loss: 0.00001877
Iteration 77/1000 | Loss: 0.00001877
Iteration 78/1000 | Loss: 0.00001877
Iteration 79/1000 | Loss: 0.00001877
Iteration 80/1000 | Loss: 0.00001876
Iteration 81/1000 | Loss: 0.00001876
Iteration 82/1000 | Loss: 0.00001876
Iteration 83/1000 | Loss: 0.00001876
Iteration 84/1000 | Loss: 0.00001875
Iteration 85/1000 | Loss: 0.00001875
Iteration 86/1000 | Loss: 0.00001875
Iteration 87/1000 | Loss: 0.00001874
Iteration 88/1000 | Loss: 0.00001874
Iteration 89/1000 | Loss: 0.00001873
Iteration 90/1000 | Loss: 0.00001873
Iteration 91/1000 | Loss: 0.00001873
Iteration 92/1000 | Loss: 0.00001873
Iteration 93/1000 | Loss: 0.00001873
Iteration 94/1000 | Loss: 0.00001873
Iteration 95/1000 | Loss: 0.00001873
Iteration 96/1000 | Loss: 0.00001872
Iteration 97/1000 | Loss: 0.00001872
Iteration 98/1000 | Loss: 0.00001872
Iteration 99/1000 | Loss: 0.00001872
Iteration 100/1000 | Loss: 0.00001872
Iteration 101/1000 | Loss: 0.00001872
Iteration 102/1000 | Loss: 0.00001872
Iteration 103/1000 | Loss: 0.00001872
Iteration 104/1000 | Loss: 0.00001872
Iteration 105/1000 | Loss: 0.00001872
Iteration 106/1000 | Loss: 0.00001872
Iteration 107/1000 | Loss: 0.00001871
Iteration 108/1000 | Loss: 0.00001871
Iteration 109/1000 | Loss: 0.00001871
Iteration 110/1000 | Loss: 0.00001870
Iteration 111/1000 | Loss: 0.00001870
Iteration 112/1000 | Loss: 0.00001870
Iteration 113/1000 | Loss: 0.00001870
Iteration 114/1000 | Loss: 0.00001870
Iteration 115/1000 | Loss: 0.00001870
Iteration 116/1000 | Loss: 0.00001870
Iteration 117/1000 | Loss: 0.00001870
Iteration 118/1000 | Loss: 0.00001870
Iteration 119/1000 | Loss: 0.00001870
Iteration 120/1000 | Loss: 0.00001870
Iteration 121/1000 | Loss: 0.00001870
Iteration 122/1000 | Loss: 0.00001870
Iteration 123/1000 | Loss: 0.00001870
Iteration 124/1000 | Loss: 0.00001870
Iteration 125/1000 | Loss: 0.00001869
Iteration 126/1000 | Loss: 0.00001868
Iteration 127/1000 | Loss: 0.00001868
Iteration 128/1000 | Loss: 0.00001868
Iteration 129/1000 | Loss: 0.00001867
Iteration 130/1000 | Loss: 0.00001867
Iteration 131/1000 | Loss: 0.00001866
Iteration 132/1000 | Loss: 0.00001866
Iteration 133/1000 | Loss: 0.00001866
Iteration 134/1000 | Loss: 0.00001866
Iteration 135/1000 | Loss: 0.00001866
Iteration 136/1000 | Loss: 0.00001866
Iteration 137/1000 | Loss: 0.00001866
Iteration 138/1000 | Loss: 0.00001866
Iteration 139/1000 | Loss: 0.00001866
Iteration 140/1000 | Loss: 0.00001866
Iteration 141/1000 | Loss: 0.00001865
Iteration 142/1000 | Loss: 0.00001865
Iteration 143/1000 | Loss: 0.00001865
Iteration 144/1000 | Loss: 0.00001865
Iteration 145/1000 | Loss: 0.00001864
Iteration 146/1000 | Loss: 0.00001864
Iteration 147/1000 | Loss: 0.00001864
Iteration 148/1000 | Loss: 0.00001864
Iteration 149/1000 | Loss: 0.00001863
Iteration 150/1000 | Loss: 0.00001863
Iteration 151/1000 | Loss: 0.00001863
Iteration 152/1000 | Loss: 0.00001863
Iteration 153/1000 | Loss: 0.00001863
Iteration 154/1000 | Loss: 0.00001863
Iteration 155/1000 | Loss: 0.00001863
Iteration 156/1000 | Loss: 0.00001862
Iteration 157/1000 | Loss: 0.00001862
Iteration 158/1000 | Loss: 0.00001862
Iteration 159/1000 | Loss: 0.00001862
Iteration 160/1000 | Loss: 0.00001861
Iteration 161/1000 | Loss: 0.00001861
Iteration 162/1000 | Loss: 0.00001861
Iteration 163/1000 | Loss: 0.00001861
Iteration 164/1000 | Loss: 0.00001861
Iteration 165/1000 | Loss: 0.00001861
Iteration 166/1000 | Loss: 0.00001861
Iteration 167/1000 | Loss: 0.00001861
Iteration 168/1000 | Loss: 0.00001861
Iteration 169/1000 | Loss: 0.00001861
Iteration 170/1000 | Loss: 0.00001861
Iteration 171/1000 | Loss: 0.00001861
Iteration 172/1000 | Loss: 0.00001861
Iteration 173/1000 | Loss: 0.00001861
Iteration 174/1000 | Loss: 0.00001860
Iteration 175/1000 | Loss: 0.00001860
Iteration 176/1000 | Loss: 0.00001860
Iteration 177/1000 | Loss: 0.00001860
Iteration 178/1000 | Loss: 0.00001860
Iteration 179/1000 | Loss: 0.00001859
Iteration 180/1000 | Loss: 0.00001859
Iteration 181/1000 | Loss: 0.00001859
Iteration 182/1000 | Loss: 0.00001859
Iteration 183/1000 | Loss: 0.00001859
Iteration 184/1000 | Loss: 0.00001858
Iteration 185/1000 | Loss: 0.00001858
Iteration 186/1000 | Loss: 0.00001858
Iteration 187/1000 | Loss: 0.00001858
Iteration 188/1000 | Loss: 0.00001858
Iteration 189/1000 | Loss: 0.00001858
Iteration 190/1000 | Loss: 0.00001858
Iteration 191/1000 | Loss: 0.00001858
Iteration 192/1000 | Loss: 0.00001858
Iteration 193/1000 | Loss: 0.00001858
Iteration 194/1000 | Loss: 0.00001858
Iteration 195/1000 | Loss: 0.00001858
Iteration 196/1000 | Loss: 0.00001858
Iteration 197/1000 | Loss: 0.00001858
Iteration 198/1000 | Loss: 0.00001858
Iteration 199/1000 | Loss: 0.00001858
Iteration 200/1000 | Loss: 0.00001857
Iteration 201/1000 | Loss: 0.00001857
Iteration 202/1000 | Loss: 0.00001857
Iteration 203/1000 | Loss: 0.00001857
Iteration 204/1000 | Loss: 0.00001857
Iteration 205/1000 | Loss: 0.00001857
Iteration 206/1000 | Loss: 0.00001857
Iteration 207/1000 | Loss: 0.00001856
Iteration 208/1000 | Loss: 0.00001856
Iteration 209/1000 | Loss: 0.00001856
Iteration 210/1000 | Loss: 0.00001856
Iteration 211/1000 | Loss: 0.00001856
Iteration 212/1000 | Loss: 0.00001856
Iteration 213/1000 | Loss: 0.00001856
Iteration 214/1000 | Loss: 0.00001856
Iteration 215/1000 | Loss: 0.00001856
Iteration 216/1000 | Loss: 0.00001855
Iteration 217/1000 | Loss: 0.00001855
Iteration 218/1000 | Loss: 0.00001855
Iteration 219/1000 | Loss: 0.00001855
Iteration 220/1000 | Loss: 0.00001855
Iteration 221/1000 | Loss: 0.00001855
Iteration 222/1000 | Loss: 0.00002138
Iteration 223/1000 | Loss: 0.00002138
Iteration 224/1000 | Loss: 0.00001855
Iteration 225/1000 | Loss: 0.00001853
Iteration 226/1000 | Loss: 0.00001853
Iteration 227/1000 | Loss: 0.00001853
Iteration 228/1000 | Loss: 0.00001853
Iteration 229/1000 | Loss: 0.00001853
Iteration 230/1000 | Loss: 0.00001853
Iteration 231/1000 | Loss: 0.00001853
Iteration 232/1000 | Loss: 0.00001853
Iteration 233/1000 | Loss: 0.00001852
Iteration 234/1000 | Loss: 0.00001852
Iteration 235/1000 | Loss: 0.00001852
Iteration 236/1000 | Loss: 0.00001852
Iteration 237/1000 | Loss: 0.00001852
Iteration 238/1000 | Loss: 0.00001852
Iteration 239/1000 | Loss: 0.00001852
Iteration 240/1000 | Loss: 0.00001852
Iteration 241/1000 | Loss: 0.00001852
Iteration 242/1000 | Loss: 0.00001852
Iteration 243/1000 | Loss: 0.00001852
Iteration 244/1000 | Loss: 0.00001852
Iteration 245/1000 | Loss: 0.00001852
Iteration 246/1000 | Loss: 0.00001852
Iteration 247/1000 | Loss: 0.00001852
Iteration 248/1000 | Loss: 0.00001852
Iteration 249/1000 | Loss: 0.00001852
Iteration 250/1000 | Loss: 0.00001851
Iteration 251/1000 | Loss: 0.00001851
Iteration 252/1000 | Loss: 0.00001851
Iteration 253/1000 | Loss: 0.00001851
Iteration 254/1000 | Loss: 0.00001851
Iteration 255/1000 | Loss: 0.00001851
Iteration 256/1000 | Loss: 0.00001851
Iteration 257/1000 | Loss: 0.00001851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.8514596376917325e-05, 1.8514596376917325e-05, 1.8514596376917325e-05, 1.8514596376917325e-05, 1.8514596376917325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8514596376917325e-05

Optimization complete. Final v2v error: 3.62203311920166 mm

Highest mean error: 3.7968709468841553 mm for frame 25

Lowest mean error: 3.40555477142334 mm for frame 211

Saving results

Total time: 99.42117595672607
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812987
Iteration 2/25 | Loss: 0.00134856
Iteration 3/25 | Loss: 0.00124350
Iteration 4/25 | Loss: 0.00123643
Iteration 5/25 | Loss: 0.00123550
Iteration 6/25 | Loss: 0.00123550
Iteration 7/25 | Loss: 0.00123550
Iteration 8/25 | Loss: 0.00123550
Iteration 9/25 | Loss: 0.00123550
Iteration 10/25 | Loss: 0.00123550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012355007929727435, 0.0012355007929727435, 0.0012355007929727435, 0.0012355007929727435, 0.0012355007929727435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012355007929727435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42205441
Iteration 2/25 | Loss: 0.00076028
Iteration 3/25 | Loss: 0.00076028
Iteration 4/25 | Loss: 0.00076028
Iteration 5/25 | Loss: 0.00076028
Iteration 6/25 | Loss: 0.00076028
Iteration 7/25 | Loss: 0.00076028
Iteration 8/25 | Loss: 0.00076028
Iteration 9/25 | Loss: 0.00076028
Iteration 10/25 | Loss: 0.00076028
Iteration 11/25 | Loss: 0.00076028
Iteration 12/25 | Loss: 0.00076028
Iteration 13/25 | Loss: 0.00076028
Iteration 14/25 | Loss: 0.00076028
Iteration 15/25 | Loss: 0.00076028
Iteration 16/25 | Loss: 0.00076028
Iteration 17/25 | Loss: 0.00076028
Iteration 18/25 | Loss: 0.00076028
Iteration 19/25 | Loss: 0.00076028
Iteration 20/25 | Loss: 0.00076028
Iteration 21/25 | Loss: 0.00076028
Iteration 22/25 | Loss: 0.00076028
Iteration 23/25 | Loss: 0.00076028
Iteration 24/25 | Loss: 0.00076028
Iteration 25/25 | Loss: 0.00076028

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076028
Iteration 2/1000 | Loss: 0.00002772
Iteration 3/1000 | Loss: 0.00001890
Iteration 4/1000 | Loss: 0.00001590
Iteration 5/1000 | Loss: 0.00001487
Iteration 6/1000 | Loss: 0.00001403
Iteration 7/1000 | Loss: 0.00001339
Iteration 8/1000 | Loss: 0.00001310
Iteration 9/1000 | Loss: 0.00001275
Iteration 10/1000 | Loss: 0.00001254
Iteration 11/1000 | Loss: 0.00001243
Iteration 12/1000 | Loss: 0.00001240
Iteration 13/1000 | Loss: 0.00001236
Iteration 14/1000 | Loss: 0.00001235
Iteration 15/1000 | Loss: 0.00001234
Iteration 16/1000 | Loss: 0.00001234
Iteration 17/1000 | Loss: 0.00001233
Iteration 18/1000 | Loss: 0.00001232
Iteration 19/1000 | Loss: 0.00001231
Iteration 20/1000 | Loss: 0.00001231
Iteration 21/1000 | Loss: 0.00001227
Iteration 22/1000 | Loss: 0.00001223
Iteration 23/1000 | Loss: 0.00001222
Iteration 24/1000 | Loss: 0.00001222
Iteration 25/1000 | Loss: 0.00001221
Iteration 26/1000 | Loss: 0.00001218
Iteration 27/1000 | Loss: 0.00001218
Iteration 28/1000 | Loss: 0.00001218
Iteration 29/1000 | Loss: 0.00001218
Iteration 30/1000 | Loss: 0.00001217
Iteration 31/1000 | Loss: 0.00001212
Iteration 32/1000 | Loss: 0.00001212
Iteration 33/1000 | Loss: 0.00001203
Iteration 34/1000 | Loss: 0.00001199
Iteration 35/1000 | Loss: 0.00001198
Iteration 36/1000 | Loss: 0.00001198
Iteration 37/1000 | Loss: 0.00001196
Iteration 38/1000 | Loss: 0.00001196
Iteration 39/1000 | Loss: 0.00001195
Iteration 40/1000 | Loss: 0.00001194
Iteration 41/1000 | Loss: 0.00001194
Iteration 42/1000 | Loss: 0.00001193
Iteration 43/1000 | Loss: 0.00001193
Iteration 44/1000 | Loss: 0.00001192
Iteration 45/1000 | Loss: 0.00001191
Iteration 46/1000 | Loss: 0.00001191
Iteration 47/1000 | Loss: 0.00001190
Iteration 48/1000 | Loss: 0.00001190
Iteration 49/1000 | Loss: 0.00001189
Iteration 50/1000 | Loss: 0.00001189
Iteration 51/1000 | Loss: 0.00001189
Iteration 52/1000 | Loss: 0.00001188
Iteration 53/1000 | Loss: 0.00001188
Iteration 54/1000 | Loss: 0.00001187
Iteration 55/1000 | Loss: 0.00001187
Iteration 56/1000 | Loss: 0.00001187
Iteration 57/1000 | Loss: 0.00001186
Iteration 58/1000 | Loss: 0.00001186
Iteration 59/1000 | Loss: 0.00001185
Iteration 60/1000 | Loss: 0.00001185
Iteration 61/1000 | Loss: 0.00001185
Iteration 62/1000 | Loss: 0.00001185
Iteration 63/1000 | Loss: 0.00001184
Iteration 64/1000 | Loss: 0.00001184
Iteration 65/1000 | Loss: 0.00001184
Iteration 66/1000 | Loss: 0.00001183
Iteration 67/1000 | Loss: 0.00001183
Iteration 68/1000 | Loss: 0.00001182
Iteration 69/1000 | Loss: 0.00001182
Iteration 70/1000 | Loss: 0.00001181
Iteration 71/1000 | Loss: 0.00001181
Iteration 72/1000 | Loss: 0.00001181
Iteration 73/1000 | Loss: 0.00001181
Iteration 74/1000 | Loss: 0.00001180
Iteration 75/1000 | Loss: 0.00001180
Iteration 76/1000 | Loss: 0.00001180
Iteration 77/1000 | Loss: 0.00001180
Iteration 78/1000 | Loss: 0.00001180
Iteration 79/1000 | Loss: 0.00001179
Iteration 80/1000 | Loss: 0.00001179
Iteration 81/1000 | Loss: 0.00001179
Iteration 82/1000 | Loss: 0.00001178
Iteration 83/1000 | Loss: 0.00001178
Iteration 84/1000 | Loss: 0.00001177
Iteration 85/1000 | Loss: 0.00001177
Iteration 86/1000 | Loss: 0.00001177
Iteration 87/1000 | Loss: 0.00001177
Iteration 88/1000 | Loss: 0.00001177
Iteration 89/1000 | Loss: 0.00001177
Iteration 90/1000 | Loss: 0.00001177
Iteration 91/1000 | Loss: 0.00001176
Iteration 92/1000 | Loss: 0.00001176
Iteration 93/1000 | Loss: 0.00001176
Iteration 94/1000 | Loss: 0.00001176
Iteration 95/1000 | Loss: 0.00001176
Iteration 96/1000 | Loss: 0.00001175
Iteration 97/1000 | Loss: 0.00001175
Iteration 98/1000 | Loss: 0.00001175
Iteration 99/1000 | Loss: 0.00001175
Iteration 100/1000 | Loss: 0.00001174
Iteration 101/1000 | Loss: 0.00001174
Iteration 102/1000 | Loss: 0.00001174
Iteration 103/1000 | Loss: 0.00001174
Iteration 104/1000 | Loss: 0.00001174
Iteration 105/1000 | Loss: 0.00001174
Iteration 106/1000 | Loss: 0.00001174
Iteration 107/1000 | Loss: 0.00001173
Iteration 108/1000 | Loss: 0.00001173
Iteration 109/1000 | Loss: 0.00001172
Iteration 110/1000 | Loss: 0.00001172
Iteration 111/1000 | Loss: 0.00001172
Iteration 112/1000 | Loss: 0.00001171
Iteration 113/1000 | Loss: 0.00001171
Iteration 114/1000 | Loss: 0.00001171
Iteration 115/1000 | Loss: 0.00001171
Iteration 116/1000 | Loss: 0.00001170
Iteration 117/1000 | Loss: 0.00001170
Iteration 118/1000 | Loss: 0.00001170
Iteration 119/1000 | Loss: 0.00001169
Iteration 120/1000 | Loss: 0.00001169
Iteration 121/1000 | Loss: 0.00001169
Iteration 122/1000 | Loss: 0.00001169
Iteration 123/1000 | Loss: 0.00001168
Iteration 124/1000 | Loss: 0.00001167
Iteration 125/1000 | Loss: 0.00001167
Iteration 126/1000 | Loss: 0.00001166
Iteration 127/1000 | Loss: 0.00001166
Iteration 128/1000 | Loss: 0.00001166
Iteration 129/1000 | Loss: 0.00001166
Iteration 130/1000 | Loss: 0.00001166
Iteration 131/1000 | Loss: 0.00001166
Iteration 132/1000 | Loss: 0.00001166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.1662677025015e-05, 1.1662677025015e-05, 1.1662677025015e-05, 1.1662677025015e-05, 1.1662677025015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1662677025015e-05

Optimization complete. Final v2v error: 2.931036949157715 mm

Highest mean error: 3.1816701889038086 mm for frame 87

Lowest mean error: 2.7637951374053955 mm for frame 15

Saving results

Total time: 42.8953812122345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00693389
Iteration 2/25 | Loss: 0.00157361
Iteration 3/25 | Loss: 0.00137829
Iteration 4/25 | Loss: 0.00135825
Iteration 5/25 | Loss: 0.00135506
Iteration 6/25 | Loss: 0.00135480
Iteration 7/25 | Loss: 0.00135480
Iteration 8/25 | Loss: 0.00135480
Iteration 9/25 | Loss: 0.00135480
Iteration 10/25 | Loss: 0.00135480
Iteration 11/25 | Loss: 0.00135480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013547984417527914, 0.0013547984417527914, 0.0013547984417527914, 0.0013547984417527914, 0.0013547984417527914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013547984417527914

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39332318
Iteration 2/25 | Loss: 0.00084713
Iteration 3/25 | Loss: 0.00084712
Iteration 4/25 | Loss: 0.00084712
Iteration 5/25 | Loss: 0.00084711
Iteration 6/25 | Loss: 0.00084711
Iteration 7/25 | Loss: 0.00084711
Iteration 8/25 | Loss: 0.00084711
Iteration 9/25 | Loss: 0.00084711
Iteration 10/25 | Loss: 0.00084711
Iteration 11/25 | Loss: 0.00084711
Iteration 12/25 | Loss: 0.00084711
Iteration 13/25 | Loss: 0.00084711
Iteration 14/25 | Loss: 0.00084711
Iteration 15/25 | Loss: 0.00084711
Iteration 16/25 | Loss: 0.00084711
Iteration 17/25 | Loss: 0.00084711
Iteration 18/25 | Loss: 0.00084711
Iteration 19/25 | Loss: 0.00084711
Iteration 20/25 | Loss: 0.00084711
Iteration 21/25 | Loss: 0.00084711
Iteration 22/25 | Loss: 0.00084711
Iteration 23/25 | Loss: 0.00084711
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008471127948723733, 0.0008471127948723733, 0.0008471127948723733, 0.0008471127948723733, 0.0008471127948723733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008471127948723733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084711
Iteration 2/1000 | Loss: 0.00003555
Iteration 3/1000 | Loss: 0.00002687
Iteration 4/1000 | Loss: 0.00002399
Iteration 5/1000 | Loss: 0.00002277
Iteration 6/1000 | Loss: 0.00002198
Iteration 7/1000 | Loss: 0.00002147
Iteration 8/1000 | Loss: 0.00002106
Iteration 9/1000 | Loss: 0.00002075
Iteration 10/1000 | Loss: 0.00002058
Iteration 11/1000 | Loss: 0.00002038
Iteration 12/1000 | Loss: 0.00002016
Iteration 13/1000 | Loss: 0.00001991
Iteration 14/1000 | Loss: 0.00001981
Iteration 15/1000 | Loss: 0.00001962
Iteration 16/1000 | Loss: 0.00001950
Iteration 17/1000 | Loss: 0.00001943
Iteration 18/1000 | Loss: 0.00001939
Iteration 19/1000 | Loss: 0.00001938
Iteration 20/1000 | Loss: 0.00001935
Iteration 21/1000 | Loss: 0.00001934
Iteration 22/1000 | Loss: 0.00001934
Iteration 23/1000 | Loss: 0.00001934
Iteration 24/1000 | Loss: 0.00001933
Iteration 25/1000 | Loss: 0.00001929
Iteration 26/1000 | Loss: 0.00001929
Iteration 27/1000 | Loss: 0.00001928
Iteration 28/1000 | Loss: 0.00001928
Iteration 29/1000 | Loss: 0.00001927
Iteration 30/1000 | Loss: 0.00001926
Iteration 31/1000 | Loss: 0.00001926
Iteration 32/1000 | Loss: 0.00001926
Iteration 33/1000 | Loss: 0.00001926
Iteration 34/1000 | Loss: 0.00001925
Iteration 35/1000 | Loss: 0.00001925
Iteration 36/1000 | Loss: 0.00001924
Iteration 37/1000 | Loss: 0.00001924
Iteration 38/1000 | Loss: 0.00001923
Iteration 39/1000 | Loss: 0.00001923
Iteration 40/1000 | Loss: 0.00001923
Iteration 41/1000 | Loss: 0.00001923
Iteration 42/1000 | Loss: 0.00001921
Iteration 43/1000 | Loss: 0.00001920
Iteration 44/1000 | Loss: 0.00001920
Iteration 45/1000 | Loss: 0.00001920
Iteration 46/1000 | Loss: 0.00001920
Iteration 47/1000 | Loss: 0.00001920
Iteration 48/1000 | Loss: 0.00001919
Iteration 49/1000 | Loss: 0.00001919
Iteration 50/1000 | Loss: 0.00001919
Iteration 51/1000 | Loss: 0.00001919
Iteration 52/1000 | Loss: 0.00001918
Iteration 53/1000 | Loss: 0.00001918
Iteration 54/1000 | Loss: 0.00001918
Iteration 55/1000 | Loss: 0.00001917
Iteration 56/1000 | Loss: 0.00001917
Iteration 57/1000 | Loss: 0.00001917
Iteration 58/1000 | Loss: 0.00001916
Iteration 59/1000 | Loss: 0.00001916
Iteration 60/1000 | Loss: 0.00001916
Iteration 61/1000 | Loss: 0.00001916
Iteration 62/1000 | Loss: 0.00001916
Iteration 63/1000 | Loss: 0.00001916
Iteration 64/1000 | Loss: 0.00001916
Iteration 65/1000 | Loss: 0.00001916
Iteration 66/1000 | Loss: 0.00001916
Iteration 67/1000 | Loss: 0.00001916
Iteration 68/1000 | Loss: 0.00001916
Iteration 69/1000 | Loss: 0.00001915
Iteration 70/1000 | Loss: 0.00001915
Iteration 71/1000 | Loss: 0.00001915
Iteration 72/1000 | Loss: 0.00001915
Iteration 73/1000 | Loss: 0.00001915
Iteration 74/1000 | Loss: 0.00001914
Iteration 75/1000 | Loss: 0.00001914
Iteration 76/1000 | Loss: 0.00001914
Iteration 77/1000 | Loss: 0.00001914
Iteration 78/1000 | Loss: 0.00001914
Iteration 79/1000 | Loss: 0.00001914
Iteration 80/1000 | Loss: 0.00001914
Iteration 81/1000 | Loss: 0.00001914
Iteration 82/1000 | Loss: 0.00001914
Iteration 83/1000 | Loss: 0.00001914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.9139972209814005e-05, 1.9139972209814005e-05, 1.9139972209814005e-05, 1.9139972209814005e-05, 1.9139972209814005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9139972209814005e-05

Optimization complete. Final v2v error: 3.7236263751983643 mm

Highest mean error: 3.923577308654785 mm for frame 91

Lowest mean error: 3.5662951469421387 mm for frame 194

Saving results

Total time: 41.04976844787598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775406
Iteration 2/25 | Loss: 0.00176774
Iteration 3/25 | Loss: 0.00149378
Iteration 4/25 | Loss: 0.00146611
Iteration 5/25 | Loss: 0.00144605
Iteration 6/25 | Loss: 0.00144103
Iteration 7/25 | Loss: 0.00144980
Iteration 8/25 | Loss: 0.00144061
Iteration 9/25 | Loss: 0.00142856
Iteration 10/25 | Loss: 0.00142613
Iteration 11/25 | Loss: 0.00142328
Iteration 12/25 | Loss: 0.00142093
Iteration 13/25 | Loss: 0.00141924
Iteration 14/25 | Loss: 0.00141358
Iteration 15/25 | Loss: 0.00140527
Iteration 16/25 | Loss: 0.00140331
Iteration 17/25 | Loss: 0.00140639
Iteration 18/25 | Loss: 0.00140114
Iteration 19/25 | Loss: 0.00139765
Iteration 20/25 | Loss: 0.00139641
Iteration 21/25 | Loss: 0.00139595
Iteration 22/25 | Loss: 0.00139656
Iteration 23/25 | Loss: 0.00140113
Iteration 24/25 | Loss: 0.00139711
Iteration 25/25 | Loss: 0.00139540

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38246357
Iteration 2/25 | Loss: 0.00075153
Iteration 3/25 | Loss: 0.00075152
Iteration 4/25 | Loss: 0.00075152
Iteration 5/25 | Loss: 0.00075152
Iteration 6/25 | Loss: 0.00075152
Iteration 7/25 | Loss: 0.00075152
Iteration 8/25 | Loss: 0.00075152
Iteration 9/25 | Loss: 0.00075152
Iteration 10/25 | Loss: 0.00075152
Iteration 11/25 | Loss: 0.00075152
Iteration 12/25 | Loss: 0.00075152
Iteration 13/25 | Loss: 0.00075152
Iteration 14/25 | Loss: 0.00075152
Iteration 15/25 | Loss: 0.00075152
Iteration 16/25 | Loss: 0.00075152
Iteration 17/25 | Loss: 0.00075152
Iteration 18/25 | Loss: 0.00075152
Iteration 19/25 | Loss: 0.00075152
Iteration 20/25 | Loss: 0.00075152
Iteration 21/25 | Loss: 0.00075152
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007515180041082203, 0.0007515180041082203, 0.0007515180041082203, 0.0007515180041082203, 0.0007515180041082203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007515180041082203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075152
Iteration 2/1000 | Loss: 0.00006036
Iteration 3/1000 | Loss: 0.00004346
Iteration 4/1000 | Loss: 0.00005299
Iteration 5/1000 | Loss: 0.00004227
Iteration 6/1000 | Loss: 0.00006158
Iteration 7/1000 | Loss: 0.00003611
Iteration 8/1000 | Loss: 0.00004393
Iteration 9/1000 | Loss: 0.00004113
Iteration 10/1000 | Loss: 0.00006877
Iteration 11/1000 | Loss: 0.00003635
Iteration 12/1000 | Loss: 0.00003967
Iteration 13/1000 | Loss: 0.00003524
Iteration 14/1000 | Loss: 0.00003200
Iteration 15/1000 | Loss: 0.00004128
Iteration 16/1000 | Loss: 0.00003523
Iteration 17/1000 | Loss: 0.00003348
Iteration 18/1000 | Loss: 0.00003985
Iteration 19/1000 | Loss: 0.00003428
Iteration 20/1000 | Loss: 0.00004862
Iteration 21/1000 | Loss: 0.00003887
Iteration 22/1000 | Loss: 0.00003930
Iteration 23/1000 | Loss: 0.00003672
Iteration 24/1000 | Loss: 0.00003867
Iteration 25/1000 | Loss: 0.00005333
Iteration 26/1000 | Loss: 0.00003398
Iteration 27/1000 | Loss: 0.00002967
Iteration 28/1000 | Loss: 0.00022034
Iteration 29/1000 | Loss: 0.00019734
Iteration 30/1000 | Loss: 0.00046675
Iteration 31/1000 | Loss: 0.00007810
Iteration 32/1000 | Loss: 0.00003603
Iteration 33/1000 | Loss: 0.00002963
Iteration 34/1000 | Loss: 0.00002825
Iteration 35/1000 | Loss: 0.00002773
Iteration 36/1000 | Loss: 0.00002743
Iteration 37/1000 | Loss: 0.00002712
Iteration 38/1000 | Loss: 0.00003366
Iteration 39/1000 | Loss: 0.00002686
Iteration 40/1000 | Loss: 0.00002676
Iteration 41/1000 | Loss: 0.00002675
Iteration 42/1000 | Loss: 0.00002675
Iteration 43/1000 | Loss: 0.00002674
Iteration 44/1000 | Loss: 0.00002673
Iteration 45/1000 | Loss: 0.00002672
Iteration 46/1000 | Loss: 0.00002672
Iteration 47/1000 | Loss: 0.00002671
Iteration 48/1000 | Loss: 0.00002671
Iteration 49/1000 | Loss: 0.00002671
Iteration 50/1000 | Loss: 0.00002671
Iteration 51/1000 | Loss: 0.00002670
Iteration 52/1000 | Loss: 0.00002669
Iteration 53/1000 | Loss: 0.00002669
Iteration 54/1000 | Loss: 0.00002669
Iteration 55/1000 | Loss: 0.00002668
Iteration 56/1000 | Loss: 0.00002667
Iteration 57/1000 | Loss: 0.00002667
Iteration 58/1000 | Loss: 0.00002667
Iteration 59/1000 | Loss: 0.00002667
Iteration 60/1000 | Loss: 0.00002666
Iteration 61/1000 | Loss: 0.00002664
Iteration 62/1000 | Loss: 0.00002664
Iteration 63/1000 | Loss: 0.00002663
Iteration 64/1000 | Loss: 0.00004380
Iteration 65/1000 | Loss: 0.00002666
Iteration 66/1000 | Loss: 0.00002658
Iteration 67/1000 | Loss: 0.00002657
Iteration 68/1000 | Loss: 0.00002657
Iteration 69/1000 | Loss: 0.00002656
Iteration 70/1000 | Loss: 0.00002656
Iteration 71/1000 | Loss: 0.00002654
Iteration 72/1000 | Loss: 0.00002654
Iteration 73/1000 | Loss: 0.00002654
Iteration 74/1000 | Loss: 0.00002654
Iteration 75/1000 | Loss: 0.00002654
Iteration 76/1000 | Loss: 0.00002654
Iteration 77/1000 | Loss: 0.00002653
Iteration 78/1000 | Loss: 0.00002653
Iteration 79/1000 | Loss: 0.00002653
Iteration 80/1000 | Loss: 0.00002653
Iteration 81/1000 | Loss: 0.00002653
Iteration 82/1000 | Loss: 0.00002653
Iteration 83/1000 | Loss: 0.00002653
Iteration 84/1000 | Loss: 0.00002652
Iteration 85/1000 | Loss: 0.00002652
Iteration 86/1000 | Loss: 0.00002652
Iteration 87/1000 | Loss: 0.00002652
Iteration 88/1000 | Loss: 0.00002652
Iteration 89/1000 | Loss: 0.00002651
Iteration 90/1000 | Loss: 0.00002651
Iteration 91/1000 | Loss: 0.00002650
Iteration 92/1000 | Loss: 0.00002650
Iteration 93/1000 | Loss: 0.00002650
Iteration 94/1000 | Loss: 0.00002649
Iteration 95/1000 | Loss: 0.00002649
Iteration 96/1000 | Loss: 0.00002648
Iteration 97/1000 | Loss: 0.00002641
Iteration 98/1000 | Loss: 0.00002640
Iteration 99/1000 | Loss: 0.00002639
Iteration 100/1000 | Loss: 0.00002639
Iteration 101/1000 | Loss: 0.00002639
Iteration 102/1000 | Loss: 0.00002639
Iteration 103/1000 | Loss: 0.00002639
Iteration 104/1000 | Loss: 0.00002638
Iteration 105/1000 | Loss: 0.00002638
Iteration 106/1000 | Loss: 0.00002638
Iteration 107/1000 | Loss: 0.00002634
Iteration 108/1000 | Loss: 0.00002633
Iteration 109/1000 | Loss: 0.00002633
Iteration 110/1000 | Loss: 0.00002633
Iteration 111/1000 | Loss: 0.00002632
Iteration 112/1000 | Loss: 0.00002632
Iteration 113/1000 | Loss: 0.00002632
Iteration 114/1000 | Loss: 0.00002631
Iteration 115/1000 | Loss: 0.00002631
Iteration 116/1000 | Loss: 0.00002631
Iteration 117/1000 | Loss: 0.00002631
Iteration 118/1000 | Loss: 0.00002630
Iteration 119/1000 | Loss: 0.00002630
Iteration 120/1000 | Loss: 0.00002630
Iteration 121/1000 | Loss: 0.00002630
Iteration 122/1000 | Loss: 0.00002630
Iteration 123/1000 | Loss: 0.00002630
Iteration 124/1000 | Loss: 0.00002629
Iteration 125/1000 | Loss: 0.00002629
Iteration 126/1000 | Loss: 0.00002629
Iteration 127/1000 | Loss: 0.00002629
Iteration 128/1000 | Loss: 0.00002628
Iteration 129/1000 | Loss: 0.00002628
Iteration 130/1000 | Loss: 0.00002627
Iteration 131/1000 | Loss: 0.00002627
Iteration 132/1000 | Loss: 0.00002627
Iteration 133/1000 | Loss: 0.00002627
Iteration 134/1000 | Loss: 0.00002627
Iteration 135/1000 | Loss: 0.00002626
Iteration 136/1000 | Loss: 0.00002626
Iteration 137/1000 | Loss: 0.00002626
Iteration 138/1000 | Loss: 0.00002626
Iteration 139/1000 | Loss: 0.00002626
Iteration 140/1000 | Loss: 0.00002626
Iteration 141/1000 | Loss: 0.00002626
Iteration 142/1000 | Loss: 0.00002626
Iteration 143/1000 | Loss: 0.00002626
Iteration 144/1000 | Loss: 0.00002626
Iteration 145/1000 | Loss: 0.00002626
Iteration 146/1000 | Loss: 0.00002626
Iteration 147/1000 | Loss: 0.00002626
Iteration 148/1000 | Loss: 0.00002626
Iteration 149/1000 | Loss: 0.00002626
Iteration 150/1000 | Loss: 0.00002626
Iteration 151/1000 | Loss: 0.00002626
Iteration 152/1000 | Loss: 0.00002626
Iteration 153/1000 | Loss: 0.00002626
Iteration 154/1000 | Loss: 0.00002626
Iteration 155/1000 | Loss: 0.00002626
Iteration 156/1000 | Loss: 0.00002626
Iteration 157/1000 | Loss: 0.00002626
Iteration 158/1000 | Loss: 0.00002626
Iteration 159/1000 | Loss: 0.00002626
Iteration 160/1000 | Loss: 0.00002626
Iteration 161/1000 | Loss: 0.00002626
Iteration 162/1000 | Loss: 0.00002626
Iteration 163/1000 | Loss: 0.00002626
Iteration 164/1000 | Loss: 0.00002626
Iteration 165/1000 | Loss: 0.00002626
Iteration 166/1000 | Loss: 0.00002626
Iteration 167/1000 | Loss: 0.00002626
Iteration 168/1000 | Loss: 0.00002626
Iteration 169/1000 | Loss: 0.00002626
Iteration 170/1000 | Loss: 0.00002626
Iteration 171/1000 | Loss: 0.00002626
Iteration 172/1000 | Loss: 0.00002626
Iteration 173/1000 | Loss: 0.00002626
Iteration 174/1000 | Loss: 0.00002626
Iteration 175/1000 | Loss: 0.00002626
Iteration 176/1000 | Loss: 0.00002626
Iteration 177/1000 | Loss: 0.00002626
Iteration 178/1000 | Loss: 0.00002626
Iteration 179/1000 | Loss: 0.00002626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.626014429552015e-05, 2.626014429552015e-05, 2.626014429552015e-05, 2.626014429552015e-05, 2.626014429552015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.626014429552015e-05

Optimization complete. Final v2v error: 4.208284854888916 mm

Highest mean error: 4.917342662811279 mm for frame 184

Lowest mean error: 3.581920862197876 mm for frame 209

Saving results

Total time: 132.81278204917908
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823995
Iteration 2/25 | Loss: 0.00152038
Iteration 3/25 | Loss: 0.00132097
Iteration 4/25 | Loss: 0.00128651
Iteration 5/25 | Loss: 0.00128182
Iteration 6/25 | Loss: 0.00128104
Iteration 7/25 | Loss: 0.00128104
Iteration 8/25 | Loss: 0.00128104
Iteration 9/25 | Loss: 0.00128104
Iteration 10/25 | Loss: 0.00128104
Iteration 11/25 | Loss: 0.00128104
Iteration 12/25 | Loss: 0.00128104
Iteration 13/25 | Loss: 0.00128104
Iteration 14/25 | Loss: 0.00128104
Iteration 15/25 | Loss: 0.00128104
Iteration 16/25 | Loss: 0.00128104
Iteration 17/25 | Loss: 0.00128104
Iteration 18/25 | Loss: 0.00128104
Iteration 19/25 | Loss: 0.00128104
Iteration 20/25 | Loss: 0.00128104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012810369953513145, 0.0012810369953513145, 0.0012810369953513145, 0.0012810369953513145, 0.0012810369953513145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012810369953513145

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02453578
Iteration 2/25 | Loss: 0.00054626
Iteration 3/25 | Loss: 0.00054625
Iteration 4/25 | Loss: 0.00054625
Iteration 5/25 | Loss: 0.00054625
Iteration 6/25 | Loss: 0.00054625
Iteration 7/25 | Loss: 0.00054625
Iteration 8/25 | Loss: 0.00054625
Iteration 9/25 | Loss: 0.00054625
Iteration 10/25 | Loss: 0.00054625
Iteration 11/25 | Loss: 0.00054625
Iteration 12/25 | Loss: 0.00054625
Iteration 13/25 | Loss: 0.00054625
Iteration 14/25 | Loss: 0.00054625
Iteration 15/25 | Loss: 0.00054625
Iteration 16/25 | Loss: 0.00054625
Iteration 17/25 | Loss: 0.00054625
Iteration 18/25 | Loss: 0.00054625
Iteration 19/25 | Loss: 0.00054625
Iteration 20/25 | Loss: 0.00054625
Iteration 21/25 | Loss: 0.00054625
Iteration 22/25 | Loss: 0.00054625
Iteration 23/25 | Loss: 0.00054625
Iteration 24/25 | Loss: 0.00054625
Iteration 25/25 | Loss: 0.00054625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054625
Iteration 2/1000 | Loss: 0.00004585
Iteration 3/1000 | Loss: 0.00003299
Iteration 4/1000 | Loss: 0.00002950
Iteration 5/1000 | Loss: 0.00002782
Iteration 6/1000 | Loss: 0.00002639
Iteration 7/1000 | Loss: 0.00002559
Iteration 8/1000 | Loss: 0.00002495
Iteration 9/1000 | Loss: 0.00002459
Iteration 10/1000 | Loss: 0.00002435
Iteration 11/1000 | Loss: 0.00002408
Iteration 12/1000 | Loss: 0.00002384
Iteration 13/1000 | Loss: 0.00002374
Iteration 14/1000 | Loss: 0.00002373
Iteration 15/1000 | Loss: 0.00002373
Iteration 16/1000 | Loss: 0.00002358
Iteration 17/1000 | Loss: 0.00002350
Iteration 18/1000 | Loss: 0.00002342
Iteration 19/1000 | Loss: 0.00002338
Iteration 20/1000 | Loss: 0.00002338
Iteration 21/1000 | Loss: 0.00002338
Iteration 22/1000 | Loss: 0.00002337
Iteration 23/1000 | Loss: 0.00002337
Iteration 24/1000 | Loss: 0.00002336
Iteration 25/1000 | Loss: 0.00002335
Iteration 26/1000 | Loss: 0.00002335
Iteration 27/1000 | Loss: 0.00002335
Iteration 28/1000 | Loss: 0.00002335
Iteration 29/1000 | Loss: 0.00002334
Iteration 30/1000 | Loss: 0.00002334
Iteration 31/1000 | Loss: 0.00002334
Iteration 32/1000 | Loss: 0.00002334
Iteration 33/1000 | Loss: 0.00002334
Iteration 34/1000 | Loss: 0.00002334
Iteration 35/1000 | Loss: 0.00002334
Iteration 36/1000 | Loss: 0.00002334
Iteration 37/1000 | Loss: 0.00002334
Iteration 38/1000 | Loss: 0.00002331
Iteration 39/1000 | Loss: 0.00002331
Iteration 40/1000 | Loss: 0.00002330
Iteration 41/1000 | Loss: 0.00002330
Iteration 42/1000 | Loss: 0.00002330
Iteration 43/1000 | Loss: 0.00002330
Iteration 44/1000 | Loss: 0.00002329
Iteration 45/1000 | Loss: 0.00002329
Iteration 46/1000 | Loss: 0.00002329
Iteration 47/1000 | Loss: 0.00002329
Iteration 48/1000 | Loss: 0.00002329
Iteration 49/1000 | Loss: 0.00002329
Iteration 50/1000 | Loss: 0.00002329
Iteration 51/1000 | Loss: 0.00002329
Iteration 52/1000 | Loss: 0.00002329
Iteration 53/1000 | Loss: 0.00002329
Iteration 54/1000 | Loss: 0.00002329
Iteration 55/1000 | Loss: 0.00002328
Iteration 56/1000 | Loss: 0.00002328
Iteration 57/1000 | Loss: 0.00002328
Iteration 58/1000 | Loss: 0.00002328
Iteration 59/1000 | Loss: 0.00002328
Iteration 60/1000 | Loss: 0.00002327
Iteration 61/1000 | Loss: 0.00002327
Iteration 62/1000 | Loss: 0.00002327
Iteration 63/1000 | Loss: 0.00002327
Iteration 64/1000 | Loss: 0.00002327
Iteration 65/1000 | Loss: 0.00002327
Iteration 66/1000 | Loss: 0.00002326
Iteration 67/1000 | Loss: 0.00002326
Iteration 68/1000 | Loss: 0.00002325
Iteration 69/1000 | Loss: 0.00002325
Iteration 70/1000 | Loss: 0.00002324
Iteration 71/1000 | Loss: 0.00002324
Iteration 72/1000 | Loss: 0.00002324
Iteration 73/1000 | Loss: 0.00002324
Iteration 74/1000 | Loss: 0.00002324
Iteration 75/1000 | Loss: 0.00002323
Iteration 76/1000 | Loss: 0.00002323
Iteration 77/1000 | Loss: 0.00002323
Iteration 78/1000 | Loss: 0.00002322
Iteration 79/1000 | Loss: 0.00002322
Iteration 80/1000 | Loss: 0.00002322
Iteration 81/1000 | Loss: 0.00002321
Iteration 82/1000 | Loss: 0.00002321
Iteration 83/1000 | Loss: 0.00002321
Iteration 84/1000 | Loss: 0.00002321
Iteration 85/1000 | Loss: 0.00002321
Iteration 86/1000 | Loss: 0.00002321
Iteration 87/1000 | Loss: 0.00002321
Iteration 88/1000 | Loss: 0.00002321
Iteration 89/1000 | Loss: 0.00002321
Iteration 90/1000 | Loss: 0.00002321
Iteration 91/1000 | Loss: 0.00002321
Iteration 92/1000 | Loss: 0.00002321
Iteration 93/1000 | Loss: 0.00002320
Iteration 94/1000 | Loss: 0.00002320
Iteration 95/1000 | Loss: 0.00002320
Iteration 96/1000 | Loss: 0.00002320
Iteration 97/1000 | Loss: 0.00002320
Iteration 98/1000 | Loss: 0.00002320
Iteration 99/1000 | Loss: 0.00002320
Iteration 100/1000 | Loss: 0.00002320
Iteration 101/1000 | Loss: 0.00002320
Iteration 102/1000 | Loss: 0.00002320
Iteration 103/1000 | Loss: 0.00002320
Iteration 104/1000 | Loss: 0.00002320
Iteration 105/1000 | Loss: 0.00002320
Iteration 106/1000 | Loss: 0.00002320
Iteration 107/1000 | Loss: 0.00002320
Iteration 108/1000 | Loss: 0.00002320
Iteration 109/1000 | Loss: 0.00002320
Iteration 110/1000 | Loss: 0.00002320
Iteration 111/1000 | Loss: 0.00002320
Iteration 112/1000 | Loss: 0.00002320
Iteration 113/1000 | Loss: 0.00002320
Iteration 114/1000 | Loss: 0.00002319
Iteration 115/1000 | Loss: 0.00002319
Iteration 116/1000 | Loss: 0.00002319
Iteration 117/1000 | Loss: 0.00002319
Iteration 118/1000 | Loss: 0.00002319
Iteration 119/1000 | Loss: 0.00002319
Iteration 120/1000 | Loss: 0.00002319
Iteration 121/1000 | Loss: 0.00002319
Iteration 122/1000 | Loss: 0.00002319
Iteration 123/1000 | Loss: 0.00002319
Iteration 124/1000 | Loss: 0.00002319
Iteration 125/1000 | Loss: 0.00002319
Iteration 126/1000 | Loss: 0.00002319
Iteration 127/1000 | Loss: 0.00002319
Iteration 128/1000 | Loss: 0.00002319
Iteration 129/1000 | Loss: 0.00002319
Iteration 130/1000 | Loss: 0.00002319
Iteration 131/1000 | Loss: 0.00002319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.319429586350452e-05, 2.319429586350452e-05, 2.319429586350452e-05, 2.319429586350452e-05, 2.319429586350452e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.319429586350452e-05

Optimization complete. Final v2v error: 4.1075897216796875 mm

Highest mean error: 4.399115085601807 mm for frame 149

Lowest mean error: 3.9401514530181885 mm for frame 25

Saving results

Total time: 38.78059458732605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944101
Iteration 2/25 | Loss: 0.00463375
Iteration 3/25 | Loss: 0.00346362
Iteration 4/25 | Loss: 0.00286622
Iteration 5/25 | Loss: 0.00241667
Iteration 6/25 | Loss: 0.00225367
Iteration 7/25 | Loss: 0.00223060
Iteration 8/25 | Loss: 0.00213632
Iteration 9/25 | Loss: 0.00201923
Iteration 10/25 | Loss: 0.00197219
Iteration 11/25 | Loss: 0.00189815
Iteration 12/25 | Loss: 0.00185224
Iteration 13/25 | Loss: 0.00181242
Iteration 14/25 | Loss: 0.00178411
Iteration 15/25 | Loss: 0.00176653
Iteration 16/25 | Loss: 0.00175619
Iteration 17/25 | Loss: 0.00175466
Iteration 18/25 | Loss: 0.00173940
Iteration 19/25 | Loss: 0.00173449
Iteration 20/25 | Loss: 0.00172684
Iteration 21/25 | Loss: 0.00173174
Iteration 22/25 | Loss: 0.00172472
Iteration 23/25 | Loss: 0.00170852
Iteration 24/25 | Loss: 0.00170868
Iteration 25/25 | Loss: 0.00170337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40573382
Iteration 2/25 | Loss: 0.00448143
Iteration 3/25 | Loss: 0.00442593
Iteration 4/25 | Loss: 0.00443746
Iteration 5/25 | Loss: 0.00442594
Iteration 6/25 | Loss: 0.00442593
Iteration 7/25 | Loss: 0.00442593
Iteration 8/25 | Loss: 0.00442593
Iteration 9/25 | Loss: 0.00442593
Iteration 10/25 | Loss: 0.00442593
Iteration 11/25 | Loss: 0.00442593
Iteration 12/25 | Loss: 0.00442593
Iteration 13/25 | Loss: 0.00442593
Iteration 14/25 | Loss: 0.00442593
Iteration 15/25 | Loss: 0.00442593
Iteration 16/25 | Loss: 0.00442593
Iteration 17/25 | Loss: 0.00442593
Iteration 18/25 | Loss: 0.00442593
Iteration 19/25 | Loss: 0.00442593
Iteration 20/25 | Loss: 0.00442593
Iteration 21/25 | Loss: 0.00442593
Iteration 22/25 | Loss: 0.00442593
Iteration 23/25 | Loss: 0.00442593
Iteration 24/25 | Loss: 0.00442593
Iteration 25/25 | Loss: 0.00442593
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.004425927065312862, 0.004425927065312862, 0.004425927065312862, 0.004425927065312862, 0.004425927065312862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004425927065312862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00442593
Iteration 2/1000 | Loss: 0.00071656
Iteration 3/1000 | Loss: 0.00065346
Iteration 4/1000 | Loss: 0.00048865
Iteration 5/1000 | Loss: 0.00031365
Iteration 6/1000 | Loss: 0.00027372
Iteration 7/1000 | Loss: 0.00024160
Iteration 8/1000 | Loss: 0.00021345
Iteration 9/1000 | Loss: 0.00019447
Iteration 10/1000 | Loss: 0.00033597
Iteration 11/1000 | Loss: 0.00043522
Iteration 12/1000 | Loss: 0.00036905
Iteration 13/1000 | Loss: 0.00029950
Iteration 14/1000 | Loss: 0.00026044
Iteration 15/1000 | Loss: 0.00017552
Iteration 16/1000 | Loss: 0.00016943
Iteration 17/1000 | Loss: 0.00016580
Iteration 18/1000 | Loss: 0.00038208
Iteration 19/1000 | Loss: 0.00016621
Iteration 20/1000 | Loss: 0.00032593
Iteration 21/1000 | Loss: 0.00281682
Iteration 22/1000 | Loss: 0.00997278
Iteration 23/1000 | Loss: 0.00181796
Iteration 24/1000 | Loss: 0.00157915
Iteration 25/1000 | Loss: 0.00101961
Iteration 26/1000 | Loss: 0.00025216
Iteration 27/1000 | Loss: 0.00053342
Iteration 28/1000 | Loss: 0.00035482
Iteration 29/1000 | Loss: 0.00097243
Iteration 30/1000 | Loss: 0.00126889
Iteration 31/1000 | Loss: 0.00018128
Iteration 32/1000 | Loss: 0.00016624
Iteration 33/1000 | Loss: 0.00026483
Iteration 34/1000 | Loss: 0.00012797
Iteration 35/1000 | Loss: 0.00022529
Iteration 36/1000 | Loss: 0.00025944
Iteration 37/1000 | Loss: 0.00020740
Iteration 38/1000 | Loss: 0.00019661
Iteration 39/1000 | Loss: 0.00060595
Iteration 40/1000 | Loss: 0.00021544
Iteration 41/1000 | Loss: 0.00031670
Iteration 42/1000 | Loss: 0.00024377
Iteration 43/1000 | Loss: 0.00037403
Iteration 44/1000 | Loss: 0.00017918
Iteration 45/1000 | Loss: 0.00019435
Iteration 46/1000 | Loss: 0.00009884
Iteration 47/1000 | Loss: 0.00016270
Iteration 48/1000 | Loss: 0.00008207
Iteration 49/1000 | Loss: 0.00011969
Iteration 50/1000 | Loss: 0.00007531
Iteration 51/1000 | Loss: 0.00034041
Iteration 52/1000 | Loss: 0.00031049
Iteration 53/1000 | Loss: 0.00028198
Iteration 54/1000 | Loss: 0.00022641
Iteration 55/1000 | Loss: 0.00007523
Iteration 56/1000 | Loss: 0.00027303
Iteration 57/1000 | Loss: 0.00014774
Iteration 58/1000 | Loss: 0.00025063
Iteration 59/1000 | Loss: 0.00033322
Iteration 60/1000 | Loss: 0.00026221
Iteration 61/1000 | Loss: 0.00056826
Iteration 62/1000 | Loss: 0.00028311
Iteration 63/1000 | Loss: 0.00057225
Iteration 64/1000 | Loss: 0.00007742
Iteration 65/1000 | Loss: 0.00006890
Iteration 66/1000 | Loss: 0.00008431
Iteration 67/1000 | Loss: 0.00006936
Iteration 68/1000 | Loss: 0.00007471
Iteration 69/1000 | Loss: 0.00007195
Iteration 70/1000 | Loss: 0.00006532
Iteration 71/1000 | Loss: 0.00021570
Iteration 72/1000 | Loss: 0.00094668
Iteration 73/1000 | Loss: 0.00008355
Iteration 74/1000 | Loss: 0.00007025
Iteration 75/1000 | Loss: 0.00006384
Iteration 76/1000 | Loss: 0.00006014
Iteration 77/1000 | Loss: 0.00017963
Iteration 78/1000 | Loss: 0.00028339
Iteration 79/1000 | Loss: 0.00009479
Iteration 80/1000 | Loss: 0.00006065
Iteration 81/1000 | Loss: 0.00006124
Iteration 82/1000 | Loss: 0.00005680
Iteration 83/1000 | Loss: 0.00005610
Iteration 84/1000 | Loss: 0.00005537
Iteration 85/1000 | Loss: 0.00021321
Iteration 86/1000 | Loss: 0.00043393
Iteration 87/1000 | Loss: 0.00035557
Iteration 88/1000 | Loss: 0.00018595
Iteration 89/1000 | Loss: 0.00040708
Iteration 90/1000 | Loss: 0.00016724
Iteration 91/1000 | Loss: 0.00009864
Iteration 92/1000 | Loss: 0.00006011
Iteration 93/1000 | Loss: 0.00005476
Iteration 94/1000 | Loss: 0.00005313
Iteration 95/1000 | Loss: 0.00023239
Iteration 96/1000 | Loss: 0.00030516
Iteration 97/1000 | Loss: 0.00028212
Iteration 98/1000 | Loss: 0.00019492
Iteration 99/1000 | Loss: 0.00018247
Iteration 100/1000 | Loss: 0.00005076
Iteration 101/1000 | Loss: 0.00018827
Iteration 102/1000 | Loss: 0.00026495
Iteration 103/1000 | Loss: 0.00025276
Iteration 104/1000 | Loss: 0.00005202
Iteration 105/1000 | Loss: 0.00017429
Iteration 106/1000 | Loss: 0.00014204
Iteration 107/1000 | Loss: 0.00005131
Iteration 108/1000 | Loss: 0.00004981
Iteration 109/1000 | Loss: 0.00004888
Iteration 110/1000 | Loss: 0.00004835
Iteration 111/1000 | Loss: 0.00004795
Iteration 112/1000 | Loss: 0.00004763
Iteration 113/1000 | Loss: 0.00004742
Iteration 114/1000 | Loss: 0.00004730
Iteration 115/1000 | Loss: 0.00004730
Iteration 116/1000 | Loss: 0.00004728
Iteration 117/1000 | Loss: 0.00004728
Iteration 118/1000 | Loss: 0.00004727
Iteration 119/1000 | Loss: 0.00004726
Iteration 120/1000 | Loss: 0.00004726
Iteration 121/1000 | Loss: 0.00004725
Iteration 122/1000 | Loss: 0.00004725
Iteration 123/1000 | Loss: 0.00004718
Iteration 124/1000 | Loss: 0.00004717
Iteration 125/1000 | Loss: 0.00004716
Iteration 126/1000 | Loss: 0.00004713
Iteration 127/1000 | Loss: 0.00004708
Iteration 128/1000 | Loss: 0.00004704
Iteration 129/1000 | Loss: 0.00004704
Iteration 130/1000 | Loss: 0.00004702
Iteration 131/1000 | Loss: 0.00004702
Iteration 132/1000 | Loss: 0.00004701
Iteration 133/1000 | Loss: 0.00004701
Iteration 134/1000 | Loss: 0.00004699
Iteration 135/1000 | Loss: 0.00004699
Iteration 136/1000 | Loss: 0.00004698
Iteration 137/1000 | Loss: 0.00004698
Iteration 138/1000 | Loss: 0.00004698
Iteration 139/1000 | Loss: 0.00004698
Iteration 140/1000 | Loss: 0.00004698
Iteration 141/1000 | Loss: 0.00004698
Iteration 142/1000 | Loss: 0.00004698
Iteration 143/1000 | Loss: 0.00004698
Iteration 144/1000 | Loss: 0.00004698
Iteration 145/1000 | Loss: 0.00004698
Iteration 146/1000 | Loss: 0.00004698
Iteration 147/1000 | Loss: 0.00004698
Iteration 148/1000 | Loss: 0.00004698
Iteration 149/1000 | Loss: 0.00004697
Iteration 150/1000 | Loss: 0.00004697
Iteration 151/1000 | Loss: 0.00004697
Iteration 152/1000 | Loss: 0.00004697
Iteration 153/1000 | Loss: 0.00004696
Iteration 154/1000 | Loss: 0.00004696
Iteration 155/1000 | Loss: 0.00004696
Iteration 156/1000 | Loss: 0.00004696
Iteration 157/1000 | Loss: 0.00004696
Iteration 158/1000 | Loss: 0.00004696
Iteration 159/1000 | Loss: 0.00004696
Iteration 160/1000 | Loss: 0.00004696
Iteration 161/1000 | Loss: 0.00004695
Iteration 162/1000 | Loss: 0.00004695
Iteration 163/1000 | Loss: 0.00004695
Iteration 164/1000 | Loss: 0.00004695
Iteration 165/1000 | Loss: 0.00004695
Iteration 166/1000 | Loss: 0.00004695
Iteration 167/1000 | Loss: 0.00004695
Iteration 168/1000 | Loss: 0.00004695
Iteration 169/1000 | Loss: 0.00004695
Iteration 170/1000 | Loss: 0.00004695
Iteration 171/1000 | Loss: 0.00004694
Iteration 172/1000 | Loss: 0.00004694
Iteration 173/1000 | Loss: 0.00004694
Iteration 174/1000 | Loss: 0.00004694
Iteration 175/1000 | Loss: 0.00004694
Iteration 176/1000 | Loss: 0.00004694
Iteration 177/1000 | Loss: 0.00004694
Iteration 178/1000 | Loss: 0.00004693
Iteration 179/1000 | Loss: 0.00004693
Iteration 180/1000 | Loss: 0.00004693
Iteration 181/1000 | Loss: 0.00004693
Iteration 182/1000 | Loss: 0.00004693
Iteration 183/1000 | Loss: 0.00004693
Iteration 184/1000 | Loss: 0.00004693
Iteration 185/1000 | Loss: 0.00004693
Iteration 186/1000 | Loss: 0.00004692
Iteration 187/1000 | Loss: 0.00004692
Iteration 188/1000 | Loss: 0.00004692
Iteration 189/1000 | Loss: 0.00004692
Iteration 190/1000 | Loss: 0.00004692
Iteration 191/1000 | Loss: 0.00004692
Iteration 192/1000 | Loss: 0.00004692
Iteration 193/1000 | Loss: 0.00004692
Iteration 194/1000 | Loss: 0.00004692
Iteration 195/1000 | Loss: 0.00004692
Iteration 196/1000 | Loss: 0.00004692
Iteration 197/1000 | Loss: 0.00004692
Iteration 198/1000 | Loss: 0.00004692
Iteration 199/1000 | Loss: 0.00004691
Iteration 200/1000 | Loss: 0.00004691
Iteration 201/1000 | Loss: 0.00004691
Iteration 202/1000 | Loss: 0.00004691
Iteration 203/1000 | Loss: 0.00004691
Iteration 204/1000 | Loss: 0.00004691
Iteration 205/1000 | Loss: 0.00004691
Iteration 206/1000 | Loss: 0.00004690
Iteration 207/1000 | Loss: 0.00004690
Iteration 208/1000 | Loss: 0.00004690
Iteration 209/1000 | Loss: 0.00004690
Iteration 210/1000 | Loss: 0.00004690
Iteration 211/1000 | Loss: 0.00004690
Iteration 212/1000 | Loss: 0.00004690
Iteration 213/1000 | Loss: 0.00004690
Iteration 214/1000 | Loss: 0.00004690
Iteration 215/1000 | Loss: 0.00004690
Iteration 216/1000 | Loss: 0.00004690
Iteration 217/1000 | Loss: 0.00004690
Iteration 218/1000 | Loss: 0.00004690
Iteration 219/1000 | Loss: 0.00004690
Iteration 220/1000 | Loss: 0.00004690
Iteration 221/1000 | Loss: 0.00004690
Iteration 222/1000 | Loss: 0.00004690
Iteration 223/1000 | Loss: 0.00004689
Iteration 224/1000 | Loss: 0.00004689
Iteration 225/1000 | Loss: 0.00004689
Iteration 226/1000 | Loss: 0.00004689
Iteration 227/1000 | Loss: 0.00004689
Iteration 228/1000 | Loss: 0.00004689
Iteration 229/1000 | Loss: 0.00004689
Iteration 230/1000 | Loss: 0.00004689
Iteration 231/1000 | Loss: 0.00004689
Iteration 232/1000 | Loss: 0.00004689
Iteration 233/1000 | Loss: 0.00004689
Iteration 234/1000 | Loss: 0.00004689
Iteration 235/1000 | Loss: 0.00004689
Iteration 236/1000 | Loss: 0.00004688
Iteration 237/1000 | Loss: 0.00004688
Iteration 238/1000 | Loss: 0.00004688
Iteration 239/1000 | Loss: 0.00004688
Iteration 240/1000 | Loss: 0.00004688
Iteration 241/1000 | Loss: 0.00004688
Iteration 242/1000 | Loss: 0.00004688
Iteration 243/1000 | Loss: 0.00004688
Iteration 244/1000 | Loss: 0.00004688
Iteration 245/1000 | Loss: 0.00004688
Iteration 246/1000 | Loss: 0.00004688
Iteration 247/1000 | Loss: 0.00004688
Iteration 248/1000 | Loss: 0.00004688
Iteration 249/1000 | Loss: 0.00004688
Iteration 250/1000 | Loss: 0.00004688
Iteration 251/1000 | Loss: 0.00004688
Iteration 252/1000 | Loss: 0.00004688
Iteration 253/1000 | Loss: 0.00004687
Iteration 254/1000 | Loss: 0.00004687
Iteration 255/1000 | Loss: 0.00004687
Iteration 256/1000 | Loss: 0.00004687
Iteration 257/1000 | Loss: 0.00004687
Iteration 258/1000 | Loss: 0.00004687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [4.687480759457685e-05, 4.687480759457685e-05, 4.687480759457685e-05, 4.687480759457685e-05, 4.687480759457685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.687480759457685e-05

Optimization complete. Final v2v error: 4.384315013885498 mm

Highest mean error: 10.81234359741211 mm for frame 221

Lowest mean error: 3.7171452045440674 mm for frame 233

Saving results

Total time: 254.0206995010376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404037
Iteration 2/25 | Loss: 0.00130789
Iteration 3/25 | Loss: 0.00126702
Iteration 4/25 | Loss: 0.00126013
Iteration 5/25 | Loss: 0.00125860
Iteration 6/25 | Loss: 0.00125860
Iteration 7/25 | Loss: 0.00125860
Iteration 8/25 | Loss: 0.00125860
Iteration 9/25 | Loss: 0.00125860
Iteration 10/25 | Loss: 0.00125860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012585966615006328, 0.0012585966615006328, 0.0012585966615006328, 0.0012585966615006328, 0.0012585966615006328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012585966615006328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42720556
Iteration 2/25 | Loss: 0.00085094
Iteration 3/25 | Loss: 0.00085094
Iteration 4/25 | Loss: 0.00085094
Iteration 5/25 | Loss: 0.00085094
Iteration 6/25 | Loss: 0.00085094
Iteration 7/25 | Loss: 0.00085094
Iteration 8/25 | Loss: 0.00085094
Iteration 9/25 | Loss: 0.00085094
Iteration 10/25 | Loss: 0.00085094
Iteration 11/25 | Loss: 0.00085094
Iteration 12/25 | Loss: 0.00085094
Iteration 13/25 | Loss: 0.00085094
Iteration 14/25 | Loss: 0.00085094
Iteration 15/25 | Loss: 0.00085094
Iteration 16/25 | Loss: 0.00085094
Iteration 17/25 | Loss: 0.00085094
Iteration 18/25 | Loss: 0.00085094
Iteration 19/25 | Loss: 0.00085094
Iteration 20/25 | Loss: 0.00085094
Iteration 21/25 | Loss: 0.00085094
Iteration 22/25 | Loss: 0.00085094
Iteration 23/25 | Loss: 0.00085094
Iteration 24/25 | Loss: 0.00085094
Iteration 25/25 | Loss: 0.00085094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085094
Iteration 2/1000 | Loss: 0.00002659
Iteration 3/1000 | Loss: 0.00001688
Iteration 4/1000 | Loss: 0.00001534
Iteration 5/1000 | Loss: 0.00001458
Iteration 6/1000 | Loss: 0.00001415
Iteration 7/1000 | Loss: 0.00001371
Iteration 8/1000 | Loss: 0.00001355
Iteration 9/1000 | Loss: 0.00001334
Iteration 10/1000 | Loss: 0.00001317
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001308
Iteration 13/1000 | Loss: 0.00001297
Iteration 14/1000 | Loss: 0.00001296
Iteration 15/1000 | Loss: 0.00001288
Iteration 16/1000 | Loss: 0.00001286
Iteration 17/1000 | Loss: 0.00001285
Iteration 18/1000 | Loss: 0.00001284
Iteration 19/1000 | Loss: 0.00001281
Iteration 20/1000 | Loss: 0.00001280
Iteration 21/1000 | Loss: 0.00001279
Iteration 22/1000 | Loss: 0.00001278
Iteration 23/1000 | Loss: 0.00001278
Iteration 24/1000 | Loss: 0.00001277
Iteration 25/1000 | Loss: 0.00001277
Iteration 26/1000 | Loss: 0.00001276
Iteration 27/1000 | Loss: 0.00001274
Iteration 28/1000 | Loss: 0.00001271
Iteration 29/1000 | Loss: 0.00001270
Iteration 30/1000 | Loss: 0.00001269
Iteration 31/1000 | Loss: 0.00001267
Iteration 32/1000 | Loss: 0.00001266
Iteration 33/1000 | Loss: 0.00001260
Iteration 34/1000 | Loss: 0.00001259
Iteration 35/1000 | Loss: 0.00001259
Iteration 36/1000 | Loss: 0.00001259
Iteration 37/1000 | Loss: 0.00001258
Iteration 38/1000 | Loss: 0.00001257
Iteration 39/1000 | Loss: 0.00001253
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00001250
Iteration 42/1000 | Loss: 0.00001250
Iteration 43/1000 | Loss: 0.00001249
Iteration 44/1000 | Loss: 0.00001243
Iteration 45/1000 | Loss: 0.00001241
Iteration 46/1000 | Loss: 0.00001239
Iteration 47/1000 | Loss: 0.00001239
Iteration 48/1000 | Loss: 0.00001239
Iteration 49/1000 | Loss: 0.00001238
Iteration 50/1000 | Loss: 0.00001236
Iteration 51/1000 | Loss: 0.00001235
Iteration 52/1000 | Loss: 0.00001234
Iteration 53/1000 | Loss: 0.00001234
Iteration 54/1000 | Loss: 0.00001234
Iteration 55/1000 | Loss: 0.00001234
Iteration 56/1000 | Loss: 0.00001234
Iteration 57/1000 | Loss: 0.00001234
Iteration 58/1000 | Loss: 0.00001233
Iteration 59/1000 | Loss: 0.00001233
Iteration 60/1000 | Loss: 0.00001233
Iteration 61/1000 | Loss: 0.00001233
Iteration 62/1000 | Loss: 0.00001233
Iteration 63/1000 | Loss: 0.00001232
Iteration 64/1000 | Loss: 0.00001230
Iteration 65/1000 | Loss: 0.00001230
Iteration 66/1000 | Loss: 0.00001229
Iteration 67/1000 | Loss: 0.00001228
Iteration 68/1000 | Loss: 0.00001228
Iteration 69/1000 | Loss: 0.00001227
Iteration 70/1000 | Loss: 0.00001227
Iteration 71/1000 | Loss: 0.00001227
Iteration 72/1000 | Loss: 0.00001226
Iteration 73/1000 | Loss: 0.00001226
Iteration 74/1000 | Loss: 0.00001222
Iteration 75/1000 | Loss: 0.00001221
Iteration 76/1000 | Loss: 0.00001221
Iteration 77/1000 | Loss: 0.00001220
Iteration 78/1000 | Loss: 0.00001220
Iteration 79/1000 | Loss: 0.00001219
Iteration 80/1000 | Loss: 0.00001219
Iteration 81/1000 | Loss: 0.00001219
Iteration 82/1000 | Loss: 0.00001218
Iteration 83/1000 | Loss: 0.00001218
Iteration 84/1000 | Loss: 0.00001218
Iteration 85/1000 | Loss: 0.00001218
Iteration 86/1000 | Loss: 0.00001218
Iteration 87/1000 | Loss: 0.00001218
Iteration 88/1000 | Loss: 0.00001217
Iteration 89/1000 | Loss: 0.00001217
Iteration 90/1000 | Loss: 0.00001217
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001217
Iteration 93/1000 | Loss: 0.00001217
Iteration 94/1000 | Loss: 0.00001217
Iteration 95/1000 | Loss: 0.00001217
Iteration 96/1000 | Loss: 0.00001217
Iteration 97/1000 | Loss: 0.00001217
Iteration 98/1000 | Loss: 0.00001217
Iteration 99/1000 | Loss: 0.00001216
Iteration 100/1000 | Loss: 0.00001216
Iteration 101/1000 | Loss: 0.00001216
Iteration 102/1000 | Loss: 0.00001216
Iteration 103/1000 | Loss: 0.00001216
Iteration 104/1000 | Loss: 0.00001216
Iteration 105/1000 | Loss: 0.00001216
Iteration 106/1000 | Loss: 0.00001216
Iteration 107/1000 | Loss: 0.00001216
Iteration 108/1000 | Loss: 0.00001216
Iteration 109/1000 | Loss: 0.00001216
Iteration 110/1000 | Loss: 0.00001216
Iteration 111/1000 | Loss: 0.00001216
Iteration 112/1000 | Loss: 0.00001216
Iteration 113/1000 | Loss: 0.00001216
Iteration 114/1000 | Loss: 0.00001216
Iteration 115/1000 | Loss: 0.00001216
Iteration 116/1000 | Loss: 0.00001216
Iteration 117/1000 | Loss: 0.00001216
Iteration 118/1000 | Loss: 0.00001216
Iteration 119/1000 | Loss: 0.00001216
Iteration 120/1000 | Loss: 0.00001216
Iteration 121/1000 | Loss: 0.00001216
Iteration 122/1000 | Loss: 0.00001216
Iteration 123/1000 | Loss: 0.00001216
Iteration 124/1000 | Loss: 0.00001216
Iteration 125/1000 | Loss: 0.00001216
Iteration 126/1000 | Loss: 0.00001216
Iteration 127/1000 | Loss: 0.00001216
Iteration 128/1000 | Loss: 0.00001216
Iteration 129/1000 | Loss: 0.00001216
Iteration 130/1000 | Loss: 0.00001216
Iteration 131/1000 | Loss: 0.00001216
Iteration 132/1000 | Loss: 0.00001216
Iteration 133/1000 | Loss: 0.00001216
Iteration 134/1000 | Loss: 0.00001216
Iteration 135/1000 | Loss: 0.00001216
Iteration 136/1000 | Loss: 0.00001216
Iteration 137/1000 | Loss: 0.00001216
Iteration 138/1000 | Loss: 0.00001216
Iteration 139/1000 | Loss: 0.00001216
Iteration 140/1000 | Loss: 0.00001216
Iteration 141/1000 | Loss: 0.00001216
Iteration 142/1000 | Loss: 0.00001216
Iteration 143/1000 | Loss: 0.00001216
Iteration 144/1000 | Loss: 0.00001216
Iteration 145/1000 | Loss: 0.00001216
Iteration 146/1000 | Loss: 0.00001216
Iteration 147/1000 | Loss: 0.00001216
Iteration 148/1000 | Loss: 0.00001216
Iteration 149/1000 | Loss: 0.00001216
Iteration 150/1000 | Loss: 0.00001216
Iteration 151/1000 | Loss: 0.00001216
Iteration 152/1000 | Loss: 0.00001216
Iteration 153/1000 | Loss: 0.00001216
Iteration 154/1000 | Loss: 0.00001216
Iteration 155/1000 | Loss: 0.00001216
Iteration 156/1000 | Loss: 0.00001216
Iteration 157/1000 | Loss: 0.00001216
Iteration 158/1000 | Loss: 0.00001216
Iteration 159/1000 | Loss: 0.00001216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.2160972801211756e-05, 1.2160972801211756e-05, 1.2160972801211756e-05, 1.2160972801211756e-05, 1.2160972801211756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2160972801211756e-05

Optimization complete. Final v2v error: 3.0284674167633057 mm

Highest mean error: 3.109477996826172 mm for frame 102

Lowest mean error: 2.9778048992156982 mm for frame 1

Saving results

Total time: 37.060988426208496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054308
Iteration 2/25 | Loss: 0.00174357
Iteration 3/25 | Loss: 0.00151087
Iteration 4/25 | Loss: 0.00144051
Iteration 5/25 | Loss: 0.00140957
Iteration 6/25 | Loss: 0.00138756
Iteration 7/25 | Loss: 0.00138376
Iteration 8/25 | Loss: 0.00137710
Iteration 9/25 | Loss: 0.00137444
Iteration 10/25 | Loss: 0.00137766
Iteration 11/25 | Loss: 0.00137392
Iteration 12/25 | Loss: 0.00137388
Iteration 13/25 | Loss: 0.00137388
Iteration 14/25 | Loss: 0.00137388
Iteration 15/25 | Loss: 0.00137388
Iteration 16/25 | Loss: 0.00137387
Iteration 17/25 | Loss: 0.00137387
Iteration 18/25 | Loss: 0.00137387
Iteration 19/25 | Loss: 0.00137387
Iteration 20/25 | Loss: 0.00137387
Iteration 21/25 | Loss: 0.00137387
Iteration 22/25 | Loss: 0.00137387
Iteration 23/25 | Loss: 0.00137387
Iteration 24/25 | Loss: 0.00137387
Iteration 25/25 | Loss: 0.00137387

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.11281395
Iteration 2/25 | Loss: 0.00106796
Iteration 3/25 | Loss: 0.00106796
Iteration 4/25 | Loss: 0.00106796
Iteration 5/25 | Loss: 0.00106796
Iteration 6/25 | Loss: 0.00106795
Iteration 7/25 | Loss: 0.00106795
Iteration 8/25 | Loss: 0.00106795
Iteration 9/25 | Loss: 0.00106795
Iteration 10/25 | Loss: 0.00106795
Iteration 11/25 | Loss: 0.00106795
Iteration 12/25 | Loss: 0.00106795
Iteration 13/25 | Loss: 0.00106795
Iteration 14/25 | Loss: 0.00106795
Iteration 15/25 | Loss: 0.00106795
Iteration 16/25 | Loss: 0.00106795
Iteration 17/25 | Loss: 0.00106795
Iteration 18/25 | Loss: 0.00106795
Iteration 19/25 | Loss: 0.00106795
Iteration 20/25 | Loss: 0.00106795
Iteration 21/25 | Loss: 0.00106795
Iteration 22/25 | Loss: 0.00106795
Iteration 23/25 | Loss: 0.00106795
Iteration 24/25 | Loss: 0.00106795
Iteration 25/25 | Loss: 0.00106795

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106795
Iteration 2/1000 | Loss: 0.00010524
Iteration 3/1000 | Loss: 0.00007016
Iteration 4/1000 | Loss: 0.00005553
Iteration 5/1000 | Loss: 0.00004988
Iteration 6/1000 | Loss: 0.00004732
Iteration 7/1000 | Loss: 0.00004536
Iteration 8/1000 | Loss: 0.00004407
Iteration 9/1000 | Loss: 0.00010647
Iteration 10/1000 | Loss: 0.00004833
Iteration 11/1000 | Loss: 0.00004191
Iteration 12/1000 | Loss: 0.00004122
Iteration 13/1000 | Loss: 0.00004095
Iteration 14/1000 | Loss: 0.00004058
Iteration 15/1000 | Loss: 0.00013808
Iteration 16/1000 | Loss: 0.00004040
Iteration 17/1000 | Loss: 0.00003996
Iteration 18/1000 | Loss: 0.00003978
Iteration 19/1000 | Loss: 0.00003960
Iteration 20/1000 | Loss: 0.00011700
Iteration 21/1000 | Loss: 0.00003952
Iteration 22/1000 | Loss: 0.00003935
Iteration 23/1000 | Loss: 0.00003935
Iteration 24/1000 | Loss: 0.00003935
Iteration 25/1000 | Loss: 0.00003935
Iteration 26/1000 | Loss: 0.00003935
Iteration 27/1000 | Loss: 0.00003935
Iteration 28/1000 | Loss: 0.00003935
Iteration 29/1000 | Loss: 0.00003935
Iteration 30/1000 | Loss: 0.00003935
Iteration 31/1000 | Loss: 0.00003935
Iteration 32/1000 | Loss: 0.00003935
Iteration 33/1000 | Loss: 0.00003934
Iteration 34/1000 | Loss: 0.00003934
Iteration 35/1000 | Loss: 0.00003934
Iteration 36/1000 | Loss: 0.00003934
Iteration 37/1000 | Loss: 0.00003928
Iteration 38/1000 | Loss: 0.00003927
Iteration 39/1000 | Loss: 0.00003924
Iteration 40/1000 | Loss: 0.00003924
Iteration 41/1000 | Loss: 0.00009940
Iteration 42/1000 | Loss: 0.00005099
Iteration 43/1000 | Loss: 0.00006391
Iteration 44/1000 | Loss: 0.00005332
Iteration 45/1000 | Loss: 0.00005426
Iteration 46/1000 | Loss: 0.00005426
Iteration 47/1000 | Loss: 0.00014070
Iteration 48/1000 | Loss: 0.00005736
Iteration 49/1000 | Loss: 0.00008048
Iteration 50/1000 | Loss: 0.00004328
Iteration 51/1000 | Loss: 0.00003910
Iteration 52/1000 | Loss: 0.00003910
Iteration 53/1000 | Loss: 0.00003909
Iteration 54/1000 | Loss: 0.00003909
Iteration 55/1000 | Loss: 0.00003909
Iteration 56/1000 | Loss: 0.00003909
Iteration 57/1000 | Loss: 0.00003909
Iteration 58/1000 | Loss: 0.00003909
Iteration 59/1000 | Loss: 0.00003909
Iteration 60/1000 | Loss: 0.00003909
Iteration 61/1000 | Loss: 0.00003909
Iteration 62/1000 | Loss: 0.00003909
Iteration 63/1000 | Loss: 0.00003909
Iteration 64/1000 | Loss: 0.00003908
Iteration 65/1000 | Loss: 0.00003908
Iteration 66/1000 | Loss: 0.00003908
Iteration 67/1000 | Loss: 0.00003907
Iteration 68/1000 | Loss: 0.00003907
Iteration 69/1000 | Loss: 0.00003907
Iteration 70/1000 | Loss: 0.00003906
Iteration 71/1000 | Loss: 0.00003906
Iteration 72/1000 | Loss: 0.00003905
Iteration 73/1000 | Loss: 0.00003904
Iteration 74/1000 | Loss: 0.00003903
Iteration 75/1000 | Loss: 0.00003902
Iteration 76/1000 | Loss: 0.00003902
Iteration 77/1000 | Loss: 0.00003902
Iteration 78/1000 | Loss: 0.00003901
Iteration 79/1000 | Loss: 0.00003901
Iteration 80/1000 | Loss: 0.00003901
Iteration 81/1000 | Loss: 0.00003900
Iteration 82/1000 | Loss: 0.00003900
Iteration 83/1000 | Loss: 0.00003900
Iteration 84/1000 | Loss: 0.00003900
Iteration 85/1000 | Loss: 0.00003899
Iteration 86/1000 | Loss: 0.00003899
Iteration 87/1000 | Loss: 0.00003899
Iteration 88/1000 | Loss: 0.00003899
Iteration 89/1000 | Loss: 0.00003899
Iteration 90/1000 | Loss: 0.00003897
Iteration 91/1000 | Loss: 0.00003897
Iteration 92/1000 | Loss: 0.00003896
Iteration 93/1000 | Loss: 0.00003896
Iteration 94/1000 | Loss: 0.00003896
Iteration 95/1000 | Loss: 0.00003896
Iteration 96/1000 | Loss: 0.00003896
Iteration 97/1000 | Loss: 0.00006551
Iteration 98/1000 | Loss: 0.00004153
Iteration 99/1000 | Loss: 0.00003895
Iteration 100/1000 | Loss: 0.00003895
Iteration 101/1000 | Loss: 0.00003895
Iteration 102/1000 | Loss: 0.00003895
Iteration 103/1000 | Loss: 0.00003895
Iteration 104/1000 | Loss: 0.00003895
Iteration 105/1000 | Loss: 0.00003895
Iteration 106/1000 | Loss: 0.00003895
Iteration 107/1000 | Loss: 0.00003895
Iteration 108/1000 | Loss: 0.00003895
Iteration 109/1000 | Loss: 0.00003895
Iteration 110/1000 | Loss: 0.00003895
Iteration 111/1000 | Loss: 0.00003895
Iteration 112/1000 | Loss: 0.00003895
Iteration 113/1000 | Loss: 0.00003895
Iteration 114/1000 | Loss: 0.00003895
Iteration 115/1000 | Loss: 0.00003894
Iteration 116/1000 | Loss: 0.00003894
Iteration 117/1000 | Loss: 0.00003894
Iteration 118/1000 | Loss: 0.00003894
Iteration 119/1000 | Loss: 0.00003894
Iteration 120/1000 | Loss: 0.00003894
Iteration 121/1000 | Loss: 0.00003893
Iteration 122/1000 | Loss: 0.00003893
Iteration 123/1000 | Loss: 0.00003893
Iteration 124/1000 | Loss: 0.00003893
Iteration 125/1000 | Loss: 0.00003893
Iteration 126/1000 | Loss: 0.00003893
Iteration 127/1000 | Loss: 0.00003892
Iteration 128/1000 | Loss: 0.00003889
Iteration 129/1000 | Loss: 0.00006906
Iteration 130/1000 | Loss: 0.00004376
Iteration 131/1000 | Loss: 0.00005988
Iteration 132/1000 | Loss: 0.00003893
Iteration 133/1000 | Loss: 0.00003880
Iteration 134/1000 | Loss: 0.00003879
Iteration 135/1000 | Loss: 0.00003877
Iteration 136/1000 | Loss: 0.00003877
Iteration 137/1000 | Loss: 0.00003877
Iteration 138/1000 | Loss: 0.00003877
Iteration 139/1000 | Loss: 0.00003876
Iteration 140/1000 | Loss: 0.00003876
Iteration 141/1000 | Loss: 0.00003875
Iteration 142/1000 | Loss: 0.00003875
Iteration 143/1000 | Loss: 0.00003874
Iteration 144/1000 | Loss: 0.00003874
Iteration 145/1000 | Loss: 0.00003874
Iteration 146/1000 | Loss: 0.00003874
Iteration 147/1000 | Loss: 0.00003873
Iteration 148/1000 | Loss: 0.00003873
Iteration 149/1000 | Loss: 0.00003873
Iteration 150/1000 | Loss: 0.00003873
Iteration 151/1000 | Loss: 0.00003873
Iteration 152/1000 | Loss: 0.00003873
Iteration 153/1000 | Loss: 0.00003873
Iteration 154/1000 | Loss: 0.00003873
Iteration 155/1000 | Loss: 0.00003873
Iteration 156/1000 | Loss: 0.00003873
Iteration 157/1000 | Loss: 0.00003873
Iteration 158/1000 | Loss: 0.00003872
Iteration 159/1000 | Loss: 0.00003872
Iteration 160/1000 | Loss: 0.00003872
Iteration 161/1000 | Loss: 0.00003872
Iteration 162/1000 | Loss: 0.00003872
Iteration 163/1000 | Loss: 0.00003872
Iteration 164/1000 | Loss: 0.00003872
Iteration 165/1000 | Loss: 0.00003872
Iteration 166/1000 | Loss: 0.00003872
Iteration 167/1000 | Loss: 0.00003872
Iteration 168/1000 | Loss: 0.00003872
Iteration 169/1000 | Loss: 0.00003872
Iteration 170/1000 | Loss: 0.00003871
Iteration 171/1000 | Loss: 0.00003871
Iteration 172/1000 | Loss: 0.00003871
Iteration 173/1000 | Loss: 0.00003871
Iteration 174/1000 | Loss: 0.00003871
Iteration 175/1000 | Loss: 0.00003871
Iteration 176/1000 | Loss: 0.00003871
Iteration 177/1000 | Loss: 0.00003871
Iteration 178/1000 | Loss: 0.00003871
Iteration 179/1000 | Loss: 0.00003871
Iteration 180/1000 | Loss: 0.00003870
Iteration 181/1000 | Loss: 0.00003870
Iteration 182/1000 | Loss: 0.00003870
Iteration 183/1000 | Loss: 0.00003870
Iteration 184/1000 | Loss: 0.00003870
Iteration 185/1000 | Loss: 0.00003870
Iteration 186/1000 | Loss: 0.00003870
Iteration 187/1000 | Loss: 0.00003870
Iteration 188/1000 | Loss: 0.00003869
Iteration 189/1000 | Loss: 0.00003869
Iteration 190/1000 | Loss: 0.00003869
Iteration 191/1000 | Loss: 0.00003869
Iteration 192/1000 | Loss: 0.00003869
Iteration 193/1000 | Loss: 0.00003869
Iteration 194/1000 | Loss: 0.00003868
Iteration 195/1000 | Loss: 0.00003868
Iteration 196/1000 | Loss: 0.00003868
Iteration 197/1000 | Loss: 0.00003868
Iteration 198/1000 | Loss: 0.00003868
Iteration 199/1000 | Loss: 0.00003867
Iteration 200/1000 | Loss: 0.00003867
Iteration 201/1000 | Loss: 0.00003867
Iteration 202/1000 | Loss: 0.00003867
Iteration 203/1000 | Loss: 0.00003867
Iteration 204/1000 | Loss: 0.00003867
Iteration 205/1000 | Loss: 0.00003867
Iteration 206/1000 | Loss: 0.00003866
Iteration 207/1000 | Loss: 0.00003866
Iteration 208/1000 | Loss: 0.00003866
Iteration 209/1000 | Loss: 0.00003865
Iteration 210/1000 | Loss: 0.00003865
Iteration 211/1000 | Loss: 0.00003864
Iteration 212/1000 | Loss: 0.00003864
Iteration 213/1000 | Loss: 0.00003863
Iteration 214/1000 | Loss: 0.00003863
Iteration 215/1000 | Loss: 0.00003863
Iteration 216/1000 | Loss: 0.00003863
Iteration 217/1000 | Loss: 0.00003863
Iteration 218/1000 | Loss: 0.00003863
Iteration 219/1000 | Loss: 0.00003862
Iteration 220/1000 | Loss: 0.00003862
Iteration 221/1000 | Loss: 0.00003862
Iteration 222/1000 | Loss: 0.00003862
Iteration 223/1000 | Loss: 0.00003862
Iteration 224/1000 | Loss: 0.00003862
Iteration 225/1000 | Loss: 0.00003862
Iteration 226/1000 | Loss: 0.00003862
Iteration 227/1000 | Loss: 0.00003862
Iteration 228/1000 | Loss: 0.00003862
Iteration 229/1000 | Loss: 0.00003862
Iteration 230/1000 | Loss: 0.00003862
Iteration 231/1000 | Loss: 0.00003861
Iteration 232/1000 | Loss: 0.00003861
Iteration 233/1000 | Loss: 0.00003861
Iteration 234/1000 | Loss: 0.00003861
Iteration 235/1000 | Loss: 0.00003861
Iteration 236/1000 | Loss: 0.00003861
Iteration 237/1000 | Loss: 0.00003861
Iteration 238/1000 | Loss: 0.00003861
Iteration 239/1000 | Loss: 0.00003861
Iteration 240/1000 | Loss: 0.00003861
Iteration 241/1000 | Loss: 0.00003861
Iteration 242/1000 | Loss: 0.00003861
Iteration 243/1000 | Loss: 0.00003861
Iteration 244/1000 | Loss: 0.00003860
Iteration 245/1000 | Loss: 0.00003860
Iteration 246/1000 | Loss: 0.00003860
Iteration 247/1000 | Loss: 0.00003860
Iteration 248/1000 | Loss: 0.00003860
Iteration 249/1000 | Loss: 0.00003860
Iteration 250/1000 | Loss: 0.00003859
Iteration 251/1000 | Loss: 0.00003859
Iteration 252/1000 | Loss: 0.00003859
Iteration 253/1000 | Loss: 0.00003859
Iteration 254/1000 | Loss: 0.00003859
Iteration 255/1000 | Loss: 0.00003859
Iteration 256/1000 | Loss: 0.00003859
Iteration 257/1000 | Loss: 0.00003859
Iteration 258/1000 | Loss: 0.00003859
Iteration 259/1000 | Loss: 0.00003859
Iteration 260/1000 | Loss: 0.00008607
Iteration 261/1000 | Loss: 0.00003866
Iteration 262/1000 | Loss: 0.00003858
Iteration 263/1000 | Loss: 0.00003857
Iteration 264/1000 | Loss: 0.00007788
Iteration 265/1000 | Loss: 0.00005179
Iteration 266/1000 | Loss: 0.00005422
Iteration 267/1000 | Loss: 0.00003865
Iteration 268/1000 | Loss: 0.00003860
Iteration 269/1000 | Loss: 0.00003860
Iteration 270/1000 | Loss: 0.00003858
Iteration 271/1000 | Loss: 0.00003857
Iteration 272/1000 | Loss: 0.00003857
Iteration 273/1000 | Loss: 0.00003857
Iteration 274/1000 | Loss: 0.00003857
Iteration 275/1000 | Loss: 0.00003857
Iteration 276/1000 | Loss: 0.00003857
Iteration 277/1000 | Loss: 0.00003857
Iteration 278/1000 | Loss: 0.00003857
Iteration 279/1000 | Loss: 0.00003856
Iteration 280/1000 | Loss: 0.00003856
Iteration 281/1000 | Loss: 0.00003856
Iteration 282/1000 | Loss: 0.00003856
Iteration 283/1000 | Loss: 0.00003856
Iteration 284/1000 | Loss: 0.00003856
Iteration 285/1000 | Loss: 0.00003856
Iteration 286/1000 | Loss: 0.00003856
Iteration 287/1000 | Loss: 0.00003856
Iteration 288/1000 | Loss: 0.00003856
Iteration 289/1000 | Loss: 0.00003856
Iteration 290/1000 | Loss: 0.00003856
Iteration 291/1000 | Loss: 0.00003856
Iteration 292/1000 | Loss: 0.00003856
Iteration 293/1000 | Loss: 0.00003856
Iteration 294/1000 | Loss: 0.00003856
Iteration 295/1000 | Loss: 0.00003856
Iteration 296/1000 | Loss: 0.00003856
Iteration 297/1000 | Loss: 0.00003856
Iteration 298/1000 | Loss: 0.00003856
Iteration 299/1000 | Loss: 0.00003856
Iteration 300/1000 | Loss: 0.00003856
Iteration 301/1000 | Loss: 0.00003856
Iteration 302/1000 | Loss: 0.00003856
Iteration 303/1000 | Loss: 0.00003856
Iteration 304/1000 | Loss: 0.00003856
Iteration 305/1000 | Loss: 0.00003856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 305. Stopping optimization.
Last 5 losses: [3.855730756185949e-05, 3.855730756185949e-05, 3.855730756185949e-05, 3.855730756185949e-05, 3.855730756185949e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.855730756185949e-05

Optimization complete. Final v2v error: 4.99796199798584 mm

Highest mean error: 7.445087909698486 mm for frame 99

Lowest mean error: 3.54111647605896 mm for frame 140

Saving results

Total time: 98.0121259689331
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00709251
Iteration 2/25 | Loss: 0.00159088
Iteration 3/25 | Loss: 0.00141891
Iteration 4/25 | Loss: 0.00139199
Iteration 5/25 | Loss: 0.00138282
Iteration 6/25 | Loss: 0.00138132
Iteration 7/25 | Loss: 0.00138132
Iteration 8/25 | Loss: 0.00138132
Iteration 9/25 | Loss: 0.00138132
Iteration 10/25 | Loss: 0.00138132
Iteration 11/25 | Loss: 0.00138132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013813202967867255, 0.0013813202967867255, 0.0013813202967867255, 0.0013813202967867255, 0.0013813202967867255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013813202967867255

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.14897013
Iteration 2/25 | Loss: 0.00106207
Iteration 3/25 | Loss: 0.00106207
Iteration 4/25 | Loss: 0.00106207
Iteration 5/25 | Loss: 0.00106207
Iteration 6/25 | Loss: 0.00106207
Iteration 7/25 | Loss: 0.00106207
Iteration 8/25 | Loss: 0.00106207
Iteration 9/25 | Loss: 0.00106207
Iteration 10/25 | Loss: 0.00106207
Iteration 11/25 | Loss: 0.00106207
Iteration 12/25 | Loss: 0.00106207
Iteration 13/25 | Loss: 0.00106207
Iteration 14/25 | Loss: 0.00106207
Iteration 15/25 | Loss: 0.00106207
Iteration 16/25 | Loss: 0.00106207
Iteration 17/25 | Loss: 0.00106207
Iteration 18/25 | Loss: 0.00106207
Iteration 19/25 | Loss: 0.00106207
Iteration 20/25 | Loss: 0.00106207
Iteration 21/25 | Loss: 0.00106207
Iteration 22/25 | Loss: 0.00106207
Iteration 23/25 | Loss: 0.00106207
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010620689718052745, 0.0010620689718052745, 0.0010620689718052745, 0.0010620689718052745, 0.0010620689718052745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010620689718052745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106207
Iteration 2/1000 | Loss: 0.00004315
Iteration 3/1000 | Loss: 0.00003095
Iteration 4/1000 | Loss: 0.00002896
Iteration 5/1000 | Loss: 0.00002789
Iteration 6/1000 | Loss: 0.00002724
Iteration 7/1000 | Loss: 0.00002674
Iteration 8/1000 | Loss: 0.00002621
Iteration 9/1000 | Loss: 0.00002566
Iteration 10/1000 | Loss: 0.00002536
Iteration 11/1000 | Loss: 0.00002515
Iteration 12/1000 | Loss: 0.00002515
Iteration 13/1000 | Loss: 0.00002495
Iteration 14/1000 | Loss: 0.00002483
Iteration 15/1000 | Loss: 0.00002480
Iteration 16/1000 | Loss: 0.00002466
Iteration 17/1000 | Loss: 0.00002459
Iteration 18/1000 | Loss: 0.00002459
Iteration 19/1000 | Loss: 0.00002448
Iteration 20/1000 | Loss: 0.00002442
Iteration 21/1000 | Loss: 0.00002433
Iteration 22/1000 | Loss: 0.00002429
Iteration 23/1000 | Loss: 0.00002424
Iteration 24/1000 | Loss: 0.00002424
Iteration 25/1000 | Loss: 0.00002420
Iteration 26/1000 | Loss: 0.00002419
Iteration 27/1000 | Loss: 0.00002418
Iteration 28/1000 | Loss: 0.00002417
Iteration 29/1000 | Loss: 0.00002417
Iteration 30/1000 | Loss: 0.00002416
Iteration 31/1000 | Loss: 0.00002415
Iteration 32/1000 | Loss: 0.00002415
Iteration 33/1000 | Loss: 0.00002413
Iteration 34/1000 | Loss: 0.00002412
Iteration 35/1000 | Loss: 0.00002412
Iteration 36/1000 | Loss: 0.00002412
Iteration 37/1000 | Loss: 0.00002412
Iteration 38/1000 | Loss: 0.00002412
Iteration 39/1000 | Loss: 0.00002412
Iteration 40/1000 | Loss: 0.00002411
Iteration 41/1000 | Loss: 0.00002411
Iteration 42/1000 | Loss: 0.00002411
Iteration 43/1000 | Loss: 0.00002411
Iteration 44/1000 | Loss: 0.00002410
Iteration 45/1000 | Loss: 0.00002410
Iteration 46/1000 | Loss: 0.00002408
Iteration 47/1000 | Loss: 0.00002408
Iteration 48/1000 | Loss: 0.00002408
Iteration 49/1000 | Loss: 0.00002408
Iteration 50/1000 | Loss: 0.00002408
Iteration 51/1000 | Loss: 0.00002408
Iteration 52/1000 | Loss: 0.00002408
Iteration 53/1000 | Loss: 0.00002408
Iteration 54/1000 | Loss: 0.00002407
Iteration 55/1000 | Loss: 0.00002407
Iteration 56/1000 | Loss: 0.00002407
Iteration 57/1000 | Loss: 0.00002407
Iteration 58/1000 | Loss: 0.00002407
Iteration 59/1000 | Loss: 0.00002407
Iteration 60/1000 | Loss: 0.00002407
Iteration 61/1000 | Loss: 0.00002407
Iteration 62/1000 | Loss: 0.00002407
Iteration 63/1000 | Loss: 0.00002407
Iteration 64/1000 | Loss: 0.00002407
Iteration 65/1000 | Loss: 0.00002406
Iteration 66/1000 | Loss: 0.00002406
Iteration 67/1000 | Loss: 0.00002406
Iteration 68/1000 | Loss: 0.00002406
Iteration 69/1000 | Loss: 0.00002405
Iteration 70/1000 | Loss: 0.00002405
Iteration 71/1000 | Loss: 0.00002405
Iteration 72/1000 | Loss: 0.00002405
Iteration 73/1000 | Loss: 0.00002405
Iteration 74/1000 | Loss: 0.00002404
Iteration 75/1000 | Loss: 0.00002404
Iteration 76/1000 | Loss: 0.00002404
Iteration 77/1000 | Loss: 0.00002404
Iteration 78/1000 | Loss: 0.00002404
Iteration 79/1000 | Loss: 0.00002403
Iteration 80/1000 | Loss: 0.00002403
Iteration 81/1000 | Loss: 0.00002403
Iteration 82/1000 | Loss: 0.00002403
Iteration 83/1000 | Loss: 0.00002403
Iteration 84/1000 | Loss: 0.00002403
Iteration 85/1000 | Loss: 0.00002403
Iteration 86/1000 | Loss: 0.00002403
Iteration 87/1000 | Loss: 0.00002403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [2.402633981546387e-05, 2.402633981546387e-05, 2.402633981546387e-05, 2.402633981546387e-05, 2.402633981546387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.402633981546387e-05

Optimization complete. Final v2v error: 4.116450309753418 mm

Highest mean error: 4.714958667755127 mm for frame 180

Lowest mean error: 3.47589373588562 mm for frame 134

Saving results

Total time: 41.567861795425415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035285
Iteration 2/25 | Loss: 0.00282611
Iteration 3/25 | Loss: 0.00192273
Iteration 4/25 | Loss: 0.00180661
Iteration 5/25 | Loss: 0.00172777
Iteration 6/25 | Loss: 0.00150013
Iteration 7/25 | Loss: 0.00140509
Iteration 8/25 | Loss: 0.00136027
Iteration 9/25 | Loss: 0.00132450
Iteration 10/25 | Loss: 0.00131310
Iteration 11/25 | Loss: 0.00130949
Iteration 12/25 | Loss: 0.00129450
Iteration 13/25 | Loss: 0.00129121
Iteration 14/25 | Loss: 0.00128299
Iteration 15/25 | Loss: 0.00128548
Iteration 16/25 | Loss: 0.00128215
Iteration 17/25 | Loss: 0.00128771
Iteration 18/25 | Loss: 0.00128309
Iteration 19/25 | Loss: 0.00128161
Iteration 20/25 | Loss: 0.00127948
Iteration 21/25 | Loss: 0.00127859
Iteration 22/25 | Loss: 0.00127856
Iteration 23/25 | Loss: 0.00127855
Iteration 24/25 | Loss: 0.00127855
Iteration 25/25 | Loss: 0.00127855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36447287
Iteration 2/25 | Loss: 0.00100816
Iteration 3/25 | Loss: 0.00096937
Iteration 4/25 | Loss: 0.00096937
Iteration 5/25 | Loss: 0.00096937
Iteration 6/25 | Loss: 0.00096937
Iteration 7/25 | Loss: 0.00096937
Iteration 8/25 | Loss: 0.00096937
Iteration 9/25 | Loss: 0.00096937
Iteration 10/25 | Loss: 0.00096937
Iteration 11/25 | Loss: 0.00096937
Iteration 12/25 | Loss: 0.00096937
Iteration 13/25 | Loss: 0.00096937
Iteration 14/25 | Loss: 0.00096937
Iteration 15/25 | Loss: 0.00096937
Iteration 16/25 | Loss: 0.00096937
Iteration 17/25 | Loss: 0.00096937
Iteration 18/25 | Loss: 0.00096937
Iteration 19/25 | Loss: 0.00096937
Iteration 20/25 | Loss: 0.00096937
Iteration 21/25 | Loss: 0.00096937
Iteration 22/25 | Loss: 0.00096937
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009693704778328538, 0.0009693704778328538, 0.0009693704778328538, 0.0009693704778328538, 0.0009693704778328538]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009693704778328538

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096937
Iteration 2/1000 | Loss: 0.00005611
Iteration 3/1000 | Loss: 0.00004169
Iteration 4/1000 | Loss: 0.00003658
Iteration 5/1000 | Loss: 0.00003395
Iteration 6/1000 | Loss: 0.00003213
Iteration 7/1000 | Loss: 0.00003135
Iteration 8/1000 | Loss: 0.00003056
Iteration 9/1000 | Loss: 0.00002985
Iteration 10/1000 | Loss: 0.00002915
Iteration 11/1000 | Loss: 0.00002849
Iteration 12/1000 | Loss: 0.00002789
Iteration 13/1000 | Loss: 0.00002751
Iteration 14/1000 | Loss: 0.00002715
Iteration 15/1000 | Loss: 0.00002692
Iteration 16/1000 | Loss: 0.00002687
Iteration 17/1000 | Loss: 0.00002672
Iteration 18/1000 | Loss: 0.00002654
Iteration 19/1000 | Loss: 0.00002643
Iteration 20/1000 | Loss: 0.00002642
Iteration 21/1000 | Loss: 0.00002637
Iteration 22/1000 | Loss: 0.00002637
Iteration 23/1000 | Loss: 0.00002635
Iteration 24/1000 | Loss: 0.00005349
Iteration 25/1000 | Loss: 0.00002615
Iteration 26/1000 | Loss: 0.00005499
Iteration 27/1000 | Loss: 0.00005499
Iteration 28/1000 | Loss: 0.00023511
Iteration 29/1000 | Loss: 0.00066665
Iteration 30/1000 | Loss: 0.00288234
Iteration 31/1000 | Loss: 0.00190055
Iteration 32/1000 | Loss: 0.00617344
Iteration 33/1000 | Loss: 0.00259973
Iteration 34/1000 | Loss: 0.00024250
Iteration 35/1000 | Loss: 0.00076902
Iteration 36/1000 | Loss: 0.00068447
Iteration 37/1000 | Loss: 0.00117885
Iteration 38/1000 | Loss: 0.00063854
Iteration 39/1000 | Loss: 0.00087871
Iteration 40/1000 | Loss: 0.00024604
Iteration 41/1000 | Loss: 0.00074466
Iteration 42/1000 | Loss: 0.00030874
Iteration 43/1000 | Loss: 0.00020528
Iteration 44/1000 | Loss: 0.00029257
Iteration 45/1000 | Loss: 0.00038143
Iteration 46/1000 | Loss: 0.00005476
Iteration 47/1000 | Loss: 0.00005162
Iteration 48/1000 | Loss: 0.00004216
Iteration 49/1000 | Loss: 0.00005147
Iteration 50/1000 | Loss: 0.00006625
Iteration 51/1000 | Loss: 0.00003664
Iteration 52/1000 | Loss: 0.00003574
Iteration 53/1000 | Loss: 0.00007303
Iteration 54/1000 | Loss: 0.00003439
Iteration 55/1000 | Loss: 0.00111319
Iteration 56/1000 | Loss: 0.00020342
Iteration 57/1000 | Loss: 0.00025388
Iteration 58/1000 | Loss: 0.00004276
Iteration 59/1000 | Loss: 0.00003360
Iteration 60/1000 | Loss: 0.00003293
Iteration 61/1000 | Loss: 0.00003225
Iteration 62/1000 | Loss: 0.00003171
Iteration 63/1000 | Loss: 0.00003129
Iteration 64/1000 | Loss: 0.00003102
Iteration 65/1000 | Loss: 0.00116866
Iteration 66/1000 | Loss: 0.00019649
Iteration 67/1000 | Loss: 0.00005462
Iteration 68/1000 | Loss: 0.00003303
Iteration 69/1000 | Loss: 0.00003122
Iteration 70/1000 | Loss: 0.00003086
Iteration 71/1000 | Loss: 0.00003064
Iteration 72/1000 | Loss: 0.00003051
Iteration 73/1000 | Loss: 0.00119278
Iteration 74/1000 | Loss: 0.00139299
Iteration 75/1000 | Loss: 0.00010082
Iteration 76/1000 | Loss: 0.00003202
Iteration 77/1000 | Loss: 0.00003087
Iteration 78/1000 | Loss: 0.00003053
Iteration 79/1000 | Loss: 0.00003034
Iteration 80/1000 | Loss: 0.00003023
Iteration 81/1000 | Loss: 0.00003023
Iteration 82/1000 | Loss: 0.00003021
Iteration 83/1000 | Loss: 0.00003021
Iteration 84/1000 | Loss: 0.00003020
Iteration 85/1000 | Loss: 0.00003020
Iteration 86/1000 | Loss: 0.00003020
Iteration 87/1000 | Loss: 0.00003018
Iteration 88/1000 | Loss: 0.00003016
Iteration 89/1000 | Loss: 0.00003016
Iteration 90/1000 | Loss: 0.00003013
Iteration 91/1000 | Loss: 0.00003010
Iteration 92/1000 | Loss: 0.00003009
Iteration 93/1000 | Loss: 0.00003009
Iteration 94/1000 | Loss: 0.00003009
Iteration 95/1000 | Loss: 0.00003009
Iteration 96/1000 | Loss: 0.00003009
Iteration 97/1000 | Loss: 0.00003008
Iteration 98/1000 | Loss: 0.00003008
Iteration 99/1000 | Loss: 0.00003007
Iteration 100/1000 | Loss: 0.00003007
Iteration 101/1000 | Loss: 0.00003007
Iteration 102/1000 | Loss: 0.00003007
Iteration 103/1000 | Loss: 0.00003006
Iteration 104/1000 | Loss: 0.00003006
Iteration 105/1000 | Loss: 0.00003005
Iteration 106/1000 | Loss: 0.00003005
Iteration 107/1000 | Loss: 0.00003005
Iteration 108/1000 | Loss: 0.00003005
Iteration 109/1000 | Loss: 0.00003005
Iteration 110/1000 | Loss: 0.00003005
Iteration 111/1000 | Loss: 0.00003005
Iteration 112/1000 | Loss: 0.00003004
Iteration 113/1000 | Loss: 0.00003004
Iteration 114/1000 | Loss: 0.00003004
Iteration 115/1000 | Loss: 0.00003003
Iteration 116/1000 | Loss: 0.00003003
Iteration 117/1000 | Loss: 0.00003002
Iteration 118/1000 | Loss: 0.00003002
Iteration 119/1000 | Loss: 0.00003002
Iteration 120/1000 | Loss: 0.00003001
Iteration 121/1000 | Loss: 0.00003001
Iteration 122/1000 | Loss: 0.00003001
Iteration 123/1000 | Loss: 0.00003001
Iteration 124/1000 | Loss: 0.00003000
Iteration 125/1000 | Loss: 0.00003000
Iteration 126/1000 | Loss: 0.00003000
Iteration 127/1000 | Loss: 0.00002999
Iteration 128/1000 | Loss: 0.00002999
Iteration 129/1000 | Loss: 0.00002999
Iteration 130/1000 | Loss: 0.00002999
Iteration 131/1000 | Loss: 0.00002999
Iteration 132/1000 | Loss: 0.00002998
Iteration 133/1000 | Loss: 0.00002998
Iteration 134/1000 | Loss: 0.00002997
Iteration 135/1000 | Loss: 0.00002997
Iteration 136/1000 | Loss: 0.00002997
Iteration 137/1000 | Loss: 0.00002997
Iteration 138/1000 | Loss: 0.00002996
Iteration 139/1000 | Loss: 0.00002996
Iteration 140/1000 | Loss: 0.00002996
Iteration 141/1000 | Loss: 0.00002995
Iteration 142/1000 | Loss: 0.00002995
Iteration 143/1000 | Loss: 0.00002995
Iteration 144/1000 | Loss: 0.00002995
Iteration 145/1000 | Loss: 0.00002995
Iteration 146/1000 | Loss: 0.00002995
Iteration 147/1000 | Loss: 0.00002995
Iteration 148/1000 | Loss: 0.00002995
Iteration 149/1000 | Loss: 0.00002995
Iteration 150/1000 | Loss: 0.00002995
Iteration 151/1000 | Loss: 0.00002995
Iteration 152/1000 | Loss: 0.00002995
Iteration 153/1000 | Loss: 0.00002995
Iteration 154/1000 | Loss: 0.00002995
Iteration 155/1000 | Loss: 0.00002995
Iteration 156/1000 | Loss: 0.00002995
Iteration 157/1000 | Loss: 0.00002995
Iteration 158/1000 | Loss: 0.00002995
Iteration 159/1000 | Loss: 0.00002995
Iteration 160/1000 | Loss: 0.00002995
Iteration 161/1000 | Loss: 0.00002995
Iteration 162/1000 | Loss: 0.00002995
Iteration 163/1000 | Loss: 0.00002995
Iteration 164/1000 | Loss: 0.00002995
Iteration 165/1000 | Loss: 0.00002995
Iteration 166/1000 | Loss: 0.00002995
Iteration 167/1000 | Loss: 0.00002995
Iteration 168/1000 | Loss: 0.00002995
Iteration 169/1000 | Loss: 0.00002995
Iteration 170/1000 | Loss: 0.00002995
Iteration 171/1000 | Loss: 0.00002995
Iteration 172/1000 | Loss: 0.00002995
Iteration 173/1000 | Loss: 0.00002995
Iteration 174/1000 | Loss: 0.00002995
Iteration 175/1000 | Loss: 0.00002995
Iteration 176/1000 | Loss: 0.00002995
Iteration 177/1000 | Loss: 0.00002995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.9947021175757982e-05, 2.9947021175757982e-05, 2.9947021175757982e-05, 2.9947021175757982e-05, 2.9947021175757982e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9947021175757982e-05

Optimization complete. Final v2v error: 3.159862995147705 mm

Highest mean error: 12.025118827819824 mm for frame 3

Lowest mean error: 2.7307419776916504 mm for frame 7

Saving results

Total time: 147.5588355064392
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827354
Iteration 2/25 | Loss: 0.00145548
Iteration 3/25 | Loss: 0.00134011
Iteration 4/25 | Loss: 0.00132566
Iteration 5/25 | Loss: 0.00132111
Iteration 6/25 | Loss: 0.00131998
Iteration 7/25 | Loss: 0.00131998
Iteration 8/25 | Loss: 0.00131998
Iteration 9/25 | Loss: 0.00131998
Iteration 10/25 | Loss: 0.00131998
Iteration 11/25 | Loss: 0.00131998
Iteration 12/25 | Loss: 0.00131998
Iteration 13/25 | Loss: 0.00131998
Iteration 14/25 | Loss: 0.00131998
Iteration 15/25 | Loss: 0.00131998
Iteration 16/25 | Loss: 0.00131998
Iteration 17/25 | Loss: 0.00131998
Iteration 18/25 | Loss: 0.00131998
Iteration 19/25 | Loss: 0.00131998
Iteration 20/25 | Loss: 0.00131998
Iteration 21/25 | Loss: 0.00131998
Iteration 22/25 | Loss: 0.00131998
Iteration 23/25 | Loss: 0.00131998
Iteration 24/25 | Loss: 0.00131998
Iteration 25/25 | Loss: 0.00131998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42498791
Iteration 2/25 | Loss: 0.00170726
Iteration 3/25 | Loss: 0.00170726
Iteration 4/25 | Loss: 0.00170726
Iteration 5/25 | Loss: 0.00170725
Iteration 6/25 | Loss: 0.00170725
Iteration 7/25 | Loss: 0.00170725
Iteration 8/25 | Loss: 0.00170725
Iteration 9/25 | Loss: 0.00170725
Iteration 10/25 | Loss: 0.00170725
Iteration 11/25 | Loss: 0.00170725
Iteration 12/25 | Loss: 0.00170725
Iteration 13/25 | Loss: 0.00170725
Iteration 14/25 | Loss: 0.00170725
Iteration 15/25 | Loss: 0.00170725
Iteration 16/25 | Loss: 0.00170725
Iteration 17/25 | Loss: 0.00170725
Iteration 18/25 | Loss: 0.00170725
Iteration 19/25 | Loss: 0.00170725
Iteration 20/25 | Loss: 0.00170725
Iteration 21/25 | Loss: 0.00170725
Iteration 22/25 | Loss: 0.00170725
Iteration 23/25 | Loss: 0.00170725
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0017072525806725025, 0.0017072525806725025, 0.0017072525806725025, 0.0017072525806725025, 0.0017072525806725025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017072525806725025

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170725
Iteration 2/1000 | Loss: 0.00012525
Iteration 3/1000 | Loss: 0.00007423
Iteration 4/1000 | Loss: 0.00006459
Iteration 5/1000 | Loss: 0.00005719
Iteration 6/1000 | Loss: 0.00005442
Iteration 7/1000 | Loss: 0.00005198
Iteration 8/1000 | Loss: 0.00005053
Iteration 9/1000 | Loss: 0.00004876
Iteration 10/1000 | Loss: 0.00004776
Iteration 11/1000 | Loss: 0.00004735
Iteration 12/1000 | Loss: 0.00004704
Iteration 13/1000 | Loss: 0.00004681
Iteration 14/1000 | Loss: 0.00004664
Iteration 15/1000 | Loss: 0.00004648
Iteration 16/1000 | Loss: 0.00004636
Iteration 17/1000 | Loss: 0.00004636
Iteration 18/1000 | Loss: 0.00004628
Iteration 19/1000 | Loss: 0.00004627
Iteration 20/1000 | Loss: 0.00004627
Iteration 21/1000 | Loss: 0.00004626
Iteration 22/1000 | Loss: 0.00004624
Iteration 23/1000 | Loss: 0.00004623
Iteration 24/1000 | Loss: 0.00004622
Iteration 25/1000 | Loss: 0.00004621
Iteration 26/1000 | Loss: 0.00004620
Iteration 27/1000 | Loss: 0.00004616
Iteration 28/1000 | Loss: 0.00004609
Iteration 29/1000 | Loss: 0.00004605
Iteration 30/1000 | Loss: 0.00004600
Iteration 31/1000 | Loss: 0.00004600
Iteration 32/1000 | Loss: 0.00004599
Iteration 33/1000 | Loss: 0.00004594
Iteration 34/1000 | Loss: 0.00004588
Iteration 35/1000 | Loss: 0.00004588
Iteration 36/1000 | Loss: 0.00004586
Iteration 37/1000 | Loss: 0.00004568
Iteration 38/1000 | Loss: 0.00018467
Iteration 39/1000 | Loss: 0.00004927
Iteration 40/1000 | Loss: 0.00015769
Iteration 41/1000 | Loss: 0.00019740
Iteration 42/1000 | Loss: 0.00005234
Iteration 43/1000 | Loss: 0.00014311
Iteration 44/1000 | Loss: 0.00013003
Iteration 45/1000 | Loss: 0.00004856
Iteration 46/1000 | Loss: 0.00004664
Iteration 47/1000 | Loss: 0.00004602
Iteration 48/1000 | Loss: 0.00004553
Iteration 49/1000 | Loss: 0.00004512
Iteration 50/1000 | Loss: 0.00004440
Iteration 51/1000 | Loss: 0.00004366
Iteration 52/1000 | Loss: 0.00004318
Iteration 53/1000 | Loss: 0.00004303
Iteration 54/1000 | Loss: 0.00004292
Iteration 55/1000 | Loss: 0.00004260
Iteration 56/1000 | Loss: 0.00004222
Iteration 57/1000 | Loss: 0.00004191
Iteration 58/1000 | Loss: 0.00004168
Iteration 59/1000 | Loss: 0.00004144
Iteration 60/1000 | Loss: 0.00004125
Iteration 61/1000 | Loss: 0.00004107
Iteration 62/1000 | Loss: 0.00004097
Iteration 63/1000 | Loss: 0.00004078
Iteration 64/1000 | Loss: 0.00004072
Iteration 65/1000 | Loss: 0.00004052
Iteration 66/1000 | Loss: 0.00004051
Iteration 67/1000 | Loss: 0.00004039
Iteration 68/1000 | Loss: 0.00004037
Iteration 69/1000 | Loss: 0.00004033
Iteration 70/1000 | Loss: 0.00004029
Iteration 71/1000 | Loss: 0.00004024
Iteration 72/1000 | Loss: 0.00004024
Iteration 73/1000 | Loss: 0.00004021
Iteration 74/1000 | Loss: 0.00004020
Iteration 75/1000 | Loss: 0.00004020
Iteration 76/1000 | Loss: 0.00004020
Iteration 77/1000 | Loss: 0.00004017
Iteration 78/1000 | Loss: 0.00004013
Iteration 79/1000 | Loss: 0.00004008
Iteration 80/1000 | Loss: 0.00004007
Iteration 81/1000 | Loss: 0.00004006
Iteration 82/1000 | Loss: 0.00004006
Iteration 83/1000 | Loss: 0.00004002
Iteration 84/1000 | Loss: 0.00004001
Iteration 85/1000 | Loss: 0.00004000
Iteration 86/1000 | Loss: 0.00003999
Iteration 87/1000 | Loss: 0.00003999
Iteration 88/1000 | Loss: 0.00003999
Iteration 89/1000 | Loss: 0.00003998
Iteration 90/1000 | Loss: 0.00003998
Iteration 91/1000 | Loss: 0.00003998
Iteration 92/1000 | Loss: 0.00003997
Iteration 93/1000 | Loss: 0.00003997
Iteration 94/1000 | Loss: 0.00003996
Iteration 95/1000 | Loss: 0.00003996
Iteration 96/1000 | Loss: 0.00003995
Iteration 97/1000 | Loss: 0.00003995
Iteration 98/1000 | Loss: 0.00003995
Iteration 99/1000 | Loss: 0.00003994
Iteration 100/1000 | Loss: 0.00003992
Iteration 101/1000 | Loss: 0.00003992
Iteration 102/1000 | Loss: 0.00003992
Iteration 103/1000 | Loss: 0.00003992
Iteration 104/1000 | Loss: 0.00003992
Iteration 105/1000 | Loss: 0.00003992
Iteration 106/1000 | Loss: 0.00003992
Iteration 107/1000 | Loss: 0.00003991
Iteration 108/1000 | Loss: 0.00003991
Iteration 109/1000 | Loss: 0.00003991
Iteration 110/1000 | Loss: 0.00003991
Iteration 111/1000 | Loss: 0.00003990
Iteration 112/1000 | Loss: 0.00003990
Iteration 113/1000 | Loss: 0.00003990
Iteration 114/1000 | Loss: 0.00003989
Iteration 115/1000 | Loss: 0.00003989
Iteration 116/1000 | Loss: 0.00003989
Iteration 117/1000 | Loss: 0.00003989
Iteration 118/1000 | Loss: 0.00003989
Iteration 119/1000 | Loss: 0.00003988
Iteration 120/1000 | Loss: 0.00003988
Iteration 121/1000 | Loss: 0.00003988
Iteration 122/1000 | Loss: 0.00003988
Iteration 123/1000 | Loss: 0.00003988
Iteration 124/1000 | Loss: 0.00003987
Iteration 125/1000 | Loss: 0.00003987
Iteration 126/1000 | Loss: 0.00003987
Iteration 127/1000 | Loss: 0.00003987
Iteration 128/1000 | Loss: 0.00003987
Iteration 129/1000 | Loss: 0.00003987
Iteration 130/1000 | Loss: 0.00003987
Iteration 131/1000 | Loss: 0.00003986
Iteration 132/1000 | Loss: 0.00003986
Iteration 133/1000 | Loss: 0.00003986
Iteration 134/1000 | Loss: 0.00003986
Iteration 135/1000 | Loss: 0.00003986
Iteration 136/1000 | Loss: 0.00003986
Iteration 137/1000 | Loss: 0.00003986
Iteration 138/1000 | Loss: 0.00003986
Iteration 139/1000 | Loss: 0.00003985
Iteration 140/1000 | Loss: 0.00003985
Iteration 141/1000 | Loss: 0.00003985
Iteration 142/1000 | Loss: 0.00003985
Iteration 143/1000 | Loss: 0.00003985
Iteration 144/1000 | Loss: 0.00003984
Iteration 145/1000 | Loss: 0.00003984
Iteration 146/1000 | Loss: 0.00003984
Iteration 147/1000 | Loss: 0.00003984
Iteration 148/1000 | Loss: 0.00003984
Iteration 149/1000 | Loss: 0.00003984
Iteration 150/1000 | Loss: 0.00003983
Iteration 151/1000 | Loss: 0.00003983
Iteration 152/1000 | Loss: 0.00003983
Iteration 153/1000 | Loss: 0.00003983
Iteration 154/1000 | Loss: 0.00003983
Iteration 155/1000 | Loss: 0.00003983
Iteration 156/1000 | Loss: 0.00003983
Iteration 157/1000 | Loss: 0.00003983
Iteration 158/1000 | Loss: 0.00003983
Iteration 159/1000 | Loss: 0.00003982
Iteration 160/1000 | Loss: 0.00003982
Iteration 161/1000 | Loss: 0.00003982
Iteration 162/1000 | Loss: 0.00003982
Iteration 163/1000 | Loss: 0.00003982
Iteration 164/1000 | Loss: 0.00003982
Iteration 165/1000 | Loss: 0.00003982
Iteration 166/1000 | Loss: 0.00003982
Iteration 167/1000 | Loss: 0.00003982
Iteration 168/1000 | Loss: 0.00003982
Iteration 169/1000 | Loss: 0.00003982
Iteration 170/1000 | Loss: 0.00003982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [3.981657209806144e-05, 3.981657209806144e-05, 3.981657209806144e-05, 3.981657209806144e-05, 3.981657209806144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.981657209806144e-05

Optimization complete. Final v2v error: 3.493837594985962 mm

Highest mean error: 11.462260246276855 mm for frame 120

Lowest mean error: 2.618520975112915 mm for frame 79

Saving results

Total time: 105.02097916603088
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00612412
Iteration 2/25 | Loss: 0.00179327
Iteration 3/25 | Loss: 0.00147553
Iteration 4/25 | Loss: 0.00146062
Iteration 5/25 | Loss: 0.00145570
Iteration 6/25 | Loss: 0.00145429
Iteration 7/25 | Loss: 0.00145429
Iteration 8/25 | Loss: 0.00145429
Iteration 9/25 | Loss: 0.00145429
Iteration 10/25 | Loss: 0.00145429
Iteration 11/25 | Loss: 0.00145429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014542919816449285, 0.0014542919816449285, 0.0014542919816449285, 0.0014542919816449285, 0.0014542919816449285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014542919816449285

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90719652
Iteration 2/25 | Loss: 0.00108853
Iteration 3/25 | Loss: 0.00108851
Iteration 4/25 | Loss: 0.00108851
Iteration 5/25 | Loss: 0.00108851
Iteration 6/25 | Loss: 0.00108851
Iteration 7/25 | Loss: 0.00108851
Iteration 8/25 | Loss: 0.00108851
Iteration 9/25 | Loss: 0.00108851
Iteration 10/25 | Loss: 0.00108851
Iteration 11/25 | Loss: 0.00108851
Iteration 12/25 | Loss: 0.00108851
Iteration 13/25 | Loss: 0.00108851
Iteration 14/25 | Loss: 0.00108851
Iteration 15/25 | Loss: 0.00108851
Iteration 16/25 | Loss: 0.00108851
Iteration 17/25 | Loss: 0.00108851
Iteration 18/25 | Loss: 0.00108851
Iteration 19/25 | Loss: 0.00108851
Iteration 20/25 | Loss: 0.00108851
Iteration 21/25 | Loss: 0.00108851
Iteration 22/25 | Loss: 0.00108851
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010885103838518262, 0.0010885103838518262, 0.0010885103838518262, 0.0010885103838518262, 0.0010885103838518262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010885103838518262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108851
Iteration 2/1000 | Loss: 0.00007296
Iteration 3/1000 | Loss: 0.00004159
Iteration 4/1000 | Loss: 0.00003668
Iteration 5/1000 | Loss: 0.00003479
Iteration 6/1000 | Loss: 0.00003380
Iteration 7/1000 | Loss: 0.00003304
Iteration 8/1000 | Loss: 0.00003238
Iteration 9/1000 | Loss: 0.00003190
Iteration 10/1000 | Loss: 0.00003150
Iteration 11/1000 | Loss: 0.00003112
Iteration 12/1000 | Loss: 0.00003080
Iteration 13/1000 | Loss: 0.00003044
Iteration 14/1000 | Loss: 0.00003013
Iteration 15/1000 | Loss: 0.00002993
Iteration 16/1000 | Loss: 0.00002969
Iteration 17/1000 | Loss: 0.00002963
Iteration 18/1000 | Loss: 0.00002947
Iteration 19/1000 | Loss: 0.00002941
Iteration 20/1000 | Loss: 0.00002930
Iteration 21/1000 | Loss: 0.00002930
Iteration 22/1000 | Loss: 0.00002926
Iteration 23/1000 | Loss: 0.00002926
Iteration 24/1000 | Loss: 0.00002924
Iteration 25/1000 | Loss: 0.00002923
Iteration 26/1000 | Loss: 0.00002922
Iteration 27/1000 | Loss: 0.00002919
Iteration 28/1000 | Loss: 0.00002918
Iteration 29/1000 | Loss: 0.00002918
Iteration 30/1000 | Loss: 0.00002917
Iteration 31/1000 | Loss: 0.00002916
Iteration 32/1000 | Loss: 0.00002916
Iteration 33/1000 | Loss: 0.00002916
Iteration 34/1000 | Loss: 0.00002916
Iteration 35/1000 | Loss: 0.00002916
Iteration 36/1000 | Loss: 0.00002916
Iteration 37/1000 | Loss: 0.00002915
Iteration 38/1000 | Loss: 0.00002915
Iteration 39/1000 | Loss: 0.00002915
Iteration 40/1000 | Loss: 0.00002914
Iteration 41/1000 | Loss: 0.00002914
Iteration 42/1000 | Loss: 0.00002914
Iteration 43/1000 | Loss: 0.00002914
Iteration 44/1000 | Loss: 0.00002914
Iteration 45/1000 | Loss: 0.00002914
Iteration 46/1000 | Loss: 0.00002913
Iteration 47/1000 | Loss: 0.00002913
Iteration 48/1000 | Loss: 0.00002913
Iteration 49/1000 | Loss: 0.00002913
Iteration 50/1000 | Loss: 0.00002913
Iteration 51/1000 | Loss: 0.00002913
Iteration 52/1000 | Loss: 0.00002913
Iteration 53/1000 | Loss: 0.00002913
Iteration 54/1000 | Loss: 0.00002912
Iteration 55/1000 | Loss: 0.00002912
Iteration 56/1000 | Loss: 0.00002912
Iteration 57/1000 | Loss: 0.00002911
Iteration 58/1000 | Loss: 0.00002911
Iteration 59/1000 | Loss: 0.00002911
Iteration 60/1000 | Loss: 0.00002911
Iteration 61/1000 | Loss: 0.00002910
Iteration 62/1000 | Loss: 0.00002910
Iteration 63/1000 | Loss: 0.00002909
Iteration 64/1000 | Loss: 0.00002909
Iteration 65/1000 | Loss: 0.00002908
Iteration 66/1000 | Loss: 0.00002908
Iteration 67/1000 | Loss: 0.00002908
Iteration 68/1000 | Loss: 0.00002907
Iteration 69/1000 | Loss: 0.00002907
Iteration 70/1000 | Loss: 0.00002907
Iteration 71/1000 | Loss: 0.00002907
Iteration 72/1000 | Loss: 0.00002907
Iteration 73/1000 | Loss: 0.00002907
Iteration 74/1000 | Loss: 0.00002907
Iteration 75/1000 | Loss: 0.00002907
Iteration 76/1000 | Loss: 0.00002907
Iteration 77/1000 | Loss: 0.00002907
Iteration 78/1000 | Loss: 0.00002907
Iteration 79/1000 | Loss: 0.00002907
Iteration 80/1000 | Loss: 0.00002907
Iteration 81/1000 | Loss: 0.00002907
Iteration 82/1000 | Loss: 0.00002907
Iteration 83/1000 | Loss: 0.00002907
Iteration 84/1000 | Loss: 0.00002906
Iteration 85/1000 | Loss: 0.00002905
Iteration 86/1000 | Loss: 0.00002905
Iteration 87/1000 | Loss: 0.00002905
Iteration 88/1000 | Loss: 0.00002905
Iteration 89/1000 | Loss: 0.00002905
Iteration 90/1000 | Loss: 0.00002905
Iteration 91/1000 | Loss: 0.00002904
Iteration 92/1000 | Loss: 0.00002904
Iteration 93/1000 | Loss: 0.00002904
Iteration 94/1000 | Loss: 0.00002903
Iteration 95/1000 | Loss: 0.00002903
Iteration 96/1000 | Loss: 0.00002903
Iteration 97/1000 | Loss: 0.00002903
Iteration 98/1000 | Loss: 0.00002903
Iteration 99/1000 | Loss: 0.00002903
Iteration 100/1000 | Loss: 0.00002903
Iteration 101/1000 | Loss: 0.00002903
Iteration 102/1000 | Loss: 0.00002903
Iteration 103/1000 | Loss: 0.00002902
Iteration 104/1000 | Loss: 0.00002902
Iteration 105/1000 | Loss: 0.00002902
Iteration 106/1000 | Loss: 0.00002902
Iteration 107/1000 | Loss: 0.00002901
Iteration 108/1000 | Loss: 0.00002901
Iteration 109/1000 | Loss: 0.00002901
Iteration 110/1000 | Loss: 0.00002901
Iteration 111/1000 | Loss: 0.00002901
Iteration 112/1000 | Loss: 0.00002901
Iteration 113/1000 | Loss: 0.00002901
Iteration 114/1000 | Loss: 0.00002900
Iteration 115/1000 | Loss: 0.00002900
Iteration 116/1000 | Loss: 0.00002900
Iteration 117/1000 | Loss: 0.00002900
Iteration 118/1000 | Loss: 0.00002900
Iteration 119/1000 | Loss: 0.00002900
Iteration 120/1000 | Loss: 0.00002899
Iteration 121/1000 | Loss: 0.00002899
Iteration 122/1000 | Loss: 0.00002899
Iteration 123/1000 | Loss: 0.00002899
Iteration 124/1000 | Loss: 0.00002899
Iteration 125/1000 | Loss: 0.00002899
Iteration 126/1000 | Loss: 0.00002899
Iteration 127/1000 | Loss: 0.00002899
Iteration 128/1000 | Loss: 0.00002899
Iteration 129/1000 | Loss: 0.00002899
Iteration 130/1000 | Loss: 0.00002899
Iteration 131/1000 | Loss: 0.00002899
Iteration 132/1000 | Loss: 0.00002899
Iteration 133/1000 | Loss: 0.00002898
Iteration 134/1000 | Loss: 0.00002898
Iteration 135/1000 | Loss: 0.00002898
Iteration 136/1000 | Loss: 0.00002898
Iteration 137/1000 | Loss: 0.00002898
Iteration 138/1000 | Loss: 0.00002898
Iteration 139/1000 | Loss: 0.00002898
Iteration 140/1000 | Loss: 0.00002898
Iteration 141/1000 | Loss: 0.00002898
Iteration 142/1000 | Loss: 0.00002898
Iteration 143/1000 | Loss: 0.00002898
Iteration 144/1000 | Loss: 0.00002898
Iteration 145/1000 | Loss: 0.00002898
Iteration 146/1000 | Loss: 0.00002898
Iteration 147/1000 | Loss: 0.00002897
Iteration 148/1000 | Loss: 0.00002897
Iteration 149/1000 | Loss: 0.00002897
Iteration 150/1000 | Loss: 0.00002897
Iteration 151/1000 | Loss: 0.00002897
Iteration 152/1000 | Loss: 0.00002897
Iteration 153/1000 | Loss: 0.00002897
Iteration 154/1000 | Loss: 0.00002897
Iteration 155/1000 | Loss: 0.00002897
Iteration 156/1000 | Loss: 0.00002897
Iteration 157/1000 | Loss: 0.00002897
Iteration 158/1000 | Loss: 0.00002897
Iteration 159/1000 | Loss: 0.00002897
Iteration 160/1000 | Loss: 0.00002897
Iteration 161/1000 | Loss: 0.00002897
Iteration 162/1000 | Loss: 0.00002897
Iteration 163/1000 | Loss: 0.00002897
Iteration 164/1000 | Loss: 0.00002896
Iteration 165/1000 | Loss: 0.00002896
Iteration 166/1000 | Loss: 0.00002896
Iteration 167/1000 | Loss: 0.00002896
Iteration 168/1000 | Loss: 0.00002896
Iteration 169/1000 | Loss: 0.00002896
Iteration 170/1000 | Loss: 0.00002896
Iteration 171/1000 | Loss: 0.00002896
Iteration 172/1000 | Loss: 0.00002896
Iteration 173/1000 | Loss: 0.00002896
Iteration 174/1000 | Loss: 0.00002896
Iteration 175/1000 | Loss: 0.00002896
Iteration 176/1000 | Loss: 0.00002896
Iteration 177/1000 | Loss: 0.00002896
Iteration 178/1000 | Loss: 0.00002896
Iteration 179/1000 | Loss: 0.00002896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.8958469556528144e-05, 2.8958469556528144e-05, 2.8958469556528144e-05, 2.8958469556528144e-05, 2.8958469556528144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8958469556528144e-05

Optimization complete. Final v2v error: 4.228574752807617 mm

Highest mean error: 5.0189008712768555 mm for frame 97

Lowest mean error: 3.450601577758789 mm for frame 42

Saving results

Total time: 47.81904363632202
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01083689
Iteration 2/25 | Loss: 0.00224758
Iteration 3/25 | Loss: 0.00172600
Iteration 4/25 | Loss: 0.00161074
Iteration 5/25 | Loss: 0.00157163
Iteration 6/25 | Loss: 0.00154797
Iteration 7/25 | Loss: 0.00153694
Iteration 8/25 | Loss: 0.00153040
Iteration 9/25 | Loss: 0.00152171
Iteration 10/25 | Loss: 0.00152001
Iteration 11/25 | Loss: 0.00151868
Iteration 12/25 | Loss: 0.00151839
Iteration 13/25 | Loss: 0.00151815
Iteration 14/25 | Loss: 0.00151929
Iteration 15/25 | Loss: 0.00151689
Iteration 16/25 | Loss: 0.00151662
Iteration 17/25 | Loss: 0.00151657
Iteration 18/25 | Loss: 0.00151657
Iteration 19/25 | Loss: 0.00151656
Iteration 20/25 | Loss: 0.00151656
Iteration 21/25 | Loss: 0.00151656
Iteration 22/25 | Loss: 0.00151656
Iteration 23/25 | Loss: 0.00151656
Iteration 24/25 | Loss: 0.00151656
Iteration 25/25 | Loss: 0.00151656

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.61750364
Iteration 2/25 | Loss: 0.00184538
Iteration 3/25 | Loss: 0.00184538
Iteration 4/25 | Loss: 0.00184538
Iteration 5/25 | Loss: 0.00184538
Iteration 6/25 | Loss: 0.00184538
Iteration 7/25 | Loss: 0.00184538
Iteration 8/25 | Loss: 0.00184538
Iteration 9/25 | Loss: 0.00184538
Iteration 10/25 | Loss: 0.00184538
Iteration 11/25 | Loss: 0.00184538
Iteration 12/25 | Loss: 0.00184538
Iteration 13/25 | Loss: 0.00184538
Iteration 14/25 | Loss: 0.00184538
Iteration 15/25 | Loss: 0.00184538
Iteration 16/25 | Loss: 0.00184538
Iteration 17/25 | Loss: 0.00184538
Iteration 18/25 | Loss: 0.00184538
Iteration 19/25 | Loss: 0.00184538
Iteration 20/25 | Loss: 0.00184538
Iteration 21/25 | Loss: 0.00184538
Iteration 22/25 | Loss: 0.00184538
Iteration 23/25 | Loss: 0.00184538
Iteration 24/25 | Loss: 0.00184538
Iteration 25/25 | Loss: 0.00184538

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184538
Iteration 2/1000 | Loss: 0.00023573
Iteration 3/1000 | Loss: 0.00033120
Iteration 4/1000 | Loss: 0.00029581
Iteration 5/1000 | Loss: 0.00011935
Iteration 6/1000 | Loss: 0.00008792
Iteration 7/1000 | Loss: 0.00007435
Iteration 8/1000 | Loss: 0.00007130
Iteration 9/1000 | Loss: 0.00012807
Iteration 10/1000 | Loss: 0.00016646
Iteration 11/1000 | Loss: 0.00018477
Iteration 12/1000 | Loss: 0.00008137
Iteration 13/1000 | Loss: 0.00010369
Iteration 14/1000 | Loss: 0.00008384
Iteration 15/1000 | Loss: 0.00006792
Iteration 16/1000 | Loss: 0.00006038
Iteration 17/1000 | Loss: 0.00005346
Iteration 18/1000 | Loss: 0.00005136
Iteration 19/1000 | Loss: 0.00022156
Iteration 20/1000 | Loss: 0.00006795
Iteration 21/1000 | Loss: 0.00005213
Iteration 22/1000 | Loss: 0.00005523
Iteration 23/1000 | Loss: 0.00005460
Iteration 24/1000 | Loss: 0.00006214
Iteration 25/1000 | Loss: 0.00007607
Iteration 26/1000 | Loss: 0.00006130
Iteration 27/1000 | Loss: 0.00005349
Iteration 28/1000 | Loss: 0.00004859
Iteration 29/1000 | Loss: 0.00005453
Iteration 30/1000 | Loss: 0.00006815
Iteration 31/1000 | Loss: 0.00006490
Iteration 32/1000 | Loss: 0.00005008
Iteration 33/1000 | Loss: 0.00005394
Iteration 34/1000 | Loss: 0.00005195
Iteration 35/1000 | Loss: 0.00005354
Iteration 36/1000 | Loss: 0.00005640
Iteration 37/1000 | Loss: 0.00005988
Iteration 38/1000 | Loss: 0.00006170
Iteration 39/1000 | Loss: 0.00006333
Iteration 40/1000 | Loss: 0.00006235
Iteration 41/1000 | Loss: 0.00005210
Iteration 42/1000 | Loss: 0.00005745
Iteration 43/1000 | Loss: 0.00010720
Iteration 44/1000 | Loss: 0.00013648
Iteration 45/1000 | Loss: 0.00008471
Iteration 46/1000 | Loss: 0.00007461
Iteration 47/1000 | Loss: 0.00005480
Iteration 48/1000 | Loss: 0.00004462
Iteration 49/1000 | Loss: 0.00004527
Iteration 50/1000 | Loss: 0.00004875
Iteration 51/1000 | Loss: 0.00005489
Iteration 52/1000 | Loss: 0.00004729
Iteration 53/1000 | Loss: 0.00007040
Iteration 54/1000 | Loss: 0.00005063
Iteration 55/1000 | Loss: 0.00004895
Iteration 56/1000 | Loss: 0.00004412
Iteration 57/1000 | Loss: 0.00005060
Iteration 58/1000 | Loss: 0.00005258
Iteration 59/1000 | Loss: 0.00006364
Iteration 60/1000 | Loss: 0.00004294
Iteration 61/1000 | Loss: 0.00004905
Iteration 62/1000 | Loss: 0.00005247
Iteration 63/1000 | Loss: 0.00004485
Iteration 64/1000 | Loss: 0.00004431
Iteration 65/1000 | Loss: 0.00004641
Iteration 66/1000 | Loss: 0.00004645
Iteration 67/1000 | Loss: 0.00004544
Iteration 68/1000 | Loss: 0.00005241
Iteration 69/1000 | Loss: 0.00010989
Iteration 70/1000 | Loss: 0.00006749
Iteration 71/1000 | Loss: 0.00005965
Iteration 72/1000 | Loss: 0.00005268
Iteration 73/1000 | Loss: 0.00005461
Iteration 74/1000 | Loss: 0.00005108
Iteration 75/1000 | Loss: 0.00005467
Iteration 76/1000 | Loss: 0.00007838
Iteration 77/1000 | Loss: 0.00004742
Iteration 78/1000 | Loss: 0.00004798
Iteration 79/1000 | Loss: 0.00008052
Iteration 80/1000 | Loss: 0.00004809
Iteration 81/1000 | Loss: 0.00005801
Iteration 82/1000 | Loss: 0.00004875
Iteration 83/1000 | Loss: 0.00004775
Iteration 84/1000 | Loss: 0.00004961
Iteration 85/1000 | Loss: 0.00005352
Iteration 86/1000 | Loss: 0.00004382
Iteration 87/1000 | Loss: 0.00007674
Iteration 88/1000 | Loss: 0.00004498
Iteration 89/1000 | Loss: 0.00004980
Iteration 90/1000 | Loss: 0.00005120
Iteration 91/1000 | Loss: 0.00004875
Iteration 92/1000 | Loss: 0.00005241
Iteration 93/1000 | Loss: 0.00004909
Iteration 94/1000 | Loss: 0.00005338
Iteration 95/1000 | Loss: 0.00004955
Iteration 96/1000 | Loss: 0.00005412
Iteration 97/1000 | Loss: 0.00005009
Iteration 98/1000 | Loss: 0.00008014
Iteration 99/1000 | Loss: 0.00005713
Iteration 100/1000 | Loss: 0.00006938
Iteration 101/1000 | Loss: 0.00005375
Iteration 102/1000 | Loss: 0.00005428
Iteration 103/1000 | Loss: 0.00005406
Iteration 104/1000 | Loss: 0.00005434
Iteration 105/1000 | Loss: 0.00004415
Iteration 106/1000 | Loss: 0.00004298
Iteration 107/1000 | Loss: 0.00004222
Iteration 108/1000 | Loss: 0.00004179
Iteration 109/1000 | Loss: 0.00004151
Iteration 110/1000 | Loss: 0.00004130
Iteration 111/1000 | Loss: 0.00004107
Iteration 112/1000 | Loss: 0.00004102
Iteration 113/1000 | Loss: 0.00004079
Iteration 114/1000 | Loss: 0.00004076
Iteration 115/1000 | Loss: 0.00004064
Iteration 116/1000 | Loss: 0.00004063
Iteration 117/1000 | Loss: 0.00004063
Iteration 118/1000 | Loss: 0.00004063
Iteration 119/1000 | Loss: 0.00004063
Iteration 120/1000 | Loss: 0.00004063
Iteration 121/1000 | Loss: 0.00004062
Iteration 122/1000 | Loss: 0.00004062
Iteration 123/1000 | Loss: 0.00004062
Iteration 124/1000 | Loss: 0.00004062
Iteration 125/1000 | Loss: 0.00004062
Iteration 126/1000 | Loss: 0.00004062
Iteration 127/1000 | Loss: 0.00004062
Iteration 128/1000 | Loss: 0.00004062
Iteration 129/1000 | Loss: 0.00004062
Iteration 130/1000 | Loss: 0.00004062
Iteration 131/1000 | Loss: 0.00004061
Iteration 132/1000 | Loss: 0.00004059
Iteration 133/1000 | Loss: 0.00004059
Iteration 134/1000 | Loss: 0.00004058
Iteration 135/1000 | Loss: 0.00004058
Iteration 136/1000 | Loss: 0.00004057
Iteration 137/1000 | Loss: 0.00004057
Iteration 138/1000 | Loss: 0.00004050
Iteration 139/1000 | Loss: 0.00004049
Iteration 140/1000 | Loss: 0.00004048
Iteration 141/1000 | Loss: 0.00004048
Iteration 142/1000 | Loss: 0.00004048
Iteration 143/1000 | Loss: 0.00004047
Iteration 144/1000 | Loss: 0.00004047
Iteration 145/1000 | Loss: 0.00004046
Iteration 146/1000 | Loss: 0.00004046
Iteration 147/1000 | Loss: 0.00004041
Iteration 148/1000 | Loss: 0.00004033
Iteration 149/1000 | Loss: 0.00004031
Iteration 150/1000 | Loss: 0.00004031
Iteration 151/1000 | Loss: 0.00004028
Iteration 152/1000 | Loss: 0.00004028
Iteration 153/1000 | Loss: 0.00004021
Iteration 154/1000 | Loss: 0.00004021
Iteration 155/1000 | Loss: 0.00004011
Iteration 156/1000 | Loss: 0.00004011
Iteration 157/1000 | Loss: 0.00004011
Iteration 158/1000 | Loss: 0.00004011
Iteration 159/1000 | Loss: 0.00004010
Iteration 160/1000 | Loss: 0.00004009
Iteration 161/1000 | Loss: 0.00004009
Iteration 162/1000 | Loss: 0.00004009
Iteration 163/1000 | Loss: 0.00004009
Iteration 164/1000 | Loss: 0.00004009
Iteration 165/1000 | Loss: 0.00004009
Iteration 166/1000 | Loss: 0.00004009
Iteration 167/1000 | Loss: 0.00004009
Iteration 168/1000 | Loss: 0.00004009
Iteration 169/1000 | Loss: 0.00004008
Iteration 170/1000 | Loss: 0.00004008
Iteration 171/1000 | Loss: 0.00004008
Iteration 172/1000 | Loss: 0.00004008
Iteration 173/1000 | Loss: 0.00004007
Iteration 174/1000 | Loss: 0.00004007
Iteration 175/1000 | Loss: 0.00004007
Iteration 176/1000 | Loss: 0.00004007
Iteration 177/1000 | Loss: 0.00004007
Iteration 178/1000 | Loss: 0.00004007
Iteration 179/1000 | Loss: 0.00004007
Iteration 180/1000 | Loss: 0.00004007
Iteration 181/1000 | Loss: 0.00004007
Iteration 182/1000 | Loss: 0.00004007
Iteration 183/1000 | Loss: 0.00004006
Iteration 184/1000 | Loss: 0.00004006
Iteration 185/1000 | Loss: 0.00004006
Iteration 186/1000 | Loss: 0.00004006
Iteration 187/1000 | Loss: 0.00004006
Iteration 188/1000 | Loss: 0.00004006
Iteration 189/1000 | Loss: 0.00004006
Iteration 190/1000 | Loss: 0.00004005
Iteration 191/1000 | Loss: 0.00004005
Iteration 192/1000 | Loss: 0.00004005
Iteration 193/1000 | Loss: 0.00004005
Iteration 194/1000 | Loss: 0.00004005
Iteration 195/1000 | Loss: 0.00004005
Iteration 196/1000 | Loss: 0.00004005
Iteration 197/1000 | Loss: 0.00004004
Iteration 198/1000 | Loss: 0.00004004
Iteration 199/1000 | Loss: 0.00004004
Iteration 200/1000 | Loss: 0.00004004
Iteration 201/1000 | Loss: 0.00004004
Iteration 202/1000 | Loss: 0.00004004
Iteration 203/1000 | Loss: 0.00004004
Iteration 204/1000 | Loss: 0.00004004
Iteration 205/1000 | Loss: 0.00004003
Iteration 206/1000 | Loss: 0.00004003
Iteration 207/1000 | Loss: 0.00004003
Iteration 208/1000 | Loss: 0.00004003
Iteration 209/1000 | Loss: 0.00004002
Iteration 210/1000 | Loss: 0.00004002
Iteration 211/1000 | Loss: 0.00004002
Iteration 212/1000 | Loss: 0.00004002
Iteration 213/1000 | Loss: 0.00004002
Iteration 214/1000 | Loss: 0.00004002
Iteration 215/1000 | Loss: 0.00004002
Iteration 216/1000 | Loss: 0.00004002
Iteration 217/1000 | Loss: 0.00004002
Iteration 218/1000 | Loss: 0.00004001
Iteration 219/1000 | Loss: 0.00004001
Iteration 220/1000 | Loss: 0.00004001
Iteration 221/1000 | Loss: 0.00004001
Iteration 222/1000 | Loss: 0.00004001
Iteration 223/1000 | Loss: 0.00004001
Iteration 224/1000 | Loss: 0.00004001
Iteration 225/1000 | Loss: 0.00004001
Iteration 226/1000 | Loss: 0.00004001
Iteration 227/1000 | Loss: 0.00004001
Iteration 228/1000 | Loss: 0.00004001
Iteration 229/1000 | Loss: 0.00004001
Iteration 230/1000 | Loss: 0.00004001
Iteration 231/1000 | Loss: 0.00004001
Iteration 232/1000 | Loss: 0.00004001
Iteration 233/1000 | Loss: 0.00004001
Iteration 234/1000 | Loss: 0.00004001
Iteration 235/1000 | Loss: 0.00004001
Iteration 236/1000 | Loss: 0.00004001
Iteration 237/1000 | Loss: 0.00004000
Iteration 238/1000 | Loss: 0.00004000
Iteration 239/1000 | Loss: 0.00004000
Iteration 240/1000 | Loss: 0.00004000
Iteration 241/1000 | Loss: 0.00004000
Iteration 242/1000 | Loss: 0.00004000
Iteration 243/1000 | Loss: 0.00004000
Iteration 244/1000 | Loss: 0.00004000
Iteration 245/1000 | Loss: 0.00004000
Iteration 246/1000 | Loss: 0.00004000
Iteration 247/1000 | Loss: 0.00004000
Iteration 248/1000 | Loss: 0.00004000
Iteration 249/1000 | Loss: 0.00004000
Iteration 250/1000 | Loss: 0.00004000
Iteration 251/1000 | Loss: 0.00004000
Iteration 252/1000 | Loss: 0.00004000
Iteration 253/1000 | Loss: 0.00004000
Iteration 254/1000 | Loss: 0.00004000
Iteration 255/1000 | Loss: 0.00004000
Iteration 256/1000 | Loss: 0.00004000
Iteration 257/1000 | Loss: 0.00004000
Iteration 258/1000 | Loss: 0.00004000
Iteration 259/1000 | Loss: 0.00004000
Iteration 260/1000 | Loss: 0.00004000
Iteration 261/1000 | Loss: 0.00004000
Iteration 262/1000 | Loss: 0.00004000
Iteration 263/1000 | Loss: 0.00004000
Iteration 264/1000 | Loss: 0.00004000
Iteration 265/1000 | Loss: 0.00004000
Iteration 266/1000 | Loss: 0.00004000
Iteration 267/1000 | Loss: 0.00004000
Iteration 268/1000 | Loss: 0.00004000
Iteration 269/1000 | Loss: 0.00004000
Iteration 270/1000 | Loss: 0.00004000
Iteration 271/1000 | Loss: 0.00004000
Iteration 272/1000 | Loss: 0.00004000
Iteration 273/1000 | Loss: 0.00004000
Iteration 274/1000 | Loss: 0.00004000
Iteration 275/1000 | Loss: 0.00004000
Iteration 276/1000 | Loss: 0.00004000
Iteration 277/1000 | Loss: 0.00004000
Iteration 278/1000 | Loss: 0.00004000
Iteration 279/1000 | Loss: 0.00004000
Iteration 280/1000 | Loss: 0.00004000
Iteration 281/1000 | Loss: 0.00004000
Iteration 282/1000 | Loss: 0.00004000
Iteration 283/1000 | Loss: 0.00004000
Iteration 284/1000 | Loss: 0.00004000
Iteration 285/1000 | Loss: 0.00004000
Iteration 286/1000 | Loss: 0.00004000
Iteration 287/1000 | Loss: 0.00004000
Iteration 288/1000 | Loss: 0.00004000
Iteration 289/1000 | Loss: 0.00004000
Iteration 290/1000 | Loss: 0.00004000
Iteration 291/1000 | Loss: 0.00004000
Iteration 292/1000 | Loss: 0.00004000
Iteration 293/1000 | Loss: 0.00004000
Iteration 294/1000 | Loss: 0.00004000
Iteration 295/1000 | Loss: 0.00004000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 295. Stopping optimization.
Last 5 losses: [4.000094850198366e-05, 4.000094850198366e-05, 4.000094850198366e-05, 4.000094850198366e-05, 4.000094850198366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.000094850198366e-05

Optimization complete. Final v2v error: 5.1669721603393555 mm

Highest mean error: 6.711630344390869 mm for frame 207

Lowest mean error: 4.085789680480957 mm for frame 1

Saving results

Total time: 236.52010345458984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013444
Iteration 2/25 | Loss: 0.00252567
Iteration 3/25 | Loss: 0.00203150
Iteration 4/25 | Loss: 0.00192581
Iteration 5/25 | Loss: 0.00176558
Iteration 6/25 | Loss: 0.00168520
Iteration 7/25 | Loss: 0.00165186
Iteration 8/25 | Loss: 0.00162755
Iteration 9/25 | Loss: 0.00159593
Iteration 10/25 | Loss: 0.00159960
Iteration 11/25 | Loss: 0.00160091
Iteration 12/25 | Loss: 0.00161252
Iteration 13/25 | Loss: 0.00160774
Iteration 14/25 | Loss: 0.00158246
Iteration 15/25 | Loss: 0.00156831
Iteration 16/25 | Loss: 0.00156006
Iteration 17/25 | Loss: 0.00155809
Iteration 18/25 | Loss: 0.00155740
Iteration 19/25 | Loss: 0.00155713
Iteration 20/25 | Loss: 0.00155690
Iteration 21/25 | Loss: 0.00155635
Iteration 22/25 | Loss: 0.00155525
Iteration 23/25 | Loss: 0.00155421
Iteration 24/25 | Loss: 0.00155370
Iteration 25/25 | Loss: 0.00155317

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.87169051
Iteration 2/25 | Loss: 0.00240069
Iteration 3/25 | Loss: 0.00240068
Iteration 4/25 | Loss: 0.00240068
Iteration 5/25 | Loss: 0.00240068
Iteration 6/25 | Loss: 0.00240068
Iteration 7/25 | Loss: 0.00240068
Iteration 8/25 | Loss: 0.00240068
Iteration 9/25 | Loss: 0.00240068
Iteration 10/25 | Loss: 0.00240068
Iteration 11/25 | Loss: 0.00240068
Iteration 12/25 | Loss: 0.00240068
Iteration 13/25 | Loss: 0.00240068
Iteration 14/25 | Loss: 0.00240068
Iteration 15/25 | Loss: 0.00240068
Iteration 16/25 | Loss: 0.00240068
Iteration 17/25 | Loss: 0.00240068
Iteration 18/25 | Loss: 0.00240068
Iteration 19/25 | Loss: 0.00240068
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0024006771855056286, 0.0024006771855056286, 0.0024006771855056286, 0.0024006771855056286, 0.0024006771855056286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024006771855056286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00240068
Iteration 2/1000 | Loss: 0.00078260
Iteration 3/1000 | Loss: 0.00064440
Iteration 4/1000 | Loss: 0.00027159
Iteration 5/1000 | Loss: 0.00026614
Iteration 6/1000 | Loss: 0.00014676
Iteration 7/1000 | Loss: 0.00057256
Iteration 8/1000 | Loss: 0.00024148
Iteration 9/1000 | Loss: 0.00011785
Iteration 10/1000 | Loss: 0.00010812
Iteration 11/1000 | Loss: 0.00010127
Iteration 12/1000 | Loss: 0.00009680
Iteration 13/1000 | Loss: 0.00009383
Iteration 14/1000 | Loss: 0.00009161
Iteration 15/1000 | Loss: 0.00030190
Iteration 16/1000 | Loss: 0.00012315
Iteration 17/1000 | Loss: 0.00030325
Iteration 18/1000 | Loss: 0.00031297
Iteration 19/1000 | Loss: 0.00030110
Iteration 20/1000 | Loss: 0.00008951
Iteration 21/1000 | Loss: 0.00008714
Iteration 22/1000 | Loss: 0.00008608
Iteration 23/1000 | Loss: 0.00008503
Iteration 24/1000 | Loss: 0.00008419
Iteration 25/1000 | Loss: 0.00027365
Iteration 26/1000 | Loss: 0.00102951
Iteration 27/1000 | Loss: 0.00037148
Iteration 28/1000 | Loss: 0.00034992
Iteration 29/1000 | Loss: 0.00027620
Iteration 30/1000 | Loss: 0.00018547
Iteration 31/1000 | Loss: 0.00026907
Iteration 32/1000 | Loss: 0.00009728
Iteration 33/1000 | Loss: 0.00009288
Iteration 34/1000 | Loss: 0.00008983
Iteration 35/1000 | Loss: 0.00014461
Iteration 36/1000 | Loss: 0.00008694
Iteration 37/1000 | Loss: 0.00008485
Iteration 38/1000 | Loss: 0.00008263
Iteration 39/1000 | Loss: 0.00008129
Iteration 40/1000 | Loss: 0.00058225
Iteration 41/1000 | Loss: 0.00058944
Iteration 42/1000 | Loss: 0.00015439
Iteration 43/1000 | Loss: 0.00008610
Iteration 44/1000 | Loss: 0.00007972
Iteration 45/1000 | Loss: 0.00007823
Iteration 46/1000 | Loss: 0.00045772
Iteration 47/1000 | Loss: 0.00021283
Iteration 48/1000 | Loss: 0.00026828
Iteration 49/1000 | Loss: 0.00016447
Iteration 50/1000 | Loss: 0.00034804
Iteration 51/1000 | Loss: 0.00045059
Iteration 52/1000 | Loss: 0.00009569
Iteration 53/1000 | Loss: 0.00008270
Iteration 54/1000 | Loss: 0.00007736
Iteration 55/1000 | Loss: 0.00007663
Iteration 56/1000 | Loss: 0.00007600
Iteration 57/1000 | Loss: 0.00007538
Iteration 58/1000 | Loss: 0.00013267
Iteration 59/1000 | Loss: 0.00014217
Iteration 60/1000 | Loss: 0.00008580
Iteration 61/1000 | Loss: 0.00008001
Iteration 62/1000 | Loss: 0.00007521
Iteration 63/1000 | Loss: 0.00007449
Iteration 64/1000 | Loss: 0.00012864
Iteration 65/1000 | Loss: 0.00008246
Iteration 66/1000 | Loss: 0.00007902
Iteration 67/1000 | Loss: 0.00012875
Iteration 68/1000 | Loss: 0.00013768
Iteration 69/1000 | Loss: 0.00040114
Iteration 70/1000 | Loss: 0.00027596
Iteration 71/1000 | Loss: 0.00022517
Iteration 72/1000 | Loss: 0.00045246
Iteration 73/1000 | Loss: 0.00043654
Iteration 74/1000 | Loss: 0.00064268
Iteration 75/1000 | Loss: 0.00035340
Iteration 76/1000 | Loss: 0.00009043
Iteration 77/1000 | Loss: 0.00007947
Iteration 78/1000 | Loss: 0.00007648
Iteration 79/1000 | Loss: 0.00007542
Iteration 80/1000 | Loss: 0.00007477
Iteration 81/1000 | Loss: 0.00007426
Iteration 82/1000 | Loss: 0.00007399
Iteration 83/1000 | Loss: 0.00055088
Iteration 84/1000 | Loss: 0.00017078
Iteration 85/1000 | Loss: 0.00007571
Iteration 86/1000 | Loss: 0.00054812
Iteration 87/1000 | Loss: 0.00050966
Iteration 88/1000 | Loss: 0.00028084
Iteration 89/1000 | Loss: 0.00040175
Iteration 90/1000 | Loss: 0.00007831
Iteration 91/1000 | Loss: 0.00007425
Iteration 92/1000 | Loss: 0.00007379
Iteration 93/1000 | Loss: 0.00054914
Iteration 94/1000 | Loss: 0.00009566
Iteration 95/1000 | Loss: 0.00010655
Iteration 96/1000 | Loss: 0.00010662
Iteration 97/1000 | Loss: 0.00010997
Iteration 98/1000 | Loss: 0.00007849
Iteration 99/1000 | Loss: 0.00007701
Iteration 100/1000 | Loss: 0.00007610
Iteration 101/1000 | Loss: 0.00007554
Iteration 102/1000 | Loss: 0.00007511
Iteration 103/1000 | Loss: 0.00007446
Iteration 104/1000 | Loss: 0.00007377
Iteration 105/1000 | Loss: 0.00007343
Iteration 106/1000 | Loss: 0.00007324
Iteration 107/1000 | Loss: 0.00007310
Iteration 108/1000 | Loss: 0.00007290
Iteration 109/1000 | Loss: 0.00007282
Iteration 110/1000 | Loss: 0.00007281
Iteration 111/1000 | Loss: 0.00007280
Iteration 112/1000 | Loss: 0.00007280
Iteration 113/1000 | Loss: 0.00007280
Iteration 114/1000 | Loss: 0.00007279
Iteration 115/1000 | Loss: 0.00007279
Iteration 116/1000 | Loss: 0.00007278
Iteration 117/1000 | Loss: 0.00007278
Iteration 118/1000 | Loss: 0.00007278
Iteration 119/1000 | Loss: 0.00007277
Iteration 120/1000 | Loss: 0.00007277
Iteration 121/1000 | Loss: 0.00007277
Iteration 122/1000 | Loss: 0.00007277
Iteration 123/1000 | Loss: 0.00007277
Iteration 124/1000 | Loss: 0.00007277
Iteration 125/1000 | Loss: 0.00007277
Iteration 126/1000 | Loss: 0.00007276
Iteration 127/1000 | Loss: 0.00007276
Iteration 128/1000 | Loss: 0.00007276
Iteration 129/1000 | Loss: 0.00007276
Iteration 130/1000 | Loss: 0.00007276
Iteration 131/1000 | Loss: 0.00007275
Iteration 132/1000 | Loss: 0.00007275
Iteration 133/1000 | Loss: 0.00007275
Iteration 134/1000 | Loss: 0.00007275
Iteration 135/1000 | Loss: 0.00007275
Iteration 136/1000 | Loss: 0.00007275
Iteration 137/1000 | Loss: 0.00007275
Iteration 138/1000 | Loss: 0.00007275
Iteration 139/1000 | Loss: 0.00007275
Iteration 140/1000 | Loss: 0.00007275
Iteration 141/1000 | Loss: 0.00007275
Iteration 142/1000 | Loss: 0.00007275
Iteration 143/1000 | Loss: 0.00007275
Iteration 144/1000 | Loss: 0.00007275
Iteration 145/1000 | Loss: 0.00007274
Iteration 146/1000 | Loss: 0.00007274
Iteration 147/1000 | Loss: 0.00007274
Iteration 148/1000 | Loss: 0.00007274
Iteration 149/1000 | Loss: 0.00007274
Iteration 150/1000 | Loss: 0.00007274
Iteration 151/1000 | Loss: 0.00007274
Iteration 152/1000 | Loss: 0.00007274
Iteration 153/1000 | Loss: 0.00007274
Iteration 154/1000 | Loss: 0.00007274
Iteration 155/1000 | Loss: 0.00007274
Iteration 156/1000 | Loss: 0.00007274
Iteration 157/1000 | Loss: 0.00007273
Iteration 158/1000 | Loss: 0.00007273
Iteration 159/1000 | Loss: 0.00007273
Iteration 160/1000 | Loss: 0.00007273
Iteration 161/1000 | Loss: 0.00007272
Iteration 162/1000 | Loss: 0.00007272
Iteration 163/1000 | Loss: 0.00007272
Iteration 164/1000 | Loss: 0.00007271
Iteration 165/1000 | Loss: 0.00007271
Iteration 166/1000 | Loss: 0.00007271
Iteration 167/1000 | Loss: 0.00007271
Iteration 168/1000 | Loss: 0.00007270
Iteration 169/1000 | Loss: 0.00007270
Iteration 170/1000 | Loss: 0.00007269
Iteration 171/1000 | Loss: 0.00007269
Iteration 172/1000 | Loss: 0.00007269
Iteration 173/1000 | Loss: 0.00007269
Iteration 174/1000 | Loss: 0.00007269
Iteration 175/1000 | Loss: 0.00007268
Iteration 176/1000 | Loss: 0.00007268
Iteration 177/1000 | Loss: 0.00007268
Iteration 178/1000 | Loss: 0.00007268
Iteration 179/1000 | Loss: 0.00007267
Iteration 180/1000 | Loss: 0.00007267
Iteration 181/1000 | Loss: 0.00007267
Iteration 182/1000 | Loss: 0.00007267
Iteration 183/1000 | Loss: 0.00007267
Iteration 184/1000 | Loss: 0.00007267
Iteration 185/1000 | Loss: 0.00007267
Iteration 186/1000 | Loss: 0.00007266
Iteration 187/1000 | Loss: 0.00007266
Iteration 188/1000 | Loss: 0.00007266
Iteration 189/1000 | Loss: 0.00007266
Iteration 190/1000 | Loss: 0.00007266
Iteration 191/1000 | Loss: 0.00007266
Iteration 192/1000 | Loss: 0.00007265
Iteration 193/1000 | Loss: 0.00007265
Iteration 194/1000 | Loss: 0.00007264
Iteration 195/1000 | Loss: 0.00007264
Iteration 196/1000 | Loss: 0.00007264
Iteration 197/1000 | Loss: 0.00007263
Iteration 198/1000 | Loss: 0.00007263
Iteration 199/1000 | Loss: 0.00007263
Iteration 200/1000 | Loss: 0.00007263
Iteration 201/1000 | Loss: 0.00007263
Iteration 202/1000 | Loss: 0.00007262
Iteration 203/1000 | Loss: 0.00007262
Iteration 204/1000 | Loss: 0.00007262
Iteration 205/1000 | Loss: 0.00007262
Iteration 206/1000 | Loss: 0.00007261
Iteration 207/1000 | Loss: 0.00007261
Iteration 208/1000 | Loss: 0.00007261
Iteration 209/1000 | Loss: 0.00007261
Iteration 210/1000 | Loss: 0.00007261
Iteration 211/1000 | Loss: 0.00007261
Iteration 212/1000 | Loss: 0.00007260
Iteration 213/1000 | Loss: 0.00007260
Iteration 214/1000 | Loss: 0.00007260
Iteration 215/1000 | Loss: 0.00007260
Iteration 216/1000 | Loss: 0.00007260
Iteration 217/1000 | Loss: 0.00007259
Iteration 218/1000 | Loss: 0.00007259
Iteration 219/1000 | Loss: 0.00007259
Iteration 220/1000 | Loss: 0.00007259
Iteration 221/1000 | Loss: 0.00007259
Iteration 222/1000 | Loss: 0.00007258
Iteration 223/1000 | Loss: 0.00007258
Iteration 224/1000 | Loss: 0.00007258
Iteration 225/1000 | Loss: 0.00007258
Iteration 226/1000 | Loss: 0.00007258
Iteration 227/1000 | Loss: 0.00007257
Iteration 228/1000 | Loss: 0.00007257
Iteration 229/1000 | Loss: 0.00007257
Iteration 230/1000 | Loss: 0.00007257
Iteration 231/1000 | Loss: 0.00007257
Iteration 232/1000 | Loss: 0.00007257
Iteration 233/1000 | Loss: 0.00007257
Iteration 234/1000 | Loss: 0.00007257
Iteration 235/1000 | Loss: 0.00007257
Iteration 236/1000 | Loss: 0.00007257
Iteration 237/1000 | Loss: 0.00007257
Iteration 238/1000 | Loss: 0.00007257
Iteration 239/1000 | Loss: 0.00007256
Iteration 240/1000 | Loss: 0.00007256
Iteration 241/1000 | Loss: 0.00007256
Iteration 242/1000 | Loss: 0.00007256
Iteration 243/1000 | Loss: 0.00007256
Iteration 244/1000 | Loss: 0.00007256
Iteration 245/1000 | Loss: 0.00007256
Iteration 246/1000 | Loss: 0.00007256
Iteration 247/1000 | Loss: 0.00007256
Iteration 248/1000 | Loss: 0.00007256
Iteration 249/1000 | Loss: 0.00007256
Iteration 250/1000 | Loss: 0.00007256
Iteration 251/1000 | Loss: 0.00007256
Iteration 252/1000 | Loss: 0.00007256
Iteration 253/1000 | Loss: 0.00007256
Iteration 254/1000 | Loss: 0.00007256
Iteration 255/1000 | Loss: 0.00007256
Iteration 256/1000 | Loss: 0.00007255
Iteration 257/1000 | Loss: 0.00007255
Iteration 258/1000 | Loss: 0.00007255
Iteration 259/1000 | Loss: 0.00007255
Iteration 260/1000 | Loss: 0.00007255
Iteration 261/1000 | Loss: 0.00007255
Iteration 262/1000 | Loss: 0.00007255
Iteration 263/1000 | Loss: 0.00007255
Iteration 264/1000 | Loss: 0.00007255
Iteration 265/1000 | Loss: 0.00007255
Iteration 266/1000 | Loss: 0.00007255
Iteration 267/1000 | Loss: 0.00007254
Iteration 268/1000 | Loss: 0.00007254
Iteration 269/1000 | Loss: 0.00007254
Iteration 270/1000 | Loss: 0.00007254
Iteration 271/1000 | Loss: 0.00007254
Iteration 272/1000 | Loss: 0.00007254
Iteration 273/1000 | Loss: 0.00007254
Iteration 274/1000 | Loss: 0.00007254
Iteration 275/1000 | Loss: 0.00007254
Iteration 276/1000 | Loss: 0.00007254
Iteration 277/1000 | Loss: 0.00007254
Iteration 278/1000 | Loss: 0.00007254
Iteration 279/1000 | Loss: 0.00007254
Iteration 280/1000 | Loss: 0.00007254
Iteration 281/1000 | Loss: 0.00007254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [7.25404970580712e-05, 7.25404970580712e-05, 7.25404970580712e-05, 7.25404970580712e-05, 7.25404970580712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.25404970580712e-05

Optimization complete. Final v2v error: 4.95224142074585 mm

Highest mean error: 12.014851570129395 mm for frame 52

Lowest mean error: 3.411350727081299 mm for frame 5

Saving results

Total time: 205.5624988079071
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008360
Iteration 2/25 | Loss: 0.01008360
Iteration 3/25 | Loss: 0.01008360
Iteration 4/25 | Loss: 0.01008360
Iteration 5/25 | Loss: 0.01008360
Iteration 6/25 | Loss: 0.01008359
Iteration 7/25 | Loss: 0.01008359
Iteration 8/25 | Loss: 0.01008359
Iteration 9/25 | Loss: 0.01008359
Iteration 10/25 | Loss: 0.01008359
Iteration 11/25 | Loss: 0.01008359
Iteration 12/25 | Loss: 0.01008359
Iteration 13/25 | Loss: 0.01008359
Iteration 14/25 | Loss: 0.01008359
Iteration 15/25 | Loss: 0.01008359
Iteration 16/25 | Loss: 0.01008359
Iteration 17/25 | Loss: 0.01008359
Iteration 18/25 | Loss: 0.01008359
Iteration 19/25 | Loss: 0.01008358
Iteration 20/25 | Loss: 0.01008358
Iteration 21/25 | Loss: 0.01008358
Iteration 22/25 | Loss: 0.01008358
Iteration 23/25 | Loss: 0.01008358
Iteration 24/25 | Loss: 0.01008358
Iteration 25/25 | Loss: 0.01008358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89662492
Iteration 2/25 | Loss: 0.08930820
Iteration 3/25 | Loss: 0.08857475
Iteration 4/25 | Loss: 0.08855635
Iteration 5/25 | Loss: 0.08855632
Iteration 6/25 | Loss: 0.08855632
Iteration 7/25 | Loss: 0.08855631
Iteration 8/25 | Loss: 0.08855630
Iteration 9/25 | Loss: 0.08855630
Iteration 10/25 | Loss: 0.08855631
Iteration 11/25 | Loss: 0.08855631
Iteration 12/25 | Loss: 0.08855631
Iteration 13/25 | Loss: 0.08855631
Iteration 14/25 | Loss: 0.08855631
Iteration 15/25 | Loss: 0.08855631
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.08855631202459335, 0.08855631202459335, 0.08855631202459335, 0.08855631202459335, 0.08855631202459335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08855631202459335

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08855630
Iteration 2/1000 | Loss: 0.00433035
Iteration 3/1000 | Loss: 0.00275414
Iteration 4/1000 | Loss: 0.00123365
Iteration 5/1000 | Loss: 0.00157146
Iteration 6/1000 | Loss: 0.00112843
Iteration 7/1000 | Loss: 0.00017021
Iteration 8/1000 | Loss: 0.00010325
Iteration 9/1000 | Loss: 0.00336852
Iteration 10/1000 | Loss: 0.00678105
Iteration 11/1000 | Loss: 0.00201263
Iteration 12/1000 | Loss: 0.00483613
Iteration 13/1000 | Loss: 0.00070993
Iteration 14/1000 | Loss: 0.00180277
Iteration 15/1000 | Loss: 0.00137955
Iteration 16/1000 | Loss: 0.00023308
Iteration 17/1000 | Loss: 0.00006203
Iteration 18/1000 | Loss: 0.00011917
Iteration 19/1000 | Loss: 0.00013571
Iteration 20/1000 | Loss: 0.00110693
Iteration 21/1000 | Loss: 0.00242228
Iteration 22/1000 | Loss: 0.00036166
Iteration 23/1000 | Loss: 0.00004436
Iteration 24/1000 | Loss: 0.00058836
Iteration 25/1000 | Loss: 0.00062371
Iteration 26/1000 | Loss: 0.00064794
Iteration 27/1000 | Loss: 0.00016767
Iteration 28/1000 | Loss: 0.00003811
Iteration 29/1000 | Loss: 0.00003417
Iteration 30/1000 | Loss: 0.00003200
Iteration 31/1000 | Loss: 0.00058047
Iteration 32/1000 | Loss: 0.00081909
Iteration 33/1000 | Loss: 0.00010780
Iteration 34/1000 | Loss: 0.00039946
Iteration 35/1000 | Loss: 0.00006591
Iteration 36/1000 | Loss: 0.00007078
Iteration 37/1000 | Loss: 0.00002902
Iteration 38/1000 | Loss: 0.00003569
Iteration 39/1000 | Loss: 0.00010795
Iteration 40/1000 | Loss: 0.00002653
Iteration 41/1000 | Loss: 0.00008411
Iteration 42/1000 | Loss: 0.00017870
Iteration 43/1000 | Loss: 0.00003084
Iteration 44/1000 | Loss: 0.00020592
Iteration 45/1000 | Loss: 0.00004525
Iteration 46/1000 | Loss: 0.00002939
Iteration 47/1000 | Loss: 0.00002404
Iteration 48/1000 | Loss: 0.00013224
Iteration 49/1000 | Loss: 0.00002364
Iteration 50/1000 | Loss: 0.00006362
Iteration 51/1000 | Loss: 0.00003926
Iteration 52/1000 | Loss: 0.00002414
Iteration 53/1000 | Loss: 0.00002637
Iteration 54/1000 | Loss: 0.00002202
Iteration 55/1000 | Loss: 0.00002172
Iteration 56/1000 | Loss: 0.00023030
Iteration 57/1000 | Loss: 0.00004331
Iteration 58/1000 | Loss: 0.00004078
Iteration 59/1000 | Loss: 0.00002487
Iteration 60/1000 | Loss: 0.00005164
Iteration 61/1000 | Loss: 0.00019936
Iteration 62/1000 | Loss: 0.00004132
Iteration 63/1000 | Loss: 0.00002273
Iteration 64/1000 | Loss: 0.00002168
Iteration 65/1000 | Loss: 0.00002738
Iteration 66/1000 | Loss: 0.00003153
Iteration 67/1000 | Loss: 0.00002090
Iteration 68/1000 | Loss: 0.00002057
Iteration 69/1000 | Loss: 0.00002041
Iteration 70/1000 | Loss: 0.00002039
Iteration 71/1000 | Loss: 0.00002035
Iteration 72/1000 | Loss: 0.00002034
Iteration 73/1000 | Loss: 0.00002034
Iteration 74/1000 | Loss: 0.00002033
Iteration 75/1000 | Loss: 0.00002028
Iteration 76/1000 | Loss: 0.00002027
Iteration 77/1000 | Loss: 0.00002027
Iteration 78/1000 | Loss: 0.00002025
Iteration 79/1000 | Loss: 0.00002022
Iteration 80/1000 | Loss: 0.00002018
Iteration 81/1000 | Loss: 0.00002017
Iteration 82/1000 | Loss: 0.00002017
Iteration 83/1000 | Loss: 0.00002017
Iteration 84/1000 | Loss: 0.00002017
Iteration 85/1000 | Loss: 0.00002017
Iteration 86/1000 | Loss: 0.00002017
Iteration 87/1000 | Loss: 0.00002017
Iteration 88/1000 | Loss: 0.00002016
Iteration 89/1000 | Loss: 0.00002016
Iteration 90/1000 | Loss: 0.00002016
Iteration 91/1000 | Loss: 0.00002016
Iteration 92/1000 | Loss: 0.00002016
Iteration 93/1000 | Loss: 0.00002016
Iteration 94/1000 | Loss: 0.00002015
Iteration 95/1000 | Loss: 0.00002014
Iteration 96/1000 | Loss: 0.00002014
Iteration 97/1000 | Loss: 0.00002013
Iteration 98/1000 | Loss: 0.00002013
Iteration 99/1000 | Loss: 0.00002013
Iteration 100/1000 | Loss: 0.00002011
Iteration 101/1000 | Loss: 0.00002011
Iteration 102/1000 | Loss: 0.00002011
Iteration 103/1000 | Loss: 0.00002011
Iteration 104/1000 | Loss: 0.00002011
Iteration 105/1000 | Loss: 0.00002011
Iteration 106/1000 | Loss: 0.00002011
Iteration 107/1000 | Loss: 0.00002011
Iteration 108/1000 | Loss: 0.00002010
Iteration 109/1000 | Loss: 0.00002010
Iteration 110/1000 | Loss: 0.00002010
Iteration 111/1000 | Loss: 0.00002010
Iteration 112/1000 | Loss: 0.00002010
Iteration 113/1000 | Loss: 0.00002009
Iteration 114/1000 | Loss: 0.00002009
Iteration 115/1000 | Loss: 0.00022040
Iteration 116/1000 | Loss: 0.00005111
Iteration 117/1000 | Loss: 0.00004826
Iteration 118/1000 | Loss: 0.00002034
Iteration 119/1000 | Loss: 0.00002017
Iteration 120/1000 | Loss: 0.00002008
Iteration 121/1000 | Loss: 0.00005959
Iteration 122/1000 | Loss: 0.00002383
Iteration 123/1000 | Loss: 0.00002002
Iteration 124/1000 | Loss: 0.00002000
Iteration 125/1000 | Loss: 0.00002000
Iteration 126/1000 | Loss: 0.00002000
Iteration 127/1000 | Loss: 0.00002000
Iteration 128/1000 | Loss: 0.00002000
Iteration 129/1000 | Loss: 0.00002000
Iteration 130/1000 | Loss: 0.00002000
Iteration 131/1000 | Loss: 0.00001999
Iteration 132/1000 | Loss: 0.00001999
Iteration 133/1000 | Loss: 0.00001999
Iteration 134/1000 | Loss: 0.00001999
Iteration 135/1000 | Loss: 0.00001999
Iteration 136/1000 | Loss: 0.00001998
Iteration 137/1000 | Loss: 0.00001998
Iteration 138/1000 | Loss: 0.00001998
Iteration 139/1000 | Loss: 0.00001998
Iteration 140/1000 | Loss: 0.00001998
Iteration 141/1000 | Loss: 0.00001998
Iteration 142/1000 | Loss: 0.00001998
Iteration 143/1000 | Loss: 0.00001998
Iteration 144/1000 | Loss: 0.00001998
Iteration 145/1000 | Loss: 0.00001998
Iteration 146/1000 | Loss: 0.00001998
Iteration 147/1000 | Loss: 0.00001998
Iteration 148/1000 | Loss: 0.00001998
Iteration 149/1000 | Loss: 0.00001998
Iteration 150/1000 | Loss: 0.00001998
Iteration 151/1000 | Loss: 0.00001998
Iteration 152/1000 | Loss: 0.00001998
Iteration 153/1000 | Loss: 0.00001998
Iteration 154/1000 | Loss: 0.00001998
Iteration 155/1000 | Loss: 0.00001998
Iteration 156/1000 | Loss: 0.00001998
Iteration 157/1000 | Loss: 0.00001998
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.998008337977808e-05, 1.998008337977808e-05, 1.998008337977808e-05, 1.998008337977808e-05, 1.998008337977808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.998008337977808e-05

Optimization complete. Final v2v error: 3.785780668258667 mm

Highest mean error: 4.335045337677002 mm for frame 90

Lowest mean error: 3.433194160461426 mm for frame 230

Saving results

Total time: 139.42176508903503
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411947
Iteration 2/25 | Loss: 0.00149198
Iteration 3/25 | Loss: 0.00128607
Iteration 4/25 | Loss: 0.00127013
Iteration 5/25 | Loss: 0.00126791
Iteration 6/25 | Loss: 0.00126733
Iteration 7/25 | Loss: 0.00126733
Iteration 8/25 | Loss: 0.00126733
Iteration 9/25 | Loss: 0.00126733
Iteration 10/25 | Loss: 0.00126733
Iteration 11/25 | Loss: 0.00126733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012673282762989402, 0.0012673282762989402, 0.0012673282762989402, 0.0012673282762989402, 0.0012673282762989402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012673282762989402

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41816211
Iteration 2/25 | Loss: 0.00067671
Iteration 3/25 | Loss: 0.00067671
Iteration 4/25 | Loss: 0.00067671
Iteration 5/25 | Loss: 0.00067671
Iteration 6/25 | Loss: 0.00067671
Iteration 7/25 | Loss: 0.00067671
Iteration 8/25 | Loss: 0.00067671
Iteration 9/25 | Loss: 0.00067671
Iteration 10/25 | Loss: 0.00067671
Iteration 11/25 | Loss: 0.00067670
Iteration 12/25 | Loss: 0.00067670
Iteration 13/25 | Loss: 0.00067670
Iteration 14/25 | Loss: 0.00067670
Iteration 15/25 | Loss: 0.00067670
Iteration 16/25 | Loss: 0.00067670
Iteration 17/25 | Loss: 0.00067670
Iteration 18/25 | Loss: 0.00067670
Iteration 19/25 | Loss: 0.00067670
Iteration 20/25 | Loss: 0.00067670
Iteration 21/25 | Loss: 0.00067670
Iteration 22/25 | Loss: 0.00067670
Iteration 23/25 | Loss: 0.00067670
Iteration 24/25 | Loss: 0.00067670
Iteration 25/25 | Loss: 0.00067670

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067670
Iteration 2/1000 | Loss: 0.00003019
Iteration 3/1000 | Loss: 0.00002101
Iteration 4/1000 | Loss: 0.00001936
Iteration 5/1000 | Loss: 0.00001820
Iteration 6/1000 | Loss: 0.00001737
Iteration 7/1000 | Loss: 0.00001673
Iteration 8/1000 | Loss: 0.00001639
Iteration 9/1000 | Loss: 0.00001617
Iteration 10/1000 | Loss: 0.00001615
Iteration 11/1000 | Loss: 0.00001592
Iteration 12/1000 | Loss: 0.00001591
Iteration 13/1000 | Loss: 0.00001582
Iteration 14/1000 | Loss: 0.00001572
Iteration 15/1000 | Loss: 0.00001567
Iteration 16/1000 | Loss: 0.00001566
Iteration 17/1000 | Loss: 0.00001566
Iteration 18/1000 | Loss: 0.00001565
Iteration 19/1000 | Loss: 0.00001564
Iteration 20/1000 | Loss: 0.00001560
Iteration 21/1000 | Loss: 0.00001560
Iteration 22/1000 | Loss: 0.00001559
Iteration 23/1000 | Loss: 0.00001551
Iteration 24/1000 | Loss: 0.00001543
Iteration 25/1000 | Loss: 0.00001541
Iteration 26/1000 | Loss: 0.00001540
Iteration 27/1000 | Loss: 0.00001540
Iteration 28/1000 | Loss: 0.00001538
Iteration 29/1000 | Loss: 0.00001535
Iteration 30/1000 | Loss: 0.00001535
Iteration 31/1000 | Loss: 0.00001534
Iteration 32/1000 | Loss: 0.00001531
Iteration 33/1000 | Loss: 0.00001531
Iteration 34/1000 | Loss: 0.00001531
Iteration 35/1000 | Loss: 0.00001531
Iteration 36/1000 | Loss: 0.00001531
Iteration 37/1000 | Loss: 0.00001531
Iteration 38/1000 | Loss: 0.00001531
Iteration 39/1000 | Loss: 0.00001531
Iteration 40/1000 | Loss: 0.00001531
Iteration 41/1000 | Loss: 0.00001531
Iteration 42/1000 | Loss: 0.00001530
Iteration 43/1000 | Loss: 0.00001529
Iteration 44/1000 | Loss: 0.00001529
Iteration 45/1000 | Loss: 0.00001528
Iteration 46/1000 | Loss: 0.00001527
Iteration 47/1000 | Loss: 0.00001526
Iteration 48/1000 | Loss: 0.00001526
Iteration 49/1000 | Loss: 0.00001525
Iteration 50/1000 | Loss: 0.00001525
Iteration 51/1000 | Loss: 0.00001524
Iteration 52/1000 | Loss: 0.00001523
Iteration 53/1000 | Loss: 0.00001522
Iteration 54/1000 | Loss: 0.00001521
Iteration 55/1000 | Loss: 0.00001521
Iteration 56/1000 | Loss: 0.00001520
Iteration 57/1000 | Loss: 0.00001520
Iteration 58/1000 | Loss: 0.00001515
Iteration 59/1000 | Loss: 0.00001514
Iteration 60/1000 | Loss: 0.00001514
Iteration 61/1000 | Loss: 0.00001511
Iteration 62/1000 | Loss: 0.00001510
Iteration 63/1000 | Loss: 0.00001510
Iteration 64/1000 | Loss: 0.00001510
Iteration 65/1000 | Loss: 0.00001509
Iteration 66/1000 | Loss: 0.00001509
Iteration 67/1000 | Loss: 0.00001508
Iteration 68/1000 | Loss: 0.00001508
Iteration 69/1000 | Loss: 0.00001507
Iteration 70/1000 | Loss: 0.00001507
Iteration 71/1000 | Loss: 0.00001507
Iteration 72/1000 | Loss: 0.00001507
Iteration 73/1000 | Loss: 0.00001506
Iteration 74/1000 | Loss: 0.00001506
Iteration 75/1000 | Loss: 0.00001506
Iteration 76/1000 | Loss: 0.00001505
Iteration 77/1000 | Loss: 0.00001505
Iteration 78/1000 | Loss: 0.00001504
Iteration 79/1000 | Loss: 0.00001501
Iteration 80/1000 | Loss: 0.00001501
Iteration 81/1000 | Loss: 0.00001501
Iteration 82/1000 | Loss: 0.00001501
Iteration 83/1000 | Loss: 0.00001501
Iteration 84/1000 | Loss: 0.00001501
Iteration 85/1000 | Loss: 0.00001501
Iteration 86/1000 | Loss: 0.00001501
Iteration 87/1000 | Loss: 0.00001500
Iteration 88/1000 | Loss: 0.00001500
Iteration 89/1000 | Loss: 0.00001499
Iteration 90/1000 | Loss: 0.00001499
Iteration 91/1000 | Loss: 0.00001499
Iteration 92/1000 | Loss: 0.00001499
Iteration 93/1000 | Loss: 0.00001498
Iteration 94/1000 | Loss: 0.00001498
Iteration 95/1000 | Loss: 0.00001498
Iteration 96/1000 | Loss: 0.00001498
Iteration 97/1000 | Loss: 0.00001498
Iteration 98/1000 | Loss: 0.00001498
Iteration 99/1000 | Loss: 0.00001498
Iteration 100/1000 | Loss: 0.00001498
Iteration 101/1000 | Loss: 0.00001498
Iteration 102/1000 | Loss: 0.00001498
Iteration 103/1000 | Loss: 0.00001498
Iteration 104/1000 | Loss: 0.00001498
Iteration 105/1000 | Loss: 0.00001498
Iteration 106/1000 | Loss: 0.00001497
Iteration 107/1000 | Loss: 0.00001497
Iteration 108/1000 | Loss: 0.00001497
Iteration 109/1000 | Loss: 0.00001497
Iteration 110/1000 | Loss: 0.00001497
Iteration 111/1000 | Loss: 0.00001496
Iteration 112/1000 | Loss: 0.00001496
Iteration 113/1000 | Loss: 0.00001496
Iteration 114/1000 | Loss: 0.00001496
Iteration 115/1000 | Loss: 0.00001496
Iteration 116/1000 | Loss: 0.00001496
Iteration 117/1000 | Loss: 0.00001496
Iteration 118/1000 | Loss: 0.00001496
Iteration 119/1000 | Loss: 0.00001496
Iteration 120/1000 | Loss: 0.00001496
Iteration 121/1000 | Loss: 0.00001496
Iteration 122/1000 | Loss: 0.00001496
Iteration 123/1000 | Loss: 0.00001496
Iteration 124/1000 | Loss: 0.00001495
Iteration 125/1000 | Loss: 0.00001495
Iteration 126/1000 | Loss: 0.00001495
Iteration 127/1000 | Loss: 0.00001495
Iteration 128/1000 | Loss: 0.00001495
Iteration 129/1000 | Loss: 0.00001495
Iteration 130/1000 | Loss: 0.00001495
Iteration 131/1000 | Loss: 0.00001495
Iteration 132/1000 | Loss: 0.00001494
Iteration 133/1000 | Loss: 0.00001494
Iteration 134/1000 | Loss: 0.00001494
Iteration 135/1000 | Loss: 0.00001494
Iteration 136/1000 | Loss: 0.00001494
Iteration 137/1000 | Loss: 0.00001494
Iteration 138/1000 | Loss: 0.00001494
Iteration 139/1000 | Loss: 0.00001494
Iteration 140/1000 | Loss: 0.00001494
Iteration 141/1000 | Loss: 0.00001494
Iteration 142/1000 | Loss: 0.00001494
Iteration 143/1000 | Loss: 0.00001494
Iteration 144/1000 | Loss: 0.00001494
Iteration 145/1000 | Loss: 0.00001494
Iteration 146/1000 | Loss: 0.00001494
Iteration 147/1000 | Loss: 0.00001494
Iteration 148/1000 | Loss: 0.00001494
Iteration 149/1000 | Loss: 0.00001493
Iteration 150/1000 | Loss: 0.00001493
Iteration 151/1000 | Loss: 0.00001493
Iteration 152/1000 | Loss: 0.00001493
Iteration 153/1000 | Loss: 0.00001493
Iteration 154/1000 | Loss: 0.00001493
Iteration 155/1000 | Loss: 0.00001493
Iteration 156/1000 | Loss: 0.00001493
Iteration 157/1000 | Loss: 0.00001493
Iteration 158/1000 | Loss: 0.00001493
Iteration 159/1000 | Loss: 0.00001492
Iteration 160/1000 | Loss: 0.00001492
Iteration 161/1000 | Loss: 0.00001492
Iteration 162/1000 | Loss: 0.00001492
Iteration 163/1000 | Loss: 0.00001492
Iteration 164/1000 | Loss: 0.00001492
Iteration 165/1000 | Loss: 0.00001492
Iteration 166/1000 | Loss: 0.00001492
Iteration 167/1000 | Loss: 0.00001492
Iteration 168/1000 | Loss: 0.00001492
Iteration 169/1000 | Loss: 0.00001492
Iteration 170/1000 | Loss: 0.00001491
Iteration 171/1000 | Loss: 0.00001491
Iteration 172/1000 | Loss: 0.00001491
Iteration 173/1000 | Loss: 0.00001491
Iteration 174/1000 | Loss: 0.00001491
Iteration 175/1000 | Loss: 0.00001491
Iteration 176/1000 | Loss: 0.00001491
Iteration 177/1000 | Loss: 0.00001490
Iteration 178/1000 | Loss: 0.00001490
Iteration 179/1000 | Loss: 0.00001490
Iteration 180/1000 | Loss: 0.00001490
Iteration 181/1000 | Loss: 0.00001490
Iteration 182/1000 | Loss: 0.00001490
Iteration 183/1000 | Loss: 0.00001490
Iteration 184/1000 | Loss: 0.00001490
Iteration 185/1000 | Loss: 0.00001489
Iteration 186/1000 | Loss: 0.00001489
Iteration 187/1000 | Loss: 0.00001489
Iteration 188/1000 | Loss: 0.00001489
Iteration 189/1000 | Loss: 0.00001488
Iteration 190/1000 | Loss: 0.00001488
Iteration 191/1000 | Loss: 0.00001488
Iteration 192/1000 | Loss: 0.00001488
Iteration 193/1000 | Loss: 0.00001488
Iteration 194/1000 | Loss: 0.00001488
Iteration 195/1000 | Loss: 0.00001487
Iteration 196/1000 | Loss: 0.00001487
Iteration 197/1000 | Loss: 0.00001487
Iteration 198/1000 | Loss: 0.00001487
Iteration 199/1000 | Loss: 0.00001487
Iteration 200/1000 | Loss: 0.00001487
Iteration 201/1000 | Loss: 0.00001487
Iteration 202/1000 | Loss: 0.00001487
Iteration 203/1000 | Loss: 0.00001487
Iteration 204/1000 | Loss: 0.00001487
Iteration 205/1000 | Loss: 0.00001487
Iteration 206/1000 | Loss: 0.00001487
Iteration 207/1000 | Loss: 0.00001487
Iteration 208/1000 | Loss: 0.00001486
Iteration 209/1000 | Loss: 0.00001486
Iteration 210/1000 | Loss: 0.00001486
Iteration 211/1000 | Loss: 0.00001486
Iteration 212/1000 | Loss: 0.00001486
Iteration 213/1000 | Loss: 0.00001486
Iteration 214/1000 | Loss: 0.00001486
Iteration 215/1000 | Loss: 0.00001486
Iteration 216/1000 | Loss: 0.00001486
Iteration 217/1000 | Loss: 0.00001486
Iteration 218/1000 | Loss: 0.00001486
Iteration 219/1000 | Loss: 0.00001485
Iteration 220/1000 | Loss: 0.00001485
Iteration 221/1000 | Loss: 0.00001485
Iteration 222/1000 | Loss: 0.00001485
Iteration 223/1000 | Loss: 0.00001485
Iteration 224/1000 | Loss: 0.00001485
Iteration 225/1000 | Loss: 0.00001485
Iteration 226/1000 | Loss: 0.00001485
Iteration 227/1000 | Loss: 0.00001485
Iteration 228/1000 | Loss: 0.00001484
Iteration 229/1000 | Loss: 0.00001484
Iteration 230/1000 | Loss: 0.00001484
Iteration 231/1000 | Loss: 0.00001484
Iteration 232/1000 | Loss: 0.00001483
Iteration 233/1000 | Loss: 0.00001483
Iteration 234/1000 | Loss: 0.00001483
Iteration 235/1000 | Loss: 0.00001483
Iteration 236/1000 | Loss: 0.00001483
Iteration 237/1000 | Loss: 0.00001483
Iteration 238/1000 | Loss: 0.00001483
Iteration 239/1000 | Loss: 0.00001483
Iteration 240/1000 | Loss: 0.00001483
Iteration 241/1000 | Loss: 0.00001483
Iteration 242/1000 | Loss: 0.00001483
Iteration 243/1000 | Loss: 0.00001483
Iteration 244/1000 | Loss: 0.00001483
Iteration 245/1000 | Loss: 0.00001483
Iteration 246/1000 | Loss: 0.00001483
Iteration 247/1000 | Loss: 0.00001483
Iteration 248/1000 | Loss: 0.00001483
Iteration 249/1000 | Loss: 0.00001483
Iteration 250/1000 | Loss: 0.00001482
Iteration 251/1000 | Loss: 0.00001482
Iteration 252/1000 | Loss: 0.00001482
Iteration 253/1000 | Loss: 0.00001482
Iteration 254/1000 | Loss: 0.00001482
Iteration 255/1000 | Loss: 0.00001482
Iteration 256/1000 | Loss: 0.00001482
Iteration 257/1000 | Loss: 0.00001482
Iteration 258/1000 | Loss: 0.00001481
Iteration 259/1000 | Loss: 0.00001481
Iteration 260/1000 | Loss: 0.00001481
Iteration 261/1000 | Loss: 0.00001481
Iteration 262/1000 | Loss: 0.00001481
Iteration 263/1000 | Loss: 0.00001481
Iteration 264/1000 | Loss: 0.00001481
Iteration 265/1000 | Loss: 0.00001481
Iteration 266/1000 | Loss: 0.00001481
Iteration 267/1000 | Loss: 0.00001481
Iteration 268/1000 | Loss: 0.00001481
Iteration 269/1000 | Loss: 0.00001481
Iteration 270/1000 | Loss: 0.00001481
Iteration 271/1000 | Loss: 0.00001481
Iteration 272/1000 | Loss: 0.00001481
Iteration 273/1000 | Loss: 0.00001481
Iteration 274/1000 | Loss: 0.00001481
Iteration 275/1000 | Loss: 0.00001481
Iteration 276/1000 | Loss: 0.00001481
Iteration 277/1000 | Loss: 0.00001480
Iteration 278/1000 | Loss: 0.00001480
Iteration 279/1000 | Loss: 0.00001480
Iteration 280/1000 | Loss: 0.00001480
Iteration 281/1000 | Loss: 0.00001480
Iteration 282/1000 | Loss: 0.00001480
Iteration 283/1000 | Loss: 0.00001480
Iteration 284/1000 | Loss: 0.00001480
Iteration 285/1000 | Loss: 0.00001480
Iteration 286/1000 | Loss: 0.00001480
Iteration 287/1000 | Loss: 0.00001480
Iteration 288/1000 | Loss: 0.00001480
Iteration 289/1000 | Loss: 0.00001479
Iteration 290/1000 | Loss: 0.00001479
Iteration 291/1000 | Loss: 0.00001479
Iteration 292/1000 | Loss: 0.00001479
Iteration 293/1000 | Loss: 0.00001479
Iteration 294/1000 | Loss: 0.00001479
Iteration 295/1000 | Loss: 0.00001479
Iteration 296/1000 | Loss: 0.00001478
Iteration 297/1000 | Loss: 0.00001478
Iteration 298/1000 | Loss: 0.00001478
Iteration 299/1000 | Loss: 0.00001478
Iteration 300/1000 | Loss: 0.00001478
Iteration 301/1000 | Loss: 0.00001478
Iteration 302/1000 | Loss: 0.00001478
Iteration 303/1000 | Loss: 0.00001478
Iteration 304/1000 | Loss: 0.00001478
Iteration 305/1000 | Loss: 0.00001478
Iteration 306/1000 | Loss: 0.00001478
Iteration 307/1000 | Loss: 0.00001478
Iteration 308/1000 | Loss: 0.00001478
Iteration 309/1000 | Loss: 0.00001478
Iteration 310/1000 | Loss: 0.00001478
Iteration 311/1000 | Loss: 0.00001478
Iteration 312/1000 | Loss: 0.00001478
Iteration 313/1000 | Loss: 0.00001478
Iteration 314/1000 | Loss: 0.00001478
Iteration 315/1000 | Loss: 0.00001478
Iteration 316/1000 | Loss: 0.00001478
Iteration 317/1000 | Loss: 0.00001478
Iteration 318/1000 | Loss: 0.00001478
Iteration 319/1000 | Loss: 0.00001478
Iteration 320/1000 | Loss: 0.00001478
Iteration 321/1000 | Loss: 0.00001478
Iteration 322/1000 | Loss: 0.00001478
Iteration 323/1000 | Loss: 0.00001478
Iteration 324/1000 | Loss: 0.00001478
Iteration 325/1000 | Loss: 0.00001478
Iteration 326/1000 | Loss: 0.00001478
Iteration 327/1000 | Loss: 0.00001478
Iteration 328/1000 | Loss: 0.00001478
Iteration 329/1000 | Loss: 0.00001478
Iteration 330/1000 | Loss: 0.00001478
Iteration 331/1000 | Loss: 0.00001478
Iteration 332/1000 | Loss: 0.00001478
Iteration 333/1000 | Loss: 0.00001478
Iteration 334/1000 | Loss: 0.00001478
Iteration 335/1000 | Loss: 0.00001478
Iteration 336/1000 | Loss: 0.00001478
Iteration 337/1000 | Loss: 0.00001478
Iteration 338/1000 | Loss: 0.00001478
Iteration 339/1000 | Loss: 0.00001478
Iteration 340/1000 | Loss: 0.00001478
Iteration 341/1000 | Loss: 0.00001478
Iteration 342/1000 | Loss: 0.00001478
Iteration 343/1000 | Loss: 0.00001478
Iteration 344/1000 | Loss: 0.00001478
Iteration 345/1000 | Loss: 0.00001478
Iteration 346/1000 | Loss: 0.00001478
Iteration 347/1000 | Loss: 0.00001478
Iteration 348/1000 | Loss: 0.00001478
Iteration 349/1000 | Loss: 0.00001478
Iteration 350/1000 | Loss: 0.00001478
Iteration 351/1000 | Loss: 0.00001478
Iteration 352/1000 | Loss: 0.00001478
Iteration 353/1000 | Loss: 0.00001478
Iteration 354/1000 | Loss: 0.00001478
Iteration 355/1000 | Loss: 0.00001478
Iteration 356/1000 | Loss: 0.00001478
Iteration 357/1000 | Loss: 0.00001478
Iteration 358/1000 | Loss: 0.00001478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 358. Stopping optimization.
Last 5 losses: [1.4776264833926689e-05, 1.4776264833926689e-05, 1.4776264833926689e-05, 1.4776264833926689e-05, 1.4776264833926689e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4776264833926689e-05

Optimization complete. Final v2v error: 3.2811801433563232 mm

Highest mean error: 3.400672674179077 mm for frame 66

Lowest mean error: 3.1370625495910645 mm for frame 141

Saving results

Total time: 50.31131434440613
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991586
Iteration 2/25 | Loss: 0.00250867
Iteration 3/25 | Loss: 0.00215185
Iteration 4/25 | Loss: 0.00198952
Iteration 5/25 | Loss: 0.00180121
Iteration 6/25 | Loss: 0.00187568
Iteration 7/25 | Loss: 0.00151468
Iteration 8/25 | Loss: 0.00142318
Iteration 9/25 | Loss: 0.00140916
Iteration 10/25 | Loss: 0.00140669
Iteration 11/25 | Loss: 0.00140500
Iteration 12/25 | Loss: 0.00140405
Iteration 13/25 | Loss: 0.00140382
Iteration 14/25 | Loss: 0.00140378
Iteration 15/25 | Loss: 0.00140378
Iteration 16/25 | Loss: 0.00140378
Iteration 17/25 | Loss: 0.00140378
Iteration 18/25 | Loss: 0.00140378
Iteration 19/25 | Loss: 0.00140378
Iteration 20/25 | Loss: 0.00140378
Iteration 21/25 | Loss: 0.00140378
Iteration 22/25 | Loss: 0.00140378
Iteration 23/25 | Loss: 0.00140378
Iteration 24/25 | Loss: 0.00140378
Iteration 25/25 | Loss: 0.00140378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41066635
Iteration 2/25 | Loss: 0.00072555
Iteration 3/25 | Loss: 0.00072554
Iteration 4/25 | Loss: 0.00072554
Iteration 5/25 | Loss: 0.00072554
Iteration 6/25 | Loss: 0.00072554
Iteration 7/25 | Loss: 0.00072554
Iteration 8/25 | Loss: 0.00072554
Iteration 9/25 | Loss: 0.00072554
Iteration 10/25 | Loss: 0.00072554
Iteration 11/25 | Loss: 0.00072554
Iteration 12/25 | Loss: 0.00072554
Iteration 13/25 | Loss: 0.00072553
Iteration 14/25 | Loss: 0.00072553
Iteration 15/25 | Loss: 0.00072553
Iteration 16/25 | Loss: 0.00072553
Iteration 17/25 | Loss: 0.00072553
Iteration 18/25 | Loss: 0.00072553
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007255349773913622, 0.0007255349773913622, 0.0007255349773913622, 0.0007255349773913622, 0.0007255349773913622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007255349773913622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072553
Iteration 2/1000 | Loss: 0.00004622
Iteration 3/1000 | Loss: 0.00003430
Iteration 4/1000 | Loss: 0.00003252
Iteration 5/1000 | Loss: 0.00003108
Iteration 6/1000 | Loss: 0.00003026
Iteration 7/1000 | Loss: 0.00002980
Iteration 8/1000 | Loss: 0.00002948
Iteration 9/1000 | Loss: 0.00002922
Iteration 10/1000 | Loss: 0.00002906
Iteration 11/1000 | Loss: 0.00002904
Iteration 12/1000 | Loss: 0.00002903
Iteration 13/1000 | Loss: 0.00002900
Iteration 14/1000 | Loss: 0.00002895
Iteration 15/1000 | Loss: 0.00002894
Iteration 16/1000 | Loss: 0.00002879
Iteration 17/1000 | Loss: 0.00002869
Iteration 18/1000 | Loss: 0.00002868
Iteration 19/1000 | Loss: 0.00002867
Iteration 20/1000 | Loss: 0.00002865
Iteration 21/1000 | Loss: 0.00002865
Iteration 22/1000 | Loss: 0.00002864
Iteration 23/1000 | Loss: 0.00002859
Iteration 24/1000 | Loss: 0.00002858
Iteration 25/1000 | Loss: 0.00002855
Iteration 26/1000 | Loss: 0.00002855
Iteration 27/1000 | Loss: 0.00002855
Iteration 28/1000 | Loss: 0.00002855
Iteration 29/1000 | Loss: 0.00002854
Iteration 30/1000 | Loss: 0.00002852
Iteration 31/1000 | Loss: 0.00002852
Iteration 32/1000 | Loss: 0.00002852
Iteration 33/1000 | Loss: 0.00002852
Iteration 34/1000 | Loss: 0.00002852
Iteration 35/1000 | Loss: 0.00002852
Iteration 36/1000 | Loss: 0.00002852
Iteration 37/1000 | Loss: 0.00002852
Iteration 38/1000 | Loss: 0.00002849
Iteration 39/1000 | Loss: 0.00002848
Iteration 40/1000 | Loss: 0.00002847
Iteration 41/1000 | Loss: 0.00002847
Iteration 42/1000 | Loss: 0.00002846
Iteration 43/1000 | Loss: 0.00002846
Iteration 44/1000 | Loss: 0.00002846
Iteration 45/1000 | Loss: 0.00002846
Iteration 46/1000 | Loss: 0.00002846
Iteration 47/1000 | Loss: 0.00002846
Iteration 48/1000 | Loss: 0.00002846
Iteration 49/1000 | Loss: 0.00002845
Iteration 50/1000 | Loss: 0.00002845
Iteration 51/1000 | Loss: 0.00002845
Iteration 52/1000 | Loss: 0.00002845
Iteration 53/1000 | Loss: 0.00002845
Iteration 54/1000 | Loss: 0.00002845
Iteration 55/1000 | Loss: 0.00002845
Iteration 56/1000 | Loss: 0.00002845
Iteration 57/1000 | Loss: 0.00002845
Iteration 58/1000 | Loss: 0.00002845
Iteration 59/1000 | Loss: 0.00002845
Iteration 60/1000 | Loss: 0.00002844
Iteration 61/1000 | Loss: 0.00002844
Iteration 62/1000 | Loss: 0.00002844
Iteration 63/1000 | Loss: 0.00002844
Iteration 64/1000 | Loss: 0.00002844
Iteration 65/1000 | Loss: 0.00002844
Iteration 66/1000 | Loss: 0.00002844
Iteration 67/1000 | Loss: 0.00002843
Iteration 68/1000 | Loss: 0.00002843
Iteration 69/1000 | Loss: 0.00002843
Iteration 70/1000 | Loss: 0.00002843
Iteration 71/1000 | Loss: 0.00002843
Iteration 72/1000 | Loss: 0.00002843
Iteration 73/1000 | Loss: 0.00002843
Iteration 74/1000 | Loss: 0.00002843
Iteration 75/1000 | Loss: 0.00002842
Iteration 76/1000 | Loss: 0.00002842
Iteration 77/1000 | Loss: 0.00002842
Iteration 78/1000 | Loss: 0.00002842
Iteration 79/1000 | Loss: 0.00002842
Iteration 80/1000 | Loss: 0.00002842
Iteration 81/1000 | Loss: 0.00002842
Iteration 82/1000 | Loss: 0.00002841
Iteration 83/1000 | Loss: 0.00002841
Iteration 84/1000 | Loss: 0.00002841
Iteration 85/1000 | Loss: 0.00002841
Iteration 86/1000 | Loss: 0.00002841
Iteration 87/1000 | Loss: 0.00002840
Iteration 88/1000 | Loss: 0.00002840
Iteration 89/1000 | Loss: 0.00002840
Iteration 90/1000 | Loss: 0.00002840
Iteration 91/1000 | Loss: 0.00002840
Iteration 92/1000 | Loss: 0.00002840
Iteration 93/1000 | Loss: 0.00002840
Iteration 94/1000 | Loss: 0.00002840
Iteration 95/1000 | Loss: 0.00002840
Iteration 96/1000 | Loss: 0.00002839
Iteration 97/1000 | Loss: 0.00002839
Iteration 98/1000 | Loss: 0.00002839
Iteration 99/1000 | Loss: 0.00002839
Iteration 100/1000 | Loss: 0.00002839
Iteration 101/1000 | Loss: 0.00002839
Iteration 102/1000 | Loss: 0.00002839
Iteration 103/1000 | Loss: 0.00002839
Iteration 104/1000 | Loss: 0.00002839
Iteration 105/1000 | Loss: 0.00002839
Iteration 106/1000 | Loss: 0.00002839
Iteration 107/1000 | Loss: 0.00002839
Iteration 108/1000 | Loss: 0.00002839
Iteration 109/1000 | Loss: 0.00002838
Iteration 110/1000 | Loss: 0.00002838
Iteration 111/1000 | Loss: 0.00002838
Iteration 112/1000 | Loss: 0.00002838
Iteration 113/1000 | Loss: 0.00002838
Iteration 114/1000 | Loss: 0.00002837
Iteration 115/1000 | Loss: 0.00002837
Iteration 116/1000 | Loss: 0.00002837
Iteration 117/1000 | Loss: 0.00002836
Iteration 118/1000 | Loss: 0.00002836
Iteration 119/1000 | Loss: 0.00002836
Iteration 120/1000 | Loss: 0.00002836
Iteration 121/1000 | Loss: 0.00002836
Iteration 122/1000 | Loss: 0.00002835
Iteration 123/1000 | Loss: 0.00002835
Iteration 124/1000 | Loss: 0.00002835
Iteration 125/1000 | Loss: 0.00002835
Iteration 126/1000 | Loss: 0.00002835
Iteration 127/1000 | Loss: 0.00002835
Iteration 128/1000 | Loss: 0.00002835
Iteration 129/1000 | Loss: 0.00002835
Iteration 130/1000 | Loss: 0.00002835
Iteration 131/1000 | Loss: 0.00002835
Iteration 132/1000 | Loss: 0.00002835
Iteration 133/1000 | Loss: 0.00002835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [2.835006125678774e-05, 2.835006125678774e-05, 2.835006125678774e-05, 2.835006125678774e-05, 2.835006125678774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.835006125678774e-05

Optimization complete. Final v2v error: 4.581827163696289 mm

Highest mean error: 4.702529430389404 mm for frame 40

Lowest mean error: 4.472738265991211 mm for frame 183

Saving results

Total time: 56.032248973846436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039032
Iteration 2/25 | Loss: 0.00172223
Iteration 3/25 | Loss: 0.00147184
Iteration 4/25 | Loss: 0.00144955
Iteration 5/25 | Loss: 0.00144393
Iteration 6/25 | Loss: 0.00144282
Iteration 7/25 | Loss: 0.00144274
Iteration 8/25 | Loss: 0.00144274
Iteration 9/25 | Loss: 0.00144274
Iteration 10/25 | Loss: 0.00144274
Iteration 11/25 | Loss: 0.00144274
Iteration 12/25 | Loss: 0.00144274
Iteration 13/25 | Loss: 0.00144274
Iteration 14/25 | Loss: 0.00144274
Iteration 15/25 | Loss: 0.00144274
Iteration 16/25 | Loss: 0.00144274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014427400892600417, 0.0014427400892600417, 0.0014427400892600417, 0.0014427400892600417, 0.0014427400892600417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014427400892600417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.51644814
Iteration 2/25 | Loss: 0.00097104
Iteration 3/25 | Loss: 0.00097102
Iteration 4/25 | Loss: 0.00097101
Iteration 5/25 | Loss: 0.00097101
Iteration 6/25 | Loss: 0.00097101
Iteration 7/25 | Loss: 0.00097101
Iteration 8/25 | Loss: 0.00097101
Iteration 9/25 | Loss: 0.00097101
Iteration 10/25 | Loss: 0.00097101
Iteration 11/25 | Loss: 0.00097101
Iteration 12/25 | Loss: 0.00097101
Iteration 13/25 | Loss: 0.00097101
Iteration 14/25 | Loss: 0.00097101
Iteration 15/25 | Loss: 0.00097101
Iteration 16/25 | Loss: 0.00097101
Iteration 17/25 | Loss: 0.00097101
Iteration 18/25 | Loss: 0.00097101
Iteration 19/25 | Loss: 0.00097101
Iteration 20/25 | Loss: 0.00097101
Iteration 21/25 | Loss: 0.00097101
Iteration 22/25 | Loss: 0.00097101
Iteration 23/25 | Loss: 0.00097101
Iteration 24/25 | Loss: 0.00097101
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009710127487778664, 0.0009710127487778664, 0.0009710127487778664, 0.0009710127487778664, 0.0009710127487778664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009710127487778664

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097101
Iteration 2/1000 | Loss: 0.00007954
Iteration 3/1000 | Loss: 0.00004637
Iteration 4/1000 | Loss: 0.00003968
Iteration 5/1000 | Loss: 0.00003807
Iteration 6/1000 | Loss: 0.00003665
Iteration 7/1000 | Loss: 0.00003564
Iteration 8/1000 | Loss: 0.00003493
Iteration 9/1000 | Loss: 0.00003450
Iteration 10/1000 | Loss: 0.00003413
Iteration 11/1000 | Loss: 0.00003380
Iteration 12/1000 | Loss: 0.00003352
Iteration 13/1000 | Loss: 0.00003345
Iteration 14/1000 | Loss: 0.00003342
Iteration 15/1000 | Loss: 0.00003329
Iteration 16/1000 | Loss: 0.00003329
Iteration 17/1000 | Loss: 0.00003324
Iteration 18/1000 | Loss: 0.00003319
Iteration 19/1000 | Loss: 0.00003311
Iteration 20/1000 | Loss: 0.00003310
Iteration 21/1000 | Loss: 0.00003309
Iteration 22/1000 | Loss: 0.00003304
Iteration 23/1000 | Loss: 0.00003303
Iteration 24/1000 | Loss: 0.00003303
Iteration 25/1000 | Loss: 0.00003303
Iteration 26/1000 | Loss: 0.00003302
Iteration 27/1000 | Loss: 0.00003301
Iteration 28/1000 | Loss: 0.00003301
Iteration 29/1000 | Loss: 0.00003301
Iteration 30/1000 | Loss: 0.00003300
Iteration 31/1000 | Loss: 0.00003300
Iteration 32/1000 | Loss: 0.00003300
Iteration 33/1000 | Loss: 0.00003299
Iteration 34/1000 | Loss: 0.00003299
Iteration 35/1000 | Loss: 0.00003298
Iteration 36/1000 | Loss: 0.00003297
Iteration 37/1000 | Loss: 0.00003297
Iteration 38/1000 | Loss: 0.00003297
Iteration 39/1000 | Loss: 0.00003296
Iteration 40/1000 | Loss: 0.00003296
Iteration 41/1000 | Loss: 0.00003296
Iteration 42/1000 | Loss: 0.00003296
Iteration 43/1000 | Loss: 0.00003296
Iteration 44/1000 | Loss: 0.00003296
Iteration 45/1000 | Loss: 0.00003295
Iteration 46/1000 | Loss: 0.00003295
Iteration 47/1000 | Loss: 0.00003295
Iteration 48/1000 | Loss: 0.00003295
Iteration 49/1000 | Loss: 0.00003295
Iteration 50/1000 | Loss: 0.00003295
Iteration 51/1000 | Loss: 0.00003295
Iteration 52/1000 | Loss: 0.00003295
Iteration 53/1000 | Loss: 0.00003294
Iteration 54/1000 | Loss: 0.00003294
Iteration 55/1000 | Loss: 0.00003294
Iteration 56/1000 | Loss: 0.00003294
Iteration 57/1000 | Loss: 0.00003294
Iteration 58/1000 | Loss: 0.00003294
Iteration 59/1000 | Loss: 0.00003294
Iteration 60/1000 | Loss: 0.00003294
Iteration 61/1000 | Loss: 0.00003293
Iteration 62/1000 | Loss: 0.00003293
Iteration 63/1000 | Loss: 0.00003293
Iteration 64/1000 | Loss: 0.00003293
Iteration 65/1000 | Loss: 0.00003293
Iteration 66/1000 | Loss: 0.00003293
Iteration 67/1000 | Loss: 0.00003292
Iteration 68/1000 | Loss: 0.00003292
Iteration 69/1000 | Loss: 0.00003292
Iteration 70/1000 | Loss: 0.00003291
Iteration 71/1000 | Loss: 0.00003291
Iteration 72/1000 | Loss: 0.00003291
Iteration 73/1000 | Loss: 0.00003291
Iteration 74/1000 | Loss: 0.00003291
Iteration 75/1000 | Loss: 0.00003291
Iteration 76/1000 | Loss: 0.00003291
Iteration 77/1000 | Loss: 0.00003291
Iteration 78/1000 | Loss: 0.00003291
Iteration 79/1000 | Loss: 0.00003291
Iteration 80/1000 | Loss: 0.00003290
Iteration 81/1000 | Loss: 0.00003290
Iteration 82/1000 | Loss: 0.00003290
Iteration 83/1000 | Loss: 0.00003290
Iteration 84/1000 | Loss: 0.00003290
Iteration 85/1000 | Loss: 0.00003289
Iteration 86/1000 | Loss: 0.00003289
Iteration 87/1000 | Loss: 0.00003289
Iteration 88/1000 | Loss: 0.00003289
Iteration 89/1000 | Loss: 0.00003288
Iteration 90/1000 | Loss: 0.00003288
Iteration 91/1000 | Loss: 0.00003288
Iteration 92/1000 | Loss: 0.00003288
Iteration 93/1000 | Loss: 0.00003288
Iteration 94/1000 | Loss: 0.00003288
Iteration 95/1000 | Loss: 0.00003288
Iteration 96/1000 | Loss: 0.00003288
Iteration 97/1000 | Loss: 0.00003288
Iteration 98/1000 | Loss: 0.00003288
Iteration 99/1000 | Loss: 0.00003288
Iteration 100/1000 | Loss: 0.00003287
Iteration 101/1000 | Loss: 0.00003287
Iteration 102/1000 | Loss: 0.00003287
Iteration 103/1000 | Loss: 0.00003287
Iteration 104/1000 | Loss: 0.00003287
Iteration 105/1000 | Loss: 0.00003287
Iteration 106/1000 | Loss: 0.00003287
Iteration 107/1000 | Loss: 0.00003287
Iteration 108/1000 | Loss: 0.00003287
Iteration 109/1000 | Loss: 0.00003286
Iteration 110/1000 | Loss: 0.00003286
Iteration 111/1000 | Loss: 0.00003286
Iteration 112/1000 | Loss: 0.00003286
Iteration 113/1000 | Loss: 0.00003286
Iteration 114/1000 | Loss: 0.00003286
Iteration 115/1000 | Loss: 0.00003286
Iteration 116/1000 | Loss: 0.00003286
Iteration 117/1000 | Loss: 0.00003286
Iteration 118/1000 | Loss: 0.00003285
Iteration 119/1000 | Loss: 0.00003285
Iteration 120/1000 | Loss: 0.00003285
Iteration 121/1000 | Loss: 0.00003285
Iteration 122/1000 | Loss: 0.00003285
Iteration 123/1000 | Loss: 0.00003285
Iteration 124/1000 | Loss: 0.00003285
Iteration 125/1000 | Loss: 0.00003285
Iteration 126/1000 | Loss: 0.00003285
Iteration 127/1000 | Loss: 0.00003285
Iteration 128/1000 | Loss: 0.00003285
Iteration 129/1000 | Loss: 0.00003285
Iteration 130/1000 | Loss: 0.00003285
Iteration 131/1000 | Loss: 0.00003284
Iteration 132/1000 | Loss: 0.00003284
Iteration 133/1000 | Loss: 0.00003284
Iteration 134/1000 | Loss: 0.00003284
Iteration 135/1000 | Loss: 0.00003284
Iteration 136/1000 | Loss: 0.00003284
Iteration 137/1000 | Loss: 0.00003284
Iteration 138/1000 | Loss: 0.00003284
Iteration 139/1000 | Loss: 0.00003284
Iteration 140/1000 | Loss: 0.00003284
Iteration 141/1000 | Loss: 0.00003284
Iteration 142/1000 | Loss: 0.00003284
Iteration 143/1000 | Loss: 0.00003283
Iteration 144/1000 | Loss: 0.00003283
Iteration 145/1000 | Loss: 0.00003283
Iteration 146/1000 | Loss: 0.00003283
Iteration 147/1000 | Loss: 0.00003283
Iteration 148/1000 | Loss: 0.00003283
Iteration 149/1000 | Loss: 0.00003283
Iteration 150/1000 | Loss: 0.00003283
Iteration 151/1000 | Loss: 0.00003283
Iteration 152/1000 | Loss: 0.00003283
Iteration 153/1000 | Loss: 0.00003282
Iteration 154/1000 | Loss: 0.00003282
Iteration 155/1000 | Loss: 0.00003282
Iteration 156/1000 | Loss: 0.00003282
Iteration 157/1000 | Loss: 0.00003281
Iteration 158/1000 | Loss: 0.00003281
Iteration 159/1000 | Loss: 0.00003281
Iteration 160/1000 | Loss: 0.00003281
Iteration 161/1000 | Loss: 0.00003281
Iteration 162/1000 | Loss: 0.00003281
Iteration 163/1000 | Loss: 0.00003280
Iteration 164/1000 | Loss: 0.00003280
Iteration 165/1000 | Loss: 0.00003280
Iteration 166/1000 | Loss: 0.00003280
Iteration 167/1000 | Loss: 0.00003280
Iteration 168/1000 | Loss: 0.00003280
Iteration 169/1000 | Loss: 0.00003280
Iteration 170/1000 | Loss: 0.00003280
Iteration 171/1000 | Loss: 0.00003280
Iteration 172/1000 | Loss: 0.00003280
Iteration 173/1000 | Loss: 0.00003279
Iteration 174/1000 | Loss: 0.00003279
Iteration 175/1000 | Loss: 0.00003279
Iteration 176/1000 | Loss: 0.00003279
Iteration 177/1000 | Loss: 0.00003279
Iteration 178/1000 | Loss: 0.00003279
Iteration 179/1000 | Loss: 0.00003279
Iteration 180/1000 | Loss: 0.00003279
Iteration 181/1000 | Loss: 0.00003279
Iteration 182/1000 | Loss: 0.00003279
Iteration 183/1000 | Loss: 0.00003279
Iteration 184/1000 | Loss: 0.00003279
Iteration 185/1000 | Loss: 0.00003279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [3.279109296272509e-05, 3.279109296272509e-05, 3.279109296272509e-05, 3.279109296272509e-05, 3.279109296272509e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.279109296272509e-05

Optimization complete. Final v2v error: 4.758383274078369 mm

Highest mean error: 5.643789291381836 mm for frame 194

Lowest mean error: 4.2430009841918945 mm for frame 100

Saving results

Total time: 47.50398302078247
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805238
Iteration 2/25 | Loss: 0.00132887
Iteration 3/25 | Loss: 0.00125834
Iteration 4/25 | Loss: 0.00123240
Iteration 5/25 | Loss: 0.00122793
Iteration 6/25 | Loss: 0.00122755
Iteration 7/25 | Loss: 0.00122755
Iteration 8/25 | Loss: 0.00122755
Iteration 9/25 | Loss: 0.00122755
Iteration 10/25 | Loss: 0.00122755
Iteration 11/25 | Loss: 0.00122755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012275532353669405, 0.0012275532353669405, 0.0012275532353669405, 0.0012275532353669405, 0.0012275532353669405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012275532353669405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.44963932
Iteration 2/25 | Loss: 0.00096397
Iteration 3/25 | Loss: 0.00096392
Iteration 4/25 | Loss: 0.00096392
Iteration 5/25 | Loss: 0.00096392
Iteration 6/25 | Loss: 0.00096392
Iteration 7/25 | Loss: 0.00096392
Iteration 8/25 | Loss: 0.00096392
Iteration 9/25 | Loss: 0.00096392
Iteration 10/25 | Loss: 0.00096392
Iteration 11/25 | Loss: 0.00096392
Iteration 12/25 | Loss: 0.00096392
Iteration 13/25 | Loss: 0.00096392
Iteration 14/25 | Loss: 0.00096392
Iteration 15/25 | Loss: 0.00096392
Iteration 16/25 | Loss: 0.00096392
Iteration 17/25 | Loss: 0.00096392
Iteration 18/25 | Loss: 0.00096392
Iteration 19/25 | Loss: 0.00096392
Iteration 20/25 | Loss: 0.00096392
Iteration 21/25 | Loss: 0.00096392
Iteration 22/25 | Loss: 0.00096392
Iteration 23/25 | Loss: 0.00096392
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009639157797209918, 0.0009639157797209918, 0.0009639157797209918, 0.0009639157797209918, 0.0009639157797209918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009639157797209918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096392
Iteration 2/1000 | Loss: 0.00003209
Iteration 3/1000 | Loss: 0.00002200
Iteration 4/1000 | Loss: 0.00001999
Iteration 5/1000 | Loss: 0.00001863
Iteration 6/1000 | Loss: 0.00001763
Iteration 7/1000 | Loss: 0.00001710
Iteration 8/1000 | Loss: 0.00001661
Iteration 9/1000 | Loss: 0.00001621
Iteration 10/1000 | Loss: 0.00001583
Iteration 11/1000 | Loss: 0.00001581
Iteration 12/1000 | Loss: 0.00001559
Iteration 13/1000 | Loss: 0.00001539
Iteration 14/1000 | Loss: 0.00001539
Iteration 15/1000 | Loss: 0.00001538
Iteration 16/1000 | Loss: 0.00001537
Iteration 17/1000 | Loss: 0.00001531
Iteration 18/1000 | Loss: 0.00001527
Iteration 19/1000 | Loss: 0.00001526
Iteration 20/1000 | Loss: 0.00001526
Iteration 21/1000 | Loss: 0.00001526
Iteration 22/1000 | Loss: 0.00001525
Iteration 23/1000 | Loss: 0.00001525
Iteration 24/1000 | Loss: 0.00001524
Iteration 25/1000 | Loss: 0.00001524
Iteration 26/1000 | Loss: 0.00001523
Iteration 27/1000 | Loss: 0.00001523
Iteration 28/1000 | Loss: 0.00001522
Iteration 29/1000 | Loss: 0.00001521
Iteration 30/1000 | Loss: 0.00001521
Iteration 31/1000 | Loss: 0.00001521
Iteration 32/1000 | Loss: 0.00001521
Iteration 33/1000 | Loss: 0.00001520
Iteration 34/1000 | Loss: 0.00001520
Iteration 35/1000 | Loss: 0.00001519
Iteration 36/1000 | Loss: 0.00001518
Iteration 37/1000 | Loss: 0.00001517
Iteration 38/1000 | Loss: 0.00001517
Iteration 39/1000 | Loss: 0.00001517
Iteration 40/1000 | Loss: 0.00001516
Iteration 41/1000 | Loss: 0.00001516
Iteration 42/1000 | Loss: 0.00001514
Iteration 43/1000 | Loss: 0.00001513
Iteration 44/1000 | Loss: 0.00001512
Iteration 45/1000 | Loss: 0.00001512
Iteration 46/1000 | Loss: 0.00001512
Iteration 47/1000 | Loss: 0.00001512
Iteration 48/1000 | Loss: 0.00001512
Iteration 49/1000 | Loss: 0.00001512
Iteration 50/1000 | Loss: 0.00001512
Iteration 51/1000 | Loss: 0.00001512
Iteration 52/1000 | Loss: 0.00001512
Iteration 53/1000 | Loss: 0.00001512
Iteration 54/1000 | Loss: 0.00001511
Iteration 55/1000 | Loss: 0.00001510
Iteration 56/1000 | Loss: 0.00001508
Iteration 57/1000 | Loss: 0.00001508
Iteration 58/1000 | Loss: 0.00001508
Iteration 59/1000 | Loss: 0.00001507
Iteration 60/1000 | Loss: 0.00001507
Iteration 61/1000 | Loss: 0.00001507
Iteration 62/1000 | Loss: 0.00001506
Iteration 63/1000 | Loss: 0.00001505
Iteration 64/1000 | Loss: 0.00001505
Iteration 65/1000 | Loss: 0.00001504
Iteration 66/1000 | Loss: 0.00001504
Iteration 67/1000 | Loss: 0.00001504
Iteration 68/1000 | Loss: 0.00001504
Iteration 69/1000 | Loss: 0.00001504
Iteration 70/1000 | Loss: 0.00001504
Iteration 71/1000 | Loss: 0.00001504
Iteration 72/1000 | Loss: 0.00001504
Iteration 73/1000 | Loss: 0.00001504
Iteration 74/1000 | Loss: 0.00001504
Iteration 75/1000 | Loss: 0.00001504
Iteration 76/1000 | Loss: 0.00001503
Iteration 77/1000 | Loss: 0.00001503
Iteration 78/1000 | Loss: 0.00001503
Iteration 79/1000 | Loss: 0.00001503
Iteration 80/1000 | Loss: 0.00001503
Iteration 81/1000 | Loss: 0.00001502
Iteration 82/1000 | Loss: 0.00001502
Iteration 83/1000 | Loss: 0.00001502
Iteration 84/1000 | Loss: 0.00001502
Iteration 85/1000 | Loss: 0.00001502
Iteration 86/1000 | Loss: 0.00001502
Iteration 87/1000 | Loss: 0.00001502
Iteration 88/1000 | Loss: 0.00001502
Iteration 89/1000 | Loss: 0.00001502
Iteration 90/1000 | Loss: 0.00001502
Iteration 91/1000 | Loss: 0.00001502
Iteration 92/1000 | Loss: 0.00001502
Iteration 93/1000 | Loss: 0.00001502
Iteration 94/1000 | Loss: 0.00001502
Iteration 95/1000 | Loss: 0.00001502
Iteration 96/1000 | Loss: 0.00001502
Iteration 97/1000 | Loss: 0.00001501
Iteration 98/1000 | Loss: 0.00001501
Iteration 99/1000 | Loss: 0.00001501
Iteration 100/1000 | Loss: 0.00001501
Iteration 101/1000 | Loss: 0.00001501
Iteration 102/1000 | Loss: 0.00001501
Iteration 103/1000 | Loss: 0.00001501
Iteration 104/1000 | Loss: 0.00001501
Iteration 105/1000 | Loss: 0.00001501
Iteration 106/1000 | Loss: 0.00001501
Iteration 107/1000 | Loss: 0.00001501
Iteration 108/1000 | Loss: 0.00001501
Iteration 109/1000 | Loss: 0.00001500
Iteration 110/1000 | Loss: 0.00001500
Iteration 111/1000 | Loss: 0.00001500
Iteration 112/1000 | Loss: 0.00001500
Iteration 113/1000 | Loss: 0.00001500
Iteration 114/1000 | Loss: 0.00001500
Iteration 115/1000 | Loss: 0.00001500
Iteration 116/1000 | Loss: 0.00001500
Iteration 117/1000 | Loss: 0.00001500
Iteration 118/1000 | Loss: 0.00001500
Iteration 119/1000 | Loss: 0.00001500
Iteration 120/1000 | Loss: 0.00001500
Iteration 121/1000 | Loss: 0.00001500
Iteration 122/1000 | Loss: 0.00001500
Iteration 123/1000 | Loss: 0.00001500
Iteration 124/1000 | Loss: 0.00001500
Iteration 125/1000 | Loss: 0.00001500
Iteration 126/1000 | Loss: 0.00001500
Iteration 127/1000 | Loss: 0.00001500
Iteration 128/1000 | Loss: 0.00001500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.4997144717199262e-05, 1.4997144717199262e-05, 1.4997144717199262e-05, 1.4997144717199262e-05, 1.4997144717199262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4997144717199262e-05

Optimization complete. Final v2v error: 3.3031187057495117 mm

Highest mean error: 3.7551474571228027 mm for frame 56

Lowest mean error: 2.962202548980713 mm for frame 223

Saving results

Total time: 39.69961738586426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053329
Iteration 2/25 | Loss: 0.01053329
Iteration 3/25 | Loss: 0.00558932
Iteration 4/25 | Loss: 0.00379038
Iteration 5/25 | Loss: 0.00310687
Iteration 6/25 | Loss: 0.00261886
Iteration 7/25 | Loss: 0.00285783
Iteration 8/25 | Loss: 0.00229626
Iteration 9/25 | Loss: 0.00218135
Iteration 10/25 | Loss: 0.00195238
Iteration 11/25 | Loss: 0.00181150
Iteration 12/25 | Loss: 0.00177277
Iteration 13/25 | Loss: 0.00169392
Iteration 14/25 | Loss: 0.00166233
Iteration 15/25 | Loss: 0.00162823
Iteration 16/25 | Loss: 0.00161185
Iteration 17/25 | Loss: 0.00158388
Iteration 18/25 | Loss: 0.00154980
Iteration 19/25 | Loss: 0.00156729
Iteration 20/25 | Loss: 0.00152534
Iteration 21/25 | Loss: 0.00152637
Iteration 22/25 | Loss: 0.00152535
Iteration 23/25 | Loss: 0.00150781
Iteration 24/25 | Loss: 0.00151144
Iteration 25/25 | Loss: 0.00150248

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.60511750
Iteration 2/25 | Loss: 0.00161869
Iteration 3/25 | Loss: 0.00114367
Iteration 4/25 | Loss: 0.00114367
Iteration 5/25 | Loss: 0.00114367
Iteration 6/25 | Loss: 0.00114367
Iteration 7/25 | Loss: 0.00114367
Iteration 8/25 | Loss: 0.00114367
Iteration 9/25 | Loss: 0.00114367
Iteration 10/25 | Loss: 0.00116338
Iteration 11/25 | Loss: 0.00114367
Iteration 12/25 | Loss: 0.00114367
Iteration 13/25 | Loss: 0.00114367
Iteration 14/25 | Loss: 0.00114367
Iteration 15/25 | Loss: 0.00114367
Iteration 16/25 | Loss: 0.00114367
Iteration 17/25 | Loss: 0.00114367
Iteration 18/25 | Loss: 0.00114367
Iteration 19/25 | Loss: 0.00114367
Iteration 20/25 | Loss: 0.00114367
Iteration 21/25 | Loss: 0.00114367
Iteration 22/25 | Loss: 0.00114367
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011436680797487497, 0.0011436680797487497, 0.0011436680797487497, 0.0011436680797487497, 0.0011436680797487497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011436680797487497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114367
Iteration 2/1000 | Loss: 0.00097162
Iteration 3/1000 | Loss: 0.00055118
Iteration 4/1000 | Loss: 0.00093793
Iteration 5/1000 | Loss: 0.00059482
Iteration 6/1000 | Loss: 0.00053993
Iteration 7/1000 | Loss: 0.00017132
Iteration 8/1000 | Loss: 0.00009987
Iteration 9/1000 | Loss: 0.00027488
Iteration 10/1000 | Loss: 0.00005214
Iteration 11/1000 | Loss: 0.00006959
Iteration 12/1000 | Loss: 0.00029709
Iteration 13/1000 | Loss: 0.00031898
Iteration 14/1000 | Loss: 0.00007052
Iteration 15/1000 | Loss: 0.00010599
Iteration 16/1000 | Loss: 0.00004498
Iteration 17/1000 | Loss: 0.00004186
Iteration 18/1000 | Loss: 0.00005853
Iteration 19/1000 | Loss: 0.00004054
Iteration 20/1000 | Loss: 0.00003882
Iteration 21/1000 | Loss: 0.00032760
Iteration 22/1000 | Loss: 0.00033624
Iteration 23/1000 | Loss: 0.00028358
Iteration 24/1000 | Loss: 0.00014269
Iteration 25/1000 | Loss: 0.00003923
Iteration 26/1000 | Loss: 0.00003725
Iteration 27/1000 | Loss: 0.00007278
Iteration 28/1000 | Loss: 0.00003569
Iteration 29/1000 | Loss: 0.00005859
Iteration 30/1000 | Loss: 0.00003832
Iteration 31/1000 | Loss: 0.00003519
Iteration 32/1000 | Loss: 0.00004407
Iteration 33/1000 | Loss: 0.00004869
Iteration 34/1000 | Loss: 0.00003916
Iteration 35/1000 | Loss: 0.00003285
Iteration 36/1000 | Loss: 0.00003778
Iteration 37/1000 | Loss: 0.00003211
Iteration 38/1000 | Loss: 0.00003174
Iteration 39/1000 | Loss: 0.00003137
Iteration 40/1000 | Loss: 0.00046414
Iteration 41/1000 | Loss: 0.00076034
Iteration 42/1000 | Loss: 0.00135990
Iteration 43/1000 | Loss: 0.00030176
Iteration 44/1000 | Loss: 0.00037429
Iteration 45/1000 | Loss: 0.00128865
Iteration 46/1000 | Loss: 0.00080201
Iteration 47/1000 | Loss: 0.00008954
Iteration 48/1000 | Loss: 0.00004913
Iteration 49/1000 | Loss: 0.00004224
Iteration 50/1000 | Loss: 0.00045426
Iteration 51/1000 | Loss: 0.00019214
Iteration 52/1000 | Loss: 0.00006045
Iteration 53/1000 | Loss: 0.00024609
Iteration 54/1000 | Loss: 0.00005361
Iteration 55/1000 | Loss: 0.00003552
Iteration 56/1000 | Loss: 0.00003384
Iteration 57/1000 | Loss: 0.00003279
Iteration 58/1000 | Loss: 0.00041230
Iteration 59/1000 | Loss: 0.00046057
Iteration 60/1000 | Loss: 0.00043014
Iteration 61/1000 | Loss: 0.00046107
Iteration 62/1000 | Loss: 0.00065021
Iteration 63/1000 | Loss: 0.00036825
Iteration 64/1000 | Loss: 0.00018076
Iteration 65/1000 | Loss: 0.00003495
Iteration 66/1000 | Loss: 0.00003170
Iteration 67/1000 | Loss: 0.00003007
Iteration 68/1000 | Loss: 0.00002948
Iteration 69/1000 | Loss: 0.00002919
Iteration 70/1000 | Loss: 0.00002886
Iteration 71/1000 | Loss: 0.00002866
Iteration 72/1000 | Loss: 0.00002852
Iteration 73/1000 | Loss: 0.00002852
Iteration 74/1000 | Loss: 0.00002851
Iteration 75/1000 | Loss: 0.00002849
Iteration 76/1000 | Loss: 0.00002849
Iteration 77/1000 | Loss: 0.00002849
Iteration 78/1000 | Loss: 0.00002849
Iteration 79/1000 | Loss: 0.00002849
Iteration 80/1000 | Loss: 0.00002849
Iteration 81/1000 | Loss: 0.00002849
Iteration 82/1000 | Loss: 0.00002849
Iteration 83/1000 | Loss: 0.00002848
Iteration 84/1000 | Loss: 0.00002848
Iteration 85/1000 | Loss: 0.00002848
Iteration 86/1000 | Loss: 0.00006456
Iteration 87/1000 | Loss: 0.00003581
Iteration 88/1000 | Loss: 0.00002846
Iteration 89/1000 | Loss: 0.00002846
Iteration 90/1000 | Loss: 0.00002846
Iteration 91/1000 | Loss: 0.00002846
Iteration 92/1000 | Loss: 0.00002846
Iteration 93/1000 | Loss: 0.00002846
Iteration 94/1000 | Loss: 0.00002846
Iteration 95/1000 | Loss: 0.00002845
Iteration 96/1000 | Loss: 0.00002845
Iteration 97/1000 | Loss: 0.00002845
Iteration 98/1000 | Loss: 0.00002845
Iteration 99/1000 | Loss: 0.00002845
Iteration 100/1000 | Loss: 0.00002845
Iteration 101/1000 | Loss: 0.00002845
Iteration 102/1000 | Loss: 0.00002845
Iteration 103/1000 | Loss: 0.00002845
Iteration 104/1000 | Loss: 0.00002845
Iteration 105/1000 | Loss: 0.00002845
Iteration 106/1000 | Loss: 0.00002845
Iteration 107/1000 | Loss: 0.00002845
Iteration 108/1000 | Loss: 0.00002845
Iteration 109/1000 | Loss: 0.00002844
Iteration 110/1000 | Loss: 0.00002844
Iteration 111/1000 | Loss: 0.00002843
Iteration 112/1000 | Loss: 0.00002843
Iteration 113/1000 | Loss: 0.00002843
Iteration 114/1000 | Loss: 0.00002843
Iteration 115/1000 | Loss: 0.00002843
Iteration 116/1000 | Loss: 0.00002843
Iteration 117/1000 | Loss: 0.00002842
Iteration 118/1000 | Loss: 0.00002842
Iteration 119/1000 | Loss: 0.00002842
Iteration 120/1000 | Loss: 0.00002842
Iteration 121/1000 | Loss: 0.00002842
Iteration 122/1000 | Loss: 0.00002841
Iteration 123/1000 | Loss: 0.00002841
Iteration 124/1000 | Loss: 0.00002840
Iteration 125/1000 | Loss: 0.00002840
Iteration 126/1000 | Loss: 0.00002840
Iteration 127/1000 | Loss: 0.00002840
Iteration 128/1000 | Loss: 0.00002840
Iteration 129/1000 | Loss: 0.00002840
Iteration 130/1000 | Loss: 0.00002839
Iteration 131/1000 | Loss: 0.00002839
Iteration 132/1000 | Loss: 0.00002839
Iteration 133/1000 | Loss: 0.00002838
Iteration 134/1000 | Loss: 0.00002838
Iteration 135/1000 | Loss: 0.00002838
Iteration 136/1000 | Loss: 0.00002838
Iteration 137/1000 | Loss: 0.00002838
Iteration 138/1000 | Loss: 0.00002838
Iteration 139/1000 | Loss: 0.00002838
Iteration 140/1000 | Loss: 0.00002838
Iteration 141/1000 | Loss: 0.00002837
Iteration 142/1000 | Loss: 0.00002837
Iteration 143/1000 | Loss: 0.00002837
Iteration 144/1000 | Loss: 0.00002837
Iteration 145/1000 | Loss: 0.00002837
Iteration 146/1000 | Loss: 0.00002837
Iteration 147/1000 | Loss: 0.00002837
Iteration 148/1000 | Loss: 0.00002837
Iteration 149/1000 | Loss: 0.00002837
Iteration 150/1000 | Loss: 0.00002837
Iteration 151/1000 | Loss: 0.00002837
Iteration 152/1000 | Loss: 0.00002837
Iteration 153/1000 | Loss: 0.00002837
Iteration 154/1000 | Loss: 0.00002837
Iteration 155/1000 | Loss: 0.00002837
Iteration 156/1000 | Loss: 0.00002837
Iteration 157/1000 | Loss: 0.00002837
Iteration 158/1000 | Loss: 0.00002837
Iteration 159/1000 | Loss: 0.00002836
Iteration 160/1000 | Loss: 0.00002836
Iteration 161/1000 | Loss: 0.00002836
Iteration 162/1000 | Loss: 0.00002836
Iteration 163/1000 | Loss: 0.00002836
Iteration 164/1000 | Loss: 0.00002836
Iteration 165/1000 | Loss: 0.00002836
Iteration 166/1000 | Loss: 0.00002836
Iteration 167/1000 | Loss: 0.00002836
Iteration 168/1000 | Loss: 0.00002836
Iteration 169/1000 | Loss: 0.00002836
Iteration 170/1000 | Loss: 0.00002836
Iteration 171/1000 | Loss: 0.00002836
Iteration 172/1000 | Loss: 0.00002836
Iteration 173/1000 | Loss: 0.00002836
Iteration 174/1000 | Loss: 0.00002835
Iteration 175/1000 | Loss: 0.00002835
Iteration 176/1000 | Loss: 0.00002835
Iteration 177/1000 | Loss: 0.00002835
Iteration 178/1000 | Loss: 0.00002835
Iteration 179/1000 | Loss: 0.00002835
Iteration 180/1000 | Loss: 0.00002835
Iteration 181/1000 | Loss: 0.00002835
Iteration 182/1000 | Loss: 0.00002835
Iteration 183/1000 | Loss: 0.00002835
Iteration 184/1000 | Loss: 0.00002835
Iteration 185/1000 | Loss: 0.00002835
Iteration 186/1000 | Loss: 0.00002835
Iteration 187/1000 | Loss: 0.00002835
Iteration 188/1000 | Loss: 0.00002835
Iteration 189/1000 | Loss: 0.00002834
Iteration 190/1000 | Loss: 0.00002834
Iteration 191/1000 | Loss: 0.00002834
Iteration 192/1000 | Loss: 0.00002834
Iteration 193/1000 | Loss: 0.00002834
Iteration 194/1000 | Loss: 0.00002834
Iteration 195/1000 | Loss: 0.00002834
Iteration 196/1000 | Loss: 0.00002834
Iteration 197/1000 | Loss: 0.00002834
Iteration 198/1000 | Loss: 0.00002833
Iteration 199/1000 | Loss: 0.00002833
Iteration 200/1000 | Loss: 0.00002833
Iteration 201/1000 | Loss: 0.00002833
Iteration 202/1000 | Loss: 0.00002833
Iteration 203/1000 | Loss: 0.00002833
Iteration 204/1000 | Loss: 0.00002832
Iteration 205/1000 | Loss: 0.00002832
Iteration 206/1000 | Loss: 0.00002832
Iteration 207/1000 | Loss: 0.00002832
Iteration 208/1000 | Loss: 0.00002832
Iteration 209/1000 | Loss: 0.00002832
Iteration 210/1000 | Loss: 0.00002832
Iteration 211/1000 | Loss: 0.00002832
Iteration 212/1000 | Loss: 0.00002832
Iteration 213/1000 | Loss: 0.00002832
Iteration 214/1000 | Loss: 0.00002832
Iteration 215/1000 | Loss: 0.00002832
Iteration 216/1000 | Loss: 0.00002832
Iteration 217/1000 | Loss: 0.00002832
Iteration 218/1000 | Loss: 0.00002832
Iteration 219/1000 | Loss: 0.00002832
Iteration 220/1000 | Loss: 0.00002832
Iteration 221/1000 | Loss: 0.00002832
Iteration 222/1000 | Loss: 0.00002832
Iteration 223/1000 | Loss: 0.00002832
Iteration 224/1000 | Loss: 0.00002832
Iteration 225/1000 | Loss: 0.00002831
Iteration 226/1000 | Loss: 0.00002831
Iteration 227/1000 | Loss: 0.00002831
Iteration 228/1000 | Loss: 0.00002831
Iteration 229/1000 | Loss: 0.00002831
Iteration 230/1000 | Loss: 0.00002831
Iteration 231/1000 | Loss: 0.00002831
Iteration 232/1000 | Loss: 0.00002831
Iteration 233/1000 | Loss: 0.00002831
Iteration 234/1000 | Loss: 0.00002831
Iteration 235/1000 | Loss: 0.00002831
Iteration 236/1000 | Loss: 0.00002831
Iteration 237/1000 | Loss: 0.00002831
Iteration 238/1000 | Loss: 0.00002831
Iteration 239/1000 | Loss: 0.00002831
Iteration 240/1000 | Loss: 0.00002831
Iteration 241/1000 | Loss: 0.00002831
Iteration 242/1000 | Loss: 0.00002831
Iteration 243/1000 | Loss: 0.00002831
Iteration 244/1000 | Loss: 0.00002831
Iteration 245/1000 | Loss: 0.00002831
Iteration 246/1000 | Loss: 0.00002831
Iteration 247/1000 | Loss: 0.00002831
Iteration 248/1000 | Loss: 0.00002831
Iteration 249/1000 | Loss: 0.00002830
Iteration 250/1000 | Loss: 0.00002830
Iteration 251/1000 | Loss: 0.00002830
Iteration 252/1000 | Loss: 0.00002830
Iteration 253/1000 | Loss: 0.00002830
Iteration 254/1000 | Loss: 0.00002830
Iteration 255/1000 | Loss: 0.00002830
Iteration 256/1000 | Loss: 0.00002830
Iteration 257/1000 | Loss: 0.00002830
Iteration 258/1000 | Loss: 0.00002830
Iteration 259/1000 | Loss: 0.00002830
Iteration 260/1000 | Loss: 0.00002830
Iteration 261/1000 | Loss: 0.00002830
Iteration 262/1000 | Loss: 0.00002830
Iteration 263/1000 | Loss: 0.00002830
Iteration 264/1000 | Loss: 0.00002830
Iteration 265/1000 | Loss: 0.00002830
Iteration 266/1000 | Loss: 0.00002830
Iteration 267/1000 | Loss: 0.00002830
Iteration 268/1000 | Loss: 0.00002830
Iteration 269/1000 | Loss: 0.00002830
Iteration 270/1000 | Loss: 0.00002830
Iteration 271/1000 | Loss: 0.00002830
Iteration 272/1000 | Loss: 0.00002830
Iteration 273/1000 | Loss: 0.00002830
Iteration 274/1000 | Loss: 0.00002830
Iteration 275/1000 | Loss: 0.00002830
Iteration 276/1000 | Loss: 0.00002830
Iteration 277/1000 | Loss: 0.00002830
Iteration 278/1000 | Loss: 0.00002830
Iteration 279/1000 | Loss: 0.00002830
Iteration 280/1000 | Loss: 0.00002830
Iteration 281/1000 | Loss: 0.00002830
Iteration 282/1000 | Loss: 0.00002830
Iteration 283/1000 | Loss: 0.00002830
Iteration 284/1000 | Loss: 0.00002830
Iteration 285/1000 | Loss: 0.00002830
Iteration 286/1000 | Loss: 0.00002830
Iteration 287/1000 | Loss: 0.00002830
Iteration 288/1000 | Loss: 0.00002830
Iteration 289/1000 | Loss: 0.00002830
Iteration 290/1000 | Loss: 0.00002830
Iteration 291/1000 | Loss: 0.00002830
Iteration 292/1000 | Loss: 0.00002830
Iteration 293/1000 | Loss: 0.00002830
Iteration 294/1000 | Loss: 0.00002830
Iteration 295/1000 | Loss: 0.00002830
Iteration 296/1000 | Loss: 0.00002830
Iteration 297/1000 | Loss: 0.00002830
Iteration 298/1000 | Loss: 0.00002830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 298. Stopping optimization.
Last 5 losses: [2.8302652935963124e-05, 2.8302652935963124e-05, 2.8302652935963124e-05, 2.8302652935963124e-05, 2.8302652935963124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8302652935963124e-05

Optimization complete. Final v2v error: 4.559634208679199 mm

Highest mean error: 6.912955284118652 mm for frame 9

Lowest mean error: 4.256150722503662 mm for frame 6

Saving results

Total time: 159.45720100402832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761379
Iteration 2/25 | Loss: 0.00150125
Iteration 3/25 | Loss: 0.00131175
Iteration 4/25 | Loss: 0.00129283
Iteration 5/25 | Loss: 0.00128940
Iteration 6/25 | Loss: 0.00128903
Iteration 7/25 | Loss: 0.00128903
Iteration 8/25 | Loss: 0.00128903
Iteration 9/25 | Loss: 0.00128903
Iteration 10/25 | Loss: 0.00128903
Iteration 11/25 | Loss: 0.00128903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001289029256440699, 0.001289029256440699, 0.001289029256440699, 0.001289029256440699, 0.001289029256440699]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001289029256440699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37982130
Iteration 2/25 | Loss: 0.00056101
Iteration 3/25 | Loss: 0.00056098
Iteration 4/25 | Loss: 0.00056098
Iteration 5/25 | Loss: 0.00056098
Iteration 6/25 | Loss: 0.00056098
Iteration 7/25 | Loss: 0.00056098
Iteration 8/25 | Loss: 0.00056098
Iteration 9/25 | Loss: 0.00056098
Iteration 10/25 | Loss: 0.00056098
Iteration 11/25 | Loss: 0.00056098
Iteration 12/25 | Loss: 0.00056098
Iteration 13/25 | Loss: 0.00056098
Iteration 14/25 | Loss: 0.00056098
Iteration 15/25 | Loss: 0.00056098
Iteration 16/25 | Loss: 0.00056098
Iteration 17/25 | Loss: 0.00056098
Iteration 18/25 | Loss: 0.00056098
Iteration 19/25 | Loss: 0.00056098
Iteration 20/25 | Loss: 0.00056098
Iteration 21/25 | Loss: 0.00056098
Iteration 22/25 | Loss: 0.00056098
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005609798827208579, 0.0005609798827208579, 0.0005609798827208579, 0.0005609798827208579, 0.0005609798827208579]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005609798827208579

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056098
Iteration 2/1000 | Loss: 0.00003443
Iteration 3/1000 | Loss: 0.00002401
Iteration 4/1000 | Loss: 0.00002138
Iteration 5/1000 | Loss: 0.00002047
Iteration 6/1000 | Loss: 0.00001923
Iteration 7/1000 | Loss: 0.00001848
Iteration 8/1000 | Loss: 0.00001780
Iteration 9/1000 | Loss: 0.00001736
Iteration 10/1000 | Loss: 0.00001701
Iteration 11/1000 | Loss: 0.00001698
Iteration 12/1000 | Loss: 0.00001681
Iteration 13/1000 | Loss: 0.00001680
Iteration 14/1000 | Loss: 0.00001677
Iteration 15/1000 | Loss: 0.00001665
Iteration 16/1000 | Loss: 0.00001664
Iteration 17/1000 | Loss: 0.00001650
Iteration 18/1000 | Loss: 0.00001639
Iteration 19/1000 | Loss: 0.00001638
Iteration 20/1000 | Loss: 0.00001638
Iteration 21/1000 | Loss: 0.00001637
Iteration 22/1000 | Loss: 0.00001634
Iteration 23/1000 | Loss: 0.00001634
Iteration 24/1000 | Loss: 0.00001634
Iteration 25/1000 | Loss: 0.00001634
Iteration 26/1000 | Loss: 0.00001634
Iteration 27/1000 | Loss: 0.00001634
Iteration 28/1000 | Loss: 0.00001634
Iteration 29/1000 | Loss: 0.00001634
Iteration 30/1000 | Loss: 0.00001634
Iteration 31/1000 | Loss: 0.00001632
Iteration 32/1000 | Loss: 0.00001630
Iteration 33/1000 | Loss: 0.00001630
Iteration 34/1000 | Loss: 0.00001629
Iteration 35/1000 | Loss: 0.00001629
Iteration 36/1000 | Loss: 0.00001629
Iteration 37/1000 | Loss: 0.00001626
Iteration 38/1000 | Loss: 0.00001626
Iteration 39/1000 | Loss: 0.00001626
Iteration 40/1000 | Loss: 0.00001625
Iteration 41/1000 | Loss: 0.00001625
Iteration 42/1000 | Loss: 0.00001625
Iteration 43/1000 | Loss: 0.00001625
Iteration 44/1000 | Loss: 0.00001625
Iteration 45/1000 | Loss: 0.00001624
Iteration 46/1000 | Loss: 0.00001624
Iteration 47/1000 | Loss: 0.00001624
Iteration 48/1000 | Loss: 0.00001623
Iteration 49/1000 | Loss: 0.00001623
Iteration 50/1000 | Loss: 0.00001623
Iteration 51/1000 | Loss: 0.00001623
Iteration 52/1000 | Loss: 0.00001623
Iteration 53/1000 | Loss: 0.00001622
Iteration 54/1000 | Loss: 0.00001622
Iteration 55/1000 | Loss: 0.00001622
Iteration 56/1000 | Loss: 0.00001620
Iteration 57/1000 | Loss: 0.00001620
Iteration 58/1000 | Loss: 0.00001618
Iteration 59/1000 | Loss: 0.00001618
Iteration 60/1000 | Loss: 0.00001617
Iteration 61/1000 | Loss: 0.00001617
Iteration 62/1000 | Loss: 0.00001616
Iteration 63/1000 | Loss: 0.00001615
Iteration 64/1000 | Loss: 0.00001615
Iteration 65/1000 | Loss: 0.00001614
Iteration 66/1000 | Loss: 0.00001614
Iteration 67/1000 | Loss: 0.00001610
Iteration 68/1000 | Loss: 0.00001610
Iteration 69/1000 | Loss: 0.00001610
Iteration 70/1000 | Loss: 0.00001610
Iteration 71/1000 | Loss: 0.00001610
Iteration 72/1000 | Loss: 0.00001610
Iteration 73/1000 | Loss: 0.00001610
Iteration 74/1000 | Loss: 0.00001610
Iteration 75/1000 | Loss: 0.00001609
Iteration 76/1000 | Loss: 0.00001609
Iteration 77/1000 | Loss: 0.00001609
Iteration 78/1000 | Loss: 0.00001609
Iteration 79/1000 | Loss: 0.00001609
Iteration 80/1000 | Loss: 0.00001608
Iteration 81/1000 | Loss: 0.00001607
Iteration 82/1000 | Loss: 0.00001607
Iteration 83/1000 | Loss: 0.00001607
Iteration 84/1000 | Loss: 0.00001607
Iteration 85/1000 | Loss: 0.00001607
Iteration 86/1000 | Loss: 0.00001607
Iteration 87/1000 | Loss: 0.00001607
Iteration 88/1000 | Loss: 0.00001607
Iteration 89/1000 | Loss: 0.00001607
Iteration 90/1000 | Loss: 0.00001606
Iteration 91/1000 | Loss: 0.00001606
Iteration 92/1000 | Loss: 0.00001606
Iteration 93/1000 | Loss: 0.00001606
Iteration 94/1000 | Loss: 0.00001606
Iteration 95/1000 | Loss: 0.00001605
Iteration 96/1000 | Loss: 0.00001605
Iteration 97/1000 | Loss: 0.00001605
Iteration 98/1000 | Loss: 0.00001605
Iteration 99/1000 | Loss: 0.00001605
Iteration 100/1000 | Loss: 0.00001605
Iteration 101/1000 | Loss: 0.00001605
Iteration 102/1000 | Loss: 0.00001604
Iteration 103/1000 | Loss: 0.00001604
Iteration 104/1000 | Loss: 0.00001604
Iteration 105/1000 | Loss: 0.00001604
Iteration 106/1000 | Loss: 0.00001604
Iteration 107/1000 | Loss: 0.00001604
Iteration 108/1000 | Loss: 0.00001604
Iteration 109/1000 | Loss: 0.00001604
Iteration 110/1000 | Loss: 0.00001604
Iteration 111/1000 | Loss: 0.00001603
Iteration 112/1000 | Loss: 0.00001603
Iteration 113/1000 | Loss: 0.00001603
Iteration 114/1000 | Loss: 0.00001603
Iteration 115/1000 | Loss: 0.00001603
Iteration 116/1000 | Loss: 0.00001602
Iteration 117/1000 | Loss: 0.00001602
Iteration 118/1000 | Loss: 0.00001602
Iteration 119/1000 | Loss: 0.00001602
Iteration 120/1000 | Loss: 0.00001602
Iteration 121/1000 | Loss: 0.00001602
Iteration 122/1000 | Loss: 0.00001602
Iteration 123/1000 | Loss: 0.00001602
Iteration 124/1000 | Loss: 0.00001602
Iteration 125/1000 | Loss: 0.00001602
Iteration 126/1000 | Loss: 0.00001602
Iteration 127/1000 | Loss: 0.00001602
Iteration 128/1000 | Loss: 0.00001602
Iteration 129/1000 | Loss: 0.00001602
Iteration 130/1000 | Loss: 0.00001602
Iteration 131/1000 | Loss: 0.00001602
Iteration 132/1000 | Loss: 0.00001602
Iteration 133/1000 | Loss: 0.00001602
Iteration 134/1000 | Loss: 0.00001602
Iteration 135/1000 | Loss: 0.00001602
Iteration 136/1000 | Loss: 0.00001602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.60199124366045e-05, 1.60199124366045e-05, 1.60199124366045e-05, 1.60199124366045e-05, 1.60199124366045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.60199124366045e-05

Optimization complete. Final v2v error: 3.393071413040161 mm

Highest mean error: 3.6461989879608154 mm for frame 109

Lowest mean error: 3.1881635189056396 mm for frame 47

Saving results

Total time: 35.944581508636475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377472
Iteration 2/25 | Loss: 0.00145855
Iteration 3/25 | Loss: 0.00127975
Iteration 4/25 | Loss: 0.00126177
Iteration 5/25 | Loss: 0.00125615
Iteration 6/25 | Loss: 0.00125521
Iteration 7/25 | Loss: 0.00125521
Iteration 8/25 | Loss: 0.00125521
Iteration 9/25 | Loss: 0.00125521
Iteration 10/25 | Loss: 0.00125521
Iteration 11/25 | Loss: 0.00125521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012552147964015603, 0.0012552147964015603, 0.0012552147964015603, 0.0012552147964015603, 0.0012552147964015603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012552147964015603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49094439
Iteration 2/25 | Loss: 0.00080394
Iteration 3/25 | Loss: 0.00080394
Iteration 4/25 | Loss: 0.00080394
Iteration 5/25 | Loss: 0.00080394
Iteration 6/25 | Loss: 0.00080394
Iteration 7/25 | Loss: 0.00080394
Iteration 8/25 | Loss: 0.00080394
Iteration 9/25 | Loss: 0.00080394
Iteration 10/25 | Loss: 0.00080393
Iteration 11/25 | Loss: 0.00080393
Iteration 12/25 | Loss: 0.00080393
Iteration 13/25 | Loss: 0.00080393
Iteration 14/25 | Loss: 0.00080393
Iteration 15/25 | Loss: 0.00080393
Iteration 16/25 | Loss: 0.00080393
Iteration 17/25 | Loss: 0.00080393
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008039346430450678, 0.0008039346430450678, 0.0008039346430450678, 0.0008039346430450678, 0.0008039346430450678]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008039346430450678

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080393
Iteration 2/1000 | Loss: 0.00004953
Iteration 3/1000 | Loss: 0.00002721
Iteration 4/1000 | Loss: 0.00001997
Iteration 5/1000 | Loss: 0.00001771
Iteration 6/1000 | Loss: 0.00001669
Iteration 7/1000 | Loss: 0.00001592
Iteration 8/1000 | Loss: 0.00001553
Iteration 9/1000 | Loss: 0.00001508
Iteration 10/1000 | Loss: 0.00001485
Iteration 11/1000 | Loss: 0.00001465
Iteration 12/1000 | Loss: 0.00001459
Iteration 13/1000 | Loss: 0.00001447
Iteration 14/1000 | Loss: 0.00001442
Iteration 15/1000 | Loss: 0.00001438
Iteration 16/1000 | Loss: 0.00001437
Iteration 17/1000 | Loss: 0.00001437
Iteration 18/1000 | Loss: 0.00001436
Iteration 19/1000 | Loss: 0.00001436
Iteration 20/1000 | Loss: 0.00001435
Iteration 21/1000 | Loss: 0.00001434
Iteration 22/1000 | Loss: 0.00001433
Iteration 23/1000 | Loss: 0.00001432
Iteration 24/1000 | Loss: 0.00001429
Iteration 25/1000 | Loss: 0.00001428
Iteration 26/1000 | Loss: 0.00001428
Iteration 27/1000 | Loss: 0.00001427
Iteration 28/1000 | Loss: 0.00001425
Iteration 29/1000 | Loss: 0.00001424
Iteration 30/1000 | Loss: 0.00001424
Iteration 31/1000 | Loss: 0.00001424
Iteration 32/1000 | Loss: 0.00001424
Iteration 33/1000 | Loss: 0.00001424
Iteration 34/1000 | Loss: 0.00001424
Iteration 35/1000 | Loss: 0.00001424
Iteration 36/1000 | Loss: 0.00001423
Iteration 37/1000 | Loss: 0.00001422
Iteration 38/1000 | Loss: 0.00001422
Iteration 39/1000 | Loss: 0.00001420
Iteration 40/1000 | Loss: 0.00001420
Iteration 41/1000 | Loss: 0.00001419
Iteration 42/1000 | Loss: 0.00001419
Iteration 43/1000 | Loss: 0.00001419
Iteration 44/1000 | Loss: 0.00001419
Iteration 45/1000 | Loss: 0.00001418
Iteration 46/1000 | Loss: 0.00001418
Iteration 47/1000 | Loss: 0.00001417
Iteration 48/1000 | Loss: 0.00001417
Iteration 49/1000 | Loss: 0.00001417
Iteration 50/1000 | Loss: 0.00001417
Iteration 51/1000 | Loss: 0.00001417
Iteration 52/1000 | Loss: 0.00001416
Iteration 53/1000 | Loss: 0.00001416
Iteration 54/1000 | Loss: 0.00001416
Iteration 55/1000 | Loss: 0.00001415
Iteration 56/1000 | Loss: 0.00001415
Iteration 57/1000 | Loss: 0.00001415
Iteration 58/1000 | Loss: 0.00001414
Iteration 59/1000 | Loss: 0.00001414
Iteration 60/1000 | Loss: 0.00001414
Iteration 61/1000 | Loss: 0.00001414
Iteration 62/1000 | Loss: 0.00001414
Iteration 63/1000 | Loss: 0.00001414
Iteration 64/1000 | Loss: 0.00001413
Iteration 65/1000 | Loss: 0.00001413
Iteration 66/1000 | Loss: 0.00001413
Iteration 67/1000 | Loss: 0.00001413
Iteration 68/1000 | Loss: 0.00001413
Iteration 69/1000 | Loss: 0.00001413
Iteration 70/1000 | Loss: 0.00001413
Iteration 71/1000 | Loss: 0.00001413
Iteration 72/1000 | Loss: 0.00001412
Iteration 73/1000 | Loss: 0.00001412
Iteration 74/1000 | Loss: 0.00001412
Iteration 75/1000 | Loss: 0.00001412
Iteration 76/1000 | Loss: 0.00001412
Iteration 77/1000 | Loss: 0.00001412
Iteration 78/1000 | Loss: 0.00001412
Iteration 79/1000 | Loss: 0.00001412
Iteration 80/1000 | Loss: 0.00001412
Iteration 81/1000 | Loss: 0.00001412
Iteration 82/1000 | Loss: 0.00001412
Iteration 83/1000 | Loss: 0.00001412
Iteration 84/1000 | Loss: 0.00001412
Iteration 85/1000 | Loss: 0.00001412
Iteration 86/1000 | Loss: 0.00001411
Iteration 87/1000 | Loss: 0.00001411
Iteration 88/1000 | Loss: 0.00001411
Iteration 89/1000 | Loss: 0.00001411
Iteration 90/1000 | Loss: 0.00001411
Iteration 91/1000 | Loss: 0.00001411
Iteration 92/1000 | Loss: 0.00001411
Iteration 93/1000 | Loss: 0.00001411
Iteration 94/1000 | Loss: 0.00001411
Iteration 95/1000 | Loss: 0.00001411
Iteration 96/1000 | Loss: 0.00001411
Iteration 97/1000 | Loss: 0.00001410
Iteration 98/1000 | Loss: 0.00001410
Iteration 99/1000 | Loss: 0.00001410
Iteration 100/1000 | Loss: 0.00001410
Iteration 101/1000 | Loss: 0.00001410
Iteration 102/1000 | Loss: 0.00001409
Iteration 103/1000 | Loss: 0.00001409
Iteration 104/1000 | Loss: 0.00001409
Iteration 105/1000 | Loss: 0.00001409
Iteration 106/1000 | Loss: 0.00001409
Iteration 107/1000 | Loss: 0.00001409
Iteration 108/1000 | Loss: 0.00001409
Iteration 109/1000 | Loss: 0.00001409
Iteration 110/1000 | Loss: 0.00001409
Iteration 111/1000 | Loss: 0.00001409
Iteration 112/1000 | Loss: 0.00001409
Iteration 113/1000 | Loss: 0.00001409
Iteration 114/1000 | Loss: 0.00001409
Iteration 115/1000 | Loss: 0.00001408
Iteration 116/1000 | Loss: 0.00001408
Iteration 117/1000 | Loss: 0.00001408
Iteration 118/1000 | Loss: 0.00001408
Iteration 119/1000 | Loss: 0.00001408
Iteration 120/1000 | Loss: 0.00001408
Iteration 121/1000 | Loss: 0.00001408
Iteration 122/1000 | Loss: 0.00001408
Iteration 123/1000 | Loss: 0.00001407
Iteration 124/1000 | Loss: 0.00001407
Iteration 125/1000 | Loss: 0.00001407
Iteration 126/1000 | Loss: 0.00001407
Iteration 127/1000 | Loss: 0.00001407
Iteration 128/1000 | Loss: 0.00001407
Iteration 129/1000 | Loss: 0.00001407
Iteration 130/1000 | Loss: 0.00001406
Iteration 131/1000 | Loss: 0.00001406
Iteration 132/1000 | Loss: 0.00001406
Iteration 133/1000 | Loss: 0.00001406
Iteration 134/1000 | Loss: 0.00001406
Iteration 135/1000 | Loss: 0.00001405
Iteration 136/1000 | Loss: 0.00001405
Iteration 137/1000 | Loss: 0.00001405
Iteration 138/1000 | Loss: 0.00001405
Iteration 139/1000 | Loss: 0.00001405
Iteration 140/1000 | Loss: 0.00001404
Iteration 141/1000 | Loss: 0.00001404
Iteration 142/1000 | Loss: 0.00001404
Iteration 143/1000 | Loss: 0.00001404
Iteration 144/1000 | Loss: 0.00001404
Iteration 145/1000 | Loss: 0.00001404
Iteration 146/1000 | Loss: 0.00001404
Iteration 147/1000 | Loss: 0.00001404
Iteration 148/1000 | Loss: 0.00001404
Iteration 149/1000 | Loss: 0.00001404
Iteration 150/1000 | Loss: 0.00001404
Iteration 151/1000 | Loss: 0.00001404
Iteration 152/1000 | Loss: 0.00001404
Iteration 153/1000 | Loss: 0.00001404
Iteration 154/1000 | Loss: 0.00001404
Iteration 155/1000 | Loss: 0.00001404
Iteration 156/1000 | Loss: 0.00001404
Iteration 157/1000 | Loss: 0.00001404
Iteration 158/1000 | Loss: 0.00001404
Iteration 159/1000 | Loss: 0.00001404
Iteration 160/1000 | Loss: 0.00001404
Iteration 161/1000 | Loss: 0.00001404
Iteration 162/1000 | Loss: 0.00001404
Iteration 163/1000 | Loss: 0.00001404
Iteration 164/1000 | Loss: 0.00001404
Iteration 165/1000 | Loss: 0.00001404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.4035395906830672e-05, 1.4035395906830672e-05, 1.4035395906830672e-05, 1.4035395906830672e-05, 1.4035395906830672e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4035395906830672e-05

Optimization complete. Final v2v error: 3.2448225021362305 mm

Highest mean error: 3.5603549480438232 mm for frame 39

Lowest mean error: 2.796022891998291 mm for frame 7

Saving results

Total time: 37.365747928619385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818614
Iteration 2/25 | Loss: 0.00180951
Iteration 3/25 | Loss: 0.00153708
Iteration 4/25 | Loss: 0.00149661
Iteration 5/25 | Loss: 0.00147338
Iteration 6/25 | Loss: 0.00142932
Iteration 7/25 | Loss: 0.00141350
Iteration 8/25 | Loss: 0.00140490
Iteration 9/25 | Loss: 0.00139706
Iteration 10/25 | Loss: 0.00139016
Iteration 11/25 | Loss: 0.00139332
Iteration 12/25 | Loss: 0.00138742
Iteration 13/25 | Loss: 0.00138115
Iteration 14/25 | Loss: 0.00137915
Iteration 15/25 | Loss: 0.00137884
Iteration 16/25 | Loss: 0.00137868
Iteration 17/25 | Loss: 0.00138232
Iteration 18/25 | Loss: 0.00137719
Iteration 19/25 | Loss: 0.00137515
Iteration 20/25 | Loss: 0.00137695
Iteration 21/25 | Loss: 0.00137640
Iteration 22/25 | Loss: 0.00137573
Iteration 23/25 | Loss: 0.00137332
Iteration 24/25 | Loss: 0.00137248
Iteration 25/25 | Loss: 0.00137230

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53844225
Iteration 2/25 | Loss: 0.00093057
Iteration 3/25 | Loss: 0.00093055
Iteration 4/25 | Loss: 0.00093055
Iteration 5/25 | Loss: 0.00093055
Iteration 6/25 | Loss: 0.00093055
Iteration 7/25 | Loss: 0.00093055
Iteration 8/25 | Loss: 0.00093055
Iteration 9/25 | Loss: 0.00093055
Iteration 10/25 | Loss: 0.00093055
Iteration 11/25 | Loss: 0.00093055
Iteration 12/25 | Loss: 0.00093055
Iteration 13/25 | Loss: 0.00093055
Iteration 14/25 | Loss: 0.00093055
Iteration 15/25 | Loss: 0.00093055
Iteration 16/25 | Loss: 0.00093055
Iteration 17/25 | Loss: 0.00093055
Iteration 18/25 | Loss: 0.00093055
Iteration 19/25 | Loss: 0.00093055
Iteration 20/25 | Loss: 0.00093055
Iteration 21/25 | Loss: 0.00093055
Iteration 22/25 | Loss: 0.00093055
Iteration 23/25 | Loss: 0.00093055
Iteration 24/25 | Loss: 0.00093055
Iteration 25/25 | Loss: 0.00093055

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093055
Iteration 2/1000 | Loss: 0.00009434
Iteration 3/1000 | Loss: 0.00006203
Iteration 4/1000 | Loss: 0.00005109
Iteration 5/1000 | Loss: 0.00004612
Iteration 6/1000 | Loss: 0.00004378
Iteration 7/1000 | Loss: 0.00004208
Iteration 8/1000 | Loss: 0.00004085
Iteration 9/1000 | Loss: 0.00052412
Iteration 10/1000 | Loss: 0.00007202
Iteration 11/1000 | Loss: 0.00004786
Iteration 12/1000 | Loss: 0.00072279
Iteration 13/1000 | Loss: 0.00004665
Iteration 14/1000 | Loss: 0.00004098
Iteration 15/1000 | Loss: 0.00003878
Iteration 16/1000 | Loss: 0.00004003
Iteration 17/1000 | Loss: 0.00003691
Iteration 18/1000 | Loss: 0.00003849
Iteration 19/1000 | Loss: 0.00003582
Iteration 20/1000 | Loss: 0.00003522
Iteration 21/1000 | Loss: 0.00003491
Iteration 22/1000 | Loss: 0.00003477
Iteration 23/1000 | Loss: 0.00003461
Iteration 24/1000 | Loss: 0.00003460
Iteration 25/1000 | Loss: 0.00003443
Iteration 26/1000 | Loss: 0.00003440
Iteration 27/1000 | Loss: 0.00003424
Iteration 28/1000 | Loss: 0.00003421
Iteration 29/1000 | Loss: 0.00003407
Iteration 30/1000 | Loss: 0.00003402
Iteration 31/1000 | Loss: 0.00003391
Iteration 32/1000 | Loss: 0.00003390
Iteration 33/1000 | Loss: 0.00003390
Iteration 34/1000 | Loss: 0.00003390
Iteration 35/1000 | Loss: 0.00003389
Iteration 36/1000 | Loss: 0.00003389
Iteration 37/1000 | Loss: 0.00003388
Iteration 38/1000 | Loss: 0.00003387
Iteration 39/1000 | Loss: 0.00003387
Iteration 40/1000 | Loss: 0.00003387
Iteration 41/1000 | Loss: 0.00003386
Iteration 42/1000 | Loss: 0.00003385
Iteration 43/1000 | Loss: 0.00003385
Iteration 44/1000 | Loss: 0.00003385
Iteration 45/1000 | Loss: 0.00003385
Iteration 46/1000 | Loss: 0.00003385
Iteration 47/1000 | Loss: 0.00003385
Iteration 48/1000 | Loss: 0.00003384
Iteration 49/1000 | Loss: 0.00003383
Iteration 50/1000 | Loss: 0.00003380
Iteration 51/1000 | Loss: 0.00003380
Iteration 52/1000 | Loss: 0.00003379
Iteration 53/1000 | Loss: 0.00003377
Iteration 54/1000 | Loss: 0.00003377
Iteration 55/1000 | Loss: 0.00003377
Iteration 56/1000 | Loss: 0.00003377
Iteration 57/1000 | Loss: 0.00003377
Iteration 58/1000 | Loss: 0.00003377
Iteration 59/1000 | Loss: 0.00003375
Iteration 60/1000 | Loss: 0.00003375
Iteration 61/1000 | Loss: 0.00003369
Iteration 62/1000 | Loss: 0.00003369
Iteration 63/1000 | Loss: 0.00003368
Iteration 64/1000 | Loss: 0.00003368
Iteration 65/1000 | Loss: 0.00003367
Iteration 66/1000 | Loss: 0.00003367
Iteration 67/1000 | Loss: 0.00003366
Iteration 68/1000 | Loss: 0.00003366
Iteration 69/1000 | Loss: 0.00003366
Iteration 70/1000 | Loss: 0.00003365
Iteration 71/1000 | Loss: 0.00003365
Iteration 72/1000 | Loss: 0.00003365
Iteration 73/1000 | Loss: 0.00003365
Iteration 74/1000 | Loss: 0.00003365
Iteration 75/1000 | Loss: 0.00003365
Iteration 76/1000 | Loss: 0.00003364
Iteration 77/1000 | Loss: 0.00003364
Iteration 78/1000 | Loss: 0.00003364
Iteration 79/1000 | Loss: 0.00003364
Iteration 80/1000 | Loss: 0.00003364
Iteration 81/1000 | Loss: 0.00003364
Iteration 82/1000 | Loss: 0.00003363
Iteration 83/1000 | Loss: 0.00003363
Iteration 84/1000 | Loss: 0.00003363
Iteration 85/1000 | Loss: 0.00003362
Iteration 86/1000 | Loss: 0.00003362
Iteration 87/1000 | Loss: 0.00032691
Iteration 88/1000 | Loss: 0.00003837
Iteration 89/1000 | Loss: 0.00003456
Iteration 90/1000 | Loss: 0.00003265
Iteration 91/1000 | Loss: 0.00003213
Iteration 92/1000 | Loss: 0.00003183
Iteration 93/1000 | Loss: 0.00003172
Iteration 94/1000 | Loss: 0.00003156
Iteration 95/1000 | Loss: 0.00003155
Iteration 96/1000 | Loss: 0.00003155
Iteration 97/1000 | Loss: 0.00003153
Iteration 98/1000 | Loss: 0.00003153
Iteration 99/1000 | Loss: 0.00003152
Iteration 100/1000 | Loss: 0.00003152
Iteration 101/1000 | Loss: 0.00003152
Iteration 102/1000 | Loss: 0.00003151
Iteration 103/1000 | Loss: 0.00003151
Iteration 104/1000 | Loss: 0.00003151
Iteration 105/1000 | Loss: 0.00003151
Iteration 106/1000 | Loss: 0.00003150
Iteration 107/1000 | Loss: 0.00003150
Iteration 108/1000 | Loss: 0.00003150
Iteration 109/1000 | Loss: 0.00003149
Iteration 110/1000 | Loss: 0.00003149
Iteration 111/1000 | Loss: 0.00003149
Iteration 112/1000 | Loss: 0.00003149
Iteration 113/1000 | Loss: 0.00003149
Iteration 114/1000 | Loss: 0.00003148
Iteration 115/1000 | Loss: 0.00003148
Iteration 116/1000 | Loss: 0.00003148
Iteration 117/1000 | Loss: 0.00003148
Iteration 118/1000 | Loss: 0.00003148
Iteration 119/1000 | Loss: 0.00003148
Iteration 120/1000 | Loss: 0.00003148
Iteration 121/1000 | Loss: 0.00003148
Iteration 122/1000 | Loss: 0.00003147
Iteration 123/1000 | Loss: 0.00003147
Iteration 124/1000 | Loss: 0.00003147
Iteration 125/1000 | Loss: 0.00003147
Iteration 126/1000 | Loss: 0.00003147
Iteration 127/1000 | Loss: 0.00003147
Iteration 128/1000 | Loss: 0.00003146
Iteration 129/1000 | Loss: 0.00003146
Iteration 130/1000 | Loss: 0.00003146
Iteration 131/1000 | Loss: 0.00003146
Iteration 132/1000 | Loss: 0.00003146
Iteration 133/1000 | Loss: 0.00003146
Iteration 134/1000 | Loss: 0.00003146
Iteration 135/1000 | Loss: 0.00003146
Iteration 136/1000 | Loss: 0.00003146
Iteration 137/1000 | Loss: 0.00003146
Iteration 138/1000 | Loss: 0.00003146
Iteration 139/1000 | Loss: 0.00003146
Iteration 140/1000 | Loss: 0.00003146
Iteration 141/1000 | Loss: 0.00003146
Iteration 142/1000 | Loss: 0.00003145
Iteration 143/1000 | Loss: 0.00003145
Iteration 144/1000 | Loss: 0.00003145
Iteration 145/1000 | Loss: 0.00003145
Iteration 146/1000 | Loss: 0.00003145
Iteration 147/1000 | Loss: 0.00003145
Iteration 148/1000 | Loss: 0.00003145
Iteration 149/1000 | Loss: 0.00003145
Iteration 150/1000 | Loss: 0.00003145
Iteration 151/1000 | Loss: 0.00003145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [3.145441587548703e-05, 3.145441587548703e-05, 3.145441587548703e-05, 3.145441587548703e-05, 3.145441587548703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.145441587548703e-05

Optimization complete. Final v2v error: 4.6844401359558105 mm

Highest mean error: 6.833378314971924 mm for frame 90

Lowest mean error: 3.6757664680480957 mm for frame 82

Saving results

Total time: 117.01998209953308
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026240
Iteration 2/25 | Loss: 0.00235331
Iteration 3/25 | Loss: 0.00198154
Iteration 4/25 | Loss: 0.00168992
Iteration 5/25 | Loss: 0.00155932
Iteration 6/25 | Loss: 0.00149153
Iteration 7/25 | Loss: 0.00144345
Iteration 8/25 | Loss: 0.00139484
Iteration 9/25 | Loss: 0.00137976
Iteration 10/25 | Loss: 0.00135567
Iteration 11/25 | Loss: 0.00135813
Iteration 12/25 | Loss: 0.00135538
Iteration 13/25 | Loss: 0.00134519
Iteration 14/25 | Loss: 0.00134403
Iteration 15/25 | Loss: 0.00134373
Iteration 16/25 | Loss: 0.00134321
Iteration 17/25 | Loss: 0.00134243
Iteration 18/25 | Loss: 0.00134212
Iteration 19/25 | Loss: 0.00134428
Iteration 20/25 | Loss: 0.00134681
Iteration 21/25 | Loss: 0.00134525
Iteration 22/25 | Loss: 0.00134247
Iteration 23/25 | Loss: 0.00134548
Iteration 24/25 | Loss: 0.00134633
Iteration 25/25 | Loss: 0.00134661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44796717
Iteration 2/25 | Loss: 0.00224667
Iteration 3/25 | Loss: 0.00104098
Iteration 4/25 | Loss: 0.00104098
Iteration 5/25 | Loss: 0.00104098
Iteration 6/25 | Loss: 0.00104098
Iteration 7/25 | Loss: 0.00104098
Iteration 8/25 | Loss: 0.00104098
Iteration 9/25 | Loss: 0.00104098
Iteration 10/25 | Loss: 0.00104098
Iteration 11/25 | Loss: 0.00104098
Iteration 12/25 | Loss: 0.00104098
Iteration 13/25 | Loss: 0.00104098
Iteration 14/25 | Loss: 0.00104098
Iteration 15/25 | Loss: 0.00104098
Iteration 16/25 | Loss: 0.00104098
Iteration 17/25 | Loss: 0.00104098
Iteration 18/25 | Loss: 0.00104098
Iteration 19/25 | Loss: 0.00104098
Iteration 20/25 | Loss: 0.00104098
Iteration 21/25 | Loss: 0.00104098
Iteration 22/25 | Loss: 0.00104098
Iteration 23/25 | Loss: 0.00104098
Iteration 24/25 | Loss: 0.00104098
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010409773094579577, 0.0010409773094579577, 0.0010409773094579577, 0.0010409773094579577, 0.0010409773094579577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010409773094579577

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104098
Iteration 2/1000 | Loss: 0.00128002
Iteration 3/1000 | Loss: 0.00341511
Iteration 4/1000 | Loss: 0.00025329
Iteration 5/1000 | Loss: 0.00005460
Iteration 6/1000 | Loss: 0.00003035
Iteration 7/1000 | Loss: 0.00003704
Iteration 8/1000 | Loss: 0.00002345
Iteration 9/1000 | Loss: 0.00009757
Iteration 10/1000 | Loss: 0.00047133
Iteration 11/1000 | Loss: 0.00075893
Iteration 12/1000 | Loss: 0.00003822
Iteration 13/1000 | Loss: 0.00002135
Iteration 14/1000 | Loss: 0.00008669
Iteration 15/1000 | Loss: 0.00002662
Iteration 16/1000 | Loss: 0.00002413
Iteration 17/1000 | Loss: 0.00002067
Iteration 18/1000 | Loss: 0.00002046
Iteration 19/1000 | Loss: 0.00002020
Iteration 20/1000 | Loss: 0.00002019
Iteration 21/1000 | Loss: 0.00002000
Iteration 22/1000 | Loss: 0.00001981
Iteration 23/1000 | Loss: 0.00002907
Iteration 24/1000 | Loss: 0.00002141
Iteration 25/1000 | Loss: 0.00016808
Iteration 26/1000 | Loss: 0.00003829
Iteration 27/1000 | Loss: 0.00002351
Iteration 28/1000 | Loss: 0.00016236
Iteration 29/1000 | Loss: 0.00002833
Iteration 30/1000 | Loss: 0.00002439
Iteration 31/1000 | Loss: 0.00001991
Iteration 32/1000 | Loss: 0.00001953
Iteration 33/1000 | Loss: 0.00002242
Iteration 34/1000 | Loss: 0.00001975
Iteration 35/1000 | Loss: 0.00001945
Iteration 36/1000 | Loss: 0.00001944
Iteration 37/1000 | Loss: 0.00001943
Iteration 38/1000 | Loss: 0.00001943
Iteration 39/1000 | Loss: 0.00001942
Iteration 40/1000 | Loss: 0.00001941
Iteration 41/1000 | Loss: 0.00001941
Iteration 42/1000 | Loss: 0.00001940
Iteration 43/1000 | Loss: 0.00001939
Iteration 44/1000 | Loss: 0.00001924
Iteration 45/1000 | Loss: 0.00001920
Iteration 46/1000 | Loss: 0.00001920
Iteration 47/1000 | Loss: 0.00017278
Iteration 48/1000 | Loss: 0.00001978
Iteration 49/1000 | Loss: 0.00001909
Iteration 50/1000 | Loss: 0.00001896
Iteration 51/1000 | Loss: 0.00001895
Iteration 52/1000 | Loss: 0.00001895
Iteration 53/1000 | Loss: 0.00001894
Iteration 54/1000 | Loss: 0.00001894
Iteration 55/1000 | Loss: 0.00001894
Iteration 56/1000 | Loss: 0.00001894
Iteration 57/1000 | Loss: 0.00001894
Iteration 58/1000 | Loss: 0.00001894
Iteration 59/1000 | Loss: 0.00001894
Iteration 60/1000 | Loss: 0.00001894
Iteration 61/1000 | Loss: 0.00001894
Iteration 62/1000 | Loss: 0.00001894
Iteration 63/1000 | Loss: 0.00001893
Iteration 64/1000 | Loss: 0.00001893
Iteration 65/1000 | Loss: 0.00001893
Iteration 66/1000 | Loss: 0.00001893
Iteration 67/1000 | Loss: 0.00001893
Iteration 68/1000 | Loss: 0.00001893
Iteration 69/1000 | Loss: 0.00001893
Iteration 70/1000 | Loss: 0.00001893
Iteration 71/1000 | Loss: 0.00001893
Iteration 72/1000 | Loss: 0.00001893
Iteration 73/1000 | Loss: 0.00001893
Iteration 74/1000 | Loss: 0.00001893
Iteration 75/1000 | Loss: 0.00001893
Iteration 76/1000 | Loss: 0.00001893
Iteration 77/1000 | Loss: 0.00001893
Iteration 78/1000 | Loss: 0.00001893
Iteration 79/1000 | Loss: 0.00001893
Iteration 80/1000 | Loss: 0.00001893
Iteration 81/1000 | Loss: 0.00001892
Iteration 82/1000 | Loss: 0.00001892
Iteration 83/1000 | Loss: 0.00001892
Iteration 84/1000 | Loss: 0.00001892
Iteration 85/1000 | Loss: 0.00001892
Iteration 86/1000 | Loss: 0.00001892
Iteration 87/1000 | Loss: 0.00001892
Iteration 88/1000 | Loss: 0.00001892
Iteration 89/1000 | Loss: 0.00001892
Iteration 90/1000 | Loss: 0.00001892
Iteration 91/1000 | Loss: 0.00001892
Iteration 92/1000 | Loss: 0.00001892
Iteration 93/1000 | Loss: 0.00001892
Iteration 94/1000 | Loss: 0.00001892
Iteration 95/1000 | Loss: 0.00001892
Iteration 96/1000 | Loss: 0.00001892
Iteration 97/1000 | Loss: 0.00001892
Iteration 98/1000 | Loss: 0.00001892
Iteration 99/1000 | Loss: 0.00001891
Iteration 100/1000 | Loss: 0.00001891
Iteration 101/1000 | Loss: 0.00001891
Iteration 102/1000 | Loss: 0.00001891
Iteration 103/1000 | Loss: 0.00001891
Iteration 104/1000 | Loss: 0.00001890
Iteration 105/1000 | Loss: 0.00001890
Iteration 106/1000 | Loss: 0.00008279
Iteration 107/1000 | Loss: 0.00001931
Iteration 108/1000 | Loss: 0.00001895
Iteration 109/1000 | Loss: 0.00001894
Iteration 110/1000 | Loss: 0.00001893
Iteration 111/1000 | Loss: 0.00001892
Iteration 112/1000 | Loss: 0.00001892
Iteration 113/1000 | Loss: 0.00001892
Iteration 114/1000 | Loss: 0.00001890
Iteration 115/1000 | Loss: 0.00001889
Iteration 116/1000 | Loss: 0.00001889
Iteration 117/1000 | Loss: 0.00001888
Iteration 118/1000 | Loss: 0.00001887
Iteration 119/1000 | Loss: 0.00001887
Iteration 120/1000 | Loss: 0.00001887
Iteration 121/1000 | Loss: 0.00001886
Iteration 122/1000 | Loss: 0.00001886
Iteration 123/1000 | Loss: 0.00001885
Iteration 124/1000 | Loss: 0.00001885
Iteration 125/1000 | Loss: 0.00001885
Iteration 126/1000 | Loss: 0.00001885
Iteration 127/1000 | Loss: 0.00001885
Iteration 128/1000 | Loss: 0.00001885
Iteration 129/1000 | Loss: 0.00001885
Iteration 130/1000 | Loss: 0.00001885
Iteration 131/1000 | Loss: 0.00001885
Iteration 132/1000 | Loss: 0.00001885
Iteration 133/1000 | Loss: 0.00001885
Iteration 134/1000 | Loss: 0.00001885
Iteration 135/1000 | Loss: 0.00001885
Iteration 136/1000 | Loss: 0.00001885
Iteration 137/1000 | Loss: 0.00001884
Iteration 138/1000 | Loss: 0.00001884
Iteration 139/1000 | Loss: 0.00001884
Iteration 140/1000 | Loss: 0.00001884
Iteration 141/1000 | Loss: 0.00001883
Iteration 142/1000 | Loss: 0.00001883
Iteration 143/1000 | Loss: 0.00001883
Iteration 144/1000 | Loss: 0.00001883
Iteration 145/1000 | Loss: 0.00001883
Iteration 146/1000 | Loss: 0.00001883
Iteration 147/1000 | Loss: 0.00001883
Iteration 148/1000 | Loss: 0.00001883
Iteration 149/1000 | Loss: 0.00001883
Iteration 150/1000 | Loss: 0.00001883
Iteration 151/1000 | Loss: 0.00001882
Iteration 152/1000 | Loss: 0.00001882
Iteration 153/1000 | Loss: 0.00001882
Iteration 154/1000 | Loss: 0.00001882
Iteration 155/1000 | Loss: 0.00001882
Iteration 156/1000 | Loss: 0.00001882
Iteration 157/1000 | Loss: 0.00001882
Iteration 158/1000 | Loss: 0.00001882
Iteration 159/1000 | Loss: 0.00001882
Iteration 160/1000 | Loss: 0.00001882
Iteration 161/1000 | Loss: 0.00001882
Iteration 162/1000 | Loss: 0.00001882
Iteration 163/1000 | Loss: 0.00001882
Iteration 164/1000 | Loss: 0.00001882
Iteration 165/1000 | Loss: 0.00001882
Iteration 166/1000 | Loss: 0.00001882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.881573007267434e-05, 1.881573007267434e-05, 1.881573007267434e-05, 1.881573007267434e-05, 1.881573007267434e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.881573007267434e-05

Optimization complete. Final v2v error: 3.5522007942199707 mm

Highest mean error: 12.291934967041016 mm for frame 39

Lowest mean error: 3.3251469135284424 mm for frame 166

Saving results

Total time: 125.78930020332336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992616
Iteration 2/25 | Loss: 0.00289623
Iteration 3/25 | Loss: 0.00208898
Iteration 4/25 | Loss: 0.00192538
Iteration 5/25 | Loss: 0.00191073
Iteration 6/25 | Loss: 0.00178680
Iteration 7/25 | Loss: 0.00155371
Iteration 8/25 | Loss: 0.00143787
Iteration 9/25 | Loss: 0.00137106
Iteration 10/25 | Loss: 0.00132742
Iteration 11/25 | Loss: 0.00131472
Iteration 12/25 | Loss: 0.00130632
Iteration 13/25 | Loss: 0.00129774
Iteration 14/25 | Loss: 0.00129652
Iteration 15/25 | Loss: 0.00130216
Iteration 16/25 | Loss: 0.00129960
Iteration 17/25 | Loss: 0.00130217
Iteration 18/25 | Loss: 0.00129887
Iteration 19/25 | Loss: 0.00130115
Iteration 20/25 | Loss: 0.00129716
Iteration 21/25 | Loss: 0.00129544
Iteration 22/25 | Loss: 0.00129520
Iteration 23/25 | Loss: 0.00129098
Iteration 24/25 | Loss: 0.00128847
Iteration 25/25 | Loss: 0.00128865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39449012
Iteration 2/25 | Loss: 0.00091514
Iteration 3/25 | Loss: 0.00091514
Iteration 4/25 | Loss: 0.00091514
Iteration 5/25 | Loss: 0.00091514
Iteration 6/25 | Loss: 0.00090721
Iteration 7/25 | Loss: 0.00090721
Iteration 8/25 | Loss: 0.00090721
Iteration 9/25 | Loss: 0.00090721
Iteration 10/25 | Loss: 0.00090721
Iteration 11/25 | Loss: 0.00090721
Iteration 12/25 | Loss: 0.00090721
Iteration 13/25 | Loss: 0.00090721
Iteration 14/25 | Loss: 0.00090721
Iteration 15/25 | Loss: 0.00090721
Iteration 16/25 | Loss: 0.00090721
Iteration 17/25 | Loss: 0.00090721
Iteration 18/25 | Loss: 0.00090721
Iteration 19/25 | Loss: 0.00090721
Iteration 20/25 | Loss: 0.00090721
Iteration 21/25 | Loss: 0.00090721
Iteration 22/25 | Loss: 0.00090721
Iteration 23/25 | Loss: 0.00090721
Iteration 24/25 | Loss: 0.00090721
Iteration 25/25 | Loss: 0.00090721

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090721
Iteration 2/1000 | Loss: 0.00008217
Iteration 3/1000 | Loss: 0.00011013
Iteration 4/1000 | Loss: 0.00009498
Iteration 5/1000 | Loss: 0.00008202
Iteration 6/1000 | Loss: 0.00008812
Iteration 7/1000 | Loss: 0.00013752
Iteration 8/1000 | Loss: 0.00011152
Iteration 9/1000 | Loss: 0.00011915
Iteration 10/1000 | Loss: 0.00005809
Iteration 11/1000 | Loss: 0.00008626
Iteration 12/1000 | Loss: 0.00009117
Iteration 13/1000 | Loss: 0.00004958
Iteration 14/1000 | Loss: 0.00003826
Iteration 15/1000 | Loss: 0.00006982
Iteration 16/1000 | Loss: 0.00012215
Iteration 17/1000 | Loss: 0.00011428
Iteration 18/1000 | Loss: 0.00002941
Iteration 19/1000 | Loss: 0.00004210
Iteration 20/1000 | Loss: 0.00005286
Iteration 21/1000 | Loss: 0.00007051
Iteration 22/1000 | Loss: 0.00006302
Iteration 23/1000 | Loss: 0.00006988
Iteration 24/1000 | Loss: 0.00003456
Iteration 25/1000 | Loss: 0.00003182
Iteration 26/1000 | Loss: 0.00011609
Iteration 27/1000 | Loss: 0.00007037
Iteration 28/1000 | Loss: 0.00007165
Iteration 29/1000 | Loss: 0.00005594
Iteration 30/1000 | Loss: 0.00005033
Iteration 31/1000 | Loss: 0.00003549
Iteration 32/1000 | Loss: 0.00003228
Iteration 33/1000 | Loss: 0.00004501
Iteration 34/1000 | Loss: 0.00004712
Iteration 35/1000 | Loss: 0.00003880
Iteration 36/1000 | Loss: 0.00004986
Iteration 37/1000 | Loss: 0.00004158
Iteration 38/1000 | Loss: 0.00004862
Iteration 39/1000 | Loss: 0.00004091
Iteration 40/1000 | Loss: 0.00004719
Iteration 41/1000 | Loss: 0.00003015
Iteration 42/1000 | Loss: 0.00004166
Iteration 43/1000 | Loss: 0.00004226
Iteration 44/1000 | Loss: 0.00003155
Iteration 45/1000 | Loss: 0.00002832
Iteration 46/1000 | Loss: 0.00002915
Iteration 47/1000 | Loss: 0.00003807
Iteration 48/1000 | Loss: 0.00004094
Iteration 49/1000 | Loss: 0.00003697
Iteration 50/1000 | Loss: 0.00004303
Iteration 51/1000 | Loss: 0.00005339
Iteration 52/1000 | Loss: 0.00004254
Iteration 53/1000 | Loss: 0.00003233
Iteration 54/1000 | Loss: 0.00004277
Iteration 55/1000 | Loss: 0.00003956
Iteration 56/1000 | Loss: 0.00003811
Iteration 57/1000 | Loss: 0.00004501
Iteration 58/1000 | Loss: 0.00003768
Iteration 59/1000 | Loss: 0.00004732
Iteration 60/1000 | Loss: 0.00003670
Iteration 61/1000 | Loss: 0.00003783
Iteration 62/1000 | Loss: 0.00003913
Iteration 63/1000 | Loss: 0.00004925
Iteration 64/1000 | Loss: 0.00004464
Iteration 65/1000 | Loss: 0.00003670
Iteration 66/1000 | Loss: 0.00004297
Iteration 67/1000 | Loss: 0.00003642
Iteration 68/1000 | Loss: 0.00004192
Iteration 69/1000 | Loss: 0.00003617
Iteration 70/1000 | Loss: 0.00003607
Iteration 71/1000 | Loss: 0.00003641
Iteration 72/1000 | Loss: 0.00004419
Iteration 73/1000 | Loss: 0.00003351
Iteration 74/1000 | Loss: 0.00004556
Iteration 75/1000 | Loss: 0.00003273
Iteration 76/1000 | Loss: 0.00003661
Iteration 77/1000 | Loss: 0.00003522
Iteration 78/1000 | Loss: 0.00004687
Iteration 79/1000 | Loss: 0.00003596
Iteration 80/1000 | Loss: 0.00004631
Iteration 81/1000 | Loss: 0.00004498
Iteration 82/1000 | Loss: 0.00004225
Iteration 83/1000 | Loss: 0.00004515
Iteration 84/1000 | Loss: 0.00004313
Iteration 85/1000 | Loss: 0.00004656
Iteration 86/1000 | Loss: 0.00004250
Iteration 87/1000 | Loss: 0.00002731
Iteration 88/1000 | Loss: 0.00003887
Iteration 89/1000 | Loss: 0.00002622
Iteration 90/1000 | Loss: 0.00004383
Iteration 91/1000 | Loss: 0.00005216
Iteration 92/1000 | Loss: 0.00004631
Iteration 93/1000 | Loss: 0.00004172
Iteration 94/1000 | Loss: 0.00003603
Iteration 95/1000 | Loss: 0.00002807
Iteration 96/1000 | Loss: 0.00005166
Iteration 97/1000 | Loss: 0.00004161
Iteration 98/1000 | Loss: 0.00004430
Iteration 99/1000 | Loss: 0.00004313
Iteration 100/1000 | Loss: 0.00004346
Iteration 101/1000 | Loss: 0.00004261
Iteration 102/1000 | Loss: 0.00004363
Iteration 103/1000 | Loss: 0.00003354
Iteration 104/1000 | Loss: 0.00003198
Iteration 105/1000 | Loss: 0.00003792
Iteration 106/1000 | Loss: 0.00003842
Iteration 107/1000 | Loss: 0.00004842
Iteration 108/1000 | Loss: 0.00004664
Iteration 109/1000 | Loss: 0.00004086
Iteration 110/1000 | Loss: 0.00003700
Iteration 111/1000 | Loss: 0.00004235
Iteration 112/1000 | Loss: 0.00005149
Iteration 113/1000 | Loss: 0.00005719
Iteration 114/1000 | Loss: 0.00002617
Iteration 115/1000 | Loss: 0.00002039
Iteration 116/1000 | Loss: 0.00001824
Iteration 117/1000 | Loss: 0.00001719
Iteration 118/1000 | Loss: 0.00001670
Iteration 119/1000 | Loss: 0.00001626
Iteration 120/1000 | Loss: 0.00001599
Iteration 121/1000 | Loss: 0.00001580
Iteration 122/1000 | Loss: 0.00001562
Iteration 123/1000 | Loss: 0.00001561
Iteration 124/1000 | Loss: 0.00001553
Iteration 125/1000 | Loss: 0.00001552
Iteration 126/1000 | Loss: 0.00001551
Iteration 127/1000 | Loss: 0.00001536
Iteration 128/1000 | Loss: 0.00001536
Iteration 129/1000 | Loss: 0.00001535
Iteration 130/1000 | Loss: 0.00001535
Iteration 131/1000 | Loss: 0.00001530
Iteration 132/1000 | Loss: 0.00001526
Iteration 133/1000 | Loss: 0.00001526
Iteration 134/1000 | Loss: 0.00001525
Iteration 135/1000 | Loss: 0.00001524
Iteration 136/1000 | Loss: 0.00001523
Iteration 137/1000 | Loss: 0.00001520
Iteration 138/1000 | Loss: 0.00001517
Iteration 139/1000 | Loss: 0.00001516
Iteration 140/1000 | Loss: 0.00001515
Iteration 141/1000 | Loss: 0.00001514
Iteration 142/1000 | Loss: 0.00001514
Iteration 143/1000 | Loss: 0.00001514
Iteration 144/1000 | Loss: 0.00001513
Iteration 145/1000 | Loss: 0.00001513
Iteration 146/1000 | Loss: 0.00001512
Iteration 147/1000 | Loss: 0.00001512
Iteration 148/1000 | Loss: 0.00001512
Iteration 149/1000 | Loss: 0.00001512
Iteration 150/1000 | Loss: 0.00001512
Iteration 151/1000 | Loss: 0.00001511
Iteration 152/1000 | Loss: 0.00001510
Iteration 153/1000 | Loss: 0.00001510
Iteration 154/1000 | Loss: 0.00001510
Iteration 155/1000 | Loss: 0.00001509
Iteration 156/1000 | Loss: 0.00001509
Iteration 157/1000 | Loss: 0.00001508
Iteration 158/1000 | Loss: 0.00001508
Iteration 159/1000 | Loss: 0.00001508
Iteration 160/1000 | Loss: 0.00001508
Iteration 161/1000 | Loss: 0.00001508
Iteration 162/1000 | Loss: 0.00001507
Iteration 163/1000 | Loss: 0.00001506
Iteration 164/1000 | Loss: 0.00001505
Iteration 165/1000 | Loss: 0.00001505
Iteration 166/1000 | Loss: 0.00001505
Iteration 167/1000 | Loss: 0.00001504
Iteration 168/1000 | Loss: 0.00001504
Iteration 169/1000 | Loss: 0.00001503
Iteration 170/1000 | Loss: 0.00001502
Iteration 171/1000 | Loss: 0.00001502
Iteration 172/1000 | Loss: 0.00001502
Iteration 173/1000 | Loss: 0.00001502
Iteration 174/1000 | Loss: 0.00001502
Iteration 175/1000 | Loss: 0.00001502
Iteration 176/1000 | Loss: 0.00001502
Iteration 177/1000 | Loss: 0.00001502
Iteration 178/1000 | Loss: 0.00001501
Iteration 179/1000 | Loss: 0.00001501
Iteration 180/1000 | Loss: 0.00001501
Iteration 181/1000 | Loss: 0.00001500
Iteration 182/1000 | Loss: 0.00001500
Iteration 183/1000 | Loss: 0.00001500
Iteration 184/1000 | Loss: 0.00001500
Iteration 185/1000 | Loss: 0.00001500
Iteration 186/1000 | Loss: 0.00001500
Iteration 187/1000 | Loss: 0.00001500
Iteration 188/1000 | Loss: 0.00001500
Iteration 189/1000 | Loss: 0.00001500
Iteration 190/1000 | Loss: 0.00001500
Iteration 191/1000 | Loss: 0.00001500
Iteration 192/1000 | Loss: 0.00001499
Iteration 193/1000 | Loss: 0.00001499
Iteration 194/1000 | Loss: 0.00001499
Iteration 195/1000 | Loss: 0.00001499
Iteration 196/1000 | Loss: 0.00001499
Iteration 197/1000 | Loss: 0.00001499
Iteration 198/1000 | Loss: 0.00001499
Iteration 199/1000 | Loss: 0.00001499
Iteration 200/1000 | Loss: 0.00001499
Iteration 201/1000 | Loss: 0.00001499
Iteration 202/1000 | Loss: 0.00001499
Iteration 203/1000 | Loss: 0.00001499
Iteration 204/1000 | Loss: 0.00001499
Iteration 205/1000 | Loss: 0.00001499
Iteration 206/1000 | Loss: 0.00001499
Iteration 207/1000 | Loss: 0.00001499
Iteration 208/1000 | Loss: 0.00001499
Iteration 209/1000 | Loss: 0.00001499
Iteration 210/1000 | Loss: 0.00001499
Iteration 211/1000 | Loss: 0.00001499
Iteration 212/1000 | Loss: 0.00001499
Iteration 213/1000 | Loss: 0.00001499
Iteration 214/1000 | Loss: 0.00001499
Iteration 215/1000 | Loss: 0.00001499
Iteration 216/1000 | Loss: 0.00001499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.4985553207225166e-05, 1.4985553207225166e-05, 1.4985553207225166e-05, 1.4985553207225166e-05, 1.4985553207225166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4985553207225166e-05

Optimization complete. Final v2v error: 3.2536404132843018 mm

Highest mean error: 4.556392669677734 mm for frame 108

Lowest mean error: 2.8774757385253906 mm for frame 83

Saving results

Total time: 225.70660495758057
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417368
Iteration 2/25 | Loss: 0.00156545
Iteration 3/25 | Loss: 0.00131683
Iteration 4/25 | Loss: 0.00128574
Iteration 5/25 | Loss: 0.00128127
Iteration 6/25 | Loss: 0.00128002
Iteration 7/25 | Loss: 0.00128002
Iteration 8/25 | Loss: 0.00128002
Iteration 9/25 | Loss: 0.00128002
Iteration 10/25 | Loss: 0.00128002
Iteration 11/25 | Loss: 0.00128002
Iteration 12/25 | Loss: 0.00128002
Iteration 13/25 | Loss: 0.00128002
Iteration 14/25 | Loss: 0.00128002
Iteration 15/25 | Loss: 0.00128002
Iteration 16/25 | Loss: 0.00128002
Iteration 17/25 | Loss: 0.00128002
Iteration 18/25 | Loss: 0.00128002
Iteration 19/25 | Loss: 0.00128002
Iteration 20/25 | Loss: 0.00128002
Iteration 21/25 | Loss: 0.00128002
Iteration 22/25 | Loss: 0.00128002
Iteration 23/25 | Loss: 0.00128002
Iteration 24/25 | Loss: 0.00128002
Iteration 25/25 | Loss: 0.00128002

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42691505
Iteration 2/25 | Loss: 0.00085542
Iteration 3/25 | Loss: 0.00085542
Iteration 4/25 | Loss: 0.00085542
Iteration 5/25 | Loss: 0.00085541
Iteration 6/25 | Loss: 0.00085541
Iteration 7/25 | Loss: 0.00085541
Iteration 8/25 | Loss: 0.00085541
Iteration 9/25 | Loss: 0.00085541
Iteration 10/25 | Loss: 0.00085541
Iteration 11/25 | Loss: 0.00085541
Iteration 12/25 | Loss: 0.00085541
Iteration 13/25 | Loss: 0.00085541
Iteration 14/25 | Loss: 0.00085541
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008554129162803292, 0.0008554129162803292, 0.0008554129162803292, 0.0008554129162803292, 0.0008554129162803292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008554129162803292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085541
Iteration 2/1000 | Loss: 0.00003573
Iteration 3/1000 | Loss: 0.00002206
Iteration 4/1000 | Loss: 0.00001880
Iteration 5/1000 | Loss: 0.00001743
Iteration 6/1000 | Loss: 0.00001639
Iteration 7/1000 | Loss: 0.00001577
Iteration 8/1000 | Loss: 0.00001519
Iteration 9/1000 | Loss: 0.00001490
Iteration 10/1000 | Loss: 0.00001461
Iteration 11/1000 | Loss: 0.00001439
Iteration 12/1000 | Loss: 0.00001434
Iteration 13/1000 | Loss: 0.00001427
Iteration 14/1000 | Loss: 0.00001426
Iteration 15/1000 | Loss: 0.00001419
Iteration 16/1000 | Loss: 0.00001419
Iteration 17/1000 | Loss: 0.00001418
Iteration 18/1000 | Loss: 0.00001415
Iteration 19/1000 | Loss: 0.00001410
Iteration 20/1000 | Loss: 0.00001403
Iteration 21/1000 | Loss: 0.00001401
Iteration 22/1000 | Loss: 0.00001401
Iteration 23/1000 | Loss: 0.00001400
Iteration 24/1000 | Loss: 0.00001400
Iteration 25/1000 | Loss: 0.00001400
Iteration 26/1000 | Loss: 0.00001399
Iteration 27/1000 | Loss: 0.00001399
Iteration 28/1000 | Loss: 0.00001398
Iteration 29/1000 | Loss: 0.00001396
Iteration 30/1000 | Loss: 0.00001396
Iteration 31/1000 | Loss: 0.00001396
Iteration 32/1000 | Loss: 0.00001396
Iteration 33/1000 | Loss: 0.00001396
Iteration 34/1000 | Loss: 0.00001395
Iteration 35/1000 | Loss: 0.00001394
Iteration 36/1000 | Loss: 0.00001394
Iteration 37/1000 | Loss: 0.00001394
Iteration 38/1000 | Loss: 0.00001393
Iteration 39/1000 | Loss: 0.00001393
Iteration 40/1000 | Loss: 0.00001393
Iteration 41/1000 | Loss: 0.00001393
Iteration 42/1000 | Loss: 0.00001392
Iteration 43/1000 | Loss: 0.00001392
Iteration 44/1000 | Loss: 0.00001392
Iteration 45/1000 | Loss: 0.00001392
Iteration 46/1000 | Loss: 0.00001392
Iteration 47/1000 | Loss: 0.00001391
Iteration 48/1000 | Loss: 0.00001391
Iteration 49/1000 | Loss: 0.00001391
Iteration 50/1000 | Loss: 0.00001390
Iteration 51/1000 | Loss: 0.00001390
Iteration 52/1000 | Loss: 0.00001390
Iteration 53/1000 | Loss: 0.00001389
Iteration 54/1000 | Loss: 0.00001389
Iteration 55/1000 | Loss: 0.00001389
Iteration 56/1000 | Loss: 0.00001389
Iteration 57/1000 | Loss: 0.00001388
Iteration 58/1000 | Loss: 0.00001388
Iteration 59/1000 | Loss: 0.00001388
Iteration 60/1000 | Loss: 0.00001388
Iteration 61/1000 | Loss: 0.00001387
Iteration 62/1000 | Loss: 0.00001387
Iteration 63/1000 | Loss: 0.00001387
Iteration 64/1000 | Loss: 0.00001387
Iteration 65/1000 | Loss: 0.00001387
Iteration 66/1000 | Loss: 0.00001387
Iteration 67/1000 | Loss: 0.00001386
Iteration 68/1000 | Loss: 0.00001386
Iteration 69/1000 | Loss: 0.00001386
Iteration 70/1000 | Loss: 0.00001385
Iteration 71/1000 | Loss: 0.00001385
Iteration 72/1000 | Loss: 0.00001385
Iteration 73/1000 | Loss: 0.00001384
Iteration 74/1000 | Loss: 0.00001384
Iteration 75/1000 | Loss: 0.00001384
Iteration 76/1000 | Loss: 0.00001384
Iteration 77/1000 | Loss: 0.00001383
Iteration 78/1000 | Loss: 0.00001383
Iteration 79/1000 | Loss: 0.00001383
Iteration 80/1000 | Loss: 0.00001383
Iteration 81/1000 | Loss: 0.00001383
Iteration 82/1000 | Loss: 0.00001383
Iteration 83/1000 | Loss: 0.00001383
Iteration 84/1000 | Loss: 0.00001382
Iteration 85/1000 | Loss: 0.00001382
Iteration 86/1000 | Loss: 0.00001381
Iteration 87/1000 | Loss: 0.00001381
Iteration 88/1000 | Loss: 0.00001381
Iteration 89/1000 | Loss: 0.00001381
Iteration 90/1000 | Loss: 0.00001381
Iteration 91/1000 | Loss: 0.00001381
Iteration 92/1000 | Loss: 0.00001380
Iteration 93/1000 | Loss: 0.00001380
Iteration 94/1000 | Loss: 0.00001380
Iteration 95/1000 | Loss: 0.00001380
Iteration 96/1000 | Loss: 0.00001380
Iteration 97/1000 | Loss: 0.00001380
Iteration 98/1000 | Loss: 0.00001380
Iteration 99/1000 | Loss: 0.00001380
Iteration 100/1000 | Loss: 0.00001379
Iteration 101/1000 | Loss: 0.00001379
Iteration 102/1000 | Loss: 0.00001379
Iteration 103/1000 | Loss: 0.00001379
Iteration 104/1000 | Loss: 0.00001379
Iteration 105/1000 | Loss: 0.00001379
Iteration 106/1000 | Loss: 0.00001378
Iteration 107/1000 | Loss: 0.00001378
Iteration 108/1000 | Loss: 0.00001378
Iteration 109/1000 | Loss: 0.00001378
Iteration 110/1000 | Loss: 0.00001377
Iteration 111/1000 | Loss: 0.00001377
Iteration 112/1000 | Loss: 0.00001377
Iteration 113/1000 | Loss: 0.00001376
Iteration 114/1000 | Loss: 0.00001376
Iteration 115/1000 | Loss: 0.00001376
Iteration 116/1000 | Loss: 0.00001376
Iteration 117/1000 | Loss: 0.00001375
Iteration 118/1000 | Loss: 0.00001375
Iteration 119/1000 | Loss: 0.00001375
Iteration 120/1000 | Loss: 0.00001375
Iteration 121/1000 | Loss: 0.00001374
Iteration 122/1000 | Loss: 0.00001374
Iteration 123/1000 | Loss: 0.00001374
Iteration 124/1000 | Loss: 0.00001374
Iteration 125/1000 | Loss: 0.00001374
Iteration 126/1000 | Loss: 0.00001373
Iteration 127/1000 | Loss: 0.00001373
Iteration 128/1000 | Loss: 0.00001373
Iteration 129/1000 | Loss: 0.00001373
Iteration 130/1000 | Loss: 0.00001373
Iteration 131/1000 | Loss: 0.00001373
Iteration 132/1000 | Loss: 0.00001372
Iteration 133/1000 | Loss: 0.00001372
Iteration 134/1000 | Loss: 0.00001371
Iteration 135/1000 | Loss: 0.00001371
Iteration 136/1000 | Loss: 0.00001371
Iteration 137/1000 | Loss: 0.00001371
Iteration 138/1000 | Loss: 0.00001370
Iteration 139/1000 | Loss: 0.00001370
Iteration 140/1000 | Loss: 0.00001369
Iteration 141/1000 | Loss: 0.00001369
Iteration 142/1000 | Loss: 0.00001369
Iteration 143/1000 | Loss: 0.00001369
Iteration 144/1000 | Loss: 0.00001369
Iteration 145/1000 | Loss: 0.00001368
Iteration 146/1000 | Loss: 0.00001368
Iteration 147/1000 | Loss: 0.00001368
Iteration 148/1000 | Loss: 0.00001368
Iteration 149/1000 | Loss: 0.00001368
Iteration 150/1000 | Loss: 0.00001368
Iteration 151/1000 | Loss: 0.00001368
Iteration 152/1000 | Loss: 0.00001368
Iteration 153/1000 | Loss: 0.00001368
Iteration 154/1000 | Loss: 0.00001367
Iteration 155/1000 | Loss: 0.00001367
Iteration 156/1000 | Loss: 0.00001367
Iteration 157/1000 | Loss: 0.00001367
Iteration 158/1000 | Loss: 0.00001366
Iteration 159/1000 | Loss: 0.00001366
Iteration 160/1000 | Loss: 0.00001366
Iteration 161/1000 | Loss: 0.00001366
Iteration 162/1000 | Loss: 0.00001366
Iteration 163/1000 | Loss: 0.00001366
Iteration 164/1000 | Loss: 0.00001366
Iteration 165/1000 | Loss: 0.00001366
Iteration 166/1000 | Loss: 0.00001366
Iteration 167/1000 | Loss: 0.00001366
Iteration 168/1000 | Loss: 0.00001366
Iteration 169/1000 | Loss: 0.00001365
Iteration 170/1000 | Loss: 0.00001365
Iteration 171/1000 | Loss: 0.00001365
Iteration 172/1000 | Loss: 0.00001365
Iteration 173/1000 | Loss: 0.00001365
Iteration 174/1000 | Loss: 0.00001365
Iteration 175/1000 | Loss: 0.00001365
Iteration 176/1000 | Loss: 0.00001364
Iteration 177/1000 | Loss: 0.00001364
Iteration 178/1000 | Loss: 0.00001364
Iteration 179/1000 | Loss: 0.00001364
Iteration 180/1000 | Loss: 0.00001364
Iteration 181/1000 | Loss: 0.00001364
Iteration 182/1000 | Loss: 0.00001363
Iteration 183/1000 | Loss: 0.00001363
Iteration 184/1000 | Loss: 0.00001363
Iteration 185/1000 | Loss: 0.00001363
Iteration 186/1000 | Loss: 0.00001363
Iteration 187/1000 | Loss: 0.00001363
Iteration 188/1000 | Loss: 0.00001363
Iteration 189/1000 | Loss: 0.00001363
Iteration 190/1000 | Loss: 0.00001363
Iteration 191/1000 | Loss: 0.00001362
Iteration 192/1000 | Loss: 0.00001362
Iteration 193/1000 | Loss: 0.00001362
Iteration 194/1000 | Loss: 0.00001361
Iteration 195/1000 | Loss: 0.00001361
Iteration 196/1000 | Loss: 0.00001361
Iteration 197/1000 | Loss: 0.00001361
Iteration 198/1000 | Loss: 0.00001361
Iteration 199/1000 | Loss: 0.00001361
Iteration 200/1000 | Loss: 0.00001361
Iteration 201/1000 | Loss: 0.00001361
Iteration 202/1000 | Loss: 0.00001361
Iteration 203/1000 | Loss: 0.00001361
Iteration 204/1000 | Loss: 0.00001361
Iteration 205/1000 | Loss: 0.00001361
Iteration 206/1000 | Loss: 0.00001361
Iteration 207/1000 | Loss: 0.00001361
Iteration 208/1000 | Loss: 0.00001361
Iteration 209/1000 | Loss: 0.00001360
Iteration 210/1000 | Loss: 0.00001360
Iteration 211/1000 | Loss: 0.00001360
Iteration 212/1000 | Loss: 0.00001360
Iteration 213/1000 | Loss: 0.00001360
Iteration 214/1000 | Loss: 0.00001360
Iteration 215/1000 | Loss: 0.00001360
Iteration 216/1000 | Loss: 0.00001360
Iteration 217/1000 | Loss: 0.00001360
Iteration 218/1000 | Loss: 0.00001360
Iteration 219/1000 | Loss: 0.00001360
Iteration 220/1000 | Loss: 0.00001360
Iteration 221/1000 | Loss: 0.00001360
Iteration 222/1000 | Loss: 0.00001360
Iteration 223/1000 | Loss: 0.00001360
Iteration 224/1000 | Loss: 0.00001360
Iteration 225/1000 | Loss: 0.00001360
Iteration 226/1000 | Loss: 0.00001360
Iteration 227/1000 | Loss: 0.00001360
Iteration 228/1000 | Loss: 0.00001360
Iteration 229/1000 | Loss: 0.00001360
Iteration 230/1000 | Loss: 0.00001360
Iteration 231/1000 | Loss: 0.00001360
Iteration 232/1000 | Loss: 0.00001360
Iteration 233/1000 | Loss: 0.00001360
Iteration 234/1000 | Loss: 0.00001360
Iteration 235/1000 | Loss: 0.00001360
Iteration 236/1000 | Loss: 0.00001360
Iteration 237/1000 | Loss: 0.00001360
Iteration 238/1000 | Loss: 0.00001360
Iteration 239/1000 | Loss: 0.00001360
Iteration 240/1000 | Loss: 0.00001360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.3598400073533412e-05, 1.3598400073533412e-05, 1.3598400073533412e-05, 1.3598400073533412e-05, 1.3598400073533412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3598400073533412e-05

Optimization complete. Final v2v error: 3.164646625518799 mm

Highest mean error: 3.736757278442383 mm for frame 76

Lowest mean error: 2.9405956268310547 mm for frame 154

Saving results

Total time: 45.55635643005371
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811326
Iteration 2/25 | Loss: 0.00133176
Iteration 3/25 | Loss: 0.00124799
Iteration 4/25 | Loss: 0.00124271
Iteration 5/25 | Loss: 0.00124167
Iteration 6/25 | Loss: 0.00124167
Iteration 7/25 | Loss: 0.00124167
Iteration 8/25 | Loss: 0.00124167
Iteration 9/25 | Loss: 0.00124167
Iteration 10/25 | Loss: 0.00124167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001241674181073904, 0.001241674181073904, 0.001241674181073904, 0.001241674181073904, 0.001241674181073904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001241674181073904

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42229724
Iteration 2/25 | Loss: 0.00077278
Iteration 3/25 | Loss: 0.00077278
Iteration 4/25 | Loss: 0.00077278
Iteration 5/25 | Loss: 0.00077278
Iteration 6/25 | Loss: 0.00077278
Iteration 7/25 | Loss: 0.00077278
Iteration 8/25 | Loss: 0.00077278
Iteration 9/25 | Loss: 0.00077277
Iteration 10/25 | Loss: 0.00077277
Iteration 11/25 | Loss: 0.00077277
Iteration 12/25 | Loss: 0.00077277
Iteration 13/25 | Loss: 0.00077277
Iteration 14/25 | Loss: 0.00077277
Iteration 15/25 | Loss: 0.00077277
Iteration 16/25 | Loss: 0.00077277
Iteration 17/25 | Loss: 0.00077277
Iteration 18/25 | Loss: 0.00077277
Iteration 19/25 | Loss: 0.00077277
Iteration 20/25 | Loss: 0.00077277
Iteration 21/25 | Loss: 0.00077277
Iteration 22/25 | Loss: 0.00077277
Iteration 23/25 | Loss: 0.00077277
Iteration 24/25 | Loss: 0.00077277
Iteration 25/25 | Loss: 0.00077277

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077277
Iteration 2/1000 | Loss: 0.00002312
Iteration 3/1000 | Loss: 0.00001752
Iteration 4/1000 | Loss: 0.00001599
Iteration 5/1000 | Loss: 0.00001506
Iteration 6/1000 | Loss: 0.00001405
Iteration 7/1000 | Loss: 0.00001349
Iteration 8/1000 | Loss: 0.00001320
Iteration 9/1000 | Loss: 0.00001287
Iteration 10/1000 | Loss: 0.00001265
Iteration 11/1000 | Loss: 0.00001262
Iteration 12/1000 | Loss: 0.00001261
Iteration 13/1000 | Loss: 0.00001260
Iteration 14/1000 | Loss: 0.00001255
Iteration 15/1000 | Loss: 0.00001244
Iteration 16/1000 | Loss: 0.00001242
Iteration 17/1000 | Loss: 0.00001241
Iteration 18/1000 | Loss: 0.00001240
Iteration 19/1000 | Loss: 0.00001237
Iteration 20/1000 | Loss: 0.00001237
Iteration 21/1000 | Loss: 0.00001237
Iteration 22/1000 | Loss: 0.00001236
Iteration 23/1000 | Loss: 0.00001235
Iteration 24/1000 | Loss: 0.00001235
Iteration 25/1000 | Loss: 0.00001233
Iteration 26/1000 | Loss: 0.00001231
Iteration 27/1000 | Loss: 0.00001231
Iteration 28/1000 | Loss: 0.00001229
Iteration 29/1000 | Loss: 0.00001228
Iteration 30/1000 | Loss: 0.00001228
Iteration 31/1000 | Loss: 0.00001228
Iteration 32/1000 | Loss: 0.00001227
Iteration 33/1000 | Loss: 0.00001227
Iteration 34/1000 | Loss: 0.00001225
Iteration 35/1000 | Loss: 0.00001224
Iteration 36/1000 | Loss: 0.00001224
Iteration 37/1000 | Loss: 0.00001221
Iteration 38/1000 | Loss: 0.00001220
Iteration 39/1000 | Loss: 0.00001217
Iteration 40/1000 | Loss: 0.00001216
Iteration 41/1000 | Loss: 0.00001216
Iteration 42/1000 | Loss: 0.00001215
Iteration 43/1000 | Loss: 0.00001215
Iteration 44/1000 | Loss: 0.00001213
Iteration 45/1000 | Loss: 0.00001207
Iteration 46/1000 | Loss: 0.00001206
Iteration 47/1000 | Loss: 0.00001206
Iteration 48/1000 | Loss: 0.00001205
Iteration 49/1000 | Loss: 0.00001203
Iteration 50/1000 | Loss: 0.00001203
Iteration 51/1000 | Loss: 0.00001202
Iteration 52/1000 | Loss: 0.00001198
Iteration 53/1000 | Loss: 0.00001198
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001197
Iteration 57/1000 | Loss: 0.00001197
Iteration 58/1000 | Loss: 0.00001197
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001196
Iteration 61/1000 | Loss: 0.00001196
Iteration 62/1000 | Loss: 0.00001195
Iteration 63/1000 | Loss: 0.00001194
Iteration 64/1000 | Loss: 0.00001194
Iteration 65/1000 | Loss: 0.00001194
Iteration 66/1000 | Loss: 0.00001194
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001193
Iteration 69/1000 | Loss: 0.00001193
Iteration 70/1000 | Loss: 0.00001192
Iteration 71/1000 | Loss: 0.00001192
Iteration 72/1000 | Loss: 0.00001191
Iteration 73/1000 | Loss: 0.00001191
Iteration 74/1000 | Loss: 0.00001191
Iteration 75/1000 | Loss: 0.00001190
Iteration 76/1000 | Loss: 0.00001190
Iteration 77/1000 | Loss: 0.00001189
Iteration 78/1000 | Loss: 0.00001188
Iteration 79/1000 | Loss: 0.00001188
Iteration 80/1000 | Loss: 0.00001188
Iteration 81/1000 | Loss: 0.00001187
Iteration 82/1000 | Loss: 0.00001187
Iteration 83/1000 | Loss: 0.00001187
Iteration 84/1000 | Loss: 0.00001187
Iteration 85/1000 | Loss: 0.00001187
Iteration 86/1000 | Loss: 0.00001187
Iteration 87/1000 | Loss: 0.00001187
Iteration 88/1000 | Loss: 0.00001187
Iteration 89/1000 | Loss: 0.00001187
Iteration 90/1000 | Loss: 0.00001186
Iteration 91/1000 | Loss: 0.00001186
Iteration 92/1000 | Loss: 0.00001186
Iteration 93/1000 | Loss: 0.00001185
Iteration 94/1000 | Loss: 0.00001185
Iteration 95/1000 | Loss: 0.00001185
Iteration 96/1000 | Loss: 0.00001185
Iteration 97/1000 | Loss: 0.00001184
Iteration 98/1000 | Loss: 0.00001184
Iteration 99/1000 | Loss: 0.00001184
Iteration 100/1000 | Loss: 0.00001184
Iteration 101/1000 | Loss: 0.00001183
Iteration 102/1000 | Loss: 0.00001183
Iteration 103/1000 | Loss: 0.00001183
Iteration 104/1000 | Loss: 0.00001183
Iteration 105/1000 | Loss: 0.00001183
Iteration 106/1000 | Loss: 0.00001182
Iteration 107/1000 | Loss: 0.00001182
Iteration 108/1000 | Loss: 0.00001181
Iteration 109/1000 | Loss: 0.00001181
Iteration 110/1000 | Loss: 0.00001181
Iteration 111/1000 | Loss: 0.00001181
Iteration 112/1000 | Loss: 0.00001180
Iteration 113/1000 | Loss: 0.00001180
Iteration 114/1000 | Loss: 0.00001180
Iteration 115/1000 | Loss: 0.00001180
Iteration 116/1000 | Loss: 0.00001180
Iteration 117/1000 | Loss: 0.00001180
Iteration 118/1000 | Loss: 0.00001180
Iteration 119/1000 | Loss: 0.00001180
Iteration 120/1000 | Loss: 0.00001180
Iteration 121/1000 | Loss: 0.00001179
Iteration 122/1000 | Loss: 0.00001179
Iteration 123/1000 | Loss: 0.00001179
Iteration 124/1000 | Loss: 0.00001178
Iteration 125/1000 | Loss: 0.00001177
Iteration 126/1000 | Loss: 0.00001177
Iteration 127/1000 | Loss: 0.00001177
Iteration 128/1000 | Loss: 0.00001177
Iteration 129/1000 | Loss: 0.00001176
Iteration 130/1000 | Loss: 0.00001176
Iteration 131/1000 | Loss: 0.00001176
Iteration 132/1000 | Loss: 0.00001176
Iteration 133/1000 | Loss: 0.00001176
Iteration 134/1000 | Loss: 0.00001176
Iteration 135/1000 | Loss: 0.00001176
Iteration 136/1000 | Loss: 0.00001176
Iteration 137/1000 | Loss: 0.00001176
Iteration 138/1000 | Loss: 0.00001176
Iteration 139/1000 | Loss: 0.00001175
Iteration 140/1000 | Loss: 0.00001175
Iteration 141/1000 | Loss: 0.00001175
Iteration 142/1000 | Loss: 0.00001174
Iteration 143/1000 | Loss: 0.00001174
Iteration 144/1000 | Loss: 0.00001174
Iteration 145/1000 | Loss: 0.00001173
Iteration 146/1000 | Loss: 0.00001173
Iteration 147/1000 | Loss: 0.00001173
Iteration 148/1000 | Loss: 0.00001173
Iteration 149/1000 | Loss: 0.00001173
Iteration 150/1000 | Loss: 0.00001173
Iteration 151/1000 | Loss: 0.00001173
Iteration 152/1000 | Loss: 0.00001173
Iteration 153/1000 | Loss: 0.00001173
Iteration 154/1000 | Loss: 0.00001172
Iteration 155/1000 | Loss: 0.00001172
Iteration 156/1000 | Loss: 0.00001172
Iteration 157/1000 | Loss: 0.00001172
Iteration 158/1000 | Loss: 0.00001172
Iteration 159/1000 | Loss: 0.00001172
Iteration 160/1000 | Loss: 0.00001172
Iteration 161/1000 | Loss: 0.00001172
Iteration 162/1000 | Loss: 0.00001171
Iteration 163/1000 | Loss: 0.00001171
Iteration 164/1000 | Loss: 0.00001171
Iteration 165/1000 | Loss: 0.00001171
Iteration 166/1000 | Loss: 0.00001171
Iteration 167/1000 | Loss: 0.00001171
Iteration 168/1000 | Loss: 0.00001171
Iteration 169/1000 | Loss: 0.00001171
Iteration 170/1000 | Loss: 0.00001170
Iteration 171/1000 | Loss: 0.00001170
Iteration 172/1000 | Loss: 0.00001170
Iteration 173/1000 | Loss: 0.00001170
Iteration 174/1000 | Loss: 0.00001170
Iteration 175/1000 | Loss: 0.00001170
Iteration 176/1000 | Loss: 0.00001170
Iteration 177/1000 | Loss: 0.00001170
Iteration 178/1000 | Loss: 0.00001169
Iteration 179/1000 | Loss: 0.00001169
Iteration 180/1000 | Loss: 0.00001169
Iteration 181/1000 | Loss: 0.00001169
Iteration 182/1000 | Loss: 0.00001169
Iteration 183/1000 | Loss: 0.00001169
Iteration 184/1000 | Loss: 0.00001169
Iteration 185/1000 | Loss: 0.00001169
Iteration 186/1000 | Loss: 0.00001169
Iteration 187/1000 | Loss: 0.00001169
Iteration 188/1000 | Loss: 0.00001169
Iteration 189/1000 | Loss: 0.00001169
Iteration 190/1000 | Loss: 0.00001169
Iteration 191/1000 | Loss: 0.00001169
Iteration 192/1000 | Loss: 0.00001169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.1688188351399731e-05, 1.1688188351399731e-05, 1.1688188351399731e-05, 1.1688188351399731e-05, 1.1688188351399731e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1688188351399731e-05

Optimization complete. Final v2v error: 2.914625406265259 mm

Highest mean error: 3.089711904525757 mm for frame 106

Lowest mean error: 2.7615909576416016 mm for frame 203

Saving results

Total time: 46.13363218307495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475135
Iteration 2/25 | Loss: 0.00151055
Iteration 3/25 | Loss: 0.00139267
Iteration 4/25 | Loss: 0.00138310
Iteration 5/25 | Loss: 0.00138051
Iteration 6/25 | Loss: 0.00138051
Iteration 7/25 | Loss: 0.00138051
Iteration 8/25 | Loss: 0.00138051
Iteration 9/25 | Loss: 0.00138051
Iteration 10/25 | Loss: 0.00138051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001380506786517799, 0.001380506786517799, 0.001380506786517799, 0.001380506786517799, 0.001380506786517799]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001380506786517799

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40244508
Iteration 2/25 | Loss: 0.00112447
Iteration 3/25 | Loss: 0.00112445
Iteration 4/25 | Loss: 0.00112445
Iteration 5/25 | Loss: 0.00112444
Iteration 6/25 | Loss: 0.00112444
Iteration 7/25 | Loss: 0.00112444
Iteration 8/25 | Loss: 0.00112444
Iteration 9/25 | Loss: 0.00112444
Iteration 10/25 | Loss: 0.00112444
Iteration 11/25 | Loss: 0.00112444
Iteration 12/25 | Loss: 0.00112444
Iteration 13/25 | Loss: 0.00112444
Iteration 14/25 | Loss: 0.00112444
Iteration 15/25 | Loss: 0.00112444
Iteration 16/25 | Loss: 0.00112444
Iteration 17/25 | Loss: 0.00112444
Iteration 18/25 | Loss: 0.00112444
Iteration 19/25 | Loss: 0.00112444
Iteration 20/25 | Loss: 0.00112444
Iteration 21/25 | Loss: 0.00112444
Iteration 22/25 | Loss: 0.00112444
Iteration 23/25 | Loss: 0.00112444
Iteration 24/25 | Loss: 0.00112444
Iteration 25/25 | Loss: 0.00112444

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112444
Iteration 2/1000 | Loss: 0.00004753
Iteration 3/1000 | Loss: 0.00003069
Iteration 4/1000 | Loss: 0.00002494
Iteration 5/1000 | Loss: 0.00002345
Iteration 6/1000 | Loss: 0.00002213
Iteration 7/1000 | Loss: 0.00002134
Iteration 8/1000 | Loss: 0.00002078
Iteration 9/1000 | Loss: 0.00002049
Iteration 10/1000 | Loss: 0.00002012
Iteration 11/1000 | Loss: 0.00001986
Iteration 12/1000 | Loss: 0.00001966
Iteration 13/1000 | Loss: 0.00001958
Iteration 14/1000 | Loss: 0.00001951
Iteration 15/1000 | Loss: 0.00001949
Iteration 16/1000 | Loss: 0.00001948
Iteration 17/1000 | Loss: 0.00001945
Iteration 18/1000 | Loss: 0.00001944
Iteration 19/1000 | Loss: 0.00001940
Iteration 20/1000 | Loss: 0.00001930
Iteration 21/1000 | Loss: 0.00001926
Iteration 22/1000 | Loss: 0.00001922
Iteration 23/1000 | Loss: 0.00001922
Iteration 24/1000 | Loss: 0.00001921
Iteration 25/1000 | Loss: 0.00001920
Iteration 26/1000 | Loss: 0.00001919
Iteration 27/1000 | Loss: 0.00001918
Iteration 28/1000 | Loss: 0.00001915
Iteration 29/1000 | Loss: 0.00001912
Iteration 30/1000 | Loss: 0.00001912
Iteration 31/1000 | Loss: 0.00001908
Iteration 32/1000 | Loss: 0.00001908
Iteration 33/1000 | Loss: 0.00001906
Iteration 34/1000 | Loss: 0.00001905
Iteration 35/1000 | Loss: 0.00001905
Iteration 36/1000 | Loss: 0.00001904
Iteration 37/1000 | Loss: 0.00001904
Iteration 38/1000 | Loss: 0.00001903
Iteration 39/1000 | Loss: 0.00001903
Iteration 40/1000 | Loss: 0.00001902
Iteration 41/1000 | Loss: 0.00001902
Iteration 42/1000 | Loss: 0.00001900
Iteration 43/1000 | Loss: 0.00001900
Iteration 44/1000 | Loss: 0.00001895
Iteration 45/1000 | Loss: 0.00001888
Iteration 46/1000 | Loss: 0.00001888
Iteration 47/1000 | Loss: 0.00001886
Iteration 48/1000 | Loss: 0.00001885
Iteration 49/1000 | Loss: 0.00001885
Iteration 50/1000 | Loss: 0.00001881
Iteration 51/1000 | Loss: 0.00001878
Iteration 52/1000 | Loss: 0.00001878
Iteration 53/1000 | Loss: 0.00001877
Iteration 54/1000 | Loss: 0.00001876
Iteration 55/1000 | Loss: 0.00001874
Iteration 56/1000 | Loss: 0.00001874
Iteration 57/1000 | Loss: 0.00001874
Iteration 58/1000 | Loss: 0.00001874
Iteration 59/1000 | Loss: 0.00001874
Iteration 60/1000 | Loss: 0.00001874
Iteration 61/1000 | Loss: 0.00001874
Iteration 62/1000 | Loss: 0.00001874
Iteration 63/1000 | Loss: 0.00001874
Iteration 64/1000 | Loss: 0.00001873
Iteration 65/1000 | Loss: 0.00001873
Iteration 66/1000 | Loss: 0.00001873
Iteration 67/1000 | Loss: 0.00001873
Iteration 68/1000 | Loss: 0.00001873
Iteration 69/1000 | Loss: 0.00001873
Iteration 70/1000 | Loss: 0.00001871
Iteration 71/1000 | Loss: 0.00001871
Iteration 72/1000 | Loss: 0.00001871
Iteration 73/1000 | Loss: 0.00001870
Iteration 74/1000 | Loss: 0.00001870
Iteration 75/1000 | Loss: 0.00001870
Iteration 76/1000 | Loss: 0.00001869
Iteration 77/1000 | Loss: 0.00001869
Iteration 78/1000 | Loss: 0.00001868
Iteration 79/1000 | Loss: 0.00001868
Iteration 80/1000 | Loss: 0.00001867
Iteration 81/1000 | Loss: 0.00001867
Iteration 82/1000 | Loss: 0.00001867
Iteration 83/1000 | Loss: 0.00001867
Iteration 84/1000 | Loss: 0.00001866
Iteration 85/1000 | Loss: 0.00001866
Iteration 86/1000 | Loss: 0.00001866
Iteration 87/1000 | Loss: 0.00001866
Iteration 88/1000 | Loss: 0.00001865
Iteration 89/1000 | Loss: 0.00001865
Iteration 90/1000 | Loss: 0.00001865
Iteration 91/1000 | Loss: 0.00001865
Iteration 92/1000 | Loss: 0.00001865
Iteration 93/1000 | Loss: 0.00001865
Iteration 94/1000 | Loss: 0.00001865
Iteration 95/1000 | Loss: 0.00001864
Iteration 96/1000 | Loss: 0.00001864
Iteration 97/1000 | Loss: 0.00001864
Iteration 98/1000 | Loss: 0.00001864
Iteration 99/1000 | Loss: 0.00001864
Iteration 100/1000 | Loss: 0.00001863
Iteration 101/1000 | Loss: 0.00001863
Iteration 102/1000 | Loss: 0.00001863
Iteration 103/1000 | Loss: 0.00001863
Iteration 104/1000 | Loss: 0.00001863
Iteration 105/1000 | Loss: 0.00001862
Iteration 106/1000 | Loss: 0.00001862
Iteration 107/1000 | Loss: 0.00001862
Iteration 108/1000 | Loss: 0.00001861
Iteration 109/1000 | Loss: 0.00001861
Iteration 110/1000 | Loss: 0.00001861
Iteration 111/1000 | Loss: 0.00001861
Iteration 112/1000 | Loss: 0.00001861
Iteration 113/1000 | Loss: 0.00001860
Iteration 114/1000 | Loss: 0.00001860
Iteration 115/1000 | Loss: 0.00001860
Iteration 116/1000 | Loss: 0.00001860
Iteration 117/1000 | Loss: 0.00001860
Iteration 118/1000 | Loss: 0.00001859
Iteration 119/1000 | Loss: 0.00001859
Iteration 120/1000 | Loss: 0.00001859
Iteration 121/1000 | Loss: 0.00001859
Iteration 122/1000 | Loss: 0.00001859
Iteration 123/1000 | Loss: 0.00001858
Iteration 124/1000 | Loss: 0.00001858
Iteration 125/1000 | Loss: 0.00001858
Iteration 126/1000 | Loss: 0.00001858
Iteration 127/1000 | Loss: 0.00001858
Iteration 128/1000 | Loss: 0.00001857
Iteration 129/1000 | Loss: 0.00001857
Iteration 130/1000 | Loss: 0.00001857
Iteration 131/1000 | Loss: 0.00001856
Iteration 132/1000 | Loss: 0.00001856
Iteration 133/1000 | Loss: 0.00001856
Iteration 134/1000 | Loss: 0.00001855
Iteration 135/1000 | Loss: 0.00001855
Iteration 136/1000 | Loss: 0.00001855
Iteration 137/1000 | Loss: 0.00001855
Iteration 138/1000 | Loss: 0.00001855
Iteration 139/1000 | Loss: 0.00001855
Iteration 140/1000 | Loss: 0.00001855
Iteration 141/1000 | Loss: 0.00001854
Iteration 142/1000 | Loss: 0.00001854
Iteration 143/1000 | Loss: 0.00001854
Iteration 144/1000 | Loss: 0.00001854
Iteration 145/1000 | Loss: 0.00001854
Iteration 146/1000 | Loss: 0.00001854
Iteration 147/1000 | Loss: 0.00001853
Iteration 148/1000 | Loss: 0.00001853
Iteration 149/1000 | Loss: 0.00001853
Iteration 150/1000 | Loss: 0.00001853
Iteration 151/1000 | Loss: 0.00001852
Iteration 152/1000 | Loss: 0.00001852
Iteration 153/1000 | Loss: 0.00001852
Iteration 154/1000 | Loss: 0.00001852
Iteration 155/1000 | Loss: 0.00001852
Iteration 156/1000 | Loss: 0.00001852
Iteration 157/1000 | Loss: 0.00001852
Iteration 158/1000 | Loss: 0.00001852
Iteration 159/1000 | Loss: 0.00001852
Iteration 160/1000 | Loss: 0.00001851
Iteration 161/1000 | Loss: 0.00001851
Iteration 162/1000 | Loss: 0.00001851
Iteration 163/1000 | Loss: 0.00001850
Iteration 164/1000 | Loss: 0.00001850
Iteration 165/1000 | Loss: 0.00001850
Iteration 166/1000 | Loss: 0.00001850
Iteration 167/1000 | Loss: 0.00001849
Iteration 168/1000 | Loss: 0.00001849
Iteration 169/1000 | Loss: 0.00001849
Iteration 170/1000 | Loss: 0.00001849
Iteration 171/1000 | Loss: 0.00001849
Iteration 172/1000 | Loss: 0.00001848
Iteration 173/1000 | Loss: 0.00001848
Iteration 174/1000 | Loss: 0.00001848
Iteration 175/1000 | Loss: 0.00001847
Iteration 176/1000 | Loss: 0.00001847
Iteration 177/1000 | Loss: 0.00001847
Iteration 178/1000 | Loss: 0.00001846
Iteration 179/1000 | Loss: 0.00001846
Iteration 180/1000 | Loss: 0.00001846
Iteration 181/1000 | Loss: 0.00001846
Iteration 182/1000 | Loss: 0.00001846
Iteration 183/1000 | Loss: 0.00001846
Iteration 184/1000 | Loss: 0.00001845
Iteration 185/1000 | Loss: 0.00001845
Iteration 186/1000 | Loss: 0.00001845
Iteration 187/1000 | Loss: 0.00001845
Iteration 188/1000 | Loss: 0.00001845
Iteration 189/1000 | Loss: 0.00001844
Iteration 190/1000 | Loss: 0.00001844
Iteration 191/1000 | Loss: 0.00001844
Iteration 192/1000 | Loss: 0.00001844
Iteration 193/1000 | Loss: 0.00001844
Iteration 194/1000 | Loss: 0.00001844
Iteration 195/1000 | Loss: 0.00001843
Iteration 196/1000 | Loss: 0.00001843
Iteration 197/1000 | Loss: 0.00001843
Iteration 198/1000 | Loss: 0.00001843
Iteration 199/1000 | Loss: 0.00001843
Iteration 200/1000 | Loss: 0.00001843
Iteration 201/1000 | Loss: 0.00001843
Iteration 202/1000 | Loss: 0.00001843
Iteration 203/1000 | Loss: 0.00001843
Iteration 204/1000 | Loss: 0.00001843
Iteration 205/1000 | Loss: 0.00001843
Iteration 206/1000 | Loss: 0.00001843
Iteration 207/1000 | Loss: 0.00001843
Iteration 208/1000 | Loss: 0.00001843
Iteration 209/1000 | Loss: 0.00001843
Iteration 210/1000 | Loss: 0.00001843
Iteration 211/1000 | Loss: 0.00001843
Iteration 212/1000 | Loss: 0.00001843
Iteration 213/1000 | Loss: 0.00001843
Iteration 214/1000 | Loss: 0.00001843
Iteration 215/1000 | Loss: 0.00001843
Iteration 216/1000 | Loss: 0.00001843
Iteration 217/1000 | Loss: 0.00001843
Iteration 218/1000 | Loss: 0.00001843
Iteration 219/1000 | Loss: 0.00001843
Iteration 220/1000 | Loss: 0.00001843
Iteration 221/1000 | Loss: 0.00001843
Iteration 222/1000 | Loss: 0.00001843
Iteration 223/1000 | Loss: 0.00001843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.843026257120073e-05, 1.843026257120073e-05, 1.843026257120073e-05, 1.843026257120073e-05, 1.843026257120073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.843026257120073e-05

Optimization complete. Final v2v error: 3.6130661964416504 mm

Highest mean error: 3.998795509338379 mm for frame 66

Lowest mean error: 3.2474794387817383 mm for frame 189

Saving results

Total time: 54.8075737953186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00750342
Iteration 2/25 | Loss: 0.00152686
Iteration 3/25 | Loss: 0.00136074
Iteration 4/25 | Loss: 0.00133166
Iteration 5/25 | Loss: 0.00132653
Iteration 6/25 | Loss: 0.00132442
Iteration 7/25 | Loss: 0.00132242
Iteration 8/25 | Loss: 0.00133577
Iteration 9/25 | Loss: 0.00133450
Iteration 10/25 | Loss: 0.00133206
Iteration 11/25 | Loss: 0.00131639
Iteration 12/25 | Loss: 0.00131340
Iteration 13/25 | Loss: 0.00131289
Iteration 14/25 | Loss: 0.00131284
Iteration 15/25 | Loss: 0.00131284
Iteration 16/25 | Loss: 0.00131284
Iteration 17/25 | Loss: 0.00131284
Iteration 18/25 | Loss: 0.00131284
Iteration 19/25 | Loss: 0.00131284
Iteration 20/25 | Loss: 0.00131283
Iteration 21/25 | Loss: 0.00131283
Iteration 22/25 | Loss: 0.00131283
Iteration 23/25 | Loss: 0.00131283
Iteration 24/25 | Loss: 0.00131283
Iteration 25/25 | Loss: 0.00131283

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.02765083
Iteration 2/25 | Loss: 0.00097010
Iteration 3/25 | Loss: 0.00097003
Iteration 4/25 | Loss: 0.00097003
Iteration 5/25 | Loss: 0.00097003
Iteration 6/25 | Loss: 0.00097003
Iteration 7/25 | Loss: 0.00097003
Iteration 8/25 | Loss: 0.00097003
Iteration 9/25 | Loss: 0.00097003
Iteration 10/25 | Loss: 0.00097003
Iteration 11/25 | Loss: 0.00097003
Iteration 12/25 | Loss: 0.00097003
Iteration 13/25 | Loss: 0.00097003
Iteration 14/25 | Loss: 0.00097003
Iteration 15/25 | Loss: 0.00097003
Iteration 16/25 | Loss: 0.00097003
Iteration 17/25 | Loss: 0.00097003
Iteration 18/25 | Loss: 0.00097003
Iteration 19/25 | Loss: 0.00097003
Iteration 20/25 | Loss: 0.00097003
Iteration 21/25 | Loss: 0.00097003
Iteration 22/25 | Loss: 0.00097003
Iteration 23/25 | Loss: 0.00097003
Iteration 24/25 | Loss: 0.00097003
Iteration 25/25 | Loss: 0.00097003

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097003
Iteration 2/1000 | Loss: 0.00003923
Iteration 3/1000 | Loss: 0.00012563
Iteration 4/1000 | Loss: 0.00002502
Iteration 5/1000 | Loss: 0.00005406
Iteration 6/1000 | Loss: 0.00002249
Iteration 7/1000 | Loss: 0.00002146
Iteration 8/1000 | Loss: 0.00002086
Iteration 9/1000 | Loss: 0.00002044
Iteration 10/1000 | Loss: 0.00002006
Iteration 11/1000 | Loss: 0.00001971
Iteration 12/1000 | Loss: 0.00001952
Iteration 13/1000 | Loss: 0.00001932
Iteration 14/1000 | Loss: 0.00001931
Iteration 15/1000 | Loss: 0.00001927
Iteration 16/1000 | Loss: 0.00001926
Iteration 17/1000 | Loss: 0.00001923
Iteration 18/1000 | Loss: 0.00001920
Iteration 19/1000 | Loss: 0.00001914
Iteration 20/1000 | Loss: 0.00001913
Iteration 21/1000 | Loss: 0.00001909
Iteration 22/1000 | Loss: 0.00001906
Iteration 23/1000 | Loss: 0.00001904
Iteration 24/1000 | Loss: 0.00001903
Iteration 25/1000 | Loss: 0.00001903
Iteration 26/1000 | Loss: 0.00001903
Iteration 27/1000 | Loss: 0.00001902
Iteration 28/1000 | Loss: 0.00001902
Iteration 29/1000 | Loss: 0.00001901
Iteration 30/1000 | Loss: 0.00001898
Iteration 31/1000 | Loss: 0.00001897
Iteration 32/1000 | Loss: 0.00001893
Iteration 33/1000 | Loss: 0.00001893
Iteration 34/1000 | Loss: 0.00001892
Iteration 35/1000 | Loss: 0.00001891
Iteration 36/1000 | Loss: 0.00001891
Iteration 37/1000 | Loss: 0.00001891
Iteration 38/1000 | Loss: 0.00001890
Iteration 39/1000 | Loss: 0.00001890
Iteration 40/1000 | Loss: 0.00001889
Iteration 41/1000 | Loss: 0.00001889
Iteration 42/1000 | Loss: 0.00001888
Iteration 43/1000 | Loss: 0.00001888
Iteration 44/1000 | Loss: 0.00001888
Iteration 45/1000 | Loss: 0.00001887
Iteration 46/1000 | Loss: 0.00001887
Iteration 47/1000 | Loss: 0.00001887
Iteration 48/1000 | Loss: 0.00001886
Iteration 49/1000 | Loss: 0.00001885
Iteration 50/1000 | Loss: 0.00001885
Iteration 51/1000 | Loss: 0.00001885
Iteration 52/1000 | Loss: 0.00001885
Iteration 53/1000 | Loss: 0.00001885
Iteration 54/1000 | Loss: 0.00001885
Iteration 55/1000 | Loss: 0.00001884
Iteration 56/1000 | Loss: 0.00001884
Iteration 57/1000 | Loss: 0.00001884
Iteration 58/1000 | Loss: 0.00001884
Iteration 59/1000 | Loss: 0.00001883
Iteration 60/1000 | Loss: 0.00001883
Iteration 61/1000 | Loss: 0.00001883
Iteration 62/1000 | Loss: 0.00001882
Iteration 63/1000 | Loss: 0.00001881
Iteration 64/1000 | Loss: 0.00001881
Iteration 65/1000 | Loss: 0.00001880
Iteration 66/1000 | Loss: 0.00001880
Iteration 67/1000 | Loss: 0.00001880
Iteration 68/1000 | Loss: 0.00001879
Iteration 69/1000 | Loss: 0.00001879
Iteration 70/1000 | Loss: 0.00001879
Iteration 71/1000 | Loss: 0.00001878
Iteration 72/1000 | Loss: 0.00001878
Iteration 73/1000 | Loss: 0.00001878
Iteration 74/1000 | Loss: 0.00001878
Iteration 75/1000 | Loss: 0.00001878
Iteration 76/1000 | Loss: 0.00001878
Iteration 77/1000 | Loss: 0.00001878
Iteration 78/1000 | Loss: 0.00001878
Iteration 79/1000 | Loss: 0.00001878
Iteration 80/1000 | Loss: 0.00001878
Iteration 81/1000 | Loss: 0.00001878
Iteration 82/1000 | Loss: 0.00001878
Iteration 83/1000 | Loss: 0.00001877
Iteration 84/1000 | Loss: 0.00001877
Iteration 85/1000 | Loss: 0.00001876
Iteration 86/1000 | Loss: 0.00001876
Iteration 87/1000 | Loss: 0.00001876
Iteration 88/1000 | Loss: 0.00001875
Iteration 89/1000 | Loss: 0.00001875
Iteration 90/1000 | Loss: 0.00001875
Iteration 91/1000 | Loss: 0.00001875
Iteration 92/1000 | Loss: 0.00001874
Iteration 93/1000 | Loss: 0.00001874
Iteration 94/1000 | Loss: 0.00001874
Iteration 95/1000 | Loss: 0.00001874
Iteration 96/1000 | Loss: 0.00001874
Iteration 97/1000 | Loss: 0.00001874
Iteration 98/1000 | Loss: 0.00001874
Iteration 99/1000 | Loss: 0.00001874
Iteration 100/1000 | Loss: 0.00001874
Iteration 101/1000 | Loss: 0.00001874
Iteration 102/1000 | Loss: 0.00001874
Iteration 103/1000 | Loss: 0.00001874
Iteration 104/1000 | Loss: 0.00001873
Iteration 105/1000 | Loss: 0.00001873
Iteration 106/1000 | Loss: 0.00001873
Iteration 107/1000 | Loss: 0.00001872
Iteration 108/1000 | Loss: 0.00001872
Iteration 109/1000 | Loss: 0.00001872
Iteration 110/1000 | Loss: 0.00001871
Iteration 111/1000 | Loss: 0.00001871
Iteration 112/1000 | Loss: 0.00001871
Iteration 113/1000 | Loss: 0.00001871
Iteration 114/1000 | Loss: 0.00001870
Iteration 115/1000 | Loss: 0.00001870
Iteration 116/1000 | Loss: 0.00001870
Iteration 117/1000 | Loss: 0.00001870
Iteration 118/1000 | Loss: 0.00001870
Iteration 119/1000 | Loss: 0.00001869
Iteration 120/1000 | Loss: 0.00001869
Iteration 121/1000 | Loss: 0.00001869
Iteration 122/1000 | Loss: 0.00001869
Iteration 123/1000 | Loss: 0.00001868
Iteration 124/1000 | Loss: 0.00001868
Iteration 125/1000 | Loss: 0.00001868
Iteration 126/1000 | Loss: 0.00001868
Iteration 127/1000 | Loss: 0.00001868
Iteration 128/1000 | Loss: 0.00001868
Iteration 129/1000 | Loss: 0.00001867
Iteration 130/1000 | Loss: 0.00001867
Iteration 131/1000 | Loss: 0.00001867
Iteration 132/1000 | Loss: 0.00001867
Iteration 133/1000 | Loss: 0.00001867
Iteration 134/1000 | Loss: 0.00001867
Iteration 135/1000 | Loss: 0.00001867
Iteration 136/1000 | Loss: 0.00001867
Iteration 137/1000 | Loss: 0.00001867
Iteration 138/1000 | Loss: 0.00001867
Iteration 139/1000 | Loss: 0.00001866
Iteration 140/1000 | Loss: 0.00001866
Iteration 141/1000 | Loss: 0.00001866
Iteration 142/1000 | Loss: 0.00001866
Iteration 143/1000 | Loss: 0.00001866
Iteration 144/1000 | Loss: 0.00001865
Iteration 145/1000 | Loss: 0.00001865
Iteration 146/1000 | Loss: 0.00001865
Iteration 147/1000 | Loss: 0.00001865
Iteration 148/1000 | Loss: 0.00001865
Iteration 149/1000 | Loss: 0.00001865
Iteration 150/1000 | Loss: 0.00001865
Iteration 151/1000 | Loss: 0.00001865
Iteration 152/1000 | Loss: 0.00001865
Iteration 153/1000 | Loss: 0.00001864
Iteration 154/1000 | Loss: 0.00001864
Iteration 155/1000 | Loss: 0.00001864
Iteration 156/1000 | Loss: 0.00001864
Iteration 157/1000 | Loss: 0.00001864
Iteration 158/1000 | Loss: 0.00001864
Iteration 159/1000 | Loss: 0.00001864
Iteration 160/1000 | Loss: 0.00001864
Iteration 161/1000 | Loss: 0.00001864
Iteration 162/1000 | Loss: 0.00001864
Iteration 163/1000 | Loss: 0.00001864
Iteration 164/1000 | Loss: 0.00001863
Iteration 165/1000 | Loss: 0.00001863
Iteration 166/1000 | Loss: 0.00001863
Iteration 167/1000 | Loss: 0.00001863
Iteration 168/1000 | Loss: 0.00001863
Iteration 169/1000 | Loss: 0.00001863
Iteration 170/1000 | Loss: 0.00001863
Iteration 171/1000 | Loss: 0.00001863
Iteration 172/1000 | Loss: 0.00001863
Iteration 173/1000 | Loss: 0.00001863
Iteration 174/1000 | Loss: 0.00001863
Iteration 175/1000 | Loss: 0.00001863
Iteration 176/1000 | Loss: 0.00001863
Iteration 177/1000 | Loss: 0.00001863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.8626056771609e-05, 1.8626056771609e-05, 1.8626056771609e-05, 1.8626056771609e-05, 1.8626056771609e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8626056771609e-05

Optimization complete. Final v2v error: 3.593987226486206 mm

Highest mean error: 4.5509138107299805 mm for frame 138

Lowest mean error: 3.048171043395996 mm for frame 190

Saving results

Total time: 67.01854157447815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429726
Iteration 2/25 | Loss: 0.00131569
Iteration 3/25 | Loss: 0.00126738
Iteration 4/25 | Loss: 0.00125771
Iteration 5/25 | Loss: 0.00125452
Iteration 6/25 | Loss: 0.00125444
Iteration 7/25 | Loss: 0.00125444
Iteration 8/25 | Loss: 0.00125444
Iteration 9/25 | Loss: 0.00125444
Iteration 10/25 | Loss: 0.00125444
Iteration 11/25 | Loss: 0.00125444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012544395867735147, 0.0012544395867735147, 0.0012544395867735147, 0.0012544395867735147, 0.0012544395867735147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012544395867735147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.58974361
Iteration 2/25 | Loss: 0.00079582
Iteration 3/25 | Loss: 0.00079581
Iteration 4/25 | Loss: 0.00079581
Iteration 5/25 | Loss: 0.00079581
Iteration 6/25 | Loss: 0.00079581
Iteration 7/25 | Loss: 0.00079581
Iteration 8/25 | Loss: 0.00079581
Iteration 9/25 | Loss: 0.00079581
Iteration 10/25 | Loss: 0.00079581
Iteration 11/25 | Loss: 0.00079581
Iteration 12/25 | Loss: 0.00079581
Iteration 13/25 | Loss: 0.00079581
Iteration 14/25 | Loss: 0.00079581
Iteration 15/25 | Loss: 0.00079581
Iteration 16/25 | Loss: 0.00079581
Iteration 17/25 | Loss: 0.00079581
Iteration 18/25 | Loss: 0.00079581
Iteration 19/25 | Loss: 0.00079581
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007958094356581569, 0.0007958094356581569, 0.0007958094356581569, 0.0007958094356581569, 0.0007958094356581569]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007958094356581569

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079581
Iteration 2/1000 | Loss: 0.00002589
Iteration 3/1000 | Loss: 0.00001862
Iteration 4/1000 | Loss: 0.00001731
Iteration 5/1000 | Loss: 0.00001653
Iteration 6/1000 | Loss: 0.00001604
Iteration 7/1000 | Loss: 0.00001582
Iteration 8/1000 | Loss: 0.00001548
Iteration 9/1000 | Loss: 0.00001521
Iteration 10/1000 | Loss: 0.00001502
Iteration 11/1000 | Loss: 0.00001496
Iteration 12/1000 | Loss: 0.00001495
Iteration 13/1000 | Loss: 0.00001494
Iteration 14/1000 | Loss: 0.00001494
Iteration 15/1000 | Loss: 0.00001492
Iteration 16/1000 | Loss: 0.00001485
Iteration 17/1000 | Loss: 0.00001484
Iteration 18/1000 | Loss: 0.00001483
Iteration 19/1000 | Loss: 0.00001482
Iteration 20/1000 | Loss: 0.00001478
Iteration 21/1000 | Loss: 0.00001477
Iteration 22/1000 | Loss: 0.00001477
Iteration 23/1000 | Loss: 0.00001472
Iteration 24/1000 | Loss: 0.00001471
Iteration 25/1000 | Loss: 0.00001471
Iteration 26/1000 | Loss: 0.00001470
Iteration 27/1000 | Loss: 0.00001469
Iteration 28/1000 | Loss: 0.00001469
Iteration 29/1000 | Loss: 0.00001469
Iteration 30/1000 | Loss: 0.00001467
Iteration 31/1000 | Loss: 0.00001467
Iteration 32/1000 | Loss: 0.00001467
Iteration 33/1000 | Loss: 0.00001467
Iteration 34/1000 | Loss: 0.00001466
Iteration 35/1000 | Loss: 0.00001466
Iteration 36/1000 | Loss: 0.00001466
Iteration 37/1000 | Loss: 0.00001465
Iteration 38/1000 | Loss: 0.00001465
Iteration 39/1000 | Loss: 0.00001465
Iteration 40/1000 | Loss: 0.00001464
Iteration 41/1000 | Loss: 0.00001463
Iteration 42/1000 | Loss: 0.00001463
Iteration 43/1000 | Loss: 0.00001462
Iteration 44/1000 | Loss: 0.00001462
Iteration 45/1000 | Loss: 0.00001462
Iteration 46/1000 | Loss: 0.00001462
Iteration 47/1000 | Loss: 0.00001461
Iteration 48/1000 | Loss: 0.00001460
Iteration 49/1000 | Loss: 0.00001459
Iteration 50/1000 | Loss: 0.00001452
Iteration 51/1000 | Loss: 0.00001450
Iteration 52/1000 | Loss: 0.00001450
Iteration 53/1000 | Loss: 0.00001449
Iteration 54/1000 | Loss: 0.00001449
Iteration 55/1000 | Loss: 0.00001449
Iteration 56/1000 | Loss: 0.00001448
Iteration 57/1000 | Loss: 0.00001448
Iteration 58/1000 | Loss: 0.00001448
Iteration 59/1000 | Loss: 0.00001445
Iteration 60/1000 | Loss: 0.00001445
Iteration 61/1000 | Loss: 0.00001443
Iteration 62/1000 | Loss: 0.00001443
Iteration 63/1000 | Loss: 0.00001442
Iteration 64/1000 | Loss: 0.00001442
Iteration 65/1000 | Loss: 0.00001441
Iteration 66/1000 | Loss: 0.00001441
Iteration 67/1000 | Loss: 0.00001441
Iteration 68/1000 | Loss: 0.00001440
Iteration 69/1000 | Loss: 0.00001440
Iteration 70/1000 | Loss: 0.00001440
Iteration 71/1000 | Loss: 0.00001440
Iteration 72/1000 | Loss: 0.00001440
Iteration 73/1000 | Loss: 0.00001439
Iteration 74/1000 | Loss: 0.00001438
Iteration 75/1000 | Loss: 0.00001438
Iteration 76/1000 | Loss: 0.00001437
Iteration 77/1000 | Loss: 0.00001437
Iteration 78/1000 | Loss: 0.00001436
Iteration 79/1000 | Loss: 0.00001436
Iteration 80/1000 | Loss: 0.00001435
Iteration 81/1000 | Loss: 0.00001434
Iteration 82/1000 | Loss: 0.00001433
Iteration 83/1000 | Loss: 0.00001433
Iteration 84/1000 | Loss: 0.00001430
Iteration 85/1000 | Loss: 0.00001430
Iteration 86/1000 | Loss: 0.00001429
Iteration 87/1000 | Loss: 0.00001429
Iteration 88/1000 | Loss: 0.00001429
Iteration 89/1000 | Loss: 0.00001429
Iteration 90/1000 | Loss: 0.00001429
Iteration 91/1000 | Loss: 0.00001429
Iteration 92/1000 | Loss: 0.00001428
Iteration 93/1000 | Loss: 0.00001428
Iteration 94/1000 | Loss: 0.00001428
Iteration 95/1000 | Loss: 0.00001428
Iteration 96/1000 | Loss: 0.00001428
Iteration 97/1000 | Loss: 0.00001428
Iteration 98/1000 | Loss: 0.00001428
Iteration 99/1000 | Loss: 0.00001428
Iteration 100/1000 | Loss: 0.00001428
Iteration 101/1000 | Loss: 0.00001428
Iteration 102/1000 | Loss: 0.00001428
Iteration 103/1000 | Loss: 0.00001428
Iteration 104/1000 | Loss: 0.00001428
Iteration 105/1000 | Loss: 0.00001428
Iteration 106/1000 | Loss: 0.00001428
Iteration 107/1000 | Loss: 0.00001428
Iteration 108/1000 | Loss: 0.00001428
Iteration 109/1000 | Loss: 0.00001428
Iteration 110/1000 | Loss: 0.00001428
Iteration 111/1000 | Loss: 0.00001428
Iteration 112/1000 | Loss: 0.00001428
Iteration 113/1000 | Loss: 0.00001428
Iteration 114/1000 | Loss: 0.00001428
Iteration 115/1000 | Loss: 0.00001428
Iteration 116/1000 | Loss: 0.00001428
Iteration 117/1000 | Loss: 0.00001428
Iteration 118/1000 | Loss: 0.00001428
Iteration 119/1000 | Loss: 0.00001428
Iteration 120/1000 | Loss: 0.00001428
Iteration 121/1000 | Loss: 0.00001428
Iteration 122/1000 | Loss: 0.00001428
Iteration 123/1000 | Loss: 0.00001428
Iteration 124/1000 | Loss: 0.00001428
Iteration 125/1000 | Loss: 0.00001428
Iteration 126/1000 | Loss: 0.00001428
Iteration 127/1000 | Loss: 0.00001428
Iteration 128/1000 | Loss: 0.00001428
Iteration 129/1000 | Loss: 0.00001428
Iteration 130/1000 | Loss: 0.00001428
Iteration 131/1000 | Loss: 0.00001428
Iteration 132/1000 | Loss: 0.00001428
Iteration 133/1000 | Loss: 0.00001428
Iteration 134/1000 | Loss: 0.00001428
Iteration 135/1000 | Loss: 0.00001428
Iteration 136/1000 | Loss: 0.00001428
Iteration 137/1000 | Loss: 0.00001428
Iteration 138/1000 | Loss: 0.00001428
Iteration 139/1000 | Loss: 0.00001428
Iteration 140/1000 | Loss: 0.00001428
Iteration 141/1000 | Loss: 0.00001428
Iteration 142/1000 | Loss: 0.00001428
Iteration 143/1000 | Loss: 0.00001428
Iteration 144/1000 | Loss: 0.00001428
Iteration 145/1000 | Loss: 0.00001428
Iteration 146/1000 | Loss: 0.00001428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.4280325558502227e-05, 1.4280325558502227e-05, 1.4280325558502227e-05, 1.4280325558502227e-05, 1.4280325558502227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4280325558502227e-05

Optimization complete. Final v2v error: 3.2225608825683594 mm

Highest mean error: 3.646199941635132 mm for frame 123

Lowest mean error: 2.902574300765991 mm for frame 192

Saving results

Total time: 37.39347243309021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964174
Iteration 2/25 | Loss: 0.00180703
Iteration 3/25 | Loss: 0.00155843
Iteration 4/25 | Loss: 0.00150706
Iteration 5/25 | Loss: 0.00148821
Iteration 6/25 | Loss: 0.00148357
Iteration 7/25 | Loss: 0.00148317
Iteration 8/25 | Loss: 0.00148317
Iteration 9/25 | Loss: 0.00148317
Iteration 10/25 | Loss: 0.00148317
Iteration 11/25 | Loss: 0.00148317
Iteration 12/25 | Loss: 0.00148317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014831688022240996, 0.0014831688022240996, 0.0014831688022240996, 0.0014831688022240996, 0.0014831688022240996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014831688022240996

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31175423
Iteration 2/25 | Loss: 0.00221702
Iteration 3/25 | Loss: 0.00221668
Iteration 4/25 | Loss: 0.00221668
Iteration 5/25 | Loss: 0.00221668
Iteration 6/25 | Loss: 0.00221668
Iteration 7/25 | Loss: 0.00221668
Iteration 8/25 | Loss: 0.00221668
Iteration 9/25 | Loss: 0.00221668
Iteration 10/25 | Loss: 0.00221668
Iteration 11/25 | Loss: 0.00221668
Iteration 12/25 | Loss: 0.00221668
Iteration 13/25 | Loss: 0.00221668
Iteration 14/25 | Loss: 0.00221668
Iteration 15/25 | Loss: 0.00221668
Iteration 16/25 | Loss: 0.00221668
Iteration 17/25 | Loss: 0.00221668
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0022166771814227104, 0.0022166771814227104, 0.0022166771814227104, 0.0022166771814227104, 0.0022166771814227104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022166771814227104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221668
Iteration 2/1000 | Loss: 0.00022767
Iteration 3/1000 | Loss: 0.00015386
Iteration 4/1000 | Loss: 0.00013219
Iteration 5/1000 | Loss: 0.00011856
Iteration 6/1000 | Loss: 0.00011360
Iteration 7/1000 | Loss: 0.00010956
Iteration 8/1000 | Loss: 0.00010498
Iteration 9/1000 | Loss: 0.00010182
Iteration 10/1000 | Loss: 0.00009857
Iteration 11/1000 | Loss: 0.00009605
Iteration 12/1000 | Loss: 0.00009377
Iteration 13/1000 | Loss: 0.00009199
Iteration 14/1000 | Loss: 0.00052330
Iteration 15/1000 | Loss: 0.00084957
Iteration 16/1000 | Loss: 0.00065988
Iteration 17/1000 | Loss: 0.00107358
Iteration 18/1000 | Loss: 0.00008963
Iteration 19/1000 | Loss: 0.00008314
Iteration 20/1000 | Loss: 0.00007998
Iteration 21/1000 | Loss: 0.00170628
Iteration 22/1000 | Loss: 0.00082512
Iteration 23/1000 | Loss: 0.00029070
Iteration 24/1000 | Loss: 0.00006651
Iteration 25/1000 | Loss: 0.00006159
Iteration 26/1000 | Loss: 0.00096925
Iteration 27/1000 | Loss: 0.00005675
Iteration 28/1000 | Loss: 0.00005254
Iteration 29/1000 | Loss: 0.00004832
Iteration 30/1000 | Loss: 0.00004628
Iteration 31/1000 | Loss: 0.00004450
Iteration 32/1000 | Loss: 0.00004314
Iteration 33/1000 | Loss: 0.00004242
Iteration 34/1000 | Loss: 0.00004177
Iteration 35/1000 | Loss: 0.00004122
Iteration 36/1000 | Loss: 0.00004083
Iteration 37/1000 | Loss: 0.00004047
Iteration 38/1000 | Loss: 0.00004020
Iteration 39/1000 | Loss: 0.00003993
Iteration 40/1000 | Loss: 0.00003983
Iteration 41/1000 | Loss: 0.00003966
Iteration 42/1000 | Loss: 0.00003957
Iteration 43/1000 | Loss: 0.00003953
Iteration 44/1000 | Loss: 0.00003951
Iteration 45/1000 | Loss: 0.00003950
Iteration 46/1000 | Loss: 0.00003949
Iteration 47/1000 | Loss: 0.00003948
Iteration 48/1000 | Loss: 0.00003947
Iteration 49/1000 | Loss: 0.00003946
Iteration 50/1000 | Loss: 0.00003943
Iteration 51/1000 | Loss: 0.00003943
Iteration 52/1000 | Loss: 0.00003943
Iteration 53/1000 | Loss: 0.00003943
Iteration 54/1000 | Loss: 0.00003942
Iteration 55/1000 | Loss: 0.00003942
Iteration 56/1000 | Loss: 0.00003941
Iteration 57/1000 | Loss: 0.00003938
Iteration 58/1000 | Loss: 0.00003938
Iteration 59/1000 | Loss: 0.00003936
Iteration 60/1000 | Loss: 0.00003935
Iteration 61/1000 | Loss: 0.00003935
Iteration 62/1000 | Loss: 0.00003935
Iteration 63/1000 | Loss: 0.00003935
Iteration 64/1000 | Loss: 0.00003934
Iteration 65/1000 | Loss: 0.00003934
Iteration 66/1000 | Loss: 0.00003934
Iteration 67/1000 | Loss: 0.00003934
Iteration 68/1000 | Loss: 0.00003934
Iteration 69/1000 | Loss: 0.00003934
Iteration 70/1000 | Loss: 0.00003933
Iteration 71/1000 | Loss: 0.00003933
Iteration 72/1000 | Loss: 0.00003933
Iteration 73/1000 | Loss: 0.00003932
Iteration 74/1000 | Loss: 0.00003932
Iteration 75/1000 | Loss: 0.00003931
Iteration 76/1000 | Loss: 0.00003931
Iteration 77/1000 | Loss: 0.00003931
Iteration 78/1000 | Loss: 0.00003930
Iteration 79/1000 | Loss: 0.00003930
Iteration 80/1000 | Loss: 0.00003930
Iteration 81/1000 | Loss: 0.00003929
Iteration 82/1000 | Loss: 0.00003929
Iteration 83/1000 | Loss: 0.00003928
Iteration 84/1000 | Loss: 0.00003928
Iteration 85/1000 | Loss: 0.00003928
Iteration 86/1000 | Loss: 0.00003927
Iteration 87/1000 | Loss: 0.00003927
Iteration 88/1000 | Loss: 0.00003927
Iteration 89/1000 | Loss: 0.00003926
Iteration 90/1000 | Loss: 0.00003926
Iteration 91/1000 | Loss: 0.00003926
Iteration 92/1000 | Loss: 0.00003924
Iteration 93/1000 | Loss: 0.00003924
Iteration 94/1000 | Loss: 0.00003923
Iteration 95/1000 | Loss: 0.00003923
Iteration 96/1000 | Loss: 0.00003922
Iteration 97/1000 | Loss: 0.00003922
Iteration 98/1000 | Loss: 0.00003921
Iteration 99/1000 | Loss: 0.00003921
Iteration 100/1000 | Loss: 0.00003920
Iteration 101/1000 | Loss: 0.00003920
Iteration 102/1000 | Loss: 0.00003919
Iteration 103/1000 | Loss: 0.00003919
Iteration 104/1000 | Loss: 0.00003919
Iteration 105/1000 | Loss: 0.00003918
Iteration 106/1000 | Loss: 0.00003918
Iteration 107/1000 | Loss: 0.00003918
Iteration 108/1000 | Loss: 0.00003917
Iteration 109/1000 | Loss: 0.00003917
Iteration 110/1000 | Loss: 0.00003916
Iteration 111/1000 | Loss: 0.00003916
Iteration 112/1000 | Loss: 0.00003915
Iteration 113/1000 | Loss: 0.00003915
Iteration 114/1000 | Loss: 0.00003915
Iteration 115/1000 | Loss: 0.00003914
Iteration 116/1000 | Loss: 0.00003914
Iteration 117/1000 | Loss: 0.00003914
Iteration 118/1000 | Loss: 0.00003913
Iteration 119/1000 | Loss: 0.00003913
Iteration 120/1000 | Loss: 0.00003913
Iteration 121/1000 | Loss: 0.00003912
Iteration 122/1000 | Loss: 0.00003912
Iteration 123/1000 | Loss: 0.00003912
Iteration 124/1000 | Loss: 0.00003911
Iteration 125/1000 | Loss: 0.00003911
Iteration 126/1000 | Loss: 0.00003911
Iteration 127/1000 | Loss: 0.00003911
Iteration 128/1000 | Loss: 0.00003910
Iteration 129/1000 | Loss: 0.00003910
Iteration 130/1000 | Loss: 0.00003909
Iteration 131/1000 | Loss: 0.00003909
Iteration 132/1000 | Loss: 0.00003909
Iteration 133/1000 | Loss: 0.00003909
Iteration 134/1000 | Loss: 0.00003908
Iteration 135/1000 | Loss: 0.00003908
Iteration 136/1000 | Loss: 0.00003908
Iteration 137/1000 | Loss: 0.00003908
Iteration 138/1000 | Loss: 0.00003908
Iteration 139/1000 | Loss: 0.00003907
Iteration 140/1000 | Loss: 0.00003907
Iteration 141/1000 | Loss: 0.00003907
Iteration 142/1000 | Loss: 0.00003907
Iteration 143/1000 | Loss: 0.00003907
Iteration 144/1000 | Loss: 0.00003907
Iteration 145/1000 | Loss: 0.00003907
Iteration 146/1000 | Loss: 0.00003906
Iteration 147/1000 | Loss: 0.00003906
Iteration 148/1000 | Loss: 0.00003906
Iteration 149/1000 | Loss: 0.00003906
Iteration 150/1000 | Loss: 0.00003906
Iteration 151/1000 | Loss: 0.00003906
Iteration 152/1000 | Loss: 0.00003906
Iteration 153/1000 | Loss: 0.00003905
Iteration 154/1000 | Loss: 0.00003905
Iteration 155/1000 | Loss: 0.00003905
Iteration 156/1000 | Loss: 0.00003905
Iteration 157/1000 | Loss: 0.00003905
Iteration 158/1000 | Loss: 0.00003905
Iteration 159/1000 | Loss: 0.00003905
Iteration 160/1000 | Loss: 0.00003905
Iteration 161/1000 | Loss: 0.00003905
Iteration 162/1000 | Loss: 0.00003905
Iteration 163/1000 | Loss: 0.00003905
Iteration 164/1000 | Loss: 0.00003905
Iteration 165/1000 | Loss: 0.00003904
Iteration 166/1000 | Loss: 0.00003904
Iteration 167/1000 | Loss: 0.00003904
Iteration 168/1000 | Loss: 0.00003904
Iteration 169/1000 | Loss: 0.00003903
Iteration 170/1000 | Loss: 0.00003903
Iteration 171/1000 | Loss: 0.00003903
Iteration 172/1000 | Loss: 0.00003903
Iteration 173/1000 | Loss: 0.00003903
Iteration 174/1000 | Loss: 0.00003902
Iteration 175/1000 | Loss: 0.00003902
Iteration 176/1000 | Loss: 0.00003902
Iteration 177/1000 | Loss: 0.00003902
Iteration 178/1000 | Loss: 0.00003901
Iteration 179/1000 | Loss: 0.00003901
Iteration 180/1000 | Loss: 0.00003901
Iteration 181/1000 | Loss: 0.00003901
Iteration 182/1000 | Loss: 0.00003901
Iteration 183/1000 | Loss: 0.00003900
Iteration 184/1000 | Loss: 0.00003900
Iteration 185/1000 | Loss: 0.00003900
Iteration 186/1000 | Loss: 0.00003900
Iteration 187/1000 | Loss: 0.00003900
Iteration 188/1000 | Loss: 0.00003900
Iteration 189/1000 | Loss: 0.00003900
Iteration 190/1000 | Loss: 0.00003900
Iteration 191/1000 | Loss: 0.00003900
Iteration 192/1000 | Loss: 0.00003899
Iteration 193/1000 | Loss: 0.00003899
Iteration 194/1000 | Loss: 0.00003899
Iteration 195/1000 | Loss: 0.00003898
Iteration 196/1000 | Loss: 0.00003898
Iteration 197/1000 | Loss: 0.00003898
Iteration 198/1000 | Loss: 0.00003898
Iteration 199/1000 | Loss: 0.00003898
Iteration 200/1000 | Loss: 0.00003898
Iteration 201/1000 | Loss: 0.00003898
Iteration 202/1000 | Loss: 0.00003897
Iteration 203/1000 | Loss: 0.00003897
Iteration 204/1000 | Loss: 0.00003897
Iteration 205/1000 | Loss: 0.00003897
Iteration 206/1000 | Loss: 0.00003897
Iteration 207/1000 | Loss: 0.00003897
Iteration 208/1000 | Loss: 0.00003897
Iteration 209/1000 | Loss: 0.00003897
Iteration 210/1000 | Loss: 0.00003897
Iteration 211/1000 | Loss: 0.00003897
Iteration 212/1000 | Loss: 0.00003896
Iteration 213/1000 | Loss: 0.00003896
Iteration 214/1000 | Loss: 0.00003896
Iteration 215/1000 | Loss: 0.00003896
Iteration 216/1000 | Loss: 0.00003896
Iteration 217/1000 | Loss: 0.00003896
Iteration 218/1000 | Loss: 0.00003896
Iteration 219/1000 | Loss: 0.00003896
Iteration 220/1000 | Loss: 0.00003896
Iteration 221/1000 | Loss: 0.00003896
Iteration 222/1000 | Loss: 0.00003896
Iteration 223/1000 | Loss: 0.00003896
Iteration 224/1000 | Loss: 0.00003896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [3.896060661645606e-05, 3.896060661645606e-05, 3.896060661645606e-05, 3.896060661645606e-05, 3.896060661645606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.896060661645606e-05

Optimization complete. Final v2v error: 4.997376918792725 mm

Highest mean error: 6.6031718254089355 mm for frame 137

Lowest mean error: 3.228344678878784 mm for frame 38

Saving results

Total time: 95.64612579345703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005673
Iteration 2/25 | Loss: 0.00227292
Iteration 3/25 | Loss: 0.00157016
Iteration 4/25 | Loss: 0.00144075
Iteration 5/25 | Loss: 0.00136716
Iteration 6/25 | Loss: 0.00134000
Iteration 7/25 | Loss: 0.00133572
Iteration 8/25 | Loss: 0.00133372
Iteration 9/25 | Loss: 0.00134591
Iteration 10/25 | Loss: 0.00133958
Iteration 11/25 | Loss: 0.00132571
Iteration 12/25 | Loss: 0.00131691
Iteration 13/25 | Loss: 0.00131304
Iteration 14/25 | Loss: 0.00131805
Iteration 15/25 | Loss: 0.00131939
Iteration 16/25 | Loss: 0.00131925
Iteration 17/25 | Loss: 0.00132001
Iteration 18/25 | Loss: 0.00131746
Iteration 19/25 | Loss: 0.00132027
Iteration 20/25 | Loss: 0.00131697
Iteration 21/25 | Loss: 0.00131352
Iteration 22/25 | Loss: 0.00130789
Iteration 23/25 | Loss: 0.00130935
Iteration 24/25 | Loss: 0.00131131
Iteration 25/25 | Loss: 0.00130832

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35474086
Iteration 2/25 | Loss: 0.00128997
Iteration 3/25 | Loss: 0.00128997
Iteration 4/25 | Loss: 0.00128997
Iteration 5/25 | Loss: 0.00128997
Iteration 6/25 | Loss: 0.00128997
Iteration 7/25 | Loss: 0.00128997
Iteration 8/25 | Loss: 0.00128997
Iteration 9/25 | Loss: 0.00128997
Iteration 10/25 | Loss: 0.00128997
Iteration 11/25 | Loss: 0.00128997
Iteration 12/25 | Loss: 0.00128997
Iteration 13/25 | Loss: 0.00128997
Iteration 14/25 | Loss: 0.00128997
Iteration 15/25 | Loss: 0.00128997
Iteration 16/25 | Loss: 0.00128997
Iteration 17/25 | Loss: 0.00128997
Iteration 18/25 | Loss: 0.00128997
Iteration 19/25 | Loss: 0.00128997
Iteration 20/25 | Loss: 0.00128997
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001289965701289475, 0.001289965701289475, 0.001289965701289475, 0.001289965701289475, 0.001289965701289475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001289965701289475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128997
Iteration 2/1000 | Loss: 0.00069744
Iteration 3/1000 | Loss: 0.00048301
Iteration 4/1000 | Loss: 0.00063449
Iteration 5/1000 | Loss: 0.00071036
Iteration 6/1000 | Loss: 0.00059744
Iteration 7/1000 | Loss: 0.00074717
Iteration 8/1000 | Loss: 0.00079210
Iteration 9/1000 | Loss: 0.00078963
Iteration 10/1000 | Loss: 0.00075068
Iteration 11/1000 | Loss: 0.00072621
Iteration 12/1000 | Loss: 0.00081053
Iteration 13/1000 | Loss: 0.00073719
Iteration 14/1000 | Loss: 0.00426829
Iteration 15/1000 | Loss: 0.00275181
Iteration 16/1000 | Loss: 0.00067535
Iteration 17/1000 | Loss: 0.00077196
Iteration 18/1000 | Loss: 0.00073993
Iteration 19/1000 | Loss: 0.00058342
Iteration 20/1000 | Loss: 0.00060451
Iteration 21/1000 | Loss: 0.00161057
Iteration 22/1000 | Loss: 0.00094553
Iteration 23/1000 | Loss: 0.00424471
Iteration 24/1000 | Loss: 0.00095912
Iteration 25/1000 | Loss: 0.00060565
Iteration 26/1000 | Loss: 0.00052540
Iteration 27/1000 | Loss: 0.00055762
Iteration 28/1000 | Loss: 0.00070404
Iteration 29/1000 | Loss: 0.00074857
Iteration 30/1000 | Loss: 0.00077421
Iteration 31/1000 | Loss: 0.00447219
Iteration 32/1000 | Loss: 0.00164642
Iteration 33/1000 | Loss: 0.00167300
Iteration 34/1000 | Loss: 0.00146264
Iteration 35/1000 | Loss: 0.00223130
Iteration 36/1000 | Loss: 0.00315748
Iteration 37/1000 | Loss: 0.00215911
Iteration 38/1000 | Loss: 0.00185098
Iteration 39/1000 | Loss: 0.00155388
Iteration 40/1000 | Loss: 0.00109008
Iteration 41/1000 | Loss: 0.00102824
Iteration 42/1000 | Loss: 0.00145332
Iteration 43/1000 | Loss: 0.00149044
Iteration 44/1000 | Loss: 0.00107870
Iteration 45/1000 | Loss: 0.00341295
Iteration 46/1000 | Loss: 0.00165616
Iteration 47/1000 | Loss: 0.00253391
Iteration 48/1000 | Loss: 0.00192511
Iteration 49/1000 | Loss: 0.00162778
Iteration 50/1000 | Loss: 0.00110978
Iteration 51/1000 | Loss: 0.00214883
Iteration 52/1000 | Loss: 0.00220682
Iteration 53/1000 | Loss: 0.00754049
Iteration 54/1000 | Loss: 0.00487451
Iteration 55/1000 | Loss: 0.00443047
Iteration 56/1000 | Loss: 0.00181162
Iteration 57/1000 | Loss: 0.00122981
Iteration 58/1000 | Loss: 0.00075015
Iteration 59/1000 | Loss: 0.00082483
Iteration 60/1000 | Loss: 0.00110898
Iteration 61/1000 | Loss: 0.00087719
Iteration 62/1000 | Loss: 0.00086138
Iteration 63/1000 | Loss: 0.00058978
Iteration 64/1000 | Loss: 0.00110925
Iteration 65/1000 | Loss: 0.00119225
Iteration 66/1000 | Loss: 0.00110276
Iteration 67/1000 | Loss: 0.00070655
Iteration 68/1000 | Loss: 0.00079827
Iteration 69/1000 | Loss: 0.00168817
Iteration 70/1000 | Loss: 0.00123453
Iteration 71/1000 | Loss: 0.00131640
Iteration 72/1000 | Loss: 0.00404208
Iteration 73/1000 | Loss: 0.00304573
Iteration 74/1000 | Loss: 0.00117519
Iteration 75/1000 | Loss: 0.00185613
Iteration 76/1000 | Loss: 0.00194947
Iteration 77/1000 | Loss: 0.00177210
Iteration 78/1000 | Loss: 0.00164406
Iteration 79/1000 | Loss: 0.00187416
Iteration 80/1000 | Loss: 0.00098441
Iteration 81/1000 | Loss: 0.00465373
Iteration 82/1000 | Loss: 0.00120830
Iteration 83/1000 | Loss: 0.00063131
Iteration 84/1000 | Loss: 0.00066795
Iteration 85/1000 | Loss: 0.00085454
Iteration 86/1000 | Loss: 0.00087982
Iteration 87/1000 | Loss: 0.00094719
Iteration 88/1000 | Loss: 0.00122334
Iteration 89/1000 | Loss: 0.00133466
Iteration 90/1000 | Loss: 0.00112652
Iteration 91/1000 | Loss: 0.00118755
Iteration 92/1000 | Loss: 0.00119555
Iteration 93/1000 | Loss: 0.00118423
Iteration 94/1000 | Loss: 0.00065900
Iteration 95/1000 | Loss: 0.00133175
Iteration 96/1000 | Loss: 0.00105036
Iteration 97/1000 | Loss: 0.00086597
Iteration 98/1000 | Loss: 0.00105745
Iteration 99/1000 | Loss: 0.00091256
Iteration 100/1000 | Loss: 0.00090008
Iteration 101/1000 | Loss: 0.00271524
Iteration 102/1000 | Loss: 0.00124271
Iteration 103/1000 | Loss: 0.00068088
Iteration 104/1000 | Loss: 0.00064787
Iteration 105/1000 | Loss: 0.00056510
Iteration 106/1000 | Loss: 0.00064736
Iteration 107/1000 | Loss: 0.00071840
Iteration 108/1000 | Loss: 0.00090759
Iteration 109/1000 | Loss: 0.00069288
Iteration 110/1000 | Loss: 0.00059132
Iteration 111/1000 | Loss: 0.00072619
Iteration 112/1000 | Loss: 0.00088002
Iteration 113/1000 | Loss: 0.00120332
Iteration 114/1000 | Loss: 0.00057211
Iteration 115/1000 | Loss: 0.00100168
Iteration 116/1000 | Loss: 0.00112716
Iteration 117/1000 | Loss: 0.00077774
Iteration 118/1000 | Loss: 0.00103013
Iteration 119/1000 | Loss: 0.00077650
Iteration 120/1000 | Loss: 0.00081139
Iteration 121/1000 | Loss: 0.00075786
Iteration 122/1000 | Loss: 0.00086769
Iteration 123/1000 | Loss: 0.00098114
Iteration 124/1000 | Loss: 0.00093031
Iteration 125/1000 | Loss: 0.00438915
Iteration 126/1000 | Loss: 0.00102363
Iteration 127/1000 | Loss: 0.00108943
Iteration 128/1000 | Loss: 0.00101408
Iteration 129/1000 | Loss: 0.00102318
Iteration 130/1000 | Loss: 0.00108870
Iteration 131/1000 | Loss: 0.00124279
Iteration 132/1000 | Loss: 0.00030926
Iteration 133/1000 | Loss: 0.00050469
Iteration 134/1000 | Loss: 0.00062267
Iteration 135/1000 | Loss: 0.00063938
Iteration 136/1000 | Loss: 0.00032002
Iteration 137/1000 | Loss: 0.00065547
Iteration 138/1000 | Loss: 0.00029429
Iteration 139/1000 | Loss: 0.00058293
Iteration 140/1000 | Loss: 0.00032972
Iteration 141/1000 | Loss: 0.00035586
Iteration 142/1000 | Loss: 0.00037370
Iteration 143/1000 | Loss: 0.00035860
Iteration 144/1000 | Loss: 0.00034121
Iteration 145/1000 | Loss: 0.00042977
Iteration 146/1000 | Loss: 0.00065739
Iteration 147/1000 | Loss: 0.00036111
Iteration 148/1000 | Loss: 0.00072397
Iteration 149/1000 | Loss: 0.00070863
Iteration 150/1000 | Loss: 0.00045834
Iteration 151/1000 | Loss: 0.00036187
Iteration 152/1000 | Loss: 0.00043828
Iteration 153/1000 | Loss: 0.00063988
Iteration 154/1000 | Loss: 0.00077214
Iteration 155/1000 | Loss: 0.00076919
Iteration 156/1000 | Loss: 0.00054482
Iteration 157/1000 | Loss: 0.00069936
Iteration 158/1000 | Loss: 0.00061534
Iteration 159/1000 | Loss: 0.00225052
Iteration 160/1000 | Loss: 0.00087156
Iteration 161/1000 | Loss: 0.00069901
Iteration 162/1000 | Loss: 0.00057167
Iteration 163/1000 | Loss: 0.00056603
Iteration 164/1000 | Loss: 0.00043976
Iteration 165/1000 | Loss: 0.00046130
Iteration 166/1000 | Loss: 0.00039219
Iteration 167/1000 | Loss: 0.00025550
Iteration 168/1000 | Loss: 0.00069774
Iteration 169/1000 | Loss: 0.00036852
Iteration 170/1000 | Loss: 0.00090431
Iteration 171/1000 | Loss: 0.00065333
Iteration 172/1000 | Loss: 0.00086786
Iteration 173/1000 | Loss: 0.00086166
Iteration 174/1000 | Loss: 0.00026453
Iteration 175/1000 | Loss: 0.00010616
Iteration 176/1000 | Loss: 0.00017782
Iteration 177/1000 | Loss: 0.00031376
Iteration 178/1000 | Loss: 0.00029300
Iteration 179/1000 | Loss: 0.00030944
Iteration 180/1000 | Loss: 0.00048544
Iteration 181/1000 | Loss: 0.00061313
Iteration 182/1000 | Loss: 0.00047180
Iteration 183/1000 | Loss: 0.00041022
Iteration 184/1000 | Loss: 0.00038827
Iteration 185/1000 | Loss: 0.00048245
Iteration 186/1000 | Loss: 0.00035759
Iteration 187/1000 | Loss: 0.00048297
Iteration 188/1000 | Loss: 0.00049519
Iteration 189/1000 | Loss: 0.00026835
Iteration 190/1000 | Loss: 0.00051823
Iteration 191/1000 | Loss: 0.00071596
Iteration 192/1000 | Loss: 0.00058365
Iteration 193/1000 | Loss: 0.00065616
Iteration 194/1000 | Loss: 0.00059494
Iteration 195/1000 | Loss: 0.00052377
Iteration 196/1000 | Loss: 0.00053917
Iteration 197/1000 | Loss: 0.00049475
Iteration 198/1000 | Loss: 0.00062039
Iteration 199/1000 | Loss: 0.00025978
Iteration 200/1000 | Loss: 0.00067573
Iteration 201/1000 | Loss: 0.00076579
Iteration 202/1000 | Loss: 0.00065646
Iteration 203/1000 | Loss: 0.00032568
Iteration 204/1000 | Loss: 0.00047003
Iteration 205/1000 | Loss: 0.00049483
Iteration 206/1000 | Loss: 0.00049769
Iteration 207/1000 | Loss: 0.00047867
Iteration 208/1000 | Loss: 0.00065782
Iteration 209/1000 | Loss: 0.00435206
Iteration 210/1000 | Loss: 0.00021540
Iteration 211/1000 | Loss: 0.00055027
Iteration 212/1000 | Loss: 0.00035070
Iteration 213/1000 | Loss: 0.00031534
Iteration 214/1000 | Loss: 0.00035039
Iteration 215/1000 | Loss: 0.00034649
Iteration 216/1000 | Loss: 0.00034673
Iteration 217/1000 | Loss: 0.00026183
Iteration 218/1000 | Loss: 0.00029626
Iteration 219/1000 | Loss: 0.00020642
Iteration 220/1000 | Loss: 0.00068102
Iteration 221/1000 | Loss: 0.00047578
Iteration 222/1000 | Loss: 0.00051590
Iteration 223/1000 | Loss: 0.00046408
Iteration 224/1000 | Loss: 0.00025180
Iteration 225/1000 | Loss: 0.00059581
Iteration 226/1000 | Loss: 0.00050206
Iteration 227/1000 | Loss: 0.00020042
Iteration 228/1000 | Loss: 0.00027975
Iteration 229/1000 | Loss: 0.00032477
Iteration 230/1000 | Loss: 0.00032574
Iteration 231/1000 | Loss: 0.00031887
Iteration 232/1000 | Loss: 0.00026823
Iteration 233/1000 | Loss: 0.00028772
Iteration 234/1000 | Loss: 0.00034870
Iteration 235/1000 | Loss: 0.00025674
Iteration 236/1000 | Loss: 0.00033331
Iteration 237/1000 | Loss: 0.00026919
Iteration 238/1000 | Loss: 0.00013115
Iteration 239/1000 | Loss: 0.00030992
Iteration 240/1000 | Loss: 0.00012258
Iteration 241/1000 | Loss: 0.00024476
Iteration 242/1000 | Loss: 0.00020015
Iteration 243/1000 | Loss: 0.00018730
Iteration 244/1000 | Loss: 0.00014180
Iteration 245/1000 | Loss: 0.00021231
Iteration 246/1000 | Loss: 0.00021960
Iteration 247/1000 | Loss: 0.00014267
Iteration 248/1000 | Loss: 0.00016313
Iteration 249/1000 | Loss: 0.00016966
Iteration 250/1000 | Loss: 0.00082247
Iteration 251/1000 | Loss: 0.00026296
Iteration 252/1000 | Loss: 0.00008488
Iteration 253/1000 | Loss: 0.00011562
Iteration 254/1000 | Loss: 0.00010514
Iteration 255/1000 | Loss: 0.00011091
Iteration 256/1000 | Loss: 0.00003052
Iteration 257/1000 | Loss: 0.00002564
Iteration 258/1000 | Loss: 0.00015374
Iteration 259/1000 | Loss: 0.00002366
Iteration 260/1000 | Loss: 0.00002291
Iteration 261/1000 | Loss: 0.00002231
Iteration 262/1000 | Loss: 0.00002175
Iteration 263/1000 | Loss: 0.00002118
Iteration 264/1000 | Loss: 0.00002079
Iteration 265/1000 | Loss: 0.00002048
Iteration 266/1000 | Loss: 0.00002024
Iteration 267/1000 | Loss: 0.00002010
Iteration 268/1000 | Loss: 0.00002009
Iteration 269/1000 | Loss: 0.00002009
Iteration 270/1000 | Loss: 0.00001989
Iteration 271/1000 | Loss: 0.00001975
Iteration 272/1000 | Loss: 0.00001969
Iteration 273/1000 | Loss: 0.00001958
Iteration 274/1000 | Loss: 0.00001952
Iteration 275/1000 | Loss: 0.00001940
Iteration 276/1000 | Loss: 0.00001938
Iteration 277/1000 | Loss: 0.00001938
Iteration 278/1000 | Loss: 0.00001937
Iteration 279/1000 | Loss: 0.00001936
Iteration 280/1000 | Loss: 0.00001936
Iteration 281/1000 | Loss: 0.00001936
Iteration 282/1000 | Loss: 0.00001935
Iteration 283/1000 | Loss: 0.00001935
Iteration 284/1000 | Loss: 0.00001934
Iteration 285/1000 | Loss: 0.00001933
Iteration 286/1000 | Loss: 0.00001933
Iteration 287/1000 | Loss: 0.00001933
Iteration 288/1000 | Loss: 0.00001933
Iteration 289/1000 | Loss: 0.00001932
Iteration 290/1000 | Loss: 0.00001932
Iteration 291/1000 | Loss: 0.00001931
Iteration 292/1000 | Loss: 0.00001931
Iteration 293/1000 | Loss: 0.00001930
Iteration 294/1000 | Loss: 0.00001930
Iteration 295/1000 | Loss: 0.00001929
Iteration 296/1000 | Loss: 0.00001929
Iteration 297/1000 | Loss: 0.00001929
Iteration 298/1000 | Loss: 0.00001928
Iteration 299/1000 | Loss: 0.00001928
Iteration 300/1000 | Loss: 0.00001927
Iteration 301/1000 | Loss: 0.00001927
Iteration 302/1000 | Loss: 0.00001927
Iteration 303/1000 | Loss: 0.00001926
Iteration 304/1000 | Loss: 0.00001926
Iteration 305/1000 | Loss: 0.00001926
Iteration 306/1000 | Loss: 0.00001925
Iteration 307/1000 | Loss: 0.00001925
Iteration 308/1000 | Loss: 0.00001925
Iteration 309/1000 | Loss: 0.00001924
Iteration 310/1000 | Loss: 0.00001924
Iteration 311/1000 | Loss: 0.00001924
Iteration 312/1000 | Loss: 0.00001924
Iteration 313/1000 | Loss: 0.00001924
Iteration 314/1000 | Loss: 0.00001924
Iteration 315/1000 | Loss: 0.00001924
Iteration 316/1000 | Loss: 0.00001924
Iteration 317/1000 | Loss: 0.00001924
Iteration 318/1000 | Loss: 0.00001923
Iteration 319/1000 | Loss: 0.00001923
Iteration 320/1000 | Loss: 0.00001923
Iteration 321/1000 | Loss: 0.00001922
Iteration 322/1000 | Loss: 0.00001922
Iteration 323/1000 | Loss: 0.00001922
Iteration 324/1000 | Loss: 0.00001922
Iteration 325/1000 | Loss: 0.00001922
Iteration 326/1000 | Loss: 0.00001922
Iteration 327/1000 | Loss: 0.00001922
Iteration 328/1000 | Loss: 0.00001922
Iteration 329/1000 | Loss: 0.00001921
Iteration 330/1000 | Loss: 0.00001921
Iteration 331/1000 | Loss: 0.00001921
Iteration 332/1000 | Loss: 0.00001921
Iteration 333/1000 | Loss: 0.00001921
Iteration 334/1000 | Loss: 0.00001921
Iteration 335/1000 | Loss: 0.00001921
Iteration 336/1000 | Loss: 0.00001921
Iteration 337/1000 | Loss: 0.00001921
Iteration 338/1000 | Loss: 0.00001920
Iteration 339/1000 | Loss: 0.00001920
Iteration 340/1000 | Loss: 0.00001920
Iteration 341/1000 | Loss: 0.00001919
Iteration 342/1000 | Loss: 0.00001919
Iteration 343/1000 | Loss: 0.00001919
Iteration 344/1000 | Loss: 0.00001919
Iteration 345/1000 | Loss: 0.00001919
Iteration 346/1000 | Loss: 0.00001918
Iteration 347/1000 | Loss: 0.00001918
Iteration 348/1000 | Loss: 0.00001918
Iteration 349/1000 | Loss: 0.00001918
Iteration 350/1000 | Loss: 0.00001918
Iteration 351/1000 | Loss: 0.00001918
Iteration 352/1000 | Loss: 0.00001917
Iteration 353/1000 | Loss: 0.00001917
Iteration 354/1000 | Loss: 0.00001917
Iteration 355/1000 | Loss: 0.00001916
Iteration 356/1000 | Loss: 0.00001916
Iteration 357/1000 | Loss: 0.00001916
Iteration 358/1000 | Loss: 0.00001916
Iteration 359/1000 | Loss: 0.00001915
Iteration 360/1000 | Loss: 0.00001915
Iteration 361/1000 | Loss: 0.00001914
Iteration 362/1000 | Loss: 0.00001914
Iteration 363/1000 | Loss: 0.00001914
Iteration 364/1000 | Loss: 0.00001913
Iteration 365/1000 | Loss: 0.00001913
Iteration 366/1000 | Loss: 0.00001913
Iteration 367/1000 | Loss: 0.00001913
Iteration 368/1000 | Loss: 0.00001913
Iteration 369/1000 | Loss: 0.00001913
Iteration 370/1000 | Loss: 0.00001913
Iteration 371/1000 | Loss: 0.00001913
Iteration 372/1000 | Loss: 0.00001912
Iteration 373/1000 | Loss: 0.00001912
Iteration 374/1000 | Loss: 0.00001912
Iteration 375/1000 | Loss: 0.00001911
Iteration 376/1000 | Loss: 0.00001911
Iteration 377/1000 | Loss: 0.00001911
Iteration 378/1000 | Loss: 0.00001911
Iteration 379/1000 | Loss: 0.00001911
Iteration 380/1000 | Loss: 0.00001911
Iteration 381/1000 | Loss: 0.00001911
Iteration 382/1000 | Loss: 0.00001911
Iteration 383/1000 | Loss: 0.00001911
Iteration 384/1000 | Loss: 0.00001911
Iteration 385/1000 | Loss: 0.00001910
Iteration 386/1000 | Loss: 0.00001910
Iteration 387/1000 | Loss: 0.00001910
Iteration 388/1000 | Loss: 0.00001910
Iteration 389/1000 | Loss: 0.00001910
Iteration 390/1000 | Loss: 0.00001909
Iteration 391/1000 | Loss: 0.00001909
Iteration 392/1000 | Loss: 0.00001909
Iteration 393/1000 | Loss: 0.00001909
Iteration 394/1000 | Loss: 0.00001909
Iteration 395/1000 | Loss: 0.00001909
Iteration 396/1000 | Loss: 0.00001909
Iteration 397/1000 | Loss: 0.00001908
Iteration 398/1000 | Loss: 0.00001908
Iteration 399/1000 | Loss: 0.00001908
Iteration 400/1000 | Loss: 0.00001908
Iteration 401/1000 | Loss: 0.00001908
Iteration 402/1000 | Loss: 0.00001908
Iteration 403/1000 | Loss: 0.00001908
Iteration 404/1000 | Loss: 0.00001908
Iteration 405/1000 | Loss: 0.00001908
Iteration 406/1000 | Loss: 0.00001908
Iteration 407/1000 | Loss: 0.00001908
Iteration 408/1000 | Loss: 0.00001908
Iteration 409/1000 | Loss: 0.00001907
Iteration 410/1000 | Loss: 0.00001907
Iteration 411/1000 | Loss: 0.00001907
Iteration 412/1000 | Loss: 0.00001907
Iteration 413/1000 | Loss: 0.00001907
Iteration 414/1000 | Loss: 0.00001907
Iteration 415/1000 | Loss: 0.00001907
Iteration 416/1000 | Loss: 0.00001907
Iteration 417/1000 | Loss: 0.00001907
Iteration 418/1000 | Loss: 0.00001906
Iteration 419/1000 | Loss: 0.00001906
Iteration 420/1000 | Loss: 0.00001906
Iteration 421/1000 | Loss: 0.00001906
Iteration 422/1000 | Loss: 0.00001906
Iteration 423/1000 | Loss: 0.00001906
Iteration 424/1000 | Loss: 0.00001906
Iteration 425/1000 | Loss: 0.00001905
Iteration 426/1000 | Loss: 0.00001905
Iteration 427/1000 | Loss: 0.00001905
Iteration 428/1000 | Loss: 0.00001905
Iteration 429/1000 | Loss: 0.00001905
Iteration 430/1000 | Loss: 0.00001905
Iteration 431/1000 | Loss: 0.00001905
Iteration 432/1000 | Loss: 0.00001905
Iteration 433/1000 | Loss: 0.00001905
Iteration 434/1000 | Loss: 0.00001904
Iteration 435/1000 | Loss: 0.00001904
Iteration 436/1000 | Loss: 0.00001904
Iteration 437/1000 | Loss: 0.00001904
Iteration 438/1000 | Loss: 0.00001904
Iteration 439/1000 | Loss: 0.00001904
Iteration 440/1000 | Loss: 0.00001904
Iteration 441/1000 | Loss: 0.00001904
Iteration 442/1000 | Loss: 0.00001903
Iteration 443/1000 | Loss: 0.00001903
Iteration 444/1000 | Loss: 0.00001903
Iteration 445/1000 | Loss: 0.00001903
Iteration 446/1000 | Loss: 0.00001903
Iteration 447/1000 | Loss: 0.00001903
Iteration 448/1000 | Loss: 0.00001903
Iteration 449/1000 | Loss: 0.00001903
Iteration 450/1000 | Loss: 0.00001903
Iteration 451/1000 | Loss: 0.00001902
Iteration 452/1000 | Loss: 0.00001902
Iteration 453/1000 | Loss: 0.00001902
Iteration 454/1000 | Loss: 0.00001902
Iteration 455/1000 | Loss: 0.00001902
Iteration 456/1000 | Loss: 0.00001902
Iteration 457/1000 | Loss: 0.00001902
Iteration 458/1000 | Loss: 0.00001902
Iteration 459/1000 | Loss: 0.00001902
Iteration 460/1000 | Loss: 0.00001902
Iteration 461/1000 | Loss: 0.00001901
Iteration 462/1000 | Loss: 0.00001901
Iteration 463/1000 | Loss: 0.00001901
Iteration 464/1000 | Loss: 0.00001901
Iteration 465/1000 | Loss: 0.00001901
Iteration 466/1000 | Loss: 0.00001901
Iteration 467/1000 | Loss: 0.00001901
Iteration 468/1000 | Loss: 0.00001901
Iteration 469/1000 | Loss: 0.00001901
Iteration 470/1000 | Loss: 0.00001901
Iteration 471/1000 | Loss: 0.00001901
Iteration 472/1000 | Loss: 0.00001901
Iteration 473/1000 | Loss: 0.00001901
Iteration 474/1000 | Loss: 0.00001901
Iteration 475/1000 | Loss: 0.00001901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 475. Stopping optimization.
Last 5 losses: [1.9009210518561304e-05, 1.9009210518561304e-05, 1.9009210518561304e-05, 1.9009210518561304e-05, 1.9009210518561304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9009210518561304e-05

Optimization complete. Final v2v error: 3.6650354862213135 mm

Highest mean error: 4.934809684753418 mm for frame 232

Lowest mean error: 3.2550179958343506 mm for frame 215

Saving results

Total time: 503.14565110206604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499282
Iteration 2/25 | Loss: 0.00160464
Iteration 3/25 | Loss: 0.00134649
Iteration 4/25 | Loss: 0.00131174
Iteration 5/25 | Loss: 0.00130551
Iteration 6/25 | Loss: 0.00130379
Iteration 7/25 | Loss: 0.00130379
Iteration 8/25 | Loss: 0.00130379
Iteration 9/25 | Loss: 0.00130379
Iteration 10/25 | Loss: 0.00130379
Iteration 11/25 | Loss: 0.00130379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013037933968007565, 0.0013037933968007565, 0.0013037933968007565, 0.0013037933968007565, 0.0013037933968007565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013037933968007565

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44463623
Iteration 2/25 | Loss: 0.00080537
Iteration 3/25 | Loss: 0.00080537
Iteration 4/25 | Loss: 0.00080537
Iteration 5/25 | Loss: 0.00080537
Iteration 6/25 | Loss: 0.00080537
Iteration 7/25 | Loss: 0.00080537
Iteration 8/25 | Loss: 0.00080537
Iteration 9/25 | Loss: 0.00080537
Iteration 10/25 | Loss: 0.00080537
Iteration 11/25 | Loss: 0.00080537
Iteration 12/25 | Loss: 0.00080537
Iteration 13/25 | Loss: 0.00080537
Iteration 14/25 | Loss: 0.00080537
Iteration 15/25 | Loss: 0.00080537
Iteration 16/25 | Loss: 0.00080537
Iteration 17/25 | Loss: 0.00080537
Iteration 18/25 | Loss: 0.00080537
Iteration 19/25 | Loss: 0.00080537
Iteration 20/25 | Loss: 0.00080537
Iteration 21/25 | Loss: 0.00080537
Iteration 22/25 | Loss: 0.00080537
Iteration 23/25 | Loss: 0.00080537
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000805367948487401, 0.000805367948487401, 0.000805367948487401, 0.000805367948487401, 0.000805367948487401]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000805367948487401

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080537
Iteration 2/1000 | Loss: 0.00003390
Iteration 3/1000 | Loss: 0.00002424
Iteration 4/1000 | Loss: 0.00002161
Iteration 5/1000 | Loss: 0.00002049
Iteration 6/1000 | Loss: 0.00001961
Iteration 7/1000 | Loss: 0.00001894
Iteration 8/1000 | Loss: 0.00001854
Iteration 9/1000 | Loss: 0.00001828
Iteration 10/1000 | Loss: 0.00001802
Iteration 11/1000 | Loss: 0.00001790
Iteration 12/1000 | Loss: 0.00001787
Iteration 13/1000 | Loss: 0.00001778
Iteration 14/1000 | Loss: 0.00001776
Iteration 15/1000 | Loss: 0.00001770
Iteration 16/1000 | Loss: 0.00001760
Iteration 17/1000 | Loss: 0.00001759
Iteration 18/1000 | Loss: 0.00001759
Iteration 19/1000 | Loss: 0.00001759
Iteration 20/1000 | Loss: 0.00001759
Iteration 21/1000 | Loss: 0.00001758
Iteration 22/1000 | Loss: 0.00001758
Iteration 23/1000 | Loss: 0.00001757
Iteration 24/1000 | Loss: 0.00001756
Iteration 25/1000 | Loss: 0.00001755
Iteration 26/1000 | Loss: 0.00001755
Iteration 27/1000 | Loss: 0.00001755
Iteration 28/1000 | Loss: 0.00001754
Iteration 29/1000 | Loss: 0.00001753
Iteration 30/1000 | Loss: 0.00001752
Iteration 31/1000 | Loss: 0.00001752
Iteration 32/1000 | Loss: 0.00001750
Iteration 33/1000 | Loss: 0.00001750
Iteration 34/1000 | Loss: 0.00001750
Iteration 35/1000 | Loss: 0.00001750
Iteration 36/1000 | Loss: 0.00001750
Iteration 37/1000 | Loss: 0.00001750
Iteration 38/1000 | Loss: 0.00001750
Iteration 39/1000 | Loss: 0.00001749
Iteration 40/1000 | Loss: 0.00001749
Iteration 41/1000 | Loss: 0.00001749
Iteration 42/1000 | Loss: 0.00001749
Iteration 43/1000 | Loss: 0.00001749
Iteration 44/1000 | Loss: 0.00001749
Iteration 45/1000 | Loss: 0.00001749
Iteration 46/1000 | Loss: 0.00001749
Iteration 47/1000 | Loss: 0.00001749
Iteration 48/1000 | Loss: 0.00001748
Iteration 49/1000 | Loss: 0.00001748
Iteration 50/1000 | Loss: 0.00001748
Iteration 51/1000 | Loss: 0.00001746
Iteration 52/1000 | Loss: 0.00001746
Iteration 53/1000 | Loss: 0.00001746
Iteration 54/1000 | Loss: 0.00001746
Iteration 55/1000 | Loss: 0.00001746
Iteration 56/1000 | Loss: 0.00001745
Iteration 57/1000 | Loss: 0.00001745
Iteration 58/1000 | Loss: 0.00001745
Iteration 59/1000 | Loss: 0.00001745
Iteration 60/1000 | Loss: 0.00001745
Iteration 61/1000 | Loss: 0.00001744
Iteration 62/1000 | Loss: 0.00001744
Iteration 63/1000 | Loss: 0.00001743
Iteration 64/1000 | Loss: 0.00001743
Iteration 65/1000 | Loss: 0.00001742
Iteration 66/1000 | Loss: 0.00001742
Iteration 67/1000 | Loss: 0.00001741
Iteration 68/1000 | Loss: 0.00001741
Iteration 69/1000 | Loss: 0.00001740
Iteration 70/1000 | Loss: 0.00001740
Iteration 71/1000 | Loss: 0.00001739
Iteration 72/1000 | Loss: 0.00001739
Iteration 73/1000 | Loss: 0.00001739
Iteration 74/1000 | Loss: 0.00001739
Iteration 75/1000 | Loss: 0.00001738
Iteration 76/1000 | Loss: 0.00001738
Iteration 77/1000 | Loss: 0.00001738
Iteration 78/1000 | Loss: 0.00001738
Iteration 79/1000 | Loss: 0.00001738
Iteration 80/1000 | Loss: 0.00001738
Iteration 81/1000 | Loss: 0.00001737
Iteration 82/1000 | Loss: 0.00001736
Iteration 83/1000 | Loss: 0.00001736
Iteration 84/1000 | Loss: 0.00001736
Iteration 85/1000 | Loss: 0.00001735
Iteration 86/1000 | Loss: 0.00001735
Iteration 87/1000 | Loss: 0.00001735
Iteration 88/1000 | Loss: 0.00001734
Iteration 89/1000 | Loss: 0.00001734
Iteration 90/1000 | Loss: 0.00001734
Iteration 91/1000 | Loss: 0.00001734
Iteration 92/1000 | Loss: 0.00001734
Iteration 93/1000 | Loss: 0.00001734
Iteration 94/1000 | Loss: 0.00001734
Iteration 95/1000 | Loss: 0.00001733
Iteration 96/1000 | Loss: 0.00001733
Iteration 97/1000 | Loss: 0.00001733
Iteration 98/1000 | Loss: 0.00001733
Iteration 99/1000 | Loss: 0.00001733
Iteration 100/1000 | Loss: 0.00001733
Iteration 101/1000 | Loss: 0.00001732
Iteration 102/1000 | Loss: 0.00001732
Iteration 103/1000 | Loss: 0.00001732
Iteration 104/1000 | Loss: 0.00001732
Iteration 105/1000 | Loss: 0.00001731
Iteration 106/1000 | Loss: 0.00001731
Iteration 107/1000 | Loss: 0.00001731
Iteration 108/1000 | Loss: 0.00001731
Iteration 109/1000 | Loss: 0.00001731
Iteration 110/1000 | Loss: 0.00001730
Iteration 111/1000 | Loss: 0.00001730
Iteration 112/1000 | Loss: 0.00001730
Iteration 113/1000 | Loss: 0.00001730
Iteration 114/1000 | Loss: 0.00001730
Iteration 115/1000 | Loss: 0.00001730
Iteration 116/1000 | Loss: 0.00001729
Iteration 117/1000 | Loss: 0.00001729
Iteration 118/1000 | Loss: 0.00001729
Iteration 119/1000 | Loss: 0.00001729
Iteration 120/1000 | Loss: 0.00001728
Iteration 121/1000 | Loss: 0.00001728
Iteration 122/1000 | Loss: 0.00001728
Iteration 123/1000 | Loss: 0.00001727
Iteration 124/1000 | Loss: 0.00001727
Iteration 125/1000 | Loss: 0.00001727
Iteration 126/1000 | Loss: 0.00001726
Iteration 127/1000 | Loss: 0.00001726
Iteration 128/1000 | Loss: 0.00001726
Iteration 129/1000 | Loss: 0.00001725
Iteration 130/1000 | Loss: 0.00001725
Iteration 131/1000 | Loss: 0.00001725
Iteration 132/1000 | Loss: 0.00001725
Iteration 133/1000 | Loss: 0.00001725
Iteration 134/1000 | Loss: 0.00001725
Iteration 135/1000 | Loss: 0.00001724
Iteration 136/1000 | Loss: 0.00001724
Iteration 137/1000 | Loss: 0.00001724
Iteration 138/1000 | Loss: 0.00001724
Iteration 139/1000 | Loss: 0.00001724
Iteration 140/1000 | Loss: 0.00001724
Iteration 141/1000 | Loss: 0.00001724
Iteration 142/1000 | Loss: 0.00001723
Iteration 143/1000 | Loss: 0.00001723
Iteration 144/1000 | Loss: 0.00001723
Iteration 145/1000 | Loss: 0.00001722
Iteration 146/1000 | Loss: 0.00001722
Iteration 147/1000 | Loss: 0.00001722
Iteration 148/1000 | Loss: 0.00001722
Iteration 149/1000 | Loss: 0.00001722
Iteration 150/1000 | Loss: 0.00001722
Iteration 151/1000 | Loss: 0.00001722
Iteration 152/1000 | Loss: 0.00001722
Iteration 153/1000 | Loss: 0.00001722
Iteration 154/1000 | Loss: 0.00001721
Iteration 155/1000 | Loss: 0.00001721
Iteration 156/1000 | Loss: 0.00001721
Iteration 157/1000 | Loss: 0.00001721
Iteration 158/1000 | Loss: 0.00001721
Iteration 159/1000 | Loss: 0.00001720
Iteration 160/1000 | Loss: 0.00001720
Iteration 161/1000 | Loss: 0.00001720
Iteration 162/1000 | Loss: 0.00001720
Iteration 163/1000 | Loss: 0.00001720
Iteration 164/1000 | Loss: 0.00001719
Iteration 165/1000 | Loss: 0.00001719
Iteration 166/1000 | Loss: 0.00001719
Iteration 167/1000 | Loss: 0.00001719
Iteration 168/1000 | Loss: 0.00001719
Iteration 169/1000 | Loss: 0.00001719
Iteration 170/1000 | Loss: 0.00001719
Iteration 171/1000 | Loss: 0.00001719
Iteration 172/1000 | Loss: 0.00001719
Iteration 173/1000 | Loss: 0.00001719
Iteration 174/1000 | Loss: 0.00001719
Iteration 175/1000 | Loss: 0.00001718
Iteration 176/1000 | Loss: 0.00001718
Iteration 177/1000 | Loss: 0.00001718
Iteration 178/1000 | Loss: 0.00001717
Iteration 179/1000 | Loss: 0.00001717
Iteration 180/1000 | Loss: 0.00001717
Iteration 181/1000 | Loss: 0.00001717
Iteration 182/1000 | Loss: 0.00001717
Iteration 183/1000 | Loss: 0.00001717
Iteration 184/1000 | Loss: 0.00001717
Iteration 185/1000 | Loss: 0.00001717
Iteration 186/1000 | Loss: 0.00001717
Iteration 187/1000 | Loss: 0.00001717
Iteration 188/1000 | Loss: 0.00001717
Iteration 189/1000 | Loss: 0.00001716
Iteration 190/1000 | Loss: 0.00001716
Iteration 191/1000 | Loss: 0.00001716
Iteration 192/1000 | Loss: 0.00001716
Iteration 193/1000 | Loss: 0.00001716
Iteration 194/1000 | Loss: 0.00001716
Iteration 195/1000 | Loss: 0.00001716
Iteration 196/1000 | Loss: 0.00001716
Iteration 197/1000 | Loss: 0.00001716
Iteration 198/1000 | Loss: 0.00001716
Iteration 199/1000 | Loss: 0.00001716
Iteration 200/1000 | Loss: 0.00001716
Iteration 201/1000 | Loss: 0.00001716
Iteration 202/1000 | Loss: 0.00001716
Iteration 203/1000 | Loss: 0.00001716
Iteration 204/1000 | Loss: 0.00001716
Iteration 205/1000 | Loss: 0.00001716
Iteration 206/1000 | Loss: 0.00001716
Iteration 207/1000 | Loss: 0.00001716
Iteration 208/1000 | Loss: 0.00001716
Iteration 209/1000 | Loss: 0.00001716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.7162514268420637e-05, 1.7162514268420637e-05, 1.7162514268420637e-05, 1.7162514268420637e-05, 1.7162514268420637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7162514268420637e-05

Optimization complete. Final v2v error: 3.469180107116699 mm

Highest mean error: 4.483066082000732 mm for frame 92

Lowest mean error: 3.127721071243286 mm for frame 27

Saving results

Total time: 43.05740237236023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00599047
Iteration 2/25 | Loss: 0.00132399
Iteration 3/25 | Loss: 0.00125386
Iteration 4/25 | Loss: 0.00124471
Iteration 5/25 | Loss: 0.00124163
Iteration 6/25 | Loss: 0.00124120
Iteration 7/25 | Loss: 0.00124120
Iteration 8/25 | Loss: 0.00124120
Iteration 9/25 | Loss: 0.00124120
Iteration 10/25 | Loss: 0.00124120
Iteration 11/25 | Loss: 0.00124120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012412037467584014, 0.0012412037467584014, 0.0012412037467584014, 0.0012412037467584014, 0.0012412037467584014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012412037467584014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.16051054
Iteration 2/25 | Loss: 0.00086891
Iteration 3/25 | Loss: 0.00086891
Iteration 4/25 | Loss: 0.00086890
Iteration 5/25 | Loss: 0.00086890
Iteration 6/25 | Loss: 0.00086890
Iteration 7/25 | Loss: 0.00086890
Iteration 8/25 | Loss: 0.00086890
Iteration 9/25 | Loss: 0.00086890
Iteration 10/25 | Loss: 0.00086890
Iteration 11/25 | Loss: 0.00086890
Iteration 12/25 | Loss: 0.00086890
Iteration 13/25 | Loss: 0.00086890
Iteration 14/25 | Loss: 0.00086890
Iteration 15/25 | Loss: 0.00086890
Iteration 16/25 | Loss: 0.00086890
Iteration 17/25 | Loss: 0.00086890
Iteration 18/25 | Loss: 0.00086890
Iteration 19/25 | Loss: 0.00086890
Iteration 20/25 | Loss: 0.00086890
Iteration 21/25 | Loss: 0.00086890
Iteration 22/25 | Loss: 0.00086890
Iteration 23/25 | Loss: 0.00086890
Iteration 24/25 | Loss: 0.00086890
Iteration 25/25 | Loss: 0.00086890

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086890
Iteration 2/1000 | Loss: 0.00002526
Iteration 3/1000 | Loss: 0.00001726
Iteration 4/1000 | Loss: 0.00001583
Iteration 5/1000 | Loss: 0.00001503
Iteration 6/1000 | Loss: 0.00001446
Iteration 7/1000 | Loss: 0.00001399
Iteration 8/1000 | Loss: 0.00001374
Iteration 9/1000 | Loss: 0.00001364
Iteration 10/1000 | Loss: 0.00001341
Iteration 11/1000 | Loss: 0.00001326
Iteration 12/1000 | Loss: 0.00001323
Iteration 13/1000 | Loss: 0.00001321
Iteration 14/1000 | Loss: 0.00001320
Iteration 15/1000 | Loss: 0.00001312
Iteration 16/1000 | Loss: 0.00001303
Iteration 17/1000 | Loss: 0.00001296
Iteration 18/1000 | Loss: 0.00001292
Iteration 19/1000 | Loss: 0.00001292
Iteration 20/1000 | Loss: 0.00001292
Iteration 21/1000 | Loss: 0.00001292
Iteration 22/1000 | Loss: 0.00001291
Iteration 23/1000 | Loss: 0.00001291
Iteration 24/1000 | Loss: 0.00001291
Iteration 25/1000 | Loss: 0.00001291
Iteration 26/1000 | Loss: 0.00001291
Iteration 27/1000 | Loss: 0.00001290
Iteration 28/1000 | Loss: 0.00001290
Iteration 29/1000 | Loss: 0.00001288
Iteration 30/1000 | Loss: 0.00001288
Iteration 31/1000 | Loss: 0.00001288
Iteration 32/1000 | Loss: 0.00001288
Iteration 33/1000 | Loss: 0.00001288
Iteration 34/1000 | Loss: 0.00001288
Iteration 35/1000 | Loss: 0.00001287
Iteration 36/1000 | Loss: 0.00001287
Iteration 37/1000 | Loss: 0.00001287
Iteration 38/1000 | Loss: 0.00001286
Iteration 39/1000 | Loss: 0.00001285
Iteration 40/1000 | Loss: 0.00001285
Iteration 41/1000 | Loss: 0.00001283
Iteration 42/1000 | Loss: 0.00001283
Iteration 43/1000 | Loss: 0.00001282
Iteration 44/1000 | Loss: 0.00001282
Iteration 45/1000 | Loss: 0.00001281
Iteration 46/1000 | Loss: 0.00001280
Iteration 47/1000 | Loss: 0.00001279
Iteration 48/1000 | Loss: 0.00001279
Iteration 49/1000 | Loss: 0.00001278
Iteration 50/1000 | Loss: 0.00001278
Iteration 51/1000 | Loss: 0.00001278
Iteration 52/1000 | Loss: 0.00001277
Iteration 53/1000 | Loss: 0.00001277
Iteration 54/1000 | Loss: 0.00001275
Iteration 55/1000 | Loss: 0.00001275
Iteration 56/1000 | Loss: 0.00001275
Iteration 57/1000 | Loss: 0.00001275
Iteration 58/1000 | Loss: 0.00001274
Iteration 59/1000 | Loss: 0.00001274
Iteration 60/1000 | Loss: 0.00001274
Iteration 61/1000 | Loss: 0.00001274
Iteration 62/1000 | Loss: 0.00001273
Iteration 63/1000 | Loss: 0.00001273
Iteration 64/1000 | Loss: 0.00001273
Iteration 65/1000 | Loss: 0.00001272
Iteration 66/1000 | Loss: 0.00001272
Iteration 67/1000 | Loss: 0.00001272
Iteration 68/1000 | Loss: 0.00001271
Iteration 69/1000 | Loss: 0.00001271
Iteration 70/1000 | Loss: 0.00001270
Iteration 71/1000 | Loss: 0.00001270
Iteration 72/1000 | Loss: 0.00001267
Iteration 73/1000 | Loss: 0.00001267
Iteration 74/1000 | Loss: 0.00001265
Iteration 75/1000 | Loss: 0.00001265
Iteration 76/1000 | Loss: 0.00001264
Iteration 77/1000 | Loss: 0.00001263
Iteration 78/1000 | Loss: 0.00001263
Iteration 79/1000 | Loss: 0.00001263
Iteration 80/1000 | Loss: 0.00001262
Iteration 81/1000 | Loss: 0.00001262
Iteration 82/1000 | Loss: 0.00001262
Iteration 83/1000 | Loss: 0.00001260
Iteration 84/1000 | Loss: 0.00001260
Iteration 85/1000 | Loss: 0.00001260
Iteration 86/1000 | Loss: 0.00001260
Iteration 87/1000 | Loss: 0.00001260
Iteration 88/1000 | Loss: 0.00001260
Iteration 89/1000 | Loss: 0.00001260
Iteration 90/1000 | Loss: 0.00001259
Iteration 91/1000 | Loss: 0.00001259
Iteration 92/1000 | Loss: 0.00001259
Iteration 93/1000 | Loss: 0.00001259
Iteration 94/1000 | Loss: 0.00001259
Iteration 95/1000 | Loss: 0.00001259
Iteration 96/1000 | Loss: 0.00001259
Iteration 97/1000 | Loss: 0.00001258
Iteration 98/1000 | Loss: 0.00001258
Iteration 99/1000 | Loss: 0.00001257
Iteration 100/1000 | Loss: 0.00001257
Iteration 101/1000 | Loss: 0.00001256
Iteration 102/1000 | Loss: 0.00001256
Iteration 103/1000 | Loss: 0.00001256
Iteration 104/1000 | Loss: 0.00001256
Iteration 105/1000 | Loss: 0.00001256
Iteration 106/1000 | Loss: 0.00001255
Iteration 107/1000 | Loss: 0.00001255
Iteration 108/1000 | Loss: 0.00001255
Iteration 109/1000 | Loss: 0.00001255
Iteration 110/1000 | Loss: 0.00001255
Iteration 111/1000 | Loss: 0.00001255
Iteration 112/1000 | Loss: 0.00001255
Iteration 113/1000 | Loss: 0.00001255
Iteration 114/1000 | Loss: 0.00001255
Iteration 115/1000 | Loss: 0.00001255
Iteration 116/1000 | Loss: 0.00001255
Iteration 117/1000 | Loss: 0.00001255
Iteration 118/1000 | Loss: 0.00001255
Iteration 119/1000 | Loss: 0.00001255
Iteration 120/1000 | Loss: 0.00001255
Iteration 121/1000 | Loss: 0.00001255
Iteration 122/1000 | Loss: 0.00001255
Iteration 123/1000 | Loss: 0.00001255
Iteration 124/1000 | Loss: 0.00001255
Iteration 125/1000 | Loss: 0.00001254
Iteration 126/1000 | Loss: 0.00001254
Iteration 127/1000 | Loss: 0.00001254
Iteration 128/1000 | Loss: 0.00001254
Iteration 129/1000 | Loss: 0.00001254
Iteration 130/1000 | Loss: 0.00001254
Iteration 131/1000 | Loss: 0.00001254
Iteration 132/1000 | Loss: 0.00001254
Iteration 133/1000 | Loss: 0.00001254
Iteration 134/1000 | Loss: 0.00001253
Iteration 135/1000 | Loss: 0.00001253
Iteration 136/1000 | Loss: 0.00001253
Iteration 137/1000 | Loss: 0.00001253
Iteration 138/1000 | Loss: 0.00001253
Iteration 139/1000 | Loss: 0.00001253
Iteration 140/1000 | Loss: 0.00001253
Iteration 141/1000 | Loss: 0.00001253
Iteration 142/1000 | Loss: 0.00001253
Iteration 143/1000 | Loss: 0.00001253
Iteration 144/1000 | Loss: 0.00001253
Iteration 145/1000 | Loss: 0.00001253
Iteration 146/1000 | Loss: 0.00001253
Iteration 147/1000 | Loss: 0.00001252
Iteration 148/1000 | Loss: 0.00001252
Iteration 149/1000 | Loss: 0.00001252
Iteration 150/1000 | Loss: 0.00001252
Iteration 151/1000 | Loss: 0.00001252
Iteration 152/1000 | Loss: 0.00001252
Iteration 153/1000 | Loss: 0.00001252
Iteration 154/1000 | Loss: 0.00001252
Iteration 155/1000 | Loss: 0.00001252
Iteration 156/1000 | Loss: 0.00001252
Iteration 157/1000 | Loss: 0.00001252
Iteration 158/1000 | Loss: 0.00001251
Iteration 159/1000 | Loss: 0.00001251
Iteration 160/1000 | Loss: 0.00001251
Iteration 161/1000 | Loss: 0.00001251
Iteration 162/1000 | Loss: 0.00001251
Iteration 163/1000 | Loss: 0.00001251
Iteration 164/1000 | Loss: 0.00001251
Iteration 165/1000 | Loss: 0.00001251
Iteration 166/1000 | Loss: 0.00001251
Iteration 167/1000 | Loss: 0.00001251
Iteration 168/1000 | Loss: 0.00001251
Iteration 169/1000 | Loss: 0.00001251
Iteration 170/1000 | Loss: 0.00001251
Iteration 171/1000 | Loss: 0.00001251
Iteration 172/1000 | Loss: 0.00001251
Iteration 173/1000 | Loss: 0.00001251
Iteration 174/1000 | Loss: 0.00001251
Iteration 175/1000 | Loss: 0.00001250
Iteration 176/1000 | Loss: 0.00001250
Iteration 177/1000 | Loss: 0.00001250
Iteration 178/1000 | Loss: 0.00001250
Iteration 179/1000 | Loss: 0.00001250
Iteration 180/1000 | Loss: 0.00001250
Iteration 181/1000 | Loss: 0.00001250
Iteration 182/1000 | Loss: 0.00001250
Iteration 183/1000 | Loss: 0.00001250
Iteration 184/1000 | Loss: 0.00001250
Iteration 185/1000 | Loss: 0.00001250
Iteration 186/1000 | Loss: 0.00001250
Iteration 187/1000 | Loss: 0.00001250
Iteration 188/1000 | Loss: 0.00001250
Iteration 189/1000 | Loss: 0.00001250
Iteration 190/1000 | Loss: 0.00001250
Iteration 191/1000 | Loss: 0.00001250
Iteration 192/1000 | Loss: 0.00001250
Iteration 193/1000 | Loss: 0.00001250
Iteration 194/1000 | Loss: 0.00001250
Iteration 195/1000 | Loss: 0.00001250
Iteration 196/1000 | Loss: 0.00001250
Iteration 197/1000 | Loss: 0.00001250
Iteration 198/1000 | Loss: 0.00001249
Iteration 199/1000 | Loss: 0.00001249
Iteration 200/1000 | Loss: 0.00001249
Iteration 201/1000 | Loss: 0.00001249
Iteration 202/1000 | Loss: 0.00001249
Iteration 203/1000 | Loss: 0.00001249
Iteration 204/1000 | Loss: 0.00001249
Iteration 205/1000 | Loss: 0.00001249
Iteration 206/1000 | Loss: 0.00001249
Iteration 207/1000 | Loss: 0.00001249
Iteration 208/1000 | Loss: 0.00001249
Iteration 209/1000 | Loss: 0.00001249
Iteration 210/1000 | Loss: 0.00001249
Iteration 211/1000 | Loss: 0.00001249
Iteration 212/1000 | Loss: 0.00001249
Iteration 213/1000 | Loss: 0.00001249
Iteration 214/1000 | Loss: 0.00001249
Iteration 215/1000 | Loss: 0.00001249
Iteration 216/1000 | Loss: 0.00001249
Iteration 217/1000 | Loss: 0.00001249
Iteration 218/1000 | Loss: 0.00001249
Iteration 219/1000 | Loss: 0.00001249
Iteration 220/1000 | Loss: 0.00001249
Iteration 221/1000 | Loss: 0.00001249
Iteration 222/1000 | Loss: 0.00001249
Iteration 223/1000 | Loss: 0.00001249
Iteration 224/1000 | Loss: 0.00001249
Iteration 225/1000 | Loss: 0.00001249
Iteration 226/1000 | Loss: 0.00001249
Iteration 227/1000 | Loss: 0.00001249
Iteration 228/1000 | Loss: 0.00001249
Iteration 229/1000 | Loss: 0.00001249
Iteration 230/1000 | Loss: 0.00001249
Iteration 231/1000 | Loss: 0.00001249
Iteration 232/1000 | Loss: 0.00001249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.2489163054851815e-05, 1.2489163054851815e-05, 1.2489163054851815e-05, 1.2489163054851815e-05, 1.2489163054851815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2489163054851815e-05

Optimization complete. Final v2v error: 3.0216028690338135 mm

Highest mean error: 3.313647508621216 mm for frame 95

Lowest mean error: 2.8445804119110107 mm for frame 119

Saving results

Total time: 41.54246926307678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00751670
Iteration 2/25 | Loss: 0.00165084
Iteration 3/25 | Loss: 0.00134254
Iteration 4/25 | Loss: 0.00131713
Iteration 5/25 | Loss: 0.00131516
Iteration 6/25 | Loss: 0.00131516
Iteration 7/25 | Loss: 0.00131516
Iteration 8/25 | Loss: 0.00131516
Iteration 9/25 | Loss: 0.00131516
Iteration 10/25 | Loss: 0.00131516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001315157045610249, 0.001315157045610249, 0.001315157045610249, 0.001315157045610249, 0.001315157045610249]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001315157045610249

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38970459
Iteration 2/25 | Loss: 0.00059115
Iteration 3/25 | Loss: 0.00059114
Iteration 4/25 | Loss: 0.00059114
Iteration 5/25 | Loss: 0.00059114
Iteration 6/25 | Loss: 0.00059113
Iteration 7/25 | Loss: 0.00059113
Iteration 8/25 | Loss: 0.00059113
Iteration 9/25 | Loss: 0.00059113
Iteration 10/25 | Loss: 0.00059113
Iteration 11/25 | Loss: 0.00059113
Iteration 12/25 | Loss: 0.00059113
Iteration 13/25 | Loss: 0.00059113
Iteration 14/25 | Loss: 0.00059113
Iteration 15/25 | Loss: 0.00059113
Iteration 16/25 | Loss: 0.00059113
Iteration 17/25 | Loss: 0.00059113
Iteration 18/25 | Loss: 0.00059113
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005911331390962005, 0.0005911331390962005, 0.0005911331390962005, 0.0005911331390962005, 0.0005911331390962005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005911331390962005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059113
Iteration 2/1000 | Loss: 0.00003287
Iteration 3/1000 | Loss: 0.00002226
Iteration 4/1000 | Loss: 0.00001994
Iteration 5/1000 | Loss: 0.00001911
Iteration 6/1000 | Loss: 0.00001821
Iteration 7/1000 | Loss: 0.00001765
Iteration 8/1000 | Loss: 0.00001713
Iteration 9/1000 | Loss: 0.00001669
Iteration 10/1000 | Loss: 0.00001643
Iteration 11/1000 | Loss: 0.00001630
Iteration 12/1000 | Loss: 0.00001608
Iteration 13/1000 | Loss: 0.00001608
Iteration 14/1000 | Loss: 0.00001606
Iteration 15/1000 | Loss: 0.00001596
Iteration 16/1000 | Loss: 0.00001596
Iteration 17/1000 | Loss: 0.00001595
Iteration 18/1000 | Loss: 0.00001595
Iteration 19/1000 | Loss: 0.00001594
Iteration 20/1000 | Loss: 0.00001589
Iteration 21/1000 | Loss: 0.00001585
Iteration 22/1000 | Loss: 0.00001585
Iteration 23/1000 | Loss: 0.00001584
Iteration 24/1000 | Loss: 0.00001584
Iteration 25/1000 | Loss: 0.00001583
Iteration 26/1000 | Loss: 0.00001582
Iteration 27/1000 | Loss: 0.00001581
Iteration 28/1000 | Loss: 0.00001581
Iteration 29/1000 | Loss: 0.00001580
Iteration 30/1000 | Loss: 0.00001580
Iteration 31/1000 | Loss: 0.00001580
Iteration 32/1000 | Loss: 0.00001580
Iteration 33/1000 | Loss: 0.00001580
Iteration 34/1000 | Loss: 0.00001580
Iteration 35/1000 | Loss: 0.00001580
Iteration 36/1000 | Loss: 0.00001580
Iteration 37/1000 | Loss: 0.00001580
Iteration 38/1000 | Loss: 0.00001580
Iteration 39/1000 | Loss: 0.00001580
Iteration 40/1000 | Loss: 0.00001580
Iteration 41/1000 | Loss: 0.00001578
Iteration 42/1000 | Loss: 0.00001577
Iteration 43/1000 | Loss: 0.00001576
Iteration 44/1000 | Loss: 0.00001576
Iteration 45/1000 | Loss: 0.00001575
Iteration 46/1000 | Loss: 0.00001574
Iteration 47/1000 | Loss: 0.00001572
Iteration 48/1000 | Loss: 0.00001572
Iteration 49/1000 | Loss: 0.00001572
Iteration 50/1000 | Loss: 0.00001571
Iteration 51/1000 | Loss: 0.00001569
Iteration 52/1000 | Loss: 0.00001567
Iteration 53/1000 | Loss: 0.00001566
Iteration 54/1000 | Loss: 0.00001566
Iteration 55/1000 | Loss: 0.00001566
Iteration 56/1000 | Loss: 0.00001565
Iteration 57/1000 | Loss: 0.00001565
Iteration 58/1000 | Loss: 0.00001565
Iteration 59/1000 | Loss: 0.00001564
Iteration 60/1000 | Loss: 0.00001564
Iteration 61/1000 | Loss: 0.00001564
Iteration 62/1000 | Loss: 0.00001563
Iteration 63/1000 | Loss: 0.00001561
Iteration 64/1000 | Loss: 0.00001561
Iteration 65/1000 | Loss: 0.00001560
Iteration 66/1000 | Loss: 0.00001560
Iteration 67/1000 | Loss: 0.00001560
Iteration 68/1000 | Loss: 0.00001559
Iteration 69/1000 | Loss: 0.00001558
Iteration 70/1000 | Loss: 0.00001558
Iteration 71/1000 | Loss: 0.00001558
Iteration 72/1000 | Loss: 0.00001558
Iteration 73/1000 | Loss: 0.00001558
Iteration 74/1000 | Loss: 0.00001558
Iteration 75/1000 | Loss: 0.00001558
Iteration 76/1000 | Loss: 0.00001557
Iteration 77/1000 | Loss: 0.00001557
Iteration 78/1000 | Loss: 0.00001557
Iteration 79/1000 | Loss: 0.00001557
Iteration 80/1000 | Loss: 0.00001556
Iteration 81/1000 | Loss: 0.00001556
Iteration 82/1000 | Loss: 0.00001556
Iteration 83/1000 | Loss: 0.00001556
Iteration 84/1000 | Loss: 0.00001556
Iteration 85/1000 | Loss: 0.00001555
Iteration 86/1000 | Loss: 0.00001555
Iteration 87/1000 | Loss: 0.00001555
Iteration 88/1000 | Loss: 0.00001555
Iteration 89/1000 | Loss: 0.00001555
Iteration 90/1000 | Loss: 0.00001555
Iteration 91/1000 | Loss: 0.00001555
Iteration 92/1000 | Loss: 0.00001555
Iteration 93/1000 | Loss: 0.00001555
Iteration 94/1000 | Loss: 0.00001554
Iteration 95/1000 | Loss: 0.00001554
Iteration 96/1000 | Loss: 0.00001554
Iteration 97/1000 | Loss: 0.00001553
Iteration 98/1000 | Loss: 0.00001553
Iteration 99/1000 | Loss: 0.00001553
Iteration 100/1000 | Loss: 0.00001552
Iteration 101/1000 | Loss: 0.00001552
Iteration 102/1000 | Loss: 0.00001552
Iteration 103/1000 | Loss: 0.00001552
Iteration 104/1000 | Loss: 0.00001552
Iteration 105/1000 | Loss: 0.00001552
Iteration 106/1000 | Loss: 0.00001552
Iteration 107/1000 | Loss: 0.00001552
Iteration 108/1000 | Loss: 0.00001552
Iteration 109/1000 | Loss: 0.00001552
Iteration 110/1000 | Loss: 0.00001552
Iteration 111/1000 | Loss: 0.00001552
Iteration 112/1000 | Loss: 0.00001552
Iteration 113/1000 | Loss: 0.00001552
Iteration 114/1000 | Loss: 0.00001552
Iteration 115/1000 | Loss: 0.00001552
Iteration 116/1000 | Loss: 0.00001552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.5516239727730863e-05, 1.5516239727730863e-05, 1.5516239727730863e-05, 1.5516239727730863e-05, 1.5516239727730863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5516239727730863e-05

Optimization complete. Final v2v error: 3.326341152191162 mm

Highest mean error: 3.4973371028900146 mm for frame 143

Lowest mean error: 3.1943283081054688 mm for frame 181

Saving results

Total time: 39.19184923171997
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00497530
Iteration 2/25 | Loss: 0.00143059
Iteration 3/25 | Loss: 0.00134111
Iteration 4/25 | Loss: 0.00132387
Iteration 5/25 | Loss: 0.00131801
Iteration 6/25 | Loss: 0.00131771
Iteration 7/25 | Loss: 0.00131771
Iteration 8/25 | Loss: 0.00131771
Iteration 9/25 | Loss: 0.00131771
Iteration 10/25 | Loss: 0.00131771
Iteration 11/25 | Loss: 0.00131771
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013177121290937066, 0.0013177121290937066, 0.0013177121290937066, 0.0013177121290937066, 0.0013177121290937066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013177121290937066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40631080
Iteration 2/25 | Loss: 0.00092592
Iteration 3/25 | Loss: 0.00092592
Iteration 4/25 | Loss: 0.00092592
Iteration 5/25 | Loss: 0.00092592
Iteration 6/25 | Loss: 0.00092592
Iteration 7/25 | Loss: 0.00092592
Iteration 8/25 | Loss: 0.00092592
Iteration 9/25 | Loss: 0.00092592
Iteration 10/25 | Loss: 0.00092592
Iteration 11/25 | Loss: 0.00092592
Iteration 12/25 | Loss: 0.00092591
Iteration 13/25 | Loss: 0.00092591
Iteration 14/25 | Loss: 0.00092591
Iteration 15/25 | Loss: 0.00092591
Iteration 16/25 | Loss: 0.00092591
Iteration 17/25 | Loss: 0.00092591
Iteration 18/25 | Loss: 0.00092591
Iteration 19/25 | Loss: 0.00092591
Iteration 20/25 | Loss: 0.00092591
Iteration 21/25 | Loss: 0.00092591
Iteration 22/25 | Loss: 0.00092591
Iteration 23/25 | Loss: 0.00092591
Iteration 24/25 | Loss: 0.00092591
Iteration 25/25 | Loss: 0.00092591

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092591
Iteration 2/1000 | Loss: 0.00004098
Iteration 3/1000 | Loss: 0.00003041
Iteration 4/1000 | Loss: 0.00002824
Iteration 5/1000 | Loss: 0.00002715
Iteration 6/1000 | Loss: 0.00002637
Iteration 7/1000 | Loss: 0.00002581
Iteration 8/1000 | Loss: 0.00002549
Iteration 9/1000 | Loss: 0.00002510
Iteration 10/1000 | Loss: 0.00002505
Iteration 11/1000 | Loss: 0.00002498
Iteration 12/1000 | Loss: 0.00002476
Iteration 13/1000 | Loss: 0.00002473
Iteration 14/1000 | Loss: 0.00002464
Iteration 15/1000 | Loss: 0.00002448
Iteration 16/1000 | Loss: 0.00002439
Iteration 17/1000 | Loss: 0.00002438
Iteration 18/1000 | Loss: 0.00002436
Iteration 19/1000 | Loss: 0.00002435
Iteration 20/1000 | Loss: 0.00002430
Iteration 21/1000 | Loss: 0.00002420
Iteration 22/1000 | Loss: 0.00002411
Iteration 23/1000 | Loss: 0.00002408
Iteration 24/1000 | Loss: 0.00002408
Iteration 25/1000 | Loss: 0.00002408
Iteration 26/1000 | Loss: 0.00002407
Iteration 27/1000 | Loss: 0.00002402
Iteration 28/1000 | Loss: 0.00002401
Iteration 29/1000 | Loss: 0.00002400
Iteration 30/1000 | Loss: 0.00002400
Iteration 31/1000 | Loss: 0.00002399
Iteration 32/1000 | Loss: 0.00002399
Iteration 33/1000 | Loss: 0.00002395
Iteration 34/1000 | Loss: 0.00002395
Iteration 35/1000 | Loss: 0.00002395
Iteration 36/1000 | Loss: 0.00002395
Iteration 37/1000 | Loss: 0.00002395
Iteration 38/1000 | Loss: 0.00002395
Iteration 39/1000 | Loss: 0.00002394
Iteration 40/1000 | Loss: 0.00002394
Iteration 41/1000 | Loss: 0.00002394
Iteration 42/1000 | Loss: 0.00002394
Iteration 43/1000 | Loss: 0.00002394
Iteration 44/1000 | Loss: 0.00002394
Iteration 45/1000 | Loss: 0.00002394
Iteration 46/1000 | Loss: 0.00002391
Iteration 47/1000 | Loss: 0.00002390
Iteration 48/1000 | Loss: 0.00002389
Iteration 49/1000 | Loss: 0.00002389
Iteration 50/1000 | Loss: 0.00002388
Iteration 51/1000 | Loss: 0.00002388
Iteration 52/1000 | Loss: 0.00002386
Iteration 53/1000 | Loss: 0.00002386
Iteration 54/1000 | Loss: 0.00002385
Iteration 55/1000 | Loss: 0.00002384
Iteration 56/1000 | Loss: 0.00002384
Iteration 57/1000 | Loss: 0.00002384
Iteration 58/1000 | Loss: 0.00002384
Iteration 59/1000 | Loss: 0.00002384
Iteration 60/1000 | Loss: 0.00002383
Iteration 61/1000 | Loss: 0.00002383
Iteration 62/1000 | Loss: 0.00002383
Iteration 63/1000 | Loss: 0.00002383
Iteration 64/1000 | Loss: 0.00002383
Iteration 65/1000 | Loss: 0.00002383
Iteration 66/1000 | Loss: 0.00002383
Iteration 67/1000 | Loss: 0.00002383
Iteration 68/1000 | Loss: 0.00002383
Iteration 69/1000 | Loss: 0.00002383
Iteration 70/1000 | Loss: 0.00002383
Iteration 71/1000 | Loss: 0.00002382
Iteration 72/1000 | Loss: 0.00002382
Iteration 73/1000 | Loss: 0.00002381
Iteration 74/1000 | Loss: 0.00002381
Iteration 75/1000 | Loss: 0.00002381
Iteration 76/1000 | Loss: 0.00002381
Iteration 77/1000 | Loss: 0.00002380
Iteration 78/1000 | Loss: 0.00002380
Iteration 79/1000 | Loss: 0.00002379
Iteration 80/1000 | Loss: 0.00002379
Iteration 81/1000 | Loss: 0.00002379
Iteration 82/1000 | Loss: 0.00002379
Iteration 83/1000 | Loss: 0.00002379
Iteration 84/1000 | Loss: 0.00002379
Iteration 85/1000 | Loss: 0.00002379
Iteration 86/1000 | Loss: 0.00002379
Iteration 87/1000 | Loss: 0.00002378
Iteration 88/1000 | Loss: 0.00002378
Iteration 89/1000 | Loss: 0.00002377
Iteration 90/1000 | Loss: 0.00002377
Iteration 91/1000 | Loss: 0.00002377
Iteration 92/1000 | Loss: 0.00002377
Iteration 93/1000 | Loss: 0.00002377
Iteration 94/1000 | Loss: 0.00002377
Iteration 95/1000 | Loss: 0.00002376
Iteration 96/1000 | Loss: 0.00002376
Iteration 97/1000 | Loss: 0.00002376
Iteration 98/1000 | Loss: 0.00002376
Iteration 99/1000 | Loss: 0.00002375
Iteration 100/1000 | Loss: 0.00002375
Iteration 101/1000 | Loss: 0.00002375
Iteration 102/1000 | Loss: 0.00002374
Iteration 103/1000 | Loss: 0.00002374
Iteration 104/1000 | Loss: 0.00002374
Iteration 105/1000 | Loss: 0.00002374
Iteration 106/1000 | Loss: 0.00002374
Iteration 107/1000 | Loss: 0.00002374
Iteration 108/1000 | Loss: 0.00002374
Iteration 109/1000 | Loss: 0.00002373
Iteration 110/1000 | Loss: 0.00002373
Iteration 111/1000 | Loss: 0.00002372
Iteration 112/1000 | Loss: 0.00002372
Iteration 113/1000 | Loss: 0.00002372
Iteration 114/1000 | Loss: 0.00002372
Iteration 115/1000 | Loss: 0.00002371
Iteration 116/1000 | Loss: 0.00002371
Iteration 117/1000 | Loss: 0.00002370
Iteration 118/1000 | Loss: 0.00002369
Iteration 119/1000 | Loss: 0.00002369
Iteration 120/1000 | Loss: 0.00002369
Iteration 121/1000 | Loss: 0.00002369
Iteration 122/1000 | Loss: 0.00002369
Iteration 123/1000 | Loss: 0.00002369
Iteration 124/1000 | Loss: 0.00002369
Iteration 125/1000 | Loss: 0.00002368
Iteration 126/1000 | Loss: 0.00002368
Iteration 127/1000 | Loss: 0.00002368
Iteration 128/1000 | Loss: 0.00002368
Iteration 129/1000 | Loss: 0.00002368
Iteration 130/1000 | Loss: 0.00002368
Iteration 131/1000 | Loss: 0.00002368
Iteration 132/1000 | Loss: 0.00002368
Iteration 133/1000 | Loss: 0.00002368
Iteration 134/1000 | Loss: 0.00002367
Iteration 135/1000 | Loss: 0.00002367
Iteration 136/1000 | Loss: 0.00002367
Iteration 137/1000 | Loss: 0.00002367
Iteration 138/1000 | Loss: 0.00002367
Iteration 139/1000 | Loss: 0.00002367
Iteration 140/1000 | Loss: 0.00002366
Iteration 141/1000 | Loss: 0.00002366
Iteration 142/1000 | Loss: 0.00002366
Iteration 143/1000 | Loss: 0.00002366
Iteration 144/1000 | Loss: 0.00002366
Iteration 145/1000 | Loss: 0.00002366
Iteration 146/1000 | Loss: 0.00002366
Iteration 147/1000 | Loss: 0.00002366
Iteration 148/1000 | Loss: 0.00002366
Iteration 149/1000 | Loss: 0.00002366
Iteration 150/1000 | Loss: 0.00002366
Iteration 151/1000 | Loss: 0.00002366
Iteration 152/1000 | Loss: 0.00002366
Iteration 153/1000 | Loss: 0.00002366
Iteration 154/1000 | Loss: 0.00002366
Iteration 155/1000 | Loss: 0.00002366
Iteration 156/1000 | Loss: 0.00002366
Iteration 157/1000 | Loss: 0.00002366
Iteration 158/1000 | Loss: 0.00002365
Iteration 159/1000 | Loss: 0.00002365
Iteration 160/1000 | Loss: 0.00002365
Iteration 161/1000 | Loss: 0.00002365
Iteration 162/1000 | Loss: 0.00002365
Iteration 163/1000 | Loss: 0.00002365
Iteration 164/1000 | Loss: 0.00002365
Iteration 165/1000 | Loss: 0.00002365
Iteration 166/1000 | Loss: 0.00002365
Iteration 167/1000 | Loss: 0.00002365
Iteration 168/1000 | Loss: 0.00002365
Iteration 169/1000 | Loss: 0.00002365
Iteration 170/1000 | Loss: 0.00002365
Iteration 171/1000 | Loss: 0.00002365
Iteration 172/1000 | Loss: 0.00002365
Iteration 173/1000 | Loss: 0.00002365
Iteration 174/1000 | Loss: 0.00002365
Iteration 175/1000 | Loss: 0.00002365
Iteration 176/1000 | Loss: 0.00002364
Iteration 177/1000 | Loss: 0.00002364
Iteration 178/1000 | Loss: 0.00002364
Iteration 179/1000 | Loss: 0.00002364
Iteration 180/1000 | Loss: 0.00002364
Iteration 181/1000 | Loss: 0.00002364
Iteration 182/1000 | Loss: 0.00002364
Iteration 183/1000 | Loss: 0.00002364
Iteration 184/1000 | Loss: 0.00002364
Iteration 185/1000 | Loss: 0.00002364
Iteration 186/1000 | Loss: 0.00002364
Iteration 187/1000 | Loss: 0.00002364
Iteration 188/1000 | Loss: 0.00002364
Iteration 189/1000 | Loss: 0.00002364
Iteration 190/1000 | Loss: 0.00002364
Iteration 191/1000 | Loss: 0.00002364
Iteration 192/1000 | Loss: 0.00002364
Iteration 193/1000 | Loss: 0.00002364
Iteration 194/1000 | Loss: 0.00002364
Iteration 195/1000 | Loss: 0.00002364
Iteration 196/1000 | Loss: 0.00002364
Iteration 197/1000 | Loss: 0.00002364
Iteration 198/1000 | Loss: 0.00002364
Iteration 199/1000 | Loss: 0.00002364
Iteration 200/1000 | Loss: 0.00002364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [2.363625208090525e-05, 2.363625208090525e-05, 2.363625208090525e-05, 2.363625208090525e-05, 2.363625208090525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.363625208090525e-05

Optimization complete. Final v2v error: 3.7751760482788086 mm

Highest mean error: 4.754571437835693 mm for frame 92

Lowest mean error: 3.297642230987549 mm for frame 135

Saving results

Total time: 44.89231872558594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408635
Iteration 2/25 | Loss: 0.00145490
Iteration 3/25 | Loss: 0.00131632
Iteration 4/25 | Loss: 0.00128502
Iteration 5/25 | Loss: 0.00127647
Iteration 6/25 | Loss: 0.00127256
Iteration 7/25 | Loss: 0.00128167
Iteration 8/25 | Loss: 0.00126832
Iteration 9/25 | Loss: 0.00126265
Iteration 10/25 | Loss: 0.00125965
Iteration 11/25 | Loss: 0.00125800
Iteration 12/25 | Loss: 0.00125714
Iteration 13/25 | Loss: 0.00125677
Iteration 14/25 | Loss: 0.00125659
Iteration 15/25 | Loss: 0.00125657
Iteration 16/25 | Loss: 0.00125656
Iteration 17/25 | Loss: 0.00125656
Iteration 18/25 | Loss: 0.00125656
Iteration 19/25 | Loss: 0.00125656
Iteration 20/25 | Loss: 0.00125656
Iteration 21/25 | Loss: 0.00125656
Iteration 22/25 | Loss: 0.00125656
Iteration 23/25 | Loss: 0.00125655
Iteration 24/25 | Loss: 0.00125655
Iteration 25/25 | Loss: 0.00125655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41642451
Iteration 2/25 | Loss: 0.00072111
Iteration 3/25 | Loss: 0.00072110
Iteration 4/25 | Loss: 0.00072110
Iteration 5/25 | Loss: 0.00072110
Iteration 6/25 | Loss: 0.00072110
Iteration 7/25 | Loss: 0.00072110
Iteration 8/25 | Loss: 0.00072110
Iteration 9/25 | Loss: 0.00072110
Iteration 10/25 | Loss: 0.00072110
Iteration 11/25 | Loss: 0.00072110
Iteration 12/25 | Loss: 0.00072110
Iteration 13/25 | Loss: 0.00072110
Iteration 14/25 | Loss: 0.00072110
Iteration 15/25 | Loss: 0.00072110
Iteration 16/25 | Loss: 0.00072110
Iteration 17/25 | Loss: 0.00072110
Iteration 18/25 | Loss: 0.00072110
Iteration 19/25 | Loss: 0.00072110
Iteration 20/25 | Loss: 0.00072110
Iteration 21/25 | Loss: 0.00072110
Iteration 22/25 | Loss: 0.00072110
Iteration 23/25 | Loss: 0.00072110
Iteration 24/25 | Loss: 0.00072110
Iteration 25/25 | Loss: 0.00072110

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072110
Iteration 2/1000 | Loss: 0.00002517
Iteration 3/1000 | Loss: 0.00001910
Iteration 4/1000 | Loss: 0.00001744
Iteration 5/1000 | Loss: 0.00001632
Iteration 6/1000 | Loss: 0.00001579
Iteration 7/1000 | Loss: 0.00001545
Iteration 8/1000 | Loss: 0.00001507
Iteration 9/1000 | Loss: 0.00001485
Iteration 10/1000 | Loss: 0.00001469
Iteration 11/1000 | Loss: 0.00001460
Iteration 12/1000 | Loss: 0.00001454
Iteration 13/1000 | Loss: 0.00001452
Iteration 14/1000 | Loss: 0.00001451
Iteration 15/1000 | Loss: 0.00001451
Iteration 16/1000 | Loss: 0.00001450
Iteration 17/1000 | Loss: 0.00001448
Iteration 18/1000 | Loss: 0.00001445
Iteration 19/1000 | Loss: 0.00001442
Iteration 20/1000 | Loss: 0.00001442
Iteration 21/1000 | Loss: 0.00001440
Iteration 22/1000 | Loss: 0.00001439
Iteration 23/1000 | Loss: 0.00001438
Iteration 24/1000 | Loss: 0.00001435
Iteration 25/1000 | Loss: 0.00001430
Iteration 26/1000 | Loss: 0.00001428
Iteration 27/1000 | Loss: 0.00001427
Iteration 28/1000 | Loss: 0.00001425
Iteration 29/1000 | Loss: 0.00001425
Iteration 30/1000 | Loss: 0.00001425
Iteration 31/1000 | Loss: 0.00001424
Iteration 32/1000 | Loss: 0.00001424
Iteration 33/1000 | Loss: 0.00001421
Iteration 34/1000 | Loss: 0.00001420
Iteration 35/1000 | Loss: 0.00001420
Iteration 36/1000 | Loss: 0.00001419
Iteration 37/1000 | Loss: 0.00001418
Iteration 38/1000 | Loss: 0.00001418
Iteration 39/1000 | Loss: 0.00001418
Iteration 40/1000 | Loss: 0.00001418
Iteration 41/1000 | Loss: 0.00001417
Iteration 42/1000 | Loss: 0.00001415
Iteration 43/1000 | Loss: 0.00001415
Iteration 44/1000 | Loss: 0.00001415
Iteration 45/1000 | Loss: 0.00001415
Iteration 46/1000 | Loss: 0.00001415
Iteration 47/1000 | Loss: 0.00001415
Iteration 48/1000 | Loss: 0.00001415
Iteration 49/1000 | Loss: 0.00001415
Iteration 50/1000 | Loss: 0.00001414
Iteration 51/1000 | Loss: 0.00001414
Iteration 52/1000 | Loss: 0.00001414
Iteration 53/1000 | Loss: 0.00001413
Iteration 54/1000 | Loss: 0.00001413
Iteration 55/1000 | Loss: 0.00001412
Iteration 56/1000 | Loss: 0.00001412
Iteration 57/1000 | Loss: 0.00001412
Iteration 58/1000 | Loss: 0.00001411
Iteration 59/1000 | Loss: 0.00001411
Iteration 60/1000 | Loss: 0.00001410
Iteration 61/1000 | Loss: 0.00001410
Iteration 62/1000 | Loss: 0.00001410
Iteration 63/1000 | Loss: 0.00001410
Iteration 64/1000 | Loss: 0.00001410
Iteration 65/1000 | Loss: 0.00001410
Iteration 66/1000 | Loss: 0.00001410
Iteration 67/1000 | Loss: 0.00001410
Iteration 68/1000 | Loss: 0.00001409
Iteration 69/1000 | Loss: 0.00001409
Iteration 70/1000 | Loss: 0.00001409
Iteration 71/1000 | Loss: 0.00001408
Iteration 72/1000 | Loss: 0.00001408
Iteration 73/1000 | Loss: 0.00001408
Iteration 74/1000 | Loss: 0.00001407
Iteration 75/1000 | Loss: 0.00001407
Iteration 76/1000 | Loss: 0.00001407
Iteration 77/1000 | Loss: 0.00001407
Iteration 78/1000 | Loss: 0.00001407
Iteration 79/1000 | Loss: 0.00001406
Iteration 80/1000 | Loss: 0.00001406
Iteration 81/1000 | Loss: 0.00001406
Iteration 82/1000 | Loss: 0.00001406
Iteration 83/1000 | Loss: 0.00001405
Iteration 84/1000 | Loss: 0.00001405
Iteration 85/1000 | Loss: 0.00001405
Iteration 86/1000 | Loss: 0.00001404
Iteration 87/1000 | Loss: 0.00001404
Iteration 88/1000 | Loss: 0.00001404
Iteration 89/1000 | Loss: 0.00001404
Iteration 90/1000 | Loss: 0.00001403
Iteration 91/1000 | Loss: 0.00001403
Iteration 92/1000 | Loss: 0.00001403
Iteration 93/1000 | Loss: 0.00001403
Iteration 94/1000 | Loss: 0.00001403
Iteration 95/1000 | Loss: 0.00001403
Iteration 96/1000 | Loss: 0.00001402
Iteration 97/1000 | Loss: 0.00001402
Iteration 98/1000 | Loss: 0.00001402
Iteration 99/1000 | Loss: 0.00001402
Iteration 100/1000 | Loss: 0.00001402
Iteration 101/1000 | Loss: 0.00001402
Iteration 102/1000 | Loss: 0.00001401
Iteration 103/1000 | Loss: 0.00001401
Iteration 104/1000 | Loss: 0.00001401
Iteration 105/1000 | Loss: 0.00001401
Iteration 106/1000 | Loss: 0.00001400
Iteration 107/1000 | Loss: 0.00001400
Iteration 108/1000 | Loss: 0.00001400
Iteration 109/1000 | Loss: 0.00001399
Iteration 110/1000 | Loss: 0.00001399
Iteration 111/1000 | Loss: 0.00001398
Iteration 112/1000 | Loss: 0.00001398
Iteration 113/1000 | Loss: 0.00001397
Iteration 114/1000 | Loss: 0.00001397
Iteration 115/1000 | Loss: 0.00001397
Iteration 116/1000 | Loss: 0.00001396
Iteration 117/1000 | Loss: 0.00001396
Iteration 118/1000 | Loss: 0.00001396
Iteration 119/1000 | Loss: 0.00001396
Iteration 120/1000 | Loss: 0.00001395
Iteration 121/1000 | Loss: 0.00001395
Iteration 122/1000 | Loss: 0.00001395
Iteration 123/1000 | Loss: 0.00001394
Iteration 124/1000 | Loss: 0.00001394
Iteration 125/1000 | Loss: 0.00001394
Iteration 126/1000 | Loss: 0.00001394
Iteration 127/1000 | Loss: 0.00001394
Iteration 128/1000 | Loss: 0.00001394
Iteration 129/1000 | Loss: 0.00001394
Iteration 130/1000 | Loss: 0.00001394
Iteration 131/1000 | Loss: 0.00001394
Iteration 132/1000 | Loss: 0.00001394
Iteration 133/1000 | Loss: 0.00001394
Iteration 134/1000 | Loss: 0.00001394
Iteration 135/1000 | Loss: 0.00001394
Iteration 136/1000 | Loss: 0.00001394
Iteration 137/1000 | Loss: 0.00001394
Iteration 138/1000 | Loss: 0.00001394
Iteration 139/1000 | Loss: 0.00001394
Iteration 140/1000 | Loss: 0.00001394
Iteration 141/1000 | Loss: 0.00001394
Iteration 142/1000 | Loss: 0.00001394
Iteration 143/1000 | Loss: 0.00001394
Iteration 144/1000 | Loss: 0.00001394
Iteration 145/1000 | Loss: 0.00001394
Iteration 146/1000 | Loss: 0.00001394
Iteration 147/1000 | Loss: 0.00001394
Iteration 148/1000 | Loss: 0.00001394
Iteration 149/1000 | Loss: 0.00001394
Iteration 150/1000 | Loss: 0.00001394
Iteration 151/1000 | Loss: 0.00001394
Iteration 152/1000 | Loss: 0.00001394
Iteration 153/1000 | Loss: 0.00001394
Iteration 154/1000 | Loss: 0.00001394
Iteration 155/1000 | Loss: 0.00001394
Iteration 156/1000 | Loss: 0.00001394
Iteration 157/1000 | Loss: 0.00001394
Iteration 158/1000 | Loss: 0.00001394
Iteration 159/1000 | Loss: 0.00001394
Iteration 160/1000 | Loss: 0.00001394
Iteration 161/1000 | Loss: 0.00001394
Iteration 162/1000 | Loss: 0.00001394
Iteration 163/1000 | Loss: 0.00001394
Iteration 164/1000 | Loss: 0.00001394
Iteration 165/1000 | Loss: 0.00001394
Iteration 166/1000 | Loss: 0.00001394
Iteration 167/1000 | Loss: 0.00001394
Iteration 168/1000 | Loss: 0.00001394
Iteration 169/1000 | Loss: 0.00001394
Iteration 170/1000 | Loss: 0.00001394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.3937145922682248e-05, 1.3937145922682248e-05, 1.3937145922682248e-05, 1.3937145922682248e-05, 1.3937145922682248e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3937145922682248e-05

Optimization complete. Final v2v error: 3.2060863971710205 mm

Highest mean error: 3.563741683959961 mm for frame 134

Lowest mean error: 3.0038270950317383 mm for frame 236

Saving results

Total time: 62.78059959411621
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416395
Iteration 2/25 | Loss: 0.00131676
Iteration 3/25 | Loss: 0.00124558
Iteration 4/25 | Loss: 0.00124002
Iteration 5/25 | Loss: 0.00123853
Iteration 6/25 | Loss: 0.00123809
Iteration 7/25 | Loss: 0.00123809
Iteration 8/25 | Loss: 0.00123809
Iteration 9/25 | Loss: 0.00123809
Iteration 10/25 | Loss: 0.00123809
Iteration 11/25 | Loss: 0.00123809
Iteration 12/25 | Loss: 0.00123809
Iteration 13/25 | Loss: 0.00123809
Iteration 14/25 | Loss: 0.00123809
Iteration 15/25 | Loss: 0.00123809
Iteration 16/25 | Loss: 0.00123809
Iteration 17/25 | Loss: 0.00123809
Iteration 18/25 | Loss: 0.00123809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012380857951939106, 0.0012380857951939106, 0.0012380857951939106, 0.0012380857951939106, 0.0012380857951939106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012380857951939106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43106163
Iteration 2/25 | Loss: 0.00087103
Iteration 3/25 | Loss: 0.00087102
Iteration 4/25 | Loss: 0.00087102
Iteration 5/25 | Loss: 0.00087102
Iteration 6/25 | Loss: 0.00087102
Iteration 7/25 | Loss: 0.00087102
Iteration 8/25 | Loss: 0.00087102
Iteration 9/25 | Loss: 0.00087102
Iteration 10/25 | Loss: 0.00087102
Iteration 11/25 | Loss: 0.00087102
Iteration 12/25 | Loss: 0.00087102
Iteration 13/25 | Loss: 0.00087102
Iteration 14/25 | Loss: 0.00087102
Iteration 15/25 | Loss: 0.00087102
Iteration 16/25 | Loss: 0.00087102
Iteration 17/25 | Loss: 0.00087102
Iteration 18/25 | Loss: 0.00087102
Iteration 19/25 | Loss: 0.00087102
Iteration 20/25 | Loss: 0.00087102
Iteration 21/25 | Loss: 0.00087102
Iteration 22/25 | Loss: 0.00087102
Iteration 23/25 | Loss: 0.00087102
Iteration 24/25 | Loss: 0.00087102
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008710207184776664, 0.0008710207184776664, 0.0008710207184776664, 0.0008710207184776664, 0.0008710207184776664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008710207184776664

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087102
Iteration 2/1000 | Loss: 0.00002504
Iteration 3/1000 | Loss: 0.00001688
Iteration 4/1000 | Loss: 0.00001478
Iteration 5/1000 | Loss: 0.00001372
Iteration 6/1000 | Loss: 0.00001287
Iteration 7/1000 | Loss: 0.00001237
Iteration 8/1000 | Loss: 0.00001215
Iteration 9/1000 | Loss: 0.00001204
Iteration 10/1000 | Loss: 0.00001187
Iteration 11/1000 | Loss: 0.00001186
Iteration 12/1000 | Loss: 0.00001184
Iteration 13/1000 | Loss: 0.00001179
Iteration 14/1000 | Loss: 0.00001179
Iteration 15/1000 | Loss: 0.00001177
Iteration 16/1000 | Loss: 0.00001176
Iteration 17/1000 | Loss: 0.00001176
Iteration 18/1000 | Loss: 0.00001175
Iteration 19/1000 | Loss: 0.00001174
Iteration 20/1000 | Loss: 0.00001173
Iteration 21/1000 | Loss: 0.00001172
Iteration 22/1000 | Loss: 0.00001172
Iteration 23/1000 | Loss: 0.00001172
Iteration 24/1000 | Loss: 0.00001170
Iteration 25/1000 | Loss: 0.00001169
Iteration 26/1000 | Loss: 0.00001168
Iteration 27/1000 | Loss: 0.00001164
Iteration 28/1000 | Loss: 0.00001160
Iteration 29/1000 | Loss: 0.00001159
Iteration 30/1000 | Loss: 0.00001159
Iteration 31/1000 | Loss: 0.00001158
Iteration 32/1000 | Loss: 0.00001157
Iteration 33/1000 | Loss: 0.00001157
Iteration 34/1000 | Loss: 0.00001156
Iteration 35/1000 | Loss: 0.00001156
Iteration 36/1000 | Loss: 0.00001156
Iteration 37/1000 | Loss: 0.00001156
Iteration 38/1000 | Loss: 0.00001156
Iteration 39/1000 | Loss: 0.00001154
Iteration 40/1000 | Loss: 0.00001154
Iteration 41/1000 | Loss: 0.00001153
Iteration 42/1000 | Loss: 0.00001153
Iteration 43/1000 | Loss: 0.00001152
Iteration 44/1000 | Loss: 0.00001152
Iteration 45/1000 | Loss: 0.00001151
Iteration 46/1000 | Loss: 0.00001151
Iteration 47/1000 | Loss: 0.00001151
Iteration 48/1000 | Loss: 0.00001150
Iteration 49/1000 | Loss: 0.00001150
Iteration 50/1000 | Loss: 0.00001150
Iteration 51/1000 | Loss: 0.00001149
Iteration 52/1000 | Loss: 0.00001149
Iteration 53/1000 | Loss: 0.00001148
Iteration 54/1000 | Loss: 0.00001148
Iteration 55/1000 | Loss: 0.00001148
Iteration 56/1000 | Loss: 0.00001148
Iteration 57/1000 | Loss: 0.00001147
Iteration 58/1000 | Loss: 0.00001147
Iteration 59/1000 | Loss: 0.00001147
Iteration 60/1000 | Loss: 0.00001147
Iteration 61/1000 | Loss: 0.00001146
Iteration 62/1000 | Loss: 0.00001145
Iteration 63/1000 | Loss: 0.00001145
Iteration 64/1000 | Loss: 0.00001144
Iteration 65/1000 | Loss: 0.00001144
Iteration 66/1000 | Loss: 0.00001144
Iteration 67/1000 | Loss: 0.00001143
Iteration 68/1000 | Loss: 0.00001143
Iteration 69/1000 | Loss: 0.00001143
Iteration 70/1000 | Loss: 0.00001143
Iteration 71/1000 | Loss: 0.00001142
Iteration 72/1000 | Loss: 0.00001142
Iteration 73/1000 | Loss: 0.00001142
Iteration 74/1000 | Loss: 0.00001142
Iteration 75/1000 | Loss: 0.00001141
Iteration 76/1000 | Loss: 0.00001141
Iteration 77/1000 | Loss: 0.00001141
Iteration 78/1000 | Loss: 0.00001141
Iteration 79/1000 | Loss: 0.00001141
Iteration 80/1000 | Loss: 0.00001141
Iteration 81/1000 | Loss: 0.00001141
Iteration 82/1000 | Loss: 0.00001141
Iteration 83/1000 | Loss: 0.00001141
Iteration 84/1000 | Loss: 0.00001140
Iteration 85/1000 | Loss: 0.00001140
Iteration 86/1000 | Loss: 0.00001140
Iteration 87/1000 | Loss: 0.00001139
Iteration 88/1000 | Loss: 0.00001139
Iteration 89/1000 | Loss: 0.00001138
Iteration 90/1000 | Loss: 0.00001138
Iteration 91/1000 | Loss: 0.00001138
Iteration 92/1000 | Loss: 0.00001137
Iteration 93/1000 | Loss: 0.00001137
Iteration 94/1000 | Loss: 0.00001137
Iteration 95/1000 | Loss: 0.00001136
Iteration 96/1000 | Loss: 0.00001136
Iteration 97/1000 | Loss: 0.00001136
Iteration 98/1000 | Loss: 0.00001136
Iteration 99/1000 | Loss: 0.00001136
Iteration 100/1000 | Loss: 0.00001135
Iteration 101/1000 | Loss: 0.00001135
Iteration 102/1000 | Loss: 0.00001135
Iteration 103/1000 | Loss: 0.00001135
Iteration 104/1000 | Loss: 0.00001135
Iteration 105/1000 | Loss: 0.00001135
Iteration 106/1000 | Loss: 0.00001134
Iteration 107/1000 | Loss: 0.00001134
Iteration 108/1000 | Loss: 0.00001134
Iteration 109/1000 | Loss: 0.00001134
Iteration 110/1000 | Loss: 0.00001133
Iteration 111/1000 | Loss: 0.00001133
Iteration 112/1000 | Loss: 0.00001133
Iteration 113/1000 | Loss: 0.00001132
Iteration 114/1000 | Loss: 0.00001132
Iteration 115/1000 | Loss: 0.00001132
Iteration 116/1000 | Loss: 0.00001132
Iteration 117/1000 | Loss: 0.00001131
Iteration 118/1000 | Loss: 0.00001131
Iteration 119/1000 | Loss: 0.00001131
Iteration 120/1000 | Loss: 0.00001130
Iteration 121/1000 | Loss: 0.00001130
Iteration 122/1000 | Loss: 0.00001130
Iteration 123/1000 | Loss: 0.00001130
Iteration 124/1000 | Loss: 0.00001130
Iteration 125/1000 | Loss: 0.00001130
Iteration 126/1000 | Loss: 0.00001130
Iteration 127/1000 | Loss: 0.00001130
Iteration 128/1000 | Loss: 0.00001129
Iteration 129/1000 | Loss: 0.00001129
Iteration 130/1000 | Loss: 0.00001129
Iteration 131/1000 | Loss: 0.00001129
Iteration 132/1000 | Loss: 0.00001129
Iteration 133/1000 | Loss: 0.00001129
Iteration 134/1000 | Loss: 0.00001129
Iteration 135/1000 | Loss: 0.00001129
Iteration 136/1000 | Loss: 0.00001128
Iteration 137/1000 | Loss: 0.00001128
Iteration 138/1000 | Loss: 0.00001128
Iteration 139/1000 | Loss: 0.00001128
Iteration 140/1000 | Loss: 0.00001128
Iteration 141/1000 | Loss: 0.00001128
Iteration 142/1000 | Loss: 0.00001128
Iteration 143/1000 | Loss: 0.00001128
Iteration 144/1000 | Loss: 0.00001128
Iteration 145/1000 | Loss: 0.00001128
Iteration 146/1000 | Loss: 0.00001128
Iteration 147/1000 | Loss: 0.00001128
Iteration 148/1000 | Loss: 0.00001128
Iteration 149/1000 | Loss: 0.00001128
Iteration 150/1000 | Loss: 0.00001127
Iteration 151/1000 | Loss: 0.00001127
Iteration 152/1000 | Loss: 0.00001127
Iteration 153/1000 | Loss: 0.00001127
Iteration 154/1000 | Loss: 0.00001127
Iteration 155/1000 | Loss: 0.00001127
Iteration 156/1000 | Loss: 0.00001127
Iteration 157/1000 | Loss: 0.00001127
Iteration 158/1000 | Loss: 0.00001127
Iteration 159/1000 | Loss: 0.00001127
Iteration 160/1000 | Loss: 0.00001127
Iteration 161/1000 | Loss: 0.00001127
Iteration 162/1000 | Loss: 0.00001127
Iteration 163/1000 | Loss: 0.00001127
Iteration 164/1000 | Loss: 0.00001127
Iteration 165/1000 | Loss: 0.00001127
Iteration 166/1000 | Loss: 0.00001126
Iteration 167/1000 | Loss: 0.00001126
Iteration 168/1000 | Loss: 0.00001126
Iteration 169/1000 | Loss: 0.00001126
Iteration 170/1000 | Loss: 0.00001126
Iteration 171/1000 | Loss: 0.00001126
Iteration 172/1000 | Loss: 0.00001126
Iteration 173/1000 | Loss: 0.00001126
Iteration 174/1000 | Loss: 0.00001126
Iteration 175/1000 | Loss: 0.00001126
Iteration 176/1000 | Loss: 0.00001126
Iteration 177/1000 | Loss: 0.00001126
Iteration 178/1000 | Loss: 0.00001126
Iteration 179/1000 | Loss: 0.00001126
Iteration 180/1000 | Loss: 0.00001126
Iteration 181/1000 | Loss: 0.00001126
Iteration 182/1000 | Loss: 0.00001126
Iteration 183/1000 | Loss: 0.00001126
Iteration 184/1000 | Loss: 0.00001126
Iteration 185/1000 | Loss: 0.00001126
Iteration 186/1000 | Loss: 0.00001126
Iteration 187/1000 | Loss: 0.00001126
Iteration 188/1000 | Loss: 0.00001126
Iteration 189/1000 | Loss: 0.00001126
Iteration 190/1000 | Loss: 0.00001126
Iteration 191/1000 | Loss: 0.00001126
Iteration 192/1000 | Loss: 0.00001126
Iteration 193/1000 | Loss: 0.00001126
Iteration 194/1000 | Loss: 0.00001126
Iteration 195/1000 | Loss: 0.00001126
Iteration 196/1000 | Loss: 0.00001126
Iteration 197/1000 | Loss: 0.00001126
Iteration 198/1000 | Loss: 0.00001126
Iteration 199/1000 | Loss: 0.00001126
Iteration 200/1000 | Loss: 0.00001126
Iteration 201/1000 | Loss: 0.00001126
Iteration 202/1000 | Loss: 0.00001126
Iteration 203/1000 | Loss: 0.00001126
Iteration 204/1000 | Loss: 0.00001126
Iteration 205/1000 | Loss: 0.00001126
Iteration 206/1000 | Loss: 0.00001126
Iteration 207/1000 | Loss: 0.00001126
Iteration 208/1000 | Loss: 0.00001126
Iteration 209/1000 | Loss: 0.00001126
Iteration 210/1000 | Loss: 0.00001126
Iteration 211/1000 | Loss: 0.00001126
Iteration 212/1000 | Loss: 0.00001126
Iteration 213/1000 | Loss: 0.00001126
Iteration 214/1000 | Loss: 0.00001126
Iteration 215/1000 | Loss: 0.00001126
Iteration 216/1000 | Loss: 0.00001126
Iteration 217/1000 | Loss: 0.00001126
Iteration 218/1000 | Loss: 0.00001126
Iteration 219/1000 | Loss: 0.00001126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.1264763998042326e-05, 1.1264763998042326e-05, 1.1264763998042326e-05, 1.1264763998042326e-05, 1.1264763998042326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1264763998042326e-05

Optimization complete. Final v2v error: 2.8722612857818604 mm

Highest mean error: 3.671851873397827 mm for frame 54

Lowest mean error: 2.7233777046203613 mm for frame 138

Saving results

Total time: 39.73449754714966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00529297
Iteration 2/25 | Loss: 0.00170931
Iteration 3/25 | Loss: 0.00141866
Iteration 4/25 | Loss: 0.00137741
Iteration 5/25 | Loss: 0.00136880
Iteration 6/25 | Loss: 0.00136650
Iteration 7/25 | Loss: 0.00136650
Iteration 8/25 | Loss: 0.00136650
Iteration 9/25 | Loss: 0.00136650
Iteration 10/25 | Loss: 0.00136650
Iteration 11/25 | Loss: 0.00136650
Iteration 12/25 | Loss: 0.00136650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013664955040439963, 0.0013664955040439963, 0.0013664955040439963, 0.0013664955040439963, 0.0013664955040439963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013664955040439963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.78431940
Iteration 2/25 | Loss: 0.00070563
Iteration 3/25 | Loss: 0.00070563
Iteration 4/25 | Loss: 0.00070563
Iteration 5/25 | Loss: 0.00070563
Iteration 6/25 | Loss: 0.00070563
Iteration 7/25 | Loss: 0.00070563
Iteration 8/25 | Loss: 0.00070563
Iteration 9/25 | Loss: 0.00070563
Iteration 10/25 | Loss: 0.00070563
Iteration 11/25 | Loss: 0.00070563
Iteration 12/25 | Loss: 0.00070563
Iteration 13/25 | Loss: 0.00070563
Iteration 14/25 | Loss: 0.00070563
Iteration 15/25 | Loss: 0.00070563
Iteration 16/25 | Loss: 0.00070563
Iteration 17/25 | Loss: 0.00070563
Iteration 18/25 | Loss: 0.00070563
Iteration 19/25 | Loss: 0.00070563
Iteration 20/25 | Loss: 0.00070563
Iteration 21/25 | Loss: 0.00070563
Iteration 22/25 | Loss: 0.00070563
Iteration 23/25 | Loss: 0.00070563
Iteration 24/25 | Loss: 0.00070563
Iteration 25/25 | Loss: 0.00070563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070563
Iteration 2/1000 | Loss: 0.00005671
Iteration 3/1000 | Loss: 0.00003485
Iteration 4/1000 | Loss: 0.00003154
Iteration 5/1000 | Loss: 0.00002938
Iteration 6/1000 | Loss: 0.00002794
Iteration 7/1000 | Loss: 0.00002744
Iteration 8/1000 | Loss: 0.00002690
Iteration 9/1000 | Loss: 0.00002661
Iteration 10/1000 | Loss: 0.00002636
Iteration 11/1000 | Loss: 0.00002629
Iteration 12/1000 | Loss: 0.00002625
Iteration 13/1000 | Loss: 0.00002624
Iteration 14/1000 | Loss: 0.00002620
Iteration 15/1000 | Loss: 0.00002614
Iteration 16/1000 | Loss: 0.00002614
Iteration 17/1000 | Loss: 0.00002613
Iteration 18/1000 | Loss: 0.00002606
Iteration 19/1000 | Loss: 0.00002603
Iteration 20/1000 | Loss: 0.00002602
Iteration 21/1000 | Loss: 0.00002599
Iteration 22/1000 | Loss: 0.00002599
Iteration 23/1000 | Loss: 0.00002598
Iteration 24/1000 | Loss: 0.00002595
Iteration 25/1000 | Loss: 0.00002595
Iteration 26/1000 | Loss: 0.00002595
Iteration 27/1000 | Loss: 0.00002595
Iteration 28/1000 | Loss: 0.00002595
Iteration 29/1000 | Loss: 0.00002594
Iteration 30/1000 | Loss: 0.00002594
Iteration 31/1000 | Loss: 0.00002594
Iteration 32/1000 | Loss: 0.00002594
Iteration 33/1000 | Loss: 0.00002594
Iteration 34/1000 | Loss: 0.00002594
Iteration 35/1000 | Loss: 0.00002593
Iteration 36/1000 | Loss: 0.00002593
Iteration 37/1000 | Loss: 0.00002592
Iteration 38/1000 | Loss: 0.00002592
Iteration 39/1000 | Loss: 0.00002591
Iteration 40/1000 | Loss: 0.00002591
Iteration 41/1000 | Loss: 0.00002591
Iteration 42/1000 | Loss: 0.00002591
Iteration 43/1000 | Loss: 0.00002591
Iteration 44/1000 | Loss: 0.00002591
Iteration 45/1000 | Loss: 0.00002591
Iteration 46/1000 | Loss: 0.00002591
Iteration 47/1000 | Loss: 0.00002590
Iteration 48/1000 | Loss: 0.00002590
Iteration 49/1000 | Loss: 0.00002590
Iteration 50/1000 | Loss: 0.00002590
Iteration 51/1000 | Loss: 0.00002590
Iteration 52/1000 | Loss: 0.00002589
Iteration 53/1000 | Loss: 0.00002589
Iteration 54/1000 | Loss: 0.00002589
Iteration 55/1000 | Loss: 0.00002589
Iteration 56/1000 | Loss: 0.00002589
Iteration 57/1000 | Loss: 0.00002589
Iteration 58/1000 | Loss: 0.00002589
Iteration 59/1000 | Loss: 0.00002588
Iteration 60/1000 | Loss: 0.00002588
Iteration 61/1000 | Loss: 0.00002588
Iteration 62/1000 | Loss: 0.00002588
Iteration 63/1000 | Loss: 0.00002588
Iteration 64/1000 | Loss: 0.00002588
Iteration 65/1000 | Loss: 0.00002588
Iteration 66/1000 | Loss: 0.00002588
Iteration 67/1000 | Loss: 0.00002588
Iteration 68/1000 | Loss: 0.00002587
Iteration 69/1000 | Loss: 0.00002587
Iteration 70/1000 | Loss: 0.00002587
Iteration 71/1000 | Loss: 0.00002587
Iteration 72/1000 | Loss: 0.00002587
Iteration 73/1000 | Loss: 0.00002587
Iteration 74/1000 | Loss: 0.00002587
Iteration 75/1000 | Loss: 0.00002587
Iteration 76/1000 | Loss: 0.00002587
Iteration 77/1000 | Loss: 0.00002587
Iteration 78/1000 | Loss: 0.00002586
Iteration 79/1000 | Loss: 0.00002586
Iteration 80/1000 | Loss: 0.00002586
Iteration 81/1000 | Loss: 0.00002585
Iteration 82/1000 | Loss: 0.00002585
Iteration 83/1000 | Loss: 0.00002585
Iteration 84/1000 | Loss: 0.00002585
Iteration 85/1000 | Loss: 0.00002585
Iteration 86/1000 | Loss: 0.00002585
Iteration 87/1000 | Loss: 0.00002585
Iteration 88/1000 | Loss: 0.00002585
Iteration 89/1000 | Loss: 0.00002585
Iteration 90/1000 | Loss: 0.00002585
Iteration 91/1000 | Loss: 0.00002585
Iteration 92/1000 | Loss: 0.00002585
Iteration 93/1000 | Loss: 0.00002585
Iteration 94/1000 | Loss: 0.00002584
Iteration 95/1000 | Loss: 0.00002584
Iteration 96/1000 | Loss: 0.00002584
Iteration 97/1000 | Loss: 0.00002584
Iteration 98/1000 | Loss: 0.00002584
Iteration 99/1000 | Loss: 0.00002584
Iteration 100/1000 | Loss: 0.00002584
Iteration 101/1000 | Loss: 0.00002584
Iteration 102/1000 | Loss: 0.00002584
Iteration 103/1000 | Loss: 0.00002584
Iteration 104/1000 | Loss: 0.00002584
Iteration 105/1000 | Loss: 0.00002584
Iteration 106/1000 | Loss: 0.00002584
Iteration 107/1000 | Loss: 0.00002584
Iteration 108/1000 | Loss: 0.00002584
Iteration 109/1000 | Loss: 0.00002584
Iteration 110/1000 | Loss: 0.00002584
Iteration 111/1000 | Loss: 0.00002584
Iteration 112/1000 | Loss: 0.00002584
Iteration 113/1000 | Loss: 0.00002584
Iteration 114/1000 | Loss: 0.00002584
Iteration 115/1000 | Loss: 0.00002584
Iteration 116/1000 | Loss: 0.00002584
Iteration 117/1000 | Loss: 0.00002584
Iteration 118/1000 | Loss: 0.00002584
Iteration 119/1000 | Loss: 0.00002584
Iteration 120/1000 | Loss: 0.00002584
Iteration 121/1000 | Loss: 0.00002584
Iteration 122/1000 | Loss: 0.00002584
Iteration 123/1000 | Loss: 0.00002584
Iteration 124/1000 | Loss: 0.00002584
Iteration 125/1000 | Loss: 0.00002584
Iteration 126/1000 | Loss: 0.00002584
Iteration 127/1000 | Loss: 0.00002584
Iteration 128/1000 | Loss: 0.00002584
Iteration 129/1000 | Loss: 0.00002584
Iteration 130/1000 | Loss: 0.00002584
Iteration 131/1000 | Loss: 0.00002584
Iteration 132/1000 | Loss: 0.00002584
Iteration 133/1000 | Loss: 0.00002584
Iteration 134/1000 | Loss: 0.00002584
Iteration 135/1000 | Loss: 0.00002584
Iteration 136/1000 | Loss: 0.00002584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.583920104370918e-05, 2.583920104370918e-05, 2.583920104370918e-05, 2.583920104370918e-05, 2.583920104370918e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.583920104370918e-05

Optimization complete. Final v2v error: 4.287922382354736 mm

Highest mean error: 4.579469680786133 mm for frame 17

Lowest mean error: 4.002048492431641 mm for frame 34

Saving results

Total time: 33.066916704177856
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824195
Iteration 2/25 | Loss: 0.00151230
Iteration 3/25 | Loss: 0.00129758
Iteration 4/25 | Loss: 0.00126996
Iteration 5/25 | Loss: 0.00126529
Iteration 6/25 | Loss: 0.00126468
Iteration 7/25 | Loss: 0.00126468
Iteration 8/25 | Loss: 0.00126468
Iteration 9/25 | Loss: 0.00126468
Iteration 10/25 | Loss: 0.00126468
Iteration 11/25 | Loss: 0.00126468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012646815739572048, 0.0012646815739572048, 0.0012646815739572048, 0.0012646815739572048, 0.0012646815739572048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012646815739572048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42893839
Iteration 2/25 | Loss: 0.00077270
Iteration 3/25 | Loss: 0.00077270
Iteration 4/25 | Loss: 0.00077270
Iteration 5/25 | Loss: 0.00077270
Iteration 6/25 | Loss: 0.00077270
Iteration 7/25 | Loss: 0.00077270
Iteration 8/25 | Loss: 0.00077270
Iteration 9/25 | Loss: 0.00077270
Iteration 10/25 | Loss: 0.00077270
Iteration 11/25 | Loss: 0.00077270
Iteration 12/25 | Loss: 0.00077270
Iteration 13/25 | Loss: 0.00077270
Iteration 14/25 | Loss: 0.00077270
Iteration 15/25 | Loss: 0.00077270
Iteration 16/25 | Loss: 0.00077270
Iteration 17/25 | Loss: 0.00077270
Iteration 18/25 | Loss: 0.00077270
Iteration 19/25 | Loss: 0.00077270
Iteration 20/25 | Loss: 0.00077270
Iteration 21/25 | Loss: 0.00077270
Iteration 22/25 | Loss: 0.00077270
Iteration 23/25 | Loss: 0.00077270
Iteration 24/25 | Loss: 0.00077270
Iteration 25/25 | Loss: 0.00077270

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077270
Iteration 2/1000 | Loss: 0.00003533
Iteration 3/1000 | Loss: 0.00002484
Iteration 4/1000 | Loss: 0.00001970
Iteration 5/1000 | Loss: 0.00001832
Iteration 6/1000 | Loss: 0.00001704
Iteration 7/1000 | Loss: 0.00001618
Iteration 8/1000 | Loss: 0.00001573
Iteration 9/1000 | Loss: 0.00001536
Iteration 10/1000 | Loss: 0.00001497
Iteration 11/1000 | Loss: 0.00001468
Iteration 12/1000 | Loss: 0.00001468
Iteration 13/1000 | Loss: 0.00001456
Iteration 14/1000 | Loss: 0.00001454
Iteration 15/1000 | Loss: 0.00001451
Iteration 16/1000 | Loss: 0.00001451
Iteration 17/1000 | Loss: 0.00001443
Iteration 18/1000 | Loss: 0.00001439
Iteration 19/1000 | Loss: 0.00001437
Iteration 20/1000 | Loss: 0.00001437
Iteration 21/1000 | Loss: 0.00001436
Iteration 22/1000 | Loss: 0.00001436
Iteration 23/1000 | Loss: 0.00001435
Iteration 24/1000 | Loss: 0.00001434
Iteration 25/1000 | Loss: 0.00001434
Iteration 26/1000 | Loss: 0.00001432
Iteration 27/1000 | Loss: 0.00001431
Iteration 28/1000 | Loss: 0.00001430
Iteration 29/1000 | Loss: 0.00001429
Iteration 30/1000 | Loss: 0.00001429
Iteration 31/1000 | Loss: 0.00001429
Iteration 32/1000 | Loss: 0.00001427
Iteration 33/1000 | Loss: 0.00001427
Iteration 34/1000 | Loss: 0.00001426
Iteration 35/1000 | Loss: 0.00001426
Iteration 36/1000 | Loss: 0.00001426
Iteration 37/1000 | Loss: 0.00001425
Iteration 38/1000 | Loss: 0.00001425
Iteration 39/1000 | Loss: 0.00001425
Iteration 40/1000 | Loss: 0.00001424
Iteration 41/1000 | Loss: 0.00001424
Iteration 42/1000 | Loss: 0.00001424
Iteration 43/1000 | Loss: 0.00001424
Iteration 44/1000 | Loss: 0.00001424
Iteration 45/1000 | Loss: 0.00001423
Iteration 46/1000 | Loss: 0.00001423
Iteration 47/1000 | Loss: 0.00001423
Iteration 48/1000 | Loss: 0.00001423
Iteration 49/1000 | Loss: 0.00001422
Iteration 50/1000 | Loss: 0.00001422
Iteration 51/1000 | Loss: 0.00001422
Iteration 52/1000 | Loss: 0.00001422
Iteration 53/1000 | Loss: 0.00001422
Iteration 54/1000 | Loss: 0.00001422
Iteration 55/1000 | Loss: 0.00001421
Iteration 56/1000 | Loss: 0.00001421
Iteration 57/1000 | Loss: 0.00001421
Iteration 58/1000 | Loss: 0.00001421
Iteration 59/1000 | Loss: 0.00001421
Iteration 60/1000 | Loss: 0.00001421
Iteration 61/1000 | Loss: 0.00001421
Iteration 62/1000 | Loss: 0.00001420
Iteration 63/1000 | Loss: 0.00001420
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001420
Iteration 66/1000 | Loss: 0.00001420
Iteration 67/1000 | Loss: 0.00001419
Iteration 68/1000 | Loss: 0.00001419
Iteration 69/1000 | Loss: 0.00001419
Iteration 70/1000 | Loss: 0.00001419
Iteration 71/1000 | Loss: 0.00001418
Iteration 72/1000 | Loss: 0.00001418
Iteration 73/1000 | Loss: 0.00001418
Iteration 74/1000 | Loss: 0.00001418
Iteration 75/1000 | Loss: 0.00001418
Iteration 76/1000 | Loss: 0.00001418
Iteration 77/1000 | Loss: 0.00001418
Iteration 78/1000 | Loss: 0.00001418
Iteration 79/1000 | Loss: 0.00001418
Iteration 80/1000 | Loss: 0.00001418
Iteration 81/1000 | Loss: 0.00001417
Iteration 82/1000 | Loss: 0.00001416
Iteration 83/1000 | Loss: 0.00001416
Iteration 84/1000 | Loss: 0.00001416
Iteration 85/1000 | Loss: 0.00001416
Iteration 86/1000 | Loss: 0.00001416
Iteration 87/1000 | Loss: 0.00001416
Iteration 88/1000 | Loss: 0.00001416
Iteration 89/1000 | Loss: 0.00001416
Iteration 90/1000 | Loss: 0.00001415
Iteration 91/1000 | Loss: 0.00001415
Iteration 92/1000 | Loss: 0.00001415
Iteration 93/1000 | Loss: 0.00001415
Iteration 94/1000 | Loss: 0.00001415
Iteration 95/1000 | Loss: 0.00001415
Iteration 96/1000 | Loss: 0.00001414
Iteration 97/1000 | Loss: 0.00001414
Iteration 98/1000 | Loss: 0.00001413
Iteration 99/1000 | Loss: 0.00001413
Iteration 100/1000 | Loss: 0.00001413
Iteration 101/1000 | Loss: 0.00001413
Iteration 102/1000 | Loss: 0.00001413
Iteration 103/1000 | Loss: 0.00001412
Iteration 104/1000 | Loss: 0.00001412
Iteration 105/1000 | Loss: 0.00001412
Iteration 106/1000 | Loss: 0.00001411
Iteration 107/1000 | Loss: 0.00001411
Iteration 108/1000 | Loss: 0.00001411
Iteration 109/1000 | Loss: 0.00001411
Iteration 110/1000 | Loss: 0.00001410
Iteration 111/1000 | Loss: 0.00001410
Iteration 112/1000 | Loss: 0.00001410
Iteration 113/1000 | Loss: 0.00001410
Iteration 114/1000 | Loss: 0.00001410
Iteration 115/1000 | Loss: 0.00001410
Iteration 116/1000 | Loss: 0.00001410
Iteration 117/1000 | Loss: 0.00001410
Iteration 118/1000 | Loss: 0.00001410
Iteration 119/1000 | Loss: 0.00001409
Iteration 120/1000 | Loss: 0.00001409
Iteration 121/1000 | Loss: 0.00001409
Iteration 122/1000 | Loss: 0.00001409
Iteration 123/1000 | Loss: 0.00001409
Iteration 124/1000 | Loss: 0.00001409
Iteration 125/1000 | Loss: 0.00001409
Iteration 126/1000 | Loss: 0.00001409
Iteration 127/1000 | Loss: 0.00001409
Iteration 128/1000 | Loss: 0.00001409
Iteration 129/1000 | Loss: 0.00001409
Iteration 130/1000 | Loss: 0.00001409
Iteration 131/1000 | Loss: 0.00001408
Iteration 132/1000 | Loss: 0.00001408
Iteration 133/1000 | Loss: 0.00001408
Iteration 134/1000 | Loss: 0.00001408
Iteration 135/1000 | Loss: 0.00001407
Iteration 136/1000 | Loss: 0.00001407
Iteration 137/1000 | Loss: 0.00001407
Iteration 138/1000 | Loss: 0.00001407
Iteration 139/1000 | Loss: 0.00001407
Iteration 140/1000 | Loss: 0.00001407
Iteration 141/1000 | Loss: 0.00001407
Iteration 142/1000 | Loss: 0.00001407
Iteration 143/1000 | Loss: 0.00001407
Iteration 144/1000 | Loss: 0.00001407
Iteration 145/1000 | Loss: 0.00001407
Iteration 146/1000 | Loss: 0.00001407
Iteration 147/1000 | Loss: 0.00001407
Iteration 148/1000 | Loss: 0.00001407
Iteration 149/1000 | Loss: 0.00001407
Iteration 150/1000 | Loss: 0.00001407
Iteration 151/1000 | Loss: 0.00001407
Iteration 152/1000 | Loss: 0.00001407
Iteration 153/1000 | Loss: 0.00001407
Iteration 154/1000 | Loss: 0.00001407
Iteration 155/1000 | Loss: 0.00001407
Iteration 156/1000 | Loss: 0.00001407
Iteration 157/1000 | Loss: 0.00001407
Iteration 158/1000 | Loss: 0.00001407
Iteration 159/1000 | Loss: 0.00001407
Iteration 160/1000 | Loss: 0.00001407
Iteration 161/1000 | Loss: 0.00001407
Iteration 162/1000 | Loss: 0.00001407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.4068281416257378e-05, 1.4068281416257378e-05, 1.4068281416257378e-05, 1.4068281416257378e-05, 1.4068281416257378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4068281416257378e-05

Optimization complete. Final v2v error: 3.210007429122925 mm

Highest mean error: 3.929286241531372 mm for frame 113

Lowest mean error: 2.8461334705352783 mm for frame 25

Saving results

Total time: 40.44556903839111
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00534768
Iteration 2/25 | Loss: 0.00135751
Iteration 3/25 | Loss: 0.00128373
Iteration 4/25 | Loss: 0.00127337
Iteration 5/25 | Loss: 0.00127004
Iteration 6/25 | Loss: 0.00126990
Iteration 7/25 | Loss: 0.00126990
Iteration 8/25 | Loss: 0.00126990
Iteration 9/25 | Loss: 0.00126990
Iteration 10/25 | Loss: 0.00126990
Iteration 11/25 | Loss: 0.00126990
Iteration 12/25 | Loss: 0.00126990
Iteration 13/25 | Loss: 0.00126990
Iteration 14/25 | Loss: 0.00126990
Iteration 15/25 | Loss: 0.00126990
Iteration 16/25 | Loss: 0.00126990
Iteration 17/25 | Loss: 0.00126990
Iteration 18/25 | Loss: 0.00126990
Iteration 19/25 | Loss: 0.00126990
Iteration 20/25 | Loss: 0.00126990
Iteration 21/25 | Loss: 0.00126990
Iteration 22/25 | Loss: 0.00126990
Iteration 23/25 | Loss: 0.00126990
Iteration 24/25 | Loss: 0.00126990
Iteration 25/25 | Loss: 0.00126990

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.68393850
Iteration 2/25 | Loss: 0.00081039
Iteration 3/25 | Loss: 0.00081036
Iteration 4/25 | Loss: 0.00081036
Iteration 5/25 | Loss: 0.00081036
Iteration 6/25 | Loss: 0.00081036
Iteration 7/25 | Loss: 0.00081036
Iteration 8/25 | Loss: 0.00081036
Iteration 9/25 | Loss: 0.00081036
Iteration 10/25 | Loss: 0.00081036
Iteration 11/25 | Loss: 0.00081036
Iteration 12/25 | Loss: 0.00081036
Iteration 13/25 | Loss: 0.00081036
Iteration 14/25 | Loss: 0.00081036
Iteration 15/25 | Loss: 0.00081036
Iteration 16/25 | Loss: 0.00081036
Iteration 17/25 | Loss: 0.00081036
Iteration 18/25 | Loss: 0.00081036
Iteration 19/25 | Loss: 0.00081036
Iteration 20/25 | Loss: 0.00081036
Iteration 21/25 | Loss: 0.00081036
Iteration 22/25 | Loss: 0.00081036
Iteration 23/25 | Loss: 0.00081036
Iteration 24/25 | Loss: 0.00081036
Iteration 25/25 | Loss: 0.00081036

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081036
Iteration 2/1000 | Loss: 0.00002921
Iteration 3/1000 | Loss: 0.00002147
Iteration 4/1000 | Loss: 0.00001869
Iteration 5/1000 | Loss: 0.00001768
Iteration 6/1000 | Loss: 0.00001688
Iteration 7/1000 | Loss: 0.00001640
Iteration 8/1000 | Loss: 0.00001593
Iteration 9/1000 | Loss: 0.00001563
Iteration 10/1000 | Loss: 0.00001531
Iteration 11/1000 | Loss: 0.00001510
Iteration 12/1000 | Loss: 0.00001496
Iteration 13/1000 | Loss: 0.00001494
Iteration 14/1000 | Loss: 0.00001490
Iteration 15/1000 | Loss: 0.00001478
Iteration 16/1000 | Loss: 0.00001474
Iteration 17/1000 | Loss: 0.00001474
Iteration 18/1000 | Loss: 0.00001472
Iteration 19/1000 | Loss: 0.00001471
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00001470
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001470
Iteration 24/1000 | Loss: 0.00001469
Iteration 25/1000 | Loss: 0.00001469
Iteration 26/1000 | Loss: 0.00001468
Iteration 27/1000 | Loss: 0.00001465
Iteration 28/1000 | Loss: 0.00001465
Iteration 29/1000 | Loss: 0.00001462
Iteration 30/1000 | Loss: 0.00001461
Iteration 31/1000 | Loss: 0.00001456
Iteration 32/1000 | Loss: 0.00001456
Iteration 33/1000 | Loss: 0.00001453
Iteration 34/1000 | Loss: 0.00001452
Iteration 35/1000 | Loss: 0.00001450
Iteration 36/1000 | Loss: 0.00001447
Iteration 37/1000 | Loss: 0.00001444
Iteration 38/1000 | Loss: 0.00001444
Iteration 39/1000 | Loss: 0.00001444
Iteration 40/1000 | Loss: 0.00001443
Iteration 41/1000 | Loss: 0.00001443
Iteration 42/1000 | Loss: 0.00001442
Iteration 43/1000 | Loss: 0.00001440
Iteration 44/1000 | Loss: 0.00001440
Iteration 45/1000 | Loss: 0.00001440
Iteration 46/1000 | Loss: 0.00001439
Iteration 47/1000 | Loss: 0.00001439
Iteration 48/1000 | Loss: 0.00001438
Iteration 49/1000 | Loss: 0.00001438
Iteration 50/1000 | Loss: 0.00001437
Iteration 51/1000 | Loss: 0.00001434
Iteration 52/1000 | Loss: 0.00001432
Iteration 53/1000 | Loss: 0.00001432
Iteration 54/1000 | Loss: 0.00001431
Iteration 55/1000 | Loss: 0.00001430
Iteration 56/1000 | Loss: 0.00001430
Iteration 57/1000 | Loss: 0.00001428
Iteration 58/1000 | Loss: 0.00001428
Iteration 59/1000 | Loss: 0.00001428
Iteration 60/1000 | Loss: 0.00001428
Iteration 61/1000 | Loss: 0.00001428
Iteration 62/1000 | Loss: 0.00001428
Iteration 63/1000 | Loss: 0.00001428
Iteration 64/1000 | Loss: 0.00001427
Iteration 65/1000 | Loss: 0.00001427
Iteration 66/1000 | Loss: 0.00001427
Iteration 67/1000 | Loss: 0.00001427
Iteration 68/1000 | Loss: 0.00001426
Iteration 69/1000 | Loss: 0.00001426
Iteration 70/1000 | Loss: 0.00001426
Iteration 71/1000 | Loss: 0.00001426
Iteration 72/1000 | Loss: 0.00001425
Iteration 73/1000 | Loss: 0.00001424
Iteration 74/1000 | Loss: 0.00001424
Iteration 75/1000 | Loss: 0.00001424
Iteration 76/1000 | Loss: 0.00001424
Iteration 77/1000 | Loss: 0.00001423
Iteration 78/1000 | Loss: 0.00001423
Iteration 79/1000 | Loss: 0.00001422
Iteration 80/1000 | Loss: 0.00001422
Iteration 81/1000 | Loss: 0.00001422
Iteration 82/1000 | Loss: 0.00001422
Iteration 83/1000 | Loss: 0.00001421
Iteration 84/1000 | Loss: 0.00001421
Iteration 85/1000 | Loss: 0.00001421
Iteration 86/1000 | Loss: 0.00001421
Iteration 87/1000 | Loss: 0.00001421
Iteration 88/1000 | Loss: 0.00001421
Iteration 89/1000 | Loss: 0.00001421
Iteration 90/1000 | Loss: 0.00001421
Iteration 91/1000 | Loss: 0.00001421
Iteration 92/1000 | Loss: 0.00001421
Iteration 93/1000 | Loss: 0.00001420
Iteration 94/1000 | Loss: 0.00001420
Iteration 95/1000 | Loss: 0.00001420
Iteration 96/1000 | Loss: 0.00001420
Iteration 97/1000 | Loss: 0.00001420
Iteration 98/1000 | Loss: 0.00001420
Iteration 99/1000 | Loss: 0.00001419
Iteration 100/1000 | Loss: 0.00001419
Iteration 101/1000 | Loss: 0.00001419
Iteration 102/1000 | Loss: 0.00001419
Iteration 103/1000 | Loss: 0.00001418
Iteration 104/1000 | Loss: 0.00001418
Iteration 105/1000 | Loss: 0.00001418
Iteration 106/1000 | Loss: 0.00001417
Iteration 107/1000 | Loss: 0.00001417
Iteration 108/1000 | Loss: 0.00001417
Iteration 109/1000 | Loss: 0.00001416
Iteration 110/1000 | Loss: 0.00001416
Iteration 111/1000 | Loss: 0.00001416
Iteration 112/1000 | Loss: 0.00001416
Iteration 113/1000 | Loss: 0.00001415
Iteration 114/1000 | Loss: 0.00001415
Iteration 115/1000 | Loss: 0.00001415
Iteration 116/1000 | Loss: 0.00001415
Iteration 117/1000 | Loss: 0.00001414
Iteration 118/1000 | Loss: 0.00001414
Iteration 119/1000 | Loss: 0.00001414
Iteration 120/1000 | Loss: 0.00001414
Iteration 121/1000 | Loss: 0.00001414
Iteration 122/1000 | Loss: 0.00001413
Iteration 123/1000 | Loss: 0.00001413
Iteration 124/1000 | Loss: 0.00001413
Iteration 125/1000 | Loss: 0.00001413
Iteration 126/1000 | Loss: 0.00001413
Iteration 127/1000 | Loss: 0.00001413
Iteration 128/1000 | Loss: 0.00001413
Iteration 129/1000 | Loss: 0.00001413
Iteration 130/1000 | Loss: 0.00001413
Iteration 131/1000 | Loss: 0.00001412
Iteration 132/1000 | Loss: 0.00001412
Iteration 133/1000 | Loss: 0.00001412
Iteration 134/1000 | Loss: 0.00001412
Iteration 135/1000 | Loss: 0.00001412
Iteration 136/1000 | Loss: 0.00001412
Iteration 137/1000 | Loss: 0.00001412
Iteration 138/1000 | Loss: 0.00001412
Iteration 139/1000 | Loss: 0.00001412
Iteration 140/1000 | Loss: 0.00001412
Iteration 141/1000 | Loss: 0.00001412
Iteration 142/1000 | Loss: 0.00001412
Iteration 143/1000 | Loss: 0.00001412
Iteration 144/1000 | Loss: 0.00001411
Iteration 145/1000 | Loss: 0.00001411
Iteration 146/1000 | Loss: 0.00001411
Iteration 147/1000 | Loss: 0.00001411
Iteration 148/1000 | Loss: 0.00001411
Iteration 149/1000 | Loss: 0.00001411
Iteration 150/1000 | Loss: 0.00001411
Iteration 151/1000 | Loss: 0.00001411
Iteration 152/1000 | Loss: 0.00001411
Iteration 153/1000 | Loss: 0.00001411
Iteration 154/1000 | Loss: 0.00001411
Iteration 155/1000 | Loss: 0.00001411
Iteration 156/1000 | Loss: 0.00001411
Iteration 157/1000 | Loss: 0.00001411
Iteration 158/1000 | Loss: 0.00001411
Iteration 159/1000 | Loss: 0.00001411
Iteration 160/1000 | Loss: 0.00001411
Iteration 161/1000 | Loss: 0.00001411
Iteration 162/1000 | Loss: 0.00001411
Iteration 163/1000 | Loss: 0.00001411
Iteration 164/1000 | Loss: 0.00001411
Iteration 165/1000 | Loss: 0.00001411
Iteration 166/1000 | Loss: 0.00001410
Iteration 167/1000 | Loss: 0.00001410
Iteration 168/1000 | Loss: 0.00001410
Iteration 169/1000 | Loss: 0.00001410
Iteration 170/1000 | Loss: 0.00001410
Iteration 171/1000 | Loss: 0.00001410
Iteration 172/1000 | Loss: 0.00001410
Iteration 173/1000 | Loss: 0.00001410
Iteration 174/1000 | Loss: 0.00001410
Iteration 175/1000 | Loss: 0.00001410
Iteration 176/1000 | Loss: 0.00001410
Iteration 177/1000 | Loss: 0.00001410
Iteration 178/1000 | Loss: 0.00001410
Iteration 179/1000 | Loss: 0.00001410
Iteration 180/1000 | Loss: 0.00001410
Iteration 181/1000 | Loss: 0.00001410
Iteration 182/1000 | Loss: 0.00001410
Iteration 183/1000 | Loss: 0.00001410
Iteration 184/1000 | Loss: 0.00001410
Iteration 185/1000 | Loss: 0.00001410
Iteration 186/1000 | Loss: 0.00001410
Iteration 187/1000 | Loss: 0.00001409
Iteration 188/1000 | Loss: 0.00001409
Iteration 189/1000 | Loss: 0.00001409
Iteration 190/1000 | Loss: 0.00001409
Iteration 191/1000 | Loss: 0.00001409
Iteration 192/1000 | Loss: 0.00001409
Iteration 193/1000 | Loss: 0.00001409
Iteration 194/1000 | Loss: 0.00001409
Iteration 195/1000 | Loss: 0.00001409
Iteration 196/1000 | Loss: 0.00001409
Iteration 197/1000 | Loss: 0.00001409
Iteration 198/1000 | Loss: 0.00001409
Iteration 199/1000 | Loss: 0.00001409
Iteration 200/1000 | Loss: 0.00001409
Iteration 201/1000 | Loss: 0.00001409
Iteration 202/1000 | Loss: 0.00001409
Iteration 203/1000 | Loss: 0.00001409
Iteration 204/1000 | Loss: 0.00001409
Iteration 205/1000 | Loss: 0.00001409
Iteration 206/1000 | Loss: 0.00001409
Iteration 207/1000 | Loss: 0.00001409
Iteration 208/1000 | Loss: 0.00001409
Iteration 209/1000 | Loss: 0.00001409
Iteration 210/1000 | Loss: 0.00001409
Iteration 211/1000 | Loss: 0.00001409
Iteration 212/1000 | Loss: 0.00001409
Iteration 213/1000 | Loss: 0.00001409
Iteration 214/1000 | Loss: 0.00001408
Iteration 215/1000 | Loss: 0.00001408
Iteration 216/1000 | Loss: 0.00001408
Iteration 217/1000 | Loss: 0.00001408
Iteration 218/1000 | Loss: 0.00001408
Iteration 219/1000 | Loss: 0.00001408
Iteration 220/1000 | Loss: 0.00001408
Iteration 221/1000 | Loss: 0.00001408
Iteration 222/1000 | Loss: 0.00001408
Iteration 223/1000 | Loss: 0.00001408
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.4083507267059758e-05, 1.4083507267059758e-05, 1.4083507267059758e-05, 1.4083507267059758e-05, 1.4083507267059758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4083507267059758e-05

Optimization complete. Final v2v error: 3.1990740299224854 mm

Highest mean error: 3.861941337585449 mm for frame 67

Lowest mean error: 2.9391486644744873 mm for frame 172

Saving results

Total time: 45.982436180114746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887306
Iteration 2/25 | Loss: 0.00156012
Iteration 3/25 | Loss: 0.00139123
Iteration 4/25 | Loss: 0.00137531
Iteration 5/25 | Loss: 0.00137034
Iteration 6/25 | Loss: 0.00136950
Iteration 7/25 | Loss: 0.00136950
Iteration 8/25 | Loss: 0.00136950
Iteration 9/25 | Loss: 0.00136950
Iteration 10/25 | Loss: 0.00136950
Iteration 11/25 | Loss: 0.00136950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013695026282221079, 0.0013695026282221079, 0.0013695026282221079, 0.0013695026282221079, 0.0013695026282221079]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013695026282221079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74385607
Iteration 2/25 | Loss: 0.00080859
Iteration 3/25 | Loss: 0.00080859
Iteration 4/25 | Loss: 0.00080859
Iteration 5/25 | Loss: 0.00080859
Iteration 6/25 | Loss: 0.00080859
Iteration 7/25 | Loss: 0.00080859
Iteration 8/25 | Loss: 0.00080859
Iteration 9/25 | Loss: 0.00080859
Iteration 10/25 | Loss: 0.00080858
Iteration 11/25 | Loss: 0.00080858
Iteration 12/25 | Loss: 0.00080858
Iteration 13/25 | Loss: 0.00080858
Iteration 14/25 | Loss: 0.00080858
Iteration 15/25 | Loss: 0.00080858
Iteration 16/25 | Loss: 0.00080858
Iteration 17/25 | Loss: 0.00080858
Iteration 18/25 | Loss: 0.00080858
Iteration 19/25 | Loss: 0.00080858
Iteration 20/25 | Loss: 0.00080858
Iteration 21/25 | Loss: 0.00080858
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008085845620371401, 0.0008085845620371401, 0.0008085845620371401, 0.0008085845620371401, 0.0008085845620371401]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008085845620371401

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080858
Iteration 2/1000 | Loss: 0.00005645
Iteration 3/1000 | Loss: 0.00003614
Iteration 4/1000 | Loss: 0.00003140
Iteration 5/1000 | Loss: 0.00002948
Iteration 6/1000 | Loss: 0.00002817
Iteration 7/1000 | Loss: 0.00002719
Iteration 8/1000 | Loss: 0.00002623
Iteration 9/1000 | Loss: 0.00002569
Iteration 10/1000 | Loss: 0.00002521
Iteration 11/1000 | Loss: 0.00002471
Iteration 12/1000 | Loss: 0.00002429
Iteration 13/1000 | Loss: 0.00002381
Iteration 14/1000 | Loss: 0.00002332
Iteration 15/1000 | Loss: 0.00002304
Iteration 16/1000 | Loss: 0.00002281
Iteration 17/1000 | Loss: 0.00002255
Iteration 18/1000 | Loss: 0.00002238
Iteration 19/1000 | Loss: 0.00002223
Iteration 20/1000 | Loss: 0.00002221
Iteration 21/1000 | Loss: 0.00002217
Iteration 22/1000 | Loss: 0.00002217
Iteration 23/1000 | Loss: 0.00002216
Iteration 24/1000 | Loss: 0.00002216
Iteration 25/1000 | Loss: 0.00002215
Iteration 26/1000 | Loss: 0.00002214
Iteration 27/1000 | Loss: 0.00002213
Iteration 28/1000 | Loss: 0.00002213
Iteration 29/1000 | Loss: 0.00002213
Iteration 30/1000 | Loss: 0.00002212
Iteration 31/1000 | Loss: 0.00002212
Iteration 32/1000 | Loss: 0.00002212
Iteration 33/1000 | Loss: 0.00002212
Iteration 34/1000 | Loss: 0.00002212
Iteration 35/1000 | Loss: 0.00002212
Iteration 36/1000 | Loss: 0.00002212
Iteration 37/1000 | Loss: 0.00002212
Iteration 38/1000 | Loss: 0.00002212
Iteration 39/1000 | Loss: 0.00002212
Iteration 40/1000 | Loss: 0.00002211
Iteration 41/1000 | Loss: 0.00002211
Iteration 42/1000 | Loss: 0.00002210
Iteration 43/1000 | Loss: 0.00002210
Iteration 44/1000 | Loss: 0.00002210
Iteration 45/1000 | Loss: 0.00002210
Iteration 46/1000 | Loss: 0.00002210
Iteration 47/1000 | Loss: 0.00002210
Iteration 48/1000 | Loss: 0.00002210
Iteration 49/1000 | Loss: 0.00002210
Iteration 50/1000 | Loss: 0.00002210
Iteration 51/1000 | Loss: 0.00002210
Iteration 52/1000 | Loss: 0.00002209
Iteration 53/1000 | Loss: 0.00002209
Iteration 54/1000 | Loss: 0.00002208
Iteration 55/1000 | Loss: 0.00002208
Iteration 56/1000 | Loss: 0.00002207
Iteration 57/1000 | Loss: 0.00002207
Iteration 58/1000 | Loss: 0.00002206
Iteration 59/1000 | Loss: 0.00002206
Iteration 60/1000 | Loss: 0.00002205
Iteration 61/1000 | Loss: 0.00002205
Iteration 62/1000 | Loss: 0.00002205
Iteration 63/1000 | Loss: 0.00002205
Iteration 64/1000 | Loss: 0.00002200
Iteration 65/1000 | Loss: 0.00002193
Iteration 66/1000 | Loss: 0.00002193
Iteration 67/1000 | Loss: 0.00002191
Iteration 68/1000 | Loss: 0.00002190
Iteration 69/1000 | Loss: 0.00002190
Iteration 70/1000 | Loss: 0.00002189
Iteration 71/1000 | Loss: 0.00002189
Iteration 72/1000 | Loss: 0.00002189
Iteration 73/1000 | Loss: 0.00002189
Iteration 74/1000 | Loss: 0.00002189
Iteration 75/1000 | Loss: 0.00002189
Iteration 76/1000 | Loss: 0.00002188
Iteration 77/1000 | Loss: 0.00002187
Iteration 78/1000 | Loss: 0.00002186
Iteration 79/1000 | Loss: 0.00002184
Iteration 80/1000 | Loss: 0.00002184
Iteration 81/1000 | Loss: 0.00002184
Iteration 82/1000 | Loss: 0.00002184
Iteration 83/1000 | Loss: 0.00002183
Iteration 84/1000 | Loss: 0.00002181
Iteration 85/1000 | Loss: 0.00002179
Iteration 86/1000 | Loss: 0.00002179
Iteration 87/1000 | Loss: 0.00002179
Iteration 88/1000 | Loss: 0.00002179
Iteration 89/1000 | Loss: 0.00002179
Iteration 90/1000 | Loss: 0.00002178
Iteration 91/1000 | Loss: 0.00002178
Iteration 92/1000 | Loss: 0.00002178
Iteration 93/1000 | Loss: 0.00002178
Iteration 94/1000 | Loss: 0.00002178
Iteration 95/1000 | Loss: 0.00002178
Iteration 96/1000 | Loss: 0.00002178
Iteration 97/1000 | Loss: 0.00002178
Iteration 98/1000 | Loss: 0.00002178
Iteration 99/1000 | Loss: 0.00002178
Iteration 100/1000 | Loss: 0.00002178
Iteration 101/1000 | Loss: 0.00002177
Iteration 102/1000 | Loss: 0.00002177
Iteration 103/1000 | Loss: 0.00002177
Iteration 104/1000 | Loss: 0.00002177
Iteration 105/1000 | Loss: 0.00002177
Iteration 106/1000 | Loss: 0.00002177
Iteration 107/1000 | Loss: 0.00002177
Iteration 108/1000 | Loss: 0.00002177
Iteration 109/1000 | Loss: 0.00002177
Iteration 110/1000 | Loss: 0.00002177
Iteration 111/1000 | Loss: 0.00002177
Iteration 112/1000 | Loss: 0.00002177
Iteration 113/1000 | Loss: 0.00002177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.1774034394184127e-05, 2.1774034394184127e-05, 2.1774034394184127e-05, 2.1774034394184127e-05, 2.1774034394184127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1774034394184127e-05

Optimization complete. Final v2v error: 3.8395614624023438 mm

Highest mean error: 4.041985988616943 mm for frame 60

Lowest mean error: 3.6030917167663574 mm for frame 162

Saving results

Total time: 45.398181438446045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533067
Iteration 2/25 | Loss: 0.00152179
Iteration 3/25 | Loss: 0.00134414
Iteration 4/25 | Loss: 0.00132234
Iteration 5/25 | Loss: 0.00131727
Iteration 6/25 | Loss: 0.00131109
Iteration 7/25 | Loss: 0.00130839
Iteration 8/25 | Loss: 0.00131221
Iteration 9/25 | Loss: 0.00131346
Iteration 10/25 | Loss: 0.00130872
Iteration 11/25 | Loss: 0.00130602
Iteration 12/25 | Loss: 0.00130505
Iteration 13/25 | Loss: 0.00130492
Iteration 14/25 | Loss: 0.00130922
Iteration 15/25 | Loss: 0.00130738
Iteration 16/25 | Loss: 0.00130591
Iteration 17/25 | Loss: 0.00130557
Iteration 18/25 | Loss: 0.00130528
Iteration 19/25 | Loss: 0.00130523
Iteration 20/25 | Loss: 0.00130523
Iteration 21/25 | Loss: 0.00130522
Iteration 22/25 | Loss: 0.00130522
Iteration 23/25 | Loss: 0.00130522
Iteration 24/25 | Loss: 0.00130522
Iteration 25/25 | Loss: 0.00130522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70197177
Iteration 2/25 | Loss: 0.00092540
Iteration 3/25 | Loss: 0.00092540
Iteration 4/25 | Loss: 0.00092540
Iteration 5/25 | Loss: 0.00092540
Iteration 6/25 | Loss: 0.00092540
Iteration 7/25 | Loss: 0.00092540
Iteration 8/25 | Loss: 0.00092539
Iteration 9/25 | Loss: 0.00092539
Iteration 10/25 | Loss: 0.00092539
Iteration 11/25 | Loss: 0.00092539
Iteration 12/25 | Loss: 0.00092539
Iteration 13/25 | Loss: 0.00092539
Iteration 14/25 | Loss: 0.00092539
Iteration 15/25 | Loss: 0.00092539
Iteration 16/25 | Loss: 0.00092539
Iteration 17/25 | Loss: 0.00092539
Iteration 18/25 | Loss: 0.00092539
Iteration 19/25 | Loss: 0.00092539
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009253942407667637, 0.0009253942407667637, 0.0009253942407667637, 0.0009253942407667637, 0.0009253942407667637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009253942407667637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092539
Iteration 2/1000 | Loss: 0.00003384
Iteration 3/1000 | Loss: 0.00002580
Iteration 4/1000 | Loss: 0.00002218
Iteration 5/1000 | Loss: 0.00002105
Iteration 6/1000 | Loss: 0.00002016
Iteration 7/1000 | Loss: 0.00001956
Iteration 8/1000 | Loss: 0.00001910
Iteration 9/1000 | Loss: 0.00001868
Iteration 10/1000 | Loss: 0.00001829
Iteration 11/1000 | Loss: 0.00001807
Iteration 12/1000 | Loss: 0.00001807
Iteration 13/1000 | Loss: 0.00001792
Iteration 14/1000 | Loss: 0.00001784
Iteration 15/1000 | Loss: 0.00001784
Iteration 16/1000 | Loss: 0.00001783
Iteration 17/1000 | Loss: 0.00001782
Iteration 18/1000 | Loss: 0.00001779
Iteration 19/1000 | Loss: 0.00001778
Iteration 20/1000 | Loss: 0.00001777
Iteration 21/1000 | Loss: 0.00001772
Iteration 22/1000 | Loss: 0.00001771
Iteration 23/1000 | Loss: 0.00001770
Iteration 24/1000 | Loss: 0.00001769
Iteration 25/1000 | Loss: 0.00001769
Iteration 26/1000 | Loss: 0.00001769
Iteration 27/1000 | Loss: 0.00001768
Iteration 28/1000 | Loss: 0.00001768
Iteration 29/1000 | Loss: 0.00001767
Iteration 30/1000 | Loss: 0.00001767
Iteration 31/1000 | Loss: 0.00001766
Iteration 32/1000 | Loss: 0.00001765
Iteration 33/1000 | Loss: 0.00001763
Iteration 34/1000 | Loss: 0.00001761
Iteration 35/1000 | Loss: 0.00001760
Iteration 36/1000 | Loss: 0.00001760
Iteration 37/1000 | Loss: 0.00001759
Iteration 38/1000 | Loss: 0.00001757
Iteration 39/1000 | Loss: 0.00001757
Iteration 40/1000 | Loss: 0.00001757
Iteration 41/1000 | Loss: 0.00001757
Iteration 42/1000 | Loss: 0.00001757
Iteration 43/1000 | Loss: 0.00001757
Iteration 44/1000 | Loss: 0.00001757
Iteration 45/1000 | Loss: 0.00001756
Iteration 46/1000 | Loss: 0.00001756
Iteration 47/1000 | Loss: 0.00001756
Iteration 48/1000 | Loss: 0.00001755
Iteration 49/1000 | Loss: 0.00001755
Iteration 50/1000 | Loss: 0.00001754
Iteration 51/1000 | Loss: 0.00001754
Iteration 52/1000 | Loss: 0.00001754
Iteration 53/1000 | Loss: 0.00001753
Iteration 54/1000 | Loss: 0.00001753
Iteration 55/1000 | Loss: 0.00001752
Iteration 56/1000 | Loss: 0.00001752
Iteration 57/1000 | Loss: 0.00001752
Iteration 58/1000 | Loss: 0.00001752
Iteration 59/1000 | Loss: 0.00001752
Iteration 60/1000 | Loss: 0.00001751
Iteration 61/1000 | Loss: 0.00001751
Iteration 62/1000 | Loss: 0.00001751
Iteration 63/1000 | Loss: 0.00001750
Iteration 64/1000 | Loss: 0.00001750
Iteration 65/1000 | Loss: 0.00001750
Iteration 66/1000 | Loss: 0.00001749
Iteration 67/1000 | Loss: 0.00001749
Iteration 68/1000 | Loss: 0.00001749
Iteration 69/1000 | Loss: 0.00001748
Iteration 70/1000 | Loss: 0.00001748
Iteration 71/1000 | Loss: 0.00001747
Iteration 72/1000 | Loss: 0.00001747
Iteration 73/1000 | Loss: 0.00001747
Iteration 74/1000 | Loss: 0.00001746
Iteration 75/1000 | Loss: 0.00001746
Iteration 76/1000 | Loss: 0.00001746
Iteration 77/1000 | Loss: 0.00001745
Iteration 78/1000 | Loss: 0.00001745
Iteration 79/1000 | Loss: 0.00001745
Iteration 80/1000 | Loss: 0.00001745
Iteration 81/1000 | Loss: 0.00001745
Iteration 82/1000 | Loss: 0.00001745
Iteration 83/1000 | Loss: 0.00001744
Iteration 84/1000 | Loss: 0.00001744
Iteration 85/1000 | Loss: 0.00001744
Iteration 86/1000 | Loss: 0.00001744
Iteration 87/1000 | Loss: 0.00001744
Iteration 88/1000 | Loss: 0.00001743
Iteration 89/1000 | Loss: 0.00001743
Iteration 90/1000 | Loss: 0.00001743
Iteration 91/1000 | Loss: 0.00001742
Iteration 92/1000 | Loss: 0.00001980
Iteration 93/1000 | Loss: 0.00001755
Iteration 94/1000 | Loss: 0.00001744
Iteration 95/1000 | Loss: 0.00001741
Iteration 96/1000 | Loss: 0.00001721
Iteration 97/1000 | Loss: 0.00001718
Iteration 98/1000 | Loss: 0.00001718
Iteration 99/1000 | Loss: 0.00001717
Iteration 100/1000 | Loss: 0.00001717
Iteration 101/1000 | Loss: 0.00001716
Iteration 102/1000 | Loss: 0.00001716
Iteration 103/1000 | Loss: 0.00001715
Iteration 104/1000 | Loss: 0.00001715
Iteration 105/1000 | Loss: 0.00001714
Iteration 106/1000 | Loss: 0.00001713
Iteration 107/1000 | Loss: 0.00001713
Iteration 108/1000 | Loss: 0.00001712
Iteration 109/1000 | Loss: 0.00001711
Iteration 110/1000 | Loss: 0.00001711
Iteration 111/1000 | Loss: 0.00001711
Iteration 112/1000 | Loss: 0.00001710
Iteration 113/1000 | Loss: 0.00001710
Iteration 114/1000 | Loss: 0.00001710
Iteration 115/1000 | Loss: 0.00001710
Iteration 116/1000 | Loss: 0.00001710
Iteration 117/1000 | Loss: 0.00001709
Iteration 118/1000 | Loss: 0.00001709
Iteration 119/1000 | Loss: 0.00001709
Iteration 120/1000 | Loss: 0.00001709
Iteration 121/1000 | Loss: 0.00001708
Iteration 122/1000 | Loss: 0.00001708
Iteration 123/1000 | Loss: 0.00001708
Iteration 124/1000 | Loss: 0.00001707
Iteration 125/1000 | Loss: 0.00001707
Iteration 126/1000 | Loss: 0.00001707
Iteration 127/1000 | Loss: 0.00001707
Iteration 128/1000 | Loss: 0.00001707
Iteration 129/1000 | Loss: 0.00001707
Iteration 130/1000 | Loss: 0.00001707
Iteration 131/1000 | Loss: 0.00001706
Iteration 132/1000 | Loss: 0.00001706
Iteration 133/1000 | Loss: 0.00001706
Iteration 134/1000 | Loss: 0.00001706
Iteration 135/1000 | Loss: 0.00001706
Iteration 136/1000 | Loss: 0.00001705
Iteration 137/1000 | Loss: 0.00001705
Iteration 138/1000 | Loss: 0.00001705
Iteration 139/1000 | Loss: 0.00001705
Iteration 140/1000 | Loss: 0.00001704
Iteration 141/1000 | Loss: 0.00001704
Iteration 142/1000 | Loss: 0.00001703
Iteration 143/1000 | Loss: 0.00001702
Iteration 144/1000 | Loss: 0.00001701
Iteration 145/1000 | Loss: 0.00001700
Iteration 146/1000 | Loss: 0.00001700
Iteration 147/1000 | Loss: 0.00001700
Iteration 148/1000 | Loss: 0.00001700
Iteration 149/1000 | Loss: 0.00001699
Iteration 150/1000 | Loss: 0.00001699
Iteration 151/1000 | Loss: 0.00001699
Iteration 152/1000 | Loss: 0.00001699
Iteration 153/1000 | Loss: 0.00001698
Iteration 154/1000 | Loss: 0.00001698
Iteration 155/1000 | Loss: 0.00001698
Iteration 156/1000 | Loss: 0.00001698
Iteration 157/1000 | Loss: 0.00001698
Iteration 158/1000 | Loss: 0.00001697
Iteration 159/1000 | Loss: 0.00001697
Iteration 160/1000 | Loss: 0.00001697
Iteration 161/1000 | Loss: 0.00001697
Iteration 162/1000 | Loss: 0.00001697
Iteration 163/1000 | Loss: 0.00001697
Iteration 164/1000 | Loss: 0.00001697
Iteration 165/1000 | Loss: 0.00001696
Iteration 166/1000 | Loss: 0.00001696
Iteration 167/1000 | Loss: 0.00001696
Iteration 168/1000 | Loss: 0.00001696
Iteration 169/1000 | Loss: 0.00001695
Iteration 170/1000 | Loss: 0.00001695
Iteration 171/1000 | Loss: 0.00001695
Iteration 172/1000 | Loss: 0.00001695
Iteration 173/1000 | Loss: 0.00001695
Iteration 174/1000 | Loss: 0.00001694
Iteration 175/1000 | Loss: 0.00001694
Iteration 176/1000 | Loss: 0.00001694
Iteration 177/1000 | Loss: 0.00001694
Iteration 178/1000 | Loss: 0.00001693
Iteration 179/1000 | Loss: 0.00001693
Iteration 180/1000 | Loss: 0.00001693
Iteration 181/1000 | Loss: 0.00001693
Iteration 182/1000 | Loss: 0.00001693
Iteration 183/1000 | Loss: 0.00001693
Iteration 184/1000 | Loss: 0.00001693
Iteration 185/1000 | Loss: 0.00001692
Iteration 186/1000 | Loss: 0.00001692
Iteration 187/1000 | Loss: 0.00001692
Iteration 188/1000 | Loss: 0.00001692
Iteration 189/1000 | Loss: 0.00001692
Iteration 190/1000 | Loss: 0.00001692
Iteration 191/1000 | Loss: 0.00001691
Iteration 192/1000 | Loss: 0.00001691
Iteration 193/1000 | Loss: 0.00001691
Iteration 194/1000 | Loss: 0.00001691
Iteration 195/1000 | Loss: 0.00001691
Iteration 196/1000 | Loss: 0.00001691
Iteration 197/1000 | Loss: 0.00001691
Iteration 198/1000 | Loss: 0.00001691
Iteration 199/1000 | Loss: 0.00001691
Iteration 200/1000 | Loss: 0.00001691
Iteration 201/1000 | Loss: 0.00001691
Iteration 202/1000 | Loss: 0.00001691
Iteration 203/1000 | Loss: 0.00001691
Iteration 204/1000 | Loss: 0.00001691
Iteration 205/1000 | Loss: 0.00001691
Iteration 206/1000 | Loss: 0.00001691
Iteration 207/1000 | Loss: 0.00001691
Iteration 208/1000 | Loss: 0.00001690
Iteration 209/1000 | Loss: 0.00001690
Iteration 210/1000 | Loss: 0.00001690
Iteration 211/1000 | Loss: 0.00001690
Iteration 212/1000 | Loss: 0.00001690
Iteration 213/1000 | Loss: 0.00001690
Iteration 214/1000 | Loss: 0.00001690
Iteration 215/1000 | Loss: 0.00001690
Iteration 216/1000 | Loss: 0.00001690
Iteration 217/1000 | Loss: 0.00001690
Iteration 218/1000 | Loss: 0.00001690
Iteration 219/1000 | Loss: 0.00001690
Iteration 220/1000 | Loss: 0.00001690
Iteration 221/1000 | Loss: 0.00001690
Iteration 222/1000 | Loss: 0.00001690
Iteration 223/1000 | Loss: 0.00001690
Iteration 224/1000 | Loss: 0.00001690
Iteration 225/1000 | Loss: 0.00001690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.689577584329527e-05, 1.689577584329527e-05, 1.689577584329527e-05, 1.689577584329527e-05, 1.689577584329527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.689577584329527e-05

Optimization complete. Final v2v error: 3.458817720413208 mm

Highest mean error: 4.639525413513184 mm for frame 131

Lowest mean error: 3.018760919570923 mm for frame 22

Saving results

Total time: 81.00092625617981
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020521
Iteration 2/25 | Loss: 0.01020521
Iteration 3/25 | Loss: 0.01020520
Iteration 4/25 | Loss: 0.01020520
Iteration 5/25 | Loss: 0.01020520
Iteration 6/25 | Loss: 0.01020520
Iteration 7/25 | Loss: 0.01020520
Iteration 8/25 | Loss: 0.01020520
Iteration 9/25 | Loss: 0.01020520
Iteration 10/25 | Loss: 0.01020520
Iteration 11/25 | Loss: 0.01020520
Iteration 12/25 | Loss: 0.01020520
Iteration 13/25 | Loss: 0.01020519
Iteration 14/25 | Loss: 0.01020519
Iteration 15/25 | Loss: 0.01020519
Iteration 16/25 | Loss: 0.01020519
Iteration 17/25 | Loss: 0.01020519
Iteration 18/25 | Loss: 0.01020519
Iteration 19/25 | Loss: 0.01020519
Iteration 20/25 | Loss: 0.01020519
Iteration 21/25 | Loss: 0.01020519
Iteration 22/25 | Loss: 0.01020519
Iteration 23/25 | Loss: 0.01020519
Iteration 24/25 | Loss: 0.01020518
Iteration 25/25 | Loss: 0.01020518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38305640
Iteration 2/25 | Loss: 0.17743056
Iteration 3/25 | Loss: 0.17653385
Iteration 4/25 | Loss: 0.17644574
Iteration 5/25 | Loss: 0.17644574
Iteration 6/25 | Loss: 0.17644569
Iteration 7/25 | Loss: 0.17644569
Iteration 8/25 | Loss: 0.17644569
Iteration 9/25 | Loss: 0.17644571
Iteration 10/25 | Loss: 0.17644574
Iteration 11/25 | Loss: 0.17644569
Iteration 12/25 | Loss: 0.17644569
Iteration 13/25 | Loss: 0.17644569
Iteration 14/25 | Loss: 0.17644569
Iteration 15/25 | Loss: 0.17644569
Iteration 16/25 | Loss: 0.17644569
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.17644569277763367, 0.17644569277763367, 0.17644569277763367, 0.17644569277763367, 0.17644569277763367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17644569277763367

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17644569
Iteration 2/1000 | Loss: 0.00667533
Iteration 3/1000 | Loss: 0.00042422
Iteration 4/1000 | Loss: 0.00370875
Iteration 5/1000 | Loss: 0.00009166
Iteration 6/1000 | Loss: 0.00046902
Iteration 7/1000 | Loss: 0.00072956
Iteration 8/1000 | Loss: 0.00003846
Iteration 9/1000 | Loss: 0.00002987
Iteration 10/1000 | Loss: 0.00012139
Iteration 11/1000 | Loss: 0.00002308
Iteration 12/1000 | Loss: 0.00011837
Iteration 13/1000 | Loss: 0.00001904
Iteration 14/1000 | Loss: 0.00025685
Iteration 15/1000 | Loss: 0.00054945
Iteration 16/1000 | Loss: 0.00307003
Iteration 17/1000 | Loss: 0.00335636
Iteration 18/1000 | Loss: 0.00343183
Iteration 19/1000 | Loss: 0.00070833
Iteration 20/1000 | Loss: 0.00125125
Iteration 21/1000 | Loss: 0.00050221
Iteration 22/1000 | Loss: 0.00024020
Iteration 23/1000 | Loss: 0.00036185
Iteration 24/1000 | Loss: 0.00065209
Iteration 25/1000 | Loss: 0.00007048
Iteration 26/1000 | Loss: 0.00013148
Iteration 27/1000 | Loss: 0.00047290
Iteration 28/1000 | Loss: 0.00016343
Iteration 29/1000 | Loss: 0.00002925
Iteration 30/1000 | Loss: 0.00002556
Iteration 31/1000 | Loss: 0.00083092
Iteration 32/1000 | Loss: 0.00077289
Iteration 33/1000 | Loss: 0.00017453
Iteration 34/1000 | Loss: 0.00028739
Iteration 35/1000 | Loss: 0.00053364
Iteration 36/1000 | Loss: 0.00025477
Iteration 37/1000 | Loss: 0.00003657
Iteration 38/1000 | Loss: 0.00019418
Iteration 39/1000 | Loss: 0.00031044
Iteration 40/1000 | Loss: 0.00049901
Iteration 41/1000 | Loss: 0.00162274
Iteration 42/1000 | Loss: 0.00084264
Iteration 43/1000 | Loss: 0.00150272
Iteration 44/1000 | Loss: 0.00101970
Iteration 45/1000 | Loss: 0.00065878
Iteration 46/1000 | Loss: 0.00135824
Iteration 47/1000 | Loss: 0.00173287
Iteration 48/1000 | Loss: 0.00191899
Iteration 49/1000 | Loss: 0.00144945
Iteration 50/1000 | Loss: 0.00134670
Iteration 51/1000 | Loss: 0.00033786
Iteration 52/1000 | Loss: 0.00011914
Iteration 53/1000 | Loss: 0.00008051
Iteration 54/1000 | Loss: 0.00014301
Iteration 55/1000 | Loss: 0.00005457
Iteration 56/1000 | Loss: 0.00004546
Iteration 57/1000 | Loss: 0.00029389
Iteration 58/1000 | Loss: 0.00095419
Iteration 59/1000 | Loss: 0.00014781
Iteration 60/1000 | Loss: 0.00002428
Iteration 61/1000 | Loss: 0.00002095
Iteration 62/1000 | Loss: 0.00012741
Iteration 63/1000 | Loss: 0.00011348
Iteration 64/1000 | Loss: 0.00002847
Iteration 65/1000 | Loss: 0.00003851
Iteration 66/1000 | Loss: 0.00012182
Iteration 67/1000 | Loss: 0.00021002
Iteration 68/1000 | Loss: 0.00007468
Iteration 69/1000 | Loss: 0.00041203
Iteration 70/1000 | Loss: 0.00011978
Iteration 71/1000 | Loss: 0.00020291
Iteration 72/1000 | Loss: 0.00007190
Iteration 73/1000 | Loss: 0.00002033
Iteration 74/1000 | Loss: 0.00001847
Iteration 75/1000 | Loss: 0.00001745
Iteration 76/1000 | Loss: 0.00001658
Iteration 77/1000 | Loss: 0.00001975
Iteration 78/1000 | Loss: 0.00001581
Iteration 79/1000 | Loss: 0.00001530
Iteration 80/1000 | Loss: 0.00001486
Iteration 81/1000 | Loss: 0.00013534
Iteration 82/1000 | Loss: 0.00013901
Iteration 83/1000 | Loss: 0.00018622
Iteration 84/1000 | Loss: 0.00001553
Iteration 85/1000 | Loss: 0.00001437
Iteration 86/1000 | Loss: 0.00001417
Iteration 87/1000 | Loss: 0.00001415
Iteration 88/1000 | Loss: 0.00001412
Iteration 89/1000 | Loss: 0.00001412
Iteration 90/1000 | Loss: 0.00001402
Iteration 91/1000 | Loss: 0.00001389
Iteration 92/1000 | Loss: 0.00001380
Iteration 93/1000 | Loss: 0.00001375
Iteration 94/1000 | Loss: 0.00001374
Iteration 95/1000 | Loss: 0.00001374
Iteration 96/1000 | Loss: 0.00001373
Iteration 97/1000 | Loss: 0.00001372
Iteration 98/1000 | Loss: 0.00001372
Iteration 99/1000 | Loss: 0.00001371
Iteration 100/1000 | Loss: 0.00001369
Iteration 101/1000 | Loss: 0.00001368
Iteration 102/1000 | Loss: 0.00001356
Iteration 103/1000 | Loss: 0.00001354
Iteration 104/1000 | Loss: 0.00001350
Iteration 105/1000 | Loss: 0.00001348
Iteration 106/1000 | Loss: 0.00001347
Iteration 107/1000 | Loss: 0.00001347
Iteration 108/1000 | Loss: 0.00001347
Iteration 109/1000 | Loss: 0.00001347
Iteration 110/1000 | Loss: 0.00001345
Iteration 111/1000 | Loss: 0.00001345
Iteration 112/1000 | Loss: 0.00001344
Iteration 113/1000 | Loss: 0.00001343
Iteration 114/1000 | Loss: 0.00001342
Iteration 115/1000 | Loss: 0.00001342
Iteration 116/1000 | Loss: 0.00001341
Iteration 117/1000 | Loss: 0.00001341
Iteration 118/1000 | Loss: 0.00001341
Iteration 119/1000 | Loss: 0.00001340
Iteration 120/1000 | Loss: 0.00001339
Iteration 121/1000 | Loss: 0.00001339
Iteration 122/1000 | Loss: 0.00001338
Iteration 123/1000 | Loss: 0.00001337
Iteration 124/1000 | Loss: 0.00001337
Iteration 125/1000 | Loss: 0.00001336
Iteration 126/1000 | Loss: 0.00001336
Iteration 127/1000 | Loss: 0.00001335
Iteration 128/1000 | Loss: 0.00001335
Iteration 129/1000 | Loss: 0.00001334
Iteration 130/1000 | Loss: 0.00001334
Iteration 131/1000 | Loss: 0.00001333
Iteration 132/1000 | Loss: 0.00001332
Iteration 133/1000 | Loss: 0.00001332
Iteration 134/1000 | Loss: 0.00001331
Iteration 135/1000 | Loss: 0.00001331
Iteration 136/1000 | Loss: 0.00001330
Iteration 137/1000 | Loss: 0.00001329
Iteration 138/1000 | Loss: 0.00001328
Iteration 139/1000 | Loss: 0.00001328
Iteration 140/1000 | Loss: 0.00001328
Iteration 141/1000 | Loss: 0.00001328
Iteration 142/1000 | Loss: 0.00001328
Iteration 143/1000 | Loss: 0.00001328
Iteration 144/1000 | Loss: 0.00001328
Iteration 145/1000 | Loss: 0.00001328
Iteration 146/1000 | Loss: 0.00001327
Iteration 147/1000 | Loss: 0.00019399
Iteration 148/1000 | Loss: 0.00001752
Iteration 149/1000 | Loss: 0.00001418
Iteration 150/1000 | Loss: 0.00001331
Iteration 151/1000 | Loss: 0.00001325
Iteration 152/1000 | Loss: 0.00001321
Iteration 153/1000 | Loss: 0.00001320
Iteration 154/1000 | Loss: 0.00001320
Iteration 155/1000 | Loss: 0.00001319
Iteration 156/1000 | Loss: 0.00001319
Iteration 157/1000 | Loss: 0.00001319
Iteration 158/1000 | Loss: 0.00001318
Iteration 159/1000 | Loss: 0.00001316
Iteration 160/1000 | Loss: 0.00001316
Iteration 161/1000 | Loss: 0.00001316
Iteration 162/1000 | Loss: 0.00001316
Iteration 163/1000 | Loss: 0.00001316
Iteration 164/1000 | Loss: 0.00001316
Iteration 165/1000 | Loss: 0.00001316
Iteration 166/1000 | Loss: 0.00001316
Iteration 167/1000 | Loss: 0.00001316
Iteration 168/1000 | Loss: 0.00001316
Iteration 169/1000 | Loss: 0.00001315
Iteration 170/1000 | Loss: 0.00001315
Iteration 171/1000 | Loss: 0.00001314
Iteration 172/1000 | Loss: 0.00001314
Iteration 173/1000 | Loss: 0.00001314
Iteration 174/1000 | Loss: 0.00001314
Iteration 175/1000 | Loss: 0.00001314
Iteration 176/1000 | Loss: 0.00001314
Iteration 177/1000 | Loss: 0.00001314
Iteration 178/1000 | Loss: 0.00001314
Iteration 179/1000 | Loss: 0.00001314
Iteration 180/1000 | Loss: 0.00001314
Iteration 181/1000 | Loss: 0.00001314
Iteration 182/1000 | Loss: 0.00001314
Iteration 183/1000 | Loss: 0.00001314
Iteration 184/1000 | Loss: 0.00001314
Iteration 185/1000 | Loss: 0.00001314
Iteration 186/1000 | Loss: 0.00001314
Iteration 187/1000 | Loss: 0.00001314
Iteration 188/1000 | Loss: 0.00001314
Iteration 189/1000 | Loss: 0.00001313
Iteration 190/1000 | Loss: 0.00001313
Iteration 191/1000 | Loss: 0.00001313
Iteration 192/1000 | Loss: 0.00001313
Iteration 193/1000 | Loss: 0.00001313
Iteration 194/1000 | Loss: 0.00001313
Iteration 195/1000 | Loss: 0.00001313
Iteration 196/1000 | Loss: 0.00001313
Iteration 197/1000 | Loss: 0.00001313
Iteration 198/1000 | Loss: 0.00001313
Iteration 199/1000 | Loss: 0.00001313
Iteration 200/1000 | Loss: 0.00001312
Iteration 201/1000 | Loss: 0.00001312
Iteration 202/1000 | Loss: 0.00001312
Iteration 203/1000 | Loss: 0.00001312
Iteration 204/1000 | Loss: 0.00001312
Iteration 205/1000 | Loss: 0.00001312
Iteration 206/1000 | Loss: 0.00001312
Iteration 207/1000 | Loss: 0.00001312
Iteration 208/1000 | Loss: 0.00001312
Iteration 209/1000 | Loss: 0.00001312
Iteration 210/1000 | Loss: 0.00001312
Iteration 211/1000 | Loss: 0.00001312
Iteration 212/1000 | Loss: 0.00001312
Iteration 213/1000 | Loss: 0.00001312
Iteration 214/1000 | Loss: 0.00001312
Iteration 215/1000 | Loss: 0.00001312
Iteration 216/1000 | Loss: 0.00001312
Iteration 217/1000 | Loss: 0.00001312
Iteration 218/1000 | Loss: 0.00001312
Iteration 219/1000 | Loss: 0.00001312
Iteration 220/1000 | Loss: 0.00001312
Iteration 221/1000 | Loss: 0.00001312
Iteration 222/1000 | Loss: 0.00001312
Iteration 223/1000 | Loss: 0.00001312
Iteration 224/1000 | Loss: 0.00001312
Iteration 225/1000 | Loss: 0.00001312
Iteration 226/1000 | Loss: 0.00001312
Iteration 227/1000 | Loss: 0.00001312
Iteration 228/1000 | Loss: 0.00001312
Iteration 229/1000 | Loss: 0.00001312
Iteration 230/1000 | Loss: 0.00001312
Iteration 231/1000 | Loss: 0.00001312
Iteration 232/1000 | Loss: 0.00001312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.3122536074661184e-05, 1.3122536074661184e-05, 1.3122536074661184e-05, 1.3122536074661184e-05, 1.3122536074661184e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3122536074661184e-05

Optimization complete. Final v2v error: 3.042670249938965 mm

Highest mean error: 4.534850597381592 mm for frame 74

Lowest mean error: 2.9326562881469727 mm for frame 183

Saving results

Total time: 166.802392244339
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00741089
Iteration 2/25 | Loss: 0.00202857
Iteration 3/25 | Loss: 0.00153453
Iteration 4/25 | Loss: 0.00145954
Iteration 5/25 | Loss: 0.00145290
Iteration 6/25 | Loss: 0.00145265
Iteration 7/25 | Loss: 0.00145265
Iteration 8/25 | Loss: 0.00145265
Iteration 9/25 | Loss: 0.00145265
Iteration 10/25 | Loss: 0.00145265
Iteration 11/25 | Loss: 0.00145265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001452651689760387, 0.001452651689760387, 0.001452651689760387, 0.001452651689760387, 0.001452651689760387]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001452651689760387

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41973162
Iteration 2/25 | Loss: 0.00077566
Iteration 3/25 | Loss: 0.00077564
Iteration 4/25 | Loss: 0.00077564
Iteration 5/25 | Loss: 0.00077564
Iteration 6/25 | Loss: 0.00077564
Iteration 7/25 | Loss: 0.00077564
Iteration 8/25 | Loss: 0.00077564
Iteration 9/25 | Loss: 0.00077564
Iteration 10/25 | Loss: 0.00077564
Iteration 11/25 | Loss: 0.00077564
Iteration 12/25 | Loss: 0.00077564
Iteration 13/25 | Loss: 0.00077564
Iteration 14/25 | Loss: 0.00077564
Iteration 15/25 | Loss: 0.00077564
Iteration 16/25 | Loss: 0.00077564
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000775636057369411, 0.000775636057369411, 0.000775636057369411, 0.000775636057369411, 0.000775636057369411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000775636057369411

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077564
Iteration 2/1000 | Loss: 0.00005055
Iteration 3/1000 | Loss: 0.00003723
Iteration 4/1000 | Loss: 0.00003444
Iteration 5/1000 | Loss: 0.00003279
Iteration 6/1000 | Loss: 0.00003177
Iteration 7/1000 | Loss: 0.00003132
Iteration 8/1000 | Loss: 0.00003101
Iteration 9/1000 | Loss: 0.00003059
Iteration 10/1000 | Loss: 0.00003035
Iteration 11/1000 | Loss: 0.00003029
Iteration 12/1000 | Loss: 0.00003021
Iteration 13/1000 | Loss: 0.00003010
Iteration 14/1000 | Loss: 0.00002993
Iteration 15/1000 | Loss: 0.00002980
Iteration 16/1000 | Loss: 0.00002978
Iteration 17/1000 | Loss: 0.00002977
Iteration 18/1000 | Loss: 0.00002977
Iteration 19/1000 | Loss: 0.00002976
Iteration 20/1000 | Loss: 0.00002976
Iteration 21/1000 | Loss: 0.00002976
Iteration 22/1000 | Loss: 0.00002974
Iteration 23/1000 | Loss: 0.00002973
Iteration 24/1000 | Loss: 0.00002972
Iteration 25/1000 | Loss: 0.00002971
Iteration 26/1000 | Loss: 0.00002971
Iteration 27/1000 | Loss: 0.00002971
Iteration 28/1000 | Loss: 0.00002971
Iteration 29/1000 | Loss: 0.00002971
Iteration 30/1000 | Loss: 0.00002971
Iteration 31/1000 | Loss: 0.00002971
Iteration 32/1000 | Loss: 0.00002971
Iteration 33/1000 | Loss: 0.00002970
Iteration 34/1000 | Loss: 0.00002970
Iteration 35/1000 | Loss: 0.00002968
Iteration 36/1000 | Loss: 0.00002968
Iteration 37/1000 | Loss: 0.00002968
Iteration 38/1000 | Loss: 0.00002968
Iteration 39/1000 | Loss: 0.00002968
Iteration 40/1000 | Loss: 0.00002968
Iteration 41/1000 | Loss: 0.00002967
Iteration 42/1000 | Loss: 0.00002967
Iteration 43/1000 | Loss: 0.00002967
Iteration 44/1000 | Loss: 0.00002967
Iteration 45/1000 | Loss: 0.00002967
Iteration 46/1000 | Loss: 0.00002967
Iteration 47/1000 | Loss: 0.00002966
Iteration 48/1000 | Loss: 0.00002966
Iteration 49/1000 | Loss: 0.00002966
Iteration 50/1000 | Loss: 0.00002965
Iteration 51/1000 | Loss: 0.00002965
Iteration 52/1000 | Loss: 0.00002965
Iteration 53/1000 | Loss: 0.00002965
Iteration 54/1000 | Loss: 0.00002965
Iteration 55/1000 | Loss: 0.00002965
Iteration 56/1000 | Loss: 0.00002965
Iteration 57/1000 | Loss: 0.00002965
Iteration 58/1000 | Loss: 0.00002964
Iteration 59/1000 | Loss: 0.00002964
Iteration 60/1000 | Loss: 0.00002964
Iteration 61/1000 | Loss: 0.00002964
Iteration 62/1000 | Loss: 0.00002964
Iteration 63/1000 | Loss: 0.00002964
Iteration 64/1000 | Loss: 0.00002964
Iteration 65/1000 | Loss: 0.00002964
Iteration 66/1000 | Loss: 0.00002964
Iteration 67/1000 | Loss: 0.00002964
Iteration 68/1000 | Loss: 0.00002964
Iteration 69/1000 | Loss: 0.00002964
Iteration 70/1000 | Loss: 0.00002964
Iteration 71/1000 | Loss: 0.00002964
Iteration 72/1000 | Loss: 0.00002964
Iteration 73/1000 | Loss: 0.00002964
Iteration 74/1000 | Loss: 0.00002963
Iteration 75/1000 | Loss: 0.00002963
Iteration 76/1000 | Loss: 0.00002963
Iteration 77/1000 | Loss: 0.00002963
Iteration 78/1000 | Loss: 0.00002962
Iteration 79/1000 | Loss: 0.00002962
Iteration 80/1000 | Loss: 0.00002962
Iteration 81/1000 | Loss: 0.00002962
Iteration 82/1000 | Loss: 0.00002962
Iteration 83/1000 | Loss: 0.00002962
Iteration 84/1000 | Loss: 0.00002962
Iteration 85/1000 | Loss: 0.00002962
Iteration 86/1000 | Loss: 0.00002962
Iteration 87/1000 | Loss: 0.00002962
Iteration 88/1000 | Loss: 0.00002962
Iteration 89/1000 | Loss: 0.00002962
Iteration 90/1000 | Loss: 0.00002962
Iteration 91/1000 | Loss: 0.00002962
Iteration 92/1000 | Loss: 0.00002961
Iteration 93/1000 | Loss: 0.00002961
Iteration 94/1000 | Loss: 0.00002961
Iteration 95/1000 | Loss: 0.00002961
Iteration 96/1000 | Loss: 0.00002961
Iteration 97/1000 | Loss: 0.00002961
Iteration 98/1000 | Loss: 0.00002961
Iteration 99/1000 | Loss: 0.00002961
Iteration 100/1000 | Loss: 0.00002961
Iteration 101/1000 | Loss: 0.00002961
Iteration 102/1000 | Loss: 0.00002961
Iteration 103/1000 | Loss: 0.00002961
Iteration 104/1000 | Loss: 0.00002961
Iteration 105/1000 | Loss: 0.00002961
Iteration 106/1000 | Loss: 0.00002960
Iteration 107/1000 | Loss: 0.00002960
Iteration 108/1000 | Loss: 0.00002960
Iteration 109/1000 | Loss: 0.00002960
Iteration 110/1000 | Loss: 0.00002960
Iteration 111/1000 | Loss: 0.00002960
Iteration 112/1000 | Loss: 0.00002960
Iteration 113/1000 | Loss: 0.00002960
Iteration 114/1000 | Loss: 0.00002960
Iteration 115/1000 | Loss: 0.00002959
Iteration 116/1000 | Loss: 0.00002959
Iteration 117/1000 | Loss: 0.00002959
Iteration 118/1000 | Loss: 0.00002959
Iteration 119/1000 | Loss: 0.00002959
Iteration 120/1000 | Loss: 0.00002959
Iteration 121/1000 | Loss: 0.00002959
Iteration 122/1000 | Loss: 0.00002959
Iteration 123/1000 | Loss: 0.00002959
Iteration 124/1000 | Loss: 0.00002959
Iteration 125/1000 | Loss: 0.00002959
Iteration 126/1000 | Loss: 0.00002959
Iteration 127/1000 | Loss: 0.00002959
Iteration 128/1000 | Loss: 0.00002959
Iteration 129/1000 | Loss: 0.00002959
Iteration 130/1000 | Loss: 0.00002959
Iteration 131/1000 | Loss: 0.00002958
Iteration 132/1000 | Loss: 0.00002958
Iteration 133/1000 | Loss: 0.00002958
Iteration 134/1000 | Loss: 0.00002958
Iteration 135/1000 | Loss: 0.00002958
Iteration 136/1000 | Loss: 0.00002958
Iteration 137/1000 | Loss: 0.00002958
Iteration 138/1000 | Loss: 0.00002958
Iteration 139/1000 | Loss: 0.00002958
Iteration 140/1000 | Loss: 0.00002958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.9584909498225898e-05, 2.9584909498225898e-05, 2.9584909498225898e-05, 2.9584909498225898e-05, 2.9584909498225898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9584909498225898e-05

Optimization complete. Final v2v error: 4.579840183258057 mm

Highest mean error: 4.903240203857422 mm for frame 9

Lowest mean error: 4.33142614364624 mm for frame 161

Saving results

Total time: 39.43334126472473
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979469
Iteration 2/25 | Loss: 0.00247516
Iteration 3/25 | Loss: 0.00222062
Iteration 4/25 | Loss: 0.00186865
Iteration 5/25 | Loss: 0.00221696
Iteration 6/25 | Loss: 0.00161356
Iteration 7/25 | Loss: 0.00142903
Iteration 8/25 | Loss: 0.00139745
Iteration 9/25 | Loss: 0.00139189
Iteration 10/25 | Loss: 0.00137929
Iteration 11/25 | Loss: 0.00137055
Iteration 12/25 | Loss: 0.00136884
Iteration 13/25 | Loss: 0.00136826
Iteration 14/25 | Loss: 0.00136757
Iteration 15/25 | Loss: 0.00136645
Iteration 16/25 | Loss: 0.00136809
Iteration 17/25 | Loss: 0.00136160
Iteration 18/25 | Loss: 0.00136043
Iteration 19/25 | Loss: 0.00135972
Iteration 20/25 | Loss: 0.00137269
Iteration 21/25 | Loss: 0.00135693
Iteration 22/25 | Loss: 0.00135466
Iteration 23/25 | Loss: 0.00135435
Iteration 24/25 | Loss: 0.00135432
Iteration 25/25 | Loss: 0.00135432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41431689
Iteration 2/25 | Loss: 0.00102188
Iteration 3/25 | Loss: 0.00102188
Iteration 4/25 | Loss: 0.00102188
Iteration 5/25 | Loss: 0.00102188
Iteration 6/25 | Loss: 0.00102188
Iteration 7/25 | Loss: 0.00102188
Iteration 8/25 | Loss: 0.00102188
Iteration 9/25 | Loss: 0.00102188
Iteration 10/25 | Loss: 0.00102188
Iteration 11/25 | Loss: 0.00102188
Iteration 12/25 | Loss: 0.00102188
Iteration 13/25 | Loss: 0.00102188
Iteration 14/25 | Loss: 0.00102188
Iteration 15/25 | Loss: 0.00102188
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010218753013759851, 0.0010218753013759851, 0.0010218753013759851, 0.0010218753013759851, 0.0010218753013759851]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010218753013759851

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102188
Iteration 2/1000 | Loss: 0.00011184
Iteration 3/1000 | Loss: 0.00015623
Iteration 4/1000 | Loss: 0.00007937
Iteration 5/1000 | Loss: 0.00005987
Iteration 6/1000 | Loss: 0.00005399
Iteration 7/1000 | Loss: 0.00004878
Iteration 8/1000 | Loss: 0.00004485
Iteration 9/1000 | Loss: 0.00004183
Iteration 10/1000 | Loss: 0.00004033
Iteration 11/1000 | Loss: 0.00003874
Iteration 12/1000 | Loss: 0.00003774
Iteration 13/1000 | Loss: 0.00166637
Iteration 14/1000 | Loss: 0.00005430
Iteration 15/1000 | Loss: 0.00004076
Iteration 16/1000 | Loss: 0.00003364
Iteration 17/1000 | Loss: 0.00002703
Iteration 18/1000 | Loss: 0.00002360
Iteration 19/1000 | Loss: 0.00002234
Iteration 20/1000 | Loss: 0.00002136
Iteration 21/1000 | Loss: 0.00002058
Iteration 22/1000 | Loss: 0.00002015
Iteration 23/1000 | Loss: 0.00001985
Iteration 24/1000 | Loss: 0.00001975
Iteration 25/1000 | Loss: 0.00001950
Iteration 26/1000 | Loss: 0.00001943
Iteration 27/1000 | Loss: 0.00001941
Iteration 28/1000 | Loss: 0.00001930
Iteration 29/1000 | Loss: 0.00001928
Iteration 30/1000 | Loss: 0.00001927
Iteration 31/1000 | Loss: 0.00001919
Iteration 32/1000 | Loss: 0.00001917
Iteration 33/1000 | Loss: 0.00001915
Iteration 34/1000 | Loss: 0.00001914
Iteration 35/1000 | Loss: 0.00001913
Iteration 36/1000 | Loss: 0.00001913
Iteration 37/1000 | Loss: 0.00001912
Iteration 38/1000 | Loss: 0.00001912
Iteration 39/1000 | Loss: 0.00001912
Iteration 40/1000 | Loss: 0.00001912
Iteration 41/1000 | Loss: 0.00001912
Iteration 42/1000 | Loss: 0.00001912
Iteration 43/1000 | Loss: 0.00001912
Iteration 44/1000 | Loss: 0.00001911
Iteration 45/1000 | Loss: 0.00001911
Iteration 46/1000 | Loss: 0.00001910
Iteration 47/1000 | Loss: 0.00001910
Iteration 48/1000 | Loss: 0.00001909
Iteration 49/1000 | Loss: 0.00001909
Iteration 50/1000 | Loss: 0.00001908
Iteration 51/1000 | Loss: 0.00001908
Iteration 52/1000 | Loss: 0.00001908
Iteration 53/1000 | Loss: 0.00001908
Iteration 54/1000 | Loss: 0.00001908
Iteration 55/1000 | Loss: 0.00001908
Iteration 56/1000 | Loss: 0.00001907
Iteration 57/1000 | Loss: 0.00001907
Iteration 58/1000 | Loss: 0.00001907
Iteration 59/1000 | Loss: 0.00001907
Iteration 60/1000 | Loss: 0.00001906
Iteration 61/1000 | Loss: 0.00001906
Iteration 62/1000 | Loss: 0.00001906
Iteration 63/1000 | Loss: 0.00001906
Iteration 64/1000 | Loss: 0.00001906
Iteration 65/1000 | Loss: 0.00001906
Iteration 66/1000 | Loss: 0.00001906
Iteration 67/1000 | Loss: 0.00001906
Iteration 68/1000 | Loss: 0.00001906
Iteration 69/1000 | Loss: 0.00001906
Iteration 70/1000 | Loss: 0.00001906
Iteration 71/1000 | Loss: 0.00001906
Iteration 72/1000 | Loss: 0.00001906
Iteration 73/1000 | Loss: 0.00001906
Iteration 74/1000 | Loss: 0.00001906
Iteration 75/1000 | Loss: 0.00001906
Iteration 76/1000 | Loss: 0.00001906
Iteration 77/1000 | Loss: 0.00001906
Iteration 78/1000 | Loss: 0.00001906
Iteration 79/1000 | Loss: 0.00001906
Iteration 80/1000 | Loss: 0.00001906
Iteration 81/1000 | Loss: 0.00001906
Iteration 82/1000 | Loss: 0.00001906
Iteration 83/1000 | Loss: 0.00001906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.905822500702925e-05, 1.905822500702925e-05, 1.905822500702925e-05, 1.905822500702925e-05, 1.905822500702925e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.905822500702925e-05

Optimization complete. Final v2v error: 3.7028777599334717 mm

Highest mean error: 3.8911678791046143 mm for frame 6

Lowest mean error: 3.623347520828247 mm for frame 8

Saving results

Total time: 80.31453061103821
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957792
Iteration 2/25 | Loss: 0.00957792
Iteration 3/25 | Loss: 0.00957792
Iteration 4/25 | Loss: 0.00957792
Iteration 5/25 | Loss: 0.00957792
Iteration 6/25 | Loss: 0.00957792
Iteration 7/25 | Loss: 0.00957792
Iteration 8/25 | Loss: 0.00957792
Iteration 9/25 | Loss: 0.00957791
Iteration 10/25 | Loss: 0.00957791
Iteration 11/25 | Loss: 0.00957791
Iteration 12/25 | Loss: 0.00957791
Iteration 13/25 | Loss: 0.00957791
Iteration 14/25 | Loss: 0.00957791
Iteration 15/25 | Loss: 0.00957791
Iteration 16/25 | Loss: 0.00957791
Iteration 17/25 | Loss: 0.00957791
Iteration 18/25 | Loss: 0.00957791
Iteration 19/25 | Loss: 0.00957790
Iteration 20/25 | Loss: 0.00957790
Iteration 21/25 | Loss: 0.00957790
Iteration 22/25 | Loss: 0.00957790
Iteration 23/25 | Loss: 0.00957790
Iteration 24/25 | Loss: 0.00957790
Iteration 25/25 | Loss: 0.00957790

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74377108
Iteration 2/25 | Loss: 0.15183231
Iteration 3/25 | Loss: 0.14956406
Iteration 4/25 | Loss: 0.14818752
Iteration 5/25 | Loss: 0.14824314
Iteration 6/25 | Loss: 0.14818662
Iteration 7/25 | Loss: 0.14818661
Iteration 8/25 | Loss: 0.14818661
Iteration 9/25 | Loss: 0.14818661
Iteration 10/25 | Loss: 0.14818661
Iteration 11/25 | Loss: 0.14818659
Iteration 12/25 | Loss: 0.14818658
Iteration 13/25 | Loss: 0.14818658
Iteration 14/25 | Loss: 0.14818658
Iteration 15/25 | Loss: 0.14818658
Iteration 16/25 | Loss: 0.14818658
Iteration 17/25 | Loss: 0.14818658
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.1481865793466568, 0.1481865793466568, 0.1481865793466568, 0.1481865793466568, 0.1481865793466568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1481865793466568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.14818658
Iteration 2/1000 | Loss: 0.00958365
Iteration 3/1000 | Loss: 0.00180060
Iteration 4/1000 | Loss: 0.00071097
Iteration 5/1000 | Loss: 0.00068152
Iteration 6/1000 | Loss: 0.00028771
Iteration 7/1000 | Loss: 0.00160594
Iteration 8/1000 | Loss: 0.00043496
Iteration 9/1000 | Loss: 0.00049483
Iteration 10/1000 | Loss: 0.00016923
Iteration 11/1000 | Loss: 0.00036766
Iteration 12/1000 | Loss: 0.00095412
Iteration 13/1000 | Loss: 0.00008536
Iteration 14/1000 | Loss: 0.00019583
Iteration 15/1000 | Loss: 0.00068937
Iteration 16/1000 | Loss: 0.00028937
Iteration 17/1000 | Loss: 0.00092117
Iteration 18/1000 | Loss: 0.00020130
Iteration 19/1000 | Loss: 0.00003772
Iteration 20/1000 | Loss: 0.00095930
Iteration 21/1000 | Loss: 0.00006756
Iteration 22/1000 | Loss: 0.00045903
Iteration 23/1000 | Loss: 0.00009825
Iteration 24/1000 | Loss: 0.00039187
Iteration 25/1000 | Loss: 0.00166375
Iteration 26/1000 | Loss: 0.00011315
Iteration 27/1000 | Loss: 0.00020395
Iteration 28/1000 | Loss: 0.00002843
Iteration 29/1000 | Loss: 0.00038671
Iteration 30/1000 | Loss: 0.00002740
Iteration 31/1000 | Loss: 0.00024092
Iteration 32/1000 | Loss: 0.00023373
Iteration 33/1000 | Loss: 0.00068162
Iteration 34/1000 | Loss: 0.00030493
Iteration 35/1000 | Loss: 0.00003540
Iteration 36/1000 | Loss: 0.00028318
Iteration 37/1000 | Loss: 0.00011754
Iteration 38/1000 | Loss: 0.00003339
Iteration 39/1000 | Loss: 0.00029212
Iteration 40/1000 | Loss: 0.00199290
Iteration 41/1000 | Loss: 0.00038013
Iteration 42/1000 | Loss: 0.00002704
Iteration 43/1000 | Loss: 0.00002430
Iteration 44/1000 | Loss: 0.00016137
Iteration 45/1000 | Loss: 0.00058915
Iteration 46/1000 | Loss: 0.00022663
Iteration 47/1000 | Loss: 0.00023852
Iteration 48/1000 | Loss: 0.00008879
Iteration 49/1000 | Loss: 0.00010049
Iteration 50/1000 | Loss: 0.00003069
Iteration 51/1000 | Loss: 0.00002891
Iteration 52/1000 | Loss: 0.00003627
Iteration 53/1000 | Loss: 0.00002293
Iteration 54/1000 | Loss: 0.00008270
Iteration 55/1000 | Loss: 0.00002327
Iteration 56/1000 | Loss: 0.00002269
Iteration 57/1000 | Loss: 0.00002255
Iteration 58/1000 | Loss: 0.00002254
Iteration 59/1000 | Loss: 0.00002245
Iteration 60/1000 | Loss: 0.00002236
Iteration 61/1000 | Loss: 0.00002221
Iteration 62/1000 | Loss: 0.00002220
Iteration 63/1000 | Loss: 0.00002206
Iteration 64/1000 | Loss: 0.00002203
Iteration 65/1000 | Loss: 0.00002202
Iteration 66/1000 | Loss: 0.00002201
Iteration 67/1000 | Loss: 0.00009231
Iteration 68/1000 | Loss: 0.00019362
Iteration 69/1000 | Loss: 0.00124584
Iteration 70/1000 | Loss: 0.00014088
Iteration 71/1000 | Loss: 0.00008895
Iteration 72/1000 | Loss: 0.00002242
Iteration 73/1000 | Loss: 0.00002192
Iteration 74/1000 | Loss: 0.00002170
Iteration 75/1000 | Loss: 0.00002170
Iteration 76/1000 | Loss: 0.00002169
Iteration 77/1000 | Loss: 0.00002168
Iteration 78/1000 | Loss: 0.00002168
Iteration 79/1000 | Loss: 0.00002165
Iteration 80/1000 | Loss: 0.00011729
Iteration 81/1000 | Loss: 0.00005521
Iteration 82/1000 | Loss: 0.00002160
Iteration 83/1000 | Loss: 0.00002160
Iteration 84/1000 | Loss: 0.00002159
Iteration 85/1000 | Loss: 0.00002159
Iteration 86/1000 | Loss: 0.00002159
Iteration 87/1000 | Loss: 0.00002159
Iteration 88/1000 | Loss: 0.00002159
Iteration 89/1000 | Loss: 0.00002159
Iteration 90/1000 | Loss: 0.00002159
Iteration 91/1000 | Loss: 0.00002159
Iteration 92/1000 | Loss: 0.00002158
Iteration 93/1000 | Loss: 0.00002158
Iteration 94/1000 | Loss: 0.00002158
Iteration 95/1000 | Loss: 0.00002158
Iteration 96/1000 | Loss: 0.00002158
Iteration 97/1000 | Loss: 0.00002158
Iteration 98/1000 | Loss: 0.00002158
Iteration 99/1000 | Loss: 0.00002158
Iteration 100/1000 | Loss: 0.00002158
Iteration 101/1000 | Loss: 0.00002158
Iteration 102/1000 | Loss: 0.00002158
Iteration 103/1000 | Loss: 0.00002158
Iteration 104/1000 | Loss: 0.00002158
Iteration 105/1000 | Loss: 0.00002158
Iteration 106/1000 | Loss: 0.00002157
Iteration 107/1000 | Loss: 0.00002157
Iteration 108/1000 | Loss: 0.00002157
Iteration 109/1000 | Loss: 0.00002157
Iteration 110/1000 | Loss: 0.00002157
Iteration 111/1000 | Loss: 0.00002157
Iteration 112/1000 | Loss: 0.00002157
Iteration 113/1000 | Loss: 0.00007928
Iteration 114/1000 | Loss: 0.00010368
Iteration 115/1000 | Loss: 0.00002155
Iteration 116/1000 | Loss: 0.00002154
Iteration 117/1000 | Loss: 0.00002153
Iteration 118/1000 | Loss: 0.00002153
Iteration 119/1000 | Loss: 0.00002152
Iteration 120/1000 | Loss: 0.00002151
Iteration 121/1000 | Loss: 0.00002151
Iteration 122/1000 | Loss: 0.00002150
Iteration 123/1000 | Loss: 0.00002150
Iteration 124/1000 | Loss: 0.00002150
Iteration 125/1000 | Loss: 0.00002150
Iteration 126/1000 | Loss: 0.00002150
Iteration 127/1000 | Loss: 0.00014828
Iteration 128/1000 | Loss: 0.00006140
Iteration 129/1000 | Loss: 0.00009316
Iteration 130/1000 | Loss: 0.00006382
Iteration 131/1000 | Loss: 0.00007813
Iteration 132/1000 | Loss: 0.00002159
Iteration 133/1000 | Loss: 0.00004498
Iteration 134/1000 | Loss: 0.00002149
Iteration 135/1000 | Loss: 0.00002145
Iteration 136/1000 | Loss: 0.00002144
Iteration 137/1000 | Loss: 0.00002144
Iteration 138/1000 | Loss: 0.00002144
Iteration 139/1000 | Loss: 0.00002144
Iteration 140/1000 | Loss: 0.00002144
Iteration 141/1000 | Loss: 0.00002144
Iteration 142/1000 | Loss: 0.00002144
Iteration 143/1000 | Loss: 0.00002144
Iteration 144/1000 | Loss: 0.00002143
Iteration 145/1000 | Loss: 0.00002143
Iteration 146/1000 | Loss: 0.00002143
Iteration 147/1000 | Loss: 0.00002142
Iteration 148/1000 | Loss: 0.00002142
Iteration 149/1000 | Loss: 0.00002142
Iteration 150/1000 | Loss: 0.00002142
Iteration 151/1000 | Loss: 0.00002142
Iteration 152/1000 | Loss: 0.00002142
Iteration 153/1000 | Loss: 0.00002141
Iteration 154/1000 | Loss: 0.00002141
Iteration 155/1000 | Loss: 0.00002141
Iteration 156/1000 | Loss: 0.00002141
Iteration 157/1000 | Loss: 0.00002141
Iteration 158/1000 | Loss: 0.00002141
Iteration 159/1000 | Loss: 0.00002141
Iteration 160/1000 | Loss: 0.00002141
Iteration 161/1000 | Loss: 0.00002141
Iteration 162/1000 | Loss: 0.00002140
Iteration 163/1000 | Loss: 0.00002140
Iteration 164/1000 | Loss: 0.00002140
Iteration 165/1000 | Loss: 0.00002140
Iteration 166/1000 | Loss: 0.00002140
Iteration 167/1000 | Loss: 0.00002140
Iteration 168/1000 | Loss: 0.00002140
Iteration 169/1000 | Loss: 0.00002140
Iteration 170/1000 | Loss: 0.00002139
Iteration 171/1000 | Loss: 0.00002139
Iteration 172/1000 | Loss: 0.00002139
Iteration 173/1000 | Loss: 0.00002139
Iteration 174/1000 | Loss: 0.00002139
Iteration 175/1000 | Loss: 0.00002139
Iteration 176/1000 | Loss: 0.00002139
Iteration 177/1000 | Loss: 0.00002139
Iteration 178/1000 | Loss: 0.00002139
Iteration 179/1000 | Loss: 0.00002139
Iteration 180/1000 | Loss: 0.00002139
Iteration 181/1000 | Loss: 0.00002139
Iteration 182/1000 | Loss: 0.00002139
Iteration 183/1000 | Loss: 0.00002138
Iteration 184/1000 | Loss: 0.00002138
Iteration 185/1000 | Loss: 0.00002138
Iteration 186/1000 | Loss: 0.00002138
Iteration 187/1000 | Loss: 0.00002138
Iteration 188/1000 | Loss: 0.00002138
Iteration 189/1000 | Loss: 0.00002137
Iteration 190/1000 | Loss: 0.00002137
Iteration 191/1000 | Loss: 0.00002137
Iteration 192/1000 | Loss: 0.00002137
Iteration 193/1000 | Loss: 0.00002137
Iteration 194/1000 | Loss: 0.00002137
Iteration 195/1000 | Loss: 0.00002137
Iteration 196/1000 | Loss: 0.00002137
Iteration 197/1000 | Loss: 0.00002137
Iteration 198/1000 | Loss: 0.00002137
Iteration 199/1000 | Loss: 0.00002137
Iteration 200/1000 | Loss: 0.00002137
Iteration 201/1000 | Loss: 0.00002137
Iteration 202/1000 | Loss: 0.00002137
Iteration 203/1000 | Loss: 0.00002137
Iteration 204/1000 | Loss: 0.00002137
Iteration 205/1000 | Loss: 0.00002137
Iteration 206/1000 | Loss: 0.00002137
Iteration 207/1000 | Loss: 0.00002137
Iteration 208/1000 | Loss: 0.00002137
Iteration 209/1000 | Loss: 0.00002137
Iteration 210/1000 | Loss: 0.00002137
Iteration 211/1000 | Loss: 0.00002137
Iteration 212/1000 | Loss: 0.00002137
Iteration 213/1000 | Loss: 0.00002137
Iteration 214/1000 | Loss: 0.00002137
Iteration 215/1000 | Loss: 0.00002137
Iteration 216/1000 | Loss: 0.00002137
Iteration 217/1000 | Loss: 0.00002137
Iteration 218/1000 | Loss: 0.00002137
Iteration 219/1000 | Loss: 0.00002137
Iteration 220/1000 | Loss: 0.00002137
Iteration 221/1000 | Loss: 0.00002137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [2.1368323359638453e-05, 2.1368323359638453e-05, 2.1368323359638453e-05, 2.1368323359638453e-05, 2.1368323359638453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1368323359638453e-05

Optimization complete. Final v2v error: 3.9602532386779785 mm

Highest mean error: 4.351624965667725 mm for frame 10

Lowest mean error: 3.6220202445983887 mm for frame 86

Saving results

Total time: 145.97206902503967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072923
Iteration 2/25 | Loss: 0.00347460
Iteration 3/25 | Loss: 0.00192147
Iteration 4/25 | Loss: 0.00178642
Iteration 5/25 | Loss: 0.00204721
Iteration 6/25 | Loss: 0.00175180
Iteration 7/25 | Loss: 0.00158229
Iteration 8/25 | Loss: 0.00153209
Iteration 9/25 | Loss: 0.00147138
Iteration 10/25 | Loss: 0.00142452
Iteration 11/25 | Loss: 0.00141530
Iteration 12/25 | Loss: 0.00141218
Iteration 13/25 | Loss: 0.00139304
Iteration 14/25 | Loss: 0.00138527
Iteration 15/25 | Loss: 0.00138073
Iteration 16/25 | Loss: 0.00137782
Iteration 17/25 | Loss: 0.00137778
Iteration 18/25 | Loss: 0.00137777
Iteration 19/25 | Loss: 0.00138068
Iteration 20/25 | Loss: 0.00137748
Iteration 21/25 | Loss: 0.00137724
Iteration 22/25 | Loss: 0.00137724
Iteration 23/25 | Loss: 0.00137724
Iteration 24/25 | Loss: 0.00137723
Iteration 25/25 | Loss: 0.00137723

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.56842500
Iteration 2/25 | Loss: 0.00088831
Iteration 3/25 | Loss: 0.00077999
Iteration 4/25 | Loss: 0.00077999
Iteration 5/25 | Loss: 0.00077999
Iteration 6/25 | Loss: 0.00077999
Iteration 7/25 | Loss: 0.00077999
Iteration 8/25 | Loss: 0.00077999
Iteration 9/25 | Loss: 0.00077999
Iteration 10/25 | Loss: 0.00077999
Iteration 11/25 | Loss: 0.00077999
Iteration 12/25 | Loss: 0.00077999
Iteration 13/25 | Loss: 0.00077999
Iteration 14/25 | Loss: 0.00077999
Iteration 15/25 | Loss: 0.00077999
Iteration 16/25 | Loss: 0.00077999
Iteration 17/25 | Loss: 0.00077999
Iteration 18/25 | Loss: 0.00077999
Iteration 19/25 | Loss: 0.00077999
Iteration 20/25 | Loss: 0.00077999
Iteration 21/25 | Loss: 0.00077999
Iteration 22/25 | Loss: 0.00077999
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007799920858815312, 0.0007799920858815312, 0.0007799920858815312, 0.0007799920858815312, 0.0007799920858815312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007799920858815312

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077999
Iteration 2/1000 | Loss: 0.00028232
Iteration 3/1000 | Loss: 0.00009817
Iteration 4/1000 | Loss: 0.00004840
Iteration 5/1000 | Loss: 0.00009923
Iteration 6/1000 | Loss: 0.00014716
Iteration 7/1000 | Loss: 0.00002331
Iteration 8/1000 | Loss: 0.00002958
Iteration 9/1000 | Loss: 0.00009280
Iteration 10/1000 | Loss: 0.00002300
Iteration 11/1000 | Loss: 0.00002705
Iteration 12/1000 | Loss: 0.00003844
Iteration 13/1000 | Loss: 0.00002629
Iteration 14/1000 | Loss: 0.00002156
Iteration 15/1000 | Loss: 0.00002381
Iteration 16/1000 | Loss: 0.00003802
Iteration 17/1000 | Loss: 0.00002145
Iteration 18/1000 | Loss: 0.00002071
Iteration 19/1000 | Loss: 0.00002206
Iteration 20/1000 | Loss: 0.00002055
Iteration 21/1000 | Loss: 0.00002396
Iteration 22/1000 | Loss: 0.00001962
Iteration 23/1000 | Loss: 0.00002396
Iteration 24/1000 | Loss: 0.00002005
Iteration 25/1000 | Loss: 0.00002358
Iteration 26/1000 | Loss: 0.00001951
Iteration 27/1000 | Loss: 0.00001961
Iteration 28/1000 | Loss: 0.00001952
Iteration 29/1000 | Loss: 0.00001951
Iteration 30/1000 | Loss: 0.00002342
Iteration 31/1000 | Loss: 0.00002007
Iteration 32/1000 | Loss: 0.00001935
Iteration 33/1000 | Loss: 0.00001935
Iteration 34/1000 | Loss: 0.00001935
Iteration 35/1000 | Loss: 0.00001934
Iteration 36/1000 | Loss: 0.00001934
Iteration 37/1000 | Loss: 0.00002035
Iteration 38/1000 | Loss: 0.00002217
Iteration 39/1000 | Loss: 0.00002658
Iteration 40/1000 | Loss: 0.00002169
Iteration 41/1000 | Loss: 0.00002878
Iteration 42/1000 | Loss: 0.00002180
Iteration 43/1000 | Loss: 0.00002062
Iteration 44/1000 | Loss: 0.00001916
Iteration 45/1000 | Loss: 0.00001916
Iteration 46/1000 | Loss: 0.00001915
Iteration 47/1000 | Loss: 0.00001920
Iteration 48/1000 | Loss: 0.00001915
Iteration 49/1000 | Loss: 0.00001915
Iteration 50/1000 | Loss: 0.00001937
Iteration 51/1000 | Loss: 0.00001936
Iteration 52/1000 | Loss: 0.00001914
Iteration 53/1000 | Loss: 0.00001980
Iteration 54/1000 | Loss: 0.00001918
Iteration 55/1000 | Loss: 0.00001912
Iteration 56/1000 | Loss: 0.00001912
Iteration 57/1000 | Loss: 0.00001912
Iteration 58/1000 | Loss: 0.00001912
Iteration 59/1000 | Loss: 0.00001912
Iteration 60/1000 | Loss: 0.00001912
Iteration 61/1000 | Loss: 0.00001912
Iteration 62/1000 | Loss: 0.00001912
Iteration 63/1000 | Loss: 0.00001912
Iteration 64/1000 | Loss: 0.00001912
Iteration 65/1000 | Loss: 0.00001912
Iteration 66/1000 | Loss: 0.00001912
Iteration 67/1000 | Loss: 0.00001912
Iteration 68/1000 | Loss: 0.00001912
Iteration 69/1000 | Loss: 0.00001912
Iteration 70/1000 | Loss: 0.00001912
Iteration 71/1000 | Loss: 0.00001912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.9119966964353807e-05, 1.9119966964353807e-05, 1.9119966964353807e-05, 1.9119966964353807e-05, 1.9119966964353807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9119966964353807e-05

Optimization complete. Final v2v error: 3.761216163635254 mm

Highest mean error: 3.8904528617858887 mm for frame 30

Lowest mean error: 3.6194944381713867 mm for frame 185

Saving results

Total time: 98.75538444519043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034479
Iteration 2/25 | Loss: 0.01034479
Iteration 3/25 | Loss: 0.01034479
Iteration 4/25 | Loss: 0.01034479
Iteration 5/25 | Loss: 0.01034479
Iteration 6/25 | Loss: 0.01034479
Iteration 7/25 | Loss: 0.01034479
Iteration 8/25 | Loss: 0.01034478
Iteration 9/25 | Loss: 0.01034478
Iteration 10/25 | Loss: 0.01034478
Iteration 11/25 | Loss: 0.01034478
Iteration 12/25 | Loss: 0.01034478
Iteration 13/25 | Loss: 0.01034478
Iteration 14/25 | Loss: 0.01034478
Iteration 15/25 | Loss: 0.01034478
Iteration 16/25 | Loss: 0.01034478
Iteration 17/25 | Loss: 0.01034477
Iteration 18/25 | Loss: 0.01034477
Iteration 19/25 | Loss: 0.01034477
Iteration 20/25 | Loss: 0.01034477
Iteration 21/25 | Loss: 0.01034477
Iteration 22/25 | Loss: 0.01034477
Iteration 23/25 | Loss: 0.01034477
Iteration 24/25 | Loss: 0.01034477
Iteration 25/25 | Loss: 0.01034477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53744829
Iteration 2/25 | Loss: 0.12673517
Iteration 3/25 | Loss: 0.12164611
Iteration 4/25 | Loss: 0.12018216
Iteration 5/25 | Loss: 0.12007143
Iteration 6/25 | Loss: 0.12007143
Iteration 7/25 | Loss: 0.12007143
Iteration 8/25 | Loss: 0.12007143
Iteration 9/25 | Loss: 0.12007143
Iteration 10/25 | Loss: 0.12007141
Iteration 11/25 | Loss: 0.12007141
Iteration 12/25 | Loss: 0.12007141
Iteration 13/25 | Loss: 0.12007141
Iteration 14/25 | Loss: 0.12007140
Iteration 15/25 | Loss: 0.12007140
Iteration 16/25 | Loss: 0.12007140
Iteration 17/25 | Loss: 0.12007140
Iteration 18/25 | Loss: 0.12007140
Iteration 19/25 | Loss: 0.12007140
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.1200714036822319, 0.1200714036822319, 0.1200714036822319, 0.1200714036822319, 0.1200714036822319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1200714036822319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12007140
Iteration 2/1000 | Loss: 0.00067810
Iteration 3/1000 | Loss: 0.00026387
Iteration 4/1000 | Loss: 0.00008667
Iteration 5/1000 | Loss: 0.00004990
Iteration 6/1000 | Loss: 0.00003382
Iteration 7/1000 | Loss: 0.00002737
Iteration 8/1000 | Loss: 0.00002340
Iteration 9/1000 | Loss: 0.00002039
Iteration 10/1000 | Loss: 0.00001866
Iteration 11/1000 | Loss: 0.00001717
Iteration 12/1000 | Loss: 0.00001630
Iteration 13/1000 | Loss: 0.00001554
Iteration 14/1000 | Loss: 0.00001496
Iteration 15/1000 | Loss: 0.00001430
Iteration 16/1000 | Loss: 0.00001384
Iteration 17/1000 | Loss: 0.00001352
Iteration 18/1000 | Loss: 0.00001326
Iteration 19/1000 | Loss: 0.00001308
Iteration 20/1000 | Loss: 0.00001290
Iteration 21/1000 | Loss: 0.00001275
Iteration 22/1000 | Loss: 0.00001250
Iteration 23/1000 | Loss: 0.00001225
Iteration 24/1000 | Loss: 0.00001205
Iteration 25/1000 | Loss: 0.00001203
Iteration 26/1000 | Loss: 0.00001203
Iteration 27/1000 | Loss: 0.00001202
Iteration 28/1000 | Loss: 0.00001202
Iteration 29/1000 | Loss: 0.00001197
Iteration 30/1000 | Loss: 0.00001194
Iteration 31/1000 | Loss: 0.00001192
Iteration 32/1000 | Loss: 0.00001188
Iteration 33/1000 | Loss: 0.00001182
Iteration 34/1000 | Loss: 0.00001181
Iteration 35/1000 | Loss: 0.00001177
Iteration 36/1000 | Loss: 0.00001177
Iteration 37/1000 | Loss: 0.00001176
Iteration 38/1000 | Loss: 0.00001176
Iteration 39/1000 | Loss: 0.00001175
Iteration 40/1000 | Loss: 0.00001175
Iteration 41/1000 | Loss: 0.00001174
Iteration 42/1000 | Loss: 0.00001171
Iteration 43/1000 | Loss: 0.00001171
Iteration 44/1000 | Loss: 0.00001171
Iteration 45/1000 | Loss: 0.00001171
Iteration 46/1000 | Loss: 0.00001171
Iteration 47/1000 | Loss: 0.00001171
Iteration 48/1000 | Loss: 0.00001171
Iteration 49/1000 | Loss: 0.00001171
Iteration 50/1000 | Loss: 0.00001171
Iteration 51/1000 | Loss: 0.00001171
Iteration 52/1000 | Loss: 0.00001171
Iteration 53/1000 | Loss: 0.00001170
Iteration 54/1000 | Loss: 0.00001170
Iteration 55/1000 | Loss: 0.00001170
Iteration 56/1000 | Loss: 0.00001168
Iteration 57/1000 | Loss: 0.00001168
Iteration 58/1000 | Loss: 0.00001168
Iteration 59/1000 | Loss: 0.00001167
Iteration 60/1000 | Loss: 0.00001165
Iteration 61/1000 | Loss: 0.00001165
Iteration 62/1000 | Loss: 0.00001165
Iteration 63/1000 | Loss: 0.00001165
Iteration 64/1000 | Loss: 0.00001162
Iteration 65/1000 | Loss: 0.00001161
Iteration 66/1000 | Loss: 0.00001161
Iteration 67/1000 | Loss: 0.00001161
Iteration 68/1000 | Loss: 0.00001160
Iteration 69/1000 | Loss: 0.00001160
Iteration 70/1000 | Loss: 0.00001159
Iteration 71/1000 | Loss: 0.00001159
Iteration 72/1000 | Loss: 0.00001156
Iteration 73/1000 | Loss: 0.00001155
Iteration 74/1000 | Loss: 0.00001155
Iteration 75/1000 | Loss: 0.00001155
Iteration 76/1000 | Loss: 0.00001154
Iteration 77/1000 | Loss: 0.00001154
Iteration 78/1000 | Loss: 0.00001153
Iteration 79/1000 | Loss: 0.00001150
Iteration 80/1000 | Loss: 0.00001150
Iteration 81/1000 | Loss: 0.00001149
Iteration 82/1000 | Loss: 0.00001149
Iteration 83/1000 | Loss: 0.00001149
Iteration 84/1000 | Loss: 0.00001148
Iteration 85/1000 | Loss: 0.00001148
Iteration 86/1000 | Loss: 0.00001148
Iteration 87/1000 | Loss: 0.00001147
Iteration 88/1000 | Loss: 0.00001147
Iteration 89/1000 | Loss: 0.00001146
Iteration 90/1000 | Loss: 0.00001146
Iteration 91/1000 | Loss: 0.00001145
Iteration 92/1000 | Loss: 0.00001145
Iteration 93/1000 | Loss: 0.00001145
Iteration 94/1000 | Loss: 0.00001145
Iteration 95/1000 | Loss: 0.00001144
Iteration 96/1000 | Loss: 0.00001144
Iteration 97/1000 | Loss: 0.00001144
Iteration 98/1000 | Loss: 0.00001144
Iteration 99/1000 | Loss: 0.00001143
Iteration 100/1000 | Loss: 0.00001143
Iteration 101/1000 | Loss: 0.00001142
Iteration 102/1000 | Loss: 0.00001141
Iteration 103/1000 | Loss: 0.00001141
Iteration 104/1000 | Loss: 0.00001141
Iteration 105/1000 | Loss: 0.00001141
Iteration 106/1000 | Loss: 0.00001140
Iteration 107/1000 | Loss: 0.00001140
Iteration 108/1000 | Loss: 0.00001140
Iteration 109/1000 | Loss: 0.00001140
Iteration 110/1000 | Loss: 0.00001139
Iteration 111/1000 | Loss: 0.00001139
Iteration 112/1000 | Loss: 0.00001138
Iteration 113/1000 | Loss: 0.00001138
Iteration 114/1000 | Loss: 0.00001137
Iteration 115/1000 | Loss: 0.00001137
Iteration 116/1000 | Loss: 0.00001137
Iteration 117/1000 | Loss: 0.00001136
Iteration 118/1000 | Loss: 0.00001136
Iteration 119/1000 | Loss: 0.00001136
Iteration 120/1000 | Loss: 0.00001136
Iteration 121/1000 | Loss: 0.00001136
Iteration 122/1000 | Loss: 0.00001136
Iteration 123/1000 | Loss: 0.00001136
Iteration 124/1000 | Loss: 0.00001136
Iteration 125/1000 | Loss: 0.00001135
Iteration 126/1000 | Loss: 0.00001135
Iteration 127/1000 | Loss: 0.00001135
Iteration 128/1000 | Loss: 0.00001135
Iteration 129/1000 | Loss: 0.00001135
Iteration 130/1000 | Loss: 0.00001135
Iteration 131/1000 | Loss: 0.00001134
Iteration 132/1000 | Loss: 0.00001134
Iteration 133/1000 | Loss: 0.00001134
Iteration 134/1000 | Loss: 0.00001134
Iteration 135/1000 | Loss: 0.00001134
Iteration 136/1000 | Loss: 0.00001134
Iteration 137/1000 | Loss: 0.00001134
Iteration 138/1000 | Loss: 0.00001134
Iteration 139/1000 | Loss: 0.00001133
Iteration 140/1000 | Loss: 0.00001133
Iteration 141/1000 | Loss: 0.00001133
Iteration 142/1000 | Loss: 0.00001133
Iteration 143/1000 | Loss: 0.00001133
Iteration 144/1000 | Loss: 0.00001133
Iteration 145/1000 | Loss: 0.00001132
Iteration 146/1000 | Loss: 0.00001132
Iteration 147/1000 | Loss: 0.00001132
Iteration 148/1000 | Loss: 0.00001132
Iteration 149/1000 | Loss: 0.00001132
Iteration 150/1000 | Loss: 0.00001132
Iteration 151/1000 | Loss: 0.00001132
Iteration 152/1000 | Loss: 0.00001132
Iteration 153/1000 | Loss: 0.00001132
Iteration 154/1000 | Loss: 0.00001132
Iteration 155/1000 | Loss: 0.00001132
Iteration 156/1000 | Loss: 0.00001132
Iteration 157/1000 | Loss: 0.00001131
Iteration 158/1000 | Loss: 0.00001131
Iteration 159/1000 | Loss: 0.00001131
Iteration 160/1000 | Loss: 0.00001131
Iteration 161/1000 | Loss: 0.00001131
Iteration 162/1000 | Loss: 0.00001131
Iteration 163/1000 | Loss: 0.00001131
Iteration 164/1000 | Loss: 0.00001131
Iteration 165/1000 | Loss: 0.00001131
Iteration 166/1000 | Loss: 0.00001131
Iteration 167/1000 | Loss: 0.00001131
Iteration 168/1000 | Loss: 0.00001131
Iteration 169/1000 | Loss: 0.00001131
Iteration 170/1000 | Loss: 0.00001131
Iteration 171/1000 | Loss: 0.00001131
Iteration 172/1000 | Loss: 0.00001131
Iteration 173/1000 | Loss: 0.00001130
Iteration 174/1000 | Loss: 0.00001130
Iteration 175/1000 | Loss: 0.00001130
Iteration 176/1000 | Loss: 0.00001130
Iteration 177/1000 | Loss: 0.00001130
Iteration 178/1000 | Loss: 0.00001130
Iteration 179/1000 | Loss: 0.00001130
Iteration 180/1000 | Loss: 0.00001130
Iteration 181/1000 | Loss: 0.00001130
Iteration 182/1000 | Loss: 0.00001130
Iteration 183/1000 | Loss: 0.00001130
Iteration 184/1000 | Loss: 0.00001130
Iteration 185/1000 | Loss: 0.00001130
Iteration 186/1000 | Loss: 0.00001130
Iteration 187/1000 | Loss: 0.00001130
Iteration 188/1000 | Loss: 0.00001130
Iteration 189/1000 | Loss: 0.00001129
Iteration 190/1000 | Loss: 0.00001129
Iteration 191/1000 | Loss: 0.00001129
Iteration 192/1000 | Loss: 0.00001129
Iteration 193/1000 | Loss: 0.00001129
Iteration 194/1000 | Loss: 0.00001129
Iteration 195/1000 | Loss: 0.00001129
Iteration 196/1000 | Loss: 0.00001129
Iteration 197/1000 | Loss: 0.00001129
Iteration 198/1000 | Loss: 0.00001129
Iteration 199/1000 | Loss: 0.00001129
Iteration 200/1000 | Loss: 0.00001129
Iteration 201/1000 | Loss: 0.00001129
Iteration 202/1000 | Loss: 0.00001129
Iteration 203/1000 | Loss: 0.00001129
Iteration 204/1000 | Loss: 0.00001129
Iteration 205/1000 | Loss: 0.00001129
Iteration 206/1000 | Loss: 0.00001129
Iteration 207/1000 | Loss: 0.00001129
Iteration 208/1000 | Loss: 0.00001129
Iteration 209/1000 | Loss: 0.00001129
Iteration 210/1000 | Loss: 0.00001129
Iteration 211/1000 | Loss: 0.00001129
Iteration 212/1000 | Loss: 0.00001129
Iteration 213/1000 | Loss: 0.00001129
Iteration 214/1000 | Loss: 0.00001129
Iteration 215/1000 | Loss: 0.00001129
Iteration 216/1000 | Loss: 0.00001129
Iteration 217/1000 | Loss: 0.00001129
Iteration 218/1000 | Loss: 0.00001129
Iteration 219/1000 | Loss: 0.00001129
Iteration 220/1000 | Loss: 0.00001129
Iteration 221/1000 | Loss: 0.00001129
Iteration 222/1000 | Loss: 0.00001129
Iteration 223/1000 | Loss: 0.00001129
Iteration 224/1000 | Loss: 0.00001129
Iteration 225/1000 | Loss: 0.00001129
Iteration 226/1000 | Loss: 0.00001129
Iteration 227/1000 | Loss: 0.00001129
Iteration 228/1000 | Loss: 0.00001129
Iteration 229/1000 | Loss: 0.00001129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.1288930181763135e-05, 1.1288930181763135e-05, 1.1288930181763135e-05, 1.1288930181763135e-05, 1.1288930181763135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1288930181763135e-05

Optimization complete. Final v2v error: 2.8883414268493652 mm

Highest mean error: 3.2207448482513428 mm for frame 79

Lowest mean error: 2.690293550491333 mm for frame 170

Saving results

Total time: 67.23689794540405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470839
Iteration 2/25 | Loss: 0.00142509
Iteration 3/25 | Loss: 0.00129696
Iteration 4/25 | Loss: 0.00128807
Iteration 5/25 | Loss: 0.00128513
Iteration 6/25 | Loss: 0.00128463
Iteration 7/25 | Loss: 0.00128463
Iteration 8/25 | Loss: 0.00128463
Iteration 9/25 | Loss: 0.00128463
Iteration 10/25 | Loss: 0.00128463
Iteration 11/25 | Loss: 0.00128463
Iteration 12/25 | Loss: 0.00128463
Iteration 13/25 | Loss: 0.00128463
Iteration 14/25 | Loss: 0.00128463
Iteration 15/25 | Loss: 0.00128463
Iteration 16/25 | Loss: 0.00128463
Iteration 17/25 | Loss: 0.00128463
Iteration 18/25 | Loss: 0.00128463
Iteration 19/25 | Loss: 0.00128463
Iteration 20/25 | Loss: 0.00128463
Iteration 21/25 | Loss: 0.00128463
Iteration 22/25 | Loss: 0.00128463
Iteration 23/25 | Loss: 0.00128463
Iteration 24/25 | Loss: 0.00128463
Iteration 25/25 | Loss: 0.00128463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52959597
Iteration 2/25 | Loss: 0.00092573
Iteration 3/25 | Loss: 0.00092573
Iteration 4/25 | Loss: 0.00092573
Iteration 5/25 | Loss: 0.00092573
Iteration 6/25 | Loss: 0.00092573
Iteration 7/25 | Loss: 0.00092573
Iteration 8/25 | Loss: 0.00092573
Iteration 9/25 | Loss: 0.00092573
Iteration 10/25 | Loss: 0.00092573
Iteration 11/25 | Loss: 0.00092573
Iteration 12/25 | Loss: 0.00092573
Iteration 13/25 | Loss: 0.00092573
Iteration 14/25 | Loss: 0.00092573
Iteration 15/25 | Loss: 0.00092573
Iteration 16/25 | Loss: 0.00092573
Iteration 17/25 | Loss: 0.00092573
Iteration 18/25 | Loss: 0.00092573
Iteration 19/25 | Loss: 0.00092573
Iteration 20/25 | Loss: 0.00092573
Iteration 21/25 | Loss: 0.00092573
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009257275378331542, 0.0009257275378331542, 0.0009257275378331542, 0.0009257275378331542, 0.0009257275378331542]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009257275378331542

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092573
Iteration 2/1000 | Loss: 0.00003108
Iteration 3/1000 | Loss: 0.00002230
Iteration 4/1000 | Loss: 0.00002021
Iteration 5/1000 | Loss: 0.00001908
Iteration 6/1000 | Loss: 0.00001848
Iteration 7/1000 | Loss: 0.00001793
Iteration 8/1000 | Loss: 0.00001744
Iteration 9/1000 | Loss: 0.00001722
Iteration 10/1000 | Loss: 0.00001697
Iteration 11/1000 | Loss: 0.00001674
Iteration 12/1000 | Loss: 0.00001673
Iteration 13/1000 | Loss: 0.00001657
Iteration 14/1000 | Loss: 0.00001640
Iteration 15/1000 | Loss: 0.00001638
Iteration 16/1000 | Loss: 0.00001637
Iteration 17/1000 | Loss: 0.00001636
Iteration 18/1000 | Loss: 0.00001635
Iteration 19/1000 | Loss: 0.00001632
Iteration 20/1000 | Loss: 0.00001632
Iteration 21/1000 | Loss: 0.00001629
Iteration 22/1000 | Loss: 0.00001625
Iteration 23/1000 | Loss: 0.00001624
Iteration 24/1000 | Loss: 0.00001624
Iteration 25/1000 | Loss: 0.00001620
Iteration 26/1000 | Loss: 0.00001619
Iteration 27/1000 | Loss: 0.00001615
Iteration 28/1000 | Loss: 0.00001614
Iteration 29/1000 | Loss: 0.00001613
Iteration 30/1000 | Loss: 0.00001613
Iteration 31/1000 | Loss: 0.00001613
Iteration 32/1000 | Loss: 0.00001613
Iteration 33/1000 | Loss: 0.00001613
Iteration 34/1000 | Loss: 0.00001612
Iteration 35/1000 | Loss: 0.00001612
Iteration 36/1000 | Loss: 0.00001611
Iteration 37/1000 | Loss: 0.00001611
Iteration 38/1000 | Loss: 0.00001611
Iteration 39/1000 | Loss: 0.00001611
Iteration 40/1000 | Loss: 0.00001610
Iteration 41/1000 | Loss: 0.00001610
Iteration 42/1000 | Loss: 0.00001609
Iteration 43/1000 | Loss: 0.00001603
Iteration 44/1000 | Loss: 0.00001601
Iteration 45/1000 | Loss: 0.00001597
Iteration 46/1000 | Loss: 0.00001597
Iteration 47/1000 | Loss: 0.00001597
Iteration 48/1000 | Loss: 0.00001595
Iteration 49/1000 | Loss: 0.00001595
Iteration 50/1000 | Loss: 0.00001594
Iteration 51/1000 | Loss: 0.00001594
Iteration 52/1000 | Loss: 0.00001593
Iteration 53/1000 | Loss: 0.00001593
Iteration 54/1000 | Loss: 0.00001592
Iteration 55/1000 | Loss: 0.00001591
Iteration 56/1000 | Loss: 0.00001591
Iteration 57/1000 | Loss: 0.00001590
Iteration 58/1000 | Loss: 0.00001590
Iteration 59/1000 | Loss: 0.00001588
Iteration 60/1000 | Loss: 0.00001586
Iteration 61/1000 | Loss: 0.00001586
Iteration 62/1000 | Loss: 0.00001585
Iteration 63/1000 | Loss: 0.00001585
Iteration 64/1000 | Loss: 0.00001585
Iteration 65/1000 | Loss: 0.00001585
Iteration 66/1000 | Loss: 0.00001585
Iteration 67/1000 | Loss: 0.00001585
Iteration 68/1000 | Loss: 0.00001585
Iteration 69/1000 | Loss: 0.00001585
Iteration 70/1000 | Loss: 0.00001585
Iteration 71/1000 | Loss: 0.00001585
Iteration 72/1000 | Loss: 0.00001585
Iteration 73/1000 | Loss: 0.00001585
Iteration 74/1000 | Loss: 0.00001585
Iteration 75/1000 | Loss: 0.00001585
Iteration 76/1000 | Loss: 0.00001585
Iteration 77/1000 | Loss: 0.00001585
Iteration 78/1000 | Loss: 0.00001585
Iteration 79/1000 | Loss: 0.00001585
Iteration 80/1000 | Loss: 0.00001585
Iteration 81/1000 | Loss: 0.00001585
Iteration 82/1000 | Loss: 0.00001584
Iteration 83/1000 | Loss: 0.00001584
Iteration 84/1000 | Loss: 0.00001583
Iteration 85/1000 | Loss: 0.00001583
Iteration 86/1000 | Loss: 0.00001583
Iteration 87/1000 | Loss: 0.00001583
Iteration 88/1000 | Loss: 0.00001582
Iteration 89/1000 | Loss: 0.00001582
Iteration 90/1000 | Loss: 0.00001582
Iteration 91/1000 | Loss: 0.00001582
Iteration 92/1000 | Loss: 0.00001582
Iteration 93/1000 | Loss: 0.00001581
Iteration 94/1000 | Loss: 0.00001581
Iteration 95/1000 | Loss: 0.00001581
Iteration 96/1000 | Loss: 0.00001581
Iteration 97/1000 | Loss: 0.00001580
Iteration 98/1000 | Loss: 0.00001580
Iteration 99/1000 | Loss: 0.00001580
Iteration 100/1000 | Loss: 0.00001580
Iteration 101/1000 | Loss: 0.00001580
Iteration 102/1000 | Loss: 0.00001580
Iteration 103/1000 | Loss: 0.00001579
Iteration 104/1000 | Loss: 0.00001579
Iteration 105/1000 | Loss: 0.00001579
Iteration 106/1000 | Loss: 0.00001579
Iteration 107/1000 | Loss: 0.00001579
Iteration 108/1000 | Loss: 0.00001579
Iteration 109/1000 | Loss: 0.00001578
Iteration 110/1000 | Loss: 0.00001578
Iteration 111/1000 | Loss: 0.00001578
Iteration 112/1000 | Loss: 0.00001578
Iteration 113/1000 | Loss: 0.00001578
Iteration 114/1000 | Loss: 0.00001578
Iteration 115/1000 | Loss: 0.00001578
Iteration 116/1000 | Loss: 0.00001577
Iteration 117/1000 | Loss: 0.00001577
Iteration 118/1000 | Loss: 0.00001577
Iteration 119/1000 | Loss: 0.00001577
Iteration 120/1000 | Loss: 0.00001577
Iteration 121/1000 | Loss: 0.00001577
Iteration 122/1000 | Loss: 0.00001577
Iteration 123/1000 | Loss: 0.00001577
Iteration 124/1000 | Loss: 0.00001577
Iteration 125/1000 | Loss: 0.00001577
Iteration 126/1000 | Loss: 0.00001577
Iteration 127/1000 | Loss: 0.00001577
Iteration 128/1000 | Loss: 0.00001577
Iteration 129/1000 | Loss: 0.00001576
Iteration 130/1000 | Loss: 0.00001576
Iteration 131/1000 | Loss: 0.00001576
Iteration 132/1000 | Loss: 0.00001576
Iteration 133/1000 | Loss: 0.00001576
Iteration 134/1000 | Loss: 0.00001576
Iteration 135/1000 | Loss: 0.00001576
Iteration 136/1000 | Loss: 0.00001576
Iteration 137/1000 | Loss: 0.00001576
Iteration 138/1000 | Loss: 0.00001576
Iteration 139/1000 | Loss: 0.00001576
Iteration 140/1000 | Loss: 0.00001575
Iteration 141/1000 | Loss: 0.00001575
Iteration 142/1000 | Loss: 0.00001575
Iteration 143/1000 | Loss: 0.00001575
Iteration 144/1000 | Loss: 0.00001575
Iteration 145/1000 | Loss: 0.00001575
Iteration 146/1000 | Loss: 0.00001574
Iteration 147/1000 | Loss: 0.00001574
Iteration 148/1000 | Loss: 0.00001574
Iteration 149/1000 | Loss: 0.00001574
Iteration 150/1000 | Loss: 0.00001574
Iteration 151/1000 | Loss: 0.00001574
Iteration 152/1000 | Loss: 0.00001574
Iteration 153/1000 | Loss: 0.00001574
Iteration 154/1000 | Loss: 0.00001573
Iteration 155/1000 | Loss: 0.00001573
Iteration 156/1000 | Loss: 0.00001573
Iteration 157/1000 | Loss: 0.00001573
Iteration 158/1000 | Loss: 0.00001573
Iteration 159/1000 | Loss: 0.00001573
Iteration 160/1000 | Loss: 0.00001573
Iteration 161/1000 | Loss: 0.00001573
Iteration 162/1000 | Loss: 0.00001573
Iteration 163/1000 | Loss: 0.00001573
Iteration 164/1000 | Loss: 0.00001573
Iteration 165/1000 | Loss: 0.00001573
Iteration 166/1000 | Loss: 0.00001573
Iteration 167/1000 | Loss: 0.00001573
Iteration 168/1000 | Loss: 0.00001573
Iteration 169/1000 | Loss: 0.00001573
Iteration 170/1000 | Loss: 0.00001573
Iteration 171/1000 | Loss: 0.00001573
Iteration 172/1000 | Loss: 0.00001573
Iteration 173/1000 | Loss: 0.00001572
Iteration 174/1000 | Loss: 0.00001572
Iteration 175/1000 | Loss: 0.00001572
Iteration 176/1000 | Loss: 0.00001572
Iteration 177/1000 | Loss: 0.00001572
Iteration 178/1000 | Loss: 0.00001572
Iteration 179/1000 | Loss: 0.00001572
Iteration 180/1000 | Loss: 0.00001572
Iteration 181/1000 | Loss: 0.00001572
Iteration 182/1000 | Loss: 0.00001572
Iteration 183/1000 | Loss: 0.00001572
Iteration 184/1000 | Loss: 0.00001571
Iteration 185/1000 | Loss: 0.00001571
Iteration 186/1000 | Loss: 0.00001571
Iteration 187/1000 | Loss: 0.00001571
Iteration 188/1000 | Loss: 0.00001571
Iteration 189/1000 | Loss: 0.00001571
Iteration 190/1000 | Loss: 0.00001571
Iteration 191/1000 | Loss: 0.00001571
Iteration 192/1000 | Loss: 0.00001571
Iteration 193/1000 | Loss: 0.00001571
Iteration 194/1000 | Loss: 0.00001571
Iteration 195/1000 | Loss: 0.00001571
Iteration 196/1000 | Loss: 0.00001570
Iteration 197/1000 | Loss: 0.00001570
Iteration 198/1000 | Loss: 0.00001570
Iteration 199/1000 | Loss: 0.00001570
Iteration 200/1000 | Loss: 0.00001570
Iteration 201/1000 | Loss: 0.00001570
Iteration 202/1000 | Loss: 0.00001570
Iteration 203/1000 | Loss: 0.00001570
Iteration 204/1000 | Loss: 0.00001570
Iteration 205/1000 | Loss: 0.00001570
Iteration 206/1000 | Loss: 0.00001570
Iteration 207/1000 | Loss: 0.00001570
Iteration 208/1000 | Loss: 0.00001570
Iteration 209/1000 | Loss: 0.00001570
Iteration 210/1000 | Loss: 0.00001570
Iteration 211/1000 | Loss: 0.00001570
Iteration 212/1000 | Loss: 0.00001570
Iteration 213/1000 | Loss: 0.00001569
Iteration 214/1000 | Loss: 0.00001569
Iteration 215/1000 | Loss: 0.00001569
Iteration 216/1000 | Loss: 0.00001569
Iteration 217/1000 | Loss: 0.00001569
Iteration 218/1000 | Loss: 0.00001569
Iteration 219/1000 | Loss: 0.00001569
Iteration 220/1000 | Loss: 0.00001569
Iteration 221/1000 | Loss: 0.00001569
Iteration 222/1000 | Loss: 0.00001569
Iteration 223/1000 | Loss: 0.00001569
Iteration 224/1000 | Loss: 0.00001569
Iteration 225/1000 | Loss: 0.00001569
Iteration 226/1000 | Loss: 0.00001569
Iteration 227/1000 | Loss: 0.00001569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.568950574437622e-05, 1.568950574437622e-05, 1.568950574437622e-05, 1.568950574437622e-05, 1.568950574437622e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.568950574437622e-05

Optimization complete. Final v2v error: 3.309337854385376 mm

Highest mean error: 3.8472046852111816 mm for frame 15

Lowest mean error: 2.760246753692627 mm for frame 115

Saving results

Total time: 45.48343586921692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_005/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_005/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984750
Iteration 2/25 | Loss: 0.00275311
Iteration 3/25 | Loss: 0.00193060
Iteration 4/25 | Loss: 0.00188368
Iteration 5/25 | Loss: 0.00179037
Iteration 6/25 | Loss: 0.00169747
Iteration 7/25 | Loss: 0.00158202
Iteration 8/25 | Loss: 0.00158696
Iteration 9/25 | Loss: 0.00152193
Iteration 10/25 | Loss: 0.00150144
Iteration 11/25 | Loss: 0.00149303
Iteration 12/25 | Loss: 0.00149858
Iteration 13/25 | Loss: 0.00148900
Iteration 14/25 | Loss: 0.00149260
Iteration 15/25 | Loss: 0.00148028
Iteration 16/25 | Loss: 0.00149477
Iteration 17/25 | Loss: 0.00147181
Iteration 18/25 | Loss: 0.00146536
Iteration 19/25 | Loss: 0.00146395
Iteration 20/25 | Loss: 0.00146365
Iteration 21/25 | Loss: 0.00146356
Iteration 22/25 | Loss: 0.00146354
Iteration 23/25 | Loss: 0.00146353
Iteration 24/25 | Loss: 0.00146353
Iteration 25/25 | Loss: 0.00146353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41825354
Iteration 2/25 | Loss: 0.00257115
Iteration 3/25 | Loss: 0.00257115
Iteration 4/25 | Loss: 0.00257115
Iteration 5/25 | Loss: 0.00257115
Iteration 6/25 | Loss: 0.00257115
Iteration 7/25 | Loss: 0.00257115
Iteration 8/25 | Loss: 0.00257115
Iteration 9/25 | Loss: 0.00257115
Iteration 10/25 | Loss: 0.00257115
Iteration 11/25 | Loss: 0.00257115
Iteration 12/25 | Loss: 0.00257115
Iteration 13/25 | Loss: 0.00257115
Iteration 14/25 | Loss: 0.00257115
Iteration 15/25 | Loss: 0.00257115
Iteration 16/25 | Loss: 0.00257115
Iteration 17/25 | Loss: 0.00257115
Iteration 18/25 | Loss: 0.00257115
Iteration 19/25 | Loss: 0.00257115
Iteration 20/25 | Loss: 0.00257115
Iteration 21/25 | Loss: 0.00257115
Iteration 22/25 | Loss: 0.00257115
Iteration 23/25 | Loss: 0.00257115
Iteration 24/25 | Loss: 0.00257115
Iteration 25/25 | Loss: 0.00257115

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00257115
Iteration 2/1000 | Loss: 0.00030668
Iteration 3/1000 | Loss: 0.00026717
Iteration 4/1000 | Loss: 0.00051484
Iteration 5/1000 | Loss: 0.00017214
Iteration 6/1000 | Loss: 0.00024899
Iteration 7/1000 | Loss: 0.00051678
Iteration 8/1000 | Loss: 0.00021865
Iteration 9/1000 | Loss: 0.00070825
Iteration 10/1000 | Loss: 0.00146896
Iteration 11/1000 | Loss: 0.00212270
Iteration 12/1000 | Loss: 0.00475199
Iteration 13/1000 | Loss: 0.00310463
Iteration 14/1000 | Loss: 0.00211649
Iteration 15/1000 | Loss: 0.00049245
Iteration 16/1000 | Loss: 0.00024460
Iteration 17/1000 | Loss: 0.00028854
Iteration 18/1000 | Loss: 0.00014762
Iteration 19/1000 | Loss: 0.00015381
Iteration 20/1000 | Loss: 0.00035972
Iteration 21/1000 | Loss: 0.00006644
Iteration 22/1000 | Loss: 0.00017837
Iteration 23/1000 | Loss: 0.00008234
Iteration 24/1000 | Loss: 0.00016269
Iteration 25/1000 | Loss: 0.00004810
Iteration 26/1000 | Loss: 0.00032619
Iteration 27/1000 | Loss: 0.00011178
Iteration 28/1000 | Loss: 0.00080641
Iteration 29/1000 | Loss: 0.00004189
Iteration 30/1000 | Loss: 0.00002656
Iteration 31/1000 | Loss: 0.00010097
Iteration 32/1000 | Loss: 0.00005887
Iteration 33/1000 | Loss: 0.00003019
Iteration 34/1000 | Loss: 0.00002780
Iteration 35/1000 | Loss: 0.00002318
Iteration 36/1000 | Loss: 0.00013054
Iteration 37/1000 | Loss: 0.00003935
Iteration 38/1000 | Loss: 0.00003393
Iteration 39/1000 | Loss: 0.00002239
Iteration 40/1000 | Loss: 0.00003202
Iteration 41/1000 | Loss: 0.00002224
Iteration 42/1000 | Loss: 0.00004010
Iteration 43/1000 | Loss: 0.00002587
Iteration 44/1000 | Loss: 0.00002432
Iteration 45/1000 | Loss: 0.00004218
Iteration 46/1000 | Loss: 0.00004240
Iteration 47/1000 | Loss: 0.00002304
Iteration 48/1000 | Loss: 0.00009897
Iteration 49/1000 | Loss: 0.00002206
Iteration 50/1000 | Loss: 0.00006126
Iteration 51/1000 | Loss: 0.00012135
Iteration 52/1000 | Loss: 0.00002191
Iteration 53/1000 | Loss: 0.00002179
Iteration 54/1000 | Loss: 0.00002179
Iteration 55/1000 | Loss: 0.00002179
Iteration 56/1000 | Loss: 0.00002179
Iteration 57/1000 | Loss: 0.00002179
Iteration 58/1000 | Loss: 0.00002179
Iteration 59/1000 | Loss: 0.00002179
Iteration 60/1000 | Loss: 0.00002179
Iteration 61/1000 | Loss: 0.00002178
Iteration 62/1000 | Loss: 0.00002178
Iteration 63/1000 | Loss: 0.00002178
Iteration 64/1000 | Loss: 0.00002178
Iteration 65/1000 | Loss: 0.00002178
Iteration 66/1000 | Loss: 0.00002178
Iteration 67/1000 | Loss: 0.00002178
Iteration 68/1000 | Loss: 0.00002177
Iteration 69/1000 | Loss: 0.00002177
Iteration 70/1000 | Loss: 0.00002177
Iteration 71/1000 | Loss: 0.00002177
Iteration 72/1000 | Loss: 0.00003201
Iteration 73/1000 | Loss: 0.00002177
Iteration 74/1000 | Loss: 0.00002175
Iteration 75/1000 | Loss: 0.00002174
Iteration 76/1000 | Loss: 0.00002174
Iteration 77/1000 | Loss: 0.00002173
Iteration 78/1000 | Loss: 0.00002172
Iteration 79/1000 | Loss: 0.00002172
Iteration 80/1000 | Loss: 0.00002172
Iteration 81/1000 | Loss: 0.00002172
Iteration 82/1000 | Loss: 0.00002172
Iteration 83/1000 | Loss: 0.00002172
Iteration 84/1000 | Loss: 0.00002171
Iteration 85/1000 | Loss: 0.00002171
Iteration 86/1000 | Loss: 0.00002171
Iteration 87/1000 | Loss: 0.00004351
Iteration 88/1000 | Loss: 0.00004351
Iteration 89/1000 | Loss: 0.00019620
Iteration 90/1000 | Loss: 0.00005107
Iteration 91/1000 | Loss: 0.00002180
Iteration 92/1000 | Loss: 0.00003223
Iteration 93/1000 | Loss: 0.00012105
Iteration 94/1000 | Loss: 0.00003152
Iteration 95/1000 | Loss: 0.00003638
Iteration 96/1000 | Loss: 0.00002157
Iteration 97/1000 | Loss: 0.00003290
Iteration 98/1000 | Loss: 0.00003290
Iteration 99/1000 | Loss: 0.00022290
Iteration 100/1000 | Loss: 0.00002205
Iteration 101/1000 | Loss: 0.00002488
Iteration 102/1000 | Loss: 0.00002134
Iteration 103/1000 | Loss: 0.00002133
Iteration 104/1000 | Loss: 0.00002129
Iteration 105/1000 | Loss: 0.00004531
Iteration 106/1000 | Loss: 0.00023202
Iteration 107/1000 | Loss: 0.00003595
Iteration 108/1000 | Loss: 0.00005901
Iteration 109/1000 | Loss: 0.00003835
Iteration 110/1000 | Loss: 0.00002438
Iteration 111/1000 | Loss: 0.00002242
Iteration 112/1000 | Loss: 0.00003725
Iteration 113/1000 | Loss: 0.00009067
Iteration 114/1000 | Loss: 0.00075041
Iteration 115/1000 | Loss: 0.00004398
Iteration 116/1000 | Loss: 0.00020114
Iteration 117/1000 | Loss: 0.00003789
Iteration 118/1000 | Loss: 0.00002275
Iteration 119/1000 | Loss: 0.00002216
Iteration 120/1000 | Loss: 0.00003654
Iteration 121/1000 | Loss: 0.00004045
Iteration 122/1000 | Loss: 0.00002188
Iteration 123/1000 | Loss: 0.00002187
Iteration 124/1000 | Loss: 0.00002187
Iteration 125/1000 | Loss: 0.00002186
Iteration 126/1000 | Loss: 0.00002186
Iteration 127/1000 | Loss: 0.00002185
Iteration 128/1000 | Loss: 0.00002185
Iteration 129/1000 | Loss: 0.00002184
Iteration 130/1000 | Loss: 0.00002184
Iteration 131/1000 | Loss: 0.00002184
Iteration 132/1000 | Loss: 0.00002184
Iteration 133/1000 | Loss: 0.00002184
Iteration 134/1000 | Loss: 0.00002184
Iteration 135/1000 | Loss: 0.00002996
Iteration 136/1000 | Loss: 0.00002666
Iteration 137/1000 | Loss: 0.00002957
Iteration 138/1000 | Loss: 0.00005257
Iteration 139/1000 | Loss: 0.00002184
Iteration 140/1000 | Loss: 0.00003606
Iteration 141/1000 | Loss: 0.00004554
Iteration 142/1000 | Loss: 0.00002810
Iteration 143/1000 | Loss: 0.00003871
Iteration 144/1000 | Loss: 0.00024153
Iteration 145/1000 | Loss: 0.00008581
Iteration 146/1000 | Loss: 0.00006132
Iteration 147/1000 | Loss: 0.00003321
Iteration 148/1000 | Loss: 0.00002175
Iteration 149/1000 | Loss: 0.00002174
Iteration 150/1000 | Loss: 0.00002171
Iteration 151/1000 | Loss: 0.00002171
Iteration 152/1000 | Loss: 0.00002170
Iteration 153/1000 | Loss: 0.00002170
Iteration 154/1000 | Loss: 0.00002403
Iteration 155/1000 | Loss: 0.00002171
Iteration 156/1000 | Loss: 0.00003130
Iteration 157/1000 | Loss: 0.00002165
Iteration 158/1000 | Loss: 0.00003157
Iteration 159/1000 | Loss: 0.00002160
Iteration 160/1000 | Loss: 0.00004140
Iteration 161/1000 | Loss: 0.00024870
Iteration 162/1000 | Loss: 0.00010362
Iteration 163/1000 | Loss: 0.00003957
Iteration 164/1000 | Loss: 0.00002907
Iteration 165/1000 | Loss: 0.00002480
Iteration 166/1000 | Loss: 0.00002357
Iteration 167/1000 | Loss: 0.00002711
Iteration 168/1000 | Loss: 0.00004346
Iteration 169/1000 | Loss: 0.00008529
Iteration 170/1000 | Loss: 0.00002151
Iteration 171/1000 | Loss: 0.00002148
Iteration 172/1000 | Loss: 0.00002148
Iteration 173/1000 | Loss: 0.00002147
Iteration 174/1000 | Loss: 0.00002147
Iteration 175/1000 | Loss: 0.00002147
Iteration 176/1000 | Loss: 0.00002147
Iteration 177/1000 | Loss: 0.00002147
Iteration 178/1000 | Loss: 0.00002147
Iteration 179/1000 | Loss: 0.00002147
Iteration 180/1000 | Loss: 0.00002147
Iteration 181/1000 | Loss: 0.00002146
Iteration 182/1000 | Loss: 0.00002146
Iteration 183/1000 | Loss: 0.00002844
Iteration 184/1000 | Loss: 0.00002150
Iteration 185/1000 | Loss: 0.00002149
Iteration 186/1000 | Loss: 0.00002148
Iteration 187/1000 | Loss: 0.00002148
Iteration 188/1000 | Loss: 0.00002148
Iteration 189/1000 | Loss: 0.00002147
Iteration 190/1000 | Loss: 0.00002147
Iteration 191/1000 | Loss: 0.00002146
Iteration 192/1000 | Loss: 0.00002173
Iteration 193/1000 | Loss: 0.00002148
Iteration 194/1000 | Loss: 0.00002146
Iteration 195/1000 | Loss: 0.00002146
Iteration 196/1000 | Loss: 0.00002146
Iteration 197/1000 | Loss: 0.00002146
Iteration 198/1000 | Loss: 0.00002146
Iteration 199/1000 | Loss: 0.00002146
Iteration 200/1000 | Loss: 0.00002146
Iteration 201/1000 | Loss: 0.00002146
Iteration 202/1000 | Loss: 0.00002145
Iteration 203/1000 | Loss: 0.00002145
Iteration 204/1000 | Loss: 0.00002145
Iteration 205/1000 | Loss: 0.00002145
Iteration 206/1000 | Loss: 0.00002145
Iteration 207/1000 | Loss: 0.00002145
Iteration 208/1000 | Loss: 0.00002145
Iteration 209/1000 | Loss: 0.00002145
Iteration 210/1000 | Loss: 0.00002145
Iteration 211/1000 | Loss: 0.00002145
Iteration 212/1000 | Loss: 0.00002145
Iteration 213/1000 | Loss: 0.00002145
Iteration 214/1000 | Loss: 0.00002145
Iteration 215/1000 | Loss: 0.00002145
Iteration 216/1000 | Loss: 0.00002145
Iteration 217/1000 | Loss: 0.00002145
Iteration 218/1000 | Loss: 0.00002145
Iteration 219/1000 | Loss: 0.00002145
Iteration 220/1000 | Loss: 0.00002145
Iteration 221/1000 | Loss: 0.00002145
Iteration 222/1000 | Loss: 0.00002145
Iteration 223/1000 | Loss: 0.00002145
Iteration 224/1000 | Loss: 0.00002145
Iteration 225/1000 | Loss: 0.00002145
Iteration 226/1000 | Loss: 0.00002145
Iteration 227/1000 | Loss: 0.00002145
Iteration 228/1000 | Loss: 0.00002145
Iteration 229/1000 | Loss: 0.00002145
Iteration 230/1000 | Loss: 0.00002145
Iteration 231/1000 | Loss: 0.00002145
Iteration 232/1000 | Loss: 0.00002145
Iteration 233/1000 | Loss: 0.00002145
Iteration 234/1000 | Loss: 0.00002145
Iteration 235/1000 | Loss: 0.00002145
Iteration 236/1000 | Loss: 0.00002145
Iteration 237/1000 | Loss: 0.00002145
Iteration 238/1000 | Loss: 0.00002145
Iteration 239/1000 | Loss: 0.00002145
Iteration 240/1000 | Loss: 0.00002145
Iteration 241/1000 | Loss: 0.00002145
Iteration 242/1000 | Loss: 0.00002145
Iteration 243/1000 | Loss: 0.00002145
Iteration 244/1000 | Loss: 0.00002145
Iteration 245/1000 | Loss: 0.00002145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [2.1451289285323583e-05, 2.1451289285323583e-05, 2.1451289285323583e-05, 2.1451289285323583e-05, 2.1451289285323583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1451289285323583e-05

Optimization complete. Final v2v error: 3.8527398109436035 mm

Highest mean error: 10.350751876831055 mm for frame 155

Lowest mean error: 3.721140146255493 mm for frame 88

Saving results

Total time: 230.77361392974854
