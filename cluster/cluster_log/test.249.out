Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=249, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13944-13999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795543
Iteration 2/25 | Loss: 0.00133133
Iteration 3/25 | Loss: 0.00122410
Iteration 4/25 | Loss: 0.00120854
Iteration 5/25 | Loss: 0.00120485
Iteration 6/25 | Loss: 0.00120465
Iteration 7/25 | Loss: 0.00120465
Iteration 8/25 | Loss: 0.00120465
Iteration 9/25 | Loss: 0.00120465
Iteration 10/25 | Loss: 0.00120465
Iteration 11/25 | Loss: 0.00120465
Iteration 12/25 | Loss: 0.00120465
Iteration 13/25 | Loss: 0.00120465
Iteration 14/25 | Loss: 0.00120465
Iteration 15/25 | Loss: 0.00120465
Iteration 16/25 | Loss: 0.00120465
Iteration 17/25 | Loss: 0.00120465
Iteration 18/25 | Loss: 0.00120465
Iteration 19/25 | Loss: 0.00120465
Iteration 20/25 | Loss: 0.00120465
Iteration 21/25 | Loss: 0.00120465
Iteration 22/25 | Loss: 0.00120465
Iteration 23/25 | Loss: 0.00120465
Iteration 24/25 | Loss: 0.00120465
Iteration 25/25 | Loss: 0.00120465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43194234
Iteration 2/25 | Loss: 0.00055811
Iteration 3/25 | Loss: 0.00055808
Iteration 4/25 | Loss: 0.00055808
Iteration 5/25 | Loss: 0.00055808
Iteration 6/25 | Loss: 0.00055808
Iteration 7/25 | Loss: 0.00055808
Iteration 8/25 | Loss: 0.00055808
Iteration 9/25 | Loss: 0.00055808
Iteration 10/25 | Loss: 0.00055808
Iteration 11/25 | Loss: 0.00055808
Iteration 12/25 | Loss: 0.00055808
Iteration 13/25 | Loss: 0.00055808
Iteration 14/25 | Loss: 0.00055808
Iteration 15/25 | Loss: 0.00055808
Iteration 16/25 | Loss: 0.00055808
Iteration 17/25 | Loss: 0.00055808
Iteration 18/25 | Loss: 0.00055808
Iteration 19/25 | Loss: 0.00055808
Iteration 20/25 | Loss: 0.00055808
Iteration 21/25 | Loss: 0.00055808
Iteration 22/25 | Loss: 0.00055808
Iteration 23/25 | Loss: 0.00055808
Iteration 24/25 | Loss: 0.00055808
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005580777069553733, 0.0005580777069553733, 0.0005580777069553733, 0.0005580777069553733, 0.0005580777069553733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005580777069553733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055808
Iteration 2/1000 | Loss: 0.00003898
Iteration 3/1000 | Loss: 0.00002735
Iteration 4/1000 | Loss: 0.00002277
Iteration 5/1000 | Loss: 0.00002131
Iteration 6/1000 | Loss: 0.00002042
Iteration 7/1000 | Loss: 0.00001969
Iteration 8/1000 | Loss: 0.00001923
Iteration 9/1000 | Loss: 0.00001895
Iteration 10/1000 | Loss: 0.00001863
Iteration 11/1000 | Loss: 0.00001844
Iteration 12/1000 | Loss: 0.00001840
Iteration 13/1000 | Loss: 0.00001839
Iteration 14/1000 | Loss: 0.00001831
Iteration 15/1000 | Loss: 0.00001829
Iteration 16/1000 | Loss: 0.00001827
Iteration 17/1000 | Loss: 0.00001815
Iteration 18/1000 | Loss: 0.00001812
Iteration 19/1000 | Loss: 0.00001811
Iteration 20/1000 | Loss: 0.00001810
Iteration 21/1000 | Loss: 0.00001809
Iteration 22/1000 | Loss: 0.00001809
Iteration 23/1000 | Loss: 0.00001808
Iteration 24/1000 | Loss: 0.00001806
Iteration 25/1000 | Loss: 0.00001806
Iteration 26/1000 | Loss: 0.00001805
Iteration 27/1000 | Loss: 0.00001804
Iteration 28/1000 | Loss: 0.00001801
Iteration 29/1000 | Loss: 0.00001798
Iteration 30/1000 | Loss: 0.00001797
Iteration 31/1000 | Loss: 0.00001797
Iteration 32/1000 | Loss: 0.00001796
Iteration 33/1000 | Loss: 0.00001795
Iteration 34/1000 | Loss: 0.00001795
Iteration 35/1000 | Loss: 0.00001794
Iteration 36/1000 | Loss: 0.00001793
Iteration 37/1000 | Loss: 0.00001793
Iteration 38/1000 | Loss: 0.00001793
Iteration 39/1000 | Loss: 0.00001792
Iteration 40/1000 | Loss: 0.00001792
Iteration 41/1000 | Loss: 0.00001791
Iteration 42/1000 | Loss: 0.00001790
Iteration 43/1000 | Loss: 0.00001790
Iteration 44/1000 | Loss: 0.00001789
Iteration 45/1000 | Loss: 0.00001788
Iteration 46/1000 | Loss: 0.00001788
Iteration 47/1000 | Loss: 0.00001787
Iteration 48/1000 | Loss: 0.00001786
Iteration 49/1000 | Loss: 0.00001780
Iteration 50/1000 | Loss: 0.00001775
Iteration 51/1000 | Loss: 0.00001775
Iteration 52/1000 | Loss: 0.00001774
Iteration 53/1000 | Loss: 0.00001774
Iteration 54/1000 | Loss: 0.00001774
Iteration 55/1000 | Loss: 0.00001773
Iteration 56/1000 | Loss: 0.00001773
Iteration 57/1000 | Loss: 0.00001772
Iteration 58/1000 | Loss: 0.00001772
Iteration 59/1000 | Loss: 0.00001771
Iteration 60/1000 | Loss: 0.00001771
Iteration 61/1000 | Loss: 0.00001771
Iteration 62/1000 | Loss: 0.00001771
Iteration 63/1000 | Loss: 0.00001771
Iteration 64/1000 | Loss: 0.00001771
Iteration 65/1000 | Loss: 0.00001771
Iteration 66/1000 | Loss: 0.00001771
Iteration 67/1000 | Loss: 0.00001771
Iteration 68/1000 | Loss: 0.00001771
Iteration 69/1000 | Loss: 0.00001769
Iteration 70/1000 | Loss: 0.00001768
Iteration 71/1000 | Loss: 0.00001766
Iteration 72/1000 | Loss: 0.00001766
Iteration 73/1000 | Loss: 0.00001766
Iteration 74/1000 | Loss: 0.00001765
Iteration 75/1000 | Loss: 0.00001765
Iteration 76/1000 | Loss: 0.00001764
Iteration 77/1000 | Loss: 0.00001764
Iteration 78/1000 | Loss: 0.00001764
Iteration 79/1000 | Loss: 0.00001763
Iteration 80/1000 | Loss: 0.00001763
Iteration 81/1000 | Loss: 0.00001763
Iteration 82/1000 | Loss: 0.00001763
Iteration 83/1000 | Loss: 0.00001763
Iteration 84/1000 | Loss: 0.00001762
Iteration 85/1000 | Loss: 0.00001762
Iteration 86/1000 | Loss: 0.00001762
Iteration 87/1000 | Loss: 0.00001762
Iteration 88/1000 | Loss: 0.00001762
Iteration 89/1000 | Loss: 0.00001762
Iteration 90/1000 | Loss: 0.00001762
Iteration 91/1000 | Loss: 0.00001762
Iteration 92/1000 | Loss: 0.00001762
Iteration 93/1000 | Loss: 0.00001762
Iteration 94/1000 | Loss: 0.00001762
Iteration 95/1000 | Loss: 0.00001761
Iteration 96/1000 | Loss: 0.00001761
Iteration 97/1000 | Loss: 0.00001761
Iteration 98/1000 | Loss: 0.00001760
Iteration 99/1000 | Loss: 0.00001760
Iteration 100/1000 | Loss: 0.00001760
Iteration 101/1000 | Loss: 0.00001760
Iteration 102/1000 | Loss: 0.00001760
Iteration 103/1000 | Loss: 0.00001760
Iteration 104/1000 | Loss: 0.00001760
Iteration 105/1000 | Loss: 0.00001760
Iteration 106/1000 | Loss: 0.00001760
Iteration 107/1000 | Loss: 0.00001760
Iteration 108/1000 | Loss: 0.00001760
Iteration 109/1000 | Loss: 0.00001760
Iteration 110/1000 | Loss: 0.00001760
Iteration 111/1000 | Loss: 0.00001760
Iteration 112/1000 | Loss: 0.00001760
Iteration 113/1000 | Loss: 0.00001760
Iteration 114/1000 | Loss: 0.00001760
Iteration 115/1000 | Loss: 0.00001760
Iteration 116/1000 | Loss: 0.00001760
Iteration 117/1000 | Loss: 0.00001760
Iteration 118/1000 | Loss: 0.00001760
Iteration 119/1000 | Loss: 0.00001760
Iteration 120/1000 | Loss: 0.00001760
Iteration 121/1000 | Loss: 0.00001760
Iteration 122/1000 | Loss: 0.00001760
Iteration 123/1000 | Loss: 0.00001760
Iteration 124/1000 | Loss: 0.00001760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.7597980331629515e-05, 1.7597980331629515e-05, 1.7597980331629515e-05, 1.7597980331629515e-05, 1.7597980331629515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7597980331629515e-05

Optimization complete. Final v2v error: 3.506307363510132 mm

Highest mean error: 3.754319190979004 mm for frame 64

Lowest mean error: 3.2564704418182373 mm for frame 33

Saving results

Total time: 42.24654221534729
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382459
Iteration 2/25 | Loss: 0.00133506
Iteration 3/25 | Loss: 0.00120384
Iteration 4/25 | Loss: 0.00118626
Iteration 5/25 | Loss: 0.00117971
Iteration 6/25 | Loss: 0.00117835
Iteration 7/25 | Loss: 0.00117821
Iteration 8/25 | Loss: 0.00117821
Iteration 9/25 | Loss: 0.00117821
Iteration 10/25 | Loss: 0.00117821
Iteration 11/25 | Loss: 0.00117821
Iteration 12/25 | Loss: 0.00117821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011782108340412378, 0.0011782108340412378, 0.0011782108340412378, 0.0011782108340412378, 0.0011782108340412378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011782108340412378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42499948
Iteration 2/25 | Loss: 0.00050773
Iteration 3/25 | Loss: 0.00050773
Iteration 4/25 | Loss: 0.00050773
Iteration 5/25 | Loss: 0.00050773
Iteration 6/25 | Loss: 0.00050773
Iteration 7/25 | Loss: 0.00050772
Iteration 8/25 | Loss: 0.00050772
Iteration 9/25 | Loss: 0.00050772
Iteration 10/25 | Loss: 0.00050772
Iteration 11/25 | Loss: 0.00050772
Iteration 12/25 | Loss: 0.00050772
Iteration 13/25 | Loss: 0.00050772
Iteration 14/25 | Loss: 0.00050772
Iteration 15/25 | Loss: 0.00050772
Iteration 16/25 | Loss: 0.00050772
Iteration 17/25 | Loss: 0.00050772
Iteration 18/25 | Loss: 0.00050772
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005077230744063854, 0.0005077230744063854, 0.0005077230744063854, 0.0005077230744063854, 0.0005077230744063854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005077230744063854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050772
Iteration 2/1000 | Loss: 0.00004447
Iteration 3/1000 | Loss: 0.00003375
Iteration 4/1000 | Loss: 0.00002883
Iteration 5/1000 | Loss: 0.00002592
Iteration 6/1000 | Loss: 0.00002466
Iteration 7/1000 | Loss: 0.00002325
Iteration 8/1000 | Loss: 0.00002245
Iteration 9/1000 | Loss: 0.00002196
Iteration 10/1000 | Loss: 0.00002155
Iteration 11/1000 | Loss: 0.00002122
Iteration 12/1000 | Loss: 0.00002096
Iteration 13/1000 | Loss: 0.00002072
Iteration 14/1000 | Loss: 0.00002052
Iteration 15/1000 | Loss: 0.00002050
Iteration 16/1000 | Loss: 0.00002043
Iteration 17/1000 | Loss: 0.00002038
Iteration 18/1000 | Loss: 0.00002026
Iteration 19/1000 | Loss: 0.00002025
Iteration 20/1000 | Loss: 0.00002025
Iteration 21/1000 | Loss: 0.00002019
Iteration 22/1000 | Loss: 0.00002012
Iteration 23/1000 | Loss: 0.00002009
Iteration 24/1000 | Loss: 0.00002009
Iteration 25/1000 | Loss: 0.00002009
Iteration 26/1000 | Loss: 0.00002008
Iteration 27/1000 | Loss: 0.00002008
Iteration 28/1000 | Loss: 0.00002008
Iteration 29/1000 | Loss: 0.00002008
Iteration 30/1000 | Loss: 0.00002008
Iteration 31/1000 | Loss: 0.00002007
Iteration 32/1000 | Loss: 0.00002007
Iteration 33/1000 | Loss: 0.00002006
Iteration 34/1000 | Loss: 0.00002006
Iteration 35/1000 | Loss: 0.00002006
Iteration 36/1000 | Loss: 0.00002006
Iteration 37/1000 | Loss: 0.00002006
Iteration 38/1000 | Loss: 0.00002005
Iteration 39/1000 | Loss: 0.00002005
Iteration 40/1000 | Loss: 0.00002005
Iteration 41/1000 | Loss: 0.00002004
Iteration 42/1000 | Loss: 0.00002004
Iteration 43/1000 | Loss: 0.00002003
Iteration 44/1000 | Loss: 0.00002003
Iteration 45/1000 | Loss: 0.00002003
Iteration 46/1000 | Loss: 0.00002002
Iteration 47/1000 | Loss: 0.00002002
Iteration 48/1000 | Loss: 0.00002001
Iteration 49/1000 | Loss: 0.00002001
Iteration 50/1000 | Loss: 0.00002001
Iteration 51/1000 | Loss: 0.00002001
Iteration 52/1000 | Loss: 0.00002000
Iteration 53/1000 | Loss: 0.00002000
Iteration 54/1000 | Loss: 0.00001999
Iteration 55/1000 | Loss: 0.00001999
Iteration 56/1000 | Loss: 0.00001999
Iteration 57/1000 | Loss: 0.00001999
Iteration 58/1000 | Loss: 0.00001998
Iteration 59/1000 | Loss: 0.00001998
Iteration 60/1000 | Loss: 0.00001998
Iteration 61/1000 | Loss: 0.00001998
Iteration 62/1000 | Loss: 0.00001998
Iteration 63/1000 | Loss: 0.00001998
Iteration 64/1000 | Loss: 0.00001998
Iteration 65/1000 | Loss: 0.00001998
Iteration 66/1000 | Loss: 0.00001997
Iteration 67/1000 | Loss: 0.00001996
Iteration 68/1000 | Loss: 0.00001996
Iteration 69/1000 | Loss: 0.00001996
Iteration 70/1000 | Loss: 0.00001995
Iteration 71/1000 | Loss: 0.00001995
Iteration 72/1000 | Loss: 0.00001995
Iteration 73/1000 | Loss: 0.00001995
Iteration 74/1000 | Loss: 0.00001994
Iteration 75/1000 | Loss: 0.00001994
Iteration 76/1000 | Loss: 0.00001994
Iteration 77/1000 | Loss: 0.00001994
Iteration 78/1000 | Loss: 0.00001993
Iteration 79/1000 | Loss: 0.00001993
Iteration 80/1000 | Loss: 0.00001993
Iteration 81/1000 | Loss: 0.00001993
Iteration 82/1000 | Loss: 0.00001993
Iteration 83/1000 | Loss: 0.00001992
Iteration 84/1000 | Loss: 0.00001992
Iteration 85/1000 | Loss: 0.00001992
Iteration 86/1000 | Loss: 0.00001992
Iteration 87/1000 | Loss: 0.00001992
Iteration 88/1000 | Loss: 0.00001992
Iteration 89/1000 | Loss: 0.00001992
Iteration 90/1000 | Loss: 0.00001991
Iteration 91/1000 | Loss: 0.00001991
Iteration 92/1000 | Loss: 0.00001991
Iteration 93/1000 | Loss: 0.00001991
Iteration 94/1000 | Loss: 0.00001991
Iteration 95/1000 | Loss: 0.00001990
Iteration 96/1000 | Loss: 0.00001990
Iteration 97/1000 | Loss: 0.00001990
Iteration 98/1000 | Loss: 0.00001990
Iteration 99/1000 | Loss: 0.00001989
Iteration 100/1000 | Loss: 0.00001989
Iteration 101/1000 | Loss: 0.00001989
Iteration 102/1000 | Loss: 0.00001989
Iteration 103/1000 | Loss: 0.00001988
Iteration 104/1000 | Loss: 0.00001988
Iteration 105/1000 | Loss: 0.00001988
Iteration 106/1000 | Loss: 0.00001987
Iteration 107/1000 | Loss: 0.00001987
Iteration 108/1000 | Loss: 0.00001987
Iteration 109/1000 | Loss: 0.00001987
Iteration 110/1000 | Loss: 0.00001987
Iteration 111/1000 | Loss: 0.00001987
Iteration 112/1000 | Loss: 0.00001987
Iteration 113/1000 | Loss: 0.00001986
Iteration 114/1000 | Loss: 0.00001986
Iteration 115/1000 | Loss: 0.00001986
Iteration 116/1000 | Loss: 0.00001986
Iteration 117/1000 | Loss: 0.00001985
Iteration 118/1000 | Loss: 0.00001985
Iteration 119/1000 | Loss: 0.00001985
Iteration 120/1000 | Loss: 0.00001985
Iteration 121/1000 | Loss: 0.00001984
Iteration 122/1000 | Loss: 0.00001984
Iteration 123/1000 | Loss: 0.00001984
Iteration 124/1000 | Loss: 0.00001984
Iteration 125/1000 | Loss: 0.00001984
Iteration 126/1000 | Loss: 0.00001984
Iteration 127/1000 | Loss: 0.00001984
Iteration 128/1000 | Loss: 0.00001984
Iteration 129/1000 | Loss: 0.00001983
Iteration 130/1000 | Loss: 0.00001983
Iteration 131/1000 | Loss: 0.00001983
Iteration 132/1000 | Loss: 0.00001983
Iteration 133/1000 | Loss: 0.00001982
Iteration 134/1000 | Loss: 0.00001982
Iteration 135/1000 | Loss: 0.00001982
Iteration 136/1000 | Loss: 0.00001982
Iteration 137/1000 | Loss: 0.00001982
Iteration 138/1000 | Loss: 0.00001982
Iteration 139/1000 | Loss: 0.00001982
Iteration 140/1000 | Loss: 0.00001982
Iteration 141/1000 | Loss: 0.00001981
Iteration 142/1000 | Loss: 0.00001981
Iteration 143/1000 | Loss: 0.00001981
Iteration 144/1000 | Loss: 0.00001981
Iteration 145/1000 | Loss: 0.00001981
Iteration 146/1000 | Loss: 0.00001981
Iteration 147/1000 | Loss: 0.00001981
Iteration 148/1000 | Loss: 0.00001981
Iteration 149/1000 | Loss: 0.00001981
Iteration 150/1000 | Loss: 0.00001981
Iteration 151/1000 | Loss: 0.00001980
Iteration 152/1000 | Loss: 0.00001980
Iteration 153/1000 | Loss: 0.00001980
Iteration 154/1000 | Loss: 0.00001980
Iteration 155/1000 | Loss: 0.00001980
Iteration 156/1000 | Loss: 0.00001980
Iteration 157/1000 | Loss: 0.00001980
Iteration 158/1000 | Loss: 0.00001980
Iteration 159/1000 | Loss: 0.00001980
Iteration 160/1000 | Loss: 0.00001980
Iteration 161/1000 | Loss: 0.00001980
Iteration 162/1000 | Loss: 0.00001980
Iteration 163/1000 | Loss: 0.00001980
Iteration 164/1000 | Loss: 0.00001980
Iteration 165/1000 | Loss: 0.00001980
Iteration 166/1000 | Loss: 0.00001980
Iteration 167/1000 | Loss: 0.00001980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.9802489987341687e-05, 1.9802489987341687e-05, 1.9802489987341687e-05, 1.9802489987341687e-05, 1.9802489987341687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9802489987341687e-05

Optimization complete. Final v2v error: 3.7066450119018555 mm

Highest mean error: 4.310991287231445 mm for frame 55

Lowest mean error: 3.0806565284729004 mm for frame 11

Saving results

Total time: 44.84774470329285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836754
Iteration 2/25 | Loss: 0.00163402
Iteration 3/25 | Loss: 0.00131045
Iteration 4/25 | Loss: 0.00126699
Iteration 5/25 | Loss: 0.00125451
Iteration 6/25 | Loss: 0.00125206
Iteration 7/25 | Loss: 0.00125191
Iteration 8/25 | Loss: 0.00125191
Iteration 9/25 | Loss: 0.00125191
Iteration 10/25 | Loss: 0.00125191
Iteration 11/25 | Loss: 0.00125191
Iteration 12/25 | Loss: 0.00125191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012519119773060083, 0.0012519119773060083, 0.0012519119773060083, 0.0012519119773060083, 0.0012519119773060083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012519119773060083

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.77945352
Iteration 2/25 | Loss: 0.00062183
Iteration 3/25 | Loss: 0.00062182
Iteration 4/25 | Loss: 0.00062182
Iteration 5/25 | Loss: 0.00062182
Iteration 6/25 | Loss: 0.00062182
Iteration 7/25 | Loss: 0.00062182
Iteration 8/25 | Loss: 0.00062182
Iteration 9/25 | Loss: 0.00062182
Iteration 10/25 | Loss: 0.00062182
Iteration 11/25 | Loss: 0.00062182
Iteration 12/25 | Loss: 0.00062182
Iteration 13/25 | Loss: 0.00062182
Iteration 14/25 | Loss: 0.00062182
Iteration 15/25 | Loss: 0.00062182
Iteration 16/25 | Loss: 0.00062182
Iteration 17/25 | Loss: 0.00062182
Iteration 18/25 | Loss: 0.00062182
Iteration 19/25 | Loss: 0.00062182
Iteration 20/25 | Loss: 0.00062182
Iteration 21/25 | Loss: 0.00062182
Iteration 22/25 | Loss: 0.00062182
Iteration 23/25 | Loss: 0.00062182
Iteration 24/25 | Loss: 0.00062182
Iteration 25/25 | Loss: 0.00062182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062182
Iteration 2/1000 | Loss: 0.00003967
Iteration 3/1000 | Loss: 0.00003077
Iteration 4/1000 | Loss: 0.00002611
Iteration 5/1000 | Loss: 0.00002469
Iteration 6/1000 | Loss: 0.00002374
Iteration 7/1000 | Loss: 0.00002320
Iteration 8/1000 | Loss: 0.00002275
Iteration 9/1000 | Loss: 0.00002242
Iteration 10/1000 | Loss: 0.00002218
Iteration 11/1000 | Loss: 0.00002196
Iteration 12/1000 | Loss: 0.00002187
Iteration 13/1000 | Loss: 0.00002186
Iteration 14/1000 | Loss: 0.00002181
Iteration 15/1000 | Loss: 0.00002165
Iteration 16/1000 | Loss: 0.00002155
Iteration 17/1000 | Loss: 0.00002152
Iteration 18/1000 | Loss: 0.00002152
Iteration 19/1000 | Loss: 0.00002151
Iteration 20/1000 | Loss: 0.00002151
Iteration 21/1000 | Loss: 0.00002151
Iteration 22/1000 | Loss: 0.00002150
Iteration 23/1000 | Loss: 0.00002150
Iteration 24/1000 | Loss: 0.00002149
Iteration 25/1000 | Loss: 0.00002149
Iteration 26/1000 | Loss: 0.00002148
Iteration 27/1000 | Loss: 0.00002148
Iteration 28/1000 | Loss: 0.00002148
Iteration 29/1000 | Loss: 0.00002147
Iteration 30/1000 | Loss: 0.00002147
Iteration 31/1000 | Loss: 0.00002147
Iteration 32/1000 | Loss: 0.00002147
Iteration 33/1000 | Loss: 0.00002146
Iteration 34/1000 | Loss: 0.00002146
Iteration 35/1000 | Loss: 0.00002146
Iteration 36/1000 | Loss: 0.00002146
Iteration 37/1000 | Loss: 0.00002145
Iteration 38/1000 | Loss: 0.00002145
Iteration 39/1000 | Loss: 0.00002145
Iteration 40/1000 | Loss: 0.00002144
Iteration 41/1000 | Loss: 0.00002144
Iteration 42/1000 | Loss: 0.00002144
Iteration 43/1000 | Loss: 0.00002144
Iteration 44/1000 | Loss: 0.00002144
Iteration 45/1000 | Loss: 0.00002144
Iteration 46/1000 | Loss: 0.00002144
Iteration 47/1000 | Loss: 0.00002144
Iteration 48/1000 | Loss: 0.00002144
Iteration 49/1000 | Loss: 0.00002144
Iteration 50/1000 | Loss: 0.00002144
Iteration 51/1000 | Loss: 0.00002144
Iteration 52/1000 | Loss: 0.00002143
Iteration 53/1000 | Loss: 0.00002143
Iteration 54/1000 | Loss: 0.00002142
Iteration 55/1000 | Loss: 0.00002141
Iteration 56/1000 | Loss: 0.00002141
Iteration 57/1000 | Loss: 0.00002141
Iteration 58/1000 | Loss: 0.00002141
Iteration 59/1000 | Loss: 0.00002140
Iteration 60/1000 | Loss: 0.00002140
Iteration 61/1000 | Loss: 0.00002140
Iteration 62/1000 | Loss: 0.00002139
Iteration 63/1000 | Loss: 0.00002139
Iteration 64/1000 | Loss: 0.00002138
Iteration 65/1000 | Loss: 0.00002138
Iteration 66/1000 | Loss: 0.00002138
Iteration 67/1000 | Loss: 0.00002138
Iteration 68/1000 | Loss: 0.00002137
Iteration 69/1000 | Loss: 0.00002137
Iteration 70/1000 | Loss: 0.00002137
Iteration 71/1000 | Loss: 0.00002137
Iteration 72/1000 | Loss: 0.00002137
Iteration 73/1000 | Loss: 0.00002137
Iteration 74/1000 | Loss: 0.00002137
Iteration 75/1000 | Loss: 0.00002137
Iteration 76/1000 | Loss: 0.00002136
Iteration 77/1000 | Loss: 0.00002136
Iteration 78/1000 | Loss: 0.00002136
Iteration 79/1000 | Loss: 0.00002136
Iteration 80/1000 | Loss: 0.00002136
Iteration 81/1000 | Loss: 0.00002136
Iteration 82/1000 | Loss: 0.00002135
Iteration 83/1000 | Loss: 0.00002135
Iteration 84/1000 | Loss: 0.00002135
Iteration 85/1000 | Loss: 0.00002135
Iteration 86/1000 | Loss: 0.00002135
Iteration 87/1000 | Loss: 0.00002135
Iteration 88/1000 | Loss: 0.00002135
Iteration 89/1000 | Loss: 0.00002135
Iteration 90/1000 | Loss: 0.00002135
Iteration 91/1000 | Loss: 0.00002135
Iteration 92/1000 | Loss: 0.00002134
Iteration 93/1000 | Loss: 0.00002134
Iteration 94/1000 | Loss: 0.00002134
Iteration 95/1000 | Loss: 0.00002134
Iteration 96/1000 | Loss: 0.00002134
Iteration 97/1000 | Loss: 0.00002134
Iteration 98/1000 | Loss: 0.00002134
Iteration 99/1000 | Loss: 0.00002133
Iteration 100/1000 | Loss: 0.00002133
Iteration 101/1000 | Loss: 0.00002133
Iteration 102/1000 | Loss: 0.00002133
Iteration 103/1000 | Loss: 0.00002132
Iteration 104/1000 | Loss: 0.00002132
Iteration 105/1000 | Loss: 0.00002132
Iteration 106/1000 | Loss: 0.00002132
Iteration 107/1000 | Loss: 0.00002132
Iteration 108/1000 | Loss: 0.00002132
Iteration 109/1000 | Loss: 0.00002132
Iteration 110/1000 | Loss: 0.00002131
Iteration 111/1000 | Loss: 0.00002131
Iteration 112/1000 | Loss: 0.00002131
Iteration 113/1000 | Loss: 0.00002131
Iteration 114/1000 | Loss: 0.00002131
Iteration 115/1000 | Loss: 0.00002131
Iteration 116/1000 | Loss: 0.00002130
Iteration 117/1000 | Loss: 0.00002130
Iteration 118/1000 | Loss: 0.00002130
Iteration 119/1000 | Loss: 0.00002129
Iteration 120/1000 | Loss: 0.00002129
Iteration 121/1000 | Loss: 0.00002128
Iteration 122/1000 | Loss: 0.00002128
Iteration 123/1000 | Loss: 0.00002128
Iteration 124/1000 | Loss: 0.00002128
Iteration 125/1000 | Loss: 0.00002128
Iteration 126/1000 | Loss: 0.00002128
Iteration 127/1000 | Loss: 0.00002128
Iteration 128/1000 | Loss: 0.00002127
Iteration 129/1000 | Loss: 0.00002127
Iteration 130/1000 | Loss: 0.00002127
Iteration 131/1000 | Loss: 0.00002126
Iteration 132/1000 | Loss: 0.00002126
Iteration 133/1000 | Loss: 0.00002126
Iteration 134/1000 | Loss: 0.00002125
Iteration 135/1000 | Loss: 0.00002125
Iteration 136/1000 | Loss: 0.00002125
Iteration 137/1000 | Loss: 0.00002125
Iteration 138/1000 | Loss: 0.00002124
Iteration 139/1000 | Loss: 0.00002124
Iteration 140/1000 | Loss: 0.00002124
Iteration 141/1000 | Loss: 0.00002124
Iteration 142/1000 | Loss: 0.00002124
Iteration 143/1000 | Loss: 0.00002124
Iteration 144/1000 | Loss: 0.00002124
Iteration 145/1000 | Loss: 0.00002124
Iteration 146/1000 | Loss: 0.00002124
Iteration 147/1000 | Loss: 0.00002123
Iteration 148/1000 | Loss: 0.00002123
Iteration 149/1000 | Loss: 0.00002123
Iteration 150/1000 | Loss: 0.00002123
Iteration 151/1000 | Loss: 0.00002123
Iteration 152/1000 | Loss: 0.00002123
Iteration 153/1000 | Loss: 0.00002123
Iteration 154/1000 | Loss: 0.00002123
Iteration 155/1000 | Loss: 0.00002123
Iteration 156/1000 | Loss: 0.00002123
Iteration 157/1000 | Loss: 0.00002123
Iteration 158/1000 | Loss: 0.00002123
Iteration 159/1000 | Loss: 0.00002123
Iteration 160/1000 | Loss: 0.00002123
Iteration 161/1000 | Loss: 0.00002123
Iteration 162/1000 | Loss: 0.00002123
Iteration 163/1000 | Loss: 0.00002123
Iteration 164/1000 | Loss: 0.00002123
Iteration 165/1000 | Loss: 0.00002123
Iteration 166/1000 | Loss: 0.00002123
Iteration 167/1000 | Loss: 0.00002123
Iteration 168/1000 | Loss: 0.00002123
Iteration 169/1000 | Loss: 0.00002123
Iteration 170/1000 | Loss: 0.00002123
Iteration 171/1000 | Loss: 0.00002123
Iteration 172/1000 | Loss: 0.00002123
Iteration 173/1000 | Loss: 0.00002123
Iteration 174/1000 | Loss: 0.00002123
Iteration 175/1000 | Loss: 0.00002123
Iteration 176/1000 | Loss: 0.00002123
Iteration 177/1000 | Loss: 0.00002123
Iteration 178/1000 | Loss: 0.00002123
Iteration 179/1000 | Loss: 0.00002123
Iteration 180/1000 | Loss: 0.00002123
Iteration 181/1000 | Loss: 0.00002123
Iteration 182/1000 | Loss: 0.00002123
Iteration 183/1000 | Loss: 0.00002123
Iteration 184/1000 | Loss: 0.00002123
Iteration 185/1000 | Loss: 0.00002123
Iteration 186/1000 | Loss: 0.00002123
Iteration 187/1000 | Loss: 0.00002123
Iteration 188/1000 | Loss: 0.00002123
Iteration 189/1000 | Loss: 0.00002123
Iteration 190/1000 | Loss: 0.00002123
Iteration 191/1000 | Loss: 0.00002123
Iteration 192/1000 | Loss: 0.00002123
Iteration 193/1000 | Loss: 0.00002123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [2.1229940102784894e-05, 2.1229940102784894e-05, 2.1229940102784894e-05, 2.1229940102784894e-05, 2.1229940102784894e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1229940102784894e-05

Optimization complete. Final v2v error: 3.7996726036071777 mm

Highest mean error: 4.133997917175293 mm for frame 93

Lowest mean error: 3.3988306522369385 mm for frame 137

Saving results

Total time: 41.74750089645386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838391
Iteration 2/25 | Loss: 0.00131042
Iteration 3/25 | Loss: 0.00122589
Iteration 4/25 | Loss: 0.00121574
Iteration 5/25 | Loss: 0.00121216
Iteration 6/25 | Loss: 0.00121198
Iteration 7/25 | Loss: 0.00121198
Iteration 8/25 | Loss: 0.00121198
Iteration 9/25 | Loss: 0.00121198
Iteration 10/25 | Loss: 0.00121198
Iteration 11/25 | Loss: 0.00121198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001211980707012117, 0.001211980707012117, 0.001211980707012117, 0.001211980707012117, 0.001211980707012117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001211980707012117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.28093243
Iteration 2/25 | Loss: 0.00069174
Iteration 3/25 | Loss: 0.00069173
Iteration 4/25 | Loss: 0.00069173
Iteration 5/25 | Loss: 0.00069173
Iteration 6/25 | Loss: 0.00069173
Iteration 7/25 | Loss: 0.00069173
Iteration 8/25 | Loss: 0.00069173
Iteration 9/25 | Loss: 0.00069173
Iteration 10/25 | Loss: 0.00069173
Iteration 11/25 | Loss: 0.00069173
Iteration 12/25 | Loss: 0.00069173
Iteration 13/25 | Loss: 0.00069173
Iteration 14/25 | Loss: 0.00069173
Iteration 15/25 | Loss: 0.00069173
Iteration 16/25 | Loss: 0.00069173
Iteration 17/25 | Loss: 0.00069173
Iteration 18/25 | Loss: 0.00069173
Iteration 19/25 | Loss: 0.00069173
Iteration 20/25 | Loss: 0.00069173
Iteration 21/25 | Loss: 0.00069173
Iteration 22/25 | Loss: 0.00069173
Iteration 23/25 | Loss: 0.00069173
Iteration 24/25 | Loss: 0.00069173
Iteration 25/25 | Loss: 0.00069173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069173
Iteration 2/1000 | Loss: 0.00003311
Iteration 3/1000 | Loss: 0.00002283
Iteration 4/1000 | Loss: 0.00001926
Iteration 5/1000 | Loss: 0.00001844
Iteration 6/1000 | Loss: 0.00001780
Iteration 7/1000 | Loss: 0.00001742
Iteration 8/1000 | Loss: 0.00001693
Iteration 9/1000 | Loss: 0.00001667
Iteration 10/1000 | Loss: 0.00001644
Iteration 11/1000 | Loss: 0.00001624
Iteration 12/1000 | Loss: 0.00001613
Iteration 13/1000 | Loss: 0.00001600
Iteration 14/1000 | Loss: 0.00001593
Iteration 15/1000 | Loss: 0.00001592
Iteration 16/1000 | Loss: 0.00001590
Iteration 17/1000 | Loss: 0.00001583
Iteration 18/1000 | Loss: 0.00001582
Iteration 19/1000 | Loss: 0.00001581
Iteration 20/1000 | Loss: 0.00001581
Iteration 21/1000 | Loss: 0.00001580
Iteration 22/1000 | Loss: 0.00001573
Iteration 23/1000 | Loss: 0.00001572
Iteration 24/1000 | Loss: 0.00001571
Iteration 25/1000 | Loss: 0.00001570
Iteration 26/1000 | Loss: 0.00001569
Iteration 27/1000 | Loss: 0.00001569
Iteration 28/1000 | Loss: 0.00001568
Iteration 29/1000 | Loss: 0.00001567
Iteration 30/1000 | Loss: 0.00001567
Iteration 31/1000 | Loss: 0.00001567
Iteration 32/1000 | Loss: 0.00001566
Iteration 33/1000 | Loss: 0.00001566
Iteration 34/1000 | Loss: 0.00001565
Iteration 35/1000 | Loss: 0.00001564
Iteration 36/1000 | Loss: 0.00001563
Iteration 37/1000 | Loss: 0.00001563
Iteration 38/1000 | Loss: 0.00001561
Iteration 39/1000 | Loss: 0.00001556
Iteration 40/1000 | Loss: 0.00001556
Iteration 41/1000 | Loss: 0.00001554
Iteration 42/1000 | Loss: 0.00001553
Iteration 43/1000 | Loss: 0.00001553
Iteration 44/1000 | Loss: 0.00001552
Iteration 45/1000 | Loss: 0.00001552
Iteration 46/1000 | Loss: 0.00001552
Iteration 47/1000 | Loss: 0.00001551
Iteration 48/1000 | Loss: 0.00001551
Iteration 49/1000 | Loss: 0.00001551
Iteration 50/1000 | Loss: 0.00001550
Iteration 51/1000 | Loss: 0.00001549
Iteration 52/1000 | Loss: 0.00001549
Iteration 53/1000 | Loss: 0.00001549
Iteration 54/1000 | Loss: 0.00001549
Iteration 55/1000 | Loss: 0.00001549
Iteration 56/1000 | Loss: 0.00001548
Iteration 57/1000 | Loss: 0.00001548
Iteration 58/1000 | Loss: 0.00001548
Iteration 59/1000 | Loss: 0.00001548
Iteration 60/1000 | Loss: 0.00001547
Iteration 61/1000 | Loss: 0.00001547
Iteration 62/1000 | Loss: 0.00001546
Iteration 63/1000 | Loss: 0.00001546
Iteration 64/1000 | Loss: 0.00001546
Iteration 65/1000 | Loss: 0.00001546
Iteration 66/1000 | Loss: 0.00001546
Iteration 67/1000 | Loss: 0.00001546
Iteration 68/1000 | Loss: 0.00001545
Iteration 69/1000 | Loss: 0.00001545
Iteration 70/1000 | Loss: 0.00001545
Iteration 71/1000 | Loss: 0.00001545
Iteration 72/1000 | Loss: 0.00001545
Iteration 73/1000 | Loss: 0.00001545
Iteration 74/1000 | Loss: 0.00001544
Iteration 75/1000 | Loss: 0.00001544
Iteration 76/1000 | Loss: 0.00001544
Iteration 77/1000 | Loss: 0.00001543
Iteration 78/1000 | Loss: 0.00001543
Iteration 79/1000 | Loss: 0.00001543
Iteration 80/1000 | Loss: 0.00001543
Iteration 81/1000 | Loss: 0.00001543
Iteration 82/1000 | Loss: 0.00001543
Iteration 83/1000 | Loss: 0.00001543
Iteration 84/1000 | Loss: 0.00001542
Iteration 85/1000 | Loss: 0.00001542
Iteration 86/1000 | Loss: 0.00001542
Iteration 87/1000 | Loss: 0.00001542
Iteration 88/1000 | Loss: 0.00001541
Iteration 89/1000 | Loss: 0.00001541
Iteration 90/1000 | Loss: 0.00001541
Iteration 91/1000 | Loss: 0.00001541
Iteration 92/1000 | Loss: 0.00001540
Iteration 93/1000 | Loss: 0.00001540
Iteration 94/1000 | Loss: 0.00001540
Iteration 95/1000 | Loss: 0.00001540
Iteration 96/1000 | Loss: 0.00001540
Iteration 97/1000 | Loss: 0.00001539
Iteration 98/1000 | Loss: 0.00001539
Iteration 99/1000 | Loss: 0.00001539
Iteration 100/1000 | Loss: 0.00001538
Iteration 101/1000 | Loss: 0.00001538
Iteration 102/1000 | Loss: 0.00001538
Iteration 103/1000 | Loss: 0.00001538
Iteration 104/1000 | Loss: 0.00001538
Iteration 105/1000 | Loss: 0.00001538
Iteration 106/1000 | Loss: 0.00001538
Iteration 107/1000 | Loss: 0.00001538
Iteration 108/1000 | Loss: 0.00001538
Iteration 109/1000 | Loss: 0.00001538
Iteration 110/1000 | Loss: 0.00001538
Iteration 111/1000 | Loss: 0.00001538
Iteration 112/1000 | Loss: 0.00001538
Iteration 113/1000 | Loss: 0.00001538
Iteration 114/1000 | Loss: 0.00001538
Iteration 115/1000 | Loss: 0.00001538
Iteration 116/1000 | Loss: 0.00001538
Iteration 117/1000 | Loss: 0.00001538
Iteration 118/1000 | Loss: 0.00001538
Iteration 119/1000 | Loss: 0.00001538
Iteration 120/1000 | Loss: 0.00001538
Iteration 121/1000 | Loss: 0.00001538
Iteration 122/1000 | Loss: 0.00001538
Iteration 123/1000 | Loss: 0.00001538
Iteration 124/1000 | Loss: 0.00001538
Iteration 125/1000 | Loss: 0.00001538
Iteration 126/1000 | Loss: 0.00001538
Iteration 127/1000 | Loss: 0.00001538
Iteration 128/1000 | Loss: 0.00001538
Iteration 129/1000 | Loss: 0.00001538
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.537760726932902e-05, 1.537760726932902e-05, 1.537760726932902e-05, 1.537760726932902e-05, 1.537760726932902e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.537760726932902e-05

Optimization complete. Final v2v error: 3.2799618244171143 mm

Highest mean error: 3.7917966842651367 mm for frame 63

Lowest mean error: 2.9165990352630615 mm for frame 140

Saving results

Total time: 38.68397545814514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400202
Iteration 2/25 | Loss: 0.00124066
Iteration 3/25 | Loss: 0.00115777
Iteration 4/25 | Loss: 0.00115222
Iteration 5/25 | Loss: 0.00115222
Iteration 6/25 | Loss: 0.00115222
Iteration 7/25 | Loss: 0.00115222
Iteration 8/25 | Loss: 0.00115222
Iteration 9/25 | Loss: 0.00115222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.001152222859673202, 0.001152222859673202, 0.001152222859673202, 0.001152222859673202, 0.001152222859673202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001152222859673202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45702505
Iteration 2/25 | Loss: 0.00053929
Iteration 3/25 | Loss: 0.00053928
Iteration 4/25 | Loss: 0.00053928
Iteration 5/25 | Loss: 0.00053928
Iteration 6/25 | Loss: 0.00053928
Iteration 7/25 | Loss: 0.00053928
Iteration 8/25 | Loss: 0.00053928
Iteration 9/25 | Loss: 0.00053928
Iteration 10/25 | Loss: 0.00053928
Iteration 11/25 | Loss: 0.00053928
Iteration 12/25 | Loss: 0.00053928
Iteration 13/25 | Loss: 0.00053928
Iteration 14/25 | Loss: 0.00053928
Iteration 15/25 | Loss: 0.00053928
Iteration 16/25 | Loss: 0.00053928
Iteration 17/25 | Loss: 0.00053928
Iteration 18/25 | Loss: 0.00053928
Iteration 19/25 | Loss: 0.00053928
Iteration 20/25 | Loss: 0.00053928
Iteration 21/25 | Loss: 0.00053928
Iteration 22/25 | Loss: 0.00053928
Iteration 23/25 | Loss: 0.00053928
Iteration 24/25 | Loss: 0.00053928
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005392784951254725, 0.0005392784951254725, 0.0005392784951254725, 0.0005392784951254725, 0.0005392784951254725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005392784951254725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053928
Iteration 2/1000 | Loss: 0.00002941
Iteration 3/1000 | Loss: 0.00001929
Iteration 4/1000 | Loss: 0.00001760
Iteration 5/1000 | Loss: 0.00001635
Iteration 6/1000 | Loss: 0.00001559
Iteration 7/1000 | Loss: 0.00001517
Iteration 8/1000 | Loss: 0.00001481
Iteration 9/1000 | Loss: 0.00001451
Iteration 10/1000 | Loss: 0.00001444
Iteration 11/1000 | Loss: 0.00001430
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00001415
Iteration 14/1000 | Loss: 0.00001411
Iteration 15/1000 | Loss: 0.00001405
Iteration 16/1000 | Loss: 0.00001400
Iteration 17/1000 | Loss: 0.00001398
Iteration 18/1000 | Loss: 0.00001396
Iteration 19/1000 | Loss: 0.00001392
Iteration 20/1000 | Loss: 0.00001392
Iteration 21/1000 | Loss: 0.00001392
Iteration 22/1000 | Loss: 0.00001392
Iteration 23/1000 | Loss: 0.00001391
Iteration 24/1000 | Loss: 0.00001391
Iteration 25/1000 | Loss: 0.00001390
Iteration 26/1000 | Loss: 0.00001390
Iteration 27/1000 | Loss: 0.00001387
Iteration 28/1000 | Loss: 0.00001386
Iteration 29/1000 | Loss: 0.00001385
Iteration 30/1000 | Loss: 0.00001377
Iteration 31/1000 | Loss: 0.00001371
Iteration 32/1000 | Loss: 0.00001369
Iteration 33/1000 | Loss: 0.00001368
Iteration 34/1000 | Loss: 0.00001368
Iteration 35/1000 | Loss: 0.00001363
Iteration 36/1000 | Loss: 0.00001362
Iteration 37/1000 | Loss: 0.00001360
Iteration 38/1000 | Loss: 0.00001355
Iteration 39/1000 | Loss: 0.00001351
Iteration 40/1000 | Loss: 0.00001349
Iteration 41/1000 | Loss: 0.00001349
Iteration 42/1000 | Loss: 0.00001347
Iteration 43/1000 | Loss: 0.00001346
Iteration 44/1000 | Loss: 0.00001344
Iteration 45/1000 | Loss: 0.00001344
Iteration 46/1000 | Loss: 0.00001344
Iteration 47/1000 | Loss: 0.00001344
Iteration 48/1000 | Loss: 0.00001344
Iteration 49/1000 | Loss: 0.00001343
Iteration 50/1000 | Loss: 0.00001343
Iteration 51/1000 | Loss: 0.00001343
Iteration 52/1000 | Loss: 0.00001343
Iteration 53/1000 | Loss: 0.00001343
Iteration 54/1000 | Loss: 0.00001343
Iteration 55/1000 | Loss: 0.00001343
Iteration 56/1000 | Loss: 0.00001342
Iteration 57/1000 | Loss: 0.00001341
Iteration 58/1000 | Loss: 0.00001341
Iteration 59/1000 | Loss: 0.00001341
Iteration 60/1000 | Loss: 0.00001341
Iteration 61/1000 | Loss: 0.00001340
Iteration 62/1000 | Loss: 0.00001340
Iteration 63/1000 | Loss: 0.00001340
Iteration 64/1000 | Loss: 0.00001340
Iteration 65/1000 | Loss: 0.00001340
Iteration 66/1000 | Loss: 0.00001339
Iteration 67/1000 | Loss: 0.00001339
Iteration 68/1000 | Loss: 0.00001338
Iteration 69/1000 | Loss: 0.00001338
Iteration 70/1000 | Loss: 0.00001337
Iteration 71/1000 | Loss: 0.00001337
Iteration 72/1000 | Loss: 0.00001337
Iteration 73/1000 | Loss: 0.00001337
Iteration 74/1000 | Loss: 0.00001337
Iteration 75/1000 | Loss: 0.00001337
Iteration 76/1000 | Loss: 0.00001336
Iteration 77/1000 | Loss: 0.00001336
Iteration 78/1000 | Loss: 0.00001336
Iteration 79/1000 | Loss: 0.00001336
Iteration 80/1000 | Loss: 0.00001336
Iteration 81/1000 | Loss: 0.00001336
Iteration 82/1000 | Loss: 0.00001335
Iteration 83/1000 | Loss: 0.00001335
Iteration 84/1000 | Loss: 0.00001334
Iteration 85/1000 | Loss: 0.00001334
Iteration 86/1000 | Loss: 0.00001333
Iteration 87/1000 | Loss: 0.00001332
Iteration 88/1000 | Loss: 0.00001331
Iteration 89/1000 | Loss: 0.00001331
Iteration 90/1000 | Loss: 0.00001330
Iteration 91/1000 | Loss: 0.00001330
Iteration 92/1000 | Loss: 0.00001329
Iteration 93/1000 | Loss: 0.00001329
Iteration 94/1000 | Loss: 0.00001324
Iteration 95/1000 | Loss: 0.00001324
Iteration 96/1000 | Loss: 0.00001324
Iteration 97/1000 | Loss: 0.00001323
Iteration 98/1000 | Loss: 0.00001323
Iteration 99/1000 | Loss: 0.00001322
Iteration 100/1000 | Loss: 0.00001322
Iteration 101/1000 | Loss: 0.00001322
Iteration 102/1000 | Loss: 0.00001322
Iteration 103/1000 | Loss: 0.00001322
Iteration 104/1000 | Loss: 0.00001321
Iteration 105/1000 | Loss: 0.00001321
Iteration 106/1000 | Loss: 0.00001321
Iteration 107/1000 | Loss: 0.00001320
Iteration 108/1000 | Loss: 0.00001320
Iteration 109/1000 | Loss: 0.00001320
Iteration 110/1000 | Loss: 0.00001320
Iteration 111/1000 | Loss: 0.00001319
Iteration 112/1000 | Loss: 0.00001319
Iteration 113/1000 | Loss: 0.00001319
Iteration 114/1000 | Loss: 0.00001319
Iteration 115/1000 | Loss: 0.00001318
Iteration 116/1000 | Loss: 0.00001318
Iteration 117/1000 | Loss: 0.00001318
Iteration 118/1000 | Loss: 0.00001318
Iteration 119/1000 | Loss: 0.00001318
Iteration 120/1000 | Loss: 0.00001318
Iteration 121/1000 | Loss: 0.00001318
Iteration 122/1000 | Loss: 0.00001318
Iteration 123/1000 | Loss: 0.00001318
Iteration 124/1000 | Loss: 0.00001318
Iteration 125/1000 | Loss: 0.00001318
Iteration 126/1000 | Loss: 0.00001318
Iteration 127/1000 | Loss: 0.00001318
Iteration 128/1000 | Loss: 0.00001318
Iteration 129/1000 | Loss: 0.00001318
Iteration 130/1000 | Loss: 0.00001318
Iteration 131/1000 | Loss: 0.00001318
Iteration 132/1000 | Loss: 0.00001318
Iteration 133/1000 | Loss: 0.00001318
Iteration 134/1000 | Loss: 0.00001318
Iteration 135/1000 | Loss: 0.00001317
Iteration 136/1000 | Loss: 0.00001317
Iteration 137/1000 | Loss: 0.00001317
Iteration 138/1000 | Loss: 0.00001317
Iteration 139/1000 | Loss: 0.00001317
Iteration 140/1000 | Loss: 0.00001317
Iteration 141/1000 | Loss: 0.00001317
Iteration 142/1000 | Loss: 0.00001317
Iteration 143/1000 | Loss: 0.00001317
Iteration 144/1000 | Loss: 0.00001317
Iteration 145/1000 | Loss: 0.00001317
Iteration 146/1000 | Loss: 0.00001317
Iteration 147/1000 | Loss: 0.00001317
Iteration 148/1000 | Loss: 0.00001317
Iteration 149/1000 | Loss: 0.00001317
Iteration 150/1000 | Loss: 0.00001317
Iteration 151/1000 | Loss: 0.00001317
Iteration 152/1000 | Loss: 0.00001317
Iteration 153/1000 | Loss: 0.00001317
Iteration 154/1000 | Loss: 0.00001317
Iteration 155/1000 | Loss: 0.00001317
Iteration 156/1000 | Loss: 0.00001317
Iteration 157/1000 | Loss: 0.00001317
Iteration 158/1000 | Loss: 0.00001317
Iteration 159/1000 | Loss: 0.00001317
Iteration 160/1000 | Loss: 0.00001317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.3171388673072215e-05, 1.3171388673072215e-05, 1.3171388673072215e-05, 1.3171388673072215e-05, 1.3171388673072215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3171388673072215e-05

Optimization complete. Final v2v error: 3.089052677154541 mm

Highest mean error: 3.359178066253662 mm for frame 159

Lowest mean error: 2.896751880645752 mm for frame 257

Saving results

Total time: 46.25183439254761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041355
Iteration 2/25 | Loss: 0.01041355
Iteration 3/25 | Loss: 0.01041355
Iteration 4/25 | Loss: 0.01041355
Iteration 5/25 | Loss: 0.01041355
Iteration 6/25 | Loss: 0.01041355
Iteration 7/25 | Loss: 0.01041355
Iteration 8/25 | Loss: 0.01041355
Iteration 9/25 | Loss: 0.01041355
Iteration 10/25 | Loss: 0.01041354
Iteration 11/25 | Loss: 0.01041354
Iteration 12/25 | Loss: 0.01041354
Iteration 13/25 | Loss: 0.01041354
Iteration 14/25 | Loss: 0.01041354
Iteration 15/25 | Loss: 0.01041354
Iteration 16/25 | Loss: 0.01041354
Iteration 17/25 | Loss: 0.01041354
Iteration 18/25 | Loss: 0.01041354
Iteration 19/25 | Loss: 0.01041354
Iteration 20/25 | Loss: 0.01041354
Iteration 21/25 | Loss: 0.01041353
Iteration 22/25 | Loss: 0.01041353
Iteration 23/25 | Loss: 0.01041353
Iteration 24/25 | Loss: 0.01041353
Iteration 25/25 | Loss: 0.01041353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03553057
Iteration 2/25 | Loss: 0.05534097
Iteration 3/25 | Loss: 0.05533316
Iteration 4/25 | Loss: 0.05533315
Iteration 5/25 | Loss: 0.05533315
Iteration 6/25 | Loss: 0.05533315
Iteration 7/25 | Loss: 0.05533315
Iteration 8/25 | Loss: 0.05533316
Iteration 9/25 | Loss: 0.05533316
Iteration 10/25 | Loss: 0.05533315
Iteration 11/25 | Loss: 0.05533315
Iteration 12/25 | Loss: 0.05533315
Iteration 13/25 | Loss: 0.05533315
Iteration 14/25 | Loss: 0.05533314
Iteration 15/25 | Loss: 0.05533314
Iteration 16/25 | Loss: 0.05533314
Iteration 17/25 | Loss: 0.05533314
Iteration 18/25 | Loss: 0.05533314
Iteration 19/25 | Loss: 0.05533314
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.05533314496278763, 0.05533314496278763, 0.05533314496278763, 0.05533314496278763, 0.05533314496278763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.05533314496278763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05533315
Iteration 2/1000 | Loss: 0.00693483
Iteration 3/1000 | Loss: 0.00207236
Iteration 4/1000 | Loss: 0.00043909
Iteration 5/1000 | Loss: 0.00107491
Iteration 6/1000 | Loss: 0.00461022
Iteration 7/1000 | Loss: 0.00056910
Iteration 8/1000 | Loss: 0.00626359
Iteration 9/1000 | Loss: 0.00052202
Iteration 10/1000 | Loss: 0.00052263
Iteration 11/1000 | Loss: 0.00052645
Iteration 12/1000 | Loss: 0.00039345
Iteration 13/1000 | Loss: 0.00012062
Iteration 14/1000 | Loss: 0.00014640
Iteration 15/1000 | Loss: 0.00092380
Iteration 16/1000 | Loss: 0.00065176
Iteration 17/1000 | Loss: 0.00010627
Iteration 18/1000 | Loss: 0.00041740
Iteration 19/1000 | Loss: 0.00004584
Iteration 20/1000 | Loss: 0.00004536
Iteration 21/1000 | Loss: 0.00026129
Iteration 22/1000 | Loss: 0.00029733
Iteration 23/1000 | Loss: 0.00010772
Iteration 24/1000 | Loss: 0.00005365
Iteration 25/1000 | Loss: 0.00005261
Iteration 26/1000 | Loss: 0.00041034
Iteration 27/1000 | Loss: 0.00003394
Iteration 28/1000 | Loss: 0.00018498
Iteration 29/1000 | Loss: 0.00027611
Iteration 30/1000 | Loss: 0.00020277
Iteration 31/1000 | Loss: 0.00006536
Iteration 32/1000 | Loss: 0.00142059
Iteration 33/1000 | Loss: 0.00014477
Iteration 34/1000 | Loss: 0.00003778
Iteration 35/1000 | Loss: 0.00002709
Iteration 36/1000 | Loss: 0.00039252
Iteration 37/1000 | Loss: 0.00010466
Iteration 38/1000 | Loss: 0.00005597
Iteration 39/1000 | Loss: 0.00100619
Iteration 40/1000 | Loss: 0.00005614
Iteration 41/1000 | Loss: 0.00002902
Iteration 42/1000 | Loss: 0.00014009
Iteration 43/1000 | Loss: 0.00008168
Iteration 44/1000 | Loss: 0.00002666
Iteration 45/1000 | Loss: 0.00002285
Iteration 46/1000 | Loss: 0.00014656
Iteration 47/1000 | Loss: 0.00002855
Iteration 48/1000 | Loss: 0.00007837
Iteration 49/1000 | Loss: 0.00010177
Iteration 50/1000 | Loss: 0.00003663
Iteration 51/1000 | Loss: 0.00011301
Iteration 52/1000 | Loss: 0.00013000
Iteration 53/1000 | Loss: 0.00007681
Iteration 54/1000 | Loss: 0.00017651
Iteration 55/1000 | Loss: 0.00010413
Iteration 56/1000 | Loss: 0.00002120
Iteration 57/1000 | Loss: 0.00002081
Iteration 58/1000 | Loss: 0.00006534
Iteration 59/1000 | Loss: 0.00005044
Iteration 60/1000 | Loss: 0.00002029
Iteration 61/1000 | Loss: 0.00016827
Iteration 62/1000 | Loss: 0.00012227
Iteration 63/1000 | Loss: 0.00007506
Iteration 64/1000 | Loss: 0.00002812
Iteration 65/1000 | Loss: 0.00013066
Iteration 66/1000 | Loss: 0.00205711
Iteration 67/1000 | Loss: 0.00013263
Iteration 68/1000 | Loss: 0.00040356
Iteration 69/1000 | Loss: 0.00004774
Iteration 70/1000 | Loss: 0.00002447
Iteration 71/1000 | Loss: 0.00006424
Iteration 72/1000 | Loss: 0.00019357
Iteration 73/1000 | Loss: 0.00002310
Iteration 74/1000 | Loss: 0.00003144
Iteration 75/1000 | Loss: 0.00006576
Iteration 76/1000 | Loss: 0.00002014
Iteration 77/1000 | Loss: 0.00002018
Iteration 78/1000 | Loss: 0.00002087
Iteration 79/1000 | Loss: 0.00006895
Iteration 80/1000 | Loss: 0.00001885
Iteration 81/1000 | Loss: 0.00002396
Iteration 82/1000 | Loss: 0.00017420
Iteration 83/1000 | Loss: 0.00003493
Iteration 84/1000 | Loss: 0.00002301
Iteration 85/1000 | Loss: 0.00002421
Iteration 86/1000 | Loss: 0.00002031
Iteration 87/1000 | Loss: 0.00003131
Iteration 88/1000 | Loss: 0.00001964
Iteration 89/1000 | Loss: 0.00001942
Iteration 90/1000 | Loss: 0.00001860
Iteration 91/1000 | Loss: 0.00001859
Iteration 92/1000 | Loss: 0.00001859
Iteration 93/1000 | Loss: 0.00001859
Iteration 94/1000 | Loss: 0.00001859
Iteration 95/1000 | Loss: 0.00001859
Iteration 96/1000 | Loss: 0.00001859
Iteration 97/1000 | Loss: 0.00001859
Iteration 98/1000 | Loss: 0.00001859
Iteration 99/1000 | Loss: 0.00001858
Iteration 100/1000 | Loss: 0.00001858
Iteration 101/1000 | Loss: 0.00005035
Iteration 102/1000 | Loss: 0.00001857
Iteration 103/1000 | Loss: 0.00001853
Iteration 104/1000 | Loss: 0.00001853
Iteration 105/1000 | Loss: 0.00001853
Iteration 106/1000 | Loss: 0.00001853
Iteration 107/1000 | Loss: 0.00001853
Iteration 108/1000 | Loss: 0.00001853
Iteration 109/1000 | Loss: 0.00001852
Iteration 110/1000 | Loss: 0.00001852
Iteration 111/1000 | Loss: 0.00001852
Iteration 112/1000 | Loss: 0.00001852
Iteration 113/1000 | Loss: 0.00001852
Iteration 114/1000 | Loss: 0.00001852
Iteration 115/1000 | Loss: 0.00002053
Iteration 116/1000 | Loss: 0.00006357
Iteration 117/1000 | Loss: 0.00008971
Iteration 118/1000 | Loss: 0.00002445
Iteration 119/1000 | Loss: 0.00001852
Iteration 120/1000 | Loss: 0.00002298
Iteration 121/1000 | Loss: 0.00001846
Iteration 122/1000 | Loss: 0.00001846
Iteration 123/1000 | Loss: 0.00001846
Iteration 124/1000 | Loss: 0.00001846
Iteration 125/1000 | Loss: 0.00001846
Iteration 126/1000 | Loss: 0.00001846
Iteration 127/1000 | Loss: 0.00001845
Iteration 128/1000 | Loss: 0.00001845
Iteration 129/1000 | Loss: 0.00001844
Iteration 130/1000 | Loss: 0.00001844
Iteration 131/1000 | Loss: 0.00001843
Iteration 132/1000 | Loss: 0.00001843
Iteration 133/1000 | Loss: 0.00001842
Iteration 134/1000 | Loss: 0.00001842
Iteration 135/1000 | Loss: 0.00001842
Iteration 136/1000 | Loss: 0.00001842
Iteration 137/1000 | Loss: 0.00001842
Iteration 138/1000 | Loss: 0.00001842
Iteration 139/1000 | Loss: 0.00001842
Iteration 140/1000 | Loss: 0.00001841
Iteration 141/1000 | Loss: 0.00001841
Iteration 142/1000 | Loss: 0.00001841
Iteration 143/1000 | Loss: 0.00001840
Iteration 144/1000 | Loss: 0.00001840
Iteration 145/1000 | Loss: 0.00001840
Iteration 146/1000 | Loss: 0.00001840
Iteration 147/1000 | Loss: 0.00001839
Iteration 148/1000 | Loss: 0.00001839
Iteration 149/1000 | Loss: 0.00004756
Iteration 150/1000 | Loss: 0.00002600
Iteration 151/1000 | Loss: 0.00001841
Iteration 152/1000 | Loss: 0.00001840
Iteration 153/1000 | Loss: 0.00001840
Iteration 154/1000 | Loss: 0.00001840
Iteration 155/1000 | Loss: 0.00001839
Iteration 156/1000 | Loss: 0.00001838
Iteration 157/1000 | Loss: 0.00001837
Iteration 158/1000 | Loss: 0.00001837
Iteration 159/1000 | Loss: 0.00001837
Iteration 160/1000 | Loss: 0.00001837
Iteration 161/1000 | Loss: 0.00001837
Iteration 162/1000 | Loss: 0.00001837
Iteration 163/1000 | Loss: 0.00001837
Iteration 164/1000 | Loss: 0.00001837
Iteration 165/1000 | Loss: 0.00001837
Iteration 166/1000 | Loss: 0.00001836
Iteration 167/1000 | Loss: 0.00001836
Iteration 168/1000 | Loss: 0.00001836
Iteration 169/1000 | Loss: 0.00001836
Iteration 170/1000 | Loss: 0.00001836
Iteration 171/1000 | Loss: 0.00001836
Iteration 172/1000 | Loss: 0.00001836
Iteration 173/1000 | Loss: 0.00001836
Iteration 174/1000 | Loss: 0.00001836
Iteration 175/1000 | Loss: 0.00001836
Iteration 176/1000 | Loss: 0.00001836
Iteration 177/1000 | Loss: 0.00001836
Iteration 178/1000 | Loss: 0.00001836
Iteration 179/1000 | Loss: 0.00001836
Iteration 180/1000 | Loss: 0.00001836
Iteration 181/1000 | Loss: 0.00001836
Iteration 182/1000 | Loss: 0.00001836
Iteration 183/1000 | Loss: 0.00001836
Iteration 184/1000 | Loss: 0.00001836
Iteration 185/1000 | Loss: 0.00001836
Iteration 186/1000 | Loss: 0.00001836
Iteration 187/1000 | Loss: 0.00001836
Iteration 188/1000 | Loss: 0.00001836
Iteration 189/1000 | Loss: 0.00001836
Iteration 190/1000 | Loss: 0.00001836
Iteration 191/1000 | Loss: 0.00001836
Iteration 192/1000 | Loss: 0.00001836
Iteration 193/1000 | Loss: 0.00001836
Iteration 194/1000 | Loss: 0.00001836
Iteration 195/1000 | Loss: 0.00001836
Iteration 196/1000 | Loss: 0.00001836
Iteration 197/1000 | Loss: 0.00001836
Iteration 198/1000 | Loss: 0.00001836
Iteration 199/1000 | Loss: 0.00001836
Iteration 200/1000 | Loss: 0.00001836
Iteration 201/1000 | Loss: 0.00001836
Iteration 202/1000 | Loss: 0.00001836
Iteration 203/1000 | Loss: 0.00001836
Iteration 204/1000 | Loss: 0.00001836
Iteration 205/1000 | Loss: 0.00001836
Iteration 206/1000 | Loss: 0.00001836
Iteration 207/1000 | Loss: 0.00001836
Iteration 208/1000 | Loss: 0.00001836
Iteration 209/1000 | Loss: 0.00001836
Iteration 210/1000 | Loss: 0.00001836
Iteration 211/1000 | Loss: 0.00001836
Iteration 212/1000 | Loss: 0.00001836
Iteration 213/1000 | Loss: 0.00001836
Iteration 214/1000 | Loss: 0.00001836
Iteration 215/1000 | Loss: 0.00001836
Iteration 216/1000 | Loss: 0.00001836
Iteration 217/1000 | Loss: 0.00001836
Iteration 218/1000 | Loss: 0.00001836
Iteration 219/1000 | Loss: 0.00001836
Iteration 220/1000 | Loss: 0.00001836
Iteration 221/1000 | Loss: 0.00001836
Iteration 222/1000 | Loss: 0.00001836
Iteration 223/1000 | Loss: 0.00001836
Iteration 224/1000 | Loss: 0.00001836
Iteration 225/1000 | Loss: 0.00001836
Iteration 226/1000 | Loss: 0.00001836
Iteration 227/1000 | Loss: 0.00001836
Iteration 228/1000 | Loss: 0.00001836
Iteration 229/1000 | Loss: 0.00001836
Iteration 230/1000 | Loss: 0.00001836
Iteration 231/1000 | Loss: 0.00001836
Iteration 232/1000 | Loss: 0.00001836
Iteration 233/1000 | Loss: 0.00001836
Iteration 234/1000 | Loss: 0.00001836
Iteration 235/1000 | Loss: 0.00001836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.836362571339123e-05, 1.836362571339123e-05, 1.836362571339123e-05, 1.836362571339123e-05, 1.836362571339123e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.836362571339123e-05

Optimization complete. Final v2v error: 3.5315935611724854 mm

Highest mean error: 5.010866641998291 mm for frame 71

Lowest mean error: 2.9491069316864014 mm for frame 176

Saving results

Total time: 170.00514817237854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00357438
Iteration 2/25 | Loss: 0.00083428
Iteration 3/25 | Loss: 0.00073487
Iteration 4/25 | Loss: 0.00072231
Iteration 5/25 | Loss: 0.00071899
Iteration 6/25 | Loss: 0.00071807
Iteration 7/25 | Loss: 0.00071807
Iteration 8/25 | Loss: 0.00071807
Iteration 9/25 | Loss: 0.00071807
Iteration 10/25 | Loss: 0.00071807
Iteration 11/25 | Loss: 0.00071807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007180682150647044, 0.0007180682150647044, 0.0007180682150647044, 0.0007180682150647044, 0.0007180682150647044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007180682150647044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71749520
Iteration 2/25 | Loss: 0.00055558
Iteration 3/25 | Loss: 0.00055558
Iteration 4/25 | Loss: 0.00055558
Iteration 5/25 | Loss: 0.00055558
Iteration 6/25 | Loss: 0.00055557
Iteration 7/25 | Loss: 0.00055557
Iteration 8/25 | Loss: 0.00055557
Iteration 9/25 | Loss: 0.00055557
Iteration 10/25 | Loss: 0.00055557
Iteration 11/25 | Loss: 0.00055557
Iteration 12/25 | Loss: 0.00055557
Iteration 13/25 | Loss: 0.00055557
Iteration 14/25 | Loss: 0.00055557
Iteration 15/25 | Loss: 0.00055557
Iteration 16/25 | Loss: 0.00055557
Iteration 17/25 | Loss: 0.00055557
Iteration 18/25 | Loss: 0.00055557
Iteration 19/25 | Loss: 0.00055557
Iteration 20/25 | Loss: 0.00055557
Iteration 21/25 | Loss: 0.00055557
Iteration 22/25 | Loss: 0.00055557
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005555744282901287, 0.0005555744282901287, 0.0005555744282901287, 0.0005555744282901287, 0.0005555744282901287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005555744282901287

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055557
Iteration 2/1000 | Loss: 0.00001902
Iteration 3/1000 | Loss: 0.00001670
Iteration 4/1000 | Loss: 0.00001580
Iteration 5/1000 | Loss: 0.00001513
Iteration 6/1000 | Loss: 0.00001476
Iteration 7/1000 | Loss: 0.00001454
Iteration 8/1000 | Loss: 0.00001450
Iteration 9/1000 | Loss: 0.00001448
Iteration 10/1000 | Loss: 0.00001448
Iteration 11/1000 | Loss: 0.00001448
Iteration 12/1000 | Loss: 0.00001448
Iteration 13/1000 | Loss: 0.00001447
Iteration 14/1000 | Loss: 0.00001447
Iteration 15/1000 | Loss: 0.00001447
Iteration 16/1000 | Loss: 0.00001447
Iteration 17/1000 | Loss: 0.00001447
Iteration 18/1000 | Loss: 0.00001446
Iteration 19/1000 | Loss: 0.00001445
Iteration 20/1000 | Loss: 0.00001444
Iteration 21/1000 | Loss: 0.00001444
Iteration 22/1000 | Loss: 0.00001443
Iteration 23/1000 | Loss: 0.00001443
Iteration 24/1000 | Loss: 0.00001442
Iteration 25/1000 | Loss: 0.00001442
Iteration 26/1000 | Loss: 0.00001441
Iteration 27/1000 | Loss: 0.00001441
Iteration 28/1000 | Loss: 0.00001440
Iteration 29/1000 | Loss: 0.00001440
Iteration 30/1000 | Loss: 0.00001440
Iteration 31/1000 | Loss: 0.00001440
Iteration 32/1000 | Loss: 0.00001440
Iteration 33/1000 | Loss: 0.00001440
Iteration 34/1000 | Loss: 0.00001440
Iteration 35/1000 | Loss: 0.00001440
Iteration 36/1000 | Loss: 0.00001439
Iteration 37/1000 | Loss: 0.00001439
Iteration 38/1000 | Loss: 0.00001439
Iteration 39/1000 | Loss: 0.00001439
Iteration 40/1000 | Loss: 0.00001438
Iteration 41/1000 | Loss: 0.00001438
Iteration 42/1000 | Loss: 0.00001438
Iteration 43/1000 | Loss: 0.00001438
Iteration 44/1000 | Loss: 0.00001438
Iteration 45/1000 | Loss: 0.00001438
Iteration 46/1000 | Loss: 0.00001438
Iteration 47/1000 | Loss: 0.00001438
Iteration 48/1000 | Loss: 0.00001438
Iteration 49/1000 | Loss: 0.00001437
Iteration 50/1000 | Loss: 0.00001437
Iteration 51/1000 | Loss: 0.00001437
Iteration 52/1000 | Loss: 0.00001437
Iteration 53/1000 | Loss: 0.00001437
Iteration 54/1000 | Loss: 0.00001437
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001436
Iteration 58/1000 | Loss: 0.00001436
Iteration 59/1000 | Loss: 0.00001436
Iteration 60/1000 | Loss: 0.00001436
Iteration 61/1000 | Loss: 0.00001436
Iteration 62/1000 | Loss: 0.00001436
Iteration 63/1000 | Loss: 0.00001436
Iteration 64/1000 | Loss: 0.00001435
Iteration 65/1000 | Loss: 0.00001435
Iteration 66/1000 | Loss: 0.00001435
Iteration 67/1000 | Loss: 0.00001435
Iteration 68/1000 | Loss: 0.00001435
Iteration 69/1000 | Loss: 0.00001435
Iteration 70/1000 | Loss: 0.00001435
Iteration 71/1000 | Loss: 0.00001434
Iteration 72/1000 | Loss: 0.00001434
Iteration 73/1000 | Loss: 0.00001434
Iteration 74/1000 | Loss: 0.00001434
Iteration 75/1000 | Loss: 0.00001434
Iteration 76/1000 | Loss: 0.00001433
Iteration 77/1000 | Loss: 0.00001433
Iteration 78/1000 | Loss: 0.00001433
Iteration 79/1000 | Loss: 0.00001433
Iteration 80/1000 | Loss: 0.00001433
Iteration 81/1000 | Loss: 0.00001433
Iteration 82/1000 | Loss: 0.00001433
Iteration 83/1000 | Loss: 0.00001433
Iteration 84/1000 | Loss: 0.00001433
Iteration 85/1000 | Loss: 0.00001433
Iteration 86/1000 | Loss: 0.00001433
Iteration 87/1000 | Loss: 0.00001433
Iteration 88/1000 | Loss: 0.00001433
Iteration 89/1000 | Loss: 0.00001433
Iteration 90/1000 | Loss: 0.00001432
Iteration 91/1000 | Loss: 0.00001432
Iteration 92/1000 | Loss: 0.00001432
Iteration 93/1000 | Loss: 0.00001432
Iteration 94/1000 | Loss: 0.00001432
Iteration 95/1000 | Loss: 0.00001431
Iteration 96/1000 | Loss: 0.00001431
Iteration 97/1000 | Loss: 0.00001431
Iteration 98/1000 | Loss: 0.00001431
Iteration 99/1000 | Loss: 0.00001431
Iteration 100/1000 | Loss: 0.00001431
Iteration 101/1000 | Loss: 0.00001431
Iteration 102/1000 | Loss: 0.00001431
Iteration 103/1000 | Loss: 0.00001431
Iteration 104/1000 | Loss: 0.00001431
Iteration 105/1000 | Loss: 0.00001431
Iteration 106/1000 | Loss: 0.00001430
Iteration 107/1000 | Loss: 0.00001430
Iteration 108/1000 | Loss: 0.00001430
Iteration 109/1000 | Loss: 0.00001430
Iteration 110/1000 | Loss: 0.00001430
Iteration 111/1000 | Loss: 0.00001430
Iteration 112/1000 | Loss: 0.00001430
Iteration 113/1000 | Loss: 0.00001430
Iteration 114/1000 | Loss: 0.00001430
Iteration 115/1000 | Loss: 0.00001430
Iteration 116/1000 | Loss: 0.00001430
Iteration 117/1000 | Loss: 0.00001430
Iteration 118/1000 | Loss: 0.00001430
Iteration 119/1000 | Loss: 0.00001430
Iteration 120/1000 | Loss: 0.00001430
Iteration 121/1000 | Loss: 0.00001430
Iteration 122/1000 | Loss: 0.00001430
Iteration 123/1000 | Loss: 0.00001429
Iteration 124/1000 | Loss: 0.00001429
Iteration 125/1000 | Loss: 0.00001429
Iteration 126/1000 | Loss: 0.00001429
Iteration 127/1000 | Loss: 0.00001429
Iteration 128/1000 | Loss: 0.00001429
Iteration 129/1000 | Loss: 0.00001429
Iteration 130/1000 | Loss: 0.00001429
Iteration 131/1000 | Loss: 0.00001429
Iteration 132/1000 | Loss: 0.00001429
Iteration 133/1000 | Loss: 0.00001429
Iteration 134/1000 | Loss: 0.00001429
Iteration 135/1000 | Loss: 0.00001429
Iteration 136/1000 | Loss: 0.00001429
Iteration 137/1000 | Loss: 0.00001429
Iteration 138/1000 | Loss: 0.00001429
Iteration 139/1000 | Loss: 0.00001429
Iteration 140/1000 | Loss: 0.00001429
Iteration 141/1000 | Loss: 0.00001429
Iteration 142/1000 | Loss: 0.00001429
Iteration 143/1000 | Loss: 0.00001429
Iteration 144/1000 | Loss: 0.00001429
Iteration 145/1000 | Loss: 0.00001429
Iteration 146/1000 | Loss: 0.00001429
Iteration 147/1000 | Loss: 0.00001429
Iteration 148/1000 | Loss: 0.00001429
Iteration 149/1000 | Loss: 0.00001429
Iteration 150/1000 | Loss: 0.00001429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.4294366337708198e-05, 1.4294366337708198e-05, 1.4294366337708198e-05, 1.4294366337708198e-05, 1.4294366337708198e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4294366337708198e-05

Optimization complete. Final v2v error: 3.235219955444336 mm

Highest mean error: 3.4729936122894287 mm for frame 139

Lowest mean error: 3.071805000305176 mm for frame 156

Saving results

Total time: 33.53247809410095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01107970
Iteration 2/25 | Loss: 0.00221055
Iteration 3/25 | Loss: 0.00151272
Iteration 4/25 | Loss: 0.00130614
Iteration 5/25 | Loss: 0.00124885
Iteration 6/25 | Loss: 0.00122597
Iteration 7/25 | Loss: 0.00121347
Iteration 8/25 | Loss: 0.00116414
Iteration 9/25 | Loss: 0.00113305
Iteration 10/25 | Loss: 0.00111705
Iteration 11/25 | Loss: 0.00109689
Iteration 12/25 | Loss: 0.00108477
Iteration 13/25 | Loss: 0.00107126
Iteration 14/25 | Loss: 0.00105997
Iteration 15/25 | Loss: 0.00104478
Iteration 16/25 | Loss: 0.00103781
Iteration 17/25 | Loss: 0.00103179
Iteration 18/25 | Loss: 0.00102985
Iteration 19/25 | Loss: 0.00103108
Iteration 20/25 | Loss: 0.00102945
Iteration 21/25 | Loss: 0.00102767
Iteration 22/25 | Loss: 0.00102684
Iteration 23/25 | Loss: 0.00102621
Iteration 24/25 | Loss: 0.00102580
Iteration 25/25 | Loss: 0.00102568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48556030
Iteration 2/25 | Loss: 0.00240622
Iteration 3/25 | Loss: 0.00240622
Iteration 4/25 | Loss: 0.00240622
Iteration 5/25 | Loss: 0.00240622
Iteration 6/25 | Loss: 0.00240622
Iteration 7/25 | Loss: 0.00240622
Iteration 8/25 | Loss: 0.00240622
Iteration 9/25 | Loss: 0.00240622
Iteration 10/25 | Loss: 0.00240622
Iteration 11/25 | Loss: 0.00240622
Iteration 12/25 | Loss: 0.00240622
Iteration 13/25 | Loss: 0.00240622
Iteration 14/25 | Loss: 0.00240622
Iteration 15/25 | Loss: 0.00240622
Iteration 16/25 | Loss: 0.00240622
Iteration 17/25 | Loss: 0.00240622
Iteration 18/25 | Loss: 0.00240622
Iteration 19/25 | Loss: 0.00240622
Iteration 20/25 | Loss: 0.00240622
Iteration 21/25 | Loss: 0.00240622
Iteration 22/25 | Loss: 0.00240622
Iteration 23/25 | Loss: 0.00240622
Iteration 24/25 | Loss: 0.00240622
Iteration 25/25 | Loss: 0.00240622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00240622
Iteration 2/1000 | Loss: 0.00047586
Iteration 3/1000 | Loss: 0.00032305
Iteration 4/1000 | Loss: 0.00078225
Iteration 5/1000 | Loss: 0.00045591
Iteration 6/1000 | Loss: 0.00019739
Iteration 7/1000 | Loss: 0.00017019
Iteration 8/1000 | Loss: 0.00014341
Iteration 9/1000 | Loss: 0.00012663
Iteration 10/1000 | Loss: 0.00011790
Iteration 11/1000 | Loss: 0.00011260
Iteration 12/1000 | Loss: 0.00010844
Iteration 13/1000 | Loss: 0.00010549
Iteration 14/1000 | Loss: 0.00010360
Iteration 15/1000 | Loss: 0.00019277
Iteration 16/1000 | Loss: 0.00304943
Iteration 17/1000 | Loss: 0.00209734
Iteration 18/1000 | Loss: 0.00027989
Iteration 19/1000 | Loss: 0.00015109
Iteration 20/1000 | Loss: 0.00010468
Iteration 21/1000 | Loss: 0.00006563
Iteration 22/1000 | Loss: 0.00004473
Iteration 23/1000 | Loss: 0.00003471
Iteration 24/1000 | Loss: 0.00003046
Iteration 25/1000 | Loss: 0.00002578
Iteration 26/1000 | Loss: 0.00002266
Iteration 27/1000 | Loss: 0.00002093
Iteration 28/1000 | Loss: 0.00001917
Iteration 29/1000 | Loss: 0.00001799
Iteration 30/1000 | Loss: 0.00001705
Iteration 31/1000 | Loss: 0.00001631
Iteration 32/1000 | Loss: 0.00001600
Iteration 33/1000 | Loss: 0.00001577
Iteration 34/1000 | Loss: 0.00001569
Iteration 35/1000 | Loss: 0.00001563
Iteration 36/1000 | Loss: 0.00001563
Iteration 37/1000 | Loss: 0.00001563
Iteration 38/1000 | Loss: 0.00001563
Iteration 39/1000 | Loss: 0.00001558
Iteration 40/1000 | Loss: 0.00001558
Iteration 41/1000 | Loss: 0.00001558
Iteration 42/1000 | Loss: 0.00001558
Iteration 43/1000 | Loss: 0.00001558
Iteration 44/1000 | Loss: 0.00001558
Iteration 45/1000 | Loss: 0.00001557
Iteration 46/1000 | Loss: 0.00001557
Iteration 47/1000 | Loss: 0.00001557
Iteration 48/1000 | Loss: 0.00001557
Iteration 49/1000 | Loss: 0.00001556
Iteration 50/1000 | Loss: 0.00001556
Iteration 51/1000 | Loss: 0.00001556
Iteration 52/1000 | Loss: 0.00001555
Iteration 53/1000 | Loss: 0.00001555
Iteration 54/1000 | Loss: 0.00001555
Iteration 55/1000 | Loss: 0.00001554
Iteration 56/1000 | Loss: 0.00001554
Iteration 57/1000 | Loss: 0.00001554
Iteration 58/1000 | Loss: 0.00001554
Iteration 59/1000 | Loss: 0.00001554
Iteration 60/1000 | Loss: 0.00001554
Iteration 61/1000 | Loss: 0.00001554
Iteration 62/1000 | Loss: 0.00001554
Iteration 63/1000 | Loss: 0.00001553
Iteration 64/1000 | Loss: 0.00001553
Iteration 65/1000 | Loss: 0.00001553
Iteration 66/1000 | Loss: 0.00001553
Iteration 67/1000 | Loss: 0.00001553
Iteration 68/1000 | Loss: 0.00001552
Iteration 69/1000 | Loss: 0.00001552
Iteration 70/1000 | Loss: 0.00001552
Iteration 71/1000 | Loss: 0.00001552
Iteration 72/1000 | Loss: 0.00001552
Iteration 73/1000 | Loss: 0.00001552
Iteration 74/1000 | Loss: 0.00001552
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001551
Iteration 77/1000 | Loss: 0.00001551
Iteration 78/1000 | Loss: 0.00001551
Iteration 79/1000 | Loss: 0.00001551
Iteration 80/1000 | Loss: 0.00001551
Iteration 81/1000 | Loss: 0.00001551
Iteration 82/1000 | Loss: 0.00001551
Iteration 83/1000 | Loss: 0.00001551
Iteration 84/1000 | Loss: 0.00001551
Iteration 85/1000 | Loss: 0.00001551
Iteration 86/1000 | Loss: 0.00001551
Iteration 87/1000 | Loss: 0.00001551
Iteration 88/1000 | Loss: 0.00001551
Iteration 89/1000 | Loss: 0.00001551
Iteration 90/1000 | Loss: 0.00001550
Iteration 91/1000 | Loss: 0.00001550
Iteration 92/1000 | Loss: 0.00001550
Iteration 93/1000 | Loss: 0.00001550
Iteration 94/1000 | Loss: 0.00001550
Iteration 95/1000 | Loss: 0.00001550
Iteration 96/1000 | Loss: 0.00001549
Iteration 97/1000 | Loss: 0.00001549
Iteration 98/1000 | Loss: 0.00001549
Iteration 99/1000 | Loss: 0.00001549
Iteration 100/1000 | Loss: 0.00001549
Iteration 101/1000 | Loss: 0.00001549
Iteration 102/1000 | Loss: 0.00001549
Iteration 103/1000 | Loss: 0.00001549
Iteration 104/1000 | Loss: 0.00001549
Iteration 105/1000 | Loss: 0.00001549
Iteration 106/1000 | Loss: 0.00001549
Iteration 107/1000 | Loss: 0.00001549
Iteration 108/1000 | Loss: 0.00001548
Iteration 109/1000 | Loss: 0.00001548
Iteration 110/1000 | Loss: 0.00001548
Iteration 111/1000 | Loss: 0.00001548
Iteration 112/1000 | Loss: 0.00001548
Iteration 113/1000 | Loss: 0.00001548
Iteration 114/1000 | Loss: 0.00001548
Iteration 115/1000 | Loss: 0.00001548
Iteration 116/1000 | Loss: 0.00001548
Iteration 117/1000 | Loss: 0.00001548
Iteration 118/1000 | Loss: 0.00001547
Iteration 119/1000 | Loss: 0.00001547
Iteration 120/1000 | Loss: 0.00001547
Iteration 121/1000 | Loss: 0.00001547
Iteration 122/1000 | Loss: 0.00001547
Iteration 123/1000 | Loss: 0.00001547
Iteration 124/1000 | Loss: 0.00001547
Iteration 125/1000 | Loss: 0.00001546
Iteration 126/1000 | Loss: 0.00001546
Iteration 127/1000 | Loss: 0.00001546
Iteration 128/1000 | Loss: 0.00001546
Iteration 129/1000 | Loss: 0.00001546
Iteration 130/1000 | Loss: 0.00001546
Iteration 131/1000 | Loss: 0.00001546
Iteration 132/1000 | Loss: 0.00001546
Iteration 133/1000 | Loss: 0.00001546
Iteration 134/1000 | Loss: 0.00001546
Iteration 135/1000 | Loss: 0.00001546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.5462763258256018e-05, 1.5462763258256018e-05, 1.5462763258256018e-05, 1.5462763258256018e-05, 1.5462763258256018e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5462763258256018e-05

Optimization complete. Final v2v error: 3.3935422897338867 mm

Highest mean error: 3.822998523712158 mm for frame 13

Lowest mean error: 3.1870155334472656 mm for frame 123

Saving results

Total time: 117.33363461494446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00367829
Iteration 2/25 | Loss: 0.00091063
Iteration 3/25 | Loss: 0.00082024
Iteration 4/25 | Loss: 0.00080246
Iteration 5/25 | Loss: 0.00079567
Iteration 6/25 | Loss: 0.00079438
Iteration 7/25 | Loss: 0.00079423
Iteration 8/25 | Loss: 0.00079423
Iteration 9/25 | Loss: 0.00079423
Iteration 10/25 | Loss: 0.00079423
Iteration 11/25 | Loss: 0.00079423
Iteration 12/25 | Loss: 0.00079423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007942252559587359, 0.0007942252559587359, 0.0007942252559587359, 0.0007942252559587359, 0.0007942252559587359]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007942252559587359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49936295
Iteration 2/25 | Loss: 0.00075212
Iteration 3/25 | Loss: 0.00075212
Iteration 4/25 | Loss: 0.00075211
Iteration 5/25 | Loss: 0.00075211
Iteration 6/25 | Loss: 0.00075211
Iteration 7/25 | Loss: 0.00075211
Iteration 8/25 | Loss: 0.00075211
Iteration 9/25 | Loss: 0.00075211
Iteration 10/25 | Loss: 0.00075211
Iteration 11/25 | Loss: 0.00075211
Iteration 12/25 | Loss: 0.00075211
Iteration 13/25 | Loss: 0.00075211
Iteration 14/25 | Loss: 0.00075211
Iteration 15/25 | Loss: 0.00075211
Iteration 16/25 | Loss: 0.00075211
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007521127117797732, 0.0007521127117797732, 0.0007521127117797732, 0.0007521127117797732, 0.0007521127117797732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007521127117797732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075211
Iteration 2/1000 | Loss: 0.00002524
Iteration 3/1000 | Loss: 0.00002246
Iteration 4/1000 | Loss: 0.00002149
Iteration 5/1000 | Loss: 0.00002095
Iteration 6/1000 | Loss: 0.00002032
Iteration 7/1000 | Loss: 0.00001994
Iteration 8/1000 | Loss: 0.00001980
Iteration 9/1000 | Loss: 0.00001970
Iteration 10/1000 | Loss: 0.00001967
Iteration 11/1000 | Loss: 0.00001967
Iteration 12/1000 | Loss: 0.00001966
Iteration 13/1000 | Loss: 0.00001965
Iteration 14/1000 | Loss: 0.00001959
Iteration 15/1000 | Loss: 0.00001958
Iteration 16/1000 | Loss: 0.00001957
Iteration 17/1000 | Loss: 0.00001955
Iteration 18/1000 | Loss: 0.00001955
Iteration 19/1000 | Loss: 0.00001953
Iteration 20/1000 | Loss: 0.00001953
Iteration 21/1000 | Loss: 0.00001953
Iteration 22/1000 | Loss: 0.00001952
Iteration 23/1000 | Loss: 0.00001952
Iteration 24/1000 | Loss: 0.00001952
Iteration 25/1000 | Loss: 0.00001952
Iteration 26/1000 | Loss: 0.00001951
Iteration 27/1000 | Loss: 0.00001951
Iteration 28/1000 | Loss: 0.00001951
Iteration 29/1000 | Loss: 0.00001951
Iteration 30/1000 | Loss: 0.00001951
Iteration 31/1000 | Loss: 0.00001951
Iteration 32/1000 | Loss: 0.00001950
Iteration 33/1000 | Loss: 0.00001950
Iteration 34/1000 | Loss: 0.00001949
Iteration 35/1000 | Loss: 0.00001949
Iteration 36/1000 | Loss: 0.00001948
Iteration 37/1000 | Loss: 0.00001948
Iteration 38/1000 | Loss: 0.00001948
Iteration 39/1000 | Loss: 0.00001948
Iteration 40/1000 | Loss: 0.00001948
Iteration 41/1000 | Loss: 0.00001948
Iteration 42/1000 | Loss: 0.00001948
Iteration 43/1000 | Loss: 0.00001947
Iteration 44/1000 | Loss: 0.00001947
Iteration 45/1000 | Loss: 0.00001947
Iteration 46/1000 | Loss: 0.00001946
Iteration 47/1000 | Loss: 0.00001946
Iteration 48/1000 | Loss: 0.00001946
Iteration 49/1000 | Loss: 0.00001946
Iteration 50/1000 | Loss: 0.00001946
Iteration 51/1000 | Loss: 0.00001946
Iteration 52/1000 | Loss: 0.00001946
Iteration 53/1000 | Loss: 0.00001946
Iteration 54/1000 | Loss: 0.00001946
Iteration 55/1000 | Loss: 0.00001946
Iteration 56/1000 | Loss: 0.00001946
Iteration 57/1000 | Loss: 0.00001946
Iteration 58/1000 | Loss: 0.00001946
Iteration 59/1000 | Loss: 0.00001946
Iteration 60/1000 | Loss: 0.00001946
Iteration 61/1000 | Loss: 0.00001946
Iteration 62/1000 | Loss: 0.00001946
Iteration 63/1000 | Loss: 0.00001946
Iteration 64/1000 | Loss: 0.00001946
Iteration 65/1000 | Loss: 0.00001946
Iteration 66/1000 | Loss: 0.00001946
Iteration 67/1000 | Loss: 0.00001946
Iteration 68/1000 | Loss: 0.00001946
Iteration 69/1000 | Loss: 0.00001946
Iteration 70/1000 | Loss: 0.00001946
Iteration 71/1000 | Loss: 0.00001946
Iteration 72/1000 | Loss: 0.00001946
Iteration 73/1000 | Loss: 0.00001946
Iteration 74/1000 | Loss: 0.00001946
Iteration 75/1000 | Loss: 0.00001946
Iteration 76/1000 | Loss: 0.00001946
Iteration 77/1000 | Loss: 0.00001946
Iteration 78/1000 | Loss: 0.00001946
Iteration 79/1000 | Loss: 0.00001946
Iteration 80/1000 | Loss: 0.00001946
Iteration 81/1000 | Loss: 0.00001946
Iteration 82/1000 | Loss: 0.00001946
Iteration 83/1000 | Loss: 0.00001946
Iteration 84/1000 | Loss: 0.00001946
Iteration 85/1000 | Loss: 0.00001946
Iteration 86/1000 | Loss: 0.00001946
Iteration 87/1000 | Loss: 0.00001946
Iteration 88/1000 | Loss: 0.00001946
Iteration 89/1000 | Loss: 0.00001946
Iteration 90/1000 | Loss: 0.00001946
Iteration 91/1000 | Loss: 0.00001946
Iteration 92/1000 | Loss: 0.00001946
Iteration 93/1000 | Loss: 0.00001946
Iteration 94/1000 | Loss: 0.00001946
Iteration 95/1000 | Loss: 0.00001946
Iteration 96/1000 | Loss: 0.00001946
Iteration 97/1000 | Loss: 0.00001946
Iteration 98/1000 | Loss: 0.00001946
Iteration 99/1000 | Loss: 0.00001946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.945542862813454e-05, 1.945542862813454e-05, 1.945542862813454e-05, 1.945542862813454e-05, 1.945542862813454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.945542862813454e-05

Optimization complete. Final v2v error: 3.7966129779815674 mm

Highest mean error: 4.088446617126465 mm for frame 125

Lowest mean error: 3.3930957317352295 mm for frame 76

Saving results

Total time: 27.343770503997803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00370855
Iteration 2/25 | Loss: 0.00112386
Iteration 3/25 | Loss: 0.00082462
Iteration 4/25 | Loss: 0.00075780
Iteration 5/25 | Loss: 0.00074628
Iteration 6/25 | Loss: 0.00074392
Iteration 7/25 | Loss: 0.00074312
Iteration 8/25 | Loss: 0.00074296
Iteration 9/25 | Loss: 0.00074296
Iteration 10/25 | Loss: 0.00074296
Iteration 11/25 | Loss: 0.00074296
Iteration 12/25 | Loss: 0.00074296
Iteration 13/25 | Loss: 0.00074296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007429638062603772, 0.0007429638062603772, 0.0007429638062603772, 0.0007429638062603772, 0.0007429638062603772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007429638062603772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59041858
Iteration 2/25 | Loss: 0.00077330
Iteration 3/25 | Loss: 0.00077330
Iteration 4/25 | Loss: 0.00077330
Iteration 5/25 | Loss: 0.00077330
Iteration 6/25 | Loss: 0.00077330
Iteration 7/25 | Loss: 0.00077330
Iteration 8/25 | Loss: 0.00077330
Iteration 9/25 | Loss: 0.00077330
Iteration 10/25 | Loss: 0.00077330
Iteration 11/25 | Loss: 0.00077330
Iteration 12/25 | Loss: 0.00077330
Iteration 13/25 | Loss: 0.00077330
Iteration 14/25 | Loss: 0.00077330
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000773302570451051, 0.000773302570451051, 0.000773302570451051, 0.000773302570451051, 0.000773302570451051]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000773302570451051

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077330
Iteration 2/1000 | Loss: 0.00002943
Iteration 3/1000 | Loss: 0.00001972
Iteration 4/1000 | Loss: 0.00001555
Iteration 5/1000 | Loss: 0.00001464
Iteration 6/1000 | Loss: 0.00001379
Iteration 7/1000 | Loss: 0.00001377
Iteration 8/1000 | Loss: 0.00001351
Iteration 9/1000 | Loss: 0.00001332
Iteration 10/1000 | Loss: 0.00001304
Iteration 11/1000 | Loss: 0.00001286
Iteration 12/1000 | Loss: 0.00001281
Iteration 13/1000 | Loss: 0.00001277
Iteration 14/1000 | Loss: 0.00001274
Iteration 15/1000 | Loss: 0.00001274
Iteration 16/1000 | Loss: 0.00001274
Iteration 17/1000 | Loss: 0.00001274
Iteration 18/1000 | Loss: 0.00001274
Iteration 19/1000 | Loss: 0.00001273
Iteration 20/1000 | Loss: 0.00001271
Iteration 21/1000 | Loss: 0.00001271
Iteration 22/1000 | Loss: 0.00001270
Iteration 23/1000 | Loss: 0.00001270
Iteration 24/1000 | Loss: 0.00001269
Iteration 25/1000 | Loss: 0.00001269
Iteration 26/1000 | Loss: 0.00001268
Iteration 27/1000 | Loss: 0.00001268
Iteration 28/1000 | Loss: 0.00001268
Iteration 29/1000 | Loss: 0.00001268
Iteration 30/1000 | Loss: 0.00001267
Iteration 31/1000 | Loss: 0.00001267
Iteration 32/1000 | Loss: 0.00001267
Iteration 33/1000 | Loss: 0.00001266
Iteration 34/1000 | Loss: 0.00001265
Iteration 35/1000 | Loss: 0.00001265
Iteration 36/1000 | Loss: 0.00001265
Iteration 37/1000 | Loss: 0.00001265
Iteration 38/1000 | Loss: 0.00001265
Iteration 39/1000 | Loss: 0.00001264
Iteration 40/1000 | Loss: 0.00001263
Iteration 41/1000 | Loss: 0.00001263
Iteration 42/1000 | Loss: 0.00001262
Iteration 43/1000 | Loss: 0.00001262
Iteration 44/1000 | Loss: 0.00001261
Iteration 45/1000 | Loss: 0.00001261
Iteration 46/1000 | Loss: 0.00001261
Iteration 47/1000 | Loss: 0.00001261
Iteration 48/1000 | Loss: 0.00001261
Iteration 49/1000 | Loss: 0.00001261
Iteration 50/1000 | Loss: 0.00001261
Iteration 51/1000 | Loss: 0.00001260
Iteration 52/1000 | Loss: 0.00001260
Iteration 53/1000 | Loss: 0.00001260
Iteration 54/1000 | Loss: 0.00001260
Iteration 55/1000 | Loss: 0.00001260
Iteration 56/1000 | Loss: 0.00001259
Iteration 57/1000 | Loss: 0.00001259
Iteration 58/1000 | Loss: 0.00001259
Iteration 59/1000 | Loss: 0.00001259
Iteration 60/1000 | Loss: 0.00001259
Iteration 61/1000 | Loss: 0.00001259
Iteration 62/1000 | Loss: 0.00001259
Iteration 63/1000 | Loss: 0.00001259
Iteration 64/1000 | Loss: 0.00001259
Iteration 65/1000 | Loss: 0.00001259
Iteration 66/1000 | Loss: 0.00001258
Iteration 67/1000 | Loss: 0.00001258
Iteration 68/1000 | Loss: 0.00001258
Iteration 69/1000 | Loss: 0.00001258
Iteration 70/1000 | Loss: 0.00001257
Iteration 71/1000 | Loss: 0.00001257
Iteration 72/1000 | Loss: 0.00001257
Iteration 73/1000 | Loss: 0.00001257
Iteration 74/1000 | Loss: 0.00001257
Iteration 75/1000 | Loss: 0.00001257
Iteration 76/1000 | Loss: 0.00001256
Iteration 77/1000 | Loss: 0.00001256
Iteration 78/1000 | Loss: 0.00001256
Iteration 79/1000 | Loss: 0.00001256
Iteration 80/1000 | Loss: 0.00001256
Iteration 81/1000 | Loss: 0.00001256
Iteration 82/1000 | Loss: 0.00001255
Iteration 83/1000 | Loss: 0.00001255
Iteration 84/1000 | Loss: 0.00001255
Iteration 85/1000 | Loss: 0.00001255
Iteration 86/1000 | Loss: 0.00001254
Iteration 87/1000 | Loss: 0.00001254
Iteration 88/1000 | Loss: 0.00001254
Iteration 89/1000 | Loss: 0.00001254
Iteration 90/1000 | Loss: 0.00001253
Iteration 91/1000 | Loss: 0.00001253
Iteration 92/1000 | Loss: 0.00001253
Iteration 93/1000 | Loss: 0.00001253
Iteration 94/1000 | Loss: 0.00001253
Iteration 95/1000 | Loss: 0.00001253
Iteration 96/1000 | Loss: 0.00001253
Iteration 97/1000 | Loss: 0.00001253
Iteration 98/1000 | Loss: 0.00001253
Iteration 99/1000 | Loss: 0.00001253
Iteration 100/1000 | Loss: 0.00001253
Iteration 101/1000 | Loss: 0.00001253
Iteration 102/1000 | Loss: 0.00001253
Iteration 103/1000 | Loss: 0.00001252
Iteration 104/1000 | Loss: 0.00001252
Iteration 105/1000 | Loss: 0.00001252
Iteration 106/1000 | Loss: 0.00001252
Iteration 107/1000 | Loss: 0.00001252
Iteration 108/1000 | Loss: 0.00001252
Iteration 109/1000 | Loss: 0.00001252
Iteration 110/1000 | Loss: 0.00001252
Iteration 111/1000 | Loss: 0.00001252
Iteration 112/1000 | Loss: 0.00001252
Iteration 113/1000 | Loss: 0.00001252
Iteration 114/1000 | Loss: 0.00001252
Iteration 115/1000 | Loss: 0.00001252
Iteration 116/1000 | Loss: 0.00001251
Iteration 117/1000 | Loss: 0.00001251
Iteration 118/1000 | Loss: 0.00001251
Iteration 119/1000 | Loss: 0.00001251
Iteration 120/1000 | Loss: 0.00001251
Iteration 121/1000 | Loss: 0.00001251
Iteration 122/1000 | Loss: 0.00001251
Iteration 123/1000 | Loss: 0.00001251
Iteration 124/1000 | Loss: 0.00001251
Iteration 125/1000 | Loss: 0.00001251
Iteration 126/1000 | Loss: 0.00001251
Iteration 127/1000 | Loss: 0.00001251
Iteration 128/1000 | Loss: 0.00001250
Iteration 129/1000 | Loss: 0.00001250
Iteration 130/1000 | Loss: 0.00001250
Iteration 131/1000 | Loss: 0.00001250
Iteration 132/1000 | Loss: 0.00001250
Iteration 133/1000 | Loss: 0.00001250
Iteration 134/1000 | Loss: 0.00001250
Iteration 135/1000 | Loss: 0.00001250
Iteration 136/1000 | Loss: 0.00001250
Iteration 137/1000 | Loss: 0.00001250
Iteration 138/1000 | Loss: 0.00001250
Iteration 139/1000 | Loss: 0.00001250
Iteration 140/1000 | Loss: 0.00001250
Iteration 141/1000 | Loss: 0.00001250
Iteration 142/1000 | Loss: 0.00001250
Iteration 143/1000 | Loss: 0.00001250
Iteration 144/1000 | Loss: 0.00001250
Iteration 145/1000 | Loss: 0.00001249
Iteration 146/1000 | Loss: 0.00001249
Iteration 147/1000 | Loss: 0.00001249
Iteration 148/1000 | Loss: 0.00001249
Iteration 149/1000 | Loss: 0.00001249
Iteration 150/1000 | Loss: 0.00001249
Iteration 151/1000 | Loss: 0.00001249
Iteration 152/1000 | Loss: 0.00001249
Iteration 153/1000 | Loss: 0.00001249
Iteration 154/1000 | Loss: 0.00001249
Iteration 155/1000 | Loss: 0.00001249
Iteration 156/1000 | Loss: 0.00001249
Iteration 157/1000 | Loss: 0.00001249
Iteration 158/1000 | Loss: 0.00001249
Iteration 159/1000 | Loss: 0.00001249
Iteration 160/1000 | Loss: 0.00001249
Iteration 161/1000 | Loss: 0.00001249
Iteration 162/1000 | Loss: 0.00001249
Iteration 163/1000 | Loss: 0.00001249
Iteration 164/1000 | Loss: 0.00001249
Iteration 165/1000 | Loss: 0.00001249
Iteration 166/1000 | Loss: 0.00001249
Iteration 167/1000 | Loss: 0.00001249
Iteration 168/1000 | Loss: 0.00001249
Iteration 169/1000 | Loss: 0.00001249
Iteration 170/1000 | Loss: 0.00001249
Iteration 171/1000 | Loss: 0.00001249
Iteration 172/1000 | Loss: 0.00001249
Iteration 173/1000 | Loss: 0.00001249
Iteration 174/1000 | Loss: 0.00001249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.2490464541770052e-05, 1.2490464541770052e-05, 1.2490464541770052e-05, 1.2490464541770052e-05, 1.2490464541770052e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2490464541770052e-05

Optimization complete. Final v2v error: 3.0581417083740234 mm

Highest mean error: 3.5781033039093018 mm for frame 119

Lowest mean error: 2.6810619831085205 mm for frame 91

Saving results

Total time: 36.5777108669281
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01160897
Iteration 2/25 | Loss: 0.00256903
Iteration 3/25 | Loss: 0.00168894
Iteration 4/25 | Loss: 0.00151500
Iteration 5/25 | Loss: 0.00168340
Iteration 6/25 | Loss: 0.00195648
Iteration 7/25 | Loss: 0.00166553
Iteration 8/25 | Loss: 0.00144881
Iteration 9/25 | Loss: 0.00133117
Iteration 10/25 | Loss: 0.00126100
Iteration 11/25 | Loss: 0.00117106
Iteration 12/25 | Loss: 0.00113268
Iteration 13/25 | Loss: 0.00109001
Iteration 14/25 | Loss: 0.00108460
Iteration 15/25 | Loss: 0.00105732
Iteration 16/25 | Loss: 0.00106512
Iteration 17/25 | Loss: 0.00104194
Iteration 18/25 | Loss: 0.00100992
Iteration 19/25 | Loss: 0.00100897
Iteration 20/25 | Loss: 0.00100905
Iteration 21/25 | Loss: 0.00099466
Iteration 22/25 | Loss: 0.00096886
Iteration 23/25 | Loss: 0.00096203
Iteration 24/25 | Loss: 0.00096434
Iteration 25/25 | Loss: 0.00096425

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09278500
Iteration 2/25 | Loss: 0.00235650
Iteration 3/25 | Loss: 0.00206186
Iteration 4/25 | Loss: 0.00206185
Iteration 5/25 | Loss: 0.00206185
Iteration 6/25 | Loss: 0.00206185
Iteration 7/25 | Loss: 0.00206185
Iteration 8/25 | Loss: 0.00206185
Iteration 9/25 | Loss: 0.00206185
Iteration 10/25 | Loss: 0.00206185
Iteration 11/25 | Loss: 0.00206185
Iteration 12/25 | Loss: 0.00206185
Iteration 13/25 | Loss: 0.00206185
Iteration 14/25 | Loss: 0.00206185
Iteration 15/25 | Loss: 0.00206185
Iteration 16/25 | Loss: 0.00206185
Iteration 17/25 | Loss: 0.00206185
Iteration 18/25 | Loss: 0.00206185
Iteration 19/25 | Loss: 0.00206185
Iteration 20/25 | Loss: 0.00206185
Iteration 21/25 | Loss: 0.00206185
Iteration 22/25 | Loss: 0.00206185
Iteration 23/25 | Loss: 0.00206185
Iteration 24/25 | Loss: 0.00206185
Iteration 25/25 | Loss: 0.00206185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206185
Iteration 2/1000 | Loss: 0.00102813
Iteration 3/1000 | Loss: 0.00078138
Iteration 4/1000 | Loss: 0.00032857
Iteration 5/1000 | Loss: 0.00102125
Iteration 6/1000 | Loss: 0.00059900
Iteration 7/1000 | Loss: 0.00049829
Iteration 8/1000 | Loss: 0.00040865
Iteration 9/1000 | Loss: 0.00021032
Iteration 10/1000 | Loss: 0.00041518
Iteration 11/1000 | Loss: 0.00121977
Iteration 12/1000 | Loss: 0.00119855
Iteration 13/1000 | Loss: 0.00056397
Iteration 14/1000 | Loss: 0.00112433
Iteration 15/1000 | Loss: 0.00114508
Iteration 16/1000 | Loss: 0.00095498
Iteration 17/1000 | Loss: 0.00081465
Iteration 18/1000 | Loss: 0.00025004
Iteration 19/1000 | Loss: 0.00080683
Iteration 20/1000 | Loss: 0.00180902
Iteration 21/1000 | Loss: 0.00046863
Iteration 22/1000 | Loss: 0.00034508
Iteration 23/1000 | Loss: 0.00051249
Iteration 24/1000 | Loss: 0.00037884
Iteration 25/1000 | Loss: 0.00015811
Iteration 26/1000 | Loss: 0.00013876
Iteration 27/1000 | Loss: 0.00068456
Iteration 28/1000 | Loss: 0.00023289
Iteration 29/1000 | Loss: 0.00034956
Iteration 30/1000 | Loss: 0.00027086
Iteration 31/1000 | Loss: 0.00091025
Iteration 32/1000 | Loss: 0.00088796
Iteration 33/1000 | Loss: 0.00045648
Iteration 34/1000 | Loss: 0.00025617
Iteration 35/1000 | Loss: 0.00044029
Iteration 36/1000 | Loss: 0.00028057
Iteration 37/1000 | Loss: 0.00018866
Iteration 38/1000 | Loss: 0.00018396
Iteration 39/1000 | Loss: 0.00021217
Iteration 40/1000 | Loss: 0.00014547
Iteration 41/1000 | Loss: 0.00015823
Iteration 42/1000 | Loss: 0.00013594
Iteration 43/1000 | Loss: 0.00018282
Iteration 44/1000 | Loss: 0.00015631
Iteration 45/1000 | Loss: 0.00010232
Iteration 46/1000 | Loss: 0.00027413
Iteration 47/1000 | Loss: 0.00015798
Iteration 48/1000 | Loss: 0.00033671
Iteration 49/1000 | Loss: 0.00064104
Iteration 50/1000 | Loss: 0.00019779
Iteration 51/1000 | Loss: 0.00067861
Iteration 52/1000 | Loss: 0.00049226
Iteration 53/1000 | Loss: 0.00041628
Iteration 54/1000 | Loss: 0.00028943
Iteration 55/1000 | Loss: 0.00074967
Iteration 56/1000 | Loss: 0.00042961
Iteration 57/1000 | Loss: 0.00026591
Iteration 58/1000 | Loss: 0.00025254
Iteration 59/1000 | Loss: 0.00040543
Iteration 60/1000 | Loss: 0.00014745
Iteration 61/1000 | Loss: 0.00064806
Iteration 62/1000 | Loss: 0.00013451
Iteration 63/1000 | Loss: 0.00017080
Iteration 64/1000 | Loss: 0.00016930
Iteration 65/1000 | Loss: 0.00017578
Iteration 66/1000 | Loss: 0.00014753
Iteration 67/1000 | Loss: 0.00011719
Iteration 68/1000 | Loss: 0.00013167
Iteration 69/1000 | Loss: 0.00016029
Iteration 70/1000 | Loss: 0.00017365
Iteration 71/1000 | Loss: 0.00037332
Iteration 72/1000 | Loss: 0.00023195
Iteration 73/1000 | Loss: 0.00074848
Iteration 74/1000 | Loss: 0.00020170
Iteration 75/1000 | Loss: 0.00012352
Iteration 76/1000 | Loss: 0.00008038
Iteration 77/1000 | Loss: 0.00021731
Iteration 78/1000 | Loss: 0.00009589
Iteration 79/1000 | Loss: 0.00008344
Iteration 80/1000 | Loss: 0.00009487
Iteration 81/1000 | Loss: 0.00006894
Iteration 82/1000 | Loss: 0.00006945
Iteration 83/1000 | Loss: 0.00006533
Iteration 84/1000 | Loss: 0.00008184
Iteration 85/1000 | Loss: 0.00035781
Iteration 86/1000 | Loss: 0.00025990
Iteration 87/1000 | Loss: 0.00025871
Iteration 88/1000 | Loss: 0.00019138
Iteration 89/1000 | Loss: 0.00007996
Iteration 90/1000 | Loss: 0.00004884
Iteration 91/1000 | Loss: 0.00055012
Iteration 92/1000 | Loss: 0.00006173
Iteration 93/1000 | Loss: 0.00004504
Iteration 94/1000 | Loss: 0.00005555
Iteration 95/1000 | Loss: 0.00005506
Iteration 96/1000 | Loss: 0.00004343
Iteration 97/1000 | Loss: 0.00004207
Iteration 98/1000 | Loss: 0.00005337
Iteration 99/1000 | Loss: 0.00005252
Iteration 100/1000 | Loss: 0.00006808
Iteration 101/1000 | Loss: 0.00004625
Iteration 102/1000 | Loss: 0.00006014
Iteration 103/1000 | Loss: 0.00004368
Iteration 104/1000 | Loss: 0.00004908
Iteration 105/1000 | Loss: 0.00003847
Iteration 106/1000 | Loss: 0.00005266
Iteration 107/1000 | Loss: 0.00004794
Iteration 108/1000 | Loss: 0.00019161
Iteration 109/1000 | Loss: 0.00003837
Iteration 110/1000 | Loss: 0.00004988
Iteration 111/1000 | Loss: 0.00040818
Iteration 112/1000 | Loss: 0.00026143
Iteration 113/1000 | Loss: 0.00016797
Iteration 114/1000 | Loss: 0.00021661
Iteration 115/1000 | Loss: 0.00006582
Iteration 116/1000 | Loss: 0.00005127
Iteration 117/1000 | Loss: 0.00005049
Iteration 118/1000 | Loss: 0.00007059
Iteration 119/1000 | Loss: 0.00023729
Iteration 120/1000 | Loss: 0.00005809
Iteration 121/1000 | Loss: 0.00005577
Iteration 122/1000 | Loss: 0.00005200
Iteration 123/1000 | Loss: 0.00003606
Iteration 124/1000 | Loss: 0.00005053
Iteration 125/1000 | Loss: 0.00003863
Iteration 126/1000 | Loss: 0.00031875
Iteration 127/1000 | Loss: 0.00019591
Iteration 128/1000 | Loss: 0.00004263
Iteration 129/1000 | Loss: 0.00031835
Iteration 130/1000 | Loss: 0.00024196
Iteration 131/1000 | Loss: 0.00027061
Iteration 132/1000 | Loss: 0.00025110
Iteration 133/1000 | Loss: 0.00023189
Iteration 134/1000 | Loss: 0.00005653
Iteration 135/1000 | Loss: 0.00003738
Iteration 136/1000 | Loss: 0.00004970
Iteration 137/1000 | Loss: 0.00005217
Iteration 138/1000 | Loss: 0.00003744
Iteration 139/1000 | Loss: 0.00004923
Iteration 140/1000 | Loss: 0.00004176
Iteration 141/1000 | Loss: 0.00003997
Iteration 142/1000 | Loss: 0.00003961
Iteration 143/1000 | Loss: 0.00004793
Iteration 144/1000 | Loss: 0.00003701
Iteration 145/1000 | Loss: 0.00004783
Iteration 146/1000 | Loss: 0.00004263
Iteration 147/1000 | Loss: 0.00004356
Iteration 148/1000 | Loss: 0.00004199
Iteration 149/1000 | Loss: 0.00003144
Iteration 150/1000 | Loss: 0.00005485
Iteration 151/1000 | Loss: 0.00006114
Iteration 152/1000 | Loss: 0.00004493
Iteration 153/1000 | Loss: 0.00006218
Iteration 154/1000 | Loss: 0.00004087
Iteration 155/1000 | Loss: 0.00004962
Iteration 156/1000 | Loss: 0.00003953
Iteration 157/1000 | Loss: 0.00004387
Iteration 158/1000 | Loss: 0.00003910
Iteration 159/1000 | Loss: 0.00004454
Iteration 160/1000 | Loss: 0.00003694
Iteration 161/1000 | Loss: 0.00003719
Iteration 162/1000 | Loss: 0.00004389
Iteration 163/1000 | Loss: 0.00003441
Iteration 164/1000 | Loss: 0.00003052
Iteration 165/1000 | Loss: 0.00020467
Iteration 166/1000 | Loss: 0.00005695
Iteration 167/1000 | Loss: 0.00003594
Iteration 168/1000 | Loss: 0.00003163
Iteration 169/1000 | Loss: 0.00002992
Iteration 170/1000 | Loss: 0.00002824
Iteration 171/1000 | Loss: 0.00002727
Iteration 172/1000 | Loss: 0.00002658
Iteration 173/1000 | Loss: 0.00002599
Iteration 174/1000 | Loss: 0.00002552
Iteration 175/1000 | Loss: 0.00002546
Iteration 176/1000 | Loss: 0.00002536
Iteration 177/1000 | Loss: 0.00002535
Iteration 178/1000 | Loss: 0.00002520
Iteration 179/1000 | Loss: 0.00002518
Iteration 180/1000 | Loss: 0.00002511
Iteration 181/1000 | Loss: 0.00002496
Iteration 182/1000 | Loss: 0.00002495
Iteration 183/1000 | Loss: 0.00002491
Iteration 184/1000 | Loss: 0.00002491
Iteration 185/1000 | Loss: 0.00002480
Iteration 186/1000 | Loss: 0.00002479
Iteration 187/1000 | Loss: 0.00002479
Iteration 188/1000 | Loss: 0.00002474
Iteration 189/1000 | Loss: 0.00002465
Iteration 190/1000 | Loss: 0.00002451
Iteration 191/1000 | Loss: 0.00002446
Iteration 192/1000 | Loss: 0.00019865
Iteration 193/1000 | Loss: 0.00003111
Iteration 194/1000 | Loss: 0.00002818
Iteration 195/1000 | Loss: 0.00002660
Iteration 196/1000 | Loss: 0.00002623
Iteration 197/1000 | Loss: 0.00002579
Iteration 198/1000 | Loss: 0.00002542
Iteration 199/1000 | Loss: 0.00002511
Iteration 200/1000 | Loss: 0.00002488
Iteration 201/1000 | Loss: 0.00002480
Iteration 202/1000 | Loss: 0.00002467
Iteration 203/1000 | Loss: 0.00002467
Iteration 204/1000 | Loss: 0.00002466
Iteration 205/1000 | Loss: 0.00019984
Iteration 206/1000 | Loss: 0.00003143
Iteration 207/1000 | Loss: 0.00002894
Iteration 208/1000 | Loss: 0.00002721
Iteration 209/1000 | Loss: 0.00002646
Iteration 210/1000 | Loss: 0.00002603
Iteration 211/1000 | Loss: 0.00002564
Iteration 212/1000 | Loss: 0.00002528
Iteration 213/1000 | Loss: 0.00002502
Iteration 214/1000 | Loss: 0.00002497
Iteration 215/1000 | Loss: 0.00002482
Iteration 216/1000 | Loss: 0.00021048
Iteration 217/1000 | Loss: 0.00003149
Iteration 218/1000 | Loss: 0.00002989
Iteration 219/1000 | Loss: 0.00002802
Iteration 220/1000 | Loss: 0.00002760
Iteration 221/1000 | Loss: 0.00002726
Iteration 222/1000 | Loss: 0.00002676
Iteration 223/1000 | Loss: 0.00002603
Iteration 224/1000 | Loss: 0.00002498
Iteration 225/1000 | Loss: 0.00002439
Iteration 226/1000 | Loss: 0.00002403
Iteration 227/1000 | Loss: 0.00002383
Iteration 228/1000 | Loss: 0.00002365
Iteration 229/1000 | Loss: 0.00002355
Iteration 230/1000 | Loss: 0.00002355
Iteration 231/1000 | Loss: 0.00002354
Iteration 232/1000 | Loss: 0.00002354
Iteration 233/1000 | Loss: 0.00002353
Iteration 234/1000 | Loss: 0.00002353
Iteration 235/1000 | Loss: 0.00002352
Iteration 236/1000 | Loss: 0.00002352
Iteration 237/1000 | Loss: 0.00002352
Iteration 238/1000 | Loss: 0.00002349
Iteration 239/1000 | Loss: 0.00002349
Iteration 240/1000 | Loss: 0.00002349
Iteration 241/1000 | Loss: 0.00002348
Iteration 242/1000 | Loss: 0.00002348
Iteration 243/1000 | Loss: 0.00002348
Iteration 244/1000 | Loss: 0.00002348
Iteration 245/1000 | Loss: 0.00002347
Iteration 246/1000 | Loss: 0.00002347
Iteration 247/1000 | Loss: 0.00002347
Iteration 248/1000 | Loss: 0.00002347
Iteration 249/1000 | Loss: 0.00002347
Iteration 250/1000 | Loss: 0.00002347
Iteration 251/1000 | Loss: 0.00002347
Iteration 252/1000 | Loss: 0.00002347
Iteration 253/1000 | Loss: 0.00002347
Iteration 254/1000 | Loss: 0.00002347
Iteration 255/1000 | Loss: 0.00002347
Iteration 256/1000 | Loss: 0.00002346
Iteration 257/1000 | Loss: 0.00002346
Iteration 258/1000 | Loss: 0.00002346
Iteration 259/1000 | Loss: 0.00002346
Iteration 260/1000 | Loss: 0.00002346
Iteration 261/1000 | Loss: 0.00002346
Iteration 262/1000 | Loss: 0.00002346
Iteration 263/1000 | Loss: 0.00002346
Iteration 264/1000 | Loss: 0.00002346
Iteration 265/1000 | Loss: 0.00002346
Iteration 266/1000 | Loss: 0.00002346
Iteration 267/1000 | Loss: 0.00002346
Iteration 268/1000 | Loss: 0.00002346
Iteration 269/1000 | Loss: 0.00002346
Iteration 270/1000 | Loss: 0.00002346
Iteration 271/1000 | Loss: 0.00002346
Iteration 272/1000 | Loss: 0.00002346
Iteration 273/1000 | Loss: 0.00002346
Iteration 274/1000 | Loss: 0.00002346
Iteration 275/1000 | Loss: 0.00002346
Iteration 276/1000 | Loss: 0.00002346
Iteration 277/1000 | Loss: 0.00002346
Iteration 278/1000 | Loss: 0.00002346
Iteration 279/1000 | Loss: 0.00002346
Iteration 280/1000 | Loss: 0.00002346
Iteration 281/1000 | Loss: 0.00002346
Iteration 282/1000 | Loss: 0.00002346
Iteration 283/1000 | Loss: 0.00002346
Iteration 284/1000 | Loss: 0.00002346
Iteration 285/1000 | Loss: 0.00002346
Iteration 286/1000 | Loss: 0.00002346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 286. Stopping optimization.
Last 5 losses: [2.3457727365894243e-05, 2.3457727365894243e-05, 2.3457727365894243e-05, 2.3457727365894243e-05, 2.3457727365894243e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3457727365894243e-05

Optimization complete. Final v2v error: 3.9060475826263428 mm

Highest mean error: 12.628463745117188 mm for frame 168

Lowest mean error: 3.5041582584381104 mm for frame 120

Saving results

Total time: 371.27051758766174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00948659
Iteration 2/25 | Loss: 0.00117014
Iteration 3/25 | Loss: 0.00087189
Iteration 4/25 | Loss: 0.00079109
Iteration 5/25 | Loss: 0.00076924
Iteration 6/25 | Loss: 0.00076493
Iteration 7/25 | Loss: 0.00076231
Iteration 8/25 | Loss: 0.00076022
Iteration 9/25 | Loss: 0.00075944
Iteration 10/25 | Loss: 0.00075901
Iteration 11/25 | Loss: 0.00075873
Iteration 12/25 | Loss: 0.00075860
Iteration 13/25 | Loss: 0.00075848
Iteration 14/25 | Loss: 0.00075847
Iteration 15/25 | Loss: 0.00075847
Iteration 16/25 | Loss: 0.00075846
Iteration 17/25 | Loss: 0.00075846
Iteration 18/25 | Loss: 0.00075846
Iteration 19/25 | Loss: 0.00075846
Iteration 20/25 | Loss: 0.00075846
Iteration 21/25 | Loss: 0.00075846
Iteration 22/25 | Loss: 0.00075846
Iteration 23/25 | Loss: 0.00075846
Iteration 24/25 | Loss: 0.00075846
Iteration 25/25 | Loss: 0.00075846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.73232174
Iteration 2/25 | Loss: 0.00063264
Iteration 3/25 | Loss: 0.00063259
Iteration 4/25 | Loss: 0.00063259
Iteration 5/25 | Loss: 0.00063259
Iteration 6/25 | Loss: 0.00063259
Iteration 7/25 | Loss: 0.00063259
Iteration 8/25 | Loss: 0.00063259
Iteration 9/25 | Loss: 0.00063259
Iteration 10/25 | Loss: 0.00063259
Iteration 11/25 | Loss: 0.00063259
Iteration 12/25 | Loss: 0.00063259
Iteration 13/25 | Loss: 0.00063259
Iteration 14/25 | Loss: 0.00063259
Iteration 15/25 | Loss: 0.00063259
Iteration 16/25 | Loss: 0.00063259
Iteration 17/25 | Loss: 0.00063259
Iteration 18/25 | Loss: 0.00063259
Iteration 19/25 | Loss: 0.00063259
Iteration 20/25 | Loss: 0.00063259
Iteration 21/25 | Loss: 0.00063259
Iteration 22/25 | Loss: 0.00063259
Iteration 23/25 | Loss: 0.00063259
Iteration 24/25 | Loss: 0.00063259
Iteration 25/25 | Loss: 0.00063259

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063259
Iteration 2/1000 | Loss: 0.00001965
Iteration 3/1000 | Loss: 0.00001711
Iteration 4/1000 | Loss: 0.00001616
Iteration 5/1000 | Loss: 0.00001558
Iteration 6/1000 | Loss: 0.00001515
Iteration 7/1000 | Loss: 0.00001484
Iteration 8/1000 | Loss: 0.00001474
Iteration 9/1000 | Loss: 0.00001472
Iteration 10/1000 | Loss: 0.00001470
Iteration 11/1000 | Loss: 0.00001468
Iteration 12/1000 | Loss: 0.00001467
Iteration 13/1000 | Loss: 0.00001466
Iteration 14/1000 | Loss: 0.00001466
Iteration 15/1000 | Loss: 0.00001465
Iteration 16/1000 | Loss: 0.00001465
Iteration 17/1000 | Loss: 0.00001464
Iteration 18/1000 | Loss: 0.00001464
Iteration 19/1000 | Loss: 0.00001464
Iteration 20/1000 | Loss: 0.00001463
Iteration 21/1000 | Loss: 0.00001463
Iteration 22/1000 | Loss: 0.00001462
Iteration 23/1000 | Loss: 0.00001462
Iteration 24/1000 | Loss: 0.00001461
Iteration 25/1000 | Loss: 0.00001461
Iteration 26/1000 | Loss: 0.00001461
Iteration 27/1000 | Loss: 0.00001461
Iteration 28/1000 | Loss: 0.00001460
Iteration 29/1000 | Loss: 0.00001460
Iteration 30/1000 | Loss: 0.00001459
Iteration 31/1000 | Loss: 0.00001459
Iteration 32/1000 | Loss: 0.00001459
Iteration 33/1000 | Loss: 0.00001458
Iteration 34/1000 | Loss: 0.00001458
Iteration 35/1000 | Loss: 0.00001458
Iteration 36/1000 | Loss: 0.00001457
Iteration 37/1000 | Loss: 0.00001457
Iteration 38/1000 | Loss: 0.00001457
Iteration 39/1000 | Loss: 0.00001456
Iteration 40/1000 | Loss: 0.00001456
Iteration 41/1000 | Loss: 0.00001456
Iteration 42/1000 | Loss: 0.00001455
Iteration 43/1000 | Loss: 0.00001455
Iteration 44/1000 | Loss: 0.00001455
Iteration 45/1000 | Loss: 0.00001454
Iteration 46/1000 | Loss: 0.00001454
Iteration 47/1000 | Loss: 0.00001453
Iteration 48/1000 | Loss: 0.00001453
Iteration 49/1000 | Loss: 0.00001453
Iteration 50/1000 | Loss: 0.00001453
Iteration 51/1000 | Loss: 0.00001453
Iteration 52/1000 | Loss: 0.00001453
Iteration 53/1000 | Loss: 0.00001452
Iteration 54/1000 | Loss: 0.00001452
Iteration 55/1000 | Loss: 0.00001452
Iteration 56/1000 | Loss: 0.00001452
Iteration 57/1000 | Loss: 0.00001451
Iteration 58/1000 | Loss: 0.00001451
Iteration 59/1000 | Loss: 0.00001451
Iteration 60/1000 | Loss: 0.00001451
Iteration 61/1000 | Loss: 0.00001450
Iteration 62/1000 | Loss: 0.00001450
Iteration 63/1000 | Loss: 0.00001450
Iteration 64/1000 | Loss: 0.00001449
Iteration 65/1000 | Loss: 0.00001449
Iteration 66/1000 | Loss: 0.00001449
Iteration 67/1000 | Loss: 0.00001448
Iteration 68/1000 | Loss: 0.00001448
Iteration 69/1000 | Loss: 0.00001448
Iteration 70/1000 | Loss: 0.00001447
Iteration 71/1000 | Loss: 0.00001447
Iteration 72/1000 | Loss: 0.00001447
Iteration 73/1000 | Loss: 0.00001447
Iteration 74/1000 | Loss: 0.00001447
Iteration 75/1000 | Loss: 0.00001447
Iteration 76/1000 | Loss: 0.00001447
Iteration 77/1000 | Loss: 0.00001446
Iteration 78/1000 | Loss: 0.00001446
Iteration 79/1000 | Loss: 0.00001446
Iteration 80/1000 | Loss: 0.00001446
Iteration 81/1000 | Loss: 0.00001446
Iteration 82/1000 | Loss: 0.00001446
Iteration 83/1000 | Loss: 0.00001446
Iteration 84/1000 | Loss: 0.00001446
Iteration 85/1000 | Loss: 0.00001446
Iteration 86/1000 | Loss: 0.00001446
Iteration 87/1000 | Loss: 0.00001446
Iteration 88/1000 | Loss: 0.00001446
Iteration 89/1000 | Loss: 0.00001446
Iteration 90/1000 | Loss: 0.00001446
Iteration 91/1000 | Loss: 0.00001446
Iteration 92/1000 | Loss: 0.00001446
Iteration 93/1000 | Loss: 0.00001446
Iteration 94/1000 | Loss: 0.00001446
Iteration 95/1000 | Loss: 0.00001446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.446422356821131e-05, 1.446422356821131e-05, 1.446422356821131e-05, 1.446422356821131e-05, 1.446422356821131e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.446422356821131e-05

Optimization complete. Final v2v error: 3.2947001457214355 mm

Highest mean error: 3.93320894241333 mm for frame 86

Lowest mean error: 2.85067081451416 mm for frame 207

Saving results

Total time: 46.04257822036743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439421
Iteration 2/25 | Loss: 0.00095725
Iteration 3/25 | Loss: 0.00086466
Iteration 4/25 | Loss: 0.00084087
Iteration 5/25 | Loss: 0.00083119
Iteration 6/25 | Loss: 0.00082946
Iteration 7/25 | Loss: 0.00082946
Iteration 8/25 | Loss: 0.00082946
Iteration 9/25 | Loss: 0.00082946
Iteration 10/25 | Loss: 0.00082946
Iteration 11/25 | Loss: 0.00082946
Iteration 12/25 | Loss: 0.00082946
Iteration 13/25 | Loss: 0.00082946
Iteration 14/25 | Loss: 0.00082946
Iteration 15/25 | Loss: 0.00082946
Iteration 16/25 | Loss: 0.00082946
Iteration 17/25 | Loss: 0.00082946
Iteration 18/25 | Loss: 0.00082946
Iteration 19/25 | Loss: 0.00082946
Iteration 20/25 | Loss: 0.00082946
Iteration 21/25 | Loss: 0.00082946
Iteration 22/25 | Loss: 0.00082946
Iteration 23/25 | Loss: 0.00082946
Iteration 24/25 | Loss: 0.00082946
Iteration 25/25 | Loss: 0.00082946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47641623
Iteration 2/25 | Loss: 0.00059282
Iteration 3/25 | Loss: 0.00059282
Iteration 4/25 | Loss: 0.00059282
Iteration 5/25 | Loss: 0.00059282
Iteration 6/25 | Loss: 0.00059282
Iteration 7/25 | Loss: 0.00059282
Iteration 8/25 | Loss: 0.00059282
Iteration 9/25 | Loss: 0.00059282
Iteration 10/25 | Loss: 0.00059282
Iteration 11/25 | Loss: 0.00059282
Iteration 12/25 | Loss: 0.00059282
Iteration 13/25 | Loss: 0.00059282
Iteration 14/25 | Loss: 0.00059282
Iteration 15/25 | Loss: 0.00059282
Iteration 16/25 | Loss: 0.00059282
Iteration 17/25 | Loss: 0.00059282
Iteration 18/25 | Loss: 0.00059282
Iteration 19/25 | Loss: 0.00059282
Iteration 20/25 | Loss: 0.00059282
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005928200553171337, 0.0005928200553171337, 0.0005928200553171337, 0.0005928200553171337, 0.0005928200553171337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005928200553171337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059282
Iteration 2/1000 | Loss: 0.00003897
Iteration 3/1000 | Loss: 0.00003467
Iteration 4/1000 | Loss: 0.00003233
Iteration 5/1000 | Loss: 0.00003046
Iteration 6/1000 | Loss: 0.00002919
Iteration 7/1000 | Loss: 0.00002864
Iteration 8/1000 | Loss: 0.00002849
Iteration 9/1000 | Loss: 0.00002841
Iteration 10/1000 | Loss: 0.00002829
Iteration 11/1000 | Loss: 0.00002828
Iteration 12/1000 | Loss: 0.00002827
Iteration 13/1000 | Loss: 0.00002827
Iteration 14/1000 | Loss: 0.00002826
Iteration 15/1000 | Loss: 0.00002822
Iteration 16/1000 | Loss: 0.00002821
Iteration 17/1000 | Loss: 0.00002821
Iteration 18/1000 | Loss: 0.00002820
Iteration 19/1000 | Loss: 0.00002820
Iteration 20/1000 | Loss: 0.00002820
Iteration 21/1000 | Loss: 0.00002820
Iteration 22/1000 | Loss: 0.00002820
Iteration 23/1000 | Loss: 0.00002819
Iteration 24/1000 | Loss: 0.00002818
Iteration 25/1000 | Loss: 0.00002818
Iteration 26/1000 | Loss: 0.00002818
Iteration 27/1000 | Loss: 0.00002817
Iteration 28/1000 | Loss: 0.00002817
Iteration 29/1000 | Loss: 0.00002816
Iteration 30/1000 | Loss: 0.00002816
Iteration 31/1000 | Loss: 0.00002816
Iteration 32/1000 | Loss: 0.00002815
Iteration 33/1000 | Loss: 0.00002814
Iteration 34/1000 | Loss: 0.00002814
Iteration 35/1000 | Loss: 0.00002814
Iteration 36/1000 | Loss: 0.00002813
Iteration 37/1000 | Loss: 0.00002813
Iteration 38/1000 | Loss: 0.00002812
Iteration 39/1000 | Loss: 0.00002812
Iteration 40/1000 | Loss: 0.00002811
Iteration 41/1000 | Loss: 0.00002811
Iteration 42/1000 | Loss: 0.00002811
Iteration 43/1000 | Loss: 0.00002810
Iteration 44/1000 | Loss: 0.00002810
Iteration 45/1000 | Loss: 0.00002810
Iteration 46/1000 | Loss: 0.00002809
Iteration 47/1000 | Loss: 0.00002809
Iteration 48/1000 | Loss: 0.00002808
Iteration 49/1000 | Loss: 0.00002808
Iteration 50/1000 | Loss: 0.00002808
Iteration 51/1000 | Loss: 0.00002808
Iteration 52/1000 | Loss: 0.00002808
Iteration 53/1000 | Loss: 0.00002808
Iteration 54/1000 | Loss: 0.00002808
Iteration 55/1000 | Loss: 0.00002807
Iteration 56/1000 | Loss: 0.00002807
Iteration 57/1000 | Loss: 0.00002807
Iteration 58/1000 | Loss: 0.00002807
Iteration 59/1000 | Loss: 0.00002806
Iteration 60/1000 | Loss: 0.00002806
Iteration 61/1000 | Loss: 0.00002806
Iteration 62/1000 | Loss: 0.00002806
Iteration 63/1000 | Loss: 0.00002805
Iteration 64/1000 | Loss: 0.00002805
Iteration 65/1000 | Loss: 0.00002805
Iteration 66/1000 | Loss: 0.00002805
Iteration 67/1000 | Loss: 0.00002804
Iteration 68/1000 | Loss: 0.00002804
Iteration 69/1000 | Loss: 0.00002804
Iteration 70/1000 | Loss: 0.00002804
Iteration 71/1000 | Loss: 0.00002804
Iteration 72/1000 | Loss: 0.00002804
Iteration 73/1000 | Loss: 0.00002804
Iteration 74/1000 | Loss: 0.00002804
Iteration 75/1000 | Loss: 0.00002803
Iteration 76/1000 | Loss: 0.00002803
Iteration 77/1000 | Loss: 0.00002803
Iteration 78/1000 | Loss: 0.00002803
Iteration 79/1000 | Loss: 0.00002803
Iteration 80/1000 | Loss: 0.00002802
Iteration 81/1000 | Loss: 0.00002802
Iteration 82/1000 | Loss: 0.00002801
Iteration 83/1000 | Loss: 0.00002801
Iteration 84/1000 | Loss: 0.00002801
Iteration 85/1000 | Loss: 0.00002801
Iteration 86/1000 | Loss: 0.00002801
Iteration 87/1000 | Loss: 0.00002801
Iteration 88/1000 | Loss: 0.00002801
Iteration 89/1000 | Loss: 0.00002801
Iteration 90/1000 | Loss: 0.00002800
Iteration 91/1000 | Loss: 0.00002800
Iteration 92/1000 | Loss: 0.00002800
Iteration 93/1000 | Loss: 0.00002800
Iteration 94/1000 | Loss: 0.00002800
Iteration 95/1000 | Loss: 0.00002800
Iteration 96/1000 | Loss: 0.00002800
Iteration 97/1000 | Loss: 0.00002799
Iteration 98/1000 | Loss: 0.00002799
Iteration 99/1000 | Loss: 0.00002799
Iteration 100/1000 | Loss: 0.00002799
Iteration 101/1000 | Loss: 0.00002798
Iteration 102/1000 | Loss: 0.00002798
Iteration 103/1000 | Loss: 0.00002798
Iteration 104/1000 | Loss: 0.00002798
Iteration 105/1000 | Loss: 0.00002798
Iteration 106/1000 | Loss: 0.00002798
Iteration 107/1000 | Loss: 0.00002798
Iteration 108/1000 | Loss: 0.00002798
Iteration 109/1000 | Loss: 0.00002798
Iteration 110/1000 | Loss: 0.00002798
Iteration 111/1000 | Loss: 0.00002798
Iteration 112/1000 | Loss: 0.00002797
Iteration 113/1000 | Loss: 0.00002797
Iteration 114/1000 | Loss: 0.00002797
Iteration 115/1000 | Loss: 0.00002797
Iteration 116/1000 | Loss: 0.00002797
Iteration 117/1000 | Loss: 0.00002797
Iteration 118/1000 | Loss: 0.00002797
Iteration 119/1000 | Loss: 0.00002797
Iteration 120/1000 | Loss: 0.00002797
Iteration 121/1000 | Loss: 0.00002797
Iteration 122/1000 | Loss: 0.00002797
Iteration 123/1000 | Loss: 0.00002797
Iteration 124/1000 | Loss: 0.00002797
Iteration 125/1000 | Loss: 0.00002797
Iteration 126/1000 | Loss: 0.00002797
Iteration 127/1000 | Loss: 0.00002797
Iteration 128/1000 | Loss: 0.00002796
Iteration 129/1000 | Loss: 0.00002796
Iteration 130/1000 | Loss: 0.00002796
Iteration 131/1000 | Loss: 0.00002796
Iteration 132/1000 | Loss: 0.00002796
Iteration 133/1000 | Loss: 0.00002796
Iteration 134/1000 | Loss: 0.00002796
Iteration 135/1000 | Loss: 0.00002796
Iteration 136/1000 | Loss: 0.00002796
Iteration 137/1000 | Loss: 0.00002796
Iteration 138/1000 | Loss: 0.00002796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.7959340513916686e-05, 2.7959340513916686e-05, 2.7959340513916686e-05, 2.7959340513916686e-05, 2.7959340513916686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7959340513916686e-05

Optimization complete. Final v2v error: 4.452576160430908 mm

Highest mean error: 4.730046272277832 mm for frame 150

Lowest mean error: 4.003023147583008 mm for frame 0

Saving results

Total time: 36.42895698547363
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842265
Iteration 2/25 | Loss: 0.00139759
Iteration 3/25 | Loss: 0.00110482
Iteration 4/25 | Loss: 0.00104167
Iteration 5/25 | Loss: 0.00103940
Iteration 6/25 | Loss: 0.00102283
Iteration 7/25 | Loss: 0.00101416
Iteration 8/25 | Loss: 0.00098261
Iteration 9/25 | Loss: 0.00097514
Iteration 10/25 | Loss: 0.00097301
Iteration 11/25 | Loss: 0.00097036
Iteration 12/25 | Loss: 0.00096951
Iteration 13/25 | Loss: 0.00096885
Iteration 14/25 | Loss: 0.00096868
Iteration 15/25 | Loss: 0.00096865
Iteration 16/25 | Loss: 0.00096865
Iteration 17/25 | Loss: 0.00096865
Iteration 18/25 | Loss: 0.00096865
Iteration 19/25 | Loss: 0.00096865
Iteration 20/25 | Loss: 0.00096865
Iteration 21/25 | Loss: 0.00096865
Iteration 22/25 | Loss: 0.00096865
Iteration 23/25 | Loss: 0.00096865
Iteration 24/25 | Loss: 0.00096864
Iteration 25/25 | Loss: 0.00096864

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43251324
Iteration 2/25 | Loss: 0.00069348
Iteration 3/25 | Loss: 0.00069345
Iteration 4/25 | Loss: 0.00069344
Iteration 5/25 | Loss: 0.00069344
Iteration 6/25 | Loss: 0.00069344
Iteration 7/25 | Loss: 0.00069344
Iteration 8/25 | Loss: 0.00069344
Iteration 9/25 | Loss: 0.00069344
Iteration 10/25 | Loss: 0.00069344
Iteration 11/25 | Loss: 0.00069344
Iteration 12/25 | Loss: 0.00069344
Iteration 13/25 | Loss: 0.00069344
Iteration 14/25 | Loss: 0.00069344
Iteration 15/25 | Loss: 0.00069344
Iteration 16/25 | Loss: 0.00069344
Iteration 17/25 | Loss: 0.00069344
Iteration 18/25 | Loss: 0.00069344
Iteration 19/25 | Loss: 0.00069344
Iteration 20/25 | Loss: 0.00069344
Iteration 21/25 | Loss: 0.00069344
Iteration 22/25 | Loss: 0.00069344
Iteration 23/25 | Loss: 0.00069344
Iteration 24/25 | Loss: 0.00069344
Iteration 25/25 | Loss: 0.00069344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069344
Iteration 2/1000 | Loss: 0.00006143
Iteration 3/1000 | Loss: 0.00004555
Iteration 4/1000 | Loss: 0.00003961
Iteration 5/1000 | Loss: 0.00003733
Iteration 6/1000 | Loss: 0.00003580
Iteration 7/1000 | Loss: 0.00003486
Iteration 8/1000 | Loss: 0.00003404
Iteration 9/1000 | Loss: 0.00003358
Iteration 10/1000 | Loss: 0.00003323
Iteration 11/1000 | Loss: 0.00003299
Iteration 12/1000 | Loss: 0.00003287
Iteration 13/1000 | Loss: 0.00003278
Iteration 14/1000 | Loss: 0.00003259
Iteration 15/1000 | Loss: 0.00003254
Iteration 16/1000 | Loss: 0.00003251
Iteration 17/1000 | Loss: 0.00003250
Iteration 18/1000 | Loss: 0.00003248
Iteration 19/1000 | Loss: 0.00003247
Iteration 20/1000 | Loss: 0.00003247
Iteration 21/1000 | Loss: 0.00003244
Iteration 22/1000 | Loss: 0.00003244
Iteration 23/1000 | Loss: 0.00003244
Iteration 24/1000 | Loss: 0.00003244
Iteration 25/1000 | Loss: 0.00003240
Iteration 26/1000 | Loss: 0.00003240
Iteration 27/1000 | Loss: 0.00003239
Iteration 28/1000 | Loss: 0.00003239
Iteration 29/1000 | Loss: 0.00003238
Iteration 30/1000 | Loss: 0.00003237
Iteration 31/1000 | Loss: 0.00003236
Iteration 32/1000 | Loss: 0.00003236
Iteration 33/1000 | Loss: 0.00003235
Iteration 34/1000 | Loss: 0.00003235
Iteration 35/1000 | Loss: 0.00003235
Iteration 36/1000 | Loss: 0.00003235
Iteration 37/1000 | Loss: 0.00003234
Iteration 38/1000 | Loss: 0.00003233
Iteration 39/1000 | Loss: 0.00003233
Iteration 40/1000 | Loss: 0.00003233
Iteration 41/1000 | Loss: 0.00003233
Iteration 42/1000 | Loss: 0.00003232
Iteration 43/1000 | Loss: 0.00003232
Iteration 44/1000 | Loss: 0.00003232
Iteration 45/1000 | Loss: 0.00003231
Iteration 46/1000 | Loss: 0.00003231
Iteration 47/1000 | Loss: 0.00003231
Iteration 48/1000 | Loss: 0.00003231
Iteration 49/1000 | Loss: 0.00003231
Iteration 50/1000 | Loss: 0.00003231
Iteration 51/1000 | Loss: 0.00003231
Iteration 52/1000 | Loss: 0.00003231
Iteration 53/1000 | Loss: 0.00003231
Iteration 54/1000 | Loss: 0.00003230
Iteration 55/1000 | Loss: 0.00003230
Iteration 56/1000 | Loss: 0.00003230
Iteration 57/1000 | Loss: 0.00003230
Iteration 58/1000 | Loss: 0.00003230
Iteration 59/1000 | Loss: 0.00003230
Iteration 60/1000 | Loss: 0.00003229
Iteration 61/1000 | Loss: 0.00003229
Iteration 62/1000 | Loss: 0.00003229
Iteration 63/1000 | Loss: 0.00003229
Iteration 64/1000 | Loss: 0.00003229
Iteration 65/1000 | Loss: 0.00003229
Iteration 66/1000 | Loss: 0.00003229
Iteration 67/1000 | Loss: 0.00003229
Iteration 68/1000 | Loss: 0.00003228
Iteration 69/1000 | Loss: 0.00003228
Iteration 70/1000 | Loss: 0.00003228
Iteration 71/1000 | Loss: 0.00003228
Iteration 72/1000 | Loss: 0.00003228
Iteration 73/1000 | Loss: 0.00003228
Iteration 74/1000 | Loss: 0.00003228
Iteration 75/1000 | Loss: 0.00003228
Iteration 76/1000 | Loss: 0.00003228
Iteration 77/1000 | Loss: 0.00003228
Iteration 78/1000 | Loss: 0.00003228
Iteration 79/1000 | Loss: 0.00003228
Iteration 80/1000 | Loss: 0.00003228
Iteration 81/1000 | Loss: 0.00003227
Iteration 82/1000 | Loss: 0.00003227
Iteration 83/1000 | Loss: 0.00003227
Iteration 84/1000 | Loss: 0.00003227
Iteration 85/1000 | Loss: 0.00003227
Iteration 86/1000 | Loss: 0.00003227
Iteration 87/1000 | Loss: 0.00003227
Iteration 88/1000 | Loss: 0.00003227
Iteration 89/1000 | Loss: 0.00003227
Iteration 90/1000 | Loss: 0.00003227
Iteration 91/1000 | Loss: 0.00003227
Iteration 92/1000 | Loss: 0.00003227
Iteration 93/1000 | Loss: 0.00003227
Iteration 94/1000 | Loss: 0.00003226
Iteration 95/1000 | Loss: 0.00003226
Iteration 96/1000 | Loss: 0.00003226
Iteration 97/1000 | Loss: 0.00003226
Iteration 98/1000 | Loss: 0.00003225
Iteration 99/1000 | Loss: 0.00003225
Iteration 100/1000 | Loss: 0.00003225
Iteration 101/1000 | Loss: 0.00003225
Iteration 102/1000 | Loss: 0.00003225
Iteration 103/1000 | Loss: 0.00003225
Iteration 104/1000 | Loss: 0.00003224
Iteration 105/1000 | Loss: 0.00003224
Iteration 106/1000 | Loss: 0.00003224
Iteration 107/1000 | Loss: 0.00003224
Iteration 108/1000 | Loss: 0.00003224
Iteration 109/1000 | Loss: 0.00003224
Iteration 110/1000 | Loss: 0.00003224
Iteration 111/1000 | Loss: 0.00003224
Iteration 112/1000 | Loss: 0.00003223
Iteration 113/1000 | Loss: 0.00003223
Iteration 114/1000 | Loss: 0.00003223
Iteration 115/1000 | Loss: 0.00003223
Iteration 116/1000 | Loss: 0.00003222
Iteration 117/1000 | Loss: 0.00003222
Iteration 118/1000 | Loss: 0.00003222
Iteration 119/1000 | Loss: 0.00003222
Iteration 120/1000 | Loss: 0.00003221
Iteration 121/1000 | Loss: 0.00003221
Iteration 122/1000 | Loss: 0.00003221
Iteration 123/1000 | Loss: 0.00003221
Iteration 124/1000 | Loss: 0.00003221
Iteration 125/1000 | Loss: 0.00003221
Iteration 126/1000 | Loss: 0.00003221
Iteration 127/1000 | Loss: 0.00003221
Iteration 128/1000 | Loss: 0.00003221
Iteration 129/1000 | Loss: 0.00003221
Iteration 130/1000 | Loss: 0.00003221
Iteration 131/1000 | Loss: 0.00003221
Iteration 132/1000 | Loss: 0.00003221
Iteration 133/1000 | Loss: 0.00003221
Iteration 134/1000 | Loss: 0.00003221
Iteration 135/1000 | Loss: 0.00003220
Iteration 136/1000 | Loss: 0.00003220
Iteration 137/1000 | Loss: 0.00003220
Iteration 138/1000 | Loss: 0.00003220
Iteration 139/1000 | Loss: 0.00003220
Iteration 140/1000 | Loss: 0.00003219
Iteration 141/1000 | Loss: 0.00003219
Iteration 142/1000 | Loss: 0.00003219
Iteration 143/1000 | Loss: 0.00003219
Iteration 144/1000 | Loss: 0.00003219
Iteration 145/1000 | Loss: 0.00003219
Iteration 146/1000 | Loss: 0.00003219
Iteration 147/1000 | Loss: 0.00003219
Iteration 148/1000 | Loss: 0.00003219
Iteration 149/1000 | Loss: 0.00003219
Iteration 150/1000 | Loss: 0.00003218
Iteration 151/1000 | Loss: 0.00003218
Iteration 152/1000 | Loss: 0.00003218
Iteration 153/1000 | Loss: 0.00003218
Iteration 154/1000 | Loss: 0.00003218
Iteration 155/1000 | Loss: 0.00003218
Iteration 156/1000 | Loss: 0.00003218
Iteration 157/1000 | Loss: 0.00003218
Iteration 158/1000 | Loss: 0.00003218
Iteration 159/1000 | Loss: 0.00003218
Iteration 160/1000 | Loss: 0.00003218
Iteration 161/1000 | Loss: 0.00003218
Iteration 162/1000 | Loss: 0.00003218
Iteration 163/1000 | Loss: 0.00003218
Iteration 164/1000 | Loss: 0.00003218
Iteration 165/1000 | Loss: 0.00003218
Iteration 166/1000 | Loss: 0.00003218
Iteration 167/1000 | Loss: 0.00003217
Iteration 168/1000 | Loss: 0.00003217
Iteration 169/1000 | Loss: 0.00003217
Iteration 170/1000 | Loss: 0.00003217
Iteration 171/1000 | Loss: 0.00003217
Iteration 172/1000 | Loss: 0.00003217
Iteration 173/1000 | Loss: 0.00003217
Iteration 174/1000 | Loss: 0.00003217
Iteration 175/1000 | Loss: 0.00003217
Iteration 176/1000 | Loss: 0.00003217
Iteration 177/1000 | Loss: 0.00003217
Iteration 178/1000 | Loss: 0.00003217
Iteration 179/1000 | Loss: 0.00003217
Iteration 180/1000 | Loss: 0.00003217
Iteration 181/1000 | Loss: 0.00003217
Iteration 182/1000 | Loss: 0.00003217
Iteration 183/1000 | Loss: 0.00003217
Iteration 184/1000 | Loss: 0.00003217
Iteration 185/1000 | Loss: 0.00003217
Iteration 186/1000 | Loss: 0.00003217
Iteration 187/1000 | Loss: 0.00003217
Iteration 188/1000 | Loss: 0.00003217
Iteration 189/1000 | Loss: 0.00003216
Iteration 190/1000 | Loss: 0.00003216
Iteration 191/1000 | Loss: 0.00003216
Iteration 192/1000 | Loss: 0.00003216
Iteration 193/1000 | Loss: 0.00003216
Iteration 194/1000 | Loss: 0.00003216
Iteration 195/1000 | Loss: 0.00003216
Iteration 196/1000 | Loss: 0.00003216
Iteration 197/1000 | Loss: 0.00003216
Iteration 198/1000 | Loss: 0.00003216
Iteration 199/1000 | Loss: 0.00003216
Iteration 200/1000 | Loss: 0.00003216
Iteration 201/1000 | Loss: 0.00003216
Iteration 202/1000 | Loss: 0.00003216
Iteration 203/1000 | Loss: 0.00003216
Iteration 204/1000 | Loss: 0.00003216
Iteration 205/1000 | Loss: 0.00003216
Iteration 206/1000 | Loss: 0.00003216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [3.215841206838377e-05, 3.215841206838377e-05, 3.215841206838377e-05, 3.215841206838377e-05, 3.215841206838377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.215841206838377e-05

Optimization complete. Final v2v error: 4.725424766540527 mm

Highest mean error: 5.408837795257568 mm for frame 125

Lowest mean error: 4.091394424438477 mm for frame 2

Saving results

Total time: 56.93443703651428
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446156
Iteration 2/25 | Loss: 0.00099985
Iteration 3/25 | Loss: 0.00081541
Iteration 4/25 | Loss: 0.00078683
Iteration 5/25 | Loss: 0.00077837
Iteration 6/25 | Loss: 0.00077550
Iteration 7/25 | Loss: 0.00077482
Iteration 8/25 | Loss: 0.00077482
Iteration 9/25 | Loss: 0.00077482
Iteration 10/25 | Loss: 0.00077482
Iteration 11/25 | Loss: 0.00077482
Iteration 12/25 | Loss: 0.00077482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007748187053948641, 0.0007748187053948641, 0.0007748187053948641, 0.0007748187053948641, 0.0007748187053948641]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007748187053948641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48492217
Iteration 2/25 | Loss: 0.00070891
Iteration 3/25 | Loss: 0.00070891
Iteration 4/25 | Loss: 0.00070891
Iteration 5/25 | Loss: 0.00070891
Iteration 6/25 | Loss: 0.00070891
Iteration 7/25 | Loss: 0.00070891
Iteration 8/25 | Loss: 0.00070891
Iteration 9/25 | Loss: 0.00070891
Iteration 10/25 | Loss: 0.00070891
Iteration 11/25 | Loss: 0.00070891
Iteration 12/25 | Loss: 0.00070891
Iteration 13/25 | Loss: 0.00070891
Iteration 14/25 | Loss: 0.00070891
Iteration 15/25 | Loss: 0.00070891
Iteration 16/25 | Loss: 0.00070891
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007089062128216028, 0.0007089062128216028, 0.0007089062128216028, 0.0007089062128216028, 0.0007089062128216028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007089062128216028

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070891
Iteration 2/1000 | Loss: 0.00002794
Iteration 3/1000 | Loss: 0.00002422
Iteration 4/1000 | Loss: 0.00002268
Iteration 5/1000 | Loss: 0.00002156
Iteration 6/1000 | Loss: 0.00002084
Iteration 7/1000 | Loss: 0.00002045
Iteration 8/1000 | Loss: 0.00002025
Iteration 9/1000 | Loss: 0.00002024
Iteration 10/1000 | Loss: 0.00002011
Iteration 11/1000 | Loss: 0.00002010
Iteration 12/1000 | Loss: 0.00002010
Iteration 13/1000 | Loss: 0.00002010
Iteration 14/1000 | Loss: 0.00002007
Iteration 15/1000 | Loss: 0.00002006
Iteration 16/1000 | Loss: 0.00002004
Iteration 17/1000 | Loss: 0.00002004
Iteration 18/1000 | Loss: 0.00002003
Iteration 19/1000 | Loss: 0.00002002
Iteration 20/1000 | Loss: 0.00002001
Iteration 21/1000 | Loss: 0.00002001
Iteration 22/1000 | Loss: 0.00002000
Iteration 23/1000 | Loss: 0.00002000
Iteration 24/1000 | Loss: 0.00001999
Iteration 25/1000 | Loss: 0.00001999
Iteration 26/1000 | Loss: 0.00001998
Iteration 27/1000 | Loss: 0.00001998
Iteration 28/1000 | Loss: 0.00001997
Iteration 29/1000 | Loss: 0.00001997
Iteration 30/1000 | Loss: 0.00001996
Iteration 31/1000 | Loss: 0.00001996
Iteration 32/1000 | Loss: 0.00001996
Iteration 33/1000 | Loss: 0.00001996
Iteration 34/1000 | Loss: 0.00001996
Iteration 35/1000 | Loss: 0.00001996
Iteration 36/1000 | Loss: 0.00001996
Iteration 37/1000 | Loss: 0.00001996
Iteration 38/1000 | Loss: 0.00001996
Iteration 39/1000 | Loss: 0.00001995
Iteration 40/1000 | Loss: 0.00001995
Iteration 41/1000 | Loss: 0.00001995
Iteration 42/1000 | Loss: 0.00001994
Iteration 43/1000 | Loss: 0.00001994
Iteration 44/1000 | Loss: 0.00001994
Iteration 45/1000 | Loss: 0.00001994
Iteration 46/1000 | Loss: 0.00001993
Iteration 47/1000 | Loss: 0.00001993
Iteration 48/1000 | Loss: 0.00001993
Iteration 49/1000 | Loss: 0.00001993
Iteration 50/1000 | Loss: 0.00001993
Iteration 51/1000 | Loss: 0.00001993
Iteration 52/1000 | Loss: 0.00001993
Iteration 53/1000 | Loss: 0.00001992
Iteration 54/1000 | Loss: 0.00001992
Iteration 55/1000 | Loss: 0.00001992
Iteration 56/1000 | Loss: 0.00001992
Iteration 57/1000 | Loss: 0.00001992
Iteration 58/1000 | Loss: 0.00001991
Iteration 59/1000 | Loss: 0.00001991
Iteration 60/1000 | Loss: 0.00001991
Iteration 61/1000 | Loss: 0.00001991
Iteration 62/1000 | Loss: 0.00001991
Iteration 63/1000 | Loss: 0.00001991
Iteration 64/1000 | Loss: 0.00001990
Iteration 65/1000 | Loss: 0.00001990
Iteration 66/1000 | Loss: 0.00001990
Iteration 67/1000 | Loss: 0.00001990
Iteration 68/1000 | Loss: 0.00001989
Iteration 69/1000 | Loss: 0.00001989
Iteration 70/1000 | Loss: 0.00001989
Iteration 71/1000 | Loss: 0.00001989
Iteration 72/1000 | Loss: 0.00001988
Iteration 73/1000 | Loss: 0.00001988
Iteration 74/1000 | Loss: 0.00001988
Iteration 75/1000 | Loss: 0.00001987
Iteration 76/1000 | Loss: 0.00001987
Iteration 77/1000 | Loss: 0.00001987
Iteration 78/1000 | Loss: 0.00001987
Iteration 79/1000 | Loss: 0.00001986
Iteration 80/1000 | Loss: 0.00001986
Iteration 81/1000 | Loss: 0.00001986
Iteration 82/1000 | Loss: 0.00001986
Iteration 83/1000 | Loss: 0.00001985
Iteration 84/1000 | Loss: 0.00001985
Iteration 85/1000 | Loss: 0.00001985
Iteration 86/1000 | Loss: 0.00001985
Iteration 87/1000 | Loss: 0.00001984
Iteration 88/1000 | Loss: 0.00001984
Iteration 89/1000 | Loss: 0.00001984
Iteration 90/1000 | Loss: 0.00001984
Iteration 91/1000 | Loss: 0.00001983
Iteration 92/1000 | Loss: 0.00001983
Iteration 93/1000 | Loss: 0.00001983
Iteration 94/1000 | Loss: 0.00001983
Iteration 95/1000 | Loss: 0.00001983
Iteration 96/1000 | Loss: 0.00001983
Iteration 97/1000 | Loss: 0.00001983
Iteration 98/1000 | Loss: 0.00001982
Iteration 99/1000 | Loss: 0.00001982
Iteration 100/1000 | Loss: 0.00001982
Iteration 101/1000 | Loss: 0.00001982
Iteration 102/1000 | Loss: 0.00001981
Iteration 103/1000 | Loss: 0.00001981
Iteration 104/1000 | Loss: 0.00001981
Iteration 105/1000 | Loss: 0.00001981
Iteration 106/1000 | Loss: 0.00001981
Iteration 107/1000 | Loss: 0.00001981
Iteration 108/1000 | Loss: 0.00001981
Iteration 109/1000 | Loss: 0.00001981
Iteration 110/1000 | Loss: 0.00001981
Iteration 111/1000 | Loss: 0.00001981
Iteration 112/1000 | Loss: 0.00001981
Iteration 113/1000 | Loss: 0.00001980
Iteration 114/1000 | Loss: 0.00001980
Iteration 115/1000 | Loss: 0.00001980
Iteration 116/1000 | Loss: 0.00001980
Iteration 117/1000 | Loss: 0.00001980
Iteration 118/1000 | Loss: 0.00001980
Iteration 119/1000 | Loss: 0.00001980
Iteration 120/1000 | Loss: 0.00001980
Iteration 121/1000 | Loss: 0.00001980
Iteration 122/1000 | Loss: 0.00001979
Iteration 123/1000 | Loss: 0.00001979
Iteration 124/1000 | Loss: 0.00001979
Iteration 125/1000 | Loss: 0.00001979
Iteration 126/1000 | Loss: 0.00001979
Iteration 127/1000 | Loss: 0.00001979
Iteration 128/1000 | Loss: 0.00001979
Iteration 129/1000 | Loss: 0.00001979
Iteration 130/1000 | Loss: 0.00001979
Iteration 131/1000 | Loss: 0.00001979
Iteration 132/1000 | Loss: 0.00001979
Iteration 133/1000 | Loss: 0.00001979
Iteration 134/1000 | Loss: 0.00001979
Iteration 135/1000 | Loss: 0.00001979
Iteration 136/1000 | Loss: 0.00001979
Iteration 137/1000 | Loss: 0.00001979
Iteration 138/1000 | Loss: 0.00001979
Iteration 139/1000 | Loss: 0.00001979
Iteration 140/1000 | Loss: 0.00001979
Iteration 141/1000 | Loss: 0.00001979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.9789424186456017e-05, 1.9789424186456017e-05, 1.9789424186456017e-05, 1.9789424186456017e-05, 1.9789424186456017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9789424186456017e-05

Optimization complete. Final v2v error: 3.754870891571045 mm

Highest mean error: 4.739592552185059 mm for frame 224

Lowest mean error: 2.964325189590454 mm for frame 95

Saving results

Total time: 38.2038049697876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885609
Iteration 2/25 | Loss: 0.00170466
Iteration 3/25 | Loss: 0.00114499
Iteration 4/25 | Loss: 0.00112406
Iteration 5/25 | Loss: 0.00111702
Iteration 6/25 | Loss: 0.00111561
Iteration 7/25 | Loss: 0.00111561
Iteration 8/25 | Loss: 0.00111561
Iteration 9/25 | Loss: 0.00111561
Iteration 10/25 | Loss: 0.00111561
Iteration 11/25 | Loss: 0.00111561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011156072141602635, 0.0011156072141602635, 0.0011156072141602635, 0.0011156072141602635, 0.0011156072141602635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011156072141602635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.40046442
Iteration 2/25 | Loss: 0.00042746
Iteration 3/25 | Loss: 0.00042745
Iteration 4/25 | Loss: 0.00042745
Iteration 5/25 | Loss: 0.00042745
Iteration 6/25 | Loss: 0.00042745
Iteration 7/25 | Loss: 0.00042745
Iteration 8/25 | Loss: 0.00042745
Iteration 9/25 | Loss: 0.00042745
Iteration 10/25 | Loss: 0.00042745
Iteration 11/25 | Loss: 0.00042745
Iteration 12/25 | Loss: 0.00042745
Iteration 13/25 | Loss: 0.00042745
Iteration 14/25 | Loss: 0.00042745
Iteration 15/25 | Loss: 0.00042745
Iteration 16/25 | Loss: 0.00042745
Iteration 17/25 | Loss: 0.00042745
Iteration 18/25 | Loss: 0.00042745
Iteration 19/25 | Loss: 0.00042745
Iteration 20/25 | Loss: 0.00042745
Iteration 21/25 | Loss: 0.00042745
Iteration 22/25 | Loss: 0.00042745
Iteration 23/25 | Loss: 0.00042745
Iteration 24/25 | Loss: 0.00042745
Iteration 25/25 | Loss: 0.00042745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042745
Iteration 2/1000 | Loss: 0.00007659
Iteration 3/1000 | Loss: 0.00006358
Iteration 4/1000 | Loss: 0.00005548
Iteration 5/1000 | Loss: 0.00005245
Iteration 6/1000 | Loss: 0.00005086
Iteration 7/1000 | Loss: 0.00004959
Iteration 8/1000 | Loss: 0.00004879
Iteration 9/1000 | Loss: 0.00004823
Iteration 10/1000 | Loss: 0.00004782
Iteration 11/1000 | Loss: 0.00004740
Iteration 12/1000 | Loss: 0.00004711
Iteration 13/1000 | Loss: 0.00004693
Iteration 14/1000 | Loss: 0.00004689
Iteration 15/1000 | Loss: 0.00004685
Iteration 16/1000 | Loss: 0.00004678
Iteration 17/1000 | Loss: 0.00004677
Iteration 18/1000 | Loss: 0.00004677
Iteration 19/1000 | Loss: 0.00004672
Iteration 20/1000 | Loss: 0.00004669
Iteration 21/1000 | Loss: 0.00004669
Iteration 22/1000 | Loss: 0.00004668
Iteration 23/1000 | Loss: 0.00004665
Iteration 24/1000 | Loss: 0.00004665
Iteration 25/1000 | Loss: 0.00004665
Iteration 26/1000 | Loss: 0.00004665
Iteration 27/1000 | Loss: 0.00004665
Iteration 28/1000 | Loss: 0.00004665
Iteration 29/1000 | Loss: 0.00004665
Iteration 30/1000 | Loss: 0.00004665
Iteration 31/1000 | Loss: 0.00004665
Iteration 32/1000 | Loss: 0.00004665
Iteration 33/1000 | Loss: 0.00004664
Iteration 34/1000 | Loss: 0.00004664
Iteration 35/1000 | Loss: 0.00004664
Iteration 36/1000 | Loss: 0.00004661
Iteration 37/1000 | Loss: 0.00004658
Iteration 38/1000 | Loss: 0.00004650
Iteration 39/1000 | Loss: 0.00004650
Iteration 40/1000 | Loss: 0.00004646
Iteration 41/1000 | Loss: 0.00004644
Iteration 42/1000 | Loss: 0.00004644
Iteration 43/1000 | Loss: 0.00004643
Iteration 44/1000 | Loss: 0.00004640
Iteration 45/1000 | Loss: 0.00004640
Iteration 46/1000 | Loss: 0.00004640
Iteration 47/1000 | Loss: 0.00004640
Iteration 48/1000 | Loss: 0.00004640
Iteration 49/1000 | Loss: 0.00004640
Iteration 50/1000 | Loss: 0.00004640
Iteration 51/1000 | Loss: 0.00004640
Iteration 52/1000 | Loss: 0.00004640
Iteration 53/1000 | Loss: 0.00004640
Iteration 54/1000 | Loss: 0.00004640
Iteration 55/1000 | Loss: 0.00004639
Iteration 56/1000 | Loss: 0.00004639
Iteration 57/1000 | Loss: 0.00004639
Iteration 58/1000 | Loss: 0.00004639
Iteration 59/1000 | Loss: 0.00004639
Iteration 60/1000 | Loss: 0.00004639
Iteration 61/1000 | Loss: 0.00004638
Iteration 62/1000 | Loss: 0.00004638
Iteration 63/1000 | Loss: 0.00004638
Iteration 64/1000 | Loss: 0.00004638
Iteration 65/1000 | Loss: 0.00004638
Iteration 66/1000 | Loss: 0.00004638
Iteration 67/1000 | Loss: 0.00004638
Iteration 68/1000 | Loss: 0.00004637
Iteration 69/1000 | Loss: 0.00004637
Iteration 70/1000 | Loss: 0.00004637
Iteration 71/1000 | Loss: 0.00004637
Iteration 72/1000 | Loss: 0.00004637
Iteration 73/1000 | Loss: 0.00004636
Iteration 74/1000 | Loss: 0.00004636
Iteration 75/1000 | Loss: 0.00004636
Iteration 76/1000 | Loss: 0.00004635
Iteration 77/1000 | Loss: 0.00004635
Iteration 78/1000 | Loss: 0.00004635
Iteration 79/1000 | Loss: 0.00004635
Iteration 80/1000 | Loss: 0.00004635
Iteration 81/1000 | Loss: 0.00004635
Iteration 82/1000 | Loss: 0.00004635
Iteration 83/1000 | Loss: 0.00004635
Iteration 84/1000 | Loss: 0.00004635
Iteration 85/1000 | Loss: 0.00004635
Iteration 86/1000 | Loss: 0.00004635
Iteration 87/1000 | Loss: 0.00004635
Iteration 88/1000 | Loss: 0.00004635
Iteration 89/1000 | Loss: 0.00004635
Iteration 90/1000 | Loss: 0.00004634
Iteration 91/1000 | Loss: 0.00004634
Iteration 92/1000 | Loss: 0.00004634
Iteration 93/1000 | Loss: 0.00004634
Iteration 94/1000 | Loss: 0.00004634
Iteration 95/1000 | Loss: 0.00004634
Iteration 96/1000 | Loss: 0.00004634
Iteration 97/1000 | Loss: 0.00004634
Iteration 98/1000 | Loss: 0.00004634
Iteration 99/1000 | Loss: 0.00004634
Iteration 100/1000 | Loss: 0.00004634
Iteration 101/1000 | Loss: 0.00004634
Iteration 102/1000 | Loss: 0.00004634
Iteration 103/1000 | Loss: 0.00004634
Iteration 104/1000 | Loss: 0.00004634
Iteration 105/1000 | Loss: 0.00004634
Iteration 106/1000 | Loss: 0.00004633
Iteration 107/1000 | Loss: 0.00004633
Iteration 108/1000 | Loss: 0.00004633
Iteration 109/1000 | Loss: 0.00004633
Iteration 110/1000 | Loss: 0.00004633
Iteration 111/1000 | Loss: 0.00004633
Iteration 112/1000 | Loss: 0.00004633
Iteration 113/1000 | Loss: 0.00004633
Iteration 114/1000 | Loss: 0.00004633
Iteration 115/1000 | Loss: 0.00004633
Iteration 116/1000 | Loss: 0.00004633
Iteration 117/1000 | Loss: 0.00004633
Iteration 118/1000 | Loss: 0.00004633
Iteration 119/1000 | Loss: 0.00004633
Iteration 120/1000 | Loss: 0.00004633
Iteration 121/1000 | Loss: 0.00004633
Iteration 122/1000 | Loss: 0.00004633
Iteration 123/1000 | Loss: 0.00004633
Iteration 124/1000 | Loss: 0.00004633
Iteration 125/1000 | Loss: 0.00004633
Iteration 126/1000 | Loss: 0.00004633
Iteration 127/1000 | Loss: 0.00004633
Iteration 128/1000 | Loss: 0.00004633
Iteration 129/1000 | Loss: 0.00004633
Iteration 130/1000 | Loss: 0.00004633
Iteration 131/1000 | Loss: 0.00004633
Iteration 132/1000 | Loss: 0.00004633
Iteration 133/1000 | Loss: 0.00004633
Iteration 134/1000 | Loss: 0.00004633
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [4.6332046622410417e-05, 4.6332046622410417e-05, 4.6332046622410417e-05, 4.6332046622410417e-05, 4.6332046622410417e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.6332046622410417e-05

Optimization complete. Final v2v error: 5.551877498626709 mm

Highest mean error: 5.732137680053711 mm for frame 34

Lowest mean error: 5.171814918518066 mm for frame 6

Saving results

Total time: 39.916226387023926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00602755
Iteration 2/25 | Loss: 0.00123710
Iteration 3/25 | Loss: 0.00109800
Iteration 4/25 | Loss: 0.00105223
Iteration 5/25 | Loss: 0.00104274
Iteration 6/25 | Loss: 0.00103976
Iteration 7/25 | Loss: 0.00103889
Iteration 8/25 | Loss: 0.00103889
Iteration 9/25 | Loss: 0.00103889
Iteration 10/25 | Loss: 0.00103889
Iteration 11/25 | Loss: 0.00103889
Iteration 12/25 | Loss: 0.00103889
Iteration 13/25 | Loss: 0.00103889
Iteration 14/25 | Loss: 0.00103889
Iteration 15/25 | Loss: 0.00103889
Iteration 16/25 | Loss: 0.00103889
Iteration 17/25 | Loss: 0.00103889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010388946393504739, 0.0010388946393504739, 0.0010388946393504739, 0.0010388946393504739, 0.0010388946393504739]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010388946393504739

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10514677
Iteration 2/25 | Loss: 0.00085171
Iteration 3/25 | Loss: 0.00085171
Iteration 4/25 | Loss: 0.00085171
Iteration 5/25 | Loss: 0.00085171
Iteration 6/25 | Loss: 0.00085171
Iteration 7/25 | Loss: 0.00085171
Iteration 8/25 | Loss: 0.00085171
Iteration 9/25 | Loss: 0.00085171
Iteration 10/25 | Loss: 0.00085171
Iteration 11/25 | Loss: 0.00085171
Iteration 12/25 | Loss: 0.00085171
Iteration 13/25 | Loss: 0.00085171
Iteration 14/25 | Loss: 0.00085171
Iteration 15/25 | Loss: 0.00085171
Iteration 16/25 | Loss: 0.00085171
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008517084643244743, 0.0008517084643244743, 0.0008517084643244743, 0.0008517084643244743, 0.0008517084643244743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008517084643244743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085171
Iteration 2/1000 | Loss: 0.00006525
Iteration 3/1000 | Loss: 0.00005148
Iteration 4/1000 | Loss: 0.00004588
Iteration 5/1000 | Loss: 0.00004269
Iteration 6/1000 | Loss: 0.00004140
Iteration 7/1000 | Loss: 0.00003940
Iteration 8/1000 | Loss: 0.00003888
Iteration 9/1000 | Loss: 0.00003849
Iteration 10/1000 | Loss: 0.00003815
Iteration 11/1000 | Loss: 0.00003814
Iteration 12/1000 | Loss: 0.00003798
Iteration 13/1000 | Loss: 0.00003797
Iteration 14/1000 | Loss: 0.00003797
Iteration 15/1000 | Loss: 0.00003797
Iteration 16/1000 | Loss: 0.00003797
Iteration 17/1000 | Loss: 0.00003797
Iteration 18/1000 | Loss: 0.00003797
Iteration 19/1000 | Loss: 0.00003797
Iteration 20/1000 | Loss: 0.00003797
Iteration 21/1000 | Loss: 0.00003797
Iteration 22/1000 | Loss: 0.00003797
Iteration 23/1000 | Loss: 0.00003797
Iteration 24/1000 | Loss: 0.00003797
Iteration 25/1000 | Loss: 0.00003797
Iteration 26/1000 | Loss: 0.00003796
Iteration 27/1000 | Loss: 0.00003796
Iteration 28/1000 | Loss: 0.00003796
Iteration 29/1000 | Loss: 0.00003796
Iteration 30/1000 | Loss: 0.00003795
Iteration 31/1000 | Loss: 0.00003795
Iteration 32/1000 | Loss: 0.00003795
Iteration 33/1000 | Loss: 0.00003795
Iteration 34/1000 | Loss: 0.00003795
Iteration 35/1000 | Loss: 0.00003794
Iteration 36/1000 | Loss: 0.00003794
Iteration 37/1000 | Loss: 0.00003794
Iteration 38/1000 | Loss: 0.00003794
Iteration 39/1000 | Loss: 0.00003794
Iteration 40/1000 | Loss: 0.00003794
Iteration 41/1000 | Loss: 0.00003794
Iteration 42/1000 | Loss: 0.00003794
Iteration 43/1000 | Loss: 0.00003794
Iteration 44/1000 | Loss: 0.00003794
Iteration 45/1000 | Loss: 0.00003793
Iteration 46/1000 | Loss: 0.00003792
Iteration 47/1000 | Loss: 0.00003790
Iteration 48/1000 | Loss: 0.00003789
Iteration 49/1000 | Loss: 0.00003789
Iteration 50/1000 | Loss: 0.00003788
Iteration 51/1000 | Loss: 0.00003788
Iteration 52/1000 | Loss: 0.00003788
Iteration 53/1000 | Loss: 0.00003788
Iteration 54/1000 | Loss: 0.00003788
Iteration 55/1000 | Loss: 0.00003788
Iteration 56/1000 | Loss: 0.00003788
Iteration 57/1000 | Loss: 0.00003788
Iteration 58/1000 | Loss: 0.00003788
Iteration 59/1000 | Loss: 0.00003788
Iteration 60/1000 | Loss: 0.00003788
Iteration 61/1000 | Loss: 0.00003788
Iteration 62/1000 | Loss: 0.00003788
Iteration 63/1000 | Loss: 0.00003788
Iteration 64/1000 | Loss: 0.00003788
Iteration 65/1000 | Loss: 0.00003787
Iteration 66/1000 | Loss: 0.00003787
Iteration 67/1000 | Loss: 0.00003787
Iteration 68/1000 | Loss: 0.00003787
Iteration 69/1000 | Loss: 0.00003787
Iteration 70/1000 | Loss: 0.00003787
Iteration 71/1000 | Loss: 0.00003787
Iteration 72/1000 | Loss: 0.00003787
Iteration 73/1000 | Loss: 0.00003787
Iteration 74/1000 | Loss: 0.00003787
Iteration 75/1000 | Loss: 0.00003787
Iteration 76/1000 | Loss: 0.00003786
Iteration 77/1000 | Loss: 0.00003786
Iteration 78/1000 | Loss: 0.00003786
Iteration 79/1000 | Loss: 0.00003786
Iteration 80/1000 | Loss: 0.00003786
Iteration 81/1000 | Loss: 0.00003786
Iteration 82/1000 | Loss: 0.00003786
Iteration 83/1000 | Loss: 0.00003786
Iteration 84/1000 | Loss: 0.00003786
Iteration 85/1000 | Loss: 0.00003786
Iteration 86/1000 | Loss: 0.00003786
Iteration 87/1000 | Loss: 0.00003786
Iteration 88/1000 | Loss: 0.00003786
Iteration 89/1000 | Loss: 0.00003786
Iteration 90/1000 | Loss: 0.00003786
Iteration 91/1000 | Loss: 0.00003786
Iteration 92/1000 | Loss: 0.00003786
Iteration 93/1000 | Loss: 0.00003786
Iteration 94/1000 | Loss: 0.00003786
Iteration 95/1000 | Loss: 0.00003786
Iteration 96/1000 | Loss: 0.00003786
Iteration 97/1000 | Loss: 0.00003786
Iteration 98/1000 | Loss: 0.00003786
Iteration 99/1000 | Loss: 0.00003786
Iteration 100/1000 | Loss: 0.00003786
Iteration 101/1000 | Loss: 0.00003786
Iteration 102/1000 | Loss: 0.00003786
Iteration 103/1000 | Loss: 0.00003786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [3.7858695577597246e-05, 3.7858695577597246e-05, 3.7858695577597246e-05, 3.7858695577597246e-05, 3.7858695577597246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7858695577597246e-05

Optimization complete. Final v2v error: 5.07523775100708 mm

Highest mean error: 5.169186592102051 mm for frame 18

Lowest mean error: 4.88196325302124 mm for frame 140

Saving results

Total time: 30.350335597991943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881914
Iteration 2/25 | Loss: 0.00118263
Iteration 3/25 | Loss: 0.00102904
Iteration 4/25 | Loss: 0.00098931
Iteration 5/25 | Loss: 0.00097954
Iteration 6/25 | Loss: 0.00097722
Iteration 7/25 | Loss: 0.00097702
Iteration 8/25 | Loss: 0.00097702
Iteration 9/25 | Loss: 0.00097702
Iteration 10/25 | Loss: 0.00097702
Iteration 11/25 | Loss: 0.00097702
Iteration 12/25 | Loss: 0.00097702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009770231554284692, 0.0009770231554284692, 0.0009770231554284692, 0.0009770231554284692, 0.0009770231554284692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009770231554284692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99840969
Iteration 2/25 | Loss: 0.00079426
Iteration 3/25 | Loss: 0.00079425
Iteration 4/25 | Loss: 0.00079425
Iteration 5/25 | Loss: 0.00079425
Iteration 6/25 | Loss: 0.00079425
Iteration 7/25 | Loss: 0.00079425
Iteration 8/25 | Loss: 0.00079425
Iteration 9/25 | Loss: 0.00079425
Iteration 10/25 | Loss: 0.00079425
Iteration 11/25 | Loss: 0.00079425
Iteration 12/25 | Loss: 0.00079425
Iteration 13/25 | Loss: 0.00079425
Iteration 14/25 | Loss: 0.00079425
Iteration 15/25 | Loss: 0.00079425
Iteration 16/25 | Loss: 0.00079425
Iteration 17/25 | Loss: 0.00079425
Iteration 18/25 | Loss: 0.00079425
Iteration 19/25 | Loss: 0.00079425
Iteration 20/25 | Loss: 0.00079425
Iteration 21/25 | Loss: 0.00079425
Iteration 22/25 | Loss: 0.00079425
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007942465017549694, 0.0007942465017549694, 0.0007942465017549694, 0.0007942465017549694, 0.0007942465017549694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007942465017549694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079425
Iteration 2/1000 | Loss: 0.00006757
Iteration 3/1000 | Loss: 0.00005457
Iteration 4/1000 | Loss: 0.00005054
Iteration 5/1000 | Loss: 0.00004661
Iteration 6/1000 | Loss: 0.00004440
Iteration 7/1000 | Loss: 0.00004267
Iteration 8/1000 | Loss: 0.00004210
Iteration 9/1000 | Loss: 0.00004170
Iteration 10/1000 | Loss: 0.00004140
Iteration 11/1000 | Loss: 0.00004117
Iteration 12/1000 | Loss: 0.00004111
Iteration 13/1000 | Loss: 0.00004109
Iteration 14/1000 | Loss: 0.00004109
Iteration 15/1000 | Loss: 0.00004109
Iteration 16/1000 | Loss: 0.00004108
Iteration 17/1000 | Loss: 0.00004106
Iteration 18/1000 | Loss: 0.00004105
Iteration 19/1000 | Loss: 0.00004105
Iteration 20/1000 | Loss: 0.00004105
Iteration 21/1000 | Loss: 0.00004105
Iteration 22/1000 | Loss: 0.00004105
Iteration 23/1000 | Loss: 0.00004105
Iteration 24/1000 | Loss: 0.00004105
Iteration 25/1000 | Loss: 0.00004105
Iteration 26/1000 | Loss: 0.00004104
Iteration 27/1000 | Loss: 0.00004103
Iteration 28/1000 | Loss: 0.00004103
Iteration 29/1000 | Loss: 0.00004103
Iteration 30/1000 | Loss: 0.00004103
Iteration 31/1000 | Loss: 0.00004103
Iteration 32/1000 | Loss: 0.00004103
Iteration 33/1000 | Loss: 0.00004103
Iteration 34/1000 | Loss: 0.00004103
Iteration 35/1000 | Loss: 0.00004103
Iteration 36/1000 | Loss: 0.00004103
Iteration 37/1000 | Loss: 0.00004103
Iteration 38/1000 | Loss: 0.00004103
Iteration 39/1000 | Loss: 0.00004103
Iteration 40/1000 | Loss: 0.00004103
Iteration 41/1000 | Loss: 0.00004103
Iteration 42/1000 | Loss: 0.00004103
Iteration 43/1000 | Loss: 0.00004103
Iteration 44/1000 | Loss: 0.00004103
Iteration 45/1000 | Loss: 0.00004103
Iteration 46/1000 | Loss: 0.00004103
Iteration 47/1000 | Loss: 0.00004103
Iteration 48/1000 | Loss: 0.00004103
Iteration 49/1000 | Loss: 0.00004103
Iteration 50/1000 | Loss: 0.00004103
Iteration 51/1000 | Loss: 0.00004103
Iteration 52/1000 | Loss: 0.00004102
Iteration 53/1000 | Loss: 0.00004102
Iteration 54/1000 | Loss: 0.00004102
Iteration 55/1000 | Loss: 0.00004102
Iteration 56/1000 | Loss: 0.00004102
Iteration 57/1000 | Loss: 0.00004101
Iteration 58/1000 | Loss: 0.00004101
Iteration 59/1000 | Loss: 0.00004101
Iteration 60/1000 | Loss: 0.00004101
Iteration 61/1000 | Loss: 0.00004101
Iteration 62/1000 | Loss: 0.00004101
Iteration 63/1000 | Loss: 0.00004101
Iteration 64/1000 | Loss: 0.00004101
Iteration 65/1000 | Loss: 0.00004101
Iteration 66/1000 | Loss: 0.00004101
Iteration 67/1000 | Loss: 0.00004100
Iteration 68/1000 | Loss: 0.00004100
Iteration 69/1000 | Loss: 0.00004100
Iteration 70/1000 | Loss: 0.00004100
Iteration 71/1000 | Loss: 0.00004100
Iteration 72/1000 | Loss: 0.00004100
Iteration 73/1000 | Loss: 0.00004100
Iteration 74/1000 | Loss: 0.00004100
Iteration 75/1000 | Loss: 0.00004100
Iteration 76/1000 | Loss: 0.00004100
Iteration 77/1000 | Loss: 0.00004100
Iteration 78/1000 | Loss: 0.00004100
Iteration 79/1000 | Loss: 0.00004100
Iteration 80/1000 | Loss: 0.00004099
Iteration 81/1000 | Loss: 0.00004099
Iteration 82/1000 | Loss: 0.00004099
Iteration 83/1000 | Loss: 0.00004099
Iteration 84/1000 | Loss: 0.00004099
Iteration 85/1000 | Loss: 0.00004099
Iteration 86/1000 | Loss: 0.00004099
Iteration 87/1000 | Loss: 0.00004099
Iteration 88/1000 | Loss: 0.00004099
Iteration 89/1000 | Loss: 0.00004099
Iteration 90/1000 | Loss: 0.00004099
Iteration 91/1000 | Loss: 0.00004099
Iteration 92/1000 | Loss: 0.00004099
Iteration 93/1000 | Loss: 0.00004099
Iteration 94/1000 | Loss: 0.00004098
Iteration 95/1000 | Loss: 0.00004098
Iteration 96/1000 | Loss: 0.00004098
Iteration 97/1000 | Loss: 0.00004098
Iteration 98/1000 | Loss: 0.00004098
Iteration 99/1000 | Loss: 0.00004098
Iteration 100/1000 | Loss: 0.00004098
Iteration 101/1000 | Loss: 0.00004098
Iteration 102/1000 | Loss: 0.00004098
Iteration 103/1000 | Loss: 0.00004098
Iteration 104/1000 | Loss: 0.00004098
Iteration 105/1000 | Loss: 0.00004098
Iteration 106/1000 | Loss: 0.00004098
Iteration 107/1000 | Loss: 0.00004097
Iteration 108/1000 | Loss: 0.00004097
Iteration 109/1000 | Loss: 0.00004097
Iteration 110/1000 | Loss: 0.00004097
Iteration 111/1000 | Loss: 0.00004097
Iteration 112/1000 | Loss: 0.00004097
Iteration 113/1000 | Loss: 0.00004097
Iteration 114/1000 | Loss: 0.00004097
Iteration 115/1000 | Loss: 0.00004097
Iteration 116/1000 | Loss: 0.00004097
Iteration 117/1000 | Loss: 0.00004097
Iteration 118/1000 | Loss: 0.00004097
Iteration 119/1000 | Loss: 0.00004097
Iteration 120/1000 | Loss: 0.00004097
Iteration 121/1000 | Loss: 0.00004097
Iteration 122/1000 | Loss: 0.00004097
Iteration 123/1000 | Loss: 0.00004097
Iteration 124/1000 | Loss: 0.00004097
Iteration 125/1000 | Loss: 0.00004097
Iteration 126/1000 | Loss: 0.00004097
Iteration 127/1000 | Loss: 0.00004097
Iteration 128/1000 | Loss: 0.00004097
Iteration 129/1000 | Loss: 0.00004097
Iteration 130/1000 | Loss: 0.00004097
Iteration 131/1000 | Loss: 0.00004097
Iteration 132/1000 | Loss: 0.00004097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [4.09661115554627e-05, 4.09661115554627e-05, 4.09661115554627e-05, 4.09661115554627e-05, 4.09661115554627e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.09661115554627e-05

Optimization complete. Final v2v error: 5.326793193817139 mm

Highest mean error: 5.62544059753418 mm for frame 29

Lowest mean error: 5.1024370193481445 mm for frame 15

Saving results

Total time: 33.65654277801514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904681
Iteration 2/25 | Loss: 0.00109553
Iteration 3/25 | Loss: 0.00082644
Iteration 4/25 | Loss: 0.00073947
Iteration 5/25 | Loss: 0.00071076
Iteration 6/25 | Loss: 0.00070239
Iteration 7/25 | Loss: 0.00069546
Iteration 8/25 | Loss: 0.00069443
Iteration 9/25 | Loss: 0.00069403
Iteration 10/25 | Loss: 0.00069339
Iteration 11/25 | Loss: 0.00069368
Iteration 12/25 | Loss: 0.00069275
Iteration 13/25 | Loss: 0.00069192
Iteration 14/25 | Loss: 0.00069221
Iteration 15/25 | Loss: 0.00069178
Iteration 16/25 | Loss: 0.00069207
Iteration 17/25 | Loss: 0.00069162
Iteration 18/25 | Loss: 0.00069200
Iteration 19/25 | Loss: 0.00069177
Iteration 20/25 | Loss: 0.00069201
Iteration 21/25 | Loss: 0.00069185
Iteration 22/25 | Loss: 0.00069207
Iteration 23/25 | Loss: 0.00069193
Iteration 24/25 | Loss: 0.00069204
Iteration 25/25 | Loss: 0.00069212

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.59546661
Iteration 2/25 | Loss: 0.00053364
Iteration 3/25 | Loss: 0.00053362
Iteration 4/25 | Loss: 0.00053362
Iteration 5/25 | Loss: 0.00053362
Iteration 6/25 | Loss: 0.00053362
Iteration 7/25 | Loss: 0.00053362
Iteration 8/25 | Loss: 0.00053362
Iteration 9/25 | Loss: 0.00053362
Iteration 10/25 | Loss: 0.00053362
Iteration 11/25 | Loss: 0.00053362
Iteration 12/25 | Loss: 0.00053362
Iteration 13/25 | Loss: 0.00053362
Iteration 14/25 | Loss: 0.00053362
Iteration 15/25 | Loss: 0.00053362
Iteration 16/25 | Loss: 0.00053362
Iteration 17/25 | Loss: 0.00053362
Iteration 18/25 | Loss: 0.00053362
Iteration 19/25 | Loss: 0.00053362
Iteration 20/25 | Loss: 0.00053362
Iteration 21/25 | Loss: 0.00053362
Iteration 22/25 | Loss: 0.00053362
Iteration 23/25 | Loss: 0.00053362
Iteration 24/25 | Loss: 0.00053362
Iteration 25/25 | Loss: 0.00053362

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053362
Iteration 2/1000 | Loss: 0.00002420
Iteration 3/1000 | Loss: 0.00002033
Iteration 4/1000 | Loss: 0.00001881
Iteration 5/1000 | Loss: 0.00004304
Iteration 6/1000 | Loss: 0.00001801
Iteration 7/1000 | Loss: 0.00003424
Iteration 8/1000 | Loss: 0.00001833
Iteration 9/1000 | Loss: 0.00002471
Iteration 10/1000 | Loss: 0.00001752
Iteration 11/1000 | Loss: 0.00001718
Iteration 12/1000 | Loss: 0.00001735
Iteration 13/1000 | Loss: 0.00001715
Iteration 14/1000 | Loss: 0.00001708
Iteration 15/1000 | Loss: 0.00001730
Iteration 16/1000 | Loss: 0.00001698
Iteration 17/1000 | Loss: 0.00001691
Iteration 18/1000 | Loss: 0.00001691
Iteration 19/1000 | Loss: 0.00001691
Iteration 20/1000 | Loss: 0.00001691
Iteration 21/1000 | Loss: 0.00001691
Iteration 22/1000 | Loss: 0.00001691
Iteration 23/1000 | Loss: 0.00001690
Iteration 24/1000 | Loss: 0.00001690
Iteration 25/1000 | Loss: 0.00001690
Iteration 26/1000 | Loss: 0.00001690
Iteration 27/1000 | Loss: 0.00001690
Iteration 28/1000 | Loss: 0.00001690
Iteration 29/1000 | Loss: 0.00001689
Iteration 30/1000 | Loss: 0.00001714
Iteration 31/1000 | Loss: 0.00001712
Iteration 32/1000 | Loss: 0.00001688
Iteration 33/1000 | Loss: 0.00001687
Iteration 34/1000 | Loss: 0.00001687
Iteration 35/1000 | Loss: 0.00001686
Iteration 36/1000 | Loss: 0.00001686
Iteration 37/1000 | Loss: 0.00001686
Iteration 38/1000 | Loss: 0.00001685
Iteration 39/1000 | Loss: 0.00001685
Iteration 40/1000 | Loss: 0.00001685
Iteration 41/1000 | Loss: 0.00001684
Iteration 42/1000 | Loss: 0.00001684
Iteration 43/1000 | Loss: 0.00001684
Iteration 44/1000 | Loss: 0.00001684
Iteration 45/1000 | Loss: 0.00001684
Iteration 46/1000 | Loss: 0.00001684
Iteration 47/1000 | Loss: 0.00001683
Iteration 48/1000 | Loss: 0.00001683
Iteration 49/1000 | Loss: 0.00001683
Iteration 50/1000 | Loss: 0.00001683
Iteration 51/1000 | Loss: 0.00001683
Iteration 52/1000 | Loss: 0.00001683
Iteration 53/1000 | Loss: 0.00001683
Iteration 54/1000 | Loss: 0.00001682
Iteration 55/1000 | Loss: 0.00001682
Iteration 56/1000 | Loss: 0.00001682
Iteration 57/1000 | Loss: 0.00001681
Iteration 58/1000 | Loss: 0.00001681
Iteration 59/1000 | Loss: 0.00001692
Iteration 60/1000 | Loss: 0.00001724
Iteration 61/1000 | Loss: 0.00001703
Iteration 62/1000 | Loss: 0.00001681
Iteration 63/1000 | Loss: 0.00001679
Iteration 64/1000 | Loss: 0.00001678
Iteration 65/1000 | Loss: 0.00001674
Iteration 66/1000 | Loss: 0.00001708
Iteration 67/1000 | Loss: 0.00001684
Iteration 68/1000 | Loss: 0.00001684
Iteration 69/1000 | Loss: 0.00001674
Iteration 70/1000 | Loss: 0.00001674
Iteration 71/1000 | Loss: 0.00001674
Iteration 72/1000 | Loss: 0.00001674
Iteration 73/1000 | Loss: 0.00001674
Iteration 74/1000 | Loss: 0.00001674
Iteration 75/1000 | Loss: 0.00001674
Iteration 76/1000 | Loss: 0.00001674
Iteration 77/1000 | Loss: 0.00001673
Iteration 78/1000 | Loss: 0.00001673
Iteration 79/1000 | Loss: 0.00001673
Iteration 80/1000 | Loss: 0.00001673
Iteration 81/1000 | Loss: 0.00001673
Iteration 82/1000 | Loss: 0.00001710
Iteration 83/1000 | Loss: 0.00001710
Iteration 84/1000 | Loss: 0.00001701
Iteration 85/1000 | Loss: 0.00001706
Iteration 86/1000 | Loss: 0.00001699
Iteration 87/1000 | Loss: 0.00001698
Iteration 88/1000 | Loss: 0.00001676
Iteration 89/1000 | Loss: 0.00001676
Iteration 90/1000 | Loss: 0.00001674
Iteration 91/1000 | Loss: 0.00001674
Iteration 92/1000 | Loss: 0.00001709
Iteration 93/1000 | Loss: 0.00001695
Iteration 94/1000 | Loss: 0.00001671
Iteration 95/1000 | Loss: 0.00001670
Iteration 96/1000 | Loss: 0.00001712
Iteration 97/1000 | Loss: 0.00001703
Iteration 98/1000 | Loss: 0.00001703
Iteration 99/1000 | Loss: 0.00001686
Iteration 100/1000 | Loss: 0.00001673
Iteration 101/1000 | Loss: 0.00001671
Iteration 102/1000 | Loss: 0.00001670
Iteration 103/1000 | Loss: 0.00001669
Iteration 104/1000 | Loss: 0.00001669
Iteration 105/1000 | Loss: 0.00001668
Iteration 106/1000 | Loss: 0.00001668
Iteration 107/1000 | Loss: 0.00001668
Iteration 108/1000 | Loss: 0.00001668
Iteration 109/1000 | Loss: 0.00001710
Iteration 110/1000 | Loss: 0.00001710
Iteration 111/1000 | Loss: 0.00001697
Iteration 112/1000 | Loss: 0.00001683
Iteration 113/1000 | Loss: 0.00001703
Iteration 114/1000 | Loss: 0.00001699
Iteration 115/1000 | Loss: 0.00001723
Iteration 116/1000 | Loss: 0.00001712
Iteration 117/1000 | Loss: 0.00001721
Iteration 118/1000 | Loss: 0.00001702
Iteration 119/1000 | Loss: 0.00001729
Iteration 120/1000 | Loss: 0.00001724
Iteration 121/1000 | Loss: 0.00001721
Iteration 122/1000 | Loss: 0.00001699
Iteration 123/1000 | Loss: 0.00001670
Iteration 124/1000 | Loss: 0.00001720
Iteration 125/1000 | Loss: 0.00001702
Iteration 126/1000 | Loss: 0.00001718
Iteration 127/1000 | Loss: 0.00001696
Iteration 128/1000 | Loss: 0.00001668
Iteration 129/1000 | Loss: 0.00004214
Iteration 130/1000 | Loss: 0.00004214
Iteration 131/1000 | Loss: 0.00037424
Iteration 132/1000 | Loss: 0.00001732
Iteration 133/1000 | Loss: 0.00001733
Iteration 134/1000 | Loss: 0.00001706
Iteration 135/1000 | Loss: 0.00001667
Iteration 136/1000 | Loss: 0.00001667
Iteration 137/1000 | Loss: 0.00001667
Iteration 138/1000 | Loss: 0.00001711
Iteration 139/1000 | Loss: 0.00001707
Iteration 140/1000 | Loss: 0.00001706
Iteration 141/1000 | Loss: 0.00001706
Iteration 142/1000 | Loss: 0.00001706
Iteration 143/1000 | Loss: 0.00001706
Iteration 144/1000 | Loss: 0.00001706
Iteration 145/1000 | Loss: 0.00001706
Iteration 146/1000 | Loss: 0.00001705
Iteration 147/1000 | Loss: 0.00001705
Iteration 148/1000 | Loss: 0.00001704
Iteration 149/1000 | Loss: 0.00001704
Iteration 150/1000 | Loss: 0.00001704
Iteration 151/1000 | Loss: 0.00001703
Iteration 152/1000 | Loss: 0.00001703
Iteration 153/1000 | Loss: 0.00001702
Iteration 154/1000 | Loss: 0.00001702
Iteration 155/1000 | Loss: 0.00001701
Iteration 156/1000 | Loss: 0.00001713
Iteration 157/1000 | Loss: 0.00001713
Iteration 158/1000 | Loss: 0.00001689
Iteration 159/1000 | Loss: 0.00001689
Iteration 160/1000 | Loss: 0.00001707
Iteration 161/1000 | Loss: 0.00001704
Iteration 162/1000 | Loss: 0.00001699
Iteration 163/1000 | Loss: 0.00004496
Iteration 164/1000 | Loss: 0.00001945
Iteration 165/1000 | Loss: 0.00001982
Iteration 166/1000 | Loss: 0.00002148
Iteration 167/1000 | Loss: 0.00002588
Iteration 168/1000 | Loss: 0.00001959
Iteration 169/1000 | Loss: 0.00001793
Iteration 170/1000 | Loss: 0.00001986
Iteration 171/1000 | Loss: 0.00001773
Iteration 172/1000 | Loss: 0.00001725
Iteration 173/1000 | Loss: 0.00001876
Iteration 174/1000 | Loss: 0.00001720
Iteration 175/1000 | Loss: 0.00001738
Iteration 176/1000 | Loss: 0.00001888
Iteration 177/1000 | Loss: 0.00001731
Iteration 178/1000 | Loss: 0.00002094
Iteration 179/1000 | Loss: 0.00001725
Iteration 180/1000 | Loss: 0.00001836
Iteration 181/1000 | Loss: 0.00001734
Iteration 182/1000 | Loss: 0.00001934
Iteration 183/1000 | Loss: 0.00001887
Iteration 184/1000 | Loss: 0.00005440
Iteration 185/1000 | Loss: 0.00006679
Iteration 186/1000 | Loss: 0.00003088
Iteration 187/1000 | Loss: 0.00002673
Iteration 188/1000 | Loss: 0.00003823
Iteration 189/1000 | Loss: 0.00002336
Iteration 190/1000 | Loss: 0.00001724
Iteration 191/1000 | Loss: 0.00001733
Iteration 192/1000 | Loss: 0.00002880
Iteration 193/1000 | Loss: 0.00002027
Iteration 194/1000 | Loss: 0.00002099
Iteration 195/1000 | Loss: 0.00001872
Iteration 196/1000 | Loss: 0.00002224
Iteration 197/1000 | Loss: 0.00003496
Iteration 198/1000 | Loss: 0.00002193
Iteration 199/1000 | Loss: 0.00001721
Iteration 200/1000 | Loss: 0.00001851
Iteration 201/1000 | Loss: 0.00001726
Iteration 202/1000 | Loss: 0.00002128
Iteration 203/1000 | Loss: 0.00002409
Iteration 204/1000 | Loss: 0.00002645
Iteration 205/1000 | Loss: 0.00001725
Iteration 206/1000 | Loss: 0.00001705
Iteration 207/1000 | Loss: 0.00002122
Iteration 208/1000 | Loss: 0.00001810
Iteration 209/1000 | Loss: 0.00001733
Iteration 210/1000 | Loss: 0.00001741
Iteration 211/1000 | Loss: 0.00001776
Iteration 212/1000 | Loss: 0.00001802
Iteration 213/1000 | Loss: 0.00001826
Iteration 214/1000 | Loss: 0.00001850
Iteration 215/1000 | Loss: 0.00001747
Iteration 216/1000 | Loss: 0.00001672
Iteration 217/1000 | Loss: 0.00001672
Iteration 218/1000 | Loss: 0.00001724
Iteration 219/1000 | Loss: 0.00001730
Iteration 220/1000 | Loss: 0.00001667
Iteration 221/1000 | Loss: 0.00001666
Iteration 222/1000 | Loss: 0.00001665
Iteration 223/1000 | Loss: 0.00001665
Iteration 224/1000 | Loss: 0.00001720
Iteration 225/1000 | Loss: 0.00002031
Iteration 226/1000 | Loss: 0.00001862
Iteration 227/1000 | Loss: 0.00001745
Iteration 228/1000 | Loss: 0.00002007
Iteration 229/1000 | Loss: 0.00001756
Iteration 230/1000 | Loss: 0.00001848
Iteration 231/1000 | Loss: 0.00001737
Iteration 232/1000 | Loss: 0.00001756
Iteration 233/1000 | Loss: 0.00001761
Iteration 234/1000 | Loss: 0.00001748
Iteration 235/1000 | Loss: 0.00001746
Iteration 236/1000 | Loss: 0.00001824
Iteration 237/1000 | Loss: 0.00001747
Iteration 238/1000 | Loss: 0.00001761
Iteration 239/1000 | Loss: 0.00001761
Iteration 240/1000 | Loss: 0.00001769
Iteration 241/1000 | Loss: 0.00001727
Iteration 242/1000 | Loss: 0.00001739
Iteration 243/1000 | Loss: 0.00001710
Iteration 244/1000 | Loss: 0.00002759
Iteration 245/1000 | Loss: 0.00002498
Iteration 246/1000 | Loss: 0.00001715
Iteration 247/1000 | Loss: 0.00002501
Iteration 248/1000 | Loss: 0.00001682
Iteration 249/1000 | Loss: 0.00001720
Iteration 250/1000 | Loss: 0.00001720
Iteration 251/1000 | Loss: 0.00001756
Iteration 252/1000 | Loss: 0.00001725
Iteration 253/1000 | Loss: 0.00001725
Iteration 254/1000 | Loss: 0.00001750
Iteration 255/1000 | Loss: 0.00001715
Iteration 256/1000 | Loss: 0.00001722
Iteration 257/1000 | Loss: 0.00001665
Iteration 258/1000 | Loss: 0.00001712
Iteration 259/1000 | Loss: 0.00001712
Iteration 260/1000 | Loss: 0.00001749
Iteration 261/1000 | Loss: 0.00001727
Iteration 262/1000 | Loss: 0.00001726
Iteration 263/1000 | Loss: 0.00001725
Iteration 264/1000 | Loss: 0.00001664
Iteration 265/1000 | Loss: 0.00001664
Iteration 266/1000 | Loss: 0.00001716
Iteration 267/1000 | Loss: 0.00001720
Iteration 268/1000 | Loss: 0.00001711
Iteration 269/1000 | Loss: 0.00001751
Iteration 270/1000 | Loss: 0.00001665
Iteration 271/1000 | Loss: 0.00001665
Iteration 272/1000 | Loss: 0.00001665
Iteration 273/1000 | Loss: 0.00001665
Iteration 274/1000 | Loss: 0.00001665
Iteration 275/1000 | Loss: 0.00001665
Iteration 276/1000 | Loss: 0.00001732
Iteration 277/1000 | Loss: 0.00001761
Iteration 278/1000 | Loss: 0.00001761
Iteration 279/1000 | Loss: 0.00001666
Iteration 280/1000 | Loss: 0.00001665
Iteration 281/1000 | Loss: 0.00001664
Iteration 282/1000 | Loss: 0.00001732
Iteration 283/1000 | Loss: 0.00001745
Iteration 284/1000 | Loss: 0.00001663
Iteration 285/1000 | Loss: 0.00001717
Iteration 286/1000 | Loss: 0.00001728
Iteration 287/1000 | Loss: 0.00001724
Iteration 288/1000 | Loss: 0.00001724
Iteration 289/1000 | Loss: 0.00001746
Iteration 290/1000 | Loss: 0.00001721
Iteration 291/1000 | Loss: 0.00001720
Iteration 292/1000 | Loss: 0.00001739
Iteration 293/1000 | Loss: 0.00001737
Iteration 294/1000 | Loss: 0.00001667
Iteration 295/1000 | Loss: 0.00001663
Iteration 296/1000 | Loss: 0.00001662
Iteration 297/1000 | Loss: 0.00001662
Iteration 298/1000 | Loss: 0.00001662
Iteration 299/1000 | Loss: 0.00001662
Iteration 300/1000 | Loss: 0.00001662
Iteration 301/1000 | Loss: 0.00001662
Iteration 302/1000 | Loss: 0.00001714
Iteration 303/1000 | Loss: 0.00001714
Iteration 304/1000 | Loss: 0.00001714
Iteration 305/1000 | Loss: 0.00001712
Iteration 306/1000 | Loss: 0.00001696
Iteration 307/1000 | Loss: 0.00001670
Iteration 308/1000 | Loss: 0.00001670
Iteration 309/1000 | Loss: 0.00001679
Iteration 310/1000 | Loss: 0.00001679
Iteration 311/1000 | Loss: 0.00001663
Iteration 312/1000 | Loss: 0.00001662
Iteration 313/1000 | Loss: 0.00001662
Iteration 314/1000 | Loss: 0.00001662
Iteration 315/1000 | Loss: 0.00001662
Iteration 316/1000 | Loss: 0.00001662
Iteration 317/1000 | Loss: 0.00001675
Iteration 318/1000 | Loss: 0.00001675
Iteration 319/1000 | Loss: 0.00001662
Iteration 320/1000 | Loss: 0.00001662
Iteration 321/1000 | Loss: 0.00001662
Iteration 322/1000 | Loss: 0.00001662
Iteration 323/1000 | Loss: 0.00001662
Iteration 324/1000 | Loss: 0.00001662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 324. Stopping optimization.
Last 5 losses: [1.6619516827631742e-05, 1.6619516827631742e-05, 1.6619516827631742e-05, 1.6619516827631742e-05, 1.6619516827631742e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6619516827631742e-05

Optimization complete. Final v2v error: 3.446303606033325 mm

Highest mean error: 9.660316467285156 mm for frame 79

Lowest mean error: 2.663426637649536 mm for frame 78

Saving results

Total time: 266.8929102420807
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408036
Iteration 2/25 | Loss: 0.00103617
Iteration 3/25 | Loss: 0.00090260
Iteration 4/25 | Loss: 0.00087805
Iteration 5/25 | Loss: 0.00087337
Iteration 6/25 | Loss: 0.00087221
Iteration 7/25 | Loss: 0.00087194
Iteration 8/25 | Loss: 0.00087194
Iteration 9/25 | Loss: 0.00087194
Iteration 10/25 | Loss: 0.00087194
Iteration 11/25 | Loss: 0.00087194
Iteration 12/25 | Loss: 0.00087194
Iteration 13/25 | Loss: 0.00087194
Iteration 14/25 | Loss: 0.00087194
Iteration 15/25 | Loss: 0.00087194
Iteration 16/25 | Loss: 0.00087194
Iteration 17/25 | Loss: 0.00087194
Iteration 18/25 | Loss: 0.00087194
Iteration 19/25 | Loss: 0.00087194
Iteration 20/25 | Loss: 0.00087194
Iteration 21/25 | Loss: 0.00087194
Iteration 22/25 | Loss: 0.00087194
Iteration 23/25 | Loss: 0.00087194
Iteration 24/25 | Loss: 0.00087194
Iteration 25/25 | Loss: 0.00087194

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51794672
Iteration 2/25 | Loss: 0.00102230
Iteration 3/25 | Loss: 0.00102230
Iteration 4/25 | Loss: 0.00102230
Iteration 5/25 | Loss: 0.00102230
Iteration 6/25 | Loss: 0.00102230
Iteration 7/25 | Loss: 0.00102230
Iteration 8/25 | Loss: 0.00102230
Iteration 9/25 | Loss: 0.00102230
Iteration 10/25 | Loss: 0.00102230
Iteration 11/25 | Loss: 0.00102230
Iteration 12/25 | Loss: 0.00102230
Iteration 13/25 | Loss: 0.00102230
Iteration 14/25 | Loss: 0.00102230
Iteration 15/25 | Loss: 0.00102230
Iteration 16/25 | Loss: 0.00102230
Iteration 17/25 | Loss: 0.00102230
Iteration 18/25 | Loss: 0.00102230
Iteration 19/25 | Loss: 0.00102230
Iteration 20/25 | Loss: 0.00102230
Iteration 21/25 | Loss: 0.00102230
Iteration 22/25 | Loss: 0.00102230
Iteration 23/25 | Loss: 0.00102230
Iteration 24/25 | Loss: 0.00102230
Iteration 25/25 | Loss: 0.00102230

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102230
Iteration 2/1000 | Loss: 0.00006079
Iteration 3/1000 | Loss: 0.00003921
Iteration 4/1000 | Loss: 0.00002878
Iteration 5/1000 | Loss: 0.00002459
Iteration 6/1000 | Loss: 0.00002268
Iteration 7/1000 | Loss: 0.00002175
Iteration 8/1000 | Loss: 0.00002102
Iteration 9/1000 | Loss: 0.00002027
Iteration 10/1000 | Loss: 0.00001979
Iteration 11/1000 | Loss: 0.00001949
Iteration 12/1000 | Loss: 0.00001926
Iteration 13/1000 | Loss: 0.00001916
Iteration 14/1000 | Loss: 0.00001902
Iteration 15/1000 | Loss: 0.00001901
Iteration 16/1000 | Loss: 0.00001899
Iteration 17/1000 | Loss: 0.00001899
Iteration 18/1000 | Loss: 0.00001898
Iteration 19/1000 | Loss: 0.00001898
Iteration 20/1000 | Loss: 0.00001898
Iteration 21/1000 | Loss: 0.00001897
Iteration 22/1000 | Loss: 0.00001897
Iteration 23/1000 | Loss: 0.00001896
Iteration 24/1000 | Loss: 0.00001895
Iteration 25/1000 | Loss: 0.00001894
Iteration 26/1000 | Loss: 0.00001893
Iteration 27/1000 | Loss: 0.00001893
Iteration 28/1000 | Loss: 0.00001892
Iteration 29/1000 | Loss: 0.00001892
Iteration 30/1000 | Loss: 0.00001891
Iteration 31/1000 | Loss: 0.00001890
Iteration 32/1000 | Loss: 0.00001890
Iteration 33/1000 | Loss: 0.00001890
Iteration 34/1000 | Loss: 0.00001890
Iteration 35/1000 | Loss: 0.00001889
Iteration 36/1000 | Loss: 0.00001889
Iteration 37/1000 | Loss: 0.00001889
Iteration 38/1000 | Loss: 0.00001889
Iteration 39/1000 | Loss: 0.00001889
Iteration 40/1000 | Loss: 0.00001889
Iteration 41/1000 | Loss: 0.00001889
Iteration 42/1000 | Loss: 0.00001889
Iteration 43/1000 | Loss: 0.00001889
Iteration 44/1000 | Loss: 0.00001888
Iteration 45/1000 | Loss: 0.00001888
Iteration 46/1000 | Loss: 0.00001888
Iteration 47/1000 | Loss: 0.00001888
Iteration 48/1000 | Loss: 0.00001888
Iteration 49/1000 | Loss: 0.00001888
Iteration 50/1000 | Loss: 0.00001888
Iteration 51/1000 | Loss: 0.00001888
Iteration 52/1000 | Loss: 0.00001888
Iteration 53/1000 | Loss: 0.00001888
Iteration 54/1000 | Loss: 0.00001888
Iteration 55/1000 | Loss: 0.00001888
Iteration 56/1000 | Loss: 0.00001887
Iteration 57/1000 | Loss: 0.00001887
Iteration 58/1000 | Loss: 0.00001887
Iteration 59/1000 | Loss: 0.00001887
Iteration 60/1000 | Loss: 0.00001887
Iteration 61/1000 | Loss: 0.00001887
Iteration 62/1000 | Loss: 0.00001887
Iteration 63/1000 | Loss: 0.00001887
Iteration 64/1000 | Loss: 0.00001887
Iteration 65/1000 | Loss: 0.00001887
Iteration 66/1000 | Loss: 0.00001887
Iteration 67/1000 | Loss: 0.00001887
Iteration 68/1000 | Loss: 0.00001886
Iteration 69/1000 | Loss: 0.00001886
Iteration 70/1000 | Loss: 0.00001886
Iteration 71/1000 | Loss: 0.00001886
Iteration 72/1000 | Loss: 0.00001886
Iteration 73/1000 | Loss: 0.00001886
Iteration 74/1000 | Loss: 0.00001886
Iteration 75/1000 | Loss: 0.00001886
Iteration 76/1000 | Loss: 0.00001885
Iteration 77/1000 | Loss: 0.00001885
Iteration 78/1000 | Loss: 0.00001885
Iteration 79/1000 | Loss: 0.00001885
Iteration 80/1000 | Loss: 0.00001885
Iteration 81/1000 | Loss: 0.00001885
Iteration 82/1000 | Loss: 0.00001885
Iteration 83/1000 | Loss: 0.00001884
Iteration 84/1000 | Loss: 0.00001884
Iteration 85/1000 | Loss: 0.00001884
Iteration 86/1000 | Loss: 0.00001884
Iteration 87/1000 | Loss: 0.00001884
Iteration 88/1000 | Loss: 0.00001884
Iteration 89/1000 | Loss: 0.00001884
Iteration 90/1000 | Loss: 0.00001884
Iteration 91/1000 | Loss: 0.00001884
Iteration 92/1000 | Loss: 0.00001884
Iteration 93/1000 | Loss: 0.00001884
Iteration 94/1000 | Loss: 0.00001884
Iteration 95/1000 | Loss: 0.00001884
Iteration 96/1000 | Loss: 0.00001884
Iteration 97/1000 | Loss: 0.00001883
Iteration 98/1000 | Loss: 0.00001883
Iteration 99/1000 | Loss: 0.00001883
Iteration 100/1000 | Loss: 0.00001883
Iteration 101/1000 | Loss: 0.00001883
Iteration 102/1000 | Loss: 0.00001883
Iteration 103/1000 | Loss: 0.00001883
Iteration 104/1000 | Loss: 0.00001883
Iteration 105/1000 | Loss: 0.00001883
Iteration 106/1000 | Loss: 0.00001883
Iteration 107/1000 | Loss: 0.00001883
Iteration 108/1000 | Loss: 0.00001883
Iteration 109/1000 | Loss: 0.00001883
Iteration 110/1000 | Loss: 0.00001883
Iteration 111/1000 | Loss: 0.00001883
Iteration 112/1000 | Loss: 0.00001883
Iteration 113/1000 | Loss: 0.00001883
Iteration 114/1000 | Loss: 0.00001883
Iteration 115/1000 | Loss: 0.00001883
Iteration 116/1000 | Loss: 0.00001883
Iteration 117/1000 | Loss: 0.00001883
Iteration 118/1000 | Loss: 0.00001883
Iteration 119/1000 | Loss: 0.00001883
Iteration 120/1000 | Loss: 0.00001883
Iteration 121/1000 | Loss: 0.00001883
Iteration 122/1000 | Loss: 0.00001883
Iteration 123/1000 | Loss: 0.00001883
Iteration 124/1000 | Loss: 0.00001883
Iteration 125/1000 | Loss: 0.00001883
Iteration 126/1000 | Loss: 0.00001883
Iteration 127/1000 | Loss: 0.00001883
Iteration 128/1000 | Loss: 0.00001883
Iteration 129/1000 | Loss: 0.00001883
Iteration 130/1000 | Loss: 0.00001883
Iteration 131/1000 | Loss: 0.00001883
Iteration 132/1000 | Loss: 0.00001883
Iteration 133/1000 | Loss: 0.00001883
Iteration 134/1000 | Loss: 0.00001883
Iteration 135/1000 | Loss: 0.00001883
Iteration 136/1000 | Loss: 0.00001883
Iteration 137/1000 | Loss: 0.00001883
Iteration 138/1000 | Loss: 0.00001883
Iteration 139/1000 | Loss: 0.00001883
Iteration 140/1000 | Loss: 0.00001883
Iteration 141/1000 | Loss: 0.00001883
Iteration 142/1000 | Loss: 0.00001883
Iteration 143/1000 | Loss: 0.00001883
Iteration 144/1000 | Loss: 0.00001883
Iteration 145/1000 | Loss: 0.00001883
Iteration 146/1000 | Loss: 0.00001883
Iteration 147/1000 | Loss: 0.00001883
Iteration 148/1000 | Loss: 0.00001883
Iteration 149/1000 | Loss: 0.00001883
Iteration 150/1000 | Loss: 0.00001883
Iteration 151/1000 | Loss: 0.00001883
Iteration 152/1000 | Loss: 0.00001883
Iteration 153/1000 | Loss: 0.00001883
Iteration 154/1000 | Loss: 0.00001883
Iteration 155/1000 | Loss: 0.00001883
Iteration 156/1000 | Loss: 0.00001883
Iteration 157/1000 | Loss: 0.00001883
Iteration 158/1000 | Loss: 0.00001883
Iteration 159/1000 | Loss: 0.00001883
Iteration 160/1000 | Loss: 0.00001883
Iteration 161/1000 | Loss: 0.00001883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.882935976027511e-05, 1.882935976027511e-05, 1.882935976027511e-05, 1.882935976027511e-05, 1.882935976027511e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.882935976027511e-05

Optimization complete. Final v2v error: 3.6930739879608154 mm

Highest mean error: 3.953014850616455 mm for frame 73

Lowest mean error: 3.362082004547119 mm for frame 2

Saving results

Total time: 38.06879425048828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00524618
Iteration 2/25 | Loss: 0.00122969
Iteration 3/25 | Loss: 0.00100185
Iteration 4/25 | Loss: 0.00096342
Iteration 5/25 | Loss: 0.00095081
Iteration 6/25 | Loss: 0.00094892
Iteration 7/25 | Loss: 0.00094800
Iteration 8/25 | Loss: 0.00094797
Iteration 9/25 | Loss: 0.00094797
Iteration 10/25 | Loss: 0.00094797
Iteration 11/25 | Loss: 0.00094797
Iteration 12/25 | Loss: 0.00094797
Iteration 13/25 | Loss: 0.00094797
Iteration 14/25 | Loss: 0.00094797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009479708387516439, 0.0009479708387516439, 0.0009479708387516439, 0.0009479708387516439, 0.0009479708387516439]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009479708387516439

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10020626
Iteration 2/25 | Loss: 0.00109987
Iteration 3/25 | Loss: 0.00109981
Iteration 4/25 | Loss: 0.00109981
Iteration 5/25 | Loss: 0.00109981
Iteration 6/25 | Loss: 0.00109981
Iteration 7/25 | Loss: 0.00109981
Iteration 8/25 | Loss: 0.00109981
Iteration 9/25 | Loss: 0.00109981
Iteration 10/25 | Loss: 0.00109981
Iteration 11/25 | Loss: 0.00109981
Iteration 12/25 | Loss: 0.00109981
Iteration 13/25 | Loss: 0.00109981
Iteration 14/25 | Loss: 0.00109981
Iteration 15/25 | Loss: 0.00109981
Iteration 16/25 | Loss: 0.00109981
Iteration 17/25 | Loss: 0.00109981
Iteration 18/25 | Loss: 0.00109981
Iteration 19/25 | Loss: 0.00109981
Iteration 20/25 | Loss: 0.00109981
Iteration 21/25 | Loss: 0.00109981
Iteration 22/25 | Loss: 0.00109981
Iteration 23/25 | Loss: 0.00109981
Iteration 24/25 | Loss: 0.00109981
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010998058132827282, 0.0010998058132827282, 0.0010998058132827282, 0.0010998058132827282, 0.0010998058132827282]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010998058132827282

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109981
Iteration 2/1000 | Loss: 0.00005715
Iteration 3/1000 | Loss: 0.00003506
Iteration 4/1000 | Loss: 0.00002893
Iteration 5/1000 | Loss: 0.00002690
Iteration 6/1000 | Loss: 0.00002596
Iteration 7/1000 | Loss: 0.00002487
Iteration 8/1000 | Loss: 0.00002427
Iteration 9/1000 | Loss: 0.00002379
Iteration 10/1000 | Loss: 0.00002342
Iteration 11/1000 | Loss: 0.00002310
Iteration 12/1000 | Loss: 0.00002295
Iteration 13/1000 | Loss: 0.00002280
Iteration 14/1000 | Loss: 0.00002278
Iteration 15/1000 | Loss: 0.00002278
Iteration 16/1000 | Loss: 0.00002277
Iteration 17/1000 | Loss: 0.00002277
Iteration 18/1000 | Loss: 0.00002277
Iteration 19/1000 | Loss: 0.00002277
Iteration 20/1000 | Loss: 0.00002276
Iteration 21/1000 | Loss: 0.00002276
Iteration 22/1000 | Loss: 0.00002276
Iteration 23/1000 | Loss: 0.00002276
Iteration 24/1000 | Loss: 0.00002276
Iteration 25/1000 | Loss: 0.00002276
Iteration 26/1000 | Loss: 0.00002276
Iteration 27/1000 | Loss: 0.00002276
Iteration 28/1000 | Loss: 0.00002276
Iteration 29/1000 | Loss: 0.00002276
Iteration 30/1000 | Loss: 0.00002276
Iteration 31/1000 | Loss: 0.00002276
Iteration 32/1000 | Loss: 0.00002275
Iteration 33/1000 | Loss: 0.00002275
Iteration 34/1000 | Loss: 0.00002275
Iteration 35/1000 | Loss: 0.00002274
Iteration 36/1000 | Loss: 0.00002270
Iteration 37/1000 | Loss: 0.00002270
Iteration 38/1000 | Loss: 0.00002269
Iteration 39/1000 | Loss: 0.00002269
Iteration 40/1000 | Loss: 0.00002266
Iteration 41/1000 | Loss: 0.00002265
Iteration 42/1000 | Loss: 0.00002265
Iteration 43/1000 | Loss: 0.00002265
Iteration 44/1000 | Loss: 0.00002265
Iteration 45/1000 | Loss: 0.00002264
Iteration 46/1000 | Loss: 0.00002264
Iteration 47/1000 | Loss: 0.00002264
Iteration 48/1000 | Loss: 0.00002264
Iteration 49/1000 | Loss: 0.00002263
Iteration 50/1000 | Loss: 0.00002263
Iteration 51/1000 | Loss: 0.00002263
Iteration 52/1000 | Loss: 0.00002263
Iteration 53/1000 | Loss: 0.00002263
Iteration 54/1000 | Loss: 0.00002263
Iteration 55/1000 | Loss: 0.00002263
Iteration 56/1000 | Loss: 0.00002262
Iteration 57/1000 | Loss: 0.00002262
Iteration 58/1000 | Loss: 0.00002262
Iteration 59/1000 | Loss: 0.00002262
Iteration 60/1000 | Loss: 0.00002262
Iteration 61/1000 | Loss: 0.00002262
Iteration 62/1000 | Loss: 0.00002262
Iteration 63/1000 | Loss: 0.00002261
Iteration 64/1000 | Loss: 0.00002261
Iteration 65/1000 | Loss: 0.00002261
Iteration 66/1000 | Loss: 0.00002261
Iteration 67/1000 | Loss: 0.00002260
Iteration 68/1000 | Loss: 0.00002260
Iteration 69/1000 | Loss: 0.00002260
Iteration 70/1000 | Loss: 0.00002260
Iteration 71/1000 | Loss: 0.00002260
Iteration 72/1000 | Loss: 0.00002260
Iteration 73/1000 | Loss: 0.00002260
Iteration 74/1000 | Loss: 0.00002260
Iteration 75/1000 | Loss: 0.00002260
Iteration 76/1000 | Loss: 0.00002260
Iteration 77/1000 | Loss: 0.00002260
Iteration 78/1000 | Loss: 0.00002260
Iteration 79/1000 | Loss: 0.00002260
Iteration 80/1000 | Loss: 0.00002259
Iteration 81/1000 | Loss: 0.00002258
Iteration 82/1000 | Loss: 0.00002258
Iteration 83/1000 | Loss: 0.00002257
Iteration 84/1000 | Loss: 0.00002257
Iteration 85/1000 | Loss: 0.00002257
Iteration 86/1000 | Loss: 0.00002257
Iteration 87/1000 | Loss: 0.00002257
Iteration 88/1000 | Loss: 0.00002257
Iteration 89/1000 | Loss: 0.00002257
Iteration 90/1000 | Loss: 0.00002257
Iteration 91/1000 | Loss: 0.00002256
Iteration 92/1000 | Loss: 0.00002256
Iteration 93/1000 | Loss: 0.00002256
Iteration 94/1000 | Loss: 0.00002256
Iteration 95/1000 | Loss: 0.00002256
Iteration 96/1000 | Loss: 0.00002256
Iteration 97/1000 | Loss: 0.00002256
Iteration 98/1000 | Loss: 0.00002256
Iteration 99/1000 | Loss: 0.00002256
Iteration 100/1000 | Loss: 0.00002256
Iteration 101/1000 | Loss: 0.00002256
Iteration 102/1000 | Loss: 0.00002255
Iteration 103/1000 | Loss: 0.00002255
Iteration 104/1000 | Loss: 0.00002255
Iteration 105/1000 | Loss: 0.00002254
Iteration 106/1000 | Loss: 0.00002254
Iteration 107/1000 | Loss: 0.00002254
Iteration 108/1000 | Loss: 0.00002253
Iteration 109/1000 | Loss: 0.00002253
Iteration 110/1000 | Loss: 0.00002253
Iteration 111/1000 | Loss: 0.00002252
Iteration 112/1000 | Loss: 0.00002252
Iteration 113/1000 | Loss: 0.00002252
Iteration 114/1000 | Loss: 0.00002252
Iteration 115/1000 | Loss: 0.00002251
Iteration 116/1000 | Loss: 0.00002251
Iteration 117/1000 | Loss: 0.00002251
Iteration 118/1000 | Loss: 0.00002251
Iteration 119/1000 | Loss: 0.00002251
Iteration 120/1000 | Loss: 0.00002251
Iteration 121/1000 | Loss: 0.00002251
Iteration 122/1000 | Loss: 0.00002251
Iteration 123/1000 | Loss: 0.00002251
Iteration 124/1000 | Loss: 0.00002251
Iteration 125/1000 | Loss: 0.00002250
Iteration 126/1000 | Loss: 0.00002250
Iteration 127/1000 | Loss: 0.00002250
Iteration 128/1000 | Loss: 0.00002250
Iteration 129/1000 | Loss: 0.00002249
Iteration 130/1000 | Loss: 0.00002249
Iteration 131/1000 | Loss: 0.00002249
Iteration 132/1000 | Loss: 0.00002249
Iteration 133/1000 | Loss: 0.00002249
Iteration 134/1000 | Loss: 0.00002248
Iteration 135/1000 | Loss: 0.00002248
Iteration 136/1000 | Loss: 0.00002247
Iteration 137/1000 | Loss: 0.00002247
Iteration 138/1000 | Loss: 0.00002247
Iteration 139/1000 | Loss: 0.00002247
Iteration 140/1000 | Loss: 0.00002247
Iteration 141/1000 | Loss: 0.00002247
Iteration 142/1000 | Loss: 0.00002246
Iteration 143/1000 | Loss: 0.00002246
Iteration 144/1000 | Loss: 0.00002246
Iteration 145/1000 | Loss: 0.00002246
Iteration 146/1000 | Loss: 0.00002246
Iteration 147/1000 | Loss: 0.00002246
Iteration 148/1000 | Loss: 0.00002245
Iteration 149/1000 | Loss: 0.00002245
Iteration 150/1000 | Loss: 0.00002245
Iteration 151/1000 | Loss: 0.00002245
Iteration 152/1000 | Loss: 0.00002245
Iteration 153/1000 | Loss: 0.00002244
Iteration 154/1000 | Loss: 0.00002244
Iteration 155/1000 | Loss: 0.00002244
Iteration 156/1000 | Loss: 0.00002244
Iteration 157/1000 | Loss: 0.00002244
Iteration 158/1000 | Loss: 0.00002244
Iteration 159/1000 | Loss: 0.00002244
Iteration 160/1000 | Loss: 0.00002244
Iteration 161/1000 | Loss: 0.00002244
Iteration 162/1000 | Loss: 0.00002244
Iteration 163/1000 | Loss: 0.00002244
Iteration 164/1000 | Loss: 0.00002243
Iteration 165/1000 | Loss: 0.00002243
Iteration 166/1000 | Loss: 0.00002243
Iteration 167/1000 | Loss: 0.00002243
Iteration 168/1000 | Loss: 0.00002243
Iteration 169/1000 | Loss: 0.00002243
Iteration 170/1000 | Loss: 0.00002243
Iteration 171/1000 | Loss: 0.00002243
Iteration 172/1000 | Loss: 0.00002242
Iteration 173/1000 | Loss: 0.00002242
Iteration 174/1000 | Loss: 0.00002242
Iteration 175/1000 | Loss: 0.00002242
Iteration 176/1000 | Loss: 0.00002242
Iteration 177/1000 | Loss: 0.00002242
Iteration 178/1000 | Loss: 0.00002242
Iteration 179/1000 | Loss: 0.00002242
Iteration 180/1000 | Loss: 0.00002242
Iteration 181/1000 | Loss: 0.00002242
Iteration 182/1000 | Loss: 0.00002242
Iteration 183/1000 | Loss: 0.00002242
Iteration 184/1000 | Loss: 0.00002241
Iteration 185/1000 | Loss: 0.00002241
Iteration 186/1000 | Loss: 0.00002241
Iteration 187/1000 | Loss: 0.00002241
Iteration 188/1000 | Loss: 0.00002241
Iteration 189/1000 | Loss: 0.00002241
Iteration 190/1000 | Loss: 0.00002241
Iteration 191/1000 | Loss: 0.00002241
Iteration 192/1000 | Loss: 0.00002241
Iteration 193/1000 | Loss: 0.00002241
Iteration 194/1000 | Loss: 0.00002241
Iteration 195/1000 | Loss: 0.00002241
Iteration 196/1000 | Loss: 0.00002241
Iteration 197/1000 | Loss: 0.00002241
Iteration 198/1000 | Loss: 0.00002241
Iteration 199/1000 | Loss: 0.00002241
Iteration 200/1000 | Loss: 0.00002241
Iteration 201/1000 | Loss: 0.00002241
Iteration 202/1000 | Loss: 0.00002240
Iteration 203/1000 | Loss: 0.00002240
Iteration 204/1000 | Loss: 0.00002240
Iteration 205/1000 | Loss: 0.00002240
Iteration 206/1000 | Loss: 0.00002240
Iteration 207/1000 | Loss: 0.00002240
Iteration 208/1000 | Loss: 0.00002240
Iteration 209/1000 | Loss: 0.00002240
Iteration 210/1000 | Loss: 0.00002240
Iteration 211/1000 | Loss: 0.00002240
Iteration 212/1000 | Loss: 0.00002240
Iteration 213/1000 | Loss: 0.00002240
Iteration 214/1000 | Loss: 0.00002240
Iteration 215/1000 | Loss: 0.00002240
Iteration 216/1000 | Loss: 0.00002239
Iteration 217/1000 | Loss: 0.00002239
Iteration 218/1000 | Loss: 0.00002239
Iteration 219/1000 | Loss: 0.00002238
Iteration 220/1000 | Loss: 0.00002238
Iteration 221/1000 | Loss: 0.00002238
Iteration 222/1000 | Loss: 0.00002238
Iteration 223/1000 | Loss: 0.00002238
Iteration 224/1000 | Loss: 0.00002238
Iteration 225/1000 | Loss: 0.00002238
Iteration 226/1000 | Loss: 0.00002238
Iteration 227/1000 | Loss: 0.00002238
Iteration 228/1000 | Loss: 0.00002238
Iteration 229/1000 | Loss: 0.00002238
Iteration 230/1000 | Loss: 0.00002237
Iteration 231/1000 | Loss: 0.00002237
Iteration 232/1000 | Loss: 0.00002237
Iteration 233/1000 | Loss: 0.00002237
Iteration 234/1000 | Loss: 0.00002237
Iteration 235/1000 | Loss: 0.00002237
Iteration 236/1000 | Loss: 0.00002237
Iteration 237/1000 | Loss: 0.00002237
Iteration 238/1000 | Loss: 0.00002237
Iteration 239/1000 | Loss: 0.00002237
Iteration 240/1000 | Loss: 0.00002237
Iteration 241/1000 | Loss: 0.00002237
Iteration 242/1000 | Loss: 0.00002237
Iteration 243/1000 | Loss: 0.00002236
Iteration 244/1000 | Loss: 0.00002236
Iteration 245/1000 | Loss: 0.00002236
Iteration 246/1000 | Loss: 0.00002235
Iteration 247/1000 | Loss: 0.00002235
Iteration 248/1000 | Loss: 0.00002235
Iteration 249/1000 | Loss: 0.00002235
Iteration 250/1000 | Loss: 0.00002235
Iteration 251/1000 | Loss: 0.00002235
Iteration 252/1000 | Loss: 0.00002235
Iteration 253/1000 | Loss: 0.00002234
Iteration 254/1000 | Loss: 0.00002234
Iteration 255/1000 | Loss: 0.00002234
Iteration 256/1000 | Loss: 0.00002234
Iteration 257/1000 | Loss: 0.00002234
Iteration 258/1000 | Loss: 0.00002234
Iteration 259/1000 | Loss: 0.00002234
Iteration 260/1000 | Loss: 0.00002234
Iteration 261/1000 | Loss: 0.00002233
Iteration 262/1000 | Loss: 0.00002233
Iteration 263/1000 | Loss: 0.00002233
Iteration 264/1000 | Loss: 0.00002233
Iteration 265/1000 | Loss: 0.00002233
Iteration 266/1000 | Loss: 0.00002233
Iteration 267/1000 | Loss: 0.00002233
Iteration 268/1000 | Loss: 0.00002232
Iteration 269/1000 | Loss: 0.00002232
Iteration 270/1000 | Loss: 0.00002232
Iteration 271/1000 | Loss: 0.00002232
Iteration 272/1000 | Loss: 0.00002232
Iteration 273/1000 | Loss: 0.00002232
Iteration 274/1000 | Loss: 0.00002232
Iteration 275/1000 | Loss: 0.00002232
Iteration 276/1000 | Loss: 0.00002232
Iteration 277/1000 | Loss: 0.00002232
Iteration 278/1000 | Loss: 0.00002232
Iteration 279/1000 | Loss: 0.00002231
Iteration 280/1000 | Loss: 0.00002231
Iteration 281/1000 | Loss: 0.00002231
Iteration 282/1000 | Loss: 0.00002231
Iteration 283/1000 | Loss: 0.00002231
Iteration 284/1000 | Loss: 0.00002231
Iteration 285/1000 | Loss: 0.00002231
Iteration 286/1000 | Loss: 0.00002231
Iteration 287/1000 | Loss: 0.00002231
Iteration 288/1000 | Loss: 0.00002231
Iteration 289/1000 | Loss: 0.00002231
Iteration 290/1000 | Loss: 0.00002230
Iteration 291/1000 | Loss: 0.00002230
Iteration 292/1000 | Loss: 0.00002230
Iteration 293/1000 | Loss: 0.00002230
Iteration 294/1000 | Loss: 0.00002230
Iteration 295/1000 | Loss: 0.00002230
Iteration 296/1000 | Loss: 0.00002230
Iteration 297/1000 | Loss: 0.00002230
Iteration 298/1000 | Loss: 0.00002230
Iteration 299/1000 | Loss: 0.00002230
Iteration 300/1000 | Loss: 0.00002230
Iteration 301/1000 | Loss: 0.00002230
Iteration 302/1000 | Loss: 0.00002230
Iteration 303/1000 | Loss: 0.00002230
Iteration 304/1000 | Loss: 0.00002229
Iteration 305/1000 | Loss: 0.00002229
Iteration 306/1000 | Loss: 0.00002229
Iteration 307/1000 | Loss: 0.00002229
Iteration 308/1000 | Loss: 0.00002229
Iteration 309/1000 | Loss: 0.00002229
Iteration 310/1000 | Loss: 0.00002229
Iteration 311/1000 | Loss: 0.00002229
Iteration 312/1000 | Loss: 0.00002229
Iteration 313/1000 | Loss: 0.00002229
Iteration 314/1000 | Loss: 0.00002229
Iteration 315/1000 | Loss: 0.00002228
Iteration 316/1000 | Loss: 0.00002228
Iteration 317/1000 | Loss: 0.00002228
Iteration 318/1000 | Loss: 0.00002228
Iteration 319/1000 | Loss: 0.00002228
Iteration 320/1000 | Loss: 0.00002228
Iteration 321/1000 | Loss: 0.00002228
Iteration 322/1000 | Loss: 0.00002228
Iteration 323/1000 | Loss: 0.00002228
Iteration 324/1000 | Loss: 0.00002227
Iteration 325/1000 | Loss: 0.00002227
Iteration 326/1000 | Loss: 0.00002227
Iteration 327/1000 | Loss: 0.00002227
Iteration 328/1000 | Loss: 0.00002227
Iteration 329/1000 | Loss: 0.00002227
Iteration 330/1000 | Loss: 0.00002227
Iteration 331/1000 | Loss: 0.00002227
Iteration 332/1000 | Loss: 0.00002227
Iteration 333/1000 | Loss: 0.00002227
Iteration 334/1000 | Loss: 0.00002227
Iteration 335/1000 | Loss: 0.00002227
Iteration 336/1000 | Loss: 0.00002226
Iteration 337/1000 | Loss: 0.00002226
Iteration 338/1000 | Loss: 0.00002226
Iteration 339/1000 | Loss: 0.00002226
Iteration 340/1000 | Loss: 0.00002226
Iteration 341/1000 | Loss: 0.00002226
Iteration 342/1000 | Loss: 0.00002226
Iteration 343/1000 | Loss: 0.00002226
Iteration 344/1000 | Loss: 0.00002226
Iteration 345/1000 | Loss: 0.00002226
Iteration 346/1000 | Loss: 0.00002226
Iteration 347/1000 | Loss: 0.00002226
Iteration 348/1000 | Loss: 0.00002226
Iteration 349/1000 | Loss: 0.00002226
Iteration 350/1000 | Loss: 0.00002226
Iteration 351/1000 | Loss: 0.00002226
Iteration 352/1000 | Loss: 0.00002226
Iteration 353/1000 | Loss: 0.00002226
Iteration 354/1000 | Loss: 0.00002226
Iteration 355/1000 | Loss: 0.00002225
Iteration 356/1000 | Loss: 0.00002225
Iteration 357/1000 | Loss: 0.00002225
Iteration 358/1000 | Loss: 0.00002225
Iteration 359/1000 | Loss: 0.00002225
Iteration 360/1000 | Loss: 0.00002225
Iteration 361/1000 | Loss: 0.00002225
Iteration 362/1000 | Loss: 0.00002225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 362. Stopping optimization.
Last 5 losses: [2.225496973551344e-05, 2.225496973551344e-05, 2.225496973551344e-05, 2.225496973551344e-05, 2.225496973551344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.225496973551344e-05

Optimization complete. Final v2v error: 3.9731528759002686 mm

Highest mean error: 4.347788333892822 mm for frame 19

Lowest mean error: 3.740037202835083 mm for frame 94

Saving results

Total time: 51.63350486755371
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00501495
Iteration 2/25 | Loss: 0.00096049
Iteration 3/25 | Loss: 0.00082797
Iteration 4/25 | Loss: 0.00080334
Iteration 5/25 | Loss: 0.00079526
Iteration 6/25 | Loss: 0.00079365
Iteration 7/25 | Loss: 0.00079330
Iteration 8/25 | Loss: 0.00079330
Iteration 9/25 | Loss: 0.00079330
Iteration 10/25 | Loss: 0.00079330
Iteration 11/25 | Loss: 0.00079330
Iteration 12/25 | Loss: 0.00079330
Iteration 13/25 | Loss: 0.00079330
Iteration 14/25 | Loss: 0.00079330
Iteration 15/25 | Loss: 0.00079330
Iteration 16/25 | Loss: 0.00079330
Iteration 17/25 | Loss: 0.00079330
Iteration 18/25 | Loss: 0.00079330
Iteration 19/25 | Loss: 0.00079330
Iteration 20/25 | Loss: 0.00079330
Iteration 21/25 | Loss: 0.00079330
Iteration 22/25 | Loss: 0.00079330
Iteration 23/25 | Loss: 0.00079330
Iteration 24/25 | Loss: 0.00079330
Iteration 25/25 | Loss: 0.00079330

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49702227
Iteration 2/25 | Loss: 0.00065911
Iteration 3/25 | Loss: 0.00065907
Iteration 4/25 | Loss: 0.00065907
Iteration 5/25 | Loss: 0.00065907
Iteration 6/25 | Loss: 0.00065907
Iteration 7/25 | Loss: 0.00065907
Iteration 8/25 | Loss: 0.00065907
Iteration 9/25 | Loss: 0.00065907
Iteration 10/25 | Loss: 0.00065907
Iteration 11/25 | Loss: 0.00065907
Iteration 12/25 | Loss: 0.00065907
Iteration 13/25 | Loss: 0.00065907
Iteration 14/25 | Loss: 0.00065907
Iteration 15/25 | Loss: 0.00065907
Iteration 16/25 | Loss: 0.00065907
Iteration 17/25 | Loss: 0.00065907
Iteration 18/25 | Loss: 0.00065907
Iteration 19/25 | Loss: 0.00065907
Iteration 20/25 | Loss: 0.00065907
Iteration 21/25 | Loss: 0.00065907
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006590713746845722, 0.0006590713746845722, 0.0006590713746845722, 0.0006590713746845722, 0.0006590713746845722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006590713746845722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065907
Iteration 2/1000 | Loss: 0.00003019
Iteration 3/1000 | Loss: 0.00002568
Iteration 4/1000 | Loss: 0.00002308
Iteration 5/1000 | Loss: 0.00002194
Iteration 6/1000 | Loss: 0.00002125
Iteration 7/1000 | Loss: 0.00002075
Iteration 8/1000 | Loss: 0.00002034
Iteration 9/1000 | Loss: 0.00002007
Iteration 10/1000 | Loss: 0.00001995
Iteration 11/1000 | Loss: 0.00001994
Iteration 12/1000 | Loss: 0.00001994
Iteration 13/1000 | Loss: 0.00001992
Iteration 14/1000 | Loss: 0.00001992
Iteration 15/1000 | Loss: 0.00001992
Iteration 16/1000 | Loss: 0.00001990
Iteration 17/1000 | Loss: 0.00001989
Iteration 18/1000 | Loss: 0.00001987
Iteration 19/1000 | Loss: 0.00001986
Iteration 20/1000 | Loss: 0.00001986
Iteration 21/1000 | Loss: 0.00001986
Iteration 22/1000 | Loss: 0.00001982
Iteration 23/1000 | Loss: 0.00001982
Iteration 24/1000 | Loss: 0.00001982
Iteration 25/1000 | Loss: 0.00001981
Iteration 26/1000 | Loss: 0.00001980
Iteration 27/1000 | Loss: 0.00001979
Iteration 28/1000 | Loss: 0.00001978
Iteration 29/1000 | Loss: 0.00001978
Iteration 30/1000 | Loss: 0.00001978
Iteration 31/1000 | Loss: 0.00001978
Iteration 32/1000 | Loss: 0.00001977
Iteration 33/1000 | Loss: 0.00001977
Iteration 34/1000 | Loss: 0.00001977
Iteration 35/1000 | Loss: 0.00001976
Iteration 36/1000 | Loss: 0.00001976
Iteration 37/1000 | Loss: 0.00001975
Iteration 38/1000 | Loss: 0.00001975
Iteration 39/1000 | Loss: 0.00001975
Iteration 40/1000 | Loss: 0.00001974
Iteration 41/1000 | Loss: 0.00001974
Iteration 42/1000 | Loss: 0.00001974
Iteration 43/1000 | Loss: 0.00001974
Iteration 44/1000 | Loss: 0.00001973
Iteration 45/1000 | Loss: 0.00001973
Iteration 46/1000 | Loss: 0.00001973
Iteration 47/1000 | Loss: 0.00001973
Iteration 48/1000 | Loss: 0.00001973
Iteration 49/1000 | Loss: 0.00001972
Iteration 50/1000 | Loss: 0.00001972
Iteration 51/1000 | Loss: 0.00001972
Iteration 52/1000 | Loss: 0.00001971
Iteration 53/1000 | Loss: 0.00001971
Iteration 54/1000 | Loss: 0.00001971
Iteration 55/1000 | Loss: 0.00001971
Iteration 56/1000 | Loss: 0.00001971
Iteration 57/1000 | Loss: 0.00001970
Iteration 58/1000 | Loss: 0.00001970
Iteration 59/1000 | Loss: 0.00001970
Iteration 60/1000 | Loss: 0.00001970
Iteration 61/1000 | Loss: 0.00001970
Iteration 62/1000 | Loss: 0.00001970
Iteration 63/1000 | Loss: 0.00001969
Iteration 64/1000 | Loss: 0.00001969
Iteration 65/1000 | Loss: 0.00001969
Iteration 66/1000 | Loss: 0.00001969
Iteration 67/1000 | Loss: 0.00001969
Iteration 68/1000 | Loss: 0.00001968
Iteration 69/1000 | Loss: 0.00001968
Iteration 70/1000 | Loss: 0.00001968
Iteration 71/1000 | Loss: 0.00001968
Iteration 72/1000 | Loss: 0.00001968
Iteration 73/1000 | Loss: 0.00001968
Iteration 74/1000 | Loss: 0.00001968
Iteration 75/1000 | Loss: 0.00001968
Iteration 76/1000 | Loss: 0.00001967
Iteration 77/1000 | Loss: 0.00001967
Iteration 78/1000 | Loss: 0.00001967
Iteration 79/1000 | Loss: 0.00001967
Iteration 80/1000 | Loss: 0.00001967
Iteration 81/1000 | Loss: 0.00001967
Iteration 82/1000 | Loss: 0.00001967
Iteration 83/1000 | Loss: 0.00001967
Iteration 84/1000 | Loss: 0.00001967
Iteration 85/1000 | Loss: 0.00001967
Iteration 86/1000 | Loss: 0.00001967
Iteration 87/1000 | Loss: 0.00001967
Iteration 88/1000 | Loss: 0.00001967
Iteration 89/1000 | Loss: 0.00001966
Iteration 90/1000 | Loss: 0.00001966
Iteration 91/1000 | Loss: 0.00001966
Iteration 92/1000 | Loss: 0.00001966
Iteration 93/1000 | Loss: 0.00001966
Iteration 94/1000 | Loss: 0.00001966
Iteration 95/1000 | Loss: 0.00001965
Iteration 96/1000 | Loss: 0.00001965
Iteration 97/1000 | Loss: 0.00001965
Iteration 98/1000 | Loss: 0.00001965
Iteration 99/1000 | Loss: 0.00001965
Iteration 100/1000 | Loss: 0.00001964
Iteration 101/1000 | Loss: 0.00001964
Iteration 102/1000 | Loss: 0.00001964
Iteration 103/1000 | Loss: 0.00001964
Iteration 104/1000 | Loss: 0.00001964
Iteration 105/1000 | Loss: 0.00001964
Iteration 106/1000 | Loss: 0.00001963
Iteration 107/1000 | Loss: 0.00001963
Iteration 108/1000 | Loss: 0.00001963
Iteration 109/1000 | Loss: 0.00001963
Iteration 110/1000 | Loss: 0.00001963
Iteration 111/1000 | Loss: 0.00001963
Iteration 112/1000 | Loss: 0.00001963
Iteration 113/1000 | Loss: 0.00001963
Iteration 114/1000 | Loss: 0.00001963
Iteration 115/1000 | Loss: 0.00001963
Iteration 116/1000 | Loss: 0.00001963
Iteration 117/1000 | Loss: 0.00001963
Iteration 118/1000 | Loss: 0.00001963
Iteration 119/1000 | Loss: 0.00001963
Iteration 120/1000 | Loss: 0.00001963
Iteration 121/1000 | Loss: 0.00001962
Iteration 122/1000 | Loss: 0.00001962
Iteration 123/1000 | Loss: 0.00001962
Iteration 124/1000 | Loss: 0.00001962
Iteration 125/1000 | Loss: 0.00001962
Iteration 126/1000 | Loss: 0.00001962
Iteration 127/1000 | Loss: 0.00001961
Iteration 128/1000 | Loss: 0.00001961
Iteration 129/1000 | Loss: 0.00001961
Iteration 130/1000 | Loss: 0.00001961
Iteration 131/1000 | Loss: 0.00001961
Iteration 132/1000 | Loss: 0.00001961
Iteration 133/1000 | Loss: 0.00001961
Iteration 134/1000 | Loss: 0.00001961
Iteration 135/1000 | Loss: 0.00001961
Iteration 136/1000 | Loss: 0.00001961
Iteration 137/1000 | Loss: 0.00001961
Iteration 138/1000 | Loss: 0.00001961
Iteration 139/1000 | Loss: 0.00001961
Iteration 140/1000 | Loss: 0.00001960
Iteration 141/1000 | Loss: 0.00001960
Iteration 142/1000 | Loss: 0.00001960
Iteration 143/1000 | Loss: 0.00001960
Iteration 144/1000 | Loss: 0.00001960
Iteration 145/1000 | Loss: 0.00001960
Iteration 146/1000 | Loss: 0.00001960
Iteration 147/1000 | Loss: 0.00001960
Iteration 148/1000 | Loss: 0.00001960
Iteration 149/1000 | Loss: 0.00001960
Iteration 150/1000 | Loss: 0.00001960
Iteration 151/1000 | Loss: 0.00001960
Iteration 152/1000 | Loss: 0.00001960
Iteration 153/1000 | Loss: 0.00001960
Iteration 154/1000 | Loss: 0.00001960
Iteration 155/1000 | Loss: 0.00001960
Iteration 156/1000 | Loss: 0.00001960
Iteration 157/1000 | Loss: 0.00001960
Iteration 158/1000 | Loss: 0.00001960
Iteration 159/1000 | Loss: 0.00001959
Iteration 160/1000 | Loss: 0.00001959
Iteration 161/1000 | Loss: 0.00001959
Iteration 162/1000 | Loss: 0.00001959
Iteration 163/1000 | Loss: 0.00001959
Iteration 164/1000 | Loss: 0.00001959
Iteration 165/1000 | Loss: 0.00001959
Iteration 166/1000 | Loss: 0.00001959
Iteration 167/1000 | Loss: 0.00001959
Iteration 168/1000 | Loss: 0.00001959
Iteration 169/1000 | Loss: 0.00001959
Iteration 170/1000 | Loss: 0.00001959
Iteration 171/1000 | Loss: 0.00001959
Iteration 172/1000 | Loss: 0.00001959
Iteration 173/1000 | Loss: 0.00001959
Iteration 174/1000 | Loss: 0.00001959
Iteration 175/1000 | Loss: 0.00001959
Iteration 176/1000 | Loss: 0.00001959
Iteration 177/1000 | Loss: 0.00001959
Iteration 178/1000 | Loss: 0.00001959
Iteration 179/1000 | Loss: 0.00001959
Iteration 180/1000 | Loss: 0.00001959
Iteration 181/1000 | Loss: 0.00001959
Iteration 182/1000 | Loss: 0.00001959
Iteration 183/1000 | Loss: 0.00001959
Iteration 184/1000 | Loss: 0.00001959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.9594577679526992e-05, 1.9594577679526992e-05, 1.9594577679526992e-05, 1.9594577679526992e-05, 1.9594577679526992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9594577679526992e-05

Optimization complete. Final v2v error: 3.831268310546875 mm

Highest mean error: 4.700356960296631 mm for frame 49

Lowest mean error: 3.4197254180908203 mm for frame 21

Saving results

Total time: 36.9869601726532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822694
Iteration 2/25 | Loss: 0.00114686
Iteration 3/25 | Loss: 0.00095369
Iteration 4/25 | Loss: 0.00091547
Iteration 5/25 | Loss: 0.00089845
Iteration 6/25 | Loss: 0.00089528
Iteration 7/25 | Loss: 0.00089455
Iteration 8/25 | Loss: 0.00089455
Iteration 9/25 | Loss: 0.00089455
Iteration 10/25 | Loss: 0.00089455
Iteration 11/25 | Loss: 0.00089455
Iteration 12/25 | Loss: 0.00089455
Iteration 13/25 | Loss: 0.00089455
Iteration 14/25 | Loss: 0.00089455
Iteration 15/25 | Loss: 0.00089455
Iteration 16/25 | Loss: 0.00089455
Iteration 17/25 | Loss: 0.00089455
Iteration 18/25 | Loss: 0.00089455
Iteration 19/25 | Loss: 0.00089455
Iteration 20/25 | Loss: 0.00089455
Iteration 21/25 | Loss: 0.00089455
Iteration 22/25 | Loss: 0.00089455
Iteration 23/25 | Loss: 0.00089455
Iteration 24/25 | Loss: 0.00089455
Iteration 25/25 | Loss: 0.00089455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51586318
Iteration 2/25 | Loss: 0.00101284
Iteration 3/25 | Loss: 0.00101283
Iteration 4/25 | Loss: 0.00101283
Iteration 5/25 | Loss: 0.00101283
Iteration 6/25 | Loss: 0.00101283
Iteration 7/25 | Loss: 0.00101283
Iteration 8/25 | Loss: 0.00101283
Iteration 9/25 | Loss: 0.00101283
Iteration 10/25 | Loss: 0.00101283
Iteration 11/25 | Loss: 0.00101283
Iteration 12/25 | Loss: 0.00101283
Iteration 13/25 | Loss: 0.00101283
Iteration 14/25 | Loss: 0.00101283
Iteration 15/25 | Loss: 0.00101283
Iteration 16/25 | Loss: 0.00101283
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010128298308700323, 0.0010128298308700323, 0.0010128298308700323, 0.0010128298308700323, 0.0010128298308700323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010128298308700323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101283
Iteration 2/1000 | Loss: 0.00004914
Iteration 3/1000 | Loss: 0.00003739
Iteration 4/1000 | Loss: 0.00003155
Iteration 5/1000 | Loss: 0.00002954
Iteration 6/1000 | Loss: 0.00002821
Iteration 7/1000 | Loss: 0.00002718
Iteration 8/1000 | Loss: 0.00002639
Iteration 9/1000 | Loss: 0.00002585
Iteration 10/1000 | Loss: 0.00002552
Iteration 11/1000 | Loss: 0.00002531
Iteration 12/1000 | Loss: 0.00002525
Iteration 13/1000 | Loss: 0.00002520
Iteration 14/1000 | Loss: 0.00002513
Iteration 15/1000 | Loss: 0.00002505
Iteration 16/1000 | Loss: 0.00002499
Iteration 17/1000 | Loss: 0.00002499
Iteration 18/1000 | Loss: 0.00002497
Iteration 19/1000 | Loss: 0.00002496
Iteration 20/1000 | Loss: 0.00002495
Iteration 21/1000 | Loss: 0.00002495
Iteration 22/1000 | Loss: 0.00002495
Iteration 23/1000 | Loss: 0.00002494
Iteration 24/1000 | Loss: 0.00002494
Iteration 25/1000 | Loss: 0.00002493
Iteration 26/1000 | Loss: 0.00002491
Iteration 27/1000 | Loss: 0.00002491
Iteration 28/1000 | Loss: 0.00002490
Iteration 29/1000 | Loss: 0.00002489
Iteration 30/1000 | Loss: 0.00002489
Iteration 31/1000 | Loss: 0.00002489
Iteration 32/1000 | Loss: 0.00002489
Iteration 33/1000 | Loss: 0.00002489
Iteration 34/1000 | Loss: 0.00002489
Iteration 35/1000 | Loss: 0.00002488
Iteration 36/1000 | Loss: 0.00002488
Iteration 37/1000 | Loss: 0.00002488
Iteration 38/1000 | Loss: 0.00002488
Iteration 39/1000 | Loss: 0.00002488
Iteration 40/1000 | Loss: 0.00002488
Iteration 41/1000 | Loss: 0.00002488
Iteration 42/1000 | Loss: 0.00002488
Iteration 43/1000 | Loss: 0.00002487
Iteration 44/1000 | Loss: 0.00002487
Iteration 45/1000 | Loss: 0.00002487
Iteration 46/1000 | Loss: 0.00002487
Iteration 47/1000 | Loss: 0.00002487
Iteration 48/1000 | Loss: 0.00002487
Iteration 49/1000 | Loss: 0.00002487
Iteration 50/1000 | Loss: 0.00002487
Iteration 51/1000 | Loss: 0.00002487
Iteration 52/1000 | Loss: 0.00002486
Iteration 53/1000 | Loss: 0.00002486
Iteration 54/1000 | Loss: 0.00002486
Iteration 55/1000 | Loss: 0.00002486
Iteration 56/1000 | Loss: 0.00002486
Iteration 57/1000 | Loss: 0.00002486
Iteration 58/1000 | Loss: 0.00002486
Iteration 59/1000 | Loss: 0.00002486
Iteration 60/1000 | Loss: 0.00002486
Iteration 61/1000 | Loss: 0.00002485
Iteration 62/1000 | Loss: 0.00002485
Iteration 63/1000 | Loss: 0.00002485
Iteration 64/1000 | Loss: 0.00002485
Iteration 65/1000 | Loss: 0.00002485
Iteration 66/1000 | Loss: 0.00002485
Iteration 67/1000 | Loss: 0.00002485
Iteration 68/1000 | Loss: 0.00002485
Iteration 69/1000 | Loss: 0.00002485
Iteration 70/1000 | Loss: 0.00002484
Iteration 71/1000 | Loss: 0.00002484
Iteration 72/1000 | Loss: 0.00002484
Iteration 73/1000 | Loss: 0.00002484
Iteration 74/1000 | Loss: 0.00002484
Iteration 75/1000 | Loss: 0.00002484
Iteration 76/1000 | Loss: 0.00002484
Iteration 77/1000 | Loss: 0.00002484
Iteration 78/1000 | Loss: 0.00002484
Iteration 79/1000 | Loss: 0.00002484
Iteration 80/1000 | Loss: 0.00002483
Iteration 81/1000 | Loss: 0.00002483
Iteration 82/1000 | Loss: 0.00002483
Iteration 83/1000 | Loss: 0.00002483
Iteration 84/1000 | Loss: 0.00002483
Iteration 85/1000 | Loss: 0.00002482
Iteration 86/1000 | Loss: 0.00002482
Iteration 87/1000 | Loss: 0.00002482
Iteration 88/1000 | Loss: 0.00002482
Iteration 89/1000 | Loss: 0.00002482
Iteration 90/1000 | Loss: 0.00002482
Iteration 91/1000 | Loss: 0.00002482
Iteration 92/1000 | Loss: 0.00002482
Iteration 93/1000 | Loss: 0.00002482
Iteration 94/1000 | Loss: 0.00002482
Iteration 95/1000 | Loss: 0.00002482
Iteration 96/1000 | Loss: 0.00002482
Iteration 97/1000 | Loss: 0.00002482
Iteration 98/1000 | Loss: 0.00002482
Iteration 99/1000 | Loss: 0.00002482
Iteration 100/1000 | Loss: 0.00002482
Iteration 101/1000 | Loss: 0.00002482
Iteration 102/1000 | Loss: 0.00002482
Iteration 103/1000 | Loss: 0.00002482
Iteration 104/1000 | Loss: 0.00002482
Iteration 105/1000 | Loss: 0.00002482
Iteration 106/1000 | Loss: 0.00002482
Iteration 107/1000 | Loss: 0.00002482
Iteration 108/1000 | Loss: 0.00002482
Iteration 109/1000 | Loss: 0.00002482
Iteration 110/1000 | Loss: 0.00002482
Iteration 111/1000 | Loss: 0.00002482
Iteration 112/1000 | Loss: 0.00002482
Iteration 113/1000 | Loss: 0.00002482
Iteration 114/1000 | Loss: 0.00002482
Iteration 115/1000 | Loss: 0.00002482
Iteration 116/1000 | Loss: 0.00002482
Iteration 117/1000 | Loss: 0.00002482
Iteration 118/1000 | Loss: 0.00002482
Iteration 119/1000 | Loss: 0.00002482
Iteration 120/1000 | Loss: 0.00002482
Iteration 121/1000 | Loss: 0.00002482
Iteration 122/1000 | Loss: 0.00002482
Iteration 123/1000 | Loss: 0.00002482
Iteration 124/1000 | Loss: 0.00002482
Iteration 125/1000 | Loss: 0.00002482
Iteration 126/1000 | Loss: 0.00002482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.4819393729558215e-05, 2.4819393729558215e-05, 2.4819393729558215e-05, 2.4819393729558215e-05, 2.4819393729558215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4819393729558215e-05

Optimization complete. Final v2v error: 4.140443801879883 mm

Highest mean error: 5.0852227210998535 mm for frame 40

Lowest mean error: 3.5328574180603027 mm for frame 135

Saving results

Total time: 38.334431409835815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887463
Iteration 2/25 | Loss: 0.00102544
Iteration 3/25 | Loss: 0.00084553
Iteration 4/25 | Loss: 0.00082163
Iteration 5/25 | Loss: 0.00081327
Iteration 6/25 | Loss: 0.00081134
Iteration 7/25 | Loss: 0.00081084
Iteration 8/25 | Loss: 0.00081084
Iteration 9/25 | Loss: 0.00081084
Iteration 10/25 | Loss: 0.00081084
Iteration 11/25 | Loss: 0.00081084
Iteration 12/25 | Loss: 0.00081084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008108392357826233, 0.0008108392357826233, 0.0008108392357826233, 0.0008108392357826233, 0.0008108392357826233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008108392357826233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62893498
Iteration 2/25 | Loss: 0.00072107
Iteration 3/25 | Loss: 0.00072106
Iteration 4/25 | Loss: 0.00072106
Iteration 5/25 | Loss: 0.00072106
Iteration 6/25 | Loss: 0.00072106
Iteration 7/25 | Loss: 0.00072106
Iteration 8/25 | Loss: 0.00072106
Iteration 9/25 | Loss: 0.00072106
Iteration 10/25 | Loss: 0.00072106
Iteration 11/25 | Loss: 0.00072106
Iteration 12/25 | Loss: 0.00072106
Iteration 13/25 | Loss: 0.00072106
Iteration 14/25 | Loss: 0.00072106
Iteration 15/25 | Loss: 0.00072106
Iteration 16/25 | Loss: 0.00072106
Iteration 17/25 | Loss: 0.00072106
Iteration 18/25 | Loss: 0.00072106
Iteration 19/25 | Loss: 0.00072106
Iteration 20/25 | Loss: 0.00072106
Iteration 21/25 | Loss: 0.00072106
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007210609619505703, 0.0007210609619505703, 0.0007210609619505703, 0.0007210609619505703, 0.0007210609619505703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007210609619505703

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072106
Iteration 2/1000 | Loss: 0.00003288
Iteration 3/1000 | Loss: 0.00002610
Iteration 4/1000 | Loss: 0.00002304
Iteration 5/1000 | Loss: 0.00002181
Iteration 6/1000 | Loss: 0.00002101
Iteration 7/1000 | Loss: 0.00002045
Iteration 8/1000 | Loss: 0.00001995
Iteration 9/1000 | Loss: 0.00001963
Iteration 10/1000 | Loss: 0.00001953
Iteration 11/1000 | Loss: 0.00001951
Iteration 12/1000 | Loss: 0.00001950
Iteration 13/1000 | Loss: 0.00001949
Iteration 14/1000 | Loss: 0.00001944
Iteration 15/1000 | Loss: 0.00001943
Iteration 16/1000 | Loss: 0.00001943
Iteration 17/1000 | Loss: 0.00001937
Iteration 18/1000 | Loss: 0.00001934
Iteration 19/1000 | Loss: 0.00001933
Iteration 20/1000 | Loss: 0.00001932
Iteration 21/1000 | Loss: 0.00001927
Iteration 22/1000 | Loss: 0.00001924
Iteration 23/1000 | Loss: 0.00001923
Iteration 24/1000 | Loss: 0.00001923
Iteration 25/1000 | Loss: 0.00001919
Iteration 26/1000 | Loss: 0.00001919
Iteration 27/1000 | Loss: 0.00001919
Iteration 28/1000 | Loss: 0.00001919
Iteration 29/1000 | Loss: 0.00001919
Iteration 30/1000 | Loss: 0.00001919
Iteration 31/1000 | Loss: 0.00001918
Iteration 32/1000 | Loss: 0.00001918
Iteration 33/1000 | Loss: 0.00001918
Iteration 34/1000 | Loss: 0.00001918
Iteration 35/1000 | Loss: 0.00001918
Iteration 36/1000 | Loss: 0.00001917
Iteration 37/1000 | Loss: 0.00001916
Iteration 38/1000 | Loss: 0.00001916
Iteration 39/1000 | Loss: 0.00001915
Iteration 40/1000 | Loss: 0.00001912
Iteration 41/1000 | Loss: 0.00001912
Iteration 42/1000 | Loss: 0.00001911
Iteration 43/1000 | Loss: 0.00001911
Iteration 44/1000 | Loss: 0.00001910
Iteration 45/1000 | Loss: 0.00001910
Iteration 46/1000 | Loss: 0.00001910
Iteration 47/1000 | Loss: 0.00001909
Iteration 48/1000 | Loss: 0.00001909
Iteration 49/1000 | Loss: 0.00001909
Iteration 50/1000 | Loss: 0.00001909
Iteration 51/1000 | Loss: 0.00001909
Iteration 52/1000 | Loss: 0.00001908
Iteration 53/1000 | Loss: 0.00001908
Iteration 54/1000 | Loss: 0.00001908
Iteration 55/1000 | Loss: 0.00001908
Iteration 56/1000 | Loss: 0.00001908
Iteration 57/1000 | Loss: 0.00001908
Iteration 58/1000 | Loss: 0.00001908
Iteration 59/1000 | Loss: 0.00001908
Iteration 60/1000 | Loss: 0.00001907
Iteration 61/1000 | Loss: 0.00001907
Iteration 62/1000 | Loss: 0.00001907
Iteration 63/1000 | Loss: 0.00001907
Iteration 64/1000 | Loss: 0.00001907
Iteration 65/1000 | Loss: 0.00001907
Iteration 66/1000 | Loss: 0.00001906
Iteration 67/1000 | Loss: 0.00001906
Iteration 68/1000 | Loss: 0.00001906
Iteration 69/1000 | Loss: 0.00001906
Iteration 70/1000 | Loss: 0.00001905
Iteration 71/1000 | Loss: 0.00001905
Iteration 72/1000 | Loss: 0.00001905
Iteration 73/1000 | Loss: 0.00001905
Iteration 74/1000 | Loss: 0.00001905
Iteration 75/1000 | Loss: 0.00001905
Iteration 76/1000 | Loss: 0.00001904
Iteration 77/1000 | Loss: 0.00001904
Iteration 78/1000 | Loss: 0.00001904
Iteration 79/1000 | Loss: 0.00001903
Iteration 80/1000 | Loss: 0.00001903
Iteration 81/1000 | Loss: 0.00001903
Iteration 82/1000 | Loss: 0.00001903
Iteration 83/1000 | Loss: 0.00001903
Iteration 84/1000 | Loss: 0.00001903
Iteration 85/1000 | Loss: 0.00001903
Iteration 86/1000 | Loss: 0.00001902
Iteration 87/1000 | Loss: 0.00001902
Iteration 88/1000 | Loss: 0.00001902
Iteration 89/1000 | Loss: 0.00001901
Iteration 90/1000 | Loss: 0.00001901
Iteration 91/1000 | Loss: 0.00001901
Iteration 92/1000 | Loss: 0.00001901
Iteration 93/1000 | Loss: 0.00001901
Iteration 94/1000 | Loss: 0.00001901
Iteration 95/1000 | Loss: 0.00001901
Iteration 96/1000 | Loss: 0.00001901
Iteration 97/1000 | Loss: 0.00001901
Iteration 98/1000 | Loss: 0.00001901
Iteration 99/1000 | Loss: 0.00001901
Iteration 100/1000 | Loss: 0.00001900
Iteration 101/1000 | Loss: 0.00001900
Iteration 102/1000 | Loss: 0.00001900
Iteration 103/1000 | Loss: 0.00001900
Iteration 104/1000 | Loss: 0.00001900
Iteration 105/1000 | Loss: 0.00001900
Iteration 106/1000 | Loss: 0.00001900
Iteration 107/1000 | Loss: 0.00001900
Iteration 108/1000 | Loss: 0.00001900
Iteration 109/1000 | Loss: 0.00001899
Iteration 110/1000 | Loss: 0.00001899
Iteration 111/1000 | Loss: 0.00001899
Iteration 112/1000 | Loss: 0.00001899
Iteration 113/1000 | Loss: 0.00001899
Iteration 114/1000 | Loss: 0.00001898
Iteration 115/1000 | Loss: 0.00001898
Iteration 116/1000 | Loss: 0.00001898
Iteration 117/1000 | Loss: 0.00001898
Iteration 118/1000 | Loss: 0.00001898
Iteration 119/1000 | Loss: 0.00001898
Iteration 120/1000 | Loss: 0.00001898
Iteration 121/1000 | Loss: 0.00001898
Iteration 122/1000 | Loss: 0.00001898
Iteration 123/1000 | Loss: 0.00001897
Iteration 124/1000 | Loss: 0.00001897
Iteration 125/1000 | Loss: 0.00001897
Iteration 126/1000 | Loss: 0.00001897
Iteration 127/1000 | Loss: 0.00001897
Iteration 128/1000 | Loss: 0.00001897
Iteration 129/1000 | Loss: 0.00001897
Iteration 130/1000 | Loss: 0.00001897
Iteration 131/1000 | Loss: 0.00001897
Iteration 132/1000 | Loss: 0.00001897
Iteration 133/1000 | Loss: 0.00001897
Iteration 134/1000 | Loss: 0.00001897
Iteration 135/1000 | Loss: 0.00001897
Iteration 136/1000 | Loss: 0.00001897
Iteration 137/1000 | Loss: 0.00001897
Iteration 138/1000 | Loss: 0.00001897
Iteration 139/1000 | Loss: 0.00001897
Iteration 140/1000 | Loss: 0.00001897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.8970240489579737e-05, 1.8970240489579737e-05, 1.8970240489579737e-05, 1.8970240489579737e-05, 1.8970240489579737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8970240489579737e-05

Optimization complete. Final v2v error: 3.738624095916748 mm

Highest mean error: 4.194028854370117 mm for frame 67

Lowest mean error: 3.31108021736145 mm for frame 146

Saving results

Total time: 37.32045841217041
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856750
Iteration 2/25 | Loss: 0.00095936
Iteration 3/25 | Loss: 0.00077188
Iteration 4/25 | Loss: 0.00074660
Iteration 5/25 | Loss: 0.00074170
Iteration 6/25 | Loss: 0.00073982
Iteration 7/25 | Loss: 0.00073961
Iteration 8/25 | Loss: 0.00073961
Iteration 9/25 | Loss: 0.00073961
Iteration 10/25 | Loss: 0.00073961
Iteration 11/25 | Loss: 0.00073961
Iteration 12/25 | Loss: 0.00073961
Iteration 13/25 | Loss: 0.00073961
Iteration 14/25 | Loss: 0.00073961
Iteration 15/25 | Loss: 0.00073961
Iteration 16/25 | Loss: 0.00073961
Iteration 17/25 | Loss: 0.00073961
Iteration 18/25 | Loss: 0.00073961
Iteration 19/25 | Loss: 0.00073961
Iteration 20/25 | Loss: 0.00073961
Iteration 21/25 | Loss: 0.00073961
Iteration 22/25 | Loss: 0.00073961
Iteration 23/25 | Loss: 0.00073961
Iteration 24/25 | Loss: 0.00073961
Iteration 25/25 | Loss: 0.00073961

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50962889
Iteration 2/25 | Loss: 0.00066809
Iteration 3/25 | Loss: 0.00066809
Iteration 4/25 | Loss: 0.00066809
Iteration 5/25 | Loss: 0.00066809
Iteration 6/25 | Loss: 0.00066809
Iteration 7/25 | Loss: 0.00066809
Iteration 8/25 | Loss: 0.00066809
Iteration 9/25 | Loss: 0.00066809
Iteration 10/25 | Loss: 0.00066809
Iteration 11/25 | Loss: 0.00066809
Iteration 12/25 | Loss: 0.00066809
Iteration 13/25 | Loss: 0.00066809
Iteration 14/25 | Loss: 0.00066809
Iteration 15/25 | Loss: 0.00066809
Iteration 16/25 | Loss: 0.00066809
Iteration 17/25 | Loss: 0.00066809
Iteration 18/25 | Loss: 0.00066809
Iteration 19/25 | Loss: 0.00066809
Iteration 20/25 | Loss: 0.00066809
Iteration 21/25 | Loss: 0.00066809
Iteration 22/25 | Loss: 0.00066809
Iteration 23/25 | Loss: 0.00066809
Iteration 24/25 | Loss: 0.00066809
Iteration 25/25 | Loss: 0.00066809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066809
Iteration 2/1000 | Loss: 0.00002054
Iteration 3/1000 | Loss: 0.00001755
Iteration 4/1000 | Loss: 0.00001603
Iteration 5/1000 | Loss: 0.00001531
Iteration 6/1000 | Loss: 0.00001502
Iteration 7/1000 | Loss: 0.00001470
Iteration 8/1000 | Loss: 0.00001459
Iteration 9/1000 | Loss: 0.00001441
Iteration 10/1000 | Loss: 0.00001435
Iteration 11/1000 | Loss: 0.00001428
Iteration 12/1000 | Loss: 0.00001428
Iteration 13/1000 | Loss: 0.00001423
Iteration 14/1000 | Loss: 0.00001422
Iteration 15/1000 | Loss: 0.00001421
Iteration 16/1000 | Loss: 0.00001421
Iteration 17/1000 | Loss: 0.00001420
Iteration 18/1000 | Loss: 0.00001414
Iteration 19/1000 | Loss: 0.00001411
Iteration 20/1000 | Loss: 0.00001410
Iteration 21/1000 | Loss: 0.00001410
Iteration 22/1000 | Loss: 0.00001409
Iteration 23/1000 | Loss: 0.00001409
Iteration 24/1000 | Loss: 0.00001408
Iteration 25/1000 | Loss: 0.00001408
Iteration 26/1000 | Loss: 0.00001408
Iteration 27/1000 | Loss: 0.00001407
Iteration 28/1000 | Loss: 0.00001407
Iteration 29/1000 | Loss: 0.00001407
Iteration 30/1000 | Loss: 0.00001406
Iteration 31/1000 | Loss: 0.00001406
Iteration 32/1000 | Loss: 0.00001406
Iteration 33/1000 | Loss: 0.00001405
Iteration 34/1000 | Loss: 0.00001405
Iteration 35/1000 | Loss: 0.00001405
Iteration 36/1000 | Loss: 0.00001405
Iteration 37/1000 | Loss: 0.00001405
Iteration 38/1000 | Loss: 0.00001404
Iteration 39/1000 | Loss: 0.00001404
Iteration 40/1000 | Loss: 0.00001404
Iteration 41/1000 | Loss: 0.00001403
Iteration 42/1000 | Loss: 0.00001402
Iteration 43/1000 | Loss: 0.00001402
Iteration 44/1000 | Loss: 0.00001401
Iteration 45/1000 | Loss: 0.00001401
Iteration 46/1000 | Loss: 0.00001400
Iteration 47/1000 | Loss: 0.00001400
Iteration 48/1000 | Loss: 0.00001400
Iteration 49/1000 | Loss: 0.00001400
Iteration 50/1000 | Loss: 0.00001400
Iteration 51/1000 | Loss: 0.00001400
Iteration 52/1000 | Loss: 0.00001399
Iteration 53/1000 | Loss: 0.00001399
Iteration 54/1000 | Loss: 0.00001399
Iteration 55/1000 | Loss: 0.00001399
Iteration 56/1000 | Loss: 0.00001399
Iteration 57/1000 | Loss: 0.00001399
Iteration 58/1000 | Loss: 0.00001399
Iteration 59/1000 | Loss: 0.00001399
Iteration 60/1000 | Loss: 0.00001399
Iteration 61/1000 | Loss: 0.00001399
Iteration 62/1000 | Loss: 0.00001399
Iteration 63/1000 | Loss: 0.00001399
Iteration 64/1000 | Loss: 0.00001399
Iteration 65/1000 | Loss: 0.00001399
Iteration 66/1000 | Loss: 0.00001399
Iteration 67/1000 | Loss: 0.00001399
Iteration 68/1000 | Loss: 0.00001399
Iteration 69/1000 | Loss: 0.00001399
Iteration 70/1000 | Loss: 0.00001399
Iteration 71/1000 | Loss: 0.00001399
Iteration 72/1000 | Loss: 0.00001399
Iteration 73/1000 | Loss: 0.00001399
Iteration 74/1000 | Loss: 0.00001399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.3985719306219835e-05, 1.3985719306219835e-05, 1.3985719306219835e-05, 1.3985719306219835e-05, 1.3985719306219835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3985719306219835e-05

Optimization complete. Final v2v error: 3.20308256149292 mm

Highest mean error: 3.7338151931762695 mm for frame 99

Lowest mean error: 2.8567605018615723 mm for frame 44

Saving results

Total time: 32.64642858505249
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406919
Iteration 2/25 | Loss: 0.00084142
Iteration 3/25 | Loss: 0.00075437
Iteration 4/25 | Loss: 0.00073762
Iteration 5/25 | Loss: 0.00073044
Iteration 6/25 | Loss: 0.00072850
Iteration 7/25 | Loss: 0.00072847
Iteration 8/25 | Loss: 0.00072847
Iteration 9/25 | Loss: 0.00072847
Iteration 10/25 | Loss: 0.00072828
Iteration 11/25 | Loss: 0.00072828
Iteration 12/25 | Loss: 0.00072828
Iteration 13/25 | Loss: 0.00072828
Iteration 14/25 | Loss: 0.00072828
Iteration 15/25 | Loss: 0.00072828
Iteration 16/25 | Loss: 0.00072828
Iteration 17/25 | Loss: 0.00072828
Iteration 18/25 | Loss: 0.00072828
Iteration 19/25 | Loss: 0.00072828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007282774313353002, 0.0007282774313353002, 0.0007282774313353002, 0.0007282774313353002, 0.0007282774313353002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007282774313353002

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48323166
Iteration 2/25 | Loss: 0.00052939
Iteration 3/25 | Loss: 0.00052939
Iteration 4/25 | Loss: 0.00052938
Iteration 5/25 | Loss: 0.00052938
Iteration 6/25 | Loss: 0.00052938
Iteration 7/25 | Loss: 0.00052938
Iteration 8/25 | Loss: 0.00052938
Iteration 9/25 | Loss: 0.00052938
Iteration 10/25 | Loss: 0.00052938
Iteration 11/25 | Loss: 0.00052938
Iteration 12/25 | Loss: 0.00052938
Iteration 13/25 | Loss: 0.00052938
Iteration 14/25 | Loss: 0.00052938
Iteration 15/25 | Loss: 0.00052938
Iteration 16/25 | Loss: 0.00052938
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005293830181472003, 0.0005293830181472003, 0.0005293830181472003, 0.0005293830181472003, 0.0005293830181472003]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005293830181472003

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052938
Iteration 2/1000 | Loss: 0.00001862
Iteration 3/1000 | Loss: 0.00001592
Iteration 4/1000 | Loss: 0.00001511
Iteration 5/1000 | Loss: 0.00001455
Iteration 6/1000 | Loss: 0.00001419
Iteration 7/1000 | Loss: 0.00001397
Iteration 8/1000 | Loss: 0.00001396
Iteration 9/1000 | Loss: 0.00001395
Iteration 10/1000 | Loss: 0.00001395
Iteration 11/1000 | Loss: 0.00001395
Iteration 12/1000 | Loss: 0.00001389
Iteration 13/1000 | Loss: 0.00001389
Iteration 14/1000 | Loss: 0.00001389
Iteration 15/1000 | Loss: 0.00001388
Iteration 16/1000 | Loss: 0.00001388
Iteration 17/1000 | Loss: 0.00001388
Iteration 18/1000 | Loss: 0.00001388
Iteration 19/1000 | Loss: 0.00001386
Iteration 20/1000 | Loss: 0.00001386
Iteration 21/1000 | Loss: 0.00001385
Iteration 22/1000 | Loss: 0.00001385
Iteration 23/1000 | Loss: 0.00001385
Iteration 24/1000 | Loss: 0.00001385
Iteration 25/1000 | Loss: 0.00001384
Iteration 26/1000 | Loss: 0.00001384
Iteration 27/1000 | Loss: 0.00001383
Iteration 28/1000 | Loss: 0.00001383
Iteration 29/1000 | Loss: 0.00001383
Iteration 30/1000 | Loss: 0.00001383
Iteration 31/1000 | Loss: 0.00001382
Iteration 32/1000 | Loss: 0.00001382
Iteration 33/1000 | Loss: 0.00001382
Iteration 34/1000 | Loss: 0.00001382
Iteration 35/1000 | Loss: 0.00001382
Iteration 36/1000 | Loss: 0.00001381
Iteration 37/1000 | Loss: 0.00001381
Iteration 38/1000 | Loss: 0.00001381
Iteration 39/1000 | Loss: 0.00001381
Iteration 40/1000 | Loss: 0.00001381
Iteration 41/1000 | Loss: 0.00001381
Iteration 42/1000 | Loss: 0.00001381
Iteration 43/1000 | Loss: 0.00001380
Iteration 44/1000 | Loss: 0.00001380
Iteration 45/1000 | Loss: 0.00001380
Iteration 46/1000 | Loss: 0.00001380
Iteration 47/1000 | Loss: 0.00001380
Iteration 48/1000 | Loss: 0.00001380
Iteration 49/1000 | Loss: 0.00001380
Iteration 50/1000 | Loss: 0.00001380
Iteration 51/1000 | Loss: 0.00001380
Iteration 52/1000 | Loss: 0.00001380
Iteration 53/1000 | Loss: 0.00001380
Iteration 54/1000 | Loss: 0.00001379
Iteration 55/1000 | Loss: 0.00001379
Iteration 56/1000 | Loss: 0.00001379
Iteration 57/1000 | Loss: 0.00001379
Iteration 58/1000 | Loss: 0.00001379
Iteration 59/1000 | Loss: 0.00001379
Iteration 60/1000 | Loss: 0.00001378
Iteration 61/1000 | Loss: 0.00001378
Iteration 62/1000 | Loss: 0.00001378
Iteration 63/1000 | Loss: 0.00001378
Iteration 64/1000 | Loss: 0.00001378
Iteration 65/1000 | Loss: 0.00001378
Iteration 66/1000 | Loss: 0.00001377
Iteration 67/1000 | Loss: 0.00001377
Iteration 68/1000 | Loss: 0.00001377
Iteration 69/1000 | Loss: 0.00001376
Iteration 70/1000 | Loss: 0.00001376
Iteration 71/1000 | Loss: 0.00001376
Iteration 72/1000 | Loss: 0.00001376
Iteration 73/1000 | Loss: 0.00001376
Iteration 74/1000 | Loss: 0.00001376
Iteration 75/1000 | Loss: 0.00001376
Iteration 76/1000 | Loss: 0.00001376
Iteration 77/1000 | Loss: 0.00001375
Iteration 78/1000 | Loss: 0.00001375
Iteration 79/1000 | Loss: 0.00001375
Iteration 80/1000 | Loss: 0.00001375
Iteration 81/1000 | Loss: 0.00001374
Iteration 82/1000 | Loss: 0.00001374
Iteration 83/1000 | Loss: 0.00001374
Iteration 84/1000 | Loss: 0.00001374
Iteration 85/1000 | Loss: 0.00001374
Iteration 86/1000 | Loss: 0.00001374
Iteration 87/1000 | Loss: 0.00001374
Iteration 88/1000 | Loss: 0.00001374
Iteration 89/1000 | Loss: 0.00001374
Iteration 90/1000 | Loss: 0.00001374
Iteration 91/1000 | Loss: 0.00001373
Iteration 92/1000 | Loss: 0.00001373
Iteration 93/1000 | Loss: 0.00001373
Iteration 94/1000 | Loss: 0.00001373
Iteration 95/1000 | Loss: 0.00001373
Iteration 96/1000 | Loss: 0.00001373
Iteration 97/1000 | Loss: 0.00001373
Iteration 98/1000 | Loss: 0.00001372
Iteration 99/1000 | Loss: 0.00001372
Iteration 100/1000 | Loss: 0.00001371
Iteration 101/1000 | Loss: 0.00001371
Iteration 102/1000 | Loss: 0.00001371
Iteration 103/1000 | Loss: 0.00001370
Iteration 104/1000 | Loss: 0.00001370
Iteration 105/1000 | Loss: 0.00001370
Iteration 106/1000 | Loss: 0.00001370
Iteration 107/1000 | Loss: 0.00001370
Iteration 108/1000 | Loss: 0.00001370
Iteration 109/1000 | Loss: 0.00001370
Iteration 110/1000 | Loss: 0.00001370
Iteration 111/1000 | Loss: 0.00001370
Iteration 112/1000 | Loss: 0.00001370
Iteration 113/1000 | Loss: 0.00001370
Iteration 114/1000 | Loss: 0.00001370
Iteration 115/1000 | Loss: 0.00001370
Iteration 116/1000 | Loss: 0.00001370
Iteration 117/1000 | Loss: 0.00001370
Iteration 118/1000 | Loss: 0.00001370
Iteration 119/1000 | Loss: 0.00001370
Iteration 120/1000 | Loss: 0.00001370
Iteration 121/1000 | Loss: 0.00001370
Iteration 122/1000 | Loss: 0.00001370
Iteration 123/1000 | Loss: 0.00001370
Iteration 124/1000 | Loss: 0.00001370
Iteration 125/1000 | Loss: 0.00001370
Iteration 126/1000 | Loss: 0.00001370
Iteration 127/1000 | Loss: 0.00001370
Iteration 128/1000 | Loss: 0.00001370
Iteration 129/1000 | Loss: 0.00001370
Iteration 130/1000 | Loss: 0.00001370
Iteration 131/1000 | Loss: 0.00001370
Iteration 132/1000 | Loss: 0.00001370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.3702385331271216e-05, 1.3702385331271216e-05, 1.3702385331271216e-05, 1.3702385331271216e-05, 1.3702385331271216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3702385331271216e-05

Optimization complete. Final v2v error: 3.162646770477295 mm

Highest mean error: 3.5429704189300537 mm for frame 45

Lowest mean error: 2.874392032623291 mm for frame 0

Saving results

Total time: 32.52996802330017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00745803
Iteration 2/25 | Loss: 0.00137493
Iteration 3/25 | Loss: 0.00089360
Iteration 4/25 | Loss: 0.00086014
Iteration 5/25 | Loss: 0.00085449
Iteration 6/25 | Loss: 0.00085341
Iteration 7/25 | Loss: 0.00085341
Iteration 8/25 | Loss: 0.00085341
Iteration 9/25 | Loss: 0.00085341
Iteration 10/25 | Loss: 0.00085341
Iteration 11/25 | Loss: 0.00085341
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008534149383194745, 0.0008534149383194745, 0.0008534149383194745, 0.0008534149383194745, 0.0008534149383194745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008534149383194745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03902149
Iteration 2/25 | Loss: 0.00077807
Iteration 3/25 | Loss: 0.00077805
Iteration 4/25 | Loss: 0.00077805
Iteration 5/25 | Loss: 0.00077805
Iteration 6/25 | Loss: 0.00077804
Iteration 7/25 | Loss: 0.00077804
Iteration 8/25 | Loss: 0.00077804
Iteration 9/25 | Loss: 0.00077804
Iteration 10/25 | Loss: 0.00077804
Iteration 11/25 | Loss: 0.00077804
Iteration 12/25 | Loss: 0.00077804
Iteration 13/25 | Loss: 0.00077804
Iteration 14/25 | Loss: 0.00077804
Iteration 15/25 | Loss: 0.00077804
Iteration 16/25 | Loss: 0.00077804
Iteration 17/25 | Loss: 0.00077804
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007780436426401138, 0.0007780436426401138, 0.0007780436426401138, 0.0007780436426401138, 0.0007780436426401138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007780436426401138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077804
Iteration 2/1000 | Loss: 0.00003214
Iteration 3/1000 | Loss: 0.00002441
Iteration 4/1000 | Loss: 0.00002224
Iteration 5/1000 | Loss: 0.00002091
Iteration 6/1000 | Loss: 0.00002027
Iteration 7/1000 | Loss: 0.00001986
Iteration 8/1000 | Loss: 0.00001955
Iteration 9/1000 | Loss: 0.00001932
Iteration 10/1000 | Loss: 0.00001926
Iteration 11/1000 | Loss: 0.00001926
Iteration 12/1000 | Loss: 0.00001919
Iteration 13/1000 | Loss: 0.00001913
Iteration 14/1000 | Loss: 0.00001913
Iteration 15/1000 | Loss: 0.00001912
Iteration 16/1000 | Loss: 0.00001912
Iteration 17/1000 | Loss: 0.00001908
Iteration 18/1000 | Loss: 0.00001908
Iteration 19/1000 | Loss: 0.00001907
Iteration 20/1000 | Loss: 0.00001896
Iteration 21/1000 | Loss: 0.00001895
Iteration 22/1000 | Loss: 0.00001894
Iteration 23/1000 | Loss: 0.00001893
Iteration 24/1000 | Loss: 0.00001892
Iteration 25/1000 | Loss: 0.00001891
Iteration 26/1000 | Loss: 0.00001891
Iteration 27/1000 | Loss: 0.00001890
Iteration 28/1000 | Loss: 0.00001890
Iteration 29/1000 | Loss: 0.00001889
Iteration 30/1000 | Loss: 0.00001889
Iteration 31/1000 | Loss: 0.00001889
Iteration 32/1000 | Loss: 0.00001889
Iteration 33/1000 | Loss: 0.00001887
Iteration 34/1000 | Loss: 0.00001886
Iteration 35/1000 | Loss: 0.00001885
Iteration 36/1000 | Loss: 0.00001885
Iteration 37/1000 | Loss: 0.00001884
Iteration 38/1000 | Loss: 0.00001884
Iteration 39/1000 | Loss: 0.00001884
Iteration 40/1000 | Loss: 0.00001883
Iteration 41/1000 | Loss: 0.00001883
Iteration 42/1000 | Loss: 0.00001882
Iteration 43/1000 | Loss: 0.00001882
Iteration 44/1000 | Loss: 0.00001882
Iteration 45/1000 | Loss: 0.00001881
Iteration 46/1000 | Loss: 0.00001881
Iteration 47/1000 | Loss: 0.00001881
Iteration 48/1000 | Loss: 0.00001881
Iteration 49/1000 | Loss: 0.00001881
Iteration 50/1000 | Loss: 0.00001880
Iteration 51/1000 | Loss: 0.00001880
Iteration 52/1000 | Loss: 0.00001880
Iteration 53/1000 | Loss: 0.00001879
Iteration 54/1000 | Loss: 0.00001879
Iteration 55/1000 | Loss: 0.00001879
Iteration 56/1000 | Loss: 0.00001879
Iteration 57/1000 | Loss: 0.00001879
Iteration 58/1000 | Loss: 0.00001879
Iteration 59/1000 | Loss: 0.00001879
Iteration 60/1000 | Loss: 0.00001878
Iteration 61/1000 | Loss: 0.00001878
Iteration 62/1000 | Loss: 0.00001878
Iteration 63/1000 | Loss: 0.00001877
Iteration 64/1000 | Loss: 0.00001877
Iteration 65/1000 | Loss: 0.00001877
Iteration 66/1000 | Loss: 0.00001876
Iteration 67/1000 | Loss: 0.00001876
Iteration 68/1000 | Loss: 0.00001876
Iteration 69/1000 | Loss: 0.00001876
Iteration 70/1000 | Loss: 0.00001876
Iteration 71/1000 | Loss: 0.00001876
Iteration 72/1000 | Loss: 0.00001876
Iteration 73/1000 | Loss: 0.00001876
Iteration 74/1000 | Loss: 0.00001876
Iteration 75/1000 | Loss: 0.00001876
Iteration 76/1000 | Loss: 0.00001876
Iteration 77/1000 | Loss: 0.00001876
Iteration 78/1000 | Loss: 0.00001876
Iteration 79/1000 | Loss: 0.00001876
Iteration 80/1000 | Loss: 0.00001876
Iteration 81/1000 | Loss: 0.00001876
Iteration 82/1000 | Loss: 0.00001876
Iteration 83/1000 | Loss: 0.00001876
Iteration 84/1000 | Loss: 0.00001876
Iteration 85/1000 | Loss: 0.00001876
Iteration 86/1000 | Loss: 0.00001876
Iteration 87/1000 | Loss: 0.00001876
Iteration 88/1000 | Loss: 0.00001876
Iteration 89/1000 | Loss: 0.00001876
Iteration 90/1000 | Loss: 0.00001876
Iteration 91/1000 | Loss: 0.00001876
Iteration 92/1000 | Loss: 0.00001876
Iteration 93/1000 | Loss: 0.00001876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.8758888472802937e-05, 1.8758888472802937e-05, 1.8758888472802937e-05, 1.8758888472802937e-05, 1.8758888472802937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8758888472802937e-05

Optimization complete. Final v2v error: 3.771355390548706 mm

Highest mean error: 4.169494152069092 mm for frame 32

Lowest mean error: 3.2714803218841553 mm for frame 238

Saving results

Total time: 35.881887912750244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00730151
Iteration 2/25 | Loss: 0.00154335
Iteration 3/25 | Loss: 0.00102979
Iteration 4/25 | Loss: 0.00085740
Iteration 5/25 | Loss: 0.00081499
Iteration 6/25 | Loss: 0.00080652
Iteration 7/25 | Loss: 0.00079773
Iteration 8/25 | Loss: 0.00079290
Iteration 9/25 | Loss: 0.00078851
Iteration 10/25 | Loss: 0.00078597
Iteration 11/25 | Loss: 0.00078951
Iteration 12/25 | Loss: 0.00078625
Iteration 13/25 | Loss: 0.00077998
Iteration 14/25 | Loss: 0.00077883
Iteration 15/25 | Loss: 0.00078122
Iteration 16/25 | Loss: 0.00078034
Iteration 17/25 | Loss: 0.00078070
Iteration 18/25 | Loss: 0.00078026
Iteration 19/25 | Loss: 0.00078010
Iteration 20/25 | Loss: 0.00077999
Iteration 21/25 | Loss: 0.00078061
Iteration 22/25 | Loss: 0.00078057
Iteration 23/25 | Loss: 0.00077975
Iteration 24/25 | Loss: 0.00078060
Iteration 25/25 | Loss: 0.00078030

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.37860012
Iteration 2/25 | Loss: 0.00065455
Iteration 3/25 | Loss: 0.00065451
Iteration 4/25 | Loss: 0.00065450
Iteration 5/25 | Loss: 0.00065450
Iteration 6/25 | Loss: 0.00065450
Iteration 7/25 | Loss: 0.00065450
Iteration 8/25 | Loss: 0.00065450
Iteration 9/25 | Loss: 0.00065450
Iteration 10/25 | Loss: 0.00065450
Iteration 11/25 | Loss: 0.00065450
Iteration 12/25 | Loss: 0.00065450
Iteration 13/25 | Loss: 0.00065450
Iteration 14/25 | Loss: 0.00065450
Iteration 15/25 | Loss: 0.00065450
Iteration 16/25 | Loss: 0.00065450
Iteration 17/25 | Loss: 0.00065450
Iteration 18/25 | Loss: 0.00065450
Iteration 19/25 | Loss: 0.00065450
Iteration 20/25 | Loss: 0.00065450
Iteration 21/25 | Loss: 0.00065450
Iteration 22/25 | Loss: 0.00065450
Iteration 23/25 | Loss: 0.00065450
Iteration 24/25 | Loss: 0.00065450
Iteration 25/25 | Loss: 0.00065450

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065450
Iteration 2/1000 | Loss: 0.00003344
Iteration 3/1000 | Loss: 0.00002771
Iteration 4/1000 | Loss: 0.00005883
Iteration 5/1000 | Loss: 0.00004350
Iteration 6/1000 | Loss: 0.00003797
Iteration 7/1000 | Loss: 0.00003884
Iteration 8/1000 | Loss: 0.00004256
Iteration 9/1000 | Loss: 0.00002893
Iteration 10/1000 | Loss: 0.00002247
Iteration 11/1000 | Loss: 0.00004250
Iteration 12/1000 | Loss: 0.00003513
Iteration 13/1000 | Loss: 0.00010110
Iteration 14/1000 | Loss: 0.00004914
Iteration 15/1000 | Loss: 0.00006976
Iteration 16/1000 | Loss: 0.00004169
Iteration 17/1000 | Loss: 0.00003832
Iteration 18/1000 | Loss: 0.00005294
Iteration 19/1000 | Loss: 0.00003524
Iteration 20/1000 | Loss: 0.00004040
Iteration 21/1000 | Loss: 0.00002291
Iteration 22/1000 | Loss: 0.00003556
Iteration 23/1000 | Loss: 0.00002551
Iteration 24/1000 | Loss: 0.00002386
Iteration 25/1000 | Loss: 0.00002247
Iteration 26/1000 | Loss: 0.00002108
Iteration 27/1000 | Loss: 0.00001989
Iteration 28/1000 | Loss: 0.00001888
Iteration 29/1000 | Loss: 0.00001837
Iteration 30/1000 | Loss: 0.00001804
Iteration 31/1000 | Loss: 0.00001793
Iteration 32/1000 | Loss: 0.00001786
Iteration 33/1000 | Loss: 0.00001779
Iteration 34/1000 | Loss: 0.00001778
Iteration 35/1000 | Loss: 0.00001777
Iteration 36/1000 | Loss: 0.00001772
Iteration 37/1000 | Loss: 0.00001771
Iteration 38/1000 | Loss: 0.00001771
Iteration 39/1000 | Loss: 0.00001770
Iteration 40/1000 | Loss: 0.00001769
Iteration 41/1000 | Loss: 0.00001765
Iteration 42/1000 | Loss: 0.00001763
Iteration 43/1000 | Loss: 0.00001763
Iteration 44/1000 | Loss: 0.00001762
Iteration 45/1000 | Loss: 0.00001762
Iteration 46/1000 | Loss: 0.00001760
Iteration 47/1000 | Loss: 0.00001760
Iteration 48/1000 | Loss: 0.00001759
Iteration 49/1000 | Loss: 0.00001759
Iteration 50/1000 | Loss: 0.00001758
Iteration 51/1000 | Loss: 0.00001758
Iteration 52/1000 | Loss: 0.00001758
Iteration 53/1000 | Loss: 0.00001757
Iteration 54/1000 | Loss: 0.00001757
Iteration 55/1000 | Loss: 0.00001756
Iteration 56/1000 | Loss: 0.00001756
Iteration 57/1000 | Loss: 0.00001755
Iteration 58/1000 | Loss: 0.00001755
Iteration 59/1000 | Loss: 0.00001754
Iteration 60/1000 | Loss: 0.00001753
Iteration 61/1000 | Loss: 0.00001752
Iteration 62/1000 | Loss: 0.00001751
Iteration 63/1000 | Loss: 0.00001750
Iteration 64/1000 | Loss: 0.00001750
Iteration 65/1000 | Loss: 0.00001750
Iteration 66/1000 | Loss: 0.00001749
Iteration 67/1000 | Loss: 0.00001749
Iteration 68/1000 | Loss: 0.00001748
Iteration 69/1000 | Loss: 0.00001748
Iteration 70/1000 | Loss: 0.00001747
Iteration 71/1000 | Loss: 0.00001747
Iteration 72/1000 | Loss: 0.00001747
Iteration 73/1000 | Loss: 0.00001747
Iteration 74/1000 | Loss: 0.00001746
Iteration 75/1000 | Loss: 0.00001746
Iteration 76/1000 | Loss: 0.00001746
Iteration 77/1000 | Loss: 0.00001746
Iteration 78/1000 | Loss: 0.00001746
Iteration 79/1000 | Loss: 0.00001746
Iteration 80/1000 | Loss: 0.00001746
Iteration 81/1000 | Loss: 0.00001745
Iteration 82/1000 | Loss: 0.00001745
Iteration 83/1000 | Loss: 0.00001745
Iteration 84/1000 | Loss: 0.00001745
Iteration 85/1000 | Loss: 0.00001745
Iteration 86/1000 | Loss: 0.00001744
Iteration 87/1000 | Loss: 0.00001744
Iteration 88/1000 | Loss: 0.00001744
Iteration 89/1000 | Loss: 0.00001744
Iteration 90/1000 | Loss: 0.00001744
Iteration 91/1000 | Loss: 0.00001744
Iteration 92/1000 | Loss: 0.00001743
Iteration 93/1000 | Loss: 0.00001743
Iteration 94/1000 | Loss: 0.00001742
Iteration 95/1000 | Loss: 0.00001742
Iteration 96/1000 | Loss: 0.00001742
Iteration 97/1000 | Loss: 0.00001742
Iteration 98/1000 | Loss: 0.00001742
Iteration 99/1000 | Loss: 0.00001742
Iteration 100/1000 | Loss: 0.00001742
Iteration 101/1000 | Loss: 0.00001742
Iteration 102/1000 | Loss: 0.00001742
Iteration 103/1000 | Loss: 0.00001741
Iteration 104/1000 | Loss: 0.00001741
Iteration 105/1000 | Loss: 0.00001741
Iteration 106/1000 | Loss: 0.00001741
Iteration 107/1000 | Loss: 0.00001741
Iteration 108/1000 | Loss: 0.00001741
Iteration 109/1000 | Loss: 0.00001741
Iteration 110/1000 | Loss: 0.00001741
Iteration 111/1000 | Loss: 0.00001740
Iteration 112/1000 | Loss: 0.00001740
Iteration 113/1000 | Loss: 0.00001740
Iteration 114/1000 | Loss: 0.00001740
Iteration 115/1000 | Loss: 0.00001740
Iteration 116/1000 | Loss: 0.00001740
Iteration 117/1000 | Loss: 0.00001740
Iteration 118/1000 | Loss: 0.00001740
Iteration 119/1000 | Loss: 0.00001740
Iteration 120/1000 | Loss: 0.00001739
Iteration 121/1000 | Loss: 0.00001739
Iteration 122/1000 | Loss: 0.00001739
Iteration 123/1000 | Loss: 0.00001739
Iteration 124/1000 | Loss: 0.00001739
Iteration 125/1000 | Loss: 0.00001738
Iteration 126/1000 | Loss: 0.00001738
Iteration 127/1000 | Loss: 0.00001738
Iteration 128/1000 | Loss: 0.00001738
Iteration 129/1000 | Loss: 0.00001738
Iteration 130/1000 | Loss: 0.00001738
Iteration 131/1000 | Loss: 0.00001738
Iteration 132/1000 | Loss: 0.00001738
Iteration 133/1000 | Loss: 0.00001738
Iteration 134/1000 | Loss: 0.00001738
Iteration 135/1000 | Loss: 0.00001737
Iteration 136/1000 | Loss: 0.00001737
Iteration 137/1000 | Loss: 0.00001737
Iteration 138/1000 | Loss: 0.00001737
Iteration 139/1000 | Loss: 0.00001737
Iteration 140/1000 | Loss: 0.00001737
Iteration 141/1000 | Loss: 0.00001737
Iteration 142/1000 | Loss: 0.00001737
Iteration 143/1000 | Loss: 0.00001737
Iteration 144/1000 | Loss: 0.00001737
Iteration 145/1000 | Loss: 0.00001737
Iteration 146/1000 | Loss: 0.00001737
Iteration 147/1000 | Loss: 0.00001737
Iteration 148/1000 | Loss: 0.00001737
Iteration 149/1000 | Loss: 0.00001737
Iteration 150/1000 | Loss: 0.00001737
Iteration 151/1000 | Loss: 0.00001737
Iteration 152/1000 | Loss: 0.00001737
Iteration 153/1000 | Loss: 0.00001737
Iteration 154/1000 | Loss: 0.00001737
Iteration 155/1000 | Loss: 0.00001737
Iteration 156/1000 | Loss: 0.00001737
Iteration 157/1000 | Loss: 0.00001737
Iteration 158/1000 | Loss: 0.00001737
Iteration 159/1000 | Loss: 0.00001737
Iteration 160/1000 | Loss: 0.00001737
Iteration 161/1000 | Loss: 0.00001737
Iteration 162/1000 | Loss: 0.00001737
Iteration 163/1000 | Loss: 0.00001737
Iteration 164/1000 | Loss: 0.00001737
Iteration 165/1000 | Loss: 0.00001737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.7368069165968336e-05, 1.7368069165968336e-05, 1.7368069165968336e-05, 1.7368069165968336e-05, 1.7368069165968336e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7368069165968336e-05

Optimization complete. Final v2v error: 3.591822624206543 mm

Highest mean error: 4.22254753112793 mm for frame 148

Lowest mean error: 2.978102684020996 mm for frame 131

Saving results

Total time: 104.53980469703674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856193
Iteration 2/25 | Loss: 0.00163422
Iteration 3/25 | Loss: 0.00099452
Iteration 4/25 | Loss: 0.00087821
Iteration 5/25 | Loss: 0.00086262
Iteration 6/25 | Loss: 0.00085976
Iteration 7/25 | Loss: 0.00085919
Iteration 8/25 | Loss: 0.00085919
Iteration 9/25 | Loss: 0.00085919
Iteration 10/25 | Loss: 0.00085919
Iteration 11/25 | Loss: 0.00085919
Iteration 12/25 | Loss: 0.00085919
Iteration 13/25 | Loss: 0.00085919
Iteration 14/25 | Loss: 0.00085919
Iteration 15/25 | Loss: 0.00085919
Iteration 16/25 | Loss: 0.00085919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008591908845119178, 0.0008591908845119178, 0.0008591908845119178, 0.0008591908845119178, 0.0008591908845119178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008591908845119178

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54226434
Iteration 2/25 | Loss: 0.00072231
Iteration 3/25 | Loss: 0.00072230
Iteration 4/25 | Loss: 0.00072230
Iteration 5/25 | Loss: 0.00072230
Iteration 6/25 | Loss: 0.00072229
Iteration 7/25 | Loss: 0.00072229
Iteration 8/25 | Loss: 0.00072229
Iteration 9/25 | Loss: 0.00072229
Iteration 10/25 | Loss: 0.00072229
Iteration 11/25 | Loss: 0.00072229
Iteration 12/25 | Loss: 0.00072229
Iteration 13/25 | Loss: 0.00072229
Iteration 14/25 | Loss: 0.00072229
Iteration 15/25 | Loss: 0.00072229
Iteration 16/25 | Loss: 0.00072229
Iteration 17/25 | Loss: 0.00072229
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000722293509170413, 0.000722293509170413, 0.000722293509170413, 0.000722293509170413, 0.000722293509170413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000722293509170413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072229
Iteration 2/1000 | Loss: 0.00002845
Iteration 3/1000 | Loss: 0.00002323
Iteration 4/1000 | Loss: 0.00002090
Iteration 5/1000 | Loss: 0.00002013
Iteration 6/1000 | Loss: 0.00001968
Iteration 7/1000 | Loss: 0.00001943
Iteration 8/1000 | Loss: 0.00001918
Iteration 9/1000 | Loss: 0.00001902
Iteration 10/1000 | Loss: 0.00001898
Iteration 11/1000 | Loss: 0.00001897
Iteration 12/1000 | Loss: 0.00001896
Iteration 13/1000 | Loss: 0.00001896
Iteration 14/1000 | Loss: 0.00001893
Iteration 15/1000 | Loss: 0.00001893
Iteration 16/1000 | Loss: 0.00001893
Iteration 17/1000 | Loss: 0.00001892
Iteration 18/1000 | Loss: 0.00001892
Iteration 19/1000 | Loss: 0.00001892
Iteration 20/1000 | Loss: 0.00001892
Iteration 21/1000 | Loss: 0.00001892
Iteration 22/1000 | Loss: 0.00001892
Iteration 23/1000 | Loss: 0.00001889
Iteration 24/1000 | Loss: 0.00001889
Iteration 25/1000 | Loss: 0.00001888
Iteration 26/1000 | Loss: 0.00001888
Iteration 27/1000 | Loss: 0.00001888
Iteration 28/1000 | Loss: 0.00001888
Iteration 29/1000 | Loss: 0.00001888
Iteration 30/1000 | Loss: 0.00001888
Iteration 31/1000 | Loss: 0.00001888
Iteration 32/1000 | Loss: 0.00001887
Iteration 33/1000 | Loss: 0.00001887
Iteration 34/1000 | Loss: 0.00001887
Iteration 35/1000 | Loss: 0.00001887
Iteration 36/1000 | Loss: 0.00001887
Iteration 37/1000 | Loss: 0.00001887
Iteration 38/1000 | Loss: 0.00001887
Iteration 39/1000 | Loss: 0.00001886
Iteration 40/1000 | Loss: 0.00001886
Iteration 41/1000 | Loss: 0.00001886
Iteration 42/1000 | Loss: 0.00001886
Iteration 43/1000 | Loss: 0.00001886
Iteration 44/1000 | Loss: 0.00001886
Iteration 45/1000 | Loss: 0.00001886
Iteration 46/1000 | Loss: 0.00001885
Iteration 47/1000 | Loss: 0.00001885
Iteration 48/1000 | Loss: 0.00001885
Iteration 49/1000 | Loss: 0.00001884
Iteration 50/1000 | Loss: 0.00001884
Iteration 51/1000 | Loss: 0.00001884
Iteration 52/1000 | Loss: 0.00001884
Iteration 53/1000 | Loss: 0.00001884
Iteration 54/1000 | Loss: 0.00001884
Iteration 55/1000 | Loss: 0.00001883
Iteration 56/1000 | Loss: 0.00001883
Iteration 57/1000 | Loss: 0.00001883
Iteration 58/1000 | Loss: 0.00001883
Iteration 59/1000 | Loss: 0.00001883
Iteration 60/1000 | Loss: 0.00001883
Iteration 61/1000 | Loss: 0.00001883
Iteration 62/1000 | Loss: 0.00001882
Iteration 63/1000 | Loss: 0.00001882
Iteration 64/1000 | Loss: 0.00001882
Iteration 65/1000 | Loss: 0.00001882
Iteration 66/1000 | Loss: 0.00001882
Iteration 67/1000 | Loss: 0.00001881
Iteration 68/1000 | Loss: 0.00001881
Iteration 69/1000 | Loss: 0.00001881
Iteration 70/1000 | Loss: 0.00001881
Iteration 71/1000 | Loss: 0.00001881
Iteration 72/1000 | Loss: 0.00001880
Iteration 73/1000 | Loss: 0.00001880
Iteration 74/1000 | Loss: 0.00001880
Iteration 75/1000 | Loss: 0.00001879
Iteration 76/1000 | Loss: 0.00001879
Iteration 77/1000 | Loss: 0.00001879
Iteration 78/1000 | Loss: 0.00001879
Iteration 79/1000 | Loss: 0.00001879
Iteration 80/1000 | Loss: 0.00001879
Iteration 81/1000 | Loss: 0.00001879
Iteration 82/1000 | Loss: 0.00001879
Iteration 83/1000 | Loss: 0.00001879
Iteration 84/1000 | Loss: 0.00001879
Iteration 85/1000 | Loss: 0.00001879
Iteration 86/1000 | Loss: 0.00001879
Iteration 87/1000 | Loss: 0.00001879
Iteration 88/1000 | Loss: 0.00001879
Iteration 89/1000 | Loss: 0.00001879
Iteration 90/1000 | Loss: 0.00001879
Iteration 91/1000 | Loss: 0.00001879
Iteration 92/1000 | Loss: 0.00001879
Iteration 93/1000 | Loss: 0.00001879
Iteration 94/1000 | Loss: 0.00001879
Iteration 95/1000 | Loss: 0.00001879
Iteration 96/1000 | Loss: 0.00001879
Iteration 97/1000 | Loss: 0.00001879
Iteration 98/1000 | Loss: 0.00001879
Iteration 99/1000 | Loss: 0.00001879
Iteration 100/1000 | Loss: 0.00001879
Iteration 101/1000 | Loss: 0.00001879
Iteration 102/1000 | Loss: 0.00001879
Iteration 103/1000 | Loss: 0.00001879
Iteration 104/1000 | Loss: 0.00001879
Iteration 105/1000 | Loss: 0.00001879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.8790333342622034e-05, 1.8790333342622034e-05, 1.8790333342622034e-05, 1.8790333342622034e-05, 1.8790333342622034e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8790333342622034e-05

Optimization complete. Final v2v error: 3.6204569339752197 mm

Highest mean error: 4.310719966888428 mm for frame 138

Lowest mean error: 3.24521803855896 mm for frame 43

Saving results

Total time: 30.75734782218933
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01115032
Iteration 2/25 | Loss: 0.00154693
Iteration 3/25 | Loss: 0.00107218
Iteration 4/25 | Loss: 0.00107021
Iteration 5/25 | Loss: 0.00087962
Iteration 6/25 | Loss: 0.00090043
Iteration 7/25 | Loss: 0.00087011
Iteration 8/25 | Loss: 0.00085535
Iteration 9/25 | Loss: 0.00081259
Iteration 10/25 | Loss: 0.00082152
Iteration 11/25 | Loss: 0.00081154
Iteration 12/25 | Loss: 0.00078854
Iteration 13/25 | Loss: 0.00077128
Iteration 14/25 | Loss: 0.00075405
Iteration 15/25 | Loss: 0.00074562
Iteration 16/25 | Loss: 0.00074506
Iteration 17/25 | Loss: 0.00074152
Iteration 18/25 | Loss: 0.00073570
Iteration 19/25 | Loss: 0.00073325
Iteration 20/25 | Loss: 0.00073155
Iteration 21/25 | Loss: 0.00073184
Iteration 22/25 | Loss: 0.00073915
Iteration 23/25 | Loss: 0.00073415
Iteration 24/25 | Loss: 0.00072900
Iteration 25/25 | Loss: 0.00073154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50589478
Iteration 2/25 | Loss: 0.00078375
Iteration 3/25 | Loss: 0.00078375
Iteration 4/25 | Loss: 0.00078375
Iteration 5/25 | Loss: 0.00078375
Iteration 6/25 | Loss: 0.00078375
Iteration 7/25 | Loss: 0.00078374
Iteration 8/25 | Loss: 0.00078374
Iteration 9/25 | Loss: 0.00078374
Iteration 10/25 | Loss: 0.00078374
Iteration 11/25 | Loss: 0.00078374
Iteration 12/25 | Loss: 0.00078374
Iteration 13/25 | Loss: 0.00078374
Iteration 14/25 | Loss: 0.00078374
Iteration 15/25 | Loss: 0.00078374
Iteration 16/25 | Loss: 0.00078374
Iteration 17/25 | Loss: 0.00078374
Iteration 18/25 | Loss: 0.00078374
Iteration 19/25 | Loss: 0.00078374
Iteration 20/25 | Loss: 0.00078374
Iteration 21/25 | Loss: 0.00078374
Iteration 22/25 | Loss: 0.00078374
Iteration 23/25 | Loss: 0.00078374
Iteration 24/25 | Loss: 0.00078374
Iteration 25/25 | Loss: 0.00078374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078374
Iteration 2/1000 | Loss: 0.00040655
Iteration 3/1000 | Loss: 0.00031614
Iteration 4/1000 | Loss: 0.00006744
Iteration 5/1000 | Loss: 0.00004732
Iteration 6/1000 | Loss: 0.00005063
Iteration 7/1000 | Loss: 0.00005957
Iteration 8/1000 | Loss: 0.00005091
Iteration 9/1000 | Loss: 0.00004664
Iteration 10/1000 | Loss: 0.00006008
Iteration 11/1000 | Loss: 0.00006203
Iteration 12/1000 | Loss: 0.00082868
Iteration 13/1000 | Loss: 0.00090832
Iteration 14/1000 | Loss: 0.00031624
Iteration 15/1000 | Loss: 0.00005167
Iteration 16/1000 | Loss: 0.00003587
Iteration 17/1000 | Loss: 0.00084786
Iteration 18/1000 | Loss: 0.00039013
Iteration 19/1000 | Loss: 0.00003016
Iteration 20/1000 | Loss: 0.00002614
Iteration 21/1000 | Loss: 0.00002192
Iteration 22/1000 | Loss: 0.00001917
Iteration 23/1000 | Loss: 0.00002560
Iteration 24/1000 | Loss: 0.00002048
Iteration 25/1000 | Loss: 0.00001907
Iteration 26/1000 | Loss: 0.00001790
Iteration 27/1000 | Loss: 0.00002655
Iteration 28/1000 | Loss: 0.00002296
Iteration 29/1000 | Loss: 0.00002111
Iteration 30/1000 | Loss: 0.00001929
Iteration 31/1000 | Loss: 0.00002787
Iteration 32/1000 | Loss: 0.00002118
Iteration 33/1000 | Loss: 0.00002397
Iteration 34/1000 | Loss: 0.00002755
Iteration 35/1000 | Loss: 0.00001992
Iteration 36/1000 | Loss: 0.00001623
Iteration 37/1000 | Loss: 0.00001467
Iteration 38/1000 | Loss: 0.00001410
Iteration 39/1000 | Loss: 0.00001380
Iteration 40/1000 | Loss: 0.00001373
Iteration 41/1000 | Loss: 0.00001361
Iteration 42/1000 | Loss: 0.00001361
Iteration 43/1000 | Loss: 0.00001360
Iteration 44/1000 | Loss: 0.00001359
Iteration 45/1000 | Loss: 0.00001359
Iteration 46/1000 | Loss: 0.00001358
Iteration 47/1000 | Loss: 0.00001358
Iteration 48/1000 | Loss: 0.00001357
Iteration 49/1000 | Loss: 0.00001357
Iteration 50/1000 | Loss: 0.00001356
Iteration 51/1000 | Loss: 0.00001356
Iteration 52/1000 | Loss: 0.00001355
Iteration 53/1000 | Loss: 0.00001354
Iteration 54/1000 | Loss: 0.00001354
Iteration 55/1000 | Loss: 0.00001354
Iteration 56/1000 | Loss: 0.00001353
Iteration 57/1000 | Loss: 0.00001353
Iteration 58/1000 | Loss: 0.00001353
Iteration 59/1000 | Loss: 0.00001353
Iteration 60/1000 | Loss: 0.00001352
Iteration 61/1000 | Loss: 0.00001352
Iteration 62/1000 | Loss: 0.00001352
Iteration 63/1000 | Loss: 0.00001352
Iteration 64/1000 | Loss: 0.00001352
Iteration 65/1000 | Loss: 0.00001352
Iteration 66/1000 | Loss: 0.00001352
Iteration 67/1000 | Loss: 0.00001352
Iteration 68/1000 | Loss: 0.00001352
Iteration 69/1000 | Loss: 0.00001352
Iteration 70/1000 | Loss: 0.00001352
Iteration 71/1000 | Loss: 0.00001352
Iteration 72/1000 | Loss: 0.00001351
Iteration 73/1000 | Loss: 0.00001351
Iteration 74/1000 | Loss: 0.00001351
Iteration 75/1000 | Loss: 0.00001350
Iteration 76/1000 | Loss: 0.00001350
Iteration 77/1000 | Loss: 0.00001350
Iteration 78/1000 | Loss: 0.00001350
Iteration 79/1000 | Loss: 0.00001350
Iteration 80/1000 | Loss: 0.00001350
Iteration 81/1000 | Loss: 0.00001350
Iteration 82/1000 | Loss: 0.00001350
Iteration 83/1000 | Loss: 0.00001350
Iteration 84/1000 | Loss: 0.00001350
Iteration 85/1000 | Loss: 0.00001350
Iteration 86/1000 | Loss: 0.00001350
Iteration 87/1000 | Loss: 0.00001350
Iteration 88/1000 | Loss: 0.00001350
Iteration 89/1000 | Loss: 0.00001350
Iteration 90/1000 | Loss: 0.00001350
Iteration 91/1000 | Loss: 0.00001350
Iteration 92/1000 | Loss: 0.00001350
Iteration 93/1000 | Loss: 0.00001350
Iteration 94/1000 | Loss: 0.00001350
Iteration 95/1000 | Loss: 0.00001350
Iteration 96/1000 | Loss: 0.00001350
Iteration 97/1000 | Loss: 0.00001350
Iteration 98/1000 | Loss: 0.00001350
Iteration 99/1000 | Loss: 0.00001350
Iteration 100/1000 | Loss: 0.00001350
Iteration 101/1000 | Loss: 0.00001350
Iteration 102/1000 | Loss: 0.00001350
Iteration 103/1000 | Loss: 0.00001350
Iteration 104/1000 | Loss: 0.00001350
Iteration 105/1000 | Loss: 0.00001350
Iteration 106/1000 | Loss: 0.00001350
Iteration 107/1000 | Loss: 0.00001350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.3498793123289943e-05, 1.3498793123289943e-05, 1.3498793123289943e-05, 1.3498793123289943e-05, 1.3498793123289943e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3498793123289943e-05

Optimization complete. Final v2v error: 3.0952484607696533 mm

Highest mean error: 9.335597038269043 mm for frame 62

Lowest mean error: 2.5987486839294434 mm for frame 63

Saving results

Total time: 110.84401535987854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1481/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1481/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845423
Iteration 2/25 | Loss: 0.00088985
Iteration 3/25 | Loss: 0.00073629
Iteration 4/25 | Loss: 0.00071803
Iteration 5/25 | Loss: 0.00071462
Iteration 6/25 | Loss: 0.00071365
Iteration 7/25 | Loss: 0.00071354
Iteration 8/25 | Loss: 0.00071354
Iteration 9/25 | Loss: 0.00071354
Iteration 10/25 | Loss: 0.00071354
Iteration 11/25 | Loss: 0.00071354
Iteration 12/25 | Loss: 0.00071354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007135393097996712, 0.0007135393097996712, 0.0007135393097996712, 0.0007135393097996712, 0.0007135393097996712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007135393097996712

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49578965
Iteration 2/25 | Loss: 0.00057942
Iteration 3/25 | Loss: 0.00057941
Iteration 4/25 | Loss: 0.00057941
Iteration 5/25 | Loss: 0.00057941
Iteration 6/25 | Loss: 0.00057941
Iteration 7/25 | Loss: 0.00057941
Iteration 8/25 | Loss: 0.00057941
Iteration 9/25 | Loss: 0.00057941
Iteration 10/25 | Loss: 0.00057941
Iteration 11/25 | Loss: 0.00057941
Iteration 12/25 | Loss: 0.00057941
Iteration 13/25 | Loss: 0.00057941
Iteration 14/25 | Loss: 0.00057941
Iteration 15/25 | Loss: 0.00057941
Iteration 16/25 | Loss: 0.00057941
Iteration 17/25 | Loss: 0.00057941
Iteration 18/25 | Loss: 0.00057941
Iteration 19/25 | Loss: 0.00057941
Iteration 20/25 | Loss: 0.00057941
Iteration 21/25 | Loss: 0.00057941
Iteration 22/25 | Loss: 0.00057941
Iteration 23/25 | Loss: 0.00057941
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005794127355329692, 0.0005794127355329692, 0.0005794127355329692, 0.0005794127355329692, 0.0005794127355329692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005794127355329692

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057941
Iteration 2/1000 | Loss: 0.00001898
Iteration 3/1000 | Loss: 0.00001574
Iteration 4/1000 | Loss: 0.00001419
Iteration 5/1000 | Loss: 0.00001349
Iteration 6/1000 | Loss: 0.00001307
Iteration 7/1000 | Loss: 0.00001276
Iteration 8/1000 | Loss: 0.00001254
Iteration 9/1000 | Loss: 0.00001253
Iteration 10/1000 | Loss: 0.00001253
Iteration 11/1000 | Loss: 0.00001252
Iteration 12/1000 | Loss: 0.00001252
Iteration 13/1000 | Loss: 0.00001252
Iteration 14/1000 | Loss: 0.00001251
Iteration 15/1000 | Loss: 0.00001249
Iteration 16/1000 | Loss: 0.00001248
Iteration 17/1000 | Loss: 0.00001248
Iteration 18/1000 | Loss: 0.00001248
Iteration 19/1000 | Loss: 0.00001248
Iteration 20/1000 | Loss: 0.00001248
Iteration 21/1000 | Loss: 0.00001247
Iteration 22/1000 | Loss: 0.00001247
Iteration 23/1000 | Loss: 0.00001245
Iteration 24/1000 | Loss: 0.00001244
Iteration 25/1000 | Loss: 0.00001244
Iteration 26/1000 | Loss: 0.00001244
Iteration 27/1000 | Loss: 0.00001243
Iteration 28/1000 | Loss: 0.00001243
Iteration 29/1000 | Loss: 0.00001243
Iteration 30/1000 | Loss: 0.00001243
Iteration 31/1000 | Loss: 0.00001243
Iteration 32/1000 | Loss: 0.00001243
Iteration 33/1000 | Loss: 0.00001243
Iteration 34/1000 | Loss: 0.00001243
Iteration 35/1000 | Loss: 0.00001243
Iteration 36/1000 | Loss: 0.00001242
Iteration 37/1000 | Loss: 0.00001242
Iteration 38/1000 | Loss: 0.00001241
Iteration 39/1000 | Loss: 0.00001241
Iteration 40/1000 | Loss: 0.00001240
Iteration 41/1000 | Loss: 0.00001240
Iteration 42/1000 | Loss: 0.00001240
Iteration 43/1000 | Loss: 0.00001240
Iteration 44/1000 | Loss: 0.00001239
Iteration 45/1000 | Loss: 0.00001239
Iteration 46/1000 | Loss: 0.00001239
Iteration 47/1000 | Loss: 0.00001239
Iteration 48/1000 | Loss: 0.00001239
Iteration 49/1000 | Loss: 0.00001239
Iteration 50/1000 | Loss: 0.00001238
Iteration 51/1000 | Loss: 0.00001238
Iteration 52/1000 | Loss: 0.00001238
Iteration 53/1000 | Loss: 0.00001238
Iteration 54/1000 | Loss: 0.00001237
Iteration 55/1000 | Loss: 0.00001237
Iteration 56/1000 | Loss: 0.00001237
Iteration 57/1000 | Loss: 0.00001237
Iteration 58/1000 | Loss: 0.00001237
Iteration 59/1000 | Loss: 0.00001237
Iteration 60/1000 | Loss: 0.00001237
Iteration 61/1000 | Loss: 0.00001237
Iteration 62/1000 | Loss: 0.00001237
Iteration 63/1000 | Loss: 0.00001237
Iteration 64/1000 | Loss: 0.00001237
Iteration 65/1000 | Loss: 0.00001237
Iteration 66/1000 | Loss: 0.00001237
Iteration 67/1000 | Loss: 0.00001237
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001237
Iteration 70/1000 | Loss: 0.00001237
Iteration 71/1000 | Loss: 0.00001236
Iteration 72/1000 | Loss: 0.00001236
Iteration 73/1000 | Loss: 0.00001236
Iteration 74/1000 | Loss: 0.00001236
Iteration 75/1000 | Loss: 0.00001236
Iteration 76/1000 | Loss: 0.00001236
Iteration 77/1000 | Loss: 0.00001236
Iteration 78/1000 | Loss: 0.00001236
Iteration 79/1000 | Loss: 0.00001236
Iteration 80/1000 | Loss: 0.00001236
Iteration 81/1000 | Loss: 0.00001236
Iteration 82/1000 | Loss: 0.00001236
Iteration 83/1000 | Loss: 0.00001236
Iteration 84/1000 | Loss: 0.00001236
Iteration 85/1000 | Loss: 0.00001236
Iteration 86/1000 | Loss: 0.00001235
Iteration 87/1000 | Loss: 0.00001235
Iteration 88/1000 | Loss: 0.00001235
Iteration 89/1000 | Loss: 0.00001235
Iteration 90/1000 | Loss: 0.00001235
Iteration 91/1000 | Loss: 0.00001235
Iteration 92/1000 | Loss: 0.00001235
Iteration 93/1000 | Loss: 0.00001235
Iteration 94/1000 | Loss: 0.00001235
Iteration 95/1000 | Loss: 0.00001235
Iteration 96/1000 | Loss: 0.00001235
Iteration 97/1000 | Loss: 0.00001235
Iteration 98/1000 | Loss: 0.00001235
Iteration 99/1000 | Loss: 0.00001234
Iteration 100/1000 | Loss: 0.00001234
Iteration 101/1000 | Loss: 0.00001234
Iteration 102/1000 | Loss: 0.00001234
Iteration 103/1000 | Loss: 0.00001234
Iteration 104/1000 | Loss: 0.00001234
Iteration 105/1000 | Loss: 0.00001234
Iteration 106/1000 | Loss: 0.00001234
Iteration 107/1000 | Loss: 0.00001234
Iteration 108/1000 | Loss: 0.00001234
Iteration 109/1000 | Loss: 0.00001234
Iteration 110/1000 | Loss: 0.00001234
Iteration 111/1000 | Loss: 0.00001234
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001233
Iteration 114/1000 | Loss: 0.00001233
Iteration 115/1000 | Loss: 0.00001233
Iteration 116/1000 | Loss: 0.00001233
Iteration 117/1000 | Loss: 0.00001233
Iteration 118/1000 | Loss: 0.00001233
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001232
Iteration 123/1000 | Loss: 0.00001232
Iteration 124/1000 | Loss: 0.00001232
Iteration 125/1000 | Loss: 0.00001232
Iteration 126/1000 | Loss: 0.00001231
Iteration 127/1000 | Loss: 0.00001231
Iteration 128/1000 | Loss: 0.00001231
Iteration 129/1000 | Loss: 0.00001231
Iteration 130/1000 | Loss: 0.00001231
Iteration 131/1000 | Loss: 0.00001231
Iteration 132/1000 | Loss: 0.00001231
Iteration 133/1000 | Loss: 0.00001231
Iteration 134/1000 | Loss: 0.00001231
Iteration 135/1000 | Loss: 0.00001231
Iteration 136/1000 | Loss: 0.00001230
Iteration 137/1000 | Loss: 0.00001230
Iteration 138/1000 | Loss: 0.00001230
Iteration 139/1000 | Loss: 0.00001230
Iteration 140/1000 | Loss: 0.00001230
Iteration 141/1000 | Loss: 0.00001230
Iteration 142/1000 | Loss: 0.00001230
Iteration 143/1000 | Loss: 0.00001230
Iteration 144/1000 | Loss: 0.00001230
Iteration 145/1000 | Loss: 0.00001229
Iteration 146/1000 | Loss: 0.00001229
Iteration 147/1000 | Loss: 0.00001229
Iteration 148/1000 | Loss: 0.00001229
Iteration 149/1000 | Loss: 0.00001229
Iteration 150/1000 | Loss: 0.00001229
Iteration 151/1000 | Loss: 0.00001229
Iteration 152/1000 | Loss: 0.00001229
Iteration 153/1000 | Loss: 0.00001229
Iteration 154/1000 | Loss: 0.00001229
Iteration 155/1000 | Loss: 0.00001229
Iteration 156/1000 | Loss: 0.00001229
Iteration 157/1000 | Loss: 0.00001229
Iteration 158/1000 | Loss: 0.00001229
Iteration 159/1000 | Loss: 0.00001229
Iteration 160/1000 | Loss: 0.00001229
Iteration 161/1000 | Loss: 0.00001229
Iteration 162/1000 | Loss: 0.00001229
Iteration 163/1000 | Loss: 0.00001229
Iteration 164/1000 | Loss: 0.00001229
Iteration 165/1000 | Loss: 0.00001229
Iteration 166/1000 | Loss: 0.00001229
Iteration 167/1000 | Loss: 0.00001229
Iteration 168/1000 | Loss: 0.00001229
Iteration 169/1000 | Loss: 0.00001229
Iteration 170/1000 | Loss: 0.00001229
Iteration 171/1000 | Loss: 0.00001229
Iteration 172/1000 | Loss: 0.00001229
Iteration 173/1000 | Loss: 0.00001229
Iteration 174/1000 | Loss: 0.00001229
Iteration 175/1000 | Loss: 0.00001229
Iteration 176/1000 | Loss: 0.00001229
Iteration 177/1000 | Loss: 0.00001229
Iteration 178/1000 | Loss: 0.00001229
Iteration 179/1000 | Loss: 0.00001229
Iteration 180/1000 | Loss: 0.00001229
Iteration 181/1000 | Loss: 0.00001229
Iteration 182/1000 | Loss: 0.00001229
Iteration 183/1000 | Loss: 0.00001229
Iteration 184/1000 | Loss: 0.00001229
Iteration 185/1000 | Loss: 0.00001229
Iteration 186/1000 | Loss: 0.00001229
Iteration 187/1000 | Loss: 0.00001229
Iteration 188/1000 | Loss: 0.00001229
Iteration 189/1000 | Loss: 0.00001229
Iteration 190/1000 | Loss: 0.00001229
Iteration 191/1000 | Loss: 0.00001229
Iteration 192/1000 | Loss: 0.00001229
Iteration 193/1000 | Loss: 0.00001229
Iteration 194/1000 | Loss: 0.00001229
Iteration 195/1000 | Loss: 0.00001229
Iteration 196/1000 | Loss: 0.00001229
Iteration 197/1000 | Loss: 0.00001229
Iteration 198/1000 | Loss: 0.00001229
Iteration 199/1000 | Loss: 0.00001229
Iteration 200/1000 | Loss: 0.00001229
Iteration 201/1000 | Loss: 0.00001229
Iteration 202/1000 | Loss: 0.00001229
Iteration 203/1000 | Loss: 0.00001229
Iteration 204/1000 | Loss: 0.00001229
Iteration 205/1000 | Loss: 0.00001229
Iteration 206/1000 | Loss: 0.00001229
Iteration 207/1000 | Loss: 0.00001229
Iteration 208/1000 | Loss: 0.00001229
Iteration 209/1000 | Loss: 0.00001229
Iteration 210/1000 | Loss: 0.00001229
Iteration 211/1000 | Loss: 0.00001229
Iteration 212/1000 | Loss: 0.00001229
Iteration 213/1000 | Loss: 0.00001229
Iteration 214/1000 | Loss: 0.00001229
Iteration 215/1000 | Loss: 0.00001229
Iteration 216/1000 | Loss: 0.00001229
Iteration 217/1000 | Loss: 0.00001229
Iteration 218/1000 | Loss: 0.00001229
Iteration 219/1000 | Loss: 0.00001229
Iteration 220/1000 | Loss: 0.00001229
Iteration 221/1000 | Loss: 0.00001229
Iteration 222/1000 | Loss: 0.00001229
Iteration 223/1000 | Loss: 0.00001229
Iteration 224/1000 | Loss: 0.00001229
Iteration 225/1000 | Loss: 0.00001229
Iteration 226/1000 | Loss: 0.00001229
Iteration 227/1000 | Loss: 0.00001229
Iteration 228/1000 | Loss: 0.00001229
Iteration 229/1000 | Loss: 0.00001229
Iteration 230/1000 | Loss: 0.00001229
Iteration 231/1000 | Loss: 0.00001229
Iteration 232/1000 | Loss: 0.00001229
Iteration 233/1000 | Loss: 0.00001229
Iteration 234/1000 | Loss: 0.00001229
Iteration 235/1000 | Loss: 0.00001229
Iteration 236/1000 | Loss: 0.00001229
Iteration 237/1000 | Loss: 0.00001229
Iteration 238/1000 | Loss: 0.00001229
Iteration 239/1000 | Loss: 0.00001229
Iteration 240/1000 | Loss: 0.00001229
Iteration 241/1000 | Loss: 0.00001229
Iteration 242/1000 | Loss: 0.00001229
Iteration 243/1000 | Loss: 0.00001229
Iteration 244/1000 | Loss: 0.00001229
Iteration 245/1000 | Loss: 0.00001229
Iteration 246/1000 | Loss: 0.00001229
Iteration 247/1000 | Loss: 0.00001229
Iteration 248/1000 | Loss: 0.00001229
Iteration 249/1000 | Loss: 0.00001229
Iteration 250/1000 | Loss: 0.00001229
Iteration 251/1000 | Loss: 0.00001229
Iteration 252/1000 | Loss: 0.00001229
Iteration 253/1000 | Loss: 0.00001229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.2286081982892938e-05, 1.2286081982892938e-05, 1.2286081982892938e-05, 1.2286081982892938e-05, 1.2286081982892938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2286081982892938e-05

Optimization complete. Final v2v error: 2.9853811264038086 mm

Highest mean error: 3.2639241218566895 mm for frame 96

Lowest mean error: 2.6834585666656494 mm for frame 159

Saving results

Total time: 32.12772822380066
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00757223
Iteration 2/25 | Loss: 0.00123237
Iteration 3/25 | Loss: 0.00088297
Iteration 4/25 | Loss: 0.00083422
Iteration 5/25 | Loss: 0.00082361
Iteration 6/25 | Loss: 0.00082260
Iteration 7/25 | Loss: 0.00082260
Iteration 8/25 | Loss: 0.00082260
Iteration 9/25 | Loss: 0.00082260
Iteration 10/25 | Loss: 0.00082260
Iteration 11/25 | Loss: 0.00082260
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008226034115068614, 0.0008226034115068614, 0.0008226034115068614, 0.0008226034115068614, 0.0008226034115068614]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008226034115068614

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49075019
Iteration 2/25 | Loss: 0.00046869
Iteration 3/25 | Loss: 0.00046869
Iteration 4/25 | Loss: 0.00046869
Iteration 5/25 | Loss: 0.00046869
Iteration 6/25 | Loss: 0.00046869
Iteration 7/25 | Loss: 0.00046869
Iteration 8/25 | Loss: 0.00046869
Iteration 9/25 | Loss: 0.00046869
Iteration 10/25 | Loss: 0.00046869
Iteration 11/25 | Loss: 0.00046869
Iteration 12/25 | Loss: 0.00046869
Iteration 13/25 | Loss: 0.00046869
Iteration 14/25 | Loss: 0.00046869
Iteration 15/25 | Loss: 0.00046869
Iteration 16/25 | Loss: 0.00046869
Iteration 17/25 | Loss: 0.00046869
Iteration 18/25 | Loss: 0.00046869
Iteration 19/25 | Loss: 0.00046869
Iteration 20/25 | Loss: 0.00046869
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0004686867177952081, 0.0004686867177952081, 0.0004686867177952081, 0.0004686867177952081, 0.0004686867177952081]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004686867177952081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046869
Iteration 2/1000 | Loss: 0.00003533
Iteration 3/1000 | Loss: 0.00002789
Iteration 4/1000 | Loss: 0.00002566
Iteration 5/1000 | Loss: 0.00002339
Iteration 6/1000 | Loss: 0.00002238
Iteration 7/1000 | Loss: 0.00002147
Iteration 8/1000 | Loss: 0.00002077
Iteration 9/1000 | Loss: 0.00002039
Iteration 10/1000 | Loss: 0.00002010
Iteration 11/1000 | Loss: 0.00001998
Iteration 12/1000 | Loss: 0.00001988
Iteration 13/1000 | Loss: 0.00001986
Iteration 14/1000 | Loss: 0.00001981
Iteration 15/1000 | Loss: 0.00001980
Iteration 16/1000 | Loss: 0.00001979
Iteration 17/1000 | Loss: 0.00001965
Iteration 18/1000 | Loss: 0.00001956
Iteration 19/1000 | Loss: 0.00001956
Iteration 20/1000 | Loss: 0.00001956
Iteration 21/1000 | Loss: 0.00001956
Iteration 22/1000 | Loss: 0.00001956
Iteration 23/1000 | Loss: 0.00001956
Iteration 24/1000 | Loss: 0.00001956
Iteration 25/1000 | Loss: 0.00001956
Iteration 26/1000 | Loss: 0.00001956
Iteration 27/1000 | Loss: 0.00001956
Iteration 28/1000 | Loss: 0.00001956
Iteration 29/1000 | Loss: 0.00001956
Iteration 30/1000 | Loss: 0.00001956
Iteration 31/1000 | Loss: 0.00001956
Iteration 32/1000 | Loss: 0.00001955
Iteration 33/1000 | Loss: 0.00001955
Iteration 34/1000 | Loss: 0.00001955
Iteration 35/1000 | Loss: 0.00001954
Iteration 36/1000 | Loss: 0.00001954
Iteration 37/1000 | Loss: 0.00001954
Iteration 38/1000 | Loss: 0.00001954
Iteration 39/1000 | Loss: 0.00001954
Iteration 40/1000 | Loss: 0.00001954
Iteration 41/1000 | Loss: 0.00001954
Iteration 42/1000 | Loss: 0.00001954
Iteration 43/1000 | Loss: 0.00001953
Iteration 44/1000 | Loss: 0.00001953
Iteration 45/1000 | Loss: 0.00001953
Iteration 46/1000 | Loss: 0.00001952
Iteration 47/1000 | Loss: 0.00001952
Iteration 48/1000 | Loss: 0.00001951
Iteration 49/1000 | Loss: 0.00001951
Iteration 50/1000 | Loss: 0.00001951
Iteration 51/1000 | Loss: 0.00001951
Iteration 52/1000 | Loss: 0.00001951
Iteration 53/1000 | Loss: 0.00001951
Iteration 54/1000 | Loss: 0.00001950
Iteration 55/1000 | Loss: 0.00001950
Iteration 56/1000 | Loss: 0.00001950
Iteration 57/1000 | Loss: 0.00001950
Iteration 58/1000 | Loss: 0.00001950
Iteration 59/1000 | Loss: 0.00001950
Iteration 60/1000 | Loss: 0.00001950
Iteration 61/1000 | Loss: 0.00001950
Iteration 62/1000 | Loss: 0.00001949
Iteration 63/1000 | Loss: 0.00001949
Iteration 64/1000 | Loss: 0.00001949
Iteration 65/1000 | Loss: 0.00001949
Iteration 66/1000 | Loss: 0.00001949
Iteration 67/1000 | Loss: 0.00001949
Iteration 68/1000 | Loss: 0.00001949
Iteration 69/1000 | Loss: 0.00001949
Iteration 70/1000 | Loss: 0.00001949
Iteration 71/1000 | Loss: 0.00001948
Iteration 72/1000 | Loss: 0.00001948
Iteration 73/1000 | Loss: 0.00001948
Iteration 74/1000 | Loss: 0.00001948
Iteration 75/1000 | Loss: 0.00001947
Iteration 76/1000 | Loss: 0.00001947
Iteration 77/1000 | Loss: 0.00001947
Iteration 78/1000 | Loss: 0.00001947
Iteration 79/1000 | Loss: 0.00001946
Iteration 80/1000 | Loss: 0.00001946
Iteration 81/1000 | Loss: 0.00001946
Iteration 82/1000 | Loss: 0.00001945
Iteration 83/1000 | Loss: 0.00001945
Iteration 84/1000 | Loss: 0.00001944
Iteration 85/1000 | Loss: 0.00001944
Iteration 86/1000 | Loss: 0.00001943
Iteration 87/1000 | Loss: 0.00001943
Iteration 88/1000 | Loss: 0.00001943
Iteration 89/1000 | Loss: 0.00001943
Iteration 90/1000 | Loss: 0.00001943
Iteration 91/1000 | Loss: 0.00001943
Iteration 92/1000 | Loss: 0.00001943
Iteration 93/1000 | Loss: 0.00001943
Iteration 94/1000 | Loss: 0.00001943
Iteration 95/1000 | Loss: 0.00001943
Iteration 96/1000 | Loss: 0.00001943
Iteration 97/1000 | Loss: 0.00001942
Iteration 98/1000 | Loss: 0.00001942
Iteration 99/1000 | Loss: 0.00001942
Iteration 100/1000 | Loss: 0.00001942
Iteration 101/1000 | Loss: 0.00001942
Iteration 102/1000 | Loss: 0.00001942
Iteration 103/1000 | Loss: 0.00001942
Iteration 104/1000 | Loss: 0.00001942
Iteration 105/1000 | Loss: 0.00001942
Iteration 106/1000 | Loss: 0.00001942
Iteration 107/1000 | Loss: 0.00001942
Iteration 108/1000 | Loss: 0.00001941
Iteration 109/1000 | Loss: 0.00001941
Iteration 110/1000 | Loss: 0.00001941
Iteration 111/1000 | Loss: 0.00001941
Iteration 112/1000 | Loss: 0.00001941
Iteration 113/1000 | Loss: 0.00001940
Iteration 114/1000 | Loss: 0.00001940
Iteration 115/1000 | Loss: 0.00001940
Iteration 116/1000 | Loss: 0.00001939
Iteration 117/1000 | Loss: 0.00001939
Iteration 118/1000 | Loss: 0.00001939
Iteration 119/1000 | Loss: 0.00001939
Iteration 120/1000 | Loss: 0.00001939
Iteration 121/1000 | Loss: 0.00001939
Iteration 122/1000 | Loss: 0.00001939
Iteration 123/1000 | Loss: 0.00001939
Iteration 124/1000 | Loss: 0.00001939
Iteration 125/1000 | Loss: 0.00001939
Iteration 126/1000 | Loss: 0.00001939
Iteration 127/1000 | Loss: 0.00001938
Iteration 128/1000 | Loss: 0.00001938
Iteration 129/1000 | Loss: 0.00001938
Iteration 130/1000 | Loss: 0.00001938
Iteration 131/1000 | Loss: 0.00001938
Iteration 132/1000 | Loss: 0.00001938
Iteration 133/1000 | Loss: 0.00001938
Iteration 134/1000 | Loss: 0.00001938
Iteration 135/1000 | Loss: 0.00001938
Iteration 136/1000 | Loss: 0.00001938
Iteration 137/1000 | Loss: 0.00001938
Iteration 138/1000 | Loss: 0.00001938
Iteration 139/1000 | Loss: 0.00001938
Iteration 140/1000 | Loss: 0.00001938
Iteration 141/1000 | Loss: 0.00001938
Iteration 142/1000 | Loss: 0.00001938
Iteration 143/1000 | Loss: 0.00001938
Iteration 144/1000 | Loss: 0.00001938
Iteration 145/1000 | Loss: 0.00001937
Iteration 146/1000 | Loss: 0.00001937
Iteration 147/1000 | Loss: 0.00001937
Iteration 148/1000 | Loss: 0.00001937
Iteration 149/1000 | Loss: 0.00001937
Iteration 150/1000 | Loss: 0.00001937
Iteration 151/1000 | Loss: 0.00001937
Iteration 152/1000 | Loss: 0.00001937
Iteration 153/1000 | Loss: 0.00001937
Iteration 154/1000 | Loss: 0.00001937
Iteration 155/1000 | Loss: 0.00001937
Iteration 156/1000 | Loss: 0.00001937
Iteration 157/1000 | Loss: 0.00001937
Iteration 158/1000 | Loss: 0.00001936
Iteration 159/1000 | Loss: 0.00001936
Iteration 160/1000 | Loss: 0.00001936
Iteration 161/1000 | Loss: 0.00001936
Iteration 162/1000 | Loss: 0.00001936
Iteration 163/1000 | Loss: 0.00001936
Iteration 164/1000 | Loss: 0.00001936
Iteration 165/1000 | Loss: 0.00001936
Iteration 166/1000 | Loss: 0.00001936
Iteration 167/1000 | Loss: 0.00001935
Iteration 168/1000 | Loss: 0.00001935
Iteration 169/1000 | Loss: 0.00001935
Iteration 170/1000 | Loss: 0.00001935
Iteration 171/1000 | Loss: 0.00001935
Iteration 172/1000 | Loss: 0.00001935
Iteration 173/1000 | Loss: 0.00001935
Iteration 174/1000 | Loss: 0.00001935
Iteration 175/1000 | Loss: 0.00001935
Iteration 176/1000 | Loss: 0.00001935
Iteration 177/1000 | Loss: 0.00001935
Iteration 178/1000 | Loss: 0.00001935
Iteration 179/1000 | Loss: 0.00001935
Iteration 180/1000 | Loss: 0.00001935
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.935185537149664e-05, 1.935185537149664e-05, 1.935185537149664e-05, 1.935185537149664e-05, 1.935185537149664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.935185537149664e-05

Optimization complete. Final v2v error: 3.713712453842163 mm

Highest mean error: 3.947801351547241 mm for frame 119

Lowest mean error: 3.431412935256958 mm for frame 97

Saving results

Total time: 43.54884386062622
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01090648
Iteration 2/25 | Loss: 0.00242376
Iteration 3/25 | Loss: 0.00154014
Iteration 4/25 | Loss: 0.00145780
Iteration 5/25 | Loss: 0.00124763
Iteration 6/25 | Loss: 0.00112477
Iteration 7/25 | Loss: 0.00108891
Iteration 8/25 | Loss: 0.00101824
Iteration 9/25 | Loss: 0.00094967
Iteration 10/25 | Loss: 0.00092491
Iteration 11/25 | Loss: 0.00090336
Iteration 12/25 | Loss: 0.00088744
Iteration 13/25 | Loss: 0.00088172
Iteration 14/25 | Loss: 0.00087774
Iteration 15/25 | Loss: 0.00087941
Iteration 16/25 | Loss: 0.00087674
Iteration 17/25 | Loss: 0.00087206
Iteration 18/25 | Loss: 0.00087033
Iteration 19/25 | Loss: 0.00086906
Iteration 20/25 | Loss: 0.00086881
Iteration 21/25 | Loss: 0.00086863
Iteration 22/25 | Loss: 0.00086836
Iteration 23/25 | Loss: 0.00086846
Iteration 24/25 | Loss: 0.00086703
Iteration 25/25 | Loss: 0.00086514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53009617
Iteration 2/25 | Loss: 0.00090210
Iteration 3/25 | Loss: 0.00073356
Iteration 4/25 | Loss: 0.00073356
Iteration 5/25 | Loss: 0.00073356
Iteration 6/25 | Loss: 0.00073356
Iteration 7/25 | Loss: 0.00073356
Iteration 8/25 | Loss: 0.00073356
Iteration 9/25 | Loss: 0.00073356
Iteration 10/25 | Loss: 0.00073356
Iteration 11/25 | Loss: 0.00073356
Iteration 12/25 | Loss: 0.00073356
Iteration 13/25 | Loss: 0.00073356
Iteration 14/25 | Loss: 0.00073356
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007335607660934329, 0.0007335607660934329, 0.0007335607660934329, 0.0007335607660934329, 0.0007335607660934329]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007335607660934329

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073356
Iteration 2/1000 | Loss: 0.00049493
Iteration 3/1000 | Loss: 0.00008583
Iteration 4/1000 | Loss: 0.00006702
Iteration 5/1000 | Loss: 0.00028878
Iteration 6/1000 | Loss: 0.00048361
Iteration 7/1000 | Loss: 0.00038341
Iteration 8/1000 | Loss: 0.00055090
Iteration 9/1000 | Loss: 0.00052696
Iteration 10/1000 | Loss: 0.00068382
Iteration 11/1000 | Loss: 0.00085067
Iteration 12/1000 | Loss: 0.00006802
Iteration 13/1000 | Loss: 0.00013910
Iteration 14/1000 | Loss: 0.00011553
Iteration 15/1000 | Loss: 0.00012622
Iteration 16/1000 | Loss: 0.00038996
Iteration 17/1000 | Loss: 0.00083223
Iteration 18/1000 | Loss: 0.00088200
Iteration 19/1000 | Loss: 0.00115687
Iteration 20/1000 | Loss: 0.00097753
Iteration 21/1000 | Loss: 0.00080534
Iteration 22/1000 | Loss: 0.00046248
Iteration 23/1000 | Loss: 0.00052272
Iteration 24/1000 | Loss: 0.00018991
Iteration 25/1000 | Loss: 0.00018194
Iteration 26/1000 | Loss: 0.00010907
Iteration 27/1000 | Loss: 0.00023921
Iteration 28/1000 | Loss: 0.00020095
Iteration 29/1000 | Loss: 0.00029242
Iteration 30/1000 | Loss: 0.00019282
Iteration 31/1000 | Loss: 0.00015984
Iteration 32/1000 | Loss: 0.00019002
Iteration 33/1000 | Loss: 0.00021948
Iteration 34/1000 | Loss: 0.00003818
Iteration 35/1000 | Loss: 0.00003536
Iteration 36/1000 | Loss: 0.00003008
Iteration 37/1000 | Loss: 0.00002838
Iteration 38/1000 | Loss: 0.00002733
Iteration 39/1000 | Loss: 0.00002628
Iteration 40/1000 | Loss: 0.00002552
Iteration 41/1000 | Loss: 0.00003147
Iteration 42/1000 | Loss: 0.00002456
Iteration 43/1000 | Loss: 0.00002363
Iteration 44/1000 | Loss: 0.00002290
Iteration 45/1000 | Loss: 0.00013420
Iteration 46/1000 | Loss: 0.00002261
Iteration 47/1000 | Loss: 0.00002209
Iteration 48/1000 | Loss: 0.00002207
Iteration 49/1000 | Loss: 0.00002206
Iteration 50/1000 | Loss: 0.00002182
Iteration 51/1000 | Loss: 0.00002173
Iteration 52/1000 | Loss: 0.00002172
Iteration 53/1000 | Loss: 0.00002157
Iteration 54/1000 | Loss: 0.00002153
Iteration 55/1000 | Loss: 0.00002152
Iteration 56/1000 | Loss: 0.00002151
Iteration 57/1000 | Loss: 0.00002151
Iteration 58/1000 | Loss: 0.00002151
Iteration 59/1000 | Loss: 0.00002150
Iteration 60/1000 | Loss: 0.00002150
Iteration 61/1000 | Loss: 0.00002150
Iteration 62/1000 | Loss: 0.00002146
Iteration 63/1000 | Loss: 0.00002139
Iteration 64/1000 | Loss: 0.00002139
Iteration 65/1000 | Loss: 0.00002137
Iteration 66/1000 | Loss: 0.00002136
Iteration 67/1000 | Loss: 0.00002136
Iteration 68/1000 | Loss: 0.00002133
Iteration 69/1000 | Loss: 0.00002132
Iteration 70/1000 | Loss: 0.00002131
Iteration 71/1000 | Loss: 0.00002130
Iteration 72/1000 | Loss: 0.00002129
Iteration 73/1000 | Loss: 0.00002128
Iteration 74/1000 | Loss: 0.00002128
Iteration 75/1000 | Loss: 0.00002127
Iteration 76/1000 | Loss: 0.00002127
Iteration 77/1000 | Loss: 0.00002127
Iteration 78/1000 | Loss: 0.00002127
Iteration 79/1000 | Loss: 0.00002127
Iteration 80/1000 | Loss: 0.00002127
Iteration 81/1000 | Loss: 0.00002126
Iteration 82/1000 | Loss: 0.00002126
Iteration 83/1000 | Loss: 0.00002126
Iteration 84/1000 | Loss: 0.00002126
Iteration 85/1000 | Loss: 0.00002126
Iteration 86/1000 | Loss: 0.00002126
Iteration 87/1000 | Loss: 0.00002126
Iteration 88/1000 | Loss: 0.00002125
Iteration 89/1000 | Loss: 0.00002125
Iteration 90/1000 | Loss: 0.00002124
Iteration 91/1000 | Loss: 0.00002123
Iteration 92/1000 | Loss: 0.00002123
Iteration 93/1000 | Loss: 0.00002123
Iteration 94/1000 | Loss: 0.00002123
Iteration 95/1000 | Loss: 0.00002122
Iteration 96/1000 | Loss: 0.00002122
Iteration 97/1000 | Loss: 0.00002122
Iteration 98/1000 | Loss: 0.00002122
Iteration 99/1000 | Loss: 0.00002122
Iteration 100/1000 | Loss: 0.00002122
Iteration 101/1000 | Loss: 0.00002122
Iteration 102/1000 | Loss: 0.00002122
Iteration 103/1000 | Loss: 0.00002122
Iteration 104/1000 | Loss: 0.00002121
Iteration 105/1000 | Loss: 0.00002121
Iteration 106/1000 | Loss: 0.00002119
Iteration 107/1000 | Loss: 0.00002119
Iteration 108/1000 | Loss: 0.00002118
Iteration 109/1000 | Loss: 0.00002118
Iteration 110/1000 | Loss: 0.00002118
Iteration 111/1000 | Loss: 0.00002117
Iteration 112/1000 | Loss: 0.00002117
Iteration 113/1000 | Loss: 0.00002116
Iteration 114/1000 | Loss: 0.00002116
Iteration 115/1000 | Loss: 0.00002116
Iteration 116/1000 | Loss: 0.00002115
Iteration 117/1000 | Loss: 0.00002115
Iteration 118/1000 | Loss: 0.00002115
Iteration 119/1000 | Loss: 0.00002115
Iteration 120/1000 | Loss: 0.00002115
Iteration 121/1000 | Loss: 0.00002115
Iteration 122/1000 | Loss: 0.00002114
Iteration 123/1000 | Loss: 0.00002114
Iteration 124/1000 | Loss: 0.00002114
Iteration 125/1000 | Loss: 0.00002113
Iteration 126/1000 | Loss: 0.00002113
Iteration 127/1000 | Loss: 0.00002113
Iteration 128/1000 | Loss: 0.00002112
Iteration 129/1000 | Loss: 0.00002112
Iteration 130/1000 | Loss: 0.00011619
Iteration 131/1000 | Loss: 0.00003174
Iteration 132/1000 | Loss: 0.00002544
Iteration 133/1000 | Loss: 0.00002185
Iteration 134/1000 | Loss: 0.00002119
Iteration 135/1000 | Loss: 0.00002112
Iteration 136/1000 | Loss: 0.00002111
Iteration 137/1000 | Loss: 0.00002111
Iteration 138/1000 | Loss: 0.00002111
Iteration 139/1000 | Loss: 0.00002111
Iteration 140/1000 | Loss: 0.00002111
Iteration 141/1000 | Loss: 0.00002111
Iteration 142/1000 | Loss: 0.00002111
Iteration 143/1000 | Loss: 0.00002111
Iteration 144/1000 | Loss: 0.00002110
Iteration 145/1000 | Loss: 0.00002110
Iteration 146/1000 | Loss: 0.00002110
Iteration 147/1000 | Loss: 0.00002110
Iteration 148/1000 | Loss: 0.00002110
Iteration 149/1000 | Loss: 0.00002109
Iteration 150/1000 | Loss: 0.00002109
Iteration 151/1000 | Loss: 0.00002109
Iteration 152/1000 | Loss: 0.00002109
Iteration 153/1000 | Loss: 0.00002108
Iteration 154/1000 | Loss: 0.00002108
Iteration 155/1000 | Loss: 0.00002108
Iteration 156/1000 | Loss: 0.00002107
Iteration 157/1000 | Loss: 0.00002106
Iteration 158/1000 | Loss: 0.00002105
Iteration 159/1000 | Loss: 0.00002105
Iteration 160/1000 | Loss: 0.00002105
Iteration 161/1000 | Loss: 0.00002105
Iteration 162/1000 | Loss: 0.00002105
Iteration 163/1000 | Loss: 0.00002105
Iteration 164/1000 | Loss: 0.00002105
Iteration 165/1000 | Loss: 0.00002105
Iteration 166/1000 | Loss: 0.00002104
Iteration 167/1000 | Loss: 0.00002104
Iteration 168/1000 | Loss: 0.00002104
Iteration 169/1000 | Loss: 0.00002103
Iteration 170/1000 | Loss: 0.00002103
Iteration 171/1000 | Loss: 0.00002103
Iteration 172/1000 | Loss: 0.00002103
Iteration 173/1000 | Loss: 0.00002103
Iteration 174/1000 | Loss: 0.00002103
Iteration 175/1000 | Loss: 0.00002103
Iteration 176/1000 | Loss: 0.00002103
Iteration 177/1000 | Loss: 0.00002103
Iteration 178/1000 | Loss: 0.00002103
Iteration 179/1000 | Loss: 0.00002103
Iteration 180/1000 | Loss: 0.00002103
Iteration 181/1000 | Loss: 0.00002102
Iteration 182/1000 | Loss: 0.00002102
Iteration 183/1000 | Loss: 0.00002102
Iteration 184/1000 | Loss: 0.00002102
Iteration 185/1000 | Loss: 0.00002102
Iteration 186/1000 | Loss: 0.00002102
Iteration 187/1000 | Loss: 0.00002102
Iteration 188/1000 | Loss: 0.00002102
Iteration 189/1000 | Loss: 0.00002102
Iteration 190/1000 | Loss: 0.00002102
Iteration 191/1000 | Loss: 0.00002102
Iteration 192/1000 | Loss: 0.00002102
Iteration 193/1000 | Loss: 0.00002102
Iteration 194/1000 | Loss: 0.00002102
Iteration 195/1000 | Loss: 0.00002102
Iteration 196/1000 | Loss: 0.00002102
Iteration 197/1000 | Loss: 0.00002102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.1023692170274444e-05, 2.1023692170274444e-05, 2.1023692170274444e-05, 2.1023692170274444e-05, 2.1023692170274444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1023692170274444e-05

Optimization complete. Final v2v error: 3.803501844406128 mm

Highest mean error: 5.649900913238525 mm for frame 239

Lowest mean error: 3.26945424079895 mm for frame 170

Saving results

Total time: 156.41502261161804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00720313
Iteration 2/25 | Loss: 0.00099066
Iteration 3/25 | Loss: 0.00081362
Iteration 4/25 | Loss: 0.00076741
Iteration 5/25 | Loss: 0.00076039
Iteration 6/25 | Loss: 0.00074957
Iteration 7/25 | Loss: 0.00075198
Iteration 8/25 | Loss: 0.00074678
Iteration 9/25 | Loss: 0.00074664
Iteration 10/25 | Loss: 0.00074664
Iteration 11/25 | Loss: 0.00074664
Iteration 12/25 | Loss: 0.00074664
Iteration 13/25 | Loss: 0.00074664
Iteration 14/25 | Loss: 0.00074663
Iteration 15/25 | Loss: 0.00074660
Iteration 16/25 | Loss: 0.00074660
Iteration 17/25 | Loss: 0.00074660
Iteration 18/25 | Loss: 0.00074660
Iteration 19/25 | Loss: 0.00074660
Iteration 20/25 | Loss: 0.00074660
Iteration 21/25 | Loss: 0.00074660
Iteration 22/25 | Loss: 0.00074660
Iteration 23/25 | Loss: 0.00074660
Iteration 24/25 | Loss: 0.00074660
Iteration 25/25 | Loss: 0.00074660

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01218963
Iteration 2/25 | Loss: 0.00054230
Iteration 3/25 | Loss: 0.00054230
Iteration 4/25 | Loss: 0.00054230
Iteration 5/25 | Loss: 0.00054230
Iteration 6/25 | Loss: 0.00054230
Iteration 7/25 | Loss: 0.00054230
Iteration 8/25 | Loss: 0.00054230
Iteration 9/25 | Loss: 0.00054230
Iteration 10/25 | Loss: 0.00054229
Iteration 11/25 | Loss: 0.00048001
Iteration 12/25 | Loss: 0.00048001
Iteration 13/25 | Loss: 0.00048001
Iteration 14/25 | Loss: 0.00048001
Iteration 15/25 | Loss: 0.00048001
Iteration 16/25 | Loss: 0.00048001
Iteration 17/25 | Loss: 0.00048001
Iteration 18/25 | Loss: 0.00048001
Iteration 19/25 | Loss: 0.00048001
Iteration 20/25 | Loss: 0.00048001
Iteration 21/25 | Loss: 0.00048001
Iteration 22/25 | Loss: 0.00048001
Iteration 23/25 | Loss: 0.00048001
Iteration 24/25 | Loss: 0.00048001
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00048001125105656683, 0.00048001125105656683, 0.00048001125105656683, 0.00048001125105656683, 0.00048001125105656683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00048001125105656683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048001
Iteration 2/1000 | Loss: 0.00002389
Iteration 3/1000 | Loss: 0.00008617
Iteration 4/1000 | Loss: 0.00001775
Iteration 5/1000 | Loss: 0.00010214
Iteration 6/1000 | Loss: 0.00009261
Iteration 7/1000 | Loss: 0.00009711
Iteration 8/1000 | Loss: 0.00008978
Iteration 9/1000 | Loss: 0.00002250
Iteration 10/1000 | Loss: 0.00002005
Iteration 11/1000 | Loss: 0.00001649
Iteration 12/1000 | Loss: 0.00001619
Iteration 13/1000 | Loss: 0.00001585
Iteration 14/1000 | Loss: 0.00001557
Iteration 15/1000 | Loss: 0.00001552
Iteration 16/1000 | Loss: 0.00001532
Iteration 17/1000 | Loss: 0.00001516
Iteration 18/1000 | Loss: 0.00001511
Iteration 19/1000 | Loss: 0.00001504
Iteration 20/1000 | Loss: 0.00001498
Iteration 21/1000 | Loss: 0.00001495
Iteration 22/1000 | Loss: 0.00001492
Iteration 23/1000 | Loss: 0.00001488
Iteration 24/1000 | Loss: 0.00001484
Iteration 25/1000 | Loss: 0.00001484
Iteration 26/1000 | Loss: 0.00001476
Iteration 27/1000 | Loss: 0.00001475
Iteration 28/1000 | Loss: 0.00001474
Iteration 29/1000 | Loss: 0.00001474
Iteration 30/1000 | Loss: 0.00001472
Iteration 31/1000 | Loss: 0.00001472
Iteration 32/1000 | Loss: 0.00001469
Iteration 33/1000 | Loss: 0.00001460
Iteration 34/1000 | Loss: 0.00001459
Iteration 35/1000 | Loss: 0.00001458
Iteration 36/1000 | Loss: 0.00001458
Iteration 37/1000 | Loss: 0.00001457
Iteration 38/1000 | Loss: 0.00001456
Iteration 39/1000 | Loss: 0.00001456
Iteration 40/1000 | Loss: 0.00001456
Iteration 41/1000 | Loss: 0.00001456
Iteration 42/1000 | Loss: 0.00001456
Iteration 43/1000 | Loss: 0.00001456
Iteration 44/1000 | Loss: 0.00001455
Iteration 45/1000 | Loss: 0.00001455
Iteration 46/1000 | Loss: 0.00001454
Iteration 47/1000 | Loss: 0.00001452
Iteration 48/1000 | Loss: 0.00001452
Iteration 49/1000 | Loss: 0.00001451
Iteration 50/1000 | Loss: 0.00001451
Iteration 51/1000 | Loss: 0.00001451
Iteration 52/1000 | Loss: 0.00001451
Iteration 53/1000 | Loss: 0.00001451
Iteration 54/1000 | Loss: 0.00001450
Iteration 55/1000 | Loss: 0.00001450
Iteration 56/1000 | Loss: 0.00001450
Iteration 57/1000 | Loss: 0.00001449
Iteration 58/1000 | Loss: 0.00001448
Iteration 59/1000 | Loss: 0.00001448
Iteration 60/1000 | Loss: 0.00001448
Iteration 61/1000 | Loss: 0.00001448
Iteration 62/1000 | Loss: 0.00001447
Iteration 63/1000 | Loss: 0.00001447
Iteration 64/1000 | Loss: 0.00001447
Iteration 65/1000 | Loss: 0.00001447
Iteration 66/1000 | Loss: 0.00001446
Iteration 67/1000 | Loss: 0.00001446
Iteration 68/1000 | Loss: 0.00001446
Iteration 69/1000 | Loss: 0.00001446
Iteration 70/1000 | Loss: 0.00001445
Iteration 71/1000 | Loss: 0.00001445
Iteration 72/1000 | Loss: 0.00001443
Iteration 73/1000 | Loss: 0.00001443
Iteration 74/1000 | Loss: 0.00001443
Iteration 75/1000 | Loss: 0.00001443
Iteration 76/1000 | Loss: 0.00001443
Iteration 77/1000 | Loss: 0.00001443
Iteration 78/1000 | Loss: 0.00001443
Iteration 79/1000 | Loss: 0.00001442
Iteration 80/1000 | Loss: 0.00001442
Iteration 81/1000 | Loss: 0.00001442
Iteration 82/1000 | Loss: 0.00001442
Iteration 83/1000 | Loss: 0.00001442
Iteration 84/1000 | Loss: 0.00001442
Iteration 85/1000 | Loss: 0.00001442
Iteration 86/1000 | Loss: 0.00001441
Iteration 87/1000 | Loss: 0.00001440
Iteration 88/1000 | Loss: 0.00001439
Iteration 89/1000 | Loss: 0.00001439
Iteration 90/1000 | Loss: 0.00001438
Iteration 91/1000 | Loss: 0.00001438
Iteration 92/1000 | Loss: 0.00001438
Iteration 93/1000 | Loss: 0.00001437
Iteration 94/1000 | Loss: 0.00001437
Iteration 95/1000 | Loss: 0.00001436
Iteration 96/1000 | Loss: 0.00001436
Iteration 97/1000 | Loss: 0.00001436
Iteration 98/1000 | Loss: 0.00001435
Iteration 99/1000 | Loss: 0.00001435
Iteration 100/1000 | Loss: 0.00001434
Iteration 101/1000 | Loss: 0.00001434
Iteration 102/1000 | Loss: 0.00001433
Iteration 103/1000 | Loss: 0.00001433
Iteration 104/1000 | Loss: 0.00001432
Iteration 105/1000 | Loss: 0.00001432
Iteration 106/1000 | Loss: 0.00001432
Iteration 107/1000 | Loss: 0.00001431
Iteration 108/1000 | Loss: 0.00001431
Iteration 109/1000 | Loss: 0.00001431
Iteration 110/1000 | Loss: 0.00001431
Iteration 111/1000 | Loss: 0.00001430
Iteration 112/1000 | Loss: 0.00001430
Iteration 113/1000 | Loss: 0.00001429
Iteration 114/1000 | Loss: 0.00001428
Iteration 115/1000 | Loss: 0.00001428
Iteration 116/1000 | Loss: 0.00001428
Iteration 117/1000 | Loss: 0.00001427
Iteration 118/1000 | Loss: 0.00001427
Iteration 119/1000 | Loss: 0.00001959
Iteration 120/1000 | Loss: 0.00001468
Iteration 121/1000 | Loss: 0.00001427
Iteration 122/1000 | Loss: 0.00001413
Iteration 123/1000 | Loss: 0.00001411
Iteration 124/1000 | Loss: 0.00001411
Iteration 125/1000 | Loss: 0.00001411
Iteration 126/1000 | Loss: 0.00001411
Iteration 127/1000 | Loss: 0.00001411
Iteration 128/1000 | Loss: 0.00001411
Iteration 129/1000 | Loss: 0.00001411
Iteration 130/1000 | Loss: 0.00001411
Iteration 131/1000 | Loss: 0.00001411
Iteration 132/1000 | Loss: 0.00001411
Iteration 133/1000 | Loss: 0.00001411
Iteration 134/1000 | Loss: 0.00001410
Iteration 135/1000 | Loss: 0.00001410
Iteration 136/1000 | Loss: 0.00001410
Iteration 137/1000 | Loss: 0.00001410
Iteration 138/1000 | Loss: 0.00001410
Iteration 139/1000 | Loss: 0.00001409
Iteration 140/1000 | Loss: 0.00001409
Iteration 141/1000 | Loss: 0.00001409
Iteration 142/1000 | Loss: 0.00001409
Iteration 143/1000 | Loss: 0.00001409
Iteration 144/1000 | Loss: 0.00001409
Iteration 145/1000 | Loss: 0.00001409
Iteration 146/1000 | Loss: 0.00001408
Iteration 147/1000 | Loss: 0.00001408
Iteration 148/1000 | Loss: 0.00001408
Iteration 149/1000 | Loss: 0.00001407
Iteration 150/1000 | Loss: 0.00001407
Iteration 151/1000 | Loss: 0.00001407
Iteration 152/1000 | Loss: 0.00001407
Iteration 153/1000 | Loss: 0.00001407
Iteration 154/1000 | Loss: 0.00001407
Iteration 155/1000 | Loss: 0.00001406
Iteration 156/1000 | Loss: 0.00001406
Iteration 157/1000 | Loss: 0.00001406
Iteration 158/1000 | Loss: 0.00001406
Iteration 159/1000 | Loss: 0.00001406
Iteration 160/1000 | Loss: 0.00001406
Iteration 161/1000 | Loss: 0.00001405
Iteration 162/1000 | Loss: 0.00001405
Iteration 163/1000 | Loss: 0.00001405
Iteration 164/1000 | Loss: 0.00001405
Iteration 165/1000 | Loss: 0.00001405
Iteration 166/1000 | Loss: 0.00001405
Iteration 167/1000 | Loss: 0.00001405
Iteration 168/1000 | Loss: 0.00001405
Iteration 169/1000 | Loss: 0.00001405
Iteration 170/1000 | Loss: 0.00001404
Iteration 171/1000 | Loss: 0.00001404
Iteration 172/1000 | Loss: 0.00001404
Iteration 173/1000 | Loss: 0.00001403
Iteration 174/1000 | Loss: 0.00001403
Iteration 175/1000 | Loss: 0.00001403
Iteration 176/1000 | Loss: 0.00001403
Iteration 177/1000 | Loss: 0.00001403
Iteration 178/1000 | Loss: 0.00001402
Iteration 179/1000 | Loss: 0.00001402
Iteration 180/1000 | Loss: 0.00001402
Iteration 181/1000 | Loss: 0.00001402
Iteration 182/1000 | Loss: 0.00001402
Iteration 183/1000 | Loss: 0.00001402
Iteration 184/1000 | Loss: 0.00001402
Iteration 185/1000 | Loss: 0.00001402
Iteration 186/1000 | Loss: 0.00001402
Iteration 187/1000 | Loss: 0.00001402
Iteration 188/1000 | Loss: 0.00001401
Iteration 189/1000 | Loss: 0.00001401
Iteration 190/1000 | Loss: 0.00001401
Iteration 191/1000 | Loss: 0.00001401
Iteration 192/1000 | Loss: 0.00001401
Iteration 193/1000 | Loss: 0.00001401
Iteration 194/1000 | Loss: 0.00001401
Iteration 195/1000 | Loss: 0.00001401
Iteration 196/1000 | Loss: 0.00001401
Iteration 197/1000 | Loss: 0.00001401
Iteration 198/1000 | Loss: 0.00001401
Iteration 199/1000 | Loss: 0.00001401
Iteration 200/1000 | Loss: 0.00001401
Iteration 201/1000 | Loss: 0.00001400
Iteration 202/1000 | Loss: 0.00001400
Iteration 203/1000 | Loss: 0.00001400
Iteration 204/1000 | Loss: 0.00001400
Iteration 205/1000 | Loss: 0.00001400
Iteration 206/1000 | Loss: 0.00001400
Iteration 207/1000 | Loss: 0.00001400
Iteration 208/1000 | Loss: 0.00001400
Iteration 209/1000 | Loss: 0.00001400
Iteration 210/1000 | Loss: 0.00001399
Iteration 211/1000 | Loss: 0.00001399
Iteration 212/1000 | Loss: 0.00001399
Iteration 213/1000 | Loss: 0.00001399
Iteration 214/1000 | Loss: 0.00001399
Iteration 215/1000 | Loss: 0.00001399
Iteration 216/1000 | Loss: 0.00001398
Iteration 217/1000 | Loss: 0.00001398
Iteration 218/1000 | Loss: 0.00001398
Iteration 219/1000 | Loss: 0.00001398
Iteration 220/1000 | Loss: 0.00001398
Iteration 221/1000 | Loss: 0.00001398
Iteration 222/1000 | Loss: 0.00001398
Iteration 223/1000 | Loss: 0.00001398
Iteration 224/1000 | Loss: 0.00001398
Iteration 225/1000 | Loss: 0.00001398
Iteration 226/1000 | Loss: 0.00001398
Iteration 227/1000 | Loss: 0.00001398
Iteration 228/1000 | Loss: 0.00001397
Iteration 229/1000 | Loss: 0.00001397
Iteration 230/1000 | Loss: 0.00001397
Iteration 231/1000 | Loss: 0.00001397
Iteration 232/1000 | Loss: 0.00001397
Iteration 233/1000 | Loss: 0.00001397
Iteration 234/1000 | Loss: 0.00001397
Iteration 235/1000 | Loss: 0.00001397
Iteration 236/1000 | Loss: 0.00001397
Iteration 237/1000 | Loss: 0.00001397
Iteration 238/1000 | Loss: 0.00001397
Iteration 239/1000 | Loss: 0.00001397
Iteration 240/1000 | Loss: 0.00001397
Iteration 241/1000 | Loss: 0.00001397
Iteration 242/1000 | Loss: 0.00001397
Iteration 243/1000 | Loss: 0.00001397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.3967215636512265e-05, 1.3967215636512265e-05, 1.3967215636512265e-05, 1.3967215636512265e-05, 1.3967215636512265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3967215636512265e-05

Optimization complete. Final v2v error: 3.174630641937256 mm

Highest mean error: 4.693050861358643 mm for frame 141

Lowest mean error: 2.9929332733154297 mm for frame 262

Saving results

Total time: 79.22319841384888
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036057
Iteration 2/25 | Loss: 0.00233432
Iteration 3/25 | Loss: 0.00148015
Iteration 4/25 | Loss: 0.00119670
Iteration 5/25 | Loss: 0.00113968
Iteration 6/25 | Loss: 0.00117364
Iteration 7/25 | Loss: 0.00109845
Iteration 8/25 | Loss: 0.00107232
Iteration 9/25 | Loss: 0.00103278
Iteration 10/25 | Loss: 0.00102892
Iteration 11/25 | Loss: 0.00099380
Iteration 12/25 | Loss: 0.00095506
Iteration 13/25 | Loss: 0.00093173
Iteration 14/25 | Loss: 0.00090769
Iteration 15/25 | Loss: 0.00089016
Iteration 16/25 | Loss: 0.00087806
Iteration 17/25 | Loss: 0.00087957
Iteration 18/25 | Loss: 0.00088228
Iteration 19/25 | Loss: 0.00087030
Iteration 20/25 | Loss: 0.00090319
Iteration 21/25 | Loss: 0.00090801
Iteration 22/25 | Loss: 0.00086389
Iteration 23/25 | Loss: 0.00083988
Iteration 24/25 | Loss: 0.00084424
Iteration 25/25 | Loss: 0.00083578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53249621
Iteration 2/25 | Loss: 0.00125305
Iteration 3/25 | Loss: 0.00125305
Iteration 4/25 | Loss: 0.00125305
Iteration 5/25 | Loss: 0.00125305
Iteration 6/25 | Loss: 0.00125305
Iteration 7/25 | Loss: 0.00125305
Iteration 8/25 | Loss: 0.00125305
Iteration 9/25 | Loss: 0.00125305
Iteration 10/25 | Loss: 0.00125305
Iteration 11/25 | Loss: 0.00125305
Iteration 12/25 | Loss: 0.00125305
Iteration 13/25 | Loss: 0.00125305
Iteration 14/25 | Loss: 0.00125305
Iteration 15/25 | Loss: 0.00125305
Iteration 16/25 | Loss: 0.00125305
Iteration 17/25 | Loss: 0.00125305
Iteration 18/25 | Loss: 0.00125305
Iteration 19/25 | Loss: 0.00125305
Iteration 20/25 | Loss: 0.00125305
Iteration 21/25 | Loss: 0.00125305
Iteration 22/25 | Loss: 0.00125305
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012530495878309011, 0.0012530495878309011, 0.0012530495878309011, 0.0012530495878309011, 0.0012530495878309011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012530495878309011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125305
Iteration 2/1000 | Loss: 0.00067077
Iteration 3/1000 | Loss: 0.00095067
Iteration 4/1000 | Loss: 0.00042903
Iteration 5/1000 | Loss: 0.00069433
Iteration 6/1000 | Loss: 0.00076723
Iteration 7/1000 | Loss: 0.00111443
Iteration 8/1000 | Loss: 0.00089276
Iteration 9/1000 | Loss: 0.00098272
Iteration 10/1000 | Loss: 0.00120115
Iteration 11/1000 | Loss: 0.00096002
Iteration 12/1000 | Loss: 0.00096384
Iteration 13/1000 | Loss: 0.00054684
Iteration 14/1000 | Loss: 0.00052828
Iteration 15/1000 | Loss: 0.00091238
Iteration 16/1000 | Loss: 0.00076348
Iteration 17/1000 | Loss: 0.00070657
Iteration 18/1000 | Loss: 0.00064454
Iteration 19/1000 | Loss: 0.00088064
Iteration 20/1000 | Loss: 0.00088816
Iteration 21/1000 | Loss: 0.00055677
Iteration 22/1000 | Loss: 0.00051747
Iteration 23/1000 | Loss: 0.00092215
Iteration 24/1000 | Loss: 0.00087281
Iteration 25/1000 | Loss: 0.00084924
Iteration 26/1000 | Loss: 0.00265218
Iteration 27/1000 | Loss: 0.00181664
Iteration 28/1000 | Loss: 0.00140694
Iteration 29/1000 | Loss: 0.00172859
Iteration 30/1000 | Loss: 0.00147977
Iteration 31/1000 | Loss: 0.00148754
Iteration 32/1000 | Loss: 0.00087319
Iteration 33/1000 | Loss: 0.00090310
Iteration 34/1000 | Loss: 0.00090636
Iteration 35/1000 | Loss: 0.00084524
Iteration 36/1000 | Loss: 0.00094217
Iteration 37/1000 | Loss: 0.00065233
Iteration 38/1000 | Loss: 0.00152461
Iteration 39/1000 | Loss: 0.00064208
Iteration 40/1000 | Loss: 0.00070897
Iteration 41/1000 | Loss: 0.00065435
Iteration 42/1000 | Loss: 0.00063235
Iteration 43/1000 | Loss: 0.00099034
Iteration 44/1000 | Loss: 0.00052199
Iteration 45/1000 | Loss: 0.00067444
Iteration 46/1000 | Loss: 0.00050189
Iteration 47/1000 | Loss: 0.00292090
Iteration 48/1000 | Loss: 0.00048833
Iteration 49/1000 | Loss: 0.00066668
Iteration 50/1000 | Loss: 0.00136996
Iteration 51/1000 | Loss: 0.00131067
Iteration 52/1000 | Loss: 0.00045732
Iteration 53/1000 | Loss: 0.00046931
Iteration 54/1000 | Loss: 0.00032881
Iteration 55/1000 | Loss: 0.00053983
Iteration 56/1000 | Loss: 0.00052531
Iteration 57/1000 | Loss: 0.00050800
Iteration 58/1000 | Loss: 0.00186115
Iteration 59/1000 | Loss: 0.00071893
Iteration 60/1000 | Loss: 0.00027481
Iteration 61/1000 | Loss: 0.00022813
Iteration 62/1000 | Loss: 0.00039600
Iteration 63/1000 | Loss: 0.00038482
Iteration 64/1000 | Loss: 0.00039194
Iteration 65/1000 | Loss: 0.00040128
Iteration 66/1000 | Loss: 0.00043714
Iteration 67/1000 | Loss: 0.00043785
Iteration 68/1000 | Loss: 0.00045825
Iteration 69/1000 | Loss: 0.00039041
Iteration 70/1000 | Loss: 0.00017299
Iteration 71/1000 | Loss: 0.00079338
Iteration 72/1000 | Loss: 0.00253365
Iteration 73/1000 | Loss: 0.00087686
Iteration 74/1000 | Loss: 0.00049140
Iteration 75/1000 | Loss: 0.00065169
Iteration 76/1000 | Loss: 0.00068947
Iteration 77/1000 | Loss: 0.00020097
Iteration 78/1000 | Loss: 0.00009019
Iteration 79/1000 | Loss: 0.00031492
Iteration 80/1000 | Loss: 0.00030331
Iteration 81/1000 | Loss: 0.00024215
Iteration 82/1000 | Loss: 0.00025787
Iteration 83/1000 | Loss: 0.00029731
Iteration 84/1000 | Loss: 0.00073562
Iteration 85/1000 | Loss: 0.00028057
Iteration 86/1000 | Loss: 0.00032702
Iteration 87/1000 | Loss: 0.00027293
Iteration 88/1000 | Loss: 0.00014527
Iteration 89/1000 | Loss: 0.00020171
Iteration 90/1000 | Loss: 0.00029188
Iteration 91/1000 | Loss: 0.00028826
Iteration 92/1000 | Loss: 0.00032518
Iteration 93/1000 | Loss: 0.00030053
Iteration 94/1000 | Loss: 0.00034969
Iteration 95/1000 | Loss: 0.00029987
Iteration 96/1000 | Loss: 0.00018514
Iteration 97/1000 | Loss: 0.00034953
Iteration 98/1000 | Loss: 0.00033139
Iteration 99/1000 | Loss: 0.00030175
Iteration 100/1000 | Loss: 0.00031219
Iteration 101/1000 | Loss: 0.00025820
Iteration 102/1000 | Loss: 0.00020398
Iteration 103/1000 | Loss: 0.00031966
Iteration 104/1000 | Loss: 0.00032157
Iteration 105/1000 | Loss: 0.00031984
Iteration 106/1000 | Loss: 0.00031158
Iteration 107/1000 | Loss: 0.00012833
Iteration 108/1000 | Loss: 0.00018850
Iteration 109/1000 | Loss: 0.00019025
Iteration 110/1000 | Loss: 0.00035846
Iteration 111/1000 | Loss: 0.00025030
Iteration 112/1000 | Loss: 0.00018201
Iteration 113/1000 | Loss: 0.00021417
Iteration 114/1000 | Loss: 0.00018660
Iteration 115/1000 | Loss: 0.00029722
Iteration 116/1000 | Loss: 0.00033400
Iteration 117/1000 | Loss: 0.00138802
Iteration 118/1000 | Loss: 0.00035542
Iteration 119/1000 | Loss: 0.00016449
Iteration 120/1000 | Loss: 0.00027720
Iteration 121/1000 | Loss: 0.00030272
Iteration 122/1000 | Loss: 0.00179974
Iteration 123/1000 | Loss: 0.00033241
Iteration 124/1000 | Loss: 0.00025135
Iteration 125/1000 | Loss: 0.00021797
Iteration 126/1000 | Loss: 0.00019839
Iteration 127/1000 | Loss: 0.00022619
Iteration 128/1000 | Loss: 0.00037756
Iteration 129/1000 | Loss: 0.00031288
Iteration 130/1000 | Loss: 0.00034131
Iteration 131/1000 | Loss: 0.00037355
Iteration 132/1000 | Loss: 0.00031570
Iteration 133/1000 | Loss: 0.00031365
Iteration 134/1000 | Loss: 0.00032512
Iteration 135/1000 | Loss: 0.00029784
Iteration 136/1000 | Loss: 0.00038158
Iteration 137/1000 | Loss: 0.00028430
Iteration 138/1000 | Loss: 0.00019103
Iteration 139/1000 | Loss: 0.00029080
Iteration 140/1000 | Loss: 0.00028942
Iteration 141/1000 | Loss: 0.00033456
Iteration 142/1000 | Loss: 0.00027983
Iteration 143/1000 | Loss: 0.00025604
Iteration 144/1000 | Loss: 0.00036354
Iteration 145/1000 | Loss: 0.00299200
Iteration 146/1000 | Loss: 0.00039845
Iteration 147/1000 | Loss: 0.00030369
Iteration 148/1000 | Loss: 0.00025379
Iteration 149/1000 | Loss: 0.00104858
Iteration 150/1000 | Loss: 0.00031127
Iteration 151/1000 | Loss: 0.00036696
Iteration 152/1000 | Loss: 0.00033758
Iteration 153/1000 | Loss: 0.00031812
Iteration 154/1000 | Loss: 0.00031418
Iteration 155/1000 | Loss: 0.00032548
Iteration 156/1000 | Loss: 0.00025767
Iteration 157/1000 | Loss: 0.00026482
Iteration 158/1000 | Loss: 0.00020248
Iteration 159/1000 | Loss: 0.00019818
Iteration 160/1000 | Loss: 0.00028345
Iteration 161/1000 | Loss: 0.00028149
Iteration 162/1000 | Loss: 0.00017747
Iteration 163/1000 | Loss: 0.00045011
Iteration 164/1000 | Loss: 0.00020891
Iteration 165/1000 | Loss: 0.00035659
Iteration 166/1000 | Loss: 0.00032517
Iteration 167/1000 | Loss: 0.00022558
Iteration 168/1000 | Loss: 0.00017326
Iteration 169/1000 | Loss: 0.00016477
Iteration 170/1000 | Loss: 0.00015614
Iteration 171/1000 | Loss: 0.00016010
Iteration 172/1000 | Loss: 0.00025210
Iteration 173/1000 | Loss: 0.00020461
Iteration 174/1000 | Loss: 0.00012738
Iteration 175/1000 | Loss: 0.00040655
Iteration 176/1000 | Loss: 0.00031151
Iteration 177/1000 | Loss: 0.00015695
Iteration 178/1000 | Loss: 0.00030461
Iteration 179/1000 | Loss: 0.00031521
Iteration 180/1000 | Loss: 0.00025953
Iteration 181/1000 | Loss: 0.00034274
Iteration 182/1000 | Loss: 0.00014087
Iteration 183/1000 | Loss: 0.00040216
Iteration 184/1000 | Loss: 0.00022480
Iteration 185/1000 | Loss: 0.00030424
Iteration 186/1000 | Loss: 0.00014036
Iteration 187/1000 | Loss: 0.00038634
Iteration 188/1000 | Loss: 0.00027981
Iteration 189/1000 | Loss: 0.00037041
Iteration 190/1000 | Loss: 0.00029666
Iteration 191/1000 | Loss: 0.00031077
Iteration 192/1000 | Loss: 0.00026110
Iteration 193/1000 | Loss: 0.00030487
Iteration 194/1000 | Loss: 0.00023032
Iteration 195/1000 | Loss: 0.00025358
Iteration 196/1000 | Loss: 0.00040344
Iteration 197/1000 | Loss: 0.00037263
Iteration 198/1000 | Loss: 0.00037104
Iteration 199/1000 | Loss: 0.00018444
Iteration 200/1000 | Loss: 0.00032335
Iteration 201/1000 | Loss: 0.00021458
Iteration 202/1000 | Loss: 0.00033902
Iteration 203/1000 | Loss: 0.00021587
Iteration 204/1000 | Loss: 0.00021500
Iteration 205/1000 | Loss: 0.00020649
Iteration 206/1000 | Loss: 0.00030763
Iteration 207/1000 | Loss: 0.00043790
Iteration 208/1000 | Loss: 0.00033022
Iteration 209/1000 | Loss: 0.00046735
Iteration 210/1000 | Loss: 0.00033890
Iteration 211/1000 | Loss: 0.00025592
Iteration 212/1000 | Loss: 0.00026921
Iteration 213/1000 | Loss: 0.00031292
Iteration 214/1000 | Loss: 0.00026742
Iteration 215/1000 | Loss: 0.00032322
Iteration 216/1000 | Loss: 0.00029559
Iteration 217/1000 | Loss: 0.00024196
Iteration 218/1000 | Loss: 0.00012825
Iteration 219/1000 | Loss: 0.00009800
Iteration 220/1000 | Loss: 0.00010468
Iteration 221/1000 | Loss: 0.00009912
Iteration 222/1000 | Loss: 0.00011305
Iteration 223/1000 | Loss: 0.00011926
Iteration 224/1000 | Loss: 0.00013452
Iteration 225/1000 | Loss: 0.00006448
Iteration 226/1000 | Loss: 0.00006207
Iteration 227/1000 | Loss: 0.00003426
Iteration 228/1000 | Loss: 0.00002502
Iteration 229/1000 | Loss: 0.00004154
Iteration 230/1000 | Loss: 0.00003304
Iteration 231/1000 | Loss: 0.00004297
Iteration 232/1000 | Loss: 0.00004379
Iteration 233/1000 | Loss: 0.00004272
Iteration 234/1000 | Loss: 0.00002967
Iteration 235/1000 | Loss: 0.00005152
Iteration 236/1000 | Loss: 0.00004675
Iteration 237/1000 | Loss: 0.00004794
Iteration 238/1000 | Loss: 0.00003801
Iteration 239/1000 | Loss: 0.00003177
Iteration 240/1000 | Loss: 0.00005952
Iteration 241/1000 | Loss: 0.00003152
Iteration 242/1000 | Loss: 0.00002135
Iteration 243/1000 | Loss: 0.00001890
Iteration 244/1000 | Loss: 0.00001745
Iteration 245/1000 | Loss: 0.00001686
Iteration 246/1000 | Loss: 0.00001633
Iteration 247/1000 | Loss: 0.00001592
Iteration 248/1000 | Loss: 0.00001565
Iteration 249/1000 | Loss: 0.00001545
Iteration 250/1000 | Loss: 0.00001524
Iteration 251/1000 | Loss: 0.00001514
Iteration 252/1000 | Loss: 0.00001513
Iteration 253/1000 | Loss: 0.00001512
Iteration 254/1000 | Loss: 0.00001512
Iteration 255/1000 | Loss: 0.00001512
Iteration 256/1000 | Loss: 0.00001506
Iteration 257/1000 | Loss: 0.00001506
Iteration 258/1000 | Loss: 0.00001499
Iteration 259/1000 | Loss: 0.00001498
Iteration 260/1000 | Loss: 0.00001493
Iteration 261/1000 | Loss: 0.00001485
Iteration 262/1000 | Loss: 0.00001481
Iteration 263/1000 | Loss: 0.00001481
Iteration 264/1000 | Loss: 0.00001480
Iteration 265/1000 | Loss: 0.00001480
Iteration 266/1000 | Loss: 0.00001480
Iteration 267/1000 | Loss: 0.00001480
Iteration 268/1000 | Loss: 0.00001479
Iteration 269/1000 | Loss: 0.00001479
Iteration 270/1000 | Loss: 0.00001479
Iteration 271/1000 | Loss: 0.00001479
Iteration 272/1000 | Loss: 0.00001479
Iteration 273/1000 | Loss: 0.00001479
Iteration 274/1000 | Loss: 0.00001478
Iteration 275/1000 | Loss: 0.00001478
Iteration 276/1000 | Loss: 0.00001478
Iteration 277/1000 | Loss: 0.00001477
Iteration 278/1000 | Loss: 0.00001477
Iteration 279/1000 | Loss: 0.00001477
Iteration 280/1000 | Loss: 0.00001477
Iteration 281/1000 | Loss: 0.00001477
Iteration 282/1000 | Loss: 0.00001477
Iteration 283/1000 | Loss: 0.00001476
Iteration 284/1000 | Loss: 0.00001476
Iteration 285/1000 | Loss: 0.00001476
Iteration 286/1000 | Loss: 0.00001475
Iteration 287/1000 | Loss: 0.00001475
Iteration 288/1000 | Loss: 0.00001474
Iteration 289/1000 | Loss: 0.00001473
Iteration 290/1000 | Loss: 0.00001473
Iteration 291/1000 | Loss: 0.00001472
Iteration 292/1000 | Loss: 0.00001472
Iteration 293/1000 | Loss: 0.00001472
Iteration 294/1000 | Loss: 0.00001471
Iteration 295/1000 | Loss: 0.00001471
Iteration 296/1000 | Loss: 0.00001467
Iteration 297/1000 | Loss: 0.00001465
Iteration 298/1000 | Loss: 0.00001464
Iteration 299/1000 | Loss: 0.00001463
Iteration 300/1000 | Loss: 0.00001463
Iteration 301/1000 | Loss: 0.00001461
Iteration 302/1000 | Loss: 0.00001460
Iteration 303/1000 | Loss: 0.00001460
Iteration 304/1000 | Loss: 0.00001459
Iteration 305/1000 | Loss: 0.00001459
Iteration 306/1000 | Loss: 0.00001459
Iteration 307/1000 | Loss: 0.00001459
Iteration 308/1000 | Loss: 0.00001459
Iteration 309/1000 | Loss: 0.00001458
Iteration 310/1000 | Loss: 0.00001458
Iteration 311/1000 | Loss: 0.00001458
Iteration 312/1000 | Loss: 0.00001458
Iteration 313/1000 | Loss: 0.00001458
Iteration 314/1000 | Loss: 0.00001457
Iteration 315/1000 | Loss: 0.00001457
Iteration 316/1000 | Loss: 0.00001457
Iteration 317/1000 | Loss: 0.00001457
Iteration 318/1000 | Loss: 0.00001457
Iteration 319/1000 | Loss: 0.00001456
Iteration 320/1000 | Loss: 0.00001456
Iteration 321/1000 | Loss: 0.00001456
Iteration 322/1000 | Loss: 0.00001455
Iteration 323/1000 | Loss: 0.00001455
Iteration 324/1000 | Loss: 0.00001455
Iteration 325/1000 | Loss: 0.00001455
Iteration 326/1000 | Loss: 0.00001454
Iteration 327/1000 | Loss: 0.00001454
Iteration 328/1000 | Loss: 0.00001454
Iteration 329/1000 | Loss: 0.00001454
Iteration 330/1000 | Loss: 0.00001454
Iteration 331/1000 | Loss: 0.00001453
Iteration 332/1000 | Loss: 0.00001453
Iteration 333/1000 | Loss: 0.00001453
Iteration 334/1000 | Loss: 0.00001453
Iteration 335/1000 | Loss: 0.00001453
Iteration 336/1000 | Loss: 0.00001453
Iteration 337/1000 | Loss: 0.00001453
Iteration 338/1000 | Loss: 0.00001452
Iteration 339/1000 | Loss: 0.00001452
Iteration 340/1000 | Loss: 0.00001452
Iteration 341/1000 | Loss: 0.00001452
Iteration 342/1000 | Loss: 0.00001452
Iteration 343/1000 | Loss: 0.00001451
Iteration 344/1000 | Loss: 0.00001451
Iteration 345/1000 | Loss: 0.00001451
Iteration 346/1000 | Loss: 0.00001451
Iteration 347/1000 | Loss: 0.00001451
Iteration 348/1000 | Loss: 0.00001451
Iteration 349/1000 | Loss: 0.00001450
Iteration 350/1000 | Loss: 0.00001450
Iteration 351/1000 | Loss: 0.00001450
Iteration 352/1000 | Loss: 0.00001450
Iteration 353/1000 | Loss: 0.00001450
Iteration 354/1000 | Loss: 0.00001450
Iteration 355/1000 | Loss: 0.00001450
Iteration 356/1000 | Loss: 0.00001450
Iteration 357/1000 | Loss: 0.00001450
Iteration 358/1000 | Loss: 0.00001450
Iteration 359/1000 | Loss: 0.00001450
Iteration 360/1000 | Loss: 0.00001450
Iteration 361/1000 | Loss: 0.00001450
Iteration 362/1000 | Loss: 0.00001449
Iteration 363/1000 | Loss: 0.00001449
Iteration 364/1000 | Loss: 0.00001449
Iteration 365/1000 | Loss: 0.00001449
Iteration 366/1000 | Loss: 0.00001449
Iteration 367/1000 | Loss: 0.00001449
Iteration 368/1000 | Loss: 0.00001449
Iteration 369/1000 | Loss: 0.00001449
Iteration 370/1000 | Loss: 0.00001449
Iteration 371/1000 | Loss: 0.00001449
Iteration 372/1000 | Loss: 0.00001449
Iteration 373/1000 | Loss: 0.00001449
Iteration 374/1000 | Loss: 0.00001449
Iteration 375/1000 | Loss: 0.00001449
Iteration 376/1000 | Loss: 0.00001448
Iteration 377/1000 | Loss: 0.00001448
Iteration 378/1000 | Loss: 0.00001448
Iteration 379/1000 | Loss: 0.00001448
Iteration 380/1000 | Loss: 0.00001448
Iteration 381/1000 | Loss: 0.00001448
Iteration 382/1000 | Loss: 0.00001448
Iteration 383/1000 | Loss: 0.00001448
Iteration 384/1000 | Loss: 0.00001448
Iteration 385/1000 | Loss: 0.00001448
Iteration 386/1000 | Loss: 0.00001447
Iteration 387/1000 | Loss: 0.00001447
Iteration 388/1000 | Loss: 0.00001447
Iteration 389/1000 | Loss: 0.00001447
Iteration 390/1000 | Loss: 0.00001447
Iteration 391/1000 | Loss: 0.00001447
Iteration 392/1000 | Loss: 0.00001447
Iteration 393/1000 | Loss: 0.00001446
Iteration 394/1000 | Loss: 0.00001446
Iteration 395/1000 | Loss: 0.00001446
Iteration 396/1000 | Loss: 0.00001446
Iteration 397/1000 | Loss: 0.00001446
Iteration 398/1000 | Loss: 0.00001446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 398. Stopping optimization.
Last 5 losses: [1.4463902516581584e-05, 1.4463902516581584e-05, 1.4463902516581584e-05, 1.4463902516581584e-05, 1.4463902516581584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4463902516581584e-05

Optimization complete. Final v2v error: 3.170233964920044 mm

Highest mean error: 4.3237810134887695 mm for frame 60

Lowest mean error: 2.4855315685272217 mm for frame 127

Saving results

Total time: 418.8690092563629
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017513
Iteration 2/25 | Loss: 0.00282543
Iteration 3/25 | Loss: 0.00186819
Iteration 4/25 | Loss: 0.00177777
Iteration 5/25 | Loss: 0.00179959
Iteration 6/25 | Loss: 0.00168363
Iteration 7/25 | Loss: 0.00167880
Iteration 8/25 | Loss: 0.00167510
Iteration 9/25 | Loss: 0.00166482
Iteration 10/25 | Loss: 0.00166386
Iteration 11/25 | Loss: 0.00166362
Iteration 12/25 | Loss: 0.00166362
Iteration 13/25 | Loss: 0.00166362
Iteration 14/25 | Loss: 0.00166362
Iteration 15/25 | Loss: 0.00166362
Iteration 16/25 | Loss: 0.00166362
Iteration 17/25 | Loss: 0.00166362
Iteration 18/25 | Loss: 0.00166362
Iteration 19/25 | Loss: 0.00166362
Iteration 20/25 | Loss: 0.00166362
Iteration 21/25 | Loss: 0.00166362
Iteration 22/25 | Loss: 0.00166362
Iteration 23/25 | Loss: 0.00166362
Iteration 24/25 | Loss: 0.00166362
Iteration 25/25 | Loss: 0.00166362

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52575254
Iteration 2/25 | Loss: 0.00782860
Iteration 3/25 | Loss: 0.00638653
Iteration 4/25 | Loss: 0.00638648
Iteration 5/25 | Loss: 0.00638648
Iteration 6/25 | Loss: 0.00638648
Iteration 7/25 | Loss: 0.00638648
Iteration 8/25 | Loss: 0.00638648
Iteration 9/25 | Loss: 0.00638648
Iteration 10/25 | Loss: 0.00638648
Iteration 11/25 | Loss: 0.00638648
Iteration 12/25 | Loss: 0.00638648
Iteration 13/25 | Loss: 0.00638648
Iteration 14/25 | Loss: 0.00638648
Iteration 15/25 | Loss: 0.00638648
Iteration 16/25 | Loss: 0.00638648
Iteration 17/25 | Loss: 0.00638648
Iteration 18/25 | Loss: 0.00638648
Iteration 19/25 | Loss: 0.00638648
Iteration 20/25 | Loss: 0.00638648
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.006386477034538984, 0.006386477034538984, 0.006386477034538984, 0.006386477034538984, 0.006386477034538984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006386477034538984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00638648
Iteration 2/1000 | Loss: 0.00359890
Iteration 3/1000 | Loss: 0.00080987
Iteration 4/1000 | Loss: 0.00075442
Iteration 5/1000 | Loss: 0.00164553
Iteration 6/1000 | Loss: 0.00053599
Iteration 7/1000 | Loss: 0.03495846
Iteration 8/1000 | Loss: 0.05459993
Iteration 9/1000 | Loss: 0.03907349
Iteration 10/1000 | Loss: 0.01177768
Iteration 11/1000 | Loss: 0.00220843
Iteration 12/1000 | Loss: 0.00472695
Iteration 13/1000 | Loss: 0.00170446
Iteration 14/1000 | Loss: 0.00196968
Iteration 15/1000 | Loss: 0.00032795
Iteration 16/1000 | Loss: 0.00038095
Iteration 17/1000 | Loss: 0.00178531
Iteration 18/1000 | Loss: 0.00068493
Iteration 19/1000 | Loss: 0.00037486
Iteration 20/1000 | Loss: 0.00008349
Iteration 21/1000 | Loss: 0.00007848
Iteration 22/1000 | Loss: 0.00005730
Iteration 23/1000 | Loss: 0.00007521
Iteration 24/1000 | Loss: 0.00004238
Iteration 25/1000 | Loss: 0.00003624
Iteration 26/1000 | Loss: 0.00003121
Iteration 27/1000 | Loss: 0.00002759
Iteration 28/1000 | Loss: 0.00002412
Iteration 29/1000 | Loss: 0.00002243
Iteration 30/1000 | Loss: 0.00003016
Iteration 31/1000 | Loss: 0.00002094
Iteration 32/1000 | Loss: 0.00001959
Iteration 33/1000 | Loss: 0.00001900
Iteration 34/1000 | Loss: 0.00003233
Iteration 35/1000 | Loss: 0.00001862
Iteration 36/1000 | Loss: 0.00001848
Iteration 37/1000 | Loss: 0.00001837
Iteration 38/1000 | Loss: 0.00001833
Iteration 39/1000 | Loss: 0.00001828
Iteration 40/1000 | Loss: 0.00001821
Iteration 41/1000 | Loss: 0.00001817
Iteration 42/1000 | Loss: 0.00003946
Iteration 43/1000 | Loss: 0.00001851
Iteration 44/1000 | Loss: 0.00001810
Iteration 45/1000 | Loss: 0.00001807
Iteration 46/1000 | Loss: 0.00001807
Iteration 47/1000 | Loss: 0.00001806
Iteration 48/1000 | Loss: 0.00001805
Iteration 49/1000 | Loss: 0.00001805
Iteration 50/1000 | Loss: 0.00001805
Iteration 51/1000 | Loss: 0.00001805
Iteration 52/1000 | Loss: 0.00001805
Iteration 53/1000 | Loss: 0.00001805
Iteration 54/1000 | Loss: 0.00001805
Iteration 55/1000 | Loss: 0.00001805
Iteration 56/1000 | Loss: 0.00001805
Iteration 57/1000 | Loss: 0.00001805
Iteration 58/1000 | Loss: 0.00001805
Iteration 59/1000 | Loss: 0.00001805
Iteration 60/1000 | Loss: 0.00001805
Iteration 61/1000 | Loss: 0.00001805
Iteration 62/1000 | Loss: 0.00001805
Iteration 63/1000 | Loss: 0.00001805
Iteration 64/1000 | Loss: 0.00001805
Iteration 65/1000 | Loss: 0.00001805
Iteration 66/1000 | Loss: 0.00001805
Iteration 67/1000 | Loss: 0.00001805
Iteration 68/1000 | Loss: 0.00001805
Iteration 69/1000 | Loss: 0.00001805
Iteration 70/1000 | Loss: 0.00001805
Iteration 71/1000 | Loss: 0.00001805
Iteration 72/1000 | Loss: 0.00001805
Iteration 73/1000 | Loss: 0.00001805
Iteration 74/1000 | Loss: 0.00001805
Iteration 75/1000 | Loss: 0.00001805
Iteration 76/1000 | Loss: 0.00001805
Iteration 77/1000 | Loss: 0.00001805
Iteration 78/1000 | Loss: 0.00001805
Iteration 79/1000 | Loss: 0.00001805
Iteration 80/1000 | Loss: 0.00001805
Iteration 81/1000 | Loss: 0.00001805
Iteration 82/1000 | Loss: 0.00001805
Iteration 83/1000 | Loss: 0.00001805
Iteration 84/1000 | Loss: 0.00001805
Iteration 85/1000 | Loss: 0.00001805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.805173815228045e-05, 1.805173815228045e-05, 1.805173815228045e-05, 1.805173815228045e-05, 1.805173815228045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.805173815228045e-05

Optimization complete. Final v2v error: 3.6211559772491455 mm

Highest mean error: 4.067075252532959 mm for frame 233

Lowest mean error: 3.576982259750366 mm for frame 31

Saving results

Total time: 89.54057884216309
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039539
Iteration 2/25 | Loss: 0.00240628
Iteration 3/25 | Loss: 0.00236357
Iteration 4/25 | Loss: 0.00142200
Iteration 5/25 | Loss: 0.00120457
Iteration 6/25 | Loss: 0.00115003
Iteration 7/25 | Loss: 0.00105036
Iteration 8/25 | Loss: 0.00096961
Iteration 9/25 | Loss: 0.00098328
Iteration 10/25 | Loss: 0.00089803
Iteration 11/25 | Loss: 0.00093361
Iteration 12/25 | Loss: 0.00086677
Iteration 13/25 | Loss: 0.00084434
Iteration 14/25 | Loss: 0.00081448
Iteration 15/25 | Loss: 0.00081081
Iteration 16/25 | Loss: 0.00080109
Iteration 17/25 | Loss: 0.00080039
Iteration 18/25 | Loss: 0.00079121
Iteration 19/25 | Loss: 0.00079713
Iteration 20/25 | Loss: 0.00079363
Iteration 21/25 | Loss: 0.00079245
Iteration 22/25 | Loss: 0.00078578
Iteration 23/25 | Loss: 0.00078090
Iteration 24/25 | Loss: 0.00077766
Iteration 25/25 | Loss: 0.00077824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57470608
Iteration 2/25 | Loss: 0.00052993
Iteration 3/25 | Loss: 0.00050658
Iteration 4/25 | Loss: 0.00050658
Iteration 5/25 | Loss: 0.00050658
Iteration 6/25 | Loss: 0.00050658
Iteration 7/25 | Loss: 0.00050658
Iteration 8/25 | Loss: 0.00050658
Iteration 9/25 | Loss: 0.00050658
Iteration 10/25 | Loss: 0.00050658
Iteration 11/25 | Loss: 0.00050658
Iteration 12/25 | Loss: 0.00050658
Iteration 13/25 | Loss: 0.00050658
Iteration 14/25 | Loss: 0.00050658
Iteration 15/25 | Loss: 0.00050658
Iteration 16/25 | Loss: 0.00050658
Iteration 17/25 | Loss: 0.00050658
Iteration 18/25 | Loss: 0.00050658
Iteration 19/25 | Loss: 0.00050658
Iteration 20/25 | Loss: 0.00050658
Iteration 21/25 | Loss: 0.00050658
Iteration 22/25 | Loss: 0.00050658
Iteration 23/25 | Loss: 0.00050658
Iteration 24/25 | Loss: 0.00050658
Iteration 25/25 | Loss: 0.00050658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050658
Iteration 2/1000 | Loss: 0.00014100
Iteration 3/1000 | Loss: 0.00018393
Iteration 4/1000 | Loss: 0.00024483
Iteration 5/1000 | Loss: 0.00022553
Iteration 6/1000 | Loss: 0.00016044
Iteration 7/1000 | Loss: 0.00019089
Iteration 8/1000 | Loss: 0.00020294
Iteration 9/1000 | Loss: 0.00010712
Iteration 10/1000 | Loss: 0.00012976
Iteration 11/1000 | Loss: 0.00021665
Iteration 12/1000 | Loss: 0.00013325
Iteration 13/1000 | Loss: 0.00028867
Iteration 14/1000 | Loss: 0.00003462
Iteration 15/1000 | Loss: 0.00005184
Iteration 16/1000 | Loss: 0.00002238
Iteration 17/1000 | Loss: 0.00005418
Iteration 18/1000 | Loss: 0.00004328
Iteration 19/1000 | Loss: 0.00005512
Iteration 20/1000 | Loss: 0.00004230
Iteration 21/1000 | Loss: 0.00002397
Iteration 22/1000 | Loss: 0.00005220
Iteration 23/1000 | Loss: 0.00003489
Iteration 24/1000 | Loss: 0.00005753
Iteration 25/1000 | Loss: 0.00004894
Iteration 26/1000 | Loss: 0.00005783
Iteration 27/1000 | Loss: 0.00004842
Iteration 28/1000 | Loss: 0.00005822
Iteration 29/1000 | Loss: 0.00002827
Iteration 30/1000 | Loss: 0.00002702
Iteration 31/1000 | Loss: 0.00002294
Iteration 32/1000 | Loss: 0.00002236
Iteration 33/1000 | Loss: 0.00001969
Iteration 34/1000 | Loss: 0.00001918
Iteration 35/1000 | Loss: 0.00001888
Iteration 36/1000 | Loss: 0.00001859
Iteration 37/1000 | Loss: 0.00001936
Iteration 38/1000 | Loss: 0.00001814
Iteration 39/1000 | Loss: 0.00001855
Iteration 40/1000 | Loss: 0.00001794
Iteration 41/1000 | Loss: 0.00001967
Iteration 42/1000 | Loss: 0.00001769
Iteration 43/1000 | Loss: 0.00001761
Iteration 44/1000 | Loss: 0.00001761
Iteration 45/1000 | Loss: 0.00001761
Iteration 46/1000 | Loss: 0.00001761
Iteration 47/1000 | Loss: 0.00001762
Iteration 48/1000 | Loss: 0.00001767
Iteration 49/1000 | Loss: 0.00001742
Iteration 50/1000 | Loss: 0.00001742
Iteration 51/1000 | Loss: 0.00001742
Iteration 52/1000 | Loss: 0.00001742
Iteration 53/1000 | Loss: 0.00001742
Iteration 54/1000 | Loss: 0.00001742
Iteration 55/1000 | Loss: 0.00001742
Iteration 56/1000 | Loss: 0.00001742
Iteration 57/1000 | Loss: 0.00001741
Iteration 58/1000 | Loss: 0.00001741
Iteration 59/1000 | Loss: 0.00001741
Iteration 60/1000 | Loss: 0.00001741
Iteration 61/1000 | Loss: 0.00001741
Iteration 62/1000 | Loss: 0.00001741
Iteration 63/1000 | Loss: 0.00001741
Iteration 64/1000 | Loss: 0.00001744
Iteration 65/1000 | Loss: 0.00001744
Iteration 66/1000 | Loss: 0.00001732
Iteration 67/1000 | Loss: 0.00001732
Iteration 68/1000 | Loss: 0.00001732
Iteration 69/1000 | Loss: 0.00001732
Iteration 70/1000 | Loss: 0.00001732
Iteration 71/1000 | Loss: 0.00001732
Iteration 72/1000 | Loss: 0.00001732
Iteration 73/1000 | Loss: 0.00001732
Iteration 74/1000 | Loss: 0.00001732
Iteration 75/1000 | Loss: 0.00001732
Iteration 76/1000 | Loss: 0.00001731
Iteration 77/1000 | Loss: 0.00001731
Iteration 78/1000 | Loss: 0.00001731
Iteration 79/1000 | Loss: 0.00001731
Iteration 80/1000 | Loss: 0.00001742
Iteration 81/1000 | Loss: 0.00001727
Iteration 82/1000 | Loss: 0.00001726
Iteration 83/1000 | Loss: 0.00001726
Iteration 84/1000 | Loss: 0.00001726
Iteration 85/1000 | Loss: 0.00001726
Iteration 86/1000 | Loss: 0.00001726
Iteration 87/1000 | Loss: 0.00001726
Iteration 88/1000 | Loss: 0.00001726
Iteration 89/1000 | Loss: 0.00001726
Iteration 90/1000 | Loss: 0.00001726
Iteration 91/1000 | Loss: 0.00001726
Iteration 92/1000 | Loss: 0.00001726
Iteration 93/1000 | Loss: 0.00001726
Iteration 94/1000 | Loss: 0.00001726
Iteration 95/1000 | Loss: 0.00001725
Iteration 96/1000 | Loss: 0.00001725
Iteration 97/1000 | Loss: 0.00001725
Iteration 98/1000 | Loss: 0.00001738
Iteration 99/1000 | Loss: 0.00001726
Iteration 100/1000 | Loss: 0.00001725
Iteration 101/1000 | Loss: 0.00001724
Iteration 102/1000 | Loss: 0.00001724
Iteration 103/1000 | Loss: 0.00001724
Iteration 104/1000 | Loss: 0.00001724
Iteration 105/1000 | Loss: 0.00001724
Iteration 106/1000 | Loss: 0.00001724
Iteration 107/1000 | Loss: 0.00001724
Iteration 108/1000 | Loss: 0.00001724
Iteration 109/1000 | Loss: 0.00001724
Iteration 110/1000 | Loss: 0.00001723
Iteration 111/1000 | Loss: 0.00001723
Iteration 112/1000 | Loss: 0.00001723
Iteration 113/1000 | Loss: 0.00001723
Iteration 114/1000 | Loss: 0.00001723
Iteration 115/1000 | Loss: 0.00001723
Iteration 116/1000 | Loss: 0.00001723
Iteration 117/1000 | Loss: 0.00001723
Iteration 118/1000 | Loss: 0.00001723
Iteration 119/1000 | Loss: 0.00001723
Iteration 120/1000 | Loss: 0.00001723
Iteration 121/1000 | Loss: 0.00001723
Iteration 122/1000 | Loss: 0.00001723
Iteration 123/1000 | Loss: 0.00001722
Iteration 124/1000 | Loss: 0.00001722
Iteration 125/1000 | Loss: 0.00001722
Iteration 126/1000 | Loss: 0.00001722
Iteration 127/1000 | Loss: 0.00001721
Iteration 128/1000 | Loss: 0.00001721
Iteration 129/1000 | Loss: 0.00001721
Iteration 130/1000 | Loss: 0.00001721
Iteration 131/1000 | Loss: 0.00001721
Iteration 132/1000 | Loss: 0.00001721
Iteration 133/1000 | Loss: 0.00001720
Iteration 134/1000 | Loss: 0.00001720
Iteration 135/1000 | Loss: 0.00001720
Iteration 136/1000 | Loss: 0.00001720
Iteration 137/1000 | Loss: 0.00001720
Iteration 138/1000 | Loss: 0.00001720
Iteration 139/1000 | Loss: 0.00001720
Iteration 140/1000 | Loss: 0.00001720
Iteration 141/1000 | Loss: 0.00001720
Iteration 142/1000 | Loss: 0.00001719
Iteration 143/1000 | Loss: 0.00001719
Iteration 144/1000 | Loss: 0.00001719
Iteration 145/1000 | Loss: 0.00001745
Iteration 146/1000 | Loss: 0.00001745
Iteration 147/1000 | Loss: 0.00001717
Iteration 148/1000 | Loss: 0.00001717
Iteration 149/1000 | Loss: 0.00001717
Iteration 150/1000 | Loss: 0.00001717
Iteration 151/1000 | Loss: 0.00001716
Iteration 152/1000 | Loss: 0.00001716
Iteration 153/1000 | Loss: 0.00001716
Iteration 154/1000 | Loss: 0.00001716
Iteration 155/1000 | Loss: 0.00001716
Iteration 156/1000 | Loss: 0.00001716
Iteration 157/1000 | Loss: 0.00001716
Iteration 158/1000 | Loss: 0.00001716
Iteration 159/1000 | Loss: 0.00001716
Iteration 160/1000 | Loss: 0.00001716
Iteration 161/1000 | Loss: 0.00001716
Iteration 162/1000 | Loss: 0.00001716
Iteration 163/1000 | Loss: 0.00001716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.7161994037451223e-05, 1.7161994037451223e-05, 1.7161994037451223e-05, 1.7161994037451223e-05, 1.7161994037451223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7161994037451223e-05

Optimization complete. Final v2v error: 3.3888564109802246 mm

Highest mean error: 7.07183837890625 mm for frame 75

Lowest mean error: 2.9046847820281982 mm for frame 0

Saving results

Total time: 119.87318658828735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003494
Iteration 2/25 | Loss: 0.00231649
Iteration 3/25 | Loss: 0.00174043
Iteration 4/25 | Loss: 0.00159232
Iteration 5/25 | Loss: 0.00118580
Iteration 6/25 | Loss: 0.00120333
Iteration 7/25 | Loss: 0.00117409
Iteration 8/25 | Loss: 0.00110987
Iteration 9/25 | Loss: 0.00103983
Iteration 10/25 | Loss: 0.00099762
Iteration 11/25 | Loss: 0.00104050
Iteration 12/25 | Loss: 0.00105278
Iteration 13/25 | Loss: 0.00100315
Iteration 14/25 | Loss: 0.00101828
Iteration 15/25 | Loss: 0.00106458
Iteration 16/25 | Loss: 0.00104169
Iteration 17/25 | Loss: 0.00100633
Iteration 18/25 | Loss: 0.00099438
Iteration 19/25 | Loss: 0.00100524
Iteration 20/25 | Loss: 0.00098032
Iteration 21/25 | Loss: 0.00106072
Iteration 22/25 | Loss: 0.00106380
Iteration 23/25 | Loss: 0.00103652
Iteration 24/25 | Loss: 0.00100945
Iteration 25/25 | Loss: 0.00100501

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50099361
Iteration 2/25 | Loss: 0.00141265
Iteration 3/25 | Loss: 0.00141265
Iteration 4/25 | Loss: 0.00141265
Iteration 5/25 | Loss: 0.00141265
Iteration 6/25 | Loss: 0.00141265
Iteration 7/25 | Loss: 0.00141265
Iteration 8/25 | Loss: 0.00141265
Iteration 9/25 | Loss: 0.00141265
Iteration 10/25 | Loss: 0.00141265
Iteration 11/25 | Loss: 0.00141265
Iteration 12/25 | Loss: 0.00141265
Iteration 13/25 | Loss: 0.00141265
Iteration 14/25 | Loss: 0.00141265
Iteration 15/25 | Loss: 0.00141265
Iteration 16/25 | Loss: 0.00141265
Iteration 17/25 | Loss: 0.00141265
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014126502210274339, 0.0014126502210274339, 0.0014126502210274339, 0.0014126502210274339, 0.0014126502210274339]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014126502210274339

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141265
Iteration 2/1000 | Loss: 0.00418617
Iteration 3/1000 | Loss: 0.00343693
Iteration 4/1000 | Loss: 0.00121858
Iteration 5/1000 | Loss: 0.00154320
Iteration 6/1000 | Loss: 0.00098153
Iteration 7/1000 | Loss: 0.00142352
Iteration 8/1000 | Loss: 0.00042656
Iteration 9/1000 | Loss: 0.00053076
Iteration 10/1000 | Loss: 0.00318885
Iteration 11/1000 | Loss: 0.00141222
Iteration 12/1000 | Loss: 0.00266217
Iteration 13/1000 | Loss: 0.00153516
Iteration 14/1000 | Loss: 0.00114087
Iteration 15/1000 | Loss: 0.00088852
Iteration 16/1000 | Loss: 0.00217880
Iteration 17/1000 | Loss: 0.00206322
Iteration 18/1000 | Loss: 0.00272508
Iteration 19/1000 | Loss: 0.00433365
Iteration 20/1000 | Loss: 0.00138703
Iteration 21/1000 | Loss: 0.00135181
Iteration 22/1000 | Loss: 0.00132349
Iteration 23/1000 | Loss: 0.00126619
Iteration 24/1000 | Loss: 0.00126740
Iteration 25/1000 | Loss: 0.00160689
Iteration 26/1000 | Loss: 0.00351072
Iteration 27/1000 | Loss: 0.00225415
Iteration 28/1000 | Loss: 0.00283961
Iteration 29/1000 | Loss: 0.00162884
Iteration 30/1000 | Loss: 0.00014212
Iteration 31/1000 | Loss: 0.00070564
Iteration 32/1000 | Loss: 0.00011258
Iteration 33/1000 | Loss: 0.00047785
Iteration 34/1000 | Loss: 0.00037669
Iteration 35/1000 | Loss: 0.00040339
Iteration 36/1000 | Loss: 0.00012241
Iteration 37/1000 | Loss: 0.00106374
Iteration 38/1000 | Loss: 0.00088849
Iteration 39/1000 | Loss: 0.00051625
Iteration 40/1000 | Loss: 0.00008505
Iteration 41/1000 | Loss: 0.00018062
Iteration 42/1000 | Loss: 0.00006876
Iteration 43/1000 | Loss: 0.00112152
Iteration 44/1000 | Loss: 0.00072662
Iteration 45/1000 | Loss: 0.00254225
Iteration 46/1000 | Loss: 0.00048451
Iteration 47/1000 | Loss: 0.00076662
Iteration 48/1000 | Loss: 0.00162968
Iteration 49/1000 | Loss: 0.00076086
Iteration 50/1000 | Loss: 0.00161344
Iteration 51/1000 | Loss: 0.00156554
Iteration 52/1000 | Loss: 0.00031487
Iteration 53/1000 | Loss: 0.00059123
Iteration 54/1000 | Loss: 0.00088819
Iteration 55/1000 | Loss: 0.00054030
Iteration 56/1000 | Loss: 0.00008795
Iteration 57/1000 | Loss: 0.00043823
Iteration 58/1000 | Loss: 0.00095200
Iteration 59/1000 | Loss: 0.00050341
Iteration 60/1000 | Loss: 0.00081942
Iteration 61/1000 | Loss: 0.00059184
Iteration 62/1000 | Loss: 0.00067365
Iteration 63/1000 | Loss: 0.00133091
Iteration 64/1000 | Loss: 0.00039485
Iteration 65/1000 | Loss: 0.00009576
Iteration 66/1000 | Loss: 0.00048501
Iteration 67/1000 | Loss: 0.00093922
Iteration 68/1000 | Loss: 0.00036521
Iteration 69/1000 | Loss: 0.00035085
Iteration 70/1000 | Loss: 0.00007247
Iteration 71/1000 | Loss: 0.00056850
Iteration 72/1000 | Loss: 0.00042953
Iteration 73/1000 | Loss: 0.00005946
Iteration 74/1000 | Loss: 0.00116642
Iteration 75/1000 | Loss: 0.00130194
Iteration 76/1000 | Loss: 0.00017574
Iteration 77/1000 | Loss: 0.00027098
Iteration 78/1000 | Loss: 0.00023092
Iteration 79/1000 | Loss: 0.00017051
Iteration 80/1000 | Loss: 0.00006891
Iteration 81/1000 | Loss: 0.00060328
Iteration 82/1000 | Loss: 0.00131080
Iteration 83/1000 | Loss: 0.00133784
Iteration 84/1000 | Loss: 0.00058322
Iteration 85/1000 | Loss: 0.00137371
Iteration 86/1000 | Loss: 0.00041566
Iteration 87/1000 | Loss: 0.00083259
Iteration 88/1000 | Loss: 0.00127072
Iteration 89/1000 | Loss: 0.00101561
Iteration 90/1000 | Loss: 0.00051726
Iteration 91/1000 | Loss: 0.00100569
Iteration 92/1000 | Loss: 0.00088335
Iteration 93/1000 | Loss: 0.00044938
Iteration 94/1000 | Loss: 0.00119643
Iteration 95/1000 | Loss: 0.00081466
Iteration 96/1000 | Loss: 0.00022012
Iteration 97/1000 | Loss: 0.00053773
Iteration 98/1000 | Loss: 0.00013263
Iteration 99/1000 | Loss: 0.00007281
Iteration 100/1000 | Loss: 0.00006765
Iteration 101/1000 | Loss: 0.00005377
Iteration 102/1000 | Loss: 0.00004878
Iteration 103/1000 | Loss: 0.00004495
Iteration 104/1000 | Loss: 0.00022935
Iteration 105/1000 | Loss: 0.00005505
Iteration 106/1000 | Loss: 0.00049263
Iteration 107/1000 | Loss: 0.00028503
Iteration 108/1000 | Loss: 0.00019759
Iteration 109/1000 | Loss: 0.00015563
Iteration 110/1000 | Loss: 0.00024606
Iteration 111/1000 | Loss: 0.00010277
Iteration 112/1000 | Loss: 0.00005542
Iteration 113/1000 | Loss: 0.00023509
Iteration 114/1000 | Loss: 0.00005135
Iteration 115/1000 | Loss: 0.00062643
Iteration 116/1000 | Loss: 0.00047409
Iteration 117/1000 | Loss: 0.00059936
Iteration 118/1000 | Loss: 0.00050827
Iteration 119/1000 | Loss: 0.00045673
Iteration 120/1000 | Loss: 0.00082395
Iteration 121/1000 | Loss: 0.00026168
Iteration 122/1000 | Loss: 0.00006866
Iteration 123/1000 | Loss: 0.00018101
Iteration 124/1000 | Loss: 0.00003360
Iteration 125/1000 | Loss: 0.00049330
Iteration 126/1000 | Loss: 0.00019559
Iteration 127/1000 | Loss: 0.00005748
Iteration 128/1000 | Loss: 0.00020675
Iteration 129/1000 | Loss: 0.00019926
Iteration 130/1000 | Loss: 0.00011466
Iteration 131/1000 | Loss: 0.00003744
Iteration 132/1000 | Loss: 0.00003476
Iteration 133/1000 | Loss: 0.00014010
Iteration 134/1000 | Loss: 0.00005000
Iteration 135/1000 | Loss: 0.00004357
Iteration 136/1000 | Loss: 0.00003747
Iteration 137/1000 | Loss: 0.00003818
Iteration 138/1000 | Loss: 0.00004083
Iteration 139/1000 | Loss: 0.00003513
Iteration 140/1000 | Loss: 0.00032430
Iteration 141/1000 | Loss: 0.00026048
Iteration 142/1000 | Loss: 0.00021361
Iteration 143/1000 | Loss: 0.00009329
Iteration 144/1000 | Loss: 0.00034714
Iteration 145/1000 | Loss: 0.00004395
Iteration 146/1000 | Loss: 0.00019955
Iteration 147/1000 | Loss: 0.00019267
Iteration 148/1000 | Loss: 0.00006471
Iteration 149/1000 | Loss: 0.00041736
Iteration 150/1000 | Loss: 0.00007898
Iteration 151/1000 | Loss: 0.00024970
Iteration 152/1000 | Loss: 0.00004763
Iteration 153/1000 | Loss: 0.00021316
Iteration 154/1000 | Loss: 0.00037659
Iteration 155/1000 | Loss: 0.00004334
Iteration 156/1000 | Loss: 0.00004571
Iteration 157/1000 | Loss: 0.00004925
Iteration 158/1000 | Loss: 0.00002871
Iteration 159/1000 | Loss: 0.00003535
Iteration 160/1000 | Loss: 0.00002790
Iteration 161/1000 | Loss: 0.00003119
Iteration 162/1000 | Loss: 0.00002850
Iteration 163/1000 | Loss: 0.00002505
Iteration 164/1000 | Loss: 0.00003338
Iteration 165/1000 | Loss: 0.00003301
Iteration 166/1000 | Loss: 0.00002982
Iteration 167/1000 | Loss: 0.00003253
Iteration 168/1000 | Loss: 0.00003540
Iteration 169/1000 | Loss: 0.00003805
Iteration 170/1000 | Loss: 0.00002280
Iteration 171/1000 | Loss: 0.00002746
Iteration 172/1000 | Loss: 0.00003278
Iteration 173/1000 | Loss: 0.00003561
Iteration 174/1000 | Loss: 0.00009879
Iteration 175/1000 | Loss: 0.00006138
Iteration 176/1000 | Loss: 0.00008021
Iteration 177/1000 | Loss: 0.00002740
Iteration 178/1000 | Loss: 0.00005440
Iteration 179/1000 | Loss: 0.00002202
Iteration 180/1000 | Loss: 0.00002024
Iteration 181/1000 | Loss: 0.00001898
Iteration 182/1000 | Loss: 0.00001833
Iteration 183/1000 | Loss: 0.00001795
Iteration 184/1000 | Loss: 0.00001774
Iteration 185/1000 | Loss: 0.00001767
Iteration 186/1000 | Loss: 0.00001760
Iteration 187/1000 | Loss: 0.00001751
Iteration 188/1000 | Loss: 0.00001749
Iteration 189/1000 | Loss: 0.00001749
Iteration 190/1000 | Loss: 0.00001749
Iteration 191/1000 | Loss: 0.00001747
Iteration 192/1000 | Loss: 0.00001747
Iteration 193/1000 | Loss: 0.00001747
Iteration 194/1000 | Loss: 0.00001747
Iteration 195/1000 | Loss: 0.00001746
Iteration 196/1000 | Loss: 0.00001746
Iteration 197/1000 | Loss: 0.00001746
Iteration 198/1000 | Loss: 0.00001746
Iteration 199/1000 | Loss: 0.00001746
Iteration 200/1000 | Loss: 0.00001746
Iteration 201/1000 | Loss: 0.00001746
Iteration 202/1000 | Loss: 0.00001746
Iteration 203/1000 | Loss: 0.00001746
Iteration 204/1000 | Loss: 0.00001746
Iteration 205/1000 | Loss: 0.00001746
Iteration 206/1000 | Loss: 0.00001746
Iteration 207/1000 | Loss: 0.00001746
Iteration 208/1000 | Loss: 0.00001746
Iteration 209/1000 | Loss: 0.00001745
Iteration 210/1000 | Loss: 0.00001745
Iteration 211/1000 | Loss: 0.00001745
Iteration 212/1000 | Loss: 0.00001745
Iteration 213/1000 | Loss: 0.00001745
Iteration 214/1000 | Loss: 0.00001744
Iteration 215/1000 | Loss: 0.00001744
Iteration 216/1000 | Loss: 0.00001744
Iteration 217/1000 | Loss: 0.00001744
Iteration 218/1000 | Loss: 0.00001744
Iteration 219/1000 | Loss: 0.00001744
Iteration 220/1000 | Loss: 0.00001743
Iteration 221/1000 | Loss: 0.00001743
Iteration 222/1000 | Loss: 0.00001743
Iteration 223/1000 | Loss: 0.00001743
Iteration 224/1000 | Loss: 0.00001743
Iteration 225/1000 | Loss: 0.00001743
Iteration 226/1000 | Loss: 0.00001743
Iteration 227/1000 | Loss: 0.00001743
Iteration 228/1000 | Loss: 0.00001743
Iteration 229/1000 | Loss: 0.00001743
Iteration 230/1000 | Loss: 0.00001743
Iteration 231/1000 | Loss: 0.00001743
Iteration 232/1000 | Loss: 0.00001743
Iteration 233/1000 | Loss: 0.00001743
Iteration 234/1000 | Loss: 0.00001743
Iteration 235/1000 | Loss: 0.00001743
Iteration 236/1000 | Loss: 0.00001743
Iteration 237/1000 | Loss: 0.00001743
Iteration 238/1000 | Loss: 0.00001743
Iteration 239/1000 | Loss: 0.00001743
Iteration 240/1000 | Loss: 0.00001743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.7426144040655345e-05, 1.7426144040655345e-05, 1.7426144040655345e-05, 1.7426144040655345e-05, 1.7426144040655345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7426144040655345e-05

Optimization complete. Final v2v error: 3.4854812622070312 mm

Highest mean error: 4.701289176940918 mm for frame 3

Lowest mean error: 3.2526235580444336 mm for frame 55

Saving results

Total time: 362.3903830051422
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798227
Iteration 2/25 | Loss: 0.00117479
Iteration 3/25 | Loss: 0.00080663
Iteration 4/25 | Loss: 0.00074648
Iteration 5/25 | Loss: 0.00074071
Iteration 6/25 | Loss: 0.00073863
Iteration 7/25 | Loss: 0.00073849
Iteration 8/25 | Loss: 0.00073849
Iteration 9/25 | Loss: 0.00073849
Iteration 10/25 | Loss: 0.00073849
Iteration 11/25 | Loss: 0.00073849
Iteration 12/25 | Loss: 0.00073849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007384889759123325, 0.0007384889759123325, 0.0007384889759123325, 0.0007384889759123325, 0.0007384889759123325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007384889759123325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51195765
Iteration 2/25 | Loss: 0.00049030
Iteration 3/25 | Loss: 0.00049030
Iteration 4/25 | Loss: 0.00049030
Iteration 5/25 | Loss: 0.00049030
Iteration 6/25 | Loss: 0.00049030
Iteration 7/25 | Loss: 0.00049030
Iteration 8/25 | Loss: 0.00049030
Iteration 9/25 | Loss: 0.00049030
Iteration 10/25 | Loss: 0.00049030
Iteration 11/25 | Loss: 0.00049030
Iteration 12/25 | Loss: 0.00049030
Iteration 13/25 | Loss: 0.00049030
Iteration 14/25 | Loss: 0.00049030
Iteration 15/25 | Loss: 0.00049030
Iteration 16/25 | Loss: 0.00049030
Iteration 17/25 | Loss: 0.00049030
Iteration 18/25 | Loss: 0.00049030
Iteration 19/25 | Loss: 0.00049030
Iteration 20/25 | Loss: 0.00049030
Iteration 21/25 | Loss: 0.00049030
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004902972723357379, 0.0004902972723357379, 0.0004902972723357379, 0.0004902972723357379, 0.0004902972723357379]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004902972723357379

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049030
Iteration 2/1000 | Loss: 0.00002420
Iteration 3/1000 | Loss: 0.00001557
Iteration 4/1000 | Loss: 0.00001397
Iteration 5/1000 | Loss: 0.00001309
Iteration 6/1000 | Loss: 0.00001233
Iteration 7/1000 | Loss: 0.00001197
Iteration 8/1000 | Loss: 0.00001173
Iteration 9/1000 | Loss: 0.00001167
Iteration 10/1000 | Loss: 0.00001150
Iteration 11/1000 | Loss: 0.00001134
Iteration 12/1000 | Loss: 0.00001131
Iteration 13/1000 | Loss: 0.00001125
Iteration 14/1000 | Loss: 0.00001125
Iteration 15/1000 | Loss: 0.00001119
Iteration 16/1000 | Loss: 0.00001119
Iteration 17/1000 | Loss: 0.00001117
Iteration 18/1000 | Loss: 0.00001115
Iteration 19/1000 | Loss: 0.00001107
Iteration 20/1000 | Loss: 0.00001105
Iteration 21/1000 | Loss: 0.00001104
Iteration 22/1000 | Loss: 0.00001103
Iteration 23/1000 | Loss: 0.00001103
Iteration 24/1000 | Loss: 0.00001102
Iteration 25/1000 | Loss: 0.00001102
Iteration 26/1000 | Loss: 0.00001102
Iteration 27/1000 | Loss: 0.00001101
Iteration 28/1000 | Loss: 0.00001101
Iteration 29/1000 | Loss: 0.00001099
Iteration 30/1000 | Loss: 0.00001099
Iteration 31/1000 | Loss: 0.00001097
Iteration 32/1000 | Loss: 0.00001096
Iteration 33/1000 | Loss: 0.00001096
Iteration 34/1000 | Loss: 0.00001094
Iteration 35/1000 | Loss: 0.00001094
Iteration 36/1000 | Loss: 0.00001092
Iteration 37/1000 | Loss: 0.00001090
Iteration 38/1000 | Loss: 0.00001089
Iteration 39/1000 | Loss: 0.00001088
Iteration 40/1000 | Loss: 0.00001088
Iteration 41/1000 | Loss: 0.00001087
Iteration 42/1000 | Loss: 0.00001086
Iteration 43/1000 | Loss: 0.00001086
Iteration 44/1000 | Loss: 0.00001086
Iteration 45/1000 | Loss: 0.00001086
Iteration 46/1000 | Loss: 0.00001086
Iteration 47/1000 | Loss: 0.00001086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 47. Stopping optimization.
Last 5 losses: [1.0857598681468517e-05, 1.0857598681468517e-05, 1.0857598681468517e-05, 1.0857598681468517e-05, 1.0857598681468517e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0857598681468517e-05

Optimization complete. Final v2v error: 2.782515525817871 mm

Highest mean error: 2.9737114906311035 mm for frame 54

Lowest mean error: 2.6271634101867676 mm for frame 2

Saving results

Total time: 34.59109687805176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493475
Iteration 2/25 | Loss: 0.00097373
Iteration 3/25 | Loss: 0.00085841
Iteration 4/25 | Loss: 0.00081962
Iteration 5/25 | Loss: 0.00080711
Iteration 6/25 | Loss: 0.00080475
Iteration 7/25 | Loss: 0.00080371
Iteration 8/25 | Loss: 0.00080365
Iteration 9/25 | Loss: 0.00080365
Iteration 10/25 | Loss: 0.00080364
Iteration 11/25 | Loss: 0.00080364
Iteration 12/25 | Loss: 0.00080364
Iteration 13/25 | Loss: 0.00080364
Iteration 14/25 | Loss: 0.00080364
Iteration 15/25 | Loss: 0.00080364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008036366198211908, 0.0008036366198211908, 0.0008036366198211908, 0.0008036366198211908, 0.0008036366198211908]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008036366198211908

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.43585396
Iteration 2/25 | Loss: 0.00044582
Iteration 3/25 | Loss: 0.00044581
Iteration 4/25 | Loss: 0.00044581
Iteration 5/25 | Loss: 0.00044580
Iteration 6/25 | Loss: 0.00044580
Iteration 7/25 | Loss: 0.00044580
Iteration 8/25 | Loss: 0.00044580
Iteration 9/25 | Loss: 0.00044580
Iteration 10/25 | Loss: 0.00044580
Iteration 11/25 | Loss: 0.00044580
Iteration 12/25 | Loss: 0.00044580
Iteration 13/25 | Loss: 0.00044580
Iteration 14/25 | Loss: 0.00044580
Iteration 15/25 | Loss: 0.00044580
Iteration 16/25 | Loss: 0.00044580
Iteration 17/25 | Loss: 0.00044580
Iteration 18/25 | Loss: 0.00044580
Iteration 19/25 | Loss: 0.00044580
Iteration 20/25 | Loss: 0.00044580
Iteration 21/25 | Loss: 0.00044580
Iteration 22/25 | Loss: 0.00044580
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0004458031908143312, 0.0004458031908143312, 0.0004458031908143312, 0.0004458031908143312, 0.0004458031908143312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004458031908143312

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044580
Iteration 2/1000 | Loss: 0.00004867
Iteration 3/1000 | Loss: 0.00003317
Iteration 4/1000 | Loss: 0.00003045
Iteration 5/1000 | Loss: 0.00002917
Iteration 6/1000 | Loss: 0.00002795
Iteration 7/1000 | Loss: 0.00002735
Iteration 8/1000 | Loss: 0.00002668
Iteration 9/1000 | Loss: 0.00002628
Iteration 10/1000 | Loss: 0.00002596
Iteration 11/1000 | Loss: 0.00002573
Iteration 12/1000 | Loss: 0.00002565
Iteration 13/1000 | Loss: 0.00002550
Iteration 14/1000 | Loss: 0.00002542
Iteration 15/1000 | Loss: 0.00002541
Iteration 16/1000 | Loss: 0.00002540
Iteration 17/1000 | Loss: 0.00002539
Iteration 18/1000 | Loss: 0.00002538
Iteration 19/1000 | Loss: 0.00002537
Iteration 20/1000 | Loss: 0.00002536
Iteration 21/1000 | Loss: 0.00002536
Iteration 22/1000 | Loss: 0.00002535
Iteration 23/1000 | Loss: 0.00002534
Iteration 24/1000 | Loss: 0.00002534
Iteration 25/1000 | Loss: 0.00002531
Iteration 26/1000 | Loss: 0.00002530
Iteration 27/1000 | Loss: 0.00002530
Iteration 28/1000 | Loss: 0.00002530
Iteration 29/1000 | Loss: 0.00002530
Iteration 30/1000 | Loss: 0.00002528
Iteration 31/1000 | Loss: 0.00002527
Iteration 32/1000 | Loss: 0.00002527
Iteration 33/1000 | Loss: 0.00002525
Iteration 34/1000 | Loss: 0.00002525
Iteration 35/1000 | Loss: 0.00002525
Iteration 36/1000 | Loss: 0.00002524
Iteration 37/1000 | Loss: 0.00002522
Iteration 38/1000 | Loss: 0.00002521
Iteration 39/1000 | Loss: 0.00002520
Iteration 40/1000 | Loss: 0.00002520
Iteration 41/1000 | Loss: 0.00002520
Iteration 42/1000 | Loss: 0.00002520
Iteration 43/1000 | Loss: 0.00002520
Iteration 44/1000 | Loss: 0.00002520
Iteration 45/1000 | Loss: 0.00002519
Iteration 46/1000 | Loss: 0.00002519
Iteration 47/1000 | Loss: 0.00002517
Iteration 48/1000 | Loss: 0.00002516
Iteration 49/1000 | Loss: 0.00002516
Iteration 50/1000 | Loss: 0.00002516
Iteration 51/1000 | Loss: 0.00002515
Iteration 52/1000 | Loss: 0.00002514
Iteration 53/1000 | Loss: 0.00002513
Iteration 54/1000 | Loss: 0.00002512
Iteration 55/1000 | Loss: 0.00002512
Iteration 56/1000 | Loss: 0.00002511
Iteration 57/1000 | Loss: 0.00002511
Iteration 58/1000 | Loss: 0.00002511
Iteration 59/1000 | Loss: 0.00002510
Iteration 60/1000 | Loss: 0.00002510
Iteration 61/1000 | Loss: 0.00002509
Iteration 62/1000 | Loss: 0.00002509
Iteration 63/1000 | Loss: 0.00002509
Iteration 64/1000 | Loss: 0.00002508
Iteration 65/1000 | Loss: 0.00002507
Iteration 66/1000 | Loss: 0.00002507
Iteration 67/1000 | Loss: 0.00002507
Iteration 68/1000 | Loss: 0.00002506
Iteration 69/1000 | Loss: 0.00002506
Iteration 70/1000 | Loss: 0.00002506
Iteration 71/1000 | Loss: 0.00002506
Iteration 72/1000 | Loss: 0.00002505
Iteration 73/1000 | Loss: 0.00002504
Iteration 74/1000 | Loss: 0.00002504
Iteration 75/1000 | Loss: 0.00002504
Iteration 76/1000 | Loss: 0.00002504
Iteration 77/1000 | Loss: 0.00002504
Iteration 78/1000 | Loss: 0.00002504
Iteration 79/1000 | Loss: 0.00002504
Iteration 80/1000 | Loss: 0.00002504
Iteration 81/1000 | Loss: 0.00002504
Iteration 82/1000 | Loss: 0.00002504
Iteration 83/1000 | Loss: 0.00002504
Iteration 84/1000 | Loss: 0.00002504
Iteration 85/1000 | Loss: 0.00002503
Iteration 86/1000 | Loss: 0.00002503
Iteration 87/1000 | Loss: 0.00002502
Iteration 88/1000 | Loss: 0.00002502
Iteration 89/1000 | Loss: 0.00002502
Iteration 90/1000 | Loss: 0.00002502
Iteration 91/1000 | Loss: 0.00002502
Iteration 92/1000 | Loss: 0.00002502
Iteration 93/1000 | Loss: 0.00002501
Iteration 94/1000 | Loss: 0.00002501
Iteration 95/1000 | Loss: 0.00002501
Iteration 96/1000 | Loss: 0.00002501
Iteration 97/1000 | Loss: 0.00002501
Iteration 98/1000 | Loss: 0.00002501
Iteration 99/1000 | Loss: 0.00002501
Iteration 100/1000 | Loss: 0.00002501
Iteration 101/1000 | Loss: 0.00002501
Iteration 102/1000 | Loss: 0.00002501
Iteration 103/1000 | Loss: 0.00002500
Iteration 104/1000 | Loss: 0.00002500
Iteration 105/1000 | Loss: 0.00002500
Iteration 106/1000 | Loss: 0.00002499
Iteration 107/1000 | Loss: 0.00002499
Iteration 108/1000 | Loss: 0.00002499
Iteration 109/1000 | Loss: 0.00002499
Iteration 110/1000 | Loss: 0.00002499
Iteration 111/1000 | Loss: 0.00002499
Iteration 112/1000 | Loss: 0.00002499
Iteration 113/1000 | Loss: 0.00002498
Iteration 114/1000 | Loss: 0.00002498
Iteration 115/1000 | Loss: 0.00002498
Iteration 116/1000 | Loss: 0.00002498
Iteration 117/1000 | Loss: 0.00002498
Iteration 118/1000 | Loss: 0.00002498
Iteration 119/1000 | Loss: 0.00002498
Iteration 120/1000 | Loss: 0.00002498
Iteration 121/1000 | Loss: 0.00002498
Iteration 122/1000 | Loss: 0.00002498
Iteration 123/1000 | Loss: 0.00002498
Iteration 124/1000 | Loss: 0.00002497
Iteration 125/1000 | Loss: 0.00002497
Iteration 126/1000 | Loss: 0.00002497
Iteration 127/1000 | Loss: 0.00002497
Iteration 128/1000 | Loss: 0.00002497
Iteration 129/1000 | Loss: 0.00002497
Iteration 130/1000 | Loss: 0.00002496
Iteration 131/1000 | Loss: 0.00002496
Iteration 132/1000 | Loss: 0.00002496
Iteration 133/1000 | Loss: 0.00002496
Iteration 134/1000 | Loss: 0.00002496
Iteration 135/1000 | Loss: 0.00002496
Iteration 136/1000 | Loss: 0.00002496
Iteration 137/1000 | Loss: 0.00002496
Iteration 138/1000 | Loss: 0.00002496
Iteration 139/1000 | Loss: 0.00002496
Iteration 140/1000 | Loss: 0.00002495
Iteration 141/1000 | Loss: 0.00002495
Iteration 142/1000 | Loss: 0.00002495
Iteration 143/1000 | Loss: 0.00002495
Iteration 144/1000 | Loss: 0.00002495
Iteration 145/1000 | Loss: 0.00002495
Iteration 146/1000 | Loss: 0.00002495
Iteration 147/1000 | Loss: 0.00002495
Iteration 148/1000 | Loss: 0.00002495
Iteration 149/1000 | Loss: 0.00002495
Iteration 150/1000 | Loss: 0.00002495
Iteration 151/1000 | Loss: 0.00002494
Iteration 152/1000 | Loss: 0.00002494
Iteration 153/1000 | Loss: 0.00002494
Iteration 154/1000 | Loss: 0.00002494
Iteration 155/1000 | Loss: 0.00002494
Iteration 156/1000 | Loss: 0.00002494
Iteration 157/1000 | Loss: 0.00002494
Iteration 158/1000 | Loss: 0.00002494
Iteration 159/1000 | Loss: 0.00002494
Iteration 160/1000 | Loss: 0.00002494
Iteration 161/1000 | Loss: 0.00002494
Iteration 162/1000 | Loss: 0.00002494
Iteration 163/1000 | Loss: 0.00002494
Iteration 164/1000 | Loss: 0.00002494
Iteration 165/1000 | Loss: 0.00002494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [2.494247382855974e-05, 2.494247382855974e-05, 2.494247382855974e-05, 2.494247382855974e-05, 2.494247382855974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.494247382855974e-05

Optimization complete. Final v2v error: 4.173068523406982 mm

Highest mean error: 4.8707098960876465 mm for frame 17

Lowest mean error: 3.485179901123047 mm for frame 3

Saving results

Total time: 43.56407880783081
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00628365
Iteration 2/25 | Loss: 0.00135459
Iteration 3/25 | Loss: 0.00106704
Iteration 4/25 | Loss: 0.00099345
Iteration 5/25 | Loss: 0.00096870
Iteration 6/25 | Loss: 0.00098355
Iteration 7/25 | Loss: 0.00092712
Iteration 8/25 | Loss: 0.00090223
Iteration 9/25 | Loss: 0.00089547
Iteration 10/25 | Loss: 0.00088947
Iteration 11/25 | Loss: 0.00088696
Iteration 12/25 | Loss: 0.00088606
Iteration 13/25 | Loss: 0.00088598
Iteration 14/25 | Loss: 0.00088594
Iteration 15/25 | Loss: 0.00088593
Iteration 16/25 | Loss: 0.00088593
Iteration 17/25 | Loss: 0.00088593
Iteration 18/25 | Loss: 0.00088593
Iteration 19/25 | Loss: 0.00088593
Iteration 20/25 | Loss: 0.00088593
Iteration 21/25 | Loss: 0.00088593
Iteration 22/25 | Loss: 0.00088593
Iteration 23/25 | Loss: 0.00088593
Iteration 24/25 | Loss: 0.00088593
Iteration 25/25 | Loss: 0.00088592

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39424074
Iteration 2/25 | Loss: 0.00075723
Iteration 3/25 | Loss: 0.00075716
Iteration 4/25 | Loss: 0.00075716
Iteration 5/25 | Loss: 0.00075716
Iteration 6/25 | Loss: 0.00075716
Iteration 7/25 | Loss: 0.00075716
Iteration 8/25 | Loss: 0.00075716
Iteration 9/25 | Loss: 0.00075716
Iteration 10/25 | Loss: 0.00075716
Iteration 11/25 | Loss: 0.00075716
Iteration 12/25 | Loss: 0.00075716
Iteration 13/25 | Loss: 0.00075716
Iteration 14/25 | Loss: 0.00075716
Iteration 15/25 | Loss: 0.00075716
Iteration 16/25 | Loss: 0.00075716
Iteration 17/25 | Loss: 0.00075716
Iteration 18/25 | Loss: 0.00075716
Iteration 19/25 | Loss: 0.00075716
Iteration 20/25 | Loss: 0.00075716
Iteration 21/25 | Loss: 0.00075716
Iteration 22/25 | Loss: 0.00075716
Iteration 23/25 | Loss: 0.00075716
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007571596652269363, 0.0007571596652269363, 0.0007571596652269363, 0.0007571596652269363, 0.0007571596652269363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007571596652269363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075716
Iteration 2/1000 | Loss: 0.00012205
Iteration 3/1000 | Loss: 0.00005462
Iteration 4/1000 | Loss: 0.00003720
Iteration 5/1000 | Loss: 0.00003168
Iteration 6/1000 | Loss: 0.00002963
Iteration 7/1000 | Loss: 0.00002841
Iteration 8/1000 | Loss: 0.00002750
Iteration 9/1000 | Loss: 0.00002677
Iteration 10/1000 | Loss: 0.00002640
Iteration 11/1000 | Loss: 0.00002603
Iteration 12/1000 | Loss: 0.00002578
Iteration 13/1000 | Loss: 0.00002574
Iteration 14/1000 | Loss: 0.00002572
Iteration 15/1000 | Loss: 0.00002551
Iteration 16/1000 | Loss: 0.00002538
Iteration 17/1000 | Loss: 0.00002523
Iteration 18/1000 | Loss: 0.00002522
Iteration 19/1000 | Loss: 0.00002517
Iteration 20/1000 | Loss: 0.00002516
Iteration 21/1000 | Loss: 0.00002515
Iteration 22/1000 | Loss: 0.00002514
Iteration 23/1000 | Loss: 0.00002514
Iteration 24/1000 | Loss: 0.00002513
Iteration 25/1000 | Loss: 0.00002513
Iteration 26/1000 | Loss: 0.00002511
Iteration 27/1000 | Loss: 0.00002506
Iteration 28/1000 | Loss: 0.00002506
Iteration 29/1000 | Loss: 0.00002503
Iteration 30/1000 | Loss: 0.00002503
Iteration 31/1000 | Loss: 0.00002503
Iteration 32/1000 | Loss: 0.00002499
Iteration 33/1000 | Loss: 0.00002497
Iteration 34/1000 | Loss: 0.00002497
Iteration 35/1000 | Loss: 0.00002496
Iteration 36/1000 | Loss: 0.00002496
Iteration 37/1000 | Loss: 0.00002495
Iteration 38/1000 | Loss: 0.00002494
Iteration 39/1000 | Loss: 0.00002494
Iteration 40/1000 | Loss: 0.00002494
Iteration 41/1000 | Loss: 0.00002493
Iteration 42/1000 | Loss: 0.00002493
Iteration 43/1000 | Loss: 0.00002493
Iteration 44/1000 | Loss: 0.00002492
Iteration 45/1000 | Loss: 0.00002492
Iteration 46/1000 | Loss: 0.00002491
Iteration 47/1000 | Loss: 0.00002491
Iteration 48/1000 | Loss: 0.00002490
Iteration 49/1000 | Loss: 0.00002490
Iteration 50/1000 | Loss: 0.00002489
Iteration 51/1000 | Loss: 0.00002489
Iteration 52/1000 | Loss: 0.00002488
Iteration 53/1000 | Loss: 0.00002488
Iteration 54/1000 | Loss: 0.00002488
Iteration 55/1000 | Loss: 0.00002488
Iteration 56/1000 | Loss: 0.00002488
Iteration 57/1000 | Loss: 0.00002488
Iteration 58/1000 | Loss: 0.00002487
Iteration 59/1000 | Loss: 0.00002487
Iteration 60/1000 | Loss: 0.00002487
Iteration 61/1000 | Loss: 0.00002486
Iteration 62/1000 | Loss: 0.00002486
Iteration 63/1000 | Loss: 0.00002485
Iteration 64/1000 | Loss: 0.00002485
Iteration 65/1000 | Loss: 0.00002485
Iteration 66/1000 | Loss: 0.00002484
Iteration 67/1000 | Loss: 0.00002484
Iteration 68/1000 | Loss: 0.00002484
Iteration 69/1000 | Loss: 0.00002483
Iteration 70/1000 | Loss: 0.00002483
Iteration 71/1000 | Loss: 0.00002483
Iteration 72/1000 | Loss: 0.00002481
Iteration 73/1000 | Loss: 0.00002481
Iteration 74/1000 | Loss: 0.00002481
Iteration 75/1000 | Loss: 0.00002480
Iteration 76/1000 | Loss: 0.00002480
Iteration 77/1000 | Loss: 0.00002479
Iteration 78/1000 | Loss: 0.00002479
Iteration 79/1000 | Loss: 0.00002478
Iteration 80/1000 | Loss: 0.00002478
Iteration 81/1000 | Loss: 0.00002478
Iteration 82/1000 | Loss: 0.00002478
Iteration 83/1000 | Loss: 0.00002477
Iteration 84/1000 | Loss: 0.00002477
Iteration 85/1000 | Loss: 0.00002477
Iteration 86/1000 | Loss: 0.00002477
Iteration 87/1000 | Loss: 0.00002477
Iteration 88/1000 | Loss: 0.00002477
Iteration 89/1000 | Loss: 0.00002476
Iteration 90/1000 | Loss: 0.00002476
Iteration 91/1000 | Loss: 0.00002476
Iteration 92/1000 | Loss: 0.00002475
Iteration 93/1000 | Loss: 0.00002475
Iteration 94/1000 | Loss: 0.00002475
Iteration 95/1000 | Loss: 0.00002474
Iteration 96/1000 | Loss: 0.00002474
Iteration 97/1000 | Loss: 0.00002474
Iteration 98/1000 | Loss: 0.00002473
Iteration 99/1000 | Loss: 0.00002473
Iteration 100/1000 | Loss: 0.00002473
Iteration 101/1000 | Loss: 0.00002473
Iteration 102/1000 | Loss: 0.00002473
Iteration 103/1000 | Loss: 0.00002473
Iteration 104/1000 | Loss: 0.00002473
Iteration 105/1000 | Loss: 0.00002473
Iteration 106/1000 | Loss: 0.00002473
Iteration 107/1000 | Loss: 0.00002472
Iteration 108/1000 | Loss: 0.00002472
Iteration 109/1000 | Loss: 0.00002472
Iteration 110/1000 | Loss: 0.00002472
Iteration 111/1000 | Loss: 0.00002472
Iteration 112/1000 | Loss: 0.00002472
Iteration 113/1000 | Loss: 0.00002472
Iteration 114/1000 | Loss: 0.00002472
Iteration 115/1000 | Loss: 0.00002472
Iteration 116/1000 | Loss: 0.00002471
Iteration 117/1000 | Loss: 0.00002471
Iteration 118/1000 | Loss: 0.00002471
Iteration 119/1000 | Loss: 0.00002471
Iteration 120/1000 | Loss: 0.00002471
Iteration 121/1000 | Loss: 0.00002471
Iteration 122/1000 | Loss: 0.00002471
Iteration 123/1000 | Loss: 0.00002471
Iteration 124/1000 | Loss: 0.00002471
Iteration 125/1000 | Loss: 0.00002471
Iteration 126/1000 | Loss: 0.00002471
Iteration 127/1000 | Loss: 0.00002470
Iteration 128/1000 | Loss: 0.00002470
Iteration 129/1000 | Loss: 0.00002470
Iteration 130/1000 | Loss: 0.00002470
Iteration 131/1000 | Loss: 0.00002470
Iteration 132/1000 | Loss: 0.00002470
Iteration 133/1000 | Loss: 0.00002470
Iteration 134/1000 | Loss: 0.00002470
Iteration 135/1000 | Loss: 0.00002470
Iteration 136/1000 | Loss: 0.00002470
Iteration 137/1000 | Loss: 0.00002470
Iteration 138/1000 | Loss: 0.00002470
Iteration 139/1000 | Loss: 0.00002470
Iteration 140/1000 | Loss: 0.00002470
Iteration 141/1000 | Loss: 0.00002469
Iteration 142/1000 | Loss: 0.00002469
Iteration 143/1000 | Loss: 0.00002469
Iteration 144/1000 | Loss: 0.00002469
Iteration 145/1000 | Loss: 0.00002469
Iteration 146/1000 | Loss: 0.00002469
Iteration 147/1000 | Loss: 0.00002469
Iteration 148/1000 | Loss: 0.00002469
Iteration 149/1000 | Loss: 0.00002469
Iteration 150/1000 | Loss: 0.00002469
Iteration 151/1000 | Loss: 0.00002468
Iteration 152/1000 | Loss: 0.00002468
Iteration 153/1000 | Loss: 0.00002468
Iteration 154/1000 | Loss: 0.00002468
Iteration 155/1000 | Loss: 0.00002468
Iteration 156/1000 | Loss: 0.00002468
Iteration 157/1000 | Loss: 0.00002468
Iteration 158/1000 | Loss: 0.00002468
Iteration 159/1000 | Loss: 0.00002468
Iteration 160/1000 | Loss: 0.00002468
Iteration 161/1000 | Loss: 0.00002468
Iteration 162/1000 | Loss: 0.00002468
Iteration 163/1000 | Loss: 0.00002468
Iteration 164/1000 | Loss: 0.00002467
Iteration 165/1000 | Loss: 0.00002467
Iteration 166/1000 | Loss: 0.00002467
Iteration 167/1000 | Loss: 0.00002467
Iteration 168/1000 | Loss: 0.00002467
Iteration 169/1000 | Loss: 0.00002467
Iteration 170/1000 | Loss: 0.00002467
Iteration 171/1000 | Loss: 0.00002467
Iteration 172/1000 | Loss: 0.00002467
Iteration 173/1000 | Loss: 0.00002467
Iteration 174/1000 | Loss: 0.00002466
Iteration 175/1000 | Loss: 0.00002466
Iteration 176/1000 | Loss: 0.00002466
Iteration 177/1000 | Loss: 0.00002466
Iteration 178/1000 | Loss: 0.00002466
Iteration 179/1000 | Loss: 0.00002466
Iteration 180/1000 | Loss: 0.00002466
Iteration 181/1000 | Loss: 0.00002466
Iteration 182/1000 | Loss: 0.00002466
Iteration 183/1000 | Loss: 0.00002466
Iteration 184/1000 | Loss: 0.00002466
Iteration 185/1000 | Loss: 0.00002466
Iteration 186/1000 | Loss: 0.00002466
Iteration 187/1000 | Loss: 0.00002466
Iteration 188/1000 | Loss: 0.00002466
Iteration 189/1000 | Loss: 0.00002466
Iteration 190/1000 | Loss: 0.00002466
Iteration 191/1000 | Loss: 0.00002465
Iteration 192/1000 | Loss: 0.00002465
Iteration 193/1000 | Loss: 0.00002465
Iteration 194/1000 | Loss: 0.00002465
Iteration 195/1000 | Loss: 0.00002465
Iteration 196/1000 | Loss: 0.00002465
Iteration 197/1000 | Loss: 0.00002465
Iteration 198/1000 | Loss: 0.00002465
Iteration 199/1000 | Loss: 0.00002465
Iteration 200/1000 | Loss: 0.00002465
Iteration 201/1000 | Loss: 0.00002465
Iteration 202/1000 | Loss: 0.00002465
Iteration 203/1000 | Loss: 0.00002465
Iteration 204/1000 | Loss: 0.00002465
Iteration 205/1000 | Loss: 0.00002465
Iteration 206/1000 | Loss: 0.00002465
Iteration 207/1000 | Loss: 0.00002465
Iteration 208/1000 | Loss: 0.00002465
Iteration 209/1000 | Loss: 0.00002464
Iteration 210/1000 | Loss: 0.00002464
Iteration 211/1000 | Loss: 0.00002464
Iteration 212/1000 | Loss: 0.00002464
Iteration 213/1000 | Loss: 0.00002464
Iteration 214/1000 | Loss: 0.00002464
Iteration 215/1000 | Loss: 0.00002464
Iteration 216/1000 | Loss: 0.00002464
Iteration 217/1000 | Loss: 0.00002464
Iteration 218/1000 | Loss: 0.00002463
Iteration 219/1000 | Loss: 0.00002463
Iteration 220/1000 | Loss: 0.00002463
Iteration 221/1000 | Loss: 0.00002463
Iteration 222/1000 | Loss: 0.00002463
Iteration 223/1000 | Loss: 0.00002463
Iteration 224/1000 | Loss: 0.00002463
Iteration 225/1000 | Loss: 0.00002463
Iteration 226/1000 | Loss: 0.00002463
Iteration 227/1000 | Loss: 0.00002463
Iteration 228/1000 | Loss: 0.00002462
Iteration 229/1000 | Loss: 0.00002462
Iteration 230/1000 | Loss: 0.00002462
Iteration 231/1000 | Loss: 0.00002462
Iteration 232/1000 | Loss: 0.00002462
Iteration 233/1000 | Loss: 0.00002462
Iteration 234/1000 | Loss: 0.00002462
Iteration 235/1000 | Loss: 0.00002462
Iteration 236/1000 | Loss: 0.00002462
Iteration 237/1000 | Loss: 0.00002462
Iteration 238/1000 | Loss: 0.00002462
Iteration 239/1000 | Loss: 0.00002461
Iteration 240/1000 | Loss: 0.00002461
Iteration 241/1000 | Loss: 0.00002461
Iteration 242/1000 | Loss: 0.00002461
Iteration 243/1000 | Loss: 0.00002461
Iteration 244/1000 | Loss: 0.00002461
Iteration 245/1000 | Loss: 0.00002461
Iteration 246/1000 | Loss: 0.00002461
Iteration 247/1000 | Loss: 0.00002461
Iteration 248/1000 | Loss: 0.00002461
Iteration 249/1000 | Loss: 0.00002461
Iteration 250/1000 | Loss: 0.00002461
Iteration 251/1000 | Loss: 0.00002461
Iteration 252/1000 | Loss: 0.00002461
Iteration 253/1000 | Loss: 0.00002461
Iteration 254/1000 | Loss: 0.00002461
Iteration 255/1000 | Loss: 0.00002461
Iteration 256/1000 | Loss: 0.00002461
Iteration 257/1000 | Loss: 0.00002461
Iteration 258/1000 | Loss: 0.00002460
Iteration 259/1000 | Loss: 0.00002460
Iteration 260/1000 | Loss: 0.00002460
Iteration 261/1000 | Loss: 0.00002460
Iteration 262/1000 | Loss: 0.00002460
Iteration 263/1000 | Loss: 0.00002460
Iteration 264/1000 | Loss: 0.00002460
Iteration 265/1000 | Loss: 0.00002460
Iteration 266/1000 | Loss: 0.00002460
Iteration 267/1000 | Loss: 0.00002460
Iteration 268/1000 | Loss: 0.00002460
Iteration 269/1000 | Loss: 0.00002460
Iteration 270/1000 | Loss: 0.00002459
Iteration 271/1000 | Loss: 0.00002459
Iteration 272/1000 | Loss: 0.00002459
Iteration 273/1000 | Loss: 0.00002459
Iteration 274/1000 | Loss: 0.00002459
Iteration 275/1000 | Loss: 0.00002459
Iteration 276/1000 | Loss: 0.00002459
Iteration 277/1000 | Loss: 0.00002459
Iteration 278/1000 | Loss: 0.00002459
Iteration 279/1000 | Loss: 0.00002459
Iteration 280/1000 | Loss: 0.00002459
Iteration 281/1000 | Loss: 0.00002459
Iteration 282/1000 | Loss: 0.00002459
Iteration 283/1000 | Loss: 0.00002459
Iteration 284/1000 | Loss: 0.00002459
Iteration 285/1000 | Loss: 0.00002459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [2.458931521687191e-05, 2.458931521687191e-05, 2.458931521687191e-05, 2.458931521687191e-05, 2.458931521687191e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.458931521687191e-05

Optimization complete. Final v2v error: 3.7595672607421875 mm

Highest mean error: 5.699304580688477 mm for frame 118

Lowest mean error: 2.832249879837036 mm for frame 135

Saving results

Total time: 68.60271263122559
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093426
Iteration 2/25 | Loss: 0.00151012
Iteration 3/25 | Loss: 0.00099568
Iteration 4/25 | Loss: 0.00088661
Iteration 5/25 | Loss: 0.00085579
Iteration 6/25 | Loss: 0.00084916
Iteration 7/25 | Loss: 0.00081951
Iteration 8/25 | Loss: 0.00081989
Iteration 9/25 | Loss: 0.00080569
Iteration 10/25 | Loss: 0.00080203
Iteration 11/25 | Loss: 0.00079945
Iteration 12/25 | Loss: 0.00079690
Iteration 13/25 | Loss: 0.00079596
Iteration 14/25 | Loss: 0.00079507
Iteration 15/25 | Loss: 0.00079470
Iteration 16/25 | Loss: 0.00079464
Iteration 17/25 | Loss: 0.00079464
Iteration 18/25 | Loss: 0.00079464
Iteration 19/25 | Loss: 0.00079463
Iteration 20/25 | Loss: 0.00079463
Iteration 21/25 | Loss: 0.00079463
Iteration 22/25 | Loss: 0.00079463
Iteration 23/25 | Loss: 0.00079463
Iteration 24/25 | Loss: 0.00079463
Iteration 25/25 | Loss: 0.00079463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48373282
Iteration 2/25 | Loss: 0.00046669
Iteration 3/25 | Loss: 0.00046668
Iteration 4/25 | Loss: 0.00046668
Iteration 5/25 | Loss: 0.00046668
Iteration 6/25 | Loss: 0.00046668
Iteration 7/25 | Loss: 0.00046668
Iteration 8/25 | Loss: 0.00046668
Iteration 9/25 | Loss: 0.00046668
Iteration 10/25 | Loss: 0.00046668
Iteration 11/25 | Loss: 0.00046668
Iteration 12/25 | Loss: 0.00046668
Iteration 13/25 | Loss: 0.00046668
Iteration 14/25 | Loss: 0.00046668
Iteration 15/25 | Loss: 0.00046668
Iteration 16/25 | Loss: 0.00046668
Iteration 17/25 | Loss: 0.00046668
Iteration 18/25 | Loss: 0.00046668
Iteration 19/25 | Loss: 0.00046668
Iteration 20/25 | Loss: 0.00046668
Iteration 21/25 | Loss: 0.00046668
Iteration 22/25 | Loss: 0.00046668
Iteration 23/25 | Loss: 0.00046668
Iteration 24/25 | Loss: 0.00046668
Iteration 25/25 | Loss: 0.00046668

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046668
Iteration 2/1000 | Loss: 0.00003244
Iteration 3/1000 | Loss: 0.00007454
Iteration 4/1000 | Loss: 0.00002800
Iteration 5/1000 | Loss: 0.00002173
Iteration 6/1000 | Loss: 0.00001972
Iteration 7/1000 | Loss: 0.00014504
Iteration 8/1000 | Loss: 0.00001924
Iteration 9/1000 | Loss: 0.00001847
Iteration 10/1000 | Loss: 0.00001805
Iteration 11/1000 | Loss: 0.00001781
Iteration 12/1000 | Loss: 0.00001761
Iteration 13/1000 | Loss: 0.00001741
Iteration 14/1000 | Loss: 0.00001725
Iteration 15/1000 | Loss: 0.00001722
Iteration 16/1000 | Loss: 0.00001721
Iteration 17/1000 | Loss: 0.00001716
Iteration 18/1000 | Loss: 0.00001713
Iteration 19/1000 | Loss: 0.00001709
Iteration 20/1000 | Loss: 0.00001709
Iteration 21/1000 | Loss: 0.00001708
Iteration 22/1000 | Loss: 0.00001707
Iteration 23/1000 | Loss: 0.00001701
Iteration 24/1000 | Loss: 0.00001701
Iteration 25/1000 | Loss: 0.00001699
Iteration 26/1000 | Loss: 0.00001699
Iteration 27/1000 | Loss: 0.00001698
Iteration 28/1000 | Loss: 0.00007565
Iteration 29/1000 | Loss: 0.00001696
Iteration 30/1000 | Loss: 0.00001690
Iteration 31/1000 | Loss: 0.00001690
Iteration 32/1000 | Loss: 0.00001689
Iteration 33/1000 | Loss: 0.00001686
Iteration 34/1000 | Loss: 0.00001684
Iteration 35/1000 | Loss: 0.00001684
Iteration 36/1000 | Loss: 0.00001683
Iteration 37/1000 | Loss: 0.00001683
Iteration 38/1000 | Loss: 0.00001683
Iteration 39/1000 | Loss: 0.00001682
Iteration 40/1000 | Loss: 0.00001681
Iteration 41/1000 | Loss: 0.00001679
Iteration 42/1000 | Loss: 0.00001679
Iteration 43/1000 | Loss: 0.00001679
Iteration 44/1000 | Loss: 0.00001679
Iteration 45/1000 | Loss: 0.00001679
Iteration 46/1000 | Loss: 0.00001679
Iteration 47/1000 | Loss: 0.00001679
Iteration 48/1000 | Loss: 0.00001679
Iteration 49/1000 | Loss: 0.00001679
Iteration 50/1000 | Loss: 0.00001679
Iteration 51/1000 | Loss: 0.00001678
Iteration 52/1000 | Loss: 0.00001678
Iteration 53/1000 | Loss: 0.00001677
Iteration 54/1000 | Loss: 0.00001676
Iteration 55/1000 | Loss: 0.00001676
Iteration 56/1000 | Loss: 0.00001676
Iteration 57/1000 | Loss: 0.00001676
Iteration 58/1000 | Loss: 0.00001676
Iteration 59/1000 | Loss: 0.00001676
Iteration 60/1000 | Loss: 0.00001676
Iteration 61/1000 | Loss: 0.00001676
Iteration 62/1000 | Loss: 0.00001675
Iteration 63/1000 | Loss: 0.00001675
Iteration 64/1000 | Loss: 0.00001674
Iteration 65/1000 | Loss: 0.00001674
Iteration 66/1000 | Loss: 0.00001674
Iteration 67/1000 | Loss: 0.00001673
Iteration 68/1000 | Loss: 0.00001673
Iteration 69/1000 | Loss: 0.00001672
Iteration 70/1000 | Loss: 0.00001672
Iteration 71/1000 | Loss: 0.00001672
Iteration 72/1000 | Loss: 0.00001672
Iteration 73/1000 | Loss: 0.00001672
Iteration 74/1000 | Loss: 0.00001672
Iteration 75/1000 | Loss: 0.00001672
Iteration 76/1000 | Loss: 0.00001672
Iteration 77/1000 | Loss: 0.00001672
Iteration 78/1000 | Loss: 0.00001672
Iteration 79/1000 | Loss: 0.00001671
Iteration 80/1000 | Loss: 0.00001671
Iteration 81/1000 | Loss: 0.00001671
Iteration 82/1000 | Loss: 0.00001670
Iteration 83/1000 | Loss: 0.00001670
Iteration 84/1000 | Loss: 0.00001670
Iteration 85/1000 | Loss: 0.00001670
Iteration 86/1000 | Loss: 0.00001669
Iteration 87/1000 | Loss: 0.00001669
Iteration 88/1000 | Loss: 0.00001669
Iteration 89/1000 | Loss: 0.00001669
Iteration 90/1000 | Loss: 0.00001669
Iteration 91/1000 | Loss: 0.00001669
Iteration 92/1000 | Loss: 0.00001669
Iteration 93/1000 | Loss: 0.00001669
Iteration 94/1000 | Loss: 0.00001669
Iteration 95/1000 | Loss: 0.00001669
Iteration 96/1000 | Loss: 0.00001668
Iteration 97/1000 | Loss: 0.00001668
Iteration 98/1000 | Loss: 0.00001668
Iteration 99/1000 | Loss: 0.00001668
Iteration 100/1000 | Loss: 0.00001668
Iteration 101/1000 | Loss: 0.00001668
Iteration 102/1000 | Loss: 0.00001667
Iteration 103/1000 | Loss: 0.00001667
Iteration 104/1000 | Loss: 0.00001667
Iteration 105/1000 | Loss: 0.00001667
Iteration 106/1000 | Loss: 0.00001667
Iteration 107/1000 | Loss: 0.00001666
Iteration 108/1000 | Loss: 0.00001666
Iteration 109/1000 | Loss: 0.00001666
Iteration 110/1000 | Loss: 0.00001666
Iteration 111/1000 | Loss: 0.00001666
Iteration 112/1000 | Loss: 0.00001666
Iteration 113/1000 | Loss: 0.00001666
Iteration 114/1000 | Loss: 0.00001666
Iteration 115/1000 | Loss: 0.00001666
Iteration 116/1000 | Loss: 0.00001666
Iteration 117/1000 | Loss: 0.00001666
Iteration 118/1000 | Loss: 0.00001666
Iteration 119/1000 | Loss: 0.00001666
Iteration 120/1000 | Loss: 0.00001666
Iteration 121/1000 | Loss: 0.00001666
Iteration 122/1000 | Loss: 0.00001666
Iteration 123/1000 | Loss: 0.00001666
Iteration 124/1000 | Loss: 0.00001666
Iteration 125/1000 | Loss: 0.00001666
Iteration 126/1000 | Loss: 0.00001666
Iteration 127/1000 | Loss: 0.00001666
Iteration 128/1000 | Loss: 0.00001665
Iteration 129/1000 | Loss: 0.00001665
Iteration 130/1000 | Loss: 0.00001665
Iteration 131/1000 | Loss: 0.00001665
Iteration 132/1000 | Loss: 0.00001665
Iteration 133/1000 | Loss: 0.00001665
Iteration 134/1000 | Loss: 0.00001665
Iteration 135/1000 | Loss: 0.00001665
Iteration 136/1000 | Loss: 0.00001665
Iteration 137/1000 | Loss: 0.00001665
Iteration 138/1000 | Loss: 0.00001665
Iteration 139/1000 | Loss: 0.00001665
Iteration 140/1000 | Loss: 0.00001665
Iteration 141/1000 | Loss: 0.00001665
Iteration 142/1000 | Loss: 0.00001665
Iteration 143/1000 | Loss: 0.00001665
Iteration 144/1000 | Loss: 0.00001665
Iteration 145/1000 | Loss: 0.00001665
Iteration 146/1000 | Loss: 0.00001665
Iteration 147/1000 | Loss: 0.00001665
Iteration 148/1000 | Loss: 0.00001664
Iteration 149/1000 | Loss: 0.00001664
Iteration 150/1000 | Loss: 0.00001664
Iteration 151/1000 | Loss: 0.00001664
Iteration 152/1000 | Loss: 0.00001664
Iteration 153/1000 | Loss: 0.00001664
Iteration 154/1000 | Loss: 0.00001664
Iteration 155/1000 | Loss: 0.00001664
Iteration 156/1000 | Loss: 0.00001664
Iteration 157/1000 | Loss: 0.00001664
Iteration 158/1000 | Loss: 0.00001664
Iteration 159/1000 | Loss: 0.00001664
Iteration 160/1000 | Loss: 0.00001663
Iteration 161/1000 | Loss: 0.00001663
Iteration 162/1000 | Loss: 0.00001663
Iteration 163/1000 | Loss: 0.00001663
Iteration 164/1000 | Loss: 0.00001663
Iteration 165/1000 | Loss: 0.00001663
Iteration 166/1000 | Loss: 0.00001663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.66344743774971e-05, 1.66344743774971e-05, 1.66344743774971e-05, 1.66344743774971e-05, 1.66344743774971e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.66344743774971e-05

Optimization complete. Final v2v error: 3.4654414653778076 mm

Highest mean error: 4.090580463409424 mm for frame 20

Lowest mean error: 3.2719664573669434 mm for frame 101

Saving results

Total time: 62.49619793891907
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795269
Iteration 2/25 | Loss: 0.00161474
Iteration 3/25 | Loss: 0.00104518
Iteration 4/25 | Loss: 0.00088571
Iteration 5/25 | Loss: 0.00084621
Iteration 6/25 | Loss: 0.00080920
Iteration 7/25 | Loss: 0.00079848
Iteration 8/25 | Loss: 0.00079144
Iteration 9/25 | Loss: 0.00078995
Iteration 10/25 | Loss: 0.00078968
Iteration 11/25 | Loss: 0.00078956
Iteration 12/25 | Loss: 0.00078955
Iteration 13/25 | Loss: 0.00078955
Iteration 14/25 | Loss: 0.00078955
Iteration 15/25 | Loss: 0.00078955
Iteration 16/25 | Loss: 0.00078955
Iteration 17/25 | Loss: 0.00078955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007895496091805398, 0.0007895496091805398, 0.0007895496091805398, 0.0007895496091805398, 0.0007895496091805398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007895496091805398

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45201147
Iteration 2/25 | Loss: 0.00053007
Iteration 3/25 | Loss: 0.00053005
Iteration 4/25 | Loss: 0.00053005
Iteration 5/25 | Loss: 0.00053005
Iteration 6/25 | Loss: 0.00053005
Iteration 7/25 | Loss: 0.00053005
Iteration 8/25 | Loss: 0.00053005
Iteration 9/25 | Loss: 0.00053005
Iteration 10/25 | Loss: 0.00053005
Iteration 11/25 | Loss: 0.00053005
Iteration 12/25 | Loss: 0.00053005
Iteration 13/25 | Loss: 0.00053005
Iteration 14/25 | Loss: 0.00053005
Iteration 15/25 | Loss: 0.00053005
Iteration 16/25 | Loss: 0.00053005
Iteration 17/25 | Loss: 0.00053005
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005300482735037804, 0.0005300482735037804, 0.0005300482735037804, 0.0005300482735037804, 0.0005300482735037804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005300482735037804

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053005
Iteration 2/1000 | Loss: 0.00002506
Iteration 3/1000 | Loss: 0.00001830
Iteration 4/1000 | Loss: 0.00001707
Iteration 5/1000 | Loss: 0.00001630
Iteration 6/1000 | Loss: 0.00001578
Iteration 7/1000 | Loss: 0.00001553
Iteration 8/1000 | Loss: 0.00001528
Iteration 9/1000 | Loss: 0.00001513
Iteration 10/1000 | Loss: 0.00001510
Iteration 11/1000 | Loss: 0.00001507
Iteration 12/1000 | Loss: 0.00001506
Iteration 13/1000 | Loss: 0.00001504
Iteration 14/1000 | Loss: 0.00001503
Iteration 15/1000 | Loss: 0.00001498
Iteration 16/1000 | Loss: 0.00001496
Iteration 17/1000 | Loss: 0.00001495
Iteration 18/1000 | Loss: 0.00001495
Iteration 19/1000 | Loss: 0.00001494
Iteration 20/1000 | Loss: 0.00001494
Iteration 21/1000 | Loss: 0.00001493
Iteration 22/1000 | Loss: 0.00001490
Iteration 23/1000 | Loss: 0.00001489
Iteration 24/1000 | Loss: 0.00001486
Iteration 25/1000 | Loss: 0.00001484
Iteration 26/1000 | Loss: 0.00001482
Iteration 27/1000 | Loss: 0.00001481
Iteration 28/1000 | Loss: 0.00001481
Iteration 29/1000 | Loss: 0.00001481
Iteration 30/1000 | Loss: 0.00001481
Iteration 31/1000 | Loss: 0.00001480
Iteration 32/1000 | Loss: 0.00001480
Iteration 33/1000 | Loss: 0.00001480
Iteration 34/1000 | Loss: 0.00001479
Iteration 35/1000 | Loss: 0.00001479
Iteration 36/1000 | Loss: 0.00001478
Iteration 37/1000 | Loss: 0.00001478
Iteration 38/1000 | Loss: 0.00001478
Iteration 39/1000 | Loss: 0.00001478
Iteration 40/1000 | Loss: 0.00001477
Iteration 41/1000 | Loss: 0.00001477
Iteration 42/1000 | Loss: 0.00001477
Iteration 43/1000 | Loss: 0.00001477
Iteration 44/1000 | Loss: 0.00001477
Iteration 45/1000 | Loss: 0.00001477
Iteration 46/1000 | Loss: 0.00001477
Iteration 47/1000 | Loss: 0.00001477
Iteration 48/1000 | Loss: 0.00001476
Iteration 49/1000 | Loss: 0.00001476
Iteration 50/1000 | Loss: 0.00001476
Iteration 51/1000 | Loss: 0.00001475
Iteration 52/1000 | Loss: 0.00001475
Iteration 53/1000 | Loss: 0.00001475
Iteration 54/1000 | Loss: 0.00001475
Iteration 55/1000 | Loss: 0.00001475
Iteration 56/1000 | Loss: 0.00001475
Iteration 57/1000 | Loss: 0.00001475
Iteration 58/1000 | Loss: 0.00001475
Iteration 59/1000 | Loss: 0.00001475
Iteration 60/1000 | Loss: 0.00001475
Iteration 61/1000 | Loss: 0.00001474
Iteration 62/1000 | Loss: 0.00001474
Iteration 63/1000 | Loss: 0.00001474
Iteration 64/1000 | Loss: 0.00001474
Iteration 65/1000 | Loss: 0.00001474
Iteration 66/1000 | Loss: 0.00001474
Iteration 67/1000 | Loss: 0.00001474
Iteration 68/1000 | Loss: 0.00001474
Iteration 69/1000 | Loss: 0.00001474
Iteration 70/1000 | Loss: 0.00001473
Iteration 71/1000 | Loss: 0.00001473
Iteration 72/1000 | Loss: 0.00001473
Iteration 73/1000 | Loss: 0.00001473
Iteration 74/1000 | Loss: 0.00001472
Iteration 75/1000 | Loss: 0.00001472
Iteration 76/1000 | Loss: 0.00001472
Iteration 77/1000 | Loss: 0.00001472
Iteration 78/1000 | Loss: 0.00001472
Iteration 79/1000 | Loss: 0.00001472
Iteration 80/1000 | Loss: 0.00001472
Iteration 81/1000 | Loss: 0.00001472
Iteration 82/1000 | Loss: 0.00001472
Iteration 83/1000 | Loss: 0.00001471
Iteration 84/1000 | Loss: 0.00001471
Iteration 85/1000 | Loss: 0.00001471
Iteration 86/1000 | Loss: 0.00001471
Iteration 87/1000 | Loss: 0.00001471
Iteration 88/1000 | Loss: 0.00001471
Iteration 89/1000 | Loss: 0.00001471
Iteration 90/1000 | Loss: 0.00001470
Iteration 91/1000 | Loss: 0.00001470
Iteration 92/1000 | Loss: 0.00001470
Iteration 93/1000 | Loss: 0.00001470
Iteration 94/1000 | Loss: 0.00001470
Iteration 95/1000 | Loss: 0.00001470
Iteration 96/1000 | Loss: 0.00001470
Iteration 97/1000 | Loss: 0.00001470
Iteration 98/1000 | Loss: 0.00001470
Iteration 99/1000 | Loss: 0.00001469
Iteration 100/1000 | Loss: 0.00001469
Iteration 101/1000 | Loss: 0.00001468
Iteration 102/1000 | Loss: 0.00001468
Iteration 103/1000 | Loss: 0.00001468
Iteration 104/1000 | Loss: 0.00001467
Iteration 105/1000 | Loss: 0.00001467
Iteration 106/1000 | Loss: 0.00001467
Iteration 107/1000 | Loss: 0.00001467
Iteration 108/1000 | Loss: 0.00001467
Iteration 109/1000 | Loss: 0.00001467
Iteration 110/1000 | Loss: 0.00001467
Iteration 111/1000 | Loss: 0.00001466
Iteration 112/1000 | Loss: 0.00001466
Iteration 113/1000 | Loss: 0.00001466
Iteration 114/1000 | Loss: 0.00001466
Iteration 115/1000 | Loss: 0.00001466
Iteration 116/1000 | Loss: 0.00001466
Iteration 117/1000 | Loss: 0.00001466
Iteration 118/1000 | Loss: 0.00001466
Iteration 119/1000 | Loss: 0.00001466
Iteration 120/1000 | Loss: 0.00001465
Iteration 121/1000 | Loss: 0.00001465
Iteration 122/1000 | Loss: 0.00001465
Iteration 123/1000 | Loss: 0.00001465
Iteration 124/1000 | Loss: 0.00001465
Iteration 125/1000 | Loss: 0.00001465
Iteration 126/1000 | Loss: 0.00001465
Iteration 127/1000 | Loss: 0.00001465
Iteration 128/1000 | Loss: 0.00001465
Iteration 129/1000 | Loss: 0.00001465
Iteration 130/1000 | Loss: 0.00001465
Iteration 131/1000 | Loss: 0.00001465
Iteration 132/1000 | Loss: 0.00001465
Iteration 133/1000 | Loss: 0.00001465
Iteration 134/1000 | Loss: 0.00001465
Iteration 135/1000 | Loss: 0.00001465
Iteration 136/1000 | Loss: 0.00001465
Iteration 137/1000 | Loss: 0.00001465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.465204513806384e-05, 1.465204513806384e-05, 1.465204513806384e-05, 1.465204513806384e-05, 1.465204513806384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.465204513806384e-05

Optimization complete. Final v2v error: 3.23260760307312 mm

Highest mean error: 3.8738198280334473 mm for frame 182

Lowest mean error: 2.965731382369995 mm for frame 100

Saving results

Total time: 49.044646978378296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00379201
Iteration 2/25 | Loss: 0.00132474
Iteration 3/25 | Loss: 0.00088501
Iteration 4/25 | Loss: 0.00077616
Iteration 5/25 | Loss: 0.00075906
Iteration 6/25 | Loss: 0.00075672
Iteration 7/25 | Loss: 0.00075619
Iteration 8/25 | Loss: 0.00075619
Iteration 9/25 | Loss: 0.00075619
Iteration 10/25 | Loss: 0.00075619
Iteration 11/25 | Loss: 0.00075619
Iteration 12/25 | Loss: 0.00075619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007561860256828368, 0.0007561860256828368, 0.0007561860256828368, 0.0007561860256828368, 0.0007561860256828368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007561860256828368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51879704
Iteration 2/25 | Loss: 0.00048920
Iteration 3/25 | Loss: 0.00048920
Iteration 4/25 | Loss: 0.00048920
Iteration 5/25 | Loss: 0.00048920
Iteration 6/25 | Loss: 0.00048920
Iteration 7/25 | Loss: 0.00048919
Iteration 8/25 | Loss: 0.00048919
Iteration 9/25 | Loss: 0.00048919
Iteration 10/25 | Loss: 0.00048919
Iteration 11/25 | Loss: 0.00048919
Iteration 12/25 | Loss: 0.00048919
Iteration 13/25 | Loss: 0.00048919
Iteration 14/25 | Loss: 0.00048919
Iteration 15/25 | Loss: 0.00048919
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0004891938297078013, 0.0004891938297078013, 0.0004891938297078013, 0.0004891938297078013, 0.0004891938297078013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004891938297078013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048919
Iteration 2/1000 | Loss: 0.00003229
Iteration 3/1000 | Loss: 0.00002095
Iteration 4/1000 | Loss: 0.00001780
Iteration 5/1000 | Loss: 0.00001601
Iteration 6/1000 | Loss: 0.00001514
Iteration 7/1000 | Loss: 0.00001463
Iteration 8/1000 | Loss: 0.00001416
Iteration 9/1000 | Loss: 0.00001389
Iteration 10/1000 | Loss: 0.00001389
Iteration 11/1000 | Loss: 0.00001369
Iteration 12/1000 | Loss: 0.00001348
Iteration 13/1000 | Loss: 0.00001342
Iteration 14/1000 | Loss: 0.00001339
Iteration 15/1000 | Loss: 0.00001338
Iteration 16/1000 | Loss: 0.00001337
Iteration 17/1000 | Loss: 0.00001337
Iteration 18/1000 | Loss: 0.00001336
Iteration 19/1000 | Loss: 0.00001336
Iteration 20/1000 | Loss: 0.00001335
Iteration 21/1000 | Loss: 0.00001335
Iteration 22/1000 | Loss: 0.00001333
Iteration 23/1000 | Loss: 0.00001332
Iteration 24/1000 | Loss: 0.00001330
Iteration 25/1000 | Loss: 0.00001329
Iteration 26/1000 | Loss: 0.00001328
Iteration 27/1000 | Loss: 0.00001327
Iteration 28/1000 | Loss: 0.00001326
Iteration 29/1000 | Loss: 0.00001325
Iteration 30/1000 | Loss: 0.00001325
Iteration 31/1000 | Loss: 0.00001325
Iteration 32/1000 | Loss: 0.00001322
Iteration 33/1000 | Loss: 0.00001320
Iteration 34/1000 | Loss: 0.00001320
Iteration 35/1000 | Loss: 0.00001320
Iteration 36/1000 | Loss: 0.00001319
Iteration 37/1000 | Loss: 0.00001319
Iteration 38/1000 | Loss: 0.00001318
Iteration 39/1000 | Loss: 0.00001318
Iteration 40/1000 | Loss: 0.00001318
Iteration 41/1000 | Loss: 0.00001318
Iteration 42/1000 | Loss: 0.00001318
Iteration 43/1000 | Loss: 0.00001318
Iteration 44/1000 | Loss: 0.00001318
Iteration 45/1000 | Loss: 0.00001317
Iteration 46/1000 | Loss: 0.00001317
Iteration 47/1000 | Loss: 0.00001317
Iteration 48/1000 | Loss: 0.00001317
Iteration 49/1000 | Loss: 0.00001317
Iteration 50/1000 | Loss: 0.00001317
Iteration 51/1000 | Loss: 0.00001317
Iteration 52/1000 | Loss: 0.00001317
Iteration 53/1000 | Loss: 0.00001317
Iteration 54/1000 | Loss: 0.00001316
Iteration 55/1000 | Loss: 0.00001316
Iteration 56/1000 | Loss: 0.00001316
Iteration 57/1000 | Loss: 0.00001315
Iteration 58/1000 | Loss: 0.00001315
Iteration 59/1000 | Loss: 0.00001315
Iteration 60/1000 | Loss: 0.00001315
Iteration 61/1000 | Loss: 0.00001315
Iteration 62/1000 | Loss: 0.00001315
Iteration 63/1000 | Loss: 0.00001315
Iteration 64/1000 | Loss: 0.00001315
Iteration 65/1000 | Loss: 0.00001315
Iteration 66/1000 | Loss: 0.00001314
Iteration 67/1000 | Loss: 0.00001314
Iteration 68/1000 | Loss: 0.00001314
Iteration 69/1000 | Loss: 0.00001314
Iteration 70/1000 | Loss: 0.00001314
Iteration 71/1000 | Loss: 0.00001314
Iteration 72/1000 | Loss: 0.00001314
Iteration 73/1000 | Loss: 0.00001314
Iteration 74/1000 | Loss: 0.00001314
Iteration 75/1000 | Loss: 0.00001313
Iteration 76/1000 | Loss: 0.00001313
Iteration 77/1000 | Loss: 0.00001313
Iteration 78/1000 | Loss: 0.00001313
Iteration 79/1000 | Loss: 0.00001313
Iteration 80/1000 | Loss: 0.00001313
Iteration 81/1000 | Loss: 0.00001313
Iteration 82/1000 | Loss: 0.00001313
Iteration 83/1000 | Loss: 0.00001313
Iteration 84/1000 | Loss: 0.00001312
Iteration 85/1000 | Loss: 0.00001312
Iteration 86/1000 | Loss: 0.00001312
Iteration 87/1000 | Loss: 0.00001312
Iteration 88/1000 | Loss: 0.00001312
Iteration 89/1000 | Loss: 0.00001312
Iteration 90/1000 | Loss: 0.00001312
Iteration 91/1000 | Loss: 0.00001312
Iteration 92/1000 | Loss: 0.00001312
Iteration 93/1000 | Loss: 0.00001312
Iteration 94/1000 | Loss: 0.00001312
Iteration 95/1000 | Loss: 0.00001312
Iteration 96/1000 | Loss: 0.00001312
Iteration 97/1000 | Loss: 0.00001312
Iteration 98/1000 | Loss: 0.00001312
Iteration 99/1000 | Loss: 0.00001312
Iteration 100/1000 | Loss: 0.00001312
Iteration 101/1000 | Loss: 0.00001312
Iteration 102/1000 | Loss: 0.00001312
Iteration 103/1000 | Loss: 0.00001312
Iteration 104/1000 | Loss: 0.00001312
Iteration 105/1000 | Loss: 0.00001312
Iteration 106/1000 | Loss: 0.00001311
Iteration 107/1000 | Loss: 0.00001311
Iteration 108/1000 | Loss: 0.00001311
Iteration 109/1000 | Loss: 0.00001311
Iteration 110/1000 | Loss: 0.00001311
Iteration 111/1000 | Loss: 0.00001311
Iteration 112/1000 | Loss: 0.00001311
Iteration 113/1000 | Loss: 0.00001311
Iteration 114/1000 | Loss: 0.00001311
Iteration 115/1000 | Loss: 0.00001311
Iteration 116/1000 | Loss: 0.00001311
Iteration 117/1000 | Loss: 0.00001311
Iteration 118/1000 | Loss: 0.00001311
Iteration 119/1000 | Loss: 0.00001311
Iteration 120/1000 | Loss: 0.00001311
Iteration 121/1000 | Loss: 0.00001311
Iteration 122/1000 | Loss: 0.00001311
Iteration 123/1000 | Loss: 0.00001311
Iteration 124/1000 | Loss: 0.00001311
Iteration 125/1000 | Loss: 0.00001311
Iteration 126/1000 | Loss: 0.00001311
Iteration 127/1000 | Loss: 0.00001311
Iteration 128/1000 | Loss: 0.00001311
Iteration 129/1000 | Loss: 0.00001311
Iteration 130/1000 | Loss: 0.00001311
Iteration 131/1000 | Loss: 0.00001311
Iteration 132/1000 | Loss: 0.00001311
Iteration 133/1000 | Loss: 0.00001311
Iteration 134/1000 | Loss: 0.00001311
Iteration 135/1000 | Loss: 0.00001311
Iteration 136/1000 | Loss: 0.00001311
Iteration 137/1000 | Loss: 0.00001311
Iteration 138/1000 | Loss: 0.00001311
Iteration 139/1000 | Loss: 0.00001311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.310889365413459e-05, 1.310889365413459e-05, 1.310889365413459e-05, 1.310889365413459e-05, 1.310889365413459e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.310889365413459e-05

Optimization complete. Final v2v error: 3.101027727127075 mm

Highest mean error: 3.4332480430603027 mm for frame 103

Lowest mean error: 2.8903937339782715 mm for frame 24

Saving results

Total time: 35.88829684257507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502079
Iteration 2/25 | Loss: 0.00100184
Iteration 3/25 | Loss: 0.00081750
Iteration 4/25 | Loss: 0.00077987
Iteration 5/25 | Loss: 0.00076982
Iteration 6/25 | Loss: 0.00076755
Iteration 7/25 | Loss: 0.00076689
Iteration 8/25 | Loss: 0.00076689
Iteration 9/25 | Loss: 0.00076689
Iteration 10/25 | Loss: 0.00076689
Iteration 11/25 | Loss: 0.00076689
Iteration 12/25 | Loss: 0.00076689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007668895414099097, 0.0007668895414099097, 0.0007668895414099097, 0.0007668895414099097, 0.0007668895414099097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007668895414099097

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.72307253
Iteration 2/25 | Loss: 0.00047502
Iteration 3/25 | Loss: 0.00047498
Iteration 4/25 | Loss: 0.00047498
Iteration 5/25 | Loss: 0.00047498
Iteration 6/25 | Loss: 0.00047498
Iteration 7/25 | Loss: 0.00047498
Iteration 8/25 | Loss: 0.00047498
Iteration 9/25 | Loss: 0.00047498
Iteration 10/25 | Loss: 0.00047498
Iteration 11/25 | Loss: 0.00047498
Iteration 12/25 | Loss: 0.00047498
Iteration 13/25 | Loss: 0.00047498
Iteration 14/25 | Loss: 0.00047498
Iteration 15/25 | Loss: 0.00047498
Iteration 16/25 | Loss: 0.00047498
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00047498237108811736, 0.00047498237108811736, 0.00047498237108811736, 0.00047498237108811736, 0.00047498237108811736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00047498237108811736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047498
Iteration 2/1000 | Loss: 0.00002479
Iteration 3/1000 | Loss: 0.00001939
Iteration 4/1000 | Loss: 0.00001797
Iteration 5/1000 | Loss: 0.00001697
Iteration 6/1000 | Loss: 0.00001652
Iteration 7/1000 | Loss: 0.00001600
Iteration 8/1000 | Loss: 0.00001570
Iteration 9/1000 | Loss: 0.00001547
Iteration 10/1000 | Loss: 0.00001528
Iteration 11/1000 | Loss: 0.00001524
Iteration 12/1000 | Loss: 0.00001520
Iteration 13/1000 | Loss: 0.00001514
Iteration 14/1000 | Loss: 0.00001510
Iteration 15/1000 | Loss: 0.00001509
Iteration 16/1000 | Loss: 0.00001505
Iteration 17/1000 | Loss: 0.00001502
Iteration 18/1000 | Loss: 0.00001501
Iteration 19/1000 | Loss: 0.00001499
Iteration 20/1000 | Loss: 0.00001499
Iteration 21/1000 | Loss: 0.00001499
Iteration 22/1000 | Loss: 0.00001499
Iteration 23/1000 | Loss: 0.00001499
Iteration 24/1000 | Loss: 0.00001499
Iteration 25/1000 | Loss: 0.00001499
Iteration 26/1000 | Loss: 0.00001499
Iteration 27/1000 | Loss: 0.00001499
Iteration 28/1000 | Loss: 0.00001499
Iteration 29/1000 | Loss: 0.00001499
Iteration 30/1000 | Loss: 0.00001499
Iteration 31/1000 | Loss: 0.00001499
Iteration 32/1000 | Loss: 0.00001499
Iteration 33/1000 | Loss: 0.00001499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 33. Stopping optimization.
Last 5 losses: [1.4993835975474212e-05, 1.4993835975474212e-05, 1.4993835975474212e-05, 1.4993835975474212e-05, 1.4993835975474212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4993835975474212e-05

Optimization complete. Final v2v error: 3.2790074348449707 mm

Highest mean error: 3.6562397480010986 mm for frame 177

Lowest mean error: 2.919252634048462 mm for frame 248

Saving results

Total time: 31.743667602539062
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875976
Iteration 2/25 | Loss: 0.00153241
Iteration 3/25 | Loss: 0.00108782
Iteration 4/25 | Loss: 0.00100561
Iteration 5/25 | Loss: 0.00098460
Iteration 6/25 | Loss: 0.00096146
Iteration 7/25 | Loss: 0.00094386
Iteration 8/25 | Loss: 0.00092711
Iteration 9/25 | Loss: 0.00091551
Iteration 10/25 | Loss: 0.00090657
Iteration 11/25 | Loss: 0.00090388
Iteration 12/25 | Loss: 0.00089725
Iteration 13/25 | Loss: 0.00089547
Iteration 14/25 | Loss: 0.00089081
Iteration 15/25 | Loss: 0.00089028
Iteration 16/25 | Loss: 0.00089398
Iteration 17/25 | Loss: 0.00089177
Iteration 18/25 | Loss: 0.00088907
Iteration 19/25 | Loss: 0.00089024
Iteration 20/25 | Loss: 0.00088801
Iteration 21/25 | Loss: 0.00088709
Iteration 22/25 | Loss: 0.00088850
Iteration 23/25 | Loss: 0.00088652
Iteration 24/25 | Loss: 0.00088481
Iteration 25/25 | Loss: 0.00088481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92772830
Iteration 2/25 | Loss: 0.00094288
Iteration 3/25 | Loss: 0.00094281
Iteration 4/25 | Loss: 0.00094281
Iteration 5/25 | Loss: 0.00094281
Iteration 6/25 | Loss: 0.00094281
Iteration 7/25 | Loss: 0.00094281
Iteration 8/25 | Loss: 0.00094281
Iteration 9/25 | Loss: 0.00094281
Iteration 10/25 | Loss: 0.00094281
Iteration 11/25 | Loss: 0.00094281
Iteration 12/25 | Loss: 0.00094281
Iteration 13/25 | Loss: 0.00094281
Iteration 14/25 | Loss: 0.00094281
Iteration 15/25 | Loss: 0.00094281
Iteration 16/25 | Loss: 0.00094281
Iteration 17/25 | Loss: 0.00094281
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009428081684745848, 0.0009428081684745848, 0.0009428081684745848, 0.0009428081684745848, 0.0009428081684745848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009428081684745848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094281
Iteration 2/1000 | Loss: 0.00023786
Iteration 3/1000 | Loss: 0.00023061
Iteration 4/1000 | Loss: 0.00023888
Iteration 5/1000 | Loss: 0.00012229
Iteration 6/1000 | Loss: 0.00035694
Iteration 7/1000 | Loss: 0.00029873
Iteration 8/1000 | Loss: 0.00021658
Iteration 9/1000 | Loss: 0.00041557
Iteration 10/1000 | Loss: 0.00041625
Iteration 11/1000 | Loss: 0.00044670
Iteration 12/1000 | Loss: 0.00027327
Iteration 13/1000 | Loss: 0.00126014
Iteration 14/1000 | Loss: 0.00045707
Iteration 15/1000 | Loss: 0.00029978
Iteration 16/1000 | Loss: 0.00014224
Iteration 17/1000 | Loss: 0.00105243
Iteration 18/1000 | Loss: 0.00028600
Iteration 19/1000 | Loss: 0.00108917
Iteration 20/1000 | Loss: 0.00101840
Iteration 21/1000 | Loss: 0.00023738
Iteration 22/1000 | Loss: 0.00021481
Iteration 23/1000 | Loss: 0.00018175
Iteration 24/1000 | Loss: 0.00083979
Iteration 25/1000 | Loss: 0.00041128
Iteration 26/1000 | Loss: 0.00094947
Iteration 27/1000 | Loss: 0.00109508
Iteration 28/1000 | Loss: 0.00111330
Iteration 29/1000 | Loss: 0.00138491
Iteration 30/1000 | Loss: 0.00029011
Iteration 31/1000 | Loss: 0.00037790
Iteration 32/1000 | Loss: 0.00011058
Iteration 33/1000 | Loss: 0.00015386
Iteration 34/1000 | Loss: 0.00021685
Iteration 35/1000 | Loss: 0.00017831
Iteration 36/1000 | Loss: 0.00028098
Iteration 37/1000 | Loss: 0.00073986
Iteration 38/1000 | Loss: 0.00021148
Iteration 39/1000 | Loss: 0.00021902
Iteration 40/1000 | Loss: 0.00020372
Iteration 41/1000 | Loss: 0.00023981
Iteration 42/1000 | Loss: 0.00023858
Iteration 43/1000 | Loss: 0.00024222
Iteration 44/1000 | Loss: 0.00025421
Iteration 45/1000 | Loss: 0.00031156
Iteration 46/1000 | Loss: 0.00053085
Iteration 47/1000 | Loss: 0.00057850
Iteration 48/1000 | Loss: 0.00090141
Iteration 49/1000 | Loss: 0.00070646
Iteration 50/1000 | Loss: 0.00044625
Iteration 51/1000 | Loss: 0.00025931
Iteration 52/1000 | Loss: 0.00017335
Iteration 53/1000 | Loss: 0.00020906
Iteration 54/1000 | Loss: 0.00027062
Iteration 55/1000 | Loss: 0.00038774
Iteration 56/1000 | Loss: 0.00060725
Iteration 57/1000 | Loss: 0.00048794
Iteration 58/1000 | Loss: 0.00076953
Iteration 59/1000 | Loss: 0.00041604
Iteration 60/1000 | Loss: 0.00036991
Iteration 61/1000 | Loss: 0.00029796
Iteration 62/1000 | Loss: 0.00069469
Iteration 63/1000 | Loss: 0.00028507
Iteration 64/1000 | Loss: 0.00045101
Iteration 65/1000 | Loss: 0.00045559
Iteration 66/1000 | Loss: 0.00028367
Iteration 67/1000 | Loss: 0.00031319
Iteration 68/1000 | Loss: 0.00011551
Iteration 69/1000 | Loss: 0.00041122
Iteration 70/1000 | Loss: 0.00035932
Iteration 71/1000 | Loss: 0.00035081
Iteration 72/1000 | Loss: 0.00034748
Iteration 73/1000 | Loss: 0.00039308
Iteration 74/1000 | Loss: 0.00031550
Iteration 75/1000 | Loss: 0.00020493
Iteration 76/1000 | Loss: 0.00005129
Iteration 77/1000 | Loss: 0.00071840
Iteration 78/1000 | Loss: 0.00025213
Iteration 79/1000 | Loss: 0.00036665
Iteration 80/1000 | Loss: 0.00032010
Iteration 81/1000 | Loss: 0.00014959
Iteration 82/1000 | Loss: 0.00047666
Iteration 83/1000 | Loss: 0.00037171
Iteration 84/1000 | Loss: 0.00027043
Iteration 85/1000 | Loss: 0.00026538
Iteration 86/1000 | Loss: 0.00011925
Iteration 87/1000 | Loss: 0.00023176
Iteration 88/1000 | Loss: 0.00023850
Iteration 89/1000 | Loss: 0.00021998
Iteration 90/1000 | Loss: 0.00024382
Iteration 91/1000 | Loss: 0.00026475
Iteration 92/1000 | Loss: 0.00025930
Iteration 93/1000 | Loss: 0.00022206
Iteration 94/1000 | Loss: 0.00031369
Iteration 95/1000 | Loss: 0.00023311
Iteration 96/1000 | Loss: 0.00009308
Iteration 97/1000 | Loss: 0.00011343
Iteration 98/1000 | Loss: 0.00025007
Iteration 99/1000 | Loss: 0.00024768
Iteration 100/1000 | Loss: 0.00024549
Iteration 101/1000 | Loss: 0.00022519
Iteration 102/1000 | Loss: 0.00032021
Iteration 103/1000 | Loss: 0.00033915
Iteration 104/1000 | Loss: 0.00017297
Iteration 105/1000 | Loss: 0.00050393
Iteration 106/1000 | Loss: 0.00009419
Iteration 107/1000 | Loss: 0.00023128
Iteration 108/1000 | Loss: 0.00071153
Iteration 109/1000 | Loss: 0.00028970
Iteration 110/1000 | Loss: 0.00008444
Iteration 111/1000 | Loss: 0.00010385
Iteration 112/1000 | Loss: 0.00015145
Iteration 113/1000 | Loss: 0.00006723
Iteration 114/1000 | Loss: 0.00016651
Iteration 115/1000 | Loss: 0.00016276
Iteration 116/1000 | Loss: 0.00017269
Iteration 117/1000 | Loss: 0.00003813
Iteration 118/1000 | Loss: 0.00003503
Iteration 119/1000 | Loss: 0.00004948
Iteration 120/1000 | Loss: 0.00061177
Iteration 121/1000 | Loss: 0.00030168
Iteration 122/1000 | Loss: 0.00048780
Iteration 123/1000 | Loss: 0.00006623
Iteration 124/1000 | Loss: 0.00015661
Iteration 125/1000 | Loss: 0.00005744
Iteration 126/1000 | Loss: 0.00011384
Iteration 127/1000 | Loss: 0.00011357
Iteration 128/1000 | Loss: 0.00014930
Iteration 129/1000 | Loss: 0.00016221
Iteration 130/1000 | Loss: 0.00044619
Iteration 131/1000 | Loss: 0.00018461
Iteration 132/1000 | Loss: 0.00019106
Iteration 133/1000 | Loss: 0.00013123
Iteration 134/1000 | Loss: 0.00016380
Iteration 135/1000 | Loss: 0.00016062
Iteration 136/1000 | Loss: 0.00011486
Iteration 137/1000 | Loss: 0.00019219
Iteration 138/1000 | Loss: 0.00008176
Iteration 139/1000 | Loss: 0.00020318
Iteration 140/1000 | Loss: 0.00012289
Iteration 141/1000 | Loss: 0.00013067
Iteration 142/1000 | Loss: 0.00008365
Iteration 143/1000 | Loss: 0.00014011
Iteration 144/1000 | Loss: 0.00010042
Iteration 145/1000 | Loss: 0.00015154
Iteration 146/1000 | Loss: 0.00003745
Iteration 147/1000 | Loss: 0.00007165
Iteration 148/1000 | Loss: 0.00006173
Iteration 149/1000 | Loss: 0.00007852
Iteration 150/1000 | Loss: 0.00011801
Iteration 151/1000 | Loss: 0.00003868
Iteration 152/1000 | Loss: 0.00014371
Iteration 153/1000 | Loss: 0.00015956
Iteration 154/1000 | Loss: 0.00003972
Iteration 155/1000 | Loss: 0.00014486
Iteration 156/1000 | Loss: 0.00010268
Iteration 157/1000 | Loss: 0.00011588
Iteration 158/1000 | Loss: 0.00011888
Iteration 159/1000 | Loss: 0.00012807
Iteration 160/1000 | Loss: 0.00011563
Iteration 161/1000 | Loss: 0.00010018
Iteration 162/1000 | Loss: 0.00016158
Iteration 163/1000 | Loss: 0.00011427
Iteration 164/1000 | Loss: 0.00010831
Iteration 165/1000 | Loss: 0.00009191
Iteration 166/1000 | Loss: 0.00005871
Iteration 167/1000 | Loss: 0.00012448
Iteration 168/1000 | Loss: 0.00010425
Iteration 169/1000 | Loss: 0.00009315
Iteration 170/1000 | Loss: 0.00009579
Iteration 171/1000 | Loss: 0.00028457
Iteration 172/1000 | Loss: 0.00003858
Iteration 173/1000 | Loss: 0.00003314
Iteration 174/1000 | Loss: 0.00003160
Iteration 175/1000 | Loss: 0.00003090
Iteration 176/1000 | Loss: 0.00003037
Iteration 177/1000 | Loss: 0.00003001
Iteration 178/1000 | Loss: 0.00002938
Iteration 179/1000 | Loss: 0.00002906
Iteration 180/1000 | Loss: 0.00002877
Iteration 181/1000 | Loss: 0.00002839
Iteration 182/1000 | Loss: 0.00002798
Iteration 183/1000 | Loss: 0.00002790
Iteration 184/1000 | Loss: 0.00002785
Iteration 185/1000 | Loss: 0.00002782
Iteration 186/1000 | Loss: 0.00002782
Iteration 187/1000 | Loss: 0.00002781
Iteration 188/1000 | Loss: 0.00002781
Iteration 189/1000 | Loss: 0.00002781
Iteration 190/1000 | Loss: 0.00002780
Iteration 191/1000 | Loss: 0.00002780
Iteration 192/1000 | Loss: 0.00002779
Iteration 193/1000 | Loss: 0.00002779
Iteration 194/1000 | Loss: 0.00002779
Iteration 195/1000 | Loss: 0.00002779
Iteration 196/1000 | Loss: 0.00002778
Iteration 197/1000 | Loss: 0.00002778
Iteration 198/1000 | Loss: 0.00002778
Iteration 199/1000 | Loss: 0.00002777
Iteration 200/1000 | Loss: 0.00002777
Iteration 201/1000 | Loss: 0.00002777
Iteration 202/1000 | Loss: 0.00002776
Iteration 203/1000 | Loss: 0.00002776
Iteration 204/1000 | Loss: 0.00002775
Iteration 205/1000 | Loss: 0.00002775
Iteration 206/1000 | Loss: 0.00002775
Iteration 207/1000 | Loss: 0.00002775
Iteration 208/1000 | Loss: 0.00002775
Iteration 209/1000 | Loss: 0.00002775
Iteration 210/1000 | Loss: 0.00002774
Iteration 211/1000 | Loss: 0.00002774
Iteration 212/1000 | Loss: 0.00002774
Iteration 213/1000 | Loss: 0.00002774
Iteration 214/1000 | Loss: 0.00002774
Iteration 215/1000 | Loss: 0.00002774
Iteration 216/1000 | Loss: 0.00002774
Iteration 217/1000 | Loss: 0.00002774
Iteration 218/1000 | Loss: 0.00002774
Iteration 219/1000 | Loss: 0.00002774
Iteration 220/1000 | Loss: 0.00002774
Iteration 221/1000 | Loss: 0.00002774
Iteration 222/1000 | Loss: 0.00002773
Iteration 223/1000 | Loss: 0.00002773
Iteration 224/1000 | Loss: 0.00002773
Iteration 225/1000 | Loss: 0.00002773
Iteration 226/1000 | Loss: 0.00002773
Iteration 227/1000 | Loss: 0.00002773
Iteration 228/1000 | Loss: 0.00002773
Iteration 229/1000 | Loss: 0.00002772
Iteration 230/1000 | Loss: 0.00002772
Iteration 231/1000 | Loss: 0.00002772
Iteration 232/1000 | Loss: 0.00002772
Iteration 233/1000 | Loss: 0.00002771
Iteration 234/1000 | Loss: 0.00002771
Iteration 235/1000 | Loss: 0.00002771
Iteration 236/1000 | Loss: 0.00002771
Iteration 237/1000 | Loss: 0.00002771
Iteration 238/1000 | Loss: 0.00002771
Iteration 239/1000 | Loss: 0.00002771
Iteration 240/1000 | Loss: 0.00002771
Iteration 241/1000 | Loss: 0.00002771
Iteration 242/1000 | Loss: 0.00002770
Iteration 243/1000 | Loss: 0.00002770
Iteration 244/1000 | Loss: 0.00002770
Iteration 245/1000 | Loss: 0.00002770
Iteration 246/1000 | Loss: 0.00002770
Iteration 247/1000 | Loss: 0.00002770
Iteration 248/1000 | Loss: 0.00002769
Iteration 249/1000 | Loss: 0.00002769
Iteration 250/1000 | Loss: 0.00002769
Iteration 251/1000 | Loss: 0.00002768
Iteration 252/1000 | Loss: 0.00002767
Iteration 253/1000 | Loss: 0.00002767
Iteration 254/1000 | Loss: 0.00002767
Iteration 255/1000 | Loss: 0.00002766
Iteration 256/1000 | Loss: 0.00002766
Iteration 257/1000 | Loss: 0.00002766
Iteration 258/1000 | Loss: 0.00002766
Iteration 259/1000 | Loss: 0.00002766
Iteration 260/1000 | Loss: 0.00002765
Iteration 261/1000 | Loss: 0.00002765
Iteration 262/1000 | Loss: 0.00002765
Iteration 263/1000 | Loss: 0.00002765
Iteration 264/1000 | Loss: 0.00002765
Iteration 265/1000 | Loss: 0.00002764
Iteration 266/1000 | Loss: 0.00002764
Iteration 267/1000 | Loss: 0.00002764
Iteration 268/1000 | Loss: 0.00002764
Iteration 269/1000 | Loss: 0.00002763
Iteration 270/1000 | Loss: 0.00002763
Iteration 271/1000 | Loss: 0.00002763
Iteration 272/1000 | Loss: 0.00002763
Iteration 273/1000 | Loss: 0.00002762
Iteration 274/1000 | Loss: 0.00002762
Iteration 275/1000 | Loss: 0.00002762
Iteration 276/1000 | Loss: 0.00002762
Iteration 277/1000 | Loss: 0.00002762
Iteration 278/1000 | Loss: 0.00002762
Iteration 279/1000 | Loss: 0.00002761
Iteration 280/1000 | Loss: 0.00002761
Iteration 281/1000 | Loss: 0.00002761
Iteration 282/1000 | Loss: 0.00002761
Iteration 283/1000 | Loss: 0.00002760
Iteration 284/1000 | Loss: 0.00002760
Iteration 285/1000 | Loss: 0.00002760
Iteration 286/1000 | Loss: 0.00002760
Iteration 287/1000 | Loss: 0.00002760
Iteration 288/1000 | Loss: 0.00002760
Iteration 289/1000 | Loss: 0.00002760
Iteration 290/1000 | Loss: 0.00002760
Iteration 291/1000 | Loss: 0.00002759
Iteration 292/1000 | Loss: 0.00002759
Iteration 293/1000 | Loss: 0.00002759
Iteration 294/1000 | Loss: 0.00002759
Iteration 295/1000 | Loss: 0.00002759
Iteration 296/1000 | Loss: 0.00002759
Iteration 297/1000 | Loss: 0.00002759
Iteration 298/1000 | Loss: 0.00002759
Iteration 299/1000 | Loss: 0.00002759
Iteration 300/1000 | Loss: 0.00002759
Iteration 301/1000 | Loss: 0.00002759
Iteration 302/1000 | Loss: 0.00002759
Iteration 303/1000 | Loss: 0.00002759
Iteration 304/1000 | Loss: 0.00002759
Iteration 305/1000 | Loss: 0.00002759
Iteration 306/1000 | Loss: 0.00002759
Iteration 307/1000 | Loss: 0.00002759
Iteration 308/1000 | Loss: 0.00002759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 308. Stopping optimization.
Last 5 losses: [2.7591819161898457e-05, 2.7591819161898457e-05, 2.7591819161898457e-05, 2.7591819161898457e-05, 2.7591819161898457e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7591819161898457e-05

Optimization complete. Final v2v error: 4.351657390594482 mm

Highest mean error: 6.797835350036621 mm for frame 143

Lowest mean error: 3.415060043334961 mm for frame 0

Saving results

Total time: 358.84764671325684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849510
Iteration 2/25 | Loss: 0.00127349
Iteration 3/25 | Loss: 0.00093260
Iteration 4/25 | Loss: 0.00085744
Iteration 5/25 | Loss: 0.00084246
Iteration 6/25 | Loss: 0.00083582
Iteration 7/25 | Loss: 0.00083361
Iteration 8/25 | Loss: 0.00082657
Iteration 9/25 | Loss: 0.00081811
Iteration 10/25 | Loss: 0.00082179
Iteration 11/25 | Loss: 0.00081952
Iteration 12/25 | Loss: 0.00082251
Iteration 13/25 | Loss: 0.00081956
Iteration 14/25 | Loss: 0.00081084
Iteration 15/25 | Loss: 0.00080663
Iteration 16/25 | Loss: 0.00080596
Iteration 17/25 | Loss: 0.00080580
Iteration 18/25 | Loss: 0.00080578
Iteration 19/25 | Loss: 0.00080578
Iteration 20/25 | Loss: 0.00080578
Iteration 21/25 | Loss: 0.00080577
Iteration 22/25 | Loss: 0.00080577
Iteration 23/25 | Loss: 0.00080577
Iteration 24/25 | Loss: 0.00080577
Iteration 25/25 | Loss: 0.00080577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40795672
Iteration 2/25 | Loss: 0.00050937
Iteration 3/25 | Loss: 0.00050933
Iteration 4/25 | Loss: 0.00050933
Iteration 5/25 | Loss: 0.00050933
Iteration 6/25 | Loss: 0.00050932
Iteration 7/25 | Loss: 0.00050932
Iteration 8/25 | Loss: 0.00050932
Iteration 9/25 | Loss: 0.00050932
Iteration 10/25 | Loss: 0.00050932
Iteration 11/25 | Loss: 0.00050932
Iteration 12/25 | Loss: 0.00050932
Iteration 13/25 | Loss: 0.00050932
Iteration 14/25 | Loss: 0.00050932
Iteration 15/25 | Loss: 0.00050932
Iteration 16/25 | Loss: 0.00050932
Iteration 17/25 | Loss: 0.00050932
Iteration 18/25 | Loss: 0.00050932
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005093233776278794, 0.0005093233776278794, 0.0005093233776278794, 0.0005093233776278794, 0.0005093233776278794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005093233776278794

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050932
Iteration 2/1000 | Loss: 0.00003877
Iteration 3/1000 | Loss: 0.00002922
Iteration 4/1000 | Loss: 0.00002559
Iteration 5/1000 | Loss: 0.00002387
Iteration 6/1000 | Loss: 0.00002300
Iteration 7/1000 | Loss: 0.00002236
Iteration 8/1000 | Loss: 0.00002179
Iteration 9/1000 | Loss: 0.00002141
Iteration 10/1000 | Loss: 0.00002115
Iteration 11/1000 | Loss: 0.00002090
Iteration 12/1000 | Loss: 0.00002083
Iteration 13/1000 | Loss: 0.00002065
Iteration 14/1000 | Loss: 0.00002050
Iteration 15/1000 | Loss: 0.00002041
Iteration 16/1000 | Loss: 0.00002033
Iteration 17/1000 | Loss: 0.00002024
Iteration 18/1000 | Loss: 0.00002021
Iteration 19/1000 | Loss: 0.00002020
Iteration 20/1000 | Loss: 0.00002011
Iteration 21/1000 | Loss: 0.00002008
Iteration 22/1000 | Loss: 0.00002006
Iteration 23/1000 | Loss: 0.00002005
Iteration 24/1000 | Loss: 0.00002005
Iteration 25/1000 | Loss: 0.00002004
Iteration 26/1000 | Loss: 0.00040798
Iteration 27/1000 | Loss: 0.00018755
Iteration 28/1000 | Loss: 0.00003971
Iteration 29/1000 | Loss: 0.00003111
Iteration 30/1000 | Loss: 0.00002640
Iteration 31/1000 | Loss: 0.00002270
Iteration 32/1000 | Loss: 0.00002077
Iteration 33/1000 | Loss: 0.00001985
Iteration 34/1000 | Loss: 0.00001952
Iteration 35/1000 | Loss: 0.00001931
Iteration 36/1000 | Loss: 0.00001929
Iteration 37/1000 | Loss: 0.00001927
Iteration 38/1000 | Loss: 0.00001903
Iteration 39/1000 | Loss: 0.00001888
Iteration 40/1000 | Loss: 0.00001887
Iteration 41/1000 | Loss: 0.00001881
Iteration 42/1000 | Loss: 0.00001879
Iteration 43/1000 | Loss: 0.00001878
Iteration 44/1000 | Loss: 0.00001878
Iteration 45/1000 | Loss: 0.00001877
Iteration 46/1000 | Loss: 0.00001877
Iteration 47/1000 | Loss: 0.00001877
Iteration 48/1000 | Loss: 0.00001876
Iteration 49/1000 | Loss: 0.00001876
Iteration 50/1000 | Loss: 0.00001875
Iteration 51/1000 | Loss: 0.00001875
Iteration 52/1000 | Loss: 0.00001874
Iteration 53/1000 | Loss: 0.00001874
Iteration 54/1000 | Loss: 0.00001873
Iteration 55/1000 | Loss: 0.00001873
Iteration 56/1000 | Loss: 0.00001872
Iteration 57/1000 | Loss: 0.00001872
Iteration 58/1000 | Loss: 0.00001872
Iteration 59/1000 | Loss: 0.00001871
Iteration 60/1000 | Loss: 0.00001871
Iteration 61/1000 | Loss: 0.00001869
Iteration 62/1000 | Loss: 0.00001869
Iteration 63/1000 | Loss: 0.00001867
Iteration 64/1000 | Loss: 0.00001866
Iteration 65/1000 | Loss: 0.00001866
Iteration 66/1000 | Loss: 0.00001865
Iteration 67/1000 | Loss: 0.00001865
Iteration 68/1000 | Loss: 0.00001864
Iteration 69/1000 | Loss: 0.00001864
Iteration 70/1000 | Loss: 0.00001864
Iteration 71/1000 | Loss: 0.00001863
Iteration 72/1000 | Loss: 0.00001863
Iteration 73/1000 | Loss: 0.00001863
Iteration 74/1000 | Loss: 0.00001863
Iteration 75/1000 | Loss: 0.00001862
Iteration 76/1000 | Loss: 0.00001862
Iteration 77/1000 | Loss: 0.00001862
Iteration 78/1000 | Loss: 0.00001862
Iteration 79/1000 | Loss: 0.00001862
Iteration 80/1000 | Loss: 0.00001862
Iteration 81/1000 | Loss: 0.00001862
Iteration 82/1000 | Loss: 0.00001861
Iteration 83/1000 | Loss: 0.00001861
Iteration 84/1000 | Loss: 0.00001861
Iteration 85/1000 | Loss: 0.00001860
Iteration 86/1000 | Loss: 0.00001860
Iteration 87/1000 | Loss: 0.00001860
Iteration 88/1000 | Loss: 0.00001860
Iteration 89/1000 | Loss: 0.00001860
Iteration 90/1000 | Loss: 0.00001860
Iteration 91/1000 | Loss: 0.00001860
Iteration 92/1000 | Loss: 0.00001859
Iteration 93/1000 | Loss: 0.00001859
Iteration 94/1000 | Loss: 0.00001859
Iteration 95/1000 | Loss: 0.00001859
Iteration 96/1000 | Loss: 0.00001859
Iteration 97/1000 | Loss: 0.00001859
Iteration 98/1000 | Loss: 0.00001859
Iteration 99/1000 | Loss: 0.00001859
Iteration 100/1000 | Loss: 0.00001859
Iteration 101/1000 | Loss: 0.00001858
Iteration 102/1000 | Loss: 0.00001858
Iteration 103/1000 | Loss: 0.00001858
Iteration 104/1000 | Loss: 0.00001858
Iteration 105/1000 | Loss: 0.00001858
Iteration 106/1000 | Loss: 0.00001857
Iteration 107/1000 | Loss: 0.00001857
Iteration 108/1000 | Loss: 0.00001857
Iteration 109/1000 | Loss: 0.00001857
Iteration 110/1000 | Loss: 0.00001857
Iteration 111/1000 | Loss: 0.00001857
Iteration 112/1000 | Loss: 0.00001857
Iteration 113/1000 | Loss: 0.00001857
Iteration 114/1000 | Loss: 0.00001857
Iteration 115/1000 | Loss: 0.00001857
Iteration 116/1000 | Loss: 0.00001857
Iteration 117/1000 | Loss: 0.00001857
Iteration 118/1000 | Loss: 0.00001857
Iteration 119/1000 | Loss: 0.00001857
Iteration 120/1000 | Loss: 0.00001857
Iteration 121/1000 | Loss: 0.00001857
Iteration 122/1000 | Loss: 0.00001857
Iteration 123/1000 | Loss: 0.00001857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.856874951045029e-05, 1.856874951045029e-05, 1.856874951045029e-05, 1.856874951045029e-05, 1.856874951045029e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.856874951045029e-05

Optimization complete. Final v2v error: 3.5802161693573 mm

Highest mean error: 4.67508602142334 mm for frame 32

Lowest mean error: 2.96420955657959 mm for frame 20

Saving results

Total time: 77.98859286308289
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841893
Iteration 2/25 | Loss: 0.00114841
Iteration 3/25 | Loss: 0.00083758
Iteration 4/25 | Loss: 0.00080865
Iteration 5/25 | Loss: 0.00079897
Iteration 6/25 | Loss: 0.00079690
Iteration 7/25 | Loss: 0.00079659
Iteration 8/25 | Loss: 0.00079659
Iteration 9/25 | Loss: 0.00079659
Iteration 10/25 | Loss: 0.00079659
Iteration 11/25 | Loss: 0.00079659
Iteration 12/25 | Loss: 0.00079659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000796586275100708, 0.000796586275100708, 0.000796586275100708, 0.000796586275100708, 0.000796586275100708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000796586275100708

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.94180298
Iteration 2/25 | Loss: 0.00050787
Iteration 3/25 | Loss: 0.00050783
Iteration 4/25 | Loss: 0.00050783
Iteration 5/25 | Loss: 0.00050783
Iteration 6/25 | Loss: 0.00050783
Iteration 7/25 | Loss: 0.00050783
Iteration 8/25 | Loss: 0.00050783
Iteration 9/25 | Loss: 0.00050783
Iteration 10/25 | Loss: 0.00050783
Iteration 11/25 | Loss: 0.00050783
Iteration 12/25 | Loss: 0.00050783
Iteration 13/25 | Loss: 0.00050783
Iteration 14/25 | Loss: 0.00050783
Iteration 15/25 | Loss: 0.00050783
Iteration 16/25 | Loss: 0.00050783
Iteration 17/25 | Loss: 0.00050783
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005078263930045068, 0.0005078263930045068, 0.0005078263930045068, 0.0005078263930045068, 0.0005078263930045068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005078263930045068

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050783
Iteration 2/1000 | Loss: 0.00003386
Iteration 3/1000 | Loss: 0.00002501
Iteration 4/1000 | Loss: 0.00002336
Iteration 5/1000 | Loss: 0.00002224
Iteration 6/1000 | Loss: 0.00002148
Iteration 7/1000 | Loss: 0.00002093
Iteration 8/1000 | Loss: 0.00002048
Iteration 9/1000 | Loss: 0.00002015
Iteration 10/1000 | Loss: 0.00001996
Iteration 11/1000 | Loss: 0.00001985
Iteration 12/1000 | Loss: 0.00001981
Iteration 13/1000 | Loss: 0.00001980
Iteration 14/1000 | Loss: 0.00001979
Iteration 15/1000 | Loss: 0.00001978
Iteration 16/1000 | Loss: 0.00001969
Iteration 17/1000 | Loss: 0.00001967
Iteration 18/1000 | Loss: 0.00001966
Iteration 19/1000 | Loss: 0.00001966
Iteration 20/1000 | Loss: 0.00001965
Iteration 21/1000 | Loss: 0.00001965
Iteration 22/1000 | Loss: 0.00001965
Iteration 23/1000 | Loss: 0.00001965
Iteration 24/1000 | Loss: 0.00001965
Iteration 25/1000 | Loss: 0.00001965
Iteration 26/1000 | Loss: 0.00001964
Iteration 27/1000 | Loss: 0.00001964
Iteration 28/1000 | Loss: 0.00001964
Iteration 29/1000 | Loss: 0.00001963
Iteration 30/1000 | Loss: 0.00001963
Iteration 31/1000 | Loss: 0.00001963
Iteration 32/1000 | Loss: 0.00001961
Iteration 33/1000 | Loss: 0.00001960
Iteration 34/1000 | Loss: 0.00001960
Iteration 35/1000 | Loss: 0.00001960
Iteration 36/1000 | Loss: 0.00001959
Iteration 37/1000 | Loss: 0.00001958
Iteration 38/1000 | Loss: 0.00001957
Iteration 39/1000 | Loss: 0.00001957
Iteration 40/1000 | Loss: 0.00001957
Iteration 41/1000 | Loss: 0.00001956
Iteration 42/1000 | Loss: 0.00001956
Iteration 43/1000 | Loss: 0.00001956
Iteration 44/1000 | Loss: 0.00001955
Iteration 45/1000 | Loss: 0.00001953
Iteration 46/1000 | Loss: 0.00001950
Iteration 47/1000 | Loss: 0.00001950
Iteration 48/1000 | Loss: 0.00001949
Iteration 49/1000 | Loss: 0.00001948
Iteration 50/1000 | Loss: 0.00001947
Iteration 51/1000 | Loss: 0.00001947
Iteration 52/1000 | Loss: 0.00001947
Iteration 53/1000 | Loss: 0.00001946
Iteration 54/1000 | Loss: 0.00001946
Iteration 55/1000 | Loss: 0.00001944
Iteration 56/1000 | Loss: 0.00001943
Iteration 57/1000 | Loss: 0.00001943
Iteration 58/1000 | Loss: 0.00001943
Iteration 59/1000 | Loss: 0.00001940
Iteration 60/1000 | Loss: 0.00001940
Iteration 61/1000 | Loss: 0.00001940
Iteration 62/1000 | Loss: 0.00001940
Iteration 63/1000 | Loss: 0.00001940
Iteration 64/1000 | Loss: 0.00001939
Iteration 65/1000 | Loss: 0.00001939
Iteration 66/1000 | Loss: 0.00001939
Iteration 67/1000 | Loss: 0.00001938
Iteration 68/1000 | Loss: 0.00001938
Iteration 69/1000 | Loss: 0.00001938
Iteration 70/1000 | Loss: 0.00001937
Iteration 71/1000 | Loss: 0.00001937
Iteration 72/1000 | Loss: 0.00001937
Iteration 73/1000 | Loss: 0.00001936
Iteration 74/1000 | Loss: 0.00001936
Iteration 75/1000 | Loss: 0.00001936
Iteration 76/1000 | Loss: 0.00001936
Iteration 77/1000 | Loss: 0.00001936
Iteration 78/1000 | Loss: 0.00001936
Iteration 79/1000 | Loss: 0.00001936
Iteration 80/1000 | Loss: 0.00001936
Iteration 81/1000 | Loss: 0.00001936
Iteration 82/1000 | Loss: 0.00001936
Iteration 83/1000 | Loss: 0.00001936
Iteration 84/1000 | Loss: 0.00001936
Iteration 85/1000 | Loss: 0.00001935
Iteration 86/1000 | Loss: 0.00001935
Iteration 87/1000 | Loss: 0.00001935
Iteration 88/1000 | Loss: 0.00001935
Iteration 89/1000 | Loss: 0.00001934
Iteration 90/1000 | Loss: 0.00001934
Iteration 91/1000 | Loss: 0.00001934
Iteration 92/1000 | Loss: 0.00001934
Iteration 93/1000 | Loss: 0.00001934
Iteration 94/1000 | Loss: 0.00001934
Iteration 95/1000 | Loss: 0.00001934
Iteration 96/1000 | Loss: 0.00001934
Iteration 97/1000 | Loss: 0.00001934
Iteration 98/1000 | Loss: 0.00001933
Iteration 99/1000 | Loss: 0.00001933
Iteration 100/1000 | Loss: 0.00001933
Iteration 101/1000 | Loss: 0.00001933
Iteration 102/1000 | Loss: 0.00001933
Iteration 103/1000 | Loss: 0.00001933
Iteration 104/1000 | Loss: 0.00001933
Iteration 105/1000 | Loss: 0.00001933
Iteration 106/1000 | Loss: 0.00001933
Iteration 107/1000 | Loss: 0.00001933
Iteration 108/1000 | Loss: 0.00001933
Iteration 109/1000 | Loss: 0.00001933
Iteration 110/1000 | Loss: 0.00001933
Iteration 111/1000 | Loss: 0.00001933
Iteration 112/1000 | Loss: 0.00001933
Iteration 113/1000 | Loss: 0.00001933
Iteration 114/1000 | Loss: 0.00001933
Iteration 115/1000 | Loss: 0.00001933
Iteration 116/1000 | Loss: 0.00001933
Iteration 117/1000 | Loss: 0.00001933
Iteration 118/1000 | Loss: 0.00001933
Iteration 119/1000 | Loss: 0.00001933
Iteration 120/1000 | Loss: 0.00001933
Iteration 121/1000 | Loss: 0.00001933
Iteration 122/1000 | Loss: 0.00001933
Iteration 123/1000 | Loss: 0.00001933
Iteration 124/1000 | Loss: 0.00001933
Iteration 125/1000 | Loss: 0.00001933
Iteration 126/1000 | Loss: 0.00001933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.932913437485695e-05, 1.932913437485695e-05, 1.932913437485695e-05, 1.932913437485695e-05, 1.932913437485695e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.932913437485695e-05

Optimization complete. Final v2v error: 3.7094075679779053 mm

Highest mean error: 4.553221225738525 mm for frame 232

Lowest mean error: 3.240109443664551 mm for frame 108

Saving results

Total time: 41.92862129211426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938471
Iteration 2/25 | Loss: 0.00140352
Iteration 3/25 | Loss: 0.00099649
Iteration 4/25 | Loss: 0.00094139
Iteration 5/25 | Loss: 0.00092624
Iteration 6/25 | Loss: 0.00092253
Iteration 7/25 | Loss: 0.00092137
Iteration 8/25 | Loss: 0.00092118
Iteration 9/25 | Loss: 0.00092118
Iteration 10/25 | Loss: 0.00092118
Iteration 11/25 | Loss: 0.00092118
Iteration 12/25 | Loss: 0.00092118
Iteration 13/25 | Loss: 0.00092118
Iteration 14/25 | Loss: 0.00092118
Iteration 15/25 | Loss: 0.00092118
Iteration 16/25 | Loss: 0.00092118
Iteration 17/25 | Loss: 0.00092118
Iteration 18/25 | Loss: 0.00092118
Iteration 19/25 | Loss: 0.00092118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009211841970682144, 0.0009211841970682144, 0.0009211841970682144, 0.0009211841970682144, 0.0009211841970682144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009211841970682144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.61469084
Iteration 2/25 | Loss: 0.00047608
Iteration 3/25 | Loss: 0.00047607
Iteration 4/25 | Loss: 0.00047607
Iteration 5/25 | Loss: 0.00047607
Iteration 6/25 | Loss: 0.00047607
Iteration 7/25 | Loss: 0.00047607
Iteration 8/25 | Loss: 0.00047607
Iteration 9/25 | Loss: 0.00047607
Iteration 10/25 | Loss: 0.00047607
Iteration 11/25 | Loss: 0.00047607
Iteration 12/25 | Loss: 0.00047607
Iteration 13/25 | Loss: 0.00047607
Iteration 14/25 | Loss: 0.00047607
Iteration 15/25 | Loss: 0.00047607
Iteration 16/25 | Loss: 0.00047607
Iteration 17/25 | Loss: 0.00047607
Iteration 18/25 | Loss: 0.00047607
Iteration 19/25 | Loss: 0.00047607
Iteration 20/25 | Loss: 0.00047607
Iteration 21/25 | Loss: 0.00047607
Iteration 22/25 | Loss: 0.00047607
Iteration 23/25 | Loss: 0.00047607
Iteration 24/25 | Loss: 0.00047607
Iteration 25/25 | Loss: 0.00047607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047607
Iteration 2/1000 | Loss: 0.00004524
Iteration 3/1000 | Loss: 0.00002988
Iteration 4/1000 | Loss: 0.00002629
Iteration 5/1000 | Loss: 0.00002534
Iteration 6/1000 | Loss: 0.00002468
Iteration 7/1000 | Loss: 0.00002427
Iteration 8/1000 | Loss: 0.00002401
Iteration 9/1000 | Loss: 0.00002379
Iteration 10/1000 | Loss: 0.00002359
Iteration 11/1000 | Loss: 0.00002344
Iteration 12/1000 | Loss: 0.00002328
Iteration 13/1000 | Loss: 0.00002315
Iteration 14/1000 | Loss: 0.00002309
Iteration 15/1000 | Loss: 0.00002307
Iteration 16/1000 | Loss: 0.00002305
Iteration 17/1000 | Loss: 0.00002302
Iteration 18/1000 | Loss: 0.00002297
Iteration 19/1000 | Loss: 0.00002297
Iteration 20/1000 | Loss: 0.00002297
Iteration 21/1000 | Loss: 0.00002297
Iteration 22/1000 | Loss: 0.00002296
Iteration 23/1000 | Loss: 0.00002296
Iteration 24/1000 | Loss: 0.00002296
Iteration 25/1000 | Loss: 0.00002296
Iteration 26/1000 | Loss: 0.00002295
Iteration 27/1000 | Loss: 0.00002294
Iteration 28/1000 | Loss: 0.00002293
Iteration 29/1000 | Loss: 0.00002293
Iteration 30/1000 | Loss: 0.00002292
Iteration 31/1000 | Loss: 0.00002292
Iteration 32/1000 | Loss: 0.00002292
Iteration 33/1000 | Loss: 0.00002291
Iteration 34/1000 | Loss: 0.00002291
Iteration 35/1000 | Loss: 0.00002291
Iteration 36/1000 | Loss: 0.00002290
Iteration 37/1000 | Loss: 0.00002290
Iteration 38/1000 | Loss: 0.00002290
Iteration 39/1000 | Loss: 0.00002290
Iteration 40/1000 | Loss: 0.00002289
Iteration 41/1000 | Loss: 0.00002289
Iteration 42/1000 | Loss: 0.00002289
Iteration 43/1000 | Loss: 0.00002288
Iteration 44/1000 | Loss: 0.00002288
Iteration 45/1000 | Loss: 0.00002288
Iteration 46/1000 | Loss: 0.00002288
Iteration 47/1000 | Loss: 0.00002288
Iteration 48/1000 | Loss: 0.00002287
Iteration 49/1000 | Loss: 0.00002287
Iteration 50/1000 | Loss: 0.00002287
Iteration 51/1000 | Loss: 0.00002287
Iteration 52/1000 | Loss: 0.00002287
Iteration 53/1000 | Loss: 0.00002287
Iteration 54/1000 | Loss: 0.00002286
Iteration 55/1000 | Loss: 0.00002286
Iteration 56/1000 | Loss: 0.00002286
Iteration 57/1000 | Loss: 0.00002286
Iteration 58/1000 | Loss: 0.00002285
Iteration 59/1000 | Loss: 0.00002285
Iteration 60/1000 | Loss: 0.00002285
Iteration 61/1000 | Loss: 0.00002285
Iteration 62/1000 | Loss: 0.00002285
Iteration 63/1000 | Loss: 0.00002285
Iteration 64/1000 | Loss: 0.00002284
Iteration 65/1000 | Loss: 0.00002284
Iteration 66/1000 | Loss: 0.00002284
Iteration 67/1000 | Loss: 0.00002284
Iteration 68/1000 | Loss: 0.00002284
Iteration 69/1000 | Loss: 0.00002284
Iteration 70/1000 | Loss: 0.00002284
Iteration 71/1000 | Loss: 0.00002284
Iteration 72/1000 | Loss: 0.00002284
Iteration 73/1000 | Loss: 0.00002284
Iteration 74/1000 | Loss: 0.00002284
Iteration 75/1000 | Loss: 0.00002284
Iteration 76/1000 | Loss: 0.00002283
Iteration 77/1000 | Loss: 0.00002283
Iteration 78/1000 | Loss: 0.00002283
Iteration 79/1000 | Loss: 0.00002283
Iteration 80/1000 | Loss: 0.00002283
Iteration 81/1000 | Loss: 0.00002283
Iteration 82/1000 | Loss: 0.00002283
Iteration 83/1000 | Loss: 0.00002283
Iteration 84/1000 | Loss: 0.00002283
Iteration 85/1000 | Loss: 0.00002283
Iteration 86/1000 | Loss: 0.00002283
Iteration 87/1000 | Loss: 0.00002283
Iteration 88/1000 | Loss: 0.00002283
Iteration 89/1000 | Loss: 0.00002283
Iteration 90/1000 | Loss: 0.00002283
Iteration 91/1000 | Loss: 0.00002283
Iteration 92/1000 | Loss: 0.00002282
Iteration 93/1000 | Loss: 0.00002282
Iteration 94/1000 | Loss: 0.00002282
Iteration 95/1000 | Loss: 0.00002282
Iteration 96/1000 | Loss: 0.00002282
Iteration 97/1000 | Loss: 0.00002282
Iteration 98/1000 | Loss: 0.00002282
Iteration 99/1000 | Loss: 0.00002282
Iteration 100/1000 | Loss: 0.00002282
Iteration 101/1000 | Loss: 0.00002282
Iteration 102/1000 | Loss: 0.00002282
Iteration 103/1000 | Loss: 0.00002282
Iteration 104/1000 | Loss: 0.00002282
Iteration 105/1000 | Loss: 0.00002282
Iteration 106/1000 | Loss: 0.00002282
Iteration 107/1000 | Loss: 0.00002282
Iteration 108/1000 | Loss: 0.00002282
Iteration 109/1000 | Loss: 0.00002282
Iteration 110/1000 | Loss: 0.00002281
Iteration 111/1000 | Loss: 0.00002281
Iteration 112/1000 | Loss: 0.00002281
Iteration 113/1000 | Loss: 0.00002281
Iteration 114/1000 | Loss: 0.00002281
Iteration 115/1000 | Loss: 0.00002281
Iteration 116/1000 | Loss: 0.00002281
Iteration 117/1000 | Loss: 0.00002281
Iteration 118/1000 | Loss: 0.00002281
Iteration 119/1000 | Loss: 0.00002281
Iteration 120/1000 | Loss: 0.00002281
Iteration 121/1000 | Loss: 0.00002281
Iteration 122/1000 | Loss: 0.00002281
Iteration 123/1000 | Loss: 0.00002281
Iteration 124/1000 | Loss: 0.00002281
Iteration 125/1000 | Loss: 0.00002281
Iteration 126/1000 | Loss: 0.00002281
Iteration 127/1000 | Loss: 0.00002281
Iteration 128/1000 | Loss: 0.00002281
Iteration 129/1000 | Loss: 0.00002281
Iteration 130/1000 | Loss: 0.00002281
Iteration 131/1000 | Loss: 0.00002281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.2814796466263942e-05, 2.2814796466263942e-05, 2.2814796466263942e-05, 2.2814796466263942e-05, 2.2814796466263942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2814796466263942e-05

Optimization complete. Final v2v error: 3.8992018699645996 mm

Highest mean error: 4.249765396118164 mm for frame 20

Lowest mean error: 3.4327449798583984 mm for frame 42

Saving results

Total time: 38.83037710189819
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001142
Iteration 2/25 | Loss: 0.00124361
Iteration 3/25 | Loss: 0.00088999
Iteration 4/25 | Loss: 0.00084848
Iteration 5/25 | Loss: 0.00084555
Iteration 6/25 | Loss: 0.00084504
Iteration 7/25 | Loss: 0.00084502
Iteration 8/25 | Loss: 0.00084502
Iteration 9/25 | Loss: 0.00084502
Iteration 10/25 | Loss: 0.00084502
Iteration 11/25 | Loss: 0.00084502
Iteration 12/25 | Loss: 0.00084502
Iteration 13/25 | Loss: 0.00084502
Iteration 14/25 | Loss: 0.00084502
Iteration 15/25 | Loss: 0.00084502
Iteration 16/25 | Loss: 0.00084502
Iteration 17/25 | Loss: 0.00084502
Iteration 18/25 | Loss: 0.00084502
Iteration 19/25 | Loss: 0.00084502
Iteration 20/25 | Loss: 0.00084502
Iteration 21/25 | Loss: 0.00084502
Iteration 22/25 | Loss: 0.00084502
Iteration 23/25 | Loss: 0.00084502
Iteration 24/25 | Loss: 0.00084502
Iteration 25/25 | Loss: 0.00084502

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49755406
Iteration 2/25 | Loss: 0.00051832
Iteration 3/25 | Loss: 0.00051832
Iteration 4/25 | Loss: 0.00051832
Iteration 5/25 | Loss: 0.00051832
Iteration 6/25 | Loss: 0.00051832
Iteration 7/25 | Loss: 0.00051832
Iteration 8/25 | Loss: 0.00051832
Iteration 9/25 | Loss: 0.00051832
Iteration 10/25 | Loss: 0.00051832
Iteration 11/25 | Loss: 0.00051832
Iteration 12/25 | Loss: 0.00051832
Iteration 13/25 | Loss: 0.00051832
Iteration 14/25 | Loss: 0.00051832
Iteration 15/25 | Loss: 0.00051832
Iteration 16/25 | Loss: 0.00051832
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005183172761462629, 0.0005183172761462629, 0.0005183172761462629, 0.0005183172761462629, 0.0005183172761462629]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005183172761462629

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051832
Iteration 2/1000 | Loss: 0.00002594
Iteration 3/1000 | Loss: 0.00001948
Iteration 4/1000 | Loss: 0.00001830
Iteration 5/1000 | Loss: 0.00001793
Iteration 6/1000 | Loss: 0.00001766
Iteration 7/1000 | Loss: 0.00001743
Iteration 8/1000 | Loss: 0.00001729
Iteration 9/1000 | Loss: 0.00001723
Iteration 10/1000 | Loss: 0.00001723
Iteration 11/1000 | Loss: 0.00001719
Iteration 12/1000 | Loss: 0.00001719
Iteration 13/1000 | Loss: 0.00001711
Iteration 14/1000 | Loss: 0.00001709
Iteration 15/1000 | Loss: 0.00001708
Iteration 16/1000 | Loss: 0.00001692
Iteration 17/1000 | Loss: 0.00001688
Iteration 18/1000 | Loss: 0.00001684
Iteration 19/1000 | Loss: 0.00001683
Iteration 20/1000 | Loss: 0.00001682
Iteration 21/1000 | Loss: 0.00001681
Iteration 22/1000 | Loss: 0.00001681
Iteration 23/1000 | Loss: 0.00001674
Iteration 24/1000 | Loss: 0.00001674
Iteration 25/1000 | Loss: 0.00001674
Iteration 26/1000 | Loss: 0.00001673
Iteration 27/1000 | Loss: 0.00001671
Iteration 28/1000 | Loss: 0.00001671
Iteration 29/1000 | Loss: 0.00001670
Iteration 30/1000 | Loss: 0.00001670
Iteration 31/1000 | Loss: 0.00001669
Iteration 32/1000 | Loss: 0.00001669
Iteration 33/1000 | Loss: 0.00001669
Iteration 34/1000 | Loss: 0.00001668
Iteration 35/1000 | Loss: 0.00001668
Iteration 36/1000 | Loss: 0.00001667
Iteration 37/1000 | Loss: 0.00001667
Iteration 38/1000 | Loss: 0.00001667
Iteration 39/1000 | Loss: 0.00001667
Iteration 40/1000 | Loss: 0.00001666
Iteration 41/1000 | Loss: 0.00001666
Iteration 42/1000 | Loss: 0.00001666
Iteration 43/1000 | Loss: 0.00001665
Iteration 44/1000 | Loss: 0.00001665
Iteration 45/1000 | Loss: 0.00001664
Iteration 46/1000 | Loss: 0.00001664
Iteration 47/1000 | Loss: 0.00001663
Iteration 48/1000 | Loss: 0.00001659
Iteration 49/1000 | Loss: 0.00001659
Iteration 50/1000 | Loss: 0.00001658
Iteration 51/1000 | Loss: 0.00001658
Iteration 52/1000 | Loss: 0.00001658
Iteration 53/1000 | Loss: 0.00001657
Iteration 54/1000 | Loss: 0.00001656
Iteration 55/1000 | Loss: 0.00001656
Iteration 56/1000 | Loss: 0.00001656
Iteration 57/1000 | Loss: 0.00001656
Iteration 58/1000 | Loss: 0.00001656
Iteration 59/1000 | Loss: 0.00001656
Iteration 60/1000 | Loss: 0.00001656
Iteration 61/1000 | Loss: 0.00001656
Iteration 62/1000 | Loss: 0.00001656
Iteration 63/1000 | Loss: 0.00001655
Iteration 64/1000 | Loss: 0.00001655
Iteration 65/1000 | Loss: 0.00001655
Iteration 66/1000 | Loss: 0.00001655
Iteration 67/1000 | Loss: 0.00001655
Iteration 68/1000 | Loss: 0.00001655
Iteration 69/1000 | Loss: 0.00001654
Iteration 70/1000 | Loss: 0.00001654
Iteration 71/1000 | Loss: 0.00001654
Iteration 72/1000 | Loss: 0.00001654
Iteration 73/1000 | Loss: 0.00001654
Iteration 74/1000 | Loss: 0.00001653
Iteration 75/1000 | Loss: 0.00001653
Iteration 76/1000 | Loss: 0.00001653
Iteration 77/1000 | Loss: 0.00001653
Iteration 78/1000 | Loss: 0.00001653
Iteration 79/1000 | Loss: 0.00001653
Iteration 80/1000 | Loss: 0.00001653
Iteration 81/1000 | Loss: 0.00001653
Iteration 82/1000 | Loss: 0.00001653
Iteration 83/1000 | Loss: 0.00001653
Iteration 84/1000 | Loss: 0.00001653
Iteration 85/1000 | Loss: 0.00001653
Iteration 86/1000 | Loss: 0.00001653
Iteration 87/1000 | Loss: 0.00001652
Iteration 88/1000 | Loss: 0.00001652
Iteration 89/1000 | Loss: 0.00001652
Iteration 90/1000 | Loss: 0.00001652
Iteration 91/1000 | Loss: 0.00001652
Iteration 92/1000 | Loss: 0.00001652
Iteration 93/1000 | Loss: 0.00001652
Iteration 94/1000 | Loss: 0.00001652
Iteration 95/1000 | Loss: 0.00001652
Iteration 96/1000 | Loss: 0.00001652
Iteration 97/1000 | Loss: 0.00001652
Iteration 98/1000 | Loss: 0.00001652
Iteration 99/1000 | Loss: 0.00001652
Iteration 100/1000 | Loss: 0.00001651
Iteration 101/1000 | Loss: 0.00001651
Iteration 102/1000 | Loss: 0.00001651
Iteration 103/1000 | Loss: 0.00001651
Iteration 104/1000 | Loss: 0.00001651
Iteration 105/1000 | Loss: 0.00001651
Iteration 106/1000 | Loss: 0.00001651
Iteration 107/1000 | Loss: 0.00001651
Iteration 108/1000 | Loss: 0.00001651
Iteration 109/1000 | Loss: 0.00001651
Iteration 110/1000 | Loss: 0.00001651
Iteration 111/1000 | Loss: 0.00001651
Iteration 112/1000 | Loss: 0.00001651
Iteration 113/1000 | Loss: 0.00001650
Iteration 114/1000 | Loss: 0.00001650
Iteration 115/1000 | Loss: 0.00001650
Iteration 116/1000 | Loss: 0.00001650
Iteration 117/1000 | Loss: 0.00001650
Iteration 118/1000 | Loss: 0.00001650
Iteration 119/1000 | Loss: 0.00001650
Iteration 120/1000 | Loss: 0.00001650
Iteration 121/1000 | Loss: 0.00001650
Iteration 122/1000 | Loss: 0.00001650
Iteration 123/1000 | Loss: 0.00001650
Iteration 124/1000 | Loss: 0.00001650
Iteration 125/1000 | Loss: 0.00001650
Iteration 126/1000 | Loss: 0.00001650
Iteration 127/1000 | Loss: 0.00001650
Iteration 128/1000 | Loss: 0.00001650
Iteration 129/1000 | Loss: 0.00001650
Iteration 130/1000 | Loss: 0.00001650
Iteration 131/1000 | Loss: 0.00001650
Iteration 132/1000 | Loss: 0.00001649
Iteration 133/1000 | Loss: 0.00001649
Iteration 134/1000 | Loss: 0.00001649
Iteration 135/1000 | Loss: 0.00001649
Iteration 136/1000 | Loss: 0.00001649
Iteration 137/1000 | Loss: 0.00001649
Iteration 138/1000 | Loss: 0.00001649
Iteration 139/1000 | Loss: 0.00001649
Iteration 140/1000 | Loss: 0.00001649
Iteration 141/1000 | Loss: 0.00001649
Iteration 142/1000 | Loss: 0.00001649
Iteration 143/1000 | Loss: 0.00001649
Iteration 144/1000 | Loss: 0.00001649
Iteration 145/1000 | Loss: 0.00001649
Iteration 146/1000 | Loss: 0.00001649
Iteration 147/1000 | Loss: 0.00001649
Iteration 148/1000 | Loss: 0.00001649
Iteration 149/1000 | Loss: 0.00001649
Iteration 150/1000 | Loss: 0.00001649
Iteration 151/1000 | Loss: 0.00001649
Iteration 152/1000 | Loss: 0.00001649
Iteration 153/1000 | Loss: 0.00001649
Iteration 154/1000 | Loss: 0.00001649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.649291516514495e-05, 1.649291516514495e-05, 1.649291516514495e-05, 1.649291516514495e-05, 1.649291516514495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.649291516514495e-05

Optimization complete. Final v2v error: 3.36862850189209 mm

Highest mean error: 3.578200340270996 mm for frame 22

Lowest mean error: 3.0310022830963135 mm for frame 0

Saving results

Total time: 36.04397678375244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01092410
Iteration 2/25 | Loss: 0.01092410
Iteration 3/25 | Loss: 0.01092410
Iteration 4/25 | Loss: 0.01092410
Iteration 5/25 | Loss: 0.01092410
Iteration 6/25 | Loss: 0.01092410
Iteration 7/25 | Loss: 0.01092410
Iteration 8/25 | Loss: 0.01092409
Iteration 9/25 | Loss: 0.01092409
Iteration 10/25 | Loss: 0.01092409
Iteration 11/25 | Loss: 0.01092409
Iteration 12/25 | Loss: 0.01092409
Iteration 13/25 | Loss: 0.01092409
Iteration 14/25 | Loss: 0.01092408
Iteration 15/25 | Loss: 0.01092408
Iteration 16/25 | Loss: 0.01092408
Iteration 17/25 | Loss: 0.01092408
Iteration 18/25 | Loss: 0.01092408
Iteration 19/25 | Loss: 0.01092407
Iteration 20/25 | Loss: 0.01092407
Iteration 21/25 | Loss: 0.01092407
Iteration 22/25 | Loss: 0.01092407
Iteration 23/25 | Loss: 0.01092407
Iteration 24/25 | Loss: 0.01092407
Iteration 25/25 | Loss: 0.01092406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85791349
Iteration 2/25 | Loss: 0.07658553
Iteration 3/25 | Loss: 0.07657167
Iteration 4/25 | Loss: 0.07657166
Iteration 5/25 | Loss: 0.07657166
Iteration 6/25 | Loss: 0.07657166
Iteration 7/25 | Loss: 0.07657166
Iteration 8/25 | Loss: 0.07657165
Iteration 9/25 | Loss: 0.07657165
Iteration 10/25 | Loss: 0.07657165
Iteration 11/25 | Loss: 0.07657165
Iteration 12/25 | Loss: 0.07657165
Iteration 13/25 | Loss: 0.07657165
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.07657165080308914, 0.07657165080308914, 0.07657165080308914, 0.07657165080308914, 0.07657165080308914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07657165080308914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07657165
Iteration 2/1000 | Loss: 0.00275407
Iteration 3/1000 | Loss: 0.00109177
Iteration 4/1000 | Loss: 0.00037902
Iteration 5/1000 | Loss: 0.00078973
Iteration 6/1000 | Loss: 0.00013822
Iteration 7/1000 | Loss: 0.00008108
Iteration 8/1000 | Loss: 0.00005692
Iteration 9/1000 | Loss: 0.00040155
Iteration 10/1000 | Loss: 0.00004241
Iteration 11/1000 | Loss: 0.00015336
Iteration 12/1000 | Loss: 0.00003499
Iteration 13/1000 | Loss: 0.00003869
Iteration 14/1000 | Loss: 0.00002991
Iteration 15/1000 | Loss: 0.00007124
Iteration 16/1000 | Loss: 0.00018001
Iteration 17/1000 | Loss: 0.00025001
Iteration 18/1000 | Loss: 0.00002904
Iteration 19/1000 | Loss: 0.00005247
Iteration 20/1000 | Loss: 0.00003131
Iteration 21/1000 | Loss: 0.00002367
Iteration 22/1000 | Loss: 0.00003764
Iteration 23/1000 | Loss: 0.00002163
Iteration 24/1000 | Loss: 0.00002091
Iteration 25/1000 | Loss: 0.00005012
Iteration 26/1000 | Loss: 0.00002647
Iteration 27/1000 | Loss: 0.00001943
Iteration 28/1000 | Loss: 0.00001847
Iteration 29/1000 | Loss: 0.00001782
Iteration 30/1000 | Loss: 0.00003778
Iteration 31/1000 | Loss: 0.00007572
Iteration 32/1000 | Loss: 0.00002086
Iteration 33/1000 | Loss: 0.00001690
Iteration 34/1000 | Loss: 0.00001688
Iteration 35/1000 | Loss: 0.00001685
Iteration 36/1000 | Loss: 0.00002015
Iteration 37/1000 | Loss: 0.00001674
Iteration 38/1000 | Loss: 0.00004427
Iteration 39/1000 | Loss: 0.00001675
Iteration 40/1000 | Loss: 0.00002945
Iteration 41/1000 | Loss: 0.00009202
Iteration 42/1000 | Loss: 0.00002189
Iteration 43/1000 | Loss: 0.00005375
Iteration 44/1000 | Loss: 0.00001642
Iteration 45/1000 | Loss: 0.00001634
Iteration 46/1000 | Loss: 0.00002377
Iteration 47/1000 | Loss: 0.00001861
Iteration 48/1000 | Loss: 0.00002294
Iteration 49/1000 | Loss: 0.00002351
Iteration 50/1000 | Loss: 0.00001778
Iteration 51/1000 | Loss: 0.00001763
Iteration 52/1000 | Loss: 0.00001621
Iteration 53/1000 | Loss: 0.00001621
Iteration 54/1000 | Loss: 0.00001621
Iteration 55/1000 | Loss: 0.00001621
Iteration 56/1000 | Loss: 0.00001621
Iteration 57/1000 | Loss: 0.00001621
Iteration 58/1000 | Loss: 0.00001621
Iteration 59/1000 | Loss: 0.00001621
Iteration 60/1000 | Loss: 0.00001621
Iteration 61/1000 | Loss: 0.00001621
Iteration 62/1000 | Loss: 0.00001620
Iteration 63/1000 | Loss: 0.00001620
Iteration 64/1000 | Loss: 0.00001620
Iteration 65/1000 | Loss: 0.00001620
Iteration 66/1000 | Loss: 0.00001620
Iteration 67/1000 | Loss: 0.00001620
Iteration 68/1000 | Loss: 0.00001620
Iteration 69/1000 | Loss: 0.00001620
Iteration 70/1000 | Loss: 0.00001620
Iteration 71/1000 | Loss: 0.00001620
Iteration 72/1000 | Loss: 0.00001619
Iteration 73/1000 | Loss: 0.00001619
Iteration 74/1000 | Loss: 0.00001619
Iteration 75/1000 | Loss: 0.00001619
Iteration 76/1000 | Loss: 0.00001619
Iteration 77/1000 | Loss: 0.00001619
Iteration 78/1000 | Loss: 0.00001619
Iteration 79/1000 | Loss: 0.00001619
Iteration 80/1000 | Loss: 0.00001619
Iteration 81/1000 | Loss: 0.00001619
Iteration 82/1000 | Loss: 0.00001619
Iteration 83/1000 | Loss: 0.00001619
Iteration 84/1000 | Loss: 0.00001619
Iteration 85/1000 | Loss: 0.00001619
Iteration 86/1000 | Loss: 0.00001619
Iteration 87/1000 | Loss: 0.00001619
Iteration 88/1000 | Loss: 0.00001619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.6191730537684634e-05, 1.6191730537684634e-05, 1.6191730537684634e-05, 1.6191730537684634e-05, 1.6191730537684634e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6191730537684634e-05

Optimization complete. Final v2v error: 3.411513566970825 mm

Highest mean error: 4.111560344696045 mm for frame 9

Lowest mean error: 3.0899441242218018 mm for frame 12

Saving results

Total time: 77.92291712760925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00937926
Iteration 2/25 | Loss: 0.00115445
Iteration 3/25 | Loss: 0.00094639
Iteration 4/25 | Loss: 0.00090911
Iteration 5/25 | Loss: 0.00089768
Iteration 6/25 | Loss: 0.00089576
Iteration 7/25 | Loss: 0.00089569
Iteration 8/25 | Loss: 0.00089569
Iteration 9/25 | Loss: 0.00089569
Iteration 10/25 | Loss: 0.00089569
Iteration 11/25 | Loss: 0.00089569
Iteration 12/25 | Loss: 0.00089569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008956876117736101, 0.0008956876117736101, 0.0008956876117736101, 0.0008956876117736101, 0.0008956876117736101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008956876117736101

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43807387
Iteration 2/25 | Loss: 0.00058466
Iteration 3/25 | Loss: 0.00058466
Iteration 4/25 | Loss: 0.00058466
Iteration 5/25 | Loss: 0.00058466
Iteration 6/25 | Loss: 0.00058466
Iteration 7/25 | Loss: 0.00058466
Iteration 8/25 | Loss: 0.00058466
Iteration 9/25 | Loss: 0.00058466
Iteration 10/25 | Loss: 0.00058466
Iteration 11/25 | Loss: 0.00058466
Iteration 12/25 | Loss: 0.00058466
Iteration 13/25 | Loss: 0.00058466
Iteration 14/25 | Loss: 0.00058466
Iteration 15/25 | Loss: 0.00058466
Iteration 16/25 | Loss: 0.00058466
Iteration 17/25 | Loss: 0.00058466
Iteration 18/25 | Loss: 0.00058466
Iteration 19/25 | Loss: 0.00058466
Iteration 20/25 | Loss: 0.00058466
Iteration 21/25 | Loss: 0.00058466
Iteration 22/25 | Loss: 0.00058466
Iteration 23/25 | Loss: 0.00058466
Iteration 24/25 | Loss: 0.00058466
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005846598069183528, 0.0005846598069183528, 0.0005846598069183528, 0.0005846598069183528, 0.0005846598069183528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005846598069183528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058466
Iteration 2/1000 | Loss: 0.00004570
Iteration 3/1000 | Loss: 0.00003577
Iteration 4/1000 | Loss: 0.00003342
Iteration 5/1000 | Loss: 0.00003211
Iteration 6/1000 | Loss: 0.00003105
Iteration 7/1000 | Loss: 0.00003057
Iteration 8/1000 | Loss: 0.00003012
Iteration 9/1000 | Loss: 0.00002975
Iteration 10/1000 | Loss: 0.00002966
Iteration 11/1000 | Loss: 0.00002956
Iteration 12/1000 | Loss: 0.00002946
Iteration 13/1000 | Loss: 0.00002943
Iteration 14/1000 | Loss: 0.00002942
Iteration 15/1000 | Loss: 0.00002942
Iteration 16/1000 | Loss: 0.00002939
Iteration 17/1000 | Loss: 0.00002927
Iteration 18/1000 | Loss: 0.00002924
Iteration 19/1000 | Loss: 0.00002923
Iteration 20/1000 | Loss: 0.00002922
Iteration 21/1000 | Loss: 0.00002922
Iteration 22/1000 | Loss: 0.00002922
Iteration 23/1000 | Loss: 0.00002922
Iteration 24/1000 | Loss: 0.00002921
Iteration 25/1000 | Loss: 0.00002921
Iteration 26/1000 | Loss: 0.00002920
Iteration 27/1000 | Loss: 0.00002920
Iteration 28/1000 | Loss: 0.00002919
Iteration 29/1000 | Loss: 0.00002917
Iteration 30/1000 | Loss: 0.00002917
Iteration 31/1000 | Loss: 0.00002916
Iteration 32/1000 | Loss: 0.00002916
Iteration 33/1000 | Loss: 0.00002916
Iteration 34/1000 | Loss: 0.00002915
Iteration 35/1000 | Loss: 0.00002914
Iteration 36/1000 | Loss: 0.00002912
Iteration 37/1000 | Loss: 0.00002912
Iteration 38/1000 | Loss: 0.00002912
Iteration 39/1000 | Loss: 0.00002911
Iteration 40/1000 | Loss: 0.00002911
Iteration 41/1000 | Loss: 0.00002911
Iteration 42/1000 | Loss: 0.00002911
Iteration 43/1000 | Loss: 0.00002911
Iteration 44/1000 | Loss: 0.00002911
Iteration 45/1000 | Loss: 0.00002911
Iteration 46/1000 | Loss: 0.00002911
Iteration 47/1000 | Loss: 0.00002910
Iteration 48/1000 | Loss: 0.00002910
Iteration 49/1000 | Loss: 0.00002910
Iteration 50/1000 | Loss: 0.00002910
Iteration 51/1000 | Loss: 0.00002909
Iteration 52/1000 | Loss: 0.00002909
Iteration 53/1000 | Loss: 0.00002909
Iteration 54/1000 | Loss: 0.00002908
Iteration 55/1000 | Loss: 0.00002908
Iteration 56/1000 | Loss: 0.00002908
Iteration 57/1000 | Loss: 0.00002908
Iteration 58/1000 | Loss: 0.00002908
Iteration 59/1000 | Loss: 0.00002908
Iteration 60/1000 | Loss: 0.00002908
Iteration 61/1000 | Loss: 0.00002908
Iteration 62/1000 | Loss: 0.00002908
Iteration 63/1000 | Loss: 0.00002908
Iteration 64/1000 | Loss: 0.00002908
Iteration 65/1000 | Loss: 0.00002908
Iteration 66/1000 | Loss: 0.00002907
Iteration 67/1000 | Loss: 0.00002907
Iteration 68/1000 | Loss: 0.00002907
Iteration 69/1000 | Loss: 0.00002906
Iteration 70/1000 | Loss: 0.00002906
Iteration 71/1000 | Loss: 0.00002906
Iteration 72/1000 | Loss: 0.00002905
Iteration 73/1000 | Loss: 0.00002905
Iteration 74/1000 | Loss: 0.00002905
Iteration 75/1000 | Loss: 0.00002904
Iteration 76/1000 | Loss: 0.00002904
Iteration 77/1000 | Loss: 0.00002904
Iteration 78/1000 | Loss: 0.00002904
Iteration 79/1000 | Loss: 0.00002904
Iteration 80/1000 | Loss: 0.00002904
Iteration 81/1000 | Loss: 0.00002904
Iteration 82/1000 | Loss: 0.00002903
Iteration 83/1000 | Loss: 0.00002903
Iteration 84/1000 | Loss: 0.00002903
Iteration 85/1000 | Loss: 0.00002903
Iteration 86/1000 | Loss: 0.00002903
Iteration 87/1000 | Loss: 0.00002902
Iteration 88/1000 | Loss: 0.00002902
Iteration 89/1000 | Loss: 0.00002902
Iteration 90/1000 | Loss: 0.00002902
Iteration 91/1000 | Loss: 0.00002902
Iteration 92/1000 | Loss: 0.00002902
Iteration 93/1000 | Loss: 0.00002902
Iteration 94/1000 | Loss: 0.00002902
Iteration 95/1000 | Loss: 0.00002902
Iteration 96/1000 | Loss: 0.00002902
Iteration 97/1000 | Loss: 0.00002901
Iteration 98/1000 | Loss: 0.00002901
Iteration 99/1000 | Loss: 0.00002901
Iteration 100/1000 | Loss: 0.00002901
Iteration 101/1000 | Loss: 0.00002901
Iteration 102/1000 | Loss: 0.00002901
Iteration 103/1000 | Loss: 0.00002901
Iteration 104/1000 | Loss: 0.00002901
Iteration 105/1000 | Loss: 0.00002901
Iteration 106/1000 | Loss: 0.00002901
Iteration 107/1000 | Loss: 0.00002901
Iteration 108/1000 | Loss: 0.00002901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [2.9014776373514906e-05, 2.9014776373514906e-05, 2.9014776373514906e-05, 2.9014776373514906e-05, 2.9014776373514906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9014776373514906e-05

Optimization complete. Final v2v error: 4.511590957641602 mm

Highest mean error: 5.21956205368042 mm for frame 0

Lowest mean error: 3.9362809658050537 mm for frame 54

Saving results

Total time: 38.47807216644287
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01092652
Iteration 2/25 | Loss: 0.01092652
Iteration 3/25 | Loss: 0.01092651
Iteration 4/25 | Loss: 0.01092651
Iteration 5/25 | Loss: 0.01092651
Iteration 6/25 | Loss: 0.01092651
Iteration 7/25 | Loss: 0.01092651
Iteration 8/25 | Loss: 0.01092651
Iteration 9/25 | Loss: 0.01092651
Iteration 10/25 | Loss: 0.01092651
Iteration 11/25 | Loss: 0.01092651
Iteration 12/25 | Loss: 0.01092651
Iteration 13/25 | Loss: 0.01092651
Iteration 14/25 | Loss: 0.01092651
Iteration 15/25 | Loss: 0.01092651
Iteration 16/25 | Loss: 0.01092651
Iteration 17/25 | Loss: 0.01092650
Iteration 18/25 | Loss: 0.01092650
Iteration 19/25 | Loss: 0.01092650
Iteration 20/25 | Loss: 0.01092650
Iteration 21/25 | Loss: 0.01092650
Iteration 22/25 | Loss: 0.01092650
Iteration 23/25 | Loss: 0.01092650
Iteration 24/25 | Loss: 0.01092650
Iteration 25/25 | Loss: 0.01092650

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80497861
Iteration 2/25 | Loss: 0.07604617
Iteration 3/25 | Loss: 0.07599819
Iteration 4/25 | Loss: 0.07599817
Iteration 5/25 | Loss: 0.07599817
Iteration 6/25 | Loss: 0.07599817
Iteration 7/25 | Loss: 0.07599817
Iteration 8/25 | Loss: 0.07599816
Iteration 9/25 | Loss: 0.07599816
Iteration 10/25 | Loss: 0.07599816
Iteration 11/25 | Loss: 0.07599816
Iteration 12/25 | Loss: 0.07599816
Iteration 13/25 | Loss: 0.07599816
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.07599816471338272, 0.07599816471338272, 0.07599816471338272, 0.07599816471338272, 0.07599816471338272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07599816471338272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07599816
Iteration 2/1000 | Loss: 0.00051055
Iteration 3/1000 | Loss: 0.00015295
Iteration 4/1000 | Loss: 0.00006370
Iteration 5/1000 | Loss: 0.00003653
Iteration 6/1000 | Loss: 0.00002999
Iteration 7/1000 | Loss: 0.00002706
Iteration 8/1000 | Loss: 0.00002424
Iteration 9/1000 | Loss: 0.00002248
Iteration 10/1000 | Loss: 0.00002037
Iteration 11/1000 | Loss: 0.00001877
Iteration 12/1000 | Loss: 0.00001787
Iteration 13/1000 | Loss: 0.00001689
Iteration 14/1000 | Loss: 0.00001590
Iteration 15/1000 | Loss: 0.00001503
Iteration 16/1000 | Loss: 0.00001416
Iteration 17/1000 | Loss: 0.00001361
Iteration 18/1000 | Loss: 0.00001309
Iteration 19/1000 | Loss: 0.00001271
Iteration 20/1000 | Loss: 0.00001235
Iteration 21/1000 | Loss: 0.00001207
Iteration 22/1000 | Loss: 0.00001179
Iteration 23/1000 | Loss: 0.00001170
Iteration 24/1000 | Loss: 0.00001163
Iteration 25/1000 | Loss: 0.00001162
Iteration 26/1000 | Loss: 0.00001162
Iteration 27/1000 | Loss: 0.00001160
Iteration 28/1000 | Loss: 0.00001160
Iteration 29/1000 | Loss: 0.00001159
Iteration 30/1000 | Loss: 0.00001158
Iteration 31/1000 | Loss: 0.00001157
Iteration 32/1000 | Loss: 0.00001156
Iteration 33/1000 | Loss: 0.00001154
Iteration 34/1000 | Loss: 0.00001154
Iteration 35/1000 | Loss: 0.00001151
Iteration 36/1000 | Loss: 0.00001150
Iteration 37/1000 | Loss: 0.00001147
Iteration 38/1000 | Loss: 0.00001144
Iteration 39/1000 | Loss: 0.00001141
Iteration 40/1000 | Loss: 0.00001140
Iteration 41/1000 | Loss: 0.00001140
Iteration 42/1000 | Loss: 0.00001139
Iteration 43/1000 | Loss: 0.00001137
Iteration 44/1000 | Loss: 0.00001137
Iteration 45/1000 | Loss: 0.00001137
Iteration 46/1000 | Loss: 0.00001136
Iteration 47/1000 | Loss: 0.00001136
Iteration 48/1000 | Loss: 0.00001136
Iteration 49/1000 | Loss: 0.00001136
Iteration 50/1000 | Loss: 0.00001136
Iteration 51/1000 | Loss: 0.00001136
Iteration 52/1000 | Loss: 0.00001135
Iteration 53/1000 | Loss: 0.00001135
Iteration 54/1000 | Loss: 0.00001134
Iteration 55/1000 | Loss: 0.00001134
Iteration 56/1000 | Loss: 0.00001133
Iteration 57/1000 | Loss: 0.00001132
Iteration 58/1000 | Loss: 0.00001132
Iteration 59/1000 | Loss: 0.00001132
Iteration 60/1000 | Loss: 0.00001132
Iteration 61/1000 | Loss: 0.00001132
Iteration 62/1000 | Loss: 0.00001132
Iteration 63/1000 | Loss: 0.00001131
Iteration 64/1000 | Loss: 0.00001131
Iteration 65/1000 | Loss: 0.00001131
Iteration 66/1000 | Loss: 0.00001131
Iteration 67/1000 | Loss: 0.00001131
Iteration 68/1000 | Loss: 0.00001131
Iteration 69/1000 | Loss: 0.00001131
Iteration 70/1000 | Loss: 0.00001131
Iteration 71/1000 | Loss: 0.00001131
Iteration 72/1000 | Loss: 0.00001131
Iteration 73/1000 | Loss: 0.00001131
Iteration 74/1000 | Loss: 0.00001130
Iteration 75/1000 | Loss: 0.00001130
Iteration 76/1000 | Loss: 0.00001130
Iteration 77/1000 | Loss: 0.00001130
Iteration 78/1000 | Loss: 0.00001130
Iteration 79/1000 | Loss: 0.00001130
Iteration 80/1000 | Loss: 0.00001129
Iteration 81/1000 | Loss: 0.00001129
Iteration 82/1000 | Loss: 0.00001129
Iteration 83/1000 | Loss: 0.00001129
Iteration 84/1000 | Loss: 0.00001129
Iteration 85/1000 | Loss: 0.00001128
Iteration 86/1000 | Loss: 0.00001128
Iteration 87/1000 | Loss: 0.00001128
Iteration 88/1000 | Loss: 0.00001128
Iteration 89/1000 | Loss: 0.00001128
Iteration 90/1000 | Loss: 0.00001128
Iteration 91/1000 | Loss: 0.00001128
Iteration 92/1000 | Loss: 0.00001128
Iteration 93/1000 | Loss: 0.00001128
Iteration 94/1000 | Loss: 0.00001127
Iteration 95/1000 | Loss: 0.00001127
Iteration 96/1000 | Loss: 0.00001127
Iteration 97/1000 | Loss: 0.00001127
Iteration 98/1000 | Loss: 0.00001127
Iteration 99/1000 | Loss: 0.00001127
Iteration 100/1000 | Loss: 0.00001127
Iteration 101/1000 | Loss: 0.00001127
Iteration 102/1000 | Loss: 0.00001127
Iteration 103/1000 | Loss: 0.00001127
Iteration 104/1000 | Loss: 0.00001127
Iteration 105/1000 | Loss: 0.00001127
Iteration 106/1000 | Loss: 0.00001127
Iteration 107/1000 | Loss: 0.00001127
Iteration 108/1000 | Loss: 0.00001127
Iteration 109/1000 | Loss: 0.00001126
Iteration 110/1000 | Loss: 0.00001126
Iteration 111/1000 | Loss: 0.00001126
Iteration 112/1000 | Loss: 0.00001126
Iteration 113/1000 | Loss: 0.00001126
Iteration 114/1000 | Loss: 0.00001126
Iteration 115/1000 | Loss: 0.00001126
Iteration 116/1000 | Loss: 0.00001126
Iteration 117/1000 | Loss: 0.00001126
Iteration 118/1000 | Loss: 0.00001126
Iteration 119/1000 | Loss: 0.00001126
Iteration 120/1000 | Loss: 0.00001126
Iteration 121/1000 | Loss: 0.00001126
Iteration 122/1000 | Loss: 0.00001126
Iteration 123/1000 | Loss: 0.00001126
Iteration 124/1000 | Loss: 0.00001126
Iteration 125/1000 | Loss: 0.00001126
Iteration 126/1000 | Loss: 0.00001125
Iteration 127/1000 | Loss: 0.00001125
Iteration 128/1000 | Loss: 0.00001125
Iteration 129/1000 | Loss: 0.00001125
Iteration 130/1000 | Loss: 0.00001125
Iteration 131/1000 | Loss: 0.00001125
Iteration 132/1000 | Loss: 0.00001125
Iteration 133/1000 | Loss: 0.00001125
Iteration 134/1000 | Loss: 0.00001125
Iteration 135/1000 | Loss: 0.00001125
Iteration 136/1000 | Loss: 0.00001125
Iteration 137/1000 | Loss: 0.00001125
Iteration 138/1000 | Loss: 0.00001125
Iteration 139/1000 | Loss: 0.00001124
Iteration 140/1000 | Loss: 0.00001124
Iteration 141/1000 | Loss: 0.00001124
Iteration 142/1000 | Loss: 0.00001124
Iteration 143/1000 | Loss: 0.00001124
Iteration 144/1000 | Loss: 0.00001124
Iteration 145/1000 | Loss: 0.00001124
Iteration 146/1000 | Loss: 0.00001124
Iteration 147/1000 | Loss: 0.00001124
Iteration 148/1000 | Loss: 0.00001124
Iteration 149/1000 | Loss: 0.00001123
Iteration 150/1000 | Loss: 0.00001123
Iteration 151/1000 | Loss: 0.00001123
Iteration 152/1000 | Loss: 0.00001123
Iteration 153/1000 | Loss: 0.00001123
Iteration 154/1000 | Loss: 0.00001123
Iteration 155/1000 | Loss: 0.00001123
Iteration 156/1000 | Loss: 0.00001123
Iteration 157/1000 | Loss: 0.00001123
Iteration 158/1000 | Loss: 0.00001123
Iteration 159/1000 | Loss: 0.00001123
Iteration 160/1000 | Loss: 0.00001123
Iteration 161/1000 | Loss: 0.00001123
Iteration 162/1000 | Loss: 0.00001123
Iteration 163/1000 | Loss: 0.00001123
Iteration 164/1000 | Loss: 0.00001123
Iteration 165/1000 | Loss: 0.00001123
Iteration 166/1000 | Loss: 0.00001122
Iteration 167/1000 | Loss: 0.00001122
Iteration 168/1000 | Loss: 0.00001122
Iteration 169/1000 | Loss: 0.00001122
Iteration 170/1000 | Loss: 0.00001122
Iteration 171/1000 | Loss: 0.00001122
Iteration 172/1000 | Loss: 0.00001122
Iteration 173/1000 | Loss: 0.00001122
Iteration 174/1000 | Loss: 0.00001122
Iteration 175/1000 | Loss: 0.00001122
Iteration 176/1000 | Loss: 0.00001122
Iteration 177/1000 | Loss: 0.00001122
Iteration 178/1000 | Loss: 0.00001122
Iteration 179/1000 | Loss: 0.00001122
Iteration 180/1000 | Loss: 0.00001122
Iteration 181/1000 | Loss: 0.00001122
Iteration 182/1000 | Loss: 0.00001122
Iteration 183/1000 | Loss: 0.00001122
Iteration 184/1000 | Loss: 0.00001122
Iteration 185/1000 | Loss: 0.00001122
Iteration 186/1000 | Loss: 0.00001122
Iteration 187/1000 | Loss: 0.00001122
Iteration 188/1000 | Loss: 0.00001122
Iteration 189/1000 | Loss: 0.00001122
Iteration 190/1000 | Loss: 0.00001122
Iteration 191/1000 | Loss: 0.00001122
Iteration 192/1000 | Loss: 0.00001122
Iteration 193/1000 | Loss: 0.00001122
Iteration 194/1000 | Loss: 0.00001122
Iteration 195/1000 | Loss: 0.00001122
Iteration 196/1000 | Loss: 0.00001122
Iteration 197/1000 | Loss: 0.00001122
Iteration 198/1000 | Loss: 0.00001122
Iteration 199/1000 | Loss: 0.00001122
Iteration 200/1000 | Loss: 0.00001122
Iteration 201/1000 | Loss: 0.00001122
Iteration 202/1000 | Loss: 0.00001122
Iteration 203/1000 | Loss: 0.00001122
Iteration 204/1000 | Loss: 0.00001122
Iteration 205/1000 | Loss: 0.00001122
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.1221252862014808e-05, 1.1221252862014808e-05, 1.1221252862014808e-05, 1.1221252862014808e-05, 1.1221252862014808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1221252862014808e-05

Optimization complete. Final v2v error: 2.866976261138916 mm

Highest mean error: 3.053651809692383 mm for frame 2

Lowest mean error: 2.712332248687744 mm for frame 184

Saving results

Total time: 53.21911549568176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00482744
Iteration 2/25 | Loss: 0.00086202
Iteration 3/25 | Loss: 0.00075217
Iteration 4/25 | Loss: 0.00073768
Iteration 5/25 | Loss: 0.00073366
Iteration 6/25 | Loss: 0.00073282
Iteration 7/25 | Loss: 0.00073282
Iteration 8/25 | Loss: 0.00073282
Iteration 9/25 | Loss: 0.00073282
Iteration 10/25 | Loss: 0.00073282
Iteration 11/25 | Loss: 0.00073282
Iteration 12/25 | Loss: 0.00073282
Iteration 13/25 | Loss: 0.00073282
Iteration 14/25 | Loss: 0.00073282
Iteration 15/25 | Loss: 0.00073282
Iteration 16/25 | Loss: 0.00073282
Iteration 17/25 | Loss: 0.00073282
Iteration 18/25 | Loss: 0.00073282
Iteration 19/25 | Loss: 0.00073282
Iteration 20/25 | Loss: 0.00073282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000732815416995436, 0.000732815416995436, 0.000732815416995436, 0.000732815416995436, 0.000732815416995436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000732815416995436

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.86493540
Iteration 2/25 | Loss: 0.00045141
Iteration 3/25 | Loss: 0.00045140
Iteration 4/25 | Loss: 0.00045140
Iteration 5/25 | Loss: 0.00045140
Iteration 6/25 | Loss: 0.00045140
Iteration 7/25 | Loss: 0.00045140
Iteration 8/25 | Loss: 0.00045140
Iteration 9/25 | Loss: 0.00045140
Iteration 10/25 | Loss: 0.00045140
Iteration 11/25 | Loss: 0.00045140
Iteration 12/25 | Loss: 0.00045140
Iteration 13/25 | Loss: 0.00045140
Iteration 14/25 | Loss: 0.00045140
Iteration 15/25 | Loss: 0.00045140
Iteration 16/25 | Loss: 0.00045140
Iteration 17/25 | Loss: 0.00045140
Iteration 18/25 | Loss: 0.00045140
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00045139697613194585, 0.00045139697613194585, 0.00045139697613194585, 0.00045139697613194585, 0.00045139697613194585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045139697613194585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045140
Iteration 2/1000 | Loss: 0.00002325
Iteration 3/1000 | Loss: 0.00001595
Iteration 4/1000 | Loss: 0.00001500
Iteration 5/1000 | Loss: 0.00001410
Iteration 6/1000 | Loss: 0.00001376
Iteration 7/1000 | Loss: 0.00001343
Iteration 8/1000 | Loss: 0.00001325
Iteration 9/1000 | Loss: 0.00001301
Iteration 10/1000 | Loss: 0.00001285
Iteration 11/1000 | Loss: 0.00001282
Iteration 12/1000 | Loss: 0.00001282
Iteration 13/1000 | Loss: 0.00001276
Iteration 14/1000 | Loss: 0.00001275
Iteration 15/1000 | Loss: 0.00001273
Iteration 16/1000 | Loss: 0.00001272
Iteration 17/1000 | Loss: 0.00001270
Iteration 18/1000 | Loss: 0.00001270
Iteration 19/1000 | Loss: 0.00001269
Iteration 20/1000 | Loss: 0.00001267
Iteration 21/1000 | Loss: 0.00001267
Iteration 22/1000 | Loss: 0.00001267
Iteration 23/1000 | Loss: 0.00001266
Iteration 24/1000 | Loss: 0.00001266
Iteration 25/1000 | Loss: 0.00001265
Iteration 26/1000 | Loss: 0.00001265
Iteration 27/1000 | Loss: 0.00001265
Iteration 28/1000 | Loss: 0.00001264
Iteration 29/1000 | Loss: 0.00001264
Iteration 30/1000 | Loss: 0.00001263
Iteration 31/1000 | Loss: 0.00001262
Iteration 32/1000 | Loss: 0.00001262
Iteration 33/1000 | Loss: 0.00001262
Iteration 34/1000 | Loss: 0.00001261
Iteration 35/1000 | Loss: 0.00001261
Iteration 36/1000 | Loss: 0.00001261
Iteration 37/1000 | Loss: 0.00001260
Iteration 38/1000 | Loss: 0.00001260
Iteration 39/1000 | Loss: 0.00001260
Iteration 40/1000 | Loss: 0.00001259
Iteration 41/1000 | Loss: 0.00001259
Iteration 42/1000 | Loss: 0.00001259
Iteration 43/1000 | Loss: 0.00001258
Iteration 44/1000 | Loss: 0.00001258
Iteration 45/1000 | Loss: 0.00001258
Iteration 46/1000 | Loss: 0.00001258
Iteration 47/1000 | Loss: 0.00001257
Iteration 48/1000 | Loss: 0.00001257
Iteration 49/1000 | Loss: 0.00001257
Iteration 50/1000 | Loss: 0.00001257
Iteration 51/1000 | Loss: 0.00001257
Iteration 52/1000 | Loss: 0.00001256
Iteration 53/1000 | Loss: 0.00001256
Iteration 54/1000 | Loss: 0.00001256
Iteration 55/1000 | Loss: 0.00001256
Iteration 56/1000 | Loss: 0.00001256
Iteration 57/1000 | Loss: 0.00001255
Iteration 58/1000 | Loss: 0.00001255
Iteration 59/1000 | Loss: 0.00001255
Iteration 60/1000 | Loss: 0.00001255
Iteration 61/1000 | Loss: 0.00001254
Iteration 62/1000 | Loss: 0.00001254
Iteration 63/1000 | Loss: 0.00001254
Iteration 64/1000 | Loss: 0.00001253
Iteration 65/1000 | Loss: 0.00001253
Iteration 66/1000 | Loss: 0.00001252
Iteration 67/1000 | Loss: 0.00001252
Iteration 68/1000 | Loss: 0.00001252
Iteration 69/1000 | Loss: 0.00001251
Iteration 70/1000 | Loss: 0.00001251
Iteration 71/1000 | Loss: 0.00001251
Iteration 72/1000 | Loss: 0.00001250
Iteration 73/1000 | Loss: 0.00001250
Iteration 74/1000 | Loss: 0.00001248
Iteration 75/1000 | Loss: 0.00001248
Iteration 76/1000 | Loss: 0.00001248
Iteration 77/1000 | Loss: 0.00001248
Iteration 78/1000 | Loss: 0.00001247
Iteration 79/1000 | Loss: 0.00001247
Iteration 80/1000 | Loss: 0.00001247
Iteration 81/1000 | Loss: 0.00001247
Iteration 82/1000 | Loss: 0.00001247
Iteration 83/1000 | Loss: 0.00001247
Iteration 84/1000 | Loss: 0.00001247
Iteration 85/1000 | Loss: 0.00001247
Iteration 86/1000 | Loss: 0.00001247
Iteration 87/1000 | Loss: 0.00001247
Iteration 88/1000 | Loss: 0.00001247
Iteration 89/1000 | Loss: 0.00001247
Iteration 90/1000 | Loss: 0.00001247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.2474019058572594e-05, 1.2474019058572594e-05, 1.2474019058572594e-05, 1.2474019058572594e-05, 1.2474019058572594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2474019058572594e-05

Optimization complete. Final v2v error: 3.0129940509796143 mm

Highest mean error: 3.4297516345977783 mm for frame 190

Lowest mean error: 2.7813658714294434 mm for frame 217

Saving results

Total time: 34.7687931060791
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435806
Iteration 2/25 | Loss: 0.00090502
Iteration 3/25 | Loss: 0.00079852
Iteration 4/25 | Loss: 0.00076426
Iteration 5/25 | Loss: 0.00075161
Iteration 6/25 | Loss: 0.00074966
Iteration 7/25 | Loss: 0.00074895
Iteration 8/25 | Loss: 0.00074895
Iteration 9/25 | Loss: 0.00074895
Iteration 10/25 | Loss: 0.00074895
Iteration 11/25 | Loss: 0.00074895
Iteration 12/25 | Loss: 0.00074895
Iteration 13/25 | Loss: 0.00074895
Iteration 14/25 | Loss: 0.00074895
Iteration 15/25 | Loss: 0.00074895
Iteration 16/25 | Loss: 0.00074895
Iteration 17/25 | Loss: 0.00074895
Iteration 18/25 | Loss: 0.00074895
Iteration 19/25 | Loss: 0.00074895
Iteration 20/25 | Loss: 0.00074895
Iteration 21/25 | Loss: 0.00074895
Iteration 22/25 | Loss: 0.00074895
Iteration 23/25 | Loss: 0.00074895
Iteration 24/25 | Loss: 0.00074895
Iteration 25/25 | Loss: 0.00074895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55514038
Iteration 2/25 | Loss: 0.00045486
Iteration 3/25 | Loss: 0.00045486
Iteration 4/25 | Loss: 0.00045486
Iteration 5/25 | Loss: 0.00045486
Iteration 6/25 | Loss: 0.00045486
Iteration 7/25 | Loss: 0.00045486
Iteration 8/25 | Loss: 0.00045486
Iteration 9/25 | Loss: 0.00045486
Iteration 10/25 | Loss: 0.00045486
Iteration 11/25 | Loss: 0.00045486
Iteration 12/25 | Loss: 0.00045486
Iteration 13/25 | Loss: 0.00045486
Iteration 14/25 | Loss: 0.00045486
Iteration 15/25 | Loss: 0.00045486
Iteration 16/25 | Loss: 0.00045486
Iteration 17/25 | Loss: 0.00045486
Iteration 18/25 | Loss: 0.00045486
Iteration 19/25 | Loss: 0.00045486
Iteration 20/25 | Loss: 0.00045486
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0004548551223706454, 0.0004548551223706454, 0.0004548551223706454, 0.0004548551223706454, 0.0004548551223706454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004548551223706454

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045486
Iteration 2/1000 | Loss: 0.00003244
Iteration 3/1000 | Loss: 0.00002019
Iteration 4/1000 | Loss: 0.00001887
Iteration 5/1000 | Loss: 0.00001784
Iteration 6/1000 | Loss: 0.00001750
Iteration 7/1000 | Loss: 0.00001702
Iteration 8/1000 | Loss: 0.00001679
Iteration 9/1000 | Loss: 0.00001669
Iteration 10/1000 | Loss: 0.00001655
Iteration 11/1000 | Loss: 0.00001643
Iteration 12/1000 | Loss: 0.00001641
Iteration 13/1000 | Loss: 0.00001641
Iteration 14/1000 | Loss: 0.00001637
Iteration 15/1000 | Loss: 0.00001637
Iteration 16/1000 | Loss: 0.00001637
Iteration 17/1000 | Loss: 0.00001637
Iteration 18/1000 | Loss: 0.00001634
Iteration 19/1000 | Loss: 0.00001631
Iteration 20/1000 | Loss: 0.00001631
Iteration 21/1000 | Loss: 0.00001630
Iteration 22/1000 | Loss: 0.00001630
Iteration 23/1000 | Loss: 0.00001630
Iteration 24/1000 | Loss: 0.00001630
Iteration 25/1000 | Loss: 0.00001629
Iteration 26/1000 | Loss: 0.00001629
Iteration 27/1000 | Loss: 0.00001628
Iteration 28/1000 | Loss: 0.00001628
Iteration 29/1000 | Loss: 0.00001628
Iteration 30/1000 | Loss: 0.00001627
Iteration 31/1000 | Loss: 0.00001627
Iteration 32/1000 | Loss: 0.00001627
Iteration 33/1000 | Loss: 0.00001626
Iteration 34/1000 | Loss: 0.00001626
Iteration 35/1000 | Loss: 0.00001625
Iteration 36/1000 | Loss: 0.00001624
Iteration 37/1000 | Loss: 0.00001624
Iteration 38/1000 | Loss: 0.00001624
Iteration 39/1000 | Loss: 0.00001624
Iteration 40/1000 | Loss: 0.00001624
Iteration 41/1000 | Loss: 0.00001623
Iteration 42/1000 | Loss: 0.00001623
Iteration 43/1000 | Loss: 0.00001623
Iteration 44/1000 | Loss: 0.00001623
Iteration 45/1000 | Loss: 0.00001623
Iteration 46/1000 | Loss: 0.00001623
Iteration 47/1000 | Loss: 0.00001622
Iteration 48/1000 | Loss: 0.00001622
Iteration 49/1000 | Loss: 0.00001622
Iteration 50/1000 | Loss: 0.00001621
Iteration 51/1000 | Loss: 0.00001621
Iteration 52/1000 | Loss: 0.00001621
Iteration 53/1000 | Loss: 0.00001621
Iteration 54/1000 | Loss: 0.00001620
Iteration 55/1000 | Loss: 0.00001620
Iteration 56/1000 | Loss: 0.00001620
Iteration 57/1000 | Loss: 0.00001620
Iteration 58/1000 | Loss: 0.00001620
Iteration 59/1000 | Loss: 0.00001619
Iteration 60/1000 | Loss: 0.00001619
Iteration 61/1000 | Loss: 0.00001619
Iteration 62/1000 | Loss: 0.00001619
Iteration 63/1000 | Loss: 0.00001618
Iteration 64/1000 | Loss: 0.00001618
Iteration 65/1000 | Loss: 0.00001617
Iteration 66/1000 | Loss: 0.00001616
Iteration 67/1000 | Loss: 0.00001616
Iteration 68/1000 | Loss: 0.00001616
Iteration 69/1000 | Loss: 0.00001616
Iteration 70/1000 | Loss: 0.00001616
Iteration 71/1000 | Loss: 0.00001615
Iteration 72/1000 | Loss: 0.00001615
Iteration 73/1000 | Loss: 0.00001615
Iteration 74/1000 | Loss: 0.00001614
Iteration 75/1000 | Loss: 0.00001612
Iteration 76/1000 | Loss: 0.00001612
Iteration 77/1000 | Loss: 0.00001612
Iteration 78/1000 | Loss: 0.00001611
Iteration 79/1000 | Loss: 0.00001611
Iteration 80/1000 | Loss: 0.00001610
Iteration 81/1000 | Loss: 0.00001609
Iteration 82/1000 | Loss: 0.00001609
Iteration 83/1000 | Loss: 0.00001608
Iteration 84/1000 | Loss: 0.00001608
Iteration 85/1000 | Loss: 0.00001608
Iteration 86/1000 | Loss: 0.00001608
Iteration 87/1000 | Loss: 0.00001608
Iteration 88/1000 | Loss: 0.00001607
Iteration 89/1000 | Loss: 0.00001607
Iteration 90/1000 | Loss: 0.00001606
Iteration 91/1000 | Loss: 0.00001606
Iteration 92/1000 | Loss: 0.00001605
Iteration 93/1000 | Loss: 0.00001605
Iteration 94/1000 | Loss: 0.00001605
Iteration 95/1000 | Loss: 0.00001605
Iteration 96/1000 | Loss: 0.00001605
Iteration 97/1000 | Loss: 0.00001604
Iteration 98/1000 | Loss: 0.00001604
Iteration 99/1000 | Loss: 0.00001604
Iteration 100/1000 | Loss: 0.00001604
Iteration 101/1000 | Loss: 0.00001604
Iteration 102/1000 | Loss: 0.00001604
Iteration 103/1000 | Loss: 0.00001604
Iteration 104/1000 | Loss: 0.00001603
Iteration 105/1000 | Loss: 0.00001603
Iteration 106/1000 | Loss: 0.00001603
Iteration 107/1000 | Loss: 0.00001603
Iteration 108/1000 | Loss: 0.00001603
Iteration 109/1000 | Loss: 0.00001602
Iteration 110/1000 | Loss: 0.00001602
Iteration 111/1000 | Loss: 0.00001602
Iteration 112/1000 | Loss: 0.00001602
Iteration 113/1000 | Loss: 0.00001602
Iteration 114/1000 | Loss: 0.00001602
Iteration 115/1000 | Loss: 0.00001602
Iteration 116/1000 | Loss: 0.00001602
Iteration 117/1000 | Loss: 0.00001602
Iteration 118/1000 | Loss: 0.00001602
Iteration 119/1000 | Loss: 0.00001602
Iteration 120/1000 | Loss: 0.00001602
Iteration 121/1000 | Loss: 0.00001602
Iteration 122/1000 | Loss: 0.00001602
Iteration 123/1000 | Loss: 0.00001602
Iteration 124/1000 | Loss: 0.00001601
Iteration 125/1000 | Loss: 0.00001601
Iteration 126/1000 | Loss: 0.00001601
Iteration 127/1000 | Loss: 0.00001601
Iteration 128/1000 | Loss: 0.00001601
Iteration 129/1000 | Loss: 0.00001601
Iteration 130/1000 | Loss: 0.00001601
Iteration 131/1000 | Loss: 0.00001601
Iteration 132/1000 | Loss: 0.00001601
Iteration 133/1000 | Loss: 0.00001601
Iteration 134/1000 | Loss: 0.00001601
Iteration 135/1000 | Loss: 0.00001601
Iteration 136/1000 | Loss: 0.00001601
Iteration 137/1000 | Loss: 0.00001600
Iteration 138/1000 | Loss: 0.00001600
Iteration 139/1000 | Loss: 0.00001600
Iteration 140/1000 | Loss: 0.00001600
Iteration 141/1000 | Loss: 0.00001600
Iteration 142/1000 | Loss: 0.00001600
Iteration 143/1000 | Loss: 0.00001600
Iteration 144/1000 | Loss: 0.00001600
Iteration 145/1000 | Loss: 0.00001600
Iteration 146/1000 | Loss: 0.00001600
Iteration 147/1000 | Loss: 0.00001600
Iteration 148/1000 | Loss: 0.00001600
Iteration 149/1000 | Loss: 0.00001600
Iteration 150/1000 | Loss: 0.00001600
Iteration 151/1000 | Loss: 0.00001600
Iteration 152/1000 | Loss: 0.00001600
Iteration 153/1000 | Loss: 0.00001599
Iteration 154/1000 | Loss: 0.00001599
Iteration 155/1000 | Loss: 0.00001599
Iteration 156/1000 | Loss: 0.00001599
Iteration 157/1000 | Loss: 0.00001599
Iteration 158/1000 | Loss: 0.00001599
Iteration 159/1000 | Loss: 0.00001599
Iteration 160/1000 | Loss: 0.00001599
Iteration 161/1000 | Loss: 0.00001599
Iteration 162/1000 | Loss: 0.00001599
Iteration 163/1000 | Loss: 0.00001599
Iteration 164/1000 | Loss: 0.00001599
Iteration 165/1000 | Loss: 0.00001599
Iteration 166/1000 | Loss: 0.00001599
Iteration 167/1000 | Loss: 0.00001599
Iteration 168/1000 | Loss: 0.00001599
Iteration 169/1000 | Loss: 0.00001599
Iteration 170/1000 | Loss: 0.00001599
Iteration 171/1000 | Loss: 0.00001599
Iteration 172/1000 | Loss: 0.00001599
Iteration 173/1000 | Loss: 0.00001599
Iteration 174/1000 | Loss: 0.00001599
Iteration 175/1000 | Loss: 0.00001599
Iteration 176/1000 | Loss: 0.00001599
Iteration 177/1000 | Loss: 0.00001599
Iteration 178/1000 | Loss: 0.00001599
Iteration 179/1000 | Loss: 0.00001599
Iteration 180/1000 | Loss: 0.00001599
Iteration 181/1000 | Loss: 0.00001599
Iteration 182/1000 | Loss: 0.00001599
Iteration 183/1000 | Loss: 0.00001599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.5991050531738438e-05, 1.5991050531738438e-05, 1.5991050531738438e-05, 1.5991050531738438e-05, 1.5991050531738438e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5991050531738438e-05

Optimization complete. Final v2v error: 3.391510486602783 mm

Highest mean error: 3.9640209674835205 mm for frame 77

Lowest mean error: 3.25020170211792 mm for frame 62

Saving results

Total time: 38.36446738243103
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_035/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_035/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00601226
Iteration 2/25 | Loss: 0.00112741
Iteration 3/25 | Loss: 0.00095112
Iteration 4/25 | Loss: 0.00090465
Iteration 5/25 | Loss: 0.00088579
Iteration 6/25 | Loss: 0.00088062
Iteration 7/25 | Loss: 0.00087889
Iteration 8/25 | Loss: 0.00087861
Iteration 9/25 | Loss: 0.00087861
Iteration 10/25 | Loss: 0.00087861
Iteration 11/25 | Loss: 0.00087861
Iteration 12/25 | Loss: 0.00087861
Iteration 13/25 | Loss: 0.00087861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008786128601059318, 0.0008786128601059318, 0.0008786128601059318, 0.0008786128601059318, 0.0008786128601059318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008786128601059318

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10990489
Iteration 2/25 | Loss: 0.00059370
Iteration 3/25 | Loss: 0.00059361
Iteration 4/25 | Loss: 0.00059361
Iteration 5/25 | Loss: 0.00059361
Iteration 6/25 | Loss: 0.00059361
Iteration 7/25 | Loss: 0.00059361
Iteration 8/25 | Loss: 0.00059361
Iteration 9/25 | Loss: 0.00059361
Iteration 10/25 | Loss: 0.00059361
Iteration 11/25 | Loss: 0.00059361
Iteration 12/25 | Loss: 0.00059361
Iteration 13/25 | Loss: 0.00059361
Iteration 14/25 | Loss: 0.00059361
Iteration 15/25 | Loss: 0.00059361
Iteration 16/25 | Loss: 0.00059361
Iteration 17/25 | Loss: 0.00059361
Iteration 18/25 | Loss: 0.00059361
Iteration 19/25 | Loss: 0.00059361
Iteration 20/25 | Loss: 0.00059361
Iteration 21/25 | Loss: 0.00059361
Iteration 22/25 | Loss: 0.00059361
Iteration 23/25 | Loss: 0.00059361
Iteration 24/25 | Loss: 0.00059361
Iteration 25/25 | Loss: 0.00059361

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059361
Iteration 2/1000 | Loss: 0.00006008
Iteration 3/1000 | Loss: 0.00003871
Iteration 4/1000 | Loss: 0.00003176
Iteration 5/1000 | Loss: 0.00003015
Iteration 6/1000 | Loss: 0.00002910
Iteration 7/1000 | Loss: 0.00002850
Iteration 8/1000 | Loss: 0.00002801
Iteration 9/1000 | Loss: 0.00002761
Iteration 10/1000 | Loss: 0.00002739
Iteration 11/1000 | Loss: 0.00002713
Iteration 12/1000 | Loss: 0.00002713
Iteration 13/1000 | Loss: 0.00002705
Iteration 14/1000 | Loss: 0.00002704
Iteration 15/1000 | Loss: 0.00002696
Iteration 16/1000 | Loss: 0.00002687
Iteration 17/1000 | Loss: 0.00002686
Iteration 18/1000 | Loss: 0.00002674
Iteration 19/1000 | Loss: 0.00002668
Iteration 20/1000 | Loss: 0.00002668
Iteration 21/1000 | Loss: 0.00002663
Iteration 22/1000 | Loss: 0.00002663
Iteration 23/1000 | Loss: 0.00002663
Iteration 24/1000 | Loss: 0.00002663
Iteration 25/1000 | Loss: 0.00002663
Iteration 26/1000 | Loss: 0.00002663
Iteration 27/1000 | Loss: 0.00002663
Iteration 28/1000 | Loss: 0.00002663
Iteration 29/1000 | Loss: 0.00002663
Iteration 30/1000 | Loss: 0.00002662
Iteration 31/1000 | Loss: 0.00002662
Iteration 32/1000 | Loss: 0.00002662
Iteration 33/1000 | Loss: 0.00002662
Iteration 34/1000 | Loss: 0.00002661
Iteration 35/1000 | Loss: 0.00002659
Iteration 36/1000 | Loss: 0.00002659
Iteration 37/1000 | Loss: 0.00002659
Iteration 38/1000 | Loss: 0.00002658
Iteration 39/1000 | Loss: 0.00002658
Iteration 40/1000 | Loss: 0.00002657
Iteration 41/1000 | Loss: 0.00002657
Iteration 42/1000 | Loss: 0.00002657
Iteration 43/1000 | Loss: 0.00002657
Iteration 44/1000 | Loss: 0.00002656
Iteration 45/1000 | Loss: 0.00002655
Iteration 46/1000 | Loss: 0.00002655
Iteration 47/1000 | Loss: 0.00002654
Iteration 48/1000 | Loss: 0.00002653
Iteration 49/1000 | Loss: 0.00002653
Iteration 50/1000 | Loss: 0.00002653
Iteration 51/1000 | Loss: 0.00002652
Iteration 52/1000 | Loss: 0.00002652
Iteration 53/1000 | Loss: 0.00002652
Iteration 54/1000 | Loss: 0.00002646
Iteration 55/1000 | Loss: 0.00002646
Iteration 56/1000 | Loss: 0.00002646
Iteration 57/1000 | Loss: 0.00002646
Iteration 58/1000 | Loss: 0.00002645
Iteration 59/1000 | Loss: 0.00002645
Iteration 60/1000 | Loss: 0.00002645
Iteration 61/1000 | Loss: 0.00002645
Iteration 62/1000 | Loss: 0.00002645
Iteration 63/1000 | Loss: 0.00002644
Iteration 64/1000 | Loss: 0.00002644
Iteration 65/1000 | Loss: 0.00002641
Iteration 66/1000 | Loss: 0.00002641
Iteration 67/1000 | Loss: 0.00002639
Iteration 68/1000 | Loss: 0.00002639
Iteration 69/1000 | Loss: 0.00002638
Iteration 70/1000 | Loss: 0.00002638
Iteration 71/1000 | Loss: 0.00002638
Iteration 72/1000 | Loss: 0.00002638
Iteration 73/1000 | Loss: 0.00002638
Iteration 74/1000 | Loss: 0.00002637
Iteration 75/1000 | Loss: 0.00002637
Iteration 76/1000 | Loss: 0.00002634
Iteration 77/1000 | Loss: 0.00002633
Iteration 78/1000 | Loss: 0.00002633
Iteration 79/1000 | Loss: 0.00002633
Iteration 80/1000 | Loss: 0.00002633
Iteration 81/1000 | Loss: 0.00002633
Iteration 82/1000 | Loss: 0.00002633
Iteration 83/1000 | Loss: 0.00002633
Iteration 84/1000 | Loss: 0.00002632
Iteration 85/1000 | Loss: 0.00002632
Iteration 86/1000 | Loss: 0.00002632
Iteration 87/1000 | Loss: 0.00002632
Iteration 88/1000 | Loss: 0.00002631
Iteration 89/1000 | Loss: 0.00002631
Iteration 90/1000 | Loss: 0.00002630
Iteration 91/1000 | Loss: 0.00002630
Iteration 92/1000 | Loss: 0.00002630
Iteration 93/1000 | Loss: 0.00002630
Iteration 94/1000 | Loss: 0.00002629
Iteration 95/1000 | Loss: 0.00002629
Iteration 96/1000 | Loss: 0.00002629
Iteration 97/1000 | Loss: 0.00002629
Iteration 98/1000 | Loss: 0.00002629
Iteration 99/1000 | Loss: 0.00002629
Iteration 100/1000 | Loss: 0.00002629
Iteration 101/1000 | Loss: 0.00002629
Iteration 102/1000 | Loss: 0.00002629
Iteration 103/1000 | Loss: 0.00002628
Iteration 104/1000 | Loss: 0.00002628
Iteration 105/1000 | Loss: 0.00002628
Iteration 106/1000 | Loss: 0.00002628
Iteration 107/1000 | Loss: 0.00002628
Iteration 108/1000 | Loss: 0.00002626
Iteration 109/1000 | Loss: 0.00002626
Iteration 110/1000 | Loss: 0.00002626
Iteration 111/1000 | Loss: 0.00002626
Iteration 112/1000 | Loss: 0.00002625
Iteration 113/1000 | Loss: 0.00002625
Iteration 114/1000 | Loss: 0.00002625
Iteration 115/1000 | Loss: 0.00002625
Iteration 116/1000 | Loss: 0.00002625
Iteration 117/1000 | Loss: 0.00002625
Iteration 118/1000 | Loss: 0.00002625
Iteration 119/1000 | Loss: 0.00002625
Iteration 120/1000 | Loss: 0.00002625
Iteration 121/1000 | Loss: 0.00002625
Iteration 122/1000 | Loss: 0.00002624
Iteration 123/1000 | Loss: 0.00002624
Iteration 124/1000 | Loss: 0.00002624
Iteration 125/1000 | Loss: 0.00002624
Iteration 126/1000 | Loss: 0.00002624
Iteration 127/1000 | Loss: 0.00002624
Iteration 128/1000 | Loss: 0.00002624
Iteration 129/1000 | Loss: 0.00002624
Iteration 130/1000 | Loss: 0.00002624
Iteration 131/1000 | Loss: 0.00002623
Iteration 132/1000 | Loss: 0.00002623
Iteration 133/1000 | Loss: 0.00002623
Iteration 134/1000 | Loss: 0.00002623
Iteration 135/1000 | Loss: 0.00002623
Iteration 136/1000 | Loss: 0.00002622
Iteration 137/1000 | Loss: 0.00002622
Iteration 138/1000 | Loss: 0.00002622
Iteration 139/1000 | Loss: 0.00002622
Iteration 140/1000 | Loss: 0.00002622
Iteration 141/1000 | Loss: 0.00002622
Iteration 142/1000 | Loss: 0.00002622
Iteration 143/1000 | Loss: 0.00002622
Iteration 144/1000 | Loss: 0.00002622
Iteration 145/1000 | Loss: 0.00002622
Iteration 146/1000 | Loss: 0.00002622
Iteration 147/1000 | Loss: 0.00002622
Iteration 148/1000 | Loss: 0.00002622
Iteration 149/1000 | Loss: 0.00002622
Iteration 150/1000 | Loss: 0.00002622
Iteration 151/1000 | Loss: 0.00002622
Iteration 152/1000 | Loss: 0.00002622
Iteration 153/1000 | Loss: 0.00002622
Iteration 154/1000 | Loss: 0.00002622
Iteration 155/1000 | Loss: 0.00002622
Iteration 156/1000 | Loss: 0.00002622
Iteration 157/1000 | Loss: 0.00002622
Iteration 158/1000 | Loss: 0.00002622
Iteration 159/1000 | Loss: 0.00002622
Iteration 160/1000 | Loss: 0.00002622
Iteration 161/1000 | Loss: 0.00002622
Iteration 162/1000 | Loss: 0.00002622
Iteration 163/1000 | Loss: 0.00002622
Iteration 164/1000 | Loss: 0.00002622
Iteration 165/1000 | Loss: 0.00002622
Iteration 166/1000 | Loss: 0.00002622
Iteration 167/1000 | Loss: 0.00002622
Iteration 168/1000 | Loss: 0.00002622
Iteration 169/1000 | Loss: 0.00002622
Iteration 170/1000 | Loss: 0.00002622
Iteration 171/1000 | Loss: 0.00002622
Iteration 172/1000 | Loss: 0.00002622
Iteration 173/1000 | Loss: 0.00002622
Iteration 174/1000 | Loss: 0.00002622
Iteration 175/1000 | Loss: 0.00002622
Iteration 176/1000 | Loss: 0.00002622
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [2.6215173420496285e-05, 2.6215173420496285e-05, 2.6215173420496285e-05, 2.6215173420496285e-05, 2.6215173420496285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6215173420496285e-05

Optimization complete. Final v2v error: 4.227795600891113 mm

Highest mean error: 4.460007667541504 mm for frame 66

Lowest mean error: 4.003156661987305 mm for frame 11

Saving results

Total time: 42.642507791519165
