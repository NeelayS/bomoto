Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=82, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 4592-4647
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00492149
Iteration 2/25 | Loss: 0.00156522
Iteration 3/25 | Loss: 0.00137841
Iteration 4/25 | Loss: 0.00135700
Iteration 5/25 | Loss: 0.00135257
Iteration 6/25 | Loss: 0.00135174
Iteration 7/25 | Loss: 0.00135174
Iteration 8/25 | Loss: 0.00135174
Iteration 9/25 | Loss: 0.00135174
Iteration 10/25 | Loss: 0.00135174
Iteration 11/25 | Loss: 0.00135174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013517396291717887, 0.0013517396291717887, 0.0013517396291717887, 0.0013517396291717887, 0.0013517396291717887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013517396291717887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40218437
Iteration 2/25 | Loss: 0.00087621
Iteration 3/25 | Loss: 0.00087619
Iteration 4/25 | Loss: 0.00087619
Iteration 5/25 | Loss: 0.00087619
Iteration 6/25 | Loss: 0.00087619
Iteration 7/25 | Loss: 0.00087619
Iteration 8/25 | Loss: 0.00087619
Iteration 9/25 | Loss: 0.00087619
Iteration 10/25 | Loss: 0.00087619
Iteration 11/25 | Loss: 0.00087619
Iteration 12/25 | Loss: 0.00087618
Iteration 13/25 | Loss: 0.00087618
Iteration 14/25 | Loss: 0.00087618
Iteration 15/25 | Loss: 0.00087618
Iteration 16/25 | Loss: 0.00087618
Iteration 17/25 | Loss: 0.00087618
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008761849021539092, 0.0008761849021539092, 0.0008761849021539092, 0.0008761849021539092, 0.0008761849021539092]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008761849021539092

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087618
Iteration 2/1000 | Loss: 0.00005493
Iteration 3/1000 | Loss: 0.00003278
Iteration 4/1000 | Loss: 0.00002773
Iteration 5/1000 | Loss: 0.00002563
Iteration 6/1000 | Loss: 0.00002385
Iteration 7/1000 | Loss: 0.00002257
Iteration 8/1000 | Loss: 0.00002160
Iteration 9/1000 | Loss: 0.00002113
Iteration 10/1000 | Loss: 0.00002062
Iteration 11/1000 | Loss: 0.00002027
Iteration 12/1000 | Loss: 0.00001998
Iteration 13/1000 | Loss: 0.00001975
Iteration 14/1000 | Loss: 0.00001956
Iteration 15/1000 | Loss: 0.00001945
Iteration 16/1000 | Loss: 0.00001938
Iteration 17/1000 | Loss: 0.00001935
Iteration 18/1000 | Loss: 0.00001935
Iteration 19/1000 | Loss: 0.00001933
Iteration 20/1000 | Loss: 0.00001931
Iteration 21/1000 | Loss: 0.00001931
Iteration 22/1000 | Loss: 0.00001929
Iteration 23/1000 | Loss: 0.00001929
Iteration 24/1000 | Loss: 0.00001928
Iteration 25/1000 | Loss: 0.00001924
Iteration 26/1000 | Loss: 0.00001919
Iteration 27/1000 | Loss: 0.00001919
Iteration 28/1000 | Loss: 0.00001915
Iteration 29/1000 | Loss: 0.00001914
Iteration 30/1000 | Loss: 0.00001914
Iteration 31/1000 | Loss: 0.00001913
Iteration 32/1000 | Loss: 0.00001912
Iteration 33/1000 | Loss: 0.00001912
Iteration 34/1000 | Loss: 0.00001911
Iteration 35/1000 | Loss: 0.00001911
Iteration 36/1000 | Loss: 0.00001910
Iteration 37/1000 | Loss: 0.00001910
Iteration 38/1000 | Loss: 0.00001909
Iteration 39/1000 | Loss: 0.00001908
Iteration 40/1000 | Loss: 0.00001907
Iteration 41/1000 | Loss: 0.00001906
Iteration 42/1000 | Loss: 0.00001906
Iteration 43/1000 | Loss: 0.00001906
Iteration 44/1000 | Loss: 0.00001905
Iteration 45/1000 | Loss: 0.00001905
Iteration 46/1000 | Loss: 0.00001905
Iteration 47/1000 | Loss: 0.00001905
Iteration 48/1000 | Loss: 0.00001905
Iteration 49/1000 | Loss: 0.00001904
Iteration 50/1000 | Loss: 0.00001904
Iteration 51/1000 | Loss: 0.00001904
Iteration 52/1000 | Loss: 0.00001904
Iteration 53/1000 | Loss: 0.00001904
Iteration 54/1000 | Loss: 0.00001903
Iteration 55/1000 | Loss: 0.00001902
Iteration 56/1000 | Loss: 0.00001902
Iteration 57/1000 | Loss: 0.00001902
Iteration 58/1000 | Loss: 0.00001902
Iteration 59/1000 | Loss: 0.00001902
Iteration 60/1000 | Loss: 0.00001901
Iteration 61/1000 | Loss: 0.00001900
Iteration 62/1000 | Loss: 0.00001900
Iteration 63/1000 | Loss: 0.00001900
Iteration 64/1000 | Loss: 0.00001899
Iteration 65/1000 | Loss: 0.00001899
Iteration 66/1000 | Loss: 0.00001898
Iteration 67/1000 | Loss: 0.00001898
Iteration 68/1000 | Loss: 0.00001897
Iteration 69/1000 | Loss: 0.00001897
Iteration 70/1000 | Loss: 0.00001896
Iteration 71/1000 | Loss: 0.00001896
Iteration 72/1000 | Loss: 0.00001896
Iteration 73/1000 | Loss: 0.00001895
Iteration 74/1000 | Loss: 0.00001895
Iteration 75/1000 | Loss: 0.00001895
Iteration 76/1000 | Loss: 0.00001894
Iteration 77/1000 | Loss: 0.00001894
Iteration 78/1000 | Loss: 0.00001894
Iteration 79/1000 | Loss: 0.00001894
Iteration 80/1000 | Loss: 0.00001894
Iteration 81/1000 | Loss: 0.00001893
Iteration 82/1000 | Loss: 0.00001893
Iteration 83/1000 | Loss: 0.00001893
Iteration 84/1000 | Loss: 0.00001893
Iteration 85/1000 | Loss: 0.00001893
Iteration 86/1000 | Loss: 0.00001892
Iteration 87/1000 | Loss: 0.00001892
Iteration 88/1000 | Loss: 0.00001892
Iteration 89/1000 | Loss: 0.00001892
Iteration 90/1000 | Loss: 0.00001892
Iteration 91/1000 | Loss: 0.00001892
Iteration 92/1000 | Loss: 0.00001892
Iteration 93/1000 | Loss: 0.00001892
Iteration 94/1000 | Loss: 0.00001892
Iteration 95/1000 | Loss: 0.00001892
Iteration 96/1000 | Loss: 0.00001892
Iteration 97/1000 | Loss: 0.00001892
Iteration 98/1000 | Loss: 0.00001892
Iteration 99/1000 | Loss: 0.00001891
Iteration 100/1000 | Loss: 0.00001891
Iteration 101/1000 | Loss: 0.00001891
Iteration 102/1000 | Loss: 0.00001891
Iteration 103/1000 | Loss: 0.00001891
Iteration 104/1000 | Loss: 0.00001891
Iteration 105/1000 | Loss: 0.00001890
Iteration 106/1000 | Loss: 0.00001890
Iteration 107/1000 | Loss: 0.00001890
Iteration 108/1000 | Loss: 0.00001890
Iteration 109/1000 | Loss: 0.00001890
Iteration 110/1000 | Loss: 0.00001890
Iteration 111/1000 | Loss: 0.00001889
Iteration 112/1000 | Loss: 0.00001889
Iteration 113/1000 | Loss: 0.00001889
Iteration 114/1000 | Loss: 0.00001889
Iteration 115/1000 | Loss: 0.00001889
Iteration 116/1000 | Loss: 0.00001889
Iteration 117/1000 | Loss: 0.00001889
Iteration 118/1000 | Loss: 0.00001889
Iteration 119/1000 | Loss: 0.00001889
Iteration 120/1000 | Loss: 0.00001888
Iteration 121/1000 | Loss: 0.00001888
Iteration 122/1000 | Loss: 0.00001888
Iteration 123/1000 | Loss: 0.00001888
Iteration 124/1000 | Loss: 0.00001888
Iteration 125/1000 | Loss: 0.00001888
Iteration 126/1000 | Loss: 0.00001888
Iteration 127/1000 | Loss: 0.00001887
Iteration 128/1000 | Loss: 0.00001887
Iteration 129/1000 | Loss: 0.00001887
Iteration 130/1000 | Loss: 0.00001887
Iteration 131/1000 | Loss: 0.00001887
Iteration 132/1000 | Loss: 0.00001887
Iteration 133/1000 | Loss: 0.00001886
Iteration 134/1000 | Loss: 0.00001886
Iteration 135/1000 | Loss: 0.00001886
Iteration 136/1000 | Loss: 0.00001886
Iteration 137/1000 | Loss: 0.00001885
Iteration 138/1000 | Loss: 0.00001885
Iteration 139/1000 | Loss: 0.00001885
Iteration 140/1000 | Loss: 0.00001885
Iteration 141/1000 | Loss: 0.00001885
Iteration 142/1000 | Loss: 0.00001885
Iteration 143/1000 | Loss: 0.00001884
Iteration 144/1000 | Loss: 0.00001884
Iteration 145/1000 | Loss: 0.00001884
Iteration 146/1000 | Loss: 0.00001884
Iteration 147/1000 | Loss: 0.00001884
Iteration 148/1000 | Loss: 0.00001884
Iteration 149/1000 | Loss: 0.00001884
Iteration 150/1000 | Loss: 0.00001884
Iteration 151/1000 | Loss: 0.00001884
Iteration 152/1000 | Loss: 0.00001883
Iteration 153/1000 | Loss: 0.00001883
Iteration 154/1000 | Loss: 0.00001883
Iteration 155/1000 | Loss: 0.00001883
Iteration 156/1000 | Loss: 0.00001883
Iteration 157/1000 | Loss: 0.00001883
Iteration 158/1000 | Loss: 0.00001883
Iteration 159/1000 | Loss: 0.00001883
Iteration 160/1000 | Loss: 0.00001883
Iteration 161/1000 | Loss: 0.00001883
Iteration 162/1000 | Loss: 0.00001883
Iteration 163/1000 | Loss: 0.00001883
Iteration 164/1000 | Loss: 0.00001883
Iteration 165/1000 | Loss: 0.00001883
Iteration 166/1000 | Loss: 0.00001883
Iteration 167/1000 | Loss: 0.00001883
Iteration 168/1000 | Loss: 0.00001883
Iteration 169/1000 | Loss: 0.00001883
Iteration 170/1000 | Loss: 0.00001883
Iteration 171/1000 | Loss: 0.00001883
Iteration 172/1000 | Loss: 0.00001883
Iteration 173/1000 | Loss: 0.00001883
Iteration 174/1000 | Loss: 0.00001883
Iteration 175/1000 | Loss: 0.00001883
Iteration 176/1000 | Loss: 0.00001883
Iteration 177/1000 | Loss: 0.00001883
Iteration 178/1000 | Loss: 0.00001883
Iteration 179/1000 | Loss: 0.00001883
Iteration 180/1000 | Loss: 0.00001883
Iteration 181/1000 | Loss: 0.00001883
Iteration 182/1000 | Loss: 0.00001883
Iteration 183/1000 | Loss: 0.00001883
Iteration 184/1000 | Loss: 0.00001883
Iteration 185/1000 | Loss: 0.00001883
Iteration 186/1000 | Loss: 0.00001883
Iteration 187/1000 | Loss: 0.00001883
Iteration 188/1000 | Loss: 0.00001883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.882512151496485e-05, 1.882512151496485e-05, 1.882512151496485e-05, 1.882512151496485e-05, 1.882512151496485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.882512151496485e-05

Optimization complete. Final v2v error: 3.67655873298645 mm

Highest mean error: 4.529685020446777 mm for frame 135

Lowest mean error: 3.2048556804656982 mm for frame 89

Saving results

Total time: 46.37221693992615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974490
Iteration 2/25 | Loss: 0.00252565
Iteration 3/25 | Loss: 0.00201992
Iteration 4/25 | Loss: 0.00190968
Iteration 5/25 | Loss: 0.00166609
Iteration 6/25 | Loss: 0.00155619
Iteration 7/25 | Loss: 0.00152428
Iteration 8/25 | Loss: 0.00150933
Iteration 9/25 | Loss: 0.00150801
Iteration 10/25 | Loss: 0.00149435
Iteration 11/25 | Loss: 0.00146199
Iteration 12/25 | Loss: 0.00145182
Iteration 13/25 | Loss: 0.00143864
Iteration 14/25 | Loss: 0.00143581
Iteration 15/25 | Loss: 0.00142824
Iteration 16/25 | Loss: 0.00142950
Iteration 17/25 | Loss: 0.00142580
Iteration 18/25 | Loss: 0.00142581
Iteration 19/25 | Loss: 0.00142502
Iteration 20/25 | Loss: 0.00142441
Iteration 21/25 | Loss: 0.00142466
Iteration 22/25 | Loss: 0.00142491
Iteration 23/25 | Loss: 0.00142460
Iteration 24/25 | Loss: 0.00142558
Iteration 25/25 | Loss: 0.00142489

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38556290
Iteration 2/25 | Loss: 0.00101015
Iteration 3/25 | Loss: 0.00101015
Iteration 4/25 | Loss: 0.00101015
Iteration 5/25 | Loss: 0.00101015
Iteration 6/25 | Loss: 0.00101015
Iteration 7/25 | Loss: 0.00101015
Iteration 8/25 | Loss: 0.00101015
Iteration 9/25 | Loss: 0.00101015
Iteration 10/25 | Loss: 0.00101015
Iteration 11/25 | Loss: 0.00101015
Iteration 12/25 | Loss: 0.00101015
Iteration 13/25 | Loss: 0.00101015
Iteration 14/25 | Loss: 0.00101015
Iteration 15/25 | Loss: 0.00101015
Iteration 16/25 | Loss: 0.00101015
Iteration 17/25 | Loss: 0.00101015
Iteration 18/25 | Loss: 0.00101015
Iteration 19/25 | Loss: 0.00101015
Iteration 20/25 | Loss: 0.00101015
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010101472726091743, 0.0010101472726091743, 0.0010101472726091743, 0.0010101472726091743, 0.0010101472726091743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010101472726091743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101015
Iteration 2/1000 | Loss: 0.00027878
Iteration 3/1000 | Loss: 0.00009579
Iteration 4/1000 | Loss: 0.00006275
Iteration 5/1000 | Loss: 0.00005516
Iteration 6/1000 | Loss: 0.00008753
Iteration 7/1000 | Loss: 0.00005215
Iteration 8/1000 | Loss: 0.00005826
Iteration 9/1000 | Loss: 0.00005879
Iteration 10/1000 | Loss: 0.00005393
Iteration 11/1000 | Loss: 0.00005465
Iteration 12/1000 | Loss: 0.00005015
Iteration 13/1000 | Loss: 0.00005060
Iteration 14/1000 | Loss: 0.00006215
Iteration 15/1000 | Loss: 0.00006647
Iteration 16/1000 | Loss: 0.00006173
Iteration 17/1000 | Loss: 0.00005935
Iteration 18/1000 | Loss: 0.00005413
Iteration 19/1000 | Loss: 0.00006250
Iteration 20/1000 | Loss: 0.00006080
Iteration 21/1000 | Loss: 0.00004602
Iteration 22/1000 | Loss: 0.00004562
Iteration 23/1000 | Loss: 0.00004044
Iteration 24/1000 | Loss: 0.00006036
Iteration 25/1000 | Loss: 0.00006418
Iteration 26/1000 | Loss: 0.00005931
Iteration 27/1000 | Loss: 0.00008645
Iteration 28/1000 | Loss: 0.00006551
Iteration 29/1000 | Loss: 0.00006196
Iteration 30/1000 | Loss: 0.00007325
Iteration 31/1000 | Loss: 0.00011959
Iteration 32/1000 | Loss: 0.00005338
Iteration 33/1000 | Loss: 0.00006418
Iteration 34/1000 | Loss: 0.00005937
Iteration 35/1000 | Loss: 0.00005264
Iteration 36/1000 | Loss: 0.00004855
Iteration 37/1000 | Loss: 0.00004955
Iteration 38/1000 | Loss: 0.00005565
Iteration 39/1000 | Loss: 0.00006199
Iteration 40/1000 | Loss: 0.00005574
Iteration 41/1000 | Loss: 0.00006961
Iteration 42/1000 | Loss: 0.00005819
Iteration 43/1000 | Loss: 0.00006666
Iteration 44/1000 | Loss: 0.00005492
Iteration 45/1000 | Loss: 0.00005640
Iteration 46/1000 | Loss: 0.00006019
Iteration 47/1000 | Loss: 0.00005866
Iteration 48/1000 | Loss: 0.00007820
Iteration 49/1000 | Loss: 0.00006021
Iteration 50/1000 | Loss: 0.00006658
Iteration 51/1000 | Loss: 0.00005893
Iteration 52/1000 | Loss: 0.00006046
Iteration 53/1000 | Loss: 0.00006831
Iteration 54/1000 | Loss: 0.00006159
Iteration 55/1000 | Loss: 0.00006500
Iteration 56/1000 | Loss: 0.00006252
Iteration 57/1000 | Loss: 0.00006853
Iteration 58/1000 | Loss: 0.00006511
Iteration 59/1000 | Loss: 0.00006841
Iteration 60/1000 | Loss: 0.00007791
Iteration 61/1000 | Loss: 0.00007125
Iteration 62/1000 | Loss: 0.00004715
Iteration 63/1000 | Loss: 0.00005707
Iteration 64/1000 | Loss: 0.00005408
Iteration 65/1000 | Loss: 0.00004665
Iteration 66/1000 | Loss: 0.00004907
Iteration 67/1000 | Loss: 0.00006608
Iteration 68/1000 | Loss: 0.00004587
Iteration 69/1000 | Loss: 0.00006291
Iteration 70/1000 | Loss: 0.00006478
Iteration 71/1000 | Loss: 0.00006513
Iteration 72/1000 | Loss: 0.00006751
Iteration 73/1000 | Loss: 0.00006835
Iteration 74/1000 | Loss: 0.00006043
Iteration 75/1000 | Loss: 0.00008069
Iteration 76/1000 | Loss: 0.00038739
Iteration 77/1000 | Loss: 0.00081886
Iteration 78/1000 | Loss: 0.00031263
Iteration 79/1000 | Loss: 0.00075933
Iteration 80/1000 | Loss: 0.00054653
Iteration 81/1000 | Loss: 0.00006854
Iteration 82/1000 | Loss: 0.00005348
Iteration 83/1000 | Loss: 0.00011307
Iteration 84/1000 | Loss: 0.00006226
Iteration 85/1000 | Loss: 0.00007266
Iteration 86/1000 | Loss: 0.00005455
Iteration 87/1000 | Loss: 0.00005189
Iteration 88/1000 | Loss: 0.00004775
Iteration 89/1000 | Loss: 0.00002949
Iteration 90/1000 | Loss: 0.00002756
Iteration 91/1000 | Loss: 0.00003263
Iteration 92/1000 | Loss: 0.00003435
Iteration 93/1000 | Loss: 0.00002452
Iteration 94/1000 | Loss: 0.00002781
Iteration 95/1000 | Loss: 0.00003081
Iteration 96/1000 | Loss: 0.00003609
Iteration 97/1000 | Loss: 0.00003210
Iteration 98/1000 | Loss: 0.00003364
Iteration 99/1000 | Loss: 0.00003333
Iteration 100/1000 | Loss: 0.00003816
Iteration 101/1000 | Loss: 0.00003133
Iteration 102/1000 | Loss: 0.00003888
Iteration 103/1000 | Loss: 0.00003222
Iteration 104/1000 | Loss: 0.00004101
Iteration 105/1000 | Loss: 0.00005028
Iteration 106/1000 | Loss: 0.00003941
Iteration 107/1000 | Loss: 0.00003201
Iteration 108/1000 | Loss: 0.00003995
Iteration 109/1000 | Loss: 0.00003084
Iteration 110/1000 | Loss: 0.00003987
Iteration 111/1000 | Loss: 0.00003164
Iteration 112/1000 | Loss: 0.00002490
Iteration 113/1000 | Loss: 0.00002361
Iteration 114/1000 | Loss: 0.00002302
Iteration 115/1000 | Loss: 0.00002271
Iteration 116/1000 | Loss: 0.00002236
Iteration 117/1000 | Loss: 0.00002192
Iteration 118/1000 | Loss: 0.00002153
Iteration 119/1000 | Loss: 0.00002127
Iteration 120/1000 | Loss: 0.00002127
Iteration 121/1000 | Loss: 0.00002115
Iteration 122/1000 | Loss: 0.00002104
Iteration 123/1000 | Loss: 0.00002089
Iteration 124/1000 | Loss: 0.00002081
Iteration 125/1000 | Loss: 0.00002079
Iteration 126/1000 | Loss: 0.00002079
Iteration 127/1000 | Loss: 0.00002077
Iteration 128/1000 | Loss: 0.00002077
Iteration 129/1000 | Loss: 0.00002076
Iteration 130/1000 | Loss: 0.00002076
Iteration 131/1000 | Loss: 0.00002076
Iteration 132/1000 | Loss: 0.00002075
Iteration 133/1000 | Loss: 0.00002075
Iteration 134/1000 | Loss: 0.00002075
Iteration 135/1000 | Loss: 0.00002075
Iteration 136/1000 | Loss: 0.00002075
Iteration 137/1000 | Loss: 0.00002074
Iteration 138/1000 | Loss: 0.00002074
Iteration 139/1000 | Loss: 0.00002074
Iteration 140/1000 | Loss: 0.00002074
Iteration 141/1000 | Loss: 0.00002074
Iteration 142/1000 | Loss: 0.00002074
Iteration 143/1000 | Loss: 0.00002073
Iteration 144/1000 | Loss: 0.00002073
Iteration 145/1000 | Loss: 0.00002073
Iteration 146/1000 | Loss: 0.00002073
Iteration 147/1000 | Loss: 0.00002073
Iteration 148/1000 | Loss: 0.00002073
Iteration 149/1000 | Loss: 0.00002073
Iteration 150/1000 | Loss: 0.00002073
Iteration 151/1000 | Loss: 0.00002073
Iteration 152/1000 | Loss: 0.00002072
Iteration 153/1000 | Loss: 0.00002072
Iteration 154/1000 | Loss: 0.00002072
Iteration 155/1000 | Loss: 0.00002072
Iteration 156/1000 | Loss: 0.00002072
Iteration 157/1000 | Loss: 0.00002072
Iteration 158/1000 | Loss: 0.00002072
Iteration 159/1000 | Loss: 0.00002072
Iteration 160/1000 | Loss: 0.00002072
Iteration 161/1000 | Loss: 0.00002072
Iteration 162/1000 | Loss: 0.00002072
Iteration 163/1000 | Loss: 0.00002072
Iteration 164/1000 | Loss: 0.00002072
Iteration 165/1000 | Loss: 0.00002071
Iteration 166/1000 | Loss: 0.00002071
Iteration 167/1000 | Loss: 0.00002071
Iteration 168/1000 | Loss: 0.00002071
Iteration 169/1000 | Loss: 0.00002071
Iteration 170/1000 | Loss: 0.00002071
Iteration 171/1000 | Loss: 0.00002071
Iteration 172/1000 | Loss: 0.00002071
Iteration 173/1000 | Loss: 0.00002071
Iteration 174/1000 | Loss: 0.00002071
Iteration 175/1000 | Loss: 0.00002071
Iteration 176/1000 | Loss: 0.00002071
Iteration 177/1000 | Loss: 0.00002071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.0712226614705287e-05, 2.0712226614705287e-05, 2.0712226614705287e-05, 2.0712226614705287e-05, 2.0712226614705287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0712226614705287e-05

Optimization complete. Final v2v error: 3.7982137203216553 mm

Highest mean error: 5.118175506591797 mm for frame 65

Lowest mean error: 3.5996832847595215 mm for frame 26

Saving results

Total time: 223.4221634864807
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00504124
Iteration 2/25 | Loss: 0.00150773
Iteration 3/25 | Loss: 0.00136899
Iteration 4/25 | Loss: 0.00133438
Iteration 5/25 | Loss: 0.00132981
Iteration 6/25 | Loss: 0.00132930
Iteration 7/25 | Loss: 0.00132930
Iteration 8/25 | Loss: 0.00132930
Iteration 9/25 | Loss: 0.00132930
Iteration 10/25 | Loss: 0.00132930
Iteration 11/25 | Loss: 0.00132930
Iteration 12/25 | Loss: 0.00132930
Iteration 13/25 | Loss: 0.00132930
Iteration 14/25 | Loss: 0.00132930
Iteration 15/25 | Loss: 0.00132930
Iteration 16/25 | Loss: 0.00132930
Iteration 17/25 | Loss: 0.00132930
Iteration 18/25 | Loss: 0.00132930
Iteration 19/25 | Loss: 0.00132930
Iteration 20/25 | Loss: 0.00132930
Iteration 21/25 | Loss: 0.00132930
Iteration 22/25 | Loss: 0.00132930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013292966177687049, 0.0013292966177687049, 0.0013292966177687049, 0.0013292966177687049, 0.0013292966177687049]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013292966177687049

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33521950
Iteration 2/25 | Loss: 0.00070674
Iteration 3/25 | Loss: 0.00070673
Iteration 4/25 | Loss: 0.00070673
Iteration 5/25 | Loss: 0.00070672
Iteration 6/25 | Loss: 0.00070672
Iteration 7/25 | Loss: 0.00070672
Iteration 8/25 | Loss: 0.00070672
Iteration 9/25 | Loss: 0.00070672
Iteration 10/25 | Loss: 0.00070672
Iteration 11/25 | Loss: 0.00070672
Iteration 12/25 | Loss: 0.00070672
Iteration 13/25 | Loss: 0.00070672
Iteration 14/25 | Loss: 0.00070672
Iteration 15/25 | Loss: 0.00070672
Iteration 16/25 | Loss: 0.00070672
Iteration 17/25 | Loss: 0.00070672
Iteration 18/25 | Loss: 0.00070672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007067223195917904, 0.0007067223195917904, 0.0007067223195917904, 0.0007067223195917904, 0.0007067223195917904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007067223195917904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070672
Iteration 2/1000 | Loss: 0.00004651
Iteration 3/1000 | Loss: 0.00002775
Iteration 4/1000 | Loss: 0.00002422
Iteration 5/1000 | Loss: 0.00002315
Iteration 6/1000 | Loss: 0.00002220
Iteration 7/1000 | Loss: 0.00002146
Iteration 8/1000 | Loss: 0.00002083
Iteration 9/1000 | Loss: 0.00002050
Iteration 10/1000 | Loss: 0.00002024
Iteration 11/1000 | Loss: 0.00002005
Iteration 12/1000 | Loss: 0.00001992
Iteration 13/1000 | Loss: 0.00001992
Iteration 14/1000 | Loss: 0.00001991
Iteration 15/1000 | Loss: 0.00001990
Iteration 16/1000 | Loss: 0.00001985
Iteration 17/1000 | Loss: 0.00001981
Iteration 18/1000 | Loss: 0.00001981
Iteration 19/1000 | Loss: 0.00001979
Iteration 20/1000 | Loss: 0.00001979
Iteration 21/1000 | Loss: 0.00001978
Iteration 22/1000 | Loss: 0.00001978
Iteration 23/1000 | Loss: 0.00001976
Iteration 24/1000 | Loss: 0.00001976
Iteration 25/1000 | Loss: 0.00001976
Iteration 26/1000 | Loss: 0.00001976
Iteration 27/1000 | Loss: 0.00001975
Iteration 28/1000 | Loss: 0.00001971
Iteration 29/1000 | Loss: 0.00001971
Iteration 30/1000 | Loss: 0.00001971
Iteration 31/1000 | Loss: 0.00001971
Iteration 32/1000 | Loss: 0.00001971
Iteration 33/1000 | Loss: 0.00001971
Iteration 34/1000 | Loss: 0.00001971
Iteration 35/1000 | Loss: 0.00001970
Iteration 36/1000 | Loss: 0.00001970
Iteration 37/1000 | Loss: 0.00001968
Iteration 38/1000 | Loss: 0.00001967
Iteration 39/1000 | Loss: 0.00001967
Iteration 40/1000 | Loss: 0.00001967
Iteration 41/1000 | Loss: 0.00001967
Iteration 42/1000 | Loss: 0.00001967
Iteration 43/1000 | Loss: 0.00001967
Iteration 44/1000 | Loss: 0.00001966
Iteration 45/1000 | Loss: 0.00001966
Iteration 46/1000 | Loss: 0.00001965
Iteration 47/1000 | Loss: 0.00001965
Iteration 48/1000 | Loss: 0.00001965
Iteration 49/1000 | Loss: 0.00001965
Iteration 50/1000 | Loss: 0.00001965
Iteration 51/1000 | Loss: 0.00001964
Iteration 52/1000 | Loss: 0.00001964
Iteration 53/1000 | Loss: 0.00001963
Iteration 54/1000 | Loss: 0.00001963
Iteration 55/1000 | Loss: 0.00001962
Iteration 56/1000 | Loss: 0.00001962
Iteration 57/1000 | Loss: 0.00001962
Iteration 58/1000 | Loss: 0.00001961
Iteration 59/1000 | Loss: 0.00001961
Iteration 60/1000 | Loss: 0.00001961
Iteration 61/1000 | Loss: 0.00001961
Iteration 62/1000 | Loss: 0.00001961
Iteration 63/1000 | Loss: 0.00001961
Iteration 64/1000 | Loss: 0.00001961
Iteration 65/1000 | Loss: 0.00001960
Iteration 66/1000 | Loss: 0.00001960
Iteration 67/1000 | Loss: 0.00001960
Iteration 68/1000 | Loss: 0.00001960
Iteration 69/1000 | Loss: 0.00001960
Iteration 70/1000 | Loss: 0.00001960
Iteration 71/1000 | Loss: 0.00001960
Iteration 72/1000 | Loss: 0.00001960
Iteration 73/1000 | Loss: 0.00001960
Iteration 74/1000 | Loss: 0.00001960
Iteration 75/1000 | Loss: 0.00001960
Iteration 76/1000 | Loss: 0.00001960
Iteration 77/1000 | Loss: 0.00001960
Iteration 78/1000 | Loss: 0.00001960
Iteration 79/1000 | Loss: 0.00001960
Iteration 80/1000 | Loss: 0.00001960
Iteration 81/1000 | Loss: 0.00001960
Iteration 82/1000 | Loss: 0.00001960
Iteration 83/1000 | Loss: 0.00001960
Iteration 84/1000 | Loss: 0.00001960
Iteration 85/1000 | Loss: 0.00001960
Iteration 86/1000 | Loss: 0.00001960
Iteration 87/1000 | Loss: 0.00001960
Iteration 88/1000 | Loss: 0.00001960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.9601851818151772e-05, 1.9601851818151772e-05, 1.9601851818151772e-05, 1.9601851818151772e-05, 1.9601851818151772e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9601851818151772e-05

Optimization complete. Final v2v error: 3.772940158843994 mm

Highest mean error: 4.055004119873047 mm for frame 133

Lowest mean error: 3.6317126750946045 mm for frame 65

Saving results

Total time: 31.383228540420532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499238
Iteration 2/25 | Loss: 0.00137737
Iteration 3/25 | Loss: 0.00130742
Iteration 4/25 | Loss: 0.00129503
Iteration 5/25 | Loss: 0.00129057
Iteration 6/25 | Loss: 0.00128932
Iteration 7/25 | Loss: 0.00128931
Iteration 8/25 | Loss: 0.00128931
Iteration 9/25 | Loss: 0.00128931
Iteration 10/25 | Loss: 0.00128931
Iteration 11/25 | Loss: 0.00128931
Iteration 12/25 | Loss: 0.00128931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012893071398139, 0.0012893071398139, 0.0012893071398139, 0.0012893071398139, 0.0012893071398139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012893071398139

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.52014112
Iteration 2/25 | Loss: 0.00089404
Iteration 3/25 | Loss: 0.00089401
Iteration 4/25 | Loss: 0.00089401
Iteration 5/25 | Loss: 0.00089401
Iteration 6/25 | Loss: 0.00089401
Iteration 7/25 | Loss: 0.00089401
Iteration 8/25 | Loss: 0.00089401
Iteration 9/25 | Loss: 0.00089401
Iteration 10/25 | Loss: 0.00089401
Iteration 11/25 | Loss: 0.00089401
Iteration 12/25 | Loss: 0.00089401
Iteration 13/25 | Loss: 0.00089401
Iteration 14/25 | Loss: 0.00089401
Iteration 15/25 | Loss: 0.00089401
Iteration 16/25 | Loss: 0.00089401
Iteration 17/25 | Loss: 0.00089401
Iteration 18/25 | Loss: 0.00089401
Iteration 19/25 | Loss: 0.00089401
Iteration 20/25 | Loss: 0.00089401
Iteration 21/25 | Loss: 0.00089401
Iteration 22/25 | Loss: 0.00089401
Iteration 23/25 | Loss: 0.00089401
Iteration 24/25 | Loss: 0.00089401
Iteration 25/25 | Loss: 0.00089401

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089401
Iteration 2/1000 | Loss: 0.00003240
Iteration 3/1000 | Loss: 0.00002348
Iteration 4/1000 | Loss: 0.00002000
Iteration 5/1000 | Loss: 0.00001852
Iteration 6/1000 | Loss: 0.00001767
Iteration 7/1000 | Loss: 0.00001707
Iteration 8/1000 | Loss: 0.00001647
Iteration 9/1000 | Loss: 0.00001606
Iteration 10/1000 | Loss: 0.00001573
Iteration 11/1000 | Loss: 0.00001536
Iteration 12/1000 | Loss: 0.00001512
Iteration 13/1000 | Loss: 0.00001495
Iteration 14/1000 | Loss: 0.00001483
Iteration 15/1000 | Loss: 0.00001473
Iteration 16/1000 | Loss: 0.00001468
Iteration 17/1000 | Loss: 0.00001467
Iteration 18/1000 | Loss: 0.00001466
Iteration 19/1000 | Loss: 0.00001466
Iteration 20/1000 | Loss: 0.00001464
Iteration 21/1000 | Loss: 0.00001463
Iteration 22/1000 | Loss: 0.00001463
Iteration 23/1000 | Loss: 0.00001462
Iteration 24/1000 | Loss: 0.00001461
Iteration 25/1000 | Loss: 0.00001461
Iteration 26/1000 | Loss: 0.00001459
Iteration 27/1000 | Loss: 0.00001459
Iteration 28/1000 | Loss: 0.00001459
Iteration 29/1000 | Loss: 0.00001458
Iteration 30/1000 | Loss: 0.00001458
Iteration 31/1000 | Loss: 0.00001458
Iteration 32/1000 | Loss: 0.00001457
Iteration 33/1000 | Loss: 0.00001455
Iteration 34/1000 | Loss: 0.00001454
Iteration 35/1000 | Loss: 0.00001453
Iteration 36/1000 | Loss: 0.00001450
Iteration 37/1000 | Loss: 0.00001446
Iteration 38/1000 | Loss: 0.00001446
Iteration 39/1000 | Loss: 0.00001445
Iteration 40/1000 | Loss: 0.00001445
Iteration 41/1000 | Loss: 0.00001445
Iteration 42/1000 | Loss: 0.00001444
Iteration 43/1000 | Loss: 0.00001441
Iteration 44/1000 | Loss: 0.00001440
Iteration 45/1000 | Loss: 0.00001439
Iteration 46/1000 | Loss: 0.00001439
Iteration 47/1000 | Loss: 0.00001439
Iteration 48/1000 | Loss: 0.00001439
Iteration 49/1000 | Loss: 0.00001439
Iteration 50/1000 | Loss: 0.00001439
Iteration 51/1000 | Loss: 0.00001439
Iteration 52/1000 | Loss: 0.00001439
Iteration 53/1000 | Loss: 0.00001439
Iteration 54/1000 | Loss: 0.00001439
Iteration 55/1000 | Loss: 0.00001438
Iteration 56/1000 | Loss: 0.00001438
Iteration 57/1000 | Loss: 0.00001438
Iteration 58/1000 | Loss: 0.00001438
Iteration 59/1000 | Loss: 0.00001438
Iteration 60/1000 | Loss: 0.00001438
Iteration 61/1000 | Loss: 0.00001438
Iteration 62/1000 | Loss: 0.00001438
Iteration 63/1000 | Loss: 0.00001438
Iteration 64/1000 | Loss: 0.00001437
Iteration 65/1000 | Loss: 0.00001437
Iteration 66/1000 | Loss: 0.00001436
Iteration 67/1000 | Loss: 0.00001436
Iteration 68/1000 | Loss: 0.00001436
Iteration 69/1000 | Loss: 0.00001435
Iteration 70/1000 | Loss: 0.00001435
Iteration 71/1000 | Loss: 0.00001435
Iteration 72/1000 | Loss: 0.00001435
Iteration 73/1000 | Loss: 0.00001434
Iteration 74/1000 | Loss: 0.00001434
Iteration 75/1000 | Loss: 0.00001434
Iteration 76/1000 | Loss: 0.00001434
Iteration 77/1000 | Loss: 0.00001433
Iteration 78/1000 | Loss: 0.00001433
Iteration 79/1000 | Loss: 0.00001433
Iteration 80/1000 | Loss: 0.00001433
Iteration 81/1000 | Loss: 0.00001433
Iteration 82/1000 | Loss: 0.00001433
Iteration 83/1000 | Loss: 0.00001433
Iteration 84/1000 | Loss: 0.00001432
Iteration 85/1000 | Loss: 0.00001432
Iteration 86/1000 | Loss: 0.00001432
Iteration 87/1000 | Loss: 0.00001431
Iteration 88/1000 | Loss: 0.00001431
Iteration 89/1000 | Loss: 0.00001431
Iteration 90/1000 | Loss: 0.00001431
Iteration 91/1000 | Loss: 0.00001430
Iteration 92/1000 | Loss: 0.00001430
Iteration 93/1000 | Loss: 0.00001430
Iteration 94/1000 | Loss: 0.00001430
Iteration 95/1000 | Loss: 0.00001430
Iteration 96/1000 | Loss: 0.00001430
Iteration 97/1000 | Loss: 0.00001430
Iteration 98/1000 | Loss: 0.00001430
Iteration 99/1000 | Loss: 0.00001430
Iteration 100/1000 | Loss: 0.00001430
Iteration 101/1000 | Loss: 0.00001430
Iteration 102/1000 | Loss: 0.00001430
Iteration 103/1000 | Loss: 0.00001430
Iteration 104/1000 | Loss: 0.00001430
Iteration 105/1000 | Loss: 0.00001430
Iteration 106/1000 | Loss: 0.00001430
Iteration 107/1000 | Loss: 0.00001430
Iteration 108/1000 | Loss: 0.00001430
Iteration 109/1000 | Loss: 0.00001430
Iteration 110/1000 | Loss: 0.00001430
Iteration 111/1000 | Loss: 0.00001430
Iteration 112/1000 | Loss: 0.00001430
Iteration 113/1000 | Loss: 0.00001430
Iteration 114/1000 | Loss: 0.00001430
Iteration 115/1000 | Loss: 0.00001430
Iteration 116/1000 | Loss: 0.00001430
Iteration 117/1000 | Loss: 0.00001430
Iteration 118/1000 | Loss: 0.00001430
Iteration 119/1000 | Loss: 0.00001430
Iteration 120/1000 | Loss: 0.00001430
Iteration 121/1000 | Loss: 0.00001430
Iteration 122/1000 | Loss: 0.00001430
Iteration 123/1000 | Loss: 0.00001430
Iteration 124/1000 | Loss: 0.00001430
Iteration 125/1000 | Loss: 0.00001430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.4295825167209841e-05, 1.4295825167209841e-05, 1.4295825167209841e-05, 1.4295825167209841e-05, 1.4295825167209841e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4295825167209841e-05

Optimization complete. Final v2v error: 3.223677396774292 mm

Highest mean error: 3.5947465896606445 mm for frame 113

Lowest mean error: 2.821887731552124 mm for frame 147

Saving results

Total time: 38.164424419403076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902051
Iteration 2/25 | Loss: 0.00178263
Iteration 3/25 | Loss: 0.00145886
Iteration 4/25 | Loss: 0.00139706
Iteration 5/25 | Loss: 0.00138922
Iteration 6/25 | Loss: 0.00138741
Iteration 7/25 | Loss: 0.00138693
Iteration 8/25 | Loss: 0.00138693
Iteration 9/25 | Loss: 0.00138693
Iteration 10/25 | Loss: 0.00138693
Iteration 11/25 | Loss: 0.00138693
Iteration 12/25 | Loss: 0.00138693
Iteration 13/25 | Loss: 0.00138693
Iteration 14/25 | Loss: 0.00138693
Iteration 15/25 | Loss: 0.00138693
Iteration 16/25 | Loss: 0.00138693
Iteration 17/25 | Loss: 0.00138693
Iteration 18/25 | Loss: 0.00138693
Iteration 19/25 | Loss: 0.00138693
Iteration 20/25 | Loss: 0.00138693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013869322137907147, 0.0013869322137907147, 0.0013869322137907147, 0.0013869322137907147, 0.0013869322137907147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013869322137907147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49035060
Iteration 2/25 | Loss: 0.00108268
Iteration 3/25 | Loss: 0.00108259
Iteration 4/25 | Loss: 0.00108259
Iteration 5/25 | Loss: 0.00108259
Iteration 6/25 | Loss: 0.00108259
Iteration 7/25 | Loss: 0.00108259
Iteration 8/25 | Loss: 0.00108259
Iteration 9/25 | Loss: 0.00108259
Iteration 10/25 | Loss: 0.00108259
Iteration 11/25 | Loss: 0.00108259
Iteration 12/25 | Loss: 0.00108259
Iteration 13/25 | Loss: 0.00108259
Iteration 14/25 | Loss: 0.00108259
Iteration 15/25 | Loss: 0.00108259
Iteration 16/25 | Loss: 0.00108259
Iteration 17/25 | Loss: 0.00108259
Iteration 18/25 | Loss: 0.00108259
Iteration 19/25 | Loss: 0.00108259
Iteration 20/25 | Loss: 0.00108259
Iteration 21/25 | Loss: 0.00108259
Iteration 22/25 | Loss: 0.00108259
Iteration 23/25 | Loss: 0.00108259
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00108258705586195, 0.00108258705586195, 0.00108258705586195, 0.00108258705586195, 0.00108258705586195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00108258705586195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108259
Iteration 2/1000 | Loss: 0.00009151
Iteration 3/1000 | Loss: 0.00005842
Iteration 4/1000 | Loss: 0.00004406
Iteration 5/1000 | Loss: 0.00004028
Iteration 6/1000 | Loss: 0.00003812
Iteration 7/1000 | Loss: 0.00003688
Iteration 8/1000 | Loss: 0.00003584
Iteration 9/1000 | Loss: 0.00003515
Iteration 10/1000 | Loss: 0.00003457
Iteration 11/1000 | Loss: 0.00003416
Iteration 12/1000 | Loss: 0.00003382
Iteration 13/1000 | Loss: 0.00003355
Iteration 14/1000 | Loss: 0.00003326
Iteration 15/1000 | Loss: 0.00003301
Iteration 16/1000 | Loss: 0.00003286
Iteration 17/1000 | Loss: 0.00003270
Iteration 18/1000 | Loss: 0.00003253
Iteration 19/1000 | Loss: 0.00003252
Iteration 20/1000 | Loss: 0.00003237
Iteration 21/1000 | Loss: 0.00003226
Iteration 22/1000 | Loss: 0.00003223
Iteration 23/1000 | Loss: 0.00003211
Iteration 24/1000 | Loss: 0.00003210
Iteration 25/1000 | Loss: 0.00003209
Iteration 26/1000 | Loss: 0.00003207
Iteration 27/1000 | Loss: 0.00003206
Iteration 28/1000 | Loss: 0.00003206
Iteration 29/1000 | Loss: 0.00003205
Iteration 30/1000 | Loss: 0.00003205
Iteration 31/1000 | Loss: 0.00003203
Iteration 32/1000 | Loss: 0.00003202
Iteration 33/1000 | Loss: 0.00003202
Iteration 34/1000 | Loss: 0.00003202
Iteration 35/1000 | Loss: 0.00003201
Iteration 36/1000 | Loss: 0.00003201
Iteration 37/1000 | Loss: 0.00003200
Iteration 38/1000 | Loss: 0.00003200
Iteration 39/1000 | Loss: 0.00003199
Iteration 40/1000 | Loss: 0.00003199
Iteration 41/1000 | Loss: 0.00003198
Iteration 42/1000 | Loss: 0.00003198
Iteration 43/1000 | Loss: 0.00003198
Iteration 44/1000 | Loss: 0.00003197
Iteration 45/1000 | Loss: 0.00003197
Iteration 46/1000 | Loss: 0.00003197
Iteration 47/1000 | Loss: 0.00003196
Iteration 48/1000 | Loss: 0.00003196
Iteration 49/1000 | Loss: 0.00003195
Iteration 50/1000 | Loss: 0.00003195
Iteration 51/1000 | Loss: 0.00003194
Iteration 52/1000 | Loss: 0.00003194
Iteration 53/1000 | Loss: 0.00003194
Iteration 54/1000 | Loss: 0.00003193
Iteration 55/1000 | Loss: 0.00003193
Iteration 56/1000 | Loss: 0.00003192
Iteration 57/1000 | Loss: 0.00003192
Iteration 58/1000 | Loss: 0.00003192
Iteration 59/1000 | Loss: 0.00003192
Iteration 60/1000 | Loss: 0.00003191
Iteration 61/1000 | Loss: 0.00003191
Iteration 62/1000 | Loss: 0.00003191
Iteration 63/1000 | Loss: 0.00003191
Iteration 64/1000 | Loss: 0.00003190
Iteration 65/1000 | Loss: 0.00003190
Iteration 66/1000 | Loss: 0.00003190
Iteration 67/1000 | Loss: 0.00003190
Iteration 68/1000 | Loss: 0.00003190
Iteration 69/1000 | Loss: 0.00003190
Iteration 70/1000 | Loss: 0.00003189
Iteration 71/1000 | Loss: 0.00003189
Iteration 72/1000 | Loss: 0.00003189
Iteration 73/1000 | Loss: 0.00003189
Iteration 74/1000 | Loss: 0.00003189
Iteration 75/1000 | Loss: 0.00003189
Iteration 76/1000 | Loss: 0.00003188
Iteration 77/1000 | Loss: 0.00003188
Iteration 78/1000 | Loss: 0.00003188
Iteration 79/1000 | Loss: 0.00003188
Iteration 80/1000 | Loss: 0.00003188
Iteration 81/1000 | Loss: 0.00003188
Iteration 82/1000 | Loss: 0.00003187
Iteration 83/1000 | Loss: 0.00003187
Iteration 84/1000 | Loss: 0.00003187
Iteration 85/1000 | Loss: 0.00003187
Iteration 86/1000 | Loss: 0.00003187
Iteration 87/1000 | Loss: 0.00003186
Iteration 88/1000 | Loss: 0.00003186
Iteration 89/1000 | Loss: 0.00003186
Iteration 90/1000 | Loss: 0.00003186
Iteration 91/1000 | Loss: 0.00003186
Iteration 92/1000 | Loss: 0.00003185
Iteration 93/1000 | Loss: 0.00003185
Iteration 94/1000 | Loss: 0.00003185
Iteration 95/1000 | Loss: 0.00003185
Iteration 96/1000 | Loss: 0.00003185
Iteration 97/1000 | Loss: 0.00003185
Iteration 98/1000 | Loss: 0.00003184
Iteration 99/1000 | Loss: 0.00003184
Iteration 100/1000 | Loss: 0.00003184
Iteration 101/1000 | Loss: 0.00003184
Iteration 102/1000 | Loss: 0.00003183
Iteration 103/1000 | Loss: 0.00003183
Iteration 104/1000 | Loss: 0.00003183
Iteration 105/1000 | Loss: 0.00003183
Iteration 106/1000 | Loss: 0.00003183
Iteration 107/1000 | Loss: 0.00003183
Iteration 108/1000 | Loss: 0.00003183
Iteration 109/1000 | Loss: 0.00003183
Iteration 110/1000 | Loss: 0.00003183
Iteration 111/1000 | Loss: 0.00003182
Iteration 112/1000 | Loss: 0.00003182
Iteration 113/1000 | Loss: 0.00003182
Iteration 114/1000 | Loss: 0.00003182
Iteration 115/1000 | Loss: 0.00003181
Iteration 116/1000 | Loss: 0.00003181
Iteration 117/1000 | Loss: 0.00003181
Iteration 118/1000 | Loss: 0.00003181
Iteration 119/1000 | Loss: 0.00003181
Iteration 120/1000 | Loss: 0.00003181
Iteration 121/1000 | Loss: 0.00003180
Iteration 122/1000 | Loss: 0.00003180
Iteration 123/1000 | Loss: 0.00003180
Iteration 124/1000 | Loss: 0.00003180
Iteration 125/1000 | Loss: 0.00003180
Iteration 126/1000 | Loss: 0.00003180
Iteration 127/1000 | Loss: 0.00003180
Iteration 128/1000 | Loss: 0.00003180
Iteration 129/1000 | Loss: 0.00003180
Iteration 130/1000 | Loss: 0.00003180
Iteration 131/1000 | Loss: 0.00003180
Iteration 132/1000 | Loss: 0.00003179
Iteration 133/1000 | Loss: 0.00003179
Iteration 134/1000 | Loss: 0.00003179
Iteration 135/1000 | Loss: 0.00003178
Iteration 136/1000 | Loss: 0.00003178
Iteration 137/1000 | Loss: 0.00003178
Iteration 138/1000 | Loss: 0.00003178
Iteration 139/1000 | Loss: 0.00003178
Iteration 140/1000 | Loss: 0.00003178
Iteration 141/1000 | Loss: 0.00003178
Iteration 142/1000 | Loss: 0.00003178
Iteration 143/1000 | Loss: 0.00003178
Iteration 144/1000 | Loss: 0.00003178
Iteration 145/1000 | Loss: 0.00003178
Iteration 146/1000 | Loss: 0.00003178
Iteration 147/1000 | Loss: 0.00003178
Iteration 148/1000 | Loss: 0.00003178
Iteration 149/1000 | Loss: 0.00003178
Iteration 150/1000 | Loss: 0.00003178
Iteration 151/1000 | Loss: 0.00003178
Iteration 152/1000 | Loss: 0.00003178
Iteration 153/1000 | Loss: 0.00003178
Iteration 154/1000 | Loss: 0.00003177
Iteration 155/1000 | Loss: 0.00003177
Iteration 156/1000 | Loss: 0.00003177
Iteration 157/1000 | Loss: 0.00003177
Iteration 158/1000 | Loss: 0.00003177
Iteration 159/1000 | Loss: 0.00003177
Iteration 160/1000 | Loss: 0.00003177
Iteration 161/1000 | Loss: 0.00003177
Iteration 162/1000 | Loss: 0.00003177
Iteration 163/1000 | Loss: 0.00003177
Iteration 164/1000 | Loss: 0.00003177
Iteration 165/1000 | Loss: 0.00003177
Iteration 166/1000 | Loss: 0.00003177
Iteration 167/1000 | Loss: 0.00003177
Iteration 168/1000 | Loss: 0.00003177
Iteration 169/1000 | Loss: 0.00003177
Iteration 170/1000 | Loss: 0.00003177
Iteration 171/1000 | Loss: 0.00003177
Iteration 172/1000 | Loss: 0.00003177
Iteration 173/1000 | Loss: 0.00003177
Iteration 174/1000 | Loss: 0.00003177
Iteration 175/1000 | Loss: 0.00003177
Iteration 176/1000 | Loss: 0.00003177
Iteration 177/1000 | Loss: 0.00003177
Iteration 178/1000 | Loss: 0.00003177
Iteration 179/1000 | Loss: 0.00003177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [3.1771825888426974e-05, 3.1771825888426974e-05, 3.1771825888426974e-05, 3.1771825888426974e-05, 3.1771825888426974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1771825888426974e-05

Optimization complete. Final v2v error: 4.645641326904297 mm

Highest mean error: 6.855982780456543 mm for frame 83

Lowest mean error: 3.4818196296691895 mm for frame 41

Saving results

Total time: 48.21599769592285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00662744
Iteration 2/25 | Loss: 0.00172776
Iteration 3/25 | Loss: 0.00150881
Iteration 4/25 | Loss: 0.00149197
Iteration 5/25 | Loss: 0.00145785
Iteration 6/25 | Loss: 0.00145266
Iteration 7/25 | Loss: 0.00144994
Iteration 8/25 | Loss: 0.00144950
Iteration 9/25 | Loss: 0.00144820
Iteration 10/25 | Loss: 0.00144657
Iteration 11/25 | Loss: 0.00144608
Iteration 12/25 | Loss: 0.00144931
Iteration 13/25 | Loss: 0.00144986
Iteration 14/25 | Loss: 0.00144794
Iteration 15/25 | Loss: 0.00144552
Iteration 16/25 | Loss: 0.00144544
Iteration 17/25 | Loss: 0.00144540
Iteration 18/25 | Loss: 0.00144540
Iteration 19/25 | Loss: 0.00144540
Iteration 20/25 | Loss: 0.00144540
Iteration 21/25 | Loss: 0.00144540
Iteration 22/25 | Loss: 0.00144540
Iteration 23/25 | Loss: 0.00144540
Iteration 24/25 | Loss: 0.00144540
Iteration 25/25 | Loss: 0.00144539

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48095083
Iteration 2/25 | Loss: 0.00085856
Iteration 3/25 | Loss: 0.00085855
Iteration 4/25 | Loss: 0.00085855
Iteration 5/25 | Loss: 0.00085855
Iteration 6/25 | Loss: 0.00085855
Iteration 7/25 | Loss: 0.00085855
Iteration 8/25 | Loss: 0.00085855
Iteration 9/25 | Loss: 0.00085855
Iteration 10/25 | Loss: 0.00085855
Iteration 11/25 | Loss: 0.00085855
Iteration 12/25 | Loss: 0.00085855
Iteration 13/25 | Loss: 0.00085855
Iteration 14/25 | Loss: 0.00085855
Iteration 15/25 | Loss: 0.00085855
Iteration 16/25 | Loss: 0.00085855
Iteration 17/25 | Loss: 0.00085855
Iteration 18/25 | Loss: 0.00085855
Iteration 19/25 | Loss: 0.00085855
Iteration 20/25 | Loss: 0.00085855
Iteration 21/25 | Loss: 0.00085855
Iteration 22/25 | Loss: 0.00085855
Iteration 23/25 | Loss: 0.00085855
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008585521718487144, 0.0008585521718487144, 0.0008585521718487144, 0.0008585521718487144, 0.0008585521718487144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008585521718487144

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085855
Iteration 2/1000 | Loss: 0.00005858
Iteration 3/1000 | Loss: 0.00003943
Iteration 4/1000 | Loss: 0.00003585
Iteration 5/1000 | Loss: 0.00003424
Iteration 6/1000 | Loss: 0.00003300
Iteration 7/1000 | Loss: 0.00003203
Iteration 8/1000 | Loss: 0.00003145
Iteration 9/1000 | Loss: 0.00003101
Iteration 10/1000 | Loss: 0.00003058
Iteration 11/1000 | Loss: 0.00003024
Iteration 12/1000 | Loss: 0.00002997
Iteration 13/1000 | Loss: 0.00002971
Iteration 14/1000 | Loss: 0.00002951
Iteration 15/1000 | Loss: 0.00002934
Iteration 16/1000 | Loss: 0.00002924
Iteration 17/1000 | Loss: 0.00002914
Iteration 18/1000 | Loss: 0.00002913
Iteration 19/1000 | Loss: 0.00002909
Iteration 20/1000 | Loss: 0.00002906
Iteration 21/1000 | Loss: 0.00002906
Iteration 22/1000 | Loss: 0.00002905
Iteration 23/1000 | Loss: 0.00002904
Iteration 24/1000 | Loss: 0.00002903
Iteration 25/1000 | Loss: 0.00002903
Iteration 26/1000 | Loss: 0.00002903
Iteration 27/1000 | Loss: 0.00002901
Iteration 28/1000 | Loss: 0.00002898
Iteration 29/1000 | Loss: 0.00002898
Iteration 30/1000 | Loss: 0.00002897
Iteration 31/1000 | Loss: 0.00002897
Iteration 32/1000 | Loss: 0.00002896
Iteration 33/1000 | Loss: 0.00002896
Iteration 34/1000 | Loss: 0.00002895
Iteration 35/1000 | Loss: 0.00002895
Iteration 36/1000 | Loss: 0.00002895
Iteration 37/1000 | Loss: 0.00002895
Iteration 38/1000 | Loss: 0.00002894
Iteration 39/1000 | Loss: 0.00002894
Iteration 40/1000 | Loss: 0.00002894
Iteration 41/1000 | Loss: 0.00002894
Iteration 42/1000 | Loss: 0.00002893
Iteration 43/1000 | Loss: 0.00002893
Iteration 44/1000 | Loss: 0.00002892
Iteration 45/1000 | Loss: 0.00002892
Iteration 46/1000 | Loss: 0.00002892
Iteration 47/1000 | Loss: 0.00002891
Iteration 48/1000 | Loss: 0.00002890
Iteration 49/1000 | Loss: 0.00002890
Iteration 50/1000 | Loss: 0.00002889
Iteration 51/1000 | Loss: 0.00002889
Iteration 52/1000 | Loss: 0.00002888
Iteration 53/1000 | Loss: 0.00002888
Iteration 54/1000 | Loss: 0.00002887
Iteration 55/1000 | Loss: 0.00002886
Iteration 56/1000 | Loss: 0.00002886
Iteration 57/1000 | Loss: 0.00002886
Iteration 58/1000 | Loss: 0.00002885
Iteration 59/1000 | Loss: 0.00002885
Iteration 60/1000 | Loss: 0.00002885
Iteration 61/1000 | Loss: 0.00002884
Iteration 62/1000 | Loss: 0.00002884
Iteration 63/1000 | Loss: 0.00002883
Iteration 64/1000 | Loss: 0.00002883
Iteration 65/1000 | Loss: 0.00002883
Iteration 66/1000 | Loss: 0.00002882
Iteration 67/1000 | Loss: 0.00002882
Iteration 68/1000 | Loss: 0.00002882
Iteration 69/1000 | Loss: 0.00002882
Iteration 70/1000 | Loss: 0.00002882
Iteration 71/1000 | Loss: 0.00002881
Iteration 72/1000 | Loss: 0.00002881
Iteration 73/1000 | Loss: 0.00002881
Iteration 74/1000 | Loss: 0.00002880
Iteration 75/1000 | Loss: 0.00002880
Iteration 76/1000 | Loss: 0.00002880
Iteration 77/1000 | Loss: 0.00002880
Iteration 78/1000 | Loss: 0.00002880
Iteration 79/1000 | Loss: 0.00002880
Iteration 80/1000 | Loss: 0.00002880
Iteration 81/1000 | Loss: 0.00002879
Iteration 82/1000 | Loss: 0.00002879
Iteration 83/1000 | Loss: 0.00002879
Iteration 84/1000 | Loss: 0.00002879
Iteration 85/1000 | Loss: 0.00002879
Iteration 86/1000 | Loss: 0.00002879
Iteration 87/1000 | Loss: 0.00002879
Iteration 88/1000 | Loss: 0.00002878
Iteration 89/1000 | Loss: 0.00002878
Iteration 90/1000 | Loss: 0.00002877
Iteration 91/1000 | Loss: 0.00002877
Iteration 92/1000 | Loss: 0.00002877
Iteration 93/1000 | Loss: 0.00002877
Iteration 94/1000 | Loss: 0.00002876
Iteration 95/1000 | Loss: 0.00002876
Iteration 96/1000 | Loss: 0.00002876
Iteration 97/1000 | Loss: 0.00002876
Iteration 98/1000 | Loss: 0.00002876
Iteration 99/1000 | Loss: 0.00002875
Iteration 100/1000 | Loss: 0.00002875
Iteration 101/1000 | Loss: 0.00002875
Iteration 102/1000 | Loss: 0.00002875
Iteration 103/1000 | Loss: 0.00002875
Iteration 104/1000 | Loss: 0.00002875
Iteration 105/1000 | Loss: 0.00002875
Iteration 106/1000 | Loss: 0.00002874
Iteration 107/1000 | Loss: 0.00002874
Iteration 108/1000 | Loss: 0.00002874
Iteration 109/1000 | Loss: 0.00002874
Iteration 110/1000 | Loss: 0.00002874
Iteration 111/1000 | Loss: 0.00002874
Iteration 112/1000 | Loss: 0.00002874
Iteration 113/1000 | Loss: 0.00002874
Iteration 114/1000 | Loss: 0.00002873
Iteration 115/1000 | Loss: 0.00002873
Iteration 116/1000 | Loss: 0.00002873
Iteration 117/1000 | Loss: 0.00002873
Iteration 118/1000 | Loss: 0.00002873
Iteration 119/1000 | Loss: 0.00002873
Iteration 120/1000 | Loss: 0.00002873
Iteration 121/1000 | Loss: 0.00002873
Iteration 122/1000 | Loss: 0.00002873
Iteration 123/1000 | Loss: 0.00002873
Iteration 124/1000 | Loss: 0.00002873
Iteration 125/1000 | Loss: 0.00002873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.8734299121424556e-05, 2.8734299121424556e-05, 2.8734299121424556e-05, 2.8734299121424556e-05, 2.8734299121424556e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8734299121424556e-05

Optimization complete. Final v2v error: 4.452568054199219 mm

Highest mean error: 6.617081642150879 mm for frame 138

Lowest mean error: 3.845108985900879 mm for frame 136

Saving results

Total time: 68.93332433700562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995876
Iteration 2/25 | Loss: 0.00254032
Iteration 3/25 | Loss: 0.00256380
Iteration 4/25 | Loss: 0.00223823
Iteration 5/25 | Loss: 0.00205314
Iteration 6/25 | Loss: 0.00202647
Iteration 7/25 | Loss: 0.00186920
Iteration 8/25 | Loss: 0.00162270
Iteration 9/25 | Loss: 0.00155308
Iteration 10/25 | Loss: 0.00154446
Iteration 11/25 | Loss: 0.00151589
Iteration 12/25 | Loss: 0.00151783
Iteration 13/25 | Loss: 0.00148000
Iteration 14/25 | Loss: 0.00147738
Iteration 15/25 | Loss: 0.00147772
Iteration 16/25 | Loss: 0.00148871
Iteration 17/25 | Loss: 0.00147451
Iteration 18/25 | Loss: 0.00146904
Iteration 19/25 | Loss: 0.00146627
Iteration 20/25 | Loss: 0.00146569
Iteration 21/25 | Loss: 0.00146551
Iteration 22/25 | Loss: 0.00146549
Iteration 23/25 | Loss: 0.00146548
Iteration 24/25 | Loss: 0.00146548
Iteration 25/25 | Loss: 0.00146547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41491997
Iteration 2/25 | Loss: 0.00149677
Iteration 3/25 | Loss: 0.00123161
Iteration 4/25 | Loss: 0.00106906
Iteration 5/25 | Loss: 0.00106906
Iteration 6/25 | Loss: 0.00106906
Iteration 7/25 | Loss: 0.00106906
Iteration 8/25 | Loss: 0.00106906
Iteration 9/25 | Loss: 0.00106906
Iteration 10/25 | Loss: 0.00106906
Iteration 11/25 | Loss: 0.00106906
Iteration 12/25 | Loss: 0.00106906
Iteration 13/25 | Loss: 0.00106906
Iteration 14/25 | Loss: 0.00106906
Iteration 15/25 | Loss: 0.00106906
Iteration 16/25 | Loss: 0.00106906
Iteration 17/25 | Loss: 0.00106906
Iteration 18/25 | Loss: 0.00106906
Iteration 19/25 | Loss: 0.00106906
Iteration 20/25 | Loss: 0.00106906
Iteration 21/25 | Loss: 0.00106906
Iteration 22/25 | Loss: 0.00106906
Iteration 23/25 | Loss: 0.00106906
Iteration 24/25 | Loss: 0.00106906
Iteration 25/25 | Loss: 0.00106906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106906
Iteration 2/1000 | Loss: 0.00154627
Iteration 3/1000 | Loss: 0.00030011
Iteration 4/1000 | Loss: 0.00240527
Iteration 5/1000 | Loss: 0.00016714
Iteration 6/1000 | Loss: 0.00012187
Iteration 7/1000 | Loss: 0.00027579
Iteration 8/1000 | Loss: 0.00013363
Iteration 9/1000 | Loss: 0.00121457
Iteration 10/1000 | Loss: 0.00009838
Iteration 11/1000 | Loss: 0.00011156
Iteration 12/1000 | Loss: 0.00012621
Iteration 13/1000 | Loss: 0.00007639
Iteration 14/1000 | Loss: 0.00017471
Iteration 15/1000 | Loss: 0.00010697
Iteration 16/1000 | Loss: 0.00007426
Iteration 17/1000 | Loss: 0.00009842
Iteration 18/1000 | Loss: 0.00008139
Iteration 19/1000 | Loss: 0.00005729
Iteration 20/1000 | Loss: 0.00005161
Iteration 21/1000 | Loss: 0.00007053
Iteration 22/1000 | Loss: 0.00005695
Iteration 23/1000 | Loss: 0.00004960
Iteration 24/1000 | Loss: 0.00014178
Iteration 25/1000 | Loss: 0.00037274
Iteration 26/1000 | Loss: 0.00108671
Iteration 27/1000 | Loss: 0.00049594
Iteration 28/1000 | Loss: 0.00030875
Iteration 29/1000 | Loss: 0.00007052
Iteration 30/1000 | Loss: 0.00008049
Iteration 31/1000 | Loss: 0.00010989
Iteration 32/1000 | Loss: 0.00052389
Iteration 33/1000 | Loss: 0.00006783
Iteration 34/1000 | Loss: 0.00008823
Iteration 35/1000 | Loss: 0.00003349
Iteration 36/1000 | Loss: 0.00003158
Iteration 37/1000 | Loss: 0.00037260
Iteration 38/1000 | Loss: 0.00019556
Iteration 39/1000 | Loss: 0.00021951
Iteration 40/1000 | Loss: 0.00007741
Iteration 41/1000 | Loss: 0.00003927
Iteration 42/1000 | Loss: 0.00003535
Iteration 43/1000 | Loss: 0.00005364
Iteration 44/1000 | Loss: 0.00003408
Iteration 45/1000 | Loss: 0.00004174
Iteration 46/1000 | Loss: 0.00003423
Iteration 47/1000 | Loss: 0.00016887
Iteration 48/1000 | Loss: 0.00010652
Iteration 49/1000 | Loss: 0.00003263
Iteration 50/1000 | Loss: 0.00007472
Iteration 51/1000 | Loss: 0.00006810
Iteration 52/1000 | Loss: 0.00006847
Iteration 53/1000 | Loss: 0.00002695
Iteration 54/1000 | Loss: 0.00003751
Iteration 55/1000 | Loss: 0.00010035
Iteration 56/1000 | Loss: 0.00005196
Iteration 57/1000 | Loss: 0.00004250
Iteration 58/1000 | Loss: 0.00002560
Iteration 59/1000 | Loss: 0.00002527
Iteration 60/1000 | Loss: 0.00002497
Iteration 61/1000 | Loss: 0.00005963
Iteration 62/1000 | Loss: 0.00002468
Iteration 63/1000 | Loss: 0.00002449
Iteration 64/1000 | Loss: 0.00002438
Iteration 65/1000 | Loss: 0.00002436
Iteration 66/1000 | Loss: 0.00002434
Iteration 67/1000 | Loss: 0.00002433
Iteration 68/1000 | Loss: 0.00002433
Iteration 69/1000 | Loss: 0.00002432
Iteration 70/1000 | Loss: 0.00002432
Iteration 71/1000 | Loss: 0.00002431
Iteration 72/1000 | Loss: 0.00002431
Iteration 73/1000 | Loss: 0.00002431
Iteration 74/1000 | Loss: 0.00002430
Iteration 75/1000 | Loss: 0.00002430
Iteration 76/1000 | Loss: 0.00002429
Iteration 77/1000 | Loss: 0.00002429
Iteration 78/1000 | Loss: 0.00002428
Iteration 79/1000 | Loss: 0.00002428
Iteration 80/1000 | Loss: 0.00002427
Iteration 81/1000 | Loss: 0.00002427
Iteration 82/1000 | Loss: 0.00002426
Iteration 83/1000 | Loss: 0.00002426
Iteration 84/1000 | Loss: 0.00002425
Iteration 85/1000 | Loss: 0.00002422
Iteration 86/1000 | Loss: 0.00002421
Iteration 87/1000 | Loss: 0.00002419
Iteration 88/1000 | Loss: 0.00002418
Iteration 89/1000 | Loss: 0.00002418
Iteration 90/1000 | Loss: 0.00002415
Iteration 91/1000 | Loss: 0.00002415
Iteration 92/1000 | Loss: 0.00002409
Iteration 93/1000 | Loss: 0.00002409
Iteration 94/1000 | Loss: 0.00002408
Iteration 95/1000 | Loss: 0.00002408
Iteration 96/1000 | Loss: 0.00002408
Iteration 97/1000 | Loss: 0.00002408
Iteration 98/1000 | Loss: 0.00002408
Iteration 99/1000 | Loss: 0.00002408
Iteration 100/1000 | Loss: 0.00002408
Iteration 101/1000 | Loss: 0.00002407
Iteration 102/1000 | Loss: 0.00002407
Iteration 103/1000 | Loss: 0.00002407
Iteration 104/1000 | Loss: 0.00002407
Iteration 105/1000 | Loss: 0.00002407
Iteration 106/1000 | Loss: 0.00002406
Iteration 107/1000 | Loss: 0.00016570
Iteration 108/1000 | Loss: 0.00016570
Iteration 109/1000 | Loss: 0.00016570
Iteration 110/1000 | Loss: 0.00051616
Iteration 111/1000 | Loss: 0.00018967
Iteration 112/1000 | Loss: 0.00009947
Iteration 113/1000 | Loss: 0.00004086
Iteration 114/1000 | Loss: 0.00009566
Iteration 115/1000 | Loss: 0.00002539
Iteration 116/1000 | Loss: 0.00002466
Iteration 117/1000 | Loss: 0.00004080
Iteration 118/1000 | Loss: 0.00018618
Iteration 119/1000 | Loss: 0.00006364
Iteration 120/1000 | Loss: 0.00002417
Iteration 121/1000 | Loss: 0.00020302
Iteration 122/1000 | Loss: 0.00002710
Iteration 123/1000 | Loss: 0.00004652
Iteration 124/1000 | Loss: 0.00002430
Iteration 125/1000 | Loss: 0.00002403
Iteration 126/1000 | Loss: 0.00002388
Iteration 127/1000 | Loss: 0.00002381
Iteration 128/1000 | Loss: 0.00002378
Iteration 129/1000 | Loss: 0.00002375
Iteration 130/1000 | Loss: 0.00002375
Iteration 131/1000 | Loss: 0.00002374
Iteration 132/1000 | Loss: 0.00002374
Iteration 133/1000 | Loss: 0.00002374
Iteration 134/1000 | Loss: 0.00002373
Iteration 135/1000 | Loss: 0.00002373
Iteration 136/1000 | Loss: 0.00002372
Iteration 137/1000 | Loss: 0.00002372
Iteration 138/1000 | Loss: 0.00002372
Iteration 139/1000 | Loss: 0.00002371
Iteration 140/1000 | Loss: 0.00002371
Iteration 141/1000 | Loss: 0.00002370
Iteration 142/1000 | Loss: 0.00002370
Iteration 143/1000 | Loss: 0.00002369
Iteration 144/1000 | Loss: 0.00002369
Iteration 145/1000 | Loss: 0.00002367
Iteration 146/1000 | Loss: 0.00002367
Iteration 147/1000 | Loss: 0.00002367
Iteration 148/1000 | Loss: 0.00002367
Iteration 149/1000 | Loss: 0.00002367
Iteration 150/1000 | Loss: 0.00002366
Iteration 151/1000 | Loss: 0.00002366
Iteration 152/1000 | Loss: 0.00002365
Iteration 153/1000 | Loss: 0.00002364
Iteration 154/1000 | Loss: 0.00002364
Iteration 155/1000 | Loss: 0.00002364
Iteration 156/1000 | Loss: 0.00002364
Iteration 157/1000 | Loss: 0.00002364
Iteration 158/1000 | Loss: 0.00002364
Iteration 159/1000 | Loss: 0.00002364
Iteration 160/1000 | Loss: 0.00002364
Iteration 161/1000 | Loss: 0.00002364
Iteration 162/1000 | Loss: 0.00002364
Iteration 163/1000 | Loss: 0.00002363
Iteration 164/1000 | Loss: 0.00002363
Iteration 165/1000 | Loss: 0.00002363
Iteration 166/1000 | Loss: 0.00002362
Iteration 167/1000 | Loss: 0.00002362
Iteration 168/1000 | Loss: 0.00002362
Iteration 169/1000 | Loss: 0.00002362
Iteration 170/1000 | Loss: 0.00002362
Iteration 171/1000 | Loss: 0.00002362
Iteration 172/1000 | Loss: 0.00002362
Iteration 173/1000 | Loss: 0.00002361
Iteration 174/1000 | Loss: 0.00002361
Iteration 175/1000 | Loss: 0.00002360
Iteration 176/1000 | Loss: 0.00002360
Iteration 177/1000 | Loss: 0.00002360
Iteration 178/1000 | Loss: 0.00002359
Iteration 179/1000 | Loss: 0.00002359
Iteration 180/1000 | Loss: 0.00002359
Iteration 181/1000 | Loss: 0.00002359
Iteration 182/1000 | Loss: 0.00002359
Iteration 183/1000 | Loss: 0.00002358
Iteration 184/1000 | Loss: 0.00002358
Iteration 185/1000 | Loss: 0.00002358
Iteration 186/1000 | Loss: 0.00002355
Iteration 187/1000 | Loss: 0.00002355
Iteration 188/1000 | Loss: 0.00002355
Iteration 189/1000 | Loss: 0.00002354
Iteration 190/1000 | Loss: 0.00002354
Iteration 191/1000 | Loss: 0.00002354
Iteration 192/1000 | Loss: 0.00002354
Iteration 193/1000 | Loss: 0.00002354
Iteration 194/1000 | Loss: 0.00002354
Iteration 195/1000 | Loss: 0.00002354
Iteration 196/1000 | Loss: 0.00002354
Iteration 197/1000 | Loss: 0.00002354
Iteration 198/1000 | Loss: 0.00002354
Iteration 199/1000 | Loss: 0.00002353
Iteration 200/1000 | Loss: 0.00002353
Iteration 201/1000 | Loss: 0.00002353
Iteration 202/1000 | Loss: 0.00002353
Iteration 203/1000 | Loss: 0.00002353
Iteration 204/1000 | Loss: 0.00002353
Iteration 205/1000 | Loss: 0.00002353
Iteration 206/1000 | Loss: 0.00002353
Iteration 207/1000 | Loss: 0.00002353
Iteration 208/1000 | Loss: 0.00002353
Iteration 209/1000 | Loss: 0.00002353
Iteration 210/1000 | Loss: 0.00002353
Iteration 211/1000 | Loss: 0.00002352
Iteration 212/1000 | Loss: 0.00002352
Iteration 213/1000 | Loss: 0.00002352
Iteration 214/1000 | Loss: 0.00002352
Iteration 215/1000 | Loss: 0.00002352
Iteration 216/1000 | Loss: 0.00002352
Iteration 217/1000 | Loss: 0.00002352
Iteration 218/1000 | Loss: 0.00002352
Iteration 219/1000 | Loss: 0.00002352
Iteration 220/1000 | Loss: 0.00002352
Iteration 221/1000 | Loss: 0.00002352
Iteration 222/1000 | Loss: 0.00002352
Iteration 223/1000 | Loss: 0.00002351
Iteration 224/1000 | Loss: 0.00002351
Iteration 225/1000 | Loss: 0.00002351
Iteration 226/1000 | Loss: 0.00002350
Iteration 227/1000 | Loss: 0.00002350
Iteration 228/1000 | Loss: 0.00002350
Iteration 229/1000 | Loss: 0.00002350
Iteration 230/1000 | Loss: 0.00002350
Iteration 231/1000 | Loss: 0.00002350
Iteration 232/1000 | Loss: 0.00002350
Iteration 233/1000 | Loss: 0.00002350
Iteration 234/1000 | Loss: 0.00002350
Iteration 235/1000 | Loss: 0.00002350
Iteration 236/1000 | Loss: 0.00002350
Iteration 237/1000 | Loss: 0.00002350
Iteration 238/1000 | Loss: 0.00002349
Iteration 239/1000 | Loss: 0.00002349
Iteration 240/1000 | Loss: 0.00002349
Iteration 241/1000 | Loss: 0.00002349
Iteration 242/1000 | Loss: 0.00002349
Iteration 243/1000 | Loss: 0.00002349
Iteration 244/1000 | Loss: 0.00002349
Iteration 245/1000 | Loss: 0.00002349
Iteration 246/1000 | Loss: 0.00002349
Iteration 247/1000 | Loss: 0.00002349
Iteration 248/1000 | Loss: 0.00002349
Iteration 249/1000 | Loss: 0.00002349
Iteration 250/1000 | Loss: 0.00002349
Iteration 251/1000 | Loss: 0.00002349
Iteration 252/1000 | Loss: 0.00002348
Iteration 253/1000 | Loss: 0.00002348
Iteration 254/1000 | Loss: 0.00002348
Iteration 255/1000 | Loss: 0.00002348
Iteration 256/1000 | Loss: 0.00002348
Iteration 257/1000 | Loss: 0.00002347
Iteration 258/1000 | Loss: 0.00002347
Iteration 259/1000 | Loss: 0.00002347
Iteration 260/1000 | Loss: 0.00002347
Iteration 261/1000 | Loss: 0.00002347
Iteration 262/1000 | Loss: 0.00002347
Iteration 263/1000 | Loss: 0.00002347
Iteration 264/1000 | Loss: 0.00002347
Iteration 265/1000 | Loss: 0.00002347
Iteration 266/1000 | Loss: 0.00002347
Iteration 267/1000 | Loss: 0.00002347
Iteration 268/1000 | Loss: 0.00002347
Iteration 269/1000 | Loss: 0.00002347
Iteration 270/1000 | Loss: 0.00002347
Iteration 271/1000 | Loss: 0.00002347
Iteration 272/1000 | Loss: 0.00002347
Iteration 273/1000 | Loss: 0.00002347
Iteration 274/1000 | Loss: 0.00002347
Iteration 275/1000 | Loss: 0.00002347
Iteration 276/1000 | Loss: 0.00002347
Iteration 277/1000 | Loss: 0.00002347
Iteration 278/1000 | Loss: 0.00002347
Iteration 279/1000 | Loss: 0.00002347
Iteration 280/1000 | Loss: 0.00002347
Iteration 281/1000 | Loss: 0.00002347
Iteration 282/1000 | Loss: 0.00002347
Iteration 283/1000 | Loss: 0.00002347
Iteration 284/1000 | Loss: 0.00002347
Iteration 285/1000 | Loss: 0.00002347
Iteration 286/1000 | Loss: 0.00002347
Iteration 287/1000 | Loss: 0.00002347
Iteration 288/1000 | Loss: 0.00002347
Iteration 289/1000 | Loss: 0.00002347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 289. Stopping optimization.
Last 5 losses: [2.3472861357731745e-05, 2.3472861357731745e-05, 2.3472861357731745e-05, 2.3472861357731745e-05, 2.3472861357731745e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3472861357731745e-05

Optimization complete. Final v2v error: 4.026101112365723 mm

Highest mean error: 5.064535140991211 mm for frame 92

Lowest mean error: 3.708513021469116 mm for frame 12

Saving results

Total time: 178.60711693763733
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00545795
Iteration 2/25 | Loss: 0.00153950
Iteration 3/25 | Loss: 0.00136738
Iteration 4/25 | Loss: 0.00134502
Iteration 5/25 | Loss: 0.00132825
Iteration 6/25 | Loss: 0.00132119
Iteration 7/25 | Loss: 0.00130021
Iteration 8/25 | Loss: 0.00129542
Iteration 9/25 | Loss: 0.00129456
Iteration 10/25 | Loss: 0.00129443
Iteration 11/25 | Loss: 0.00129443
Iteration 12/25 | Loss: 0.00129443
Iteration 13/25 | Loss: 0.00129443
Iteration 14/25 | Loss: 0.00129443
Iteration 15/25 | Loss: 0.00129442
Iteration 16/25 | Loss: 0.00129442
Iteration 17/25 | Loss: 0.00129442
Iteration 18/25 | Loss: 0.00129442
Iteration 19/25 | Loss: 0.00129442
Iteration 20/25 | Loss: 0.00129442
Iteration 21/25 | Loss: 0.00129442
Iteration 22/25 | Loss: 0.00129442
Iteration 23/25 | Loss: 0.00129442
Iteration 24/25 | Loss: 0.00129442
Iteration 25/25 | Loss: 0.00129441

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79183102
Iteration 2/25 | Loss: 0.00088407
Iteration 3/25 | Loss: 0.00088405
Iteration 4/25 | Loss: 0.00088405
Iteration 5/25 | Loss: 0.00088405
Iteration 6/25 | Loss: 0.00088405
Iteration 7/25 | Loss: 0.00088405
Iteration 8/25 | Loss: 0.00088405
Iteration 9/25 | Loss: 0.00088405
Iteration 10/25 | Loss: 0.00088405
Iteration 11/25 | Loss: 0.00088405
Iteration 12/25 | Loss: 0.00088405
Iteration 13/25 | Loss: 0.00088405
Iteration 14/25 | Loss: 0.00088405
Iteration 15/25 | Loss: 0.00088405
Iteration 16/25 | Loss: 0.00088405
Iteration 17/25 | Loss: 0.00088405
Iteration 18/25 | Loss: 0.00088405
Iteration 19/25 | Loss: 0.00088405
Iteration 20/25 | Loss: 0.00088405
Iteration 21/25 | Loss: 0.00088405
Iteration 22/25 | Loss: 0.00088405
Iteration 23/25 | Loss: 0.00088405
Iteration 24/25 | Loss: 0.00088405
Iteration 25/25 | Loss: 0.00088405

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088405
Iteration 2/1000 | Loss: 0.00003663
Iteration 3/1000 | Loss: 0.00002733
Iteration 4/1000 | Loss: 0.00002473
Iteration 5/1000 | Loss: 0.00002336
Iteration 6/1000 | Loss: 0.00002230
Iteration 7/1000 | Loss: 0.00002157
Iteration 8/1000 | Loss: 0.00002108
Iteration 9/1000 | Loss: 0.00002080
Iteration 10/1000 | Loss: 0.00002040
Iteration 11/1000 | Loss: 0.00002014
Iteration 12/1000 | Loss: 0.00001995
Iteration 13/1000 | Loss: 0.00001978
Iteration 14/1000 | Loss: 0.00001969
Iteration 15/1000 | Loss: 0.00001969
Iteration 16/1000 | Loss: 0.00001967
Iteration 17/1000 | Loss: 0.00001966
Iteration 18/1000 | Loss: 0.00001965
Iteration 19/1000 | Loss: 0.00001964
Iteration 20/1000 | Loss: 0.00001959
Iteration 21/1000 | Loss: 0.00001958
Iteration 22/1000 | Loss: 0.00001956
Iteration 23/1000 | Loss: 0.00001956
Iteration 24/1000 | Loss: 0.00001953
Iteration 25/1000 | Loss: 0.00001950
Iteration 26/1000 | Loss: 0.00001950
Iteration 27/1000 | Loss: 0.00001945
Iteration 28/1000 | Loss: 0.00001942
Iteration 29/1000 | Loss: 0.00001941
Iteration 30/1000 | Loss: 0.00001941
Iteration 31/1000 | Loss: 0.00001940
Iteration 32/1000 | Loss: 0.00001939
Iteration 33/1000 | Loss: 0.00001939
Iteration 34/1000 | Loss: 0.00001938
Iteration 35/1000 | Loss: 0.00001935
Iteration 36/1000 | Loss: 0.00001935
Iteration 37/1000 | Loss: 0.00001933
Iteration 38/1000 | Loss: 0.00001932
Iteration 39/1000 | Loss: 0.00001932
Iteration 40/1000 | Loss: 0.00001931
Iteration 41/1000 | Loss: 0.00001931
Iteration 42/1000 | Loss: 0.00001931
Iteration 43/1000 | Loss: 0.00001931
Iteration 44/1000 | Loss: 0.00001931
Iteration 45/1000 | Loss: 0.00001930
Iteration 46/1000 | Loss: 0.00001930
Iteration 47/1000 | Loss: 0.00001930
Iteration 48/1000 | Loss: 0.00001930
Iteration 49/1000 | Loss: 0.00001929
Iteration 50/1000 | Loss: 0.00001929
Iteration 51/1000 | Loss: 0.00001929
Iteration 52/1000 | Loss: 0.00001928
Iteration 53/1000 | Loss: 0.00001928
Iteration 54/1000 | Loss: 0.00001928
Iteration 55/1000 | Loss: 0.00001927
Iteration 56/1000 | Loss: 0.00001926
Iteration 57/1000 | Loss: 0.00001926
Iteration 58/1000 | Loss: 0.00001926
Iteration 59/1000 | Loss: 0.00001925
Iteration 60/1000 | Loss: 0.00001925
Iteration 61/1000 | Loss: 0.00001924
Iteration 62/1000 | Loss: 0.00001924
Iteration 63/1000 | Loss: 0.00001924
Iteration 64/1000 | Loss: 0.00001924
Iteration 65/1000 | Loss: 0.00001924
Iteration 66/1000 | Loss: 0.00001924
Iteration 67/1000 | Loss: 0.00001923
Iteration 68/1000 | Loss: 0.00001923
Iteration 69/1000 | Loss: 0.00001923
Iteration 70/1000 | Loss: 0.00001923
Iteration 71/1000 | Loss: 0.00001923
Iteration 72/1000 | Loss: 0.00001922
Iteration 73/1000 | Loss: 0.00001922
Iteration 74/1000 | Loss: 0.00001922
Iteration 75/1000 | Loss: 0.00001922
Iteration 76/1000 | Loss: 0.00001921
Iteration 77/1000 | Loss: 0.00001921
Iteration 78/1000 | Loss: 0.00001921
Iteration 79/1000 | Loss: 0.00001921
Iteration 80/1000 | Loss: 0.00001921
Iteration 81/1000 | Loss: 0.00001921
Iteration 82/1000 | Loss: 0.00001921
Iteration 83/1000 | Loss: 0.00001921
Iteration 84/1000 | Loss: 0.00001920
Iteration 85/1000 | Loss: 0.00001920
Iteration 86/1000 | Loss: 0.00001920
Iteration 87/1000 | Loss: 0.00001920
Iteration 88/1000 | Loss: 0.00001920
Iteration 89/1000 | Loss: 0.00001920
Iteration 90/1000 | Loss: 0.00001920
Iteration 91/1000 | Loss: 0.00001919
Iteration 92/1000 | Loss: 0.00001919
Iteration 93/1000 | Loss: 0.00001919
Iteration 94/1000 | Loss: 0.00001919
Iteration 95/1000 | Loss: 0.00001919
Iteration 96/1000 | Loss: 0.00001919
Iteration 97/1000 | Loss: 0.00001919
Iteration 98/1000 | Loss: 0.00001918
Iteration 99/1000 | Loss: 0.00001918
Iteration 100/1000 | Loss: 0.00001918
Iteration 101/1000 | Loss: 0.00001918
Iteration 102/1000 | Loss: 0.00001918
Iteration 103/1000 | Loss: 0.00001918
Iteration 104/1000 | Loss: 0.00001917
Iteration 105/1000 | Loss: 0.00001917
Iteration 106/1000 | Loss: 0.00001917
Iteration 107/1000 | Loss: 0.00001917
Iteration 108/1000 | Loss: 0.00001917
Iteration 109/1000 | Loss: 0.00001917
Iteration 110/1000 | Loss: 0.00001917
Iteration 111/1000 | Loss: 0.00001916
Iteration 112/1000 | Loss: 0.00001916
Iteration 113/1000 | Loss: 0.00001916
Iteration 114/1000 | Loss: 0.00001916
Iteration 115/1000 | Loss: 0.00001916
Iteration 116/1000 | Loss: 0.00001916
Iteration 117/1000 | Loss: 0.00001915
Iteration 118/1000 | Loss: 0.00001915
Iteration 119/1000 | Loss: 0.00001915
Iteration 120/1000 | Loss: 0.00001915
Iteration 121/1000 | Loss: 0.00001914
Iteration 122/1000 | Loss: 0.00001914
Iteration 123/1000 | Loss: 0.00001914
Iteration 124/1000 | Loss: 0.00001914
Iteration 125/1000 | Loss: 0.00001913
Iteration 126/1000 | Loss: 0.00001913
Iteration 127/1000 | Loss: 0.00001913
Iteration 128/1000 | Loss: 0.00001912
Iteration 129/1000 | Loss: 0.00001912
Iteration 130/1000 | Loss: 0.00001912
Iteration 131/1000 | Loss: 0.00001912
Iteration 132/1000 | Loss: 0.00001912
Iteration 133/1000 | Loss: 0.00001911
Iteration 134/1000 | Loss: 0.00001911
Iteration 135/1000 | Loss: 0.00001911
Iteration 136/1000 | Loss: 0.00001910
Iteration 137/1000 | Loss: 0.00001910
Iteration 138/1000 | Loss: 0.00001910
Iteration 139/1000 | Loss: 0.00001909
Iteration 140/1000 | Loss: 0.00001909
Iteration 141/1000 | Loss: 0.00001909
Iteration 142/1000 | Loss: 0.00001908
Iteration 143/1000 | Loss: 0.00001908
Iteration 144/1000 | Loss: 0.00001908
Iteration 145/1000 | Loss: 0.00001908
Iteration 146/1000 | Loss: 0.00001908
Iteration 147/1000 | Loss: 0.00001907
Iteration 148/1000 | Loss: 0.00001907
Iteration 149/1000 | Loss: 0.00001907
Iteration 150/1000 | Loss: 0.00001907
Iteration 151/1000 | Loss: 0.00001907
Iteration 152/1000 | Loss: 0.00001907
Iteration 153/1000 | Loss: 0.00001906
Iteration 154/1000 | Loss: 0.00001906
Iteration 155/1000 | Loss: 0.00001906
Iteration 156/1000 | Loss: 0.00001906
Iteration 157/1000 | Loss: 0.00001905
Iteration 158/1000 | Loss: 0.00001905
Iteration 159/1000 | Loss: 0.00001905
Iteration 160/1000 | Loss: 0.00001905
Iteration 161/1000 | Loss: 0.00001905
Iteration 162/1000 | Loss: 0.00001905
Iteration 163/1000 | Loss: 0.00001905
Iteration 164/1000 | Loss: 0.00001904
Iteration 165/1000 | Loss: 0.00001904
Iteration 166/1000 | Loss: 0.00001904
Iteration 167/1000 | Loss: 0.00001904
Iteration 168/1000 | Loss: 0.00001904
Iteration 169/1000 | Loss: 0.00001904
Iteration 170/1000 | Loss: 0.00001904
Iteration 171/1000 | Loss: 0.00001904
Iteration 172/1000 | Loss: 0.00001904
Iteration 173/1000 | Loss: 0.00001904
Iteration 174/1000 | Loss: 0.00001904
Iteration 175/1000 | Loss: 0.00001903
Iteration 176/1000 | Loss: 0.00001903
Iteration 177/1000 | Loss: 0.00001903
Iteration 178/1000 | Loss: 0.00001903
Iteration 179/1000 | Loss: 0.00001903
Iteration 180/1000 | Loss: 0.00001903
Iteration 181/1000 | Loss: 0.00001903
Iteration 182/1000 | Loss: 0.00001903
Iteration 183/1000 | Loss: 0.00001903
Iteration 184/1000 | Loss: 0.00001903
Iteration 185/1000 | Loss: 0.00001903
Iteration 186/1000 | Loss: 0.00001902
Iteration 187/1000 | Loss: 0.00001902
Iteration 188/1000 | Loss: 0.00001902
Iteration 189/1000 | Loss: 0.00001902
Iteration 190/1000 | Loss: 0.00001902
Iteration 191/1000 | Loss: 0.00001902
Iteration 192/1000 | Loss: 0.00001902
Iteration 193/1000 | Loss: 0.00001902
Iteration 194/1000 | Loss: 0.00001902
Iteration 195/1000 | Loss: 0.00001901
Iteration 196/1000 | Loss: 0.00001901
Iteration 197/1000 | Loss: 0.00001901
Iteration 198/1000 | Loss: 0.00001901
Iteration 199/1000 | Loss: 0.00001901
Iteration 200/1000 | Loss: 0.00001901
Iteration 201/1000 | Loss: 0.00001901
Iteration 202/1000 | Loss: 0.00001901
Iteration 203/1000 | Loss: 0.00001901
Iteration 204/1000 | Loss: 0.00001901
Iteration 205/1000 | Loss: 0.00001901
Iteration 206/1000 | Loss: 0.00001901
Iteration 207/1000 | Loss: 0.00001901
Iteration 208/1000 | Loss: 0.00001900
Iteration 209/1000 | Loss: 0.00001900
Iteration 210/1000 | Loss: 0.00001900
Iteration 211/1000 | Loss: 0.00001900
Iteration 212/1000 | Loss: 0.00001900
Iteration 213/1000 | Loss: 0.00001900
Iteration 214/1000 | Loss: 0.00001900
Iteration 215/1000 | Loss: 0.00001900
Iteration 216/1000 | Loss: 0.00001900
Iteration 217/1000 | Loss: 0.00001900
Iteration 218/1000 | Loss: 0.00001900
Iteration 219/1000 | Loss: 0.00001900
Iteration 220/1000 | Loss: 0.00001900
Iteration 221/1000 | Loss: 0.00001900
Iteration 222/1000 | Loss: 0.00001900
Iteration 223/1000 | Loss: 0.00001900
Iteration 224/1000 | Loss: 0.00001900
Iteration 225/1000 | Loss: 0.00001900
Iteration 226/1000 | Loss: 0.00001900
Iteration 227/1000 | Loss: 0.00001900
Iteration 228/1000 | Loss: 0.00001900
Iteration 229/1000 | Loss: 0.00001900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.900254937936552e-05, 1.900254937936552e-05, 1.900254937936552e-05, 1.900254937936552e-05, 1.900254937936552e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.900254937936552e-05

Optimization complete. Final v2v error: 3.6250712871551514 mm

Highest mean error: 4.703062534332275 mm for frame 107

Lowest mean error: 3.073824167251587 mm for frame 127

Saving results

Total time: 61.39676523208618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00779570
Iteration 2/25 | Loss: 0.00148911
Iteration 3/25 | Loss: 0.00140673
Iteration 4/25 | Loss: 0.00137786
Iteration 5/25 | Loss: 0.00137247
Iteration 6/25 | Loss: 0.00138474
Iteration 7/25 | Loss: 0.00136472
Iteration 8/25 | Loss: 0.00137461
Iteration 9/25 | Loss: 0.00136185
Iteration 10/25 | Loss: 0.00136121
Iteration 11/25 | Loss: 0.00136075
Iteration 12/25 | Loss: 0.00136060
Iteration 13/25 | Loss: 0.00136058
Iteration 14/25 | Loss: 0.00136052
Iteration 15/25 | Loss: 0.00136051
Iteration 16/25 | Loss: 0.00136051
Iteration 17/25 | Loss: 0.00136051
Iteration 18/25 | Loss: 0.00136051
Iteration 19/25 | Loss: 0.00136051
Iteration 20/25 | Loss: 0.00136051
Iteration 21/25 | Loss: 0.00136051
Iteration 22/25 | Loss: 0.00136051
Iteration 23/25 | Loss: 0.00136051
Iteration 24/25 | Loss: 0.00136051
Iteration 25/25 | Loss: 0.00136050

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.93844414
Iteration 2/25 | Loss: 0.00086096
Iteration 3/25 | Loss: 0.00086089
Iteration 4/25 | Loss: 0.00086089
Iteration 5/25 | Loss: 0.00086088
Iteration 6/25 | Loss: 0.00086088
Iteration 7/25 | Loss: 0.00086088
Iteration 8/25 | Loss: 0.00086088
Iteration 9/25 | Loss: 0.00086088
Iteration 10/25 | Loss: 0.00086088
Iteration 11/25 | Loss: 0.00086088
Iteration 12/25 | Loss: 0.00086088
Iteration 13/25 | Loss: 0.00086088
Iteration 14/25 | Loss: 0.00086088
Iteration 15/25 | Loss: 0.00086088
Iteration 16/25 | Loss: 0.00086088
Iteration 17/25 | Loss: 0.00086088
Iteration 18/25 | Loss: 0.00086088
Iteration 19/25 | Loss: 0.00086088
Iteration 20/25 | Loss: 0.00086088
Iteration 21/25 | Loss: 0.00086088
Iteration 22/25 | Loss: 0.00086088
Iteration 23/25 | Loss: 0.00086088
Iteration 24/25 | Loss: 0.00086088
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008608817588537931, 0.0008608817588537931, 0.0008608817588537931, 0.0008608817588537931, 0.0008608817588537931]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008608817588537931

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086088
Iteration 2/1000 | Loss: 0.00003575
Iteration 3/1000 | Loss: 0.00002459
Iteration 4/1000 | Loss: 0.00002210
Iteration 5/1000 | Loss: 0.00002103
Iteration 6/1000 | Loss: 0.00002028
Iteration 7/1000 | Loss: 0.00001979
Iteration 8/1000 | Loss: 0.00001945
Iteration 9/1000 | Loss: 0.00001912
Iteration 10/1000 | Loss: 0.00001883
Iteration 11/1000 | Loss: 0.00001864
Iteration 12/1000 | Loss: 0.00001857
Iteration 13/1000 | Loss: 0.00001851
Iteration 14/1000 | Loss: 0.00001839
Iteration 15/1000 | Loss: 0.00001837
Iteration 16/1000 | Loss: 0.00001836
Iteration 17/1000 | Loss: 0.00001833
Iteration 18/1000 | Loss: 0.00001831
Iteration 19/1000 | Loss: 0.00001829
Iteration 20/1000 | Loss: 0.00001825
Iteration 21/1000 | Loss: 0.00001823
Iteration 22/1000 | Loss: 0.00001822
Iteration 23/1000 | Loss: 0.00001817
Iteration 24/1000 | Loss: 0.00001814
Iteration 25/1000 | Loss: 0.00001812
Iteration 26/1000 | Loss: 0.00001811
Iteration 27/1000 | Loss: 0.00001811
Iteration 28/1000 | Loss: 0.00001804
Iteration 29/1000 | Loss: 0.00001804
Iteration 30/1000 | Loss: 0.00001802
Iteration 31/1000 | Loss: 0.00001802
Iteration 32/1000 | Loss: 0.00001802
Iteration 33/1000 | Loss: 0.00001801
Iteration 34/1000 | Loss: 0.00001800
Iteration 35/1000 | Loss: 0.00001799
Iteration 36/1000 | Loss: 0.00001799
Iteration 37/1000 | Loss: 0.00001799
Iteration 38/1000 | Loss: 0.00001798
Iteration 39/1000 | Loss: 0.00001798
Iteration 40/1000 | Loss: 0.00001798
Iteration 41/1000 | Loss: 0.00001797
Iteration 42/1000 | Loss: 0.00001796
Iteration 43/1000 | Loss: 0.00001795
Iteration 44/1000 | Loss: 0.00001795
Iteration 45/1000 | Loss: 0.00001794
Iteration 46/1000 | Loss: 0.00001794
Iteration 47/1000 | Loss: 0.00001794
Iteration 48/1000 | Loss: 0.00001793
Iteration 49/1000 | Loss: 0.00001793
Iteration 50/1000 | Loss: 0.00001792
Iteration 51/1000 | Loss: 0.00001792
Iteration 52/1000 | Loss: 0.00001792
Iteration 53/1000 | Loss: 0.00001792
Iteration 54/1000 | Loss: 0.00001792
Iteration 55/1000 | Loss: 0.00001791
Iteration 56/1000 | Loss: 0.00001791
Iteration 57/1000 | Loss: 0.00001791
Iteration 58/1000 | Loss: 0.00001791
Iteration 59/1000 | Loss: 0.00001791
Iteration 60/1000 | Loss: 0.00001790
Iteration 61/1000 | Loss: 0.00001790
Iteration 62/1000 | Loss: 0.00001789
Iteration 63/1000 | Loss: 0.00001789
Iteration 64/1000 | Loss: 0.00001789
Iteration 65/1000 | Loss: 0.00001788
Iteration 66/1000 | Loss: 0.00001787
Iteration 67/1000 | Loss: 0.00001787
Iteration 68/1000 | Loss: 0.00001787
Iteration 69/1000 | Loss: 0.00001786
Iteration 70/1000 | Loss: 0.00001786
Iteration 71/1000 | Loss: 0.00001786
Iteration 72/1000 | Loss: 0.00001785
Iteration 73/1000 | Loss: 0.00001785
Iteration 74/1000 | Loss: 0.00001784
Iteration 75/1000 | Loss: 0.00001784
Iteration 76/1000 | Loss: 0.00001784
Iteration 77/1000 | Loss: 0.00001783
Iteration 78/1000 | Loss: 0.00001783
Iteration 79/1000 | Loss: 0.00001783
Iteration 80/1000 | Loss: 0.00001782
Iteration 81/1000 | Loss: 0.00001782
Iteration 82/1000 | Loss: 0.00001782
Iteration 83/1000 | Loss: 0.00001782
Iteration 84/1000 | Loss: 0.00001782
Iteration 85/1000 | Loss: 0.00001781
Iteration 86/1000 | Loss: 0.00001781
Iteration 87/1000 | Loss: 0.00001781
Iteration 88/1000 | Loss: 0.00001781
Iteration 89/1000 | Loss: 0.00001780
Iteration 90/1000 | Loss: 0.00001780
Iteration 91/1000 | Loss: 0.00001780
Iteration 92/1000 | Loss: 0.00001779
Iteration 93/1000 | Loss: 0.00001779
Iteration 94/1000 | Loss: 0.00001779
Iteration 95/1000 | Loss: 0.00001779
Iteration 96/1000 | Loss: 0.00001779
Iteration 97/1000 | Loss: 0.00001779
Iteration 98/1000 | Loss: 0.00001779
Iteration 99/1000 | Loss: 0.00001778
Iteration 100/1000 | Loss: 0.00001778
Iteration 101/1000 | Loss: 0.00001778
Iteration 102/1000 | Loss: 0.00001778
Iteration 103/1000 | Loss: 0.00001778
Iteration 104/1000 | Loss: 0.00001778
Iteration 105/1000 | Loss: 0.00001777
Iteration 106/1000 | Loss: 0.00001777
Iteration 107/1000 | Loss: 0.00001777
Iteration 108/1000 | Loss: 0.00001777
Iteration 109/1000 | Loss: 0.00001777
Iteration 110/1000 | Loss: 0.00001777
Iteration 111/1000 | Loss: 0.00001777
Iteration 112/1000 | Loss: 0.00001777
Iteration 113/1000 | Loss: 0.00001777
Iteration 114/1000 | Loss: 0.00001776
Iteration 115/1000 | Loss: 0.00001776
Iteration 116/1000 | Loss: 0.00001776
Iteration 117/1000 | Loss: 0.00001776
Iteration 118/1000 | Loss: 0.00001776
Iteration 119/1000 | Loss: 0.00001776
Iteration 120/1000 | Loss: 0.00001776
Iteration 121/1000 | Loss: 0.00001776
Iteration 122/1000 | Loss: 0.00001776
Iteration 123/1000 | Loss: 0.00001776
Iteration 124/1000 | Loss: 0.00001776
Iteration 125/1000 | Loss: 0.00001776
Iteration 126/1000 | Loss: 0.00001776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.776186218194198e-05, 1.776186218194198e-05, 1.776186218194198e-05, 1.776186218194198e-05, 1.776186218194198e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.776186218194198e-05

Optimization complete. Final v2v error: 3.520785331726074 mm

Highest mean error: 4.06832218170166 mm for frame 165

Lowest mean error: 3.0125820636749268 mm for frame 0

Saving results

Total time: 55.20690178871155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923305
Iteration 2/25 | Loss: 0.00339359
Iteration 3/25 | Loss: 0.00249236
Iteration 4/25 | Loss: 0.00238281
Iteration 5/25 | Loss: 0.00214413
Iteration 6/25 | Loss: 0.00200738
Iteration 7/25 | Loss: 0.00190014
Iteration 8/25 | Loss: 0.00170370
Iteration 9/25 | Loss: 0.00162810
Iteration 10/25 | Loss: 0.00160365
Iteration 11/25 | Loss: 0.00159487
Iteration 12/25 | Loss: 0.00159116
Iteration 13/25 | Loss: 0.00158020
Iteration 14/25 | Loss: 0.00157888
Iteration 15/25 | Loss: 0.00157857
Iteration 16/25 | Loss: 0.00157846
Iteration 17/25 | Loss: 0.00157846
Iteration 18/25 | Loss: 0.00157846
Iteration 19/25 | Loss: 0.00157846
Iteration 20/25 | Loss: 0.00157846
Iteration 21/25 | Loss: 0.00157846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001578464638441801, 0.001578464638441801, 0.001578464638441801, 0.001578464638441801, 0.001578464638441801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001578464638441801

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38514483
Iteration 2/25 | Loss: 0.00264371
Iteration 3/25 | Loss: 0.00264371
Iteration 4/25 | Loss: 0.00264371
Iteration 5/25 | Loss: 0.00264371
Iteration 6/25 | Loss: 0.00264371
Iteration 7/25 | Loss: 0.00264371
Iteration 8/25 | Loss: 0.00264371
Iteration 9/25 | Loss: 0.00264371
Iteration 10/25 | Loss: 0.00264371
Iteration 11/25 | Loss: 0.00264371
Iteration 12/25 | Loss: 0.00264371
Iteration 13/25 | Loss: 0.00264371
Iteration 14/25 | Loss: 0.00264371
Iteration 15/25 | Loss: 0.00264371
Iteration 16/25 | Loss: 0.00264371
Iteration 17/25 | Loss: 0.00264371
Iteration 18/25 | Loss: 0.00264371
Iteration 19/25 | Loss: 0.00264371
Iteration 20/25 | Loss: 0.00264371
Iteration 21/25 | Loss: 0.00264371
Iteration 22/25 | Loss: 0.00264371
Iteration 23/25 | Loss: 0.00264371
Iteration 24/25 | Loss: 0.00264371
Iteration 25/25 | Loss: 0.00264371

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00264371
Iteration 2/1000 | Loss: 0.00189284
Iteration 3/1000 | Loss: 0.00022422
Iteration 4/1000 | Loss: 0.00015951
Iteration 5/1000 | Loss: 0.00010719
Iteration 6/1000 | Loss: 0.00008418
Iteration 7/1000 | Loss: 0.00006395
Iteration 8/1000 | Loss: 0.00005194
Iteration 9/1000 | Loss: 0.00004260
Iteration 10/1000 | Loss: 0.00003726
Iteration 11/1000 | Loss: 0.00022608
Iteration 12/1000 | Loss: 0.00024860
Iteration 13/1000 | Loss: 0.00004906
Iteration 14/1000 | Loss: 0.00003370
Iteration 15/1000 | Loss: 0.00003035
Iteration 16/1000 | Loss: 0.00002888
Iteration 17/1000 | Loss: 0.00002756
Iteration 18/1000 | Loss: 0.00002680
Iteration 19/1000 | Loss: 0.00002618
Iteration 20/1000 | Loss: 0.00018281
Iteration 21/1000 | Loss: 0.00002949
Iteration 22/1000 | Loss: 0.00002651
Iteration 23/1000 | Loss: 0.00002552
Iteration 24/1000 | Loss: 0.00002458
Iteration 25/1000 | Loss: 0.00002402
Iteration 26/1000 | Loss: 0.00002368
Iteration 27/1000 | Loss: 0.00002347
Iteration 28/1000 | Loss: 0.00002328
Iteration 29/1000 | Loss: 0.00002326
Iteration 30/1000 | Loss: 0.00002325
Iteration 31/1000 | Loss: 0.00002318
Iteration 32/1000 | Loss: 0.00002318
Iteration 33/1000 | Loss: 0.00002315
Iteration 34/1000 | Loss: 0.00002311
Iteration 35/1000 | Loss: 0.00002301
Iteration 36/1000 | Loss: 0.00002293
Iteration 37/1000 | Loss: 0.00002293
Iteration 38/1000 | Loss: 0.00002292
Iteration 39/1000 | Loss: 0.00002292
Iteration 40/1000 | Loss: 0.00002292
Iteration 41/1000 | Loss: 0.00002291
Iteration 42/1000 | Loss: 0.00002291
Iteration 43/1000 | Loss: 0.00002290
Iteration 44/1000 | Loss: 0.00002290
Iteration 45/1000 | Loss: 0.00002289
Iteration 46/1000 | Loss: 0.00002289
Iteration 47/1000 | Loss: 0.00002289
Iteration 48/1000 | Loss: 0.00002289
Iteration 49/1000 | Loss: 0.00002289
Iteration 50/1000 | Loss: 0.00002289
Iteration 51/1000 | Loss: 0.00002289
Iteration 52/1000 | Loss: 0.00002288
Iteration 53/1000 | Loss: 0.00002288
Iteration 54/1000 | Loss: 0.00002288
Iteration 55/1000 | Loss: 0.00002288
Iteration 56/1000 | Loss: 0.00002288
Iteration 57/1000 | Loss: 0.00002287
Iteration 58/1000 | Loss: 0.00002287
Iteration 59/1000 | Loss: 0.00002287
Iteration 60/1000 | Loss: 0.00002287
Iteration 61/1000 | Loss: 0.00002286
Iteration 62/1000 | Loss: 0.00002286
Iteration 63/1000 | Loss: 0.00002286
Iteration 64/1000 | Loss: 0.00002286
Iteration 65/1000 | Loss: 0.00002286
Iteration 66/1000 | Loss: 0.00002286
Iteration 67/1000 | Loss: 0.00002286
Iteration 68/1000 | Loss: 0.00002286
Iteration 69/1000 | Loss: 0.00002286
Iteration 70/1000 | Loss: 0.00002286
Iteration 71/1000 | Loss: 0.00002286
Iteration 72/1000 | Loss: 0.00002286
Iteration 73/1000 | Loss: 0.00002286
Iteration 74/1000 | Loss: 0.00002286
Iteration 75/1000 | Loss: 0.00002286
Iteration 76/1000 | Loss: 0.00002286
Iteration 77/1000 | Loss: 0.00002286
Iteration 78/1000 | Loss: 0.00002286
Iteration 79/1000 | Loss: 0.00002285
Iteration 80/1000 | Loss: 0.00002285
Iteration 81/1000 | Loss: 0.00002285
Iteration 82/1000 | Loss: 0.00002285
Iteration 83/1000 | Loss: 0.00002285
Iteration 84/1000 | Loss: 0.00002285
Iteration 85/1000 | Loss: 0.00002285
Iteration 86/1000 | Loss: 0.00002285
Iteration 87/1000 | Loss: 0.00002285
Iteration 88/1000 | Loss: 0.00002285
Iteration 89/1000 | Loss: 0.00002285
Iteration 90/1000 | Loss: 0.00002285
Iteration 91/1000 | Loss: 0.00002285
Iteration 92/1000 | Loss: 0.00002285
Iteration 93/1000 | Loss: 0.00002285
Iteration 94/1000 | Loss: 0.00002285
Iteration 95/1000 | Loss: 0.00002285
Iteration 96/1000 | Loss: 0.00002284
Iteration 97/1000 | Loss: 0.00002284
Iteration 98/1000 | Loss: 0.00002284
Iteration 99/1000 | Loss: 0.00002284
Iteration 100/1000 | Loss: 0.00002284
Iteration 101/1000 | Loss: 0.00002284
Iteration 102/1000 | Loss: 0.00002284
Iteration 103/1000 | Loss: 0.00002284
Iteration 104/1000 | Loss: 0.00002284
Iteration 105/1000 | Loss: 0.00002284
Iteration 106/1000 | Loss: 0.00002284
Iteration 107/1000 | Loss: 0.00002284
Iteration 108/1000 | Loss: 0.00002284
Iteration 109/1000 | Loss: 0.00002284
Iteration 110/1000 | Loss: 0.00002284
Iteration 111/1000 | Loss: 0.00002284
Iteration 112/1000 | Loss: 0.00002284
Iteration 113/1000 | Loss: 0.00002284
Iteration 114/1000 | Loss: 0.00002284
Iteration 115/1000 | Loss: 0.00002284
Iteration 116/1000 | Loss: 0.00002284
Iteration 117/1000 | Loss: 0.00002284
Iteration 118/1000 | Loss: 0.00002284
Iteration 119/1000 | Loss: 0.00002283
Iteration 120/1000 | Loss: 0.00002283
Iteration 121/1000 | Loss: 0.00002283
Iteration 122/1000 | Loss: 0.00002283
Iteration 123/1000 | Loss: 0.00002283
Iteration 124/1000 | Loss: 0.00002283
Iteration 125/1000 | Loss: 0.00002283
Iteration 126/1000 | Loss: 0.00002283
Iteration 127/1000 | Loss: 0.00002283
Iteration 128/1000 | Loss: 0.00002283
Iteration 129/1000 | Loss: 0.00002283
Iteration 130/1000 | Loss: 0.00002283
Iteration 131/1000 | Loss: 0.00002283
Iteration 132/1000 | Loss: 0.00002283
Iteration 133/1000 | Loss: 0.00002283
Iteration 134/1000 | Loss: 0.00002283
Iteration 135/1000 | Loss: 0.00002283
Iteration 136/1000 | Loss: 0.00002283
Iteration 137/1000 | Loss: 0.00002283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.283466892549768e-05, 2.283466892549768e-05, 2.283466892549768e-05, 2.283466892549768e-05, 2.283466892549768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.283466892549768e-05

Optimization complete. Final v2v error: 4.095165729522705 mm

Highest mean error: 5.315390586853027 mm for frame 5

Lowest mean error: 3.553842306137085 mm for frame 155

Saving results

Total time: 76.77140784263611
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993311
Iteration 2/25 | Loss: 0.00352656
Iteration 3/25 | Loss: 0.00228167
Iteration 4/25 | Loss: 0.00190260
Iteration 5/25 | Loss: 0.00180327
Iteration 6/25 | Loss: 0.00163953
Iteration 7/25 | Loss: 0.00156696
Iteration 8/25 | Loss: 0.00151080
Iteration 9/25 | Loss: 0.00149901
Iteration 10/25 | Loss: 0.00149262
Iteration 11/25 | Loss: 0.00148523
Iteration 12/25 | Loss: 0.00147597
Iteration 13/25 | Loss: 0.00146741
Iteration 14/25 | Loss: 0.00146329
Iteration 15/25 | Loss: 0.00146615
Iteration 16/25 | Loss: 0.00146391
Iteration 17/25 | Loss: 0.00147263
Iteration 18/25 | Loss: 0.00146033
Iteration 19/25 | Loss: 0.00145952
Iteration 20/25 | Loss: 0.00146146
Iteration 21/25 | Loss: 0.00146257
Iteration 22/25 | Loss: 0.00145847
Iteration 23/25 | Loss: 0.00145566
Iteration 24/25 | Loss: 0.00145460
Iteration 25/25 | Loss: 0.00145353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38023949
Iteration 2/25 | Loss: 0.00214792
Iteration 3/25 | Loss: 0.00176684
Iteration 4/25 | Loss: 0.00175421
Iteration 5/25 | Loss: 0.00174104
Iteration 6/25 | Loss: 0.00174104
Iteration 7/25 | Loss: 0.00174104
Iteration 8/25 | Loss: 0.00174104
Iteration 9/25 | Loss: 0.00174104
Iteration 10/25 | Loss: 0.00174104
Iteration 11/25 | Loss: 0.00174104
Iteration 12/25 | Loss: 0.00174104
Iteration 13/25 | Loss: 0.00174104
Iteration 14/25 | Loss: 0.00174104
Iteration 15/25 | Loss: 0.00174104
Iteration 16/25 | Loss: 0.00174104
Iteration 17/25 | Loss: 0.00174104
Iteration 18/25 | Loss: 0.00174104
Iteration 19/25 | Loss: 0.00174104
Iteration 20/25 | Loss: 0.00174104
Iteration 21/25 | Loss: 0.00174104
Iteration 22/25 | Loss: 0.00174104
Iteration 23/25 | Loss: 0.00174104
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0017410406144335866, 0.0017410406144335866, 0.0017410406144335866, 0.0017410406144335866, 0.0017410406144335866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017410406144335866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174104
Iteration 2/1000 | Loss: 0.00096922
Iteration 3/1000 | Loss: 0.00046682
Iteration 4/1000 | Loss: 0.00031338
Iteration 5/1000 | Loss: 0.00019609
Iteration 6/1000 | Loss: 0.00023363
Iteration 7/1000 | Loss: 0.00015238
Iteration 8/1000 | Loss: 0.00012664
Iteration 9/1000 | Loss: 0.00012273
Iteration 10/1000 | Loss: 0.00023201
Iteration 11/1000 | Loss: 0.00031666
Iteration 12/1000 | Loss: 0.00037623
Iteration 13/1000 | Loss: 0.00047195
Iteration 14/1000 | Loss: 0.00059910
Iteration 15/1000 | Loss: 0.00020971
Iteration 16/1000 | Loss: 0.00016082
Iteration 17/1000 | Loss: 0.00011172
Iteration 18/1000 | Loss: 0.00015627
Iteration 19/1000 | Loss: 0.00011442
Iteration 20/1000 | Loss: 0.00011564
Iteration 21/1000 | Loss: 0.00018520
Iteration 22/1000 | Loss: 0.00009992
Iteration 23/1000 | Loss: 0.00024162
Iteration 24/1000 | Loss: 0.00094873
Iteration 25/1000 | Loss: 0.00126362
Iteration 26/1000 | Loss: 0.00119776
Iteration 27/1000 | Loss: 0.00096403
Iteration 28/1000 | Loss: 0.00081420
Iteration 29/1000 | Loss: 0.00065524
Iteration 30/1000 | Loss: 0.00214990
Iteration 31/1000 | Loss: 0.00584655
Iteration 32/1000 | Loss: 0.00660017
Iteration 33/1000 | Loss: 0.00058565
Iteration 34/1000 | Loss: 0.00026073
Iteration 35/1000 | Loss: 0.00024472
Iteration 36/1000 | Loss: 0.00272002
Iteration 37/1000 | Loss: 0.00169253
Iteration 38/1000 | Loss: 0.00029411
Iteration 39/1000 | Loss: 0.00051100
Iteration 40/1000 | Loss: 0.00014557
Iteration 41/1000 | Loss: 0.00009488
Iteration 42/1000 | Loss: 0.00008369
Iteration 43/1000 | Loss: 0.00006159
Iteration 44/1000 | Loss: 0.00015229
Iteration 45/1000 | Loss: 0.00005864
Iteration 46/1000 | Loss: 0.00005598
Iteration 47/1000 | Loss: 0.00005729
Iteration 48/1000 | Loss: 0.00004117
Iteration 49/1000 | Loss: 0.00005703
Iteration 50/1000 | Loss: 0.00003926
Iteration 51/1000 | Loss: 0.00013303
Iteration 52/1000 | Loss: 0.00004606
Iteration 53/1000 | Loss: 0.00003204
Iteration 54/1000 | Loss: 0.00003388
Iteration 55/1000 | Loss: 0.00003338
Iteration 56/1000 | Loss: 0.00003219
Iteration 57/1000 | Loss: 0.00004006
Iteration 58/1000 | Loss: 0.00004272
Iteration 59/1000 | Loss: 0.00002652
Iteration 60/1000 | Loss: 0.00003693
Iteration 61/1000 | Loss: 0.00003635
Iteration 62/1000 | Loss: 0.00002669
Iteration 63/1000 | Loss: 0.00003233
Iteration 64/1000 | Loss: 0.00005385
Iteration 65/1000 | Loss: 0.00003799
Iteration 66/1000 | Loss: 0.00003197
Iteration 67/1000 | Loss: 0.00003340
Iteration 68/1000 | Loss: 0.00003032
Iteration 69/1000 | Loss: 0.00003926
Iteration 70/1000 | Loss: 0.00005345
Iteration 71/1000 | Loss: 0.00004576
Iteration 72/1000 | Loss: 0.00003020
Iteration 73/1000 | Loss: 0.00003018
Iteration 74/1000 | Loss: 0.00003087
Iteration 75/1000 | Loss: 0.00003268
Iteration 76/1000 | Loss: 0.00003810
Iteration 77/1000 | Loss: 0.00005381
Iteration 78/1000 | Loss: 0.00004244
Iteration 79/1000 | Loss: 0.00007993
Iteration 80/1000 | Loss: 0.00006681
Iteration 81/1000 | Loss: 0.00004635
Iteration 82/1000 | Loss: 0.00003859
Iteration 83/1000 | Loss: 0.00003871
Iteration 84/1000 | Loss: 0.00002973
Iteration 85/1000 | Loss: 0.00008916
Iteration 86/1000 | Loss: 0.00002418
Iteration 87/1000 | Loss: 0.00002310
Iteration 88/1000 | Loss: 0.00006142
Iteration 89/1000 | Loss: 0.00001956
Iteration 90/1000 | Loss: 0.00002919
Iteration 91/1000 | Loss: 0.00002565
Iteration 92/1000 | Loss: 0.00002135
Iteration 93/1000 | Loss: 0.00001769
Iteration 94/1000 | Loss: 0.00001764
Iteration 95/1000 | Loss: 0.00001761
Iteration 96/1000 | Loss: 0.00001904
Iteration 97/1000 | Loss: 0.00001760
Iteration 98/1000 | Loss: 0.00001760
Iteration 99/1000 | Loss: 0.00001760
Iteration 100/1000 | Loss: 0.00001759
Iteration 101/1000 | Loss: 0.00001771
Iteration 102/1000 | Loss: 0.00001945
Iteration 103/1000 | Loss: 0.00001751
Iteration 104/1000 | Loss: 0.00001750
Iteration 105/1000 | Loss: 0.00001750
Iteration 106/1000 | Loss: 0.00001749
Iteration 107/1000 | Loss: 0.00001749
Iteration 108/1000 | Loss: 0.00001748
Iteration 109/1000 | Loss: 0.00001747
Iteration 110/1000 | Loss: 0.00001746
Iteration 111/1000 | Loss: 0.00001746
Iteration 112/1000 | Loss: 0.00002013
Iteration 113/1000 | Loss: 0.00001757
Iteration 114/1000 | Loss: 0.00001740
Iteration 115/1000 | Loss: 0.00001740
Iteration 116/1000 | Loss: 0.00001739
Iteration 117/1000 | Loss: 0.00001898
Iteration 118/1000 | Loss: 0.00002125
Iteration 119/1000 | Loss: 0.00001741
Iteration 120/1000 | Loss: 0.00001740
Iteration 121/1000 | Loss: 0.00001738
Iteration 122/1000 | Loss: 0.00002389
Iteration 123/1000 | Loss: 0.00001759
Iteration 124/1000 | Loss: 0.00001778
Iteration 125/1000 | Loss: 0.00001978
Iteration 126/1000 | Loss: 0.00001942
Iteration 127/1000 | Loss: 0.00001733
Iteration 128/1000 | Loss: 0.00001719
Iteration 129/1000 | Loss: 0.00001719
Iteration 130/1000 | Loss: 0.00001719
Iteration 131/1000 | Loss: 0.00001709
Iteration 132/1000 | Loss: 0.00001709
Iteration 133/1000 | Loss: 0.00001709
Iteration 134/1000 | Loss: 0.00001709
Iteration 135/1000 | Loss: 0.00001709
Iteration 136/1000 | Loss: 0.00001709
Iteration 137/1000 | Loss: 0.00001709
Iteration 138/1000 | Loss: 0.00001709
Iteration 139/1000 | Loss: 0.00001709
Iteration 140/1000 | Loss: 0.00001709
Iteration 141/1000 | Loss: 0.00001709
Iteration 142/1000 | Loss: 0.00001709
Iteration 143/1000 | Loss: 0.00001709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.708632589725312e-05, 1.708632589725312e-05, 1.708632589725312e-05, 1.708632589725312e-05, 1.708632589725312e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.708632589725312e-05

Optimization complete. Final v2v error: 3.4833426475524902 mm

Highest mean error: 4.94069242477417 mm for frame 1

Lowest mean error: 3.1063694953918457 mm for frame 169

Saving results

Total time: 221.7065510749817
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055683
Iteration 2/25 | Loss: 0.00229220
Iteration 3/25 | Loss: 0.00201403
Iteration 4/25 | Loss: 0.00175612
Iteration 5/25 | Loss: 0.00159388
Iteration 6/25 | Loss: 0.00155661
Iteration 7/25 | Loss: 0.00155351
Iteration 8/25 | Loss: 0.00155311
Iteration 9/25 | Loss: 0.00155311
Iteration 10/25 | Loss: 0.00155311
Iteration 11/25 | Loss: 0.00155311
Iteration 12/25 | Loss: 0.00155311
Iteration 13/25 | Loss: 0.00155311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001553107751533389, 0.001553107751533389, 0.001553107751533389, 0.001553107751533389, 0.001553107751533389]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001553107751533389

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22923017
Iteration 2/25 | Loss: 0.00119297
Iteration 3/25 | Loss: 0.00119296
Iteration 4/25 | Loss: 0.00119296
Iteration 5/25 | Loss: 0.00119296
Iteration 6/25 | Loss: 0.00119296
Iteration 7/25 | Loss: 0.00119296
Iteration 8/25 | Loss: 0.00119296
Iteration 9/25 | Loss: 0.00119296
Iteration 10/25 | Loss: 0.00119296
Iteration 11/25 | Loss: 0.00119296
Iteration 12/25 | Loss: 0.00119296
Iteration 13/25 | Loss: 0.00119296
Iteration 14/25 | Loss: 0.00119296
Iteration 15/25 | Loss: 0.00119296
Iteration 16/25 | Loss: 0.00119296
Iteration 17/25 | Loss: 0.00119296
Iteration 18/25 | Loss: 0.00119296
Iteration 19/25 | Loss: 0.00119296
Iteration 20/25 | Loss: 0.00119296
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011929592583328485, 0.0011929592583328485, 0.0011929592583328485, 0.0011929592583328485, 0.0011929592583328485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011929592583328485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119296
Iteration 2/1000 | Loss: 0.00005298
Iteration 3/1000 | Loss: 0.00003895
Iteration 4/1000 | Loss: 0.00003573
Iteration 5/1000 | Loss: 0.00003459
Iteration 6/1000 | Loss: 0.00003335
Iteration 7/1000 | Loss: 0.00003281
Iteration 8/1000 | Loss: 0.00003253
Iteration 9/1000 | Loss: 0.00003228
Iteration 10/1000 | Loss: 0.00003206
Iteration 11/1000 | Loss: 0.00003198
Iteration 12/1000 | Loss: 0.00003193
Iteration 13/1000 | Loss: 0.00003188
Iteration 14/1000 | Loss: 0.00003188
Iteration 15/1000 | Loss: 0.00003188
Iteration 16/1000 | Loss: 0.00003187
Iteration 17/1000 | Loss: 0.00003187
Iteration 18/1000 | Loss: 0.00003185
Iteration 19/1000 | Loss: 0.00003185
Iteration 20/1000 | Loss: 0.00003185
Iteration 21/1000 | Loss: 0.00003184
Iteration 22/1000 | Loss: 0.00003184
Iteration 23/1000 | Loss: 0.00003184
Iteration 24/1000 | Loss: 0.00003184
Iteration 25/1000 | Loss: 0.00003184
Iteration 26/1000 | Loss: 0.00003184
Iteration 27/1000 | Loss: 0.00003184
Iteration 28/1000 | Loss: 0.00003184
Iteration 29/1000 | Loss: 0.00003184
Iteration 30/1000 | Loss: 0.00003184
Iteration 31/1000 | Loss: 0.00003184
Iteration 32/1000 | Loss: 0.00003184
Iteration 33/1000 | Loss: 0.00003184
Iteration 34/1000 | Loss: 0.00003184
Iteration 35/1000 | Loss: 0.00003184
Iteration 36/1000 | Loss: 0.00003184
Iteration 37/1000 | Loss: 0.00003184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 37. Stopping optimization.
Last 5 losses: [3.183501758030616e-05, 3.183501758030616e-05, 3.183501758030616e-05, 3.183501758030616e-05, 3.183501758030616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.183501758030616e-05

Optimization complete. Final v2v error: 4.635537624359131 mm

Highest mean error: 4.951135635375977 mm for frame 125

Lowest mean error: 4.419072151184082 mm for frame 166

Saving results

Total time: 32.72043204307556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792598
Iteration 2/25 | Loss: 0.00141556
Iteration 3/25 | Loss: 0.00133269
Iteration 4/25 | Loss: 0.00132453
Iteration 5/25 | Loss: 0.00132203
Iteration 6/25 | Loss: 0.00132203
Iteration 7/25 | Loss: 0.00132203
Iteration 8/25 | Loss: 0.00132203
Iteration 9/25 | Loss: 0.00132203
Iteration 10/25 | Loss: 0.00132203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013220281107351184, 0.0013220281107351184, 0.0013220281107351184, 0.0013220281107351184, 0.0013220281107351184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013220281107351184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37263858
Iteration 2/25 | Loss: 0.00074780
Iteration 3/25 | Loss: 0.00074779
Iteration 4/25 | Loss: 0.00074779
Iteration 5/25 | Loss: 0.00074779
Iteration 6/25 | Loss: 0.00074779
Iteration 7/25 | Loss: 0.00074779
Iteration 8/25 | Loss: 0.00074779
Iteration 9/25 | Loss: 0.00074779
Iteration 10/25 | Loss: 0.00074779
Iteration 11/25 | Loss: 0.00074779
Iteration 12/25 | Loss: 0.00074779
Iteration 13/25 | Loss: 0.00074779
Iteration 14/25 | Loss: 0.00074779
Iteration 15/25 | Loss: 0.00074779
Iteration 16/25 | Loss: 0.00074779
Iteration 17/25 | Loss: 0.00074779
Iteration 18/25 | Loss: 0.00074779
Iteration 19/25 | Loss: 0.00074779
Iteration 20/25 | Loss: 0.00074779
Iteration 21/25 | Loss: 0.00074779
Iteration 22/25 | Loss: 0.00074779
Iteration 23/25 | Loss: 0.00074779
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007477860781364143, 0.0007477860781364143, 0.0007477860781364143, 0.0007477860781364143, 0.0007477860781364143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007477860781364143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074779
Iteration 2/1000 | Loss: 0.00003019
Iteration 3/1000 | Loss: 0.00002291
Iteration 4/1000 | Loss: 0.00002105
Iteration 5/1000 | Loss: 0.00002007
Iteration 6/1000 | Loss: 0.00001951
Iteration 7/1000 | Loss: 0.00001935
Iteration 8/1000 | Loss: 0.00001903
Iteration 9/1000 | Loss: 0.00001861
Iteration 10/1000 | Loss: 0.00001834
Iteration 11/1000 | Loss: 0.00001820
Iteration 12/1000 | Loss: 0.00001818
Iteration 13/1000 | Loss: 0.00001814
Iteration 14/1000 | Loss: 0.00001800
Iteration 15/1000 | Loss: 0.00001798
Iteration 16/1000 | Loss: 0.00001797
Iteration 17/1000 | Loss: 0.00001786
Iteration 18/1000 | Loss: 0.00001786
Iteration 19/1000 | Loss: 0.00001785
Iteration 20/1000 | Loss: 0.00001779
Iteration 21/1000 | Loss: 0.00001777
Iteration 22/1000 | Loss: 0.00001776
Iteration 23/1000 | Loss: 0.00001768
Iteration 24/1000 | Loss: 0.00001765
Iteration 25/1000 | Loss: 0.00001765
Iteration 26/1000 | Loss: 0.00001765
Iteration 27/1000 | Loss: 0.00001765
Iteration 28/1000 | Loss: 0.00001765
Iteration 29/1000 | Loss: 0.00001765
Iteration 30/1000 | Loss: 0.00001765
Iteration 31/1000 | Loss: 0.00001764
Iteration 32/1000 | Loss: 0.00001764
Iteration 33/1000 | Loss: 0.00001764
Iteration 34/1000 | Loss: 0.00001764
Iteration 35/1000 | Loss: 0.00001764
Iteration 36/1000 | Loss: 0.00001764
Iteration 37/1000 | Loss: 0.00001764
Iteration 38/1000 | Loss: 0.00001764
Iteration 39/1000 | Loss: 0.00001763
Iteration 40/1000 | Loss: 0.00001763
Iteration 41/1000 | Loss: 0.00001760
Iteration 42/1000 | Loss: 0.00001760
Iteration 43/1000 | Loss: 0.00001760
Iteration 44/1000 | Loss: 0.00001760
Iteration 45/1000 | Loss: 0.00001760
Iteration 46/1000 | Loss: 0.00001760
Iteration 47/1000 | Loss: 0.00001760
Iteration 48/1000 | Loss: 0.00001760
Iteration 49/1000 | Loss: 0.00001760
Iteration 50/1000 | Loss: 0.00001760
Iteration 51/1000 | Loss: 0.00001759
Iteration 52/1000 | Loss: 0.00001759
Iteration 53/1000 | Loss: 0.00001759
Iteration 54/1000 | Loss: 0.00001759
Iteration 55/1000 | Loss: 0.00001758
Iteration 56/1000 | Loss: 0.00001757
Iteration 57/1000 | Loss: 0.00001757
Iteration 58/1000 | Loss: 0.00001757
Iteration 59/1000 | Loss: 0.00001757
Iteration 60/1000 | Loss: 0.00001757
Iteration 61/1000 | Loss: 0.00001757
Iteration 62/1000 | Loss: 0.00001757
Iteration 63/1000 | Loss: 0.00001757
Iteration 64/1000 | Loss: 0.00001756
Iteration 65/1000 | Loss: 0.00001756
Iteration 66/1000 | Loss: 0.00001756
Iteration 67/1000 | Loss: 0.00001756
Iteration 68/1000 | Loss: 0.00001755
Iteration 69/1000 | Loss: 0.00001752
Iteration 70/1000 | Loss: 0.00001752
Iteration 71/1000 | Loss: 0.00001752
Iteration 72/1000 | Loss: 0.00001752
Iteration 73/1000 | Loss: 0.00001751
Iteration 74/1000 | Loss: 0.00001750
Iteration 75/1000 | Loss: 0.00001749
Iteration 76/1000 | Loss: 0.00001748
Iteration 77/1000 | Loss: 0.00001748
Iteration 78/1000 | Loss: 0.00001747
Iteration 79/1000 | Loss: 0.00001747
Iteration 80/1000 | Loss: 0.00001746
Iteration 81/1000 | Loss: 0.00001746
Iteration 82/1000 | Loss: 0.00001746
Iteration 83/1000 | Loss: 0.00001746
Iteration 84/1000 | Loss: 0.00001746
Iteration 85/1000 | Loss: 0.00001746
Iteration 86/1000 | Loss: 0.00001745
Iteration 87/1000 | Loss: 0.00001745
Iteration 88/1000 | Loss: 0.00001745
Iteration 89/1000 | Loss: 0.00001744
Iteration 90/1000 | Loss: 0.00001744
Iteration 91/1000 | Loss: 0.00001744
Iteration 92/1000 | Loss: 0.00001743
Iteration 93/1000 | Loss: 0.00001743
Iteration 94/1000 | Loss: 0.00001743
Iteration 95/1000 | Loss: 0.00001743
Iteration 96/1000 | Loss: 0.00001743
Iteration 97/1000 | Loss: 0.00001743
Iteration 98/1000 | Loss: 0.00001743
Iteration 99/1000 | Loss: 0.00001743
Iteration 100/1000 | Loss: 0.00001743
Iteration 101/1000 | Loss: 0.00001743
Iteration 102/1000 | Loss: 0.00001742
Iteration 103/1000 | Loss: 0.00001742
Iteration 104/1000 | Loss: 0.00001742
Iteration 105/1000 | Loss: 0.00001742
Iteration 106/1000 | Loss: 0.00001741
Iteration 107/1000 | Loss: 0.00001741
Iteration 108/1000 | Loss: 0.00001741
Iteration 109/1000 | Loss: 0.00001740
Iteration 110/1000 | Loss: 0.00001740
Iteration 111/1000 | Loss: 0.00001740
Iteration 112/1000 | Loss: 0.00001739
Iteration 113/1000 | Loss: 0.00001739
Iteration 114/1000 | Loss: 0.00001739
Iteration 115/1000 | Loss: 0.00001739
Iteration 116/1000 | Loss: 0.00001739
Iteration 117/1000 | Loss: 0.00001739
Iteration 118/1000 | Loss: 0.00001739
Iteration 119/1000 | Loss: 0.00001738
Iteration 120/1000 | Loss: 0.00001738
Iteration 121/1000 | Loss: 0.00001738
Iteration 122/1000 | Loss: 0.00001738
Iteration 123/1000 | Loss: 0.00001738
Iteration 124/1000 | Loss: 0.00001737
Iteration 125/1000 | Loss: 0.00001737
Iteration 126/1000 | Loss: 0.00001737
Iteration 127/1000 | Loss: 0.00001737
Iteration 128/1000 | Loss: 0.00001737
Iteration 129/1000 | Loss: 0.00001737
Iteration 130/1000 | Loss: 0.00001737
Iteration 131/1000 | Loss: 0.00001737
Iteration 132/1000 | Loss: 0.00001737
Iteration 133/1000 | Loss: 0.00001737
Iteration 134/1000 | Loss: 0.00001737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.73710777744418e-05, 1.73710777744418e-05, 1.73710777744418e-05, 1.73710777744418e-05, 1.73710777744418e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.73710777744418e-05

Optimization complete. Final v2v error: 3.473482370376587 mm

Highest mean error: 3.6291306018829346 mm for frame 139

Lowest mean error: 3.385749578475952 mm for frame 100

Saving results

Total time: 41.34036183357239
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402636
Iteration 2/25 | Loss: 0.00135283
Iteration 3/25 | Loss: 0.00127301
Iteration 4/25 | Loss: 0.00126203
Iteration 5/25 | Loss: 0.00125915
Iteration 6/25 | Loss: 0.00125885
Iteration 7/25 | Loss: 0.00125885
Iteration 8/25 | Loss: 0.00125885
Iteration 9/25 | Loss: 0.00125885
Iteration 10/25 | Loss: 0.00125885
Iteration 11/25 | Loss: 0.00125885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012588459067046642, 0.0012588459067046642, 0.0012588459067046642, 0.0012588459067046642, 0.0012588459067046642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012588459067046642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39044738
Iteration 2/25 | Loss: 0.00092574
Iteration 3/25 | Loss: 0.00092573
Iteration 4/25 | Loss: 0.00092573
Iteration 5/25 | Loss: 0.00092573
Iteration 6/25 | Loss: 0.00092573
Iteration 7/25 | Loss: 0.00092573
Iteration 8/25 | Loss: 0.00092573
Iteration 9/25 | Loss: 0.00092572
Iteration 10/25 | Loss: 0.00092572
Iteration 11/25 | Loss: 0.00092572
Iteration 12/25 | Loss: 0.00092572
Iteration 13/25 | Loss: 0.00092572
Iteration 14/25 | Loss: 0.00092572
Iteration 15/25 | Loss: 0.00092572
Iteration 16/25 | Loss: 0.00092572
Iteration 17/25 | Loss: 0.00092572
Iteration 18/25 | Loss: 0.00092572
Iteration 19/25 | Loss: 0.00092572
Iteration 20/25 | Loss: 0.00092572
Iteration 21/25 | Loss: 0.00092572
Iteration 22/25 | Loss: 0.00092572
Iteration 23/25 | Loss: 0.00092572
Iteration 24/25 | Loss: 0.00092572
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009257241617888212, 0.0009257241617888212, 0.0009257241617888212, 0.0009257241617888212, 0.0009257241617888212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009257241617888212

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092572
Iteration 2/1000 | Loss: 0.00003030
Iteration 3/1000 | Loss: 0.00001899
Iteration 4/1000 | Loss: 0.00001473
Iteration 5/1000 | Loss: 0.00001341
Iteration 6/1000 | Loss: 0.00001252
Iteration 7/1000 | Loss: 0.00001196
Iteration 8/1000 | Loss: 0.00001168
Iteration 9/1000 | Loss: 0.00001162
Iteration 10/1000 | Loss: 0.00001155
Iteration 11/1000 | Loss: 0.00001153
Iteration 12/1000 | Loss: 0.00001135
Iteration 13/1000 | Loss: 0.00001131
Iteration 14/1000 | Loss: 0.00001130
Iteration 15/1000 | Loss: 0.00001128
Iteration 16/1000 | Loss: 0.00001126
Iteration 17/1000 | Loss: 0.00001125
Iteration 18/1000 | Loss: 0.00001124
Iteration 19/1000 | Loss: 0.00001123
Iteration 20/1000 | Loss: 0.00001121
Iteration 21/1000 | Loss: 0.00001121
Iteration 22/1000 | Loss: 0.00001120
Iteration 23/1000 | Loss: 0.00001119
Iteration 24/1000 | Loss: 0.00001119
Iteration 25/1000 | Loss: 0.00001118
Iteration 26/1000 | Loss: 0.00001118
Iteration 27/1000 | Loss: 0.00001117
Iteration 28/1000 | Loss: 0.00001106
Iteration 29/1000 | Loss: 0.00001105
Iteration 30/1000 | Loss: 0.00001104
Iteration 31/1000 | Loss: 0.00001100
Iteration 32/1000 | Loss: 0.00001100
Iteration 33/1000 | Loss: 0.00001099
Iteration 34/1000 | Loss: 0.00001099
Iteration 35/1000 | Loss: 0.00001097
Iteration 36/1000 | Loss: 0.00001097
Iteration 37/1000 | Loss: 0.00001096
Iteration 38/1000 | Loss: 0.00001096
Iteration 39/1000 | Loss: 0.00001094
Iteration 40/1000 | Loss: 0.00001094
Iteration 41/1000 | Loss: 0.00001094
Iteration 42/1000 | Loss: 0.00001094
Iteration 43/1000 | Loss: 0.00001094
Iteration 44/1000 | Loss: 0.00001094
Iteration 45/1000 | Loss: 0.00001094
Iteration 46/1000 | Loss: 0.00001094
Iteration 47/1000 | Loss: 0.00001094
Iteration 48/1000 | Loss: 0.00001093
Iteration 49/1000 | Loss: 0.00001093
Iteration 50/1000 | Loss: 0.00001092
Iteration 51/1000 | Loss: 0.00001092
Iteration 52/1000 | Loss: 0.00001091
Iteration 53/1000 | Loss: 0.00001090
Iteration 54/1000 | Loss: 0.00001090
Iteration 55/1000 | Loss: 0.00001090
Iteration 56/1000 | Loss: 0.00001089
Iteration 57/1000 | Loss: 0.00001089
Iteration 58/1000 | Loss: 0.00001089
Iteration 59/1000 | Loss: 0.00001089
Iteration 60/1000 | Loss: 0.00001089
Iteration 61/1000 | Loss: 0.00001088
Iteration 62/1000 | Loss: 0.00001088
Iteration 63/1000 | Loss: 0.00001087
Iteration 64/1000 | Loss: 0.00001085
Iteration 65/1000 | Loss: 0.00001085
Iteration 66/1000 | Loss: 0.00001085
Iteration 67/1000 | Loss: 0.00001085
Iteration 68/1000 | Loss: 0.00001085
Iteration 69/1000 | Loss: 0.00001085
Iteration 70/1000 | Loss: 0.00001085
Iteration 71/1000 | Loss: 0.00001084
Iteration 72/1000 | Loss: 0.00001084
Iteration 73/1000 | Loss: 0.00001082
Iteration 74/1000 | Loss: 0.00001082
Iteration 75/1000 | Loss: 0.00001081
Iteration 76/1000 | Loss: 0.00001081
Iteration 77/1000 | Loss: 0.00001080
Iteration 78/1000 | Loss: 0.00001080
Iteration 79/1000 | Loss: 0.00001079
Iteration 80/1000 | Loss: 0.00001078
Iteration 81/1000 | Loss: 0.00001078
Iteration 82/1000 | Loss: 0.00001078
Iteration 83/1000 | Loss: 0.00001078
Iteration 84/1000 | Loss: 0.00001078
Iteration 85/1000 | Loss: 0.00001078
Iteration 86/1000 | Loss: 0.00001077
Iteration 87/1000 | Loss: 0.00001077
Iteration 88/1000 | Loss: 0.00001077
Iteration 89/1000 | Loss: 0.00001077
Iteration 90/1000 | Loss: 0.00001076
Iteration 91/1000 | Loss: 0.00001076
Iteration 92/1000 | Loss: 0.00001076
Iteration 93/1000 | Loss: 0.00001075
Iteration 94/1000 | Loss: 0.00001075
Iteration 95/1000 | Loss: 0.00001075
Iteration 96/1000 | Loss: 0.00001074
Iteration 97/1000 | Loss: 0.00001073
Iteration 98/1000 | Loss: 0.00001073
Iteration 99/1000 | Loss: 0.00001072
Iteration 100/1000 | Loss: 0.00001072
Iteration 101/1000 | Loss: 0.00001071
Iteration 102/1000 | Loss: 0.00001071
Iteration 103/1000 | Loss: 0.00001067
Iteration 104/1000 | Loss: 0.00001067
Iteration 105/1000 | Loss: 0.00001067
Iteration 106/1000 | Loss: 0.00001066
Iteration 107/1000 | Loss: 0.00001066
Iteration 108/1000 | Loss: 0.00001065
Iteration 109/1000 | Loss: 0.00001063
Iteration 110/1000 | Loss: 0.00001062
Iteration 111/1000 | Loss: 0.00001062
Iteration 112/1000 | Loss: 0.00001062
Iteration 113/1000 | Loss: 0.00001062
Iteration 114/1000 | Loss: 0.00001061
Iteration 115/1000 | Loss: 0.00001060
Iteration 116/1000 | Loss: 0.00001060
Iteration 117/1000 | Loss: 0.00001059
Iteration 118/1000 | Loss: 0.00001059
Iteration 119/1000 | Loss: 0.00001059
Iteration 120/1000 | Loss: 0.00001059
Iteration 121/1000 | Loss: 0.00001059
Iteration 122/1000 | Loss: 0.00001058
Iteration 123/1000 | Loss: 0.00001058
Iteration 124/1000 | Loss: 0.00001058
Iteration 125/1000 | Loss: 0.00001058
Iteration 126/1000 | Loss: 0.00001058
Iteration 127/1000 | Loss: 0.00001058
Iteration 128/1000 | Loss: 0.00001057
Iteration 129/1000 | Loss: 0.00001057
Iteration 130/1000 | Loss: 0.00001057
Iteration 131/1000 | Loss: 0.00001057
Iteration 132/1000 | Loss: 0.00001057
Iteration 133/1000 | Loss: 0.00001057
Iteration 134/1000 | Loss: 0.00001057
Iteration 135/1000 | Loss: 0.00001057
Iteration 136/1000 | Loss: 0.00001057
Iteration 137/1000 | Loss: 0.00001056
Iteration 138/1000 | Loss: 0.00001056
Iteration 139/1000 | Loss: 0.00001056
Iteration 140/1000 | Loss: 0.00001055
Iteration 141/1000 | Loss: 0.00001055
Iteration 142/1000 | Loss: 0.00001055
Iteration 143/1000 | Loss: 0.00001055
Iteration 144/1000 | Loss: 0.00001055
Iteration 145/1000 | Loss: 0.00001055
Iteration 146/1000 | Loss: 0.00001055
Iteration 147/1000 | Loss: 0.00001055
Iteration 148/1000 | Loss: 0.00001054
Iteration 149/1000 | Loss: 0.00001054
Iteration 150/1000 | Loss: 0.00001054
Iteration 151/1000 | Loss: 0.00001054
Iteration 152/1000 | Loss: 0.00001053
Iteration 153/1000 | Loss: 0.00001053
Iteration 154/1000 | Loss: 0.00001053
Iteration 155/1000 | Loss: 0.00001053
Iteration 156/1000 | Loss: 0.00001053
Iteration 157/1000 | Loss: 0.00001053
Iteration 158/1000 | Loss: 0.00001053
Iteration 159/1000 | Loss: 0.00001052
Iteration 160/1000 | Loss: 0.00001052
Iteration 161/1000 | Loss: 0.00001052
Iteration 162/1000 | Loss: 0.00001052
Iteration 163/1000 | Loss: 0.00001052
Iteration 164/1000 | Loss: 0.00001052
Iteration 165/1000 | Loss: 0.00001052
Iteration 166/1000 | Loss: 0.00001052
Iteration 167/1000 | Loss: 0.00001052
Iteration 168/1000 | Loss: 0.00001052
Iteration 169/1000 | Loss: 0.00001052
Iteration 170/1000 | Loss: 0.00001052
Iteration 171/1000 | Loss: 0.00001052
Iteration 172/1000 | Loss: 0.00001052
Iteration 173/1000 | Loss: 0.00001052
Iteration 174/1000 | Loss: 0.00001052
Iteration 175/1000 | Loss: 0.00001052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.0517873306525871e-05, 1.0517873306525871e-05, 1.0517873306525871e-05, 1.0517873306525871e-05, 1.0517873306525871e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0517873306525871e-05

Optimization complete. Final v2v error: 2.7929067611694336 mm

Highest mean error: 2.8986449241638184 mm for frame 83

Lowest mean error: 2.733062267303467 mm for frame 52

Saving results

Total time: 37.88929343223572
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789836
Iteration 2/25 | Loss: 0.00145848
Iteration 3/25 | Loss: 0.00129382
Iteration 4/25 | Loss: 0.00127102
Iteration 5/25 | Loss: 0.00126608
Iteration 6/25 | Loss: 0.00126595
Iteration 7/25 | Loss: 0.00126595
Iteration 8/25 | Loss: 0.00126595
Iteration 9/25 | Loss: 0.00126595
Iteration 10/25 | Loss: 0.00126595
Iteration 11/25 | Loss: 0.00126595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012659470085054636, 0.0012659470085054636, 0.0012659470085054636, 0.0012659470085054636, 0.0012659470085054636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012659470085054636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38960254
Iteration 2/25 | Loss: 0.00086398
Iteration 3/25 | Loss: 0.00086398
Iteration 4/25 | Loss: 0.00086398
Iteration 5/25 | Loss: 0.00086398
Iteration 6/25 | Loss: 0.00086398
Iteration 7/25 | Loss: 0.00086398
Iteration 8/25 | Loss: 0.00086398
Iteration 9/25 | Loss: 0.00086398
Iteration 10/25 | Loss: 0.00086398
Iteration 11/25 | Loss: 0.00086398
Iteration 12/25 | Loss: 0.00086398
Iteration 13/25 | Loss: 0.00086398
Iteration 14/25 | Loss: 0.00086398
Iteration 15/25 | Loss: 0.00086398
Iteration 16/25 | Loss: 0.00086398
Iteration 17/25 | Loss: 0.00086398
Iteration 18/25 | Loss: 0.00086398
Iteration 19/25 | Loss: 0.00086398
Iteration 20/25 | Loss: 0.00086398
Iteration 21/25 | Loss: 0.00086398
Iteration 22/25 | Loss: 0.00086398
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008639806765131652, 0.0008639806765131652, 0.0008639806765131652, 0.0008639806765131652, 0.0008639806765131652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008639806765131652

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086398
Iteration 2/1000 | Loss: 0.00002835
Iteration 3/1000 | Loss: 0.00002001
Iteration 4/1000 | Loss: 0.00001758
Iteration 5/1000 | Loss: 0.00001601
Iteration 6/1000 | Loss: 0.00001498
Iteration 7/1000 | Loss: 0.00001434
Iteration 8/1000 | Loss: 0.00001387
Iteration 9/1000 | Loss: 0.00001361
Iteration 10/1000 | Loss: 0.00001318
Iteration 11/1000 | Loss: 0.00001296
Iteration 12/1000 | Loss: 0.00001288
Iteration 13/1000 | Loss: 0.00001287
Iteration 14/1000 | Loss: 0.00001286
Iteration 15/1000 | Loss: 0.00001286
Iteration 16/1000 | Loss: 0.00001284
Iteration 17/1000 | Loss: 0.00001284
Iteration 18/1000 | Loss: 0.00001272
Iteration 19/1000 | Loss: 0.00001267
Iteration 20/1000 | Loss: 0.00001266
Iteration 21/1000 | Loss: 0.00001262
Iteration 22/1000 | Loss: 0.00001259
Iteration 23/1000 | Loss: 0.00001258
Iteration 24/1000 | Loss: 0.00001257
Iteration 25/1000 | Loss: 0.00001253
Iteration 26/1000 | Loss: 0.00001252
Iteration 27/1000 | Loss: 0.00001252
Iteration 28/1000 | Loss: 0.00001252
Iteration 29/1000 | Loss: 0.00001251
Iteration 30/1000 | Loss: 0.00001250
Iteration 31/1000 | Loss: 0.00001250
Iteration 32/1000 | Loss: 0.00001249
Iteration 33/1000 | Loss: 0.00001249
Iteration 34/1000 | Loss: 0.00001248
Iteration 35/1000 | Loss: 0.00001248
Iteration 36/1000 | Loss: 0.00001248
Iteration 37/1000 | Loss: 0.00001248
Iteration 38/1000 | Loss: 0.00001247
Iteration 39/1000 | Loss: 0.00001247
Iteration 40/1000 | Loss: 0.00001246
Iteration 41/1000 | Loss: 0.00001246
Iteration 42/1000 | Loss: 0.00001246
Iteration 43/1000 | Loss: 0.00001245
Iteration 44/1000 | Loss: 0.00001245
Iteration 45/1000 | Loss: 0.00001244
Iteration 46/1000 | Loss: 0.00001244
Iteration 47/1000 | Loss: 0.00001244
Iteration 48/1000 | Loss: 0.00001243
Iteration 49/1000 | Loss: 0.00001243
Iteration 50/1000 | Loss: 0.00001242
Iteration 51/1000 | Loss: 0.00001241
Iteration 52/1000 | Loss: 0.00001240
Iteration 53/1000 | Loss: 0.00001240
Iteration 54/1000 | Loss: 0.00001239
Iteration 55/1000 | Loss: 0.00001239
Iteration 56/1000 | Loss: 0.00001239
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001237
Iteration 60/1000 | Loss: 0.00001236
Iteration 61/1000 | Loss: 0.00001236
Iteration 62/1000 | Loss: 0.00001235
Iteration 63/1000 | Loss: 0.00001235
Iteration 64/1000 | Loss: 0.00001234
Iteration 65/1000 | Loss: 0.00001234
Iteration 66/1000 | Loss: 0.00001234
Iteration 67/1000 | Loss: 0.00001233
Iteration 68/1000 | Loss: 0.00001233
Iteration 69/1000 | Loss: 0.00001233
Iteration 70/1000 | Loss: 0.00001232
Iteration 71/1000 | Loss: 0.00001232
Iteration 72/1000 | Loss: 0.00001232
Iteration 73/1000 | Loss: 0.00001231
Iteration 74/1000 | Loss: 0.00001231
Iteration 75/1000 | Loss: 0.00001231
Iteration 76/1000 | Loss: 0.00001231
Iteration 77/1000 | Loss: 0.00001230
Iteration 78/1000 | Loss: 0.00001230
Iteration 79/1000 | Loss: 0.00001230
Iteration 80/1000 | Loss: 0.00001230
Iteration 81/1000 | Loss: 0.00001229
Iteration 82/1000 | Loss: 0.00001229
Iteration 83/1000 | Loss: 0.00001229
Iteration 84/1000 | Loss: 0.00001229
Iteration 85/1000 | Loss: 0.00001229
Iteration 86/1000 | Loss: 0.00001228
Iteration 87/1000 | Loss: 0.00001228
Iteration 88/1000 | Loss: 0.00001228
Iteration 89/1000 | Loss: 0.00001228
Iteration 90/1000 | Loss: 0.00001228
Iteration 91/1000 | Loss: 0.00001228
Iteration 92/1000 | Loss: 0.00001228
Iteration 93/1000 | Loss: 0.00001228
Iteration 94/1000 | Loss: 0.00001228
Iteration 95/1000 | Loss: 0.00001228
Iteration 96/1000 | Loss: 0.00001227
Iteration 97/1000 | Loss: 0.00001227
Iteration 98/1000 | Loss: 0.00001227
Iteration 99/1000 | Loss: 0.00001227
Iteration 100/1000 | Loss: 0.00001226
Iteration 101/1000 | Loss: 0.00001226
Iteration 102/1000 | Loss: 0.00001226
Iteration 103/1000 | Loss: 0.00001226
Iteration 104/1000 | Loss: 0.00001226
Iteration 105/1000 | Loss: 0.00001226
Iteration 106/1000 | Loss: 0.00001225
Iteration 107/1000 | Loss: 0.00001225
Iteration 108/1000 | Loss: 0.00001225
Iteration 109/1000 | Loss: 0.00001225
Iteration 110/1000 | Loss: 0.00001225
Iteration 111/1000 | Loss: 0.00001224
Iteration 112/1000 | Loss: 0.00001224
Iteration 113/1000 | Loss: 0.00001224
Iteration 114/1000 | Loss: 0.00001224
Iteration 115/1000 | Loss: 0.00001223
Iteration 116/1000 | Loss: 0.00001223
Iteration 117/1000 | Loss: 0.00001223
Iteration 118/1000 | Loss: 0.00001223
Iteration 119/1000 | Loss: 0.00001223
Iteration 120/1000 | Loss: 0.00001223
Iteration 121/1000 | Loss: 0.00001223
Iteration 122/1000 | Loss: 0.00001223
Iteration 123/1000 | Loss: 0.00001223
Iteration 124/1000 | Loss: 0.00001223
Iteration 125/1000 | Loss: 0.00001223
Iteration 126/1000 | Loss: 0.00001222
Iteration 127/1000 | Loss: 0.00001222
Iteration 128/1000 | Loss: 0.00001222
Iteration 129/1000 | Loss: 0.00001222
Iteration 130/1000 | Loss: 0.00001222
Iteration 131/1000 | Loss: 0.00001221
Iteration 132/1000 | Loss: 0.00001221
Iteration 133/1000 | Loss: 0.00001221
Iteration 134/1000 | Loss: 0.00001220
Iteration 135/1000 | Loss: 0.00001220
Iteration 136/1000 | Loss: 0.00001220
Iteration 137/1000 | Loss: 0.00001220
Iteration 138/1000 | Loss: 0.00001220
Iteration 139/1000 | Loss: 0.00001219
Iteration 140/1000 | Loss: 0.00001219
Iteration 141/1000 | Loss: 0.00001218
Iteration 142/1000 | Loss: 0.00001218
Iteration 143/1000 | Loss: 0.00001218
Iteration 144/1000 | Loss: 0.00001218
Iteration 145/1000 | Loss: 0.00001218
Iteration 146/1000 | Loss: 0.00001218
Iteration 147/1000 | Loss: 0.00001218
Iteration 148/1000 | Loss: 0.00001218
Iteration 149/1000 | Loss: 0.00001218
Iteration 150/1000 | Loss: 0.00001218
Iteration 151/1000 | Loss: 0.00001218
Iteration 152/1000 | Loss: 0.00001218
Iteration 153/1000 | Loss: 0.00001218
Iteration 154/1000 | Loss: 0.00001218
Iteration 155/1000 | Loss: 0.00001218
Iteration 156/1000 | Loss: 0.00001218
Iteration 157/1000 | Loss: 0.00001218
Iteration 158/1000 | Loss: 0.00001218
Iteration 159/1000 | Loss: 0.00001218
Iteration 160/1000 | Loss: 0.00001218
Iteration 161/1000 | Loss: 0.00001218
Iteration 162/1000 | Loss: 0.00001218
Iteration 163/1000 | Loss: 0.00001218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.2175650226708967e-05, 1.2175650226708967e-05, 1.2175650226708967e-05, 1.2175650226708967e-05, 1.2175650226708967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2175650226708967e-05

Optimization complete. Final v2v error: 3.0038678646087646 mm

Highest mean error: 3.172882556915283 mm for frame 109

Lowest mean error: 2.872115135192871 mm for frame 128

Saving results

Total time: 43.16375207901001
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967017
Iteration 2/25 | Loss: 0.00249750
Iteration 3/25 | Loss: 0.00216153
Iteration 4/25 | Loss: 0.00184433
Iteration 5/25 | Loss: 0.00208906
Iteration 6/25 | Loss: 0.00190570
Iteration 7/25 | Loss: 0.00148430
Iteration 8/25 | Loss: 0.00143153
Iteration 9/25 | Loss: 0.00143065
Iteration 10/25 | Loss: 0.00142219
Iteration 11/25 | Loss: 0.00142093
Iteration 12/25 | Loss: 0.00142067
Iteration 13/25 | Loss: 0.00142052
Iteration 14/25 | Loss: 0.00142038
Iteration 15/25 | Loss: 0.00142030
Iteration 16/25 | Loss: 0.00142025
Iteration 17/25 | Loss: 0.00142024
Iteration 18/25 | Loss: 0.00142022
Iteration 19/25 | Loss: 0.00142018
Iteration 20/25 | Loss: 0.00142018
Iteration 21/25 | Loss: 0.00142018
Iteration 22/25 | Loss: 0.00142016
Iteration 23/25 | Loss: 0.00142015
Iteration 24/25 | Loss: 0.00142015
Iteration 25/25 | Loss: 0.00142015

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38819492
Iteration 2/25 | Loss: 0.00090442
Iteration 3/25 | Loss: 0.00090442
Iteration 4/25 | Loss: 0.00090442
Iteration 5/25 | Loss: 0.00090442
Iteration 6/25 | Loss: 0.00090442
Iteration 7/25 | Loss: 0.00090442
Iteration 8/25 | Loss: 0.00090442
Iteration 9/25 | Loss: 0.00090442
Iteration 10/25 | Loss: 0.00090442
Iteration 11/25 | Loss: 0.00090442
Iteration 12/25 | Loss: 0.00090442
Iteration 13/25 | Loss: 0.00090442
Iteration 14/25 | Loss: 0.00090442
Iteration 15/25 | Loss: 0.00090442
Iteration 16/25 | Loss: 0.00090442
Iteration 17/25 | Loss: 0.00090442
Iteration 18/25 | Loss: 0.00090442
Iteration 19/25 | Loss: 0.00090442
Iteration 20/25 | Loss: 0.00090442
Iteration 21/25 | Loss: 0.00090442
Iteration 22/25 | Loss: 0.00090442
Iteration 23/25 | Loss: 0.00090442
Iteration 24/25 | Loss: 0.00090442
Iteration 25/25 | Loss: 0.00090442
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009044151520356536, 0.0009044151520356536, 0.0009044151520356536, 0.0009044151520356536, 0.0009044151520356536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009044151520356536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090442
Iteration 2/1000 | Loss: 0.00011621
Iteration 3/1000 | Loss: 0.00004723
Iteration 4/1000 | Loss: 0.00003854
Iteration 5/1000 | Loss: 0.00003507
Iteration 6/1000 | Loss: 0.00003276
Iteration 7/1000 | Loss: 0.00003167
Iteration 8/1000 | Loss: 0.00003093
Iteration 9/1000 | Loss: 0.00003052
Iteration 10/1000 | Loss: 0.00003001
Iteration 11/1000 | Loss: 0.00002966
Iteration 12/1000 | Loss: 0.00002941
Iteration 13/1000 | Loss: 0.00015123
Iteration 14/1000 | Loss: 0.00003193
Iteration 15/1000 | Loss: 0.00002994
Iteration 16/1000 | Loss: 0.00002871
Iteration 17/1000 | Loss: 0.00002776
Iteration 18/1000 | Loss: 0.00002719
Iteration 19/1000 | Loss: 0.00002708
Iteration 20/1000 | Loss: 0.00002693
Iteration 21/1000 | Loss: 0.00002676
Iteration 22/1000 | Loss: 0.00002671
Iteration 23/1000 | Loss: 0.00002666
Iteration 24/1000 | Loss: 0.00002658
Iteration 25/1000 | Loss: 0.00002656
Iteration 26/1000 | Loss: 0.00002656
Iteration 27/1000 | Loss: 0.00002655
Iteration 28/1000 | Loss: 0.00002655
Iteration 29/1000 | Loss: 0.00002654
Iteration 30/1000 | Loss: 0.00002654
Iteration 31/1000 | Loss: 0.00002654
Iteration 32/1000 | Loss: 0.00002653
Iteration 33/1000 | Loss: 0.00002652
Iteration 34/1000 | Loss: 0.00002647
Iteration 35/1000 | Loss: 0.00002645
Iteration 36/1000 | Loss: 0.00002642
Iteration 37/1000 | Loss: 0.00002642
Iteration 38/1000 | Loss: 0.00002641
Iteration 39/1000 | Loss: 0.00002641
Iteration 40/1000 | Loss: 0.00002641
Iteration 41/1000 | Loss: 0.00002640
Iteration 42/1000 | Loss: 0.00002640
Iteration 43/1000 | Loss: 0.00002640
Iteration 44/1000 | Loss: 0.00002639
Iteration 45/1000 | Loss: 0.00002639
Iteration 46/1000 | Loss: 0.00002639
Iteration 47/1000 | Loss: 0.00002639
Iteration 48/1000 | Loss: 0.00002639
Iteration 49/1000 | Loss: 0.00002639
Iteration 50/1000 | Loss: 0.00002639
Iteration 51/1000 | Loss: 0.00002639
Iteration 52/1000 | Loss: 0.00002639
Iteration 53/1000 | Loss: 0.00002638
Iteration 54/1000 | Loss: 0.00002638
Iteration 55/1000 | Loss: 0.00002638
Iteration 56/1000 | Loss: 0.00002638
Iteration 57/1000 | Loss: 0.00002638
Iteration 58/1000 | Loss: 0.00002638
Iteration 59/1000 | Loss: 0.00002638
Iteration 60/1000 | Loss: 0.00002638
Iteration 61/1000 | Loss: 0.00002637
Iteration 62/1000 | Loss: 0.00002637
Iteration 63/1000 | Loss: 0.00002637
Iteration 64/1000 | Loss: 0.00002637
Iteration 65/1000 | Loss: 0.00002637
Iteration 66/1000 | Loss: 0.00002637
Iteration 67/1000 | Loss: 0.00002637
Iteration 68/1000 | Loss: 0.00002637
Iteration 69/1000 | Loss: 0.00002637
Iteration 70/1000 | Loss: 0.00002637
Iteration 71/1000 | Loss: 0.00002637
Iteration 72/1000 | Loss: 0.00002637
Iteration 73/1000 | Loss: 0.00002637
Iteration 74/1000 | Loss: 0.00002637
Iteration 75/1000 | Loss: 0.00002637
Iteration 76/1000 | Loss: 0.00002637
Iteration 77/1000 | Loss: 0.00002637
Iteration 78/1000 | Loss: 0.00002637
Iteration 79/1000 | Loss: 0.00002636
Iteration 80/1000 | Loss: 0.00002636
Iteration 81/1000 | Loss: 0.00002636
Iteration 82/1000 | Loss: 0.00002636
Iteration 83/1000 | Loss: 0.00002636
Iteration 84/1000 | Loss: 0.00002636
Iteration 85/1000 | Loss: 0.00002636
Iteration 86/1000 | Loss: 0.00002636
Iteration 87/1000 | Loss: 0.00002636
Iteration 88/1000 | Loss: 0.00002636
Iteration 89/1000 | Loss: 0.00002635
Iteration 90/1000 | Loss: 0.00002635
Iteration 91/1000 | Loss: 0.00002635
Iteration 92/1000 | Loss: 0.00002635
Iteration 93/1000 | Loss: 0.00002635
Iteration 94/1000 | Loss: 0.00002635
Iteration 95/1000 | Loss: 0.00002635
Iteration 96/1000 | Loss: 0.00002635
Iteration 97/1000 | Loss: 0.00002635
Iteration 98/1000 | Loss: 0.00002635
Iteration 99/1000 | Loss: 0.00002635
Iteration 100/1000 | Loss: 0.00002635
Iteration 101/1000 | Loss: 0.00002635
Iteration 102/1000 | Loss: 0.00002635
Iteration 103/1000 | Loss: 0.00002634
Iteration 104/1000 | Loss: 0.00002634
Iteration 105/1000 | Loss: 0.00002634
Iteration 106/1000 | Loss: 0.00002634
Iteration 107/1000 | Loss: 0.00002634
Iteration 108/1000 | Loss: 0.00002634
Iteration 109/1000 | Loss: 0.00002634
Iteration 110/1000 | Loss: 0.00002634
Iteration 111/1000 | Loss: 0.00002634
Iteration 112/1000 | Loss: 0.00002634
Iteration 113/1000 | Loss: 0.00002634
Iteration 114/1000 | Loss: 0.00002634
Iteration 115/1000 | Loss: 0.00002634
Iteration 116/1000 | Loss: 0.00002634
Iteration 117/1000 | Loss: 0.00002634
Iteration 118/1000 | Loss: 0.00002634
Iteration 119/1000 | Loss: 0.00002634
Iteration 120/1000 | Loss: 0.00002634
Iteration 121/1000 | Loss: 0.00002634
Iteration 122/1000 | Loss: 0.00002634
Iteration 123/1000 | Loss: 0.00002634
Iteration 124/1000 | Loss: 0.00002634
Iteration 125/1000 | Loss: 0.00002633
Iteration 126/1000 | Loss: 0.00002633
Iteration 127/1000 | Loss: 0.00002633
Iteration 128/1000 | Loss: 0.00002633
Iteration 129/1000 | Loss: 0.00002633
Iteration 130/1000 | Loss: 0.00002633
Iteration 131/1000 | Loss: 0.00002633
Iteration 132/1000 | Loss: 0.00002633
Iteration 133/1000 | Loss: 0.00002633
Iteration 134/1000 | Loss: 0.00002633
Iteration 135/1000 | Loss: 0.00002633
Iteration 136/1000 | Loss: 0.00002633
Iteration 137/1000 | Loss: 0.00002633
Iteration 138/1000 | Loss: 0.00002633
Iteration 139/1000 | Loss: 0.00002633
Iteration 140/1000 | Loss: 0.00002633
Iteration 141/1000 | Loss: 0.00002633
Iteration 142/1000 | Loss: 0.00002633
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.6327528757974505e-05, 2.6327528757974505e-05, 2.6327528757974505e-05, 2.6327528757974505e-05, 2.6327528757974505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6327528757974505e-05

Optimization complete. Final v2v error: 4.360544204711914 mm

Highest mean error: 4.605965614318848 mm for frame 0

Lowest mean error: 4.042262077331543 mm for frame 1

Saving results

Total time: 65.86620998382568
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825341
Iteration 2/25 | Loss: 0.00170199
Iteration 3/25 | Loss: 0.00139860
Iteration 4/25 | Loss: 0.00136294
Iteration 5/25 | Loss: 0.00135387
Iteration 6/25 | Loss: 0.00135220
Iteration 7/25 | Loss: 0.00135220
Iteration 8/25 | Loss: 0.00135220
Iteration 9/25 | Loss: 0.00135220
Iteration 10/25 | Loss: 0.00135220
Iteration 11/25 | Loss: 0.00135220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013522023800760508, 0.0013522023800760508, 0.0013522023800760508, 0.0013522023800760508, 0.0013522023800760508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013522023800760508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.70489264
Iteration 2/25 | Loss: 0.00083567
Iteration 3/25 | Loss: 0.00083566
Iteration 4/25 | Loss: 0.00083566
Iteration 5/25 | Loss: 0.00083566
Iteration 6/25 | Loss: 0.00083566
Iteration 7/25 | Loss: 0.00083566
Iteration 8/25 | Loss: 0.00083566
Iteration 9/25 | Loss: 0.00083566
Iteration 10/25 | Loss: 0.00083566
Iteration 11/25 | Loss: 0.00083566
Iteration 12/25 | Loss: 0.00083566
Iteration 13/25 | Loss: 0.00083566
Iteration 14/25 | Loss: 0.00083566
Iteration 15/25 | Loss: 0.00083566
Iteration 16/25 | Loss: 0.00083566
Iteration 17/25 | Loss: 0.00083566
Iteration 18/25 | Loss: 0.00083566
Iteration 19/25 | Loss: 0.00083566
Iteration 20/25 | Loss: 0.00083566
Iteration 21/25 | Loss: 0.00083566
Iteration 22/25 | Loss: 0.00083566
Iteration 23/25 | Loss: 0.00083566
Iteration 24/25 | Loss: 0.00083566
Iteration 25/25 | Loss: 0.00083566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083566
Iteration 2/1000 | Loss: 0.00004388
Iteration 3/1000 | Loss: 0.00003330
Iteration 4/1000 | Loss: 0.00002770
Iteration 5/1000 | Loss: 0.00002605
Iteration 6/1000 | Loss: 0.00002491
Iteration 7/1000 | Loss: 0.00002426
Iteration 8/1000 | Loss: 0.00002368
Iteration 9/1000 | Loss: 0.00002326
Iteration 10/1000 | Loss: 0.00002291
Iteration 11/1000 | Loss: 0.00002260
Iteration 12/1000 | Loss: 0.00002237
Iteration 13/1000 | Loss: 0.00002233
Iteration 14/1000 | Loss: 0.00002228
Iteration 15/1000 | Loss: 0.00002215
Iteration 16/1000 | Loss: 0.00002198
Iteration 17/1000 | Loss: 0.00002194
Iteration 18/1000 | Loss: 0.00002193
Iteration 19/1000 | Loss: 0.00002193
Iteration 20/1000 | Loss: 0.00002192
Iteration 21/1000 | Loss: 0.00002192
Iteration 22/1000 | Loss: 0.00002190
Iteration 23/1000 | Loss: 0.00002189
Iteration 24/1000 | Loss: 0.00002189
Iteration 25/1000 | Loss: 0.00002188
Iteration 26/1000 | Loss: 0.00002185
Iteration 27/1000 | Loss: 0.00002185
Iteration 28/1000 | Loss: 0.00002185
Iteration 29/1000 | Loss: 0.00002185
Iteration 30/1000 | Loss: 0.00002185
Iteration 31/1000 | Loss: 0.00002185
Iteration 32/1000 | Loss: 0.00002185
Iteration 33/1000 | Loss: 0.00002185
Iteration 34/1000 | Loss: 0.00002184
Iteration 35/1000 | Loss: 0.00002184
Iteration 36/1000 | Loss: 0.00002182
Iteration 37/1000 | Loss: 0.00002181
Iteration 38/1000 | Loss: 0.00002181
Iteration 39/1000 | Loss: 0.00002181
Iteration 40/1000 | Loss: 0.00002181
Iteration 41/1000 | Loss: 0.00002181
Iteration 42/1000 | Loss: 0.00002180
Iteration 43/1000 | Loss: 0.00002180
Iteration 44/1000 | Loss: 0.00002179
Iteration 45/1000 | Loss: 0.00002179
Iteration 46/1000 | Loss: 0.00002177
Iteration 47/1000 | Loss: 0.00002177
Iteration 48/1000 | Loss: 0.00002176
Iteration 49/1000 | Loss: 0.00002176
Iteration 50/1000 | Loss: 0.00002176
Iteration 51/1000 | Loss: 0.00002175
Iteration 52/1000 | Loss: 0.00002175
Iteration 53/1000 | Loss: 0.00002175
Iteration 54/1000 | Loss: 0.00002175
Iteration 55/1000 | Loss: 0.00002174
Iteration 56/1000 | Loss: 0.00002174
Iteration 57/1000 | Loss: 0.00002174
Iteration 58/1000 | Loss: 0.00002174
Iteration 59/1000 | Loss: 0.00002174
Iteration 60/1000 | Loss: 0.00002174
Iteration 61/1000 | Loss: 0.00002174
Iteration 62/1000 | Loss: 0.00002173
Iteration 63/1000 | Loss: 0.00002173
Iteration 64/1000 | Loss: 0.00002173
Iteration 65/1000 | Loss: 0.00002173
Iteration 66/1000 | Loss: 0.00002173
Iteration 67/1000 | Loss: 0.00002173
Iteration 68/1000 | Loss: 0.00002172
Iteration 69/1000 | Loss: 0.00002172
Iteration 70/1000 | Loss: 0.00002172
Iteration 71/1000 | Loss: 0.00002172
Iteration 72/1000 | Loss: 0.00002171
Iteration 73/1000 | Loss: 0.00002171
Iteration 74/1000 | Loss: 0.00002171
Iteration 75/1000 | Loss: 0.00002171
Iteration 76/1000 | Loss: 0.00002171
Iteration 77/1000 | Loss: 0.00002171
Iteration 78/1000 | Loss: 0.00002170
Iteration 79/1000 | Loss: 0.00002170
Iteration 80/1000 | Loss: 0.00002170
Iteration 81/1000 | Loss: 0.00002170
Iteration 82/1000 | Loss: 0.00002170
Iteration 83/1000 | Loss: 0.00002170
Iteration 84/1000 | Loss: 0.00002170
Iteration 85/1000 | Loss: 0.00002169
Iteration 86/1000 | Loss: 0.00002169
Iteration 87/1000 | Loss: 0.00002169
Iteration 88/1000 | Loss: 0.00002169
Iteration 89/1000 | Loss: 0.00002168
Iteration 90/1000 | Loss: 0.00002167
Iteration 91/1000 | Loss: 0.00002167
Iteration 92/1000 | Loss: 0.00002167
Iteration 93/1000 | Loss: 0.00002166
Iteration 94/1000 | Loss: 0.00002166
Iteration 95/1000 | Loss: 0.00002166
Iteration 96/1000 | Loss: 0.00002166
Iteration 97/1000 | Loss: 0.00002166
Iteration 98/1000 | Loss: 0.00002165
Iteration 99/1000 | Loss: 0.00002165
Iteration 100/1000 | Loss: 0.00002164
Iteration 101/1000 | Loss: 0.00002164
Iteration 102/1000 | Loss: 0.00002163
Iteration 103/1000 | Loss: 0.00002162
Iteration 104/1000 | Loss: 0.00002162
Iteration 105/1000 | Loss: 0.00002160
Iteration 106/1000 | Loss: 0.00002160
Iteration 107/1000 | Loss: 0.00002160
Iteration 108/1000 | Loss: 0.00002158
Iteration 109/1000 | Loss: 0.00002158
Iteration 110/1000 | Loss: 0.00002157
Iteration 111/1000 | Loss: 0.00002157
Iteration 112/1000 | Loss: 0.00002157
Iteration 113/1000 | Loss: 0.00002157
Iteration 114/1000 | Loss: 0.00002156
Iteration 115/1000 | Loss: 0.00002156
Iteration 116/1000 | Loss: 0.00002155
Iteration 117/1000 | Loss: 0.00002155
Iteration 118/1000 | Loss: 0.00002155
Iteration 119/1000 | Loss: 0.00002155
Iteration 120/1000 | Loss: 0.00002154
Iteration 121/1000 | Loss: 0.00002154
Iteration 122/1000 | Loss: 0.00002153
Iteration 123/1000 | Loss: 0.00002153
Iteration 124/1000 | Loss: 0.00002153
Iteration 125/1000 | Loss: 0.00002153
Iteration 126/1000 | Loss: 0.00002153
Iteration 127/1000 | Loss: 0.00002152
Iteration 128/1000 | Loss: 0.00002152
Iteration 129/1000 | Loss: 0.00002152
Iteration 130/1000 | Loss: 0.00002152
Iteration 131/1000 | Loss: 0.00002152
Iteration 132/1000 | Loss: 0.00002152
Iteration 133/1000 | Loss: 0.00002152
Iteration 134/1000 | Loss: 0.00002152
Iteration 135/1000 | Loss: 0.00002151
Iteration 136/1000 | Loss: 0.00002151
Iteration 137/1000 | Loss: 0.00002151
Iteration 138/1000 | Loss: 0.00002151
Iteration 139/1000 | Loss: 0.00002151
Iteration 140/1000 | Loss: 0.00002151
Iteration 141/1000 | Loss: 0.00002151
Iteration 142/1000 | Loss: 0.00002151
Iteration 143/1000 | Loss: 0.00002151
Iteration 144/1000 | Loss: 0.00002151
Iteration 145/1000 | Loss: 0.00002151
Iteration 146/1000 | Loss: 0.00002150
Iteration 147/1000 | Loss: 0.00002150
Iteration 148/1000 | Loss: 0.00002150
Iteration 149/1000 | Loss: 0.00002150
Iteration 150/1000 | Loss: 0.00002150
Iteration 151/1000 | Loss: 0.00002150
Iteration 152/1000 | Loss: 0.00002150
Iteration 153/1000 | Loss: 0.00002150
Iteration 154/1000 | Loss: 0.00002150
Iteration 155/1000 | Loss: 0.00002150
Iteration 156/1000 | Loss: 0.00002150
Iteration 157/1000 | Loss: 0.00002150
Iteration 158/1000 | Loss: 0.00002150
Iteration 159/1000 | Loss: 0.00002150
Iteration 160/1000 | Loss: 0.00002150
Iteration 161/1000 | Loss: 0.00002150
Iteration 162/1000 | Loss: 0.00002150
Iteration 163/1000 | Loss: 0.00002150
Iteration 164/1000 | Loss: 0.00002150
Iteration 165/1000 | Loss: 0.00002149
Iteration 166/1000 | Loss: 0.00002149
Iteration 167/1000 | Loss: 0.00002149
Iteration 168/1000 | Loss: 0.00002149
Iteration 169/1000 | Loss: 0.00002149
Iteration 170/1000 | Loss: 0.00002149
Iteration 171/1000 | Loss: 0.00002149
Iteration 172/1000 | Loss: 0.00002149
Iteration 173/1000 | Loss: 0.00002149
Iteration 174/1000 | Loss: 0.00002149
Iteration 175/1000 | Loss: 0.00002149
Iteration 176/1000 | Loss: 0.00002149
Iteration 177/1000 | Loss: 0.00002149
Iteration 178/1000 | Loss: 0.00002149
Iteration 179/1000 | Loss: 0.00002149
Iteration 180/1000 | Loss: 0.00002149
Iteration 181/1000 | Loss: 0.00002149
Iteration 182/1000 | Loss: 0.00002149
Iteration 183/1000 | Loss: 0.00002149
Iteration 184/1000 | Loss: 0.00002149
Iteration 185/1000 | Loss: 0.00002149
Iteration 186/1000 | Loss: 0.00002149
Iteration 187/1000 | Loss: 0.00002148
Iteration 188/1000 | Loss: 0.00002148
Iteration 189/1000 | Loss: 0.00002148
Iteration 190/1000 | Loss: 0.00002148
Iteration 191/1000 | Loss: 0.00002148
Iteration 192/1000 | Loss: 0.00002148
Iteration 193/1000 | Loss: 0.00002148
Iteration 194/1000 | Loss: 0.00002148
Iteration 195/1000 | Loss: 0.00002148
Iteration 196/1000 | Loss: 0.00002148
Iteration 197/1000 | Loss: 0.00002148
Iteration 198/1000 | Loss: 0.00002148
Iteration 199/1000 | Loss: 0.00002148
Iteration 200/1000 | Loss: 0.00002148
Iteration 201/1000 | Loss: 0.00002148
Iteration 202/1000 | Loss: 0.00002148
Iteration 203/1000 | Loss: 0.00002148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [2.1478981580003165e-05, 2.1478981580003165e-05, 2.1478981580003165e-05, 2.1478981580003165e-05, 2.1478981580003165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1478981580003165e-05

Optimization complete. Final v2v error: 3.8572287559509277 mm

Highest mean error: 4.227060317993164 mm for frame 93

Lowest mean error: 3.4414238929748535 mm for frame 137

Saving results

Total time: 41.76299047470093
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785966
Iteration 2/25 | Loss: 0.00135574
Iteration 3/25 | Loss: 0.00128572
Iteration 4/25 | Loss: 0.00127406
Iteration 5/25 | Loss: 0.00127155
Iteration 6/25 | Loss: 0.00127155
Iteration 7/25 | Loss: 0.00127155
Iteration 8/25 | Loss: 0.00127155
Iteration 9/25 | Loss: 0.00127155
Iteration 10/25 | Loss: 0.00127155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001271552057005465, 0.001271552057005465, 0.001271552057005465, 0.001271552057005465, 0.001271552057005465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001271552057005465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43111634
Iteration 2/25 | Loss: 0.00081374
Iteration 3/25 | Loss: 0.00081374
Iteration 4/25 | Loss: 0.00081374
Iteration 5/25 | Loss: 0.00081374
Iteration 6/25 | Loss: 0.00081374
Iteration 7/25 | Loss: 0.00081374
Iteration 8/25 | Loss: 0.00081374
Iteration 9/25 | Loss: 0.00081374
Iteration 10/25 | Loss: 0.00081374
Iteration 11/25 | Loss: 0.00081374
Iteration 12/25 | Loss: 0.00081374
Iteration 13/25 | Loss: 0.00081374
Iteration 14/25 | Loss: 0.00081374
Iteration 15/25 | Loss: 0.00081374
Iteration 16/25 | Loss: 0.00081374
Iteration 17/25 | Loss: 0.00081374
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008137359982356429, 0.0008137359982356429, 0.0008137359982356429, 0.0008137359982356429, 0.0008137359982356429]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008137359982356429

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081374
Iteration 2/1000 | Loss: 0.00002710
Iteration 3/1000 | Loss: 0.00001972
Iteration 4/1000 | Loss: 0.00001775
Iteration 5/1000 | Loss: 0.00001714
Iteration 6/1000 | Loss: 0.00001653
Iteration 7/1000 | Loss: 0.00001597
Iteration 8/1000 | Loss: 0.00001558
Iteration 9/1000 | Loss: 0.00001542
Iteration 10/1000 | Loss: 0.00001511
Iteration 11/1000 | Loss: 0.00001483
Iteration 12/1000 | Loss: 0.00001481
Iteration 13/1000 | Loss: 0.00001480
Iteration 14/1000 | Loss: 0.00001479
Iteration 15/1000 | Loss: 0.00001476
Iteration 16/1000 | Loss: 0.00001458
Iteration 17/1000 | Loss: 0.00001455
Iteration 18/1000 | Loss: 0.00001450
Iteration 19/1000 | Loss: 0.00001436
Iteration 20/1000 | Loss: 0.00001429
Iteration 21/1000 | Loss: 0.00001426
Iteration 22/1000 | Loss: 0.00001426
Iteration 23/1000 | Loss: 0.00001425
Iteration 24/1000 | Loss: 0.00001424
Iteration 25/1000 | Loss: 0.00001424
Iteration 26/1000 | Loss: 0.00001421
Iteration 27/1000 | Loss: 0.00001420
Iteration 28/1000 | Loss: 0.00001419
Iteration 29/1000 | Loss: 0.00001418
Iteration 30/1000 | Loss: 0.00001417
Iteration 31/1000 | Loss: 0.00001416
Iteration 32/1000 | Loss: 0.00001415
Iteration 33/1000 | Loss: 0.00001413
Iteration 34/1000 | Loss: 0.00001413
Iteration 35/1000 | Loss: 0.00001411
Iteration 36/1000 | Loss: 0.00001408
Iteration 37/1000 | Loss: 0.00001405
Iteration 38/1000 | Loss: 0.00001404
Iteration 39/1000 | Loss: 0.00001403
Iteration 40/1000 | Loss: 0.00001403
Iteration 41/1000 | Loss: 0.00001400
Iteration 42/1000 | Loss: 0.00001397
Iteration 43/1000 | Loss: 0.00001397
Iteration 44/1000 | Loss: 0.00001396
Iteration 45/1000 | Loss: 0.00001396
Iteration 46/1000 | Loss: 0.00001396
Iteration 47/1000 | Loss: 0.00001396
Iteration 48/1000 | Loss: 0.00001395
Iteration 49/1000 | Loss: 0.00001395
Iteration 50/1000 | Loss: 0.00001394
Iteration 51/1000 | Loss: 0.00001394
Iteration 52/1000 | Loss: 0.00001393
Iteration 53/1000 | Loss: 0.00001393
Iteration 54/1000 | Loss: 0.00001393
Iteration 55/1000 | Loss: 0.00001392
Iteration 56/1000 | Loss: 0.00001392
Iteration 57/1000 | Loss: 0.00001392
Iteration 58/1000 | Loss: 0.00001391
Iteration 59/1000 | Loss: 0.00001391
Iteration 60/1000 | Loss: 0.00001390
Iteration 61/1000 | Loss: 0.00001390
Iteration 62/1000 | Loss: 0.00001390
Iteration 63/1000 | Loss: 0.00001388
Iteration 64/1000 | Loss: 0.00001387
Iteration 65/1000 | Loss: 0.00001386
Iteration 66/1000 | Loss: 0.00001386
Iteration 67/1000 | Loss: 0.00001386
Iteration 68/1000 | Loss: 0.00001385
Iteration 69/1000 | Loss: 0.00001385
Iteration 70/1000 | Loss: 0.00001385
Iteration 71/1000 | Loss: 0.00001385
Iteration 72/1000 | Loss: 0.00001385
Iteration 73/1000 | Loss: 0.00001385
Iteration 74/1000 | Loss: 0.00001385
Iteration 75/1000 | Loss: 0.00001385
Iteration 76/1000 | Loss: 0.00001385
Iteration 77/1000 | Loss: 0.00001385
Iteration 78/1000 | Loss: 0.00001384
Iteration 79/1000 | Loss: 0.00001383
Iteration 80/1000 | Loss: 0.00001383
Iteration 81/1000 | Loss: 0.00001383
Iteration 82/1000 | Loss: 0.00001382
Iteration 83/1000 | Loss: 0.00001382
Iteration 84/1000 | Loss: 0.00001381
Iteration 85/1000 | Loss: 0.00001381
Iteration 86/1000 | Loss: 0.00001381
Iteration 87/1000 | Loss: 0.00001381
Iteration 88/1000 | Loss: 0.00001381
Iteration 89/1000 | Loss: 0.00001381
Iteration 90/1000 | Loss: 0.00001381
Iteration 91/1000 | Loss: 0.00001380
Iteration 92/1000 | Loss: 0.00001380
Iteration 93/1000 | Loss: 0.00001380
Iteration 94/1000 | Loss: 0.00001380
Iteration 95/1000 | Loss: 0.00001380
Iteration 96/1000 | Loss: 0.00001380
Iteration 97/1000 | Loss: 0.00001380
Iteration 98/1000 | Loss: 0.00001380
Iteration 99/1000 | Loss: 0.00001380
Iteration 100/1000 | Loss: 0.00001380
Iteration 101/1000 | Loss: 0.00001379
Iteration 102/1000 | Loss: 0.00001379
Iteration 103/1000 | Loss: 0.00001379
Iteration 104/1000 | Loss: 0.00001379
Iteration 105/1000 | Loss: 0.00001379
Iteration 106/1000 | Loss: 0.00001379
Iteration 107/1000 | Loss: 0.00001379
Iteration 108/1000 | Loss: 0.00001379
Iteration 109/1000 | Loss: 0.00001378
Iteration 110/1000 | Loss: 0.00001378
Iteration 111/1000 | Loss: 0.00001378
Iteration 112/1000 | Loss: 0.00001378
Iteration 113/1000 | Loss: 0.00001377
Iteration 114/1000 | Loss: 0.00001377
Iteration 115/1000 | Loss: 0.00001377
Iteration 116/1000 | Loss: 0.00001377
Iteration 117/1000 | Loss: 0.00001377
Iteration 118/1000 | Loss: 0.00001377
Iteration 119/1000 | Loss: 0.00001377
Iteration 120/1000 | Loss: 0.00001377
Iteration 121/1000 | Loss: 0.00001377
Iteration 122/1000 | Loss: 0.00001377
Iteration 123/1000 | Loss: 0.00001377
Iteration 124/1000 | Loss: 0.00001376
Iteration 125/1000 | Loss: 0.00001376
Iteration 126/1000 | Loss: 0.00001376
Iteration 127/1000 | Loss: 0.00001376
Iteration 128/1000 | Loss: 0.00001376
Iteration 129/1000 | Loss: 0.00001376
Iteration 130/1000 | Loss: 0.00001376
Iteration 131/1000 | Loss: 0.00001376
Iteration 132/1000 | Loss: 0.00001376
Iteration 133/1000 | Loss: 0.00001376
Iteration 134/1000 | Loss: 0.00001376
Iteration 135/1000 | Loss: 0.00001376
Iteration 136/1000 | Loss: 0.00001376
Iteration 137/1000 | Loss: 0.00001375
Iteration 138/1000 | Loss: 0.00001375
Iteration 139/1000 | Loss: 0.00001375
Iteration 140/1000 | Loss: 0.00001375
Iteration 141/1000 | Loss: 0.00001375
Iteration 142/1000 | Loss: 0.00001375
Iteration 143/1000 | Loss: 0.00001375
Iteration 144/1000 | Loss: 0.00001375
Iteration 145/1000 | Loss: 0.00001375
Iteration 146/1000 | Loss: 0.00001374
Iteration 147/1000 | Loss: 0.00001374
Iteration 148/1000 | Loss: 0.00001374
Iteration 149/1000 | Loss: 0.00001374
Iteration 150/1000 | Loss: 0.00001374
Iteration 151/1000 | Loss: 0.00001374
Iteration 152/1000 | Loss: 0.00001374
Iteration 153/1000 | Loss: 0.00001374
Iteration 154/1000 | Loss: 0.00001374
Iteration 155/1000 | Loss: 0.00001374
Iteration 156/1000 | Loss: 0.00001374
Iteration 157/1000 | Loss: 0.00001374
Iteration 158/1000 | Loss: 0.00001374
Iteration 159/1000 | Loss: 0.00001374
Iteration 160/1000 | Loss: 0.00001374
Iteration 161/1000 | Loss: 0.00001374
Iteration 162/1000 | Loss: 0.00001374
Iteration 163/1000 | Loss: 0.00001373
Iteration 164/1000 | Loss: 0.00001373
Iteration 165/1000 | Loss: 0.00001373
Iteration 166/1000 | Loss: 0.00001373
Iteration 167/1000 | Loss: 0.00001373
Iteration 168/1000 | Loss: 0.00001373
Iteration 169/1000 | Loss: 0.00001373
Iteration 170/1000 | Loss: 0.00001373
Iteration 171/1000 | Loss: 0.00001373
Iteration 172/1000 | Loss: 0.00001373
Iteration 173/1000 | Loss: 0.00001373
Iteration 174/1000 | Loss: 0.00001373
Iteration 175/1000 | Loss: 0.00001373
Iteration 176/1000 | Loss: 0.00001372
Iteration 177/1000 | Loss: 0.00001372
Iteration 178/1000 | Loss: 0.00001372
Iteration 179/1000 | Loss: 0.00001372
Iteration 180/1000 | Loss: 0.00001372
Iteration 181/1000 | Loss: 0.00001372
Iteration 182/1000 | Loss: 0.00001372
Iteration 183/1000 | Loss: 0.00001372
Iteration 184/1000 | Loss: 0.00001372
Iteration 185/1000 | Loss: 0.00001372
Iteration 186/1000 | Loss: 0.00001372
Iteration 187/1000 | Loss: 0.00001372
Iteration 188/1000 | Loss: 0.00001372
Iteration 189/1000 | Loss: 0.00001372
Iteration 190/1000 | Loss: 0.00001372
Iteration 191/1000 | Loss: 0.00001372
Iteration 192/1000 | Loss: 0.00001372
Iteration 193/1000 | Loss: 0.00001372
Iteration 194/1000 | Loss: 0.00001372
Iteration 195/1000 | Loss: 0.00001372
Iteration 196/1000 | Loss: 0.00001372
Iteration 197/1000 | Loss: 0.00001372
Iteration 198/1000 | Loss: 0.00001372
Iteration 199/1000 | Loss: 0.00001372
Iteration 200/1000 | Loss: 0.00001372
Iteration 201/1000 | Loss: 0.00001372
Iteration 202/1000 | Loss: 0.00001372
Iteration 203/1000 | Loss: 0.00001372
Iteration 204/1000 | Loss: 0.00001372
Iteration 205/1000 | Loss: 0.00001372
Iteration 206/1000 | Loss: 0.00001372
Iteration 207/1000 | Loss: 0.00001372
Iteration 208/1000 | Loss: 0.00001372
Iteration 209/1000 | Loss: 0.00001372
Iteration 210/1000 | Loss: 0.00001372
Iteration 211/1000 | Loss: 0.00001372
Iteration 212/1000 | Loss: 0.00001372
Iteration 213/1000 | Loss: 0.00001372
Iteration 214/1000 | Loss: 0.00001372
Iteration 215/1000 | Loss: 0.00001372
Iteration 216/1000 | Loss: 0.00001372
Iteration 217/1000 | Loss: 0.00001372
Iteration 218/1000 | Loss: 0.00001372
Iteration 219/1000 | Loss: 0.00001372
Iteration 220/1000 | Loss: 0.00001372
Iteration 221/1000 | Loss: 0.00001372
Iteration 222/1000 | Loss: 0.00001372
Iteration 223/1000 | Loss: 0.00001372
Iteration 224/1000 | Loss: 0.00001372
Iteration 225/1000 | Loss: 0.00001372
Iteration 226/1000 | Loss: 0.00001372
Iteration 227/1000 | Loss: 0.00001372
Iteration 228/1000 | Loss: 0.00001372
Iteration 229/1000 | Loss: 0.00001372
Iteration 230/1000 | Loss: 0.00001372
Iteration 231/1000 | Loss: 0.00001372
Iteration 232/1000 | Loss: 0.00001372
Iteration 233/1000 | Loss: 0.00001372
Iteration 234/1000 | Loss: 0.00001372
Iteration 235/1000 | Loss: 0.00001372
Iteration 236/1000 | Loss: 0.00001372
Iteration 237/1000 | Loss: 0.00001372
Iteration 238/1000 | Loss: 0.00001372
Iteration 239/1000 | Loss: 0.00001372
Iteration 240/1000 | Loss: 0.00001372
Iteration 241/1000 | Loss: 0.00001372
Iteration 242/1000 | Loss: 0.00001372
Iteration 243/1000 | Loss: 0.00001372
Iteration 244/1000 | Loss: 0.00001372
Iteration 245/1000 | Loss: 0.00001372
Iteration 246/1000 | Loss: 0.00001372
Iteration 247/1000 | Loss: 0.00001372
Iteration 248/1000 | Loss: 0.00001372
Iteration 249/1000 | Loss: 0.00001372
Iteration 250/1000 | Loss: 0.00001372
Iteration 251/1000 | Loss: 0.00001372
Iteration 252/1000 | Loss: 0.00001372
Iteration 253/1000 | Loss: 0.00001372
Iteration 254/1000 | Loss: 0.00001372
Iteration 255/1000 | Loss: 0.00001372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [1.3720376955461688e-05, 1.3720376955461688e-05, 1.3720376955461688e-05, 1.3720376955461688e-05, 1.3720376955461688e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3720376955461688e-05

Optimization complete. Final v2v error: 3.1599552631378174 mm

Highest mean error: 3.4340126514434814 mm for frame 66

Lowest mean error: 3.047152519226074 mm for frame 146

Saving results

Total time: 43.00961709022522
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484007
Iteration 2/25 | Loss: 0.00152780
Iteration 3/25 | Loss: 0.00134087
Iteration 4/25 | Loss: 0.00132274
Iteration 5/25 | Loss: 0.00131973
Iteration 6/25 | Loss: 0.00131888
Iteration 7/25 | Loss: 0.00131888
Iteration 8/25 | Loss: 0.00131888
Iteration 9/25 | Loss: 0.00131888
Iteration 10/25 | Loss: 0.00131888
Iteration 11/25 | Loss: 0.00131888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013188814045861363, 0.0013188814045861363, 0.0013188814045861363, 0.0013188814045861363, 0.0013188814045861363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013188814045861363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41473436
Iteration 2/25 | Loss: 0.00084264
Iteration 3/25 | Loss: 0.00084264
Iteration 4/25 | Loss: 0.00084264
Iteration 5/25 | Loss: 0.00084264
Iteration 6/25 | Loss: 0.00084264
Iteration 7/25 | Loss: 0.00084264
Iteration 8/25 | Loss: 0.00084264
Iteration 9/25 | Loss: 0.00084264
Iteration 10/25 | Loss: 0.00084264
Iteration 11/25 | Loss: 0.00084264
Iteration 12/25 | Loss: 0.00084264
Iteration 13/25 | Loss: 0.00084264
Iteration 14/25 | Loss: 0.00084264
Iteration 15/25 | Loss: 0.00084264
Iteration 16/25 | Loss: 0.00084264
Iteration 17/25 | Loss: 0.00084264
Iteration 18/25 | Loss: 0.00084264
Iteration 19/25 | Loss: 0.00084264
Iteration 20/25 | Loss: 0.00084264
Iteration 21/25 | Loss: 0.00084264
Iteration 22/25 | Loss: 0.00084264
Iteration 23/25 | Loss: 0.00084264
Iteration 24/25 | Loss: 0.00084264
Iteration 25/25 | Loss: 0.00084264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084264
Iteration 2/1000 | Loss: 0.00004270
Iteration 3/1000 | Loss: 0.00002587
Iteration 4/1000 | Loss: 0.00002233
Iteration 5/1000 | Loss: 0.00002076
Iteration 6/1000 | Loss: 0.00001984
Iteration 7/1000 | Loss: 0.00001904
Iteration 8/1000 | Loss: 0.00001849
Iteration 9/1000 | Loss: 0.00001819
Iteration 10/1000 | Loss: 0.00001795
Iteration 11/1000 | Loss: 0.00001774
Iteration 12/1000 | Loss: 0.00001767
Iteration 13/1000 | Loss: 0.00001766
Iteration 14/1000 | Loss: 0.00001759
Iteration 15/1000 | Loss: 0.00001759
Iteration 16/1000 | Loss: 0.00001756
Iteration 17/1000 | Loss: 0.00001753
Iteration 18/1000 | Loss: 0.00001753
Iteration 19/1000 | Loss: 0.00001749
Iteration 20/1000 | Loss: 0.00001748
Iteration 21/1000 | Loss: 0.00001746
Iteration 22/1000 | Loss: 0.00001744
Iteration 23/1000 | Loss: 0.00001743
Iteration 24/1000 | Loss: 0.00001738
Iteration 25/1000 | Loss: 0.00001733
Iteration 26/1000 | Loss: 0.00001733
Iteration 27/1000 | Loss: 0.00001731
Iteration 28/1000 | Loss: 0.00001730
Iteration 29/1000 | Loss: 0.00001729
Iteration 30/1000 | Loss: 0.00001728
Iteration 31/1000 | Loss: 0.00001728
Iteration 32/1000 | Loss: 0.00001728
Iteration 33/1000 | Loss: 0.00001727
Iteration 34/1000 | Loss: 0.00001727
Iteration 35/1000 | Loss: 0.00001726
Iteration 36/1000 | Loss: 0.00001726
Iteration 37/1000 | Loss: 0.00001725
Iteration 38/1000 | Loss: 0.00001724
Iteration 39/1000 | Loss: 0.00001723
Iteration 40/1000 | Loss: 0.00001722
Iteration 41/1000 | Loss: 0.00001721
Iteration 42/1000 | Loss: 0.00001720
Iteration 43/1000 | Loss: 0.00001720
Iteration 44/1000 | Loss: 0.00001719
Iteration 45/1000 | Loss: 0.00001719
Iteration 46/1000 | Loss: 0.00001719
Iteration 47/1000 | Loss: 0.00001718
Iteration 48/1000 | Loss: 0.00001718
Iteration 49/1000 | Loss: 0.00001718
Iteration 50/1000 | Loss: 0.00001717
Iteration 51/1000 | Loss: 0.00001717
Iteration 52/1000 | Loss: 0.00001717
Iteration 53/1000 | Loss: 0.00001717
Iteration 54/1000 | Loss: 0.00001716
Iteration 55/1000 | Loss: 0.00001716
Iteration 56/1000 | Loss: 0.00001715
Iteration 57/1000 | Loss: 0.00001715
Iteration 58/1000 | Loss: 0.00001714
Iteration 59/1000 | Loss: 0.00001714
Iteration 60/1000 | Loss: 0.00001714
Iteration 61/1000 | Loss: 0.00001714
Iteration 62/1000 | Loss: 0.00001714
Iteration 63/1000 | Loss: 0.00001713
Iteration 64/1000 | Loss: 0.00001713
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001711
Iteration 67/1000 | Loss: 0.00001711
Iteration 68/1000 | Loss: 0.00001711
Iteration 69/1000 | Loss: 0.00001710
Iteration 70/1000 | Loss: 0.00001710
Iteration 71/1000 | Loss: 0.00001710
Iteration 72/1000 | Loss: 0.00001708
Iteration 73/1000 | Loss: 0.00001708
Iteration 74/1000 | Loss: 0.00001708
Iteration 75/1000 | Loss: 0.00001708
Iteration 76/1000 | Loss: 0.00001708
Iteration 77/1000 | Loss: 0.00001708
Iteration 78/1000 | Loss: 0.00001708
Iteration 79/1000 | Loss: 0.00001708
Iteration 80/1000 | Loss: 0.00001707
Iteration 81/1000 | Loss: 0.00001707
Iteration 82/1000 | Loss: 0.00001707
Iteration 83/1000 | Loss: 0.00001707
Iteration 84/1000 | Loss: 0.00001707
Iteration 85/1000 | Loss: 0.00001707
Iteration 86/1000 | Loss: 0.00001707
Iteration 87/1000 | Loss: 0.00001705
Iteration 88/1000 | Loss: 0.00001705
Iteration 89/1000 | Loss: 0.00001705
Iteration 90/1000 | Loss: 0.00001705
Iteration 91/1000 | Loss: 0.00001705
Iteration 92/1000 | Loss: 0.00001705
Iteration 93/1000 | Loss: 0.00001704
Iteration 94/1000 | Loss: 0.00001704
Iteration 95/1000 | Loss: 0.00001704
Iteration 96/1000 | Loss: 0.00001704
Iteration 97/1000 | Loss: 0.00001704
Iteration 98/1000 | Loss: 0.00001704
Iteration 99/1000 | Loss: 0.00001704
Iteration 100/1000 | Loss: 0.00001703
Iteration 101/1000 | Loss: 0.00001702
Iteration 102/1000 | Loss: 0.00001701
Iteration 103/1000 | Loss: 0.00001700
Iteration 104/1000 | Loss: 0.00001700
Iteration 105/1000 | Loss: 0.00001700
Iteration 106/1000 | Loss: 0.00001700
Iteration 107/1000 | Loss: 0.00001700
Iteration 108/1000 | Loss: 0.00001700
Iteration 109/1000 | Loss: 0.00001700
Iteration 110/1000 | Loss: 0.00001699
Iteration 111/1000 | Loss: 0.00001699
Iteration 112/1000 | Loss: 0.00001699
Iteration 113/1000 | Loss: 0.00001699
Iteration 114/1000 | Loss: 0.00001699
Iteration 115/1000 | Loss: 0.00001699
Iteration 116/1000 | Loss: 0.00001699
Iteration 117/1000 | Loss: 0.00001699
Iteration 118/1000 | Loss: 0.00001699
Iteration 119/1000 | Loss: 0.00001698
Iteration 120/1000 | Loss: 0.00001697
Iteration 121/1000 | Loss: 0.00001697
Iteration 122/1000 | Loss: 0.00001697
Iteration 123/1000 | Loss: 0.00001696
Iteration 124/1000 | Loss: 0.00001696
Iteration 125/1000 | Loss: 0.00001696
Iteration 126/1000 | Loss: 0.00001696
Iteration 127/1000 | Loss: 0.00001696
Iteration 128/1000 | Loss: 0.00001696
Iteration 129/1000 | Loss: 0.00001695
Iteration 130/1000 | Loss: 0.00001695
Iteration 131/1000 | Loss: 0.00001695
Iteration 132/1000 | Loss: 0.00001695
Iteration 133/1000 | Loss: 0.00001695
Iteration 134/1000 | Loss: 0.00001694
Iteration 135/1000 | Loss: 0.00001694
Iteration 136/1000 | Loss: 0.00001694
Iteration 137/1000 | Loss: 0.00001694
Iteration 138/1000 | Loss: 0.00001694
Iteration 139/1000 | Loss: 0.00001693
Iteration 140/1000 | Loss: 0.00001693
Iteration 141/1000 | Loss: 0.00001693
Iteration 142/1000 | Loss: 0.00001693
Iteration 143/1000 | Loss: 0.00001693
Iteration 144/1000 | Loss: 0.00001693
Iteration 145/1000 | Loss: 0.00001693
Iteration 146/1000 | Loss: 0.00001692
Iteration 147/1000 | Loss: 0.00001692
Iteration 148/1000 | Loss: 0.00001692
Iteration 149/1000 | Loss: 0.00001692
Iteration 150/1000 | Loss: 0.00001691
Iteration 151/1000 | Loss: 0.00001691
Iteration 152/1000 | Loss: 0.00001691
Iteration 153/1000 | Loss: 0.00001691
Iteration 154/1000 | Loss: 0.00001690
Iteration 155/1000 | Loss: 0.00001690
Iteration 156/1000 | Loss: 0.00001690
Iteration 157/1000 | Loss: 0.00001690
Iteration 158/1000 | Loss: 0.00001690
Iteration 159/1000 | Loss: 0.00001689
Iteration 160/1000 | Loss: 0.00001689
Iteration 161/1000 | Loss: 0.00001689
Iteration 162/1000 | Loss: 0.00001689
Iteration 163/1000 | Loss: 0.00001689
Iteration 164/1000 | Loss: 0.00001689
Iteration 165/1000 | Loss: 0.00001689
Iteration 166/1000 | Loss: 0.00001689
Iteration 167/1000 | Loss: 0.00001689
Iteration 168/1000 | Loss: 0.00001689
Iteration 169/1000 | Loss: 0.00001689
Iteration 170/1000 | Loss: 0.00001689
Iteration 171/1000 | Loss: 0.00001689
Iteration 172/1000 | Loss: 0.00001689
Iteration 173/1000 | Loss: 0.00001689
Iteration 174/1000 | Loss: 0.00001689
Iteration 175/1000 | Loss: 0.00001688
Iteration 176/1000 | Loss: 0.00001688
Iteration 177/1000 | Loss: 0.00001688
Iteration 178/1000 | Loss: 0.00001688
Iteration 179/1000 | Loss: 0.00001688
Iteration 180/1000 | Loss: 0.00001688
Iteration 181/1000 | Loss: 0.00001688
Iteration 182/1000 | Loss: 0.00001688
Iteration 183/1000 | Loss: 0.00001688
Iteration 184/1000 | Loss: 0.00001688
Iteration 185/1000 | Loss: 0.00001687
Iteration 186/1000 | Loss: 0.00001687
Iteration 187/1000 | Loss: 0.00001687
Iteration 188/1000 | Loss: 0.00001687
Iteration 189/1000 | Loss: 0.00001687
Iteration 190/1000 | Loss: 0.00001687
Iteration 191/1000 | Loss: 0.00001687
Iteration 192/1000 | Loss: 0.00001687
Iteration 193/1000 | Loss: 0.00001687
Iteration 194/1000 | Loss: 0.00001686
Iteration 195/1000 | Loss: 0.00001686
Iteration 196/1000 | Loss: 0.00001686
Iteration 197/1000 | Loss: 0.00001686
Iteration 198/1000 | Loss: 0.00001686
Iteration 199/1000 | Loss: 0.00001686
Iteration 200/1000 | Loss: 0.00001685
Iteration 201/1000 | Loss: 0.00001685
Iteration 202/1000 | Loss: 0.00001685
Iteration 203/1000 | Loss: 0.00001684
Iteration 204/1000 | Loss: 0.00001684
Iteration 205/1000 | Loss: 0.00001684
Iteration 206/1000 | Loss: 0.00001684
Iteration 207/1000 | Loss: 0.00001684
Iteration 208/1000 | Loss: 0.00001684
Iteration 209/1000 | Loss: 0.00001683
Iteration 210/1000 | Loss: 0.00001683
Iteration 211/1000 | Loss: 0.00001683
Iteration 212/1000 | Loss: 0.00001683
Iteration 213/1000 | Loss: 0.00001683
Iteration 214/1000 | Loss: 0.00001683
Iteration 215/1000 | Loss: 0.00001683
Iteration 216/1000 | Loss: 0.00001683
Iteration 217/1000 | Loss: 0.00001683
Iteration 218/1000 | Loss: 0.00001683
Iteration 219/1000 | Loss: 0.00001682
Iteration 220/1000 | Loss: 0.00001682
Iteration 221/1000 | Loss: 0.00001682
Iteration 222/1000 | Loss: 0.00001682
Iteration 223/1000 | Loss: 0.00001682
Iteration 224/1000 | Loss: 0.00001681
Iteration 225/1000 | Loss: 0.00001681
Iteration 226/1000 | Loss: 0.00001681
Iteration 227/1000 | Loss: 0.00001681
Iteration 228/1000 | Loss: 0.00001681
Iteration 229/1000 | Loss: 0.00001681
Iteration 230/1000 | Loss: 0.00001681
Iteration 231/1000 | Loss: 0.00001681
Iteration 232/1000 | Loss: 0.00001681
Iteration 233/1000 | Loss: 0.00001681
Iteration 234/1000 | Loss: 0.00001680
Iteration 235/1000 | Loss: 0.00001680
Iteration 236/1000 | Loss: 0.00001680
Iteration 237/1000 | Loss: 0.00001680
Iteration 238/1000 | Loss: 0.00001680
Iteration 239/1000 | Loss: 0.00001680
Iteration 240/1000 | Loss: 0.00001680
Iteration 241/1000 | Loss: 0.00001680
Iteration 242/1000 | Loss: 0.00001680
Iteration 243/1000 | Loss: 0.00001680
Iteration 244/1000 | Loss: 0.00001680
Iteration 245/1000 | Loss: 0.00001680
Iteration 246/1000 | Loss: 0.00001680
Iteration 247/1000 | Loss: 0.00001679
Iteration 248/1000 | Loss: 0.00001679
Iteration 249/1000 | Loss: 0.00001679
Iteration 250/1000 | Loss: 0.00001679
Iteration 251/1000 | Loss: 0.00001679
Iteration 252/1000 | Loss: 0.00001679
Iteration 253/1000 | Loss: 0.00001679
Iteration 254/1000 | Loss: 0.00001679
Iteration 255/1000 | Loss: 0.00001679
Iteration 256/1000 | Loss: 0.00001679
Iteration 257/1000 | Loss: 0.00001679
Iteration 258/1000 | Loss: 0.00001679
Iteration 259/1000 | Loss: 0.00001679
Iteration 260/1000 | Loss: 0.00001679
Iteration 261/1000 | Loss: 0.00001679
Iteration 262/1000 | Loss: 0.00001679
Iteration 263/1000 | Loss: 0.00001679
Iteration 264/1000 | Loss: 0.00001679
Iteration 265/1000 | Loss: 0.00001679
Iteration 266/1000 | Loss: 0.00001679
Iteration 267/1000 | Loss: 0.00001679
Iteration 268/1000 | Loss: 0.00001679
Iteration 269/1000 | Loss: 0.00001678
Iteration 270/1000 | Loss: 0.00001678
Iteration 271/1000 | Loss: 0.00001678
Iteration 272/1000 | Loss: 0.00001678
Iteration 273/1000 | Loss: 0.00001678
Iteration 274/1000 | Loss: 0.00001678
Iteration 275/1000 | Loss: 0.00001678
Iteration 276/1000 | Loss: 0.00001678
Iteration 277/1000 | Loss: 0.00001678
Iteration 278/1000 | Loss: 0.00001678
Iteration 279/1000 | Loss: 0.00001678
Iteration 280/1000 | Loss: 0.00001678
Iteration 281/1000 | Loss: 0.00001678
Iteration 282/1000 | Loss: 0.00001678
Iteration 283/1000 | Loss: 0.00001678
Iteration 284/1000 | Loss: 0.00001678
Iteration 285/1000 | Loss: 0.00001678
Iteration 286/1000 | Loss: 0.00001678
Iteration 287/1000 | Loss: 0.00001678
Iteration 288/1000 | Loss: 0.00001678
Iteration 289/1000 | Loss: 0.00001677
Iteration 290/1000 | Loss: 0.00001677
Iteration 291/1000 | Loss: 0.00001677
Iteration 292/1000 | Loss: 0.00001677
Iteration 293/1000 | Loss: 0.00001677
Iteration 294/1000 | Loss: 0.00001677
Iteration 295/1000 | Loss: 0.00001677
Iteration 296/1000 | Loss: 0.00001677
Iteration 297/1000 | Loss: 0.00001677
Iteration 298/1000 | Loss: 0.00001677
Iteration 299/1000 | Loss: 0.00001677
Iteration 300/1000 | Loss: 0.00001677
Iteration 301/1000 | Loss: 0.00001677
Iteration 302/1000 | Loss: 0.00001677
Iteration 303/1000 | Loss: 0.00001677
Iteration 304/1000 | Loss: 0.00001677
Iteration 305/1000 | Loss: 0.00001677
Iteration 306/1000 | Loss: 0.00001677
Iteration 307/1000 | Loss: 0.00001677
Iteration 308/1000 | Loss: 0.00001676
Iteration 309/1000 | Loss: 0.00001676
Iteration 310/1000 | Loss: 0.00001676
Iteration 311/1000 | Loss: 0.00001676
Iteration 312/1000 | Loss: 0.00001676
Iteration 313/1000 | Loss: 0.00001676
Iteration 314/1000 | Loss: 0.00001676
Iteration 315/1000 | Loss: 0.00001676
Iteration 316/1000 | Loss: 0.00001676
Iteration 317/1000 | Loss: 0.00001676
Iteration 318/1000 | Loss: 0.00001676
Iteration 319/1000 | Loss: 0.00001676
Iteration 320/1000 | Loss: 0.00001676
Iteration 321/1000 | Loss: 0.00001676
Iteration 322/1000 | Loss: 0.00001676
Iteration 323/1000 | Loss: 0.00001676
Iteration 324/1000 | Loss: 0.00001676
Iteration 325/1000 | Loss: 0.00001676
Iteration 326/1000 | Loss: 0.00001676
Iteration 327/1000 | Loss: 0.00001676
Iteration 328/1000 | Loss: 0.00001676
Iteration 329/1000 | Loss: 0.00001676
Iteration 330/1000 | Loss: 0.00001676
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 330. Stopping optimization.
Last 5 losses: [1.6762320228735916e-05, 1.6762320228735916e-05, 1.6762320228735916e-05, 1.6762320228735916e-05, 1.6762320228735916e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6762320228735916e-05

Optimization complete. Final v2v error: 3.346769094467163 mm

Highest mean error: 4.708273887634277 mm for frame 72

Lowest mean error: 2.926558256149292 mm for frame 160

Saving results

Total time: 48.54817461967468
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850851
Iteration 2/25 | Loss: 0.00250012
Iteration 3/25 | Loss: 0.00203617
Iteration 4/25 | Loss: 0.00170863
Iteration 5/25 | Loss: 0.00161768
Iteration 6/25 | Loss: 0.00161918
Iteration 7/25 | Loss: 0.00164346
Iteration 8/25 | Loss: 0.00159388
Iteration 9/25 | Loss: 0.00153918
Iteration 10/25 | Loss: 0.00153205
Iteration 11/25 | Loss: 0.00151769
Iteration 12/25 | Loss: 0.00151385
Iteration 13/25 | Loss: 0.00151078
Iteration 14/25 | Loss: 0.00151106
Iteration 15/25 | Loss: 0.00151102
Iteration 16/25 | Loss: 0.00151069
Iteration 17/25 | Loss: 0.00151294
Iteration 18/25 | Loss: 0.00151505
Iteration 19/25 | Loss: 0.00151253
Iteration 20/25 | Loss: 0.00151127
Iteration 21/25 | Loss: 0.00151667
Iteration 22/25 | Loss: 0.00151294
Iteration 23/25 | Loss: 0.00151232
Iteration 24/25 | Loss: 0.00151224
Iteration 25/25 | Loss: 0.00151141

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16883981
Iteration 2/25 | Loss: 0.00093238
Iteration 3/25 | Loss: 0.00093237
Iteration 4/25 | Loss: 0.00093237
Iteration 5/25 | Loss: 0.00093237
Iteration 6/25 | Loss: 0.00093237
Iteration 7/25 | Loss: 0.00093237
Iteration 8/25 | Loss: 0.00093237
Iteration 9/25 | Loss: 0.00093236
Iteration 10/25 | Loss: 0.00093236
Iteration 11/25 | Loss: 0.00093236
Iteration 12/25 | Loss: 0.00093236
Iteration 13/25 | Loss: 0.00093236
Iteration 14/25 | Loss: 0.00093236
Iteration 15/25 | Loss: 0.00093236
Iteration 16/25 | Loss: 0.00093236
Iteration 17/25 | Loss: 0.00093236
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009323644335381687, 0.0009323644335381687, 0.0009323644335381687, 0.0009323644335381687, 0.0009323644335381687]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009323644335381687

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093236
Iteration 2/1000 | Loss: 0.00014540
Iteration 3/1000 | Loss: 0.00019932
Iteration 4/1000 | Loss: 0.00013639
Iteration 5/1000 | Loss: 0.00013128
Iteration 6/1000 | Loss: 0.00017649
Iteration 7/1000 | Loss: 0.00014646
Iteration 8/1000 | Loss: 0.00023999
Iteration 9/1000 | Loss: 0.00017571
Iteration 10/1000 | Loss: 0.00017851
Iteration 11/1000 | Loss: 0.00010468
Iteration 12/1000 | Loss: 0.00012330
Iteration 13/1000 | Loss: 0.00016298
Iteration 14/1000 | Loss: 0.00014316
Iteration 15/1000 | Loss: 0.00015352
Iteration 16/1000 | Loss: 0.00011612
Iteration 17/1000 | Loss: 0.00012170
Iteration 18/1000 | Loss: 0.00014971
Iteration 19/1000 | Loss: 0.00014851
Iteration 20/1000 | Loss: 0.00021998
Iteration 21/1000 | Loss: 0.00021959
Iteration 22/1000 | Loss: 0.00015957
Iteration 23/1000 | Loss: 0.00012767
Iteration 24/1000 | Loss: 0.00010704
Iteration 25/1000 | Loss: 0.00011760
Iteration 26/1000 | Loss: 0.00015274
Iteration 27/1000 | Loss: 0.00016033
Iteration 28/1000 | Loss: 0.00015276
Iteration 29/1000 | Loss: 0.00018045
Iteration 30/1000 | Loss: 0.00020635
Iteration 31/1000 | Loss: 0.00016930
Iteration 32/1000 | Loss: 0.00019715
Iteration 33/1000 | Loss: 0.00015302
Iteration 34/1000 | Loss: 0.00017318
Iteration 35/1000 | Loss: 0.00015634
Iteration 36/1000 | Loss: 0.00012761
Iteration 37/1000 | Loss: 0.00018795
Iteration 38/1000 | Loss: 0.00017423
Iteration 39/1000 | Loss: 0.00016251
Iteration 40/1000 | Loss: 0.00010569
Iteration 41/1000 | Loss: 0.00012705
Iteration 42/1000 | Loss: 0.00012493
Iteration 43/1000 | Loss: 0.00012647
Iteration 44/1000 | Loss: 0.00014872
Iteration 45/1000 | Loss: 0.00016779
Iteration 46/1000 | Loss: 0.00016141
Iteration 47/1000 | Loss: 0.00014808
Iteration 48/1000 | Loss: 0.00011837
Iteration 49/1000 | Loss: 0.00013236
Iteration 50/1000 | Loss: 0.00012308
Iteration 51/1000 | Loss: 0.00020119
Iteration 52/1000 | Loss: 0.00018047
Iteration 53/1000 | Loss: 0.00014669
Iteration 54/1000 | Loss: 0.00014914
Iteration 55/1000 | Loss: 0.00013378
Iteration 56/1000 | Loss: 0.00014732
Iteration 57/1000 | Loss: 0.00016627
Iteration 58/1000 | Loss: 0.00016001
Iteration 59/1000 | Loss: 0.00015776
Iteration 60/1000 | Loss: 0.00014187
Iteration 61/1000 | Loss: 0.00014685
Iteration 62/1000 | Loss: 0.00024273
Iteration 63/1000 | Loss: 0.00016734
Iteration 64/1000 | Loss: 0.00016207
Iteration 65/1000 | Loss: 0.00012639
Iteration 66/1000 | Loss: 0.00009984
Iteration 67/1000 | Loss: 0.00017491
Iteration 68/1000 | Loss: 0.00014273
Iteration 69/1000 | Loss: 0.00015785
Iteration 70/1000 | Loss: 0.00014297
Iteration 71/1000 | Loss: 0.00014140
Iteration 72/1000 | Loss: 0.00011180
Iteration 73/1000 | Loss: 0.00014441
Iteration 74/1000 | Loss: 0.00012012
Iteration 75/1000 | Loss: 0.00011262
Iteration 76/1000 | Loss: 0.00012672
Iteration 77/1000 | Loss: 0.00014110
Iteration 78/1000 | Loss: 0.00012315
Iteration 79/1000 | Loss: 0.00012565
Iteration 80/1000 | Loss: 0.00012065
Iteration 81/1000 | Loss: 0.00011427
Iteration 82/1000 | Loss: 0.00015761
Iteration 83/1000 | Loss: 0.00013118
Iteration 84/1000 | Loss: 0.00012030
Iteration 85/1000 | Loss: 0.00026409
Iteration 86/1000 | Loss: 0.00012991
Iteration 87/1000 | Loss: 0.00015353
Iteration 88/1000 | Loss: 0.00019594
Iteration 89/1000 | Loss: 0.00014551
Iteration 90/1000 | Loss: 0.00011861
Iteration 91/1000 | Loss: 0.00014675
Iteration 92/1000 | Loss: 0.00016535
Iteration 93/1000 | Loss: 0.00013758
Iteration 94/1000 | Loss: 0.00016011
Iteration 95/1000 | Loss: 0.00012727
Iteration 96/1000 | Loss: 0.00012387
Iteration 97/1000 | Loss: 0.00013465
Iteration 98/1000 | Loss: 0.00015916
Iteration 99/1000 | Loss: 0.00015617
Iteration 100/1000 | Loss: 0.00013645
Iteration 101/1000 | Loss: 0.00013314
Iteration 102/1000 | Loss: 0.00017636
Iteration 103/1000 | Loss: 0.00016987
Iteration 104/1000 | Loss: 0.00015930
Iteration 105/1000 | Loss: 0.00011396
Iteration 106/1000 | Loss: 0.00013554
Iteration 107/1000 | Loss: 0.00013175
Iteration 108/1000 | Loss: 0.00016155
Iteration 109/1000 | Loss: 0.00016871
Iteration 110/1000 | Loss: 0.00015382
Iteration 111/1000 | Loss: 0.00015596
Iteration 112/1000 | Loss: 0.00018091
Iteration 113/1000 | Loss: 0.00013868
Iteration 114/1000 | Loss: 0.00014847
Iteration 115/1000 | Loss: 0.00014756
Iteration 116/1000 | Loss: 0.00016556
Iteration 117/1000 | Loss: 0.00017750
Iteration 118/1000 | Loss: 0.00014151
Iteration 119/1000 | Loss: 0.00017792
Iteration 120/1000 | Loss: 0.00015671
Iteration 121/1000 | Loss: 0.00012177
Iteration 122/1000 | Loss: 0.00012469
Iteration 123/1000 | Loss: 0.00013566
Iteration 124/1000 | Loss: 0.00013735
Iteration 125/1000 | Loss: 0.00009480
Iteration 126/1000 | Loss: 0.00012369
Iteration 127/1000 | Loss: 0.00014910
Iteration 128/1000 | Loss: 0.00008741
Iteration 129/1000 | Loss: 0.00015420
Iteration 130/1000 | Loss: 0.00015382
Iteration 131/1000 | Loss: 0.00017353
Iteration 132/1000 | Loss: 0.00013437
Iteration 133/1000 | Loss: 0.00014457
Iteration 134/1000 | Loss: 0.00015268
Iteration 135/1000 | Loss: 0.00014739
Iteration 136/1000 | Loss: 0.00016864
Iteration 137/1000 | Loss: 0.00014728
Iteration 138/1000 | Loss: 0.00014329
Iteration 139/1000 | Loss: 0.00013722
Iteration 140/1000 | Loss: 0.00016083
Iteration 141/1000 | Loss: 0.00014902
Iteration 142/1000 | Loss: 0.00017750
Iteration 143/1000 | Loss: 0.00014643
Iteration 144/1000 | Loss: 0.00017565
Iteration 145/1000 | Loss: 0.00021251
Iteration 146/1000 | Loss: 0.00012646
Iteration 147/1000 | Loss: 0.00009987
Iteration 148/1000 | Loss: 0.00009728
Iteration 149/1000 | Loss: 0.00016032
Iteration 150/1000 | Loss: 0.00013034
Iteration 151/1000 | Loss: 0.00016697
Iteration 152/1000 | Loss: 0.00014820
Iteration 153/1000 | Loss: 0.00014103
Iteration 154/1000 | Loss: 0.00012141
Iteration 155/1000 | Loss: 0.00011341
Iteration 156/1000 | Loss: 0.00012690
Iteration 157/1000 | Loss: 0.00012058
Iteration 158/1000 | Loss: 0.00011637
Iteration 159/1000 | Loss: 0.00018080
Iteration 160/1000 | Loss: 0.00012716
Iteration 161/1000 | Loss: 0.00011840
Iteration 162/1000 | Loss: 0.00010099
Iteration 163/1000 | Loss: 0.00010394
Iteration 164/1000 | Loss: 0.00009316
Iteration 165/1000 | Loss: 0.00008686
Iteration 166/1000 | Loss: 0.00012589
Iteration 167/1000 | Loss: 0.00014223
Iteration 168/1000 | Loss: 0.00009473
Iteration 169/1000 | Loss: 0.00009780
Iteration 170/1000 | Loss: 0.00012691
Iteration 171/1000 | Loss: 0.00011524
Iteration 172/1000 | Loss: 0.00012264
Iteration 173/1000 | Loss: 0.00011981
Iteration 174/1000 | Loss: 0.00008515
Iteration 175/1000 | Loss: 0.00008457
Iteration 176/1000 | Loss: 0.00010207
Iteration 177/1000 | Loss: 0.00011220
Iteration 178/1000 | Loss: 0.00012349
Iteration 179/1000 | Loss: 0.00011835
Iteration 180/1000 | Loss: 0.00007298
Iteration 181/1000 | Loss: 0.00007535
Iteration 182/1000 | Loss: 0.00007536
Iteration 183/1000 | Loss: 0.00007990
Iteration 184/1000 | Loss: 0.00008695
Iteration 185/1000 | Loss: 0.00007614
Iteration 186/1000 | Loss: 0.00005573
Iteration 187/1000 | Loss: 0.00005078
Iteration 188/1000 | Loss: 0.00006302
Iteration 189/1000 | Loss: 0.00006323
Iteration 190/1000 | Loss: 0.00007771
Iteration 191/1000 | Loss: 0.00006801
Iteration 192/1000 | Loss: 0.00008896
Iteration 193/1000 | Loss: 0.00015065
Iteration 194/1000 | Loss: 0.00007617
Iteration 195/1000 | Loss: 0.00008942
Iteration 196/1000 | Loss: 0.00006881
Iteration 197/1000 | Loss: 0.00005912
Iteration 198/1000 | Loss: 0.00005708
Iteration 199/1000 | Loss: 0.00005700
Iteration 200/1000 | Loss: 0.00005004
Iteration 201/1000 | Loss: 0.00005878
Iteration 202/1000 | Loss: 0.00006777
Iteration 203/1000 | Loss: 0.00008888
Iteration 204/1000 | Loss: 0.00008605
Iteration 205/1000 | Loss: 0.00008276
Iteration 206/1000 | Loss: 0.00005466
Iteration 207/1000 | Loss: 0.00004907
Iteration 208/1000 | Loss: 0.00005415
Iteration 209/1000 | Loss: 0.00005349
Iteration 210/1000 | Loss: 0.00006863
Iteration 211/1000 | Loss: 0.00006980
Iteration 212/1000 | Loss: 0.00008080
Iteration 213/1000 | Loss: 0.00006417
Iteration 214/1000 | Loss: 0.00005939
Iteration 215/1000 | Loss: 0.00006418
Iteration 216/1000 | Loss: 0.00004801
Iteration 217/1000 | Loss: 0.00005777
Iteration 218/1000 | Loss: 0.00005970
Iteration 219/1000 | Loss: 0.00004633
Iteration 220/1000 | Loss: 0.00006170
Iteration 221/1000 | Loss: 0.00005803
Iteration 222/1000 | Loss: 0.00005575
Iteration 223/1000 | Loss: 0.00006124
Iteration 224/1000 | Loss: 0.00005842
Iteration 225/1000 | Loss: 0.00006039
Iteration 226/1000 | Loss: 0.00006490
Iteration 227/1000 | Loss: 0.00005806
Iteration 228/1000 | Loss: 0.00006384
Iteration 229/1000 | Loss: 0.00007581
Iteration 230/1000 | Loss: 0.00006015
Iteration 231/1000 | Loss: 0.00007080
Iteration 232/1000 | Loss: 0.00007326
Iteration 233/1000 | Loss: 0.00006558
Iteration 234/1000 | Loss: 0.00004139
Iteration 235/1000 | Loss: 0.00006117
Iteration 236/1000 | Loss: 0.00005635
Iteration 237/1000 | Loss: 0.00005461
Iteration 238/1000 | Loss: 0.00005353
Iteration 239/1000 | Loss: 0.00004912
Iteration 240/1000 | Loss: 0.00004862
Iteration 241/1000 | Loss: 0.00006054
Iteration 242/1000 | Loss: 0.00005928
Iteration 243/1000 | Loss: 0.00005659
Iteration 244/1000 | Loss: 0.00003494
Iteration 245/1000 | Loss: 0.00007369
Iteration 246/1000 | Loss: 0.00006931
Iteration 247/1000 | Loss: 0.00006439
Iteration 248/1000 | Loss: 0.00005708
Iteration 249/1000 | Loss: 0.00005581
Iteration 250/1000 | Loss: 0.00005385
Iteration 251/1000 | Loss: 0.00005636
Iteration 252/1000 | Loss: 0.00006390
Iteration 253/1000 | Loss: 0.00006146
Iteration 254/1000 | Loss: 0.00006193
Iteration 255/1000 | Loss: 0.00006035
Iteration 256/1000 | Loss: 0.00005361
Iteration 257/1000 | Loss: 0.00005444
Iteration 258/1000 | Loss: 0.00006662
Iteration 259/1000 | Loss: 0.00009814
Iteration 260/1000 | Loss: 0.00008445
Iteration 261/1000 | Loss: 0.00005569
Iteration 262/1000 | Loss: 0.00005038
Iteration 263/1000 | Loss: 0.00006315
Iteration 264/1000 | Loss: 0.00004627
Iteration 265/1000 | Loss: 0.00004885
Iteration 266/1000 | Loss: 0.00005213
Iteration 267/1000 | Loss: 0.00006792
Iteration 268/1000 | Loss: 0.00003857
Iteration 269/1000 | Loss: 0.00004794
Iteration 270/1000 | Loss: 0.00004544
Iteration 271/1000 | Loss: 0.00004040
Iteration 272/1000 | Loss: 0.00003907
Iteration 273/1000 | Loss: 0.00003647
Iteration 274/1000 | Loss: 0.00003652
Iteration 275/1000 | Loss: 0.00004345
Iteration 276/1000 | Loss: 0.00003802
Iteration 277/1000 | Loss: 0.00004165
Iteration 278/1000 | Loss: 0.00005717
Iteration 279/1000 | Loss: 0.00005031
Iteration 280/1000 | Loss: 0.00004462
Iteration 281/1000 | Loss: 0.00007040
Iteration 282/1000 | Loss: 0.00005395
Iteration 283/1000 | Loss: 0.00005057
Iteration 284/1000 | Loss: 0.00005636
Iteration 285/1000 | Loss: 0.00005325
Iteration 286/1000 | Loss: 0.00004314
Iteration 287/1000 | Loss: 0.00004674
Iteration 288/1000 | Loss: 0.00005288
Iteration 289/1000 | Loss: 0.00004888
Iteration 290/1000 | Loss: 0.00003166
Iteration 291/1000 | Loss: 0.00006590
Iteration 292/1000 | Loss: 0.00006002
Iteration 293/1000 | Loss: 0.00006956
Iteration 294/1000 | Loss: 0.00006045
Iteration 295/1000 | Loss: 0.00006584
Iteration 296/1000 | Loss: 0.00007731
Iteration 297/1000 | Loss: 0.00006904
Iteration 298/1000 | Loss: 0.00006315
Iteration 299/1000 | Loss: 0.00005983
Iteration 300/1000 | Loss: 0.00004979
Iteration 301/1000 | Loss: 0.00005227
Iteration 302/1000 | Loss: 0.00004728
Iteration 303/1000 | Loss: 0.00005003
Iteration 304/1000 | Loss: 0.00005893
Iteration 305/1000 | Loss: 0.00004758
Iteration 306/1000 | Loss: 0.00003640
Iteration 307/1000 | Loss: 0.00003490
Iteration 308/1000 | Loss: 0.00003735
Iteration 309/1000 | Loss: 0.00005857
Iteration 310/1000 | Loss: 0.00007541
Iteration 311/1000 | Loss: 0.00005912
Iteration 312/1000 | Loss: 0.00004658
Iteration 313/1000 | Loss: 0.00004779
Iteration 314/1000 | Loss: 0.00004670
Iteration 315/1000 | Loss: 0.00006279
Iteration 316/1000 | Loss: 0.00006112
Iteration 317/1000 | Loss: 0.00005043
Iteration 318/1000 | Loss: 0.00003978
Iteration 319/1000 | Loss: 0.00004018
Iteration 320/1000 | Loss: 0.00003985
Iteration 321/1000 | Loss: 0.00004365
Iteration 322/1000 | Loss: 0.00004234
Iteration 323/1000 | Loss: 0.00002962
Iteration 324/1000 | Loss: 0.00003763
Iteration 325/1000 | Loss: 0.00003238
Iteration 326/1000 | Loss: 0.00002730
Iteration 327/1000 | Loss: 0.00003204
Iteration 328/1000 | Loss: 0.00003628
Iteration 329/1000 | Loss: 0.00002756
Iteration 330/1000 | Loss: 0.00003222
Iteration 331/1000 | Loss: 0.00003343
Iteration 332/1000 | Loss: 0.00002999
Iteration 333/1000 | Loss: 0.00002852
Iteration 334/1000 | Loss: 0.00002750
Iteration 335/1000 | Loss: 0.00002695
Iteration 336/1000 | Loss: 0.00002660
Iteration 337/1000 | Loss: 0.00002639
Iteration 338/1000 | Loss: 0.00002619
Iteration 339/1000 | Loss: 0.00002583
Iteration 340/1000 | Loss: 0.00002557
Iteration 341/1000 | Loss: 0.00002530
Iteration 342/1000 | Loss: 0.00002516
Iteration 343/1000 | Loss: 0.00010502
Iteration 344/1000 | Loss: 0.00002965
Iteration 345/1000 | Loss: 0.00002812
Iteration 346/1000 | Loss: 0.00002690
Iteration 347/1000 | Loss: 0.00002631
Iteration 348/1000 | Loss: 0.00002554
Iteration 349/1000 | Loss: 0.00002520
Iteration 350/1000 | Loss: 0.00002515
Iteration 351/1000 | Loss: 0.00002511
Iteration 352/1000 | Loss: 0.00002506
Iteration 353/1000 | Loss: 0.00002506
Iteration 354/1000 | Loss: 0.00002505
Iteration 355/1000 | Loss: 0.00004504
Iteration 356/1000 | Loss: 0.00003324
Iteration 357/1000 | Loss: 0.00004399
Iteration 358/1000 | Loss: 0.00003153
Iteration 359/1000 | Loss: 0.00003227
Iteration 360/1000 | Loss: 0.00004513
Iteration 361/1000 | Loss: 0.00003435
Iteration 362/1000 | Loss: 0.00004475
Iteration 363/1000 | Loss: 0.00003596
Iteration 364/1000 | Loss: 0.00002699
Iteration 365/1000 | Loss: 0.00002767
Iteration 366/1000 | Loss: 0.00003621
Iteration 367/1000 | Loss: 0.00003758
Iteration 368/1000 | Loss: 0.00003718
Iteration 369/1000 | Loss: 0.00003642
Iteration 370/1000 | Loss: 0.00003265
Iteration 371/1000 | Loss: 0.00004101
Iteration 372/1000 | Loss: 0.00003923
Iteration 373/1000 | Loss: 0.00002947
Iteration 374/1000 | Loss: 0.00002732
Iteration 375/1000 | Loss: 0.00002519
Iteration 376/1000 | Loss: 0.00003937
Iteration 377/1000 | Loss: 0.00002784
Iteration 378/1000 | Loss: 0.00003409
Iteration 379/1000 | Loss: 0.00002841
Iteration 380/1000 | Loss: 0.00003321
Iteration 381/1000 | Loss: 0.00002538
Iteration 382/1000 | Loss: 0.00003285
Iteration 383/1000 | Loss: 0.00002982
Iteration 384/1000 | Loss: 0.00003311
Iteration 385/1000 | Loss: 0.00003231
Iteration 386/1000 | Loss: 0.00003507
Iteration 387/1000 | Loss: 0.00004066
Iteration 388/1000 | Loss: 0.00003730
Iteration 389/1000 | Loss: 0.00003714
Iteration 390/1000 | Loss: 0.00002911
Iteration 391/1000 | Loss: 0.00003708
Iteration 392/1000 | Loss: 0.00002964
Iteration 393/1000 | Loss: 0.00003403
Iteration 394/1000 | Loss: 0.00003280
Iteration 395/1000 | Loss: 0.00003705
Iteration 396/1000 | Loss: 0.00002632
Iteration 397/1000 | Loss: 0.00004151
Iteration 398/1000 | Loss: 0.00003051
Iteration 399/1000 | Loss: 0.00002475
Iteration 400/1000 | Loss: 0.00002466
Iteration 401/1000 | Loss: 0.00002465
Iteration 402/1000 | Loss: 0.00002465
Iteration 403/1000 | Loss: 0.00002465
Iteration 404/1000 | Loss: 0.00002465
Iteration 405/1000 | Loss: 0.00002465
Iteration 406/1000 | Loss: 0.00002465
Iteration 407/1000 | Loss: 0.00002465
Iteration 408/1000 | Loss: 0.00002465
Iteration 409/1000 | Loss: 0.00002465
Iteration 410/1000 | Loss: 0.00002465
Iteration 411/1000 | Loss: 0.00002465
Iteration 412/1000 | Loss: 0.00002465
Iteration 413/1000 | Loss: 0.00002465
Iteration 414/1000 | Loss: 0.00002464
Iteration 415/1000 | Loss: 0.00002464
Iteration 416/1000 | Loss: 0.00002464
Iteration 417/1000 | Loss: 0.00002464
Iteration 418/1000 | Loss: 0.00002464
Iteration 419/1000 | Loss: 0.00002463
Iteration 420/1000 | Loss: 0.00002463
Iteration 421/1000 | Loss: 0.00002807
Iteration 422/1000 | Loss: 0.00002587
Iteration 423/1000 | Loss: 0.00003821
Iteration 424/1000 | Loss: 0.00003493
Iteration 425/1000 | Loss: 0.00003806
Iteration 426/1000 | Loss: 0.00003530
Iteration 427/1000 | Loss: 0.00003721
Iteration 428/1000 | Loss: 0.00003547
Iteration 429/1000 | Loss: 0.00003673
Iteration 430/1000 | Loss: 0.00003315
Iteration 431/1000 | Loss: 0.00002585
Iteration 432/1000 | Loss: 0.00002523
Iteration 433/1000 | Loss: 0.00002511
Iteration 434/1000 | Loss: 0.00002504
Iteration 435/1000 | Loss: 0.00002497
Iteration 436/1000 | Loss: 0.00002497
Iteration 437/1000 | Loss: 0.00002496
Iteration 438/1000 | Loss: 0.00002495
Iteration 439/1000 | Loss: 0.00002494
Iteration 440/1000 | Loss: 0.00002477
Iteration 441/1000 | Loss: 0.00002460
Iteration 442/1000 | Loss: 0.00002459
Iteration 443/1000 | Loss: 0.00002458
Iteration 444/1000 | Loss: 0.00002456
Iteration 445/1000 | Loss: 0.00002456
Iteration 446/1000 | Loss: 0.00002456
Iteration 447/1000 | Loss: 0.00002456
Iteration 448/1000 | Loss: 0.00002456
Iteration 449/1000 | Loss: 0.00002455
Iteration 450/1000 | Loss: 0.00002455
Iteration 451/1000 | Loss: 0.00002455
Iteration 452/1000 | Loss: 0.00002454
Iteration 453/1000 | Loss: 0.00002454
Iteration 454/1000 | Loss: 0.00002454
Iteration 455/1000 | Loss: 0.00002453
Iteration 456/1000 | Loss: 0.00002453
Iteration 457/1000 | Loss: 0.00002451
Iteration 458/1000 | Loss: 0.00002451
Iteration 459/1000 | Loss: 0.00002450
Iteration 460/1000 | Loss: 0.00002450
Iteration 461/1000 | Loss: 0.00002450
Iteration 462/1000 | Loss: 0.00002449
Iteration 463/1000 | Loss: 0.00002448
Iteration 464/1000 | Loss: 0.00002448
Iteration 465/1000 | Loss: 0.00002448
Iteration 466/1000 | Loss: 0.00002448
Iteration 467/1000 | Loss: 0.00002448
Iteration 468/1000 | Loss: 0.00002448
Iteration 469/1000 | Loss: 0.00002448
Iteration 470/1000 | Loss: 0.00002447
Iteration 471/1000 | Loss: 0.00002447
Iteration 472/1000 | Loss: 0.00002447
Iteration 473/1000 | Loss: 0.00002447
Iteration 474/1000 | Loss: 0.00002447
Iteration 475/1000 | Loss: 0.00002447
Iteration 476/1000 | Loss: 0.00002447
Iteration 477/1000 | Loss: 0.00002447
Iteration 478/1000 | Loss: 0.00002447
Iteration 479/1000 | Loss: 0.00002447
Iteration 480/1000 | Loss: 0.00002447
Iteration 481/1000 | Loss: 0.00002447
Iteration 482/1000 | Loss: 0.00002447
Iteration 483/1000 | Loss: 0.00002447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 483. Stopping optimization.
Last 5 losses: [2.4467144612572156e-05, 2.4467144612572156e-05, 2.4467144612572156e-05, 2.4467144612572156e-05, 2.4467144612572156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4467144612572156e-05

Optimization complete. Final v2v error: 4.072540283203125 mm

Highest mean error: 6.550339698791504 mm for frame 178

Lowest mean error: 3.862266778945923 mm for frame 135

Saving results

Total time: 696.0761098861694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00956867
Iteration 2/25 | Loss: 0.00387665
Iteration 3/25 | Loss: 0.00304215
Iteration 4/25 | Loss: 0.00226820
Iteration 5/25 | Loss: 0.00206347
Iteration 6/25 | Loss: 0.00203100
Iteration 7/25 | Loss: 0.00199659
Iteration 8/25 | Loss: 0.00197787
Iteration 9/25 | Loss: 0.00197400
Iteration 10/25 | Loss: 0.00197156
Iteration 11/25 | Loss: 0.00197180
Iteration 12/25 | Loss: 0.00197148
Iteration 13/25 | Loss: 0.00197148
Iteration 14/25 | Loss: 0.00197148
Iteration 15/25 | Loss: 0.00197148
Iteration 16/25 | Loss: 0.00197148
Iteration 17/25 | Loss: 0.00197148
Iteration 18/25 | Loss: 0.00197147
Iteration 19/25 | Loss: 0.00197147
Iteration 20/25 | Loss: 0.00197147
Iteration 21/25 | Loss: 0.00197147
Iteration 22/25 | Loss: 0.00197147
Iteration 23/25 | Loss: 0.00197147
Iteration 24/25 | Loss: 0.00197147
Iteration 25/25 | Loss: 0.00197147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34357524
Iteration 2/25 | Loss: 0.00443394
Iteration 3/25 | Loss: 0.00445742
Iteration 4/25 | Loss: 0.00447244
Iteration 5/25 | Loss: 0.00445845
Iteration 6/25 | Loss: 0.00446776
Iteration 7/25 | Loss: 0.00443178
Iteration 8/25 | Loss: 0.00443146
Iteration 9/25 | Loss: 0.00444909
Iteration 10/25 | Loss: 0.00444044
Iteration 11/25 | Loss: 0.00443947
Iteration 12/25 | Loss: 0.00443082
Iteration 13/25 | Loss: 0.00444004
Iteration 14/25 | Loss: 0.00443958
Iteration 15/25 | Loss: 0.00442567
Iteration 16/25 | Loss: 0.00442567
Iteration 17/25 | Loss: 0.00442567
Iteration 18/25 | Loss: 0.00442567
Iteration 19/25 | Loss: 0.00442567
Iteration 20/25 | Loss: 0.00442567
Iteration 21/25 | Loss: 0.00442567
Iteration 22/25 | Loss: 0.00442567
Iteration 23/25 | Loss: 0.00442567
Iteration 24/25 | Loss: 0.00442566
Iteration 25/25 | Loss: 0.00442566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00442566
Iteration 2/1000 | Loss: 0.00061747
Iteration 3/1000 | Loss: 0.00052657
Iteration 4/1000 | Loss: 0.00045055
Iteration 5/1000 | Loss: 0.00186763
Iteration 6/1000 | Loss: 0.00044908
Iteration 7/1000 | Loss: 0.00037017
Iteration 8/1000 | Loss: 0.00035126
Iteration 9/1000 | Loss: 0.00033201
Iteration 10/1000 | Loss: 0.00032070
Iteration 11/1000 | Loss: 0.00264831
Iteration 12/1000 | Loss: 0.00036841
Iteration 13/1000 | Loss: 0.00033786
Iteration 14/1000 | Loss: 0.00029545
Iteration 15/1000 | Loss: 0.00040379
Iteration 16/1000 | Loss: 0.00029257
Iteration 17/1000 | Loss: 0.00551552
Iteration 18/1000 | Loss: 0.01180996
Iteration 19/1000 | Loss: 0.00081122
Iteration 20/1000 | Loss: 0.00035904
Iteration 21/1000 | Loss: 0.00028087
Iteration 22/1000 | Loss: 0.00021189
Iteration 23/1000 | Loss: 0.00014729
Iteration 24/1000 | Loss: 0.00014113
Iteration 25/1000 | Loss: 0.00011713
Iteration 26/1000 | Loss: 0.00009155
Iteration 27/1000 | Loss: 0.00009456
Iteration 28/1000 | Loss: 0.00006261
Iteration 29/1000 | Loss: 0.00005600
Iteration 30/1000 | Loss: 0.00010734
Iteration 31/1000 | Loss: 0.00005927
Iteration 32/1000 | Loss: 0.00010685
Iteration 33/1000 | Loss: 0.00009079
Iteration 34/1000 | Loss: 0.00008487
Iteration 35/1000 | Loss: 0.00005905
Iteration 36/1000 | Loss: 0.00007476
Iteration 37/1000 | Loss: 0.00008031
Iteration 38/1000 | Loss: 0.00003030
Iteration 39/1000 | Loss: 0.00006572
Iteration 40/1000 | Loss: 0.00009167
Iteration 41/1000 | Loss: 0.00002578
Iteration 42/1000 | Loss: 0.00002748
Iteration 43/1000 | Loss: 0.00002504
Iteration 44/1000 | Loss: 0.00002394
Iteration 45/1000 | Loss: 0.00002137
Iteration 46/1000 | Loss: 0.00002095
Iteration 47/1000 | Loss: 0.00002037
Iteration 48/1000 | Loss: 0.00002915
Iteration 49/1000 | Loss: 0.00002047
Iteration 50/1000 | Loss: 0.00003021
Iteration 51/1000 | Loss: 0.00002316
Iteration 52/1000 | Loss: 0.00002652
Iteration 53/1000 | Loss: 0.00002442
Iteration 54/1000 | Loss: 0.00002339
Iteration 55/1000 | Loss: 0.00001989
Iteration 56/1000 | Loss: 0.00001973
Iteration 57/1000 | Loss: 0.00001973
Iteration 58/1000 | Loss: 0.00001973
Iteration 59/1000 | Loss: 0.00001973
Iteration 60/1000 | Loss: 0.00001973
Iteration 61/1000 | Loss: 0.00001973
Iteration 62/1000 | Loss: 0.00001973
Iteration 63/1000 | Loss: 0.00001972
Iteration 64/1000 | Loss: 0.00001972
Iteration 65/1000 | Loss: 0.00001978
Iteration 66/1000 | Loss: 0.00001970
Iteration 67/1000 | Loss: 0.00001970
Iteration 68/1000 | Loss: 0.00001970
Iteration 69/1000 | Loss: 0.00001970
Iteration 70/1000 | Loss: 0.00001970
Iteration 71/1000 | Loss: 0.00001970
Iteration 72/1000 | Loss: 0.00001970
Iteration 73/1000 | Loss: 0.00001970
Iteration 74/1000 | Loss: 0.00002184
Iteration 75/1000 | Loss: 0.00001977
Iteration 76/1000 | Loss: 0.00001969
Iteration 77/1000 | Loss: 0.00001969
Iteration 78/1000 | Loss: 0.00001968
Iteration 79/1000 | Loss: 0.00001968
Iteration 80/1000 | Loss: 0.00001967
Iteration 81/1000 | Loss: 0.00001995
Iteration 82/1000 | Loss: 0.00001964
Iteration 83/1000 | Loss: 0.00001964
Iteration 84/1000 | Loss: 0.00001963
Iteration 85/1000 | Loss: 0.00001963
Iteration 86/1000 | Loss: 0.00001963
Iteration 87/1000 | Loss: 0.00001963
Iteration 88/1000 | Loss: 0.00001963
Iteration 89/1000 | Loss: 0.00001963
Iteration 90/1000 | Loss: 0.00001963
Iteration 91/1000 | Loss: 0.00001963
Iteration 92/1000 | Loss: 0.00001963
Iteration 93/1000 | Loss: 0.00001963
Iteration 94/1000 | Loss: 0.00001963
Iteration 95/1000 | Loss: 0.00001963
Iteration 96/1000 | Loss: 0.00001963
Iteration 97/1000 | Loss: 0.00001963
Iteration 98/1000 | Loss: 0.00001963
Iteration 99/1000 | Loss: 0.00001962
Iteration 100/1000 | Loss: 0.00001962
Iteration 101/1000 | Loss: 0.00001958
Iteration 102/1000 | Loss: 0.00001958
Iteration 103/1000 | Loss: 0.00001958
Iteration 104/1000 | Loss: 0.00001958
Iteration 105/1000 | Loss: 0.00001958
Iteration 106/1000 | Loss: 0.00001958
Iteration 107/1000 | Loss: 0.00001958
Iteration 108/1000 | Loss: 0.00001958
Iteration 109/1000 | Loss: 0.00001958
Iteration 110/1000 | Loss: 0.00001958
Iteration 111/1000 | Loss: 0.00001957
Iteration 112/1000 | Loss: 0.00001957
Iteration 113/1000 | Loss: 0.00001957
Iteration 114/1000 | Loss: 0.00001956
Iteration 115/1000 | Loss: 0.00001956
Iteration 116/1000 | Loss: 0.00001956
Iteration 117/1000 | Loss: 0.00001955
Iteration 118/1000 | Loss: 0.00001955
Iteration 119/1000 | Loss: 0.00001955
Iteration 120/1000 | Loss: 0.00001954
Iteration 121/1000 | Loss: 0.00001954
Iteration 122/1000 | Loss: 0.00001954
Iteration 123/1000 | Loss: 0.00001953
Iteration 124/1000 | Loss: 0.00001953
Iteration 125/1000 | Loss: 0.00001952
Iteration 126/1000 | Loss: 0.00001952
Iteration 127/1000 | Loss: 0.00001952
Iteration 128/1000 | Loss: 0.00001964
Iteration 129/1000 | Loss: 0.00001964
Iteration 130/1000 | Loss: 0.00002018
Iteration 131/1000 | Loss: 0.00001970
Iteration 132/1000 | Loss: 0.00002034
Iteration 133/1000 | Loss: 0.00001966
Iteration 134/1000 | Loss: 0.00001970
Iteration 135/1000 | Loss: 0.00001946
Iteration 136/1000 | Loss: 0.00001946
Iteration 137/1000 | Loss: 0.00001946
Iteration 138/1000 | Loss: 0.00001993
Iteration 139/1000 | Loss: 0.00001965
Iteration 140/1000 | Loss: 0.00001965
Iteration 141/1000 | Loss: 0.00002011
Iteration 142/1000 | Loss: 0.00001946
Iteration 143/1000 | Loss: 0.00001946
Iteration 144/1000 | Loss: 0.00001946
Iteration 145/1000 | Loss: 0.00001946
Iteration 146/1000 | Loss: 0.00001946
Iteration 147/1000 | Loss: 0.00001946
Iteration 148/1000 | Loss: 0.00001946
Iteration 149/1000 | Loss: 0.00001946
Iteration 150/1000 | Loss: 0.00001945
Iteration 151/1000 | Loss: 0.00001945
Iteration 152/1000 | Loss: 0.00001945
Iteration 153/1000 | Loss: 0.00001945
Iteration 154/1000 | Loss: 0.00001945
Iteration 155/1000 | Loss: 0.00001945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.9454782886896282e-05, 1.9454782886896282e-05, 1.9454782886896282e-05, 1.9454782886896282e-05, 1.9454782886896282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9454782886896282e-05

Optimization complete. Final v2v error: 3.752657651901245 mm

Highest mean error: 3.8948724269866943 mm for frame 28

Lowest mean error: 3.2928102016448975 mm for frame 130

Saving results

Total time: 114.6331627368927
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493684
Iteration 2/25 | Loss: 0.00140815
Iteration 3/25 | Loss: 0.00132824
Iteration 4/25 | Loss: 0.00131122
Iteration 5/25 | Loss: 0.00130784
Iteration 6/25 | Loss: 0.00130784
Iteration 7/25 | Loss: 0.00130784
Iteration 8/25 | Loss: 0.00130784
Iteration 9/25 | Loss: 0.00130784
Iteration 10/25 | Loss: 0.00130784
Iteration 11/25 | Loss: 0.00130784
Iteration 12/25 | Loss: 0.00130784
Iteration 13/25 | Loss: 0.00130784
Iteration 14/25 | Loss: 0.00130784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013078380143269897, 0.0013078380143269897, 0.0013078380143269897, 0.0013078380143269897, 0.0013078380143269897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013078380143269897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.65000200
Iteration 2/25 | Loss: 0.00092287
Iteration 3/25 | Loss: 0.00092287
Iteration 4/25 | Loss: 0.00092287
Iteration 5/25 | Loss: 0.00092287
Iteration 6/25 | Loss: 0.00092286
Iteration 7/25 | Loss: 0.00092286
Iteration 8/25 | Loss: 0.00092286
Iteration 9/25 | Loss: 0.00092286
Iteration 10/25 | Loss: 0.00092286
Iteration 11/25 | Loss: 0.00092286
Iteration 12/25 | Loss: 0.00092286
Iteration 13/25 | Loss: 0.00092286
Iteration 14/25 | Loss: 0.00092286
Iteration 15/25 | Loss: 0.00092286
Iteration 16/25 | Loss: 0.00092286
Iteration 17/25 | Loss: 0.00092286
Iteration 18/25 | Loss: 0.00092286
Iteration 19/25 | Loss: 0.00092286
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000922863429877907, 0.000922863429877907, 0.000922863429877907, 0.000922863429877907, 0.000922863429877907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000922863429877907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092286
Iteration 2/1000 | Loss: 0.00003092
Iteration 3/1000 | Loss: 0.00002267
Iteration 4/1000 | Loss: 0.00002094
Iteration 5/1000 | Loss: 0.00001948
Iteration 6/1000 | Loss: 0.00001872
Iteration 7/1000 | Loss: 0.00001820
Iteration 8/1000 | Loss: 0.00001763
Iteration 9/1000 | Loss: 0.00001730
Iteration 10/1000 | Loss: 0.00001705
Iteration 11/1000 | Loss: 0.00001682
Iteration 12/1000 | Loss: 0.00001671
Iteration 13/1000 | Loss: 0.00001665
Iteration 14/1000 | Loss: 0.00001663
Iteration 15/1000 | Loss: 0.00001662
Iteration 16/1000 | Loss: 0.00001653
Iteration 17/1000 | Loss: 0.00001642
Iteration 18/1000 | Loss: 0.00001641
Iteration 19/1000 | Loss: 0.00001640
Iteration 20/1000 | Loss: 0.00001640
Iteration 21/1000 | Loss: 0.00001640
Iteration 22/1000 | Loss: 0.00001639
Iteration 23/1000 | Loss: 0.00001639
Iteration 24/1000 | Loss: 0.00001638
Iteration 25/1000 | Loss: 0.00001637
Iteration 26/1000 | Loss: 0.00001637
Iteration 27/1000 | Loss: 0.00001636
Iteration 28/1000 | Loss: 0.00001635
Iteration 29/1000 | Loss: 0.00001635
Iteration 30/1000 | Loss: 0.00001634
Iteration 31/1000 | Loss: 0.00001634
Iteration 32/1000 | Loss: 0.00001631
Iteration 33/1000 | Loss: 0.00001630
Iteration 34/1000 | Loss: 0.00001630
Iteration 35/1000 | Loss: 0.00001626
Iteration 36/1000 | Loss: 0.00001626
Iteration 37/1000 | Loss: 0.00001625
Iteration 38/1000 | Loss: 0.00001625
Iteration 39/1000 | Loss: 0.00001625
Iteration 40/1000 | Loss: 0.00001625
Iteration 41/1000 | Loss: 0.00001625
Iteration 42/1000 | Loss: 0.00001624
Iteration 43/1000 | Loss: 0.00001623
Iteration 44/1000 | Loss: 0.00001622
Iteration 45/1000 | Loss: 0.00001622
Iteration 46/1000 | Loss: 0.00001622
Iteration 47/1000 | Loss: 0.00001622
Iteration 48/1000 | Loss: 0.00001622
Iteration 49/1000 | Loss: 0.00001621
Iteration 50/1000 | Loss: 0.00001621
Iteration 51/1000 | Loss: 0.00001621
Iteration 52/1000 | Loss: 0.00001621
Iteration 53/1000 | Loss: 0.00001621
Iteration 54/1000 | Loss: 0.00001621
Iteration 55/1000 | Loss: 0.00001621
Iteration 56/1000 | Loss: 0.00001621
Iteration 57/1000 | Loss: 0.00001621
Iteration 58/1000 | Loss: 0.00001621
Iteration 59/1000 | Loss: 0.00001621
Iteration 60/1000 | Loss: 0.00001620
Iteration 61/1000 | Loss: 0.00001620
Iteration 62/1000 | Loss: 0.00001619
Iteration 63/1000 | Loss: 0.00001619
Iteration 64/1000 | Loss: 0.00001619
Iteration 65/1000 | Loss: 0.00001619
Iteration 66/1000 | Loss: 0.00001618
Iteration 67/1000 | Loss: 0.00001618
Iteration 68/1000 | Loss: 0.00001618
Iteration 69/1000 | Loss: 0.00001618
Iteration 70/1000 | Loss: 0.00001618
Iteration 71/1000 | Loss: 0.00001618
Iteration 72/1000 | Loss: 0.00001618
Iteration 73/1000 | Loss: 0.00001617
Iteration 74/1000 | Loss: 0.00001617
Iteration 75/1000 | Loss: 0.00001615
Iteration 76/1000 | Loss: 0.00001615
Iteration 77/1000 | Loss: 0.00001615
Iteration 78/1000 | Loss: 0.00001615
Iteration 79/1000 | Loss: 0.00001615
Iteration 80/1000 | Loss: 0.00001615
Iteration 81/1000 | Loss: 0.00001615
Iteration 82/1000 | Loss: 0.00001615
Iteration 83/1000 | Loss: 0.00001614
Iteration 84/1000 | Loss: 0.00001614
Iteration 85/1000 | Loss: 0.00001614
Iteration 86/1000 | Loss: 0.00001613
Iteration 87/1000 | Loss: 0.00001613
Iteration 88/1000 | Loss: 0.00001613
Iteration 89/1000 | Loss: 0.00001612
Iteration 90/1000 | Loss: 0.00001612
Iteration 91/1000 | Loss: 0.00001611
Iteration 92/1000 | Loss: 0.00001611
Iteration 93/1000 | Loss: 0.00001611
Iteration 94/1000 | Loss: 0.00001611
Iteration 95/1000 | Loss: 0.00001610
Iteration 96/1000 | Loss: 0.00001610
Iteration 97/1000 | Loss: 0.00001610
Iteration 98/1000 | Loss: 0.00001610
Iteration 99/1000 | Loss: 0.00001610
Iteration 100/1000 | Loss: 0.00001609
Iteration 101/1000 | Loss: 0.00001609
Iteration 102/1000 | Loss: 0.00001609
Iteration 103/1000 | Loss: 0.00001608
Iteration 104/1000 | Loss: 0.00001607
Iteration 105/1000 | Loss: 0.00001607
Iteration 106/1000 | Loss: 0.00001607
Iteration 107/1000 | Loss: 0.00001607
Iteration 108/1000 | Loss: 0.00001607
Iteration 109/1000 | Loss: 0.00001605
Iteration 110/1000 | Loss: 0.00001605
Iteration 111/1000 | Loss: 0.00001605
Iteration 112/1000 | Loss: 0.00001604
Iteration 113/1000 | Loss: 0.00001603
Iteration 114/1000 | Loss: 0.00001603
Iteration 115/1000 | Loss: 0.00001603
Iteration 116/1000 | Loss: 0.00001602
Iteration 117/1000 | Loss: 0.00001602
Iteration 118/1000 | Loss: 0.00001602
Iteration 119/1000 | Loss: 0.00001602
Iteration 120/1000 | Loss: 0.00001602
Iteration 121/1000 | Loss: 0.00001602
Iteration 122/1000 | Loss: 0.00001602
Iteration 123/1000 | Loss: 0.00001602
Iteration 124/1000 | Loss: 0.00001602
Iteration 125/1000 | Loss: 0.00001602
Iteration 126/1000 | Loss: 0.00001601
Iteration 127/1000 | Loss: 0.00001601
Iteration 128/1000 | Loss: 0.00001601
Iteration 129/1000 | Loss: 0.00001601
Iteration 130/1000 | Loss: 0.00001600
Iteration 131/1000 | Loss: 0.00001600
Iteration 132/1000 | Loss: 0.00001600
Iteration 133/1000 | Loss: 0.00001600
Iteration 134/1000 | Loss: 0.00001600
Iteration 135/1000 | Loss: 0.00001600
Iteration 136/1000 | Loss: 0.00001600
Iteration 137/1000 | Loss: 0.00001600
Iteration 138/1000 | Loss: 0.00001600
Iteration 139/1000 | Loss: 0.00001600
Iteration 140/1000 | Loss: 0.00001600
Iteration 141/1000 | Loss: 0.00001600
Iteration 142/1000 | Loss: 0.00001599
Iteration 143/1000 | Loss: 0.00001599
Iteration 144/1000 | Loss: 0.00001599
Iteration 145/1000 | Loss: 0.00001599
Iteration 146/1000 | Loss: 0.00001599
Iteration 147/1000 | Loss: 0.00001599
Iteration 148/1000 | Loss: 0.00001599
Iteration 149/1000 | Loss: 0.00001599
Iteration 150/1000 | Loss: 0.00001599
Iteration 151/1000 | Loss: 0.00001598
Iteration 152/1000 | Loss: 0.00001598
Iteration 153/1000 | Loss: 0.00001598
Iteration 154/1000 | Loss: 0.00001598
Iteration 155/1000 | Loss: 0.00001598
Iteration 156/1000 | Loss: 0.00001598
Iteration 157/1000 | Loss: 0.00001598
Iteration 158/1000 | Loss: 0.00001598
Iteration 159/1000 | Loss: 0.00001598
Iteration 160/1000 | Loss: 0.00001598
Iteration 161/1000 | Loss: 0.00001598
Iteration 162/1000 | Loss: 0.00001598
Iteration 163/1000 | Loss: 0.00001598
Iteration 164/1000 | Loss: 0.00001598
Iteration 165/1000 | Loss: 0.00001598
Iteration 166/1000 | Loss: 0.00001598
Iteration 167/1000 | Loss: 0.00001598
Iteration 168/1000 | Loss: 0.00001598
Iteration 169/1000 | Loss: 0.00001598
Iteration 170/1000 | Loss: 0.00001598
Iteration 171/1000 | Loss: 0.00001598
Iteration 172/1000 | Loss: 0.00001598
Iteration 173/1000 | Loss: 0.00001598
Iteration 174/1000 | Loss: 0.00001598
Iteration 175/1000 | Loss: 0.00001598
Iteration 176/1000 | Loss: 0.00001598
Iteration 177/1000 | Loss: 0.00001598
Iteration 178/1000 | Loss: 0.00001598
Iteration 179/1000 | Loss: 0.00001598
Iteration 180/1000 | Loss: 0.00001598
Iteration 181/1000 | Loss: 0.00001598
Iteration 182/1000 | Loss: 0.00001598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.5977208022377454e-05, 1.5977208022377454e-05, 1.5977208022377454e-05, 1.5977208022377454e-05, 1.5977208022377454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5977208022377454e-05

Optimization complete. Final v2v error: 3.3675537109375 mm

Highest mean error: 3.5887608528137207 mm for frame 266

Lowest mean error: 3.1524319648742676 mm for frame 14

Saving results

Total time: 45.5001482963562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403381
Iteration 2/25 | Loss: 0.00137808
Iteration 3/25 | Loss: 0.00130913
Iteration 4/25 | Loss: 0.00130615
Iteration 5/25 | Loss: 0.00130615
Iteration 6/25 | Loss: 0.00130615
Iteration 7/25 | Loss: 0.00130615
Iteration 8/25 | Loss: 0.00130615
Iteration 9/25 | Loss: 0.00130615
Iteration 10/25 | Loss: 0.00130615
Iteration 11/25 | Loss: 0.00130615
Iteration 12/25 | Loss: 0.00130615
Iteration 13/25 | Loss: 0.00130615
Iteration 14/25 | Loss: 0.00130615
Iteration 15/25 | Loss: 0.00130615
Iteration 16/25 | Loss: 0.00130615
Iteration 17/25 | Loss: 0.00130615
Iteration 18/25 | Loss: 0.00130615
Iteration 19/25 | Loss: 0.00130615
Iteration 20/25 | Loss: 0.00130615
Iteration 21/25 | Loss: 0.00130615
Iteration 22/25 | Loss: 0.00130615
Iteration 23/25 | Loss: 0.00130615
Iteration 24/25 | Loss: 0.00130615
Iteration 25/25 | Loss: 0.00130615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68793023
Iteration 2/25 | Loss: 0.00080038
Iteration 3/25 | Loss: 0.00080034
Iteration 4/25 | Loss: 0.00080034
Iteration 5/25 | Loss: 0.00080034
Iteration 6/25 | Loss: 0.00080034
Iteration 7/25 | Loss: 0.00080034
Iteration 8/25 | Loss: 0.00080034
Iteration 9/25 | Loss: 0.00080034
Iteration 10/25 | Loss: 0.00080034
Iteration 11/25 | Loss: 0.00080034
Iteration 12/25 | Loss: 0.00080034
Iteration 13/25 | Loss: 0.00080034
Iteration 14/25 | Loss: 0.00080034
Iteration 15/25 | Loss: 0.00080034
Iteration 16/25 | Loss: 0.00080034
Iteration 17/25 | Loss: 0.00080034
Iteration 18/25 | Loss: 0.00080034
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008003419497981668, 0.0008003419497981668, 0.0008003419497981668, 0.0008003419497981668, 0.0008003419497981668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008003419497981668

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080034
Iteration 2/1000 | Loss: 0.00002844
Iteration 3/1000 | Loss: 0.00001913
Iteration 4/1000 | Loss: 0.00001741
Iteration 5/1000 | Loss: 0.00001667
Iteration 6/1000 | Loss: 0.00001609
Iteration 7/1000 | Loss: 0.00001573
Iteration 8/1000 | Loss: 0.00001534
Iteration 9/1000 | Loss: 0.00001502
Iteration 10/1000 | Loss: 0.00001479
Iteration 11/1000 | Loss: 0.00001458
Iteration 12/1000 | Loss: 0.00001439
Iteration 13/1000 | Loss: 0.00001428
Iteration 14/1000 | Loss: 0.00001425
Iteration 15/1000 | Loss: 0.00001419
Iteration 16/1000 | Loss: 0.00001418
Iteration 17/1000 | Loss: 0.00001417
Iteration 18/1000 | Loss: 0.00001416
Iteration 19/1000 | Loss: 0.00001416
Iteration 20/1000 | Loss: 0.00001408
Iteration 21/1000 | Loss: 0.00001407
Iteration 22/1000 | Loss: 0.00001400
Iteration 23/1000 | Loss: 0.00001400
Iteration 24/1000 | Loss: 0.00001398
Iteration 25/1000 | Loss: 0.00001393
Iteration 26/1000 | Loss: 0.00001393
Iteration 27/1000 | Loss: 0.00001391
Iteration 28/1000 | Loss: 0.00001391
Iteration 29/1000 | Loss: 0.00001377
Iteration 30/1000 | Loss: 0.00001371
Iteration 31/1000 | Loss: 0.00001363
Iteration 32/1000 | Loss: 0.00001360
Iteration 33/1000 | Loss: 0.00001359
Iteration 34/1000 | Loss: 0.00001358
Iteration 35/1000 | Loss: 0.00001357
Iteration 36/1000 | Loss: 0.00001357
Iteration 37/1000 | Loss: 0.00001356
Iteration 38/1000 | Loss: 0.00001356
Iteration 39/1000 | Loss: 0.00001355
Iteration 40/1000 | Loss: 0.00001354
Iteration 41/1000 | Loss: 0.00001354
Iteration 42/1000 | Loss: 0.00001353
Iteration 43/1000 | Loss: 0.00001352
Iteration 44/1000 | Loss: 0.00001352
Iteration 45/1000 | Loss: 0.00001351
Iteration 46/1000 | Loss: 0.00001350
Iteration 47/1000 | Loss: 0.00001350
Iteration 48/1000 | Loss: 0.00001348
Iteration 49/1000 | Loss: 0.00001348
Iteration 50/1000 | Loss: 0.00001346
Iteration 51/1000 | Loss: 0.00001345
Iteration 52/1000 | Loss: 0.00001344
Iteration 53/1000 | Loss: 0.00001344
Iteration 54/1000 | Loss: 0.00001343
Iteration 55/1000 | Loss: 0.00001343
Iteration 56/1000 | Loss: 0.00001343
Iteration 57/1000 | Loss: 0.00001343
Iteration 58/1000 | Loss: 0.00001343
Iteration 59/1000 | Loss: 0.00001342
Iteration 60/1000 | Loss: 0.00001342
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001342
Iteration 64/1000 | Loss: 0.00001342
Iteration 65/1000 | Loss: 0.00001341
Iteration 66/1000 | Loss: 0.00001341
Iteration 67/1000 | Loss: 0.00001340
Iteration 68/1000 | Loss: 0.00001340
Iteration 69/1000 | Loss: 0.00001339
Iteration 70/1000 | Loss: 0.00001339
Iteration 71/1000 | Loss: 0.00001339
Iteration 72/1000 | Loss: 0.00001338
Iteration 73/1000 | Loss: 0.00001338
Iteration 74/1000 | Loss: 0.00001338
Iteration 75/1000 | Loss: 0.00001337
Iteration 76/1000 | Loss: 0.00001337
Iteration 77/1000 | Loss: 0.00001337
Iteration 78/1000 | Loss: 0.00001336
Iteration 79/1000 | Loss: 0.00001336
Iteration 80/1000 | Loss: 0.00001336
Iteration 81/1000 | Loss: 0.00001335
Iteration 82/1000 | Loss: 0.00001335
Iteration 83/1000 | Loss: 0.00001335
Iteration 84/1000 | Loss: 0.00001334
Iteration 85/1000 | Loss: 0.00001334
Iteration 86/1000 | Loss: 0.00001334
Iteration 87/1000 | Loss: 0.00001333
Iteration 88/1000 | Loss: 0.00001333
Iteration 89/1000 | Loss: 0.00001333
Iteration 90/1000 | Loss: 0.00001333
Iteration 91/1000 | Loss: 0.00001333
Iteration 92/1000 | Loss: 0.00001333
Iteration 93/1000 | Loss: 0.00001332
Iteration 94/1000 | Loss: 0.00001332
Iteration 95/1000 | Loss: 0.00001332
Iteration 96/1000 | Loss: 0.00001332
Iteration 97/1000 | Loss: 0.00001332
Iteration 98/1000 | Loss: 0.00001332
Iteration 99/1000 | Loss: 0.00001332
Iteration 100/1000 | Loss: 0.00001332
Iteration 101/1000 | Loss: 0.00001331
Iteration 102/1000 | Loss: 0.00001331
Iteration 103/1000 | Loss: 0.00001331
Iteration 104/1000 | Loss: 0.00001330
Iteration 105/1000 | Loss: 0.00001329
Iteration 106/1000 | Loss: 0.00001328
Iteration 107/1000 | Loss: 0.00001328
Iteration 108/1000 | Loss: 0.00001328
Iteration 109/1000 | Loss: 0.00001327
Iteration 110/1000 | Loss: 0.00001327
Iteration 111/1000 | Loss: 0.00001327
Iteration 112/1000 | Loss: 0.00001327
Iteration 113/1000 | Loss: 0.00001327
Iteration 114/1000 | Loss: 0.00001327
Iteration 115/1000 | Loss: 0.00001327
Iteration 116/1000 | Loss: 0.00001327
Iteration 117/1000 | Loss: 0.00001327
Iteration 118/1000 | Loss: 0.00001327
Iteration 119/1000 | Loss: 0.00001327
Iteration 120/1000 | Loss: 0.00001327
Iteration 121/1000 | Loss: 0.00001327
Iteration 122/1000 | Loss: 0.00001327
Iteration 123/1000 | Loss: 0.00001327
Iteration 124/1000 | Loss: 0.00001327
Iteration 125/1000 | Loss: 0.00001327
Iteration 126/1000 | Loss: 0.00001327
Iteration 127/1000 | Loss: 0.00001327
Iteration 128/1000 | Loss: 0.00001327
Iteration 129/1000 | Loss: 0.00001327
Iteration 130/1000 | Loss: 0.00001327
Iteration 131/1000 | Loss: 0.00001327
Iteration 132/1000 | Loss: 0.00001327
Iteration 133/1000 | Loss: 0.00001327
Iteration 134/1000 | Loss: 0.00001327
Iteration 135/1000 | Loss: 0.00001327
Iteration 136/1000 | Loss: 0.00001327
Iteration 137/1000 | Loss: 0.00001327
Iteration 138/1000 | Loss: 0.00001327
Iteration 139/1000 | Loss: 0.00001327
Iteration 140/1000 | Loss: 0.00001327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.3271525858726818e-05, 1.3271525858726818e-05, 1.3271525858726818e-05, 1.3271525858726818e-05, 1.3271525858726818e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3271525858726818e-05

Optimization complete. Final v2v error: 3.1095147132873535 mm

Highest mean error: 3.3430123329162598 mm for frame 2

Lowest mean error: 2.968618869781494 mm for frame 43

Saving results

Total time: 45.47473669052124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805663
Iteration 2/25 | Loss: 0.00138584
Iteration 3/25 | Loss: 0.00129798
Iteration 4/25 | Loss: 0.00128869
Iteration 5/25 | Loss: 0.00128600
Iteration 6/25 | Loss: 0.00128596
Iteration 7/25 | Loss: 0.00128596
Iteration 8/25 | Loss: 0.00128596
Iteration 9/25 | Loss: 0.00128596
Iteration 10/25 | Loss: 0.00128596
Iteration 11/25 | Loss: 0.00128596
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012859618291258812, 0.0012859618291258812, 0.0012859618291258812, 0.0012859618291258812, 0.0012859618291258812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012859618291258812

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39413035
Iteration 2/25 | Loss: 0.00091525
Iteration 3/25 | Loss: 0.00091525
Iteration 4/25 | Loss: 0.00091525
Iteration 5/25 | Loss: 0.00091525
Iteration 6/25 | Loss: 0.00091525
Iteration 7/25 | Loss: 0.00091525
Iteration 8/25 | Loss: 0.00091525
Iteration 9/25 | Loss: 0.00091524
Iteration 10/25 | Loss: 0.00091524
Iteration 11/25 | Loss: 0.00091524
Iteration 12/25 | Loss: 0.00091524
Iteration 13/25 | Loss: 0.00091524
Iteration 14/25 | Loss: 0.00091524
Iteration 15/25 | Loss: 0.00091524
Iteration 16/25 | Loss: 0.00091524
Iteration 17/25 | Loss: 0.00091524
Iteration 18/25 | Loss: 0.00091524
Iteration 19/25 | Loss: 0.00091524
Iteration 20/25 | Loss: 0.00091524
Iteration 21/25 | Loss: 0.00091524
Iteration 22/25 | Loss: 0.00091524
Iteration 23/25 | Loss: 0.00091524
Iteration 24/25 | Loss: 0.00091524
Iteration 25/25 | Loss: 0.00091524

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091524
Iteration 2/1000 | Loss: 0.00003058
Iteration 3/1000 | Loss: 0.00001985
Iteration 4/1000 | Loss: 0.00001729
Iteration 5/1000 | Loss: 0.00001601
Iteration 6/1000 | Loss: 0.00001521
Iteration 7/1000 | Loss: 0.00001445
Iteration 8/1000 | Loss: 0.00001412
Iteration 9/1000 | Loss: 0.00001379
Iteration 10/1000 | Loss: 0.00001351
Iteration 11/1000 | Loss: 0.00001345
Iteration 12/1000 | Loss: 0.00001336
Iteration 13/1000 | Loss: 0.00001336
Iteration 14/1000 | Loss: 0.00001323
Iteration 15/1000 | Loss: 0.00001318
Iteration 16/1000 | Loss: 0.00001317
Iteration 17/1000 | Loss: 0.00001317
Iteration 18/1000 | Loss: 0.00001316
Iteration 19/1000 | Loss: 0.00001316
Iteration 20/1000 | Loss: 0.00001315
Iteration 21/1000 | Loss: 0.00001315
Iteration 22/1000 | Loss: 0.00001315
Iteration 23/1000 | Loss: 0.00001313
Iteration 24/1000 | Loss: 0.00001312
Iteration 25/1000 | Loss: 0.00001310
Iteration 26/1000 | Loss: 0.00001310
Iteration 27/1000 | Loss: 0.00001310
Iteration 28/1000 | Loss: 0.00001310
Iteration 29/1000 | Loss: 0.00001310
Iteration 30/1000 | Loss: 0.00001310
Iteration 31/1000 | Loss: 0.00001310
Iteration 32/1000 | Loss: 0.00001310
Iteration 33/1000 | Loss: 0.00001310
Iteration 34/1000 | Loss: 0.00001310
Iteration 35/1000 | Loss: 0.00001310
Iteration 36/1000 | Loss: 0.00001309
Iteration 37/1000 | Loss: 0.00001309
Iteration 38/1000 | Loss: 0.00001309
Iteration 39/1000 | Loss: 0.00001308
Iteration 40/1000 | Loss: 0.00001308
Iteration 41/1000 | Loss: 0.00001305
Iteration 42/1000 | Loss: 0.00001305
Iteration 43/1000 | Loss: 0.00001305
Iteration 44/1000 | Loss: 0.00001304
Iteration 45/1000 | Loss: 0.00001304
Iteration 46/1000 | Loss: 0.00001303
Iteration 47/1000 | Loss: 0.00001303
Iteration 48/1000 | Loss: 0.00001302
Iteration 49/1000 | Loss: 0.00001301
Iteration 50/1000 | Loss: 0.00001301
Iteration 51/1000 | Loss: 0.00001300
Iteration 52/1000 | Loss: 0.00001300
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001299
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00001298
Iteration 57/1000 | Loss: 0.00001298
Iteration 58/1000 | Loss: 0.00001298
Iteration 59/1000 | Loss: 0.00001297
Iteration 60/1000 | Loss: 0.00001297
Iteration 61/1000 | Loss: 0.00001296
Iteration 62/1000 | Loss: 0.00001296
Iteration 63/1000 | Loss: 0.00001296
Iteration 64/1000 | Loss: 0.00001295
Iteration 65/1000 | Loss: 0.00001295
Iteration 66/1000 | Loss: 0.00001295
Iteration 67/1000 | Loss: 0.00001295
Iteration 68/1000 | Loss: 0.00001294
Iteration 69/1000 | Loss: 0.00001294
Iteration 70/1000 | Loss: 0.00001294
Iteration 71/1000 | Loss: 0.00001293
Iteration 72/1000 | Loss: 0.00001293
Iteration 73/1000 | Loss: 0.00001292
Iteration 74/1000 | Loss: 0.00001291
Iteration 75/1000 | Loss: 0.00001291
Iteration 76/1000 | Loss: 0.00001290
Iteration 77/1000 | Loss: 0.00001290
Iteration 78/1000 | Loss: 0.00001289
Iteration 79/1000 | Loss: 0.00001289
Iteration 80/1000 | Loss: 0.00001289
Iteration 81/1000 | Loss: 0.00001288
Iteration 82/1000 | Loss: 0.00001287
Iteration 83/1000 | Loss: 0.00001287
Iteration 84/1000 | Loss: 0.00001287
Iteration 85/1000 | Loss: 0.00001286
Iteration 86/1000 | Loss: 0.00001286
Iteration 87/1000 | Loss: 0.00001286
Iteration 88/1000 | Loss: 0.00001286
Iteration 89/1000 | Loss: 0.00001286
Iteration 90/1000 | Loss: 0.00001286
Iteration 91/1000 | Loss: 0.00001286
Iteration 92/1000 | Loss: 0.00001286
Iteration 93/1000 | Loss: 0.00001285
Iteration 94/1000 | Loss: 0.00001285
Iteration 95/1000 | Loss: 0.00001285
Iteration 96/1000 | Loss: 0.00001284
Iteration 97/1000 | Loss: 0.00001284
Iteration 98/1000 | Loss: 0.00001284
Iteration 99/1000 | Loss: 0.00001284
Iteration 100/1000 | Loss: 0.00001284
Iteration 101/1000 | Loss: 0.00001283
Iteration 102/1000 | Loss: 0.00001283
Iteration 103/1000 | Loss: 0.00001283
Iteration 104/1000 | Loss: 0.00001283
Iteration 105/1000 | Loss: 0.00001283
Iteration 106/1000 | Loss: 0.00001283
Iteration 107/1000 | Loss: 0.00001282
Iteration 108/1000 | Loss: 0.00001282
Iteration 109/1000 | Loss: 0.00001282
Iteration 110/1000 | Loss: 0.00001282
Iteration 111/1000 | Loss: 0.00001282
Iteration 112/1000 | Loss: 0.00001282
Iteration 113/1000 | Loss: 0.00001281
Iteration 114/1000 | Loss: 0.00001281
Iteration 115/1000 | Loss: 0.00001281
Iteration 116/1000 | Loss: 0.00001281
Iteration 117/1000 | Loss: 0.00001281
Iteration 118/1000 | Loss: 0.00001280
Iteration 119/1000 | Loss: 0.00001280
Iteration 120/1000 | Loss: 0.00001280
Iteration 121/1000 | Loss: 0.00001279
Iteration 122/1000 | Loss: 0.00001279
Iteration 123/1000 | Loss: 0.00001279
Iteration 124/1000 | Loss: 0.00001278
Iteration 125/1000 | Loss: 0.00001278
Iteration 126/1000 | Loss: 0.00001278
Iteration 127/1000 | Loss: 0.00001278
Iteration 128/1000 | Loss: 0.00001278
Iteration 129/1000 | Loss: 0.00001278
Iteration 130/1000 | Loss: 0.00001277
Iteration 131/1000 | Loss: 0.00001277
Iteration 132/1000 | Loss: 0.00001277
Iteration 133/1000 | Loss: 0.00001277
Iteration 134/1000 | Loss: 0.00001277
Iteration 135/1000 | Loss: 0.00001276
Iteration 136/1000 | Loss: 0.00001276
Iteration 137/1000 | Loss: 0.00001276
Iteration 138/1000 | Loss: 0.00001275
Iteration 139/1000 | Loss: 0.00001275
Iteration 140/1000 | Loss: 0.00001275
Iteration 141/1000 | Loss: 0.00001275
Iteration 142/1000 | Loss: 0.00001275
Iteration 143/1000 | Loss: 0.00001275
Iteration 144/1000 | Loss: 0.00001275
Iteration 145/1000 | Loss: 0.00001275
Iteration 146/1000 | Loss: 0.00001274
Iteration 147/1000 | Loss: 0.00001274
Iteration 148/1000 | Loss: 0.00001274
Iteration 149/1000 | Loss: 0.00001274
Iteration 150/1000 | Loss: 0.00001274
Iteration 151/1000 | Loss: 0.00001274
Iteration 152/1000 | Loss: 0.00001273
Iteration 153/1000 | Loss: 0.00001273
Iteration 154/1000 | Loss: 0.00001273
Iteration 155/1000 | Loss: 0.00001272
Iteration 156/1000 | Loss: 0.00001272
Iteration 157/1000 | Loss: 0.00001272
Iteration 158/1000 | Loss: 0.00001272
Iteration 159/1000 | Loss: 0.00001272
Iteration 160/1000 | Loss: 0.00001272
Iteration 161/1000 | Loss: 0.00001272
Iteration 162/1000 | Loss: 0.00001272
Iteration 163/1000 | Loss: 0.00001272
Iteration 164/1000 | Loss: 0.00001272
Iteration 165/1000 | Loss: 0.00001272
Iteration 166/1000 | Loss: 0.00001272
Iteration 167/1000 | Loss: 0.00001272
Iteration 168/1000 | Loss: 0.00001271
Iteration 169/1000 | Loss: 0.00001271
Iteration 170/1000 | Loss: 0.00001271
Iteration 171/1000 | Loss: 0.00001271
Iteration 172/1000 | Loss: 0.00001271
Iteration 173/1000 | Loss: 0.00001271
Iteration 174/1000 | Loss: 0.00001271
Iteration 175/1000 | Loss: 0.00001271
Iteration 176/1000 | Loss: 0.00001271
Iteration 177/1000 | Loss: 0.00001271
Iteration 178/1000 | Loss: 0.00001271
Iteration 179/1000 | Loss: 0.00001271
Iteration 180/1000 | Loss: 0.00001271
Iteration 181/1000 | Loss: 0.00001270
Iteration 182/1000 | Loss: 0.00001270
Iteration 183/1000 | Loss: 0.00001270
Iteration 184/1000 | Loss: 0.00001270
Iteration 185/1000 | Loss: 0.00001270
Iteration 186/1000 | Loss: 0.00001270
Iteration 187/1000 | Loss: 0.00001270
Iteration 188/1000 | Loss: 0.00001270
Iteration 189/1000 | Loss: 0.00001270
Iteration 190/1000 | Loss: 0.00001270
Iteration 191/1000 | Loss: 0.00001269
Iteration 192/1000 | Loss: 0.00001269
Iteration 193/1000 | Loss: 0.00001269
Iteration 194/1000 | Loss: 0.00001269
Iteration 195/1000 | Loss: 0.00001269
Iteration 196/1000 | Loss: 0.00001269
Iteration 197/1000 | Loss: 0.00001269
Iteration 198/1000 | Loss: 0.00001269
Iteration 199/1000 | Loss: 0.00001269
Iteration 200/1000 | Loss: 0.00001269
Iteration 201/1000 | Loss: 0.00001269
Iteration 202/1000 | Loss: 0.00001269
Iteration 203/1000 | Loss: 0.00001269
Iteration 204/1000 | Loss: 0.00001269
Iteration 205/1000 | Loss: 0.00001269
Iteration 206/1000 | Loss: 0.00001269
Iteration 207/1000 | Loss: 0.00001268
Iteration 208/1000 | Loss: 0.00001268
Iteration 209/1000 | Loss: 0.00001268
Iteration 210/1000 | Loss: 0.00001268
Iteration 211/1000 | Loss: 0.00001268
Iteration 212/1000 | Loss: 0.00001268
Iteration 213/1000 | Loss: 0.00001268
Iteration 214/1000 | Loss: 0.00001268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.2684934517892543e-05, 1.2684934517892543e-05, 1.2684934517892543e-05, 1.2684934517892543e-05, 1.2684934517892543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2684934517892543e-05

Optimization complete. Final v2v error: 3.0228254795074463 mm

Highest mean error: 3.9975364208221436 mm for frame 65

Lowest mean error: 2.723494052886963 mm for frame 186

Saving results

Total time: 40.60732293128967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996307
Iteration 2/25 | Loss: 0.00262283
Iteration 3/25 | Loss: 0.00194270
Iteration 4/25 | Loss: 0.00182688
Iteration 5/25 | Loss: 0.00162042
Iteration 6/25 | Loss: 0.00155486
Iteration 7/25 | Loss: 0.00152557
Iteration 8/25 | Loss: 0.00150459
Iteration 9/25 | Loss: 0.00150544
Iteration 10/25 | Loss: 0.00150869
Iteration 11/25 | Loss: 0.00148183
Iteration 12/25 | Loss: 0.00147341
Iteration 13/25 | Loss: 0.00148260
Iteration 14/25 | Loss: 0.00147542
Iteration 15/25 | Loss: 0.00146865
Iteration 16/25 | Loss: 0.00146704
Iteration 17/25 | Loss: 0.00147022
Iteration 18/25 | Loss: 0.00147045
Iteration 19/25 | Loss: 0.00147403
Iteration 20/25 | Loss: 0.00146462
Iteration 21/25 | Loss: 0.00146306
Iteration 22/25 | Loss: 0.00147090
Iteration 23/25 | Loss: 0.00146386
Iteration 24/25 | Loss: 0.00146343
Iteration 25/25 | Loss: 0.00146150

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43471801
Iteration 2/25 | Loss: 0.00157948
Iteration 3/25 | Loss: 0.00149940
Iteration 4/25 | Loss: 0.00149940
Iteration 5/25 | Loss: 0.00149940
Iteration 6/25 | Loss: 0.00149940
Iteration 7/25 | Loss: 0.00149940
Iteration 8/25 | Loss: 0.00149940
Iteration 9/25 | Loss: 0.00149940
Iteration 10/25 | Loss: 0.00149940
Iteration 11/25 | Loss: 0.00149940
Iteration 12/25 | Loss: 0.00149940
Iteration 13/25 | Loss: 0.00149940
Iteration 14/25 | Loss: 0.00149940
Iteration 15/25 | Loss: 0.00149940
Iteration 16/25 | Loss: 0.00149940
Iteration 17/25 | Loss: 0.00149940
Iteration 18/25 | Loss: 0.00149940
Iteration 19/25 | Loss: 0.00149940
Iteration 20/25 | Loss: 0.00149940
Iteration 21/25 | Loss: 0.00149940
Iteration 22/25 | Loss: 0.00149940
Iteration 23/25 | Loss: 0.00149940
Iteration 24/25 | Loss: 0.00149940
Iteration 25/25 | Loss: 0.00149940

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149940
Iteration 2/1000 | Loss: 0.00018628
Iteration 3/1000 | Loss: 0.00025549
Iteration 4/1000 | Loss: 0.00009872
Iteration 5/1000 | Loss: 0.00017404
Iteration 6/1000 | Loss: 0.00012146
Iteration 7/1000 | Loss: 0.00026163
Iteration 8/1000 | Loss: 0.00204756
Iteration 9/1000 | Loss: 0.00010172
Iteration 10/1000 | Loss: 0.00037788
Iteration 11/1000 | Loss: 0.00013233
Iteration 12/1000 | Loss: 0.00006551
Iteration 13/1000 | Loss: 0.00011134
Iteration 14/1000 | Loss: 0.00006257
Iteration 15/1000 | Loss: 0.00019097
Iteration 16/1000 | Loss: 0.00009526
Iteration 17/1000 | Loss: 0.00011996
Iteration 18/1000 | Loss: 0.00006049
Iteration 19/1000 | Loss: 0.00009130
Iteration 20/1000 | Loss: 0.00005990
Iteration 21/1000 | Loss: 0.00005929
Iteration 22/1000 | Loss: 0.00025085
Iteration 23/1000 | Loss: 0.00528541
Iteration 24/1000 | Loss: 0.00151823
Iteration 25/1000 | Loss: 0.00045718
Iteration 26/1000 | Loss: 0.00031248
Iteration 27/1000 | Loss: 0.00069886
Iteration 28/1000 | Loss: 0.00010073
Iteration 29/1000 | Loss: 0.00019598
Iteration 30/1000 | Loss: 0.00047447
Iteration 31/1000 | Loss: 0.00121568
Iteration 32/1000 | Loss: 0.00031794
Iteration 33/1000 | Loss: 0.00028502
Iteration 34/1000 | Loss: 0.00016150
Iteration 35/1000 | Loss: 0.00010644
Iteration 36/1000 | Loss: 0.00036507
Iteration 37/1000 | Loss: 0.00003867
Iteration 38/1000 | Loss: 0.00009620
Iteration 39/1000 | Loss: 0.00003171
Iteration 40/1000 | Loss: 0.00006936
Iteration 41/1000 | Loss: 0.00002688
Iteration 42/1000 | Loss: 0.00024614
Iteration 43/1000 | Loss: 0.00007538
Iteration 44/1000 | Loss: 0.00036115
Iteration 45/1000 | Loss: 0.00010632
Iteration 46/1000 | Loss: 0.00047384
Iteration 47/1000 | Loss: 0.00015043
Iteration 48/1000 | Loss: 0.00004251
Iteration 49/1000 | Loss: 0.00004020
Iteration 50/1000 | Loss: 0.00004389
Iteration 51/1000 | Loss: 0.00003425
Iteration 52/1000 | Loss: 0.00018224
Iteration 53/1000 | Loss: 0.00010093
Iteration 54/1000 | Loss: 0.00003134
Iteration 55/1000 | Loss: 0.00003398
Iteration 56/1000 | Loss: 0.00004498
Iteration 57/1000 | Loss: 0.00002063
Iteration 58/1000 | Loss: 0.00005637
Iteration 59/1000 | Loss: 0.00023137
Iteration 60/1000 | Loss: 0.00005520
Iteration 61/1000 | Loss: 0.00003844
Iteration 62/1000 | Loss: 0.00001996
Iteration 63/1000 | Loss: 0.00008659
Iteration 64/1000 | Loss: 0.00002445
Iteration 65/1000 | Loss: 0.00001967
Iteration 66/1000 | Loss: 0.00006954
Iteration 67/1000 | Loss: 0.00011942
Iteration 68/1000 | Loss: 0.00003972
Iteration 69/1000 | Loss: 0.00004569
Iteration 70/1000 | Loss: 0.00006710
Iteration 71/1000 | Loss: 0.00020337
Iteration 72/1000 | Loss: 0.00026200
Iteration 73/1000 | Loss: 0.00002084
Iteration 74/1000 | Loss: 0.00001945
Iteration 75/1000 | Loss: 0.00001934
Iteration 76/1000 | Loss: 0.00001934
Iteration 77/1000 | Loss: 0.00001933
Iteration 78/1000 | Loss: 0.00001933
Iteration 79/1000 | Loss: 0.00001933
Iteration 80/1000 | Loss: 0.00001933
Iteration 81/1000 | Loss: 0.00001933
Iteration 82/1000 | Loss: 0.00001932
Iteration 83/1000 | Loss: 0.00001932
Iteration 84/1000 | Loss: 0.00001932
Iteration 85/1000 | Loss: 0.00001932
Iteration 86/1000 | Loss: 0.00001932
Iteration 87/1000 | Loss: 0.00001932
Iteration 88/1000 | Loss: 0.00001932
Iteration 89/1000 | Loss: 0.00001932
Iteration 90/1000 | Loss: 0.00001932
Iteration 91/1000 | Loss: 0.00001931
Iteration 92/1000 | Loss: 0.00001931
Iteration 93/1000 | Loss: 0.00001931
Iteration 94/1000 | Loss: 0.00001931
Iteration 95/1000 | Loss: 0.00001931
Iteration 96/1000 | Loss: 0.00001931
Iteration 97/1000 | Loss: 0.00001931
Iteration 98/1000 | Loss: 0.00001931
Iteration 99/1000 | Loss: 0.00001930
Iteration 100/1000 | Loss: 0.00001930
Iteration 101/1000 | Loss: 0.00001930
Iteration 102/1000 | Loss: 0.00001930
Iteration 103/1000 | Loss: 0.00001930
Iteration 104/1000 | Loss: 0.00001930
Iteration 105/1000 | Loss: 0.00001930
Iteration 106/1000 | Loss: 0.00001930
Iteration 107/1000 | Loss: 0.00001929
Iteration 108/1000 | Loss: 0.00001929
Iteration 109/1000 | Loss: 0.00001929
Iteration 110/1000 | Loss: 0.00001928
Iteration 111/1000 | Loss: 0.00001928
Iteration 112/1000 | Loss: 0.00001928
Iteration 113/1000 | Loss: 0.00001928
Iteration 114/1000 | Loss: 0.00001927
Iteration 115/1000 | Loss: 0.00001927
Iteration 116/1000 | Loss: 0.00001927
Iteration 117/1000 | Loss: 0.00001927
Iteration 118/1000 | Loss: 0.00001927
Iteration 119/1000 | Loss: 0.00001927
Iteration 120/1000 | Loss: 0.00001927
Iteration 121/1000 | Loss: 0.00001927
Iteration 122/1000 | Loss: 0.00001927
Iteration 123/1000 | Loss: 0.00001927
Iteration 124/1000 | Loss: 0.00001927
Iteration 125/1000 | Loss: 0.00001927
Iteration 126/1000 | Loss: 0.00001927
Iteration 127/1000 | Loss: 0.00001927
Iteration 128/1000 | Loss: 0.00001927
Iteration 129/1000 | Loss: 0.00001927
Iteration 130/1000 | Loss: 0.00001927
Iteration 131/1000 | Loss: 0.00001927
Iteration 132/1000 | Loss: 0.00001927
Iteration 133/1000 | Loss: 0.00001927
Iteration 134/1000 | Loss: 0.00001927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.927096127474215e-05, 1.927096127474215e-05, 1.927096127474215e-05, 1.927096127474215e-05, 1.927096127474215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.927096127474215e-05

Optimization complete. Final v2v error: 3.667327404022217 mm

Highest mean error: 4.00479793548584 mm for frame 81

Lowest mean error: 3.2729055881500244 mm for frame 66

Saving results

Total time: 144.95772099494934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998920
Iteration 2/25 | Loss: 0.00208409
Iteration 3/25 | Loss: 0.00163401
Iteration 4/25 | Loss: 0.00140582
Iteration 5/25 | Loss: 0.00138201
Iteration 6/25 | Loss: 0.00138015
Iteration 7/25 | Loss: 0.00138460
Iteration 8/25 | Loss: 0.00138946
Iteration 9/25 | Loss: 0.00137228
Iteration 10/25 | Loss: 0.00135717
Iteration 11/25 | Loss: 0.00135577
Iteration 12/25 | Loss: 0.00135527
Iteration 13/25 | Loss: 0.00135039
Iteration 14/25 | Loss: 0.00134648
Iteration 15/25 | Loss: 0.00134481
Iteration 16/25 | Loss: 0.00134433
Iteration 17/25 | Loss: 0.00134418
Iteration 18/25 | Loss: 0.00134411
Iteration 19/25 | Loss: 0.00134411
Iteration 20/25 | Loss: 0.00134410
Iteration 21/25 | Loss: 0.00134410
Iteration 22/25 | Loss: 0.00134410
Iteration 23/25 | Loss: 0.00134410
Iteration 24/25 | Loss: 0.00134410
Iteration 25/25 | Loss: 0.00134410

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51720905
Iteration 2/25 | Loss: 0.00109223
Iteration 3/25 | Loss: 0.00109223
Iteration 4/25 | Loss: 0.00109223
Iteration 5/25 | Loss: 0.00109223
Iteration 6/25 | Loss: 0.00109223
Iteration 7/25 | Loss: 0.00109223
Iteration 8/25 | Loss: 0.00109223
Iteration 9/25 | Loss: 0.00109223
Iteration 10/25 | Loss: 0.00109223
Iteration 11/25 | Loss: 0.00109223
Iteration 12/25 | Loss: 0.00109223
Iteration 13/25 | Loss: 0.00109223
Iteration 14/25 | Loss: 0.00109223
Iteration 15/25 | Loss: 0.00109223
Iteration 16/25 | Loss: 0.00109223
Iteration 17/25 | Loss: 0.00109223
Iteration 18/25 | Loss: 0.00109223
Iteration 19/25 | Loss: 0.00109223
Iteration 20/25 | Loss: 0.00109223
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010922268265858293, 0.0010922268265858293, 0.0010922268265858293, 0.0010922268265858293, 0.0010922268265858293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010922268265858293

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109223
Iteration 2/1000 | Loss: 0.00003835
Iteration 3/1000 | Loss: 0.00002785
Iteration 4/1000 | Loss: 0.00002520
Iteration 5/1000 | Loss: 0.00023146
Iteration 6/1000 | Loss: 0.00017837
Iteration 7/1000 | Loss: 0.00003082
Iteration 8/1000 | Loss: 0.00020969
Iteration 9/1000 | Loss: 0.00012930
Iteration 10/1000 | Loss: 0.00009665
Iteration 11/1000 | Loss: 0.00015324
Iteration 12/1000 | Loss: 0.00008365
Iteration 13/1000 | Loss: 0.00013930
Iteration 14/1000 | Loss: 0.00015131
Iteration 15/1000 | Loss: 0.00009785
Iteration 16/1000 | Loss: 0.00023620
Iteration 17/1000 | Loss: 0.00010543
Iteration 18/1000 | Loss: 0.00019400
Iteration 19/1000 | Loss: 0.00004700
Iteration 20/1000 | Loss: 0.00012722
Iteration 21/1000 | Loss: 0.00004939
Iteration 22/1000 | Loss: 0.00012021
Iteration 23/1000 | Loss: 0.00005053
Iteration 24/1000 | Loss: 0.00011424
Iteration 25/1000 | Loss: 0.00005573
Iteration 26/1000 | Loss: 0.00014549
Iteration 27/1000 | Loss: 0.00009369
Iteration 28/1000 | Loss: 0.00013707
Iteration 29/1000 | Loss: 0.00008733
Iteration 30/1000 | Loss: 0.00005490
Iteration 31/1000 | Loss: 0.00005540
Iteration 32/1000 | Loss: 0.00002679
Iteration 33/1000 | Loss: 0.00002463
Iteration 34/1000 | Loss: 0.00002325
Iteration 35/1000 | Loss: 0.00002267
Iteration 36/1000 | Loss: 0.00014917
Iteration 37/1000 | Loss: 0.00014676
Iteration 38/1000 | Loss: 0.00002331
Iteration 39/1000 | Loss: 0.00014942
Iteration 40/1000 | Loss: 0.00012552
Iteration 41/1000 | Loss: 0.00002284
Iteration 42/1000 | Loss: 0.00015140
Iteration 43/1000 | Loss: 0.00003017
Iteration 44/1000 | Loss: 0.00002448
Iteration 45/1000 | Loss: 0.00002309
Iteration 46/1000 | Loss: 0.00002177
Iteration 47/1000 | Loss: 0.00002137
Iteration 48/1000 | Loss: 0.00002070
Iteration 49/1000 | Loss: 0.00001960
Iteration 50/1000 | Loss: 0.00001915
Iteration 51/1000 | Loss: 0.00001898
Iteration 52/1000 | Loss: 0.00001898
Iteration 53/1000 | Loss: 0.00001896
Iteration 54/1000 | Loss: 0.00001896
Iteration 55/1000 | Loss: 0.00001894
Iteration 56/1000 | Loss: 0.00001894
Iteration 57/1000 | Loss: 0.00001894
Iteration 58/1000 | Loss: 0.00001894
Iteration 59/1000 | Loss: 0.00001893
Iteration 60/1000 | Loss: 0.00001891
Iteration 61/1000 | Loss: 0.00001890
Iteration 62/1000 | Loss: 0.00001890
Iteration 63/1000 | Loss: 0.00001890
Iteration 64/1000 | Loss: 0.00001890
Iteration 65/1000 | Loss: 0.00001890
Iteration 66/1000 | Loss: 0.00001890
Iteration 67/1000 | Loss: 0.00001889
Iteration 68/1000 | Loss: 0.00001889
Iteration 69/1000 | Loss: 0.00001888
Iteration 70/1000 | Loss: 0.00001888
Iteration 71/1000 | Loss: 0.00001888
Iteration 72/1000 | Loss: 0.00001888
Iteration 73/1000 | Loss: 0.00001888
Iteration 74/1000 | Loss: 0.00001888
Iteration 75/1000 | Loss: 0.00001888
Iteration 76/1000 | Loss: 0.00001887
Iteration 77/1000 | Loss: 0.00001887
Iteration 78/1000 | Loss: 0.00001887
Iteration 79/1000 | Loss: 0.00001887
Iteration 80/1000 | Loss: 0.00001887
Iteration 81/1000 | Loss: 0.00001887
Iteration 82/1000 | Loss: 0.00001887
Iteration 83/1000 | Loss: 0.00001887
Iteration 84/1000 | Loss: 0.00001887
Iteration 85/1000 | Loss: 0.00001887
Iteration 86/1000 | Loss: 0.00001887
Iteration 87/1000 | Loss: 0.00001887
Iteration 88/1000 | Loss: 0.00001887
Iteration 89/1000 | Loss: 0.00001887
Iteration 90/1000 | Loss: 0.00001887
Iteration 91/1000 | Loss: 0.00001887
Iteration 92/1000 | Loss: 0.00001887
Iteration 93/1000 | Loss: 0.00001887
Iteration 94/1000 | Loss: 0.00001887
Iteration 95/1000 | Loss: 0.00001887
Iteration 96/1000 | Loss: 0.00001887
Iteration 97/1000 | Loss: 0.00001887
Iteration 98/1000 | Loss: 0.00001887
Iteration 99/1000 | Loss: 0.00001887
Iteration 100/1000 | Loss: 0.00001887
Iteration 101/1000 | Loss: 0.00001887
Iteration 102/1000 | Loss: 0.00001887
Iteration 103/1000 | Loss: 0.00001887
Iteration 104/1000 | Loss: 0.00001887
Iteration 105/1000 | Loss: 0.00001887
Iteration 106/1000 | Loss: 0.00001887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.8868271581595764e-05, 1.8868271581595764e-05, 1.8868271581595764e-05, 1.8868271581595764e-05, 1.8868271581595764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8868271581595764e-05

Optimization complete. Final v2v error: 3.7296783924102783 mm

Highest mean error: 5.435908317565918 mm for frame 134

Lowest mean error: 3.2560858726501465 mm for frame 200

Saving results

Total time: 110.5911808013916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473192
Iteration 2/25 | Loss: 0.00151416
Iteration 3/25 | Loss: 0.00132656
Iteration 4/25 | Loss: 0.00130711
Iteration 5/25 | Loss: 0.00130430
Iteration 6/25 | Loss: 0.00130388
Iteration 7/25 | Loss: 0.00130388
Iteration 8/25 | Loss: 0.00130388
Iteration 9/25 | Loss: 0.00130388
Iteration 10/25 | Loss: 0.00130388
Iteration 11/25 | Loss: 0.00130388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013038788456469774, 0.0013038788456469774, 0.0013038788456469774, 0.0013038788456469774, 0.0013038788456469774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013038788456469774

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40050662
Iteration 2/25 | Loss: 0.00080221
Iteration 3/25 | Loss: 0.00080221
Iteration 4/25 | Loss: 0.00080220
Iteration 5/25 | Loss: 0.00080220
Iteration 6/25 | Loss: 0.00080220
Iteration 7/25 | Loss: 0.00080220
Iteration 8/25 | Loss: 0.00080220
Iteration 9/25 | Loss: 0.00080220
Iteration 10/25 | Loss: 0.00080220
Iteration 11/25 | Loss: 0.00080220
Iteration 12/25 | Loss: 0.00080220
Iteration 13/25 | Loss: 0.00080220
Iteration 14/25 | Loss: 0.00080220
Iteration 15/25 | Loss: 0.00080220
Iteration 16/25 | Loss: 0.00080220
Iteration 17/25 | Loss: 0.00080220
Iteration 18/25 | Loss: 0.00080220
Iteration 19/25 | Loss: 0.00080220
Iteration 20/25 | Loss: 0.00080220
Iteration 21/25 | Loss: 0.00080220
Iteration 22/25 | Loss: 0.00080220
Iteration 23/25 | Loss: 0.00080220
Iteration 24/25 | Loss: 0.00080220
Iteration 25/25 | Loss: 0.00080220

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080220
Iteration 2/1000 | Loss: 0.00003227
Iteration 3/1000 | Loss: 0.00002164
Iteration 4/1000 | Loss: 0.00001809
Iteration 5/1000 | Loss: 0.00001672
Iteration 6/1000 | Loss: 0.00001589
Iteration 7/1000 | Loss: 0.00001526
Iteration 8/1000 | Loss: 0.00001476
Iteration 9/1000 | Loss: 0.00001455
Iteration 10/1000 | Loss: 0.00001429
Iteration 11/1000 | Loss: 0.00001403
Iteration 12/1000 | Loss: 0.00001402
Iteration 13/1000 | Loss: 0.00001395
Iteration 14/1000 | Loss: 0.00001387
Iteration 15/1000 | Loss: 0.00001384
Iteration 16/1000 | Loss: 0.00001383
Iteration 17/1000 | Loss: 0.00001380
Iteration 18/1000 | Loss: 0.00001376
Iteration 19/1000 | Loss: 0.00001374
Iteration 20/1000 | Loss: 0.00001366
Iteration 21/1000 | Loss: 0.00001363
Iteration 22/1000 | Loss: 0.00001362
Iteration 23/1000 | Loss: 0.00001362
Iteration 24/1000 | Loss: 0.00001362
Iteration 25/1000 | Loss: 0.00001362
Iteration 26/1000 | Loss: 0.00001361
Iteration 27/1000 | Loss: 0.00001360
Iteration 28/1000 | Loss: 0.00001359
Iteration 29/1000 | Loss: 0.00001358
Iteration 30/1000 | Loss: 0.00001358
Iteration 31/1000 | Loss: 0.00001358
Iteration 32/1000 | Loss: 0.00001358
Iteration 33/1000 | Loss: 0.00001357
Iteration 34/1000 | Loss: 0.00001357
Iteration 35/1000 | Loss: 0.00001357
Iteration 36/1000 | Loss: 0.00001356
Iteration 37/1000 | Loss: 0.00001356
Iteration 38/1000 | Loss: 0.00001356
Iteration 39/1000 | Loss: 0.00001355
Iteration 40/1000 | Loss: 0.00001355
Iteration 41/1000 | Loss: 0.00001354
Iteration 42/1000 | Loss: 0.00001354
Iteration 43/1000 | Loss: 0.00001354
Iteration 44/1000 | Loss: 0.00001354
Iteration 45/1000 | Loss: 0.00001353
Iteration 46/1000 | Loss: 0.00001353
Iteration 47/1000 | Loss: 0.00001353
Iteration 48/1000 | Loss: 0.00001351
Iteration 49/1000 | Loss: 0.00001350
Iteration 50/1000 | Loss: 0.00001350
Iteration 51/1000 | Loss: 0.00001349
Iteration 52/1000 | Loss: 0.00001349
Iteration 53/1000 | Loss: 0.00001348
Iteration 54/1000 | Loss: 0.00001347
Iteration 55/1000 | Loss: 0.00001346
Iteration 56/1000 | Loss: 0.00001346
Iteration 57/1000 | Loss: 0.00001346
Iteration 58/1000 | Loss: 0.00001345
Iteration 59/1000 | Loss: 0.00001345
Iteration 60/1000 | Loss: 0.00001345
Iteration 61/1000 | Loss: 0.00001345
Iteration 62/1000 | Loss: 0.00001344
Iteration 63/1000 | Loss: 0.00001344
Iteration 64/1000 | Loss: 0.00001344
Iteration 65/1000 | Loss: 0.00001344
Iteration 66/1000 | Loss: 0.00001343
Iteration 67/1000 | Loss: 0.00001343
Iteration 68/1000 | Loss: 0.00001342
Iteration 69/1000 | Loss: 0.00001341
Iteration 70/1000 | Loss: 0.00001341
Iteration 71/1000 | Loss: 0.00001341
Iteration 72/1000 | Loss: 0.00001340
Iteration 73/1000 | Loss: 0.00001340
Iteration 74/1000 | Loss: 0.00001340
Iteration 75/1000 | Loss: 0.00001340
Iteration 76/1000 | Loss: 0.00001340
Iteration 77/1000 | Loss: 0.00001340
Iteration 78/1000 | Loss: 0.00001339
Iteration 79/1000 | Loss: 0.00001339
Iteration 80/1000 | Loss: 0.00001336
Iteration 81/1000 | Loss: 0.00001336
Iteration 82/1000 | Loss: 0.00001336
Iteration 83/1000 | Loss: 0.00001336
Iteration 84/1000 | Loss: 0.00001336
Iteration 85/1000 | Loss: 0.00001335
Iteration 86/1000 | Loss: 0.00001335
Iteration 87/1000 | Loss: 0.00001335
Iteration 88/1000 | Loss: 0.00001334
Iteration 89/1000 | Loss: 0.00001334
Iteration 90/1000 | Loss: 0.00001334
Iteration 91/1000 | Loss: 0.00001333
Iteration 92/1000 | Loss: 0.00001333
Iteration 93/1000 | Loss: 0.00001333
Iteration 94/1000 | Loss: 0.00001332
Iteration 95/1000 | Loss: 0.00001332
Iteration 96/1000 | Loss: 0.00001331
Iteration 97/1000 | Loss: 0.00001331
Iteration 98/1000 | Loss: 0.00001330
Iteration 99/1000 | Loss: 0.00001330
Iteration 100/1000 | Loss: 0.00001330
Iteration 101/1000 | Loss: 0.00001330
Iteration 102/1000 | Loss: 0.00001330
Iteration 103/1000 | Loss: 0.00001330
Iteration 104/1000 | Loss: 0.00001330
Iteration 105/1000 | Loss: 0.00001329
Iteration 106/1000 | Loss: 0.00001329
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001328
Iteration 110/1000 | Loss: 0.00001328
Iteration 111/1000 | Loss: 0.00001327
Iteration 112/1000 | Loss: 0.00001327
Iteration 113/1000 | Loss: 0.00001327
Iteration 114/1000 | Loss: 0.00001327
Iteration 115/1000 | Loss: 0.00001327
Iteration 116/1000 | Loss: 0.00001326
Iteration 117/1000 | Loss: 0.00001326
Iteration 118/1000 | Loss: 0.00001326
Iteration 119/1000 | Loss: 0.00001326
Iteration 120/1000 | Loss: 0.00001325
Iteration 121/1000 | Loss: 0.00001325
Iteration 122/1000 | Loss: 0.00001325
Iteration 123/1000 | Loss: 0.00001325
Iteration 124/1000 | Loss: 0.00001324
Iteration 125/1000 | Loss: 0.00001324
Iteration 126/1000 | Loss: 0.00001324
Iteration 127/1000 | Loss: 0.00001324
Iteration 128/1000 | Loss: 0.00001323
Iteration 129/1000 | Loss: 0.00001323
Iteration 130/1000 | Loss: 0.00001323
Iteration 131/1000 | Loss: 0.00001323
Iteration 132/1000 | Loss: 0.00001323
Iteration 133/1000 | Loss: 0.00001322
Iteration 134/1000 | Loss: 0.00001322
Iteration 135/1000 | Loss: 0.00001321
Iteration 136/1000 | Loss: 0.00001321
Iteration 137/1000 | Loss: 0.00001320
Iteration 138/1000 | Loss: 0.00001320
Iteration 139/1000 | Loss: 0.00001320
Iteration 140/1000 | Loss: 0.00001320
Iteration 141/1000 | Loss: 0.00001319
Iteration 142/1000 | Loss: 0.00001319
Iteration 143/1000 | Loss: 0.00001319
Iteration 144/1000 | Loss: 0.00001319
Iteration 145/1000 | Loss: 0.00001319
Iteration 146/1000 | Loss: 0.00001318
Iteration 147/1000 | Loss: 0.00001318
Iteration 148/1000 | Loss: 0.00001318
Iteration 149/1000 | Loss: 0.00001318
Iteration 150/1000 | Loss: 0.00001318
Iteration 151/1000 | Loss: 0.00001318
Iteration 152/1000 | Loss: 0.00001318
Iteration 153/1000 | Loss: 0.00001318
Iteration 154/1000 | Loss: 0.00001318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.3182346265239175e-05, 1.3182346265239175e-05, 1.3182346265239175e-05, 1.3182346265239175e-05, 1.3182346265239175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3182346265239175e-05

Optimization complete. Final v2v error: 3.0709903240203857 mm

Highest mean error: 3.6437957286834717 mm for frame 83

Lowest mean error: 2.8063154220581055 mm for frame 156

Saving results

Total time: 39.936450481414795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489925
Iteration 2/25 | Loss: 0.00142501
Iteration 3/25 | Loss: 0.00134193
Iteration 4/25 | Loss: 0.00132913
Iteration 5/25 | Loss: 0.00132538
Iteration 6/25 | Loss: 0.00132461
Iteration 7/25 | Loss: 0.00132461
Iteration 8/25 | Loss: 0.00132461
Iteration 9/25 | Loss: 0.00132461
Iteration 10/25 | Loss: 0.00132461
Iteration 11/25 | Loss: 0.00132461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013246106682345271, 0.0013246106682345271, 0.0013246106682345271, 0.0013246106682345271, 0.0013246106682345271]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013246106682345271

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44607365
Iteration 2/25 | Loss: 0.00101437
Iteration 3/25 | Loss: 0.00101435
Iteration 4/25 | Loss: 0.00101435
Iteration 5/25 | Loss: 0.00101435
Iteration 6/25 | Loss: 0.00101435
Iteration 7/25 | Loss: 0.00101435
Iteration 8/25 | Loss: 0.00101435
Iteration 9/25 | Loss: 0.00101435
Iteration 10/25 | Loss: 0.00101435
Iteration 11/25 | Loss: 0.00101435
Iteration 12/25 | Loss: 0.00101435
Iteration 13/25 | Loss: 0.00101435
Iteration 14/25 | Loss: 0.00101435
Iteration 15/25 | Loss: 0.00101435
Iteration 16/25 | Loss: 0.00101435
Iteration 17/25 | Loss: 0.00101435
Iteration 18/25 | Loss: 0.00101435
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010143488179892302, 0.0010143488179892302, 0.0010143488179892302, 0.0010143488179892302, 0.0010143488179892302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010143488179892302

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101435
Iteration 2/1000 | Loss: 0.00003198
Iteration 3/1000 | Loss: 0.00002317
Iteration 4/1000 | Loss: 0.00002137
Iteration 5/1000 | Loss: 0.00002031
Iteration 6/1000 | Loss: 0.00001951
Iteration 7/1000 | Loss: 0.00001899
Iteration 8/1000 | Loss: 0.00001870
Iteration 9/1000 | Loss: 0.00001838
Iteration 10/1000 | Loss: 0.00001806
Iteration 11/1000 | Loss: 0.00001787
Iteration 12/1000 | Loss: 0.00001783
Iteration 13/1000 | Loss: 0.00001777
Iteration 14/1000 | Loss: 0.00001772
Iteration 15/1000 | Loss: 0.00001768
Iteration 16/1000 | Loss: 0.00001764
Iteration 17/1000 | Loss: 0.00001764
Iteration 18/1000 | Loss: 0.00001763
Iteration 19/1000 | Loss: 0.00001763
Iteration 20/1000 | Loss: 0.00001761
Iteration 21/1000 | Loss: 0.00001761
Iteration 22/1000 | Loss: 0.00001761
Iteration 23/1000 | Loss: 0.00001760
Iteration 24/1000 | Loss: 0.00001757
Iteration 25/1000 | Loss: 0.00001757
Iteration 26/1000 | Loss: 0.00001751
Iteration 27/1000 | Loss: 0.00001751
Iteration 28/1000 | Loss: 0.00001749
Iteration 29/1000 | Loss: 0.00001747
Iteration 30/1000 | Loss: 0.00001747
Iteration 31/1000 | Loss: 0.00001746
Iteration 32/1000 | Loss: 0.00001745
Iteration 33/1000 | Loss: 0.00001745
Iteration 34/1000 | Loss: 0.00001744
Iteration 35/1000 | Loss: 0.00001744
Iteration 36/1000 | Loss: 0.00001744
Iteration 37/1000 | Loss: 0.00001743
Iteration 38/1000 | Loss: 0.00001743
Iteration 39/1000 | Loss: 0.00001742
Iteration 40/1000 | Loss: 0.00001741
Iteration 41/1000 | Loss: 0.00001741
Iteration 42/1000 | Loss: 0.00001740
Iteration 43/1000 | Loss: 0.00001740
Iteration 44/1000 | Loss: 0.00001740
Iteration 45/1000 | Loss: 0.00001740
Iteration 46/1000 | Loss: 0.00001740
Iteration 47/1000 | Loss: 0.00001740
Iteration 48/1000 | Loss: 0.00001739
Iteration 49/1000 | Loss: 0.00001739
Iteration 50/1000 | Loss: 0.00001739
Iteration 51/1000 | Loss: 0.00001739
Iteration 52/1000 | Loss: 0.00001739
Iteration 53/1000 | Loss: 0.00001738
Iteration 54/1000 | Loss: 0.00001738
Iteration 55/1000 | Loss: 0.00001738
Iteration 56/1000 | Loss: 0.00001737
Iteration 57/1000 | Loss: 0.00001737
Iteration 58/1000 | Loss: 0.00001737
Iteration 59/1000 | Loss: 0.00001736
Iteration 60/1000 | Loss: 0.00001736
Iteration 61/1000 | Loss: 0.00001735
Iteration 62/1000 | Loss: 0.00001735
Iteration 63/1000 | Loss: 0.00001735
Iteration 64/1000 | Loss: 0.00001735
Iteration 65/1000 | Loss: 0.00001735
Iteration 66/1000 | Loss: 0.00001735
Iteration 67/1000 | Loss: 0.00001735
Iteration 68/1000 | Loss: 0.00001735
Iteration 69/1000 | Loss: 0.00001735
Iteration 70/1000 | Loss: 0.00001735
Iteration 71/1000 | Loss: 0.00001734
Iteration 72/1000 | Loss: 0.00001734
Iteration 73/1000 | Loss: 0.00001734
Iteration 74/1000 | Loss: 0.00001734
Iteration 75/1000 | Loss: 0.00001732
Iteration 76/1000 | Loss: 0.00001732
Iteration 77/1000 | Loss: 0.00001731
Iteration 78/1000 | Loss: 0.00001731
Iteration 79/1000 | Loss: 0.00001731
Iteration 80/1000 | Loss: 0.00001730
Iteration 81/1000 | Loss: 0.00001730
Iteration 82/1000 | Loss: 0.00001730
Iteration 83/1000 | Loss: 0.00001729
Iteration 84/1000 | Loss: 0.00001729
Iteration 85/1000 | Loss: 0.00001729
Iteration 86/1000 | Loss: 0.00001728
Iteration 87/1000 | Loss: 0.00001728
Iteration 88/1000 | Loss: 0.00001727
Iteration 89/1000 | Loss: 0.00001727
Iteration 90/1000 | Loss: 0.00001727
Iteration 91/1000 | Loss: 0.00001727
Iteration 92/1000 | Loss: 0.00001727
Iteration 93/1000 | Loss: 0.00001727
Iteration 94/1000 | Loss: 0.00001727
Iteration 95/1000 | Loss: 0.00001727
Iteration 96/1000 | Loss: 0.00001727
Iteration 97/1000 | Loss: 0.00001727
Iteration 98/1000 | Loss: 0.00001727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.727441485854797e-05, 1.727441485854797e-05, 1.727441485854797e-05, 1.727441485854797e-05, 1.727441485854797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.727441485854797e-05

Optimization complete. Final v2v error: 3.4872140884399414 mm

Highest mean error: 3.9826231002807617 mm for frame 116

Lowest mean error: 2.9655892848968506 mm for frame 41

Saving results

Total time: 38.5419487953186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944827
Iteration 2/25 | Loss: 0.00354409
Iteration 3/25 | Loss: 0.00241310
Iteration 4/25 | Loss: 0.00214308
Iteration 5/25 | Loss: 0.00201364
Iteration 6/25 | Loss: 0.00200810
Iteration 7/25 | Loss: 0.00185282
Iteration 8/25 | Loss: 0.00177332
Iteration 9/25 | Loss: 0.00174967
Iteration 10/25 | Loss: 0.00171173
Iteration 11/25 | Loss: 0.00169859
Iteration 12/25 | Loss: 0.00168461
Iteration 13/25 | Loss: 0.00165216
Iteration 14/25 | Loss: 0.00162695
Iteration 15/25 | Loss: 0.00161451
Iteration 16/25 | Loss: 0.00160860
Iteration 17/25 | Loss: 0.00160039
Iteration 18/25 | Loss: 0.00160430
Iteration 19/25 | Loss: 0.00159054
Iteration 20/25 | Loss: 0.00158672
Iteration 21/25 | Loss: 0.00157304
Iteration 22/25 | Loss: 0.00156052
Iteration 23/25 | Loss: 0.00154862
Iteration 24/25 | Loss: 0.00153877
Iteration 25/25 | Loss: 0.00154385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39345241
Iteration 2/25 | Loss: 0.00304830
Iteration 3/25 | Loss: 0.00286072
Iteration 4/25 | Loss: 0.00286072
Iteration 5/25 | Loss: 0.00286072
Iteration 6/25 | Loss: 0.00286072
Iteration 7/25 | Loss: 0.00286072
Iteration 8/25 | Loss: 0.00286072
Iteration 9/25 | Loss: 0.00286072
Iteration 10/25 | Loss: 0.00286072
Iteration 11/25 | Loss: 0.00286072
Iteration 12/25 | Loss: 0.00286072
Iteration 13/25 | Loss: 0.00286072
Iteration 14/25 | Loss: 0.00286072
Iteration 15/25 | Loss: 0.00286072
Iteration 16/25 | Loss: 0.00286072
Iteration 17/25 | Loss: 0.00286072
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0028607207350432873, 0.0028607207350432873, 0.0028607207350432873, 0.0028607207350432873, 0.0028607207350432873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028607207350432873

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00286072
Iteration 2/1000 | Loss: 0.00116187
Iteration 3/1000 | Loss: 0.00265312
Iteration 4/1000 | Loss: 0.00326282
Iteration 5/1000 | Loss: 0.00389147
Iteration 6/1000 | Loss: 0.00080889
Iteration 7/1000 | Loss: 0.00091163
Iteration 8/1000 | Loss: 0.00021831
Iteration 9/1000 | Loss: 0.00105048
Iteration 10/1000 | Loss: 0.00017840
Iteration 11/1000 | Loss: 0.00012990
Iteration 12/1000 | Loss: 0.00099347
Iteration 13/1000 | Loss: 0.00111446
Iteration 14/1000 | Loss: 0.00061181
Iteration 15/1000 | Loss: 0.00030698
Iteration 16/1000 | Loss: 0.00031571
Iteration 17/1000 | Loss: 0.00031779
Iteration 18/1000 | Loss: 0.00114882
Iteration 19/1000 | Loss: 0.00060208
Iteration 20/1000 | Loss: 0.00017746
Iteration 21/1000 | Loss: 0.00040000
Iteration 22/1000 | Loss: 0.00034651
Iteration 23/1000 | Loss: 0.00017723
Iteration 24/1000 | Loss: 0.00036381
Iteration 25/1000 | Loss: 0.00081231
Iteration 26/1000 | Loss: 0.00138545
Iteration 27/1000 | Loss: 0.00177575
Iteration 28/1000 | Loss: 0.00149214
Iteration 29/1000 | Loss: 0.00147520
Iteration 30/1000 | Loss: 0.00195735
Iteration 31/1000 | Loss: 0.00090549
Iteration 32/1000 | Loss: 0.00043648
Iteration 33/1000 | Loss: 0.00084757
Iteration 34/1000 | Loss: 0.00096887
Iteration 35/1000 | Loss: 0.00064951
Iteration 36/1000 | Loss: 0.00050821
Iteration 37/1000 | Loss: 0.00027991
Iteration 38/1000 | Loss: 0.00081463
Iteration 39/1000 | Loss: 0.00047305
Iteration 40/1000 | Loss: 0.00036459
Iteration 41/1000 | Loss: 0.00024671
Iteration 42/1000 | Loss: 0.00073034
Iteration 43/1000 | Loss: 0.00185355
Iteration 44/1000 | Loss: 0.00158658
Iteration 45/1000 | Loss: 0.00044039
Iteration 46/1000 | Loss: 0.00046947
Iteration 47/1000 | Loss: 0.00063003
Iteration 48/1000 | Loss: 0.00011792
Iteration 49/1000 | Loss: 0.00011044
Iteration 50/1000 | Loss: 0.00032235
Iteration 51/1000 | Loss: 0.00087762
Iteration 52/1000 | Loss: 0.00061374
Iteration 53/1000 | Loss: 0.00047680
Iteration 54/1000 | Loss: 0.00030333
Iteration 55/1000 | Loss: 0.00069354
Iteration 56/1000 | Loss: 0.00023587
Iteration 57/1000 | Loss: 0.00059873
Iteration 58/1000 | Loss: 0.00024599
Iteration 59/1000 | Loss: 0.00019050
Iteration 60/1000 | Loss: 0.00013913
Iteration 61/1000 | Loss: 0.00009779
Iteration 62/1000 | Loss: 0.00030552
Iteration 63/1000 | Loss: 0.00033954
Iteration 64/1000 | Loss: 0.00012820
Iteration 65/1000 | Loss: 0.00089316
Iteration 66/1000 | Loss: 0.00009672
Iteration 67/1000 | Loss: 0.00008857
Iteration 68/1000 | Loss: 0.00007945
Iteration 69/1000 | Loss: 0.00037710
Iteration 70/1000 | Loss: 0.00009232
Iteration 71/1000 | Loss: 0.00033342
Iteration 72/1000 | Loss: 0.00010845
Iteration 73/1000 | Loss: 0.00008285
Iteration 74/1000 | Loss: 0.00008624
Iteration 75/1000 | Loss: 0.00007904
Iteration 76/1000 | Loss: 0.00023918
Iteration 77/1000 | Loss: 0.00024574
Iteration 78/1000 | Loss: 0.00030193
Iteration 79/1000 | Loss: 0.00038560
Iteration 80/1000 | Loss: 0.00006734
Iteration 81/1000 | Loss: 0.00006337
Iteration 82/1000 | Loss: 0.00018934
Iteration 83/1000 | Loss: 0.00006995
Iteration 84/1000 | Loss: 0.00006736
Iteration 85/1000 | Loss: 0.00025982
Iteration 86/1000 | Loss: 0.00050271
Iteration 87/1000 | Loss: 0.00013241
Iteration 88/1000 | Loss: 0.00009964
Iteration 89/1000 | Loss: 0.00008380
Iteration 90/1000 | Loss: 0.00006897
Iteration 91/1000 | Loss: 0.00006796
Iteration 92/1000 | Loss: 0.00005530
Iteration 93/1000 | Loss: 0.00006092
Iteration 94/1000 | Loss: 0.00026166
Iteration 95/1000 | Loss: 0.00051177
Iteration 96/1000 | Loss: 0.00019528
Iteration 97/1000 | Loss: 0.00006250
Iteration 98/1000 | Loss: 0.00023171
Iteration 99/1000 | Loss: 0.00006274
Iteration 100/1000 | Loss: 0.00006519
Iteration 101/1000 | Loss: 0.00016938
Iteration 102/1000 | Loss: 0.00060094
Iteration 103/1000 | Loss: 0.00018242
Iteration 104/1000 | Loss: 0.00028909
Iteration 105/1000 | Loss: 0.00017210
Iteration 106/1000 | Loss: 0.00006227
Iteration 107/1000 | Loss: 0.00005026
Iteration 108/1000 | Loss: 0.00004746
Iteration 109/1000 | Loss: 0.00006788
Iteration 110/1000 | Loss: 0.00005791
Iteration 111/1000 | Loss: 0.00009671
Iteration 112/1000 | Loss: 0.00004782
Iteration 113/1000 | Loss: 0.00004177
Iteration 114/1000 | Loss: 0.00011870
Iteration 115/1000 | Loss: 0.00022950
Iteration 116/1000 | Loss: 0.00046753
Iteration 117/1000 | Loss: 0.00004888
Iteration 118/1000 | Loss: 0.00004699
Iteration 119/1000 | Loss: 0.00005562
Iteration 120/1000 | Loss: 0.00004639
Iteration 121/1000 | Loss: 0.00008967
Iteration 122/1000 | Loss: 0.00006053
Iteration 123/1000 | Loss: 0.00004614
Iteration 124/1000 | Loss: 0.00005167
Iteration 125/1000 | Loss: 0.00005281
Iteration 126/1000 | Loss: 0.00004675
Iteration 127/1000 | Loss: 0.00004967
Iteration 128/1000 | Loss: 0.00005556
Iteration 129/1000 | Loss: 0.00006269
Iteration 130/1000 | Loss: 0.00004977
Iteration 131/1000 | Loss: 0.00006254
Iteration 132/1000 | Loss: 0.00016324
Iteration 133/1000 | Loss: 0.00005738
Iteration 134/1000 | Loss: 0.00005543
Iteration 135/1000 | Loss: 0.00005336
Iteration 136/1000 | Loss: 0.00005249
Iteration 137/1000 | Loss: 0.00004592
Iteration 138/1000 | Loss: 0.00004460
Iteration 139/1000 | Loss: 0.00004777
Iteration 140/1000 | Loss: 0.00004676
Iteration 141/1000 | Loss: 0.00005270
Iteration 142/1000 | Loss: 0.00005520
Iteration 143/1000 | Loss: 0.00007024
Iteration 144/1000 | Loss: 0.00005707
Iteration 145/1000 | Loss: 0.00006399
Iteration 146/1000 | Loss: 0.00004676
Iteration 147/1000 | Loss: 0.00004483
Iteration 148/1000 | Loss: 0.00027818
Iteration 149/1000 | Loss: 0.00006418
Iteration 150/1000 | Loss: 0.00004698
Iteration 151/1000 | Loss: 0.00004624
Iteration 152/1000 | Loss: 0.00027438
Iteration 153/1000 | Loss: 0.00006911
Iteration 154/1000 | Loss: 0.00024994
Iteration 155/1000 | Loss: 0.00010868
Iteration 156/1000 | Loss: 0.00004151
Iteration 157/1000 | Loss: 0.00003937
Iteration 158/1000 | Loss: 0.00003825
Iteration 159/1000 | Loss: 0.00003718
Iteration 160/1000 | Loss: 0.00003647
Iteration 161/1000 | Loss: 0.00003598
Iteration 162/1000 | Loss: 0.00003567
Iteration 163/1000 | Loss: 0.00003534
Iteration 164/1000 | Loss: 0.00003503
Iteration 165/1000 | Loss: 0.00003484
Iteration 166/1000 | Loss: 0.00003483
Iteration 167/1000 | Loss: 0.00010409
Iteration 168/1000 | Loss: 0.00004209
Iteration 169/1000 | Loss: 0.00003772
Iteration 170/1000 | Loss: 0.00003472
Iteration 171/1000 | Loss: 0.00003462
Iteration 172/1000 | Loss: 0.00003460
Iteration 173/1000 | Loss: 0.00003459
Iteration 174/1000 | Loss: 0.00006150
Iteration 175/1000 | Loss: 0.00006150
Iteration 176/1000 | Loss: 0.00003615
Iteration 177/1000 | Loss: 0.00003525
Iteration 178/1000 | Loss: 0.00008556
Iteration 179/1000 | Loss: 0.00003662
Iteration 180/1000 | Loss: 0.00003593
Iteration 181/1000 | Loss: 0.00003482
Iteration 182/1000 | Loss: 0.00003453
Iteration 183/1000 | Loss: 0.00003453
Iteration 184/1000 | Loss: 0.00003453
Iteration 185/1000 | Loss: 0.00003453
Iteration 186/1000 | Loss: 0.00003453
Iteration 187/1000 | Loss: 0.00003453
Iteration 188/1000 | Loss: 0.00003453
Iteration 189/1000 | Loss: 0.00003453
Iteration 190/1000 | Loss: 0.00003452
Iteration 191/1000 | Loss: 0.00003452
Iteration 192/1000 | Loss: 0.00003452
Iteration 193/1000 | Loss: 0.00003452
Iteration 194/1000 | Loss: 0.00003452
Iteration 195/1000 | Loss: 0.00003452
Iteration 196/1000 | Loss: 0.00003452
Iteration 197/1000 | Loss: 0.00003452
Iteration 198/1000 | Loss: 0.00003452
Iteration 199/1000 | Loss: 0.00003452
Iteration 200/1000 | Loss: 0.00003451
Iteration 201/1000 | Loss: 0.00003451
Iteration 202/1000 | Loss: 0.00003451
Iteration 203/1000 | Loss: 0.00003451
Iteration 204/1000 | Loss: 0.00003451
Iteration 205/1000 | Loss: 0.00003450
Iteration 206/1000 | Loss: 0.00003450
Iteration 207/1000 | Loss: 0.00003450
Iteration 208/1000 | Loss: 0.00003449
Iteration 209/1000 | Loss: 0.00003449
Iteration 210/1000 | Loss: 0.00004035
Iteration 211/1000 | Loss: 0.00003452
Iteration 212/1000 | Loss: 0.00003449
Iteration 213/1000 | Loss: 0.00003448
Iteration 214/1000 | Loss: 0.00003448
Iteration 215/1000 | Loss: 0.00003448
Iteration 216/1000 | Loss: 0.00003448
Iteration 217/1000 | Loss: 0.00003448
Iteration 218/1000 | Loss: 0.00003448
Iteration 219/1000 | Loss: 0.00003448
Iteration 220/1000 | Loss: 0.00003448
Iteration 221/1000 | Loss: 0.00003448
Iteration 222/1000 | Loss: 0.00003448
Iteration 223/1000 | Loss: 0.00003447
Iteration 224/1000 | Loss: 0.00003447
Iteration 225/1000 | Loss: 0.00003447
Iteration 226/1000 | Loss: 0.00003447
Iteration 227/1000 | Loss: 0.00003447
Iteration 228/1000 | Loss: 0.00003446
Iteration 229/1000 | Loss: 0.00003446
Iteration 230/1000 | Loss: 0.00003446
Iteration 231/1000 | Loss: 0.00003445
Iteration 232/1000 | Loss: 0.00003445
Iteration 233/1000 | Loss: 0.00003445
Iteration 234/1000 | Loss: 0.00003444
Iteration 235/1000 | Loss: 0.00003540
Iteration 236/1000 | Loss: 0.00003442
Iteration 237/1000 | Loss: 0.00003442
Iteration 238/1000 | Loss: 0.00003442
Iteration 239/1000 | Loss: 0.00003442
Iteration 240/1000 | Loss: 0.00003442
Iteration 241/1000 | Loss: 0.00003442
Iteration 242/1000 | Loss: 0.00003442
Iteration 243/1000 | Loss: 0.00003442
Iteration 244/1000 | Loss: 0.00003442
Iteration 245/1000 | Loss: 0.00003442
Iteration 246/1000 | Loss: 0.00003442
Iteration 247/1000 | Loss: 0.00003442
Iteration 248/1000 | Loss: 0.00003442
Iteration 249/1000 | Loss: 0.00003442
Iteration 250/1000 | Loss: 0.00003442
Iteration 251/1000 | Loss: 0.00003441
Iteration 252/1000 | Loss: 0.00003441
Iteration 253/1000 | Loss: 0.00003441
Iteration 254/1000 | Loss: 0.00003441
Iteration 255/1000 | Loss: 0.00003441
Iteration 256/1000 | Loss: 0.00003441
Iteration 257/1000 | Loss: 0.00003441
Iteration 258/1000 | Loss: 0.00003440
Iteration 259/1000 | Loss: 0.00003440
Iteration 260/1000 | Loss: 0.00003440
Iteration 261/1000 | Loss: 0.00003439
Iteration 262/1000 | Loss: 0.00003439
Iteration 263/1000 | Loss: 0.00003439
Iteration 264/1000 | Loss: 0.00003439
Iteration 265/1000 | Loss: 0.00003439
Iteration 266/1000 | Loss: 0.00003438
Iteration 267/1000 | Loss: 0.00003438
Iteration 268/1000 | Loss: 0.00003438
Iteration 269/1000 | Loss: 0.00003438
Iteration 270/1000 | Loss: 0.00003438
Iteration 271/1000 | Loss: 0.00003438
Iteration 272/1000 | Loss: 0.00003438
Iteration 273/1000 | Loss: 0.00003438
Iteration 274/1000 | Loss: 0.00003438
Iteration 275/1000 | Loss: 0.00003438
Iteration 276/1000 | Loss: 0.00003437
Iteration 277/1000 | Loss: 0.00003437
Iteration 278/1000 | Loss: 0.00003437
Iteration 279/1000 | Loss: 0.00003436
Iteration 280/1000 | Loss: 0.00003436
Iteration 281/1000 | Loss: 0.00003436
Iteration 282/1000 | Loss: 0.00003436
Iteration 283/1000 | Loss: 0.00003436
Iteration 284/1000 | Loss: 0.00003436
Iteration 285/1000 | Loss: 0.00003436
Iteration 286/1000 | Loss: 0.00003436
Iteration 287/1000 | Loss: 0.00003435
Iteration 288/1000 | Loss: 0.00003435
Iteration 289/1000 | Loss: 0.00003435
Iteration 290/1000 | Loss: 0.00003435
Iteration 291/1000 | Loss: 0.00003435
Iteration 292/1000 | Loss: 0.00003435
Iteration 293/1000 | Loss: 0.00003435
Iteration 294/1000 | Loss: 0.00003435
Iteration 295/1000 | Loss: 0.00003434
Iteration 296/1000 | Loss: 0.00003434
Iteration 297/1000 | Loss: 0.00003434
Iteration 298/1000 | Loss: 0.00003434
Iteration 299/1000 | Loss: 0.00003434
Iteration 300/1000 | Loss: 0.00003434
Iteration 301/1000 | Loss: 0.00003434
Iteration 302/1000 | Loss: 0.00003434
Iteration 303/1000 | Loss: 0.00003434
Iteration 304/1000 | Loss: 0.00003434
Iteration 305/1000 | Loss: 0.00003434
Iteration 306/1000 | Loss: 0.00003434
Iteration 307/1000 | Loss: 0.00003434
Iteration 308/1000 | Loss: 0.00003434
Iteration 309/1000 | Loss: 0.00003433
Iteration 310/1000 | Loss: 0.00003433
Iteration 311/1000 | Loss: 0.00003433
Iteration 312/1000 | Loss: 0.00003433
Iteration 313/1000 | Loss: 0.00003433
Iteration 314/1000 | Loss: 0.00003433
Iteration 315/1000 | Loss: 0.00003433
Iteration 316/1000 | Loss: 0.00003433
Iteration 317/1000 | Loss: 0.00003433
Iteration 318/1000 | Loss: 0.00003433
Iteration 319/1000 | Loss: 0.00003433
Iteration 320/1000 | Loss: 0.00003433
Iteration 321/1000 | Loss: 0.00003433
Iteration 322/1000 | Loss: 0.00003433
Iteration 323/1000 | Loss: 0.00003433
Iteration 324/1000 | Loss: 0.00003433
Iteration 325/1000 | Loss: 0.00003433
Iteration 326/1000 | Loss: 0.00003433
Iteration 327/1000 | Loss: 0.00003432
Iteration 328/1000 | Loss: 0.00003432
Iteration 329/1000 | Loss: 0.00003432
Iteration 330/1000 | Loss: 0.00003432
Iteration 331/1000 | Loss: 0.00003432
Iteration 332/1000 | Loss: 0.00003432
Iteration 333/1000 | Loss: 0.00003432
Iteration 334/1000 | Loss: 0.00003432
Iteration 335/1000 | Loss: 0.00003432
Iteration 336/1000 | Loss: 0.00003432
Iteration 337/1000 | Loss: 0.00003432
Iteration 338/1000 | Loss: 0.00003432
Iteration 339/1000 | Loss: 0.00003432
Iteration 340/1000 | Loss: 0.00003432
Iteration 341/1000 | Loss: 0.00003432
Iteration 342/1000 | Loss: 0.00003432
Iteration 343/1000 | Loss: 0.00003432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 343. Stopping optimization.
Last 5 losses: [3.432316589169204e-05, 3.432316589169204e-05, 3.432316589169204e-05, 3.432316589169204e-05, 3.432316589169204e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.432316589169204e-05

Optimization complete. Final v2v error: 4.0247979164123535 mm

Highest mean error: 10.762311935424805 mm for frame 126

Lowest mean error: 3.4213500022888184 mm for frame 108

Saving results

Total time: 302.91232919692993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941656
Iteration 2/25 | Loss: 0.00185255
Iteration 3/25 | Loss: 0.00138519
Iteration 4/25 | Loss: 0.00134715
Iteration 5/25 | Loss: 0.00133709
Iteration 6/25 | Loss: 0.00133153
Iteration 7/25 | Loss: 0.00132693
Iteration 8/25 | Loss: 0.00132609
Iteration 9/25 | Loss: 0.00132582
Iteration 10/25 | Loss: 0.00132572
Iteration 11/25 | Loss: 0.00132563
Iteration 12/25 | Loss: 0.00132563
Iteration 13/25 | Loss: 0.00132563
Iteration 14/25 | Loss: 0.00132563
Iteration 15/25 | Loss: 0.00132563
Iteration 16/25 | Loss: 0.00132563
Iteration 17/25 | Loss: 0.00132563
Iteration 18/25 | Loss: 0.00132562
Iteration 19/25 | Loss: 0.00132562
Iteration 20/25 | Loss: 0.00132562
Iteration 21/25 | Loss: 0.00132562
Iteration 22/25 | Loss: 0.00132562
Iteration 23/25 | Loss: 0.00132562
Iteration 24/25 | Loss: 0.00132562
Iteration 25/25 | Loss: 0.00132562

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.69414902
Iteration 2/25 | Loss: 0.00082242
Iteration 3/25 | Loss: 0.00082241
Iteration 4/25 | Loss: 0.00082241
Iteration 5/25 | Loss: 0.00082241
Iteration 6/25 | Loss: 0.00082241
Iteration 7/25 | Loss: 0.00082241
Iteration 8/25 | Loss: 0.00082241
Iteration 9/25 | Loss: 0.00082241
Iteration 10/25 | Loss: 0.00082241
Iteration 11/25 | Loss: 0.00082241
Iteration 12/25 | Loss: 0.00082241
Iteration 13/25 | Loss: 0.00082241
Iteration 14/25 | Loss: 0.00082241
Iteration 15/25 | Loss: 0.00082241
Iteration 16/25 | Loss: 0.00082241
Iteration 17/25 | Loss: 0.00082241
Iteration 18/25 | Loss: 0.00082241
Iteration 19/25 | Loss: 0.00082241
Iteration 20/25 | Loss: 0.00082241
Iteration 21/25 | Loss: 0.00082241
Iteration 22/25 | Loss: 0.00082241
Iteration 23/25 | Loss: 0.00082241
Iteration 24/25 | Loss: 0.00082241
Iteration 25/25 | Loss: 0.00082241

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082241
Iteration 2/1000 | Loss: 0.00002911
Iteration 3/1000 | Loss: 0.00002087
Iteration 4/1000 | Loss: 0.00001965
Iteration 5/1000 | Loss: 0.00001896
Iteration 6/1000 | Loss: 0.00001854
Iteration 7/1000 | Loss: 0.00001825
Iteration 8/1000 | Loss: 0.00001790
Iteration 9/1000 | Loss: 0.00001779
Iteration 10/1000 | Loss: 0.00001776
Iteration 11/1000 | Loss: 0.00001755
Iteration 12/1000 | Loss: 0.00001742
Iteration 13/1000 | Loss: 0.00001729
Iteration 14/1000 | Loss: 0.00001727
Iteration 15/1000 | Loss: 0.00001727
Iteration 16/1000 | Loss: 0.00001720
Iteration 17/1000 | Loss: 0.00001715
Iteration 18/1000 | Loss: 0.00001699
Iteration 19/1000 | Loss: 0.00001684
Iteration 20/1000 | Loss: 0.00001682
Iteration 21/1000 | Loss: 0.00001678
Iteration 22/1000 | Loss: 0.00001677
Iteration 23/1000 | Loss: 0.00001675
Iteration 24/1000 | Loss: 0.00001675
Iteration 25/1000 | Loss: 0.00001674
Iteration 26/1000 | Loss: 0.00001673
Iteration 27/1000 | Loss: 0.00001668
Iteration 28/1000 | Loss: 0.00001666
Iteration 29/1000 | Loss: 0.00001664
Iteration 30/1000 | Loss: 0.00001664
Iteration 31/1000 | Loss: 0.00001663
Iteration 32/1000 | Loss: 0.00001663
Iteration 33/1000 | Loss: 0.00001663
Iteration 34/1000 | Loss: 0.00001663
Iteration 35/1000 | Loss: 0.00001663
Iteration 36/1000 | Loss: 0.00001663
Iteration 37/1000 | Loss: 0.00001662
Iteration 38/1000 | Loss: 0.00001662
Iteration 39/1000 | Loss: 0.00001662
Iteration 40/1000 | Loss: 0.00001662
Iteration 41/1000 | Loss: 0.00001661
Iteration 42/1000 | Loss: 0.00001661
Iteration 43/1000 | Loss: 0.00001661
Iteration 44/1000 | Loss: 0.00001660
Iteration 45/1000 | Loss: 0.00001660
Iteration 46/1000 | Loss: 0.00001658
Iteration 47/1000 | Loss: 0.00001658
Iteration 48/1000 | Loss: 0.00001658
Iteration 49/1000 | Loss: 0.00001658
Iteration 50/1000 | Loss: 0.00001657
Iteration 51/1000 | Loss: 0.00001657
Iteration 52/1000 | Loss: 0.00001657
Iteration 53/1000 | Loss: 0.00001657
Iteration 54/1000 | Loss: 0.00001657
Iteration 55/1000 | Loss: 0.00001657
Iteration 56/1000 | Loss: 0.00001656
Iteration 57/1000 | Loss: 0.00001656
Iteration 58/1000 | Loss: 0.00001655
Iteration 59/1000 | Loss: 0.00001655
Iteration 60/1000 | Loss: 0.00001655
Iteration 61/1000 | Loss: 0.00001654
Iteration 62/1000 | Loss: 0.00001654
Iteration 63/1000 | Loss: 0.00001654
Iteration 64/1000 | Loss: 0.00001654
Iteration 65/1000 | Loss: 0.00001653
Iteration 66/1000 | Loss: 0.00001653
Iteration 67/1000 | Loss: 0.00001652
Iteration 68/1000 | Loss: 0.00001651
Iteration 69/1000 | Loss: 0.00001651
Iteration 70/1000 | Loss: 0.00001651
Iteration 71/1000 | Loss: 0.00001651
Iteration 72/1000 | Loss: 0.00001650
Iteration 73/1000 | Loss: 0.00001650
Iteration 74/1000 | Loss: 0.00001650
Iteration 75/1000 | Loss: 0.00001649
Iteration 76/1000 | Loss: 0.00001649
Iteration 77/1000 | Loss: 0.00001649
Iteration 78/1000 | Loss: 0.00001648
Iteration 79/1000 | Loss: 0.00001648
Iteration 80/1000 | Loss: 0.00001648
Iteration 81/1000 | Loss: 0.00001648
Iteration 82/1000 | Loss: 0.00001648
Iteration 83/1000 | Loss: 0.00001647
Iteration 84/1000 | Loss: 0.00001647
Iteration 85/1000 | Loss: 0.00001646
Iteration 86/1000 | Loss: 0.00001646
Iteration 87/1000 | Loss: 0.00001646
Iteration 88/1000 | Loss: 0.00001646
Iteration 89/1000 | Loss: 0.00001646
Iteration 90/1000 | Loss: 0.00001645
Iteration 91/1000 | Loss: 0.00001645
Iteration 92/1000 | Loss: 0.00001645
Iteration 93/1000 | Loss: 0.00001645
Iteration 94/1000 | Loss: 0.00001645
Iteration 95/1000 | Loss: 0.00001645
Iteration 96/1000 | Loss: 0.00001644
Iteration 97/1000 | Loss: 0.00001644
Iteration 98/1000 | Loss: 0.00001644
Iteration 99/1000 | Loss: 0.00001644
Iteration 100/1000 | Loss: 0.00001644
Iteration 101/1000 | Loss: 0.00001643
Iteration 102/1000 | Loss: 0.00001643
Iteration 103/1000 | Loss: 0.00001643
Iteration 104/1000 | Loss: 0.00001643
Iteration 105/1000 | Loss: 0.00001643
Iteration 106/1000 | Loss: 0.00001643
Iteration 107/1000 | Loss: 0.00001643
Iteration 108/1000 | Loss: 0.00001643
Iteration 109/1000 | Loss: 0.00001643
Iteration 110/1000 | Loss: 0.00001643
Iteration 111/1000 | Loss: 0.00001643
Iteration 112/1000 | Loss: 0.00001643
Iteration 113/1000 | Loss: 0.00001643
Iteration 114/1000 | Loss: 0.00001643
Iteration 115/1000 | Loss: 0.00001643
Iteration 116/1000 | Loss: 0.00001643
Iteration 117/1000 | Loss: 0.00001643
Iteration 118/1000 | Loss: 0.00001643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.642666757106781e-05, 1.642666757106781e-05, 1.642666757106781e-05, 1.642666757106781e-05, 1.642666757106781e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.642666757106781e-05

Optimization complete. Final v2v error: 3.4137558937072754 mm

Highest mean error: 3.5735504627227783 mm for frame 89

Lowest mean error: 3.2601966857910156 mm for frame 22

Saving results

Total time: 45.707916259765625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817961
Iteration 2/25 | Loss: 0.00141330
Iteration 3/25 | Loss: 0.00129969
Iteration 4/25 | Loss: 0.00128238
Iteration 5/25 | Loss: 0.00127829
Iteration 6/25 | Loss: 0.00127702
Iteration 7/25 | Loss: 0.00127671
Iteration 8/25 | Loss: 0.00127671
Iteration 9/25 | Loss: 0.00127671
Iteration 10/25 | Loss: 0.00127671
Iteration 11/25 | Loss: 0.00127671
Iteration 12/25 | Loss: 0.00127671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012767149601131678, 0.0012767149601131678, 0.0012767149601131678, 0.0012767149601131678, 0.0012767149601131678]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012767149601131678

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42824614
Iteration 2/25 | Loss: 0.00090829
Iteration 3/25 | Loss: 0.00090828
Iteration 4/25 | Loss: 0.00090828
Iteration 5/25 | Loss: 0.00090828
Iteration 6/25 | Loss: 0.00090828
Iteration 7/25 | Loss: 0.00090828
Iteration 8/25 | Loss: 0.00090828
Iteration 9/25 | Loss: 0.00090828
Iteration 10/25 | Loss: 0.00090828
Iteration 11/25 | Loss: 0.00090828
Iteration 12/25 | Loss: 0.00090828
Iteration 13/25 | Loss: 0.00090828
Iteration 14/25 | Loss: 0.00090828
Iteration 15/25 | Loss: 0.00090828
Iteration 16/25 | Loss: 0.00090828
Iteration 17/25 | Loss: 0.00090828
Iteration 18/25 | Loss: 0.00090828
Iteration 19/25 | Loss: 0.00090828
Iteration 20/25 | Loss: 0.00090828
Iteration 21/25 | Loss: 0.00090828
Iteration 22/25 | Loss: 0.00090828
Iteration 23/25 | Loss: 0.00090828
Iteration 24/25 | Loss: 0.00090828
Iteration 25/25 | Loss: 0.00090828

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090828
Iteration 2/1000 | Loss: 0.00002846
Iteration 3/1000 | Loss: 0.00001815
Iteration 4/1000 | Loss: 0.00001582
Iteration 5/1000 | Loss: 0.00001483
Iteration 6/1000 | Loss: 0.00001412
Iteration 7/1000 | Loss: 0.00001354
Iteration 8/1000 | Loss: 0.00001321
Iteration 9/1000 | Loss: 0.00001291
Iteration 10/1000 | Loss: 0.00001262
Iteration 11/1000 | Loss: 0.00001256
Iteration 12/1000 | Loss: 0.00001254
Iteration 13/1000 | Loss: 0.00001252
Iteration 14/1000 | Loss: 0.00001250
Iteration 15/1000 | Loss: 0.00001246
Iteration 16/1000 | Loss: 0.00001245
Iteration 17/1000 | Loss: 0.00001244
Iteration 18/1000 | Loss: 0.00001243
Iteration 19/1000 | Loss: 0.00001243
Iteration 20/1000 | Loss: 0.00001241
Iteration 21/1000 | Loss: 0.00001238
Iteration 22/1000 | Loss: 0.00001237
Iteration 23/1000 | Loss: 0.00001236
Iteration 24/1000 | Loss: 0.00001235
Iteration 25/1000 | Loss: 0.00001234
Iteration 26/1000 | Loss: 0.00001234
Iteration 27/1000 | Loss: 0.00001231
Iteration 28/1000 | Loss: 0.00001231
Iteration 29/1000 | Loss: 0.00001230
Iteration 30/1000 | Loss: 0.00001230
Iteration 31/1000 | Loss: 0.00001229
Iteration 32/1000 | Loss: 0.00001229
Iteration 33/1000 | Loss: 0.00001228
Iteration 34/1000 | Loss: 0.00001227
Iteration 35/1000 | Loss: 0.00001227
Iteration 36/1000 | Loss: 0.00001226
Iteration 37/1000 | Loss: 0.00001225
Iteration 38/1000 | Loss: 0.00001224
Iteration 39/1000 | Loss: 0.00001224
Iteration 40/1000 | Loss: 0.00001223
Iteration 41/1000 | Loss: 0.00001223
Iteration 42/1000 | Loss: 0.00001222
Iteration 43/1000 | Loss: 0.00001222
Iteration 44/1000 | Loss: 0.00001221
Iteration 45/1000 | Loss: 0.00001221
Iteration 46/1000 | Loss: 0.00001220
Iteration 47/1000 | Loss: 0.00001220
Iteration 48/1000 | Loss: 0.00001220
Iteration 49/1000 | Loss: 0.00001219
Iteration 50/1000 | Loss: 0.00001219
Iteration 51/1000 | Loss: 0.00001219
Iteration 52/1000 | Loss: 0.00001219
Iteration 53/1000 | Loss: 0.00001218
Iteration 54/1000 | Loss: 0.00001218
Iteration 55/1000 | Loss: 0.00001218
Iteration 56/1000 | Loss: 0.00001217
Iteration 57/1000 | Loss: 0.00001216
Iteration 58/1000 | Loss: 0.00001216
Iteration 59/1000 | Loss: 0.00001216
Iteration 60/1000 | Loss: 0.00001216
Iteration 61/1000 | Loss: 0.00001216
Iteration 62/1000 | Loss: 0.00001216
Iteration 63/1000 | Loss: 0.00001216
Iteration 64/1000 | Loss: 0.00001216
Iteration 65/1000 | Loss: 0.00001216
Iteration 66/1000 | Loss: 0.00001215
Iteration 67/1000 | Loss: 0.00001215
Iteration 68/1000 | Loss: 0.00001215
Iteration 69/1000 | Loss: 0.00001215
Iteration 70/1000 | Loss: 0.00001215
Iteration 71/1000 | Loss: 0.00001215
Iteration 72/1000 | Loss: 0.00001214
Iteration 73/1000 | Loss: 0.00001213
Iteration 74/1000 | Loss: 0.00001213
Iteration 75/1000 | Loss: 0.00001212
Iteration 76/1000 | Loss: 0.00001212
Iteration 77/1000 | Loss: 0.00001212
Iteration 78/1000 | Loss: 0.00001212
Iteration 79/1000 | Loss: 0.00001211
Iteration 80/1000 | Loss: 0.00001211
Iteration 81/1000 | Loss: 0.00001211
Iteration 82/1000 | Loss: 0.00001210
Iteration 83/1000 | Loss: 0.00001210
Iteration 84/1000 | Loss: 0.00001209
Iteration 85/1000 | Loss: 0.00001209
Iteration 86/1000 | Loss: 0.00001208
Iteration 87/1000 | Loss: 0.00001207
Iteration 88/1000 | Loss: 0.00001207
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001207
Iteration 91/1000 | Loss: 0.00001206
Iteration 92/1000 | Loss: 0.00001206
Iteration 93/1000 | Loss: 0.00001206
Iteration 94/1000 | Loss: 0.00001206
Iteration 95/1000 | Loss: 0.00001206
Iteration 96/1000 | Loss: 0.00001205
Iteration 97/1000 | Loss: 0.00001205
Iteration 98/1000 | Loss: 0.00001204
Iteration 99/1000 | Loss: 0.00001204
Iteration 100/1000 | Loss: 0.00001203
Iteration 101/1000 | Loss: 0.00001202
Iteration 102/1000 | Loss: 0.00001202
Iteration 103/1000 | Loss: 0.00001202
Iteration 104/1000 | Loss: 0.00001202
Iteration 105/1000 | Loss: 0.00001202
Iteration 106/1000 | Loss: 0.00001202
Iteration 107/1000 | Loss: 0.00001201
Iteration 108/1000 | Loss: 0.00001201
Iteration 109/1000 | Loss: 0.00001201
Iteration 110/1000 | Loss: 0.00001201
Iteration 111/1000 | Loss: 0.00001201
Iteration 112/1000 | Loss: 0.00001201
Iteration 113/1000 | Loss: 0.00001201
Iteration 114/1000 | Loss: 0.00001200
Iteration 115/1000 | Loss: 0.00001200
Iteration 116/1000 | Loss: 0.00001200
Iteration 117/1000 | Loss: 0.00001200
Iteration 118/1000 | Loss: 0.00001200
Iteration 119/1000 | Loss: 0.00001199
Iteration 120/1000 | Loss: 0.00001199
Iteration 121/1000 | Loss: 0.00001199
Iteration 122/1000 | Loss: 0.00001199
Iteration 123/1000 | Loss: 0.00001199
Iteration 124/1000 | Loss: 0.00001199
Iteration 125/1000 | Loss: 0.00001198
Iteration 126/1000 | Loss: 0.00001198
Iteration 127/1000 | Loss: 0.00001198
Iteration 128/1000 | Loss: 0.00001198
Iteration 129/1000 | Loss: 0.00001197
Iteration 130/1000 | Loss: 0.00001197
Iteration 131/1000 | Loss: 0.00001197
Iteration 132/1000 | Loss: 0.00001197
Iteration 133/1000 | Loss: 0.00001197
Iteration 134/1000 | Loss: 0.00001196
Iteration 135/1000 | Loss: 0.00001196
Iteration 136/1000 | Loss: 0.00001196
Iteration 137/1000 | Loss: 0.00001196
Iteration 138/1000 | Loss: 0.00001196
Iteration 139/1000 | Loss: 0.00001195
Iteration 140/1000 | Loss: 0.00001195
Iteration 141/1000 | Loss: 0.00001194
Iteration 142/1000 | Loss: 0.00001194
Iteration 143/1000 | Loss: 0.00001194
Iteration 144/1000 | Loss: 0.00001194
Iteration 145/1000 | Loss: 0.00001193
Iteration 146/1000 | Loss: 0.00001193
Iteration 147/1000 | Loss: 0.00001193
Iteration 148/1000 | Loss: 0.00001193
Iteration 149/1000 | Loss: 0.00001193
Iteration 150/1000 | Loss: 0.00001193
Iteration 151/1000 | Loss: 0.00001192
Iteration 152/1000 | Loss: 0.00001192
Iteration 153/1000 | Loss: 0.00001192
Iteration 154/1000 | Loss: 0.00001192
Iteration 155/1000 | Loss: 0.00001192
Iteration 156/1000 | Loss: 0.00001192
Iteration 157/1000 | Loss: 0.00001192
Iteration 158/1000 | Loss: 0.00001191
Iteration 159/1000 | Loss: 0.00001191
Iteration 160/1000 | Loss: 0.00001191
Iteration 161/1000 | Loss: 0.00001191
Iteration 162/1000 | Loss: 0.00001191
Iteration 163/1000 | Loss: 0.00001191
Iteration 164/1000 | Loss: 0.00001191
Iteration 165/1000 | Loss: 0.00001191
Iteration 166/1000 | Loss: 0.00001191
Iteration 167/1000 | Loss: 0.00001191
Iteration 168/1000 | Loss: 0.00001190
Iteration 169/1000 | Loss: 0.00001190
Iteration 170/1000 | Loss: 0.00001190
Iteration 171/1000 | Loss: 0.00001190
Iteration 172/1000 | Loss: 0.00001190
Iteration 173/1000 | Loss: 0.00001190
Iteration 174/1000 | Loss: 0.00001190
Iteration 175/1000 | Loss: 0.00001190
Iteration 176/1000 | Loss: 0.00001190
Iteration 177/1000 | Loss: 0.00001190
Iteration 178/1000 | Loss: 0.00001189
Iteration 179/1000 | Loss: 0.00001189
Iteration 180/1000 | Loss: 0.00001189
Iteration 181/1000 | Loss: 0.00001189
Iteration 182/1000 | Loss: 0.00001189
Iteration 183/1000 | Loss: 0.00001188
Iteration 184/1000 | Loss: 0.00001188
Iteration 185/1000 | Loss: 0.00001188
Iteration 186/1000 | Loss: 0.00001188
Iteration 187/1000 | Loss: 0.00001188
Iteration 188/1000 | Loss: 0.00001188
Iteration 189/1000 | Loss: 0.00001188
Iteration 190/1000 | Loss: 0.00001188
Iteration 191/1000 | Loss: 0.00001188
Iteration 192/1000 | Loss: 0.00001187
Iteration 193/1000 | Loss: 0.00001187
Iteration 194/1000 | Loss: 0.00001187
Iteration 195/1000 | Loss: 0.00001187
Iteration 196/1000 | Loss: 0.00001187
Iteration 197/1000 | Loss: 0.00001186
Iteration 198/1000 | Loss: 0.00001186
Iteration 199/1000 | Loss: 0.00001186
Iteration 200/1000 | Loss: 0.00001186
Iteration 201/1000 | Loss: 0.00001185
Iteration 202/1000 | Loss: 0.00001185
Iteration 203/1000 | Loss: 0.00001185
Iteration 204/1000 | Loss: 0.00001185
Iteration 205/1000 | Loss: 0.00001185
Iteration 206/1000 | Loss: 0.00001185
Iteration 207/1000 | Loss: 0.00001185
Iteration 208/1000 | Loss: 0.00001185
Iteration 209/1000 | Loss: 0.00001185
Iteration 210/1000 | Loss: 0.00001185
Iteration 211/1000 | Loss: 0.00001185
Iteration 212/1000 | Loss: 0.00001185
Iteration 213/1000 | Loss: 0.00001185
Iteration 214/1000 | Loss: 0.00001184
Iteration 215/1000 | Loss: 0.00001184
Iteration 216/1000 | Loss: 0.00001184
Iteration 217/1000 | Loss: 0.00001184
Iteration 218/1000 | Loss: 0.00001184
Iteration 219/1000 | Loss: 0.00001184
Iteration 220/1000 | Loss: 0.00001184
Iteration 221/1000 | Loss: 0.00001184
Iteration 222/1000 | Loss: 0.00001184
Iteration 223/1000 | Loss: 0.00001184
Iteration 224/1000 | Loss: 0.00001184
Iteration 225/1000 | Loss: 0.00001184
Iteration 226/1000 | Loss: 0.00001184
Iteration 227/1000 | Loss: 0.00001184
Iteration 228/1000 | Loss: 0.00001184
Iteration 229/1000 | Loss: 0.00001184
Iteration 230/1000 | Loss: 0.00001184
Iteration 231/1000 | Loss: 0.00001184
Iteration 232/1000 | Loss: 0.00001184
Iteration 233/1000 | Loss: 0.00001184
Iteration 234/1000 | Loss: 0.00001184
Iteration 235/1000 | Loss: 0.00001184
Iteration 236/1000 | Loss: 0.00001184
Iteration 237/1000 | Loss: 0.00001184
Iteration 238/1000 | Loss: 0.00001184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.1842185813293327e-05, 1.1842185813293327e-05, 1.1842185813293327e-05, 1.1842185813293327e-05, 1.1842185813293327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1842185813293327e-05

Optimization complete. Final v2v error: 2.9314653873443604 mm

Highest mean error: 4.06481409072876 mm for frame 90

Lowest mean error: 2.6839208602905273 mm for frame 7

Saving results

Total time: 48.74889922142029
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471020
Iteration 2/25 | Loss: 0.00158025
Iteration 3/25 | Loss: 0.00140366
Iteration 4/25 | Loss: 0.00138277
Iteration 5/25 | Loss: 0.00137906
Iteration 6/25 | Loss: 0.00137903
Iteration 7/25 | Loss: 0.00137903
Iteration 8/25 | Loss: 0.00137903
Iteration 9/25 | Loss: 0.00137903
Iteration 10/25 | Loss: 0.00137903
Iteration 11/25 | Loss: 0.00137903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013790325028821826, 0.0013790325028821826, 0.0013790325028821826, 0.0013790325028821826, 0.0013790325028821826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013790325028821826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38060820
Iteration 2/25 | Loss: 0.00096032
Iteration 3/25 | Loss: 0.00096032
Iteration 4/25 | Loss: 0.00096032
Iteration 5/25 | Loss: 0.00096032
Iteration 6/25 | Loss: 0.00096032
Iteration 7/25 | Loss: 0.00096032
Iteration 8/25 | Loss: 0.00096032
Iteration 9/25 | Loss: 0.00096032
Iteration 10/25 | Loss: 0.00096032
Iteration 11/25 | Loss: 0.00096032
Iteration 12/25 | Loss: 0.00096032
Iteration 13/25 | Loss: 0.00096031
Iteration 14/25 | Loss: 0.00096031
Iteration 15/25 | Loss: 0.00096031
Iteration 16/25 | Loss: 0.00096031
Iteration 17/25 | Loss: 0.00096031
Iteration 18/25 | Loss: 0.00096031
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009603149374015629, 0.0009603149374015629, 0.0009603149374015629, 0.0009603149374015629, 0.0009603149374015629]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009603149374015629

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096031
Iteration 2/1000 | Loss: 0.00005159
Iteration 3/1000 | Loss: 0.00004020
Iteration 4/1000 | Loss: 0.00003725
Iteration 5/1000 | Loss: 0.00003521
Iteration 6/1000 | Loss: 0.00003352
Iteration 7/1000 | Loss: 0.00003247
Iteration 8/1000 | Loss: 0.00003171
Iteration 9/1000 | Loss: 0.00003135
Iteration 10/1000 | Loss: 0.00003100
Iteration 11/1000 | Loss: 0.00003079
Iteration 12/1000 | Loss: 0.00003061
Iteration 13/1000 | Loss: 0.00003042
Iteration 14/1000 | Loss: 0.00003020
Iteration 15/1000 | Loss: 0.00003020
Iteration 16/1000 | Loss: 0.00003010
Iteration 17/1000 | Loss: 0.00003006
Iteration 18/1000 | Loss: 0.00002992
Iteration 19/1000 | Loss: 0.00002989
Iteration 20/1000 | Loss: 0.00002976
Iteration 21/1000 | Loss: 0.00002974
Iteration 22/1000 | Loss: 0.00002972
Iteration 23/1000 | Loss: 0.00002967
Iteration 24/1000 | Loss: 0.00002966
Iteration 25/1000 | Loss: 0.00002964
Iteration 26/1000 | Loss: 0.00002963
Iteration 27/1000 | Loss: 0.00002963
Iteration 28/1000 | Loss: 0.00002963
Iteration 29/1000 | Loss: 0.00002963
Iteration 30/1000 | Loss: 0.00002963
Iteration 31/1000 | Loss: 0.00002963
Iteration 32/1000 | Loss: 0.00002963
Iteration 33/1000 | Loss: 0.00002963
Iteration 34/1000 | Loss: 0.00002963
Iteration 35/1000 | Loss: 0.00002963
Iteration 36/1000 | Loss: 0.00002963
Iteration 37/1000 | Loss: 0.00002962
Iteration 38/1000 | Loss: 0.00002962
Iteration 39/1000 | Loss: 0.00002962
Iteration 40/1000 | Loss: 0.00002962
Iteration 41/1000 | Loss: 0.00002962
Iteration 42/1000 | Loss: 0.00002962
Iteration 43/1000 | Loss: 0.00002960
Iteration 44/1000 | Loss: 0.00002960
Iteration 45/1000 | Loss: 0.00002960
Iteration 46/1000 | Loss: 0.00002959
Iteration 47/1000 | Loss: 0.00002959
Iteration 48/1000 | Loss: 0.00002959
Iteration 49/1000 | Loss: 0.00002958
Iteration 50/1000 | Loss: 0.00002957
Iteration 51/1000 | Loss: 0.00002957
Iteration 52/1000 | Loss: 0.00002957
Iteration 53/1000 | Loss: 0.00002957
Iteration 54/1000 | Loss: 0.00002957
Iteration 55/1000 | Loss: 0.00002957
Iteration 56/1000 | Loss: 0.00002957
Iteration 57/1000 | Loss: 0.00002957
Iteration 58/1000 | Loss: 0.00002957
Iteration 59/1000 | Loss: 0.00002957
Iteration 60/1000 | Loss: 0.00002956
Iteration 61/1000 | Loss: 0.00002956
Iteration 62/1000 | Loss: 0.00002956
Iteration 63/1000 | Loss: 0.00002956
Iteration 64/1000 | Loss: 0.00002955
Iteration 65/1000 | Loss: 0.00002955
Iteration 66/1000 | Loss: 0.00002955
Iteration 67/1000 | Loss: 0.00002955
Iteration 68/1000 | Loss: 0.00002955
Iteration 69/1000 | Loss: 0.00002955
Iteration 70/1000 | Loss: 0.00002955
Iteration 71/1000 | Loss: 0.00002955
Iteration 72/1000 | Loss: 0.00002955
Iteration 73/1000 | Loss: 0.00002955
Iteration 74/1000 | Loss: 0.00002955
Iteration 75/1000 | Loss: 0.00002954
Iteration 76/1000 | Loss: 0.00002954
Iteration 77/1000 | Loss: 0.00002954
Iteration 78/1000 | Loss: 0.00002954
Iteration 79/1000 | Loss: 0.00002954
Iteration 80/1000 | Loss: 0.00002954
Iteration 81/1000 | Loss: 0.00002954
Iteration 82/1000 | Loss: 0.00002953
Iteration 83/1000 | Loss: 0.00002953
Iteration 84/1000 | Loss: 0.00002953
Iteration 85/1000 | Loss: 0.00002953
Iteration 86/1000 | Loss: 0.00002953
Iteration 87/1000 | Loss: 0.00002953
Iteration 88/1000 | Loss: 0.00002953
Iteration 89/1000 | Loss: 0.00002953
Iteration 90/1000 | Loss: 0.00002953
Iteration 91/1000 | Loss: 0.00002952
Iteration 92/1000 | Loss: 0.00002952
Iteration 93/1000 | Loss: 0.00002952
Iteration 94/1000 | Loss: 0.00002952
Iteration 95/1000 | Loss: 0.00002952
Iteration 96/1000 | Loss: 0.00002951
Iteration 97/1000 | Loss: 0.00002951
Iteration 98/1000 | Loss: 0.00002951
Iteration 99/1000 | Loss: 0.00002951
Iteration 100/1000 | Loss: 0.00002951
Iteration 101/1000 | Loss: 0.00002951
Iteration 102/1000 | Loss: 0.00002951
Iteration 103/1000 | Loss: 0.00002951
Iteration 104/1000 | Loss: 0.00002950
Iteration 105/1000 | Loss: 0.00002950
Iteration 106/1000 | Loss: 0.00002950
Iteration 107/1000 | Loss: 0.00002950
Iteration 108/1000 | Loss: 0.00002950
Iteration 109/1000 | Loss: 0.00002950
Iteration 110/1000 | Loss: 0.00002950
Iteration 111/1000 | Loss: 0.00002950
Iteration 112/1000 | Loss: 0.00002950
Iteration 113/1000 | Loss: 0.00002950
Iteration 114/1000 | Loss: 0.00002950
Iteration 115/1000 | Loss: 0.00002950
Iteration 116/1000 | Loss: 0.00002950
Iteration 117/1000 | Loss: 0.00002950
Iteration 118/1000 | Loss: 0.00002950
Iteration 119/1000 | Loss: 0.00002950
Iteration 120/1000 | Loss: 0.00002950
Iteration 121/1000 | Loss: 0.00002950
Iteration 122/1000 | Loss: 0.00002950
Iteration 123/1000 | Loss: 0.00002950
Iteration 124/1000 | Loss: 0.00002950
Iteration 125/1000 | Loss: 0.00002950
Iteration 126/1000 | Loss: 0.00002950
Iteration 127/1000 | Loss: 0.00002950
Iteration 128/1000 | Loss: 0.00002950
Iteration 129/1000 | Loss: 0.00002950
Iteration 130/1000 | Loss: 0.00002950
Iteration 131/1000 | Loss: 0.00002950
Iteration 132/1000 | Loss: 0.00002950
Iteration 133/1000 | Loss: 0.00002950
Iteration 134/1000 | Loss: 0.00002950
Iteration 135/1000 | Loss: 0.00002950
Iteration 136/1000 | Loss: 0.00002950
Iteration 137/1000 | Loss: 0.00002950
Iteration 138/1000 | Loss: 0.00002950
Iteration 139/1000 | Loss: 0.00002950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [2.9499711672542617e-05, 2.9499711672542617e-05, 2.9499711672542617e-05, 2.9499711672542617e-05, 2.9499711672542617e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9499711672542617e-05

Optimization complete. Final v2v error: 4.421486854553223 mm

Highest mean error: 4.7756266593933105 mm for frame 159

Lowest mean error: 3.731621265411377 mm for frame 120

Saving results

Total time: 45.14365863800049
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804503
Iteration 2/25 | Loss: 0.00139023
Iteration 3/25 | Loss: 0.00130178
Iteration 4/25 | Loss: 0.00129192
Iteration 5/25 | Loss: 0.00128151
Iteration 6/25 | Loss: 0.00128042
Iteration 7/25 | Loss: 0.00128016
Iteration 8/25 | Loss: 0.00128001
Iteration 9/25 | Loss: 0.00127999
Iteration 10/25 | Loss: 0.00127998
Iteration 11/25 | Loss: 0.00127998
Iteration 12/25 | Loss: 0.00127997
Iteration 13/25 | Loss: 0.00127997
Iteration 14/25 | Loss: 0.00127997
Iteration 15/25 | Loss: 0.00127997
Iteration 16/25 | Loss: 0.00127997
Iteration 17/25 | Loss: 0.00127996
Iteration 18/25 | Loss: 0.00127996
Iteration 19/25 | Loss: 0.00127996
Iteration 20/25 | Loss: 0.00127996
Iteration 21/25 | Loss: 0.00127996
Iteration 22/25 | Loss: 0.00127996
Iteration 23/25 | Loss: 0.00127996
Iteration 24/25 | Loss: 0.00127996
Iteration 25/25 | Loss: 0.00127996

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.51120973
Iteration 2/25 | Loss: 0.00090347
Iteration 3/25 | Loss: 0.00090347
Iteration 4/25 | Loss: 0.00090347
Iteration 5/25 | Loss: 0.00090347
Iteration 6/25 | Loss: 0.00090347
Iteration 7/25 | Loss: 0.00090347
Iteration 8/25 | Loss: 0.00090347
Iteration 9/25 | Loss: 0.00090346
Iteration 10/25 | Loss: 0.00090346
Iteration 11/25 | Loss: 0.00090346
Iteration 12/25 | Loss: 0.00090346
Iteration 13/25 | Loss: 0.00090346
Iteration 14/25 | Loss: 0.00090346
Iteration 15/25 | Loss: 0.00090346
Iteration 16/25 | Loss: 0.00090346
Iteration 17/25 | Loss: 0.00090346
Iteration 18/25 | Loss: 0.00090346
Iteration 19/25 | Loss: 0.00090346
Iteration 20/25 | Loss: 0.00090346
Iteration 21/25 | Loss: 0.00090346
Iteration 22/25 | Loss: 0.00090346
Iteration 23/25 | Loss: 0.00090346
Iteration 24/25 | Loss: 0.00090346
Iteration 25/25 | Loss: 0.00090346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090346
Iteration 2/1000 | Loss: 0.00003025
Iteration 3/1000 | Loss: 0.00002106
Iteration 4/1000 | Loss: 0.00001791
Iteration 5/1000 | Loss: 0.00001693
Iteration 6/1000 | Loss: 0.00010492
Iteration 7/1000 | Loss: 0.00003206
Iteration 8/1000 | Loss: 0.00001611
Iteration 9/1000 | Loss: 0.00006659
Iteration 10/1000 | Loss: 0.00001560
Iteration 11/1000 | Loss: 0.00001533
Iteration 12/1000 | Loss: 0.00001504
Iteration 13/1000 | Loss: 0.00001497
Iteration 14/1000 | Loss: 0.00001475
Iteration 15/1000 | Loss: 0.00001456
Iteration 16/1000 | Loss: 0.00009998
Iteration 17/1000 | Loss: 0.00001442
Iteration 18/1000 | Loss: 0.00001430
Iteration 19/1000 | Loss: 0.00001428
Iteration 20/1000 | Loss: 0.00001426
Iteration 21/1000 | Loss: 0.00001424
Iteration 22/1000 | Loss: 0.00001424
Iteration 23/1000 | Loss: 0.00001424
Iteration 24/1000 | Loss: 0.00001423
Iteration 25/1000 | Loss: 0.00001423
Iteration 26/1000 | Loss: 0.00001422
Iteration 27/1000 | Loss: 0.00001422
Iteration 28/1000 | Loss: 0.00001422
Iteration 29/1000 | Loss: 0.00001422
Iteration 30/1000 | Loss: 0.00001421
Iteration 31/1000 | Loss: 0.00001421
Iteration 32/1000 | Loss: 0.00001421
Iteration 33/1000 | Loss: 0.00001420
Iteration 34/1000 | Loss: 0.00001420
Iteration 35/1000 | Loss: 0.00001420
Iteration 36/1000 | Loss: 0.00001420
Iteration 37/1000 | Loss: 0.00001420
Iteration 38/1000 | Loss: 0.00001412
Iteration 39/1000 | Loss: 0.00001412
Iteration 40/1000 | Loss: 0.00001404
Iteration 41/1000 | Loss: 0.00001400
Iteration 42/1000 | Loss: 0.00001399
Iteration 43/1000 | Loss: 0.00001399
Iteration 44/1000 | Loss: 0.00001399
Iteration 45/1000 | Loss: 0.00001399
Iteration 46/1000 | Loss: 0.00001399
Iteration 47/1000 | Loss: 0.00001398
Iteration 48/1000 | Loss: 0.00001398
Iteration 49/1000 | Loss: 0.00001397
Iteration 50/1000 | Loss: 0.00001397
Iteration 51/1000 | Loss: 0.00001396
Iteration 52/1000 | Loss: 0.00001396
Iteration 53/1000 | Loss: 0.00001396
Iteration 54/1000 | Loss: 0.00001396
Iteration 55/1000 | Loss: 0.00001396
Iteration 56/1000 | Loss: 0.00001395
Iteration 57/1000 | Loss: 0.00001393
Iteration 58/1000 | Loss: 0.00001393
Iteration 59/1000 | Loss: 0.00001393
Iteration 60/1000 | Loss: 0.00001393
Iteration 61/1000 | Loss: 0.00001392
Iteration 62/1000 | Loss: 0.00001392
Iteration 63/1000 | Loss: 0.00001392
Iteration 64/1000 | Loss: 0.00001392
Iteration 65/1000 | Loss: 0.00001392
Iteration 66/1000 | Loss: 0.00001392
Iteration 67/1000 | Loss: 0.00001392
Iteration 68/1000 | Loss: 0.00001391
Iteration 69/1000 | Loss: 0.00001391
Iteration 70/1000 | Loss: 0.00001391
Iteration 71/1000 | Loss: 0.00001391
Iteration 72/1000 | Loss: 0.00001391
Iteration 73/1000 | Loss: 0.00001391
Iteration 74/1000 | Loss: 0.00001391
Iteration 75/1000 | Loss: 0.00001391
Iteration 76/1000 | Loss: 0.00001391
Iteration 77/1000 | Loss: 0.00001391
Iteration 78/1000 | Loss: 0.00001391
Iteration 79/1000 | Loss: 0.00001391
Iteration 80/1000 | Loss: 0.00001391
Iteration 81/1000 | Loss: 0.00001391
Iteration 82/1000 | Loss: 0.00001391
Iteration 83/1000 | Loss: 0.00001391
Iteration 84/1000 | Loss: 0.00001391
Iteration 85/1000 | Loss: 0.00001391
Iteration 86/1000 | Loss: 0.00001391
Iteration 87/1000 | Loss: 0.00001391
Iteration 88/1000 | Loss: 0.00001391
Iteration 89/1000 | Loss: 0.00001391
Iteration 90/1000 | Loss: 0.00001391
Iteration 91/1000 | Loss: 0.00001391
Iteration 92/1000 | Loss: 0.00001391
Iteration 93/1000 | Loss: 0.00001391
Iteration 94/1000 | Loss: 0.00001391
Iteration 95/1000 | Loss: 0.00001391
Iteration 96/1000 | Loss: 0.00001391
Iteration 97/1000 | Loss: 0.00001391
Iteration 98/1000 | Loss: 0.00001391
Iteration 99/1000 | Loss: 0.00001391
Iteration 100/1000 | Loss: 0.00001391
Iteration 101/1000 | Loss: 0.00001391
Iteration 102/1000 | Loss: 0.00001391
Iteration 103/1000 | Loss: 0.00001391
Iteration 104/1000 | Loss: 0.00001391
Iteration 105/1000 | Loss: 0.00001391
Iteration 106/1000 | Loss: 0.00001391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.3908069377066568e-05, 1.3908069377066568e-05, 1.3908069377066568e-05, 1.3908069377066568e-05, 1.3908069377066568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3908069377066568e-05

Optimization complete. Final v2v error: 3.1959712505340576 mm

Highest mean error: 3.5511727333068848 mm for frame 49

Lowest mean error: 2.9150359630584717 mm for frame 156

Saving results

Total time: 46.15765404701233
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793626
Iteration 2/25 | Loss: 0.00194559
Iteration 3/25 | Loss: 0.00174436
Iteration 4/25 | Loss: 0.00177365
Iteration 5/25 | Loss: 0.00191720
Iteration 6/25 | Loss: 0.00163970
Iteration 7/25 | Loss: 0.00153910
Iteration 8/25 | Loss: 0.00139741
Iteration 9/25 | Loss: 0.00138177
Iteration 10/25 | Loss: 0.00132840
Iteration 11/25 | Loss: 0.00131847
Iteration 12/25 | Loss: 0.00131388
Iteration 13/25 | Loss: 0.00131003
Iteration 14/25 | Loss: 0.00130402
Iteration 15/25 | Loss: 0.00130285
Iteration 16/25 | Loss: 0.00130343
Iteration 17/25 | Loss: 0.00130439
Iteration 18/25 | Loss: 0.00130303
Iteration 19/25 | Loss: 0.00130312
Iteration 20/25 | Loss: 0.00130435
Iteration 21/25 | Loss: 0.00130301
Iteration 22/25 | Loss: 0.00130302
Iteration 23/25 | Loss: 0.00130281
Iteration 24/25 | Loss: 0.00130075
Iteration 25/25 | Loss: 0.00130061

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38683689
Iteration 2/25 | Loss: 0.00123081
Iteration 3/25 | Loss: 0.00096820
Iteration 4/25 | Loss: 0.00096820
Iteration 5/25 | Loss: 0.00096820
Iteration 6/25 | Loss: 0.00096820
Iteration 7/25 | Loss: 0.00096820
Iteration 8/25 | Loss: 0.00096820
Iteration 9/25 | Loss: 0.00096820
Iteration 10/25 | Loss: 0.00096820
Iteration 11/25 | Loss: 0.00096820
Iteration 12/25 | Loss: 0.00096820
Iteration 13/25 | Loss: 0.00096820
Iteration 14/25 | Loss: 0.00096820
Iteration 15/25 | Loss: 0.00096820
Iteration 16/25 | Loss: 0.00096820
Iteration 17/25 | Loss: 0.00096820
Iteration 18/25 | Loss: 0.00096820
Iteration 19/25 | Loss: 0.00096820
Iteration 20/25 | Loss: 0.00096820
Iteration 21/25 | Loss: 0.00096820
Iteration 22/25 | Loss: 0.00096820
Iteration 23/25 | Loss: 0.00096820
Iteration 24/25 | Loss: 0.00096820
Iteration 25/25 | Loss: 0.00096820

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096820
Iteration 2/1000 | Loss: 0.00042742
Iteration 3/1000 | Loss: 0.00009850
Iteration 4/1000 | Loss: 0.00010110
Iteration 5/1000 | Loss: 0.00004193
Iteration 6/1000 | Loss: 0.00005168
Iteration 7/1000 | Loss: 0.00011836
Iteration 8/1000 | Loss: 0.00010749
Iteration 9/1000 | Loss: 0.00015583
Iteration 10/1000 | Loss: 0.00011148
Iteration 11/1000 | Loss: 0.00003925
Iteration 12/1000 | Loss: 0.00010692
Iteration 13/1000 | Loss: 0.00012009
Iteration 14/1000 | Loss: 0.00010397
Iteration 15/1000 | Loss: 0.00012110
Iteration 16/1000 | Loss: 0.00010918
Iteration 17/1000 | Loss: 0.00011943
Iteration 18/1000 | Loss: 0.00014105
Iteration 19/1000 | Loss: 0.00012169
Iteration 20/1000 | Loss: 0.00013722
Iteration 21/1000 | Loss: 0.00013764
Iteration 22/1000 | Loss: 0.00010875
Iteration 23/1000 | Loss: 0.00012433
Iteration 24/1000 | Loss: 0.00010670
Iteration 25/1000 | Loss: 0.00008192
Iteration 26/1000 | Loss: 0.00002196
Iteration 27/1000 | Loss: 0.00001971
Iteration 28/1000 | Loss: 0.00017223
Iteration 29/1000 | Loss: 0.00001968
Iteration 30/1000 | Loss: 0.00001826
Iteration 31/1000 | Loss: 0.00001768
Iteration 32/1000 | Loss: 0.00001726
Iteration 33/1000 | Loss: 0.00001692
Iteration 34/1000 | Loss: 0.00001668
Iteration 35/1000 | Loss: 0.00001664
Iteration 36/1000 | Loss: 0.00001653
Iteration 37/1000 | Loss: 0.00001648
Iteration 38/1000 | Loss: 0.00001648
Iteration 39/1000 | Loss: 0.00001648
Iteration 40/1000 | Loss: 0.00001648
Iteration 41/1000 | Loss: 0.00001648
Iteration 42/1000 | Loss: 0.00001648
Iteration 43/1000 | Loss: 0.00001648
Iteration 44/1000 | Loss: 0.00001648
Iteration 45/1000 | Loss: 0.00001647
Iteration 46/1000 | Loss: 0.00001647
Iteration 47/1000 | Loss: 0.00001647
Iteration 48/1000 | Loss: 0.00001647
Iteration 49/1000 | Loss: 0.00001646
Iteration 50/1000 | Loss: 0.00001645
Iteration 51/1000 | Loss: 0.00001645
Iteration 52/1000 | Loss: 0.00001642
Iteration 53/1000 | Loss: 0.00001640
Iteration 54/1000 | Loss: 0.00001640
Iteration 55/1000 | Loss: 0.00001634
Iteration 56/1000 | Loss: 0.00001631
Iteration 57/1000 | Loss: 0.00001631
Iteration 58/1000 | Loss: 0.00001620
Iteration 59/1000 | Loss: 0.00001620
Iteration 60/1000 | Loss: 0.00001618
Iteration 61/1000 | Loss: 0.00001617
Iteration 62/1000 | Loss: 0.00001617
Iteration 63/1000 | Loss: 0.00001617
Iteration 64/1000 | Loss: 0.00001616
Iteration 65/1000 | Loss: 0.00001616
Iteration 66/1000 | Loss: 0.00001616
Iteration 67/1000 | Loss: 0.00001615
Iteration 68/1000 | Loss: 0.00001615
Iteration 69/1000 | Loss: 0.00001615
Iteration 70/1000 | Loss: 0.00001615
Iteration 71/1000 | Loss: 0.00001615
Iteration 72/1000 | Loss: 0.00001615
Iteration 73/1000 | Loss: 0.00001614
Iteration 74/1000 | Loss: 0.00001614
Iteration 75/1000 | Loss: 0.00001614
Iteration 76/1000 | Loss: 0.00001614
Iteration 77/1000 | Loss: 0.00001614
Iteration 78/1000 | Loss: 0.00001614
Iteration 79/1000 | Loss: 0.00001614
Iteration 80/1000 | Loss: 0.00001614
Iteration 81/1000 | Loss: 0.00001614
Iteration 82/1000 | Loss: 0.00001614
Iteration 83/1000 | Loss: 0.00001614
Iteration 84/1000 | Loss: 0.00001614
Iteration 85/1000 | Loss: 0.00001614
Iteration 86/1000 | Loss: 0.00001614
Iteration 87/1000 | Loss: 0.00001614
Iteration 88/1000 | Loss: 0.00001614
Iteration 89/1000 | Loss: 0.00001614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.6143776520038955e-05, 1.6143776520038955e-05, 1.6143776520038955e-05, 1.6143776520038955e-05, 1.6143776520038955e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6143776520038955e-05

Optimization complete. Final v2v error: 3.3533172607421875 mm

Highest mean error: 4.6769280433654785 mm for frame 152

Lowest mean error: 2.776036024093628 mm for frame 5

Saving results

Total time: 109.28168201446533
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034931
Iteration 2/25 | Loss: 0.00177781
Iteration 3/25 | Loss: 0.00149560
Iteration 4/25 | Loss: 0.00146813
Iteration 5/25 | Loss: 0.00146216
Iteration 6/25 | Loss: 0.00146096
Iteration 7/25 | Loss: 0.00146083
Iteration 8/25 | Loss: 0.00146083
Iteration 9/25 | Loss: 0.00146083
Iteration 10/25 | Loss: 0.00146083
Iteration 11/25 | Loss: 0.00146083
Iteration 12/25 | Loss: 0.00146083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001460834639146924, 0.001460834639146924, 0.001460834639146924, 0.001460834639146924, 0.001460834639146924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001460834639146924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.51959538
Iteration 2/25 | Loss: 0.00097203
Iteration 3/25 | Loss: 0.00097201
Iteration 4/25 | Loss: 0.00097201
Iteration 5/25 | Loss: 0.00097201
Iteration 6/25 | Loss: 0.00097201
Iteration 7/25 | Loss: 0.00097201
Iteration 8/25 | Loss: 0.00097201
Iteration 9/25 | Loss: 0.00097201
Iteration 10/25 | Loss: 0.00097201
Iteration 11/25 | Loss: 0.00097201
Iteration 12/25 | Loss: 0.00097201
Iteration 13/25 | Loss: 0.00097201
Iteration 14/25 | Loss: 0.00097201
Iteration 15/25 | Loss: 0.00097201
Iteration 16/25 | Loss: 0.00097201
Iteration 17/25 | Loss: 0.00097201
Iteration 18/25 | Loss: 0.00097201
Iteration 19/25 | Loss: 0.00097201
Iteration 20/25 | Loss: 0.00097201
Iteration 21/25 | Loss: 0.00097201
Iteration 22/25 | Loss: 0.00097201
Iteration 23/25 | Loss: 0.00097201
Iteration 24/25 | Loss: 0.00097201
Iteration 25/25 | Loss: 0.00097201

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097201
Iteration 2/1000 | Loss: 0.00008035
Iteration 3/1000 | Loss: 0.00004635
Iteration 4/1000 | Loss: 0.00004040
Iteration 5/1000 | Loss: 0.00003820
Iteration 6/1000 | Loss: 0.00003677
Iteration 7/1000 | Loss: 0.00003558
Iteration 8/1000 | Loss: 0.00003486
Iteration 9/1000 | Loss: 0.00003437
Iteration 10/1000 | Loss: 0.00003400
Iteration 11/1000 | Loss: 0.00003374
Iteration 12/1000 | Loss: 0.00003349
Iteration 13/1000 | Loss: 0.00003332
Iteration 14/1000 | Loss: 0.00003323
Iteration 15/1000 | Loss: 0.00003322
Iteration 16/1000 | Loss: 0.00003310
Iteration 17/1000 | Loss: 0.00003306
Iteration 18/1000 | Loss: 0.00003306
Iteration 19/1000 | Loss: 0.00003305
Iteration 20/1000 | Loss: 0.00003305
Iteration 21/1000 | Loss: 0.00003301
Iteration 22/1000 | Loss: 0.00003299
Iteration 23/1000 | Loss: 0.00003299
Iteration 24/1000 | Loss: 0.00003299
Iteration 25/1000 | Loss: 0.00003299
Iteration 26/1000 | Loss: 0.00003299
Iteration 27/1000 | Loss: 0.00003299
Iteration 28/1000 | Loss: 0.00003297
Iteration 29/1000 | Loss: 0.00003296
Iteration 30/1000 | Loss: 0.00003296
Iteration 31/1000 | Loss: 0.00003296
Iteration 32/1000 | Loss: 0.00003296
Iteration 33/1000 | Loss: 0.00003296
Iteration 34/1000 | Loss: 0.00003295
Iteration 35/1000 | Loss: 0.00003295
Iteration 36/1000 | Loss: 0.00003295
Iteration 37/1000 | Loss: 0.00003294
Iteration 38/1000 | Loss: 0.00003293
Iteration 39/1000 | Loss: 0.00003293
Iteration 40/1000 | Loss: 0.00003293
Iteration 41/1000 | Loss: 0.00003293
Iteration 42/1000 | Loss: 0.00003293
Iteration 43/1000 | Loss: 0.00003293
Iteration 44/1000 | Loss: 0.00003293
Iteration 45/1000 | Loss: 0.00003293
Iteration 46/1000 | Loss: 0.00003293
Iteration 47/1000 | Loss: 0.00003293
Iteration 48/1000 | Loss: 0.00003293
Iteration 49/1000 | Loss: 0.00003293
Iteration 50/1000 | Loss: 0.00003292
Iteration 51/1000 | Loss: 0.00003292
Iteration 52/1000 | Loss: 0.00003292
Iteration 53/1000 | Loss: 0.00003292
Iteration 54/1000 | Loss: 0.00003292
Iteration 55/1000 | Loss: 0.00003292
Iteration 56/1000 | Loss: 0.00003291
Iteration 57/1000 | Loss: 0.00003291
Iteration 58/1000 | Loss: 0.00003291
Iteration 59/1000 | Loss: 0.00003290
Iteration 60/1000 | Loss: 0.00003290
Iteration 61/1000 | Loss: 0.00003289
Iteration 62/1000 | Loss: 0.00003289
Iteration 63/1000 | Loss: 0.00003289
Iteration 64/1000 | Loss: 0.00003289
Iteration 65/1000 | Loss: 0.00003289
Iteration 66/1000 | Loss: 0.00003289
Iteration 67/1000 | Loss: 0.00003289
Iteration 68/1000 | Loss: 0.00003288
Iteration 69/1000 | Loss: 0.00003288
Iteration 70/1000 | Loss: 0.00003288
Iteration 71/1000 | Loss: 0.00003288
Iteration 72/1000 | Loss: 0.00003288
Iteration 73/1000 | Loss: 0.00003288
Iteration 74/1000 | Loss: 0.00003287
Iteration 75/1000 | Loss: 0.00003287
Iteration 76/1000 | Loss: 0.00003287
Iteration 77/1000 | Loss: 0.00003287
Iteration 78/1000 | Loss: 0.00003287
Iteration 79/1000 | Loss: 0.00003287
Iteration 80/1000 | Loss: 0.00003286
Iteration 81/1000 | Loss: 0.00003286
Iteration 82/1000 | Loss: 0.00003286
Iteration 83/1000 | Loss: 0.00003286
Iteration 84/1000 | Loss: 0.00003286
Iteration 85/1000 | Loss: 0.00003286
Iteration 86/1000 | Loss: 0.00003285
Iteration 87/1000 | Loss: 0.00003285
Iteration 88/1000 | Loss: 0.00003285
Iteration 89/1000 | Loss: 0.00003285
Iteration 90/1000 | Loss: 0.00003285
Iteration 91/1000 | Loss: 0.00003285
Iteration 92/1000 | Loss: 0.00003285
Iteration 93/1000 | Loss: 0.00003285
Iteration 94/1000 | Loss: 0.00003285
Iteration 95/1000 | Loss: 0.00003285
Iteration 96/1000 | Loss: 0.00003285
Iteration 97/1000 | Loss: 0.00003284
Iteration 98/1000 | Loss: 0.00003284
Iteration 99/1000 | Loss: 0.00003284
Iteration 100/1000 | Loss: 0.00003284
Iteration 101/1000 | Loss: 0.00003284
Iteration 102/1000 | Loss: 0.00003284
Iteration 103/1000 | Loss: 0.00003283
Iteration 104/1000 | Loss: 0.00003283
Iteration 105/1000 | Loss: 0.00003283
Iteration 106/1000 | Loss: 0.00003283
Iteration 107/1000 | Loss: 0.00003283
Iteration 108/1000 | Loss: 0.00003283
Iteration 109/1000 | Loss: 0.00003283
Iteration 110/1000 | Loss: 0.00003282
Iteration 111/1000 | Loss: 0.00003282
Iteration 112/1000 | Loss: 0.00003282
Iteration 113/1000 | Loss: 0.00003282
Iteration 114/1000 | Loss: 0.00003282
Iteration 115/1000 | Loss: 0.00003282
Iteration 116/1000 | Loss: 0.00003282
Iteration 117/1000 | Loss: 0.00003282
Iteration 118/1000 | Loss: 0.00003282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [3.282283432781696e-05, 3.282283432781696e-05, 3.282283432781696e-05, 3.282283432781696e-05, 3.282283432781696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.282283432781696e-05

Optimization complete. Final v2v error: 4.758106708526611 mm

Highest mean error: 5.638486862182617 mm for frame 194

Lowest mean error: 4.2466139793396 mm for frame 100

Saving results

Total time: 43.01497960090637
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454324
Iteration 2/25 | Loss: 0.00138725
Iteration 3/25 | Loss: 0.00131781
Iteration 4/25 | Loss: 0.00130455
Iteration 5/25 | Loss: 0.00130092
Iteration 6/25 | Loss: 0.00130092
Iteration 7/25 | Loss: 0.00130092
Iteration 8/25 | Loss: 0.00130092
Iteration 9/25 | Loss: 0.00130092
Iteration 10/25 | Loss: 0.00130092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013009213143959641, 0.0013009213143959641, 0.0013009213143959641, 0.0013009213143959641, 0.0013009213143959641]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013009213143959641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41152167
Iteration 2/25 | Loss: 0.00093240
Iteration 3/25 | Loss: 0.00093240
Iteration 4/25 | Loss: 0.00093240
Iteration 5/25 | Loss: 0.00093240
Iteration 6/25 | Loss: 0.00093240
Iteration 7/25 | Loss: 0.00093240
Iteration 8/25 | Loss: 0.00093240
Iteration 9/25 | Loss: 0.00093240
Iteration 10/25 | Loss: 0.00093240
Iteration 11/25 | Loss: 0.00093240
Iteration 12/25 | Loss: 0.00093240
Iteration 13/25 | Loss: 0.00093240
Iteration 14/25 | Loss: 0.00093240
Iteration 15/25 | Loss: 0.00093240
Iteration 16/25 | Loss: 0.00093240
Iteration 17/25 | Loss: 0.00093240
Iteration 18/25 | Loss: 0.00093240
Iteration 19/25 | Loss: 0.00093240
Iteration 20/25 | Loss: 0.00093240
Iteration 21/25 | Loss: 0.00093240
Iteration 22/25 | Loss: 0.00093240
Iteration 23/25 | Loss: 0.00093240
Iteration 24/25 | Loss: 0.00093240
Iteration 25/25 | Loss: 0.00093240

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093240
Iteration 2/1000 | Loss: 0.00002434
Iteration 3/1000 | Loss: 0.00001950
Iteration 4/1000 | Loss: 0.00001811
Iteration 5/1000 | Loss: 0.00001729
Iteration 6/1000 | Loss: 0.00001681
Iteration 7/1000 | Loss: 0.00001657
Iteration 8/1000 | Loss: 0.00001636
Iteration 9/1000 | Loss: 0.00001636
Iteration 10/1000 | Loss: 0.00001612
Iteration 11/1000 | Loss: 0.00001592
Iteration 12/1000 | Loss: 0.00001588
Iteration 13/1000 | Loss: 0.00001586
Iteration 14/1000 | Loss: 0.00001581
Iteration 15/1000 | Loss: 0.00001579
Iteration 16/1000 | Loss: 0.00001576
Iteration 17/1000 | Loss: 0.00001576
Iteration 18/1000 | Loss: 0.00001575
Iteration 19/1000 | Loss: 0.00001573
Iteration 20/1000 | Loss: 0.00001572
Iteration 21/1000 | Loss: 0.00001558
Iteration 22/1000 | Loss: 0.00001552
Iteration 23/1000 | Loss: 0.00001548
Iteration 24/1000 | Loss: 0.00001546
Iteration 25/1000 | Loss: 0.00001545
Iteration 26/1000 | Loss: 0.00001544
Iteration 27/1000 | Loss: 0.00001543
Iteration 28/1000 | Loss: 0.00001543
Iteration 29/1000 | Loss: 0.00001542
Iteration 30/1000 | Loss: 0.00001541
Iteration 31/1000 | Loss: 0.00001541
Iteration 32/1000 | Loss: 0.00001540
Iteration 33/1000 | Loss: 0.00001539
Iteration 34/1000 | Loss: 0.00001539
Iteration 35/1000 | Loss: 0.00001539
Iteration 36/1000 | Loss: 0.00001538
Iteration 37/1000 | Loss: 0.00001538
Iteration 38/1000 | Loss: 0.00001536
Iteration 39/1000 | Loss: 0.00001535
Iteration 40/1000 | Loss: 0.00001535
Iteration 41/1000 | Loss: 0.00001533
Iteration 42/1000 | Loss: 0.00001533
Iteration 43/1000 | Loss: 0.00001532
Iteration 44/1000 | Loss: 0.00001529
Iteration 45/1000 | Loss: 0.00001529
Iteration 46/1000 | Loss: 0.00001528
Iteration 47/1000 | Loss: 0.00001528
Iteration 48/1000 | Loss: 0.00001528
Iteration 49/1000 | Loss: 0.00001527
Iteration 50/1000 | Loss: 0.00001527
Iteration 51/1000 | Loss: 0.00001524
Iteration 52/1000 | Loss: 0.00001524
Iteration 53/1000 | Loss: 0.00001522
Iteration 54/1000 | Loss: 0.00001522
Iteration 55/1000 | Loss: 0.00001522
Iteration 56/1000 | Loss: 0.00001522
Iteration 57/1000 | Loss: 0.00001522
Iteration 58/1000 | Loss: 0.00001522
Iteration 59/1000 | Loss: 0.00001522
Iteration 60/1000 | Loss: 0.00001522
Iteration 61/1000 | Loss: 0.00001522
Iteration 62/1000 | Loss: 0.00001522
Iteration 63/1000 | Loss: 0.00001522
Iteration 64/1000 | Loss: 0.00001522
Iteration 65/1000 | Loss: 0.00001521
Iteration 66/1000 | Loss: 0.00001521
Iteration 67/1000 | Loss: 0.00001520
Iteration 68/1000 | Loss: 0.00001520
Iteration 69/1000 | Loss: 0.00001520
Iteration 70/1000 | Loss: 0.00001519
Iteration 71/1000 | Loss: 0.00001517
Iteration 72/1000 | Loss: 0.00001517
Iteration 73/1000 | Loss: 0.00001517
Iteration 74/1000 | Loss: 0.00001517
Iteration 75/1000 | Loss: 0.00001516
Iteration 76/1000 | Loss: 0.00001516
Iteration 77/1000 | Loss: 0.00001516
Iteration 78/1000 | Loss: 0.00001515
Iteration 79/1000 | Loss: 0.00001515
Iteration 80/1000 | Loss: 0.00001514
Iteration 81/1000 | Loss: 0.00001514
Iteration 82/1000 | Loss: 0.00001513
Iteration 83/1000 | Loss: 0.00001513
Iteration 84/1000 | Loss: 0.00001513
Iteration 85/1000 | Loss: 0.00001513
Iteration 86/1000 | Loss: 0.00001513
Iteration 87/1000 | Loss: 0.00001513
Iteration 88/1000 | Loss: 0.00001513
Iteration 89/1000 | Loss: 0.00001513
Iteration 90/1000 | Loss: 0.00001513
Iteration 91/1000 | Loss: 0.00001512
Iteration 92/1000 | Loss: 0.00001512
Iteration 93/1000 | Loss: 0.00001512
Iteration 94/1000 | Loss: 0.00001512
Iteration 95/1000 | Loss: 0.00001511
Iteration 96/1000 | Loss: 0.00001511
Iteration 97/1000 | Loss: 0.00001511
Iteration 98/1000 | Loss: 0.00001511
Iteration 99/1000 | Loss: 0.00001511
Iteration 100/1000 | Loss: 0.00001510
Iteration 101/1000 | Loss: 0.00001510
Iteration 102/1000 | Loss: 0.00001510
Iteration 103/1000 | Loss: 0.00001510
Iteration 104/1000 | Loss: 0.00001510
Iteration 105/1000 | Loss: 0.00001510
Iteration 106/1000 | Loss: 0.00001509
Iteration 107/1000 | Loss: 0.00001509
Iteration 108/1000 | Loss: 0.00001508
Iteration 109/1000 | Loss: 0.00001508
Iteration 110/1000 | Loss: 0.00001508
Iteration 111/1000 | Loss: 0.00001508
Iteration 112/1000 | Loss: 0.00001508
Iteration 113/1000 | Loss: 0.00001508
Iteration 114/1000 | Loss: 0.00001507
Iteration 115/1000 | Loss: 0.00001507
Iteration 116/1000 | Loss: 0.00001507
Iteration 117/1000 | Loss: 0.00001507
Iteration 118/1000 | Loss: 0.00001506
Iteration 119/1000 | Loss: 0.00001506
Iteration 120/1000 | Loss: 0.00001505
Iteration 121/1000 | Loss: 0.00001505
Iteration 122/1000 | Loss: 0.00001504
Iteration 123/1000 | Loss: 0.00001504
Iteration 124/1000 | Loss: 0.00001504
Iteration 125/1000 | Loss: 0.00001503
Iteration 126/1000 | Loss: 0.00001503
Iteration 127/1000 | Loss: 0.00001503
Iteration 128/1000 | Loss: 0.00001503
Iteration 129/1000 | Loss: 0.00001502
Iteration 130/1000 | Loss: 0.00001502
Iteration 131/1000 | Loss: 0.00001502
Iteration 132/1000 | Loss: 0.00001502
Iteration 133/1000 | Loss: 0.00001500
Iteration 134/1000 | Loss: 0.00001500
Iteration 135/1000 | Loss: 0.00001500
Iteration 136/1000 | Loss: 0.00001500
Iteration 137/1000 | Loss: 0.00001500
Iteration 138/1000 | Loss: 0.00001499
Iteration 139/1000 | Loss: 0.00001499
Iteration 140/1000 | Loss: 0.00001499
Iteration 141/1000 | Loss: 0.00001499
Iteration 142/1000 | Loss: 0.00001499
Iteration 143/1000 | Loss: 0.00001499
Iteration 144/1000 | Loss: 0.00001499
Iteration 145/1000 | Loss: 0.00001499
Iteration 146/1000 | Loss: 0.00001498
Iteration 147/1000 | Loss: 0.00001498
Iteration 148/1000 | Loss: 0.00001498
Iteration 149/1000 | Loss: 0.00001498
Iteration 150/1000 | Loss: 0.00001498
Iteration 151/1000 | Loss: 0.00001498
Iteration 152/1000 | Loss: 0.00001498
Iteration 153/1000 | Loss: 0.00001498
Iteration 154/1000 | Loss: 0.00001498
Iteration 155/1000 | Loss: 0.00001498
Iteration 156/1000 | Loss: 0.00001497
Iteration 157/1000 | Loss: 0.00001497
Iteration 158/1000 | Loss: 0.00001497
Iteration 159/1000 | Loss: 0.00001497
Iteration 160/1000 | Loss: 0.00001497
Iteration 161/1000 | Loss: 0.00001497
Iteration 162/1000 | Loss: 0.00001496
Iteration 163/1000 | Loss: 0.00001496
Iteration 164/1000 | Loss: 0.00001496
Iteration 165/1000 | Loss: 0.00001496
Iteration 166/1000 | Loss: 0.00001496
Iteration 167/1000 | Loss: 0.00001496
Iteration 168/1000 | Loss: 0.00001496
Iteration 169/1000 | Loss: 0.00001496
Iteration 170/1000 | Loss: 0.00001496
Iteration 171/1000 | Loss: 0.00001496
Iteration 172/1000 | Loss: 0.00001496
Iteration 173/1000 | Loss: 0.00001495
Iteration 174/1000 | Loss: 0.00001495
Iteration 175/1000 | Loss: 0.00001495
Iteration 176/1000 | Loss: 0.00001495
Iteration 177/1000 | Loss: 0.00001495
Iteration 178/1000 | Loss: 0.00001495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.4953870959288906e-05, 1.4953870959288906e-05, 1.4953870959288906e-05, 1.4953870959288906e-05, 1.4953870959288906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4953870959288906e-05

Optimization complete. Final v2v error: 3.2369580268859863 mm

Highest mean error: 3.535881519317627 mm for frame 97

Lowest mean error: 3.02970552444458 mm for frame 1

Saving results

Total time: 43.335787534713745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025857
Iteration 2/25 | Loss: 0.00178381
Iteration 3/25 | Loss: 0.00148840
Iteration 4/25 | Loss: 0.00146513
Iteration 5/25 | Loss: 0.00142968
Iteration 6/25 | Loss: 0.00139373
Iteration 7/25 | Loss: 0.00138991
Iteration 8/25 | Loss: 0.00137263
Iteration 9/25 | Loss: 0.00136031
Iteration 10/25 | Loss: 0.00135928
Iteration 11/25 | Loss: 0.00136196
Iteration 12/25 | Loss: 0.00136089
Iteration 13/25 | Loss: 0.00136026
Iteration 14/25 | Loss: 0.00136138
Iteration 15/25 | Loss: 0.00136065
Iteration 16/25 | Loss: 0.00135872
Iteration 17/25 | Loss: 0.00135863
Iteration 18/25 | Loss: 0.00135862
Iteration 19/25 | Loss: 0.00135862
Iteration 20/25 | Loss: 0.00135862
Iteration 21/25 | Loss: 0.00135862
Iteration 22/25 | Loss: 0.00135862
Iteration 23/25 | Loss: 0.00135862
Iteration 24/25 | Loss: 0.00135861
Iteration 25/25 | Loss: 0.00135861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64936161
Iteration 2/25 | Loss: 0.00119127
Iteration 3/25 | Loss: 0.00109181
Iteration 4/25 | Loss: 0.00109181
Iteration 5/25 | Loss: 0.00109181
Iteration 6/25 | Loss: 0.00109181
Iteration 7/25 | Loss: 0.00109181
Iteration 8/25 | Loss: 0.00109181
Iteration 9/25 | Loss: 0.00109180
Iteration 10/25 | Loss: 0.00109180
Iteration 11/25 | Loss: 0.00109180
Iteration 12/25 | Loss: 0.00109180
Iteration 13/25 | Loss: 0.00109180
Iteration 14/25 | Loss: 0.00109180
Iteration 15/25 | Loss: 0.00109180
Iteration 16/25 | Loss: 0.00109180
Iteration 17/25 | Loss: 0.00109180
Iteration 18/25 | Loss: 0.00109180
Iteration 19/25 | Loss: 0.00109180
Iteration 20/25 | Loss: 0.00109180
Iteration 21/25 | Loss: 0.00109180
Iteration 22/25 | Loss: 0.00109180
Iteration 23/25 | Loss: 0.00109180
Iteration 24/25 | Loss: 0.00109180
Iteration 25/25 | Loss: 0.00109180

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109180
Iteration 2/1000 | Loss: 0.00003701
Iteration 3/1000 | Loss: 0.00008743
Iteration 4/1000 | Loss: 0.00002402
Iteration 5/1000 | Loss: 0.00007192
Iteration 6/1000 | Loss: 0.00015029
Iteration 7/1000 | Loss: 0.00002772
Iteration 8/1000 | Loss: 0.00006092
Iteration 9/1000 | Loss: 0.00002983
Iteration 10/1000 | Loss: 0.00003359
Iteration 11/1000 | Loss: 0.00002134
Iteration 12/1000 | Loss: 0.00008884
Iteration 13/1000 | Loss: 0.00002108
Iteration 14/1000 | Loss: 0.00002078
Iteration 15/1000 | Loss: 0.00002055
Iteration 16/1000 | Loss: 0.00006598
Iteration 17/1000 | Loss: 0.00002031
Iteration 18/1000 | Loss: 0.00002006
Iteration 19/1000 | Loss: 0.00001986
Iteration 20/1000 | Loss: 0.00001971
Iteration 21/1000 | Loss: 0.00001961
Iteration 22/1000 | Loss: 0.00001957
Iteration 23/1000 | Loss: 0.00006345
Iteration 24/1000 | Loss: 0.00001962
Iteration 25/1000 | Loss: 0.00001940
Iteration 26/1000 | Loss: 0.00001937
Iteration 27/1000 | Loss: 0.00001937
Iteration 28/1000 | Loss: 0.00001937
Iteration 29/1000 | Loss: 0.00001937
Iteration 30/1000 | Loss: 0.00001937
Iteration 31/1000 | Loss: 0.00001936
Iteration 32/1000 | Loss: 0.00001936
Iteration 33/1000 | Loss: 0.00001936
Iteration 34/1000 | Loss: 0.00001936
Iteration 35/1000 | Loss: 0.00001935
Iteration 36/1000 | Loss: 0.00001932
Iteration 37/1000 | Loss: 0.00001932
Iteration 38/1000 | Loss: 0.00001931
Iteration 39/1000 | Loss: 0.00001931
Iteration 40/1000 | Loss: 0.00001931
Iteration 41/1000 | Loss: 0.00001930
Iteration 42/1000 | Loss: 0.00001930
Iteration 43/1000 | Loss: 0.00001928
Iteration 44/1000 | Loss: 0.00001927
Iteration 45/1000 | Loss: 0.00001925
Iteration 46/1000 | Loss: 0.00007788
Iteration 47/1000 | Loss: 0.00003117
Iteration 48/1000 | Loss: 0.00001929
Iteration 49/1000 | Loss: 0.00003892
Iteration 50/1000 | Loss: 0.00002563
Iteration 51/1000 | Loss: 0.00004528
Iteration 52/1000 | Loss: 0.00002408
Iteration 53/1000 | Loss: 0.00002427
Iteration 54/1000 | Loss: 0.00001915
Iteration 55/1000 | Loss: 0.00001915
Iteration 56/1000 | Loss: 0.00001915
Iteration 57/1000 | Loss: 0.00001915
Iteration 58/1000 | Loss: 0.00001915
Iteration 59/1000 | Loss: 0.00001915
Iteration 60/1000 | Loss: 0.00001915
Iteration 61/1000 | Loss: 0.00001915
Iteration 62/1000 | Loss: 0.00001915
Iteration 63/1000 | Loss: 0.00001915
Iteration 64/1000 | Loss: 0.00001915
Iteration 65/1000 | Loss: 0.00001915
Iteration 66/1000 | Loss: 0.00001914
Iteration 67/1000 | Loss: 0.00001914
Iteration 68/1000 | Loss: 0.00001914
Iteration 69/1000 | Loss: 0.00001914
Iteration 70/1000 | Loss: 0.00001914
Iteration 71/1000 | Loss: 0.00001914
Iteration 72/1000 | Loss: 0.00001914
Iteration 73/1000 | Loss: 0.00001914
Iteration 74/1000 | Loss: 0.00001914
Iteration 75/1000 | Loss: 0.00001914
Iteration 76/1000 | Loss: 0.00001914
Iteration 77/1000 | Loss: 0.00001914
Iteration 78/1000 | Loss: 0.00001914
Iteration 79/1000 | Loss: 0.00001914
Iteration 80/1000 | Loss: 0.00001914
Iteration 81/1000 | Loss: 0.00001913
Iteration 82/1000 | Loss: 0.00001913
Iteration 83/1000 | Loss: 0.00001913
Iteration 84/1000 | Loss: 0.00001913
Iteration 85/1000 | Loss: 0.00001913
Iteration 86/1000 | Loss: 0.00001913
Iteration 87/1000 | Loss: 0.00001913
Iteration 88/1000 | Loss: 0.00001913
Iteration 89/1000 | Loss: 0.00001913
Iteration 90/1000 | Loss: 0.00001913
Iteration 91/1000 | Loss: 0.00001913
Iteration 92/1000 | Loss: 0.00001913
Iteration 93/1000 | Loss: 0.00001912
Iteration 94/1000 | Loss: 0.00001912
Iteration 95/1000 | Loss: 0.00001912
Iteration 96/1000 | Loss: 0.00001912
Iteration 97/1000 | Loss: 0.00001912
Iteration 98/1000 | Loss: 0.00001912
Iteration 99/1000 | Loss: 0.00001912
Iteration 100/1000 | Loss: 0.00001912
Iteration 101/1000 | Loss: 0.00001912
Iteration 102/1000 | Loss: 0.00001912
Iteration 103/1000 | Loss: 0.00001912
Iteration 104/1000 | Loss: 0.00001912
Iteration 105/1000 | Loss: 0.00001912
Iteration 106/1000 | Loss: 0.00001912
Iteration 107/1000 | Loss: 0.00001912
Iteration 108/1000 | Loss: 0.00001912
Iteration 109/1000 | Loss: 0.00001912
Iteration 110/1000 | Loss: 0.00001912
Iteration 111/1000 | Loss: 0.00001911
Iteration 112/1000 | Loss: 0.00001911
Iteration 113/1000 | Loss: 0.00001911
Iteration 114/1000 | Loss: 0.00001911
Iteration 115/1000 | Loss: 0.00001910
Iteration 116/1000 | Loss: 0.00001910
Iteration 117/1000 | Loss: 0.00001910
Iteration 118/1000 | Loss: 0.00001910
Iteration 119/1000 | Loss: 0.00003691
Iteration 120/1000 | Loss: 0.00003691
Iteration 121/1000 | Loss: 0.00023479
Iteration 122/1000 | Loss: 0.00084630
Iteration 123/1000 | Loss: 0.00003784
Iteration 124/1000 | Loss: 0.00001968
Iteration 125/1000 | Loss: 0.00006709
Iteration 126/1000 | Loss: 0.00003259
Iteration 127/1000 | Loss: 0.00005716
Iteration 128/1000 | Loss: 0.00001911
Iteration 129/1000 | Loss: 0.00001907
Iteration 130/1000 | Loss: 0.00001907
Iteration 131/1000 | Loss: 0.00001907
Iteration 132/1000 | Loss: 0.00001907
Iteration 133/1000 | Loss: 0.00001907
Iteration 134/1000 | Loss: 0.00001906
Iteration 135/1000 | Loss: 0.00001906
Iteration 136/1000 | Loss: 0.00001906
Iteration 137/1000 | Loss: 0.00001906
Iteration 138/1000 | Loss: 0.00001905
Iteration 139/1000 | Loss: 0.00001905
Iteration 140/1000 | Loss: 0.00001905
Iteration 141/1000 | Loss: 0.00001904
Iteration 142/1000 | Loss: 0.00001904
Iteration 143/1000 | Loss: 0.00001904
Iteration 144/1000 | Loss: 0.00001904
Iteration 145/1000 | Loss: 0.00001904
Iteration 146/1000 | Loss: 0.00001904
Iteration 147/1000 | Loss: 0.00001904
Iteration 148/1000 | Loss: 0.00001903
Iteration 149/1000 | Loss: 0.00001903
Iteration 150/1000 | Loss: 0.00001903
Iteration 151/1000 | Loss: 0.00001903
Iteration 152/1000 | Loss: 0.00001902
Iteration 153/1000 | Loss: 0.00001902
Iteration 154/1000 | Loss: 0.00001902
Iteration 155/1000 | Loss: 0.00001902
Iteration 156/1000 | Loss: 0.00001902
Iteration 157/1000 | Loss: 0.00001902
Iteration 158/1000 | Loss: 0.00001902
Iteration 159/1000 | Loss: 0.00001902
Iteration 160/1000 | Loss: 0.00001902
Iteration 161/1000 | Loss: 0.00001902
Iteration 162/1000 | Loss: 0.00001902
Iteration 163/1000 | Loss: 0.00001902
Iteration 164/1000 | Loss: 0.00001902
Iteration 165/1000 | Loss: 0.00001902
Iteration 166/1000 | Loss: 0.00001902
Iteration 167/1000 | Loss: 0.00001902
Iteration 168/1000 | Loss: 0.00001902
Iteration 169/1000 | Loss: 0.00001901
Iteration 170/1000 | Loss: 0.00001901
Iteration 171/1000 | Loss: 0.00001901
Iteration 172/1000 | Loss: 0.00001901
Iteration 173/1000 | Loss: 0.00001901
Iteration 174/1000 | Loss: 0.00001901
Iteration 175/1000 | Loss: 0.00001901
Iteration 176/1000 | Loss: 0.00001901
Iteration 177/1000 | Loss: 0.00001901
Iteration 178/1000 | Loss: 0.00001901
Iteration 179/1000 | Loss: 0.00003770
Iteration 180/1000 | Loss: 0.00003770
Iteration 181/1000 | Loss: 0.00036683
Iteration 182/1000 | Loss: 0.00012762
Iteration 183/1000 | Loss: 0.00001991
Iteration 184/1000 | Loss: 0.00015320
Iteration 185/1000 | Loss: 0.00008737
Iteration 186/1000 | Loss: 0.00003300
Iteration 187/1000 | Loss: 0.00008999
Iteration 188/1000 | Loss: 0.00025218
Iteration 189/1000 | Loss: 0.00011851
Iteration 190/1000 | Loss: 0.00010016
Iteration 191/1000 | Loss: 0.00012163
Iteration 192/1000 | Loss: 0.00011347
Iteration 193/1000 | Loss: 0.00017218
Iteration 194/1000 | Loss: 0.00008877
Iteration 195/1000 | Loss: 0.00059876
Iteration 196/1000 | Loss: 0.00017990
Iteration 197/1000 | Loss: 0.00012522
Iteration 198/1000 | Loss: 0.00017660
Iteration 199/1000 | Loss: 0.00006837
Iteration 200/1000 | Loss: 0.00004610
Iteration 201/1000 | Loss: 0.00002522
Iteration 202/1000 | Loss: 0.00011598
Iteration 203/1000 | Loss: 0.00005157
Iteration 204/1000 | Loss: 0.00002481
Iteration 205/1000 | Loss: 0.00013775
Iteration 206/1000 | Loss: 0.00013980
Iteration 207/1000 | Loss: 0.00015662
Iteration 208/1000 | Loss: 0.00010019
Iteration 209/1000 | Loss: 0.00010798
Iteration 210/1000 | Loss: 0.00014757
Iteration 211/1000 | Loss: 0.00010267
Iteration 212/1000 | Loss: 0.00005685
Iteration 213/1000 | Loss: 0.00014720
Iteration 214/1000 | Loss: 0.00008307
Iteration 215/1000 | Loss: 0.00012373
Iteration 216/1000 | Loss: 0.00009557
Iteration 217/1000 | Loss: 0.00007931
Iteration 218/1000 | Loss: 0.00006564
Iteration 219/1000 | Loss: 0.00021888
Iteration 220/1000 | Loss: 0.00006865
Iteration 221/1000 | Loss: 0.00008708
Iteration 222/1000 | Loss: 0.00022124
Iteration 223/1000 | Loss: 0.00007565
Iteration 224/1000 | Loss: 0.00011668
Iteration 225/1000 | Loss: 0.00010763
Iteration 226/1000 | Loss: 0.00011166
Iteration 227/1000 | Loss: 0.00012943
Iteration 228/1000 | Loss: 0.00011689
Iteration 229/1000 | Loss: 0.00011898
Iteration 230/1000 | Loss: 0.00013285
Iteration 231/1000 | Loss: 0.00011593
Iteration 232/1000 | Loss: 0.00009351
Iteration 233/1000 | Loss: 0.00021873
Iteration 234/1000 | Loss: 0.00013056
Iteration 235/1000 | Loss: 0.00013620
Iteration 236/1000 | Loss: 0.00010703
Iteration 237/1000 | Loss: 0.00012450
Iteration 238/1000 | Loss: 0.00002631
Iteration 239/1000 | Loss: 0.00002414
Iteration 240/1000 | Loss: 0.00007020
Iteration 241/1000 | Loss: 0.00009353
Iteration 242/1000 | Loss: 0.00014843
Iteration 243/1000 | Loss: 0.00017945
Iteration 244/1000 | Loss: 0.00012561
Iteration 245/1000 | Loss: 0.00003015
Iteration 246/1000 | Loss: 0.00002148
Iteration 247/1000 | Loss: 0.00010823
Iteration 248/1000 | Loss: 0.00002932
Iteration 249/1000 | Loss: 0.00009437
Iteration 250/1000 | Loss: 0.00010120
Iteration 251/1000 | Loss: 0.00012618
Iteration 252/1000 | Loss: 0.00010000
Iteration 253/1000 | Loss: 0.00010285
Iteration 254/1000 | Loss: 0.00014549
Iteration 255/1000 | Loss: 0.00006579
Iteration 256/1000 | Loss: 0.00010086
Iteration 257/1000 | Loss: 0.00011838
Iteration 258/1000 | Loss: 0.00010990
Iteration 259/1000 | Loss: 0.00011540
Iteration 260/1000 | Loss: 0.00007066
Iteration 261/1000 | Loss: 0.00010362
Iteration 262/1000 | Loss: 0.00016340
Iteration 263/1000 | Loss: 0.00033093
Iteration 264/1000 | Loss: 0.00013321
Iteration 265/1000 | Loss: 0.00015166
Iteration 266/1000 | Loss: 0.00011125
Iteration 267/1000 | Loss: 0.00002381
Iteration 268/1000 | Loss: 0.00002209
Iteration 269/1000 | Loss: 0.00002098
Iteration 270/1000 | Loss: 0.00023136
Iteration 271/1000 | Loss: 0.00006115
Iteration 272/1000 | Loss: 0.00005726
Iteration 273/1000 | Loss: 0.00010071
Iteration 274/1000 | Loss: 0.00004630
Iteration 275/1000 | Loss: 0.00003510
Iteration 276/1000 | Loss: 0.00002287
Iteration 277/1000 | Loss: 0.00002206
Iteration 278/1000 | Loss: 0.00002155
Iteration 279/1000 | Loss: 0.00003485
Iteration 280/1000 | Loss: 0.00006389
Iteration 281/1000 | Loss: 0.00002206
Iteration 282/1000 | Loss: 0.00003633
Iteration 283/1000 | Loss: 0.00003505
Iteration 284/1000 | Loss: 0.00002621
Iteration 285/1000 | Loss: 0.00003692
Iteration 286/1000 | Loss: 0.00003235
Iteration 287/1000 | Loss: 0.00003477
Iteration 288/1000 | Loss: 0.00003533
Iteration 289/1000 | Loss: 0.00002341
Iteration 290/1000 | Loss: 0.00003223
Iteration 291/1000 | Loss: 0.00002814
Iteration 292/1000 | Loss: 0.00003243
Iteration 293/1000 | Loss: 0.00002651
Iteration 294/1000 | Loss: 0.00004032
Iteration 295/1000 | Loss: 0.00002082
Iteration 296/1000 | Loss: 0.00002012
Iteration 297/1000 | Loss: 0.00001957
Iteration 298/1000 | Loss: 0.00001935
Iteration 299/1000 | Loss: 0.00001916
Iteration 300/1000 | Loss: 0.00001898
Iteration 301/1000 | Loss: 0.00001888
Iteration 302/1000 | Loss: 0.00001887
Iteration 303/1000 | Loss: 0.00001886
Iteration 304/1000 | Loss: 0.00001886
Iteration 305/1000 | Loss: 0.00001883
Iteration 306/1000 | Loss: 0.00001872
Iteration 307/1000 | Loss: 0.00001871
Iteration 308/1000 | Loss: 0.00001871
Iteration 309/1000 | Loss: 0.00001871
Iteration 310/1000 | Loss: 0.00001870
Iteration 311/1000 | Loss: 0.00001870
Iteration 312/1000 | Loss: 0.00001869
Iteration 313/1000 | Loss: 0.00001868
Iteration 314/1000 | Loss: 0.00001868
Iteration 315/1000 | Loss: 0.00001868
Iteration 316/1000 | Loss: 0.00001867
Iteration 317/1000 | Loss: 0.00001867
Iteration 318/1000 | Loss: 0.00001866
Iteration 319/1000 | Loss: 0.00001866
Iteration 320/1000 | Loss: 0.00001866
Iteration 321/1000 | Loss: 0.00001866
Iteration 322/1000 | Loss: 0.00001866
Iteration 323/1000 | Loss: 0.00001866
Iteration 324/1000 | Loss: 0.00001866
Iteration 325/1000 | Loss: 0.00001866
Iteration 326/1000 | Loss: 0.00001866
Iteration 327/1000 | Loss: 0.00001866
Iteration 328/1000 | Loss: 0.00001866
Iteration 329/1000 | Loss: 0.00001865
Iteration 330/1000 | Loss: 0.00001865
Iteration 331/1000 | Loss: 0.00001865
Iteration 332/1000 | Loss: 0.00001864
Iteration 333/1000 | Loss: 0.00001864
Iteration 334/1000 | Loss: 0.00001864
Iteration 335/1000 | Loss: 0.00001864
Iteration 336/1000 | Loss: 0.00001864
Iteration 337/1000 | Loss: 0.00001864
Iteration 338/1000 | Loss: 0.00001864
Iteration 339/1000 | Loss: 0.00001864
Iteration 340/1000 | Loss: 0.00001863
Iteration 341/1000 | Loss: 0.00001863
Iteration 342/1000 | Loss: 0.00001863
Iteration 343/1000 | Loss: 0.00001863
Iteration 344/1000 | Loss: 0.00001863
Iteration 345/1000 | Loss: 0.00001863
Iteration 346/1000 | Loss: 0.00001863
Iteration 347/1000 | Loss: 0.00001863
Iteration 348/1000 | Loss: 0.00001863
Iteration 349/1000 | Loss: 0.00001863
Iteration 350/1000 | Loss: 0.00001863
Iteration 351/1000 | Loss: 0.00001863
Iteration 352/1000 | Loss: 0.00001863
Iteration 353/1000 | Loss: 0.00001863
Iteration 354/1000 | Loss: 0.00001863
Iteration 355/1000 | Loss: 0.00001863
Iteration 356/1000 | Loss: 0.00001862
Iteration 357/1000 | Loss: 0.00001862
Iteration 358/1000 | Loss: 0.00001862
Iteration 359/1000 | Loss: 0.00001862
Iteration 360/1000 | Loss: 0.00001862
Iteration 361/1000 | Loss: 0.00001862
Iteration 362/1000 | Loss: 0.00001862
Iteration 363/1000 | Loss: 0.00001862
Iteration 364/1000 | Loss: 0.00001862
Iteration 365/1000 | Loss: 0.00001862
Iteration 366/1000 | Loss: 0.00001862
Iteration 367/1000 | Loss: 0.00001862
Iteration 368/1000 | Loss: 0.00001862
Iteration 369/1000 | Loss: 0.00001862
Iteration 370/1000 | Loss: 0.00001862
Iteration 371/1000 | Loss: 0.00001862
Iteration 372/1000 | Loss: 0.00001862
Iteration 373/1000 | Loss: 0.00001862
Iteration 374/1000 | Loss: 0.00001862
Iteration 375/1000 | Loss: 0.00001862
Iteration 376/1000 | Loss: 0.00001862
Iteration 377/1000 | Loss: 0.00001862
Iteration 378/1000 | Loss: 0.00001862
Iteration 379/1000 | Loss: 0.00001862
Iteration 380/1000 | Loss: 0.00001862
Iteration 381/1000 | Loss: 0.00001862
Iteration 382/1000 | Loss: 0.00001862
Iteration 383/1000 | Loss: 0.00001862
Iteration 384/1000 | Loss: 0.00001862
Iteration 385/1000 | Loss: 0.00001862
Iteration 386/1000 | Loss: 0.00001862
Iteration 387/1000 | Loss: 0.00001862
Iteration 388/1000 | Loss: 0.00001862
Iteration 389/1000 | Loss: 0.00001862
Iteration 390/1000 | Loss: 0.00001862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 390. Stopping optimization.
Last 5 losses: [1.861513555923011e-05, 1.861513555923011e-05, 1.861513555923011e-05, 1.861513555923011e-05, 1.861513555923011e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.861513555923011e-05

Optimization complete. Final v2v error: 3.625627279281616 mm

Highest mean error: 5.938617706298828 mm for frame 67

Lowest mean error: 3.216317892074585 mm for frame 201

Saving results

Total time: 298.788542509079
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041271
Iteration 2/25 | Loss: 0.00310675
Iteration 3/25 | Loss: 0.00253193
Iteration 4/25 | Loss: 0.00219174
Iteration 5/25 | Loss: 0.00175565
Iteration 6/25 | Loss: 0.00171540
Iteration 7/25 | Loss: 0.00165320
Iteration 8/25 | Loss: 0.00157132
Iteration 9/25 | Loss: 0.00144240
Iteration 10/25 | Loss: 0.00143103
Iteration 11/25 | Loss: 0.00136518
Iteration 12/25 | Loss: 0.00136309
Iteration 13/25 | Loss: 0.00135295
Iteration 14/25 | Loss: 0.00134428
Iteration 15/25 | Loss: 0.00134290
Iteration 16/25 | Loss: 0.00134252
Iteration 17/25 | Loss: 0.00135060
Iteration 18/25 | Loss: 0.00134068
Iteration 19/25 | Loss: 0.00133866
Iteration 20/25 | Loss: 0.00133833
Iteration 21/25 | Loss: 0.00133826
Iteration 22/25 | Loss: 0.00133826
Iteration 23/25 | Loss: 0.00133825
Iteration 24/25 | Loss: 0.00133823
Iteration 25/25 | Loss: 0.00133823

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44342577
Iteration 2/25 | Loss: 0.00135829
Iteration 3/25 | Loss: 0.00090888
Iteration 4/25 | Loss: 0.00090888
Iteration 5/25 | Loss: 0.00090888
Iteration 6/25 | Loss: 0.00090888
Iteration 7/25 | Loss: 0.00090888
Iteration 8/25 | Loss: 0.00090887
Iteration 9/25 | Loss: 0.00090887
Iteration 10/25 | Loss: 0.00090887
Iteration 11/25 | Loss: 0.00090887
Iteration 12/25 | Loss: 0.00090887
Iteration 13/25 | Loss: 0.00090887
Iteration 14/25 | Loss: 0.00090887
Iteration 15/25 | Loss: 0.00090887
Iteration 16/25 | Loss: 0.00090887
Iteration 17/25 | Loss: 0.00090887
Iteration 18/25 | Loss: 0.00090887
Iteration 19/25 | Loss: 0.00090887
Iteration 20/25 | Loss: 0.00090887
Iteration 21/25 | Loss: 0.00090887
Iteration 22/25 | Loss: 0.00090887
Iteration 23/25 | Loss: 0.00090887
Iteration 24/25 | Loss: 0.00090887
Iteration 25/25 | Loss: 0.00090887

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090887
Iteration 2/1000 | Loss: 0.00016102
Iteration 3/1000 | Loss: 0.00074087
Iteration 4/1000 | Loss: 0.00224359
Iteration 5/1000 | Loss: 0.00016104
Iteration 6/1000 | Loss: 0.00017416
Iteration 7/1000 | Loss: 0.00003110
Iteration 8/1000 | Loss: 0.00002813
Iteration 9/1000 | Loss: 0.00036389
Iteration 10/1000 | Loss: 0.00069968
Iteration 11/1000 | Loss: 0.00002799
Iteration 12/1000 | Loss: 0.00039589
Iteration 13/1000 | Loss: 0.00002699
Iteration 14/1000 | Loss: 0.00002547
Iteration 15/1000 | Loss: 0.00002492
Iteration 16/1000 | Loss: 0.00002437
Iteration 17/1000 | Loss: 0.00002406
Iteration 18/1000 | Loss: 0.00002368
Iteration 19/1000 | Loss: 0.00050890
Iteration 20/1000 | Loss: 0.00028704
Iteration 21/1000 | Loss: 0.00002478
Iteration 22/1000 | Loss: 0.00002276
Iteration 23/1000 | Loss: 0.00002110
Iteration 24/1000 | Loss: 0.00001995
Iteration 25/1000 | Loss: 0.00001923
Iteration 26/1000 | Loss: 0.00001893
Iteration 27/1000 | Loss: 0.00001863
Iteration 28/1000 | Loss: 0.00001843
Iteration 29/1000 | Loss: 0.00001837
Iteration 30/1000 | Loss: 0.00001834
Iteration 31/1000 | Loss: 0.00001813
Iteration 32/1000 | Loss: 0.00001792
Iteration 33/1000 | Loss: 0.00001787
Iteration 34/1000 | Loss: 0.00001783
Iteration 35/1000 | Loss: 0.00001783
Iteration 36/1000 | Loss: 0.00001783
Iteration 37/1000 | Loss: 0.00001782
Iteration 38/1000 | Loss: 0.00001782
Iteration 39/1000 | Loss: 0.00001780
Iteration 40/1000 | Loss: 0.00001779
Iteration 41/1000 | Loss: 0.00001778
Iteration 42/1000 | Loss: 0.00001778
Iteration 43/1000 | Loss: 0.00001778
Iteration 44/1000 | Loss: 0.00001778
Iteration 45/1000 | Loss: 0.00001778
Iteration 46/1000 | Loss: 0.00001778
Iteration 47/1000 | Loss: 0.00001777
Iteration 48/1000 | Loss: 0.00001777
Iteration 49/1000 | Loss: 0.00001777
Iteration 50/1000 | Loss: 0.00001776
Iteration 51/1000 | Loss: 0.00001776
Iteration 52/1000 | Loss: 0.00001775
Iteration 53/1000 | Loss: 0.00001774
Iteration 54/1000 | Loss: 0.00001774
Iteration 55/1000 | Loss: 0.00001773
Iteration 56/1000 | Loss: 0.00001773
Iteration 57/1000 | Loss: 0.00001773
Iteration 58/1000 | Loss: 0.00001772
Iteration 59/1000 | Loss: 0.00001771
Iteration 60/1000 | Loss: 0.00001771
Iteration 61/1000 | Loss: 0.00001771
Iteration 62/1000 | Loss: 0.00001771
Iteration 63/1000 | Loss: 0.00001771
Iteration 64/1000 | Loss: 0.00001770
Iteration 65/1000 | Loss: 0.00001770
Iteration 66/1000 | Loss: 0.00001770
Iteration 67/1000 | Loss: 0.00001770
Iteration 68/1000 | Loss: 0.00001770
Iteration 69/1000 | Loss: 0.00001770
Iteration 70/1000 | Loss: 0.00001770
Iteration 71/1000 | Loss: 0.00001769
Iteration 72/1000 | Loss: 0.00001769
Iteration 73/1000 | Loss: 0.00001769
Iteration 74/1000 | Loss: 0.00001768
Iteration 75/1000 | Loss: 0.00001768
Iteration 76/1000 | Loss: 0.00001768
Iteration 77/1000 | Loss: 0.00001767
Iteration 78/1000 | Loss: 0.00001767
Iteration 79/1000 | Loss: 0.00001767
Iteration 80/1000 | Loss: 0.00001766
Iteration 81/1000 | Loss: 0.00001766
Iteration 82/1000 | Loss: 0.00001766
Iteration 83/1000 | Loss: 0.00001766
Iteration 84/1000 | Loss: 0.00001766
Iteration 85/1000 | Loss: 0.00001766
Iteration 86/1000 | Loss: 0.00001766
Iteration 87/1000 | Loss: 0.00001766
Iteration 88/1000 | Loss: 0.00001765
Iteration 89/1000 | Loss: 0.00001765
Iteration 90/1000 | Loss: 0.00001765
Iteration 91/1000 | Loss: 0.00001765
Iteration 92/1000 | Loss: 0.00001765
Iteration 93/1000 | Loss: 0.00001765
Iteration 94/1000 | Loss: 0.00001765
Iteration 95/1000 | Loss: 0.00001765
Iteration 96/1000 | Loss: 0.00001765
Iteration 97/1000 | Loss: 0.00001765
Iteration 98/1000 | Loss: 0.00001765
Iteration 99/1000 | Loss: 0.00001765
Iteration 100/1000 | Loss: 0.00001765
Iteration 101/1000 | Loss: 0.00001765
Iteration 102/1000 | Loss: 0.00001765
Iteration 103/1000 | Loss: 0.00001765
Iteration 104/1000 | Loss: 0.00001764
Iteration 105/1000 | Loss: 0.00001764
Iteration 106/1000 | Loss: 0.00001764
Iteration 107/1000 | Loss: 0.00001764
Iteration 108/1000 | Loss: 0.00001764
Iteration 109/1000 | Loss: 0.00001764
Iteration 110/1000 | Loss: 0.00001764
Iteration 111/1000 | Loss: 0.00001764
Iteration 112/1000 | Loss: 0.00001764
Iteration 113/1000 | Loss: 0.00001764
Iteration 114/1000 | Loss: 0.00001764
Iteration 115/1000 | Loss: 0.00001764
Iteration 116/1000 | Loss: 0.00001764
Iteration 117/1000 | Loss: 0.00001764
Iteration 118/1000 | Loss: 0.00001764
Iteration 119/1000 | Loss: 0.00001764
Iteration 120/1000 | Loss: 0.00001764
Iteration 121/1000 | Loss: 0.00001764
Iteration 122/1000 | Loss: 0.00001764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.764096668921411e-05, 1.764096668921411e-05, 1.764096668921411e-05, 1.764096668921411e-05, 1.764096668921411e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.764096668921411e-05

Optimization complete. Final v2v error: 3.558960199356079 mm

Highest mean error: 4.087966442108154 mm for frame 23

Lowest mean error: 3.104569435119629 mm for frame 122

Saving results

Total time: 83.25344657897949
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999497
Iteration 2/25 | Loss: 0.00217374
Iteration 3/25 | Loss: 0.00158519
Iteration 4/25 | Loss: 0.00151041
Iteration 5/25 | Loss: 0.00142135
Iteration 6/25 | Loss: 0.00139720
Iteration 7/25 | Loss: 0.00139197
Iteration 8/25 | Loss: 0.00139080
Iteration 9/25 | Loss: 0.00139070
Iteration 10/25 | Loss: 0.00139070
Iteration 11/25 | Loss: 0.00139070
Iteration 12/25 | Loss: 0.00139070
Iteration 13/25 | Loss: 0.00139070
Iteration 14/25 | Loss: 0.00139070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013906974345445633, 0.0013906974345445633, 0.0013906974345445633, 0.0013906974345445633, 0.0013906974345445633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013906974345445633

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37900400
Iteration 2/25 | Loss: 0.00087937
Iteration 3/25 | Loss: 0.00087936
Iteration 4/25 | Loss: 0.00087936
Iteration 5/25 | Loss: 0.00087936
Iteration 6/25 | Loss: 0.00087936
Iteration 7/25 | Loss: 0.00087936
Iteration 8/25 | Loss: 0.00087936
Iteration 9/25 | Loss: 0.00087936
Iteration 10/25 | Loss: 0.00087936
Iteration 11/25 | Loss: 0.00087936
Iteration 12/25 | Loss: 0.00087936
Iteration 13/25 | Loss: 0.00087936
Iteration 14/25 | Loss: 0.00087936
Iteration 15/25 | Loss: 0.00087936
Iteration 16/25 | Loss: 0.00087936
Iteration 17/25 | Loss: 0.00087936
Iteration 18/25 | Loss: 0.00087936
Iteration 19/25 | Loss: 0.00087936
Iteration 20/25 | Loss: 0.00087936
Iteration 21/25 | Loss: 0.00087936
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008793606539256871, 0.0008793606539256871, 0.0008793606539256871, 0.0008793606539256871, 0.0008793606539256871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008793606539256871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087936
Iteration 2/1000 | Loss: 0.00004319
Iteration 3/1000 | Loss: 0.00071386
Iteration 4/1000 | Loss: 0.00008440
Iteration 5/1000 | Loss: 0.00005406
Iteration 6/1000 | Loss: 0.00004627
Iteration 7/1000 | Loss: 0.00003682
Iteration 8/1000 | Loss: 0.00003121
Iteration 9/1000 | Loss: 0.00002839
Iteration 10/1000 | Loss: 0.00002737
Iteration 11/1000 | Loss: 0.00002687
Iteration 12/1000 | Loss: 0.00002648
Iteration 13/1000 | Loss: 0.00002599
Iteration 14/1000 | Loss: 0.00002575
Iteration 15/1000 | Loss: 0.00002547
Iteration 16/1000 | Loss: 0.00002521
Iteration 17/1000 | Loss: 0.00002504
Iteration 18/1000 | Loss: 0.00002499
Iteration 19/1000 | Loss: 0.00002493
Iteration 20/1000 | Loss: 0.00002490
Iteration 21/1000 | Loss: 0.00002490
Iteration 22/1000 | Loss: 0.00002490
Iteration 23/1000 | Loss: 0.00002489
Iteration 24/1000 | Loss: 0.00002487
Iteration 25/1000 | Loss: 0.00002487
Iteration 26/1000 | Loss: 0.00002486
Iteration 27/1000 | Loss: 0.00002486
Iteration 28/1000 | Loss: 0.00002486
Iteration 29/1000 | Loss: 0.00002486
Iteration 30/1000 | Loss: 0.00002486
Iteration 31/1000 | Loss: 0.00002486
Iteration 32/1000 | Loss: 0.00002486
Iteration 33/1000 | Loss: 0.00002486
Iteration 34/1000 | Loss: 0.00002486
Iteration 35/1000 | Loss: 0.00002486
Iteration 36/1000 | Loss: 0.00002485
Iteration 37/1000 | Loss: 0.00002485
Iteration 38/1000 | Loss: 0.00002485
Iteration 39/1000 | Loss: 0.00002485
Iteration 40/1000 | Loss: 0.00002485
Iteration 41/1000 | Loss: 0.00002485
Iteration 42/1000 | Loss: 0.00002485
Iteration 43/1000 | Loss: 0.00002485
Iteration 44/1000 | Loss: 0.00002485
Iteration 45/1000 | Loss: 0.00002485
Iteration 46/1000 | Loss: 0.00002485
Iteration 47/1000 | Loss: 0.00002485
Iteration 48/1000 | Loss: 0.00002485
Iteration 49/1000 | Loss: 0.00002485
Iteration 50/1000 | Loss: 0.00002485
Iteration 51/1000 | Loss: 0.00002484
Iteration 52/1000 | Loss: 0.00002484
Iteration 53/1000 | Loss: 0.00002484
Iteration 54/1000 | Loss: 0.00002484
Iteration 55/1000 | Loss: 0.00002484
Iteration 56/1000 | Loss: 0.00002484
Iteration 57/1000 | Loss: 0.00002484
Iteration 58/1000 | Loss: 0.00002484
Iteration 59/1000 | Loss: 0.00002484
Iteration 60/1000 | Loss: 0.00002484
Iteration 61/1000 | Loss: 0.00002484
Iteration 62/1000 | Loss: 0.00002484
Iteration 63/1000 | Loss: 0.00002484
Iteration 64/1000 | Loss: 0.00002484
Iteration 65/1000 | Loss: 0.00002484
Iteration 66/1000 | Loss: 0.00002484
Iteration 67/1000 | Loss: 0.00002484
Iteration 68/1000 | Loss: 0.00002484
Iteration 69/1000 | Loss: 0.00002484
Iteration 70/1000 | Loss: 0.00002484
Iteration 71/1000 | Loss: 0.00002484
Iteration 72/1000 | Loss: 0.00002484
Iteration 73/1000 | Loss: 0.00002484
Iteration 74/1000 | Loss: 0.00002484
Iteration 75/1000 | Loss: 0.00002484
Iteration 76/1000 | Loss: 0.00002484
Iteration 77/1000 | Loss: 0.00002484
Iteration 78/1000 | Loss: 0.00002484
Iteration 79/1000 | Loss: 0.00002484
Iteration 80/1000 | Loss: 0.00002484
Iteration 81/1000 | Loss: 0.00002484
Iteration 82/1000 | Loss: 0.00002484
Iteration 83/1000 | Loss: 0.00002484
Iteration 84/1000 | Loss: 0.00002484
Iteration 85/1000 | Loss: 0.00002484
Iteration 86/1000 | Loss: 0.00002484
Iteration 87/1000 | Loss: 0.00002484
Iteration 88/1000 | Loss: 0.00002484
Iteration 89/1000 | Loss: 0.00002484
Iteration 90/1000 | Loss: 0.00002484
Iteration 91/1000 | Loss: 0.00002484
Iteration 92/1000 | Loss: 0.00002484
Iteration 93/1000 | Loss: 0.00002484
Iteration 94/1000 | Loss: 0.00002484
Iteration 95/1000 | Loss: 0.00002484
Iteration 96/1000 | Loss: 0.00002484
Iteration 97/1000 | Loss: 0.00002484
Iteration 98/1000 | Loss: 0.00002484
Iteration 99/1000 | Loss: 0.00002484
Iteration 100/1000 | Loss: 0.00002484
Iteration 101/1000 | Loss: 0.00002484
Iteration 102/1000 | Loss: 0.00002484
Iteration 103/1000 | Loss: 0.00002484
Iteration 104/1000 | Loss: 0.00002484
Iteration 105/1000 | Loss: 0.00002484
Iteration 106/1000 | Loss: 0.00002484
Iteration 107/1000 | Loss: 0.00002484
Iteration 108/1000 | Loss: 0.00002484
Iteration 109/1000 | Loss: 0.00002484
Iteration 110/1000 | Loss: 0.00002484
Iteration 111/1000 | Loss: 0.00002484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.4843538994900882e-05, 2.4843538994900882e-05, 2.4843538994900882e-05, 2.4843538994900882e-05, 2.4843538994900882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4843538994900882e-05

Optimization complete. Final v2v error: 4.197397708892822 mm

Highest mean error: 4.3792572021484375 mm for frame 22

Lowest mean error: 4.05051851272583 mm for frame 104

Saving results

Total time: 41.288185119628906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880811
Iteration 2/25 | Loss: 0.00178589
Iteration 3/25 | Loss: 0.00156059
Iteration 4/25 | Loss: 0.00155007
Iteration 5/25 | Loss: 0.00154811
Iteration 6/25 | Loss: 0.00154811
Iteration 7/25 | Loss: 0.00154811
Iteration 8/25 | Loss: 0.00154811
Iteration 9/25 | Loss: 0.00154811
Iteration 10/25 | Loss: 0.00154811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0015481066657230258, 0.0015481066657230258, 0.0015481066657230258, 0.0015481066657230258, 0.0015481066657230258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015481066657230258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.58610755
Iteration 2/25 | Loss: 0.00120592
Iteration 3/25 | Loss: 0.00120592
Iteration 4/25 | Loss: 0.00120592
Iteration 5/25 | Loss: 0.00120592
Iteration 6/25 | Loss: 0.00120592
Iteration 7/25 | Loss: 0.00120592
Iteration 8/25 | Loss: 0.00120592
Iteration 9/25 | Loss: 0.00120592
Iteration 10/25 | Loss: 0.00120592
Iteration 11/25 | Loss: 0.00120592
Iteration 12/25 | Loss: 0.00120592
Iteration 13/25 | Loss: 0.00120592
Iteration 14/25 | Loss: 0.00120592
Iteration 15/25 | Loss: 0.00120592
Iteration 16/25 | Loss: 0.00120592
Iteration 17/25 | Loss: 0.00120592
Iteration 18/25 | Loss: 0.00120592
Iteration 19/25 | Loss: 0.00120592
Iteration 20/25 | Loss: 0.00120592
Iteration 21/25 | Loss: 0.00120592
Iteration 22/25 | Loss: 0.00120592
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012059189612045884, 0.0012059189612045884, 0.0012059189612045884, 0.0012059189612045884, 0.0012059189612045884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012059189612045884

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120592
Iteration 2/1000 | Loss: 0.00005482
Iteration 3/1000 | Loss: 0.00003841
Iteration 4/1000 | Loss: 0.00003542
Iteration 5/1000 | Loss: 0.00003398
Iteration 6/1000 | Loss: 0.00003278
Iteration 7/1000 | Loss: 0.00003236
Iteration 8/1000 | Loss: 0.00003195
Iteration 9/1000 | Loss: 0.00003149
Iteration 10/1000 | Loss: 0.00003121
Iteration 11/1000 | Loss: 0.00003097
Iteration 12/1000 | Loss: 0.00003079
Iteration 13/1000 | Loss: 0.00003071
Iteration 14/1000 | Loss: 0.00003070
Iteration 15/1000 | Loss: 0.00003069
Iteration 16/1000 | Loss: 0.00003064
Iteration 17/1000 | Loss: 0.00003060
Iteration 18/1000 | Loss: 0.00003060
Iteration 19/1000 | Loss: 0.00003059
Iteration 20/1000 | Loss: 0.00003059
Iteration 21/1000 | Loss: 0.00003059
Iteration 22/1000 | Loss: 0.00003059
Iteration 23/1000 | Loss: 0.00003059
Iteration 24/1000 | Loss: 0.00003059
Iteration 25/1000 | Loss: 0.00003059
Iteration 26/1000 | Loss: 0.00003059
Iteration 27/1000 | Loss: 0.00003059
Iteration 28/1000 | Loss: 0.00003059
Iteration 29/1000 | Loss: 0.00003059
Iteration 30/1000 | Loss: 0.00003059
Iteration 31/1000 | Loss: 0.00003059
Iteration 32/1000 | Loss: 0.00003059
Iteration 33/1000 | Loss: 0.00003059
Iteration 34/1000 | Loss: 0.00003059
Iteration 35/1000 | Loss: 0.00003059
Iteration 36/1000 | Loss: 0.00003059
Iteration 37/1000 | Loss: 0.00003059
Iteration 38/1000 | Loss: 0.00003059
Iteration 39/1000 | Loss: 0.00003059
Iteration 40/1000 | Loss: 0.00003059
Iteration 41/1000 | Loss: 0.00003059
Iteration 42/1000 | Loss: 0.00003059
Iteration 43/1000 | Loss: 0.00003059
Iteration 44/1000 | Loss: 0.00003059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 44. Stopping optimization.
Last 5 losses: [3.058834045077674e-05, 3.058834045077674e-05, 3.058834045077674e-05, 3.058834045077674e-05, 3.058834045077674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.058834045077674e-05

Optimization complete. Final v2v error: 4.630178451538086 mm

Highest mean error: 4.92034387588501 mm for frame 98

Lowest mean error: 4.449656963348389 mm for frame 47

Saving results

Total time: 26.736799716949463
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787405
Iteration 2/25 | Loss: 0.00135201
Iteration 3/25 | Loss: 0.00127837
Iteration 4/25 | Loss: 0.00126830
Iteration 5/25 | Loss: 0.00126594
Iteration 6/25 | Loss: 0.00126594
Iteration 7/25 | Loss: 0.00126594
Iteration 8/25 | Loss: 0.00126594
Iteration 9/25 | Loss: 0.00126594
Iteration 10/25 | Loss: 0.00126594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012659422354772687, 0.0012659422354772687, 0.0012659422354772687, 0.0012659422354772687, 0.0012659422354772687]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012659422354772687

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39848042
Iteration 2/25 | Loss: 0.00085315
Iteration 3/25 | Loss: 0.00085315
Iteration 4/25 | Loss: 0.00085315
Iteration 5/25 | Loss: 0.00085315
Iteration 6/25 | Loss: 0.00085314
Iteration 7/25 | Loss: 0.00085314
Iteration 8/25 | Loss: 0.00085314
Iteration 9/25 | Loss: 0.00085314
Iteration 10/25 | Loss: 0.00085314
Iteration 11/25 | Loss: 0.00085314
Iteration 12/25 | Loss: 0.00085314
Iteration 13/25 | Loss: 0.00085314
Iteration 14/25 | Loss: 0.00085314
Iteration 15/25 | Loss: 0.00085314
Iteration 16/25 | Loss: 0.00085314
Iteration 17/25 | Loss: 0.00085314
Iteration 18/25 | Loss: 0.00085314
Iteration 19/25 | Loss: 0.00085314
Iteration 20/25 | Loss: 0.00085314
Iteration 21/25 | Loss: 0.00085314
Iteration 22/25 | Loss: 0.00085314
Iteration 23/25 | Loss: 0.00085314
Iteration 24/25 | Loss: 0.00085314
Iteration 25/25 | Loss: 0.00085314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085314
Iteration 2/1000 | Loss: 0.00002973
Iteration 3/1000 | Loss: 0.00001973
Iteration 4/1000 | Loss: 0.00001693
Iteration 5/1000 | Loss: 0.00001525
Iteration 6/1000 | Loss: 0.00001452
Iteration 7/1000 | Loss: 0.00001379
Iteration 8/1000 | Loss: 0.00001329
Iteration 9/1000 | Loss: 0.00001313
Iteration 10/1000 | Loss: 0.00001288
Iteration 11/1000 | Loss: 0.00001256
Iteration 12/1000 | Loss: 0.00001248
Iteration 13/1000 | Loss: 0.00001239
Iteration 14/1000 | Loss: 0.00001239
Iteration 15/1000 | Loss: 0.00001239
Iteration 16/1000 | Loss: 0.00001229
Iteration 17/1000 | Loss: 0.00001226
Iteration 18/1000 | Loss: 0.00001224
Iteration 19/1000 | Loss: 0.00001220
Iteration 20/1000 | Loss: 0.00001219
Iteration 21/1000 | Loss: 0.00001219
Iteration 22/1000 | Loss: 0.00001218
Iteration 23/1000 | Loss: 0.00001217
Iteration 24/1000 | Loss: 0.00001216
Iteration 25/1000 | Loss: 0.00001216
Iteration 26/1000 | Loss: 0.00001215
Iteration 27/1000 | Loss: 0.00001215
Iteration 28/1000 | Loss: 0.00001215
Iteration 29/1000 | Loss: 0.00001214
Iteration 30/1000 | Loss: 0.00001213
Iteration 31/1000 | Loss: 0.00001213
Iteration 32/1000 | Loss: 0.00001213
Iteration 33/1000 | Loss: 0.00001212
Iteration 34/1000 | Loss: 0.00001212
Iteration 35/1000 | Loss: 0.00001212
Iteration 36/1000 | Loss: 0.00001211
Iteration 37/1000 | Loss: 0.00001211
Iteration 38/1000 | Loss: 0.00001208
Iteration 39/1000 | Loss: 0.00001208
Iteration 40/1000 | Loss: 0.00001207
Iteration 41/1000 | Loss: 0.00001207
Iteration 42/1000 | Loss: 0.00001206
Iteration 43/1000 | Loss: 0.00001206
Iteration 44/1000 | Loss: 0.00001206
Iteration 45/1000 | Loss: 0.00001205
Iteration 46/1000 | Loss: 0.00001205
Iteration 47/1000 | Loss: 0.00001204
Iteration 48/1000 | Loss: 0.00001204
Iteration 49/1000 | Loss: 0.00001204
Iteration 50/1000 | Loss: 0.00001204
Iteration 51/1000 | Loss: 0.00001204
Iteration 52/1000 | Loss: 0.00001204
Iteration 53/1000 | Loss: 0.00001203
Iteration 54/1000 | Loss: 0.00001203
Iteration 55/1000 | Loss: 0.00001203
Iteration 56/1000 | Loss: 0.00001203
Iteration 57/1000 | Loss: 0.00001202
Iteration 58/1000 | Loss: 0.00001202
Iteration 59/1000 | Loss: 0.00001202
Iteration 60/1000 | Loss: 0.00001202
Iteration 61/1000 | Loss: 0.00001201
Iteration 62/1000 | Loss: 0.00001201
Iteration 63/1000 | Loss: 0.00001201
Iteration 64/1000 | Loss: 0.00001201
Iteration 65/1000 | Loss: 0.00001200
Iteration 66/1000 | Loss: 0.00001199
Iteration 67/1000 | Loss: 0.00001199
Iteration 68/1000 | Loss: 0.00001198
Iteration 69/1000 | Loss: 0.00001198
Iteration 70/1000 | Loss: 0.00001198
Iteration 71/1000 | Loss: 0.00001197
Iteration 72/1000 | Loss: 0.00001197
Iteration 73/1000 | Loss: 0.00001197
Iteration 74/1000 | Loss: 0.00001197
Iteration 75/1000 | Loss: 0.00001196
Iteration 76/1000 | Loss: 0.00001193
Iteration 77/1000 | Loss: 0.00001193
Iteration 78/1000 | Loss: 0.00001192
Iteration 79/1000 | Loss: 0.00001192
Iteration 80/1000 | Loss: 0.00001192
Iteration 81/1000 | Loss: 0.00001191
Iteration 82/1000 | Loss: 0.00001191
Iteration 83/1000 | Loss: 0.00001191
Iteration 84/1000 | Loss: 0.00001190
Iteration 85/1000 | Loss: 0.00001190
Iteration 86/1000 | Loss: 0.00001190
Iteration 87/1000 | Loss: 0.00001190
Iteration 88/1000 | Loss: 0.00001189
Iteration 89/1000 | Loss: 0.00001189
Iteration 90/1000 | Loss: 0.00001189
Iteration 91/1000 | Loss: 0.00001188
Iteration 92/1000 | Loss: 0.00001188
Iteration 93/1000 | Loss: 0.00001187
Iteration 94/1000 | Loss: 0.00001187
Iteration 95/1000 | Loss: 0.00001187
Iteration 96/1000 | Loss: 0.00001186
Iteration 97/1000 | Loss: 0.00001186
Iteration 98/1000 | Loss: 0.00001185
Iteration 99/1000 | Loss: 0.00001185
Iteration 100/1000 | Loss: 0.00001185
Iteration 101/1000 | Loss: 0.00001185
Iteration 102/1000 | Loss: 0.00001185
Iteration 103/1000 | Loss: 0.00001184
Iteration 104/1000 | Loss: 0.00001184
Iteration 105/1000 | Loss: 0.00001184
Iteration 106/1000 | Loss: 0.00001184
Iteration 107/1000 | Loss: 0.00001183
Iteration 108/1000 | Loss: 0.00001183
Iteration 109/1000 | Loss: 0.00001183
Iteration 110/1000 | Loss: 0.00001183
Iteration 111/1000 | Loss: 0.00001183
Iteration 112/1000 | Loss: 0.00001183
Iteration 113/1000 | Loss: 0.00001182
Iteration 114/1000 | Loss: 0.00001182
Iteration 115/1000 | Loss: 0.00001182
Iteration 116/1000 | Loss: 0.00001182
Iteration 117/1000 | Loss: 0.00001182
Iteration 118/1000 | Loss: 0.00001181
Iteration 119/1000 | Loss: 0.00001181
Iteration 120/1000 | Loss: 0.00001181
Iteration 121/1000 | Loss: 0.00001180
Iteration 122/1000 | Loss: 0.00001180
Iteration 123/1000 | Loss: 0.00001180
Iteration 124/1000 | Loss: 0.00001180
Iteration 125/1000 | Loss: 0.00001180
Iteration 126/1000 | Loss: 0.00001180
Iteration 127/1000 | Loss: 0.00001180
Iteration 128/1000 | Loss: 0.00001180
Iteration 129/1000 | Loss: 0.00001179
Iteration 130/1000 | Loss: 0.00001179
Iteration 131/1000 | Loss: 0.00001179
Iteration 132/1000 | Loss: 0.00001179
Iteration 133/1000 | Loss: 0.00001179
Iteration 134/1000 | Loss: 0.00001178
Iteration 135/1000 | Loss: 0.00001178
Iteration 136/1000 | Loss: 0.00001178
Iteration 137/1000 | Loss: 0.00001178
Iteration 138/1000 | Loss: 0.00001178
Iteration 139/1000 | Loss: 0.00001178
Iteration 140/1000 | Loss: 0.00001178
Iteration 141/1000 | Loss: 0.00001178
Iteration 142/1000 | Loss: 0.00001177
Iteration 143/1000 | Loss: 0.00001177
Iteration 144/1000 | Loss: 0.00001177
Iteration 145/1000 | Loss: 0.00001177
Iteration 146/1000 | Loss: 0.00001177
Iteration 147/1000 | Loss: 0.00001177
Iteration 148/1000 | Loss: 0.00001177
Iteration 149/1000 | Loss: 0.00001176
Iteration 150/1000 | Loss: 0.00001176
Iteration 151/1000 | Loss: 0.00001176
Iteration 152/1000 | Loss: 0.00001176
Iteration 153/1000 | Loss: 0.00001176
Iteration 154/1000 | Loss: 0.00001176
Iteration 155/1000 | Loss: 0.00001176
Iteration 156/1000 | Loss: 0.00001176
Iteration 157/1000 | Loss: 0.00001176
Iteration 158/1000 | Loss: 0.00001176
Iteration 159/1000 | Loss: 0.00001176
Iteration 160/1000 | Loss: 0.00001176
Iteration 161/1000 | Loss: 0.00001175
Iteration 162/1000 | Loss: 0.00001175
Iteration 163/1000 | Loss: 0.00001175
Iteration 164/1000 | Loss: 0.00001175
Iteration 165/1000 | Loss: 0.00001174
Iteration 166/1000 | Loss: 0.00001174
Iteration 167/1000 | Loss: 0.00001174
Iteration 168/1000 | Loss: 0.00001174
Iteration 169/1000 | Loss: 0.00001174
Iteration 170/1000 | Loss: 0.00001174
Iteration 171/1000 | Loss: 0.00001173
Iteration 172/1000 | Loss: 0.00001173
Iteration 173/1000 | Loss: 0.00001173
Iteration 174/1000 | Loss: 0.00001173
Iteration 175/1000 | Loss: 0.00001173
Iteration 176/1000 | Loss: 0.00001173
Iteration 177/1000 | Loss: 0.00001173
Iteration 178/1000 | Loss: 0.00001173
Iteration 179/1000 | Loss: 0.00001173
Iteration 180/1000 | Loss: 0.00001173
Iteration 181/1000 | Loss: 0.00001173
Iteration 182/1000 | Loss: 0.00001173
Iteration 183/1000 | Loss: 0.00001172
Iteration 184/1000 | Loss: 0.00001172
Iteration 185/1000 | Loss: 0.00001172
Iteration 186/1000 | Loss: 0.00001172
Iteration 187/1000 | Loss: 0.00001172
Iteration 188/1000 | Loss: 0.00001172
Iteration 189/1000 | Loss: 0.00001172
Iteration 190/1000 | Loss: 0.00001172
Iteration 191/1000 | Loss: 0.00001172
Iteration 192/1000 | Loss: 0.00001172
Iteration 193/1000 | Loss: 0.00001172
Iteration 194/1000 | Loss: 0.00001172
Iteration 195/1000 | Loss: 0.00001171
Iteration 196/1000 | Loss: 0.00001171
Iteration 197/1000 | Loss: 0.00001171
Iteration 198/1000 | Loss: 0.00001171
Iteration 199/1000 | Loss: 0.00001171
Iteration 200/1000 | Loss: 0.00001171
Iteration 201/1000 | Loss: 0.00001171
Iteration 202/1000 | Loss: 0.00001171
Iteration 203/1000 | Loss: 0.00001170
Iteration 204/1000 | Loss: 0.00001170
Iteration 205/1000 | Loss: 0.00001170
Iteration 206/1000 | Loss: 0.00001170
Iteration 207/1000 | Loss: 0.00001169
Iteration 208/1000 | Loss: 0.00001169
Iteration 209/1000 | Loss: 0.00001169
Iteration 210/1000 | Loss: 0.00001169
Iteration 211/1000 | Loss: 0.00001168
Iteration 212/1000 | Loss: 0.00001168
Iteration 213/1000 | Loss: 0.00001168
Iteration 214/1000 | Loss: 0.00001168
Iteration 215/1000 | Loss: 0.00001168
Iteration 216/1000 | Loss: 0.00001168
Iteration 217/1000 | Loss: 0.00001168
Iteration 218/1000 | Loss: 0.00001168
Iteration 219/1000 | Loss: 0.00001168
Iteration 220/1000 | Loss: 0.00001168
Iteration 221/1000 | Loss: 0.00001168
Iteration 222/1000 | Loss: 0.00001168
Iteration 223/1000 | Loss: 0.00001168
Iteration 224/1000 | Loss: 0.00001168
Iteration 225/1000 | Loss: 0.00001168
Iteration 226/1000 | Loss: 0.00001168
Iteration 227/1000 | Loss: 0.00001167
Iteration 228/1000 | Loss: 0.00001167
Iteration 229/1000 | Loss: 0.00001167
Iteration 230/1000 | Loss: 0.00001167
Iteration 231/1000 | Loss: 0.00001167
Iteration 232/1000 | Loss: 0.00001167
Iteration 233/1000 | Loss: 0.00001167
Iteration 234/1000 | Loss: 0.00001167
Iteration 235/1000 | Loss: 0.00001167
Iteration 236/1000 | Loss: 0.00001167
Iteration 237/1000 | Loss: 0.00001167
Iteration 238/1000 | Loss: 0.00001167
Iteration 239/1000 | Loss: 0.00001167
Iteration 240/1000 | Loss: 0.00001167
Iteration 241/1000 | Loss: 0.00001167
Iteration 242/1000 | Loss: 0.00001167
Iteration 243/1000 | Loss: 0.00001167
Iteration 244/1000 | Loss: 0.00001167
Iteration 245/1000 | Loss: 0.00001167
Iteration 246/1000 | Loss: 0.00001167
Iteration 247/1000 | Loss: 0.00001167
Iteration 248/1000 | Loss: 0.00001167
Iteration 249/1000 | Loss: 0.00001167
Iteration 250/1000 | Loss: 0.00001167
Iteration 251/1000 | Loss: 0.00001167
Iteration 252/1000 | Loss: 0.00001167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [1.1669025298033375e-05, 1.1669025298033375e-05, 1.1669025298033375e-05, 1.1669025298033375e-05, 1.1669025298033375e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1669025298033375e-05

Optimization complete. Final v2v error: 2.9056241512298584 mm

Highest mean error: 3.1773688793182373 mm for frame 53

Lowest mean error: 2.7644951343536377 mm for frame 15

Saving results

Total time: 44.24042248725891
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799832
Iteration 2/25 | Loss: 0.00134488
Iteration 3/25 | Loss: 0.00129691
Iteration 4/25 | Loss: 0.00129070
Iteration 5/25 | Loss: 0.00128870
Iteration 6/25 | Loss: 0.00128866
Iteration 7/25 | Loss: 0.00128866
Iteration 8/25 | Loss: 0.00128866
Iteration 9/25 | Loss: 0.00128866
Iteration 10/25 | Loss: 0.00128866
Iteration 11/25 | Loss: 0.00128866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012886596377938986, 0.0012886596377938986, 0.0012886596377938986, 0.0012886596377938986, 0.0012886596377938986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012886596377938986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.15955353
Iteration 2/25 | Loss: 0.00084871
Iteration 3/25 | Loss: 0.00084870
Iteration 4/25 | Loss: 0.00084870
Iteration 5/25 | Loss: 0.00084869
Iteration 6/25 | Loss: 0.00084869
Iteration 7/25 | Loss: 0.00084869
Iteration 8/25 | Loss: 0.00084869
Iteration 9/25 | Loss: 0.00084869
Iteration 10/25 | Loss: 0.00084869
Iteration 11/25 | Loss: 0.00084869
Iteration 12/25 | Loss: 0.00084869
Iteration 13/25 | Loss: 0.00084869
Iteration 14/25 | Loss: 0.00084869
Iteration 15/25 | Loss: 0.00084869
Iteration 16/25 | Loss: 0.00084869
Iteration 17/25 | Loss: 0.00084869
Iteration 18/25 | Loss: 0.00084869
Iteration 19/25 | Loss: 0.00084869
Iteration 20/25 | Loss: 0.00084869
Iteration 21/25 | Loss: 0.00084869
Iteration 22/25 | Loss: 0.00084869
Iteration 23/25 | Loss: 0.00084869
Iteration 24/25 | Loss: 0.00084869
Iteration 25/25 | Loss: 0.00084869
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008486927254125476, 0.0008486927254125476, 0.0008486927254125476, 0.0008486927254125476, 0.0008486927254125476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008486927254125476

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084869
Iteration 2/1000 | Loss: 0.00003340
Iteration 3/1000 | Loss: 0.00002271
Iteration 4/1000 | Loss: 0.00001868
Iteration 5/1000 | Loss: 0.00001775
Iteration 6/1000 | Loss: 0.00001695
Iteration 7/1000 | Loss: 0.00001657
Iteration 8/1000 | Loss: 0.00001607
Iteration 9/1000 | Loss: 0.00001591
Iteration 10/1000 | Loss: 0.00001567
Iteration 11/1000 | Loss: 0.00001543
Iteration 12/1000 | Loss: 0.00001526
Iteration 13/1000 | Loss: 0.00001522
Iteration 14/1000 | Loss: 0.00001519
Iteration 15/1000 | Loss: 0.00001511
Iteration 16/1000 | Loss: 0.00001510
Iteration 17/1000 | Loss: 0.00001507
Iteration 18/1000 | Loss: 0.00001503
Iteration 19/1000 | Loss: 0.00001496
Iteration 20/1000 | Loss: 0.00001490
Iteration 21/1000 | Loss: 0.00001489
Iteration 22/1000 | Loss: 0.00001489
Iteration 23/1000 | Loss: 0.00001488
Iteration 24/1000 | Loss: 0.00001488
Iteration 25/1000 | Loss: 0.00001488
Iteration 26/1000 | Loss: 0.00001487
Iteration 27/1000 | Loss: 0.00001487
Iteration 28/1000 | Loss: 0.00001486
Iteration 29/1000 | Loss: 0.00001486
Iteration 30/1000 | Loss: 0.00001485
Iteration 31/1000 | Loss: 0.00001485
Iteration 32/1000 | Loss: 0.00001485
Iteration 33/1000 | Loss: 0.00001485
Iteration 34/1000 | Loss: 0.00001484
Iteration 35/1000 | Loss: 0.00001484
Iteration 36/1000 | Loss: 0.00001484
Iteration 37/1000 | Loss: 0.00001484
Iteration 38/1000 | Loss: 0.00001484
Iteration 39/1000 | Loss: 0.00001484
Iteration 40/1000 | Loss: 0.00001483
Iteration 41/1000 | Loss: 0.00001483
Iteration 42/1000 | Loss: 0.00001483
Iteration 43/1000 | Loss: 0.00001482
Iteration 44/1000 | Loss: 0.00001482
Iteration 45/1000 | Loss: 0.00001482
Iteration 46/1000 | Loss: 0.00001481
Iteration 47/1000 | Loss: 0.00001481
Iteration 48/1000 | Loss: 0.00001480
Iteration 49/1000 | Loss: 0.00001480
Iteration 50/1000 | Loss: 0.00001480
Iteration 51/1000 | Loss: 0.00001480
Iteration 52/1000 | Loss: 0.00001480
Iteration 53/1000 | Loss: 0.00001479
Iteration 54/1000 | Loss: 0.00001479
Iteration 55/1000 | Loss: 0.00001479
Iteration 56/1000 | Loss: 0.00001479
Iteration 57/1000 | Loss: 0.00001478
Iteration 58/1000 | Loss: 0.00001478
Iteration 59/1000 | Loss: 0.00001478
Iteration 60/1000 | Loss: 0.00001477
Iteration 61/1000 | Loss: 0.00001477
Iteration 62/1000 | Loss: 0.00001476
Iteration 63/1000 | Loss: 0.00001476
Iteration 64/1000 | Loss: 0.00001476
Iteration 65/1000 | Loss: 0.00001473
Iteration 66/1000 | Loss: 0.00001473
Iteration 67/1000 | Loss: 0.00001473
Iteration 68/1000 | Loss: 0.00001473
Iteration 69/1000 | Loss: 0.00001473
Iteration 70/1000 | Loss: 0.00001473
Iteration 71/1000 | Loss: 0.00001473
Iteration 72/1000 | Loss: 0.00001473
Iteration 73/1000 | Loss: 0.00001473
Iteration 74/1000 | Loss: 0.00001472
Iteration 75/1000 | Loss: 0.00001472
Iteration 76/1000 | Loss: 0.00001471
Iteration 77/1000 | Loss: 0.00001471
Iteration 78/1000 | Loss: 0.00001470
Iteration 79/1000 | Loss: 0.00001469
Iteration 80/1000 | Loss: 0.00001469
Iteration 81/1000 | Loss: 0.00001469
Iteration 82/1000 | Loss: 0.00001469
Iteration 83/1000 | Loss: 0.00001469
Iteration 84/1000 | Loss: 0.00001468
Iteration 85/1000 | Loss: 0.00001468
Iteration 86/1000 | Loss: 0.00001468
Iteration 87/1000 | Loss: 0.00001467
Iteration 88/1000 | Loss: 0.00001467
Iteration 89/1000 | Loss: 0.00001467
Iteration 90/1000 | Loss: 0.00001467
Iteration 91/1000 | Loss: 0.00001467
Iteration 92/1000 | Loss: 0.00001466
Iteration 93/1000 | Loss: 0.00001466
Iteration 94/1000 | Loss: 0.00001466
Iteration 95/1000 | Loss: 0.00001466
Iteration 96/1000 | Loss: 0.00001466
Iteration 97/1000 | Loss: 0.00001465
Iteration 98/1000 | Loss: 0.00001465
Iteration 99/1000 | Loss: 0.00001465
Iteration 100/1000 | Loss: 0.00001465
Iteration 101/1000 | Loss: 0.00001465
Iteration 102/1000 | Loss: 0.00001465
Iteration 103/1000 | Loss: 0.00001465
Iteration 104/1000 | Loss: 0.00001465
Iteration 105/1000 | Loss: 0.00001464
Iteration 106/1000 | Loss: 0.00001464
Iteration 107/1000 | Loss: 0.00001464
Iteration 108/1000 | Loss: 0.00001464
Iteration 109/1000 | Loss: 0.00001463
Iteration 110/1000 | Loss: 0.00001463
Iteration 111/1000 | Loss: 0.00001463
Iteration 112/1000 | Loss: 0.00001462
Iteration 113/1000 | Loss: 0.00001462
Iteration 114/1000 | Loss: 0.00001462
Iteration 115/1000 | Loss: 0.00001462
Iteration 116/1000 | Loss: 0.00001462
Iteration 117/1000 | Loss: 0.00001462
Iteration 118/1000 | Loss: 0.00001462
Iteration 119/1000 | Loss: 0.00001462
Iteration 120/1000 | Loss: 0.00001461
Iteration 121/1000 | Loss: 0.00001461
Iteration 122/1000 | Loss: 0.00001461
Iteration 123/1000 | Loss: 0.00001461
Iteration 124/1000 | Loss: 0.00001460
Iteration 125/1000 | Loss: 0.00001460
Iteration 126/1000 | Loss: 0.00001460
Iteration 127/1000 | Loss: 0.00001460
Iteration 128/1000 | Loss: 0.00001460
Iteration 129/1000 | Loss: 0.00001460
Iteration 130/1000 | Loss: 0.00001459
Iteration 131/1000 | Loss: 0.00001459
Iteration 132/1000 | Loss: 0.00001459
Iteration 133/1000 | Loss: 0.00001459
Iteration 134/1000 | Loss: 0.00001459
Iteration 135/1000 | Loss: 0.00001459
Iteration 136/1000 | Loss: 0.00001459
Iteration 137/1000 | Loss: 0.00001458
Iteration 138/1000 | Loss: 0.00001458
Iteration 139/1000 | Loss: 0.00001458
Iteration 140/1000 | Loss: 0.00001458
Iteration 141/1000 | Loss: 0.00001458
Iteration 142/1000 | Loss: 0.00001458
Iteration 143/1000 | Loss: 0.00001458
Iteration 144/1000 | Loss: 0.00001458
Iteration 145/1000 | Loss: 0.00001458
Iteration 146/1000 | Loss: 0.00001458
Iteration 147/1000 | Loss: 0.00001457
Iteration 148/1000 | Loss: 0.00001457
Iteration 149/1000 | Loss: 0.00001457
Iteration 150/1000 | Loss: 0.00001457
Iteration 151/1000 | Loss: 0.00001457
Iteration 152/1000 | Loss: 0.00001457
Iteration 153/1000 | Loss: 0.00001457
Iteration 154/1000 | Loss: 0.00001457
Iteration 155/1000 | Loss: 0.00001456
Iteration 156/1000 | Loss: 0.00001456
Iteration 157/1000 | Loss: 0.00001456
Iteration 158/1000 | Loss: 0.00001456
Iteration 159/1000 | Loss: 0.00001456
Iteration 160/1000 | Loss: 0.00001456
Iteration 161/1000 | Loss: 0.00001456
Iteration 162/1000 | Loss: 0.00001456
Iteration 163/1000 | Loss: 0.00001455
Iteration 164/1000 | Loss: 0.00001455
Iteration 165/1000 | Loss: 0.00001455
Iteration 166/1000 | Loss: 0.00001455
Iteration 167/1000 | Loss: 0.00001455
Iteration 168/1000 | Loss: 0.00001455
Iteration 169/1000 | Loss: 0.00001455
Iteration 170/1000 | Loss: 0.00001455
Iteration 171/1000 | Loss: 0.00001455
Iteration 172/1000 | Loss: 0.00001455
Iteration 173/1000 | Loss: 0.00001455
Iteration 174/1000 | Loss: 0.00001455
Iteration 175/1000 | Loss: 0.00001455
Iteration 176/1000 | Loss: 0.00001455
Iteration 177/1000 | Loss: 0.00001455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.4551313142874278e-05, 1.4551313142874278e-05, 1.4551313142874278e-05, 1.4551313142874278e-05, 1.4551313142874278e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4551313142874278e-05

Optimization complete. Final v2v error: 3.2334866523742676 mm

Highest mean error: 3.418745279312134 mm for frame 22

Lowest mean error: 3.0155580043792725 mm for frame 2

Saving results

Total time: 38.87671208381653
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781803
Iteration 2/25 | Loss: 0.00137968
Iteration 3/25 | Loss: 0.00127882
Iteration 4/25 | Loss: 0.00126929
Iteration 5/25 | Loss: 0.00126671
Iteration 6/25 | Loss: 0.00126668
Iteration 7/25 | Loss: 0.00126668
Iteration 8/25 | Loss: 0.00126668
Iteration 9/25 | Loss: 0.00126668
Iteration 10/25 | Loss: 0.00126668
Iteration 11/25 | Loss: 0.00126668
Iteration 12/25 | Loss: 0.00126668
Iteration 13/25 | Loss: 0.00126668
Iteration 14/25 | Loss: 0.00126668
Iteration 15/25 | Loss: 0.00126668
Iteration 16/25 | Loss: 0.00126668
Iteration 17/25 | Loss: 0.00126668
Iteration 18/25 | Loss: 0.00126668
Iteration 19/25 | Loss: 0.00126668
Iteration 20/25 | Loss: 0.00126668
Iteration 21/25 | Loss: 0.00126668
Iteration 22/25 | Loss: 0.00126668
Iteration 23/25 | Loss: 0.00126668
Iteration 24/25 | Loss: 0.00126668
Iteration 25/25 | Loss: 0.00126668

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40559888
Iteration 2/25 | Loss: 0.00082924
Iteration 3/25 | Loss: 0.00082923
Iteration 4/25 | Loss: 0.00082923
Iteration 5/25 | Loss: 0.00082923
Iteration 6/25 | Loss: 0.00082923
Iteration 7/25 | Loss: 0.00082923
Iteration 8/25 | Loss: 0.00082923
Iteration 9/25 | Loss: 0.00082923
Iteration 10/25 | Loss: 0.00082923
Iteration 11/25 | Loss: 0.00082923
Iteration 12/25 | Loss: 0.00082923
Iteration 13/25 | Loss: 0.00082923
Iteration 14/25 | Loss: 0.00082923
Iteration 15/25 | Loss: 0.00082923
Iteration 16/25 | Loss: 0.00082923
Iteration 17/25 | Loss: 0.00082923
Iteration 18/25 | Loss: 0.00082923
Iteration 19/25 | Loss: 0.00082923
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008292306447401643, 0.0008292306447401643, 0.0008292306447401643, 0.0008292306447401643, 0.0008292306447401643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008292306447401643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082923
Iteration 2/1000 | Loss: 0.00003184
Iteration 3/1000 | Loss: 0.00002013
Iteration 4/1000 | Loss: 0.00001690
Iteration 5/1000 | Loss: 0.00001524
Iteration 6/1000 | Loss: 0.00001448
Iteration 7/1000 | Loss: 0.00001385
Iteration 8/1000 | Loss: 0.00001335
Iteration 9/1000 | Loss: 0.00001325
Iteration 10/1000 | Loss: 0.00001316
Iteration 11/1000 | Loss: 0.00001289
Iteration 12/1000 | Loss: 0.00001271
Iteration 13/1000 | Loss: 0.00001270
Iteration 14/1000 | Loss: 0.00001270
Iteration 15/1000 | Loss: 0.00001264
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001251
Iteration 18/1000 | Loss: 0.00001251
Iteration 19/1000 | Loss: 0.00001247
Iteration 20/1000 | Loss: 0.00001245
Iteration 21/1000 | Loss: 0.00001237
Iteration 22/1000 | Loss: 0.00001237
Iteration 23/1000 | Loss: 0.00001236
Iteration 24/1000 | Loss: 0.00001235
Iteration 25/1000 | Loss: 0.00001235
Iteration 26/1000 | Loss: 0.00001234
Iteration 27/1000 | Loss: 0.00001234
Iteration 28/1000 | Loss: 0.00001232
Iteration 29/1000 | Loss: 0.00001226
Iteration 30/1000 | Loss: 0.00001223
Iteration 31/1000 | Loss: 0.00001222
Iteration 32/1000 | Loss: 0.00001222
Iteration 33/1000 | Loss: 0.00001222
Iteration 34/1000 | Loss: 0.00001221
Iteration 35/1000 | Loss: 0.00001221
Iteration 36/1000 | Loss: 0.00001221
Iteration 37/1000 | Loss: 0.00001220
Iteration 38/1000 | Loss: 0.00001220
Iteration 39/1000 | Loss: 0.00001220
Iteration 40/1000 | Loss: 0.00001220
Iteration 41/1000 | Loss: 0.00001220
Iteration 42/1000 | Loss: 0.00001220
Iteration 43/1000 | Loss: 0.00001219
Iteration 44/1000 | Loss: 0.00001219
Iteration 45/1000 | Loss: 0.00001219
Iteration 46/1000 | Loss: 0.00001219
Iteration 47/1000 | Loss: 0.00001219
Iteration 48/1000 | Loss: 0.00001218
Iteration 49/1000 | Loss: 0.00001218
Iteration 50/1000 | Loss: 0.00001217
Iteration 51/1000 | Loss: 0.00001217
Iteration 52/1000 | Loss: 0.00001216
Iteration 53/1000 | Loss: 0.00001216
Iteration 54/1000 | Loss: 0.00001216
Iteration 55/1000 | Loss: 0.00001215
Iteration 56/1000 | Loss: 0.00001214
Iteration 57/1000 | Loss: 0.00001213
Iteration 58/1000 | Loss: 0.00001212
Iteration 59/1000 | Loss: 0.00001212
Iteration 60/1000 | Loss: 0.00001212
Iteration 61/1000 | Loss: 0.00001211
Iteration 62/1000 | Loss: 0.00001211
Iteration 63/1000 | Loss: 0.00001211
Iteration 64/1000 | Loss: 0.00001211
Iteration 65/1000 | Loss: 0.00001210
Iteration 66/1000 | Loss: 0.00001210
Iteration 67/1000 | Loss: 0.00001209
Iteration 68/1000 | Loss: 0.00001207
Iteration 69/1000 | Loss: 0.00001207
Iteration 70/1000 | Loss: 0.00001207
Iteration 71/1000 | Loss: 0.00001207
Iteration 72/1000 | Loss: 0.00001207
Iteration 73/1000 | Loss: 0.00001207
Iteration 74/1000 | Loss: 0.00001207
Iteration 75/1000 | Loss: 0.00001207
Iteration 76/1000 | Loss: 0.00001207
Iteration 77/1000 | Loss: 0.00001207
Iteration 78/1000 | Loss: 0.00001207
Iteration 79/1000 | Loss: 0.00001206
Iteration 80/1000 | Loss: 0.00001206
Iteration 81/1000 | Loss: 0.00001206
Iteration 82/1000 | Loss: 0.00001206
Iteration 83/1000 | Loss: 0.00001206
Iteration 84/1000 | Loss: 0.00001206
Iteration 85/1000 | Loss: 0.00001205
Iteration 86/1000 | Loss: 0.00001204
Iteration 87/1000 | Loss: 0.00001204
Iteration 88/1000 | Loss: 0.00001204
Iteration 89/1000 | Loss: 0.00001204
Iteration 90/1000 | Loss: 0.00001204
Iteration 91/1000 | Loss: 0.00001203
Iteration 92/1000 | Loss: 0.00001203
Iteration 93/1000 | Loss: 0.00001203
Iteration 94/1000 | Loss: 0.00001202
Iteration 95/1000 | Loss: 0.00001202
Iteration 96/1000 | Loss: 0.00001202
Iteration 97/1000 | Loss: 0.00001202
Iteration 98/1000 | Loss: 0.00001201
Iteration 99/1000 | Loss: 0.00001201
Iteration 100/1000 | Loss: 0.00001200
Iteration 101/1000 | Loss: 0.00001200
Iteration 102/1000 | Loss: 0.00001199
Iteration 103/1000 | Loss: 0.00001199
Iteration 104/1000 | Loss: 0.00001198
Iteration 105/1000 | Loss: 0.00001198
Iteration 106/1000 | Loss: 0.00001198
Iteration 107/1000 | Loss: 0.00001197
Iteration 108/1000 | Loss: 0.00001197
Iteration 109/1000 | Loss: 0.00001197
Iteration 110/1000 | Loss: 0.00001197
Iteration 111/1000 | Loss: 0.00001197
Iteration 112/1000 | Loss: 0.00001196
Iteration 113/1000 | Loss: 0.00001196
Iteration 114/1000 | Loss: 0.00001196
Iteration 115/1000 | Loss: 0.00001196
Iteration 116/1000 | Loss: 0.00001195
Iteration 117/1000 | Loss: 0.00001195
Iteration 118/1000 | Loss: 0.00001195
Iteration 119/1000 | Loss: 0.00001195
Iteration 120/1000 | Loss: 0.00001195
Iteration 121/1000 | Loss: 0.00001194
Iteration 122/1000 | Loss: 0.00001194
Iteration 123/1000 | Loss: 0.00001194
Iteration 124/1000 | Loss: 0.00001193
Iteration 125/1000 | Loss: 0.00001193
Iteration 126/1000 | Loss: 0.00001193
Iteration 127/1000 | Loss: 0.00001193
Iteration 128/1000 | Loss: 0.00001192
Iteration 129/1000 | Loss: 0.00001192
Iteration 130/1000 | Loss: 0.00001192
Iteration 131/1000 | Loss: 0.00001192
Iteration 132/1000 | Loss: 0.00001192
Iteration 133/1000 | Loss: 0.00001191
Iteration 134/1000 | Loss: 0.00001191
Iteration 135/1000 | Loss: 0.00001191
Iteration 136/1000 | Loss: 0.00001191
Iteration 137/1000 | Loss: 0.00001191
Iteration 138/1000 | Loss: 0.00001191
Iteration 139/1000 | Loss: 0.00001191
Iteration 140/1000 | Loss: 0.00001191
Iteration 141/1000 | Loss: 0.00001191
Iteration 142/1000 | Loss: 0.00001191
Iteration 143/1000 | Loss: 0.00001191
Iteration 144/1000 | Loss: 0.00001191
Iteration 145/1000 | Loss: 0.00001191
Iteration 146/1000 | Loss: 0.00001191
Iteration 147/1000 | Loss: 0.00001191
Iteration 148/1000 | Loss: 0.00001191
Iteration 149/1000 | Loss: 0.00001191
Iteration 150/1000 | Loss: 0.00001191
Iteration 151/1000 | Loss: 0.00001191
Iteration 152/1000 | Loss: 0.00001191
Iteration 153/1000 | Loss: 0.00001191
Iteration 154/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.1908833585039247e-05, 1.1908833585039247e-05, 1.1908833585039247e-05, 1.1908833585039247e-05, 1.1908833585039247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1908833585039247e-05

Optimization complete. Final v2v error: 2.9426870346069336 mm

Highest mean error: 3.150879144668579 mm for frame 45

Lowest mean error: 2.798553466796875 mm for frame 162

Saving results

Total time: 37.69694709777832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831432
Iteration 2/25 | Loss: 0.00136744
Iteration 3/25 | Loss: 0.00129347
Iteration 4/25 | Loss: 0.00128536
Iteration 5/25 | Loss: 0.00128317
Iteration 6/25 | Loss: 0.00128317
Iteration 7/25 | Loss: 0.00128317
Iteration 8/25 | Loss: 0.00128317
Iteration 9/25 | Loss: 0.00128317
Iteration 10/25 | Loss: 0.00128317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012831693748012185, 0.0012831693748012185, 0.0012831693748012185, 0.0012831693748012185, 0.0012831693748012185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012831693748012185

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.83416891
Iteration 2/25 | Loss: 0.00090987
Iteration 3/25 | Loss: 0.00090987
Iteration 4/25 | Loss: 0.00090987
Iteration 5/25 | Loss: 0.00090987
Iteration 6/25 | Loss: 0.00090987
Iteration 7/25 | Loss: 0.00090987
Iteration 8/25 | Loss: 0.00090987
Iteration 9/25 | Loss: 0.00090987
Iteration 10/25 | Loss: 0.00090987
Iteration 11/25 | Loss: 0.00090987
Iteration 12/25 | Loss: 0.00090987
Iteration 13/25 | Loss: 0.00090987
Iteration 14/25 | Loss: 0.00090987
Iteration 15/25 | Loss: 0.00090987
Iteration 16/25 | Loss: 0.00090987
Iteration 17/25 | Loss: 0.00090987
Iteration 18/25 | Loss: 0.00090987
Iteration 19/25 | Loss: 0.00090987
Iteration 20/25 | Loss: 0.00090987
Iteration 21/25 | Loss: 0.00090987
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009098667651414871, 0.0009098667651414871, 0.0009098667651414871, 0.0009098667651414871, 0.0009098667651414871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009098667651414871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090987
Iteration 2/1000 | Loss: 0.00002221
Iteration 3/1000 | Loss: 0.00001690
Iteration 4/1000 | Loss: 0.00001534
Iteration 5/1000 | Loss: 0.00001449
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001351
Iteration 8/1000 | Loss: 0.00001322
Iteration 9/1000 | Loss: 0.00001284
Iteration 10/1000 | Loss: 0.00001267
Iteration 11/1000 | Loss: 0.00001255
Iteration 12/1000 | Loss: 0.00001255
Iteration 13/1000 | Loss: 0.00001246
Iteration 14/1000 | Loss: 0.00001245
Iteration 15/1000 | Loss: 0.00001235
Iteration 16/1000 | Loss: 0.00001227
Iteration 17/1000 | Loss: 0.00001227
Iteration 18/1000 | Loss: 0.00001224
Iteration 19/1000 | Loss: 0.00001222
Iteration 20/1000 | Loss: 0.00001221
Iteration 21/1000 | Loss: 0.00001221
Iteration 22/1000 | Loss: 0.00001216
Iteration 23/1000 | Loss: 0.00001215
Iteration 24/1000 | Loss: 0.00001206
Iteration 25/1000 | Loss: 0.00001206
Iteration 26/1000 | Loss: 0.00001205
Iteration 27/1000 | Loss: 0.00001205
Iteration 28/1000 | Loss: 0.00001204
Iteration 29/1000 | Loss: 0.00001200
Iteration 30/1000 | Loss: 0.00001200
Iteration 31/1000 | Loss: 0.00001198
Iteration 32/1000 | Loss: 0.00001198
Iteration 33/1000 | Loss: 0.00001197
Iteration 34/1000 | Loss: 0.00001197
Iteration 35/1000 | Loss: 0.00001197
Iteration 36/1000 | Loss: 0.00001197
Iteration 37/1000 | Loss: 0.00001196
Iteration 38/1000 | Loss: 0.00001196
Iteration 39/1000 | Loss: 0.00001196
Iteration 40/1000 | Loss: 0.00001195
Iteration 41/1000 | Loss: 0.00001195
Iteration 42/1000 | Loss: 0.00001193
Iteration 43/1000 | Loss: 0.00001193
Iteration 44/1000 | Loss: 0.00001193
Iteration 45/1000 | Loss: 0.00001193
Iteration 46/1000 | Loss: 0.00001192
Iteration 47/1000 | Loss: 0.00001192
Iteration 48/1000 | Loss: 0.00001192
Iteration 49/1000 | Loss: 0.00001192
Iteration 50/1000 | Loss: 0.00001192
Iteration 51/1000 | Loss: 0.00001191
Iteration 52/1000 | Loss: 0.00001191
Iteration 53/1000 | Loss: 0.00001189
Iteration 54/1000 | Loss: 0.00001187
Iteration 55/1000 | Loss: 0.00001187
Iteration 56/1000 | Loss: 0.00001186
Iteration 57/1000 | Loss: 0.00001186
Iteration 58/1000 | Loss: 0.00001185
Iteration 59/1000 | Loss: 0.00001185
Iteration 60/1000 | Loss: 0.00001184
Iteration 61/1000 | Loss: 0.00001183
Iteration 62/1000 | Loss: 0.00001182
Iteration 63/1000 | Loss: 0.00001182
Iteration 64/1000 | Loss: 0.00001181
Iteration 65/1000 | Loss: 0.00001180
Iteration 66/1000 | Loss: 0.00001180
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001179
Iteration 69/1000 | Loss: 0.00001178
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001178
Iteration 72/1000 | Loss: 0.00001178
Iteration 73/1000 | Loss: 0.00001178
Iteration 74/1000 | Loss: 0.00001177
Iteration 75/1000 | Loss: 0.00001177
Iteration 76/1000 | Loss: 0.00001177
Iteration 77/1000 | Loss: 0.00001176
Iteration 78/1000 | Loss: 0.00001176
Iteration 79/1000 | Loss: 0.00001175
Iteration 80/1000 | Loss: 0.00001175
Iteration 81/1000 | Loss: 0.00001175
Iteration 82/1000 | Loss: 0.00001175
Iteration 83/1000 | Loss: 0.00001175
Iteration 84/1000 | Loss: 0.00001175
Iteration 85/1000 | Loss: 0.00001175
Iteration 86/1000 | Loss: 0.00001175
Iteration 87/1000 | Loss: 0.00001175
Iteration 88/1000 | Loss: 0.00001175
Iteration 89/1000 | Loss: 0.00001175
Iteration 90/1000 | Loss: 0.00001175
Iteration 91/1000 | Loss: 0.00001175
Iteration 92/1000 | Loss: 0.00001174
Iteration 93/1000 | Loss: 0.00001174
Iteration 94/1000 | Loss: 0.00001174
Iteration 95/1000 | Loss: 0.00001173
Iteration 96/1000 | Loss: 0.00001173
Iteration 97/1000 | Loss: 0.00001173
Iteration 98/1000 | Loss: 0.00001172
Iteration 99/1000 | Loss: 0.00001172
Iteration 100/1000 | Loss: 0.00001172
Iteration 101/1000 | Loss: 0.00001172
Iteration 102/1000 | Loss: 0.00001172
Iteration 103/1000 | Loss: 0.00001171
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001170
Iteration 106/1000 | Loss: 0.00001170
Iteration 107/1000 | Loss: 0.00001170
Iteration 108/1000 | Loss: 0.00001170
Iteration 109/1000 | Loss: 0.00001169
Iteration 110/1000 | Loss: 0.00001169
Iteration 111/1000 | Loss: 0.00001169
Iteration 112/1000 | Loss: 0.00001169
Iteration 113/1000 | Loss: 0.00001169
Iteration 114/1000 | Loss: 0.00001169
Iteration 115/1000 | Loss: 0.00001168
Iteration 116/1000 | Loss: 0.00001168
Iteration 117/1000 | Loss: 0.00001168
Iteration 118/1000 | Loss: 0.00001168
Iteration 119/1000 | Loss: 0.00001167
Iteration 120/1000 | Loss: 0.00001167
Iteration 121/1000 | Loss: 0.00001167
Iteration 122/1000 | Loss: 0.00001167
Iteration 123/1000 | Loss: 0.00001167
Iteration 124/1000 | Loss: 0.00001167
Iteration 125/1000 | Loss: 0.00001167
Iteration 126/1000 | Loss: 0.00001167
Iteration 127/1000 | Loss: 0.00001166
Iteration 128/1000 | Loss: 0.00001166
Iteration 129/1000 | Loss: 0.00001166
Iteration 130/1000 | Loss: 0.00001166
Iteration 131/1000 | Loss: 0.00001166
Iteration 132/1000 | Loss: 0.00001166
Iteration 133/1000 | Loss: 0.00001166
Iteration 134/1000 | Loss: 0.00001166
Iteration 135/1000 | Loss: 0.00001166
Iteration 136/1000 | Loss: 0.00001166
Iteration 137/1000 | Loss: 0.00001166
Iteration 138/1000 | Loss: 0.00001165
Iteration 139/1000 | Loss: 0.00001165
Iteration 140/1000 | Loss: 0.00001165
Iteration 141/1000 | Loss: 0.00001165
Iteration 142/1000 | Loss: 0.00001165
Iteration 143/1000 | Loss: 0.00001165
Iteration 144/1000 | Loss: 0.00001165
Iteration 145/1000 | Loss: 0.00001165
Iteration 146/1000 | Loss: 0.00001165
Iteration 147/1000 | Loss: 0.00001165
Iteration 148/1000 | Loss: 0.00001165
Iteration 149/1000 | Loss: 0.00001165
Iteration 150/1000 | Loss: 0.00001165
Iteration 151/1000 | Loss: 0.00001165
Iteration 152/1000 | Loss: 0.00001165
Iteration 153/1000 | Loss: 0.00001165
Iteration 154/1000 | Loss: 0.00001164
Iteration 155/1000 | Loss: 0.00001164
Iteration 156/1000 | Loss: 0.00001164
Iteration 157/1000 | Loss: 0.00001164
Iteration 158/1000 | Loss: 0.00001164
Iteration 159/1000 | Loss: 0.00001164
Iteration 160/1000 | Loss: 0.00001164
Iteration 161/1000 | Loss: 0.00001164
Iteration 162/1000 | Loss: 0.00001164
Iteration 163/1000 | Loss: 0.00001164
Iteration 164/1000 | Loss: 0.00001163
Iteration 165/1000 | Loss: 0.00001163
Iteration 166/1000 | Loss: 0.00001163
Iteration 167/1000 | Loss: 0.00001163
Iteration 168/1000 | Loss: 0.00001163
Iteration 169/1000 | Loss: 0.00001163
Iteration 170/1000 | Loss: 0.00001163
Iteration 171/1000 | Loss: 0.00001163
Iteration 172/1000 | Loss: 0.00001163
Iteration 173/1000 | Loss: 0.00001163
Iteration 174/1000 | Loss: 0.00001163
Iteration 175/1000 | Loss: 0.00001163
Iteration 176/1000 | Loss: 0.00001163
Iteration 177/1000 | Loss: 0.00001163
Iteration 178/1000 | Loss: 0.00001163
Iteration 179/1000 | Loss: 0.00001163
Iteration 180/1000 | Loss: 0.00001163
Iteration 181/1000 | Loss: 0.00001163
Iteration 182/1000 | Loss: 0.00001163
Iteration 183/1000 | Loss: 0.00001163
Iteration 184/1000 | Loss: 0.00001163
Iteration 185/1000 | Loss: 0.00001163
Iteration 186/1000 | Loss: 0.00001163
Iteration 187/1000 | Loss: 0.00001163
Iteration 188/1000 | Loss: 0.00001163
Iteration 189/1000 | Loss: 0.00001163
Iteration 190/1000 | Loss: 0.00001163
Iteration 191/1000 | Loss: 0.00001163
Iteration 192/1000 | Loss: 0.00001163
Iteration 193/1000 | Loss: 0.00001163
Iteration 194/1000 | Loss: 0.00001163
Iteration 195/1000 | Loss: 0.00001163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.1634104339464102e-05, 1.1634104339464102e-05, 1.1634104339464102e-05, 1.1634104339464102e-05, 1.1634104339464102e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1634104339464102e-05

Optimization complete. Final v2v error: 2.9249231815338135 mm

Highest mean error: 3.155245304107666 mm for frame 238

Lowest mean error: 2.755640983581543 mm for frame 139

Saving results

Total time: 45.82703995704651
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00642523
Iteration 2/25 | Loss: 0.00156677
Iteration 3/25 | Loss: 0.00139171
Iteration 4/25 | Loss: 0.00135154
Iteration 5/25 | Loss: 0.00134301
Iteration 6/25 | Loss: 0.00135000
Iteration 7/25 | Loss: 0.00134692
Iteration 8/25 | Loss: 0.00133808
Iteration 9/25 | Loss: 0.00133466
Iteration 10/25 | Loss: 0.00133285
Iteration 11/25 | Loss: 0.00133087
Iteration 12/25 | Loss: 0.00133034
Iteration 13/25 | Loss: 0.00133011
Iteration 14/25 | Loss: 0.00133001
Iteration 15/25 | Loss: 0.00133000
Iteration 16/25 | Loss: 0.00132999
Iteration 17/25 | Loss: 0.00132999
Iteration 18/25 | Loss: 0.00132999
Iteration 19/25 | Loss: 0.00132999
Iteration 20/25 | Loss: 0.00132998
Iteration 21/25 | Loss: 0.00132998
Iteration 22/25 | Loss: 0.00132998
Iteration 23/25 | Loss: 0.00132998
Iteration 24/25 | Loss: 0.00132998
Iteration 25/25 | Loss: 0.00132998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.74612904
Iteration 2/25 | Loss: 0.00085919
Iteration 3/25 | Loss: 0.00085919
Iteration 4/25 | Loss: 0.00085918
Iteration 5/25 | Loss: 0.00085918
Iteration 6/25 | Loss: 0.00085918
Iteration 7/25 | Loss: 0.00085918
Iteration 8/25 | Loss: 0.00085918
Iteration 9/25 | Loss: 0.00085918
Iteration 10/25 | Loss: 0.00085918
Iteration 11/25 | Loss: 0.00085918
Iteration 12/25 | Loss: 0.00085918
Iteration 13/25 | Loss: 0.00085918
Iteration 14/25 | Loss: 0.00085918
Iteration 15/25 | Loss: 0.00085918
Iteration 16/25 | Loss: 0.00085918
Iteration 17/25 | Loss: 0.00085918
Iteration 18/25 | Loss: 0.00085918
Iteration 19/25 | Loss: 0.00085918
Iteration 20/25 | Loss: 0.00085918
Iteration 21/25 | Loss: 0.00085918
Iteration 22/25 | Loss: 0.00085918
Iteration 23/25 | Loss: 0.00085918
Iteration 24/25 | Loss: 0.00085918
Iteration 25/25 | Loss: 0.00085918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085918
Iteration 2/1000 | Loss: 0.00003320
Iteration 3/1000 | Loss: 0.00002340
Iteration 4/1000 | Loss: 0.00002084
Iteration 5/1000 | Loss: 0.00001988
Iteration 6/1000 | Loss: 0.00004988
Iteration 7/1000 | Loss: 0.00004714
Iteration 8/1000 | Loss: 0.00015620
Iteration 9/1000 | Loss: 0.00002150
Iteration 10/1000 | Loss: 0.00001869
Iteration 11/1000 | Loss: 0.00001818
Iteration 12/1000 | Loss: 0.00001804
Iteration 13/1000 | Loss: 0.00001789
Iteration 14/1000 | Loss: 0.00005639
Iteration 15/1000 | Loss: 0.00007213
Iteration 16/1000 | Loss: 0.00002099
Iteration 17/1000 | Loss: 0.00002459
Iteration 18/1000 | Loss: 0.00001804
Iteration 19/1000 | Loss: 0.00001749
Iteration 20/1000 | Loss: 0.00001749
Iteration 21/1000 | Loss: 0.00001748
Iteration 22/1000 | Loss: 0.00001748
Iteration 23/1000 | Loss: 0.00001746
Iteration 24/1000 | Loss: 0.00001746
Iteration 25/1000 | Loss: 0.00001746
Iteration 26/1000 | Loss: 0.00001745
Iteration 27/1000 | Loss: 0.00001745
Iteration 28/1000 | Loss: 0.00001745
Iteration 29/1000 | Loss: 0.00001745
Iteration 30/1000 | Loss: 0.00001744
Iteration 31/1000 | Loss: 0.00001744
Iteration 32/1000 | Loss: 0.00001744
Iteration 33/1000 | Loss: 0.00001744
Iteration 34/1000 | Loss: 0.00001744
Iteration 35/1000 | Loss: 0.00001744
Iteration 36/1000 | Loss: 0.00001743
Iteration 37/1000 | Loss: 0.00001743
Iteration 38/1000 | Loss: 0.00001888
Iteration 39/1000 | Loss: 0.00004927
Iteration 40/1000 | Loss: 0.00002926
Iteration 41/1000 | Loss: 0.00001730
Iteration 42/1000 | Loss: 0.00001728
Iteration 43/1000 | Loss: 0.00001727
Iteration 44/1000 | Loss: 0.00001727
Iteration 45/1000 | Loss: 0.00001727
Iteration 46/1000 | Loss: 0.00001727
Iteration 47/1000 | Loss: 0.00001727
Iteration 48/1000 | Loss: 0.00001727
Iteration 49/1000 | Loss: 0.00001727
Iteration 50/1000 | Loss: 0.00001726
Iteration 51/1000 | Loss: 0.00001726
Iteration 52/1000 | Loss: 0.00001726
Iteration 53/1000 | Loss: 0.00001726
Iteration 54/1000 | Loss: 0.00001726
Iteration 55/1000 | Loss: 0.00001726
Iteration 56/1000 | Loss: 0.00001725
Iteration 57/1000 | Loss: 0.00001721
Iteration 58/1000 | Loss: 0.00001721
Iteration 59/1000 | Loss: 0.00001721
Iteration 60/1000 | Loss: 0.00001720
Iteration 61/1000 | Loss: 0.00001719
Iteration 62/1000 | Loss: 0.00001719
Iteration 63/1000 | Loss: 0.00001719
Iteration 64/1000 | Loss: 0.00001718
Iteration 65/1000 | Loss: 0.00001718
Iteration 66/1000 | Loss: 0.00001718
Iteration 67/1000 | Loss: 0.00001718
Iteration 68/1000 | Loss: 0.00001717
Iteration 69/1000 | Loss: 0.00001717
Iteration 70/1000 | Loss: 0.00001717
Iteration 71/1000 | Loss: 0.00001717
Iteration 72/1000 | Loss: 0.00001717
Iteration 73/1000 | Loss: 0.00001717
Iteration 74/1000 | Loss: 0.00001717
Iteration 75/1000 | Loss: 0.00001717
Iteration 76/1000 | Loss: 0.00001716
Iteration 77/1000 | Loss: 0.00001716
Iteration 78/1000 | Loss: 0.00001716
Iteration 79/1000 | Loss: 0.00001714
Iteration 80/1000 | Loss: 0.00001714
Iteration 81/1000 | Loss: 0.00001714
Iteration 82/1000 | Loss: 0.00001714
Iteration 83/1000 | Loss: 0.00001714
Iteration 84/1000 | Loss: 0.00001714
Iteration 85/1000 | Loss: 0.00001714
Iteration 86/1000 | Loss: 0.00001714
Iteration 87/1000 | Loss: 0.00001714
Iteration 88/1000 | Loss: 0.00001714
Iteration 89/1000 | Loss: 0.00001714
Iteration 90/1000 | Loss: 0.00001714
Iteration 91/1000 | Loss: 0.00001713
Iteration 92/1000 | Loss: 0.00001713
Iteration 93/1000 | Loss: 0.00001713
Iteration 94/1000 | Loss: 0.00001713
Iteration 95/1000 | Loss: 0.00001713
Iteration 96/1000 | Loss: 0.00001713
Iteration 97/1000 | Loss: 0.00001713
Iteration 98/1000 | Loss: 0.00001713
Iteration 99/1000 | Loss: 0.00001713
Iteration 100/1000 | Loss: 0.00001713
Iteration 101/1000 | Loss: 0.00001713
Iteration 102/1000 | Loss: 0.00001713
Iteration 103/1000 | Loss: 0.00001713
Iteration 104/1000 | Loss: 0.00001713
Iteration 105/1000 | Loss: 0.00001713
Iteration 106/1000 | Loss: 0.00001713
Iteration 107/1000 | Loss: 0.00001713
Iteration 108/1000 | Loss: 0.00001713
Iteration 109/1000 | Loss: 0.00001713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.7133826986537315e-05, 1.7133826986537315e-05, 1.7133826986537315e-05, 1.7133826986537315e-05, 1.7133826986537315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7133826986537315e-05

Optimization complete. Final v2v error: 3.4435009956359863 mm

Highest mean error: 5.939742565155029 mm for frame 130

Lowest mean error: 3.069652557373047 mm for frame 155

Saving results

Total time: 67.04534578323364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444290
Iteration 2/25 | Loss: 0.00141075
Iteration 3/25 | Loss: 0.00134389
Iteration 4/25 | Loss: 0.00133741
Iteration 5/25 | Loss: 0.00133609
Iteration 6/25 | Loss: 0.00133609
Iteration 7/25 | Loss: 0.00133609
Iteration 8/25 | Loss: 0.00133609
Iteration 9/25 | Loss: 0.00133609
Iteration 10/25 | Loss: 0.00133609
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013360915472730994, 0.0013360915472730994, 0.0013360915472730994, 0.0013360915472730994, 0.0013360915472730994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013360915472730994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40554678
Iteration 2/25 | Loss: 0.00087473
Iteration 3/25 | Loss: 0.00087473
Iteration 4/25 | Loss: 0.00087473
Iteration 5/25 | Loss: 0.00087473
Iteration 6/25 | Loss: 0.00087473
Iteration 7/25 | Loss: 0.00087473
Iteration 8/25 | Loss: 0.00087473
Iteration 9/25 | Loss: 0.00087473
Iteration 10/25 | Loss: 0.00087473
Iteration 11/25 | Loss: 0.00087473
Iteration 12/25 | Loss: 0.00087473
Iteration 13/25 | Loss: 0.00087473
Iteration 14/25 | Loss: 0.00087473
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008747262181714177, 0.0008747262181714177, 0.0008747262181714177, 0.0008747262181714177, 0.0008747262181714177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008747262181714177

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087473
Iteration 2/1000 | Loss: 0.00002883
Iteration 3/1000 | Loss: 0.00002033
Iteration 4/1000 | Loss: 0.00001883
Iteration 5/1000 | Loss: 0.00001789
Iteration 6/1000 | Loss: 0.00001747
Iteration 7/1000 | Loss: 0.00001721
Iteration 8/1000 | Loss: 0.00001709
Iteration 9/1000 | Loss: 0.00001686
Iteration 10/1000 | Loss: 0.00001677
Iteration 11/1000 | Loss: 0.00001664
Iteration 12/1000 | Loss: 0.00001663
Iteration 13/1000 | Loss: 0.00001663
Iteration 14/1000 | Loss: 0.00001658
Iteration 15/1000 | Loss: 0.00001656
Iteration 16/1000 | Loss: 0.00001644
Iteration 17/1000 | Loss: 0.00001633
Iteration 18/1000 | Loss: 0.00001624
Iteration 19/1000 | Loss: 0.00001616
Iteration 20/1000 | Loss: 0.00001616
Iteration 21/1000 | Loss: 0.00001605
Iteration 22/1000 | Loss: 0.00001604
Iteration 23/1000 | Loss: 0.00001599
Iteration 24/1000 | Loss: 0.00001599
Iteration 25/1000 | Loss: 0.00001597
Iteration 26/1000 | Loss: 0.00001597
Iteration 27/1000 | Loss: 0.00001597
Iteration 28/1000 | Loss: 0.00001596
Iteration 29/1000 | Loss: 0.00001596
Iteration 30/1000 | Loss: 0.00001596
Iteration 31/1000 | Loss: 0.00001595
Iteration 32/1000 | Loss: 0.00001595
Iteration 33/1000 | Loss: 0.00001594
Iteration 34/1000 | Loss: 0.00001593
Iteration 35/1000 | Loss: 0.00001593
Iteration 36/1000 | Loss: 0.00001593
Iteration 37/1000 | Loss: 0.00001592
Iteration 38/1000 | Loss: 0.00001592
Iteration 39/1000 | Loss: 0.00001591
Iteration 40/1000 | Loss: 0.00001591
Iteration 41/1000 | Loss: 0.00001590
Iteration 42/1000 | Loss: 0.00001590
Iteration 43/1000 | Loss: 0.00001590
Iteration 44/1000 | Loss: 0.00001590
Iteration 45/1000 | Loss: 0.00001590
Iteration 46/1000 | Loss: 0.00001590
Iteration 47/1000 | Loss: 0.00001590
Iteration 48/1000 | Loss: 0.00001590
Iteration 49/1000 | Loss: 0.00001590
Iteration 50/1000 | Loss: 0.00001590
Iteration 51/1000 | Loss: 0.00001590
Iteration 52/1000 | Loss: 0.00001590
Iteration 53/1000 | Loss: 0.00001590
Iteration 54/1000 | Loss: 0.00001590
Iteration 55/1000 | Loss: 0.00001590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 55. Stopping optimization.
Last 5 losses: [1.5895011529210024e-05, 1.5895011529210024e-05, 1.5895011529210024e-05, 1.5895011529210024e-05, 1.5895011529210024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5895011529210024e-05

Optimization complete. Final v2v error: 3.377835750579834 mm

Highest mean error: 3.580744743347168 mm for frame 178

Lowest mean error: 3.2037439346313477 mm for frame 83

Saving results

Total time: 33.16651010513306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00696808
Iteration 2/25 | Loss: 0.00155207
Iteration 3/25 | Loss: 0.00138409
Iteration 4/25 | Loss: 0.00135651
Iteration 5/25 | Loss: 0.00134935
Iteration 6/25 | Loss: 0.00134864
Iteration 7/25 | Loss: 0.00134647
Iteration 8/25 | Loss: 0.00134421
Iteration 9/25 | Loss: 0.00134158
Iteration 10/25 | Loss: 0.00134054
Iteration 11/25 | Loss: 0.00134018
Iteration 12/25 | Loss: 0.00133901
Iteration 13/25 | Loss: 0.00133894
Iteration 14/25 | Loss: 0.00133894
Iteration 15/25 | Loss: 0.00133893
Iteration 16/25 | Loss: 0.00133893
Iteration 17/25 | Loss: 0.00133893
Iteration 18/25 | Loss: 0.00133892
Iteration 19/25 | Loss: 0.00133892
Iteration 20/25 | Loss: 0.00133892
Iteration 21/25 | Loss: 0.00133892
Iteration 22/25 | Loss: 0.00133892
Iteration 23/25 | Loss: 0.00133892
Iteration 24/25 | Loss: 0.00133892
Iteration 25/25 | Loss: 0.00133892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.54715967
Iteration 2/25 | Loss: 0.00092925
Iteration 3/25 | Loss: 0.00091956
Iteration 4/25 | Loss: 0.00091956
Iteration 5/25 | Loss: 0.00091956
Iteration 6/25 | Loss: 0.00091956
Iteration 7/25 | Loss: 0.00091956
Iteration 8/25 | Loss: 0.00091956
Iteration 9/25 | Loss: 0.00091956
Iteration 10/25 | Loss: 0.00091956
Iteration 11/25 | Loss: 0.00091956
Iteration 12/25 | Loss: 0.00091956
Iteration 13/25 | Loss: 0.00091956
Iteration 14/25 | Loss: 0.00091956
Iteration 15/25 | Loss: 0.00091956
Iteration 16/25 | Loss: 0.00091956
Iteration 17/25 | Loss: 0.00091956
Iteration 18/25 | Loss: 0.00091956
Iteration 19/25 | Loss: 0.00091956
Iteration 20/25 | Loss: 0.00091956
Iteration 21/25 | Loss: 0.00091956
Iteration 22/25 | Loss: 0.00091956
Iteration 23/25 | Loss: 0.00091956
Iteration 24/25 | Loss: 0.00091956
Iteration 25/25 | Loss: 0.00091956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091956
Iteration 2/1000 | Loss: 0.00009165
Iteration 3/1000 | Loss: 0.00003733
Iteration 4/1000 | Loss: 0.00003047
Iteration 5/1000 | Loss: 0.00002764
Iteration 6/1000 | Loss: 0.00007900
Iteration 7/1000 | Loss: 0.00003005
Iteration 8/1000 | Loss: 0.00002747
Iteration 9/1000 | Loss: 0.00002631
Iteration 10/1000 | Loss: 0.00002568
Iteration 11/1000 | Loss: 0.00002520
Iteration 12/1000 | Loss: 0.00012207
Iteration 13/1000 | Loss: 0.00007845
Iteration 14/1000 | Loss: 0.00008284
Iteration 15/1000 | Loss: 0.00010637
Iteration 16/1000 | Loss: 0.00003038
Iteration 17/1000 | Loss: 0.00008433
Iteration 18/1000 | Loss: 0.00006474
Iteration 19/1000 | Loss: 0.00008889
Iteration 20/1000 | Loss: 0.00003106
Iteration 21/1000 | Loss: 0.00002650
Iteration 22/1000 | Loss: 0.00002580
Iteration 23/1000 | Loss: 0.00002469
Iteration 24/1000 | Loss: 0.00011309
Iteration 25/1000 | Loss: 0.00007408
Iteration 26/1000 | Loss: 0.00002364
Iteration 27/1000 | Loss: 0.00005665
Iteration 28/1000 | Loss: 0.00002248
Iteration 29/1000 | Loss: 0.00002187
Iteration 30/1000 | Loss: 0.00002157
Iteration 31/1000 | Loss: 0.00002123
Iteration 32/1000 | Loss: 0.00002102
Iteration 33/1000 | Loss: 0.00002102
Iteration 34/1000 | Loss: 0.00004136
Iteration 35/1000 | Loss: 0.00002699
Iteration 36/1000 | Loss: 0.00002126
Iteration 37/1000 | Loss: 0.00004112
Iteration 38/1000 | Loss: 0.00003989
Iteration 39/1000 | Loss: 0.00002794
Iteration 40/1000 | Loss: 0.00002410
Iteration 41/1000 | Loss: 0.00002268
Iteration 42/1000 | Loss: 0.00002221
Iteration 43/1000 | Loss: 0.00002177
Iteration 44/1000 | Loss: 0.00002130
Iteration 45/1000 | Loss: 0.00002456
Iteration 46/1000 | Loss: 0.00002101
Iteration 47/1000 | Loss: 0.00002061
Iteration 48/1000 | Loss: 0.00002040
Iteration 49/1000 | Loss: 0.00002036
Iteration 50/1000 | Loss: 0.00002031
Iteration 51/1000 | Loss: 0.00002031
Iteration 52/1000 | Loss: 0.00002030
Iteration 53/1000 | Loss: 0.00002030
Iteration 54/1000 | Loss: 0.00002029
Iteration 55/1000 | Loss: 0.00002029
Iteration 56/1000 | Loss: 0.00002029
Iteration 57/1000 | Loss: 0.00002028
Iteration 58/1000 | Loss: 0.00002028
Iteration 59/1000 | Loss: 0.00002028
Iteration 60/1000 | Loss: 0.00002028
Iteration 61/1000 | Loss: 0.00002027
Iteration 62/1000 | Loss: 0.00002027
Iteration 63/1000 | Loss: 0.00002027
Iteration 64/1000 | Loss: 0.00002027
Iteration 65/1000 | Loss: 0.00002027
Iteration 66/1000 | Loss: 0.00002027
Iteration 67/1000 | Loss: 0.00002027
Iteration 68/1000 | Loss: 0.00002027
Iteration 69/1000 | Loss: 0.00002027
Iteration 70/1000 | Loss: 0.00002027
Iteration 71/1000 | Loss: 0.00002027
Iteration 72/1000 | Loss: 0.00002027
Iteration 73/1000 | Loss: 0.00002027
Iteration 74/1000 | Loss: 0.00002027
Iteration 75/1000 | Loss: 0.00002026
Iteration 76/1000 | Loss: 0.00002026
Iteration 77/1000 | Loss: 0.00002026
Iteration 78/1000 | Loss: 0.00002026
Iteration 79/1000 | Loss: 0.00002026
Iteration 80/1000 | Loss: 0.00002026
Iteration 81/1000 | Loss: 0.00002026
Iteration 82/1000 | Loss: 0.00002026
Iteration 83/1000 | Loss: 0.00002026
Iteration 84/1000 | Loss: 0.00002026
Iteration 85/1000 | Loss: 0.00002026
Iteration 86/1000 | Loss: 0.00002026
Iteration 87/1000 | Loss: 0.00002025
Iteration 88/1000 | Loss: 0.00002025
Iteration 89/1000 | Loss: 0.00002025
Iteration 90/1000 | Loss: 0.00002025
Iteration 91/1000 | Loss: 0.00002025
Iteration 92/1000 | Loss: 0.00002025
Iteration 93/1000 | Loss: 0.00002025
Iteration 94/1000 | Loss: 0.00002025
Iteration 95/1000 | Loss: 0.00002025
Iteration 96/1000 | Loss: 0.00002025
Iteration 97/1000 | Loss: 0.00002025
Iteration 98/1000 | Loss: 0.00002025
Iteration 99/1000 | Loss: 0.00002025
Iteration 100/1000 | Loss: 0.00002025
Iteration 101/1000 | Loss: 0.00002025
Iteration 102/1000 | Loss: 0.00002024
Iteration 103/1000 | Loss: 0.00002024
Iteration 104/1000 | Loss: 0.00002024
Iteration 105/1000 | Loss: 0.00002024
Iteration 106/1000 | Loss: 0.00002024
Iteration 107/1000 | Loss: 0.00002024
Iteration 108/1000 | Loss: 0.00002024
Iteration 109/1000 | Loss: 0.00002024
Iteration 110/1000 | Loss: 0.00002024
Iteration 111/1000 | Loss: 0.00002023
Iteration 112/1000 | Loss: 0.00002023
Iteration 113/1000 | Loss: 0.00002023
Iteration 114/1000 | Loss: 0.00002023
Iteration 115/1000 | Loss: 0.00002023
Iteration 116/1000 | Loss: 0.00002023
Iteration 117/1000 | Loss: 0.00002023
Iteration 118/1000 | Loss: 0.00002023
Iteration 119/1000 | Loss: 0.00002023
Iteration 120/1000 | Loss: 0.00002023
Iteration 121/1000 | Loss: 0.00002022
Iteration 122/1000 | Loss: 0.00002022
Iteration 123/1000 | Loss: 0.00002022
Iteration 124/1000 | Loss: 0.00002022
Iteration 125/1000 | Loss: 0.00002022
Iteration 126/1000 | Loss: 0.00002022
Iteration 127/1000 | Loss: 0.00002022
Iteration 128/1000 | Loss: 0.00002022
Iteration 129/1000 | Loss: 0.00002022
Iteration 130/1000 | Loss: 0.00002022
Iteration 131/1000 | Loss: 0.00002022
Iteration 132/1000 | Loss: 0.00002022
Iteration 133/1000 | Loss: 0.00002022
Iteration 134/1000 | Loss: 0.00002021
Iteration 135/1000 | Loss: 0.00002021
Iteration 136/1000 | Loss: 0.00002021
Iteration 137/1000 | Loss: 0.00002021
Iteration 138/1000 | Loss: 0.00002021
Iteration 139/1000 | Loss: 0.00002021
Iteration 140/1000 | Loss: 0.00002021
Iteration 141/1000 | Loss: 0.00002021
Iteration 142/1000 | Loss: 0.00002020
Iteration 143/1000 | Loss: 0.00002020
Iteration 144/1000 | Loss: 0.00002020
Iteration 145/1000 | Loss: 0.00002020
Iteration 146/1000 | Loss: 0.00002020
Iteration 147/1000 | Loss: 0.00002020
Iteration 148/1000 | Loss: 0.00002020
Iteration 149/1000 | Loss: 0.00002019
Iteration 150/1000 | Loss: 0.00002019
Iteration 151/1000 | Loss: 0.00002019
Iteration 152/1000 | Loss: 0.00002019
Iteration 153/1000 | Loss: 0.00002019
Iteration 154/1000 | Loss: 0.00002019
Iteration 155/1000 | Loss: 0.00002018
Iteration 156/1000 | Loss: 0.00002018
Iteration 157/1000 | Loss: 0.00002018
Iteration 158/1000 | Loss: 0.00002018
Iteration 159/1000 | Loss: 0.00002018
Iteration 160/1000 | Loss: 0.00002018
Iteration 161/1000 | Loss: 0.00002018
Iteration 162/1000 | Loss: 0.00002018
Iteration 163/1000 | Loss: 0.00002017
Iteration 164/1000 | Loss: 0.00002017
Iteration 165/1000 | Loss: 0.00002016
Iteration 166/1000 | Loss: 0.00002015
Iteration 167/1000 | Loss: 0.00002015
Iteration 168/1000 | Loss: 0.00002015
Iteration 169/1000 | Loss: 0.00002015
Iteration 170/1000 | Loss: 0.00002015
Iteration 171/1000 | Loss: 0.00002014
Iteration 172/1000 | Loss: 0.00002014
Iteration 173/1000 | Loss: 0.00002014
Iteration 174/1000 | Loss: 0.00002013
Iteration 175/1000 | Loss: 0.00002013
Iteration 176/1000 | Loss: 0.00002012
Iteration 177/1000 | Loss: 0.00002012
Iteration 178/1000 | Loss: 0.00002011
Iteration 179/1000 | Loss: 0.00002011
Iteration 180/1000 | Loss: 0.00002011
Iteration 181/1000 | Loss: 0.00002011
Iteration 182/1000 | Loss: 0.00002010
Iteration 183/1000 | Loss: 0.00002010
Iteration 184/1000 | Loss: 0.00002010
Iteration 185/1000 | Loss: 0.00002010
Iteration 186/1000 | Loss: 0.00002010
Iteration 187/1000 | Loss: 0.00002010
Iteration 188/1000 | Loss: 0.00002009
Iteration 189/1000 | Loss: 0.00002009
Iteration 190/1000 | Loss: 0.00002009
Iteration 191/1000 | Loss: 0.00002009
Iteration 192/1000 | Loss: 0.00002009
Iteration 193/1000 | Loss: 0.00002009
Iteration 194/1000 | Loss: 0.00002009
Iteration 195/1000 | Loss: 0.00002008
Iteration 196/1000 | Loss: 0.00002008
Iteration 197/1000 | Loss: 0.00002008
Iteration 198/1000 | Loss: 0.00002008
Iteration 199/1000 | Loss: 0.00002008
Iteration 200/1000 | Loss: 0.00002008
Iteration 201/1000 | Loss: 0.00002008
Iteration 202/1000 | Loss: 0.00002008
Iteration 203/1000 | Loss: 0.00002007
Iteration 204/1000 | Loss: 0.00002007
Iteration 205/1000 | Loss: 0.00002007
Iteration 206/1000 | Loss: 0.00002007
Iteration 207/1000 | Loss: 0.00002007
Iteration 208/1000 | Loss: 0.00002006
Iteration 209/1000 | Loss: 0.00002006
Iteration 210/1000 | Loss: 0.00002006
Iteration 211/1000 | Loss: 0.00002006
Iteration 212/1000 | Loss: 0.00002005
Iteration 213/1000 | Loss: 0.00002005
Iteration 214/1000 | Loss: 0.00002005
Iteration 215/1000 | Loss: 0.00002005
Iteration 216/1000 | Loss: 0.00002005
Iteration 217/1000 | Loss: 0.00002005
Iteration 218/1000 | Loss: 0.00002005
Iteration 219/1000 | Loss: 0.00002004
Iteration 220/1000 | Loss: 0.00002004
Iteration 221/1000 | Loss: 0.00002004
Iteration 222/1000 | Loss: 0.00002004
Iteration 223/1000 | Loss: 0.00002004
Iteration 224/1000 | Loss: 0.00002004
Iteration 225/1000 | Loss: 0.00002004
Iteration 226/1000 | Loss: 0.00002004
Iteration 227/1000 | Loss: 0.00002004
Iteration 228/1000 | Loss: 0.00002004
Iteration 229/1000 | Loss: 0.00002004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [2.0044517441419885e-05, 2.0044517441419885e-05, 2.0044517441419885e-05, 2.0044517441419885e-05, 2.0044517441419885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0044517441419885e-05

Optimization complete. Final v2v error: 3.707632064819336 mm

Highest mean error: 5.750269412994385 mm for frame 67

Lowest mean error: 2.9386355876922607 mm for frame 197

Saving results

Total time: 114.4444625377655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812602
Iteration 2/25 | Loss: 0.00134224
Iteration 3/25 | Loss: 0.00127970
Iteration 4/25 | Loss: 0.00127253
Iteration 5/25 | Loss: 0.00127107
Iteration 6/25 | Loss: 0.00127107
Iteration 7/25 | Loss: 0.00127107
Iteration 8/25 | Loss: 0.00127107
Iteration 9/25 | Loss: 0.00127107
Iteration 10/25 | Loss: 0.00127107
Iteration 11/25 | Loss: 0.00127107
Iteration 12/25 | Loss: 0.00127107
Iteration 13/25 | Loss: 0.00127107
Iteration 14/25 | Loss: 0.00127107
Iteration 15/25 | Loss: 0.00127107
Iteration 16/25 | Loss: 0.00127107
Iteration 17/25 | Loss: 0.00127107
Iteration 18/25 | Loss: 0.00127107
Iteration 19/25 | Loss: 0.00127107
Iteration 20/25 | Loss: 0.00127107
Iteration 21/25 | Loss: 0.00127107
Iteration 22/25 | Loss: 0.00127107
Iteration 23/25 | Loss: 0.00127107
Iteration 24/25 | Loss: 0.00127107
Iteration 25/25 | Loss: 0.00127107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38684523
Iteration 2/25 | Loss: 0.00076781
Iteration 3/25 | Loss: 0.00076778
Iteration 4/25 | Loss: 0.00076778
Iteration 5/25 | Loss: 0.00076778
Iteration 6/25 | Loss: 0.00076778
Iteration 7/25 | Loss: 0.00076778
Iteration 8/25 | Loss: 0.00076778
Iteration 9/25 | Loss: 0.00076778
Iteration 10/25 | Loss: 0.00076778
Iteration 11/25 | Loss: 0.00076778
Iteration 12/25 | Loss: 0.00076778
Iteration 13/25 | Loss: 0.00076778
Iteration 14/25 | Loss: 0.00076778
Iteration 15/25 | Loss: 0.00076778
Iteration 16/25 | Loss: 0.00076778
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007677779067307711, 0.0007677779067307711, 0.0007677779067307711, 0.0007677779067307711, 0.0007677779067307711]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007677779067307711

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076778
Iteration 2/1000 | Loss: 0.00002399
Iteration 3/1000 | Loss: 0.00001647
Iteration 4/1000 | Loss: 0.00001459
Iteration 5/1000 | Loss: 0.00001375
Iteration 6/1000 | Loss: 0.00001334
Iteration 7/1000 | Loss: 0.00001279
Iteration 8/1000 | Loss: 0.00001260
Iteration 9/1000 | Loss: 0.00001247
Iteration 10/1000 | Loss: 0.00001246
Iteration 11/1000 | Loss: 0.00001225
Iteration 12/1000 | Loss: 0.00001220
Iteration 13/1000 | Loss: 0.00001215
Iteration 14/1000 | Loss: 0.00001211
Iteration 15/1000 | Loss: 0.00001210
Iteration 16/1000 | Loss: 0.00001209
Iteration 17/1000 | Loss: 0.00001199
Iteration 18/1000 | Loss: 0.00001198
Iteration 19/1000 | Loss: 0.00001192
Iteration 20/1000 | Loss: 0.00001190
Iteration 21/1000 | Loss: 0.00001189
Iteration 22/1000 | Loss: 0.00001185
Iteration 23/1000 | Loss: 0.00001184
Iteration 24/1000 | Loss: 0.00001183
Iteration 25/1000 | Loss: 0.00001182
Iteration 26/1000 | Loss: 0.00001180
Iteration 27/1000 | Loss: 0.00001173
Iteration 28/1000 | Loss: 0.00001165
Iteration 29/1000 | Loss: 0.00001165
Iteration 30/1000 | Loss: 0.00001163
Iteration 31/1000 | Loss: 0.00001163
Iteration 32/1000 | Loss: 0.00001163
Iteration 33/1000 | Loss: 0.00001163
Iteration 34/1000 | Loss: 0.00001163
Iteration 35/1000 | Loss: 0.00001163
Iteration 36/1000 | Loss: 0.00001163
Iteration 37/1000 | Loss: 0.00001163
Iteration 38/1000 | Loss: 0.00001163
Iteration 39/1000 | Loss: 0.00001163
Iteration 40/1000 | Loss: 0.00001162
Iteration 41/1000 | Loss: 0.00001161
Iteration 42/1000 | Loss: 0.00001160
Iteration 43/1000 | Loss: 0.00001160
Iteration 44/1000 | Loss: 0.00001160
Iteration 45/1000 | Loss: 0.00001160
Iteration 46/1000 | Loss: 0.00001159
Iteration 47/1000 | Loss: 0.00001159
Iteration 48/1000 | Loss: 0.00001159
Iteration 49/1000 | Loss: 0.00001159
Iteration 50/1000 | Loss: 0.00001159
Iteration 51/1000 | Loss: 0.00001158
Iteration 52/1000 | Loss: 0.00001158
Iteration 53/1000 | Loss: 0.00001158
Iteration 54/1000 | Loss: 0.00001157
Iteration 55/1000 | Loss: 0.00001157
Iteration 56/1000 | Loss: 0.00001157
Iteration 57/1000 | Loss: 0.00001157
Iteration 58/1000 | Loss: 0.00001156
Iteration 59/1000 | Loss: 0.00001156
Iteration 60/1000 | Loss: 0.00001155
Iteration 61/1000 | Loss: 0.00001155
Iteration 62/1000 | Loss: 0.00001155
Iteration 63/1000 | Loss: 0.00001154
Iteration 64/1000 | Loss: 0.00001154
Iteration 65/1000 | Loss: 0.00001153
Iteration 66/1000 | Loss: 0.00001152
Iteration 67/1000 | Loss: 0.00001152
Iteration 68/1000 | Loss: 0.00001152
Iteration 69/1000 | Loss: 0.00001152
Iteration 70/1000 | Loss: 0.00001152
Iteration 71/1000 | Loss: 0.00001152
Iteration 72/1000 | Loss: 0.00001152
Iteration 73/1000 | Loss: 0.00001152
Iteration 74/1000 | Loss: 0.00001152
Iteration 75/1000 | Loss: 0.00001152
Iteration 76/1000 | Loss: 0.00001152
Iteration 77/1000 | Loss: 0.00001152
Iteration 78/1000 | Loss: 0.00001152
Iteration 79/1000 | Loss: 0.00001152
Iteration 80/1000 | Loss: 0.00001152
Iteration 81/1000 | Loss: 0.00001152
Iteration 82/1000 | Loss: 0.00001152
Iteration 83/1000 | Loss: 0.00001152
Iteration 84/1000 | Loss: 0.00001152
Iteration 85/1000 | Loss: 0.00001152
Iteration 86/1000 | Loss: 0.00001152
Iteration 87/1000 | Loss: 0.00001152
Iteration 88/1000 | Loss: 0.00001152
Iteration 89/1000 | Loss: 0.00001152
Iteration 90/1000 | Loss: 0.00001152
Iteration 91/1000 | Loss: 0.00001152
Iteration 92/1000 | Loss: 0.00001152
Iteration 93/1000 | Loss: 0.00001152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.1519211511767935e-05, 1.1519211511767935e-05, 1.1519211511767935e-05, 1.1519211511767935e-05, 1.1519211511767935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1519211511767935e-05

Optimization complete. Final v2v error: 2.90295672416687 mm

Highest mean error: 3.0168001651763916 mm for frame 122

Lowest mean error: 2.8084702491760254 mm for frame 38

Saving results

Total time: 30.308114767074585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993275
Iteration 2/25 | Loss: 0.00993275
Iteration 3/25 | Loss: 0.00252644
Iteration 4/25 | Loss: 0.00212186
Iteration 5/25 | Loss: 0.00210790
Iteration 6/25 | Loss: 0.00205431
Iteration 7/25 | Loss: 0.00187209
Iteration 8/25 | Loss: 0.00177826
Iteration 9/25 | Loss: 0.00179721
Iteration 10/25 | Loss: 0.00178803
Iteration 11/25 | Loss: 0.00170763
Iteration 12/25 | Loss: 0.00164540
Iteration 13/25 | Loss: 0.00158755
Iteration 14/25 | Loss: 0.00155990
Iteration 15/25 | Loss: 0.00153666
Iteration 16/25 | Loss: 0.00152458
Iteration 17/25 | Loss: 0.00151749
Iteration 18/25 | Loss: 0.00151170
Iteration 19/25 | Loss: 0.00150789
Iteration 20/25 | Loss: 0.00150838
Iteration 21/25 | Loss: 0.00150619
Iteration 22/25 | Loss: 0.00150244
Iteration 23/25 | Loss: 0.00149963
Iteration 24/25 | Loss: 0.00150070
Iteration 25/25 | Loss: 0.00149861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33991623
Iteration 2/25 | Loss: 0.00313278
Iteration 3/25 | Loss: 0.00313277
Iteration 4/25 | Loss: 0.00313277
Iteration 5/25 | Loss: 0.00313277
Iteration 6/25 | Loss: 0.00313277
Iteration 7/25 | Loss: 0.00313277
Iteration 8/25 | Loss: 0.00313277
Iteration 9/25 | Loss: 0.00313277
Iteration 10/25 | Loss: 0.00313277
Iteration 11/25 | Loss: 0.00313277
Iteration 12/25 | Loss: 0.00313277
Iteration 13/25 | Loss: 0.00313277
Iteration 14/25 | Loss: 0.00313277
Iteration 15/25 | Loss: 0.00313277
Iteration 16/25 | Loss: 0.00313277
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003132770536467433, 0.003132770536467433, 0.003132770536467433, 0.003132770536467433, 0.003132770536467433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003132770536467433

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00313277
Iteration 2/1000 | Loss: 0.00045794
Iteration 3/1000 | Loss: 0.00035746
Iteration 4/1000 | Loss: 0.00022486
Iteration 5/1000 | Loss: 0.00019868
Iteration 6/1000 | Loss: 0.00018650
Iteration 7/1000 | Loss: 0.00016825
Iteration 8/1000 | Loss: 0.00021196
Iteration 9/1000 | Loss: 0.00026001
Iteration 10/1000 | Loss: 0.00015098
Iteration 11/1000 | Loss: 0.00015210
Iteration 12/1000 | Loss: 0.00020633
Iteration 13/1000 | Loss: 0.00017754
Iteration 14/1000 | Loss: 0.00017702
Iteration 15/1000 | Loss: 0.00015429
Iteration 16/1000 | Loss: 0.00020582
Iteration 17/1000 | Loss: 0.00024050
Iteration 18/1000 | Loss: 0.00019988
Iteration 19/1000 | Loss: 0.00019363
Iteration 20/1000 | Loss: 0.00019659
Iteration 21/1000 | Loss: 0.00022435
Iteration 22/1000 | Loss: 0.00026271
Iteration 23/1000 | Loss: 0.00024776
Iteration 24/1000 | Loss: 0.00060342
Iteration 25/1000 | Loss: 0.00049787
Iteration 26/1000 | Loss: 0.00014303
Iteration 27/1000 | Loss: 0.00014070
Iteration 28/1000 | Loss: 0.00013686
Iteration 29/1000 | Loss: 0.00013547
Iteration 30/1000 | Loss: 0.00013382
Iteration 31/1000 | Loss: 0.00013305
Iteration 32/1000 | Loss: 0.00013257
Iteration 33/1000 | Loss: 0.00013225
Iteration 34/1000 | Loss: 0.00013219
Iteration 35/1000 | Loss: 0.00013192
Iteration 36/1000 | Loss: 0.00013172
Iteration 37/1000 | Loss: 0.00013171
Iteration 38/1000 | Loss: 0.00013161
Iteration 39/1000 | Loss: 0.00013149
Iteration 40/1000 | Loss: 0.00013131
Iteration 41/1000 | Loss: 0.00013122
Iteration 42/1000 | Loss: 0.00013122
Iteration 43/1000 | Loss: 0.00013121
Iteration 44/1000 | Loss: 0.00013116
Iteration 45/1000 | Loss: 0.00013116
Iteration 46/1000 | Loss: 0.00013115
Iteration 47/1000 | Loss: 0.00013114
Iteration 48/1000 | Loss: 0.00013113
Iteration 49/1000 | Loss: 0.00013113
Iteration 50/1000 | Loss: 0.00013113
Iteration 51/1000 | Loss: 0.00013112
Iteration 52/1000 | Loss: 0.00013112
Iteration 53/1000 | Loss: 0.00013111
Iteration 54/1000 | Loss: 0.00013111
Iteration 55/1000 | Loss: 0.00013111
Iteration 56/1000 | Loss: 0.00013111
Iteration 57/1000 | Loss: 0.00013110
Iteration 58/1000 | Loss: 0.00013104
Iteration 59/1000 | Loss: 0.00013098
Iteration 60/1000 | Loss: 0.00013092
Iteration 61/1000 | Loss: 0.00013067
Iteration 62/1000 | Loss: 0.00013637
Iteration 63/1000 | Loss: 0.00013047
Iteration 64/1000 | Loss: 0.00012933
Iteration 65/1000 | Loss: 0.00012721
Iteration 66/1000 | Loss: 0.00012435
Iteration 67/1000 | Loss: 0.00165970
Iteration 68/1000 | Loss: 0.00260718
Iteration 69/1000 | Loss: 0.00050912
Iteration 70/1000 | Loss: 0.00065525
Iteration 71/1000 | Loss: 0.00088580
Iteration 72/1000 | Loss: 0.00034267
Iteration 73/1000 | Loss: 0.00060946
Iteration 74/1000 | Loss: 0.00011839
Iteration 75/1000 | Loss: 0.00011007
Iteration 76/1000 | Loss: 0.00010356
Iteration 77/1000 | Loss: 0.00009859
Iteration 78/1000 | Loss: 0.00009531
Iteration 79/1000 | Loss: 0.00009331
Iteration 80/1000 | Loss: 0.00009143
Iteration 81/1000 | Loss: 0.00009023
Iteration 82/1000 | Loss: 0.00008930
Iteration 83/1000 | Loss: 0.00008778
Iteration 84/1000 | Loss: 0.00008691
Iteration 85/1000 | Loss: 0.00008619
Iteration 86/1000 | Loss: 0.00008549
Iteration 87/1000 | Loss: 0.00008493
Iteration 88/1000 | Loss: 0.00008449
Iteration 89/1000 | Loss: 0.00008409
Iteration 90/1000 | Loss: 0.00008366
Iteration 91/1000 | Loss: 0.00008341
Iteration 92/1000 | Loss: 0.00008320
Iteration 93/1000 | Loss: 0.00008292
Iteration 94/1000 | Loss: 0.00008261
Iteration 95/1000 | Loss: 0.00018951
Iteration 96/1000 | Loss: 0.00008518
Iteration 97/1000 | Loss: 0.00008285
Iteration 98/1000 | Loss: 0.00008115
Iteration 99/1000 | Loss: 0.00008036
Iteration 100/1000 | Loss: 0.00007917
Iteration 101/1000 | Loss: 0.00123625
Iteration 102/1000 | Loss: 0.00090717
Iteration 103/1000 | Loss: 0.00023791
Iteration 104/1000 | Loss: 0.00012876
Iteration 105/1000 | Loss: 0.00018139
Iteration 106/1000 | Loss: 0.00193866
Iteration 107/1000 | Loss: 0.00035526
Iteration 108/1000 | Loss: 0.00068580
Iteration 109/1000 | Loss: 0.00051164
Iteration 110/1000 | Loss: 0.00036258
Iteration 111/1000 | Loss: 0.00020132
Iteration 112/1000 | Loss: 0.00007563
Iteration 113/1000 | Loss: 0.00006886
Iteration 114/1000 | Loss: 0.00012092
Iteration 115/1000 | Loss: 0.00008973
Iteration 116/1000 | Loss: 0.00007949
Iteration 117/1000 | Loss: 0.00005751
Iteration 118/1000 | Loss: 0.00016905
Iteration 119/1000 | Loss: 0.00083028
Iteration 120/1000 | Loss: 0.00054091
Iteration 121/1000 | Loss: 0.00032591
Iteration 122/1000 | Loss: 0.00137382
Iteration 123/1000 | Loss: 0.00126750
Iteration 124/1000 | Loss: 0.00011161
Iteration 125/1000 | Loss: 0.00022909
Iteration 126/1000 | Loss: 0.00106142
Iteration 127/1000 | Loss: 0.00117004
Iteration 128/1000 | Loss: 0.00142450
Iteration 129/1000 | Loss: 0.00015363
Iteration 130/1000 | Loss: 0.00009096
Iteration 131/1000 | Loss: 0.00123492
Iteration 132/1000 | Loss: 0.00055397
Iteration 133/1000 | Loss: 0.00045874
Iteration 134/1000 | Loss: 0.00038617
Iteration 135/1000 | Loss: 0.00029353
Iteration 136/1000 | Loss: 0.00024488
Iteration 137/1000 | Loss: 0.00013457
Iteration 138/1000 | Loss: 0.00006749
Iteration 139/1000 | Loss: 0.00003918
Iteration 140/1000 | Loss: 0.00003023
Iteration 141/1000 | Loss: 0.00002755
Iteration 142/1000 | Loss: 0.00002533
Iteration 143/1000 | Loss: 0.00002350
Iteration 144/1000 | Loss: 0.00002174
Iteration 145/1000 | Loss: 0.00002087
Iteration 146/1000 | Loss: 0.00002020
Iteration 147/1000 | Loss: 0.00001971
Iteration 148/1000 | Loss: 0.00001922
Iteration 149/1000 | Loss: 0.00001878
Iteration 150/1000 | Loss: 0.00001837
Iteration 151/1000 | Loss: 0.00001806
Iteration 152/1000 | Loss: 0.00001793
Iteration 153/1000 | Loss: 0.00001790
Iteration 154/1000 | Loss: 0.00001778
Iteration 155/1000 | Loss: 0.00001771
Iteration 156/1000 | Loss: 0.00001770
Iteration 157/1000 | Loss: 0.00001769
Iteration 158/1000 | Loss: 0.00001765
Iteration 159/1000 | Loss: 0.00001764
Iteration 160/1000 | Loss: 0.00001761
Iteration 161/1000 | Loss: 0.00001761
Iteration 162/1000 | Loss: 0.00001761
Iteration 163/1000 | Loss: 0.00001760
Iteration 164/1000 | Loss: 0.00001759
Iteration 165/1000 | Loss: 0.00001759
Iteration 166/1000 | Loss: 0.00001758
Iteration 167/1000 | Loss: 0.00001758
Iteration 168/1000 | Loss: 0.00001756
Iteration 169/1000 | Loss: 0.00001756
Iteration 170/1000 | Loss: 0.00001756
Iteration 171/1000 | Loss: 0.00001755
Iteration 172/1000 | Loss: 0.00001755
Iteration 173/1000 | Loss: 0.00001755
Iteration 174/1000 | Loss: 0.00001755
Iteration 175/1000 | Loss: 0.00001755
Iteration 176/1000 | Loss: 0.00001754
Iteration 177/1000 | Loss: 0.00001754
Iteration 178/1000 | Loss: 0.00001754
Iteration 179/1000 | Loss: 0.00001754
Iteration 180/1000 | Loss: 0.00001754
Iteration 181/1000 | Loss: 0.00001754
Iteration 182/1000 | Loss: 0.00001754
Iteration 183/1000 | Loss: 0.00001754
Iteration 184/1000 | Loss: 0.00001753
Iteration 185/1000 | Loss: 0.00001753
Iteration 186/1000 | Loss: 0.00001753
Iteration 187/1000 | Loss: 0.00001753
Iteration 188/1000 | Loss: 0.00001752
Iteration 189/1000 | Loss: 0.00001752
Iteration 190/1000 | Loss: 0.00001752
Iteration 191/1000 | Loss: 0.00001752
Iteration 192/1000 | Loss: 0.00001752
Iteration 193/1000 | Loss: 0.00001752
Iteration 194/1000 | Loss: 0.00001752
Iteration 195/1000 | Loss: 0.00001752
Iteration 196/1000 | Loss: 0.00001752
Iteration 197/1000 | Loss: 0.00001752
Iteration 198/1000 | Loss: 0.00001752
Iteration 199/1000 | Loss: 0.00001752
Iteration 200/1000 | Loss: 0.00001752
Iteration 201/1000 | Loss: 0.00001752
Iteration 202/1000 | Loss: 0.00001751
Iteration 203/1000 | Loss: 0.00001751
Iteration 204/1000 | Loss: 0.00001751
Iteration 205/1000 | Loss: 0.00001751
Iteration 206/1000 | Loss: 0.00001751
Iteration 207/1000 | Loss: 0.00001751
Iteration 208/1000 | Loss: 0.00001751
Iteration 209/1000 | Loss: 0.00001751
Iteration 210/1000 | Loss: 0.00001751
Iteration 211/1000 | Loss: 0.00001750
Iteration 212/1000 | Loss: 0.00001750
Iteration 213/1000 | Loss: 0.00001750
Iteration 214/1000 | Loss: 0.00001749
Iteration 215/1000 | Loss: 0.00001749
Iteration 216/1000 | Loss: 0.00001749
Iteration 217/1000 | Loss: 0.00001749
Iteration 218/1000 | Loss: 0.00001749
Iteration 219/1000 | Loss: 0.00001749
Iteration 220/1000 | Loss: 0.00001749
Iteration 221/1000 | Loss: 0.00001749
Iteration 222/1000 | Loss: 0.00001749
Iteration 223/1000 | Loss: 0.00001749
Iteration 224/1000 | Loss: 0.00001749
Iteration 225/1000 | Loss: 0.00001749
Iteration 226/1000 | Loss: 0.00001749
Iteration 227/1000 | Loss: 0.00001748
Iteration 228/1000 | Loss: 0.00001748
Iteration 229/1000 | Loss: 0.00001748
Iteration 230/1000 | Loss: 0.00001748
Iteration 231/1000 | Loss: 0.00001748
Iteration 232/1000 | Loss: 0.00001748
Iteration 233/1000 | Loss: 0.00001748
Iteration 234/1000 | Loss: 0.00001748
Iteration 235/1000 | Loss: 0.00001748
Iteration 236/1000 | Loss: 0.00001748
Iteration 237/1000 | Loss: 0.00001748
Iteration 238/1000 | Loss: 0.00001748
Iteration 239/1000 | Loss: 0.00001748
Iteration 240/1000 | Loss: 0.00001748
Iteration 241/1000 | Loss: 0.00001748
Iteration 242/1000 | Loss: 0.00001748
Iteration 243/1000 | Loss: 0.00001748
Iteration 244/1000 | Loss: 0.00001747
Iteration 245/1000 | Loss: 0.00001747
Iteration 246/1000 | Loss: 0.00001747
Iteration 247/1000 | Loss: 0.00001747
Iteration 248/1000 | Loss: 0.00001747
Iteration 249/1000 | Loss: 0.00001747
Iteration 250/1000 | Loss: 0.00001747
Iteration 251/1000 | Loss: 0.00001747
Iteration 252/1000 | Loss: 0.00001747
Iteration 253/1000 | Loss: 0.00001747
Iteration 254/1000 | Loss: 0.00001747
Iteration 255/1000 | Loss: 0.00001747
Iteration 256/1000 | Loss: 0.00001747
Iteration 257/1000 | Loss: 0.00001747
Iteration 258/1000 | Loss: 0.00001747
Iteration 259/1000 | Loss: 0.00001747
Iteration 260/1000 | Loss: 0.00001747
Iteration 261/1000 | Loss: 0.00001747
Iteration 262/1000 | Loss: 0.00001747
Iteration 263/1000 | Loss: 0.00001747
Iteration 264/1000 | Loss: 0.00001747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [1.746889938658569e-05, 1.746889938658569e-05, 1.746889938658569e-05, 1.746889938658569e-05, 1.746889938658569e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.746889938658569e-05

Optimization complete. Final v2v error: 3.4951093196868896 mm

Highest mean error: 3.6975655555725098 mm for frame 116

Lowest mean error: 3.3166451454162598 mm for frame 21

Saving results

Total time: 275.37450075149536
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447630
Iteration 2/25 | Loss: 0.00139200
Iteration 3/25 | Loss: 0.00132965
Iteration 4/25 | Loss: 0.00131695
Iteration 5/25 | Loss: 0.00131137
Iteration 6/25 | Loss: 0.00131039
Iteration 7/25 | Loss: 0.00131039
Iteration 8/25 | Loss: 0.00131039
Iteration 9/25 | Loss: 0.00131039
Iteration 10/25 | Loss: 0.00131039
Iteration 11/25 | Loss: 0.00131039
Iteration 12/25 | Loss: 0.00131039
Iteration 13/25 | Loss: 0.00131039
Iteration 14/25 | Loss: 0.00131039
Iteration 15/25 | Loss: 0.00131039
Iteration 16/25 | Loss: 0.00131039
Iteration 17/25 | Loss: 0.00131039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013103944947943091, 0.0013103944947943091, 0.0013103944947943091, 0.0013103944947943091, 0.0013103944947943091]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013103944947943091

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55055785
Iteration 2/25 | Loss: 0.00094105
Iteration 3/25 | Loss: 0.00094105
Iteration 4/25 | Loss: 0.00094104
Iteration 5/25 | Loss: 0.00094104
Iteration 6/25 | Loss: 0.00094104
Iteration 7/25 | Loss: 0.00094104
Iteration 8/25 | Loss: 0.00094104
Iteration 9/25 | Loss: 0.00094104
Iteration 10/25 | Loss: 0.00094104
Iteration 11/25 | Loss: 0.00094104
Iteration 12/25 | Loss: 0.00094104
Iteration 13/25 | Loss: 0.00094104
Iteration 14/25 | Loss: 0.00094104
Iteration 15/25 | Loss: 0.00094104
Iteration 16/25 | Loss: 0.00094104
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009410422062501311, 0.0009410422062501311, 0.0009410422062501311, 0.0009410422062501311, 0.0009410422062501311]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009410422062501311

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094104
Iteration 2/1000 | Loss: 0.00005252
Iteration 3/1000 | Loss: 0.00003445
Iteration 4/1000 | Loss: 0.00002990
Iteration 5/1000 | Loss: 0.00002757
Iteration 6/1000 | Loss: 0.00002608
Iteration 7/1000 | Loss: 0.00002490
Iteration 8/1000 | Loss: 0.00002419
Iteration 9/1000 | Loss: 0.00002356
Iteration 10/1000 | Loss: 0.00002310
Iteration 11/1000 | Loss: 0.00002270
Iteration 12/1000 | Loss: 0.00002233
Iteration 13/1000 | Loss: 0.00002211
Iteration 14/1000 | Loss: 0.00002206
Iteration 15/1000 | Loss: 0.00002204
Iteration 16/1000 | Loss: 0.00002197
Iteration 17/1000 | Loss: 0.00002187
Iteration 18/1000 | Loss: 0.00002183
Iteration 19/1000 | Loss: 0.00002173
Iteration 20/1000 | Loss: 0.00002168
Iteration 21/1000 | Loss: 0.00002155
Iteration 22/1000 | Loss: 0.00002154
Iteration 23/1000 | Loss: 0.00002153
Iteration 24/1000 | Loss: 0.00002151
Iteration 25/1000 | Loss: 0.00002151
Iteration 26/1000 | Loss: 0.00002150
Iteration 27/1000 | Loss: 0.00002147
Iteration 28/1000 | Loss: 0.00002145
Iteration 29/1000 | Loss: 0.00002144
Iteration 30/1000 | Loss: 0.00002142
Iteration 31/1000 | Loss: 0.00002141
Iteration 32/1000 | Loss: 0.00002141
Iteration 33/1000 | Loss: 0.00002140
Iteration 34/1000 | Loss: 0.00002139
Iteration 35/1000 | Loss: 0.00002139
Iteration 36/1000 | Loss: 0.00002138
Iteration 37/1000 | Loss: 0.00002137
Iteration 38/1000 | Loss: 0.00002137
Iteration 39/1000 | Loss: 0.00002136
Iteration 40/1000 | Loss: 0.00002136
Iteration 41/1000 | Loss: 0.00002135
Iteration 42/1000 | Loss: 0.00002135
Iteration 43/1000 | Loss: 0.00002134
Iteration 44/1000 | Loss: 0.00002134
Iteration 45/1000 | Loss: 0.00002133
Iteration 46/1000 | Loss: 0.00002133
Iteration 47/1000 | Loss: 0.00002133
Iteration 48/1000 | Loss: 0.00002132
Iteration 49/1000 | Loss: 0.00002131
Iteration 50/1000 | Loss: 0.00002130
Iteration 51/1000 | Loss: 0.00002130
Iteration 52/1000 | Loss: 0.00002130
Iteration 53/1000 | Loss: 0.00002129
Iteration 54/1000 | Loss: 0.00002129
Iteration 55/1000 | Loss: 0.00002128
Iteration 56/1000 | Loss: 0.00002128
Iteration 57/1000 | Loss: 0.00002128
Iteration 58/1000 | Loss: 0.00002127
Iteration 59/1000 | Loss: 0.00002127
Iteration 60/1000 | Loss: 0.00002127
Iteration 61/1000 | Loss: 0.00002127
Iteration 62/1000 | Loss: 0.00002127
Iteration 63/1000 | Loss: 0.00002127
Iteration 64/1000 | Loss: 0.00002126
Iteration 65/1000 | Loss: 0.00002126
Iteration 66/1000 | Loss: 0.00002126
Iteration 67/1000 | Loss: 0.00002126
Iteration 68/1000 | Loss: 0.00002125
Iteration 69/1000 | Loss: 0.00002125
Iteration 70/1000 | Loss: 0.00002125
Iteration 71/1000 | Loss: 0.00002125
Iteration 72/1000 | Loss: 0.00002124
Iteration 73/1000 | Loss: 0.00002124
Iteration 74/1000 | Loss: 0.00002124
Iteration 75/1000 | Loss: 0.00002123
Iteration 76/1000 | Loss: 0.00002123
Iteration 77/1000 | Loss: 0.00002122
Iteration 78/1000 | Loss: 0.00002122
Iteration 79/1000 | Loss: 0.00002122
Iteration 80/1000 | Loss: 0.00002121
Iteration 81/1000 | Loss: 0.00002121
Iteration 82/1000 | Loss: 0.00002121
Iteration 83/1000 | Loss: 0.00002121
Iteration 84/1000 | Loss: 0.00002120
Iteration 85/1000 | Loss: 0.00002120
Iteration 86/1000 | Loss: 0.00002120
Iteration 87/1000 | Loss: 0.00002120
Iteration 88/1000 | Loss: 0.00002119
Iteration 89/1000 | Loss: 0.00002119
Iteration 90/1000 | Loss: 0.00002119
Iteration 91/1000 | Loss: 0.00002118
Iteration 92/1000 | Loss: 0.00002118
Iteration 93/1000 | Loss: 0.00002118
Iteration 94/1000 | Loss: 0.00002118
Iteration 95/1000 | Loss: 0.00002118
Iteration 96/1000 | Loss: 0.00002117
Iteration 97/1000 | Loss: 0.00002117
Iteration 98/1000 | Loss: 0.00002117
Iteration 99/1000 | Loss: 0.00002117
Iteration 100/1000 | Loss: 0.00002117
Iteration 101/1000 | Loss: 0.00002116
Iteration 102/1000 | Loss: 0.00002116
Iteration 103/1000 | Loss: 0.00002116
Iteration 104/1000 | Loss: 0.00002116
Iteration 105/1000 | Loss: 0.00002116
Iteration 106/1000 | Loss: 0.00002116
Iteration 107/1000 | Loss: 0.00002115
Iteration 108/1000 | Loss: 0.00002115
Iteration 109/1000 | Loss: 0.00002115
Iteration 110/1000 | Loss: 0.00002115
Iteration 111/1000 | Loss: 0.00002115
Iteration 112/1000 | Loss: 0.00002115
Iteration 113/1000 | Loss: 0.00002115
Iteration 114/1000 | Loss: 0.00002115
Iteration 115/1000 | Loss: 0.00002115
Iteration 116/1000 | Loss: 0.00002114
Iteration 117/1000 | Loss: 0.00002114
Iteration 118/1000 | Loss: 0.00002114
Iteration 119/1000 | Loss: 0.00002114
Iteration 120/1000 | Loss: 0.00002114
Iteration 121/1000 | Loss: 0.00002114
Iteration 122/1000 | Loss: 0.00002114
Iteration 123/1000 | Loss: 0.00002113
Iteration 124/1000 | Loss: 0.00002113
Iteration 125/1000 | Loss: 0.00002113
Iteration 126/1000 | Loss: 0.00002113
Iteration 127/1000 | Loss: 0.00002112
Iteration 128/1000 | Loss: 0.00002112
Iteration 129/1000 | Loss: 0.00002112
Iteration 130/1000 | Loss: 0.00002112
Iteration 131/1000 | Loss: 0.00002112
Iteration 132/1000 | Loss: 0.00002111
Iteration 133/1000 | Loss: 0.00002111
Iteration 134/1000 | Loss: 0.00002111
Iteration 135/1000 | Loss: 0.00002111
Iteration 136/1000 | Loss: 0.00002110
Iteration 137/1000 | Loss: 0.00002110
Iteration 138/1000 | Loss: 0.00002110
Iteration 139/1000 | Loss: 0.00002110
Iteration 140/1000 | Loss: 0.00002110
Iteration 141/1000 | Loss: 0.00002109
Iteration 142/1000 | Loss: 0.00002109
Iteration 143/1000 | Loss: 0.00002109
Iteration 144/1000 | Loss: 0.00002109
Iteration 145/1000 | Loss: 0.00002109
Iteration 146/1000 | Loss: 0.00002109
Iteration 147/1000 | Loss: 0.00002109
Iteration 148/1000 | Loss: 0.00002109
Iteration 149/1000 | Loss: 0.00002109
Iteration 150/1000 | Loss: 0.00002108
Iteration 151/1000 | Loss: 0.00002108
Iteration 152/1000 | Loss: 0.00002108
Iteration 153/1000 | Loss: 0.00002108
Iteration 154/1000 | Loss: 0.00002108
Iteration 155/1000 | Loss: 0.00002108
Iteration 156/1000 | Loss: 0.00002108
Iteration 157/1000 | Loss: 0.00002108
Iteration 158/1000 | Loss: 0.00002108
Iteration 159/1000 | Loss: 0.00002108
Iteration 160/1000 | Loss: 0.00002107
Iteration 161/1000 | Loss: 0.00002107
Iteration 162/1000 | Loss: 0.00002107
Iteration 163/1000 | Loss: 0.00002107
Iteration 164/1000 | Loss: 0.00002107
Iteration 165/1000 | Loss: 0.00002107
Iteration 166/1000 | Loss: 0.00002107
Iteration 167/1000 | Loss: 0.00002107
Iteration 168/1000 | Loss: 0.00002107
Iteration 169/1000 | Loss: 0.00002107
Iteration 170/1000 | Loss: 0.00002106
Iteration 171/1000 | Loss: 0.00002106
Iteration 172/1000 | Loss: 0.00002106
Iteration 173/1000 | Loss: 0.00002106
Iteration 174/1000 | Loss: 0.00002106
Iteration 175/1000 | Loss: 0.00002106
Iteration 176/1000 | Loss: 0.00002106
Iteration 177/1000 | Loss: 0.00002106
Iteration 178/1000 | Loss: 0.00002106
Iteration 179/1000 | Loss: 0.00002105
Iteration 180/1000 | Loss: 0.00002105
Iteration 181/1000 | Loss: 0.00002105
Iteration 182/1000 | Loss: 0.00002105
Iteration 183/1000 | Loss: 0.00002105
Iteration 184/1000 | Loss: 0.00002105
Iteration 185/1000 | Loss: 0.00002105
Iteration 186/1000 | Loss: 0.00002104
Iteration 187/1000 | Loss: 0.00002104
Iteration 188/1000 | Loss: 0.00002104
Iteration 189/1000 | Loss: 0.00002104
Iteration 190/1000 | Loss: 0.00002104
Iteration 191/1000 | Loss: 0.00002104
Iteration 192/1000 | Loss: 0.00002104
Iteration 193/1000 | Loss: 0.00002104
Iteration 194/1000 | Loss: 0.00002103
Iteration 195/1000 | Loss: 0.00002103
Iteration 196/1000 | Loss: 0.00002103
Iteration 197/1000 | Loss: 0.00002103
Iteration 198/1000 | Loss: 0.00002103
Iteration 199/1000 | Loss: 0.00002102
Iteration 200/1000 | Loss: 0.00002102
Iteration 201/1000 | Loss: 0.00002102
Iteration 202/1000 | Loss: 0.00002102
Iteration 203/1000 | Loss: 0.00002102
Iteration 204/1000 | Loss: 0.00002102
Iteration 205/1000 | Loss: 0.00002102
Iteration 206/1000 | Loss: 0.00002102
Iteration 207/1000 | Loss: 0.00002102
Iteration 208/1000 | Loss: 0.00002102
Iteration 209/1000 | Loss: 0.00002102
Iteration 210/1000 | Loss: 0.00002102
Iteration 211/1000 | Loss: 0.00002102
Iteration 212/1000 | Loss: 0.00002102
Iteration 213/1000 | Loss: 0.00002102
Iteration 214/1000 | Loss: 0.00002101
Iteration 215/1000 | Loss: 0.00002101
Iteration 216/1000 | Loss: 0.00002101
Iteration 217/1000 | Loss: 0.00002101
Iteration 218/1000 | Loss: 0.00002101
Iteration 219/1000 | Loss: 0.00002101
Iteration 220/1000 | Loss: 0.00002101
Iteration 221/1000 | Loss: 0.00002101
Iteration 222/1000 | Loss: 0.00002101
Iteration 223/1000 | Loss: 0.00002101
Iteration 224/1000 | Loss: 0.00002101
Iteration 225/1000 | Loss: 0.00002101
Iteration 226/1000 | Loss: 0.00002101
Iteration 227/1000 | Loss: 0.00002101
Iteration 228/1000 | Loss: 0.00002101
Iteration 229/1000 | Loss: 0.00002101
Iteration 230/1000 | Loss: 0.00002101
Iteration 231/1000 | Loss: 0.00002101
Iteration 232/1000 | Loss: 0.00002101
Iteration 233/1000 | Loss: 0.00002101
Iteration 234/1000 | Loss: 0.00002100
Iteration 235/1000 | Loss: 0.00002100
Iteration 236/1000 | Loss: 0.00002100
Iteration 237/1000 | Loss: 0.00002100
Iteration 238/1000 | Loss: 0.00002100
Iteration 239/1000 | Loss: 0.00002100
Iteration 240/1000 | Loss: 0.00002100
Iteration 241/1000 | Loss: 0.00002100
Iteration 242/1000 | Loss: 0.00002100
Iteration 243/1000 | Loss: 0.00002100
Iteration 244/1000 | Loss: 0.00002100
Iteration 245/1000 | Loss: 0.00002100
Iteration 246/1000 | Loss: 0.00002100
Iteration 247/1000 | Loss: 0.00002100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [2.1001023924327455e-05, 2.1001023924327455e-05, 2.1001023924327455e-05, 2.1001023924327455e-05, 2.1001023924327455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1001023924327455e-05

Optimization complete. Final v2v error: 3.8143177032470703 mm

Highest mean error: 6.148633003234863 mm for frame 65

Lowest mean error: 2.9144747257232666 mm for frame 100

Saving results

Total time: 55.96762681007385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441581
Iteration 2/25 | Loss: 0.00137263
Iteration 3/25 | Loss: 0.00129356
Iteration 4/25 | Loss: 0.00128080
Iteration 5/25 | Loss: 0.00127599
Iteration 6/25 | Loss: 0.00127528
Iteration 7/25 | Loss: 0.00127528
Iteration 8/25 | Loss: 0.00127528
Iteration 9/25 | Loss: 0.00127528
Iteration 10/25 | Loss: 0.00127528
Iteration 11/25 | Loss: 0.00127528
Iteration 12/25 | Loss: 0.00127528
Iteration 13/25 | Loss: 0.00127528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012752818875014782, 0.0012752818875014782, 0.0012752818875014782, 0.0012752818875014782, 0.0012752818875014782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012752818875014782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.62785959
Iteration 2/25 | Loss: 0.00088909
Iteration 3/25 | Loss: 0.00088908
Iteration 4/25 | Loss: 0.00088908
Iteration 5/25 | Loss: 0.00088908
Iteration 6/25 | Loss: 0.00088908
Iteration 7/25 | Loss: 0.00088908
Iteration 8/25 | Loss: 0.00088908
Iteration 9/25 | Loss: 0.00088908
Iteration 10/25 | Loss: 0.00088908
Iteration 11/25 | Loss: 0.00088908
Iteration 12/25 | Loss: 0.00088908
Iteration 13/25 | Loss: 0.00088908
Iteration 14/25 | Loss: 0.00088908
Iteration 15/25 | Loss: 0.00088908
Iteration 16/25 | Loss: 0.00088908
Iteration 17/25 | Loss: 0.00088908
Iteration 18/25 | Loss: 0.00088908
Iteration 19/25 | Loss: 0.00088908
Iteration 20/25 | Loss: 0.00088908
Iteration 21/25 | Loss: 0.00088908
Iteration 22/25 | Loss: 0.00088908
Iteration 23/25 | Loss: 0.00088908
Iteration 24/25 | Loss: 0.00088908
Iteration 25/25 | Loss: 0.00088908

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088908
Iteration 2/1000 | Loss: 0.00002824
Iteration 3/1000 | Loss: 0.00002052
Iteration 4/1000 | Loss: 0.00001804
Iteration 5/1000 | Loss: 0.00001723
Iteration 6/1000 | Loss: 0.00001642
Iteration 7/1000 | Loss: 0.00001581
Iteration 8/1000 | Loss: 0.00001546
Iteration 9/1000 | Loss: 0.00001516
Iteration 10/1000 | Loss: 0.00001480
Iteration 11/1000 | Loss: 0.00001464
Iteration 12/1000 | Loss: 0.00001456
Iteration 13/1000 | Loss: 0.00001448
Iteration 14/1000 | Loss: 0.00001445
Iteration 15/1000 | Loss: 0.00001433
Iteration 16/1000 | Loss: 0.00001431
Iteration 17/1000 | Loss: 0.00001421
Iteration 18/1000 | Loss: 0.00001413
Iteration 19/1000 | Loss: 0.00001410
Iteration 20/1000 | Loss: 0.00001409
Iteration 21/1000 | Loss: 0.00001409
Iteration 22/1000 | Loss: 0.00001405
Iteration 23/1000 | Loss: 0.00001404
Iteration 24/1000 | Loss: 0.00001401
Iteration 25/1000 | Loss: 0.00001399
Iteration 26/1000 | Loss: 0.00001397
Iteration 27/1000 | Loss: 0.00001397
Iteration 28/1000 | Loss: 0.00001396
Iteration 29/1000 | Loss: 0.00001395
Iteration 30/1000 | Loss: 0.00001395
Iteration 31/1000 | Loss: 0.00001386
Iteration 32/1000 | Loss: 0.00001386
Iteration 33/1000 | Loss: 0.00001383
Iteration 34/1000 | Loss: 0.00001382
Iteration 35/1000 | Loss: 0.00001382
Iteration 36/1000 | Loss: 0.00001381
Iteration 37/1000 | Loss: 0.00001380
Iteration 38/1000 | Loss: 0.00001380
Iteration 39/1000 | Loss: 0.00001379
Iteration 40/1000 | Loss: 0.00001378
Iteration 41/1000 | Loss: 0.00001378
Iteration 42/1000 | Loss: 0.00001378
Iteration 43/1000 | Loss: 0.00001378
Iteration 44/1000 | Loss: 0.00001378
Iteration 45/1000 | Loss: 0.00001378
Iteration 46/1000 | Loss: 0.00001377
Iteration 47/1000 | Loss: 0.00001377
Iteration 48/1000 | Loss: 0.00001377
Iteration 49/1000 | Loss: 0.00001376
Iteration 50/1000 | Loss: 0.00001376
Iteration 51/1000 | Loss: 0.00001376
Iteration 52/1000 | Loss: 0.00001375
Iteration 53/1000 | Loss: 0.00001375
Iteration 54/1000 | Loss: 0.00001375
Iteration 55/1000 | Loss: 0.00001374
Iteration 56/1000 | Loss: 0.00001374
Iteration 57/1000 | Loss: 0.00001374
Iteration 58/1000 | Loss: 0.00001374
Iteration 59/1000 | Loss: 0.00001372
Iteration 60/1000 | Loss: 0.00001372
Iteration 61/1000 | Loss: 0.00001371
Iteration 62/1000 | Loss: 0.00001371
Iteration 63/1000 | Loss: 0.00001370
Iteration 64/1000 | Loss: 0.00001369
Iteration 65/1000 | Loss: 0.00001369
Iteration 66/1000 | Loss: 0.00001366
Iteration 67/1000 | Loss: 0.00001366
Iteration 68/1000 | Loss: 0.00001365
Iteration 69/1000 | Loss: 0.00001365
Iteration 70/1000 | Loss: 0.00001364
Iteration 71/1000 | Loss: 0.00001363
Iteration 72/1000 | Loss: 0.00001362
Iteration 73/1000 | Loss: 0.00001362
Iteration 74/1000 | Loss: 0.00001362
Iteration 75/1000 | Loss: 0.00001361
Iteration 76/1000 | Loss: 0.00001361
Iteration 77/1000 | Loss: 0.00001361
Iteration 78/1000 | Loss: 0.00001360
Iteration 79/1000 | Loss: 0.00001360
Iteration 80/1000 | Loss: 0.00001359
Iteration 81/1000 | Loss: 0.00001359
Iteration 82/1000 | Loss: 0.00001359
Iteration 83/1000 | Loss: 0.00001358
Iteration 84/1000 | Loss: 0.00001358
Iteration 85/1000 | Loss: 0.00001358
Iteration 86/1000 | Loss: 0.00001357
Iteration 87/1000 | Loss: 0.00001357
Iteration 88/1000 | Loss: 0.00001357
Iteration 89/1000 | Loss: 0.00001356
Iteration 90/1000 | Loss: 0.00001356
Iteration 91/1000 | Loss: 0.00001356
Iteration 92/1000 | Loss: 0.00001356
Iteration 93/1000 | Loss: 0.00001356
Iteration 94/1000 | Loss: 0.00001355
Iteration 95/1000 | Loss: 0.00001355
Iteration 96/1000 | Loss: 0.00001355
Iteration 97/1000 | Loss: 0.00001355
Iteration 98/1000 | Loss: 0.00001355
Iteration 99/1000 | Loss: 0.00001354
Iteration 100/1000 | Loss: 0.00001354
Iteration 101/1000 | Loss: 0.00001354
Iteration 102/1000 | Loss: 0.00001354
Iteration 103/1000 | Loss: 0.00001354
Iteration 104/1000 | Loss: 0.00001354
Iteration 105/1000 | Loss: 0.00001354
Iteration 106/1000 | Loss: 0.00001353
Iteration 107/1000 | Loss: 0.00001353
Iteration 108/1000 | Loss: 0.00001353
Iteration 109/1000 | Loss: 0.00001353
Iteration 110/1000 | Loss: 0.00001353
Iteration 111/1000 | Loss: 0.00001353
Iteration 112/1000 | Loss: 0.00001353
Iteration 113/1000 | Loss: 0.00001353
Iteration 114/1000 | Loss: 0.00001353
Iteration 115/1000 | Loss: 0.00001353
Iteration 116/1000 | Loss: 0.00001353
Iteration 117/1000 | Loss: 0.00001353
Iteration 118/1000 | Loss: 0.00001352
Iteration 119/1000 | Loss: 0.00001352
Iteration 120/1000 | Loss: 0.00001352
Iteration 121/1000 | Loss: 0.00001352
Iteration 122/1000 | Loss: 0.00001352
Iteration 123/1000 | Loss: 0.00001352
Iteration 124/1000 | Loss: 0.00001352
Iteration 125/1000 | Loss: 0.00001352
Iteration 126/1000 | Loss: 0.00001352
Iteration 127/1000 | Loss: 0.00001352
Iteration 128/1000 | Loss: 0.00001352
Iteration 129/1000 | Loss: 0.00001351
Iteration 130/1000 | Loss: 0.00001351
Iteration 131/1000 | Loss: 0.00001351
Iteration 132/1000 | Loss: 0.00001351
Iteration 133/1000 | Loss: 0.00001351
Iteration 134/1000 | Loss: 0.00001351
Iteration 135/1000 | Loss: 0.00001351
Iteration 136/1000 | Loss: 0.00001351
Iteration 137/1000 | Loss: 0.00001351
Iteration 138/1000 | Loss: 0.00001351
Iteration 139/1000 | Loss: 0.00001351
Iteration 140/1000 | Loss: 0.00001351
Iteration 141/1000 | Loss: 0.00001351
Iteration 142/1000 | Loss: 0.00001351
Iteration 143/1000 | Loss: 0.00001351
Iteration 144/1000 | Loss: 0.00001350
Iteration 145/1000 | Loss: 0.00001350
Iteration 146/1000 | Loss: 0.00001350
Iteration 147/1000 | Loss: 0.00001350
Iteration 148/1000 | Loss: 0.00001350
Iteration 149/1000 | Loss: 0.00001350
Iteration 150/1000 | Loss: 0.00001350
Iteration 151/1000 | Loss: 0.00001350
Iteration 152/1000 | Loss: 0.00001350
Iteration 153/1000 | Loss: 0.00001350
Iteration 154/1000 | Loss: 0.00001350
Iteration 155/1000 | Loss: 0.00001350
Iteration 156/1000 | Loss: 0.00001350
Iteration 157/1000 | Loss: 0.00001350
Iteration 158/1000 | Loss: 0.00001350
Iteration 159/1000 | Loss: 0.00001350
Iteration 160/1000 | Loss: 0.00001350
Iteration 161/1000 | Loss: 0.00001350
Iteration 162/1000 | Loss: 0.00001350
Iteration 163/1000 | Loss: 0.00001350
Iteration 164/1000 | Loss: 0.00001350
Iteration 165/1000 | Loss: 0.00001350
Iteration 166/1000 | Loss: 0.00001350
Iteration 167/1000 | Loss: 0.00001350
Iteration 168/1000 | Loss: 0.00001350
Iteration 169/1000 | Loss: 0.00001350
Iteration 170/1000 | Loss: 0.00001350
Iteration 171/1000 | Loss: 0.00001350
Iteration 172/1000 | Loss: 0.00001350
Iteration 173/1000 | Loss: 0.00001350
Iteration 174/1000 | Loss: 0.00001350
Iteration 175/1000 | Loss: 0.00001350
Iteration 176/1000 | Loss: 0.00001350
Iteration 177/1000 | Loss: 0.00001350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.3498603038897272e-05, 1.3498603038897272e-05, 1.3498603038897272e-05, 1.3498603038897272e-05, 1.3498603038897272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3498603038897272e-05

Optimization complete. Final v2v error: 3.1465725898742676 mm

Highest mean error: 3.7468466758728027 mm for frame 92

Lowest mean error: 2.8531131744384766 mm for frame 207

Saving results

Total time: 46.060445070266724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00956628
Iteration 2/25 | Loss: 0.00181781
Iteration 3/25 | Loss: 0.00148400
Iteration 4/25 | Loss: 0.00146368
Iteration 5/25 | Loss: 0.00145706
Iteration 6/25 | Loss: 0.00145644
Iteration 7/25 | Loss: 0.00145644
Iteration 8/25 | Loss: 0.00145644
Iteration 9/25 | Loss: 0.00145644
Iteration 10/25 | Loss: 0.00145644
Iteration 11/25 | Loss: 0.00145644
Iteration 12/25 | Loss: 0.00145644
Iteration 13/25 | Loss: 0.00145644
Iteration 14/25 | Loss: 0.00145644
Iteration 15/25 | Loss: 0.00145644
Iteration 16/25 | Loss: 0.00145644
Iteration 17/25 | Loss: 0.00145644
Iteration 18/25 | Loss: 0.00145644
Iteration 19/25 | Loss: 0.00145644
Iteration 20/25 | Loss: 0.00145644
Iteration 21/25 | Loss: 0.00145644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014564378652721643, 0.0014564378652721643, 0.0014564378652721643, 0.0014564378652721643, 0.0014564378652721643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014564378652721643

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78118187
Iteration 2/25 | Loss: 0.00101377
Iteration 3/25 | Loss: 0.00101377
Iteration 4/25 | Loss: 0.00101376
Iteration 5/25 | Loss: 0.00101376
Iteration 6/25 | Loss: 0.00101376
Iteration 7/25 | Loss: 0.00101376
Iteration 8/25 | Loss: 0.00101376
Iteration 9/25 | Loss: 0.00101376
Iteration 10/25 | Loss: 0.00101376
Iteration 11/25 | Loss: 0.00101376
Iteration 12/25 | Loss: 0.00101376
Iteration 13/25 | Loss: 0.00101376
Iteration 14/25 | Loss: 0.00101376
Iteration 15/25 | Loss: 0.00101376
Iteration 16/25 | Loss: 0.00101376
Iteration 17/25 | Loss: 0.00101376
Iteration 18/25 | Loss: 0.00101376
Iteration 19/25 | Loss: 0.00101376
Iteration 20/25 | Loss: 0.00101376
Iteration 21/25 | Loss: 0.00101376
Iteration 22/25 | Loss: 0.00101376
Iteration 23/25 | Loss: 0.00101376
Iteration 24/25 | Loss: 0.00101376
Iteration 25/25 | Loss: 0.00101376

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101376
Iteration 2/1000 | Loss: 0.00005896
Iteration 3/1000 | Loss: 0.00004512
Iteration 4/1000 | Loss: 0.00003891
Iteration 5/1000 | Loss: 0.00003744
Iteration 6/1000 | Loss: 0.00003601
Iteration 7/1000 | Loss: 0.00003518
Iteration 8/1000 | Loss: 0.00003435
Iteration 9/1000 | Loss: 0.00003395
Iteration 10/1000 | Loss: 0.00003343
Iteration 11/1000 | Loss: 0.00003296
Iteration 12/1000 | Loss: 0.00003260
Iteration 13/1000 | Loss: 0.00003225
Iteration 14/1000 | Loss: 0.00003186
Iteration 15/1000 | Loss: 0.00003154
Iteration 16/1000 | Loss: 0.00003128
Iteration 17/1000 | Loss: 0.00003106
Iteration 18/1000 | Loss: 0.00003099
Iteration 19/1000 | Loss: 0.00003083
Iteration 20/1000 | Loss: 0.00003069
Iteration 21/1000 | Loss: 0.00003067
Iteration 22/1000 | Loss: 0.00003059
Iteration 23/1000 | Loss: 0.00003053
Iteration 24/1000 | Loss: 0.00003050
Iteration 25/1000 | Loss: 0.00003049
Iteration 26/1000 | Loss: 0.00003049
Iteration 27/1000 | Loss: 0.00003049
Iteration 28/1000 | Loss: 0.00003048
Iteration 29/1000 | Loss: 0.00003048
Iteration 30/1000 | Loss: 0.00003048
Iteration 31/1000 | Loss: 0.00003045
Iteration 32/1000 | Loss: 0.00003044
Iteration 33/1000 | Loss: 0.00003042
Iteration 34/1000 | Loss: 0.00003042
Iteration 35/1000 | Loss: 0.00003041
Iteration 36/1000 | Loss: 0.00003041
Iteration 37/1000 | Loss: 0.00003041
Iteration 38/1000 | Loss: 0.00003040
Iteration 39/1000 | Loss: 0.00003039
Iteration 40/1000 | Loss: 0.00003039
Iteration 41/1000 | Loss: 0.00003039
Iteration 42/1000 | Loss: 0.00003039
Iteration 43/1000 | Loss: 0.00003039
Iteration 44/1000 | Loss: 0.00003039
Iteration 45/1000 | Loss: 0.00003039
Iteration 46/1000 | Loss: 0.00003039
Iteration 47/1000 | Loss: 0.00003039
Iteration 48/1000 | Loss: 0.00003039
Iteration 49/1000 | Loss: 0.00003039
Iteration 50/1000 | Loss: 0.00003039
Iteration 51/1000 | Loss: 0.00003038
Iteration 52/1000 | Loss: 0.00003038
Iteration 53/1000 | Loss: 0.00003038
Iteration 54/1000 | Loss: 0.00003038
Iteration 55/1000 | Loss: 0.00003038
Iteration 56/1000 | Loss: 0.00003038
Iteration 57/1000 | Loss: 0.00003038
Iteration 58/1000 | Loss: 0.00003038
Iteration 59/1000 | Loss: 0.00003038
Iteration 60/1000 | Loss: 0.00003038
Iteration 61/1000 | Loss: 0.00003038
Iteration 62/1000 | Loss: 0.00003038
Iteration 63/1000 | Loss: 0.00003038
Iteration 64/1000 | Loss: 0.00003038
Iteration 65/1000 | Loss: 0.00003037
Iteration 66/1000 | Loss: 0.00003036
Iteration 67/1000 | Loss: 0.00003036
Iteration 68/1000 | Loss: 0.00003036
Iteration 69/1000 | Loss: 0.00003036
Iteration 70/1000 | Loss: 0.00003036
Iteration 71/1000 | Loss: 0.00003036
Iteration 72/1000 | Loss: 0.00003036
Iteration 73/1000 | Loss: 0.00003036
Iteration 74/1000 | Loss: 0.00003036
Iteration 75/1000 | Loss: 0.00003036
Iteration 76/1000 | Loss: 0.00003036
Iteration 77/1000 | Loss: 0.00003036
Iteration 78/1000 | Loss: 0.00003035
Iteration 79/1000 | Loss: 0.00003035
Iteration 80/1000 | Loss: 0.00003034
Iteration 81/1000 | Loss: 0.00003034
Iteration 82/1000 | Loss: 0.00003034
Iteration 83/1000 | Loss: 0.00003033
Iteration 84/1000 | Loss: 0.00003033
Iteration 85/1000 | Loss: 0.00003033
Iteration 86/1000 | Loss: 0.00003033
Iteration 87/1000 | Loss: 0.00003033
Iteration 88/1000 | Loss: 0.00003033
Iteration 89/1000 | Loss: 0.00003033
Iteration 90/1000 | Loss: 0.00003033
Iteration 91/1000 | Loss: 0.00003033
Iteration 92/1000 | Loss: 0.00003032
Iteration 93/1000 | Loss: 0.00003032
Iteration 94/1000 | Loss: 0.00003032
Iteration 95/1000 | Loss: 0.00003032
Iteration 96/1000 | Loss: 0.00003032
Iteration 97/1000 | Loss: 0.00003032
Iteration 98/1000 | Loss: 0.00003032
Iteration 99/1000 | Loss: 0.00003031
Iteration 100/1000 | Loss: 0.00003031
Iteration 101/1000 | Loss: 0.00003031
Iteration 102/1000 | Loss: 0.00003031
Iteration 103/1000 | Loss: 0.00003031
Iteration 104/1000 | Loss: 0.00003031
Iteration 105/1000 | Loss: 0.00003031
Iteration 106/1000 | Loss: 0.00003030
Iteration 107/1000 | Loss: 0.00003030
Iteration 108/1000 | Loss: 0.00003030
Iteration 109/1000 | Loss: 0.00003030
Iteration 110/1000 | Loss: 0.00003030
Iteration 111/1000 | Loss: 0.00003030
Iteration 112/1000 | Loss: 0.00003030
Iteration 113/1000 | Loss: 0.00003030
Iteration 114/1000 | Loss: 0.00003029
Iteration 115/1000 | Loss: 0.00003029
Iteration 116/1000 | Loss: 0.00003029
Iteration 117/1000 | Loss: 0.00003029
Iteration 118/1000 | Loss: 0.00003029
Iteration 119/1000 | Loss: 0.00003029
Iteration 120/1000 | Loss: 0.00003029
Iteration 121/1000 | Loss: 0.00003028
Iteration 122/1000 | Loss: 0.00003028
Iteration 123/1000 | Loss: 0.00003028
Iteration 124/1000 | Loss: 0.00003028
Iteration 125/1000 | Loss: 0.00003028
Iteration 126/1000 | Loss: 0.00003028
Iteration 127/1000 | Loss: 0.00003028
Iteration 128/1000 | Loss: 0.00003028
Iteration 129/1000 | Loss: 0.00003028
Iteration 130/1000 | Loss: 0.00003028
Iteration 131/1000 | Loss: 0.00003028
Iteration 132/1000 | Loss: 0.00003027
Iteration 133/1000 | Loss: 0.00003027
Iteration 134/1000 | Loss: 0.00003027
Iteration 135/1000 | Loss: 0.00003026
Iteration 136/1000 | Loss: 0.00003026
Iteration 137/1000 | Loss: 0.00003026
Iteration 138/1000 | Loss: 0.00003026
Iteration 139/1000 | Loss: 0.00003026
Iteration 140/1000 | Loss: 0.00003026
Iteration 141/1000 | Loss: 0.00003026
Iteration 142/1000 | Loss: 0.00003026
Iteration 143/1000 | Loss: 0.00003025
Iteration 144/1000 | Loss: 0.00003025
Iteration 145/1000 | Loss: 0.00003025
Iteration 146/1000 | Loss: 0.00003025
Iteration 147/1000 | Loss: 0.00003025
Iteration 148/1000 | Loss: 0.00003025
Iteration 149/1000 | Loss: 0.00003025
Iteration 150/1000 | Loss: 0.00003025
Iteration 151/1000 | Loss: 0.00003025
Iteration 152/1000 | Loss: 0.00003025
Iteration 153/1000 | Loss: 0.00003024
Iteration 154/1000 | Loss: 0.00003024
Iteration 155/1000 | Loss: 0.00003024
Iteration 156/1000 | Loss: 0.00003024
Iteration 157/1000 | Loss: 0.00003024
Iteration 158/1000 | Loss: 0.00003024
Iteration 159/1000 | Loss: 0.00003024
Iteration 160/1000 | Loss: 0.00003023
Iteration 161/1000 | Loss: 0.00003023
Iteration 162/1000 | Loss: 0.00003023
Iteration 163/1000 | Loss: 0.00003023
Iteration 164/1000 | Loss: 0.00003023
Iteration 165/1000 | Loss: 0.00003023
Iteration 166/1000 | Loss: 0.00003023
Iteration 167/1000 | Loss: 0.00003023
Iteration 168/1000 | Loss: 0.00003023
Iteration 169/1000 | Loss: 0.00003023
Iteration 170/1000 | Loss: 0.00003022
Iteration 171/1000 | Loss: 0.00003022
Iteration 172/1000 | Loss: 0.00003022
Iteration 173/1000 | Loss: 0.00003022
Iteration 174/1000 | Loss: 0.00003022
Iteration 175/1000 | Loss: 0.00003022
Iteration 176/1000 | Loss: 0.00003022
Iteration 177/1000 | Loss: 0.00003022
Iteration 178/1000 | Loss: 0.00003022
Iteration 179/1000 | Loss: 0.00003022
Iteration 180/1000 | Loss: 0.00003022
Iteration 181/1000 | Loss: 0.00003022
Iteration 182/1000 | Loss: 0.00003022
Iteration 183/1000 | Loss: 0.00003022
Iteration 184/1000 | Loss: 0.00003022
Iteration 185/1000 | Loss: 0.00003021
Iteration 186/1000 | Loss: 0.00003021
Iteration 187/1000 | Loss: 0.00003021
Iteration 188/1000 | Loss: 0.00003021
Iteration 189/1000 | Loss: 0.00003021
Iteration 190/1000 | Loss: 0.00003021
Iteration 191/1000 | Loss: 0.00003021
Iteration 192/1000 | Loss: 0.00003021
Iteration 193/1000 | Loss: 0.00003021
Iteration 194/1000 | Loss: 0.00003021
Iteration 195/1000 | Loss: 0.00003021
Iteration 196/1000 | Loss: 0.00003021
Iteration 197/1000 | Loss: 0.00003021
Iteration 198/1000 | Loss: 0.00003021
Iteration 199/1000 | Loss: 0.00003021
Iteration 200/1000 | Loss: 0.00003021
Iteration 201/1000 | Loss: 0.00003021
Iteration 202/1000 | Loss: 0.00003021
Iteration 203/1000 | Loss: 0.00003021
Iteration 204/1000 | Loss: 0.00003021
Iteration 205/1000 | Loss: 0.00003021
Iteration 206/1000 | Loss: 0.00003021
Iteration 207/1000 | Loss: 0.00003021
Iteration 208/1000 | Loss: 0.00003021
Iteration 209/1000 | Loss: 0.00003021
Iteration 210/1000 | Loss: 0.00003021
Iteration 211/1000 | Loss: 0.00003021
Iteration 212/1000 | Loss: 0.00003021
Iteration 213/1000 | Loss: 0.00003021
Iteration 214/1000 | Loss: 0.00003021
Iteration 215/1000 | Loss: 0.00003021
Iteration 216/1000 | Loss: 0.00003021
Iteration 217/1000 | Loss: 0.00003021
Iteration 218/1000 | Loss: 0.00003021
Iteration 219/1000 | Loss: 0.00003021
Iteration 220/1000 | Loss: 0.00003021
Iteration 221/1000 | Loss: 0.00003021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [3.0209248507162556e-05, 3.0209248507162556e-05, 3.0209248507162556e-05, 3.0209248507162556e-05, 3.0209248507162556e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0209248507162556e-05

Optimization complete. Final v2v error: 4.674529075622559 mm

Highest mean error: 5.196351528167725 mm for frame 192

Lowest mean error: 4.175038814544678 mm for frame 52

Saving results

Total time: 51.93651986122131
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458125
Iteration 2/25 | Loss: 0.00142019
Iteration 3/25 | Loss: 0.00133694
Iteration 4/25 | Loss: 0.00132506
Iteration 5/25 | Loss: 0.00132005
Iteration 6/25 | Loss: 0.00131995
Iteration 7/25 | Loss: 0.00131995
Iteration 8/25 | Loss: 0.00131995
Iteration 9/25 | Loss: 0.00131995
Iteration 10/25 | Loss: 0.00131995
Iteration 11/25 | Loss: 0.00131995
Iteration 12/25 | Loss: 0.00131995
Iteration 13/25 | Loss: 0.00131995
Iteration 14/25 | Loss: 0.00131995
Iteration 15/25 | Loss: 0.00131995
Iteration 16/25 | Loss: 0.00131995
Iteration 17/25 | Loss: 0.00131995
Iteration 18/25 | Loss: 0.00131995
Iteration 19/25 | Loss: 0.00131995
Iteration 20/25 | Loss: 0.00131995
Iteration 21/25 | Loss: 0.00131995
Iteration 22/25 | Loss: 0.00131995
Iteration 23/25 | Loss: 0.00131995
Iteration 24/25 | Loss: 0.00131995
Iteration 25/25 | Loss: 0.00131995

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40283740
Iteration 2/25 | Loss: 0.00108639
Iteration 3/25 | Loss: 0.00108639
Iteration 4/25 | Loss: 0.00108639
Iteration 5/25 | Loss: 0.00108639
Iteration 6/25 | Loss: 0.00108639
Iteration 7/25 | Loss: 0.00108639
Iteration 8/25 | Loss: 0.00108639
Iteration 9/25 | Loss: 0.00108639
Iteration 10/25 | Loss: 0.00108639
Iteration 11/25 | Loss: 0.00108639
Iteration 12/25 | Loss: 0.00108639
Iteration 13/25 | Loss: 0.00108639
Iteration 14/25 | Loss: 0.00108639
Iteration 15/25 | Loss: 0.00108639
Iteration 16/25 | Loss: 0.00108639
Iteration 17/25 | Loss: 0.00108639
Iteration 18/25 | Loss: 0.00108639
Iteration 19/25 | Loss: 0.00108639
Iteration 20/25 | Loss: 0.00108639
Iteration 21/25 | Loss: 0.00108639
Iteration 22/25 | Loss: 0.00108639
Iteration 23/25 | Loss: 0.00108639
Iteration 24/25 | Loss: 0.00108639
Iteration 25/25 | Loss: 0.00108639

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108639
Iteration 2/1000 | Loss: 0.00002534
Iteration 3/1000 | Loss: 0.00001931
Iteration 4/1000 | Loss: 0.00001771
Iteration 5/1000 | Loss: 0.00001701
Iteration 6/1000 | Loss: 0.00001652
Iteration 7/1000 | Loss: 0.00001628
Iteration 8/1000 | Loss: 0.00001625
Iteration 9/1000 | Loss: 0.00001596
Iteration 10/1000 | Loss: 0.00001563
Iteration 11/1000 | Loss: 0.00001553
Iteration 12/1000 | Loss: 0.00001539
Iteration 13/1000 | Loss: 0.00001538
Iteration 14/1000 | Loss: 0.00001538
Iteration 15/1000 | Loss: 0.00001537
Iteration 16/1000 | Loss: 0.00001533
Iteration 17/1000 | Loss: 0.00001530
Iteration 18/1000 | Loss: 0.00001529
Iteration 19/1000 | Loss: 0.00001528
Iteration 20/1000 | Loss: 0.00001527
Iteration 21/1000 | Loss: 0.00001527
Iteration 22/1000 | Loss: 0.00001526
Iteration 23/1000 | Loss: 0.00001526
Iteration 24/1000 | Loss: 0.00001525
Iteration 25/1000 | Loss: 0.00001525
Iteration 26/1000 | Loss: 0.00001523
Iteration 27/1000 | Loss: 0.00001523
Iteration 28/1000 | Loss: 0.00001522
Iteration 29/1000 | Loss: 0.00001521
Iteration 30/1000 | Loss: 0.00001521
Iteration 31/1000 | Loss: 0.00001521
Iteration 32/1000 | Loss: 0.00001520
Iteration 33/1000 | Loss: 0.00001520
Iteration 34/1000 | Loss: 0.00001519
Iteration 35/1000 | Loss: 0.00001519
Iteration 36/1000 | Loss: 0.00001519
Iteration 37/1000 | Loss: 0.00001519
Iteration 38/1000 | Loss: 0.00001518
Iteration 39/1000 | Loss: 0.00001518
Iteration 40/1000 | Loss: 0.00001518
Iteration 41/1000 | Loss: 0.00001518
Iteration 42/1000 | Loss: 0.00001517
Iteration 43/1000 | Loss: 0.00001517
Iteration 44/1000 | Loss: 0.00001517
Iteration 45/1000 | Loss: 0.00001517
Iteration 46/1000 | Loss: 0.00001516
Iteration 47/1000 | Loss: 0.00001514
Iteration 48/1000 | Loss: 0.00001513
Iteration 49/1000 | Loss: 0.00001512
Iteration 50/1000 | Loss: 0.00001511
Iteration 51/1000 | Loss: 0.00001509
Iteration 52/1000 | Loss: 0.00001508
Iteration 53/1000 | Loss: 0.00001508
Iteration 54/1000 | Loss: 0.00001506
Iteration 55/1000 | Loss: 0.00001506
Iteration 56/1000 | Loss: 0.00001505
Iteration 57/1000 | Loss: 0.00001505
Iteration 58/1000 | Loss: 0.00001505
Iteration 59/1000 | Loss: 0.00001504
Iteration 60/1000 | Loss: 0.00001504
Iteration 61/1000 | Loss: 0.00001503
Iteration 62/1000 | Loss: 0.00001499
Iteration 63/1000 | Loss: 0.00001498
Iteration 64/1000 | Loss: 0.00001498
Iteration 65/1000 | Loss: 0.00001497
Iteration 66/1000 | Loss: 0.00001496
Iteration 67/1000 | Loss: 0.00001496
Iteration 68/1000 | Loss: 0.00001495
Iteration 69/1000 | Loss: 0.00001495
Iteration 70/1000 | Loss: 0.00001495
Iteration 71/1000 | Loss: 0.00001494
Iteration 72/1000 | Loss: 0.00001493
Iteration 73/1000 | Loss: 0.00001492
Iteration 74/1000 | Loss: 0.00001492
Iteration 75/1000 | Loss: 0.00001491
Iteration 76/1000 | Loss: 0.00001491
Iteration 77/1000 | Loss: 0.00001490
Iteration 78/1000 | Loss: 0.00001490
Iteration 79/1000 | Loss: 0.00001489
Iteration 80/1000 | Loss: 0.00001489
Iteration 81/1000 | Loss: 0.00001489
Iteration 82/1000 | Loss: 0.00001488
Iteration 83/1000 | Loss: 0.00001488
Iteration 84/1000 | Loss: 0.00001488
Iteration 85/1000 | Loss: 0.00001488
Iteration 86/1000 | Loss: 0.00001488
Iteration 87/1000 | Loss: 0.00001487
Iteration 88/1000 | Loss: 0.00001487
Iteration 89/1000 | Loss: 0.00001487
Iteration 90/1000 | Loss: 0.00001487
Iteration 91/1000 | Loss: 0.00001487
Iteration 92/1000 | Loss: 0.00001487
Iteration 93/1000 | Loss: 0.00001487
Iteration 94/1000 | Loss: 0.00001486
Iteration 95/1000 | Loss: 0.00001486
Iteration 96/1000 | Loss: 0.00001486
Iteration 97/1000 | Loss: 0.00001486
Iteration 98/1000 | Loss: 0.00001486
Iteration 99/1000 | Loss: 0.00001486
Iteration 100/1000 | Loss: 0.00001486
Iteration 101/1000 | Loss: 0.00001486
Iteration 102/1000 | Loss: 0.00001486
Iteration 103/1000 | Loss: 0.00001486
Iteration 104/1000 | Loss: 0.00001486
Iteration 105/1000 | Loss: 0.00001486
Iteration 106/1000 | Loss: 0.00001486
Iteration 107/1000 | Loss: 0.00001486
Iteration 108/1000 | Loss: 0.00001486
Iteration 109/1000 | Loss: 0.00001486
Iteration 110/1000 | Loss: 0.00001486
Iteration 111/1000 | Loss: 0.00001486
Iteration 112/1000 | Loss: 0.00001486
Iteration 113/1000 | Loss: 0.00001486
Iteration 114/1000 | Loss: 0.00001486
Iteration 115/1000 | Loss: 0.00001486
Iteration 116/1000 | Loss: 0.00001486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.485704615333816e-05, 1.485704615333816e-05, 1.485704615333816e-05, 1.485704615333816e-05, 1.485704615333816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.485704615333816e-05

Optimization complete. Final v2v error: 3.2274346351623535 mm

Highest mean error: 3.6890294551849365 mm for frame 43

Lowest mean error: 2.9474949836730957 mm for frame 208

Saving results

Total time: 37.83694887161255
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463072
Iteration 2/25 | Loss: 0.00146037
Iteration 3/25 | Loss: 0.00132780
Iteration 4/25 | Loss: 0.00131560
Iteration 5/25 | Loss: 0.00131177
Iteration 6/25 | Loss: 0.00131170
Iteration 7/25 | Loss: 0.00131170
Iteration 8/25 | Loss: 0.00131170
Iteration 9/25 | Loss: 0.00131170
Iteration 10/25 | Loss: 0.00131170
Iteration 11/25 | Loss: 0.00131170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013116974150761962, 0.0013116974150761962, 0.0013116974150761962, 0.0013116974150761962, 0.0013116974150761962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013116974150761962

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39820349
Iteration 2/25 | Loss: 0.00103664
Iteration 3/25 | Loss: 0.00103664
Iteration 4/25 | Loss: 0.00103664
Iteration 5/25 | Loss: 0.00103664
Iteration 6/25 | Loss: 0.00103664
Iteration 7/25 | Loss: 0.00103664
Iteration 8/25 | Loss: 0.00103664
Iteration 9/25 | Loss: 0.00103664
Iteration 10/25 | Loss: 0.00103664
Iteration 11/25 | Loss: 0.00103664
Iteration 12/25 | Loss: 0.00103664
Iteration 13/25 | Loss: 0.00103664
Iteration 14/25 | Loss: 0.00103664
Iteration 15/25 | Loss: 0.00103664
Iteration 16/25 | Loss: 0.00103664
Iteration 17/25 | Loss: 0.00103664
Iteration 18/25 | Loss: 0.00103664
Iteration 19/25 | Loss: 0.00103664
Iteration 20/25 | Loss: 0.00103664
Iteration 21/25 | Loss: 0.00103664
Iteration 22/25 | Loss: 0.00103664
Iteration 23/25 | Loss: 0.00103664
Iteration 24/25 | Loss: 0.00103664
Iteration 25/25 | Loss: 0.00103664

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103664
Iteration 2/1000 | Loss: 0.00004254
Iteration 3/1000 | Loss: 0.00002761
Iteration 4/1000 | Loss: 0.00002400
Iteration 5/1000 | Loss: 0.00002250
Iteration 6/1000 | Loss: 0.00002133
Iteration 7/1000 | Loss: 0.00002064
Iteration 8/1000 | Loss: 0.00002024
Iteration 9/1000 | Loss: 0.00001990
Iteration 10/1000 | Loss: 0.00001972
Iteration 11/1000 | Loss: 0.00001968
Iteration 12/1000 | Loss: 0.00001966
Iteration 13/1000 | Loss: 0.00001951
Iteration 14/1000 | Loss: 0.00001935
Iteration 15/1000 | Loss: 0.00001933
Iteration 16/1000 | Loss: 0.00001929
Iteration 17/1000 | Loss: 0.00001929
Iteration 18/1000 | Loss: 0.00001921
Iteration 19/1000 | Loss: 0.00001911
Iteration 20/1000 | Loss: 0.00001904
Iteration 21/1000 | Loss: 0.00001903
Iteration 22/1000 | Loss: 0.00001898
Iteration 23/1000 | Loss: 0.00001898
Iteration 24/1000 | Loss: 0.00001896
Iteration 25/1000 | Loss: 0.00001896
Iteration 26/1000 | Loss: 0.00001895
Iteration 27/1000 | Loss: 0.00001894
Iteration 28/1000 | Loss: 0.00001892
Iteration 29/1000 | Loss: 0.00001892
Iteration 30/1000 | Loss: 0.00001891
Iteration 31/1000 | Loss: 0.00001883
Iteration 32/1000 | Loss: 0.00001883
Iteration 33/1000 | Loss: 0.00001877
Iteration 34/1000 | Loss: 0.00001874
Iteration 35/1000 | Loss: 0.00001873
Iteration 36/1000 | Loss: 0.00001873
Iteration 37/1000 | Loss: 0.00001872
Iteration 38/1000 | Loss: 0.00001872
Iteration 39/1000 | Loss: 0.00001871
Iteration 40/1000 | Loss: 0.00001871
Iteration 41/1000 | Loss: 0.00001870
Iteration 42/1000 | Loss: 0.00001870
Iteration 43/1000 | Loss: 0.00001870
Iteration 44/1000 | Loss: 0.00001869
Iteration 45/1000 | Loss: 0.00001869
Iteration 46/1000 | Loss: 0.00001869
Iteration 47/1000 | Loss: 0.00001869
Iteration 48/1000 | Loss: 0.00001867
Iteration 49/1000 | Loss: 0.00001867
Iteration 50/1000 | Loss: 0.00001866
Iteration 51/1000 | Loss: 0.00001865
Iteration 52/1000 | Loss: 0.00001865
Iteration 53/1000 | Loss: 0.00001865
Iteration 54/1000 | Loss: 0.00001864
Iteration 55/1000 | Loss: 0.00001862
Iteration 56/1000 | Loss: 0.00001862
Iteration 57/1000 | Loss: 0.00001861
Iteration 58/1000 | Loss: 0.00001860
Iteration 59/1000 | Loss: 0.00001860
Iteration 60/1000 | Loss: 0.00001860
Iteration 61/1000 | Loss: 0.00001860
Iteration 62/1000 | Loss: 0.00001859
Iteration 63/1000 | Loss: 0.00001859
Iteration 64/1000 | Loss: 0.00001859
Iteration 65/1000 | Loss: 0.00001859
Iteration 66/1000 | Loss: 0.00001859
Iteration 67/1000 | Loss: 0.00001858
Iteration 68/1000 | Loss: 0.00001858
Iteration 69/1000 | Loss: 0.00001858
Iteration 70/1000 | Loss: 0.00001857
Iteration 71/1000 | Loss: 0.00001857
Iteration 72/1000 | Loss: 0.00001857
Iteration 73/1000 | Loss: 0.00001857
Iteration 74/1000 | Loss: 0.00001857
Iteration 75/1000 | Loss: 0.00001856
Iteration 76/1000 | Loss: 0.00001856
Iteration 77/1000 | Loss: 0.00001856
Iteration 78/1000 | Loss: 0.00001856
Iteration 79/1000 | Loss: 0.00001856
Iteration 80/1000 | Loss: 0.00001856
Iteration 81/1000 | Loss: 0.00001856
Iteration 82/1000 | Loss: 0.00001856
Iteration 83/1000 | Loss: 0.00001856
Iteration 84/1000 | Loss: 0.00001855
Iteration 85/1000 | Loss: 0.00001855
Iteration 86/1000 | Loss: 0.00001855
Iteration 87/1000 | Loss: 0.00001855
Iteration 88/1000 | Loss: 0.00001855
Iteration 89/1000 | Loss: 0.00001855
Iteration 90/1000 | Loss: 0.00001855
Iteration 91/1000 | Loss: 0.00001855
Iteration 92/1000 | Loss: 0.00001855
Iteration 93/1000 | Loss: 0.00001855
Iteration 94/1000 | Loss: 0.00001855
Iteration 95/1000 | Loss: 0.00001855
Iteration 96/1000 | Loss: 0.00001854
Iteration 97/1000 | Loss: 0.00001854
Iteration 98/1000 | Loss: 0.00001854
Iteration 99/1000 | Loss: 0.00001854
Iteration 100/1000 | Loss: 0.00001854
Iteration 101/1000 | Loss: 0.00001854
Iteration 102/1000 | Loss: 0.00001854
Iteration 103/1000 | Loss: 0.00001854
Iteration 104/1000 | Loss: 0.00001854
Iteration 105/1000 | Loss: 0.00001854
Iteration 106/1000 | Loss: 0.00001854
Iteration 107/1000 | Loss: 0.00001854
Iteration 108/1000 | Loss: 0.00001853
Iteration 109/1000 | Loss: 0.00001853
Iteration 110/1000 | Loss: 0.00001853
Iteration 111/1000 | Loss: 0.00001853
Iteration 112/1000 | Loss: 0.00001853
Iteration 113/1000 | Loss: 0.00001853
Iteration 114/1000 | Loss: 0.00001853
Iteration 115/1000 | Loss: 0.00001853
Iteration 116/1000 | Loss: 0.00001853
Iteration 117/1000 | Loss: 0.00001853
Iteration 118/1000 | Loss: 0.00001853
Iteration 119/1000 | Loss: 0.00001853
Iteration 120/1000 | Loss: 0.00001853
Iteration 121/1000 | Loss: 0.00001853
Iteration 122/1000 | Loss: 0.00001853
Iteration 123/1000 | Loss: 0.00001852
Iteration 124/1000 | Loss: 0.00001852
Iteration 125/1000 | Loss: 0.00001852
Iteration 126/1000 | Loss: 0.00001852
Iteration 127/1000 | Loss: 0.00001852
Iteration 128/1000 | Loss: 0.00001852
Iteration 129/1000 | Loss: 0.00001852
Iteration 130/1000 | Loss: 0.00001852
Iteration 131/1000 | Loss: 0.00001852
Iteration 132/1000 | Loss: 0.00001852
Iteration 133/1000 | Loss: 0.00001851
Iteration 134/1000 | Loss: 0.00001851
Iteration 135/1000 | Loss: 0.00001851
Iteration 136/1000 | Loss: 0.00001851
Iteration 137/1000 | Loss: 0.00001851
Iteration 138/1000 | Loss: 0.00001851
Iteration 139/1000 | Loss: 0.00001851
Iteration 140/1000 | Loss: 0.00001851
Iteration 141/1000 | Loss: 0.00001851
Iteration 142/1000 | Loss: 0.00001851
Iteration 143/1000 | Loss: 0.00001851
Iteration 144/1000 | Loss: 0.00001851
Iteration 145/1000 | Loss: 0.00001851
Iteration 146/1000 | Loss: 0.00001851
Iteration 147/1000 | Loss: 0.00001851
Iteration 148/1000 | Loss: 0.00001850
Iteration 149/1000 | Loss: 0.00001850
Iteration 150/1000 | Loss: 0.00001850
Iteration 151/1000 | Loss: 0.00001850
Iteration 152/1000 | Loss: 0.00001850
Iteration 153/1000 | Loss: 0.00001850
Iteration 154/1000 | Loss: 0.00001850
Iteration 155/1000 | Loss: 0.00001850
Iteration 156/1000 | Loss: 0.00001850
Iteration 157/1000 | Loss: 0.00001849
Iteration 158/1000 | Loss: 0.00001849
Iteration 159/1000 | Loss: 0.00001849
Iteration 160/1000 | Loss: 0.00001849
Iteration 161/1000 | Loss: 0.00001849
Iteration 162/1000 | Loss: 0.00001849
Iteration 163/1000 | Loss: 0.00001849
Iteration 164/1000 | Loss: 0.00001849
Iteration 165/1000 | Loss: 0.00001849
Iteration 166/1000 | Loss: 0.00001849
Iteration 167/1000 | Loss: 0.00001849
Iteration 168/1000 | Loss: 0.00001849
Iteration 169/1000 | Loss: 0.00001849
Iteration 170/1000 | Loss: 0.00001849
Iteration 171/1000 | Loss: 0.00001849
Iteration 172/1000 | Loss: 0.00001848
Iteration 173/1000 | Loss: 0.00001848
Iteration 174/1000 | Loss: 0.00001848
Iteration 175/1000 | Loss: 0.00001848
Iteration 176/1000 | Loss: 0.00001848
Iteration 177/1000 | Loss: 0.00001848
Iteration 178/1000 | Loss: 0.00001848
Iteration 179/1000 | Loss: 0.00001848
Iteration 180/1000 | Loss: 0.00001848
Iteration 181/1000 | Loss: 0.00001848
Iteration 182/1000 | Loss: 0.00001848
Iteration 183/1000 | Loss: 0.00001848
Iteration 184/1000 | Loss: 0.00001848
Iteration 185/1000 | Loss: 0.00001848
Iteration 186/1000 | Loss: 0.00001847
Iteration 187/1000 | Loss: 0.00001847
Iteration 188/1000 | Loss: 0.00001847
Iteration 189/1000 | Loss: 0.00001847
Iteration 190/1000 | Loss: 0.00001847
Iteration 191/1000 | Loss: 0.00001847
Iteration 192/1000 | Loss: 0.00001847
Iteration 193/1000 | Loss: 0.00001847
Iteration 194/1000 | Loss: 0.00001847
Iteration 195/1000 | Loss: 0.00001847
Iteration 196/1000 | Loss: 0.00001847
Iteration 197/1000 | Loss: 0.00001847
Iteration 198/1000 | Loss: 0.00001847
Iteration 199/1000 | Loss: 0.00001847
Iteration 200/1000 | Loss: 0.00001847
Iteration 201/1000 | Loss: 0.00001847
Iteration 202/1000 | Loss: 0.00001847
Iteration 203/1000 | Loss: 0.00001847
Iteration 204/1000 | Loss: 0.00001847
Iteration 205/1000 | Loss: 0.00001847
Iteration 206/1000 | Loss: 0.00001847
Iteration 207/1000 | Loss: 0.00001847
Iteration 208/1000 | Loss: 0.00001847
Iteration 209/1000 | Loss: 0.00001847
Iteration 210/1000 | Loss: 0.00001847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.8472343072062358e-05, 1.8472343072062358e-05, 1.8472343072062358e-05, 1.8472343072062358e-05, 1.8472343072062358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8472343072062358e-05

Optimization complete. Final v2v error: 3.597987413406372 mm

Highest mean error: 4.46134090423584 mm for frame 115

Lowest mean error: 2.8468077182769775 mm for frame 201

Saving results

Total time: 49.016878604888916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466204
Iteration 2/25 | Loss: 0.00145676
Iteration 3/25 | Loss: 0.00135247
Iteration 4/25 | Loss: 0.00134148
Iteration 5/25 | Loss: 0.00133845
Iteration 6/25 | Loss: 0.00133804
Iteration 7/25 | Loss: 0.00133804
Iteration 8/25 | Loss: 0.00133804
Iteration 9/25 | Loss: 0.00133804
Iteration 10/25 | Loss: 0.00133804
Iteration 11/25 | Loss: 0.00133804
Iteration 12/25 | Loss: 0.00133804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013380382442846894, 0.0013380382442846894, 0.0013380382442846894, 0.0013380382442846894, 0.0013380382442846894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013380382442846894

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48234224
Iteration 2/25 | Loss: 0.00078050
Iteration 3/25 | Loss: 0.00078050
Iteration 4/25 | Loss: 0.00078050
Iteration 5/25 | Loss: 0.00078050
Iteration 6/25 | Loss: 0.00078050
Iteration 7/25 | Loss: 0.00078050
Iteration 8/25 | Loss: 0.00078050
Iteration 9/25 | Loss: 0.00078050
Iteration 10/25 | Loss: 0.00078050
Iteration 11/25 | Loss: 0.00078050
Iteration 12/25 | Loss: 0.00078050
Iteration 13/25 | Loss: 0.00078050
Iteration 14/25 | Loss: 0.00078050
Iteration 15/25 | Loss: 0.00078050
Iteration 16/25 | Loss: 0.00078050
Iteration 17/25 | Loss: 0.00078050
Iteration 18/25 | Loss: 0.00078050
Iteration 19/25 | Loss: 0.00078050
Iteration 20/25 | Loss: 0.00078050
Iteration 21/25 | Loss: 0.00078050
Iteration 22/25 | Loss: 0.00078050
Iteration 23/25 | Loss: 0.00078050
Iteration 24/25 | Loss: 0.00078050
Iteration 25/25 | Loss: 0.00078050

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078050
Iteration 2/1000 | Loss: 0.00004117
Iteration 3/1000 | Loss: 0.00002698
Iteration 4/1000 | Loss: 0.00002228
Iteration 5/1000 | Loss: 0.00002091
Iteration 6/1000 | Loss: 0.00002010
Iteration 7/1000 | Loss: 0.00001945
Iteration 8/1000 | Loss: 0.00001889
Iteration 9/1000 | Loss: 0.00001850
Iteration 10/1000 | Loss: 0.00001824
Iteration 11/1000 | Loss: 0.00001799
Iteration 12/1000 | Loss: 0.00001785
Iteration 13/1000 | Loss: 0.00001784
Iteration 14/1000 | Loss: 0.00001768
Iteration 15/1000 | Loss: 0.00001765
Iteration 16/1000 | Loss: 0.00001753
Iteration 17/1000 | Loss: 0.00001742
Iteration 18/1000 | Loss: 0.00001740
Iteration 19/1000 | Loss: 0.00001735
Iteration 20/1000 | Loss: 0.00001732
Iteration 21/1000 | Loss: 0.00001731
Iteration 22/1000 | Loss: 0.00001729
Iteration 23/1000 | Loss: 0.00001725
Iteration 24/1000 | Loss: 0.00001725
Iteration 25/1000 | Loss: 0.00001724
Iteration 26/1000 | Loss: 0.00001720
Iteration 27/1000 | Loss: 0.00001720
Iteration 28/1000 | Loss: 0.00001718
Iteration 29/1000 | Loss: 0.00001717
Iteration 30/1000 | Loss: 0.00001717
Iteration 31/1000 | Loss: 0.00001716
Iteration 32/1000 | Loss: 0.00001716
Iteration 33/1000 | Loss: 0.00001716
Iteration 34/1000 | Loss: 0.00001716
Iteration 35/1000 | Loss: 0.00001716
Iteration 36/1000 | Loss: 0.00001716
Iteration 37/1000 | Loss: 0.00001716
Iteration 38/1000 | Loss: 0.00001716
Iteration 39/1000 | Loss: 0.00001716
Iteration 40/1000 | Loss: 0.00001716
Iteration 41/1000 | Loss: 0.00001715
Iteration 42/1000 | Loss: 0.00001715
Iteration 43/1000 | Loss: 0.00001715
Iteration 44/1000 | Loss: 0.00001715
Iteration 45/1000 | Loss: 0.00001710
Iteration 46/1000 | Loss: 0.00001710
Iteration 47/1000 | Loss: 0.00001710
Iteration 48/1000 | Loss: 0.00001709
Iteration 49/1000 | Loss: 0.00001709
Iteration 50/1000 | Loss: 0.00001709
Iteration 51/1000 | Loss: 0.00001708
Iteration 52/1000 | Loss: 0.00001708
Iteration 53/1000 | Loss: 0.00001707
Iteration 54/1000 | Loss: 0.00001707
Iteration 55/1000 | Loss: 0.00001704
Iteration 56/1000 | Loss: 0.00001703
Iteration 57/1000 | Loss: 0.00001702
Iteration 58/1000 | Loss: 0.00001701
Iteration 59/1000 | Loss: 0.00001699
Iteration 60/1000 | Loss: 0.00001698
Iteration 61/1000 | Loss: 0.00001698
Iteration 62/1000 | Loss: 0.00001698
Iteration 63/1000 | Loss: 0.00001697
Iteration 64/1000 | Loss: 0.00001697
Iteration 65/1000 | Loss: 0.00001697
Iteration 66/1000 | Loss: 0.00001696
Iteration 67/1000 | Loss: 0.00001696
Iteration 68/1000 | Loss: 0.00001695
Iteration 69/1000 | Loss: 0.00001695
Iteration 70/1000 | Loss: 0.00001695
Iteration 71/1000 | Loss: 0.00001694
Iteration 72/1000 | Loss: 0.00001694
Iteration 73/1000 | Loss: 0.00001694
Iteration 74/1000 | Loss: 0.00001693
Iteration 75/1000 | Loss: 0.00001693
Iteration 76/1000 | Loss: 0.00001692
Iteration 77/1000 | Loss: 0.00001692
Iteration 78/1000 | Loss: 0.00001691
Iteration 79/1000 | Loss: 0.00001691
Iteration 80/1000 | Loss: 0.00001691
Iteration 81/1000 | Loss: 0.00001691
Iteration 82/1000 | Loss: 0.00001690
Iteration 83/1000 | Loss: 0.00001690
Iteration 84/1000 | Loss: 0.00001690
Iteration 85/1000 | Loss: 0.00001690
Iteration 86/1000 | Loss: 0.00001689
Iteration 87/1000 | Loss: 0.00001689
Iteration 88/1000 | Loss: 0.00001689
Iteration 89/1000 | Loss: 0.00001689
Iteration 90/1000 | Loss: 0.00001688
Iteration 91/1000 | Loss: 0.00001688
Iteration 92/1000 | Loss: 0.00001688
Iteration 93/1000 | Loss: 0.00001688
Iteration 94/1000 | Loss: 0.00001687
Iteration 95/1000 | Loss: 0.00001687
Iteration 96/1000 | Loss: 0.00001687
Iteration 97/1000 | Loss: 0.00001687
Iteration 98/1000 | Loss: 0.00001687
Iteration 99/1000 | Loss: 0.00001686
Iteration 100/1000 | Loss: 0.00001686
Iteration 101/1000 | Loss: 0.00001686
Iteration 102/1000 | Loss: 0.00001686
Iteration 103/1000 | Loss: 0.00001686
Iteration 104/1000 | Loss: 0.00001686
Iteration 105/1000 | Loss: 0.00001686
Iteration 106/1000 | Loss: 0.00001686
Iteration 107/1000 | Loss: 0.00001685
Iteration 108/1000 | Loss: 0.00001685
Iteration 109/1000 | Loss: 0.00001685
Iteration 110/1000 | Loss: 0.00001685
Iteration 111/1000 | Loss: 0.00001685
Iteration 112/1000 | Loss: 0.00001685
Iteration 113/1000 | Loss: 0.00001685
Iteration 114/1000 | Loss: 0.00001685
Iteration 115/1000 | Loss: 0.00001684
Iteration 116/1000 | Loss: 0.00001684
Iteration 117/1000 | Loss: 0.00001684
Iteration 118/1000 | Loss: 0.00001684
Iteration 119/1000 | Loss: 0.00001684
Iteration 120/1000 | Loss: 0.00001684
Iteration 121/1000 | Loss: 0.00001684
Iteration 122/1000 | Loss: 0.00001684
Iteration 123/1000 | Loss: 0.00001684
Iteration 124/1000 | Loss: 0.00001684
Iteration 125/1000 | Loss: 0.00001684
Iteration 126/1000 | Loss: 0.00001684
Iteration 127/1000 | Loss: 0.00001683
Iteration 128/1000 | Loss: 0.00001683
Iteration 129/1000 | Loss: 0.00001683
Iteration 130/1000 | Loss: 0.00001683
Iteration 131/1000 | Loss: 0.00001683
Iteration 132/1000 | Loss: 0.00001683
Iteration 133/1000 | Loss: 0.00001683
Iteration 134/1000 | Loss: 0.00001683
Iteration 135/1000 | Loss: 0.00001683
Iteration 136/1000 | Loss: 0.00001683
Iteration 137/1000 | Loss: 0.00001683
Iteration 138/1000 | Loss: 0.00001682
Iteration 139/1000 | Loss: 0.00001682
Iteration 140/1000 | Loss: 0.00001682
Iteration 141/1000 | Loss: 0.00001682
Iteration 142/1000 | Loss: 0.00001682
Iteration 143/1000 | Loss: 0.00001682
Iteration 144/1000 | Loss: 0.00001682
Iteration 145/1000 | Loss: 0.00001682
Iteration 146/1000 | Loss: 0.00001682
Iteration 147/1000 | Loss: 0.00001682
Iteration 148/1000 | Loss: 0.00001682
Iteration 149/1000 | Loss: 0.00001682
Iteration 150/1000 | Loss: 0.00001682
Iteration 151/1000 | Loss: 0.00001682
Iteration 152/1000 | Loss: 0.00001682
Iteration 153/1000 | Loss: 0.00001682
Iteration 154/1000 | Loss: 0.00001682
Iteration 155/1000 | Loss: 0.00001682
Iteration 156/1000 | Loss: 0.00001682
Iteration 157/1000 | Loss: 0.00001682
Iteration 158/1000 | Loss: 0.00001682
Iteration 159/1000 | Loss: 0.00001682
Iteration 160/1000 | Loss: 0.00001682
Iteration 161/1000 | Loss: 0.00001682
Iteration 162/1000 | Loss: 0.00001682
Iteration 163/1000 | Loss: 0.00001682
Iteration 164/1000 | Loss: 0.00001682
Iteration 165/1000 | Loss: 0.00001682
Iteration 166/1000 | Loss: 0.00001682
Iteration 167/1000 | Loss: 0.00001682
Iteration 168/1000 | Loss: 0.00001682
Iteration 169/1000 | Loss: 0.00001682
Iteration 170/1000 | Loss: 0.00001682
Iteration 171/1000 | Loss: 0.00001682
Iteration 172/1000 | Loss: 0.00001682
Iteration 173/1000 | Loss: 0.00001682
Iteration 174/1000 | Loss: 0.00001682
Iteration 175/1000 | Loss: 0.00001682
Iteration 176/1000 | Loss: 0.00001682
Iteration 177/1000 | Loss: 0.00001682
Iteration 178/1000 | Loss: 0.00001682
Iteration 179/1000 | Loss: 0.00001682
Iteration 180/1000 | Loss: 0.00001682
Iteration 181/1000 | Loss: 0.00001682
Iteration 182/1000 | Loss: 0.00001682
Iteration 183/1000 | Loss: 0.00001682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.6815212802612223e-05, 1.6815212802612223e-05, 1.6815212802612223e-05, 1.6815212802612223e-05, 1.6815212802612223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6815212802612223e-05

Optimization complete. Final v2v error: 3.4369349479675293 mm

Highest mean error: 4.82142972946167 mm for frame 67

Lowest mean error: 2.9545538425445557 mm for frame 29

Saving results

Total time: 40.739649057388306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946760
Iteration 2/25 | Loss: 0.00227968
Iteration 3/25 | Loss: 0.00154691
Iteration 4/25 | Loss: 0.00146665
Iteration 5/25 | Loss: 0.00146876
Iteration 6/25 | Loss: 0.00144651
Iteration 7/25 | Loss: 0.00140201
Iteration 8/25 | Loss: 0.00137780
Iteration 9/25 | Loss: 0.00137336
Iteration 10/25 | Loss: 0.00137266
Iteration 11/25 | Loss: 0.00137104
Iteration 12/25 | Loss: 0.00136712
Iteration 13/25 | Loss: 0.00136578
Iteration 14/25 | Loss: 0.00136707
Iteration 15/25 | Loss: 0.00136648
Iteration 16/25 | Loss: 0.00136653
Iteration 17/25 | Loss: 0.00136660
Iteration 18/25 | Loss: 0.00136659
Iteration 19/25 | Loss: 0.00136587
Iteration 20/25 | Loss: 0.00136488
Iteration 21/25 | Loss: 0.00136626
Iteration 22/25 | Loss: 0.00136592
Iteration 23/25 | Loss: 0.00136481
Iteration 24/25 | Loss: 0.00136297
Iteration 25/25 | Loss: 0.00136572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39052641
Iteration 2/25 | Loss: 0.00080631
Iteration 3/25 | Loss: 0.00080631
Iteration 4/25 | Loss: 0.00080631
Iteration 5/25 | Loss: 0.00080631
Iteration 6/25 | Loss: 0.00080631
Iteration 7/25 | Loss: 0.00080631
Iteration 8/25 | Loss: 0.00080631
Iteration 9/25 | Loss: 0.00080631
Iteration 10/25 | Loss: 0.00080631
Iteration 11/25 | Loss: 0.00080631
Iteration 12/25 | Loss: 0.00080631
Iteration 13/25 | Loss: 0.00080631
Iteration 14/25 | Loss: 0.00080631
Iteration 15/25 | Loss: 0.00080631
Iteration 16/25 | Loss: 0.00080631
Iteration 17/25 | Loss: 0.00080631
Iteration 18/25 | Loss: 0.00080631
Iteration 19/25 | Loss: 0.00080631
Iteration 20/25 | Loss: 0.00080631
Iteration 21/25 | Loss: 0.00080631
Iteration 22/25 | Loss: 0.00080631
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008063056156970561, 0.0008063056156970561, 0.0008063056156970561, 0.0008063056156970561, 0.0008063056156970561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008063056156970561

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080631
Iteration 2/1000 | Loss: 0.00004670
Iteration 3/1000 | Loss: 0.00002924
Iteration 4/1000 | Loss: 0.00002596
Iteration 5/1000 | Loss: 0.00007793
Iteration 6/1000 | Loss: 0.00002408
Iteration 7/1000 | Loss: 0.00002253
Iteration 8/1000 | Loss: 0.00002199
Iteration 9/1000 | Loss: 0.00002158
Iteration 10/1000 | Loss: 0.00002112
Iteration 11/1000 | Loss: 0.00002082
Iteration 12/1000 | Loss: 0.00002058
Iteration 13/1000 | Loss: 0.00002043
Iteration 14/1000 | Loss: 0.00002036
Iteration 15/1000 | Loss: 0.00002033
Iteration 16/1000 | Loss: 0.00011387
Iteration 17/1000 | Loss: 0.00033857
Iteration 18/1000 | Loss: 0.00014816
Iteration 19/1000 | Loss: 0.00002556
Iteration 20/1000 | Loss: 0.00002211
Iteration 21/1000 | Loss: 0.00005401
Iteration 22/1000 | Loss: 0.00011887
Iteration 23/1000 | Loss: 0.00014126
Iteration 24/1000 | Loss: 0.00004005
Iteration 25/1000 | Loss: 0.00002002
Iteration 26/1000 | Loss: 0.00001962
Iteration 27/1000 | Loss: 0.00001928
Iteration 28/1000 | Loss: 0.00015774
Iteration 29/1000 | Loss: 0.00003217
Iteration 30/1000 | Loss: 0.00001916
Iteration 31/1000 | Loss: 0.00009521
Iteration 32/1000 | Loss: 0.00001906
Iteration 33/1000 | Loss: 0.00001883
Iteration 34/1000 | Loss: 0.00001880
Iteration 35/1000 | Loss: 0.00001866
Iteration 36/1000 | Loss: 0.00001865
Iteration 37/1000 | Loss: 0.00001865
Iteration 38/1000 | Loss: 0.00001864
Iteration 39/1000 | Loss: 0.00001861
Iteration 40/1000 | Loss: 0.00001860
Iteration 41/1000 | Loss: 0.00001860
Iteration 42/1000 | Loss: 0.00001860
Iteration 43/1000 | Loss: 0.00001860
Iteration 44/1000 | Loss: 0.00001859
Iteration 45/1000 | Loss: 0.00001859
Iteration 46/1000 | Loss: 0.00001859
Iteration 47/1000 | Loss: 0.00001858
Iteration 48/1000 | Loss: 0.00001858
Iteration 49/1000 | Loss: 0.00001858
Iteration 50/1000 | Loss: 0.00001858
Iteration 51/1000 | Loss: 0.00001858
Iteration 52/1000 | Loss: 0.00001858
Iteration 53/1000 | Loss: 0.00001858
Iteration 54/1000 | Loss: 0.00001857
Iteration 55/1000 | Loss: 0.00001857
Iteration 56/1000 | Loss: 0.00001856
Iteration 57/1000 | Loss: 0.00001856
Iteration 58/1000 | Loss: 0.00001856
Iteration 59/1000 | Loss: 0.00001855
Iteration 60/1000 | Loss: 0.00001855
Iteration 61/1000 | Loss: 0.00001854
Iteration 62/1000 | Loss: 0.00001854
Iteration 63/1000 | Loss: 0.00001854
Iteration 64/1000 | Loss: 0.00001853
Iteration 65/1000 | Loss: 0.00001853
Iteration 66/1000 | Loss: 0.00001853
Iteration 67/1000 | Loss: 0.00001853
Iteration 68/1000 | Loss: 0.00001852
Iteration 69/1000 | Loss: 0.00001852
Iteration 70/1000 | Loss: 0.00001851
Iteration 71/1000 | Loss: 0.00001849
Iteration 72/1000 | Loss: 0.00001849
Iteration 73/1000 | Loss: 0.00001848
Iteration 74/1000 | Loss: 0.00001848
Iteration 75/1000 | Loss: 0.00001847
Iteration 76/1000 | Loss: 0.00001847
Iteration 77/1000 | Loss: 0.00001847
Iteration 78/1000 | Loss: 0.00001847
Iteration 79/1000 | Loss: 0.00001846
Iteration 80/1000 | Loss: 0.00001846
Iteration 81/1000 | Loss: 0.00001846
Iteration 82/1000 | Loss: 0.00001845
Iteration 83/1000 | Loss: 0.00001845
Iteration 84/1000 | Loss: 0.00001845
Iteration 85/1000 | Loss: 0.00001844
Iteration 86/1000 | Loss: 0.00001844
Iteration 87/1000 | Loss: 0.00001843
Iteration 88/1000 | Loss: 0.00001843
Iteration 89/1000 | Loss: 0.00001842
Iteration 90/1000 | Loss: 0.00001841
Iteration 91/1000 | Loss: 0.00001841
Iteration 92/1000 | Loss: 0.00001841
Iteration 93/1000 | Loss: 0.00001841
Iteration 94/1000 | Loss: 0.00001841
Iteration 95/1000 | Loss: 0.00001841
Iteration 96/1000 | Loss: 0.00001841
Iteration 97/1000 | Loss: 0.00001841
Iteration 98/1000 | Loss: 0.00001841
Iteration 99/1000 | Loss: 0.00001841
Iteration 100/1000 | Loss: 0.00001841
Iteration 101/1000 | Loss: 0.00001841
Iteration 102/1000 | Loss: 0.00001841
Iteration 103/1000 | Loss: 0.00001841
Iteration 104/1000 | Loss: 0.00001841
Iteration 105/1000 | Loss: 0.00001840
Iteration 106/1000 | Loss: 0.00001840
Iteration 107/1000 | Loss: 0.00001840
Iteration 108/1000 | Loss: 0.00001839
Iteration 109/1000 | Loss: 0.00001839
Iteration 110/1000 | Loss: 0.00001839
Iteration 111/1000 | Loss: 0.00001839
Iteration 112/1000 | Loss: 0.00001838
Iteration 113/1000 | Loss: 0.00001838
Iteration 114/1000 | Loss: 0.00001838
Iteration 115/1000 | Loss: 0.00001838
Iteration 116/1000 | Loss: 0.00001838
Iteration 117/1000 | Loss: 0.00001837
Iteration 118/1000 | Loss: 0.00001837
Iteration 119/1000 | Loss: 0.00001837
Iteration 120/1000 | Loss: 0.00001837
Iteration 121/1000 | Loss: 0.00001837
Iteration 122/1000 | Loss: 0.00001837
Iteration 123/1000 | Loss: 0.00001837
Iteration 124/1000 | Loss: 0.00001837
Iteration 125/1000 | Loss: 0.00001837
Iteration 126/1000 | Loss: 0.00001837
Iteration 127/1000 | Loss: 0.00001837
Iteration 128/1000 | Loss: 0.00001837
Iteration 129/1000 | Loss: 0.00001837
Iteration 130/1000 | Loss: 0.00001836
Iteration 131/1000 | Loss: 0.00001836
Iteration 132/1000 | Loss: 0.00001836
Iteration 133/1000 | Loss: 0.00001836
Iteration 134/1000 | Loss: 0.00001836
Iteration 135/1000 | Loss: 0.00001836
Iteration 136/1000 | Loss: 0.00001836
Iteration 137/1000 | Loss: 0.00001836
Iteration 138/1000 | Loss: 0.00001836
Iteration 139/1000 | Loss: 0.00001836
Iteration 140/1000 | Loss: 0.00001836
Iteration 141/1000 | Loss: 0.00001836
Iteration 142/1000 | Loss: 0.00001835
Iteration 143/1000 | Loss: 0.00001835
Iteration 144/1000 | Loss: 0.00001835
Iteration 145/1000 | Loss: 0.00001835
Iteration 146/1000 | Loss: 0.00001835
Iteration 147/1000 | Loss: 0.00001835
Iteration 148/1000 | Loss: 0.00001835
Iteration 149/1000 | Loss: 0.00001834
Iteration 150/1000 | Loss: 0.00001834
Iteration 151/1000 | Loss: 0.00001834
Iteration 152/1000 | Loss: 0.00001834
Iteration 153/1000 | Loss: 0.00001834
Iteration 154/1000 | Loss: 0.00001834
Iteration 155/1000 | Loss: 0.00001834
Iteration 156/1000 | Loss: 0.00001834
Iteration 157/1000 | Loss: 0.00001834
Iteration 158/1000 | Loss: 0.00001833
Iteration 159/1000 | Loss: 0.00001833
Iteration 160/1000 | Loss: 0.00001833
Iteration 161/1000 | Loss: 0.00001833
Iteration 162/1000 | Loss: 0.00001833
Iteration 163/1000 | Loss: 0.00001833
Iteration 164/1000 | Loss: 0.00001833
Iteration 165/1000 | Loss: 0.00001833
Iteration 166/1000 | Loss: 0.00001833
Iteration 167/1000 | Loss: 0.00001833
Iteration 168/1000 | Loss: 0.00001833
Iteration 169/1000 | Loss: 0.00001833
Iteration 170/1000 | Loss: 0.00001833
Iteration 171/1000 | Loss: 0.00001833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.8327746147406287e-05, 1.8327746147406287e-05, 1.8327746147406287e-05, 1.8327746147406287e-05, 1.8327746147406287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8327746147406287e-05

Optimization complete. Final v2v error: 3.568646192550659 mm

Highest mean error: 4.792995929718018 mm for frame 85

Lowest mean error: 3.0707075595855713 mm for frame 203

Saving results

Total time: 116.22962665557861
