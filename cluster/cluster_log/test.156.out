Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=156, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8736-8791
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077203
Iteration 2/25 | Loss: 0.00364493
Iteration 3/25 | Loss: 0.00271022
Iteration 4/25 | Loss: 0.00214279
Iteration 5/25 | Loss: 0.00214644
Iteration 6/25 | Loss: 0.00193921
Iteration 7/25 | Loss: 0.00174011
Iteration 8/25 | Loss: 0.00166492
Iteration 9/25 | Loss: 0.00162683
Iteration 10/25 | Loss: 0.00158065
Iteration 11/25 | Loss: 0.00156238
Iteration 12/25 | Loss: 0.00153405
Iteration 13/25 | Loss: 0.00152431
Iteration 14/25 | Loss: 0.00151223
Iteration 15/25 | Loss: 0.00149464
Iteration 16/25 | Loss: 0.00148230
Iteration 17/25 | Loss: 0.00147483
Iteration 18/25 | Loss: 0.00148175
Iteration 19/25 | Loss: 0.00146560
Iteration 20/25 | Loss: 0.00145447
Iteration 21/25 | Loss: 0.00144069
Iteration 22/25 | Loss: 0.00143476
Iteration 23/25 | Loss: 0.00140786
Iteration 24/25 | Loss: 0.00140320
Iteration 25/25 | Loss: 0.00139805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49383640
Iteration 2/25 | Loss: 0.00477727
Iteration 3/25 | Loss: 0.00297434
Iteration 4/25 | Loss: 0.00297419
Iteration 5/25 | Loss: 0.00297419
Iteration 6/25 | Loss: 0.00297419
Iteration 7/25 | Loss: 0.00297419
Iteration 8/25 | Loss: 0.00297419
Iteration 9/25 | Loss: 0.00297419
Iteration 10/25 | Loss: 0.00297418
Iteration 11/25 | Loss: 0.00297418
Iteration 12/25 | Loss: 0.00297418
Iteration 13/25 | Loss: 0.00297418
Iteration 14/25 | Loss: 0.00297418
Iteration 15/25 | Loss: 0.00297418
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0029741849284619093, 0.0029741849284619093, 0.0029741849284619093, 0.0029741849284619093, 0.0029741849284619093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029741849284619093

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00297418
Iteration 2/1000 | Loss: 0.00076752
Iteration 3/1000 | Loss: 0.00194098
Iteration 4/1000 | Loss: 0.00123272
Iteration 5/1000 | Loss: 0.00118956
Iteration 6/1000 | Loss: 0.00072568
Iteration 7/1000 | Loss: 0.00034225
Iteration 8/1000 | Loss: 0.00035387
Iteration 9/1000 | Loss: 0.00064563
Iteration 10/1000 | Loss: 0.00068157
Iteration 11/1000 | Loss: 0.00045293
Iteration 12/1000 | Loss: 0.00141309
Iteration 13/1000 | Loss: 0.00070434
Iteration 14/1000 | Loss: 0.00167220
Iteration 15/1000 | Loss: 0.00041682
Iteration 16/1000 | Loss: 0.00014948
Iteration 17/1000 | Loss: 0.00068386
Iteration 18/1000 | Loss: 0.00083477
Iteration 19/1000 | Loss: 0.00014795
Iteration 20/1000 | Loss: 0.00054333
Iteration 21/1000 | Loss: 0.00018825
Iteration 22/1000 | Loss: 0.00106309
Iteration 23/1000 | Loss: 0.00148441
Iteration 24/1000 | Loss: 0.00152014
Iteration 25/1000 | Loss: 0.00288766
Iteration 26/1000 | Loss: 0.00121657
Iteration 27/1000 | Loss: 0.00045629
Iteration 28/1000 | Loss: 0.00205720
Iteration 29/1000 | Loss: 0.00218891
Iteration 30/1000 | Loss: 0.00310348
Iteration 31/1000 | Loss: 0.00017251
Iteration 32/1000 | Loss: 0.00015222
Iteration 33/1000 | Loss: 0.00009929
Iteration 34/1000 | Loss: 0.00055745
Iteration 35/1000 | Loss: 0.00012659
Iteration 36/1000 | Loss: 0.00009296
Iteration 37/1000 | Loss: 0.00009284
Iteration 38/1000 | Loss: 0.00008439
Iteration 39/1000 | Loss: 0.00009512
Iteration 40/1000 | Loss: 0.00009589
Iteration 41/1000 | Loss: 0.00044620
Iteration 42/1000 | Loss: 0.00013545
Iteration 43/1000 | Loss: 0.00027414
Iteration 44/1000 | Loss: 0.00008156
Iteration 45/1000 | Loss: 0.00014809
Iteration 46/1000 | Loss: 0.00009602
Iteration 47/1000 | Loss: 0.00009289
Iteration 48/1000 | Loss: 0.00010000
Iteration 49/1000 | Loss: 0.00008915
Iteration 50/1000 | Loss: 0.00036099
Iteration 51/1000 | Loss: 0.00007384
Iteration 52/1000 | Loss: 0.00038737
Iteration 53/1000 | Loss: 0.00025395
Iteration 54/1000 | Loss: 0.00009860
Iteration 55/1000 | Loss: 0.00029570
Iteration 56/1000 | Loss: 0.00021745
Iteration 57/1000 | Loss: 0.00013364
Iteration 58/1000 | Loss: 0.00005656
Iteration 59/1000 | Loss: 0.00013880
Iteration 60/1000 | Loss: 0.00025637
Iteration 61/1000 | Loss: 0.00035358
Iteration 62/1000 | Loss: 0.00054349
Iteration 63/1000 | Loss: 0.00008493
Iteration 64/1000 | Loss: 0.00012907
Iteration 65/1000 | Loss: 0.00005273
Iteration 66/1000 | Loss: 0.00017960
Iteration 67/1000 | Loss: 0.00004694
Iteration 68/1000 | Loss: 0.00004617
Iteration 69/1000 | Loss: 0.00033093
Iteration 70/1000 | Loss: 0.00004549
Iteration 71/1000 | Loss: 0.00004432
Iteration 72/1000 | Loss: 0.00004376
Iteration 73/1000 | Loss: 0.00004364
Iteration 74/1000 | Loss: 0.00004306
Iteration 75/1000 | Loss: 0.00004281
Iteration 76/1000 | Loss: 0.00004324
Iteration 77/1000 | Loss: 0.00004323
Iteration 78/1000 | Loss: 0.00004291
Iteration 79/1000 | Loss: 0.00004227
Iteration 80/1000 | Loss: 0.00004213
Iteration 81/1000 | Loss: 0.00004208
Iteration 82/1000 | Loss: 0.00004207
Iteration 83/1000 | Loss: 0.00004207
Iteration 84/1000 | Loss: 0.00004205
Iteration 85/1000 | Loss: 0.00004205
Iteration 86/1000 | Loss: 0.00004197
Iteration 87/1000 | Loss: 0.00004195
Iteration 88/1000 | Loss: 0.00004194
Iteration 89/1000 | Loss: 0.00004192
Iteration 90/1000 | Loss: 0.00004192
Iteration 91/1000 | Loss: 0.00004192
Iteration 92/1000 | Loss: 0.00004221
Iteration 93/1000 | Loss: 0.00004288
Iteration 94/1000 | Loss: 0.00004220
Iteration 95/1000 | Loss: 0.00004190
Iteration 96/1000 | Loss: 0.00004202
Iteration 97/1000 | Loss: 0.00004177
Iteration 98/1000 | Loss: 0.00004176
Iteration 99/1000 | Loss: 0.00004176
Iteration 100/1000 | Loss: 0.00004176
Iteration 101/1000 | Loss: 0.00004176
Iteration 102/1000 | Loss: 0.00004176
Iteration 103/1000 | Loss: 0.00004176
Iteration 104/1000 | Loss: 0.00004176
Iteration 105/1000 | Loss: 0.00004176
Iteration 106/1000 | Loss: 0.00004176
Iteration 107/1000 | Loss: 0.00004176
Iteration 108/1000 | Loss: 0.00004176
Iteration 109/1000 | Loss: 0.00004175
Iteration 110/1000 | Loss: 0.00004175
Iteration 111/1000 | Loss: 0.00004175
Iteration 112/1000 | Loss: 0.00004174
Iteration 113/1000 | Loss: 0.00004174
Iteration 114/1000 | Loss: 0.00004174
Iteration 115/1000 | Loss: 0.00004174
Iteration 116/1000 | Loss: 0.00004174
Iteration 117/1000 | Loss: 0.00004174
Iteration 118/1000 | Loss: 0.00004174
Iteration 119/1000 | Loss: 0.00004174
Iteration 120/1000 | Loss: 0.00004174
Iteration 121/1000 | Loss: 0.00004174
Iteration 122/1000 | Loss: 0.00004173
Iteration 123/1000 | Loss: 0.00004173
Iteration 124/1000 | Loss: 0.00004173
Iteration 125/1000 | Loss: 0.00004173
Iteration 126/1000 | Loss: 0.00004172
Iteration 127/1000 | Loss: 0.00004172
Iteration 128/1000 | Loss: 0.00004172
Iteration 129/1000 | Loss: 0.00004171
Iteration 130/1000 | Loss: 0.00004202
Iteration 131/1000 | Loss: 0.00004185
Iteration 132/1000 | Loss: 0.00004195
Iteration 133/1000 | Loss: 0.00004185
Iteration 134/1000 | Loss: 0.00004184
Iteration 135/1000 | Loss: 0.00004181
Iteration 136/1000 | Loss: 0.00004168
Iteration 137/1000 | Loss: 0.00004166
Iteration 138/1000 | Loss: 0.00004166
Iteration 139/1000 | Loss: 0.00004165
Iteration 140/1000 | Loss: 0.00004165
Iteration 141/1000 | Loss: 0.00004165
Iteration 142/1000 | Loss: 0.00004165
Iteration 143/1000 | Loss: 0.00004165
Iteration 144/1000 | Loss: 0.00004165
Iteration 145/1000 | Loss: 0.00004165
Iteration 146/1000 | Loss: 0.00004165
Iteration 147/1000 | Loss: 0.00004165
Iteration 148/1000 | Loss: 0.00004165
Iteration 149/1000 | Loss: 0.00004165
Iteration 150/1000 | Loss: 0.00004165
Iteration 151/1000 | Loss: 0.00004164
Iteration 152/1000 | Loss: 0.00004164
Iteration 153/1000 | Loss: 0.00004164
Iteration 154/1000 | Loss: 0.00004164
Iteration 155/1000 | Loss: 0.00004164
Iteration 156/1000 | Loss: 0.00004164
Iteration 157/1000 | Loss: 0.00004164
Iteration 158/1000 | Loss: 0.00004164
Iteration 159/1000 | Loss: 0.00004164
Iteration 160/1000 | Loss: 0.00004164
Iteration 161/1000 | Loss: 0.00004164
Iteration 162/1000 | Loss: 0.00004163
Iteration 163/1000 | Loss: 0.00004163
Iteration 164/1000 | Loss: 0.00004163
Iteration 165/1000 | Loss: 0.00004163
Iteration 166/1000 | Loss: 0.00004163
Iteration 167/1000 | Loss: 0.00004163
Iteration 168/1000 | Loss: 0.00004163
Iteration 169/1000 | Loss: 0.00004163
Iteration 170/1000 | Loss: 0.00004163
Iteration 171/1000 | Loss: 0.00004163
Iteration 172/1000 | Loss: 0.00004163
Iteration 173/1000 | Loss: 0.00004163
Iteration 174/1000 | Loss: 0.00004163
Iteration 175/1000 | Loss: 0.00004163
Iteration 176/1000 | Loss: 0.00004163
Iteration 177/1000 | Loss: 0.00004163
Iteration 178/1000 | Loss: 0.00004163
Iteration 179/1000 | Loss: 0.00004163
Iteration 180/1000 | Loss: 0.00004162
Iteration 181/1000 | Loss: 0.00004162
Iteration 182/1000 | Loss: 0.00004162
Iteration 183/1000 | Loss: 0.00004162
Iteration 184/1000 | Loss: 0.00004162
Iteration 185/1000 | Loss: 0.00004162
Iteration 186/1000 | Loss: 0.00004161
Iteration 187/1000 | Loss: 0.00004161
Iteration 188/1000 | Loss: 0.00004161
Iteration 189/1000 | Loss: 0.00004161
Iteration 190/1000 | Loss: 0.00004161
Iteration 191/1000 | Loss: 0.00004161
Iteration 192/1000 | Loss: 0.00004161
Iteration 193/1000 | Loss: 0.00004161
Iteration 194/1000 | Loss: 0.00004161
Iteration 195/1000 | Loss: 0.00004161
Iteration 196/1000 | Loss: 0.00004161
Iteration 197/1000 | Loss: 0.00004161
Iteration 198/1000 | Loss: 0.00004161
Iteration 199/1000 | Loss: 0.00004160
Iteration 200/1000 | Loss: 0.00004160
Iteration 201/1000 | Loss: 0.00004160
Iteration 202/1000 | Loss: 0.00004160
Iteration 203/1000 | Loss: 0.00004160
Iteration 204/1000 | Loss: 0.00004160
Iteration 205/1000 | Loss: 0.00004160
Iteration 206/1000 | Loss: 0.00004160
Iteration 207/1000 | Loss: 0.00004160
Iteration 208/1000 | Loss: 0.00004160
Iteration 209/1000 | Loss: 0.00004160
Iteration 210/1000 | Loss: 0.00004160
Iteration 211/1000 | Loss: 0.00004160
Iteration 212/1000 | Loss: 0.00004160
Iteration 213/1000 | Loss: 0.00004160
Iteration 214/1000 | Loss: 0.00004160
Iteration 215/1000 | Loss: 0.00004160
Iteration 216/1000 | Loss: 0.00004160
Iteration 217/1000 | Loss: 0.00004160
Iteration 218/1000 | Loss: 0.00004160
Iteration 219/1000 | Loss: 0.00004160
Iteration 220/1000 | Loss: 0.00004160
Iteration 221/1000 | Loss: 0.00004160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [4.159555464866571e-05, 4.159555464866571e-05, 4.159555464866571e-05, 4.159555464866571e-05, 4.159555464866571e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.159555464866571e-05

Optimization complete. Final v2v error: 5.2441582679748535 mm

Highest mean error: 12.621223449707031 mm for frame 24

Lowest mean error: 4.438104152679443 mm for frame 122

Saving results

Total time: 203.53788590431213
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884514
Iteration 2/25 | Loss: 0.00168771
Iteration 3/25 | Loss: 0.00136654
Iteration 4/25 | Loss: 0.00132904
Iteration 5/25 | Loss: 0.00131772
Iteration 6/25 | Loss: 0.00131762
Iteration 7/25 | Loss: 0.00131849
Iteration 8/25 | Loss: 0.00131691
Iteration 9/25 | Loss: 0.00131506
Iteration 10/25 | Loss: 0.00131547
Iteration 11/25 | Loss: 0.00131578
Iteration 12/25 | Loss: 0.00131491
Iteration 13/25 | Loss: 0.00131109
Iteration 14/25 | Loss: 0.00131413
Iteration 15/25 | Loss: 0.00131229
Iteration 16/25 | Loss: 0.00131211
Iteration 17/25 | Loss: 0.00131249
Iteration 18/25 | Loss: 0.00131280
Iteration 19/25 | Loss: 0.00131203
Iteration 20/25 | Loss: 0.00131180
Iteration 21/25 | Loss: 0.00131151
Iteration 22/25 | Loss: 0.00131168
Iteration 23/25 | Loss: 0.00131281
Iteration 24/25 | Loss: 0.00131137
Iteration 25/25 | Loss: 0.00131354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.74376273
Iteration 2/25 | Loss: 0.00142130
Iteration 3/25 | Loss: 0.00142129
Iteration 4/25 | Loss: 0.00142129
Iteration 5/25 | Loss: 0.00142129
Iteration 6/25 | Loss: 0.00142129
Iteration 7/25 | Loss: 0.00142129
Iteration 8/25 | Loss: 0.00142129
Iteration 9/25 | Loss: 0.00142129
Iteration 10/25 | Loss: 0.00142129
Iteration 11/25 | Loss: 0.00142129
Iteration 12/25 | Loss: 0.00142129
Iteration 13/25 | Loss: 0.00142129
Iteration 14/25 | Loss: 0.00142129
Iteration 15/25 | Loss: 0.00142129
Iteration 16/25 | Loss: 0.00142129
Iteration 17/25 | Loss: 0.00142129
Iteration 18/25 | Loss: 0.00142129
Iteration 19/25 | Loss: 0.00142129
Iteration 20/25 | Loss: 0.00142129
Iteration 21/25 | Loss: 0.00142129
Iteration 22/25 | Loss: 0.00142129
Iteration 23/25 | Loss: 0.00142129
Iteration 24/25 | Loss: 0.00142129
Iteration 25/25 | Loss: 0.00142129

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142129
Iteration 2/1000 | Loss: 0.00005040
Iteration 3/1000 | Loss: 0.00004398
Iteration 4/1000 | Loss: 0.00007033
Iteration 5/1000 | Loss: 0.00004788
Iteration 6/1000 | Loss: 0.00004522
Iteration 7/1000 | Loss: 0.00005183
Iteration 8/1000 | Loss: 0.00005874
Iteration 9/1000 | Loss: 0.00005127
Iteration 10/1000 | Loss: 0.00005135
Iteration 11/1000 | Loss: 0.00004798
Iteration 12/1000 | Loss: 0.00004415
Iteration 13/1000 | Loss: 0.00005174
Iteration 14/1000 | Loss: 0.00005971
Iteration 15/1000 | Loss: 0.00005303
Iteration 16/1000 | Loss: 0.00005055
Iteration 17/1000 | Loss: 0.00004661
Iteration 18/1000 | Loss: 0.00005788
Iteration 19/1000 | Loss: 0.00004758
Iteration 20/1000 | Loss: 0.00005181
Iteration 21/1000 | Loss: 0.00005170
Iteration 22/1000 | Loss: 0.00005128
Iteration 23/1000 | Loss: 0.00005287
Iteration 24/1000 | Loss: 0.00004798
Iteration 25/1000 | Loss: 0.00005024
Iteration 26/1000 | Loss: 0.00004894
Iteration 27/1000 | Loss: 0.00004843
Iteration 28/1000 | Loss: 0.00005013
Iteration 29/1000 | Loss: 0.00005007
Iteration 30/1000 | Loss: 0.00004666
Iteration 31/1000 | Loss: 0.00004902
Iteration 32/1000 | Loss: 0.00005381
Iteration 33/1000 | Loss: 0.00004739
Iteration 34/1000 | Loss: 0.00004896
Iteration 35/1000 | Loss: 0.00004272
Iteration 36/1000 | Loss: 0.00004709
Iteration 37/1000 | Loss: 0.00005298
Iteration 38/1000 | Loss: 0.00004704
Iteration 39/1000 | Loss: 0.00005162
Iteration 40/1000 | Loss: 0.00004691
Iteration 41/1000 | Loss: 0.00004854
Iteration 42/1000 | Loss: 0.00004708
Iteration 43/1000 | Loss: 0.00005024
Iteration 44/1000 | Loss: 0.00004685
Iteration 45/1000 | Loss: 0.00004685
Iteration 46/1000 | Loss: 0.00004165
Iteration 47/1000 | Loss: 0.00005011
Iteration 48/1000 | Loss: 0.00004799
Iteration 49/1000 | Loss: 0.00004651
Iteration 50/1000 | Loss: 0.00004315
Iteration 51/1000 | Loss: 0.00004695
Iteration 52/1000 | Loss: 0.00004648
Iteration 53/1000 | Loss: 0.00006609
Iteration 54/1000 | Loss: 0.00004932
Iteration 55/1000 | Loss: 0.00005021
Iteration 56/1000 | Loss: 0.00003674
Iteration 57/1000 | Loss: 0.00003556
Iteration 58/1000 | Loss: 0.00003449
Iteration 59/1000 | Loss: 0.00003387
Iteration 60/1000 | Loss: 0.00003349
Iteration 61/1000 | Loss: 0.00003338
Iteration 62/1000 | Loss: 0.00003320
Iteration 63/1000 | Loss: 0.00003319
Iteration 64/1000 | Loss: 0.00003312
Iteration 65/1000 | Loss: 0.00003312
Iteration 66/1000 | Loss: 0.00003312
Iteration 67/1000 | Loss: 0.00003311
Iteration 68/1000 | Loss: 0.00003311
Iteration 69/1000 | Loss: 0.00003311
Iteration 70/1000 | Loss: 0.00003311
Iteration 71/1000 | Loss: 0.00003311
Iteration 72/1000 | Loss: 0.00003310
Iteration 73/1000 | Loss: 0.00003307
Iteration 74/1000 | Loss: 0.00003307
Iteration 75/1000 | Loss: 0.00003306
Iteration 76/1000 | Loss: 0.00003306
Iteration 77/1000 | Loss: 0.00003306
Iteration 78/1000 | Loss: 0.00003305
Iteration 79/1000 | Loss: 0.00003305
Iteration 80/1000 | Loss: 0.00003304
Iteration 81/1000 | Loss: 0.00003304
Iteration 82/1000 | Loss: 0.00003303
Iteration 83/1000 | Loss: 0.00003302
Iteration 84/1000 | Loss: 0.00003302
Iteration 85/1000 | Loss: 0.00003302
Iteration 86/1000 | Loss: 0.00003302
Iteration 87/1000 | Loss: 0.00003301
Iteration 88/1000 | Loss: 0.00003301
Iteration 89/1000 | Loss: 0.00003301
Iteration 90/1000 | Loss: 0.00003301
Iteration 91/1000 | Loss: 0.00003300
Iteration 92/1000 | Loss: 0.00003299
Iteration 93/1000 | Loss: 0.00003299
Iteration 94/1000 | Loss: 0.00003299
Iteration 95/1000 | Loss: 0.00003299
Iteration 96/1000 | Loss: 0.00003299
Iteration 97/1000 | Loss: 0.00003299
Iteration 98/1000 | Loss: 0.00003298
Iteration 99/1000 | Loss: 0.00003298
Iteration 100/1000 | Loss: 0.00003298
Iteration 101/1000 | Loss: 0.00003298
Iteration 102/1000 | Loss: 0.00003298
Iteration 103/1000 | Loss: 0.00003298
Iteration 104/1000 | Loss: 0.00003298
Iteration 105/1000 | Loss: 0.00003298
Iteration 106/1000 | Loss: 0.00003298
Iteration 107/1000 | Loss: 0.00003298
Iteration 108/1000 | Loss: 0.00003298
Iteration 109/1000 | Loss: 0.00003297
Iteration 110/1000 | Loss: 0.00003297
Iteration 111/1000 | Loss: 0.00003297
Iteration 112/1000 | Loss: 0.00003297
Iteration 113/1000 | Loss: 0.00003297
Iteration 114/1000 | Loss: 0.00003297
Iteration 115/1000 | Loss: 0.00003296
Iteration 116/1000 | Loss: 0.00003296
Iteration 117/1000 | Loss: 0.00003296
Iteration 118/1000 | Loss: 0.00003296
Iteration 119/1000 | Loss: 0.00003296
Iteration 120/1000 | Loss: 0.00003296
Iteration 121/1000 | Loss: 0.00003296
Iteration 122/1000 | Loss: 0.00003296
Iteration 123/1000 | Loss: 0.00003296
Iteration 124/1000 | Loss: 0.00003295
Iteration 125/1000 | Loss: 0.00003295
Iteration 126/1000 | Loss: 0.00003295
Iteration 127/1000 | Loss: 0.00003295
Iteration 128/1000 | Loss: 0.00003295
Iteration 129/1000 | Loss: 0.00003295
Iteration 130/1000 | Loss: 0.00003295
Iteration 131/1000 | Loss: 0.00003294
Iteration 132/1000 | Loss: 0.00003294
Iteration 133/1000 | Loss: 0.00003294
Iteration 134/1000 | Loss: 0.00003294
Iteration 135/1000 | Loss: 0.00003294
Iteration 136/1000 | Loss: 0.00003294
Iteration 137/1000 | Loss: 0.00003294
Iteration 138/1000 | Loss: 0.00003294
Iteration 139/1000 | Loss: 0.00003293
Iteration 140/1000 | Loss: 0.00003293
Iteration 141/1000 | Loss: 0.00003293
Iteration 142/1000 | Loss: 0.00003293
Iteration 143/1000 | Loss: 0.00003293
Iteration 144/1000 | Loss: 0.00003293
Iteration 145/1000 | Loss: 0.00003293
Iteration 146/1000 | Loss: 0.00003293
Iteration 147/1000 | Loss: 0.00003293
Iteration 148/1000 | Loss: 0.00003293
Iteration 149/1000 | Loss: 0.00003293
Iteration 150/1000 | Loss: 0.00003293
Iteration 151/1000 | Loss: 0.00003293
Iteration 152/1000 | Loss: 0.00003293
Iteration 153/1000 | Loss: 0.00003293
Iteration 154/1000 | Loss: 0.00003293
Iteration 155/1000 | Loss: 0.00003293
Iteration 156/1000 | Loss: 0.00003293
Iteration 157/1000 | Loss: 0.00003293
Iteration 158/1000 | Loss: 0.00003293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [3.292946348665282e-05, 3.292946348665282e-05, 3.292946348665282e-05, 3.292946348665282e-05, 3.292946348665282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.292946348665282e-05

Optimization complete. Final v2v error: 4.95223331451416 mm

Highest mean error: 10.836811065673828 mm for frame 52

Lowest mean error: 4.549428939819336 mm for frame 37

Saving results

Total time: 156.5953209400177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952818
Iteration 2/25 | Loss: 0.00145558
Iteration 3/25 | Loss: 0.00135211
Iteration 4/25 | Loss: 0.00133844
Iteration 5/25 | Loss: 0.00133444
Iteration 6/25 | Loss: 0.00133386
Iteration 7/25 | Loss: 0.00133386
Iteration 8/25 | Loss: 0.00133386
Iteration 9/25 | Loss: 0.00133386
Iteration 10/25 | Loss: 0.00133386
Iteration 11/25 | Loss: 0.00133386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013338618446141481, 0.0013338618446141481, 0.0013338618446141481, 0.0013338618446141481, 0.0013338618446141481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013338618446141481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.86527967
Iteration 2/25 | Loss: 0.00153761
Iteration 3/25 | Loss: 0.00153761
Iteration 4/25 | Loss: 0.00153761
Iteration 5/25 | Loss: 0.00153761
Iteration 6/25 | Loss: 0.00153761
Iteration 7/25 | Loss: 0.00153761
Iteration 8/25 | Loss: 0.00153761
Iteration 9/25 | Loss: 0.00153761
Iteration 10/25 | Loss: 0.00153761
Iteration 11/25 | Loss: 0.00153761
Iteration 12/25 | Loss: 0.00153761
Iteration 13/25 | Loss: 0.00153761
Iteration 14/25 | Loss: 0.00153761
Iteration 15/25 | Loss: 0.00153761
Iteration 16/25 | Loss: 0.00153761
Iteration 17/25 | Loss: 0.00153761
Iteration 18/25 | Loss: 0.00153761
Iteration 19/25 | Loss: 0.00153761
Iteration 20/25 | Loss: 0.00153761
Iteration 21/25 | Loss: 0.00153761
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015376118244603276, 0.0015376118244603276, 0.0015376118244603276, 0.0015376118244603276, 0.0015376118244603276]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015376118244603276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153761
Iteration 2/1000 | Loss: 0.00003290
Iteration 3/1000 | Loss: 0.00002438
Iteration 4/1000 | Loss: 0.00002227
Iteration 5/1000 | Loss: 0.00002129
Iteration 6/1000 | Loss: 0.00002079
Iteration 7/1000 | Loss: 0.00002049
Iteration 8/1000 | Loss: 0.00002029
Iteration 9/1000 | Loss: 0.00002024
Iteration 10/1000 | Loss: 0.00002020
Iteration 11/1000 | Loss: 0.00002019
Iteration 12/1000 | Loss: 0.00002018
Iteration 13/1000 | Loss: 0.00002018
Iteration 14/1000 | Loss: 0.00002018
Iteration 15/1000 | Loss: 0.00002018
Iteration 16/1000 | Loss: 0.00002018
Iteration 17/1000 | Loss: 0.00002017
Iteration 18/1000 | Loss: 0.00002017
Iteration 19/1000 | Loss: 0.00002017
Iteration 20/1000 | Loss: 0.00002016
Iteration 21/1000 | Loss: 0.00002015
Iteration 22/1000 | Loss: 0.00002015
Iteration 23/1000 | Loss: 0.00002014
Iteration 24/1000 | Loss: 0.00002014
Iteration 25/1000 | Loss: 0.00002014
Iteration 26/1000 | Loss: 0.00002013
Iteration 27/1000 | Loss: 0.00002013
Iteration 28/1000 | Loss: 0.00002013
Iteration 29/1000 | Loss: 0.00002013
Iteration 30/1000 | Loss: 0.00002012
Iteration 31/1000 | Loss: 0.00002012
Iteration 32/1000 | Loss: 0.00002012
Iteration 33/1000 | Loss: 0.00002012
Iteration 34/1000 | Loss: 0.00002011
Iteration 35/1000 | Loss: 0.00002011
Iteration 36/1000 | Loss: 0.00002011
Iteration 37/1000 | Loss: 0.00002011
Iteration 38/1000 | Loss: 0.00002011
Iteration 39/1000 | Loss: 0.00002011
Iteration 40/1000 | Loss: 0.00002011
Iteration 41/1000 | Loss: 0.00002011
Iteration 42/1000 | Loss: 0.00002011
Iteration 43/1000 | Loss: 0.00002010
Iteration 44/1000 | Loss: 0.00002010
Iteration 45/1000 | Loss: 0.00002010
Iteration 46/1000 | Loss: 0.00002010
Iteration 47/1000 | Loss: 0.00002010
Iteration 48/1000 | Loss: 0.00002009
Iteration 49/1000 | Loss: 0.00002009
Iteration 50/1000 | Loss: 0.00002009
Iteration 51/1000 | Loss: 0.00002009
Iteration 52/1000 | Loss: 0.00002009
Iteration 53/1000 | Loss: 0.00002009
Iteration 54/1000 | Loss: 0.00002009
Iteration 55/1000 | Loss: 0.00002009
Iteration 56/1000 | Loss: 0.00002009
Iteration 57/1000 | Loss: 0.00002009
Iteration 58/1000 | Loss: 0.00002009
Iteration 59/1000 | Loss: 0.00002009
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [2.00853082787944e-05, 2.00853082787944e-05, 2.00853082787944e-05, 2.00853082787944e-05, 2.00853082787944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.00853082787944e-05

Optimization complete. Final v2v error: 3.862630844116211 mm

Highest mean error: 4.232182025909424 mm for frame 225

Lowest mean error: 3.6461234092712402 mm for frame 150

Saving results

Total time: 26.761244773864746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032133
Iteration 2/25 | Loss: 0.00148988
Iteration 3/25 | Loss: 0.00134516
Iteration 4/25 | Loss: 0.00133480
Iteration 5/25 | Loss: 0.00133150
Iteration 6/25 | Loss: 0.00132996
Iteration 7/25 | Loss: 0.00132967
Iteration 8/25 | Loss: 0.00132965
Iteration 9/25 | Loss: 0.00132965
Iteration 10/25 | Loss: 0.00132965
Iteration 11/25 | Loss: 0.00132965
Iteration 12/25 | Loss: 0.00132965
Iteration 13/25 | Loss: 0.00132965
Iteration 14/25 | Loss: 0.00132965
Iteration 15/25 | Loss: 0.00132965
Iteration 16/25 | Loss: 0.00132965
Iteration 17/25 | Loss: 0.00132965
Iteration 18/25 | Loss: 0.00132965
Iteration 19/25 | Loss: 0.00132965
Iteration 20/25 | Loss: 0.00132965
Iteration 21/25 | Loss: 0.00132965
Iteration 22/25 | Loss: 0.00132965
Iteration 23/25 | Loss: 0.00132965
Iteration 24/25 | Loss: 0.00132965
Iteration 25/25 | Loss: 0.00132965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32154846
Iteration 2/25 | Loss: 0.00173886
Iteration 3/25 | Loss: 0.00173884
Iteration 4/25 | Loss: 0.00173884
Iteration 5/25 | Loss: 0.00173884
Iteration 6/25 | Loss: 0.00173884
Iteration 7/25 | Loss: 0.00173884
Iteration 8/25 | Loss: 0.00173884
Iteration 9/25 | Loss: 0.00173884
Iteration 10/25 | Loss: 0.00173884
Iteration 11/25 | Loss: 0.00173884
Iteration 12/25 | Loss: 0.00173884
Iteration 13/25 | Loss: 0.00173884
Iteration 14/25 | Loss: 0.00173884
Iteration 15/25 | Loss: 0.00173884
Iteration 16/25 | Loss: 0.00173884
Iteration 17/25 | Loss: 0.00173884
Iteration 18/25 | Loss: 0.00173884
Iteration 19/25 | Loss: 0.00173884
Iteration 20/25 | Loss: 0.00173884
Iteration 21/25 | Loss: 0.00173884
Iteration 22/25 | Loss: 0.00173884
Iteration 23/25 | Loss: 0.00173884
Iteration 24/25 | Loss: 0.00173884
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0017388418782502413, 0.0017388418782502413, 0.0017388418782502413, 0.0017388418782502413, 0.0017388418782502413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017388418782502413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173884
Iteration 2/1000 | Loss: 0.00003740
Iteration 3/1000 | Loss: 0.00002535
Iteration 4/1000 | Loss: 0.00002241
Iteration 5/1000 | Loss: 0.00002155
Iteration 6/1000 | Loss: 0.00002098
Iteration 7/1000 | Loss: 0.00002064
Iteration 8/1000 | Loss: 0.00002049
Iteration 9/1000 | Loss: 0.00002033
Iteration 10/1000 | Loss: 0.00002031
Iteration 11/1000 | Loss: 0.00002025
Iteration 12/1000 | Loss: 0.00002019
Iteration 13/1000 | Loss: 0.00002019
Iteration 14/1000 | Loss: 0.00002019
Iteration 15/1000 | Loss: 0.00002019
Iteration 16/1000 | Loss: 0.00002019
Iteration 17/1000 | Loss: 0.00002019
Iteration 18/1000 | Loss: 0.00002016
Iteration 19/1000 | Loss: 0.00002016
Iteration 20/1000 | Loss: 0.00002016
Iteration 21/1000 | Loss: 0.00002015
Iteration 22/1000 | Loss: 0.00002015
Iteration 23/1000 | Loss: 0.00002014
Iteration 24/1000 | Loss: 0.00002014
Iteration 25/1000 | Loss: 0.00002012
Iteration 26/1000 | Loss: 0.00002011
Iteration 27/1000 | Loss: 0.00002011
Iteration 28/1000 | Loss: 0.00002010
Iteration 29/1000 | Loss: 0.00002010
Iteration 30/1000 | Loss: 0.00002009
Iteration 31/1000 | Loss: 0.00002009
Iteration 32/1000 | Loss: 0.00002009
Iteration 33/1000 | Loss: 0.00002008
Iteration 34/1000 | Loss: 0.00002008
Iteration 35/1000 | Loss: 0.00002007
Iteration 36/1000 | Loss: 0.00002007
Iteration 37/1000 | Loss: 0.00002007
Iteration 38/1000 | Loss: 0.00002007
Iteration 39/1000 | Loss: 0.00002007
Iteration 40/1000 | Loss: 0.00002006
Iteration 41/1000 | Loss: 0.00002006
Iteration 42/1000 | Loss: 0.00002006
Iteration 43/1000 | Loss: 0.00002005
Iteration 44/1000 | Loss: 0.00002005
Iteration 45/1000 | Loss: 0.00002005
Iteration 46/1000 | Loss: 0.00002005
Iteration 47/1000 | Loss: 0.00002005
Iteration 48/1000 | Loss: 0.00002005
Iteration 49/1000 | Loss: 0.00002004
Iteration 50/1000 | Loss: 0.00002004
Iteration 51/1000 | Loss: 0.00002004
Iteration 52/1000 | Loss: 0.00002004
Iteration 53/1000 | Loss: 0.00002004
Iteration 54/1000 | Loss: 0.00002004
Iteration 55/1000 | Loss: 0.00002004
Iteration 56/1000 | Loss: 0.00002004
Iteration 57/1000 | Loss: 0.00002004
Iteration 58/1000 | Loss: 0.00002004
Iteration 59/1000 | Loss: 0.00002004
Iteration 60/1000 | Loss: 0.00002004
Iteration 61/1000 | Loss: 0.00002003
Iteration 62/1000 | Loss: 0.00002003
Iteration 63/1000 | Loss: 0.00002003
Iteration 64/1000 | Loss: 0.00002003
Iteration 65/1000 | Loss: 0.00002003
Iteration 66/1000 | Loss: 0.00002002
Iteration 67/1000 | Loss: 0.00002002
Iteration 68/1000 | Loss: 0.00002002
Iteration 69/1000 | Loss: 0.00002002
Iteration 70/1000 | Loss: 0.00002002
Iteration 71/1000 | Loss: 0.00002002
Iteration 72/1000 | Loss: 0.00002002
Iteration 73/1000 | Loss: 0.00002001
Iteration 74/1000 | Loss: 0.00002001
Iteration 75/1000 | Loss: 0.00002001
Iteration 76/1000 | Loss: 0.00002001
Iteration 77/1000 | Loss: 0.00002001
Iteration 78/1000 | Loss: 0.00002001
Iteration 79/1000 | Loss: 0.00002001
Iteration 80/1000 | Loss: 0.00002000
Iteration 81/1000 | Loss: 0.00002000
Iteration 82/1000 | Loss: 0.00002000
Iteration 83/1000 | Loss: 0.00002000
Iteration 84/1000 | Loss: 0.00002000
Iteration 85/1000 | Loss: 0.00002000
Iteration 86/1000 | Loss: 0.00002000
Iteration 87/1000 | Loss: 0.00002000
Iteration 88/1000 | Loss: 0.00002000
Iteration 89/1000 | Loss: 0.00002000
Iteration 90/1000 | Loss: 0.00002000
Iteration 91/1000 | Loss: 0.00002000
Iteration 92/1000 | Loss: 0.00002000
Iteration 93/1000 | Loss: 0.00002000
Iteration 94/1000 | Loss: 0.00001999
Iteration 95/1000 | Loss: 0.00001999
Iteration 96/1000 | Loss: 0.00001999
Iteration 97/1000 | Loss: 0.00001999
Iteration 98/1000 | Loss: 0.00001999
Iteration 99/1000 | Loss: 0.00001999
Iteration 100/1000 | Loss: 0.00001999
Iteration 101/1000 | Loss: 0.00001999
Iteration 102/1000 | Loss: 0.00001999
Iteration 103/1000 | Loss: 0.00001999
Iteration 104/1000 | Loss: 0.00001998
Iteration 105/1000 | Loss: 0.00001998
Iteration 106/1000 | Loss: 0.00001998
Iteration 107/1000 | Loss: 0.00001998
Iteration 108/1000 | Loss: 0.00001998
Iteration 109/1000 | Loss: 0.00001998
Iteration 110/1000 | Loss: 0.00001998
Iteration 111/1000 | Loss: 0.00001998
Iteration 112/1000 | Loss: 0.00001998
Iteration 113/1000 | Loss: 0.00001998
Iteration 114/1000 | Loss: 0.00001998
Iteration 115/1000 | Loss: 0.00001998
Iteration 116/1000 | Loss: 0.00001998
Iteration 117/1000 | Loss: 0.00001998
Iteration 118/1000 | Loss: 0.00001998
Iteration 119/1000 | Loss: 0.00001998
Iteration 120/1000 | Loss: 0.00001998
Iteration 121/1000 | Loss: 0.00001998
Iteration 122/1000 | Loss: 0.00001998
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.9980690922238864e-05, 1.9980690922238864e-05, 1.9980690922238864e-05, 1.9980690922238864e-05, 1.9980690922238864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9980690922238864e-05

Optimization complete. Final v2v error: 3.9487318992614746 mm

Highest mean error: 4.282952785491943 mm for frame 124

Lowest mean error: 3.7589550018310547 mm for frame 83

Saving results

Total time: 30.36940598487854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01110067
Iteration 2/25 | Loss: 0.01110067
Iteration 3/25 | Loss: 0.00300377
Iteration 4/25 | Loss: 0.00214995
Iteration 5/25 | Loss: 0.00213739
Iteration 6/25 | Loss: 0.00198848
Iteration 7/25 | Loss: 0.00186665
Iteration 8/25 | Loss: 0.00158684
Iteration 9/25 | Loss: 0.00147356
Iteration 10/25 | Loss: 0.00145137
Iteration 11/25 | Loss: 0.00139884
Iteration 12/25 | Loss: 0.00138172
Iteration 13/25 | Loss: 0.00136926
Iteration 14/25 | Loss: 0.00137297
Iteration 15/25 | Loss: 0.00134966
Iteration 16/25 | Loss: 0.00133971
Iteration 17/25 | Loss: 0.00133431
Iteration 18/25 | Loss: 0.00132486
Iteration 19/25 | Loss: 0.00132447
Iteration 20/25 | Loss: 0.00132284
Iteration 21/25 | Loss: 0.00132494
Iteration 22/25 | Loss: 0.00132486
Iteration 23/25 | Loss: 0.00132222
Iteration 24/25 | Loss: 0.00131901
Iteration 25/25 | Loss: 0.00131582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49258733
Iteration 2/25 | Loss: 0.00358107
Iteration 3/25 | Loss: 0.00358107
Iteration 4/25 | Loss: 0.00342630
Iteration 5/25 | Loss: 0.00342628
Iteration 6/25 | Loss: 0.00342628
Iteration 7/25 | Loss: 0.00342628
Iteration 8/25 | Loss: 0.00342628
Iteration 9/25 | Loss: 0.00342628
Iteration 10/25 | Loss: 0.00342628
Iteration 11/25 | Loss: 0.00342628
Iteration 12/25 | Loss: 0.00342628
Iteration 13/25 | Loss: 0.00342628
Iteration 14/25 | Loss: 0.00342628
Iteration 15/25 | Loss: 0.00342628
Iteration 16/25 | Loss: 0.00342628
Iteration 17/25 | Loss: 0.00342628
Iteration 18/25 | Loss: 0.00342628
Iteration 19/25 | Loss: 0.00342628
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.003426279639825225, 0.003426279639825225, 0.003426279639825225, 0.003426279639825225, 0.003426279639825225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003426279639825225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00342628
Iteration 2/1000 | Loss: 0.00197108
Iteration 3/1000 | Loss: 0.00073049
Iteration 4/1000 | Loss: 0.00033383
Iteration 5/1000 | Loss: 0.00052606
Iteration 6/1000 | Loss: 0.00030219
Iteration 7/1000 | Loss: 0.00112023
Iteration 8/1000 | Loss: 0.00079323
Iteration 9/1000 | Loss: 0.00043305
Iteration 10/1000 | Loss: 0.00063978
Iteration 11/1000 | Loss: 0.00047738
Iteration 12/1000 | Loss: 0.00039437
Iteration 13/1000 | Loss: 0.00030661
Iteration 14/1000 | Loss: 0.00014839
Iteration 15/1000 | Loss: 0.00051256
Iteration 16/1000 | Loss: 0.00230602
Iteration 17/1000 | Loss: 0.00152936
Iteration 18/1000 | Loss: 0.00050671
Iteration 19/1000 | Loss: 0.00045160
Iteration 20/1000 | Loss: 0.00080798
Iteration 21/1000 | Loss: 0.00070802
Iteration 22/1000 | Loss: 0.00054461
Iteration 23/1000 | Loss: 0.00040555
Iteration 24/1000 | Loss: 0.00033882
Iteration 25/1000 | Loss: 0.00012503
Iteration 26/1000 | Loss: 0.00251170
Iteration 27/1000 | Loss: 0.00166202
Iteration 28/1000 | Loss: 0.00165781
Iteration 29/1000 | Loss: 0.00089081
Iteration 30/1000 | Loss: 0.00043933
Iteration 31/1000 | Loss: 0.00056268
Iteration 32/1000 | Loss: 0.00016471
Iteration 33/1000 | Loss: 0.00017299
Iteration 34/1000 | Loss: 0.00014058
Iteration 35/1000 | Loss: 0.00009145
Iteration 36/1000 | Loss: 0.00037527
Iteration 37/1000 | Loss: 0.00040086
Iteration 38/1000 | Loss: 0.00109453
Iteration 39/1000 | Loss: 0.00022958
Iteration 40/1000 | Loss: 0.00022495
Iteration 41/1000 | Loss: 0.00009350
Iteration 42/1000 | Loss: 0.00006369
Iteration 43/1000 | Loss: 0.00007569
Iteration 44/1000 | Loss: 0.00006944
Iteration 45/1000 | Loss: 0.00005414
Iteration 46/1000 | Loss: 0.00016863
Iteration 47/1000 | Loss: 0.00020628
Iteration 48/1000 | Loss: 0.00007984
Iteration 49/1000 | Loss: 0.00018356
Iteration 50/1000 | Loss: 0.00019937
Iteration 51/1000 | Loss: 0.00008165
Iteration 52/1000 | Loss: 0.00017969
Iteration 53/1000 | Loss: 0.00028585
Iteration 54/1000 | Loss: 0.00011256
Iteration 55/1000 | Loss: 0.00005810
Iteration 56/1000 | Loss: 0.00006054
Iteration 57/1000 | Loss: 0.00006378
Iteration 58/1000 | Loss: 0.00005261
Iteration 59/1000 | Loss: 0.00007731
Iteration 60/1000 | Loss: 0.00004909
Iteration 61/1000 | Loss: 0.00005004
Iteration 62/1000 | Loss: 0.00004757
Iteration 63/1000 | Loss: 0.00004806
Iteration 64/1000 | Loss: 0.00004387
Iteration 65/1000 | Loss: 0.00004829
Iteration 66/1000 | Loss: 0.00004367
Iteration 67/1000 | Loss: 0.00004265
Iteration 68/1000 | Loss: 0.00004181
Iteration 69/1000 | Loss: 0.00005769
Iteration 70/1000 | Loss: 0.00004279
Iteration 71/1000 | Loss: 0.00017139
Iteration 72/1000 | Loss: 0.00004492
Iteration 73/1000 | Loss: 0.00004233
Iteration 74/1000 | Loss: 0.00004223
Iteration 75/1000 | Loss: 0.00004060
Iteration 76/1000 | Loss: 0.00004204
Iteration 77/1000 | Loss: 0.00004005
Iteration 78/1000 | Loss: 0.00004003
Iteration 79/1000 | Loss: 0.00003997
Iteration 80/1000 | Loss: 0.00003996
Iteration 81/1000 | Loss: 0.00004168
Iteration 82/1000 | Loss: 0.00003985
Iteration 83/1000 | Loss: 0.00003984
Iteration 84/1000 | Loss: 0.00003983
Iteration 85/1000 | Loss: 0.00004044
Iteration 86/1000 | Loss: 0.00003985
Iteration 87/1000 | Loss: 0.00004247
Iteration 88/1000 | Loss: 0.00004135
Iteration 89/1000 | Loss: 0.00023967
Iteration 90/1000 | Loss: 0.00008374
Iteration 91/1000 | Loss: 0.00004380
Iteration 92/1000 | Loss: 0.00005623
Iteration 93/1000 | Loss: 0.00005011
Iteration 94/1000 | Loss: 0.00004291
Iteration 95/1000 | Loss: 0.00004160
Iteration 96/1000 | Loss: 0.00011204
Iteration 97/1000 | Loss: 0.00004439
Iteration 98/1000 | Loss: 0.00006286
Iteration 99/1000 | Loss: 0.00005262
Iteration 100/1000 | Loss: 0.00004643
Iteration 101/1000 | Loss: 0.00004084
Iteration 102/1000 | Loss: 0.00004030
Iteration 103/1000 | Loss: 0.00004399
Iteration 104/1000 | Loss: 0.00003981
Iteration 105/1000 | Loss: 0.00004111
Iteration 106/1000 | Loss: 0.00003922
Iteration 107/1000 | Loss: 0.00003991
Iteration 108/1000 | Loss: 0.00004400
Iteration 109/1000 | Loss: 0.00003887
Iteration 110/1000 | Loss: 0.00003861
Iteration 111/1000 | Loss: 0.00004423
Iteration 112/1000 | Loss: 0.00003962
Iteration 113/1000 | Loss: 0.00003867
Iteration 114/1000 | Loss: 0.00003851
Iteration 115/1000 | Loss: 0.00004080
Iteration 116/1000 | Loss: 0.00003909
Iteration 117/1000 | Loss: 0.00004111
Iteration 118/1000 | Loss: 0.00003888
Iteration 119/1000 | Loss: 0.00003839
Iteration 120/1000 | Loss: 0.00003839
Iteration 121/1000 | Loss: 0.00003839
Iteration 122/1000 | Loss: 0.00003839
Iteration 123/1000 | Loss: 0.00003838
Iteration 124/1000 | Loss: 0.00003838
Iteration 125/1000 | Loss: 0.00003838
Iteration 126/1000 | Loss: 0.00003838
Iteration 127/1000 | Loss: 0.00003838
Iteration 128/1000 | Loss: 0.00003838
Iteration 129/1000 | Loss: 0.00003838
Iteration 130/1000 | Loss: 0.00003838
Iteration 131/1000 | Loss: 0.00003838
Iteration 132/1000 | Loss: 0.00003838
Iteration 133/1000 | Loss: 0.00003838
Iteration 134/1000 | Loss: 0.00003837
Iteration 135/1000 | Loss: 0.00003837
Iteration 136/1000 | Loss: 0.00003837
Iteration 137/1000 | Loss: 0.00003837
Iteration 138/1000 | Loss: 0.00003837
Iteration 139/1000 | Loss: 0.00003837
Iteration 140/1000 | Loss: 0.00003837
Iteration 141/1000 | Loss: 0.00003837
Iteration 142/1000 | Loss: 0.00003837
Iteration 143/1000 | Loss: 0.00003837
Iteration 144/1000 | Loss: 0.00003837
Iteration 145/1000 | Loss: 0.00003837
Iteration 146/1000 | Loss: 0.00003837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [3.8369562389561906e-05, 3.8369562389561906e-05, 3.8369562389561906e-05, 3.8369562389561906e-05, 3.8369562389561906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8369562389561906e-05

Optimization complete. Final v2v error: 4.514469623565674 mm

Highest mean error: 21.545610427856445 mm for frame 110

Lowest mean error: 3.7606823444366455 mm for frame 216

Saving results

Total time: 226.3524386882782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01144509
Iteration 2/25 | Loss: 0.00197512
Iteration 3/25 | Loss: 0.00192586
Iteration 4/25 | Loss: 0.00162877
Iteration 5/25 | Loss: 0.00154707
Iteration 6/25 | Loss: 0.00160495
Iteration 7/25 | Loss: 0.00154733
Iteration 8/25 | Loss: 0.00147499
Iteration 9/25 | Loss: 0.00143213
Iteration 10/25 | Loss: 0.00150007
Iteration 11/25 | Loss: 0.00140071
Iteration 12/25 | Loss: 0.00137803
Iteration 13/25 | Loss: 0.00138297
Iteration 14/25 | Loss: 0.00131301
Iteration 15/25 | Loss: 0.00130269
Iteration 16/25 | Loss: 0.00126854
Iteration 17/25 | Loss: 0.00126889
Iteration 18/25 | Loss: 0.00127303
Iteration 19/25 | Loss: 0.00126669
Iteration 20/25 | Loss: 0.00127407
Iteration 21/25 | Loss: 0.00126838
Iteration 22/25 | Loss: 0.00126758
Iteration 23/25 | Loss: 0.00126332
Iteration 24/25 | Loss: 0.00126596
Iteration 25/25 | Loss: 0.00126351

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54942000
Iteration 2/25 | Loss: 0.00263873
Iteration 3/25 | Loss: 0.00239594
Iteration 4/25 | Loss: 0.00239594
Iteration 5/25 | Loss: 0.00239594
Iteration 6/25 | Loss: 0.00239594
Iteration 7/25 | Loss: 0.00239594
Iteration 8/25 | Loss: 0.00239594
Iteration 9/25 | Loss: 0.00239594
Iteration 10/25 | Loss: 0.00239594
Iteration 11/25 | Loss: 0.00239594
Iteration 12/25 | Loss: 0.00239594
Iteration 13/25 | Loss: 0.00239594
Iteration 14/25 | Loss: 0.00239594
Iteration 15/25 | Loss: 0.00239594
Iteration 16/25 | Loss: 0.00239594
Iteration 17/25 | Loss: 0.00239594
Iteration 18/25 | Loss: 0.00239594
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002395941410213709, 0.002395941410213709, 0.002395941410213709, 0.002395941410213709, 0.002395941410213709]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002395941410213709

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00239594
Iteration 2/1000 | Loss: 0.00157763
Iteration 3/1000 | Loss: 0.00103693
Iteration 4/1000 | Loss: 0.00117203
Iteration 5/1000 | Loss: 0.00248541
Iteration 6/1000 | Loss: 0.00180789
Iteration 7/1000 | Loss: 0.00230913
Iteration 8/1000 | Loss: 0.00020651
Iteration 9/1000 | Loss: 0.00029542
Iteration 10/1000 | Loss: 0.00063284
Iteration 11/1000 | Loss: 0.00056159
Iteration 12/1000 | Loss: 0.00069261
Iteration 13/1000 | Loss: 0.00040554
Iteration 14/1000 | Loss: 0.00053245
Iteration 15/1000 | Loss: 0.00064539
Iteration 16/1000 | Loss: 0.00012082
Iteration 17/1000 | Loss: 0.00086264
Iteration 18/1000 | Loss: 0.00032585
Iteration 19/1000 | Loss: 0.00043456
Iteration 20/1000 | Loss: 0.00039913
Iteration 21/1000 | Loss: 0.00051329
Iteration 22/1000 | Loss: 0.00054739
Iteration 23/1000 | Loss: 0.00056418
Iteration 24/1000 | Loss: 0.00092383
Iteration 25/1000 | Loss: 0.00062559
Iteration 26/1000 | Loss: 0.00087716
Iteration 27/1000 | Loss: 0.00064259
Iteration 28/1000 | Loss: 0.00066550
Iteration 29/1000 | Loss: 0.00046196
Iteration 30/1000 | Loss: 0.00054679
Iteration 31/1000 | Loss: 0.00064739
Iteration 32/1000 | Loss: 0.00058733
Iteration 33/1000 | Loss: 0.00052142
Iteration 34/1000 | Loss: 0.00086350
Iteration 35/1000 | Loss: 0.00061429
Iteration 36/1000 | Loss: 0.00012263
Iteration 37/1000 | Loss: 0.00019215
Iteration 38/1000 | Loss: 0.00082605
Iteration 39/1000 | Loss: 0.00041763
Iteration 40/1000 | Loss: 0.00033003
Iteration 41/1000 | Loss: 0.00038804
Iteration 42/1000 | Loss: 0.00116031
Iteration 43/1000 | Loss: 0.00047366
Iteration 44/1000 | Loss: 0.00059661
Iteration 45/1000 | Loss: 0.00134178
Iteration 46/1000 | Loss: 0.00039481
Iteration 47/1000 | Loss: 0.00064027
Iteration 48/1000 | Loss: 0.00104592
Iteration 49/1000 | Loss: 0.00052307
Iteration 50/1000 | Loss: 0.00049915
Iteration 51/1000 | Loss: 0.00040124
Iteration 52/1000 | Loss: 0.00020035
Iteration 53/1000 | Loss: 0.00050090
Iteration 54/1000 | Loss: 0.00020764
Iteration 55/1000 | Loss: 0.00048760
Iteration 56/1000 | Loss: 0.00038886
Iteration 57/1000 | Loss: 0.00027521
Iteration 58/1000 | Loss: 0.00080981
Iteration 59/1000 | Loss: 0.00036750
Iteration 60/1000 | Loss: 0.00034330
Iteration 61/1000 | Loss: 0.00036801
Iteration 62/1000 | Loss: 0.00023644
Iteration 63/1000 | Loss: 0.00046543
Iteration 64/1000 | Loss: 0.00021090
Iteration 65/1000 | Loss: 0.00012615
Iteration 66/1000 | Loss: 0.00023348
Iteration 67/1000 | Loss: 0.00104500
Iteration 68/1000 | Loss: 0.00019395
Iteration 69/1000 | Loss: 0.00040464
Iteration 70/1000 | Loss: 0.00027784
Iteration 71/1000 | Loss: 0.00056392
Iteration 72/1000 | Loss: 0.00040643
Iteration 73/1000 | Loss: 0.00024393
Iteration 74/1000 | Loss: 0.00026905
Iteration 75/1000 | Loss: 0.00025049
Iteration 76/1000 | Loss: 0.00026683
Iteration 77/1000 | Loss: 0.00022617
Iteration 78/1000 | Loss: 0.00041059
Iteration 79/1000 | Loss: 0.00049540
Iteration 80/1000 | Loss: 0.00085858
Iteration 81/1000 | Loss: 0.00048258
Iteration 82/1000 | Loss: 0.00043867
Iteration 83/1000 | Loss: 0.00040988
Iteration 84/1000 | Loss: 0.00051339
Iteration 85/1000 | Loss: 0.00055189
Iteration 86/1000 | Loss: 0.00034494
Iteration 87/1000 | Loss: 0.00038494
Iteration 88/1000 | Loss: 0.00051669
Iteration 89/1000 | Loss: 0.00042350
Iteration 90/1000 | Loss: 0.00040842
Iteration 91/1000 | Loss: 0.00017341
Iteration 92/1000 | Loss: 0.00020054
Iteration 93/1000 | Loss: 0.00045751
Iteration 94/1000 | Loss: 0.00031562
Iteration 95/1000 | Loss: 0.00010755
Iteration 96/1000 | Loss: 0.00038950
Iteration 97/1000 | Loss: 0.00036364
Iteration 98/1000 | Loss: 0.00031082
Iteration 99/1000 | Loss: 0.00014260
Iteration 100/1000 | Loss: 0.00011813
Iteration 101/1000 | Loss: 0.00036921
Iteration 102/1000 | Loss: 0.00026384
Iteration 103/1000 | Loss: 0.00014325
Iteration 104/1000 | Loss: 0.00028569
Iteration 105/1000 | Loss: 0.00038043
Iteration 106/1000 | Loss: 0.00039114
Iteration 107/1000 | Loss: 0.00044764
Iteration 108/1000 | Loss: 0.00067816
Iteration 109/1000 | Loss: 0.00032743
Iteration 110/1000 | Loss: 0.00035128
Iteration 111/1000 | Loss: 0.00044004
Iteration 112/1000 | Loss: 0.00033813
Iteration 113/1000 | Loss: 0.00016907
Iteration 114/1000 | Loss: 0.00049977
Iteration 115/1000 | Loss: 0.00035395
Iteration 116/1000 | Loss: 0.00024912
Iteration 117/1000 | Loss: 0.00047906
Iteration 118/1000 | Loss: 0.00026579
Iteration 119/1000 | Loss: 0.00056995
Iteration 120/1000 | Loss: 0.00041742
Iteration 121/1000 | Loss: 0.00031706
Iteration 122/1000 | Loss: 0.00032988
Iteration 123/1000 | Loss: 0.00034030
Iteration 124/1000 | Loss: 0.00013193
Iteration 125/1000 | Loss: 0.00014626
Iteration 126/1000 | Loss: 0.00015901
Iteration 127/1000 | Loss: 0.00031931
Iteration 128/1000 | Loss: 0.00029737
Iteration 129/1000 | Loss: 0.00018587
Iteration 130/1000 | Loss: 0.00020172
Iteration 131/1000 | Loss: 0.00027976
Iteration 132/1000 | Loss: 0.00020117
Iteration 133/1000 | Loss: 0.00026340
Iteration 134/1000 | Loss: 0.00025553
Iteration 135/1000 | Loss: 0.00046124
Iteration 136/1000 | Loss: 0.00025380
Iteration 137/1000 | Loss: 0.00021347
Iteration 138/1000 | Loss: 0.00026419
Iteration 139/1000 | Loss: 0.00031683
Iteration 140/1000 | Loss: 0.00047252
Iteration 141/1000 | Loss: 0.00035453
Iteration 142/1000 | Loss: 0.00021414
Iteration 143/1000 | Loss: 0.00035363
Iteration 144/1000 | Loss: 0.00025307
Iteration 145/1000 | Loss: 0.00031543
Iteration 146/1000 | Loss: 0.00029688
Iteration 147/1000 | Loss: 0.00069574
Iteration 148/1000 | Loss: 0.00029355
Iteration 149/1000 | Loss: 0.00029453
Iteration 150/1000 | Loss: 0.00029502
Iteration 151/1000 | Loss: 0.00021224
Iteration 152/1000 | Loss: 0.00039485
Iteration 153/1000 | Loss: 0.00022966
Iteration 154/1000 | Loss: 0.00034096
Iteration 155/1000 | Loss: 0.00014594
Iteration 156/1000 | Loss: 0.00040631
Iteration 157/1000 | Loss: 0.00040115
Iteration 158/1000 | Loss: 0.00089085
Iteration 159/1000 | Loss: 0.00060123
Iteration 160/1000 | Loss: 0.00067577
Iteration 161/1000 | Loss: 0.00118552
Iteration 162/1000 | Loss: 0.00034510
Iteration 163/1000 | Loss: 0.00025918
Iteration 164/1000 | Loss: 0.00096452
Iteration 165/1000 | Loss: 0.00006017
Iteration 166/1000 | Loss: 0.00016752
Iteration 167/1000 | Loss: 0.00021826
Iteration 168/1000 | Loss: 0.00019102
Iteration 169/1000 | Loss: 0.00004300
Iteration 170/1000 | Loss: 0.00006129
Iteration 171/1000 | Loss: 0.00003806
Iteration 172/1000 | Loss: 0.00004691
Iteration 173/1000 | Loss: 0.00003134
Iteration 174/1000 | Loss: 0.00003025
Iteration 175/1000 | Loss: 0.00003933
Iteration 176/1000 | Loss: 0.00003424
Iteration 177/1000 | Loss: 0.00002972
Iteration 178/1000 | Loss: 0.00002852
Iteration 179/1000 | Loss: 0.00002805
Iteration 180/1000 | Loss: 0.00002793
Iteration 181/1000 | Loss: 0.00002775
Iteration 182/1000 | Loss: 0.00002759
Iteration 183/1000 | Loss: 0.00002744
Iteration 184/1000 | Loss: 0.00002742
Iteration 185/1000 | Loss: 0.00002741
Iteration 186/1000 | Loss: 0.00002740
Iteration 187/1000 | Loss: 0.00002740
Iteration 188/1000 | Loss: 0.00002739
Iteration 189/1000 | Loss: 0.00002738
Iteration 190/1000 | Loss: 0.00002738
Iteration 191/1000 | Loss: 0.00002731
Iteration 192/1000 | Loss: 0.00002731
Iteration 193/1000 | Loss: 0.00002731
Iteration 194/1000 | Loss: 0.00002730
Iteration 195/1000 | Loss: 0.00002730
Iteration 196/1000 | Loss: 0.00002730
Iteration 197/1000 | Loss: 0.00002729
Iteration 198/1000 | Loss: 0.00002729
Iteration 199/1000 | Loss: 0.00002728
Iteration 200/1000 | Loss: 0.00002728
Iteration 201/1000 | Loss: 0.00002727
Iteration 202/1000 | Loss: 0.00002727
Iteration 203/1000 | Loss: 0.00002727
Iteration 204/1000 | Loss: 0.00002727
Iteration 205/1000 | Loss: 0.00002727
Iteration 206/1000 | Loss: 0.00002727
Iteration 207/1000 | Loss: 0.00002727
Iteration 208/1000 | Loss: 0.00002727
Iteration 209/1000 | Loss: 0.00002726
Iteration 210/1000 | Loss: 0.00002726
Iteration 211/1000 | Loss: 0.00002726
Iteration 212/1000 | Loss: 0.00002726
Iteration 213/1000 | Loss: 0.00002726
Iteration 214/1000 | Loss: 0.00002726
Iteration 215/1000 | Loss: 0.00002726
Iteration 216/1000 | Loss: 0.00002726
Iteration 217/1000 | Loss: 0.00002726
Iteration 218/1000 | Loss: 0.00002726
Iteration 219/1000 | Loss: 0.00002726
Iteration 220/1000 | Loss: 0.00002726
Iteration 221/1000 | Loss: 0.00002726
Iteration 222/1000 | Loss: 0.00002726
Iteration 223/1000 | Loss: 0.00002726
Iteration 224/1000 | Loss: 0.00002726
Iteration 225/1000 | Loss: 0.00002726
Iteration 226/1000 | Loss: 0.00002726
Iteration 227/1000 | Loss: 0.00002726
Iteration 228/1000 | Loss: 0.00002726
Iteration 229/1000 | Loss: 0.00002726
Iteration 230/1000 | Loss: 0.00002726
Iteration 231/1000 | Loss: 0.00002726
Iteration 232/1000 | Loss: 0.00002726
Iteration 233/1000 | Loss: 0.00002726
Iteration 234/1000 | Loss: 0.00002726
Iteration 235/1000 | Loss: 0.00002726
Iteration 236/1000 | Loss: 0.00002726
Iteration 237/1000 | Loss: 0.00002726
Iteration 238/1000 | Loss: 0.00002726
Iteration 239/1000 | Loss: 0.00002726
Iteration 240/1000 | Loss: 0.00002726
Iteration 241/1000 | Loss: 0.00002726
Iteration 242/1000 | Loss: 0.00002726
Iteration 243/1000 | Loss: 0.00002726
Iteration 244/1000 | Loss: 0.00002726
Iteration 245/1000 | Loss: 0.00002726
Iteration 246/1000 | Loss: 0.00002726
Iteration 247/1000 | Loss: 0.00002726
Iteration 248/1000 | Loss: 0.00002726
Iteration 249/1000 | Loss: 0.00002726
Iteration 250/1000 | Loss: 0.00002726
Iteration 251/1000 | Loss: 0.00002726
Iteration 252/1000 | Loss: 0.00002726
Iteration 253/1000 | Loss: 0.00002726
Iteration 254/1000 | Loss: 0.00002726
Iteration 255/1000 | Loss: 0.00002726
Iteration 256/1000 | Loss: 0.00002726
Iteration 257/1000 | Loss: 0.00002726
Iteration 258/1000 | Loss: 0.00002726
Iteration 259/1000 | Loss: 0.00002726
Iteration 260/1000 | Loss: 0.00002726
Iteration 261/1000 | Loss: 0.00002726
Iteration 262/1000 | Loss: 0.00002726
Iteration 263/1000 | Loss: 0.00002726
Iteration 264/1000 | Loss: 0.00002726
Iteration 265/1000 | Loss: 0.00002726
Iteration 266/1000 | Loss: 0.00002726
Iteration 267/1000 | Loss: 0.00002726
Iteration 268/1000 | Loss: 0.00002726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [2.7260888600721955e-05, 2.7260888600721955e-05, 2.7260888600721955e-05, 2.7260888600721955e-05, 2.7260888600721955e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7260888600721955e-05

Optimization complete. Final v2v error: 4.485025405883789 mm

Highest mean error: 10.985913276672363 mm for frame 87

Lowest mean error: 4.030026912689209 mm for frame 28

Saving results

Total time: 304.34724950790405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00913983
Iteration 2/25 | Loss: 0.00140638
Iteration 3/25 | Loss: 0.00131025
Iteration 4/25 | Loss: 0.00130203
Iteration 5/25 | Loss: 0.00129905
Iteration 6/25 | Loss: 0.00129827
Iteration 7/25 | Loss: 0.00129827
Iteration 8/25 | Loss: 0.00129827
Iteration 9/25 | Loss: 0.00129827
Iteration 10/25 | Loss: 0.00129827
Iteration 11/25 | Loss: 0.00129827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012982707703486085, 0.0012982707703486085, 0.0012982707703486085, 0.0012982707703486085, 0.0012982707703486085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012982707703486085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49665284
Iteration 2/25 | Loss: 0.00142055
Iteration 3/25 | Loss: 0.00142055
Iteration 4/25 | Loss: 0.00142055
Iteration 5/25 | Loss: 0.00142055
Iteration 6/25 | Loss: 0.00142055
Iteration 7/25 | Loss: 0.00142055
Iteration 8/25 | Loss: 0.00142055
Iteration 9/25 | Loss: 0.00142055
Iteration 10/25 | Loss: 0.00142055
Iteration 11/25 | Loss: 0.00142055
Iteration 12/25 | Loss: 0.00142055
Iteration 13/25 | Loss: 0.00142055
Iteration 14/25 | Loss: 0.00142055
Iteration 15/25 | Loss: 0.00142055
Iteration 16/25 | Loss: 0.00142055
Iteration 17/25 | Loss: 0.00142055
Iteration 18/25 | Loss: 0.00142055
Iteration 19/25 | Loss: 0.00142055
Iteration 20/25 | Loss: 0.00142055
Iteration 21/25 | Loss: 0.00142055
Iteration 22/25 | Loss: 0.00142055
Iteration 23/25 | Loss: 0.00142055
Iteration 24/25 | Loss: 0.00142055
Iteration 25/25 | Loss: 0.00142055

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142055
Iteration 2/1000 | Loss: 0.00004078
Iteration 3/1000 | Loss: 0.00002817
Iteration 4/1000 | Loss: 0.00002487
Iteration 5/1000 | Loss: 0.00002332
Iteration 6/1000 | Loss: 0.00002237
Iteration 7/1000 | Loss: 0.00002183
Iteration 8/1000 | Loss: 0.00002178
Iteration 9/1000 | Loss: 0.00002155
Iteration 10/1000 | Loss: 0.00002134
Iteration 11/1000 | Loss: 0.00002134
Iteration 12/1000 | Loss: 0.00002119
Iteration 13/1000 | Loss: 0.00002114
Iteration 14/1000 | Loss: 0.00002113
Iteration 15/1000 | Loss: 0.00002112
Iteration 16/1000 | Loss: 0.00002112
Iteration 17/1000 | Loss: 0.00002106
Iteration 18/1000 | Loss: 0.00002101
Iteration 19/1000 | Loss: 0.00002096
Iteration 20/1000 | Loss: 0.00002096
Iteration 21/1000 | Loss: 0.00002093
Iteration 22/1000 | Loss: 0.00002090
Iteration 23/1000 | Loss: 0.00002090
Iteration 24/1000 | Loss: 0.00002088
Iteration 25/1000 | Loss: 0.00002088
Iteration 26/1000 | Loss: 0.00002088
Iteration 27/1000 | Loss: 0.00002087
Iteration 28/1000 | Loss: 0.00002087
Iteration 29/1000 | Loss: 0.00002086
Iteration 30/1000 | Loss: 0.00002086
Iteration 31/1000 | Loss: 0.00002086
Iteration 32/1000 | Loss: 0.00002085
Iteration 33/1000 | Loss: 0.00002085
Iteration 34/1000 | Loss: 0.00002085
Iteration 35/1000 | Loss: 0.00002084
Iteration 36/1000 | Loss: 0.00002084
Iteration 37/1000 | Loss: 0.00002084
Iteration 38/1000 | Loss: 0.00002084
Iteration 39/1000 | Loss: 0.00002084
Iteration 40/1000 | Loss: 0.00002084
Iteration 41/1000 | Loss: 0.00002084
Iteration 42/1000 | Loss: 0.00002083
Iteration 43/1000 | Loss: 0.00002083
Iteration 44/1000 | Loss: 0.00002083
Iteration 45/1000 | Loss: 0.00002083
Iteration 46/1000 | Loss: 0.00002083
Iteration 47/1000 | Loss: 0.00002083
Iteration 48/1000 | Loss: 0.00002083
Iteration 49/1000 | Loss: 0.00002082
Iteration 50/1000 | Loss: 0.00002082
Iteration 51/1000 | Loss: 0.00002082
Iteration 52/1000 | Loss: 0.00002081
Iteration 53/1000 | Loss: 0.00002081
Iteration 54/1000 | Loss: 0.00002081
Iteration 55/1000 | Loss: 0.00002081
Iteration 56/1000 | Loss: 0.00002081
Iteration 57/1000 | Loss: 0.00002081
Iteration 58/1000 | Loss: 0.00002081
Iteration 59/1000 | Loss: 0.00002081
Iteration 60/1000 | Loss: 0.00002081
Iteration 61/1000 | Loss: 0.00002081
Iteration 62/1000 | Loss: 0.00002081
Iteration 63/1000 | Loss: 0.00002081
Iteration 64/1000 | Loss: 0.00002081
Iteration 65/1000 | Loss: 0.00002080
Iteration 66/1000 | Loss: 0.00002080
Iteration 67/1000 | Loss: 0.00002080
Iteration 68/1000 | Loss: 0.00002080
Iteration 69/1000 | Loss: 0.00002080
Iteration 70/1000 | Loss: 0.00002080
Iteration 71/1000 | Loss: 0.00002080
Iteration 72/1000 | Loss: 0.00002080
Iteration 73/1000 | Loss: 0.00002080
Iteration 74/1000 | Loss: 0.00002079
Iteration 75/1000 | Loss: 0.00002079
Iteration 76/1000 | Loss: 0.00002079
Iteration 77/1000 | Loss: 0.00002079
Iteration 78/1000 | Loss: 0.00002079
Iteration 79/1000 | Loss: 0.00002079
Iteration 80/1000 | Loss: 0.00002079
Iteration 81/1000 | Loss: 0.00002079
Iteration 82/1000 | Loss: 0.00002079
Iteration 83/1000 | Loss: 0.00002078
Iteration 84/1000 | Loss: 0.00002078
Iteration 85/1000 | Loss: 0.00002078
Iteration 86/1000 | Loss: 0.00002078
Iteration 87/1000 | Loss: 0.00002078
Iteration 88/1000 | Loss: 0.00002078
Iteration 89/1000 | Loss: 0.00002078
Iteration 90/1000 | Loss: 0.00002078
Iteration 91/1000 | Loss: 0.00002078
Iteration 92/1000 | Loss: 0.00002078
Iteration 93/1000 | Loss: 0.00002078
Iteration 94/1000 | Loss: 0.00002078
Iteration 95/1000 | Loss: 0.00002077
Iteration 96/1000 | Loss: 0.00002077
Iteration 97/1000 | Loss: 0.00002077
Iteration 98/1000 | Loss: 0.00002077
Iteration 99/1000 | Loss: 0.00002077
Iteration 100/1000 | Loss: 0.00002077
Iteration 101/1000 | Loss: 0.00002077
Iteration 102/1000 | Loss: 0.00002077
Iteration 103/1000 | Loss: 0.00002077
Iteration 104/1000 | Loss: 0.00002077
Iteration 105/1000 | Loss: 0.00002077
Iteration 106/1000 | Loss: 0.00002076
Iteration 107/1000 | Loss: 0.00002076
Iteration 108/1000 | Loss: 0.00002076
Iteration 109/1000 | Loss: 0.00002076
Iteration 110/1000 | Loss: 0.00002076
Iteration 111/1000 | Loss: 0.00002076
Iteration 112/1000 | Loss: 0.00002076
Iteration 113/1000 | Loss: 0.00002076
Iteration 114/1000 | Loss: 0.00002076
Iteration 115/1000 | Loss: 0.00002076
Iteration 116/1000 | Loss: 0.00002076
Iteration 117/1000 | Loss: 0.00002076
Iteration 118/1000 | Loss: 0.00002076
Iteration 119/1000 | Loss: 0.00002076
Iteration 120/1000 | Loss: 0.00002075
Iteration 121/1000 | Loss: 0.00002075
Iteration 122/1000 | Loss: 0.00002075
Iteration 123/1000 | Loss: 0.00002075
Iteration 124/1000 | Loss: 0.00002075
Iteration 125/1000 | Loss: 0.00002075
Iteration 126/1000 | Loss: 0.00002075
Iteration 127/1000 | Loss: 0.00002075
Iteration 128/1000 | Loss: 0.00002075
Iteration 129/1000 | Loss: 0.00002075
Iteration 130/1000 | Loss: 0.00002075
Iteration 131/1000 | Loss: 0.00002075
Iteration 132/1000 | Loss: 0.00002075
Iteration 133/1000 | Loss: 0.00002075
Iteration 134/1000 | Loss: 0.00002075
Iteration 135/1000 | Loss: 0.00002075
Iteration 136/1000 | Loss: 0.00002075
Iteration 137/1000 | Loss: 0.00002075
Iteration 138/1000 | Loss: 0.00002075
Iteration 139/1000 | Loss: 0.00002075
Iteration 140/1000 | Loss: 0.00002075
Iteration 141/1000 | Loss: 0.00002075
Iteration 142/1000 | Loss: 0.00002075
Iteration 143/1000 | Loss: 0.00002075
Iteration 144/1000 | Loss: 0.00002075
Iteration 145/1000 | Loss: 0.00002075
Iteration 146/1000 | Loss: 0.00002075
Iteration 147/1000 | Loss: 0.00002075
Iteration 148/1000 | Loss: 0.00002075
Iteration 149/1000 | Loss: 0.00002075
Iteration 150/1000 | Loss: 0.00002075
Iteration 151/1000 | Loss: 0.00002075
Iteration 152/1000 | Loss: 0.00002075
Iteration 153/1000 | Loss: 0.00002075
Iteration 154/1000 | Loss: 0.00002075
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.0750134353875183e-05, 2.0750134353875183e-05, 2.0750134353875183e-05, 2.0750134353875183e-05, 2.0750134353875183e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0750134353875183e-05

Optimization complete. Final v2v error: 3.939087390899658 mm

Highest mean error: 4.1076788902282715 mm for frame 57

Lowest mean error: 3.8225865364074707 mm for frame 116

Saving results

Total time: 33.05359721183777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109194
Iteration 2/25 | Loss: 0.00282514
Iteration 3/25 | Loss: 0.00198535
Iteration 4/25 | Loss: 0.00189607
Iteration 5/25 | Loss: 0.00191128
Iteration 6/25 | Loss: 0.00189403
Iteration 7/25 | Loss: 0.00187556
Iteration 8/25 | Loss: 0.00184628
Iteration 9/25 | Loss: 0.00180800
Iteration 10/25 | Loss: 0.00176390
Iteration 11/25 | Loss: 0.00174826
Iteration 12/25 | Loss: 0.00174174
Iteration 13/25 | Loss: 0.00174094
Iteration 14/25 | Loss: 0.00173802
Iteration 15/25 | Loss: 0.00173758
Iteration 16/25 | Loss: 0.00173258
Iteration 17/25 | Loss: 0.00173869
Iteration 18/25 | Loss: 0.00174936
Iteration 19/25 | Loss: 0.00174460
Iteration 20/25 | Loss: 0.00175314
Iteration 21/25 | Loss: 0.00175686
Iteration 22/25 | Loss: 0.00174125
Iteration 23/25 | Loss: 0.00173352
Iteration 24/25 | Loss: 0.00172943
Iteration 25/25 | Loss: 0.00172812

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53090978
Iteration 2/25 | Loss: 0.00516991
Iteration 3/25 | Loss: 0.00502328
Iteration 4/25 | Loss: 0.00502328
Iteration 5/25 | Loss: 0.00502328
Iteration 6/25 | Loss: 0.00502328
Iteration 7/25 | Loss: 0.00502328
Iteration 8/25 | Loss: 0.00502328
Iteration 9/25 | Loss: 0.00502328
Iteration 10/25 | Loss: 0.00502328
Iteration 11/25 | Loss: 0.00502328
Iteration 12/25 | Loss: 0.00502328
Iteration 13/25 | Loss: 0.00502328
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.005023278295993805, 0.005023278295993805, 0.005023278295993805, 0.005023278295993805, 0.005023278295993805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005023278295993805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00502328
Iteration 2/1000 | Loss: 0.00149016
Iteration 3/1000 | Loss: 0.00097381
Iteration 4/1000 | Loss: 0.00118987
Iteration 5/1000 | Loss: 0.00192739
Iteration 6/1000 | Loss: 0.00188717
Iteration 7/1000 | Loss: 0.00090812
Iteration 8/1000 | Loss: 0.00115930
Iteration 9/1000 | Loss: 0.00074026
Iteration 10/1000 | Loss: 0.00081815
Iteration 11/1000 | Loss: 0.00082620
Iteration 12/1000 | Loss: 0.00134595
Iteration 13/1000 | Loss: 0.00064200
Iteration 14/1000 | Loss: 0.00055491
Iteration 15/1000 | Loss: 0.00103861
Iteration 16/1000 | Loss: 0.00105825
Iteration 17/1000 | Loss: 0.00140236
Iteration 18/1000 | Loss: 0.00080565
Iteration 19/1000 | Loss: 0.00033167
Iteration 20/1000 | Loss: 0.00030690
Iteration 21/1000 | Loss: 0.00125011
Iteration 22/1000 | Loss: 0.00037694
Iteration 23/1000 | Loss: 0.00028622
Iteration 24/1000 | Loss: 0.00138840
Iteration 25/1000 | Loss: 0.00034320
Iteration 26/1000 | Loss: 0.00127395
Iteration 27/1000 | Loss: 0.00184221
Iteration 28/1000 | Loss: 0.00085576
Iteration 29/1000 | Loss: 0.00072366
Iteration 30/1000 | Loss: 0.00045414
Iteration 31/1000 | Loss: 0.00054682
Iteration 32/1000 | Loss: 0.00129079
Iteration 33/1000 | Loss: 0.00029893
Iteration 34/1000 | Loss: 0.00025782
Iteration 35/1000 | Loss: 0.00050290
Iteration 36/1000 | Loss: 0.00169429
Iteration 37/1000 | Loss: 0.00027137
Iteration 38/1000 | Loss: 0.00050342
Iteration 39/1000 | Loss: 0.00038733
Iteration 40/1000 | Loss: 0.00049216
Iteration 41/1000 | Loss: 0.00038084
Iteration 42/1000 | Loss: 0.00058607
Iteration 43/1000 | Loss: 0.00131621
Iteration 44/1000 | Loss: 0.00059250
Iteration 45/1000 | Loss: 0.00176390
Iteration 46/1000 | Loss: 0.00103454
Iteration 47/1000 | Loss: 0.00026295
Iteration 48/1000 | Loss: 0.00024232
Iteration 49/1000 | Loss: 0.00050175
Iteration 50/1000 | Loss: 0.00079923
Iteration 51/1000 | Loss: 0.00102791
Iteration 52/1000 | Loss: 0.00024911
Iteration 53/1000 | Loss: 0.00078864
Iteration 54/1000 | Loss: 0.00084148
Iteration 55/1000 | Loss: 0.00069664
Iteration 56/1000 | Loss: 0.00055733
Iteration 57/1000 | Loss: 0.00118706
Iteration 58/1000 | Loss: 0.00026364
Iteration 59/1000 | Loss: 0.00061855
Iteration 60/1000 | Loss: 0.00026587
Iteration 61/1000 | Loss: 0.00098304
Iteration 62/1000 | Loss: 0.00066400
Iteration 63/1000 | Loss: 0.00038479
Iteration 64/1000 | Loss: 0.00061410
Iteration 65/1000 | Loss: 0.00124520
Iteration 66/1000 | Loss: 0.00122081
Iteration 67/1000 | Loss: 0.00103072
Iteration 68/1000 | Loss: 0.00025560
Iteration 69/1000 | Loss: 0.00065877
Iteration 70/1000 | Loss: 0.00044927
Iteration 71/1000 | Loss: 0.00089030
Iteration 72/1000 | Loss: 0.00118249
Iteration 73/1000 | Loss: 0.00081588
Iteration 74/1000 | Loss: 0.00108043
Iteration 75/1000 | Loss: 0.00086036
Iteration 76/1000 | Loss: 0.00025383
Iteration 77/1000 | Loss: 0.00022261
Iteration 78/1000 | Loss: 0.00022492
Iteration 79/1000 | Loss: 0.00023773
Iteration 80/1000 | Loss: 0.00022854
Iteration 81/1000 | Loss: 0.00023533
Iteration 82/1000 | Loss: 0.00023557
Iteration 83/1000 | Loss: 0.00022786
Iteration 84/1000 | Loss: 0.00024126
Iteration 85/1000 | Loss: 0.00023616
Iteration 86/1000 | Loss: 0.00022241
Iteration 87/1000 | Loss: 0.00022285
Iteration 88/1000 | Loss: 0.00065785
Iteration 89/1000 | Loss: 0.00023698
Iteration 90/1000 | Loss: 0.00045686
Iteration 91/1000 | Loss: 0.00047813
Iteration 92/1000 | Loss: 0.00033791
Iteration 93/1000 | Loss: 0.00023318
Iteration 94/1000 | Loss: 0.00131497
Iteration 95/1000 | Loss: 0.00107926
Iteration 96/1000 | Loss: 0.00078476
Iteration 97/1000 | Loss: 0.00109173
Iteration 98/1000 | Loss: 0.00093490
Iteration 99/1000 | Loss: 0.00092768
Iteration 100/1000 | Loss: 0.00056314
Iteration 101/1000 | Loss: 0.00090858
Iteration 102/1000 | Loss: 0.00033686
Iteration 103/1000 | Loss: 0.00023725
Iteration 104/1000 | Loss: 0.00023526
Iteration 105/1000 | Loss: 0.00022087
Iteration 106/1000 | Loss: 0.00020993
Iteration 107/1000 | Loss: 0.00039417
Iteration 108/1000 | Loss: 0.00022845
Iteration 109/1000 | Loss: 0.00021714
Iteration 110/1000 | Loss: 0.00024194
Iteration 111/1000 | Loss: 0.00024914
Iteration 112/1000 | Loss: 0.00023637
Iteration 113/1000 | Loss: 0.00025246
Iteration 114/1000 | Loss: 0.00023062
Iteration 115/1000 | Loss: 0.00054142
Iteration 116/1000 | Loss: 0.00049412
Iteration 117/1000 | Loss: 0.00053683
Iteration 118/1000 | Loss: 0.00049192
Iteration 119/1000 | Loss: 0.00046142
Iteration 120/1000 | Loss: 0.00023148
Iteration 121/1000 | Loss: 0.00042978
Iteration 122/1000 | Loss: 0.00051443
Iteration 123/1000 | Loss: 0.00030857
Iteration 124/1000 | Loss: 0.00053392
Iteration 125/1000 | Loss: 0.00054915
Iteration 126/1000 | Loss: 0.00051929
Iteration 127/1000 | Loss: 0.00053536
Iteration 128/1000 | Loss: 0.00049319
Iteration 129/1000 | Loss: 0.00045655
Iteration 130/1000 | Loss: 0.00043235
Iteration 131/1000 | Loss: 0.00046629
Iteration 132/1000 | Loss: 0.00045196
Iteration 133/1000 | Loss: 0.00046062
Iteration 134/1000 | Loss: 0.00022328
Iteration 135/1000 | Loss: 0.00021429
Iteration 136/1000 | Loss: 0.00021039
Iteration 137/1000 | Loss: 0.00020810
Iteration 138/1000 | Loss: 0.00023903
Iteration 139/1000 | Loss: 0.00023439
Iteration 140/1000 | Loss: 0.00023804
Iteration 141/1000 | Loss: 0.00058422
Iteration 142/1000 | Loss: 0.00026372
Iteration 143/1000 | Loss: 0.00024911
Iteration 144/1000 | Loss: 0.00024293
Iteration 145/1000 | Loss: 0.00020935
Iteration 146/1000 | Loss: 0.00020533
Iteration 147/1000 | Loss: 0.00020250
Iteration 148/1000 | Loss: 0.00020071
Iteration 149/1000 | Loss: 0.00019990
Iteration 150/1000 | Loss: 0.00019940
Iteration 151/1000 | Loss: 0.00019878
Iteration 152/1000 | Loss: 0.00019828
Iteration 153/1000 | Loss: 0.00021113
Iteration 154/1000 | Loss: 0.00020004
Iteration 155/1000 | Loss: 0.00021356
Iteration 156/1000 | Loss: 0.00019820
Iteration 157/1000 | Loss: 0.00019794
Iteration 158/1000 | Loss: 0.00019740
Iteration 159/1000 | Loss: 0.00019692
Iteration 160/1000 | Loss: 0.00019673
Iteration 161/1000 | Loss: 0.00019655
Iteration 162/1000 | Loss: 0.00019654
Iteration 163/1000 | Loss: 0.00019654
Iteration 164/1000 | Loss: 0.00019654
Iteration 165/1000 | Loss: 0.00019654
Iteration 166/1000 | Loss: 0.00019654
Iteration 167/1000 | Loss: 0.00019654
Iteration 168/1000 | Loss: 0.00019654
Iteration 169/1000 | Loss: 0.00019654
Iteration 170/1000 | Loss: 0.00019654
Iteration 171/1000 | Loss: 0.00019654
Iteration 172/1000 | Loss: 0.00019653
Iteration 173/1000 | Loss: 0.00019653
Iteration 174/1000 | Loss: 0.00019653
Iteration 175/1000 | Loss: 0.00019653
Iteration 176/1000 | Loss: 0.00019652
Iteration 177/1000 | Loss: 0.00019652
Iteration 178/1000 | Loss: 0.00019652
Iteration 179/1000 | Loss: 0.00019651
Iteration 180/1000 | Loss: 0.00019651
Iteration 181/1000 | Loss: 0.00019651
Iteration 182/1000 | Loss: 0.00019651
Iteration 183/1000 | Loss: 0.00019651
Iteration 184/1000 | Loss: 0.00019650
Iteration 185/1000 | Loss: 0.00019650
Iteration 186/1000 | Loss: 0.00019650
Iteration 187/1000 | Loss: 0.00019649
Iteration 188/1000 | Loss: 0.00019649
Iteration 189/1000 | Loss: 0.00019648
Iteration 190/1000 | Loss: 0.00019648
Iteration 191/1000 | Loss: 0.00019648
Iteration 192/1000 | Loss: 0.00019648
Iteration 193/1000 | Loss: 0.00019648
Iteration 194/1000 | Loss: 0.00019647
Iteration 195/1000 | Loss: 0.00019647
Iteration 196/1000 | Loss: 0.00019647
Iteration 197/1000 | Loss: 0.00019647
Iteration 198/1000 | Loss: 0.00019647
Iteration 199/1000 | Loss: 0.00019647
Iteration 200/1000 | Loss: 0.00019647
Iteration 201/1000 | Loss: 0.00019646
Iteration 202/1000 | Loss: 0.00019646
Iteration 203/1000 | Loss: 0.00019646
Iteration 204/1000 | Loss: 0.00019646
Iteration 205/1000 | Loss: 0.00019646
Iteration 206/1000 | Loss: 0.00019646
Iteration 207/1000 | Loss: 0.00019646
Iteration 208/1000 | Loss: 0.00019646
Iteration 209/1000 | Loss: 0.00019646
Iteration 210/1000 | Loss: 0.00019646
Iteration 211/1000 | Loss: 0.00019645
Iteration 212/1000 | Loss: 0.00019645
Iteration 213/1000 | Loss: 0.00019645
Iteration 214/1000 | Loss: 0.00019645
Iteration 215/1000 | Loss: 0.00019645
Iteration 216/1000 | Loss: 0.00019645
Iteration 217/1000 | Loss: 0.00019644
Iteration 218/1000 | Loss: 0.00019644
Iteration 219/1000 | Loss: 0.00019644
Iteration 220/1000 | Loss: 0.00019644
Iteration 221/1000 | Loss: 0.00019644
Iteration 222/1000 | Loss: 0.00019643
Iteration 223/1000 | Loss: 0.00019643
Iteration 224/1000 | Loss: 0.00019643
Iteration 225/1000 | Loss: 0.00019643
Iteration 226/1000 | Loss: 0.00019643
Iteration 227/1000 | Loss: 0.00019643
Iteration 228/1000 | Loss: 0.00019643
Iteration 229/1000 | Loss: 0.00019642
Iteration 230/1000 | Loss: 0.00019642
Iteration 231/1000 | Loss: 0.00019642
Iteration 232/1000 | Loss: 0.00019642
Iteration 233/1000 | Loss: 0.00019642
Iteration 234/1000 | Loss: 0.00019642
Iteration 235/1000 | Loss: 0.00019642
Iteration 236/1000 | Loss: 0.00019641
Iteration 237/1000 | Loss: 0.00019641
Iteration 238/1000 | Loss: 0.00019641
Iteration 239/1000 | Loss: 0.00019641
Iteration 240/1000 | Loss: 0.00019641
Iteration 241/1000 | Loss: 0.00019641
Iteration 242/1000 | Loss: 0.00019641
Iteration 243/1000 | Loss: 0.00019641
Iteration 244/1000 | Loss: 0.00019641
Iteration 245/1000 | Loss: 0.00019641
Iteration 246/1000 | Loss: 0.00019641
Iteration 247/1000 | Loss: 0.00019641
Iteration 248/1000 | Loss: 0.00019641
Iteration 249/1000 | Loss: 0.00019640
Iteration 250/1000 | Loss: 0.00019640
Iteration 251/1000 | Loss: 0.00019640
Iteration 252/1000 | Loss: 0.00019640
Iteration 253/1000 | Loss: 0.00019640
Iteration 254/1000 | Loss: 0.00019640
Iteration 255/1000 | Loss: 0.00019640
Iteration 256/1000 | Loss: 0.00019640
Iteration 257/1000 | Loss: 0.00019640
Iteration 258/1000 | Loss: 0.00019639
Iteration 259/1000 | Loss: 0.00019639
Iteration 260/1000 | Loss: 0.00019639
Iteration 261/1000 | Loss: 0.00019639
Iteration 262/1000 | Loss: 0.00019639
Iteration 263/1000 | Loss: 0.00019639
Iteration 264/1000 | Loss: 0.00019639
Iteration 265/1000 | Loss: 0.00019639
Iteration 266/1000 | Loss: 0.00019639
Iteration 267/1000 | Loss: 0.00019639
Iteration 268/1000 | Loss: 0.00019639
Iteration 269/1000 | Loss: 0.00019638
Iteration 270/1000 | Loss: 0.00019638
Iteration 271/1000 | Loss: 0.00019638
Iteration 272/1000 | Loss: 0.00019638
Iteration 273/1000 | Loss: 0.00019638
Iteration 274/1000 | Loss: 0.00019638
Iteration 275/1000 | Loss: 0.00019637
Iteration 276/1000 | Loss: 0.00019637
Iteration 277/1000 | Loss: 0.00019637
Iteration 278/1000 | Loss: 0.00019637
Iteration 279/1000 | Loss: 0.00019637
Iteration 280/1000 | Loss: 0.00019637
Iteration 281/1000 | Loss: 0.00019637
Iteration 282/1000 | Loss: 0.00019636
Iteration 283/1000 | Loss: 0.00019636
Iteration 284/1000 | Loss: 0.00019636
Iteration 285/1000 | Loss: 0.00019636
Iteration 286/1000 | Loss: 0.00019636
Iteration 287/1000 | Loss: 0.00019636
Iteration 288/1000 | Loss: 0.00019636
Iteration 289/1000 | Loss: 0.00019636
Iteration 290/1000 | Loss: 0.00019636
Iteration 291/1000 | Loss: 0.00019635
Iteration 292/1000 | Loss: 0.00019635
Iteration 293/1000 | Loss: 0.00019635
Iteration 294/1000 | Loss: 0.00019635
Iteration 295/1000 | Loss: 0.00019635
Iteration 296/1000 | Loss: 0.00019635
Iteration 297/1000 | Loss: 0.00019635
Iteration 298/1000 | Loss: 0.00019635
Iteration 299/1000 | Loss: 0.00019635
Iteration 300/1000 | Loss: 0.00019635
Iteration 301/1000 | Loss: 0.00019635
Iteration 302/1000 | Loss: 0.00019635
Iteration 303/1000 | Loss: 0.00019635
Iteration 304/1000 | Loss: 0.00019635
Iteration 305/1000 | Loss: 0.00019634
Iteration 306/1000 | Loss: 0.00019634
Iteration 307/1000 | Loss: 0.00019634
Iteration 308/1000 | Loss: 0.00019634
Iteration 309/1000 | Loss: 0.00019634
Iteration 310/1000 | Loss: 0.00019634
Iteration 311/1000 | Loss: 0.00019634
Iteration 312/1000 | Loss: 0.00019634
Iteration 313/1000 | Loss: 0.00019634
Iteration 314/1000 | Loss: 0.00019634
Iteration 315/1000 | Loss: 0.00019634
Iteration 316/1000 | Loss: 0.00019634
Iteration 317/1000 | Loss: 0.00019633
Iteration 318/1000 | Loss: 0.00019633
Iteration 319/1000 | Loss: 0.00019633
Iteration 320/1000 | Loss: 0.00019633
Iteration 321/1000 | Loss: 0.00019633
Iteration 322/1000 | Loss: 0.00019633
Iteration 323/1000 | Loss: 0.00019633
Iteration 324/1000 | Loss: 0.00019633
Iteration 325/1000 | Loss: 0.00019633
Iteration 326/1000 | Loss: 0.00019633
Iteration 327/1000 | Loss: 0.00019633
Iteration 328/1000 | Loss: 0.00019632
Iteration 329/1000 | Loss: 0.00019632
Iteration 330/1000 | Loss: 0.00019632
Iteration 331/1000 | Loss: 0.00019632
Iteration 332/1000 | Loss: 0.00019632
Iteration 333/1000 | Loss: 0.00019632
Iteration 334/1000 | Loss: 0.00019632
Iteration 335/1000 | Loss: 0.00019632
Iteration 336/1000 | Loss: 0.00019632
Iteration 337/1000 | Loss: 0.00019632
Iteration 338/1000 | Loss: 0.00019632
Iteration 339/1000 | Loss: 0.00019632
Iteration 340/1000 | Loss: 0.00019632
Iteration 341/1000 | Loss: 0.00019632
Iteration 342/1000 | Loss: 0.00019632
Iteration 343/1000 | Loss: 0.00019632
Iteration 344/1000 | Loss: 0.00019632
Iteration 345/1000 | Loss: 0.00019632
Iteration 346/1000 | Loss: 0.00019632
Iteration 347/1000 | Loss: 0.00019632
Iteration 348/1000 | Loss: 0.00019632
Iteration 349/1000 | Loss: 0.00019632
Iteration 350/1000 | Loss: 0.00019632
Iteration 351/1000 | Loss: 0.00019632
Iteration 352/1000 | Loss: 0.00019632
Iteration 353/1000 | Loss: 0.00019632
Iteration 354/1000 | Loss: 0.00019632
Iteration 355/1000 | Loss: 0.00019632
Iteration 356/1000 | Loss: 0.00019632
Iteration 357/1000 | Loss: 0.00019632
Iteration 358/1000 | Loss: 0.00019632
Iteration 359/1000 | Loss: 0.00019632
Iteration 360/1000 | Loss: 0.00019632
Iteration 361/1000 | Loss: 0.00019632
Iteration 362/1000 | Loss: 0.00019632
Iteration 363/1000 | Loss: 0.00019632
Iteration 364/1000 | Loss: 0.00019632
Iteration 365/1000 | Loss: 0.00019632
Iteration 366/1000 | Loss: 0.00019632
Iteration 367/1000 | Loss: 0.00019632
Iteration 368/1000 | Loss: 0.00019632
Iteration 369/1000 | Loss: 0.00019632
Iteration 370/1000 | Loss: 0.00019632
Iteration 371/1000 | Loss: 0.00019632
Iteration 372/1000 | Loss: 0.00019632
Iteration 373/1000 | Loss: 0.00019632
Iteration 374/1000 | Loss: 0.00019632
Iteration 375/1000 | Loss: 0.00019632
Iteration 376/1000 | Loss: 0.00019632
Iteration 377/1000 | Loss: 0.00019632
Iteration 378/1000 | Loss: 0.00019632
Iteration 379/1000 | Loss: 0.00019632
Iteration 380/1000 | Loss: 0.00019632
Iteration 381/1000 | Loss: 0.00019632
Iteration 382/1000 | Loss: 0.00019632
Iteration 383/1000 | Loss: 0.00019632
Iteration 384/1000 | Loss: 0.00019632
Iteration 385/1000 | Loss: 0.00019632
Iteration 386/1000 | Loss: 0.00019632
Iteration 387/1000 | Loss: 0.00019632
Iteration 388/1000 | Loss: 0.00019632
Iteration 389/1000 | Loss: 0.00019632
Iteration 390/1000 | Loss: 0.00019632
Iteration 391/1000 | Loss: 0.00019632
Iteration 392/1000 | Loss: 0.00019632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 392. Stopping optimization.
Last 5 losses: [0.00019631862232927233, 0.00019631862232927233, 0.00019631862232927233, 0.00019631862232927233, 0.00019631862232927233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00019631862232927233

Optimization complete. Final v2v error: 7.719979763031006 mm

Highest mean error: 13.120905876159668 mm for frame 20

Lowest mean error: 5.151187419891357 mm for frame 118

Saving results

Total time: 284.7816812992096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00516633
Iteration 2/25 | Loss: 0.00152457
Iteration 3/25 | Loss: 0.00145858
Iteration 4/25 | Loss: 0.00143945
Iteration 5/25 | Loss: 0.00143289
Iteration 6/25 | Loss: 0.00143209
Iteration 7/25 | Loss: 0.00143209
Iteration 8/25 | Loss: 0.00143209
Iteration 9/25 | Loss: 0.00143209
Iteration 10/25 | Loss: 0.00143209
Iteration 11/25 | Loss: 0.00143209
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014320873888209462, 0.0014320873888209462, 0.0014320873888209462, 0.0014320873888209462, 0.0014320873888209462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014320873888209462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.09209752
Iteration 2/25 | Loss: 0.00162319
Iteration 3/25 | Loss: 0.00162314
Iteration 4/25 | Loss: 0.00162314
Iteration 5/25 | Loss: 0.00162314
Iteration 6/25 | Loss: 0.00162314
Iteration 7/25 | Loss: 0.00162314
Iteration 8/25 | Loss: 0.00162314
Iteration 9/25 | Loss: 0.00162314
Iteration 10/25 | Loss: 0.00162314
Iteration 11/25 | Loss: 0.00162314
Iteration 12/25 | Loss: 0.00162314
Iteration 13/25 | Loss: 0.00162314
Iteration 14/25 | Loss: 0.00162314
Iteration 15/25 | Loss: 0.00162314
Iteration 16/25 | Loss: 0.00162314
Iteration 17/25 | Loss: 0.00162314
Iteration 18/25 | Loss: 0.00162314
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016231430927291512, 0.0016231430927291512, 0.0016231430927291512, 0.0016231430927291512, 0.0016231430927291512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016231430927291512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162314
Iteration 2/1000 | Loss: 0.00005506
Iteration 3/1000 | Loss: 0.00003883
Iteration 4/1000 | Loss: 0.00003617
Iteration 5/1000 | Loss: 0.00003456
Iteration 6/1000 | Loss: 0.00003352
Iteration 7/1000 | Loss: 0.00003273
Iteration 8/1000 | Loss: 0.00003240
Iteration 9/1000 | Loss: 0.00003203
Iteration 10/1000 | Loss: 0.00003184
Iteration 11/1000 | Loss: 0.00003178
Iteration 12/1000 | Loss: 0.00003177
Iteration 13/1000 | Loss: 0.00003177
Iteration 14/1000 | Loss: 0.00003170
Iteration 15/1000 | Loss: 0.00003170
Iteration 16/1000 | Loss: 0.00003169
Iteration 17/1000 | Loss: 0.00003168
Iteration 18/1000 | Loss: 0.00003168
Iteration 19/1000 | Loss: 0.00003167
Iteration 20/1000 | Loss: 0.00003163
Iteration 21/1000 | Loss: 0.00003160
Iteration 22/1000 | Loss: 0.00003160
Iteration 23/1000 | Loss: 0.00003159
Iteration 24/1000 | Loss: 0.00003159
Iteration 25/1000 | Loss: 0.00003159
Iteration 26/1000 | Loss: 0.00003158
Iteration 27/1000 | Loss: 0.00003158
Iteration 28/1000 | Loss: 0.00003157
Iteration 29/1000 | Loss: 0.00003157
Iteration 30/1000 | Loss: 0.00003157
Iteration 31/1000 | Loss: 0.00003157
Iteration 32/1000 | Loss: 0.00003156
Iteration 33/1000 | Loss: 0.00003156
Iteration 34/1000 | Loss: 0.00003155
Iteration 35/1000 | Loss: 0.00003155
Iteration 36/1000 | Loss: 0.00003154
Iteration 37/1000 | Loss: 0.00003154
Iteration 38/1000 | Loss: 0.00003153
Iteration 39/1000 | Loss: 0.00003152
Iteration 40/1000 | Loss: 0.00003152
Iteration 41/1000 | Loss: 0.00003152
Iteration 42/1000 | Loss: 0.00003151
Iteration 43/1000 | Loss: 0.00003151
Iteration 44/1000 | Loss: 0.00003151
Iteration 45/1000 | Loss: 0.00003151
Iteration 46/1000 | Loss: 0.00003151
Iteration 47/1000 | Loss: 0.00003151
Iteration 48/1000 | Loss: 0.00003151
Iteration 49/1000 | Loss: 0.00003151
Iteration 50/1000 | Loss: 0.00003151
Iteration 51/1000 | Loss: 0.00003151
Iteration 52/1000 | Loss: 0.00003150
Iteration 53/1000 | Loss: 0.00003150
Iteration 54/1000 | Loss: 0.00003150
Iteration 55/1000 | Loss: 0.00003150
Iteration 56/1000 | Loss: 0.00003149
Iteration 57/1000 | Loss: 0.00003149
Iteration 58/1000 | Loss: 0.00003149
Iteration 59/1000 | Loss: 0.00003148
Iteration 60/1000 | Loss: 0.00003148
Iteration 61/1000 | Loss: 0.00003148
Iteration 62/1000 | Loss: 0.00003147
Iteration 63/1000 | Loss: 0.00003146
Iteration 64/1000 | Loss: 0.00003146
Iteration 65/1000 | Loss: 0.00003146
Iteration 66/1000 | Loss: 0.00003146
Iteration 67/1000 | Loss: 0.00003146
Iteration 68/1000 | Loss: 0.00003146
Iteration 69/1000 | Loss: 0.00003146
Iteration 70/1000 | Loss: 0.00003146
Iteration 71/1000 | Loss: 0.00003145
Iteration 72/1000 | Loss: 0.00003145
Iteration 73/1000 | Loss: 0.00003145
Iteration 74/1000 | Loss: 0.00003145
Iteration 75/1000 | Loss: 0.00003145
Iteration 76/1000 | Loss: 0.00003145
Iteration 77/1000 | Loss: 0.00003145
Iteration 78/1000 | Loss: 0.00003145
Iteration 79/1000 | Loss: 0.00003144
Iteration 80/1000 | Loss: 0.00003144
Iteration 81/1000 | Loss: 0.00003143
Iteration 82/1000 | Loss: 0.00003143
Iteration 83/1000 | Loss: 0.00003143
Iteration 84/1000 | Loss: 0.00003143
Iteration 85/1000 | Loss: 0.00003143
Iteration 86/1000 | Loss: 0.00003142
Iteration 87/1000 | Loss: 0.00003142
Iteration 88/1000 | Loss: 0.00003142
Iteration 89/1000 | Loss: 0.00003142
Iteration 90/1000 | Loss: 0.00003142
Iteration 91/1000 | Loss: 0.00003142
Iteration 92/1000 | Loss: 0.00003142
Iteration 93/1000 | Loss: 0.00003142
Iteration 94/1000 | Loss: 0.00003142
Iteration 95/1000 | Loss: 0.00003142
Iteration 96/1000 | Loss: 0.00003141
Iteration 97/1000 | Loss: 0.00003141
Iteration 98/1000 | Loss: 0.00003141
Iteration 99/1000 | Loss: 0.00003141
Iteration 100/1000 | Loss: 0.00003141
Iteration 101/1000 | Loss: 0.00003141
Iteration 102/1000 | Loss: 0.00003141
Iteration 103/1000 | Loss: 0.00003141
Iteration 104/1000 | Loss: 0.00003141
Iteration 105/1000 | Loss: 0.00003141
Iteration 106/1000 | Loss: 0.00003141
Iteration 107/1000 | Loss: 0.00003141
Iteration 108/1000 | Loss: 0.00003141
Iteration 109/1000 | Loss: 0.00003141
Iteration 110/1000 | Loss: 0.00003141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [3.140893022646196e-05, 3.140893022646196e-05, 3.140893022646196e-05, 3.140893022646196e-05, 3.140893022646196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.140893022646196e-05

Optimization complete. Final v2v error: 4.824639320373535 mm

Highest mean error: 5.122018814086914 mm for frame 1

Lowest mean error: 4.548940658569336 mm for frame 125

Saving results

Total time: 36.36663866043091
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458282
Iteration 2/25 | Loss: 0.00146907
Iteration 3/25 | Loss: 0.00140090
Iteration 4/25 | Loss: 0.00139160
Iteration 5/25 | Loss: 0.00138500
Iteration 6/25 | Loss: 0.00138356
Iteration 7/25 | Loss: 0.00138356
Iteration 8/25 | Loss: 0.00138356
Iteration 9/25 | Loss: 0.00138356
Iteration 10/25 | Loss: 0.00138356
Iteration 11/25 | Loss: 0.00138356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013835603604093194, 0.0013835603604093194, 0.0013835603604093194, 0.0013835603604093194, 0.0013835603604093194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013835603604093194

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79871023
Iteration 2/25 | Loss: 0.00183001
Iteration 3/25 | Loss: 0.00183001
Iteration 4/25 | Loss: 0.00183001
Iteration 5/25 | Loss: 0.00183001
Iteration 6/25 | Loss: 0.00183001
Iteration 7/25 | Loss: 0.00183000
Iteration 8/25 | Loss: 0.00183000
Iteration 9/25 | Loss: 0.00183000
Iteration 10/25 | Loss: 0.00183000
Iteration 11/25 | Loss: 0.00183000
Iteration 12/25 | Loss: 0.00183000
Iteration 13/25 | Loss: 0.00183000
Iteration 14/25 | Loss: 0.00183000
Iteration 15/25 | Loss: 0.00183000
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001830004039220512, 0.001830004039220512, 0.001830004039220512, 0.001830004039220512, 0.001830004039220512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001830004039220512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183000
Iteration 2/1000 | Loss: 0.00005386
Iteration 3/1000 | Loss: 0.00003793
Iteration 4/1000 | Loss: 0.00003294
Iteration 5/1000 | Loss: 0.00003080
Iteration 6/1000 | Loss: 0.00002924
Iteration 7/1000 | Loss: 0.00002826
Iteration 8/1000 | Loss: 0.00002774
Iteration 9/1000 | Loss: 0.00002733
Iteration 10/1000 | Loss: 0.00002695
Iteration 11/1000 | Loss: 0.00002661
Iteration 12/1000 | Loss: 0.00002635
Iteration 13/1000 | Loss: 0.00002630
Iteration 14/1000 | Loss: 0.00002618
Iteration 15/1000 | Loss: 0.00002616
Iteration 16/1000 | Loss: 0.00002615
Iteration 17/1000 | Loss: 0.00002614
Iteration 18/1000 | Loss: 0.00002614
Iteration 19/1000 | Loss: 0.00002613
Iteration 20/1000 | Loss: 0.00002612
Iteration 21/1000 | Loss: 0.00002611
Iteration 22/1000 | Loss: 0.00002609
Iteration 23/1000 | Loss: 0.00002609
Iteration 24/1000 | Loss: 0.00002606
Iteration 25/1000 | Loss: 0.00002606
Iteration 26/1000 | Loss: 0.00002605
Iteration 27/1000 | Loss: 0.00002605
Iteration 28/1000 | Loss: 0.00002605
Iteration 29/1000 | Loss: 0.00002604
Iteration 30/1000 | Loss: 0.00002604
Iteration 31/1000 | Loss: 0.00002604
Iteration 32/1000 | Loss: 0.00002604
Iteration 33/1000 | Loss: 0.00002604
Iteration 34/1000 | Loss: 0.00002603
Iteration 35/1000 | Loss: 0.00002603
Iteration 36/1000 | Loss: 0.00002602
Iteration 37/1000 | Loss: 0.00002602
Iteration 38/1000 | Loss: 0.00002602
Iteration 39/1000 | Loss: 0.00002601
Iteration 40/1000 | Loss: 0.00002600
Iteration 41/1000 | Loss: 0.00002600
Iteration 42/1000 | Loss: 0.00002600
Iteration 43/1000 | Loss: 0.00002599
Iteration 44/1000 | Loss: 0.00002599
Iteration 45/1000 | Loss: 0.00002599
Iteration 46/1000 | Loss: 0.00002598
Iteration 47/1000 | Loss: 0.00002598
Iteration 48/1000 | Loss: 0.00002598
Iteration 49/1000 | Loss: 0.00002598
Iteration 50/1000 | Loss: 0.00002598
Iteration 51/1000 | Loss: 0.00002598
Iteration 52/1000 | Loss: 0.00002598
Iteration 53/1000 | Loss: 0.00002598
Iteration 54/1000 | Loss: 0.00002598
Iteration 55/1000 | Loss: 0.00002598
Iteration 56/1000 | Loss: 0.00002598
Iteration 57/1000 | Loss: 0.00002598
Iteration 58/1000 | Loss: 0.00002598
Iteration 59/1000 | Loss: 0.00002598
Iteration 60/1000 | Loss: 0.00002598
Iteration 61/1000 | Loss: 0.00002598
Iteration 62/1000 | Loss: 0.00002598
Iteration 63/1000 | Loss: 0.00002598
Iteration 64/1000 | Loss: 0.00002598
Iteration 65/1000 | Loss: 0.00002598
Iteration 66/1000 | Loss: 0.00002598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [2.5976401957450435e-05, 2.5976401957450435e-05, 2.5976401957450435e-05, 2.5976401957450435e-05, 2.5976401957450435e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5976401957450435e-05

Optimization complete. Final v2v error: 4.547074317932129 mm

Highest mean error: 4.778491497039795 mm for frame 204

Lowest mean error: 4.26563835144043 mm for frame 167

Saving results

Total time: 35.499972105026245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01139657
Iteration 2/25 | Loss: 0.00231005
Iteration 3/25 | Loss: 0.00158799
Iteration 4/25 | Loss: 0.00143702
Iteration 5/25 | Loss: 0.00148494
Iteration 6/25 | Loss: 0.00149265
Iteration 7/25 | Loss: 0.00144597
Iteration 8/25 | Loss: 0.00134201
Iteration 9/25 | Loss: 0.00130843
Iteration 10/25 | Loss: 0.00126747
Iteration 11/25 | Loss: 0.00125984
Iteration 12/25 | Loss: 0.00124699
Iteration 13/25 | Loss: 0.00123671
Iteration 14/25 | Loss: 0.00122627
Iteration 15/25 | Loss: 0.00123706
Iteration 16/25 | Loss: 0.00123620
Iteration 17/25 | Loss: 0.00122549
Iteration 18/25 | Loss: 0.00123063
Iteration 19/25 | Loss: 0.00122866
Iteration 20/25 | Loss: 0.00123461
Iteration 21/25 | Loss: 0.00123300
Iteration 22/25 | Loss: 0.00122344
Iteration 23/25 | Loss: 0.00121859
Iteration 24/25 | Loss: 0.00123117
Iteration 25/25 | Loss: 0.00123254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53826213
Iteration 2/25 | Loss: 0.00132963
Iteration 3/25 | Loss: 0.00132963
Iteration 4/25 | Loss: 0.00132963
Iteration 5/25 | Loss: 0.00132963
Iteration 6/25 | Loss: 0.00132963
Iteration 7/25 | Loss: 0.00132963
Iteration 8/25 | Loss: 0.00132963
Iteration 9/25 | Loss: 0.00132963
Iteration 10/25 | Loss: 0.00132963
Iteration 11/25 | Loss: 0.00131675
Iteration 12/25 | Loss: 0.00131675
Iteration 13/25 | Loss: 0.00131675
Iteration 14/25 | Loss: 0.00131675
Iteration 15/25 | Loss: 0.00131675
Iteration 16/25 | Loss: 0.00131675
Iteration 17/25 | Loss: 0.00131675
Iteration 18/25 | Loss: 0.00131675
Iteration 19/25 | Loss: 0.00131675
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013167454162612557, 0.0013167454162612557, 0.0013167454162612557, 0.0013167454162612557, 0.0013167454162612557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013167454162612557

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131675
Iteration 2/1000 | Loss: 0.00013910
Iteration 3/1000 | Loss: 0.00004695
Iteration 4/1000 | Loss: 0.00004151
Iteration 5/1000 | Loss: 0.00006035
Iteration 6/1000 | Loss: 0.00004079
Iteration 7/1000 | Loss: 0.00021185
Iteration 8/1000 | Loss: 0.00018085
Iteration 9/1000 | Loss: 0.00005458
Iteration 10/1000 | Loss: 0.00019831
Iteration 11/1000 | Loss: 0.00027149
Iteration 12/1000 | Loss: 0.00019908
Iteration 13/1000 | Loss: 0.00017532
Iteration 14/1000 | Loss: 0.00022029
Iteration 15/1000 | Loss: 0.00016286
Iteration 16/1000 | Loss: 0.00004244
Iteration 17/1000 | Loss: 0.00051536
Iteration 18/1000 | Loss: 0.00013768
Iteration 19/1000 | Loss: 0.00005851
Iteration 20/1000 | Loss: 0.00005611
Iteration 21/1000 | Loss: 0.00003343
Iteration 22/1000 | Loss: 0.00002945
Iteration 23/1000 | Loss: 0.00002785
Iteration 24/1000 | Loss: 0.00002672
Iteration 25/1000 | Loss: 0.00002597
Iteration 26/1000 | Loss: 0.00002561
Iteration 27/1000 | Loss: 0.00002532
Iteration 28/1000 | Loss: 0.00002503
Iteration 29/1000 | Loss: 0.00002498
Iteration 30/1000 | Loss: 0.00002491
Iteration 31/1000 | Loss: 0.00002470
Iteration 32/1000 | Loss: 0.00028786
Iteration 33/1000 | Loss: 0.00055924
Iteration 34/1000 | Loss: 0.00019221
Iteration 35/1000 | Loss: 0.00041510
Iteration 36/1000 | Loss: 0.00005556
Iteration 37/1000 | Loss: 0.00007858
Iteration 38/1000 | Loss: 0.00017051
Iteration 39/1000 | Loss: 0.00024101
Iteration 40/1000 | Loss: 0.00014843
Iteration 41/1000 | Loss: 0.00003039
Iteration 42/1000 | Loss: 0.00002694
Iteration 43/1000 | Loss: 0.00002578
Iteration 44/1000 | Loss: 0.00017548
Iteration 45/1000 | Loss: 0.00003360
Iteration 46/1000 | Loss: 0.00002881
Iteration 47/1000 | Loss: 0.00002607
Iteration 48/1000 | Loss: 0.00015031
Iteration 49/1000 | Loss: 0.00002500
Iteration 50/1000 | Loss: 0.00011164
Iteration 51/1000 | Loss: 0.00025003
Iteration 52/1000 | Loss: 0.00023652
Iteration 53/1000 | Loss: 0.00028588
Iteration 54/1000 | Loss: 0.00019328
Iteration 55/1000 | Loss: 0.00018836
Iteration 56/1000 | Loss: 0.00019762
Iteration 57/1000 | Loss: 0.00016161
Iteration 58/1000 | Loss: 0.00016496
Iteration 59/1000 | Loss: 0.00014145
Iteration 60/1000 | Loss: 0.00005908
Iteration 61/1000 | Loss: 0.00025247
Iteration 62/1000 | Loss: 0.00026980
Iteration 63/1000 | Loss: 0.00021921
Iteration 64/1000 | Loss: 0.00024596
Iteration 65/1000 | Loss: 0.00021487
Iteration 66/1000 | Loss: 0.00020777
Iteration 67/1000 | Loss: 0.00015121
Iteration 68/1000 | Loss: 0.00003741
Iteration 69/1000 | Loss: 0.00025891
Iteration 70/1000 | Loss: 0.00025629
Iteration 71/1000 | Loss: 0.00023738
Iteration 72/1000 | Loss: 0.00026124
Iteration 73/1000 | Loss: 0.00010226
Iteration 74/1000 | Loss: 0.00008408
Iteration 75/1000 | Loss: 0.00052998
Iteration 76/1000 | Loss: 0.00010592
Iteration 77/1000 | Loss: 0.00013431
Iteration 78/1000 | Loss: 0.00003267
Iteration 79/1000 | Loss: 0.00002850
Iteration 80/1000 | Loss: 0.00002599
Iteration 81/1000 | Loss: 0.00002479
Iteration 82/1000 | Loss: 0.00002432
Iteration 83/1000 | Loss: 0.00002386
Iteration 84/1000 | Loss: 0.00002354
Iteration 85/1000 | Loss: 0.00002345
Iteration 86/1000 | Loss: 0.00002339
Iteration 87/1000 | Loss: 0.00002337
Iteration 88/1000 | Loss: 0.00002335
Iteration 89/1000 | Loss: 0.00002328
Iteration 90/1000 | Loss: 0.00002326
Iteration 91/1000 | Loss: 0.00002325
Iteration 92/1000 | Loss: 0.00002325
Iteration 93/1000 | Loss: 0.00002325
Iteration 94/1000 | Loss: 0.00002325
Iteration 95/1000 | Loss: 0.00002324
Iteration 96/1000 | Loss: 0.00002324
Iteration 97/1000 | Loss: 0.00002324
Iteration 98/1000 | Loss: 0.00002323
Iteration 99/1000 | Loss: 0.00002323
Iteration 100/1000 | Loss: 0.00002321
Iteration 101/1000 | Loss: 0.00002321
Iteration 102/1000 | Loss: 0.00002319
Iteration 103/1000 | Loss: 0.00002319
Iteration 104/1000 | Loss: 0.00002318
Iteration 105/1000 | Loss: 0.00002318
Iteration 106/1000 | Loss: 0.00002317
Iteration 107/1000 | Loss: 0.00002317
Iteration 108/1000 | Loss: 0.00002317
Iteration 109/1000 | Loss: 0.00002317
Iteration 110/1000 | Loss: 0.00002317
Iteration 111/1000 | Loss: 0.00002317
Iteration 112/1000 | Loss: 0.00002317
Iteration 113/1000 | Loss: 0.00002316
Iteration 114/1000 | Loss: 0.00002316
Iteration 115/1000 | Loss: 0.00002316
Iteration 116/1000 | Loss: 0.00002316
Iteration 117/1000 | Loss: 0.00002316
Iteration 118/1000 | Loss: 0.00002316
Iteration 119/1000 | Loss: 0.00002316
Iteration 120/1000 | Loss: 0.00002316
Iteration 121/1000 | Loss: 0.00002315
Iteration 122/1000 | Loss: 0.00002315
Iteration 123/1000 | Loss: 0.00002315
Iteration 124/1000 | Loss: 0.00002315
Iteration 125/1000 | Loss: 0.00002315
Iteration 126/1000 | Loss: 0.00002315
Iteration 127/1000 | Loss: 0.00002315
Iteration 128/1000 | Loss: 0.00002315
Iteration 129/1000 | Loss: 0.00002315
Iteration 130/1000 | Loss: 0.00002315
Iteration 131/1000 | Loss: 0.00002315
Iteration 132/1000 | Loss: 0.00002315
Iteration 133/1000 | Loss: 0.00002315
Iteration 134/1000 | Loss: 0.00002315
Iteration 135/1000 | Loss: 0.00002315
Iteration 136/1000 | Loss: 0.00002315
Iteration 137/1000 | Loss: 0.00002315
Iteration 138/1000 | Loss: 0.00002315
Iteration 139/1000 | Loss: 0.00002315
Iteration 140/1000 | Loss: 0.00002315
Iteration 141/1000 | Loss: 0.00002315
Iteration 142/1000 | Loss: 0.00002315
Iteration 143/1000 | Loss: 0.00002315
Iteration 144/1000 | Loss: 0.00002315
Iteration 145/1000 | Loss: 0.00002315
Iteration 146/1000 | Loss: 0.00002315
Iteration 147/1000 | Loss: 0.00002315
Iteration 148/1000 | Loss: 0.00002315
Iteration 149/1000 | Loss: 0.00002315
Iteration 150/1000 | Loss: 0.00002315
Iteration 151/1000 | Loss: 0.00002315
Iteration 152/1000 | Loss: 0.00002315
Iteration 153/1000 | Loss: 0.00002315
Iteration 154/1000 | Loss: 0.00002315
Iteration 155/1000 | Loss: 0.00002315
Iteration 156/1000 | Loss: 0.00002315
Iteration 157/1000 | Loss: 0.00002315
Iteration 158/1000 | Loss: 0.00002315
Iteration 159/1000 | Loss: 0.00002315
Iteration 160/1000 | Loss: 0.00002315
Iteration 161/1000 | Loss: 0.00002315
Iteration 162/1000 | Loss: 0.00002315
Iteration 163/1000 | Loss: 0.00002315
Iteration 164/1000 | Loss: 0.00002315
Iteration 165/1000 | Loss: 0.00002315
Iteration 166/1000 | Loss: 0.00002315
Iteration 167/1000 | Loss: 0.00002315
Iteration 168/1000 | Loss: 0.00002315
Iteration 169/1000 | Loss: 0.00002315
Iteration 170/1000 | Loss: 0.00002315
Iteration 171/1000 | Loss: 0.00002315
Iteration 172/1000 | Loss: 0.00002315
Iteration 173/1000 | Loss: 0.00002315
Iteration 174/1000 | Loss: 0.00002315
Iteration 175/1000 | Loss: 0.00002315
Iteration 176/1000 | Loss: 0.00002315
Iteration 177/1000 | Loss: 0.00002315
Iteration 178/1000 | Loss: 0.00002315
Iteration 179/1000 | Loss: 0.00002315
Iteration 180/1000 | Loss: 0.00002315
Iteration 181/1000 | Loss: 0.00002315
Iteration 182/1000 | Loss: 0.00002315
Iteration 183/1000 | Loss: 0.00002315
Iteration 184/1000 | Loss: 0.00002315
Iteration 185/1000 | Loss: 0.00002315
Iteration 186/1000 | Loss: 0.00002315
Iteration 187/1000 | Loss: 0.00002315
Iteration 188/1000 | Loss: 0.00002315
Iteration 189/1000 | Loss: 0.00002315
Iteration 190/1000 | Loss: 0.00002315
Iteration 191/1000 | Loss: 0.00002315
Iteration 192/1000 | Loss: 0.00002315
Iteration 193/1000 | Loss: 0.00002315
Iteration 194/1000 | Loss: 0.00002315
Iteration 195/1000 | Loss: 0.00002315
Iteration 196/1000 | Loss: 0.00002315
Iteration 197/1000 | Loss: 0.00002315
Iteration 198/1000 | Loss: 0.00002315
Iteration 199/1000 | Loss: 0.00002315
Iteration 200/1000 | Loss: 0.00002315
Iteration 201/1000 | Loss: 0.00002315
Iteration 202/1000 | Loss: 0.00002315
Iteration 203/1000 | Loss: 0.00002315
Iteration 204/1000 | Loss: 0.00002315
Iteration 205/1000 | Loss: 0.00002315
Iteration 206/1000 | Loss: 0.00002315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [2.3145941668190062e-05, 2.3145941668190062e-05, 2.3145941668190062e-05, 2.3145941668190062e-05, 2.3145941668190062e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3145941668190062e-05

Optimization complete. Final v2v error: 3.9909090995788574 mm

Highest mean error: 10.248430252075195 mm for frame 66

Lowest mean error: 3.539367198944092 mm for frame 130

Saving results

Total time: 167.63081693649292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817694
Iteration 2/25 | Loss: 0.00177331
Iteration 3/25 | Loss: 0.00157063
Iteration 4/25 | Loss: 0.00153924
Iteration 5/25 | Loss: 0.00152462
Iteration 6/25 | Loss: 0.00152048
Iteration 7/25 | Loss: 0.00151746
Iteration 8/25 | Loss: 0.00152190
Iteration 9/25 | Loss: 0.00151758
Iteration 10/25 | Loss: 0.00151443
Iteration 11/25 | Loss: 0.00151514
Iteration 12/25 | Loss: 0.00151123
Iteration 13/25 | Loss: 0.00151258
Iteration 14/25 | Loss: 0.00150907
Iteration 15/25 | Loss: 0.00150941
Iteration 16/25 | Loss: 0.00150813
Iteration 17/25 | Loss: 0.00150744
Iteration 18/25 | Loss: 0.00150664
Iteration 19/25 | Loss: 0.00150607
Iteration 20/25 | Loss: 0.00150585
Iteration 21/25 | Loss: 0.00150578
Iteration 22/25 | Loss: 0.00150578
Iteration 23/25 | Loss: 0.00150578
Iteration 24/25 | Loss: 0.00150578
Iteration 25/25 | Loss: 0.00150578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39097917
Iteration 2/25 | Loss: 0.00199365
Iteration 3/25 | Loss: 0.00199362
Iteration 4/25 | Loss: 0.00199362
Iteration 5/25 | Loss: 0.00199362
Iteration 6/25 | Loss: 0.00199362
Iteration 7/25 | Loss: 0.00199362
Iteration 8/25 | Loss: 0.00199362
Iteration 9/25 | Loss: 0.00199362
Iteration 10/25 | Loss: 0.00199362
Iteration 11/25 | Loss: 0.00199362
Iteration 12/25 | Loss: 0.00199362
Iteration 13/25 | Loss: 0.00199362
Iteration 14/25 | Loss: 0.00199362
Iteration 15/25 | Loss: 0.00199362
Iteration 16/25 | Loss: 0.00199362
Iteration 17/25 | Loss: 0.00199362
Iteration 18/25 | Loss: 0.00199362
Iteration 19/25 | Loss: 0.00199362
Iteration 20/25 | Loss: 0.00199362
Iteration 21/25 | Loss: 0.00199362
Iteration 22/25 | Loss: 0.00199362
Iteration 23/25 | Loss: 0.00199362
Iteration 24/25 | Loss: 0.00199362
Iteration 25/25 | Loss: 0.00199362

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199362
Iteration 2/1000 | Loss: 0.00012646
Iteration 3/1000 | Loss: 0.00008434
Iteration 4/1000 | Loss: 0.00007114
Iteration 5/1000 | Loss: 0.00006591
Iteration 6/1000 | Loss: 0.00256776
Iteration 7/1000 | Loss: 0.00038667
Iteration 8/1000 | Loss: 0.00073311
Iteration 9/1000 | Loss: 0.00036572
Iteration 10/1000 | Loss: 0.00243029
Iteration 11/1000 | Loss: 0.00082454
Iteration 12/1000 | Loss: 0.00106897
Iteration 13/1000 | Loss: 0.00046531
Iteration 14/1000 | Loss: 0.00187789
Iteration 15/1000 | Loss: 0.00011765
Iteration 16/1000 | Loss: 0.00006397
Iteration 17/1000 | Loss: 0.00005231
Iteration 18/1000 | Loss: 0.00004914
Iteration 19/1000 | Loss: 0.00004713
Iteration 20/1000 | Loss: 0.00004586
Iteration 21/1000 | Loss: 0.00004475
Iteration 22/1000 | Loss: 0.00004399
Iteration 23/1000 | Loss: 0.00004360
Iteration 24/1000 | Loss: 0.00004337
Iteration 25/1000 | Loss: 0.00004313
Iteration 26/1000 | Loss: 0.00004298
Iteration 27/1000 | Loss: 0.00004291
Iteration 28/1000 | Loss: 0.00004288
Iteration 29/1000 | Loss: 0.00004287
Iteration 30/1000 | Loss: 0.00004286
Iteration 31/1000 | Loss: 0.00004274
Iteration 32/1000 | Loss: 0.00004274
Iteration 33/1000 | Loss: 0.00004274
Iteration 34/1000 | Loss: 0.00004273
Iteration 35/1000 | Loss: 0.00004269
Iteration 36/1000 | Loss: 0.00004269
Iteration 37/1000 | Loss: 0.00004268
Iteration 38/1000 | Loss: 0.00004265
Iteration 39/1000 | Loss: 0.00004264
Iteration 40/1000 | Loss: 0.00004263
Iteration 41/1000 | Loss: 0.00004263
Iteration 42/1000 | Loss: 0.00004263
Iteration 43/1000 | Loss: 0.00004263
Iteration 44/1000 | Loss: 0.00004263
Iteration 45/1000 | Loss: 0.00004263
Iteration 46/1000 | Loss: 0.00004263
Iteration 47/1000 | Loss: 0.00004263
Iteration 48/1000 | Loss: 0.00004263
Iteration 49/1000 | Loss: 0.00004263
Iteration 50/1000 | Loss: 0.00004263
Iteration 51/1000 | Loss: 0.00004262
Iteration 52/1000 | Loss: 0.00004262
Iteration 53/1000 | Loss: 0.00004261
Iteration 54/1000 | Loss: 0.00004261
Iteration 55/1000 | Loss: 0.00004260
Iteration 56/1000 | Loss: 0.00004260
Iteration 57/1000 | Loss: 0.00004259
Iteration 58/1000 | Loss: 0.00004259
Iteration 59/1000 | Loss: 0.00004259
Iteration 60/1000 | Loss: 0.00004259
Iteration 61/1000 | Loss: 0.00004259
Iteration 62/1000 | Loss: 0.00004259
Iteration 63/1000 | Loss: 0.00004259
Iteration 64/1000 | Loss: 0.00004259
Iteration 65/1000 | Loss: 0.00004259
Iteration 66/1000 | Loss: 0.00004258
Iteration 67/1000 | Loss: 0.00004258
Iteration 68/1000 | Loss: 0.00004258
Iteration 69/1000 | Loss: 0.00004258
Iteration 70/1000 | Loss: 0.00004257
Iteration 71/1000 | Loss: 0.00004257
Iteration 72/1000 | Loss: 0.00004257
Iteration 73/1000 | Loss: 0.00004256
Iteration 74/1000 | Loss: 0.00004256
Iteration 75/1000 | Loss: 0.00004256
Iteration 76/1000 | Loss: 0.00004255
Iteration 77/1000 | Loss: 0.00004255
Iteration 78/1000 | Loss: 0.00004255
Iteration 79/1000 | Loss: 0.00004255
Iteration 80/1000 | Loss: 0.00004255
Iteration 81/1000 | Loss: 0.00004254
Iteration 82/1000 | Loss: 0.00004254
Iteration 83/1000 | Loss: 0.00004254
Iteration 84/1000 | Loss: 0.00004254
Iteration 85/1000 | Loss: 0.00004253
Iteration 86/1000 | Loss: 0.00004253
Iteration 87/1000 | Loss: 0.00004253
Iteration 88/1000 | Loss: 0.00004253
Iteration 89/1000 | Loss: 0.00004252
Iteration 90/1000 | Loss: 0.00004252
Iteration 91/1000 | Loss: 0.00004252
Iteration 92/1000 | Loss: 0.00004252
Iteration 93/1000 | Loss: 0.00004252
Iteration 94/1000 | Loss: 0.00004252
Iteration 95/1000 | Loss: 0.00004252
Iteration 96/1000 | Loss: 0.00004252
Iteration 97/1000 | Loss: 0.00004252
Iteration 98/1000 | Loss: 0.00004251
Iteration 99/1000 | Loss: 0.00004251
Iteration 100/1000 | Loss: 0.00004251
Iteration 101/1000 | Loss: 0.00004251
Iteration 102/1000 | Loss: 0.00004251
Iteration 103/1000 | Loss: 0.00004251
Iteration 104/1000 | Loss: 0.00004251
Iteration 105/1000 | Loss: 0.00004251
Iteration 106/1000 | Loss: 0.00004250
Iteration 107/1000 | Loss: 0.00004250
Iteration 108/1000 | Loss: 0.00004250
Iteration 109/1000 | Loss: 0.00004250
Iteration 110/1000 | Loss: 0.00004250
Iteration 111/1000 | Loss: 0.00004250
Iteration 112/1000 | Loss: 0.00004250
Iteration 113/1000 | Loss: 0.00004250
Iteration 114/1000 | Loss: 0.00004250
Iteration 115/1000 | Loss: 0.00004250
Iteration 116/1000 | Loss: 0.00004250
Iteration 117/1000 | Loss: 0.00004250
Iteration 118/1000 | Loss: 0.00004250
Iteration 119/1000 | Loss: 0.00004250
Iteration 120/1000 | Loss: 0.00004250
Iteration 121/1000 | Loss: 0.00004250
Iteration 122/1000 | Loss: 0.00004250
Iteration 123/1000 | Loss: 0.00004249
Iteration 124/1000 | Loss: 0.00004249
Iteration 125/1000 | Loss: 0.00004249
Iteration 126/1000 | Loss: 0.00004249
Iteration 127/1000 | Loss: 0.00004249
Iteration 128/1000 | Loss: 0.00004249
Iteration 129/1000 | Loss: 0.00004249
Iteration 130/1000 | Loss: 0.00004249
Iteration 131/1000 | Loss: 0.00004249
Iteration 132/1000 | Loss: 0.00004249
Iteration 133/1000 | Loss: 0.00004249
Iteration 134/1000 | Loss: 0.00004249
Iteration 135/1000 | Loss: 0.00004249
Iteration 136/1000 | Loss: 0.00004249
Iteration 137/1000 | Loss: 0.00004249
Iteration 138/1000 | Loss: 0.00004249
Iteration 139/1000 | Loss: 0.00004249
Iteration 140/1000 | Loss: 0.00004249
Iteration 141/1000 | Loss: 0.00004249
Iteration 142/1000 | Loss: 0.00004249
Iteration 143/1000 | Loss: 0.00004249
Iteration 144/1000 | Loss: 0.00004249
Iteration 145/1000 | Loss: 0.00004249
Iteration 146/1000 | Loss: 0.00004249
Iteration 147/1000 | Loss: 0.00004249
Iteration 148/1000 | Loss: 0.00004249
Iteration 149/1000 | Loss: 0.00004249
Iteration 150/1000 | Loss: 0.00004249
Iteration 151/1000 | Loss: 0.00004249
Iteration 152/1000 | Loss: 0.00004249
Iteration 153/1000 | Loss: 0.00004249
Iteration 154/1000 | Loss: 0.00004249
Iteration 155/1000 | Loss: 0.00004249
Iteration 156/1000 | Loss: 0.00004249
Iteration 157/1000 | Loss: 0.00004249
Iteration 158/1000 | Loss: 0.00004249
Iteration 159/1000 | Loss: 0.00004249
Iteration 160/1000 | Loss: 0.00004249
Iteration 161/1000 | Loss: 0.00004249
Iteration 162/1000 | Loss: 0.00004249
Iteration 163/1000 | Loss: 0.00004249
Iteration 164/1000 | Loss: 0.00004249
Iteration 165/1000 | Loss: 0.00004249
Iteration 166/1000 | Loss: 0.00004249
Iteration 167/1000 | Loss: 0.00004249
Iteration 168/1000 | Loss: 0.00004249
Iteration 169/1000 | Loss: 0.00004249
Iteration 170/1000 | Loss: 0.00004249
Iteration 171/1000 | Loss: 0.00004249
Iteration 172/1000 | Loss: 0.00004249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [4.248948971508071e-05, 4.248948971508071e-05, 4.248948971508071e-05, 4.248948971508071e-05, 4.248948971508071e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.248948971508071e-05

Optimization complete. Final v2v error: 5.279234409332275 mm

Highest mean error: 13.930930137634277 mm for frame 92

Lowest mean error: 4.673098087310791 mm for frame 239

Saving results

Total time: 94.35502767562866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01244813
Iteration 2/25 | Loss: 0.00178111
Iteration 3/25 | Loss: 0.00154311
Iteration 4/25 | Loss: 0.00150133
Iteration 5/25 | Loss: 0.00149871
Iteration 6/25 | Loss: 0.00149122
Iteration 7/25 | Loss: 0.00149045
Iteration 8/25 | Loss: 0.00148999
Iteration 9/25 | Loss: 0.00148996
Iteration 10/25 | Loss: 0.00149576
Iteration 11/25 | Loss: 0.00149290
Iteration 12/25 | Loss: 0.00148997
Iteration 13/25 | Loss: 0.00148992
Iteration 14/25 | Loss: 0.00148991
Iteration 15/25 | Loss: 0.00148991
Iteration 16/25 | Loss: 0.00148991
Iteration 17/25 | Loss: 0.00148991
Iteration 18/25 | Loss: 0.00148991
Iteration 19/25 | Loss: 0.00148991
Iteration 20/25 | Loss: 0.00148990
Iteration 21/25 | Loss: 0.00148990
Iteration 22/25 | Loss: 0.00148990
Iteration 23/25 | Loss: 0.00148990
Iteration 24/25 | Loss: 0.00148990
Iteration 25/25 | Loss: 0.00148990

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89543843
Iteration 2/25 | Loss: 0.00172607
Iteration 3/25 | Loss: 0.00167660
Iteration 4/25 | Loss: 0.00167660
Iteration 5/25 | Loss: 0.00167660
Iteration 6/25 | Loss: 0.00167660
Iteration 7/25 | Loss: 0.00167660
Iteration 8/25 | Loss: 0.00167660
Iteration 9/25 | Loss: 0.00167660
Iteration 10/25 | Loss: 0.00167660
Iteration 11/25 | Loss: 0.00167660
Iteration 12/25 | Loss: 0.00167660
Iteration 13/25 | Loss: 0.00167660
Iteration 14/25 | Loss: 0.00167660
Iteration 15/25 | Loss: 0.00167660
Iteration 16/25 | Loss: 0.00167660
Iteration 17/25 | Loss: 0.00167660
Iteration 18/25 | Loss: 0.00167660
Iteration 19/25 | Loss: 0.00167660
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016766006592661142, 0.0016766006592661142, 0.0016766006592661142, 0.0016766006592661142, 0.0016766006592661142]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016766006592661142

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167660
Iteration 2/1000 | Loss: 0.00006099
Iteration 3/1000 | Loss: 0.00004831
Iteration 4/1000 | Loss: 0.00004254
Iteration 5/1000 | Loss: 0.00004074
Iteration 6/1000 | Loss: 0.00003989
Iteration 7/1000 | Loss: 0.00003921
Iteration 8/1000 | Loss: 0.00003878
Iteration 9/1000 | Loss: 0.00003843
Iteration 10/1000 | Loss: 0.00003820
Iteration 11/1000 | Loss: 0.00013048
Iteration 12/1000 | Loss: 0.00003806
Iteration 13/1000 | Loss: 0.00003795
Iteration 14/1000 | Loss: 0.00003794
Iteration 15/1000 | Loss: 0.00003788
Iteration 16/1000 | Loss: 0.00003788
Iteration 17/1000 | Loss: 0.00003788
Iteration 18/1000 | Loss: 0.00003788
Iteration 19/1000 | Loss: 0.00003788
Iteration 20/1000 | Loss: 0.00003788
Iteration 21/1000 | Loss: 0.00003788
Iteration 22/1000 | Loss: 0.00003788
Iteration 23/1000 | Loss: 0.00003788
Iteration 24/1000 | Loss: 0.00003787
Iteration 25/1000 | Loss: 0.00003787
Iteration 26/1000 | Loss: 0.00003786
Iteration 27/1000 | Loss: 0.00003786
Iteration 28/1000 | Loss: 0.00003785
Iteration 29/1000 | Loss: 0.00003785
Iteration 30/1000 | Loss: 0.00003785
Iteration 31/1000 | Loss: 0.00003785
Iteration 32/1000 | Loss: 0.00003785
Iteration 33/1000 | Loss: 0.00003785
Iteration 34/1000 | Loss: 0.00003785
Iteration 35/1000 | Loss: 0.00003785
Iteration 36/1000 | Loss: 0.00003785
Iteration 37/1000 | Loss: 0.00003784
Iteration 38/1000 | Loss: 0.00003783
Iteration 39/1000 | Loss: 0.00003782
Iteration 40/1000 | Loss: 0.00003782
Iteration 41/1000 | Loss: 0.00003782
Iteration 42/1000 | Loss: 0.00003781
Iteration 43/1000 | Loss: 0.00003781
Iteration 44/1000 | Loss: 0.00003781
Iteration 45/1000 | Loss: 0.00003780
Iteration 46/1000 | Loss: 0.00003780
Iteration 47/1000 | Loss: 0.00003780
Iteration 48/1000 | Loss: 0.00003780
Iteration 49/1000 | Loss: 0.00003780
Iteration 50/1000 | Loss: 0.00003780
Iteration 51/1000 | Loss: 0.00003779
Iteration 52/1000 | Loss: 0.00003779
Iteration 53/1000 | Loss: 0.00003779
Iteration 54/1000 | Loss: 0.00003779
Iteration 55/1000 | Loss: 0.00003779
Iteration 56/1000 | Loss: 0.00003779
Iteration 57/1000 | Loss: 0.00003779
Iteration 58/1000 | Loss: 0.00003779
Iteration 59/1000 | Loss: 0.00003779
Iteration 60/1000 | Loss: 0.00003779
Iteration 61/1000 | Loss: 0.00003779
Iteration 62/1000 | Loss: 0.00003779
Iteration 63/1000 | Loss: 0.00003779
Iteration 64/1000 | Loss: 0.00003779
Iteration 65/1000 | Loss: 0.00003779
Iteration 66/1000 | Loss: 0.00003779
Iteration 67/1000 | Loss: 0.00003779
Iteration 68/1000 | Loss: 0.00003778
Iteration 69/1000 | Loss: 0.00003778
Iteration 70/1000 | Loss: 0.00003778
Iteration 71/1000 | Loss: 0.00003778
Iteration 72/1000 | Loss: 0.00003778
Iteration 73/1000 | Loss: 0.00003778
Iteration 74/1000 | Loss: 0.00003778
Iteration 75/1000 | Loss: 0.00003778
Iteration 76/1000 | Loss: 0.00003778
Iteration 77/1000 | Loss: 0.00003778
Iteration 78/1000 | Loss: 0.00003777
Iteration 79/1000 | Loss: 0.00003777
Iteration 80/1000 | Loss: 0.00003777
Iteration 81/1000 | Loss: 0.00003777
Iteration 82/1000 | Loss: 0.00003777
Iteration 83/1000 | Loss: 0.00003777
Iteration 84/1000 | Loss: 0.00003777
Iteration 85/1000 | Loss: 0.00003776
Iteration 86/1000 | Loss: 0.00003776
Iteration 87/1000 | Loss: 0.00003776
Iteration 88/1000 | Loss: 0.00003776
Iteration 89/1000 | Loss: 0.00003775
Iteration 90/1000 | Loss: 0.00003775
Iteration 91/1000 | Loss: 0.00003775
Iteration 92/1000 | Loss: 0.00003775
Iteration 93/1000 | Loss: 0.00003774
Iteration 94/1000 | Loss: 0.00003773
Iteration 95/1000 | Loss: 0.00003773
Iteration 96/1000 | Loss: 0.00003773
Iteration 97/1000 | Loss: 0.00003772
Iteration 98/1000 | Loss: 0.00003771
Iteration 99/1000 | Loss: 0.00003771
Iteration 100/1000 | Loss: 0.00003771
Iteration 101/1000 | Loss: 0.00003771
Iteration 102/1000 | Loss: 0.00003771
Iteration 103/1000 | Loss: 0.00003771
Iteration 104/1000 | Loss: 0.00003770
Iteration 105/1000 | Loss: 0.00003770
Iteration 106/1000 | Loss: 0.00003770
Iteration 107/1000 | Loss: 0.00003770
Iteration 108/1000 | Loss: 0.00003769
Iteration 109/1000 | Loss: 0.00003769
Iteration 110/1000 | Loss: 0.00003769
Iteration 111/1000 | Loss: 0.00003769
Iteration 112/1000 | Loss: 0.00003769
Iteration 113/1000 | Loss: 0.00003769
Iteration 114/1000 | Loss: 0.00003769
Iteration 115/1000 | Loss: 0.00003769
Iteration 116/1000 | Loss: 0.00003769
Iteration 117/1000 | Loss: 0.00003769
Iteration 118/1000 | Loss: 0.00003769
Iteration 119/1000 | Loss: 0.00003769
Iteration 120/1000 | Loss: 0.00003769
Iteration 121/1000 | Loss: 0.00003769
Iteration 122/1000 | Loss: 0.00003769
Iteration 123/1000 | Loss: 0.00003769
Iteration 124/1000 | Loss: 0.00003769
Iteration 125/1000 | Loss: 0.00003769
Iteration 126/1000 | Loss: 0.00003769
Iteration 127/1000 | Loss: 0.00003769
Iteration 128/1000 | Loss: 0.00003769
Iteration 129/1000 | Loss: 0.00003769
Iteration 130/1000 | Loss: 0.00003769
Iteration 131/1000 | Loss: 0.00003769
Iteration 132/1000 | Loss: 0.00003769
Iteration 133/1000 | Loss: 0.00003769
Iteration 134/1000 | Loss: 0.00003769
Iteration 135/1000 | Loss: 0.00003769
Iteration 136/1000 | Loss: 0.00003769
Iteration 137/1000 | Loss: 0.00003769
Iteration 138/1000 | Loss: 0.00003769
Iteration 139/1000 | Loss: 0.00003769
Iteration 140/1000 | Loss: 0.00003769
Iteration 141/1000 | Loss: 0.00003769
Iteration 142/1000 | Loss: 0.00003769
Iteration 143/1000 | Loss: 0.00003769
Iteration 144/1000 | Loss: 0.00003769
Iteration 145/1000 | Loss: 0.00003769
Iteration 146/1000 | Loss: 0.00003769
Iteration 147/1000 | Loss: 0.00003769
Iteration 148/1000 | Loss: 0.00003769
Iteration 149/1000 | Loss: 0.00003769
Iteration 150/1000 | Loss: 0.00003769
Iteration 151/1000 | Loss: 0.00003769
Iteration 152/1000 | Loss: 0.00003769
Iteration 153/1000 | Loss: 0.00003769
Iteration 154/1000 | Loss: 0.00003769
Iteration 155/1000 | Loss: 0.00003769
Iteration 156/1000 | Loss: 0.00003769
Iteration 157/1000 | Loss: 0.00003769
Iteration 158/1000 | Loss: 0.00003769
Iteration 159/1000 | Loss: 0.00003769
Iteration 160/1000 | Loss: 0.00003769
Iteration 161/1000 | Loss: 0.00003769
Iteration 162/1000 | Loss: 0.00003769
Iteration 163/1000 | Loss: 0.00003769
Iteration 164/1000 | Loss: 0.00003769
Iteration 165/1000 | Loss: 0.00003769
Iteration 166/1000 | Loss: 0.00003769
Iteration 167/1000 | Loss: 0.00003769
Iteration 168/1000 | Loss: 0.00003769
Iteration 169/1000 | Loss: 0.00003769
Iteration 170/1000 | Loss: 0.00003769
Iteration 171/1000 | Loss: 0.00003769
Iteration 172/1000 | Loss: 0.00003769
Iteration 173/1000 | Loss: 0.00003769
Iteration 174/1000 | Loss: 0.00003769
Iteration 175/1000 | Loss: 0.00003769
Iteration 176/1000 | Loss: 0.00003769
Iteration 177/1000 | Loss: 0.00003769
Iteration 178/1000 | Loss: 0.00003769
Iteration 179/1000 | Loss: 0.00003769
Iteration 180/1000 | Loss: 0.00003769
Iteration 181/1000 | Loss: 0.00003769
Iteration 182/1000 | Loss: 0.00003769
Iteration 183/1000 | Loss: 0.00003769
Iteration 184/1000 | Loss: 0.00003769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [3.768777969526127e-05, 3.768777969526127e-05, 3.768777969526127e-05, 3.768777969526127e-05, 3.768777969526127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.768777969526127e-05

Optimization complete. Final v2v error: 5.295029640197754 mm

Highest mean error: 5.800045967102051 mm for frame 104

Lowest mean error: 4.837487697601318 mm for frame 57

Saving results

Total time: 45.962380170822144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903383
Iteration 2/25 | Loss: 0.00179374
Iteration 3/25 | Loss: 0.00143710
Iteration 4/25 | Loss: 0.00137166
Iteration 5/25 | Loss: 0.00136100
Iteration 6/25 | Loss: 0.00135821
Iteration 7/25 | Loss: 0.00135812
Iteration 8/25 | Loss: 0.00135812
Iteration 9/25 | Loss: 0.00135812
Iteration 10/25 | Loss: 0.00135812
Iteration 11/25 | Loss: 0.00135812
Iteration 12/25 | Loss: 0.00135812
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013581226812675595, 0.0013581226812675595, 0.0013581226812675595, 0.0013581226812675595, 0.0013581226812675595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013581226812675595

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52068985
Iteration 2/25 | Loss: 0.00174210
Iteration 3/25 | Loss: 0.00174210
Iteration 4/25 | Loss: 0.00174210
Iteration 5/25 | Loss: 0.00174210
Iteration 6/25 | Loss: 0.00174210
Iteration 7/25 | Loss: 0.00174210
Iteration 8/25 | Loss: 0.00174210
Iteration 9/25 | Loss: 0.00174210
Iteration 10/25 | Loss: 0.00174210
Iteration 11/25 | Loss: 0.00174210
Iteration 12/25 | Loss: 0.00174210
Iteration 13/25 | Loss: 0.00174210
Iteration 14/25 | Loss: 0.00174210
Iteration 15/25 | Loss: 0.00174210
Iteration 16/25 | Loss: 0.00174210
Iteration 17/25 | Loss: 0.00174210
Iteration 18/25 | Loss: 0.00174210
Iteration 19/25 | Loss: 0.00174210
Iteration 20/25 | Loss: 0.00174210
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0017420983640477061, 0.0017420983640477061, 0.0017420983640477061, 0.0017420983640477061, 0.0017420983640477061]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017420983640477061

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174210
Iteration 2/1000 | Loss: 0.00005239
Iteration 3/1000 | Loss: 0.00003998
Iteration 4/1000 | Loss: 0.00003611
Iteration 5/1000 | Loss: 0.00003438
Iteration 6/1000 | Loss: 0.00003305
Iteration 7/1000 | Loss: 0.00003236
Iteration 8/1000 | Loss: 0.00003166
Iteration 9/1000 | Loss: 0.00003131
Iteration 10/1000 | Loss: 0.00003095
Iteration 11/1000 | Loss: 0.00003076
Iteration 12/1000 | Loss: 0.00003054
Iteration 13/1000 | Loss: 0.00003047
Iteration 14/1000 | Loss: 0.00003047
Iteration 15/1000 | Loss: 0.00003046
Iteration 16/1000 | Loss: 0.00003046
Iteration 17/1000 | Loss: 0.00003036
Iteration 18/1000 | Loss: 0.00003035
Iteration 19/1000 | Loss: 0.00003034
Iteration 20/1000 | Loss: 0.00003034
Iteration 21/1000 | Loss: 0.00003034
Iteration 22/1000 | Loss: 0.00003033
Iteration 23/1000 | Loss: 0.00003033
Iteration 24/1000 | Loss: 0.00003033
Iteration 25/1000 | Loss: 0.00003030
Iteration 26/1000 | Loss: 0.00003030
Iteration 27/1000 | Loss: 0.00003030
Iteration 28/1000 | Loss: 0.00003030
Iteration 29/1000 | Loss: 0.00003030
Iteration 30/1000 | Loss: 0.00003030
Iteration 31/1000 | Loss: 0.00003030
Iteration 32/1000 | Loss: 0.00003030
Iteration 33/1000 | Loss: 0.00003030
Iteration 34/1000 | Loss: 0.00003030
Iteration 35/1000 | Loss: 0.00003030
Iteration 36/1000 | Loss: 0.00003030
Iteration 37/1000 | Loss: 0.00003023
Iteration 38/1000 | Loss: 0.00003023
Iteration 39/1000 | Loss: 0.00003023
Iteration 40/1000 | Loss: 0.00003023
Iteration 41/1000 | Loss: 0.00003022
Iteration 42/1000 | Loss: 0.00003022
Iteration 43/1000 | Loss: 0.00003021
Iteration 44/1000 | Loss: 0.00003021
Iteration 45/1000 | Loss: 0.00003021
Iteration 46/1000 | Loss: 0.00003020
Iteration 47/1000 | Loss: 0.00003020
Iteration 48/1000 | Loss: 0.00003019
Iteration 49/1000 | Loss: 0.00003019
Iteration 50/1000 | Loss: 0.00003019
Iteration 51/1000 | Loss: 0.00003019
Iteration 52/1000 | Loss: 0.00003018
Iteration 53/1000 | Loss: 0.00003018
Iteration 54/1000 | Loss: 0.00003018
Iteration 55/1000 | Loss: 0.00003018
Iteration 56/1000 | Loss: 0.00003018
Iteration 57/1000 | Loss: 0.00003017
Iteration 58/1000 | Loss: 0.00003017
Iteration 59/1000 | Loss: 0.00003017
Iteration 60/1000 | Loss: 0.00003017
Iteration 61/1000 | Loss: 0.00003017
Iteration 62/1000 | Loss: 0.00003017
Iteration 63/1000 | Loss: 0.00003017
Iteration 64/1000 | Loss: 0.00003017
Iteration 65/1000 | Loss: 0.00003017
Iteration 66/1000 | Loss: 0.00003017
Iteration 67/1000 | Loss: 0.00003017
Iteration 68/1000 | Loss: 0.00003017
Iteration 69/1000 | Loss: 0.00003017
Iteration 70/1000 | Loss: 0.00003017
Iteration 71/1000 | Loss: 0.00003017
Iteration 72/1000 | Loss: 0.00003017
Iteration 73/1000 | Loss: 0.00003016
Iteration 74/1000 | Loss: 0.00003016
Iteration 75/1000 | Loss: 0.00003016
Iteration 76/1000 | Loss: 0.00003016
Iteration 77/1000 | Loss: 0.00003016
Iteration 78/1000 | Loss: 0.00003016
Iteration 79/1000 | Loss: 0.00003016
Iteration 80/1000 | Loss: 0.00003016
Iteration 81/1000 | Loss: 0.00003016
Iteration 82/1000 | Loss: 0.00003016
Iteration 83/1000 | Loss: 0.00003016
Iteration 84/1000 | Loss: 0.00003016
Iteration 85/1000 | Loss: 0.00003016
Iteration 86/1000 | Loss: 0.00003016
Iteration 87/1000 | Loss: 0.00003016
Iteration 88/1000 | Loss: 0.00003016
Iteration 89/1000 | Loss: 0.00003016
Iteration 90/1000 | Loss: 0.00003016
Iteration 91/1000 | Loss: 0.00003016
Iteration 92/1000 | Loss: 0.00003016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [3.0159735615598038e-05, 3.0159735615598038e-05, 3.0159735615598038e-05, 3.0159735615598038e-05, 3.0159735615598038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0159735615598038e-05

Optimization complete. Final v2v error: 4.829776287078857 mm

Highest mean error: 5.3146772384643555 mm for frame 130

Lowest mean error: 4.356775283813477 mm for frame 37

Saving results

Total time: 38.659475564956665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938086
Iteration 2/25 | Loss: 0.00252739
Iteration 3/25 | Loss: 0.00161056
Iteration 4/25 | Loss: 0.00135780
Iteration 5/25 | Loss: 0.00130007
Iteration 6/25 | Loss: 0.00120914
Iteration 7/25 | Loss: 0.00112729
Iteration 8/25 | Loss: 0.00107474
Iteration 9/25 | Loss: 0.00104430
Iteration 10/25 | Loss: 0.00100974
Iteration 11/25 | Loss: 0.00099552
Iteration 12/25 | Loss: 0.00098024
Iteration 13/25 | Loss: 0.00097580
Iteration 14/25 | Loss: 0.00097174
Iteration 15/25 | Loss: 0.00096744
Iteration 16/25 | Loss: 0.00096455
Iteration 17/25 | Loss: 0.00096208
Iteration 18/25 | Loss: 0.00096104
Iteration 19/25 | Loss: 0.00096078
Iteration 20/25 | Loss: 0.00096070
Iteration 21/25 | Loss: 0.00096070
Iteration 22/25 | Loss: 0.00096070
Iteration 23/25 | Loss: 0.00096069
Iteration 24/25 | Loss: 0.00096069
Iteration 25/25 | Loss: 0.00096069

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03534281
Iteration 2/25 | Loss: 0.00034655
Iteration 3/25 | Loss: 0.00034655
Iteration 4/25 | Loss: 0.00034655
Iteration 5/25 | Loss: 0.00034655
Iteration 6/25 | Loss: 0.00034655
Iteration 7/25 | Loss: 0.00034655
Iteration 8/25 | Loss: 0.00034655
Iteration 9/25 | Loss: 0.00034655
Iteration 10/25 | Loss: 0.00034655
Iteration 11/25 | Loss: 0.00034655
Iteration 12/25 | Loss: 0.00034655
Iteration 13/25 | Loss: 0.00034655
Iteration 14/25 | Loss: 0.00034655
Iteration 15/25 | Loss: 0.00034655
Iteration 16/25 | Loss: 0.00034655
Iteration 17/25 | Loss: 0.00034655
Iteration 18/25 | Loss: 0.00034655
Iteration 19/25 | Loss: 0.00034655
Iteration 20/25 | Loss: 0.00034655
Iteration 21/25 | Loss: 0.00034655
Iteration 22/25 | Loss: 0.00034655
Iteration 23/25 | Loss: 0.00034655
Iteration 24/25 | Loss: 0.00034655
Iteration 25/25 | Loss: 0.00034655
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0003465496120043099, 0.0003465496120043099, 0.0003465496120043099, 0.0003465496120043099, 0.0003465496120043099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003465496120043099

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034655
Iteration 2/1000 | Loss: 0.00004106
Iteration 3/1000 | Loss: 0.00002928
Iteration 4/1000 | Loss: 0.00002518
Iteration 5/1000 | Loss: 0.00002406
Iteration 6/1000 | Loss: 0.00002344
Iteration 7/1000 | Loss: 0.00002278
Iteration 8/1000 | Loss: 0.00002229
Iteration 9/1000 | Loss: 0.00002191
Iteration 10/1000 | Loss: 0.00002158
Iteration 11/1000 | Loss: 0.00002132
Iteration 12/1000 | Loss: 0.00002108
Iteration 13/1000 | Loss: 0.00002089
Iteration 14/1000 | Loss: 0.00002087
Iteration 15/1000 | Loss: 0.00002080
Iteration 16/1000 | Loss: 0.00002080
Iteration 17/1000 | Loss: 0.00002079
Iteration 18/1000 | Loss: 0.00002079
Iteration 19/1000 | Loss: 0.00002078
Iteration 20/1000 | Loss: 0.00002077
Iteration 21/1000 | Loss: 0.00002071
Iteration 22/1000 | Loss: 0.00002068
Iteration 23/1000 | Loss: 0.00002063
Iteration 24/1000 | Loss: 0.00002063
Iteration 25/1000 | Loss: 0.00002060
Iteration 26/1000 | Loss: 0.00002060
Iteration 27/1000 | Loss: 0.00002059
Iteration 28/1000 | Loss: 0.00002059
Iteration 29/1000 | Loss: 0.00002058
Iteration 30/1000 | Loss: 0.00002058
Iteration 31/1000 | Loss: 0.00002058
Iteration 32/1000 | Loss: 0.00002058
Iteration 33/1000 | Loss: 0.00002058
Iteration 34/1000 | Loss: 0.00002058
Iteration 35/1000 | Loss: 0.00002058
Iteration 36/1000 | Loss: 0.00002058
Iteration 37/1000 | Loss: 0.00002058
Iteration 38/1000 | Loss: 0.00002058
Iteration 39/1000 | Loss: 0.00002057
Iteration 40/1000 | Loss: 0.00002057
Iteration 41/1000 | Loss: 0.00002057
Iteration 42/1000 | Loss: 0.00002057
Iteration 43/1000 | Loss: 0.00002056
Iteration 44/1000 | Loss: 0.00002056
Iteration 45/1000 | Loss: 0.00002056
Iteration 46/1000 | Loss: 0.00002056
Iteration 47/1000 | Loss: 0.00002056
Iteration 48/1000 | Loss: 0.00002056
Iteration 49/1000 | Loss: 0.00002055
Iteration 50/1000 | Loss: 0.00002055
Iteration 51/1000 | Loss: 0.00002055
Iteration 52/1000 | Loss: 0.00002054
Iteration 53/1000 | Loss: 0.00002054
Iteration 54/1000 | Loss: 0.00002054
Iteration 55/1000 | Loss: 0.00002054
Iteration 56/1000 | Loss: 0.00002054
Iteration 57/1000 | Loss: 0.00002053
Iteration 58/1000 | Loss: 0.00002053
Iteration 59/1000 | Loss: 0.00002053
Iteration 60/1000 | Loss: 0.00002052
Iteration 61/1000 | Loss: 0.00002052
Iteration 62/1000 | Loss: 0.00002052
Iteration 63/1000 | Loss: 0.00002052
Iteration 64/1000 | Loss: 0.00002052
Iteration 65/1000 | Loss: 0.00002052
Iteration 66/1000 | Loss: 0.00002052
Iteration 67/1000 | Loss: 0.00002052
Iteration 68/1000 | Loss: 0.00002051
Iteration 69/1000 | Loss: 0.00002051
Iteration 70/1000 | Loss: 0.00002051
Iteration 71/1000 | Loss: 0.00002051
Iteration 72/1000 | Loss: 0.00002051
Iteration 73/1000 | Loss: 0.00002051
Iteration 74/1000 | Loss: 0.00002051
Iteration 75/1000 | Loss: 0.00002050
Iteration 76/1000 | Loss: 0.00002050
Iteration 77/1000 | Loss: 0.00002050
Iteration 78/1000 | Loss: 0.00002050
Iteration 79/1000 | Loss: 0.00002050
Iteration 80/1000 | Loss: 0.00002050
Iteration 81/1000 | Loss: 0.00002050
Iteration 82/1000 | Loss: 0.00002049
Iteration 83/1000 | Loss: 0.00002049
Iteration 84/1000 | Loss: 0.00002049
Iteration 85/1000 | Loss: 0.00002049
Iteration 86/1000 | Loss: 0.00002049
Iteration 87/1000 | Loss: 0.00002049
Iteration 88/1000 | Loss: 0.00002049
Iteration 89/1000 | Loss: 0.00002049
Iteration 90/1000 | Loss: 0.00002048
Iteration 91/1000 | Loss: 0.00002048
Iteration 92/1000 | Loss: 0.00002048
Iteration 93/1000 | Loss: 0.00002048
Iteration 94/1000 | Loss: 0.00002048
Iteration 95/1000 | Loss: 0.00002048
Iteration 96/1000 | Loss: 0.00002048
Iteration 97/1000 | Loss: 0.00002048
Iteration 98/1000 | Loss: 0.00002048
Iteration 99/1000 | Loss: 0.00002048
Iteration 100/1000 | Loss: 0.00002047
Iteration 101/1000 | Loss: 0.00002047
Iteration 102/1000 | Loss: 0.00002047
Iteration 103/1000 | Loss: 0.00002047
Iteration 104/1000 | Loss: 0.00002047
Iteration 105/1000 | Loss: 0.00002047
Iteration 106/1000 | Loss: 0.00002047
Iteration 107/1000 | Loss: 0.00002047
Iteration 108/1000 | Loss: 0.00002047
Iteration 109/1000 | Loss: 0.00002047
Iteration 110/1000 | Loss: 0.00002047
Iteration 111/1000 | Loss: 0.00002047
Iteration 112/1000 | Loss: 0.00002047
Iteration 113/1000 | Loss: 0.00002046
Iteration 114/1000 | Loss: 0.00002046
Iteration 115/1000 | Loss: 0.00002046
Iteration 116/1000 | Loss: 0.00002046
Iteration 117/1000 | Loss: 0.00002046
Iteration 118/1000 | Loss: 0.00002046
Iteration 119/1000 | Loss: 0.00002046
Iteration 120/1000 | Loss: 0.00002046
Iteration 121/1000 | Loss: 0.00002046
Iteration 122/1000 | Loss: 0.00002046
Iteration 123/1000 | Loss: 0.00002046
Iteration 124/1000 | Loss: 0.00002046
Iteration 125/1000 | Loss: 0.00002046
Iteration 126/1000 | Loss: 0.00002046
Iteration 127/1000 | Loss: 0.00002046
Iteration 128/1000 | Loss: 0.00002046
Iteration 129/1000 | Loss: 0.00002046
Iteration 130/1000 | Loss: 0.00002046
Iteration 131/1000 | Loss: 0.00002046
Iteration 132/1000 | Loss: 0.00002046
Iteration 133/1000 | Loss: 0.00002046
Iteration 134/1000 | Loss: 0.00002046
Iteration 135/1000 | Loss: 0.00002046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.0456571292015724e-05, 2.0456571292015724e-05, 2.0456571292015724e-05, 2.0456571292015724e-05, 2.0456571292015724e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0456571292015724e-05

Optimization complete. Final v2v error: 3.924044370651245 mm

Highest mean error: 4.813011169433594 mm for frame 106

Lowest mean error: 3.4403562545776367 mm for frame 173

Saving results

Total time: 71.42222595214844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047925
Iteration 2/25 | Loss: 0.00289614
Iteration 3/25 | Loss: 0.00230711
Iteration 4/25 | Loss: 0.00170084
Iteration 5/25 | Loss: 0.00155383
Iteration 6/25 | Loss: 0.00149241
Iteration 7/25 | Loss: 0.00147548
Iteration 8/25 | Loss: 0.00149466
Iteration 9/25 | Loss: 0.00144635
Iteration 10/25 | Loss: 0.00160718
Iteration 11/25 | Loss: 0.00157132
Iteration 12/25 | Loss: 0.00131029
Iteration 13/25 | Loss: 0.00122078
Iteration 14/25 | Loss: 0.00123771
Iteration 15/25 | Loss: 0.00128275
Iteration 16/25 | Loss: 0.00125627
Iteration 17/25 | Loss: 0.00120826
Iteration 18/25 | Loss: 0.00114285
Iteration 19/25 | Loss: 0.00107813
Iteration 20/25 | Loss: 0.00104052
Iteration 21/25 | Loss: 0.00102761
Iteration 22/25 | Loss: 0.00101368
Iteration 23/25 | Loss: 0.00101577
Iteration 24/25 | Loss: 0.00101551
Iteration 25/25 | Loss: 0.00101052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29797292
Iteration 2/25 | Loss: 0.00053533
Iteration 3/25 | Loss: 0.00053533
Iteration 4/25 | Loss: 0.00053532
Iteration 5/25 | Loss: 0.00053532
Iteration 6/25 | Loss: 0.00053532
Iteration 7/25 | Loss: 0.00053532
Iteration 8/25 | Loss: 0.00053532
Iteration 9/25 | Loss: 0.00053532
Iteration 10/25 | Loss: 0.00053532
Iteration 11/25 | Loss: 0.00053532
Iteration 12/25 | Loss: 0.00053532
Iteration 13/25 | Loss: 0.00053532
Iteration 14/25 | Loss: 0.00053532
Iteration 15/25 | Loss: 0.00053532
Iteration 16/25 | Loss: 0.00053532
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005353228189051151, 0.0005353228189051151, 0.0005353228189051151, 0.0005353228189051151, 0.0005353228189051151]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005353228189051151

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053532
Iteration 2/1000 | Loss: 0.00004875
Iteration 3/1000 | Loss: 0.00005300
Iteration 4/1000 | Loss: 0.00003700
Iteration 5/1000 | Loss: 0.00004352
Iteration 6/1000 | Loss: 0.00023724
Iteration 7/1000 | Loss: 0.00016663
Iteration 8/1000 | Loss: 0.00011661
Iteration 9/1000 | Loss: 0.00005111
Iteration 10/1000 | Loss: 0.00004102
Iteration 11/1000 | Loss: 0.00005050
Iteration 12/1000 | Loss: 0.00003379
Iteration 13/1000 | Loss: 0.00004331
Iteration 14/1000 | Loss: 0.00002986
Iteration 15/1000 | Loss: 0.00002838
Iteration 16/1000 | Loss: 0.00003625
Iteration 17/1000 | Loss: 0.00002969
Iteration 18/1000 | Loss: 0.00002801
Iteration 19/1000 | Loss: 0.00004547
Iteration 20/1000 | Loss: 0.00002639
Iteration 21/1000 | Loss: 0.00002569
Iteration 22/1000 | Loss: 0.00002517
Iteration 23/1000 | Loss: 0.00002495
Iteration 24/1000 | Loss: 0.00002490
Iteration 25/1000 | Loss: 0.00002487
Iteration 26/1000 | Loss: 0.00002478
Iteration 27/1000 | Loss: 0.00002465
Iteration 28/1000 | Loss: 0.00002456
Iteration 29/1000 | Loss: 0.00002456
Iteration 30/1000 | Loss: 0.00002456
Iteration 31/1000 | Loss: 0.00002456
Iteration 32/1000 | Loss: 0.00002455
Iteration 33/1000 | Loss: 0.00002455
Iteration 34/1000 | Loss: 0.00002454
Iteration 35/1000 | Loss: 0.00003511
Iteration 36/1000 | Loss: 0.00002453
Iteration 37/1000 | Loss: 0.00002448
Iteration 38/1000 | Loss: 0.00002445
Iteration 39/1000 | Loss: 0.00002444
Iteration 40/1000 | Loss: 0.00002444
Iteration 41/1000 | Loss: 0.00002444
Iteration 42/1000 | Loss: 0.00002443
Iteration 43/1000 | Loss: 0.00002443
Iteration 44/1000 | Loss: 0.00002442
Iteration 45/1000 | Loss: 0.00002442
Iteration 46/1000 | Loss: 0.00002441
Iteration 47/1000 | Loss: 0.00002440
Iteration 48/1000 | Loss: 0.00002440
Iteration 49/1000 | Loss: 0.00002440
Iteration 50/1000 | Loss: 0.00002440
Iteration 51/1000 | Loss: 0.00002440
Iteration 52/1000 | Loss: 0.00002440
Iteration 53/1000 | Loss: 0.00002440
Iteration 54/1000 | Loss: 0.00002439
Iteration 55/1000 | Loss: 0.00002439
Iteration 56/1000 | Loss: 0.00002439
Iteration 57/1000 | Loss: 0.00002439
Iteration 58/1000 | Loss: 0.00002439
Iteration 59/1000 | Loss: 0.00002439
Iteration 60/1000 | Loss: 0.00002438
Iteration 61/1000 | Loss: 0.00002438
Iteration 62/1000 | Loss: 0.00002438
Iteration 63/1000 | Loss: 0.00002438
Iteration 64/1000 | Loss: 0.00002438
Iteration 65/1000 | Loss: 0.00002438
Iteration 66/1000 | Loss: 0.00002437
Iteration 67/1000 | Loss: 0.00002437
Iteration 68/1000 | Loss: 0.00002437
Iteration 69/1000 | Loss: 0.00002437
Iteration 70/1000 | Loss: 0.00002437
Iteration 71/1000 | Loss: 0.00002436
Iteration 72/1000 | Loss: 0.00002436
Iteration 73/1000 | Loss: 0.00002435
Iteration 74/1000 | Loss: 0.00002435
Iteration 75/1000 | Loss: 0.00002435
Iteration 76/1000 | Loss: 0.00002434
Iteration 77/1000 | Loss: 0.00002434
Iteration 78/1000 | Loss: 0.00002434
Iteration 79/1000 | Loss: 0.00002434
Iteration 80/1000 | Loss: 0.00019535
Iteration 81/1000 | Loss: 0.00012552
Iteration 82/1000 | Loss: 0.00003482
Iteration 83/1000 | Loss: 0.00004947
Iteration 84/1000 | Loss: 0.00003012
Iteration 85/1000 | Loss: 0.00004222
Iteration 86/1000 | Loss: 0.00010692
Iteration 87/1000 | Loss: 0.00003568
Iteration 88/1000 | Loss: 0.00003067
Iteration 89/1000 | Loss: 0.00002809
Iteration 90/1000 | Loss: 0.00007192
Iteration 91/1000 | Loss: 0.00003443
Iteration 92/1000 | Loss: 0.00002658
Iteration 93/1000 | Loss: 0.00002606
Iteration 94/1000 | Loss: 0.00002559
Iteration 95/1000 | Loss: 0.00002532
Iteration 96/1000 | Loss: 0.00002531
Iteration 97/1000 | Loss: 0.00003757
Iteration 98/1000 | Loss: 0.00002508
Iteration 99/1000 | Loss: 0.00002500
Iteration 100/1000 | Loss: 0.00002500
Iteration 101/1000 | Loss: 0.00002499
Iteration 102/1000 | Loss: 0.00002498
Iteration 103/1000 | Loss: 0.00002476
Iteration 104/1000 | Loss: 0.00005068
Iteration 105/1000 | Loss: 0.00004544
Iteration 106/1000 | Loss: 0.00003236
Iteration 107/1000 | Loss: 0.00004532
Iteration 108/1000 | Loss: 0.00006495
Iteration 109/1000 | Loss: 0.00003849
Iteration 110/1000 | Loss: 0.00003088
Iteration 111/1000 | Loss: 0.00004341
Iteration 112/1000 | Loss: 0.00002804
Iteration 113/1000 | Loss: 0.00003684
Iteration 114/1000 | Loss: 0.00004177
Iteration 115/1000 | Loss: 0.00002634
Iteration 116/1000 | Loss: 0.00002539
Iteration 117/1000 | Loss: 0.00002465
Iteration 118/1000 | Loss: 0.00003355
Iteration 119/1000 | Loss: 0.00002711
Iteration 120/1000 | Loss: 0.00003007
Iteration 121/1000 | Loss: 0.00002414
Iteration 122/1000 | Loss: 0.00002412
Iteration 123/1000 | Loss: 0.00002411
Iteration 124/1000 | Loss: 0.00002411
Iteration 125/1000 | Loss: 0.00002411
Iteration 126/1000 | Loss: 0.00002411
Iteration 127/1000 | Loss: 0.00002411
Iteration 128/1000 | Loss: 0.00002411
Iteration 129/1000 | Loss: 0.00002410
Iteration 130/1000 | Loss: 0.00002410
Iteration 131/1000 | Loss: 0.00002410
Iteration 132/1000 | Loss: 0.00002410
Iteration 133/1000 | Loss: 0.00002410
Iteration 134/1000 | Loss: 0.00002410
Iteration 135/1000 | Loss: 0.00002410
Iteration 136/1000 | Loss: 0.00002410
Iteration 137/1000 | Loss: 0.00002410
Iteration 138/1000 | Loss: 0.00002410
Iteration 139/1000 | Loss: 0.00002410
Iteration 140/1000 | Loss: 0.00002410
Iteration 141/1000 | Loss: 0.00002410
Iteration 142/1000 | Loss: 0.00002410
Iteration 143/1000 | Loss: 0.00002410
Iteration 144/1000 | Loss: 0.00002409
Iteration 145/1000 | Loss: 0.00002409
Iteration 146/1000 | Loss: 0.00002409
Iteration 147/1000 | Loss: 0.00002409
Iteration 148/1000 | Loss: 0.00002409
Iteration 149/1000 | Loss: 0.00002409
Iteration 150/1000 | Loss: 0.00002409
Iteration 151/1000 | Loss: 0.00002409
Iteration 152/1000 | Loss: 0.00002409
Iteration 153/1000 | Loss: 0.00002409
Iteration 154/1000 | Loss: 0.00002409
Iteration 155/1000 | Loss: 0.00002408
Iteration 156/1000 | Loss: 0.00002408
Iteration 157/1000 | Loss: 0.00002408
Iteration 158/1000 | Loss: 0.00002407
Iteration 159/1000 | Loss: 0.00002407
Iteration 160/1000 | Loss: 0.00002404
Iteration 161/1000 | Loss: 0.00002404
Iteration 162/1000 | Loss: 0.00002404
Iteration 163/1000 | Loss: 0.00002404
Iteration 164/1000 | Loss: 0.00002404
Iteration 165/1000 | Loss: 0.00002404
Iteration 166/1000 | Loss: 0.00002403
Iteration 167/1000 | Loss: 0.00002403
Iteration 168/1000 | Loss: 0.00002403
Iteration 169/1000 | Loss: 0.00002403
Iteration 170/1000 | Loss: 0.00003097
Iteration 171/1000 | Loss: 0.00002429
Iteration 172/1000 | Loss: 0.00002400
Iteration 173/1000 | Loss: 0.00002398
Iteration 174/1000 | Loss: 0.00002396
Iteration 175/1000 | Loss: 0.00002396
Iteration 176/1000 | Loss: 0.00002396
Iteration 177/1000 | Loss: 0.00002396
Iteration 178/1000 | Loss: 0.00002396
Iteration 179/1000 | Loss: 0.00002396
Iteration 180/1000 | Loss: 0.00002396
Iteration 181/1000 | Loss: 0.00002396
Iteration 182/1000 | Loss: 0.00002395
Iteration 183/1000 | Loss: 0.00002395
Iteration 184/1000 | Loss: 0.00002395
Iteration 185/1000 | Loss: 0.00002395
Iteration 186/1000 | Loss: 0.00002395
Iteration 187/1000 | Loss: 0.00002395
Iteration 188/1000 | Loss: 0.00002395
Iteration 189/1000 | Loss: 0.00002395
Iteration 190/1000 | Loss: 0.00002395
Iteration 191/1000 | Loss: 0.00002395
Iteration 192/1000 | Loss: 0.00002395
Iteration 193/1000 | Loss: 0.00002395
Iteration 194/1000 | Loss: 0.00002395
Iteration 195/1000 | Loss: 0.00002395
Iteration 196/1000 | Loss: 0.00002395
Iteration 197/1000 | Loss: 0.00002395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.3950336981215514e-05, 2.3950336981215514e-05, 2.3950336981215514e-05, 2.3950336981215514e-05, 2.3950336981215514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3950336981215514e-05

Optimization complete. Final v2v error: 4.001727104187012 mm

Highest mean error: 4.821769714355469 mm for frame 103

Lowest mean error: 3.65130615234375 mm for frame 198

Saving results

Total time: 153.3036527633667
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01186344
Iteration 2/25 | Loss: 0.00362481
Iteration 3/25 | Loss: 0.00261237
Iteration 4/25 | Loss: 0.00166529
Iteration 5/25 | Loss: 0.00145543
Iteration 6/25 | Loss: 0.00165255
Iteration 7/25 | Loss: 0.00151327
Iteration 8/25 | Loss: 0.00144490
Iteration 9/25 | Loss: 0.00129318
Iteration 10/25 | Loss: 0.00126233
Iteration 11/25 | Loss: 0.00121181
Iteration 12/25 | Loss: 0.00118755
Iteration 13/25 | Loss: 0.00116597
Iteration 14/25 | Loss: 0.00115254
Iteration 15/25 | Loss: 0.00115178
Iteration 16/25 | Loss: 0.00114705
Iteration 17/25 | Loss: 0.00114306
Iteration 18/25 | Loss: 0.00113442
Iteration 19/25 | Loss: 0.00112875
Iteration 20/25 | Loss: 0.00113080
Iteration 21/25 | Loss: 0.00112167
Iteration 22/25 | Loss: 0.00112548
Iteration 23/25 | Loss: 0.00112932
Iteration 24/25 | Loss: 0.00112608
Iteration 25/25 | Loss: 0.00112627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76985455
Iteration 2/25 | Loss: 0.00565587
Iteration 3/25 | Loss: 0.00106645
Iteration 4/25 | Loss: 0.00106645
Iteration 5/25 | Loss: 0.00106645
Iteration 6/25 | Loss: 0.00106645
Iteration 7/25 | Loss: 0.00106645
Iteration 8/25 | Loss: 0.00106645
Iteration 9/25 | Loss: 0.00106645
Iteration 10/25 | Loss: 0.00106645
Iteration 11/25 | Loss: 0.00106645
Iteration 12/25 | Loss: 0.00106645
Iteration 13/25 | Loss: 0.00106645
Iteration 14/25 | Loss: 0.00106645
Iteration 15/25 | Loss: 0.00106645
Iteration 16/25 | Loss: 0.00106645
Iteration 17/25 | Loss: 0.00106645
Iteration 18/25 | Loss: 0.00106645
Iteration 19/25 | Loss: 0.00106645
Iteration 20/25 | Loss: 0.00106645
Iteration 21/25 | Loss: 0.00106645
Iteration 22/25 | Loss: 0.00106645
Iteration 23/25 | Loss: 0.00106645
Iteration 24/25 | Loss: 0.00106645
Iteration 25/25 | Loss: 0.00106645

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106645
Iteration 2/1000 | Loss: 0.00040453
Iteration 3/1000 | Loss: 0.00037380
Iteration 4/1000 | Loss: 0.00073395
Iteration 5/1000 | Loss: 0.00057462
Iteration 6/1000 | Loss: 0.00039179
Iteration 7/1000 | Loss: 0.00044677
Iteration 8/1000 | Loss: 0.00102856
Iteration 9/1000 | Loss: 0.00174374
Iteration 10/1000 | Loss: 0.00055942
Iteration 11/1000 | Loss: 0.00040147
Iteration 12/1000 | Loss: 0.00031137
Iteration 13/1000 | Loss: 0.00041570
Iteration 14/1000 | Loss: 0.00095268
Iteration 15/1000 | Loss: 0.00078388
Iteration 16/1000 | Loss: 0.00139568
Iteration 17/1000 | Loss: 0.00038208
Iteration 18/1000 | Loss: 0.00012253
Iteration 19/1000 | Loss: 0.00051796
Iteration 20/1000 | Loss: 0.00017073
Iteration 21/1000 | Loss: 0.00014111
Iteration 22/1000 | Loss: 0.00030927
Iteration 23/1000 | Loss: 0.00015333
Iteration 24/1000 | Loss: 0.00185737
Iteration 25/1000 | Loss: 0.00144763
Iteration 26/1000 | Loss: 0.00113477
Iteration 27/1000 | Loss: 0.00087610
Iteration 28/1000 | Loss: 0.00009849
Iteration 29/1000 | Loss: 0.00039464
Iteration 30/1000 | Loss: 0.00021927
Iteration 31/1000 | Loss: 0.00007528
Iteration 32/1000 | Loss: 0.00089794
Iteration 33/1000 | Loss: 0.00262085
Iteration 34/1000 | Loss: 0.00218774
Iteration 35/1000 | Loss: 0.00180953
Iteration 36/1000 | Loss: 0.00033706
Iteration 37/1000 | Loss: 0.00017875
Iteration 38/1000 | Loss: 0.00015859
Iteration 39/1000 | Loss: 0.00024507
Iteration 40/1000 | Loss: 0.00088129
Iteration 41/1000 | Loss: 0.00056832
Iteration 42/1000 | Loss: 0.00022258
Iteration 43/1000 | Loss: 0.00011455
Iteration 44/1000 | Loss: 0.00011753
Iteration 45/1000 | Loss: 0.00050384
Iteration 46/1000 | Loss: 0.00010861
Iteration 47/1000 | Loss: 0.00030355
Iteration 48/1000 | Loss: 0.00063592
Iteration 49/1000 | Loss: 0.00037617
Iteration 50/1000 | Loss: 0.00021904
Iteration 51/1000 | Loss: 0.00060785
Iteration 52/1000 | Loss: 0.00018600
Iteration 53/1000 | Loss: 0.00012295
Iteration 54/1000 | Loss: 0.00014869
Iteration 55/1000 | Loss: 0.00024485
Iteration 56/1000 | Loss: 0.00014952
Iteration 57/1000 | Loss: 0.00005405
Iteration 58/1000 | Loss: 0.00036233
Iteration 59/1000 | Loss: 0.00007516
Iteration 60/1000 | Loss: 0.00018460
Iteration 61/1000 | Loss: 0.00007323
Iteration 62/1000 | Loss: 0.00004843
Iteration 63/1000 | Loss: 0.00004605
Iteration 64/1000 | Loss: 0.00004814
Iteration 65/1000 | Loss: 0.00004398
Iteration 66/1000 | Loss: 0.00004213
Iteration 67/1000 | Loss: 0.00004455
Iteration 68/1000 | Loss: 0.00004322
Iteration 69/1000 | Loss: 0.00016003
Iteration 70/1000 | Loss: 0.00004938
Iteration 71/1000 | Loss: 0.00035906
Iteration 72/1000 | Loss: 0.00031035
Iteration 73/1000 | Loss: 0.00036013
Iteration 74/1000 | Loss: 0.00012269
Iteration 75/1000 | Loss: 0.00006497
Iteration 76/1000 | Loss: 0.00023394
Iteration 77/1000 | Loss: 0.00041693
Iteration 78/1000 | Loss: 0.00008818
Iteration 79/1000 | Loss: 0.00004771
Iteration 80/1000 | Loss: 0.00004290
Iteration 81/1000 | Loss: 0.00005608
Iteration 82/1000 | Loss: 0.00016861
Iteration 83/1000 | Loss: 0.00004451
Iteration 84/1000 | Loss: 0.00004209
Iteration 85/1000 | Loss: 0.00003962
Iteration 86/1000 | Loss: 0.00003840
Iteration 87/1000 | Loss: 0.00015378
Iteration 88/1000 | Loss: 0.00021101
Iteration 89/1000 | Loss: 0.00012299
Iteration 90/1000 | Loss: 0.00025286
Iteration 91/1000 | Loss: 0.00042790
Iteration 92/1000 | Loss: 0.00011603
Iteration 93/1000 | Loss: 0.00018373
Iteration 94/1000 | Loss: 0.00018516
Iteration 95/1000 | Loss: 0.00004763
Iteration 96/1000 | Loss: 0.00012015
Iteration 97/1000 | Loss: 0.00004203
Iteration 98/1000 | Loss: 0.00021748
Iteration 99/1000 | Loss: 0.00022044
Iteration 100/1000 | Loss: 0.00006054
Iteration 101/1000 | Loss: 0.00005702
Iteration 102/1000 | Loss: 0.00004369
Iteration 103/1000 | Loss: 0.00021122
Iteration 104/1000 | Loss: 0.00005010
Iteration 105/1000 | Loss: 0.00059114
Iteration 106/1000 | Loss: 0.00007269
Iteration 107/1000 | Loss: 0.00022282
Iteration 108/1000 | Loss: 0.00045470
Iteration 109/1000 | Loss: 0.00043288
Iteration 110/1000 | Loss: 0.00004477
Iteration 111/1000 | Loss: 0.00004027
Iteration 112/1000 | Loss: 0.00003810
Iteration 113/1000 | Loss: 0.00003711
Iteration 114/1000 | Loss: 0.00003635
Iteration 115/1000 | Loss: 0.00042661
Iteration 116/1000 | Loss: 0.00003676
Iteration 117/1000 | Loss: 0.00003545
Iteration 118/1000 | Loss: 0.00003508
Iteration 119/1000 | Loss: 0.00003483
Iteration 120/1000 | Loss: 0.00003463
Iteration 121/1000 | Loss: 0.00003460
Iteration 122/1000 | Loss: 0.00003458
Iteration 123/1000 | Loss: 0.00003458
Iteration 124/1000 | Loss: 0.00003457
Iteration 125/1000 | Loss: 0.00003457
Iteration 126/1000 | Loss: 0.00003456
Iteration 127/1000 | Loss: 0.00003454
Iteration 128/1000 | Loss: 0.00003448
Iteration 129/1000 | Loss: 0.00003448
Iteration 130/1000 | Loss: 0.00003447
Iteration 131/1000 | Loss: 0.00003445
Iteration 132/1000 | Loss: 0.00003444
Iteration 133/1000 | Loss: 0.00003444
Iteration 134/1000 | Loss: 0.00003443
Iteration 135/1000 | Loss: 0.00003436
Iteration 136/1000 | Loss: 0.00003435
Iteration 137/1000 | Loss: 0.00003435
Iteration 138/1000 | Loss: 0.00003434
Iteration 139/1000 | Loss: 0.00003433
Iteration 140/1000 | Loss: 0.00003431
Iteration 141/1000 | Loss: 0.00003431
Iteration 142/1000 | Loss: 0.00003431
Iteration 143/1000 | Loss: 0.00003431
Iteration 144/1000 | Loss: 0.00003431
Iteration 145/1000 | Loss: 0.00003431
Iteration 146/1000 | Loss: 0.00003428
Iteration 147/1000 | Loss: 0.00003427
Iteration 148/1000 | Loss: 0.00003424
Iteration 149/1000 | Loss: 0.00003423
Iteration 150/1000 | Loss: 0.00003422
Iteration 151/1000 | Loss: 0.00003421
Iteration 152/1000 | Loss: 0.00003421
Iteration 153/1000 | Loss: 0.00003420
Iteration 154/1000 | Loss: 0.00003420
Iteration 155/1000 | Loss: 0.00003419
Iteration 156/1000 | Loss: 0.00003419
Iteration 157/1000 | Loss: 0.00003419
Iteration 158/1000 | Loss: 0.00003418
Iteration 159/1000 | Loss: 0.00003418
Iteration 160/1000 | Loss: 0.00003418
Iteration 161/1000 | Loss: 0.00003418
Iteration 162/1000 | Loss: 0.00003418
Iteration 163/1000 | Loss: 0.00003418
Iteration 164/1000 | Loss: 0.00003418
Iteration 165/1000 | Loss: 0.00003418
Iteration 166/1000 | Loss: 0.00003418
Iteration 167/1000 | Loss: 0.00003418
Iteration 168/1000 | Loss: 0.00003418
Iteration 169/1000 | Loss: 0.00003418
Iteration 170/1000 | Loss: 0.00003417
Iteration 171/1000 | Loss: 0.00003417
Iteration 172/1000 | Loss: 0.00003417
Iteration 173/1000 | Loss: 0.00003417
Iteration 174/1000 | Loss: 0.00003417
Iteration 175/1000 | Loss: 0.00003417
Iteration 176/1000 | Loss: 0.00003417
Iteration 177/1000 | Loss: 0.00003417
Iteration 178/1000 | Loss: 0.00003416
Iteration 179/1000 | Loss: 0.00003416
Iteration 180/1000 | Loss: 0.00003416
Iteration 181/1000 | Loss: 0.00003416
Iteration 182/1000 | Loss: 0.00003416
Iteration 183/1000 | Loss: 0.00003416
Iteration 184/1000 | Loss: 0.00003416
Iteration 185/1000 | Loss: 0.00003416
Iteration 186/1000 | Loss: 0.00003416
Iteration 187/1000 | Loss: 0.00003416
Iteration 188/1000 | Loss: 0.00003416
Iteration 189/1000 | Loss: 0.00003416
Iteration 190/1000 | Loss: 0.00003415
Iteration 191/1000 | Loss: 0.00003415
Iteration 192/1000 | Loss: 0.00003415
Iteration 193/1000 | Loss: 0.00003415
Iteration 194/1000 | Loss: 0.00003415
Iteration 195/1000 | Loss: 0.00003415
Iteration 196/1000 | Loss: 0.00003415
Iteration 197/1000 | Loss: 0.00003414
Iteration 198/1000 | Loss: 0.00003414
Iteration 199/1000 | Loss: 0.00003414
Iteration 200/1000 | Loss: 0.00003414
Iteration 201/1000 | Loss: 0.00003414
Iteration 202/1000 | Loss: 0.00003414
Iteration 203/1000 | Loss: 0.00003414
Iteration 204/1000 | Loss: 0.00003414
Iteration 205/1000 | Loss: 0.00003414
Iteration 206/1000 | Loss: 0.00003414
Iteration 207/1000 | Loss: 0.00003414
Iteration 208/1000 | Loss: 0.00003414
Iteration 209/1000 | Loss: 0.00003414
Iteration 210/1000 | Loss: 0.00003414
Iteration 211/1000 | Loss: 0.00003414
Iteration 212/1000 | Loss: 0.00003414
Iteration 213/1000 | Loss: 0.00003414
Iteration 214/1000 | Loss: 0.00003414
Iteration 215/1000 | Loss: 0.00003414
Iteration 216/1000 | Loss: 0.00003414
Iteration 217/1000 | Loss: 0.00003414
Iteration 218/1000 | Loss: 0.00003414
Iteration 219/1000 | Loss: 0.00003414
Iteration 220/1000 | Loss: 0.00003414
Iteration 221/1000 | Loss: 0.00003414
Iteration 222/1000 | Loss: 0.00003414
Iteration 223/1000 | Loss: 0.00003414
Iteration 224/1000 | Loss: 0.00003414
Iteration 225/1000 | Loss: 0.00003414
Iteration 226/1000 | Loss: 0.00003414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [3.41404156642966e-05, 3.41404156642966e-05, 3.41404156642966e-05, 3.41404156642966e-05, 3.41404156642966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.41404156642966e-05

Optimization complete. Final v2v error: 4.548140525817871 mm

Highest mean error: 21.594478607177734 mm for frame 188

Lowest mean error: 3.5971195697784424 mm for frame 77

Saving results

Total time: 256.2924048900604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440980
Iteration 2/25 | Loss: 0.00116816
Iteration 3/25 | Loss: 0.00098849
Iteration 4/25 | Loss: 0.00095119
Iteration 5/25 | Loss: 0.00093641
Iteration 6/25 | Loss: 0.00093365
Iteration 7/25 | Loss: 0.00093301
Iteration 8/25 | Loss: 0.00093301
Iteration 9/25 | Loss: 0.00093301
Iteration 10/25 | Loss: 0.00093301
Iteration 11/25 | Loss: 0.00093301
Iteration 12/25 | Loss: 0.00093301
Iteration 13/25 | Loss: 0.00093301
Iteration 14/25 | Loss: 0.00093301
Iteration 15/25 | Loss: 0.00093301
Iteration 16/25 | Loss: 0.00093301
Iteration 17/25 | Loss: 0.00093301
Iteration 18/25 | Loss: 0.00093301
Iteration 19/25 | Loss: 0.00093301
Iteration 20/25 | Loss: 0.00093301
Iteration 21/25 | Loss: 0.00093301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009330051834695041, 0.0009330051834695041, 0.0009330051834695041, 0.0009330051834695041, 0.0009330051834695041]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009330051834695041

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33933640
Iteration 2/25 | Loss: 0.00031271
Iteration 3/25 | Loss: 0.00031270
Iteration 4/25 | Loss: 0.00031270
Iteration 5/25 | Loss: 0.00031270
Iteration 6/25 | Loss: 0.00031270
Iteration 7/25 | Loss: 0.00031270
Iteration 8/25 | Loss: 0.00031270
Iteration 9/25 | Loss: 0.00031270
Iteration 10/25 | Loss: 0.00031270
Iteration 11/25 | Loss: 0.00031270
Iteration 12/25 | Loss: 0.00031270
Iteration 13/25 | Loss: 0.00031270
Iteration 14/25 | Loss: 0.00031270
Iteration 15/25 | Loss: 0.00031270
Iteration 16/25 | Loss: 0.00031270
Iteration 17/25 | Loss: 0.00031270
Iteration 18/25 | Loss: 0.00031270
Iteration 19/25 | Loss: 0.00031270
Iteration 20/25 | Loss: 0.00031270
Iteration 21/25 | Loss: 0.00031270
Iteration 22/25 | Loss: 0.00031270
Iteration 23/25 | Loss: 0.00031270
Iteration 24/25 | Loss: 0.00031270
Iteration 25/25 | Loss: 0.00031270

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031270
Iteration 2/1000 | Loss: 0.00003832
Iteration 3/1000 | Loss: 0.00002462
Iteration 4/1000 | Loss: 0.00002227
Iteration 5/1000 | Loss: 0.00002134
Iteration 6/1000 | Loss: 0.00002079
Iteration 7/1000 | Loss: 0.00002038
Iteration 8/1000 | Loss: 0.00002008
Iteration 9/1000 | Loss: 0.00001979
Iteration 10/1000 | Loss: 0.00001969
Iteration 11/1000 | Loss: 0.00001967
Iteration 12/1000 | Loss: 0.00001967
Iteration 13/1000 | Loss: 0.00001966
Iteration 14/1000 | Loss: 0.00001964
Iteration 15/1000 | Loss: 0.00001964
Iteration 16/1000 | Loss: 0.00001962
Iteration 17/1000 | Loss: 0.00001961
Iteration 18/1000 | Loss: 0.00001961
Iteration 19/1000 | Loss: 0.00001960
Iteration 20/1000 | Loss: 0.00001959
Iteration 21/1000 | Loss: 0.00001958
Iteration 22/1000 | Loss: 0.00001956
Iteration 23/1000 | Loss: 0.00001955
Iteration 24/1000 | Loss: 0.00001955
Iteration 25/1000 | Loss: 0.00001955
Iteration 26/1000 | Loss: 0.00001954
Iteration 27/1000 | Loss: 0.00001954
Iteration 28/1000 | Loss: 0.00001952
Iteration 29/1000 | Loss: 0.00001951
Iteration 30/1000 | Loss: 0.00001951
Iteration 31/1000 | Loss: 0.00001950
Iteration 32/1000 | Loss: 0.00001950
Iteration 33/1000 | Loss: 0.00001950
Iteration 34/1000 | Loss: 0.00001950
Iteration 35/1000 | Loss: 0.00001950
Iteration 36/1000 | Loss: 0.00001950
Iteration 37/1000 | Loss: 0.00001950
Iteration 38/1000 | Loss: 0.00001949
Iteration 39/1000 | Loss: 0.00001949
Iteration 40/1000 | Loss: 0.00001949
Iteration 41/1000 | Loss: 0.00001949
Iteration 42/1000 | Loss: 0.00001949
Iteration 43/1000 | Loss: 0.00001949
Iteration 44/1000 | Loss: 0.00001948
Iteration 45/1000 | Loss: 0.00001948
Iteration 46/1000 | Loss: 0.00001947
Iteration 47/1000 | Loss: 0.00001947
Iteration 48/1000 | Loss: 0.00001947
Iteration 49/1000 | Loss: 0.00001947
Iteration 50/1000 | Loss: 0.00001947
Iteration 51/1000 | Loss: 0.00001947
Iteration 52/1000 | Loss: 0.00001946
Iteration 53/1000 | Loss: 0.00001946
Iteration 54/1000 | Loss: 0.00001946
Iteration 55/1000 | Loss: 0.00001946
Iteration 56/1000 | Loss: 0.00001946
Iteration 57/1000 | Loss: 0.00001946
Iteration 58/1000 | Loss: 0.00001946
Iteration 59/1000 | Loss: 0.00001946
Iteration 60/1000 | Loss: 0.00001945
Iteration 61/1000 | Loss: 0.00001945
Iteration 62/1000 | Loss: 0.00001945
Iteration 63/1000 | Loss: 0.00001945
Iteration 64/1000 | Loss: 0.00001945
Iteration 65/1000 | Loss: 0.00001945
Iteration 66/1000 | Loss: 0.00001945
Iteration 67/1000 | Loss: 0.00001945
Iteration 68/1000 | Loss: 0.00001945
Iteration 69/1000 | Loss: 0.00001945
Iteration 70/1000 | Loss: 0.00001945
Iteration 71/1000 | Loss: 0.00001945
Iteration 72/1000 | Loss: 0.00001944
Iteration 73/1000 | Loss: 0.00001944
Iteration 74/1000 | Loss: 0.00001944
Iteration 75/1000 | Loss: 0.00001944
Iteration 76/1000 | Loss: 0.00001944
Iteration 77/1000 | Loss: 0.00001944
Iteration 78/1000 | Loss: 0.00001944
Iteration 79/1000 | Loss: 0.00001944
Iteration 80/1000 | Loss: 0.00001944
Iteration 81/1000 | Loss: 0.00001944
Iteration 82/1000 | Loss: 0.00001944
Iteration 83/1000 | Loss: 0.00001943
Iteration 84/1000 | Loss: 0.00001943
Iteration 85/1000 | Loss: 0.00001943
Iteration 86/1000 | Loss: 0.00001943
Iteration 87/1000 | Loss: 0.00001942
Iteration 88/1000 | Loss: 0.00001942
Iteration 89/1000 | Loss: 0.00001942
Iteration 90/1000 | Loss: 0.00001942
Iteration 91/1000 | Loss: 0.00001942
Iteration 92/1000 | Loss: 0.00001942
Iteration 93/1000 | Loss: 0.00001942
Iteration 94/1000 | Loss: 0.00001941
Iteration 95/1000 | Loss: 0.00001941
Iteration 96/1000 | Loss: 0.00001941
Iteration 97/1000 | Loss: 0.00001941
Iteration 98/1000 | Loss: 0.00001941
Iteration 99/1000 | Loss: 0.00001941
Iteration 100/1000 | Loss: 0.00001941
Iteration 101/1000 | Loss: 0.00001941
Iteration 102/1000 | Loss: 0.00001941
Iteration 103/1000 | Loss: 0.00001941
Iteration 104/1000 | Loss: 0.00001940
Iteration 105/1000 | Loss: 0.00001940
Iteration 106/1000 | Loss: 0.00001940
Iteration 107/1000 | Loss: 0.00001940
Iteration 108/1000 | Loss: 0.00001940
Iteration 109/1000 | Loss: 0.00001940
Iteration 110/1000 | Loss: 0.00001939
Iteration 111/1000 | Loss: 0.00001939
Iteration 112/1000 | Loss: 0.00001939
Iteration 113/1000 | Loss: 0.00001939
Iteration 114/1000 | Loss: 0.00001939
Iteration 115/1000 | Loss: 0.00001939
Iteration 116/1000 | Loss: 0.00001939
Iteration 117/1000 | Loss: 0.00001938
Iteration 118/1000 | Loss: 0.00001938
Iteration 119/1000 | Loss: 0.00001938
Iteration 120/1000 | Loss: 0.00001938
Iteration 121/1000 | Loss: 0.00001938
Iteration 122/1000 | Loss: 0.00001938
Iteration 123/1000 | Loss: 0.00001938
Iteration 124/1000 | Loss: 0.00001938
Iteration 125/1000 | Loss: 0.00001937
Iteration 126/1000 | Loss: 0.00001937
Iteration 127/1000 | Loss: 0.00001937
Iteration 128/1000 | Loss: 0.00001937
Iteration 129/1000 | Loss: 0.00001937
Iteration 130/1000 | Loss: 0.00001937
Iteration 131/1000 | Loss: 0.00001937
Iteration 132/1000 | Loss: 0.00001937
Iteration 133/1000 | Loss: 0.00001937
Iteration 134/1000 | Loss: 0.00001937
Iteration 135/1000 | Loss: 0.00001937
Iteration 136/1000 | Loss: 0.00001937
Iteration 137/1000 | Loss: 0.00001937
Iteration 138/1000 | Loss: 0.00001937
Iteration 139/1000 | Loss: 0.00001937
Iteration 140/1000 | Loss: 0.00001937
Iteration 141/1000 | Loss: 0.00001937
Iteration 142/1000 | Loss: 0.00001937
Iteration 143/1000 | Loss: 0.00001937
Iteration 144/1000 | Loss: 0.00001937
Iteration 145/1000 | Loss: 0.00001937
Iteration 146/1000 | Loss: 0.00001937
Iteration 147/1000 | Loss: 0.00001937
Iteration 148/1000 | Loss: 0.00001937
Iteration 149/1000 | Loss: 0.00001937
Iteration 150/1000 | Loss: 0.00001937
Iteration 151/1000 | Loss: 0.00001937
Iteration 152/1000 | Loss: 0.00001937
Iteration 153/1000 | Loss: 0.00001937
Iteration 154/1000 | Loss: 0.00001937
Iteration 155/1000 | Loss: 0.00001937
Iteration 156/1000 | Loss: 0.00001937
Iteration 157/1000 | Loss: 0.00001937
Iteration 158/1000 | Loss: 0.00001937
Iteration 159/1000 | Loss: 0.00001937
Iteration 160/1000 | Loss: 0.00001937
Iteration 161/1000 | Loss: 0.00001937
Iteration 162/1000 | Loss: 0.00001937
Iteration 163/1000 | Loss: 0.00001937
Iteration 164/1000 | Loss: 0.00001937
Iteration 165/1000 | Loss: 0.00001937
Iteration 166/1000 | Loss: 0.00001937
Iteration 167/1000 | Loss: 0.00001937
Iteration 168/1000 | Loss: 0.00001937
Iteration 169/1000 | Loss: 0.00001937
Iteration 170/1000 | Loss: 0.00001937
Iteration 171/1000 | Loss: 0.00001937
Iteration 172/1000 | Loss: 0.00001937
Iteration 173/1000 | Loss: 0.00001937
Iteration 174/1000 | Loss: 0.00001937
Iteration 175/1000 | Loss: 0.00001937
Iteration 176/1000 | Loss: 0.00001937
Iteration 177/1000 | Loss: 0.00001937
Iteration 178/1000 | Loss: 0.00001937
Iteration 179/1000 | Loss: 0.00001937
Iteration 180/1000 | Loss: 0.00001937
Iteration 181/1000 | Loss: 0.00001937
Iteration 182/1000 | Loss: 0.00001937
Iteration 183/1000 | Loss: 0.00001937
Iteration 184/1000 | Loss: 0.00001937
Iteration 185/1000 | Loss: 0.00001937
Iteration 186/1000 | Loss: 0.00001937
Iteration 187/1000 | Loss: 0.00001937
Iteration 188/1000 | Loss: 0.00001937
Iteration 189/1000 | Loss: 0.00001937
Iteration 190/1000 | Loss: 0.00001937
Iteration 191/1000 | Loss: 0.00001937
Iteration 192/1000 | Loss: 0.00001937
Iteration 193/1000 | Loss: 0.00001937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.936544867930934e-05, 1.936544867930934e-05, 1.936544867930934e-05, 1.936544867930934e-05, 1.936544867930934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.936544867930934e-05

Optimization complete. Final v2v error: 3.8285107612609863 mm

Highest mean error: 4.406149387359619 mm for frame 0

Lowest mean error: 3.324204683303833 mm for frame 116

Saving results

Total time: 39.41179895401001
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917277
Iteration 2/25 | Loss: 0.00213189
Iteration 3/25 | Loss: 0.00126920
Iteration 4/25 | Loss: 0.00117296
Iteration 5/25 | Loss: 0.00114665
Iteration 6/25 | Loss: 0.00112945
Iteration 7/25 | Loss: 0.00108142
Iteration 8/25 | Loss: 0.00106280
Iteration 9/25 | Loss: 0.00105422
Iteration 10/25 | Loss: 0.00104780
Iteration 11/25 | Loss: 0.00104831
Iteration 12/25 | Loss: 0.00104799
Iteration 13/25 | Loss: 0.00104199
Iteration 14/25 | Loss: 0.00104256
Iteration 15/25 | Loss: 0.00104472
Iteration 16/25 | Loss: 0.00104137
Iteration 17/25 | Loss: 0.00104171
Iteration 18/25 | Loss: 0.00104077
Iteration 19/25 | Loss: 0.00103953
Iteration 20/25 | Loss: 0.00103870
Iteration 21/25 | Loss: 0.00103845
Iteration 22/25 | Loss: 0.00103610
Iteration 23/25 | Loss: 0.00103435
Iteration 24/25 | Loss: 0.00103434
Iteration 25/25 | Loss: 0.00103481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41748548
Iteration 2/25 | Loss: 0.00052057
Iteration 3/25 | Loss: 0.00052055
Iteration 4/25 | Loss: 0.00052055
Iteration 5/25 | Loss: 0.00052055
Iteration 6/25 | Loss: 0.00052055
Iteration 7/25 | Loss: 0.00052055
Iteration 8/25 | Loss: 0.00052055
Iteration 9/25 | Loss: 0.00052055
Iteration 10/25 | Loss: 0.00052055
Iteration 11/25 | Loss: 0.00052055
Iteration 12/25 | Loss: 0.00052055
Iteration 13/25 | Loss: 0.00052055
Iteration 14/25 | Loss: 0.00052055
Iteration 15/25 | Loss: 0.00052055
Iteration 16/25 | Loss: 0.00052055
Iteration 17/25 | Loss: 0.00052055
Iteration 18/25 | Loss: 0.00052055
Iteration 19/25 | Loss: 0.00052055
Iteration 20/25 | Loss: 0.00052055
Iteration 21/25 | Loss: 0.00052055
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005205511697567999, 0.0005205511697567999, 0.0005205511697567999, 0.0005205511697567999, 0.0005205511697567999]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005205511697567999

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052055
Iteration 2/1000 | Loss: 0.00012364
Iteration 3/1000 | Loss: 0.00017360
Iteration 4/1000 | Loss: 0.00012983
Iteration 5/1000 | Loss: 0.00013005
Iteration 6/1000 | Loss: 0.00014564
Iteration 7/1000 | Loss: 0.00015481
Iteration 8/1000 | Loss: 0.00016269
Iteration 9/1000 | Loss: 0.00015329
Iteration 10/1000 | Loss: 0.00008842
Iteration 11/1000 | Loss: 0.00010890
Iteration 12/1000 | Loss: 0.00009885
Iteration 13/1000 | Loss: 0.00015939
Iteration 14/1000 | Loss: 0.00012097
Iteration 15/1000 | Loss: 0.00009564
Iteration 16/1000 | Loss: 0.00011337
Iteration 17/1000 | Loss: 0.00014491
Iteration 18/1000 | Loss: 0.00018508
Iteration 19/1000 | Loss: 0.00012110
Iteration 20/1000 | Loss: 0.00011946
Iteration 21/1000 | Loss: 0.00010783
Iteration 22/1000 | Loss: 0.00010255
Iteration 23/1000 | Loss: 0.00008754
Iteration 24/1000 | Loss: 0.00009574
Iteration 25/1000 | Loss: 0.00009880
Iteration 26/1000 | Loss: 0.00012612
Iteration 27/1000 | Loss: 0.00012746
Iteration 28/1000 | Loss: 0.00010512
Iteration 29/1000 | Loss: 0.00007782
Iteration 30/1000 | Loss: 0.00020656
Iteration 31/1000 | Loss: 0.00013304
Iteration 32/1000 | Loss: 0.00009038
Iteration 33/1000 | Loss: 0.00009162
Iteration 34/1000 | Loss: 0.00010199
Iteration 35/1000 | Loss: 0.00022159
Iteration 36/1000 | Loss: 0.00020808
Iteration 37/1000 | Loss: 0.00009104
Iteration 38/1000 | Loss: 0.00010108
Iteration 39/1000 | Loss: 0.00008959
Iteration 40/1000 | Loss: 0.00010074
Iteration 41/1000 | Loss: 0.00009820
Iteration 42/1000 | Loss: 0.00010879
Iteration 43/1000 | Loss: 0.00009285
Iteration 44/1000 | Loss: 0.00014764
Iteration 45/1000 | Loss: 0.00010119
Iteration 46/1000 | Loss: 0.00010814
Iteration 47/1000 | Loss: 0.00011726
Iteration 48/1000 | Loss: 0.00008748
Iteration 49/1000 | Loss: 0.00010998
Iteration 50/1000 | Loss: 0.00010261
Iteration 51/1000 | Loss: 0.00012173
Iteration 52/1000 | Loss: 0.00010957
Iteration 53/1000 | Loss: 0.00011484
Iteration 54/1000 | Loss: 0.00011852
Iteration 55/1000 | Loss: 0.00009731
Iteration 56/1000 | Loss: 0.00010027
Iteration 57/1000 | Loss: 0.00010555
Iteration 58/1000 | Loss: 0.00011484
Iteration 59/1000 | Loss: 0.00010549
Iteration 60/1000 | Loss: 0.00009452
Iteration 61/1000 | Loss: 0.00008476
Iteration 62/1000 | Loss: 0.00010418
Iteration 63/1000 | Loss: 0.00011880
Iteration 64/1000 | Loss: 0.00012108
Iteration 65/1000 | Loss: 0.00012294
Iteration 66/1000 | Loss: 0.00012367
Iteration 67/1000 | Loss: 0.00011675
Iteration 68/1000 | Loss: 0.00012217
Iteration 69/1000 | Loss: 0.00013155
Iteration 70/1000 | Loss: 0.00012647
Iteration 71/1000 | Loss: 0.00010094
Iteration 72/1000 | Loss: 0.00008692
Iteration 73/1000 | Loss: 0.00008812
Iteration 74/1000 | Loss: 0.00011201
Iteration 75/1000 | Loss: 0.00010268
Iteration 76/1000 | Loss: 0.00008405
Iteration 77/1000 | Loss: 0.00009080
Iteration 78/1000 | Loss: 0.00013053
Iteration 79/1000 | Loss: 0.00008451
Iteration 80/1000 | Loss: 0.00009924
Iteration 81/1000 | Loss: 0.00009077
Iteration 82/1000 | Loss: 0.00010060
Iteration 83/1000 | Loss: 0.00008143
Iteration 84/1000 | Loss: 0.00008111
Iteration 85/1000 | Loss: 0.00007840
Iteration 86/1000 | Loss: 0.00010469
Iteration 87/1000 | Loss: 0.00010273
Iteration 88/1000 | Loss: 0.00010163
Iteration 89/1000 | Loss: 0.00010146
Iteration 90/1000 | Loss: 0.00010317
Iteration 91/1000 | Loss: 0.00009376
Iteration 92/1000 | Loss: 0.00009269
Iteration 93/1000 | Loss: 0.00013270
Iteration 94/1000 | Loss: 0.00010987
Iteration 95/1000 | Loss: 0.00008968
Iteration 96/1000 | Loss: 0.00009966
Iteration 97/1000 | Loss: 0.00008843
Iteration 98/1000 | Loss: 0.00009513
Iteration 99/1000 | Loss: 0.00006944
Iteration 100/1000 | Loss: 0.00007451
Iteration 101/1000 | Loss: 0.00005497
Iteration 102/1000 | Loss: 0.00010496
Iteration 103/1000 | Loss: 0.00007842
Iteration 104/1000 | Loss: 0.00006025
Iteration 105/1000 | Loss: 0.00008484
Iteration 106/1000 | Loss: 0.00007894
Iteration 107/1000 | Loss: 0.00008205
Iteration 108/1000 | Loss: 0.00008446
Iteration 109/1000 | Loss: 0.00009869
Iteration 110/1000 | Loss: 0.00011250
Iteration 111/1000 | Loss: 0.00011209
Iteration 112/1000 | Loss: 0.00018663
Iteration 113/1000 | Loss: 0.00010868
Iteration 114/1000 | Loss: 0.00007441
Iteration 115/1000 | Loss: 0.00008708
Iteration 116/1000 | Loss: 0.00008325
Iteration 117/1000 | Loss: 0.00009064
Iteration 118/1000 | Loss: 0.00006305
Iteration 119/1000 | Loss: 0.00009613
Iteration 120/1000 | Loss: 0.00008942
Iteration 121/1000 | Loss: 0.00004628
Iteration 122/1000 | Loss: 0.00018555
Iteration 123/1000 | Loss: 0.00005606
Iteration 124/1000 | Loss: 0.00005971
Iteration 125/1000 | Loss: 0.00007987
Iteration 126/1000 | Loss: 0.00005559
Iteration 127/1000 | Loss: 0.00004340
Iteration 128/1000 | Loss: 0.00005062
Iteration 129/1000 | Loss: 0.00005900
Iteration 130/1000 | Loss: 0.00005179
Iteration 131/1000 | Loss: 0.00005411
Iteration 132/1000 | Loss: 0.00004703
Iteration 133/1000 | Loss: 0.00004742
Iteration 134/1000 | Loss: 0.00003219
Iteration 135/1000 | Loss: 0.00004094
Iteration 136/1000 | Loss: 0.00004323
Iteration 137/1000 | Loss: 0.00003528
Iteration 138/1000 | Loss: 0.00003750
Iteration 139/1000 | Loss: 0.00003233
Iteration 140/1000 | Loss: 0.00003501
Iteration 141/1000 | Loss: 0.00003640
Iteration 142/1000 | Loss: 0.00003167
Iteration 143/1000 | Loss: 0.00003723
Iteration 144/1000 | Loss: 0.00004417
Iteration 145/1000 | Loss: 0.00004740
Iteration 146/1000 | Loss: 0.00004024
Iteration 147/1000 | Loss: 0.00004248
Iteration 148/1000 | Loss: 0.00003968
Iteration 149/1000 | Loss: 0.00004572
Iteration 150/1000 | Loss: 0.00004829
Iteration 151/1000 | Loss: 0.00004526
Iteration 152/1000 | Loss: 0.00005156
Iteration 153/1000 | Loss: 0.00004575
Iteration 154/1000 | Loss: 0.00004087
Iteration 155/1000 | Loss: 0.00003360
Iteration 156/1000 | Loss: 0.00002976
Iteration 157/1000 | Loss: 0.00002885
Iteration 158/1000 | Loss: 0.00002838
Iteration 159/1000 | Loss: 0.00002806
Iteration 160/1000 | Loss: 0.00002787
Iteration 161/1000 | Loss: 0.00002778
Iteration 162/1000 | Loss: 0.00002778
Iteration 163/1000 | Loss: 0.00002778
Iteration 164/1000 | Loss: 0.00002778
Iteration 165/1000 | Loss: 0.00002777
Iteration 166/1000 | Loss: 0.00002776
Iteration 167/1000 | Loss: 0.00002776
Iteration 168/1000 | Loss: 0.00002776
Iteration 169/1000 | Loss: 0.00002775
Iteration 170/1000 | Loss: 0.00002774
Iteration 171/1000 | Loss: 0.00002774
Iteration 172/1000 | Loss: 0.00002774
Iteration 173/1000 | Loss: 0.00002774
Iteration 174/1000 | Loss: 0.00002774
Iteration 175/1000 | Loss: 0.00002773
Iteration 176/1000 | Loss: 0.00002773
Iteration 177/1000 | Loss: 0.00002772
Iteration 178/1000 | Loss: 0.00002772
Iteration 179/1000 | Loss: 0.00002771
Iteration 180/1000 | Loss: 0.00002771
Iteration 181/1000 | Loss: 0.00002770
Iteration 182/1000 | Loss: 0.00002770
Iteration 183/1000 | Loss: 0.00002770
Iteration 184/1000 | Loss: 0.00002769
Iteration 185/1000 | Loss: 0.00002769
Iteration 186/1000 | Loss: 0.00002768
Iteration 187/1000 | Loss: 0.00002768
Iteration 188/1000 | Loss: 0.00002766
Iteration 189/1000 | Loss: 0.00002766
Iteration 190/1000 | Loss: 0.00002766
Iteration 191/1000 | Loss: 0.00002766
Iteration 192/1000 | Loss: 0.00002766
Iteration 193/1000 | Loss: 0.00002766
Iteration 194/1000 | Loss: 0.00002766
Iteration 195/1000 | Loss: 0.00002766
Iteration 196/1000 | Loss: 0.00002766
Iteration 197/1000 | Loss: 0.00002765
Iteration 198/1000 | Loss: 0.00002764
Iteration 199/1000 | Loss: 0.00002764
Iteration 200/1000 | Loss: 0.00002764
Iteration 201/1000 | Loss: 0.00002764
Iteration 202/1000 | Loss: 0.00002763
Iteration 203/1000 | Loss: 0.00002763
Iteration 204/1000 | Loss: 0.00002763
Iteration 205/1000 | Loss: 0.00002763
Iteration 206/1000 | Loss: 0.00002763
Iteration 207/1000 | Loss: 0.00002762
Iteration 208/1000 | Loss: 0.00002762
Iteration 209/1000 | Loss: 0.00002762
Iteration 210/1000 | Loss: 0.00002762
Iteration 211/1000 | Loss: 0.00002762
Iteration 212/1000 | Loss: 0.00002762
Iteration 213/1000 | Loss: 0.00002762
Iteration 214/1000 | Loss: 0.00002762
Iteration 215/1000 | Loss: 0.00002762
Iteration 216/1000 | Loss: 0.00002761
Iteration 217/1000 | Loss: 0.00002761
Iteration 218/1000 | Loss: 0.00002761
Iteration 219/1000 | Loss: 0.00002761
Iteration 220/1000 | Loss: 0.00002761
Iteration 221/1000 | Loss: 0.00002761
Iteration 222/1000 | Loss: 0.00002761
Iteration 223/1000 | Loss: 0.00002761
Iteration 224/1000 | Loss: 0.00002761
Iteration 225/1000 | Loss: 0.00002761
Iteration 226/1000 | Loss: 0.00002761
Iteration 227/1000 | Loss: 0.00002760
Iteration 228/1000 | Loss: 0.00002760
Iteration 229/1000 | Loss: 0.00002760
Iteration 230/1000 | Loss: 0.00002760
Iteration 231/1000 | Loss: 0.00002760
Iteration 232/1000 | Loss: 0.00002760
Iteration 233/1000 | Loss: 0.00002760
Iteration 234/1000 | Loss: 0.00002760
Iteration 235/1000 | Loss: 0.00002760
Iteration 236/1000 | Loss: 0.00002760
Iteration 237/1000 | Loss: 0.00002760
Iteration 238/1000 | Loss: 0.00002759
Iteration 239/1000 | Loss: 0.00002759
Iteration 240/1000 | Loss: 0.00002759
Iteration 241/1000 | Loss: 0.00002759
Iteration 242/1000 | Loss: 0.00002759
Iteration 243/1000 | Loss: 0.00002759
Iteration 244/1000 | Loss: 0.00002759
Iteration 245/1000 | Loss: 0.00002758
Iteration 246/1000 | Loss: 0.00002758
Iteration 247/1000 | Loss: 0.00002758
Iteration 248/1000 | Loss: 0.00002758
Iteration 249/1000 | Loss: 0.00002758
Iteration 250/1000 | Loss: 0.00002758
Iteration 251/1000 | Loss: 0.00002758
Iteration 252/1000 | Loss: 0.00002758
Iteration 253/1000 | Loss: 0.00002757
Iteration 254/1000 | Loss: 0.00002757
Iteration 255/1000 | Loss: 0.00002757
Iteration 256/1000 | Loss: 0.00002757
Iteration 257/1000 | Loss: 0.00002757
Iteration 258/1000 | Loss: 0.00002757
Iteration 259/1000 | Loss: 0.00002757
Iteration 260/1000 | Loss: 0.00002757
Iteration 261/1000 | Loss: 0.00002757
Iteration 262/1000 | Loss: 0.00002757
Iteration 263/1000 | Loss: 0.00002757
Iteration 264/1000 | Loss: 0.00002757
Iteration 265/1000 | Loss: 0.00002757
Iteration 266/1000 | Loss: 0.00002757
Iteration 267/1000 | Loss: 0.00002757
Iteration 268/1000 | Loss: 0.00002757
Iteration 269/1000 | Loss: 0.00002757
Iteration 270/1000 | Loss: 0.00002757
Iteration 271/1000 | Loss: 0.00002757
Iteration 272/1000 | Loss: 0.00002757
Iteration 273/1000 | Loss: 0.00002757
Iteration 274/1000 | Loss: 0.00002757
Iteration 275/1000 | Loss: 0.00002757
Iteration 276/1000 | Loss: 0.00002757
Iteration 277/1000 | Loss: 0.00002757
Iteration 278/1000 | Loss: 0.00002757
Iteration 279/1000 | Loss: 0.00002757
Iteration 280/1000 | Loss: 0.00002757
Iteration 281/1000 | Loss: 0.00002757
Iteration 282/1000 | Loss: 0.00002757
Iteration 283/1000 | Loss: 0.00002757
Iteration 284/1000 | Loss: 0.00002757
Iteration 285/1000 | Loss: 0.00002757
Iteration 286/1000 | Loss: 0.00002757
Iteration 287/1000 | Loss: 0.00002757
Iteration 288/1000 | Loss: 0.00002757
Iteration 289/1000 | Loss: 0.00002757
Iteration 290/1000 | Loss: 0.00002757
Iteration 291/1000 | Loss: 0.00002757
Iteration 292/1000 | Loss: 0.00002757
Iteration 293/1000 | Loss: 0.00002757
Iteration 294/1000 | Loss: 0.00002757
Iteration 295/1000 | Loss: 0.00002757
Iteration 296/1000 | Loss: 0.00002757
Iteration 297/1000 | Loss: 0.00002757
Iteration 298/1000 | Loss: 0.00002757
Iteration 299/1000 | Loss: 0.00002757
Iteration 300/1000 | Loss: 0.00002757
Iteration 301/1000 | Loss: 0.00002757
Iteration 302/1000 | Loss: 0.00002757
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 302. Stopping optimization.
Last 5 losses: [2.7565736672841012e-05, 2.7565736672841012e-05, 2.7565736672841012e-05, 2.7565736672841012e-05, 2.7565736672841012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7565736672841012e-05

Optimization complete. Final v2v error: 4.589528560638428 mm

Highest mean error: 5.556951522827148 mm for frame 176

Lowest mean error: 4.030778408050537 mm for frame 78

Saving results

Total time: 318.66846680641174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00645514
Iteration 2/25 | Loss: 0.00105809
Iteration 3/25 | Loss: 0.00094665
Iteration 4/25 | Loss: 0.00091838
Iteration 5/25 | Loss: 0.00090936
Iteration 6/25 | Loss: 0.00090745
Iteration 7/25 | Loss: 0.00090718
Iteration 8/25 | Loss: 0.00090718
Iteration 9/25 | Loss: 0.00090718
Iteration 10/25 | Loss: 0.00090718
Iteration 11/25 | Loss: 0.00090718
Iteration 12/25 | Loss: 0.00090718
Iteration 13/25 | Loss: 0.00090718
Iteration 14/25 | Loss: 0.00090718
Iteration 15/25 | Loss: 0.00090718
Iteration 16/25 | Loss: 0.00090718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009071811218746006, 0.0009071811218746006, 0.0009071811218746006, 0.0009071811218746006, 0.0009071811218746006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009071811218746006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.87672544
Iteration 2/25 | Loss: 0.00036841
Iteration 3/25 | Loss: 0.00036841
Iteration 4/25 | Loss: 0.00036841
Iteration 5/25 | Loss: 0.00036841
Iteration 6/25 | Loss: 0.00036841
Iteration 7/25 | Loss: 0.00036841
Iteration 8/25 | Loss: 0.00036841
Iteration 9/25 | Loss: 0.00036841
Iteration 10/25 | Loss: 0.00036841
Iteration 11/25 | Loss: 0.00036841
Iteration 12/25 | Loss: 0.00036841
Iteration 13/25 | Loss: 0.00036841
Iteration 14/25 | Loss: 0.00036841
Iteration 15/25 | Loss: 0.00036841
Iteration 16/25 | Loss: 0.00036841
Iteration 17/25 | Loss: 0.00036841
Iteration 18/25 | Loss: 0.00036841
Iteration 19/25 | Loss: 0.00036841
Iteration 20/25 | Loss: 0.00036841
Iteration 21/25 | Loss: 0.00036841
Iteration 22/25 | Loss: 0.00036841
Iteration 23/25 | Loss: 0.00036841
Iteration 24/25 | Loss: 0.00036841
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00036840824759565294, 0.00036840824759565294, 0.00036840824759565294, 0.00036840824759565294, 0.00036840824759565294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00036840824759565294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036841
Iteration 2/1000 | Loss: 0.00003493
Iteration 3/1000 | Loss: 0.00002305
Iteration 4/1000 | Loss: 0.00002134
Iteration 5/1000 | Loss: 0.00002044
Iteration 6/1000 | Loss: 0.00001975
Iteration 7/1000 | Loss: 0.00001927
Iteration 8/1000 | Loss: 0.00001902
Iteration 9/1000 | Loss: 0.00001885
Iteration 10/1000 | Loss: 0.00001883
Iteration 11/1000 | Loss: 0.00001881
Iteration 12/1000 | Loss: 0.00001873
Iteration 13/1000 | Loss: 0.00001872
Iteration 14/1000 | Loss: 0.00001871
Iteration 15/1000 | Loss: 0.00001871
Iteration 16/1000 | Loss: 0.00001870
Iteration 17/1000 | Loss: 0.00001868
Iteration 18/1000 | Loss: 0.00001868
Iteration 19/1000 | Loss: 0.00001867
Iteration 20/1000 | Loss: 0.00001867
Iteration 21/1000 | Loss: 0.00001865
Iteration 22/1000 | Loss: 0.00001864
Iteration 23/1000 | Loss: 0.00001864
Iteration 24/1000 | Loss: 0.00001863
Iteration 25/1000 | Loss: 0.00001863
Iteration 26/1000 | Loss: 0.00001862
Iteration 27/1000 | Loss: 0.00001862
Iteration 28/1000 | Loss: 0.00001858
Iteration 29/1000 | Loss: 0.00001858
Iteration 30/1000 | Loss: 0.00001857
Iteration 31/1000 | Loss: 0.00001857
Iteration 32/1000 | Loss: 0.00001857
Iteration 33/1000 | Loss: 0.00001854
Iteration 34/1000 | Loss: 0.00001854
Iteration 35/1000 | Loss: 0.00001854
Iteration 36/1000 | Loss: 0.00001854
Iteration 37/1000 | Loss: 0.00001853
Iteration 38/1000 | Loss: 0.00001853
Iteration 39/1000 | Loss: 0.00001853
Iteration 40/1000 | Loss: 0.00001853
Iteration 41/1000 | Loss: 0.00001853
Iteration 42/1000 | Loss: 0.00001853
Iteration 43/1000 | Loss: 0.00001852
Iteration 44/1000 | Loss: 0.00001852
Iteration 45/1000 | Loss: 0.00001852
Iteration 46/1000 | Loss: 0.00001852
Iteration 47/1000 | Loss: 0.00001852
Iteration 48/1000 | Loss: 0.00001852
Iteration 49/1000 | Loss: 0.00001851
Iteration 50/1000 | Loss: 0.00001850
Iteration 51/1000 | Loss: 0.00001850
Iteration 52/1000 | Loss: 0.00001850
Iteration 53/1000 | Loss: 0.00001850
Iteration 54/1000 | Loss: 0.00001850
Iteration 55/1000 | Loss: 0.00001850
Iteration 56/1000 | Loss: 0.00001850
Iteration 57/1000 | Loss: 0.00001850
Iteration 58/1000 | Loss: 0.00001850
Iteration 59/1000 | Loss: 0.00001850
Iteration 60/1000 | Loss: 0.00001850
Iteration 61/1000 | Loss: 0.00001850
Iteration 62/1000 | Loss: 0.00001850
Iteration 63/1000 | Loss: 0.00001850
Iteration 64/1000 | Loss: 0.00001850
Iteration 65/1000 | Loss: 0.00001850
Iteration 66/1000 | Loss: 0.00001850
Iteration 67/1000 | Loss: 0.00001850
Iteration 68/1000 | Loss: 0.00001850
Iteration 69/1000 | Loss: 0.00001850
Iteration 70/1000 | Loss: 0.00001850
Iteration 71/1000 | Loss: 0.00001850
Iteration 72/1000 | Loss: 0.00001850
Iteration 73/1000 | Loss: 0.00001850
Iteration 74/1000 | Loss: 0.00001850
Iteration 75/1000 | Loss: 0.00001850
Iteration 76/1000 | Loss: 0.00001850
Iteration 77/1000 | Loss: 0.00001850
Iteration 78/1000 | Loss: 0.00001850
Iteration 79/1000 | Loss: 0.00001850
Iteration 80/1000 | Loss: 0.00001850
Iteration 81/1000 | Loss: 0.00001850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.8500961232348345e-05, 1.8500961232348345e-05, 1.8500961232348345e-05, 1.8500961232348345e-05, 1.8500961232348345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8500961232348345e-05

Optimization complete. Final v2v error: 3.6999378204345703 mm

Highest mean error: 4.008295059204102 mm for frame 158

Lowest mean error: 3.2964115142822266 mm for frame 3

Saving results

Total time: 29.52095913887024
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00555357
Iteration 2/25 | Loss: 0.00164967
Iteration 3/25 | Loss: 0.00113258
Iteration 4/25 | Loss: 0.00102061
Iteration 5/25 | Loss: 0.00099561
Iteration 6/25 | Loss: 0.00098774
Iteration 7/25 | Loss: 0.00098607
Iteration 8/25 | Loss: 0.00098547
Iteration 9/25 | Loss: 0.00098523
Iteration 10/25 | Loss: 0.00098523
Iteration 11/25 | Loss: 0.00098523
Iteration 12/25 | Loss: 0.00098523
Iteration 13/25 | Loss: 0.00098523
Iteration 14/25 | Loss: 0.00098523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009852265939116478, 0.0009852265939116478, 0.0009852265939116478, 0.0009852265939116478, 0.0009852265939116478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009852265939116478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41121316
Iteration 2/25 | Loss: 0.00040643
Iteration 3/25 | Loss: 0.00040641
Iteration 4/25 | Loss: 0.00040641
Iteration 5/25 | Loss: 0.00040641
Iteration 6/25 | Loss: 0.00040641
Iteration 7/25 | Loss: 0.00040641
Iteration 8/25 | Loss: 0.00040641
Iteration 9/25 | Loss: 0.00040641
Iteration 10/25 | Loss: 0.00040641
Iteration 11/25 | Loss: 0.00040641
Iteration 12/25 | Loss: 0.00040641
Iteration 13/25 | Loss: 0.00040641
Iteration 14/25 | Loss: 0.00040641
Iteration 15/25 | Loss: 0.00040641
Iteration 16/25 | Loss: 0.00040641
Iteration 17/25 | Loss: 0.00040641
Iteration 18/25 | Loss: 0.00040641
Iteration 19/25 | Loss: 0.00040641
Iteration 20/25 | Loss: 0.00040641
Iteration 21/25 | Loss: 0.00040641
Iteration 22/25 | Loss: 0.00040641
Iteration 23/25 | Loss: 0.00040641
Iteration 24/25 | Loss: 0.00040641
Iteration 25/25 | Loss: 0.00040641

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040641
Iteration 2/1000 | Loss: 0.00005259
Iteration 3/1000 | Loss: 0.00003614
Iteration 4/1000 | Loss: 0.00003222
Iteration 5/1000 | Loss: 0.00003072
Iteration 6/1000 | Loss: 0.00002992
Iteration 7/1000 | Loss: 0.00002911
Iteration 8/1000 | Loss: 0.00002863
Iteration 9/1000 | Loss: 0.00002820
Iteration 10/1000 | Loss: 0.00002780
Iteration 11/1000 | Loss: 0.00002755
Iteration 12/1000 | Loss: 0.00002736
Iteration 13/1000 | Loss: 0.00002720
Iteration 14/1000 | Loss: 0.00002710
Iteration 15/1000 | Loss: 0.00002708
Iteration 16/1000 | Loss: 0.00002707
Iteration 17/1000 | Loss: 0.00002703
Iteration 18/1000 | Loss: 0.00002701
Iteration 19/1000 | Loss: 0.00002700
Iteration 20/1000 | Loss: 0.00002700
Iteration 21/1000 | Loss: 0.00002700
Iteration 22/1000 | Loss: 0.00002700
Iteration 23/1000 | Loss: 0.00002699
Iteration 24/1000 | Loss: 0.00002699
Iteration 25/1000 | Loss: 0.00002699
Iteration 26/1000 | Loss: 0.00002698
Iteration 27/1000 | Loss: 0.00002698
Iteration 28/1000 | Loss: 0.00002698
Iteration 29/1000 | Loss: 0.00002696
Iteration 30/1000 | Loss: 0.00002696
Iteration 31/1000 | Loss: 0.00002696
Iteration 32/1000 | Loss: 0.00002695
Iteration 33/1000 | Loss: 0.00002695
Iteration 34/1000 | Loss: 0.00002695
Iteration 35/1000 | Loss: 0.00002694
Iteration 36/1000 | Loss: 0.00002694
Iteration 37/1000 | Loss: 0.00002694
Iteration 38/1000 | Loss: 0.00002693
Iteration 39/1000 | Loss: 0.00002693
Iteration 40/1000 | Loss: 0.00002693
Iteration 41/1000 | Loss: 0.00002692
Iteration 42/1000 | Loss: 0.00002692
Iteration 43/1000 | Loss: 0.00002692
Iteration 44/1000 | Loss: 0.00002692
Iteration 45/1000 | Loss: 0.00002691
Iteration 46/1000 | Loss: 0.00002691
Iteration 47/1000 | Loss: 0.00002691
Iteration 48/1000 | Loss: 0.00002691
Iteration 49/1000 | Loss: 0.00002691
Iteration 50/1000 | Loss: 0.00002690
Iteration 51/1000 | Loss: 0.00002690
Iteration 52/1000 | Loss: 0.00002690
Iteration 53/1000 | Loss: 0.00002689
Iteration 54/1000 | Loss: 0.00002689
Iteration 55/1000 | Loss: 0.00002688
Iteration 56/1000 | Loss: 0.00002688
Iteration 57/1000 | Loss: 0.00002687
Iteration 58/1000 | Loss: 0.00002687
Iteration 59/1000 | Loss: 0.00002687
Iteration 60/1000 | Loss: 0.00002686
Iteration 61/1000 | Loss: 0.00002686
Iteration 62/1000 | Loss: 0.00002686
Iteration 63/1000 | Loss: 0.00002686
Iteration 64/1000 | Loss: 0.00002686
Iteration 65/1000 | Loss: 0.00002686
Iteration 66/1000 | Loss: 0.00002686
Iteration 67/1000 | Loss: 0.00002686
Iteration 68/1000 | Loss: 0.00002685
Iteration 69/1000 | Loss: 0.00002685
Iteration 70/1000 | Loss: 0.00002684
Iteration 71/1000 | Loss: 0.00002684
Iteration 72/1000 | Loss: 0.00002684
Iteration 73/1000 | Loss: 0.00002684
Iteration 74/1000 | Loss: 0.00002684
Iteration 75/1000 | Loss: 0.00002683
Iteration 76/1000 | Loss: 0.00002683
Iteration 77/1000 | Loss: 0.00002682
Iteration 78/1000 | Loss: 0.00002681
Iteration 79/1000 | Loss: 0.00002681
Iteration 80/1000 | Loss: 0.00002681
Iteration 81/1000 | Loss: 0.00002681
Iteration 82/1000 | Loss: 0.00002681
Iteration 83/1000 | Loss: 0.00002681
Iteration 84/1000 | Loss: 0.00002681
Iteration 85/1000 | Loss: 0.00002680
Iteration 86/1000 | Loss: 0.00002679
Iteration 87/1000 | Loss: 0.00002678
Iteration 88/1000 | Loss: 0.00002678
Iteration 89/1000 | Loss: 0.00002678
Iteration 90/1000 | Loss: 0.00002678
Iteration 91/1000 | Loss: 0.00002678
Iteration 92/1000 | Loss: 0.00002677
Iteration 93/1000 | Loss: 0.00002677
Iteration 94/1000 | Loss: 0.00002677
Iteration 95/1000 | Loss: 0.00002677
Iteration 96/1000 | Loss: 0.00002676
Iteration 97/1000 | Loss: 0.00002676
Iteration 98/1000 | Loss: 0.00002676
Iteration 99/1000 | Loss: 0.00002676
Iteration 100/1000 | Loss: 0.00002676
Iteration 101/1000 | Loss: 0.00002676
Iteration 102/1000 | Loss: 0.00002675
Iteration 103/1000 | Loss: 0.00002675
Iteration 104/1000 | Loss: 0.00002675
Iteration 105/1000 | Loss: 0.00002675
Iteration 106/1000 | Loss: 0.00002675
Iteration 107/1000 | Loss: 0.00002675
Iteration 108/1000 | Loss: 0.00002675
Iteration 109/1000 | Loss: 0.00002675
Iteration 110/1000 | Loss: 0.00002674
Iteration 111/1000 | Loss: 0.00002674
Iteration 112/1000 | Loss: 0.00002674
Iteration 113/1000 | Loss: 0.00002674
Iteration 114/1000 | Loss: 0.00002674
Iteration 115/1000 | Loss: 0.00002674
Iteration 116/1000 | Loss: 0.00002674
Iteration 117/1000 | Loss: 0.00002674
Iteration 118/1000 | Loss: 0.00002673
Iteration 119/1000 | Loss: 0.00002673
Iteration 120/1000 | Loss: 0.00002673
Iteration 121/1000 | Loss: 0.00002673
Iteration 122/1000 | Loss: 0.00002673
Iteration 123/1000 | Loss: 0.00002673
Iteration 124/1000 | Loss: 0.00002673
Iteration 125/1000 | Loss: 0.00002673
Iteration 126/1000 | Loss: 0.00002673
Iteration 127/1000 | Loss: 0.00002672
Iteration 128/1000 | Loss: 0.00002672
Iteration 129/1000 | Loss: 0.00002672
Iteration 130/1000 | Loss: 0.00002672
Iteration 131/1000 | Loss: 0.00002672
Iteration 132/1000 | Loss: 0.00002672
Iteration 133/1000 | Loss: 0.00002672
Iteration 134/1000 | Loss: 0.00002672
Iteration 135/1000 | Loss: 0.00002671
Iteration 136/1000 | Loss: 0.00002671
Iteration 137/1000 | Loss: 0.00002671
Iteration 138/1000 | Loss: 0.00002671
Iteration 139/1000 | Loss: 0.00002671
Iteration 140/1000 | Loss: 0.00002671
Iteration 141/1000 | Loss: 0.00002671
Iteration 142/1000 | Loss: 0.00002671
Iteration 143/1000 | Loss: 0.00002671
Iteration 144/1000 | Loss: 0.00002671
Iteration 145/1000 | Loss: 0.00002671
Iteration 146/1000 | Loss: 0.00002671
Iteration 147/1000 | Loss: 0.00002670
Iteration 148/1000 | Loss: 0.00002670
Iteration 149/1000 | Loss: 0.00002670
Iteration 150/1000 | Loss: 0.00002670
Iteration 151/1000 | Loss: 0.00002670
Iteration 152/1000 | Loss: 0.00002670
Iteration 153/1000 | Loss: 0.00002670
Iteration 154/1000 | Loss: 0.00002670
Iteration 155/1000 | Loss: 0.00002669
Iteration 156/1000 | Loss: 0.00002669
Iteration 157/1000 | Loss: 0.00002669
Iteration 158/1000 | Loss: 0.00002669
Iteration 159/1000 | Loss: 0.00002669
Iteration 160/1000 | Loss: 0.00002669
Iteration 161/1000 | Loss: 0.00002669
Iteration 162/1000 | Loss: 0.00002669
Iteration 163/1000 | Loss: 0.00002669
Iteration 164/1000 | Loss: 0.00002668
Iteration 165/1000 | Loss: 0.00002668
Iteration 166/1000 | Loss: 0.00002668
Iteration 167/1000 | Loss: 0.00002668
Iteration 168/1000 | Loss: 0.00002668
Iteration 169/1000 | Loss: 0.00002668
Iteration 170/1000 | Loss: 0.00002668
Iteration 171/1000 | Loss: 0.00002667
Iteration 172/1000 | Loss: 0.00002667
Iteration 173/1000 | Loss: 0.00002667
Iteration 174/1000 | Loss: 0.00002667
Iteration 175/1000 | Loss: 0.00002667
Iteration 176/1000 | Loss: 0.00002667
Iteration 177/1000 | Loss: 0.00002667
Iteration 178/1000 | Loss: 0.00002667
Iteration 179/1000 | Loss: 0.00002667
Iteration 180/1000 | Loss: 0.00002667
Iteration 181/1000 | Loss: 0.00002667
Iteration 182/1000 | Loss: 0.00002667
Iteration 183/1000 | Loss: 0.00002666
Iteration 184/1000 | Loss: 0.00002666
Iteration 185/1000 | Loss: 0.00002666
Iteration 186/1000 | Loss: 0.00002666
Iteration 187/1000 | Loss: 0.00002666
Iteration 188/1000 | Loss: 0.00002666
Iteration 189/1000 | Loss: 0.00002666
Iteration 190/1000 | Loss: 0.00002666
Iteration 191/1000 | Loss: 0.00002666
Iteration 192/1000 | Loss: 0.00002666
Iteration 193/1000 | Loss: 0.00002666
Iteration 194/1000 | Loss: 0.00002666
Iteration 195/1000 | Loss: 0.00002666
Iteration 196/1000 | Loss: 0.00002665
Iteration 197/1000 | Loss: 0.00002665
Iteration 198/1000 | Loss: 0.00002665
Iteration 199/1000 | Loss: 0.00002665
Iteration 200/1000 | Loss: 0.00002665
Iteration 201/1000 | Loss: 0.00002664
Iteration 202/1000 | Loss: 0.00002664
Iteration 203/1000 | Loss: 0.00002664
Iteration 204/1000 | Loss: 0.00002664
Iteration 205/1000 | Loss: 0.00002664
Iteration 206/1000 | Loss: 0.00002664
Iteration 207/1000 | Loss: 0.00002663
Iteration 208/1000 | Loss: 0.00002663
Iteration 209/1000 | Loss: 0.00002663
Iteration 210/1000 | Loss: 0.00002663
Iteration 211/1000 | Loss: 0.00002663
Iteration 212/1000 | Loss: 0.00002663
Iteration 213/1000 | Loss: 0.00002662
Iteration 214/1000 | Loss: 0.00002662
Iteration 215/1000 | Loss: 0.00002662
Iteration 216/1000 | Loss: 0.00002662
Iteration 217/1000 | Loss: 0.00002662
Iteration 218/1000 | Loss: 0.00002661
Iteration 219/1000 | Loss: 0.00002661
Iteration 220/1000 | Loss: 0.00002661
Iteration 221/1000 | Loss: 0.00002661
Iteration 222/1000 | Loss: 0.00002661
Iteration 223/1000 | Loss: 0.00002661
Iteration 224/1000 | Loss: 0.00002661
Iteration 225/1000 | Loss: 0.00002661
Iteration 226/1000 | Loss: 0.00002661
Iteration 227/1000 | Loss: 0.00002661
Iteration 228/1000 | Loss: 0.00002661
Iteration 229/1000 | Loss: 0.00002661
Iteration 230/1000 | Loss: 0.00002661
Iteration 231/1000 | Loss: 0.00002660
Iteration 232/1000 | Loss: 0.00002660
Iteration 233/1000 | Loss: 0.00002660
Iteration 234/1000 | Loss: 0.00002660
Iteration 235/1000 | Loss: 0.00002660
Iteration 236/1000 | Loss: 0.00002660
Iteration 237/1000 | Loss: 0.00002660
Iteration 238/1000 | Loss: 0.00002660
Iteration 239/1000 | Loss: 0.00002660
Iteration 240/1000 | Loss: 0.00002659
Iteration 241/1000 | Loss: 0.00002659
Iteration 242/1000 | Loss: 0.00002659
Iteration 243/1000 | Loss: 0.00002659
Iteration 244/1000 | Loss: 0.00002659
Iteration 245/1000 | Loss: 0.00002659
Iteration 246/1000 | Loss: 0.00002659
Iteration 247/1000 | Loss: 0.00002659
Iteration 248/1000 | Loss: 0.00002659
Iteration 249/1000 | Loss: 0.00002659
Iteration 250/1000 | Loss: 0.00002659
Iteration 251/1000 | Loss: 0.00002659
Iteration 252/1000 | Loss: 0.00002658
Iteration 253/1000 | Loss: 0.00002658
Iteration 254/1000 | Loss: 0.00002658
Iteration 255/1000 | Loss: 0.00002658
Iteration 256/1000 | Loss: 0.00002658
Iteration 257/1000 | Loss: 0.00002658
Iteration 258/1000 | Loss: 0.00002658
Iteration 259/1000 | Loss: 0.00002658
Iteration 260/1000 | Loss: 0.00002658
Iteration 261/1000 | Loss: 0.00002657
Iteration 262/1000 | Loss: 0.00002657
Iteration 263/1000 | Loss: 0.00002657
Iteration 264/1000 | Loss: 0.00002657
Iteration 265/1000 | Loss: 0.00002656
Iteration 266/1000 | Loss: 0.00002656
Iteration 267/1000 | Loss: 0.00002656
Iteration 268/1000 | Loss: 0.00002656
Iteration 269/1000 | Loss: 0.00002656
Iteration 270/1000 | Loss: 0.00002656
Iteration 271/1000 | Loss: 0.00002656
Iteration 272/1000 | Loss: 0.00002656
Iteration 273/1000 | Loss: 0.00002656
Iteration 274/1000 | Loss: 0.00002656
Iteration 275/1000 | Loss: 0.00002656
Iteration 276/1000 | Loss: 0.00002656
Iteration 277/1000 | Loss: 0.00002656
Iteration 278/1000 | Loss: 0.00002656
Iteration 279/1000 | Loss: 0.00002656
Iteration 280/1000 | Loss: 0.00002656
Iteration 281/1000 | Loss: 0.00002656
Iteration 282/1000 | Loss: 0.00002656
Iteration 283/1000 | Loss: 0.00002656
Iteration 284/1000 | Loss: 0.00002656
Iteration 285/1000 | Loss: 0.00002656
Iteration 286/1000 | Loss: 0.00002656
Iteration 287/1000 | Loss: 0.00002656
Iteration 288/1000 | Loss: 0.00002656
Iteration 289/1000 | Loss: 0.00002656
Iteration 290/1000 | Loss: 0.00002656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [2.6558431272860616e-05, 2.6558431272860616e-05, 2.6558431272860616e-05, 2.6558431272860616e-05, 2.6558431272860616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6558431272860616e-05

Optimization complete. Final v2v error: 4.229342937469482 mm

Highest mean error: 6.292087078094482 mm for frame 58

Lowest mean error: 3.12243390083313 mm for frame 15

Saving results

Total time: 52.152730226516724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00766502
Iteration 2/25 | Loss: 0.00140100
Iteration 3/25 | Loss: 0.00106202
Iteration 4/25 | Loss: 0.00098790
Iteration 5/25 | Loss: 0.00096794
Iteration 6/25 | Loss: 0.00096296
Iteration 7/25 | Loss: 0.00096204
Iteration 8/25 | Loss: 0.00096204
Iteration 9/25 | Loss: 0.00096204
Iteration 10/25 | Loss: 0.00096204
Iteration 11/25 | Loss: 0.00096204
Iteration 12/25 | Loss: 0.00096204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009620395139791071, 0.0009620395139791071, 0.0009620395139791071, 0.0009620395139791071, 0.0009620395139791071]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009620395139791071

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36714232
Iteration 2/25 | Loss: 0.00045153
Iteration 3/25 | Loss: 0.00045152
Iteration 4/25 | Loss: 0.00045152
Iteration 5/25 | Loss: 0.00045152
Iteration 6/25 | Loss: 0.00045152
Iteration 7/25 | Loss: 0.00045152
Iteration 8/25 | Loss: 0.00045152
Iteration 9/25 | Loss: 0.00045152
Iteration 10/25 | Loss: 0.00045152
Iteration 11/25 | Loss: 0.00045152
Iteration 12/25 | Loss: 0.00045152
Iteration 13/25 | Loss: 0.00045152
Iteration 14/25 | Loss: 0.00045152
Iteration 15/25 | Loss: 0.00045152
Iteration 16/25 | Loss: 0.00045152
Iteration 17/25 | Loss: 0.00045152
Iteration 18/25 | Loss: 0.00045152
Iteration 19/25 | Loss: 0.00045152
Iteration 20/25 | Loss: 0.00045152
Iteration 21/25 | Loss: 0.00045152
Iteration 22/25 | Loss: 0.00045152
Iteration 23/25 | Loss: 0.00045152
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00045152229722589254, 0.00045152229722589254, 0.00045152229722589254, 0.00045152229722589254, 0.00045152229722589254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045152229722589254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045152
Iteration 2/1000 | Loss: 0.00003746
Iteration 3/1000 | Loss: 0.00002644
Iteration 4/1000 | Loss: 0.00002339
Iteration 5/1000 | Loss: 0.00002257
Iteration 6/1000 | Loss: 0.00002178
Iteration 7/1000 | Loss: 0.00002116
Iteration 8/1000 | Loss: 0.00002078
Iteration 9/1000 | Loss: 0.00002039
Iteration 10/1000 | Loss: 0.00002016
Iteration 11/1000 | Loss: 0.00002013
Iteration 12/1000 | Loss: 0.00002006
Iteration 13/1000 | Loss: 0.00001998
Iteration 14/1000 | Loss: 0.00001996
Iteration 15/1000 | Loss: 0.00001996
Iteration 16/1000 | Loss: 0.00001995
Iteration 17/1000 | Loss: 0.00001994
Iteration 18/1000 | Loss: 0.00001993
Iteration 19/1000 | Loss: 0.00001993
Iteration 20/1000 | Loss: 0.00001993
Iteration 21/1000 | Loss: 0.00001992
Iteration 22/1000 | Loss: 0.00001991
Iteration 23/1000 | Loss: 0.00001991
Iteration 24/1000 | Loss: 0.00001991
Iteration 25/1000 | Loss: 0.00001991
Iteration 26/1000 | Loss: 0.00001991
Iteration 27/1000 | Loss: 0.00001991
Iteration 28/1000 | Loss: 0.00001991
Iteration 29/1000 | Loss: 0.00001991
Iteration 30/1000 | Loss: 0.00001991
Iteration 31/1000 | Loss: 0.00001991
Iteration 32/1000 | Loss: 0.00001990
Iteration 33/1000 | Loss: 0.00001989
Iteration 34/1000 | Loss: 0.00001989
Iteration 35/1000 | Loss: 0.00001989
Iteration 36/1000 | Loss: 0.00001989
Iteration 37/1000 | Loss: 0.00001989
Iteration 38/1000 | Loss: 0.00001988
Iteration 39/1000 | Loss: 0.00001988
Iteration 40/1000 | Loss: 0.00001988
Iteration 41/1000 | Loss: 0.00001988
Iteration 42/1000 | Loss: 0.00001988
Iteration 43/1000 | Loss: 0.00001988
Iteration 44/1000 | Loss: 0.00001988
Iteration 45/1000 | Loss: 0.00001988
Iteration 46/1000 | Loss: 0.00001988
Iteration 47/1000 | Loss: 0.00001988
Iteration 48/1000 | Loss: 0.00001988
Iteration 49/1000 | Loss: 0.00001987
Iteration 50/1000 | Loss: 0.00001987
Iteration 51/1000 | Loss: 0.00001987
Iteration 52/1000 | Loss: 0.00001986
Iteration 53/1000 | Loss: 0.00001986
Iteration 54/1000 | Loss: 0.00001986
Iteration 55/1000 | Loss: 0.00001985
Iteration 56/1000 | Loss: 0.00001985
Iteration 57/1000 | Loss: 0.00001985
Iteration 58/1000 | Loss: 0.00001985
Iteration 59/1000 | Loss: 0.00001984
Iteration 60/1000 | Loss: 0.00001984
Iteration 61/1000 | Loss: 0.00001984
Iteration 62/1000 | Loss: 0.00001984
Iteration 63/1000 | Loss: 0.00001983
Iteration 64/1000 | Loss: 0.00001983
Iteration 65/1000 | Loss: 0.00001983
Iteration 66/1000 | Loss: 0.00001982
Iteration 67/1000 | Loss: 0.00001982
Iteration 68/1000 | Loss: 0.00001982
Iteration 69/1000 | Loss: 0.00001982
Iteration 70/1000 | Loss: 0.00001981
Iteration 71/1000 | Loss: 0.00001981
Iteration 72/1000 | Loss: 0.00001981
Iteration 73/1000 | Loss: 0.00001981
Iteration 74/1000 | Loss: 0.00001980
Iteration 75/1000 | Loss: 0.00001980
Iteration 76/1000 | Loss: 0.00001980
Iteration 77/1000 | Loss: 0.00001979
Iteration 78/1000 | Loss: 0.00001979
Iteration 79/1000 | Loss: 0.00001979
Iteration 80/1000 | Loss: 0.00001979
Iteration 81/1000 | Loss: 0.00001978
Iteration 82/1000 | Loss: 0.00001978
Iteration 83/1000 | Loss: 0.00001978
Iteration 84/1000 | Loss: 0.00001978
Iteration 85/1000 | Loss: 0.00001978
Iteration 86/1000 | Loss: 0.00001978
Iteration 87/1000 | Loss: 0.00001978
Iteration 88/1000 | Loss: 0.00001978
Iteration 89/1000 | Loss: 0.00001978
Iteration 90/1000 | Loss: 0.00001978
Iteration 91/1000 | Loss: 0.00001978
Iteration 92/1000 | Loss: 0.00001978
Iteration 93/1000 | Loss: 0.00001978
Iteration 94/1000 | Loss: 0.00001977
Iteration 95/1000 | Loss: 0.00001977
Iteration 96/1000 | Loss: 0.00001977
Iteration 97/1000 | Loss: 0.00001977
Iteration 98/1000 | Loss: 0.00001976
Iteration 99/1000 | Loss: 0.00001976
Iteration 100/1000 | Loss: 0.00001976
Iteration 101/1000 | Loss: 0.00001976
Iteration 102/1000 | Loss: 0.00001976
Iteration 103/1000 | Loss: 0.00001976
Iteration 104/1000 | Loss: 0.00001976
Iteration 105/1000 | Loss: 0.00001976
Iteration 106/1000 | Loss: 0.00001976
Iteration 107/1000 | Loss: 0.00001975
Iteration 108/1000 | Loss: 0.00001975
Iteration 109/1000 | Loss: 0.00001975
Iteration 110/1000 | Loss: 0.00001975
Iteration 111/1000 | Loss: 0.00001975
Iteration 112/1000 | Loss: 0.00001974
Iteration 113/1000 | Loss: 0.00001974
Iteration 114/1000 | Loss: 0.00001974
Iteration 115/1000 | Loss: 0.00001974
Iteration 116/1000 | Loss: 0.00001974
Iteration 117/1000 | Loss: 0.00001974
Iteration 118/1000 | Loss: 0.00001974
Iteration 119/1000 | Loss: 0.00001974
Iteration 120/1000 | Loss: 0.00001974
Iteration 121/1000 | Loss: 0.00001974
Iteration 122/1000 | Loss: 0.00001974
Iteration 123/1000 | Loss: 0.00001974
Iteration 124/1000 | Loss: 0.00001974
Iteration 125/1000 | Loss: 0.00001974
Iteration 126/1000 | Loss: 0.00001974
Iteration 127/1000 | Loss: 0.00001974
Iteration 128/1000 | Loss: 0.00001974
Iteration 129/1000 | Loss: 0.00001974
Iteration 130/1000 | Loss: 0.00001974
Iteration 131/1000 | Loss: 0.00001974
Iteration 132/1000 | Loss: 0.00001974
Iteration 133/1000 | Loss: 0.00001974
Iteration 134/1000 | Loss: 0.00001974
Iteration 135/1000 | Loss: 0.00001974
Iteration 136/1000 | Loss: 0.00001974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.9741879441426136e-05, 1.9741879441426136e-05, 1.9741879441426136e-05, 1.9741879441426136e-05, 1.9741879441426136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9741879441426136e-05

Optimization complete. Final v2v error: 3.856893301010132 mm

Highest mean error: 4.189798831939697 mm for frame 38

Lowest mean error: 3.5015714168548584 mm for frame 266

Saving results

Total time: 40.023815393447876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00664033
Iteration 2/25 | Loss: 0.00103346
Iteration 3/25 | Loss: 0.00095620
Iteration 4/25 | Loss: 0.00094054
Iteration 5/25 | Loss: 0.00093549
Iteration 6/25 | Loss: 0.00093506
Iteration 7/25 | Loss: 0.00093506
Iteration 8/25 | Loss: 0.00093506
Iteration 9/25 | Loss: 0.00093506
Iteration 10/25 | Loss: 0.00093506
Iteration 11/25 | Loss: 0.00093506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000935058924369514, 0.000935058924369514, 0.000935058924369514, 0.000935058924369514, 0.000935058924369514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000935058924369514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39402449
Iteration 2/25 | Loss: 0.00040655
Iteration 3/25 | Loss: 0.00040655
Iteration 4/25 | Loss: 0.00040654
Iteration 5/25 | Loss: 0.00040654
Iteration 6/25 | Loss: 0.00040654
Iteration 7/25 | Loss: 0.00040654
Iteration 8/25 | Loss: 0.00040654
Iteration 9/25 | Loss: 0.00040654
Iteration 10/25 | Loss: 0.00040654
Iteration 11/25 | Loss: 0.00040654
Iteration 12/25 | Loss: 0.00040654
Iteration 13/25 | Loss: 0.00040654
Iteration 14/25 | Loss: 0.00040654
Iteration 15/25 | Loss: 0.00040654
Iteration 16/25 | Loss: 0.00040654
Iteration 17/25 | Loss: 0.00040654
Iteration 18/25 | Loss: 0.00040654
Iteration 19/25 | Loss: 0.00040654
Iteration 20/25 | Loss: 0.00040654
Iteration 21/25 | Loss: 0.00040654
Iteration 22/25 | Loss: 0.00040654
Iteration 23/25 | Loss: 0.00040654
Iteration 24/25 | Loss: 0.00040654
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0004065434914082289, 0.0004065434914082289, 0.0004065434914082289, 0.0004065434914082289, 0.0004065434914082289]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004065434914082289

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040654
Iteration 2/1000 | Loss: 0.00003373
Iteration 3/1000 | Loss: 0.00002400
Iteration 4/1000 | Loss: 0.00002203
Iteration 5/1000 | Loss: 0.00002106
Iteration 6/1000 | Loss: 0.00002038
Iteration 7/1000 | Loss: 0.00001979
Iteration 8/1000 | Loss: 0.00001947
Iteration 9/1000 | Loss: 0.00001922
Iteration 10/1000 | Loss: 0.00001918
Iteration 11/1000 | Loss: 0.00001915
Iteration 12/1000 | Loss: 0.00001913
Iteration 13/1000 | Loss: 0.00001912
Iteration 14/1000 | Loss: 0.00001912
Iteration 15/1000 | Loss: 0.00001911
Iteration 16/1000 | Loss: 0.00001911
Iteration 17/1000 | Loss: 0.00001910
Iteration 18/1000 | Loss: 0.00001908
Iteration 19/1000 | Loss: 0.00001907
Iteration 20/1000 | Loss: 0.00001900
Iteration 21/1000 | Loss: 0.00001899
Iteration 22/1000 | Loss: 0.00001899
Iteration 23/1000 | Loss: 0.00001899
Iteration 24/1000 | Loss: 0.00001899
Iteration 25/1000 | Loss: 0.00001898
Iteration 26/1000 | Loss: 0.00001898
Iteration 27/1000 | Loss: 0.00001898
Iteration 28/1000 | Loss: 0.00001897
Iteration 29/1000 | Loss: 0.00001894
Iteration 30/1000 | Loss: 0.00001894
Iteration 31/1000 | Loss: 0.00001893
Iteration 32/1000 | Loss: 0.00001892
Iteration 33/1000 | Loss: 0.00001892
Iteration 34/1000 | Loss: 0.00001891
Iteration 35/1000 | Loss: 0.00001890
Iteration 36/1000 | Loss: 0.00001890
Iteration 37/1000 | Loss: 0.00001890
Iteration 38/1000 | Loss: 0.00001890
Iteration 39/1000 | Loss: 0.00001890
Iteration 40/1000 | Loss: 0.00001890
Iteration 41/1000 | Loss: 0.00001889
Iteration 42/1000 | Loss: 0.00001889
Iteration 43/1000 | Loss: 0.00001889
Iteration 44/1000 | Loss: 0.00001889
Iteration 45/1000 | Loss: 0.00001887
Iteration 46/1000 | Loss: 0.00001887
Iteration 47/1000 | Loss: 0.00001887
Iteration 48/1000 | Loss: 0.00001886
Iteration 49/1000 | Loss: 0.00001886
Iteration 50/1000 | Loss: 0.00001886
Iteration 51/1000 | Loss: 0.00001885
Iteration 52/1000 | Loss: 0.00001885
Iteration 53/1000 | Loss: 0.00001885
Iteration 54/1000 | Loss: 0.00001885
Iteration 55/1000 | Loss: 0.00001885
Iteration 56/1000 | Loss: 0.00001885
Iteration 57/1000 | Loss: 0.00001885
Iteration 58/1000 | Loss: 0.00001885
Iteration 59/1000 | Loss: 0.00001885
Iteration 60/1000 | Loss: 0.00001885
Iteration 61/1000 | Loss: 0.00001885
Iteration 62/1000 | Loss: 0.00001885
Iteration 63/1000 | Loss: 0.00001885
Iteration 64/1000 | Loss: 0.00001885
Iteration 65/1000 | Loss: 0.00001885
Iteration 66/1000 | Loss: 0.00001885
Iteration 67/1000 | Loss: 0.00001885
Iteration 68/1000 | Loss: 0.00001885
Iteration 69/1000 | Loss: 0.00001885
Iteration 70/1000 | Loss: 0.00001885
Iteration 71/1000 | Loss: 0.00001885
Iteration 72/1000 | Loss: 0.00001885
Iteration 73/1000 | Loss: 0.00001885
Iteration 74/1000 | Loss: 0.00001885
Iteration 75/1000 | Loss: 0.00001885
Iteration 76/1000 | Loss: 0.00001885
Iteration 77/1000 | Loss: 0.00001885
Iteration 78/1000 | Loss: 0.00001885
Iteration 79/1000 | Loss: 0.00001885
Iteration 80/1000 | Loss: 0.00001885
Iteration 81/1000 | Loss: 0.00001885
Iteration 82/1000 | Loss: 0.00001885
Iteration 83/1000 | Loss: 0.00001885
Iteration 84/1000 | Loss: 0.00001885
Iteration 85/1000 | Loss: 0.00001885
Iteration 86/1000 | Loss: 0.00001885
Iteration 87/1000 | Loss: 0.00001885
Iteration 88/1000 | Loss: 0.00001885
Iteration 89/1000 | Loss: 0.00001885
Iteration 90/1000 | Loss: 0.00001885
Iteration 91/1000 | Loss: 0.00001885
Iteration 92/1000 | Loss: 0.00001885
Iteration 93/1000 | Loss: 0.00001885
Iteration 94/1000 | Loss: 0.00001885
Iteration 95/1000 | Loss: 0.00001885
Iteration 96/1000 | Loss: 0.00001885
Iteration 97/1000 | Loss: 0.00001885
Iteration 98/1000 | Loss: 0.00001885
Iteration 99/1000 | Loss: 0.00001885
Iteration 100/1000 | Loss: 0.00001885
Iteration 101/1000 | Loss: 0.00001885
Iteration 102/1000 | Loss: 0.00001885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.8846963939722627e-05, 1.8846963939722627e-05, 1.8846963939722627e-05, 1.8846963939722627e-05, 1.8846963939722627e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8846963939722627e-05

Optimization complete. Final v2v error: 3.798297882080078 mm

Highest mean error: 4.055812835693359 mm for frame 8

Lowest mean error: 3.55938458442688 mm for frame 69

Saving results

Total time: 30.582146406173706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01304956
Iteration 2/25 | Loss: 0.01304956
Iteration 3/25 | Loss: 0.00392689
Iteration 4/25 | Loss: 0.00269485
Iteration 5/25 | Loss: 0.00244973
Iteration 6/25 | Loss: 0.00222430
Iteration 7/25 | Loss: 0.00179830
Iteration 8/25 | Loss: 0.00169169
Iteration 9/25 | Loss: 0.00170861
Iteration 10/25 | Loss: 0.00164797
Iteration 11/25 | Loss: 0.00161227
Iteration 12/25 | Loss: 0.00161333
Iteration 13/25 | Loss: 0.00155489
Iteration 14/25 | Loss: 0.00158856
Iteration 15/25 | Loss: 0.00154781
Iteration 16/25 | Loss: 0.00152727
Iteration 17/25 | Loss: 0.00153453
Iteration 18/25 | Loss: 0.00151611
Iteration 19/25 | Loss: 0.00153827
Iteration 20/25 | Loss: 0.00155570
Iteration 21/25 | Loss: 0.00156018
Iteration 22/25 | Loss: 0.00149936
Iteration 23/25 | Loss: 0.00148033
Iteration 24/25 | Loss: 0.00149507
Iteration 25/25 | Loss: 0.00150875

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.61670637
Iteration 2/25 | Loss: 0.00532391
Iteration 3/25 | Loss: 0.00532390
Iteration 4/25 | Loss: 0.00532390
Iteration 5/25 | Loss: 0.00532390
Iteration 6/25 | Loss: 0.00532390
Iteration 7/25 | Loss: 0.00532390
Iteration 8/25 | Loss: 0.00532390
Iteration 9/25 | Loss: 0.00532390
Iteration 10/25 | Loss: 0.00532390
Iteration 11/25 | Loss: 0.00532390
Iteration 12/25 | Loss: 0.00532390
Iteration 13/25 | Loss: 0.00532390
Iteration 14/25 | Loss: 0.00532390
Iteration 15/25 | Loss: 0.00532390
Iteration 16/25 | Loss: 0.00532390
Iteration 17/25 | Loss: 0.00532390
Iteration 18/25 | Loss: 0.00532390
Iteration 19/25 | Loss: 0.00532390
Iteration 20/25 | Loss: 0.00532390
Iteration 21/25 | Loss: 0.00532390
Iteration 22/25 | Loss: 0.00532390
Iteration 23/25 | Loss: 0.00532390
Iteration 24/25 | Loss: 0.00532390
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.005323898978531361, 0.005323898978531361, 0.005323898978531361, 0.005323898978531361, 0.005323898978531361]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005323898978531361

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00532390
Iteration 2/1000 | Loss: 0.00372385
Iteration 3/1000 | Loss: 0.00314945
Iteration 4/1000 | Loss: 0.00237338
Iteration 5/1000 | Loss: 0.00227953
Iteration 6/1000 | Loss: 0.00140015
Iteration 7/1000 | Loss: 0.00299535
Iteration 8/1000 | Loss: 0.00170053
Iteration 9/1000 | Loss: 0.00423669
Iteration 10/1000 | Loss: 0.00214272
Iteration 11/1000 | Loss: 0.00116549
Iteration 12/1000 | Loss: 0.00138781
Iteration 13/1000 | Loss: 0.00113395
Iteration 14/1000 | Loss: 0.00106047
Iteration 15/1000 | Loss: 0.00080790
Iteration 16/1000 | Loss: 0.00239195
Iteration 17/1000 | Loss: 0.00077419
Iteration 18/1000 | Loss: 0.00154870
Iteration 19/1000 | Loss: 0.00056521
Iteration 20/1000 | Loss: 0.00533142
Iteration 21/1000 | Loss: 0.00606072
Iteration 22/1000 | Loss: 0.00278567
Iteration 23/1000 | Loss: 0.00211176
Iteration 24/1000 | Loss: 0.00272840
Iteration 25/1000 | Loss: 0.00226051
Iteration 26/1000 | Loss: 0.00057193
Iteration 27/1000 | Loss: 0.00167704
Iteration 28/1000 | Loss: 0.00199398
Iteration 29/1000 | Loss: 0.00093198
Iteration 30/1000 | Loss: 0.00166107
Iteration 31/1000 | Loss: 0.00181763
Iteration 32/1000 | Loss: 0.00230031
Iteration 33/1000 | Loss: 0.00137250
Iteration 34/1000 | Loss: 0.00135533
Iteration 35/1000 | Loss: 0.00336366
Iteration 36/1000 | Loss: 0.00359526
Iteration 37/1000 | Loss: 0.00132886
Iteration 38/1000 | Loss: 0.00231443
Iteration 39/1000 | Loss: 0.00157691
Iteration 40/1000 | Loss: 0.00186991
Iteration 41/1000 | Loss: 0.00315602
Iteration 42/1000 | Loss: 0.00045242
Iteration 43/1000 | Loss: 0.00233930
Iteration 44/1000 | Loss: 0.00178175
Iteration 45/1000 | Loss: 0.00155236
Iteration 46/1000 | Loss: 0.00276286
Iteration 47/1000 | Loss: 0.00118996
Iteration 48/1000 | Loss: 0.00302217
Iteration 49/1000 | Loss: 0.00051245
Iteration 50/1000 | Loss: 0.00272859
Iteration 51/1000 | Loss: 0.00098510
Iteration 52/1000 | Loss: 0.00038362
Iteration 53/1000 | Loss: 0.00045123
Iteration 54/1000 | Loss: 0.00021449
Iteration 55/1000 | Loss: 0.00028536
Iteration 56/1000 | Loss: 0.00094993
Iteration 57/1000 | Loss: 0.00204916
Iteration 58/1000 | Loss: 0.00023492
Iteration 59/1000 | Loss: 0.00060974
Iteration 60/1000 | Loss: 0.00082212
Iteration 61/1000 | Loss: 0.00079528
Iteration 62/1000 | Loss: 0.00046663
Iteration 63/1000 | Loss: 0.00113564
Iteration 64/1000 | Loss: 0.00151263
Iteration 65/1000 | Loss: 0.00153618
Iteration 66/1000 | Loss: 0.00105512
Iteration 67/1000 | Loss: 0.00080879
Iteration 68/1000 | Loss: 0.00062873
Iteration 69/1000 | Loss: 0.00202894
Iteration 70/1000 | Loss: 0.00151734
Iteration 71/1000 | Loss: 0.00029462
Iteration 72/1000 | Loss: 0.00139109
Iteration 73/1000 | Loss: 0.00096104
Iteration 74/1000 | Loss: 0.00076160
Iteration 75/1000 | Loss: 0.00082573
Iteration 76/1000 | Loss: 0.00083946
Iteration 77/1000 | Loss: 0.00173007
Iteration 78/1000 | Loss: 0.00066658
Iteration 79/1000 | Loss: 0.00018038
Iteration 80/1000 | Loss: 0.00072446
Iteration 81/1000 | Loss: 0.00104718
Iteration 82/1000 | Loss: 0.00098071
Iteration 83/1000 | Loss: 0.00084955
Iteration 84/1000 | Loss: 0.00071107
Iteration 85/1000 | Loss: 0.00075567
Iteration 86/1000 | Loss: 0.00040399
Iteration 87/1000 | Loss: 0.00050157
Iteration 88/1000 | Loss: 0.00054196
Iteration 89/1000 | Loss: 0.00101481
Iteration 90/1000 | Loss: 0.00057444
Iteration 91/1000 | Loss: 0.00017939
Iteration 92/1000 | Loss: 0.00049586
Iteration 93/1000 | Loss: 0.00026068
Iteration 94/1000 | Loss: 0.00026956
Iteration 95/1000 | Loss: 0.00018400
Iteration 96/1000 | Loss: 0.00054126
Iteration 97/1000 | Loss: 0.00026339
Iteration 98/1000 | Loss: 0.00035823
Iteration 99/1000 | Loss: 0.00037652
Iteration 100/1000 | Loss: 0.00041226
Iteration 101/1000 | Loss: 0.00234672
Iteration 102/1000 | Loss: 0.00171290
Iteration 103/1000 | Loss: 0.00049754
Iteration 104/1000 | Loss: 0.00067982
Iteration 105/1000 | Loss: 0.00074366
Iteration 106/1000 | Loss: 0.00070716
Iteration 107/1000 | Loss: 0.00123993
Iteration 108/1000 | Loss: 0.00084592
Iteration 109/1000 | Loss: 0.00141748
Iteration 110/1000 | Loss: 0.00180880
Iteration 111/1000 | Loss: 0.00132539
Iteration 112/1000 | Loss: 0.00053518
Iteration 113/1000 | Loss: 0.00023788
Iteration 114/1000 | Loss: 0.00035002
Iteration 115/1000 | Loss: 0.00110877
Iteration 116/1000 | Loss: 0.00077963
Iteration 117/1000 | Loss: 0.00021252
Iteration 118/1000 | Loss: 0.00011237
Iteration 119/1000 | Loss: 0.00301864
Iteration 120/1000 | Loss: 0.00167333
Iteration 121/1000 | Loss: 0.00213942
Iteration 122/1000 | Loss: 0.00162949
Iteration 123/1000 | Loss: 0.00077135
Iteration 124/1000 | Loss: 0.00100446
Iteration 125/1000 | Loss: 0.00073309
Iteration 126/1000 | Loss: 0.00057363
Iteration 127/1000 | Loss: 0.00014605
Iteration 128/1000 | Loss: 0.00224964
Iteration 129/1000 | Loss: 0.00236934
Iteration 130/1000 | Loss: 0.00177016
Iteration 131/1000 | Loss: 0.00099244
Iteration 132/1000 | Loss: 0.00076497
Iteration 133/1000 | Loss: 0.00064589
Iteration 134/1000 | Loss: 0.00102453
Iteration 135/1000 | Loss: 0.00096664
Iteration 136/1000 | Loss: 0.00022839
Iteration 137/1000 | Loss: 0.00027061
Iteration 138/1000 | Loss: 0.00022111
Iteration 139/1000 | Loss: 0.00038233
Iteration 140/1000 | Loss: 0.00009310
Iteration 141/1000 | Loss: 0.00205409
Iteration 142/1000 | Loss: 0.00147318
Iteration 143/1000 | Loss: 0.00013381
Iteration 144/1000 | Loss: 0.00009685
Iteration 145/1000 | Loss: 0.00026650
Iteration 146/1000 | Loss: 0.00020652
Iteration 147/1000 | Loss: 0.00008563
Iteration 148/1000 | Loss: 0.00007772
Iteration 149/1000 | Loss: 0.00006938
Iteration 150/1000 | Loss: 0.00109096
Iteration 151/1000 | Loss: 0.00027707
Iteration 152/1000 | Loss: 0.00029531
Iteration 153/1000 | Loss: 0.00040265
Iteration 154/1000 | Loss: 0.00027713
Iteration 155/1000 | Loss: 0.00031284
Iteration 156/1000 | Loss: 0.00007603
Iteration 157/1000 | Loss: 0.00007103
Iteration 158/1000 | Loss: 0.00006562
Iteration 159/1000 | Loss: 0.00006265
Iteration 160/1000 | Loss: 0.00086446
Iteration 161/1000 | Loss: 0.00006113
Iteration 162/1000 | Loss: 0.00005710
Iteration 163/1000 | Loss: 0.00005438
Iteration 164/1000 | Loss: 0.00005157
Iteration 165/1000 | Loss: 0.00005008
Iteration 166/1000 | Loss: 0.00004943
Iteration 167/1000 | Loss: 0.00147938
Iteration 168/1000 | Loss: 0.00094999
Iteration 169/1000 | Loss: 0.00007805
Iteration 170/1000 | Loss: 0.00005924
Iteration 171/1000 | Loss: 0.00005171
Iteration 172/1000 | Loss: 0.00004837
Iteration 173/1000 | Loss: 0.00005858
Iteration 174/1000 | Loss: 0.00005246
Iteration 175/1000 | Loss: 0.00005054
Iteration 176/1000 | Loss: 0.00005245
Iteration 177/1000 | Loss: 0.00005022
Iteration 178/1000 | Loss: 0.00004716
Iteration 179/1000 | Loss: 0.00004420
Iteration 180/1000 | Loss: 0.00004275
Iteration 181/1000 | Loss: 0.00004211
Iteration 182/1000 | Loss: 0.00004159
Iteration 183/1000 | Loss: 0.00004103
Iteration 184/1000 | Loss: 0.00004060
Iteration 185/1000 | Loss: 0.00004041
Iteration 186/1000 | Loss: 0.00004041
Iteration 187/1000 | Loss: 0.00004041
Iteration 188/1000 | Loss: 0.00004040
Iteration 189/1000 | Loss: 0.00004039
Iteration 190/1000 | Loss: 0.00004038
Iteration 191/1000 | Loss: 0.00004036
Iteration 192/1000 | Loss: 0.00004033
Iteration 193/1000 | Loss: 0.00004031
Iteration 194/1000 | Loss: 0.00004030
Iteration 195/1000 | Loss: 0.00004029
Iteration 196/1000 | Loss: 0.00004029
Iteration 197/1000 | Loss: 0.00004028
Iteration 198/1000 | Loss: 0.00004028
Iteration 199/1000 | Loss: 0.00004028
Iteration 200/1000 | Loss: 0.00004026
Iteration 201/1000 | Loss: 0.00004026
Iteration 202/1000 | Loss: 0.00004026
Iteration 203/1000 | Loss: 0.00004026
Iteration 204/1000 | Loss: 0.00004025
Iteration 205/1000 | Loss: 0.00004025
Iteration 206/1000 | Loss: 0.00004025
Iteration 207/1000 | Loss: 0.00004025
Iteration 208/1000 | Loss: 0.00004025
Iteration 209/1000 | Loss: 0.00004025
Iteration 210/1000 | Loss: 0.00004025
Iteration 211/1000 | Loss: 0.00004025
Iteration 212/1000 | Loss: 0.00004025
Iteration 213/1000 | Loss: 0.00004024
Iteration 214/1000 | Loss: 0.00004024
Iteration 215/1000 | Loss: 0.00004022
Iteration 216/1000 | Loss: 0.00004021
Iteration 217/1000 | Loss: 0.00004020
Iteration 218/1000 | Loss: 0.00004019
Iteration 219/1000 | Loss: 0.00004019
Iteration 220/1000 | Loss: 0.00004018
Iteration 221/1000 | Loss: 0.00004018
Iteration 222/1000 | Loss: 0.00004018
Iteration 223/1000 | Loss: 0.00004017
Iteration 224/1000 | Loss: 0.00004017
Iteration 225/1000 | Loss: 0.00004016
Iteration 226/1000 | Loss: 0.00004013
Iteration 227/1000 | Loss: 0.00004012
Iteration 228/1000 | Loss: 0.00004012
Iteration 229/1000 | Loss: 0.00004011
Iteration 230/1000 | Loss: 0.00004010
Iteration 231/1000 | Loss: 0.00004010
Iteration 232/1000 | Loss: 0.00004009
Iteration 233/1000 | Loss: 0.00004009
Iteration 234/1000 | Loss: 0.00004008
Iteration 235/1000 | Loss: 0.00004008
Iteration 236/1000 | Loss: 0.00004008
Iteration 237/1000 | Loss: 0.00004007
Iteration 238/1000 | Loss: 0.00004007
Iteration 239/1000 | Loss: 0.00004005
Iteration 240/1000 | Loss: 0.00004005
Iteration 241/1000 | Loss: 0.00004005
Iteration 242/1000 | Loss: 0.00004005
Iteration 243/1000 | Loss: 0.00004004
Iteration 244/1000 | Loss: 0.00004003
Iteration 245/1000 | Loss: 0.00004003
Iteration 246/1000 | Loss: 0.00004002
Iteration 247/1000 | Loss: 0.00004002
Iteration 248/1000 | Loss: 0.00004002
Iteration 249/1000 | Loss: 0.00004000
Iteration 250/1000 | Loss: 0.00003999
Iteration 251/1000 | Loss: 0.00003998
Iteration 252/1000 | Loss: 0.00003998
Iteration 253/1000 | Loss: 0.00003998
Iteration 254/1000 | Loss: 0.00003998
Iteration 255/1000 | Loss: 0.00003998
Iteration 256/1000 | Loss: 0.00003997
Iteration 257/1000 | Loss: 0.00003997
Iteration 258/1000 | Loss: 0.00003996
Iteration 259/1000 | Loss: 0.00003994
Iteration 260/1000 | Loss: 0.00003993
Iteration 261/1000 | Loss: 0.00003993
Iteration 262/1000 | Loss: 0.00003983
Iteration 263/1000 | Loss: 0.00003983
Iteration 264/1000 | Loss: 0.00003982
Iteration 265/1000 | Loss: 0.00003981
Iteration 266/1000 | Loss: 0.00003981
Iteration 267/1000 | Loss: 0.00003980
Iteration 268/1000 | Loss: 0.00003980
Iteration 269/1000 | Loss: 0.00003980
Iteration 270/1000 | Loss: 0.00003979
Iteration 271/1000 | Loss: 0.00003979
Iteration 272/1000 | Loss: 0.00003974
Iteration 273/1000 | Loss: 0.00003973
Iteration 274/1000 | Loss: 0.00003973
Iteration 275/1000 | Loss: 0.00003973
Iteration 276/1000 | Loss: 0.00003972
Iteration 277/1000 | Loss: 0.00003972
Iteration 278/1000 | Loss: 0.00003972
Iteration 279/1000 | Loss: 0.00003972
Iteration 280/1000 | Loss: 0.00003972
Iteration 281/1000 | Loss: 0.00003972
Iteration 282/1000 | Loss: 0.00003972
Iteration 283/1000 | Loss: 0.00003971
Iteration 284/1000 | Loss: 0.00003971
Iteration 285/1000 | Loss: 0.00003971
Iteration 286/1000 | Loss: 0.00003971
Iteration 287/1000 | Loss: 0.00003971
Iteration 288/1000 | Loss: 0.00003971
Iteration 289/1000 | Loss: 0.00003971
Iteration 290/1000 | Loss: 0.00003971
Iteration 291/1000 | Loss: 0.00003971
Iteration 292/1000 | Loss: 0.00003971
Iteration 293/1000 | Loss: 0.00003970
Iteration 294/1000 | Loss: 0.00003970
Iteration 295/1000 | Loss: 0.00003970
Iteration 296/1000 | Loss: 0.00003970
Iteration 297/1000 | Loss: 0.00003970
Iteration 298/1000 | Loss: 0.00003970
Iteration 299/1000 | Loss: 0.00003970
Iteration 300/1000 | Loss: 0.00003969
Iteration 301/1000 | Loss: 0.00003969
Iteration 302/1000 | Loss: 0.00003969
Iteration 303/1000 | Loss: 0.00003969
Iteration 304/1000 | Loss: 0.00003969
Iteration 305/1000 | Loss: 0.00003968
Iteration 306/1000 | Loss: 0.00003968
Iteration 307/1000 | Loss: 0.00003968
Iteration 308/1000 | Loss: 0.00003968
Iteration 309/1000 | Loss: 0.00003968
Iteration 310/1000 | Loss: 0.00003968
Iteration 311/1000 | Loss: 0.00003968
Iteration 312/1000 | Loss: 0.00003968
Iteration 313/1000 | Loss: 0.00003968
Iteration 314/1000 | Loss: 0.00003968
Iteration 315/1000 | Loss: 0.00003968
Iteration 316/1000 | Loss: 0.00003968
Iteration 317/1000 | Loss: 0.00003967
Iteration 318/1000 | Loss: 0.00003967
Iteration 319/1000 | Loss: 0.00003967
Iteration 320/1000 | Loss: 0.00003967
Iteration 321/1000 | Loss: 0.00003967
Iteration 322/1000 | Loss: 0.00003967
Iteration 323/1000 | Loss: 0.00003967
Iteration 324/1000 | Loss: 0.00003967
Iteration 325/1000 | Loss: 0.00003967
Iteration 326/1000 | Loss: 0.00003966
Iteration 327/1000 | Loss: 0.00003966
Iteration 328/1000 | Loss: 0.00003966
Iteration 329/1000 | Loss: 0.00003966
Iteration 330/1000 | Loss: 0.00003966
Iteration 331/1000 | Loss: 0.00003966
Iteration 332/1000 | Loss: 0.00003966
Iteration 333/1000 | Loss: 0.00003966
Iteration 334/1000 | Loss: 0.00003966
Iteration 335/1000 | Loss: 0.00003966
Iteration 336/1000 | Loss: 0.00003966
Iteration 337/1000 | Loss: 0.00003966
Iteration 338/1000 | Loss: 0.00003966
Iteration 339/1000 | Loss: 0.00003966
Iteration 340/1000 | Loss: 0.00003966
Iteration 341/1000 | Loss: 0.00003966
Iteration 342/1000 | Loss: 0.00003965
Iteration 343/1000 | Loss: 0.00003965
Iteration 344/1000 | Loss: 0.00003965
Iteration 345/1000 | Loss: 0.00003965
Iteration 346/1000 | Loss: 0.00003965
Iteration 347/1000 | Loss: 0.00003965
Iteration 348/1000 | Loss: 0.00003965
Iteration 349/1000 | Loss: 0.00003965
Iteration 350/1000 | Loss: 0.00003965
Iteration 351/1000 | Loss: 0.00003965
Iteration 352/1000 | Loss: 0.00003965
Iteration 353/1000 | Loss: 0.00003965
Iteration 354/1000 | Loss: 0.00003965
Iteration 355/1000 | Loss: 0.00003965
Iteration 356/1000 | Loss: 0.00003965
Iteration 357/1000 | Loss: 0.00003965
Iteration 358/1000 | Loss: 0.00003965
Iteration 359/1000 | Loss: 0.00003965
Iteration 360/1000 | Loss: 0.00003965
Iteration 361/1000 | Loss: 0.00003965
Iteration 362/1000 | Loss: 0.00003965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 362. Stopping optimization.
Last 5 losses: [3.965071664424613e-05, 3.965071664424613e-05, 3.965071664424613e-05, 3.965071664424613e-05, 3.965071664424613e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.965071664424613e-05

Optimization complete. Final v2v error: 5.1013946533203125 mm

Highest mean error: 6.2244110107421875 mm for frame 148

Lowest mean error: 4.198085784912109 mm for frame 65

Saving results

Total time: 322.278977394104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00900571
Iteration 2/25 | Loss: 0.00132901
Iteration 3/25 | Loss: 0.00105159
Iteration 4/25 | Loss: 0.00102203
Iteration 5/25 | Loss: 0.00101592
Iteration 6/25 | Loss: 0.00101445
Iteration 7/25 | Loss: 0.00101445
Iteration 8/25 | Loss: 0.00101445
Iteration 9/25 | Loss: 0.00101445
Iteration 10/25 | Loss: 0.00101445
Iteration 11/25 | Loss: 0.00101445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010144549887627363, 0.0010144549887627363, 0.0010144549887627363, 0.0010144549887627363, 0.0010144549887627363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010144549887627363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89464200
Iteration 2/25 | Loss: 0.00052418
Iteration 3/25 | Loss: 0.00052418
Iteration 4/25 | Loss: 0.00052418
Iteration 5/25 | Loss: 0.00052418
Iteration 6/25 | Loss: 0.00052418
Iteration 7/25 | Loss: 0.00052418
Iteration 8/25 | Loss: 0.00052418
Iteration 9/25 | Loss: 0.00052418
Iteration 10/25 | Loss: 0.00052418
Iteration 11/25 | Loss: 0.00052418
Iteration 12/25 | Loss: 0.00052418
Iteration 13/25 | Loss: 0.00052418
Iteration 14/25 | Loss: 0.00052418
Iteration 15/25 | Loss: 0.00052418
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005241786129772663, 0.0005241786129772663, 0.0005241786129772663, 0.0005241786129772663, 0.0005241786129772663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005241786129772663

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052418
Iteration 2/1000 | Loss: 0.00004793
Iteration 3/1000 | Loss: 0.00003514
Iteration 4/1000 | Loss: 0.00003255
Iteration 5/1000 | Loss: 0.00003115
Iteration 6/1000 | Loss: 0.00003015
Iteration 7/1000 | Loss: 0.00002951
Iteration 8/1000 | Loss: 0.00002896
Iteration 9/1000 | Loss: 0.00002872
Iteration 10/1000 | Loss: 0.00002857
Iteration 11/1000 | Loss: 0.00002843
Iteration 12/1000 | Loss: 0.00002841
Iteration 13/1000 | Loss: 0.00002839
Iteration 14/1000 | Loss: 0.00002835
Iteration 15/1000 | Loss: 0.00002834
Iteration 16/1000 | Loss: 0.00002834
Iteration 17/1000 | Loss: 0.00002834
Iteration 18/1000 | Loss: 0.00002834
Iteration 19/1000 | Loss: 0.00002834
Iteration 20/1000 | Loss: 0.00002834
Iteration 21/1000 | Loss: 0.00002834
Iteration 22/1000 | Loss: 0.00002834
Iteration 23/1000 | Loss: 0.00002834
Iteration 24/1000 | Loss: 0.00002833
Iteration 25/1000 | Loss: 0.00002831
Iteration 26/1000 | Loss: 0.00002830
Iteration 27/1000 | Loss: 0.00002822
Iteration 28/1000 | Loss: 0.00002820
Iteration 29/1000 | Loss: 0.00002820
Iteration 30/1000 | Loss: 0.00002820
Iteration 31/1000 | Loss: 0.00002820
Iteration 32/1000 | Loss: 0.00002820
Iteration 33/1000 | Loss: 0.00002819
Iteration 34/1000 | Loss: 0.00002819
Iteration 35/1000 | Loss: 0.00002819
Iteration 36/1000 | Loss: 0.00002819
Iteration 37/1000 | Loss: 0.00002819
Iteration 38/1000 | Loss: 0.00002818
Iteration 39/1000 | Loss: 0.00002818
Iteration 40/1000 | Loss: 0.00002818
Iteration 41/1000 | Loss: 0.00002817
Iteration 42/1000 | Loss: 0.00002817
Iteration 43/1000 | Loss: 0.00002817
Iteration 44/1000 | Loss: 0.00002816
Iteration 45/1000 | Loss: 0.00002816
Iteration 46/1000 | Loss: 0.00002816
Iteration 47/1000 | Loss: 0.00002816
Iteration 48/1000 | Loss: 0.00002815
Iteration 49/1000 | Loss: 0.00002815
Iteration 50/1000 | Loss: 0.00002815
Iteration 51/1000 | Loss: 0.00002815
Iteration 52/1000 | Loss: 0.00002814
Iteration 53/1000 | Loss: 0.00002814
Iteration 54/1000 | Loss: 0.00002814
Iteration 55/1000 | Loss: 0.00002814
Iteration 56/1000 | Loss: 0.00002814
Iteration 57/1000 | Loss: 0.00002813
Iteration 58/1000 | Loss: 0.00002813
Iteration 59/1000 | Loss: 0.00002813
Iteration 60/1000 | Loss: 0.00002813
Iteration 61/1000 | Loss: 0.00002813
Iteration 62/1000 | Loss: 0.00002813
Iteration 63/1000 | Loss: 0.00002812
Iteration 64/1000 | Loss: 0.00002812
Iteration 65/1000 | Loss: 0.00002812
Iteration 66/1000 | Loss: 0.00002812
Iteration 67/1000 | Loss: 0.00002812
Iteration 68/1000 | Loss: 0.00002811
Iteration 69/1000 | Loss: 0.00002811
Iteration 70/1000 | Loss: 0.00002811
Iteration 71/1000 | Loss: 0.00002811
Iteration 72/1000 | Loss: 0.00002811
Iteration 73/1000 | Loss: 0.00002811
Iteration 74/1000 | Loss: 0.00002811
Iteration 75/1000 | Loss: 0.00002811
Iteration 76/1000 | Loss: 0.00002811
Iteration 77/1000 | Loss: 0.00002810
Iteration 78/1000 | Loss: 0.00002810
Iteration 79/1000 | Loss: 0.00002810
Iteration 80/1000 | Loss: 0.00002809
Iteration 81/1000 | Loss: 0.00002809
Iteration 82/1000 | Loss: 0.00002809
Iteration 83/1000 | Loss: 0.00002809
Iteration 84/1000 | Loss: 0.00002809
Iteration 85/1000 | Loss: 0.00002809
Iteration 86/1000 | Loss: 0.00002809
Iteration 87/1000 | Loss: 0.00002809
Iteration 88/1000 | Loss: 0.00002809
Iteration 89/1000 | Loss: 0.00002809
Iteration 90/1000 | Loss: 0.00002809
Iteration 91/1000 | Loss: 0.00002808
Iteration 92/1000 | Loss: 0.00002808
Iteration 93/1000 | Loss: 0.00002808
Iteration 94/1000 | Loss: 0.00002808
Iteration 95/1000 | Loss: 0.00002808
Iteration 96/1000 | Loss: 0.00002808
Iteration 97/1000 | Loss: 0.00002808
Iteration 98/1000 | Loss: 0.00002808
Iteration 99/1000 | Loss: 0.00002808
Iteration 100/1000 | Loss: 0.00002808
Iteration 101/1000 | Loss: 0.00002807
Iteration 102/1000 | Loss: 0.00002807
Iteration 103/1000 | Loss: 0.00002807
Iteration 104/1000 | Loss: 0.00002807
Iteration 105/1000 | Loss: 0.00002807
Iteration 106/1000 | Loss: 0.00002807
Iteration 107/1000 | Loss: 0.00002807
Iteration 108/1000 | Loss: 0.00002807
Iteration 109/1000 | Loss: 0.00002807
Iteration 110/1000 | Loss: 0.00002807
Iteration 111/1000 | Loss: 0.00002806
Iteration 112/1000 | Loss: 0.00002806
Iteration 113/1000 | Loss: 0.00002806
Iteration 114/1000 | Loss: 0.00002806
Iteration 115/1000 | Loss: 0.00002806
Iteration 116/1000 | Loss: 0.00002806
Iteration 117/1000 | Loss: 0.00002806
Iteration 118/1000 | Loss: 0.00002806
Iteration 119/1000 | Loss: 0.00002806
Iteration 120/1000 | Loss: 0.00002806
Iteration 121/1000 | Loss: 0.00002806
Iteration 122/1000 | Loss: 0.00002806
Iteration 123/1000 | Loss: 0.00002806
Iteration 124/1000 | Loss: 0.00002806
Iteration 125/1000 | Loss: 0.00002806
Iteration 126/1000 | Loss: 0.00002806
Iteration 127/1000 | Loss: 0.00002806
Iteration 128/1000 | Loss: 0.00002806
Iteration 129/1000 | Loss: 0.00002805
Iteration 130/1000 | Loss: 0.00002805
Iteration 131/1000 | Loss: 0.00002805
Iteration 132/1000 | Loss: 0.00002805
Iteration 133/1000 | Loss: 0.00002805
Iteration 134/1000 | Loss: 0.00002805
Iteration 135/1000 | Loss: 0.00002805
Iteration 136/1000 | Loss: 0.00002805
Iteration 137/1000 | Loss: 0.00002805
Iteration 138/1000 | Loss: 0.00002805
Iteration 139/1000 | Loss: 0.00002805
Iteration 140/1000 | Loss: 0.00002805
Iteration 141/1000 | Loss: 0.00002805
Iteration 142/1000 | Loss: 0.00002805
Iteration 143/1000 | Loss: 0.00002804
Iteration 144/1000 | Loss: 0.00002804
Iteration 145/1000 | Loss: 0.00002804
Iteration 146/1000 | Loss: 0.00002804
Iteration 147/1000 | Loss: 0.00002804
Iteration 148/1000 | Loss: 0.00002804
Iteration 149/1000 | Loss: 0.00002804
Iteration 150/1000 | Loss: 0.00002804
Iteration 151/1000 | Loss: 0.00002804
Iteration 152/1000 | Loss: 0.00002804
Iteration 153/1000 | Loss: 0.00002804
Iteration 154/1000 | Loss: 0.00002804
Iteration 155/1000 | Loss: 0.00002804
Iteration 156/1000 | Loss: 0.00002804
Iteration 157/1000 | Loss: 0.00002804
Iteration 158/1000 | Loss: 0.00002804
Iteration 159/1000 | Loss: 0.00002804
Iteration 160/1000 | Loss: 0.00002804
Iteration 161/1000 | Loss: 0.00002804
Iteration 162/1000 | Loss: 0.00002804
Iteration 163/1000 | Loss: 0.00002804
Iteration 164/1000 | Loss: 0.00002804
Iteration 165/1000 | Loss: 0.00002804
Iteration 166/1000 | Loss: 0.00002804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.803544521157164e-05, 2.803544521157164e-05, 2.803544521157164e-05, 2.803544521157164e-05, 2.803544521157164e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.803544521157164e-05

Optimization complete. Final v2v error: 4.57079553604126 mm

Highest mean error: 4.763175964355469 mm for frame 50

Lowest mean error: 4.295152187347412 mm for frame 131

Saving results

Total time: 41.21435880661011
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874309
Iteration 2/25 | Loss: 0.00104074
Iteration 3/25 | Loss: 0.00090585
Iteration 4/25 | Loss: 0.00088392
Iteration 5/25 | Loss: 0.00087955
Iteration 6/25 | Loss: 0.00087901
Iteration 7/25 | Loss: 0.00087901
Iteration 8/25 | Loss: 0.00087901
Iteration 9/25 | Loss: 0.00087901
Iteration 10/25 | Loss: 0.00087901
Iteration 11/25 | Loss: 0.00087901
Iteration 12/25 | Loss: 0.00087901
Iteration 13/25 | Loss: 0.00087901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008790069259703159, 0.0008790069259703159, 0.0008790069259703159, 0.0008790069259703159, 0.0008790069259703159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008790069259703159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36226249
Iteration 2/25 | Loss: 0.00037288
Iteration 3/25 | Loss: 0.00037288
Iteration 4/25 | Loss: 0.00037288
Iteration 5/25 | Loss: 0.00037288
Iteration 6/25 | Loss: 0.00037288
Iteration 7/25 | Loss: 0.00037288
Iteration 8/25 | Loss: 0.00037288
Iteration 9/25 | Loss: 0.00037288
Iteration 10/25 | Loss: 0.00037288
Iteration 11/25 | Loss: 0.00037288
Iteration 12/25 | Loss: 0.00037288
Iteration 13/25 | Loss: 0.00037288
Iteration 14/25 | Loss: 0.00037288
Iteration 15/25 | Loss: 0.00037288
Iteration 16/25 | Loss: 0.00037288
Iteration 17/25 | Loss: 0.00037288
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000372881448129192, 0.000372881448129192, 0.000372881448129192, 0.000372881448129192, 0.000372881448129192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000372881448129192

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037288
Iteration 2/1000 | Loss: 0.00004009
Iteration 3/1000 | Loss: 0.00002040
Iteration 4/1000 | Loss: 0.00001770
Iteration 5/1000 | Loss: 0.00001692
Iteration 6/1000 | Loss: 0.00001645
Iteration 7/1000 | Loss: 0.00001608
Iteration 8/1000 | Loss: 0.00001580
Iteration 9/1000 | Loss: 0.00001577
Iteration 10/1000 | Loss: 0.00001573
Iteration 11/1000 | Loss: 0.00001565
Iteration 12/1000 | Loss: 0.00001563
Iteration 13/1000 | Loss: 0.00001554
Iteration 14/1000 | Loss: 0.00001550
Iteration 15/1000 | Loss: 0.00001542
Iteration 16/1000 | Loss: 0.00001542
Iteration 17/1000 | Loss: 0.00001541
Iteration 18/1000 | Loss: 0.00001541
Iteration 19/1000 | Loss: 0.00001541
Iteration 20/1000 | Loss: 0.00001541
Iteration 21/1000 | Loss: 0.00001541
Iteration 22/1000 | Loss: 0.00001540
Iteration 23/1000 | Loss: 0.00001540
Iteration 24/1000 | Loss: 0.00001539
Iteration 25/1000 | Loss: 0.00001539
Iteration 26/1000 | Loss: 0.00001538
Iteration 27/1000 | Loss: 0.00001538
Iteration 28/1000 | Loss: 0.00001538
Iteration 29/1000 | Loss: 0.00001537
Iteration 30/1000 | Loss: 0.00001537
Iteration 31/1000 | Loss: 0.00001536
Iteration 32/1000 | Loss: 0.00001536
Iteration 33/1000 | Loss: 0.00001536
Iteration 34/1000 | Loss: 0.00001533
Iteration 35/1000 | Loss: 0.00001533
Iteration 36/1000 | Loss: 0.00001533
Iteration 37/1000 | Loss: 0.00001533
Iteration 38/1000 | Loss: 0.00001533
Iteration 39/1000 | Loss: 0.00001532
Iteration 40/1000 | Loss: 0.00001532
Iteration 41/1000 | Loss: 0.00001531
Iteration 42/1000 | Loss: 0.00001531
Iteration 43/1000 | Loss: 0.00001531
Iteration 44/1000 | Loss: 0.00001527
Iteration 45/1000 | Loss: 0.00001527
Iteration 46/1000 | Loss: 0.00001527
Iteration 47/1000 | Loss: 0.00001527
Iteration 48/1000 | Loss: 0.00001524
Iteration 49/1000 | Loss: 0.00001524
Iteration 50/1000 | Loss: 0.00001524
Iteration 51/1000 | Loss: 0.00001523
Iteration 52/1000 | Loss: 0.00001523
Iteration 53/1000 | Loss: 0.00001523
Iteration 54/1000 | Loss: 0.00001523
Iteration 55/1000 | Loss: 0.00001523
Iteration 56/1000 | Loss: 0.00001523
Iteration 57/1000 | Loss: 0.00001523
Iteration 58/1000 | Loss: 0.00001523
Iteration 59/1000 | Loss: 0.00001523
Iteration 60/1000 | Loss: 0.00001522
Iteration 61/1000 | Loss: 0.00001522
Iteration 62/1000 | Loss: 0.00001521
Iteration 63/1000 | Loss: 0.00001521
Iteration 64/1000 | Loss: 0.00001521
Iteration 65/1000 | Loss: 0.00001521
Iteration 66/1000 | Loss: 0.00001521
Iteration 67/1000 | Loss: 0.00001521
Iteration 68/1000 | Loss: 0.00001521
Iteration 69/1000 | Loss: 0.00001521
Iteration 70/1000 | Loss: 0.00001521
Iteration 71/1000 | Loss: 0.00001521
Iteration 72/1000 | Loss: 0.00001521
Iteration 73/1000 | Loss: 0.00001521
Iteration 74/1000 | Loss: 0.00001521
Iteration 75/1000 | Loss: 0.00001521
Iteration 76/1000 | Loss: 0.00001521
Iteration 77/1000 | Loss: 0.00001521
Iteration 78/1000 | Loss: 0.00001521
Iteration 79/1000 | Loss: 0.00001521
Iteration 80/1000 | Loss: 0.00001521
Iteration 81/1000 | Loss: 0.00001521
Iteration 82/1000 | Loss: 0.00001521
Iteration 83/1000 | Loss: 0.00001521
Iteration 84/1000 | Loss: 0.00001521
Iteration 85/1000 | Loss: 0.00001521
Iteration 86/1000 | Loss: 0.00001521
Iteration 87/1000 | Loss: 0.00001521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.5206745047180448e-05, 1.5206745047180448e-05, 1.5206745047180448e-05, 1.5206745047180448e-05, 1.5206745047180448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5206745047180448e-05

Optimization complete. Final v2v error: 3.4016852378845215 mm

Highest mean error: 3.570530652999878 mm for frame 17

Lowest mean error: 3.2221920490264893 mm for frame 32

Saving results

Total time: 29.532867670059204
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00888067
Iteration 2/25 | Loss: 0.00145835
Iteration 3/25 | Loss: 0.00115025
Iteration 4/25 | Loss: 0.00108436
Iteration 5/25 | Loss: 0.00106648
Iteration 6/25 | Loss: 0.00105718
Iteration 7/25 | Loss: 0.00104759
Iteration 8/25 | Loss: 0.00105086
Iteration 9/25 | Loss: 0.00104070
Iteration 10/25 | Loss: 0.00103624
Iteration 11/25 | Loss: 0.00104124
Iteration 12/25 | Loss: 0.00103386
Iteration 13/25 | Loss: 0.00103449
Iteration 14/25 | Loss: 0.00103122
Iteration 15/25 | Loss: 0.00102996
Iteration 16/25 | Loss: 0.00102930
Iteration 17/25 | Loss: 0.00102893
Iteration 18/25 | Loss: 0.00102883
Iteration 19/25 | Loss: 0.00102881
Iteration 20/25 | Loss: 0.00102881
Iteration 21/25 | Loss: 0.00102879
Iteration 22/25 | Loss: 0.00102879
Iteration 23/25 | Loss: 0.00102879
Iteration 24/25 | Loss: 0.00102878
Iteration 25/25 | Loss: 0.00102878

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42159724
Iteration 2/25 | Loss: 0.00066449
Iteration 3/25 | Loss: 0.00066449
Iteration 4/25 | Loss: 0.00066449
Iteration 5/25 | Loss: 0.00066448
Iteration 6/25 | Loss: 0.00066448
Iteration 7/25 | Loss: 0.00066448
Iteration 8/25 | Loss: 0.00066448
Iteration 9/25 | Loss: 0.00066448
Iteration 10/25 | Loss: 0.00066448
Iteration 11/25 | Loss: 0.00066448
Iteration 12/25 | Loss: 0.00066448
Iteration 13/25 | Loss: 0.00066448
Iteration 14/25 | Loss: 0.00066448
Iteration 15/25 | Loss: 0.00066448
Iteration 16/25 | Loss: 0.00066448
Iteration 17/25 | Loss: 0.00066448
Iteration 18/25 | Loss: 0.00066448
Iteration 19/25 | Loss: 0.00066448
Iteration 20/25 | Loss: 0.00066448
Iteration 21/25 | Loss: 0.00066448
Iteration 22/25 | Loss: 0.00066448
Iteration 23/25 | Loss: 0.00066448
Iteration 24/25 | Loss: 0.00066448
Iteration 25/25 | Loss: 0.00066448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066448
Iteration 2/1000 | Loss: 0.00009115
Iteration 3/1000 | Loss: 0.00006272
Iteration 4/1000 | Loss: 0.00005429
Iteration 5/1000 | Loss: 0.00004960
Iteration 6/1000 | Loss: 0.00060115
Iteration 7/1000 | Loss: 0.00048863
Iteration 8/1000 | Loss: 0.00006460
Iteration 9/1000 | Loss: 0.00005252
Iteration 10/1000 | Loss: 0.00004753
Iteration 11/1000 | Loss: 0.00004523
Iteration 12/1000 | Loss: 0.00054194
Iteration 13/1000 | Loss: 0.00005136
Iteration 14/1000 | Loss: 0.00004539
Iteration 15/1000 | Loss: 0.00004270
Iteration 16/1000 | Loss: 0.00049880
Iteration 17/1000 | Loss: 0.00040825
Iteration 18/1000 | Loss: 0.00005005
Iteration 19/1000 | Loss: 0.00004279
Iteration 20/1000 | Loss: 0.00003946
Iteration 21/1000 | Loss: 0.00003741
Iteration 22/1000 | Loss: 0.00003597
Iteration 23/1000 | Loss: 0.00003504
Iteration 24/1000 | Loss: 0.00003389
Iteration 25/1000 | Loss: 0.00003224
Iteration 26/1000 | Loss: 0.00003157
Iteration 27/1000 | Loss: 0.00003125
Iteration 28/1000 | Loss: 0.00003110
Iteration 29/1000 | Loss: 0.00003109
Iteration 30/1000 | Loss: 0.00003106
Iteration 31/1000 | Loss: 0.00003103
Iteration 32/1000 | Loss: 0.00003102
Iteration 33/1000 | Loss: 0.00003102
Iteration 34/1000 | Loss: 0.00003101
Iteration 35/1000 | Loss: 0.00003101
Iteration 36/1000 | Loss: 0.00003100
Iteration 37/1000 | Loss: 0.00003100
Iteration 38/1000 | Loss: 0.00003100
Iteration 39/1000 | Loss: 0.00003099
Iteration 40/1000 | Loss: 0.00003099
Iteration 41/1000 | Loss: 0.00003099
Iteration 42/1000 | Loss: 0.00003098
Iteration 43/1000 | Loss: 0.00003098
Iteration 44/1000 | Loss: 0.00003098
Iteration 45/1000 | Loss: 0.00003098
Iteration 46/1000 | Loss: 0.00003098
Iteration 47/1000 | Loss: 0.00003098
Iteration 48/1000 | Loss: 0.00003097
Iteration 49/1000 | Loss: 0.00003096
Iteration 50/1000 | Loss: 0.00003096
Iteration 51/1000 | Loss: 0.00003096
Iteration 52/1000 | Loss: 0.00003096
Iteration 53/1000 | Loss: 0.00003096
Iteration 54/1000 | Loss: 0.00003096
Iteration 55/1000 | Loss: 0.00003095
Iteration 56/1000 | Loss: 0.00003095
Iteration 57/1000 | Loss: 0.00003094
Iteration 58/1000 | Loss: 0.00003094
Iteration 59/1000 | Loss: 0.00003094
Iteration 60/1000 | Loss: 0.00003093
Iteration 61/1000 | Loss: 0.00003093
Iteration 62/1000 | Loss: 0.00003093
Iteration 63/1000 | Loss: 0.00003093
Iteration 64/1000 | Loss: 0.00003093
Iteration 65/1000 | Loss: 0.00003093
Iteration 66/1000 | Loss: 0.00003092
Iteration 67/1000 | Loss: 0.00003092
Iteration 68/1000 | Loss: 0.00003092
Iteration 69/1000 | Loss: 0.00003092
Iteration 70/1000 | Loss: 0.00003092
Iteration 71/1000 | Loss: 0.00003092
Iteration 72/1000 | Loss: 0.00003092
Iteration 73/1000 | Loss: 0.00003092
Iteration 74/1000 | Loss: 0.00003092
Iteration 75/1000 | Loss: 0.00003092
Iteration 76/1000 | Loss: 0.00003092
Iteration 77/1000 | Loss: 0.00003092
Iteration 78/1000 | Loss: 0.00003092
Iteration 79/1000 | Loss: 0.00003092
Iteration 80/1000 | Loss: 0.00003092
Iteration 81/1000 | Loss: 0.00003092
Iteration 82/1000 | Loss: 0.00003092
Iteration 83/1000 | Loss: 0.00003091
Iteration 84/1000 | Loss: 0.00003091
Iteration 85/1000 | Loss: 0.00003091
Iteration 86/1000 | Loss: 0.00003091
Iteration 87/1000 | Loss: 0.00003091
Iteration 88/1000 | Loss: 0.00003091
Iteration 89/1000 | Loss: 0.00003091
Iteration 90/1000 | Loss: 0.00003091
Iteration 91/1000 | Loss: 0.00003091
Iteration 92/1000 | Loss: 0.00003091
Iteration 93/1000 | Loss: 0.00003091
Iteration 94/1000 | Loss: 0.00003091
Iteration 95/1000 | Loss: 0.00003091
Iteration 96/1000 | Loss: 0.00003091
Iteration 97/1000 | Loss: 0.00003091
Iteration 98/1000 | Loss: 0.00003091
Iteration 99/1000 | Loss: 0.00003091
Iteration 100/1000 | Loss: 0.00003091
Iteration 101/1000 | Loss: 0.00003091
Iteration 102/1000 | Loss: 0.00003091
Iteration 103/1000 | Loss: 0.00003091
Iteration 104/1000 | Loss: 0.00003091
Iteration 105/1000 | Loss: 0.00003091
Iteration 106/1000 | Loss: 0.00003091
Iteration 107/1000 | Loss: 0.00003091
Iteration 108/1000 | Loss: 0.00003091
Iteration 109/1000 | Loss: 0.00003091
Iteration 110/1000 | Loss: 0.00003091
Iteration 111/1000 | Loss: 0.00003091
Iteration 112/1000 | Loss: 0.00003091
Iteration 113/1000 | Loss: 0.00003091
Iteration 114/1000 | Loss: 0.00003091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [3.0910330679034814e-05, 3.0910330679034814e-05, 3.0910330679034814e-05, 3.0910330679034814e-05, 3.0910330679034814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0910330679034814e-05

Optimization complete. Final v2v error: 4.653868675231934 mm

Highest mean error: 5.107502460479736 mm for frame 73

Lowest mean error: 3.965005397796631 mm for frame 238

Saving results

Total time: 87.51883387565613
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835337
Iteration 2/25 | Loss: 0.00144008
Iteration 3/25 | Loss: 0.00101255
Iteration 4/25 | Loss: 0.00095633
Iteration 5/25 | Loss: 0.00094417
Iteration 6/25 | Loss: 0.00094230
Iteration 7/25 | Loss: 0.00094218
Iteration 8/25 | Loss: 0.00094218
Iteration 9/25 | Loss: 0.00094218
Iteration 10/25 | Loss: 0.00094218
Iteration 11/25 | Loss: 0.00094218
Iteration 12/25 | Loss: 0.00094218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009421833092346787, 0.0009421833092346787, 0.0009421833092346787, 0.0009421833092346787, 0.0009421833092346787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009421833092346787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34815431
Iteration 2/25 | Loss: 0.00041400
Iteration 3/25 | Loss: 0.00041399
Iteration 4/25 | Loss: 0.00041399
Iteration 5/25 | Loss: 0.00041399
Iteration 6/25 | Loss: 0.00041399
Iteration 7/25 | Loss: 0.00041399
Iteration 8/25 | Loss: 0.00041399
Iteration 9/25 | Loss: 0.00041399
Iteration 10/25 | Loss: 0.00041399
Iteration 11/25 | Loss: 0.00041399
Iteration 12/25 | Loss: 0.00041399
Iteration 13/25 | Loss: 0.00041399
Iteration 14/25 | Loss: 0.00041399
Iteration 15/25 | Loss: 0.00041399
Iteration 16/25 | Loss: 0.00041399
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00041398577741347253, 0.00041398577741347253, 0.00041398577741347253, 0.00041398577741347253, 0.00041398577741347253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00041398577741347253

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041399
Iteration 2/1000 | Loss: 0.00003117
Iteration 3/1000 | Loss: 0.00002133
Iteration 4/1000 | Loss: 0.00001982
Iteration 5/1000 | Loss: 0.00001923
Iteration 6/1000 | Loss: 0.00001877
Iteration 7/1000 | Loss: 0.00001847
Iteration 8/1000 | Loss: 0.00001818
Iteration 9/1000 | Loss: 0.00001816
Iteration 10/1000 | Loss: 0.00001798
Iteration 11/1000 | Loss: 0.00001791
Iteration 12/1000 | Loss: 0.00001788
Iteration 13/1000 | Loss: 0.00001787
Iteration 14/1000 | Loss: 0.00001787
Iteration 15/1000 | Loss: 0.00001786
Iteration 16/1000 | Loss: 0.00001786
Iteration 17/1000 | Loss: 0.00001785
Iteration 18/1000 | Loss: 0.00001785
Iteration 19/1000 | Loss: 0.00001779
Iteration 20/1000 | Loss: 0.00001779
Iteration 21/1000 | Loss: 0.00001778
Iteration 22/1000 | Loss: 0.00001775
Iteration 23/1000 | Loss: 0.00001775
Iteration 24/1000 | Loss: 0.00001775
Iteration 25/1000 | Loss: 0.00001775
Iteration 26/1000 | Loss: 0.00001774
Iteration 27/1000 | Loss: 0.00001774
Iteration 28/1000 | Loss: 0.00001774
Iteration 29/1000 | Loss: 0.00001774
Iteration 30/1000 | Loss: 0.00001774
Iteration 31/1000 | Loss: 0.00001774
Iteration 32/1000 | Loss: 0.00001774
Iteration 33/1000 | Loss: 0.00001771
Iteration 34/1000 | Loss: 0.00001771
Iteration 35/1000 | Loss: 0.00001770
Iteration 36/1000 | Loss: 0.00001770
Iteration 37/1000 | Loss: 0.00001770
Iteration 38/1000 | Loss: 0.00001770
Iteration 39/1000 | Loss: 0.00001770
Iteration 40/1000 | Loss: 0.00001769
Iteration 41/1000 | Loss: 0.00001769
Iteration 42/1000 | Loss: 0.00001769
Iteration 43/1000 | Loss: 0.00001769
Iteration 44/1000 | Loss: 0.00001769
Iteration 45/1000 | Loss: 0.00001769
Iteration 46/1000 | Loss: 0.00001769
Iteration 47/1000 | Loss: 0.00001769
Iteration 48/1000 | Loss: 0.00001768
Iteration 49/1000 | Loss: 0.00001768
Iteration 50/1000 | Loss: 0.00001768
Iteration 51/1000 | Loss: 0.00001768
Iteration 52/1000 | Loss: 0.00001768
Iteration 53/1000 | Loss: 0.00001768
Iteration 54/1000 | Loss: 0.00001768
Iteration 55/1000 | Loss: 0.00001768
Iteration 56/1000 | Loss: 0.00001767
Iteration 57/1000 | Loss: 0.00001767
Iteration 58/1000 | Loss: 0.00001767
Iteration 59/1000 | Loss: 0.00001767
Iteration 60/1000 | Loss: 0.00001767
Iteration 61/1000 | Loss: 0.00001766
Iteration 62/1000 | Loss: 0.00001766
Iteration 63/1000 | Loss: 0.00001766
Iteration 64/1000 | Loss: 0.00001766
Iteration 65/1000 | Loss: 0.00001766
Iteration 66/1000 | Loss: 0.00001766
Iteration 67/1000 | Loss: 0.00001766
Iteration 68/1000 | Loss: 0.00001766
Iteration 69/1000 | Loss: 0.00001765
Iteration 70/1000 | Loss: 0.00001765
Iteration 71/1000 | Loss: 0.00001765
Iteration 72/1000 | Loss: 0.00001765
Iteration 73/1000 | Loss: 0.00001765
Iteration 74/1000 | Loss: 0.00001765
Iteration 75/1000 | Loss: 0.00001765
Iteration 76/1000 | Loss: 0.00001765
Iteration 77/1000 | Loss: 0.00001764
Iteration 78/1000 | Loss: 0.00001764
Iteration 79/1000 | Loss: 0.00001764
Iteration 80/1000 | Loss: 0.00001764
Iteration 81/1000 | Loss: 0.00001764
Iteration 82/1000 | Loss: 0.00001764
Iteration 83/1000 | Loss: 0.00001764
Iteration 84/1000 | Loss: 0.00001764
Iteration 85/1000 | Loss: 0.00001764
Iteration 86/1000 | Loss: 0.00001763
Iteration 87/1000 | Loss: 0.00001763
Iteration 88/1000 | Loss: 0.00001763
Iteration 89/1000 | Loss: 0.00001763
Iteration 90/1000 | Loss: 0.00001763
Iteration 91/1000 | Loss: 0.00001763
Iteration 92/1000 | Loss: 0.00001763
Iteration 93/1000 | Loss: 0.00001763
Iteration 94/1000 | Loss: 0.00001763
Iteration 95/1000 | Loss: 0.00001763
Iteration 96/1000 | Loss: 0.00001763
Iteration 97/1000 | Loss: 0.00001763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.76307021320099e-05, 1.76307021320099e-05, 1.76307021320099e-05, 1.76307021320099e-05, 1.76307021320099e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.76307021320099e-05

Optimization complete. Final v2v error: 3.7502729892730713 mm

Highest mean error: 3.9344658851623535 mm for frame 34

Lowest mean error: 3.5212130546569824 mm for frame 104

Saving results

Total time: 33.550981283187866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050420
Iteration 2/25 | Loss: 0.00393864
Iteration 3/25 | Loss: 0.00202480
Iteration 4/25 | Loss: 0.00178263
Iteration 5/25 | Loss: 0.00163692
Iteration 6/25 | Loss: 0.00161354
Iteration 7/25 | Loss: 0.00139468
Iteration 8/25 | Loss: 0.00140482
Iteration 9/25 | Loss: 0.00122414
Iteration 10/25 | Loss: 0.00118214
Iteration 11/25 | Loss: 0.00115213
Iteration 12/25 | Loss: 0.00114907
Iteration 13/25 | Loss: 0.00115719
Iteration 14/25 | Loss: 0.00113912
Iteration 15/25 | Loss: 0.00111343
Iteration 16/25 | Loss: 0.00109064
Iteration 17/25 | Loss: 0.00107041
Iteration 18/25 | Loss: 0.00105799
Iteration 19/25 | Loss: 0.00106301
Iteration 20/25 | Loss: 0.00104995
Iteration 21/25 | Loss: 0.00105480
Iteration 22/25 | Loss: 0.00104356
Iteration 23/25 | Loss: 0.00104605
Iteration 24/25 | Loss: 0.00104228
Iteration 25/25 | Loss: 0.00104917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35886407
Iteration 2/25 | Loss: 0.00345423
Iteration 3/25 | Loss: 0.00203665
Iteration 4/25 | Loss: 0.00203665
Iteration 5/25 | Loss: 0.00203665
Iteration 6/25 | Loss: 0.00203665
Iteration 7/25 | Loss: 0.00203665
Iteration 8/25 | Loss: 0.00203665
Iteration 9/25 | Loss: 0.00203665
Iteration 10/25 | Loss: 0.00203665
Iteration 11/25 | Loss: 0.00203665
Iteration 12/25 | Loss: 0.00203665
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.002036648103967309, 0.002036648103967309, 0.002036648103967309, 0.002036648103967309, 0.002036648103967309]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002036648103967309

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00203665
Iteration 2/1000 | Loss: 0.00401486
Iteration 3/1000 | Loss: 0.00500809
Iteration 4/1000 | Loss: 0.00113973
Iteration 5/1000 | Loss: 0.00051453
Iteration 6/1000 | Loss: 0.00070476
Iteration 7/1000 | Loss: 0.00140997
Iteration 8/1000 | Loss: 0.00044740
Iteration 9/1000 | Loss: 0.00147002
Iteration 10/1000 | Loss: 0.00045661
Iteration 11/1000 | Loss: 0.00120168
Iteration 12/1000 | Loss: 0.00099738
Iteration 13/1000 | Loss: 0.00086884
Iteration 14/1000 | Loss: 0.00029368
Iteration 15/1000 | Loss: 0.00013631
Iteration 16/1000 | Loss: 0.00204559
Iteration 17/1000 | Loss: 0.00135352
Iteration 18/1000 | Loss: 0.00084541
Iteration 19/1000 | Loss: 0.00039123
Iteration 20/1000 | Loss: 0.00061504
Iteration 21/1000 | Loss: 0.00044428
Iteration 22/1000 | Loss: 0.00067597
Iteration 23/1000 | Loss: 0.00023119
Iteration 24/1000 | Loss: 0.00094715
Iteration 25/1000 | Loss: 0.00034607
Iteration 26/1000 | Loss: 0.00049573
Iteration 27/1000 | Loss: 0.00018174
Iteration 28/1000 | Loss: 0.00010701
Iteration 29/1000 | Loss: 0.00052232
Iteration 30/1000 | Loss: 0.00007092
Iteration 31/1000 | Loss: 0.00056960
Iteration 32/1000 | Loss: 0.00100343
Iteration 33/1000 | Loss: 0.00072274
Iteration 34/1000 | Loss: 0.00013348
Iteration 35/1000 | Loss: 0.00052467
Iteration 36/1000 | Loss: 0.00025894
Iteration 37/1000 | Loss: 0.00008323
Iteration 38/1000 | Loss: 0.00007467
Iteration 39/1000 | Loss: 0.00009413
Iteration 40/1000 | Loss: 0.00005031
Iteration 41/1000 | Loss: 0.00053684
Iteration 42/1000 | Loss: 0.00021327
Iteration 43/1000 | Loss: 0.00005830
Iteration 44/1000 | Loss: 0.00014689
Iteration 45/1000 | Loss: 0.00031936
Iteration 46/1000 | Loss: 0.00024955
Iteration 47/1000 | Loss: 0.00016601
Iteration 48/1000 | Loss: 0.00004268
Iteration 49/1000 | Loss: 0.00003841
Iteration 50/1000 | Loss: 0.00005072
Iteration 51/1000 | Loss: 0.00003644
Iteration 52/1000 | Loss: 0.00003735
Iteration 53/1000 | Loss: 0.00004295
Iteration 54/1000 | Loss: 0.00014738
Iteration 55/1000 | Loss: 0.00003584
Iteration 56/1000 | Loss: 0.00060245
Iteration 57/1000 | Loss: 0.00007191
Iteration 58/1000 | Loss: 0.00032478
Iteration 59/1000 | Loss: 0.00024753
Iteration 60/1000 | Loss: 0.00036891
Iteration 61/1000 | Loss: 0.00005325
Iteration 62/1000 | Loss: 0.00006644
Iteration 63/1000 | Loss: 0.00077091
Iteration 64/1000 | Loss: 0.00020270
Iteration 65/1000 | Loss: 0.00025804
Iteration 66/1000 | Loss: 0.00004582
Iteration 67/1000 | Loss: 0.00003772
Iteration 68/1000 | Loss: 0.00005511
Iteration 69/1000 | Loss: 0.00038783
Iteration 70/1000 | Loss: 0.00006652
Iteration 71/1000 | Loss: 0.00003950
Iteration 72/1000 | Loss: 0.00003148
Iteration 73/1000 | Loss: 0.00002878
Iteration 74/1000 | Loss: 0.00004901
Iteration 75/1000 | Loss: 0.00003502
Iteration 76/1000 | Loss: 0.00002632
Iteration 77/1000 | Loss: 0.00002592
Iteration 78/1000 | Loss: 0.00002562
Iteration 79/1000 | Loss: 0.00002554
Iteration 80/1000 | Loss: 0.00004601
Iteration 81/1000 | Loss: 0.00002527
Iteration 82/1000 | Loss: 0.00002523
Iteration 83/1000 | Loss: 0.00002516
Iteration 84/1000 | Loss: 0.00002513
Iteration 85/1000 | Loss: 0.00002513
Iteration 86/1000 | Loss: 0.00002513
Iteration 87/1000 | Loss: 0.00002512
Iteration 88/1000 | Loss: 0.00002511
Iteration 89/1000 | Loss: 0.00002511
Iteration 90/1000 | Loss: 0.00002510
Iteration 91/1000 | Loss: 0.00002509
Iteration 92/1000 | Loss: 0.00002509
Iteration 93/1000 | Loss: 0.00002508
Iteration 94/1000 | Loss: 0.00002508
Iteration 95/1000 | Loss: 0.00002508
Iteration 96/1000 | Loss: 0.00002508
Iteration 97/1000 | Loss: 0.00002508
Iteration 98/1000 | Loss: 0.00002507
Iteration 99/1000 | Loss: 0.00002507
Iteration 100/1000 | Loss: 0.00002505
Iteration 101/1000 | Loss: 0.00002505
Iteration 102/1000 | Loss: 0.00002505
Iteration 103/1000 | Loss: 0.00002505
Iteration 104/1000 | Loss: 0.00002505
Iteration 105/1000 | Loss: 0.00002505
Iteration 106/1000 | Loss: 0.00002505
Iteration 107/1000 | Loss: 0.00002505
Iteration 108/1000 | Loss: 0.00002505
Iteration 109/1000 | Loss: 0.00002505
Iteration 110/1000 | Loss: 0.00002505
Iteration 111/1000 | Loss: 0.00002499
Iteration 112/1000 | Loss: 0.00002499
Iteration 113/1000 | Loss: 0.00002498
Iteration 114/1000 | Loss: 0.00002497
Iteration 115/1000 | Loss: 0.00002497
Iteration 116/1000 | Loss: 0.00002497
Iteration 117/1000 | Loss: 0.00002497
Iteration 118/1000 | Loss: 0.00002497
Iteration 119/1000 | Loss: 0.00002497
Iteration 120/1000 | Loss: 0.00002497
Iteration 121/1000 | Loss: 0.00002497
Iteration 122/1000 | Loss: 0.00002497
Iteration 123/1000 | Loss: 0.00002497
Iteration 124/1000 | Loss: 0.00002497
Iteration 125/1000 | Loss: 0.00002496
Iteration 126/1000 | Loss: 0.00002496
Iteration 127/1000 | Loss: 0.00002495
Iteration 128/1000 | Loss: 0.00002495
Iteration 129/1000 | Loss: 0.00002495
Iteration 130/1000 | Loss: 0.00002494
Iteration 131/1000 | Loss: 0.00002494
Iteration 132/1000 | Loss: 0.00002493
Iteration 133/1000 | Loss: 0.00002493
Iteration 134/1000 | Loss: 0.00002493
Iteration 135/1000 | Loss: 0.00002492
Iteration 136/1000 | Loss: 0.00002492
Iteration 137/1000 | Loss: 0.00002491
Iteration 138/1000 | Loss: 0.00002491
Iteration 139/1000 | Loss: 0.00002491
Iteration 140/1000 | Loss: 0.00002491
Iteration 141/1000 | Loss: 0.00002490
Iteration 142/1000 | Loss: 0.00002490
Iteration 143/1000 | Loss: 0.00002490
Iteration 144/1000 | Loss: 0.00002490
Iteration 145/1000 | Loss: 0.00002489
Iteration 146/1000 | Loss: 0.00002489
Iteration 147/1000 | Loss: 0.00002489
Iteration 148/1000 | Loss: 0.00002489
Iteration 149/1000 | Loss: 0.00002488
Iteration 150/1000 | Loss: 0.00002488
Iteration 151/1000 | Loss: 0.00002488
Iteration 152/1000 | Loss: 0.00002487
Iteration 153/1000 | Loss: 0.00002487
Iteration 154/1000 | Loss: 0.00002487
Iteration 155/1000 | Loss: 0.00002487
Iteration 156/1000 | Loss: 0.00002486
Iteration 157/1000 | Loss: 0.00002486
Iteration 158/1000 | Loss: 0.00002486
Iteration 159/1000 | Loss: 0.00002486
Iteration 160/1000 | Loss: 0.00002486
Iteration 161/1000 | Loss: 0.00002486
Iteration 162/1000 | Loss: 0.00002486
Iteration 163/1000 | Loss: 0.00002486
Iteration 164/1000 | Loss: 0.00002486
Iteration 165/1000 | Loss: 0.00002486
Iteration 166/1000 | Loss: 0.00002486
Iteration 167/1000 | Loss: 0.00002486
Iteration 168/1000 | Loss: 0.00002485
Iteration 169/1000 | Loss: 0.00002485
Iteration 170/1000 | Loss: 0.00002485
Iteration 171/1000 | Loss: 0.00002485
Iteration 172/1000 | Loss: 0.00002484
Iteration 173/1000 | Loss: 0.00002484
Iteration 174/1000 | Loss: 0.00002484
Iteration 175/1000 | Loss: 0.00002484
Iteration 176/1000 | Loss: 0.00002484
Iteration 177/1000 | Loss: 0.00002484
Iteration 178/1000 | Loss: 0.00002484
Iteration 179/1000 | Loss: 0.00002484
Iteration 180/1000 | Loss: 0.00002484
Iteration 181/1000 | Loss: 0.00002484
Iteration 182/1000 | Loss: 0.00003923
Iteration 183/1000 | Loss: 0.00002658
Iteration 184/1000 | Loss: 0.00002484
Iteration 185/1000 | Loss: 0.00002484
Iteration 186/1000 | Loss: 0.00002484
Iteration 187/1000 | Loss: 0.00002484
Iteration 188/1000 | Loss: 0.00002484
Iteration 189/1000 | Loss: 0.00002483
Iteration 190/1000 | Loss: 0.00002483
Iteration 191/1000 | Loss: 0.00002483
Iteration 192/1000 | Loss: 0.00002483
Iteration 193/1000 | Loss: 0.00002483
Iteration 194/1000 | Loss: 0.00002483
Iteration 195/1000 | Loss: 0.00002483
Iteration 196/1000 | Loss: 0.00002483
Iteration 197/1000 | Loss: 0.00002483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.483235402905848e-05, 2.483235402905848e-05, 2.483235402905848e-05, 2.483235402905848e-05, 2.483235402905848e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.483235402905848e-05

Optimization complete. Final v2v error: 4.094302654266357 mm

Highest mean error: 9.941513061523438 mm for frame 32

Lowest mean error: 3.4860012531280518 mm for frame 9

Saving results

Total time: 195.79157757759094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491491
Iteration 2/25 | Loss: 0.00129060
Iteration 3/25 | Loss: 0.00098640
Iteration 4/25 | Loss: 0.00092604
Iteration 5/25 | Loss: 0.00091292
Iteration 6/25 | Loss: 0.00090927
Iteration 7/25 | Loss: 0.00090846
Iteration 8/25 | Loss: 0.00090841
Iteration 9/25 | Loss: 0.00090841
Iteration 10/25 | Loss: 0.00090841
Iteration 11/25 | Loss: 0.00090841
Iteration 12/25 | Loss: 0.00090841
Iteration 13/25 | Loss: 0.00090841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009084127959795296, 0.0009084127959795296, 0.0009084127959795296, 0.0009084127959795296, 0.0009084127959795296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009084127959795296

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35995162
Iteration 2/25 | Loss: 0.00040052
Iteration 3/25 | Loss: 0.00040052
Iteration 4/25 | Loss: 0.00040052
Iteration 5/25 | Loss: 0.00040052
Iteration 6/25 | Loss: 0.00040052
Iteration 7/25 | Loss: 0.00040052
Iteration 8/25 | Loss: 0.00040052
Iteration 9/25 | Loss: 0.00040052
Iteration 10/25 | Loss: 0.00040052
Iteration 11/25 | Loss: 0.00040051
Iteration 12/25 | Loss: 0.00040051
Iteration 13/25 | Loss: 0.00040051
Iteration 14/25 | Loss: 0.00040051
Iteration 15/25 | Loss: 0.00040051
Iteration 16/25 | Loss: 0.00040052
Iteration 17/25 | Loss: 0.00040052
Iteration 18/25 | Loss: 0.00040052
Iteration 19/25 | Loss: 0.00040051
Iteration 20/25 | Loss: 0.00040051
Iteration 21/25 | Loss: 0.00040052
Iteration 22/25 | Loss: 0.00040052
Iteration 23/25 | Loss: 0.00040052
Iteration 24/25 | Loss: 0.00040052
Iteration 25/25 | Loss: 0.00040052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040052
Iteration 2/1000 | Loss: 0.00003578
Iteration 3/1000 | Loss: 0.00002373
Iteration 4/1000 | Loss: 0.00002154
Iteration 5/1000 | Loss: 0.00002079
Iteration 6/1000 | Loss: 0.00002031
Iteration 7/1000 | Loss: 0.00001986
Iteration 8/1000 | Loss: 0.00001955
Iteration 9/1000 | Loss: 0.00001935
Iteration 10/1000 | Loss: 0.00001921
Iteration 11/1000 | Loss: 0.00001909
Iteration 12/1000 | Loss: 0.00001903
Iteration 13/1000 | Loss: 0.00001894
Iteration 14/1000 | Loss: 0.00001890
Iteration 15/1000 | Loss: 0.00001889
Iteration 16/1000 | Loss: 0.00001888
Iteration 17/1000 | Loss: 0.00001887
Iteration 18/1000 | Loss: 0.00001887
Iteration 19/1000 | Loss: 0.00001886
Iteration 20/1000 | Loss: 0.00001885
Iteration 21/1000 | Loss: 0.00001885
Iteration 22/1000 | Loss: 0.00001885
Iteration 23/1000 | Loss: 0.00001884
Iteration 24/1000 | Loss: 0.00001882
Iteration 25/1000 | Loss: 0.00001882
Iteration 26/1000 | Loss: 0.00001882
Iteration 27/1000 | Loss: 0.00001882
Iteration 28/1000 | Loss: 0.00001882
Iteration 29/1000 | Loss: 0.00001881
Iteration 30/1000 | Loss: 0.00001881
Iteration 31/1000 | Loss: 0.00001881
Iteration 32/1000 | Loss: 0.00001881
Iteration 33/1000 | Loss: 0.00001881
Iteration 34/1000 | Loss: 0.00001881
Iteration 35/1000 | Loss: 0.00001881
Iteration 36/1000 | Loss: 0.00001881
Iteration 37/1000 | Loss: 0.00001881
Iteration 38/1000 | Loss: 0.00001881
Iteration 39/1000 | Loss: 0.00001881
Iteration 40/1000 | Loss: 0.00001880
Iteration 41/1000 | Loss: 0.00001880
Iteration 42/1000 | Loss: 0.00001878
Iteration 43/1000 | Loss: 0.00001878
Iteration 44/1000 | Loss: 0.00001877
Iteration 45/1000 | Loss: 0.00001877
Iteration 46/1000 | Loss: 0.00001877
Iteration 47/1000 | Loss: 0.00001877
Iteration 48/1000 | Loss: 0.00001877
Iteration 49/1000 | Loss: 0.00001876
Iteration 50/1000 | Loss: 0.00001876
Iteration 51/1000 | Loss: 0.00001876
Iteration 52/1000 | Loss: 0.00001876
Iteration 53/1000 | Loss: 0.00001876
Iteration 54/1000 | Loss: 0.00001876
Iteration 55/1000 | Loss: 0.00001875
Iteration 56/1000 | Loss: 0.00001875
Iteration 57/1000 | Loss: 0.00001875
Iteration 58/1000 | Loss: 0.00001875
Iteration 59/1000 | Loss: 0.00001875
Iteration 60/1000 | Loss: 0.00001875
Iteration 61/1000 | Loss: 0.00001874
Iteration 62/1000 | Loss: 0.00001874
Iteration 63/1000 | Loss: 0.00001874
Iteration 64/1000 | Loss: 0.00001874
Iteration 65/1000 | Loss: 0.00001874
Iteration 66/1000 | Loss: 0.00001874
Iteration 67/1000 | Loss: 0.00001873
Iteration 68/1000 | Loss: 0.00001873
Iteration 69/1000 | Loss: 0.00001873
Iteration 70/1000 | Loss: 0.00001872
Iteration 71/1000 | Loss: 0.00001872
Iteration 72/1000 | Loss: 0.00001872
Iteration 73/1000 | Loss: 0.00001872
Iteration 74/1000 | Loss: 0.00001872
Iteration 75/1000 | Loss: 0.00001872
Iteration 76/1000 | Loss: 0.00001872
Iteration 77/1000 | Loss: 0.00001872
Iteration 78/1000 | Loss: 0.00001872
Iteration 79/1000 | Loss: 0.00001872
Iteration 80/1000 | Loss: 0.00001872
Iteration 81/1000 | Loss: 0.00001872
Iteration 82/1000 | Loss: 0.00001872
Iteration 83/1000 | Loss: 0.00001872
Iteration 84/1000 | Loss: 0.00001872
Iteration 85/1000 | Loss: 0.00001871
Iteration 86/1000 | Loss: 0.00001871
Iteration 87/1000 | Loss: 0.00001871
Iteration 88/1000 | Loss: 0.00001871
Iteration 89/1000 | Loss: 0.00001871
Iteration 90/1000 | Loss: 0.00001871
Iteration 91/1000 | Loss: 0.00001871
Iteration 92/1000 | Loss: 0.00001871
Iteration 93/1000 | Loss: 0.00001870
Iteration 94/1000 | Loss: 0.00001870
Iteration 95/1000 | Loss: 0.00001870
Iteration 96/1000 | Loss: 0.00001870
Iteration 97/1000 | Loss: 0.00001870
Iteration 98/1000 | Loss: 0.00001870
Iteration 99/1000 | Loss: 0.00001870
Iteration 100/1000 | Loss: 0.00001870
Iteration 101/1000 | Loss: 0.00001870
Iteration 102/1000 | Loss: 0.00001870
Iteration 103/1000 | Loss: 0.00001870
Iteration 104/1000 | Loss: 0.00001870
Iteration 105/1000 | Loss: 0.00001870
Iteration 106/1000 | Loss: 0.00001870
Iteration 107/1000 | Loss: 0.00001870
Iteration 108/1000 | Loss: 0.00001870
Iteration 109/1000 | Loss: 0.00001870
Iteration 110/1000 | Loss: 0.00001870
Iteration 111/1000 | Loss: 0.00001870
Iteration 112/1000 | Loss: 0.00001870
Iteration 113/1000 | Loss: 0.00001870
Iteration 114/1000 | Loss: 0.00001869
Iteration 115/1000 | Loss: 0.00001869
Iteration 116/1000 | Loss: 0.00001869
Iteration 117/1000 | Loss: 0.00001869
Iteration 118/1000 | Loss: 0.00001869
Iteration 119/1000 | Loss: 0.00001869
Iteration 120/1000 | Loss: 0.00001869
Iteration 121/1000 | Loss: 0.00001869
Iteration 122/1000 | Loss: 0.00001869
Iteration 123/1000 | Loss: 0.00001869
Iteration 124/1000 | Loss: 0.00001868
Iteration 125/1000 | Loss: 0.00001868
Iteration 126/1000 | Loss: 0.00001868
Iteration 127/1000 | Loss: 0.00001868
Iteration 128/1000 | Loss: 0.00001868
Iteration 129/1000 | Loss: 0.00001868
Iteration 130/1000 | Loss: 0.00001868
Iteration 131/1000 | Loss: 0.00001868
Iteration 132/1000 | Loss: 0.00001868
Iteration 133/1000 | Loss: 0.00001868
Iteration 134/1000 | Loss: 0.00001868
Iteration 135/1000 | Loss: 0.00001868
Iteration 136/1000 | Loss: 0.00001867
Iteration 137/1000 | Loss: 0.00001867
Iteration 138/1000 | Loss: 0.00001867
Iteration 139/1000 | Loss: 0.00001867
Iteration 140/1000 | Loss: 0.00001867
Iteration 141/1000 | Loss: 0.00001867
Iteration 142/1000 | Loss: 0.00001867
Iteration 143/1000 | Loss: 0.00001867
Iteration 144/1000 | Loss: 0.00001867
Iteration 145/1000 | Loss: 0.00001867
Iteration 146/1000 | Loss: 0.00001867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.866927050286904e-05, 1.866927050286904e-05, 1.866927050286904e-05, 1.866927050286904e-05, 1.866927050286904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.866927050286904e-05

Optimization complete. Final v2v error: 3.760521650314331 mm

Highest mean error: 4.418788433074951 mm for frame 75

Lowest mean error: 3.151319742202759 mm for frame 1

Saving results

Total time: 36.76103186607361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882318
Iteration 2/25 | Loss: 0.00173122
Iteration 3/25 | Loss: 0.00107283
Iteration 4/25 | Loss: 0.00102303
Iteration 5/25 | Loss: 0.00098682
Iteration 6/25 | Loss: 0.00094157
Iteration 7/25 | Loss: 0.00092990
Iteration 8/25 | Loss: 0.00089176
Iteration 9/25 | Loss: 0.00089118
Iteration 10/25 | Loss: 0.00085724
Iteration 11/25 | Loss: 0.00083974
Iteration 12/25 | Loss: 0.00082764
Iteration 13/25 | Loss: 0.00082004
Iteration 14/25 | Loss: 0.00082059
Iteration 15/25 | Loss: 0.00081542
Iteration 16/25 | Loss: 0.00081023
Iteration 17/25 | Loss: 0.00081218
Iteration 18/25 | Loss: 0.00080868
Iteration 19/25 | Loss: 0.00080800
Iteration 20/25 | Loss: 0.00080611
Iteration 21/25 | Loss: 0.00080751
Iteration 22/25 | Loss: 0.00080556
Iteration 23/25 | Loss: 0.00080490
Iteration 24/25 | Loss: 0.00080509
Iteration 25/25 | Loss: 0.00080497

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.26240635
Iteration 2/25 | Loss: 0.00037569
Iteration 3/25 | Loss: 0.00033558
Iteration 4/25 | Loss: 0.00033558
Iteration 5/25 | Loss: 0.00033558
Iteration 6/25 | Loss: 0.00033558
Iteration 7/25 | Loss: 0.00033558
Iteration 8/25 | Loss: 0.00033558
Iteration 9/25 | Loss: 0.00033558
Iteration 10/25 | Loss: 0.00033558
Iteration 11/25 | Loss: 0.00033558
Iteration 12/25 | Loss: 0.00033558
Iteration 13/25 | Loss: 0.00033558
Iteration 14/25 | Loss: 0.00033558
Iteration 15/25 | Loss: 0.00033558
Iteration 16/25 | Loss: 0.00033558
Iteration 17/25 | Loss: 0.00033558
Iteration 18/25 | Loss: 0.00033558
Iteration 19/25 | Loss: 0.00033558
Iteration 20/25 | Loss: 0.00033558
Iteration 21/25 | Loss: 0.00033558
Iteration 22/25 | Loss: 0.00033558
Iteration 23/25 | Loss: 0.00033558
Iteration 24/25 | Loss: 0.00033558
Iteration 25/25 | Loss: 0.00033558
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00033558058203198016, 0.00033558058203198016, 0.00033558058203198016, 0.00033558058203198016, 0.00033558058203198016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033558058203198016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033558
Iteration 2/1000 | Loss: 0.00005171
Iteration 3/1000 | Loss: 0.00006398
Iteration 4/1000 | Loss: 0.00003439
Iteration 5/1000 | Loss: 0.00007542
Iteration 6/1000 | Loss: 0.00004072
Iteration 7/1000 | Loss: 0.00003057
Iteration 8/1000 | Loss: 0.00002659
Iteration 9/1000 | Loss: 0.00002501
Iteration 10/1000 | Loss: 0.00002416
Iteration 11/1000 | Loss: 0.00002369
Iteration 12/1000 | Loss: 0.00002329
Iteration 13/1000 | Loss: 0.00002299
Iteration 14/1000 | Loss: 0.00002279
Iteration 15/1000 | Loss: 0.00002274
Iteration 16/1000 | Loss: 0.00002266
Iteration 17/1000 | Loss: 0.00002265
Iteration 18/1000 | Loss: 0.00002264
Iteration 19/1000 | Loss: 0.00002257
Iteration 20/1000 | Loss: 0.00002248
Iteration 21/1000 | Loss: 0.00002241
Iteration 22/1000 | Loss: 0.00002241
Iteration 23/1000 | Loss: 0.00002240
Iteration 24/1000 | Loss: 0.00002240
Iteration 25/1000 | Loss: 0.00002239
Iteration 26/1000 | Loss: 0.00002238
Iteration 27/1000 | Loss: 0.00002238
Iteration 28/1000 | Loss: 0.00002238
Iteration 29/1000 | Loss: 0.00002236
Iteration 30/1000 | Loss: 0.00002235
Iteration 31/1000 | Loss: 0.00002235
Iteration 32/1000 | Loss: 0.00002235
Iteration 33/1000 | Loss: 0.00002234
Iteration 34/1000 | Loss: 0.00002234
Iteration 35/1000 | Loss: 0.00002234
Iteration 36/1000 | Loss: 0.00002234
Iteration 37/1000 | Loss: 0.00002234
Iteration 38/1000 | Loss: 0.00002233
Iteration 39/1000 | Loss: 0.00002233
Iteration 40/1000 | Loss: 0.00002233
Iteration 41/1000 | Loss: 0.00002233
Iteration 42/1000 | Loss: 0.00002233
Iteration 43/1000 | Loss: 0.00002232
Iteration 44/1000 | Loss: 0.00002232
Iteration 45/1000 | Loss: 0.00002232
Iteration 46/1000 | Loss: 0.00002232
Iteration 47/1000 | Loss: 0.00002232
Iteration 48/1000 | Loss: 0.00002232
Iteration 49/1000 | Loss: 0.00002232
Iteration 50/1000 | Loss: 0.00002231
Iteration 51/1000 | Loss: 0.00002231
Iteration 52/1000 | Loss: 0.00002231
Iteration 53/1000 | Loss: 0.00002231
Iteration 54/1000 | Loss: 0.00002231
Iteration 55/1000 | Loss: 0.00002231
Iteration 56/1000 | Loss: 0.00002231
Iteration 57/1000 | Loss: 0.00002230
Iteration 58/1000 | Loss: 0.00002230
Iteration 59/1000 | Loss: 0.00002230
Iteration 60/1000 | Loss: 0.00002230
Iteration 61/1000 | Loss: 0.00002230
Iteration 62/1000 | Loss: 0.00002229
Iteration 63/1000 | Loss: 0.00002229
Iteration 64/1000 | Loss: 0.00002229
Iteration 65/1000 | Loss: 0.00002229
Iteration 66/1000 | Loss: 0.00002229
Iteration 67/1000 | Loss: 0.00002229
Iteration 68/1000 | Loss: 0.00002229
Iteration 69/1000 | Loss: 0.00002229
Iteration 70/1000 | Loss: 0.00002228
Iteration 71/1000 | Loss: 0.00002228
Iteration 72/1000 | Loss: 0.00002228
Iteration 73/1000 | Loss: 0.00002228
Iteration 74/1000 | Loss: 0.00002228
Iteration 75/1000 | Loss: 0.00002228
Iteration 76/1000 | Loss: 0.00002228
Iteration 77/1000 | Loss: 0.00002228
Iteration 78/1000 | Loss: 0.00002228
Iteration 79/1000 | Loss: 0.00002228
Iteration 80/1000 | Loss: 0.00002228
Iteration 81/1000 | Loss: 0.00002227
Iteration 82/1000 | Loss: 0.00002227
Iteration 83/1000 | Loss: 0.00002227
Iteration 84/1000 | Loss: 0.00002227
Iteration 85/1000 | Loss: 0.00002227
Iteration 86/1000 | Loss: 0.00002227
Iteration 87/1000 | Loss: 0.00002227
Iteration 88/1000 | Loss: 0.00002227
Iteration 89/1000 | Loss: 0.00002227
Iteration 90/1000 | Loss: 0.00002227
Iteration 91/1000 | Loss: 0.00002227
Iteration 92/1000 | Loss: 0.00002227
Iteration 93/1000 | Loss: 0.00002227
Iteration 94/1000 | Loss: 0.00002226
Iteration 95/1000 | Loss: 0.00002226
Iteration 96/1000 | Loss: 0.00002226
Iteration 97/1000 | Loss: 0.00002226
Iteration 98/1000 | Loss: 0.00002226
Iteration 99/1000 | Loss: 0.00002226
Iteration 100/1000 | Loss: 0.00002226
Iteration 101/1000 | Loss: 0.00002226
Iteration 102/1000 | Loss: 0.00002226
Iteration 103/1000 | Loss: 0.00002226
Iteration 104/1000 | Loss: 0.00002226
Iteration 105/1000 | Loss: 0.00002225
Iteration 106/1000 | Loss: 0.00002225
Iteration 107/1000 | Loss: 0.00002225
Iteration 108/1000 | Loss: 0.00002225
Iteration 109/1000 | Loss: 0.00002225
Iteration 110/1000 | Loss: 0.00002225
Iteration 111/1000 | Loss: 0.00002225
Iteration 112/1000 | Loss: 0.00002225
Iteration 113/1000 | Loss: 0.00002225
Iteration 114/1000 | Loss: 0.00002224
Iteration 115/1000 | Loss: 0.00002224
Iteration 116/1000 | Loss: 0.00002224
Iteration 117/1000 | Loss: 0.00002224
Iteration 118/1000 | Loss: 0.00002224
Iteration 119/1000 | Loss: 0.00002224
Iteration 120/1000 | Loss: 0.00002224
Iteration 121/1000 | Loss: 0.00002224
Iteration 122/1000 | Loss: 0.00002224
Iteration 123/1000 | Loss: 0.00002223
Iteration 124/1000 | Loss: 0.00002223
Iteration 125/1000 | Loss: 0.00002223
Iteration 126/1000 | Loss: 0.00002223
Iteration 127/1000 | Loss: 0.00002223
Iteration 128/1000 | Loss: 0.00002223
Iteration 129/1000 | Loss: 0.00002223
Iteration 130/1000 | Loss: 0.00002223
Iteration 131/1000 | Loss: 0.00002223
Iteration 132/1000 | Loss: 0.00002223
Iteration 133/1000 | Loss: 0.00002223
Iteration 134/1000 | Loss: 0.00002223
Iteration 135/1000 | Loss: 0.00002223
Iteration 136/1000 | Loss: 0.00002223
Iteration 137/1000 | Loss: 0.00002223
Iteration 138/1000 | Loss: 0.00002223
Iteration 139/1000 | Loss: 0.00002223
Iteration 140/1000 | Loss: 0.00002223
Iteration 141/1000 | Loss: 0.00002223
Iteration 142/1000 | Loss: 0.00002223
Iteration 143/1000 | Loss: 0.00002223
Iteration 144/1000 | Loss: 0.00002223
Iteration 145/1000 | Loss: 0.00002223
Iteration 146/1000 | Loss: 0.00002223
Iteration 147/1000 | Loss: 0.00002223
Iteration 148/1000 | Loss: 0.00002223
Iteration 149/1000 | Loss: 0.00002223
Iteration 150/1000 | Loss: 0.00002223
Iteration 151/1000 | Loss: 0.00002223
Iteration 152/1000 | Loss: 0.00002223
Iteration 153/1000 | Loss: 0.00002223
Iteration 154/1000 | Loss: 0.00002223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.222843249910511e-05, 2.222843249910511e-05, 2.222843249910511e-05, 2.222843249910511e-05, 2.222843249910511e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.222843249910511e-05

Optimization complete. Final v2v error: 3.6175026893615723 mm

Highest mean error: 22.298328399658203 mm for frame 70

Lowest mean error: 3.087348461151123 mm for frame 153

Saving results

Total time: 90.6927375793457
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420842
Iteration 2/25 | Loss: 0.00101529
Iteration 3/25 | Loss: 0.00092326
Iteration 4/25 | Loss: 0.00090389
Iteration 5/25 | Loss: 0.00089949
Iteration 6/25 | Loss: 0.00089870
Iteration 7/25 | Loss: 0.00089865
Iteration 8/25 | Loss: 0.00089865
Iteration 9/25 | Loss: 0.00089865
Iteration 10/25 | Loss: 0.00089865
Iteration 11/25 | Loss: 0.00089865
Iteration 12/25 | Loss: 0.00089865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008986456086859107, 0.0008986456086859107, 0.0008986456086859107, 0.0008986456086859107, 0.0008986456086859107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008986456086859107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35844111
Iteration 2/25 | Loss: 0.00040323
Iteration 3/25 | Loss: 0.00040323
Iteration 4/25 | Loss: 0.00040323
Iteration 5/25 | Loss: 0.00040323
Iteration 6/25 | Loss: 0.00040323
Iteration 7/25 | Loss: 0.00040323
Iteration 8/25 | Loss: 0.00040323
Iteration 9/25 | Loss: 0.00040323
Iteration 10/25 | Loss: 0.00040323
Iteration 11/25 | Loss: 0.00040323
Iteration 12/25 | Loss: 0.00040323
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00040323365828953683, 0.00040323365828953683, 0.00040323365828953683, 0.00040323365828953683, 0.00040323365828953683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00040323365828953683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040323
Iteration 2/1000 | Loss: 0.00004345
Iteration 3/1000 | Loss: 0.00002534
Iteration 4/1000 | Loss: 0.00002268
Iteration 5/1000 | Loss: 0.00002134
Iteration 6/1000 | Loss: 0.00002074
Iteration 7/1000 | Loss: 0.00002030
Iteration 8/1000 | Loss: 0.00002002
Iteration 9/1000 | Loss: 0.00001989
Iteration 10/1000 | Loss: 0.00001972
Iteration 11/1000 | Loss: 0.00001972
Iteration 12/1000 | Loss: 0.00001959
Iteration 13/1000 | Loss: 0.00001958
Iteration 14/1000 | Loss: 0.00001957
Iteration 15/1000 | Loss: 0.00001954
Iteration 16/1000 | Loss: 0.00001954
Iteration 17/1000 | Loss: 0.00001953
Iteration 18/1000 | Loss: 0.00001953
Iteration 19/1000 | Loss: 0.00001953
Iteration 20/1000 | Loss: 0.00001952
Iteration 21/1000 | Loss: 0.00001952
Iteration 22/1000 | Loss: 0.00001948
Iteration 23/1000 | Loss: 0.00001947
Iteration 24/1000 | Loss: 0.00001945
Iteration 25/1000 | Loss: 0.00001944
Iteration 26/1000 | Loss: 0.00001944
Iteration 27/1000 | Loss: 0.00001943
Iteration 28/1000 | Loss: 0.00001943
Iteration 29/1000 | Loss: 0.00001942
Iteration 30/1000 | Loss: 0.00001942
Iteration 31/1000 | Loss: 0.00001942
Iteration 32/1000 | Loss: 0.00001941
Iteration 33/1000 | Loss: 0.00001941
Iteration 34/1000 | Loss: 0.00001941
Iteration 35/1000 | Loss: 0.00001941
Iteration 36/1000 | Loss: 0.00001940
Iteration 37/1000 | Loss: 0.00001940
Iteration 38/1000 | Loss: 0.00001939
Iteration 39/1000 | Loss: 0.00001939
Iteration 40/1000 | Loss: 0.00001939
Iteration 41/1000 | Loss: 0.00001938
Iteration 42/1000 | Loss: 0.00001938
Iteration 43/1000 | Loss: 0.00001938
Iteration 44/1000 | Loss: 0.00001937
Iteration 45/1000 | Loss: 0.00001937
Iteration 46/1000 | Loss: 0.00001936
Iteration 47/1000 | Loss: 0.00001936
Iteration 48/1000 | Loss: 0.00001936
Iteration 49/1000 | Loss: 0.00001936
Iteration 50/1000 | Loss: 0.00001935
Iteration 51/1000 | Loss: 0.00001935
Iteration 52/1000 | Loss: 0.00001935
Iteration 53/1000 | Loss: 0.00001935
Iteration 54/1000 | Loss: 0.00001935
Iteration 55/1000 | Loss: 0.00001935
Iteration 56/1000 | Loss: 0.00001935
Iteration 57/1000 | Loss: 0.00001935
Iteration 58/1000 | Loss: 0.00001934
Iteration 59/1000 | Loss: 0.00001934
Iteration 60/1000 | Loss: 0.00001934
Iteration 61/1000 | Loss: 0.00001934
Iteration 62/1000 | Loss: 0.00001934
Iteration 63/1000 | Loss: 0.00001934
Iteration 64/1000 | Loss: 0.00001934
Iteration 65/1000 | Loss: 0.00001934
Iteration 66/1000 | Loss: 0.00001934
Iteration 67/1000 | Loss: 0.00001934
Iteration 68/1000 | Loss: 0.00001934
Iteration 69/1000 | Loss: 0.00001934
Iteration 70/1000 | Loss: 0.00001934
Iteration 71/1000 | Loss: 0.00001934
Iteration 72/1000 | Loss: 0.00001934
Iteration 73/1000 | Loss: 0.00001933
Iteration 74/1000 | Loss: 0.00001933
Iteration 75/1000 | Loss: 0.00001933
Iteration 76/1000 | Loss: 0.00001933
Iteration 77/1000 | Loss: 0.00001933
Iteration 78/1000 | Loss: 0.00001933
Iteration 79/1000 | Loss: 0.00001933
Iteration 80/1000 | Loss: 0.00001933
Iteration 81/1000 | Loss: 0.00001933
Iteration 82/1000 | Loss: 0.00001933
Iteration 83/1000 | Loss: 0.00001933
Iteration 84/1000 | Loss: 0.00001933
Iteration 85/1000 | Loss: 0.00001933
Iteration 86/1000 | Loss: 0.00001933
Iteration 87/1000 | Loss: 0.00001933
Iteration 88/1000 | Loss: 0.00001933
Iteration 89/1000 | Loss: 0.00001933
Iteration 90/1000 | Loss: 0.00001933
Iteration 91/1000 | Loss: 0.00001933
Iteration 92/1000 | Loss: 0.00001933
Iteration 93/1000 | Loss: 0.00001933
Iteration 94/1000 | Loss: 0.00001933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.9327881091157906e-05, 1.9327881091157906e-05, 1.9327881091157906e-05, 1.9327881091157906e-05, 1.9327881091157906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9327881091157906e-05

Optimization complete. Final v2v error: 3.8564369678497314 mm

Highest mean error: 4.079784393310547 mm for frame 36

Lowest mean error: 3.610398769378662 mm for frame 82

Saving results

Total time: 28.69617748260498
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840832
Iteration 2/25 | Loss: 0.00129428
Iteration 3/25 | Loss: 0.00108413
Iteration 4/25 | Loss: 0.00103927
Iteration 5/25 | Loss: 0.00102391
Iteration 6/25 | Loss: 0.00102176
Iteration 7/25 | Loss: 0.00102161
Iteration 8/25 | Loss: 0.00102161
Iteration 9/25 | Loss: 0.00102161
Iteration 10/25 | Loss: 0.00102161
Iteration 11/25 | Loss: 0.00102161
Iteration 12/25 | Loss: 0.00102161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00102161371614784, 0.00102161371614784, 0.00102161371614784, 0.00102161371614784, 0.00102161371614784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00102161371614784

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34919488
Iteration 2/25 | Loss: 0.00050882
Iteration 3/25 | Loss: 0.00050879
Iteration 4/25 | Loss: 0.00050879
Iteration 5/25 | Loss: 0.00050879
Iteration 6/25 | Loss: 0.00050878
Iteration 7/25 | Loss: 0.00050878
Iteration 8/25 | Loss: 0.00050878
Iteration 9/25 | Loss: 0.00050878
Iteration 10/25 | Loss: 0.00050878
Iteration 11/25 | Loss: 0.00050878
Iteration 12/25 | Loss: 0.00050878
Iteration 13/25 | Loss: 0.00050878
Iteration 14/25 | Loss: 0.00050878
Iteration 15/25 | Loss: 0.00050878
Iteration 16/25 | Loss: 0.00050878
Iteration 17/25 | Loss: 0.00050878
Iteration 18/25 | Loss: 0.00050878
Iteration 19/25 | Loss: 0.00050878
Iteration 20/25 | Loss: 0.00050878
Iteration 21/25 | Loss: 0.00050878
Iteration 22/25 | Loss: 0.00050878
Iteration 23/25 | Loss: 0.00050878
Iteration 24/25 | Loss: 0.00050878
Iteration 25/25 | Loss: 0.00050878

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050878
Iteration 2/1000 | Loss: 0.00003734
Iteration 3/1000 | Loss: 0.00002832
Iteration 4/1000 | Loss: 0.00002617
Iteration 5/1000 | Loss: 0.00002505
Iteration 6/1000 | Loss: 0.00002412
Iteration 7/1000 | Loss: 0.00002354
Iteration 8/1000 | Loss: 0.00002298
Iteration 9/1000 | Loss: 0.00002264
Iteration 10/1000 | Loss: 0.00002244
Iteration 11/1000 | Loss: 0.00002236
Iteration 12/1000 | Loss: 0.00002230
Iteration 13/1000 | Loss: 0.00002215
Iteration 14/1000 | Loss: 0.00002214
Iteration 15/1000 | Loss: 0.00002214
Iteration 16/1000 | Loss: 0.00002212
Iteration 17/1000 | Loss: 0.00002212
Iteration 18/1000 | Loss: 0.00002212
Iteration 19/1000 | Loss: 0.00002211
Iteration 20/1000 | Loss: 0.00002210
Iteration 21/1000 | Loss: 0.00002205
Iteration 22/1000 | Loss: 0.00002205
Iteration 23/1000 | Loss: 0.00002205
Iteration 24/1000 | Loss: 0.00002204
Iteration 25/1000 | Loss: 0.00002204
Iteration 26/1000 | Loss: 0.00002202
Iteration 27/1000 | Loss: 0.00002202
Iteration 28/1000 | Loss: 0.00002201
Iteration 29/1000 | Loss: 0.00002201
Iteration 30/1000 | Loss: 0.00002201
Iteration 31/1000 | Loss: 0.00002201
Iteration 32/1000 | Loss: 0.00002200
Iteration 33/1000 | Loss: 0.00002200
Iteration 34/1000 | Loss: 0.00002200
Iteration 35/1000 | Loss: 0.00002200
Iteration 36/1000 | Loss: 0.00002200
Iteration 37/1000 | Loss: 0.00002199
Iteration 38/1000 | Loss: 0.00002199
Iteration 39/1000 | Loss: 0.00002199
Iteration 40/1000 | Loss: 0.00002199
Iteration 41/1000 | Loss: 0.00002198
Iteration 42/1000 | Loss: 0.00002198
Iteration 43/1000 | Loss: 0.00002198
Iteration 44/1000 | Loss: 0.00002197
Iteration 45/1000 | Loss: 0.00002197
Iteration 46/1000 | Loss: 0.00002196
Iteration 47/1000 | Loss: 0.00002196
Iteration 48/1000 | Loss: 0.00002196
Iteration 49/1000 | Loss: 0.00002196
Iteration 50/1000 | Loss: 0.00002196
Iteration 51/1000 | Loss: 0.00002196
Iteration 52/1000 | Loss: 0.00002196
Iteration 53/1000 | Loss: 0.00002196
Iteration 54/1000 | Loss: 0.00002196
Iteration 55/1000 | Loss: 0.00002195
Iteration 56/1000 | Loss: 0.00002195
Iteration 57/1000 | Loss: 0.00002195
Iteration 58/1000 | Loss: 0.00002195
Iteration 59/1000 | Loss: 0.00002195
Iteration 60/1000 | Loss: 0.00002195
Iteration 61/1000 | Loss: 0.00002195
Iteration 62/1000 | Loss: 0.00002195
Iteration 63/1000 | Loss: 0.00002195
Iteration 64/1000 | Loss: 0.00002195
Iteration 65/1000 | Loss: 0.00002195
Iteration 66/1000 | Loss: 0.00002195
Iteration 67/1000 | Loss: 0.00002195
Iteration 68/1000 | Loss: 0.00002195
Iteration 69/1000 | Loss: 0.00002195
Iteration 70/1000 | Loss: 0.00002195
Iteration 71/1000 | Loss: 0.00002195
Iteration 72/1000 | Loss: 0.00002195
Iteration 73/1000 | Loss: 0.00002195
Iteration 74/1000 | Loss: 0.00002195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [2.1945716071058996e-05, 2.1945716071058996e-05, 2.1945716071058996e-05, 2.1945716071058996e-05, 2.1945716071058996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1945716071058996e-05

Optimization complete. Final v2v error: 4.064798831939697 mm

Highest mean error: 4.426623344421387 mm for frame 200

Lowest mean error: 3.271134614944458 mm for frame 9

Saving results

Total time: 35.58628273010254
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01142733
Iteration 2/25 | Loss: 0.00303790
Iteration 3/25 | Loss: 0.00238697
Iteration 4/25 | Loss: 0.00208494
Iteration 5/25 | Loss: 0.00162528
Iteration 6/25 | Loss: 0.00142949
Iteration 7/25 | Loss: 0.00130038
Iteration 8/25 | Loss: 0.00117188
Iteration 9/25 | Loss: 0.00108919
Iteration 10/25 | Loss: 0.00102663
Iteration 11/25 | Loss: 0.00100971
Iteration 12/25 | Loss: 0.00100268
Iteration 13/25 | Loss: 0.00100440
Iteration 14/25 | Loss: 0.00099849
Iteration 15/25 | Loss: 0.00099196
Iteration 16/25 | Loss: 0.00098953
Iteration 17/25 | Loss: 0.00099083
Iteration 18/25 | Loss: 0.00099190
Iteration 19/25 | Loss: 0.00098858
Iteration 20/25 | Loss: 0.00099035
Iteration 21/25 | Loss: 0.00098798
Iteration 22/25 | Loss: 0.00098338
Iteration 23/25 | Loss: 0.00098208
Iteration 24/25 | Loss: 0.00098441
Iteration 25/25 | Loss: 0.00098138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.55204445
Iteration 2/25 | Loss: 0.00087440
Iteration 3/25 | Loss: 0.00087440
Iteration 4/25 | Loss: 0.00087440
Iteration 5/25 | Loss: 0.00087440
Iteration 6/25 | Loss: 0.00087440
Iteration 7/25 | Loss: 0.00087440
Iteration 8/25 | Loss: 0.00087440
Iteration 9/25 | Loss: 0.00087440
Iteration 10/25 | Loss: 0.00087440
Iteration 11/25 | Loss: 0.00087440
Iteration 12/25 | Loss: 0.00087440
Iteration 13/25 | Loss: 0.00087440
Iteration 14/25 | Loss: 0.00087440
Iteration 15/25 | Loss: 0.00087440
Iteration 16/25 | Loss: 0.00087440
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008743972284719348, 0.0008743972284719348, 0.0008743972284719348, 0.0008743972284719348, 0.0008743972284719348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008743972284719348

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087440
Iteration 2/1000 | Loss: 0.00019070
Iteration 3/1000 | Loss: 0.00012373
Iteration 4/1000 | Loss: 0.00009931
Iteration 5/1000 | Loss: 0.00008528
Iteration 6/1000 | Loss: 0.00007959
Iteration 7/1000 | Loss: 0.00007672
Iteration 8/1000 | Loss: 0.00007531
Iteration 9/1000 | Loss: 0.00007398
Iteration 10/1000 | Loss: 0.00007311
Iteration 11/1000 | Loss: 0.00007235
Iteration 12/1000 | Loss: 0.00007173
Iteration 13/1000 | Loss: 0.00007072
Iteration 14/1000 | Loss: 0.00062269
Iteration 15/1000 | Loss: 0.00109198
Iteration 16/1000 | Loss: 0.00062164
Iteration 17/1000 | Loss: 0.00011558
Iteration 18/1000 | Loss: 0.00218056
Iteration 19/1000 | Loss: 0.00052012
Iteration 20/1000 | Loss: 0.00086717
Iteration 21/1000 | Loss: 0.00081626
Iteration 22/1000 | Loss: 0.00122115
Iteration 23/1000 | Loss: 0.00039917
Iteration 24/1000 | Loss: 0.00110304
Iteration 25/1000 | Loss: 0.00007433
Iteration 26/1000 | Loss: 0.00013117
Iteration 27/1000 | Loss: 0.00005452
Iteration 28/1000 | Loss: 0.00005106
Iteration 29/1000 | Loss: 0.00004866
Iteration 30/1000 | Loss: 0.00004738
Iteration 31/1000 | Loss: 0.00004642
Iteration 32/1000 | Loss: 0.00004545
Iteration 33/1000 | Loss: 0.00064467
Iteration 34/1000 | Loss: 0.00004497
Iteration 35/1000 | Loss: 0.00004339
Iteration 36/1000 | Loss: 0.00004291
Iteration 37/1000 | Loss: 0.00004267
Iteration 38/1000 | Loss: 0.00004255
Iteration 39/1000 | Loss: 0.00004247
Iteration 40/1000 | Loss: 0.00004236
Iteration 41/1000 | Loss: 0.00004236
Iteration 42/1000 | Loss: 0.00004228
Iteration 43/1000 | Loss: 0.00004226
Iteration 44/1000 | Loss: 0.00004225
Iteration 45/1000 | Loss: 0.00004208
Iteration 46/1000 | Loss: 0.00004189
Iteration 47/1000 | Loss: 0.00004162
Iteration 48/1000 | Loss: 0.00007363
Iteration 49/1000 | Loss: 0.00004904
Iteration 50/1000 | Loss: 0.00004321
Iteration 51/1000 | Loss: 0.00004183
Iteration 52/1000 | Loss: 0.00007399
Iteration 53/1000 | Loss: 0.00229627
Iteration 54/1000 | Loss: 0.00031846
Iteration 55/1000 | Loss: 0.00020638
Iteration 56/1000 | Loss: 0.00004608
Iteration 57/1000 | Loss: 0.00006900
Iteration 58/1000 | Loss: 0.00005853
Iteration 59/1000 | Loss: 0.00006534
Iteration 60/1000 | Loss: 0.00005291
Iteration 61/1000 | Loss: 0.00005918
Iteration 62/1000 | Loss: 0.00005603
Iteration 63/1000 | Loss: 0.00004139
Iteration 64/1000 | Loss: 0.00006326
Iteration 65/1000 | Loss: 0.00005035
Iteration 66/1000 | Loss: 0.00006514
Iteration 67/1000 | Loss: 0.00005424
Iteration 68/1000 | Loss: 0.00004509
Iteration 69/1000 | Loss: 0.00004364
Iteration 70/1000 | Loss: 0.00006162
Iteration 71/1000 | Loss: 0.00004402
Iteration 72/1000 | Loss: 0.00004869
Iteration 73/1000 | Loss: 0.00005366
Iteration 74/1000 | Loss: 0.00004727
Iteration 75/1000 | Loss: 0.00005796
Iteration 76/1000 | Loss: 0.00004309
Iteration 77/1000 | Loss: 0.00004588
Iteration 78/1000 | Loss: 0.00006048
Iteration 79/1000 | Loss: 0.00004756
Iteration 80/1000 | Loss: 0.00006355
Iteration 81/1000 | Loss: 0.00004192
Iteration 82/1000 | Loss: 0.00004148
Iteration 83/1000 | Loss: 0.00004113
Iteration 84/1000 | Loss: 0.00004107
Iteration 85/1000 | Loss: 0.00004094
Iteration 86/1000 | Loss: 0.00004084
Iteration 87/1000 | Loss: 0.00004082
Iteration 88/1000 | Loss: 0.00004076
Iteration 89/1000 | Loss: 0.00004075
Iteration 90/1000 | Loss: 0.00004074
Iteration 91/1000 | Loss: 0.00004074
Iteration 92/1000 | Loss: 0.00004073
Iteration 93/1000 | Loss: 0.00004073
Iteration 94/1000 | Loss: 0.00004073
Iteration 95/1000 | Loss: 0.00004072
Iteration 96/1000 | Loss: 0.00004072
Iteration 97/1000 | Loss: 0.00004072
Iteration 98/1000 | Loss: 0.00004072
Iteration 99/1000 | Loss: 0.00004072
Iteration 100/1000 | Loss: 0.00004072
Iteration 101/1000 | Loss: 0.00004072
Iteration 102/1000 | Loss: 0.00004072
Iteration 103/1000 | Loss: 0.00004072
Iteration 104/1000 | Loss: 0.00004072
Iteration 105/1000 | Loss: 0.00004072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [4.071932926308364e-05, 4.071932926308364e-05, 4.071932926308364e-05, 4.071932926308364e-05, 4.071932926308364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.071932926308364e-05

Optimization complete. Final v2v error: 5.323422908782959 mm

Highest mean error: 10.50448989868164 mm for frame 41

Lowest mean error: 4.887762546539307 mm for frame 26

Saving results

Total time: 158.50908303260803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889976
Iteration 2/25 | Loss: 0.00132129
Iteration 3/25 | Loss: 0.00115291
Iteration 4/25 | Loss: 0.00109994
Iteration 5/25 | Loss: 0.00108468
Iteration 6/25 | Loss: 0.00107994
Iteration 7/25 | Loss: 0.00107870
Iteration 8/25 | Loss: 0.00107870
Iteration 9/25 | Loss: 0.00107870
Iteration 10/25 | Loss: 0.00107870
Iteration 11/25 | Loss: 0.00107870
Iteration 12/25 | Loss: 0.00107870
Iteration 13/25 | Loss: 0.00107870
Iteration 14/25 | Loss: 0.00107870
Iteration 15/25 | Loss: 0.00107870
Iteration 16/25 | Loss: 0.00107870
Iteration 17/25 | Loss: 0.00107870
Iteration 18/25 | Loss: 0.00107870
Iteration 19/25 | Loss: 0.00107870
Iteration 20/25 | Loss: 0.00107870
Iteration 21/25 | Loss: 0.00107870
Iteration 22/25 | Loss: 0.00107870
Iteration 23/25 | Loss: 0.00107870
Iteration 24/25 | Loss: 0.00107870
Iteration 25/25 | Loss: 0.00107870

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45411897
Iteration 2/25 | Loss: 0.00044570
Iteration 3/25 | Loss: 0.00044570
Iteration 4/25 | Loss: 0.00044569
Iteration 5/25 | Loss: 0.00044569
Iteration 6/25 | Loss: 0.00044569
Iteration 7/25 | Loss: 0.00044569
Iteration 8/25 | Loss: 0.00044569
Iteration 9/25 | Loss: 0.00044569
Iteration 10/25 | Loss: 0.00044569
Iteration 11/25 | Loss: 0.00044569
Iteration 12/25 | Loss: 0.00044569
Iteration 13/25 | Loss: 0.00044569
Iteration 14/25 | Loss: 0.00044569
Iteration 15/25 | Loss: 0.00044569
Iteration 16/25 | Loss: 0.00044569
Iteration 17/25 | Loss: 0.00044569
Iteration 18/25 | Loss: 0.00044569
Iteration 19/25 | Loss: 0.00044569
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004456915776245296, 0.0004456915776245296, 0.0004456915776245296, 0.0004456915776245296, 0.0004456915776245296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004456915776245296

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044569
Iteration 2/1000 | Loss: 0.00008132
Iteration 3/1000 | Loss: 0.00004928
Iteration 4/1000 | Loss: 0.00004123
Iteration 5/1000 | Loss: 0.00003984
Iteration 6/1000 | Loss: 0.00003834
Iteration 7/1000 | Loss: 0.00003750
Iteration 8/1000 | Loss: 0.00003688
Iteration 9/1000 | Loss: 0.00003636
Iteration 10/1000 | Loss: 0.00003593
Iteration 11/1000 | Loss: 0.00003562
Iteration 12/1000 | Loss: 0.00003546
Iteration 13/1000 | Loss: 0.00003523
Iteration 14/1000 | Loss: 0.00003511
Iteration 15/1000 | Loss: 0.00003510
Iteration 16/1000 | Loss: 0.00003510
Iteration 17/1000 | Loss: 0.00003509
Iteration 18/1000 | Loss: 0.00003508
Iteration 19/1000 | Loss: 0.00003507
Iteration 20/1000 | Loss: 0.00003507
Iteration 21/1000 | Loss: 0.00003506
Iteration 22/1000 | Loss: 0.00003506
Iteration 23/1000 | Loss: 0.00003506
Iteration 24/1000 | Loss: 0.00003506
Iteration 25/1000 | Loss: 0.00003506
Iteration 26/1000 | Loss: 0.00003506
Iteration 27/1000 | Loss: 0.00003505
Iteration 28/1000 | Loss: 0.00003505
Iteration 29/1000 | Loss: 0.00003503
Iteration 30/1000 | Loss: 0.00003502
Iteration 31/1000 | Loss: 0.00003502
Iteration 32/1000 | Loss: 0.00003502
Iteration 33/1000 | Loss: 0.00003501
Iteration 34/1000 | Loss: 0.00003501
Iteration 35/1000 | Loss: 0.00003501
Iteration 36/1000 | Loss: 0.00003500
Iteration 37/1000 | Loss: 0.00003500
Iteration 38/1000 | Loss: 0.00003500
Iteration 39/1000 | Loss: 0.00003499
Iteration 40/1000 | Loss: 0.00003499
Iteration 41/1000 | Loss: 0.00003498
Iteration 42/1000 | Loss: 0.00003497
Iteration 43/1000 | Loss: 0.00003497
Iteration 44/1000 | Loss: 0.00003497
Iteration 45/1000 | Loss: 0.00003496
Iteration 46/1000 | Loss: 0.00003496
Iteration 47/1000 | Loss: 0.00003496
Iteration 48/1000 | Loss: 0.00003496
Iteration 49/1000 | Loss: 0.00003496
Iteration 50/1000 | Loss: 0.00003496
Iteration 51/1000 | Loss: 0.00003496
Iteration 52/1000 | Loss: 0.00003495
Iteration 53/1000 | Loss: 0.00003495
Iteration 54/1000 | Loss: 0.00003494
Iteration 55/1000 | Loss: 0.00003494
Iteration 56/1000 | Loss: 0.00003493
Iteration 57/1000 | Loss: 0.00003493
Iteration 58/1000 | Loss: 0.00003493
Iteration 59/1000 | Loss: 0.00003493
Iteration 60/1000 | Loss: 0.00003493
Iteration 61/1000 | Loss: 0.00003493
Iteration 62/1000 | Loss: 0.00003492
Iteration 63/1000 | Loss: 0.00003492
Iteration 64/1000 | Loss: 0.00003492
Iteration 65/1000 | Loss: 0.00003492
Iteration 66/1000 | Loss: 0.00003492
Iteration 67/1000 | Loss: 0.00003491
Iteration 68/1000 | Loss: 0.00003491
Iteration 69/1000 | Loss: 0.00003491
Iteration 70/1000 | Loss: 0.00003491
Iteration 71/1000 | Loss: 0.00003491
Iteration 72/1000 | Loss: 0.00003491
Iteration 73/1000 | Loss: 0.00003491
Iteration 74/1000 | Loss: 0.00003491
Iteration 75/1000 | Loss: 0.00003491
Iteration 76/1000 | Loss: 0.00003491
Iteration 77/1000 | Loss: 0.00003491
Iteration 78/1000 | Loss: 0.00003491
Iteration 79/1000 | Loss: 0.00003491
Iteration 80/1000 | Loss: 0.00003490
Iteration 81/1000 | Loss: 0.00003490
Iteration 82/1000 | Loss: 0.00003490
Iteration 83/1000 | Loss: 0.00003490
Iteration 84/1000 | Loss: 0.00003490
Iteration 85/1000 | Loss: 0.00003490
Iteration 86/1000 | Loss: 0.00003490
Iteration 87/1000 | Loss: 0.00003490
Iteration 88/1000 | Loss: 0.00003489
Iteration 89/1000 | Loss: 0.00003489
Iteration 90/1000 | Loss: 0.00003489
Iteration 91/1000 | Loss: 0.00003489
Iteration 92/1000 | Loss: 0.00003489
Iteration 93/1000 | Loss: 0.00003489
Iteration 94/1000 | Loss: 0.00003489
Iteration 95/1000 | Loss: 0.00003489
Iteration 96/1000 | Loss: 0.00003489
Iteration 97/1000 | Loss: 0.00003489
Iteration 98/1000 | Loss: 0.00003489
Iteration 99/1000 | Loss: 0.00003489
Iteration 100/1000 | Loss: 0.00003488
Iteration 101/1000 | Loss: 0.00003488
Iteration 102/1000 | Loss: 0.00003488
Iteration 103/1000 | Loss: 0.00003488
Iteration 104/1000 | Loss: 0.00003488
Iteration 105/1000 | Loss: 0.00003488
Iteration 106/1000 | Loss: 0.00003488
Iteration 107/1000 | Loss: 0.00003487
Iteration 108/1000 | Loss: 0.00003487
Iteration 109/1000 | Loss: 0.00003487
Iteration 110/1000 | Loss: 0.00003487
Iteration 111/1000 | Loss: 0.00003487
Iteration 112/1000 | Loss: 0.00003487
Iteration 113/1000 | Loss: 0.00003487
Iteration 114/1000 | Loss: 0.00003487
Iteration 115/1000 | Loss: 0.00003487
Iteration 116/1000 | Loss: 0.00003486
Iteration 117/1000 | Loss: 0.00003486
Iteration 118/1000 | Loss: 0.00003486
Iteration 119/1000 | Loss: 0.00003486
Iteration 120/1000 | Loss: 0.00003486
Iteration 121/1000 | Loss: 0.00003486
Iteration 122/1000 | Loss: 0.00003486
Iteration 123/1000 | Loss: 0.00003486
Iteration 124/1000 | Loss: 0.00003486
Iteration 125/1000 | Loss: 0.00003486
Iteration 126/1000 | Loss: 0.00003486
Iteration 127/1000 | Loss: 0.00003486
Iteration 128/1000 | Loss: 0.00003485
Iteration 129/1000 | Loss: 0.00003485
Iteration 130/1000 | Loss: 0.00003485
Iteration 131/1000 | Loss: 0.00003485
Iteration 132/1000 | Loss: 0.00003485
Iteration 133/1000 | Loss: 0.00003485
Iteration 134/1000 | Loss: 0.00003485
Iteration 135/1000 | Loss: 0.00003485
Iteration 136/1000 | Loss: 0.00003485
Iteration 137/1000 | Loss: 0.00003485
Iteration 138/1000 | Loss: 0.00003485
Iteration 139/1000 | Loss: 0.00003485
Iteration 140/1000 | Loss: 0.00003485
Iteration 141/1000 | Loss: 0.00003484
Iteration 142/1000 | Loss: 0.00003484
Iteration 143/1000 | Loss: 0.00003484
Iteration 144/1000 | Loss: 0.00003484
Iteration 145/1000 | Loss: 0.00003484
Iteration 146/1000 | Loss: 0.00003484
Iteration 147/1000 | Loss: 0.00003484
Iteration 148/1000 | Loss: 0.00003484
Iteration 149/1000 | Loss: 0.00003484
Iteration 150/1000 | Loss: 0.00003484
Iteration 151/1000 | Loss: 0.00003484
Iteration 152/1000 | Loss: 0.00003484
Iteration 153/1000 | Loss: 0.00003484
Iteration 154/1000 | Loss: 0.00003484
Iteration 155/1000 | Loss: 0.00003484
Iteration 156/1000 | Loss: 0.00003484
Iteration 157/1000 | Loss: 0.00003484
Iteration 158/1000 | Loss: 0.00003484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [3.484032640699297e-05, 3.484032640699297e-05, 3.484032640699297e-05, 3.484032640699297e-05, 3.484032640699297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.484032640699297e-05

Optimization complete. Final v2v error: 4.998145580291748 mm

Highest mean error: 5.502806663513184 mm for frame 75

Lowest mean error: 4.247918128967285 mm for frame 0

Saving results

Total time: 38.111140966415405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00715305
Iteration 2/25 | Loss: 0.00162092
Iteration 3/25 | Loss: 0.00104044
Iteration 4/25 | Loss: 0.00093963
Iteration 5/25 | Loss: 0.00093674
Iteration 6/25 | Loss: 0.00091347
Iteration 7/25 | Loss: 0.00091698
Iteration 8/25 | Loss: 0.00091124
Iteration 9/25 | Loss: 0.00091105
Iteration 10/25 | Loss: 0.00091101
Iteration 11/25 | Loss: 0.00091101
Iteration 12/25 | Loss: 0.00091101
Iteration 13/25 | Loss: 0.00091101
Iteration 14/25 | Loss: 0.00091101
Iteration 15/25 | Loss: 0.00091101
Iteration 16/25 | Loss: 0.00091101
Iteration 17/25 | Loss: 0.00091101
Iteration 18/25 | Loss: 0.00091100
Iteration 19/25 | Loss: 0.00091100
Iteration 20/25 | Loss: 0.00091100
Iteration 21/25 | Loss: 0.00091100
Iteration 22/25 | Loss: 0.00091100
Iteration 23/25 | Loss: 0.00091100
Iteration 24/25 | Loss: 0.00091100
Iteration 25/25 | Loss: 0.00091100

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.76754665
Iteration 2/25 | Loss: 0.00025925
Iteration 3/25 | Loss: 0.00025917
Iteration 4/25 | Loss: 0.00025917
Iteration 5/25 | Loss: 0.00025917
Iteration 6/25 | Loss: 0.00025917
Iteration 7/25 | Loss: 0.00025917
Iteration 8/25 | Loss: 0.00025917
Iteration 9/25 | Loss: 0.00025917
Iteration 10/25 | Loss: 0.00025917
Iteration 11/25 | Loss: 0.00025917
Iteration 12/25 | Loss: 0.00025917
Iteration 13/25 | Loss: 0.00025917
Iteration 14/25 | Loss: 0.00025917
Iteration 15/25 | Loss: 0.00025917
Iteration 16/25 | Loss: 0.00025917
Iteration 17/25 | Loss: 0.00025917
Iteration 18/25 | Loss: 0.00025917
Iteration 19/25 | Loss: 0.00025917
Iteration 20/25 | Loss: 0.00025917
Iteration 21/25 | Loss: 0.00025917
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0002591715310700238, 0.0002591715310700238, 0.0002591715310700238, 0.0002591715310700238, 0.0002591715310700238]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002591715310700238

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025917
Iteration 2/1000 | Loss: 0.00003511
Iteration 3/1000 | Loss: 0.00002411
Iteration 4/1000 | Loss: 0.00002186
Iteration 5/1000 | Loss: 0.00002078
Iteration 6/1000 | Loss: 0.00001977
Iteration 7/1000 | Loss: 0.00001916
Iteration 8/1000 | Loss: 0.00001888
Iteration 9/1000 | Loss: 0.00001869
Iteration 10/1000 | Loss: 0.00001865
Iteration 11/1000 | Loss: 0.00001853
Iteration 12/1000 | Loss: 0.00001853
Iteration 13/1000 | Loss: 0.00001853
Iteration 14/1000 | Loss: 0.00001848
Iteration 15/1000 | Loss: 0.00001848
Iteration 16/1000 | Loss: 0.00001847
Iteration 17/1000 | Loss: 0.00001847
Iteration 18/1000 | Loss: 0.00001840
Iteration 19/1000 | Loss: 0.00001837
Iteration 20/1000 | Loss: 0.00001837
Iteration 21/1000 | Loss: 0.00001836
Iteration 22/1000 | Loss: 0.00001836
Iteration 23/1000 | Loss: 0.00001835
Iteration 24/1000 | Loss: 0.00001835
Iteration 25/1000 | Loss: 0.00001834
Iteration 26/1000 | Loss: 0.00001834
Iteration 27/1000 | Loss: 0.00001833
Iteration 28/1000 | Loss: 0.00001833
Iteration 29/1000 | Loss: 0.00001833
Iteration 30/1000 | Loss: 0.00001833
Iteration 31/1000 | Loss: 0.00001832
Iteration 32/1000 | Loss: 0.00001832
Iteration 33/1000 | Loss: 0.00001832
Iteration 34/1000 | Loss: 0.00001831
Iteration 35/1000 | Loss: 0.00001831
Iteration 36/1000 | Loss: 0.00001830
Iteration 37/1000 | Loss: 0.00001830
Iteration 38/1000 | Loss: 0.00001830
Iteration 39/1000 | Loss: 0.00001829
Iteration 40/1000 | Loss: 0.00001829
Iteration 41/1000 | Loss: 0.00001829
Iteration 42/1000 | Loss: 0.00001828
Iteration 43/1000 | Loss: 0.00001828
Iteration 44/1000 | Loss: 0.00001828
Iteration 45/1000 | Loss: 0.00001827
Iteration 46/1000 | Loss: 0.00001827
Iteration 47/1000 | Loss: 0.00001827
Iteration 48/1000 | Loss: 0.00001827
Iteration 49/1000 | Loss: 0.00001827
Iteration 50/1000 | Loss: 0.00001826
Iteration 51/1000 | Loss: 0.00001826
Iteration 52/1000 | Loss: 0.00001826
Iteration 53/1000 | Loss: 0.00001826
Iteration 54/1000 | Loss: 0.00001826
Iteration 55/1000 | Loss: 0.00001826
Iteration 56/1000 | Loss: 0.00001826
Iteration 57/1000 | Loss: 0.00001825
Iteration 58/1000 | Loss: 0.00001825
Iteration 59/1000 | Loss: 0.00001825
Iteration 60/1000 | Loss: 0.00001825
Iteration 61/1000 | Loss: 0.00001825
Iteration 62/1000 | Loss: 0.00001825
Iteration 63/1000 | Loss: 0.00001825
Iteration 64/1000 | Loss: 0.00001825
Iteration 65/1000 | Loss: 0.00001825
Iteration 66/1000 | Loss: 0.00001825
Iteration 67/1000 | Loss: 0.00001825
Iteration 68/1000 | Loss: 0.00001825
Iteration 69/1000 | Loss: 0.00001824
Iteration 70/1000 | Loss: 0.00001824
Iteration 71/1000 | Loss: 0.00001824
Iteration 72/1000 | Loss: 0.00001824
Iteration 73/1000 | Loss: 0.00001823
Iteration 74/1000 | Loss: 0.00001823
Iteration 75/1000 | Loss: 0.00001823
Iteration 76/1000 | Loss: 0.00001823
Iteration 77/1000 | Loss: 0.00001823
Iteration 78/1000 | Loss: 0.00001823
Iteration 79/1000 | Loss: 0.00001823
Iteration 80/1000 | Loss: 0.00001823
Iteration 81/1000 | Loss: 0.00001823
Iteration 82/1000 | Loss: 0.00001823
Iteration 83/1000 | Loss: 0.00001823
Iteration 84/1000 | Loss: 0.00001823
Iteration 85/1000 | Loss: 0.00001823
Iteration 86/1000 | Loss: 0.00001823
Iteration 87/1000 | Loss: 0.00001823
Iteration 88/1000 | Loss: 0.00001823
Iteration 89/1000 | Loss: 0.00001822
Iteration 90/1000 | Loss: 0.00001822
Iteration 91/1000 | Loss: 0.00001822
Iteration 92/1000 | Loss: 0.00001822
Iteration 93/1000 | Loss: 0.00001822
Iteration 94/1000 | Loss: 0.00001822
Iteration 95/1000 | Loss: 0.00001822
Iteration 96/1000 | Loss: 0.00001822
Iteration 97/1000 | Loss: 0.00001822
Iteration 98/1000 | Loss: 0.00001822
Iteration 99/1000 | Loss: 0.00001822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.8224440282210708e-05, 1.8224440282210708e-05, 1.8224440282210708e-05, 1.8224440282210708e-05, 1.8224440282210708e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8224440282210708e-05

Optimization complete. Final v2v error: 3.6229841709136963 mm

Highest mean error: 4.328500270843506 mm for frame 239

Lowest mean error: 3.303633451461792 mm for frame 12

Saving results

Total time: 43.593695402145386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00932104
Iteration 2/25 | Loss: 0.00156802
Iteration 3/25 | Loss: 0.00110704
Iteration 4/25 | Loss: 0.00104365
Iteration 5/25 | Loss: 0.00101413
Iteration 6/25 | Loss: 0.00100645
Iteration 7/25 | Loss: 0.00100418
Iteration 8/25 | Loss: 0.00100311
Iteration 9/25 | Loss: 0.00100292
Iteration 10/25 | Loss: 0.00100292
Iteration 11/25 | Loss: 0.00100292
Iteration 12/25 | Loss: 0.00100292
Iteration 13/25 | Loss: 0.00100292
Iteration 14/25 | Loss: 0.00100292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010029218392446637, 0.0010029218392446637, 0.0010029218392446637, 0.0010029218392446637, 0.0010029218392446637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010029218392446637

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.17833114
Iteration 2/25 | Loss: 0.00041478
Iteration 3/25 | Loss: 0.00041478
Iteration 4/25 | Loss: 0.00041478
Iteration 5/25 | Loss: 0.00041478
Iteration 6/25 | Loss: 0.00041478
Iteration 7/25 | Loss: 0.00041478
Iteration 8/25 | Loss: 0.00041478
Iteration 9/25 | Loss: 0.00041478
Iteration 10/25 | Loss: 0.00041478
Iteration 11/25 | Loss: 0.00041478
Iteration 12/25 | Loss: 0.00041478
Iteration 13/25 | Loss: 0.00041478
Iteration 14/25 | Loss: 0.00041478
Iteration 15/25 | Loss: 0.00041478
Iteration 16/25 | Loss: 0.00041478
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004147817671764642, 0.0004147817671764642, 0.0004147817671764642, 0.0004147817671764642, 0.0004147817671764642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004147817671764642

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041478
Iteration 2/1000 | Loss: 0.00007661
Iteration 3/1000 | Loss: 0.00004439
Iteration 4/1000 | Loss: 0.00003592
Iteration 5/1000 | Loss: 0.00003297
Iteration 6/1000 | Loss: 0.00003162
Iteration 7/1000 | Loss: 0.00003068
Iteration 8/1000 | Loss: 0.00002997
Iteration 9/1000 | Loss: 0.00002941
Iteration 10/1000 | Loss: 0.00002906
Iteration 11/1000 | Loss: 0.00002873
Iteration 12/1000 | Loss: 0.00002845
Iteration 13/1000 | Loss: 0.00002825
Iteration 14/1000 | Loss: 0.00002823
Iteration 15/1000 | Loss: 0.00002810
Iteration 16/1000 | Loss: 0.00002809
Iteration 17/1000 | Loss: 0.00002807
Iteration 18/1000 | Loss: 0.00002798
Iteration 19/1000 | Loss: 0.00002798
Iteration 20/1000 | Loss: 0.00002798
Iteration 21/1000 | Loss: 0.00002795
Iteration 22/1000 | Loss: 0.00002795
Iteration 23/1000 | Loss: 0.00002794
Iteration 24/1000 | Loss: 0.00002792
Iteration 25/1000 | Loss: 0.00002792
Iteration 26/1000 | Loss: 0.00002791
Iteration 27/1000 | Loss: 0.00002790
Iteration 28/1000 | Loss: 0.00002790
Iteration 29/1000 | Loss: 0.00002786
Iteration 30/1000 | Loss: 0.00002785
Iteration 31/1000 | Loss: 0.00002785
Iteration 32/1000 | Loss: 0.00002785
Iteration 33/1000 | Loss: 0.00002785
Iteration 34/1000 | Loss: 0.00002785
Iteration 35/1000 | Loss: 0.00002784
Iteration 36/1000 | Loss: 0.00002784
Iteration 37/1000 | Loss: 0.00002784
Iteration 38/1000 | Loss: 0.00002784
Iteration 39/1000 | Loss: 0.00002784
Iteration 40/1000 | Loss: 0.00002784
Iteration 41/1000 | Loss: 0.00002783
Iteration 42/1000 | Loss: 0.00002783
Iteration 43/1000 | Loss: 0.00002782
Iteration 44/1000 | Loss: 0.00002782
Iteration 45/1000 | Loss: 0.00002782
Iteration 46/1000 | Loss: 0.00002782
Iteration 47/1000 | Loss: 0.00002782
Iteration 48/1000 | Loss: 0.00002782
Iteration 49/1000 | Loss: 0.00002781
Iteration 50/1000 | Loss: 0.00002781
Iteration 51/1000 | Loss: 0.00002781
Iteration 52/1000 | Loss: 0.00002781
Iteration 53/1000 | Loss: 0.00002781
Iteration 54/1000 | Loss: 0.00002781
Iteration 55/1000 | Loss: 0.00002781
Iteration 56/1000 | Loss: 0.00002780
Iteration 57/1000 | Loss: 0.00002780
Iteration 58/1000 | Loss: 0.00002780
Iteration 59/1000 | Loss: 0.00002780
Iteration 60/1000 | Loss: 0.00002780
Iteration 61/1000 | Loss: 0.00002780
Iteration 62/1000 | Loss: 0.00002779
Iteration 63/1000 | Loss: 0.00002779
Iteration 64/1000 | Loss: 0.00002779
Iteration 65/1000 | Loss: 0.00002779
Iteration 66/1000 | Loss: 0.00002778
Iteration 67/1000 | Loss: 0.00002778
Iteration 68/1000 | Loss: 0.00002778
Iteration 69/1000 | Loss: 0.00002778
Iteration 70/1000 | Loss: 0.00002778
Iteration 71/1000 | Loss: 0.00002778
Iteration 72/1000 | Loss: 0.00002778
Iteration 73/1000 | Loss: 0.00002778
Iteration 74/1000 | Loss: 0.00002777
Iteration 75/1000 | Loss: 0.00002777
Iteration 76/1000 | Loss: 0.00002777
Iteration 77/1000 | Loss: 0.00002777
Iteration 78/1000 | Loss: 0.00002777
Iteration 79/1000 | Loss: 0.00002777
Iteration 80/1000 | Loss: 0.00002777
Iteration 81/1000 | Loss: 0.00002777
Iteration 82/1000 | Loss: 0.00002777
Iteration 83/1000 | Loss: 0.00002776
Iteration 84/1000 | Loss: 0.00002776
Iteration 85/1000 | Loss: 0.00002776
Iteration 86/1000 | Loss: 0.00002776
Iteration 87/1000 | Loss: 0.00002776
Iteration 88/1000 | Loss: 0.00002776
Iteration 89/1000 | Loss: 0.00002775
Iteration 90/1000 | Loss: 0.00002775
Iteration 91/1000 | Loss: 0.00002775
Iteration 92/1000 | Loss: 0.00002775
Iteration 93/1000 | Loss: 0.00002775
Iteration 94/1000 | Loss: 0.00002775
Iteration 95/1000 | Loss: 0.00002775
Iteration 96/1000 | Loss: 0.00002774
Iteration 97/1000 | Loss: 0.00002774
Iteration 98/1000 | Loss: 0.00002774
Iteration 99/1000 | Loss: 0.00002774
Iteration 100/1000 | Loss: 0.00002774
Iteration 101/1000 | Loss: 0.00002774
Iteration 102/1000 | Loss: 0.00002774
Iteration 103/1000 | Loss: 0.00002774
Iteration 104/1000 | Loss: 0.00002774
Iteration 105/1000 | Loss: 0.00002774
Iteration 106/1000 | Loss: 0.00002774
Iteration 107/1000 | Loss: 0.00002773
Iteration 108/1000 | Loss: 0.00002773
Iteration 109/1000 | Loss: 0.00002773
Iteration 110/1000 | Loss: 0.00002773
Iteration 111/1000 | Loss: 0.00002773
Iteration 112/1000 | Loss: 0.00002773
Iteration 113/1000 | Loss: 0.00002773
Iteration 114/1000 | Loss: 0.00002773
Iteration 115/1000 | Loss: 0.00002773
Iteration 116/1000 | Loss: 0.00002773
Iteration 117/1000 | Loss: 0.00002772
Iteration 118/1000 | Loss: 0.00002772
Iteration 119/1000 | Loss: 0.00002772
Iteration 120/1000 | Loss: 0.00002772
Iteration 121/1000 | Loss: 0.00002772
Iteration 122/1000 | Loss: 0.00002772
Iteration 123/1000 | Loss: 0.00002772
Iteration 124/1000 | Loss: 0.00002772
Iteration 125/1000 | Loss: 0.00002772
Iteration 126/1000 | Loss: 0.00002772
Iteration 127/1000 | Loss: 0.00002772
Iteration 128/1000 | Loss: 0.00002772
Iteration 129/1000 | Loss: 0.00002772
Iteration 130/1000 | Loss: 0.00002771
Iteration 131/1000 | Loss: 0.00002771
Iteration 132/1000 | Loss: 0.00002771
Iteration 133/1000 | Loss: 0.00002771
Iteration 134/1000 | Loss: 0.00002771
Iteration 135/1000 | Loss: 0.00002771
Iteration 136/1000 | Loss: 0.00002771
Iteration 137/1000 | Loss: 0.00002770
Iteration 138/1000 | Loss: 0.00002770
Iteration 139/1000 | Loss: 0.00002770
Iteration 140/1000 | Loss: 0.00002770
Iteration 141/1000 | Loss: 0.00002770
Iteration 142/1000 | Loss: 0.00002770
Iteration 143/1000 | Loss: 0.00002770
Iteration 144/1000 | Loss: 0.00002770
Iteration 145/1000 | Loss: 0.00002770
Iteration 146/1000 | Loss: 0.00002770
Iteration 147/1000 | Loss: 0.00002770
Iteration 148/1000 | Loss: 0.00002770
Iteration 149/1000 | Loss: 0.00002770
Iteration 150/1000 | Loss: 0.00002770
Iteration 151/1000 | Loss: 0.00002770
Iteration 152/1000 | Loss: 0.00002770
Iteration 153/1000 | Loss: 0.00002770
Iteration 154/1000 | Loss: 0.00002770
Iteration 155/1000 | Loss: 0.00002770
Iteration 156/1000 | Loss: 0.00002770
Iteration 157/1000 | Loss: 0.00002770
Iteration 158/1000 | Loss: 0.00002770
Iteration 159/1000 | Loss: 0.00002770
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.7702517400030047e-05, 2.7702517400030047e-05, 2.7702517400030047e-05, 2.7702517400030047e-05, 2.7702517400030047e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7702517400030047e-05

Optimization complete. Final v2v error: 4.469876766204834 mm

Highest mean error: 6.599026203155518 mm for frame 91

Lowest mean error: 3.466146230697632 mm for frame 10

Saving results

Total time: 42.32309365272522
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00537898
Iteration 2/25 | Loss: 0.00117823
Iteration 3/25 | Loss: 0.00097391
Iteration 4/25 | Loss: 0.00094294
Iteration 5/25 | Loss: 0.00093217
Iteration 6/25 | Loss: 0.00092881
Iteration 7/25 | Loss: 0.00092789
Iteration 8/25 | Loss: 0.00092789
Iteration 9/25 | Loss: 0.00092789
Iteration 10/25 | Loss: 0.00092789
Iteration 11/25 | Loss: 0.00092789
Iteration 12/25 | Loss: 0.00092789
Iteration 13/25 | Loss: 0.00092789
Iteration 14/25 | Loss: 0.00092789
Iteration 15/25 | Loss: 0.00092789
Iteration 16/25 | Loss: 0.00092789
Iteration 17/25 | Loss: 0.00092789
Iteration 18/25 | Loss: 0.00092789
Iteration 19/25 | Loss: 0.00092789
Iteration 20/25 | Loss: 0.00092789
Iteration 21/25 | Loss: 0.00092789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009278897196054459, 0.0009278897196054459, 0.0009278897196054459, 0.0009278897196054459, 0.0009278897196054459]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009278897196054459

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.94040728
Iteration 2/25 | Loss: 0.00036050
Iteration 3/25 | Loss: 0.00036050
Iteration 4/25 | Loss: 0.00036050
Iteration 5/25 | Loss: 0.00036049
Iteration 6/25 | Loss: 0.00036049
Iteration 7/25 | Loss: 0.00036049
Iteration 8/25 | Loss: 0.00036049
Iteration 9/25 | Loss: 0.00036049
Iteration 10/25 | Loss: 0.00036049
Iteration 11/25 | Loss: 0.00036049
Iteration 12/25 | Loss: 0.00036049
Iteration 13/25 | Loss: 0.00036049
Iteration 14/25 | Loss: 0.00036049
Iteration 15/25 | Loss: 0.00036049
Iteration 16/25 | Loss: 0.00036049
Iteration 17/25 | Loss: 0.00036049
Iteration 18/25 | Loss: 0.00036049
Iteration 19/25 | Loss: 0.00036049
Iteration 20/25 | Loss: 0.00036049
Iteration 21/25 | Loss: 0.00036049
Iteration 22/25 | Loss: 0.00036049
Iteration 23/25 | Loss: 0.00036049
Iteration 24/25 | Loss: 0.00036049
Iteration 25/25 | Loss: 0.00036049
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00036049398477189243, 0.00036049398477189243, 0.00036049398477189243, 0.00036049398477189243, 0.00036049398477189243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00036049398477189243

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036049
Iteration 2/1000 | Loss: 0.00003881
Iteration 3/1000 | Loss: 0.00002646
Iteration 4/1000 | Loss: 0.00002381
Iteration 5/1000 | Loss: 0.00002291
Iteration 6/1000 | Loss: 0.00002230
Iteration 7/1000 | Loss: 0.00002173
Iteration 8/1000 | Loss: 0.00002142
Iteration 9/1000 | Loss: 0.00002115
Iteration 10/1000 | Loss: 0.00002102
Iteration 11/1000 | Loss: 0.00002099
Iteration 12/1000 | Loss: 0.00002092
Iteration 13/1000 | Loss: 0.00002090
Iteration 14/1000 | Loss: 0.00002083
Iteration 15/1000 | Loss: 0.00002082
Iteration 16/1000 | Loss: 0.00002080
Iteration 17/1000 | Loss: 0.00002080
Iteration 18/1000 | Loss: 0.00002080
Iteration 19/1000 | Loss: 0.00002080
Iteration 20/1000 | Loss: 0.00002077
Iteration 21/1000 | Loss: 0.00002076
Iteration 22/1000 | Loss: 0.00002075
Iteration 23/1000 | Loss: 0.00002075
Iteration 24/1000 | Loss: 0.00002075
Iteration 25/1000 | Loss: 0.00002073
Iteration 26/1000 | Loss: 0.00002072
Iteration 27/1000 | Loss: 0.00002072
Iteration 28/1000 | Loss: 0.00002071
Iteration 29/1000 | Loss: 0.00002071
Iteration 30/1000 | Loss: 0.00002070
Iteration 31/1000 | Loss: 0.00002070
Iteration 32/1000 | Loss: 0.00002070
Iteration 33/1000 | Loss: 0.00002069
Iteration 34/1000 | Loss: 0.00002069
Iteration 35/1000 | Loss: 0.00002069
Iteration 36/1000 | Loss: 0.00002069
Iteration 37/1000 | Loss: 0.00002069
Iteration 38/1000 | Loss: 0.00002068
Iteration 39/1000 | Loss: 0.00002068
Iteration 40/1000 | Loss: 0.00002068
Iteration 41/1000 | Loss: 0.00002067
Iteration 42/1000 | Loss: 0.00002067
Iteration 43/1000 | Loss: 0.00002067
Iteration 44/1000 | Loss: 0.00002066
Iteration 45/1000 | Loss: 0.00002066
Iteration 46/1000 | Loss: 0.00002066
Iteration 47/1000 | Loss: 0.00002066
Iteration 48/1000 | Loss: 0.00002066
Iteration 49/1000 | Loss: 0.00002066
Iteration 50/1000 | Loss: 0.00002065
Iteration 51/1000 | Loss: 0.00002065
Iteration 52/1000 | Loss: 0.00002065
Iteration 53/1000 | Loss: 0.00002065
Iteration 54/1000 | Loss: 0.00002065
Iteration 55/1000 | Loss: 0.00002065
Iteration 56/1000 | Loss: 0.00002064
Iteration 57/1000 | Loss: 0.00002064
Iteration 58/1000 | Loss: 0.00002064
Iteration 59/1000 | Loss: 0.00002064
Iteration 60/1000 | Loss: 0.00002064
Iteration 61/1000 | Loss: 0.00002063
Iteration 62/1000 | Loss: 0.00002063
Iteration 63/1000 | Loss: 0.00002063
Iteration 64/1000 | Loss: 0.00002063
Iteration 65/1000 | Loss: 0.00002062
Iteration 66/1000 | Loss: 0.00002062
Iteration 67/1000 | Loss: 0.00002062
Iteration 68/1000 | Loss: 0.00002062
Iteration 69/1000 | Loss: 0.00002062
Iteration 70/1000 | Loss: 0.00002062
Iteration 71/1000 | Loss: 0.00002062
Iteration 72/1000 | Loss: 0.00002061
Iteration 73/1000 | Loss: 0.00002061
Iteration 74/1000 | Loss: 0.00002061
Iteration 75/1000 | Loss: 0.00002061
Iteration 76/1000 | Loss: 0.00002061
Iteration 77/1000 | Loss: 0.00002061
Iteration 78/1000 | Loss: 0.00002061
Iteration 79/1000 | Loss: 0.00002060
Iteration 80/1000 | Loss: 0.00002060
Iteration 81/1000 | Loss: 0.00002060
Iteration 82/1000 | Loss: 0.00002060
Iteration 83/1000 | Loss: 0.00002060
Iteration 84/1000 | Loss: 0.00002060
Iteration 85/1000 | Loss: 0.00002060
Iteration 86/1000 | Loss: 0.00002060
Iteration 87/1000 | Loss: 0.00002060
Iteration 88/1000 | Loss: 0.00002060
Iteration 89/1000 | Loss: 0.00002060
Iteration 90/1000 | Loss: 0.00002060
Iteration 91/1000 | Loss: 0.00002060
Iteration 92/1000 | Loss: 0.00002060
Iteration 93/1000 | Loss: 0.00002060
Iteration 94/1000 | Loss: 0.00002060
Iteration 95/1000 | Loss: 0.00002060
Iteration 96/1000 | Loss: 0.00002060
Iteration 97/1000 | Loss: 0.00002060
Iteration 98/1000 | Loss: 0.00002060
Iteration 99/1000 | Loss: 0.00002060
Iteration 100/1000 | Loss: 0.00002060
Iteration 101/1000 | Loss: 0.00002060
Iteration 102/1000 | Loss: 0.00002060
Iteration 103/1000 | Loss: 0.00002060
Iteration 104/1000 | Loss: 0.00002060
Iteration 105/1000 | Loss: 0.00002060
Iteration 106/1000 | Loss: 0.00002060
Iteration 107/1000 | Loss: 0.00002060
Iteration 108/1000 | Loss: 0.00002060
Iteration 109/1000 | Loss: 0.00002060
Iteration 110/1000 | Loss: 0.00002060
Iteration 111/1000 | Loss: 0.00002060
Iteration 112/1000 | Loss: 0.00002060
Iteration 113/1000 | Loss: 0.00002060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.0598337869159877e-05, 2.0598337869159877e-05, 2.0598337869159877e-05, 2.0598337869159877e-05, 2.0598337869159877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0598337869159877e-05

Optimization complete. Final v2v error: 3.8402228355407715 mm

Highest mean error: 4.890644550323486 mm for frame 61

Lowest mean error: 3.2682597637176514 mm for frame 12

Saving results

Total time: 38.614224910736084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1670/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1670/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904860
Iteration 2/25 | Loss: 0.00145430
Iteration 3/25 | Loss: 0.00114891
Iteration 4/25 | Loss: 0.00110358
Iteration 5/25 | Loss: 0.00109419
Iteration 6/25 | Loss: 0.00109240
Iteration 7/25 | Loss: 0.00109191
Iteration 8/25 | Loss: 0.00109191
Iteration 9/25 | Loss: 0.00109191
Iteration 10/25 | Loss: 0.00109191
Iteration 11/25 | Loss: 0.00109191
Iteration 12/25 | Loss: 0.00109191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010919091291725636, 0.0010919091291725636, 0.0010919091291725636, 0.0010919091291725636, 0.0010919091291725636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010919091291725636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94961083
Iteration 2/25 | Loss: 0.00056398
Iteration 3/25 | Loss: 0.00056398
Iteration 4/25 | Loss: 0.00056398
Iteration 5/25 | Loss: 0.00056398
Iteration 6/25 | Loss: 0.00056398
Iteration 7/25 | Loss: 0.00056398
Iteration 8/25 | Loss: 0.00056398
Iteration 9/25 | Loss: 0.00056398
Iteration 10/25 | Loss: 0.00056398
Iteration 11/25 | Loss: 0.00056398
Iteration 12/25 | Loss: 0.00056398
Iteration 13/25 | Loss: 0.00056398
Iteration 14/25 | Loss: 0.00056398
Iteration 15/25 | Loss: 0.00056398
Iteration 16/25 | Loss: 0.00056398
Iteration 17/25 | Loss: 0.00056398
Iteration 18/25 | Loss: 0.00056398
Iteration 19/25 | Loss: 0.00056398
Iteration 20/25 | Loss: 0.00056398
Iteration 21/25 | Loss: 0.00056398
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005639753071591258, 0.0005639753071591258, 0.0005639753071591258, 0.0005639753071591258, 0.0005639753071591258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005639753071591258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056398
Iteration 2/1000 | Loss: 0.00006165
Iteration 3/1000 | Loss: 0.00004562
Iteration 4/1000 | Loss: 0.00004238
Iteration 5/1000 | Loss: 0.00004001
Iteration 6/1000 | Loss: 0.00003894
Iteration 7/1000 | Loss: 0.00003794
Iteration 8/1000 | Loss: 0.00003736
Iteration 9/1000 | Loss: 0.00003688
Iteration 10/1000 | Loss: 0.00003659
Iteration 11/1000 | Loss: 0.00003656
Iteration 12/1000 | Loss: 0.00003649
Iteration 13/1000 | Loss: 0.00003639
Iteration 14/1000 | Loss: 0.00003639
Iteration 15/1000 | Loss: 0.00003639
Iteration 16/1000 | Loss: 0.00003639
Iteration 17/1000 | Loss: 0.00003639
Iteration 18/1000 | Loss: 0.00003639
Iteration 19/1000 | Loss: 0.00003627
Iteration 20/1000 | Loss: 0.00003626
Iteration 21/1000 | Loss: 0.00003620
Iteration 22/1000 | Loss: 0.00003620
Iteration 23/1000 | Loss: 0.00003619
Iteration 24/1000 | Loss: 0.00003618
Iteration 25/1000 | Loss: 0.00003615
Iteration 26/1000 | Loss: 0.00003615
Iteration 27/1000 | Loss: 0.00003615
Iteration 28/1000 | Loss: 0.00003613
Iteration 29/1000 | Loss: 0.00003613
Iteration 30/1000 | Loss: 0.00003612
Iteration 31/1000 | Loss: 0.00003612
Iteration 32/1000 | Loss: 0.00003611
Iteration 33/1000 | Loss: 0.00003609
Iteration 34/1000 | Loss: 0.00003609
Iteration 35/1000 | Loss: 0.00003609
Iteration 36/1000 | Loss: 0.00003609
Iteration 37/1000 | Loss: 0.00003609
Iteration 38/1000 | Loss: 0.00003609
Iteration 39/1000 | Loss: 0.00003609
Iteration 40/1000 | Loss: 0.00003609
Iteration 41/1000 | Loss: 0.00003609
Iteration 42/1000 | Loss: 0.00003609
Iteration 43/1000 | Loss: 0.00003609
Iteration 44/1000 | Loss: 0.00003609
Iteration 45/1000 | Loss: 0.00003608
Iteration 46/1000 | Loss: 0.00003608
Iteration 47/1000 | Loss: 0.00003608
Iteration 48/1000 | Loss: 0.00003608
Iteration 49/1000 | Loss: 0.00003605
Iteration 50/1000 | Loss: 0.00003605
Iteration 51/1000 | Loss: 0.00003604
Iteration 52/1000 | Loss: 0.00003602
Iteration 53/1000 | Loss: 0.00003601
Iteration 54/1000 | Loss: 0.00003598
Iteration 55/1000 | Loss: 0.00003598
Iteration 56/1000 | Loss: 0.00003597
Iteration 57/1000 | Loss: 0.00003597
Iteration 58/1000 | Loss: 0.00003596
Iteration 59/1000 | Loss: 0.00003596
Iteration 60/1000 | Loss: 0.00003596
Iteration 61/1000 | Loss: 0.00003596
Iteration 62/1000 | Loss: 0.00003596
Iteration 63/1000 | Loss: 0.00003596
Iteration 64/1000 | Loss: 0.00003596
Iteration 65/1000 | Loss: 0.00003595
Iteration 66/1000 | Loss: 0.00003595
Iteration 67/1000 | Loss: 0.00003595
Iteration 68/1000 | Loss: 0.00003595
Iteration 69/1000 | Loss: 0.00003595
Iteration 70/1000 | Loss: 0.00003595
Iteration 71/1000 | Loss: 0.00003594
Iteration 72/1000 | Loss: 0.00003594
Iteration 73/1000 | Loss: 0.00003594
Iteration 74/1000 | Loss: 0.00003594
Iteration 75/1000 | Loss: 0.00003594
Iteration 76/1000 | Loss: 0.00003594
Iteration 77/1000 | Loss: 0.00003594
Iteration 78/1000 | Loss: 0.00003594
Iteration 79/1000 | Loss: 0.00003594
Iteration 80/1000 | Loss: 0.00003594
Iteration 81/1000 | Loss: 0.00003594
Iteration 82/1000 | Loss: 0.00003594
Iteration 83/1000 | Loss: 0.00003594
Iteration 84/1000 | Loss: 0.00003594
Iteration 85/1000 | Loss: 0.00003594
Iteration 86/1000 | Loss: 0.00003594
Iteration 87/1000 | Loss: 0.00003594
Iteration 88/1000 | Loss: 0.00003593
Iteration 89/1000 | Loss: 0.00003593
Iteration 90/1000 | Loss: 0.00003593
Iteration 91/1000 | Loss: 0.00003593
Iteration 92/1000 | Loss: 0.00003593
Iteration 93/1000 | Loss: 0.00003593
Iteration 94/1000 | Loss: 0.00003593
Iteration 95/1000 | Loss: 0.00003593
Iteration 96/1000 | Loss: 0.00003593
Iteration 97/1000 | Loss: 0.00003592
Iteration 98/1000 | Loss: 0.00003592
Iteration 99/1000 | Loss: 0.00003592
Iteration 100/1000 | Loss: 0.00003591
Iteration 101/1000 | Loss: 0.00003591
Iteration 102/1000 | Loss: 0.00003591
Iteration 103/1000 | Loss: 0.00003591
Iteration 104/1000 | Loss: 0.00003591
Iteration 105/1000 | Loss: 0.00003591
Iteration 106/1000 | Loss: 0.00003591
Iteration 107/1000 | Loss: 0.00003591
Iteration 108/1000 | Loss: 0.00003591
Iteration 109/1000 | Loss: 0.00003591
Iteration 110/1000 | Loss: 0.00003591
Iteration 111/1000 | Loss: 0.00003591
Iteration 112/1000 | Loss: 0.00003590
Iteration 113/1000 | Loss: 0.00003590
Iteration 114/1000 | Loss: 0.00003590
Iteration 115/1000 | Loss: 0.00003590
Iteration 116/1000 | Loss: 0.00003590
Iteration 117/1000 | Loss: 0.00003590
Iteration 118/1000 | Loss: 0.00003590
Iteration 119/1000 | Loss: 0.00003590
Iteration 120/1000 | Loss: 0.00003590
Iteration 121/1000 | Loss: 0.00003590
Iteration 122/1000 | Loss: 0.00003590
Iteration 123/1000 | Loss: 0.00003590
Iteration 124/1000 | Loss: 0.00003590
Iteration 125/1000 | Loss: 0.00003590
Iteration 126/1000 | Loss: 0.00003590
Iteration 127/1000 | Loss: 0.00003590
Iteration 128/1000 | Loss: 0.00003590
Iteration 129/1000 | Loss: 0.00003590
Iteration 130/1000 | Loss: 0.00003590
Iteration 131/1000 | Loss: 0.00003589
Iteration 132/1000 | Loss: 0.00003589
Iteration 133/1000 | Loss: 0.00003589
Iteration 134/1000 | Loss: 0.00003589
Iteration 135/1000 | Loss: 0.00003589
Iteration 136/1000 | Loss: 0.00003589
Iteration 137/1000 | Loss: 0.00003589
Iteration 138/1000 | Loss: 0.00003589
Iteration 139/1000 | Loss: 0.00003589
Iteration 140/1000 | Loss: 0.00003589
Iteration 141/1000 | Loss: 0.00003589
Iteration 142/1000 | Loss: 0.00003589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [3.5894645407097414e-05, 3.5894645407097414e-05, 3.5894645407097414e-05, 3.5894645407097414e-05, 3.5894645407097414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5894645407097414e-05

Optimization complete. Final v2v error: 5.097165107727051 mm

Highest mean error: 5.152492523193359 mm for frame 50

Lowest mean error: 5.04707670211792 mm for frame 142

Saving results

Total time: 35.79610800743103
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00610839
Iteration 2/25 | Loss: 0.00111222
Iteration 3/25 | Loss: 0.00104334
Iteration 4/25 | Loss: 0.00103192
Iteration 5/25 | Loss: 0.00102818
Iteration 6/25 | Loss: 0.00102767
Iteration 7/25 | Loss: 0.00102767
Iteration 8/25 | Loss: 0.00102767
Iteration 9/25 | Loss: 0.00102767
Iteration 10/25 | Loss: 0.00102767
Iteration 11/25 | Loss: 0.00102767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010276706889271736, 0.0010276706889271736, 0.0010276706889271736, 0.0010276706889271736, 0.0010276706889271736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010276706889271736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.66349888
Iteration 2/25 | Loss: 0.00063898
Iteration 3/25 | Loss: 0.00063898
Iteration 4/25 | Loss: 0.00063898
Iteration 5/25 | Loss: 0.00063898
Iteration 6/25 | Loss: 0.00063898
Iteration 7/25 | Loss: 0.00063898
Iteration 8/25 | Loss: 0.00063898
Iteration 9/25 | Loss: 0.00063898
Iteration 10/25 | Loss: 0.00063898
Iteration 11/25 | Loss: 0.00063898
Iteration 12/25 | Loss: 0.00063898
Iteration 13/25 | Loss: 0.00063898
Iteration 14/25 | Loss: 0.00063898
Iteration 15/25 | Loss: 0.00063898
Iteration 16/25 | Loss: 0.00063898
Iteration 17/25 | Loss: 0.00063898
Iteration 18/25 | Loss: 0.00063898
Iteration 19/25 | Loss: 0.00063898
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006389797781594098, 0.0006389797781594098, 0.0006389797781594098, 0.0006389797781594098, 0.0006389797781594098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006389797781594098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063898
Iteration 2/1000 | Loss: 0.00002241
Iteration 3/1000 | Loss: 0.00001571
Iteration 4/1000 | Loss: 0.00001347
Iteration 5/1000 | Loss: 0.00001273
Iteration 6/1000 | Loss: 0.00001204
Iteration 7/1000 | Loss: 0.00001159
Iteration 8/1000 | Loss: 0.00001134
Iteration 9/1000 | Loss: 0.00001111
Iteration 10/1000 | Loss: 0.00001084
Iteration 11/1000 | Loss: 0.00001077
Iteration 12/1000 | Loss: 0.00001070
Iteration 13/1000 | Loss: 0.00001065
Iteration 14/1000 | Loss: 0.00001060
Iteration 15/1000 | Loss: 0.00001058
Iteration 16/1000 | Loss: 0.00001052
Iteration 17/1000 | Loss: 0.00001051
Iteration 18/1000 | Loss: 0.00001051
Iteration 19/1000 | Loss: 0.00001050
Iteration 20/1000 | Loss: 0.00001050
Iteration 21/1000 | Loss: 0.00001050
Iteration 22/1000 | Loss: 0.00001044
Iteration 23/1000 | Loss: 0.00001043
Iteration 24/1000 | Loss: 0.00001043
Iteration 25/1000 | Loss: 0.00001042
Iteration 26/1000 | Loss: 0.00001042
Iteration 27/1000 | Loss: 0.00001042
Iteration 28/1000 | Loss: 0.00001042
Iteration 29/1000 | Loss: 0.00001039
Iteration 30/1000 | Loss: 0.00001038
Iteration 31/1000 | Loss: 0.00001038
Iteration 32/1000 | Loss: 0.00001038
Iteration 33/1000 | Loss: 0.00001038
Iteration 34/1000 | Loss: 0.00001038
Iteration 35/1000 | Loss: 0.00001037
Iteration 36/1000 | Loss: 0.00001037
Iteration 37/1000 | Loss: 0.00001036
Iteration 38/1000 | Loss: 0.00001036
Iteration 39/1000 | Loss: 0.00001033
Iteration 40/1000 | Loss: 0.00001032
Iteration 41/1000 | Loss: 0.00001031
Iteration 42/1000 | Loss: 0.00001031
Iteration 43/1000 | Loss: 0.00001030
Iteration 44/1000 | Loss: 0.00001030
Iteration 45/1000 | Loss: 0.00001029
Iteration 46/1000 | Loss: 0.00001027
Iteration 47/1000 | Loss: 0.00001027
Iteration 48/1000 | Loss: 0.00001027
Iteration 49/1000 | Loss: 0.00001020
Iteration 50/1000 | Loss: 0.00001020
Iteration 51/1000 | Loss: 0.00001013
Iteration 52/1000 | Loss: 0.00001012
Iteration 53/1000 | Loss: 0.00001012
Iteration 54/1000 | Loss: 0.00001011
Iteration 55/1000 | Loss: 0.00001010
Iteration 56/1000 | Loss: 0.00001010
Iteration 57/1000 | Loss: 0.00001010
Iteration 58/1000 | Loss: 0.00001010
Iteration 59/1000 | Loss: 0.00001010
Iteration 60/1000 | Loss: 0.00001010
Iteration 61/1000 | Loss: 0.00001010
Iteration 62/1000 | Loss: 0.00001009
Iteration 63/1000 | Loss: 0.00001009
Iteration 64/1000 | Loss: 0.00001008
Iteration 65/1000 | Loss: 0.00001008
Iteration 66/1000 | Loss: 0.00001007
Iteration 67/1000 | Loss: 0.00001007
Iteration 68/1000 | Loss: 0.00001007
Iteration 69/1000 | Loss: 0.00001007
Iteration 70/1000 | Loss: 0.00001007
Iteration 71/1000 | Loss: 0.00001006
Iteration 72/1000 | Loss: 0.00001006
Iteration 73/1000 | Loss: 0.00001006
Iteration 74/1000 | Loss: 0.00001006
Iteration 75/1000 | Loss: 0.00001005
Iteration 76/1000 | Loss: 0.00001004
Iteration 77/1000 | Loss: 0.00001004
Iteration 78/1000 | Loss: 0.00001004
Iteration 79/1000 | Loss: 0.00001004
Iteration 80/1000 | Loss: 0.00001004
Iteration 81/1000 | Loss: 0.00001004
Iteration 82/1000 | Loss: 0.00001004
Iteration 83/1000 | Loss: 0.00001004
Iteration 84/1000 | Loss: 0.00001004
Iteration 85/1000 | Loss: 0.00001003
Iteration 86/1000 | Loss: 0.00001003
Iteration 87/1000 | Loss: 0.00001003
Iteration 88/1000 | Loss: 0.00001003
Iteration 89/1000 | Loss: 0.00001003
Iteration 90/1000 | Loss: 0.00001003
Iteration 91/1000 | Loss: 0.00001003
Iteration 92/1000 | Loss: 0.00001002
Iteration 93/1000 | Loss: 0.00001002
Iteration 94/1000 | Loss: 0.00001002
Iteration 95/1000 | Loss: 0.00001002
Iteration 96/1000 | Loss: 0.00001001
Iteration 97/1000 | Loss: 0.00001001
Iteration 98/1000 | Loss: 0.00001000
Iteration 99/1000 | Loss: 0.00001000
Iteration 100/1000 | Loss: 0.00001000
Iteration 101/1000 | Loss: 0.00001000
Iteration 102/1000 | Loss: 0.00000999
Iteration 103/1000 | Loss: 0.00000999
Iteration 104/1000 | Loss: 0.00000999
Iteration 105/1000 | Loss: 0.00000998
Iteration 106/1000 | Loss: 0.00000998
Iteration 107/1000 | Loss: 0.00000997
Iteration 108/1000 | Loss: 0.00000997
Iteration 109/1000 | Loss: 0.00000997
Iteration 110/1000 | Loss: 0.00000997
Iteration 111/1000 | Loss: 0.00000996
Iteration 112/1000 | Loss: 0.00000996
Iteration 113/1000 | Loss: 0.00000995
Iteration 114/1000 | Loss: 0.00000995
Iteration 115/1000 | Loss: 0.00000995
Iteration 116/1000 | Loss: 0.00000995
Iteration 117/1000 | Loss: 0.00000995
Iteration 118/1000 | Loss: 0.00000994
Iteration 119/1000 | Loss: 0.00000994
Iteration 120/1000 | Loss: 0.00000994
Iteration 121/1000 | Loss: 0.00000994
Iteration 122/1000 | Loss: 0.00000994
Iteration 123/1000 | Loss: 0.00000994
Iteration 124/1000 | Loss: 0.00000994
Iteration 125/1000 | Loss: 0.00000994
Iteration 126/1000 | Loss: 0.00000994
Iteration 127/1000 | Loss: 0.00000994
Iteration 128/1000 | Loss: 0.00000994
Iteration 129/1000 | Loss: 0.00000993
Iteration 130/1000 | Loss: 0.00000993
Iteration 131/1000 | Loss: 0.00000993
Iteration 132/1000 | Loss: 0.00000993
Iteration 133/1000 | Loss: 0.00000993
Iteration 134/1000 | Loss: 0.00000993
Iteration 135/1000 | Loss: 0.00000993
Iteration 136/1000 | Loss: 0.00000993
Iteration 137/1000 | Loss: 0.00000993
Iteration 138/1000 | Loss: 0.00000993
Iteration 139/1000 | Loss: 0.00000993
Iteration 140/1000 | Loss: 0.00000992
Iteration 141/1000 | Loss: 0.00000992
Iteration 142/1000 | Loss: 0.00000992
Iteration 143/1000 | Loss: 0.00000992
Iteration 144/1000 | Loss: 0.00000992
Iteration 145/1000 | Loss: 0.00000992
Iteration 146/1000 | Loss: 0.00000992
Iteration 147/1000 | Loss: 0.00000992
Iteration 148/1000 | Loss: 0.00000992
Iteration 149/1000 | Loss: 0.00000992
Iteration 150/1000 | Loss: 0.00000992
Iteration 151/1000 | Loss: 0.00000992
Iteration 152/1000 | Loss: 0.00000992
Iteration 153/1000 | Loss: 0.00000992
Iteration 154/1000 | Loss: 0.00000992
Iteration 155/1000 | Loss: 0.00000992
Iteration 156/1000 | Loss: 0.00000992
Iteration 157/1000 | Loss: 0.00000992
Iteration 158/1000 | Loss: 0.00000992
Iteration 159/1000 | Loss: 0.00000992
Iteration 160/1000 | Loss: 0.00000991
Iteration 161/1000 | Loss: 0.00000991
Iteration 162/1000 | Loss: 0.00000991
Iteration 163/1000 | Loss: 0.00000991
Iteration 164/1000 | Loss: 0.00000991
Iteration 165/1000 | Loss: 0.00000991
Iteration 166/1000 | Loss: 0.00000991
Iteration 167/1000 | Loss: 0.00000990
Iteration 168/1000 | Loss: 0.00000990
Iteration 169/1000 | Loss: 0.00000990
Iteration 170/1000 | Loss: 0.00000990
Iteration 171/1000 | Loss: 0.00000990
Iteration 172/1000 | Loss: 0.00000990
Iteration 173/1000 | Loss: 0.00000990
Iteration 174/1000 | Loss: 0.00000990
Iteration 175/1000 | Loss: 0.00000990
Iteration 176/1000 | Loss: 0.00000990
Iteration 177/1000 | Loss: 0.00000990
Iteration 178/1000 | Loss: 0.00000990
Iteration 179/1000 | Loss: 0.00000990
Iteration 180/1000 | Loss: 0.00000990
Iteration 181/1000 | Loss: 0.00000990
Iteration 182/1000 | Loss: 0.00000990
Iteration 183/1000 | Loss: 0.00000990
Iteration 184/1000 | Loss: 0.00000990
Iteration 185/1000 | Loss: 0.00000990
Iteration 186/1000 | Loss: 0.00000990
Iteration 187/1000 | Loss: 0.00000989
Iteration 188/1000 | Loss: 0.00000989
Iteration 189/1000 | Loss: 0.00000989
Iteration 190/1000 | Loss: 0.00000989
Iteration 191/1000 | Loss: 0.00000989
Iteration 192/1000 | Loss: 0.00000989
Iteration 193/1000 | Loss: 0.00000989
Iteration 194/1000 | Loss: 0.00000988
Iteration 195/1000 | Loss: 0.00000988
Iteration 196/1000 | Loss: 0.00000988
Iteration 197/1000 | Loss: 0.00000988
Iteration 198/1000 | Loss: 0.00000988
Iteration 199/1000 | Loss: 0.00000988
Iteration 200/1000 | Loss: 0.00000988
Iteration 201/1000 | Loss: 0.00000988
Iteration 202/1000 | Loss: 0.00000988
Iteration 203/1000 | Loss: 0.00000988
Iteration 204/1000 | Loss: 0.00000988
Iteration 205/1000 | Loss: 0.00000988
Iteration 206/1000 | Loss: 0.00000988
Iteration 207/1000 | Loss: 0.00000988
Iteration 208/1000 | Loss: 0.00000988
Iteration 209/1000 | Loss: 0.00000988
Iteration 210/1000 | Loss: 0.00000988
Iteration 211/1000 | Loss: 0.00000988
Iteration 212/1000 | Loss: 0.00000988
Iteration 213/1000 | Loss: 0.00000988
Iteration 214/1000 | Loss: 0.00000988
Iteration 215/1000 | Loss: 0.00000987
Iteration 216/1000 | Loss: 0.00000987
Iteration 217/1000 | Loss: 0.00000987
Iteration 218/1000 | Loss: 0.00000987
Iteration 219/1000 | Loss: 0.00000987
Iteration 220/1000 | Loss: 0.00000987
Iteration 221/1000 | Loss: 0.00000987
Iteration 222/1000 | Loss: 0.00000987
Iteration 223/1000 | Loss: 0.00000987
Iteration 224/1000 | Loss: 0.00000987
Iteration 225/1000 | Loss: 0.00000987
Iteration 226/1000 | Loss: 0.00000987
Iteration 227/1000 | Loss: 0.00000987
Iteration 228/1000 | Loss: 0.00000987
Iteration 229/1000 | Loss: 0.00000987
Iteration 230/1000 | Loss: 0.00000987
Iteration 231/1000 | Loss: 0.00000987
Iteration 232/1000 | Loss: 0.00000987
Iteration 233/1000 | Loss: 0.00000987
Iteration 234/1000 | Loss: 0.00000987
Iteration 235/1000 | Loss: 0.00000987
Iteration 236/1000 | Loss: 0.00000987
Iteration 237/1000 | Loss: 0.00000987
Iteration 238/1000 | Loss: 0.00000987
Iteration 239/1000 | Loss: 0.00000987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [9.866276741377078e-06, 9.866276741377078e-06, 9.866276741377078e-06, 9.866276741377078e-06, 9.866276741377078e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.866276741377078e-06

Optimization complete. Final v2v error: 2.70888614654541 mm

Highest mean error: 3.04974627494812 mm for frame 54

Lowest mean error: 2.527519941329956 mm for frame 160

Saving results

Total time: 43.613112926483154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027704
Iteration 2/25 | Loss: 0.00242109
Iteration 3/25 | Loss: 0.00186852
Iteration 4/25 | Loss: 0.00174245
Iteration 5/25 | Loss: 0.00161608
Iteration 6/25 | Loss: 0.00150546
Iteration 7/25 | Loss: 0.00145900
Iteration 8/25 | Loss: 0.00143753
Iteration 9/25 | Loss: 0.00146407
Iteration 10/25 | Loss: 0.00144577
Iteration 11/25 | Loss: 0.00140180
Iteration 12/25 | Loss: 0.00139998
Iteration 13/25 | Loss: 0.00140203
Iteration 14/25 | Loss: 0.00139410
Iteration 15/25 | Loss: 0.00137888
Iteration 16/25 | Loss: 0.00137469
Iteration 17/25 | Loss: 0.00137247
Iteration 18/25 | Loss: 0.00137138
Iteration 19/25 | Loss: 0.00137082
Iteration 20/25 | Loss: 0.00137542
Iteration 21/25 | Loss: 0.00136963
Iteration 22/25 | Loss: 0.00136792
Iteration 23/25 | Loss: 0.00136754
Iteration 24/25 | Loss: 0.00136735
Iteration 25/25 | Loss: 0.00136702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.82292604
Iteration 2/25 | Loss: 0.00233122
Iteration 3/25 | Loss: 0.00233122
Iteration 4/25 | Loss: 0.00233122
Iteration 5/25 | Loss: 0.00233122
Iteration 6/25 | Loss: 0.00233122
Iteration 7/25 | Loss: 0.00233122
Iteration 8/25 | Loss: 0.00233122
Iteration 9/25 | Loss: 0.00233122
Iteration 10/25 | Loss: 0.00233122
Iteration 11/25 | Loss: 0.00233122
Iteration 12/25 | Loss: 0.00233122
Iteration 13/25 | Loss: 0.00233122
Iteration 14/25 | Loss: 0.00233122
Iteration 15/25 | Loss: 0.00233122
Iteration 16/25 | Loss: 0.00233122
Iteration 17/25 | Loss: 0.00233122
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002331216586753726, 0.002331216586753726, 0.002331216586753726, 0.002331216586753726, 0.002331216586753726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002331216586753726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233122
Iteration 2/1000 | Loss: 0.00069622
Iteration 3/1000 | Loss: 0.00057463
Iteration 4/1000 | Loss: 0.00043497
Iteration 5/1000 | Loss: 0.00016444
Iteration 6/1000 | Loss: 0.00014787
Iteration 7/1000 | Loss: 0.00012815
Iteration 8/1000 | Loss: 0.00011359
Iteration 9/1000 | Loss: 0.00010480
Iteration 10/1000 | Loss: 0.00045466
Iteration 11/1000 | Loss: 0.00035294
Iteration 12/1000 | Loss: 0.00041484
Iteration 13/1000 | Loss: 0.00116705
Iteration 14/1000 | Loss: 0.00011441
Iteration 15/1000 | Loss: 0.00009994
Iteration 16/1000 | Loss: 0.00043763
Iteration 17/1000 | Loss: 0.00041266
Iteration 18/1000 | Loss: 0.00010660
Iteration 19/1000 | Loss: 0.00010108
Iteration 20/1000 | Loss: 0.00014903
Iteration 21/1000 | Loss: 0.00010020
Iteration 22/1000 | Loss: 0.00009434
Iteration 23/1000 | Loss: 0.00009148
Iteration 24/1000 | Loss: 0.00008855
Iteration 25/1000 | Loss: 0.00019576
Iteration 26/1000 | Loss: 0.00011402
Iteration 27/1000 | Loss: 0.00009745
Iteration 28/1000 | Loss: 0.00009091
Iteration 29/1000 | Loss: 0.00008691
Iteration 30/1000 | Loss: 0.00008539
Iteration 31/1000 | Loss: 0.00008429
Iteration 32/1000 | Loss: 0.00008330
Iteration 33/1000 | Loss: 0.00008251
Iteration 34/1000 | Loss: 0.00008174
Iteration 35/1000 | Loss: 0.00008106
Iteration 36/1000 | Loss: 0.00008066
Iteration 37/1000 | Loss: 0.00008022
Iteration 38/1000 | Loss: 0.00035166
Iteration 39/1000 | Loss: 0.00008677
Iteration 40/1000 | Loss: 0.00008279
Iteration 41/1000 | Loss: 0.00008065
Iteration 42/1000 | Loss: 0.00007913
Iteration 43/1000 | Loss: 0.00007780
Iteration 44/1000 | Loss: 0.00007726
Iteration 45/1000 | Loss: 0.00007695
Iteration 46/1000 | Loss: 0.00007669
Iteration 47/1000 | Loss: 0.00007646
Iteration 48/1000 | Loss: 0.00007624
Iteration 49/1000 | Loss: 0.00007619
Iteration 50/1000 | Loss: 0.00007598
Iteration 51/1000 | Loss: 0.00007591
Iteration 52/1000 | Loss: 0.00007590
Iteration 53/1000 | Loss: 0.00007589
Iteration 54/1000 | Loss: 0.00007584
Iteration 55/1000 | Loss: 0.00007584
Iteration 56/1000 | Loss: 0.00007582
Iteration 57/1000 | Loss: 0.00007581
Iteration 58/1000 | Loss: 0.00007581
Iteration 59/1000 | Loss: 0.00007581
Iteration 60/1000 | Loss: 0.00007580
Iteration 61/1000 | Loss: 0.00007580
Iteration 62/1000 | Loss: 0.00007580
Iteration 63/1000 | Loss: 0.00007580
Iteration 64/1000 | Loss: 0.00007579
Iteration 65/1000 | Loss: 0.00007579
Iteration 66/1000 | Loss: 0.00007579
Iteration 67/1000 | Loss: 0.00007578
Iteration 68/1000 | Loss: 0.00007578
Iteration 69/1000 | Loss: 0.00007578
Iteration 70/1000 | Loss: 0.00007578
Iteration 71/1000 | Loss: 0.00007577
Iteration 72/1000 | Loss: 0.00007577
Iteration 73/1000 | Loss: 0.00007577
Iteration 74/1000 | Loss: 0.00007576
Iteration 75/1000 | Loss: 0.00007576
Iteration 76/1000 | Loss: 0.00007576
Iteration 77/1000 | Loss: 0.00007576
Iteration 78/1000 | Loss: 0.00007576
Iteration 79/1000 | Loss: 0.00007576
Iteration 80/1000 | Loss: 0.00007576
Iteration 81/1000 | Loss: 0.00007575
Iteration 82/1000 | Loss: 0.00007575
Iteration 83/1000 | Loss: 0.00007575
Iteration 84/1000 | Loss: 0.00007575
Iteration 85/1000 | Loss: 0.00007575
Iteration 86/1000 | Loss: 0.00007575
Iteration 87/1000 | Loss: 0.00007574
Iteration 88/1000 | Loss: 0.00007574
Iteration 89/1000 | Loss: 0.00007574
Iteration 90/1000 | Loss: 0.00007573
Iteration 91/1000 | Loss: 0.00007573
Iteration 92/1000 | Loss: 0.00007573
Iteration 93/1000 | Loss: 0.00007573
Iteration 94/1000 | Loss: 0.00007573
Iteration 95/1000 | Loss: 0.00007573
Iteration 96/1000 | Loss: 0.00007573
Iteration 97/1000 | Loss: 0.00007572
Iteration 98/1000 | Loss: 0.00007572
Iteration 99/1000 | Loss: 0.00007572
Iteration 100/1000 | Loss: 0.00007572
Iteration 101/1000 | Loss: 0.00007572
Iteration 102/1000 | Loss: 0.00007572
Iteration 103/1000 | Loss: 0.00007571
Iteration 104/1000 | Loss: 0.00007571
Iteration 105/1000 | Loss: 0.00007571
Iteration 106/1000 | Loss: 0.00007570
Iteration 107/1000 | Loss: 0.00007570
Iteration 108/1000 | Loss: 0.00007570
Iteration 109/1000 | Loss: 0.00007570
Iteration 110/1000 | Loss: 0.00007570
Iteration 111/1000 | Loss: 0.00007570
Iteration 112/1000 | Loss: 0.00007570
Iteration 113/1000 | Loss: 0.00007570
Iteration 114/1000 | Loss: 0.00007570
Iteration 115/1000 | Loss: 0.00007570
Iteration 116/1000 | Loss: 0.00007570
Iteration 117/1000 | Loss: 0.00007570
Iteration 118/1000 | Loss: 0.00007570
Iteration 119/1000 | Loss: 0.00007569
Iteration 120/1000 | Loss: 0.00007569
Iteration 121/1000 | Loss: 0.00007569
Iteration 122/1000 | Loss: 0.00007569
Iteration 123/1000 | Loss: 0.00007569
Iteration 124/1000 | Loss: 0.00007569
Iteration 125/1000 | Loss: 0.00007569
Iteration 126/1000 | Loss: 0.00007569
Iteration 127/1000 | Loss: 0.00007568
Iteration 128/1000 | Loss: 0.00007568
Iteration 129/1000 | Loss: 0.00007568
Iteration 130/1000 | Loss: 0.00007568
Iteration 131/1000 | Loss: 0.00007567
Iteration 132/1000 | Loss: 0.00007567
Iteration 133/1000 | Loss: 0.00007567
Iteration 134/1000 | Loss: 0.00007567
Iteration 135/1000 | Loss: 0.00007567
Iteration 136/1000 | Loss: 0.00007566
Iteration 137/1000 | Loss: 0.00007566
Iteration 138/1000 | Loss: 0.00007566
Iteration 139/1000 | Loss: 0.00007566
Iteration 140/1000 | Loss: 0.00007566
Iteration 141/1000 | Loss: 0.00007566
Iteration 142/1000 | Loss: 0.00007566
Iteration 143/1000 | Loss: 0.00007566
Iteration 144/1000 | Loss: 0.00007566
Iteration 145/1000 | Loss: 0.00007566
Iteration 146/1000 | Loss: 0.00007566
Iteration 147/1000 | Loss: 0.00007566
Iteration 148/1000 | Loss: 0.00007566
Iteration 149/1000 | Loss: 0.00007566
Iteration 150/1000 | Loss: 0.00007566
Iteration 151/1000 | Loss: 0.00007566
Iteration 152/1000 | Loss: 0.00007565
Iteration 153/1000 | Loss: 0.00007565
Iteration 154/1000 | Loss: 0.00007565
Iteration 155/1000 | Loss: 0.00007565
Iteration 156/1000 | Loss: 0.00007565
Iteration 157/1000 | Loss: 0.00007565
Iteration 158/1000 | Loss: 0.00007565
Iteration 159/1000 | Loss: 0.00007565
Iteration 160/1000 | Loss: 0.00007565
Iteration 161/1000 | Loss: 0.00007565
Iteration 162/1000 | Loss: 0.00007565
Iteration 163/1000 | Loss: 0.00007565
Iteration 164/1000 | Loss: 0.00007565
Iteration 165/1000 | Loss: 0.00007565
Iteration 166/1000 | Loss: 0.00007565
Iteration 167/1000 | Loss: 0.00007565
Iteration 168/1000 | Loss: 0.00007565
Iteration 169/1000 | Loss: 0.00007564
Iteration 170/1000 | Loss: 0.00007564
Iteration 171/1000 | Loss: 0.00007564
Iteration 172/1000 | Loss: 0.00007564
Iteration 173/1000 | Loss: 0.00007564
Iteration 174/1000 | Loss: 0.00007564
Iteration 175/1000 | Loss: 0.00007564
Iteration 176/1000 | Loss: 0.00007564
Iteration 177/1000 | Loss: 0.00007564
Iteration 178/1000 | Loss: 0.00007564
Iteration 179/1000 | Loss: 0.00007564
Iteration 180/1000 | Loss: 0.00007564
Iteration 181/1000 | Loss: 0.00007564
Iteration 182/1000 | Loss: 0.00007564
Iteration 183/1000 | Loss: 0.00007563
Iteration 184/1000 | Loss: 0.00007563
Iteration 185/1000 | Loss: 0.00007563
Iteration 186/1000 | Loss: 0.00007563
Iteration 187/1000 | Loss: 0.00007563
Iteration 188/1000 | Loss: 0.00007563
Iteration 189/1000 | Loss: 0.00007563
Iteration 190/1000 | Loss: 0.00007563
Iteration 191/1000 | Loss: 0.00007563
Iteration 192/1000 | Loss: 0.00007563
Iteration 193/1000 | Loss: 0.00007563
Iteration 194/1000 | Loss: 0.00007563
Iteration 195/1000 | Loss: 0.00007563
Iteration 196/1000 | Loss: 0.00007563
Iteration 197/1000 | Loss: 0.00007563
Iteration 198/1000 | Loss: 0.00007563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [7.56328518036753e-05, 7.56328518036753e-05, 7.56328518036753e-05, 7.56328518036753e-05, 7.56328518036753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.56328518036753e-05

Optimization complete. Final v2v error: 4.72665548324585 mm

Highest mean error: 11.646138191223145 mm for frame 51

Lowest mean error: 2.9659175872802734 mm for frame 2

Saving results

Total time: 124.16985559463501
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047986
Iteration 2/25 | Loss: 0.00394348
Iteration 3/25 | Loss: 0.00251543
Iteration 4/25 | Loss: 0.00201430
Iteration 5/25 | Loss: 0.00203884
Iteration 6/25 | Loss: 0.00165214
Iteration 7/25 | Loss: 0.00143754
Iteration 8/25 | Loss: 0.00136348
Iteration 9/25 | Loss: 0.00127867
Iteration 10/25 | Loss: 0.00125308
Iteration 11/25 | Loss: 0.00123950
Iteration 12/25 | Loss: 0.00122268
Iteration 13/25 | Loss: 0.00121748
Iteration 14/25 | Loss: 0.00123098
Iteration 15/25 | Loss: 0.00123725
Iteration 16/25 | Loss: 0.00121146
Iteration 17/25 | Loss: 0.00117109
Iteration 18/25 | Loss: 0.00116854
Iteration 19/25 | Loss: 0.00116731
Iteration 20/25 | Loss: 0.00116974
Iteration 21/25 | Loss: 0.00116616
Iteration 22/25 | Loss: 0.00116406
Iteration 23/25 | Loss: 0.00116344
Iteration 24/25 | Loss: 0.00116326
Iteration 25/25 | Loss: 0.00116318

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33867145
Iteration 2/25 | Loss: 0.00404201
Iteration 3/25 | Loss: 0.00404200
Iteration 4/25 | Loss: 0.00155136
Iteration 5/25 | Loss: 0.00154655
Iteration 6/25 | Loss: 0.00154654
Iteration 7/25 | Loss: 0.00154654
Iteration 8/25 | Loss: 0.00154654
Iteration 9/25 | Loss: 0.00154654
Iteration 10/25 | Loss: 0.00154654
Iteration 11/25 | Loss: 0.00154654
Iteration 12/25 | Loss: 0.00154654
Iteration 13/25 | Loss: 0.00154654
Iteration 14/25 | Loss: 0.00154654
Iteration 15/25 | Loss: 0.00154654
Iteration 16/25 | Loss: 0.00154654
Iteration 17/25 | Loss: 0.00154654
Iteration 18/25 | Loss: 0.00154654
Iteration 19/25 | Loss: 0.00154654
Iteration 20/25 | Loss: 0.00154654
Iteration 21/25 | Loss: 0.00154654
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015465427422896028, 0.0015465427422896028, 0.0015465427422896028, 0.0015465427422896028, 0.0015465427422896028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015465427422896028

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154654
Iteration 2/1000 | Loss: 0.00144743
Iteration 3/1000 | Loss: 0.00010520
Iteration 4/1000 | Loss: 0.00007593
Iteration 5/1000 | Loss: 0.00006673
Iteration 6/1000 | Loss: 0.00006245
Iteration 7/1000 | Loss: 0.00006014
Iteration 8/1000 | Loss: 0.00096817
Iteration 9/1000 | Loss: 0.00005808
Iteration 10/1000 | Loss: 0.00109481
Iteration 11/1000 | Loss: 0.00563602
Iteration 12/1000 | Loss: 0.00007523
Iteration 13/1000 | Loss: 0.00022889
Iteration 14/1000 | Loss: 0.00021639
Iteration 15/1000 | Loss: 0.00004271
Iteration 16/1000 | Loss: 0.00005122
Iteration 17/1000 | Loss: 0.00003119
Iteration 18/1000 | Loss: 0.00002770
Iteration 19/1000 | Loss: 0.00002534
Iteration 20/1000 | Loss: 0.00002356
Iteration 21/1000 | Loss: 0.00002248
Iteration 22/1000 | Loss: 0.00002152
Iteration 23/1000 | Loss: 0.00002069
Iteration 24/1000 | Loss: 0.00010858
Iteration 25/1000 | Loss: 0.00001909
Iteration 26/1000 | Loss: 0.00001846
Iteration 27/1000 | Loss: 0.00001778
Iteration 28/1000 | Loss: 0.00001722
Iteration 29/1000 | Loss: 0.00001691
Iteration 30/1000 | Loss: 0.00001672
Iteration 31/1000 | Loss: 0.00001672
Iteration 32/1000 | Loss: 0.00001669
Iteration 33/1000 | Loss: 0.00001662
Iteration 34/1000 | Loss: 0.00001658
Iteration 35/1000 | Loss: 0.00001658
Iteration 36/1000 | Loss: 0.00001655
Iteration 37/1000 | Loss: 0.00001652
Iteration 38/1000 | Loss: 0.00001651
Iteration 39/1000 | Loss: 0.00001651
Iteration 40/1000 | Loss: 0.00001651
Iteration 41/1000 | Loss: 0.00001650
Iteration 42/1000 | Loss: 0.00001650
Iteration 43/1000 | Loss: 0.00001649
Iteration 44/1000 | Loss: 0.00001649
Iteration 45/1000 | Loss: 0.00001648
Iteration 46/1000 | Loss: 0.00001647
Iteration 47/1000 | Loss: 0.00001647
Iteration 48/1000 | Loss: 0.00001646
Iteration 49/1000 | Loss: 0.00001646
Iteration 50/1000 | Loss: 0.00001646
Iteration 51/1000 | Loss: 0.00001646
Iteration 52/1000 | Loss: 0.00001646
Iteration 53/1000 | Loss: 0.00001646
Iteration 54/1000 | Loss: 0.00001646
Iteration 55/1000 | Loss: 0.00001645
Iteration 56/1000 | Loss: 0.00001643
Iteration 57/1000 | Loss: 0.00001643
Iteration 58/1000 | Loss: 0.00001643
Iteration 59/1000 | Loss: 0.00001643
Iteration 60/1000 | Loss: 0.00001643
Iteration 61/1000 | Loss: 0.00001643
Iteration 62/1000 | Loss: 0.00001643
Iteration 63/1000 | Loss: 0.00001643
Iteration 64/1000 | Loss: 0.00001643
Iteration 65/1000 | Loss: 0.00001643
Iteration 66/1000 | Loss: 0.00001643
Iteration 67/1000 | Loss: 0.00001642
Iteration 68/1000 | Loss: 0.00001642
Iteration 69/1000 | Loss: 0.00001642
Iteration 70/1000 | Loss: 0.00001641
Iteration 71/1000 | Loss: 0.00001640
Iteration 72/1000 | Loss: 0.00001640
Iteration 73/1000 | Loss: 0.00001640
Iteration 74/1000 | Loss: 0.00001640
Iteration 75/1000 | Loss: 0.00001639
Iteration 76/1000 | Loss: 0.00001639
Iteration 77/1000 | Loss: 0.00001639
Iteration 78/1000 | Loss: 0.00001639
Iteration 79/1000 | Loss: 0.00001638
Iteration 80/1000 | Loss: 0.00001638
Iteration 81/1000 | Loss: 0.00001638
Iteration 82/1000 | Loss: 0.00001638
Iteration 83/1000 | Loss: 0.00001638
Iteration 84/1000 | Loss: 0.00001637
Iteration 85/1000 | Loss: 0.00001637
Iteration 86/1000 | Loss: 0.00001637
Iteration 87/1000 | Loss: 0.00001636
Iteration 88/1000 | Loss: 0.00001636
Iteration 89/1000 | Loss: 0.00001636
Iteration 90/1000 | Loss: 0.00001636
Iteration 91/1000 | Loss: 0.00001636
Iteration 92/1000 | Loss: 0.00001636
Iteration 93/1000 | Loss: 0.00001636
Iteration 94/1000 | Loss: 0.00001636
Iteration 95/1000 | Loss: 0.00001636
Iteration 96/1000 | Loss: 0.00001636
Iteration 97/1000 | Loss: 0.00001635
Iteration 98/1000 | Loss: 0.00001635
Iteration 99/1000 | Loss: 0.00001635
Iteration 100/1000 | Loss: 0.00001635
Iteration 101/1000 | Loss: 0.00001635
Iteration 102/1000 | Loss: 0.00001635
Iteration 103/1000 | Loss: 0.00001635
Iteration 104/1000 | Loss: 0.00001634
Iteration 105/1000 | Loss: 0.00001634
Iteration 106/1000 | Loss: 0.00001634
Iteration 107/1000 | Loss: 0.00001634
Iteration 108/1000 | Loss: 0.00001634
Iteration 109/1000 | Loss: 0.00001634
Iteration 110/1000 | Loss: 0.00001634
Iteration 111/1000 | Loss: 0.00001634
Iteration 112/1000 | Loss: 0.00001634
Iteration 113/1000 | Loss: 0.00001633
Iteration 114/1000 | Loss: 0.00001633
Iteration 115/1000 | Loss: 0.00001633
Iteration 116/1000 | Loss: 0.00001633
Iteration 117/1000 | Loss: 0.00001633
Iteration 118/1000 | Loss: 0.00001633
Iteration 119/1000 | Loss: 0.00001633
Iteration 120/1000 | Loss: 0.00001633
Iteration 121/1000 | Loss: 0.00001633
Iteration 122/1000 | Loss: 0.00001633
Iteration 123/1000 | Loss: 0.00001633
Iteration 124/1000 | Loss: 0.00001633
Iteration 125/1000 | Loss: 0.00001633
Iteration 126/1000 | Loss: 0.00001633
Iteration 127/1000 | Loss: 0.00001632
Iteration 128/1000 | Loss: 0.00001632
Iteration 129/1000 | Loss: 0.00001632
Iteration 130/1000 | Loss: 0.00001632
Iteration 131/1000 | Loss: 0.00001632
Iteration 132/1000 | Loss: 0.00001632
Iteration 133/1000 | Loss: 0.00001632
Iteration 134/1000 | Loss: 0.00001632
Iteration 135/1000 | Loss: 0.00001632
Iteration 136/1000 | Loss: 0.00001632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.6315525499521755e-05, 1.6315525499521755e-05, 1.6315525499521755e-05, 1.6315525499521755e-05, 1.6315525499521755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6315525499521755e-05

Optimization complete. Final v2v error: 3.4168214797973633 mm

Highest mean error: 4.636765480041504 mm for frame 42

Lowest mean error: 2.854457139968872 mm for frame 181

Saving results

Total time: 103.61499261856079
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00698364
Iteration 2/25 | Loss: 0.00140863
Iteration 3/25 | Loss: 0.00118271
Iteration 4/25 | Loss: 0.00116322
Iteration 5/25 | Loss: 0.00116079
Iteration 6/25 | Loss: 0.00116079
Iteration 7/25 | Loss: 0.00116079
Iteration 8/25 | Loss: 0.00116079
Iteration 9/25 | Loss: 0.00116079
Iteration 10/25 | Loss: 0.00116079
Iteration 11/25 | Loss: 0.00116079
Iteration 12/25 | Loss: 0.00116079
Iteration 13/25 | Loss: 0.00116079
Iteration 14/25 | Loss: 0.00116079
Iteration 15/25 | Loss: 0.00116079
Iteration 16/25 | Loss: 0.00116079
Iteration 17/25 | Loss: 0.00116079
Iteration 18/25 | Loss: 0.00116079
Iteration 19/25 | Loss: 0.00116079
Iteration 20/25 | Loss: 0.00116079
Iteration 21/25 | Loss: 0.00116079
Iteration 22/25 | Loss: 0.00116079
Iteration 23/25 | Loss: 0.00116079
Iteration 24/25 | Loss: 0.00116079
Iteration 25/25 | Loss: 0.00116079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34787166
Iteration 2/25 | Loss: 0.00065229
Iteration 3/25 | Loss: 0.00065228
Iteration 4/25 | Loss: 0.00065227
Iteration 5/25 | Loss: 0.00065227
Iteration 6/25 | Loss: 0.00065227
Iteration 7/25 | Loss: 0.00065227
Iteration 8/25 | Loss: 0.00065227
Iteration 9/25 | Loss: 0.00065227
Iteration 10/25 | Loss: 0.00065227
Iteration 11/25 | Loss: 0.00065227
Iteration 12/25 | Loss: 0.00065227
Iteration 13/25 | Loss: 0.00065227
Iteration 14/25 | Loss: 0.00065227
Iteration 15/25 | Loss: 0.00065227
Iteration 16/25 | Loss: 0.00065227
Iteration 17/25 | Loss: 0.00065227
Iteration 18/25 | Loss: 0.00065227
Iteration 19/25 | Loss: 0.00065227
Iteration 20/25 | Loss: 0.00065227
Iteration 21/25 | Loss: 0.00065227
Iteration 22/25 | Loss: 0.00065227
Iteration 23/25 | Loss: 0.00065227
Iteration 24/25 | Loss: 0.00065227
Iteration 25/25 | Loss: 0.00065227

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065227
Iteration 2/1000 | Loss: 0.00002893
Iteration 3/1000 | Loss: 0.00002123
Iteration 4/1000 | Loss: 0.00001882
Iteration 5/1000 | Loss: 0.00001802
Iteration 6/1000 | Loss: 0.00001742
Iteration 7/1000 | Loss: 0.00001722
Iteration 8/1000 | Loss: 0.00001691
Iteration 9/1000 | Loss: 0.00001681
Iteration 10/1000 | Loss: 0.00001659
Iteration 11/1000 | Loss: 0.00001639
Iteration 12/1000 | Loss: 0.00001635
Iteration 13/1000 | Loss: 0.00001630
Iteration 14/1000 | Loss: 0.00001623
Iteration 15/1000 | Loss: 0.00001611
Iteration 16/1000 | Loss: 0.00001601
Iteration 17/1000 | Loss: 0.00001584
Iteration 18/1000 | Loss: 0.00001574
Iteration 19/1000 | Loss: 0.00001570
Iteration 20/1000 | Loss: 0.00001554
Iteration 21/1000 | Loss: 0.00001554
Iteration 22/1000 | Loss: 0.00001553
Iteration 23/1000 | Loss: 0.00001552
Iteration 24/1000 | Loss: 0.00001551
Iteration 25/1000 | Loss: 0.00001550
Iteration 26/1000 | Loss: 0.00001549
Iteration 27/1000 | Loss: 0.00001548
Iteration 28/1000 | Loss: 0.00001547
Iteration 29/1000 | Loss: 0.00001547
Iteration 30/1000 | Loss: 0.00001546
Iteration 31/1000 | Loss: 0.00001546
Iteration 32/1000 | Loss: 0.00001545
Iteration 33/1000 | Loss: 0.00001545
Iteration 34/1000 | Loss: 0.00001545
Iteration 35/1000 | Loss: 0.00001545
Iteration 36/1000 | Loss: 0.00001544
Iteration 37/1000 | Loss: 0.00001544
Iteration 38/1000 | Loss: 0.00001539
Iteration 39/1000 | Loss: 0.00001539
Iteration 40/1000 | Loss: 0.00001535
Iteration 41/1000 | Loss: 0.00001535
Iteration 42/1000 | Loss: 0.00001535
Iteration 43/1000 | Loss: 0.00001535
Iteration 44/1000 | Loss: 0.00001535
Iteration 45/1000 | Loss: 0.00001535
Iteration 46/1000 | Loss: 0.00001535
Iteration 47/1000 | Loss: 0.00001534
Iteration 48/1000 | Loss: 0.00001534
Iteration 49/1000 | Loss: 0.00001534
Iteration 50/1000 | Loss: 0.00001534
Iteration 51/1000 | Loss: 0.00001533
Iteration 52/1000 | Loss: 0.00001532
Iteration 53/1000 | Loss: 0.00001532
Iteration 54/1000 | Loss: 0.00001531
Iteration 55/1000 | Loss: 0.00001531
Iteration 56/1000 | Loss: 0.00001531
Iteration 57/1000 | Loss: 0.00001531
Iteration 58/1000 | Loss: 0.00001531
Iteration 59/1000 | Loss: 0.00001531
Iteration 60/1000 | Loss: 0.00001530
Iteration 61/1000 | Loss: 0.00001530
Iteration 62/1000 | Loss: 0.00001530
Iteration 63/1000 | Loss: 0.00001530
Iteration 64/1000 | Loss: 0.00001529
Iteration 65/1000 | Loss: 0.00001529
Iteration 66/1000 | Loss: 0.00001529
Iteration 67/1000 | Loss: 0.00001528
Iteration 68/1000 | Loss: 0.00001528
Iteration 69/1000 | Loss: 0.00001528
Iteration 70/1000 | Loss: 0.00001527
Iteration 71/1000 | Loss: 0.00001527
Iteration 72/1000 | Loss: 0.00001527
Iteration 73/1000 | Loss: 0.00001527
Iteration 74/1000 | Loss: 0.00001527
Iteration 75/1000 | Loss: 0.00001527
Iteration 76/1000 | Loss: 0.00001526
Iteration 77/1000 | Loss: 0.00001526
Iteration 78/1000 | Loss: 0.00001526
Iteration 79/1000 | Loss: 0.00001526
Iteration 80/1000 | Loss: 0.00001526
Iteration 81/1000 | Loss: 0.00001526
Iteration 82/1000 | Loss: 0.00001526
Iteration 83/1000 | Loss: 0.00001526
Iteration 84/1000 | Loss: 0.00001526
Iteration 85/1000 | Loss: 0.00001525
Iteration 86/1000 | Loss: 0.00001525
Iteration 87/1000 | Loss: 0.00001524
Iteration 88/1000 | Loss: 0.00001524
Iteration 89/1000 | Loss: 0.00001524
Iteration 90/1000 | Loss: 0.00001524
Iteration 91/1000 | Loss: 0.00001524
Iteration 92/1000 | Loss: 0.00001524
Iteration 93/1000 | Loss: 0.00001524
Iteration 94/1000 | Loss: 0.00001523
Iteration 95/1000 | Loss: 0.00001523
Iteration 96/1000 | Loss: 0.00001523
Iteration 97/1000 | Loss: 0.00001523
Iteration 98/1000 | Loss: 0.00001523
Iteration 99/1000 | Loss: 0.00001523
Iteration 100/1000 | Loss: 0.00001523
Iteration 101/1000 | Loss: 0.00001523
Iteration 102/1000 | Loss: 0.00001523
Iteration 103/1000 | Loss: 0.00001523
Iteration 104/1000 | Loss: 0.00001522
Iteration 105/1000 | Loss: 0.00001522
Iteration 106/1000 | Loss: 0.00001522
Iteration 107/1000 | Loss: 0.00001522
Iteration 108/1000 | Loss: 0.00001522
Iteration 109/1000 | Loss: 0.00001522
Iteration 110/1000 | Loss: 0.00001522
Iteration 111/1000 | Loss: 0.00001522
Iteration 112/1000 | Loss: 0.00001522
Iteration 113/1000 | Loss: 0.00001522
Iteration 114/1000 | Loss: 0.00001522
Iteration 115/1000 | Loss: 0.00001522
Iteration 116/1000 | Loss: 0.00001522
Iteration 117/1000 | Loss: 0.00001521
Iteration 118/1000 | Loss: 0.00001521
Iteration 119/1000 | Loss: 0.00001521
Iteration 120/1000 | Loss: 0.00001521
Iteration 121/1000 | Loss: 0.00001521
Iteration 122/1000 | Loss: 0.00001521
Iteration 123/1000 | Loss: 0.00001521
Iteration 124/1000 | Loss: 0.00001521
Iteration 125/1000 | Loss: 0.00001521
Iteration 126/1000 | Loss: 0.00001521
Iteration 127/1000 | Loss: 0.00001521
Iteration 128/1000 | Loss: 0.00001521
Iteration 129/1000 | Loss: 0.00001521
Iteration 130/1000 | Loss: 0.00001521
Iteration 131/1000 | Loss: 0.00001521
Iteration 132/1000 | Loss: 0.00001521
Iteration 133/1000 | Loss: 0.00001520
Iteration 134/1000 | Loss: 0.00001520
Iteration 135/1000 | Loss: 0.00001520
Iteration 136/1000 | Loss: 0.00001520
Iteration 137/1000 | Loss: 0.00001520
Iteration 138/1000 | Loss: 0.00001520
Iteration 139/1000 | Loss: 0.00001520
Iteration 140/1000 | Loss: 0.00001519
Iteration 141/1000 | Loss: 0.00001519
Iteration 142/1000 | Loss: 0.00001519
Iteration 143/1000 | Loss: 0.00001519
Iteration 144/1000 | Loss: 0.00001519
Iteration 145/1000 | Loss: 0.00001519
Iteration 146/1000 | Loss: 0.00001519
Iteration 147/1000 | Loss: 0.00001519
Iteration 148/1000 | Loss: 0.00001519
Iteration 149/1000 | Loss: 0.00001519
Iteration 150/1000 | Loss: 0.00001519
Iteration 151/1000 | Loss: 0.00001519
Iteration 152/1000 | Loss: 0.00001519
Iteration 153/1000 | Loss: 0.00001518
Iteration 154/1000 | Loss: 0.00001518
Iteration 155/1000 | Loss: 0.00001518
Iteration 156/1000 | Loss: 0.00001518
Iteration 157/1000 | Loss: 0.00001518
Iteration 158/1000 | Loss: 0.00001518
Iteration 159/1000 | Loss: 0.00001518
Iteration 160/1000 | Loss: 0.00001518
Iteration 161/1000 | Loss: 0.00001518
Iteration 162/1000 | Loss: 0.00001518
Iteration 163/1000 | Loss: 0.00001518
Iteration 164/1000 | Loss: 0.00001518
Iteration 165/1000 | Loss: 0.00001518
Iteration 166/1000 | Loss: 0.00001517
Iteration 167/1000 | Loss: 0.00001517
Iteration 168/1000 | Loss: 0.00001517
Iteration 169/1000 | Loss: 0.00001517
Iteration 170/1000 | Loss: 0.00001517
Iteration 171/1000 | Loss: 0.00001517
Iteration 172/1000 | Loss: 0.00001517
Iteration 173/1000 | Loss: 0.00001517
Iteration 174/1000 | Loss: 0.00001517
Iteration 175/1000 | Loss: 0.00001517
Iteration 176/1000 | Loss: 0.00001517
Iteration 177/1000 | Loss: 0.00001517
Iteration 178/1000 | Loss: 0.00001517
Iteration 179/1000 | Loss: 0.00001517
Iteration 180/1000 | Loss: 0.00001517
Iteration 181/1000 | Loss: 0.00001516
Iteration 182/1000 | Loss: 0.00001516
Iteration 183/1000 | Loss: 0.00001516
Iteration 184/1000 | Loss: 0.00001516
Iteration 185/1000 | Loss: 0.00001516
Iteration 186/1000 | Loss: 0.00001516
Iteration 187/1000 | Loss: 0.00001516
Iteration 188/1000 | Loss: 0.00001516
Iteration 189/1000 | Loss: 0.00001516
Iteration 190/1000 | Loss: 0.00001516
Iteration 191/1000 | Loss: 0.00001516
Iteration 192/1000 | Loss: 0.00001516
Iteration 193/1000 | Loss: 0.00001516
Iteration 194/1000 | Loss: 0.00001516
Iteration 195/1000 | Loss: 0.00001516
Iteration 196/1000 | Loss: 0.00001516
Iteration 197/1000 | Loss: 0.00001516
Iteration 198/1000 | Loss: 0.00001516
Iteration 199/1000 | Loss: 0.00001516
Iteration 200/1000 | Loss: 0.00001516
Iteration 201/1000 | Loss: 0.00001516
Iteration 202/1000 | Loss: 0.00001516
Iteration 203/1000 | Loss: 0.00001516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.5161726878432091e-05, 1.5161726878432091e-05, 1.5161726878432091e-05, 1.5161726878432091e-05, 1.5161726878432091e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5161726878432091e-05

Optimization complete. Final v2v error: 3.305549383163452 mm

Highest mean error: 3.49871826171875 mm for frame 91

Lowest mean error: 3.111795663833618 mm for frame 194

Saving results

Total time: 47.594048738479614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00611535
Iteration 2/25 | Loss: 0.00127991
Iteration 3/25 | Loss: 0.00114445
Iteration 4/25 | Loss: 0.00112375
Iteration 5/25 | Loss: 0.00112130
Iteration 6/25 | Loss: 0.00112098
Iteration 7/25 | Loss: 0.00112098
Iteration 8/25 | Loss: 0.00112098
Iteration 9/25 | Loss: 0.00112098
Iteration 10/25 | Loss: 0.00112098
Iteration 11/25 | Loss: 0.00112098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011209839722141623, 0.0011209839722141623, 0.0011209839722141623, 0.0011209839722141623, 0.0011209839722141623]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011209839722141623

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.57740760
Iteration 2/25 | Loss: 0.00070075
Iteration 3/25 | Loss: 0.00070075
Iteration 4/25 | Loss: 0.00070075
Iteration 5/25 | Loss: 0.00070075
Iteration 6/25 | Loss: 0.00070075
Iteration 7/25 | Loss: 0.00070075
Iteration 8/25 | Loss: 0.00070075
Iteration 9/25 | Loss: 0.00070075
Iteration 10/25 | Loss: 0.00070075
Iteration 11/25 | Loss: 0.00070075
Iteration 12/25 | Loss: 0.00070075
Iteration 13/25 | Loss: 0.00070075
Iteration 14/25 | Loss: 0.00070075
Iteration 15/25 | Loss: 0.00070075
Iteration 16/25 | Loss: 0.00070075
Iteration 17/25 | Loss: 0.00070075
Iteration 18/25 | Loss: 0.00070075
Iteration 19/25 | Loss: 0.00070075
Iteration 20/25 | Loss: 0.00070075
Iteration 21/25 | Loss: 0.00070075
Iteration 22/25 | Loss: 0.00070075
Iteration 23/25 | Loss: 0.00070075
Iteration 24/25 | Loss: 0.00070075
Iteration 25/25 | Loss: 0.00070075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070075
Iteration 2/1000 | Loss: 0.00003460
Iteration 3/1000 | Loss: 0.00002435
Iteration 4/1000 | Loss: 0.00002249
Iteration 5/1000 | Loss: 0.00002153
Iteration 6/1000 | Loss: 0.00002102
Iteration 7/1000 | Loss: 0.00002060
Iteration 8/1000 | Loss: 0.00002043
Iteration 9/1000 | Loss: 0.00002043
Iteration 10/1000 | Loss: 0.00002043
Iteration 11/1000 | Loss: 0.00002043
Iteration 12/1000 | Loss: 0.00002043
Iteration 13/1000 | Loss: 0.00002043
Iteration 14/1000 | Loss: 0.00002043
Iteration 15/1000 | Loss: 0.00002043
Iteration 16/1000 | Loss: 0.00002043
Iteration 17/1000 | Loss: 0.00002043
Iteration 18/1000 | Loss: 0.00002043
Iteration 19/1000 | Loss: 0.00002042
Iteration 20/1000 | Loss: 0.00002042
Iteration 21/1000 | Loss: 0.00002042
Iteration 22/1000 | Loss: 0.00002042
Iteration 23/1000 | Loss: 0.00002042
Iteration 24/1000 | Loss: 0.00002042
Iteration 25/1000 | Loss: 0.00002042
Iteration 26/1000 | Loss: 0.00002042
Iteration 27/1000 | Loss: 0.00002042
Iteration 28/1000 | Loss: 0.00002042
Iteration 29/1000 | Loss: 0.00002042
Iteration 30/1000 | Loss: 0.00002042
Iteration 31/1000 | Loss: 0.00002042
Iteration 32/1000 | Loss: 0.00002014
Iteration 33/1000 | Loss: 0.00002014
Iteration 34/1000 | Loss: 0.00002012
Iteration 35/1000 | Loss: 0.00002003
Iteration 36/1000 | Loss: 0.00002002
Iteration 37/1000 | Loss: 0.00002002
Iteration 38/1000 | Loss: 0.00001998
Iteration 39/1000 | Loss: 0.00001994
Iteration 40/1000 | Loss: 0.00001991
Iteration 41/1000 | Loss: 0.00001980
Iteration 42/1000 | Loss: 0.00001978
Iteration 43/1000 | Loss: 0.00001975
Iteration 44/1000 | Loss: 0.00001973
Iteration 45/1000 | Loss: 0.00001972
Iteration 46/1000 | Loss: 0.00001972
Iteration 47/1000 | Loss: 0.00001971
Iteration 48/1000 | Loss: 0.00001971
Iteration 49/1000 | Loss: 0.00001971
Iteration 50/1000 | Loss: 0.00001971
Iteration 51/1000 | Loss: 0.00001971
Iteration 52/1000 | Loss: 0.00001971
Iteration 53/1000 | Loss: 0.00001970
Iteration 54/1000 | Loss: 0.00001970
Iteration 55/1000 | Loss: 0.00001970
Iteration 56/1000 | Loss: 0.00001970
Iteration 57/1000 | Loss: 0.00001969
Iteration 58/1000 | Loss: 0.00001969
Iteration 59/1000 | Loss: 0.00001969
Iteration 60/1000 | Loss: 0.00001969
Iteration 61/1000 | Loss: 0.00001968
Iteration 62/1000 | Loss: 0.00001968
Iteration 63/1000 | Loss: 0.00001967
Iteration 64/1000 | Loss: 0.00001967
Iteration 65/1000 | Loss: 0.00001967
Iteration 66/1000 | Loss: 0.00001967
Iteration 67/1000 | Loss: 0.00001967
Iteration 68/1000 | Loss: 0.00001966
Iteration 69/1000 | Loss: 0.00001966
Iteration 70/1000 | Loss: 0.00001965
Iteration 71/1000 | Loss: 0.00001964
Iteration 72/1000 | Loss: 0.00001964
Iteration 73/1000 | Loss: 0.00001963
Iteration 74/1000 | Loss: 0.00001963
Iteration 75/1000 | Loss: 0.00001962
Iteration 76/1000 | Loss: 0.00001962
Iteration 77/1000 | Loss: 0.00001962
Iteration 78/1000 | Loss: 0.00001962
Iteration 79/1000 | Loss: 0.00001961
Iteration 80/1000 | Loss: 0.00001961
Iteration 81/1000 | Loss: 0.00001961
Iteration 82/1000 | Loss: 0.00001961
Iteration 83/1000 | Loss: 0.00001961
Iteration 84/1000 | Loss: 0.00001961
Iteration 85/1000 | Loss: 0.00001961
Iteration 86/1000 | Loss: 0.00001960
Iteration 87/1000 | Loss: 0.00001960
Iteration 88/1000 | Loss: 0.00001959
Iteration 89/1000 | Loss: 0.00001959
Iteration 90/1000 | Loss: 0.00001959
Iteration 91/1000 | Loss: 0.00001958
Iteration 92/1000 | Loss: 0.00001957
Iteration 93/1000 | Loss: 0.00001957
Iteration 94/1000 | Loss: 0.00001955
Iteration 95/1000 | Loss: 0.00001955
Iteration 96/1000 | Loss: 0.00001955
Iteration 97/1000 | Loss: 0.00001955
Iteration 98/1000 | Loss: 0.00001955
Iteration 99/1000 | Loss: 0.00001955
Iteration 100/1000 | Loss: 0.00001954
Iteration 101/1000 | Loss: 0.00001954
Iteration 102/1000 | Loss: 0.00001954
Iteration 103/1000 | Loss: 0.00001954
Iteration 104/1000 | Loss: 0.00001954
Iteration 105/1000 | Loss: 0.00001952
Iteration 106/1000 | Loss: 0.00001952
Iteration 107/1000 | Loss: 0.00001951
Iteration 108/1000 | Loss: 0.00001951
Iteration 109/1000 | Loss: 0.00001951
Iteration 110/1000 | Loss: 0.00001951
Iteration 111/1000 | Loss: 0.00001950
Iteration 112/1000 | Loss: 0.00001950
Iteration 113/1000 | Loss: 0.00001950
Iteration 114/1000 | Loss: 0.00001949
Iteration 115/1000 | Loss: 0.00001949
Iteration 116/1000 | Loss: 0.00001948
Iteration 117/1000 | Loss: 0.00001947
Iteration 118/1000 | Loss: 0.00001947
Iteration 119/1000 | Loss: 0.00001946
Iteration 120/1000 | Loss: 0.00001946
Iteration 121/1000 | Loss: 0.00001946
Iteration 122/1000 | Loss: 0.00001946
Iteration 123/1000 | Loss: 0.00001945
Iteration 124/1000 | Loss: 0.00001945
Iteration 125/1000 | Loss: 0.00001945
Iteration 126/1000 | Loss: 0.00001944
Iteration 127/1000 | Loss: 0.00001943
Iteration 128/1000 | Loss: 0.00001943
Iteration 129/1000 | Loss: 0.00001943
Iteration 130/1000 | Loss: 0.00001943
Iteration 131/1000 | Loss: 0.00001942
Iteration 132/1000 | Loss: 0.00001942
Iteration 133/1000 | Loss: 0.00001942
Iteration 134/1000 | Loss: 0.00001942
Iteration 135/1000 | Loss: 0.00001941
Iteration 136/1000 | Loss: 0.00001941
Iteration 137/1000 | Loss: 0.00001941
Iteration 138/1000 | Loss: 0.00001941
Iteration 139/1000 | Loss: 0.00001941
Iteration 140/1000 | Loss: 0.00001941
Iteration 141/1000 | Loss: 0.00001940
Iteration 142/1000 | Loss: 0.00001940
Iteration 143/1000 | Loss: 0.00001940
Iteration 144/1000 | Loss: 0.00001940
Iteration 145/1000 | Loss: 0.00001940
Iteration 146/1000 | Loss: 0.00001939
Iteration 147/1000 | Loss: 0.00001939
Iteration 148/1000 | Loss: 0.00001939
Iteration 149/1000 | Loss: 0.00001939
Iteration 150/1000 | Loss: 0.00001939
Iteration 151/1000 | Loss: 0.00001939
Iteration 152/1000 | Loss: 0.00001939
Iteration 153/1000 | Loss: 0.00001939
Iteration 154/1000 | Loss: 0.00001939
Iteration 155/1000 | Loss: 0.00001939
Iteration 156/1000 | Loss: 0.00001938
Iteration 157/1000 | Loss: 0.00001938
Iteration 158/1000 | Loss: 0.00001938
Iteration 159/1000 | Loss: 0.00001938
Iteration 160/1000 | Loss: 0.00001938
Iteration 161/1000 | Loss: 0.00001937
Iteration 162/1000 | Loss: 0.00001937
Iteration 163/1000 | Loss: 0.00001937
Iteration 164/1000 | Loss: 0.00001937
Iteration 165/1000 | Loss: 0.00001937
Iteration 166/1000 | Loss: 0.00001937
Iteration 167/1000 | Loss: 0.00001936
Iteration 168/1000 | Loss: 0.00001936
Iteration 169/1000 | Loss: 0.00001936
Iteration 170/1000 | Loss: 0.00001936
Iteration 171/1000 | Loss: 0.00001936
Iteration 172/1000 | Loss: 0.00001936
Iteration 173/1000 | Loss: 0.00001936
Iteration 174/1000 | Loss: 0.00001936
Iteration 175/1000 | Loss: 0.00001935
Iteration 176/1000 | Loss: 0.00001935
Iteration 177/1000 | Loss: 0.00001935
Iteration 178/1000 | Loss: 0.00001935
Iteration 179/1000 | Loss: 0.00001935
Iteration 180/1000 | Loss: 0.00001935
Iteration 181/1000 | Loss: 0.00001935
Iteration 182/1000 | Loss: 0.00001935
Iteration 183/1000 | Loss: 0.00001935
Iteration 184/1000 | Loss: 0.00001935
Iteration 185/1000 | Loss: 0.00001934
Iteration 186/1000 | Loss: 0.00001934
Iteration 187/1000 | Loss: 0.00001934
Iteration 188/1000 | Loss: 0.00001934
Iteration 189/1000 | Loss: 0.00001934
Iteration 190/1000 | Loss: 0.00001933
Iteration 191/1000 | Loss: 0.00001933
Iteration 192/1000 | Loss: 0.00001933
Iteration 193/1000 | Loss: 0.00001932
Iteration 194/1000 | Loss: 0.00001932
Iteration 195/1000 | Loss: 0.00001932
Iteration 196/1000 | Loss: 0.00001932
Iteration 197/1000 | Loss: 0.00001931
Iteration 198/1000 | Loss: 0.00001931
Iteration 199/1000 | Loss: 0.00001931
Iteration 200/1000 | Loss: 0.00001931
Iteration 201/1000 | Loss: 0.00001931
Iteration 202/1000 | Loss: 0.00001931
Iteration 203/1000 | Loss: 0.00001930
Iteration 204/1000 | Loss: 0.00001930
Iteration 205/1000 | Loss: 0.00001930
Iteration 206/1000 | Loss: 0.00001930
Iteration 207/1000 | Loss: 0.00001930
Iteration 208/1000 | Loss: 0.00001930
Iteration 209/1000 | Loss: 0.00001930
Iteration 210/1000 | Loss: 0.00001930
Iteration 211/1000 | Loss: 0.00001930
Iteration 212/1000 | Loss: 0.00001929
Iteration 213/1000 | Loss: 0.00001929
Iteration 214/1000 | Loss: 0.00001929
Iteration 215/1000 | Loss: 0.00001929
Iteration 216/1000 | Loss: 0.00001929
Iteration 217/1000 | Loss: 0.00001929
Iteration 218/1000 | Loss: 0.00001929
Iteration 219/1000 | Loss: 0.00001929
Iteration 220/1000 | Loss: 0.00001929
Iteration 221/1000 | Loss: 0.00001929
Iteration 222/1000 | Loss: 0.00001929
Iteration 223/1000 | Loss: 0.00001929
Iteration 224/1000 | Loss: 0.00001929
Iteration 225/1000 | Loss: 0.00001929
Iteration 226/1000 | Loss: 0.00001929
Iteration 227/1000 | Loss: 0.00001929
Iteration 228/1000 | Loss: 0.00001929
Iteration 229/1000 | Loss: 0.00001929
Iteration 230/1000 | Loss: 0.00001929
Iteration 231/1000 | Loss: 0.00001929
Iteration 232/1000 | Loss: 0.00001929
Iteration 233/1000 | Loss: 0.00001928
Iteration 234/1000 | Loss: 0.00001928
Iteration 235/1000 | Loss: 0.00001928
Iteration 236/1000 | Loss: 0.00001928
Iteration 237/1000 | Loss: 0.00001928
Iteration 238/1000 | Loss: 0.00001928
Iteration 239/1000 | Loss: 0.00001928
Iteration 240/1000 | Loss: 0.00001928
Iteration 241/1000 | Loss: 0.00001928
Iteration 242/1000 | Loss: 0.00001928
Iteration 243/1000 | Loss: 0.00001928
Iteration 244/1000 | Loss: 0.00001928
Iteration 245/1000 | Loss: 0.00001928
Iteration 246/1000 | Loss: 0.00001927
Iteration 247/1000 | Loss: 0.00001927
Iteration 248/1000 | Loss: 0.00001927
Iteration 249/1000 | Loss: 0.00001927
Iteration 250/1000 | Loss: 0.00001927
Iteration 251/1000 | Loss: 0.00001927
Iteration 252/1000 | Loss: 0.00001927
Iteration 253/1000 | Loss: 0.00001927
Iteration 254/1000 | Loss: 0.00001927
Iteration 255/1000 | Loss: 0.00001927
Iteration 256/1000 | Loss: 0.00001926
Iteration 257/1000 | Loss: 0.00001926
Iteration 258/1000 | Loss: 0.00001926
Iteration 259/1000 | Loss: 0.00001926
Iteration 260/1000 | Loss: 0.00001926
Iteration 261/1000 | Loss: 0.00001926
Iteration 262/1000 | Loss: 0.00001926
Iteration 263/1000 | Loss: 0.00001926
Iteration 264/1000 | Loss: 0.00001926
Iteration 265/1000 | Loss: 0.00001926
Iteration 266/1000 | Loss: 0.00001926
Iteration 267/1000 | Loss: 0.00001926
Iteration 268/1000 | Loss: 0.00001926
Iteration 269/1000 | Loss: 0.00001926
Iteration 270/1000 | Loss: 0.00001926
Iteration 271/1000 | Loss: 0.00001926
Iteration 272/1000 | Loss: 0.00001926
Iteration 273/1000 | Loss: 0.00001926
Iteration 274/1000 | Loss: 0.00001926
Iteration 275/1000 | Loss: 0.00001926
Iteration 276/1000 | Loss: 0.00001926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 276. Stopping optimization.
Last 5 losses: [1.925576725625433e-05, 1.925576725625433e-05, 1.925576725625433e-05, 1.925576725625433e-05, 1.925576725625433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.925576725625433e-05

Optimization complete. Final v2v error: 3.7059926986694336 mm

Highest mean error: 3.944692611694336 mm for frame 66

Lowest mean error: 3.5953571796417236 mm for frame 118

Saving results

Total time: 41.99230742454529
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00275534
Iteration 2/25 | Loss: 0.00128188
Iteration 3/25 | Loss: 0.00108162
Iteration 4/25 | Loss: 0.00105102
Iteration 5/25 | Loss: 0.00104175
Iteration 6/25 | Loss: 0.00103776
Iteration 7/25 | Loss: 0.00103676
Iteration 8/25 | Loss: 0.00103676
Iteration 9/25 | Loss: 0.00103676
Iteration 10/25 | Loss: 0.00103676
Iteration 11/25 | Loss: 0.00103676
Iteration 12/25 | Loss: 0.00103676
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010367551585659385, 0.0010367551585659385, 0.0010367551585659385, 0.0010367551585659385, 0.0010367551585659385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010367551585659385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34004176
Iteration 2/25 | Loss: 0.00084948
Iteration 3/25 | Loss: 0.00084948
Iteration 4/25 | Loss: 0.00084948
Iteration 5/25 | Loss: 0.00084948
Iteration 6/25 | Loss: 0.00084948
Iteration 7/25 | Loss: 0.00084948
Iteration 8/25 | Loss: 0.00084948
Iteration 9/25 | Loss: 0.00084948
Iteration 10/25 | Loss: 0.00084948
Iteration 11/25 | Loss: 0.00084948
Iteration 12/25 | Loss: 0.00084948
Iteration 13/25 | Loss: 0.00084948
Iteration 14/25 | Loss: 0.00084948
Iteration 15/25 | Loss: 0.00084948
Iteration 16/25 | Loss: 0.00084948
Iteration 17/25 | Loss: 0.00084948
Iteration 18/25 | Loss: 0.00084948
Iteration 19/25 | Loss: 0.00084948
Iteration 20/25 | Loss: 0.00084948
Iteration 21/25 | Loss: 0.00084948
Iteration 22/25 | Loss: 0.00084948
Iteration 23/25 | Loss: 0.00084948
Iteration 24/25 | Loss: 0.00084948
Iteration 25/25 | Loss: 0.00084948

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084948
Iteration 2/1000 | Loss: 0.00003731
Iteration 3/1000 | Loss: 0.00002308
Iteration 4/1000 | Loss: 0.00001712
Iteration 5/1000 | Loss: 0.00001565
Iteration 6/1000 | Loss: 0.00001445
Iteration 7/1000 | Loss: 0.00001369
Iteration 8/1000 | Loss: 0.00001313
Iteration 9/1000 | Loss: 0.00001283
Iteration 10/1000 | Loss: 0.00001254
Iteration 11/1000 | Loss: 0.00001232
Iteration 12/1000 | Loss: 0.00001223
Iteration 13/1000 | Loss: 0.00001205
Iteration 14/1000 | Loss: 0.00001203
Iteration 15/1000 | Loss: 0.00001202
Iteration 16/1000 | Loss: 0.00001197
Iteration 17/1000 | Loss: 0.00001196
Iteration 18/1000 | Loss: 0.00001193
Iteration 19/1000 | Loss: 0.00001192
Iteration 20/1000 | Loss: 0.00001191
Iteration 21/1000 | Loss: 0.00001191
Iteration 22/1000 | Loss: 0.00001190
Iteration 23/1000 | Loss: 0.00001189
Iteration 24/1000 | Loss: 0.00001188
Iteration 25/1000 | Loss: 0.00001187
Iteration 26/1000 | Loss: 0.00001187
Iteration 27/1000 | Loss: 0.00001186
Iteration 28/1000 | Loss: 0.00001186
Iteration 29/1000 | Loss: 0.00001185
Iteration 30/1000 | Loss: 0.00001185
Iteration 31/1000 | Loss: 0.00001184
Iteration 32/1000 | Loss: 0.00001184
Iteration 33/1000 | Loss: 0.00001183
Iteration 34/1000 | Loss: 0.00001183
Iteration 35/1000 | Loss: 0.00001182
Iteration 36/1000 | Loss: 0.00001182
Iteration 37/1000 | Loss: 0.00001181
Iteration 38/1000 | Loss: 0.00001180
Iteration 39/1000 | Loss: 0.00001179
Iteration 40/1000 | Loss: 0.00001179
Iteration 41/1000 | Loss: 0.00001179
Iteration 42/1000 | Loss: 0.00001178
Iteration 43/1000 | Loss: 0.00001178
Iteration 44/1000 | Loss: 0.00001177
Iteration 45/1000 | Loss: 0.00001177
Iteration 46/1000 | Loss: 0.00001176
Iteration 47/1000 | Loss: 0.00001176
Iteration 48/1000 | Loss: 0.00001176
Iteration 49/1000 | Loss: 0.00001175
Iteration 50/1000 | Loss: 0.00001175
Iteration 51/1000 | Loss: 0.00001175
Iteration 52/1000 | Loss: 0.00001175
Iteration 53/1000 | Loss: 0.00001175
Iteration 54/1000 | Loss: 0.00001174
Iteration 55/1000 | Loss: 0.00001174
Iteration 56/1000 | Loss: 0.00001173
Iteration 57/1000 | Loss: 0.00001173
Iteration 58/1000 | Loss: 0.00001173
Iteration 59/1000 | Loss: 0.00001173
Iteration 60/1000 | Loss: 0.00001173
Iteration 61/1000 | Loss: 0.00001173
Iteration 62/1000 | Loss: 0.00001172
Iteration 63/1000 | Loss: 0.00001172
Iteration 64/1000 | Loss: 0.00001172
Iteration 65/1000 | Loss: 0.00001171
Iteration 66/1000 | Loss: 0.00001171
Iteration 67/1000 | Loss: 0.00001171
Iteration 68/1000 | Loss: 0.00001170
Iteration 69/1000 | Loss: 0.00001170
Iteration 70/1000 | Loss: 0.00001170
Iteration 71/1000 | Loss: 0.00001170
Iteration 72/1000 | Loss: 0.00001170
Iteration 73/1000 | Loss: 0.00001169
Iteration 74/1000 | Loss: 0.00001169
Iteration 75/1000 | Loss: 0.00001169
Iteration 76/1000 | Loss: 0.00001168
Iteration 77/1000 | Loss: 0.00001168
Iteration 78/1000 | Loss: 0.00001168
Iteration 79/1000 | Loss: 0.00001167
Iteration 80/1000 | Loss: 0.00001167
Iteration 81/1000 | Loss: 0.00001167
Iteration 82/1000 | Loss: 0.00001166
Iteration 83/1000 | Loss: 0.00001166
Iteration 84/1000 | Loss: 0.00001166
Iteration 85/1000 | Loss: 0.00001166
Iteration 86/1000 | Loss: 0.00001166
Iteration 87/1000 | Loss: 0.00001165
Iteration 88/1000 | Loss: 0.00001165
Iteration 89/1000 | Loss: 0.00001165
Iteration 90/1000 | Loss: 0.00001165
Iteration 91/1000 | Loss: 0.00001165
Iteration 92/1000 | Loss: 0.00001164
Iteration 93/1000 | Loss: 0.00001164
Iteration 94/1000 | Loss: 0.00001163
Iteration 95/1000 | Loss: 0.00001163
Iteration 96/1000 | Loss: 0.00001163
Iteration 97/1000 | Loss: 0.00001162
Iteration 98/1000 | Loss: 0.00001162
Iteration 99/1000 | Loss: 0.00001162
Iteration 100/1000 | Loss: 0.00001161
Iteration 101/1000 | Loss: 0.00001161
Iteration 102/1000 | Loss: 0.00001161
Iteration 103/1000 | Loss: 0.00001160
Iteration 104/1000 | Loss: 0.00001160
Iteration 105/1000 | Loss: 0.00001160
Iteration 106/1000 | Loss: 0.00001160
Iteration 107/1000 | Loss: 0.00001159
Iteration 108/1000 | Loss: 0.00001159
Iteration 109/1000 | Loss: 0.00001159
Iteration 110/1000 | Loss: 0.00001159
Iteration 111/1000 | Loss: 0.00001158
Iteration 112/1000 | Loss: 0.00001158
Iteration 113/1000 | Loss: 0.00001158
Iteration 114/1000 | Loss: 0.00001158
Iteration 115/1000 | Loss: 0.00001157
Iteration 116/1000 | Loss: 0.00001157
Iteration 117/1000 | Loss: 0.00001157
Iteration 118/1000 | Loss: 0.00001157
Iteration 119/1000 | Loss: 0.00001157
Iteration 120/1000 | Loss: 0.00001156
Iteration 121/1000 | Loss: 0.00001156
Iteration 122/1000 | Loss: 0.00001156
Iteration 123/1000 | Loss: 0.00001156
Iteration 124/1000 | Loss: 0.00001155
Iteration 125/1000 | Loss: 0.00001155
Iteration 126/1000 | Loss: 0.00001155
Iteration 127/1000 | Loss: 0.00001155
Iteration 128/1000 | Loss: 0.00001155
Iteration 129/1000 | Loss: 0.00001155
Iteration 130/1000 | Loss: 0.00001155
Iteration 131/1000 | Loss: 0.00001155
Iteration 132/1000 | Loss: 0.00001155
Iteration 133/1000 | Loss: 0.00001155
Iteration 134/1000 | Loss: 0.00001155
Iteration 135/1000 | Loss: 0.00001155
Iteration 136/1000 | Loss: 0.00001155
Iteration 137/1000 | Loss: 0.00001154
Iteration 138/1000 | Loss: 0.00001154
Iteration 139/1000 | Loss: 0.00001154
Iteration 140/1000 | Loss: 0.00001154
Iteration 141/1000 | Loss: 0.00001154
Iteration 142/1000 | Loss: 0.00001154
Iteration 143/1000 | Loss: 0.00001154
Iteration 144/1000 | Loss: 0.00001154
Iteration 145/1000 | Loss: 0.00001154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.1543531400093343e-05, 1.1543531400093343e-05, 1.1543531400093343e-05, 1.1543531400093343e-05, 1.1543531400093343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1543531400093343e-05

Optimization complete. Final v2v error: 2.9274072647094727 mm

Highest mean error: 3.100715398788452 mm for frame 19

Lowest mean error: 2.5694100856781006 mm for frame 0

Saving results

Total time: 44.49139213562012
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395246
Iteration 2/25 | Loss: 0.00114791
Iteration 3/25 | Loss: 0.00108305
Iteration 4/25 | Loss: 0.00107656
Iteration 5/25 | Loss: 0.00107342
Iteration 6/25 | Loss: 0.00107342
Iteration 7/25 | Loss: 0.00107342
Iteration 8/25 | Loss: 0.00107342
Iteration 9/25 | Loss: 0.00107342
Iteration 10/25 | Loss: 0.00107342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001073421910405159, 0.001073421910405159, 0.001073421910405159, 0.001073421910405159, 0.001073421910405159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001073421910405159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66410697
Iteration 2/25 | Loss: 0.00061158
Iteration 3/25 | Loss: 0.00061153
Iteration 4/25 | Loss: 0.00061153
Iteration 5/25 | Loss: 0.00061153
Iteration 6/25 | Loss: 0.00061153
Iteration 7/25 | Loss: 0.00061153
Iteration 8/25 | Loss: 0.00061153
Iteration 9/25 | Loss: 0.00061153
Iteration 10/25 | Loss: 0.00061153
Iteration 11/25 | Loss: 0.00061153
Iteration 12/25 | Loss: 0.00061153
Iteration 13/25 | Loss: 0.00061153
Iteration 14/25 | Loss: 0.00061153
Iteration 15/25 | Loss: 0.00061153
Iteration 16/25 | Loss: 0.00061153
Iteration 17/25 | Loss: 0.00061153
Iteration 18/25 | Loss: 0.00061153
Iteration 19/25 | Loss: 0.00061153
Iteration 20/25 | Loss: 0.00061153
Iteration 21/25 | Loss: 0.00061153
Iteration 22/25 | Loss: 0.00061153
Iteration 23/25 | Loss: 0.00061153
Iteration 24/25 | Loss: 0.00061153
Iteration 25/25 | Loss: 0.00061153
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006115272990427911, 0.0006115272990427911, 0.0006115272990427911, 0.0006115272990427911, 0.0006115272990427911]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006115272990427911

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061153
Iteration 2/1000 | Loss: 0.00002160
Iteration 3/1000 | Loss: 0.00001346
Iteration 4/1000 | Loss: 0.00001214
Iteration 5/1000 | Loss: 0.00001143
Iteration 6/1000 | Loss: 0.00001100
Iteration 7/1000 | Loss: 0.00001085
Iteration 8/1000 | Loss: 0.00001085
Iteration 9/1000 | Loss: 0.00001083
Iteration 10/1000 | Loss: 0.00001074
Iteration 11/1000 | Loss: 0.00001054
Iteration 12/1000 | Loss: 0.00001033
Iteration 13/1000 | Loss: 0.00001022
Iteration 14/1000 | Loss: 0.00001021
Iteration 15/1000 | Loss: 0.00001020
Iteration 16/1000 | Loss: 0.00001019
Iteration 17/1000 | Loss: 0.00001018
Iteration 18/1000 | Loss: 0.00001017
Iteration 19/1000 | Loss: 0.00001014
Iteration 20/1000 | Loss: 0.00000997
Iteration 21/1000 | Loss: 0.00000996
Iteration 22/1000 | Loss: 0.00000983
Iteration 23/1000 | Loss: 0.00000982
Iteration 24/1000 | Loss: 0.00000982
Iteration 25/1000 | Loss: 0.00000980
Iteration 26/1000 | Loss: 0.00000980
Iteration 27/1000 | Loss: 0.00000979
Iteration 28/1000 | Loss: 0.00000979
Iteration 29/1000 | Loss: 0.00000979
Iteration 30/1000 | Loss: 0.00000979
Iteration 31/1000 | Loss: 0.00000979
Iteration 32/1000 | Loss: 0.00000978
Iteration 33/1000 | Loss: 0.00000978
Iteration 34/1000 | Loss: 0.00000978
Iteration 35/1000 | Loss: 0.00000978
Iteration 36/1000 | Loss: 0.00000978
Iteration 37/1000 | Loss: 0.00000978
Iteration 38/1000 | Loss: 0.00000977
Iteration 39/1000 | Loss: 0.00000977
Iteration 40/1000 | Loss: 0.00000977
Iteration 41/1000 | Loss: 0.00000977
Iteration 42/1000 | Loss: 0.00000977
Iteration 43/1000 | Loss: 0.00000976
Iteration 44/1000 | Loss: 0.00000976
Iteration 45/1000 | Loss: 0.00000976
Iteration 46/1000 | Loss: 0.00000975
Iteration 47/1000 | Loss: 0.00000975
Iteration 48/1000 | Loss: 0.00000975
Iteration 49/1000 | Loss: 0.00000975
Iteration 50/1000 | Loss: 0.00000975
Iteration 51/1000 | Loss: 0.00000975
Iteration 52/1000 | Loss: 0.00000975
Iteration 53/1000 | Loss: 0.00000974
Iteration 54/1000 | Loss: 0.00000974
Iteration 55/1000 | Loss: 0.00000974
Iteration 56/1000 | Loss: 0.00000974
Iteration 57/1000 | Loss: 0.00000973
Iteration 58/1000 | Loss: 0.00000973
Iteration 59/1000 | Loss: 0.00000973
Iteration 60/1000 | Loss: 0.00000972
Iteration 61/1000 | Loss: 0.00000972
Iteration 62/1000 | Loss: 0.00000972
Iteration 63/1000 | Loss: 0.00000972
Iteration 64/1000 | Loss: 0.00000971
Iteration 65/1000 | Loss: 0.00000971
Iteration 66/1000 | Loss: 0.00000971
Iteration 67/1000 | Loss: 0.00000971
Iteration 68/1000 | Loss: 0.00000971
Iteration 69/1000 | Loss: 0.00000971
Iteration 70/1000 | Loss: 0.00000971
Iteration 71/1000 | Loss: 0.00000971
Iteration 72/1000 | Loss: 0.00000970
Iteration 73/1000 | Loss: 0.00000970
Iteration 74/1000 | Loss: 0.00000970
Iteration 75/1000 | Loss: 0.00000969
Iteration 76/1000 | Loss: 0.00000969
Iteration 77/1000 | Loss: 0.00000968
Iteration 78/1000 | Loss: 0.00000968
Iteration 79/1000 | Loss: 0.00000967
Iteration 80/1000 | Loss: 0.00000967
Iteration 81/1000 | Loss: 0.00000967
Iteration 82/1000 | Loss: 0.00000967
Iteration 83/1000 | Loss: 0.00000966
Iteration 84/1000 | Loss: 0.00000966
Iteration 85/1000 | Loss: 0.00000966
Iteration 86/1000 | Loss: 0.00000966
Iteration 87/1000 | Loss: 0.00000965
Iteration 88/1000 | Loss: 0.00000965
Iteration 89/1000 | Loss: 0.00000965
Iteration 90/1000 | Loss: 0.00000965
Iteration 91/1000 | Loss: 0.00000965
Iteration 92/1000 | Loss: 0.00000965
Iteration 93/1000 | Loss: 0.00000965
Iteration 94/1000 | Loss: 0.00000965
Iteration 95/1000 | Loss: 0.00000965
Iteration 96/1000 | Loss: 0.00000965
Iteration 97/1000 | Loss: 0.00000964
Iteration 98/1000 | Loss: 0.00000964
Iteration 99/1000 | Loss: 0.00000964
Iteration 100/1000 | Loss: 0.00000964
Iteration 101/1000 | Loss: 0.00000964
Iteration 102/1000 | Loss: 0.00000964
Iteration 103/1000 | Loss: 0.00000964
Iteration 104/1000 | Loss: 0.00000964
Iteration 105/1000 | Loss: 0.00000964
Iteration 106/1000 | Loss: 0.00000964
Iteration 107/1000 | Loss: 0.00000964
Iteration 108/1000 | Loss: 0.00000964
Iteration 109/1000 | Loss: 0.00000964
Iteration 110/1000 | Loss: 0.00000964
Iteration 111/1000 | Loss: 0.00000964
Iteration 112/1000 | Loss: 0.00000964
Iteration 113/1000 | Loss: 0.00000963
Iteration 114/1000 | Loss: 0.00000963
Iteration 115/1000 | Loss: 0.00000963
Iteration 116/1000 | Loss: 0.00000963
Iteration 117/1000 | Loss: 0.00000963
Iteration 118/1000 | Loss: 0.00000963
Iteration 119/1000 | Loss: 0.00000963
Iteration 120/1000 | Loss: 0.00000963
Iteration 121/1000 | Loss: 0.00000962
Iteration 122/1000 | Loss: 0.00000962
Iteration 123/1000 | Loss: 0.00000962
Iteration 124/1000 | Loss: 0.00000962
Iteration 125/1000 | Loss: 0.00000962
Iteration 126/1000 | Loss: 0.00000961
Iteration 127/1000 | Loss: 0.00000961
Iteration 128/1000 | Loss: 0.00000961
Iteration 129/1000 | Loss: 0.00000961
Iteration 130/1000 | Loss: 0.00000961
Iteration 131/1000 | Loss: 0.00000961
Iteration 132/1000 | Loss: 0.00000961
Iteration 133/1000 | Loss: 0.00000961
Iteration 134/1000 | Loss: 0.00000961
Iteration 135/1000 | Loss: 0.00000961
Iteration 136/1000 | Loss: 0.00000960
Iteration 137/1000 | Loss: 0.00000960
Iteration 138/1000 | Loss: 0.00000960
Iteration 139/1000 | Loss: 0.00000960
Iteration 140/1000 | Loss: 0.00000960
Iteration 141/1000 | Loss: 0.00000959
Iteration 142/1000 | Loss: 0.00000959
Iteration 143/1000 | Loss: 0.00000959
Iteration 144/1000 | Loss: 0.00000959
Iteration 145/1000 | Loss: 0.00000959
Iteration 146/1000 | Loss: 0.00000959
Iteration 147/1000 | Loss: 0.00000959
Iteration 148/1000 | Loss: 0.00000959
Iteration 149/1000 | Loss: 0.00000959
Iteration 150/1000 | Loss: 0.00000959
Iteration 151/1000 | Loss: 0.00000959
Iteration 152/1000 | Loss: 0.00000959
Iteration 153/1000 | Loss: 0.00000959
Iteration 154/1000 | Loss: 0.00000959
Iteration 155/1000 | Loss: 0.00000959
Iteration 156/1000 | Loss: 0.00000959
Iteration 157/1000 | Loss: 0.00000959
Iteration 158/1000 | Loss: 0.00000959
Iteration 159/1000 | Loss: 0.00000959
Iteration 160/1000 | Loss: 0.00000959
Iteration 161/1000 | Loss: 0.00000959
Iteration 162/1000 | Loss: 0.00000959
Iteration 163/1000 | Loss: 0.00000959
Iteration 164/1000 | Loss: 0.00000959
Iteration 165/1000 | Loss: 0.00000959
Iteration 166/1000 | Loss: 0.00000959
Iteration 167/1000 | Loss: 0.00000959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [9.585376574250404e-06, 9.585376574250404e-06, 9.585376574250404e-06, 9.585376574250404e-06, 9.585376574250404e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.585376574250404e-06

Optimization complete. Final v2v error: 2.6612415313720703 mm

Highest mean error: 2.8715178966522217 mm for frame 2

Lowest mean error: 2.491997241973877 mm for frame 37

Saving results

Total time: 39.016352891922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00511822
Iteration 2/25 | Loss: 0.00114948
Iteration 3/25 | Loss: 0.00107122
Iteration 4/25 | Loss: 0.00105870
Iteration 5/25 | Loss: 0.00105466
Iteration 6/25 | Loss: 0.00105440
Iteration 7/25 | Loss: 0.00105440
Iteration 8/25 | Loss: 0.00105440
Iteration 9/25 | Loss: 0.00105440
Iteration 10/25 | Loss: 0.00105440
Iteration 11/25 | Loss: 0.00105440
Iteration 12/25 | Loss: 0.00105440
Iteration 13/25 | Loss: 0.00105440
Iteration 14/25 | Loss: 0.00105440
Iteration 15/25 | Loss: 0.00105440
Iteration 16/25 | Loss: 0.00105440
Iteration 17/25 | Loss: 0.00105440
Iteration 18/25 | Loss: 0.00105440
Iteration 19/25 | Loss: 0.00105440
Iteration 20/25 | Loss: 0.00105440
Iteration 21/25 | Loss: 0.00105440
Iteration 22/25 | Loss: 0.00105440
Iteration 23/25 | Loss: 0.00105440
Iteration 24/25 | Loss: 0.00105440
Iteration 25/25 | Loss: 0.00105440

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.41016817
Iteration 2/25 | Loss: 0.00071454
Iteration 3/25 | Loss: 0.00071454
Iteration 4/25 | Loss: 0.00071454
Iteration 5/25 | Loss: 0.00071454
Iteration 6/25 | Loss: 0.00071453
Iteration 7/25 | Loss: 0.00071453
Iteration 8/25 | Loss: 0.00071453
Iteration 9/25 | Loss: 0.00071453
Iteration 10/25 | Loss: 0.00071453
Iteration 11/25 | Loss: 0.00071453
Iteration 12/25 | Loss: 0.00071453
Iteration 13/25 | Loss: 0.00071453
Iteration 14/25 | Loss: 0.00071453
Iteration 15/25 | Loss: 0.00071453
Iteration 16/25 | Loss: 0.00071453
Iteration 17/25 | Loss: 0.00071453
Iteration 18/25 | Loss: 0.00071453
Iteration 19/25 | Loss: 0.00071453
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007145327399484813, 0.0007145327399484813, 0.0007145327399484813, 0.0007145327399484813, 0.0007145327399484813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007145327399484813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071453
Iteration 2/1000 | Loss: 0.00002022
Iteration 3/1000 | Loss: 0.00001514
Iteration 4/1000 | Loss: 0.00001381
Iteration 5/1000 | Loss: 0.00001293
Iteration 6/1000 | Loss: 0.00001233
Iteration 7/1000 | Loss: 0.00001198
Iteration 8/1000 | Loss: 0.00001165
Iteration 9/1000 | Loss: 0.00001135
Iteration 10/1000 | Loss: 0.00001131
Iteration 11/1000 | Loss: 0.00001131
Iteration 12/1000 | Loss: 0.00001121
Iteration 13/1000 | Loss: 0.00001109
Iteration 14/1000 | Loss: 0.00001105
Iteration 15/1000 | Loss: 0.00001105
Iteration 16/1000 | Loss: 0.00001104
Iteration 17/1000 | Loss: 0.00001103
Iteration 18/1000 | Loss: 0.00001103
Iteration 19/1000 | Loss: 0.00001102
Iteration 20/1000 | Loss: 0.00001102
Iteration 21/1000 | Loss: 0.00001101
Iteration 22/1000 | Loss: 0.00001100
Iteration 23/1000 | Loss: 0.00001100
Iteration 24/1000 | Loss: 0.00001094
Iteration 25/1000 | Loss: 0.00001091
Iteration 26/1000 | Loss: 0.00001088
Iteration 27/1000 | Loss: 0.00001081
Iteration 28/1000 | Loss: 0.00001080
Iteration 29/1000 | Loss: 0.00001079
Iteration 30/1000 | Loss: 0.00001078
Iteration 31/1000 | Loss: 0.00001076
Iteration 32/1000 | Loss: 0.00001073
Iteration 33/1000 | Loss: 0.00001071
Iteration 34/1000 | Loss: 0.00001071
Iteration 35/1000 | Loss: 0.00001068
Iteration 36/1000 | Loss: 0.00001068
Iteration 37/1000 | Loss: 0.00001068
Iteration 38/1000 | Loss: 0.00001068
Iteration 39/1000 | Loss: 0.00001067
Iteration 40/1000 | Loss: 0.00001066
Iteration 41/1000 | Loss: 0.00001066
Iteration 42/1000 | Loss: 0.00001065
Iteration 43/1000 | Loss: 0.00001065
Iteration 44/1000 | Loss: 0.00001063
Iteration 45/1000 | Loss: 0.00001063
Iteration 46/1000 | Loss: 0.00001062
Iteration 47/1000 | Loss: 0.00001062
Iteration 48/1000 | Loss: 0.00001060
Iteration 49/1000 | Loss: 0.00001060
Iteration 50/1000 | Loss: 0.00001060
Iteration 51/1000 | Loss: 0.00001059
Iteration 52/1000 | Loss: 0.00001059
Iteration 53/1000 | Loss: 0.00001058
Iteration 54/1000 | Loss: 0.00001057
Iteration 55/1000 | Loss: 0.00001057
Iteration 56/1000 | Loss: 0.00001057
Iteration 57/1000 | Loss: 0.00001056
Iteration 58/1000 | Loss: 0.00001055
Iteration 59/1000 | Loss: 0.00001055
Iteration 60/1000 | Loss: 0.00001054
Iteration 61/1000 | Loss: 0.00001054
Iteration 62/1000 | Loss: 0.00001054
Iteration 63/1000 | Loss: 0.00001053
Iteration 64/1000 | Loss: 0.00001052
Iteration 65/1000 | Loss: 0.00001052
Iteration 66/1000 | Loss: 0.00001051
Iteration 67/1000 | Loss: 0.00001051
Iteration 68/1000 | Loss: 0.00001051
Iteration 69/1000 | Loss: 0.00001051
Iteration 70/1000 | Loss: 0.00001051
Iteration 71/1000 | Loss: 0.00001051
Iteration 72/1000 | Loss: 0.00001050
Iteration 73/1000 | Loss: 0.00001049
Iteration 74/1000 | Loss: 0.00001049
Iteration 75/1000 | Loss: 0.00001048
Iteration 76/1000 | Loss: 0.00001047
Iteration 77/1000 | Loss: 0.00001047
Iteration 78/1000 | Loss: 0.00001047
Iteration 79/1000 | Loss: 0.00001046
Iteration 80/1000 | Loss: 0.00001046
Iteration 81/1000 | Loss: 0.00001046
Iteration 82/1000 | Loss: 0.00001046
Iteration 83/1000 | Loss: 0.00001046
Iteration 84/1000 | Loss: 0.00001046
Iteration 85/1000 | Loss: 0.00001044
Iteration 86/1000 | Loss: 0.00001044
Iteration 87/1000 | Loss: 0.00001044
Iteration 88/1000 | Loss: 0.00001043
Iteration 89/1000 | Loss: 0.00001043
Iteration 90/1000 | Loss: 0.00001043
Iteration 91/1000 | Loss: 0.00001042
Iteration 92/1000 | Loss: 0.00001042
Iteration 93/1000 | Loss: 0.00001042
Iteration 94/1000 | Loss: 0.00001039
Iteration 95/1000 | Loss: 0.00001038
Iteration 96/1000 | Loss: 0.00001038
Iteration 97/1000 | Loss: 0.00001037
Iteration 98/1000 | Loss: 0.00001037
Iteration 99/1000 | Loss: 0.00001037
Iteration 100/1000 | Loss: 0.00001037
Iteration 101/1000 | Loss: 0.00001037
Iteration 102/1000 | Loss: 0.00001037
Iteration 103/1000 | Loss: 0.00001036
Iteration 104/1000 | Loss: 0.00001036
Iteration 105/1000 | Loss: 0.00001036
Iteration 106/1000 | Loss: 0.00001036
Iteration 107/1000 | Loss: 0.00001036
Iteration 108/1000 | Loss: 0.00001036
Iteration 109/1000 | Loss: 0.00001036
Iteration 110/1000 | Loss: 0.00001036
Iteration 111/1000 | Loss: 0.00001035
Iteration 112/1000 | Loss: 0.00001035
Iteration 113/1000 | Loss: 0.00001035
Iteration 114/1000 | Loss: 0.00001035
Iteration 115/1000 | Loss: 0.00001035
Iteration 116/1000 | Loss: 0.00001035
Iteration 117/1000 | Loss: 0.00001035
Iteration 118/1000 | Loss: 0.00001035
Iteration 119/1000 | Loss: 0.00001035
Iteration 120/1000 | Loss: 0.00001035
Iteration 121/1000 | Loss: 0.00001035
Iteration 122/1000 | Loss: 0.00001035
Iteration 123/1000 | Loss: 0.00001035
Iteration 124/1000 | Loss: 0.00001035
Iteration 125/1000 | Loss: 0.00001035
Iteration 126/1000 | Loss: 0.00001035
Iteration 127/1000 | Loss: 0.00001035
Iteration 128/1000 | Loss: 0.00001035
Iteration 129/1000 | Loss: 0.00001035
Iteration 130/1000 | Loss: 0.00001034
Iteration 131/1000 | Loss: 0.00001034
Iteration 132/1000 | Loss: 0.00001034
Iteration 133/1000 | Loss: 0.00001034
Iteration 134/1000 | Loss: 0.00001034
Iteration 135/1000 | Loss: 0.00001034
Iteration 136/1000 | Loss: 0.00001034
Iteration 137/1000 | Loss: 0.00001034
Iteration 138/1000 | Loss: 0.00001034
Iteration 139/1000 | Loss: 0.00001034
Iteration 140/1000 | Loss: 0.00001034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.0344106158299837e-05, 1.0344106158299837e-05, 1.0344106158299837e-05, 1.0344106158299837e-05, 1.0344106158299837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0344106158299837e-05

Optimization complete. Final v2v error: 2.795614004135132 mm

Highest mean error: 3.0233728885650635 mm for frame 233

Lowest mean error: 2.5565624237060547 mm for frame 9

Saving results

Total time: 43.258034467697144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772209
Iteration 2/25 | Loss: 0.00211821
Iteration 3/25 | Loss: 0.00155281
Iteration 4/25 | Loss: 0.00145242
Iteration 5/25 | Loss: 0.00141969
Iteration 6/25 | Loss: 0.00133323
Iteration 7/25 | Loss: 0.00129439
Iteration 8/25 | Loss: 0.00123926
Iteration 9/25 | Loss: 0.00123283
Iteration 10/25 | Loss: 0.00122150
Iteration 11/25 | Loss: 0.00121375
Iteration 12/25 | Loss: 0.00120452
Iteration 13/25 | Loss: 0.00120749
Iteration 14/25 | Loss: 0.00119265
Iteration 15/25 | Loss: 0.00118840
Iteration 16/25 | Loss: 0.00118717
Iteration 17/25 | Loss: 0.00118681
Iteration 18/25 | Loss: 0.00118673
Iteration 19/25 | Loss: 0.00118672
Iteration 20/25 | Loss: 0.00118672
Iteration 21/25 | Loss: 0.00118672
Iteration 22/25 | Loss: 0.00118672
Iteration 23/25 | Loss: 0.00118672
Iteration 24/25 | Loss: 0.00118672
Iteration 25/25 | Loss: 0.00118672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.15431309
Iteration 2/25 | Loss: 0.00069576
Iteration 3/25 | Loss: 0.00069562
Iteration 4/25 | Loss: 0.00069562
Iteration 5/25 | Loss: 0.00069562
Iteration 6/25 | Loss: 0.00069562
Iteration 7/25 | Loss: 0.00069562
Iteration 8/25 | Loss: 0.00069561
Iteration 9/25 | Loss: 0.00069561
Iteration 10/25 | Loss: 0.00069561
Iteration 11/25 | Loss: 0.00069561
Iteration 12/25 | Loss: 0.00069561
Iteration 13/25 | Loss: 0.00069561
Iteration 14/25 | Loss: 0.00069561
Iteration 15/25 | Loss: 0.00069561
Iteration 16/25 | Loss: 0.00069561
Iteration 17/25 | Loss: 0.00069561
Iteration 18/25 | Loss: 0.00069561
Iteration 19/25 | Loss: 0.00069561
Iteration 20/25 | Loss: 0.00069561
Iteration 21/25 | Loss: 0.00069561
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006956136785447598, 0.0006956136785447598, 0.0006956136785447598, 0.0006956136785447598, 0.0006956136785447598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006956136785447598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069561
Iteration 2/1000 | Loss: 0.00003474
Iteration 3/1000 | Loss: 0.00017458
Iteration 4/1000 | Loss: 0.00002195
Iteration 5/1000 | Loss: 0.00002082
Iteration 6/1000 | Loss: 0.00002008
Iteration 7/1000 | Loss: 0.00001963
Iteration 8/1000 | Loss: 0.00001932
Iteration 9/1000 | Loss: 0.00001907
Iteration 10/1000 | Loss: 0.00001889
Iteration 11/1000 | Loss: 0.00001873
Iteration 12/1000 | Loss: 0.00001872
Iteration 13/1000 | Loss: 0.00001871
Iteration 14/1000 | Loss: 0.00001871
Iteration 15/1000 | Loss: 0.00001866
Iteration 16/1000 | Loss: 0.00001861
Iteration 17/1000 | Loss: 0.00001861
Iteration 18/1000 | Loss: 0.00001860
Iteration 19/1000 | Loss: 0.00001859
Iteration 20/1000 | Loss: 0.00001853
Iteration 21/1000 | Loss: 0.00001851
Iteration 22/1000 | Loss: 0.00001848
Iteration 23/1000 | Loss: 0.00001848
Iteration 24/1000 | Loss: 0.00001848
Iteration 25/1000 | Loss: 0.00001848
Iteration 26/1000 | Loss: 0.00001848
Iteration 27/1000 | Loss: 0.00001848
Iteration 28/1000 | Loss: 0.00001848
Iteration 29/1000 | Loss: 0.00001847
Iteration 30/1000 | Loss: 0.00001847
Iteration 31/1000 | Loss: 0.00001847
Iteration 32/1000 | Loss: 0.00001847
Iteration 33/1000 | Loss: 0.00001847
Iteration 34/1000 | Loss: 0.00001847
Iteration 35/1000 | Loss: 0.00001847
Iteration 36/1000 | Loss: 0.00001847
Iteration 37/1000 | Loss: 0.00001844
Iteration 38/1000 | Loss: 0.00001844
Iteration 39/1000 | Loss: 0.00001844
Iteration 40/1000 | Loss: 0.00001844
Iteration 41/1000 | Loss: 0.00001844
Iteration 42/1000 | Loss: 0.00001844
Iteration 43/1000 | Loss: 0.00001844
Iteration 44/1000 | Loss: 0.00001843
Iteration 45/1000 | Loss: 0.00001843
Iteration 46/1000 | Loss: 0.00001843
Iteration 47/1000 | Loss: 0.00001843
Iteration 48/1000 | Loss: 0.00001843
Iteration 49/1000 | Loss: 0.00001843
Iteration 50/1000 | Loss: 0.00001843
Iteration 51/1000 | Loss: 0.00001843
Iteration 52/1000 | Loss: 0.00001842
Iteration 53/1000 | Loss: 0.00001842
Iteration 54/1000 | Loss: 0.00001842
Iteration 55/1000 | Loss: 0.00001842
Iteration 56/1000 | Loss: 0.00001842
Iteration 57/1000 | Loss: 0.00001841
Iteration 58/1000 | Loss: 0.00001841
Iteration 59/1000 | Loss: 0.00001841
Iteration 60/1000 | Loss: 0.00001841
Iteration 61/1000 | Loss: 0.00001841
Iteration 62/1000 | Loss: 0.00001840
Iteration 63/1000 | Loss: 0.00001840
Iteration 64/1000 | Loss: 0.00001839
Iteration 65/1000 | Loss: 0.00001839
Iteration 66/1000 | Loss: 0.00001839
Iteration 67/1000 | Loss: 0.00001839
Iteration 68/1000 | Loss: 0.00001839
Iteration 69/1000 | Loss: 0.00001839
Iteration 70/1000 | Loss: 0.00001839
Iteration 71/1000 | Loss: 0.00001838
Iteration 72/1000 | Loss: 0.00001838
Iteration 73/1000 | Loss: 0.00001838
Iteration 74/1000 | Loss: 0.00001838
Iteration 75/1000 | Loss: 0.00001838
Iteration 76/1000 | Loss: 0.00001837
Iteration 77/1000 | Loss: 0.00001837
Iteration 78/1000 | Loss: 0.00001837
Iteration 79/1000 | Loss: 0.00001837
Iteration 80/1000 | Loss: 0.00001837
Iteration 81/1000 | Loss: 0.00001837
Iteration 82/1000 | Loss: 0.00001836
Iteration 83/1000 | Loss: 0.00001836
Iteration 84/1000 | Loss: 0.00001836
Iteration 85/1000 | Loss: 0.00001836
Iteration 86/1000 | Loss: 0.00001836
Iteration 87/1000 | Loss: 0.00001836
Iteration 88/1000 | Loss: 0.00001835
Iteration 89/1000 | Loss: 0.00001835
Iteration 90/1000 | Loss: 0.00001835
Iteration 91/1000 | Loss: 0.00001835
Iteration 92/1000 | Loss: 0.00001835
Iteration 93/1000 | Loss: 0.00001834
Iteration 94/1000 | Loss: 0.00001834
Iteration 95/1000 | Loss: 0.00001834
Iteration 96/1000 | Loss: 0.00001834
Iteration 97/1000 | Loss: 0.00001834
Iteration 98/1000 | Loss: 0.00001833
Iteration 99/1000 | Loss: 0.00001833
Iteration 100/1000 | Loss: 0.00001833
Iteration 101/1000 | Loss: 0.00001833
Iteration 102/1000 | Loss: 0.00001833
Iteration 103/1000 | Loss: 0.00001833
Iteration 104/1000 | Loss: 0.00001833
Iteration 105/1000 | Loss: 0.00001832
Iteration 106/1000 | Loss: 0.00001832
Iteration 107/1000 | Loss: 0.00001832
Iteration 108/1000 | Loss: 0.00001832
Iteration 109/1000 | Loss: 0.00001831
Iteration 110/1000 | Loss: 0.00001831
Iteration 111/1000 | Loss: 0.00001831
Iteration 112/1000 | Loss: 0.00001831
Iteration 113/1000 | Loss: 0.00001831
Iteration 114/1000 | Loss: 0.00001830
Iteration 115/1000 | Loss: 0.00001830
Iteration 116/1000 | Loss: 0.00001830
Iteration 117/1000 | Loss: 0.00001830
Iteration 118/1000 | Loss: 0.00001829
Iteration 119/1000 | Loss: 0.00001829
Iteration 120/1000 | Loss: 0.00001829
Iteration 121/1000 | Loss: 0.00001829
Iteration 122/1000 | Loss: 0.00001828
Iteration 123/1000 | Loss: 0.00001828
Iteration 124/1000 | Loss: 0.00001828
Iteration 125/1000 | Loss: 0.00001828
Iteration 126/1000 | Loss: 0.00001828
Iteration 127/1000 | Loss: 0.00001828
Iteration 128/1000 | Loss: 0.00001828
Iteration 129/1000 | Loss: 0.00001828
Iteration 130/1000 | Loss: 0.00001828
Iteration 131/1000 | Loss: 0.00001828
Iteration 132/1000 | Loss: 0.00001828
Iteration 133/1000 | Loss: 0.00001828
Iteration 134/1000 | Loss: 0.00001828
Iteration 135/1000 | Loss: 0.00001827
Iteration 136/1000 | Loss: 0.00001827
Iteration 137/1000 | Loss: 0.00001827
Iteration 138/1000 | Loss: 0.00001827
Iteration 139/1000 | Loss: 0.00001827
Iteration 140/1000 | Loss: 0.00001827
Iteration 141/1000 | Loss: 0.00001827
Iteration 142/1000 | Loss: 0.00001826
Iteration 143/1000 | Loss: 0.00001826
Iteration 144/1000 | Loss: 0.00001826
Iteration 145/1000 | Loss: 0.00001825
Iteration 146/1000 | Loss: 0.00001825
Iteration 147/1000 | Loss: 0.00001825
Iteration 148/1000 | Loss: 0.00001825
Iteration 149/1000 | Loss: 0.00001825
Iteration 150/1000 | Loss: 0.00001824
Iteration 151/1000 | Loss: 0.00001824
Iteration 152/1000 | Loss: 0.00001823
Iteration 153/1000 | Loss: 0.00001823
Iteration 154/1000 | Loss: 0.00001823
Iteration 155/1000 | Loss: 0.00001823
Iteration 156/1000 | Loss: 0.00001823
Iteration 157/1000 | Loss: 0.00001823
Iteration 158/1000 | Loss: 0.00001823
Iteration 159/1000 | Loss: 0.00001823
Iteration 160/1000 | Loss: 0.00001822
Iteration 161/1000 | Loss: 0.00001822
Iteration 162/1000 | Loss: 0.00001822
Iteration 163/1000 | Loss: 0.00001821
Iteration 164/1000 | Loss: 0.00001821
Iteration 165/1000 | Loss: 0.00001821
Iteration 166/1000 | Loss: 0.00001821
Iteration 167/1000 | Loss: 0.00001821
Iteration 168/1000 | Loss: 0.00001821
Iteration 169/1000 | Loss: 0.00001821
Iteration 170/1000 | Loss: 0.00001821
Iteration 171/1000 | Loss: 0.00001821
Iteration 172/1000 | Loss: 0.00001821
Iteration 173/1000 | Loss: 0.00001821
Iteration 174/1000 | Loss: 0.00001821
Iteration 175/1000 | Loss: 0.00001821
Iteration 176/1000 | Loss: 0.00001821
Iteration 177/1000 | Loss: 0.00001821
Iteration 178/1000 | Loss: 0.00001821
Iteration 179/1000 | Loss: 0.00001821
Iteration 180/1000 | Loss: 0.00001820
Iteration 181/1000 | Loss: 0.00001820
Iteration 182/1000 | Loss: 0.00001820
Iteration 183/1000 | Loss: 0.00001820
Iteration 184/1000 | Loss: 0.00001820
Iteration 185/1000 | Loss: 0.00001820
Iteration 186/1000 | Loss: 0.00001820
Iteration 187/1000 | Loss: 0.00001820
Iteration 188/1000 | Loss: 0.00001820
Iteration 189/1000 | Loss: 0.00001820
Iteration 190/1000 | Loss: 0.00001820
Iteration 191/1000 | Loss: 0.00001819
Iteration 192/1000 | Loss: 0.00001819
Iteration 193/1000 | Loss: 0.00001819
Iteration 194/1000 | Loss: 0.00001819
Iteration 195/1000 | Loss: 0.00001819
Iteration 196/1000 | Loss: 0.00001819
Iteration 197/1000 | Loss: 0.00001819
Iteration 198/1000 | Loss: 0.00001819
Iteration 199/1000 | Loss: 0.00001819
Iteration 200/1000 | Loss: 0.00001819
Iteration 201/1000 | Loss: 0.00001819
Iteration 202/1000 | Loss: 0.00001819
Iteration 203/1000 | Loss: 0.00001818
Iteration 204/1000 | Loss: 0.00001818
Iteration 205/1000 | Loss: 0.00001818
Iteration 206/1000 | Loss: 0.00001818
Iteration 207/1000 | Loss: 0.00001818
Iteration 208/1000 | Loss: 0.00001818
Iteration 209/1000 | Loss: 0.00001818
Iteration 210/1000 | Loss: 0.00001818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.818469536374323e-05, 1.818469536374323e-05, 1.818469536374323e-05, 1.818469536374323e-05, 1.818469536374323e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.818469536374323e-05

Optimization complete. Final v2v error: 3.5149049758911133 mm

Highest mean error: 4.005443572998047 mm for frame 7

Lowest mean error: 3.183065414428711 mm for frame 148

Saving results

Total time: 62.378910779953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790834
Iteration 2/25 | Loss: 0.00128916
Iteration 3/25 | Loss: 0.00110931
Iteration 4/25 | Loss: 0.00109312
Iteration 5/25 | Loss: 0.00108929
Iteration 6/25 | Loss: 0.00108854
Iteration 7/25 | Loss: 0.00108854
Iteration 8/25 | Loss: 0.00108854
Iteration 9/25 | Loss: 0.00108854
Iteration 10/25 | Loss: 0.00108854
Iteration 11/25 | Loss: 0.00108854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010885404190048575, 0.0010885404190048575, 0.0010885404190048575, 0.0010885404190048575, 0.0010885404190048575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010885404190048575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37402904
Iteration 2/25 | Loss: 0.00065891
Iteration 3/25 | Loss: 0.00065891
Iteration 4/25 | Loss: 0.00065891
Iteration 5/25 | Loss: 0.00065891
Iteration 6/25 | Loss: 0.00065891
Iteration 7/25 | Loss: 0.00065891
Iteration 8/25 | Loss: 0.00065891
Iteration 9/25 | Loss: 0.00065891
Iteration 10/25 | Loss: 0.00065891
Iteration 11/25 | Loss: 0.00065891
Iteration 12/25 | Loss: 0.00065891
Iteration 13/25 | Loss: 0.00065891
Iteration 14/25 | Loss: 0.00065891
Iteration 15/25 | Loss: 0.00065891
Iteration 16/25 | Loss: 0.00065891
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006589067634195089, 0.0006589067634195089, 0.0006589067634195089, 0.0006589067634195089, 0.0006589067634195089]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006589067634195089

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065891
Iteration 2/1000 | Loss: 0.00003232
Iteration 3/1000 | Loss: 0.00002111
Iteration 4/1000 | Loss: 0.00001863
Iteration 5/1000 | Loss: 0.00001752
Iteration 6/1000 | Loss: 0.00001669
Iteration 7/1000 | Loss: 0.00001611
Iteration 8/1000 | Loss: 0.00001558
Iteration 9/1000 | Loss: 0.00001519
Iteration 10/1000 | Loss: 0.00001492
Iteration 11/1000 | Loss: 0.00001470
Iteration 12/1000 | Loss: 0.00001462
Iteration 13/1000 | Loss: 0.00001447
Iteration 14/1000 | Loss: 0.00001441
Iteration 15/1000 | Loss: 0.00001436
Iteration 16/1000 | Loss: 0.00001436
Iteration 17/1000 | Loss: 0.00001430
Iteration 18/1000 | Loss: 0.00001430
Iteration 19/1000 | Loss: 0.00001428
Iteration 20/1000 | Loss: 0.00001423
Iteration 21/1000 | Loss: 0.00001422
Iteration 22/1000 | Loss: 0.00001421
Iteration 23/1000 | Loss: 0.00001421
Iteration 24/1000 | Loss: 0.00001420
Iteration 25/1000 | Loss: 0.00001420
Iteration 26/1000 | Loss: 0.00001420
Iteration 27/1000 | Loss: 0.00001419
Iteration 28/1000 | Loss: 0.00001419
Iteration 29/1000 | Loss: 0.00001419
Iteration 30/1000 | Loss: 0.00001418
Iteration 31/1000 | Loss: 0.00001418
Iteration 32/1000 | Loss: 0.00001417
Iteration 33/1000 | Loss: 0.00001417
Iteration 34/1000 | Loss: 0.00001417
Iteration 35/1000 | Loss: 0.00001417
Iteration 36/1000 | Loss: 0.00001417
Iteration 37/1000 | Loss: 0.00001416
Iteration 38/1000 | Loss: 0.00001416
Iteration 39/1000 | Loss: 0.00001416
Iteration 40/1000 | Loss: 0.00001416
Iteration 41/1000 | Loss: 0.00001415
Iteration 42/1000 | Loss: 0.00001415
Iteration 43/1000 | Loss: 0.00001415
Iteration 44/1000 | Loss: 0.00001415
Iteration 45/1000 | Loss: 0.00001414
Iteration 46/1000 | Loss: 0.00001414
Iteration 47/1000 | Loss: 0.00001414
Iteration 48/1000 | Loss: 0.00001413
Iteration 49/1000 | Loss: 0.00001413
Iteration 50/1000 | Loss: 0.00001413
Iteration 51/1000 | Loss: 0.00001413
Iteration 52/1000 | Loss: 0.00001413
Iteration 53/1000 | Loss: 0.00001412
Iteration 54/1000 | Loss: 0.00001412
Iteration 55/1000 | Loss: 0.00001412
Iteration 56/1000 | Loss: 0.00001412
Iteration 57/1000 | Loss: 0.00001412
Iteration 58/1000 | Loss: 0.00001412
Iteration 59/1000 | Loss: 0.00001411
Iteration 60/1000 | Loss: 0.00001411
Iteration 61/1000 | Loss: 0.00001411
Iteration 62/1000 | Loss: 0.00001411
Iteration 63/1000 | Loss: 0.00001410
Iteration 64/1000 | Loss: 0.00001410
Iteration 65/1000 | Loss: 0.00001410
Iteration 66/1000 | Loss: 0.00001410
Iteration 67/1000 | Loss: 0.00001410
Iteration 68/1000 | Loss: 0.00001410
Iteration 69/1000 | Loss: 0.00001410
Iteration 70/1000 | Loss: 0.00001410
Iteration 71/1000 | Loss: 0.00001409
Iteration 72/1000 | Loss: 0.00001409
Iteration 73/1000 | Loss: 0.00001409
Iteration 74/1000 | Loss: 0.00001409
Iteration 75/1000 | Loss: 0.00001409
Iteration 76/1000 | Loss: 0.00001409
Iteration 77/1000 | Loss: 0.00001409
Iteration 78/1000 | Loss: 0.00001408
Iteration 79/1000 | Loss: 0.00001408
Iteration 80/1000 | Loss: 0.00001408
Iteration 81/1000 | Loss: 0.00001408
Iteration 82/1000 | Loss: 0.00001408
Iteration 83/1000 | Loss: 0.00001407
Iteration 84/1000 | Loss: 0.00001407
Iteration 85/1000 | Loss: 0.00001407
Iteration 86/1000 | Loss: 0.00001407
Iteration 87/1000 | Loss: 0.00001407
Iteration 88/1000 | Loss: 0.00001406
Iteration 89/1000 | Loss: 0.00001406
Iteration 90/1000 | Loss: 0.00001406
Iteration 91/1000 | Loss: 0.00001406
Iteration 92/1000 | Loss: 0.00001405
Iteration 93/1000 | Loss: 0.00001405
Iteration 94/1000 | Loss: 0.00001405
Iteration 95/1000 | Loss: 0.00001405
Iteration 96/1000 | Loss: 0.00001405
Iteration 97/1000 | Loss: 0.00001405
Iteration 98/1000 | Loss: 0.00001405
Iteration 99/1000 | Loss: 0.00001405
Iteration 100/1000 | Loss: 0.00001405
Iteration 101/1000 | Loss: 0.00001405
Iteration 102/1000 | Loss: 0.00001405
Iteration 103/1000 | Loss: 0.00001404
Iteration 104/1000 | Loss: 0.00001404
Iteration 105/1000 | Loss: 0.00001404
Iteration 106/1000 | Loss: 0.00001404
Iteration 107/1000 | Loss: 0.00001404
Iteration 108/1000 | Loss: 0.00001404
Iteration 109/1000 | Loss: 0.00001404
Iteration 110/1000 | Loss: 0.00001403
Iteration 111/1000 | Loss: 0.00001403
Iteration 112/1000 | Loss: 0.00001403
Iteration 113/1000 | Loss: 0.00001403
Iteration 114/1000 | Loss: 0.00001403
Iteration 115/1000 | Loss: 0.00001403
Iteration 116/1000 | Loss: 0.00001403
Iteration 117/1000 | Loss: 0.00001402
Iteration 118/1000 | Loss: 0.00001402
Iteration 119/1000 | Loss: 0.00001402
Iteration 120/1000 | Loss: 0.00001402
Iteration 121/1000 | Loss: 0.00001402
Iteration 122/1000 | Loss: 0.00001402
Iteration 123/1000 | Loss: 0.00001402
Iteration 124/1000 | Loss: 0.00001402
Iteration 125/1000 | Loss: 0.00001402
Iteration 126/1000 | Loss: 0.00001401
Iteration 127/1000 | Loss: 0.00001401
Iteration 128/1000 | Loss: 0.00001401
Iteration 129/1000 | Loss: 0.00001401
Iteration 130/1000 | Loss: 0.00001401
Iteration 131/1000 | Loss: 0.00001401
Iteration 132/1000 | Loss: 0.00001401
Iteration 133/1000 | Loss: 0.00001401
Iteration 134/1000 | Loss: 0.00001401
Iteration 135/1000 | Loss: 0.00001401
Iteration 136/1000 | Loss: 0.00001401
Iteration 137/1000 | Loss: 0.00001401
Iteration 138/1000 | Loss: 0.00001401
Iteration 139/1000 | Loss: 0.00001401
Iteration 140/1000 | Loss: 0.00001401
Iteration 141/1000 | Loss: 0.00001400
Iteration 142/1000 | Loss: 0.00001400
Iteration 143/1000 | Loss: 0.00001400
Iteration 144/1000 | Loss: 0.00001400
Iteration 145/1000 | Loss: 0.00001400
Iteration 146/1000 | Loss: 0.00001400
Iteration 147/1000 | Loss: 0.00001400
Iteration 148/1000 | Loss: 0.00001400
Iteration 149/1000 | Loss: 0.00001400
Iteration 150/1000 | Loss: 0.00001400
Iteration 151/1000 | Loss: 0.00001400
Iteration 152/1000 | Loss: 0.00001400
Iteration 153/1000 | Loss: 0.00001400
Iteration 154/1000 | Loss: 0.00001400
Iteration 155/1000 | Loss: 0.00001400
Iteration 156/1000 | Loss: 0.00001400
Iteration 157/1000 | Loss: 0.00001400
Iteration 158/1000 | Loss: 0.00001400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.4002977877680678e-05, 1.4002977877680678e-05, 1.4002977877680678e-05, 1.4002977877680678e-05, 1.4002977877680678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4002977877680678e-05

Optimization complete. Final v2v error: 3.1317217350006104 mm

Highest mean error: 3.838268518447876 mm for frame 88

Lowest mean error: 2.7282803058624268 mm for frame 55

Saving results

Total time: 37.410969495773315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786551
Iteration 2/25 | Loss: 0.00150252
Iteration 3/25 | Loss: 0.00129949
Iteration 4/25 | Loss: 0.00121755
Iteration 5/25 | Loss: 0.00127236
Iteration 6/25 | Loss: 0.00123642
Iteration 7/25 | Loss: 0.00121132
Iteration 8/25 | Loss: 0.00118877
Iteration 9/25 | Loss: 0.00119203
Iteration 10/25 | Loss: 0.00117899
Iteration 11/25 | Loss: 0.00118737
Iteration 12/25 | Loss: 0.00116353
Iteration 13/25 | Loss: 0.00115348
Iteration 14/25 | Loss: 0.00115010
Iteration 15/25 | Loss: 0.00114339
Iteration 16/25 | Loss: 0.00115651
Iteration 17/25 | Loss: 0.00114452
Iteration 18/25 | Loss: 0.00113189
Iteration 19/25 | Loss: 0.00113324
Iteration 20/25 | Loss: 0.00113165
Iteration 21/25 | Loss: 0.00113371
Iteration 22/25 | Loss: 0.00113387
Iteration 23/25 | Loss: 0.00113303
Iteration 24/25 | Loss: 0.00113263
Iteration 25/25 | Loss: 0.00113131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.23536634
Iteration 2/25 | Loss: 0.00098585
Iteration 3/25 | Loss: 0.00098583
Iteration 4/25 | Loss: 0.00098583
Iteration 5/25 | Loss: 0.00098583
Iteration 6/25 | Loss: 0.00098583
Iteration 7/25 | Loss: 0.00098583
Iteration 8/25 | Loss: 0.00098582
Iteration 9/25 | Loss: 0.00098582
Iteration 10/25 | Loss: 0.00098582
Iteration 11/25 | Loss: 0.00098582
Iteration 12/25 | Loss: 0.00098582
Iteration 13/25 | Loss: 0.00098582
Iteration 14/25 | Loss: 0.00098582
Iteration 15/25 | Loss: 0.00098582
Iteration 16/25 | Loss: 0.00098582
Iteration 17/25 | Loss: 0.00098582
Iteration 18/25 | Loss: 0.00098582
Iteration 19/25 | Loss: 0.00098582
Iteration 20/25 | Loss: 0.00098582
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009858239209279418, 0.0009858239209279418, 0.0009858239209279418, 0.0009858239209279418, 0.0009858239209279418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009858239209279418

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098582
Iteration 2/1000 | Loss: 0.00022762
Iteration 3/1000 | Loss: 0.00015108
Iteration 4/1000 | Loss: 0.00008426
Iteration 5/1000 | Loss: 0.00017402
Iteration 6/1000 | Loss: 0.00013505
Iteration 7/1000 | Loss: 0.00015012
Iteration 8/1000 | Loss: 0.00015432
Iteration 9/1000 | Loss: 0.00029801
Iteration 10/1000 | Loss: 0.00005861
Iteration 11/1000 | Loss: 0.00015804
Iteration 12/1000 | Loss: 0.00017651
Iteration 13/1000 | Loss: 0.00041012
Iteration 14/1000 | Loss: 0.00011227
Iteration 15/1000 | Loss: 0.00016388
Iteration 16/1000 | Loss: 0.00026058
Iteration 17/1000 | Loss: 0.00021870
Iteration 18/1000 | Loss: 0.00019254
Iteration 19/1000 | Loss: 0.00013026
Iteration 20/1000 | Loss: 0.00029086
Iteration 21/1000 | Loss: 0.00019633
Iteration 22/1000 | Loss: 0.00008874
Iteration 23/1000 | Loss: 0.00017127
Iteration 24/1000 | Loss: 0.00011168
Iteration 25/1000 | Loss: 0.00020457
Iteration 26/1000 | Loss: 0.00010599
Iteration 27/1000 | Loss: 0.00008324
Iteration 28/1000 | Loss: 0.00008351
Iteration 29/1000 | Loss: 0.00022352
Iteration 30/1000 | Loss: 0.00020351
Iteration 31/1000 | Loss: 0.00019143
Iteration 32/1000 | Loss: 0.00007335
Iteration 33/1000 | Loss: 0.00022398
Iteration 34/1000 | Loss: 0.00006737
Iteration 35/1000 | Loss: 0.00028029
Iteration 36/1000 | Loss: 0.00014005
Iteration 37/1000 | Loss: 0.00015233
Iteration 38/1000 | Loss: 0.00006370
Iteration 39/1000 | Loss: 0.00013279
Iteration 40/1000 | Loss: 0.00007834
Iteration 41/1000 | Loss: 0.00004486
Iteration 42/1000 | Loss: 0.00002746
Iteration 43/1000 | Loss: 0.00003449
Iteration 44/1000 | Loss: 0.00003392
Iteration 45/1000 | Loss: 0.00003010
Iteration 46/1000 | Loss: 0.00003873
Iteration 47/1000 | Loss: 0.00003044
Iteration 48/1000 | Loss: 0.00003434
Iteration 49/1000 | Loss: 0.00002817
Iteration 50/1000 | Loss: 0.00003362
Iteration 51/1000 | Loss: 0.00003653
Iteration 52/1000 | Loss: 0.00004680
Iteration 53/1000 | Loss: 0.00003670
Iteration 54/1000 | Loss: 0.00003843
Iteration 55/1000 | Loss: 0.00003057
Iteration 56/1000 | Loss: 0.00004378
Iteration 57/1000 | Loss: 0.00003495
Iteration 58/1000 | Loss: 0.00003914
Iteration 59/1000 | Loss: 0.00005051
Iteration 60/1000 | Loss: 0.00004324
Iteration 61/1000 | Loss: 0.00005082
Iteration 62/1000 | Loss: 0.00003758
Iteration 63/1000 | Loss: 0.00003239
Iteration 64/1000 | Loss: 0.00003471
Iteration 65/1000 | Loss: 0.00003221
Iteration 66/1000 | Loss: 0.00003119
Iteration 67/1000 | Loss: 0.00003529
Iteration 68/1000 | Loss: 0.00003117
Iteration 69/1000 | Loss: 0.00003596
Iteration 70/1000 | Loss: 0.00003094
Iteration 71/1000 | Loss: 0.00003580
Iteration 72/1000 | Loss: 0.00003091
Iteration 73/1000 | Loss: 0.00003511
Iteration 74/1000 | Loss: 0.00003975
Iteration 75/1000 | Loss: 0.00004011
Iteration 76/1000 | Loss: 0.00003434
Iteration 77/1000 | Loss: 0.00003357
Iteration 78/1000 | Loss: 0.00003514
Iteration 79/1000 | Loss: 0.00003841
Iteration 80/1000 | Loss: 0.00002971
Iteration 81/1000 | Loss: 0.00002474
Iteration 82/1000 | Loss: 0.00002143
Iteration 83/1000 | Loss: 0.00003380
Iteration 84/1000 | Loss: 0.00003622
Iteration 85/1000 | Loss: 0.00003901
Iteration 86/1000 | Loss: 0.00003584
Iteration 87/1000 | Loss: 0.00004572
Iteration 88/1000 | Loss: 0.00004251
Iteration 89/1000 | Loss: 0.00003977
Iteration 90/1000 | Loss: 0.00003527
Iteration 91/1000 | Loss: 0.00003694
Iteration 92/1000 | Loss: 0.00003408
Iteration 93/1000 | Loss: 0.00004027
Iteration 94/1000 | Loss: 0.00003432
Iteration 95/1000 | Loss: 0.00004049
Iteration 96/1000 | Loss: 0.00004040
Iteration 97/1000 | Loss: 0.00003735
Iteration 98/1000 | Loss: 0.00003779
Iteration 99/1000 | Loss: 0.00003226
Iteration 100/1000 | Loss: 0.00003712
Iteration 101/1000 | Loss: 0.00003940
Iteration 102/1000 | Loss: 0.00003702
Iteration 103/1000 | Loss: 0.00003208
Iteration 104/1000 | Loss: 0.00002788
Iteration 105/1000 | Loss: 0.00003550
Iteration 106/1000 | Loss: 0.00002912
Iteration 107/1000 | Loss: 0.00002370
Iteration 108/1000 | Loss: 0.00003492
Iteration 109/1000 | Loss: 0.00002869
Iteration 110/1000 | Loss: 0.00003014
Iteration 111/1000 | Loss: 0.00003481
Iteration 112/1000 | Loss: 0.00003154
Iteration 113/1000 | Loss: 0.00002900
Iteration 114/1000 | Loss: 0.00003317
Iteration 115/1000 | Loss: 0.00002247
Iteration 116/1000 | Loss: 0.00003147
Iteration 117/1000 | Loss: 0.00003139
Iteration 118/1000 | Loss: 0.00003136
Iteration 119/1000 | Loss: 0.00002993
Iteration 120/1000 | Loss: 0.00002081
Iteration 121/1000 | Loss: 0.00003286
Iteration 122/1000 | Loss: 0.00003692
Iteration 123/1000 | Loss: 0.00003379
Iteration 124/1000 | Loss: 0.00002975
Iteration 125/1000 | Loss: 0.00003109
Iteration 126/1000 | Loss: 0.00002901
Iteration 127/1000 | Loss: 0.00003461
Iteration 128/1000 | Loss: 0.00002359
Iteration 129/1000 | Loss: 0.00003253
Iteration 130/1000 | Loss: 0.00003125
Iteration 131/1000 | Loss: 0.00003036
Iteration 132/1000 | Loss: 0.00002312
Iteration 133/1000 | Loss: 0.00002125
Iteration 134/1000 | Loss: 0.00002045
Iteration 135/1000 | Loss: 0.00002490
Iteration 136/1000 | Loss: 0.00002845
Iteration 137/1000 | Loss: 0.00003399
Iteration 138/1000 | Loss: 0.00003008
Iteration 139/1000 | Loss: 0.00002537
Iteration 140/1000 | Loss: 0.00003097
Iteration 141/1000 | Loss: 0.00002066
Iteration 142/1000 | Loss: 0.00001915
Iteration 143/1000 | Loss: 0.00001884
Iteration 144/1000 | Loss: 0.00001862
Iteration 145/1000 | Loss: 0.00001819
Iteration 146/1000 | Loss: 0.00001784
Iteration 147/1000 | Loss: 0.00001759
Iteration 148/1000 | Loss: 0.00001747
Iteration 149/1000 | Loss: 0.00001739
Iteration 150/1000 | Loss: 0.00001725
Iteration 151/1000 | Loss: 0.00001724
Iteration 152/1000 | Loss: 0.00001720
Iteration 153/1000 | Loss: 0.00001718
Iteration 154/1000 | Loss: 0.00001708
Iteration 155/1000 | Loss: 0.00001707
Iteration 156/1000 | Loss: 0.00001707
Iteration 157/1000 | Loss: 0.00001707
Iteration 158/1000 | Loss: 0.00001706
Iteration 159/1000 | Loss: 0.00001706
Iteration 160/1000 | Loss: 0.00001705
Iteration 161/1000 | Loss: 0.00001705
Iteration 162/1000 | Loss: 0.00001704
Iteration 163/1000 | Loss: 0.00001703
Iteration 164/1000 | Loss: 0.00001702
Iteration 165/1000 | Loss: 0.00001702
Iteration 166/1000 | Loss: 0.00001701
Iteration 167/1000 | Loss: 0.00001701
Iteration 168/1000 | Loss: 0.00001701
Iteration 169/1000 | Loss: 0.00001699
Iteration 170/1000 | Loss: 0.00001699
Iteration 171/1000 | Loss: 0.00001699
Iteration 172/1000 | Loss: 0.00001699
Iteration 173/1000 | Loss: 0.00001699
Iteration 174/1000 | Loss: 0.00001699
Iteration 175/1000 | Loss: 0.00001699
Iteration 176/1000 | Loss: 0.00001699
Iteration 177/1000 | Loss: 0.00001698
Iteration 178/1000 | Loss: 0.00001698
Iteration 179/1000 | Loss: 0.00001698
Iteration 180/1000 | Loss: 0.00001698
Iteration 181/1000 | Loss: 0.00001698
Iteration 182/1000 | Loss: 0.00001698
Iteration 183/1000 | Loss: 0.00001698
Iteration 184/1000 | Loss: 0.00001697
Iteration 185/1000 | Loss: 0.00001697
Iteration 186/1000 | Loss: 0.00001697
Iteration 187/1000 | Loss: 0.00001697
Iteration 188/1000 | Loss: 0.00001696
Iteration 189/1000 | Loss: 0.00001696
Iteration 190/1000 | Loss: 0.00001696
Iteration 191/1000 | Loss: 0.00001696
Iteration 192/1000 | Loss: 0.00001695
Iteration 193/1000 | Loss: 0.00001695
Iteration 194/1000 | Loss: 0.00001695
Iteration 195/1000 | Loss: 0.00001695
Iteration 196/1000 | Loss: 0.00001695
Iteration 197/1000 | Loss: 0.00001695
Iteration 198/1000 | Loss: 0.00001695
Iteration 199/1000 | Loss: 0.00001695
Iteration 200/1000 | Loss: 0.00001694
Iteration 201/1000 | Loss: 0.00001694
Iteration 202/1000 | Loss: 0.00001694
Iteration 203/1000 | Loss: 0.00001694
Iteration 204/1000 | Loss: 0.00001693
Iteration 205/1000 | Loss: 0.00001693
Iteration 206/1000 | Loss: 0.00001693
Iteration 207/1000 | Loss: 0.00001693
Iteration 208/1000 | Loss: 0.00001693
Iteration 209/1000 | Loss: 0.00001693
Iteration 210/1000 | Loss: 0.00001693
Iteration 211/1000 | Loss: 0.00001692
Iteration 212/1000 | Loss: 0.00001692
Iteration 213/1000 | Loss: 0.00001692
Iteration 214/1000 | Loss: 0.00001691
Iteration 215/1000 | Loss: 0.00001691
Iteration 216/1000 | Loss: 0.00001691
Iteration 217/1000 | Loss: 0.00001691
Iteration 218/1000 | Loss: 0.00001691
Iteration 219/1000 | Loss: 0.00001691
Iteration 220/1000 | Loss: 0.00001691
Iteration 221/1000 | Loss: 0.00001691
Iteration 222/1000 | Loss: 0.00001691
Iteration 223/1000 | Loss: 0.00001691
Iteration 224/1000 | Loss: 0.00001691
Iteration 225/1000 | Loss: 0.00001691
Iteration 226/1000 | Loss: 0.00001691
Iteration 227/1000 | Loss: 0.00001690
Iteration 228/1000 | Loss: 0.00001690
Iteration 229/1000 | Loss: 0.00001690
Iteration 230/1000 | Loss: 0.00001690
Iteration 231/1000 | Loss: 0.00001690
Iteration 232/1000 | Loss: 0.00001690
Iteration 233/1000 | Loss: 0.00001690
Iteration 234/1000 | Loss: 0.00001690
Iteration 235/1000 | Loss: 0.00001689
Iteration 236/1000 | Loss: 0.00001689
Iteration 237/1000 | Loss: 0.00001689
Iteration 238/1000 | Loss: 0.00001689
Iteration 239/1000 | Loss: 0.00001689
Iteration 240/1000 | Loss: 0.00001689
Iteration 241/1000 | Loss: 0.00001689
Iteration 242/1000 | Loss: 0.00001689
Iteration 243/1000 | Loss: 0.00001689
Iteration 244/1000 | Loss: 0.00001689
Iteration 245/1000 | Loss: 0.00001689
Iteration 246/1000 | Loss: 0.00001689
Iteration 247/1000 | Loss: 0.00001689
Iteration 248/1000 | Loss: 0.00001689
Iteration 249/1000 | Loss: 0.00001689
Iteration 250/1000 | Loss: 0.00001689
Iteration 251/1000 | Loss: 0.00001689
Iteration 252/1000 | Loss: 0.00001689
Iteration 253/1000 | Loss: 0.00001689
Iteration 254/1000 | Loss: 0.00001689
Iteration 255/1000 | Loss: 0.00001689
Iteration 256/1000 | Loss: 0.00001689
Iteration 257/1000 | Loss: 0.00001689
Iteration 258/1000 | Loss: 0.00001689
Iteration 259/1000 | Loss: 0.00001689
Iteration 260/1000 | Loss: 0.00001689
Iteration 261/1000 | Loss: 0.00001689
Iteration 262/1000 | Loss: 0.00001689
Iteration 263/1000 | Loss: 0.00001689
Iteration 264/1000 | Loss: 0.00001689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [1.6890924598556012e-05, 1.6890924598556012e-05, 1.6890924598556012e-05, 1.6890924598556012e-05, 1.6890924598556012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6890924598556012e-05

Optimization complete. Final v2v error: 3.2199392318725586 mm

Highest mean error: 10.531094551086426 mm for frame 237

Lowest mean error: 2.3755078315734863 mm for frame 177

Saving results

Total time: 297.5436460971832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00655263
Iteration 2/25 | Loss: 0.00112758
Iteration 3/25 | Loss: 0.00105586
Iteration 4/25 | Loss: 0.00104579
Iteration 5/25 | Loss: 0.00104309
Iteration 6/25 | Loss: 0.00104277
Iteration 7/25 | Loss: 0.00104277
Iteration 8/25 | Loss: 0.00104277
Iteration 9/25 | Loss: 0.00104277
Iteration 10/25 | Loss: 0.00104277
Iteration 11/25 | Loss: 0.00104277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010427705710753798, 0.0010427705710753798, 0.0010427705710753798, 0.0010427705710753798, 0.0010427705710753798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010427705710753798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.16482687
Iteration 2/25 | Loss: 0.00072271
Iteration 3/25 | Loss: 0.00072271
Iteration 4/25 | Loss: 0.00072271
Iteration 5/25 | Loss: 0.00072270
Iteration 6/25 | Loss: 0.00072270
Iteration 7/25 | Loss: 0.00072270
Iteration 8/25 | Loss: 0.00072270
Iteration 9/25 | Loss: 0.00072270
Iteration 10/25 | Loss: 0.00072270
Iteration 11/25 | Loss: 0.00072270
Iteration 12/25 | Loss: 0.00072270
Iteration 13/25 | Loss: 0.00072270
Iteration 14/25 | Loss: 0.00072270
Iteration 15/25 | Loss: 0.00072270
Iteration 16/25 | Loss: 0.00072270
Iteration 17/25 | Loss: 0.00072270
Iteration 18/25 | Loss: 0.00072270
Iteration 19/25 | Loss: 0.00072270
Iteration 20/25 | Loss: 0.00072270
Iteration 21/25 | Loss: 0.00072270
Iteration 22/25 | Loss: 0.00072270
Iteration 23/25 | Loss: 0.00072270
Iteration 24/25 | Loss: 0.00072270
Iteration 25/25 | Loss: 0.00072270

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072270
Iteration 2/1000 | Loss: 0.00001817
Iteration 3/1000 | Loss: 0.00001272
Iteration 4/1000 | Loss: 0.00001094
Iteration 5/1000 | Loss: 0.00001034
Iteration 6/1000 | Loss: 0.00001007
Iteration 7/1000 | Loss: 0.00000980
Iteration 8/1000 | Loss: 0.00000958
Iteration 9/1000 | Loss: 0.00000948
Iteration 10/1000 | Loss: 0.00000928
Iteration 11/1000 | Loss: 0.00000921
Iteration 12/1000 | Loss: 0.00000921
Iteration 13/1000 | Loss: 0.00000913
Iteration 14/1000 | Loss: 0.00000911
Iteration 15/1000 | Loss: 0.00000910
Iteration 16/1000 | Loss: 0.00000909
Iteration 17/1000 | Loss: 0.00000902
Iteration 18/1000 | Loss: 0.00000902
Iteration 19/1000 | Loss: 0.00000902
Iteration 20/1000 | Loss: 0.00000902
Iteration 21/1000 | Loss: 0.00000902
Iteration 22/1000 | Loss: 0.00000902
Iteration 23/1000 | Loss: 0.00000902
Iteration 24/1000 | Loss: 0.00000898
Iteration 25/1000 | Loss: 0.00000897
Iteration 26/1000 | Loss: 0.00000896
Iteration 27/1000 | Loss: 0.00000895
Iteration 28/1000 | Loss: 0.00000895
Iteration 29/1000 | Loss: 0.00000894
Iteration 30/1000 | Loss: 0.00000894
Iteration 31/1000 | Loss: 0.00000893
Iteration 32/1000 | Loss: 0.00000892
Iteration 33/1000 | Loss: 0.00000891
Iteration 34/1000 | Loss: 0.00000889
Iteration 35/1000 | Loss: 0.00000889
Iteration 36/1000 | Loss: 0.00000889
Iteration 37/1000 | Loss: 0.00000889
Iteration 38/1000 | Loss: 0.00000889
Iteration 39/1000 | Loss: 0.00000888
Iteration 40/1000 | Loss: 0.00000888
Iteration 41/1000 | Loss: 0.00000888
Iteration 42/1000 | Loss: 0.00000888
Iteration 43/1000 | Loss: 0.00000887
Iteration 44/1000 | Loss: 0.00000887
Iteration 45/1000 | Loss: 0.00000885
Iteration 46/1000 | Loss: 0.00000885
Iteration 47/1000 | Loss: 0.00000885
Iteration 48/1000 | Loss: 0.00000885
Iteration 49/1000 | Loss: 0.00000885
Iteration 50/1000 | Loss: 0.00000885
Iteration 51/1000 | Loss: 0.00000885
Iteration 52/1000 | Loss: 0.00000885
Iteration 53/1000 | Loss: 0.00000884
Iteration 54/1000 | Loss: 0.00000884
Iteration 55/1000 | Loss: 0.00000884
Iteration 56/1000 | Loss: 0.00000884
Iteration 57/1000 | Loss: 0.00000884
Iteration 58/1000 | Loss: 0.00000882
Iteration 59/1000 | Loss: 0.00000880
Iteration 60/1000 | Loss: 0.00000880
Iteration 61/1000 | Loss: 0.00000880
Iteration 62/1000 | Loss: 0.00000880
Iteration 63/1000 | Loss: 0.00000880
Iteration 64/1000 | Loss: 0.00000880
Iteration 65/1000 | Loss: 0.00000880
Iteration 66/1000 | Loss: 0.00000879
Iteration 67/1000 | Loss: 0.00000879
Iteration 68/1000 | Loss: 0.00000879
Iteration 69/1000 | Loss: 0.00000878
Iteration 70/1000 | Loss: 0.00000878
Iteration 71/1000 | Loss: 0.00000877
Iteration 72/1000 | Loss: 0.00000877
Iteration 73/1000 | Loss: 0.00000877
Iteration 74/1000 | Loss: 0.00000877
Iteration 75/1000 | Loss: 0.00000877
Iteration 76/1000 | Loss: 0.00000877
Iteration 77/1000 | Loss: 0.00000877
Iteration 78/1000 | Loss: 0.00000876
Iteration 79/1000 | Loss: 0.00000876
Iteration 80/1000 | Loss: 0.00000876
Iteration 81/1000 | Loss: 0.00000876
Iteration 82/1000 | Loss: 0.00000876
Iteration 83/1000 | Loss: 0.00000875
Iteration 84/1000 | Loss: 0.00000874
Iteration 85/1000 | Loss: 0.00000874
Iteration 86/1000 | Loss: 0.00000874
Iteration 87/1000 | Loss: 0.00000874
Iteration 88/1000 | Loss: 0.00000874
Iteration 89/1000 | Loss: 0.00000873
Iteration 90/1000 | Loss: 0.00000873
Iteration 91/1000 | Loss: 0.00000873
Iteration 92/1000 | Loss: 0.00000873
Iteration 93/1000 | Loss: 0.00000873
Iteration 94/1000 | Loss: 0.00000873
Iteration 95/1000 | Loss: 0.00000873
Iteration 96/1000 | Loss: 0.00000873
Iteration 97/1000 | Loss: 0.00000873
Iteration 98/1000 | Loss: 0.00000873
Iteration 99/1000 | Loss: 0.00000873
Iteration 100/1000 | Loss: 0.00000873
Iteration 101/1000 | Loss: 0.00000872
Iteration 102/1000 | Loss: 0.00000872
Iteration 103/1000 | Loss: 0.00000872
Iteration 104/1000 | Loss: 0.00000871
Iteration 105/1000 | Loss: 0.00000870
Iteration 106/1000 | Loss: 0.00000870
Iteration 107/1000 | Loss: 0.00000870
Iteration 108/1000 | Loss: 0.00000870
Iteration 109/1000 | Loss: 0.00000869
Iteration 110/1000 | Loss: 0.00000869
Iteration 111/1000 | Loss: 0.00000869
Iteration 112/1000 | Loss: 0.00000869
Iteration 113/1000 | Loss: 0.00000868
Iteration 114/1000 | Loss: 0.00000868
Iteration 115/1000 | Loss: 0.00000868
Iteration 116/1000 | Loss: 0.00000867
Iteration 117/1000 | Loss: 0.00000867
Iteration 118/1000 | Loss: 0.00000867
Iteration 119/1000 | Loss: 0.00000867
Iteration 120/1000 | Loss: 0.00000866
Iteration 121/1000 | Loss: 0.00000866
Iteration 122/1000 | Loss: 0.00000866
Iteration 123/1000 | Loss: 0.00000866
Iteration 124/1000 | Loss: 0.00000866
Iteration 125/1000 | Loss: 0.00000866
Iteration 126/1000 | Loss: 0.00000866
Iteration 127/1000 | Loss: 0.00000866
Iteration 128/1000 | Loss: 0.00000865
Iteration 129/1000 | Loss: 0.00000865
Iteration 130/1000 | Loss: 0.00000865
Iteration 131/1000 | Loss: 0.00000864
Iteration 132/1000 | Loss: 0.00000864
Iteration 133/1000 | Loss: 0.00000864
Iteration 134/1000 | Loss: 0.00000864
Iteration 135/1000 | Loss: 0.00000864
Iteration 136/1000 | Loss: 0.00000864
Iteration 137/1000 | Loss: 0.00000864
Iteration 138/1000 | Loss: 0.00000864
Iteration 139/1000 | Loss: 0.00000864
Iteration 140/1000 | Loss: 0.00000863
Iteration 141/1000 | Loss: 0.00000863
Iteration 142/1000 | Loss: 0.00000863
Iteration 143/1000 | Loss: 0.00000862
Iteration 144/1000 | Loss: 0.00000862
Iteration 145/1000 | Loss: 0.00000862
Iteration 146/1000 | Loss: 0.00000862
Iteration 147/1000 | Loss: 0.00000862
Iteration 148/1000 | Loss: 0.00000862
Iteration 149/1000 | Loss: 0.00000862
Iteration 150/1000 | Loss: 0.00000862
Iteration 151/1000 | Loss: 0.00000862
Iteration 152/1000 | Loss: 0.00000862
Iteration 153/1000 | Loss: 0.00000862
Iteration 154/1000 | Loss: 0.00000862
Iteration 155/1000 | Loss: 0.00000862
Iteration 156/1000 | Loss: 0.00000862
Iteration 157/1000 | Loss: 0.00000862
Iteration 158/1000 | Loss: 0.00000862
Iteration 159/1000 | Loss: 0.00000862
Iteration 160/1000 | Loss: 0.00000862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [8.618517313152552e-06, 8.618517313152552e-06, 8.618517313152552e-06, 8.618517313152552e-06, 8.618517313152552e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.618517313152552e-06

Optimization complete. Final v2v error: 2.5206167697906494 mm

Highest mean error: 2.895601272583008 mm for frame 77

Lowest mean error: 2.3388965129852295 mm for frame 105

Saving results

Total time: 35.497761249542236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009464
Iteration 2/25 | Loss: 0.01009464
Iteration 3/25 | Loss: 0.01009464
Iteration 4/25 | Loss: 0.01009463
Iteration 5/25 | Loss: 0.01009463
Iteration 6/25 | Loss: 0.01009463
Iteration 7/25 | Loss: 0.01009463
Iteration 8/25 | Loss: 0.01009463
Iteration 9/25 | Loss: 0.01009463
Iteration 10/25 | Loss: 0.01009463
Iteration 11/25 | Loss: 0.01009463
Iteration 12/25 | Loss: 0.01009463
Iteration 13/25 | Loss: 0.01009463
Iteration 14/25 | Loss: 0.01009463
Iteration 15/25 | Loss: 0.01009463
Iteration 16/25 | Loss: 0.01009462
Iteration 17/25 | Loss: 0.01009462
Iteration 18/25 | Loss: 0.01009462
Iteration 19/25 | Loss: 0.01009462
Iteration 20/25 | Loss: 0.01009462
Iteration 21/25 | Loss: 0.01009462
Iteration 22/25 | Loss: 0.01009462
Iteration 23/25 | Loss: 0.01009462
Iteration 24/25 | Loss: 0.01009462
Iteration 25/25 | Loss: 0.01009462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62041175
Iteration 2/25 | Loss: 0.11711642
Iteration 3/25 | Loss: 0.11711567
Iteration 4/25 | Loss: 0.11711567
Iteration 5/25 | Loss: 0.11711567
Iteration 6/25 | Loss: 0.11711567
Iteration 7/25 | Loss: 0.11711565
Iteration 8/25 | Loss: 0.11711567
Iteration 9/25 | Loss: 0.11711567
Iteration 10/25 | Loss: 0.11711565
Iteration 11/25 | Loss: 0.11711565
Iteration 12/25 | Loss: 0.11711565
Iteration 13/25 | Loss: 0.11711565
Iteration 14/25 | Loss: 0.11711565
Iteration 15/25 | Loss: 0.11711565
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.11711564660072327, 0.11711564660072327, 0.11711564660072327, 0.11711564660072327, 0.11711564660072327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11711564660072327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11711565
Iteration 2/1000 | Loss: 0.00322995
Iteration 3/1000 | Loss: 0.00732361
Iteration 4/1000 | Loss: 0.00247979
Iteration 5/1000 | Loss: 0.00054307
Iteration 6/1000 | Loss: 0.00018074
Iteration 7/1000 | Loss: 0.00009522
Iteration 8/1000 | Loss: 0.00006346
Iteration 9/1000 | Loss: 0.00010382
Iteration 10/1000 | Loss: 0.00006669
Iteration 11/1000 | Loss: 0.00021217
Iteration 12/1000 | Loss: 0.00026085
Iteration 13/1000 | Loss: 0.00025806
Iteration 14/1000 | Loss: 0.00002852
Iteration 15/1000 | Loss: 0.00005827
Iteration 16/1000 | Loss: 0.00013067
Iteration 17/1000 | Loss: 0.00002341
Iteration 18/1000 | Loss: 0.00006418
Iteration 19/1000 | Loss: 0.00017539
Iteration 20/1000 | Loss: 0.00006894
Iteration 21/1000 | Loss: 0.00003751
Iteration 22/1000 | Loss: 0.00061286
Iteration 23/1000 | Loss: 0.00575166
Iteration 24/1000 | Loss: 0.00007910
Iteration 25/1000 | Loss: 0.00015655
Iteration 26/1000 | Loss: 0.00002149
Iteration 27/1000 | Loss: 0.00002144
Iteration 28/1000 | Loss: 0.00007815
Iteration 29/1000 | Loss: 0.00019697
Iteration 30/1000 | Loss: 0.00029899
Iteration 31/1000 | Loss: 0.00108355
Iteration 32/1000 | Loss: 0.00009252
Iteration 33/1000 | Loss: 0.00008874
Iteration 34/1000 | Loss: 0.00013882
Iteration 35/1000 | Loss: 0.00002517
Iteration 36/1000 | Loss: 0.00002460
Iteration 37/1000 | Loss: 0.00001924
Iteration 38/1000 | Loss: 0.00002772
Iteration 39/1000 | Loss: 0.00007603
Iteration 40/1000 | Loss: 0.00003520
Iteration 41/1000 | Loss: 0.00001693
Iteration 42/1000 | Loss: 0.00006627
Iteration 43/1000 | Loss: 0.00002140
Iteration 44/1000 | Loss: 0.00001554
Iteration 45/1000 | Loss: 0.00023183
Iteration 46/1000 | Loss: 0.00027530
Iteration 47/1000 | Loss: 0.00002431
Iteration 48/1000 | Loss: 0.00009058
Iteration 49/1000 | Loss: 0.00003045
Iteration 50/1000 | Loss: 0.00001572
Iteration 51/1000 | Loss: 0.00002387
Iteration 52/1000 | Loss: 0.00019634
Iteration 53/1000 | Loss: 0.00002201
Iteration 54/1000 | Loss: 0.00002936
Iteration 55/1000 | Loss: 0.00004079
Iteration 56/1000 | Loss: 0.00013079
Iteration 57/1000 | Loss: 0.00001567
Iteration 58/1000 | Loss: 0.00001481
Iteration 59/1000 | Loss: 0.00001474
Iteration 60/1000 | Loss: 0.00002975
Iteration 61/1000 | Loss: 0.00002857
Iteration 62/1000 | Loss: 0.00004503
Iteration 63/1000 | Loss: 0.00001550
Iteration 64/1000 | Loss: 0.00002789
Iteration 65/1000 | Loss: 0.00001509
Iteration 66/1000 | Loss: 0.00001446
Iteration 67/1000 | Loss: 0.00005001
Iteration 68/1000 | Loss: 0.00001971
Iteration 69/1000 | Loss: 0.00002194
Iteration 70/1000 | Loss: 0.00001432
Iteration 71/1000 | Loss: 0.00001432
Iteration 72/1000 | Loss: 0.00001432
Iteration 73/1000 | Loss: 0.00001432
Iteration 74/1000 | Loss: 0.00001428
Iteration 75/1000 | Loss: 0.00001428
Iteration 76/1000 | Loss: 0.00001428
Iteration 77/1000 | Loss: 0.00001427
Iteration 78/1000 | Loss: 0.00001427
Iteration 79/1000 | Loss: 0.00001427
Iteration 80/1000 | Loss: 0.00001427
Iteration 81/1000 | Loss: 0.00001427
Iteration 82/1000 | Loss: 0.00001427
Iteration 83/1000 | Loss: 0.00001427
Iteration 84/1000 | Loss: 0.00001427
Iteration 85/1000 | Loss: 0.00001723
Iteration 86/1000 | Loss: 0.00002681
Iteration 87/1000 | Loss: 0.00001535
Iteration 88/1000 | Loss: 0.00001504
Iteration 89/1000 | Loss: 0.00001418
Iteration 90/1000 | Loss: 0.00001418
Iteration 91/1000 | Loss: 0.00001418
Iteration 92/1000 | Loss: 0.00001418
Iteration 93/1000 | Loss: 0.00001418
Iteration 94/1000 | Loss: 0.00001418
Iteration 95/1000 | Loss: 0.00002524
Iteration 96/1000 | Loss: 0.00002524
Iteration 97/1000 | Loss: 0.00021967
Iteration 98/1000 | Loss: 0.00009839
Iteration 99/1000 | Loss: 0.00001887
Iteration 100/1000 | Loss: 0.00004153
Iteration 101/1000 | Loss: 0.00004295
Iteration 102/1000 | Loss: 0.00002945
Iteration 103/1000 | Loss: 0.00003536
Iteration 104/1000 | Loss: 0.00006449
Iteration 105/1000 | Loss: 0.00002371
Iteration 106/1000 | Loss: 0.00002460
Iteration 107/1000 | Loss: 0.00001530
Iteration 108/1000 | Loss: 0.00001805
Iteration 109/1000 | Loss: 0.00002241
Iteration 110/1000 | Loss: 0.00017166
Iteration 111/1000 | Loss: 0.00001742
Iteration 112/1000 | Loss: 0.00001404
Iteration 113/1000 | Loss: 0.00002253
Iteration 114/1000 | Loss: 0.00002673
Iteration 115/1000 | Loss: 0.00001433
Iteration 116/1000 | Loss: 0.00001400
Iteration 117/1000 | Loss: 0.00001400
Iteration 118/1000 | Loss: 0.00001400
Iteration 119/1000 | Loss: 0.00001400
Iteration 120/1000 | Loss: 0.00001400
Iteration 121/1000 | Loss: 0.00001400
Iteration 122/1000 | Loss: 0.00001400
Iteration 123/1000 | Loss: 0.00001960
Iteration 124/1000 | Loss: 0.00002572
Iteration 125/1000 | Loss: 0.00007295
Iteration 126/1000 | Loss: 0.00002010
Iteration 127/1000 | Loss: 0.00001867
Iteration 128/1000 | Loss: 0.00002568
Iteration 129/1000 | Loss: 0.00002028
Iteration 130/1000 | Loss: 0.00001714
Iteration 131/1000 | Loss: 0.00001642
Iteration 132/1000 | Loss: 0.00002044
Iteration 133/1000 | Loss: 0.00002245
Iteration 134/1000 | Loss: 0.00001440
Iteration 135/1000 | Loss: 0.00001571
Iteration 136/1000 | Loss: 0.00001571
Iteration 137/1000 | Loss: 0.00001400
Iteration 138/1000 | Loss: 0.00001400
Iteration 139/1000 | Loss: 0.00001400
Iteration 140/1000 | Loss: 0.00001400
Iteration 141/1000 | Loss: 0.00001400
Iteration 142/1000 | Loss: 0.00001400
Iteration 143/1000 | Loss: 0.00001400
Iteration 144/1000 | Loss: 0.00001400
Iteration 145/1000 | Loss: 0.00001400
Iteration 146/1000 | Loss: 0.00001400
Iteration 147/1000 | Loss: 0.00001400
Iteration 148/1000 | Loss: 0.00001400
Iteration 149/1000 | Loss: 0.00001400
Iteration 150/1000 | Loss: 0.00001400
Iteration 151/1000 | Loss: 0.00001400
Iteration 152/1000 | Loss: 0.00001400
Iteration 153/1000 | Loss: 0.00001400
Iteration 154/1000 | Loss: 0.00001400
Iteration 155/1000 | Loss: 0.00001400
Iteration 156/1000 | Loss: 0.00001400
Iteration 157/1000 | Loss: 0.00001400
Iteration 158/1000 | Loss: 0.00001400
Iteration 159/1000 | Loss: 0.00001400
Iteration 160/1000 | Loss: 0.00001400
Iteration 161/1000 | Loss: 0.00001400
Iteration 162/1000 | Loss: 0.00001400
Iteration 163/1000 | Loss: 0.00001400
Iteration 164/1000 | Loss: 0.00001400
Iteration 165/1000 | Loss: 0.00001400
Iteration 166/1000 | Loss: 0.00001400
Iteration 167/1000 | Loss: 0.00001400
Iteration 168/1000 | Loss: 0.00001400
Iteration 169/1000 | Loss: 0.00001400
Iteration 170/1000 | Loss: 0.00001400
Iteration 171/1000 | Loss: 0.00001400
Iteration 172/1000 | Loss: 0.00001400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.4000484043208417e-05, 1.4000484043208417e-05, 1.4000484043208417e-05, 1.4000484043208417e-05, 1.4000484043208417e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4000484043208417e-05

Optimization complete. Final v2v error: 3.1610283851623535 mm

Highest mean error: 3.404512643814087 mm for frame 90

Lowest mean error: 3.0442380905151367 mm for frame 36

Saving results

Total time: 157.75544095039368
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010727
Iteration 2/25 | Loss: 0.00353166
Iteration 3/25 | Loss: 0.00218732
Iteration 4/25 | Loss: 0.00175063
Iteration 5/25 | Loss: 0.00161482
Iteration 6/25 | Loss: 0.00163223
Iteration 7/25 | Loss: 0.00157280
Iteration 8/25 | Loss: 0.00157240
Iteration 9/25 | Loss: 0.00155508
Iteration 10/25 | Loss: 0.00153514
Iteration 11/25 | Loss: 0.00152069
Iteration 12/25 | Loss: 0.00151578
Iteration 13/25 | Loss: 0.00150880
Iteration 14/25 | Loss: 0.00150836
Iteration 15/25 | Loss: 0.00150997
Iteration 16/25 | Loss: 0.00150628
Iteration 17/25 | Loss: 0.00150041
Iteration 18/25 | Loss: 0.00150262
Iteration 19/25 | Loss: 0.00149454
Iteration 20/25 | Loss: 0.00149200
Iteration 21/25 | Loss: 0.00149024
Iteration 22/25 | Loss: 0.00148930
Iteration 23/25 | Loss: 0.00148843
Iteration 24/25 | Loss: 0.00149407
Iteration 25/25 | Loss: 0.00148117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.98981524
Iteration 2/25 | Loss: 0.00505056
Iteration 3/25 | Loss: 0.00503675
Iteration 4/25 | Loss: 0.00503675
Iteration 5/25 | Loss: 0.00503675
Iteration 6/25 | Loss: 0.00503675
Iteration 7/25 | Loss: 0.00503675
Iteration 8/25 | Loss: 0.00503675
Iteration 9/25 | Loss: 0.00503675
Iteration 10/25 | Loss: 0.00503675
Iteration 11/25 | Loss: 0.00503675
Iteration 12/25 | Loss: 0.00503674
Iteration 13/25 | Loss: 0.00503674
Iteration 14/25 | Loss: 0.00503674
Iteration 15/25 | Loss: 0.00503674
Iteration 16/25 | Loss: 0.00503674
Iteration 17/25 | Loss: 0.00503674
Iteration 18/25 | Loss: 0.00503674
Iteration 19/25 | Loss: 0.00503674
Iteration 20/25 | Loss: 0.00503674
Iteration 21/25 | Loss: 0.00503674
Iteration 22/25 | Loss: 0.00503674
Iteration 23/25 | Loss: 0.00503674
Iteration 24/25 | Loss: 0.00503674
Iteration 25/25 | Loss: 0.00503674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00503674
Iteration 2/1000 | Loss: 0.00208304
Iteration 3/1000 | Loss: 0.00078684
Iteration 4/1000 | Loss: 0.00095036
Iteration 5/1000 | Loss: 0.00378452
Iteration 6/1000 | Loss: 0.00267460
Iteration 7/1000 | Loss: 0.00031803
Iteration 8/1000 | Loss: 0.00197721
Iteration 9/1000 | Loss: 0.00135783
Iteration 10/1000 | Loss: 0.00344222
Iteration 11/1000 | Loss: 0.00144387
Iteration 12/1000 | Loss: 0.00096678
Iteration 13/1000 | Loss: 0.00116536
Iteration 14/1000 | Loss: 0.00199758
Iteration 15/1000 | Loss: 0.00033989
Iteration 16/1000 | Loss: 0.00009742
Iteration 17/1000 | Loss: 0.00108182
Iteration 18/1000 | Loss: 0.00067699
Iteration 19/1000 | Loss: 0.00093845
Iteration 20/1000 | Loss: 0.00082598
Iteration 21/1000 | Loss: 0.00010078
Iteration 22/1000 | Loss: 0.00014141
Iteration 23/1000 | Loss: 0.00013063
Iteration 24/1000 | Loss: 0.00044403
Iteration 25/1000 | Loss: 0.00036874
Iteration 26/1000 | Loss: 0.00034775
Iteration 27/1000 | Loss: 0.00006307
Iteration 28/1000 | Loss: 0.00004836
Iteration 29/1000 | Loss: 0.00004295
Iteration 30/1000 | Loss: 0.00004112
Iteration 31/1000 | Loss: 0.00050181
Iteration 32/1000 | Loss: 0.00073913
Iteration 33/1000 | Loss: 0.00061525
Iteration 34/1000 | Loss: 0.00018647
Iteration 35/1000 | Loss: 0.00096642
Iteration 36/1000 | Loss: 0.00028292
Iteration 37/1000 | Loss: 0.00080630
Iteration 38/1000 | Loss: 0.00008579
Iteration 39/1000 | Loss: 0.00005285
Iteration 40/1000 | Loss: 0.00003742
Iteration 41/1000 | Loss: 0.00016592
Iteration 42/1000 | Loss: 0.00047002
Iteration 43/1000 | Loss: 0.00004562
Iteration 44/1000 | Loss: 0.00004037
Iteration 45/1000 | Loss: 0.00003616
Iteration 46/1000 | Loss: 0.00020677
Iteration 47/1000 | Loss: 0.00072686
Iteration 48/1000 | Loss: 0.00039610
Iteration 49/1000 | Loss: 0.00011965
Iteration 50/1000 | Loss: 0.00004639
Iteration 51/1000 | Loss: 0.00003802
Iteration 52/1000 | Loss: 0.00003377
Iteration 53/1000 | Loss: 0.00006423
Iteration 54/1000 | Loss: 0.00003015
Iteration 55/1000 | Loss: 0.00002843
Iteration 56/1000 | Loss: 0.00002708
Iteration 57/1000 | Loss: 0.00019720
Iteration 58/1000 | Loss: 0.00013218
Iteration 59/1000 | Loss: 0.00002681
Iteration 60/1000 | Loss: 0.00018012
Iteration 61/1000 | Loss: 0.00011882
Iteration 62/1000 | Loss: 0.00011594
Iteration 63/1000 | Loss: 0.00002773
Iteration 64/1000 | Loss: 0.00002556
Iteration 65/1000 | Loss: 0.00023393
Iteration 66/1000 | Loss: 0.00030000
Iteration 67/1000 | Loss: 0.00020185
Iteration 68/1000 | Loss: 0.00008678
Iteration 69/1000 | Loss: 0.00002464
Iteration 70/1000 | Loss: 0.00002205
Iteration 71/1000 | Loss: 0.00002052
Iteration 72/1000 | Loss: 0.00001950
Iteration 73/1000 | Loss: 0.00001887
Iteration 74/1000 | Loss: 0.00001835
Iteration 75/1000 | Loss: 0.00001795
Iteration 76/1000 | Loss: 0.00001774
Iteration 77/1000 | Loss: 0.00001771
Iteration 78/1000 | Loss: 0.00001765
Iteration 79/1000 | Loss: 0.00001758
Iteration 80/1000 | Loss: 0.00001754
Iteration 81/1000 | Loss: 0.00001750
Iteration 82/1000 | Loss: 0.00001749
Iteration 83/1000 | Loss: 0.00001740
Iteration 84/1000 | Loss: 0.00001738
Iteration 85/1000 | Loss: 0.00001738
Iteration 86/1000 | Loss: 0.00001738
Iteration 87/1000 | Loss: 0.00001738
Iteration 88/1000 | Loss: 0.00001738
Iteration 89/1000 | Loss: 0.00001738
Iteration 90/1000 | Loss: 0.00001737
Iteration 91/1000 | Loss: 0.00001737
Iteration 92/1000 | Loss: 0.00001737
Iteration 93/1000 | Loss: 0.00001737
Iteration 94/1000 | Loss: 0.00001736
Iteration 95/1000 | Loss: 0.00001736
Iteration 96/1000 | Loss: 0.00001736
Iteration 97/1000 | Loss: 0.00001735
Iteration 98/1000 | Loss: 0.00001735
Iteration 99/1000 | Loss: 0.00001735
Iteration 100/1000 | Loss: 0.00001735
Iteration 101/1000 | Loss: 0.00001735
Iteration 102/1000 | Loss: 0.00001735
Iteration 103/1000 | Loss: 0.00001735
Iteration 104/1000 | Loss: 0.00001734
Iteration 105/1000 | Loss: 0.00001734
Iteration 106/1000 | Loss: 0.00001734
Iteration 107/1000 | Loss: 0.00001734
Iteration 108/1000 | Loss: 0.00001733
Iteration 109/1000 | Loss: 0.00001733
Iteration 110/1000 | Loss: 0.00001733
Iteration 111/1000 | Loss: 0.00001732
Iteration 112/1000 | Loss: 0.00001732
Iteration 113/1000 | Loss: 0.00001732
Iteration 114/1000 | Loss: 0.00001732
Iteration 115/1000 | Loss: 0.00001731
Iteration 116/1000 | Loss: 0.00001731
Iteration 117/1000 | Loss: 0.00001731
Iteration 118/1000 | Loss: 0.00001731
Iteration 119/1000 | Loss: 0.00001731
Iteration 120/1000 | Loss: 0.00001731
Iteration 121/1000 | Loss: 0.00001730
Iteration 122/1000 | Loss: 0.00001730
Iteration 123/1000 | Loss: 0.00001730
Iteration 124/1000 | Loss: 0.00001728
Iteration 125/1000 | Loss: 0.00001728
Iteration 126/1000 | Loss: 0.00001728
Iteration 127/1000 | Loss: 0.00001727
Iteration 128/1000 | Loss: 0.00001727
Iteration 129/1000 | Loss: 0.00001727
Iteration 130/1000 | Loss: 0.00001727
Iteration 131/1000 | Loss: 0.00001727
Iteration 132/1000 | Loss: 0.00001727
Iteration 133/1000 | Loss: 0.00001727
Iteration 134/1000 | Loss: 0.00001727
Iteration 135/1000 | Loss: 0.00001727
Iteration 136/1000 | Loss: 0.00001727
Iteration 137/1000 | Loss: 0.00001726
Iteration 138/1000 | Loss: 0.00001726
Iteration 139/1000 | Loss: 0.00001725
Iteration 140/1000 | Loss: 0.00001725
Iteration 141/1000 | Loss: 0.00001725
Iteration 142/1000 | Loss: 0.00001725
Iteration 143/1000 | Loss: 0.00001725
Iteration 144/1000 | Loss: 0.00001725
Iteration 145/1000 | Loss: 0.00001725
Iteration 146/1000 | Loss: 0.00001725
Iteration 147/1000 | Loss: 0.00001724
Iteration 148/1000 | Loss: 0.00001724
Iteration 149/1000 | Loss: 0.00001724
Iteration 150/1000 | Loss: 0.00001724
Iteration 151/1000 | Loss: 0.00001724
Iteration 152/1000 | Loss: 0.00001724
Iteration 153/1000 | Loss: 0.00001724
Iteration 154/1000 | Loss: 0.00001724
Iteration 155/1000 | Loss: 0.00001724
Iteration 156/1000 | Loss: 0.00001723
Iteration 157/1000 | Loss: 0.00001723
Iteration 158/1000 | Loss: 0.00001723
Iteration 159/1000 | Loss: 0.00001722
Iteration 160/1000 | Loss: 0.00001722
Iteration 161/1000 | Loss: 0.00001722
Iteration 162/1000 | Loss: 0.00001722
Iteration 163/1000 | Loss: 0.00001722
Iteration 164/1000 | Loss: 0.00001722
Iteration 165/1000 | Loss: 0.00001722
Iteration 166/1000 | Loss: 0.00001722
Iteration 167/1000 | Loss: 0.00001722
Iteration 168/1000 | Loss: 0.00001721
Iteration 169/1000 | Loss: 0.00001721
Iteration 170/1000 | Loss: 0.00001721
Iteration 171/1000 | Loss: 0.00001721
Iteration 172/1000 | Loss: 0.00001721
Iteration 173/1000 | Loss: 0.00001721
Iteration 174/1000 | Loss: 0.00001721
Iteration 175/1000 | Loss: 0.00001721
Iteration 176/1000 | Loss: 0.00001721
Iteration 177/1000 | Loss: 0.00001721
Iteration 178/1000 | Loss: 0.00001721
Iteration 179/1000 | Loss: 0.00001720
Iteration 180/1000 | Loss: 0.00001720
Iteration 181/1000 | Loss: 0.00001720
Iteration 182/1000 | Loss: 0.00001720
Iteration 183/1000 | Loss: 0.00001720
Iteration 184/1000 | Loss: 0.00001720
Iteration 185/1000 | Loss: 0.00001720
Iteration 186/1000 | Loss: 0.00001719
Iteration 187/1000 | Loss: 0.00001719
Iteration 188/1000 | Loss: 0.00001719
Iteration 189/1000 | Loss: 0.00001719
Iteration 190/1000 | Loss: 0.00001719
Iteration 191/1000 | Loss: 0.00001719
Iteration 192/1000 | Loss: 0.00001719
Iteration 193/1000 | Loss: 0.00001719
Iteration 194/1000 | Loss: 0.00001719
Iteration 195/1000 | Loss: 0.00001719
Iteration 196/1000 | Loss: 0.00001719
Iteration 197/1000 | Loss: 0.00001719
Iteration 198/1000 | Loss: 0.00001719
Iteration 199/1000 | Loss: 0.00001719
Iteration 200/1000 | Loss: 0.00001719
Iteration 201/1000 | Loss: 0.00001719
Iteration 202/1000 | Loss: 0.00001719
Iteration 203/1000 | Loss: 0.00001719
Iteration 204/1000 | Loss: 0.00001719
Iteration 205/1000 | Loss: 0.00001718
Iteration 206/1000 | Loss: 0.00001718
Iteration 207/1000 | Loss: 0.00001718
Iteration 208/1000 | Loss: 0.00001718
Iteration 209/1000 | Loss: 0.00001718
Iteration 210/1000 | Loss: 0.00001718
Iteration 211/1000 | Loss: 0.00001718
Iteration 212/1000 | Loss: 0.00001718
Iteration 213/1000 | Loss: 0.00001718
Iteration 214/1000 | Loss: 0.00001718
Iteration 215/1000 | Loss: 0.00001718
Iteration 216/1000 | Loss: 0.00001718
Iteration 217/1000 | Loss: 0.00001718
Iteration 218/1000 | Loss: 0.00001718
Iteration 219/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.7176533219753765e-05, 1.7176533219753765e-05, 1.7176533219753765e-05, 1.7176533219753765e-05, 1.7176533219753765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7176533219753765e-05

Optimization complete. Final v2v error: 3.284123659133911 mm

Highest mean error: 4.946877956390381 mm for frame 79

Lowest mean error: 2.544929027557373 mm for frame 141

Saving results

Total time: 166.43582558631897
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00943667
Iteration 2/25 | Loss: 0.00279504
Iteration 3/25 | Loss: 0.00220780
Iteration 4/25 | Loss: 0.00215104
Iteration 5/25 | Loss: 0.00188589
Iteration 6/25 | Loss: 0.00167175
Iteration 7/25 | Loss: 0.00153855
Iteration 8/25 | Loss: 0.00153444
Iteration 9/25 | Loss: 0.00147752
Iteration 10/25 | Loss: 0.00148464
Iteration 11/25 | Loss: 0.00143172
Iteration 12/25 | Loss: 0.00138407
Iteration 13/25 | Loss: 0.00137713
Iteration 14/25 | Loss: 0.00137038
Iteration 15/25 | Loss: 0.00136863
Iteration 16/25 | Loss: 0.00136730
Iteration 17/25 | Loss: 0.00136579
Iteration 18/25 | Loss: 0.00136549
Iteration 19/25 | Loss: 0.00136516
Iteration 20/25 | Loss: 0.00136547
Iteration 21/25 | Loss: 0.00136512
Iteration 22/25 | Loss: 0.00136465
Iteration 23/25 | Loss: 0.00136493
Iteration 24/25 | Loss: 0.00136462
Iteration 25/25 | Loss: 0.00136413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37255132
Iteration 2/25 | Loss: 0.00113251
Iteration 3/25 | Loss: 0.00113251
Iteration 4/25 | Loss: 0.00113251
Iteration 5/25 | Loss: 0.00113251
Iteration 6/25 | Loss: 0.00113251
Iteration 7/25 | Loss: 0.00113251
Iteration 8/25 | Loss: 0.00113251
Iteration 9/25 | Loss: 0.00113251
Iteration 10/25 | Loss: 0.00113251
Iteration 11/25 | Loss: 0.00113251
Iteration 12/25 | Loss: 0.00113251
Iteration 13/25 | Loss: 0.00113251
Iteration 14/25 | Loss: 0.00113251
Iteration 15/25 | Loss: 0.00113251
Iteration 16/25 | Loss: 0.00113251
Iteration 17/25 | Loss: 0.00113251
Iteration 18/25 | Loss: 0.00113251
Iteration 19/25 | Loss: 0.00113251
Iteration 20/25 | Loss: 0.00113251
Iteration 21/25 | Loss: 0.00113251
Iteration 22/25 | Loss: 0.00113251
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011325107188895345, 0.0011325107188895345, 0.0011325107188895345, 0.0011325107188895345, 0.0011325107188895345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011325107188895345

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113251
Iteration 2/1000 | Loss: 0.00087449
Iteration 3/1000 | Loss: 0.00042529
Iteration 4/1000 | Loss: 0.00050377
Iteration 5/1000 | Loss: 0.00093708
Iteration 6/1000 | Loss: 0.00058383
Iteration 7/1000 | Loss: 0.00075877
Iteration 8/1000 | Loss: 0.00078438
Iteration 9/1000 | Loss: 0.00043569
Iteration 10/1000 | Loss: 0.00074550
Iteration 11/1000 | Loss: 0.00054799
Iteration 12/1000 | Loss: 0.00016072
Iteration 13/1000 | Loss: 0.00027489
Iteration 14/1000 | Loss: 0.00007301
Iteration 15/1000 | Loss: 0.00087115
Iteration 16/1000 | Loss: 0.00162572
Iteration 17/1000 | Loss: 0.00040072
Iteration 18/1000 | Loss: 0.00063192
Iteration 19/1000 | Loss: 0.00087777
Iteration 20/1000 | Loss: 0.00170467
Iteration 21/1000 | Loss: 0.00087648
Iteration 22/1000 | Loss: 0.00071642
Iteration 23/1000 | Loss: 0.00079220
Iteration 24/1000 | Loss: 0.00112003
Iteration 25/1000 | Loss: 0.00035456
Iteration 26/1000 | Loss: 0.00007093
Iteration 27/1000 | Loss: 0.00017170
Iteration 28/1000 | Loss: 0.00006491
Iteration 29/1000 | Loss: 0.00006263
Iteration 30/1000 | Loss: 0.00033711
Iteration 31/1000 | Loss: 0.00042309
Iteration 32/1000 | Loss: 0.00019175
Iteration 33/1000 | Loss: 0.00035949
Iteration 34/1000 | Loss: 0.00022712
Iteration 35/1000 | Loss: 0.00035006
Iteration 36/1000 | Loss: 0.00129517
Iteration 37/1000 | Loss: 0.00073119
Iteration 38/1000 | Loss: 0.00014916
Iteration 39/1000 | Loss: 0.00006988
Iteration 40/1000 | Loss: 0.00034159
Iteration 41/1000 | Loss: 0.00039130
Iteration 42/1000 | Loss: 0.00005685
Iteration 43/1000 | Loss: 0.00004926
Iteration 44/1000 | Loss: 0.00004546
Iteration 45/1000 | Loss: 0.00056610
Iteration 46/1000 | Loss: 0.00029192
Iteration 47/1000 | Loss: 0.00044128
Iteration 48/1000 | Loss: 0.00046600
Iteration 49/1000 | Loss: 0.00036081
Iteration 50/1000 | Loss: 0.00004218
Iteration 51/1000 | Loss: 0.00009782
Iteration 52/1000 | Loss: 0.00004617
Iteration 53/1000 | Loss: 0.00003976
Iteration 54/1000 | Loss: 0.00062273
Iteration 55/1000 | Loss: 0.00003863
Iteration 56/1000 | Loss: 0.00003744
Iteration 57/1000 | Loss: 0.00003692
Iteration 58/1000 | Loss: 0.00021585
Iteration 59/1000 | Loss: 0.00007926
Iteration 60/1000 | Loss: 0.00021418
Iteration 61/1000 | Loss: 0.00043959
Iteration 62/1000 | Loss: 0.00007258
Iteration 63/1000 | Loss: 0.00003605
Iteration 64/1000 | Loss: 0.00003500
Iteration 65/1000 | Loss: 0.00003467
Iteration 66/1000 | Loss: 0.00003456
Iteration 67/1000 | Loss: 0.00003446
Iteration 68/1000 | Loss: 0.00003445
Iteration 69/1000 | Loss: 0.00003445
Iteration 70/1000 | Loss: 0.00003445
Iteration 71/1000 | Loss: 0.00003444
Iteration 72/1000 | Loss: 0.00003444
Iteration 73/1000 | Loss: 0.00003444
Iteration 74/1000 | Loss: 0.00003444
Iteration 75/1000 | Loss: 0.00003443
Iteration 76/1000 | Loss: 0.00003442
Iteration 77/1000 | Loss: 0.00003442
Iteration 78/1000 | Loss: 0.00003442
Iteration 79/1000 | Loss: 0.00003442
Iteration 80/1000 | Loss: 0.00003442
Iteration 81/1000 | Loss: 0.00003442
Iteration 82/1000 | Loss: 0.00003442
Iteration 83/1000 | Loss: 0.00003441
Iteration 84/1000 | Loss: 0.00003441
Iteration 85/1000 | Loss: 0.00003441
Iteration 86/1000 | Loss: 0.00003441
Iteration 87/1000 | Loss: 0.00003441
Iteration 88/1000 | Loss: 0.00003441
Iteration 89/1000 | Loss: 0.00003441
Iteration 90/1000 | Loss: 0.00003441
Iteration 91/1000 | Loss: 0.00003441
Iteration 92/1000 | Loss: 0.00003441
Iteration 93/1000 | Loss: 0.00003440
Iteration 94/1000 | Loss: 0.00003439
Iteration 95/1000 | Loss: 0.00003439
Iteration 96/1000 | Loss: 0.00003439
Iteration 97/1000 | Loss: 0.00003439
Iteration 98/1000 | Loss: 0.00003439
Iteration 99/1000 | Loss: 0.00003439
Iteration 100/1000 | Loss: 0.00003439
Iteration 101/1000 | Loss: 0.00003439
Iteration 102/1000 | Loss: 0.00003439
Iteration 103/1000 | Loss: 0.00003439
Iteration 104/1000 | Loss: 0.00003439
Iteration 105/1000 | Loss: 0.00003438
Iteration 106/1000 | Loss: 0.00003437
Iteration 107/1000 | Loss: 0.00003437
Iteration 108/1000 | Loss: 0.00003437
Iteration 109/1000 | Loss: 0.00003437
Iteration 110/1000 | Loss: 0.00003436
Iteration 111/1000 | Loss: 0.00003436
Iteration 112/1000 | Loss: 0.00003436
Iteration 113/1000 | Loss: 0.00003436
Iteration 114/1000 | Loss: 0.00003436
Iteration 115/1000 | Loss: 0.00003436
Iteration 116/1000 | Loss: 0.00003436
Iteration 117/1000 | Loss: 0.00003436
Iteration 118/1000 | Loss: 0.00003436
Iteration 119/1000 | Loss: 0.00003436
Iteration 120/1000 | Loss: 0.00003435
Iteration 121/1000 | Loss: 0.00003435
Iteration 122/1000 | Loss: 0.00003435
Iteration 123/1000 | Loss: 0.00003435
Iteration 124/1000 | Loss: 0.00003435
Iteration 125/1000 | Loss: 0.00003435
Iteration 126/1000 | Loss: 0.00003435
Iteration 127/1000 | Loss: 0.00003435
Iteration 128/1000 | Loss: 0.00003435
Iteration 129/1000 | Loss: 0.00003435
Iteration 130/1000 | Loss: 0.00003435
Iteration 131/1000 | Loss: 0.00003435
Iteration 132/1000 | Loss: 0.00003434
Iteration 133/1000 | Loss: 0.00003434
Iteration 134/1000 | Loss: 0.00003434
Iteration 135/1000 | Loss: 0.00003434
Iteration 136/1000 | Loss: 0.00003434
Iteration 137/1000 | Loss: 0.00003433
Iteration 138/1000 | Loss: 0.00003433
Iteration 139/1000 | Loss: 0.00003433
Iteration 140/1000 | Loss: 0.00003433
Iteration 141/1000 | Loss: 0.00003433
Iteration 142/1000 | Loss: 0.00003433
Iteration 143/1000 | Loss: 0.00003433
Iteration 144/1000 | Loss: 0.00003433
Iteration 145/1000 | Loss: 0.00003433
Iteration 146/1000 | Loss: 0.00003433
Iteration 147/1000 | Loss: 0.00003433
Iteration 148/1000 | Loss: 0.00003432
Iteration 149/1000 | Loss: 0.00003432
Iteration 150/1000 | Loss: 0.00003432
Iteration 151/1000 | Loss: 0.00003432
Iteration 152/1000 | Loss: 0.00003431
Iteration 153/1000 | Loss: 0.00003431
Iteration 154/1000 | Loss: 0.00003431
Iteration 155/1000 | Loss: 0.00003431
Iteration 156/1000 | Loss: 0.00003431
Iteration 157/1000 | Loss: 0.00003430
Iteration 158/1000 | Loss: 0.00003430
Iteration 159/1000 | Loss: 0.00003430
Iteration 160/1000 | Loss: 0.00003430
Iteration 161/1000 | Loss: 0.00003430
Iteration 162/1000 | Loss: 0.00003429
Iteration 163/1000 | Loss: 0.00003429
Iteration 164/1000 | Loss: 0.00003429
Iteration 165/1000 | Loss: 0.00003429
Iteration 166/1000 | Loss: 0.00003429
Iteration 167/1000 | Loss: 0.00003429
Iteration 168/1000 | Loss: 0.00003429
Iteration 169/1000 | Loss: 0.00003429
Iteration 170/1000 | Loss: 0.00003429
Iteration 171/1000 | Loss: 0.00003429
Iteration 172/1000 | Loss: 0.00003428
Iteration 173/1000 | Loss: 0.00003428
Iteration 174/1000 | Loss: 0.00003428
Iteration 175/1000 | Loss: 0.00003428
Iteration 176/1000 | Loss: 0.00003428
Iteration 177/1000 | Loss: 0.00017025
Iteration 178/1000 | Loss: 0.00003500
Iteration 179/1000 | Loss: 0.00003343
Iteration 180/1000 | Loss: 0.00003267
Iteration 181/1000 | Loss: 0.00003191
Iteration 182/1000 | Loss: 0.00003147
Iteration 183/1000 | Loss: 0.00003117
Iteration 184/1000 | Loss: 0.00003099
Iteration 185/1000 | Loss: 0.00003093
Iteration 186/1000 | Loss: 0.00003085
Iteration 187/1000 | Loss: 0.00003085
Iteration 188/1000 | Loss: 0.00003081
Iteration 189/1000 | Loss: 0.00003078
Iteration 190/1000 | Loss: 0.00003077
Iteration 191/1000 | Loss: 0.00003076
Iteration 192/1000 | Loss: 0.00003076
Iteration 193/1000 | Loss: 0.00003076
Iteration 194/1000 | Loss: 0.00003076
Iteration 195/1000 | Loss: 0.00003075
Iteration 196/1000 | Loss: 0.00003075
Iteration 197/1000 | Loss: 0.00003074
Iteration 198/1000 | Loss: 0.00003074
Iteration 199/1000 | Loss: 0.00003073
Iteration 200/1000 | Loss: 0.00003073
Iteration 201/1000 | Loss: 0.00003072
Iteration 202/1000 | Loss: 0.00003072
Iteration 203/1000 | Loss: 0.00003072
Iteration 204/1000 | Loss: 0.00003072
Iteration 205/1000 | Loss: 0.00003071
Iteration 206/1000 | Loss: 0.00003071
Iteration 207/1000 | Loss: 0.00003071
Iteration 208/1000 | Loss: 0.00003070
Iteration 209/1000 | Loss: 0.00003070
Iteration 210/1000 | Loss: 0.00003070
Iteration 211/1000 | Loss: 0.00003070
Iteration 212/1000 | Loss: 0.00003070
Iteration 213/1000 | Loss: 0.00003070
Iteration 214/1000 | Loss: 0.00003070
Iteration 215/1000 | Loss: 0.00003070
Iteration 216/1000 | Loss: 0.00003070
Iteration 217/1000 | Loss: 0.00003069
Iteration 218/1000 | Loss: 0.00003069
Iteration 219/1000 | Loss: 0.00003069
Iteration 220/1000 | Loss: 0.00003069
Iteration 221/1000 | Loss: 0.00003069
Iteration 222/1000 | Loss: 0.00003069
Iteration 223/1000 | Loss: 0.00003069
Iteration 224/1000 | Loss: 0.00003069
Iteration 225/1000 | Loss: 0.00003069
Iteration 226/1000 | Loss: 0.00003069
Iteration 227/1000 | Loss: 0.00003069
Iteration 228/1000 | Loss: 0.00003068
Iteration 229/1000 | Loss: 0.00003068
Iteration 230/1000 | Loss: 0.00003068
Iteration 231/1000 | Loss: 0.00003068
Iteration 232/1000 | Loss: 0.00003068
Iteration 233/1000 | Loss: 0.00003068
Iteration 234/1000 | Loss: 0.00003068
Iteration 235/1000 | Loss: 0.00003068
Iteration 236/1000 | Loss: 0.00003067
Iteration 237/1000 | Loss: 0.00003067
Iteration 238/1000 | Loss: 0.00003067
Iteration 239/1000 | Loss: 0.00003067
Iteration 240/1000 | Loss: 0.00003067
Iteration 241/1000 | Loss: 0.00003066
Iteration 242/1000 | Loss: 0.00003066
Iteration 243/1000 | Loss: 0.00003066
Iteration 244/1000 | Loss: 0.00003066
Iteration 245/1000 | Loss: 0.00003066
Iteration 246/1000 | Loss: 0.00003066
Iteration 247/1000 | Loss: 0.00003066
Iteration 248/1000 | Loss: 0.00003066
Iteration 249/1000 | Loss: 0.00003066
Iteration 250/1000 | Loss: 0.00003066
Iteration 251/1000 | Loss: 0.00003066
Iteration 252/1000 | Loss: 0.00003066
Iteration 253/1000 | Loss: 0.00003066
Iteration 254/1000 | Loss: 0.00003066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [3.0658284231321886e-05, 3.0658284231321886e-05, 3.0658284231321886e-05, 3.0658284231321886e-05, 3.0658284231321886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0658284231321886e-05

Optimization complete. Final v2v error: 4.165137767791748 mm

Highest mean error: 11.292373657226562 mm for frame 129

Lowest mean error: 3.8180227279663086 mm for frame 97

Saving results

Total time: 156.62460732460022
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029594
Iteration 2/25 | Loss: 0.00175428
Iteration 3/25 | Loss: 0.00145511
Iteration 4/25 | Loss: 0.00130560
Iteration 5/25 | Loss: 0.00128725
Iteration 6/25 | Loss: 0.00120240
Iteration 7/25 | Loss: 0.00118101
Iteration 8/25 | Loss: 0.00118518
Iteration 9/25 | Loss: 0.00117098
Iteration 10/25 | Loss: 0.00117457
Iteration 11/25 | Loss: 0.00116014
Iteration 12/25 | Loss: 0.00115851
Iteration 13/25 | Loss: 0.00115400
Iteration 14/25 | Loss: 0.00114913
Iteration 15/25 | Loss: 0.00115454
Iteration 16/25 | Loss: 0.00114509
Iteration 17/25 | Loss: 0.00114389
Iteration 18/25 | Loss: 0.00114687
Iteration 19/25 | Loss: 0.00114778
Iteration 20/25 | Loss: 0.00114727
Iteration 21/25 | Loss: 0.00115536
Iteration 22/25 | Loss: 0.00115441
Iteration 23/25 | Loss: 0.00115832
Iteration 24/25 | Loss: 0.00115984
Iteration 25/25 | Loss: 0.00114889

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38588428
Iteration 2/25 | Loss: 0.00172092
Iteration 3/25 | Loss: 0.00127868
Iteration 4/25 | Loss: 0.00127868
Iteration 5/25 | Loss: 0.00127867
Iteration 6/25 | Loss: 0.00127867
Iteration 7/25 | Loss: 0.00127867
Iteration 8/25 | Loss: 0.00127867
Iteration 9/25 | Loss: 0.00127867
Iteration 10/25 | Loss: 0.00127867
Iteration 11/25 | Loss: 0.00127867
Iteration 12/25 | Loss: 0.00127867
Iteration 13/25 | Loss: 0.00127867
Iteration 14/25 | Loss: 0.00127867
Iteration 15/25 | Loss: 0.00127867
Iteration 16/25 | Loss: 0.00127867
Iteration 17/25 | Loss: 0.00127867
Iteration 18/25 | Loss: 0.00127867
Iteration 19/25 | Loss: 0.00127867
Iteration 20/25 | Loss: 0.00127867
Iteration 21/25 | Loss: 0.00127867
Iteration 22/25 | Loss: 0.00127867
Iteration 23/25 | Loss: 0.00127867
Iteration 24/25 | Loss: 0.00127867
Iteration 25/25 | Loss: 0.00127867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127867
Iteration 2/1000 | Loss: 0.00049496
Iteration 3/1000 | Loss: 0.00130545
Iteration 4/1000 | Loss: 0.00019246
Iteration 5/1000 | Loss: 0.00033923
Iteration 6/1000 | Loss: 0.00031634
Iteration 7/1000 | Loss: 0.00077970
Iteration 8/1000 | Loss: 0.00016706
Iteration 9/1000 | Loss: 0.00062545
Iteration 10/1000 | Loss: 0.00016877
Iteration 11/1000 | Loss: 0.00015603
Iteration 12/1000 | Loss: 0.00071314
Iteration 13/1000 | Loss: 0.00007802
Iteration 14/1000 | Loss: 0.00022162
Iteration 15/1000 | Loss: 0.00015680
Iteration 16/1000 | Loss: 0.00020881
Iteration 17/1000 | Loss: 0.00018504
Iteration 18/1000 | Loss: 0.00011466
Iteration 19/1000 | Loss: 0.00024095
Iteration 20/1000 | Loss: 0.00045591
Iteration 21/1000 | Loss: 0.00075802
Iteration 22/1000 | Loss: 0.00008560
Iteration 23/1000 | Loss: 0.00007417
Iteration 24/1000 | Loss: 0.00006063
Iteration 25/1000 | Loss: 0.00005884
Iteration 26/1000 | Loss: 0.00005610
Iteration 27/1000 | Loss: 0.00021306
Iteration 28/1000 | Loss: 0.00006141
Iteration 29/1000 | Loss: 0.00004654
Iteration 30/1000 | Loss: 0.00006451
Iteration 31/1000 | Loss: 0.00004295
Iteration 32/1000 | Loss: 0.00007012
Iteration 33/1000 | Loss: 0.00018503
Iteration 34/1000 | Loss: 0.00043091
Iteration 35/1000 | Loss: 0.00043695
Iteration 36/1000 | Loss: 0.00070875
Iteration 37/1000 | Loss: 0.00014692
Iteration 38/1000 | Loss: 0.00020120
Iteration 39/1000 | Loss: 0.00013801
Iteration 40/1000 | Loss: 0.00030127
Iteration 41/1000 | Loss: 0.00041157
Iteration 42/1000 | Loss: 0.00020944
Iteration 43/1000 | Loss: 0.00011560
Iteration 44/1000 | Loss: 0.00015667
Iteration 45/1000 | Loss: 0.00028757
Iteration 46/1000 | Loss: 0.00017157
Iteration 47/1000 | Loss: 0.00027361
Iteration 48/1000 | Loss: 0.00066048
Iteration 49/1000 | Loss: 0.00069717
Iteration 50/1000 | Loss: 0.00097649
Iteration 51/1000 | Loss: 0.00028337
Iteration 52/1000 | Loss: 0.00015537
Iteration 53/1000 | Loss: 0.00025858
Iteration 54/1000 | Loss: 0.00024637
Iteration 55/1000 | Loss: 0.00016454
Iteration 56/1000 | Loss: 0.00050604
Iteration 57/1000 | Loss: 0.00029587
Iteration 58/1000 | Loss: 0.00017446
Iteration 59/1000 | Loss: 0.00011957
Iteration 60/1000 | Loss: 0.00004691
Iteration 61/1000 | Loss: 0.00004442
Iteration 62/1000 | Loss: 0.00004755
Iteration 63/1000 | Loss: 0.00004954
Iteration 64/1000 | Loss: 0.00004775
Iteration 65/1000 | Loss: 0.00008646
Iteration 66/1000 | Loss: 0.00018113
Iteration 67/1000 | Loss: 0.00017057
Iteration 68/1000 | Loss: 0.00004169
Iteration 69/1000 | Loss: 0.00029905
Iteration 70/1000 | Loss: 0.00023274
Iteration 71/1000 | Loss: 0.00009205
Iteration 72/1000 | Loss: 0.00021159
Iteration 73/1000 | Loss: 0.00041929
Iteration 74/1000 | Loss: 0.00006434
Iteration 75/1000 | Loss: 0.00009600
Iteration 76/1000 | Loss: 0.00004185
Iteration 77/1000 | Loss: 0.00005549
Iteration 78/1000 | Loss: 0.00004494
Iteration 79/1000 | Loss: 0.00003810
Iteration 80/1000 | Loss: 0.00003770
Iteration 81/1000 | Loss: 0.00006042
Iteration 82/1000 | Loss: 0.00003719
Iteration 83/1000 | Loss: 0.00003693
Iteration 84/1000 | Loss: 0.00009615
Iteration 85/1000 | Loss: 0.00045398
Iteration 86/1000 | Loss: 0.00024550
Iteration 87/1000 | Loss: 0.00029585
Iteration 88/1000 | Loss: 0.00030407
Iteration 89/1000 | Loss: 0.00025012
Iteration 90/1000 | Loss: 0.00006096
Iteration 91/1000 | Loss: 0.00004708
Iteration 92/1000 | Loss: 0.00025052
Iteration 93/1000 | Loss: 0.00004002
Iteration 94/1000 | Loss: 0.00005066
Iteration 95/1000 | Loss: 0.00004392
Iteration 96/1000 | Loss: 0.00003713
Iteration 97/1000 | Loss: 0.00003688
Iteration 98/1000 | Loss: 0.00021674
Iteration 99/1000 | Loss: 0.00005492
Iteration 100/1000 | Loss: 0.00014822
Iteration 101/1000 | Loss: 0.00007218
Iteration 102/1000 | Loss: 0.00012582
Iteration 103/1000 | Loss: 0.00003897
Iteration 104/1000 | Loss: 0.00004391
Iteration 105/1000 | Loss: 0.00016234
Iteration 106/1000 | Loss: 0.00004446
Iteration 107/1000 | Loss: 0.00005899
Iteration 108/1000 | Loss: 0.00003910
Iteration 109/1000 | Loss: 0.00003573
Iteration 110/1000 | Loss: 0.00003641
Iteration 111/1000 | Loss: 0.00003476
Iteration 112/1000 | Loss: 0.00005880
Iteration 113/1000 | Loss: 0.00004924
Iteration 114/1000 | Loss: 0.00007589
Iteration 115/1000 | Loss: 0.00003739
Iteration 116/1000 | Loss: 0.00003427
Iteration 117/1000 | Loss: 0.00003402
Iteration 118/1000 | Loss: 0.00024898
Iteration 119/1000 | Loss: 0.00011836
Iteration 120/1000 | Loss: 0.00008181
Iteration 121/1000 | Loss: 0.00004825
Iteration 122/1000 | Loss: 0.00003577
Iteration 123/1000 | Loss: 0.00028597
Iteration 124/1000 | Loss: 0.00005262
Iteration 125/1000 | Loss: 0.00003864
Iteration 126/1000 | Loss: 0.00003594
Iteration 127/1000 | Loss: 0.00003406
Iteration 128/1000 | Loss: 0.00025999
Iteration 129/1000 | Loss: 0.00052353
Iteration 130/1000 | Loss: 0.00022138
Iteration 131/1000 | Loss: 0.00015434
Iteration 132/1000 | Loss: 0.00019265
Iteration 133/1000 | Loss: 0.00004521
Iteration 134/1000 | Loss: 0.00009478
Iteration 135/1000 | Loss: 0.00003297
Iteration 136/1000 | Loss: 0.00006358
Iteration 137/1000 | Loss: 0.00008952
Iteration 138/1000 | Loss: 0.00003582
Iteration 139/1000 | Loss: 0.00003055
Iteration 140/1000 | Loss: 0.00006251
Iteration 141/1000 | Loss: 0.00003367
Iteration 142/1000 | Loss: 0.00003106
Iteration 143/1000 | Loss: 0.00002955
Iteration 144/1000 | Loss: 0.00002930
Iteration 145/1000 | Loss: 0.00002907
Iteration 146/1000 | Loss: 0.00004170
Iteration 147/1000 | Loss: 0.00002893
Iteration 148/1000 | Loss: 0.00004418
Iteration 149/1000 | Loss: 0.00005939
Iteration 150/1000 | Loss: 0.00003865
Iteration 151/1000 | Loss: 0.00004168
Iteration 152/1000 | Loss: 0.00002881
Iteration 153/1000 | Loss: 0.00002875
Iteration 154/1000 | Loss: 0.00002875
Iteration 155/1000 | Loss: 0.00002874
Iteration 156/1000 | Loss: 0.00002873
Iteration 157/1000 | Loss: 0.00002873
Iteration 158/1000 | Loss: 0.00002872
Iteration 159/1000 | Loss: 0.00002871
Iteration 160/1000 | Loss: 0.00002871
Iteration 161/1000 | Loss: 0.00002871
Iteration 162/1000 | Loss: 0.00002871
Iteration 163/1000 | Loss: 0.00002871
Iteration 164/1000 | Loss: 0.00002871
Iteration 165/1000 | Loss: 0.00002870
Iteration 166/1000 | Loss: 0.00002870
Iteration 167/1000 | Loss: 0.00002870
Iteration 168/1000 | Loss: 0.00002869
Iteration 169/1000 | Loss: 0.00002869
Iteration 170/1000 | Loss: 0.00002869
Iteration 171/1000 | Loss: 0.00002869
Iteration 172/1000 | Loss: 0.00002869
Iteration 173/1000 | Loss: 0.00002869
Iteration 174/1000 | Loss: 0.00002869
Iteration 175/1000 | Loss: 0.00002869
Iteration 176/1000 | Loss: 0.00002868
Iteration 177/1000 | Loss: 0.00002868
Iteration 178/1000 | Loss: 0.00002868
Iteration 179/1000 | Loss: 0.00002868
Iteration 180/1000 | Loss: 0.00002868
Iteration 181/1000 | Loss: 0.00002868
Iteration 182/1000 | Loss: 0.00002867
Iteration 183/1000 | Loss: 0.00002867
Iteration 184/1000 | Loss: 0.00002866
Iteration 185/1000 | Loss: 0.00002866
Iteration 186/1000 | Loss: 0.00002865
Iteration 187/1000 | Loss: 0.00004218
Iteration 188/1000 | Loss: 0.00004145
Iteration 189/1000 | Loss: 0.00004273
Iteration 190/1000 | Loss: 0.00003193
Iteration 191/1000 | Loss: 0.00002861
Iteration 192/1000 | Loss: 0.00002861
Iteration 193/1000 | Loss: 0.00002861
Iteration 194/1000 | Loss: 0.00002861
Iteration 195/1000 | Loss: 0.00002861
Iteration 196/1000 | Loss: 0.00002861
Iteration 197/1000 | Loss: 0.00002860
Iteration 198/1000 | Loss: 0.00002860
Iteration 199/1000 | Loss: 0.00002859
Iteration 200/1000 | Loss: 0.00002859
Iteration 201/1000 | Loss: 0.00002859
Iteration 202/1000 | Loss: 0.00002859
Iteration 203/1000 | Loss: 0.00003316
Iteration 204/1000 | Loss: 0.00002859
Iteration 205/1000 | Loss: 0.00002859
Iteration 206/1000 | Loss: 0.00002858
Iteration 207/1000 | Loss: 0.00002858
Iteration 208/1000 | Loss: 0.00002858
Iteration 209/1000 | Loss: 0.00002858
Iteration 210/1000 | Loss: 0.00002858
Iteration 211/1000 | Loss: 0.00002858
Iteration 212/1000 | Loss: 0.00002858
Iteration 213/1000 | Loss: 0.00002858
Iteration 214/1000 | Loss: 0.00002858
Iteration 215/1000 | Loss: 0.00002858
Iteration 216/1000 | Loss: 0.00002858
Iteration 217/1000 | Loss: 0.00002857
Iteration 218/1000 | Loss: 0.00002857
Iteration 219/1000 | Loss: 0.00002857
Iteration 220/1000 | Loss: 0.00002857
Iteration 221/1000 | Loss: 0.00002857
Iteration 222/1000 | Loss: 0.00002857
Iteration 223/1000 | Loss: 0.00002857
Iteration 224/1000 | Loss: 0.00002857
Iteration 225/1000 | Loss: 0.00002857
Iteration 226/1000 | Loss: 0.00002857
Iteration 227/1000 | Loss: 0.00002857
Iteration 228/1000 | Loss: 0.00002857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [2.857092476915568e-05, 2.857092476915568e-05, 2.857092476915568e-05, 2.857092476915568e-05, 2.857092476915568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.857092476915568e-05

Optimization complete. Final v2v error: 3.159862995147705 mm

Highest mean error: 10.225295066833496 mm for frame 180

Lowest mean error: 2.5252504348754883 mm for frame 0

Saving results

Total time: 306.3171615600586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_013/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_013/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388104
Iteration 2/25 | Loss: 0.00113695
Iteration 3/25 | Loss: 0.00104713
Iteration 4/25 | Loss: 0.00103826
Iteration 5/25 | Loss: 0.00103625
Iteration 6/25 | Loss: 0.00103566
Iteration 7/25 | Loss: 0.00103564
Iteration 8/25 | Loss: 0.00103564
Iteration 9/25 | Loss: 0.00103564
Iteration 10/25 | Loss: 0.00103564
Iteration 11/25 | Loss: 0.00103564
Iteration 12/25 | Loss: 0.00103564
Iteration 13/25 | Loss: 0.00103564
Iteration 14/25 | Loss: 0.00103564
Iteration 15/25 | Loss: 0.00103564
Iteration 16/25 | Loss: 0.00103564
Iteration 17/25 | Loss: 0.00103564
Iteration 18/25 | Loss: 0.00103564
Iteration 19/25 | Loss: 0.00103564
Iteration 20/25 | Loss: 0.00103564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010356385027989745, 0.0010356385027989745, 0.0010356385027989745, 0.0010356385027989745, 0.0010356385027989745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010356385027989745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38336062
Iteration 2/25 | Loss: 0.00075311
Iteration 3/25 | Loss: 0.00075311
Iteration 4/25 | Loss: 0.00075311
Iteration 5/25 | Loss: 0.00075311
Iteration 6/25 | Loss: 0.00075311
Iteration 7/25 | Loss: 0.00075311
Iteration 8/25 | Loss: 0.00075311
Iteration 9/25 | Loss: 0.00075311
Iteration 10/25 | Loss: 0.00075311
Iteration 11/25 | Loss: 0.00075311
Iteration 12/25 | Loss: 0.00075311
Iteration 13/25 | Loss: 0.00075311
Iteration 14/25 | Loss: 0.00075311
Iteration 15/25 | Loss: 0.00075311
Iteration 16/25 | Loss: 0.00075311
Iteration 17/25 | Loss: 0.00075311
Iteration 18/25 | Loss: 0.00075311
Iteration 19/25 | Loss: 0.00075311
Iteration 20/25 | Loss: 0.00075311
Iteration 21/25 | Loss: 0.00075311
Iteration 22/25 | Loss: 0.00075311
Iteration 23/25 | Loss: 0.00075311
Iteration 24/25 | Loss: 0.00075311
Iteration 25/25 | Loss: 0.00075311

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075311
Iteration 2/1000 | Loss: 0.00001787
Iteration 3/1000 | Loss: 0.00001238
Iteration 4/1000 | Loss: 0.00001106
Iteration 5/1000 | Loss: 0.00001031
Iteration 6/1000 | Loss: 0.00000982
Iteration 7/1000 | Loss: 0.00000951
Iteration 8/1000 | Loss: 0.00000938
Iteration 9/1000 | Loss: 0.00000931
Iteration 10/1000 | Loss: 0.00000915
Iteration 11/1000 | Loss: 0.00000915
Iteration 12/1000 | Loss: 0.00000914
Iteration 13/1000 | Loss: 0.00000913
Iteration 14/1000 | Loss: 0.00000913
Iteration 15/1000 | Loss: 0.00000912
Iteration 16/1000 | Loss: 0.00000911
Iteration 17/1000 | Loss: 0.00000911
Iteration 18/1000 | Loss: 0.00000910
Iteration 19/1000 | Loss: 0.00000906
Iteration 20/1000 | Loss: 0.00000904
Iteration 21/1000 | Loss: 0.00000903
Iteration 22/1000 | Loss: 0.00000903
Iteration 23/1000 | Loss: 0.00000903
Iteration 24/1000 | Loss: 0.00000902
Iteration 25/1000 | Loss: 0.00000902
Iteration 26/1000 | Loss: 0.00000901
Iteration 27/1000 | Loss: 0.00000901
Iteration 28/1000 | Loss: 0.00000900
Iteration 29/1000 | Loss: 0.00000900
Iteration 30/1000 | Loss: 0.00000900
Iteration 31/1000 | Loss: 0.00000899
Iteration 32/1000 | Loss: 0.00000899
Iteration 33/1000 | Loss: 0.00000898
Iteration 34/1000 | Loss: 0.00000898
Iteration 35/1000 | Loss: 0.00000898
Iteration 36/1000 | Loss: 0.00000897
Iteration 37/1000 | Loss: 0.00000897
Iteration 38/1000 | Loss: 0.00000897
Iteration 39/1000 | Loss: 0.00000896
Iteration 40/1000 | Loss: 0.00000895
Iteration 41/1000 | Loss: 0.00000895
Iteration 42/1000 | Loss: 0.00000894
Iteration 43/1000 | Loss: 0.00000893
Iteration 44/1000 | Loss: 0.00000893
Iteration 45/1000 | Loss: 0.00000893
Iteration 46/1000 | Loss: 0.00000893
Iteration 47/1000 | Loss: 0.00000893
Iteration 48/1000 | Loss: 0.00000893
Iteration 49/1000 | Loss: 0.00000893
Iteration 50/1000 | Loss: 0.00000893
Iteration 51/1000 | Loss: 0.00000893
Iteration 52/1000 | Loss: 0.00000893
Iteration 53/1000 | Loss: 0.00000892
Iteration 54/1000 | Loss: 0.00000892
Iteration 55/1000 | Loss: 0.00000892
Iteration 56/1000 | Loss: 0.00000892
Iteration 57/1000 | Loss: 0.00000892
Iteration 58/1000 | Loss: 0.00000892
Iteration 59/1000 | Loss: 0.00000891
Iteration 60/1000 | Loss: 0.00000891
Iteration 61/1000 | Loss: 0.00000891
Iteration 62/1000 | Loss: 0.00000890
Iteration 63/1000 | Loss: 0.00000890
Iteration 64/1000 | Loss: 0.00000890
Iteration 65/1000 | Loss: 0.00000890
Iteration 66/1000 | Loss: 0.00000890
Iteration 67/1000 | Loss: 0.00000889
Iteration 68/1000 | Loss: 0.00000889
Iteration 69/1000 | Loss: 0.00000889
Iteration 70/1000 | Loss: 0.00000889
Iteration 71/1000 | Loss: 0.00000889
Iteration 72/1000 | Loss: 0.00000889
Iteration 73/1000 | Loss: 0.00000888
Iteration 74/1000 | Loss: 0.00000888
Iteration 75/1000 | Loss: 0.00000888
Iteration 76/1000 | Loss: 0.00000888
Iteration 77/1000 | Loss: 0.00000887
Iteration 78/1000 | Loss: 0.00000887
Iteration 79/1000 | Loss: 0.00000887
Iteration 80/1000 | Loss: 0.00000886
Iteration 81/1000 | Loss: 0.00000886
Iteration 82/1000 | Loss: 0.00000886
Iteration 83/1000 | Loss: 0.00000886
Iteration 84/1000 | Loss: 0.00000886
Iteration 85/1000 | Loss: 0.00000886
Iteration 86/1000 | Loss: 0.00000886
Iteration 87/1000 | Loss: 0.00000885
Iteration 88/1000 | Loss: 0.00000885
Iteration 89/1000 | Loss: 0.00000885
Iteration 90/1000 | Loss: 0.00000885
Iteration 91/1000 | Loss: 0.00000885
Iteration 92/1000 | Loss: 0.00000885
Iteration 93/1000 | Loss: 0.00000884
Iteration 94/1000 | Loss: 0.00000884
Iteration 95/1000 | Loss: 0.00000884
Iteration 96/1000 | Loss: 0.00000884
Iteration 97/1000 | Loss: 0.00000884
Iteration 98/1000 | Loss: 0.00000883
Iteration 99/1000 | Loss: 0.00000883
Iteration 100/1000 | Loss: 0.00000883
Iteration 101/1000 | Loss: 0.00000883
Iteration 102/1000 | Loss: 0.00000883
Iteration 103/1000 | Loss: 0.00000883
Iteration 104/1000 | Loss: 0.00000883
Iteration 105/1000 | Loss: 0.00000883
Iteration 106/1000 | Loss: 0.00000883
Iteration 107/1000 | Loss: 0.00000883
Iteration 108/1000 | Loss: 0.00000883
Iteration 109/1000 | Loss: 0.00000883
Iteration 110/1000 | Loss: 0.00000883
Iteration 111/1000 | Loss: 0.00000882
Iteration 112/1000 | Loss: 0.00000882
Iteration 113/1000 | Loss: 0.00000882
Iteration 114/1000 | Loss: 0.00000882
Iteration 115/1000 | Loss: 0.00000882
Iteration 116/1000 | Loss: 0.00000882
Iteration 117/1000 | Loss: 0.00000881
Iteration 118/1000 | Loss: 0.00000881
Iteration 119/1000 | Loss: 0.00000881
Iteration 120/1000 | Loss: 0.00000881
Iteration 121/1000 | Loss: 0.00000881
Iteration 122/1000 | Loss: 0.00000881
Iteration 123/1000 | Loss: 0.00000881
Iteration 124/1000 | Loss: 0.00000881
Iteration 125/1000 | Loss: 0.00000880
Iteration 126/1000 | Loss: 0.00000880
Iteration 127/1000 | Loss: 0.00000880
Iteration 128/1000 | Loss: 0.00000879
Iteration 129/1000 | Loss: 0.00000879
Iteration 130/1000 | Loss: 0.00000879
Iteration 131/1000 | Loss: 0.00000879
Iteration 132/1000 | Loss: 0.00000878
Iteration 133/1000 | Loss: 0.00000878
Iteration 134/1000 | Loss: 0.00000877
Iteration 135/1000 | Loss: 0.00000877
Iteration 136/1000 | Loss: 0.00000877
Iteration 137/1000 | Loss: 0.00000876
Iteration 138/1000 | Loss: 0.00000876
Iteration 139/1000 | Loss: 0.00000875
Iteration 140/1000 | Loss: 0.00000875
Iteration 141/1000 | Loss: 0.00000875
Iteration 142/1000 | Loss: 0.00000874
Iteration 143/1000 | Loss: 0.00000874
Iteration 144/1000 | Loss: 0.00000874
Iteration 145/1000 | Loss: 0.00000874
Iteration 146/1000 | Loss: 0.00000874
Iteration 147/1000 | Loss: 0.00000874
Iteration 148/1000 | Loss: 0.00000873
Iteration 149/1000 | Loss: 0.00000873
Iteration 150/1000 | Loss: 0.00000873
Iteration 151/1000 | Loss: 0.00000873
Iteration 152/1000 | Loss: 0.00000873
Iteration 153/1000 | Loss: 0.00000873
Iteration 154/1000 | Loss: 0.00000873
Iteration 155/1000 | Loss: 0.00000873
Iteration 156/1000 | Loss: 0.00000873
Iteration 157/1000 | Loss: 0.00000873
Iteration 158/1000 | Loss: 0.00000873
Iteration 159/1000 | Loss: 0.00000873
Iteration 160/1000 | Loss: 0.00000872
Iteration 161/1000 | Loss: 0.00000872
Iteration 162/1000 | Loss: 0.00000872
Iteration 163/1000 | Loss: 0.00000872
Iteration 164/1000 | Loss: 0.00000872
Iteration 165/1000 | Loss: 0.00000871
Iteration 166/1000 | Loss: 0.00000871
Iteration 167/1000 | Loss: 0.00000871
Iteration 168/1000 | Loss: 0.00000871
Iteration 169/1000 | Loss: 0.00000870
Iteration 170/1000 | Loss: 0.00000870
Iteration 171/1000 | Loss: 0.00000870
Iteration 172/1000 | Loss: 0.00000870
Iteration 173/1000 | Loss: 0.00000869
Iteration 174/1000 | Loss: 0.00000869
Iteration 175/1000 | Loss: 0.00000869
Iteration 176/1000 | Loss: 0.00000868
Iteration 177/1000 | Loss: 0.00000868
Iteration 178/1000 | Loss: 0.00000868
Iteration 179/1000 | Loss: 0.00000868
Iteration 180/1000 | Loss: 0.00000868
Iteration 181/1000 | Loss: 0.00000867
Iteration 182/1000 | Loss: 0.00000867
Iteration 183/1000 | Loss: 0.00000867
Iteration 184/1000 | Loss: 0.00000867
Iteration 185/1000 | Loss: 0.00000867
Iteration 186/1000 | Loss: 0.00000867
Iteration 187/1000 | Loss: 0.00000867
Iteration 188/1000 | Loss: 0.00000866
Iteration 189/1000 | Loss: 0.00000866
Iteration 190/1000 | Loss: 0.00000866
Iteration 191/1000 | Loss: 0.00000866
Iteration 192/1000 | Loss: 0.00000866
Iteration 193/1000 | Loss: 0.00000866
Iteration 194/1000 | Loss: 0.00000866
Iteration 195/1000 | Loss: 0.00000866
Iteration 196/1000 | Loss: 0.00000866
Iteration 197/1000 | Loss: 0.00000866
Iteration 198/1000 | Loss: 0.00000866
Iteration 199/1000 | Loss: 0.00000865
Iteration 200/1000 | Loss: 0.00000865
Iteration 201/1000 | Loss: 0.00000865
Iteration 202/1000 | Loss: 0.00000865
Iteration 203/1000 | Loss: 0.00000865
Iteration 204/1000 | Loss: 0.00000865
Iteration 205/1000 | Loss: 0.00000865
Iteration 206/1000 | Loss: 0.00000865
Iteration 207/1000 | Loss: 0.00000865
Iteration 208/1000 | Loss: 0.00000865
Iteration 209/1000 | Loss: 0.00000865
Iteration 210/1000 | Loss: 0.00000865
Iteration 211/1000 | Loss: 0.00000865
Iteration 212/1000 | Loss: 0.00000864
Iteration 213/1000 | Loss: 0.00000864
Iteration 214/1000 | Loss: 0.00000864
Iteration 215/1000 | Loss: 0.00000864
Iteration 216/1000 | Loss: 0.00000864
Iteration 217/1000 | Loss: 0.00000864
Iteration 218/1000 | Loss: 0.00000864
Iteration 219/1000 | Loss: 0.00000864
Iteration 220/1000 | Loss: 0.00000864
Iteration 221/1000 | Loss: 0.00000864
Iteration 222/1000 | Loss: 0.00000864
Iteration 223/1000 | Loss: 0.00000864
Iteration 224/1000 | Loss: 0.00000863
Iteration 225/1000 | Loss: 0.00000863
Iteration 226/1000 | Loss: 0.00000863
Iteration 227/1000 | Loss: 0.00000863
Iteration 228/1000 | Loss: 0.00000863
Iteration 229/1000 | Loss: 0.00000863
Iteration 230/1000 | Loss: 0.00000863
Iteration 231/1000 | Loss: 0.00000863
Iteration 232/1000 | Loss: 0.00000863
Iteration 233/1000 | Loss: 0.00000863
Iteration 234/1000 | Loss: 0.00000862
Iteration 235/1000 | Loss: 0.00000862
Iteration 236/1000 | Loss: 0.00000862
Iteration 237/1000 | Loss: 0.00000862
Iteration 238/1000 | Loss: 0.00000862
Iteration 239/1000 | Loss: 0.00000862
Iteration 240/1000 | Loss: 0.00000862
Iteration 241/1000 | Loss: 0.00000862
Iteration 242/1000 | Loss: 0.00000862
Iteration 243/1000 | Loss: 0.00000862
Iteration 244/1000 | Loss: 0.00000862
Iteration 245/1000 | Loss: 0.00000862
Iteration 246/1000 | Loss: 0.00000861
Iteration 247/1000 | Loss: 0.00000861
Iteration 248/1000 | Loss: 0.00000861
Iteration 249/1000 | Loss: 0.00000861
Iteration 250/1000 | Loss: 0.00000861
Iteration 251/1000 | Loss: 0.00000861
Iteration 252/1000 | Loss: 0.00000861
Iteration 253/1000 | Loss: 0.00000861
Iteration 254/1000 | Loss: 0.00000861
Iteration 255/1000 | Loss: 0.00000861
Iteration 256/1000 | Loss: 0.00000861
Iteration 257/1000 | Loss: 0.00000861
Iteration 258/1000 | Loss: 0.00000861
Iteration 259/1000 | Loss: 0.00000861
Iteration 260/1000 | Loss: 0.00000861
Iteration 261/1000 | Loss: 0.00000861
Iteration 262/1000 | Loss: 0.00000861
Iteration 263/1000 | Loss: 0.00000861
Iteration 264/1000 | Loss: 0.00000861
Iteration 265/1000 | Loss: 0.00000861
Iteration 266/1000 | Loss: 0.00000861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [8.611874363850802e-06, 8.611874363850802e-06, 8.611874363850802e-06, 8.611874363850802e-06, 8.611874363850802e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.611874363850802e-06

Optimization complete. Final v2v error: 2.48140811920166 mm

Highest mean error: 3.592933177947998 mm for frame 70

Lowest mean error: 2.3255796432495117 mm for frame 101

Saving results

Total time: 41.198375940322876
