Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=272, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 15232-15287
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407433
Iteration 2/25 | Loss: 0.00084798
Iteration 3/25 | Loss: 0.00074508
Iteration 4/25 | Loss: 0.00072045
Iteration 5/25 | Loss: 0.00071374
Iteration 6/25 | Loss: 0.00071220
Iteration 7/25 | Loss: 0.00071169
Iteration 8/25 | Loss: 0.00071168
Iteration 9/25 | Loss: 0.00071168
Iteration 10/25 | Loss: 0.00071168
Iteration 11/25 | Loss: 0.00071168
Iteration 12/25 | Loss: 0.00071168
Iteration 13/25 | Loss: 0.00071168
Iteration 14/25 | Loss: 0.00071168
Iteration 15/25 | Loss: 0.00071168
Iteration 16/25 | Loss: 0.00071168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007116772467270494, 0.0007116772467270494, 0.0007116772467270494, 0.0007116772467270494, 0.0007116772467270494]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007116772467270494

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10947871
Iteration 2/25 | Loss: 0.00083688
Iteration 3/25 | Loss: 0.00083688
Iteration 4/25 | Loss: 0.00083688
Iteration 5/25 | Loss: 0.00083688
Iteration 6/25 | Loss: 0.00083688
Iteration 7/25 | Loss: 0.00083688
Iteration 8/25 | Loss: 0.00083688
Iteration 9/25 | Loss: 0.00083688
Iteration 10/25 | Loss: 0.00083688
Iteration 11/25 | Loss: 0.00083687
Iteration 12/25 | Loss: 0.00083687
Iteration 13/25 | Loss: 0.00083687
Iteration 14/25 | Loss: 0.00083687
Iteration 15/25 | Loss: 0.00083687
Iteration 16/25 | Loss: 0.00083687
Iteration 17/25 | Loss: 0.00083687
Iteration 18/25 | Loss: 0.00083687
Iteration 19/25 | Loss: 0.00083687
Iteration 20/25 | Loss: 0.00083687
Iteration 21/25 | Loss: 0.00083687
Iteration 22/25 | Loss: 0.00083687
Iteration 23/25 | Loss: 0.00083687
Iteration 24/25 | Loss: 0.00083687
Iteration 25/25 | Loss: 0.00083687

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083687
Iteration 2/1000 | Loss: 0.00002791
Iteration 3/1000 | Loss: 0.00001811
Iteration 4/1000 | Loss: 0.00001587
Iteration 5/1000 | Loss: 0.00001514
Iteration 6/1000 | Loss: 0.00001462
Iteration 7/1000 | Loss: 0.00001430
Iteration 8/1000 | Loss: 0.00001400
Iteration 9/1000 | Loss: 0.00001383
Iteration 10/1000 | Loss: 0.00001378
Iteration 11/1000 | Loss: 0.00001376
Iteration 12/1000 | Loss: 0.00001375
Iteration 13/1000 | Loss: 0.00001372
Iteration 14/1000 | Loss: 0.00001370
Iteration 15/1000 | Loss: 0.00001360
Iteration 16/1000 | Loss: 0.00001356
Iteration 17/1000 | Loss: 0.00001354
Iteration 18/1000 | Loss: 0.00001353
Iteration 19/1000 | Loss: 0.00001353
Iteration 20/1000 | Loss: 0.00001351
Iteration 21/1000 | Loss: 0.00001350
Iteration 22/1000 | Loss: 0.00001350
Iteration 23/1000 | Loss: 0.00001347
Iteration 24/1000 | Loss: 0.00001346
Iteration 25/1000 | Loss: 0.00001344
Iteration 26/1000 | Loss: 0.00001343
Iteration 27/1000 | Loss: 0.00001343
Iteration 28/1000 | Loss: 0.00001343
Iteration 29/1000 | Loss: 0.00001342
Iteration 30/1000 | Loss: 0.00001342
Iteration 31/1000 | Loss: 0.00001341
Iteration 32/1000 | Loss: 0.00001340
Iteration 33/1000 | Loss: 0.00001340
Iteration 34/1000 | Loss: 0.00001339
Iteration 35/1000 | Loss: 0.00001339
Iteration 36/1000 | Loss: 0.00001338
Iteration 37/1000 | Loss: 0.00001338
Iteration 38/1000 | Loss: 0.00001337
Iteration 39/1000 | Loss: 0.00001337
Iteration 40/1000 | Loss: 0.00001337
Iteration 41/1000 | Loss: 0.00001336
Iteration 42/1000 | Loss: 0.00001336
Iteration 43/1000 | Loss: 0.00001335
Iteration 44/1000 | Loss: 0.00001334
Iteration 45/1000 | Loss: 0.00001334
Iteration 46/1000 | Loss: 0.00001334
Iteration 47/1000 | Loss: 0.00001334
Iteration 48/1000 | Loss: 0.00001334
Iteration 49/1000 | Loss: 0.00001334
Iteration 50/1000 | Loss: 0.00001333
Iteration 51/1000 | Loss: 0.00001333
Iteration 52/1000 | Loss: 0.00001333
Iteration 53/1000 | Loss: 0.00001333
Iteration 54/1000 | Loss: 0.00001332
Iteration 55/1000 | Loss: 0.00001332
Iteration 56/1000 | Loss: 0.00001331
Iteration 57/1000 | Loss: 0.00001331
Iteration 58/1000 | Loss: 0.00001331
Iteration 59/1000 | Loss: 0.00001331
Iteration 60/1000 | Loss: 0.00001331
Iteration 61/1000 | Loss: 0.00001330
Iteration 62/1000 | Loss: 0.00001330
Iteration 63/1000 | Loss: 0.00001330
Iteration 64/1000 | Loss: 0.00001329
Iteration 65/1000 | Loss: 0.00001329
Iteration 66/1000 | Loss: 0.00001329
Iteration 67/1000 | Loss: 0.00001329
Iteration 68/1000 | Loss: 0.00001329
Iteration 69/1000 | Loss: 0.00001329
Iteration 70/1000 | Loss: 0.00001328
Iteration 71/1000 | Loss: 0.00001328
Iteration 72/1000 | Loss: 0.00001328
Iteration 73/1000 | Loss: 0.00001328
Iteration 74/1000 | Loss: 0.00001328
Iteration 75/1000 | Loss: 0.00001328
Iteration 76/1000 | Loss: 0.00001328
Iteration 77/1000 | Loss: 0.00001327
Iteration 78/1000 | Loss: 0.00001327
Iteration 79/1000 | Loss: 0.00001327
Iteration 80/1000 | Loss: 0.00001327
Iteration 81/1000 | Loss: 0.00001327
Iteration 82/1000 | Loss: 0.00001327
Iteration 83/1000 | Loss: 0.00001327
Iteration 84/1000 | Loss: 0.00001327
Iteration 85/1000 | Loss: 0.00001327
Iteration 86/1000 | Loss: 0.00001327
Iteration 87/1000 | Loss: 0.00001326
Iteration 88/1000 | Loss: 0.00001326
Iteration 89/1000 | Loss: 0.00001326
Iteration 90/1000 | Loss: 0.00001326
Iteration 91/1000 | Loss: 0.00001326
Iteration 92/1000 | Loss: 0.00001325
Iteration 93/1000 | Loss: 0.00001325
Iteration 94/1000 | Loss: 0.00001325
Iteration 95/1000 | Loss: 0.00001325
Iteration 96/1000 | Loss: 0.00001324
Iteration 97/1000 | Loss: 0.00001324
Iteration 98/1000 | Loss: 0.00001324
Iteration 99/1000 | Loss: 0.00001323
Iteration 100/1000 | Loss: 0.00001323
Iteration 101/1000 | Loss: 0.00001323
Iteration 102/1000 | Loss: 0.00001322
Iteration 103/1000 | Loss: 0.00001322
Iteration 104/1000 | Loss: 0.00001322
Iteration 105/1000 | Loss: 0.00001321
Iteration 106/1000 | Loss: 0.00001321
Iteration 107/1000 | Loss: 0.00001321
Iteration 108/1000 | Loss: 0.00001320
Iteration 109/1000 | Loss: 0.00001320
Iteration 110/1000 | Loss: 0.00001320
Iteration 111/1000 | Loss: 0.00001320
Iteration 112/1000 | Loss: 0.00001319
Iteration 113/1000 | Loss: 0.00001319
Iteration 114/1000 | Loss: 0.00001319
Iteration 115/1000 | Loss: 0.00001319
Iteration 116/1000 | Loss: 0.00001318
Iteration 117/1000 | Loss: 0.00001318
Iteration 118/1000 | Loss: 0.00001318
Iteration 119/1000 | Loss: 0.00001318
Iteration 120/1000 | Loss: 0.00001318
Iteration 121/1000 | Loss: 0.00001317
Iteration 122/1000 | Loss: 0.00001317
Iteration 123/1000 | Loss: 0.00001317
Iteration 124/1000 | Loss: 0.00001317
Iteration 125/1000 | Loss: 0.00001317
Iteration 126/1000 | Loss: 0.00001317
Iteration 127/1000 | Loss: 0.00001317
Iteration 128/1000 | Loss: 0.00001317
Iteration 129/1000 | Loss: 0.00001317
Iteration 130/1000 | Loss: 0.00001317
Iteration 131/1000 | Loss: 0.00001316
Iteration 132/1000 | Loss: 0.00001316
Iteration 133/1000 | Loss: 0.00001316
Iteration 134/1000 | Loss: 0.00001316
Iteration 135/1000 | Loss: 0.00001316
Iteration 136/1000 | Loss: 0.00001316
Iteration 137/1000 | Loss: 0.00001315
Iteration 138/1000 | Loss: 0.00001315
Iteration 139/1000 | Loss: 0.00001315
Iteration 140/1000 | Loss: 0.00001315
Iteration 141/1000 | Loss: 0.00001314
Iteration 142/1000 | Loss: 0.00001314
Iteration 143/1000 | Loss: 0.00001314
Iteration 144/1000 | Loss: 0.00001313
Iteration 145/1000 | Loss: 0.00001313
Iteration 146/1000 | Loss: 0.00001313
Iteration 147/1000 | Loss: 0.00001313
Iteration 148/1000 | Loss: 0.00001313
Iteration 149/1000 | Loss: 0.00001313
Iteration 150/1000 | Loss: 0.00001313
Iteration 151/1000 | Loss: 0.00001313
Iteration 152/1000 | Loss: 0.00001313
Iteration 153/1000 | Loss: 0.00001313
Iteration 154/1000 | Loss: 0.00001313
Iteration 155/1000 | Loss: 0.00001313
Iteration 156/1000 | Loss: 0.00001313
Iteration 157/1000 | Loss: 0.00001312
Iteration 158/1000 | Loss: 0.00001312
Iteration 159/1000 | Loss: 0.00001312
Iteration 160/1000 | Loss: 0.00001312
Iteration 161/1000 | Loss: 0.00001312
Iteration 162/1000 | Loss: 0.00001312
Iteration 163/1000 | Loss: 0.00001312
Iteration 164/1000 | Loss: 0.00001312
Iteration 165/1000 | Loss: 0.00001312
Iteration 166/1000 | Loss: 0.00001312
Iteration 167/1000 | Loss: 0.00001312
Iteration 168/1000 | Loss: 0.00001312
Iteration 169/1000 | Loss: 0.00001312
Iteration 170/1000 | Loss: 0.00001312
Iteration 171/1000 | Loss: 0.00001312
Iteration 172/1000 | Loss: 0.00001312
Iteration 173/1000 | Loss: 0.00001312
Iteration 174/1000 | Loss: 0.00001312
Iteration 175/1000 | Loss: 0.00001312
Iteration 176/1000 | Loss: 0.00001312
Iteration 177/1000 | Loss: 0.00001312
Iteration 178/1000 | Loss: 0.00001312
Iteration 179/1000 | Loss: 0.00001312
Iteration 180/1000 | Loss: 0.00001312
Iteration 181/1000 | Loss: 0.00001312
Iteration 182/1000 | Loss: 0.00001312
Iteration 183/1000 | Loss: 0.00001312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.3115476576786023e-05, 1.3115476576786023e-05, 1.3115476576786023e-05, 1.3115476576786023e-05, 1.3115476576786023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3115476576786023e-05

Optimization complete. Final v2v error: 3.0485172271728516 mm

Highest mean error: 4.024092197418213 mm for frame 61

Lowest mean error: 2.781093120574951 mm for frame 83

Saving results

Total time: 58.803356409072876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812719
Iteration 2/25 | Loss: 0.00088696
Iteration 3/25 | Loss: 0.00074386
Iteration 4/25 | Loss: 0.00071817
Iteration 5/25 | Loss: 0.00071395
Iteration 6/25 | Loss: 0.00071304
Iteration 7/25 | Loss: 0.00071300
Iteration 8/25 | Loss: 0.00071300
Iteration 9/25 | Loss: 0.00071300
Iteration 10/25 | Loss: 0.00071300
Iteration 11/25 | Loss: 0.00071300
Iteration 12/25 | Loss: 0.00071300
Iteration 13/25 | Loss: 0.00071300
Iteration 14/25 | Loss: 0.00071300
Iteration 15/25 | Loss: 0.00071300
Iteration 16/25 | Loss: 0.00071300
Iteration 17/25 | Loss: 0.00071300
Iteration 18/25 | Loss: 0.00071300
Iteration 19/25 | Loss: 0.00071300
Iteration 20/25 | Loss: 0.00071300
Iteration 21/25 | Loss: 0.00071300
Iteration 22/25 | Loss: 0.00071300
Iteration 23/25 | Loss: 0.00071300
Iteration 24/25 | Loss: 0.00071300
Iteration 25/25 | Loss: 0.00071300

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55445576
Iteration 2/25 | Loss: 0.00083143
Iteration 3/25 | Loss: 0.00083143
Iteration 4/25 | Loss: 0.00083143
Iteration 5/25 | Loss: 0.00083143
Iteration 6/25 | Loss: 0.00083143
Iteration 7/25 | Loss: 0.00083143
Iteration 8/25 | Loss: 0.00083143
Iteration 9/25 | Loss: 0.00083143
Iteration 10/25 | Loss: 0.00083143
Iteration 11/25 | Loss: 0.00083143
Iteration 12/25 | Loss: 0.00083143
Iteration 13/25 | Loss: 0.00083143
Iteration 14/25 | Loss: 0.00083143
Iteration 15/25 | Loss: 0.00083143
Iteration 16/25 | Loss: 0.00083143
Iteration 17/25 | Loss: 0.00083143
Iteration 18/25 | Loss: 0.00083143
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008314287988469005, 0.0008314287988469005, 0.0008314287988469005, 0.0008314287988469005, 0.0008314287988469005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008314287988469005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083143
Iteration 2/1000 | Loss: 0.00002162
Iteration 3/1000 | Loss: 0.00001444
Iteration 4/1000 | Loss: 0.00001206
Iteration 5/1000 | Loss: 0.00001151
Iteration 6/1000 | Loss: 0.00001098
Iteration 7/1000 | Loss: 0.00001086
Iteration 8/1000 | Loss: 0.00001068
Iteration 9/1000 | Loss: 0.00001055
Iteration 10/1000 | Loss: 0.00001052
Iteration 11/1000 | Loss: 0.00001052
Iteration 12/1000 | Loss: 0.00001052
Iteration 13/1000 | Loss: 0.00001049
Iteration 14/1000 | Loss: 0.00001049
Iteration 15/1000 | Loss: 0.00001049
Iteration 16/1000 | Loss: 0.00001049
Iteration 17/1000 | Loss: 0.00001049
Iteration 18/1000 | Loss: 0.00001046
Iteration 19/1000 | Loss: 0.00001044
Iteration 20/1000 | Loss: 0.00001043
Iteration 21/1000 | Loss: 0.00001041
Iteration 22/1000 | Loss: 0.00001040
Iteration 23/1000 | Loss: 0.00001039
Iteration 24/1000 | Loss: 0.00001039
Iteration 25/1000 | Loss: 0.00001039
Iteration 26/1000 | Loss: 0.00001039
Iteration 27/1000 | Loss: 0.00001037
Iteration 28/1000 | Loss: 0.00001036
Iteration 29/1000 | Loss: 0.00001036
Iteration 30/1000 | Loss: 0.00001036
Iteration 31/1000 | Loss: 0.00001035
Iteration 32/1000 | Loss: 0.00001034
Iteration 33/1000 | Loss: 0.00001031
Iteration 34/1000 | Loss: 0.00001031
Iteration 35/1000 | Loss: 0.00001031
Iteration 36/1000 | Loss: 0.00001030
Iteration 37/1000 | Loss: 0.00001030
Iteration 38/1000 | Loss: 0.00001028
Iteration 39/1000 | Loss: 0.00001028
Iteration 40/1000 | Loss: 0.00001027
Iteration 41/1000 | Loss: 0.00001027
Iteration 42/1000 | Loss: 0.00001027
Iteration 43/1000 | Loss: 0.00001026
Iteration 44/1000 | Loss: 0.00001026
Iteration 45/1000 | Loss: 0.00001026
Iteration 46/1000 | Loss: 0.00001026
Iteration 47/1000 | Loss: 0.00001026
Iteration 48/1000 | Loss: 0.00001026
Iteration 49/1000 | Loss: 0.00001026
Iteration 50/1000 | Loss: 0.00001025
Iteration 51/1000 | Loss: 0.00001025
Iteration 52/1000 | Loss: 0.00001025
Iteration 53/1000 | Loss: 0.00001024
Iteration 54/1000 | Loss: 0.00001024
Iteration 55/1000 | Loss: 0.00001024
Iteration 56/1000 | Loss: 0.00001023
Iteration 57/1000 | Loss: 0.00001023
Iteration 58/1000 | Loss: 0.00001023
Iteration 59/1000 | Loss: 0.00001023
Iteration 60/1000 | Loss: 0.00001022
Iteration 61/1000 | Loss: 0.00001022
Iteration 62/1000 | Loss: 0.00001021
Iteration 63/1000 | Loss: 0.00001021
Iteration 64/1000 | Loss: 0.00001021
Iteration 65/1000 | Loss: 0.00001021
Iteration 66/1000 | Loss: 0.00001020
Iteration 67/1000 | Loss: 0.00001020
Iteration 68/1000 | Loss: 0.00001020
Iteration 69/1000 | Loss: 0.00001020
Iteration 70/1000 | Loss: 0.00001019
Iteration 71/1000 | Loss: 0.00001019
Iteration 72/1000 | Loss: 0.00001019
Iteration 73/1000 | Loss: 0.00001019
Iteration 74/1000 | Loss: 0.00001019
Iteration 75/1000 | Loss: 0.00001019
Iteration 76/1000 | Loss: 0.00001019
Iteration 77/1000 | Loss: 0.00001019
Iteration 78/1000 | Loss: 0.00001019
Iteration 79/1000 | Loss: 0.00001019
Iteration 80/1000 | Loss: 0.00001019
Iteration 81/1000 | Loss: 0.00001019
Iteration 82/1000 | Loss: 0.00001019
Iteration 83/1000 | Loss: 0.00001019
Iteration 84/1000 | Loss: 0.00001019
Iteration 85/1000 | Loss: 0.00001019
Iteration 86/1000 | Loss: 0.00001019
Iteration 87/1000 | Loss: 0.00001019
Iteration 88/1000 | Loss: 0.00001019
Iteration 89/1000 | Loss: 0.00001019
Iteration 90/1000 | Loss: 0.00001019
Iteration 91/1000 | Loss: 0.00001019
Iteration 92/1000 | Loss: 0.00001019
Iteration 93/1000 | Loss: 0.00001019
Iteration 94/1000 | Loss: 0.00001019
Iteration 95/1000 | Loss: 0.00001019
Iteration 96/1000 | Loss: 0.00001019
Iteration 97/1000 | Loss: 0.00001019
Iteration 98/1000 | Loss: 0.00001019
Iteration 99/1000 | Loss: 0.00001019
Iteration 100/1000 | Loss: 0.00001019
Iteration 101/1000 | Loss: 0.00001019
Iteration 102/1000 | Loss: 0.00001019
Iteration 103/1000 | Loss: 0.00001019
Iteration 104/1000 | Loss: 0.00001019
Iteration 105/1000 | Loss: 0.00001019
Iteration 106/1000 | Loss: 0.00001019
Iteration 107/1000 | Loss: 0.00001019
Iteration 108/1000 | Loss: 0.00001019
Iteration 109/1000 | Loss: 0.00001019
Iteration 110/1000 | Loss: 0.00001019
Iteration 111/1000 | Loss: 0.00001019
Iteration 112/1000 | Loss: 0.00001019
Iteration 113/1000 | Loss: 0.00001019
Iteration 114/1000 | Loss: 0.00001019
Iteration 115/1000 | Loss: 0.00001019
Iteration 116/1000 | Loss: 0.00001019
Iteration 117/1000 | Loss: 0.00001019
Iteration 118/1000 | Loss: 0.00001019
Iteration 119/1000 | Loss: 0.00001019
Iteration 120/1000 | Loss: 0.00001019
Iteration 121/1000 | Loss: 0.00001019
Iteration 122/1000 | Loss: 0.00001019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.0185290193476249e-05, 1.0185290193476249e-05, 1.0185290193476249e-05, 1.0185290193476249e-05, 1.0185290193476249e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0185290193476249e-05

Optimization complete. Final v2v error: 2.694843053817749 mm

Highest mean error: 2.8535990715026855 mm for frame 90

Lowest mean error: 2.563209056854248 mm for frame 148

Saving results

Total time: 31.249404668807983
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461721
Iteration 2/25 | Loss: 0.00091628
Iteration 3/25 | Loss: 0.00076466
Iteration 4/25 | Loss: 0.00074381
Iteration 5/25 | Loss: 0.00073611
Iteration 6/25 | Loss: 0.00073424
Iteration 7/25 | Loss: 0.00073364
Iteration 8/25 | Loss: 0.00073357
Iteration 9/25 | Loss: 0.00073357
Iteration 10/25 | Loss: 0.00073357
Iteration 11/25 | Loss: 0.00073357
Iteration 12/25 | Loss: 0.00073357
Iteration 13/25 | Loss: 0.00073357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007335670525208116, 0.0007335670525208116, 0.0007335670525208116, 0.0007335670525208116, 0.0007335670525208116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007335670525208116

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.21394491
Iteration 2/25 | Loss: 0.00077445
Iteration 3/25 | Loss: 0.00077444
Iteration 4/25 | Loss: 0.00077444
Iteration 5/25 | Loss: 0.00077444
Iteration 6/25 | Loss: 0.00077444
Iteration 7/25 | Loss: 0.00077444
Iteration 8/25 | Loss: 0.00077444
Iteration 9/25 | Loss: 0.00077444
Iteration 10/25 | Loss: 0.00077444
Iteration 11/25 | Loss: 0.00077444
Iteration 12/25 | Loss: 0.00077444
Iteration 13/25 | Loss: 0.00077444
Iteration 14/25 | Loss: 0.00077444
Iteration 15/25 | Loss: 0.00077444
Iteration 16/25 | Loss: 0.00077444
Iteration 17/25 | Loss: 0.00077444
Iteration 18/25 | Loss: 0.00077444
Iteration 19/25 | Loss: 0.00077444
Iteration 20/25 | Loss: 0.00077444
Iteration 21/25 | Loss: 0.00077444
Iteration 22/25 | Loss: 0.00077444
Iteration 23/25 | Loss: 0.00077444
Iteration 24/25 | Loss: 0.00077444
Iteration 25/25 | Loss: 0.00077444

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077444
Iteration 2/1000 | Loss: 0.00002716
Iteration 3/1000 | Loss: 0.00001964
Iteration 4/1000 | Loss: 0.00001739
Iteration 5/1000 | Loss: 0.00001654
Iteration 6/1000 | Loss: 0.00001599
Iteration 7/1000 | Loss: 0.00001554
Iteration 8/1000 | Loss: 0.00001524
Iteration 9/1000 | Loss: 0.00001511
Iteration 10/1000 | Loss: 0.00001508
Iteration 11/1000 | Loss: 0.00001494
Iteration 12/1000 | Loss: 0.00001491
Iteration 13/1000 | Loss: 0.00001490
Iteration 14/1000 | Loss: 0.00001490
Iteration 15/1000 | Loss: 0.00001485
Iteration 16/1000 | Loss: 0.00001485
Iteration 17/1000 | Loss: 0.00001484
Iteration 18/1000 | Loss: 0.00001479
Iteration 19/1000 | Loss: 0.00001478
Iteration 20/1000 | Loss: 0.00001477
Iteration 21/1000 | Loss: 0.00001476
Iteration 22/1000 | Loss: 0.00001476
Iteration 23/1000 | Loss: 0.00001475
Iteration 24/1000 | Loss: 0.00001475
Iteration 25/1000 | Loss: 0.00001474
Iteration 26/1000 | Loss: 0.00001474
Iteration 27/1000 | Loss: 0.00001472
Iteration 28/1000 | Loss: 0.00001472
Iteration 29/1000 | Loss: 0.00001471
Iteration 30/1000 | Loss: 0.00001471
Iteration 31/1000 | Loss: 0.00001471
Iteration 32/1000 | Loss: 0.00001470
Iteration 33/1000 | Loss: 0.00001470
Iteration 34/1000 | Loss: 0.00001469
Iteration 35/1000 | Loss: 0.00001469
Iteration 36/1000 | Loss: 0.00001468
Iteration 37/1000 | Loss: 0.00001468
Iteration 38/1000 | Loss: 0.00001468
Iteration 39/1000 | Loss: 0.00001468
Iteration 40/1000 | Loss: 0.00001468
Iteration 41/1000 | Loss: 0.00001467
Iteration 42/1000 | Loss: 0.00001467
Iteration 43/1000 | Loss: 0.00001467
Iteration 44/1000 | Loss: 0.00001467
Iteration 45/1000 | Loss: 0.00001467
Iteration 46/1000 | Loss: 0.00001467
Iteration 47/1000 | Loss: 0.00001467
Iteration 48/1000 | Loss: 0.00001467
Iteration 49/1000 | Loss: 0.00001466
Iteration 50/1000 | Loss: 0.00001466
Iteration 51/1000 | Loss: 0.00001466
Iteration 52/1000 | Loss: 0.00001466
Iteration 53/1000 | Loss: 0.00001466
Iteration 54/1000 | Loss: 0.00001466
Iteration 55/1000 | Loss: 0.00001466
Iteration 56/1000 | Loss: 0.00001466
Iteration 57/1000 | Loss: 0.00001466
Iteration 58/1000 | Loss: 0.00001466
Iteration 59/1000 | Loss: 0.00001465
Iteration 60/1000 | Loss: 0.00001465
Iteration 61/1000 | Loss: 0.00001465
Iteration 62/1000 | Loss: 0.00001465
Iteration 63/1000 | Loss: 0.00001465
Iteration 64/1000 | Loss: 0.00001465
Iteration 65/1000 | Loss: 0.00001465
Iteration 66/1000 | Loss: 0.00001465
Iteration 67/1000 | Loss: 0.00001464
Iteration 68/1000 | Loss: 0.00001464
Iteration 69/1000 | Loss: 0.00001464
Iteration 70/1000 | Loss: 0.00001464
Iteration 71/1000 | Loss: 0.00001464
Iteration 72/1000 | Loss: 0.00001464
Iteration 73/1000 | Loss: 0.00001464
Iteration 74/1000 | Loss: 0.00001463
Iteration 75/1000 | Loss: 0.00001463
Iteration 76/1000 | Loss: 0.00001463
Iteration 77/1000 | Loss: 0.00001463
Iteration 78/1000 | Loss: 0.00001463
Iteration 79/1000 | Loss: 0.00001463
Iteration 80/1000 | Loss: 0.00001463
Iteration 81/1000 | Loss: 0.00001463
Iteration 82/1000 | Loss: 0.00001463
Iteration 83/1000 | Loss: 0.00001463
Iteration 84/1000 | Loss: 0.00001463
Iteration 85/1000 | Loss: 0.00001463
Iteration 86/1000 | Loss: 0.00001463
Iteration 87/1000 | Loss: 0.00001463
Iteration 88/1000 | Loss: 0.00001462
Iteration 89/1000 | Loss: 0.00001462
Iteration 90/1000 | Loss: 0.00001462
Iteration 91/1000 | Loss: 0.00001462
Iteration 92/1000 | Loss: 0.00001462
Iteration 93/1000 | Loss: 0.00001462
Iteration 94/1000 | Loss: 0.00001462
Iteration 95/1000 | Loss: 0.00001462
Iteration 96/1000 | Loss: 0.00001461
Iteration 97/1000 | Loss: 0.00001461
Iteration 98/1000 | Loss: 0.00001461
Iteration 99/1000 | Loss: 0.00001460
Iteration 100/1000 | Loss: 0.00001460
Iteration 101/1000 | Loss: 0.00001460
Iteration 102/1000 | Loss: 0.00001460
Iteration 103/1000 | Loss: 0.00001460
Iteration 104/1000 | Loss: 0.00001459
Iteration 105/1000 | Loss: 0.00001459
Iteration 106/1000 | Loss: 0.00001459
Iteration 107/1000 | Loss: 0.00001458
Iteration 108/1000 | Loss: 0.00001458
Iteration 109/1000 | Loss: 0.00001458
Iteration 110/1000 | Loss: 0.00001458
Iteration 111/1000 | Loss: 0.00001457
Iteration 112/1000 | Loss: 0.00001457
Iteration 113/1000 | Loss: 0.00001457
Iteration 114/1000 | Loss: 0.00001457
Iteration 115/1000 | Loss: 0.00001457
Iteration 116/1000 | Loss: 0.00001456
Iteration 117/1000 | Loss: 0.00001456
Iteration 118/1000 | Loss: 0.00001456
Iteration 119/1000 | Loss: 0.00001456
Iteration 120/1000 | Loss: 0.00001456
Iteration 121/1000 | Loss: 0.00001455
Iteration 122/1000 | Loss: 0.00001455
Iteration 123/1000 | Loss: 0.00001455
Iteration 124/1000 | Loss: 0.00001455
Iteration 125/1000 | Loss: 0.00001455
Iteration 126/1000 | Loss: 0.00001455
Iteration 127/1000 | Loss: 0.00001455
Iteration 128/1000 | Loss: 0.00001455
Iteration 129/1000 | Loss: 0.00001455
Iteration 130/1000 | Loss: 0.00001455
Iteration 131/1000 | Loss: 0.00001455
Iteration 132/1000 | Loss: 0.00001455
Iteration 133/1000 | Loss: 0.00001454
Iteration 134/1000 | Loss: 0.00001454
Iteration 135/1000 | Loss: 0.00001454
Iteration 136/1000 | Loss: 0.00001454
Iteration 137/1000 | Loss: 0.00001454
Iteration 138/1000 | Loss: 0.00001454
Iteration 139/1000 | Loss: 0.00001454
Iteration 140/1000 | Loss: 0.00001454
Iteration 141/1000 | Loss: 0.00001454
Iteration 142/1000 | Loss: 0.00001453
Iteration 143/1000 | Loss: 0.00001453
Iteration 144/1000 | Loss: 0.00001453
Iteration 145/1000 | Loss: 0.00001453
Iteration 146/1000 | Loss: 0.00001453
Iteration 147/1000 | Loss: 0.00001453
Iteration 148/1000 | Loss: 0.00001453
Iteration 149/1000 | Loss: 0.00001452
Iteration 150/1000 | Loss: 0.00001452
Iteration 151/1000 | Loss: 0.00001452
Iteration 152/1000 | Loss: 0.00001452
Iteration 153/1000 | Loss: 0.00001452
Iteration 154/1000 | Loss: 0.00001452
Iteration 155/1000 | Loss: 0.00001452
Iteration 156/1000 | Loss: 0.00001452
Iteration 157/1000 | Loss: 0.00001452
Iteration 158/1000 | Loss: 0.00001452
Iteration 159/1000 | Loss: 0.00001451
Iteration 160/1000 | Loss: 0.00001451
Iteration 161/1000 | Loss: 0.00001451
Iteration 162/1000 | Loss: 0.00001451
Iteration 163/1000 | Loss: 0.00001451
Iteration 164/1000 | Loss: 0.00001451
Iteration 165/1000 | Loss: 0.00001451
Iteration 166/1000 | Loss: 0.00001451
Iteration 167/1000 | Loss: 0.00001451
Iteration 168/1000 | Loss: 0.00001451
Iteration 169/1000 | Loss: 0.00001451
Iteration 170/1000 | Loss: 0.00001451
Iteration 171/1000 | Loss: 0.00001451
Iteration 172/1000 | Loss: 0.00001451
Iteration 173/1000 | Loss: 0.00001451
Iteration 174/1000 | Loss: 0.00001451
Iteration 175/1000 | Loss: 0.00001451
Iteration 176/1000 | Loss: 0.00001451
Iteration 177/1000 | Loss: 0.00001451
Iteration 178/1000 | Loss: 0.00001451
Iteration 179/1000 | Loss: 0.00001451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.45126450661337e-05, 1.45126450661337e-05, 1.45126450661337e-05, 1.45126450661337e-05, 1.45126450661337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.45126450661337e-05

Optimization complete. Final v2v error: 3.2191779613494873 mm

Highest mean error: 3.575913190841675 mm for frame 61

Lowest mean error: 2.924675703048706 mm for frame 96

Saving results

Total time: 67.96071648597717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016052
Iteration 2/25 | Loss: 0.00277915
Iteration 3/25 | Loss: 0.00169465
Iteration 4/25 | Loss: 0.00145747
Iteration 5/25 | Loss: 0.00128564
Iteration 6/25 | Loss: 0.00114310
Iteration 7/25 | Loss: 0.00103830
Iteration 8/25 | Loss: 0.00097232
Iteration 9/25 | Loss: 0.00095629
Iteration 10/25 | Loss: 0.00091502
Iteration 11/25 | Loss: 0.00090325
Iteration 12/25 | Loss: 0.00089911
Iteration 13/25 | Loss: 0.00092597
Iteration 14/25 | Loss: 0.00086232
Iteration 15/25 | Loss: 0.00084675
Iteration 16/25 | Loss: 0.00084648
Iteration 17/25 | Loss: 0.00084642
Iteration 18/25 | Loss: 0.00084538
Iteration 19/25 | Loss: 0.00084443
Iteration 20/25 | Loss: 0.00084410
Iteration 21/25 | Loss: 0.00084410
Iteration 22/25 | Loss: 0.00084410
Iteration 23/25 | Loss: 0.00084410
Iteration 24/25 | Loss: 0.00084410
Iteration 25/25 | Loss: 0.00084409

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56389093
Iteration 2/25 | Loss: 0.00107108
Iteration 3/25 | Loss: 0.00102994
Iteration 4/25 | Loss: 0.00102994
Iteration 5/25 | Loss: 0.00102994
Iteration 6/25 | Loss: 0.00102994
Iteration 7/25 | Loss: 0.00102994
Iteration 8/25 | Loss: 0.00102994
Iteration 9/25 | Loss: 0.00102994
Iteration 10/25 | Loss: 0.00102994
Iteration 11/25 | Loss: 0.00102994
Iteration 12/25 | Loss: 0.00102994
Iteration 13/25 | Loss: 0.00102994
Iteration 14/25 | Loss: 0.00102994
Iteration 15/25 | Loss: 0.00102994
Iteration 16/25 | Loss: 0.00102994
Iteration 17/25 | Loss: 0.00102994
Iteration 18/25 | Loss: 0.00102994
Iteration 19/25 | Loss: 0.00102994
Iteration 20/25 | Loss: 0.00102994
Iteration 21/25 | Loss: 0.00102994
Iteration 22/25 | Loss: 0.00102994
Iteration 23/25 | Loss: 0.00102994
Iteration 24/25 | Loss: 0.00102994
Iteration 25/25 | Loss: 0.00102994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102994
Iteration 2/1000 | Loss: 0.00005409
Iteration 3/1000 | Loss: 0.00009338
Iteration 4/1000 | Loss: 0.00005733
Iteration 5/1000 | Loss: 0.00004030
Iteration 6/1000 | Loss: 0.00012560
Iteration 7/1000 | Loss: 0.00003300
Iteration 8/1000 | Loss: 0.00014056
Iteration 9/1000 | Loss: 0.00004290
Iteration 10/1000 | Loss: 0.00016652
Iteration 11/1000 | Loss: 0.00003617
Iteration 12/1000 | Loss: 0.00003022
Iteration 13/1000 | Loss: 0.00002811
Iteration 14/1000 | Loss: 0.00002732
Iteration 15/1000 | Loss: 0.00002655
Iteration 16/1000 | Loss: 0.00002614
Iteration 17/1000 | Loss: 0.00002582
Iteration 18/1000 | Loss: 0.00006258
Iteration 19/1000 | Loss: 0.00002849
Iteration 20/1000 | Loss: 0.00003241
Iteration 21/1000 | Loss: 0.00002527
Iteration 22/1000 | Loss: 0.00002526
Iteration 23/1000 | Loss: 0.00002526
Iteration 24/1000 | Loss: 0.00002525
Iteration 25/1000 | Loss: 0.00002525
Iteration 26/1000 | Loss: 0.00002524
Iteration 27/1000 | Loss: 0.00002523
Iteration 28/1000 | Loss: 0.00002522
Iteration 29/1000 | Loss: 0.00002521
Iteration 30/1000 | Loss: 0.00002520
Iteration 31/1000 | Loss: 0.00002519
Iteration 32/1000 | Loss: 0.00002518
Iteration 33/1000 | Loss: 0.00002516
Iteration 34/1000 | Loss: 0.00002516
Iteration 35/1000 | Loss: 0.00002516
Iteration 36/1000 | Loss: 0.00002515
Iteration 37/1000 | Loss: 0.00002514
Iteration 38/1000 | Loss: 0.00002514
Iteration 39/1000 | Loss: 0.00002513
Iteration 40/1000 | Loss: 0.00002513
Iteration 41/1000 | Loss: 0.00002513
Iteration 42/1000 | Loss: 0.00002512
Iteration 43/1000 | Loss: 0.00002510
Iteration 44/1000 | Loss: 0.00002509
Iteration 45/1000 | Loss: 0.00002508
Iteration 46/1000 | Loss: 0.00002508
Iteration 47/1000 | Loss: 0.00002508
Iteration 48/1000 | Loss: 0.00002507
Iteration 49/1000 | Loss: 0.00002507
Iteration 50/1000 | Loss: 0.00002506
Iteration 51/1000 | Loss: 0.00002506
Iteration 52/1000 | Loss: 0.00002506
Iteration 53/1000 | Loss: 0.00002506
Iteration 54/1000 | Loss: 0.00002504
Iteration 55/1000 | Loss: 0.00002504
Iteration 56/1000 | Loss: 0.00002504
Iteration 57/1000 | Loss: 0.00002503
Iteration 58/1000 | Loss: 0.00002502
Iteration 59/1000 | Loss: 0.00002502
Iteration 60/1000 | Loss: 0.00002502
Iteration 61/1000 | Loss: 0.00002502
Iteration 62/1000 | Loss: 0.00002502
Iteration 63/1000 | Loss: 0.00002501
Iteration 64/1000 | Loss: 0.00002501
Iteration 65/1000 | Loss: 0.00002501
Iteration 66/1000 | Loss: 0.00002501
Iteration 67/1000 | Loss: 0.00002501
Iteration 68/1000 | Loss: 0.00002501
Iteration 69/1000 | Loss: 0.00002501
Iteration 70/1000 | Loss: 0.00002501
Iteration 71/1000 | Loss: 0.00002501
Iteration 72/1000 | Loss: 0.00002501
Iteration 73/1000 | Loss: 0.00002500
Iteration 74/1000 | Loss: 0.00002500
Iteration 75/1000 | Loss: 0.00002500
Iteration 76/1000 | Loss: 0.00002500
Iteration 77/1000 | Loss: 0.00002500
Iteration 78/1000 | Loss: 0.00002500
Iteration 79/1000 | Loss: 0.00002500
Iteration 80/1000 | Loss: 0.00002500
Iteration 81/1000 | Loss: 0.00002500
Iteration 82/1000 | Loss: 0.00002500
Iteration 83/1000 | Loss: 0.00002500
Iteration 84/1000 | Loss: 0.00002500
Iteration 85/1000 | Loss: 0.00002500
Iteration 86/1000 | Loss: 0.00002500
Iteration 87/1000 | Loss: 0.00002499
Iteration 88/1000 | Loss: 0.00002499
Iteration 89/1000 | Loss: 0.00002499
Iteration 90/1000 | Loss: 0.00002499
Iteration 91/1000 | Loss: 0.00002499
Iteration 92/1000 | Loss: 0.00002499
Iteration 93/1000 | Loss: 0.00002499
Iteration 94/1000 | Loss: 0.00002499
Iteration 95/1000 | Loss: 0.00002499
Iteration 96/1000 | Loss: 0.00002499
Iteration 97/1000 | Loss: 0.00002499
Iteration 98/1000 | Loss: 0.00002499
Iteration 99/1000 | Loss: 0.00002499
Iteration 100/1000 | Loss: 0.00002498
Iteration 101/1000 | Loss: 0.00002498
Iteration 102/1000 | Loss: 0.00002498
Iteration 103/1000 | Loss: 0.00002498
Iteration 104/1000 | Loss: 0.00002498
Iteration 105/1000 | Loss: 0.00002498
Iteration 106/1000 | Loss: 0.00002498
Iteration 107/1000 | Loss: 0.00002497
Iteration 108/1000 | Loss: 0.00002497
Iteration 109/1000 | Loss: 0.00002497
Iteration 110/1000 | Loss: 0.00002496
Iteration 111/1000 | Loss: 0.00002496
Iteration 112/1000 | Loss: 0.00002496
Iteration 113/1000 | Loss: 0.00002496
Iteration 114/1000 | Loss: 0.00002496
Iteration 115/1000 | Loss: 0.00002496
Iteration 116/1000 | Loss: 0.00002496
Iteration 117/1000 | Loss: 0.00002496
Iteration 118/1000 | Loss: 0.00002496
Iteration 119/1000 | Loss: 0.00002495
Iteration 120/1000 | Loss: 0.00002495
Iteration 121/1000 | Loss: 0.00002495
Iteration 122/1000 | Loss: 0.00002495
Iteration 123/1000 | Loss: 0.00002495
Iteration 124/1000 | Loss: 0.00002495
Iteration 125/1000 | Loss: 0.00002495
Iteration 126/1000 | Loss: 0.00002495
Iteration 127/1000 | Loss: 0.00002495
Iteration 128/1000 | Loss: 0.00002495
Iteration 129/1000 | Loss: 0.00002495
Iteration 130/1000 | Loss: 0.00002495
Iteration 131/1000 | Loss: 0.00002495
Iteration 132/1000 | Loss: 0.00002495
Iteration 133/1000 | Loss: 0.00002495
Iteration 134/1000 | Loss: 0.00002495
Iteration 135/1000 | Loss: 0.00002495
Iteration 136/1000 | Loss: 0.00002495
Iteration 137/1000 | Loss: 0.00002495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.4951852537924424e-05, 2.4951852537924424e-05, 2.4951852537924424e-05, 2.4951852537924424e-05, 2.4951852537924424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4951852537924424e-05

Optimization complete. Final v2v error: 4.102430820465088 mm

Highest mean error: 4.835770606994629 mm for frame 169

Lowest mean error: 3.6192562580108643 mm for frame 85

Saving results

Total time: 95.37088418006897
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021088
Iteration 2/25 | Loss: 0.00273081
Iteration 3/25 | Loss: 0.00169530
Iteration 4/25 | Loss: 0.00151858
Iteration 5/25 | Loss: 0.00133009
Iteration 6/25 | Loss: 0.00124370
Iteration 7/25 | Loss: 0.00135350
Iteration 8/25 | Loss: 0.00118659
Iteration 9/25 | Loss: 0.00107072
Iteration 10/25 | Loss: 0.00102046
Iteration 11/25 | Loss: 0.00102117
Iteration 12/25 | Loss: 0.00100792
Iteration 13/25 | Loss: 0.00099784
Iteration 14/25 | Loss: 0.00098129
Iteration 15/25 | Loss: 0.00095396
Iteration 16/25 | Loss: 0.00094577
Iteration 17/25 | Loss: 0.00092731
Iteration 18/25 | Loss: 0.00091067
Iteration 19/25 | Loss: 0.00090612
Iteration 20/25 | Loss: 0.00089840
Iteration 21/25 | Loss: 0.00090177
Iteration 22/25 | Loss: 0.00090008
Iteration 23/25 | Loss: 0.00089847
Iteration 24/25 | Loss: 0.00089871
Iteration 25/25 | Loss: 0.00090128

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56455171
Iteration 2/25 | Loss: 0.00136540
Iteration 3/25 | Loss: 0.00136540
Iteration 4/25 | Loss: 0.00136540
Iteration 5/25 | Loss: 0.00136540
Iteration 6/25 | Loss: 0.00136540
Iteration 7/25 | Loss: 0.00136540
Iteration 8/25 | Loss: 0.00136540
Iteration 9/25 | Loss: 0.00136540
Iteration 10/25 | Loss: 0.00136540
Iteration 11/25 | Loss: 0.00136540
Iteration 12/25 | Loss: 0.00136540
Iteration 13/25 | Loss: 0.00136540
Iteration 14/25 | Loss: 0.00136540
Iteration 15/25 | Loss: 0.00136540
Iteration 16/25 | Loss: 0.00136540
Iteration 17/25 | Loss: 0.00136540
Iteration 18/25 | Loss: 0.00136540
Iteration 19/25 | Loss: 0.00136540
Iteration 20/25 | Loss: 0.00136540
Iteration 21/25 | Loss: 0.00136540
Iteration 22/25 | Loss: 0.00136540
Iteration 23/25 | Loss: 0.00136540
Iteration 24/25 | Loss: 0.00136540
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013653981732204556, 0.0013653981732204556, 0.0013653981732204556, 0.0013653981732204556, 0.0013653981732204556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013653981732204556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136540
Iteration 2/1000 | Loss: 0.00024288
Iteration 3/1000 | Loss: 0.00022902
Iteration 4/1000 | Loss: 0.00019507
Iteration 5/1000 | Loss: 0.00024177
Iteration 6/1000 | Loss: 0.00032516
Iteration 7/1000 | Loss: 0.00010111
Iteration 8/1000 | Loss: 0.00006630
Iteration 9/1000 | Loss: 0.00031474
Iteration 10/1000 | Loss: 0.00006193
Iteration 11/1000 | Loss: 0.00005433
Iteration 12/1000 | Loss: 0.00005066
Iteration 13/1000 | Loss: 0.00008750
Iteration 14/1000 | Loss: 0.00005187
Iteration 15/1000 | Loss: 0.00004751
Iteration 16/1000 | Loss: 0.00004547
Iteration 17/1000 | Loss: 0.00005166
Iteration 18/1000 | Loss: 0.00004557
Iteration 19/1000 | Loss: 0.00005438
Iteration 20/1000 | Loss: 0.00004420
Iteration 21/1000 | Loss: 0.00072929
Iteration 22/1000 | Loss: 0.00005623
Iteration 23/1000 | Loss: 0.00004527
Iteration 24/1000 | Loss: 0.00004160
Iteration 25/1000 | Loss: 0.00004025
Iteration 26/1000 | Loss: 0.00003946
Iteration 27/1000 | Loss: 0.00003868
Iteration 28/1000 | Loss: 0.00003825
Iteration 29/1000 | Loss: 0.00003782
Iteration 30/1000 | Loss: 0.00004125
Iteration 31/1000 | Loss: 0.00003815
Iteration 32/1000 | Loss: 0.00003727
Iteration 33/1000 | Loss: 0.00087157
Iteration 34/1000 | Loss: 0.00006143
Iteration 35/1000 | Loss: 0.00005131
Iteration 36/1000 | Loss: 0.00004583
Iteration 37/1000 | Loss: 0.00004574
Iteration 38/1000 | Loss: 0.00004218
Iteration 39/1000 | Loss: 0.00003893
Iteration 40/1000 | Loss: 0.00004449
Iteration 41/1000 | Loss: 0.00004096
Iteration 42/1000 | Loss: 0.00004398
Iteration 43/1000 | Loss: 0.00005871
Iteration 44/1000 | Loss: 0.00003991
Iteration 45/1000 | Loss: 0.00003685
Iteration 46/1000 | Loss: 0.00003423
Iteration 47/1000 | Loss: 0.00004581
Iteration 48/1000 | Loss: 0.00003191
Iteration 49/1000 | Loss: 0.00003134
Iteration 50/1000 | Loss: 0.00003111
Iteration 51/1000 | Loss: 0.00003099
Iteration 52/1000 | Loss: 0.00003095
Iteration 53/1000 | Loss: 0.00003095
Iteration 54/1000 | Loss: 0.00003094
Iteration 55/1000 | Loss: 0.00003090
Iteration 56/1000 | Loss: 0.00003080
Iteration 57/1000 | Loss: 0.00003079
Iteration 58/1000 | Loss: 0.00003079
Iteration 59/1000 | Loss: 0.00003079
Iteration 60/1000 | Loss: 0.00003079
Iteration 61/1000 | Loss: 0.00003078
Iteration 62/1000 | Loss: 0.00003078
Iteration 63/1000 | Loss: 0.00003077
Iteration 64/1000 | Loss: 0.00003077
Iteration 65/1000 | Loss: 0.00003077
Iteration 66/1000 | Loss: 0.00003076
Iteration 67/1000 | Loss: 0.00003076
Iteration 68/1000 | Loss: 0.00003076
Iteration 69/1000 | Loss: 0.00003075
Iteration 70/1000 | Loss: 0.00003075
Iteration 71/1000 | Loss: 0.00003075
Iteration 72/1000 | Loss: 0.00003074
Iteration 73/1000 | Loss: 0.00003073
Iteration 74/1000 | Loss: 0.00003073
Iteration 75/1000 | Loss: 0.00003073
Iteration 76/1000 | Loss: 0.00003072
Iteration 77/1000 | Loss: 0.00003072
Iteration 78/1000 | Loss: 0.00003072
Iteration 79/1000 | Loss: 0.00003071
Iteration 80/1000 | Loss: 0.00003071
Iteration 81/1000 | Loss: 0.00003070
Iteration 82/1000 | Loss: 0.00003070
Iteration 83/1000 | Loss: 0.00003070
Iteration 84/1000 | Loss: 0.00003070
Iteration 85/1000 | Loss: 0.00003070
Iteration 86/1000 | Loss: 0.00003070
Iteration 87/1000 | Loss: 0.00003070
Iteration 88/1000 | Loss: 0.00003070
Iteration 89/1000 | Loss: 0.00003070
Iteration 90/1000 | Loss: 0.00003070
Iteration 91/1000 | Loss: 0.00003070
Iteration 92/1000 | Loss: 0.00003070
Iteration 93/1000 | Loss: 0.00003070
Iteration 94/1000 | Loss: 0.00003070
Iteration 95/1000 | Loss: 0.00003069
Iteration 96/1000 | Loss: 0.00003069
Iteration 97/1000 | Loss: 0.00003068
Iteration 98/1000 | Loss: 0.00003068
Iteration 99/1000 | Loss: 0.00003068
Iteration 100/1000 | Loss: 0.00003068
Iteration 101/1000 | Loss: 0.00003068
Iteration 102/1000 | Loss: 0.00003068
Iteration 103/1000 | Loss: 0.00003068
Iteration 104/1000 | Loss: 0.00003068
Iteration 105/1000 | Loss: 0.00003068
Iteration 106/1000 | Loss: 0.00003068
Iteration 107/1000 | Loss: 0.00003068
Iteration 108/1000 | Loss: 0.00003068
Iteration 109/1000 | Loss: 0.00003068
Iteration 110/1000 | Loss: 0.00003067
Iteration 111/1000 | Loss: 0.00003067
Iteration 112/1000 | Loss: 0.00003067
Iteration 113/1000 | Loss: 0.00003067
Iteration 114/1000 | Loss: 0.00003067
Iteration 115/1000 | Loss: 0.00003066
Iteration 116/1000 | Loss: 0.00003066
Iteration 117/1000 | Loss: 0.00003066
Iteration 118/1000 | Loss: 0.00003066
Iteration 119/1000 | Loss: 0.00003066
Iteration 120/1000 | Loss: 0.00003066
Iteration 121/1000 | Loss: 0.00003066
Iteration 122/1000 | Loss: 0.00003066
Iteration 123/1000 | Loss: 0.00003065
Iteration 124/1000 | Loss: 0.00003065
Iteration 125/1000 | Loss: 0.00003065
Iteration 126/1000 | Loss: 0.00003065
Iteration 127/1000 | Loss: 0.00003065
Iteration 128/1000 | Loss: 0.00003065
Iteration 129/1000 | Loss: 0.00003064
Iteration 130/1000 | Loss: 0.00003064
Iteration 131/1000 | Loss: 0.00003064
Iteration 132/1000 | Loss: 0.00003064
Iteration 133/1000 | Loss: 0.00003064
Iteration 134/1000 | Loss: 0.00003064
Iteration 135/1000 | Loss: 0.00003064
Iteration 136/1000 | Loss: 0.00003064
Iteration 137/1000 | Loss: 0.00003063
Iteration 138/1000 | Loss: 0.00003063
Iteration 139/1000 | Loss: 0.00003063
Iteration 140/1000 | Loss: 0.00003063
Iteration 141/1000 | Loss: 0.00003063
Iteration 142/1000 | Loss: 0.00003063
Iteration 143/1000 | Loss: 0.00003063
Iteration 144/1000 | Loss: 0.00003063
Iteration 145/1000 | Loss: 0.00003063
Iteration 146/1000 | Loss: 0.00003063
Iteration 147/1000 | Loss: 0.00003063
Iteration 148/1000 | Loss: 0.00003063
Iteration 149/1000 | Loss: 0.00003063
Iteration 150/1000 | Loss: 0.00003062
Iteration 151/1000 | Loss: 0.00003062
Iteration 152/1000 | Loss: 0.00003062
Iteration 153/1000 | Loss: 0.00003062
Iteration 154/1000 | Loss: 0.00003062
Iteration 155/1000 | Loss: 0.00003061
Iteration 156/1000 | Loss: 0.00003061
Iteration 157/1000 | Loss: 0.00003061
Iteration 158/1000 | Loss: 0.00003061
Iteration 159/1000 | Loss: 0.00003061
Iteration 160/1000 | Loss: 0.00003061
Iteration 161/1000 | Loss: 0.00003060
Iteration 162/1000 | Loss: 0.00003060
Iteration 163/1000 | Loss: 0.00003060
Iteration 164/1000 | Loss: 0.00003060
Iteration 165/1000 | Loss: 0.00003060
Iteration 166/1000 | Loss: 0.00003060
Iteration 167/1000 | Loss: 0.00003060
Iteration 168/1000 | Loss: 0.00003059
Iteration 169/1000 | Loss: 0.00003059
Iteration 170/1000 | Loss: 0.00003059
Iteration 171/1000 | Loss: 0.00003059
Iteration 172/1000 | Loss: 0.00003059
Iteration 173/1000 | Loss: 0.00003059
Iteration 174/1000 | Loss: 0.00003058
Iteration 175/1000 | Loss: 0.00003058
Iteration 176/1000 | Loss: 0.00003058
Iteration 177/1000 | Loss: 0.00003058
Iteration 178/1000 | Loss: 0.00003058
Iteration 179/1000 | Loss: 0.00003058
Iteration 180/1000 | Loss: 0.00003058
Iteration 181/1000 | Loss: 0.00003058
Iteration 182/1000 | Loss: 0.00003058
Iteration 183/1000 | Loss: 0.00003058
Iteration 184/1000 | Loss: 0.00003058
Iteration 185/1000 | Loss: 0.00003058
Iteration 186/1000 | Loss: 0.00003058
Iteration 187/1000 | Loss: 0.00003058
Iteration 188/1000 | Loss: 0.00003058
Iteration 189/1000 | Loss: 0.00003058
Iteration 190/1000 | Loss: 0.00003058
Iteration 191/1000 | Loss: 0.00003057
Iteration 192/1000 | Loss: 0.00003057
Iteration 193/1000 | Loss: 0.00003057
Iteration 194/1000 | Loss: 0.00003057
Iteration 195/1000 | Loss: 0.00003057
Iteration 196/1000 | Loss: 0.00003057
Iteration 197/1000 | Loss: 0.00003057
Iteration 198/1000 | Loss: 0.00003057
Iteration 199/1000 | Loss: 0.00003057
Iteration 200/1000 | Loss: 0.00003057
Iteration 201/1000 | Loss: 0.00003057
Iteration 202/1000 | Loss: 0.00003057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [3.05739558825735e-05, 3.05739558825735e-05, 3.05739558825735e-05, 3.05739558825735e-05, 3.05739558825735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.05739558825735e-05

Optimization complete. Final v2v error: 4.3779072761535645 mm

Highest mean error: 6.344143867492676 mm for frame 103

Lowest mean error: 3.087584972381592 mm for frame 1

Saving results

Total time: 127.89349341392517
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992667
Iteration 2/25 | Loss: 0.00156852
Iteration 3/25 | Loss: 0.00102712
Iteration 4/25 | Loss: 0.00097182
Iteration 5/25 | Loss: 0.00095905
Iteration 6/25 | Loss: 0.00095700
Iteration 7/25 | Loss: 0.00095700
Iteration 8/25 | Loss: 0.00095700
Iteration 9/25 | Loss: 0.00095700
Iteration 10/25 | Loss: 0.00095700
Iteration 11/25 | Loss: 0.00095700
Iteration 12/25 | Loss: 0.00095700
Iteration 13/25 | Loss: 0.00095700
Iteration 14/25 | Loss: 0.00095700
Iteration 15/25 | Loss: 0.00095700
Iteration 16/25 | Loss: 0.00095700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009570019319653511, 0.0009570019319653511, 0.0009570019319653511, 0.0009570019319653511, 0.0009570019319653511]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009570019319653511

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92996311
Iteration 2/25 | Loss: 0.00060803
Iteration 3/25 | Loss: 0.00060803
Iteration 4/25 | Loss: 0.00060803
Iteration 5/25 | Loss: 0.00060802
Iteration 6/25 | Loss: 0.00060802
Iteration 7/25 | Loss: 0.00060802
Iteration 8/25 | Loss: 0.00060802
Iteration 9/25 | Loss: 0.00060802
Iteration 10/25 | Loss: 0.00060802
Iteration 11/25 | Loss: 0.00060802
Iteration 12/25 | Loss: 0.00060802
Iteration 13/25 | Loss: 0.00060802
Iteration 14/25 | Loss: 0.00060802
Iteration 15/25 | Loss: 0.00060802
Iteration 16/25 | Loss: 0.00060802
Iteration 17/25 | Loss: 0.00060802
Iteration 18/25 | Loss: 0.00060802
Iteration 19/25 | Loss: 0.00060802
Iteration 20/25 | Loss: 0.00060802
Iteration 21/25 | Loss: 0.00060802
Iteration 22/25 | Loss: 0.00060802
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006080229068174958, 0.0006080229068174958, 0.0006080229068174958, 0.0006080229068174958, 0.0006080229068174958]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006080229068174958

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060802
Iteration 2/1000 | Loss: 0.00006400
Iteration 3/1000 | Loss: 0.00004623
Iteration 4/1000 | Loss: 0.00004314
Iteration 5/1000 | Loss: 0.00004140
Iteration 6/1000 | Loss: 0.00004040
Iteration 7/1000 | Loss: 0.00003953
Iteration 8/1000 | Loss: 0.00003899
Iteration 9/1000 | Loss: 0.00003859
Iteration 10/1000 | Loss: 0.00003823
Iteration 11/1000 | Loss: 0.00003788
Iteration 12/1000 | Loss: 0.00003767
Iteration 13/1000 | Loss: 0.00003745
Iteration 14/1000 | Loss: 0.00003726
Iteration 15/1000 | Loss: 0.00003717
Iteration 16/1000 | Loss: 0.00003712
Iteration 17/1000 | Loss: 0.00003711
Iteration 18/1000 | Loss: 0.00003707
Iteration 19/1000 | Loss: 0.00003706
Iteration 20/1000 | Loss: 0.00003704
Iteration 21/1000 | Loss: 0.00003703
Iteration 22/1000 | Loss: 0.00003702
Iteration 23/1000 | Loss: 0.00003701
Iteration 24/1000 | Loss: 0.00003701
Iteration 25/1000 | Loss: 0.00003700
Iteration 26/1000 | Loss: 0.00003698
Iteration 27/1000 | Loss: 0.00003698
Iteration 28/1000 | Loss: 0.00003695
Iteration 29/1000 | Loss: 0.00003694
Iteration 30/1000 | Loss: 0.00003693
Iteration 31/1000 | Loss: 0.00003691
Iteration 32/1000 | Loss: 0.00003690
Iteration 33/1000 | Loss: 0.00003687
Iteration 34/1000 | Loss: 0.00003686
Iteration 35/1000 | Loss: 0.00003685
Iteration 36/1000 | Loss: 0.00003684
Iteration 37/1000 | Loss: 0.00003684
Iteration 38/1000 | Loss: 0.00003681
Iteration 39/1000 | Loss: 0.00003681
Iteration 40/1000 | Loss: 0.00003681
Iteration 41/1000 | Loss: 0.00003678
Iteration 42/1000 | Loss: 0.00003678
Iteration 43/1000 | Loss: 0.00003678
Iteration 44/1000 | Loss: 0.00003677
Iteration 45/1000 | Loss: 0.00003677
Iteration 46/1000 | Loss: 0.00003677
Iteration 47/1000 | Loss: 0.00003677
Iteration 48/1000 | Loss: 0.00003677
Iteration 49/1000 | Loss: 0.00003677
Iteration 50/1000 | Loss: 0.00003677
Iteration 51/1000 | Loss: 0.00003676
Iteration 52/1000 | Loss: 0.00003676
Iteration 53/1000 | Loss: 0.00003676
Iteration 54/1000 | Loss: 0.00003675
Iteration 55/1000 | Loss: 0.00003675
Iteration 56/1000 | Loss: 0.00003675
Iteration 57/1000 | Loss: 0.00003675
Iteration 58/1000 | Loss: 0.00003674
Iteration 59/1000 | Loss: 0.00003674
Iteration 60/1000 | Loss: 0.00003674
Iteration 61/1000 | Loss: 0.00003674
Iteration 62/1000 | Loss: 0.00003673
Iteration 63/1000 | Loss: 0.00003673
Iteration 64/1000 | Loss: 0.00003673
Iteration 65/1000 | Loss: 0.00003673
Iteration 66/1000 | Loss: 0.00003673
Iteration 67/1000 | Loss: 0.00003673
Iteration 68/1000 | Loss: 0.00003672
Iteration 69/1000 | Loss: 0.00003672
Iteration 70/1000 | Loss: 0.00003672
Iteration 71/1000 | Loss: 0.00003672
Iteration 72/1000 | Loss: 0.00003672
Iteration 73/1000 | Loss: 0.00003672
Iteration 74/1000 | Loss: 0.00003672
Iteration 75/1000 | Loss: 0.00003672
Iteration 76/1000 | Loss: 0.00003672
Iteration 77/1000 | Loss: 0.00003672
Iteration 78/1000 | Loss: 0.00003672
Iteration 79/1000 | Loss: 0.00003672
Iteration 80/1000 | Loss: 0.00003672
Iteration 81/1000 | Loss: 0.00003672
Iteration 82/1000 | Loss: 0.00003672
Iteration 83/1000 | Loss: 0.00003671
Iteration 84/1000 | Loss: 0.00003671
Iteration 85/1000 | Loss: 0.00003671
Iteration 86/1000 | Loss: 0.00003671
Iteration 87/1000 | Loss: 0.00003671
Iteration 88/1000 | Loss: 0.00003671
Iteration 89/1000 | Loss: 0.00003671
Iteration 90/1000 | Loss: 0.00003670
Iteration 91/1000 | Loss: 0.00003670
Iteration 92/1000 | Loss: 0.00003670
Iteration 93/1000 | Loss: 0.00003670
Iteration 94/1000 | Loss: 0.00003670
Iteration 95/1000 | Loss: 0.00003670
Iteration 96/1000 | Loss: 0.00003670
Iteration 97/1000 | Loss: 0.00003670
Iteration 98/1000 | Loss: 0.00003670
Iteration 99/1000 | Loss: 0.00003670
Iteration 100/1000 | Loss: 0.00003670
Iteration 101/1000 | Loss: 0.00003670
Iteration 102/1000 | Loss: 0.00003670
Iteration 103/1000 | Loss: 0.00003670
Iteration 104/1000 | Loss: 0.00003670
Iteration 105/1000 | Loss: 0.00003670
Iteration 106/1000 | Loss: 0.00003670
Iteration 107/1000 | Loss: 0.00003670
Iteration 108/1000 | Loss: 0.00003670
Iteration 109/1000 | Loss: 0.00003670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [3.6699009797303006e-05, 3.6699009797303006e-05, 3.6699009797303006e-05, 3.6699009797303006e-05, 3.6699009797303006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6699009797303006e-05

Optimization complete. Final v2v error: 4.996399402618408 mm

Highest mean error: 6.122780799865723 mm for frame 94

Lowest mean error: 4.092586517333984 mm for frame 57

Saving results

Total time: 106.95559120178223
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00753626
Iteration 2/25 | Loss: 0.00131327
Iteration 3/25 | Loss: 0.00097793
Iteration 4/25 | Loss: 0.00093327
Iteration 5/25 | Loss: 0.00092372
Iteration 6/25 | Loss: 0.00092251
Iteration 7/25 | Loss: 0.00092211
Iteration 8/25 | Loss: 0.00092211
Iteration 9/25 | Loss: 0.00092211
Iteration 10/25 | Loss: 0.00092211
Iteration 11/25 | Loss: 0.00092211
Iteration 12/25 | Loss: 0.00092211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00092210533330217, 0.00092210533330217, 0.00092210533330217, 0.00092210533330217, 0.00092210533330217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00092210533330217

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87567639
Iteration 2/25 | Loss: 0.00065474
Iteration 3/25 | Loss: 0.00065467
Iteration 4/25 | Loss: 0.00065467
Iteration 5/25 | Loss: 0.00065467
Iteration 6/25 | Loss: 0.00065467
Iteration 7/25 | Loss: 0.00065467
Iteration 8/25 | Loss: 0.00065467
Iteration 9/25 | Loss: 0.00065467
Iteration 10/25 | Loss: 0.00065467
Iteration 11/25 | Loss: 0.00065467
Iteration 12/25 | Loss: 0.00065467
Iteration 13/25 | Loss: 0.00065467
Iteration 14/25 | Loss: 0.00065467
Iteration 15/25 | Loss: 0.00065467
Iteration 16/25 | Loss: 0.00065467
Iteration 17/25 | Loss: 0.00065467
Iteration 18/25 | Loss: 0.00065467
Iteration 19/25 | Loss: 0.00065467
Iteration 20/25 | Loss: 0.00065467
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006546686636283994, 0.0006546686636283994, 0.0006546686636283994, 0.0006546686636283994, 0.0006546686636283994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006546686636283994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065467
Iteration 2/1000 | Loss: 0.00005721
Iteration 3/1000 | Loss: 0.00004242
Iteration 4/1000 | Loss: 0.00003746
Iteration 5/1000 | Loss: 0.00003527
Iteration 6/1000 | Loss: 0.00003420
Iteration 7/1000 | Loss: 0.00003346
Iteration 8/1000 | Loss: 0.00003286
Iteration 9/1000 | Loss: 0.00003232
Iteration 10/1000 | Loss: 0.00003188
Iteration 11/1000 | Loss: 0.00003159
Iteration 12/1000 | Loss: 0.00003136
Iteration 13/1000 | Loss: 0.00003117
Iteration 14/1000 | Loss: 0.00003092
Iteration 15/1000 | Loss: 0.00003073
Iteration 16/1000 | Loss: 0.00003060
Iteration 17/1000 | Loss: 0.00003052
Iteration 18/1000 | Loss: 0.00003048
Iteration 19/1000 | Loss: 0.00003038
Iteration 20/1000 | Loss: 0.00003037
Iteration 21/1000 | Loss: 0.00003036
Iteration 22/1000 | Loss: 0.00003036
Iteration 23/1000 | Loss: 0.00003036
Iteration 24/1000 | Loss: 0.00003036
Iteration 25/1000 | Loss: 0.00003035
Iteration 26/1000 | Loss: 0.00003035
Iteration 27/1000 | Loss: 0.00003034
Iteration 28/1000 | Loss: 0.00003033
Iteration 29/1000 | Loss: 0.00003033
Iteration 30/1000 | Loss: 0.00003022
Iteration 31/1000 | Loss: 0.00003014
Iteration 32/1000 | Loss: 0.00003014
Iteration 33/1000 | Loss: 0.00003002
Iteration 34/1000 | Loss: 0.00003000
Iteration 35/1000 | Loss: 0.00002999
Iteration 36/1000 | Loss: 0.00002994
Iteration 37/1000 | Loss: 0.00002989
Iteration 38/1000 | Loss: 0.00002989
Iteration 39/1000 | Loss: 0.00002986
Iteration 40/1000 | Loss: 0.00002984
Iteration 41/1000 | Loss: 0.00002983
Iteration 42/1000 | Loss: 0.00002983
Iteration 43/1000 | Loss: 0.00002982
Iteration 44/1000 | Loss: 0.00002982
Iteration 45/1000 | Loss: 0.00002981
Iteration 46/1000 | Loss: 0.00002981
Iteration 47/1000 | Loss: 0.00002980
Iteration 48/1000 | Loss: 0.00002980
Iteration 49/1000 | Loss: 0.00002979
Iteration 50/1000 | Loss: 0.00002978
Iteration 51/1000 | Loss: 0.00002978
Iteration 52/1000 | Loss: 0.00002978
Iteration 53/1000 | Loss: 0.00002977
Iteration 54/1000 | Loss: 0.00002977
Iteration 55/1000 | Loss: 0.00002976
Iteration 56/1000 | Loss: 0.00002975
Iteration 57/1000 | Loss: 0.00002975
Iteration 58/1000 | Loss: 0.00002974
Iteration 59/1000 | Loss: 0.00002974
Iteration 60/1000 | Loss: 0.00002973
Iteration 61/1000 | Loss: 0.00002973
Iteration 62/1000 | Loss: 0.00002973
Iteration 63/1000 | Loss: 0.00002973
Iteration 64/1000 | Loss: 0.00002972
Iteration 65/1000 | Loss: 0.00002972
Iteration 66/1000 | Loss: 0.00002971
Iteration 67/1000 | Loss: 0.00002971
Iteration 68/1000 | Loss: 0.00002970
Iteration 69/1000 | Loss: 0.00002970
Iteration 70/1000 | Loss: 0.00002969
Iteration 71/1000 | Loss: 0.00002969
Iteration 72/1000 | Loss: 0.00002969
Iteration 73/1000 | Loss: 0.00002968
Iteration 74/1000 | Loss: 0.00002968
Iteration 75/1000 | Loss: 0.00002968
Iteration 76/1000 | Loss: 0.00002968
Iteration 77/1000 | Loss: 0.00002968
Iteration 78/1000 | Loss: 0.00002967
Iteration 79/1000 | Loss: 0.00002967
Iteration 80/1000 | Loss: 0.00002967
Iteration 81/1000 | Loss: 0.00002967
Iteration 82/1000 | Loss: 0.00002967
Iteration 83/1000 | Loss: 0.00002967
Iteration 84/1000 | Loss: 0.00002967
Iteration 85/1000 | Loss: 0.00002966
Iteration 86/1000 | Loss: 0.00002966
Iteration 87/1000 | Loss: 0.00002966
Iteration 88/1000 | Loss: 0.00002966
Iteration 89/1000 | Loss: 0.00002966
Iteration 90/1000 | Loss: 0.00002966
Iteration 91/1000 | Loss: 0.00002966
Iteration 92/1000 | Loss: 0.00002966
Iteration 93/1000 | Loss: 0.00002966
Iteration 94/1000 | Loss: 0.00002965
Iteration 95/1000 | Loss: 0.00002965
Iteration 96/1000 | Loss: 0.00002965
Iteration 97/1000 | Loss: 0.00002965
Iteration 98/1000 | Loss: 0.00002965
Iteration 99/1000 | Loss: 0.00002965
Iteration 100/1000 | Loss: 0.00002965
Iteration 101/1000 | Loss: 0.00002965
Iteration 102/1000 | Loss: 0.00002965
Iteration 103/1000 | Loss: 0.00002965
Iteration 104/1000 | Loss: 0.00002965
Iteration 105/1000 | Loss: 0.00002965
Iteration 106/1000 | Loss: 0.00002965
Iteration 107/1000 | Loss: 0.00002965
Iteration 108/1000 | Loss: 0.00002965
Iteration 109/1000 | Loss: 0.00002965
Iteration 110/1000 | Loss: 0.00002965
Iteration 111/1000 | Loss: 0.00002965
Iteration 112/1000 | Loss: 0.00002965
Iteration 113/1000 | Loss: 0.00002965
Iteration 114/1000 | Loss: 0.00002965
Iteration 115/1000 | Loss: 0.00002965
Iteration 116/1000 | Loss: 0.00002965
Iteration 117/1000 | Loss: 0.00002965
Iteration 118/1000 | Loss: 0.00002965
Iteration 119/1000 | Loss: 0.00002965
Iteration 120/1000 | Loss: 0.00002965
Iteration 121/1000 | Loss: 0.00002965
Iteration 122/1000 | Loss: 0.00002965
Iteration 123/1000 | Loss: 0.00002965
Iteration 124/1000 | Loss: 0.00002965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.9652708690264262e-05, 2.9652708690264262e-05, 2.9652708690264262e-05, 2.9652708690264262e-05, 2.9652708690264262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9652708690264262e-05

Optimization complete. Final v2v error: 4.333101272583008 mm

Highest mean error: 5.525681018829346 mm for frame 126

Lowest mean error: 3.512977361679077 mm for frame 0

Saving results

Total time: 59.92007780075073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842130
Iteration 2/25 | Loss: 0.00104188
Iteration 3/25 | Loss: 0.00085502
Iteration 4/25 | Loss: 0.00080682
Iteration 5/25 | Loss: 0.00079962
Iteration 6/25 | Loss: 0.00079793
Iteration 7/25 | Loss: 0.00079750
Iteration 8/25 | Loss: 0.00079750
Iteration 9/25 | Loss: 0.00079750
Iteration 10/25 | Loss: 0.00079750
Iteration 11/25 | Loss: 0.00079750
Iteration 12/25 | Loss: 0.00079750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007974991458468139, 0.0007974991458468139, 0.0007974991458468139, 0.0007974991458468139, 0.0007974991458468139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007974991458468139

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53807878
Iteration 2/25 | Loss: 0.00092463
Iteration 3/25 | Loss: 0.00092461
Iteration 4/25 | Loss: 0.00092461
Iteration 5/25 | Loss: 0.00092461
Iteration 6/25 | Loss: 0.00092461
Iteration 7/25 | Loss: 0.00092461
Iteration 8/25 | Loss: 0.00092461
Iteration 9/25 | Loss: 0.00092461
Iteration 10/25 | Loss: 0.00092461
Iteration 11/25 | Loss: 0.00092461
Iteration 12/25 | Loss: 0.00092461
Iteration 13/25 | Loss: 0.00092461
Iteration 14/25 | Loss: 0.00092461
Iteration 15/25 | Loss: 0.00092461
Iteration 16/25 | Loss: 0.00092461
Iteration 17/25 | Loss: 0.00092461
Iteration 18/25 | Loss: 0.00092461
Iteration 19/25 | Loss: 0.00092461
Iteration 20/25 | Loss: 0.00092461
Iteration 21/25 | Loss: 0.00092461
Iteration 22/25 | Loss: 0.00092461
Iteration 23/25 | Loss: 0.00092461
Iteration 24/25 | Loss: 0.00092461
Iteration 25/25 | Loss: 0.00092461

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092461
Iteration 2/1000 | Loss: 0.00005776
Iteration 3/1000 | Loss: 0.00004097
Iteration 4/1000 | Loss: 0.00003537
Iteration 5/1000 | Loss: 0.00003309
Iteration 6/1000 | Loss: 0.00003115
Iteration 7/1000 | Loss: 0.00003005
Iteration 8/1000 | Loss: 0.00002911
Iteration 9/1000 | Loss: 0.00002849
Iteration 10/1000 | Loss: 0.00002811
Iteration 11/1000 | Loss: 0.00002781
Iteration 12/1000 | Loss: 0.00002766
Iteration 13/1000 | Loss: 0.00002745
Iteration 14/1000 | Loss: 0.00002732
Iteration 15/1000 | Loss: 0.00002727
Iteration 16/1000 | Loss: 0.00002723
Iteration 17/1000 | Loss: 0.00002723
Iteration 18/1000 | Loss: 0.00002717
Iteration 19/1000 | Loss: 0.00002714
Iteration 20/1000 | Loss: 0.00002713
Iteration 21/1000 | Loss: 0.00002710
Iteration 22/1000 | Loss: 0.00002710
Iteration 23/1000 | Loss: 0.00002708
Iteration 24/1000 | Loss: 0.00002707
Iteration 25/1000 | Loss: 0.00002707
Iteration 26/1000 | Loss: 0.00002707
Iteration 27/1000 | Loss: 0.00002707
Iteration 28/1000 | Loss: 0.00002707
Iteration 29/1000 | Loss: 0.00002706
Iteration 30/1000 | Loss: 0.00002706
Iteration 31/1000 | Loss: 0.00002706
Iteration 32/1000 | Loss: 0.00002706
Iteration 33/1000 | Loss: 0.00002706
Iteration 34/1000 | Loss: 0.00002706
Iteration 35/1000 | Loss: 0.00002706
Iteration 36/1000 | Loss: 0.00002706
Iteration 37/1000 | Loss: 0.00002706
Iteration 38/1000 | Loss: 0.00002705
Iteration 39/1000 | Loss: 0.00002703
Iteration 40/1000 | Loss: 0.00002703
Iteration 41/1000 | Loss: 0.00002702
Iteration 42/1000 | Loss: 0.00002702
Iteration 43/1000 | Loss: 0.00002702
Iteration 44/1000 | Loss: 0.00002701
Iteration 45/1000 | Loss: 0.00002701
Iteration 46/1000 | Loss: 0.00002701
Iteration 47/1000 | Loss: 0.00002701
Iteration 48/1000 | Loss: 0.00002701
Iteration 49/1000 | Loss: 0.00002701
Iteration 50/1000 | Loss: 0.00002701
Iteration 51/1000 | Loss: 0.00002701
Iteration 52/1000 | Loss: 0.00002701
Iteration 53/1000 | Loss: 0.00002701
Iteration 54/1000 | Loss: 0.00002700
Iteration 55/1000 | Loss: 0.00002700
Iteration 56/1000 | Loss: 0.00002700
Iteration 57/1000 | Loss: 0.00002700
Iteration 58/1000 | Loss: 0.00002700
Iteration 59/1000 | Loss: 0.00002699
Iteration 60/1000 | Loss: 0.00002699
Iteration 61/1000 | Loss: 0.00002699
Iteration 62/1000 | Loss: 0.00002699
Iteration 63/1000 | Loss: 0.00002699
Iteration 64/1000 | Loss: 0.00002699
Iteration 65/1000 | Loss: 0.00002699
Iteration 66/1000 | Loss: 0.00002698
Iteration 67/1000 | Loss: 0.00002698
Iteration 68/1000 | Loss: 0.00002698
Iteration 69/1000 | Loss: 0.00002698
Iteration 70/1000 | Loss: 0.00002698
Iteration 71/1000 | Loss: 0.00002697
Iteration 72/1000 | Loss: 0.00002697
Iteration 73/1000 | Loss: 0.00002697
Iteration 74/1000 | Loss: 0.00002697
Iteration 75/1000 | Loss: 0.00002696
Iteration 76/1000 | Loss: 0.00002696
Iteration 77/1000 | Loss: 0.00002695
Iteration 78/1000 | Loss: 0.00002695
Iteration 79/1000 | Loss: 0.00002695
Iteration 80/1000 | Loss: 0.00002695
Iteration 81/1000 | Loss: 0.00002695
Iteration 82/1000 | Loss: 0.00002695
Iteration 83/1000 | Loss: 0.00002695
Iteration 84/1000 | Loss: 0.00002694
Iteration 85/1000 | Loss: 0.00002694
Iteration 86/1000 | Loss: 0.00002694
Iteration 87/1000 | Loss: 0.00002694
Iteration 88/1000 | Loss: 0.00002693
Iteration 89/1000 | Loss: 0.00002693
Iteration 90/1000 | Loss: 0.00002693
Iteration 91/1000 | Loss: 0.00002692
Iteration 92/1000 | Loss: 0.00002692
Iteration 93/1000 | Loss: 0.00002692
Iteration 94/1000 | Loss: 0.00002692
Iteration 95/1000 | Loss: 0.00002692
Iteration 96/1000 | Loss: 0.00002691
Iteration 97/1000 | Loss: 0.00002691
Iteration 98/1000 | Loss: 0.00002691
Iteration 99/1000 | Loss: 0.00002691
Iteration 100/1000 | Loss: 0.00002690
Iteration 101/1000 | Loss: 0.00002690
Iteration 102/1000 | Loss: 0.00002690
Iteration 103/1000 | Loss: 0.00002690
Iteration 104/1000 | Loss: 0.00002690
Iteration 105/1000 | Loss: 0.00002689
Iteration 106/1000 | Loss: 0.00002689
Iteration 107/1000 | Loss: 0.00002689
Iteration 108/1000 | Loss: 0.00002689
Iteration 109/1000 | Loss: 0.00002689
Iteration 110/1000 | Loss: 0.00002689
Iteration 111/1000 | Loss: 0.00002689
Iteration 112/1000 | Loss: 0.00002688
Iteration 113/1000 | Loss: 0.00002688
Iteration 114/1000 | Loss: 0.00002688
Iteration 115/1000 | Loss: 0.00002688
Iteration 116/1000 | Loss: 0.00002688
Iteration 117/1000 | Loss: 0.00002688
Iteration 118/1000 | Loss: 0.00002688
Iteration 119/1000 | Loss: 0.00002688
Iteration 120/1000 | Loss: 0.00002688
Iteration 121/1000 | Loss: 0.00002687
Iteration 122/1000 | Loss: 0.00002687
Iteration 123/1000 | Loss: 0.00002687
Iteration 124/1000 | Loss: 0.00002687
Iteration 125/1000 | Loss: 0.00002687
Iteration 126/1000 | Loss: 0.00002687
Iteration 127/1000 | Loss: 0.00002687
Iteration 128/1000 | Loss: 0.00002687
Iteration 129/1000 | Loss: 0.00002687
Iteration 130/1000 | Loss: 0.00002687
Iteration 131/1000 | Loss: 0.00002687
Iteration 132/1000 | Loss: 0.00002687
Iteration 133/1000 | Loss: 0.00002687
Iteration 134/1000 | Loss: 0.00002686
Iteration 135/1000 | Loss: 0.00002686
Iteration 136/1000 | Loss: 0.00002686
Iteration 137/1000 | Loss: 0.00002686
Iteration 138/1000 | Loss: 0.00002686
Iteration 139/1000 | Loss: 0.00002686
Iteration 140/1000 | Loss: 0.00002686
Iteration 141/1000 | Loss: 0.00002686
Iteration 142/1000 | Loss: 0.00002685
Iteration 143/1000 | Loss: 0.00002685
Iteration 144/1000 | Loss: 0.00002685
Iteration 145/1000 | Loss: 0.00002685
Iteration 146/1000 | Loss: 0.00002685
Iteration 147/1000 | Loss: 0.00002685
Iteration 148/1000 | Loss: 0.00002685
Iteration 149/1000 | Loss: 0.00002685
Iteration 150/1000 | Loss: 0.00002685
Iteration 151/1000 | Loss: 0.00002685
Iteration 152/1000 | Loss: 0.00002685
Iteration 153/1000 | Loss: 0.00002685
Iteration 154/1000 | Loss: 0.00002685
Iteration 155/1000 | Loss: 0.00002685
Iteration 156/1000 | Loss: 0.00002684
Iteration 157/1000 | Loss: 0.00002684
Iteration 158/1000 | Loss: 0.00002684
Iteration 159/1000 | Loss: 0.00002684
Iteration 160/1000 | Loss: 0.00002684
Iteration 161/1000 | Loss: 0.00002684
Iteration 162/1000 | Loss: 0.00002684
Iteration 163/1000 | Loss: 0.00002684
Iteration 164/1000 | Loss: 0.00002684
Iteration 165/1000 | Loss: 0.00002684
Iteration 166/1000 | Loss: 0.00002684
Iteration 167/1000 | Loss: 0.00002684
Iteration 168/1000 | Loss: 0.00002684
Iteration 169/1000 | Loss: 0.00002684
Iteration 170/1000 | Loss: 0.00002684
Iteration 171/1000 | Loss: 0.00002684
Iteration 172/1000 | Loss: 0.00002684
Iteration 173/1000 | Loss: 0.00002684
Iteration 174/1000 | Loss: 0.00002684
Iteration 175/1000 | Loss: 0.00002684
Iteration 176/1000 | Loss: 0.00002684
Iteration 177/1000 | Loss: 0.00002684
Iteration 178/1000 | Loss: 0.00002684
Iteration 179/1000 | Loss: 0.00002684
Iteration 180/1000 | Loss: 0.00002684
Iteration 181/1000 | Loss: 0.00002684
Iteration 182/1000 | Loss: 0.00002684
Iteration 183/1000 | Loss: 0.00002684
Iteration 184/1000 | Loss: 0.00002684
Iteration 185/1000 | Loss: 0.00002684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.6843212253879756e-05, 2.6843212253879756e-05, 2.6843212253879756e-05, 2.6843212253879756e-05, 2.6843212253879756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6843212253879756e-05

Optimization complete. Final v2v error: 4.302446365356445 mm

Highest mean error: 4.995014667510986 mm for frame 23

Lowest mean error: 3.9236979484558105 mm for frame 4

Saving results

Total time: 74.86101412773132
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392486
Iteration 2/25 | Loss: 0.00082984
Iteration 3/25 | Loss: 0.00073519
Iteration 4/25 | Loss: 0.00071825
Iteration 5/25 | Loss: 0.00071118
Iteration 6/25 | Loss: 0.00070980
Iteration 7/25 | Loss: 0.00070972
Iteration 8/25 | Loss: 0.00070972
Iteration 9/25 | Loss: 0.00070971
Iteration 10/25 | Loss: 0.00070971
Iteration 11/25 | Loss: 0.00070971
Iteration 12/25 | Loss: 0.00070971
Iteration 13/25 | Loss: 0.00070971
Iteration 14/25 | Loss: 0.00070971
Iteration 15/25 | Loss: 0.00070971
Iteration 16/25 | Loss: 0.00070971
Iteration 17/25 | Loss: 0.00070971
Iteration 18/25 | Loss: 0.00070971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007097096531651914, 0.0007097096531651914, 0.0007097096531651914, 0.0007097096531651914, 0.0007097096531651914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007097096531651914

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55547440
Iteration 2/25 | Loss: 0.00079967
Iteration 3/25 | Loss: 0.00079967
Iteration 4/25 | Loss: 0.00079967
Iteration 5/25 | Loss: 0.00079967
Iteration 6/25 | Loss: 0.00079967
Iteration 7/25 | Loss: 0.00079967
Iteration 8/25 | Loss: 0.00079967
Iteration 9/25 | Loss: 0.00079967
Iteration 10/25 | Loss: 0.00079967
Iteration 11/25 | Loss: 0.00079967
Iteration 12/25 | Loss: 0.00079967
Iteration 13/25 | Loss: 0.00079967
Iteration 14/25 | Loss: 0.00079967
Iteration 15/25 | Loss: 0.00079967
Iteration 16/25 | Loss: 0.00079967
Iteration 17/25 | Loss: 0.00079967
Iteration 18/25 | Loss: 0.00079967
Iteration 19/25 | Loss: 0.00079967
Iteration 20/25 | Loss: 0.00079967
Iteration 21/25 | Loss: 0.00079967
Iteration 22/25 | Loss: 0.00079967
Iteration 23/25 | Loss: 0.00079967
Iteration 24/25 | Loss: 0.00079967
Iteration 25/25 | Loss: 0.00079967

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079967
Iteration 2/1000 | Loss: 0.00002840
Iteration 3/1000 | Loss: 0.00001603
Iteration 4/1000 | Loss: 0.00001468
Iteration 5/1000 | Loss: 0.00001390
Iteration 6/1000 | Loss: 0.00001354
Iteration 7/1000 | Loss: 0.00001318
Iteration 8/1000 | Loss: 0.00001310
Iteration 9/1000 | Loss: 0.00001304
Iteration 10/1000 | Loss: 0.00001303
Iteration 11/1000 | Loss: 0.00001284
Iteration 12/1000 | Loss: 0.00001283
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001271
Iteration 15/1000 | Loss: 0.00001267
Iteration 16/1000 | Loss: 0.00001267
Iteration 17/1000 | Loss: 0.00001266
Iteration 18/1000 | Loss: 0.00001260
Iteration 19/1000 | Loss: 0.00001259
Iteration 20/1000 | Loss: 0.00001259
Iteration 21/1000 | Loss: 0.00001258
Iteration 22/1000 | Loss: 0.00001258
Iteration 23/1000 | Loss: 0.00001257
Iteration 24/1000 | Loss: 0.00001255
Iteration 25/1000 | Loss: 0.00001254
Iteration 26/1000 | Loss: 0.00001254
Iteration 27/1000 | Loss: 0.00001254
Iteration 28/1000 | Loss: 0.00001254
Iteration 29/1000 | Loss: 0.00001254
Iteration 30/1000 | Loss: 0.00001254
Iteration 31/1000 | Loss: 0.00001254
Iteration 32/1000 | Loss: 0.00001254
Iteration 33/1000 | Loss: 0.00001254
Iteration 34/1000 | Loss: 0.00001254
Iteration 35/1000 | Loss: 0.00001253
Iteration 36/1000 | Loss: 0.00001253
Iteration 37/1000 | Loss: 0.00001251
Iteration 38/1000 | Loss: 0.00001251
Iteration 39/1000 | Loss: 0.00001250
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00001250
Iteration 42/1000 | Loss: 0.00001249
Iteration 43/1000 | Loss: 0.00001248
Iteration 44/1000 | Loss: 0.00001248
Iteration 45/1000 | Loss: 0.00001248
Iteration 46/1000 | Loss: 0.00001248
Iteration 47/1000 | Loss: 0.00001248
Iteration 48/1000 | Loss: 0.00001247
Iteration 49/1000 | Loss: 0.00001247
Iteration 50/1000 | Loss: 0.00001247
Iteration 51/1000 | Loss: 0.00001247
Iteration 52/1000 | Loss: 0.00001247
Iteration 53/1000 | Loss: 0.00001247
Iteration 54/1000 | Loss: 0.00001247
Iteration 55/1000 | Loss: 0.00001247
Iteration 56/1000 | Loss: 0.00001247
Iteration 57/1000 | Loss: 0.00001247
Iteration 58/1000 | Loss: 0.00001247
Iteration 59/1000 | Loss: 0.00001247
Iteration 60/1000 | Loss: 0.00001247
Iteration 61/1000 | Loss: 0.00001247
Iteration 62/1000 | Loss: 0.00001246
Iteration 63/1000 | Loss: 0.00001246
Iteration 64/1000 | Loss: 0.00001246
Iteration 65/1000 | Loss: 0.00001246
Iteration 66/1000 | Loss: 0.00001245
Iteration 67/1000 | Loss: 0.00001245
Iteration 68/1000 | Loss: 0.00001245
Iteration 69/1000 | Loss: 0.00001245
Iteration 70/1000 | Loss: 0.00001244
Iteration 71/1000 | Loss: 0.00001244
Iteration 72/1000 | Loss: 0.00001244
Iteration 73/1000 | Loss: 0.00001244
Iteration 74/1000 | Loss: 0.00001243
Iteration 75/1000 | Loss: 0.00001243
Iteration 76/1000 | Loss: 0.00001242
Iteration 77/1000 | Loss: 0.00001241
Iteration 78/1000 | Loss: 0.00001241
Iteration 79/1000 | Loss: 0.00001241
Iteration 80/1000 | Loss: 0.00001240
Iteration 81/1000 | Loss: 0.00001240
Iteration 82/1000 | Loss: 0.00001240
Iteration 83/1000 | Loss: 0.00001240
Iteration 84/1000 | Loss: 0.00001240
Iteration 85/1000 | Loss: 0.00001240
Iteration 86/1000 | Loss: 0.00001240
Iteration 87/1000 | Loss: 0.00001239
Iteration 88/1000 | Loss: 0.00001239
Iteration 89/1000 | Loss: 0.00001238
Iteration 90/1000 | Loss: 0.00001238
Iteration 91/1000 | Loss: 0.00001237
Iteration 92/1000 | Loss: 0.00001237
Iteration 93/1000 | Loss: 0.00001237
Iteration 94/1000 | Loss: 0.00001237
Iteration 95/1000 | Loss: 0.00001237
Iteration 96/1000 | Loss: 0.00001236
Iteration 97/1000 | Loss: 0.00001236
Iteration 98/1000 | Loss: 0.00001236
Iteration 99/1000 | Loss: 0.00001236
Iteration 100/1000 | Loss: 0.00001236
Iteration 101/1000 | Loss: 0.00001235
Iteration 102/1000 | Loss: 0.00001235
Iteration 103/1000 | Loss: 0.00001235
Iteration 104/1000 | Loss: 0.00001235
Iteration 105/1000 | Loss: 0.00001234
Iteration 106/1000 | Loss: 0.00001234
Iteration 107/1000 | Loss: 0.00001234
Iteration 108/1000 | Loss: 0.00001234
Iteration 109/1000 | Loss: 0.00001234
Iteration 110/1000 | Loss: 0.00001233
Iteration 111/1000 | Loss: 0.00001233
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001232
Iteration 114/1000 | Loss: 0.00001232
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001232
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001231
Iteration 122/1000 | Loss: 0.00001231
Iteration 123/1000 | Loss: 0.00001231
Iteration 124/1000 | Loss: 0.00001231
Iteration 125/1000 | Loss: 0.00001231
Iteration 126/1000 | Loss: 0.00001231
Iteration 127/1000 | Loss: 0.00001230
Iteration 128/1000 | Loss: 0.00001230
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001230
Iteration 131/1000 | Loss: 0.00001230
Iteration 132/1000 | Loss: 0.00001230
Iteration 133/1000 | Loss: 0.00001230
Iteration 134/1000 | Loss: 0.00001230
Iteration 135/1000 | Loss: 0.00001229
Iteration 136/1000 | Loss: 0.00001229
Iteration 137/1000 | Loss: 0.00001229
Iteration 138/1000 | Loss: 0.00001229
Iteration 139/1000 | Loss: 0.00001229
Iteration 140/1000 | Loss: 0.00001229
Iteration 141/1000 | Loss: 0.00001229
Iteration 142/1000 | Loss: 0.00001229
Iteration 143/1000 | Loss: 0.00001229
Iteration 144/1000 | Loss: 0.00001229
Iteration 145/1000 | Loss: 0.00001229
Iteration 146/1000 | Loss: 0.00001229
Iteration 147/1000 | Loss: 0.00001229
Iteration 148/1000 | Loss: 0.00001229
Iteration 149/1000 | Loss: 0.00001229
Iteration 150/1000 | Loss: 0.00001228
Iteration 151/1000 | Loss: 0.00001228
Iteration 152/1000 | Loss: 0.00001228
Iteration 153/1000 | Loss: 0.00001228
Iteration 154/1000 | Loss: 0.00001228
Iteration 155/1000 | Loss: 0.00001228
Iteration 156/1000 | Loss: 0.00001228
Iteration 157/1000 | Loss: 0.00001228
Iteration 158/1000 | Loss: 0.00001228
Iteration 159/1000 | Loss: 0.00001228
Iteration 160/1000 | Loss: 0.00001228
Iteration 161/1000 | Loss: 0.00001228
Iteration 162/1000 | Loss: 0.00001228
Iteration 163/1000 | Loss: 0.00001228
Iteration 164/1000 | Loss: 0.00001228
Iteration 165/1000 | Loss: 0.00001228
Iteration 166/1000 | Loss: 0.00001228
Iteration 167/1000 | Loss: 0.00001228
Iteration 168/1000 | Loss: 0.00001228
Iteration 169/1000 | Loss: 0.00001228
Iteration 170/1000 | Loss: 0.00001228
Iteration 171/1000 | Loss: 0.00001228
Iteration 172/1000 | Loss: 0.00001228
Iteration 173/1000 | Loss: 0.00001228
Iteration 174/1000 | Loss: 0.00001228
Iteration 175/1000 | Loss: 0.00001228
Iteration 176/1000 | Loss: 0.00001228
Iteration 177/1000 | Loss: 0.00001228
Iteration 178/1000 | Loss: 0.00001227
Iteration 179/1000 | Loss: 0.00001227
Iteration 180/1000 | Loss: 0.00001227
Iteration 181/1000 | Loss: 0.00001227
Iteration 182/1000 | Loss: 0.00001227
Iteration 183/1000 | Loss: 0.00001227
Iteration 184/1000 | Loss: 0.00001227
Iteration 185/1000 | Loss: 0.00001227
Iteration 186/1000 | Loss: 0.00001227
Iteration 187/1000 | Loss: 0.00001227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.2273915672267321e-05, 1.2273915672267321e-05, 1.2273915672267321e-05, 1.2273915672267321e-05, 1.2273915672267321e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2273915672267321e-05

Optimization complete. Final v2v error: 2.9968724250793457 mm

Highest mean error: 3.1318156719207764 mm for frame 39

Lowest mean error: 2.8784539699554443 mm for frame 0

Saving results

Total time: 39.485764265060425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016267
Iteration 2/25 | Loss: 0.00188386
Iteration 3/25 | Loss: 0.00127659
Iteration 4/25 | Loss: 0.00113827
Iteration 5/25 | Loss: 0.00108989
Iteration 6/25 | Loss: 0.00112006
Iteration 7/25 | Loss: 0.00149131
Iteration 8/25 | Loss: 0.00173544
Iteration 9/25 | Loss: 0.00102681
Iteration 10/25 | Loss: 0.00080948
Iteration 11/25 | Loss: 0.00074974
Iteration 12/25 | Loss: 0.00074435
Iteration 13/25 | Loss: 0.00074308
Iteration 14/25 | Loss: 0.00074258
Iteration 15/25 | Loss: 0.00074258
Iteration 16/25 | Loss: 0.00074258
Iteration 17/25 | Loss: 0.00074258
Iteration 18/25 | Loss: 0.00074258
Iteration 19/25 | Loss: 0.00074258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007425830699503422, 0.0007425830699503422, 0.0007425830699503422, 0.0007425830699503422, 0.0007425830699503422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007425830699503422

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52251327
Iteration 2/25 | Loss: 0.00085298
Iteration 3/25 | Loss: 0.00085298
Iteration 4/25 | Loss: 0.00085298
Iteration 5/25 | Loss: 0.00085298
Iteration 6/25 | Loss: 0.00085298
Iteration 7/25 | Loss: 0.00085298
Iteration 8/25 | Loss: 0.00085298
Iteration 9/25 | Loss: 0.00085298
Iteration 10/25 | Loss: 0.00085298
Iteration 11/25 | Loss: 0.00085298
Iteration 12/25 | Loss: 0.00085298
Iteration 13/25 | Loss: 0.00085298
Iteration 14/25 | Loss: 0.00085298
Iteration 15/25 | Loss: 0.00085298
Iteration 16/25 | Loss: 0.00085298
Iteration 17/25 | Loss: 0.00085298
Iteration 18/25 | Loss: 0.00085298
Iteration 19/25 | Loss: 0.00085298
Iteration 20/25 | Loss: 0.00085298
Iteration 21/25 | Loss: 0.00085298
Iteration 22/25 | Loss: 0.00085298
Iteration 23/25 | Loss: 0.00085298
Iteration 24/25 | Loss: 0.00085298
Iteration 25/25 | Loss: 0.00085298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085298
Iteration 2/1000 | Loss: 0.00001762
Iteration 3/1000 | Loss: 0.00001510
Iteration 4/1000 | Loss: 0.00001381
Iteration 5/1000 | Loss: 0.00001316
Iteration 6/1000 | Loss: 0.00001272
Iteration 7/1000 | Loss: 0.00001249
Iteration 8/1000 | Loss: 0.00001234
Iteration 9/1000 | Loss: 0.00001229
Iteration 10/1000 | Loss: 0.00001228
Iteration 11/1000 | Loss: 0.00001213
Iteration 12/1000 | Loss: 0.00001209
Iteration 13/1000 | Loss: 0.00001208
Iteration 14/1000 | Loss: 0.00001205
Iteration 15/1000 | Loss: 0.00001205
Iteration 16/1000 | Loss: 0.00001205
Iteration 17/1000 | Loss: 0.00001205
Iteration 18/1000 | Loss: 0.00001205
Iteration 19/1000 | Loss: 0.00001205
Iteration 20/1000 | Loss: 0.00001205
Iteration 21/1000 | Loss: 0.00001205
Iteration 22/1000 | Loss: 0.00001204
Iteration 23/1000 | Loss: 0.00001204
Iteration 24/1000 | Loss: 0.00001204
Iteration 25/1000 | Loss: 0.00001204
Iteration 26/1000 | Loss: 0.00001203
Iteration 27/1000 | Loss: 0.00001203
Iteration 28/1000 | Loss: 0.00001202
Iteration 29/1000 | Loss: 0.00001201
Iteration 30/1000 | Loss: 0.00001201
Iteration 31/1000 | Loss: 0.00001200
Iteration 32/1000 | Loss: 0.00001200
Iteration 33/1000 | Loss: 0.00001200
Iteration 34/1000 | Loss: 0.00001199
Iteration 35/1000 | Loss: 0.00001199
Iteration 36/1000 | Loss: 0.00001199
Iteration 37/1000 | Loss: 0.00001199
Iteration 38/1000 | Loss: 0.00001199
Iteration 39/1000 | Loss: 0.00001199
Iteration 40/1000 | Loss: 0.00001198
Iteration 41/1000 | Loss: 0.00001198
Iteration 42/1000 | Loss: 0.00001198
Iteration 43/1000 | Loss: 0.00001198
Iteration 44/1000 | Loss: 0.00001198
Iteration 45/1000 | Loss: 0.00001198
Iteration 46/1000 | Loss: 0.00001198
Iteration 47/1000 | Loss: 0.00001198
Iteration 48/1000 | Loss: 0.00001197
Iteration 49/1000 | Loss: 0.00001197
Iteration 50/1000 | Loss: 0.00001197
Iteration 51/1000 | Loss: 0.00001197
Iteration 52/1000 | Loss: 0.00001197
Iteration 53/1000 | Loss: 0.00001197
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001197
Iteration 57/1000 | Loss: 0.00001196
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001196
Iteration 61/1000 | Loss: 0.00001196
Iteration 62/1000 | Loss: 0.00001196
Iteration 63/1000 | Loss: 0.00001196
Iteration 64/1000 | Loss: 0.00001195
Iteration 65/1000 | Loss: 0.00001195
Iteration 66/1000 | Loss: 0.00001195
Iteration 67/1000 | Loss: 0.00001195
Iteration 68/1000 | Loss: 0.00001195
Iteration 69/1000 | Loss: 0.00001195
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001195
Iteration 72/1000 | Loss: 0.00001195
Iteration 73/1000 | Loss: 0.00001195
Iteration 74/1000 | Loss: 0.00001194
Iteration 75/1000 | Loss: 0.00001194
Iteration 76/1000 | Loss: 0.00001194
Iteration 77/1000 | Loss: 0.00001194
Iteration 78/1000 | Loss: 0.00001194
Iteration 79/1000 | Loss: 0.00001194
Iteration 80/1000 | Loss: 0.00001194
Iteration 81/1000 | Loss: 0.00001194
Iteration 82/1000 | Loss: 0.00001194
Iteration 83/1000 | Loss: 0.00001193
Iteration 84/1000 | Loss: 0.00001193
Iteration 85/1000 | Loss: 0.00001193
Iteration 86/1000 | Loss: 0.00001193
Iteration 87/1000 | Loss: 0.00001193
Iteration 88/1000 | Loss: 0.00001193
Iteration 89/1000 | Loss: 0.00001193
Iteration 90/1000 | Loss: 0.00001193
Iteration 91/1000 | Loss: 0.00001193
Iteration 92/1000 | Loss: 0.00001193
Iteration 93/1000 | Loss: 0.00001193
Iteration 94/1000 | Loss: 0.00001193
Iteration 95/1000 | Loss: 0.00001193
Iteration 96/1000 | Loss: 0.00001193
Iteration 97/1000 | Loss: 0.00001193
Iteration 98/1000 | Loss: 0.00001193
Iteration 99/1000 | Loss: 0.00001192
Iteration 100/1000 | Loss: 0.00001192
Iteration 101/1000 | Loss: 0.00001192
Iteration 102/1000 | Loss: 0.00001192
Iteration 103/1000 | Loss: 0.00001192
Iteration 104/1000 | Loss: 0.00001192
Iteration 105/1000 | Loss: 0.00001192
Iteration 106/1000 | Loss: 0.00001192
Iteration 107/1000 | Loss: 0.00001192
Iteration 108/1000 | Loss: 0.00001192
Iteration 109/1000 | Loss: 0.00001192
Iteration 110/1000 | Loss: 0.00001192
Iteration 111/1000 | Loss: 0.00001192
Iteration 112/1000 | Loss: 0.00001192
Iteration 113/1000 | Loss: 0.00001192
Iteration 114/1000 | Loss: 0.00001192
Iteration 115/1000 | Loss: 0.00001191
Iteration 116/1000 | Loss: 0.00001191
Iteration 117/1000 | Loss: 0.00001191
Iteration 118/1000 | Loss: 0.00001191
Iteration 119/1000 | Loss: 0.00001191
Iteration 120/1000 | Loss: 0.00001191
Iteration 121/1000 | Loss: 0.00001191
Iteration 122/1000 | Loss: 0.00001191
Iteration 123/1000 | Loss: 0.00001191
Iteration 124/1000 | Loss: 0.00001191
Iteration 125/1000 | Loss: 0.00001191
Iteration 126/1000 | Loss: 0.00001191
Iteration 127/1000 | Loss: 0.00001191
Iteration 128/1000 | Loss: 0.00001191
Iteration 129/1000 | Loss: 0.00001191
Iteration 130/1000 | Loss: 0.00001191
Iteration 131/1000 | Loss: 0.00001191
Iteration 132/1000 | Loss: 0.00001191
Iteration 133/1000 | Loss: 0.00001191
Iteration 134/1000 | Loss: 0.00001191
Iteration 135/1000 | Loss: 0.00001191
Iteration 136/1000 | Loss: 0.00001191
Iteration 137/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.190717830468202e-05, 1.190717830468202e-05, 1.190717830468202e-05, 1.190717830468202e-05, 1.190717830468202e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.190717830468202e-05

Optimization complete. Final v2v error: 2.8916685581207275 mm

Highest mean error: 2.9877560138702393 mm for frame 2

Lowest mean error: 2.831421136856079 mm for frame 56

Saving results

Total time: 44.97891306877136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844116
Iteration 2/25 | Loss: 0.00114592
Iteration 3/25 | Loss: 0.00078389
Iteration 4/25 | Loss: 0.00072879
Iteration 5/25 | Loss: 0.00072074
Iteration 6/25 | Loss: 0.00071880
Iteration 7/25 | Loss: 0.00071804
Iteration 8/25 | Loss: 0.00071782
Iteration 9/25 | Loss: 0.00071779
Iteration 10/25 | Loss: 0.00071779
Iteration 11/25 | Loss: 0.00071779
Iteration 12/25 | Loss: 0.00071779
Iteration 13/25 | Loss: 0.00071779
Iteration 14/25 | Loss: 0.00071779
Iteration 15/25 | Loss: 0.00071779
Iteration 16/25 | Loss: 0.00071779
Iteration 17/25 | Loss: 0.00071779
Iteration 18/25 | Loss: 0.00071779
Iteration 19/25 | Loss: 0.00071779
Iteration 20/25 | Loss: 0.00071779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007177944644354284, 0.0007177944644354284, 0.0007177944644354284, 0.0007177944644354284, 0.0007177944644354284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007177944644354284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55572188
Iteration 2/25 | Loss: 0.00086311
Iteration 3/25 | Loss: 0.00086311
Iteration 4/25 | Loss: 0.00086311
Iteration 5/25 | Loss: 0.00086311
Iteration 6/25 | Loss: 0.00086311
Iteration 7/25 | Loss: 0.00086311
Iteration 8/25 | Loss: 0.00086311
Iteration 9/25 | Loss: 0.00086311
Iteration 10/25 | Loss: 0.00086311
Iteration 11/25 | Loss: 0.00086311
Iteration 12/25 | Loss: 0.00086311
Iteration 13/25 | Loss: 0.00086311
Iteration 14/25 | Loss: 0.00086311
Iteration 15/25 | Loss: 0.00086311
Iteration 16/25 | Loss: 0.00086311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008631070959381759, 0.0008631070959381759, 0.0008631070959381759, 0.0008631070959381759, 0.0008631070959381759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008631070959381759

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086311
Iteration 2/1000 | Loss: 0.00001825
Iteration 3/1000 | Loss: 0.00001291
Iteration 4/1000 | Loss: 0.00001211
Iteration 5/1000 | Loss: 0.00001152
Iteration 6/1000 | Loss: 0.00001126
Iteration 7/1000 | Loss: 0.00001100
Iteration 8/1000 | Loss: 0.00001088
Iteration 9/1000 | Loss: 0.00001086
Iteration 10/1000 | Loss: 0.00001085
Iteration 11/1000 | Loss: 0.00001078
Iteration 12/1000 | Loss: 0.00001074
Iteration 13/1000 | Loss: 0.00001073
Iteration 14/1000 | Loss: 0.00001072
Iteration 15/1000 | Loss: 0.00001071
Iteration 16/1000 | Loss: 0.00001070
Iteration 17/1000 | Loss: 0.00001070
Iteration 18/1000 | Loss: 0.00001068
Iteration 19/1000 | Loss: 0.00001067
Iteration 20/1000 | Loss: 0.00001067
Iteration 21/1000 | Loss: 0.00001066
Iteration 22/1000 | Loss: 0.00001066
Iteration 23/1000 | Loss: 0.00001065
Iteration 24/1000 | Loss: 0.00001064
Iteration 25/1000 | Loss: 0.00001061
Iteration 26/1000 | Loss: 0.00001060
Iteration 27/1000 | Loss: 0.00001057
Iteration 28/1000 | Loss: 0.00001057
Iteration 29/1000 | Loss: 0.00001056
Iteration 30/1000 | Loss: 0.00001056
Iteration 31/1000 | Loss: 0.00001056
Iteration 32/1000 | Loss: 0.00001056
Iteration 33/1000 | Loss: 0.00001055
Iteration 34/1000 | Loss: 0.00001054
Iteration 35/1000 | Loss: 0.00001054
Iteration 36/1000 | Loss: 0.00001054
Iteration 37/1000 | Loss: 0.00001053
Iteration 38/1000 | Loss: 0.00001052
Iteration 39/1000 | Loss: 0.00001052
Iteration 40/1000 | Loss: 0.00001052
Iteration 41/1000 | Loss: 0.00001052
Iteration 42/1000 | Loss: 0.00001052
Iteration 43/1000 | Loss: 0.00001051
Iteration 44/1000 | Loss: 0.00001050
Iteration 45/1000 | Loss: 0.00001049
Iteration 46/1000 | Loss: 0.00001049
Iteration 47/1000 | Loss: 0.00001048
Iteration 48/1000 | Loss: 0.00001048
Iteration 49/1000 | Loss: 0.00001047
Iteration 50/1000 | Loss: 0.00001047
Iteration 51/1000 | Loss: 0.00001047
Iteration 52/1000 | Loss: 0.00001047
Iteration 53/1000 | Loss: 0.00001047
Iteration 54/1000 | Loss: 0.00001047
Iteration 55/1000 | Loss: 0.00001046
Iteration 56/1000 | Loss: 0.00001046
Iteration 57/1000 | Loss: 0.00001046
Iteration 58/1000 | Loss: 0.00001045
Iteration 59/1000 | Loss: 0.00001045
Iteration 60/1000 | Loss: 0.00001045
Iteration 61/1000 | Loss: 0.00001045
Iteration 62/1000 | Loss: 0.00001044
Iteration 63/1000 | Loss: 0.00001044
Iteration 64/1000 | Loss: 0.00001044
Iteration 65/1000 | Loss: 0.00001044
Iteration 66/1000 | Loss: 0.00001043
Iteration 67/1000 | Loss: 0.00001043
Iteration 68/1000 | Loss: 0.00001043
Iteration 69/1000 | Loss: 0.00001043
Iteration 70/1000 | Loss: 0.00001043
Iteration 71/1000 | Loss: 0.00001043
Iteration 72/1000 | Loss: 0.00001043
Iteration 73/1000 | Loss: 0.00001043
Iteration 74/1000 | Loss: 0.00001042
Iteration 75/1000 | Loss: 0.00001042
Iteration 76/1000 | Loss: 0.00001042
Iteration 77/1000 | Loss: 0.00001042
Iteration 78/1000 | Loss: 0.00001042
Iteration 79/1000 | Loss: 0.00001042
Iteration 80/1000 | Loss: 0.00001042
Iteration 81/1000 | Loss: 0.00001042
Iteration 82/1000 | Loss: 0.00001042
Iteration 83/1000 | Loss: 0.00001042
Iteration 84/1000 | Loss: 0.00001042
Iteration 85/1000 | Loss: 0.00001042
Iteration 86/1000 | Loss: 0.00001042
Iteration 87/1000 | Loss: 0.00001042
Iteration 88/1000 | Loss: 0.00001042
Iteration 89/1000 | Loss: 0.00001042
Iteration 90/1000 | Loss: 0.00001042
Iteration 91/1000 | Loss: 0.00001042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.0422472769278102e-05, 1.0422472769278102e-05, 1.0422472769278102e-05, 1.0422472769278102e-05, 1.0422472769278102e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0422472769278102e-05

Optimization complete. Final v2v error: 2.7298288345336914 mm

Highest mean error: 3.0611765384674072 mm for frame 103

Lowest mean error: 2.5850422382354736 mm for frame 20

Saving results

Total time: 54.038163900375366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891063
Iteration 2/25 | Loss: 0.00092957
Iteration 3/25 | Loss: 0.00075331
Iteration 4/25 | Loss: 0.00072777
Iteration 5/25 | Loss: 0.00072028
Iteration 6/25 | Loss: 0.00071872
Iteration 7/25 | Loss: 0.00071824
Iteration 8/25 | Loss: 0.00071824
Iteration 9/25 | Loss: 0.00071824
Iteration 10/25 | Loss: 0.00071824
Iteration 11/25 | Loss: 0.00071824
Iteration 12/25 | Loss: 0.00071824
Iteration 13/25 | Loss: 0.00071824
Iteration 14/25 | Loss: 0.00071824
Iteration 15/25 | Loss: 0.00071824
Iteration 16/25 | Loss: 0.00071824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007182408007793128, 0.0007182408007793128, 0.0007182408007793128, 0.0007182408007793128, 0.0007182408007793128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007182408007793128

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.19577479
Iteration 2/25 | Loss: 0.00086505
Iteration 3/25 | Loss: 0.00086505
Iteration 4/25 | Loss: 0.00086505
Iteration 5/25 | Loss: 0.00086505
Iteration 6/25 | Loss: 0.00086505
Iteration 7/25 | Loss: 0.00086505
Iteration 8/25 | Loss: 0.00086505
Iteration 9/25 | Loss: 0.00086505
Iteration 10/25 | Loss: 0.00086505
Iteration 11/25 | Loss: 0.00086505
Iteration 12/25 | Loss: 0.00086505
Iteration 13/25 | Loss: 0.00086505
Iteration 14/25 | Loss: 0.00086505
Iteration 15/25 | Loss: 0.00086505
Iteration 16/25 | Loss: 0.00086505
Iteration 17/25 | Loss: 0.00086505
Iteration 18/25 | Loss: 0.00086505
Iteration 19/25 | Loss: 0.00086505
Iteration 20/25 | Loss: 0.00086505
Iteration 21/25 | Loss: 0.00086505
Iteration 22/25 | Loss: 0.00086505
Iteration 23/25 | Loss: 0.00086505
Iteration 24/25 | Loss: 0.00086505
Iteration 25/25 | Loss: 0.00086505

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086505
Iteration 2/1000 | Loss: 0.00002515
Iteration 3/1000 | Loss: 0.00001608
Iteration 4/1000 | Loss: 0.00001421
Iteration 5/1000 | Loss: 0.00001361
Iteration 6/1000 | Loss: 0.00001328
Iteration 7/1000 | Loss: 0.00001304
Iteration 8/1000 | Loss: 0.00001289
Iteration 9/1000 | Loss: 0.00001287
Iteration 10/1000 | Loss: 0.00001276
Iteration 11/1000 | Loss: 0.00001272
Iteration 12/1000 | Loss: 0.00001271
Iteration 13/1000 | Loss: 0.00001271
Iteration 14/1000 | Loss: 0.00001270
Iteration 15/1000 | Loss: 0.00001270
Iteration 16/1000 | Loss: 0.00001269
Iteration 17/1000 | Loss: 0.00001266
Iteration 18/1000 | Loss: 0.00001262
Iteration 19/1000 | Loss: 0.00001260
Iteration 20/1000 | Loss: 0.00001259
Iteration 21/1000 | Loss: 0.00001254
Iteration 22/1000 | Loss: 0.00001253
Iteration 23/1000 | Loss: 0.00001251
Iteration 24/1000 | Loss: 0.00001250
Iteration 25/1000 | Loss: 0.00001250
Iteration 26/1000 | Loss: 0.00001249
Iteration 27/1000 | Loss: 0.00001249
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001248
Iteration 30/1000 | Loss: 0.00001247
Iteration 31/1000 | Loss: 0.00001247
Iteration 32/1000 | Loss: 0.00001246
Iteration 33/1000 | Loss: 0.00001244
Iteration 34/1000 | Loss: 0.00001244
Iteration 35/1000 | Loss: 0.00001243
Iteration 36/1000 | Loss: 0.00001243
Iteration 37/1000 | Loss: 0.00001240
Iteration 38/1000 | Loss: 0.00001240
Iteration 39/1000 | Loss: 0.00001239
Iteration 40/1000 | Loss: 0.00001239
Iteration 41/1000 | Loss: 0.00001238
Iteration 42/1000 | Loss: 0.00001238
Iteration 43/1000 | Loss: 0.00001236
Iteration 44/1000 | Loss: 0.00001236
Iteration 45/1000 | Loss: 0.00001235
Iteration 46/1000 | Loss: 0.00001234
Iteration 47/1000 | Loss: 0.00001234
Iteration 48/1000 | Loss: 0.00001234
Iteration 49/1000 | Loss: 0.00001233
Iteration 50/1000 | Loss: 0.00001233
Iteration 51/1000 | Loss: 0.00001232
Iteration 52/1000 | Loss: 0.00001231
Iteration 53/1000 | Loss: 0.00001231
Iteration 54/1000 | Loss: 0.00001230
Iteration 55/1000 | Loss: 0.00001230
Iteration 56/1000 | Loss: 0.00001230
Iteration 57/1000 | Loss: 0.00001230
Iteration 58/1000 | Loss: 0.00001230
Iteration 59/1000 | Loss: 0.00001230
Iteration 60/1000 | Loss: 0.00001230
Iteration 61/1000 | Loss: 0.00001230
Iteration 62/1000 | Loss: 0.00001230
Iteration 63/1000 | Loss: 0.00001230
Iteration 64/1000 | Loss: 0.00001229
Iteration 65/1000 | Loss: 0.00001229
Iteration 66/1000 | Loss: 0.00001229
Iteration 67/1000 | Loss: 0.00001229
Iteration 68/1000 | Loss: 0.00001229
Iteration 69/1000 | Loss: 0.00001229
Iteration 70/1000 | Loss: 0.00001228
Iteration 71/1000 | Loss: 0.00001228
Iteration 72/1000 | Loss: 0.00001228
Iteration 73/1000 | Loss: 0.00001228
Iteration 74/1000 | Loss: 0.00001227
Iteration 75/1000 | Loss: 0.00001227
Iteration 76/1000 | Loss: 0.00001227
Iteration 77/1000 | Loss: 0.00001227
Iteration 78/1000 | Loss: 0.00001227
Iteration 79/1000 | Loss: 0.00001227
Iteration 80/1000 | Loss: 0.00001226
Iteration 81/1000 | Loss: 0.00001226
Iteration 82/1000 | Loss: 0.00001226
Iteration 83/1000 | Loss: 0.00001226
Iteration 84/1000 | Loss: 0.00001226
Iteration 85/1000 | Loss: 0.00001226
Iteration 86/1000 | Loss: 0.00001226
Iteration 87/1000 | Loss: 0.00001226
Iteration 88/1000 | Loss: 0.00001226
Iteration 89/1000 | Loss: 0.00001225
Iteration 90/1000 | Loss: 0.00001225
Iteration 91/1000 | Loss: 0.00001225
Iteration 92/1000 | Loss: 0.00001225
Iteration 93/1000 | Loss: 0.00001224
Iteration 94/1000 | Loss: 0.00001224
Iteration 95/1000 | Loss: 0.00001224
Iteration 96/1000 | Loss: 0.00001224
Iteration 97/1000 | Loss: 0.00001223
Iteration 98/1000 | Loss: 0.00001223
Iteration 99/1000 | Loss: 0.00001222
Iteration 100/1000 | Loss: 0.00001222
Iteration 101/1000 | Loss: 0.00001222
Iteration 102/1000 | Loss: 0.00001221
Iteration 103/1000 | Loss: 0.00001221
Iteration 104/1000 | Loss: 0.00001221
Iteration 105/1000 | Loss: 0.00001221
Iteration 106/1000 | Loss: 0.00001220
Iteration 107/1000 | Loss: 0.00001220
Iteration 108/1000 | Loss: 0.00001219
Iteration 109/1000 | Loss: 0.00001219
Iteration 110/1000 | Loss: 0.00001219
Iteration 111/1000 | Loss: 0.00001218
Iteration 112/1000 | Loss: 0.00001218
Iteration 113/1000 | Loss: 0.00001218
Iteration 114/1000 | Loss: 0.00001218
Iteration 115/1000 | Loss: 0.00001218
Iteration 116/1000 | Loss: 0.00001217
Iteration 117/1000 | Loss: 0.00001217
Iteration 118/1000 | Loss: 0.00001217
Iteration 119/1000 | Loss: 0.00001217
Iteration 120/1000 | Loss: 0.00001217
Iteration 121/1000 | Loss: 0.00001217
Iteration 122/1000 | Loss: 0.00001217
Iteration 123/1000 | Loss: 0.00001217
Iteration 124/1000 | Loss: 0.00001216
Iteration 125/1000 | Loss: 0.00001216
Iteration 126/1000 | Loss: 0.00001216
Iteration 127/1000 | Loss: 0.00001216
Iteration 128/1000 | Loss: 0.00001216
Iteration 129/1000 | Loss: 0.00001216
Iteration 130/1000 | Loss: 0.00001216
Iteration 131/1000 | Loss: 0.00001216
Iteration 132/1000 | Loss: 0.00001216
Iteration 133/1000 | Loss: 0.00001216
Iteration 134/1000 | Loss: 0.00001216
Iteration 135/1000 | Loss: 0.00001216
Iteration 136/1000 | Loss: 0.00001215
Iteration 137/1000 | Loss: 0.00001215
Iteration 138/1000 | Loss: 0.00001215
Iteration 139/1000 | Loss: 0.00001215
Iteration 140/1000 | Loss: 0.00001215
Iteration 141/1000 | Loss: 0.00001215
Iteration 142/1000 | Loss: 0.00001215
Iteration 143/1000 | Loss: 0.00001215
Iteration 144/1000 | Loss: 0.00001215
Iteration 145/1000 | Loss: 0.00001215
Iteration 146/1000 | Loss: 0.00001214
Iteration 147/1000 | Loss: 0.00001214
Iteration 148/1000 | Loss: 0.00001214
Iteration 149/1000 | Loss: 0.00001214
Iteration 150/1000 | Loss: 0.00001214
Iteration 151/1000 | Loss: 0.00001214
Iteration 152/1000 | Loss: 0.00001214
Iteration 153/1000 | Loss: 0.00001214
Iteration 154/1000 | Loss: 0.00001214
Iteration 155/1000 | Loss: 0.00001214
Iteration 156/1000 | Loss: 0.00001214
Iteration 157/1000 | Loss: 0.00001214
Iteration 158/1000 | Loss: 0.00001214
Iteration 159/1000 | Loss: 0.00001214
Iteration 160/1000 | Loss: 0.00001214
Iteration 161/1000 | Loss: 0.00001214
Iteration 162/1000 | Loss: 0.00001214
Iteration 163/1000 | Loss: 0.00001214
Iteration 164/1000 | Loss: 0.00001214
Iteration 165/1000 | Loss: 0.00001214
Iteration 166/1000 | Loss: 0.00001214
Iteration 167/1000 | Loss: 0.00001214
Iteration 168/1000 | Loss: 0.00001214
Iteration 169/1000 | Loss: 0.00001214
Iteration 170/1000 | Loss: 0.00001214
Iteration 171/1000 | Loss: 0.00001214
Iteration 172/1000 | Loss: 0.00001214
Iteration 173/1000 | Loss: 0.00001214
Iteration 174/1000 | Loss: 0.00001214
Iteration 175/1000 | Loss: 0.00001214
Iteration 176/1000 | Loss: 0.00001214
Iteration 177/1000 | Loss: 0.00001214
Iteration 178/1000 | Loss: 0.00001214
Iteration 179/1000 | Loss: 0.00001214
Iteration 180/1000 | Loss: 0.00001214
Iteration 181/1000 | Loss: 0.00001214
Iteration 182/1000 | Loss: 0.00001214
Iteration 183/1000 | Loss: 0.00001214
Iteration 184/1000 | Loss: 0.00001214
Iteration 185/1000 | Loss: 0.00001214
Iteration 186/1000 | Loss: 0.00001214
Iteration 187/1000 | Loss: 0.00001214
Iteration 188/1000 | Loss: 0.00001214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.2139754289819393e-05, 1.2139754289819393e-05, 1.2139754289819393e-05, 1.2139754289819393e-05, 1.2139754289819393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2139754289819393e-05

Optimization complete. Final v2v error: 2.9116950035095215 mm

Highest mean error: 3.54862117767334 mm for frame 51

Lowest mean error: 2.701380729675293 mm for frame 12

Saving results

Total time: 36.2574725151062
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01154167
Iteration 2/25 | Loss: 0.00178754
Iteration 3/25 | Loss: 0.00099789
Iteration 4/25 | Loss: 0.00090607
Iteration 5/25 | Loss: 0.00089894
Iteration 6/25 | Loss: 0.00091221
Iteration 7/25 | Loss: 0.00091400
Iteration 8/25 | Loss: 0.00089242
Iteration 9/25 | Loss: 0.00090760
Iteration 10/25 | Loss: 0.00088566
Iteration 11/25 | Loss: 0.00088415
Iteration 12/25 | Loss: 0.00088117
Iteration 13/25 | Loss: 0.00088036
Iteration 14/25 | Loss: 0.00088070
Iteration 15/25 | Loss: 0.00088514
Iteration 16/25 | Loss: 0.00089562
Iteration 17/25 | Loss: 0.00088105
Iteration 18/25 | Loss: 0.00089439
Iteration 19/25 | Loss: 0.00088242
Iteration 20/25 | Loss: 0.00089305
Iteration 21/25 | Loss: 0.00089782
Iteration 22/25 | Loss: 0.00089494
Iteration 23/25 | Loss: 0.00089355
Iteration 24/25 | Loss: 0.00088808
Iteration 25/25 | Loss: 0.00089167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57930255
Iteration 2/25 | Loss: 0.00146862
Iteration 3/25 | Loss: 0.00146862
Iteration 4/25 | Loss: 0.00146862
Iteration 5/25 | Loss: 0.00146862
Iteration 6/25 | Loss: 0.00146862
Iteration 7/25 | Loss: 0.00146862
Iteration 8/25 | Loss: 0.00146862
Iteration 9/25 | Loss: 0.00146862
Iteration 10/25 | Loss: 0.00146862
Iteration 11/25 | Loss: 0.00146862
Iteration 12/25 | Loss: 0.00146862
Iteration 13/25 | Loss: 0.00146862
Iteration 14/25 | Loss: 0.00146862
Iteration 15/25 | Loss: 0.00146862
Iteration 16/25 | Loss: 0.00146862
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001468618051148951, 0.001468618051148951, 0.001468618051148951, 0.001468618051148951, 0.001468618051148951]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001468618051148951

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146862
Iteration 2/1000 | Loss: 0.00074685
Iteration 3/1000 | Loss: 0.00043712
Iteration 4/1000 | Loss: 0.00083179
Iteration 5/1000 | Loss: 0.00049875
Iteration 6/1000 | Loss: 0.00022979
Iteration 7/1000 | Loss: 0.00048354
Iteration 8/1000 | Loss: 0.00061547
Iteration 9/1000 | Loss: 0.00003659
Iteration 10/1000 | Loss: 0.00002597
Iteration 11/1000 | Loss: 0.00002388
Iteration 12/1000 | Loss: 0.00002262
Iteration 13/1000 | Loss: 0.00002213
Iteration 14/1000 | Loss: 0.00002178
Iteration 15/1000 | Loss: 0.00002153
Iteration 16/1000 | Loss: 0.00002132
Iteration 17/1000 | Loss: 0.00002131
Iteration 18/1000 | Loss: 0.00002110
Iteration 19/1000 | Loss: 0.00002108
Iteration 20/1000 | Loss: 0.00002107
Iteration 21/1000 | Loss: 0.00002106
Iteration 22/1000 | Loss: 0.00002100
Iteration 23/1000 | Loss: 0.00002092
Iteration 24/1000 | Loss: 0.00002089
Iteration 25/1000 | Loss: 0.00002087
Iteration 26/1000 | Loss: 0.00002087
Iteration 27/1000 | Loss: 0.00002086
Iteration 28/1000 | Loss: 0.00002085
Iteration 29/1000 | Loss: 0.00002083
Iteration 30/1000 | Loss: 0.00002072
Iteration 31/1000 | Loss: 0.00014674
Iteration 32/1000 | Loss: 0.00002706
Iteration 33/1000 | Loss: 0.00002203
Iteration 34/1000 | Loss: 0.00002037
Iteration 35/1000 | Loss: 0.00002001
Iteration 36/1000 | Loss: 0.00001961
Iteration 37/1000 | Loss: 0.00001942
Iteration 38/1000 | Loss: 0.00001936
Iteration 39/1000 | Loss: 0.00001934
Iteration 40/1000 | Loss: 0.00001934
Iteration 41/1000 | Loss: 0.00001932
Iteration 42/1000 | Loss: 0.00001932
Iteration 43/1000 | Loss: 0.00001932
Iteration 44/1000 | Loss: 0.00001932
Iteration 45/1000 | Loss: 0.00001932
Iteration 46/1000 | Loss: 0.00001932
Iteration 47/1000 | Loss: 0.00001932
Iteration 48/1000 | Loss: 0.00001932
Iteration 49/1000 | Loss: 0.00001932
Iteration 50/1000 | Loss: 0.00001932
Iteration 51/1000 | Loss: 0.00001932
Iteration 52/1000 | Loss: 0.00001932
Iteration 53/1000 | Loss: 0.00001931
Iteration 54/1000 | Loss: 0.00001931
Iteration 55/1000 | Loss: 0.00001931
Iteration 56/1000 | Loss: 0.00001931
Iteration 57/1000 | Loss: 0.00001930
Iteration 58/1000 | Loss: 0.00001930
Iteration 59/1000 | Loss: 0.00001930
Iteration 60/1000 | Loss: 0.00001929
Iteration 61/1000 | Loss: 0.00001929
Iteration 62/1000 | Loss: 0.00001929
Iteration 63/1000 | Loss: 0.00001929
Iteration 64/1000 | Loss: 0.00001929
Iteration 65/1000 | Loss: 0.00001928
Iteration 66/1000 | Loss: 0.00001928
Iteration 67/1000 | Loss: 0.00001928
Iteration 68/1000 | Loss: 0.00001927
Iteration 69/1000 | Loss: 0.00001927
Iteration 70/1000 | Loss: 0.00001927
Iteration 71/1000 | Loss: 0.00001927
Iteration 72/1000 | Loss: 0.00001927
Iteration 73/1000 | Loss: 0.00001927
Iteration 74/1000 | Loss: 0.00001926
Iteration 75/1000 | Loss: 0.00001926
Iteration 76/1000 | Loss: 0.00001926
Iteration 77/1000 | Loss: 0.00001926
Iteration 78/1000 | Loss: 0.00001926
Iteration 79/1000 | Loss: 0.00001926
Iteration 80/1000 | Loss: 0.00001926
Iteration 81/1000 | Loss: 0.00001926
Iteration 82/1000 | Loss: 0.00001926
Iteration 83/1000 | Loss: 0.00001926
Iteration 84/1000 | Loss: 0.00001926
Iteration 85/1000 | Loss: 0.00001926
Iteration 86/1000 | Loss: 0.00001926
Iteration 87/1000 | Loss: 0.00001926
Iteration 88/1000 | Loss: 0.00001926
Iteration 89/1000 | Loss: 0.00001926
Iteration 90/1000 | Loss: 0.00001925
Iteration 91/1000 | Loss: 0.00001925
Iteration 92/1000 | Loss: 0.00001925
Iteration 93/1000 | Loss: 0.00001925
Iteration 94/1000 | Loss: 0.00001925
Iteration 95/1000 | Loss: 0.00001925
Iteration 96/1000 | Loss: 0.00001925
Iteration 97/1000 | Loss: 0.00001925
Iteration 98/1000 | Loss: 0.00001925
Iteration 99/1000 | Loss: 0.00001925
Iteration 100/1000 | Loss: 0.00001925
Iteration 101/1000 | Loss: 0.00001925
Iteration 102/1000 | Loss: 0.00001925
Iteration 103/1000 | Loss: 0.00001925
Iteration 104/1000 | Loss: 0.00001925
Iteration 105/1000 | Loss: 0.00001925
Iteration 106/1000 | Loss: 0.00001925
Iteration 107/1000 | Loss: 0.00001925
Iteration 108/1000 | Loss: 0.00001925
Iteration 109/1000 | Loss: 0.00001925
Iteration 110/1000 | Loss: 0.00001925
Iteration 111/1000 | Loss: 0.00001925
Iteration 112/1000 | Loss: 0.00001925
Iteration 113/1000 | Loss: 0.00001925
Iteration 114/1000 | Loss: 0.00001925
Iteration 115/1000 | Loss: 0.00001925
Iteration 116/1000 | Loss: 0.00001925
Iteration 117/1000 | Loss: 0.00001925
Iteration 118/1000 | Loss: 0.00001925
Iteration 119/1000 | Loss: 0.00001925
Iteration 120/1000 | Loss: 0.00001925
Iteration 121/1000 | Loss: 0.00001925
Iteration 122/1000 | Loss: 0.00001925
Iteration 123/1000 | Loss: 0.00001925
Iteration 124/1000 | Loss: 0.00001925
Iteration 125/1000 | Loss: 0.00001925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.9247612726758234e-05, 1.9247612726758234e-05, 1.9247612726758234e-05, 1.9247612726758234e-05, 1.9247612726758234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9247612726758234e-05

Optimization complete. Final v2v error: 3.499133825302124 mm

Highest mean error: 4.723700046539307 mm for frame 89

Lowest mean error: 2.979964017868042 mm for frame 201

Saving results

Total time: 135.30531406402588
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00628542
Iteration 2/25 | Loss: 0.00106799
Iteration 3/25 | Loss: 0.00092995
Iteration 4/25 | Loss: 0.00087309
Iteration 5/25 | Loss: 0.00086524
Iteration 6/25 | Loss: 0.00086259
Iteration 7/25 | Loss: 0.00086164
Iteration 8/25 | Loss: 0.00086158
Iteration 9/25 | Loss: 0.00086158
Iteration 10/25 | Loss: 0.00086158
Iteration 11/25 | Loss: 0.00086158
Iteration 12/25 | Loss: 0.00086158
Iteration 13/25 | Loss: 0.00086158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008615817059762776, 0.0008615817059762776, 0.0008615817059762776, 0.0008615817059762776, 0.0008615817059762776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008615817059762776

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.73785257
Iteration 2/25 | Loss: 0.00107348
Iteration 3/25 | Loss: 0.00107348
Iteration 4/25 | Loss: 0.00107348
Iteration 5/25 | Loss: 0.00107348
Iteration 6/25 | Loss: 0.00107348
Iteration 7/25 | Loss: 0.00107348
Iteration 8/25 | Loss: 0.00107347
Iteration 9/25 | Loss: 0.00107347
Iteration 10/25 | Loss: 0.00107347
Iteration 11/25 | Loss: 0.00107347
Iteration 12/25 | Loss: 0.00107347
Iteration 13/25 | Loss: 0.00107347
Iteration 14/25 | Loss: 0.00107347
Iteration 15/25 | Loss: 0.00107347
Iteration 16/25 | Loss: 0.00107347
Iteration 17/25 | Loss: 0.00107347
Iteration 18/25 | Loss: 0.00107347
Iteration 19/25 | Loss: 0.00107347
Iteration 20/25 | Loss: 0.00107347
Iteration 21/25 | Loss: 0.00107347
Iteration 22/25 | Loss: 0.00107347
Iteration 23/25 | Loss: 0.00107347
Iteration 24/25 | Loss: 0.00107347
Iteration 25/25 | Loss: 0.00107347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107347
Iteration 2/1000 | Loss: 0.00005796
Iteration 3/1000 | Loss: 0.00003356
Iteration 4/1000 | Loss: 0.00002927
Iteration 5/1000 | Loss: 0.00002793
Iteration 6/1000 | Loss: 0.00002686
Iteration 7/1000 | Loss: 0.00002638
Iteration 8/1000 | Loss: 0.00002608
Iteration 9/1000 | Loss: 0.00002587
Iteration 10/1000 | Loss: 0.00002564
Iteration 11/1000 | Loss: 0.00002554
Iteration 12/1000 | Loss: 0.00002546
Iteration 13/1000 | Loss: 0.00002546
Iteration 14/1000 | Loss: 0.00002542
Iteration 15/1000 | Loss: 0.00002542
Iteration 16/1000 | Loss: 0.00002541
Iteration 17/1000 | Loss: 0.00002541
Iteration 18/1000 | Loss: 0.00002541
Iteration 19/1000 | Loss: 0.00002540
Iteration 20/1000 | Loss: 0.00002540
Iteration 21/1000 | Loss: 0.00002540
Iteration 22/1000 | Loss: 0.00002539
Iteration 23/1000 | Loss: 0.00002539
Iteration 24/1000 | Loss: 0.00002538
Iteration 25/1000 | Loss: 0.00002538
Iteration 26/1000 | Loss: 0.00002537
Iteration 27/1000 | Loss: 0.00002537
Iteration 28/1000 | Loss: 0.00002537
Iteration 29/1000 | Loss: 0.00002537
Iteration 30/1000 | Loss: 0.00002536
Iteration 31/1000 | Loss: 0.00002536
Iteration 32/1000 | Loss: 0.00002536
Iteration 33/1000 | Loss: 0.00002535
Iteration 34/1000 | Loss: 0.00002535
Iteration 35/1000 | Loss: 0.00002535
Iteration 36/1000 | Loss: 0.00002534
Iteration 37/1000 | Loss: 0.00002534
Iteration 38/1000 | Loss: 0.00002534
Iteration 39/1000 | Loss: 0.00002533
Iteration 40/1000 | Loss: 0.00002533
Iteration 41/1000 | Loss: 0.00002533
Iteration 42/1000 | Loss: 0.00002533
Iteration 43/1000 | Loss: 0.00002533
Iteration 44/1000 | Loss: 0.00002533
Iteration 45/1000 | Loss: 0.00002533
Iteration 46/1000 | Loss: 0.00002533
Iteration 47/1000 | Loss: 0.00002533
Iteration 48/1000 | Loss: 0.00002533
Iteration 49/1000 | Loss: 0.00002533
Iteration 50/1000 | Loss: 0.00002533
Iteration 51/1000 | Loss: 0.00002532
Iteration 52/1000 | Loss: 0.00002532
Iteration 53/1000 | Loss: 0.00002532
Iteration 54/1000 | Loss: 0.00002532
Iteration 55/1000 | Loss: 0.00002531
Iteration 56/1000 | Loss: 0.00002531
Iteration 57/1000 | Loss: 0.00002531
Iteration 58/1000 | Loss: 0.00002531
Iteration 59/1000 | Loss: 0.00002530
Iteration 60/1000 | Loss: 0.00002529
Iteration 61/1000 | Loss: 0.00002529
Iteration 62/1000 | Loss: 0.00002529
Iteration 63/1000 | Loss: 0.00002529
Iteration 64/1000 | Loss: 0.00002529
Iteration 65/1000 | Loss: 0.00002529
Iteration 66/1000 | Loss: 0.00002528
Iteration 67/1000 | Loss: 0.00002528
Iteration 68/1000 | Loss: 0.00002528
Iteration 69/1000 | Loss: 0.00002528
Iteration 70/1000 | Loss: 0.00002527
Iteration 71/1000 | Loss: 0.00002527
Iteration 72/1000 | Loss: 0.00002527
Iteration 73/1000 | Loss: 0.00002526
Iteration 74/1000 | Loss: 0.00002526
Iteration 75/1000 | Loss: 0.00002526
Iteration 76/1000 | Loss: 0.00002526
Iteration 77/1000 | Loss: 0.00002526
Iteration 78/1000 | Loss: 0.00002526
Iteration 79/1000 | Loss: 0.00002526
Iteration 80/1000 | Loss: 0.00002526
Iteration 81/1000 | Loss: 0.00002526
Iteration 82/1000 | Loss: 0.00002526
Iteration 83/1000 | Loss: 0.00002525
Iteration 84/1000 | Loss: 0.00002525
Iteration 85/1000 | Loss: 0.00002525
Iteration 86/1000 | Loss: 0.00002525
Iteration 87/1000 | Loss: 0.00002525
Iteration 88/1000 | Loss: 0.00002525
Iteration 89/1000 | Loss: 0.00002525
Iteration 90/1000 | Loss: 0.00002525
Iteration 91/1000 | Loss: 0.00002525
Iteration 92/1000 | Loss: 0.00002524
Iteration 93/1000 | Loss: 0.00002524
Iteration 94/1000 | Loss: 0.00002523
Iteration 95/1000 | Loss: 0.00002523
Iteration 96/1000 | Loss: 0.00002522
Iteration 97/1000 | Loss: 0.00002522
Iteration 98/1000 | Loss: 0.00002522
Iteration 99/1000 | Loss: 0.00002522
Iteration 100/1000 | Loss: 0.00002521
Iteration 101/1000 | Loss: 0.00002521
Iteration 102/1000 | Loss: 0.00002521
Iteration 103/1000 | Loss: 0.00002520
Iteration 104/1000 | Loss: 0.00002520
Iteration 105/1000 | Loss: 0.00002520
Iteration 106/1000 | Loss: 0.00002519
Iteration 107/1000 | Loss: 0.00002519
Iteration 108/1000 | Loss: 0.00002519
Iteration 109/1000 | Loss: 0.00002519
Iteration 110/1000 | Loss: 0.00002519
Iteration 111/1000 | Loss: 0.00002519
Iteration 112/1000 | Loss: 0.00002518
Iteration 113/1000 | Loss: 0.00002518
Iteration 114/1000 | Loss: 0.00002518
Iteration 115/1000 | Loss: 0.00002518
Iteration 116/1000 | Loss: 0.00002518
Iteration 117/1000 | Loss: 0.00002518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.5184903279296122e-05, 2.5184903279296122e-05, 2.5184903279296122e-05, 2.5184903279296122e-05, 2.5184903279296122e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5184903279296122e-05

Optimization complete. Final v2v error: 4.220658302307129 mm

Highest mean error: 4.416487693786621 mm for frame 65

Lowest mean error: 4.112372875213623 mm for frame 116

Saving results

Total time: 36.23164200782776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903150
Iteration 2/25 | Loss: 0.00161285
Iteration 3/25 | Loss: 0.00125780
Iteration 4/25 | Loss: 0.00119016
Iteration 5/25 | Loss: 0.00116901
Iteration 6/25 | Loss: 0.00116283
Iteration 7/25 | Loss: 0.00117010
Iteration 8/25 | Loss: 0.00117105
Iteration 9/25 | Loss: 0.00116101
Iteration 10/25 | Loss: 0.00115329
Iteration 11/25 | Loss: 0.00114537
Iteration 12/25 | Loss: 0.00113459
Iteration 13/25 | Loss: 0.00112795
Iteration 14/25 | Loss: 0.00111992
Iteration 15/25 | Loss: 0.00111590
Iteration 16/25 | Loss: 0.00111311
Iteration 17/25 | Loss: 0.00111216
Iteration 18/25 | Loss: 0.00111154
Iteration 19/25 | Loss: 0.00111132
Iteration 20/25 | Loss: 0.00111122
Iteration 21/25 | Loss: 0.00111114
Iteration 22/25 | Loss: 0.00111104
Iteration 23/25 | Loss: 0.00111098
Iteration 24/25 | Loss: 0.00111097
Iteration 25/25 | Loss: 0.00111097

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43104875
Iteration 2/25 | Loss: 0.00316853
Iteration 3/25 | Loss: 0.00316846
Iteration 4/25 | Loss: 0.00316846
Iteration 5/25 | Loss: 0.00316845
Iteration 6/25 | Loss: 0.00316845
Iteration 7/25 | Loss: 0.00316845
Iteration 8/25 | Loss: 0.00316845
Iteration 9/25 | Loss: 0.00316845
Iteration 10/25 | Loss: 0.00316845
Iteration 11/25 | Loss: 0.00316845
Iteration 12/25 | Loss: 0.00316845
Iteration 13/25 | Loss: 0.00316845
Iteration 14/25 | Loss: 0.00316845
Iteration 15/25 | Loss: 0.00316845
Iteration 16/25 | Loss: 0.00316845
Iteration 17/25 | Loss: 0.00316845
Iteration 18/25 | Loss: 0.00316845
Iteration 19/25 | Loss: 0.00316845
Iteration 20/25 | Loss: 0.00316845
Iteration 21/25 | Loss: 0.00316845
Iteration 22/25 | Loss: 0.00316845
Iteration 23/25 | Loss: 0.00316845
Iteration 24/25 | Loss: 0.00316845
Iteration 25/25 | Loss: 0.00316845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00316845
Iteration 2/1000 | Loss: 0.00049031
Iteration 3/1000 | Loss: 0.00034167
Iteration 4/1000 | Loss: 0.00028143
Iteration 5/1000 | Loss: 0.00025352
Iteration 6/1000 | Loss: 0.00069365
Iteration 7/1000 | Loss: 0.00032558
Iteration 8/1000 | Loss: 0.00142582
Iteration 9/1000 | Loss: 0.00054169
Iteration 10/1000 | Loss: 0.00021458
Iteration 11/1000 | Loss: 0.00019731
Iteration 12/1000 | Loss: 0.00018551
Iteration 13/1000 | Loss: 0.00124436
Iteration 14/1000 | Loss: 0.00017523
Iteration 15/1000 | Loss: 0.00084776
Iteration 16/1000 | Loss: 0.00017140
Iteration 17/1000 | Loss: 0.00015945
Iteration 18/1000 | Loss: 0.00015356
Iteration 19/1000 | Loss: 0.00076504
Iteration 20/1000 | Loss: 0.00015343
Iteration 21/1000 | Loss: 0.00014173
Iteration 22/1000 | Loss: 0.00072953
Iteration 23/1000 | Loss: 0.00014241
Iteration 24/1000 | Loss: 0.00013046
Iteration 25/1000 | Loss: 0.00145004
Iteration 26/1000 | Loss: 0.00012896
Iteration 27/1000 | Loss: 0.00012385
Iteration 28/1000 | Loss: 0.00233515
Iteration 29/1000 | Loss: 0.01498503
Iteration 30/1000 | Loss: 0.01539091
Iteration 31/1000 | Loss: 0.00358611
Iteration 32/1000 | Loss: 0.00022582
Iteration 33/1000 | Loss: 0.00013970
Iteration 34/1000 | Loss: 0.00012342
Iteration 35/1000 | Loss: 0.00011648
Iteration 36/1000 | Loss: 0.00115529
Iteration 37/1000 | Loss: 0.00255571
Iteration 38/1000 | Loss: 0.00191862
Iteration 39/1000 | Loss: 0.00062532
Iteration 40/1000 | Loss: 0.00046666
Iteration 41/1000 | Loss: 0.00026792
Iteration 42/1000 | Loss: 0.00126534
Iteration 43/1000 | Loss: 0.00230185
Iteration 44/1000 | Loss: 0.00116977
Iteration 45/1000 | Loss: 0.00185543
Iteration 46/1000 | Loss: 0.00085042
Iteration 47/1000 | Loss: 0.00061074
Iteration 48/1000 | Loss: 0.00033740
Iteration 49/1000 | Loss: 0.00165404
Iteration 50/1000 | Loss: 0.00017303
Iteration 51/1000 | Loss: 0.00141092
Iteration 52/1000 | Loss: 0.00087007
Iteration 53/1000 | Loss: 0.00111981
Iteration 54/1000 | Loss: 0.00009819
Iteration 55/1000 | Loss: 0.00141150
Iteration 56/1000 | Loss: 0.00315700
Iteration 57/1000 | Loss: 0.00226794
Iteration 58/1000 | Loss: 0.00009451
Iteration 59/1000 | Loss: 0.00140119
Iteration 60/1000 | Loss: 0.00215461
Iteration 61/1000 | Loss: 0.00121909
Iteration 62/1000 | Loss: 0.00220767
Iteration 63/1000 | Loss: 0.00100761
Iteration 64/1000 | Loss: 0.00007347
Iteration 65/1000 | Loss: 0.00245436
Iteration 66/1000 | Loss: 0.00225181
Iteration 67/1000 | Loss: 0.00192330
Iteration 68/1000 | Loss: 0.00184745
Iteration 69/1000 | Loss: 0.00155532
Iteration 70/1000 | Loss: 0.00008732
Iteration 71/1000 | Loss: 0.00006482
Iteration 72/1000 | Loss: 0.00005304
Iteration 73/1000 | Loss: 0.00042655
Iteration 74/1000 | Loss: 0.00049517
Iteration 75/1000 | Loss: 0.00025744
Iteration 76/1000 | Loss: 0.00004986
Iteration 77/1000 | Loss: 0.00004536
Iteration 78/1000 | Loss: 0.00114606
Iteration 79/1000 | Loss: 0.00004488
Iteration 80/1000 | Loss: 0.00003929
Iteration 81/1000 | Loss: 0.00003666
Iteration 82/1000 | Loss: 0.00003381
Iteration 83/1000 | Loss: 0.00003212
Iteration 84/1000 | Loss: 0.00003108
Iteration 85/1000 | Loss: 0.00003041
Iteration 86/1000 | Loss: 0.00002986
Iteration 87/1000 | Loss: 0.00002946
Iteration 88/1000 | Loss: 0.00090438
Iteration 89/1000 | Loss: 0.00003678
Iteration 90/1000 | Loss: 0.00002968
Iteration 91/1000 | Loss: 0.00002777
Iteration 92/1000 | Loss: 0.00002657
Iteration 93/1000 | Loss: 0.00002557
Iteration 94/1000 | Loss: 0.00002483
Iteration 95/1000 | Loss: 0.00002450
Iteration 96/1000 | Loss: 0.00002430
Iteration 97/1000 | Loss: 0.00002407
Iteration 98/1000 | Loss: 0.00002402
Iteration 99/1000 | Loss: 0.00002389
Iteration 100/1000 | Loss: 0.00002387
Iteration 101/1000 | Loss: 0.00002384
Iteration 102/1000 | Loss: 0.00002384
Iteration 103/1000 | Loss: 0.00002384
Iteration 104/1000 | Loss: 0.00002383
Iteration 105/1000 | Loss: 0.00002383
Iteration 106/1000 | Loss: 0.00002382
Iteration 107/1000 | Loss: 0.00002381
Iteration 108/1000 | Loss: 0.00002380
Iteration 109/1000 | Loss: 0.00002380
Iteration 110/1000 | Loss: 0.00002380
Iteration 111/1000 | Loss: 0.00002380
Iteration 112/1000 | Loss: 0.00002380
Iteration 113/1000 | Loss: 0.00002379
Iteration 114/1000 | Loss: 0.00002379
Iteration 115/1000 | Loss: 0.00002379
Iteration 116/1000 | Loss: 0.00002379
Iteration 117/1000 | Loss: 0.00002377
Iteration 118/1000 | Loss: 0.00002377
Iteration 119/1000 | Loss: 0.00002377
Iteration 120/1000 | Loss: 0.00002377
Iteration 121/1000 | Loss: 0.00002377
Iteration 122/1000 | Loss: 0.00002377
Iteration 123/1000 | Loss: 0.00002377
Iteration 124/1000 | Loss: 0.00002376
Iteration 125/1000 | Loss: 0.00002376
Iteration 126/1000 | Loss: 0.00002376
Iteration 127/1000 | Loss: 0.00002376
Iteration 128/1000 | Loss: 0.00002376
Iteration 129/1000 | Loss: 0.00002376
Iteration 130/1000 | Loss: 0.00002375
Iteration 131/1000 | Loss: 0.00002374
Iteration 132/1000 | Loss: 0.00002373
Iteration 133/1000 | Loss: 0.00002373
Iteration 134/1000 | Loss: 0.00002373
Iteration 135/1000 | Loss: 0.00002370
Iteration 136/1000 | Loss: 0.00002370
Iteration 137/1000 | Loss: 0.00002370
Iteration 138/1000 | Loss: 0.00002370
Iteration 139/1000 | Loss: 0.00002370
Iteration 140/1000 | Loss: 0.00002370
Iteration 141/1000 | Loss: 0.00002370
Iteration 142/1000 | Loss: 0.00002370
Iteration 143/1000 | Loss: 0.00002370
Iteration 144/1000 | Loss: 0.00002369
Iteration 145/1000 | Loss: 0.00002369
Iteration 146/1000 | Loss: 0.00002368
Iteration 147/1000 | Loss: 0.00002368
Iteration 148/1000 | Loss: 0.00002368
Iteration 149/1000 | Loss: 0.00002368
Iteration 150/1000 | Loss: 0.00002368
Iteration 151/1000 | Loss: 0.00002368
Iteration 152/1000 | Loss: 0.00002368
Iteration 153/1000 | Loss: 0.00002368
Iteration 154/1000 | Loss: 0.00002367
Iteration 155/1000 | Loss: 0.00002367
Iteration 156/1000 | Loss: 0.00002367
Iteration 157/1000 | Loss: 0.00002367
Iteration 158/1000 | Loss: 0.00002367
Iteration 159/1000 | Loss: 0.00002367
Iteration 160/1000 | Loss: 0.00002367
Iteration 161/1000 | Loss: 0.00002367
Iteration 162/1000 | Loss: 0.00002366
Iteration 163/1000 | Loss: 0.00002366
Iteration 164/1000 | Loss: 0.00002366
Iteration 165/1000 | Loss: 0.00002366
Iteration 166/1000 | Loss: 0.00002366
Iteration 167/1000 | Loss: 0.00002366
Iteration 168/1000 | Loss: 0.00002366
Iteration 169/1000 | Loss: 0.00002365
Iteration 170/1000 | Loss: 0.00002365
Iteration 171/1000 | Loss: 0.00002365
Iteration 172/1000 | Loss: 0.00002365
Iteration 173/1000 | Loss: 0.00002364
Iteration 174/1000 | Loss: 0.00002364
Iteration 175/1000 | Loss: 0.00002364
Iteration 176/1000 | Loss: 0.00002363
Iteration 177/1000 | Loss: 0.00002363
Iteration 178/1000 | Loss: 0.00002363
Iteration 179/1000 | Loss: 0.00002363
Iteration 180/1000 | Loss: 0.00002363
Iteration 181/1000 | Loss: 0.00002362
Iteration 182/1000 | Loss: 0.00002362
Iteration 183/1000 | Loss: 0.00002362
Iteration 184/1000 | Loss: 0.00002362
Iteration 185/1000 | Loss: 0.00002362
Iteration 186/1000 | Loss: 0.00002362
Iteration 187/1000 | Loss: 0.00002362
Iteration 188/1000 | Loss: 0.00002362
Iteration 189/1000 | Loss: 0.00002362
Iteration 190/1000 | Loss: 0.00002362
Iteration 191/1000 | Loss: 0.00002362
Iteration 192/1000 | Loss: 0.00002362
Iteration 193/1000 | Loss: 0.00002362
Iteration 194/1000 | Loss: 0.00002362
Iteration 195/1000 | Loss: 0.00002362
Iteration 196/1000 | Loss: 0.00002362
Iteration 197/1000 | Loss: 0.00002361
Iteration 198/1000 | Loss: 0.00002361
Iteration 199/1000 | Loss: 0.00002361
Iteration 200/1000 | Loss: 0.00002360
Iteration 201/1000 | Loss: 0.00002360
Iteration 202/1000 | Loss: 0.00002360
Iteration 203/1000 | Loss: 0.00002360
Iteration 204/1000 | Loss: 0.00002360
Iteration 205/1000 | Loss: 0.00002359
Iteration 206/1000 | Loss: 0.00002359
Iteration 207/1000 | Loss: 0.00002359
Iteration 208/1000 | Loss: 0.00002359
Iteration 209/1000 | Loss: 0.00002359
Iteration 210/1000 | Loss: 0.00002359
Iteration 211/1000 | Loss: 0.00002359
Iteration 212/1000 | Loss: 0.00002359
Iteration 213/1000 | Loss: 0.00002359
Iteration 214/1000 | Loss: 0.00002358
Iteration 215/1000 | Loss: 0.00002358
Iteration 216/1000 | Loss: 0.00002358
Iteration 217/1000 | Loss: 0.00002358
Iteration 218/1000 | Loss: 0.00002358
Iteration 219/1000 | Loss: 0.00002358
Iteration 220/1000 | Loss: 0.00002358
Iteration 221/1000 | Loss: 0.00002358
Iteration 222/1000 | Loss: 0.00002358
Iteration 223/1000 | Loss: 0.00002358
Iteration 224/1000 | Loss: 0.00002358
Iteration 225/1000 | Loss: 0.00002358
Iteration 226/1000 | Loss: 0.00002358
Iteration 227/1000 | Loss: 0.00002358
Iteration 228/1000 | Loss: 0.00002358
Iteration 229/1000 | Loss: 0.00002358
Iteration 230/1000 | Loss: 0.00002358
Iteration 231/1000 | Loss: 0.00002358
Iteration 232/1000 | Loss: 0.00002357
Iteration 233/1000 | Loss: 0.00002357
Iteration 234/1000 | Loss: 0.00002357
Iteration 235/1000 | Loss: 0.00002357
Iteration 236/1000 | Loss: 0.00002357
Iteration 237/1000 | Loss: 0.00002357
Iteration 238/1000 | Loss: 0.00002357
Iteration 239/1000 | Loss: 0.00002357
Iteration 240/1000 | Loss: 0.00002357
Iteration 241/1000 | Loss: 0.00002357
Iteration 242/1000 | Loss: 0.00002357
Iteration 243/1000 | Loss: 0.00002357
Iteration 244/1000 | Loss: 0.00002357
Iteration 245/1000 | Loss: 0.00002357
Iteration 246/1000 | Loss: 0.00002357
Iteration 247/1000 | Loss: 0.00002357
Iteration 248/1000 | Loss: 0.00002357
Iteration 249/1000 | Loss: 0.00002357
Iteration 250/1000 | Loss: 0.00002357
Iteration 251/1000 | Loss: 0.00002357
Iteration 252/1000 | Loss: 0.00002357
Iteration 253/1000 | Loss: 0.00002357
Iteration 254/1000 | Loss: 0.00002357
Iteration 255/1000 | Loss: 0.00002357
Iteration 256/1000 | Loss: 0.00002357
Iteration 257/1000 | Loss: 0.00002357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [2.357337507419288e-05, 2.357337507419288e-05, 2.357337507419288e-05, 2.357337507419288e-05, 2.357337507419288e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.357337507419288e-05

Optimization complete. Final v2v error: 3.984508514404297 mm

Highest mean error: 4.584108829498291 mm for frame 111

Lowest mean error: 3.722503662109375 mm for frame 13

Saving results

Total time: 250.26561641693115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441317
Iteration 2/25 | Loss: 0.00098297
Iteration 3/25 | Loss: 0.00080151
Iteration 4/25 | Loss: 0.00077522
Iteration 5/25 | Loss: 0.00076782
Iteration 6/25 | Loss: 0.00076570
Iteration 7/25 | Loss: 0.00076534
Iteration 8/25 | Loss: 0.00076534
Iteration 9/25 | Loss: 0.00076534
Iteration 10/25 | Loss: 0.00076534
Iteration 11/25 | Loss: 0.00076534
Iteration 12/25 | Loss: 0.00076534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007653443026356399, 0.0007653443026356399, 0.0007653443026356399, 0.0007653443026356399, 0.0007653443026356399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007653443026356399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56397164
Iteration 2/25 | Loss: 0.00100514
Iteration 3/25 | Loss: 0.00100513
Iteration 4/25 | Loss: 0.00100513
Iteration 5/25 | Loss: 0.00100513
Iteration 6/25 | Loss: 0.00100513
Iteration 7/25 | Loss: 0.00100513
Iteration 8/25 | Loss: 0.00100513
Iteration 9/25 | Loss: 0.00100513
Iteration 10/25 | Loss: 0.00100513
Iteration 11/25 | Loss: 0.00100513
Iteration 12/25 | Loss: 0.00100513
Iteration 13/25 | Loss: 0.00100513
Iteration 14/25 | Loss: 0.00100513
Iteration 15/25 | Loss: 0.00100513
Iteration 16/25 | Loss: 0.00100513
Iteration 17/25 | Loss: 0.00100513
Iteration 18/25 | Loss: 0.00100513
Iteration 19/25 | Loss: 0.00100513
Iteration 20/25 | Loss: 0.00100513
Iteration 21/25 | Loss: 0.00100513
Iteration 22/25 | Loss: 0.00100513
Iteration 23/25 | Loss: 0.00100513
Iteration 24/25 | Loss: 0.00100513
Iteration 25/25 | Loss: 0.00100513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100513
Iteration 2/1000 | Loss: 0.00003858
Iteration 3/1000 | Loss: 0.00002332
Iteration 4/1000 | Loss: 0.00001974
Iteration 5/1000 | Loss: 0.00001833
Iteration 6/1000 | Loss: 0.00001738
Iteration 7/1000 | Loss: 0.00001690
Iteration 8/1000 | Loss: 0.00001649
Iteration 9/1000 | Loss: 0.00001617
Iteration 10/1000 | Loss: 0.00001594
Iteration 11/1000 | Loss: 0.00001576
Iteration 12/1000 | Loss: 0.00001573
Iteration 13/1000 | Loss: 0.00001557
Iteration 14/1000 | Loss: 0.00001555
Iteration 15/1000 | Loss: 0.00001553
Iteration 16/1000 | Loss: 0.00001550
Iteration 17/1000 | Loss: 0.00001543
Iteration 18/1000 | Loss: 0.00001538
Iteration 19/1000 | Loss: 0.00001537
Iteration 20/1000 | Loss: 0.00001534
Iteration 21/1000 | Loss: 0.00001529
Iteration 22/1000 | Loss: 0.00001529
Iteration 23/1000 | Loss: 0.00001528
Iteration 24/1000 | Loss: 0.00001527
Iteration 25/1000 | Loss: 0.00001527
Iteration 26/1000 | Loss: 0.00001526
Iteration 27/1000 | Loss: 0.00001525
Iteration 28/1000 | Loss: 0.00001525
Iteration 29/1000 | Loss: 0.00001525
Iteration 30/1000 | Loss: 0.00001524
Iteration 31/1000 | Loss: 0.00001523
Iteration 32/1000 | Loss: 0.00001523
Iteration 33/1000 | Loss: 0.00001523
Iteration 34/1000 | Loss: 0.00001522
Iteration 35/1000 | Loss: 0.00001521
Iteration 36/1000 | Loss: 0.00001521
Iteration 37/1000 | Loss: 0.00001521
Iteration 38/1000 | Loss: 0.00001520
Iteration 39/1000 | Loss: 0.00001519
Iteration 40/1000 | Loss: 0.00001519
Iteration 41/1000 | Loss: 0.00001519
Iteration 42/1000 | Loss: 0.00001518
Iteration 43/1000 | Loss: 0.00001518
Iteration 44/1000 | Loss: 0.00001518
Iteration 45/1000 | Loss: 0.00001517
Iteration 46/1000 | Loss: 0.00001517
Iteration 47/1000 | Loss: 0.00001517
Iteration 48/1000 | Loss: 0.00001516
Iteration 49/1000 | Loss: 0.00001516
Iteration 50/1000 | Loss: 0.00001516
Iteration 51/1000 | Loss: 0.00001516
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001515
Iteration 54/1000 | Loss: 0.00001515
Iteration 55/1000 | Loss: 0.00001515
Iteration 56/1000 | Loss: 0.00001514
Iteration 57/1000 | Loss: 0.00001514
Iteration 58/1000 | Loss: 0.00001514
Iteration 59/1000 | Loss: 0.00001513
Iteration 60/1000 | Loss: 0.00001513
Iteration 61/1000 | Loss: 0.00001513
Iteration 62/1000 | Loss: 0.00001513
Iteration 63/1000 | Loss: 0.00001512
Iteration 64/1000 | Loss: 0.00001512
Iteration 65/1000 | Loss: 0.00001512
Iteration 66/1000 | Loss: 0.00001512
Iteration 67/1000 | Loss: 0.00001511
Iteration 68/1000 | Loss: 0.00001511
Iteration 69/1000 | Loss: 0.00001511
Iteration 70/1000 | Loss: 0.00001511
Iteration 71/1000 | Loss: 0.00001511
Iteration 72/1000 | Loss: 0.00001511
Iteration 73/1000 | Loss: 0.00001510
Iteration 74/1000 | Loss: 0.00001510
Iteration 75/1000 | Loss: 0.00001510
Iteration 76/1000 | Loss: 0.00001510
Iteration 77/1000 | Loss: 0.00001509
Iteration 78/1000 | Loss: 0.00001509
Iteration 79/1000 | Loss: 0.00001509
Iteration 80/1000 | Loss: 0.00001509
Iteration 81/1000 | Loss: 0.00001509
Iteration 82/1000 | Loss: 0.00001509
Iteration 83/1000 | Loss: 0.00001509
Iteration 84/1000 | Loss: 0.00001509
Iteration 85/1000 | Loss: 0.00001508
Iteration 86/1000 | Loss: 0.00001508
Iteration 87/1000 | Loss: 0.00001508
Iteration 88/1000 | Loss: 0.00001508
Iteration 89/1000 | Loss: 0.00001508
Iteration 90/1000 | Loss: 0.00001508
Iteration 91/1000 | Loss: 0.00001507
Iteration 92/1000 | Loss: 0.00001507
Iteration 93/1000 | Loss: 0.00001507
Iteration 94/1000 | Loss: 0.00001507
Iteration 95/1000 | Loss: 0.00001506
Iteration 96/1000 | Loss: 0.00001506
Iteration 97/1000 | Loss: 0.00001506
Iteration 98/1000 | Loss: 0.00001506
Iteration 99/1000 | Loss: 0.00001505
Iteration 100/1000 | Loss: 0.00001505
Iteration 101/1000 | Loss: 0.00001505
Iteration 102/1000 | Loss: 0.00001505
Iteration 103/1000 | Loss: 0.00001505
Iteration 104/1000 | Loss: 0.00001504
Iteration 105/1000 | Loss: 0.00001504
Iteration 106/1000 | Loss: 0.00001504
Iteration 107/1000 | Loss: 0.00001504
Iteration 108/1000 | Loss: 0.00001504
Iteration 109/1000 | Loss: 0.00001504
Iteration 110/1000 | Loss: 0.00001504
Iteration 111/1000 | Loss: 0.00001504
Iteration 112/1000 | Loss: 0.00001504
Iteration 113/1000 | Loss: 0.00001504
Iteration 114/1000 | Loss: 0.00001504
Iteration 115/1000 | Loss: 0.00001504
Iteration 116/1000 | Loss: 0.00001504
Iteration 117/1000 | Loss: 0.00001503
Iteration 118/1000 | Loss: 0.00001503
Iteration 119/1000 | Loss: 0.00001503
Iteration 120/1000 | Loss: 0.00001503
Iteration 121/1000 | Loss: 0.00001503
Iteration 122/1000 | Loss: 0.00001503
Iteration 123/1000 | Loss: 0.00001503
Iteration 124/1000 | Loss: 0.00001503
Iteration 125/1000 | Loss: 0.00001503
Iteration 126/1000 | Loss: 0.00001503
Iteration 127/1000 | Loss: 0.00001502
Iteration 128/1000 | Loss: 0.00001502
Iteration 129/1000 | Loss: 0.00001502
Iteration 130/1000 | Loss: 0.00001502
Iteration 131/1000 | Loss: 0.00001502
Iteration 132/1000 | Loss: 0.00001502
Iteration 133/1000 | Loss: 0.00001502
Iteration 134/1000 | Loss: 0.00001502
Iteration 135/1000 | Loss: 0.00001502
Iteration 136/1000 | Loss: 0.00001502
Iteration 137/1000 | Loss: 0.00001502
Iteration 138/1000 | Loss: 0.00001502
Iteration 139/1000 | Loss: 0.00001502
Iteration 140/1000 | Loss: 0.00001502
Iteration 141/1000 | Loss: 0.00001502
Iteration 142/1000 | Loss: 0.00001501
Iteration 143/1000 | Loss: 0.00001501
Iteration 144/1000 | Loss: 0.00001501
Iteration 145/1000 | Loss: 0.00001501
Iteration 146/1000 | Loss: 0.00001501
Iteration 147/1000 | Loss: 0.00001501
Iteration 148/1000 | Loss: 0.00001501
Iteration 149/1000 | Loss: 0.00001501
Iteration 150/1000 | Loss: 0.00001500
Iteration 151/1000 | Loss: 0.00001500
Iteration 152/1000 | Loss: 0.00001500
Iteration 153/1000 | Loss: 0.00001500
Iteration 154/1000 | Loss: 0.00001500
Iteration 155/1000 | Loss: 0.00001500
Iteration 156/1000 | Loss: 0.00001500
Iteration 157/1000 | Loss: 0.00001500
Iteration 158/1000 | Loss: 0.00001500
Iteration 159/1000 | Loss: 0.00001500
Iteration 160/1000 | Loss: 0.00001500
Iteration 161/1000 | Loss: 0.00001500
Iteration 162/1000 | Loss: 0.00001500
Iteration 163/1000 | Loss: 0.00001500
Iteration 164/1000 | Loss: 0.00001500
Iteration 165/1000 | Loss: 0.00001500
Iteration 166/1000 | Loss: 0.00001500
Iteration 167/1000 | Loss: 0.00001500
Iteration 168/1000 | Loss: 0.00001500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.5003241060185246e-05, 1.5003241060185246e-05, 1.5003241060185246e-05, 1.5003241060185246e-05, 1.5003241060185246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5003241060185246e-05

Optimization complete. Final v2v error: 3.2458457946777344 mm

Highest mean error: 3.806403160095215 mm for frame 144

Lowest mean error: 2.6132147312164307 mm for frame 161

Saving results

Total time: 65.3532543182373
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00573138
Iteration 2/25 | Loss: 0.00180065
Iteration 3/25 | Loss: 0.00114808
Iteration 4/25 | Loss: 0.00105581
Iteration 5/25 | Loss: 0.00103247
Iteration 6/25 | Loss: 0.00102678
Iteration 7/25 | Loss: 0.00102500
Iteration 8/25 | Loss: 0.00102425
Iteration 9/25 | Loss: 0.00102411
Iteration 10/25 | Loss: 0.00102411
Iteration 11/25 | Loss: 0.00102411
Iteration 12/25 | Loss: 0.00102411
Iteration 13/25 | Loss: 0.00102411
Iteration 14/25 | Loss: 0.00102411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010241142008453608, 0.0010241142008453608, 0.0010241142008453608, 0.0010241142008453608, 0.0010241142008453608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010241142008453608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13103640
Iteration 2/25 | Loss: 0.00131903
Iteration 3/25 | Loss: 0.00131901
Iteration 4/25 | Loss: 0.00131901
Iteration 5/25 | Loss: 0.00131901
Iteration 6/25 | Loss: 0.00131901
Iteration 7/25 | Loss: 0.00131901
Iteration 8/25 | Loss: 0.00131901
Iteration 9/25 | Loss: 0.00131901
Iteration 10/25 | Loss: 0.00131901
Iteration 11/25 | Loss: 0.00131901
Iteration 12/25 | Loss: 0.00131901
Iteration 13/25 | Loss: 0.00131901
Iteration 14/25 | Loss: 0.00131901
Iteration 15/25 | Loss: 0.00131901
Iteration 16/25 | Loss: 0.00131901
Iteration 17/25 | Loss: 0.00131901
Iteration 18/25 | Loss: 0.00131901
Iteration 19/25 | Loss: 0.00131901
Iteration 20/25 | Loss: 0.00131901
Iteration 21/25 | Loss: 0.00131901
Iteration 22/25 | Loss: 0.00131901
Iteration 23/25 | Loss: 0.00131901
Iteration 24/25 | Loss: 0.00131901
Iteration 25/25 | Loss: 0.00131901

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131901
Iteration 2/1000 | Loss: 0.00009965
Iteration 3/1000 | Loss: 0.00007111
Iteration 4/1000 | Loss: 0.00006201
Iteration 5/1000 | Loss: 0.00005822
Iteration 6/1000 | Loss: 0.00005622
Iteration 7/1000 | Loss: 0.00005477
Iteration 8/1000 | Loss: 0.00005350
Iteration 9/1000 | Loss: 0.00053993
Iteration 10/1000 | Loss: 0.00055417
Iteration 11/1000 | Loss: 0.00014119
Iteration 12/1000 | Loss: 0.00008217
Iteration 13/1000 | Loss: 0.00006490
Iteration 14/1000 | Loss: 0.00005849
Iteration 15/1000 | Loss: 0.00005458
Iteration 16/1000 | Loss: 0.00005307
Iteration 17/1000 | Loss: 0.00005159
Iteration 18/1000 | Loss: 0.00032924
Iteration 19/1000 | Loss: 0.00008495
Iteration 20/1000 | Loss: 0.00005819
Iteration 21/1000 | Loss: 0.00005451
Iteration 22/1000 | Loss: 0.00029453
Iteration 23/1000 | Loss: 0.00006268
Iteration 24/1000 | Loss: 0.00005414
Iteration 25/1000 | Loss: 0.00005160
Iteration 26/1000 | Loss: 0.00005008
Iteration 27/1000 | Loss: 0.00028868
Iteration 28/1000 | Loss: 0.00006160
Iteration 29/1000 | Loss: 0.00005254
Iteration 30/1000 | Loss: 0.00053914
Iteration 31/1000 | Loss: 0.00008026
Iteration 32/1000 | Loss: 0.00030329
Iteration 33/1000 | Loss: 0.00007069
Iteration 34/1000 | Loss: 0.00005856
Iteration 35/1000 | Loss: 0.00031421
Iteration 36/1000 | Loss: 0.00031150
Iteration 37/1000 | Loss: 0.00026510
Iteration 38/1000 | Loss: 0.00006413
Iteration 39/1000 | Loss: 0.00005357
Iteration 40/1000 | Loss: 0.00004933
Iteration 41/1000 | Loss: 0.00004607
Iteration 42/1000 | Loss: 0.00005028
Iteration 43/1000 | Loss: 0.00004270
Iteration 44/1000 | Loss: 0.00052111
Iteration 45/1000 | Loss: 0.00012139
Iteration 46/1000 | Loss: 0.00005271
Iteration 47/1000 | Loss: 0.00004687
Iteration 48/1000 | Loss: 0.00029244
Iteration 49/1000 | Loss: 0.00006500
Iteration 50/1000 | Loss: 0.00004744
Iteration 51/1000 | Loss: 0.00004462
Iteration 52/1000 | Loss: 0.00028668
Iteration 53/1000 | Loss: 0.00009364
Iteration 54/1000 | Loss: 0.00026110
Iteration 55/1000 | Loss: 0.00031082
Iteration 56/1000 | Loss: 0.00009762
Iteration 57/1000 | Loss: 0.00005513
Iteration 58/1000 | Loss: 0.00005144
Iteration 59/1000 | Loss: 0.00004612
Iteration 60/1000 | Loss: 0.00004297
Iteration 61/1000 | Loss: 0.00004112
Iteration 62/1000 | Loss: 0.00003955
Iteration 63/1000 | Loss: 0.00004514
Iteration 64/1000 | Loss: 0.00003801
Iteration 65/1000 | Loss: 0.00003728
Iteration 66/1000 | Loss: 0.00003632
Iteration 67/1000 | Loss: 0.00003599
Iteration 68/1000 | Loss: 0.00003569
Iteration 69/1000 | Loss: 0.00003545
Iteration 70/1000 | Loss: 0.00003522
Iteration 71/1000 | Loss: 0.00003520
Iteration 72/1000 | Loss: 0.00003507
Iteration 73/1000 | Loss: 0.00003506
Iteration 74/1000 | Loss: 0.00003501
Iteration 75/1000 | Loss: 0.00003498
Iteration 76/1000 | Loss: 0.00003494
Iteration 77/1000 | Loss: 0.00003486
Iteration 78/1000 | Loss: 0.00003485
Iteration 79/1000 | Loss: 0.00003484
Iteration 80/1000 | Loss: 0.00003484
Iteration 81/1000 | Loss: 0.00003477
Iteration 82/1000 | Loss: 0.00003477
Iteration 83/1000 | Loss: 0.00003477
Iteration 84/1000 | Loss: 0.00003476
Iteration 85/1000 | Loss: 0.00003476
Iteration 86/1000 | Loss: 0.00003476
Iteration 87/1000 | Loss: 0.00003476
Iteration 88/1000 | Loss: 0.00003475
Iteration 89/1000 | Loss: 0.00003475
Iteration 90/1000 | Loss: 0.00003475
Iteration 91/1000 | Loss: 0.00003475
Iteration 92/1000 | Loss: 0.00003474
Iteration 93/1000 | Loss: 0.00003474
Iteration 94/1000 | Loss: 0.00003474
Iteration 95/1000 | Loss: 0.00003474
Iteration 96/1000 | Loss: 0.00003474
Iteration 97/1000 | Loss: 0.00003474
Iteration 98/1000 | Loss: 0.00003474
Iteration 99/1000 | Loss: 0.00003474
Iteration 100/1000 | Loss: 0.00003473
Iteration 101/1000 | Loss: 0.00003473
Iteration 102/1000 | Loss: 0.00003473
Iteration 103/1000 | Loss: 0.00003473
Iteration 104/1000 | Loss: 0.00003473
Iteration 105/1000 | Loss: 0.00003473
Iteration 106/1000 | Loss: 0.00003472
Iteration 107/1000 | Loss: 0.00003472
Iteration 108/1000 | Loss: 0.00003472
Iteration 109/1000 | Loss: 0.00003472
Iteration 110/1000 | Loss: 0.00003472
Iteration 111/1000 | Loss: 0.00003472
Iteration 112/1000 | Loss: 0.00003471
Iteration 113/1000 | Loss: 0.00003471
Iteration 114/1000 | Loss: 0.00003471
Iteration 115/1000 | Loss: 0.00003471
Iteration 116/1000 | Loss: 0.00003471
Iteration 117/1000 | Loss: 0.00003471
Iteration 118/1000 | Loss: 0.00003471
Iteration 119/1000 | Loss: 0.00003471
Iteration 120/1000 | Loss: 0.00003471
Iteration 121/1000 | Loss: 0.00003471
Iteration 122/1000 | Loss: 0.00003471
Iteration 123/1000 | Loss: 0.00003471
Iteration 124/1000 | Loss: 0.00003471
Iteration 125/1000 | Loss: 0.00003471
Iteration 126/1000 | Loss: 0.00003471
Iteration 127/1000 | Loss: 0.00003471
Iteration 128/1000 | Loss: 0.00003471
Iteration 129/1000 | Loss: 0.00003471
Iteration 130/1000 | Loss: 0.00003471
Iteration 131/1000 | Loss: 0.00003471
Iteration 132/1000 | Loss: 0.00003471
Iteration 133/1000 | Loss: 0.00003471
Iteration 134/1000 | Loss: 0.00003471
Iteration 135/1000 | Loss: 0.00003471
Iteration 136/1000 | Loss: 0.00003471
Iteration 137/1000 | Loss: 0.00003471
Iteration 138/1000 | Loss: 0.00003471
Iteration 139/1000 | Loss: 0.00003471
Iteration 140/1000 | Loss: 0.00003471
Iteration 141/1000 | Loss: 0.00003471
Iteration 142/1000 | Loss: 0.00003471
Iteration 143/1000 | Loss: 0.00003471
Iteration 144/1000 | Loss: 0.00003471
Iteration 145/1000 | Loss: 0.00003471
Iteration 146/1000 | Loss: 0.00003471
Iteration 147/1000 | Loss: 0.00003471
Iteration 148/1000 | Loss: 0.00003471
Iteration 149/1000 | Loss: 0.00003471
Iteration 150/1000 | Loss: 0.00003471
Iteration 151/1000 | Loss: 0.00003471
Iteration 152/1000 | Loss: 0.00003471
Iteration 153/1000 | Loss: 0.00003471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [3.470537922112271e-05, 3.470537922112271e-05, 3.470537922112271e-05, 3.470537922112271e-05, 3.470537922112271e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.470537922112271e-05

Optimization complete. Final v2v error: 4.576124668121338 mm

Highest mean error: 5.797427177429199 mm for frame 22

Lowest mean error: 3.4570019245147705 mm for frame 104

Saving results

Total time: 145.3915708065033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006819
Iteration 2/25 | Loss: 0.00126151
Iteration 3/25 | Loss: 0.00114504
Iteration 4/25 | Loss: 0.00077899
Iteration 5/25 | Loss: 0.00073991
Iteration 6/25 | Loss: 0.00073934
Iteration 7/25 | Loss: 0.00073531
Iteration 8/25 | Loss: 0.00073232
Iteration 9/25 | Loss: 0.00073055
Iteration 10/25 | Loss: 0.00072948
Iteration 11/25 | Loss: 0.00073180
Iteration 12/25 | Loss: 0.00072851
Iteration 13/25 | Loss: 0.00072664
Iteration 14/25 | Loss: 0.00072603
Iteration 15/25 | Loss: 0.00072571
Iteration 16/25 | Loss: 0.00072564
Iteration 17/25 | Loss: 0.00072564
Iteration 18/25 | Loss: 0.00072564
Iteration 19/25 | Loss: 0.00072564
Iteration 20/25 | Loss: 0.00072564
Iteration 21/25 | Loss: 0.00072564
Iteration 22/25 | Loss: 0.00072563
Iteration 23/25 | Loss: 0.00072563
Iteration 24/25 | Loss: 0.00072563
Iteration 25/25 | Loss: 0.00072563

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54355180
Iteration 2/25 | Loss: 0.00081980
Iteration 3/25 | Loss: 0.00081979
Iteration 4/25 | Loss: 0.00081979
Iteration 5/25 | Loss: 0.00081979
Iteration 6/25 | Loss: 0.00081979
Iteration 7/25 | Loss: 0.00081979
Iteration 8/25 | Loss: 0.00081979
Iteration 9/25 | Loss: 0.00081979
Iteration 10/25 | Loss: 0.00081979
Iteration 11/25 | Loss: 0.00081979
Iteration 12/25 | Loss: 0.00081979
Iteration 13/25 | Loss: 0.00081979
Iteration 14/25 | Loss: 0.00081979
Iteration 15/25 | Loss: 0.00081979
Iteration 16/25 | Loss: 0.00081979
Iteration 17/25 | Loss: 0.00081979
Iteration 18/25 | Loss: 0.00081979
Iteration 19/25 | Loss: 0.00081979
Iteration 20/25 | Loss: 0.00081979
Iteration 21/25 | Loss: 0.00081979
Iteration 22/25 | Loss: 0.00081979
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008197927381843328, 0.0008197927381843328, 0.0008197927381843328, 0.0008197927381843328, 0.0008197927381843328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008197927381843328

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081979
Iteration 2/1000 | Loss: 0.00003160
Iteration 3/1000 | Loss: 0.00002205
Iteration 4/1000 | Loss: 0.00001901
Iteration 5/1000 | Loss: 0.00001802
Iteration 6/1000 | Loss: 0.00001721
Iteration 7/1000 | Loss: 0.00001676
Iteration 8/1000 | Loss: 0.00001633
Iteration 9/1000 | Loss: 0.00001609
Iteration 10/1000 | Loss: 0.00001582
Iteration 11/1000 | Loss: 0.00001575
Iteration 12/1000 | Loss: 0.00001574
Iteration 13/1000 | Loss: 0.00001571
Iteration 14/1000 | Loss: 0.00001565
Iteration 15/1000 | Loss: 0.00001556
Iteration 16/1000 | Loss: 0.00001555
Iteration 17/1000 | Loss: 0.00001555
Iteration 18/1000 | Loss: 0.00001554
Iteration 19/1000 | Loss: 0.00001542
Iteration 20/1000 | Loss: 0.00001540
Iteration 21/1000 | Loss: 0.00001539
Iteration 22/1000 | Loss: 0.00001538
Iteration 23/1000 | Loss: 0.00001538
Iteration 24/1000 | Loss: 0.00001537
Iteration 25/1000 | Loss: 0.00001537
Iteration 26/1000 | Loss: 0.00001537
Iteration 27/1000 | Loss: 0.00001537
Iteration 28/1000 | Loss: 0.00001537
Iteration 29/1000 | Loss: 0.00001537
Iteration 30/1000 | Loss: 0.00001537
Iteration 31/1000 | Loss: 0.00001537
Iteration 32/1000 | Loss: 0.00001537
Iteration 33/1000 | Loss: 0.00001537
Iteration 34/1000 | Loss: 0.00001537
Iteration 35/1000 | Loss: 0.00001536
Iteration 36/1000 | Loss: 0.00001536
Iteration 37/1000 | Loss: 0.00001536
Iteration 38/1000 | Loss: 0.00001536
Iteration 39/1000 | Loss: 0.00001536
Iteration 40/1000 | Loss: 0.00001536
Iteration 41/1000 | Loss: 0.00001535
Iteration 42/1000 | Loss: 0.00001534
Iteration 43/1000 | Loss: 0.00001534
Iteration 44/1000 | Loss: 0.00001534
Iteration 45/1000 | Loss: 0.00001533
Iteration 46/1000 | Loss: 0.00001533
Iteration 47/1000 | Loss: 0.00001532
Iteration 48/1000 | Loss: 0.00001532
Iteration 49/1000 | Loss: 0.00001532
Iteration 50/1000 | Loss: 0.00001531
Iteration 51/1000 | Loss: 0.00001531
Iteration 52/1000 | Loss: 0.00001531
Iteration 53/1000 | Loss: 0.00001530
Iteration 54/1000 | Loss: 0.00001530
Iteration 55/1000 | Loss: 0.00001530
Iteration 56/1000 | Loss: 0.00001529
Iteration 57/1000 | Loss: 0.00001529
Iteration 58/1000 | Loss: 0.00001528
Iteration 59/1000 | Loss: 0.00001528
Iteration 60/1000 | Loss: 0.00001527
Iteration 61/1000 | Loss: 0.00001527
Iteration 62/1000 | Loss: 0.00001526
Iteration 63/1000 | Loss: 0.00001524
Iteration 64/1000 | Loss: 0.00001523
Iteration 65/1000 | Loss: 0.00001522
Iteration 66/1000 | Loss: 0.00001522
Iteration 67/1000 | Loss: 0.00001522
Iteration 68/1000 | Loss: 0.00001521
Iteration 69/1000 | Loss: 0.00001521
Iteration 70/1000 | Loss: 0.00001521
Iteration 71/1000 | Loss: 0.00001520
Iteration 72/1000 | Loss: 0.00001520
Iteration 73/1000 | Loss: 0.00001519
Iteration 74/1000 | Loss: 0.00001519
Iteration 75/1000 | Loss: 0.00001517
Iteration 76/1000 | Loss: 0.00001516
Iteration 77/1000 | Loss: 0.00001516
Iteration 78/1000 | Loss: 0.00001515
Iteration 79/1000 | Loss: 0.00001515
Iteration 80/1000 | Loss: 0.00001514
Iteration 81/1000 | Loss: 0.00001514
Iteration 82/1000 | Loss: 0.00001513
Iteration 83/1000 | Loss: 0.00001513
Iteration 84/1000 | Loss: 0.00001513
Iteration 85/1000 | Loss: 0.00001513
Iteration 86/1000 | Loss: 0.00001512
Iteration 87/1000 | Loss: 0.00001512
Iteration 88/1000 | Loss: 0.00001512
Iteration 89/1000 | Loss: 0.00001512
Iteration 90/1000 | Loss: 0.00001512
Iteration 91/1000 | Loss: 0.00001512
Iteration 92/1000 | Loss: 0.00001512
Iteration 93/1000 | Loss: 0.00001512
Iteration 94/1000 | Loss: 0.00001512
Iteration 95/1000 | Loss: 0.00001512
Iteration 96/1000 | Loss: 0.00001512
Iteration 97/1000 | Loss: 0.00001512
Iteration 98/1000 | Loss: 0.00001512
Iteration 99/1000 | Loss: 0.00001512
Iteration 100/1000 | Loss: 0.00001512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.5116793292691e-05, 1.5116793292691e-05, 1.5116793292691e-05, 1.5116793292691e-05, 1.5116793292691e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5116793292691e-05

Optimization complete. Final v2v error: 3.2829809188842773 mm

Highest mean error: 3.6687753200531006 mm for frame 107

Lowest mean error: 3.0398106575012207 mm for frame 157

Saving results

Total time: 79.7423505783081
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00581008
Iteration 2/25 | Loss: 0.00132278
Iteration 3/25 | Loss: 0.00105009
Iteration 4/25 | Loss: 0.00096597
Iteration 5/25 | Loss: 0.00094913
Iteration 6/25 | Loss: 0.00094562
Iteration 7/25 | Loss: 0.00094477
Iteration 8/25 | Loss: 0.00094449
Iteration 9/25 | Loss: 0.00094442
Iteration 10/25 | Loss: 0.00094442
Iteration 11/25 | Loss: 0.00094442
Iteration 12/25 | Loss: 0.00094442
Iteration 13/25 | Loss: 0.00094442
Iteration 14/25 | Loss: 0.00094442
Iteration 15/25 | Loss: 0.00094442
Iteration 16/25 | Loss: 0.00094442
Iteration 17/25 | Loss: 0.00094441
Iteration 18/25 | Loss: 0.00094441
Iteration 19/25 | Loss: 0.00094441
Iteration 20/25 | Loss: 0.00094441
Iteration 21/25 | Loss: 0.00094441
Iteration 22/25 | Loss: 0.00094441
Iteration 23/25 | Loss: 0.00094441
Iteration 24/25 | Loss: 0.00094441
Iteration 25/25 | Loss: 0.00094441

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.31060600
Iteration 2/25 | Loss: 0.00056665
Iteration 3/25 | Loss: 0.00052439
Iteration 4/25 | Loss: 0.00052439
Iteration 5/25 | Loss: 0.00052439
Iteration 6/25 | Loss: 0.00052439
Iteration 7/25 | Loss: 0.00052439
Iteration 8/25 | Loss: 0.00052439
Iteration 9/25 | Loss: 0.00052439
Iteration 10/25 | Loss: 0.00052439
Iteration 11/25 | Loss: 0.00052439
Iteration 12/25 | Loss: 0.00052439
Iteration 13/25 | Loss: 0.00052439
Iteration 14/25 | Loss: 0.00052439
Iteration 15/25 | Loss: 0.00052439
Iteration 16/25 | Loss: 0.00052439
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005243915948085487, 0.0005243915948085487, 0.0005243915948085487, 0.0005243915948085487, 0.0005243915948085487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005243915948085487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052439
Iteration 2/1000 | Loss: 0.00003781
Iteration 3/1000 | Loss: 0.00002848
Iteration 4/1000 | Loss: 0.00006252
Iteration 5/1000 | Loss: 0.00002349
Iteration 6/1000 | Loss: 0.00004558
Iteration 7/1000 | Loss: 0.00003780
Iteration 8/1000 | Loss: 0.00002156
Iteration 9/1000 | Loss: 0.00002128
Iteration 10/1000 | Loss: 0.00002100
Iteration 11/1000 | Loss: 0.00002077
Iteration 12/1000 | Loss: 0.00002071
Iteration 13/1000 | Loss: 0.00002071
Iteration 14/1000 | Loss: 0.00002070
Iteration 15/1000 | Loss: 0.00002069
Iteration 16/1000 | Loss: 0.00002065
Iteration 17/1000 | Loss: 0.00002064
Iteration 18/1000 | Loss: 0.00002063
Iteration 19/1000 | Loss: 0.00002062
Iteration 20/1000 | Loss: 0.00002054
Iteration 21/1000 | Loss: 0.00002053
Iteration 22/1000 | Loss: 0.00002053
Iteration 23/1000 | Loss: 0.00002051
Iteration 24/1000 | Loss: 0.00002050
Iteration 25/1000 | Loss: 0.00002049
Iteration 26/1000 | Loss: 0.00002049
Iteration 27/1000 | Loss: 0.00002047
Iteration 28/1000 | Loss: 0.00002045
Iteration 29/1000 | Loss: 0.00002044
Iteration 30/1000 | Loss: 0.00002043
Iteration 31/1000 | Loss: 0.00002043
Iteration 32/1000 | Loss: 0.00002042
Iteration 33/1000 | Loss: 0.00002039
Iteration 34/1000 | Loss: 0.00002039
Iteration 35/1000 | Loss: 0.00002039
Iteration 36/1000 | Loss: 0.00002039
Iteration 37/1000 | Loss: 0.00002039
Iteration 38/1000 | Loss: 0.00002039
Iteration 39/1000 | Loss: 0.00002039
Iteration 40/1000 | Loss: 0.00002039
Iteration 41/1000 | Loss: 0.00002039
Iteration 42/1000 | Loss: 0.00002038
Iteration 43/1000 | Loss: 0.00002038
Iteration 44/1000 | Loss: 0.00002038
Iteration 45/1000 | Loss: 0.00002037
Iteration 46/1000 | Loss: 0.00002037
Iteration 47/1000 | Loss: 0.00002036
Iteration 48/1000 | Loss: 0.00002036
Iteration 49/1000 | Loss: 0.00002036
Iteration 50/1000 | Loss: 0.00002036
Iteration 51/1000 | Loss: 0.00002036
Iteration 52/1000 | Loss: 0.00004804
Iteration 53/1000 | Loss: 0.00002442
Iteration 54/1000 | Loss: 0.00002040
Iteration 55/1000 | Loss: 0.00002037
Iteration 56/1000 | Loss: 0.00002036
Iteration 57/1000 | Loss: 0.00002036
Iteration 58/1000 | Loss: 0.00002035
Iteration 59/1000 | Loss: 0.00002035
Iteration 60/1000 | Loss: 0.00002035
Iteration 61/1000 | Loss: 0.00002035
Iteration 62/1000 | Loss: 0.00002035
Iteration 63/1000 | Loss: 0.00002035
Iteration 64/1000 | Loss: 0.00002035
Iteration 65/1000 | Loss: 0.00002035
Iteration 66/1000 | Loss: 0.00002034
Iteration 67/1000 | Loss: 0.00002034
Iteration 68/1000 | Loss: 0.00002033
Iteration 69/1000 | Loss: 0.00002033
Iteration 70/1000 | Loss: 0.00002033
Iteration 71/1000 | Loss: 0.00002033
Iteration 72/1000 | Loss: 0.00002032
Iteration 73/1000 | Loss: 0.00002031
Iteration 74/1000 | Loss: 0.00002031
Iteration 75/1000 | Loss: 0.00002031
Iteration 76/1000 | Loss: 0.00002031
Iteration 77/1000 | Loss: 0.00002031
Iteration 78/1000 | Loss: 0.00002031
Iteration 79/1000 | Loss: 0.00003112
Iteration 80/1000 | Loss: 0.00002149
Iteration 81/1000 | Loss: 0.00002033
Iteration 82/1000 | Loss: 0.00002033
Iteration 83/1000 | Loss: 0.00002032
Iteration 84/1000 | Loss: 0.00002032
Iteration 85/1000 | Loss: 0.00002032
Iteration 86/1000 | Loss: 0.00002031
Iteration 87/1000 | Loss: 0.00002031
Iteration 88/1000 | Loss: 0.00002030
Iteration 89/1000 | Loss: 0.00002030
Iteration 90/1000 | Loss: 0.00002030
Iteration 91/1000 | Loss: 0.00002029
Iteration 92/1000 | Loss: 0.00002029
Iteration 93/1000 | Loss: 0.00002029
Iteration 94/1000 | Loss: 0.00002029
Iteration 95/1000 | Loss: 0.00002029
Iteration 96/1000 | Loss: 0.00002029
Iteration 97/1000 | Loss: 0.00003496
Iteration 98/1000 | Loss: 0.00002029
Iteration 99/1000 | Loss: 0.00002029
Iteration 100/1000 | Loss: 0.00002029
Iteration 101/1000 | Loss: 0.00002028
Iteration 102/1000 | Loss: 0.00002028
Iteration 103/1000 | Loss: 0.00002028
Iteration 104/1000 | Loss: 0.00002028
Iteration 105/1000 | Loss: 0.00002028
Iteration 106/1000 | Loss: 0.00002028
Iteration 107/1000 | Loss: 0.00002028
Iteration 108/1000 | Loss: 0.00002028
Iteration 109/1000 | Loss: 0.00002028
Iteration 110/1000 | Loss: 0.00002028
Iteration 111/1000 | Loss: 0.00002028
Iteration 112/1000 | Loss: 0.00002028
Iteration 113/1000 | Loss: 0.00002027
Iteration 114/1000 | Loss: 0.00002027
Iteration 115/1000 | Loss: 0.00002027
Iteration 116/1000 | Loss: 0.00002027
Iteration 117/1000 | Loss: 0.00002026
Iteration 118/1000 | Loss: 0.00002026
Iteration 119/1000 | Loss: 0.00002025
Iteration 120/1000 | Loss: 0.00002025
Iteration 121/1000 | Loss: 0.00002025
Iteration 122/1000 | Loss: 0.00002025
Iteration 123/1000 | Loss: 0.00002025
Iteration 124/1000 | Loss: 0.00002025
Iteration 125/1000 | Loss: 0.00002025
Iteration 126/1000 | Loss: 0.00002025
Iteration 127/1000 | Loss: 0.00002025
Iteration 128/1000 | Loss: 0.00002025
Iteration 129/1000 | Loss: 0.00002025
Iteration 130/1000 | Loss: 0.00002025
Iteration 131/1000 | Loss: 0.00002025
Iteration 132/1000 | Loss: 0.00002024
Iteration 133/1000 | Loss: 0.00002024
Iteration 134/1000 | Loss: 0.00002024
Iteration 135/1000 | Loss: 0.00002024
Iteration 136/1000 | Loss: 0.00002024
Iteration 137/1000 | Loss: 0.00002024
Iteration 138/1000 | Loss: 0.00002024
Iteration 139/1000 | Loss: 0.00002024
Iteration 140/1000 | Loss: 0.00002024
Iteration 141/1000 | Loss: 0.00002024
Iteration 142/1000 | Loss: 0.00002024
Iteration 143/1000 | Loss: 0.00002024
Iteration 144/1000 | Loss: 0.00002024
Iteration 145/1000 | Loss: 0.00002024
Iteration 146/1000 | Loss: 0.00002024
Iteration 147/1000 | Loss: 0.00002024
Iteration 148/1000 | Loss: 0.00002024
Iteration 149/1000 | Loss: 0.00002024
Iteration 150/1000 | Loss: 0.00002024
Iteration 151/1000 | Loss: 0.00002024
Iteration 152/1000 | Loss: 0.00002024
Iteration 153/1000 | Loss: 0.00002024
Iteration 154/1000 | Loss: 0.00002024
Iteration 155/1000 | Loss: 0.00002024
Iteration 156/1000 | Loss: 0.00002024
Iteration 157/1000 | Loss: 0.00002024
Iteration 158/1000 | Loss: 0.00002024
Iteration 159/1000 | Loss: 0.00002024
Iteration 160/1000 | Loss: 0.00002024
Iteration 161/1000 | Loss: 0.00002024
Iteration 162/1000 | Loss: 0.00002024
Iteration 163/1000 | Loss: 0.00002024
Iteration 164/1000 | Loss: 0.00002024
Iteration 165/1000 | Loss: 0.00002024
Iteration 166/1000 | Loss: 0.00002024
Iteration 167/1000 | Loss: 0.00002024
Iteration 168/1000 | Loss: 0.00002024
Iteration 169/1000 | Loss: 0.00002024
Iteration 170/1000 | Loss: 0.00002024
Iteration 171/1000 | Loss: 0.00002024
Iteration 172/1000 | Loss: 0.00002024
Iteration 173/1000 | Loss: 0.00002024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.024011882895138e-05, 2.024011882895138e-05, 2.024011882895138e-05, 2.024011882895138e-05, 2.024011882895138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.024011882895138e-05

Optimization complete. Final v2v error: 3.8781075477600098 mm

Highest mean error: 4.351751804351807 mm for frame 164

Lowest mean error: 3.6264240741729736 mm for frame 24

Saving results

Total time: 94.29418754577637
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00554004
Iteration 2/25 | Loss: 0.00122687
Iteration 3/25 | Loss: 0.00100445
Iteration 4/25 | Loss: 0.00098092
Iteration 5/25 | Loss: 0.00097287
Iteration 6/25 | Loss: 0.00097019
Iteration 7/25 | Loss: 0.00096911
Iteration 8/25 | Loss: 0.00096904
Iteration 9/25 | Loss: 0.00096904
Iteration 10/25 | Loss: 0.00096904
Iteration 11/25 | Loss: 0.00096904
Iteration 12/25 | Loss: 0.00096904
Iteration 13/25 | Loss: 0.00096904
Iteration 14/25 | Loss: 0.00096904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009690388105809689, 0.0009690388105809689, 0.0009690388105809689, 0.0009690388105809689, 0.0009690388105809689]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009690388105809689

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79302299
Iteration 2/25 | Loss: 0.00057554
Iteration 3/25 | Loss: 0.00057554
Iteration 4/25 | Loss: 0.00057554
Iteration 5/25 | Loss: 0.00057554
Iteration 6/25 | Loss: 0.00057554
Iteration 7/25 | Loss: 0.00057554
Iteration 8/25 | Loss: 0.00057554
Iteration 9/25 | Loss: 0.00057554
Iteration 10/25 | Loss: 0.00057553
Iteration 11/25 | Loss: 0.00057553
Iteration 12/25 | Loss: 0.00057553
Iteration 13/25 | Loss: 0.00057553
Iteration 14/25 | Loss: 0.00057553
Iteration 15/25 | Loss: 0.00057553
Iteration 16/25 | Loss: 0.00057553
Iteration 17/25 | Loss: 0.00057553
Iteration 18/25 | Loss: 0.00057553
Iteration 19/25 | Loss: 0.00057553
Iteration 20/25 | Loss: 0.00057553
Iteration 21/25 | Loss: 0.00057553
Iteration 22/25 | Loss: 0.00057553
Iteration 23/25 | Loss: 0.00057553
Iteration 24/25 | Loss: 0.00057553
Iteration 25/25 | Loss: 0.00057553

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057553
Iteration 2/1000 | Loss: 0.00004242
Iteration 3/1000 | Loss: 0.00002939
Iteration 4/1000 | Loss: 0.00002586
Iteration 5/1000 | Loss: 0.00002341
Iteration 6/1000 | Loss: 0.00002222
Iteration 7/1000 | Loss: 0.00002163
Iteration 8/1000 | Loss: 0.00002130
Iteration 9/1000 | Loss: 0.00002092
Iteration 10/1000 | Loss: 0.00002070
Iteration 11/1000 | Loss: 0.00002056
Iteration 12/1000 | Loss: 0.00002055
Iteration 13/1000 | Loss: 0.00002053
Iteration 14/1000 | Loss: 0.00002053
Iteration 15/1000 | Loss: 0.00002053
Iteration 16/1000 | Loss: 0.00002051
Iteration 17/1000 | Loss: 0.00002049
Iteration 18/1000 | Loss: 0.00002048
Iteration 19/1000 | Loss: 0.00002048
Iteration 20/1000 | Loss: 0.00002048
Iteration 21/1000 | Loss: 0.00002048
Iteration 22/1000 | Loss: 0.00002047
Iteration 23/1000 | Loss: 0.00002044
Iteration 24/1000 | Loss: 0.00002044
Iteration 25/1000 | Loss: 0.00002044
Iteration 26/1000 | Loss: 0.00002044
Iteration 27/1000 | Loss: 0.00002043
Iteration 28/1000 | Loss: 0.00002043
Iteration 29/1000 | Loss: 0.00002043
Iteration 30/1000 | Loss: 0.00002043
Iteration 31/1000 | Loss: 0.00002043
Iteration 32/1000 | Loss: 0.00002043
Iteration 33/1000 | Loss: 0.00002043
Iteration 34/1000 | Loss: 0.00002043
Iteration 35/1000 | Loss: 0.00002043
Iteration 36/1000 | Loss: 0.00002043
Iteration 37/1000 | Loss: 0.00002043
Iteration 38/1000 | Loss: 0.00002043
Iteration 39/1000 | Loss: 0.00002043
Iteration 40/1000 | Loss: 0.00002043
Iteration 41/1000 | Loss: 0.00002043
Iteration 42/1000 | Loss: 0.00002043
Iteration 43/1000 | Loss: 0.00002043
Iteration 44/1000 | Loss: 0.00002043
Iteration 45/1000 | Loss: 0.00002043
Iteration 46/1000 | Loss: 0.00002043
Iteration 47/1000 | Loss: 0.00002043
Iteration 48/1000 | Loss: 0.00002043
Iteration 49/1000 | Loss: 0.00002043
Iteration 50/1000 | Loss: 0.00002043
Iteration 51/1000 | Loss: 0.00002043
Iteration 52/1000 | Loss: 0.00002043
Iteration 53/1000 | Loss: 0.00002043
Iteration 54/1000 | Loss: 0.00002043
Iteration 55/1000 | Loss: 0.00002043
Iteration 56/1000 | Loss: 0.00002043
Iteration 57/1000 | Loss: 0.00002043
Iteration 58/1000 | Loss: 0.00002043
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [2.0431112716323696e-05, 2.0431112716323696e-05, 2.0431112716323696e-05, 2.0431112716323696e-05, 2.0431112716323696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0431112716323696e-05

Optimization complete. Final v2v error: 3.9228694438934326 mm

Highest mean error: 4.560750484466553 mm for frame 239

Lowest mean error: 3.5906219482421875 mm for frame 1

Saving results

Total time: 34.592063188552856
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975501
Iteration 2/25 | Loss: 0.00166633
Iteration 3/25 | Loss: 0.00114843
Iteration 4/25 | Loss: 0.00110124
Iteration 5/25 | Loss: 0.00109183
Iteration 6/25 | Loss: 0.00108880
Iteration 7/25 | Loss: 0.00108772
Iteration 8/25 | Loss: 0.00108768
Iteration 9/25 | Loss: 0.00108768
Iteration 10/25 | Loss: 0.00108768
Iteration 11/25 | Loss: 0.00108768
Iteration 12/25 | Loss: 0.00108768
Iteration 13/25 | Loss: 0.00108768
Iteration 14/25 | Loss: 0.00108768
Iteration 15/25 | Loss: 0.00108768
Iteration 16/25 | Loss: 0.00108768
Iteration 17/25 | Loss: 0.00108768
Iteration 18/25 | Loss: 0.00108768
Iteration 19/25 | Loss: 0.00108768
Iteration 20/25 | Loss: 0.00108768
Iteration 21/25 | Loss: 0.00108768
Iteration 22/25 | Loss: 0.00108768
Iteration 23/25 | Loss: 0.00108768
Iteration 24/25 | Loss: 0.00108768
Iteration 25/25 | Loss: 0.00108768

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.60209018
Iteration 2/25 | Loss: 0.00057354
Iteration 3/25 | Loss: 0.00057354
Iteration 4/25 | Loss: 0.00057354
Iteration 5/25 | Loss: 0.00057354
Iteration 6/25 | Loss: 0.00057354
Iteration 7/25 | Loss: 0.00057354
Iteration 8/25 | Loss: 0.00057354
Iteration 9/25 | Loss: 0.00057354
Iteration 10/25 | Loss: 0.00057354
Iteration 11/25 | Loss: 0.00057354
Iteration 12/25 | Loss: 0.00057354
Iteration 13/25 | Loss: 0.00057354
Iteration 14/25 | Loss: 0.00057354
Iteration 15/25 | Loss: 0.00057354
Iteration 16/25 | Loss: 0.00057354
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005735366721637547, 0.0005735366721637547, 0.0005735366721637547, 0.0005735366721637547, 0.0005735366721637547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005735366721637547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057354
Iteration 2/1000 | Loss: 0.00005982
Iteration 3/1000 | Loss: 0.00004438
Iteration 4/1000 | Loss: 0.00003922
Iteration 5/1000 | Loss: 0.00003525
Iteration 6/1000 | Loss: 0.00003351
Iteration 7/1000 | Loss: 0.00003283
Iteration 8/1000 | Loss: 0.00003235
Iteration 9/1000 | Loss: 0.00003198
Iteration 10/1000 | Loss: 0.00003163
Iteration 11/1000 | Loss: 0.00003134
Iteration 12/1000 | Loss: 0.00003117
Iteration 13/1000 | Loss: 0.00003112
Iteration 14/1000 | Loss: 0.00003110
Iteration 15/1000 | Loss: 0.00003108
Iteration 16/1000 | Loss: 0.00003108
Iteration 17/1000 | Loss: 0.00003107
Iteration 18/1000 | Loss: 0.00003106
Iteration 19/1000 | Loss: 0.00003106
Iteration 20/1000 | Loss: 0.00003106
Iteration 21/1000 | Loss: 0.00003105
Iteration 22/1000 | Loss: 0.00003105
Iteration 23/1000 | Loss: 0.00003104
Iteration 24/1000 | Loss: 0.00003104
Iteration 25/1000 | Loss: 0.00003103
Iteration 26/1000 | Loss: 0.00003103
Iteration 27/1000 | Loss: 0.00003103
Iteration 28/1000 | Loss: 0.00003102
Iteration 29/1000 | Loss: 0.00003102
Iteration 30/1000 | Loss: 0.00003102
Iteration 31/1000 | Loss: 0.00003101
Iteration 32/1000 | Loss: 0.00003101
Iteration 33/1000 | Loss: 0.00003100
Iteration 34/1000 | Loss: 0.00003100
Iteration 35/1000 | Loss: 0.00003099
Iteration 36/1000 | Loss: 0.00003099
Iteration 37/1000 | Loss: 0.00003099
Iteration 38/1000 | Loss: 0.00003098
Iteration 39/1000 | Loss: 0.00003097
Iteration 40/1000 | Loss: 0.00003097
Iteration 41/1000 | Loss: 0.00003097
Iteration 42/1000 | Loss: 0.00003097
Iteration 43/1000 | Loss: 0.00003097
Iteration 44/1000 | Loss: 0.00003096
Iteration 45/1000 | Loss: 0.00003096
Iteration 46/1000 | Loss: 0.00003096
Iteration 47/1000 | Loss: 0.00003096
Iteration 48/1000 | Loss: 0.00003095
Iteration 49/1000 | Loss: 0.00003095
Iteration 50/1000 | Loss: 0.00003095
Iteration 51/1000 | Loss: 0.00003095
Iteration 52/1000 | Loss: 0.00003095
Iteration 53/1000 | Loss: 0.00003095
Iteration 54/1000 | Loss: 0.00003095
Iteration 55/1000 | Loss: 0.00003095
Iteration 56/1000 | Loss: 0.00003094
Iteration 57/1000 | Loss: 0.00003094
Iteration 58/1000 | Loss: 0.00003094
Iteration 59/1000 | Loss: 0.00003094
Iteration 60/1000 | Loss: 0.00003094
Iteration 61/1000 | Loss: 0.00003093
Iteration 62/1000 | Loss: 0.00003093
Iteration 63/1000 | Loss: 0.00003093
Iteration 64/1000 | Loss: 0.00003092
Iteration 65/1000 | Loss: 0.00003092
Iteration 66/1000 | Loss: 0.00003091
Iteration 67/1000 | Loss: 0.00003090
Iteration 68/1000 | Loss: 0.00003090
Iteration 69/1000 | Loss: 0.00003090
Iteration 70/1000 | Loss: 0.00003090
Iteration 71/1000 | Loss: 0.00003089
Iteration 72/1000 | Loss: 0.00003089
Iteration 73/1000 | Loss: 0.00003089
Iteration 74/1000 | Loss: 0.00003088
Iteration 75/1000 | Loss: 0.00003088
Iteration 76/1000 | Loss: 0.00003088
Iteration 77/1000 | Loss: 0.00003088
Iteration 78/1000 | Loss: 0.00003088
Iteration 79/1000 | Loss: 0.00003088
Iteration 80/1000 | Loss: 0.00003088
Iteration 81/1000 | Loss: 0.00003088
Iteration 82/1000 | Loss: 0.00003088
Iteration 83/1000 | Loss: 0.00003087
Iteration 84/1000 | Loss: 0.00003087
Iteration 85/1000 | Loss: 0.00003087
Iteration 86/1000 | Loss: 0.00003087
Iteration 87/1000 | Loss: 0.00003087
Iteration 88/1000 | Loss: 0.00003087
Iteration 89/1000 | Loss: 0.00003087
Iteration 90/1000 | Loss: 0.00003087
Iteration 91/1000 | Loss: 0.00003087
Iteration 92/1000 | Loss: 0.00003087
Iteration 93/1000 | Loss: 0.00003087
Iteration 94/1000 | Loss: 0.00003087
Iteration 95/1000 | Loss: 0.00003087
Iteration 96/1000 | Loss: 0.00003087
Iteration 97/1000 | Loss: 0.00003087
Iteration 98/1000 | Loss: 0.00003087
Iteration 99/1000 | Loss: 0.00003087
Iteration 100/1000 | Loss: 0.00003087
Iteration 101/1000 | Loss: 0.00003087
Iteration 102/1000 | Loss: 0.00003087
Iteration 103/1000 | Loss: 0.00003087
Iteration 104/1000 | Loss: 0.00003087
Iteration 105/1000 | Loss: 0.00003087
Iteration 106/1000 | Loss: 0.00003087
Iteration 107/1000 | Loss: 0.00003087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [3.087248114752583e-05, 3.087248114752583e-05, 3.087248114752583e-05, 3.087248114752583e-05, 3.087248114752583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.087248114752583e-05

Optimization complete. Final v2v error: 4.7373552322387695 mm

Highest mean error: 5.136074066162109 mm for frame 99

Lowest mean error: 4.433691024780273 mm for frame 87

Saving results

Total time: 36.5638165473938
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01089260
Iteration 2/25 | Loss: 0.01089260
Iteration 3/25 | Loss: 0.01089260
Iteration 4/25 | Loss: 0.00232932
Iteration 5/25 | Loss: 0.00143863
Iteration 6/25 | Loss: 0.00140784
Iteration 7/25 | Loss: 0.00121001
Iteration 8/25 | Loss: 0.00109950
Iteration 9/25 | Loss: 0.00106171
Iteration 10/25 | Loss: 0.00104162
Iteration 11/25 | Loss: 0.00102572
Iteration 12/25 | Loss: 0.00102262
Iteration 13/25 | Loss: 0.00098370
Iteration 14/25 | Loss: 0.00096801
Iteration 15/25 | Loss: 0.00096384
Iteration 16/25 | Loss: 0.00095286
Iteration 17/25 | Loss: 0.00094948
Iteration 18/25 | Loss: 0.00094833
Iteration 19/25 | Loss: 0.00095225
Iteration 20/25 | Loss: 0.00094651
Iteration 21/25 | Loss: 0.00094747
Iteration 22/25 | Loss: 0.00094530
Iteration 23/25 | Loss: 0.00094746
Iteration 24/25 | Loss: 0.00094543
Iteration 25/25 | Loss: 0.00094243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43092573
Iteration 2/25 | Loss: 0.00155378
Iteration 3/25 | Loss: 0.00124951
Iteration 4/25 | Loss: 0.00124951
Iteration 5/25 | Loss: 0.00124951
Iteration 6/25 | Loss: 0.00124951
Iteration 7/25 | Loss: 0.00124951
Iteration 8/25 | Loss: 0.00124951
Iteration 9/25 | Loss: 0.00124951
Iteration 10/25 | Loss: 0.00124951
Iteration 11/25 | Loss: 0.00124951
Iteration 12/25 | Loss: 0.00124951
Iteration 13/25 | Loss: 0.00124951
Iteration 14/25 | Loss: 0.00124951
Iteration 15/25 | Loss: 0.00124951
Iteration 16/25 | Loss: 0.00124951
Iteration 17/25 | Loss: 0.00124951
Iteration 18/25 | Loss: 0.00124951
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001249506138265133, 0.001249506138265133, 0.001249506138265133, 0.001249506138265133, 0.001249506138265133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001249506138265133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124951
Iteration 2/1000 | Loss: 0.00040010
Iteration 3/1000 | Loss: 0.00114676
Iteration 4/1000 | Loss: 0.00053017
Iteration 5/1000 | Loss: 0.00024067
Iteration 6/1000 | Loss: 0.00038092
Iteration 7/1000 | Loss: 0.00042845
Iteration 8/1000 | Loss: 0.00117719
Iteration 9/1000 | Loss: 0.00024591
Iteration 10/1000 | Loss: 0.00061032
Iteration 11/1000 | Loss: 0.00043939
Iteration 12/1000 | Loss: 0.00072013
Iteration 13/1000 | Loss: 0.00043297
Iteration 14/1000 | Loss: 0.00055658
Iteration 15/1000 | Loss: 0.00012497
Iteration 16/1000 | Loss: 0.00011149
Iteration 17/1000 | Loss: 0.00130592
Iteration 18/1000 | Loss: 0.00093242
Iteration 19/1000 | Loss: 0.00041806
Iteration 20/1000 | Loss: 0.00024334
Iteration 21/1000 | Loss: 0.00012197
Iteration 22/1000 | Loss: 0.00040084
Iteration 23/1000 | Loss: 0.00011060
Iteration 24/1000 | Loss: 0.00059823
Iteration 25/1000 | Loss: 0.00023097
Iteration 26/1000 | Loss: 0.00019841
Iteration 27/1000 | Loss: 0.00023720
Iteration 28/1000 | Loss: 0.00030860
Iteration 29/1000 | Loss: 0.00014896
Iteration 30/1000 | Loss: 0.00049563
Iteration 31/1000 | Loss: 0.00020165
Iteration 32/1000 | Loss: 0.00008732
Iteration 33/1000 | Loss: 0.00010933
Iteration 34/1000 | Loss: 0.00010807
Iteration 35/1000 | Loss: 0.00094399
Iteration 36/1000 | Loss: 0.00043078
Iteration 37/1000 | Loss: 0.00037224
Iteration 38/1000 | Loss: 0.00051499
Iteration 39/1000 | Loss: 0.00240057
Iteration 40/1000 | Loss: 0.00250401
Iteration 41/1000 | Loss: 0.00129929
Iteration 42/1000 | Loss: 0.00252875
Iteration 43/1000 | Loss: 0.00118405
Iteration 44/1000 | Loss: 0.00180388
Iteration 45/1000 | Loss: 0.00113366
Iteration 46/1000 | Loss: 0.00043171
Iteration 47/1000 | Loss: 0.00012443
Iteration 48/1000 | Loss: 0.00009584
Iteration 49/1000 | Loss: 0.00036033
Iteration 50/1000 | Loss: 0.00018681
Iteration 51/1000 | Loss: 0.00009768
Iteration 52/1000 | Loss: 0.00007270
Iteration 53/1000 | Loss: 0.00008886
Iteration 54/1000 | Loss: 0.00031122
Iteration 55/1000 | Loss: 0.00014237
Iteration 56/1000 | Loss: 0.00061721
Iteration 57/1000 | Loss: 0.00006713
Iteration 58/1000 | Loss: 0.00006536
Iteration 59/1000 | Loss: 0.00016274
Iteration 60/1000 | Loss: 0.00006416
Iteration 61/1000 | Loss: 0.00034777
Iteration 62/1000 | Loss: 0.00025505
Iteration 63/1000 | Loss: 0.00019076
Iteration 64/1000 | Loss: 0.00006998
Iteration 65/1000 | Loss: 0.00032718
Iteration 66/1000 | Loss: 0.00444828
Iteration 67/1000 | Loss: 0.00049315
Iteration 68/1000 | Loss: 0.00018183
Iteration 69/1000 | Loss: 0.00011586
Iteration 70/1000 | Loss: 0.00018582
Iteration 71/1000 | Loss: 0.00013805
Iteration 72/1000 | Loss: 0.00006388
Iteration 73/1000 | Loss: 0.00022035
Iteration 74/1000 | Loss: 0.00005058
Iteration 75/1000 | Loss: 0.00013525
Iteration 76/1000 | Loss: 0.00004721
Iteration 77/1000 | Loss: 0.00009128
Iteration 78/1000 | Loss: 0.00004568
Iteration 79/1000 | Loss: 0.00004483
Iteration 80/1000 | Loss: 0.00004507
Iteration 81/1000 | Loss: 0.00007910
Iteration 82/1000 | Loss: 0.00006525
Iteration 83/1000 | Loss: 0.00004355
Iteration 84/1000 | Loss: 0.00007917
Iteration 85/1000 | Loss: 0.00004818
Iteration 86/1000 | Loss: 0.00005203
Iteration 87/1000 | Loss: 0.00004300
Iteration 88/1000 | Loss: 0.00004369
Iteration 89/1000 | Loss: 0.00004293
Iteration 90/1000 | Loss: 0.00004264
Iteration 91/1000 | Loss: 0.00004261
Iteration 92/1000 | Loss: 0.00004259
Iteration 93/1000 | Loss: 0.00004258
Iteration 94/1000 | Loss: 0.00004258
Iteration 95/1000 | Loss: 0.00004258
Iteration 96/1000 | Loss: 0.00004258
Iteration 97/1000 | Loss: 0.00004257
Iteration 98/1000 | Loss: 0.00004254
Iteration 99/1000 | Loss: 0.00004254
Iteration 100/1000 | Loss: 0.00004251
Iteration 101/1000 | Loss: 0.00004250
Iteration 102/1000 | Loss: 0.00006288
Iteration 103/1000 | Loss: 0.00004301
Iteration 104/1000 | Loss: 0.00004254
Iteration 105/1000 | Loss: 0.00004243
Iteration 106/1000 | Loss: 0.00004243
Iteration 107/1000 | Loss: 0.00004243
Iteration 108/1000 | Loss: 0.00004243
Iteration 109/1000 | Loss: 0.00004243
Iteration 110/1000 | Loss: 0.00004243
Iteration 111/1000 | Loss: 0.00004243
Iteration 112/1000 | Loss: 0.00004243
Iteration 113/1000 | Loss: 0.00004243
Iteration 114/1000 | Loss: 0.00004243
Iteration 115/1000 | Loss: 0.00004242
Iteration 116/1000 | Loss: 0.00004242
Iteration 117/1000 | Loss: 0.00004242
Iteration 118/1000 | Loss: 0.00004242
Iteration 119/1000 | Loss: 0.00004242
Iteration 120/1000 | Loss: 0.00004242
Iteration 121/1000 | Loss: 0.00004242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [4.2423558625159785e-05, 4.2423558625159785e-05, 4.2423558625159785e-05, 4.2423558625159785e-05, 4.2423558625159785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.2423558625159785e-05

Optimization complete. Final v2v error: 4.534208297729492 mm

Highest mean error: 21.827970504760742 mm for frame 136

Lowest mean error: 3.4440696239471436 mm for frame 20

Saving results

Total time: 209.79798889160156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00364336
Iteration 2/25 | Loss: 0.00120948
Iteration 3/25 | Loss: 0.00103940
Iteration 4/25 | Loss: 0.00101344
Iteration 5/25 | Loss: 0.00100682
Iteration 6/25 | Loss: 0.00100576
Iteration 7/25 | Loss: 0.00100576
Iteration 8/25 | Loss: 0.00100576
Iteration 9/25 | Loss: 0.00100576
Iteration 10/25 | Loss: 0.00100576
Iteration 11/25 | Loss: 0.00100576
Iteration 12/25 | Loss: 0.00100576
Iteration 13/25 | Loss: 0.00100576
Iteration 14/25 | Loss: 0.00100576
Iteration 15/25 | Loss: 0.00100576
Iteration 16/25 | Loss: 0.00100576
Iteration 17/25 | Loss: 0.00100576
Iteration 18/25 | Loss: 0.00100576
Iteration 19/25 | Loss: 0.00100576
Iteration 20/25 | Loss: 0.00100576
Iteration 21/25 | Loss: 0.00100576
Iteration 22/25 | Loss: 0.00100576
Iteration 23/25 | Loss: 0.00100576
Iteration 24/25 | Loss: 0.00100576
Iteration 25/25 | Loss: 0.00100576

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81639552
Iteration 2/25 | Loss: 0.00059382
Iteration 3/25 | Loss: 0.00059382
Iteration 4/25 | Loss: 0.00059382
Iteration 5/25 | Loss: 0.00059382
Iteration 6/25 | Loss: 0.00059382
Iteration 7/25 | Loss: 0.00059382
Iteration 8/25 | Loss: 0.00059382
Iteration 9/25 | Loss: 0.00059382
Iteration 10/25 | Loss: 0.00059382
Iteration 11/25 | Loss: 0.00059382
Iteration 12/25 | Loss: 0.00059382
Iteration 13/25 | Loss: 0.00059382
Iteration 14/25 | Loss: 0.00059382
Iteration 15/25 | Loss: 0.00059382
Iteration 16/25 | Loss: 0.00059382
Iteration 17/25 | Loss: 0.00059382
Iteration 18/25 | Loss: 0.00059382
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005938163958489895, 0.0005938163958489895, 0.0005938163958489895, 0.0005938163958489895, 0.0005938163958489895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005938163958489895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059382
Iteration 2/1000 | Loss: 0.00003976
Iteration 3/1000 | Loss: 0.00003148
Iteration 4/1000 | Loss: 0.00002720
Iteration 5/1000 | Loss: 0.00002545
Iteration 6/1000 | Loss: 0.00002441
Iteration 7/1000 | Loss: 0.00002382
Iteration 8/1000 | Loss: 0.00002344
Iteration 9/1000 | Loss: 0.00002321
Iteration 10/1000 | Loss: 0.00002310
Iteration 11/1000 | Loss: 0.00002293
Iteration 12/1000 | Loss: 0.00002288
Iteration 13/1000 | Loss: 0.00002284
Iteration 14/1000 | Loss: 0.00002283
Iteration 15/1000 | Loss: 0.00002282
Iteration 16/1000 | Loss: 0.00002282
Iteration 17/1000 | Loss: 0.00002281
Iteration 18/1000 | Loss: 0.00002281
Iteration 19/1000 | Loss: 0.00002280
Iteration 20/1000 | Loss: 0.00002279
Iteration 21/1000 | Loss: 0.00002279
Iteration 22/1000 | Loss: 0.00002278
Iteration 23/1000 | Loss: 0.00002277
Iteration 24/1000 | Loss: 0.00002277
Iteration 25/1000 | Loss: 0.00002276
Iteration 26/1000 | Loss: 0.00002276
Iteration 27/1000 | Loss: 0.00002275
Iteration 28/1000 | Loss: 0.00002275
Iteration 29/1000 | Loss: 0.00002274
Iteration 30/1000 | Loss: 0.00002274
Iteration 31/1000 | Loss: 0.00002274
Iteration 32/1000 | Loss: 0.00002274
Iteration 33/1000 | Loss: 0.00002274
Iteration 34/1000 | Loss: 0.00002274
Iteration 35/1000 | Loss: 0.00002274
Iteration 36/1000 | Loss: 0.00002273
Iteration 37/1000 | Loss: 0.00002273
Iteration 38/1000 | Loss: 0.00002273
Iteration 39/1000 | Loss: 0.00002273
Iteration 40/1000 | Loss: 0.00002273
Iteration 41/1000 | Loss: 0.00002273
Iteration 42/1000 | Loss: 0.00002273
Iteration 43/1000 | Loss: 0.00002273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 43. Stopping optimization.
Last 5 losses: [2.273188147228211e-05, 2.273188147228211e-05, 2.273188147228211e-05, 2.273188147228211e-05, 2.273188147228211e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.273188147228211e-05

Optimization complete. Final v2v error: 4.14631986618042 mm

Highest mean error: 4.4720458984375 mm for frame 238

Lowest mean error: 3.8815252780914307 mm for frame 228

Saving results

Total time: 30.181268215179443
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463424
Iteration 2/25 | Loss: 0.00114629
Iteration 3/25 | Loss: 0.00100635
Iteration 4/25 | Loss: 0.00099073
Iteration 5/25 | Loss: 0.00098598
Iteration 6/25 | Loss: 0.00098478
Iteration 7/25 | Loss: 0.00098467
Iteration 8/25 | Loss: 0.00098467
Iteration 9/25 | Loss: 0.00098467
Iteration 10/25 | Loss: 0.00098467
Iteration 11/25 | Loss: 0.00098467
Iteration 12/25 | Loss: 0.00098467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000984666869044304, 0.000984666869044304, 0.000984666869044304, 0.000984666869044304, 0.000984666869044304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000984666869044304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44726610
Iteration 2/25 | Loss: 0.00060512
Iteration 3/25 | Loss: 0.00060511
Iteration 4/25 | Loss: 0.00060511
Iteration 5/25 | Loss: 0.00060511
Iteration 6/25 | Loss: 0.00060511
Iteration 7/25 | Loss: 0.00060511
Iteration 8/25 | Loss: 0.00060511
Iteration 9/25 | Loss: 0.00060511
Iteration 10/25 | Loss: 0.00060511
Iteration 11/25 | Loss: 0.00060511
Iteration 12/25 | Loss: 0.00060511
Iteration 13/25 | Loss: 0.00060511
Iteration 14/25 | Loss: 0.00060511
Iteration 15/25 | Loss: 0.00060511
Iteration 16/25 | Loss: 0.00060511
Iteration 17/25 | Loss: 0.00060511
Iteration 18/25 | Loss: 0.00060511
Iteration 19/25 | Loss: 0.00060511
Iteration 20/25 | Loss: 0.00060511
Iteration 21/25 | Loss: 0.00060511
Iteration 22/25 | Loss: 0.00060511
Iteration 23/25 | Loss: 0.00060511
Iteration 24/25 | Loss: 0.00060511
Iteration 25/25 | Loss: 0.00060511

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060511
Iteration 2/1000 | Loss: 0.00004921
Iteration 3/1000 | Loss: 0.00003266
Iteration 4/1000 | Loss: 0.00002804
Iteration 5/1000 | Loss: 0.00002495
Iteration 6/1000 | Loss: 0.00002330
Iteration 7/1000 | Loss: 0.00002224
Iteration 8/1000 | Loss: 0.00002170
Iteration 9/1000 | Loss: 0.00002147
Iteration 10/1000 | Loss: 0.00002125
Iteration 11/1000 | Loss: 0.00002105
Iteration 12/1000 | Loss: 0.00002105
Iteration 13/1000 | Loss: 0.00002103
Iteration 14/1000 | Loss: 0.00002102
Iteration 15/1000 | Loss: 0.00002097
Iteration 16/1000 | Loss: 0.00002096
Iteration 17/1000 | Loss: 0.00002096
Iteration 18/1000 | Loss: 0.00002095
Iteration 19/1000 | Loss: 0.00002094
Iteration 20/1000 | Loss: 0.00002094
Iteration 21/1000 | Loss: 0.00002093
Iteration 22/1000 | Loss: 0.00002093
Iteration 23/1000 | Loss: 0.00002092
Iteration 24/1000 | Loss: 0.00002092
Iteration 25/1000 | Loss: 0.00002092
Iteration 26/1000 | Loss: 0.00002091
Iteration 27/1000 | Loss: 0.00002091
Iteration 28/1000 | Loss: 0.00002091
Iteration 29/1000 | Loss: 0.00002090
Iteration 30/1000 | Loss: 0.00002090
Iteration 31/1000 | Loss: 0.00002090
Iteration 32/1000 | Loss: 0.00002090
Iteration 33/1000 | Loss: 0.00002090
Iteration 34/1000 | Loss: 0.00002090
Iteration 35/1000 | Loss: 0.00002090
Iteration 36/1000 | Loss: 0.00002090
Iteration 37/1000 | Loss: 0.00002090
Iteration 38/1000 | Loss: 0.00002089
Iteration 39/1000 | Loss: 0.00002089
Iteration 40/1000 | Loss: 0.00002089
Iteration 41/1000 | Loss: 0.00002088
Iteration 42/1000 | Loss: 0.00002088
Iteration 43/1000 | Loss: 0.00002088
Iteration 44/1000 | Loss: 0.00002088
Iteration 45/1000 | Loss: 0.00002087
Iteration 46/1000 | Loss: 0.00002087
Iteration 47/1000 | Loss: 0.00002087
Iteration 48/1000 | Loss: 0.00002087
Iteration 49/1000 | Loss: 0.00002086
Iteration 50/1000 | Loss: 0.00002086
Iteration 51/1000 | Loss: 0.00002086
Iteration 52/1000 | Loss: 0.00002085
Iteration 53/1000 | Loss: 0.00002085
Iteration 54/1000 | Loss: 0.00002085
Iteration 55/1000 | Loss: 0.00002085
Iteration 56/1000 | Loss: 0.00002085
Iteration 57/1000 | Loss: 0.00002085
Iteration 58/1000 | Loss: 0.00002085
Iteration 59/1000 | Loss: 0.00002084
Iteration 60/1000 | Loss: 0.00002084
Iteration 61/1000 | Loss: 0.00002084
Iteration 62/1000 | Loss: 0.00002084
Iteration 63/1000 | Loss: 0.00002083
Iteration 64/1000 | Loss: 0.00002083
Iteration 65/1000 | Loss: 0.00002083
Iteration 66/1000 | Loss: 0.00002083
Iteration 67/1000 | Loss: 0.00002083
Iteration 68/1000 | Loss: 0.00002083
Iteration 69/1000 | Loss: 0.00002083
Iteration 70/1000 | Loss: 0.00002083
Iteration 71/1000 | Loss: 0.00002083
Iteration 72/1000 | Loss: 0.00002083
Iteration 73/1000 | Loss: 0.00002083
Iteration 74/1000 | Loss: 0.00002083
Iteration 75/1000 | Loss: 0.00002083
Iteration 76/1000 | Loss: 0.00002082
Iteration 77/1000 | Loss: 0.00002082
Iteration 78/1000 | Loss: 0.00002082
Iteration 79/1000 | Loss: 0.00002082
Iteration 80/1000 | Loss: 0.00002082
Iteration 81/1000 | Loss: 0.00002082
Iteration 82/1000 | Loss: 0.00002082
Iteration 83/1000 | Loss: 0.00002082
Iteration 84/1000 | Loss: 0.00002082
Iteration 85/1000 | Loss: 0.00002082
Iteration 86/1000 | Loss: 0.00002082
Iteration 87/1000 | Loss: 0.00002081
Iteration 88/1000 | Loss: 0.00002081
Iteration 89/1000 | Loss: 0.00002081
Iteration 90/1000 | Loss: 0.00002081
Iteration 91/1000 | Loss: 0.00002081
Iteration 92/1000 | Loss: 0.00002081
Iteration 93/1000 | Loss: 0.00002081
Iteration 94/1000 | Loss: 0.00002081
Iteration 95/1000 | Loss: 0.00002081
Iteration 96/1000 | Loss: 0.00002081
Iteration 97/1000 | Loss: 0.00002081
Iteration 98/1000 | Loss: 0.00002081
Iteration 99/1000 | Loss: 0.00002081
Iteration 100/1000 | Loss: 0.00002081
Iteration 101/1000 | Loss: 0.00002081
Iteration 102/1000 | Loss: 0.00002081
Iteration 103/1000 | Loss: 0.00002081
Iteration 104/1000 | Loss: 0.00002081
Iteration 105/1000 | Loss: 0.00002081
Iteration 106/1000 | Loss: 0.00002081
Iteration 107/1000 | Loss: 0.00002081
Iteration 108/1000 | Loss: 0.00002081
Iteration 109/1000 | Loss: 0.00002081
Iteration 110/1000 | Loss: 0.00002081
Iteration 111/1000 | Loss: 0.00002081
Iteration 112/1000 | Loss: 0.00002081
Iteration 113/1000 | Loss: 0.00002081
Iteration 114/1000 | Loss: 0.00002081
Iteration 115/1000 | Loss: 0.00002081
Iteration 116/1000 | Loss: 0.00002081
Iteration 117/1000 | Loss: 0.00002081
Iteration 118/1000 | Loss: 0.00002081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.081114871543832e-05, 2.081114871543832e-05, 2.081114871543832e-05, 2.081114871543832e-05, 2.081114871543832e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.081114871543832e-05

Optimization complete. Final v2v error: 3.9801509380340576 mm

Highest mean error: 4.92109489440918 mm for frame 160

Lowest mean error: 3.4900197982788086 mm for frame 47

Saving results

Total time: 32.39609909057617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976184
Iteration 2/25 | Loss: 0.00130297
Iteration 3/25 | Loss: 0.00110421
Iteration 4/25 | Loss: 0.00106922
Iteration 5/25 | Loss: 0.00105047
Iteration 6/25 | Loss: 0.00104612
Iteration 7/25 | Loss: 0.00104520
Iteration 8/25 | Loss: 0.00104520
Iteration 9/25 | Loss: 0.00104520
Iteration 10/25 | Loss: 0.00104520
Iteration 11/25 | Loss: 0.00104520
Iteration 12/25 | Loss: 0.00104520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010452043497934937, 0.0010452043497934937, 0.0010452043497934937, 0.0010452043497934937, 0.0010452043497934937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010452043497934937

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44275379
Iteration 2/25 | Loss: 0.00054329
Iteration 3/25 | Loss: 0.00054327
Iteration 4/25 | Loss: 0.00054327
Iteration 5/25 | Loss: 0.00054327
Iteration 6/25 | Loss: 0.00054326
Iteration 7/25 | Loss: 0.00054326
Iteration 8/25 | Loss: 0.00054326
Iteration 9/25 | Loss: 0.00054326
Iteration 10/25 | Loss: 0.00054326
Iteration 11/25 | Loss: 0.00054326
Iteration 12/25 | Loss: 0.00054326
Iteration 13/25 | Loss: 0.00054326
Iteration 14/25 | Loss: 0.00054326
Iteration 15/25 | Loss: 0.00054326
Iteration 16/25 | Loss: 0.00054326
Iteration 17/25 | Loss: 0.00054326
Iteration 18/25 | Loss: 0.00054326
Iteration 19/25 | Loss: 0.00054326
Iteration 20/25 | Loss: 0.00054326
Iteration 21/25 | Loss: 0.00054326
Iteration 22/25 | Loss: 0.00054326
Iteration 23/25 | Loss: 0.00054326
Iteration 24/25 | Loss: 0.00054326
Iteration 25/25 | Loss: 0.00054326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054326
Iteration 2/1000 | Loss: 0.00005696
Iteration 3/1000 | Loss: 0.00004326
Iteration 4/1000 | Loss: 0.00003825
Iteration 5/1000 | Loss: 0.00003577
Iteration 6/1000 | Loss: 0.00003425
Iteration 7/1000 | Loss: 0.00003334
Iteration 8/1000 | Loss: 0.00003262
Iteration 9/1000 | Loss: 0.00003222
Iteration 10/1000 | Loss: 0.00003195
Iteration 11/1000 | Loss: 0.00003170
Iteration 12/1000 | Loss: 0.00003158
Iteration 13/1000 | Loss: 0.00003153
Iteration 14/1000 | Loss: 0.00003149
Iteration 15/1000 | Loss: 0.00003148
Iteration 16/1000 | Loss: 0.00003148
Iteration 17/1000 | Loss: 0.00003148
Iteration 18/1000 | Loss: 0.00003148
Iteration 19/1000 | Loss: 0.00003148
Iteration 20/1000 | Loss: 0.00003147
Iteration 21/1000 | Loss: 0.00003147
Iteration 22/1000 | Loss: 0.00003147
Iteration 23/1000 | Loss: 0.00003146
Iteration 24/1000 | Loss: 0.00003146
Iteration 25/1000 | Loss: 0.00003145
Iteration 26/1000 | Loss: 0.00003145
Iteration 27/1000 | Loss: 0.00003144
Iteration 28/1000 | Loss: 0.00003143
Iteration 29/1000 | Loss: 0.00003143
Iteration 30/1000 | Loss: 0.00003143
Iteration 31/1000 | Loss: 0.00003143
Iteration 32/1000 | Loss: 0.00003143
Iteration 33/1000 | Loss: 0.00003142
Iteration 34/1000 | Loss: 0.00003142
Iteration 35/1000 | Loss: 0.00003142
Iteration 36/1000 | Loss: 0.00003141
Iteration 37/1000 | Loss: 0.00003141
Iteration 38/1000 | Loss: 0.00003141
Iteration 39/1000 | Loss: 0.00003140
Iteration 40/1000 | Loss: 0.00003140
Iteration 41/1000 | Loss: 0.00003140
Iteration 42/1000 | Loss: 0.00003140
Iteration 43/1000 | Loss: 0.00003139
Iteration 44/1000 | Loss: 0.00003139
Iteration 45/1000 | Loss: 0.00003139
Iteration 46/1000 | Loss: 0.00003139
Iteration 47/1000 | Loss: 0.00003139
Iteration 48/1000 | Loss: 0.00003139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 48. Stopping optimization.
Last 5 losses: [3.139473119517788e-05, 3.139473119517788e-05, 3.139473119517788e-05, 3.139473119517788e-05, 3.139473119517788e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.139473119517788e-05

Optimization complete. Final v2v error: 4.7876296043396 mm

Highest mean error: 6.393525123596191 mm for frame 69

Lowest mean error: 4.19930362701416 mm for frame 140

Saving results

Total time: 50.405513286590576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01125067
Iteration 2/25 | Loss: 0.00199047
Iteration 3/25 | Loss: 0.00173841
Iteration 4/25 | Loss: 0.00106382
Iteration 5/25 | Loss: 0.00100732
Iteration 6/25 | Loss: 0.00099740
Iteration 7/25 | Loss: 0.00099283
Iteration 8/25 | Loss: 0.00099945
Iteration 9/25 | Loss: 0.00099220
Iteration 10/25 | Loss: 0.00098989
Iteration 11/25 | Loss: 0.00098874
Iteration 12/25 | Loss: 0.00098837
Iteration 13/25 | Loss: 0.00098817
Iteration 14/25 | Loss: 0.00098816
Iteration 15/25 | Loss: 0.00098816
Iteration 16/25 | Loss: 0.00098816
Iteration 17/25 | Loss: 0.00098815
Iteration 18/25 | Loss: 0.00098815
Iteration 19/25 | Loss: 0.00098815
Iteration 20/25 | Loss: 0.00098815
Iteration 21/25 | Loss: 0.00098815
Iteration 22/25 | Loss: 0.00098815
Iteration 23/25 | Loss: 0.00098815
Iteration 24/25 | Loss: 0.00098815
Iteration 25/25 | Loss: 0.00098815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44039619
Iteration 2/25 | Loss: 0.00048699
Iteration 3/25 | Loss: 0.00048699
Iteration 4/25 | Loss: 0.00048699
Iteration 5/25 | Loss: 0.00048699
Iteration 6/25 | Loss: 0.00048699
Iteration 7/25 | Loss: 0.00048699
Iteration 8/25 | Loss: 0.00048698
Iteration 9/25 | Loss: 0.00048698
Iteration 10/25 | Loss: 0.00048698
Iteration 11/25 | Loss: 0.00048698
Iteration 12/25 | Loss: 0.00048698
Iteration 13/25 | Loss: 0.00048698
Iteration 14/25 | Loss: 0.00048698
Iteration 15/25 | Loss: 0.00048698
Iteration 16/25 | Loss: 0.00048698
Iteration 17/25 | Loss: 0.00048698
Iteration 18/25 | Loss: 0.00048698
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004869843542110175, 0.0004869843542110175, 0.0004869843542110175, 0.0004869843542110175, 0.0004869843542110175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004869843542110175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048698
Iteration 2/1000 | Loss: 0.00004713
Iteration 3/1000 | Loss: 0.00003253
Iteration 4/1000 | Loss: 0.00002792
Iteration 5/1000 | Loss: 0.00017986
Iteration 6/1000 | Loss: 0.00038827
Iteration 7/1000 | Loss: 0.00002888
Iteration 8/1000 | Loss: 0.00002435
Iteration 9/1000 | Loss: 0.00002325
Iteration 10/1000 | Loss: 0.00002275
Iteration 11/1000 | Loss: 0.00014770
Iteration 12/1000 | Loss: 0.00002261
Iteration 13/1000 | Loss: 0.00002231
Iteration 14/1000 | Loss: 0.00002204
Iteration 15/1000 | Loss: 0.00002187
Iteration 16/1000 | Loss: 0.00002172
Iteration 17/1000 | Loss: 0.00002168
Iteration 18/1000 | Loss: 0.00002161
Iteration 19/1000 | Loss: 0.00002159
Iteration 20/1000 | Loss: 0.00002159
Iteration 21/1000 | Loss: 0.00002158
Iteration 22/1000 | Loss: 0.00002158
Iteration 23/1000 | Loss: 0.00002158
Iteration 24/1000 | Loss: 0.00002158
Iteration 25/1000 | Loss: 0.00002158
Iteration 26/1000 | Loss: 0.00002158
Iteration 27/1000 | Loss: 0.00002158
Iteration 28/1000 | Loss: 0.00002157
Iteration 29/1000 | Loss: 0.00002157
Iteration 30/1000 | Loss: 0.00002157
Iteration 31/1000 | Loss: 0.00002157
Iteration 32/1000 | Loss: 0.00002157
Iteration 33/1000 | Loss: 0.00002157
Iteration 34/1000 | Loss: 0.00002157
Iteration 35/1000 | Loss: 0.00002156
Iteration 36/1000 | Loss: 0.00002156
Iteration 37/1000 | Loss: 0.00002156
Iteration 38/1000 | Loss: 0.00002155
Iteration 39/1000 | Loss: 0.00002155
Iteration 40/1000 | Loss: 0.00002155
Iteration 41/1000 | Loss: 0.00002155
Iteration 42/1000 | Loss: 0.00002155
Iteration 43/1000 | Loss: 0.00002155
Iteration 44/1000 | Loss: 0.00002154
Iteration 45/1000 | Loss: 0.00002154
Iteration 46/1000 | Loss: 0.00002154
Iteration 47/1000 | Loss: 0.00002153
Iteration 48/1000 | Loss: 0.00002153
Iteration 49/1000 | Loss: 0.00002153
Iteration 50/1000 | Loss: 0.00002153
Iteration 51/1000 | Loss: 0.00002153
Iteration 52/1000 | Loss: 0.00002153
Iteration 53/1000 | Loss: 0.00002152
Iteration 54/1000 | Loss: 0.00002152
Iteration 55/1000 | Loss: 0.00002152
Iteration 56/1000 | Loss: 0.00002152
Iteration 57/1000 | Loss: 0.00002152
Iteration 58/1000 | Loss: 0.00002152
Iteration 59/1000 | Loss: 0.00002152
Iteration 60/1000 | Loss: 0.00002152
Iteration 61/1000 | Loss: 0.00002152
Iteration 62/1000 | Loss: 0.00002152
Iteration 63/1000 | Loss: 0.00002152
Iteration 64/1000 | Loss: 0.00002152
Iteration 65/1000 | Loss: 0.00002151
Iteration 66/1000 | Loss: 0.00002151
Iteration 67/1000 | Loss: 0.00002151
Iteration 68/1000 | Loss: 0.00002150
Iteration 69/1000 | Loss: 0.00002150
Iteration 70/1000 | Loss: 0.00002150
Iteration 71/1000 | Loss: 0.00002150
Iteration 72/1000 | Loss: 0.00002150
Iteration 73/1000 | Loss: 0.00002150
Iteration 74/1000 | Loss: 0.00002150
Iteration 75/1000 | Loss: 0.00002150
Iteration 76/1000 | Loss: 0.00002150
Iteration 77/1000 | Loss: 0.00002150
Iteration 78/1000 | Loss: 0.00002150
Iteration 79/1000 | Loss: 0.00002150
Iteration 80/1000 | Loss: 0.00002150
Iteration 81/1000 | Loss: 0.00002150
Iteration 82/1000 | Loss: 0.00002150
Iteration 83/1000 | Loss: 0.00002150
Iteration 84/1000 | Loss: 0.00002150
Iteration 85/1000 | Loss: 0.00002149
Iteration 86/1000 | Loss: 0.00002149
Iteration 87/1000 | Loss: 0.00002149
Iteration 88/1000 | Loss: 0.00002149
Iteration 89/1000 | Loss: 0.00002149
Iteration 90/1000 | Loss: 0.00002149
Iteration 91/1000 | Loss: 0.00002148
Iteration 92/1000 | Loss: 0.00002148
Iteration 93/1000 | Loss: 0.00002148
Iteration 94/1000 | Loss: 0.00016160
Iteration 95/1000 | Loss: 0.00003655
Iteration 96/1000 | Loss: 0.00003768
Iteration 97/1000 | Loss: 0.00002153
Iteration 98/1000 | Loss: 0.00002152
Iteration 99/1000 | Loss: 0.00002150
Iteration 100/1000 | Loss: 0.00002149
Iteration 101/1000 | Loss: 0.00002148
Iteration 102/1000 | Loss: 0.00002148
Iteration 103/1000 | Loss: 0.00002147
Iteration 104/1000 | Loss: 0.00002147
Iteration 105/1000 | Loss: 0.00002142
Iteration 106/1000 | Loss: 0.00002141
Iteration 107/1000 | Loss: 0.00002141
Iteration 108/1000 | Loss: 0.00002141
Iteration 109/1000 | Loss: 0.00002141
Iteration 110/1000 | Loss: 0.00002140
Iteration 111/1000 | Loss: 0.00002140
Iteration 112/1000 | Loss: 0.00002140
Iteration 113/1000 | Loss: 0.00002139
Iteration 114/1000 | Loss: 0.00002139
Iteration 115/1000 | Loss: 0.00002139
Iteration 116/1000 | Loss: 0.00002139
Iteration 117/1000 | Loss: 0.00002139
Iteration 118/1000 | Loss: 0.00002139
Iteration 119/1000 | Loss: 0.00002138
Iteration 120/1000 | Loss: 0.00002138
Iteration 121/1000 | Loss: 0.00002138
Iteration 122/1000 | Loss: 0.00002138
Iteration 123/1000 | Loss: 0.00002138
Iteration 124/1000 | Loss: 0.00002138
Iteration 125/1000 | Loss: 0.00002137
Iteration 126/1000 | Loss: 0.00002137
Iteration 127/1000 | Loss: 0.00002137
Iteration 128/1000 | Loss: 0.00002137
Iteration 129/1000 | Loss: 0.00002137
Iteration 130/1000 | Loss: 0.00002137
Iteration 131/1000 | Loss: 0.00002137
Iteration 132/1000 | Loss: 0.00002137
Iteration 133/1000 | Loss: 0.00002137
Iteration 134/1000 | Loss: 0.00002137
Iteration 135/1000 | Loss: 0.00002137
Iteration 136/1000 | Loss: 0.00002137
Iteration 137/1000 | Loss: 0.00002137
Iteration 138/1000 | Loss: 0.00002137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.1371593902586028e-05, 2.1371593902586028e-05, 2.1371593902586028e-05, 2.1371593902586028e-05, 2.1371593902586028e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1371593902586028e-05

Optimization complete. Final v2v error: 3.996771812438965 mm

Highest mean error: 4.288935661315918 mm for frame 93

Lowest mean error: 3.594325065612793 mm for frame 4

Saving results

Total time: 75.03120732307434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01200036
Iteration 2/25 | Loss: 0.01200036
Iteration 3/25 | Loss: 0.01200036
Iteration 4/25 | Loss: 0.01200036
Iteration 5/25 | Loss: 0.01200035
Iteration 6/25 | Loss: 0.01200035
Iteration 7/25 | Loss: 0.01200035
Iteration 8/25 | Loss: 0.01200035
Iteration 9/25 | Loss: 0.01200035
Iteration 10/25 | Loss: 0.01200035
Iteration 11/25 | Loss: 0.01200035
Iteration 12/25 | Loss: 0.01200035
Iteration 13/25 | Loss: 0.01200035
Iteration 14/25 | Loss: 0.01200034
Iteration 15/25 | Loss: 0.01200034
Iteration 16/25 | Loss: 0.01200034
Iteration 17/25 | Loss: 0.01200034
Iteration 18/25 | Loss: 0.01200034
Iteration 19/25 | Loss: 0.01200034
Iteration 20/25 | Loss: 0.01200034
Iteration 21/25 | Loss: 0.01200034
Iteration 22/25 | Loss: 0.01200034
Iteration 23/25 | Loss: 0.01200034
Iteration 24/25 | Loss: 0.01200034
Iteration 25/25 | Loss: 0.01200033

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.58315945
Iteration 2/25 | Loss: 0.17718743
Iteration 3/25 | Loss: 0.17532319
Iteration 4/25 | Loss: 0.17412151
Iteration 5/25 | Loss: 0.17367248
Iteration 6/25 | Loss: 0.17367248
Iteration 7/25 | Loss: 0.17367245
Iteration 8/25 | Loss: 0.17367244
Iteration 9/25 | Loss: 0.17367244
Iteration 10/25 | Loss: 0.17367244
Iteration 11/25 | Loss: 0.17367244
Iteration 12/25 | Loss: 0.17367244
Iteration 13/25 | Loss: 0.17367244
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.17367243766784668, 0.17367243766784668, 0.17367243766784668, 0.17367243766784668, 0.17367243766784668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17367243766784668

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17367244
Iteration 2/1000 | Loss: 0.00337556
Iteration 3/1000 | Loss: 0.00121030
Iteration 4/1000 | Loss: 0.00041255
Iteration 5/1000 | Loss: 0.00040199
Iteration 6/1000 | Loss: 0.00019601
Iteration 7/1000 | Loss: 0.00015136
Iteration 8/1000 | Loss: 0.00014579
Iteration 9/1000 | Loss: 0.00010797
Iteration 10/1000 | Loss: 0.00008883
Iteration 11/1000 | Loss: 0.00011113
Iteration 12/1000 | Loss: 0.00008009
Iteration 13/1000 | Loss: 0.00008334
Iteration 14/1000 | Loss: 0.00005227
Iteration 15/1000 | Loss: 0.00005332
Iteration 16/1000 | Loss: 0.00010129
Iteration 17/1000 | Loss: 0.00004119
Iteration 18/1000 | Loss: 0.00005476
Iteration 19/1000 | Loss: 0.00004466
Iteration 20/1000 | Loss: 0.00005413
Iteration 21/1000 | Loss: 0.00005681
Iteration 22/1000 | Loss: 0.00005724
Iteration 23/1000 | Loss: 0.00004118
Iteration 24/1000 | Loss: 0.00005202
Iteration 25/1000 | Loss: 0.00004024
Iteration 26/1000 | Loss: 0.00003707
Iteration 27/1000 | Loss: 0.00003543
Iteration 28/1000 | Loss: 0.00003618
Iteration 29/1000 | Loss: 0.00004384
Iteration 30/1000 | Loss: 0.00003569
Iteration 31/1000 | Loss: 0.00003504
Iteration 32/1000 | Loss: 0.00003346
Iteration 33/1000 | Loss: 0.00003766
Iteration 34/1000 | Loss: 0.00003459
Iteration 35/1000 | Loss: 0.00003264
Iteration 36/1000 | Loss: 0.00003245
Iteration 37/1000 | Loss: 0.00004737
Iteration 38/1000 | Loss: 0.00011759
Iteration 39/1000 | Loss: 0.00004537
Iteration 40/1000 | Loss: 0.00003229
Iteration 41/1000 | Loss: 0.00003229
Iteration 42/1000 | Loss: 0.00003229
Iteration 43/1000 | Loss: 0.00003229
Iteration 44/1000 | Loss: 0.00003229
Iteration 45/1000 | Loss: 0.00003229
Iteration 46/1000 | Loss: 0.00003229
Iteration 47/1000 | Loss: 0.00003229
Iteration 48/1000 | Loss: 0.00003228
Iteration 49/1000 | Loss: 0.00003228
Iteration 50/1000 | Loss: 0.00003228
Iteration 51/1000 | Loss: 0.00003228
Iteration 52/1000 | Loss: 0.00003228
Iteration 53/1000 | Loss: 0.00003228
Iteration 54/1000 | Loss: 0.00004786
Iteration 55/1000 | Loss: 0.00003223
Iteration 56/1000 | Loss: 0.00003222
Iteration 57/1000 | Loss: 0.00003221
Iteration 58/1000 | Loss: 0.00003220
Iteration 59/1000 | Loss: 0.00003827
Iteration 60/1000 | Loss: 0.00004290
Iteration 61/1000 | Loss: 0.00004716
Iteration 62/1000 | Loss: 0.00003419
Iteration 63/1000 | Loss: 0.00004201
Iteration 64/1000 | Loss: 0.00003200
Iteration 65/1000 | Loss: 0.00003200
Iteration 66/1000 | Loss: 0.00003200
Iteration 67/1000 | Loss: 0.00003200
Iteration 68/1000 | Loss: 0.00003200
Iteration 69/1000 | Loss: 0.00003200
Iteration 70/1000 | Loss: 0.00003199
Iteration 71/1000 | Loss: 0.00003199
Iteration 72/1000 | Loss: 0.00003199
Iteration 73/1000 | Loss: 0.00003199
Iteration 74/1000 | Loss: 0.00003199
Iteration 75/1000 | Loss: 0.00003199
Iteration 76/1000 | Loss: 0.00003199
Iteration 77/1000 | Loss: 0.00003199
Iteration 78/1000 | Loss: 0.00003199
Iteration 79/1000 | Loss: 0.00003198
Iteration 80/1000 | Loss: 0.00003198
Iteration 81/1000 | Loss: 0.00003197
Iteration 82/1000 | Loss: 0.00003197
Iteration 83/1000 | Loss: 0.00003197
Iteration 84/1000 | Loss: 0.00003197
Iteration 85/1000 | Loss: 0.00003197
Iteration 86/1000 | Loss: 0.00003197
Iteration 87/1000 | Loss: 0.00003197
Iteration 88/1000 | Loss: 0.00003197
Iteration 89/1000 | Loss: 0.00003197
Iteration 90/1000 | Loss: 0.00003571
Iteration 91/1000 | Loss: 0.00003199
Iteration 92/1000 | Loss: 0.00003199
Iteration 93/1000 | Loss: 0.00003199
Iteration 94/1000 | Loss: 0.00003199
Iteration 95/1000 | Loss: 0.00003199
Iteration 96/1000 | Loss: 0.00003199
Iteration 97/1000 | Loss: 0.00003198
Iteration 98/1000 | Loss: 0.00003198
Iteration 99/1000 | Loss: 0.00003198
Iteration 100/1000 | Loss: 0.00003198
Iteration 101/1000 | Loss: 0.00003198
Iteration 102/1000 | Loss: 0.00003198
Iteration 103/1000 | Loss: 0.00003198
Iteration 104/1000 | Loss: 0.00003198
Iteration 105/1000 | Loss: 0.00003197
Iteration 106/1000 | Loss: 0.00003197
Iteration 107/1000 | Loss: 0.00003197
Iteration 108/1000 | Loss: 0.00003197
Iteration 109/1000 | Loss: 0.00003197
Iteration 110/1000 | Loss: 0.00003295
Iteration 111/1000 | Loss: 0.00003194
Iteration 112/1000 | Loss: 0.00003194
Iteration 113/1000 | Loss: 0.00003194
Iteration 114/1000 | Loss: 0.00003194
Iteration 115/1000 | Loss: 0.00003194
Iteration 116/1000 | Loss: 0.00003194
Iteration 117/1000 | Loss: 0.00003193
Iteration 118/1000 | Loss: 0.00003193
Iteration 119/1000 | Loss: 0.00003193
Iteration 120/1000 | Loss: 0.00003193
Iteration 121/1000 | Loss: 0.00003193
Iteration 122/1000 | Loss: 0.00003193
Iteration 123/1000 | Loss: 0.00003193
Iteration 124/1000 | Loss: 0.00003193
Iteration 125/1000 | Loss: 0.00003193
Iteration 126/1000 | Loss: 0.00003193
Iteration 127/1000 | Loss: 0.00003193
Iteration 128/1000 | Loss: 0.00003193
Iteration 129/1000 | Loss: 0.00003193
Iteration 130/1000 | Loss: 0.00003193
Iteration 131/1000 | Loss: 0.00003193
Iteration 132/1000 | Loss: 0.00003193
Iteration 133/1000 | Loss: 0.00003193
Iteration 134/1000 | Loss: 0.00003193
Iteration 135/1000 | Loss: 0.00003193
Iteration 136/1000 | Loss: 0.00003193
Iteration 137/1000 | Loss: 0.00003193
Iteration 138/1000 | Loss: 0.00003193
Iteration 139/1000 | Loss: 0.00003193
Iteration 140/1000 | Loss: 0.00003193
Iteration 141/1000 | Loss: 0.00003193
Iteration 142/1000 | Loss: 0.00003193
Iteration 143/1000 | Loss: 0.00003193
Iteration 144/1000 | Loss: 0.00003193
Iteration 145/1000 | Loss: 0.00003193
Iteration 146/1000 | Loss: 0.00003193
Iteration 147/1000 | Loss: 0.00003193
Iteration 148/1000 | Loss: 0.00003193
Iteration 149/1000 | Loss: 0.00003193
Iteration 150/1000 | Loss: 0.00003193
Iteration 151/1000 | Loss: 0.00003193
Iteration 152/1000 | Loss: 0.00003193
Iteration 153/1000 | Loss: 0.00003193
Iteration 154/1000 | Loss: 0.00003193
Iteration 155/1000 | Loss: 0.00003193
Iteration 156/1000 | Loss: 0.00003193
Iteration 157/1000 | Loss: 0.00003193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [3.193395968992263e-05, 3.193395968992263e-05, 3.193395968992263e-05, 3.193395968992263e-05, 3.193395968992263e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.193395968992263e-05

Optimization complete. Final v2v error: 4.319809436798096 mm

Highest mean error: 20.216516494750977 mm for frame 150

Lowest mean error: 3.8056247234344482 mm for frame 186

Saving results

Total time: 120.88927435874939
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043783
Iteration 2/25 | Loss: 0.00222388
Iteration 3/25 | Loss: 0.00156555
Iteration 4/25 | Loss: 0.00145689
Iteration 5/25 | Loss: 0.00136951
Iteration 6/25 | Loss: 0.00135773
Iteration 7/25 | Loss: 0.00146466
Iteration 8/25 | Loss: 0.00120438
Iteration 9/25 | Loss: 0.00107842
Iteration 10/25 | Loss: 0.00106211
Iteration 11/25 | Loss: 0.00101175
Iteration 12/25 | Loss: 0.00099497
Iteration 13/25 | Loss: 0.00099504
Iteration 14/25 | Loss: 0.00100249
Iteration 15/25 | Loss: 0.00098966
Iteration 16/25 | Loss: 0.00098535
Iteration 17/25 | Loss: 0.00098683
Iteration 18/25 | Loss: 0.00097241
Iteration 19/25 | Loss: 0.00096658
Iteration 20/25 | Loss: 0.00096371
Iteration 21/25 | Loss: 0.00096085
Iteration 22/25 | Loss: 0.00096389
Iteration 23/25 | Loss: 0.00096871
Iteration 24/25 | Loss: 0.00095855
Iteration 25/25 | Loss: 0.00095219

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44771898
Iteration 2/25 | Loss: 0.00060658
Iteration 3/25 | Loss: 0.00060658
Iteration 4/25 | Loss: 0.00060658
Iteration 5/25 | Loss: 0.00060658
Iteration 6/25 | Loss: 0.00060658
Iteration 7/25 | Loss: 0.00060658
Iteration 8/25 | Loss: 0.00060658
Iteration 9/25 | Loss: 0.00060658
Iteration 10/25 | Loss: 0.00060658
Iteration 11/25 | Loss: 0.00060658
Iteration 12/25 | Loss: 0.00060658
Iteration 13/25 | Loss: 0.00060658
Iteration 14/25 | Loss: 0.00060658
Iteration 15/25 | Loss: 0.00060658
Iteration 16/25 | Loss: 0.00060658
Iteration 17/25 | Loss: 0.00060658
Iteration 18/25 | Loss: 0.00060658
Iteration 19/25 | Loss: 0.00060658
Iteration 20/25 | Loss: 0.00060658
Iteration 21/25 | Loss: 0.00060658
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006065774359740317, 0.0006065774359740317, 0.0006065774359740317, 0.0006065774359740317, 0.0006065774359740317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006065774359740317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060658
Iteration 2/1000 | Loss: 0.00006178
Iteration 3/1000 | Loss: 0.00004171
Iteration 4/1000 | Loss: 0.00007726
Iteration 5/1000 | Loss: 0.00006062
Iteration 6/1000 | Loss: 0.00007280
Iteration 7/1000 | Loss: 0.00004354
Iteration 8/1000 | Loss: 0.00003123
Iteration 9/1000 | Loss: 0.00005283
Iteration 10/1000 | Loss: 0.00003674
Iteration 11/1000 | Loss: 0.00005373
Iteration 12/1000 | Loss: 0.00006421
Iteration 13/1000 | Loss: 0.00003773
Iteration 14/1000 | Loss: 0.00003521
Iteration 15/1000 | Loss: 0.00003100
Iteration 16/1000 | Loss: 0.00002907
Iteration 17/1000 | Loss: 0.00002802
Iteration 18/1000 | Loss: 0.00002766
Iteration 19/1000 | Loss: 0.00002740
Iteration 20/1000 | Loss: 0.00002710
Iteration 21/1000 | Loss: 0.00002697
Iteration 22/1000 | Loss: 0.00002690
Iteration 23/1000 | Loss: 0.00002690
Iteration 24/1000 | Loss: 0.00002687
Iteration 25/1000 | Loss: 0.00002687
Iteration 26/1000 | Loss: 0.00002673
Iteration 27/1000 | Loss: 0.00002672
Iteration 28/1000 | Loss: 0.00002662
Iteration 29/1000 | Loss: 0.00002660
Iteration 30/1000 | Loss: 0.00002658
Iteration 31/1000 | Loss: 0.00002658
Iteration 32/1000 | Loss: 0.00002658
Iteration 33/1000 | Loss: 0.00002658
Iteration 34/1000 | Loss: 0.00002658
Iteration 35/1000 | Loss: 0.00002658
Iteration 36/1000 | Loss: 0.00002657
Iteration 37/1000 | Loss: 0.00002656
Iteration 38/1000 | Loss: 0.00002656
Iteration 39/1000 | Loss: 0.00002654
Iteration 40/1000 | Loss: 0.00002654
Iteration 41/1000 | Loss: 0.00002654
Iteration 42/1000 | Loss: 0.00002654
Iteration 43/1000 | Loss: 0.00002654
Iteration 44/1000 | Loss: 0.00002654
Iteration 45/1000 | Loss: 0.00002654
Iteration 46/1000 | Loss: 0.00002654
Iteration 47/1000 | Loss: 0.00002654
Iteration 48/1000 | Loss: 0.00002654
Iteration 49/1000 | Loss: 0.00002653
Iteration 50/1000 | Loss: 0.00002653
Iteration 51/1000 | Loss: 0.00002653
Iteration 52/1000 | Loss: 0.00002653
Iteration 53/1000 | Loss: 0.00002653
Iteration 54/1000 | Loss: 0.00002652
Iteration 55/1000 | Loss: 0.00002652
Iteration 56/1000 | Loss: 0.00002652
Iteration 57/1000 | Loss: 0.00002652
Iteration 58/1000 | Loss: 0.00002651
Iteration 59/1000 | Loss: 0.00002650
Iteration 60/1000 | Loss: 0.00002650
Iteration 61/1000 | Loss: 0.00002650
Iteration 62/1000 | Loss: 0.00002649
Iteration 63/1000 | Loss: 0.00002649
Iteration 64/1000 | Loss: 0.00002649
Iteration 65/1000 | Loss: 0.00002648
Iteration 66/1000 | Loss: 0.00002648
Iteration 67/1000 | Loss: 0.00002647
Iteration 68/1000 | Loss: 0.00002647
Iteration 69/1000 | Loss: 0.00002647
Iteration 70/1000 | Loss: 0.00002647
Iteration 71/1000 | Loss: 0.00002647
Iteration 72/1000 | Loss: 0.00002647
Iteration 73/1000 | Loss: 0.00002647
Iteration 74/1000 | Loss: 0.00002647
Iteration 75/1000 | Loss: 0.00002647
Iteration 76/1000 | Loss: 0.00002647
Iteration 77/1000 | Loss: 0.00002647
Iteration 78/1000 | Loss: 0.00002646
Iteration 79/1000 | Loss: 0.00002646
Iteration 80/1000 | Loss: 0.00002646
Iteration 81/1000 | Loss: 0.00002646
Iteration 82/1000 | Loss: 0.00002645
Iteration 83/1000 | Loss: 0.00002645
Iteration 84/1000 | Loss: 0.00002645
Iteration 85/1000 | Loss: 0.00002645
Iteration 86/1000 | Loss: 0.00002645
Iteration 87/1000 | Loss: 0.00002645
Iteration 88/1000 | Loss: 0.00002645
Iteration 89/1000 | Loss: 0.00002645
Iteration 90/1000 | Loss: 0.00002645
Iteration 91/1000 | Loss: 0.00002645
Iteration 92/1000 | Loss: 0.00002644
Iteration 93/1000 | Loss: 0.00002644
Iteration 94/1000 | Loss: 0.00002644
Iteration 95/1000 | Loss: 0.00002644
Iteration 96/1000 | Loss: 0.00002644
Iteration 97/1000 | Loss: 0.00002644
Iteration 98/1000 | Loss: 0.00002644
Iteration 99/1000 | Loss: 0.00002644
Iteration 100/1000 | Loss: 0.00002644
Iteration 101/1000 | Loss: 0.00002644
Iteration 102/1000 | Loss: 0.00002644
Iteration 103/1000 | Loss: 0.00002644
Iteration 104/1000 | Loss: 0.00002644
Iteration 105/1000 | Loss: 0.00002644
Iteration 106/1000 | Loss: 0.00002644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.6440186047693714e-05, 2.6440186047693714e-05, 2.6440186047693714e-05, 2.6440186047693714e-05, 2.6440186047693714e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6440186047693714e-05

Optimization complete. Final v2v error: 4.410141944885254 mm

Highest mean error: 10.536381721496582 mm for frame 82

Lowest mean error: 3.999321222305298 mm for frame 18

Saving results

Total time: 105.6640772819519
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439853
Iteration 2/25 | Loss: 0.00110740
Iteration 3/25 | Loss: 0.00101098
Iteration 4/25 | Loss: 0.00098927
Iteration 5/25 | Loss: 0.00097863
Iteration 6/25 | Loss: 0.00097642
Iteration 7/25 | Loss: 0.00097568
Iteration 8/25 | Loss: 0.00097568
Iteration 9/25 | Loss: 0.00097568
Iteration 10/25 | Loss: 0.00097568
Iteration 11/25 | Loss: 0.00097568
Iteration 12/25 | Loss: 0.00097568
Iteration 13/25 | Loss: 0.00097568
Iteration 14/25 | Loss: 0.00097568
Iteration 15/25 | Loss: 0.00097568
Iteration 16/25 | Loss: 0.00097568
Iteration 17/25 | Loss: 0.00097568
Iteration 18/25 | Loss: 0.00097568
Iteration 19/25 | Loss: 0.00097568
Iteration 20/25 | Loss: 0.00097568
Iteration 21/25 | Loss: 0.00097568
Iteration 22/25 | Loss: 0.00097568
Iteration 23/25 | Loss: 0.00097568
Iteration 24/25 | Loss: 0.00097568
Iteration 25/25 | Loss: 0.00097568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48221219
Iteration 2/25 | Loss: 0.00054733
Iteration 3/25 | Loss: 0.00054733
Iteration 4/25 | Loss: 0.00054733
Iteration 5/25 | Loss: 0.00054733
Iteration 6/25 | Loss: 0.00054732
Iteration 7/25 | Loss: 0.00054732
Iteration 8/25 | Loss: 0.00054732
Iteration 9/25 | Loss: 0.00054732
Iteration 10/25 | Loss: 0.00054732
Iteration 11/25 | Loss: 0.00054732
Iteration 12/25 | Loss: 0.00054732
Iteration 13/25 | Loss: 0.00054732
Iteration 14/25 | Loss: 0.00054732
Iteration 15/25 | Loss: 0.00054732
Iteration 16/25 | Loss: 0.00054732
Iteration 17/25 | Loss: 0.00054732
Iteration 18/25 | Loss: 0.00054732
Iteration 19/25 | Loss: 0.00054732
Iteration 20/25 | Loss: 0.00054732
Iteration 21/25 | Loss: 0.00054732
Iteration 22/25 | Loss: 0.00054732
Iteration 23/25 | Loss: 0.00054732
Iteration 24/25 | Loss: 0.00054732
Iteration 25/25 | Loss: 0.00054732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054732
Iteration 2/1000 | Loss: 0.00003822
Iteration 3/1000 | Loss: 0.00002849
Iteration 4/1000 | Loss: 0.00002520
Iteration 5/1000 | Loss: 0.00002362
Iteration 6/1000 | Loss: 0.00002303
Iteration 7/1000 | Loss: 0.00002260
Iteration 8/1000 | Loss: 0.00002244
Iteration 9/1000 | Loss: 0.00002242
Iteration 10/1000 | Loss: 0.00002235
Iteration 11/1000 | Loss: 0.00002233
Iteration 12/1000 | Loss: 0.00002223
Iteration 13/1000 | Loss: 0.00002215
Iteration 14/1000 | Loss: 0.00002214
Iteration 15/1000 | Loss: 0.00002213
Iteration 16/1000 | Loss: 0.00002213
Iteration 17/1000 | Loss: 0.00002213
Iteration 18/1000 | Loss: 0.00002212
Iteration 19/1000 | Loss: 0.00002212
Iteration 20/1000 | Loss: 0.00002212
Iteration 21/1000 | Loss: 0.00002211
Iteration 22/1000 | Loss: 0.00002204
Iteration 23/1000 | Loss: 0.00002202
Iteration 24/1000 | Loss: 0.00002202
Iteration 25/1000 | Loss: 0.00002202
Iteration 26/1000 | Loss: 0.00002201
Iteration 27/1000 | Loss: 0.00002201
Iteration 28/1000 | Loss: 0.00002201
Iteration 29/1000 | Loss: 0.00002201
Iteration 30/1000 | Loss: 0.00002201
Iteration 31/1000 | Loss: 0.00002201
Iteration 32/1000 | Loss: 0.00002201
Iteration 33/1000 | Loss: 0.00002201
Iteration 34/1000 | Loss: 0.00002201
Iteration 35/1000 | Loss: 0.00002201
Iteration 36/1000 | Loss: 0.00002200
Iteration 37/1000 | Loss: 0.00002200
Iteration 38/1000 | Loss: 0.00002200
Iteration 39/1000 | Loss: 0.00002200
Iteration 40/1000 | Loss: 0.00002200
Iteration 41/1000 | Loss: 0.00002200
Iteration 42/1000 | Loss: 0.00002200
Iteration 43/1000 | Loss: 0.00002200
Iteration 44/1000 | Loss: 0.00002200
Iteration 45/1000 | Loss: 0.00002200
Iteration 46/1000 | Loss: 0.00002200
Iteration 47/1000 | Loss: 0.00002200
Iteration 48/1000 | Loss: 0.00002200
Iteration 49/1000 | Loss: 0.00002199
Iteration 50/1000 | Loss: 0.00002199
Iteration 51/1000 | Loss: 0.00002199
Iteration 52/1000 | Loss: 0.00002199
Iteration 53/1000 | Loss: 0.00002199
Iteration 54/1000 | Loss: 0.00002199
Iteration 55/1000 | Loss: 0.00002199
Iteration 56/1000 | Loss: 0.00002199
Iteration 57/1000 | Loss: 0.00002199
Iteration 58/1000 | Loss: 0.00002199
Iteration 59/1000 | Loss: 0.00002199
Iteration 60/1000 | Loss: 0.00002199
Iteration 61/1000 | Loss: 0.00002199
Iteration 62/1000 | Loss: 0.00002199
Iteration 63/1000 | Loss: 0.00002199
Iteration 64/1000 | Loss: 0.00002198
Iteration 65/1000 | Loss: 0.00002198
Iteration 66/1000 | Loss: 0.00002198
Iteration 67/1000 | Loss: 0.00002198
Iteration 68/1000 | Loss: 0.00002198
Iteration 69/1000 | Loss: 0.00002198
Iteration 70/1000 | Loss: 0.00002198
Iteration 71/1000 | Loss: 0.00002198
Iteration 72/1000 | Loss: 0.00002198
Iteration 73/1000 | Loss: 0.00002198
Iteration 74/1000 | Loss: 0.00002198
Iteration 75/1000 | Loss: 0.00002198
Iteration 76/1000 | Loss: 0.00002198
Iteration 77/1000 | Loss: 0.00002198
Iteration 78/1000 | Loss: 0.00002197
Iteration 79/1000 | Loss: 0.00002197
Iteration 80/1000 | Loss: 0.00002197
Iteration 81/1000 | Loss: 0.00002197
Iteration 82/1000 | Loss: 0.00002197
Iteration 83/1000 | Loss: 0.00002197
Iteration 84/1000 | Loss: 0.00002197
Iteration 85/1000 | Loss: 0.00002197
Iteration 86/1000 | Loss: 0.00002197
Iteration 87/1000 | Loss: 0.00002197
Iteration 88/1000 | Loss: 0.00002197
Iteration 89/1000 | Loss: 0.00002197
Iteration 90/1000 | Loss: 0.00002197
Iteration 91/1000 | Loss: 0.00002197
Iteration 92/1000 | Loss: 0.00002197
Iteration 93/1000 | Loss: 0.00002197
Iteration 94/1000 | Loss: 0.00002197
Iteration 95/1000 | Loss: 0.00002197
Iteration 96/1000 | Loss: 0.00002197
Iteration 97/1000 | Loss: 0.00002197
Iteration 98/1000 | Loss: 0.00002197
Iteration 99/1000 | Loss: 0.00002197
Iteration 100/1000 | Loss: 0.00002197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.1973663024255075e-05, 2.1973663024255075e-05, 2.1973663024255075e-05, 2.1973663024255075e-05, 2.1973663024255075e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1973663024255075e-05

Optimization complete. Final v2v error: 4.038813591003418 mm

Highest mean error: 4.496457099914551 mm for frame 80

Lowest mean error: 3.5102086067199707 mm for frame 1

Saving results

Total time: 34.88261866569519
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01094227
Iteration 2/25 | Loss: 0.01094227
Iteration 3/25 | Loss: 0.01094227
Iteration 4/25 | Loss: 0.01094227
Iteration 5/25 | Loss: 0.01094226
Iteration 6/25 | Loss: 0.01094226
Iteration 7/25 | Loss: 0.01094226
Iteration 8/25 | Loss: 0.01094226
Iteration 9/25 | Loss: 0.01094226
Iteration 10/25 | Loss: 0.01094226
Iteration 11/25 | Loss: 0.01094226
Iteration 12/25 | Loss: 0.01094226
Iteration 13/25 | Loss: 0.01094226
Iteration 14/25 | Loss: 0.01094226
Iteration 15/25 | Loss: 0.01094226
Iteration 16/25 | Loss: 0.01094226
Iteration 17/25 | Loss: 0.01094225
Iteration 18/25 | Loss: 0.01094225
Iteration 19/25 | Loss: 0.01094225
Iteration 20/25 | Loss: 0.01094225
Iteration 21/25 | Loss: 0.01094225
Iteration 22/25 | Loss: 0.01094225
Iteration 23/25 | Loss: 0.01094225
Iteration 24/25 | Loss: 0.01094225
Iteration 25/25 | Loss: 0.01094225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98032475
Iteration 2/25 | Loss: 0.09269331
Iteration 3/25 | Loss: 0.09216382
Iteration 4/25 | Loss: 0.09215956
Iteration 5/25 | Loss: 0.09215954
Iteration 6/25 | Loss: 0.09215954
Iteration 7/25 | Loss: 0.09215954
Iteration 8/25 | Loss: 0.09215954
Iteration 9/25 | Loss: 0.09215954
Iteration 10/25 | Loss: 0.09215954
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.09215953946113586, 0.09215953946113586, 0.09215953946113586, 0.09215953946113586, 0.09215953946113586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09215953946113586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09215954
Iteration 2/1000 | Loss: 0.00647138
Iteration 3/1000 | Loss: 0.00358363
Iteration 4/1000 | Loss: 0.00423536
Iteration 5/1000 | Loss: 0.00234934
Iteration 6/1000 | Loss: 0.00059881
Iteration 7/1000 | Loss: 0.00054782
Iteration 8/1000 | Loss: 0.00027416
Iteration 9/1000 | Loss: 0.00018830
Iteration 10/1000 | Loss: 0.00015084
Iteration 11/1000 | Loss: 0.00028985
Iteration 12/1000 | Loss: 0.00027257
Iteration 13/1000 | Loss: 0.00013509
Iteration 14/1000 | Loss: 0.00027427
Iteration 15/1000 | Loss: 0.00008743
Iteration 16/1000 | Loss: 0.00018986
Iteration 17/1000 | Loss: 0.00017302
Iteration 18/1000 | Loss: 0.00013255
Iteration 19/1000 | Loss: 0.00013259
Iteration 20/1000 | Loss: 0.00060682
Iteration 21/1000 | Loss: 0.00025718
Iteration 22/1000 | Loss: 0.00028859
Iteration 23/1000 | Loss: 0.00030998
Iteration 24/1000 | Loss: 0.00016366
Iteration 25/1000 | Loss: 0.00021121
Iteration 26/1000 | Loss: 0.00039634
Iteration 27/1000 | Loss: 0.00017005
Iteration 28/1000 | Loss: 0.00027365
Iteration 29/1000 | Loss: 0.00009175
Iteration 30/1000 | Loss: 0.00010776
Iteration 31/1000 | Loss: 0.00007201
Iteration 32/1000 | Loss: 0.00007434
Iteration 33/1000 | Loss: 0.00022636
Iteration 34/1000 | Loss: 0.00008652
Iteration 35/1000 | Loss: 0.00035742
Iteration 36/1000 | Loss: 0.00024898
Iteration 37/1000 | Loss: 0.00008433
Iteration 38/1000 | Loss: 0.00012055
Iteration 39/1000 | Loss: 0.00005960
Iteration 40/1000 | Loss: 0.00011390
Iteration 41/1000 | Loss: 0.00006067
Iteration 42/1000 | Loss: 0.00010356
Iteration 43/1000 | Loss: 0.00012696
Iteration 44/1000 | Loss: 0.00005165
Iteration 45/1000 | Loss: 0.00027193
Iteration 46/1000 | Loss: 0.00007650
Iteration 47/1000 | Loss: 0.00012570
Iteration 48/1000 | Loss: 0.00004851
Iteration 49/1000 | Loss: 0.00005846
Iteration 50/1000 | Loss: 0.00004629
Iteration 51/1000 | Loss: 0.00010601
Iteration 52/1000 | Loss: 0.00004409
Iteration 53/1000 | Loss: 0.00022894
Iteration 54/1000 | Loss: 0.00015378
Iteration 55/1000 | Loss: 0.00013676
Iteration 56/1000 | Loss: 0.00004639
Iteration 57/1000 | Loss: 0.00007818
Iteration 58/1000 | Loss: 0.00005435
Iteration 59/1000 | Loss: 0.00005213
Iteration 60/1000 | Loss: 0.00004277
Iteration 61/1000 | Loss: 0.00005200
Iteration 62/1000 | Loss: 0.00005540
Iteration 63/1000 | Loss: 0.00004166
Iteration 64/1000 | Loss: 0.00004998
Iteration 65/1000 | Loss: 0.00021035
Iteration 66/1000 | Loss: 0.00021416
Iteration 67/1000 | Loss: 0.00006441
Iteration 68/1000 | Loss: 0.00020554
Iteration 69/1000 | Loss: 0.00009806
Iteration 70/1000 | Loss: 0.00010224
Iteration 71/1000 | Loss: 0.00017969
Iteration 72/1000 | Loss: 0.00004797
Iteration 73/1000 | Loss: 0.00004614
Iteration 74/1000 | Loss: 0.00006187
Iteration 75/1000 | Loss: 0.00006086
Iteration 76/1000 | Loss: 0.00005617
Iteration 77/1000 | Loss: 0.00005613
Iteration 78/1000 | Loss: 0.00004116
Iteration 79/1000 | Loss: 0.00004081
Iteration 80/1000 | Loss: 0.00004052
Iteration 81/1000 | Loss: 0.00004035
Iteration 82/1000 | Loss: 0.00004031
Iteration 83/1000 | Loss: 0.00004031
Iteration 84/1000 | Loss: 0.00004030
Iteration 85/1000 | Loss: 0.00004029
Iteration 86/1000 | Loss: 0.00004026
Iteration 87/1000 | Loss: 0.00004025
Iteration 88/1000 | Loss: 0.00004024
Iteration 89/1000 | Loss: 0.00004021
Iteration 90/1000 | Loss: 0.00004011
Iteration 91/1000 | Loss: 0.00004010
Iteration 92/1000 | Loss: 0.00004010
Iteration 93/1000 | Loss: 0.00004003
Iteration 94/1000 | Loss: 0.00004001
Iteration 95/1000 | Loss: 0.00004000
Iteration 96/1000 | Loss: 0.00004000
Iteration 97/1000 | Loss: 0.00004000
Iteration 98/1000 | Loss: 0.00004000
Iteration 99/1000 | Loss: 0.00003999
Iteration 100/1000 | Loss: 0.00003999
Iteration 101/1000 | Loss: 0.00003999
Iteration 102/1000 | Loss: 0.00003999
Iteration 103/1000 | Loss: 0.00003998
Iteration 104/1000 | Loss: 0.00003998
Iteration 105/1000 | Loss: 0.00003998
Iteration 106/1000 | Loss: 0.00003997
Iteration 107/1000 | Loss: 0.00003997
Iteration 108/1000 | Loss: 0.00006111
Iteration 109/1000 | Loss: 0.00012473
Iteration 110/1000 | Loss: 0.00004032
Iteration 111/1000 | Loss: 0.00003988
Iteration 112/1000 | Loss: 0.00003988
Iteration 113/1000 | Loss: 0.00003988
Iteration 114/1000 | Loss: 0.00003988
Iteration 115/1000 | Loss: 0.00003988
Iteration 116/1000 | Loss: 0.00003988
Iteration 117/1000 | Loss: 0.00003988
Iteration 118/1000 | Loss: 0.00003988
Iteration 119/1000 | Loss: 0.00003988
Iteration 120/1000 | Loss: 0.00003988
Iteration 121/1000 | Loss: 0.00003988
Iteration 122/1000 | Loss: 0.00003988
Iteration 123/1000 | Loss: 0.00003987
Iteration 124/1000 | Loss: 0.00003987
Iteration 125/1000 | Loss: 0.00003987
Iteration 126/1000 | Loss: 0.00003987
Iteration 127/1000 | Loss: 0.00003987
Iteration 128/1000 | Loss: 0.00003987
Iteration 129/1000 | Loss: 0.00003986
Iteration 130/1000 | Loss: 0.00003986
Iteration 131/1000 | Loss: 0.00003986
Iteration 132/1000 | Loss: 0.00003985
Iteration 133/1000 | Loss: 0.00003985
Iteration 134/1000 | Loss: 0.00003985
Iteration 135/1000 | Loss: 0.00003985
Iteration 136/1000 | Loss: 0.00003984
Iteration 137/1000 | Loss: 0.00003984
Iteration 138/1000 | Loss: 0.00003984
Iteration 139/1000 | Loss: 0.00003983
Iteration 140/1000 | Loss: 0.00003983
Iteration 141/1000 | Loss: 0.00005884
Iteration 142/1000 | Loss: 0.00003981
Iteration 143/1000 | Loss: 0.00003980
Iteration 144/1000 | Loss: 0.00003980
Iteration 145/1000 | Loss: 0.00003980
Iteration 146/1000 | Loss: 0.00003980
Iteration 147/1000 | Loss: 0.00003980
Iteration 148/1000 | Loss: 0.00003980
Iteration 149/1000 | Loss: 0.00003980
Iteration 150/1000 | Loss: 0.00003980
Iteration 151/1000 | Loss: 0.00003980
Iteration 152/1000 | Loss: 0.00003980
Iteration 153/1000 | Loss: 0.00003980
Iteration 154/1000 | Loss: 0.00003979
Iteration 155/1000 | Loss: 0.00003979
Iteration 156/1000 | Loss: 0.00003979
Iteration 157/1000 | Loss: 0.00003979
Iteration 158/1000 | Loss: 0.00003979
Iteration 159/1000 | Loss: 0.00003979
Iteration 160/1000 | Loss: 0.00003979
Iteration 161/1000 | Loss: 0.00003979
Iteration 162/1000 | Loss: 0.00003979
Iteration 163/1000 | Loss: 0.00003978
Iteration 164/1000 | Loss: 0.00003977
Iteration 165/1000 | Loss: 0.00008134
Iteration 166/1000 | Loss: 0.00008133
Iteration 167/1000 | Loss: 0.00026200
Iteration 168/1000 | Loss: 0.00010128
Iteration 169/1000 | Loss: 0.00007695
Iteration 170/1000 | Loss: 0.00003992
Iteration 171/1000 | Loss: 0.00003972
Iteration 172/1000 | Loss: 0.00003969
Iteration 173/1000 | Loss: 0.00003968
Iteration 174/1000 | Loss: 0.00003968
Iteration 175/1000 | Loss: 0.00003968
Iteration 176/1000 | Loss: 0.00003967
Iteration 177/1000 | Loss: 0.00003967
Iteration 178/1000 | Loss: 0.00003967
Iteration 179/1000 | Loss: 0.00003966
Iteration 180/1000 | Loss: 0.00003966
Iteration 181/1000 | Loss: 0.00004097
Iteration 182/1000 | Loss: 0.00004005
Iteration 183/1000 | Loss: 0.00003964
Iteration 184/1000 | Loss: 0.00003964
Iteration 185/1000 | Loss: 0.00003964
Iteration 186/1000 | Loss: 0.00003964
Iteration 187/1000 | Loss: 0.00003964
Iteration 188/1000 | Loss: 0.00003964
Iteration 189/1000 | Loss: 0.00003964
Iteration 190/1000 | Loss: 0.00003964
Iteration 191/1000 | Loss: 0.00003964
Iteration 192/1000 | Loss: 0.00003964
Iteration 193/1000 | Loss: 0.00003964
Iteration 194/1000 | Loss: 0.00003964
Iteration 195/1000 | Loss: 0.00003964
Iteration 196/1000 | Loss: 0.00003964
Iteration 197/1000 | Loss: 0.00003964
Iteration 198/1000 | Loss: 0.00003964
Iteration 199/1000 | Loss: 0.00003963
Iteration 200/1000 | Loss: 0.00003963
Iteration 201/1000 | Loss: 0.00003963
Iteration 202/1000 | Loss: 0.00003963
Iteration 203/1000 | Loss: 0.00003963
Iteration 204/1000 | Loss: 0.00005553
Iteration 205/1000 | Loss: 0.00006012
Iteration 206/1000 | Loss: 0.00004524
Iteration 207/1000 | Loss: 0.00004825
Iteration 208/1000 | Loss: 0.00003986
Iteration 209/1000 | Loss: 0.00005715
Iteration 210/1000 | Loss: 0.00003962
Iteration 211/1000 | Loss: 0.00003957
Iteration 212/1000 | Loss: 0.00003957
Iteration 213/1000 | Loss: 0.00004057
Iteration 214/1000 | Loss: 0.00004404
Iteration 215/1000 | Loss: 0.00004206
Iteration 216/1000 | Loss: 0.00004832
Iteration 217/1000 | Loss: 0.00004384
Iteration 218/1000 | Loss: 0.00004063
Iteration 219/1000 | Loss: 0.00004873
Iteration 220/1000 | Loss: 0.00004296
Iteration 221/1000 | Loss: 0.00004031
Iteration 222/1000 | Loss: 0.00004031
Iteration 223/1000 | Loss: 0.00004057
Iteration 224/1000 | Loss: 0.00004024
Iteration 225/1000 | Loss: 0.00004041
Iteration 226/1000 | Loss: 0.00004036
Iteration 227/1000 | Loss: 0.00004040
Iteration 228/1000 | Loss: 0.00004021
Iteration 229/1000 | Loss: 0.00005039
Iteration 230/1000 | Loss: 0.00004294
Iteration 231/1000 | Loss: 0.00004050
Iteration 232/1000 | Loss: 0.00004000
Iteration 233/1000 | Loss: 0.00003951
Iteration 234/1000 | Loss: 0.00003951
Iteration 235/1000 | Loss: 0.00003951
Iteration 236/1000 | Loss: 0.00003951
Iteration 237/1000 | Loss: 0.00003951
Iteration 238/1000 | Loss: 0.00003951
Iteration 239/1000 | Loss: 0.00003951
Iteration 240/1000 | Loss: 0.00003951
Iteration 241/1000 | Loss: 0.00003951
Iteration 242/1000 | Loss: 0.00003951
Iteration 243/1000 | Loss: 0.00003951
Iteration 244/1000 | Loss: 0.00003951
Iteration 245/1000 | Loss: 0.00003951
Iteration 246/1000 | Loss: 0.00003950
Iteration 247/1000 | Loss: 0.00003950
Iteration 248/1000 | Loss: 0.00003950
Iteration 249/1000 | Loss: 0.00003950
Iteration 250/1000 | Loss: 0.00003950
Iteration 251/1000 | Loss: 0.00003950
Iteration 252/1000 | Loss: 0.00004034
Iteration 253/1000 | Loss: 0.00004034
Iteration 254/1000 | Loss: 0.00007559
Iteration 255/1000 | Loss: 0.00004143
Iteration 256/1000 | Loss: 0.00004370
Iteration 257/1000 | Loss: 0.00004411
Iteration 258/1000 | Loss: 0.00004196
Iteration 259/1000 | Loss: 0.00004230
Iteration 260/1000 | Loss: 0.00004025
Iteration 261/1000 | Loss: 0.00004043
Iteration 262/1000 | Loss: 0.00004099
Iteration 263/1000 | Loss: 0.00009154
Iteration 264/1000 | Loss: 0.00008190
Iteration 265/1000 | Loss: 0.00007073
Iteration 266/1000 | Loss: 0.00008265
Iteration 267/1000 | Loss: 0.00005925
Iteration 268/1000 | Loss: 0.00007362
Iteration 269/1000 | Loss: 0.00004685
Iteration 270/1000 | Loss: 0.00004775
Iteration 271/1000 | Loss: 0.00006175
Iteration 272/1000 | Loss: 0.00004505
Iteration 273/1000 | Loss: 0.00004516
Iteration 274/1000 | Loss: 0.00008038
Iteration 275/1000 | Loss: 0.00004994
Iteration 276/1000 | Loss: 0.00012625
Iteration 277/1000 | Loss: 0.00004191
Iteration 278/1000 | Loss: 0.00004105
Iteration 279/1000 | Loss: 0.00004577
Iteration 280/1000 | Loss: 0.00004069
Iteration 281/1000 | Loss: 0.00005505
Iteration 282/1000 | Loss: 0.00004097
Iteration 283/1000 | Loss: 0.00004102
Iteration 284/1000 | Loss: 0.00004191
Iteration 285/1000 | Loss: 0.00004085
Iteration 286/1000 | Loss: 0.00004034
Iteration 287/1000 | Loss: 0.00004531
Iteration 288/1000 | Loss: 0.00004514
Iteration 289/1000 | Loss: 0.00004508
Iteration 290/1000 | Loss: 0.00007992
Iteration 291/1000 | Loss: 0.00004574
Iteration 292/1000 | Loss: 0.00004845
Iteration 293/1000 | Loss: 0.00004523
Iteration 294/1000 | Loss: 0.00004038
Iteration 295/1000 | Loss: 0.00004055
Iteration 296/1000 | Loss: 0.00004038
Iteration 297/1000 | Loss: 0.00004038
Iteration 298/1000 | Loss: 0.00004907
Iteration 299/1000 | Loss: 0.00004182
Iteration 300/1000 | Loss: 0.00004423
Iteration 301/1000 | Loss: 0.00007249
Iteration 302/1000 | Loss: 0.00004134
Iteration 303/1000 | Loss: 0.00004025
Iteration 304/1000 | Loss: 0.00004413
Iteration 305/1000 | Loss: 0.00004026
Iteration 306/1000 | Loss: 0.00004181
Iteration 307/1000 | Loss: 0.00004024
Iteration 308/1000 | Loss: 0.00004047
Iteration 309/1000 | Loss: 0.00004247
Iteration 310/1000 | Loss: 0.00005227
Iteration 311/1000 | Loss: 0.00004162
Iteration 312/1000 | Loss: 0.00004057
Iteration 313/1000 | Loss: 0.00004115
Iteration 314/1000 | Loss: 0.00004477
Iteration 315/1000 | Loss: 0.00004372
Iteration 316/1000 | Loss: 0.00004264
Iteration 317/1000 | Loss: 0.00004037
Iteration 318/1000 | Loss: 0.00004259
Iteration 319/1000 | Loss: 0.00004029
Iteration 320/1000 | Loss: 0.00004242
Iteration 321/1000 | Loss: 0.00004115
Iteration 322/1000 | Loss: 0.00003951
Iteration 323/1000 | Loss: 0.00003936
Iteration 324/1000 | Loss: 0.00003936
Iteration 325/1000 | Loss: 0.00003936
Iteration 326/1000 | Loss: 0.00003935
Iteration 327/1000 | Loss: 0.00003935
Iteration 328/1000 | Loss: 0.00003935
Iteration 329/1000 | Loss: 0.00003935
Iteration 330/1000 | Loss: 0.00003935
Iteration 331/1000 | Loss: 0.00003935
Iteration 332/1000 | Loss: 0.00003935
Iteration 333/1000 | Loss: 0.00003935
Iteration 334/1000 | Loss: 0.00003935
Iteration 335/1000 | Loss: 0.00003935
Iteration 336/1000 | Loss: 0.00003935
Iteration 337/1000 | Loss: 0.00003935
Iteration 338/1000 | Loss: 0.00003935
Iteration 339/1000 | Loss: 0.00003935
Iteration 340/1000 | Loss: 0.00003934
Iteration 341/1000 | Loss: 0.00003934
Iteration 342/1000 | Loss: 0.00003934
Iteration 343/1000 | Loss: 0.00003934
Iteration 344/1000 | Loss: 0.00003934
Iteration 345/1000 | Loss: 0.00003934
Iteration 346/1000 | Loss: 0.00003934
Iteration 347/1000 | Loss: 0.00003933
Iteration 348/1000 | Loss: 0.00003933
Iteration 349/1000 | Loss: 0.00003933
Iteration 350/1000 | Loss: 0.00004067
Iteration 351/1000 | Loss: 0.00004166
Iteration 352/1000 | Loss: 0.00004265
Iteration 353/1000 | Loss: 0.00004080
Iteration 354/1000 | Loss: 0.00004400
Iteration 355/1000 | Loss: 0.00004052
Iteration 356/1000 | Loss: 0.00004275
Iteration 357/1000 | Loss: 0.00004382
Iteration 358/1000 | Loss: 0.00006123
Iteration 359/1000 | Loss: 0.00003930
Iteration 360/1000 | Loss: 0.00003929
Iteration 361/1000 | Loss: 0.00003929
Iteration 362/1000 | Loss: 0.00004061
Iteration 363/1000 | Loss: 0.00004452
Iteration 364/1000 | Loss: 0.00004089
Iteration 365/1000 | Loss: 0.00004146
Iteration 366/1000 | Loss: 0.00004150
Iteration 367/1000 | Loss: 0.00004044
Iteration 368/1000 | Loss: 0.00004276
Iteration 369/1000 | Loss: 0.00006218
Iteration 370/1000 | Loss: 0.00004070
Iteration 371/1000 | Loss: 0.00004050
Iteration 372/1000 | Loss: 0.00004042
Iteration 373/1000 | Loss: 0.00004037
Iteration 374/1000 | Loss: 0.00004059
Iteration 375/1000 | Loss: 0.00004040
Iteration 376/1000 | Loss: 0.00004052
Iteration 377/1000 | Loss: 0.00004038
Iteration 378/1000 | Loss: 0.00004764
Iteration 379/1000 | Loss: 0.00005264
Iteration 380/1000 | Loss: 0.00004054
Iteration 381/1000 | Loss: 0.00004050
Iteration 382/1000 | Loss: 0.00004038
Iteration 383/1000 | Loss: 0.00004028
Iteration 384/1000 | Loss: 0.00004056
Iteration 385/1000 | Loss: 0.00004034
Iteration 386/1000 | Loss: 0.00004064
Iteration 387/1000 | Loss: 0.00004958
Iteration 388/1000 | Loss: 0.00005098
Iteration 389/1000 | Loss: 0.00004063
Iteration 390/1000 | Loss: 0.00004087
Iteration 391/1000 | Loss: 0.00004824
Iteration 392/1000 | Loss: 0.00004162
Iteration 393/1000 | Loss: 0.00004184
Iteration 394/1000 | Loss: 0.00004120
Iteration 395/1000 | Loss: 0.00004069
Iteration 396/1000 | Loss: 0.00004065
Iteration 397/1000 | Loss: 0.00004099
Iteration 398/1000 | Loss: 0.00004047
Iteration 399/1000 | Loss: 0.00004043
Iteration 400/1000 | Loss: 0.00004591
Iteration 401/1000 | Loss: 0.00004026
Iteration 402/1000 | Loss: 0.00004050
Iteration 403/1000 | Loss: 0.00012234
Iteration 404/1000 | Loss: 0.00004060
Iteration 405/1000 | Loss: 0.00004033
Iteration 406/1000 | Loss: 0.00004048
Iteration 407/1000 | Loss: 0.00004036
Iteration 408/1000 | Loss: 0.00004045
Iteration 409/1000 | Loss: 0.00004045
Iteration 410/1000 | Loss: 0.00004045
Iteration 411/1000 | Loss: 0.00005157
Iteration 412/1000 | Loss: 0.00004064
Iteration 413/1000 | Loss: 0.00004234
Iteration 414/1000 | Loss: 0.00005512
Iteration 415/1000 | Loss: 0.00004166
Iteration 416/1000 | Loss: 0.00006203
Iteration 417/1000 | Loss: 0.00004019
Iteration 418/1000 | Loss: 0.00004048
Iteration 419/1000 | Loss: 0.00004008
Iteration 420/1000 | Loss: 0.00004035
Iteration 421/1000 | Loss: 0.00004024
Iteration 422/1000 | Loss: 0.00004042
Iteration 423/1000 | Loss: 0.00004011
Iteration 424/1000 | Loss: 0.00004038
Iteration 425/1000 | Loss: 0.00004246
Iteration 426/1000 | Loss: 0.00004051
Iteration 427/1000 | Loss: 0.00004065
Iteration 428/1000 | Loss: 0.00004588
Iteration 429/1000 | Loss: 0.00004038
Iteration 430/1000 | Loss: 0.00004961
Iteration 431/1000 | Loss: 0.00004021
Iteration 432/1000 | Loss: 0.00004021
Iteration 433/1000 | Loss: 0.00004020
Iteration 434/1000 | Loss: 0.00004016
Iteration 435/1000 | Loss: 0.00004009
Iteration 436/1000 | Loss: 0.00004008
Iteration 437/1000 | Loss: 0.00004781
Iteration 438/1000 | Loss: 0.00004004
Iteration 439/1000 | Loss: 0.00004436
Iteration 440/1000 | Loss: 0.00006170
Iteration 441/1000 | Loss: 0.00005848
Iteration 442/1000 | Loss: 0.00007088
Iteration 443/1000 | Loss: 0.00004521
Iteration 444/1000 | Loss: 0.00005092
Iteration 445/1000 | Loss: 0.00004327
Iteration 446/1000 | Loss: 0.00005454
Iteration 447/1000 | Loss: 0.00004349
Iteration 448/1000 | Loss: 0.00005554
Iteration 449/1000 | Loss: 0.00004325
Iteration 450/1000 | Loss: 0.00005219
Iteration 451/1000 | Loss: 0.00004283
Iteration 452/1000 | Loss: 0.00004330
Iteration 453/1000 | Loss: 0.00004163
Iteration 454/1000 | Loss: 0.00009613
Iteration 455/1000 | Loss: 0.00004317
Iteration 456/1000 | Loss: 0.00004403
Iteration 457/1000 | Loss: 0.00004029
Iteration 458/1000 | Loss: 0.00003995
Iteration 459/1000 | Loss: 0.00004014
Iteration 460/1000 | Loss: 0.00004014
Iteration 461/1000 | Loss: 0.00006318
Iteration 462/1000 | Loss: 0.00004215
Iteration 463/1000 | Loss: 0.00004002
Iteration 464/1000 | Loss: 0.00003996
Iteration 465/1000 | Loss: 0.00003980
Iteration 466/1000 | Loss: 0.00003973
Iteration 467/1000 | Loss: 0.00003979
Iteration 468/1000 | Loss: 0.00003995
Iteration 469/1000 | Loss: 0.00003983
Iteration 470/1000 | Loss: 0.00003996
Iteration 471/1000 | Loss: 0.00003996
Iteration 472/1000 | Loss: 0.00009464
Iteration 473/1000 | Loss: 0.00004290
Iteration 474/1000 | Loss: 0.00003995
Iteration 475/1000 | Loss: 0.00003986
Iteration 476/1000 | Loss: 0.00003975
Iteration 477/1000 | Loss: 0.00003979
Iteration 478/1000 | Loss: 0.00005428
Iteration 479/1000 | Loss: 0.00004162
Iteration 480/1000 | Loss: 0.00004002
Iteration 481/1000 | Loss: 0.00003998
Iteration 482/1000 | Loss: 0.00004488
Iteration 483/1000 | Loss: 0.00003994
Iteration 484/1000 | Loss: 0.00003991
Iteration 485/1000 | Loss: 0.00003988
Iteration 486/1000 | Loss: 0.00003982
Iteration 487/1000 | Loss: 0.00003980
Iteration 488/1000 | Loss: 0.00006445
Iteration 489/1000 | Loss: 0.00004443
Iteration 490/1000 | Loss: 0.00004551
Iteration 491/1000 | Loss: 0.00004010
Iteration 492/1000 | Loss: 0.00003993
Iteration 493/1000 | Loss: 0.00003983
Iteration 494/1000 | Loss: 0.00004376
Iteration 495/1000 | Loss: 0.00004880
Iteration 496/1000 | Loss: 0.00005262
Iteration 497/1000 | Loss: 0.00004554
Iteration 498/1000 | Loss: 0.00004015
Iteration 499/1000 | Loss: 0.00004319
Iteration 500/1000 | Loss: 0.00004036
Iteration 501/1000 | Loss: 0.00003999
Iteration 502/1000 | Loss: 0.00003999
Iteration 503/1000 | Loss: 0.00003990
Iteration 504/1000 | Loss: 0.00003979
Iteration 505/1000 | Loss: 0.00004375
Iteration 506/1000 | Loss: 0.00003982
Iteration 507/1000 | Loss: 0.00004060
Iteration 508/1000 | Loss: 0.00004038
Iteration 509/1000 | Loss: 0.00004096
Iteration 510/1000 | Loss: 0.00004145
Iteration 511/1000 | Loss: 0.00004006
Iteration 512/1000 | Loss: 0.00003994
Iteration 513/1000 | Loss: 0.00004088
Iteration 514/1000 | Loss: 0.00003989
Iteration 515/1000 | Loss: 0.00004101
Iteration 516/1000 | Loss: 0.00004272
Iteration 517/1000 | Loss: 0.00004002
Iteration 518/1000 | Loss: 0.00004030
Iteration 519/1000 | Loss: 0.00004378
Iteration 520/1000 | Loss: 0.00004002
Iteration 521/1000 | Loss: 0.00004014
Iteration 522/1000 | Loss: 0.00004004
Iteration 523/1000 | Loss: 0.00004016
Iteration 524/1000 | Loss: 0.00004048
Iteration 525/1000 | Loss: 0.00004532
Iteration 526/1000 | Loss: 0.00003999
Iteration 527/1000 | Loss: 0.00003997
Iteration 528/1000 | Loss: 0.00003997
Iteration 529/1000 | Loss: 0.00003988
Iteration 530/1000 | Loss: 0.00003965
Iteration 531/1000 | Loss: 0.00003981
Iteration 532/1000 | Loss: 0.00003971
Iteration 533/1000 | Loss: 0.00003983
Iteration 534/1000 | Loss: 0.00003995
Iteration 535/1000 | Loss: 0.00003992
Iteration 536/1000 | Loss: 0.00003989
Iteration 537/1000 | Loss: 0.00003980
Iteration 538/1000 | Loss: 0.00003996
Iteration 539/1000 | Loss: 0.00003996
Iteration 540/1000 | Loss: 0.00005197
Iteration 541/1000 | Loss: 0.00004018
Iteration 542/1000 | Loss: 0.00004148
Iteration 543/1000 | Loss: 0.00004557
Iteration 544/1000 | Loss: 0.00003983
Iteration 545/1000 | Loss: 0.00003994
Iteration 546/1000 | Loss: 0.00004169
Iteration 547/1000 | Loss: 0.00004605
Iteration 548/1000 | Loss: 0.00003996
Iteration 549/1000 | Loss: 0.00003991
Iteration 550/1000 | Loss: 0.00003989
Iteration 551/1000 | Loss: 0.00004010
Iteration 552/1000 | Loss: 0.00004010
Iteration 553/1000 | Loss: 0.00003990
Iteration 554/1000 | Loss: 0.00003985
Iteration 555/1000 | Loss: 0.00004360
Iteration 556/1000 | Loss: 0.00004008
Iteration 557/1000 | Loss: 0.00003994
Iteration 558/1000 | Loss: 0.00003989
Iteration 559/1000 | Loss: 0.00003990
Iteration 560/1000 | Loss: 0.00003989
Iteration 561/1000 | Loss: 0.00003969
Iteration 562/1000 | Loss: 0.00003966
Iteration 563/1000 | Loss: 0.00003995
Iteration 564/1000 | Loss: 0.00003980
Iteration 565/1000 | Loss: 0.00003999
Iteration 566/1000 | Loss: 0.00003991
Iteration 567/1000 | Loss: 0.00003990
Iteration 568/1000 | Loss: 0.00004513
Iteration 569/1000 | Loss: 0.00007450
Iteration 570/1000 | Loss: 0.00004423
Iteration 571/1000 | Loss: 0.00004964
Iteration 572/1000 | Loss: 0.00004009
Iteration 573/1000 | Loss: 0.00003993
Iteration 574/1000 | Loss: 0.00003980
Iteration 575/1000 | Loss: 0.00003985
Iteration 576/1000 | Loss: 0.00003991
Iteration 577/1000 | Loss: 0.00003988
Iteration 578/1000 | Loss: 0.00004319
Iteration 579/1000 | Loss: 0.00004001
Iteration 580/1000 | Loss: 0.00004000
Iteration 581/1000 | Loss: 0.00004000
Iteration 582/1000 | Loss: 0.00004000
Iteration 583/1000 | Loss: 0.00004000
Iteration 584/1000 | Loss: 0.00004000
Iteration 585/1000 | Loss: 0.00004000
Iteration 586/1000 | Loss: 0.00003980
Iteration 587/1000 | Loss: 0.00003974
Iteration 588/1000 | Loss: 0.00005932
Iteration 589/1000 | Loss: 0.00004026
Iteration 590/1000 | Loss: 0.00004199
Iteration 591/1000 | Loss: 0.00004129
Iteration 592/1000 | Loss: 0.00008150
Iteration 593/1000 | Loss: 0.00004500
Iteration 594/1000 | Loss: 0.00004254
Iteration 595/1000 | Loss: 0.00003979
Iteration 596/1000 | Loss: 0.00003994
Iteration 597/1000 | Loss: 0.00003972
Iteration 598/1000 | Loss: 0.00003995
Iteration 599/1000 | Loss: 0.00004171
Iteration 600/1000 | Loss: 0.00004171
Iteration 601/1000 | Loss: 0.00003991
Iteration 602/1000 | Loss: 0.00003985
Iteration 603/1000 | Loss: 0.00003997
Iteration 604/1000 | Loss: 0.00003992
Iteration 605/1000 | Loss: 0.00004274
Iteration 606/1000 | Loss: 0.00004703
Iteration 607/1000 | Loss: 0.00004111
Iteration 608/1000 | Loss: 0.00004149
Iteration 609/1000 | Loss: 0.00004010
Iteration 610/1000 | Loss: 0.00004014
Iteration 611/1000 | Loss: 0.00003987
Iteration 612/1000 | Loss: 0.00004004
Iteration 613/1000 | Loss: 0.00004012
Iteration 614/1000 | Loss: 0.00003893
Iteration 615/1000 | Loss: 0.00003892
Iteration 616/1000 | Loss: 0.00003892
Iteration 617/1000 | Loss: 0.00003892
Iteration 618/1000 | Loss: 0.00004000
Iteration 619/1000 | Loss: 0.00003994
Iteration 620/1000 | Loss: 0.00004015
Iteration 621/1000 | Loss: 0.00004046
Iteration 622/1000 | Loss: 0.00004076
Iteration 623/1000 | Loss: 0.00003984
Iteration 624/1000 | Loss: 0.00004011
Iteration 625/1000 | Loss: 0.00004004
Iteration 626/1000 | Loss: 0.00004024
Iteration 627/1000 | Loss: 0.00004024
Iteration 628/1000 | Loss: 0.00004420
Iteration 629/1000 | Loss: 0.00004420
Iteration 630/1000 | Loss: 0.00015344
Iteration 631/1000 | Loss: 0.00005151
Iteration 632/1000 | Loss: 0.00004515
Iteration 633/1000 | Loss: 0.00004038
Iteration 634/1000 | Loss: 0.00004005
Iteration 635/1000 | Loss: 0.00005703
Iteration 636/1000 | Loss: 0.00008348
Iteration 637/1000 | Loss: 0.00004004
Iteration 638/1000 | Loss: 0.00003901
Iteration 639/1000 | Loss: 0.00003895
Iteration 640/1000 | Loss: 0.00003895
Iteration 641/1000 | Loss: 0.00003893
Iteration 642/1000 | Loss: 0.00003893
Iteration 643/1000 | Loss: 0.00003893
Iteration 644/1000 | Loss: 0.00003893
Iteration 645/1000 | Loss: 0.00003991
Iteration 646/1000 | Loss: 0.00003991
Iteration 647/1000 | Loss: 0.00005946
Iteration 648/1000 | Loss: 0.00007017
Iteration 649/1000 | Loss: 0.00004040
Iteration 650/1000 | Loss: 0.00004658
Iteration 651/1000 | Loss: 0.00005134
Iteration 652/1000 | Loss: 0.00003900
Iteration 653/1000 | Loss: 0.00003994
Iteration 654/1000 | Loss: 0.00004777
Iteration 655/1000 | Loss: 0.00004032
Iteration 656/1000 | Loss: 0.00004058
Iteration 657/1000 | Loss: 0.00004023
Iteration 658/1000 | Loss: 0.00003898
Iteration 659/1000 | Loss: 0.00005623
Iteration 660/1000 | Loss: 0.00003995
Iteration 661/1000 | Loss: 0.00004175
Iteration 662/1000 | Loss: 0.00007593
Iteration 663/1000 | Loss: 0.00004407
Iteration 664/1000 | Loss: 0.00004061
Iteration 665/1000 | Loss: 0.00003997
Iteration 666/1000 | Loss: 0.00003994
Iteration 667/1000 | Loss: 0.00004057
Iteration 668/1000 | Loss: 0.00004280
Iteration 669/1000 | Loss: 0.00003995
Iteration 670/1000 | Loss: 0.00003895
Iteration 671/1000 | Loss: 0.00003894
Iteration 672/1000 | Loss: 0.00003894
Iteration 673/1000 | Loss: 0.00003894
Iteration 674/1000 | Loss: 0.00003894
Iteration 675/1000 | Loss: 0.00003893
Iteration 676/1000 | Loss: 0.00003893
Iteration 677/1000 | Loss: 0.00003893
Iteration 678/1000 | Loss: 0.00003893
Iteration 679/1000 | Loss: 0.00003992
Iteration 680/1000 | Loss: 0.00004740
Iteration 681/1000 | Loss: 0.00003923
Iteration 682/1000 | Loss: 0.00003899
Iteration 683/1000 | Loss: 0.00004010
Iteration 684/1000 | Loss: 0.00004016
Iteration 685/1000 | Loss: 0.00003898
Iteration 686/1000 | Loss: 0.00003895
Iteration 687/1000 | Loss: 0.00003895
Iteration 688/1000 | Loss: 0.00003894
Iteration 689/1000 | Loss: 0.00003894
Iteration 690/1000 | Loss: 0.00003894
Iteration 691/1000 | Loss: 0.00003894
Iteration 692/1000 | Loss: 0.00003894
Iteration 693/1000 | Loss: 0.00003894
Iteration 694/1000 | Loss: 0.00003894
Iteration 695/1000 | Loss: 0.00003893
Iteration 696/1000 | Loss: 0.00003893
Iteration 697/1000 | Loss: 0.00003893
Iteration 698/1000 | Loss: 0.00003893
Iteration 699/1000 | Loss: 0.00003893
Iteration 700/1000 | Loss: 0.00003893
Iteration 701/1000 | Loss: 0.00003893
Iteration 702/1000 | Loss: 0.00003892
Iteration 703/1000 | Loss: 0.00003892
Iteration 704/1000 | Loss: 0.00003892
Iteration 705/1000 | Loss: 0.00003892
Iteration 706/1000 | Loss: 0.00003892
Iteration 707/1000 | Loss: 0.00003892
Iteration 708/1000 | Loss: 0.00003892
Iteration 709/1000 | Loss: 0.00003892
Iteration 710/1000 | Loss: 0.00003892
Iteration 711/1000 | Loss: 0.00003892
Iteration 712/1000 | Loss: 0.00003892
Iteration 713/1000 | Loss: 0.00003892
Iteration 714/1000 | Loss: 0.00003892
Iteration 715/1000 | Loss: 0.00003891
Iteration 716/1000 | Loss: 0.00003891
Iteration 717/1000 | Loss: 0.00003891
Iteration 718/1000 | Loss: 0.00003891
Iteration 719/1000 | Loss: 0.00003891
Iteration 720/1000 | Loss: 0.00003891
Iteration 721/1000 | Loss: 0.00003891
Iteration 722/1000 | Loss: 0.00004006
Iteration 723/1000 | Loss: 0.00004002
Iteration 724/1000 | Loss: 0.00004695
Iteration 725/1000 | Loss: 0.00004014
Iteration 726/1000 | Loss: 0.00004153
Iteration 727/1000 | Loss: 0.00003985
Iteration 728/1000 | Loss: 0.00004016
Iteration 729/1000 | Loss: 0.00003999
Iteration 730/1000 | Loss: 0.00004007
Iteration 731/1000 | Loss: 0.00003891
Iteration 732/1000 | Loss: 0.00003891
Iteration 733/1000 | Loss: 0.00003891
Iteration 734/1000 | Loss: 0.00003891
Iteration 735/1000 | Loss: 0.00003891
Iteration 736/1000 | Loss: 0.00003891
Iteration 737/1000 | Loss: 0.00003891
Iteration 738/1000 | Loss: 0.00003891
Iteration 739/1000 | Loss: 0.00003891
Iteration 740/1000 | Loss: 0.00003891
Iteration 741/1000 | Loss: 0.00003891
Iteration 742/1000 | Loss: 0.00003891
Iteration 743/1000 | Loss: 0.00003891
Iteration 744/1000 | Loss: 0.00003891
Iteration 745/1000 | Loss: 0.00003891
Iteration 746/1000 | Loss: 0.00003891
Iteration 747/1000 | Loss: 0.00003891
Iteration 748/1000 | Loss: 0.00003891
Iteration 749/1000 | Loss: 0.00003891
Iteration 750/1000 | Loss: 0.00003891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 750. Stopping optimization.
Last 5 losses: [3.8910853618290275e-05, 3.8910853618290275e-05, 3.8910853618290275e-05, 3.8910853618290275e-05, 3.8910853618290275e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8910853618290275e-05

Optimization complete. Final v2v error: 4.229684352874756 mm

Highest mean error: 21.08342170715332 mm for frame 219

Lowest mean error: 3.5080902576446533 mm for frame 144

Saving results

Total time: 731.2041394710541
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827390
Iteration 2/25 | Loss: 0.00137174
Iteration 3/25 | Loss: 0.00108817
Iteration 4/25 | Loss: 0.00104799
Iteration 5/25 | Loss: 0.00102483
Iteration 6/25 | Loss: 0.00101290
Iteration 7/25 | Loss: 0.00101216
Iteration 8/25 | Loss: 0.00101145
Iteration 9/25 | Loss: 0.00101081
Iteration 10/25 | Loss: 0.00101056
Iteration 11/25 | Loss: 0.00101047
Iteration 12/25 | Loss: 0.00101042
Iteration 13/25 | Loss: 0.00101042
Iteration 14/25 | Loss: 0.00101041
Iteration 15/25 | Loss: 0.00101041
Iteration 16/25 | Loss: 0.00101041
Iteration 17/25 | Loss: 0.00101041
Iteration 18/25 | Loss: 0.00101041
Iteration 19/25 | Loss: 0.00101041
Iteration 20/25 | Loss: 0.00101041
Iteration 21/25 | Loss: 0.00101041
Iteration 22/25 | Loss: 0.00101041
Iteration 23/25 | Loss: 0.00101041
Iteration 24/25 | Loss: 0.00101041
Iteration 25/25 | Loss: 0.00101041

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.22233748
Iteration 2/25 | Loss: 0.00065751
Iteration 3/25 | Loss: 0.00065750
Iteration 4/25 | Loss: 0.00065750
Iteration 5/25 | Loss: 0.00065750
Iteration 6/25 | Loss: 0.00065750
Iteration 7/25 | Loss: 0.00065750
Iteration 8/25 | Loss: 0.00065750
Iteration 9/25 | Loss: 0.00065750
Iteration 10/25 | Loss: 0.00065750
Iteration 11/25 | Loss: 0.00065750
Iteration 12/25 | Loss: 0.00065750
Iteration 13/25 | Loss: 0.00065750
Iteration 14/25 | Loss: 0.00065750
Iteration 15/25 | Loss: 0.00065750
Iteration 16/25 | Loss: 0.00065750
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006575022707693279, 0.0006575022707693279, 0.0006575022707693279, 0.0006575022707693279, 0.0006575022707693279]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006575022707693279

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065750
Iteration 2/1000 | Loss: 0.00006070
Iteration 3/1000 | Loss: 0.00003984
Iteration 4/1000 | Loss: 0.00003396
Iteration 5/1000 | Loss: 0.00003064
Iteration 6/1000 | Loss: 0.00002919
Iteration 7/1000 | Loss: 0.00002868
Iteration 8/1000 | Loss: 0.00002833
Iteration 9/1000 | Loss: 0.00002804
Iteration 10/1000 | Loss: 0.00002788
Iteration 11/1000 | Loss: 0.00002764
Iteration 12/1000 | Loss: 0.00002752
Iteration 13/1000 | Loss: 0.00002752
Iteration 14/1000 | Loss: 0.00002752
Iteration 15/1000 | Loss: 0.00002751
Iteration 16/1000 | Loss: 0.00002742
Iteration 17/1000 | Loss: 0.00002738
Iteration 18/1000 | Loss: 0.00002737
Iteration 19/1000 | Loss: 0.00002737
Iteration 20/1000 | Loss: 0.00002736
Iteration 21/1000 | Loss: 0.00002736
Iteration 22/1000 | Loss: 0.00002736
Iteration 23/1000 | Loss: 0.00002735
Iteration 24/1000 | Loss: 0.00002735
Iteration 25/1000 | Loss: 0.00002735
Iteration 26/1000 | Loss: 0.00002734
Iteration 27/1000 | Loss: 0.00002734
Iteration 28/1000 | Loss: 0.00002734
Iteration 29/1000 | Loss: 0.00002734
Iteration 30/1000 | Loss: 0.00002734
Iteration 31/1000 | Loss: 0.00002734
Iteration 32/1000 | Loss: 0.00002734
Iteration 33/1000 | Loss: 0.00002734
Iteration 34/1000 | Loss: 0.00002734
Iteration 35/1000 | Loss: 0.00002734
Iteration 36/1000 | Loss: 0.00002733
Iteration 37/1000 | Loss: 0.00002733
Iteration 38/1000 | Loss: 0.00002733
Iteration 39/1000 | Loss: 0.00002733
Iteration 40/1000 | Loss: 0.00002733
Iteration 41/1000 | Loss: 0.00002733
Iteration 42/1000 | Loss: 0.00002733
Iteration 43/1000 | Loss: 0.00002733
Iteration 44/1000 | Loss: 0.00002733
Iteration 45/1000 | Loss: 0.00002733
Iteration 46/1000 | Loss: 0.00002733
Iteration 47/1000 | Loss: 0.00002733
Iteration 48/1000 | Loss: 0.00002733
Iteration 49/1000 | Loss: 0.00002733
Iteration 50/1000 | Loss: 0.00002733
Iteration 51/1000 | Loss: 0.00002733
Iteration 52/1000 | Loss: 0.00002733
Iteration 53/1000 | Loss: 0.00002733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 53. Stopping optimization.
Last 5 losses: [2.732912435021717e-05, 2.732912435021717e-05, 2.732912435021717e-05, 2.732912435021717e-05, 2.732912435021717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.732912435021717e-05

Optimization complete. Final v2v error: 4.482802867889404 mm

Highest mean error: 10.590718269348145 mm for frame 18

Lowest mean error: 4.28346061706543 mm for frame 110

Saving results

Total time: 69.77992153167725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00742953
Iteration 2/25 | Loss: 0.00114298
Iteration 3/25 | Loss: 0.00104047
Iteration 4/25 | Loss: 0.00102217
Iteration 5/25 | Loss: 0.00101355
Iteration 6/25 | Loss: 0.00101006
Iteration 7/25 | Loss: 0.00100938
Iteration 8/25 | Loss: 0.00100938
Iteration 9/25 | Loss: 0.00100938
Iteration 10/25 | Loss: 0.00100938
Iteration 11/25 | Loss: 0.00100938
Iteration 12/25 | Loss: 0.00100938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010093810269609094, 0.0010093810269609094, 0.0010093810269609094, 0.0010093810269609094, 0.0010093810269609094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010093810269609094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.78588486
Iteration 2/25 | Loss: 0.00052048
Iteration 3/25 | Loss: 0.00052046
Iteration 4/25 | Loss: 0.00052046
Iteration 5/25 | Loss: 0.00052046
Iteration 6/25 | Loss: 0.00052046
Iteration 7/25 | Loss: 0.00052046
Iteration 8/25 | Loss: 0.00052046
Iteration 9/25 | Loss: 0.00052046
Iteration 10/25 | Loss: 0.00052046
Iteration 11/25 | Loss: 0.00052046
Iteration 12/25 | Loss: 0.00052046
Iteration 13/25 | Loss: 0.00052046
Iteration 14/25 | Loss: 0.00052046
Iteration 15/25 | Loss: 0.00052046
Iteration 16/25 | Loss: 0.00052046
Iteration 17/25 | Loss: 0.00052046
Iteration 18/25 | Loss: 0.00052046
Iteration 19/25 | Loss: 0.00052046
Iteration 20/25 | Loss: 0.00052046
Iteration 21/25 | Loss: 0.00052046
Iteration 22/25 | Loss: 0.00052046
Iteration 23/25 | Loss: 0.00052046
Iteration 24/25 | Loss: 0.00052046
Iteration 25/25 | Loss: 0.00052046

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052046
Iteration 2/1000 | Loss: 0.00003993
Iteration 3/1000 | Loss: 0.00003089
Iteration 4/1000 | Loss: 0.00002751
Iteration 5/1000 | Loss: 0.00002599
Iteration 6/1000 | Loss: 0.00002486
Iteration 7/1000 | Loss: 0.00002439
Iteration 8/1000 | Loss: 0.00002408
Iteration 9/1000 | Loss: 0.00002381
Iteration 10/1000 | Loss: 0.00002373
Iteration 11/1000 | Loss: 0.00002363
Iteration 12/1000 | Loss: 0.00002363
Iteration 13/1000 | Loss: 0.00002356
Iteration 14/1000 | Loss: 0.00002356
Iteration 15/1000 | Loss: 0.00002354
Iteration 16/1000 | Loss: 0.00002353
Iteration 17/1000 | Loss: 0.00002353
Iteration 18/1000 | Loss: 0.00002353
Iteration 19/1000 | Loss: 0.00002353
Iteration 20/1000 | Loss: 0.00002352
Iteration 21/1000 | Loss: 0.00002352
Iteration 22/1000 | Loss: 0.00002352
Iteration 23/1000 | Loss: 0.00002352
Iteration 24/1000 | Loss: 0.00002352
Iteration 25/1000 | Loss: 0.00002352
Iteration 26/1000 | Loss: 0.00002351
Iteration 27/1000 | Loss: 0.00002351
Iteration 28/1000 | Loss: 0.00002351
Iteration 29/1000 | Loss: 0.00002351
Iteration 30/1000 | Loss: 0.00002350
Iteration 31/1000 | Loss: 0.00002350
Iteration 32/1000 | Loss: 0.00002350
Iteration 33/1000 | Loss: 0.00002350
Iteration 34/1000 | Loss: 0.00002349
Iteration 35/1000 | Loss: 0.00002349
Iteration 36/1000 | Loss: 0.00002349
Iteration 37/1000 | Loss: 0.00002348
Iteration 38/1000 | Loss: 0.00002348
Iteration 39/1000 | Loss: 0.00002348
Iteration 40/1000 | Loss: 0.00002348
Iteration 41/1000 | Loss: 0.00002348
Iteration 42/1000 | Loss: 0.00002348
Iteration 43/1000 | Loss: 0.00002348
Iteration 44/1000 | Loss: 0.00002348
Iteration 45/1000 | Loss: 0.00002348
Iteration 46/1000 | Loss: 0.00002348
Iteration 47/1000 | Loss: 0.00002348
Iteration 48/1000 | Loss: 0.00002348
Iteration 49/1000 | Loss: 0.00002348
Iteration 50/1000 | Loss: 0.00002348
Iteration 51/1000 | Loss: 0.00002348
Iteration 52/1000 | Loss: 0.00002347
Iteration 53/1000 | Loss: 0.00002347
Iteration 54/1000 | Loss: 0.00002347
Iteration 55/1000 | Loss: 0.00002347
Iteration 56/1000 | Loss: 0.00002347
Iteration 57/1000 | Loss: 0.00002347
Iteration 58/1000 | Loss: 0.00002346
Iteration 59/1000 | Loss: 0.00002346
Iteration 60/1000 | Loss: 0.00002346
Iteration 61/1000 | Loss: 0.00002346
Iteration 62/1000 | Loss: 0.00002346
Iteration 63/1000 | Loss: 0.00002346
Iteration 64/1000 | Loss: 0.00002346
Iteration 65/1000 | Loss: 0.00002346
Iteration 66/1000 | Loss: 0.00002346
Iteration 67/1000 | Loss: 0.00002346
Iteration 68/1000 | Loss: 0.00002346
Iteration 69/1000 | Loss: 0.00002346
Iteration 70/1000 | Loss: 0.00002346
Iteration 71/1000 | Loss: 0.00002346
Iteration 72/1000 | Loss: 0.00002346
Iteration 73/1000 | Loss: 0.00002346
Iteration 74/1000 | Loss: 0.00002346
Iteration 75/1000 | Loss: 0.00002346
Iteration 76/1000 | Loss: 0.00002346
Iteration 77/1000 | Loss: 0.00002346
Iteration 78/1000 | Loss: 0.00002346
Iteration 79/1000 | Loss: 0.00002346
Iteration 80/1000 | Loss: 0.00002346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [2.3463138859369792e-05, 2.3463138859369792e-05, 2.3463138859369792e-05, 2.3463138859369792e-05, 2.3463138859369792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3463138859369792e-05

Optimization complete. Final v2v error: 4.234949111938477 mm

Highest mean error: 4.494719505310059 mm for frame 153

Lowest mean error: 3.9935684204101562 mm for frame 131

Saving results

Total time: 50.84851002693176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889844
Iteration 2/25 | Loss: 0.00140205
Iteration 3/25 | Loss: 0.00099963
Iteration 4/25 | Loss: 0.00095006
Iteration 5/25 | Loss: 0.00094049
Iteration 6/25 | Loss: 0.00093766
Iteration 7/25 | Loss: 0.00093651
Iteration 8/25 | Loss: 0.00093642
Iteration 9/25 | Loss: 0.00093642
Iteration 10/25 | Loss: 0.00093642
Iteration 11/25 | Loss: 0.00093642
Iteration 12/25 | Loss: 0.00093642
Iteration 13/25 | Loss: 0.00093642
Iteration 14/25 | Loss: 0.00093642
Iteration 15/25 | Loss: 0.00093642
Iteration 16/25 | Loss: 0.00093642
Iteration 17/25 | Loss: 0.00093642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009364206343889236, 0.0009364206343889236, 0.0009364206343889236, 0.0009364206343889236, 0.0009364206343889236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009364206343889236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43193877
Iteration 2/25 | Loss: 0.00052122
Iteration 3/25 | Loss: 0.00052122
Iteration 4/25 | Loss: 0.00052122
Iteration 5/25 | Loss: 0.00052122
Iteration 6/25 | Loss: 0.00052122
Iteration 7/25 | Loss: 0.00052122
Iteration 8/25 | Loss: 0.00052122
Iteration 9/25 | Loss: 0.00052122
Iteration 10/25 | Loss: 0.00052122
Iteration 11/25 | Loss: 0.00052122
Iteration 12/25 | Loss: 0.00052122
Iteration 13/25 | Loss: 0.00052122
Iteration 14/25 | Loss: 0.00052122
Iteration 15/25 | Loss: 0.00052122
Iteration 16/25 | Loss: 0.00052122
Iteration 17/25 | Loss: 0.00052122
Iteration 18/25 | Loss: 0.00052122
Iteration 19/25 | Loss: 0.00052122
Iteration 20/25 | Loss: 0.00052122
Iteration 21/25 | Loss: 0.00052122
Iteration 22/25 | Loss: 0.00052122
Iteration 23/25 | Loss: 0.00052122
Iteration 24/25 | Loss: 0.00052122
Iteration 25/25 | Loss: 0.00052122

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052122
Iteration 2/1000 | Loss: 0.00004209
Iteration 3/1000 | Loss: 0.00002795
Iteration 4/1000 | Loss: 0.00002437
Iteration 5/1000 | Loss: 0.00002192
Iteration 6/1000 | Loss: 0.00002052
Iteration 7/1000 | Loss: 0.00001987
Iteration 8/1000 | Loss: 0.00001961
Iteration 9/1000 | Loss: 0.00001958
Iteration 10/1000 | Loss: 0.00001946
Iteration 11/1000 | Loss: 0.00001937
Iteration 12/1000 | Loss: 0.00001931
Iteration 13/1000 | Loss: 0.00001930
Iteration 14/1000 | Loss: 0.00001929
Iteration 15/1000 | Loss: 0.00001928
Iteration 16/1000 | Loss: 0.00001927
Iteration 17/1000 | Loss: 0.00001923
Iteration 18/1000 | Loss: 0.00001922
Iteration 19/1000 | Loss: 0.00001922
Iteration 20/1000 | Loss: 0.00001922
Iteration 21/1000 | Loss: 0.00001921
Iteration 22/1000 | Loss: 0.00001919
Iteration 23/1000 | Loss: 0.00001918
Iteration 24/1000 | Loss: 0.00001918
Iteration 25/1000 | Loss: 0.00001915
Iteration 26/1000 | Loss: 0.00001915
Iteration 27/1000 | Loss: 0.00001914
Iteration 28/1000 | Loss: 0.00001914
Iteration 29/1000 | Loss: 0.00001914
Iteration 30/1000 | Loss: 0.00001914
Iteration 31/1000 | Loss: 0.00001914
Iteration 32/1000 | Loss: 0.00001912
Iteration 33/1000 | Loss: 0.00001912
Iteration 34/1000 | Loss: 0.00001911
Iteration 35/1000 | Loss: 0.00001911
Iteration 36/1000 | Loss: 0.00001911
Iteration 37/1000 | Loss: 0.00001910
Iteration 38/1000 | Loss: 0.00001910
Iteration 39/1000 | Loss: 0.00001910
Iteration 40/1000 | Loss: 0.00001910
Iteration 41/1000 | Loss: 0.00001910
Iteration 42/1000 | Loss: 0.00001910
Iteration 43/1000 | Loss: 0.00001910
Iteration 44/1000 | Loss: 0.00001910
Iteration 45/1000 | Loss: 0.00001909
Iteration 46/1000 | Loss: 0.00001909
Iteration 47/1000 | Loss: 0.00001909
Iteration 48/1000 | Loss: 0.00001909
Iteration 49/1000 | Loss: 0.00001909
Iteration 50/1000 | Loss: 0.00001909
Iteration 51/1000 | Loss: 0.00001908
Iteration 52/1000 | Loss: 0.00001908
Iteration 53/1000 | Loss: 0.00001908
Iteration 54/1000 | Loss: 0.00001908
Iteration 55/1000 | Loss: 0.00001908
Iteration 56/1000 | Loss: 0.00001907
Iteration 57/1000 | Loss: 0.00001907
Iteration 58/1000 | Loss: 0.00001907
Iteration 59/1000 | Loss: 0.00001907
Iteration 60/1000 | Loss: 0.00001906
Iteration 61/1000 | Loss: 0.00001906
Iteration 62/1000 | Loss: 0.00001906
Iteration 63/1000 | Loss: 0.00001906
Iteration 64/1000 | Loss: 0.00001906
Iteration 65/1000 | Loss: 0.00001906
Iteration 66/1000 | Loss: 0.00001905
Iteration 67/1000 | Loss: 0.00001905
Iteration 68/1000 | Loss: 0.00001905
Iteration 69/1000 | Loss: 0.00001905
Iteration 70/1000 | Loss: 0.00001905
Iteration 71/1000 | Loss: 0.00001905
Iteration 72/1000 | Loss: 0.00001905
Iteration 73/1000 | Loss: 0.00001905
Iteration 74/1000 | Loss: 0.00001905
Iteration 75/1000 | Loss: 0.00001905
Iteration 76/1000 | Loss: 0.00001904
Iteration 77/1000 | Loss: 0.00001904
Iteration 78/1000 | Loss: 0.00001904
Iteration 79/1000 | Loss: 0.00001904
Iteration 80/1000 | Loss: 0.00001904
Iteration 81/1000 | Loss: 0.00001904
Iteration 82/1000 | Loss: 0.00001904
Iteration 83/1000 | Loss: 0.00001904
Iteration 84/1000 | Loss: 0.00001904
Iteration 85/1000 | Loss: 0.00001904
Iteration 86/1000 | Loss: 0.00001904
Iteration 87/1000 | Loss: 0.00001904
Iteration 88/1000 | Loss: 0.00001904
Iteration 89/1000 | Loss: 0.00001903
Iteration 90/1000 | Loss: 0.00001903
Iteration 91/1000 | Loss: 0.00001903
Iteration 92/1000 | Loss: 0.00001903
Iteration 93/1000 | Loss: 0.00001903
Iteration 94/1000 | Loss: 0.00001903
Iteration 95/1000 | Loss: 0.00001903
Iteration 96/1000 | Loss: 0.00001903
Iteration 97/1000 | Loss: 0.00001903
Iteration 98/1000 | Loss: 0.00001903
Iteration 99/1000 | Loss: 0.00001903
Iteration 100/1000 | Loss: 0.00001903
Iteration 101/1000 | Loss: 0.00001903
Iteration 102/1000 | Loss: 0.00001902
Iteration 103/1000 | Loss: 0.00001902
Iteration 104/1000 | Loss: 0.00001902
Iteration 105/1000 | Loss: 0.00001902
Iteration 106/1000 | Loss: 0.00001902
Iteration 107/1000 | Loss: 0.00001901
Iteration 108/1000 | Loss: 0.00001901
Iteration 109/1000 | Loss: 0.00001901
Iteration 110/1000 | Loss: 0.00001901
Iteration 111/1000 | Loss: 0.00001901
Iteration 112/1000 | Loss: 0.00001901
Iteration 113/1000 | Loss: 0.00001901
Iteration 114/1000 | Loss: 0.00001901
Iteration 115/1000 | Loss: 0.00001900
Iteration 116/1000 | Loss: 0.00001900
Iteration 117/1000 | Loss: 0.00001900
Iteration 118/1000 | Loss: 0.00001900
Iteration 119/1000 | Loss: 0.00001900
Iteration 120/1000 | Loss: 0.00001900
Iteration 121/1000 | Loss: 0.00001900
Iteration 122/1000 | Loss: 0.00001900
Iteration 123/1000 | Loss: 0.00001899
Iteration 124/1000 | Loss: 0.00001899
Iteration 125/1000 | Loss: 0.00001899
Iteration 126/1000 | Loss: 0.00001899
Iteration 127/1000 | Loss: 0.00001899
Iteration 128/1000 | Loss: 0.00001899
Iteration 129/1000 | Loss: 0.00001899
Iteration 130/1000 | Loss: 0.00001899
Iteration 131/1000 | Loss: 0.00001899
Iteration 132/1000 | Loss: 0.00001899
Iteration 133/1000 | Loss: 0.00001899
Iteration 134/1000 | Loss: 0.00001899
Iteration 135/1000 | Loss: 0.00001899
Iteration 136/1000 | Loss: 0.00001899
Iteration 137/1000 | Loss: 0.00001899
Iteration 138/1000 | Loss: 0.00001899
Iteration 139/1000 | Loss: 0.00001899
Iteration 140/1000 | Loss: 0.00001898
Iteration 141/1000 | Loss: 0.00001898
Iteration 142/1000 | Loss: 0.00001898
Iteration 143/1000 | Loss: 0.00001898
Iteration 144/1000 | Loss: 0.00001898
Iteration 145/1000 | Loss: 0.00001898
Iteration 146/1000 | Loss: 0.00001898
Iteration 147/1000 | Loss: 0.00001898
Iteration 148/1000 | Loss: 0.00001898
Iteration 149/1000 | Loss: 0.00001898
Iteration 150/1000 | Loss: 0.00001898
Iteration 151/1000 | Loss: 0.00001898
Iteration 152/1000 | Loss: 0.00001898
Iteration 153/1000 | Loss: 0.00001898
Iteration 154/1000 | Loss: 0.00001898
Iteration 155/1000 | Loss: 0.00001898
Iteration 156/1000 | Loss: 0.00001898
Iteration 157/1000 | Loss: 0.00001898
Iteration 158/1000 | Loss: 0.00001898
Iteration 159/1000 | Loss: 0.00001897
Iteration 160/1000 | Loss: 0.00001897
Iteration 161/1000 | Loss: 0.00001897
Iteration 162/1000 | Loss: 0.00001897
Iteration 163/1000 | Loss: 0.00001897
Iteration 164/1000 | Loss: 0.00001897
Iteration 165/1000 | Loss: 0.00001897
Iteration 166/1000 | Loss: 0.00001897
Iteration 167/1000 | Loss: 0.00001897
Iteration 168/1000 | Loss: 0.00001897
Iteration 169/1000 | Loss: 0.00001897
Iteration 170/1000 | Loss: 0.00001896
Iteration 171/1000 | Loss: 0.00001896
Iteration 172/1000 | Loss: 0.00001896
Iteration 173/1000 | Loss: 0.00001896
Iteration 174/1000 | Loss: 0.00001896
Iteration 175/1000 | Loss: 0.00001896
Iteration 176/1000 | Loss: 0.00001896
Iteration 177/1000 | Loss: 0.00001896
Iteration 178/1000 | Loss: 0.00001896
Iteration 179/1000 | Loss: 0.00001896
Iteration 180/1000 | Loss: 0.00001896
Iteration 181/1000 | Loss: 0.00001896
Iteration 182/1000 | Loss: 0.00001896
Iteration 183/1000 | Loss: 0.00001896
Iteration 184/1000 | Loss: 0.00001896
Iteration 185/1000 | Loss: 0.00001896
Iteration 186/1000 | Loss: 0.00001896
Iteration 187/1000 | Loss: 0.00001896
Iteration 188/1000 | Loss: 0.00001896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.8958380678668618e-05, 1.8958380678668618e-05, 1.8958380678668618e-05, 1.8958380678668618e-05, 1.8958380678668618e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8958380678668618e-05

Optimization complete. Final v2v error: 3.7717795372009277 mm

Highest mean error: 4.062577724456787 mm for frame 109

Lowest mean error: 3.4883511066436768 mm for frame 4

Saving results

Total time: 83.94252562522888
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01137559
Iteration 2/25 | Loss: 0.00175575
Iteration 3/25 | Loss: 0.00119323
Iteration 4/25 | Loss: 0.00111634
Iteration 5/25 | Loss: 0.00131176
Iteration 6/25 | Loss: 0.00120776
Iteration 7/25 | Loss: 0.00115013
Iteration 8/25 | Loss: 0.00105531
Iteration 9/25 | Loss: 0.00099613
Iteration 10/25 | Loss: 0.00104107
Iteration 11/25 | Loss: 0.00096559
Iteration 12/25 | Loss: 0.00095835
Iteration 13/25 | Loss: 0.00095376
Iteration 14/25 | Loss: 0.00095261
Iteration 15/25 | Loss: 0.00095229
Iteration 16/25 | Loss: 0.00095406
Iteration 17/25 | Loss: 0.00095528
Iteration 18/25 | Loss: 0.00095600
Iteration 19/25 | Loss: 0.00095493
Iteration 20/25 | Loss: 0.00095425
Iteration 21/25 | Loss: 0.00095064
Iteration 22/25 | Loss: 0.00095044
Iteration 23/25 | Loss: 0.00095685
Iteration 24/25 | Loss: 0.00095498
Iteration 25/25 | Loss: 0.00095511

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55893779
Iteration 2/25 | Loss: 0.00056122
Iteration 3/25 | Loss: 0.00054250
Iteration 4/25 | Loss: 0.00054250
Iteration 5/25 | Loss: 0.00054250
Iteration 6/25 | Loss: 0.00054250
Iteration 7/25 | Loss: 0.00054250
Iteration 8/25 | Loss: 0.00054250
Iteration 9/25 | Loss: 0.00054250
Iteration 10/25 | Loss: 0.00054250
Iteration 11/25 | Loss: 0.00054250
Iteration 12/25 | Loss: 0.00054250
Iteration 13/25 | Loss: 0.00054250
Iteration 14/25 | Loss: 0.00054250
Iteration 15/25 | Loss: 0.00054250
Iteration 16/25 | Loss: 0.00054250
Iteration 17/25 | Loss: 0.00054250
Iteration 18/25 | Loss: 0.00054250
Iteration 19/25 | Loss: 0.00054250
Iteration 20/25 | Loss: 0.00054250
Iteration 21/25 | Loss: 0.00054250
Iteration 22/25 | Loss: 0.00054250
Iteration 23/25 | Loss: 0.00054250
Iteration 24/25 | Loss: 0.00054250
Iteration 25/25 | Loss: 0.00054250

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054250
Iteration 2/1000 | Loss: 0.00010658
Iteration 3/1000 | Loss: 0.00012228
Iteration 4/1000 | Loss: 0.00007906
Iteration 5/1000 | Loss: 0.00017362
Iteration 6/1000 | Loss: 0.00003456
Iteration 7/1000 | Loss: 0.00013309
Iteration 8/1000 | Loss: 0.00017562
Iteration 9/1000 | Loss: 0.00023653
Iteration 10/1000 | Loss: 0.00016615
Iteration 11/1000 | Loss: 0.00018732
Iteration 12/1000 | Loss: 0.00012431
Iteration 13/1000 | Loss: 0.00018130
Iteration 14/1000 | Loss: 0.00011256
Iteration 15/1000 | Loss: 0.00002455
Iteration 16/1000 | Loss: 0.00002249
Iteration 17/1000 | Loss: 0.00002161
Iteration 18/1000 | Loss: 0.00002122
Iteration 19/1000 | Loss: 0.00002091
Iteration 20/1000 | Loss: 0.00002072
Iteration 21/1000 | Loss: 0.00002065
Iteration 22/1000 | Loss: 0.00002058
Iteration 23/1000 | Loss: 0.00002047
Iteration 24/1000 | Loss: 0.00002047
Iteration 25/1000 | Loss: 0.00002047
Iteration 26/1000 | Loss: 0.00002047
Iteration 27/1000 | Loss: 0.00002046
Iteration 28/1000 | Loss: 0.00002045
Iteration 29/1000 | Loss: 0.00002044
Iteration 30/1000 | Loss: 0.00002044
Iteration 31/1000 | Loss: 0.00002044
Iteration 32/1000 | Loss: 0.00002044
Iteration 33/1000 | Loss: 0.00002043
Iteration 34/1000 | Loss: 0.00002043
Iteration 35/1000 | Loss: 0.00026161
Iteration 36/1000 | Loss: 0.00008901
Iteration 37/1000 | Loss: 0.00005669
Iteration 38/1000 | Loss: 0.00002589
Iteration 39/1000 | Loss: 0.00002421
Iteration 40/1000 | Loss: 0.00002332
Iteration 41/1000 | Loss: 0.00004450
Iteration 42/1000 | Loss: 0.00002197
Iteration 43/1000 | Loss: 0.00004305
Iteration 44/1000 | Loss: 0.00002161
Iteration 45/1000 | Loss: 0.00003871
Iteration 46/1000 | Loss: 0.00007058
Iteration 47/1000 | Loss: 0.00004955
Iteration 48/1000 | Loss: 0.00008283
Iteration 49/1000 | Loss: 0.00002985
Iteration 50/1000 | Loss: 0.00004425
Iteration 51/1000 | Loss: 0.00002468
Iteration 52/1000 | Loss: 0.00002757
Iteration 53/1000 | Loss: 0.00002134
Iteration 54/1000 | Loss: 0.00002134
Iteration 55/1000 | Loss: 0.00002134
Iteration 56/1000 | Loss: 0.00002134
Iteration 57/1000 | Loss: 0.00002134
Iteration 58/1000 | Loss: 0.00002134
Iteration 59/1000 | Loss: 0.00002134
Iteration 60/1000 | Loss: 0.00002134
Iteration 61/1000 | Loss: 0.00002134
Iteration 62/1000 | Loss: 0.00002134
Iteration 63/1000 | Loss: 0.00002134
Iteration 64/1000 | Loss: 0.00002134
Iteration 65/1000 | Loss: 0.00002134
Iteration 66/1000 | Loss: 0.00002134
Iteration 67/1000 | Loss: 0.00002134
Iteration 68/1000 | Loss: 0.00002134
Iteration 69/1000 | Loss: 0.00002134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [2.1338852093322203e-05, 2.1338852093322203e-05, 2.1338852093322203e-05, 2.1338852093322203e-05, 2.1338852093322203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1338852093322203e-05

Optimization complete. Final v2v error: 3.8774356842041016 mm

Highest mean error: 8.25802993774414 mm for frame 92

Lowest mean error: 3.5879974365234375 mm for frame 27

Saving results

Total time: 125.8840503692627
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951711
Iteration 2/25 | Loss: 0.00211634
Iteration 3/25 | Loss: 0.00128588
Iteration 4/25 | Loss: 0.00120218
Iteration 5/25 | Loss: 0.00113346
Iteration 6/25 | Loss: 0.00112530
Iteration 7/25 | Loss: 0.00111782
Iteration 8/25 | Loss: 0.00109747
Iteration 9/25 | Loss: 0.00108257
Iteration 10/25 | Loss: 0.00107850
Iteration 11/25 | Loss: 0.00107009
Iteration 12/25 | Loss: 0.00107176
Iteration 13/25 | Loss: 0.00106293
Iteration 14/25 | Loss: 0.00106227
Iteration 15/25 | Loss: 0.00106136
Iteration 16/25 | Loss: 0.00106101
Iteration 17/25 | Loss: 0.00106086
Iteration 18/25 | Loss: 0.00106085
Iteration 19/25 | Loss: 0.00106085
Iteration 20/25 | Loss: 0.00106085
Iteration 21/25 | Loss: 0.00106085
Iteration 22/25 | Loss: 0.00106085
Iteration 23/25 | Loss: 0.00106085
Iteration 24/25 | Loss: 0.00106085
Iteration 25/25 | Loss: 0.00106085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.29491115
Iteration 2/25 | Loss: 0.00066656
Iteration 3/25 | Loss: 0.00066656
Iteration 4/25 | Loss: 0.00066656
Iteration 5/25 | Loss: 0.00066656
Iteration 6/25 | Loss: 0.00066656
Iteration 7/25 | Loss: 0.00066656
Iteration 8/25 | Loss: 0.00066656
Iteration 9/25 | Loss: 0.00066656
Iteration 10/25 | Loss: 0.00066656
Iteration 11/25 | Loss: 0.00066656
Iteration 12/25 | Loss: 0.00066656
Iteration 13/25 | Loss: 0.00066656
Iteration 14/25 | Loss: 0.00066656
Iteration 15/25 | Loss: 0.00066656
Iteration 16/25 | Loss: 0.00066656
Iteration 17/25 | Loss: 0.00066656
Iteration 18/25 | Loss: 0.00066656
Iteration 19/25 | Loss: 0.00066656
Iteration 20/25 | Loss: 0.00066656
Iteration 21/25 | Loss: 0.00066656
Iteration 22/25 | Loss: 0.00066656
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006665566470474005, 0.0006665566470474005, 0.0006665566470474005, 0.0006665566470474005, 0.0006665566470474005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006665566470474005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066656
Iteration 2/1000 | Loss: 0.00006186
Iteration 3/1000 | Loss: 0.00004952
Iteration 4/1000 | Loss: 0.00004455
Iteration 5/1000 | Loss: 0.00004254
Iteration 6/1000 | Loss: 0.00004153
Iteration 7/1000 | Loss: 0.00004105
Iteration 8/1000 | Loss: 0.00004074
Iteration 9/1000 | Loss: 0.00004051
Iteration 10/1000 | Loss: 0.00004035
Iteration 11/1000 | Loss: 0.00004032
Iteration 12/1000 | Loss: 0.00004025
Iteration 13/1000 | Loss: 0.00004025
Iteration 14/1000 | Loss: 0.00004022
Iteration 15/1000 | Loss: 0.00004022
Iteration 16/1000 | Loss: 0.00004021
Iteration 17/1000 | Loss: 0.00004021
Iteration 18/1000 | Loss: 0.00004021
Iteration 19/1000 | Loss: 0.00004021
Iteration 20/1000 | Loss: 0.00004020
Iteration 21/1000 | Loss: 0.00004020
Iteration 22/1000 | Loss: 0.00004019
Iteration 23/1000 | Loss: 0.00004018
Iteration 24/1000 | Loss: 0.00004018
Iteration 25/1000 | Loss: 0.00004018
Iteration 26/1000 | Loss: 0.00004018
Iteration 27/1000 | Loss: 0.00004017
Iteration 28/1000 | Loss: 0.00004017
Iteration 29/1000 | Loss: 0.00004017
Iteration 30/1000 | Loss: 0.00004017
Iteration 31/1000 | Loss: 0.00004017
Iteration 32/1000 | Loss: 0.00004016
Iteration 33/1000 | Loss: 0.00004016
Iteration 34/1000 | Loss: 0.00004016
Iteration 35/1000 | Loss: 0.00004016
Iteration 36/1000 | Loss: 0.00004016
Iteration 37/1000 | Loss: 0.00004015
Iteration 38/1000 | Loss: 0.00004015
Iteration 39/1000 | Loss: 0.00004015
Iteration 40/1000 | Loss: 0.00004015
Iteration 41/1000 | Loss: 0.00004015
Iteration 42/1000 | Loss: 0.00004014
Iteration 43/1000 | Loss: 0.00004014
Iteration 44/1000 | Loss: 0.00004014
Iteration 45/1000 | Loss: 0.00004014
Iteration 46/1000 | Loss: 0.00004014
Iteration 47/1000 | Loss: 0.00004014
Iteration 48/1000 | Loss: 0.00004014
Iteration 49/1000 | Loss: 0.00004014
Iteration 50/1000 | Loss: 0.00004014
Iteration 51/1000 | Loss: 0.00004014
Iteration 52/1000 | Loss: 0.00004014
Iteration 53/1000 | Loss: 0.00004013
Iteration 54/1000 | Loss: 0.00004013
Iteration 55/1000 | Loss: 0.00004013
Iteration 56/1000 | Loss: 0.00004013
Iteration 57/1000 | Loss: 0.00004013
Iteration 58/1000 | Loss: 0.00004013
Iteration 59/1000 | Loss: 0.00004012
Iteration 60/1000 | Loss: 0.00004012
Iteration 61/1000 | Loss: 0.00004012
Iteration 62/1000 | Loss: 0.00004012
Iteration 63/1000 | Loss: 0.00004012
Iteration 64/1000 | Loss: 0.00004012
Iteration 65/1000 | Loss: 0.00004012
Iteration 66/1000 | Loss: 0.00004012
Iteration 67/1000 | Loss: 0.00004012
Iteration 68/1000 | Loss: 0.00004012
Iteration 69/1000 | Loss: 0.00004011
Iteration 70/1000 | Loss: 0.00004011
Iteration 71/1000 | Loss: 0.00004011
Iteration 72/1000 | Loss: 0.00004011
Iteration 73/1000 | Loss: 0.00004011
Iteration 74/1000 | Loss: 0.00004011
Iteration 75/1000 | Loss: 0.00004011
Iteration 76/1000 | Loss: 0.00004011
Iteration 77/1000 | Loss: 0.00004011
Iteration 78/1000 | Loss: 0.00004011
Iteration 79/1000 | Loss: 0.00004010
Iteration 80/1000 | Loss: 0.00004010
Iteration 81/1000 | Loss: 0.00004010
Iteration 82/1000 | Loss: 0.00004010
Iteration 83/1000 | Loss: 0.00004009
Iteration 84/1000 | Loss: 0.00004009
Iteration 85/1000 | Loss: 0.00004009
Iteration 86/1000 | Loss: 0.00004009
Iteration 87/1000 | Loss: 0.00004009
Iteration 88/1000 | Loss: 0.00004009
Iteration 89/1000 | Loss: 0.00004009
Iteration 90/1000 | Loss: 0.00004009
Iteration 91/1000 | Loss: 0.00004009
Iteration 92/1000 | Loss: 0.00004009
Iteration 93/1000 | Loss: 0.00008837
Iteration 94/1000 | Loss: 0.00004088
Iteration 95/1000 | Loss: 0.00004042
Iteration 96/1000 | Loss: 0.00004040
Iteration 97/1000 | Loss: 0.00005614
Iteration 98/1000 | Loss: 0.00004016
Iteration 99/1000 | Loss: 0.00004012
Iteration 100/1000 | Loss: 0.00007160
Iteration 101/1000 | Loss: 0.00004024
Iteration 102/1000 | Loss: 0.00004017
Iteration 103/1000 | Loss: 0.00004017
Iteration 104/1000 | Loss: 0.00004017
Iteration 105/1000 | Loss: 0.00004017
Iteration 106/1000 | Loss: 0.00004017
Iteration 107/1000 | Loss: 0.00004016
Iteration 108/1000 | Loss: 0.00004016
Iteration 109/1000 | Loss: 0.00004015
Iteration 110/1000 | Loss: 0.00004015
Iteration 111/1000 | Loss: 0.00004015
Iteration 112/1000 | Loss: 0.00004014
Iteration 113/1000 | Loss: 0.00004014
Iteration 114/1000 | Loss: 0.00004014
Iteration 115/1000 | Loss: 0.00004014
Iteration 116/1000 | Loss: 0.00004014
Iteration 117/1000 | Loss: 0.00004014
Iteration 118/1000 | Loss: 0.00004014
Iteration 119/1000 | Loss: 0.00004014
Iteration 120/1000 | Loss: 0.00004014
Iteration 121/1000 | Loss: 0.00004014
Iteration 122/1000 | Loss: 0.00004014
Iteration 123/1000 | Loss: 0.00004014
Iteration 124/1000 | Loss: 0.00004013
Iteration 125/1000 | Loss: 0.00004013
Iteration 126/1000 | Loss: 0.00004013
Iteration 127/1000 | Loss: 0.00004013
Iteration 128/1000 | Loss: 0.00004013
Iteration 129/1000 | Loss: 0.00004013
Iteration 130/1000 | Loss: 0.00004013
Iteration 131/1000 | Loss: 0.00004013
Iteration 132/1000 | Loss: 0.00004013
Iteration 133/1000 | Loss: 0.00004013
Iteration 134/1000 | Loss: 0.00004013
Iteration 135/1000 | Loss: 0.00004013
Iteration 136/1000 | Loss: 0.00004013
Iteration 137/1000 | Loss: 0.00004013
Iteration 138/1000 | Loss: 0.00004013
Iteration 139/1000 | Loss: 0.00004013
Iteration 140/1000 | Loss: 0.00004013
Iteration 141/1000 | Loss: 0.00004013
Iteration 142/1000 | Loss: 0.00004012
Iteration 143/1000 | Loss: 0.00004012
Iteration 144/1000 | Loss: 0.00004012
Iteration 145/1000 | Loss: 0.00004012
Iteration 146/1000 | Loss: 0.00004012
Iteration 147/1000 | Loss: 0.00004012
Iteration 148/1000 | Loss: 0.00004012
Iteration 149/1000 | Loss: 0.00004012
Iteration 150/1000 | Loss: 0.00004012
Iteration 151/1000 | Loss: 0.00004012
Iteration 152/1000 | Loss: 0.00004012
Iteration 153/1000 | Loss: 0.00004012
Iteration 154/1000 | Loss: 0.00004012
Iteration 155/1000 | Loss: 0.00004012
Iteration 156/1000 | Loss: 0.00004012
Iteration 157/1000 | Loss: 0.00004012
Iteration 158/1000 | Loss: 0.00004012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [4.012439967482351e-05, 4.012439967482351e-05, 4.012439967482351e-05, 4.012439967482351e-05, 4.012439967482351e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.012439967482351e-05

Optimization complete. Final v2v error: 4.829099178314209 mm

Highest mean error: 21.126787185668945 mm for frame 2

Lowest mean error: 4.222800254821777 mm for frame 104

Saving results

Total time: 70.84790587425232
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00547445
Iteration 2/25 | Loss: 0.00136697
Iteration 3/25 | Loss: 0.00115051
Iteration 4/25 | Loss: 0.00109031
Iteration 5/25 | Loss: 0.00107230
Iteration 6/25 | Loss: 0.00107076
Iteration 7/25 | Loss: 0.00107056
Iteration 8/25 | Loss: 0.00107056
Iteration 9/25 | Loss: 0.00107056
Iteration 10/25 | Loss: 0.00107056
Iteration 11/25 | Loss: 0.00107056
Iteration 12/25 | Loss: 0.00107056
Iteration 13/25 | Loss: 0.00107056
Iteration 14/25 | Loss: 0.00107056
Iteration 15/25 | Loss: 0.00107056
Iteration 16/25 | Loss: 0.00107056
Iteration 17/25 | Loss: 0.00107056
Iteration 18/25 | Loss: 0.00107056
Iteration 19/25 | Loss: 0.00107056
Iteration 20/25 | Loss: 0.00107056
Iteration 21/25 | Loss: 0.00107056
Iteration 22/25 | Loss: 0.00107056
Iteration 23/25 | Loss: 0.00107056
Iteration 24/25 | Loss: 0.00107056
Iteration 25/25 | Loss: 0.00107056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38366830
Iteration 2/25 | Loss: 0.00063920
Iteration 3/25 | Loss: 0.00063919
Iteration 4/25 | Loss: 0.00063919
Iteration 5/25 | Loss: 0.00063919
Iteration 6/25 | Loss: 0.00063919
Iteration 7/25 | Loss: 0.00063919
Iteration 8/25 | Loss: 0.00063919
Iteration 9/25 | Loss: 0.00063919
Iteration 10/25 | Loss: 0.00063919
Iteration 11/25 | Loss: 0.00063919
Iteration 12/25 | Loss: 0.00063919
Iteration 13/25 | Loss: 0.00063919
Iteration 14/25 | Loss: 0.00063919
Iteration 15/25 | Loss: 0.00063919
Iteration 16/25 | Loss: 0.00063919
Iteration 17/25 | Loss: 0.00063919
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006391872302629054, 0.0006391872302629054, 0.0006391872302629054, 0.0006391872302629054, 0.0006391872302629054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006391872302629054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063919
Iteration 2/1000 | Loss: 0.00004346
Iteration 3/1000 | Loss: 0.00003245
Iteration 4/1000 | Loss: 0.00002851
Iteration 5/1000 | Loss: 0.00002619
Iteration 6/1000 | Loss: 0.00002479
Iteration 7/1000 | Loss: 0.00002407
Iteration 8/1000 | Loss: 0.00002360
Iteration 9/1000 | Loss: 0.00002346
Iteration 10/1000 | Loss: 0.00002329
Iteration 11/1000 | Loss: 0.00002326
Iteration 12/1000 | Loss: 0.00002314
Iteration 13/1000 | Loss: 0.00002313
Iteration 14/1000 | Loss: 0.00002312
Iteration 15/1000 | Loss: 0.00002309
Iteration 16/1000 | Loss: 0.00002307
Iteration 17/1000 | Loss: 0.00002304
Iteration 18/1000 | Loss: 0.00002301
Iteration 19/1000 | Loss: 0.00002301
Iteration 20/1000 | Loss: 0.00002300
Iteration 21/1000 | Loss: 0.00002300
Iteration 22/1000 | Loss: 0.00002300
Iteration 23/1000 | Loss: 0.00002300
Iteration 24/1000 | Loss: 0.00002300
Iteration 25/1000 | Loss: 0.00002299
Iteration 26/1000 | Loss: 0.00002299
Iteration 27/1000 | Loss: 0.00002299
Iteration 28/1000 | Loss: 0.00002298
Iteration 29/1000 | Loss: 0.00002298
Iteration 30/1000 | Loss: 0.00002298
Iteration 31/1000 | Loss: 0.00002297
Iteration 32/1000 | Loss: 0.00002297
Iteration 33/1000 | Loss: 0.00002297
Iteration 34/1000 | Loss: 0.00002297
Iteration 35/1000 | Loss: 0.00002296
Iteration 36/1000 | Loss: 0.00002296
Iteration 37/1000 | Loss: 0.00002295
Iteration 38/1000 | Loss: 0.00002295
Iteration 39/1000 | Loss: 0.00002295
Iteration 40/1000 | Loss: 0.00002295
Iteration 41/1000 | Loss: 0.00002294
Iteration 42/1000 | Loss: 0.00002294
Iteration 43/1000 | Loss: 0.00002294
Iteration 44/1000 | Loss: 0.00002294
Iteration 45/1000 | Loss: 0.00002293
Iteration 46/1000 | Loss: 0.00002293
Iteration 47/1000 | Loss: 0.00002293
Iteration 48/1000 | Loss: 0.00002293
Iteration 49/1000 | Loss: 0.00002292
Iteration 50/1000 | Loss: 0.00002292
Iteration 51/1000 | Loss: 0.00002292
Iteration 52/1000 | Loss: 0.00002292
Iteration 53/1000 | Loss: 0.00002292
Iteration 54/1000 | Loss: 0.00002292
Iteration 55/1000 | Loss: 0.00002291
Iteration 56/1000 | Loss: 0.00002291
Iteration 57/1000 | Loss: 0.00002291
Iteration 58/1000 | Loss: 0.00002291
Iteration 59/1000 | Loss: 0.00002290
Iteration 60/1000 | Loss: 0.00002290
Iteration 61/1000 | Loss: 0.00002290
Iteration 62/1000 | Loss: 0.00002290
Iteration 63/1000 | Loss: 0.00002290
Iteration 64/1000 | Loss: 0.00002290
Iteration 65/1000 | Loss: 0.00002290
Iteration 66/1000 | Loss: 0.00002290
Iteration 67/1000 | Loss: 0.00002290
Iteration 68/1000 | Loss: 0.00002290
Iteration 69/1000 | Loss: 0.00002290
Iteration 70/1000 | Loss: 0.00002290
Iteration 71/1000 | Loss: 0.00002290
Iteration 72/1000 | Loss: 0.00002290
Iteration 73/1000 | Loss: 0.00002289
Iteration 74/1000 | Loss: 0.00002289
Iteration 75/1000 | Loss: 0.00002289
Iteration 76/1000 | Loss: 0.00002289
Iteration 77/1000 | Loss: 0.00002289
Iteration 78/1000 | Loss: 0.00002289
Iteration 79/1000 | Loss: 0.00002289
Iteration 80/1000 | Loss: 0.00002289
Iteration 81/1000 | Loss: 0.00002289
Iteration 82/1000 | Loss: 0.00002289
Iteration 83/1000 | Loss: 0.00002289
Iteration 84/1000 | Loss: 0.00002289
Iteration 85/1000 | Loss: 0.00002289
Iteration 86/1000 | Loss: 0.00002289
Iteration 87/1000 | Loss: 0.00002289
Iteration 88/1000 | Loss: 0.00002289
Iteration 89/1000 | Loss: 0.00002289
Iteration 90/1000 | Loss: 0.00002288
Iteration 91/1000 | Loss: 0.00002288
Iteration 92/1000 | Loss: 0.00002288
Iteration 93/1000 | Loss: 0.00002288
Iteration 94/1000 | Loss: 0.00002288
Iteration 95/1000 | Loss: 0.00002288
Iteration 96/1000 | Loss: 0.00002288
Iteration 97/1000 | Loss: 0.00002288
Iteration 98/1000 | Loss: 0.00002288
Iteration 99/1000 | Loss: 0.00002288
Iteration 100/1000 | Loss: 0.00002288
Iteration 101/1000 | Loss: 0.00002288
Iteration 102/1000 | Loss: 0.00002288
Iteration 103/1000 | Loss: 0.00002288
Iteration 104/1000 | Loss: 0.00002288
Iteration 105/1000 | Loss: 0.00002288
Iteration 106/1000 | Loss: 0.00002288
Iteration 107/1000 | Loss: 0.00002288
Iteration 108/1000 | Loss: 0.00002288
Iteration 109/1000 | Loss: 0.00002288
Iteration 110/1000 | Loss: 0.00002287
Iteration 111/1000 | Loss: 0.00002287
Iteration 112/1000 | Loss: 0.00002287
Iteration 113/1000 | Loss: 0.00002287
Iteration 114/1000 | Loss: 0.00002287
Iteration 115/1000 | Loss: 0.00002287
Iteration 116/1000 | Loss: 0.00002287
Iteration 117/1000 | Loss: 0.00002287
Iteration 118/1000 | Loss: 0.00002287
Iteration 119/1000 | Loss: 0.00002287
Iteration 120/1000 | Loss: 0.00002287
Iteration 121/1000 | Loss: 0.00002287
Iteration 122/1000 | Loss: 0.00002287
Iteration 123/1000 | Loss: 0.00002287
Iteration 124/1000 | Loss: 0.00002287
Iteration 125/1000 | Loss: 0.00002286
Iteration 126/1000 | Loss: 0.00002286
Iteration 127/1000 | Loss: 0.00002286
Iteration 128/1000 | Loss: 0.00002286
Iteration 129/1000 | Loss: 0.00002286
Iteration 130/1000 | Loss: 0.00002286
Iteration 131/1000 | Loss: 0.00002286
Iteration 132/1000 | Loss: 0.00002286
Iteration 133/1000 | Loss: 0.00002286
Iteration 134/1000 | Loss: 0.00002286
Iteration 135/1000 | Loss: 0.00002286
Iteration 136/1000 | Loss: 0.00002286
Iteration 137/1000 | Loss: 0.00002286
Iteration 138/1000 | Loss: 0.00002286
Iteration 139/1000 | Loss: 0.00002286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [2.2861893739900552e-05, 2.2861893739900552e-05, 2.2861893739900552e-05, 2.2861893739900552e-05, 2.2861893739900552e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2861893739900552e-05

Optimization complete. Final v2v error: 4.083561420440674 mm

Highest mean error: 4.346026420593262 mm for frame 97

Lowest mean error: 3.8893396854400635 mm for frame 10

Saving results

Total time: 33.448569774627686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00524171
Iteration 2/25 | Loss: 0.00133551
Iteration 3/25 | Loss: 0.00106278
Iteration 4/25 | Loss: 0.00102441
Iteration 5/25 | Loss: 0.00100737
Iteration 6/25 | Loss: 0.00100295
Iteration 7/25 | Loss: 0.00100101
Iteration 8/25 | Loss: 0.00100101
Iteration 9/25 | Loss: 0.00100101
Iteration 10/25 | Loss: 0.00100101
Iteration 11/25 | Loss: 0.00100101
Iteration 12/25 | Loss: 0.00100101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001001014024950564, 0.001001014024950564, 0.001001014024950564, 0.001001014024950564, 0.001001014024950564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001001014024950564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79929811
Iteration 2/25 | Loss: 0.00065209
Iteration 3/25 | Loss: 0.00065208
Iteration 4/25 | Loss: 0.00065208
Iteration 5/25 | Loss: 0.00065208
Iteration 6/25 | Loss: 0.00065208
Iteration 7/25 | Loss: 0.00065208
Iteration 8/25 | Loss: 0.00065208
Iteration 9/25 | Loss: 0.00065208
Iteration 10/25 | Loss: 0.00065208
Iteration 11/25 | Loss: 0.00065208
Iteration 12/25 | Loss: 0.00065208
Iteration 13/25 | Loss: 0.00065208
Iteration 14/25 | Loss: 0.00065208
Iteration 15/25 | Loss: 0.00065208
Iteration 16/25 | Loss: 0.00065208
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006520827882923186, 0.0006520827882923186, 0.0006520827882923186, 0.0006520827882923186, 0.0006520827882923186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006520827882923186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065208
Iteration 2/1000 | Loss: 0.00003798
Iteration 3/1000 | Loss: 0.00002749
Iteration 4/1000 | Loss: 0.00002399
Iteration 5/1000 | Loss: 0.00002238
Iteration 6/1000 | Loss: 0.00002148
Iteration 7/1000 | Loss: 0.00002116
Iteration 8/1000 | Loss: 0.00002088
Iteration 9/1000 | Loss: 0.00002072
Iteration 10/1000 | Loss: 0.00002048
Iteration 11/1000 | Loss: 0.00002043
Iteration 12/1000 | Loss: 0.00002036
Iteration 13/1000 | Loss: 0.00002034
Iteration 14/1000 | Loss: 0.00002033
Iteration 15/1000 | Loss: 0.00002032
Iteration 16/1000 | Loss: 0.00002032
Iteration 17/1000 | Loss: 0.00002031
Iteration 18/1000 | Loss: 0.00002031
Iteration 19/1000 | Loss: 0.00002031
Iteration 20/1000 | Loss: 0.00002031
Iteration 21/1000 | Loss: 0.00002031
Iteration 22/1000 | Loss: 0.00002030
Iteration 23/1000 | Loss: 0.00002030
Iteration 24/1000 | Loss: 0.00002030
Iteration 25/1000 | Loss: 0.00002030
Iteration 26/1000 | Loss: 0.00002030
Iteration 27/1000 | Loss: 0.00002030
Iteration 28/1000 | Loss: 0.00002030
Iteration 29/1000 | Loss: 0.00002030
Iteration 30/1000 | Loss: 0.00002030
Iteration 31/1000 | Loss: 0.00002028
Iteration 32/1000 | Loss: 0.00002028
Iteration 33/1000 | Loss: 0.00002028
Iteration 34/1000 | Loss: 0.00002028
Iteration 35/1000 | Loss: 0.00002028
Iteration 36/1000 | Loss: 0.00002028
Iteration 37/1000 | Loss: 0.00002028
Iteration 38/1000 | Loss: 0.00002028
Iteration 39/1000 | Loss: 0.00002028
Iteration 40/1000 | Loss: 0.00002028
Iteration 41/1000 | Loss: 0.00002027
Iteration 42/1000 | Loss: 0.00002027
Iteration 43/1000 | Loss: 0.00002027
Iteration 44/1000 | Loss: 0.00002027
Iteration 45/1000 | Loss: 0.00002027
Iteration 46/1000 | Loss: 0.00002026
Iteration 47/1000 | Loss: 0.00002025
Iteration 48/1000 | Loss: 0.00002025
Iteration 49/1000 | Loss: 0.00002025
Iteration 50/1000 | Loss: 0.00002025
Iteration 51/1000 | Loss: 0.00002025
Iteration 52/1000 | Loss: 0.00002025
Iteration 53/1000 | Loss: 0.00002025
Iteration 54/1000 | Loss: 0.00002025
Iteration 55/1000 | Loss: 0.00002025
Iteration 56/1000 | Loss: 0.00002025
Iteration 57/1000 | Loss: 0.00002025
Iteration 58/1000 | Loss: 0.00002025
Iteration 59/1000 | Loss: 0.00002025
Iteration 60/1000 | Loss: 0.00002025
Iteration 61/1000 | Loss: 0.00002025
Iteration 62/1000 | Loss: 0.00002025
Iteration 63/1000 | Loss: 0.00002025
Iteration 64/1000 | Loss: 0.00002025
Iteration 65/1000 | Loss: 0.00002025
Iteration 66/1000 | Loss: 0.00002025
Iteration 67/1000 | Loss: 0.00002025
Iteration 68/1000 | Loss: 0.00002025
Iteration 69/1000 | Loss: 0.00002025
Iteration 70/1000 | Loss: 0.00002025
Iteration 71/1000 | Loss: 0.00002025
Iteration 72/1000 | Loss: 0.00002025
Iteration 73/1000 | Loss: 0.00002025
Iteration 74/1000 | Loss: 0.00002025
Iteration 75/1000 | Loss: 0.00002025
Iteration 76/1000 | Loss: 0.00002025
Iteration 77/1000 | Loss: 0.00002025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [2.0247456632205285e-05, 2.0247456632205285e-05, 2.0247456632205285e-05, 2.0247456632205285e-05, 2.0247456632205285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0247456632205285e-05

Optimization complete. Final v2v error: 3.8656370639801025 mm

Highest mean error: 4.226383686065674 mm for frame 4

Lowest mean error: 3.5118520259857178 mm for frame 31

Saving results

Total time: 64.83891940116882
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00915494
Iteration 2/25 | Loss: 0.00113454
Iteration 3/25 | Loss: 0.00097680
Iteration 4/25 | Loss: 0.00096038
Iteration 5/25 | Loss: 0.00095635
Iteration 6/25 | Loss: 0.00095487
Iteration 7/25 | Loss: 0.00095487
Iteration 8/25 | Loss: 0.00095487
Iteration 9/25 | Loss: 0.00095487
Iteration 10/25 | Loss: 0.00095487
Iteration 11/25 | Loss: 0.00095487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009548670495860279, 0.0009548670495860279, 0.0009548670495860279, 0.0009548670495860279, 0.0009548670495860279]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009548670495860279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43219173
Iteration 2/25 | Loss: 0.00053835
Iteration 3/25 | Loss: 0.00053835
Iteration 4/25 | Loss: 0.00053834
Iteration 5/25 | Loss: 0.00053834
Iteration 6/25 | Loss: 0.00053834
Iteration 7/25 | Loss: 0.00053834
Iteration 8/25 | Loss: 0.00053834
Iteration 9/25 | Loss: 0.00053834
Iteration 10/25 | Loss: 0.00053834
Iteration 11/25 | Loss: 0.00053834
Iteration 12/25 | Loss: 0.00053834
Iteration 13/25 | Loss: 0.00053834
Iteration 14/25 | Loss: 0.00053834
Iteration 15/25 | Loss: 0.00053834
Iteration 16/25 | Loss: 0.00053834
Iteration 17/25 | Loss: 0.00053834
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005383424577303231, 0.0005383424577303231, 0.0005383424577303231, 0.0005383424577303231, 0.0005383424577303231]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005383424577303231

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053834
Iteration 2/1000 | Loss: 0.00003561
Iteration 3/1000 | Loss: 0.00002793
Iteration 4/1000 | Loss: 0.00002419
Iteration 5/1000 | Loss: 0.00002261
Iteration 6/1000 | Loss: 0.00002155
Iteration 7/1000 | Loss: 0.00002094
Iteration 8/1000 | Loss: 0.00002062
Iteration 9/1000 | Loss: 0.00002030
Iteration 10/1000 | Loss: 0.00002011
Iteration 11/1000 | Loss: 0.00002011
Iteration 12/1000 | Loss: 0.00002009
Iteration 13/1000 | Loss: 0.00002009
Iteration 14/1000 | Loss: 0.00002008
Iteration 15/1000 | Loss: 0.00002007
Iteration 16/1000 | Loss: 0.00002006
Iteration 17/1000 | Loss: 0.00002002
Iteration 18/1000 | Loss: 0.00002001
Iteration 19/1000 | Loss: 0.00002001
Iteration 20/1000 | Loss: 0.00002001
Iteration 21/1000 | Loss: 0.00002000
Iteration 22/1000 | Loss: 0.00002000
Iteration 23/1000 | Loss: 0.00001999
Iteration 24/1000 | Loss: 0.00001999
Iteration 25/1000 | Loss: 0.00001999
Iteration 26/1000 | Loss: 0.00001998
Iteration 27/1000 | Loss: 0.00001998
Iteration 28/1000 | Loss: 0.00001998
Iteration 29/1000 | Loss: 0.00001998
Iteration 30/1000 | Loss: 0.00001998
Iteration 31/1000 | Loss: 0.00001998
Iteration 32/1000 | Loss: 0.00001998
Iteration 33/1000 | Loss: 0.00001998
Iteration 34/1000 | Loss: 0.00001998
Iteration 35/1000 | Loss: 0.00001998
Iteration 36/1000 | Loss: 0.00001998
Iteration 37/1000 | Loss: 0.00001997
Iteration 38/1000 | Loss: 0.00001997
Iteration 39/1000 | Loss: 0.00001997
Iteration 40/1000 | Loss: 0.00001996
Iteration 41/1000 | Loss: 0.00001995
Iteration 42/1000 | Loss: 0.00001995
Iteration 43/1000 | Loss: 0.00001995
Iteration 44/1000 | Loss: 0.00001994
Iteration 45/1000 | Loss: 0.00001994
Iteration 46/1000 | Loss: 0.00001992
Iteration 47/1000 | Loss: 0.00001992
Iteration 48/1000 | Loss: 0.00001991
Iteration 49/1000 | Loss: 0.00001991
Iteration 50/1000 | Loss: 0.00001991
Iteration 51/1000 | Loss: 0.00001991
Iteration 52/1000 | Loss: 0.00001991
Iteration 53/1000 | Loss: 0.00001990
Iteration 54/1000 | Loss: 0.00001990
Iteration 55/1000 | Loss: 0.00001990
Iteration 56/1000 | Loss: 0.00001990
Iteration 57/1000 | Loss: 0.00001990
Iteration 58/1000 | Loss: 0.00001989
Iteration 59/1000 | Loss: 0.00001989
Iteration 60/1000 | Loss: 0.00001989
Iteration 61/1000 | Loss: 0.00001988
Iteration 62/1000 | Loss: 0.00001988
Iteration 63/1000 | Loss: 0.00001987
Iteration 64/1000 | Loss: 0.00001987
Iteration 65/1000 | Loss: 0.00001987
Iteration 66/1000 | Loss: 0.00001986
Iteration 67/1000 | Loss: 0.00001986
Iteration 68/1000 | Loss: 0.00001986
Iteration 69/1000 | Loss: 0.00001985
Iteration 70/1000 | Loss: 0.00001985
Iteration 71/1000 | Loss: 0.00001984
Iteration 72/1000 | Loss: 0.00001984
Iteration 73/1000 | Loss: 0.00001984
Iteration 74/1000 | Loss: 0.00001984
Iteration 75/1000 | Loss: 0.00001984
Iteration 76/1000 | Loss: 0.00001984
Iteration 77/1000 | Loss: 0.00001983
Iteration 78/1000 | Loss: 0.00001983
Iteration 79/1000 | Loss: 0.00001982
Iteration 80/1000 | Loss: 0.00001982
Iteration 81/1000 | Loss: 0.00001982
Iteration 82/1000 | Loss: 0.00001982
Iteration 83/1000 | Loss: 0.00001981
Iteration 84/1000 | Loss: 0.00001981
Iteration 85/1000 | Loss: 0.00001981
Iteration 86/1000 | Loss: 0.00001981
Iteration 87/1000 | Loss: 0.00001981
Iteration 88/1000 | Loss: 0.00001981
Iteration 89/1000 | Loss: 0.00001981
Iteration 90/1000 | Loss: 0.00001981
Iteration 91/1000 | Loss: 0.00001981
Iteration 92/1000 | Loss: 0.00001981
Iteration 93/1000 | Loss: 0.00001981
Iteration 94/1000 | Loss: 0.00001981
Iteration 95/1000 | Loss: 0.00001980
Iteration 96/1000 | Loss: 0.00001980
Iteration 97/1000 | Loss: 0.00001980
Iteration 98/1000 | Loss: 0.00001980
Iteration 99/1000 | Loss: 0.00001979
Iteration 100/1000 | Loss: 0.00001979
Iteration 101/1000 | Loss: 0.00001979
Iteration 102/1000 | Loss: 0.00001979
Iteration 103/1000 | Loss: 0.00001979
Iteration 104/1000 | Loss: 0.00001979
Iteration 105/1000 | Loss: 0.00001979
Iteration 106/1000 | Loss: 0.00001979
Iteration 107/1000 | Loss: 0.00001979
Iteration 108/1000 | Loss: 0.00001979
Iteration 109/1000 | Loss: 0.00001979
Iteration 110/1000 | Loss: 0.00001979
Iteration 111/1000 | Loss: 0.00001978
Iteration 112/1000 | Loss: 0.00001978
Iteration 113/1000 | Loss: 0.00001978
Iteration 114/1000 | Loss: 0.00001978
Iteration 115/1000 | Loss: 0.00001978
Iteration 116/1000 | Loss: 0.00001978
Iteration 117/1000 | Loss: 0.00001978
Iteration 118/1000 | Loss: 0.00001978
Iteration 119/1000 | Loss: 0.00001978
Iteration 120/1000 | Loss: 0.00001977
Iteration 121/1000 | Loss: 0.00001977
Iteration 122/1000 | Loss: 0.00001977
Iteration 123/1000 | Loss: 0.00001977
Iteration 124/1000 | Loss: 0.00001977
Iteration 125/1000 | Loss: 0.00001977
Iteration 126/1000 | Loss: 0.00001977
Iteration 127/1000 | Loss: 0.00001977
Iteration 128/1000 | Loss: 0.00001977
Iteration 129/1000 | Loss: 0.00001976
Iteration 130/1000 | Loss: 0.00001976
Iteration 131/1000 | Loss: 0.00001976
Iteration 132/1000 | Loss: 0.00001976
Iteration 133/1000 | Loss: 0.00001976
Iteration 134/1000 | Loss: 0.00001976
Iteration 135/1000 | Loss: 0.00001976
Iteration 136/1000 | Loss: 0.00001976
Iteration 137/1000 | Loss: 0.00001976
Iteration 138/1000 | Loss: 0.00001976
Iteration 139/1000 | Loss: 0.00001976
Iteration 140/1000 | Loss: 0.00001976
Iteration 141/1000 | Loss: 0.00001976
Iteration 142/1000 | Loss: 0.00001976
Iteration 143/1000 | Loss: 0.00001976
Iteration 144/1000 | Loss: 0.00001975
Iteration 145/1000 | Loss: 0.00001975
Iteration 146/1000 | Loss: 0.00001975
Iteration 147/1000 | Loss: 0.00001975
Iteration 148/1000 | Loss: 0.00001975
Iteration 149/1000 | Loss: 0.00001975
Iteration 150/1000 | Loss: 0.00001974
Iteration 151/1000 | Loss: 0.00001974
Iteration 152/1000 | Loss: 0.00001974
Iteration 153/1000 | Loss: 0.00001974
Iteration 154/1000 | Loss: 0.00001974
Iteration 155/1000 | Loss: 0.00001974
Iteration 156/1000 | Loss: 0.00001974
Iteration 157/1000 | Loss: 0.00001974
Iteration 158/1000 | Loss: 0.00001974
Iteration 159/1000 | Loss: 0.00001974
Iteration 160/1000 | Loss: 0.00001974
Iteration 161/1000 | Loss: 0.00001974
Iteration 162/1000 | Loss: 0.00001974
Iteration 163/1000 | Loss: 0.00001974
Iteration 164/1000 | Loss: 0.00001974
Iteration 165/1000 | Loss: 0.00001974
Iteration 166/1000 | Loss: 0.00001974
Iteration 167/1000 | Loss: 0.00001974
Iteration 168/1000 | Loss: 0.00001974
Iteration 169/1000 | Loss: 0.00001974
Iteration 170/1000 | Loss: 0.00001974
Iteration 171/1000 | Loss: 0.00001974
Iteration 172/1000 | Loss: 0.00001974
Iteration 173/1000 | Loss: 0.00001974
Iteration 174/1000 | Loss: 0.00001974
Iteration 175/1000 | Loss: 0.00001974
Iteration 176/1000 | Loss: 0.00001974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.9742064978345297e-05, 1.9742064978345297e-05, 1.9742064978345297e-05, 1.9742064978345297e-05, 1.9742064978345297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9742064978345297e-05

Optimization complete. Final v2v error: 3.835479259490967 mm

Highest mean error: 4.262694358825684 mm for frame 143

Lowest mean error: 3.5239803791046143 mm for frame 227

Saving results

Total time: 40.427632331848145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816504
Iteration 2/25 | Loss: 0.00196206
Iteration 3/25 | Loss: 0.00118617
Iteration 4/25 | Loss: 0.00109462
Iteration 5/25 | Loss: 0.00104846
Iteration 6/25 | Loss: 0.00103341
Iteration 7/25 | Loss: 0.00102765
Iteration 8/25 | Loss: 0.00102376
Iteration 9/25 | Loss: 0.00102680
Iteration 10/25 | Loss: 0.00102226
Iteration 11/25 | Loss: 0.00101797
Iteration 12/25 | Loss: 0.00100880
Iteration 13/25 | Loss: 0.00100789
Iteration 14/25 | Loss: 0.00100580
Iteration 15/25 | Loss: 0.00100409
Iteration 16/25 | Loss: 0.00099966
Iteration 17/25 | Loss: 0.00099505
Iteration 18/25 | Loss: 0.00099421
Iteration 19/25 | Loss: 0.00099390
Iteration 20/25 | Loss: 0.00099659
Iteration 21/25 | Loss: 0.00099782
Iteration 22/25 | Loss: 0.00099389
Iteration 23/25 | Loss: 0.00099593
Iteration 24/25 | Loss: 0.00099193
Iteration 25/25 | Loss: 0.00099102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.63614464
Iteration 2/25 | Loss: 0.00056534
Iteration 3/25 | Loss: 0.00056531
Iteration 4/25 | Loss: 0.00056531
Iteration 5/25 | Loss: 0.00056531
Iteration 6/25 | Loss: 0.00056531
Iteration 7/25 | Loss: 0.00056530
Iteration 8/25 | Loss: 0.00056530
Iteration 9/25 | Loss: 0.00056530
Iteration 10/25 | Loss: 0.00056530
Iteration 11/25 | Loss: 0.00056530
Iteration 12/25 | Loss: 0.00056530
Iteration 13/25 | Loss: 0.00056530
Iteration 14/25 | Loss: 0.00056530
Iteration 15/25 | Loss: 0.00056530
Iteration 16/25 | Loss: 0.00056530
Iteration 17/25 | Loss: 0.00056530
Iteration 18/25 | Loss: 0.00056530
Iteration 19/25 | Loss: 0.00056530
Iteration 20/25 | Loss: 0.00056530
Iteration 21/25 | Loss: 0.00056530
Iteration 22/25 | Loss: 0.00056530
Iteration 23/25 | Loss: 0.00056530
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005653037806041539, 0.0005653037806041539, 0.0005653037806041539, 0.0005653037806041539, 0.0005653037806041539]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005653037806041539

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056530
Iteration 2/1000 | Loss: 0.00004570
Iteration 3/1000 | Loss: 0.00003126
Iteration 4/1000 | Loss: 0.00002775
Iteration 5/1000 | Loss: 0.00002526
Iteration 6/1000 | Loss: 0.00002393
Iteration 7/1000 | Loss: 0.00002338
Iteration 8/1000 | Loss: 0.00002296
Iteration 9/1000 | Loss: 0.00020205
Iteration 10/1000 | Loss: 0.00003018
Iteration 11/1000 | Loss: 0.00002427
Iteration 12/1000 | Loss: 0.00002259
Iteration 13/1000 | Loss: 0.00002175
Iteration 14/1000 | Loss: 0.00002149
Iteration 15/1000 | Loss: 0.00002134
Iteration 16/1000 | Loss: 0.00002133
Iteration 17/1000 | Loss: 0.00002132
Iteration 18/1000 | Loss: 0.00002128
Iteration 19/1000 | Loss: 0.00002127
Iteration 20/1000 | Loss: 0.00002126
Iteration 21/1000 | Loss: 0.00002125
Iteration 22/1000 | Loss: 0.00002122
Iteration 23/1000 | Loss: 0.00002122
Iteration 24/1000 | Loss: 0.00002118
Iteration 25/1000 | Loss: 0.00002117
Iteration 26/1000 | Loss: 0.00002117
Iteration 27/1000 | Loss: 0.00002117
Iteration 28/1000 | Loss: 0.00002117
Iteration 29/1000 | Loss: 0.00002116
Iteration 30/1000 | Loss: 0.00002115
Iteration 31/1000 | Loss: 0.00002113
Iteration 32/1000 | Loss: 0.00002113
Iteration 33/1000 | Loss: 0.00002113
Iteration 34/1000 | Loss: 0.00002112
Iteration 35/1000 | Loss: 0.00002112
Iteration 36/1000 | Loss: 0.00002112
Iteration 37/1000 | Loss: 0.00002109
Iteration 38/1000 | Loss: 0.00002109
Iteration 39/1000 | Loss: 0.00002109
Iteration 40/1000 | Loss: 0.00002109
Iteration 41/1000 | Loss: 0.00002109
Iteration 42/1000 | Loss: 0.00002109
Iteration 43/1000 | Loss: 0.00002108
Iteration 44/1000 | Loss: 0.00002108
Iteration 45/1000 | Loss: 0.00002108
Iteration 46/1000 | Loss: 0.00002108
Iteration 47/1000 | Loss: 0.00002108
Iteration 48/1000 | Loss: 0.00002108
Iteration 49/1000 | Loss: 0.00002108
Iteration 50/1000 | Loss: 0.00002108
Iteration 51/1000 | Loss: 0.00002108
Iteration 52/1000 | Loss: 0.00002108
Iteration 53/1000 | Loss: 0.00002107
Iteration 54/1000 | Loss: 0.00002107
Iteration 55/1000 | Loss: 0.00002106
Iteration 56/1000 | Loss: 0.00002106
Iteration 57/1000 | Loss: 0.00002105
Iteration 58/1000 | Loss: 0.00002105
Iteration 59/1000 | Loss: 0.00002105
Iteration 60/1000 | Loss: 0.00002105
Iteration 61/1000 | Loss: 0.00002105
Iteration 62/1000 | Loss: 0.00002105
Iteration 63/1000 | Loss: 0.00002105
Iteration 64/1000 | Loss: 0.00002104
Iteration 65/1000 | Loss: 0.00002104
Iteration 66/1000 | Loss: 0.00002104
Iteration 67/1000 | Loss: 0.00002104
Iteration 68/1000 | Loss: 0.00002103
Iteration 69/1000 | Loss: 0.00002103
Iteration 70/1000 | Loss: 0.00002103
Iteration 71/1000 | Loss: 0.00002103
Iteration 72/1000 | Loss: 0.00002102
Iteration 73/1000 | Loss: 0.00002102
Iteration 74/1000 | Loss: 0.00002102
Iteration 75/1000 | Loss: 0.00002102
Iteration 76/1000 | Loss: 0.00002101
Iteration 77/1000 | Loss: 0.00002101
Iteration 78/1000 | Loss: 0.00002101
Iteration 79/1000 | Loss: 0.00002100
Iteration 80/1000 | Loss: 0.00002100
Iteration 81/1000 | Loss: 0.00002100
Iteration 82/1000 | Loss: 0.00002100
Iteration 83/1000 | Loss: 0.00002100
Iteration 84/1000 | Loss: 0.00002100
Iteration 85/1000 | Loss: 0.00002100
Iteration 86/1000 | Loss: 0.00002100
Iteration 87/1000 | Loss: 0.00002100
Iteration 88/1000 | Loss: 0.00002100
Iteration 89/1000 | Loss: 0.00002100
Iteration 90/1000 | Loss: 0.00002100
Iteration 91/1000 | Loss: 0.00002100
Iteration 92/1000 | Loss: 0.00002099
Iteration 93/1000 | Loss: 0.00002099
Iteration 94/1000 | Loss: 0.00002099
Iteration 95/1000 | Loss: 0.00002099
Iteration 96/1000 | Loss: 0.00002099
Iteration 97/1000 | Loss: 0.00002099
Iteration 98/1000 | Loss: 0.00002098
Iteration 99/1000 | Loss: 0.00002098
Iteration 100/1000 | Loss: 0.00002098
Iteration 101/1000 | Loss: 0.00002098
Iteration 102/1000 | Loss: 0.00002098
Iteration 103/1000 | Loss: 0.00002098
Iteration 104/1000 | Loss: 0.00002097
Iteration 105/1000 | Loss: 0.00002097
Iteration 106/1000 | Loss: 0.00002097
Iteration 107/1000 | Loss: 0.00002097
Iteration 108/1000 | Loss: 0.00002097
Iteration 109/1000 | Loss: 0.00002097
Iteration 110/1000 | Loss: 0.00002097
Iteration 111/1000 | Loss: 0.00002097
Iteration 112/1000 | Loss: 0.00002097
Iteration 113/1000 | Loss: 0.00002096
Iteration 114/1000 | Loss: 0.00002096
Iteration 115/1000 | Loss: 0.00002096
Iteration 116/1000 | Loss: 0.00002096
Iteration 117/1000 | Loss: 0.00002096
Iteration 118/1000 | Loss: 0.00002096
Iteration 119/1000 | Loss: 0.00002096
Iteration 120/1000 | Loss: 0.00002096
Iteration 121/1000 | Loss: 0.00002095
Iteration 122/1000 | Loss: 0.00002095
Iteration 123/1000 | Loss: 0.00002095
Iteration 124/1000 | Loss: 0.00002095
Iteration 125/1000 | Loss: 0.00002095
Iteration 126/1000 | Loss: 0.00002095
Iteration 127/1000 | Loss: 0.00002095
Iteration 128/1000 | Loss: 0.00002095
Iteration 129/1000 | Loss: 0.00002095
Iteration 130/1000 | Loss: 0.00002095
Iteration 131/1000 | Loss: 0.00002095
Iteration 132/1000 | Loss: 0.00002095
Iteration 133/1000 | Loss: 0.00002095
Iteration 134/1000 | Loss: 0.00002095
Iteration 135/1000 | Loss: 0.00002095
Iteration 136/1000 | Loss: 0.00002095
Iteration 137/1000 | Loss: 0.00002094
Iteration 138/1000 | Loss: 0.00002094
Iteration 139/1000 | Loss: 0.00002094
Iteration 140/1000 | Loss: 0.00002094
Iteration 141/1000 | Loss: 0.00002094
Iteration 142/1000 | Loss: 0.00002094
Iteration 143/1000 | Loss: 0.00002094
Iteration 144/1000 | Loss: 0.00002094
Iteration 145/1000 | Loss: 0.00002094
Iteration 146/1000 | Loss: 0.00002094
Iteration 147/1000 | Loss: 0.00002094
Iteration 148/1000 | Loss: 0.00002094
Iteration 149/1000 | Loss: 0.00002094
Iteration 150/1000 | Loss: 0.00002094
Iteration 151/1000 | Loss: 0.00002094
Iteration 152/1000 | Loss: 0.00002094
Iteration 153/1000 | Loss: 0.00002094
Iteration 154/1000 | Loss: 0.00002094
Iteration 155/1000 | Loss: 0.00002094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.0940537069691345e-05, 2.0940537069691345e-05, 2.0940537069691345e-05, 2.0940537069691345e-05, 2.0940537069691345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0940537069691345e-05

Optimization complete. Final v2v error: 3.929922342300415 mm

Highest mean error: 4.73665714263916 mm for frame 220

Lowest mean error: 3.293776750564575 mm for frame 93

Saving results

Total time: 119.89948964118958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452955
Iteration 2/25 | Loss: 0.00122298
Iteration 3/25 | Loss: 0.00099354
Iteration 4/25 | Loss: 0.00097391
Iteration 5/25 | Loss: 0.00096929
Iteration 6/25 | Loss: 0.00096834
Iteration 7/25 | Loss: 0.00096834
Iteration 8/25 | Loss: 0.00096834
Iteration 9/25 | Loss: 0.00096834
Iteration 10/25 | Loss: 0.00096834
Iteration 11/25 | Loss: 0.00096834
Iteration 12/25 | Loss: 0.00096834
Iteration 13/25 | Loss: 0.00096834
Iteration 14/25 | Loss: 0.00096834
Iteration 15/25 | Loss: 0.00096834
Iteration 16/25 | Loss: 0.00096834
Iteration 17/25 | Loss: 0.00096834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009683382231742144, 0.0009683382231742144, 0.0009683382231742144, 0.0009683382231742144, 0.0009683382231742144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009683382231742144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.87030959
Iteration 2/25 | Loss: 0.00046277
Iteration 3/25 | Loss: 0.00046275
Iteration 4/25 | Loss: 0.00046275
Iteration 5/25 | Loss: 0.00046275
Iteration 6/25 | Loss: 0.00046275
Iteration 7/25 | Loss: 0.00046275
Iteration 8/25 | Loss: 0.00046275
Iteration 9/25 | Loss: 0.00046275
Iteration 10/25 | Loss: 0.00046275
Iteration 11/25 | Loss: 0.00046275
Iteration 12/25 | Loss: 0.00046275
Iteration 13/25 | Loss: 0.00046275
Iteration 14/25 | Loss: 0.00046275
Iteration 15/25 | Loss: 0.00046275
Iteration 16/25 | Loss: 0.00046275
Iteration 17/25 | Loss: 0.00046275
Iteration 18/25 | Loss: 0.00046275
Iteration 19/25 | Loss: 0.00046275
Iteration 20/25 | Loss: 0.00046275
Iteration 21/25 | Loss: 0.00046275
Iteration 22/25 | Loss: 0.00046275
Iteration 23/25 | Loss: 0.00046275
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004627475864253938, 0.0004627475864253938, 0.0004627475864253938, 0.0004627475864253938, 0.0004627475864253938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004627475864253938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046275
Iteration 2/1000 | Loss: 0.00004102
Iteration 3/1000 | Loss: 0.00002931
Iteration 4/1000 | Loss: 0.00002617
Iteration 5/1000 | Loss: 0.00002377
Iteration 6/1000 | Loss: 0.00002247
Iteration 7/1000 | Loss: 0.00002188
Iteration 8/1000 | Loss: 0.00002161
Iteration 9/1000 | Loss: 0.00002158
Iteration 10/1000 | Loss: 0.00002157
Iteration 11/1000 | Loss: 0.00002155
Iteration 12/1000 | Loss: 0.00002154
Iteration 13/1000 | Loss: 0.00002153
Iteration 14/1000 | Loss: 0.00002151
Iteration 15/1000 | Loss: 0.00002134
Iteration 16/1000 | Loss: 0.00002119
Iteration 17/1000 | Loss: 0.00002114
Iteration 18/1000 | Loss: 0.00002111
Iteration 19/1000 | Loss: 0.00002109
Iteration 20/1000 | Loss: 0.00002108
Iteration 21/1000 | Loss: 0.00002107
Iteration 22/1000 | Loss: 0.00002107
Iteration 23/1000 | Loss: 0.00002107
Iteration 24/1000 | Loss: 0.00002107
Iteration 25/1000 | Loss: 0.00002106
Iteration 26/1000 | Loss: 0.00002106
Iteration 27/1000 | Loss: 0.00002106
Iteration 28/1000 | Loss: 0.00002106
Iteration 29/1000 | Loss: 0.00002106
Iteration 30/1000 | Loss: 0.00002106
Iteration 31/1000 | Loss: 0.00002104
Iteration 32/1000 | Loss: 0.00002104
Iteration 33/1000 | Loss: 0.00002103
Iteration 34/1000 | Loss: 0.00002102
Iteration 35/1000 | Loss: 0.00002102
Iteration 36/1000 | Loss: 0.00002101
Iteration 37/1000 | Loss: 0.00002101
Iteration 38/1000 | Loss: 0.00002101
Iteration 39/1000 | Loss: 0.00002100
Iteration 40/1000 | Loss: 0.00002100
Iteration 41/1000 | Loss: 0.00002100
Iteration 42/1000 | Loss: 0.00002100
Iteration 43/1000 | Loss: 0.00002099
Iteration 44/1000 | Loss: 0.00002099
Iteration 45/1000 | Loss: 0.00002099
Iteration 46/1000 | Loss: 0.00002099
Iteration 47/1000 | Loss: 0.00002099
Iteration 48/1000 | Loss: 0.00002098
Iteration 49/1000 | Loss: 0.00002098
Iteration 50/1000 | Loss: 0.00002098
Iteration 51/1000 | Loss: 0.00002098
Iteration 52/1000 | Loss: 0.00002098
Iteration 53/1000 | Loss: 0.00002098
Iteration 54/1000 | Loss: 0.00002098
Iteration 55/1000 | Loss: 0.00002098
Iteration 56/1000 | Loss: 0.00002098
Iteration 57/1000 | Loss: 0.00002098
Iteration 58/1000 | Loss: 0.00002098
Iteration 59/1000 | Loss: 0.00002098
Iteration 60/1000 | Loss: 0.00002098
Iteration 61/1000 | Loss: 0.00002098
Iteration 62/1000 | Loss: 0.00002097
Iteration 63/1000 | Loss: 0.00002097
Iteration 64/1000 | Loss: 0.00002097
Iteration 65/1000 | Loss: 0.00002097
Iteration 66/1000 | Loss: 0.00002097
Iteration 67/1000 | Loss: 0.00002097
Iteration 68/1000 | Loss: 0.00002097
Iteration 69/1000 | Loss: 0.00002097
Iteration 70/1000 | Loss: 0.00002097
Iteration 71/1000 | Loss: 0.00002096
Iteration 72/1000 | Loss: 0.00002096
Iteration 73/1000 | Loss: 0.00002096
Iteration 74/1000 | Loss: 0.00002096
Iteration 75/1000 | Loss: 0.00002096
Iteration 76/1000 | Loss: 0.00002096
Iteration 77/1000 | Loss: 0.00002096
Iteration 78/1000 | Loss: 0.00002096
Iteration 79/1000 | Loss: 0.00002096
Iteration 80/1000 | Loss: 0.00002096
Iteration 81/1000 | Loss: 0.00002095
Iteration 82/1000 | Loss: 0.00002095
Iteration 83/1000 | Loss: 0.00002095
Iteration 84/1000 | Loss: 0.00002095
Iteration 85/1000 | Loss: 0.00002095
Iteration 86/1000 | Loss: 0.00002095
Iteration 87/1000 | Loss: 0.00002095
Iteration 88/1000 | Loss: 0.00002095
Iteration 89/1000 | Loss: 0.00002095
Iteration 90/1000 | Loss: 0.00002094
Iteration 91/1000 | Loss: 0.00002094
Iteration 92/1000 | Loss: 0.00002094
Iteration 93/1000 | Loss: 0.00002093
Iteration 94/1000 | Loss: 0.00002093
Iteration 95/1000 | Loss: 0.00002093
Iteration 96/1000 | Loss: 0.00002092
Iteration 97/1000 | Loss: 0.00002092
Iteration 98/1000 | Loss: 0.00002092
Iteration 99/1000 | Loss: 0.00002092
Iteration 100/1000 | Loss: 0.00002092
Iteration 101/1000 | Loss: 0.00002092
Iteration 102/1000 | Loss: 0.00002092
Iteration 103/1000 | Loss: 0.00002092
Iteration 104/1000 | Loss: 0.00002091
Iteration 105/1000 | Loss: 0.00002091
Iteration 106/1000 | Loss: 0.00002091
Iteration 107/1000 | Loss: 0.00002091
Iteration 108/1000 | Loss: 0.00002091
Iteration 109/1000 | Loss: 0.00002091
Iteration 110/1000 | Loss: 0.00002091
Iteration 111/1000 | Loss: 0.00002091
Iteration 112/1000 | Loss: 0.00002091
Iteration 113/1000 | Loss: 0.00002091
Iteration 114/1000 | Loss: 0.00002091
Iteration 115/1000 | Loss: 0.00002090
Iteration 116/1000 | Loss: 0.00002090
Iteration 117/1000 | Loss: 0.00002090
Iteration 118/1000 | Loss: 0.00002090
Iteration 119/1000 | Loss: 0.00002090
Iteration 120/1000 | Loss: 0.00002089
Iteration 121/1000 | Loss: 0.00002089
Iteration 122/1000 | Loss: 0.00002089
Iteration 123/1000 | Loss: 0.00002089
Iteration 124/1000 | Loss: 0.00002089
Iteration 125/1000 | Loss: 0.00002089
Iteration 126/1000 | Loss: 0.00002089
Iteration 127/1000 | Loss: 0.00002089
Iteration 128/1000 | Loss: 0.00002089
Iteration 129/1000 | Loss: 0.00002089
Iteration 130/1000 | Loss: 0.00002089
Iteration 131/1000 | Loss: 0.00002089
Iteration 132/1000 | Loss: 0.00002089
Iteration 133/1000 | Loss: 0.00002089
Iteration 134/1000 | Loss: 0.00002089
Iteration 135/1000 | Loss: 0.00002089
Iteration 136/1000 | Loss: 0.00002088
Iteration 137/1000 | Loss: 0.00002088
Iteration 138/1000 | Loss: 0.00002088
Iteration 139/1000 | Loss: 0.00002088
Iteration 140/1000 | Loss: 0.00002088
Iteration 141/1000 | Loss: 0.00002088
Iteration 142/1000 | Loss: 0.00002088
Iteration 143/1000 | Loss: 0.00002088
Iteration 144/1000 | Loss: 0.00002088
Iteration 145/1000 | Loss: 0.00002088
Iteration 146/1000 | Loss: 0.00002088
Iteration 147/1000 | Loss: 0.00002088
Iteration 148/1000 | Loss: 0.00002088
Iteration 149/1000 | Loss: 0.00002088
Iteration 150/1000 | Loss: 0.00002088
Iteration 151/1000 | Loss: 0.00002087
Iteration 152/1000 | Loss: 0.00002087
Iteration 153/1000 | Loss: 0.00002087
Iteration 154/1000 | Loss: 0.00002087
Iteration 155/1000 | Loss: 0.00002087
Iteration 156/1000 | Loss: 0.00002087
Iteration 157/1000 | Loss: 0.00002087
Iteration 158/1000 | Loss: 0.00002087
Iteration 159/1000 | Loss: 0.00002087
Iteration 160/1000 | Loss: 0.00002087
Iteration 161/1000 | Loss: 0.00002087
Iteration 162/1000 | Loss: 0.00002087
Iteration 163/1000 | Loss: 0.00002087
Iteration 164/1000 | Loss: 0.00002087
Iteration 165/1000 | Loss: 0.00002087
Iteration 166/1000 | Loss: 0.00002087
Iteration 167/1000 | Loss: 0.00002087
Iteration 168/1000 | Loss: 0.00002087
Iteration 169/1000 | Loss: 0.00002087
Iteration 170/1000 | Loss: 0.00002087
Iteration 171/1000 | Loss: 0.00002087
Iteration 172/1000 | Loss: 0.00002087
Iteration 173/1000 | Loss: 0.00002087
Iteration 174/1000 | Loss: 0.00002087
Iteration 175/1000 | Loss: 0.00002087
Iteration 176/1000 | Loss: 0.00002087
Iteration 177/1000 | Loss: 0.00002087
Iteration 178/1000 | Loss: 0.00002087
Iteration 179/1000 | Loss: 0.00002087
Iteration 180/1000 | Loss: 0.00002087
Iteration 181/1000 | Loss: 0.00002087
Iteration 182/1000 | Loss: 0.00002087
Iteration 183/1000 | Loss: 0.00002087
Iteration 184/1000 | Loss: 0.00002087
Iteration 185/1000 | Loss: 0.00002087
Iteration 186/1000 | Loss: 0.00002087
Iteration 187/1000 | Loss: 0.00002087
Iteration 188/1000 | Loss: 0.00002087
Iteration 189/1000 | Loss: 0.00002087
Iteration 190/1000 | Loss: 0.00002087
Iteration 191/1000 | Loss: 0.00002087
Iteration 192/1000 | Loss: 0.00002087
Iteration 193/1000 | Loss: 0.00002087
Iteration 194/1000 | Loss: 0.00002087
Iteration 195/1000 | Loss: 0.00002087
Iteration 196/1000 | Loss: 0.00002087
Iteration 197/1000 | Loss: 0.00002087
Iteration 198/1000 | Loss: 0.00002087
Iteration 199/1000 | Loss: 0.00002087
Iteration 200/1000 | Loss: 0.00002087
Iteration 201/1000 | Loss: 0.00002087
Iteration 202/1000 | Loss: 0.00002087
Iteration 203/1000 | Loss: 0.00002087
Iteration 204/1000 | Loss: 0.00002087
Iteration 205/1000 | Loss: 0.00002087
Iteration 206/1000 | Loss: 0.00002087
Iteration 207/1000 | Loss: 0.00002087
Iteration 208/1000 | Loss: 0.00002087
Iteration 209/1000 | Loss: 0.00002087
Iteration 210/1000 | Loss: 0.00002087
Iteration 211/1000 | Loss: 0.00002087
Iteration 212/1000 | Loss: 0.00002087
Iteration 213/1000 | Loss: 0.00002087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [2.0866911654593423e-05, 2.0866911654593423e-05, 2.0866911654593423e-05, 2.0866911654593423e-05, 2.0866911654593423e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0866911654593423e-05

Optimization complete. Final v2v error: 3.9283125400543213 mm

Highest mean error: 4.147787094116211 mm for frame 109

Lowest mean error: 3.5864830017089844 mm for frame 53

Saving results

Total time: 55.68015933036804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00647698
Iteration 2/25 | Loss: 0.00126884
Iteration 3/25 | Loss: 0.00112527
Iteration 4/25 | Loss: 0.00109596
Iteration 5/25 | Loss: 0.00108914
Iteration 6/25 | Loss: 0.00108681
Iteration 7/25 | Loss: 0.00108675
Iteration 8/25 | Loss: 0.00108675
Iteration 9/25 | Loss: 0.00108675
Iteration 10/25 | Loss: 0.00108675
Iteration 11/25 | Loss: 0.00108675
Iteration 12/25 | Loss: 0.00108675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001086752163246274, 0.001086752163246274, 0.001086752163246274, 0.001086752163246274, 0.001086752163246274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001086752163246274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63175368
Iteration 2/25 | Loss: 0.00069632
Iteration 3/25 | Loss: 0.00069632
Iteration 4/25 | Loss: 0.00069632
Iteration 5/25 | Loss: 0.00069632
Iteration 6/25 | Loss: 0.00069632
Iteration 7/25 | Loss: 0.00069632
Iteration 8/25 | Loss: 0.00069632
Iteration 9/25 | Loss: 0.00069632
Iteration 10/25 | Loss: 0.00069632
Iteration 11/25 | Loss: 0.00069632
Iteration 12/25 | Loss: 0.00069632
Iteration 13/25 | Loss: 0.00069632
Iteration 14/25 | Loss: 0.00069632
Iteration 15/25 | Loss: 0.00069632
Iteration 16/25 | Loss: 0.00069632
Iteration 17/25 | Loss: 0.00069632
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006963161868043244, 0.0006963161868043244, 0.0006963161868043244, 0.0006963161868043244, 0.0006963161868043244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006963161868043244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069632
Iteration 2/1000 | Loss: 0.00007017
Iteration 3/1000 | Loss: 0.00003955
Iteration 4/1000 | Loss: 0.00003431
Iteration 5/1000 | Loss: 0.00002876
Iteration 6/1000 | Loss: 0.00002668
Iteration 7/1000 | Loss: 0.00002500
Iteration 8/1000 | Loss: 0.00002422
Iteration 9/1000 | Loss: 0.00002358
Iteration 10/1000 | Loss: 0.00002302
Iteration 11/1000 | Loss: 0.00002257
Iteration 12/1000 | Loss: 0.00002240
Iteration 13/1000 | Loss: 0.00002220
Iteration 14/1000 | Loss: 0.00002203
Iteration 15/1000 | Loss: 0.00002203
Iteration 16/1000 | Loss: 0.00002203
Iteration 17/1000 | Loss: 0.00002192
Iteration 18/1000 | Loss: 0.00002178
Iteration 19/1000 | Loss: 0.00002164
Iteration 20/1000 | Loss: 0.00002157
Iteration 21/1000 | Loss: 0.00002154
Iteration 22/1000 | Loss: 0.00002149
Iteration 23/1000 | Loss: 0.00002147
Iteration 24/1000 | Loss: 0.00002146
Iteration 25/1000 | Loss: 0.00002146
Iteration 26/1000 | Loss: 0.00002145
Iteration 27/1000 | Loss: 0.00002145
Iteration 28/1000 | Loss: 0.00002144
Iteration 29/1000 | Loss: 0.00002144
Iteration 30/1000 | Loss: 0.00002144
Iteration 31/1000 | Loss: 0.00002143
Iteration 32/1000 | Loss: 0.00002143
Iteration 33/1000 | Loss: 0.00002143
Iteration 34/1000 | Loss: 0.00002142
Iteration 35/1000 | Loss: 0.00002142
Iteration 36/1000 | Loss: 0.00002141
Iteration 37/1000 | Loss: 0.00002141
Iteration 38/1000 | Loss: 0.00002141
Iteration 39/1000 | Loss: 0.00002141
Iteration 40/1000 | Loss: 0.00002141
Iteration 41/1000 | Loss: 0.00002141
Iteration 42/1000 | Loss: 0.00002141
Iteration 43/1000 | Loss: 0.00002140
Iteration 44/1000 | Loss: 0.00002140
Iteration 45/1000 | Loss: 0.00002138
Iteration 46/1000 | Loss: 0.00002138
Iteration 47/1000 | Loss: 0.00002137
Iteration 48/1000 | Loss: 0.00002137
Iteration 49/1000 | Loss: 0.00002137
Iteration 50/1000 | Loss: 0.00002137
Iteration 51/1000 | Loss: 0.00002137
Iteration 52/1000 | Loss: 0.00002137
Iteration 53/1000 | Loss: 0.00002136
Iteration 54/1000 | Loss: 0.00002135
Iteration 55/1000 | Loss: 0.00002135
Iteration 56/1000 | Loss: 0.00002134
Iteration 57/1000 | Loss: 0.00002134
Iteration 58/1000 | Loss: 0.00002134
Iteration 59/1000 | Loss: 0.00002133
Iteration 60/1000 | Loss: 0.00002133
Iteration 61/1000 | Loss: 0.00002133
Iteration 62/1000 | Loss: 0.00002132
Iteration 63/1000 | Loss: 0.00002132
Iteration 64/1000 | Loss: 0.00002132
Iteration 65/1000 | Loss: 0.00002131
Iteration 66/1000 | Loss: 0.00002130
Iteration 67/1000 | Loss: 0.00002130
Iteration 68/1000 | Loss: 0.00002129
Iteration 69/1000 | Loss: 0.00002129
Iteration 70/1000 | Loss: 0.00002129
Iteration 71/1000 | Loss: 0.00002128
Iteration 72/1000 | Loss: 0.00002128
Iteration 73/1000 | Loss: 0.00002128
Iteration 74/1000 | Loss: 0.00002128
Iteration 75/1000 | Loss: 0.00002128
Iteration 76/1000 | Loss: 0.00002128
Iteration 77/1000 | Loss: 0.00002128
Iteration 78/1000 | Loss: 0.00002128
Iteration 79/1000 | Loss: 0.00002128
Iteration 80/1000 | Loss: 0.00002128
Iteration 81/1000 | Loss: 0.00002128
Iteration 82/1000 | Loss: 0.00002128
Iteration 83/1000 | Loss: 0.00002127
Iteration 84/1000 | Loss: 0.00002127
Iteration 85/1000 | Loss: 0.00002127
Iteration 86/1000 | Loss: 0.00002127
Iteration 87/1000 | Loss: 0.00002126
Iteration 88/1000 | Loss: 0.00002126
Iteration 89/1000 | Loss: 0.00002126
Iteration 90/1000 | Loss: 0.00002126
Iteration 91/1000 | Loss: 0.00002126
Iteration 92/1000 | Loss: 0.00002126
Iteration 93/1000 | Loss: 0.00002125
Iteration 94/1000 | Loss: 0.00002125
Iteration 95/1000 | Loss: 0.00002125
Iteration 96/1000 | Loss: 0.00002125
Iteration 97/1000 | Loss: 0.00002125
Iteration 98/1000 | Loss: 0.00002125
Iteration 99/1000 | Loss: 0.00002125
Iteration 100/1000 | Loss: 0.00002125
Iteration 101/1000 | Loss: 0.00002125
Iteration 102/1000 | Loss: 0.00002125
Iteration 103/1000 | Loss: 0.00002125
Iteration 104/1000 | Loss: 0.00002125
Iteration 105/1000 | Loss: 0.00002124
Iteration 106/1000 | Loss: 0.00002124
Iteration 107/1000 | Loss: 0.00002123
Iteration 108/1000 | Loss: 0.00002122
Iteration 109/1000 | Loss: 0.00002122
Iteration 110/1000 | Loss: 0.00002122
Iteration 111/1000 | Loss: 0.00002121
Iteration 112/1000 | Loss: 0.00002121
Iteration 113/1000 | Loss: 0.00002121
Iteration 114/1000 | Loss: 0.00002121
Iteration 115/1000 | Loss: 0.00002121
Iteration 116/1000 | Loss: 0.00002121
Iteration 117/1000 | Loss: 0.00002120
Iteration 118/1000 | Loss: 0.00002120
Iteration 119/1000 | Loss: 0.00002119
Iteration 120/1000 | Loss: 0.00002118
Iteration 121/1000 | Loss: 0.00002118
Iteration 122/1000 | Loss: 0.00002118
Iteration 123/1000 | Loss: 0.00002117
Iteration 124/1000 | Loss: 0.00002117
Iteration 125/1000 | Loss: 0.00002117
Iteration 126/1000 | Loss: 0.00002117
Iteration 127/1000 | Loss: 0.00002116
Iteration 128/1000 | Loss: 0.00002116
Iteration 129/1000 | Loss: 0.00002116
Iteration 130/1000 | Loss: 0.00002115
Iteration 131/1000 | Loss: 0.00002115
Iteration 132/1000 | Loss: 0.00002115
Iteration 133/1000 | Loss: 0.00002114
Iteration 134/1000 | Loss: 0.00002114
Iteration 135/1000 | Loss: 0.00002114
Iteration 136/1000 | Loss: 0.00002114
Iteration 137/1000 | Loss: 0.00002114
Iteration 138/1000 | Loss: 0.00002113
Iteration 139/1000 | Loss: 0.00002113
Iteration 140/1000 | Loss: 0.00002113
Iteration 141/1000 | Loss: 0.00002112
Iteration 142/1000 | Loss: 0.00002112
Iteration 143/1000 | Loss: 0.00002112
Iteration 144/1000 | Loss: 0.00002111
Iteration 145/1000 | Loss: 0.00002111
Iteration 146/1000 | Loss: 0.00002111
Iteration 147/1000 | Loss: 0.00002111
Iteration 148/1000 | Loss: 0.00002110
Iteration 149/1000 | Loss: 0.00002110
Iteration 150/1000 | Loss: 0.00002110
Iteration 151/1000 | Loss: 0.00002110
Iteration 152/1000 | Loss: 0.00002110
Iteration 153/1000 | Loss: 0.00002110
Iteration 154/1000 | Loss: 0.00002110
Iteration 155/1000 | Loss: 0.00002109
Iteration 156/1000 | Loss: 0.00002109
Iteration 157/1000 | Loss: 0.00002109
Iteration 158/1000 | Loss: 0.00002109
Iteration 159/1000 | Loss: 0.00002109
Iteration 160/1000 | Loss: 0.00002109
Iteration 161/1000 | Loss: 0.00002109
Iteration 162/1000 | Loss: 0.00002109
Iteration 163/1000 | Loss: 0.00002109
Iteration 164/1000 | Loss: 0.00002109
Iteration 165/1000 | Loss: 0.00002108
Iteration 166/1000 | Loss: 0.00002108
Iteration 167/1000 | Loss: 0.00002108
Iteration 168/1000 | Loss: 0.00002108
Iteration 169/1000 | Loss: 0.00002108
Iteration 170/1000 | Loss: 0.00002108
Iteration 171/1000 | Loss: 0.00002108
Iteration 172/1000 | Loss: 0.00002108
Iteration 173/1000 | Loss: 0.00002108
Iteration 174/1000 | Loss: 0.00002107
Iteration 175/1000 | Loss: 0.00002107
Iteration 176/1000 | Loss: 0.00002107
Iteration 177/1000 | Loss: 0.00002107
Iteration 178/1000 | Loss: 0.00002107
Iteration 179/1000 | Loss: 0.00002107
Iteration 180/1000 | Loss: 0.00002107
Iteration 181/1000 | Loss: 0.00002107
Iteration 182/1000 | Loss: 0.00002107
Iteration 183/1000 | Loss: 0.00002107
Iteration 184/1000 | Loss: 0.00002107
Iteration 185/1000 | Loss: 0.00002107
Iteration 186/1000 | Loss: 0.00002107
Iteration 187/1000 | Loss: 0.00002107
Iteration 188/1000 | Loss: 0.00002106
Iteration 189/1000 | Loss: 0.00002106
Iteration 190/1000 | Loss: 0.00002106
Iteration 191/1000 | Loss: 0.00002106
Iteration 192/1000 | Loss: 0.00002106
Iteration 193/1000 | Loss: 0.00002106
Iteration 194/1000 | Loss: 0.00002106
Iteration 195/1000 | Loss: 0.00002106
Iteration 196/1000 | Loss: 0.00002106
Iteration 197/1000 | Loss: 0.00002106
Iteration 198/1000 | Loss: 0.00002106
Iteration 199/1000 | Loss: 0.00002106
Iteration 200/1000 | Loss: 0.00002106
Iteration 201/1000 | Loss: 0.00002106
Iteration 202/1000 | Loss: 0.00002106
Iteration 203/1000 | Loss: 0.00002106
Iteration 204/1000 | Loss: 0.00002106
Iteration 205/1000 | Loss: 0.00002106
Iteration 206/1000 | Loss: 0.00002106
Iteration 207/1000 | Loss: 0.00002106
Iteration 208/1000 | Loss: 0.00002106
Iteration 209/1000 | Loss: 0.00002106
Iteration 210/1000 | Loss: 0.00002106
Iteration 211/1000 | Loss: 0.00002106
Iteration 212/1000 | Loss: 0.00002106
Iteration 213/1000 | Loss: 0.00002106
Iteration 214/1000 | Loss: 0.00002106
Iteration 215/1000 | Loss: 0.00002106
Iteration 216/1000 | Loss: 0.00002106
Iteration 217/1000 | Loss: 0.00002106
Iteration 218/1000 | Loss: 0.00002106
Iteration 219/1000 | Loss: 0.00002106
Iteration 220/1000 | Loss: 0.00002106
Iteration 221/1000 | Loss: 0.00002106
Iteration 222/1000 | Loss: 0.00002106
Iteration 223/1000 | Loss: 0.00002106
Iteration 224/1000 | Loss: 0.00002106
Iteration 225/1000 | Loss: 0.00002106
Iteration 226/1000 | Loss: 0.00002106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [2.105810199282132e-05, 2.105810199282132e-05, 2.105810199282132e-05, 2.105810199282132e-05, 2.105810199282132e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.105810199282132e-05

Optimization complete. Final v2v error: 4.111328125 mm

Highest mean error: 4.451359272003174 mm for frame 11

Lowest mean error: 3.8151769638061523 mm for frame 131

Saving results

Total time: 60.4207763671875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946094
Iteration 2/25 | Loss: 0.00226971
Iteration 3/25 | Loss: 0.00125115
Iteration 4/25 | Loss: 0.00119758
Iteration 5/25 | Loss: 0.00118046
Iteration 6/25 | Loss: 0.00117489
Iteration 7/25 | Loss: 0.00117335
Iteration 8/25 | Loss: 0.00117335
Iteration 9/25 | Loss: 0.00117335
Iteration 10/25 | Loss: 0.00117335
Iteration 11/25 | Loss: 0.00117335
Iteration 12/25 | Loss: 0.00117335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011733544524759054, 0.0011733544524759054, 0.0011733544524759054, 0.0011733544524759054, 0.0011733544524759054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011733544524759054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93141085
Iteration 2/25 | Loss: 0.00062579
Iteration 3/25 | Loss: 0.00062579
Iteration 4/25 | Loss: 0.00062579
Iteration 5/25 | Loss: 0.00062579
Iteration 6/25 | Loss: 0.00062579
Iteration 7/25 | Loss: 0.00062579
Iteration 8/25 | Loss: 0.00062579
Iteration 9/25 | Loss: 0.00062579
Iteration 10/25 | Loss: 0.00062579
Iteration 11/25 | Loss: 0.00062579
Iteration 12/25 | Loss: 0.00062579
Iteration 13/25 | Loss: 0.00062579
Iteration 14/25 | Loss: 0.00062579
Iteration 15/25 | Loss: 0.00062579
Iteration 16/25 | Loss: 0.00062579
Iteration 17/25 | Loss: 0.00062579
Iteration 18/25 | Loss: 0.00062579
Iteration 19/25 | Loss: 0.00062579
Iteration 20/25 | Loss: 0.00062579
Iteration 21/25 | Loss: 0.00062579
Iteration 22/25 | Loss: 0.00062579
Iteration 23/25 | Loss: 0.00062579
Iteration 24/25 | Loss: 0.00062579
Iteration 25/25 | Loss: 0.00062579

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062579
Iteration 2/1000 | Loss: 0.00007191
Iteration 3/1000 | Loss: 0.00005681
Iteration 4/1000 | Loss: 0.00004822
Iteration 5/1000 | Loss: 0.00004424
Iteration 6/1000 | Loss: 0.00004183
Iteration 7/1000 | Loss: 0.00004023
Iteration 8/1000 | Loss: 0.00003938
Iteration 9/1000 | Loss: 0.00003887
Iteration 10/1000 | Loss: 0.00003837
Iteration 11/1000 | Loss: 0.00003793
Iteration 12/1000 | Loss: 0.00003765
Iteration 13/1000 | Loss: 0.00003749
Iteration 14/1000 | Loss: 0.00003738
Iteration 15/1000 | Loss: 0.00003728
Iteration 16/1000 | Loss: 0.00003719
Iteration 17/1000 | Loss: 0.00003714
Iteration 18/1000 | Loss: 0.00003714
Iteration 19/1000 | Loss: 0.00003707
Iteration 20/1000 | Loss: 0.00003698
Iteration 21/1000 | Loss: 0.00003697
Iteration 22/1000 | Loss: 0.00003692
Iteration 23/1000 | Loss: 0.00003686
Iteration 24/1000 | Loss: 0.00003679
Iteration 25/1000 | Loss: 0.00003677
Iteration 26/1000 | Loss: 0.00003675
Iteration 27/1000 | Loss: 0.00003675
Iteration 28/1000 | Loss: 0.00003674
Iteration 29/1000 | Loss: 0.00003674
Iteration 30/1000 | Loss: 0.00003674
Iteration 31/1000 | Loss: 0.00003672
Iteration 32/1000 | Loss: 0.00003672
Iteration 33/1000 | Loss: 0.00003671
Iteration 34/1000 | Loss: 0.00003670
Iteration 35/1000 | Loss: 0.00003670
Iteration 36/1000 | Loss: 0.00003670
Iteration 37/1000 | Loss: 0.00003667
Iteration 38/1000 | Loss: 0.00003667
Iteration 39/1000 | Loss: 0.00003667
Iteration 40/1000 | Loss: 0.00003667
Iteration 41/1000 | Loss: 0.00003667
Iteration 42/1000 | Loss: 0.00003667
Iteration 43/1000 | Loss: 0.00003667
Iteration 44/1000 | Loss: 0.00003666
Iteration 45/1000 | Loss: 0.00003666
Iteration 46/1000 | Loss: 0.00003665
Iteration 47/1000 | Loss: 0.00003665
Iteration 48/1000 | Loss: 0.00003665
Iteration 49/1000 | Loss: 0.00003665
Iteration 50/1000 | Loss: 0.00003664
Iteration 51/1000 | Loss: 0.00003664
Iteration 52/1000 | Loss: 0.00003664
Iteration 53/1000 | Loss: 0.00003663
Iteration 54/1000 | Loss: 0.00003663
Iteration 55/1000 | Loss: 0.00003662
Iteration 56/1000 | Loss: 0.00003662
Iteration 57/1000 | Loss: 0.00003662
Iteration 58/1000 | Loss: 0.00003661
Iteration 59/1000 | Loss: 0.00003661
Iteration 60/1000 | Loss: 0.00003661
Iteration 61/1000 | Loss: 0.00003661
Iteration 62/1000 | Loss: 0.00003661
Iteration 63/1000 | Loss: 0.00003661
Iteration 64/1000 | Loss: 0.00003661
Iteration 65/1000 | Loss: 0.00003660
Iteration 66/1000 | Loss: 0.00003660
Iteration 67/1000 | Loss: 0.00003660
Iteration 68/1000 | Loss: 0.00003660
Iteration 69/1000 | Loss: 0.00003660
Iteration 70/1000 | Loss: 0.00003660
Iteration 71/1000 | Loss: 0.00003660
Iteration 72/1000 | Loss: 0.00003659
Iteration 73/1000 | Loss: 0.00003659
Iteration 74/1000 | Loss: 0.00003659
Iteration 75/1000 | Loss: 0.00003659
Iteration 76/1000 | Loss: 0.00003659
Iteration 77/1000 | Loss: 0.00003659
Iteration 78/1000 | Loss: 0.00003659
Iteration 79/1000 | Loss: 0.00003659
Iteration 80/1000 | Loss: 0.00003659
Iteration 81/1000 | Loss: 0.00003659
Iteration 82/1000 | Loss: 0.00003659
Iteration 83/1000 | Loss: 0.00003658
Iteration 84/1000 | Loss: 0.00003658
Iteration 85/1000 | Loss: 0.00003658
Iteration 86/1000 | Loss: 0.00003658
Iteration 87/1000 | Loss: 0.00003658
Iteration 88/1000 | Loss: 0.00003658
Iteration 89/1000 | Loss: 0.00003658
Iteration 90/1000 | Loss: 0.00003657
Iteration 91/1000 | Loss: 0.00003657
Iteration 92/1000 | Loss: 0.00003657
Iteration 93/1000 | Loss: 0.00003656
Iteration 94/1000 | Loss: 0.00003656
Iteration 95/1000 | Loss: 0.00003656
Iteration 96/1000 | Loss: 0.00003656
Iteration 97/1000 | Loss: 0.00003656
Iteration 98/1000 | Loss: 0.00003656
Iteration 99/1000 | Loss: 0.00003656
Iteration 100/1000 | Loss: 0.00003655
Iteration 101/1000 | Loss: 0.00003655
Iteration 102/1000 | Loss: 0.00003655
Iteration 103/1000 | Loss: 0.00003655
Iteration 104/1000 | Loss: 0.00003655
Iteration 105/1000 | Loss: 0.00003655
Iteration 106/1000 | Loss: 0.00003654
Iteration 107/1000 | Loss: 0.00003654
Iteration 108/1000 | Loss: 0.00003653
Iteration 109/1000 | Loss: 0.00003653
Iteration 110/1000 | Loss: 0.00003653
Iteration 111/1000 | Loss: 0.00003653
Iteration 112/1000 | Loss: 0.00003652
Iteration 113/1000 | Loss: 0.00003652
Iteration 114/1000 | Loss: 0.00003652
Iteration 115/1000 | Loss: 0.00003651
Iteration 116/1000 | Loss: 0.00003651
Iteration 117/1000 | Loss: 0.00003651
Iteration 118/1000 | Loss: 0.00003650
Iteration 119/1000 | Loss: 0.00003650
Iteration 120/1000 | Loss: 0.00003650
Iteration 121/1000 | Loss: 0.00003650
Iteration 122/1000 | Loss: 0.00003650
Iteration 123/1000 | Loss: 0.00003650
Iteration 124/1000 | Loss: 0.00003650
Iteration 125/1000 | Loss: 0.00003650
Iteration 126/1000 | Loss: 0.00003650
Iteration 127/1000 | Loss: 0.00003649
Iteration 128/1000 | Loss: 0.00003649
Iteration 129/1000 | Loss: 0.00003649
Iteration 130/1000 | Loss: 0.00003649
Iteration 131/1000 | Loss: 0.00003649
Iteration 132/1000 | Loss: 0.00003649
Iteration 133/1000 | Loss: 0.00003649
Iteration 134/1000 | Loss: 0.00003649
Iteration 135/1000 | Loss: 0.00003648
Iteration 136/1000 | Loss: 0.00003648
Iteration 137/1000 | Loss: 0.00003648
Iteration 138/1000 | Loss: 0.00003648
Iteration 139/1000 | Loss: 0.00003647
Iteration 140/1000 | Loss: 0.00003647
Iteration 141/1000 | Loss: 0.00003647
Iteration 142/1000 | Loss: 0.00003647
Iteration 143/1000 | Loss: 0.00003647
Iteration 144/1000 | Loss: 0.00003647
Iteration 145/1000 | Loss: 0.00003647
Iteration 146/1000 | Loss: 0.00003647
Iteration 147/1000 | Loss: 0.00003646
Iteration 148/1000 | Loss: 0.00003646
Iteration 149/1000 | Loss: 0.00003646
Iteration 150/1000 | Loss: 0.00003646
Iteration 151/1000 | Loss: 0.00003646
Iteration 152/1000 | Loss: 0.00003645
Iteration 153/1000 | Loss: 0.00003645
Iteration 154/1000 | Loss: 0.00003645
Iteration 155/1000 | Loss: 0.00003645
Iteration 156/1000 | Loss: 0.00003645
Iteration 157/1000 | Loss: 0.00003645
Iteration 158/1000 | Loss: 0.00003645
Iteration 159/1000 | Loss: 0.00003644
Iteration 160/1000 | Loss: 0.00003644
Iteration 161/1000 | Loss: 0.00003644
Iteration 162/1000 | Loss: 0.00003644
Iteration 163/1000 | Loss: 0.00003644
Iteration 164/1000 | Loss: 0.00003644
Iteration 165/1000 | Loss: 0.00003644
Iteration 166/1000 | Loss: 0.00003644
Iteration 167/1000 | Loss: 0.00003644
Iteration 168/1000 | Loss: 0.00003644
Iteration 169/1000 | Loss: 0.00003644
Iteration 170/1000 | Loss: 0.00003644
Iteration 171/1000 | Loss: 0.00003643
Iteration 172/1000 | Loss: 0.00003643
Iteration 173/1000 | Loss: 0.00003643
Iteration 174/1000 | Loss: 0.00003643
Iteration 175/1000 | Loss: 0.00003643
Iteration 176/1000 | Loss: 0.00003643
Iteration 177/1000 | Loss: 0.00003642
Iteration 178/1000 | Loss: 0.00003642
Iteration 179/1000 | Loss: 0.00003642
Iteration 180/1000 | Loss: 0.00003641
Iteration 181/1000 | Loss: 0.00003641
Iteration 182/1000 | Loss: 0.00003641
Iteration 183/1000 | Loss: 0.00003641
Iteration 184/1000 | Loss: 0.00003641
Iteration 185/1000 | Loss: 0.00003641
Iteration 186/1000 | Loss: 0.00003640
Iteration 187/1000 | Loss: 0.00003640
Iteration 188/1000 | Loss: 0.00003640
Iteration 189/1000 | Loss: 0.00003640
Iteration 190/1000 | Loss: 0.00003639
Iteration 191/1000 | Loss: 0.00003639
Iteration 192/1000 | Loss: 0.00003639
Iteration 193/1000 | Loss: 0.00003638
Iteration 194/1000 | Loss: 0.00003638
Iteration 195/1000 | Loss: 0.00003638
Iteration 196/1000 | Loss: 0.00003638
Iteration 197/1000 | Loss: 0.00003638
Iteration 198/1000 | Loss: 0.00003638
Iteration 199/1000 | Loss: 0.00003638
Iteration 200/1000 | Loss: 0.00003638
Iteration 201/1000 | Loss: 0.00003638
Iteration 202/1000 | Loss: 0.00003638
Iteration 203/1000 | Loss: 0.00003638
Iteration 204/1000 | Loss: 0.00003638
Iteration 205/1000 | Loss: 0.00003638
Iteration 206/1000 | Loss: 0.00003638
Iteration 207/1000 | Loss: 0.00003638
Iteration 208/1000 | Loss: 0.00003638
Iteration 209/1000 | Loss: 0.00003638
Iteration 210/1000 | Loss: 0.00003638
Iteration 211/1000 | Loss: 0.00003638
Iteration 212/1000 | Loss: 0.00003638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [3.6379118682816625e-05, 3.6379118682816625e-05, 3.6379118682816625e-05, 3.6379118682816625e-05, 3.6379118682816625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6379118682816625e-05

Optimization complete. Final v2v error: 5.158036708831787 mm

Highest mean error: 5.869601726531982 mm for frame 28

Lowest mean error: 4.328829288482666 mm for frame 236

Saving results

Total time: 59.988653898239136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1502/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1502/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01162756
Iteration 2/25 | Loss: 0.00172624
Iteration 3/25 | Loss: 0.00119453
Iteration 4/25 | Loss: 0.00114806
Iteration 5/25 | Loss: 0.00113373
Iteration 6/25 | Loss: 0.00113013
Iteration 7/25 | Loss: 0.00112931
Iteration 8/25 | Loss: 0.00112931
Iteration 9/25 | Loss: 0.00112931
Iteration 10/25 | Loss: 0.00112931
Iteration 11/25 | Loss: 0.00112931
Iteration 12/25 | Loss: 0.00112931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011293069692328572, 0.0011293069692328572, 0.0011293069692328572, 0.0011293069692328572, 0.0011293069692328572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011293069692328572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91676754
Iteration 2/25 | Loss: 0.00047123
Iteration 3/25 | Loss: 0.00047123
Iteration 4/25 | Loss: 0.00047123
Iteration 5/25 | Loss: 0.00047123
Iteration 6/25 | Loss: 0.00047123
Iteration 7/25 | Loss: 0.00047123
Iteration 8/25 | Loss: 0.00047123
Iteration 9/25 | Loss: 0.00047123
Iteration 10/25 | Loss: 0.00047123
Iteration 11/25 | Loss: 0.00047123
Iteration 12/25 | Loss: 0.00047123
Iteration 13/25 | Loss: 0.00047123
Iteration 14/25 | Loss: 0.00047123
Iteration 15/25 | Loss: 0.00047123
Iteration 16/25 | Loss: 0.00047123
Iteration 17/25 | Loss: 0.00047123
Iteration 18/25 | Loss: 0.00047123
Iteration 19/25 | Loss: 0.00047123
Iteration 20/25 | Loss: 0.00047123
Iteration 21/25 | Loss: 0.00047123
Iteration 22/25 | Loss: 0.00047123
Iteration 23/25 | Loss: 0.00047123
Iteration 24/25 | Loss: 0.00047123
Iteration 25/25 | Loss: 0.00047123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047123
Iteration 2/1000 | Loss: 0.00006598
Iteration 3/1000 | Loss: 0.00005355
Iteration 4/1000 | Loss: 0.00004711
Iteration 5/1000 | Loss: 0.00004456
Iteration 6/1000 | Loss: 0.00004276
Iteration 7/1000 | Loss: 0.00004166
Iteration 8/1000 | Loss: 0.00004123
Iteration 9/1000 | Loss: 0.00004076
Iteration 10/1000 | Loss: 0.00004035
Iteration 11/1000 | Loss: 0.00004012
Iteration 12/1000 | Loss: 0.00004007
Iteration 13/1000 | Loss: 0.00004007
Iteration 14/1000 | Loss: 0.00004007
Iteration 15/1000 | Loss: 0.00004002
Iteration 16/1000 | Loss: 0.00004002
Iteration 17/1000 | Loss: 0.00004001
Iteration 18/1000 | Loss: 0.00004001
Iteration 19/1000 | Loss: 0.00004001
Iteration 20/1000 | Loss: 0.00004001
Iteration 21/1000 | Loss: 0.00004001
Iteration 22/1000 | Loss: 0.00004001
Iteration 23/1000 | Loss: 0.00004001
Iteration 24/1000 | Loss: 0.00004001
Iteration 25/1000 | Loss: 0.00004001
Iteration 26/1000 | Loss: 0.00004000
Iteration 27/1000 | Loss: 0.00004000
Iteration 28/1000 | Loss: 0.00004000
Iteration 29/1000 | Loss: 0.00003999
Iteration 30/1000 | Loss: 0.00003999
Iteration 31/1000 | Loss: 0.00003999
Iteration 32/1000 | Loss: 0.00003998
Iteration 33/1000 | Loss: 0.00003998
Iteration 34/1000 | Loss: 0.00003998
Iteration 35/1000 | Loss: 0.00003998
Iteration 36/1000 | Loss: 0.00003998
Iteration 37/1000 | Loss: 0.00003997
Iteration 38/1000 | Loss: 0.00003997
Iteration 39/1000 | Loss: 0.00003997
Iteration 40/1000 | Loss: 0.00003996
Iteration 41/1000 | Loss: 0.00003996
Iteration 42/1000 | Loss: 0.00003995
Iteration 43/1000 | Loss: 0.00003995
Iteration 44/1000 | Loss: 0.00003995
Iteration 45/1000 | Loss: 0.00003995
Iteration 46/1000 | Loss: 0.00003995
Iteration 47/1000 | Loss: 0.00003995
Iteration 48/1000 | Loss: 0.00003995
Iteration 49/1000 | Loss: 0.00003994
Iteration 50/1000 | Loss: 0.00003994
Iteration 51/1000 | Loss: 0.00003994
Iteration 52/1000 | Loss: 0.00003994
Iteration 53/1000 | Loss: 0.00003993
Iteration 54/1000 | Loss: 0.00003993
Iteration 55/1000 | Loss: 0.00003992
Iteration 56/1000 | Loss: 0.00003992
Iteration 57/1000 | Loss: 0.00003992
Iteration 58/1000 | Loss: 0.00003992
Iteration 59/1000 | Loss: 0.00003992
Iteration 60/1000 | Loss: 0.00003992
Iteration 61/1000 | Loss: 0.00003991
Iteration 62/1000 | Loss: 0.00003991
Iteration 63/1000 | Loss: 0.00003991
Iteration 64/1000 | Loss: 0.00003991
Iteration 65/1000 | Loss: 0.00003991
Iteration 66/1000 | Loss: 0.00003991
Iteration 67/1000 | Loss: 0.00003990
Iteration 68/1000 | Loss: 0.00003990
Iteration 69/1000 | Loss: 0.00003990
Iteration 70/1000 | Loss: 0.00003989
Iteration 71/1000 | Loss: 0.00003989
Iteration 72/1000 | Loss: 0.00003989
Iteration 73/1000 | Loss: 0.00003989
Iteration 74/1000 | Loss: 0.00003989
Iteration 75/1000 | Loss: 0.00003989
Iteration 76/1000 | Loss: 0.00003989
Iteration 77/1000 | Loss: 0.00003988
Iteration 78/1000 | Loss: 0.00003988
Iteration 79/1000 | Loss: 0.00003988
Iteration 80/1000 | Loss: 0.00003988
Iteration 81/1000 | Loss: 0.00003988
Iteration 82/1000 | Loss: 0.00003988
Iteration 83/1000 | Loss: 0.00003987
Iteration 84/1000 | Loss: 0.00003987
Iteration 85/1000 | Loss: 0.00003987
Iteration 86/1000 | Loss: 0.00003987
Iteration 87/1000 | Loss: 0.00003987
Iteration 88/1000 | Loss: 0.00003987
Iteration 89/1000 | Loss: 0.00003987
Iteration 90/1000 | Loss: 0.00003987
Iteration 91/1000 | Loss: 0.00003987
Iteration 92/1000 | Loss: 0.00003986
Iteration 93/1000 | Loss: 0.00003986
Iteration 94/1000 | Loss: 0.00003986
Iteration 95/1000 | Loss: 0.00003986
Iteration 96/1000 | Loss: 0.00003986
Iteration 97/1000 | Loss: 0.00003986
Iteration 98/1000 | Loss: 0.00003986
Iteration 99/1000 | Loss: 0.00003986
Iteration 100/1000 | Loss: 0.00003985
Iteration 101/1000 | Loss: 0.00003985
Iteration 102/1000 | Loss: 0.00003985
Iteration 103/1000 | Loss: 0.00003985
Iteration 104/1000 | Loss: 0.00003985
Iteration 105/1000 | Loss: 0.00003985
Iteration 106/1000 | Loss: 0.00003985
Iteration 107/1000 | Loss: 0.00003984
Iteration 108/1000 | Loss: 0.00003984
Iteration 109/1000 | Loss: 0.00003984
Iteration 110/1000 | Loss: 0.00003984
Iteration 111/1000 | Loss: 0.00003984
Iteration 112/1000 | Loss: 0.00003984
Iteration 113/1000 | Loss: 0.00003984
Iteration 114/1000 | Loss: 0.00003984
Iteration 115/1000 | Loss: 0.00003983
Iteration 116/1000 | Loss: 0.00003983
Iteration 117/1000 | Loss: 0.00003983
Iteration 118/1000 | Loss: 0.00003983
Iteration 119/1000 | Loss: 0.00003983
Iteration 120/1000 | Loss: 0.00003983
Iteration 121/1000 | Loss: 0.00003983
Iteration 122/1000 | Loss: 0.00003983
Iteration 123/1000 | Loss: 0.00003983
Iteration 124/1000 | Loss: 0.00003983
Iteration 125/1000 | Loss: 0.00003983
Iteration 126/1000 | Loss: 0.00003983
Iteration 127/1000 | Loss: 0.00003983
Iteration 128/1000 | Loss: 0.00003983
Iteration 129/1000 | Loss: 0.00003983
Iteration 130/1000 | Loss: 0.00003983
Iteration 131/1000 | Loss: 0.00003983
Iteration 132/1000 | Loss: 0.00003983
Iteration 133/1000 | Loss: 0.00003983
Iteration 134/1000 | Loss: 0.00003983
Iteration 135/1000 | Loss: 0.00003983
Iteration 136/1000 | Loss: 0.00003983
Iteration 137/1000 | Loss: 0.00003983
Iteration 138/1000 | Loss: 0.00003983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [3.9825037674745545e-05, 3.9825037674745545e-05, 3.9825037674745545e-05, 3.9825037674745545e-05, 3.9825037674745545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9825037674745545e-05

Optimization complete. Final v2v error: 5.299683094024658 mm

Highest mean error: 5.807640075683594 mm for frame 4

Lowest mean error: 4.664608478546143 mm for frame 105

Saving results

Total time: 39.65520930290222
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00905075
Iteration 2/25 | Loss: 0.00155693
Iteration 3/25 | Loss: 0.00128747
Iteration 4/25 | Loss: 0.00125712
Iteration 5/25 | Loss: 0.00126682
Iteration 6/25 | Loss: 0.00126832
Iteration 7/25 | Loss: 0.00124990
Iteration 8/25 | Loss: 0.00124605
Iteration 9/25 | Loss: 0.00124610
Iteration 10/25 | Loss: 0.00123961
Iteration 11/25 | Loss: 0.00123643
Iteration 12/25 | Loss: 0.00123704
Iteration 13/25 | Loss: 0.00123535
Iteration 14/25 | Loss: 0.00123485
Iteration 15/25 | Loss: 0.00123386
Iteration 16/25 | Loss: 0.00123329
Iteration 17/25 | Loss: 0.00123639
Iteration 18/25 | Loss: 0.00123449
Iteration 19/25 | Loss: 0.00123143
Iteration 20/25 | Loss: 0.00123013
Iteration 21/25 | Loss: 0.00122987
Iteration 22/25 | Loss: 0.00122978
Iteration 23/25 | Loss: 0.00122970
Iteration 24/25 | Loss: 0.00122968
Iteration 25/25 | Loss: 0.00122968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.05765104
Iteration 2/25 | Loss: 0.00059118
Iteration 3/25 | Loss: 0.00059112
Iteration 4/25 | Loss: 0.00059112
Iteration 5/25 | Loss: 0.00059111
Iteration 6/25 | Loss: 0.00059111
Iteration 7/25 | Loss: 0.00059111
Iteration 8/25 | Loss: 0.00059111
Iteration 9/25 | Loss: 0.00059111
Iteration 10/25 | Loss: 0.00059111
Iteration 11/25 | Loss: 0.00059111
Iteration 12/25 | Loss: 0.00059111
Iteration 13/25 | Loss: 0.00059111
Iteration 14/25 | Loss: 0.00059111
Iteration 15/25 | Loss: 0.00059111
Iteration 16/25 | Loss: 0.00059111
Iteration 17/25 | Loss: 0.00059111
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000591112591791898, 0.000591112591791898, 0.000591112591791898, 0.000591112591791898, 0.000591112591791898]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000591112591791898

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059111
Iteration 2/1000 | Loss: 0.00004389
Iteration 3/1000 | Loss: 0.00002661
Iteration 4/1000 | Loss: 0.00002410
Iteration 5/1000 | Loss: 0.00002300
Iteration 6/1000 | Loss: 0.00002243
Iteration 7/1000 | Loss: 0.00002197
Iteration 8/1000 | Loss: 0.00002158
Iteration 9/1000 | Loss: 0.00002127
Iteration 10/1000 | Loss: 0.00002107
Iteration 11/1000 | Loss: 0.00002097
Iteration 12/1000 | Loss: 0.00002093
Iteration 13/1000 | Loss: 0.00002083
Iteration 14/1000 | Loss: 0.00002079
Iteration 15/1000 | Loss: 0.00002078
Iteration 16/1000 | Loss: 0.00002077
Iteration 17/1000 | Loss: 0.00002076
Iteration 18/1000 | Loss: 0.00002075
Iteration 19/1000 | Loss: 0.00002074
Iteration 20/1000 | Loss: 0.00002074
Iteration 21/1000 | Loss: 0.00002073
Iteration 22/1000 | Loss: 0.00002072
Iteration 23/1000 | Loss: 0.00002072
Iteration 24/1000 | Loss: 0.00002072
Iteration 25/1000 | Loss: 0.00002071
Iteration 26/1000 | Loss: 0.00002071
Iteration 27/1000 | Loss: 0.00002071
Iteration 28/1000 | Loss: 0.00002071
Iteration 29/1000 | Loss: 0.00002070
Iteration 30/1000 | Loss: 0.00002070
Iteration 31/1000 | Loss: 0.00002070
Iteration 32/1000 | Loss: 0.00002069
Iteration 33/1000 | Loss: 0.00002069
Iteration 34/1000 | Loss: 0.00002069
Iteration 35/1000 | Loss: 0.00002069
Iteration 36/1000 | Loss: 0.00002069
Iteration 37/1000 | Loss: 0.00002069
Iteration 38/1000 | Loss: 0.00002069
Iteration 39/1000 | Loss: 0.00002069
Iteration 40/1000 | Loss: 0.00002069
Iteration 41/1000 | Loss: 0.00002069
Iteration 42/1000 | Loss: 0.00002069
Iteration 43/1000 | Loss: 0.00002069
Iteration 44/1000 | Loss: 0.00002069
Iteration 45/1000 | Loss: 0.00002068
Iteration 46/1000 | Loss: 0.00002068
Iteration 47/1000 | Loss: 0.00002068
Iteration 48/1000 | Loss: 0.00002068
Iteration 49/1000 | Loss: 0.00002068
Iteration 50/1000 | Loss: 0.00002068
Iteration 51/1000 | Loss: 0.00002068
Iteration 52/1000 | Loss: 0.00002068
Iteration 53/1000 | Loss: 0.00002067
Iteration 54/1000 | Loss: 0.00002067
Iteration 55/1000 | Loss: 0.00002067
Iteration 56/1000 | Loss: 0.00002067
Iteration 57/1000 | Loss: 0.00002067
Iteration 58/1000 | Loss: 0.00002067
Iteration 59/1000 | Loss: 0.00002066
Iteration 60/1000 | Loss: 0.00002066
Iteration 61/1000 | Loss: 0.00002066
Iteration 62/1000 | Loss: 0.00002066
Iteration 63/1000 | Loss: 0.00002066
Iteration 64/1000 | Loss: 0.00002066
Iteration 65/1000 | Loss: 0.00002066
Iteration 66/1000 | Loss: 0.00002066
Iteration 67/1000 | Loss: 0.00002065
Iteration 68/1000 | Loss: 0.00002065
Iteration 69/1000 | Loss: 0.00002065
Iteration 70/1000 | Loss: 0.00002065
Iteration 71/1000 | Loss: 0.00002065
Iteration 72/1000 | Loss: 0.00002065
Iteration 73/1000 | Loss: 0.00002065
Iteration 74/1000 | Loss: 0.00002065
Iteration 75/1000 | Loss: 0.00002065
Iteration 76/1000 | Loss: 0.00002065
Iteration 77/1000 | Loss: 0.00002065
Iteration 78/1000 | Loss: 0.00002065
Iteration 79/1000 | Loss: 0.00002065
Iteration 80/1000 | Loss: 0.00002065
Iteration 81/1000 | Loss: 0.00002065
Iteration 82/1000 | Loss: 0.00002065
Iteration 83/1000 | Loss: 0.00002065
Iteration 84/1000 | Loss: 0.00002065
Iteration 85/1000 | Loss: 0.00002065
Iteration 86/1000 | Loss: 0.00002065
Iteration 87/1000 | Loss: 0.00002065
Iteration 88/1000 | Loss: 0.00002065
Iteration 89/1000 | Loss: 0.00002065
Iteration 90/1000 | Loss: 0.00002065
Iteration 91/1000 | Loss: 0.00002065
Iteration 92/1000 | Loss: 0.00002065
Iteration 93/1000 | Loss: 0.00002065
Iteration 94/1000 | Loss: 0.00002065
Iteration 95/1000 | Loss: 0.00002065
Iteration 96/1000 | Loss: 0.00002065
Iteration 97/1000 | Loss: 0.00002065
Iteration 98/1000 | Loss: 0.00002065
Iteration 99/1000 | Loss: 0.00002065
Iteration 100/1000 | Loss: 0.00002065
Iteration 101/1000 | Loss: 0.00002065
Iteration 102/1000 | Loss: 0.00002065
Iteration 103/1000 | Loss: 0.00002065
Iteration 104/1000 | Loss: 0.00002065
Iteration 105/1000 | Loss: 0.00002065
Iteration 106/1000 | Loss: 0.00002065
Iteration 107/1000 | Loss: 0.00002065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.0648280042223632e-05, 2.0648280042223632e-05, 2.0648280042223632e-05, 2.0648280042223632e-05, 2.0648280042223632e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0648280042223632e-05

Optimization complete. Final v2v error: 3.9027881622314453 mm

Highest mean error: 7.130768775939941 mm for frame 59

Lowest mean error: 3.0533664226531982 mm for frame 174

Saving results

Total time: 64.3503065109253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904653
Iteration 2/25 | Loss: 0.00181357
Iteration 3/25 | Loss: 0.00139915
Iteration 4/25 | Loss: 0.00133929
Iteration 5/25 | Loss: 0.00132851
Iteration 6/25 | Loss: 0.00131687
Iteration 7/25 | Loss: 0.00130883
Iteration 8/25 | Loss: 0.00133186
Iteration 9/25 | Loss: 0.00131661
Iteration 10/25 | Loss: 0.00130651
Iteration 11/25 | Loss: 0.00129848
Iteration 12/25 | Loss: 0.00129396
Iteration 13/25 | Loss: 0.00129310
Iteration 14/25 | Loss: 0.00129293
Iteration 15/25 | Loss: 0.00129289
Iteration 16/25 | Loss: 0.00129289
Iteration 17/25 | Loss: 0.00129289
Iteration 18/25 | Loss: 0.00129289
Iteration 19/25 | Loss: 0.00129289
Iteration 20/25 | Loss: 0.00129289
Iteration 21/25 | Loss: 0.00129288
Iteration 22/25 | Loss: 0.00129288
Iteration 23/25 | Loss: 0.00129288
Iteration 24/25 | Loss: 0.00129288
Iteration 25/25 | Loss: 0.00129288

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24003386
Iteration 2/25 | Loss: 0.00066180
Iteration 3/25 | Loss: 0.00066176
Iteration 4/25 | Loss: 0.00066176
Iteration 5/25 | Loss: 0.00066175
Iteration 6/25 | Loss: 0.00066175
Iteration 7/25 | Loss: 0.00066175
Iteration 8/25 | Loss: 0.00066175
Iteration 9/25 | Loss: 0.00066175
Iteration 10/25 | Loss: 0.00066175
Iteration 11/25 | Loss: 0.00066175
Iteration 12/25 | Loss: 0.00066175
Iteration 13/25 | Loss: 0.00066175
Iteration 14/25 | Loss: 0.00066175
Iteration 15/25 | Loss: 0.00066175
Iteration 16/25 | Loss: 0.00066175
Iteration 17/25 | Loss: 0.00066175
Iteration 18/25 | Loss: 0.00066175
Iteration 19/25 | Loss: 0.00066175
Iteration 20/25 | Loss: 0.00066175
Iteration 21/25 | Loss: 0.00066175
Iteration 22/25 | Loss: 0.00066175
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006617530598305166, 0.0006617530598305166, 0.0006617530598305166, 0.0006617530598305166, 0.0006617530598305166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006617530598305166

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066175
Iteration 2/1000 | Loss: 0.00007833
Iteration 3/1000 | Loss: 0.00004711
Iteration 4/1000 | Loss: 0.00003522
Iteration 5/1000 | Loss: 0.00003271
Iteration 6/1000 | Loss: 0.00003067
Iteration 7/1000 | Loss: 0.00002928
Iteration 8/1000 | Loss: 0.00002806
Iteration 9/1000 | Loss: 0.00002706
Iteration 10/1000 | Loss: 0.00002625
Iteration 11/1000 | Loss: 0.00002559
Iteration 12/1000 | Loss: 0.00002493
Iteration 13/1000 | Loss: 0.00002448
Iteration 14/1000 | Loss: 0.00002422
Iteration 15/1000 | Loss: 0.00002405
Iteration 16/1000 | Loss: 0.00002392
Iteration 17/1000 | Loss: 0.00002385
Iteration 18/1000 | Loss: 0.00002385
Iteration 19/1000 | Loss: 0.00002380
Iteration 20/1000 | Loss: 0.00002377
Iteration 21/1000 | Loss: 0.00002377
Iteration 22/1000 | Loss: 0.00002375
Iteration 23/1000 | Loss: 0.00002375
Iteration 24/1000 | Loss: 0.00002375
Iteration 25/1000 | Loss: 0.00002375
Iteration 26/1000 | Loss: 0.00002374
Iteration 27/1000 | Loss: 0.00002374
Iteration 28/1000 | Loss: 0.00002374
Iteration 29/1000 | Loss: 0.00002374
Iteration 30/1000 | Loss: 0.00002374
Iteration 31/1000 | Loss: 0.00002373
Iteration 32/1000 | Loss: 0.00002373
Iteration 33/1000 | Loss: 0.00002373
Iteration 34/1000 | Loss: 0.00002372
Iteration 35/1000 | Loss: 0.00002372
Iteration 36/1000 | Loss: 0.00002372
Iteration 37/1000 | Loss: 0.00002372
Iteration 38/1000 | Loss: 0.00002372
Iteration 39/1000 | Loss: 0.00002372
Iteration 40/1000 | Loss: 0.00002372
Iteration 41/1000 | Loss: 0.00002372
Iteration 42/1000 | Loss: 0.00002372
Iteration 43/1000 | Loss: 0.00002371
Iteration 44/1000 | Loss: 0.00002371
Iteration 45/1000 | Loss: 0.00002371
Iteration 46/1000 | Loss: 0.00002371
Iteration 47/1000 | Loss: 0.00002371
Iteration 48/1000 | Loss: 0.00002371
Iteration 49/1000 | Loss: 0.00002370
Iteration 50/1000 | Loss: 0.00002370
Iteration 51/1000 | Loss: 0.00002370
Iteration 52/1000 | Loss: 0.00002370
Iteration 53/1000 | Loss: 0.00002369
Iteration 54/1000 | Loss: 0.00002369
Iteration 55/1000 | Loss: 0.00002369
Iteration 56/1000 | Loss: 0.00002369
Iteration 57/1000 | Loss: 0.00002369
Iteration 58/1000 | Loss: 0.00002369
Iteration 59/1000 | Loss: 0.00002369
Iteration 60/1000 | Loss: 0.00002369
Iteration 61/1000 | Loss: 0.00002369
Iteration 62/1000 | Loss: 0.00002369
Iteration 63/1000 | Loss: 0.00002368
Iteration 64/1000 | Loss: 0.00002368
Iteration 65/1000 | Loss: 0.00002368
Iteration 66/1000 | Loss: 0.00002368
Iteration 67/1000 | Loss: 0.00002368
Iteration 68/1000 | Loss: 0.00002368
Iteration 69/1000 | Loss: 0.00002368
Iteration 70/1000 | Loss: 0.00002368
Iteration 71/1000 | Loss: 0.00002368
Iteration 72/1000 | Loss: 0.00002368
Iteration 73/1000 | Loss: 0.00002367
Iteration 74/1000 | Loss: 0.00002367
Iteration 75/1000 | Loss: 0.00002367
Iteration 76/1000 | Loss: 0.00002367
Iteration 77/1000 | Loss: 0.00002367
Iteration 78/1000 | Loss: 0.00002366
Iteration 79/1000 | Loss: 0.00002366
Iteration 80/1000 | Loss: 0.00002366
Iteration 81/1000 | Loss: 0.00002366
Iteration 82/1000 | Loss: 0.00002366
Iteration 83/1000 | Loss: 0.00002366
Iteration 84/1000 | Loss: 0.00002366
Iteration 85/1000 | Loss: 0.00002366
Iteration 86/1000 | Loss: 0.00002365
Iteration 87/1000 | Loss: 0.00002365
Iteration 88/1000 | Loss: 0.00002365
Iteration 89/1000 | Loss: 0.00002364
Iteration 90/1000 | Loss: 0.00002364
Iteration 91/1000 | Loss: 0.00002364
Iteration 92/1000 | Loss: 0.00002364
Iteration 93/1000 | Loss: 0.00002363
Iteration 94/1000 | Loss: 0.00002363
Iteration 95/1000 | Loss: 0.00002363
Iteration 96/1000 | Loss: 0.00002363
Iteration 97/1000 | Loss: 0.00002363
Iteration 98/1000 | Loss: 0.00002363
Iteration 99/1000 | Loss: 0.00002363
Iteration 100/1000 | Loss: 0.00002363
Iteration 101/1000 | Loss: 0.00002362
Iteration 102/1000 | Loss: 0.00002362
Iteration 103/1000 | Loss: 0.00002362
Iteration 104/1000 | Loss: 0.00002362
Iteration 105/1000 | Loss: 0.00002362
Iteration 106/1000 | Loss: 0.00002362
Iteration 107/1000 | Loss: 0.00002362
Iteration 108/1000 | Loss: 0.00002361
Iteration 109/1000 | Loss: 0.00002361
Iteration 110/1000 | Loss: 0.00002361
Iteration 111/1000 | Loss: 0.00002361
Iteration 112/1000 | Loss: 0.00002361
Iteration 113/1000 | Loss: 0.00002361
Iteration 114/1000 | Loss: 0.00002361
Iteration 115/1000 | Loss: 0.00002361
Iteration 116/1000 | Loss: 0.00002360
Iteration 117/1000 | Loss: 0.00002360
Iteration 118/1000 | Loss: 0.00002360
Iteration 119/1000 | Loss: 0.00002360
Iteration 120/1000 | Loss: 0.00002360
Iteration 121/1000 | Loss: 0.00002360
Iteration 122/1000 | Loss: 0.00002360
Iteration 123/1000 | Loss: 0.00002359
Iteration 124/1000 | Loss: 0.00002359
Iteration 125/1000 | Loss: 0.00002359
Iteration 126/1000 | Loss: 0.00002359
Iteration 127/1000 | Loss: 0.00002359
Iteration 128/1000 | Loss: 0.00002359
Iteration 129/1000 | Loss: 0.00002359
Iteration 130/1000 | Loss: 0.00002359
Iteration 131/1000 | Loss: 0.00002359
Iteration 132/1000 | Loss: 0.00002359
Iteration 133/1000 | Loss: 0.00002359
Iteration 134/1000 | Loss: 0.00002359
Iteration 135/1000 | Loss: 0.00002359
Iteration 136/1000 | Loss: 0.00002359
Iteration 137/1000 | Loss: 0.00002358
Iteration 138/1000 | Loss: 0.00002358
Iteration 139/1000 | Loss: 0.00002358
Iteration 140/1000 | Loss: 0.00002358
Iteration 141/1000 | Loss: 0.00002358
Iteration 142/1000 | Loss: 0.00002358
Iteration 143/1000 | Loss: 0.00002358
Iteration 144/1000 | Loss: 0.00002358
Iteration 145/1000 | Loss: 0.00002358
Iteration 146/1000 | Loss: 0.00002358
Iteration 147/1000 | Loss: 0.00002358
Iteration 148/1000 | Loss: 0.00002358
Iteration 149/1000 | Loss: 0.00002358
Iteration 150/1000 | Loss: 0.00002358
Iteration 151/1000 | Loss: 0.00002358
Iteration 152/1000 | Loss: 0.00002358
Iteration 153/1000 | Loss: 0.00002358
Iteration 154/1000 | Loss: 0.00002358
Iteration 155/1000 | Loss: 0.00002358
Iteration 156/1000 | Loss: 0.00002358
Iteration 157/1000 | Loss: 0.00002358
Iteration 158/1000 | Loss: 0.00002358
Iteration 159/1000 | Loss: 0.00002358
Iteration 160/1000 | Loss: 0.00002358
Iteration 161/1000 | Loss: 0.00002358
Iteration 162/1000 | Loss: 0.00002358
Iteration 163/1000 | Loss: 0.00002358
Iteration 164/1000 | Loss: 0.00002358
Iteration 165/1000 | Loss: 0.00002358
Iteration 166/1000 | Loss: 0.00002358
Iteration 167/1000 | Loss: 0.00002358
Iteration 168/1000 | Loss: 0.00002358
Iteration 169/1000 | Loss: 0.00002358
Iteration 170/1000 | Loss: 0.00002358
Iteration 171/1000 | Loss: 0.00002358
Iteration 172/1000 | Loss: 0.00002358
Iteration 173/1000 | Loss: 0.00002358
Iteration 174/1000 | Loss: 0.00002358
Iteration 175/1000 | Loss: 0.00002358
Iteration 176/1000 | Loss: 0.00002358
Iteration 177/1000 | Loss: 0.00002358
Iteration 178/1000 | Loss: 0.00002358
Iteration 179/1000 | Loss: 0.00002358
Iteration 180/1000 | Loss: 0.00002358
Iteration 181/1000 | Loss: 0.00002358
Iteration 182/1000 | Loss: 0.00002358
Iteration 183/1000 | Loss: 0.00002358
Iteration 184/1000 | Loss: 0.00002358
Iteration 185/1000 | Loss: 0.00002358
Iteration 186/1000 | Loss: 0.00002358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [2.357585253776051e-05, 2.357585253776051e-05, 2.357585253776051e-05, 2.357585253776051e-05, 2.357585253776051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.357585253776051e-05

Optimization complete. Final v2v error: 4.132791042327881 mm

Highest mean error: 4.4050679206848145 mm for frame 149

Lowest mean error: 3.8500144481658936 mm for frame 36

Saving results

Total time: 67.44207644462585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793258
Iteration 2/25 | Loss: 0.00149430
Iteration 3/25 | Loss: 0.00134255
Iteration 4/25 | Loss: 0.00127152
Iteration 5/25 | Loss: 0.00125114
Iteration 6/25 | Loss: 0.00124704
Iteration 7/25 | Loss: 0.00124561
Iteration 8/25 | Loss: 0.00124504
Iteration 9/25 | Loss: 0.00124491
Iteration 10/25 | Loss: 0.00124491
Iteration 11/25 | Loss: 0.00124491
Iteration 12/25 | Loss: 0.00124491
Iteration 13/25 | Loss: 0.00124491
Iteration 14/25 | Loss: 0.00124491
Iteration 15/25 | Loss: 0.00124491
Iteration 16/25 | Loss: 0.00124491
Iteration 17/25 | Loss: 0.00124491
Iteration 18/25 | Loss: 0.00124491
Iteration 19/25 | Loss: 0.00124491
Iteration 20/25 | Loss: 0.00124491
Iteration 21/25 | Loss: 0.00124491
Iteration 22/25 | Loss: 0.00124491
Iteration 23/25 | Loss: 0.00124491
Iteration 24/25 | Loss: 0.00124491
Iteration 25/25 | Loss: 0.00124491

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.68442202
Iteration 2/25 | Loss: 0.00068189
Iteration 3/25 | Loss: 0.00068185
Iteration 4/25 | Loss: 0.00068185
Iteration 5/25 | Loss: 0.00068185
Iteration 6/25 | Loss: 0.00068185
Iteration 7/25 | Loss: 0.00068185
Iteration 8/25 | Loss: 0.00068185
Iteration 9/25 | Loss: 0.00068184
Iteration 10/25 | Loss: 0.00068184
Iteration 11/25 | Loss: 0.00068184
Iteration 12/25 | Loss: 0.00068184
Iteration 13/25 | Loss: 0.00068184
Iteration 14/25 | Loss: 0.00068184
Iteration 15/25 | Loss: 0.00068184
Iteration 16/25 | Loss: 0.00068184
Iteration 17/25 | Loss: 0.00068184
Iteration 18/25 | Loss: 0.00068184
Iteration 19/25 | Loss: 0.00068184
Iteration 20/25 | Loss: 0.00068184
Iteration 21/25 | Loss: 0.00068184
Iteration 22/25 | Loss: 0.00068184
Iteration 23/25 | Loss: 0.00068184
Iteration 24/25 | Loss: 0.00068184
Iteration 25/25 | Loss: 0.00068184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068184
Iteration 2/1000 | Loss: 0.00006876
Iteration 3/1000 | Loss: 0.00003228
Iteration 4/1000 | Loss: 0.00002620
Iteration 5/1000 | Loss: 0.00002454
Iteration 6/1000 | Loss: 0.00002350
Iteration 7/1000 | Loss: 0.00002272
Iteration 8/1000 | Loss: 0.00002222
Iteration 9/1000 | Loss: 0.00002185
Iteration 10/1000 | Loss: 0.00002140
Iteration 11/1000 | Loss: 0.00002112
Iteration 12/1000 | Loss: 0.00002093
Iteration 13/1000 | Loss: 0.00002092
Iteration 14/1000 | Loss: 0.00002086
Iteration 15/1000 | Loss: 0.00002082
Iteration 16/1000 | Loss: 0.00002080
Iteration 17/1000 | Loss: 0.00002079
Iteration 18/1000 | Loss: 0.00002079
Iteration 19/1000 | Loss: 0.00002077
Iteration 20/1000 | Loss: 0.00002075
Iteration 21/1000 | Loss: 0.00002073
Iteration 22/1000 | Loss: 0.00002073
Iteration 23/1000 | Loss: 0.00002072
Iteration 24/1000 | Loss: 0.00002070
Iteration 25/1000 | Loss: 0.00002069
Iteration 26/1000 | Loss: 0.00002069
Iteration 27/1000 | Loss: 0.00002068
Iteration 28/1000 | Loss: 0.00002067
Iteration 29/1000 | Loss: 0.00002066
Iteration 30/1000 | Loss: 0.00002065
Iteration 31/1000 | Loss: 0.00002064
Iteration 32/1000 | Loss: 0.00002063
Iteration 33/1000 | Loss: 0.00002060
Iteration 34/1000 | Loss: 0.00002060
Iteration 35/1000 | Loss: 0.00002060
Iteration 36/1000 | Loss: 0.00002060
Iteration 37/1000 | Loss: 0.00002060
Iteration 38/1000 | Loss: 0.00002060
Iteration 39/1000 | Loss: 0.00002060
Iteration 40/1000 | Loss: 0.00002060
Iteration 41/1000 | Loss: 0.00002059
Iteration 42/1000 | Loss: 0.00002059
Iteration 43/1000 | Loss: 0.00002059
Iteration 44/1000 | Loss: 0.00002059
Iteration 45/1000 | Loss: 0.00002059
Iteration 46/1000 | Loss: 0.00002058
Iteration 47/1000 | Loss: 0.00002058
Iteration 48/1000 | Loss: 0.00002058
Iteration 49/1000 | Loss: 0.00002057
Iteration 50/1000 | Loss: 0.00002057
Iteration 51/1000 | Loss: 0.00002057
Iteration 52/1000 | Loss: 0.00002056
Iteration 53/1000 | Loss: 0.00002056
Iteration 54/1000 | Loss: 0.00002056
Iteration 55/1000 | Loss: 0.00002056
Iteration 56/1000 | Loss: 0.00002056
Iteration 57/1000 | Loss: 0.00002055
Iteration 58/1000 | Loss: 0.00002055
Iteration 59/1000 | Loss: 0.00002055
Iteration 60/1000 | Loss: 0.00002055
Iteration 61/1000 | Loss: 0.00002055
Iteration 62/1000 | Loss: 0.00002055
Iteration 63/1000 | Loss: 0.00002055
Iteration 64/1000 | Loss: 0.00002054
Iteration 65/1000 | Loss: 0.00002054
Iteration 66/1000 | Loss: 0.00002053
Iteration 67/1000 | Loss: 0.00002053
Iteration 68/1000 | Loss: 0.00002053
Iteration 69/1000 | Loss: 0.00002053
Iteration 70/1000 | Loss: 0.00002053
Iteration 71/1000 | Loss: 0.00002053
Iteration 72/1000 | Loss: 0.00002053
Iteration 73/1000 | Loss: 0.00002053
Iteration 74/1000 | Loss: 0.00002052
Iteration 75/1000 | Loss: 0.00002052
Iteration 76/1000 | Loss: 0.00002052
Iteration 77/1000 | Loss: 0.00002052
Iteration 78/1000 | Loss: 0.00002052
Iteration 79/1000 | Loss: 0.00002052
Iteration 80/1000 | Loss: 0.00002052
Iteration 81/1000 | Loss: 0.00002051
Iteration 82/1000 | Loss: 0.00002051
Iteration 83/1000 | Loss: 0.00002051
Iteration 84/1000 | Loss: 0.00002051
Iteration 85/1000 | Loss: 0.00002051
Iteration 86/1000 | Loss: 0.00002051
Iteration 87/1000 | Loss: 0.00002051
Iteration 88/1000 | Loss: 0.00002051
Iteration 89/1000 | Loss: 0.00002051
Iteration 90/1000 | Loss: 0.00002051
Iteration 91/1000 | Loss: 0.00002051
Iteration 92/1000 | Loss: 0.00002051
Iteration 93/1000 | Loss: 0.00002051
Iteration 94/1000 | Loss: 0.00002051
Iteration 95/1000 | Loss: 0.00002051
Iteration 96/1000 | Loss: 0.00002051
Iteration 97/1000 | Loss: 0.00002051
Iteration 98/1000 | Loss: 0.00002051
Iteration 99/1000 | Loss: 0.00002051
Iteration 100/1000 | Loss: 0.00002050
Iteration 101/1000 | Loss: 0.00002050
Iteration 102/1000 | Loss: 0.00002050
Iteration 103/1000 | Loss: 0.00002050
Iteration 104/1000 | Loss: 0.00002050
Iteration 105/1000 | Loss: 0.00002050
Iteration 106/1000 | Loss: 0.00002050
Iteration 107/1000 | Loss: 0.00002050
Iteration 108/1000 | Loss: 0.00002050
Iteration 109/1000 | Loss: 0.00002050
Iteration 110/1000 | Loss: 0.00002050
Iteration 111/1000 | Loss: 0.00002050
Iteration 112/1000 | Loss: 0.00002050
Iteration 113/1000 | Loss: 0.00002050
Iteration 114/1000 | Loss: 0.00002050
Iteration 115/1000 | Loss: 0.00002050
Iteration 116/1000 | Loss: 0.00002050
Iteration 117/1000 | Loss: 0.00002050
Iteration 118/1000 | Loss: 0.00002050
Iteration 119/1000 | Loss: 0.00002050
Iteration 120/1000 | Loss: 0.00002050
Iteration 121/1000 | Loss: 0.00002050
Iteration 122/1000 | Loss: 0.00002050
Iteration 123/1000 | Loss: 0.00002049
Iteration 124/1000 | Loss: 0.00002049
Iteration 125/1000 | Loss: 0.00002049
Iteration 126/1000 | Loss: 0.00002049
Iteration 127/1000 | Loss: 0.00002049
Iteration 128/1000 | Loss: 0.00002049
Iteration 129/1000 | Loss: 0.00002049
Iteration 130/1000 | Loss: 0.00002049
Iteration 131/1000 | Loss: 0.00002049
Iteration 132/1000 | Loss: 0.00002049
Iteration 133/1000 | Loss: 0.00002049
Iteration 134/1000 | Loss: 0.00002049
Iteration 135/1000 | Loss: 0.00002049
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.049486465693917e-05, 2.049486465693917e-05, 2.049486465693917e-05, 2.049486465693917e-05, 2.049486465693917e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.049486465693917e-05

Optimization complete. Final v2v error: 3.8155810832977295 mm

Highest mean error: 4.280195713043213 mm for frame 97

Lowest mean error: 3.292137384414673 mm for frame 18

Saving results

Total time: 112.37479972839355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429564
Iteration 2/25 | Loss: 0.00128793
Iteration 3/25 | Loss: 0.00117726
Iteration 4/25 | Loss: 0.00116294
Iteration 5/25 | Loss: 0.00115840
Iteration 6/25 | Loss: 0.00115714
Iteration 7/25 | Loss: 0.00115714
Iteration 8/25 | Loss: 0.00115714
Iteration 9/25 | Loss: 0.00115714
Iteration 10/25 | Loss: 0.00115714
Iteration 11/25 | Loss: 0.00115714
Iteration 12/25 | Loss: 0.00115714
Iteration 13/25 | Loss: 0.00115714
Iteration 14/25 | Loss: 0.00115714
Iteration 15/25 | Loss: 0.00115714
Iteration 16/25 | Loss: 0.00115714
Iteration 17/25 | Loss: 0.00115714
Iteration 18/25 | Loss: 0.00115714
Iteration 19/25 | Loss: 0.00115714
Iteration 20/25 | Loss: 0.00115714
Iteration 21/25 | Loss: 0.00115714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001157138030976057, 0.001157138030976057, 0.001157138030976057, 0.001157138030976057, 0.001157138030976057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001157138030976057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39497006
Iteration 2/25 | Loss: 0.00059576
Iteration 3/25 | Loss: 0.00059576
Iteration 4/25 | Loss: 0.00059576
Iteration 5/25 | Loss: 0.00059576
Iteration 6/25 | Loss: 0.00059576
Iteration 7/25 | Loss: 0.00059576
Iteration 8/25 | Loss: 0.00059576
Iteration 9/25 | Loss: 0.00059576
Iteration 10/25 | Loss: 0.00059575
Iteration 11/25 | Loss: 0.00059575
Iteration 12/25 | Loss: 0.00059575
Iteration 13/25 | Loss: 0.00059575
Iteration 14/25 | Loss: 0.00059575
Iteration 15/25 | Loss: 0.00059575
Iteration 16/25 | Loss: 0.00059575
Iteration 17/25 | Loss: 0.00059575
Iteration 18/25 | Loss: 0.00059575
Iteration 19/25 | Loss: 0.00059575
Iteration 20/25 | Loss: 0.00059575
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005957545363344252, 0.0005957545363344252, 0.0005957545363344252, 0.0005957545363344252, 0.0005957545363344252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005957545363344252

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059575
Iteration 2/1000 | Loss: 0.00003966
Iteration 3/1000 | Loss: 0.00002273
Iteration 4/1000 | Loss: 0.00001999
Iteration 5/1000 | Loss: 0.00001825
Iteration 6/1000 | Loss: 0.00001738
Iteration 7/1000 | Loss: 0.00001674
Iteration 8/1000 | Loss: 0.00001620
Iteration 9/1000 | Loss: 0.00001580
Iteration 10/1000 | Loss: 0.00001554
Iteration 11/1000 | Loss: 0.00001540
Iteration 12/1000 | Loss: 0.00001529
Iteration 13/1000 | Loss: 0.00001528
Iteration 14/1000 | Loss: 0.00001527
Iteration 15/1000 | Loss: 0.00001526
Iteration 16/1000 | Loss: 0.00001525
Iteration 17/1000 | Loss: 0.00001525
Iteration 18/1000 | Loss: 0.00001525
Iteration 19/1000 | Loss: 0.00001525
Iteration 20/1000 | Loss: 0.00001525
Iteration 21/1000 | Loss: 0.00001524
Iteration 22/1000 | Loss: 0.00001522
Iteration 23/1000 | Loss: 0.00001522
Iteration 24/1000 | Loss: 0.00001521
Iteration 25/1000 | Loss: 0.00001520
Iteration 26/1000 | Loss: 0.00001520
Iteration 27/1000 | Loss: 0.00001520
Iteration 28/1000 | Loss: 0.00001520
Iteration 29/1000 | Loss: 0.00001519
Iteration 30/1000 | Loss: 0.00001517
Iteration 31/1000 | Loss: 0.00001517
Iteration 32/1000 | Loss: 0.00001517
Iteration 33/1000 | Loss: 0.00001516
Iteration 34/1000 | Loss: 0.00001516
Iteration 35/1000 | Loss: 0.00001515
Iteration 36/1000 | Loss: 0.00001515
Iteration 37/1000 | Loss: 0.00001515
Iteration 38/1000 | Loss: 0.00001515
Iteration 39/1000 | Loss: 0.00001515
Iteration 40/1000 | Loss: 0.00001515
Iteration 41/1000 | Loss: 0.00001514
Iteration 42/1000 | Loss: 0.00001514
Iteration 43/1000 | Loss: 0.00001514
Iteration 44/1000 | Loss: 0.00001513
Iteration 45/1000 | Loss: 0.00001513
Iteration 46/1000 | Loss: 0.00001513
Iteration 47/1000 | Loss: 0.00001512
Iteration 48/1000 | Loss: 0.00001512
Iteration 49/1000 | Loss: 0.00001512
Iteration 50/1000 | Loss: 0.00001511
Iteration 51/1000 | Loss: 0.00001511
Iteration 52/1000 | Loss: 0.00001511
Iteration 53/1000 | Loss: 0.00001511
Iteration 54/1000 | Loss: 0.00001510
Iteration 55/1000 | Loss: 0.00001510
Iteration 56/1000 | Loss: 0.00001510
Iteration 57/1000 | Loss: 0.00001510
Iteration 58/1000 | Loss: 0.00001509
Iteration 59/1000 | Loss: 0.00001509
Iteration 60/1000 | Loss: 0.00001509
Iteration 61/1000 | Loss: 0.00001509
Iteration 62/1000 | Loss: 0.00001509
Iteration 63/1000 | Loss: 0.00001508
Iteration 64/1000 | Loss: 0.00001508
Iteration 65/1000 | Loss: 0.00001508
Iteration 66/1000 | Loss: 0.00001508
Iteration 67/1000 | Loss: 0.00001508
Iteration 68/1000 | Loss: 0.00001508
Iteration 69/1000 | Loss: 0.00001508
Iteration 70/1000 | Loss: 0.00001508
Iteration 71/1000 | Loss: 0.00001508
Iteration 72/1000 | Loss: 0.00001508
Iteration 73/1000 | Loss: 0.00001508
Iteration 74/1000 | Loss: 0.00001507
Iteration 75/1000 | Loss: 0.00001507
Iteration 76/1000 | Loss: 0.00001507
Iteration 77/1000 | Loss: 0.00001507
Iteration 78/1000 | Loss: 0.00001507
Iteration 79/1000 | Loss: 0.00001507
Iteration 80/1000 | Loss: 0.00001507
Iteration 81/1000 | Loss: 0.00001506
Iteration 82/1000 | Loss: 0.00001506
Iteration 83/1000 | Loss: 0.00001506
Iteration 84/1000 | Loss: 0.00001506
Iteration 85/1000 | Loss: 0.00001506
Iteration 86/1000 | Loss: 0.00001506
Iteration 87/1000 | Loss: 0.00001506
Iteration 88/1000 | Loss: 0.00001506
Iteration 89/1000 | Loss: 0.00001506
Iteration 90/1000 | Loss: 0.00001506
Iteration 91/1000 | Loss: 0.00001506
Iteration 92/1000 | Loss: 0.00001506
Iteration 93/1000 | Loss: 0.00001506
Iteration 94/1000 | Loss: 0.00001506
Iteration 95/1000 | Loss: 0.00001506
Iteration 96/1000 | Loss: 0.00001506
Iteration 97/1000 | Loss: 0.00001505
Iteration 98/1000 | Loss: 0.00001505
Iteration 99/1000 | Loss: 0.00001505
Iteration 100/1000 | Loss: 0.00001505
Iteration 101/1000 | Loss: 0.00001505
Iteration 102/1000 | Loss: 0.00001505
Iteration 103/1000 | Loss: 0.00001505
Iteration 104/1000 | Loss: 0.00001505
Iteration 105/1000 | Loss: 0.00001505
Iteration 106/1000 | Loss: 0.00001505
Iteration 107/1000 | Loss: 0.00001505
Iteration 108/1000 | Loss: 0.00001505
Iteration 109/1000 | Loss: 0.00001505
Iteration 110/1000 | Loss: 0.00001505
Iteration 111/1000 | Loss: 0.00001505
Iteration 112/1000 | Loss: 0.00001505
Iteration 113/1000 | Loss: 0.00001504
Iteration 114/1000 | Loss: 0.00001504
Iteration 115/1000 | Loss: 0.00001504
Iteration 116/1000 | Loss: 0.00001504
Iteration 117/1000 | Loss: 0.00001504
Iteration 118/1000 | Loss: 0.00001504
Iteration 119/1000 | Loss: 0.00001504
Iteration 120/1000 | Loss: 0.00001504
Iteration 121/1000 | Loss: 0.00001504
Iteration 122/1000 | Loss: 0.00001504
Iteration 123/1000 | Loss: 0.00001504
Iteration 124/1000 | Loss: 0.00001504
Iteration 125/1000 | Loss: 0.00001503
Iteration 126/1000 | Loss: 0.00001503
Iteration 127/1000 | Loss: 0.00001503
Iteration 128/1000 | Loss: 0.00001503
Iteration 129/1000 | Loss: 0.00001503
Iteration 130/1000 | Loss: 0.00001503
Iteration 131/1000 | Loss: 0.00001503
Iteration 132/1000 | Loss: 0.00001503
Iteration 133/1000 | Loss: 0.00001503
Iteration 134/1000 | Loss: 0.00001502
Iteration 135/1000 | Loss: 0.00001502
Iteration 136/1000 | Loss: 0.00001502
Iteration 137/1000 | Loss: 0.00001502
Iteration 138/1000 | Loss: 0.00001502
Iteration 139/1000 | Loss: 0.00001501
Iteration 140/1000 | Loss: 0.00001501
Iteration 141/1000 | Loss: 0.00001501
Iteration 142/1000 | Loss: 0.00001501
Iteration 143/1000 | Loss: 0.00001501
Iteration 144/1000 | Loss: 0.00001500
Iteration 145/1000 | Loss: 0.00001500
Iteration 146/1000 | Loss: 0.00001500
Iteration 147/1000 | Loss: 0.00001500
Iteration 148/1000 | Loss: 0.00001500
Iteration 149/1000 | Loss: 0.00001499
Iteration 150/1000 | Loss: 0.00001499
Iteration 151/1000 | Loss: 0.00001499
Iteration 152/1000 | Loss: 0.00001499
Iteration 153/1000 | Loss: 0.00001499
Iteration 154/1000 | Loss: 0.00001499
Iteration 155/1000 | Loss: 0.00001499
Iteration 156/1000 | Loss: 0.00001499
Iteration 157/1000 | Loss: 0.00001499
Iteration 158/1000 | Loss: 0.00001498
Iteration 159/1000 | Loss: 0.00001498
Iteration 160/1000 | Loss: 0.00001498
Iteration 161/1000 | Loss: 0.00001498
Iteration 162/1000 | Loss: 0.00001497
Iteration 163/1000 | Loss: 0.00001497
Iteration 164/1000 | Loss: 0.00001497
Iteration 165/1000 | Loss: 0.00001497
Iteration 166/1000 | Loss: 0.00001497
Iteration 167/1000 | Loss: 0.00001497
Iteration 168/1000 | Loss: 0.00001497
Iteration 169/1000 | Loss: 0.00001497
Iteration 170/1000 | Loss: 0.00001497
Iteration 171/1000 | Loss: 0.00001497
Iteration 172/1000 | Loss: 0.00001497
Iteration 173/1000 | Loss: 0.00001497
Iteration 174/1000 | Loss: 0.00001497
Iteration 175/1000 | Loss: 0.00001497
Iteration 176/1000 | Loss: 0.00001497
Iteration 177/1000 | Loss: 0.00001497
Iteration 178/1000 | Loss: 0.00001497
Iteration 179/1000 | Loss: 0.00001497
Iteration 180/1000 | Loss: 0.00001497
Iteration 181/1000 | Loss: 0.00001497
Iteration 182/1000 | Loss: 0.00001497
Iteration 183/1000 | Loss: 0.00001497
Iteration 184/1000 | Loss: 0.00001497
Iteration 185/1000 | Loss: 0.00001497
Iteration 186/1000 | Loss: 0.00001497
Iteration 187/1000 | Loss: 0.00001497
Iteration 188/1000 | Loss: 0.00001497
Iteration 189/1000 | Loss: 0.00001497
Iteration 190/1000 | Loss: 0.00001497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.4971070413594134e-05, 1.4971070413594134e-05, 1.4971070413594134e-05, 1.4971070413594134e-05, 1.4971070413594134e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4971070413594134e-05

Optimization complete. Final v2v error: 3.27607798576355 mm

Highest mean error: 3.498958110809326 mm for frame 12

Lowest mean error: 3.0714967250823975 mm for frame 30

Saving results

Total time: 45.82581377029419
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892023
Iteration 2/25 | Loss: 0.00127143
Iteration 3/25 | Loss: 0.00115446
Iteration 4/25 | Loss: 0.00114482
Iteration 5/25 | Loss: 0.00114192
Iteration 6/25 | Loss: 0.00114175
Iteration 7/25 | Loss: 0.00114175
Iteration 8/25 | Loss: 0.00114175
Iteration 9/25 | Loss: 0.00114175
Iteration 10/25 | Loss: 0.00114175
Iteration 11/25 | Loss: 0.00114175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011417533969506621, 0.0011417533969506621, 0.0011417533969506621, 0.0011417533969506621, 0.0011417533969506621]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011417533969506621

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32436860
Iteration 2/25 | Loss: 0.00055128
Iteration 3/25 | Loss: 0.00055128
Iteration 4/25 | Loss: 0.00055128
Iteration 5/25 | Loss: 0.00055128
Iteration 6/25 | Loss: 0.00055128
Iteration 7/25 | Loss: 0.00055128
Iteration 8/25 | Loss: 0.00055128
Iteration 9/25 | Loss: 0.00055128
Iteration 10/25 | Loss: 0.00055128
Iteration 11/25 | Loss: 0.00055128
Iteration 12/25 | Loss: 0.00055128
Iteration 13/25 | Loss: 0.00055128
Iteration 14/25 | Loss: 0.00055128
Iteration 15/25 | Loss: 0.00055128
Iteration 16/25 | Loss: 0.00055128
Iteration 17/25 | Loss: 0.00055128
Iteration 18/25 | Loss: 0.00055128
Iteration 19/25 | Loss: 0.00055128
Iteration 20/25 | Loss: 0.00055128
Iteration 21/25 | Loss: 0.00055128
Iteration 22/25 | Loss: 0.00055128
Iteration 23/25 | Loss: 0.00055128
Iteration 24/25 | Loss: 0.00055128
Iteration 25/25 | Loss: 0.00055128

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055128
Iteration 2/1000 | Loss: 0.00004975
Iteration 3/1000 | Loss: 0.00002299
Iteration 4/1000 | Loss: 0.00001914
Iteration 5/1000 | Loss: 0.00001822
Iteration 6/1000 | Loss: 0.00001727
Iteration 7/1000 | Loss: 0.00001689
Iteration 8/1000 | Loss: 0.00001651
Iteration 9/1000 | Loss: 0.00001628
Iteration 10/1000 | Loss: 0.00001601
Iteration 11/1000 | Loss: 0.00001581
Iteration 12/1000 | Loss: 0.00001577
Iteration 13/1000 | Loss: 0.00001573
Iteration 14/1000 | Loss: 0.00001572
Iteration 15/1000 | Loss: 0.00001571
Iteration 16/1000 | Loss: 0.00001570
Iteration 17/1000 | Loss: 0.00001569
Iteration 18/1000 | Loss: 0.00001568
Iteration 19/1000 | Loss: 0.00001568
Iteration 20/1000 | Loss: 0.00001567
Iteration 21/1000 | Loss: 0.00001567
Iteration 22/1000 | Loss: 0.00001566
Iteration 23/1000 | Loss: 0.00001566
Iteration 24/1000 | Loss: 0.00001565
Iteration 25/1000 | Loss: 0.00001565
Iteration 26/1000 | Loss: 0.00001564
Iteration 27/1000 | Loss: 0.00001564
Iteration 28/1000 | Loss: 0.00001564
Iteration 29/1000 | Loss: 0.00001564
Iteration 30/1000 | Loss: 0.00001563
Iteration 31/1000 | Loss: 0.00001563
Iteration 32/1000 | Loss: 0.00001563
Iteration 33/1000 | Loss: 0.00001563
Iteration 34/1000 | Loss: 0.00001563
Iteration 35/1000 | Loss: 0.00001563
Iteration 36/1000 | Loss: 0.00001561
Iteration 37/1000 | Loss: 0.00001560
Iteration 38/1000 | Loss: 0.00001560
Iteration 39/1000 | Loss: 0.00001560
Iteration 40/1000 | Loss: 0.00001559
Iteration 41/1000 | Loss: 0.00001559
Iteration 42/1000 | Loss: 0.00001559
Iteration 43/1000 | Loss: 0.00001559
Iteration 44/1000 | Loss: 0.00001559
Iteration 45/1000 | Loss: 0.00001558
Iteration 46/1000 | Loss: 0.00001558
Iteration 47/1000 | Loss: 0.00001558
Iteration 48/1000 | Loss: 0.00001558
Iteration 49/1000 | Loss: 0.00001558
Iteration 50/1000 | Loss: 0.00001557
Iteration 51/1000 | Loss: 0.00001557
Iteration 52/1000 | Loss: 0.00001557
Iteration 53/1000 | Loss: 0.00001557
Iteration 54/1000 | Loss: 0.00001557
Iteration 55/1000 | Loss: 0.00001557
Iteration 56/1000 | Loss: 0.00001557
Iteration 57/1000 | Loss: 0.00001557
Iteration 58/1000 | Loss: 0.00001557
Iteration 59/1000 | Loss: 0.00001557
Iteration 60/1000 | Loss: 0.00001557
Iteration 61/1000 | Loss: 0.00001557
Iteration 62/1000 | Loss: 0.00001557
Iteration 63/1000 | Loss: 0.00001557
Iteration 64/1000 | Loss: 0.00001556
Iteration 65/1000 | Loss: 0.00001556
Iteration 66/1000 | Loss: 0.00001556
Iteration 67/1000 | Loss: 0.00001556
Iteration 68/1000 | Loss: 0.00001556
Iteration 69/1000 | Loss: 0.00001556
Iteration 70/1000 | Loss: 0.00001556
Iteration 71/1000 | Loss: 0.00001556
Iteration 72/1000 | Loss: 0.00001556
Iteration 73/1000 | Loss: 0.00001555
Iteration 74/1000 | Loss: 0.00001555
Iteration 75/1000 | Loss: 0.00001555
Iteration 76/1000 | Loss: 0.00001555
Iteration 77/1000 | Loss: 0.00001554
Iteration 78/1000 | Loss: 0.00001554
Iteration 79/1000 | Loss: 0.00001554
Iteration 80/1000 | Loss: 0.00001554
Iteration 81/1000 | Loss: 0.00001554
Iteration 82/1000 | Loss: 0.00001554
Iteration 83/1000 | Loss: 0.00001553
Iteration 84/1000 | Loss: 0.00001553
Iteration 85/1000 | Loss: 0.00001553
Iteration 86/1000 | Loss: 0.00001553
Iteration 87/1000 | Loss: 0.00001552
Iteration 88/1000 | Loss: 0.00001552
Iteration 89/1000 | Loss: 0.00001552
Iteration 90/1000 | Loss: 0.00001552
Iteration 91/1000 | Loss: 0.00001552
Iteration 92/1000 | Loss: 0.00001552
Iteration 93/1000 | Loss: 0.00001552
Iteration 94/1000 | Loss: 0.00001552
Iteration 95/1000 | Loss: 0.00001552
Iteration 96/1000 | Loss: 0.00001552
Iteration 97/1000 | Loss: 0.00001552
Iteration 98/1000 | Loss: 0.00001552
Iteration 99/1000 | Loss: 0.00001552
Iteration 100/1000 | Loss: 0.00001552
Iteration 101/1000 | Loss: 0.00001552
Iteration 102/1000 | Loss: 0.00001552
Iteration 103/1000 | Loss: 0.00001552
Iteration 104/1000 | Loss: 0.00001552
Iteration 105/1000 | Loss: 0.00001551
Iteration 106/1000 | Loss: 0.00001551
Iteration 107/1000 | Loss: 0.00001551
Iteration 108/1000 | Loss: 0.00001551
Iteration 109/1000 | Loss: 0.00001551
Iteration 110/1000 | Loss: 0.00001551
Iteration 111/1000 | Loss: 0.00001551
Iteration 112/1000 | Loss: 0.00001551
Iteration 113/1000 | Loss: 0.00001551
Iteration 114/1000 | Loss: 0.00001550
Iteration 115/1000 | Loss: 0.00001550
Iteration 116/1000 | Loss: 0.00001550
Iteration 117/1000 | Loss: 0.00001550
Iteration 118/1000 | Loss: 0.00001550
Iteration 119/1000 | Loss: 0.00001550
Iteration 120/1000 | Loss: 0.00001550
Iteration 121/1000 | Loss: 0.00001550
Iteration 122/1000 | Loss: 0.00001550
Iteration 123/1000 | Loss: 0.00001550
Iteration 124/1000 | Loss: 0.00001550
Iteration 125/1000 | Loss: 0.00001550
Iteration 126/1000 | Loss: 0.00001550
Iteration 127/1000 | Loss: 0.00001550
Iteration 128/1000 | Loss: 0.00001550
Iteration 129/1000 | Loss: 0.00001550
Iteration 130/1000 | Loss: 0.00001550
Iteration 131/1000 | Loss: 0.00001549
Iteration 132/1000 | Loss: 0.00001549
Iteration 133/1000 | Loss: 0.00001549
Iteration 134/1000 | Loss: 0.00001549
Iteration 135/1000 | Loss: 0.00001549
Iteration 136/1000 | Loss: 0.00001549
Iteration 137/1000 | Loss: 0.00001549
Iteration 138/1000 | Loss: 0.00001549
Iteration 139/1000 | Loss: 0.00001549
Iteration 140/1000 | Loss: 0.00001549
Iteration 141/1000 | Loss: 0.00001549
Iteration 142/1000 | Loss: 0.00001549
Iteration 143/1000 | Loss: 0.00001549
Iteration 144/1000 | Loss: 0.00001549
Iteration 145/1000 | Loss: 0.00001549
Iteration 146/1000 | Loss: 0.00001548
Iteration 147/1000 | Loss: 0.00001548
Iteration 148/1000 | Loss: 0.00001548
Iteration 149/1000 | Loss: 0.00001548
Iteration 150/1000 | Loss: 0.00001548
Iteration 151/1000 | Loss: 0.00001548
Iteration 152/1000 | Loss: 0.00001548
Iteration 153/1000 | Loss: 0.00001548
Iteration 154/1000 | Loss: 0.00001548
Iteration 155/1000 | Loss: 0.00001548
Iteration 156/1000 | Loss: 0.00001548
Iteration 157/1000 | Loss: 0.00001548
Iteration 158/1000 | Loss: 0.00001548
Iteration 159/1000 | Loss: 0.00001548
Iteration 160/1000 | Loss: 0.00001548
Iteration 161/1000 | Loss: 0.00001548
Iteration 162/1000 | Loss: 0.00001548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.548004729556851e-05, 1.548004729556851e-05, 1.548004729556851e-05, 1.548004729556851e-05, 1.548004729556851e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.548004729556851e-05

Optimization complete. Final v2v error: 3.347407817840576 mm

Highest mean error: 3.5713908672332764 mm for frame 103

Lowest mean error: 3.0165750980377197 mm for frame 0

Saving results

Total time: 33.709656953811646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455893
Iteration 2/25 | Loss: 0.00131115
Iteration 3/25 | Loss: 0.00118882
Iteration 4/25 | Loss: 0.00117515
Iteration 5/25 | Loss: 0.00117131
Iteration 6/25 | Loss: 0.00117076
Iteration 7/25 | Loss: 0.00117076
Iteration 8/25 | Loss: 0.00117076
Iteration 9/25 | Loss: 0.00117076
Iteration 10/25 | Loss: 0.00117076
Iteration 11/25 | Loss: 0.00117076
Iteration 12/25 | Loss: 0.00117076
Iteration 13/25 | Loss: 0.00117076
Iteration 14/25 | Loss: 0.00117076
Iteration 15/25 | Loss: 0.00117076
Iteration 16/25 | Loss: 0.00117076
Iteration 17/25 | Loss: 0.00117076
Iteration 18/25 | Loss: 0.00117076
Iteration 19/25 | Loss: 0.00117076
Iteration 20/25 | Loss: 0.00117076
Iteration 21/25 | Loss: 0.00117076
Iteration 22/25 | Loss: 0.00117076
Iteration 23/25 | Loss: 0.00117076
Iteration 24/25 | Loss: 0.00117076
Iteration 25/25 | Loss: 0.00117076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.77123308
Iteration 2/25 | Loss: 0.00058667
Iteration 3/25 | Loss: 0.00058665
Iteration 4/25 | Loss: 0.00058665
Iteration 5/25 | Loss: 0.00058665
Iteration 6/25 | Loss: 0.00058665
Iteration 7/25 | Loss: 0.00058665
Iteration 8/25 | Loss: 0.00058665
Iteration 9/25 | Loss: 0.00058665
Iteration 10/25 | Loss: 0.00058665
Iteration 11/25 | Loss: 0.00058665
Iteration 12/25 | Loss: 0.00058665
Iteration 13/25 | Loss: 0.00058665
Iteration 14/25 | Loss: 0.00058665
Iteration 15/25 | Loss: 0.00058665
Iteration 16/25 | Loss: 0.00058665
Iteration 17/25 | Loss: 0.00058665
Iteration 18/25 | Loss: 0.00058665
Iteration 19/25 | Loss: 0.00058665
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000586648762691766, 0.000586648762691766, 0.000586648762691766, 0.000586648762691766, 0.000586648762691766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000586648762691766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058665
Iteration 2/1000 | Loss: 0.00006261
Iteration 3/1000 | Loss: 0.00002800
Iteration 4/1000 | Loss: 0.00002480
Iteration 5/1000 | Loss: 0.00002342
Iteration 6/1000 | Loss: 0.00002253
Iteration 7/1000 | Loss: 0.00002194
Iteration 8/1000 | Loss: 0.00002137
Iteration 9/1000 | Loss: 0.00002096
Iteration 10/1000 | Loss: 0.00002065
Iteration 11/1000 | Loss: 0.00002044
Iteration 12/1000 | Loss: 0.00002034
Iteration 13/1000 | Loss: 0.00002032
Iteration 14/1000 | Loss: 0.00002029
Iteration 15/1000 | Loss: 0.00002029
Iteration 16/1000 | Loss: 0.00002025
Iteration 17/1000 | Loss: 0.00002025
Iteration 18/1000 | Loss: 0.00002023
Iteration 19/1000 | Loss: 0.00002022
Iteration 20/1000 | Loss: 0.00002022
Iteration 21/1000 | Loss: 0.00002022
Iteration 22/1000 | Loss: 0.00002021
Iteration 23/1000 | Loss: 0.00002021
Iteration 24/1000 | Loss: 0.00002021
Iteration 25/1000 | Loss: 0.00002021
Iteration 26/1000 | Loss: 0.00002020
Iteration 27/1000 | Loss: 0.00002018
Iteration 28/1000 | Loss: 0.00002018
Iteration 29/1000 | Loss: 0.00002018
Iteration 30/1000 | Loss: 0.00002018
Iteration 31/1000 | Loss: 0.00002018
Iteration 32/1000 | Loss: 0.00002018
Iteration 33/1000 | Loss: 0.00002018
Iteration 34/1000 | Loss: 0.00002018
Iteration 35/1000 | Loss: 0.00002018
Iteration 36/1000 | Loss: 0.00002017
Iteration 37/1000 | Loss: 0.00002017
Iteration 38/1000 | Loss: 0.00002017
Iteration 39/1000 | Loss: 0.00002017
Iteration 40/1000 | Loss: 0.00002017
Iteration 41/1000 | Loss: 0.00002016
Iteration 42/1000 | Loss: 0.00002016
Iteration 43/1000 | Loss: 0.00002016
Iteration 44/1000 | Loss: 0.00002016
Iteration 45/1000 | Loss: 0.00002015
Iteration 46/1000 | Loss: 0.00002014
Iteration 47/1000 | Loss: 0.00002014
Iteration 48/1000 | Loss: 0.00002014
Iteration 49/1000 | Loss: 0.00002014
Iteration 50/1000 | Loss: 0.00002013
Iteration 51/1000 | Loss: 0.00002013
Iteration 52/1000 | Loss: 0.00002013
Iteration 53/1000 | Loss: 0.00002012
Iteration 54/1000 | Loss: 0.00002012
Iteration 55/1000 | Loss: 0.00002012
Iteration 56/1000 | Loss: 0.00002011
Iteration 57/1000 | Loss: 0.00002011
Iteration 58/1000 | Loss: 0.00002011
Iteration 59/1000 | Loss: 0.00002011
Iteration 60/1000 | Loss: 0.00002011
Iteration 61/1000 | Loss: 0.00002010
Iteration 62/1000 | Loss: 0.00002010
Iteration 63/1000 | Loss: 0.00002010
Iteration 64/1000 | Loss: 0.00002010
Iteration 65/1000 | Loss: 0.00002010
Iteration 66/1000 | Loss: 0.00002010
Iteration 67/1000 | Loss: 0.00002010
Iteration 68/1000 | Loss: 0.00002010
Iteration 69/1000 | Loss: 0.00002010
Iteration 70/1000 | Loss: 0.00002010
Iteration 71/1000 | Loss: 0.00002009
Iteration 72/1000 | Loss: 0.00002009
Iteration 73/1000 | Loss: 0.00002009
Iteration 74/1000 | Loss: 0.00002009
Iteration 75/1000 | Loss: 0.00002009
Iteration 76/1000 | Loss: 0.00002009
Iteration 77/1000 | Loss: 0.00002008
Iteration 78/1000 | Loss: 0.00002008
Iteration 79/1000 | Loss: 0.00002008
Iteration 80/1000 | Loss: 0.00002008
Iteration 81/1000 | Loss: 0.00002008
Iteration 82/1000 | Loss: 0.00002008
Iteration 83/1000 | Loss: 0.00002007
Iteration 84/1000 | Loss: 0.00002007
Iteration 85/1000 | Loss: 0.00002007
Iteration 86/1000 | Loss: 0.00002007
Iteration 87/1000 | Loss: 0.00002007
Iteration 88/1000 | Loss: 0.00002007
Iteration 89/1000 | Loss: 0.00002007
Iteration 90/1000 | Loss: 0.00002007
Iteration 91/1000 | Loss: 0.00002007
Iteration 92/1000 | Loss: 0.00002007
Iteration 93/1000 | Loss: 0.00002007
Iteration 94/1000 | Loss: 0.00002007
Iteration 95/1000 | Loss: 0.00002006
Iteration 96/1000 | Loss: 0.00002006
Iteration 97/1000 | Loss: 0.00002006
Iteration 98/1000 | Loss: 0.00002006
Iteration 99/1000 | Loss: 0.00002006
Iteration 100/1000 | Loss: 0.00002006
Iteration 101/1000 | Loss: 0.00002005
Iteration 102/1000 | Loss: 0.00002005
Iteration 103/1000 | Loss: 0.00002005
Iteration 104/1000 | Loss: 0.00002005
Iteration 105/1000 | Loss: 0.00002005
Iteration 106/1000 | Loss: 0.00002005
Iteration 107/1000 | Loss: 0.00002005
Iteration 108/1000 | Loss: 0.00002005
Iteration 109/1000 | Loss: 0.00002005
Iteration 110/1000 | Loss: 0.00002005
Iteration 111/1000 | Loss: 0.00002005
Iteration 112/1000 | Loss: 0.00002004
Iteration 113/1000 | Loss: 0.00002004
Iteration 114/1000 | Loss: 0.00002004
Iteration 115/1000 | Loss: 0.00002004
Iteration 116/1000 | Loss: 0.00002004
Iteration 117/1000 | Loss: 0.00002004
Iteration 118/1000 | Loss: 0.00002004
Iteration 119/1000 | Loss: 0.00002004
Iteration 120/1000 | Loss: 0.00002004
Iteration 121/1000 | Loss: 0.00002004
Iteration 122/1000 | Loss: 0.00002003
Iteration 123/1000 | Loss: 0.00002003
Iteration 124/1000 | Loss: 0.00002003
Iteration 125/1000 | Loss: 0.00002003
Iteration 126/1000 | Loss: 0.00002003
Iteration 127/1000 | Loss: 0.00002003
Iteration 128/1000 | Loss: 0.00002003
Iteration 129/1000 | Loss: 0.00002003
Iteration 130/1000 | Loss: 0.00002003
Iteration 131/1000 | Loss: 0.00002002
Iteration 132/1000 | Loss: 0.00002002
Iteration 133/1000 | Loss: 0.00002002
Iteration 134/1000 | Loss: 0.00002002
Iteration 135/1000 | Loss: 0.00002002
Iteration 136/1000 | Loss: 0.00002002
Iteration 137/1000 | Loss: 0.00002002
Iteration 138/1000 | Loss: 0.00002002
Iteration 139/1000 | Loss: 0.00002002
Iteration 140/1000 | Loss: 0.00002002
Iteration 141/1000 | Loss: 0.00002002
Iteration 142/1000 | Loss: 0.00002002
Iteration 143/1000 | Loss: 0.00002002
Iteration 144/1000 | Loss: 0.00002002
Iteration 145/1000 | Loss: 0.00002002
Iteration 146/1000 | Loss: 0.00002002
Iteration 147/1000 | Loss: 0.00002001
Iteration 148/1000 | Loss: 0.00002001
Iteration 149/1000 | Loss: 0.00002001
Iteration 150/1000 | Loss: 0.00002001
Iteration 151/1000 | Loss: 0.00002001
Iteration 152/1000 | Loss: 0.00002001
Iteration 153/1000 | Loss: 0.00002001
Iteration 154/1000 | Loss: 0.00002000
Iteration 155/1000 | Loss: 0.00002000
Iteration 156/1000 | Loss: 0.00002000
Iteration 157/1000 | Loss: 0.00002000
Iteration 158/1000 | Loss: 0.00002000
Iteration 159/1000 | Loss: 0.00002000
Iteration 160/1000 | Loss: 0.00002000
Iteration 161/1000 | Loss: 0.00002000
Iteration 162/1000 | Loss: 0.00002000
Iteration 163/1000 | Loss: 0.00001999
Iteration 164/1000 | Loss: 0.00001999
Iteration 165/1000 | Loss: 0.00001999
Iteration 166/1000 | Loss: 0.00001999
Iteration 167/1000 | Loss: 0.00001999
Iteration 168/1000 | Loss: 0.00001999
Iteration 169/1000 | Loss: 0.00001999
Iteration 170/1000 | Loss: 0.00001999
Iteration 171/1000 | Loss: 0.00001999
Iteration 172/1000 | Loss: 0.00001999
Iteration 173/1000 | Loss: 0.00001999
Iteration 174/1000 | Loss: 0.00001998
Iteration 175/1000 | Loss: 0.00001998
Iteration 176/1000 | Loss: 0.00001998
Iteration 177/1000 | Loss: 0.00001998
Iteration 178/1000 | Loss: 0.00001998
Iteration 179/1000 | Loss: 0.00001998
Iteration 180/1000 | Loss: 0.00001998
Iteration 181/1000 | Loss: 0.00001998
Iteration 182/1000 | Loss: 0.00001998
Iteration 183/1000 | Loss: 0.00001998
Iteration 184/1000 | Loss: 0.00001998
Iteration 185/1000 | Loss: 0.00001998
Iteration 186/1000 | Loss: 0.00001998
Iteration 187/1000 | Loss: 0.00001998
Iteration 188/1000 | Loss: 0.00001998
Iteration 189/1000 | Loss: 0.00001998
Iteration 190/1000 | Loss: 0.00001998
Iteration 191/1000 | Loss: 0.00001998
Iteration 192/1000 | Loss: 0.00001997
Iteration 193/1000 | Loss: 0.00001997
Iteration 194/1000 | Loss: 0.00001997
Iteration 195/1000 | Loss: 0.00001997
Iteration 196/1000 | Loss: 0.00001997
Iteration 197/1000 | Loss: 0.00001997
Iteration 198/1000 | Loss: 0.00001997
Iteration 199/1000 | Loss: 0.00001997
Iteration 200/1000 | Loss: 0.00001997
Iteration 201/1000 | Loss: 0.00001997
Iteration 202/1000 | Loss: 0.00001997
Iteration 203/1000 | Loss: 0.00001997
Iteration 204/1000 | Loss: 0.00001997
Iteration 205/1000 | Loss: 0.00001997
Iteration 206/1000 | Loss: 0.00001997
Iteration 207/1000 | Loss: 0.00001997
Iteration 208/1000 | Loss: 0.00001997
Iteration 209/1000 | Loss: 0.00001997
Iteration 210/1000 | Loss: 0.00001997
Iteration 211/1000 | Loss: 0.00001997
Iteration 212/1000 | Loss: 0.00001997
Iteration 213/1000 | Loss: 0.00001996
Iteration 214/1000 | Loss: 0.00001996
Iteration 215/1000 | Loss: 0.00001996
Iteration 216/1000 | Loss: 0.00001996
Iteration 217/1000 | Loss: 0.00001996
Iteration 218/1000 | Loss: 0.00001996
Iteration 219/1000 | Loss: 0.00001996
Iteration 220/1000 | Loss: 0.00001996
Iteration 221/1000 | Loss: 0.00001996
Iteration 222/1000 | Loss: 0.00001996
Iteration 223/1000 | Loss: 0.00001996
Iteration 224/1000 | Loss: 0.00001996
Iteration 225/1000 | Loss: 0.00001996
Iteration 226/1000 | Loss: 0.00001996
Iteration 227/1000 | Loss: 0.00001996
Iteration 228/1000 | Loss: 0.00001996
Iteration 229/1000 | Loss: 0.00001996
Iteration 230/1000 | Loss: 0.00001996
Iteration 231/1000 | Loss: 0.00001996
Iteration 232/1000 | Loss: 0.00001996
Iteration 233/1000 | Loss: 0.00001996
Iteration 234/1000 | Loss: 0.00001996
Iteration 235/1000 | Loss: 0.00001996
Iteration 236/1000 | Loss: 0.00001996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.9962028090958484e-05, 1.9962028090958484e-05, 1.9962028090958484e-05, 1.9962028090958484e-05, 1.9962028090958484e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9962028090958484e-05

Optimization complete. Final v2v error: 3.786632776260376 mm

Highest mean error: 4.155359268188477 mm for frame 116

Lowest mean error: 3.3124659061431885 mm for frame 149

Saving results

Total time: 40.27910351753235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00558380
Iteration 2/25 | Loss: 0.00168685
Iteration 3/25 | Loss: 0.00132993
Iteration 4/25 | Loss: 0.00130360
Iteration 5/25 | Loss: 0.00129646
Iteration 6/25 | Loss: 0.00129394
Iteration 7/25 | Loss: 0.00129358
Iteration 8/25 | Loss: 0.00129358
Iteration 9/25 | Loss: 0.00129358
Iteration 10/25 | Loss: 0.00129358
Iteration 11/25 | Loss: 0.00129358
Iteration 12/25 | Loss: 0.00129358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012935821432620287, 0.0012935821432620287, 0.0012935821432620287, 0.0012935821432620287, 0.0012935821432620287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012935821432620287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89573258
Iteration 2/25 | Loss: 0.00073578
Iteration 3/25 | Loss: 0.00073578
Iteration 4/25 | Loss: 0.00073578
Iteration 5/25 | Loss: 0.00073578
Iteration 6/25 | Loss: 0.00073578
Iteration 7/25 | Loss: 0.00073578
Iteration 8/25 | Loss: 0.00073578
Iteration 9/25 | Loss: 0.00073578
Iteration 10/25 | Loss: 0.00073578
Iteration 11/25 | Loss: 0.00073578
Iteration 12/25 | Loss: 0.00073578
Iteration 13/25 | Loss: 0.00073578
Iteration 14/25 | Loss: 0.00073578
Iteration 15/25 | Loss: 0.00073578
Iteration 16/25 | Loss: 0.00073578
Iteration 17/25 | Loss: 0.00073578
Iteration 18/25 | Loss: 0.00073578
Iteration 19/25 | Loss: 0.00073578
Iteration 20/25 | Loss: 0.00073578
Iteration 21/25 | Loss: 0.00073578
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007357752765528858, 0.0007357752765528858, 0.0007357752765528858, 0.0007357752765528858, 0.0007357752765528858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007357752765528858

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073578
Iteration 2/1000 | Loss: 0.00007419
Iteration 3/1000 | Loss: 0.00005080
Iteration 4/1000 | Loss: 0.00004521
Iteration 5/1000 | Loss: 0.00004205
Iteration 6/1000 | Loss: 0.00004047
Iteration 7/1000 | Loss: 0.00003886
Iteration 8/1000 | Loss: 0.00003751
Iteration 9/1000 | Loss: 0.00003636
Iteration 10/1000 | Loss: 0.00003546
Iteration 11/1000 | Loss: 0.00003489
Iteration 12/1000 | Loss: 0.00003434
Iteration 13/1000 | Loss: 0.00003394
Iteration 14/1000 | Loss: 0.00003366
Iteration 15/1000 | Loss: 0.00003332
Iteration 16/1000 | Loss: 0.00003309
Iteration 17/1000 | Loss: 0.00003292
Iteration 18/1000 | Loss: 0.00003272
Iteration 19/1000 | Loss: 0.00003256
Iteration 20/1000 | Loss: 0.00003240
Iteration 21/1000 | Loss: 0.00003235
Iteration 22/1000 | Loss: 0.00003222
Iteration 23/1000 | Loss: 0.00003211
Iteration 24/1000 | Loss: 0.00003210
Iteration 25/1000 | Loss: 0.00003210
Iteration 26/1000 | Loss: 0.00003209
Iteration 27/1000 | Loss: 0.00003209
Iteration 28/1000 | Loss: 0.00003208
Iteration 29/1000 | Loss: 0.00003205
Iteration 30/1000 | Loss: 0.00003202
Iteration 31/1000 | Loss: 0.00003202
Iteration 32/1000 | Loss: 0.00003202
Iteration 33/1000 | Loss: 0.00003202
Iteration 34/1000 | Loss: 0.00003202
Iteration 35/1000 | Loss: 0.00003201
Iteration 36/1000 | Loss: 0.00003201
Iteration 37/1000 | Loss: 0.00003201
Iteration 38/1000 | Loss: 0.00003201
Iteration 39/1000 | Loss: 0.00003200
Iteration 40/1000 | Loss: 0.00003200
Iteration 41/1000 | Loss: 0.00003200
Iteration 42/1000 | Loss: 0.00003200
Iteration 43/1000 | Loss: 0.00003200
Iteration 44/1000 | Loss: 0.00003198
Iteration 45/1000 | Loss: 0.00003198
Iteration 46/1000 | Loss: 0.00003197
Iteration 47/1000 | Loss: 0.00003194
Iteration 48/1000 | Loss: 0.00003190
Iteration 49/1000 | Loss: 0.00003190
Iteration 50/1000 | Loss: 0.00003190
Iteration 51/1000 | Loss: 0.00003189
Iteration 52/1000 | Loss: 0.00003187
Iteration 53/1000 | Loss: 0.00003185
Iteration 54/1000 | Loss: 0.00003185
Iteration 55/1000 | Loss: 0.00003184
Iteration 56/1000 | Loss: 0.00003184
Iteration 57/1000 | Loss: 0.00003183
Iteration 58/1000 | Loss: 0.00003183
Iteration 59/1000 | Loss: 0.00003182
Iteration 60/1000 | Loss: 0.00003180
Iteration 61/1000 | Loss: 0.00003180
Iteration 62/1000 | Loss: 0.00003179
Iteration 63/1000 | Loss: 0.00003178
Iteration 64/1000 | Loss: 0.00003177
Iteration 65/1000 | Loss: 0.00003176
Iteration 66/1000 | Loss: 0.00003176
Iteration 67/1000 | Loss: 0.00003175
Iteration 68/1000 | Loss: 0.00003175
Iteration 69/1000 | Loss: 0.00003175
Iteration 70/1000 | Loss: 0.00003175
Iteration 71/1000 | Loss: 0.00003175
Iteration 72/1000 | Loss: 0.00003174
Iteration 73/1000 | Loss: 0.00003174
Iteration 74/1000 | Loss: 0.00003174
Iteration 75/1000 | Loss: 0.00003173
Iteration 76/1000 | Loss: 0.00003173
Iteration 77/1000 | Loss: 0.00003173
Iteration 78/1000 | Loss: 0.00003173
Iteration 79/1000 | Loss: 0.00003173
Iteration 80/1000 | Loss: 0.00003172
Iteration 81/1000 | Loss: 0.00003172
Iteration 82/1000 | Loss: 0.00003172
Iteration 83/1000 | Loss: 0.00003172
Iteration 84/1000 | Loss: 0.00003172
Iteration 85/1000 | Loss: 0.00003172
Iteration 86/1000 | Loss: 0.00003171
Iteration 87/1000 | Loss: 0.00003171
Iteration 88/1000 | Loss: 0.00003171
Iteration 89/1000 | Loss: 0.00003171
Iteration 90/1000 | Loss: 0.00003170
Iteration 91/1000 | Loss: 0.00003170
Iteration 92/1000 | Loss: 0.00003170
Iteration 93/1000 | Loss: 0.00003170
Iteration 94/1000 | Loss: 0.00003169
Iteration 95/1000 | Loss: 0.00003169
Iteration 96/1000 | Loss: 0.00003169
Iteration 97/1000 | Loss: 0.00003169
Iteration 98/1000 | Loss: 0.00003169
Iteration 99/1000 | Loss: 0.00003169
Iteration 100/1000 | Loss: 0.00003169
Iteration 101/1000 | Loss: 0.00003169
Iteration 102/1000 | Loss: 0.00003168
Iteration 103/1000 | Loss: 0.00003168
Iteration 104/1000 | Loss: 0.00003168
Iteration 105/1000 | Loss: 0.00003168
Iteration 106/1000 | Loss: 0.00003168
Iteration 107/1000 | Loss: 0.00003168
Iteration 108/1000 | Loss: 0.00003168
Iteration 109/1000 | Loss: 0.00003168
Iteration 110/1000 | Loss: 0.00003168
Iteration 111/1000 | Loss: 0.00003168
Iteration 112/1000 | Loss: 0.00003168
Iteration 113/1000 | Loss: 0.00003167
Iteration 114/1000 | Loss: 0.00003167
Iteration 115/1000 | Loss: 0.00003167
Iteration 116/1000 | Loss: 0.00003167
Iteration 117/1000 | Loss: 0.00003167
Iteration 118/1000 | Loss: 0.00003167
Iteration 119/1000 | Loss: 0.00003167
Iteration 120/1000 | Loss: 0.00003167
Iteration 121/1000 | Loss: 0.00003167
Iteration 122/1000 | Loss: 0.00003166
Iteration 123/1000 | Loss: 0.00003166
Iteration 124/1000 | Loss: 0.00003166
Iteration 125/1000 | Loss: 0.00003166
Iteration 126/1000 | Loss: 0.00003166
Iteration 127/1000 | Loss: 0.00003166
Iteration 128/1000 | Loss: 0.00003166
Iteration 129/1000 | Loss: 0.00003166
Iteration 130/1000 | Loss: 0.00003166
Iteration 131/1000 | Loss: 0.00003165
Iteration 132/1000 | Loss: 0.00003165
Iteration 133/1000 | Loss: 0.00003165
Iteration 134/1000 | Loss: 0.00003165
Iteration 135/1000 | Loss: 0.00003165
Iteration 136/1000 | Loss: 0.00003165
Iteration 137/1000 | Loss: 0.00003165
Iteration 138/1000 | Loss: 0.00003165
Iteration 139/1000 | Loss: 0.00003165
Iteration 140/1000 | Loss: 0.00003165
Iteration 141/1000 | Loss: 0.00003165
Iteration 142/1000 | Loss: 0.00003165
Iteration 143/1000 | Loss: 0.00003165
Iteration 144/1000 | Loss: 0.00003165
Iteration 145/1000 | Loss: 0.00003164
Iteration 146/1000 | Loss: 0.00003164
Iteration 147/1000 | Loss: 0.00003164
Iteration 148/1000 | Loss: 0.00003164
Iteration 149/1000 | Loss: 0.00003164
Iteration 150/1000 | Loss: 0.00003164
Iteration 151/1000 | Loss: 0.00003164
Iteration 152/1000 | Loss: 0.00003164
Iteration 153/1000 | Loss: 0.00003164
Iteration 154/1000 | Loss: 0.00003163
Iteration 155/1000 | Loss: 0.00003163
Iteration 156/1000 | Loss: 0.00003163
Iteration 157/1000 | Loss: 0.00003163
Iteration 158/1000 | Loss: 0.00003163
Iteration 159/1000 | Loss: 0.00003163
Iteration 160/1000 | Loss: 0.00003163
Iteration 161/1000 | Loss: 0.00003163
Iteration 162/1000 | Loss: 0.00003163
Iteration 163/1000 | Loss: 0.00003163
Iteration 164/1000 | Loss: 0.00003163
Iteration 165/1000 | Loss: 0.00003162
Iteration 166/1000 | Loss: 0.00003162
Iteration 167/1000 | Loss: 0.00003162
Iteration 168/1000 | Loss: 0.00003162
Iteration 169/1000 | Loss: 0.00003162
Iteration 170/1000 | Loss: 0.00003162
Iteration 171/1000 | Loss: 0.00003162
Iteration 172/1000 | Loss: 0.00003161
Iteration 173/1000 | Loss: 0.00003161
Iteration 174/1000 | Loss: 0.00003161
Iteration 175/1000 | Loss: 0.00003161
Iteration 176/1000 | Loss: 0.00003161
Iteration 177/1000 | Loss: 0.00003161
Iteration 178/1000 | Loss: 0.00003161
Iteration 179/1000 | Loss: 0.00003161
Iteration 180/1000 | Loss: 0.00003161
Iteration 181/1000 | Loss: 0.00003161
Iteration 182/1000 | Loss: 0.00003161
Iteration 183/1000 | Loss: 0.00003161
Iteration 184/1000 | Loss: 0.00003161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [3.161008135066368e-05, 3.161008135066368e-05, 3.161008135066368e-05, 3.161008135066368e-05, 3.161008135066368e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.161008135066368e-05

Optimization complete. Final v2v error: 4.501011848449707 mm

Highest mean error: 5.828650951385498 mm for frame 64

Lowest mean error: 3.4872500896453857 mm for frame 173

Saving results

Total time: 81.64372992515564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885851
Iteration 2/25 | Loss: 0.00179706
Iteration 3/25 | Loss: 0.00154288
Iteration 4/25 | Loss: 0.00148235
Iteration 5/25 | Loss: 0.00145663
Iteration 6/25 | Loss: 0.00146793
Iteration 7/25 | Loss: 0.00145408
Iteration 8/25 | Loss: 0.00142821
Iteration 9/25 | Loss: 0.00140477
Iteration 10/25 | Loss: 0.00139437
Iteration 11/25 | Loss: 0.00139956
Iteration 12/25 | Loss: 0.00138766
Iteration 13/25 | Loss: 0.00138323
Iteration 14/25 | Loss: 0.00137594
Iteration 15/25 | Loss: 0.00137574
Iteration 16/25 | Loss: 0.00137979
Iteration 17/25 | Loss: 0.00137845
Iteration 18/25 | Loss: 0.00138001
Iteration 19/25 | Loss: 0.00137742
Iteration 20/25 | Loss: 0.00137879
Iteration 21/25 | Loss: 0.00137046
Iteration 22/25 | Loss: 0.00136690
Iteration 23/25 | Loss: 0.00136478
Iteration 24/25 | Loss: 0.00135830
Iteration 25/25 | Loss: 0.00135229

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25243938
Iteration 2/25 | Loss: 0.00206116
Iteration 3/25 | Loss: 0.00206115
Iteration 4/25 | Loss: 0.00206114
Iteration 5/25 | Loss: 0.00206114
Iteration 6/25 | Loss: 0.00206114
Iteration 7/25 | Loss: 0.00206114
Iteration 8/25 | Loss: 0.00206114
Iteration 9/25 | Loss: 0.00206114
Iteration 10/25 | Loss: 0.00206114
Iteration 11/25 | Loss: 0.00206114
Iteration 12/25 | Loss: 0.00206114
Iteration 13/25 | Loss: 0.00206114
Iteration 14/25 | Loss: 0.00206114
Iteration 15/25 | Loss: 0.00206114
Iteration 16/25 | Loss: 0.00206114
Iteration 17/25 | Loss: 0.00206114
Iteration 18/25 | Loss: 0.00206114
Iteration 19/25 | Loss: 0.00206114
Iteration 20/25 | Loss: 0.00206114
Iteration 21/25 | Loss: 0.00206114
Iteration 22/25 | Loss: 0.00206114
Iteration 23/25 | Loss: 0.00206114
Iteration 24/25 | Loss: 0.00206114
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002061141887679696, 0.002061141887679696, 0.002061141887679696, 0.002061141887679696, 0.002061141887679696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002061141887679696

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206114
Iteration 2/1000 | Loss: 0.00030025
Iteration 3/1000 | Loss: 0.00021121
Iteration 4/1000 | Loss: 0.00029595
Iteration 5/1000 | Loss: 0.00035902
Iteration 6/1000 | Loss: 0.00031994
Iteration 7/1000 | Loss: 0.00074676
Iteration 8/1000 | Loss: 0.00066429
Iteration 9/1000 | Loss: 0.00036101
Iteration 10/1000 | Loss: 0.00013617
Iteration 11/1000 | Loss: 0.00083510
Iteration 12/1000 | Loss: 0.00015969
Iteration 13/1000 | Loss: 0.00069794
Iteration 14/1000 | Loss: 0.00009346
Iteration 15/1000 | Loss: 0.00006744
Iteration 16/1000 | Loss: 0.00005308
Iteration 17/1000 | Loss: 0.00004268
Iteration 18/1000 | Loss: 0.00003590
Iteration 19/1000 | Loss: 0.00003235
Iteration 20/1000 | Loss: 0.00002976
Iteration 21/1000 | Loss: 0.00042309
Iteration 22/1000 | Loss: 0.00091638
Iteration 23/1000 | Loss: 0.00072722
Iteration 24/1000 | Loss: 0.00004957
Iteration 25/1000 | Loss: 0.00040692
Iteration 26/1000 | Loss: 0.00006240
Iteration 27/1000 | Loss: 0.00003774
Iteration 28/1000 | Loss: 0.00002975
Iteration 29/1000 | Loss: 0.00002484
Iteration 30/1000 | Loss: 0.00002323
Iteration 31/1000 | Loss: 0.00002222
Iteration 32/1000 | Loss: 0.00002149
Iteration 33/1000 | Loss: 0.00002109
Iteration 34/1000 | Loss: 0.00002074
Iteration 35/1000 | Loss: 0.00002068
Iteration 36/1000 | Loss: 0.00002051
Iteration 37/1000 | Loss: 0.00002039
Iteration 38/1000 | Loss: 0.00002038
Iteration 39/1000 | Loss: 0.00002038
Iteration 40/1000 | Loss: 0.00002037
Iteration 41/1000 | Loss: 0.00002033
Iteration 42/1000 | Loss: 0.00002031
Iteration 43/1000 | Loss: 0.00002031
Iteration 44/1000 | Loss: 0.00002029
Iteration 45/1000 | Loss: 0.00002027
Iteration 46/1000 | Loss: 0.00002026
Iteration 47/1000 | Loss: 0.00002024
Iteration 48/1000 | Loss: 0.00002024
Iteration 49/1000 | Loss: 0.00002023
Iteration 50/1000 | Loss: 0.00002023
Iteration 51/1000 | Loss: 0.00002023
Iteration 52/1000 | Loss: 0.00002022
Iteration 53/1000 | Loss: 0.00002022
Iteration 54/1000 | Loss: 0.00002022
Iteration 55/1000 | Loss: 0.00002021
Iteration 56/1000 | Loss: 0.00002021
Iteration 57/1000 | Loss: 0.00002020
Iteration 58/1000 | Loss: 0.00002020
Iteration 59/1000 | Loss: 0.00002017
Iteration 60/1000 | Loss: 0.00002017
Iteration 61/1000 | Loss: 0.00002015
Iteration 62/1000 | Loss: 0.00002014
Iteration 63/1000 | Loss: 0.00002014
Iteration 64/1000 | Loss: 0.00002013
Iteration 65/1000 | Loss: 0.00002013
Iteration 66/1000 | Loss: 0.00002013
Iteration 67/1000 | Loss: 0.00002013
Iteration 68/1000 | Loss: 0.00002012
Iteration 69/1000 | Loss: 0.00002012
Iteration 70/1000 | Loss: 0.00002011
Iteration 71/1000 | Loss: 0.00002011
Iteration 72/1000 | Loss: 0.00002011
Iteration 73/1000 | Loss: 0.00002010
Iteration 74/1000 | Loss: 0.00002010
Iteration 75/1000 | Loss: 0.00002009
Iteration 76/1000 | Loss: 0.00002009
Iteration 77/1000 | Loss: 0.00002009
Iteration 78/1000 | Loss: 0.00002008
Iteration 79/1000 | Loss: 0.00002008
Iteration 80/1000 | Loss: 0.00002008
Iteration 81/1000 | Loss: 0.00002008
Iteration 82/1000 | Loss: 0.00002007
Iteration 83/1000 | Loss: 0.00002007
Iteration 84/1000 | Loss: 0.00002007
Iteration 85/1000 | Loss: 0.00002007
Iteration 86/1000 | Loss: 0.00002007
Iteration 87/1000 | Loss: 0.00002007
Iteration 88/1000 | Loss: 0.00002007
Iteration 89/1000 | Loss: 0.00002007
Iteration 90/1000 | Loss: 0.00002007
Iteration 91/1000 | Loss: 0.00002006
Iteration 92/1000 | Loss: 0.00002006
Iteration 93/1000 | Loss: 0.00002006
Iteration 94/1000 | Loss: 0.00002006
Iteration 95/1000 | Loss: 0.00002006
Iteration 96/1000 | Loss: 0.00002006
Iteration 97/1000 | Loss: 0.00002006
Iteration 98/1000 | Loss: 0.00002006
Iteration 99/1000 | Loss: 0.00002005
Iteration 100/1000 | Loss: 0.00002005
Iteration 101/1000 | Loss: 0.00002005
Iteration 102/1000 | Loss: 0.00002005
Iteration 103/1000 | Loss: 0.00002005
Iteration 104/1000 | Loss: 0.00002005
Iteration 105/1000 | Loss: 0.00002005
Iteration 106/1000 | Loss: 0.00002005
Iteration 107/1000 | Loss: 0.00002004
Iteration 108/1000 | Loss: 0.00002004
Iteration 109/1000 | Loss: 0.00002004
Iteration 110/1000 | Loss: 0.00002004
Iteration 111/1000 | Loss: 0.00002004
Iteration 112/1000 | Loss: 0.00002004
Iteration 113/1000 | Loss: 0.00002004
Iteration 114/1000 | Loss: 0.00002003
Iteration 115/1000 | Loss: 0.00002003
Iteration 116/1000 | Loss: 0.00002003
Iteration 117/1000 | Loss: 0.00002002
Iteration 118/1000 | Loss: 0.00002002
Iteration 119/1000 | Loss: 0.00002002
Iteration 120/1000 | Loss: 0.00002002
Iteration 121/1000 | Loss: 0.00002002
Iteration 122/1000 | Loss: 0.00002002
Iteration 123/1000 | Loss: 0.00002002
Iteration 124/1000 | Loss: 0.00002002
Iteration 125/1000 | Loss: 0.00002002
Iteration 126/1000 | Loss: 0.00002002
Iteration 127/1000 | Loss: 0.00002002
Iteration 128/1000 | Loss: 0.00002002
Iteration 129/1000 | Loss: 0.00002002
Iteration 130/1000 | Loss: 0.00002002
Iteration 131/1000 | Loss: 0.00002002
Iteration 132/1000 | Loss: 0.00002002
Iteration 133/1000 | Loss: 0.00002002
Iteration 134/1000 | Loss: 0.00002002
Iteration 135/1000 | Loss: 0.00002002
Iteration 136/1000 | Loss: 0.00002002
Iteration 137/1000 | Loss: 0.00002002
Iteration 138/1000 | Loss: 0.00002002
Iteration 139/1000 | Loss: 0.00002002
Iteration 140/1000 | Loss: 0.00002002
Iteration 141/1000 | Loss: 0.00002002
Iteration 142/1000 | Loss: 0.00002002
Iteration 143/1000 | Loss: 0.00002002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [2.0015128029626794e-05, 2.0015128029626794e-05, 2.0015128029626794e-05, 2.0015128029626794e-05, 2.0015128029626794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0015128029626794e-05

Optimization complete. Final v2v error: 3.767573595046997 mm

Highest mean error: 4.366184711456299 mm for frame 128

Lowest mean error: 3.0980660915374756 mm for frame 1

Saving results

Total time: 105.11715745925903
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413103
Iteration 2/25 | Loss: 0.00123965
Iteration 3/25 | Loss: 0.00114301
Iteration 4/25 | Loss: 0.00113019
Iteration 5/25 | Loss: 0.00112706
Iteration 6/25 | Loss: 0.00112706
Iteration 7/25 | Loss: 0.00112706
Iteration 8/25 | Loss: 0.00112706
Iteration 9/25 | Loss: 0.00112706
Iteration 10/25 | Loss: 0.00112706
Iteration 11/25 | Loss: 0.00112706
Iteration 12/25 | Loss: 0.00112706
Iteration 13/25 | Loss: 0.00112706
Iteration 14/25 | Loss: 0.00112706
Iteration 15/25 | Loss: 0.00112706
Iteration 16/25 | Loss: 0.00112706
Iteration 17/25 | Loss: 0.00112706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011270609684288502, 0.0011270609684288502, 0.0011270609684288502, 0.0011270609684288502, 0.0011270609684288502]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011270609684288502

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10632157
Iteration 2/25 | Loss: 0.00054108
Iteration 3/25 | Loss: 0.00054107
Iteration 4/25 | Loss: 0.00054107
Iteration 5/25 | Loss: 0.00054107
Iteration 6/25 | Loss: 0.00054107
Iteration 7/25 | Loss: 0.00054107
Iteration 8/25 | Loss: 0.00054107
Iteration 9/25 | Loss: 0.00054107
Iteration 10/25 | Loss: 0.00054107
Iteration 11/25 | Loss: 0.00054107
Iteration 12/25 | Loss: 0.00054107
Iteration 13/25 | Loss: 0.00054107
Iteration 14/25 | Loss: 0.00054107
Iteration 15/25 | Loss: 0.00054107
Iteration 16/25 | Loss: 0.00054107
Iteration 17/25 | Loss: 0.00054107
Iteration 18/25 | Loss: 0.00054107
Iteration 19/25 | Loss: 0.00054107
Iteration 20/25 | Loss: 0.00054107
Iteration 21/25 | Loss: 0.00054107
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005410685553215444, 0.0005410685553215444, 0.0005410685553215444, 0.0005410685553215444, 0.0005410685553215444]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005410685553215444

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054107
Iteration 2/1000 | Loss: 0.00002688
Iteration 3/1000 | Loss: 0.00001900
Iteration 4/1000 | Loss: 0.00001784
Iteration 5/1000 | Loss: 0.00001699
Iteration 6/1000 | Loss: 0.00001659
Iteration 7/1000 | Loss: 0.00001631
Iteration 8/1000 | Loss: 0.00001598
Iteration 9/1000 | Loss: 0.00001586
Iteration 10/1000 | Loss: 0.00001583
Iteration 11/1000 | Loss: 0.00001578
Iteration 12/1000 | Loss: 0.00001572
Iteration 13/1000 | Loss: 0.00001569
Iteration 14/1000 | Loss: 0.00001568
Iteration 15/1000 | Loss: 0.00001567
Iteration 16/1000 | Loss: 0.00001567
Iteration 17/1000 | Loss: 0.00001566
Iteration 18/1000 | Loss: 0.00001566
Iteration 19/1000 | Loss: 0.00001565
Iteration 20/1000 | Loss: 0.00001562
Iteration 21/1000 | Loss: 0.00001562
Iteration 22/1000 | Loss: 0.00001561
Iteration 23/1000 | Loss: 0.00001561
Iteration 24/1000 | Loss: 0.00001561
Iteration 25/1000 | Loss: 0.00001561
Iteration 26/1000 | Loss: 0.00001561
Iteration 27/1000 | Loss: 0.00001561
Iteration 28/1000 | Loss: 0.00001560
Iteration 29/1000 | Loss: 0.00001560
Iteration 30/1000 | Loss: 0.00001560
Iteration 31/1000 | Loss: 0.00001560
Iteration 32/1000 | Loss: 0.00001559
Iteration 33/1000 | Loss: 0.00001559
Iteration 34/1000 | Loss: 0.00001558
Iteration 35/1000 | Loss: 0.00001558
Iteration 36/1000 | Loss: 0.00001558
Iteration 37/1000 | Loss: 0.00001556
Iteration 38/1000 | Loss: 0.00001556
Iteration 39/1000 | Loss: 0.00001555
Iteration 40/1000 | Loss: 0.00001555
Iteration 41/1000 | Loss: 0.00001555
Iteration 42/1000 | Loss: 0.00001555
Iteration 43/1000 | Loss: 0.00001554
Iteration 44/1000 | Loss: 0.00001554
Iteration 45/1000 | Loss: 0.00001554
Iteration 46/1000 | Loss: 0.00001553
Iteration 47/1000 | Loss: 0.00001553
Iteration 48/1000 | Loss: 0.00001553
Iteration 49/1000 | Loss: 0.00001553
Iteration 50/1000 | Loss: 0.00001553
Iteration 51/1000 | Loss: 0.00001552
Iteration 52/1000 | Loss: 0.00001552
Iteration 53/1000 | Loss: 0.00001551
Iteration 54/1000 | Loss: 0.00001551
Iteration 55/1000 | Loss: 0.00001551
Iteration 56/1000 | Loss: 0.00001551
Iteration 57/1000 | Loss: 0.00001551
Iteration 58/1000 | Loss: 0.00001551
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001550
Iteration 61/1000 | Loss: 0.00001550
Iteration 62/1000 | Loss: 0.00001549
Iteration 63/1000 | Loss: 0.00001549
Iteration 64/1000 | Loss: 0.00001549
Iteration 65/1000 | Loss: 0.00001549
Iteration 66/1000 | Loss: 0.00001549
Iteration 67/1000 | Loss: 0.00001548
Iteration 68/1000 | Loss: 0.00001548
Iteration 69/1000 | Loss: 0.00001548
Iteration 70/1000 | Loss: 0.00001548
Iteration 71/1000 | Loss: 0.00001548
Iteration 72/1000 | Loss: 0.00001548
Iteration 73/1000 | Loss: 0.00001548
Iteration 74/1000 | Loss: 0.00001548
Iteration 75/1000 | Loss: 0.00001548
Iteration 76/1000 | Loss: 0.00001548
Iteration 77/1000 | Loss: 0.00001548
Iteration 78/1000 | Loss: 0.00001548
Iteration 79/1000 | Loss: 0.00001548
Iteration 80/1000 | Loss: 0.00001548
Iteration 81/1000 | Loss: 0.00001547
Iteration 82/1000 | Loss: 0.00001547
Iteration 83/1000 | Loss: 0.00001547
Iteration 84/1000 | Loss: 0.00001547
Iteration 85/1000 | Loss: 0.00001547
Iteration 86/1000 | Loss: 0.00001547
Iteration 87/1000 | Loss: 0.00001547
Iteration 88/1000 | Loss: 0.00001547
Iteration 89/1000 | Loss: 0.00001547
Iteration 90/1000 | Loss: 0.00001547
Iteration 91/1000 | Loss: 0.00001547
Iteration 92/1000 | Loss: 0.00001547
Iteration 93/1000 | Loss: 0.00001547
Iteration 94/1000 | Loss: 0.00001547
Iteration 95/1000 | Loss: 0.00001547
Iteration 96/1000 | Loss: 0.00001547
Iteration 97/1000 | Loss: 0.00001547
Iteration 98/1000 | Loss: 0.00001547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.5474288375116885e-05, 1.5474288375116885e-05, 1.5474288375116885e-05, 1.5474288375116885e-05, 1.5474288375116885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5474288375116885e-05

Optimization complete. Final v2v error: 3.323805570602417 mm

Highest mean error: 3.7691261768341064 mm for frame 208

Lowest mean error: 3.1059343814849854 mm for frame 25

Saving results

Total time: 33.82197070121765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877513
Iteration 2/25 | Loss: 0.00138986
Iteration 3/25 | Loss: 0.00120870
Iteration 4/25 | Loss: 0.00121003
Iteration 5/25 | Loss: 0.00118302
Iteration 6/25 | Loss: 0.00118169
Iteration 7/25 | Loss: 0.00118137
Iteration 8/25 | Loss: 0.00118126
Iteration 9/25 | Loss: 0.00118126
Iteration 10/25 | Loss: 0.00118126
Iteration 11/25 | Loss: 0.00118126
Iteration 12/25 | Loss: 0.00118126
Iteration 13/25 | Loss: 0.00118126
Iteration 14/25 | Loss: 0.00118126
Iteration 15/25 | Loss: 0.00118126
Iteration 16/25 | Loss: 0.00118126
Iteration 17/25 | Loss: 0.00118126
Iteration 18/25 | Loss: 0.00118126
Iteration 19/25 | Loss: 0.00118126
Iteration 20/25 | Loss: 0.00118126
Iteration 21/25 | Loss: 0.00118126
Iteration 22/25 | Loss: 0.00118126
Iteration 23/25 | Loss: 0.00118126
Iteration 24/25 | Loss: 0.00118126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011812557931989431, 0.0011812557931989431, 0.0011812557931989431, 0.0011812557931989431, 0.0011812557931989431]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011812557931989431

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41336548
Iteration 2/25 | Loss: 0.00052556
Iteration 3/25 | Loss: 0.00052555
Iteration 4/25 | Loss: 0.00052555
Iteration 5/25 | Loss: 0.00052555
Iteration 6/25 | Loss: 0.00052555
Iteration 7/25 | Loss: 0.00052555
Iteration 8/25 | Loss: 0.00052555
Iteration 9/25 | Loss: 0.00052555
Iteration 10/25 | Loss: 0.00052555
Iteration 11/25 | Loss: 0.00052555
Iteration 12/25 | Loss: 0.00052555
Iteration 13/25 | Loss: 0.00052555
Iteration 14/25 | Loss: 0.00052555
Iteration 15/25 | Loss: 0.00052555
Iteration 16/25 | Loss: 0.00052555
Iteration 17/25 | Loss: 0.00052555
Iteration 18/25 | Loss: 0.00052555
Iteration 19/25 | Loss: 0.00052555
Iteration 20/25 | Loss: 0.00052555
Iteration 21/25 | Loss: 0.00052555
Iteration 22/25 | Loss: 0.00052555
Iteration 23/25 | Loss: 0.00052555
Iteration 24/25 | Loss: 0.00052555
Iteration 25/25 | Loss: 0.00052555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052555
Iteration 2/1000 | Loss: 0.00004667
Iteration 3/1000 | Loss: 0.00002395
Iteration 4/1000 | Loss: 0.00002137
Iteration 5/1000 | Loss: 0.00002006
Iteration 6/1000 | Loss: 0.00001939
Iteration 7/1000 | Loss: 0.00001897
Iteration 8/1000 | Loss: 0.00001841
Iteration 9/1000 | Loss: 0.00001808
Iteration 10/1000 | Loss: 0.00001786
Iteration 11/1000 | Loss: 0.00001761
Iteration 12/1000 | Loss: 0.00001751
Iteration 13/1000 | Loss: 0.00001743
Iteration 14/1000 | Loss: 0.00001743
Iteration 15/1000 | Loss: 0.00001742
Iteration 16/1000 | Loss: 0.00001742
Iteration 17/1000 | Loss: 0.00001739
Iteration 18/1000 | Loss: 0.00001734
Iteration 19/1000 | Loss: 0.00001731
Iteration 20/1000 | Loss: 0.00001731
Iteration 21/1000 | Loss: 0.00001731
Iteration 22/1000 | Loss: 0.00001727
Iteration 23/1000 | Loss: 0.00001727
Iteration 24/1000 | Loss: 0.00001727
Iteration 25/1000 | Loss: 0.00001727
Iteration 26/1000 | Loss: 0.00001727
Iteration 27/1000 | Loss: 0.00001727
Iteration 28/1000 | Loss: 0.00001727
Iteration 29/1000 | Loss: 0.00001727
Iteration 30/1000 | Loss: 0.00001727
Iteration 31/1000 | Loss: 0.00001727
Iteration 32/1000 | Loss: 0.00001726
Iteration 33/1000 | Loss: 0.00001726
Iteration 34/1000 | Loss: 0.00001726
Iteration 35/1000 | Loss: 0.00001726
Iteration 36/1000 | Loss: 0.00001726
Iteration 37/1000 | Loss: 0.00001726
Iteration 38/1000 | Loss: 0.00001726
Iteration 39/1000 | Loss: 0.00001726
Iteration 40/1000 | Loss: 0.00001725
Iteration 41/1000 | Loss: 0.00001725
Iteration 42/1000 | Loss: 0.00001724
Iteration 43/1000 | Loss: 0.00001724
Iteration 44/1000 | Loss: 0.00001724
Iteration 45/1000 | Loss: 0.00001723
Iteration 46/1000 | Loss: 0.00001723
Iteration 47/1000 | Loss: 0.00001723
Iteration 48/1000 | Loss: 0.00001723
Iteration 49/1000 | Loss: 0.00001723
Iteration 50/1000 | Loss: 0.00001723
Iteration 51/1000 | Loss: 0.00001723
Iteration 52/1000 | Loss: 0.00001723
Iteration 53/1000 | Loss: 0.00001722
Iteration 54/1000 | Loss: 0.00001722
Iteration 55/1000 | Loss: 0.00001722
Iteration 56/1000 | Loss: 0.00001722
Iteration 57/1000 | Loss: 0.00001722
Iteration 58/1000 | Loss: 0.00001722
Iteration 59/1000 | Loss: 0.00001721
Iteration 60/1000 | Loss: 0.00001721
Iteration 61/1000 | Loss: 0.00001721
Iteration 62/1000 | Loss: 0.00001720
Iteration 63/1000 | Loss: 0.00001720
Iteration 64/1000 | Loss: 0.00001720
Iteration 65/1000 | Loss: 0.00001720
Iteration 66/1000 | Loss: 0.00001719
Iteration 67/1000 | Loss: 0.00001719
Iteration 68/1000 | Loss: 0.00001719
Iteration 69/1000 | Loss: 0.00001719
Iteration 70/1000 | Loss: 0.00001719
Iteration 71/1000 | Loss: 0.00001719
Iteration 72/1000 | Loss: 0.00001719
Iteration 73/1000 | Loss: 0.00001719
Iteration 74/1000 | Loss: 0.00001719
Iteration 75/1000 | Loss: 0.00001719
Iteration 76/1000 | Loss: 0.00001719
Iteration 77/1000 | Loss: 0.00001719
Iteration 78/1000 | Loss: 0.00001719
Iteration 79/1000 | Loss: 0.00001719
Iteration 80/1000 | Loss: 0.00001719
Iteration 81/1000 | Loss: 0.00001719
Iteration 82/1000 | Loss: 0.00001719
Iteration 83/1000 | Loss: 0.00001718
Iteration 84/1000 | Loss: 0.00001718
Iteration 85/1000 | Loss: 0.00001718
Iteration 86/1000 | Loss: 0.00001718
Iteration 87/1000 | Loss: 0.00001718
Iteration 88/1000 | Loss: 0.00001718
Iteration 89/1000 | Loss: 0.00001718
Iteration 90/1000 | Loss: 0.00001718
Iteration 91/1000 | Loss: 0.00001718
Iteration 92/1000 | Loss: 0.00001718
Iteration 93/1000 | Loss: 0.00001718
Iteration 94/1000 | Loss: 0.00001718
Iteration 95/1000 | Loss: 0.00001718
Iteration 96/1000 | Loss: 0.00001718
Iteration 97/1000 | Loss: 0.00001718
Iteration 98/1000 | Loss: 0.00001718
Iteration 99/1000 | Loss: 0.00001718
Iteration 100/1000 | Loss: 0.00001718
Iteration 101/1000 | Loss: 0.00001718
Iteration 102/1000 | Loss: 0.00001718
Iteration 103/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.718398016237188e-05, 1.718398016237188e-05, 1.718398016237188e-05, 1.718398016237188e-05, 1.718398016237188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.718398016237188e-05

Optimization complete. Final v2v error: 3.5037198066711426 mm

Highest mean error: 4.075300693511963 mm for frame 68

Lowest mean error: 3.1714041233062744 mm for frame 2

Saving results

Total time: 64.55673575401306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490329
Iteration 2/25 | Loss: 0.00129770
Iteration 3/25 | Loss: 0.00117118
Iteration 4/25 | Loss: 0.00115617
Iteration 5/25 | Loss: 0.00115212
Iteration 6/25 | Loss: 0.00115066
Iteration 7/25 | Loss: 0.00115062
Iteration 8/25 | Loss: 0.00115062
Iteration 9/25 | Loss: 0.00115062
Iteration 10/25 | Loss: 0.00115062
Iteration 11/25 | Loss: 0.00115062
Iteration 12/25 | Loss: 0.00115062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011506150476634502, 0.0011506150476634502, 0.0011506150476634502, 0.0011506150476634502, 0.0011506150476634502]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011506150476634502

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35116410
Iteration 2/25 | Loss: 0.00056864
Iteration 3/25 | Loss: 0.00056862
Iteration 4/25 | Loss: 0.00056862
Iteration 5/25 | Loss: 0.00056862
Iteration 6/25 | Loss: 0.00056862
Iteration 7/25 | Loss: 0.00056862
Iteration 8/25 | Loss: 0.00056862
Iteration 9/25 | Loss: 0.00056862
Iteration 10/25 | Loss: 0.00056862
Iteration 11/25 | Loss: 0.00056862
Iteration 12/25 | Loss: 0.00056862
Iteration 13/25 | Loss: 0.00056862
Iteration 14/25 | Loss: 0.00056862
Iteration 15/25 | Loss: 0.00056862
Iteration 16/25 | Loss: 0.00056862
Iteration 17/25 | Loss: 0.00056862
Iteration 18/25 | Loss: 0.00056862
Iteration 19/25 | Loss: 0.00056862
Iteration 20/25 | Loss: 0.00056862
Iteration 21/25 | Loss: 0.00056862
Iteration 22/25 | Loss: 0.00056862
Iteration 23/25 | Loss: 0.00056862
Iteration 24/25 | Loss: 0.00056862
Iteration 25/25 | Loss: 0.00056862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056862
Iteration 2/1000 | Loss: 0.00004808
Iteration 3/1000 | Loss: 0.00002592
Iteration 4/1000 | Loss: 0.00002211
Iteration 5/1000 | Loss: 0.00002053
Iteration 6/1000 | Loss: 0.00001955
Iteration 7/1000 | Loss: 0.00001900
Iteration 8/1000 | Loss: 0.00001854
Iteration 9/1000 | Loss: 0.00001819
Iteration 10/1000 | Loss: 0.00001797
Iteration 11/1000 | Loss: 0.00001780
Iteration 12/1000 | Loss: 0.00001772
Iteration 13/1000 | Loss: 0.00001756
Iteration 14/1000 | Loss: 0.00001747
Iteration 15/1000 | Loss: 0.00001746
Iteration 16/1000 | Loss: 0.00001745
Iteration 17/1000 | Loss: 0.00001744
Iteration 18/1000 | Loss: 0.00001743
Iteration 19/1000 | Loss: 0.00001742
Iteration 20/1000 | Loss: 0.00001742
Iteration 21/1000 | Loss: 0.00001741
Iteration 22/1000 | Loss: 0.00001740
Iteration 23/1000 | Loss: 0.00001739
Iteration 24/1000 | Loss: 0.00001739
Iteration 25/1000 | Loss: 0.00001735
Iteration 26/1000 | Loss: 0.00001733
Iteration 27/1000 | Loss: 0.00001731
Iteration 28/1000 | Loss: 0.00001730
Iteration 29/1000 | Loss: 0.00001729
Iteration 30/1000 | Loss: 0.00001728
Iteration 31/1000 | Loss: 0.00001727
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001726
Iteration 34/1000 | Loss: 0.00001724
Iteration 35/1000 | Loss: 0.00001724
Iteration 36/1000 | Loss: 0.00001723
Iteration 37/1000 | Loss: 0.00001723
Iteration 38/1000 | Loss: 0.00001723
Iteration 39/1000 | Loss: 0.00001722
Iteration 40/1000 | Loss: 0.00001722
Iteration 41/1000 | Loss: 0.00001722
Iteration 42/1000 | Loss: 0.00001722
Iteration 43/1000 | Loss: 0.00001721
Iteration 44/1000 | Loss: 0.00001721
Iteration 45/1000 | Loss: 0.00001721
Iteration 46/1000 | Loss: 0.00001720
Iteration 47/1000 | Loss: 0.00001720
Iteration 48/1000 | Loss: 0.00001719
Iteration 49/1000 | Loss: 0.00001719
Iteration 50/1000 | Loss: 0.00001719
Iteration 51/1000 | Loss: 0.00001719
Iteration 52/1000 | Loss: 0.00001719
Iteration 53/1000 | Loss: 0.00001719
Iteration 54/1000 | Loss: 0.00001718
Iteration 55/1000 | Loss: 0.00001718
Iteration 56/1000 | Loss: 0.00001718
Iteration 57/1000 | Loss: 0.00001718
Iteration 58/1000 | Loss: 0.00001717
Iteration 59/1000 | Loss: 0.00001717
Iteration 60/1000 | Loss: 0.00001717
Iteration 61/1000 | Loss: 0.00001717
Iteration 62/1000 | Loss: 0.00001717
Iteration 63/1000 | Loss: 0.00001717
Iteration 64/1000 | Loss: 0.00001717
Iteration 65/1000 | Loss: 0.00001717
Iteration 66/1000 | Loss: 0.00001717
Iteration 67/1000 | Loss: 0.00001717
Iteration 68/1000 | Loss: 0.00001717
Iteration 69/1000 | Loss: 0.00001717
Iteration 70/1000 | Loss: 0.00001716
Iteration 71/1000 | Loss: 0.00001716
Iteration 72/1000 | Loss: 0.00001716
Iteration 73/1000 | Loss: 0.00001716
Iteration 74/1000 | Loss: 0.00001716
Iteration 75/1000 | Loss: 0.00001716
Iteration 76/1000 | Loss: 0.00001715
Iteration 77/1000 | Loss: 0.00001715
Iteration 78/1000 | Loss: 0.00001715
Iteration 79/1000 | Loss: 0.00001715
Iteration 80/1000 | Loss: 0.00001715
Iteration 81/1000 | Loss: 0.00001715
Iteration 82/1000 | Loss: 0.00001715
Iteration 83/1000 | Loss: 0.00001714
Iteration 84/1000 | Loss: 0.00001714
Iteration 85/1000 | Loss: 0.00001714
Iteration 86/1000 | Loss: 0.00001714
Iteration 87/1000 | Loss: 0.00001714
Iteration 88/1000 | Loss: 0.00001714
Iteration 89/1000 | Loss: 0.00001714
Iteration 90/1000 | Loss: 0.00001714
Iteration 91/1000 | Loss: 0.00001714
Iteration 92/1000 | Loss: 0.00001714
Iteration 93/1000 | Loss: 0.00001714
Iteration 94/1000 | Loss: 0.00001714
Iteration 95/1000 | Loss: 0.00001713
Iteration 96/1000 | Loss: 0.00001713
Iteration 97/1000 | Loss: 0.00001713
Iteration 98/1000 | Loss: 0.00001713
Iteration 99/1000 | Loss: 0.00001713
Iteration 100/1000 | Loss: 0.00001713
Iteration 101/1000 | Loss: 0.00001713
Iteration 102/1000 | Loss: 0.00001713
Iteration 103/1000 | Loss: 0.00001713
Iteration 104/1000 | Loss: 0.00001713
Iteration 105/1000 | Loss: 0.00001713
Iteration 106/1000 | Loss: 0.00001713
Iteration 107/1000 | Loss: 0.00001713
Iteration 108/1000 | Loss: 0.00001713
Iteration 109/1000 | Loss: 0.00001712
Iteration 110/1000 | Loss: 0.00001712
Iteration 111/1000 | Loss: 0.00001712
Iteration 112/1000 | Loss: 0.00001712
Iteration 113/1000 | Loss: 0.00001712
Iteration 114/1000 | Loss: 0.00001712
Iteration 115/1000 | Loss: 0.00001712
Iteration 116/1000 | Loss: 0.00001712
Iteration 117/1000 | Loss: 0.00001712
Iteration 118/1000 | Loss: 0.00001712
Iteration 119/1000 | Loss: 0.00001712
Iteration 120/1000 | Loss: 0.00001711
Iteration 121/1000 | Loss: 0.00001711
Iteration 122/1000 | Loss: 0.00001711
Iteration 123/1000 | Loss: 0.00001711
Iteration 124/1000 | Loss: 0.00001711
Iteration 125/1000 | Loss: 0.00001711
Iteration 126/1000 | Loss: 0.00001710
Iteration 127/1000 | Loss: 0.00001710
Iteration 128/1000 | Loss: 0.00001710
Iteration 129/1000 | Loss: 0.00001709
Iteration 130/1000 | Loss: 0.00001709
Iteration 131/1000 | Loss: 0.00001709
Iteration 132/1000 | Loss: 0.00001708
Iteration 133/1000 | Loss: 0.00001708
Iteration 134/1000 | Loss: 0.00001707
Iteration 135/1000 | Loss: 0.00001707
Iteration 136/1000 | Loss: 0.00001707
Iteration 137/1000 | Loss: 0.00001707
Iteration 138/1000 | Loss: 0.00001706
Iteration 139/1000 | Loss: 0.00001706
Iteration 140/1000 | Loss: 0.00001706
Iteration 141/1000 | Loss: 0.00001706
Iteration 142/1000 | Loss: 0.00001706
Iteration 143/1000 | Loss: 0.00001706
Iteration 144/1000 | Loss: 0.00001705
Iteration 145/1000 | Loss: 0.00001705
Iteration 146/1000 | Loss: 0.00001705
Iteration 147/1000 | Loss: 0.00001705
Iteration 148/1000 | Loss: 0.00001705
Iteration 149/1000 | Loss: 0.00001705
Iteration 150/1000 | Loss: 0.00001705
Iteration 151/1000 | Loss: 0.00001704
Iteration 152/1000 | Loss: 0.00001704
Iteration 153/1000 | Loss: 0.00001704
Iteration 154/1000 | Loss: 0.00001704
Iteration 155/1000 | Loss: 0.00001704
Iteration 156/1000 | Loss: 0.00001704
Iteration 157/1000 | Loss: 0.00001704
Iteration 158/1000 | Loss: 0.00001704
Iteration 159/1000 | Loss: 0.00001704
Iteration 160/1000 | Loss: 0.00001704
Iteration 161/1000 | Loss: 0.00001704
Iteration 162/1000 | Loss: 0.00001704
Iteration 163/1000 | Loss: 0.00001704
Iteration 164/1000 | Loss: 0.00001704
Iteration 165/1000 | Loss: 0.00001704
Iteration 166/1000 | Loss: 0.00001704
Iteration 167/1000 | Loss: 0.00001704
Iteration 168/1000 | Loss: 0.00001704
Iteration 169/1000 | Loss: 0.00001704
Iteration 170/1000 | Loss: 0.00001704
Iteration 171/1000 | Loss: 0.00001704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.703956331766676e-05, 1.703956331766676e-05, 1.703956331766676e-05, 1.703956331766676e-05, 1.703956331766676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.703956331766676e-05

Optimization complete. Final v2v error: 3.3925280570983887 mm

Highest mean error: 3.7591030597686768 mm for frame 60

Lowest mean error: 3.216594696044922 mm for frame 129

Saving results

Total time: 37.7601420879364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425958
Iteration 2/25 | Loss: 0.00126823
Iteration 3/25 | Loss: 0.00114916
Iteration 4/25 | Loss: 0.00114261
Iteration 5/25 | Loss: 0.00114114
Iteration 6/25 | Loss: 0.00114082
Iteration 7/25 | Loss: 0.00114082
Iteration 8/25 | Loss: 0.00114082
Iteration 9/25 | Loss: 0.00114082
Iteration 10/25 | Loss: 0.00114082
Iteration 11/25 | Loss: 0.00114082
Iteration 12/25 | Loss: 0.00114082
Iteration 13/25 | Loss: 0.00114082
Iteration 14/25 | Loss: 0.00114082
Iteration 15/25 | Loss: 0.00114082
Iteration 16/25 | Loss: 0.00114082
Iteration 17/25 | Loss: 0.00114082
Iteration 18/25 | Loss: 0.00114082
Iteration 19/25 | Loss: 0.00114082
Iteration 20/25 | Loss: 0.00114082
Iteration 21/25 | Loss: 0.00114082
Iteration 22/25 | Loss: 0.00114082
Iteration 23/25 | Loss: 0.00114082
Iteration 24/25 | Loss: 0.00114082
Iteration 25/25 | Loss: 0.00114082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32881379
Iteration 2/25 | Loss: 0.00058135
Iteration 3/25 | Loss: 0.00058135
Iteration 4/25 | Loss: 0.00058135
Iteration 5/25 | Loss: 0.00058135
Iteration 6/25 | Loss: 0.00058135
Iteration 7/25 | Loss: 0.00058134
Iteration 8/25 | Loss: 0.00058134
Iteration 9/25 | Loss: 0.00058134
Iteration 10/25 | Loss: 0.00058134
Iteration 11/25 | Loss: 0.00058134
Iteration 12/25 | Loss: 0.00058134
Iteration 13/25 | Loss: 0.00058134
Iteration 14/25 | Loss: 0.00058134
Iteration 15/25 | Loss: 0.00058134
Iteration 16/25 | Loss: 0.00058134
Iteration 17/25 | Loss: 0.00058134
Iteration 18/25 | Loss: 0.00058134
Iteration 19/25 | Loss: 0.00058134
Iteration 20/25 | Loss: 0.00058134
Iteration 21/25 | Loss: 0.00058134
Iteration 22/25 | Loss: 0.00058134
Iteration 23/25 | Loss: 0.00058134
Iteration 24/25 | Loss: 0.00058134
Iteration 25/25 | Loss: 0.00058134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058134
Iteration 2/1000 | Loss: 0.00004788
Iteration 3/1000 | Loss: 0.00002452
Iteration 4/1000 | Loss: 0.00001980
Iteration 5/1000 | Loss: 0.00001866
Iteration 6/1000 | Loss: 0.00001800
Iteration 7/1000 | Loss: 0.00001745
Iteration 8/1000 | Loss: 0.00001705
Iteration 9/1000 | Loss: 0.00001669
Iteration 10/1000 | Loss: 0.00001646
Iteration 11/1000 | Loss: 0.00001629
Iteration 12/1000 | Loss: 0.00001624
Iteration 13/1000 | Loss: 0.00001622
Iteration 14/1000 | Loss: 0.00001622
Iteration 15/1000 | Loss: 0.00001621
Iteration 16/1000 | Loss: 0.00001617
Iteration 17/1000 | Loss: 0.00001612
Iteration 18/1000 | Loss: 0.00001607
Iteration 19/1000 | Loss: 0.00001605
Iteration 20/1000 | Loss: 0.00001603
Iteration 21/1000 | Loss: 0.00001603
Iteration 22/1000 | Loss: 0.00001602
Iteration 23/1000 | Loss: 0.00001602
Iteration 24/1000 | Loss: 0.00001601
Iteration 25/1000 | Loss: 0.00001601
Iteration 26/1000 | Loss: 0.00001599
Iteration 27/1000 | Loss: 0.00001599
Iteration 28/1000 | Loss: 0.00001599
Iteration 29/1000 | Loss: 0.00001598
Iteration 30/1000 | Loss: 0.00001598
Iteration 31/1000 | Loss: 0.00001598
Iteration 32/1000 | Loss: 0.00001598
Iteration 33/1000 | Loss: 0.00001598
Iteration 34/1000 | Loss: 0.00001598
Iteration 35/1000 | Loss: 0.00001597
Iteration 36/1000 | Loss: 0.00001596
Iteration 37/1000 | Loss: 0.00001596
Iteration 38/1000 | Loss: 0.00001596
Iteration 39/1000 | Loss: 0.00001595
Iteration 40/1000 | Loss: 0.00001595
Iteration 41/1000 | Loss: 0.00001595
Iteration 42/1000 | Loss: 0.00001595
Iteration 43/1000 | Loss: 0.00001594
Iteration 44/1000 | Loss: 0.00001594
Iteration 45/1000 | Loss: 0.00001594
Iteration 46/1000 | Loss: 0.00001593
Iteration 47/1000 | Loss: 0.00001593
Iteration 48/1000 | Loss: 0.00001593
Iteration 49/1000 | Loss: 0.00001593
Iteration 50/1000 | Loss: 0.00001593
Iteration 51/1000 | Loss: 0.00001592
Iteration 52/1000 | Loss: 0.00001592
Iteration 53/1000 | Loss: 0.00001592
Iteration 54/1000 | Loss: 0.00001592
Iteration 55/1000 | Loss: 0.00001591
Iteration 56/1000 | Loss: 0.00001591
Iteration 57/1000 | Loss: 0.00001591
Iteration 58/1000 | Loss: 0.00001591
Iteration 59/1000 | Loss: 0.00001591
Iteration 60/1000 | Loss: 0.00001590
Iteration 61/1000 | Loss: 0.00001590
Iteration 62/1000 | Loss: 0.00001590
Iteration 63/1000 | Loss: 0.00001590
Iteration 64/1000 | Loss: 0.00001590
Iteration 65/1000 | Loss: 0.00001590
Iteration 66/1000 | Loss: 0.00001590
Iteration 67/1000 | Loss: 0.00001590
Iteration 68/1000 | Loss: 0.00001590
Iteration 69/1000 | Loss: 0.00001590
Iteration 70/1000 | Loss: 0.00001590
Iteration 71/1000 | Loss: 0.00001590
Iteration 72/1000 | Loss: 0.00001590
Iteration 73/1000 | Loss: 0.00001590
Iteration 74/1000 | Loss: 0.00001589
Iteration 75/1000 | Loss: 0.00001589
Iteration 76/1000 | Loss: 0.00001589
Iteration 77/1000 | Loss: 0.00001589
Iteration 78/1000 | Loss: 0.00001589
Iteration 79/1000 | Loss: 0.00001589
Iteration 80/1000 | Loss: 0.00001589
Iteration 81/1000 | Loss: 0.00001589
Iteration 82/1000 | Loss: 0.00001589
Iteration 83/1000 | Loss: 0.00001589
Iteration 84/1000 | Loss: 0.00001589
Iteration 85/1000 | Loss: 0.00001589
Iteration 86/1000 | Loss: 0.00001588
Iteration 87/1000 | Loss: 0.00001588
Iteration 88/1000 | Loss: 0.00001588
Iteration 89/1000 | Loss: 0.00001588
Iteration 90/1000 | Loss: 0.00001588
Iteration 91/1000 | Loss: 0.00001588
Iteration 92/1000 | Loss: 0.00001587
Iteration 93/1000 | Loss: 0.00001587
Iteration 94/1000 | Loss: 0.00001587
Iteration 95/1000 | Loss: 0.00001587
Iteration 96/1000 | Loss: 0.00001587
Iteration 97/1000 | Loss: 0.00001587
Iteration 98/1000 | Loss: 0.00001587
Iteration 99/1000 | Loss: 0.00001587
Iteration 100/1000 | Loss: 0.00001587
Iteration 101/1000 | Loss: 0.00001587
Iteration 102/1000 | Loss: 0.00001587
Iteration 103/1000 | Loss: 0.00001587
Iteration 104/1000 | Loss: 0.00001587
Iteration 105/1000 | Loss: 0.00001586
Iteration 106/1000 | Loss: 0.00001586
Iteration 107/1000 | Loss: 0.00001586
Iteration 108/1000 | Loss: 0.00001586
Iteration 109/1000 | Loss: 0.00001586
Iteration 110/1000 | Loss: 0.00001586
Iteration 111/1000 | Loss: 0.00001586
Iteration 112/1000 | Loss: 0.00001586
Iteration 113/1000 | Loss: 0.00001586
Iteration 114/1000 | Loss: 0.00001585
Iteration 115/1000 | Loss: 0.00001585
Iteration 116/1000 | Loss: 0.00001585
Iteration 117/1000 | Loss: 0.00001585
Iteration 118/1000 | Loss: 0.00001585
Iteration 119/1000 | Loss: 0.00001585
Iteration 120/1000 | Loss: 0.00001585
Iteration 121/1000 | Loss: 0.00001585
Iteration 122/1000 | Loss: 0.00001585
Iteration 123/1000 | Loss: 0.00001585
Iteration 124/1000 | Loss: 0.00001584
Iteration 125/1000 | Loss: 0.00001584
Iteration 126/1000 | Loss: 0.00001584
Iteration 127/1000 | Loss: 0.00001584
Iteration 128/1000 | Loss: 0.00001584
Iteration 129/1000 | Loss: 0.00001584
Iteration 130/1000 | Loss: 0.00001584
Iteration 131/1000 | Loss: 0.00001584
Iteration 132/1000 | Loss: 0.00001583
Iteration 133/1000 | Loss: 0.00001583
Iteration 134/1000 | Loss: 0.00001583
Iteration 135/1000 | Loss: 0.00001583
Iteration 136/1000 | Loss: 0.00001583
Iteration 137/1000 | Loss: 0.00001583
Iteration 138/1000 | Loss: 0.00001583
Iteration 139/1000 | Loss: 0.00001582
Iteration 140/1000 | Loss: 0.00001582
Iteration 141/1000 | Loss: 0.00001582
Iteration 142/1000 | Loss: 0.00001582
Iteration 143/1000 | Loss: 0.00001582
Iteration 144/1000 | Loss: 0.00001581
Iteration 145/1000 | Loss: 0.00001581
Iteration 146/1000 | Loss: 0.00001581
Iteration 147/1000 | Loss: 0.00001581
Iteration 148/1000 | Loss: 0.00001581
Iteration 149/1000 | Loss: 0.00001581
Iteration 150/1000 | Loss: 0.00001581
Iteration 151/1000 | Loss: 0.00001581
Iteration 152/1000 | Loss: 0.00001581
Iteration 153/1000 | Loss: 0.00001581
Iteration 154/1000 | Loss: 0.00001581
Iteration 155/1000 | Loss: 0.00001581
Iteration 156/1000 | Loss: 0.00001581
Iteration 157/1000 | Loss: 0.00001581
Iteration 158/1000 | Loss: 0.00001581
Iteration 159/1000 | Loss: 0.00001581
Iteration 160/1000 | Loss: 0.00001581
Iteration 161/1000 | Loss: 0.00001581
Iteration 162/1000 | Loss: 0.00001581
Iteration 163/1000 | Loss: 0.00001581
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.5809351680218242e-05, 1.5809351680218242e-05, 1.5809351680218242e-05, 1.5809351680218242e-05, 1.5809351680218242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5809351680218242e-05

Optimization complete. Final v2v error: 3.2725272178649902 mm

Highest mean error: 4.2116923332214355 mm for frame 61

Lowest mean error: 2.9764628410339355 mm for frame 123

Saving results

Total time: 36.58835434913635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_5365/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_5365/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465423
Iteration 2/25 | Loss: 0.00131454
Iteration 3/25 | Loss: 0.00121082
Iteration 4/25 | Loss: 0.00119325
Iteration 5/25 | Loss: 0.00118597
Iteration 6/25 | Loss: 0.00118414
Iteration 7/25 | Loss: 0.00118378
Iteration 8/25 | Loss: 0.00118378
Iteration 9/25 | Loss: 0.00118378
Iteration 10/25 | Loss: 0.00118378
Iteration 11/25 | Loss: 0.00118378
Iteration 12/25 | Loss: 0.00118378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011837807251140475, 0.0011837807251140475, 0.0011837807251140475, 0.0011837807251140475, 0.0011837807251140475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011837807251140475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.81701756
Iteration 2/25 | Loss: 0.00060775
Iteration 3/25 | Loss: 0.00060773
Iteration 4/25 | Loss: 0.00060773
Iteration 5/25 | Loss: 0.00060773
Iteration 6/25 | Loss: 0.00060773
Iteration 7/25 | Loss: 0.00060773
Iteration 8/25 | Loss: 0.00060773
Iteration 9/25 | Loss: 0.00060773
Iteration 10/25 | Loss: 0.00060773
Iteration 11/25 | Loss: 0.00060773
Iteration 12/25 | Loss: 0.00060773
Iteration 13/25 | Loss: 0.00060773
Iteration 14/25 | Loss: 0.00060773
Iteration 15/25 | Loss: 0.00060773
Iteration 16/25 | Loss: 0.00060773
Iteration 17/25 | Loss: 0.00060773
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006077283178456128, 0.0006077283178456128, 0.0006077283178456128, 0.0006077283178456128, 0.0006077283178456128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006077283178456128

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060773
Iteration 2/1000 | Loss: 0.00005512
Iteration 3/1000 | Loss: 0.00002770
Iteration 4/1000 | Loss: 0.00002445
Iteration 5/1000 | Loss: 0.00002313
Iteration 6/1000 | Loss: 0.00002222
Iteration 7/1000 | Loss: 0.00002163
Iteration 8/1000 | Loss: 0.00002097
Iteration 9/1000 | Loss: 0.00002059
Iteration 10/1000 | Loss: 0.00002037
Iteration 11/1000 | Loss: 0.00002021
Iteration 12/1000 | Loss: 0.00002009
Iteration 13/1000 | Loss: 0.00002009
Iteration 14/1000 | Loss: 0.00002008
Iteration 15/1000 | Loss: 0.00002007
Iteration 16/1000 | Loss: 0.00002003
Iteration 17/1000 | Loss: 0.00002003
Iteration 18/1000 | Loss: 0.00002002
Iteration 19/1000 | Loss: 0.00002002
Iteration 20/1000 | Loss: 0.00001998
Iteration 21/1000 | Loss: 0.00001996
Iteration 22/1000 | Loss: 0.00001996
Iteration 23/1000 | Loss: 0.00001996
Iteration 24/1000 | Loss: 0.00001996
Iteration 25/1000 | Loss: 0.00001996
Iteration 26/1000 | Loss: 0.00001995
Iteration 27/1000 | Loss: 0.00001995
Iteration 28/1000 | Loss: 0.00001993
Iteration 29/1000 | Loss: 0.00001992
Iteration 30/1000 | Loss: 0.00001992
Iteration 31/1000 | Loss: 0.00001991
Iteration 32/1000 | Loss: 0.00001991
Iteration 33/1000 | Loss: 0.00001990
Iteration 34/1000 | Loss: 0.00001990
Iteration 35/1000 | Loss: 0.00001989
Iteration 36/1000 | Loss: 0.00001989
Iteration 37/1000 | Loss: 0.00001988
Iteration 38/1000 | Loss: 0.00001988
Iteration 39/1000 | Loss: 0.00001987
Iteration 40/1000 | Loss: 0.00001987
Iteration 41/1000 | Loss: 0.00001987
Iteration 42/1000 | Loss: 0.00001986
Iteration 43/1000 | Loss: 0.00001986
Iteration 44/1000 | Loss: 0.00001985
Iteration 45/1000 | Loss: 0.00001985
Iteration 46/1000 | Loss: 0.00001985
Iteration 47/1000 | Loss: 0.00001984
Iteration 48/1000 | Loss: 0.00001984
Iteration 49/1000 | Loss: 0.00001984
Iteration 50/1000 | Loss: 0.00001984
Iteration 51/1000 | Loss: 0.00001983
Iteration 52/1000 | Loss: 0.00001983
Iteration 53/1000 | Loss: 0.00001983
Iteration 54/1000 | Loss: 0.00001983
Iteration 55/1000 | Loss: 0.00001983
Iteration 56/1000 | Loss: 0.00001982
Iteration 57/1000 | Loss: 0.00001982
Iteration 58/1000 | Loss: 0.00001982
Iteration 59/1000 | Loss: 0.00001982
Iteration 60/1000 | Loss: 0.00001982
Iteration 61/1000 | Loss: 0.00001982
Iteration 62/1000 | Loss: 0.00001981
Iteration 63/1000 | Loss: 0.00001981
Iteration 64/1000 | Loss: 0.00001981
Iteration 65/1000 | Loss: 0.00001980
Iteration 66/1000 | Loss: 0.00001980
Iteration 67/1000 | Loss: 0.00001980
Iteration 68/1000 | Loss: 0.00001980
Iteration 69/1000 | Loss: 0.00001980
Iteration 70/1000 | Loss: 0.00001979
Iteration 71/1000 | Loss: 0.00001979
Iteration 72/1000 | Loss: 0.00001979
Iteration 73/1000 | Loss: 0.00001979
Iteration 74/1000 | Loss: 0.00001978
Iteration 75/1000 | Loss: 0.00001978
Iteration 76/1000 | Loss: 0.00001978
Iteration 77/1000 | Loss: 0.00001978
Iteration 78/1000 | Loss: 0.00001978
Iteration 79/1000 | Loss: 0.00001977
Iteration 80/1000 | Loss: 0.00001977
Iteration 81/1000 | Loss: 0.00001977
Iteration 82/1000 | Loss: 0.00001977
Iteration 83/1000 | Loss: 0.00001977
Iteration 84/1000 | Loss: 0.00001977
Iteration 85/1000 | Loss: 0.00001977
Iteration 86/1000 | Loss: 0.00001977
Iteration 87/1000 | Loss: 0.00001976
Iteration 88/1000 | Loss: 0.00001976
Iteration 89/1000 | Loss: 0.00001976
Iteration 90/1000 | Loss: 0.00001976
Iteration 91/1000 | Loss: 0.00001976
Iteration 92/1000 | Loss: 0.00001976
Iteration 93/1000 | Loss: 0.00001976
Iteration 94/1000 | Loss: 0.00001976
Iteration 95/1000 | Loss: 0.00001975
Iteration 96/1000 | Loss: 0.00001975
Iteration 97/1000 | Loss: 0.00001975
Iteration 98/1000 | Loss: 0.00001975
Iteration 99/1000 | Loss: 0.00001975
Iteration 100/1000 | Loss: 0.00001975
Iteration 101/1000 | Loss: 0.00001975
Iteration 102/1000 | Loss: 0.00001974
Iteration 103/1000 | Loss: 0.00001974
Iteration 104/1000 | Loss: 0.00001974
Iteration 105/1000 | Loss: 0.00001974
Iteration 106/1000 | Loss: 0.00001974
Iteration 107/1000 | Loss: 0.00001974
Iteration 108/1000 | Loss: 0.00001974
Iteration 109/1000 | Loss: 0.00001973
Iteration 110/1000 | Loss: 0.00001973
Iteration 111/1000 | Loss: 0.00001973
Iteration 112/1000 | Loss: 0.00001973
Iteration 113/1000 | Loss: 0.00001973
Iteration 114/1000 | Loss: 0.00001973
Iteration 115/1000 | Loss: 0.00001973
Iteration 116/1000 | Loss: 0.00001973
Iteration 117/1000 | Loss: 0.00001973
Iteration 118/1000 | Loss: 0.00001973
Iteration 119/1000 | Loss: 0.00001973
Iteration 120/1000 | Loss: 0.00001973
Iteration 121/1000 | Loss: 0.00001972
Iteration 122/1000 | Loss: 0.00001972
Iteration 123/1000 | Loss: 0.00001972
Iteration 124/1000 | Loss: 0.00001972
Iteration 125/1000 | Loss: 0.00001972
Iteration 126/1000 | Loss: 0.00001972
Iteration 127/1000 | Loss: 0.00001972
Iteration 128/1000 | Loss: 0.00001972
Iteration 129/1000 | Loss: 0.00001972
Iteration 130/1000 | Loss: 0.00001972
Iteration 131/1000 | Loss: 0.00001972
Iteration 132/1000 | Loss: 0.00001972
Iteration 133/1000 | Loss: 0.00001972
Iteration 134/1000 | Loss: 0.00001972
Iteration 135/1000 | Loss: 0.00001972
Iteration 136/1000 | Loss: 0.00001971
Iteration 137/1000 | Loss: 0.00001971
Iteration 138/1000 | Loss: 0.00001971
Iteration 139/1000 | Loss: 0.00001971
Iteration 140/1000 | Loss: 0.00001971
Iteration 141/1000 | Loss: 0.00001971
Iteration 142/1000 | Loss: 0.00001971
Iteration 143/1000 | Loss: 0.00001971
Iteration 144/1000 | Loss: 0.00001971
Iteration 145/1000 | Loss: 0.00001971
Iteration 146/1000 | Loss: 0.00001971
Iteration 147/1000 | Loss: 0.00001971
Iteration 148/1000 | Loss: 0.00001971
Iteration 149/1000 | Loss: 0.00001971
Iteration 150/1000 | Loss: 0.00001971
Iteration 151/1000 | Loss: 0.00001971
Iteration 152/1000 | Loss: 0.00001970
Iteration 153/1000 | Loss: 0.00001970
Iteration 154/1000 | Loss: 0.00001970
Iteration 155/1000 | Loss: 0.00001970
Iteration 156/1000 | Loss: 0.00001970
Iteration 157/1000 | Loss: 0.00001970
Iteration 158/1000 | Loss: 0.00001969
Iteration 159/1000 | Loss: 0.00001969
Iteration 160/1000 | Loss: 0.00001969
Iteration 161/1000 | Loss: 0.00001969
Iteration 162/1000 | Loss: 0.00001969
Iteration 163/1000 | Loss: 0.00001969
Iteration 164/1000 | Loss: 0.00001969
Iteration 165/1000 | Loss: 0.00001969
Iteration 166/1000 | Loss: 0.00001969
Iteration 167/1000 | Loss: 0.00001969
Iteration 168/1000 | Loss: 0.00001969
Iteration 169/1000 | Loss: 0.00001969
Iteration 170/1000 | Loss: 0.00001969
Iteration 171/1000 | Loss: 0.00001968
Iteration 172/1000 | Loss: 0.00001968
Iteration 173/1000 | Loss: 0.00001968
Iteration 174/1000 | Loss: 0.00001968
Iteration 175/1000 | Loss: 0.00001968
Iteration 176/1000 | Loss: 0.00001968
Iteration 177/1000 | Loss: 0.00001968
Iteration 178/1000 | Loss: 0.00001968
Iteration 179/1000 | Loss: 0.00001968
Iteration 180/1000 | Loss: 0.00001967
Iteration 181/1000 | Loss: 0.00001967
Iteration 182/1000 | Loss: 0.00001967
Iteration 183/1000 | Loss: 0.00001967
Iteration 184/1000 | Loss: 0.00001967
Iteration 185/1000 | Loss: 0.00001967
Iteration 186/1000 | Loss: 0.00001967
Iteration 187/1000 | Loss: 0.00001967
Iteration 188/1000 | Loss: 0.00001967
Iteration 189/1000 | Loss: 0.00001967
Iteration 190/1000 | Loss: 0.00001967
Iteration 191/1000 | Loss: 0.00001967
Iteration 192/1000 | Loss: 0.00001967
Iteration 193/1000 | Loss: 0.00001967
Iteration 194/1000 | Loss: 0.00001967
Iteration 195/1000 | Loss: 0.00001967
Iteration 196/1000 | Loss: 0.00001967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.967157186300028e-05, 1.967157186300028e-05, 1.967157186300028e-05, 1.967157186300028e-05, 1.967157186300028e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.967157186300028e-05

Optimization complete. Final v2v error: 3.769632339477539 mm

Highest mean error: 4.365050792694092 mm for frame 23

Lowest mean error: 3.195148229598999 mm for frame 123

Saving results

Total time: 64.93757605552673
