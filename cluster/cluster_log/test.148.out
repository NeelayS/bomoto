Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=148, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8288-8343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437674
Iteration 2/25 | Loss: 0.00141901
Iteration 3/25 | Loss: 0.00129824
Iteration 4/25 | Loss: 0.00128653
Iteration 5/25 | Loss: 0.00128334
Iteration 6/25 | Loss: 0.00128248
Iteration 7/25 | Loss: 0.00128248
Iteration 8/25 | Loss: 0.00128248
Iteration 9/25 | Loss: 0.00128248
Iteration 10/25 | Loss: 0.00128248
Iteration 11/25 | Loss: 0.00128248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012824847362935543, 0.0012824847362935543, 0.0012824847362935543, 0.0012824847362935543, 0.0012824847362935543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012824847362935543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.96938658
Iteration 2/25 | Loss: 0.00086521
Iteration 3/25 | Loss: 0.00086518
Iteration 4/25 | Loss: 0.00086518
Iteration 5/25 | Loss: 0.00086518
Iteration 6/25 | Loss: 0.00086518
Iteration 7/25 | Loss: 0.00086518
Iteration 8/25 | Loss: 0.00086518
Iteration 9/25 | Loss: 0.00086518
Iteration 10/25 | Loss: 0.00086518
Iteration 11/25 | Loss: 0.00086518
Iteration 12/25 | Loss: 0.00086518
Iteration 13/25 | Loss: 0.00086518
Iteration 14/25 | Loss: 0.00086518
Iteration 15/25 | Loss: 0.00086518
Iteration 16/25 | Loss: 0.00086518
Iteration 17/25 | Loss: 0.00086518
Iteration 18/25 | Loss: 0.00086518
Iteration 19/25 | Loss: 0.00086518
Iteration 20/25 | Loss: 0.00086518
Iteration 21/25 | Loss: 0.00086518
Iteration 22/25 | Loss: 0.00086518
Iteration 23/25 | Loss: 0.00086518
Iteration 24/25 | Loss: 0.00086518
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008651795797049999, 0.0008651795797049999, 0.0008651795797049999, 0.0008651795797049999, 0.0008651795797049999]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008651795797049999

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086518
Iteration 2/1000 | Loss: 0.00003986
Iteration 3/1000 | Loss: 0.00002763
Iteration 4/1000 | Loss: 0.00002563
Iteration 5/1000 | Loss: 0.00002457
Iteration 6/1000 | Loss: 0.00002399
Iteration 7/1000 | Loss: 0.00002329
Iteration 8/1000 | Loss: 0.00002286
Iteration 9/1000 | Loss: 0.00002242
Iteration 10/1000 | Loss: 0.00002211
Iteration 11/1000 | Loss: 0.00002198
Iteration 12/1000 | Loss: 0.00002180
Iteration 13/1000 | Loss: 0.00002173
Iteration 14/1000 | Loss: 0.00002171
Iteration 15/1000 | Loss: 0.00002170
Iteration 16/1000 | Loss: 0.00002165
Iteration 17/1000 | Loss: 0.00002163
Iteration 18/1000 | Loss: 0.00002161
Iteration 19/1000 | Loss: 0.00002159
Iteration 20/1000 | Loss: 0.00002155
Iteration 21/1000 | Loss: 0.00002154
Iteration 22/1000 | Loss: 0.00002154
Iteration 23/1000 | Loss: 0.00002151
Iteration 24/1000 | Loss: 0.00002150
Iteration 25/1000 | Loss: 0.00002147
Iteration 26/1000 | Loss: 0.00002145
Iteration 27/1000 | Loss: 0.00002144
Iteration 28/1000 | Loss: 0.00002143
Iteration 29/1000 | Loss: 0.00002143
Iteration 30/1000 | Loss: 0.00002141
Iteration 31/1000 | Loss: 0.00002140
Iteration 32/1000 | Loss: 0.00002139
Iteration 33/1000 | Loss: 0.00002139
Iteration 34/1000 | Loss: 0.00002139
Iteration 35/1000 | Loss: 0.00002138
Iteration 36/1000 | Loss: 0.00002135
Iteration 37/1000 | Loss: 0.00002135
Iteration 38/1000 | Loss: 0.00002134
Iteration 39/1000 | Loss: 0.00002134
Iteration 40/1000 | Loss: 0.00002134
Iteration 41/1000 | Loss: 0.00002134
Iteration 42/1000 | Loss: 0.00002133
Iteration 43/1000 | Loss: 0.00002133
Iteration 44/1000 | Loss: 0.00002132
Iteration 45/1000 | Loss: 0.00002132
Iteration 46/1000 | Loss: 0.00002131
Iteration 47/1000 | Loss: 0.00002131
Iteration 48/1000 | Loss: 0.00002131
Iteration 49/1000 | Loss: 0.00002131
Iteration 50/1000 | Loss: 0.00002131
Iteration 51/1000 | Loss: 0.00002131
Iteration 52/1000 | Loss: 0.00002131
Iteration 53/1000 | Loss: 0.00002131
Iteration 54/1000 | Loss: 0.00002131
Iteration 55/1000 | Loss: 0.00002131
Iteration 56/1000 | Loss: 0.00002131
Iteration 57/1000 | Loss: 0.00002131
Iteration 58/1000 | Loss: 0.00002131
Iteration 59/1000 | Loss: 0.00002130
Iteration 60/1000 | Loss: 0.00002130
Iteration 61/1000 | Loss: 0.00002130
Iteration 62/1000 | Loss: 0.00002130
Iteration 63/1000 | Loss: 0.00002130
Iteration 64/1000 | Loss: 0.00002130
Iteration 65/1000 | Loss: 0.00002129
Iteration 66/1000 | Loss: 0.00002129
Iteration 67/1000 | Loss: 0.00002129
Iteration 68/1000 | Loss: 0.00002129
Iteration 69/1000 | Loss: 0.00002129
Iteration 70/1000 | Loss: 0.00002128
Iteration 71/1000 | Loss: 0.00002128
Iteration 72/1000 | Loss: 0.00002128
Iteration 73/1000 | Loss: 0.00002128
Iteration 74/1000 | Loss: 0.00002128
Iteration 75/1000 | Loss: 0.00002128
Iteration 76/1000 | Loss: 0.00002127
Iteration 77/1000 | Loss: 0.00002127
Iteration 78/1000 | Loss: 0.00002127
Iteration 79/1000 | Loss: 0.00002127
Iteration 80/1000 | Loss: 0.00002127
Iteration 81/1000 | Loss: 0.00002127
Iteration 82/1000 | Loss: 0.00002127
Iteration 83/1000 | Loss: 0.00002126
Iteration 84/1000 | Loss: 0.00002126
Iteration 85/1000 | Loss: 0.00002126
Iteration 86/1000 | Loss: 0.00002126
Iteration 87/1000 | Loss: 0.00002125
Iteration 88/1000 | Loss: 0.00002125
Iteration 89/1000 | Loss: 0.00002125
Iteration 90/1000 | Loss: 0.00002125
Iteration 91/1000 | Loss: 0.00002124
Iteration 92/1000 | Loss: 0.00002124
Iteration 93/1000 | Loss: 0.00002124
Iteration 94/1000 | Loss: 0.00002124
Iteration 95/1000 | Loss: 0.00002124
Iteration 96/1000 | Loss: 0.00002124
Iteration 97/1000 | Loss: 0.00002123
Iteration 98/1000 | Loss: 0.00002123
Iteration 99/1000 | Loss: 0.00002123
Iteration 100/1000 | Loss: 0.00002123
Iteration 101/1000 | Loss: 0.00002123
Iteration 102/1000 | Loss: 0.00002123
Iteration 103/1000 | Loss: 0.00002123
Iteration 104/1000 | Loss: 0.00002123
Iteration 105/1000 | Loss: 0.00002122
Iteration 106/1000 | Loss: 0.00002122
Iteration 107/1000 | Loss: 0.00002122
Iteration 108/1000 | Loss: 0.00002122
Iteration 109/1000 | Loss: 0.00002122
Iteration 110/1000 | Loss: 0.00002122
Iteration 111/1000 | Loss: 0.00002121
Iteration 112/1000 | Loss: 0.00002121
Iteration 113/1000 | Loss: 0.00002121
Iteration 114/1000 | Loss: 0.00002121
Iteration 115/1000 | Loss: 0.00002120
Iteration 116/1000 | Loss: 0.00002120
Iteration 117/1000 | Loss: 0.00002120
Iteration 118/1000 | Loss: 0.00002120
Iteration 119/1000 | Loss: 0.00002120
Iteration 120/1000 | Loss: 0.00002120
Iteration 121/1000 | Loss: 0.00002120
Iteration 122/1000 | Loss: 0.00002120
Iteration 123/1000 | Loss: 0.00002119
Iteration 124/1000 | Loss: 0.00002119
Iteration 125/1000 | Loss: 0.00002119
Iteration 126/1000 | Loss: 0.00002119
Iteration 127/1000 | Loss: 0.00002119
Iteration 128/1000 | Loss: 0.00002119
Iteration 129/1000 | Loss: 0.00002119
Iteration 130/1000 | Loss: 0.00002119
Iteration 131/1000 | Loss: 0.00002119
Iteration 132/1000 | Loss: 0.00002118
Iteration 133/1000 | Loss: 0.00002118
Iteration 134/1000 | Loss: 0.00002118
Iteration 135/1000 | Loss: 0.00002118
Iteration 136/1000 | Loss: 0.00002118
Iteration 137/1000 | Loss: 0.00002118
Iteration 138/1000 | Loss: 0.00002118
Iteration 139/1000 | Loss: 0.00002118
Iteration 140/1000 | Loss: 0.00002118
Iteration 141/1000 | Loss: 0.00002118
Iteration 142/1000 | Loss: 0.00002118
Iteration 143/1000 | Loss: 0.00002118
Iteration 144/1000 | Loss: 0.00002117
Iteration 145/1000 | Loss: 0.00002117
Iteration 146/1000 | Loss: 0.00002117
Iteration 147/1000 | Loss: 0.00002117
Iteration 148/1000 | Loss: 0.00002117
Iteration 149/1000 | Loss: 0.00002117
Iteration 150/1000 | Loss: 0.00002117
Iteration 151/1000 | Loss: 0.00002117
Iteration 152/1000 | Loss: 0.00002117
Iteration 153/1000 | Loss: 0.00002117
Iteration 154/1000 | Loss: 0.00002117
Iteration 155/1000 | Loss: 0.00002117
Iteration 156/1000 | Loss: 0.00002117
Iteration 157/1000 | Loss: 0.00002117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.1172476408537477e-05, 2.1172476408537477e-05, 2.1172476408537477e-05, 2.1172476408537477e-05, 2.1172476408537477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1172476408537477e-05

Optimization complete. Final v2v error: 3.895437240600586 mm

Highest mean error: 4.396584987640381 mm for frame 59

Lowest mean error: 3.426973342895508 mm for frame 4

Saving results

Total time: 40.06009006500244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736422
Iteration 2/25 | Loss: 0.00134380
Iteration 3/25 | Loss: 0.00126048
Iteration 4/25 | Loss: 0.00125123
Iteration 5/25 | Loss: 0.00124857
Iteration 6/25 | Loss: 0.00124809
Iteration 7/25 | Loss: 0.00124809
Iteration 8/25 | Loss: 0.00124809
Iteration 9/25 | Loss: 0.00124809
Iteration 10/25 | Loss: 0.00124809
Iteration 11/25 | Loss: 0.00124809
Iteration 12/25 | Loss: 0.00124809
Iteration 13/25 | Loss: 0.00124809
Iteration 14/25 | Loss: 0.00124809
Iteration 15/25 | Loss: 0.00124809
Iteration 16/25 | Loss: 0.00124809
Iteration 17/25 | Loss: 0.00124809
Iteration 18/25 | Loss: 0.00124809
Iteration 19/25 | Loss: 0.00124809
Iteration 20/25 | Loss: 0.00124809
Iteration 21/25 | Loss: 0.00124809
Iteration 22/25 | Loss: 0.00124809
Iteration 23/25 | Loss: 0.00124809
Iteration 24/25 | Loss: 0.00124809
Iteration 25/25 | Loss: 0.00124809

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34387600
Iteration 2/25 | Loss: 0.00085170
Iteration 3/25 | Loss: 0.00085162
Iteration 4/25 | Loss: 0.00085162
Iteration 5/25 | Loss: 0.00085162
Iteration 6/25 | Loss: 0.00085162
Iteration 7/25 | Loss: 0.00085162
Iteration 8/25 | Loss: 0.00085162
Iteration 9/25 | Loss: 0.00085162
Iteration 10/25 | Loss: 0.00085162
Iteration 11/25 | Loss: 0.00085162
Iteration 12/25 | Loss: 0.00085162
Iteration 13/25 | Loss: 0.00085162
Iteration 14/25 | Loss: 0.00085162
Iteration 15/25 | Loss: 0.00085162
Iteration 16/25 | Loss: 0.00085162
Iteration 17/25 | Loss: 0.00085162
Iteration 18/25 | Loss: 0.00085162
Iteration 19/25 | Loss: 0.00085162
Iteration 20/25 | Loss: 0.00085162
Iteration 21/25 | Loss: 0.00085162
Iteration 22/25 | Loss: 0.00085162
Iteration 23/25 | Loss: 0.00085162
Iteration 24/25 | Loss: 0.00085162
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008516223751939833, 0.0008516223751939833, 0.0008516223751939833, 0.0008516223751939833, 0.0008516223751939833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008516223751939833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085162
Iteration 2/1000 | Loss: 0.00003972
Iteration 3/1000 | Loss: 0.00003057
Iteration 4/1000 | Loss: 0.00002590
Iteration 5/1000 | Loss: 0.00002399
Iteration 6/1000 | Loss: 0.00002251
Iteration 7/1000 | Loss: 0.00002184
Iteration 8/1000 | Loss: 0.00002127
Iteration 9/1000 | Loss: 0.00002084
Iteration 10/1000 | Loss: 0.00002060
Iteration 11/1000 | Loss: 0.00002034
Iteration 12/1000 | Loss: 0.00002011
Iteration 13/1000 | Loss: 0.00001989
Iteration 14/1000 | Loss: 0.00001966
Iteration 15/1000 | Loss: 0.00001940
Iteration 16/1000 | Loss: 0.00001920
Iteration 17/1000 | Loss: 0.00001920
Iteration 18/1000 | Loss: 0.00001916
Iteration 19/1000 | Loss: 0.00001915
Iteration 20/1000 | Loss: 0.00001914
Iteration 21/1000 | Loss: 0.00001911
Iteration 22/1000 | Loss: 0.00001908
Iteration 23/1000 | Loss: 0.00001908
Iteration 24/1000 | Loss: 0.00001907
Iteration 25/1000 | Loss: 0.00001907
Iteration 26/1000 | Loss: 0.00001905
Iteration 27/1000 | Loss: 0.00001904
Iteration 28/1000 | Loss: 0.00001904
Iteration 29/1000 | Loss: 0.00001904
Iteration 30/1000 | Loss: 0.00001904
Iteration 31/1000 | Loss: 0.00001903
Iteration 32/1000 | Loss: 0.00001903
Iteration 33/1000 | Loss: 0.00001902
Iteration 34/1000 | Loss: 0.00001901
Iteration 35/1000 | Loss: 0.00001901
Iteration 36/1000 | Loss: 0.00001901
Iteration 37/1000 | Loss: 0.00001901
Iteration 38/1000 | Loss: 0.00001901
Iteration 39/1000 | Loss: 0.00001901
Iteration 40/1000 | Loss: 0.00001901
Iteration 41/1000 | Loss: 0.00001901
Iteration 42/1000 | Loss: 0.00001900
Iteration 43/1000 | Loss: 0.00001896
Iteration 44/1000 | Loss: 0.00001895
Iteration 45/1000 | Loss: 0.00001895
Iteration 46/1000 | Loss: 0.00001895
Iteration 47/1000 | Loss: 0.00001894
Iteration 48/1000 | Loss: 0.00001894
Iteration 49/1000 | Loss: 0.00001893
Iteration 50/1000 | Loss: 0.00001893
Iteration 51/1000 | Loss: 0.00001892
Iteration 52/1000 | Loss: 0.00001892
Iteration 53/1000 | Loss: 0.00001892
Iteration 54/1000 | Loss: 0.00001891
Iteration 55/1000 | Loss: 0.00001891
Iteration 56/1000 | Loss: 0.00001891
Iteration 57/1000 | Loss: 0.00001891
Iteration 58/1000 | Loss: 0.00001891
Iteration 59/1000 | Loss: 0.00001889
Iteration 60/1000 | Loss: 0.00001889
Iteration 61/1000 | Loss: 0.00001888
Iteration 62/1000 | Loss: 0.00001888
Iteration 63/1000 | Loss: 0.00001888
Iteration 64/1000 | Loss: 0.00001888
Iteration 65/1000 | Loss: 0.00001888
Iteration 66/1000 | Loss: 0.00001888
Iteration 67/1000 | Loss: 0.00001888
Iteration 68/1000 | Loss: 0.00001888
Iteration 69/1000 | Loss: 0.00001888
Iteration 70/1000 | Loss: 0.00001888
Iteration 71/1000 | Loss: 0.00001887
Iteration 72/1000 | Loss: 0.00001887
Iteration 73/1000 | Loss: 0.00001886
Iteration 74/1000 | Loss: 0.00001885
Iteration 75/1000 | Loss: 0.00001885
Iteration 76/1000 | Loss: 0.00001884
Iteration 77/1000 | Loss: 0.00001884
Iteration 78/1000 | Loss: 0.00001884
Iteration 79/1000 | Loss: 0.00001884
Iteration 80/1000 | Loss: 0.00001884
Iteration 81/1000 | Loss: 0.00001884
Iteration 82/1000 | Loss: 0.00001883
Iteration 83/1000 | Loss: 0.00001883
Iteration 84/1000 | Loss: 0.00001883
Iteration 85/1000 | Loss: 0.00001882
Iteration 86/1000 | Loss: 0.00001882
Iteration 87/1000 | Loss: 0.00001881
Iteration 88/1000 | Loss: 0.00001881
Iteration 89/1000 | Loss: 0.00001881
Iteration 90/1000 | Loss: 0.00001881
Iteration 91/1000 | Loss: 0.00001881
Iteration 92/1000 | Loss: 0.00001881
Iteration 93/1000 | Loss: 0.00001881
Iteration 94/1000 | Loss: 0.00001881
Iteration 95/1000 | Loss: 0.00001881
Iteration 96/1000 | Loss: 0.00001881
Iteration 97/1000 | Loss: 0.00001880
Iteration 98/1000 | Loss: 0.00001880
Iteration 99/1000 | Loss: 0.00001880
Iteration 100/1000 | Loss: 0.00001880
Iteration 101/1000 | Loss: 0.00001880
Iteration 102/1000 | Loss: 0.00001879
Iteration 103/1000 | Loss: 0.00001879
Iteration 104/1000 | Loss: 0.00001879
Iteration 105/1000 | Loss: 0.00001879
Iteration 106/1000 | Loss: 0.00001879
Iteration 107/1000 | Loss: 0.00001879
Iteration 108/1000 | Loss: 0.00001878
Iteration 109/1000 | Loss: 0.00001878
Iteration 110/1000 | Loss: 0.00001878
Iteration 111/1000 | Loss: 0.00001878
Iteration 112/1000 | Loss: 0.00001878
Iteration 113/1000 | Loss: 0.00001878
Iteration 114/1000 | Loss: 0.00001878
Iteration 115/1000 | Loss: 0.00001878
Iteration 116/1000 | Loss: 0.00001878
Iteration 117/1000 | Loss: 0.00001878
Iteration 118/1000 | Loss: 0.00001877
Iteration 119/1000 | Loss: 0.00001877
Iteration 120/1000 | Loss: 0.00001877
Iteration 121/1000 | Loss: 0.00001877
Iteration 122/1000 | Loss: 0.00001877
Iteration 123/1000 | Loss: 0.00001877
Iteration 124/1000 | Loss: 0.00001876
Iteration 125/1000 | Loss: 0.00001876
Iteration 126/1000 | Loss: 0.00001876
Iteration 127/1000 | Loss: 0.00001876
Iteration 128/1000 | Loss: 0.00001876
Iteration 129/1000 | Loss: 0.00001875
Iteration 130/1000 | Loss: 0.00001875
Iteration 131/1000 | Loss: 0.00001875
Iteration 132/1000 | Loss: 0.00001875
Iteration 133/1000 | Loss: 0.00001875
Iteration 134/1000 | Loss: 0.00001875
Iteration 135/1000 | Loss: 0.00001875
Iteration 136/1000 | Loss: 0.00001874
Iteration 137/1000 | Loss: 0.00001874
Iteration 138/1000 | Loss: 0.00001873
Iteration 139/1000 | Loss: 0.00001873
Iteration 140/1000 | Loss: 0.00001873
Iteration 141/1000 | Loss: 0.00001873
Iteration 142/1000 | Loss: 0.00001873
Iteration 143/1000 | Loss: 0.00001873
Iteration 144/1000 | Loss: 0.00001873
Iteration 145/1000 | Loss: 0.00001873
Iteration 146/1000 | Loss: 0.00001873
Iteration 147/1000 | Loss: 0.00001873
Iteration 148/1000 | Loss: 0.00001873
Iteration 149/1000 | Loss: 0.00001873
Iteration 150/1000 | Loss: 0.00001873
Iteration 151/1000 | Loss: 0.00001873
Iteration 152/1000 | Loss: 0.00001873
Iteration 153/1000 | Loss: 0.00001873
Iteration 154/1000 | Loss: 0.00001873
Iteration 155/1000 | Loss: 0.00001873
Iteration 156/1000 | Loss: 0.00001873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.872611574071925e-05, 1.872611574071925e-05, 1.872611574071925e-05, 1.872611574071925e-05, 1.872611574071925e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.872611574071925e-05

Optimization complete. Final v2v error: 3.625643253326416 mm

Highest mean error: 3.8557634353637695 mm for frame 41

Lowest mean error: 3.5034215450286865 mm for frame 97

Saving results

Total time: 43.11433386802673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01108963
Iteration 2/25 | Loss: 0.00202307
Iteration 3/25 | Loss: 0.00168956
Iteration 4/25 | Loss: 0.00148845
Iteration 5/25 | Loss: 0.00144901
Iteration 6/25 | Loss: 0.00142627
Iteration 7/25 | Loss: 0.00141804
Iteration 8/25 | Loss: 0.00141683
Iteration 9/25 | Loss: 0.00141788
Iteration 10/25 | Loss: 0.00141814
Iteration 11/25 | Loss: 0.00142023
Iteration 12/25 | Loss: 0.00140995
Iteration 13/25 | Loss: 0.00140722
Iteration 14/25 | Loss: 0.00141795
Iteration 15/25 | Loss: 0.00140075
Iteration 16/25 | Loss: 0.00139571
Iteration 17/25 | Loss: 0.00139451
Iteration 18/25 | Loss: 0.00139411
Iteration 19/25 | Loss: 0.00139402
Iteration 20/25 | Loss: 0.00139402
Iteration 21/25 | Loss: 0.00139402
Iteration 22/25 | Loss: 0.00139402
Iteration 23/25 | Loss: 0.00139402
Iteration 24/25 | Loss: 0.00139402
Iteration 25/25 | Loss: 0.00139401

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83937621
Iteration 2/25 | Loss: 0.00103497
Iteration 3/25 | Loss: 0.00103497
Iteration 4/25 | Loss: 0.00103497
Iteration 5/25 | Loss: 0.00103497
Iteration 6/25 | Loss: 0.00103497
Iteration 7/25 | Loss: 0.00103497
Iteration 8/25 | Loss: 0.00103497
Iteration 9/25 | Loss: 0.00103496
Iteration 10/25 | Loss: 0.00103496
Iteration 11/25 | Loss: 0.00103496
Iteration 12/25 | Loss: 0.00103496
Iteration 13/25 | Loss: 0.00103496
Iteration 14/25 | Loss: 0.00103496
Iteration 15/25 | Loss: 0.00103496
Iteration 16/25 | Loss: 0.00103496
Iteration 17/25 | Loss: 0.00103496
Iteration 18/25 | Loss: 0.00103496
Iteration 19/25 | Loss: 0.00103496
Iteration 20/25 | Loss: 0.00103496
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010349637595936656, 0.0010349637595936656, 0.0010349637595936656, 0.0010349637595936656, 0.0010349637595936656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010349637595936656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103496
Iteration 2/1000 | Loss: 0.00007586
Iteration 3/1000 | Loss: 0.00005548
Iteration 4/1000 | Loss: 0.00018587
Iteration 5/1000 | Loss: 0.00004698
Iteration 6/1000 | Loss: 0.00011445
Iteration 7/1000 | Loss: 0.00028096
Iteration 8/1000 | Loss: 0.00028369
Iteration 9/1000 | Loss: 0.00005589
Iteration 10/1000 | Loss: 0.00004531
Iteration 11/1000 | Loss: 0.00004279
Iteration 12/1000 | Loss: 0.00004100
Iteration 13/1000 | Loss: 0.00003983
Iteration 14/1000 | Loss: 0.00003919
Iteration 15/1000 | Loss: 0.00003873
Iteration 16/1000 | Loss: 0.00003832
Iteration 17/1000 | Loss: 0.00003802
Iteration 18/1000 | Loss: 0.00003771
Iteration 19/1000 | Loss: 0.00003751
Iteration 20/1000 | Loss: 0.00003730
Iteration 21/1000 | Loss: 0.00003710
Iteration 22/1000 | Loss: 0.00003691
Iteration 23/1000 | Loss: 0.00003678
Iteration 24/1000 | Loss: 0.00003666
Iteration 25/1000 | Loss: 0.00003660
Iteration 26/1000 | Loss: 0.00003656
Iteration 27/1000 | Loss: 0.00003656
Iteration 28/1000 | Loss: 0.00003646
Iteration 29/1000 | Loss: 0.00003640
Iteration 30/1000 | Loss: 0.00003639
Iteration 31/1000 | Loss: 0.00003639
Iteration 32/1000 | Loss: 0.00003639
Iteration 33/1000 | Loss: 0.00003639
Iteration 34/1000 | Loss: 0.00003638
Iteration 35/1000 | Loss: 0.00003638
Iteration 36/1000 | Loss: 0.00003638
Iteration 37/1000 | Loss: 0.00003638
Iteration 38/1000 | Loss: 0.00003638
Iteration 39/1000 | Loss: 0.00003638
Iteration 40/1000 | Loss: 0.00003628
Iteration 41/1000 | Loss: 0.00003620
Iteration 42/1000 | Loss: 0.00003619
Iteration 43/1000 | Loss: 0.00003612
Iteration 44/1000 | Loss: 0.00003607
Iteration 45/1000 | Loss: 0.00003607
Iteration 46/1000 | Loss: 0.00003607
Iteration 47/1000 | Loss: 0.00003606
Iteration 48/1000 | Loss: 0.00003606
Iteration 49/1000 | Loss: 0.00003605
Iteration 50/1000 | Loss: 0.00003605
Iteration 51/1000 | Loss: 0.00003605
Iteration 52/1000 | Loss: 0.00003604
Iteration 53/1000 | Loss: 0.00003604
Iteration 54/1000 | Loss: 0.00003604
Iteration 55/1000 | Loss: 0.00003604
Iteration 56/1000 | Loss: 0.00003603
Iteration 57/1000 | Loss: 0.00003603
Iteration 58/1000 | Loss: 0.00003603
Iteration 59/1000 | Loss: 0.00003603
Iteration 60/1000 | Loss: 0.00003603
Iteration 61/1000 | Loss: 0.00003603
Iteration 62/1000 | Loss: 0.00003603
Iteration 63/1000 | Loss: 0.00003602
Iteration 64/1000 | Loss: 0.00003602
Iteration 65/1000 | Loss: 0.00003602
Iteration 66/1000 | Loss: 0.00003602
Iteration 67/1000 | Loss: 0.00003601
Iteration 68/1000 | Loss: 0.00003601
Iteration 69/1000 | Loss: 0.00003601
Iteration 70/1000 | Loss: 0.00003601
Iteration 71/1000 | Loss: 0.00003600
Iteration 72/1000 | Loss: 0.00003600
Iteration 73/1000 | Loss: 0.00003599
Iteration 74/1000 | Loss: 0.00003597
Iteration 75/1000 | Loss: 0.00003597
Iteration 76/1000 | Loss: 0.00003597
Iteration 77/1000 | Loss: 0.00003597
Iteration 78/1000 | Loss: 0.00003597
Iteration 79/1000 | Loss: 0.00003597
Iteration 80/1000 | Loss: 0.00003597
Iteration 81/1000 | Loss: 0.00003597
Iteration 82/1000 | Loss: 0.00003597
Iteration 83/1000 | Loss: 0.00003597
Iteration 84/1000 | Loss: 0.00003597
Iteration 85/1000 | Loss: 0.00003596
Iteration 86/1000 | Loss: 0.00003596
Iteration 87/1000 | Loss: 0.00003596
Iteration 88/1000 | Loss: 0.00003596
Iteration 89/1000 | Loss: 0.00003595
Iteration 90/1000 | Loss: 0.00003595
Iteration 91/1000 | Loss: 0.00003594
Iteration 92/1000 | Loss: 0.00003594
Iteration 93/1000 | Loss: 0.00003594
Iteration 94/1000 | Loss: 0.00003594
Iteration 95/1000 | Loss: 0.00003594
Iteration 96/1000 | Loss: 0.00003594
Iteration 97/1000 | Loss: 0.00003594
Iteration 98/1000 | Loss: 0.00003594
Iteration 99/1000 | Loss: 0.00003594
Iteration 100/1000 | Loss: 0.00003593
Iteration 101/1000 | Loss: 0.00003593
Iteration 102/1000 | Loss: 0.00003592
Iteration 103/1000 | Loss: 0.00003592
Iteration 104/1000 | Loss: 0.00003591
Iteration 105/1000 | Loss: 0.00003590
Iteration 106/1000 | Loss: 0.00003589
Iteration 107/1000 | Loss: 0.00003589
Iteration 108/1000 | Loss: 0.00003589
Iteration 109/1000 | Loss: 0.00003589
Iteration 110/1000 | Loss: 0.00003589
Iteration 111/1000 | Loss: 0.00003589
Iteration 112/1000 | Loss: 0.00003589
Iteration 113/1000 | Loss: 0.00003589
Iteration 114/1000 | Loss: 0.00003588
Iteration 115/1000 | Loss: 0.00003588
Iteration 116/1000 | Loss: 0.00003588
Iteration 117/1000 | Loss: 0.00003588
Iteration 118/1000 | Loss: 0.00003588
Iteration 119/1000 | Loss: 0.00003588
Iteration 120/1000 | Loss: 0.00003588
Iteration 121/1000 | Loss: 0.00003588
Iteration 122/1000 | Loss: 0.00003588
Iteration 123/1000 | Loss: 0.00003588
Iteration 124/1000 | Loss: 0.00003587
Iteration 125/1000 | Loss: 0.00003587
Iteration 126/1000 | Loss: 0.00003587
Iteration 127/1000 | Loss: 0.00003587
Iteration 128/1000 | Loss: 0.00003587
Iteration 129/1000 | Loss: 0.00003586
Iteration 130/1000 | Loss: 0.00003586
Iteration 131/1000 | Loss: 0.00003586
Iteration 132/1000 | Loss: 0.00003586
Iteration 133/1000 | Loss: 0.00003586
Iteration 134/1000 | Loss: 0.00003586
Iteration 135/1000 | Loss: 0.00003586
Iteration 136/1000 | Loss: 0.00003586
Iteration 137/1000 | Loss: 0.00003586
Iteration 138/1000 | Loss: 0.00003586
Iteration 139/1000 | Loss: 0.00003586
Iteration 140/1000 | Loss: 0.00003586
Iteration 141/1000 | Loss: 0.00003586
Iteration 142/1000 | Loss: 0.00003586
Iteration 143/1000 | Loss: 0.00003586
Iteration 144/1000 | Loss: 0.00003586
Iteration 145/1000 | Loss: 0.00003586
Iteration 146/1000 | Loss: 0.00003586
Iteration 147/1000 | Loss: 0.00003586
Iteration 148/1000 | Loss: 0.00003585
Iteration 149/1000 | Loss: 0.00003585
Iteration 150/1000 | Loss: 0.00003585
Iteration 151/1000 | Loss: 0.00003585
Iteration 152/1000 | Loss: 0.00003585
Iteration 153/1000 | Loss: 0.00003585
Iteration 154/1000 | Loss: 0.00003585
Iteration 155/1000 | Loss: 0.00003585
Iteration 156/1000 | Loss: 0.00003585
Iteration 157/1000 | Loss: 0.00003585
Iteration 158/1000 | Loss: 0.00003585
Iteration 159/1000 | Loss: 0.00003585
Iteration 160/1000 | Loss: 0.00003585
Iteration 161/1000 | Loss: 0.00003585
Iteration 162/1000 | Loss: 0.00003585
Iteration 163/1000 | Loss: 0.00003585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [3.5853598092217e-05, 3.5853598092217e-05, 3.5853598092217e-05, 3.5853598092217e-05, 3.5853598092217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5853598092217e-05

Optimization complete. Final v2v error: 4.6541643142700195 mm

Highest mean error: 5.200537204742432 mm for frame 0

Lowest mean error: 3.972107172012329 mm for frame 138

Saving results

Total time: 95.73000645637512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00537822
Iteration 2/25 | Loss: 0.00152761
Iteration 3/25 | Loss: 0.00134496
Iteration 4/25 | Loss: 0.00132608
Iteration 5/25 | Loss: 0.00132293
Iteration 6/25 | Loss: 0.00132252
Iteration 7/25 | Loss: 0.00132252
Iteration 8/25 | Loss: 0.00132252
Iteration 9/25 | Loss: 0.00132252
Iteration 10/25 | Loss: 0.00132252
Iteration 11/25 | Loss: 0.00132252
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001322522759437561, 0.001322522759437561, 0.001322522759437561, 0.001322522759437561, 0.001322522759437561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001322522759437561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35545659
Iteration 2/25 | Loss: 0.00086337
Iteration 3/25 | Loss: 0.00086337
Iteration 4/25 | Loss: 0.00086337
Iteration 5/25 | Loss: 0.00086336
Iteration 6/25 | Loss: 0.00086336
Iteration 7/25 | Loss: 0.00086336
Iteration 8/25 | Loss: 0.00086336
Iteration 9/25 | Loss: 0.00086336
Iteration 10/25 | Loss: 0.00086336
Iteration 11/25 | Loss: 0.00086336
Iteration 12/25 | Loss: 0.00086336
Iteration 13/25 | Loss: 0.00086336
Iteration 14/25 | Loss: 0.00086336
Iteration 15/25 | Loss: 0.00086336
Iteration 16/25 | Loss: 0.00086336
Iteration 17/25 | Loss: 0.00086336
Iteration 18/25 | Loss: 0.00086336
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008633631514385343, 0.0008633631514385343, 0.0008633631514385343, 0.0008633631514385343, 0.0008633631514385343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008633631514385343

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086336
Iteration 2/1000 | Loss: 0.00004108
Iteration 3/1000 | Loss: 0.00002858
Iteration 4/1000 | Loss: 0.00002547
Iteration 5/1000 | Loss: 0.00002446
Iteration 6/1000 | Loss: 0.00002395
Iteration 7/1000 | Loss: 0.00002351
Iteration 8/1000 | Loss: 0.00002294
Iteration 9/1000 | Loss: 0.00002253
Iteration 10/1000 | Loss: 0.00002221
Iteration 11/1000 | Loss: 0.00002202
Iteration 12/1000 | Loss: 0.00002178
Iteration 13/1000 | Loss: 0.00002160
Iteration 14/1000 | Loss: 0.00002154
Iteration 15/1000 | Loss: 0.00002137
Iteration 16/1000 | Loss: 0.00002135
Iteration 17/1000 | Loss: 0.00002124
Iteration 18/1000 | Loss: 0.00002122
Iteration 19/1000 | Loss: 0.00002119
Iteration 20/1000 | Loss: 0.00002117
Iteration 21/1000 | Loss: 0.00002116
Iteration 22/1000 | Loss: 0.00002116
Iteration 23/1000 | Loss: 0.00002115
Iteration 24/1000 | Loss: 0.00002115
Iteration 25/1000 | Loss: 0.00002115
Iteration 26/1000 | Loss: 0.00002114
Iteration 27/1000 | Loss: 0.00002114
Iteration 28/1000 | Loss: 0.00002113
Iteration 29/1000 | Loss: 0.00002113
Iteration 30/1000 | Loss: 0.00002113
Iteration 31/1000 | Loss: 0.00002113
Iteration 32/1000 | Loss: 0.00002113
Iteration 33/1000 | Loss: 0.00002113
Iteration 34/1000 | Loss: 0.00002113
Iteration 35/1000 | Loss: 0.00002112
Iteration 36/1000 | Loss: 0.00002112
Iteration 37/1000 | Loss: 0.00002112
Iteration 38/1000 | Loss: 0.00002111
Iteration 39/1000 | Loss: 0.00002110
Iteration 40/1000 | Loss: 0.00002110
Iteration 41/1000 | Loss: 0.00002109
Iteration 42/1000 | Loss: 0.00002109
Iteration 43/1000 | Loss: 0.00002108
Iteration 44/1000 | Loss: 0.00002108
Iteration 45/1000 | Loss: 0.00002108
Iteration 46/1000 | Loss: 0.00002108
Iteration 47/1000 | Loss: 0.00002108
Iteration 48/1000 | Loss: 0.00002107
Iteration 49/1000 | Loss: 0.00002107
Iteration 50/1000 | Loss: 0.00002107
Iteration 51/1000 | Loss: 0.00002106
Iteration 52/1000 | Loss: 0.00002105
Iteration 53/1000 | Loss: 0.00002105
Iteration 54/1000 | Loss: 0.00002105
Iteration 55/1000 | Loss: 0.00002105
Iteration 56/1000 | Loss: 0.00002105
Iteration 57/1000 | Loss: 0.00002105
Iteration 58/1000 | Loss: 0.00002105
Iteration 59/1000 | Loss: 0.00002104
Iteration 60/1000 | Loss: 0.00002104
Iteration 61/1000 | Loss: 0.00002104
Iteration 62/1000 | Loss: 0.00002104
Iteration 63/1000 | Loss: 0.00002104
Iteration 64/1000 | Loss: 0.00002104
Iteration 65/1000 | Loss: 0.00002104
Iteration 66/1000 | Loss: 0.00002103
Iteration 67/1000 | Loss: 0.00002103
Iteration 68/1000 | Loss: 0.00002102
Iteration 69/1000 | Loss: 0.00002102
Iteration 70/1000 | Loss: 0.00002102
Iteration 71/1000 | Loss: 0.00002101
Iteration 72/1000 | Loss: 0.00002101
Iteration 73/1000 | Loss: 0.00002101
Iteration 74/1000 | Loss: 0.00002101
Iteration 75/1000 | Loss: 0.00002101
Iteration 76/1000 | Loss: 0.00002100
Iteration 77/1000 | Loss: 0.00002099
Iteration 78/1000 | Loss: 0.00002099
Iteration 79/1000 | Loss: 0.00002099
Iteration 80/1000 | Loss: 0.00002099
Iteration 81/1000 | Loss: 0.00002098
Iteration 82/1000 | Loss: 0.00002098
Iteration 83/1000 | Loss: 0.00002098
Iteration 84/1000 | Loss: 0.00002098
Iteration 85/1000 | Loss: 0.00002097
Iteration 86/1000 | Loss: 0.00002097
Iteration 87/1000 | Loss: 0.00002097
Iteration 88/1000 | Loss: 0.00002097
Iteration 89/1000 | Loss: 0.00002096
Iteration 90/1000 | Loss: 0.00002096
Iteration 91/1000 | Loss: 0.00002096
Iteration 92/1000 | Loss: 0.00002096
Iteration 93/1000 | Loss: 0.00002095
Iteration 94/1000 | Loss: 0.00002095
Iteration 95/1000 | Loss: 0.00002095
Iteration 96/1000 | Loss: 0.00002095
Iteration 97/1000 | Loss: 0.00002095
Iteration 98/1000 | Loss: 0.00002094
Iteration 99/1000 | Loss: 0.00002094
Iteration 100/1000 | Loss: 0.00002094
Iteration 101/1000 | Loss: 0.00002094
Iteration 102/1000 | Loss: 0.00002094
Iteration 103/1000 | Loss: 0.00002093
Iteration 104/1000 | Loss: 0.00002093
Iteration 105/1000 | Loss: 0.00002093
Iteration 106/1000 | Loss: 0.00002093
Iteration 107/1000 | Loss: 0.00002093
Iteration 108/1000 | Loss: 0.00002093
Iteration 109/1000 | Loss: 0.00002093
Iteration 110/1000 | Loss: 0.00002093
Iteration 111/1000 | Loss: 0.00002093
Iteration 112/1000 | Loss: 0.00002092
Iteration 113/1000 | Loss: 0.00002092
Iteration 114/1000 | Loss: 0.00002092
Iteration 115/1000 | Loss: 0.00002092
Iteration 116/1000 | Loss: 0.00002092
Iteration 117/1000 | Loss: 0.00002092
Iteration 118/1000 | Loss: 0.00002091
Iteration 119/1000 | Loss: 0.00002091
Iteration 120/1000 | Loss: 0.00002091
Iteration 121/1000 | Loss: 0.00002091
Iteration 122/1000 | Loss: 0.00002091
Iteration 123/1000 | Loss: 0.00002091
Iteration 124/1000 | Loss: 0.00002091
Iteration 125/1000 | Loss: 0.00002091
Iteration 126/1000 | Loss: 0.00002091
Iteration 127/1000 | Loss: 0.00002091
Iteration 128/1000 | Loss: 0.00002091
Iteration 129/1000 | Loss: 0.00002091
Iteration 130/1000 | Loss: 0.00002091
Iteration 131/1000 | Loss: 0.00002090
Iteration 132/1000 | Loss: 0.00002090
Iteration 133/1000 | Loss: 0.00002090
Iteration 134/1000 | Loss: 0.00002090
Iteration 135/1000 | Loss: 0.00002090
Iteration 136/1000 | Loss: 0.00002090
Iteration 137/1000 | Loss: 0.00002090
Iteration 138/1000 | Loss: 0.00002090
Iteration 139/1000 | Loss: 0.00002090
Iteration 140/1000 | Loss: 0.00002090
Iteration 141/1000 | Loss: 0.00002090
Iteration 142/1000 | Loss: 0.00002090
Iteration 143/1000 | Loss: 0.00002090
Iteration 144/1000 | Loss: 0.00002090
Iteration 145/1000 | Loss: 0.00002090
Iteration 146/1000 | Loss: 0.00002090
Iteration 147/1000 | Loss: 0.00002090
Iteration 148/1000 | Loss: 0.00002090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.0895819034194574e-05, 2.0895819034194574e-05, 2.0895819034194574e-05, 2.0895819034194574e-05, 2.0895819034194574e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0895819034194574e-05

Optimization complete. Final v2v error: 3.836426019668579 mm

Highest mean error: 4.055140018463135 mm for frame 90

Lowest mean error: 3.3932347297668457 mm for frame 13

Saving results

Total time: 38.6523323059082
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021271
Iteration 2/25 | Loss: 0.00249809
Iteration 3/25 | Loss: 0.00209336
Iteration 4/25 | Loss: 0.00196363
Iteration 5/25 | Loss: 0.00206077
Iteration 6/25 | Loss: 0.00196055
Iteration 7/25 | Loss: 0.00171826
Iteration 8/25 | Loss: 0.00157346
Iteration 9/25 | Loss: 0.00151807
Iteration 10/25 | Loss: 0.00149408
Iteration 11/25 | Loss: 0.00148409
Iteration 12/25 | Loss: 0.00145738
Iteration 13/25 | Loss: 0.00145174
Iteration 14/25 | Loss: 0.00145700
Iteration 15/25 | Loss: 0.00145778
Iteration 16/25 | Loss: 0.00143833
Iteration 17/25 | Loss: 0.00143904
Iteration 18/25 | Loss: 0.00143170
Iteration 19/25 | Loss: 0.00143369
Iteration 20/25 | Loss: 0.00143226
Iteration 21/25 | Loss: 0.00143199
Iteration 22/25 | Loss: 0.00142574
Iteration 23/25 | Loss: 0.00142919
Iteration 24/25 | Loss: 0.00142062
Iteration 25/25 | Loss: 0.00142183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33872890
Iteration 2/25 | Loss: 0.00301587
Iteration 3/25 | Loss: 0.00183516
Iteration 4/25 | Loss: 0.00183516
Iteration 5/25 | Loss: 0.00183516
Iteration 6/25 | Loss: 0.00183516
Iteration 7/25 | Loss: 0.00183516
Iteration 8/25 | Loss: 0.00183516
Iteration 9/25 | Loss: 0.00183516
Iteration 10/25 | Loss: 0.00183516
Iteration 11/25 | Loss: 0.00183516
Iteration 12/25 | Loss: 0.00183516
Iteration 13/25 | Loss: 0.00183516
Iteration 14/25 | Loss: 0.00183516
Iteration 15/25 | Loss: 0.00183516
Iteration 16/25 | Loss: 0.00183516
Iteration 17/25 | Loss: 0.00183516
Iteration 18/25 | Loss: 0.00183516
Iteration 19/25 | Loss: 0.00183516
Iteration 20/25 | Loss: 0.00183516
Iteration 21/25 | Loss: 0.00183516
Iteration 22/25 | Loss: 0.00183516
Iteration 23/25 | Loss: 0.00183516
Iteration 24/25 | Loss: 0.00183516
Iteration 25/25 | Loss: 0.00183516

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183516
Iteration 2/1000 | Loss: 0.00099553
Iteration 3/1000 | Loss: 0.00055240
Iteration 4/1000 | Loss: 0.00027261
Iteration 5/1000 | Loss: 0.00017398
Iteration 6/1000 | Loss: 0.00030716
Iteration 7/1000 | Loss: 0.00230994
Iteration 8/1000 | Loss: 0.00020927
Iteration 9/1000 | Loss: 0.00030118
Iteration 10/1000 | Loss: 0.00016964
Iteration 11/1000 | Loss: 0.00013723
Iteration 12/1000 | Loss: 0.00019931
Iteration 13/1000 | Loss: 0.00021380
Iteration 14/1000 | Loss: 0.00062536
Iteration 15/1000 | Loss: 0.00064781
Iteration 16/1000 | Loss: 0.00067780
Iteration 17/1000 | Loss: 0.00030178
Iteration 18/1000 | Loss: 0.00017239
Iteration 19/1000 | Loss: 0.00016021
Iteration 20/1000 | Loss: 0.00085050
Iteration 21/1000 | Loss: 0.00453299
Iteration 22/1000 | Loss: 0.02226586
Iteration 23/1000 | Loss: 0.00222010
Iteration 24/1000 | Loss: 0.00268112
Iteration 25/1000 | Loss: 0.00029137
Iteration 26/1000 | Loss: 0.00024375
Iteration 27/1000 | Loss: 0.00014607
Iteration 28/1000 | Loss: 0.00199414
Iteration 29/1000 | Loss: 0.00032740
Iteration 30/1000 | Loss: 0.00246673
Iteration 31/1000 | Loss: 0.00058999
Iteration 32/1000 | Loss: 0.00277162
Iteration 33/1000 | Loss: 0.00147887
Iteration 34/1000 | Loss: 0.00015064
Iteration 35/1000 | Loss: 0.00180092
Iteration 36/1000 | Loss: 0.00217677
Iteration 37/1000 | Loss: 0.00197347
Iteration 38/1000 | Loss: 0.00017028
Iteration 39/1000 | Loss: 0.00014869
Iteration 40/1000 | Loss: 0.00008790
Iteration 41/1000 | Loss: 0.00006872
Iteration 42/1000 | Loss: 0.00007695
Iteration 43/1000 | Loss: 0.00007060
Iteration 44/1000 | Loss: 0.00014401
Iteration 45/1000 | Loss: 0.00008132
Iteration 46/1000 | Loss: 0.00049671
Iteration 47/1000 | Loss: 0.00013019
Iteration 48/1000 | Loss: 0.00046901
Iteration 49/1000 | Loss: 0.00002639
Iteration 50/1000 | Loss: 0.00009998
Iteration 51/1000 | Loss: 0.00010805
Iteration 52/1000 | Loss: 0.00001597
Iteration 53/1000 | Loss: 0.00003484
Iteration 54/1000 | Loss: 0.00003914
Iteration 55/1000 | Loss: 0.00042135
Iteration 56/1000 | Loss: 0.00002917
Iteration 57/1000 | Loss: 0.00009385
Iteration 58/1000 | Loss: 0.00001484
Iteration 59/1000 | Loss: 0.00020987
Iteration 60/1000 | Loss: 0.00004952
Iteration 61/1000 | Loss: 0.00039934
Iteration 62/1000 | Loss: 0.00036520
Iteration 63/1000 | Loss: 0.00015159
Iteration 64/1000 | Loss: 0.00001217
Iteration 65/1000 | Loss: 0.00011682
Iteration 66/1000 | Loss: 0.00001331
Iteration 67/1000 | Loss: 0.00001788
Iteration 68/1000 | Loss: 0.00001102
Iteration 69/1000 | Loss: 0.00001064
Iteration 70/1000 | Loss: 0.00001748
Iteration 71/1000 | Loss: 0.00001021
Iteration 72/1000 | Loss: 0.00001018
Iteration 73/1000 | Loss: 0.00001005
Iteration 74/1000 | Loss: 0.00009441
Iteration 75/1000 | Loss: 0.00001214
Iteration 76/1000 | Loss: 0.00000990
Iteration 77/1000 | Loss: 0.00000985
Iteration 78/1000 | Loss: 0.00000982
Iteration 79/1000 | Loss: 0.00000981
Iteration 80/1000 | Loss: 0.00000981
Iteration 81/1000 | Loss: 0.00000980
Iteration 82/1000 | Loss: 0.00000980
Iteration 83/1000 | Loss: 0.00000980
Iteration 84/1000 | Loss: 0.00000979
Iteration 85/1000 | Loss: 0.00000979
Iteration 86/1000 | Loss: 0.00002375
Iteration 87/1000 | Loss: 0.00000979
Iteration 88/1000 | Loss: 0.00000977
Iteration 89/1000 | Loss: 0.00000975
Iteration 90/1000 | Loss: 0.00000975
Iteration 91/1000 | Loss: 0.00000974
Iteration 92/1000 | Loss: 0.00000974
Iteration 93/1000 | Loss: 0.00000973
Iteration 94/1000 | Loss: 0.00001711
Iteration 95/1000 | Loss: 0.00000975
Iteration 96/1000 | Loss: 0.00000972
Iteration 97/1000 | Loss: 0.00000972
Iteration 98/1000 | Loss: 0.00000971
Iteration 99/1000 | Loss: 0.00000971
Iteration 100/1000 | Loss: 0.00000970
Iteration 101/1000 | Loss: 0.00000969
Iteration 102/1000 | Loss: 0.00001833
Iteration 103/1000 | Loss: 0.00001518
Iteration 104/1000 | Loss: 0.00000962
Iteration 105/1000 | Loss: 0.00000962
Iteration 106/1000 | Loss: 0.00000962
Iteration 107/1000 | Loss: 0.00000962
Iteration 108/1000 | Loss: 0.00000961
Iteration 109/1000 | Loss: 0.00000961
Iteration 110/1000 | Loss: 0.00000961
Iteration 111/1000 | Loss: 0.00000961
Iteration 112/1000 | Loss: 0.00000961
Iteration 113/1000 | Loss: 0.00000961
Iteration 114/1000 | Loss: 0.00000961
Iteration 115/1000 | Loss: 0.00000961
Iteration 116/1000 | Loss: 0.00000961
Iteration 117/1000 | Loss: 0.00000961
Iteration 118/1000 | Loss: 0.00000961
Iteration 119/1000 | Loss: 0.00000961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [9.611922905605752e-06, 9.611922905605752e-06, 9.611922905605752e-06, 9.611922905605752e-06, 9.611922905605752e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.611922905605752e-06

Optimization complete. Final v2v error: 2.6606738567352295 mm

Highest mean error: 3.8242387771606445 mm for frame 95

Lowest mean error: 2.536882162094116 mm for frame 100

Saving results

Total time: 155.8202383518219
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410426
Iteration 2/25 | Loss: 0.00135674
Iteration 3/25 | Loss: 0.00124565
Iteration 4/25 | Loss: 0.00123320
Iteration 5/25 | Loss: 0.00122851
Iteration 6/25 | Loss: 0.00122733
Iteration 7/25 | Loss: 0.00122698
Iteration 8/25 | Loss: 0.00122691
Iteration 9/25 | Loss: 0.00122691
Iteration 10/25 | Loss: 0.00122691
Iteration 11/25 | Loss: 0.00122691
Iteration 12/25 | Loss: 0.00122691
Iteration 13/25 | Loss: 0.00122691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012269080616533756, 0.0012269080616533756, 0.0012269080616533756, 0.0012269080616533756, 0.0012269080616533756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012269080616533756

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33835542
Iteration 2/25 | Loss: 0.00118523
Iteration 3/25 | Loss: 0.00118523
Iteration 4/25 | Loss: 0.00118523
Iteration 5/25 | Loss: 0.00118523
Iteration 6/25 | Loss: 0.00118523
Iteration 7/25 | Loss: 0.00118522
Iteration 8/25 | Loss: 0.00118522
Iteration 9/25 | Loss: 0.00118522
Iteration 10/25 | Loss: 0.00118522
Iteration 11/25 | Loss: 0.00118522
Iteration 12/25 | Loss: 0.00118522
Iteration 13/25 | Loss: 0.00118522
Iteration 14/25 | Loss: 0.00118522
Iteration 15/25 | Loss: 0.00118522
Iteration 16/25 | Loss: 0.00118522
Iteration 17/25 | Loss: 0.00118522
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011852247407659888, 0.0011852247407659888, 0.0011852247407659888, 0.0011852247407659888, 0.0011852247407659888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011852247407659888

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118522
Iteration 2/1000 | Loss: 0.00004733
Iteration 3/1000 | Loss: 0.00003355
Iteration 4/1000 | Loss: 0.00002703
Iteration 5/1000 | Loss: 0.00002493
Iteration 6/1000 | Loss: 0.00002378
Iteration 7/1000 | Loss: 0.00002279
Iteration 8/1000 | Loss: 0.00002220
Iteration 9/1000 | Loss: 0.00002164
Iteration 10/1000 | Loss: 0.00002123
Iteration 11/1000 | Loss: 0.00002090
Iteration 12/1000 | Loss: 0.00002062
Iteration 13/1000 | Loss: 0.00002043
Iteration 14/1000 | Loss: 0.00002025
Iteration 15/1000 | Loss: 0.00002013
Iteration 16/1000 | Loss: 0.00002012
Iteration 17/1000 | Loss: 0.00002010
Iteration 18/1000 | Loss: 0.00002009
Iteration 19/1000 | Loss: 0.00002009
Iteration 20/1000 | Loss: 0.00002004
Iteration 21/1000 | Loss: 0.00002002
Iteration 22/1000 | Loss: 0.00002001
Iteration 23/1000 | Loss: 0.00002000
Iteration 24/1000 | Loss: 0.00001999
Iteration 25/1000 | Loss: 0.00001998
Iteration 26/1000 | Loss: 0.00001998
Iteration 27/1000 | Loss: 0.00001997
Iteration 28/1000 | Loss: 0.00001996
Iteration 29/1000 | Loss: 0.00001996
Iteration 30/1000 | Loss: 0.00001995
Iteration 31/1000 | Loss: 0.00001994
Iteration 32/1000 | Loss: 0.00001993
Iteration 33/1000 | Loss: 0.00001992
Iteration 34/1000 | Loss: 0.00001991
Iteration 35/1000 | Loss: 0.00001989
Iteration 36/1000 | Loss: 0.00001988
Iteration 37/1000 | Loss: 0.00001986
Iteration 38/1000 | Loss: 0.00001986
Iteration 39/1000 | Loss: 0.00001986
Iteration 40/1000 | Loss: 0.00001985
Iteration 41/1000 | Loss: 0.00001982
Iteration 42/1000 | Loss: 0.00001982
Iteration 43/1000 | Loss: 0.00001982
Iteration 44/1000 | Loss: 0.00001980
Iteration 45/1000 | Loss: 0.00001979
Iteration 46/1000 | Loss: 0.00001979
Iteration 47/1000 | Loss: 0.00001979
Iteration 48/1000 | Loss: 0.00001978
Iteration 49/1000 | Loss: 0.00001978
Iteration 50/1000 | Loss: 0.00001977
Iteration 51/1000 | Loss: 0.00001977
Iteration 52/1000 | Loss: 0.00001977
Iteration 53/1000 | Loss: 0.00001977
Iteration 54/1000 | Loss: 0.00001976
Iteration 55/1000 | Loss: 0.00001976
Iteration 56/1000 | Loss: 0.00001976
Iteration 57/1000 | Loss: 0.00001976
Iteration 58/1000 | Loss: 0.00001976
Iteration 59/1000 | Loss: 0.00001976
Iteration 60/1000 | Loss: 0.00001976
Iteration 61/1000 | Loss: 0.00001976
Iteration 62/1000 | Loss: 0.00001975
Iteration 63/1000 | Loss: 0.00001975
Iteration 64/1000 | Loss: 0.00001975
Iteration 65/1000 | Loss: 0.00001974
Iteration 66/1000 | Loss: 0.00001974
Iteration 67/1000 | Loss: 0.00001973
Iteration 68/1000 | Loss: 0.00001973
Iteration 69/1000 | Loss: 0.00001973
Iteration 70/1000 | Loss: 0.00001972
Iteration 71/1000 | Loss: 0.00001972
Iteration 72/1000 | Loss: 0.00001972
Iteration 73/1000 | Loss: 0.00001971
Iteration 74/1000 | Loss: 0.00001971
Iteration 75/1000 | Loss: 0.00001970
Iteration 76/1000 | Loss: 0.00001970
Iteration 77/1000 | Loss: 0.00001970
Iteration 78/1000 | Loss: 0.00001970
Iteration 79/1000 | Loss: 0.00001969
Iteration 80/1000 | Loss: 0.00001969
Iteration 81/1000 | Loss: 0.00001969
Iteration 82/1000 | Loss: 0.00001969
Iteration 83/1000 | Loss: 0.00001969
Iteration 84/1000 | Loss: 0.00001969
Iteration 85/1000 | Loss: 0.00001969
Iteration 86/1000 | Loss: 0.00001969
Iteration 87/1000 | Loss: 0.00001969
Iteration 88/1000 | Loss: 0.00001969
Iteration 89/1000 | Loss: 0.00001969
Iteration 90/1000 | Loss: 0.00001968
Iteration 91/1000 | Loss: 0.00001968
Iteration 92/1000 | Loss: 0.00001968
Iteration 93/1000 | Loss: 0.00001968
Iteration 94/1000 | Loss: 0.00001967
Iteration 95/1000 | Loss: 0.00001967
Iteration 96/1000 | Loss: 0.00001967
Iteration 97/1000 | Loss: 0.00001967
Iteration 98/1000 | Loss: 0.00001967
Iteration 99/1000 | Loss: 0.00001966
Iteration 100/1000 | Loss: 0.00001966
Iteration 101/1000 | Loss: 0.00001966
Iteration 102/1000 | Loss: 0.00001965
Iteration 103/1000 | Loss: 0.00001965
Iteration 104/1000 | Loss: 0.00001965
Iteration 105/1000 | Loss: 0.00001965
Iteration 106/1000 | Loss: 0.00001964
Iteration 107/1000 | Loss: 0.00001964
Iteration 108/1000 | Loss: 0.00001964
Iteration 109/1000 | Loss: 0.00001963
Iteration 110/1000 | Loss: 0.00001963
Iteration 111/1000 | Loss: 0.00001963
Iteration 112/1000 | Loss: 0.00001962
Iteration 113/1000 | Loss: 0.00001962
Iteration 114/1000 | Loss: 0.00001962
Iteration 115/1000 | Loss: 0.00001961
Iteration 116/1000 | Loss: 0.00001961
Iteration 117/1000 | Loss: 0.00001960
Iteration 118/1000 | Loss: 0.00001960
Iteration 119/1000 | Loss: 0.00001960
Iteration 120/1000 | Loss: 0.00001959
Iteration 121/1000 | Loss: 0.00001959
Iteration 122/1000 | Loss: 0.00001959
Iteration 123/1000 | Loss: 0.00001959
Iteration 124/1000 | Loss: 0.00001959
Iteration 125/1000 | Loss: 0.00001958
Iteration 126/1000 | Loss: 0.00001958
Iteration 127/1000 | Loss: 0.00001958
Iteration 128/1000 | Loss: 0.00001958
Iteration 129/1000 | Loss: 0.00001958
Iteration 130/1000 | Loss: 0.00001957
Iteration 131/1000 | Loss: 0.00001957
Iteration 132/1000 | Loss: 0.00001956
Iteration 133/1000 | Loss: 0.00001956
Iteration 134/1000 | Loss: 0.00001956
Iteration 135/1000 | Loss: 0.00001956
Iteration 136/1000 | Loss: 0.00001956
Iteration 137/1000 | Loss: 0.00001956
Iteration 138/1000 | Loss: 0.00001955
Iteration 139/1000 | Loss: 0.00001955
Iteration 140/1000 | Loss: 0.00001955
Iteration 141/1000 | Loss: 0.00002393
Iteration 142/1000 | Loss: 0.00002090
Iteration 143/1000 | Loss: 0.00001986
Iteration 144/1000 | Loss: 0.00001954
Iteration 145/1000 | Loss: 0.00001934
Iteration 146/1000 | Loss: 0.00001929
Iteration 147/1000 | Loss: 0.00001927
Iteration 148/1000 | Loss: 0.00001922
Iteration 149/1000 | Loss: 0.00001922
Iteration 150/1000 | Loss: 0.00001919
Iteration 151/1000 | Loss: 0.00001915
Iteration 152/1000 | Loss: 0.00001911
Iteration 153/1000 | Loss: 0.00001910
Iteration 154/1000 | Loss: 0.00001909
Iteration 155/1000 | Loss: 0.00001907
Iteration 156/1000 | Loss: 0.00001905
Iteration 157/1000 | Loss: 0.00001900
Iteration 158/1000 | Loss: 0.00001896
Iteration 159/1000 | Loss: 0.00001896
Iteration 160/1000 | Loss: 0.00001895
Iteration 161/1000 | Loss: 0.00001894
Iteration 162/1000 | Loss: 0.00001893
Iteration 163/1000 | Loss: 0.00001893
Iteration 164/1000 | Loss: 0.00001892
Iteration 165/1000 | Loss: 0.00001892
Iteration 166/1000 | Loss: 0.00001891
Iteration 167/1000 | Loss: 0.00001891
Iteration 168/1000 | Loss: 0.00001890
Iteration 169/1000 | Loss: 0.00001888
Iteration 170/1000 | Loss: 0.00001888
Iteration 171/1000 | Loss: 0.00001888
Iteration 172/1000 | Loss: 0.00001888
Iteration 173/1000 | Loss: 0.00001888
Iteration 174/1000 | Loss: 0.00001888
Iteration 175/1000 | Loss: 0.00001888
Iteration 176/1000 | Loss: 0.00001888
Iteration 177/1000 | Loss: 0.00001888
Iteration 178/1000 | Loss: 0.00001888
Iteration 179/1000 | Loss: 0.00001887
Iteration 180/1000 | Loss: 0.00001887
Iteration 181/1000 | Loss: 0.00001887
Iteration 182/1000 | Loss: 0.00001887
Iteration 183/1000 | Loss: 0.00001886
Iteration 184/1000 | Loss: 0.00001886
Iteration 185/1000 | Loss: 0.00001885
Iteration 186/1000 | Loss: 0.00001885
Iteration 187/1000 | Loss: 0.00001884
Iteration 188/1000 | Loss: 0.00001884
Iteration 189/1000 | Loss: 0.00001884
Iteration 190/1000 | Loss: 0.00001883
Iteration 191/1000 | Loss: 0.00001883
Iteration 192/1000 | Loss: 0.00001883
Iteration 193/1000 | Loss: 0.00001883
Iteration 194/1000 | Loss: 0.00001882
Iteration 195/1000 | Loss: 0.00001882
Iteration 196/1000 | Loss: 0.00001882
Iteration 197/1000 | Loss: 0.00001882
Iteration 198/1000 | Loss: 0.00001882
Iteration 199/1000 | Loss: 0.00001882
Iteration 200/1000 | Loss: 0.00001882
Iteration 201/1000 | Loss: 0.00001881
Iteration 202/1000 | Loss: 0.00001881
Iteration 203/1000 | Loss: 0.00001881
Iteration 204/1000 | Loss: 0.00001880
Iteration 205/1000 | Loss: 0.00001880
Iteration 206/1000 | Loss: 0.00001880
Iteration 207/1000 | Loss: 0.00001880
Iteration 208/1000 | Loss: 0.00001880
Iteration 209/1000 | Loss: 0.00001880
Iteration 210/1000 | Loss: 0.00001879
Iteration 211/1000 | Loss: 0.00001879
Iteration 212/1000 | Loss: 0.00001879
Iteration 213/1000 | Loss: 0.00001879
Iteration 214/1000 | Loss: 0.00001879
Iteration 215/1000 | Loss: 0.00001879
Iteration 216/1000 | Loss: 0.00001879
Iteration 217/1000 | Loss: 0.00001879
Iteration 218/1000 | Loss: 0.00001879
Iteration 219/1000 | Loss: 0.00001879
Iteration 220/1000 | Loss: 0.00001879
Iteration 221/1000 | Loss: 0.00001879
Iteration 222/1000 | Loss: 0.00001878
Iteration 223/1000 | Loss: 0.00001878
Iteration 224/1000 | Loss: 0.00001878
Iteration 225/1000 | Loss: 0.00001877
Iteration 226/1000 | Loss: 0.00001877
Iteration 227/1000 | Loss: 0.00001877
Iteration 228/1000 | Loss: 0.00001877
Iteration 229/1000 | Loss: 0.00001877
Iteration 230/1000 | Loss: 0.00001877
Iteration 231/1000 | Loss: 0.00001877
Iteration 232/1000 | Loss: 0.00001877
Iteration 233/1000 | Loss: 0.00001876
Iteration 234/1000 | Loss: 0.00001876
Iteration 235/1000 | Loss: 0.00001876
Iteration 236/1000 | Loss: 0.00001876
Iteration 237/1000 | Loss: 0.00001876
Iteration 238/1000 | Loss: 0.00001876
Iteration 239/1000 | Loss: 0.00001876
Iteration 240/1000 | Loss: 0.00001876
Iteration 241/1000 | Loss: 0.00001876
Iteration 242/1000 | Loss: 0.00001875
Iteration 243/1000 | Loss: 0.00001875
Iteration 244/1000 | Loss: 0.00001875
Iteration 245/1000 | Loss: 0.00001875
Iteration 246/1000 | Loss: 0.00001874
Iteration 247/1000 | Loss: 0.00001874
Iteration 248/1000 | Loss: 0.00001874
Iteration 249/1000 | Loss: 0.00001874
Iteration 250/1000 | Loss: 0.00001874
Iteration 251/1000 | Loss: 0.00001874
Iteration 252/1000 | Loss: 0.00001873
Iteration 253/1000 | Loss: 0.00001873
Iteration 254/1000 | Loss: 0.00001873
Iteration 255/1000 | Loss: 0.00001873
Iteration 256/1000 | Loss: 0.00001873
Iteration 257/1000 | Loss: 0.00001873
Iteration 258/1000 | Loss: 0.00001873
Iteration 259/1000 | Loss: 0.00001873
Iteration 260/1000 | Loss: 0.00001873
Iteration 261/1000 | Loss: 0.00001873
Iteration 262/1000 | Loss: 0.00001873
Iteration 263/1000 | Loss: 0.00001873
Iteration 264/1000 | Loss: 0.00001873
Iteration 265/1000 | Loss: 0.00001872
Iteration 266/1000 | Loss: 0.00001872
Iteration 267/1000 | Loss: 0.00001872
Iteration 268/1000 | Loss: 0.00001872
Iteration 269/1000 | Loss: 0.00001872
Iteration 270/1000 | Loss: 0.00001872
Iteration 271/1000 | Loss: 0.00001872
Iteration 272/1000 | Loss: 0.00001872
Iteration 273/1000 | Loss: 0.00001872
Iteration 274/1000 | Loss: 0.00001871
Iteration 275/1000 | Loss: 0.00001871
Iteration 276/1000 | Loss: 0.00001871
Iteration 277/1000 | Loss: 0.00001871
Iteration 278/1000 | Loss: 0.00001871
Iteration 279/1000 | Loss: 0.00001871
Iteration 280/1000 | Loss: 0.00001871
Iteration 281/1000 | Loss: 0.00001871
Iteration 282/1000 | Loss: 0.00001870
Iteration 283/1000 | Loss: 0.00001870
Iteration 284/1000 | Loss: 0.00001870
Iteration 285/1000 | Loss: 0.00001870
Iteration 286/1000 | Loss: 0.00001870
Iteration 287/1000 | Loss: 0.00001870
Iteration 288/1000 | Loss: 0.00001870
Iteration 289/1000 | Loss: 0.00001869
Iteration 290/1000 | Loss: 0.00001869
Iteration 291/1000 | Loss: 0.00001869
Iteration 292/1000 | Loss: 0.00001869
Iteration 293/1000 | Loss: 0.00001869
Iteration 294/1000 | Loss: 0.00001869
Iteration 295/1000 | Loss: 0.00001869
Iteration 296/1000 | Loss: 0.00001869
Iteration 297/1000 | Loss: 0.00001868
Iteration 298/1000 | Loss: 0.00001868
Iteration 299/1000 | Loss: 0.00001868
Iteration 300/1000 | Loss: 0.00001868
Iteration 301/1000 | Loss: 0.00001868
Iteration 302/1000 | Loss: 0.00001868
Iteration 303/1000 | Loss: 0.00001868
Iteration 304/1000 | Loss: 0.00001868
Iteration 305/1000 | Loss: 0.00001868
Iteration 306/1000 | Loss: 0.00001868
Iteration 307/1000 | Loss: 0.00001868
Iteration 308/1000 | Loss: 0.00001868
Iteration 309/1000 | Loss: 0.00001868
Iteration 310/1000 | Loss: 0.00001868
Iteration 311/1000 | Loss: 0.00001868
Iteration 312/1000 | Loss: 0.00001868
Iteration 313/1000 | Loss: 0.00001868
Iteration 314/1000 | Loss: 0.00001868
Iteration 315/1000 | Loss: 0.00001868
Iteration 316/1000 | Loss: 0.00001868
Iteration 317/1000 | Loss: 0.00001868
Iteration 318/1000 | Loss: 0.00001868
Iteration 319/1000 | Loss: 0.00001867
Iteration 320/1000 | Loss: 0.00001867
Iteration 321/1000 | Loss: 0.00001867
Iteration 322/1000 | Loss: 0.00001867
Iteration 323/1000 | Loss: 0.00001867
Iteration 324/1000 | Loss: 0.00001867
Iteration 325/1000 | Loss: 0.00001867
Iteration 326/1000 | Loss: 0.00001867
Iteration 327/1000 | Loss: 0.00001867
Iteration 328/1000 | Loss: 0.00001867
Iteration 329/1000 | Loss: 0.00001867
Iteration 330/1000 | Loss: 0.00001867
Iteration 331/1000 | Loss: 0.00001867
Iteration 332/1000 | Loss: 0.00001867
Iteration 333/1000 | Loss: 0.00001867
Iteration 334/1000 | Loss: 0.00001867
Iteration 335/1000 | Loss: 0.00001867
Iteration 336/1000 | Loss: 0.00001867
Iteration 337/1000 | Loss: 0.00001867
Iteration 338/1000 | Loss: 0.00001867
Iteration 339/1000 | Loss: 0.00001867
Iteration 340/1000 | Loss: 0.00001867
Iteration 341/1000 | Loss: 0.00001867
Iteration 342/1000 | Loss: 0.00001867
Iteration 343/1000 | Loss: 0.00001866
Iteration 344/1000 | Loss: 0.00001866
Iteration 345/1000 | Loss: 0.00001866
Iteration 346/1000 | Loss: 0.00001866
Iteration 347/1000 | Loss: 0.00001866
Iteration 348/1000 | Loss: 0.00001866
Iteration 349/1000 | Loss: 0.00001866
Iteration 350/1000 | Loss: 0.00001866
Iteration 351/1000 | Loss: 0.00001866
Iteration 352/1000 | Loss: 0.00001866
Iteration 353/1000 | Loss: 0.00001866
Iteration 354/1000 | Loss: 0.00001866
Iteration 355/1000 | Loss: 0.00001866
Iteration 356/1000 | Loss: 0.00001866
Iteration 357/1000 | Loss: 0.00001866
Iteration 358/1000 | Loss: 0.00001866
Iteration 359/1000 | Loss: 0.00001866
Iteration 360/1000 | Loss: 0.00001866
Iteration 361/1000 | Loss: 0.00001866
Iteration 362/1000 | Loss: 0.00001866
Iteration 363/1000 | Loss: 0.00001866
Iteration 364/1000 | Loss: 0.00001866
Iteration 365/1000 | Loss: 0.00001866
Iteration 366/1000 | Loss: 0.00001866
Iteration 367/1000 | Loss: 0.00001866
Iteration 368/1000 | Loss: 0.00001866
Iteration 369/1000 | Loss: 0.00001866
Iteration 370/1000 | Loss: 0.00001866
Iteration 371/1000 | Loss: 0.00001866
Iteration 372/1000 | Loss: 0.00001866
Iteration 373/1000 | Loss: 0.00001866
Iteration 374/1000 | Loss: 0.00001866
Iteration 375/1000 | Loss: 0.00001866
Iteration 376/1000 | Loss: 0.00001866
Iteration 377/1000 | Loss: 0.00001866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 377. Stopping optimization.
Last 5 losses: [1.8660317437024787e-05, 1.8660317437024787e-05, 1.8660317437024787e-05, 1.8660317437024787e-05, 1.8660317437024787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8660317437024787e-05

Optimization complete. Final v2v error: 3.562675714492798 mm

Highest mean error: 4.275461673736572 mm for frame 54

Lowest mean error: 2.964897871017456 mm for frame 172

Saving results

Total time: 69.13672518730164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992879
Iteration 2/25 | Loss: 0.00203779
Iteration 3/25 | Loss: 0.00151598
Iteration 4/25 | Loss: 0.00146527
Iteration 5/25 | Loss: 0.00148404
Iteration 6/25 | Loss: 0.00143474
Iteration 7/25 | Loss: 0.00140492
Iteration 8/25 | Loss: 0.00138072
Iteration 9/25 | Loss: 0.00138145
Iteration 10/25 | Loss: 0.00136019
Iteration 11/25 | Loss: 0.00135143
Iteration 12/25 | Loss: 0.00133811
Iteration 13/25 | Loss: 0.00133210
Iteration 14/25 | Loss: 0.00131837
Iteration 15/25 | Loss: 0.00131885
Iteration 16/25 | Loss: 0.00130135
Iteration 17/25 | Loss: 0.00129824
Iteration 18/25 | Loss: 0.00129217
Iteration 19/25 | Loss: 0.00128914
Iteration 20/25 | Loss: 0.00128358
Iteration 21/25 | Loss: 0.00128280
Iteration 22/25 | Loss: 0.00128031
Iteration 23/25 | Loss: 0.00127798
Iteration 24/25 | Loss: 0.00127676
Iteration 25/25 | Loss: 0.00127485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41151774
Iteration 2/25 | Loss: 0.00236240
Iteration 3/25 | Loss: 0.00219655
Iteration 4/25 | Loss: 0.00219655
Iteration 5/25 | Loss: 0.00219655
Iteration 6/25 | Loss: 0.00219655
Iteration 7/25 | Loss: 0.00219655
Iteration 8/25 | Loss: 0.00219655
Iteration 9/25 | Loss: 0.00219655
Iteration 10/25 | Loss: 0.00219655
Iteration 11/25 | Loss: 0.00219655
Iteration 12/25 | Loss: 0.00219655
Iteration 13/25 | Loss: 0.00219655
Iteration 14/25 | Loss: 0.00219655
Iteration 15/25 | Loss: 0.00219655
Iteration 16/25 | Loss: 0.00219655
Iteration 17/25 | Loss: 0.00219655
Iteration 18/25 | Loss: 0.00219655
Iteration 19/25 | Loss: 0.00219655
Iteration 20/25 | Loss: 0.00219655
Iteration 21/25 | Loss: 0.00219655
Iteration 22/25 | Loss: 0.00219655
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002196550602093339, 0.002196550602093339, 0.002196550602093339, 0.002196550602093339, 0.002196550602093339]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002196550602093339

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219655
Iteration 2/1000 | Loss: 0.00044682
Iteration 3/1000 | Loss: 0.00098612
Iteration 4/1000 | Loss: 0.00067611
Iteration 5/1000 | Loss: 0.00101668
Iteration 6/1000 | Loss: 0.00021064
Iteration 7/1000 | Loss: 0.00029939
Iteration 8/1000 | Loss: 0.00024637
Iteration 9/1000 | Loss: 0.00040570
Iteration 10/1000 | Loss: 0.00045832
Iteration 11/1000 | Loss: 0.00023762
Iteration 12/1000 | Loss: 0.00056741
Iteration 13/1000 | Loss: 0.00054202
Iteration 14/1000 | Loss: 0.00069775
Iteration 15/1000 | Loss: 0.00062682
Iteration 16/1000 | Loss: 0.00040052
Iteration 17/1000 | Loss: 0.00020608
Iteration 18/1000 | Loss: 0.00017684
Iteration 19/1000 | Loss: 0.00014992
Iteration 20/1000 | Loss: 0.00006196
Iteration 21/1000 | Loss: 0.00030646
Iteration 22/1000 | Loss: 0.00026865
Iteration 23/1000 | Loss: 0.00079028
Iteration 24/1000 | Loss: 0.00032237
Iteration 25/1000 | Loss: 0.00026259
Iteration 26/1000 | Loss: 0.00022474
Iteration 27/1000 | Loss: 0.00027346
Iteration 28/1000 | Loss: 0.00026212
Iteration 29/1000 | Loss: 0.00031832
Iteration 30/1000 | Loss: 0.00026167
Iteration 31/1000 | Loss: 0.00018134
Iteration 32/1000 | Loss: 0.00023109
Iteration 33/1000 | Loss: 0.00015338
Iteration 34/1000 | Loss: 0.00029143
Iteration 35/1000 | Loss: 0.00011108
Iteration 36/1000 | Loss: 0.00021213
Iteration 37/1000 | Loss: 0.00004745
Iteration 38/1000 | Loss: 0.00009484
Iteration 39/1000 | Loss: 0.00005374
Iteration 40/1000 | Loss: 0.00004824
Iteration 41/1000 | Loss: 0.00016380
Iteration 42/1000 | Loss: 0.00008883
Iteration 43/1000 | Loss: 0.00004679
Iteration 44/1000 | Loss: 0.00009031
Iteration 45/1000 | Loss: 0.00005555
Iteration 46/1000 | Loss: 0.00004746
Iteration 47/1000 | Loss: 0.00007476
Iteration 48/1000 | Loss: 0.00006012
Iteration 49/1000 | Loss: 0.00007358
Iteration 50/1000 | Loss: 0.00006697
Iteration 51/1000 | Loss: 0.00007408
Iteration 52/1000 | Loss: 0.00007039
Iteration 53/1000 | Loss: 0.00007509
Iteration 54/1000 | Loss: 0.00009742
Iteration 55/1000 | Loss: 0.00008116
Iteration 56/1000 | Loss: 0.00010526
Iteration 57/1000 | Loss: 0.00007931
Iteration 58/1000 | Loss: 0.00008332
Iteration 59/1000 | Loss: 0.00012712
Iteration 60/1000 | Loss: 0.00008301
Iteration 61/1000 | Loss: 0.00016658
Iteration 62/1000 | Loss: 0.00013459
Iteration 63/1000 | Loss: 0.00013430
Iteration 64/1000 | Loss: 0.00017760
Iteration 65/1000 | Loss: 0.00013752
Iteration 66/1000 | Loss: 0.00009774
Iteration 67/1000 | Loss: 0.00008270
Iteration 68/1000 | Loss: 0.00011707
Iteration 69/1000 | Loss: 0.00005285
Iteration 70/1000 | Loss: 0.00016977
Iteration 71/1000 | Loss: 0.00010017
Iteration 72/1000 | Loss: 0.00005352
Iteration 73/1000 | Loss: 0.00009730
Iteration 74/1000 | Loss: 0.00006863
Iteration 75/1000 | Loss: 0.00007982
Iteration 76/1000 | Loss: 0.00007498
Iteration 77/1000 | Loss: 0.00007370
Iteration 78/1000 | Loss: 0.00007602
Iteration 79/1000 | Loss: 0.00006789
Iteration 80/1000 | Loss: 0.00007436
Iteration 81/1000 | Loss: 0.00008776
Iteration 82/1000 | Loss: 0.00013603
Iteration 83/1000 | Loss: 0.00018348
Iteration 84/1000 | Loss: 0.00019851
Iteration 85/1000 | Loss: 0.00016850
Iteration 86/1000 | Loss: 0.00011876
Iteration 87/1000 | Loss: 0.00013148
Iteration 88/1000 | Loss: 0.00013685
Iteration 89/1000 | Loss: 0.00012955
Iteration 90/1000 | Loss: 0.00015147
Iteration 91/1000 | Loss: 0.00013720
Iteration 92/1000 | Loss: 0.00009055
Iteration 93/1000 | Loss: 0.00006759
Iteration 94/1000 | Loss: 0.00021091
Iteration 95/1000 | Loss: 0.00012553
Iteration 96/1000 | Loss: 0.00011323
Iteration 97/1000 | Loss: 0.00003250
Iteration 98/1000 | Loss: 0.00014417
Iteration 99/1000 | Loss: 0.00009422
Iteration 100/1000 | Loss: 0.00003157
Iteration 101/1000 | Loss: 0.00006466
Iteration 102/1000 | Loss: 0.00014290
Iteration 103/1000 | Loss: 0.00005447
Iteration 104/1000 | Loss: 0.00003951
Iteration 105/1000 | Loss: 0.00009198
Iteration 106/1000 | Loss: 0.00012687
Iteration 107/1000 | Loss: 0.00011838
Iteration 108/1000 | Loss: 0.00013665
Iteration 109/1000 | Loss: 0.00011674
Iteration 110/1000 | Loss: 0.00013004
Iteration 111/1000 | Loss: 0.00010224
Iteration 112/1000 | Loss: 0.00015141
Iteration 113/1000 | Loss: 0.00009618
Iteration 114/1000 | Loss: 0.00008819
Iteration 115/1000 | Loss: 0.00003462
Iteration 116/1000 | Loss: 0.00009417
Iteration 117/1000 | Loss: 0.00005210
Iteration 118/1000 | Loss: 0.00004677
Iteration 119/1000 | Loss: 0.00003616
Iteration 120/1000 | Loss: 0.00004352
Iteration 121/1000 | Loss: 0.00004129
Iteration 122/1000 | Loss: 0.00004192
Iteration 123/1000 | Loss: 0.00004074
Iteration 124/1000 | Loss: 0.00003743
Iteration 125/1000 | Loss: 0.00004218
Iteration 126/1000 | Loss: 0.00004170
Iteration 127/1000 | Loss: 0.00004377
Iteration 128/1000 | Loss: 0.00005217
Iteration 129/1000 | Loss: 0.00004180
Iteration 130/1000 | Loss: 0.00004693
Iteration 131/1000 | Loss: 0.00003640
Iteration 132/1000 | Loss: 0.00004289
Iteration 133/1000 | Loss: 0.00004139
Iteration 134/1000 | Loss: 0.00004125
Iteration 135/1000 | Loss: 0.00004064
Iteration 136/1000 | Loss: 0.00004138
Iteration 137/1000 | Loss: 0.00005102
Iteration 138/1000 | Loss: 0.00016108
Iteration 139/1000 | Loss: 0.00004942
Iteration 140/1000 | Loss: 0.00004250
Iteration 141/1000 | Loss: 0.00004001
Iteration 142/1000 | Loss: 0.00005782
Iteration 143/1000 | Loss: 0.00004051
Iteration 144/1000 | Loss: 0.00005466
Iteration 145/1000 | Loss: 0.00003992
Iteration 146/1000 | Loss: 0.00004383
Iteration 147/1000 | Loss: 0.00004771
Iteration 148/1000 | Loss: 0.00004671
Iteration 149/1000 | Loss: 0.00004083
Iteration 150/1000 | Loss: 0.00005596
Iteration 151/1000 | Loss: 0.00003973
Iteration 152/1000 | Loss: 0.00004247
Iteration 153/1000 | Loss: 0.00004414
Iteration 154/1000 | Loss: 0.00003702
Iteration 155/1000 | Loss: 0.00003995
Iteration 156/1000 | Loss: 0.00004412
Iteration 157/1000 | Loss: 0.00004041
Iteration 158/1000 | Loss: 0.00003221
Iteration 159/1000 | Loss: 0.00003867
Iteration 160/1000 | Loss: 0.00004331
Iteration 161/1000 | Loss: 0.00003978
Iteration 162/1000 | Loss: 0.00006269
Iteration 163/1000 | Loss: 0.00015728
Iteration 164/1000 | Loss: 0.00006157
Iteration 165/1000 | Loss: 0.00004235
Iteration 166/1000 | Loss: 0.00004842
Iteration 167/1000 | Loss: 0.00004093
Iteration 168/1000 | Loss: 0.00004731
Iteration 169/1000 | Loss: 0.00003979
Iteration 170/1000 | Loss: 0.00004739
Iteration 171/1000 | Loss: 0.00008528
Iteration 172/1000 | Loss: 0.00003793
Iteration 173/1000 | Loss: 0.00010263
Iteration 174/1000 | Loss: 0.00004317
Iteration 175/1000 | Loss: 0.00005916
Iteration 176/1000 | Loss: 0.00005469
Iteration 177/1000 | Loss: 0.00023239
Iteration 178/1000 | Loss: 0.00006130
Iteration 179/1000 | Loss: 0.00010834
Iteration 180/1000 | Loss: 0.00002686
Iteration 181/1000 | Loss: 0.00005254
Iteration 182/1000 | Loss: 0.00005003
Iteration 183/1000 | Loss: 0.00003693
Iteration 184/1000 | Loss: 0.00004591
Iteration 185/1000 | Loss: 0.00004063
Iteration 186/1000 | Loss: 0.00011908
Iteration 187/1000 | Loss: 0.00002742
Iteration 188/1000 | Loss: 0.00003442
Iteration 189/1000 | Loss: 0.00003332
Iteration 190/1000 | Loss: 0.00003205
Iteration 191/1000 | Loss: 0.00003316
Iteration 192/1000 | Loss: 0.00003146
Iteration 193/1000 | Loss: 0.00004399
Iteration 194/1000 | Loss: 0.00002858
Iteration 195/1000 | Loss: 0.00015441
Iteration 196/1000 | Loss: 0.00027629
Iteration 197/1000 | Loss: 0.00020198
Iteration 198/1000 | Loss: 0.00022474
Iteration 199/1000 | Loss: 0.00019956
Iteration 200/1000 | Loss: 0.00009575
Iteration 201/1000 | Loss: 0.00003134
Iteration 202/1000 | Loss: 0.00012568
Iteration 203/1000 | Loss: 0.00027563
Iteration 204/1000 | Loss: 0.00019431
Iteration 205/1000 | Loss: 0.00003001
Iteration 206/1000 | Loss: 0.00022571
Iteration 207/1000 | Loss: 0.00026474
Iteration 208/1000 | Loss: 0.00022225
Iteration 209/1000 | Loss: 0.00033770
Iteration 210/1000 | Loss: 0.00016340
Iteration 211/1000 | Loss: 0.00028815
Iteration 212/1000 | Loss: 0.00015529
Iteration 213/1000 | Loss: 0.00024310
Iteration 214/1000 | Loss: 0.00016583
Iteration 215/1000 | Loss: 0.00018966
Iteration 216/1000 | Loss: 0.00013824
Iteration 217/1000 | Loss: 0.00018372
Iteration 218/1000 | Loss: 0.00027176
Iteration 219/1000 | Loss: 0.00014414
Iteration 220/1000 | Loss: 0.00013165
Iteration 221/1000 | Loss: 0.00008112
Iteration 222/1000 | Loss: 0.00010118
Iteration 223/1000 | Loss: 0.00047402
Iteration 224/1000 | Loss: 0.00036508
Iteration 225/1000 | Loss: 0.00009120
Iteration 226/1000 | Loss: 0.00026732
Iteration 227/1000 | Loss: 0.00003303
Iteration 228/1000 | Loss: 0.00002636
Iteration 229/1000 | Loss: 0.00004575
Iteration 230/1000 | Loss: 0.00002203
Iteration 231/1000 | Loss: 0.00002106
Iteration 232/1000 | Loss: 0.00002044
Iteration 233/1000 | Loss: 0.00001999
Iteration 234/1000 | Loss: 0.00001961
Iteration 235/1000 | Loss: 0.00001916
Iteration 236/1000 | Loss: 0.00017502
Iteration 237/1000 | Loss: 0.00011944
Iteration 238/1000 | Loss: 0.00019002
Iteration 239/1000 | Loss: 0.00003047
Iteration 240/1000 | Loss: 0.00002258
Iteration 241/1000 | Loss: 0.00002026
Iteration 242/1000 | Loss: 0.00002556
Iteration 243/1000 | Loss: 0.00001921
Iteration 244/1000 | Loss: 0.00001774
Iteration 245/1000 | Loss: 0.00001726
Iteration 246/1000 | Loss: 0.00001687
Iteration 247/1000 | Loss: 0.00002682
Iteration 248/1000 | Loss: 0.00001784
Iteration 249/1000 | Loss: 0.00001642
Iteration 250/1000 | Loss: 0.00001637
Iteration 251/1000 | Loss: 0.00001636
Iteration 252/1000 | Loss: 0.00001630
Iteration 253/1000 | Loss: 0.00001626
Iteration 254/1000 | Loss: 0.00001625
Iteration 255/1000 | Loss: 0.00001625
Iteration 256/1000 | Loss: 0.00001624
Iteration 257/1000 | Loss: 0.00001623
Iteration 258/1000 | Loss: 0.00001620
Iteration 259/1000 | Loss: 0.00001617
Iteration 260/1000 | Loss: 0.00001616
Iteration 261/1000 | Loss: 0.00001616
Iteration 262/1000 | Loss: 0.00001615
Iteration 263/1000 | Loss: 0.00001615
Iteration 264/1000 | Loss: 0.00001614
Iteration 265/1000 | Loss: 0.00001613
Iteration 266/1000 | Loss: 0.00001612
Iteration 267/1000 | Loss: 0.00001612
Iteration 268/1000 | Loss: 0.00001611
Iteration 269/1000 | Loss: 0.00001611
Iteration 270/1000 | Loss: 0.00001611
Iteration 271/1000 | Loss: 0.00001609
Iteration 272/1000 | Loss: 0.00001608
Iteration 273/1000 | Loss: 0.00001605
Iteration 274/1000 | Loss: 0.00001605
Iteration 275/1000 | Loss: 0.00001605
Iteration 276/1000 | Loss: 0.00001604
Iteration 277/1000 | Loss: 0.00016350
Iteration 278/1000 | Loss: 0.00014360
Iteration 279/1000 | Loss: 0.00016001
Iteration 280/1000 | Loss: 0.00003547
Iteration 281/1000 | Loss: 0.00002165
Iteration 282/1000 | Loss: 0.00001891
Iteration 283/1000 | Loss: 0.00001798
Iteration 284/1000 | Loss: 0.00001738
Iteration 285/1000 | Loss: 0.00001688
Iteration 286/1000 | Loss: 0.00001662
Iteration 287/1000 | Loss: 0.00001639
Iteration 288/1000 | Loss: 0.00001629
Iteration 289/1000 | Loss: 0.00001621
Iteration 290/1000 | Loss: 0.00001603
Iteration 291/1000 | Loss: 0.00001601
Iteration 292/1000 | Loss: 0.00001593
Iteration 293/1000 | Loss: 0.00001582
Iteration 294/1000 | Loss: 0.00001579
Iteration 295/1000 | Loss: 0.00001578
Iteration 296/1000 | Loss: 0.00001575
Iteration 297/1000 | Loss: 0.00001573
Iteration 298/1000 | Loss: 0.00001572
Iteration 299/1000 | Loss: 0.00001571
Iteration 300/1000 | Loss: 0.00001570
Iteration 301/1000 | Loss: 0.00001569
Iteration 302/1000 | Loss: 0.00001568
Iteration 303/1000 | Loss: 0.00001568
Iteration 304/1000 | Loss: 0.00001568
Iteration 305/1000 | Loss: 0.00001567
Iteration 306/1000 | Loss: 0.00001566
Iteration 307/1000 | Loss: 0.00001566
Iteration 308/1000 | Loss: 0.00001565
Iteration 309/1000 | Loss: 0.00001565
Iteration 310/1000 | Loss: 0.00001563
Iteration 311/1000 | Loss: 0.00001563
Iteration 312/1000 | Loss: 0.00001562
Iteration 313/1000 | Loss: 0.00001562
Iteration 314/1000 | Loss: 0.00001562
Iteration 315/1000 | Loss: 0.00001561
Iteration 316/1000 | Loss: 0.00001561
Iteration 317/1000 | Loss: 0.00001559
Iteration 318/1000 | Loss: 0.00001559
Iteration 319/1000 | Loss: 0.00001558
Iteration 320/1000 | Loss: 0.00001557
Iteration 321/1000 | Loss: 0.00001557
Iteration 322/1000 | Loss: 0.00001556
Iteration 323/1000 | Loss: 0.00001556
Iteration 324/1000 | Loss: 0.00001556
Iteration 325/1000 | Loss: 0.00001555
Iteration 326/1000 | Loss: 0.00001555
Iteration 327/1000 | Loss: 0.00001555
Iteration 328/1000 | Loss: 0.00001555
Iteration 329/1000 | Loss: 0.00001555
Iteration 330/1000 | Loss: 0.00001554
Iteration 331/1000 | Loss: 0.00001554
Iteration 332/1000 | Loss: 0.00001554
Iteration 333/1000 | Loss: 0.00001554
Iteration 334/1000 | Loss: 0.00001554
Iteration 335/1000 | Loss: 0.00001554
Iteration 336/1000 | Loss: 0.00001554
Iteration 337/1000 | Loss: 0.00001554
Iteration 338/1000 | Loss: 0.00001554
Iteration 339/1000 | Loss: 0.00001554
Iteration 340/1000 | Loss: 0.00001554
Iteration 341/1000 | Loss: 0.00001554
Iteration 342/1000 | Loss: 0.00001553
Iteration 343/1000 | Loss: 0.00001553
Iteration 344/1000 | Loss: 0.00001553
Iteration 345/1000 | Loss: 0.00001553
Iteration 346/1000 | Loss: 0.00001553
Iteration 347/1000 | Loss: 0.00001553
Iteration 348/1000 | Loss: 0.00001553
Iteration 349/1000 | Loss: 0.00001553
Iteration 350/1000 | Loss: 0.00001552
Iteration 351/1000 | Loss: 0.00001552
Iteration 352/1000 | Loss: 0.00001552
Iteration 353/1000 | Loss: 0.00001552
Iteration 354/1000 | Loss: 0.00001552
Iteration 355/1000 | Loss: 0.00001551
Iteration 356/1000 | Loss: 0.00001551
Iteration 357/1000 | Loss: 0.00001551
Iteration 358/1000 | Loss: 0.00001551
Iteration 359/1000 | Loss: 0.00001551
Iteration 360/1000 | Loss: 0.00001551
Iteration 361/1000 | Loss: 0.00001550
Iteration 362/1000 | Loss: 0.00001550
Iteration 363/1000 | Loss: 0.00001550
Iteration 364/1000 | Loss: 0.00001550
Iteration 365/1000 | Loss: 0.00001550
Iteration 366/1000 | Loss: 0.00001550
Iteration 367/1000 | Loss: 0.00001550
Iteration 368/1000 | Loss: 0.00001550
Iteration 369/1000 | Loss: 0.00001550
Iteration 370/1000 | Loss: 0.00001550
Iteration 371/1000 | Loss: 0.00001550
Iteration 372/1000 | Loss: 0.00001550
Iteration 373/1000 | Loss: 0.00001550
Iteration 374/1000 | Loss: 0.00001549
Iteration 375/1000 | Loss: 0.00001549
Iteration 376/1000 | Loss: 0.00001549
Iteration 377/1000 | Loss: 0.00001549
Iteration 378/1000 | Loss: 0.00001549
Iteration 379/1000 | Loss: 0.00001549
Iteration 380/1000 | Loss: 0.00001549
Iteration 381/1000 | Loss: 0.00001549
Iteration 382/1000 | Loss: 0.00001549
Iteration 383/1000 | Loss: 0.00001549
Iteration 384/1000 | Loss: 0.00001549
Iteration 385/1000 | Loss: 0.00001549
Iteration 386/1000 | Loss: 0.00001549
Iteration 387/1000 | Loss: 0.00001549
Iteration 388/1000 | Loss: 0.00001549
Iteration 389/1000 | Loss: 0.00001549
Iteration 390/1000 | Loss: 0.00001548
Iteration 391/1000 | Loss: 0.00001548
Iteration 392/1000 | Loss: 0.00001548
Iteration 393/1000 | Loss: 0.00001548
Iteration 394/1000 | Loss: 0.00001548
Iteration 395/1000 | Loss: 0.00001548
Iteration 396/1000 | Loss: 0.00001548
Iteration 397/1000 | Loss: 0.00001548
Iteration 398/1000 | Loss: 0.00001548
Iteration 399/1000 | Loss: 0.00001548
Iteration 400/1000 | Loss: 0.00001548
Iteration 401/1000 | Loss: 0.00001548
Iteration 402/1000 | Loss: 0.00001548
Iteration 403/1000 | Loss: 0.00001548
Iteration 404/1000 | Loss: 0.00001548
Iteration 405/1000 | Loss: 0.00001548
Iteration 406/1000 | Loss: 0.00001548
Iteration 407/1000 | Loss: 0.00001548
Iteration 408/1000 | Loss: 0.00001548
Iteration 409/1000 | Loss: 0.00001547
Iteration 410/1000 | Loss: 0.00001547
Iteration 411/1000 | Loss: 0.00001547
Iteration 412/1000 | Loss: 0.00001547
Iteration 413/1000 | Loss: 0.00001547
Iteration 414/1000 | Loss: 0.00001547
Iteration 415/1000 | Loss: 0.00001547
Iteration 416/1000 | Loss: 0.00001547
Iteration 417/1000 | Loss: 0.00001547
Iteration 418/1000 | Loss: 0.00001547
Iteration 419/1000 | Loss: 0.00001547
Iteration 420/1000 | Loss: 0.00001547
Iteration 421/1000 | Loss: 0.00001547
Iteration 422/1000 | Loss: 0.00001547
Iteration 423/1000 | Loss: 0.00001547
Iteration 424/1000 | Loss: 0.00001547
Iteration 425/1000 | Loss: 0.00001547
Iteration 426/1000 | Loss: 0.00001547
Iteration 427/1000 | Loss: 0.00001547
Iteration 428/1000 | Loss: 0.00001547
Iteration 429/1000 | Loss: 0.00001547
Iteration 430/1000 | Loss: 0.00001547
Iteration 431/1000 | Loss: 0.00001547
Iteration 432/1000 | Loss: 0.00001546
Iteration 433/1000 | Loss: 0.00001546
Iteration 434/1000 | Loss: 0.00001546
Iteration 435/1000 | Loss: 0.00001546
Iteration 436/1000 | Loss: 0.00001546
Iteration 437/1000 | Loss: 0.00001546
Iteration 438/1000 | Loss: 0.00001546
Iteration 439/1000 | Loss: 0.00001546
Iteration 440/1000 | Loss: 0.00001546
Iteration 441/1000 | Loss: 0.00001546
Iteration 442/1000 | Loss: 0.00001546
Iteration 443/1000 | Loss: 0.00001546
Iteration 444/1000 | Loss: 0.00001546
Iteration 445/1000 | Loss: 0.00001546
Iteration 446/1000 | Loss: 0.00001546
Iteration 447/1000 | Loss: 0.00001546
Iteration 448/1000 | Loss: 0.00001546
Iteration 449/1000 | Loss: 0.00001546
Iteration 450/1000 | Loss: 0.00001546
Iteration 451/1000 | Loss: 0.00001546
Iteration 452/1000 | Loss: 0.00001546
Iteration 453/1000 | Loss: 0.00001546
Iteration 454/1000 | Loss: 0.00001546
Iteration 455/1000 | Loss: 0.00001546
Iteration 456/1000 | Loss: 0.00001546
Iteration 457/1000 | Loss: 0.00001546
Iteration 458/1000 | Loss: 0.00001546
Iteration 459/1000 | Loss: 0.00001546
Iteration 460/1000 | Loss: 0.00001546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 460. Stopping optimization.
Last 5 losses: [1.545675695524551e-05, 1.545675695524551e-05, 1.545675695524551e-05, 1.545675695524551e-05, 1.545675695524551e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.545675695524551e-05

Optimization complete. Final v2v error: 3.172114372253418 mm

Highest mean error: 10.837825775146484 mm for frame 30

Lowest mean error: 2.561868906021118 mm for frame 201

Saving results

Total time: 496.8743646144867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071927
Iteration 2/25 | Loss: 0.00207067
Iteration 3/25 | Loss: 0.00159292
Iteration 4/25 | Loss: 0.00151193
Iteration 5/25 | Loss: 0.00177016
Iteration 6/25 | Loss: 0.00159482
Iteration 7/25 | Loss: 0.00145656
Iteration 8/25 | Loss: 0.00135580
Iteration 9/25 | Loss: 0.00134115
Iteration 10/25 | Loss: 0.00133563
Iteration 11/25 | Loss: 0.00133322
Iteration 12/25 | Loss: 0.00133136
Iteration 13/25 | Loss: 0.00133156
Iteration 14/25 | Loss: 0.00132920
Iteration 15/25 | Loss: 0.00132850
Iteration 16/25 | Loss: 0.00132835
Iteration 17/25 | Loss: 0.00132829
Iteration 18/25 | Loss: 0.00132829
Iteration 19/25 | Loss: 0.00132828
Iteration 20/25 | Loss: 0.00132828
Iteration 21/25 | Loss: 0.00132828
Iteration 22/25 | Loss: 0.00132828
Iteration 23/25 | Loss: 0.00132828
Iteration 24/25 | Loss: 0.00132828
Iteration 25/25 | Loss: 0.00132828

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07203531
Iteration 2/25 | Loss: 0.00129473
Iteration 3/25 | Loss: 0.00129472
Iteration 4/25 | Loss: 0.00129472
Iteration 5/25 | Loss: 0.00129472
Iteration 6/25 | Loss: 0.00129472
Iteration 7/25 | Loss: 0.00129472
Iteration 8/25 | Loss: 0.00129472
Iteration 9/25 | Loss: 0.00129471
Iteration 10/25 | Loss: 0.00129471
Iteration 11/25 | Loss: 0.00129471
Iteration 12/25 | Loss: 0.00129471
Iteration 13/25 | Loss: 0.00129471
Iteration 14/25 | Loss: 0.00129471
Iteration 15/25 | Loss: 0.00129471
Iteration 16/25 | Loss: 0.00129471
Iteration 17/25 | Loss: 0.00129471
Iteration 18/25 | Loss: 0.00129471
Iteration 19/25 | Loss: 0.00129471
Iteration 20/25 | Loss: 0.00129471
Iteration 21/25 | Loss: 0.00129471
Iteration 22/25 | Loss: 0.00129471
Iteration 23/25 | Loss: 0.00129471
Iteration 24/25 | Loss: 0.00129471
Iteration 25/25 | Loss: 0.00129471

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129471
Iteration 2/1000 | Loss: 0.00007184
Iteration 3/1000 | Loss: 0.00004247
Iteration 4/1000 | Loss: 0.00003161
Iteration 5/1000 | Loss: 0.00002839
Iteration 6/1000 | Loss: 0.00002691
Iteration 7/1000 | Loss: 0.00002601
Iteration 8/1000 | Loss: 0.00002532
Iteration 9/1000 | Loss: 0.00002486
Iteration 10/1000 | Loss: 0.00002459
Iteration 11/1000 | Loss: 0.00002427
Iteration 12/1000 | Loss: 0.00002406
Iteration 13/1000 | Loss: 0.00002393
Iteration 14/1000 | Loss: 0.00002391
Iteration 15/1000 | Loss: 0.00002377
Iteration 16/1000 | Loss: 0.00002363
Iteration 17/1000 | Loss: 0.00002354
Iteration 18/1000 | Loss: 0.00002349
Iteration 19/1000 | Loss: 0.00002347
Iteration 20/1000 | Loss: 0.00002345
Iteration 21/1000 | Loss: 0.00002344
Iteration 22/1000 | Loss: 0.00002340
Iteration 23/1000 | Loss: 0.00002338
Iteration 24/1000 | Loss: 0.00002338
Iteration 25/1000 | Loss: 0.00002338
Iteration 26/1000 | Loss: 0.00002338
Iteration 27/1000 | Loss: 0.00002337
Iteration 28/1000 | Loss: 0.00002337
Iteration 29/1000 | Loss: 0.00002336
Iteration 30/1000 | Loss: 0.00002336
Iteration 31/1000 | Loss: 0.00002335
Iteration 32/1000 | Loss: 0.00002334
Iteration 33/1000 | Loss: 0.00002334
Iteration 34/1000 | Loss: 0.00002334
Iteration 35/1000 | Loss: 0.00002333
Iteration 36/1000 | Loss: 0.00002332
Iteration 37/1000 | Loss: 0.00002329
Iteration 38/1000 | Loss: 0.00002326
Iteration 39/1000 | Loss: 0.00002317
Iteration 40/1000 | Loss: 0.00002314
Iteration 41/1000 | Loss: 0.00002314
Iteration 42/1000 | Loss: 0.00002314
Iteration 43/1000 | Loss: 0.00002313
Iteration 44/1000 | Loss: 0.00002313
Iteration 45/1000 | Loss: 0.00002312
Iteration 46/1000 | Loss: 0.00002312
Iteration 47/1000 | Loss: 0.00002312
Iteration 48/1000 | Loss: 0.00002312
Iteration 49/1000 | Loss: 0.00002311
Iteration 50/1000 | Loss: 0.00002311
Iteration 51/1000 | Loss: 0.00002310
Iteration 52/1000 | Loss: 0.00002310
Iteration 53/1000 | Loss: 0.00002310
Iteration 54/1000 | Loss: 0.00002310
Iteration 55/1000 | Loss: 0.00002310
Iteration 56/1000 | Loss: 0.00002309
Iteration 57/1000 | Loss: 0.00002309
Iteration 58/1000 | Loss: 0.00002308
Iteration 59/1000 | Loss: 0.00002308
Iteration 60/1000 | Loss: 0.00002308
Iteration 61/1000 | Loss: 0.00002308
Iteration 62/1000 | Loss: 0.00002308
Iteration 63/1000 | Loss: 0.00002308
Iteration 64/1000 | Loss: 0.00002308
Iteration 65/1000 | Loss: 0.00002308
Iteration 66/1000 | Loss: 0.00002308
Iteration 67/1000 | Loss: 0.00002307
Iteration 68/1000 | Loss: 0.00002307
Iteration 69/1000 | Loss: 0.00002307
Iteration 70/1000 | Loss: 0.00002307
Iteration 71/1000 | Loss: 0.00002307
Iteration 72/1000 | Loss: 0.00002307
Iteration 73/1000 | Loss: 0.00002307
Iteration 74/1000 | Loss: 0.00002307
Iteration 75/1000 | Loss: 0.00002307
Iteration 76/1000 | Loss: 0.00002307
Iteration 77/1000 | Loss: 0.00002306
Iteration 78/1000 | Loss: 0.00002306
Iteration 79/1000 | Loss: 0.00002306
Iteration 80/1000 | Loss: 0.00002306
Iteration 81/1000 | Loss: 0.00002306
Iteration 82/1000 | Loss: 0.00002306
Iteration 83/1000 | Loss: 0.00002306
Iteration 84/1000 | Loss: 0.00002306
Iteration 85/1000 | Loss: 0.00002306
Iteration 86/1000 | Loss: 0.00002306
Iteration 87/1000 | Loss: 0.00002306
Iteration 88/1000 | Loss: 0.00002305
Iteration 89/1000 | Loss: 0.00002305
Iteration 90/1000 | Loss: 0.00002305
Iteration 91/1000 | Loss: 0.00002305
Iteration 92/1000 | Loss: 0.00002305
Iteration 93/1000 | Loss: 0.00002305
Iteration 94/1000 | Loss: 0.00002305
Iteration 95/1000 | Loss: 0.00002305
Iteration 96/1000 | Loss: 0.00002303
Iteration 97/1000 | Loss: 0.00002303
Iteration 98/1000 | Loss: 0.00002303
Iteration 99/1000 | Loss: 0.00002303
Iteration 100/1000 | Loss: 0.00002303
Iteration 101/1000 | Loss: 0.00002303
Iteration 102/1000 | Loss: 0.00002302
Iteration 103/1000 | Loss: 0.00002302
Iteration 104/1000 | Loss: 0.00002302
Iteration 105/1000 | Loss: 0.00002302
Iteration 106/1000 | Loss: 0.00002302
Iteration 107/1000 | Loss: 0.00002302
Iteration 108/1000 | Loss: 0.00002301
Iteration 109/1000 | Loss: 0.00002301
Iteration 110/1000 | Loss: 0.00002300
Iteration 111/1000 | Loss: 0.00002300
Iteration 112/1000 | Loss: 0.00002300
Iteration 113/1000 | Loss: 0.00002300
Iteration 114/1000 | Loss: 0.00002299
Iteration 115/1000 | Loss: 0.00002299
Iteration 116/1000 | Loss: 0.00002299
Iteration 117/1000 | Loss: 0.00002298
Iteration 118/1000 | Loss: 0.00002298
Iteration 119/1000 | Loss: 0.00002298
Iteration 120/1000 | Loss: 0.00002298
Iteration 121/1000 | Loss: 0.00002298
Iteration 122/1000 | Loss: 0.00002298
Iteration 123/1000 | Loss: 0.00002298
Iteration 124/1000 | Loss: 0.00002298
Iteration 125/1000 | Loss: 0.00002297
Iteration 126/1000 | Loss: 0.00002296
Iteration 127/1000 | Loss: 0.00002296
Iteration 128/1000 | Loss: 0.00002296
Iteration 129/1000 | Loss: 0.00002296
Iteration 130/1000 | Loss: 0.00002296
Iteration 131/1000 | Loss: 0.00002296
Iteration 132/1000 | Loss: 0.00002295
Iteration 133/1000 | Loss: 0.00002295
Iteration 134/1000 | Loss: 0.00002295
Iteration 135/1000 | Loss: 0.00002294
Iteration 136/1000 | Loss: 0.00002294
Iteration 137/1000 | Loss: 0.00002294
Iteration 138/1000 | Loss: 0.00002294
Iteration 139/1000 | Loss: 0.00002294
Iteration 140/1000 | Loss: 0.00002294
Iteration 141/1000 | Loss: 0.00002293
Iteration 142/1000 | Loss: 0.00002293
Iteration 143/1000 | Loss: 0.00002293
Iteration 144/1000 | Loss: 0.00002293
Iteration 145/1000 | Loss: 0.00002292
Iteration 146/1000 | Loss: 0.00002292
Iteration 147/1000 | Loss: 0.00002292
Iteration 148/1000 | Loss: 0.00002291
Iteration 149/1000 | Loss: 0.00002291
Iteration 150/1000 | Loss: 0.00002291
Iteration 151/1000 | Loss: 0.00002291
Iteration 152/1000 | Loss: 0.00002290
Iteration 153/1000 | Loss: 0.00002290
Iteration 154/1000 | Loss: 0.00002290
Iteration 155/1000 | Loss: 0.00002290
Iteration 156/1000 | Loss: 0.00002289
Iteration 157/1000 | Loss: 0.00002289
Iteration 158/1000 | Loss: 0.00002289
Iteration 159/1000 | Loss: 0.00002289
Iteration 160/1000 | Loss: 0.00002288
Iteration 161/1000 | Loss: 0.00002288
Iteration 162/1000 | Loss: 0.00002288
Iteration 163/1000 | Loss: 0.00002288
Iteration 164/1000 | Loss: 0.00002287
Iteration 165/1000 | Loss: 0.00002287
Iteration 166/1000 | Loss: 0.00002287
Iteration 167/1000 | Loss: 0.00002287
Iteration 168/1000 | Loss: 0.00002287
Iteration 169/1000 | Loss: 0.00002287
Iteration 170/1000 | Loss: 0.00002287
Iteration 171/1000 | Loss: 0.00002287
Iteration 172/1000 | Loss: 0.00002286
Iteration 173/1000 | Loss: 0.00002286
Iteration 174/1000 | Loss: 0.00002286
Iteration 175/1000 | Loss: 0.00002286
Iteration 176/1000 | Loss: 0.00002286
Iteration 177/1000 | Loss: 0.00002286
Iteration 178/1000 | Loss: 0.00002286
Iteration 179/1000 | Loss: 0.00002286
Iteration 180/1000 | Loss: 0.00002286
Iteration 181/1000 | Loss: 0.00002286
Iteration 182/1000 | Loss: 0.00002285
Iteration 183/1000 | Loss: 0.00002285
Iteration 184/1000 | Loss: 0.00002285
Iteration 185/1000 | Loss: 0.00002285
Iteration 186/1000 | Loss: 0.00002285
Iteration 187/1000 | Loss: 0.00002285
Iteration 188/1000 | Loss: 0.00002285
Iteration 189/1000 | Loss: 0.00002285
Iteration 190/1000 | Loss: 0.00002285
Iteration 191/1000 | Loss: 0.00002285
Iteration 192/1000 | Loss: 0.00002285
Iteration 193/1000 | Loss: 0.00002285
Iteration 194/1000 | Loss: 0.00002285
Iteration 195/1000 | Loss: 0.00002285
Iteration 196/1000 | Loss: 0.00002285
Iteration 197/1000 | Loss: 0.00002285
Iteration 198/1000 | Loss: 0.00002285
Iteration 199/1000 | Loss: 0.00002285
Iteration 200/1000 | Loss: 0.00002285
Iteration 201/1000 | Loss: 0.00002285
Iteration 202/1000 | Loss: 0.00002285
Iteration 203/1000 | Loss: 0.00002285
Iteration 204/1000 | Loss: 0.00002285
Iteration 205/1000 | Loss: 0.00002285
Iteration 206/1000 | Loss: 0.00002285
Iteration 207/1000 | Loss: 0.00002285
Iteration 208/1000 | Loss: 0.00002285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [2.284893525938969e-05, 2.284893525938969e-05, 2.284893525938969e-05, 2.284893525938969e-05, 2.284893525938969e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.284893525938969e-05

Optimization complete. Final v2v error: 3.900547504425049 mm

Highest mean error: 5.186197757720947 mm for frame 50

Lowest mean error: 3.334897518157959 mm for frame 99

Saving results

Total time: 65.92136883735657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821548
Iteration 2/25 | Loss: 0.00203828
Iteration 3/25 | Loss: 0.00171176
Iteration 4/25 | Loss: 0.00166441
Iteration 5/25 | Loss: 0.00208104
Iteration 6/25 | Loss: 0.00175002
Iteration 7/25 | Loss: 0.00152925
Iteration 8/25 | Loss: 0.00147445
Iteration 9/25 | Loss: 0.00146908
Iteration 10/25 | Loss: 0.00146828
Iteration 11/25 | Loss: 0.00146828
Iteration 12/25 | Loss: 0.00146828
Iteration 13/25 | Loss: 0.00146828
Iteration 14/25 | Loss: 0.00146828
Iteration 15/25 | Loss: 0.00146828
Iteration 16/25 | Loss: 0.00146828
Iteration 17/25 | Loss: 0.00146828
Iteration 18/25 | Loss: 0.00146828
Iteration 19/25 | Loss: 0.00146828
Iteration 20/25 | Loss: 0.00146828
Iteration 21/25 | Loss: 0.00146828
Iteration 22/25 | Loss: 0.00146828
Iteration 23/25 | Loss: 0.00146828
Iteration 24/25 | Loss: 0.00146828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014682799810543656, 0.0014682799810543656, 0.0014682799810543656, 0.0014682799810543656, 0.0014682799810543656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014682799810543656

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07754314
Iteration 2/25 | Loss: 0.00103529
Iteration 3/25 | Loss: 0.00103529
Iteration 4/25 | Loss: 0.00103529
Iteration 5/25 | Loss: 0.00103529
Iteration 6/25 | Loss: 0.00103529
Iteration 7/25 | Loss: 0.00103529
Iteration 8/25 | Loss: 0.00103529
Iteration 9/25 | Loss: 0.00103529
Iteration 10/25 | Loss: 0.00103529
Iteration 11/25 | Loss: 0.00103529
Iteration 12/25 | Loss: 0.00103529
Iteration 13/25 | Loss: 0.00103529
Iteration 14/25 | Loss: 0.00103529
Iteration 15/25 | Loss: 0.00103529
Iteration 16/25 | Loss: 0.00103529
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010352913523092866, 0.0010352913523092866, 0.0010352913523092866, 0.0010352913523092866, 0.0010352913523092866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010352913523092866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103529
Iteration 2/1000 | Loss: 0.00006514
Iteration 3/1000 | Loss: 0.00005290
Iteration 4/1000 | Loss: 0.00004996
Iteration 5/1000 | Loss: 0.00004836
Iteration 6/1000 | Loss: 0.00004748
Iteration 7/1000 | Loss: 0.00004668
Iteration 8/1000 | Loss: 0.00004606
Iteration 9/1000 | Loss: 0.00004569
Iteration 10/1000 | Loss: 0.00004536
Iteration 11/1000 | Loss: 0.00004510
Iteration 12/1000 | Loss: 0.00004486
Iteration 13/1000 | Loss: 0.00004482
Iteration 14/1000 | Loss: 0.00004482
Iteration 15/1000 | Loss: 0.00004464
Iteration 16/1000 | Loss: 0.00004451
Iteration 17/1000 | Loss: 0.00004449
Iteration 18/1000 | Loss: 0.00004449
Iteration 19/1000 | Loss: 0.00004449
Iteration 20/1000 | Loss: 0.00004449
Iteration 21/1000 | Loss: 0.00004449
Iteration 22/1000 | Loss: 0.00004449
Iteration 23/1000 | Loss: 0.00004448
Iteration 24/1000 | Loss: 0.00004448
Iteration 25/1000 | Loss: 0.00004443
Iteration 26/1000 | Loss: 0.00004443
Iteration 27/1000 | Loss: 0.00004443
Iteration 28/1000 | Loss: 0.00004443
Iteration 29/1000 | Loss: 0.00004443
Iteration 30/1000 | Loss: 0.00004443
Iteration 31/1000 | Loss: 0.00004443
Iteration 32/1000 | Loss: 0.00004443
Iteration 33/1000 | Loss: 0.00004442
Iteration 34/1000 | Loss: 0.00004442
Iteration 35/1000 | Loss: 0.00004442
Iteration 36/1000 | Loss: 0.00004442
Iteration 37/1000 | Loss: 0.00004442
Iteration 38/1000 | Loss: 0.00004442
Iteration 39/1000 | Loss: 0.00004436
Iteration 40/1000 | Loss: 0.00004436
Iteration 41/1000 | Loss: 0.00004436
Iteration 42/1000 | Loss: 0.00004436
Iteration 43/1000 | Loss: 0.00004436
Iteration 44/1000 | Loss: 0.00004436
Iteration 45/1000 | Loss: 0.00004435
Iteration 46/1000 | Loss: 0.00004435
Iteration 47/1000 | Loss: 0.00004435
Iteration 48/1000 | Loss: 0.00004435
Iteration 49/1000 | Loss: 0.00004435
Iteration 50/1000 | Loss: 0.00004435
Iteration 51/1000 | Loss: 0.00004435
Iteration 52/1000 | Loss: 0.00004434
Iteration 53/1000 | Loss: 0.00004434
Iteration 54/1000 | Loss: 0.00004434
Iteration 55/1000 | Loss: 0.00004434
Iteration 56/1000 | Loss: 0.00004434
Iteration 57/1000 | Loss: 0.00004433
Iteration 58/1000 | Loss: 0.00004433
Iteration 59/1000 | Loss: 0.00004433
Iteration 60/1000 | Loss: 0.00004433
Iteration 61/1000 | Loss: 0.00004433
Iteration 62/1000 | Loss: 0.00004433
Iteration 63/1000 | Loss: 0.00004433
Iteration 64/1000 | Loss: 0.00004433
Iteration 65/1000 | Loss: 0.00004432
Iteration 66/1000 | Loss: 0.00004431
Iteration 67/1000 | Loss: 0.00004431
Iteration 68/1000 | Loss: 0.00004431
Iteration 69/1000 | Loss: 0.00004430
Iteration 70/1000 | Loss: 0.00004427
Iteration 71/1000 | Loss: 0.00004427
Iteration 72/1000 | Loss: 0.00004427
Iteration 73/1000 | Loss: 0.00004427
Iteration 74/1000 | Loss: 0.00004426
Iteration 75/1000 | Loss: 0.00004426
Iteration 76/1000 | Loss: 0.00004426
Iteration 77/1000 | Loss: 0.00004426
Iteration 78/1000 | Loss: 0.00004425
Iteration 79/1000 | Loss: 0.00004425
Iteration 80/1000 | Loss: 0.00004423
Iteration 81/1000 | Loss: 0.00004422
Iteration 82/1000 | Loss: 0.00004422
Iteration 83/1000 | Loss: 0.00004422
Iteration 84/1000 | Loss: 0.00004422
Iteration 85/1000 | Loss: 0.00004421
Iteration 86/1000 | Loss: 0.00004421
Iteration 87/1000 | Loss: 0.00004421
Iteration 88/1000 | Loss: 0.00004421
Iteration 89/1000 | Loss: 0.00004421
Iteration 90/1000 | Loss: 0.00004421
Iteration 91/1000 | Loss: 0.00004421
Iteration 92/1000 | Loss: 0.00004421
Iteration 93/1000 | Loss: 0.00004421
Iteration 94/1000 | Loss: 0.00004421
Iteration 95/1000 | Loss: 0.00004420
Iteration 96/1000 | Loss: 0.00004420
Iteration 97/1000 | Loss: 0.00004419
Iteration 98/1000 | Loss: 0.00004419
Iteration 99/1000 | Loss: 0.00004418
Iteration 100/1000 | Loss: 0.00004418
Iteration 101/1000 | Loss: 0.00004418
Iteration 102/1000 | Loss: 0.00004418
Iteration 103/1000 | Loss: 0.00004417
Iteration 104/1000 | Loss: 0.00004416
Iteration 105/1000 | Loss: 0.00004415
Iteration 106/1000 | Loss: 0.00004415
Iteration 107/1000 | Loss: 0.00004415
Iteration 108/1000 | Loss: 0.00004414
Iteration 109/1000 | Loss: 0.00004414
Iteration 110/1000 | Loss: 0.00004414
Iteration 111/1000 | Loss: 0.00004413
Iteration 112/1000 | Loss: 0.00004413
Iteration 113/1000 | Loss: 0.00004413
Iteration 114/1000 | Loss: 0.00004413
Iteration 115/1000 | Loss: 0.00004412
Iteration 116/1000 | Loss: 0.00004412
Iteration 117/1000 | Loss: 0.00004412
Iteration 118/1000 | Loss: 0.00004411
Iteration 119/1000 | Loss: 0.00004411
Iteration 120/1000 | Loss: 0.00004411
Iteration 121/1000 | Loss: 0.00004411
Iteration 122/1000 | Loss: 0.00004410
Iteration 123/1000 | Loss: 0.00004410
Iteration 124/1000 | Loss: 0.00004409
Iteration 125/1000 | Loss: 0.00004409
Iteration 126/1000 | Loss: 0.00004409
Iteration 127/1000 | Loss: 0.00004408
Iteration 128/1000 | Loss: 0.00004408
Iteration 129/1000 | Loss: 0.00004408
Iteration 130/1000 | Loss: 0.00004408
Iteration 131/1000 | Loss: 0.00004407
Iteration 132/1000 | Loss: 0.00004407
Iteration 133/1000 | Loss: 0.00004407
Iteration 134/1000 | Loss: 0.00004407
Iteration 135/1000 | Loss: 0.00004406
Iteration 136/1000 | Loss: 0.00004406
Iteration 137/1000 | Loss: 0.00004406
Iteration 138/1000 | Loss: 0.00004406
Iteration 139/1000 | Loss: 0.00004406
Iteration 140/1000 | Loss: 0.00004406
Iteration 141/1000 | Loss: 0.00004406
Iteration 142/1000 | Loss: 0.00004406
Iteration 143/1000 | Loss: 0.00004406
Iteration 144/1000 | Loss: 0.00004406
Iteration 145/1000 | Loss: 0.00004406
Iteration 146/1000 | Loss: 0.00004405
Iteration 147/1000 | Loss: 0.00004405
Iteration 148/1000 | Loss: 0.00004405
Iteration 149/1000 | Loss: 0.00004405
Iteration 150/1000 | Loss: 0.00004405
Iteration 151/1000 | Loss: 0.00004405
Iteration 152/1000 | Loss: 0.00004405
Iteration 153/1000 | Loss: 0.00004405
Iteration 154/1000 | Loss: 0.00004405
Iteration 155/1000 | Loss: 0.00004405
Iteration 156/1000 | Loss: 0.00004404
Iteration 157/1000 | Loss: 0.00004404
Iteration 158/1000 | Loss: 0.00004403
Iteration 159/1000 | Loss: 0.00004403
Iteration 160/1000 | Loss: 0.00004403
Iteration 161/1000 | Loss: 0.00004403
Iteration 162/1000 | Loss: 0.00004402
Iteration 163/1000 | Loss: 0.00004402
Iteration 164/1000 | Loss: 0.00004402
Iteration 165/1000 | Loss: 0.00004402
Iteration 166/1000 | Loss: 0.00004402
Iteration 167/1000 | Loss: 0.00004401
Iteration 168/1000 | Loss: 0.00004401
Iteration 169/1000 | Loss: 0.00004401
Iteration 170/1000 | Loss: 0.00004401
Iteration 171/1000 | Loss: 0.00004401
Iteration 172/1000 | Loss: 0.00004401
Iteration 173/1000 | Loss: 0.00004401
Iteration 174/1000 | Loss: 0.00004401
Iteration 175/1000 | Loss: 0.00004401
Iteration 176/1000 | Loss: 0.00004401
Iteration 177/1000 | Loss: 0.00004401
Iteration 178/1000 | Loss: 0.00004401
Iteration 179/1000 | Loss: 0.00004401
Iteration 180/1000 | Loss: 0.00004400
Iteration 181/1000 | Loss: 0.00004400
Iteration 182/1000 | Loss: 0.00004400
Iteration 183/1000 | Loss: 0.00004400
Iteration 184/1000 | Loss: 0.00004400
Iteration 185/1000 | Loss: 0.00004400
Iteration 186/1000 | Loss: 0.00004400
Iteration 187/1000 | Loss: 0.00004400
Iteration 188/1000 | Loss: 0.00004400
Iteration 189/1000 | Loss: 0.00004400
Iteration 190/1000 | Loss: 0.00004400
Iteration 191/1000 | Loss: 0.00004400
Iteration 192/1000 | Loss: 0.00004400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [4.400104808155447e-05, 4.400104808155447e-05, 4.400104808155447e-05, 4.400104808155447e-05, 4.400104808155447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.400104808155447e-05

Optimization complete. Final v2v error: 5.319815635681152 mm

Highest mean error: 5.462561130523682 mm for frame 26

Lowest mean error: 5.2592549324035645 mm for frame 100

Saving results

Total time: 48.10714864730835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000841
Iteration 2/25 | Loss: 0.00366436
Iteration 3/25 | Loss: 0.00206267
Iteration 4/25 | Loss: 0.00157328
Iteration 5/25 | Loss: 0.00145403
Iteration 6/25 | Loss: 0.00142857
Iteration 7/25 | Loss: 0.00132411
Iteration 8/25 | Loss: 0.00128818
Iteration 9/25 | Loss: 0.00127538
Iteration 10/25 | Loss: 0.00127054
Iteration 11/25 | Loss: 0.00126890
Iteration 12/25 | Loss: 0.00126500
Iteration 13/25 | Loss: 0.00126466
Iteration 14/25 | Loss: 0.00126455
Iteration 15/25 | Loss: 0.00126454
Iteration 16/25 | Loss: 0.00126452
Iteration 17/25 | Loss: 0.00126452
Iteration 18/25 | Loss: 0.00126452
Iteration 19/25 | Loss: 0.00126452
Iteration 20/25 | Loss: 0.00126451
Iteration 21/25 | Loss: 0.00126451
Iteration 22/25 | Loss: 0.00126451
Iteration 23/25 | Loss: 0.00126451
Iteration 24/25 | Loss: 0.00126451
Iteration 25/25 | Loss: 0.00126451

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35482550
Iteration 2/25 | Loss: 0.00095481
Iteration 3/25 | Loss: 0.00095481
Iteration 4/25 | Loss: 0.00095481
Iteration 5/25 | Loss: 0.00095481
Iteration 6/25 | Loss: 0.00095480
Iteration 7/25 | Loss: 0.00095480
Iteration 8/25 | Loss: 0.00095480
Iteration 9/25 | Loss: 0.00095480
Iteration 10/25 | Loss: 0.00095480
Iteration 11/25 | Loss: 0.00095480
Iteration 12/25 | Loss: 0.00095480
Iteration 13/25 | Loss: 0.00095480
Iteration 14/25 | Loss: 0.00095480
Iteration 15/25 | Loss: 0.00095480
Iteration 16/25 | Loss: 0.00095480
Iteration 17/25 | Loss: 0.00095480
Iteration 18/25 | Loss: 0.00095480
Iteration 19/25 | Loss: 0.00095480
Iteration 20/25 | Loss: 0.00095480
Iteration 21/25 | Loss: 0.00095480
Iteration 22/25 | Loss: 0.00095480
Iteration 23/25 | Loss: 0.00095480
Iteration 24/25 | Loss: 0.00095480
Iteration 25/25 | Loss: 0.00095480

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095480
Iteration 2/1000 | Loss: 0.00003484
Iteration 3/1000 | Loss: 0.00008678
Iteration 4/1000 | Loss: 0.00002895
Iteration 5/1000 | Loss: 0.00002105
Iteration 6/1000 | Loss: 0.00002014
Iteration 7/1000 | Loss: 0.00074227
Iteration 8/1000 | Loss: 0.00018976
Iteration 9/1000 | Loss: 0.00057344
Iteration 10/1000 | Loss: 0.00045973
Iteration 11/1000 | Loss: 0.00002847
Iteration 12/1000 | Loss: 0.00001903
Iteration 13/1000 | Loss: 0.00001863
Iteration 14/1000 | Loss: 0.00019997
Iteration 15/1000 | Loss: 0.00034254
Iteration 16/1000 | Loss: 0.00002858
Iteration 17/1000 | Loss: 0.00012069
Iteration 18/1000 | Loss: 0.00001902
Iteration 19/1000 | Loss: 0.00001839
Iteration 20/1000 | Loss: 0.00009739
Iteration 21/1000 | Loss: 0.00005112
Iteration 22/1000 | Loss: 0.00002384
Iteration 23/1000 | Loss: 0.00010511
Iteration 24/1000 | Loss: 0.00004090
Iteration 25/1000 | Loss: 0.00017213
Iteration 26/1000 | Loss: 0.00002794
Iteration 27/1000 | Loss: 0.00001780
Iteration 28/1000 | Loss: 0.00007548
Iteration 29/1000 | Loss: 0.00001880
Iteration 30/1000 | Loss: 0.00001761
Iteration 31/1000 | Loss: 0.00002250
Iteration 32/1000 | Loss: 0.00014091
Iteration 33/1000 | Loss: 0.00002864
Iteration 34/1000 | Loss: 0.00001857
Iteration 35/1000 | Loss: 0.00002628
Iteration 36/1000 | Loss: 0.00001745
Iteration 37/1000 | Loss: 0.00006289
Iteration 38/1000 | Loss: 0.00002025
Iteration 39/1000 | Loss: 0.00002054
Iteration 40/1000 | Loss: 0.00002219
Iteration 41/1000 | Loss: 0.00002219
Iteration 42/1000 | Loss: 0.00002219
Iteration 43/1000 | Loss: 0.00014761
Iteration 44/1000 | Loss: 0.00006026
Iteration 45/1000 | Loss: 0.00007039
Iteration 46/1000 | Loss: 0.00007124
Iteration 47/1000 | Loss: 0.00015473
Iteration 48/1000 | Loss: 0.00004600
Iteration 49/1000 | Loss: 0.00001774
Iteration 50/1000 | Loss: 0.00001744
Iteration 51/1000 | Loss: 0.00004168
Iteration 52/1000 | Loss: 0.00001996
Iteration 53/1000 | Loss: 0.00001736
Iteration 54/1000 | Loss: 0.00001734
Iteration 55/1000 | Loss: 0.00001734
Iteration 56/1000 | Loss: 0.00002022
Iteration 57/1000 | Loss: 0.00001724
Iteration 58/1000 | Loss: 0.00001723
Iteration 59/1000 | Loss: 0.00001723
Iteration 60/1000 | Loss: 0.00001723
Iteration 61/1000 | Loss: 0.00001723
Iteration 62/1000 | Loss: 0.00001723
Iteration 63/1000 | Loss: 0.00001723
Iteration 64/1000 | Loss: 0.00001722
Iteration 65/1000 | Loss: 0.00001722
Iteration 66/1000 | Loss: 0.00001722
Iteration 67/1000 | Loss: 0.00001722
Iteration 68/1000 | Loss: 0.00001721
Iteration 69/1000 | Loss: 0.00001721
Iteration 70/1000 | Loss: 0.00001721
Iteration 71/1000 | Loss: 0.00001721
Iteration 72/1000 | Loss: 0.00001720
Iteration 73/1000 | Loss: 0.00001720
Iteration 74/1000 | Loss: 0.00001720
Iteration 75/1000 | Loss: 0.00001719
Iteration 76/1000 | Loss: 0.00001719
Iteration 77/1000 | Loss: 0.00001718
Iteration 78/1000 | Loss: 0.00001718
Iteration 79/1000 | Loss: 0.00001718
Iteration 80/1000 | Loss: 0.00001717
Iteration 81/1000 | Loss: 0.00001717
Iteration 82/1000 | Loss: 0.00001717
Iteration 83/1000 | Loss: 0.00001716
Iteration 84/1000 | Loss: 0.00001716
Iteration 85/1000 | Loss: 0.00001716
Iteration 86/1000 | Loss: 0.00001716
Iteration 87/1000 | Loss: 0.00001716
Iteration 88/1000 | Loss: 0.00001715
Iteration 89/1000 | Loss: 0.00001715
Iteration 90/1000 | Loss: 0.00001715
Iteration 91/1000 | Loss: 0.00001715
Iteration 92/1000 | Loss: 0.00001715
Iteration 93/1000 | Loss: 0.00001715
Iteration 94/1000 | Loss: 0.00001714
Iteration 95/1000 | Loss: 0.00001714
Iteration 96/1000 | Loss: 0.00001714
Iteration 97/1000 | Loss: 0.00001713
Iteration 98/1000 | Loss: 0.00001713
Iteration 99/1000 | Loss: 0.00001713
Iteration 100/1000 | Loss: 0.00001713
Iteration 101/1000 | Loss: 0.00001712
Iteration 102/1000 | Loss: 0.00001712
Iteration 103/1000 | Loss: 0.00001712
Iteration 104/1000 | Loss: 0.00001712
Iteration 105/1000 | Loss: 0.00001712
Iteration 106/1000 | Loss: 0.00001712
Iteration 107/1000 | Loss: 0.00001712
Iteration 108/1000 | Loss: 0.00001712
Iteration 109/1000 | Loss: 0.00001712
Iteration 110/1000 | Loss: 0.00001711
Iteration 111/1000 | Loss: 0.00001711
Iteration 112/1000 | Loss: 0.00001711
Iteration 113/1000 | Loss: 0.00001711
Iteration 114/1000 | Loss: 0.00001711
Iteration 115/1000 | Loss: 0.00001711
Iteration 116/1000 | Loss: 0.00001711
Iteration 117/1000 | Loss: 0.00001711
Iteration 118/1000 | Loss: 0.00001711
Iteration 119/1000 | Loss: 0.00001711
Iteration 120/1000 | Loss: 0.00001711
Iteration 121/1000 | Loss: 0.00001711
Iteration 122/1000 | Loss: 0.00001711
Iteration 123/1000 | Loss: 0.00001711
Iteration 124/1000 | Loss: 0.00001711
Iteration 125/1000 | Loss: 0.00001711
Iteration 126/1000 | Loss: 0.00001711
Iteration 127/1000 | Loss: 0.00001711
Iteration 128/1000 | Loss: 0.00001711
Iteration 129/1000 | Loss: 0.00001711
Iteration 130/1000 | Loss: 0.00001711
Iteration 131/1000 | Loss: 0.00001711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.710772812657524e-05, 1.710772812657524e-05, 1.710772812657524e-05, 1.710772812657524e-05, 1.710772812657524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.710772812657524e-05

Optimization complete. Final v2v error: 3.509532928466797 mm

Highest mean error: 9.001607894897461 mm for frame 51

Lowest mean error: 3.260648012161255 mm for frame 170

Saving results

Total time: 113.07511949539185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00521112
Iteration 2/25 | Loss: 0.00157778
Iteration 3/25 | Loss: 0.00132935
Iteration 4/25 | Loss: 0.00131064
Iteration 5/25 | Loss: 0.00130547
Iteration 6/25 | Loss: 0.00130386
Iteration 7/25 | Loss: 0.00130334
Iteration 8/25 | Loss: 0.00130334
Iteration 9/25 | Loss: 0.00130334
Iteration 10/25 | Loss: 0.00130334
Iteration 11/25 | Loss: 0.00130334
Iteration 12/25 | Loss: 0.00130334
Iteration 13/25 | Loss: 0.00130334
Iteration 14/25 | Loss: 0.00130334
Iteration 15/25 | Loss: 0.00130334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013033439172431827, 0.0013033439172431827, 0.0013033439172431827, 0.0013033439172431827, 0.0013033439172431827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013033439172431827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26481068
Iteration 2/25 | Loss: 0.00101167
Iteration 3/25 | Loss: 0.00101165
Iteration 4/25 | Loss: 0.00101165
Iteration 5/25 | Loss: 0.00101165
Iteration 6/25 | Loss: 0.00101165
Iteration 7/25 | Loss: 0.00101165
Iteration 8/25 | Loss: 0.00101165
Iteration 9/25 | Loss: 0.00101165
Iteration 10/25 | Loss: 0.00101165
Iteration 11/25 | Loss: 0.00101165
Iteration 12/25 | Loss: 0.00101165
Iteration 13/25 | Loss: 0.00101165
Iteration 14/25 | Loss: 0.00101165
Iteration 15/25 | Loss: 0.00101165
Iteration 16/25 | Loss: 0.00101165
Iteration 17/25 | Loss: 0.00101165
Iteration 18/25 | Loss: 0.00101165
Iteration 19/25 | Loss: 0.00101165
Iteration 20/25 | Loss: 0.00101165
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001011645537801087, 0.001011645537801087, 0.001011645537801087, 0.001011645537801087, 0.001011645537801087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001011645537801087

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101165
Iteration 2/1000 | Loss: 0.00005483
Iteration 3/1000 | Loss: 0.00003261
Iteration 4/1000 | Loss: 0.00002840
Iteration 5/1000 | Loss: 0.00002648
Iteration 6/1000 | Loss: 0.00002547
Iteration 7/1000 | Loss: 0.00002464
Iteration 8/1000 | Loss: 0.00002403
Iteration 9/1000 | Loss: 0.00002362
Iteration 10/1000 | Loss: 0.00002330
Iteration 11/1000 | Loss: 0.00002302
Iteration 12/1000 | Loss: 0.00002278
Iteration 13/1000 | Loss: 0.00002254
Iteration 14/1000 | Loss: 0.00002233
Iteration 15/1000 | Loss: 0.00002211
Iteration 16/1000 | Loss: 0.00002209
Iteration 17/1000 | Loss: 0.00002203
Iteration 18/1000 | Loss: 0.00002193
Iteration 19/1000 | Loss: 0.00002183
Iteration 20/1000 | Loss: 0.00002179
Iteration 21/1000 | Loss: 0.00002179
Iteration 22/1000 | Loss: 0.00002178
Iteration 23/1000 | Loss: 0.00002178
Iteration 24/1000 | Loss: 0.00002177
Iteration 25/1000 | Loss: 0.00002168
Iteration 26/1000 | Loss: 0.00002166
Iteration 27/1000 | Loss: 0.00002163
Iteration 28/1000 | Loss: 0.00002163
Iteration 29/1000 | Loss: 0.00002163
Iteration 30/1000 | Loss: 0.00002162
Iteration 31/1000 | Loss: 0.00002162
Iteration 32/1000 | Loss: 0.00002161
Iteration 33/1000 | Loss: 0.00002160
Iteration 34/1000 | Loss: 0.00002159
Iteration 35/1000 | Loss: 0.00002158
Iteration 36/1000 | Loss: 0.00002158
Iteration 37/1000 | Loss: 0.00002157
Iteration 38/1000 | Loss: 0.00002156
Iteration 39/1000 | Loss: 0.00002150
Iteration 40/1000 | Loss: 0.00002150
Iteration 41/1000 | Loss: 0.00002148
Iteration 42/1000 | Loss: 0.00002147
Iteration 43/1000 | Loss: 0.00002147
Iteration 44/1000 | Loss: 0.00002147
Iteration 45/1000 | Loss: 0.00002146
Iteration 46/1000 | Loss: 0.00002146
Iteration 47/1000 | Loss: 0.00002146
Iteration 48/1000 | Loss: 0.00002146
Iteration 49/1000 | Loss: 0.00002146
Iteration 50/1000 | Loss: 0.00002146
Iteration 51/1000 | Loss: 0.00002146
Iteration 52/1000 | Loss: 0.00002146
Iteration 53/1000 | Loss: 0.00002146
Iteration 54/1000 | Loss: 0.00002146
Iteration 55/1000 | Loss: 0.00002145
Iteration 56/1000 | Loss: 0.00002145
Iteration 57/1000 | Loss: 0.00002144
Iteration 58/1000 | Loss: 0.00002144
Iteration 59/1000 | Loss: 0.00002144
Iteration 60/1000 | Loss: 0.00002144
Iteration 61/1000 | Loss: 0.00002143
Iteration 62/1000 | Loss: 0.00002143
Iteration 63/1000 | Loss: 0.00002143
Iteration 64/1000 | Loss: 0.00002142
Iteration 65/1000 | Loss: 0.00002142
Iteration 66/1000 | Loss: 0.00002141
Iteration 67/1000 | Loss: 0.00002141
Iteration 68/1000 | Loss: 0.00002141
Iteration 69/1000 | Loss: 0.00002141
Iteration 70/1000 | Loss: 0.00002141
Iteration 71/1000 | Loss: 0.00002141
Iteration 72/1000 | Loss: 0.00002141
Iteration 73/1000 | Loss: 0.00002140
Iteration 74/1000 | Loss: 0.00002140
Iteration 75/1000 | Loss: 0.00002140
Iteration 76/1000 | Loss: 0.00002140
Iteration 77/1000 | Loss: 0.00002140
Iteration 78/1000 | Loss: 0.00002140
Iteration 79/1000 | Loss: 0.00002140
Iteration 80/1000 | Loss: 0.00002140
Iteration 81/1000 | Loss: 0.00002140
Iteration 82/1000 | Loss: 0.00002139
Iteration 83/1000 | Loss: 0.00002139
Iteration 84/1000 | Loss: 0.00002139
Iteration 85/1000 | Loss: 0.00002138
Iteration 86/1000 | Loss: 0.00002138
Iteration 87/1000 | Loss: 0.00002138
Iteration 88/1000 | Loss: 0.00002138
Iteration 89/1000 | Loss: 0.00002138
Iteration 90/1000 | Loss: 0.00002138
Iteration 91/1000 | Loss: 0.00002138
Iteration 92/1000 | Loss: 0.00002138
Iteration 93/1000 | Loss: 0.00002138
Iteration 94/1000 | Loss: 0.00002138
Iteration 95/1000 | Loss: 0.00002138
Iteration 96/1000 | Loss: 0.00002138
Iteration 97/1000 | Loss: 0.00002137
Iteration 98/1000 | Loss: 0.00002137
Iteration 99/1000 | Loss: 0.00002137
Iteration 100/1000 | Loss: 0.00002137
Iteration 101/1000 | Loss: 0.00002137
Iteration 102/1000 | Loss: 0.00002136
Iteration 103/1000 | Loss: 0.00002136
Iteration 104/1000 | Loss: 0.00002136
Iteration 105/1000 | Loss: 0.00002136
Iteration 106/1000 | Loss: 0.00002136
Iteration 107/1000 | Loss: 0.00002136
Iteration 108/1000 | Loss: 0.00002136
Iteration 109/1000 | Loss: 0.00002136
Iteration 110/1000 | Loss: 0.00002136
Iteration 111/1000 | Loss: 0.00002136
Iteration 112/1000 | Loss: 0.00002136
Iteration 113/1000 | Loss: 0.00002136
Iteration 114/1000 | Loss: 0.00002136
Iteration 115/1000 | Loss: 0.00002136
Iteration 116/1000 | Loss: 0.00002136
Iteration 117/1000 | Loss: 0.00002136
Iteration 118/1000 | Loss: 0.00002136
Iteration 119/1000 | Loss: 0.00002136
Iteration 120/1000 | Loss: 0.00002136
Iteration 121/1000 | Loss: 0.00002136
Iteration 122/1000 | Loss: 0.00002136
Iteration 123/1000 | Loss: 0.00002136
Iteration 124/1000 | Loss: 0.00002136
Iteration 125/1000 | Loss: 0.00002136
Iteration 126/1000 | Loss: 0.00002136
Iteration 127/1000 | Loss: 0.00002136
Iteration 128/1000 | Loss: 0.00002136
Iteration 129/1000 | Loss: 0.00002136
Iteration 130/1000 | Loss: 0.00002136
Iteration 131/1000 | Loss: 0.00002136
Iteration 132/1000 | Loss: 0.00002136
Iteration 133/1000 | Loss: 0.00002136
Iteration 134/1000 | Loss: 0.00002136
Iteration 135/1000 | Loss: 0.00002136
Iteration 136/1000 | Loss: 0.00002136
Iteration 137/1000 | Loss: 0.00002136
Iteration 138/1000 | Loss: 0.00002136
Iteration 139/1000 | Loss: 0.00002136
Iteration 140/1000 | Loss: 0.00002136
Iteration 141/1000 | Loss: 0.00002136
Iteration 142/1000 | Loss: 0.00002136
Iteration 143/1000 | Loss: 0.00002136
Iteration 144/1000 | Loss: 0.00002136
Iteration 145/1000 | Loss: 0.00002136
Iteration 146/1000 | Loss: 0.00002136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.1356834622565657e-05, 2.1356834622565657e-05, 2.1356834622565657e-05, 2.1356834622565657e-05, 2.1356834622565657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1356834622565657e-05

Optimization complete. Final v2v error: 3.7577030658721924 mm

Highest mean error: 5.762599945068359 mm for frame 57

Lowest mean error: 2.8684921264648438 mm for frame 20

Saving results

Total time: 45.21897792816162
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990340
Iteration 2/25 | Loss: 0.00492633
Iteration 3/25 | Loss: 0.00434294
Iteration 4/25 | Loss: 0.00320504
Iteration 5/25 | Loss: 0.00263105
Iteration 6/25 | Loss: 0.00225542
Iteration 7/25 | Loss: 0.00215365
Iteration 8/25 | Loss: 0.00210575
Iteration 9/25 | Loss: 0.00207140
Iteration 10/25 | Loss: 0.00203933
Iteration 11/25 | Loss: 0.00202498
Iteration 12/25 | Loss: 0.00202369
Iteration 13/25 | Loss: 0.00202499
Iteration 14/25 | Loss: 0.00200566
Iteration 15/25 | Loss: 0.00199246
Iteration 16/25 | Loss: 0.00198806
Iteration 17/25 | Loss: 0.00196459
Iteration 18/25 | Loss: 0.00195684
Iteration 19/25 | Loss: 0.00195889
Iteration 20/25 | Loss: 0.00195735
Iteration 21/25 | Loss: 0.00194962
Iteration 22/25 | Loss: 0.00194316
Iteration 23/25 | Loss: 0.00194078
Iteration 24/25 | Loss: 0.00194033
Iteration 25/25 | Loss: 0.00193999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27027881
Iteration 2/25 | Loss: 0.00753476
Iteration 3/25 | Loss: 0.00700081
Iteration 4/25 | Loss: 0.00700081
Iteration 5/25 | Loss: 0.00700081
Iteration 6/25 | Loss: 0.00700081
Iteration 7/25 | Loss: 0.00700081
Iteration 8/25 | Loss: 0.00700081
Iteration 9/25 | Loss: 0.00700081
Iteration 10/25 | Loss: 0.00700081
Iteration 11/25 | Loss: 0.00700081
Iteration 12/25 | Loss: 0.00700081
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.007000813260674477, 0.007000813260674477, 0.007000813260674477, 0.007000813260674477, 0.007000813260674477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007000813260674477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00700081
Iteration 2/1000 | Loss: 0.00134112
Iteration 3/1000 | Loss: 0.00061108
Iteration 4/1000 | Loss: 0.00061648
Iteration 5/1000 | Loss: 0.00066739
Iteration 6/1000 | Loss: 0.00046126
Iteration 7/1000 | Loss: 0.00043213
Iteration 8/1000 | Loss: 0.00040925
Iteration 9/1000 | Loss: 0.00038001
Iteration 10/1000 | Loss: 0.00036573
Iteration 11/1000 | Loss: 0.00035030
Iteration 12/1000 | Loss: 0.00034144
Iteration 13/1000 | Loss: 0.00033374
Iteration 14/1000 | Loss: 0.00591521
Iteration 15/1000 | Loss: 0.03500427
Iteration 16/1000 | Loss: 0.03901462
Iteration 17/1000 | Loss: 0.01638390
Iteration 18/1000 | Loss: 0.00130090
Iteration 19/1000 | Loss: 0.00072761
Iteration 20/1000 | Loss: 0.00085039
Iteration 21/1000 | Loss: 0.00152678
Iteration 22/1000 | Loss: 0.00237944
Iteration 23/1000 | Loss: 0.00259168
Iteration 24/1000 | Loss: 0.00041602
Iteration 25/1000 | Loss: 0.00279910
Iteration 26/1000 | Loss: 0.00178002
Iteration 27/1000 | Loss: 0.00098303
Iteration 28/1000 | Loss: 0.00084887
Iteration 29/1000 | Loss: 0.00028092
Iteration 30/1000 | Loss: 0.00022706
Iteration 31/1000 | Loss: 0.00032258
Iteration 32/1000 | Loss: 0.00022124
Iteration 33/1000 | Loss: 0.00015064
Iteration 34/1000 | Loss: 0.00015625
Iteration 35/1000 | Loss: 0.00012515
Iteration 36/1000 | Loss: 0.00011718
Iteration 37/1000 | Loss: 0.00011290
Iteration 38/1000 | Loss: 0.00011078
Iteration 39/1000 | Loss: 0.00010845
Iteration 40/1000 | Loss: 0.00124296
Iteration 41/1000 | Loss: 0.00441293
Iteration 42/1000 | Loss: 0.00151979
Iteration 43/1000 | Loss: 0.00025567
Iteration 44/1000 | Loss: 0.00017617
Iteration 45/1000 | Loss: 0.00026874
Iteration 46/1000 | Loss: 0.00011820
Iteration 47/1000 | Loss: 0.00013706
Iteration 48/1000 | Loss: 0.00010425
Iteration 49/1000 | Loss: 0.00007227
Iteration 50/1000 | Loss: 0.00011374
Iteration 51/1000 | Loss: 0.00009989
Iteration 52/1000 | Loss: 0.00007597
Iteration 53/1000 | Loss: 0.00010710
Iteration 54/1000 | Loss: 0.00006873
Iteration 55/1000 | Loss: 0.00005927
Iteration 56/1000 | Loss: 0.00005484
Iteration 57/1000 | Loss: 0.00005259
Iteration 58/1000 | Loss: 0.00005086
Iteration 59/1000 | Loss: 0.00007299
Iteration 60/1000 | Loss: 0.00006136
Iteration 61/1000 | Loss: 0.00005434
Iteration 62/1000 | Loss: 0.00005035
Iteration 63/1000 | Loss: 0.00004883
Iteration 64/1000 | Loss: 0.00005704
Iteration 65/1000 | Loss: 0.00013218
Iteration 66/1000 | Loss: 0.00030163
Iteration 67/1000 | Loss: 0.00005582
Iteration 68/1000 | Loss: 0.00004985
Iteration 69/1000 | Loss: 0.00006488
Iteration 70/1000 | Loss: 0.00004981
Iteration 71/1000 | Loss: 0.00004702
Iteration 72/1000 | Loss: 0.00004568
Iteration 73/1000 | Loss: 0.00004496
Iteration 74/1000 | Loss: 0.00004462
Iteration 75/1000 | Loss: 0.00004419
Iteration 76/1000 | Loss: 0.00019880
Iteration 77/1000 | Loss: 0.00010901
Iteration 78/1000 | Loss: 0.00004788
Iteration 79/1000 | Loss: 0.00004515
Iteration 80/1000 | Loss: 0.00020082
Iteration 81/1000 | Loss: 0.00011971
Iteration 82/1000 | Loss: 0.00004633
Iteration 83/1000 | Loss: 0.00004414
Iteration 84/1000 | Loss: 0.00004390
Iteration 85/1000 | Loss: 0.00017342
Iteration 86/1000 | Loss: 0.00017038
Iteration 87/1000 | Loss: 0.00015976
Iteration 88/1000 | Loss: 0.00013934
Iteration 89/1000 | Loss: 0.00004397
Iteration 90/1000 | Loss: 0.00016157
Iteration 91/1000 | Loss: 0.00012469
Iteration 92/1000 | Loss: 0.00004406
Iteration 93/1000 | Loss: 0.00016049
Iteration 94/1000 | Loss: 0.00012221
Iteration 95/1000 | Loss: 0.00006009
Iteration 96/1000 | Loss: 0.00005500
Iteration 97/1000 | Loss: 0.00005409
Iteration 98/1000 | Loss: 0.00004406
Iteration 99/1000 | Loss: 0.00004308
Iteration 100/1000 | Loss: 0.00004276
Iteration 101/1000 | Loss: 0.00004260
Iteration 102/1000 | Loss: 0.00004255
Iteration 103/1000 | Loss: 0.00004253
Iteration 104/1000 | Loss: 0.00004252
Iteration 105/1000 | Loss: 0.00004252
Iteration 106/1000 | Loss: 0.00004249
Iteration 107/1000 | Loss: 0.00004248
Iteration 108/1000 | Loss: 0.00004247
Iteration 109/1000 | Loss: 0.00004247
Iteration 110/1000 | Loss: 0.00004245
Iteration 111/1000 | Loss: 0.00004245
Iteration 112/1000 | Loss: 0.00004245
Iteration 113/1000 | Loss: 0.00004245
Iteration 114/1000 | Loss: 0.00004244
Iteration 115/1000 | Loss: 0.00004244
Iteration 116/1000 | Loss: 0.00004243
Iteration 117/1000 | Loss: 0.00004243
Iteration 118/1000 | Loss: 0.00004243
Iteration 119/1000 | Loss: 0.00004243
Iteration 120/1000 | Loss: 0.00004243
Iteration 121/1000 | Loss: 0.00004243
Iteration 122/1000 | Loss: 0.00004243
Iteration 123/1000 | Loss: 0.00004243
Iteration 124/1000 | Loss: 0.00004243
Iteration 125/1000 | Loss: 0.00004243
Iteration 126/1000 | Loss: 0.00004242
Iteration 127/1000 | Loss: 0.00004241
Iteration 128/1000 | Loss: 0.00004241
Iteration 129/1000 | Loss: 0.00004239
Iteration 130/1000 | Loss: 0.00004239
Iteration 131/1000 | Loss: 0.00004238
Iteration 132/1000 | Loss: 0.00004238
Iteration 133/1000 | Loss: 0.00004238
Iteration 134/1000 | Loss: 0.00004236
Iteration 135/1000 | Loss: 0.00004236
Iteration 136/1000 | Loss: 0.00004236
Iteration 137/1000 | Loss: 0.00004236
Iteration 138/1000 | Loss: 0.00004236
Iteration 139/1000 | Loss: 0.00004236
Iteration 140/1000 | Loss: 0.00004236
Iteration 141/1000 | Loss: 0.00004236
Iteration 142/1000 | Loss: 0.00004236
Iteration 143/1000 | Loss: 0.00013144
Iteration 144/1000 | Loss: 0.00010377
Iteration 145/1000 | Loss: 0.00016811
Iteration 146/1000 | Loss: 0.00011501
Iteration 147/1000 | Loss: 0.00015929
Iteration 148/1000 | Loss: 0.00011045
Iteration 149/1000 | Loss: 0.00012183
Iteration 150/1000 | Loss: 0.00010471
Iteration 151/1000 | Loss: 0.00014448
Iteration 152/1000 | Loss: 0.00011413
Iteration 153/1000 | Loss: 0.00004557
Iteration 154/1000 | Loss: 0.00004367
Iteration 155/1000 | Loss: 0.00015394
Iteration 156/1000 | Loss: 0.00011598
Iteration 157/1000 | Loss: 0.00012768
Iteration 158/1000 | Loss: 0.00016848
Iteration 159/1000 | Loss: 0.00004517
Iteration 160/1000 | Loss: 0.00006650
Iteration 161/1000 | Loss: 0.00004335
Iteration 162/1000 | Loss: 0.00020967
Iteration 163/1000 | Loss: 0.00006977
Iteration 164/1000 | Loss: 0.00004780
Iteration 165/1000 | Loss: 0.00004481
Iteration 166/1000 | Loss: 0.00004350
Iteration 167/1000 | Loss: 0.00004302
Iteration 168/1000 | Loss: 0.00004259
Iteration 169/1000 | Loss: 0.00004252
Iteration 170/1000 | Loss: 0.00004229
Iteration 171/1000 | Loss: 0.00004222
Iteration 172/1000 | Loss: 0.00004215
Iteration 173/1000 | Loss: 0.00004201
Iteration 174/1000 | Loss: 0.00004190
Iteration 175/1000 | Loss: 0.00004184
Iteration 176/1000 | Loss: 0.00004184
Iteration 177/1000 | Loss: 0.00004183
Iteration 178/1000 | Loss: 0.00004182
Iteration 179/1000 | Loss: 0.00004182
Iteration 180/1000 | Loss: 0.00004180
Iteration 181/1000 | Loss: 0.00004178
Iteration 182/1000 | Loss: 0.00004178
Iteration 183/1000 | Loss: 0.00004177
Iteration 184/1000 | Loss: 0.00004177
Iteration 185/1000 | Loss: 0.00004177
Iteration 186/1000 | Loss: 0.00004177
Iteration 187/1000 | Loss: 0.00004177
Iteration 188/1000 | Loss: 0.00004177
Iteration 189/1000 | Loss: 0.00004176
Iteration 190/1000 | Loss: 0.00004176
Iteration 191/1000 | Loss: 0.00004176
Iteration 192/1000 | Loss: 0.00004175
Iteration 193/1000 | Loss: 0.00004175
Iteration 194/1000 | Loss: 0.00004175
Iteration 195/1000 | Loss: 0.00004175
Iteration 196/1000 | Loss: 0.00004175
Iteration 197/1000 | Loss: 0.00004174
Iteration 198/1000 | Loss: 0.00004174
Iteration 199/1000 | Loss: 0.00004174
Iteration 200/1000 | Loss: 0.00004174
Iteration 201/1000 | Loss: 0.00004173
Iteration 202/1000 | Loss: 0.00004173
Iteration 203/1000 | Loss: 0.00004173
Iteration 204/1000 | Loss: 0.00004173
Iteration 205/1000 | Loss: 0.00004173
Iteration 206/1000 | Loss: 0.00004172
Iteration 207/1000 | Loss: 0.00004172
Iteration 208/1000 | Loss: 0.00004172
Iteration 209/1000 | Loss: 0.00004172
Iteration 210/1000 | Loss: 0.00004171
Iteration 211/1000 | Loss: 0.00004171
Iteration 212/1000 | Loss: 0.00004171
Iteration 213/1000 | Loss: 0.00004171
Iteration 214/1000 | Loss: 0.00004171
Iteration 215/1000 | Loss: 0.00004171
Iteration 216/1000 | Loss: 0.00004171
Iteration 217/1000 | Loss: 0.00004171
Iteration 218/1000 | Loss: 0.00004171
Iteration 219/1000 | Loss: 0.00004171
Iteration 220/1000 | Loss: 0.00004171
Iteration 221/1000 | Loss: 0.00004171
Iteration 222/1000 | Loss: 0.00004171
Iteration 223/1000 | Loss: 0.00004171
Iteration 224/1000 | Loss: 0.00004171
Iteration 225/1000 | Loss: 0.00004171
Iteration 226/1000 | Loss: 0.00004170
Iteration 227/1000 | Loss: 0.00004170
Iteration 228/1000 | Loss: 0.00004170
Iteration 229/1000 | Loss: 0.00004170
Iteration 230/1000 | Loss: 0.00004170
Iteration 231/1000 | Loss: 0.00004170
Iteration 232/1000 | Loss: 0.00004170
Iteration 233/1000 | Loss: 0.00004170
Iteration 234/1000 | Loss: 0.00004170
Iteration 235/1000 | Loss: 0.00004170
Iteration 236/1000 | Loss: 0.00004170
Iteration 237/1000 | Loss: 0.00004170
Iteration 238/1000 | Loss: 0.00004170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [4.170262400293723e-05, 4.170262400293723e-05, 4.170262400293723e-05, 4.170262400293723e-05, 4.170262400293723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.170262400293723e-05

Optimization complete. Final v2v error: 4.016180992126465 mm

Highest mean error: 10.895787239074707 mm for frame 234

Lowest mean error: 3.3304388523101807 mm for frame 40

Saving results

Total time: 269.9751591682434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405466
Iteration 2/25 | Loss: 0.00137775
Iteration 3/25 | Loss: 0.00122390
Iteration 4/25 | Loss: 0.00121099
Iteration 5/25 | Loss: 0.00120912
Iteration 6/25 | Loss: 0.00120867
Iteration 7/25 | Loss: 0.00120867
Iteration 8/25 | Loss: 0.00120867
Iteration 9/25 | Loss: 0.00120867
Iteration 10/25 | Loss: 0.00120867
Iteration 11/25 | Loss: 0.00120867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012086667120456696, 0.0012086667120456696, 0.0012086667120456696, 0.0012086667120456696, 0.0012086667120456696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012086667120456696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34532952
Iteration 2/25 | Loss: 0.00079795
Iteration 3/25 | Loss: 0.00079794
Iteration 4/25 | Loss: 0.00079794
Iteration 5/25 | Loss: 0.00079794
Iteration 6/25 | Loss: 0.00079794
Iteration 7/25 | Loss: 0.00079794
Iteration 8/25 | Loss: 0.00079794
Iteration 9/25 | Loss: 0.00079794
Iteration 10/25 | Loss: 0.00079794
Iteration 11/25 | Loss: 0.00079794
Iteration 12/25 | Loss: 0.00079794
Iteration 13/25 | Loss: 0.00079794
Iteration 14/25 | Loss: 0.00079794
Iteration 15/25 | Loss: 0.00079794
Iteration 16/25 | Loss: 0.00079794
Iteration 17/25 | Loss: 0.00079794
Iteration 18/25 | Loss: 0.00079794
Iteration 19/25 | Loss: 0.00079794
Iteration 20/25 | Loss: 0.00079794
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007979373913258314, 0.0007979373913258314, 0.0007979373913258314, 0.0007979373913258314, 0.0007979373913258314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007979373913258314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079794
Iteration 2/1000 | Loss: 0.00002527
Iteration 3/1000 | Loss: 0.00001928
Iteration 4/1000 | Loss: 0.00001689
Iteration 5/1000 | Loss: 0.00001600
Iteration 6/1000 | Loss: 0.00001522
Iteration 7/1000 | Loss: 0.00001469
Iteration 8/1000 | Loss: 0.00001437
Iteration 9/1000 | Loss: 0.00001405
Iteration 10/1000 | Loss: 0.00001381
Iteration 11/1000 | Loss: 0.00001366
Iteration 12/1000 | Loss: 0.00001362
Iteration 13/1000 | Loss: 0.00001359
Iteration 14/1000 | Loss: 0.00001354
Iteration 15/1000 | Loss: 0.00001354
Iteration 16/1000 | Loss: 0.00001352
Iteration 17/1000 | Loss: 0.00001351
Iteration 18/1000 | Loss: 0.00001343
Iteration 19/1000 | Loss: 0.00001338
Iteration 20/1000 | Loss: 0.00001338
Iteration 21/1000 | Loss: 0.00001338
Iteration 22/1000 | Loss: 0.00001338
Iteration 23/1000 | Loss: 0.00001338
Iteration 24/1000 | Loss: 0.00001338
Iteration 25/1000 | Loss: 0.00001338
Iteration 26/1000 | Loss: 0.00001338
Iteration 27/1000 | Loss: 0.00001338
Iteration 28/1000 | Loss: 0.00001337
Iteration 29/1000 | Loss: 0.00001337
Iteration 30/1000 | Loss: 0.00001337
Iteration 31/1000 | Loss: 0.00001337
Iteration 32/1000 | Loss: 0.00001336
Iteration 33/1000 | Loss: 0.00001336
Iteration 34/1000 | Loss: 0.00001336
Iteration 35/1000 | Loss: 0.00001335
Iteration 36/1000 | Loss: 0.00001334
Iteration 37/1000 | Loss: 0.00001334
Iteration 38/1000 | Loss: 0.00001333
Iteration 39/1000 | Loss: 0.00001332
Iteration 40/1000 | Loss: 0.00001332
Iteration 41/1000 | Loss: 0.00001331
Iteration 42/1000 | Loss: 0.00001330
Iteration 43/1000 | Loss: 0.00001328
Iteration 44/1000 | Loss: 0.00001325
Iteration 45/1000 | Loss: 0.00001325
Iteration 46/1000 | Loss: 0.00001324
Iteration 47/1000 | Loss: 0.00001324
Iteration 48/1000 | Loss: 0.00001323
Iteration 49/1000 | Loss: 0.00001323
Iteration 50/1000 | Loss: 0.00001323
Iteration 51/1000 | Loss: 0.00001323
Iteration 52/1000 | Loss: 0.00001322
Iteration 53/1000 | Loss: 0.00001322
Iteration 54/1000 | Loss: 0.00001322
Iteration 55/1000 | Loss: 0.00001322
Iteration 56/1000 | Loss: 0.00001321
Iteration 57/1000 | Loss: 0.00001320
Iteration 58/1000 | Loss: 0.00001319
Iteration 59/1000 | Loss: 0.00001319
Iteration 60/1000 | Loss: 0.00001317
Iteration 61/1000 | Loss: 0.00001316
Iteration 62/1000 | Loss: 0.00001316
Iteration 63/1000 | Loss: 0.00001314
Iteration 64/1000 | Loss: 0.00001312
Iteration 65/1000 | Loss: 0.00001311
Iteration 66/1000 | Loss: 0.00001310
Iteration 67/1000 | Loss: 0.00001309
Iteration 68/1000 | Loss: 0.00001308
Iteration 69/1000 | Loss: 0.00001307
Iteration 70/1000 | Loss: 0.00001305
Iteration 71/1000 | Loss: 0.00001304
Iteration 72/1000 | Loss: 0.00001304
Iteration 73/1000 | Loss: 0.00001303
Iteration 74/1000 | Loss: 0.00001302
Iteration 75/1000 | Loss: 0.00001301
Iteration 76/1000 | Loss: 0.00001301
Iteration 77/1000 | Loss: 0.00001300
Iteration 78/1000 | Loss: 0.00001300
Iteration 79/1000 | Loss: 0.00001300
Iteration 80/1000 | Loss: 0.00001300
Iteration 81/1000 | Loss: 0.00001300
Iteration 82/1000 | Loss: 0.00001300
Iteration 83/1000 | Loss: 0.00001300
Iteration 84/1000 | Loss: 0.00001300
Iteration 85/1000 | Loss: 0.00001299
Iteration 86/1000 | Loss: 0.00001298
Iteration 87/1000 | Loss: 0.00001298
Iteration 88/1000 | Loss: 0.00001298
Iteration 89/1000 | Loss: 0.00001298
Iteration 90/1000 | Loss: 0.00001298
Iteration 91/1000 | Loss: 0.00001298
Iteration 92/1000 | Loss: 0.00001298
Iteration 93/1000 | Loss: 0.00001297
Iteration 94/1000 | Loss: 0.00001297
Iteration 95/1000 | Loss: 0.00001297
Iteration 96/1000 | Loss: 0.00001296
Iteration 97/1000 | Loss: 0.00001295
Iteration 98/1000 | Loss: 0.00001293
Iteration 99/1000 | Loss: 0.00001293
Iteration 100/1000 | Loss: 0.00001293
Iteration 101/1000 | Loss: 0.00001293
Iteration 102/1000 | Loss: 0.00001293
Iteration 103/1000 | Loss: 0.00001293
Iteration 104/1000 | Loss: 0.00001293
Iteration 105/1000 | Loss: 0.00001293
Iteration 106/1000 | Loss: 0.00001293
Iteration 107/1000 | Loss: 0.00001293
Iteration 108/1000 | Loss: 0.00001293
Iteration 109/1000 | Loss: 0.00001292
Iteration 110/1000 | Loss: 0.00001292
Iteration 111/1000 | Loss: 0.00001292
Iteration 112/1000 | Loss: 0.00001292
Iteration 113/1000 | Loss: 0.00001292
Iteration 114/1000 | Loss: 0.00001291
Iteration 115/1000 | Loss: 0.00001291
Iteration 116/1000 | Loss: 0.00001291
Iteration 117/1000 | Loss: 0.00001291
Iteration 118/1000 | Loss: 0.00001291
Iteration 119/1000 | Loss: 0.00001291
Iteration 120/1000 | Loss: 0.00001290
Iteration 121/1000 | Loss: 0.00001290
Iteration 122/1000 | Loss: 0.00001290
Iteration 123/1000 | Loss: 0.00001290
Iteration 124/1000 | Loss: 0.00001290
Iteration 125/1000 | Loss: 0.00001290
Iteration 126/1000 | Loss: 0.00001290
Iteration 127/1000 | Loss: 0.00001290
Iteration 128/1000 | Loss: 0.00001290
Iteration 129/1000 | Loss: 0.00001290
Iteration 130/1000 | Loss: 0.00001290
Iteration 131/1000 | Loss: 0.00001290
Iteration 132/1000 | Loss: 0.00001289
Iteration 133/1000 | Loss: 0.00001289
Iteration 134/1000 | Loss: 0.00001289
Iteration 135/1000 | Loss: 0.00001289
Iteration 136/1000 | Loss: 0.00001289
Iteration 137/1000 | Loss: 0.00001289
Iteration 138/1000 | Loss: 0.00001289
Iteration 139/1000 | Loss: 0.00001289
Iteration 140/1000 | Loss: 0.00001289
Iteration 141/1000 | Loss: 0.00001289
Iteration 142/1000 | Loss: 0.00001288
Iteration 143/1000 | Loss: 0.00001288
Iteration 144/1000 | Loss: 0.00001288
Iteration 145/1000 | Loss: 0.00001288
Iteration 146/1000 | Loss: 0.00001288
Iteration 147/1000 | Loss: 0.00001288
Iteration 148/1000 | Loss: 0.00001288
Iteration 149/1000 | Loss: 0.00001288
Iteration 150/1000 | Loss: 0.00001287
Iteration 151/1000 | Loss: 0.00001287
Iteration 152/1000 | Loss: 0.00001287
Iteration 153/1000 | Loss: 0.00001287
Iteration 154/1000 | Loss: 0.00001287
Iteration 155/1000 | Loss: 0.00001287
Iteration 156/1000 | Loss: 0.00001287
Iteration 157/1000 | Loss: 0.00001287
Iteration 158/1000 | Loss: 0.00001286
Iteration 159/1000 | Loss: 0.00001286
Iteration 160/1000 | Loss: 0.00001286
Iteration 161/1000 | Loss: 0.00001286
Iteration 162/1000 | Loss: 0.00001286
Iteration 163/1000 | Loss: 0.00001286
Iteration 164/1000 | Loss: 0.00001285
Iteration 165/1000 | Loss: 0.00001285
Iteration 166/1000 | Loss: 0.00001285
Iteration 167/1000 | Loss: 0.00001285
Iteration 168/1000 | Loss: 0.00001285
Iteration 169/1000 | Loss: 0.00001285
Iteration 170/1000 | Loss: 0.00001285
Iteration 171/1000 | Loss: 0.00001285
Iteration 172/1000 | Loss: 0.00001285
Iteration 173/1000 | Loss: 0.00001285
Iteration 174/1000 | Loss: 0.00001285
Iteration 175/1000 | Loss: 0.00001285
Iteration 176/1000 | Loss: 0.00001285
Iteration 177/1000 | Loss: 0.00001285
Iteration 178/1000 | Loss: 0.00001285
Iteration 179/1000 | Loss: 0.00001285
Iteration 180/1000 | Loss: 0.00001285
Iteration 181/1000 | Loss: 0.00001284
Iteration 182/1000 | Loss: 0.00001284
Iteration 183/1000 | Loss: 0.00001284
Iteration 184/1000 | Loss: 0.00001284
Iteration 185/1000 | Loss: 0.00001284
Iteration 186/1000 | Loss: 0.00001284
Iteration 187/1000 | Loss: 0.00001284
Iteration 188/1000 | Loss: 0.00001284
Iteration 189/1000 | Loss: 0.00001283
Iteration 190/1000 | Loss: 0.00001283
Iteration 191/1000 | Loss: 0.00001283
Iteration 192/1000 | Loss: 0.00001283
Iteration 193/1000 | Loss: 0.00001283
Iteration 194/1000 | Loss: 0.00001283
Iteration 195/1000 | Loss: 0.00001283
Iteration 196/1000 | Loss: 0.00001283
Iteration 197/1000 | Loss: 0.00001283
Iteration 198/1000 | Loss: 0.00001282
Iteration 199/1000 | Loss: 0.00001282
Iteration 200/1000 | Loss: 0.00001282
Iteration 201/1000 | Loss: 0.00001281
Iteration 202/1000 | Loss: 0.00001281
Iteration 203/1000 | Loss: 0.00001281
Iteration 204/1000 | Loss: 0.00001281
Iteration 205/1000 | Loss: 0.00001281
Iteration 206/1000 | Loss: 0.00001281
Iteration 207/1000 | Loss: 0.00001280
Iteration 208/1000 | Loss: 0.00001280
Iteration 209/1000 | Loss: 0.00001280
Iteration 210/1000 | Loss: 0.00001279
Iteration 211/1000 | Loss: 0.00001279
Iteration 212/1000 | Loss: 0.00001279
Iteration 213/1000 | Loss: 0.00001278
Iteration 214/1000 | Loss: 0.00001278
Iteration 215/1000 | Loss: 0.00001278
Iteration 216/1000 | Loss: 0.00001278
Iteration 217/1000 | Loss: 0.00001278
Iteration 218/1000 | Loss: 0.00001278
Iteration 219/1000 | Loss: 0.00001278
Iteration 220/1000 | Loss: 0.00001278
Iteration 221/1000 | Loss: 0.00001278
Iteration 222/1000 | Loss: 0.00001278
Iteration 223/1000 | Loss: 0.00001278
Iteration 224/1000 | Loss: 0.00001278
Iteration 225/1000 | Loss: 0.00001278
Iteration 226/1000 | Loss: 0.00001278
Iteration 227/1000 | Loss: 0.00001278
Iteration 228/1000 | Loss: 0.00001278
Iteration 229/1000 | Loss: 0.00001278
Iteration 230/1000 | Loss: 0.00001278
Iteration 231/1000 | Loss: 0.00001278
Iteration 232/1000 | Loss: 0.00001278
Iteration 233/1000 | Loss: 0.00001278
Iteration 234/1000 | Loss: 0.00001278
Iteration 235/1000 | Loss: 0.00001278
Iteration 236/1000 | Loss: 0.00001278
Iteration 237/1000 | Loss: 0.00001278
Iteration 238/1000 | Loss: 0.00001278
Iteration 239/1000 | Loss: 0.00001278
Iteration 240/1000 | Loss: 0.00001278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.277775845665019e-05, 1.277775845665019e-05, 1.277775845665019e-05, 1.277775845665019e-05, 1.277775845665019e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.277775845665019e-05

Optimization complete. Final v2v error: 3.0633492469787598 mm

Highest mean error: 3.1850736141204834 mm for frame 26

Lowest mean error: 2.913752555847168 mm for frame 2

Saving results

Total time: 42.66463828086853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493282
Iteration 2/25 | Loss: 0.00162222
Iteration 3/25 | Loss: 0.00133170
Iteration 4/25 | Loss: 0.00128762
Iteration 5/25 | Loss: 0.00128129
Iteration 6/25 | Loss: 0.00127954
Iteration 7/25 | Loss: 0.00127930
Iteration 8/25 | Loss: 0.00127930
Iteration 9/25 | Loss: 0.00127930
Iteration 10/25 | Loss: 0.00127930
Iteration 11/25 | Loss: 0.00127930
Iteration 12/25 | Loss: 0.00127930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012793013593181968, 0.0012793013593181968, 0.0012793013593181968, 0.0012793013593181968, 0.0012793013593181968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012793013593181968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32981873
Iteration 2/25 | Loss: 0.00081049
Iteration 3/25 | Loss: 0.00081049
Iteration 4/25 | Loss: 0.00081049
Iteration 5/25 | Loss: 0.00081049
Iteration 6/25 | Loss: 0.00081049
Iteration 7/25 | Loss: 0.00081049
Iteration 8/25 | Loss: 0.00081049
Iteration 9/25 | Loss: 0.00081049
Iteration 10/25 | Loss: 0.00081049
Iteration 11/25 | Loss: 0.00081049
Iteration 12/25 | Loss: 0.00081049
Iteration 13/25 | Loss: 0.00081049
Iteration 14/25 | Loss: 0.00081049
Iteration 15/25 | Loss: 0.00081049
Iteration 16/25 | Loss: 0.00081049
Iteration 17/25 | Loss: 0.00081049
Iteration 18/25 | Loss: 0.00081049
Iteration 19/25 | Loss: 0.00081049
Iteration 20/25 | Loss: 0.00081049
Iteration 21/25 | Loss: 0.00081049
Iteration 22/25 | Loss: 0.00081049
Iteration 23/25 | Loss: 0.00081049
Iteration 24/25 | Loss: 0.00081049
Iteration 25/25 | Loss: 0.00081049

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081049
Iteration 2/1000 | Loss: 0.00004855
Iteration 3/1000 | Loss: 0.00002612
Iteration 4/1000 | Loss: 0.00002061
Iteration 5/1000 | Loss: 0.00001950
Iteration 6/1000 | Loss: 0.00001889
Iteration 7/1000 | Loss: 0.00001838
Iteration 8/1000 | Loss: 0.00001808
Iteration 9/1000 | Loss: 0.00001807
Iteration 10/1000 | Loss: 0.00001785
Iteration 11/1000 | Loss: 0.00001767
Iteration 12/1000 | Loss: 0.00001763
Iteration 13/1000 | Loss: 0.00001762
Iteration 14/1000 | Loss: 0.00001760
Iteration 15/1000 | Loss: 0.00001749
Iteration 16/1000 | Loss: 0.00001749
Iteration 17/1000 | Loss: 0.00001749
Iteration 18/1000 | Loss: 0.00001748
Iteration 19/1000 | Loss: 0.00001748
Iteration 20/1000 | Loss: 0.00001748
Iteration 21/1000 | Loss: 0.00001748
Iteration 22/1000 | Loss: 0.00001748
Iteration 23/1000 | Loss: 0.00001748
Iteration 24/1000 | Loss: 0.00001748
Iteration 25/1000 | Loss: 0.00001748
Iteration 26/1000 | Loss: 0.00001748
Iteration 27/1000 | Loss: 0.00001748
Iteration 28/1000 | Loss: 0.00001747
Iteration 29/1000 | Loss: 0.00001747
Iteration 30/1000 | Loss: 0.00001746
Iteration 31/1000 | Loss: 0.00001743
Iteration 32/1000 | Loss: 0.00001735
Iteration 33/1000 | Loss: 0.00001731
Iteration 34/1000 | Loss: 0.00001731
Iteration 35/1000 | Loss: 0.00001729
Iteration 36/1000 | Loss: 0.00001729
Iteration 37/1000 | Loss: 0.00001727
Iteration 38/1000 | Loss: 0.00001726
Iteration 39/1000 | Loss: 0.00001726
Iteration 40/1000 | Loss: 0.00001725
Iteration 41/1000 | Loss: 0.00001725
Iteration 42/1000 | Loss: 0.00001724
Iteration 43/1000 | Loss: 0.00001723
Iteration 44/1000 | Loss: 0.00001723
Iteration 45/1000 | Loss: 0.00001722
Iteration 46/1000 | Loss: 0.00001722
Iteration 47/1000 | Loss: 0.00001722
Iteration 48/1000 | Loss: 0.00001722
Iteration 49/1000 | Loss: 0.00001722
Iteration 50/1000 | Loss: 0.00001721
Iteration 51/1000 | Loss: 0.00001721
Iteration 52/1000 | Loss: 0.00001721
Iteration 53/1000 | Loss: 0.00001720
Iteration 54/1000 | Loss: 0.00001719
Iteration 55/1000 | Loss: 0.00001719
Iteration 56/1000 | Loss: 0.00001716
Iteration 57/1000 | Loss: 0.00001715
Iteration 58/1000 | Loss: 0.00001714
Iteration 59/1000 | Loss: 0.00001713
Iteration 60/1000 | Loss: 0.00001713
Iteration 61/1000 | Loss: 0.00001712
Iteration 62/1000 | Loss: 0.00001712
Iteration 63/1000 | Loss: 0.00001712
Iteration 64/1000 | Loss: 0.00001712
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001712
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001712
Iteration 70/1000 | Loss: 0.00001712
Iteration 71/1000 | Loss: 0.00001712
Iteration 72/1000 | Loss: 0.00001712
Iteration 73/1000 | Loss: 0.00001711
Iteration 74/1000 | Loss: 0.00001711
Iteration 75/1000 | Loss: 0.00001711
Iteration 76/1000 | Loss: 0.00001711
Iteration 77/1000 | Loss: 0.00001711
Iteration 78/1000 | Loss: 0.00001711
Iteration 79/1000 | Loss: 0.00001711
Iteration 80/1000 | Loss: 0.00001711
Iteration 81/1000 | Loss: 0.00001711
Iteration 82/1000 | Loss: 0.00001711
Iteration 83/1000 | Loss: 0.00001711
Iteration 84/1000 | Loss: 0.00001711
Iteration 85/1000 | Loss: 0.00001711
Iteration 86/1000 | Loss: 0.00001711
Iteration 87/1000 | Loss: 0.00001711
Iteration 88/1000 | Loss: 0.00001711
Iteration 89/1000 | Loss: 0.00001711
Iteration 90/1000 | Loss: 0.00001711
Iteration 91/1000 | Loss: 0.00001711
Iteration 92/1000 | Loss: 0.00001711
Iteration 93/1000 | Loss: 0.00001711
Iteration 94/1000 | Loss: 0.00001711
Iteration 95/1000 | Loss: 0.00001711
Iteration 96/1000 | Loss: 0.00001711
Iteration 97/1000 | Loss: 0.00001711
Iteration 98/1000 | Loss: 0.00001711
Iteration 99/1000 | Loss: 0.00001711
Iteration 100/1000 | Loss: 0.00001711
Iteration 101/1000 | Loss: 0.00001711
Iteration 102/1000 | Loss: 0.00001711
Iteration 103/1000 | Loss: 0.00001711
Iteration 104/1000 | Loss: 0.00001711
Iteration 105/1000 | Loss: 0.00001711
Iteration 106/1000 | Loss: 0.00001711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.710723699943628e-05, 1.710723699943628e-05, 1.710723699943628e-05, 1.710723699943628e-05, 1.710723699943628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.710723699943628e-05

Optimization complete. Final v2v error: 3.586301326751709 mm

Highest mean error: 3.7157249450683594 mm for frame 51

Lowest mean error: 3.2396063804626465 mm for frame 2

Saving results

Total time: 32.77451419830322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00929937
Iteration 2/25 | Loss: 0.00155477
Iteration 3/25 | Loss: 0.00139137
Iteration 4/25 | Loss: 0.00136753
Iteration 5/25 | Loss: 0.00135918
Iteration 6/25 | Loss: 0.00134770
Iteration 7/25 | Loss: 0.00134357
Iteration 8/25 | Loss: 0.00133834
Iteration 9/25 | Loss: 0.00133729
Iteration 10/25 | Loss: 0.00133696
Iteration 11/25 | Loss: 0.00133685
Iteration 12/25 | Loss: 0.00133682
Iteration 13/25 | Loss: 0.00133681
Iteration 14/25 | Loss: 0.00133676
Iteration 15/25 | Loss: 0.00133676
Iteration 16/25 | Loss: 0.00133676
Iteration 17/25 | Loss: 0.00133676
Iteration 18/25 | Loss: 0.00133676
Iteration 19/25 | Loss: 0.00133676
Iteration 20/25 | Loss: 0.00133676
Iteration 21/25 | Loss: 0.00133676
Iteration 22/25 | Loss: 0.00133676
Iteration 23/25 | Loss: 0.00133675
Iteration 24/25 | Loss: 0.00133675
Iteration 25/25 | Loss: 0.00133675

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07743764
Iteration 2/25 | Loss: 0.00104750
Iteration 3/25 | Loss: 0.00104746
Iteration 4/25 | Loss: 0.00104746
Iteration 5/25 | Loss: 0.00104746
Iteration 6/25 | Loss: 0.00104746
Iteration 7/25 | Loss: 0.00104746
Iteration 8/25 | Loss: 0.00104746
Iteration 9/25 | Loss: 0.00104746
Iteration 10/25 | Loss: 0.00104746
Iteration 11/25 | Loss: 0.00104746
Iteration 12/25 | Loss: 0.00104746
Iteration 13/25 | Loss: 0.00104746
Iteration 14/25 | Loss: 0.00104746
Iteration 15/25 | Loss: 0.00104746
Iteration 16/25 | Loss: 0.00104746
Iteration 17/25 | Loss: 0.00104746
Iteration 18/25 | Loss: 0.00104746
Iteration 19/25 | Loss: 0.00104746
Iteration 20/25 | Loss: 0.00104746
Iteration 21/25 | Loss: 0.00104746
Iteration 22/25 | Loss: 0.00104746
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010474580340087414, 0.0010474580340087414, 0.0010474580340087414, 0.0010474580340087414, 0.0010474580340087414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010474580340087414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104746
Iteration 2/1000 | Loss: 0.00004684
Iteration 3/1000 | Loss: 0.00003181
Iteration 4/1000 | Loss: 0.00002874
Iteration 5/1000 | Loss: 0.00002717
Iteration 6/1000 | Loss: 0.00002656
Iteration 7/1000 | Loss: 0.00002609
Iteration 8/1000 | Loss: 0.00002578
Iteration 9/1000 | Loss: 0.00002544
Iteration 10/1000 | Loss: 0.00002529
Iteration 11/1000 | Loss: 0.00002513
Iteration 12/1000 | Loss: 0.00002494
Iteration 13/1000 | Loss: 0.00002488
Iteration 14/1000 | Loss: 0.00002481
Iteration 15/1000 | Loss: 0.00002472
Iteration 16/1000 | Loss: 0.00002471
Iteration 17/1000 | Loss: 0.00002470
Iteration 18/1000 | Loss: 0.00002470
Iteration 19/1000 | Loss: 0.00002467
Iteration 20/1000 | Loss: 0.00002466
Iteration 21/1000 | Loss: 0.00002465
Iteration 22/1000 | Loss: 0.00002464
Iteration 23/1000 | Loss: 0.00002461
Iteration 24/1000 | Loss: 0.00002461
Iteration 25/1000 | Loss: 0.00002461
Iteration 26/1000 | Loss: 0.00002461
Iteration 27/1000 | Loss: 0.00002460
Iteration 28/1000 | Loss: 0.00002460
Iteration 29/1000 | Loss: 0.00002460
Iteration 30/1000 | Loss: 0.00002459
Iteration 31/1000 | Loss: 0.00002456
Iteration 32/1000 | Loss: 0.00002456
Iteration 33/1000 | Loss: 0.00002456
Iteration 34/1000 | Loss: 0.00002456
Iteration 35/1000 | Loss: 0.00002456
Iteration 36/1000 | Loss: 0.00002456
Iteration 37/1000 | Loss: 0.00002456
Iteration 38/1000 | Loss: 0.00002456
Iteration 39/1000 | Loss: 0.00002455
Iteration 40/1000 | Loss: 0.00002455
Iteration 41/1000 | Loss: 0.00002455
Iteration 42/1000 | Loss: 0.00002455
Iteration 43/1000 | Loss: 0.00002455
Iteration 44/1000 | Loss: 0.00002455
Iteration 45/1000 | Loss: 0.00002453
Iteration 46/1000 | Loss: 0.00002452
Iteration 47/1000 | Loss: 0.00002452
Iteration 48/1000 | Loss: 0.00002452
Iteration 49/1000 | Loss: 0.00002452
Iteration 50/1000 | Loss: 0.00002452
Iteration 51/1000 | Loss: 0.00002452
Iteration 52/1000 | Loss: 0.00002452
Iteration 53/1000 | Loss: 0.00002452
Iteration 54/1000 | Loss: 0.00002451
Iteration 55/1000 | Loss: 0.00002451
Iteration 56/1000 | Loss: 0.00002451
Iteration 57/1000 | Loss: 0.00002451
Iteration 58/1000 | Loss: 0.00002451
Iteration 59/1000 | Loss: 0.00002451
Iteration 60/1000 | Loss: 0.00002451
Iteration 61/1000 | Loss: 0.00002450
Iteration 62/1000 | Loss: 0.00002450
Iteration 63/1000 | Loss: 0.00002450
Iteration 64/1000 | Loss: 0.00002449
Iteration 65/1000 | Loss: 0.00002449
Iteration 66/1000 | Loss: 0.00002449
Iteration 67/1000 | Loss: 0.00002449
Iteration 68/1000 | Loss: 0.00002449
Iteration 69/1000 | Loss: 0.00002449
Iteration 70/1000 | Loss: 0.00002449
Iteration 71/1000 | Loss: 0.00002449
Iteration 72/1000 | Loss: 0.00002448
Iteration 73/1000 | Loss: 0.00002448
Iteration 74/1000 | Loss: 0.00002448
Iteration 75/1000 | Loss: 0.00002448
Iteration 76/1000 | Loss: 0.00002448
Iteration 77/1000 | Loss: 0.00002448
Iteration 78/1000 | Loss: 0.00002448
Iteration 79/1000 | Loss: 0.00002448
Iteration 80/1000 | Loss: 0.00002448
Iteration 81/1000 | Loss: 0.00002448
Iteration 82/1000 | Loss: 0.00002448
Iteration 83/1000 | Loss: 0.00002448
Iteration 84/1000 | Loss: 0.00002447
Iteration 85/1000 | Loss: 0.00002447
Iteration 86/1000 | Loss: 0.00002447
Iteration 87/1000 | Loss: 0.00002447
Iteration 88/1000 | Loss: 0.00002446
Iteration 89/1000 | Loss: 0.00002446
Iteration 90/1000 | Loss: 0.00002446
Iteration 91/1000 | Loss: 0.00002446
Iteration 92/1000 | Loss: 0.00002446
Iteration 93/1000 | Loss: 0.00002445
Iteration 94/1000 | Loss: 0.00002445
Iteration 95/1000 | Loss: 0.00002445
Iteration 96/1000 | Loss: 0.00002445
Iteration 97/1000 | Loss: 0.00002445
Iteration 98/1000 | Loss: 0.00002445
Iteration 99/1000 | Loss: 0.00002444
Iteration 100/1000 | Loss: 0.00002444
Iteration 101/1000 | Loss: 0.00002444
Iteration 102/1000 | Loss: 0.00002444
Iteration 103/1000 | Loss: 0.00002444
Iteration 104/1000 | Loss: 0.00002444
Iteration 105/1000 | Loss: 0.00002444
Iteration 106/1000 | Loss: 0.00002444
Iteration 107/1000 | Loss: 0.00002443
Iteration 108/1000 | Loss: 0.00002443
Iteration 109/1000 | Loss: 0.00002443
Iteration 110/1000 | Loss: 0.00002443
Iteration 111/1000 | Loss: 0.00002442
Iteration 112/1000 | Loss: 0.00002442
Iteration 113/1000 | Loss: 0.00002442
Iteration 114/1000 | Loss: 0.00002442
Iteration 115/1000 | Loss: 0.00002441
Iteration 116/1000 | Loss: 0.00002441
Iteration 117/1000 | Loss: 0.00002440
Iteration 118/1000 | Loss: 0.00002440
Iteration 119/1000 | Loss: 0.00002440
Iteration 120/1000 | Loss: 0.00002440
Iteration 121/1000 | Loss: 0.00002440
Iteration 122/1000 | Loss: 0.00002440
Iteration 123/1000 | Loss: 0.00002440
Iteration 124/1000 | Loss: 0.00002440
Iteration 125/1000 | Loss: 0.00002440
Iteration 126/1000 | Loss: 0.00002440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.4402015696978197e-05, 2.4402015696978197e-05, 2.4402015696978197e-05, 2.4402015696978197e-05, 2.4402015696978197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4402015696978197e-05

Optimization complete. Final v2v error: 3.8793036937713623 mm

Highest mean error: 4.503994464874268 mm for frame 104

Lowest mean error: 3.4155874252319336 mm for frame 147

Saving results

Total time: 53.609129190444946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787456
Iteration 2/25 | Loss: 0.00146296
Iteration 3/25 | Loss: 0.00124444
Iteration 4/25 | Loss: 0.00122297
Iteration 5/25 | Loss: 0.00121844
Iteration 6/25 | Loss: 0.00121751
Iteration 7/25 | Loss: 0.00121751
Iteration 8/25 | Loss: 0.00121751
Iteration 9/25 | Loss: 0.00121751
Iteration 10/25 | Loss: 0.00121751
Iteration 11/25 | Loss: 0.00121751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012175102019682527, 0.0012175102019682527, 0.0012175102019682527, 0.0012175102019682527, 0.0012175102019682527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012175102019682527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.67884469
Iteration 2/25 | Loss: 0.00098316
Iteration 3/25 | Loss: 0.00098310
Iteration 4/25 | Loss: 0.00098310
Iteration 5/25 | Loss: 0.00098310
Iteration 6/25 | Loss: 0.00098310
Iteration 7/25 | Loss: 0.00098310
Iteration 8/25 | Loss: 0.00098310
Iteration 9/25 | Loss: 0.00098310
Iteration 10/25 | Loss: 0.00098310
Iteration 11/25 | Loss: 0.00098310
Iteration 12/25 | Loss: 0.00098310
Iteration 13/25 | Loss: 0.00098310
Iteration 14/25 | Loss: 0.00098310
Iteration 15/25 | Loss: 0.00098310
Iteration 16/25 | Loss: 0.00098310
Iteration 17/25 | Loss: 0.00098310
Iteration 18/25 | Loss: 0.00098310
Iteration 19/25 | Loss: 0.00098310
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009830970084294677, 0.0009830970084294677, 0.0009830970084294677, 0.0009830970084294677, 0.0009830970084294677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009830970084294677

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098310
Iteration 2/1000 | Loss: 0.00003836
Iteration 3/1000 | Loss: 0.00002749
Iteration 4/1000 | Loss: 0.00002471
Iteration 5/1000 | Loss: 0.00002333
Iteration 6/1000 | Loss: 0.00002241
Iteration 7/1000 | Loss: 0.00002177
Iteration 8/1000 | Loss: 0.00002130
Iteration 9/1000 | Loss: 0.00002101
Iteration 10/1000 | Loss: 0.00002061
Iteration 11/1000 | Loss: 0.00002037
Iteration 12/1000 | Loss: 0.00002021
Iteration 13/1000 | Loss: 0.00002010
Iteration 14/1000 | Loss: 0.00002002
Iteration 15/1000 | Loss: 0.00001987
Iteration 16/1000 | Loss: 0.00001986
Iteration 17/1000 | Loss: 0.00001982
Iteration 18/1000 | Loss: 0.00001978
Iteration 19/1000 | Loss: 0.00001978
Iteration 20/1000 | Loss: 0.00001971
Iteration 21/1000 | Loss: 0.00001971
Iteration 22/1000 | Loss: 0.00001969
Iteration 23/1000 | Loss: 0.00001964
Iteration 24/1000 | Loss: 0.00001962
Iteration 25/1000 | Loss: 0.00001958
Iteration 26/1000 | Loss: 0.00001956
Iteration 27/1000 | Loss: 0.00001956
Iteration 28/1000 | Loss: 0.00001955
Iteration 29/1000 | Loss: 0.00001955
Iteration 30/1000 | Loss: 0.00001954
Iteration 31/1000 | Loss: 0.00001954
Iteration 32/1000 | Loss: 0.00001953
Iteration 33/1000 | Loss: 0.00001953
Iteration 34/1000 | Loss: 0.00001953
Iteration 35/1000 | Loss: 0.00001952
Iteration 36/1000 | Loss: 0.00001952
Iteration 37/1000 | Loss: 0.00001952
Iteration 38/1000 | Loss: 0.00001952
Iteration 39/1000 | Loss: 0.00001952
Iteration 40/1000 | Loss: 0.00001952
Iteration 41/1000 | Loss: 0.00001952
Iteration 42/1000 | Loss: 0.00001952
Iteration 43/1000 | Loss: 0.00001952
Iteration 44/1000 | Loss: 0.00001952
Iteration 45/1000 | Loss: 0.00001951
Iteration 46/1000 | Loss: 0.00001951
Iteration 47/1000 | Loss: 0.00001950
Iteration 48/1000 | Loss: 0.00001950
Iteration 49/1000 | Loss: 0.00001950
Iteration 50/1000 | Loss: 0.00001949
Iteration 51/1000 | Loss: 0.00001949
Iteration 52/1000 | Loss: 0.00001949
Iteration 53/1000 | Loss: 0.00001948
Iteration 54/1000 | Loss: 0.00001948
Iteration 55/1000 | Loss: 0.00001948
Iteration 56/1000 | Loss: 0.00001947
Iteration 57/1000 | Loss: 0.00001947
Iteration 58/1000 | Loss: 0.00001947
Iteration 59/1000 | Loss: 0.00001947
Iteration 60/1000 | Loss: 0.00001947
Iteration 61/1000 | Loss: 0.00001947
Iteration 62/1000 | Loss: 0.00001947
Iteration 63/1000 | Loss: 0.00001946
Iteration 64/1000 | Loss: 0.00001946
Iteration 65/1000 | Loss: 0.00001946
Iteration 66/1000 | Loss: 0.00001946
Iteration 67/1000 | Loss: 0.00001945
Iteration 68/1000 | Loss: 0.00001945
Iteration 69/1000 | Loss: 0.00001945
Iteration 70/1000 | Loss: 0.00001944
Iteration 71/1000 | Loss: 0.00001944
Iteration 72/1000 | Loss: 0.00001944
Iteration 73/1000 | Loss: 0.00001944
Iteration 74/1000 | Loss: 0.00001943
Iteration 75/1000 | Loss: 0.00001943
Iteration 76/1000 | Loss: 0.00001943
Iteration 77/1000 | Loss: 0.00001943
Iteration 78/1000 | Loss: 0.00001943
Iteration 79/1000 | Loss: 0.00001943
Iteration 80/1000 | Loss: 0.00001943
Iteration 81/1000 | Loss: 0.00001943
Iteration 82/1000 | Loss: 0.00001943
Iteration 83/1000 | Loss: 0.00001943
Iteration 84/1000 | Loss: 0.00001943
Iteration 85/1000 | Loss: 0.00001943
Iteration 86/1000 | Loss: 0.00001943
Iteration 87/1000 | Loss: 0.00001942
Iteration 88/1000 | Loss: 0.00001942
Iteration 89/1000 | Loss: 0.00001942
Iteration 90/1000 | Loss: 0.00001942
Iteration 91/1000 | Loss: 0.00001942
Iteration 92/1000 | Loss: 0.00001942
Iteration 93/1000 | Loss: 0.00001942
Iteration 94/1000 | Loss: 0.00001941
Iteration 95/1000 | Loss: 0.00001941
Iteration 96/1000 | Loss: 0.00001940
Iteration 97/1000 | Loss: 0.00001940
Iteration 98/1000 | Loss: 0.00001940
Iteration 99/1000 | Loss: 0.00001940
Iteration 100/1000 | Loss: 0.00001940
Iteration 101/1000 | Loss: 0.00001939
Iteration 102/1000 | Loss: 0.00001939
Iteration 103/1000 | Loss: 0.00001939
Iteration 104/1000 | Loss: 0.00001939
Iteration 105/1000 | Loss: 0.00001939
Iteration 106/1000 | Loss: 0.00001939
Iteration 107/1000 | Loss: 0.00001939
Iteration 108/1000 | Loss: 0.00001938
Iteration 109/1000 | Loss: 0.00001938
Iteration 110/1000 | Loss: 0.00001938
Iteration 111/1000 | Loss: 0.00001938
Iteration 112/1000 | Loss: 0.00001938
Iteration 113/1000 | Loss: 0.00001937
Iteration 114/1000 | Loss: 0.00001937
Iteration 115/1000 | Loss: 0.00001937
Iteration 116/1000 | Loss: 0.00001937
Iteration 117/1000 | Loss: 0.00001937
Iteration 118/1000 | Loss: 0.00001937
Iteration 119/1000 | Loss: 0.00001937
Iteration 120/1000 | Loss: 0.00001937
Iteration 121/1000 | Loss: 0.00001937
Iteration 122/1000 | Loss: 0.00001937
Iteration 123/1000 | Loss: 0.00001937
Iteration 124/1000 | Loss: 0.00001937
Iteration 125/1000 | Loss: 0.00001937
Iteration 126/1000 | Loss: 0.00001936
Iteration 127/1000 | Loss: 0.00001936
Iteration 128/1000 | Loss: 0.00001936
Iteration 129/1000 | Loss: 0.00001936
Iteration 130/1000 | Loss: 0.00001935
Iteration 131/1000 | Loss: 0.00001935
Iteration 132/1000 | Loss: 0.00001935
Iteration 133/1000 | Loss: 0.00001935
Iteration 134/1000 | Loss: 0.00001935
Iteration 135/1000 | Loss: 0.00001935
Iteration 136/1000 | Loss: 0.00001935
Iteration 137/1000 | Loss: 0.00001935
Iteration 138/1000 | Loss: 0.00001935
Iteration 139/1000 | Loss: 0.00001935
Iteration 140/1000 | Loss: 0.00001934
Iteration 141/1000 | Loss: 0.00001934
Iteration 142/1000 | Loss: 0.00001934
Iteration 143/1000 | Loss: 0.00001934
Iteration 144/1000 | Loss: 0.00001934
Iteration 145/1000 | Loss: 0.00001934
Iteration 146/1000 | Loss: 0.00001934
Iteration 147/1000 | Loss: 0.00001934
Iteration 148/1000 | Loss: 0.00001934
Iteration 149/1000 | Loss: 0.00001934
Iteration 150/1000 | Loss: 0.00001934
Iteration 151/1000 | Loss: 0.00001934
Iteration 152/1000 | Loss: 0.00001934
Iteration 153/1000 | Loss: 0.00001934
Iteration 154/1000 | Loss: 0.00001934
Iteration 155/1000 | Loss: 0.00001934
Iteration 156/1000 | Loss: 0.00001933
Iteration 157/1000 | Loss: 0.00001933
Iteration 158/1000 | Loss: 0.00001933
Iteration 159/1000 | Loss: 0.00001933
Iteration 160/1000 | Loss: 0.00001933
Iteration 161/1000 | Loss: 0.00001933
Iteration 162/1000 | Loss: 0.00001933
Iteration 163/1000 | Loss: 0.00001933
Iteration 164/1000 | Loss: 0.00001932
Iteration 165/1000 | Loss: 0.00001932
Iteration 166/1000 | Loss: 0.00001932
Iteration 167/1000 | Loss: 0.00001932
Iteration 168/1000 | Loss: 0.00001932
Iteration 169/1000 | Loss: 0.00001932
Iteration 170/1000 | Loss: 0.00001932
Iteration 171/1000 | Loss: 0.00001932
Iteration 172/1000 | Loss: 0.00001932
Iteration 173/1000 | Loss: 0.00001932
Iteration 174/1000 | Loss: 0.00001932
Iteration 175/1000 | Loss: 0.00001932
Iteration 176/1000 | Loss: 0.00001932
Iteration 177/1000 | Loss: 0.00001931
Iteration 178/1000 | Loss: 0.00001931
Iteration 179/1000 | Loss: 0.00001931
Iteration 180/1000 | Loss: 0.00001931
Iteration 181/1000 | Loss: 0.00001931
Iteration 182/1000 | Loss: 0.00001931
Iteration 183/1000 | Loss: 0.00001931
Iteration 184/1000 | Loss: 0.00001930
Iteration 185/1000 | Loss: 0.00001930
Iteration 186/1000 | Loss: 0.00001930
Iteration 187/1000 | Loss: 0.00001930
Iteration 188/1000 | Loss: 0.00001930
Iteration 189/1000 | Loss: 0.00001930
Iteration 190/1000 | Loss: 0.00001930
Iteration 191/1000 | Loss: 0.00001929
Iteration 192/1000 | Loss: 0.00001929
Iteration 193/1000 | Loss: 0.00001929
Iteration 194/1000 | Loss: 0.00001929
Iteration 195/1000 | Loss: 0.00001929
Iteration 196/1000 | Loss: 0.00001929
Iteration 197/1000 | Loss: 0.00001929
Iteration 198/1000 | Loss: 0.00001929
Iteration 199/1000 | Loss: 0.00001929
Iteration 200/1000 | Loss: 0.00001929
Iteration 201/1000 | Loss: 0.00001929
Iteration 202/1000 | Loss: 0.00001929
Iteration 203/1000 | Loss: 0.00001929
Iteration 204/1000 | Loss: 0.00001929
Iteration 205/1000 | Loss: 0.00001929
Iteration 206/1000 | Loss: 0.00001929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.9286189854028635e-05, 1.9286189854028635e-05, 1.9286189854028635e-05, 1.9286189854028635e-05, 1.9286189854028635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9286189854028635e-05

Optimization complete. Final v2v error: 3.6305649280548096 mm

Highest mean error: 4.823450565338135 mm for frame 130

Lowest mean error: 2.7874629497528076 mm for frame 41

Saving results

Total time: 49.875388622283936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812744
Iteration 2/25 | Loss: 0.00151280
Iteration 3/25 | Loss: 0.00130392
Iteration 4/25 | Loss: 0.00127778
Iteration 5/25 | Loss: 0.00127396
Iteration 6/25 | Loss: 0.00127373
Iteration 7/25 | Loss: 0.00127373
Iteration 8/25 | Loss: 0.00127373
Iteration 9/25 | Loss: 0.00127373
Iteration 10/25 | Loss: 0.00127373
Iteration 11/25 | Loss: 0.00127373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012737311189994216, 0.0012737311189994216, 0.0012737311189994216, 0.0012737311189994216, 0.0012737311189994216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012737311189994216

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30717301
Iteration 2/25 | Loss: 0.00120475
Iteration 3/25 | Loss: 0.00120473
Iteration 4/25 | Loss: 0.00120473
Iteration 5/25 | Loss: 0.00120473
Iteration 6/25 | Loss: 0.00120473
Iteration 7/25 | Loss: 0.00120473
Iteration 8/25 | Loss: 0.00120473
Iteration 9/25 | Loss: 0.00120473
Iteration 10/25 | Loss: 0.00120473
Iteration 11/25 | Loss: 0.00120473
Iteration 12/25 | Loss: 0.00120473
Iteration 13/25 | Loss: 0.00120473
Iteration 14/25 | Loss: 0.00120473
Iteration 15/25 | Loss: 0.00120473
Iteration 16/25 | Loss: 0.00120473
Iteration 17/25 | Loss: 0.00120473
Iteration 18/25 | Loss: 0.00120473
Iteration 19/25 | Loss: 0.00120473
Iteration 20/25 | Loss: 0.00120473
Iteration 21/25 | Loss: 0.00120473
Iteration 22/25 | Loss: 0.00120473
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012047264026477933, 0.0012047264026477933, 0.0012047264026477933, 0.0012047264026477933, 0.0012047264026477933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012047264026477933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120473
Iteration 2/1000 | Loss: 0.00003171
Iteration 3/1000 | Loss: 0.00002178
Iteration 4/1000 | Loss: 0.00001859
Iteration 5/1000 | Loss: 0.00001764
Iteration 6/1000 | Loss: 0.00001691
Iteration 7/1000 | Loss: 0.00001643
Iteration 8/1000 | Loss: 0.00001610
Iteration 9/1000 | Loss: 0.00001585
Iteration 10/1000 | Loss: 0.00001557
Iteration 11/1000 | Loss: 0.00001539
Iteration 12/1000 | Loss: 0.00001537
Iteration 13/1000 | Loss: 0.00001533
Iteration 14/1000 | Loss: 0.00001529
Iteration 15/1000 | Loss: 0.00001525
Iteration 16/1000 | Loss: 0.00001524
Iteration 17/1000 | Loss: 0.00001514
Iteration 18/1000 | Loss: 0.00001499
Iteration 19/1000 | Loss: 0.00001497
Iteration 20/1000 | Loss: 0.00001496
Iteration 21/1000 | Loss: 0.00001496
Iteration 22/1000 | Loss: 0.00001495
Iteration 23/1000 | Loss: 0.00001494
Iteration 24/1000 | Loss: 0.00001494
Iteration 25/1000 | Loss: 0.00001494
Iteration 26/1000 | Loss: 0.00001487
Iteration 27/1000 | Loss: 0.00001484
Iteration 28/1000 | Loss: 0.00001483
Iteration 29/1000 | Loss: 0.00001483
Iteration 30/1000 | Loss: 0.00001482
Iteration 31/1000 | Loss: 0.00001482
Iteration 32/1000 | Loss: 0.00001481
Iteration 33/1000 | Loss: 0.00001481
Iteration 34/1000 | Loss: 0.00001481
Iteration 35/1000 | Loss: 0.00001481
Iteration 36/1000 | Loss: 0.00001481
Iteration 37/1000 | Loss: 0.00001481
Iteration 38/1000 | Loss: 0.00001481
Iteration 39/1000 | Loss: 0.00001481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 39. Stopping optimization.
Last 5 losses: [1.4811712389928289e-05, 1.4811712389928289e-05, 1.4811712389928289e-05, 1.4811712389928289e-05, 1.4811712389928289e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4811712389928289e-05

Optimization complete. Final v2v error: 3.2410295009613037 mm

Highest mean error: 4.043959617614746 mm for frame 2

Lowest mean error: 3.0506319999694824 mm for frame 42

Saving results

Total time: 30.220800399780273
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764997
Iteration 2/25 | Loss: 0.00158122
Iteration 3/25 | Loss: 0.00128222
Iteration 4/25 | Loss: 0.00123684
Iteration 5/25 | Loss: 0.00123024
Iteration 6/25 | Loss: 0.00121734
Iteration 7/25 | Loss: 0.00120903
Iteration 8/25 | Loss: 0.00120546
Iteration 9/25 | Loss: 0.00120364
Iteration 10/25 | Loss: 0.00120244
Iteration 11/25 | Loss: 0.00120185
Iteration 12/25 | Loss: 0.00120156
Iteration 13/25 | Loss: 0.00120142
Iteration 14/25 | Loss: 0.00120140
Iteration 15/25 | Loss: 0.00120139
Iteration 16/25 | Loss: 0.00120139
Iteration 17/25 | Loss: 0.00120139
Iteration 18/25 | Loss: 0.00120139
Iteration 19/25 | Loss: 0.00120139
Iteration 20/25 | Loss: 0.00120139
Iteration 21/25 | Loss: 0.00120139
Iteration 22/25 | Loss: 0.00120139
Iteration 23/25 | Loss: 0.00120139
Iteration 24/25 | Loss: 0.00120139
Iteration 25/25 | Loss: 0.00120139

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.88409638
Iteration 2/25 | Loss: 0.00095573
Iteration 3/25 | Loss: 0.00095573
Iteration 4/25 | Loss: 0.00095573
Iteration 5/25 | Loss: 0.00095573
Iteration 6/25 | Loss: 0.00095573
Iteration 7/25 | Loss: 0.00095573
Iteration 8/25 | Loss: 0.00095573
Iteration 9/25 | Loss: 0.00095573
Iteration 10/25 | Loss: 0.00095573
Iteration 11/25 | Loss: 0.00095573
Iteration 12/25 | Loss: 0.00095573
Iteration 13/25 | Loss: 0.00095573
Iteration 14/25 | Loss: 0.00095573
Iteration 15/25 | Loss: 0.00095573
Iteration 16/25 | Loss: 0.00095573
Iteration 17/25 | Loss: 0.00095573
Iteration 18/25 | Loss: 0.00095573
Iteration 19/25 | Loss: 0.00095573
Iteration 20/25 | Loss: 0.00095573
Iteration 21/25 | Loss: 0.00095573
Iteration 22/25 | Loss: 0.00095573
Iteration 23/25 | Loss: 0.00095573
Iteration 24/25 | Loss: 0.00095573
Iteration 25/25 | Loss: 0.00095573

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095573
Iteration 2/1000 | Loss: 0.00002230
Iteration 3/1000 | Loss: 0.00001825
Iteration 4/1000 | Loss: 0.00001694
Iteration 5/1000 | Loss: 0.00001609
Iteration 6/1000 | Loss: 0.00001546
Iteration 7/1000 | Loss: 0.00001502
Iteration 8/1000 | Loss: 0.00001466
Iteration 9/1000 | Loss: 0.00001442
Iteration 10/1000 | Loss: 0.00001436
Iteration 11/1000 | Loss: 0.00001425
Iteration 12/1000 | Loss: 0.00001410
Iteration 13/1000 | Loss: 0.00001403
Iteration 14/1000 | Loss: 0.00001403
Iteration 15/1000 | Loss: 0.00001396
Iteration 16/1000 | Loss: 0.00001388
Iteration 17/1000 | Loss: 0.00001376
Iteration 18/1000 | Loss: 0.00001375
Iteration 19/1000 | Loss: 0.00001366
Iteration 20/1000 | Loss: 0.00001359
Iteration 21/1000 | Loss: 0.00001358
Iteration 22/1000 | Loss: 0.00001358
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001356
Iteration 25/1000 | Loss: 0.00001351
Iteration 26/1000 | Loss: 0.00001350
Iteration 27/1000 | Loss: 0.00001350
Iteration 28/1000 | Loss: 0.00001350
Iteration 29/1000 | Loss: 0.00001349
Iteration 30/1000 | Loss: 0.00001348
Iteration 31/1000 | Loss: 0.00001348
Iteration 32/1000 | Loss: 0.00001348
Iteration 33/1000 | Loss: 0.00001348
Iteration 34/1000 | Loss: 0.00001346
Iteration 35/1000 | Loss: 0.00001346
Iteration 36/1000 | Loss: 0.00001346
Iteration 37/1000 | Loss: 0.00001345
Iteration 38/1000 | Loss: 0.00001345
Iteration 39/1000 | Loss: 0.00001345
Iteration 40/1000 | Loss: 0.00001344
Iteration 41/1000 | Loss: 0.00001344
Iteration 42/1000 | Loss: 0.00001343
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001341
Iteration 46/1000 | Loss: 0.00001341
Iteration 47/1000 | Loss: 0.00001341
Iteration 48/1000 | Loss: 0.00001341
Iteration 49/1000 | Loss: 0.00001340
Iteration 50/1000 | Loss: 0.00001340
Iteration 51/1000 | Loss: 0.00001340
Iteration 52/1000 | Loss: 0.00001340
Iteration 53/1000 | Loss: 0.00001340
Iteration 54/1000 | Loss: 0.00001340
Iteration 55/1000 | Loss: 0.00001340
Iteration 56/1000 | Loss: 0.00001339
Iteration 57/1000 | Loss: 0.00001339
Iteration 58/1000 | Loss: 0.00001339
Iteration 59/1000 | Loss: 0.00001338
Iteration 60/1000 | Loss: 0.00001338
Iteration 61/1000 | Loss: 0.00001338
Iteration 62/1000 | Loss: 0.00001338
Iteration 63/1000 | Loss: 0.00001337
Iteration 64/1000 | Loss: 0.00001337
Iteration 65/1000 | Loss: 0.00001337
Iteration 66/1000 | Loss: 0.00001337
Iteration 67/1000 | Loss: 0.00001337
Iteration 68/1000 | Loss: 0.00001337
Iteration 69/1000 | Loss: 0.00001337
Iteration 70/1000 | Loss: 0.00001336
Iteration 71/1000 | Loss: 0.00001335
Iteration 72/1000 | Loss: 0.00001335
Iteration 73/1000 | Loss: 0.00001335
Iteration 74/1000 | Loss: 0.00001334
Iteration 75/1000 | Loss: 0.00001334
Iteration 76/1000 | Loss: 0.00001334
Iteration 77/1000 | Loss: 0.00001334
Iteration 78/1000 | Loss: 0.00001334
Iteration 79/1000 | Loss: 0.00001334
Iteration 80/1000 | Loss: 0.00001334
Iteration 81/1000 | Loss: 0.00001334
Iteration 82/1000 | Loss: 0.00001334
Iteration 83/1000 | Loss: 0.00001333
Iteration 84/1000 | Loss: 0.00001333
Iteration 85/1000 | Loss: 0.00001333
Iteration 86/1000 | Loss: 0.00001333
Iteration 87/1000 | Loss: 0.00001333
Iteration 88/1000 | Loss: 0.00001333
Iteration 89/1000 | Loss: 0.00001333
Iteration 90/1000 | Loss: 0.00001333
Iteration 91/1000 | Loss: 0.00001332
Iteration 92/1000 | Loss: 0.00001332
Iteration 93/1000 | Loss: 0.00001332
Iteration 94/1000 | Loss: 0.00001332
Iteration 95/1000 | Loss: 0.00001332
Iteration 96/1000 | Loss: 0.00001332
Iteration 97/1000 | Loss: 0.00001331
Iteration 98/1000 | Loss: 0.00001331
Iteration 99/1000 | Loss: 0.00001331
Iteration 100/1000 | Loss: 0.00001330
Iteration 101/1000 | Loss: 0.00001330
Iteration 102/1000 | Loss: 0.00001330
Iteration 103/1000 | Loss: 0.00001330
Iteration 104/1000 | Loss: 0.00001330
Iteration 105/1000 | Loss: 0.00001330
Iteration 106/1000 | Loss: 0.00001330
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001329
Iteration 110/1000 | Loss: 0.00001328
Iteration 111/1000 | Loss: 0.00001328
Iteration 112/1000 | Loss: 0.00001327
Iteration 113/1000 | Loss: 0.00001327
Iteration 114/1000 | Loss: 0.00001327
Iteration 115/1000 | Loss: 0.00001327
Iteration 116/1000 | Loss: 0.00001327
Iteration 117/1000 | Loss: 0.00001327
Iteration 118/1000 | Loss: 0.00001327
Iteration 119/1000 | Loss: 0.00001327
Iteration 120/1000 | Loss: 0.00001327
Iteration 121/1000 | Loss: 0.00001327
Iteration 122/1000 | Loss: 0.00001327
Iteration 123/1000 | Loss: 0.00001327
Iteration 124/1000 | Loss: 0.00001327
Iteration 125/1000 | Loss: 0.00001327
Iteration 126/1000 | Loss: 0.00001327
Iteration 127/1000 | Loss: 0.00001327
Iteration 128/1000 | Loss: 0.00001327
Iteration 129/1000 | Loss: 0.00001327
Iteration 130/1000 | Loss: 0.00001327
Iteration 131/1000 | Loss: 0.00001327
Iteration 132/1000 | Loss: 0.00001327
Iteration 133/1000 | Loss: 0.00001327
Iteration 134/1000 | Loss: 0.00001327
Iteration 135/1000 | Loss: 0.00001327
Iteration 136/1000 | Loss: 0.00001327
Iteration 137/1000 | Loss: 0.00001327
Iteration 138/1000 | Loss: 0.00001327
Iteration 139/1000 | Loss: 0.00001327
Iteration 140/1000 | Loss: 0.00001327
Iteration 141/1000 | Loss: 0.00001327
Iteration 142/1000 | Loss: 0.00001327
Iteration 143/1000 | Loss: 0.00001327
Iteration 144/1000 | Loss: 0.00001327
Iteration 145/1000 | Loss: 0.00001327
Iteration 146/1000 | Loss: 0.00001327
Iteration 147/1000 | Loss: 0.00001327
Iteration 148/1000 | Loss: 0.00001327
Iteration 149/1000 | Loss: 0.00001327
Iteration 150/1000 | Loss: 0.00001327
Iteration 151/1000 | Loss: 0.00001327
Iteration 152/1000 | Loss: 0.00001327
Iteration 153/1000 | Loss: 0.00001327
Iteration 154/1000 | Loss: 0.00001327
Iteration 155/1000 | Loss: 0.00001327
Iteration 156/1000 | Loss: 0.00001327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.326599704043474e-05, 1.326599704043474e-05, 1.326599704043474e-05, 1.326599704043474e-05, 1.326599704043474e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.326599704043474e-05

Optimization complete. Final v2v error: 3.140268087387085 mm

Highest mean error: 3.5560176372528076 mm for frame 81

Lowest mean error: 2.8807075023651123 mm for frame 131

Saving results

Total time: 54.07833528518677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00539290
Iteration 2/25 | Loss: 0.00127563
Iteration 3/25 | Loss: 0.00121567
Iteration 4/25 | Loss: 0.00120699
Iteration 5/25 | Loss: 0.00120404
Iteration 6/25 | Loss: 0.00120386
Iteration 7/25 | Loss: 0.00120386
Iteration 8/25 | Loss: 0.00120386
Iteration 9/25 | Loss: 0.00120386
Iteration 10/25 | Loss: 0.00120386
Iteration 11/25 | Loss: 0.00120386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012038603890687227, 0.0012038603890687227, 0.0012038603890687227, 0.0012038603890687227, 0.0012038603890687227]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012038603890687227

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.41827822
Iteration 2/25 | Loss: 0.00093506
Iteration 3/25 | Loss: 0.00093504
Iteration 4/25 | Loss: 0.00093504
Iteration 5/25 | Loss: 0.00093504
Iteration 6/25 | Loss: 0.00093504
Iteration 7/25 | Loss: 0.00093504
Iteration 8/25 | Loss: 0.00093504
Iteration 9/25 | Loss: 0.00093504
Iteration 10/25 | Loss: 0.00093504
Iteration 11/25 | Loss: 0.00093504
Iteration 12/25 | Loss: 0.00093504
Iteration 13/25 | Loss: 0.00093504
Iteration 14/25 | Loss: 0.00093504
Iteration 15/25 | Loss: 0.00093504
Iteration 16/25 | Loss: 0.00093504
Iteration 17/25 | Loss: 0.00093504
Iteration 18/25 | Loss: 0.00093504
Iteration 19/25 | Loss: 0.00093504
Iteration 20/25 | Loss: 0.00093504
Iteration 21/25 | Loss: 0.00093504
Iteration 22/25 | Loss: 0.00093504
Iteration 23/25 | Loss: 0.00093504
Iteration 24/25 | Loss: 0.00093504
Iteration 25/25 | Loss: 0.00093504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093504
Iteration 2/1000 | Loss: 0.00002673
Iteration 3/1000 | Loss: 0.00001931
Iteration 4/1000 | Loss: 0.00001627
Iteration 5/1000 | Loss: 0.00001534
Iteration 6/1000 | Loss: 0.00001464
Iteration 7/1000 | Loss: 0.00001419
Iteration 8/1000 | Loss: 0.00001376
Iteration 9/1000 | Loss: 0.00001366
Iteration 10/1000 | Loss: 0.00001336
Iteration 11/1000 | Loss: 0.00001311
Iteration 12/1000 | Loss: 0.00001292
Iteration 13/1000 | Loss: 0.00001276
Iteration 14/1000 | Loss: 0.00001273
Iteration 15/1000 | Loss: 0.00001267
Iteration 16/1000 | Loss: 0.00001267
Iteration 17/1000 | Loss: 0.00001267
Iteration 18/1000 | Loss: 0.00001264
Iteration 19/1000 | Loss: 0.00001263
Iteration 20/1000 | Loss: 0.00001262
Iteration 21/1000 | Loss: 0.00001262
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001260
Iteration 24/1000 | Loss: 0.00001259
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001257
Iteration 27/1000 | Loss: 0.00001257
Iteration 28/1000 | Loss: 0.00001257
Iteration 29/1000 | Loss: 0.00001256
Iteration 30/1000 | Loss: 0.00001256
Iteration 31/1000 | Loss: 0.00001255
Iteration 32/1000 | Loss: 0.00001253
Iteration 33/1000 | Loss: 0.00001250
Iteration 34/1000 | Loss: 0.00001247
Iteration 35/1000 | Loss: 0.00001246
Iteration 36/1000 | Loss: 0.00001246
Iteration 37/1000 | Loss: 0.00001246
Iteration 38/1000 | Loss: 0.00001245
Iteration 39/1000 | Loss: 0.00001245
Iteration 40/1000 | Loss: 0.00001245
Iteration 41/1000 | Loss: 0.00001236
Iteration 42/1000 | Loss: 0.00001234
Iteration 43/1000 | Loss: 0.00001233
Iteration 44/1000 | Loss: 0.00001232
Iteration 45/1000 | Loss: 0.00001230
Iteration 46/1000 | Loss: 0.00001230
Iteration 47/1000 | Loss: 0.00001230
Iteration 48/1000 | Loss: 0.00001229
Iteration 49/1000 | Loss: 0.00001229
Iteration 50/1000 | Loss: 0.00001228
Iteration 51/1000 | Loss: 0.00001227
Iteration 52/1000 | Loss: 0.00001225
Iteration 53/1000 | Loss: 0.00001225
Iteration 54/1000 | Loss: 0.00001224
Iteration 55/1000 | Loss: 0.00001223
Iteration 56/1000 | Loss: 0.00001222
Iteration 57/1000 | Loss: 0.00001221
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001220
Iteration 61/1000 | Loss: 0.00001219
Iteration 62/1000 | Loss: 0.00001217
Iteration 63/1000 | Loss: 0.00001217
Iteration 64/1000 | Loss: 0.00001217
Iteration 65/1000 | Loss: 0.00001217
Iteration 66/1000 | Loss: 0.00001217
Iteration 67/1000 | Loss: 0.00001217
Iteration 68/1000 | Loss: 0.00001217
Iteration 69/1000 | Loss: 0.00001217
Iteration 70/1000 | Loss: 0.00001217
Iteration 71/1000 | Loss: 0.00001216
Iteration 72/1000 | Loss: 0.00001216
Iteration 73/1000 | Loss: 0.00001215
Iteration 74/1000 | Loss: 0.00001215
Iteration 75/1000 | Loss: 0.00001215
Iteration 76/1000 | Loss: 0.00001214
Iteration 77/1000 | Loss: 0.00001214
Iteration 78/1000 | Loss: 0.00001213
Iteration 79/1000 | Loss: 0.00001213
Iteration 80/1000 | Loss: 0.00001213
Iteration 81/1000 | Loss: 0.00001213
Iteration 82/1000 | Loss: 0.00001213
Iteration 83/1000 | Loss: 0.00001212
Iteration 84/1000 | Loss: 0.00001212
Iteration 85/1000 | Loss: 0.00001212
Iteration 86/1000 | Loss: 0.00001212
Iteration 87/1000 | Loss: 0.00001212
Iteration 88/1000 | Loss: 0.00001212
Iteration 89/1000 | Loss: 0.00001212
Iteration 90/1000 | Loss: 0.00001211
Iteration 91/1000 | Loss: 0.00001211
Iteration 92/1000 | Loss: 0.00001211
Iteration 93/1000 | Loss: 0.00001211
Iteration 94/1000 | Loss: 0.00001211
Iteration 95/1000 | Loss: 0.00001210
Iteration 96/1000 | Loss: 0.00001210
Iteration 97/1000 | Loss: 0.00001210
Iteration 98/1000 | Loss: 0.00001210
Iteration 99/1000 | Loss: 0.00001210
Iteration 100/1000 | Loss: 0.00001210
Iteration 101/1000 | Loss: 0.00001210
Iteration 102/1000 | Loss: 0.00001209
Iteration 103/1000 | Loss: 0.00001209
Iteration 104/1000 | Loss: 0.00001209
Iteration 105/1000 | Loss: 0.00001209
Iteration 106/1000 | Loss: 0.00001209
Iteration 107/1000 | Loss: 0.00001209
Iteration 108/1000 | Loss: 0.00001209
Iteration 109/1000 | Loss: 0.00001209
Iteration 110/1000 | Loss: 0.00001209
Iteration 111/1000 | Loss: 0.00001208
Iteration 112/1000 | Loss: 0.00001208
Iteration 113/1000 | Loss: 0.00001208
Iteration 114/1000 | Loss: 0.00001208
Iteration 115/1000 | Loss: 0.00001208
Iteration 116/1000 | Loss: 0.00001208
Iteration 117/1000 | Loss: 0.00001207
Iteration 118/1000 | Loss: 0.00001207
Iteration 119/1000 | Loss: 0.00001207
Iteration 120/1000 | Loss: 0.00001206
Iteration 121/1000 | Loss: 0.00001206
Iteration 122/1000 | Loss: 0.00001206
Iteration 123/1000 | Loss: 0.00001206
Iteration 124/1000 | Loss: 0.00001205
Iteration 125/1000 | Loss: 0.00001205
Iteration 126/1000 | Loss: 0.00001205
Iteration 127/1000 | Loss: 0.00001205
Iteration 128/1000 | Loss: 0.00001204
Iteration 129/1000 | Loss: 0.00001204
Iteration 130/1000 | Loss: 0.00001204
Iteration 131/1000 | Loss: 0.00001204
Iteration 132/1000 | Loss: 0.00001204
Iteration 133/1000 | Loss: 0.00001204
Iteration 134/1000 | Loss: 0.00001204
Iteration 135/1000 | Loss: 0.00001204
Iteration 136/1000 | Loss: 0.00001204
Iteration 137/1000 | Loss: 0.00001204
Iteration 138/1000 | Loss: 0.00001203
Iteration 139/1000 | Loss: 0.00001203
Iteration 140/1000 | Loss: 0.00001203
Iteration 141/1000 | Loss: 0.00001203
Iteration 142/1000 | Loss: 0.00001203
Iteration 143/1000 | Loss: 0.00001203
Iteration 144/1000 | Loss: 0.00001203
Iteration 145/1000 | Loss: 0.00001203
Iteration 146/1000 | Loss: 0.00001203
Iteration 147/1000 | Loss: 0.00001203
Iteration 148/1000 | Loss: 0.00001203
Iteration 149/1000 | Loss: 0.00001203
Iteration 150/1000 | Loss: 0.00001203
Iteration 151/1000 | Loss: 0.00001203
Iteration 152/1000 | Loss: 0.00001203
Iteration 153/1000 | Loss: 0.00001202
Iteration 154/1000 | Loss: 0.00001202
Iteration 155/1000 | Loss: 0.00001202
Iteration 156/1000 | Loss: 0.00001202
Iteration 157/1000 | Loss: 0.00001202
Iteration 158/1000 | Loss: 0.00001202
Iteration 159/1000 | Loss: 0.00001202
Iteration 160/1000 | Loss: 0.00001202
Iteration 161/1000 | Loss: 0.00001202
Iteration 162/1000 | Loss: 0.00001202
Iteration 163/1000 | Loss: 0.00001202
Iteration 164/1000 | Loss: 0.00001202
Iteration 165/1000 | Loss: 0.00001202
Iteration 166/1000 | Loss: 0.00001202
Iteration 167/1000 | Loss: 0.00001202
Iteration 168/1000 | Loss: 0.00001202
Iteration 169/1000 | Loss: 0.00001202
Iteration 170/1000 | Loss: 0.00001202
Iteration 171/1000 | Loss: 0.00001202
Iteration 172/1000 | Loss: 0.00001202
Iteration 173/1000 | Loss: 0.00001202
Iteration 174/1000 | Loss: 0.00001202
Iteration 175/1000 | Loss: 0.00001202
Iteration 176/1000 | Loss: 0.00001202
Iteration 177/1000 | Loss: 0.00001202
Iteration 178/1000 | Loss: 0.00001202
Iteration 179/1000 | Loss: 0.00001202
Iteration 180/1000 | Loss: 0.00001202
Iteration 181/1000 | Loss: 0.00001202
Iteration 182/1000 | Loss: 0.00001202
Iteration 183/1000 | Loss: 0.00001202
Iteration 184/1000 | Loss: 0.00001202
Iteration 185/1000 | Loss: 0.00001202
Iteration 186/1000 | Loss: 0.00001202
Iteration 187/1000 | Loss: 0.00001202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.2017410881526303e-05, 1.2017410881526303e-05, 1.2017410881526303e-05, 1.2017410881526303e-05, 1.2017410881526303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2017410881526303e-05

Optimization complete. Final v2v error: 2.9545392990112305 mm

Highest mean error: 3.629758358001709 mm for frame 92

Lowest mean error: 2.6485183238983154 mm for frame 35

Saving results

Total time: 40.62323474884033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420810
Iteration 2/25 | Loss: 0.00127098
Iteration 3/25 | Loss: 0.00120680
Iteration 4/25 | Loss: 0.00119697
Iteration 5/25 | Loss: 0.00119355
Iteration 6/25 | Loss: 0.00119277
Iteration 7/25 | Loss: 0.00119277
Iteration 8/25 | Loss: 0.00119277
Iteration 9/25 | Loss: 0.00119277
Iteration 10/25 | Loss: 0.00119277
Iteration 11/25 | Loss: 0.00119277
Iteration 12/25 | Loss: 0.00119277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001192767173051834, 0.001192767173051834, 0.001192767173051834, 0.001192767173051834, 0.001192767173051834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001192767173051834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69943142
Iteration 2/25 | Loss: 0.00101761
Iteration 3/25 | Loss: 0.00101761
Iteration 4/25 | Loss: 0.00101760
Iteration 5/25 | Loss: 0.00101760
Iteration 6/25 | Loss: 0.00101760
Iteration 7/25 | Loss: 0.00101760
Iteration 8/25 | Loss: 0.00101760
Iteration 9/25 | Loss: 0.00101760
Iteration 10/25 | Loss: 0.00101760
Iteration 11/25 | Loss: 0.00101760
Iteration 12/25 | Loss: 0.00101760
Iteration 13/25 | Loss: 0.00101760
Iteration 14/25 | Loss: 0.00101760
Iteration 15/25 | Loss: 0.00101760
Iteration 16/25 | Loss: 0.00101760
Iteration 17/25 | Loss: 0.00101760
Iteration 18/25 | Loss: 0.00101760
Iteration 19/25 | Loss: 0.00101760
Iteration 20/25 | Loss: 0.00101760
Iteration 21/25 | Loss: 0.00101760
Iteration 22/25 | Loss: 0.00101760
Iteration 23/25 | Loss: 0.00101760
Iteration 24/25 | Loss: 0.00101760
Iteration 25/25 | Loss: 0.00101760

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101760
Iteration 2/1000 | Loss: 0.00003166
Iteration 3/1000 | Loss: 0.00002078
Iteration 4/1000 | Loss: 0.00001685
Iteration 5/1000 | Loss: 0.00001538
Iteration 6/1000 | Loss: 0.00001446
Iteration 7/1000 | Loss: 0.00001385
Iteration 8/1000 | Loss: 0.00001347
Iteration 9/1000 | Loss: 0.00001309
Iteration 10/1000 | Loss: 0.00001304
Iteration 11/1000 | Loss: 0.00001292
Iteration 12/1000 | Loss: 0.00001274
Iteration 13/1000 | Loss: 0.00001262
Iteration 14/1000 | Loss: 0.00001257
Iteration 15/1000 | Loss: 0.00001251
Iteration 16/1000 | Loss: 0.00001249
Iteration 17/1000 | Loss: 0.00001247
Iteration 18/1000 | Loss: 0.00001241
Iteration 19/1000 | Loss: 0.00001234
Iteration 20/1000 | Loss: 0.00001228
Iteration 21/1000 | Loss: 0.00001224
Iteration 22/1000 | Loss: 0.00001221
Iteration 23/1000 | Loss: 0.00001221
Iteration 24/1000 | Loss: 0.00001220
Iteration 25/1000 | Loss: 0.00001220
Iteration 26/1000 | Loss: 0.00001219
Iteration 27/1000 | Loss: 0.00001219
Iteration 28/1000 | Loss: 0.00001219
Iteration 29/1000 | Loss: 0.00001218
Iteration 30/1000 | Loss: 0.00001218
Iteration 31/1000 | Loss: 0.00001218
Iteration 32/1000 | Loss: 0.00001217
Iteration 33/1000 | Loss: 0.00001217
Iteration 34/1000 | Loss: 0.00001217
Iteration 35/1000 | Loss: 0.00001217
Iteration 36/1000 | Loss: 0.00001217
Iteration 37/1000 | Loss: 0.00001216
Iteration 38/1000 | Loss: 0.00001216
Iteration 39/1000 | Loss: 0.00001216
Iteration 40/1000 | Loss: 0.00001215
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00001214
Iteration 43/1000 | Loss: 0.00001214
Iteration 44/1000 | Loss: 0.00001213
Iteration 45/1000 | Loss: 0.00001213
Iteration 46/1000 | Loss: 0.00001213
Iteration 47/1000 | Loss: 0.00001212
Iteration 48/1000 | Loss: 0.00001212
Iteration 49/1000 | Loss: 0.00001210
Iteration 50/1000 | Loss: 0.00001210
Iteration 51/1000 | Loss: 0.00001210
Iteration 52/1000 | Loss: 0.00001209
Iteration 53/1000 | Loss: 0.00001209
Iteration 54/1000 | Loss: 0.00001208
Iteration 55/1000 | Loss: 0.00001208
Iteration 56/1000 | Loss: 0.00001208
Iteration 57/1000 | Loss: 0.00001208
Iteration 58/1000 | Loss: 0.00001208
Iteration 59/1000 | Loss: 0.00001208
Iteration 60/1000 | Loss: 0.00001208
Iteration 61/1000 | Loss: 0.00001208
Iteration 62/1000 | Loss: 0.00001207
Iteration 63/1000 | Loss: 0.00001207
Iteration 64/1000 | Loss: 0.00001207
Iteration 65/1000 | Loss: 0.00001206
Iteration 66/1000 | Loss: 0.00001206
Iteration 67/1000 | Loss: 0.00001206
Iteration 68/1000 | Loss: 0.00001205
Iteration 69/1000 | Loss: 0.00001205
Iteration 70/1000 | Loss: 0.00001205
Iteration 71/1000 | Loss: 0.00001205
Iteration 72/1000 | Loss: 0.00001205
Iteration 73/1000 | Loss: 0.00001205
Iteration 74/1000 | Loss: 0.00001204
Iteration 75/1000 | Loss: 0.00001204
Iteration 76/1000 | Loss: 0.00001204
Iteration 77/1000 | Loss: 0.00001204
Iteration 78/1000 | Loss: 0.00001204
Iteration 79/1000 | Loss: 0.00001204
Iteration 80/1000 | Loss: 0.00001204
Iteration 81/1000 | Loss: 0.00001204
Iteration 82/1000 | Loss: 0.00001203
Iteration 83/1000 | Loss: 0.00001203
Iteration 84/1000 | Loss: 0.00001203
Iteration 85/1000 | Loss: 0.00001203
Iteration 86/1000 | Loss: 0.00001203
Iteration 87/1000 | Loss: 0.00001202
Iteration 88/1000 | Loss: 0.00001202
Iteration 89/1000 | Loss: 0.00001202
Iteration 90/1000 | Loss: 0.00001202
Iteration 91/1000 | Loss: 0.00001202
Iteration 92/1000 | Loss: 0.00001202
Iteration 93/1000 | Loss: 0.00001201
Iteration 94/1000 | Loss: 0.00001201
Iteration 95/1000 | Loss: 0.00001201
Iteration 96/1000 | Loss: 0.00001201
Iteration 97/1000 | Loss: 0.00001200
Iteration 98/1000 | Loss: 0.00001200
Iteration 99/1000 | Loss: 0.00001200
Iteration 100/1000 | Loss: 0.00001200
Iteration 101/1000 | Loss: 0.00001200
Iteration 102/1000 | Loss: 0.00001200
Iteration 103/1000 | Loss: 0.00001199
Iteration 104/1000 | Loss: 0.00001199
Iteration 105/1000 | Loss: 0.00001198
Iteration 106/1000 | Loss: 0.00001198
Iteration 107/1000 | Loss: 0.00001197
Iteration 108/1000 | Loss: 0.00001197
Iteration 109/1000 | Loss: 0.00001197
Iteration 110/1000 | Loss: 0.00001197
Iteration 111/1000 | Loss: 0.00001196
Iteration 112/1000 | Loss: 0.00001196
Iteration 113/1000 | Loss: 0.00001196
Iteration 114/1000 | Loss: 0.00001195
Iteration 115/1000 | Loss: 0.00001195
Iteration 116/1000 | Loss: 0.00001195
Iteration 117/1000 | Loss: 0.00001194
Iteration 118/1000 | Loss: 0.00001194
Iteration 119/1000 | Loss: 0.00001192
Iteration 120/1000 | Loss: 0.00001192
Iteration 121/1000 | Loss: 0.00001191
Iteration 122/1000 | Loss: 0.00001191
Iteration 123/1000 | Loss: 0.00001190
Iteration 124/1000 | Loss: 0.00001190
Iteration 125/1000 | Loss: 0.00001190
Iteration 126/1000 | Loss: 0.00001189
Iteration 127/1000 | Loss: 0.00001189
Iteration 128/1000 | Loss: 0.00001188
Iteration 129/1000 | Loss: 0.00001188
Iteration 130/1000 | Loss: 0.00001188
Iteration 131/1000 | Loss: 0.00001188
Iteration 132/1000 | Loss: 0.00001188
Iteration 133/1000 | Loss: 0.00001187
Iteration 134/1000 | Loss: 0.00001187
Iteration 135/1000 | Loss: 0.00001186
Iteration 136/1000 | Loss: 0.00001186
Iteration 137/1000 | Loss: 0.00001186
Iteration 138/1000 | Loss: 0.00001186
Iteration 139/1000 | Loss: 0.00001186
Iteration 140/1000 | Loss: 0.00001186
Iteration 141/1000 | Loss: 0.00001185
Iteration 142/1000 | Loss: 0.00001185
Iteration 143/1000 | Loss: 0.00001185
Iteration 144/1000 | Loss: 0.00001184
Iteration 145/1000 | Loss: 0.00001184
Iteration 146/1000 | Loss: 0.00001184
Iteration 147/1000 | Loss: 0.00001184
Iteration 148/1000 | Loss: 0.00001184
Iteration 149/1000 | Loss: 0.00001184
Iteration 150/1000 | Loss: 0.00001183
Iteration 151/1000 | Loss: 0.00001183
Iteration 152/1000 | Loss: 0.00001183
Iteration 153/1000 | Loss: 0.00001183
Iteration 154/1000 | Loss: 0.00001183
Iteration 155/1000 | Loss: 0.00001183
Iteration 156/1000 | Loss: 0.00001183
Iteration 157/1000 | Loss: 0.00001182
Iteration 158/1000 | Loss: 0.00001182
Iteration 159/1000 | Loss: 0.00001182
Iteration 160/1000 | Loss: 0.00001182
Iteration 161/1000 | Loss: 0.00001182
Iteration 162/1000 | Loss: 0.00001182
Iteration 163/1000 | Loss: 0.00001182
Iteration 164/1000 | Loss: 0.00001182
Iteration 165/1000 | Loss: 0.00001182
Iteration 166/1000 | Loss: 0.00001182
Iteration 167/1000 | Loss: 0.00001182
Iteration 168/1000 | Loss: 0.00001182
Iteration 169/1000 | Loss: 0.00001182
Iteration 170/1000 | Loss: 0.00001181
Iteration 171/1000 | Loss: 0.00001181
Iteration 172/1000 | Loss: 0.00001181
Iteration 173/1000 | Loss: 0.00001181
Iteration 174/1000 | Loss: 0.00001181
Iteration 175/1000 | Loss: 0.00001181
Iteration 176/1000 | Loss: 0.00001181
Iteration 177/1000 | Loss: 0.00001181
Iteration 178/1000 | Loss: 0.00001181
Iteration 179/1000 | Loss: 0.00001181
Iteration 180/1000 | Loss: 0.00001180
Iteration 181/1000 | Loss: 0.00001180
Iteration 182/1000 | Loss: 0.00001180
Iteration 183/1000 | Loss: 0.00001180
Iteration 184/1000 | Loss: 0.00001180
Iteration 185/1000 | Loss: 0.00001180
Iteration 186/1000 | Loss: 0.00001180
Iteration 187/1000 | Loss: 0.00001180
Iteration 188/1000 | Loss: 0.00001180
Iteration 189/1000 | Loss: 0.00001180
Iteration 190/1000 | Loss: 0.00001180
Iteration 191/1000 | Loss: 0.00001180
Iteration 192/1000 | Loss: 0.00001179
Iteration 193/1000 | Loss: 0.00001179
Iteration 194/1000 | Loss: 0.00001179
Iteration 195/1000 | Loss: 0.00001179
Iteration 196/1000 | Loss: 0.00001179
Iteration 197/1000 | Loss: 0.00001179
Iteration 198/1000 | Loss: 0.00001179
Iteration 199/1000 | Loss: 0.00001179
Iteration 200/1000 | Loss: 0.00001179
Iteration 201/1000 | Loss: 0.00001179
Iteration 202/1000 | Loss: 0.00001179
Iteration 203/1000 | Loss: 0.00001179
Iteration 204/1000 | Loss: 0.00001178
Iteration 205/1000 | Loss: 0.00001178
Iteration 206/1000 | Loss: 0.00001178
Iteration 207/1000 | Loss: 0.00001178
Iteration 208/1000 | Loss: 0.00001178
Iteration 209/1000 | Loss: 0.00001177
Iteration 210/1000 | Loss: 0.00001177
Iteration 211/1000 | Loss: 0.00001177
Iteration 212/1000 | Loss: 0.00001177
Iteration 213/1000 | Loss: 0.00001177
Iteration 214/1000 | Loss: 0.00001177
Iteration 215/1000 | Loss: 0.00001177
Iteration 216/1000 | Loss: 0.00001177
Iteration 217/1000 | Loss: 0.00001177
Iteration 218/1000 | Loss: 0.00001177
Iteration 219/1000 | Loss: 0.00001177
Iteration 220/1000 | Loss: 0.00001177
Iteration 221/1000 | Loss: 0.00001177
Iteration 222/1000 | Loss: 0.00001177
Iteration 223/1000 | Loss: 0.00001177
Iteration 224/1000 | Loss: 0.00001177
Iteration 225/1000 | Loss: 0.00001177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.1768500371545088e-05, 1.1768500371545088e-05, 1.1768500371545088e-05, 1.1768500371545088e-05, 1.1768500371545088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1768500371545088e-05

Optimization complete. Final v2v error: 2.9276840686798096 mm

Highest mean error: 3.8440253734588623 mm for frame 74

Lowest mean error: 2.637636184692383 mm for frame 97

Saving results

Total time: 43.628618240356445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968982
Iteration 2/25 | Loss: 0.00208094
Iteration 3/25 | Loss: 0.00160465
Iteration 4/25 | Loss: 0.00171499
Iteration 5/25 | Loss: 0.00140673
Iteration 6/25 | Loss: 0.00135540
Iteration 7/25 | Loss: 0.00134456
Iteration 8/25 | Loss: 0.00131840
Iteration 9/25 | Loss: 0.00129575
Iteration 10/25 | Loss: 0.00128975
Iteration 11/25 | Loss: 0.00128756
Iteration 12/25 | Loss: 0.00128512
Iteration 13/25 | Loss: 0.00128499
Iteration 14/25 | Loss: 0.00128357
Iteration 15/25 | Loss: 0.00128443
Iteration 16/25 | Loss: 0.00128594
Iteration 17/25 | Loss: 0.00128442
Iteration 18/25 | Loss: 0.00128429
Iteration 19/25 | Loss: 0.00128805
Iteration 20/25 | Loss: 0.00128695
Iteration 21/25 | Loss: 0.00128463
Iteration 22/25 | Loss: 0.00128550
Iteration 23/25 | Loss: 0.00128525
Iteration 24/25 | Loss: 0.00128522
Iteration 25/25 | Loss: 0.00128554

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32865679
Iteration 2/25 | Loss: 0.00084137
Iteration 3/25 | Loss: 0.00084137
Iteration 4/25 | Loss: 0.00084137
Iteration 5/25 | Loss: 0.00084137
Iteration 6/25 | Loss: 0.00084137
Iteration 7/25 | Loss: 0.00084137
Iteration 8/25 | Loss: 0.00084137
Iteration 9/25 | Loss: 0.00084137
Iteration 10/25 | Loss: 0.00084137
Iteration 11/25 | Loss: 0.00084137
Iteration 12/25 | Loss: 0.00084137
Iteration 13/25 | Loss: 0.00084137
Iteration 14/25 | Loss: 0.00084137
Iteration 15/25 | Loss: 0.00084137
Iteration 16/25 | Loss: 0.00084137
Iteration 17/25 | Loss: 0.00084137
Iteration 18/25 | Loss: 0.00084137
Iteration 19/25 | Loss: 0.00084137
Iteration 20/25 | Loss: 0.00084137
Iteration 21/25 | Loss: 0.00084137
Iteration 22/25 | Loss: 0.00084137
Iteration 23/25 | Loss: 0.00084137
Iteration 24/25 | Loss: 0.00084137
Iteration 25/25 | Loss: 0.00084137

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084137
Iteration 2/1000 | Loss: 0.00005107
Iteration 3/1000 | Loss: 0.00007464
Iteration 4/1000 | Loss: 0.00008243
Iteration 5/1000 | Loss: 0.00007111
Iteration 6/1000 | Loss: 0.00005380
Iteration 7/1000 | Loss: 0.00007824
Iteration 8/1000 | Loss: 0.00006981
Iteration 9/1000 | Loss: 0.00006536
Iteration 10/1000 | Loss: 0.00003613
Iteration 11/1000 | Loss: 0.00005238
Iteration 12/1000 | Loss: 0.00006134
Iteration 13/1000 | Loss: 0.00005763
Iteration 14/1000 | Loss: 0.00006128
Iteration 15/1000 | Loss: 0.00006147
Iteration 16/1000 | Loss: 0.00005546
Iteration 17/1000 | Loss: 0.00006358
Iteration 18/1000 | Loss: 0.00006542
Iteration 19/1000 | Loss: 0.00007026
Iteration 20/1000 | Loss: 0.00006963
Iteration 21/1000 | Loss: 0.00007130
Iteration 22/1000 | Loss: 0.00006776
Iteration 23/1000 | Loss: 0.00006967
Iteration 24/1000 | Loss: 0.00006054
Iteration 25/1000 | Loss: 0.00004841
Iteration 26/1000 | Loss: 0.00006535
Iteration 27/1000 | Loss: 0.00006004
Iteration 28/1000 | Loss: 0.00006936
Iteration 29/1000 | Loss: 0.00006091
Iteration 30/1000 | Loss: 0.00006572
Iteration 31/1000 | Loss: 0.00006560
Iteration 32/1000 | Loss: 0.00007022
Iteration 33/1000 | Loss: 0.00004906
Iteration 34/1000 | Loss: 0.00004607
Iteration 35/1000 | Loss: 0.00004182
Iteration 36/1000 | Loss: 0.00003765
Iteration 37/1000 | Loss: 0.00004099
Iteration 38/1000 | Loss: 0.00004995
Iteration 39/1000 | Loss: 0.00004954
Iteration 40/1000 | Loss: 0.00004848
Iteration 41/1000 | Loss: 0.00006396
Iteration 42/1000 | Loss: 0.00005369
Iteration 43/1000 | Loss: 0.00006105
Iteration 44/1000 | Loss: 0.00005112
Iteration 45/1000 | Loss: 0.00004045
Iteration 46/1000 | Loss: 0.00004077
Iteration 47/1000 | Loss: 0.00004574
Iteration 48/1000 | Loss: 0.00004007
Iteration 49/1000 | Loss: 0.00004249
Iteration 50/1000 | Loss: 0.00004135
Iteration 51/1000 | Loss: 0.00005032
Iteration 52/1000 | Loss: 0.00005112
Iteration 53/1000 | Loss: 0.00004683
Iteration 54/1000 | Loss: 0.00004702
Iteration 55/1000 | Loss: 0.00004991
Iteration 56/1000 | Loss: 0.00004992
Iteration 57/1000 | Loss: 0.00004828
Iteration 58/1000 | Loss: 0.00008426
Iteration 59/1000 | Loss: 0.00011369
Iteration 60/1000 | Loss: 0.00004261
Iteration 61/1000 | Loss: 0.00003892
Iteration 62/1000 | Loss: 0.00003537
Iteration 63/1000 | Loss: 0.00003763
Iteration 64/1000 | Loss: 0.00003227
Iteration 65/1000 | Loss: 0.00004347
Iteration 66/1000 | Loss: 0.00004084
Iteration 67/1000 | Loss: 0.00003672
Iteration 68/1000 | Loss: 0.00003385
Iteration 69/1000 | Loss: 0.00004345
Iteration 70/1000 | Loss: 0.00003974
Iteration 71/1000 | Loss: 0.00003879
Iteration 72/1000 | Loss: 0.00005780
Iteration 73/1000 | Loss: 0.00004629
Iteration 74/1000 | Loss: 0.00003301
Iteration 75/1000 | Loss: 0.00004112
Iteration 76/1000 | Loss: 0.00004065
Iteration 77/1000 | Loss: 0.00004658
Iteration 78/1000 | Loss: 0.00003608
Iteration 79/1000 | Loss: 0.00006607
Iteration 80/1000 | Loss: 0.00005121
Iteration 81/1000 | Loss: 0.00005852
Iteration 82/1000 | Loss: 0.00004287
Iteration 83/1000 | Loss: 0.00003117
Iteration 84/1000 | Loss: 0.00002843
Iteration 85/1000 | Loss: 0.00003577
Iteration 86/1000 | Loss: 0.00003722
Iteration 87/1000 | Loss: 0.00004330
Iteration 88/1000 | Loss: 0.00004529
Iteration 89/1000 | Loss: 0.00003710
Iteration 90/1000 | Loss: 0.00003656
Iteration 91/1000 | Loss: 0.00002773
Iteration 92/1000 | Loss: 0.00005431
Iteration 93/1000 | Loss: 0.00004879
Iteration 94/1000 | Loss: 0.00004147
Iteration 95/1000 | Loss: 0.00006215
Iteration 96/1000 | Loss: 0.00004378
Iteration 97/1000 | Loss: 0.00004207
Iteration 98/1000 | Loss: 0.00004366
Iteration 99/1000 | Loss: 0.00003831
Iteration 100/1000 | Loss: 0.00004961
Iteration 101/1000 | Loss: 0.00004237
Iteration 102/1000 | Loss: 0.00004602
Iteration 103/1000 | Loss: 0.00004181
Iteration 104/1000 | Loss: 0.00003079
Iteration 105/1000 | Loss: 0.00004927
Iteration 106/1000 | Loss: 0.00004893
Iteration 107/1000 | Loss: 0.00004720
Iteration 108/1000 | Loss: 0.00003587
Iteration 109/1000 | Loss: 0.00003062
Iteration 110/1000 | Loss: 0.00002590
Iteration 111/1000 | Loss: 0.00004914
Iteration 112/1000 | Loss: 0.00002675
Iteration 113/1000 | Loss: 0.00003350
Iteration 114/1000 | Loss: 0.00002675
Iteration 115/1000 | Loss: 0.00003926
Iteration 116/1000 | Loss: 0.00003829
Iteration 117/1000 | Loss: 0.00003367
Iteration 118/1000 | Loss: 0.00003093
Iteration 119/1000 | Loss: 0.00002820
Iteration 120/1000 | Loss: 0.00002586
Iteration 121/1000 | Loss: 0.00002417
Iteration 122/1000 | Loss: 0.00002307
Iteration 123/1000 | Loss: 0.00002237
Iteration 124/1000 | Loss: 0.00002170
Iteration 125/1000 | Loss: 0.00002130
Iteration 126/1000 | Loss: 0.00002114
Iteration 127/1000 | Loss: 0.00002113
Iteration 128/1000 | Loss: 0.00002111
Iteration 129/1000 | Loss: 0.00002110
Iteration 130/1000 | Loss: 0.00002107
Iteration 131/1000 | Loss: 0.00002106
Iteration 132/1000 | Loss: 0.00002099
Iteration 133/1000 | Loss: 0.00002096
Iteration 134/1000 | Loss: 0.00002095
Iteration 135/1000 | Loss: 0.00002095
Iteration 136/1000 | Loss: 0.00002094
Iteration 137/1000 | Loss: 0.00002094
Iteration 138/1000 | Loss: 0.00002093
Iteration 139/1000 | Loss: 0.00002093
Iteration 140/1000 | Loss: 0.00002092
Iteration 141/1000 | Loss: 0.00002092
Iteration 142/1000 | Loss: 0.00002092
Iteration 143/1000 | Loss: 0.00002091
Iteration 144/1000 | Loss: 0.00002091
Iteration 145/1000 | Loss: 0.00002091
Iteration 146/1000 | Loss: 0.00002089
Iteration 147/1000 | Loss: 0.00002089
Iteration 148/1000 | Loss: 0.00002089
Iteration 149/1000 | Loss: 0.00002089
Iteration 150/1000 | Loss: 0.00002089
Iteration 151/1000 | Loss: 0.00002089
Iteration 152/1000 | Loss: 0.00002089
Iteration 153/1000 | Loss: 0.00002089
Iteration 154/1000 | Loss: 0.00002088
Iteration 155/1000 | Loss: 0.00002088
Iteration 156/1000 | Loss: 0.00002088
Iteration 157/1000 | Loss: 0.00002088
Iteration 158/1000 | Loss: 0.00002088
Iteration 159/1000 | Loss: 0.00002087
Iteration 160/1000 | Loss: 0.00002087
Iteration 161/1000 | Loss: 0.00002087
Iteration 162/1000 | Loss: 0.00002086
Iteration 163/1000 | Loss: 0.00002086
Iteration 164/1000 | Loss: 0.00002086
Iteration 165/1000 | Loss: 0.00002086
Iteration 166/1000 | Loss: 0.00002086
Iteration 167/1000 | Loss: 0.00002086
Iteration 168/1000 | Loss: 0.00002086
Iteration 169/1000 | Loss: 0.00002086
Iteration 170/1000 | Loss: 0.00002085
Iteration 171/1000 | Loss: 0.00002085
Iteration 172/1000 | Loss: 0.00002084
Iteration 173/1000 | Loss: 0.00002084
Iteration 174/1000 | Loss: 0.00002083
Iteration 175/1000 | Loss: 0.00002083
Iteration 176/1000 | Loss: 0.00002083
Iteration 177/1000 | Loss: 0.00002083
Iteration 178/1000 | Loss: 0.00002083
Iteration 179/1000 | Loss: 0.00002082
Iteration 180/1000 | Loss: 0.00002082
Iteration 181/1000 | Loss: 0.00002082
Iteration 182/1000 | Loss: 0.00002082
Iteration 183/1000 | Loss: 0.00002082
Iteration 184/1000 | Loss: 0.00002082
Iteration 185/1000 | Loss: 0.00002081
Iteration 186/1000 | Loss: 0.00002081
Iteration 187/1000 | Loss: 0.00002081
Iteration 188/1000 | Loss: 0.00002081
Iteration 189/1000 | Loss: 0.00002081
Iteration 190/1000 | Loss: 0.00002081
Iteration 191/1000 | Loss: 0.00002081
Iteration 192/1000 | Loss: 0.00002081
Iteration 193/1000 | Loss: 0.00002081
Iteration 194/1000 | Loss: 0.00002081
Iteration 195/1000 | Loss: 0.00002081
Iteration 196/1000 | Loss: 0.00002081
Iteration 197/1000 | Loss: 0.00002081
Iteration 198/1000 | Loss: 0.00002081
Iteration 199/1000 | Loss: 0.00002081
Iteration 200/1000 | Loss: 0.00002081
Iteration 201/1000 | Loss: 0.00002081
Iteration 202/1000 | Loss: 0.00002081
Iteration 203/1000 | Loss: 0.00002081
Iteration 204/1000 | Loss: 0.00002081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.081207094306592e-05, 2.081207094306592e-05, 2.081207094306592e-05, 2.081207094306592e-05, 2.081207094306592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.081207094306592e-05

Optimization complete. Final v2v error: 3.832141399383545 mm

Highest mean error: 4.9657979011535645 mm for frame 227

Lowest mean error: 3.672607421875 mm for frame 223

Saving results

Total time: 260.5581932067871
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793082
Iteration 2/25 | Loss: 0.00134308
Iteration 3/25 | Loss: 0.00124795
Iteration 4/25 | Loss: 0.00123375
Iteration 5/25 | Loss: 0.00123016
Iteration 6/25 | Loss: 0.00122960
Iteration 7/25 | Loss: 0.00122960
Iteration 8/25 | Loss: 0.00122960
Iteration 9/25 | Loss: 0.00122960
Iteration 10/25 | Loss: 0.00122960
Iteration 11/25 | Loss: 0.00122960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012295980704948306, 0.0012295980704948306, 0.0012295980704948306, 0.0012295980704948306, 0.0012295980704948306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012295980704948306

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39716256
Iteration 2/25 | Loss: 0.00090858
Iteration 3/25 | Loss: 0.00090858
Iteration 4/25 | Loss: 0.00090858
Iteration 5/25 | Loss: 0.00090858
Iteration 6/25 | Loss: 0.00090858
Iteration 7/25 | Loss: 0.00090858
Iteration 8/25 | Loss: 0.00090858
Iteration 9/25 | Loss: 0.00090858
Iteration 10/25 | Loss: 0.00090858
Iteration 11/25 | Loss: 0.00090858
Iteration 12/25 | Loss: 0.00090858
Iteration 13/25 | Loss: 0.00090858
Iteration 14/25 | Loss: 0.00090858
Iteration 15/25 | Loss: 0.00090858
Iteration 16/25 | Loss: 0.00090858
Iteration 17/25 | Loss: 0.00090858
Iteration 18/25 | Loss: 0.00090858
Iteration 19/25 | Loss: 0.00090858
Iteration 20/25 | Loss: 0.00090858
Iteration 21/25 | Loss: 0.00090858
Iteration 22/25 | Loss: 0.00090858
Iteration 23/25 | Loss: 0.00090858
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009085767087526619, 0.0009085767087526619, 0.0009085767087526619, 0.0009085767087526619, 0.0009085767087526619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009085767087526619

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090858
Iteration 2/1000 | Loss: 0.00003143
Iteration 3/1000 | Loss: 0.00002241
Iteration 4/1000 | Loss: 0.00001875
Iteration 5/1000 | Loss: 0.00001759
Iteration 6/1000 | Loss: 0.00001692
Iteration 7/1000 | Loss: 0.00001647
Iteration 8/1000 | Loss: 0.00001596
Iteration 9/1000 | Loss: 0.00001565
Iteration 10/1000 | Loss: 0.00001531
Iteration 11/1000 | Loss: 0.00001503
Iteration 12/1000 | Loss: 0.00001486
Iteration 13/1000 | Loss: 0.00001473
Iteration 14/1000 | Loss: 0.00001468
Iteration 15/1000 | Loss: 0.00001459
Iteration 16/1000 | Loss: 0.00001459
Iteration 17/1000 | Loss: 0.00001454
Iteration 18/1000 | Loss: 0.00001453
Iteration 19/1000 | Loss: 0.00001453
Iteration 20/1000 | Loss: 0.00001452
Iteration 21/1000 | Loss: 0.00001452
Iteration 22/1000 | Loss: 0.00001448
Iteration 23/1000 | Loss: 0.00001445
Iteration 24/1000 | Loss: 0.00001444
Iteration 25/1000 | Loss: 0.00001444
Iteration 26/1000 | Loss: 0.00001444
Iteration 27/1000 | Loss: 0.00001443
Iteration 28/1000 | Loss: 0.00001442
Iteration 29/1000 | Loss: 0.00001442
Iteration 30/1000 | Loss: 0.00001441
Iteration 31/1000 | Loss: 0.00001441
Iteration 32/1000 | Loss: 0.00001441
Iteration 33/1000 | Loss: 0.00001440
Iteration 34/1000 | Loss: 0.00001440
Iteration 35/1000 | Loss: 0.00001439
Iteration 36/1000 | Loss: 0.00001437
Iteration 37/1000 | Loss: 0.00001437
Iteration 38/1000 | Loss: 0.00001437
Iteration 39/1000 | Loss: 0.00001437
Iteration 40/1000 | Loss: 0.00001437
Iteration 41/1000 | Loss: 0.00001437
Iteration 42/1000 | Loss: 0.00001436
Iteration 43/1000 | Loss: 0.00001436
Iteration 44/1000 | Loss: 0.00001436
Iteration 45/1000 | Loss: 0.00001436
Iteration 46/1000 | Loss: 0.00001436
Iteration 47/1000 | Loss: 0.00001436
Iteration 48/1000 | Loss: 0.00001436
Iteration 49/1000 | Loss: 0.00001436
Iteration 50/1000 | Loss: 0.00001435
Iteration 51/1000 | Loss: 0.00001435
Iteration 52/1000 | Loss: 0.00001435
Iteration 53/1000 | Loss: 0.00001435
Iteration 54/1000 | Loss: 0.00001433
Iteration 55/1000 | Loss: 0.00001432
Iteration 56/1000 | Loss: 0.00001431
Iteration 57/1000 | Loss: 0.00001431
Iteration 58/1000 | Loss: 0.00001431
Iteration 59/1000 | Loss: 0.00001430
Iteration 60/1000 | Loss: 0.00001430
Iteration 61/1000 | Loss: 0.00001430
Iteration 62/1000 | Loss: 0.00001430
Iteration 63/1000 | Loss: 0.00001430
Iteration 64/1000 | Loss: 0.00001430
Iteration 65/1000 | Loss: 0.00001430
Iteration 66/1000 | Loss: 0.00001430
Iteration 67/1000 | Loss: 0.00001430
Iteration 68/1000 | Loss: 0.00001430
Iteration 69/1000 | Loss: 0.00001430
Iteration 70/1000 | Loss: 0.00001429
Iteration 71/1000 | Loss: 0.00001429
Iteration 72/1000 | Loss: 0.00001429
Iteration 73/1000 | Loss: 0.00001429
Iteration 74/1000 | Loss: 0.00001429
Iteration 75/1000 | Loss: 0.00001427
Iteration 76/1000 | Loss: 0.00001427
Iteration 77/1000 | Loss: 0.00001427
Iteration 78/1000 | Loss: 0.00001427
Iteration 79/1000 | Loss: 0.00001427
Iteration 80/1000 | Loss: 0.00001427
Iteration 81/1000 | Loss: 0.00001427
Iteration 82/1000 | Loss: 0.00001426
Iteration 83/1000 | Loss: 0.00001426
Iteration 84/1000 | Loss: 0.00001426
Iteration 85/1000 | Loss: 0.00001426
Iteration 86/1000 | Loss: 0.00001426
Iteration 87/1000 | Loss: 0.00001426
Iteration 88/1000 | Loss: 0.00001426
Iteration 89/1000 | Loss: 0.00001426
Iteration 90/1000 | Loss: 0.00001426
Iteration 91/1000 | Loss: 0.00001426
Iteration 92/1000 | Loss: 0.00001426
Iteration 93/1000 | Loss: 0.00001426
Iteration 94/1000 | Loss: 0.00001425
Iteration 95/1000 | Loss: 0.00001425
Iteration 96/1000 | Loss: 0.00001425
Iteration 97/1000 | Loss: 0.00001425
Iteration 98/1000 | Loss: 0.00001425
Iteration 99/1000 | Loss: 0.00001425
Iteration 100/1000 | Loss: 0.00001424
Iteration 101/1000 | Loss: 0.00001423
Iteration 102/1000 | Loss: 0.00001423
Iteration 103/1000 | Loss: 0.00001423
Iteration 104/1000 | Loss: 0.00001422
Iteration 105/1000 | Loss: 0.00001421
Iteration 106/1000 | Loss: 0.00001421
Iteration 107/1000 | Loss: 0.00001421
Iteration 108/1000 | Loss: 0.00001421
Iteration 109/1000 | Loss: 0.00001421
Iteration 110/1000 | Loss: 0.00001420
Iteration 111/1000 | Loss: 0.00001420
Iteration 112/1000 | Loss: 0.00001420
Iteration 113/1000 | Loss: 0.00001420
Iteration 114/1000 | Loss: 0.00001419
Iteration 115/1000 | Loss: 0.00001419
Iteration 116/1000 | Loss: 0.00001419
Iteration 117/1000 | Loss: 0.00001418
Iteration 118/1000 | Loss: 0.00001418
Iteration 119/1000 | Loss: 0.00001418
Iteration 120/1000 | Loss: 0.00001418
Iteration 121/1000 | Loss: 0.00001418
Iteration 122/1000 | Loss: 0.00001417
Iteration 123/1000 | Loss: 0.00001417
Iteration 124/1000 | Loss: 0.00001417
Iteration 125/1000 | Loss: 0.00001417
Iteration 126/1000 | Loss: 0.00001417
Iteration 127/1000 | Loss: 0.00001417
Iteration 128/1000 | Loss: 0.00001417
Iteration 129/1000 | Loss: 0.00001417
Iteration 130/1000 | Loss: 0.00001417
Iteration 131/1000 | Loss: 0.00001417
Iteration 132/1000 | Loss: 0.00001416
Iteration 133/1000 | Loss: 0.00001416
Iteration 134/1000 | Loss: 0.00001416
Iteration 135/1000 | Loss: 0.00001416
Iteration 136/1000 | Loss: 0.00001415
Iteration 137/1000 | Loss: 0.00001415
Iteration 138/1000 | Loss: 0.00001415
Iteration 139/1000 | Loss: 0.00001415
Iteration 140/1000 | Loss: 0.00001414
Iteration 141/1000 | Loss: 0.00001414
Iteration 142/1000 | Loss: 0.00001414
Iteration 143/1000 | Loss: 0.00001414
Iteration 144/1000 | Loss: 0.00001414
Iteration 145/1000 | Loss: 0.00001414
Iteration 146/1000 | Loss: 0.00001414
Iteration 147/1000 | Loss: 0.00001414
Iteration 148/1000 | Loss: 0.00001414
Iteration 149/1000 | Loss: 0.00001414
Iteration 150/1000 | Loss: 0.00001414
Iteration 151/1000 | Loss: 0.00001414
Iteration 152/1000 | Loss: 0.00001414
Iteration 153/1000 | Loss: 0.00001414
Iteration 154/1000 | Loss: 0.00001414
Iteration 155/1000 | Loss: 0.00001414
Iteration 156/1000 | Loss: 0.00001414
Iteration 157/1000 | Loss: 0.00001414
Iteration 158/1000 | Loss: 0.00001414
Iteration 159/1000 | Loss: 0.00001414
Iteration 160/1000 | Loss: 0.00001414
Iteration 161/1000 | Loss: 0.00001414
Iteration 162/1000 | Loss: 0.00001414
Iteration 163/1000 | Loss: 0.00001414
Iteration 164/1000 | Loss: 0.00001414
Iteration 165/1000 | Loss: 0.00001414
Iteration 166/1000 | Loss: 0.00001414
Iteration 167/1000 | Loss: 0.00001414
Iteration 168/1000 | Loss: 0.00001414
Iteration 169/1000 | Loss: 0.00001414
Iteration 170/1000 | Loss: 0.00001414
Iteration 171/1000 | Loss: 0.00001414
Iteration 172/1000 | Loss: 0.00001414
Iteration 173/1000 | Loss: 0.00001414
Iteration 174/1000 | Loss: 0.00001414
Iteration 175/1000 | Loss: 0.00001414
Iteration 176/1000 | Loss: 0.00001414
Iteration 177/1000 | Loss: 0.00001414
Iteration 178/1000 | Loss: 0.00001414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.4144045053399168e-05, 1.4144045053399168e-05, 1.4144045053399168e-05, 1.4144045053399168e-05, 1.4144045053399168e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4144045053399168e-05

Optimization complete. Final v2v error: 3.224668025970459 mm

Highest mean error: 3.941650152206421 mm for frame 87

Lowest mean error: 3.0349345207214355 mm for frame 135

Saving results

Total time: 38.003015756607056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452283
Iteration 2/25 | Loss: 0.00130073
Iteration 3/25 | Loss: 0.00122554
Iteration 4/25 | Loss: 0.00121680
Iteration 5/25 | Loss: 0.00121476
Iteration 6/25 | Loss: 0.00121476
Iteration 7/25 | Loss: 0.00121476
Iteration 8/25 | Loss: 0.00121476
Iteration 9/25 | Loss: 0.00121476
Iteration 10/25 | Loss: 0.00121476
Iteration 11/25 | Loss: 0.00121476
Iteration 12/25 | Loss: 0.00121476
Iteration 13/25 | Loss: 0.00121476
Iteration 14/25 | Loss: 0.00121476
Iteration 15/25 | Loss: 0.00121476
Iteration 16/25 | Loss: 0.00121476
Iteration 17/25 | Loss: 0.00121476
Iteration 18/25 | Loss: 0.00121476
Iteration 19/25 | Loss: 0.00121476
Iteration 20/25 | Loss: 0.00121476
Iteration 21/25 | Loss: 0.00121476
Iteration 22/25 | Loss: 0.00121476
Iteration 23/25 | Loss: 0.00121476
Iteration 24/25 | Loss: 0.00121476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012147584930062294, 0.0012147584930062294, 0.0012147584930062294, 0.0012147584930062294, 0.0012147584930062294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012147584930062294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36075127
Iteration 2/25 | Loss: 0.00106083
Iteration 3/25 | Loss: 0.00106083
Iteration 4/25 | Loss: 0.00106083
Iteration 5/25 | Loss: 0.00106083
Iteration 6/25 | Loss: 0.00106083
Iteration 7/25 | Loss: 0.00106083
Iteration 8/25 | Loss: 0.00106083
Iteration 9/25 | Loss: 0.00106083
Iteration 10/25 | Loss: 0.00106083
Iteration 11/25 | Loss: 0.00106083
Iteration 12/25 | Loss: 0.00106083
Iteration 13/25 | Loss: 0.00106083
Iteration 14/25 | Loss: 0.00106083
Iteration 15/25 | Loss: 0.00106083
Iteration 16/25 | Loss: 0.00106083
Iteration 17/25 | Loss: 0.00106083
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010608261218294501, 0.0010608261218294501, 0.0010608261218294501, 0.0010608261218294501, 0.0010608261218294501]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010608261218294501

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106083
Iteration 2/1000 | Loss: 0.00002277
Iteration 3/1000 | Loss: 0.00001646
Iteration 4/1000 | Loss: 0.00001491
Iteration 5/1000 | Loss: 0.00001427
Iteration 6/1000 | Loss: 0.00001375
Iteration 7/1000 | Loss: 0.00001344
Iteration 8/1000 | Loss: 0.00001344
Iteration 9/1000 | Loss: 0.00001318
Iteration 10/1000 | Loss: 0.00001286
Iteration 11/1000 | Loss: 0.00001268
Iteration 12/1000 | Loss: 0.00001261
Iteration 13/1000 | Loss: 0.00001252
Iteration 14/1000 | Loss: 0.00001249
Iteration 15/1000 | Loss: 0.00001234
Iteration 16/1000 | Loss: 0.00001226
Iteration 17/1000 | Loss: 0.00001211
Iteration 18/1000 | Loss: 0.00001199
Iteration 19/1000 | Loss: 0.00001197
Iteration 20/1000 | Loss: 0.00001195
Iteration 21/1000 | Loss: 0.00001195
Iteration 22/1000 | Loss: 0.00001187
Iteration 23/1000 | Loss: 0.00001182
Iteration 24/1000 | Loss: 0.00001182
Iteration 25/1000 | Loss: 0.00001182
Iteration 26/1000 | Loss: 0.00001182
Iteration 27/1000 | Loss: 0.00001181
Iteration 28/1000 | Loss: 0.00001181
Iteration 29/1000 | Loss: 0.00001181
Iteration 30/1000 | Loss: 0.00001181
Iteration 31/1000 | Loss: 0.00001181
Iteration 32/1000 | Loss: 0.00001179
Iteration 33/1000 | Loss: 0.00001179
Iteration 34/1000 | Loss: 0.00001178
Iteration 35/1000 | Loss: 0.00001178
Iteration 36/1000 | Loss: 0.00001177
Iteration 37/1000 | Loss: 0.00001176
Iteration 38/1000 | Loss: 0.00001176
Iteration 39/1000 | Loss: 0.00001176
Iteration 40/1000 | Loss: 0.00001176
Iteration 41/1000 | Loss: 0.00001176
Iteration 42/1000 | Loss: 0.00001176
Iteration 43/1000 | Loss: 0.00001176
Iteration 44/1000 | Loss: 0.00001176
Iteration 45/1000 | Loss: 0.00001176
Iteration 46/1000 | Loss: 0.00001176
Iteration 47/1000 | Loss: 0.00001172
Iteration 48/1000 | Loss: 0.00001172
Iteration 49/1000 | Loss: 0.00001171
Iteration 50/1000 | Loss: 0.00001171
Iteration 51/1000 | Loss: 0.00001170
Iteration 52/1000 | Loss: 0.00001168
Iteration 53/1000 | Loss: 0.00001168
Iteration 54/1000 | Loss: 0.00001165
Iteration 55/1000 | Loss: 0.00001165
Iteration 56/1000 | Loss: 0.00001163
Iteration 57/1000 | Loss: 0.00001163
Iteration 58/1000 | Loss: 0.00001163
Iteration 59/1000 | Loss: 0.00001163
Iteration 60/1000 | Loss: 0.00001163
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001163
Iteration 65/1000 | Loss: 0.00001162
Iteration 66/1000 | Loss: 0.00001162
Iteration 67/1000 | Loss: 0.00001162
Iteration 68/1000 | Loss: 0.00001162
Iteration 69/1000 | Loss: 0.00001162
Iteration 70/1000 | Loss: 0.00001162
Iteration 71/1000 | Loss: 0.00001162
Iteration 72/1000 | Loss: 0.00001162
Iteration 73/1000 | Loss: 0.00001161
Iteration 74/1000 | Loss: 0.00001161
Iteration 75/1000 | Loss: 0.00001161
Iteration 76/1000 | Loss: 0.00001161
Iteration 77/1000 | Loss: 0.00001161
Iteration 78/1000 | Loss: 0.00001161
Iteration 79/1000 | Loss: 0.00001161
Iteration 80/1000 | Loss: 0.00001161
Iteration 81/1000 | Loss: 0.00001161
Iteration 82/1000 | Loss: 0.00001161
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001160
Iteration 85/1000 | Loss: 0.00001160
Iteration 86/1000 | Loss: 0.00001160
Iteration 87/1000 | Loss: 0.00001160
Iteration 88/1000 | Loss: 0.00001160
Iteration 89/1000 | Loss: 0.00001160
Iteration 90/1000 | Loss: 0.00001160
Iteration 91/1000 | Loss: 0.00001160
Iteration 92/1000 | Loss: 0.00001160
Iteration 93/1000 | Loss: 0.00001160
Iteration 94/1000 | Loss: 0.00001160
Iteration 95/1000 | Loss: 0.00001160
Iteration 96/1000 | Loss: 0.00001160
Iteration 97/1000 | Loss: 0.00001160
Iteration 98/1000 | Loss: 0.00001160
Iteration 99/1000 | Loss: 0.00001159
Iteration 100/1000 | Loss: 0.00001159
Iteration 101/1000 | Loss: 0.00001159
Iteration 102/1000 | Loss: 0.00001159
Iteration 103/1000 | Loss: 0.00001159
Iteration 104/1000 | Loss: 0.00001159
Iteration 105/1000 | Loss: 0.00001159
Iteration 106/1000 | Loss: 0.00001159
Iteration 107/1000 | Loss: 0.00001159
Iteration 108/1000 | Loss: 0.00001159
Iteration 109/1000 | Loss: 0.00001159
Iteration 110/1000 | Loss: 0.00001159
Iteration 111/1000 | Loss: 0.00001159
Iteration 112/1000 | Loss: 0.00001159
Iteration 113/1000 | Loss: 0.00001159
Iteration 114/1000 | Loss: 0.00001159
Iteration 115/1000 | Loss: 0.00001159
Iteration 116/1000 | Loss: 0.00001159
Iteration 117/1000 | Loss: 0.00001159
Iteration 118/1000 | Loss: 0.00001159
Iteration 119/1000 | Loss: 0.00001159
Iteration 120/1000 | Loss: 0.00001159
Iteration 121/1000 | Loss: 0.00001159
Iteration 122/1000 | Loss: 0.00001159
Iteration 123/1000 | Loss: 0.00001159
Iteration 124/1000 | Loss: 0.00001159
Iteration 125/1000 | Loss: 0.00001159
Iteration 126/1000 | Loss: 0.00001159
Iteration 127/1000 | Loss: 0.00001159
Iteration 128/1000 | Loss: 0.00001159
Iteration 129/1000 | Loss: 0.00001159
Iteration 130/1000 | Loss: 0.00001159
Iteration 131/1000 | Loss: 0.00001159
Iteration 132/1000 | Loss: 0.00001159
Iteration 133/1000 | Loss: 0.00001159
Iteration 134/1000 | Loss: 0.00001159
Iteration 135/1000 | Loss: 0.00001159
Iteration 136/1000 | Loss: 0.00001159
Iteration 137/1000 | Loss: 0.00001159
Iteration 138/1000 | Loss: 0.00001159
Iteration 139/1000 | Loss: 0.00001159
Iteration 140/1000 | Loss: 0.00001159
Iteration 141/1000 | Loss: 0.00001159
Iteration 142/1000 | Loss: 0.00001159
Iteration 143/1000 | Loss: 0.00001159
Iteration 144/1000 | Loss: 0.00001159
Iteration 145/1000 | Loss: 0.00001159
Iteration 146/1000 | Loss: 0.00001159
Iteration 147/1000 | Loss: 0.00001159
Iteration 148/1000 | Loss: 0.00001159
Iteration 149/1000 | Loss: 0.00001159
Iteration 150/1000 | Loss: 0.00001159
Iteration 151/1000 | Loss: 0.00001159
Iteration 152/1000 | Loss: 0.00001159
Iteration 153/1000 | Loss: 0.00001159
Iteration 154/1000 | Loss: 0.00001159
Iteration 155/1000 | Loss: 0.00001159
Iteration 156/1000 | Loss: 0.00001159
Iteration 157/1000 | Loss: 0.00001159
Iteration 158/1000 | Loss: 0.00001159
Iteration 159/1000 | Loss: 0.00001159
Iteration 160/1000 | Loss: 0.00001159
Iteration 161/1000 | Loss: 0.00001159
Iteration 162/1000 | Loss: 0.00001159
Iteration 163/1000 | Loss: 0.00001159
Iteration 164/1000 | Loss: 0.00001159
Iteration 165/1000 | Loss: 0.00001159
Iteration 166/1000 | Loss: 0.00001159
Iteration 167/1000 | Loss: 0.00001159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.159256134997122e-05, 1.159256134997122e-05, 1.159256134997122e-05, 1.159256134997122e-05, 1.159256134997122e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.159256134997122e-05

Optimization complete. Final v2v error: 2.9164226055145264 mm

Highest mean error: 3.352487802505493 mm for frame 221

Lowest mean error: 2.6998322010040283 mm for frame 133

Saving results

Total time: 41.65700054168701
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798175
Iteration 2/25 | Loss: 0.00143392
Iteration 3/25 | Loss: 0.00123998
Iteration 4/25 | Loss: 0.00121359
Iteration 5/25 | Loss: 0.00120646
Iteration 6/25 | Loss: 0.00120445
Iteration 7/25 | Loss: 0.00120445
Iteration 8/25 | Loss: 0.00120445
Iteration 9/25 | Loss: 0.00120445
Iteration 10/25 | Loss: 0.00120445
Iteration 11/25 | Loss: 0.00120445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012044510804116726, 0.0012044510804116726, 0.0012044510804116726, 0.0012044510804116726, 0.0012044510804116726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012044510804116726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37997770
Iteration 2/25 | Loss: 0.00116945
Iteration 3/25 | Loss: 0.00116945
Iteration 4/25 | Loss: 0.00116945
Iteration 5/25 | Loss: 0.00116945
Iteration 6/25 | Loss: 0.00116945
Iteration 7/25 | Loss: 0.00116945
Iteration 8/25 | Loss: 0.00116945
Iteration 9/25 | Loss: 0.00116945
Iteration 10/25 | Loss: 0.00116945
Iteration 11/25 | Loss: 0.00116945
Iteration 12/25 | Loss: 0.00116945
Iteration 13/25 | Loss: 0.00116945
Iteration 14/25 | Loss: 0.00116945
Iteration 15/25 | Loss: 0.00116945
Iteration 16/25 | Loss: 0.00116945
Iteration 17/25 | Loss: 0.00116945
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011694476706907153, 0.0011694476706907153, 0.0011694476706907153, 0.0011694476706907153, 0.0011694476706907153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011694476706907153

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116945
Iteration 2/1000 | Loss: 0.00004394
Iteration 3/1000 | Loss: 0.00002821
Iteration 4/1000 | Loss: 0.00002049
Iteration 5/1000 | Loss: 0.00001856
Iteration 6/1000 | Loss: 0.00001733
Iteration 7/1000 | Loss: 0.00001615
Iteration 8/1000 | Loss: 0.00001544
Iteration 9/1000 | Loss: 0.00001497
Iteration 10/1000 | Loss: 0.00001461
Iteration 11/1000 | Loss: 0.00001434
Iteration 12/1000 | Loss: 0.00001412
Iteration 13/1000 | Loss: 0.00001396
Iteration 14/1000 | Loss: 0.00001395
Iteration 15/1000 | Loss: 0.00001386
Iteration 16/1000 | Loss: 0.00001382
Iteration 17/1000 | Loss: 0.00001382
Iteration 18/1000 | Loss: 0.00001380
Iteration 19/1000 | Loss: 0.00001374
Iteration 20/1000 | Loss: 0.00001371
Iteration 21/1000 | Loss: 0.00001370
Iteration 22/1000 | Loss: 0.00001370
Iteration 23/1000 | Loss: 0.00001367
Iteration 24/1000 | Loss: 0.00001367
Iteration 25/1000 | Loss: 0.00001366
Iteration 26/1000 | Loss: 0.00001366
Iteration 27/1000 | Loss: 0.00001366
Iteration 28/1000 | Loss: 0.00001365
Iteration 29/1000 | Loss: 0.00001364
Iteration 30/1000 | Loss: 0.00001363
Iteration 31/1000 | Loss: 0.00001363
Iteration 32/1000 | Loss: 0.00001362
Iteration 33/1000 | Loss: 0.00001361
Iteration 34/1000 | Loss: 0.00001361
Iteration 35/1000 | Loss: 0.00001360
Iteration 36/1000 | Loss: 0.00001360
Iteration 37/1000 | Loss: 0.00001360
Iteration 38/1000 | Loss: 0.00001359
Iteration 39/1000 | Loss: 0.00001359
Iteration 40/1000 | Loss: 0.00001359
Iteration 41/1000 | Loss: 0.00001359
Iteration 42/1000 | Loss: 0.00001359
Iteration 43/1000 | Loss: 0.00001358
Iteration 44/1000 | Loss: 0.00001358
Iteration 45/1000 | Loss: 0.00001358
Iteration 46/1000 | Loss: 0.00001357
Iteration 47/1000 | Loss: 0.00001357
Iteration 48/1000 | Loss: 0.00001357
Iteration 49/1000 | Loss: 0.00001356
Iteration 50/1000 | Loss: 0.00001356
Iteration 51/1000 | Loss: 0.00001355
Iteration 52/1000 | Loss: 0.00001355
Iteration 53/1000 | Loss: 0.00001355
Iteration 54/1000 | Loss: 0.00001354
Iteration 55/1000 | Loss: 0.00001354
Iteration 56/1000 | Loss: 0.00001354
Iteration 57/1000 | Loss: 0.00001354
Iteration 58/1000 | Loss: 0.00001353
Iteration 59/1000 | Loss: 0.00001353
Iteration 60/1000 | Loss: 0.00001353
Iteration 61/1000 | Loss: 0.00001352
Iteration 62/1000 | Loss: 0.00001352
Iteration 63/1000 | Loss: 0.00001351
Iteration 64/1000 | Loss: 0.00001351
Iteration 65/1000 | Loss: 0.00001351
Iteration 66/1000 | Loss: 0.00001350
Iteration 67/1000 | Loss: 0.00001350
Iteration 68/1000 | Loss: 0.00001349
Iteration 69/1000 | Loss: 0.00001349
Iteration 70/1000 | Loss: 0.00001348
Iteration 71/1000 | Loss: 0.00001348
Iteration 72/1000 | Loss: 0.00001348
Iteration 73/1000 | Loss: 0.00001347
Iteration 74/1000 | Loss: 0.00001347
Iteration 75/1000 | Loss: 0.00001347
Iteration 76/1000 | Loss: 0.00001346
Iteration 77/1000 | Loss: 0.00001346
Iteration 78/1000 | Loss: 0.00001346
Iteration 79/1000 | Loss: 0.00001346
Iteration 80/1000 | Loss: 0.00001346
Iteration 81/1000 | Loss: 0.00001345
Iteration 82/1000 | Loss: 0.00001345
Iteration 83/1000 | Loss: 0.00001345
Iteration 84/1000 | Loss: 0.00001345
Iteration 85/1000 | Loss: 0.00001344
Iteration 86/1000 | Loss: 0.00001344
Iteration 87/1000 | Loss: 0.00001344
Iteration 88/1000 | Loss: 0.00001344
Iteration 89/1000 | Loss: 0.00001343
Iteration 90/1000 | Loss: 0.00001343
Iteration 91/1000 | Loss: 0.00001343
Iteration 92/1000 | Loss: 0.00001343
Iteration 93/1000 | Loss: 0.00001342
Iteration 94/1000 | Loss: 0.00001342
Iteration 95/1000 | Loss: 0.00001342
Iteration 96/1000 | Loss: 0.00001341
Iteration 97/1000 | Loss: 0.00001341
Iteration 98/1000 | Loss: 0.00001341
Iteration 99/1000 | Loss: 0.00001341
Iteration 100/1000 | Loss: 0.00001341
Iteration 101/1000 | Loss: 0.00001341
Iteration 102/1000 | Loss: 0.00001341
Iteration 103/1000 | Loss: 0.00001341
Iteration 104/1000 | Loss: 0.00001340
Iteration 105/1000 | Loss: 0.00001340
Iteration 106/1000 | Loss: 0.00001340
Iteration 107/1000 | Loss: 0.00001339
Iteration 108/1000 | Loss: 0.00001339
Iteration 109/1000 | Loss: 0.00001339
Iteration 110/1000 | Loss: 0.00001339
Iteration 111/1000 | Loss: 0.00001339
Iteration 112/1000 | Loss: 0.00001339
Iteration 113/1000 | Loss: 0.00001339
Iteration 114/1000 | Loss: 0.00001339
Iteration 115/1000 | Loss: 0.00001338
Iteration 116/1000 | Loss: 0.00001338
Iteration 117/1000 | Loss: 0.00001338
Iteration 118/1000 | Loss: 0.00001338
Iteration 119/1000 | Loss: 0.00001337
Iteration 120/1000 | Loss: 0.00001337
Iteration 121/1000 | Loss: 0.00001337
Iteration 122/1000 | Loss: 0.00001336
Iteration 123/1000 | Loss: 0.00001336
Iteration 124/1000 | Loss: 0.00001336
Iteration 125/1000 | Loss: 0.00001336
Iteration 126/1000 | Loss: 0.00001335
Iteration 127/1000 | Loss: 0.00001335
Iteration 128/1000 | Loss: 0.00001335
Iteration 129/1000 | Loss: 0.00001335
Iteration 130/1000 | Loss: 0.00001334
Iteration 131/1000 | Loss: 0.00001334
Iteration 132/1000 | Loss: 0.00001334
Iteration 133/1000 | Loss: 0.00001333
Iteration 134/1000 | Loss: 0.00001333
Iteration 135/1000 | Loss: 0.00001333
Iteration 136/1000 | Loss: 0.00001332
Iteration 137/1000 | Loss: 0.00001332
Iteration 138/1000 | Loss: 0.00001332
Iteration 139/1000 | Loss: 0.00001332
Iteration 140/1000 | Loss: 0.00001332
Iteration 141/1000 | Loss: 0.00001332
Iteration 142/1000 | Loss: 0.00001332
Iteration 143/1000 | Loss: 0.00001332
Iteration 144/1000 | Loss: 0.00001332
Iteration 145/1000 | Loss: 0.00001332
Iteration 146/1000 | Loss: 0.00001331
Iteration 147/1000 | Loss: 0.00001331
Iteration 148/1000 | Loss: 0.00001331
Iteration 149/1000 | Loss: 0.00001331
Iteration 150/1000 | Loss: 0.00001331
Iteration 151/1000 | Loss: 0.00001330
Iteration 152/1000 | Loss: 0.00001330
Iteration 153/1000 | Loss: 0.00001330
Iteration 154/1000 | Loss: 0.00001330
Iteration 155/1000 | Loss: 0.00001329
Iteration 156/1000 | Loss: 0.00001329
Iteration 157/1000 | Loss: 0.00001329
Iteration 158/1000 | Loss: 0.00001328
Iteration 159/1000 | Loss: 0.00001328
Iteration 160/1000 | Loss: 0.00001328
Iteration 161/1000 | Loss: 0.00001327
Iteration 162/1000 | Loss: 0.00001327
Iteration 163/1000 | Loss: 0.00001327
Iteration 164/1000 | Loss: 0.00001327
Iteration 165/1000 | Loss: 0.00001327
Iteration 166/1000 | Loss: 0.00001327
Iteration 167/1000 | Loss: 0.00001327
Iteration 168/1000 | Loss: 0.00001327
Iteration 169/1000 | Loss: 0.00001327
Iteration 170/1000 | Loss: 0.00001327
Iteration 171/1000 | Loss: 0.00001326
Iteration 172/1000 | Loss: 0.00001326
Iteration 173/1000 | Loss: 0.00001326
Iteration 174/1000 | Loss: 0.00001326
Iteration 175/1000 | Loss: 0.00001326
Iteration 176/1000 | Loss: 0.00001326
Iteration 177/1000 | Loss: 0.00001325
Iteration 178/1000 | Loss: 0.00001325
Iteration 179/1000 | Loss: 0.00001325
Iteration 180/1000 | Loss: 0.00001325
Iteration 181/1000 | Loss: 0.00001324
Iteration 182/1000 | Loss: 0.00001324
Iteration 183/1000 | Loss: 0.00001324
Iteration 184/1000 | Loss: 0.00001323
Iteration 185/1000 | Loss: 0.00001323
Iteration 186/1000 | Loss: 0.00001323
Iteration 187/1000 | Loss: 0.00001323
Iteration 188/1000 | Loss: 0.00001322
Iteration 189/1000 | Loss: 0.00001322
Iteration 190/1000 | Loss: 0.00001322
Iteration 191/1000 | Loss: 0.00001322
Iteration 192/1000 | Loss: 0.00001322
Iteration 193/1000 | Loss: 0.00001321
Iteration 194/1000 | Loss: 0.00001321
Iteration 195/1000 | Loss: 0.00001321
Iteration 196/1000 | Loss: 0.00001321
Iteration 197/1000 | Loss: 0.00001321
Iteration 198/1000 | Loss: 0.00001320
Iteration 199/1000 | Loss: 0.00001320
Iteration 200/1000 | Loss: 0.00001320
Iteration 201/1000 | Loss: 0.00001320
Iteration 202/1000 | Loss: 0.00001320
Iteration 203/1000 | Loss: 0.00001319
Iteration 204/1000 | Loss: 0.00001319
Iteration 205/1000 | Loss: 0.00001319
Iteration 206/1000 | Loss: 0.00001319
Iteration 207/1000 | Loss: 0.00001319
Iteration 208/1000 | Loss: 0.00001319
Iteration 209/1000 | Loss: 0.00001319
Iteration 210/1000 | Loss: 0.00001319
Iteration 211/1000 | Loss: 0.00001319
Iteration 212/1000 | Loss: 0.00001319
Iteration 213/1000 | Loss: 0.00001319
Iteration 214/1000 | Loss: 0.00001318
Iteration 215/1000 | Loss: 0.00001318
Iteration 216/1000 | Loss: 0.00001318
Iteration 217/1000 | Loss: 0.00001318
Iteration 218/1000 | Loss: 0.00001318
Iteration 219/1000 | Loss: 0.00001318
Iteration 220/1000 | Loss: 0.00001318
Iteration 221/1000 | Loss: 0.00001318
Iteration 222/1000 | Loss: 0.00001318
Iteration 223/1000 | Loss: 0.00001317
Iteration 224/1000 | Loss: 0.00001317
Iteration 225/1000 | Loss: 0.00001317
Iteration 226/1000 | Loss: 0.00001317
Iteration 227/1000 | Loss: 0.00001317
Iteration 228/1000 | Loss: 0.00001317
Iteration 229/1000 | Loss: 0.00001317
Iteration 230/1000 | Loss: 0.00001317
Iteration 231/1000 | Loss: 0.00001317
Iteration 232/1000 | Loss: 0.00001317
Iteration 233/1000 | Loss: 0.00001317
Iteration 234/1000 | Loss: 0.00001317
Iteration 235/1000 | Loss: 0.00001316
Iteration 236/1000 | Loss: 0.00001316
Iteration 237/1000 | Loss: 0.00001316
Iteration 238/1000 | Loss: 0.00001316
Iteration 239/1000 | Loss: 0.00001316
Iteration 240/1000 | Loss: 0.00001316
Iteration 241/1000 | Loss: 0.00001316
Iteration 242/1000 | Loss: 0.00001316
Iteration 243/1000 | Loss: 0.00001315
Iteration 244/1000 | Loss: 0.00001315
Iteration 245/1000 | Loss: 0.00001315
Iteration 246/1000 | Loss: 0.00001315
Iteration 247/1000 | Loss: 0.00001315
Iteration 248/1000 | Loss: 0.00001314
Iteration 249/1000 | Loss: 0.00001314
Iteration 250/1000 | Loss: 0.00001314
Iteration 251/1000 | Loss: 0.00001314
Iteration 252/1000 | Loss: 0.00001314
Iteration 253/1000 | Loss: 0.00001314
Iteration 254/1000 | Loss: 0.00001314
Iteration 255/1000 | Loss: 0.00001314
Iteration 256/1000 | Loss: 0.00001314
Iteration 257/1000 | Loss: 0.00001313
Iteration 258/1000 | Loss: 0.00001313
Iteration 259/1000 | Loss: 0.00001313
Iteration 260/1000 | Loss: 0.00001313
Iteration 261/1000 | Loss: 0.00001313
Iteration 262/1000 | Loss: 0.00001313
Iteration 263/1000 | Loss: 0.00001313
Iteration 264/1000 | Loss: 0.00001313
Iteration 265/1000 | Loss: 0.00001313
Iteration 266/1000 | Loss: 0.00001312
Iteration 267/1000 | Loss: 0.00001312
Iteration 268/1000 | Loss: 0.00001312
Iteration 269/1000 | Loss: 0.00001312
Iteration 270/1000 | Loss: 0.00001312
Iteration 271/1000 | Loss: 0.00001312
Iteration 272/1000 | Loss: 0.00001312
Iteration 273/1000 | Loss: 0.00001312
Iteration 274/1000 | Loss: 0.00001312
Iteration 275/1000 | Loss: 0.00001312
Iteration 276/1000 | Loss: 0.00001312
Iteration 277/1000 | Loss: 0.00001312
Iteration 278/1000 | Loss: 0.00001312
Iteration 279/1000 | Loss: 0.00001312
Iteration 280/1000 | Loss: 0.00001312
Iteration 281/1000 | Loss: 0.00001311
Iteration 282/1000 | Loss: 0.00001311
Iteration 283/1000 | Loss: 0.00001311
Iteration 284/1000 | Loss: 0.00001311
Iteration 285/1000 | Loss: 0.00001311
Iteration 286/1000 | Loss: 0.00001311
Iteration 287/1000 | Loss: 0.00001311
Iteration 288/1000 | Loss: 0.00001311
Iteration 289/1000 | Loss: 0.00001311
Iteration 290/1000 | Loss: 0.00001311
Iteration 291/1000 | Loss: 0.00001310
Iteration 292/1000 | Loss: 0.00001310
Iteration 293/1000 | Loss: 0.00001310
Iteration 294/1000 | Loss: 0.00001310
Iteration 295/1000 | Loss: 0.00001310
Iteration 296/1000 | Loss: 0.00001310
Iteration 297/1000 | Loss: 0.00001310
Iteration 298/1000 | Loss: 0.00001309
Iteration 299/1000 | Loss: 0.00001309
Iteration 300/1000 | Loss: 0.00001309
Iteration 301/1000 | Loss: 0.00001309
Iteration 302/1000 | Loss: 0.00001309
Iteration 303/1000 | Loss: 0.00001309
Iteration 304/1000 | Loss: 0.00001309
Iteration 305/1000 | Loss: 0.00001309
Iteration 306/1000 | Loss: 0.00001309
Iteration 307/1000 | Loss: 0.00001308
Iteration 308/1000 | Loss: 0.00001308
Iteration 309/1000 | Loss: 0.00001308
Iteration 310/1000 | Loss: 0.00001308
Iteration 311/1000 | Loss: 0.00001308
Iteration 312/1000 | Loss: 0.00001308
Iteration 313/1000 | Loss: 0.00001308
Iteration 314/1000 | Loss: 0.00001308
Iteration 315/1000 | Loss: 0.00001308
Iteration 316/1000 | Loss: 0.00001308
Iteration 317/1000 | Loss: 0.00001308
Iteration 318/1000 | Loss: 0.00001308
Iteration 319/1000 | Loss: 0.00001308
Iteration 320/1000 | Loss: 0.00001308
Iteration 321/1000 | Loss: 0.00001308
Iteration 322/1000 | Loss: 0.00001308
Iteration 323/1000 | Loss: 0.00001308
Iteration 324/1000 | Loss: 0.00001308
Iteration 325/1000 | Loss: 0.00001307
Iteration 326/1000 | Loss: 0.00001307
Iteration 327/1000 | Loss: 0.00001307
Iteration 328/1000 | Loss: 0.00001307
Iteration 329/1000 | Loss: 0.00001307
Iteration 330/1000 | Loss: 0.00001307
Iteration 331/1000 | Loss: 0.00001307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 331. Stopping optimization.
Last 5 losses: [1.3074542948743328e-05, 1.3074542948743328e-05, 1.3074542948743328e-05, 1.3074542948743328e-05, 1.3074542948743328e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3074542948743328e-05

Optimization complete. Final v2v error: 3.076843023300171 mm

Highest mean error: 3.525172233581543 mm for frame 79

Lowest mean error: 2.719395160675049 mm for frame 154

Saving results

Total time: 52.308120250701904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467290
Iteration 2/25 | Loss: 0.00152780
Iteration 3/25 | Loss: 0.00132579
Iteration 4/25 | Loss: 0.00130149
Iteration 5/25 | Loss: 0.00129826
Iteration 6/25 | Loss: 0.00129797
Iteration 7/25 | Loss: 0.00129797
Iteration 8/25 | Loss: 0.00129797
Iteration 9/25 | Loss: 0.00129797
Iteration 10/25 | Loss: 0.00129797
Iteration 11/25 | Loss: 0.00129797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012979683233425021, 0.0012979683233425021, 0.0012979683233425021, 0.0012979683233425021, 0.0012979683233425021]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012979683233425021

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34947395
Iteration 2/25 | Loss: 0.00104333
Iteration 3/25 | Loss: 0.00104332
Iteration 4/25 | Loss: 0.00104332
Iteration 5/25 | Loss: 0.00104332
Iteration 6/25 | Loss: 0.00104332
Iteration 7/25 | Loss: 0.00104332
Iteration 8/25 | Loss: 0.00104332
Iteration 9/25 | Loss: 0.00104332
Iteration 10/25 | Loss: 0.00104331
Iteration 11/25 | Loss: 0.00104331
Iteration 12/25 | Loss: 0.00104331
Iteration 13/25 | Loss: 0.00104331
Iteration 14/25 | Loss: 0.00104331
Iteration 15/25 | Loss: 0.00104331
Iteration 16/25 | Loss: 0.00104331
Iteration 17/25 | Loss: 0.00104331
Iteration 18/25 | Loss: 0.00104331
Iteration 19/25 | Loss: 0.00104331
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010433144634589553, 0.0010433144634589553, 0.0010433144634589553, 0.0010433144634589553, 0.0010433144634589553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010433144634589553

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104331
Iteration 2/1000 | Loss: 0.00004627
Iteration 3/1000 | Loss: 0.00002819
Iteration 4/1000 | Loss: 0.00002566
Iteration 5/1000 | Loss: 0.00002410
Iteration 6/1000 | Loss: 0.00002295
Iteration 7/1000 | Loss: 0.00002222
Iteration 8/1000 | Loss: 0.00002161
Iteration 9/1000 | Loss: 0.00002107
Iteration 10/1000 | Loss: 0.00002067
Iteration 11/1000 | Loss: 0.00002040
Iteration 12/1000 | Loss: 0.00002025
Iteration 13/1000 | Loss: 0.00002023
Iteration 14/1000 | Loss: 0.00002015
Iteration 15/1000 | Loss: 0.00002015
Iteration 16/1000 | Loss: 0.00001999
Iteration 17/1000 | Loss: 0.00001998
Iteration 18/1000 | Loss: 0.00001990
Iteration 19/1000 | Loss: 0.00001983
Iteration 20/1000 | Loss: 0.00001979
Iteration 21/1000 | Loss: 0.00001976
Iteration 22/1000 | Loss: 0.00001975
Iteration 23/1000 | Loss: 0.00001971
Iteration 24/1000 | Loss: 0.00001971
Iteration 25/1000 | Loss: 0.00001971
Iteration 26/1000 | Loss: 0.00001971
Iteration 27/1000 | Loss: 0.00001971
Iteration 28/1000 | Loss: 0.00001971
Iteration 29/1000 | Loss: 0.00001971
Iteration 30/1000 | Loss: 0.00001971
Iteration 31/1000 | Loss: 0.00001971
Iteration 32/1000 | Loss: 0.00001971
Iteration 33/1000 | Loss: 0.00001970
Iteration 34/1000 | Loss: 0.00001970
Iteration 35/1000 | Loss: 0.00001970
Iteration 36/1000 | Loss: 0.00001970
Iteration 37/1000 | Loss: 0.00001970
Iteration 38/1000 | Loss: 0.00001970
Iteration 39/1000 | Loss: 0.00001970
Iteration 40/1000 | Loss: 0.00001969
Iteration 41/1000 | Loss: 0.00001969
Iteration 42/1000 | Loss: 0.00001968
Iteration 43/1000 | Loss: 0.00001968
Iteration 44/1000 | Loss: 0.00001968
Iteration 45/1000 | Loss: 0.00001967
Iteration 46/1000 | Loss: 0.00001967
Iteration 47/1000 | Loss: 0.00001967
Iteration 48/1000 | Loss: 0.00001967
Iteration 49/1000 | Loss: 0.00001967
Iteration 50/1000 | Loss: 0.00001966
Iteration 51/1000 | Loss: 0.00001966
Iteration 52/1000 | Loss: 0.00001966
Iteration 53/1000 | Loss: 0.00001965
Iteration 54/1000 | Loss: 0.00001965
Iteration 55/1000 | Loss: 0.00001965
Iteration 56/1000 | Loss: 0.00001965
Iteration 57/1000 | Loss: 0.00001964
Iteration 58/1000 | Loss: 0.00001964
Iteration 59/1000 | Loss: 0.00001963
Iteration 60/1000 | Loss: 0.00001963
Iteration 61/1000 | Loss: 0.00001962
Iteration 62/1000 | Loss: 0.00001961
Iteration 63/1000 | Loss: 0.00001960
Iteration 64/1000 | Loss: 0.00001960
Iteration 65/1000 | Loss: 0.00001959
Iteration 66/1000 | Loss: 0.00001958
Iteration 67/1000 | Loss: 0.00001958
Iteration 68/1000 | Loss: 0.00001957
Iteration 69/1000 | Loss: 0.00001956
Iteration 70/1000 | Loss: 0.00001956
Iteration 71/1000 | Loss: 0.00001955
Iteration 72/1000 | Loss: 0.00001955
Iteration 73/1000 | Loss: 0.00001955
Iteration 74/1000 | Loss: 0.00001955
Iteration 75/1000 | Loss: 0.00001954
Iteration 76/1000 | Loss: 0.00001954
Iteration 77/1000 | Loss: 0.00001954
Iteration 78/1000 | Loss: 0.00001953
Iteration 79/1000 | Loss: 0.00001952
Iteration 80/1000 | Loss: 0.00001952
Iteration 81/1000 | Loss: 0.00001952
Iteration 82/1000 | Loss: 0.00001951
Iteration 83/1000 | Loss: 0.00001951
Iteration 84/1000 | Loss: 0.00001951
Iteration 85/1000 | Loss: 0.00001950
Iteration 86/1000 | Loss: 0.00001950
Iteration 87/1000 | Loss: 0.00001950
Iteration 88/1000 | Loss: 0.00001950
Iteration 89/1000 | Loss: 0.00001950
Iteration 90/1000 | Loss: 0.00001950
Iteration 91/1000 | Loss: 0.00001950
Iteration 92/1000 | Loss: 0.00001950
Iteration 93/1000 | Loss: 0.00001950
Iteration 94/1000 | Loss: 0.00001950
Iteration 95/1000 | Loss: 0.00001950
Iteration 96/1000 | Loss: 0.00001950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.9495571905281395e-05, 1.9495571905281395e-05, 1.9495571905281395e-05, 1.9495571905281395e-05, 1.9495571905281395e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9495571905281395e-05

Optimization complete. Final v2v error: 3.6952195167541504 mm

Highest mean error: 3.9884209632873535 mm for frame 162

Lowest mean error: 3.272533655166626 mm for frame 27

Saving results

Total time: 41.377723932266235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072998
Iteration 2/25 | Loss: 0.00259921
Iteration 3/25 | Loss: 0.00185298
Iteration 4/25 | Loss: 0.00174102
Iteration 5/25 | Loss: 0.00164197
Iteration 6/25 | Loss: 0.00162316
Iteration 7/25 | Loss: 0.00154460
Iteration 8/25 | Loss: 0.00153939
Iteration 9/25 | Loss: 0.00154037
Iteration 10/25 | Loss: 0.00147399
Iteration 11/25 | Loss: 0.00144394
Iteration 12/25 | Loss: 0.00141785
Iteration 13/25 | Loss: 0.00138794
Iteration 14/25 | Loss: 0.00137607
Iteration 15/25 | Loss: 0.00136071
Iteration 16/25 | Loss: 0.00135911
Iteration 17/25 | Loss: 0.00134545
Iteration 18/25 | Loss: 0.00133872
Iteration 19/25 | Loss: 0.00133098
Iteration 20/25 | Loss: 0.00133456
Iteration 21/25 | Loss: 0.00132990
Iteration 22/25 | Loss: 0.00132504
Iteration 23/25 | Loss: 0.00132347
Iteration 24/25 | Loss: 0.00132149
Iteration 25/25 | Loss: 0.00132106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08798599
Iteration 2/25 | Loss: 0.00112122
Iteration 3/25 | Loss: 0.00112122
Iteration 4/25 | Loss: 0.00112122
Iteration 5/25 | Loss: 0.00112122
Iteration 6/25 | Loss: 0.00112122
Iteration 7/25 | Loss: 0.00112122
Iteration 8/25 | Loss: 0.00112122
Iteration 9/25 | Loss: 0.00112122
Iteration 10/25 | Loss: 0.00112122
Iteration 11/25 | Loss: 0.00112121
Iteration 12/25 | Loss: 0.00112121
Iteration 13/25 | Loss: 0.00112121
Iteration 14/25 | Loss: 0.00112121
Iteration 15/25 | Loss: 0.00112121
Iteration 16/25 | Loss: 0.00112121
Iteration 17/25 | Loss: 0.00112121
Iteration 18/25 | Loss: 0.00112121
Iteration 19/25 | Loss: 0.00112121
Iteration 20/25 | Loss: 0.00112121
Iteration 21/25 | Loss: 0.00112121
Iteration 22/25 | Loss: 0.00112121
Iteration 23/25 | Loss: 0.00112121
Iteration 24/25 | Loss: 0.00112121
Iteration 25/25 | Loss: 0.00112121

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112121
Iteration 2/1000 | Loss: 0.00012489
Iteration 3/1000 | Loss: 0.00007828
Iteration 4/1000 | Loss: 0.00006378
Iteration 5/1000 | Loss: 0.00005369
Iteration 6/1000 | Loss: 0.00004825
Iteration 7/1000 | Loss: 0.00004313
Iteration 8/1000 | Loss: 0.00004101
Iteration 9/1000 | Loss: 0.00003940
Iteration 10/1000 | Loss: 0.00003840
Iteration 11/1000 | Loss: 0.00003748
Iteration 12/1000 | Loss: 0.00017424
Iteration 13/1000 | Loss: 0.00054575
Iteration 14/1000 | Loss: 0.00006348
Iteration 15/1000 | Loss: 0.00003747
Iteration 16/1000 | Loss: 0.00003190
Iteration 17/1000 | Loss: 0.00002759
Iteration 18/1000 | Loss: 0.00002502
Iteration 19/1000 | Loss: 0.00002310
Iteration 20/1000 | Loss: 0.00002197
Iteration 21/1000 | Loss: 0.00002130
Iteration 22/1000 | Loss: 0.00002080
Iteration 23/1000 | Loss: 0.00002031
Iteration 24/1000 | Loss: 0.00002010
Iteration 25/1000 | Loss: 0.00001982
Iteration 26/1000 | Loss: 0.00001956
Iteration 27/1000 | Loss: 0.00001948
Iteration 28/1000 | Loss: 0.00001937
Iteration 29/1000 | Loss: 0.00001935
Iteration 30/1000 | Loss: 0.00001935
Iteration 31/1000 | Loss: 0.00001934
Iteration 32/1000 | Loss: 0.00001934
Iteration 33/1000 | Loss: 0.00001933
Iteration 34/1000 | Loss: 0.00001932
Iteration 35/1000 | Loss: 0.00001932
Iteration 36/1000 | Loss: 0.00001931
Iteration 37/1000 | Loss: 0.00001931
Iteration 38/1000 | Loss: 0.00001931
Iteration 39/1000 | Loss: 0.00001930
Iteration 40/1000 | Loss: 0.00001930
Iteration 41/1000 | Loss: 0.00001930
Iteration 42/1000 | Loss: 0.00001929
Iteration 43/1000 | Loss: 0.00001929
Iteration 44/1000 | Loss: 0.00001929
Iteration 45/1000 | Loss: 0.00001929
Iteration 46/1000 | Loss: 0.00001929
Iteration 47/1000 | Loss: 0.00001928
Iteration 48/1000 | Loss: 0.00001928
Iteration 49/1000 | Loss: 0.00001927
Iteration 50/1000 | Loss: 0.00001927
Iteration 51/1000 | Loss: 0.00001927
Iteration 52/1000 | Loss: 0.00001926
Iteration 53/1000 | Loss: 0.00001926
Iteration 54/1000 | Loss: 0.00001926
Iteration 55/1000 | Loss: 0.00001926
Iteration 56/1000 | Loss: 0.00001926
Iteration 57/1000 | Loss: 0.00001925
Iteration 58/1000 | Loss: 0.00001925
Iteration 59/1000 | Loss: 0.00001925
Iteration 60/1000 | Loss: 0.00001925
Iteration 61/1000 | Loss: 0.00001925
Iteration 62/1000 | Loss: 0.00001925
Iteration 63/1000 | Loss: 0.00001924
Iteration 64/1000 | Loss: 0.00001924
Iteration 65/1000 | Loss: 0.00001924
Iteration 66/1000 | Loss: 0.00001924
Iteration 67/1000 | Loss: 0.00001924
Iteration 68/1000 | Loss: 0.00001924
Iteration 69/1000 | Loss: 0.00001924
Iteration 70/1000 | Loss: 0.00001924
Iteration 71/1000 | Loss: 0.00001924
Iteration 72/1000 | Loss: 0.00001924
Iteration 73/1000 | Loss: 0.00001924
Iteration 74/1000 | Loss: 0.00001924
Iteration 75/1000 | Loss: 0.00001924
Iteration 76/1000 | Loss: 0.00001924
Iteration 77/1000 | Loss: 0.00001924
Iteration 78/1000 | Loss: 0.00001924
Iteration 79/1000 | Loss: 0.00001924
Iteration 80/1000 | Loss: 0.00001924
Iteration 81/1000 | Loss: 0.00001924
Iteration 82/1000 | Loss: 0.00001924
Iteration 83/1000 | Loss: 0.00001924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.92380539374426e-05, 1.92380539374426e-05, 1.92380539374426e-05, 1.92380539374426e-05, 1.92380539374426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.92380539374426e-05

Optimization complete. Final v2v error: 3.7300400733947754 mm

Highest mean error: 4.071091651916504 mm for frame 112

Lowest mean error: 3.5378191471099854 mm for frame 130

Saving results

Total time: 84.72374105453491
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00375270
Iteration 2/25 | Loss: 0.00125757
Iteration 3/25 | Loss: 0.00121569
Iteration 4/25 | Loss: 0.00120545
Iteration 5/25 | Loss: 0.00120298
Iteration 6/25 | Loss: 0.00120256
Iteration 7/25 | Loss: 0.00120256
Iteration 8/25 | Loss: 0.00120256
Iteration 9/25 | Loss: 0.00120256
Iteration 10/25 | Loss: 0.00120256
Iteration 11/25 | Loss: 0.00120256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001202555256895721, 0.001202555256895721, 0.001202555256895721, 0.001202555256895721, 0.001202555256895721]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001202555256895721

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37390637
Iteration 2/25 | Loss: 0.00117939
Iteration 3/25 | Loss: 0.00117939
Iteration 4/25 | Loss: 0.00117939
Iteration 5/25 | Loss: 0.00117939
Iteration 6/25 | Loss: 0.00117939
Iteration 7/25 | Loss: 0.00117939
Iteration 8/25 | Loss: 0.00117939
Iteration 9/25 | Loss: 0.00117939
Iteration 10/25 | Loss: 0.00117939
Iteration 11/25 | Loss: 0.00117939
Iteration 12/25 | Loss: 0.00117939
Iteration 13/25 | Loss: 0.00117939
Iteration 14/25 | Loss: 0.00117939
Iteration 15/25 | Loss: 0.00117939
Iteration 16/25 | Loss: 0.00117939
Iteration 17/25 | Loss: 0.00117939
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011793903540819883, 0.0011793903540819883, 0.0011793903540819883, 0.0011793903540819883, 0.0011793903540819883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011793903540819883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117939
Iteration 2/1000 | Loss: 0.00002810
Iteration 3/1000 | Loss: 0.00001921
Iteration 4/1000 | Loss: 0.00001675
Iteration 5/1000 | Loss: 0.00001607
Iteration 6/1000 | Loss: 0.00001571
Iteration 7/1000 | Loss: 0.00001548
Iteration 8/1000 | Loss: 0.00001513
Iteration 9/1000 | Loss: 0.00001512
Iteration 10/1000 | Loss: 0.00001496
Iteration 11/1000 | Loss: 0.00001475
Iteration 12/1000 | Loss: 0.00001456
Iteration 13/1000 | Loss: 0.00001455
Iteration 14/1000 | Loss: 0.00001455
Iteration 15/1000 | Loss: 0.00001454
Iteration 16/1000 | Loss: 0.00001454
Iteration 17/1000 | Loss: 0.00001449
Iteration 18/1000 | Loss: 0.00001447
Iteration 19/1000 | Loss: 0.00001447
Iteration 20/1000 | Loss: 0.00001446
Iteration 21/1000 | Loss: 0.00001446
Iteration 22/1000 | Loss: 0.00001445
Iteration 23/1000 | Loss: 0.00001445
Iteration 24/1000 | Loss: 0.00001445
Iteration 25/1000 | Loss: 0.00001444
Iteration 26/1000 | Loss: 0.00001444
Iteration 27/1000 | Loss: 0.00001443
Iteration 28/1000 | Loss: 0.00001442
Iteration 29/1000 | Loss: 0.00001442
Iteration 30/1000 | Loss: 0.00001441
Iteration 31/1000 | Loss: 0.00001441
Iteration 32/1000 | Loss: 0.00001441
Iteration 33/1000 | Loss: 0.00001440
Iteration 34/1000 | Loss: 0.00001440
Iteration 35/1000 | Loss: 0.00001439
Iteration 36/1000 | Loss: 0.00001435
Iteration 37/1000 | Loss: 0.00001434
Iteration 38/1000 | Loss: 0.00001434
Iteration 39/1000 | Loss: 0.00001432
Iteration 40/1000 | Loss: 0.00001431
Iteration 41/1000 | Loss: 0.00001430
Iteration 42/1000 | Loss: 0.00001430
Iteration 43/1000 | Loss: 0.00001429
Iteration 44/1000 | Loss: 0.00001429
Iteration 45/1000 | Loss: 0.00001429
Iteration 46/1000 | Loss: 0.00001429
Iteration 47/1000 | Loss: 0.00001429
Iteration 48/1000 | Loss: 0.00001429
Iteration 49/1000 | Loss: 0.00001429
Iteration 50/1000 | Loss: 0.00001429
Iteration 51/1000 | Loss: 0.00001429
Iteration 52/1000 | Loss: 0.00001428
Iteration 53/1000 | Loss: 0.00001428
Iteration 54/1000 | Loss: 0.00001428
Iteration 55/1000 | Loss: 0.00001428
Iteration 56/1000 | Loss: 0.00001428
Iteration 57/1000 | Loss: 0.00001427
Iteration 58/1000 | Loss: 0.00001427
Iteration 59/1000 | Loss: 0.00001427
Iteration 60/1000 | Loss: 0.00001427
Iteration 61/1000 | Loss: 0.00001427
Iteration 62/1000 | Loss: 0.00001427
Iteration 63/1000 | Loss: 0.00001427
Iteration 64/1000 | Loss: 0.00001427
Iteration 65/1000 | Loss: 0.00001427
Iteration 66/1000 | Loss: 0.00001426
Iteration 67/1000 | Loss: 0.00001426
Iteration 68/1000 | Loss: 0.00001426
Iteration 69/1000 | Loss: 0.00001426
Iteration 70/1000 | Loss: 0.00001426
Iteration 71/1000 | Loss: 0.00001426
Iteration 72/1000 | Loss: 0.00001425
Iteration 73/1000 | Loss: 0.00001425
Iteration 74/1000 | Loss: 0.00001425
Iteration 75/1000 | Loss: 0.00001425
Iteration 76/1000 | Loss: 0.00001425
Iteration 77/1000 | Loss: 0.00001425
Iteration 78/1000 | Loss: 0.00001424
Iteration 79/1000 | Loss: 0.00001424
Iteration 80/1000 | Loss: 0.00001424
Iteration 81/1000 | Loss: 0.00001424
Iteration 82/1000 | Loss: 0.00001423
Iteration 83/1000 | Loss: 0.00001423
Iteration 84/1000 | Loss: 0.00001423
Iteration 85/1000 | Loss: 0.00001422
Iteration 86/1000 | Loss: 0.00001422
Iteration 87/1000 | Loss: 0.00001422
Iteration 88/1000 | Loss: 0.00001422
Iteration 89/1000 | Loss: 0.00001421
Iteration 90/1000 | Loss: 0.00001421
Iteration 91/1000 | Loss: 0.00001421
Iteration 92/1000 | Loss: 0.00001421
Iteration 93/1000 | Loss: 0.00001421
Iteration 94/1000 | Loss: 0.00001421
Iteration 95/1000 | Loss: 0.00001421
Iteration 96/1000 | Loss: 0.00001421
Iteration 97/1000 | Loss: 0.00001420
Iteration 98/1000 | Loss: 0.00001420
Iteration 99/1000 | Loss: 0.00001420
Iteration 100/1000 | Loss: 0.00001420
Iteration 101/1000 | Loss: 0.00001420
Iteration 102/1000 | Loss: 0.00001420
Iteration 103/1000 | Loss: 0.00001419
Iteration 104/1000 | Loss: 0.00001419
Iteration 105/1000 | Loss: 0.00001419
Iteration 106/1000 | Loss: 0.00001419
Iteration 107/1000 | Loss: 0.00001419
Iteration 108/1000 | Loss: 0.00001419
Iteration 109/1000 | Loss: 0.00001419
Iteration 110/1000 | Loss: 0.00001419
Iteration 111/1000 | Loss: 0.00001419
Iteration 112/1000 | Loss: 0.00001419
Iteration 113/1000 | Loss: 0.00001419
Iteration 114/1000 | Loss: 0.00001419
Iteration 115/1000 | Loss: 0.00001419
Iteration 116/1000 | Loss: 0.00001419
Iteration 117/1000 | Loss: 0.00001419
Iteration 118/1000 | Loss: 0.00001418
Iteration 119/1000 | Loss: 0.00001418
Iteration 120/1000 | Loss: 0.00001418
Iteration 121/1000 | Loss: 0.00001418
Iteration 122/1000 | Loss: 0.00001418
Iteration 123/1000 | Loss: 0.00001418
Iteration 124/1000 | Loss: 0.00001418
Iteration 125/1000 | Loss: 0.00001418
Iteration 126/1000 | Loss: 0.00001418
Iteration 127/1000 | Loss: 0.00001417
Iteration 128/1000 | Loss: 0.00001417
Iteration 129/1000 | Loss: 0.00001417
Iteration 130/1000 | Loss: 0.00001417
Iteration 131/1000 | Loss: 0.00001416
Iteration 132/1000 | Loss: 0.00001416
Iteration 133/1000 | Loss: 0.00001416
Iteration 134/1000 | Loss: 0.00001416
Iteration 135/1000 | Loss: 0.00001416
Iteration 136/1000 | Loss: 0.00001416
Iteration 137/1000 | Loss: 0.00001416
Iteration 138/1000 | Loss: 0.00001416
Iteration 139/1000 | Loss: 0.00001416
Iteration 140/1000 | Loss: 0.00001416
Iteration 141/1000 | Loss: 0.00001416
Iteration 142/1000 | Loss: 0.00001416
Iteration 143/1000 | Loss: 0.00001415
Iteration 144/1000 | Loss: 0.00001415
Iteration 145/1000 | Loss: 0.00001415
Iteration 146/1000 | Loss: 0.00001415
Iteration 147/1000 | Loss: 0.00001415
Iteration 148/1000 | Loss: 0.00001415
Iteration 149/1000 | Loss: 0.00001415
Iteration 150/1000 | Loss: 0.00001415
Iteration 151/1000 | Loss: 0.00001415
Iteration 152/1000 | Loss: 0.00001415
Iteration 153/1000 | Loss: 0.00001415
Iteration 154/1000 | Loss: 0.00001415
Iteration 155/1000 | Loss: 0.00001415
Iteration 156/1000 | Loss: 0.00001415
Iteration 157/1000 | Loss: 0.00001415
Iteration 158/1000 | Loss: 0.00001415
Iteration 159/1000 | Loss: 0.00001415
Iteration 160/1000 | Loss: 0.00001415
Iteration 161/1000 | Loss: 0.00001415
Iteration 162/1000 | Loss: 0.00001415
Iteration 163/1000 | Loss: 0.00001415
Iteration 164/1000 | Loss: 0.00001415
Iteration 165/1000 | Loss: 0.00001415
Iteration 166/1000 | Loss: 0.00001415
Iteration 167/1000 | Loss: 0.00001415
Iteration 168/1000 | Loss: 0.00001415
Iteration 169/1000 | Loss: 0.00001415
Iteration 170/1000 | Loss: 0.00001415
Iteration 171/1000 | Loss: 0.00001415
Iteration 172/1000 | Loss: 0.00001415
Iteration 173/1000 | Loss: 0.00001415
Iteration 174/1000 | Loss: 0.00001415
Iteration 175/1000 | Loss: 0.00001415
Iteration 176/1000 | Loss: 0.00001415
Iteration 177/1000 | Loss: 0.00001415
Iteration 178/1000 | Loss: 0.00001415
Iteration 179/1000 | Loss: 0.00001415
Iteration 180/1000 | Loss: 0.00001415
Iteration 181/1000 | Loss: 0.00001415
Iteration 182/1000 | Loss: 0.00001415
Iteration 183/1000 | Loss: 0.00001415
Iteration 184/1000 | Loss: 0.00001415
Iteration 185/1000 | Loss: 0.00001415
Iteration 186/1000 | Loss: 0.00001415
Iteration 187/1000 | Loss: 0.00001415
Iteration 188/1000 | Loss: 0.00001415
Iteration 189/1000 | Loss: 0.00001415
Iteration 190/1000 | Loss: 0.00001415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.4147404726827517e-05, 1.4147404726827517e-05, 1.4147404726827517e-05, 1.4147404726827517e-05, 1.4147404726827517e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4147404726827517e-05

Optimization complete. Final v2v error: 3.1509974002838135 mm

Highest mean error: 3.3375678062438965 mm for frame 89

Lowest mean error: 2.9691028594970703 mm for frame 24

Saving results

Total time: 34.18844795227051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507297
Iteration 2/25 | Loss: 0.00149890
Iteration 3/25 | Loss: 0.00131070
Iteration 4/25 | Loss: 0.00129462
Iteration 5/25 | Loss: 0.00128893
Iteration 6/25 | Loss: 0.00128698
Iteration 7/25 | Loss: 0.00128659
Iteration 8/25 | Loss: 0.00128659
Iteration 9/25 | Loss: 0.00128659
Iteration 10/25 | Loss: 0.00128659
Iteration 11/25 | Loss: 0.00128659
Iteration 12/25 | Loss: 0.00128659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012865880271419883, 0.0012865880271419883, 0.0012865880271419883, 0.0012865880271419883, 0.0012865880271419883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012865880271419883

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06949961
Iteration 2/25 | Loss: 0.00118520
Iteration 3/25 | Loss: 0.00118519
Iteration 4/25 | Loss: 0.00118519
Iteration 5/25 | Loss: 0.00118519
Iteration 6/25 | Loss: 0.00118519
Iteration 7/25 | Loss: 0.00118519
Iteration 8/25 | Loss: 0.00118519
Iteration 9/25 | Loss: 0.00118519
Iteration 10/25 | Loss: 0.00118519
Iteration 11/25 | Loss: 0.00118519
Iteration 12/25 | Loss: 0.00118519
Iteration 13/25 | Loss: 0.00118519
Iteration 14/25 | Loss: 0.00118519
Iteration 15/25 | Loss: 0.00118519
Iteration 16/25 | Loss: 0.00118519
Iteration 17/25 | Loss: 0.00118519
Iteration 18/25 | Loss: 0.00118519
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011851892340928316, 0.0011851892340928316, 0.0011851892340928316, 0.0011851892340928316, 0.0011851892340928316]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011851892340928316

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118519
Iteration 2/1000 | Loss: 0.00006975
Iteration 3/1000 | Loss: 0.00003948
Iteration 4/1000 | Loss: 0.00003161
Iteration 5/1000 | Loss: 0.00002950
Iteration 6/1000 | Loss: 0.00002798
Iteration 7/1000 | Loss: 0.00002689
Iteration 8/1000 | Loss: 0.00002608
Iteration 9/1000 | Loss: 0.00002560
Iteration 10/1000 | Loss: 0.00002518
Iteration 11/1000 | Loss: 0.00002477
Iteration 12/1000 | Loss: 0.00002448
Iteration 13/1000 | Loss: 0.00002423
Iteration 14/1000 | Loss: 0.00002401
Iteration 15/1000 | Loss: 0.00002388
Iteration 16/1000 | Loss: 0.00002374
Iteration 17/1000 | Loss: 0.00002369
Iteration 18/1000 | Loss: 0.00002362
Iteration 19/1000 | Loss: 0.00002360
Iteration 20/1000 | Loss: 0.00002359
Iteration 21/1000 | Loss: 0.00002358
Iteration 22/1000 | Loss: 0.00002355
Iteration 23/1000 | Loss: 0.00002354
Iteration 24/1000 | Loss: 0.00002351
Iteration 25/1000 | Loss: 0.00002350
Iteration 26/1000 | Loss: 0.00002350
Iteration 27/1000 | Loss: 0.00002350
Iteration 28/1000 | Loss: 0.00002345
Iteration 29/1000 | Loss: 0.00002345
Iteration 30/1000 | Loss: 0.00002345
Iteration 31/1000 | Loss: 0.00002344
Iteration 32/1000 | Loss: 0.00002343
Iteration 33/1000 | Loss: 0.00002341
Iteration 34/1000 | Loss: 0.00002340
Iteration 35/1000 | Loss: 0.00002340
Iteration 36/1000 | Loss: 0.00002340
Iteration 37/1000 | Loss: 0.00002340
Iteration 38/1000 | Loss: 0.00002340
Iteration 39/1000 | Loss: 0.00002338
Iteration 40/1000 | Loss: 0.00002338
Iteration 41/1000 | Loss: 0.00002338
Iteration 42/1000 | Loss: 0.00002337
Iteration 43/1000 | Loss: 0.00002337
Iteration 44/1000 | Loss: 0.00002336
Iteration 45/1000 | Loss: 0.00002336
Iteration 46/1000 | Loss: 0.00002336
Iteration 47/1000 | Loss: 0.00002335
Iteration 48/1000 | Loss: 0.00002335
Iteration 49/1000 | Loss: 0.00002335
Iteration 50/1000 | Loss: 0.00002334
Iteration 51/1000 | Loss: 0.00002334
Iteration 52/1000 | Loss: 0.00002334
Iteration 53/1000 | Loss: 0.00002333
Iteration 54/1000 | Loss: 0.00002333
Iteration 55/1000 | Loss: 0.00002333
Iteration 56/1000 | Loss: 0.00002333
Iteration 57/1000 | Loss: 0.00002333
Iteration 58/1000 | Loss: 0.00002333
Iteration 59/1000 | Loss: 0.00002333
Iteration 60/1000 | Loss: 0.00002333
Iteration 61/1000 | Loss: 0.00002333
Iteration 62/1000 | Loss: 0.00002333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [2.3326214432017878e-05, 2.3326214432017878e-05, 2.3326214432017878e-05, 2.3326214432017878e-05, 2.3326214432017878e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3326214432017878e-05

Optimization complete. Final v2v error: 3.9524388313293457 mm

Highest mean error: 5.148151397705078 mm for frame 103

Lowest mean error: 2.798445701599121 mm for frame 72

Saving results

Total time: 44.41498517990112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048709
Iteration 2/25 | Loss: 0.01048709
Iteration 3/25 | Loss: 0.00428369
Iteration 4/25 | Loss: 0.00337494
Iteration 5/25 | Loss: 0.00373504
Iteration 6/25 | Loss: 0.00331788
Iteration 7/25 | Loss: 0.00298296
Iteration 8/25 | Loss: 0.00276546
Iteration 9/25 | Loss: 0.00246350
Iteration 10/25 | Loss: 0.00242594
Iteration 11/25 | Loss: 0.00229706
Iteration 12/25 | Loss: 0.00220032
Iteration 13/25 | Loss: 0.00210385
Iteration 14/25 | Loss: 0.00206085
Iteration 15/25 | Loss: 0.00186268
Iteration 16/25 | Loss: 0.00185591
Iteration 17/25 | Loss: 0.00177970
Iteration 18/25 | Loss: 0.00178131
Iteration 19/25 | Loss: 0.00172508
Iteration 20/25 | Loss: 0.00172360
Iteration 21/25 | Loss: 0.00171965
Iteration 22/25 | Loss: 0.00168133
Iteration 23/25 | Loss: 0.00173225
Iteration 24/25 | Loss: 0.00167272
Iteration 25/25 | Loss: 0.00165549

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07200611
Iteration 2/25 | Loss: 0.00311130
Iteration 3/25 | Loss: 0.00290772
Iteration 4/25 | Loss: 0.00290772
Iteration 5/25 | Loss: 0.00290772
Iteration 6/25 | Loss: 0.00290772
Iteration 7/25 | Loss: 0.00290772
Iteration 8/25 | Loss: 0.00290772
Iteration 9/25 | Loss: 0.00290772
Iteration 10/25 | Loss: 0.00290771
Iteration 11/25 | Loss: 0.00290771
Iteration 12/25 | Loss: 0.00290771
Iteration 13/25 | Loss: 0.00290771
Iteration 14/25 | Loss: 0.00290771
Iteration 15/25 | Loss: 0.00290771
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002907714806497097, 0.002907714806497097, 0.002907714806497097, 0.002907714806497097, 0.002907714806497097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002907714806497097

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00290771
Iteration 2/1000 | Loss: 0.00247239
Iteration 3/1000 | Loss: 0.00029606
Iteration 4/1000 | Loss: 0.00035880
Iteration 5/1000 | Loss: 0.00034526
Iteration 6/1000 | Loss: 0.00023926
Iteration 7/1000 | Loss: 0.00128013
Iteration 8/1000 | Loss: 0.00070526
Iteration 9/1000 | Loss: 0.00073057
Iteration 10/1000 | Loss: 0.00035549
Iteration 11/1000 | Loss: 0.00021880
Iteration 12/1000 | Loss: 0.00021171
Iteration 13/1000 | Loss: 0.00019730
Iteration 14/1000 | Loss: 0.00018753
Iteration 15/1000 | Loss: 0.00019132
Iteration 16/1000 | Loss: 0.00091508
Iteration 17/1000 | Loss: 0.00032172
Iteration 18/1000 | Loss: 0.00028232
Iteration 19/1000 | Loss: 0.00042125
Iteration 20/1000 | Loss: 0.00076061
Iteration 21/1000 | Loss: 0.00128525
Iteration 22/1000 | Loss: 0.00024743
Iteration 23/1000 | Loss: 0.00023411
Iteration 24/1000 | Loss: 0.00020839
Iteration 25/1000 | Loss: 0.00018927
Iteration 26/1000 | Loss: 0.00515808
Iteration 27/1000 | Loss: 0.00512065
Iteration 28/1000 | Loss: 0.00082705
Iteration 29/1000 | Loss: 0.00053989
Iteration 30/1000 | Loss: 0.00065081
Iteration 31/1000 | Loss: 0.00050391
Iteration 32/1000 | Loss: 0.00041571
Iteration 33/1000 | Loss: 0.00022649
Iteration 34/1000 | Loss: 0.00013317
Iteration 35/1000 | Loss: 0.00011125
Iteration 36/1000 | Loss: 0.00009416
Iteration 37/1000 | Loss: 0.00010253
Iteration 38/1000 | Loss: 0.00032464
Iteration 39/1000 | Loss: 0.00011125
Iteration 40/1000 | Loss: 0.00009361
Iteration 41/1000 | Loss: 0.00032948
Iteration 42/1000 | Loss: 0.00006574
Iteration 43/1000 | Loss: 0.00006539
Iteration 44/1000 | Loss: 0.00005967
Iteration 45/1000 | Loss: 0.00007074
Iteration 46/1000 | Loss: 0.00006450
Iteration 47/1000 | Loss: 0.00006922
Iteration 48/1000 | Loss: 0.00007027
Iteration 49/1000 | Loss: 0.00007022
Iteration 50/1000 | Loss: 0.00006241
Iteration 51/1000 | Loss: 0.00005928
Iteration 52/1000 | Loss: 0.00005991
Iteration 53/1000 | Loss: 0.00005097
Iteration 54/1000 | Loss: 0.00006425
Iteration 55/1000 | Loss: 0.00005855
Iteration 56/1000 | Loss: 0.00021794
Iteration 57/1000 | Loss: 0.00008915
Iteration 58/1000 | Loss: 0.00014186
Iteration 59/1000 | Loss: 0.00020928
Iteration 60/1000 | Loss: 0.00005538
Iteration 61/1000 | Loss: 0.00006350
Iteration 62/1000 | Loss: 0.00006004
Iteration 63/1000 | Loss: 0.00006148
Iteration 64/1000 | Loss: 0.00005526
Iteration 65/1000 | Loss: 0.00005760
Iteration 66/1000 | Loss: 0.00005839
Iteration 67/1000 | Loss: 0.00005848
Iteration 68/1000 | Loss: 0.00005988
Iteration 69/1000 | Loss: 0.00006136
Iteration 70/1000 | Loss: 0.00006036
Iteration 71/1000 | Loss: 0.00005932
Iteration 72/1000 | Loss: 0.00005810
Iteration 73/1000 | Loss: 0.00005804
Iteration 74/1000 | Loss: 0.00005702
Iteration 75/1000 | Loss: 0.00005462
Iteration 76/1000 | Loss: 0.00005964
Iteration 77/1000 | Loss: 0.00005865
Iteration 78/1000 | Loss: 0.00005853
Iteration 79/1000 | Loss: 0.00004047
Iteration 80/1000 | Loss: 0.00004989
Iteration 81/1000 | Loss: 0.00005482
Iteration 82/1000 | Loss: 0.00005815
Iteration 83/1000 | Loss: 0.00004950
Iteration 84/1000 | Loss: 0.00005369
Iteration 85/1000 | Loss: 0.00005573
Iteration 86/1000 | Loss: 0.00003968
Iteration 87/1000 | Loss: 0.00003932
Iteration 88/1000 | Loss: 0.00005054
Iteration 89/1000 | Loss: 0.00005237
Iteration 90/1000 | Loss: 0.00005608
Iteration 91/1000 | Loss: 0.00004966
Iteration 92/1000 | Loss: 0.00004117
Iteration 93/1000 | Loss: 0.00005407
Iteration 94/1000 | Loss: 0.00005047
Iteration 95/1000 | Loss: 0.00005127
Iteration 96/1000 | Loss: 0.00004490
Iteration 97/1000 | Loss: 0.00004278
Iteration 98/1000 | Loss: 0.00004554
Iteration 99/1000 | Loss: 0.00004105
Iteration 100/1000 | Loss: 0.00004476
Iteration 101/1000 | Loss: 0.00005428
Iteration 102/1000 | Loss: 0.00005007
Iteration 103/1000 | Loss: 0.00005778
Iteration 104/1000 | Loss: 0.00005620
Iteration 105/1000 | Loss: 0.00005561
Iteration 106/1000 | Loss: 0.00005580
Iteration 107/1000 | Loss: 0.00005525
Iteration 108/1000 | Loss: 0.00004557
Iteration 109/1000 | Loss: 0.00003883
Iteration 110/1000 | Loss: 0.00005365
Iteration 111/1000 | Loss: 0.00005365
Iteration 112/1000 | Loss: 0.00005729
Iteration 113/1000 | Loss: 0.00005518
Iteration 114/1000 | Loss: 0.00005585
Iteration 115/1000 | Loss: 0.00004318
Iteration 116/1000 | Loss: 0.00005893
Iteration 117/1000 | Loss: 0.00005273
Iteration 118/1000 | Loss: 0.00005328
Iteration 119/1000 | Loss: 0.00005367
Iteration 120/1000 | Loss: 0.00005535
Iteration 121/1000 | Loss: 0.00005524
Iteration 122/1000 | Loss: 0.00005632
Iteration 123/1000 | Loss: 0.00004952
Iteration 124/1000 | Loss: 0.00005340
Iteration 125/1000 | Loss: 0.00005387
Iteration 126/1000 | Loss: 0.00006066
Iteration 127/1000 | Loss: 0.00004386
Iteration 128/1000 | Loss: 0.00004007
Iteration 129/1000 | Loss: 0.00003854
Iteration 130/1000 | Loss: 0.00003781
Iteration 131/1000 | Loss: 0.00003752
Iteration 132/1000 | Loss: 0.00003734
Iteration 133/1000 | Loss: 0.00003734
Iteration 134/1000 | Loss: 0.00003730
Iteration 135/1000 | Loss: 0.00003721
Iteration 136/1000 | Loss: 0.00003717
Iteration 137/1000 | Loss: 0.00003717
Iteration 138/1000 | Loss: 0.00003716
Iteration 139/1000 | Loss: 0.00003716
Iteration 140/1000 | Loss: 0.00003716
Iteration 141/1000 | Loss: 0.00003716
Iteration 142/1000 | Loss: 0.00003715
Iteration 143/1000 | Loss: 0.00003715
Iteration 144/1000 | Loss: 0.00003715
Iteration 145/1000 | Loss: 0.00003714
Iteration 146/1000 | Loss: 0.00003714
Iteration 147/1000 | Loss: 0.00003714
Iteration 148/1000 | Loss: 0.00003712
Iteration 149/1000 | Loss: 0.00003712
Iteration 150/1000 | Loss: 0.00003711
Iteration 151/1000 | Loss: 0.00003711
Iteration 152/1000 | Loss: 0.00003711
Iteration 153/1000 | Loss: 0.00003710
Iteration 154/1000 | Loss: 0.00003710
Iteration 155/1000 | Loss: 0.00003710
Iteration 156/1000 | Loss: 0.00003709
Iteration 157/1000 | Loss: 0.00003709
Iteration 158/1000 | Loss: 0.00003708
Iteration 159/1000 | Loss: 0.00003708
Iteration 160/1000 | Loss: 0.00003708
Iteration 161/1000 | Loss: 0.00003707
Iteration 162/1000 | Loss: 0.00003707
Iteration 163/1000 | Loss: 0.00003707
Iteration 164/1000 | Loss: 0.00003706
Iteration 165/1000 | Loss: 0.00003706
Iteration 166/1000 | Loss: 0.00003706
Iteration 167/1000 | Loss: 0.00003706
Iteration 168/1000 | Loss: 0.00003705
Iteration 169/1000 | Loss: 0.00003705
Iteration 170/1000 | Loss: 0.00003704
Iteration 171/1000 | Loss: 0.00003704
Iteration 172/1000 | Loss: 0.00003704
Iteration 173/1000 | Loss: 0.00003704
Iteration 174/1000 | Loss: 0.00003704
Iteration 175/1000 | Loss: 0.00003704
Iteration 176/1000 | Loss: 0.00003704
Iteration 177/1000 | Loss: 0.00003704
Iteration 178/1000 | Loss: 0.00003704
Iteration 179/1000 | Loss: 0.00003703
Iteration 180/1000 | Loss: 0.00003701
Iteration 181/1000 | Loss: 0.00003701
Iteration 182/1000 | Loss: 0.00003701
Iteration 183/1000 | Loss: 0.00003701
Iteration 184/1000 | Loss: 0.00003701
Iteration 185/1000 | Loss: 0.00003701
Iteration 186/1000 | Loss: 0.00003700
Iteration 187/1000 | Loss: 0.00003700
Iteration 188/1000 | Loss: 0.00003700
Iteration 189/1000 | Loss: 0.00003699
Iteration 190/1000 | Loss: 0.00003697
Iteration 191/1000 | Loss: 0.00003697
Iteration 192/1000 | Loss: 0.00003697
Iteration 193/1000 | Loss: 0.00003697
Iteration 194/1000 | Loss: 0.00003696
Iteration 195/1000 | Loss: 0.00003695
Iteration 196/1000 | Loss: 0.00003695
Iteration 197/1000 | Loss: 0.00003695
Iteration 198/1000 | Loss: 0.00003694
Iteration 199/1000 | Loss: 0.00003694
Iteration 200/1000 | Loss: 0.00003694
Iteration 201/1000 | Loss: 0.00003694
Iteration 202/1000 | Loss: 0.00003694
Iteration 203/1000 | Loss: 0.00003694
Iteration 204/1000 | Loss: 0.00003694
Iteration 205/1000 | Loss: 0.00003694
Iteration 206/1000 | Loss: 0.00003694
Iteration 207/1000 | Loss: 0.00003694
Iteration 208/1000 | Loss: 0.00003694
Iteration 209/1000 | Loss: 0.00003694
Iteration 210/1000 | Loss: 0.00003693
Iteration 211/1000 | Loss: 0.00003693
Iteration 212/1000 | Loss: 0.00003692
Iteration 213/1000 | Loss: 0.00003692
Iteration 214/1000 | Loss: 0.00003692
Iteration 215/1000 | Loss: 0.00003692
Iteration 216/1000 | Loss: 0.00003691
Iteration 217/1000 | Loss: 0.00003691
Iteration 218/1000 | Loss: 0.00003691
Iteration 219/1000 | Loss: 0.00003691
Iteration 220/1000 | Loss: 0.00003691
Iteration 221/1000 | Loss: 0.00003691
Iteration 222/1000 | Loss: 0.00003691
Iteration 223/1000 | Loss: 0.00003691
Iteration 224/1000 | Loss: 0.00003690
Iteration 225/1000 | Loss: 0.00003690
Iteration 226/1000 | Loss: 0.00003690
Iteration 227/1000 | Loss: 0.00003689
Iteration 228/1000 | Loss: 0.00003689
Iteration 229/1000 | Loss: 0.00003689
Iteration 230/1000 | Loss: 0.00003689
Iteration 231/1000 | Loss: 0.00003689
Iteration 232/1000 | Loss: 0.00003689
Iteration 233/1000 | Loss: 0.00003689
Iteration 234/1000 | Loss: 0.00003689
Iteration 235/1000 | Loss: 0.00003689
Iteration 236/1000 | Loss: 0.00003689
Iteration 237/1000 | Loss: 0.00003689
Iteration 238/1000 | Loss: 0.00003689
Iteration 239/1000 | Loss: 0.00003689
Iteration 240/1000 | Loss: 0.00003689
Iteration 241/1000 | Loss: 0.00003689
Iteration 242/1000 | Loss: 0.00003689
Iteration 243/1000 | Loss: 0.00003689
Iteration 244/1000 | Loss: 0.00003689
Iteration 245/1000 | Loss: 0.00003689
Iteration 246/1000 | Loss: 0.00003689
Iteration 247/1000 | Loss: 0.00003689
Iteration 248/1000 | Loss: 0.00003689
Iteration 249/1000 | Loss: 0.00003689
Iteration 250/1000 | Loss: 0.00003689
Iteration 251/1000 | Loss: 0.00003689
Iteration 252/1000 | Loss: 0.00003689
Iteration 253/1000 | Loss: 0.00003689
Iteration 254/1000 | Loss: 0.00003689
Iteration 255/1000 | Loss: 0.00003689
Iteration 256/1000 | Loss: 0.00003689
Iteration 257/1000 | Loss: 0.00003689
Iteration 258/1000 | Loss: 0.00003689
Iteration 259/1000 | Loss: 0.00003689
Iteration 260/1000 | Loss: 0.00003689
Iteration 261/1000 | Loss: 0.00003689
Iteration 262/1000 | Loss: 0.00003689
Iteration 263/1000 | Loss: 0.00003689
Iteration 264/1000 | Loss: 0.00003689
Iteration 265/1000 | Loss: 0.00003689
Iteration 266/1000 | Loss: 0.00003689
Iteration 267/1000 | Loss: 0.00003689
Iteration 268/1000 | Loss: 0.00003689
Iteration 269/1000 | Loss: 0.00003689
Iteration 270/1000 | Loss: 0.00003689
Iteration 271/1000 | Loss: 0.00003689
Iteration 272/1000 | Loss: 0.00003689
Iteration 273/1000 | Loss: 0.00003689
Iteration 274/1000 | Loss: 0.00003689
Iteration 275/1000 | Loss: 0.00003689
Iteration 276/1000 | Loss: 0.00003689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 276. Stopping optimization.
Last 5 losses: [3.689029108500108e-05, 3.689029108500108e-05, 3.689029108500108e-05, 3.689029108500108e-05, 3.689029108500108e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.689029108500108e-05

Optimization complete. Final v2v error: 4.510827541351318 mm

Highest mean error: 10.645232200622559 mm for frame 47

Lowest mean error: 4.049427509307861 mm for frame 163

Saving results

Total time: 241.3808193206787
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807883
Iteration 2/25 | Loss: 0.00130685
Iteration 3/25 | Loss: 0.00121372
Iteration 4/25 | Loss: 0.00120519
Iteration 5/25 | Loss: 0.00120302
Iteration 6/25 | Loss: 0.00120302
Iteration 7/25 | Loss: 0.00120302
Iteration 8/25 | Loss: 0.00120302
Iteration 9/25 | Loss: 0.00120302
Iteration 10/25 | Loss: 0.00120302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012030209181830287, 0.0012030209181830287, 0.0012030209181830287, 0.0012030209181830287, 0.0012030209181830287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012030209181830287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35101497
Iteration 2/25 | Loss: 0.00107167
Iteration 3/25 | Loss: 0.00107167
Iteration 4/25 | Loss: 0.00107166
Iteration 5/25 | Loss: 0.00107166
Iteration 6/25 | Loss: 0.00107166
Iteration 7/25 | Loss: 0.00107166
Iteration 8/25 | Loss: 0.00107166
Iteration 9/25 | Loss: 0.00107166
Iteration 10/25 | Loss: 0.00107166
Iteration 11/25 | Loss: 0.00107166
Iteration 12/25 | Loss: 0.00107166
Iteration 13/25 | Loss: 0.00107166
Iteration 14/25 | Loss: 0.00107166
Iteration 15/25 | Loss: 0.00107166
Iteration 16/25 | Loss: 0.00107166
Iteration 17/25 | Loss: 0.00107166
Iteration 18/25 | Loss: 0.00107166
Iteration 19/25 | Loss: 0.00107166
Iteration 20/25 | Loss: 0.00107166
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010716620599851012, 0.0010716620599851012, 0.0010716620599851012, 0.0010716620599851012, 0.0010716620599851012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010716620599851012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107166
Iteration 2/1000 | Loss: 0.00003139
Iteration 3/1000 | Loss: 0.00002051
Iteration 4/1000 | Loss: 0.00001665
Iteration 5/1000 | Loss: 0.00001463
Iteration 6/1000 | Loss: 0.00001360
Iteration 7/1000 | Loss: 0.00001299
Iteration 8/1000 | Loss: 0.00001248
Iteration 9/1000 | Loss: 0.00001217
Iteration 10/1000 | Loss: 0.00001195
Iteration 11/1000 | Loss: 0.00001162
Iteration 12/1000 | Loss: 0.00001162
Iteration 13/1000 | Loss: 0.00001157
Iteration 14/1000 | Loss: 0.00001151
Iteration 15/1000 | Loss: 0.00001140
Iteration 16/1000 | Loss: 0.00001131
Iteration 17/1000 | Loss: 0.00001131
Iteration 18/1000 | Loss: 0.00001131
Iteration 19/1000 | Loss: 0.00001130
Iteration 20/1000 | Loss: 0.00001126
Iteration 21/1000 | Loss: 0.00001126
Iteration 22/1000 | Loss: 0.00001126
Iteration 23/1000 | Loss: 0.00001125
Iteration 24/1000 | Loss: 0.00001125
Iteration 25/1000 | Loss: 0.00001125
Iteration 26/1000 | Loss: 0.00001124
Iteration 27/1000 | Loss: 0.00001124
Iteration 28/1000 | Loss: 0.00001124
Iteration 29/1000 | Loss: 0.00001124
Iteration 30/1000 | Loss: 0.00001123
Iteration 31/1000 | Loss: 0.00001123
Iteration 32/1000 | Loss: 0.00001123
Iteration 33/1000 | Loss: 0.00001122
Iteration 34/1000 | Loss: 0.00001121
Iteration 35/1000 | Loss: 0.00001120
Iteration 36/1000 | Loss: 0.00001120
Iteration 37/1000 | Loss: 0.00001119
Iteration 38/1000 | Loss: 0.00001119
Iteration 39/1000 | Loss: 0.00001119
Iteration 40/1000 | Loss: 0.00001118
Iteration 41/1000 | Loss: 0.00001118
Iteration 42/1000 | Loss: 0.00001114
Iteration 43/1000 | Loss: 0.00001114
Iteration 44/1000 | Loss: 0.00001113
Iteration 45/1000 | Loss: 0.00001112
Iteration 46/1000 | Loss: 0.00001111
Iteration 47/1000 | Loss: 0.00001110
Iteration 48/1000 | Loss: 0.00001110
Iteration 49/1000 | Loss: 0.00001110
Iteration 50/1000 | Loss: 0.00001109
Iteration 51/1000 | Loss: 0.00001109
Iteration 52/1000 | Loss: 0.00001108
Iteration 53/1000 | Loss: 0.00001108
Iteration 54/1000 | Loss: 0.00001108
Iteration 55/1000 | Loss: 0.00001107
Iteration 56/1000 | Loss: 0.00001107
Iteration 57/1000 | Loss: 0.00001107
Iteration 58/1000 | Loss: 0.00001107
Iteration 59/1000 | Loss: 0.00001106
Iteration 60/1000 | Loss: 0.00001106
Iteration 61/1000 | Loss: 0.00001106
Iteration 62/1000 | Loss: 0.00001105
Iteration 63/1000 | Loss: 0.00001105
Iteration 64/1000 | Loss: 0.00001105
Iteration 65/1000 | Loss: 0.00001105
Iteration 66/1000 | Loss: 0.00001105
Iteration 67/1000 | Loss: 0.00001104
Iteration 68/1000 | Loss: 0.00001104
Iteration 69/1000 | Loss: 0.00001104
Iteration 70/1000 | Loss: 0.00001104
Iteration 71/1000 | Loss: 0.00001104
Iteration 72/1000 | Loss: 0.00001103
Iteration 73/1000 | Loss: 0.00001103
Iteration 74/1000 | Loss: 0.00001102
Iteration 75/1000 | Loss: 0.00001101
Iteration 76/1000 | Loss: 0.00001101
Iteration 77/1000 | Loss: 0.00001100
Iteration 78/1000 | Loss: 0.00001100
Iteration 79/1000 | Loss: 0.00001099
Iteration 80/1000 | Loss: 0.00001099
Iteration 81/1000 | Loss: 0.00001098
Iteration 82/1000 | Loss: 0.00001098
Iteration 83/1000 | Loss: 0.00001098
Iteration 84/1000 | Loss: 0.00001098
Iteration 85/1000 | Loss: 0.00001097
Iteration 86/1000 | Loss: 0.00001097
Iteration 87/1000 | Loss: 0.00001097
Iteration 88/1000 | Loss: 0.00001096
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001095
Iteration 91/1000 | Loss: 0.00001095
Iteration 92/1000 | Loss: 0.00001095
Iteration 93/1000 | Loss: 0.00001095
Iteration 94/1000 | Loss: 0.00001095
Iteration 95/1000 | Loss: 0.00001095
Iteration 96/1000 | Loss: 0.00001094
Iteration 97/1000 | Loss: 0.00001094
Iteration 98/1000 | Loss: 0.00001093
Iteration 99/1000 | Loss: 0.00001093
Iteration 100/1000 | Loss: 0.00001093
Iteration 101/1000 | Loss: 0.00001093
Iteration 102/1000 | Loss: 0.00001093
Iteration 103/1000 | Loss: 0.00001092
Iteration 104/1000 | Loss: 0.00001092
Iteration 105/1000 | Loss: 0.00001092
Iteration 106/1000 | Loss: 0.00001091
Iteration 107/1000 | Loss: 0.00001091
Iteration 108/1000 | Loss: 0.00001091
Iteration 109/1000 | Loss: 0.00001091
Iteration 110/1000 | Loss: 0.00001090
Iteration 111/1000 | Loss: 0.00001090
Iteration 112/1000 | Loss: 0.00001090
Iteration 113/1000 | Loss: 0.00001089
Iteration 114/1000 | Loss: 0.00001088
Iteration 115/1000 | Loss: 0.00001087
Iteration 116/1000 | Loss: 0.00001087
Iteration 117/1000 | Loss: 0.00001087
Iteration 118/1000 | Loss: 0.00001087
Iteration 119/1000 | Loss: 0.00001087
Iteration 120/1000 | Loss: 0.00001087
Iteration 121/1000 | Loss: 0.00001087
Iteration 122/1000 | Loss: 0.00001086
Iteration 123/1000 | Loss: 0.00001086
Iteration 124/1000 | Loss: 0.00001086
Iteration 125/1000 | Loss: 0.00001086
Iteration 126/1000 | Loss: 0.00001086
Iteration 127/1000 | Loss: 0.00001086
Iteration 128/1000 | Loss: 0.00001085
Iteration 129/1000 | Loss: 0.00001085
Iteration 130/1000 | Loss: 0.00001084
Iteration 131/1000 | Loss: 0.00001084
Iteration 132/1000 | Loss: 0.00001084
Iteration 133/1000 | Loss: 0.00001084
Iteration 134/1000 | Loss: 0.00001084
Iteration 135/1000 | Loss: 0.00001084
Iteration 136/1000 | Loss: 0.00001084
Iteration 137/1000 | Loss: 0.00001083
Iteration 138/1000 | Loss: 0.00001083
Iteration 139/1000 | Loss: 0.00001083
Iteration 140/1000 | Loss: 0.00001083
Iteration 141/1000 | Loss: 0.00001083
Iteration 142/1000 | Loss: 0.00001083
Iteration 143/1000 | Loss: 0.00001083
Iteration 144/1000 | Loss: 0.00001083
Iteration 145/1000 | Loss: 0.00001082
Iteration 146/1000 | Loss: 0.00001082
Iteration 147/1000 | Loss: 0.00001082
Iteration 148/1000 | Loss: 0.00001082
Iteration 149/1000 | Loss: 0.00001082
Iteration 150/1000 | Loss: 0.00001082
Iteration 151/1000 | Loss: 0.00001082
Iteration 152/1000 | Loss: 0.00001081
Iteration 153/1000 | Loss: 0.00001081
Iteration 154/1000 | Loss: 0.00001081
Iteration 155/1000 | Loss: 0.00001081
Iteration 156/1000 | Loss: 0.00001081
Iteration 157/1000 | Loss: 0.00001081
Iteration 158/1000 | Loss: 0.00001081
Iteration 159/1000 | Loss: 0.00001081
Iteration 160/1000 | Loss: 0.00001081
Iteration 161/1000 | Loss: 0.00001081
Iteration 162/1000 | Loss: 0.00001081
Iteration 163/1000 | Loss: 0.00001081
Iteration 164/1000 | Loss: 0.00001081
Iteration 165/1000 | Loss: 0.00001081
Iteration 166/1000 | Loss: 0.00001080
Iteration 167/1000 | Loss: 0.00001080
Iteration 168/1000 | Loss: 0.00001080
Iteration 169/1000 | Loss: 0.00001080
Iteration 170/1000 | Loss: 0.00001080
Iteration 171/1000 | Loss: 0.00001080
Iteration 172/1000 | Loss: 0.00001080
Iteration 173/1000 | Loss: 0.00001080
Iteration 174/1000 | Loss: 0.00001080
Iteration 175/1000 | Loss: 0.00001079
Iteration 176/1000 | Loss: 0.00001079
Iteration 177/1000 | Loss: 0.00001079
Iteration 178/1000 | Loss: 0.00001079
Iteration 179/1000 | Loss: 0.00001079
Iteration 180/1000 | Loss: 0.00001079
Iteration 181/1000 | Loss: 0.00001079
Iteration 182/1000 | Loss: 0.00001078
Iteration 183/1000 | Loss: 0.00001078
Iteration 184/1000 | Loss: 0.00001078
Iteration 185/1000 | Loss: 0.00001078
Iteration 186/1000 | Loss: 0.00001078
Iteration 187/1000 | Loss: 0.00001078
Iteration 188/1000 | Loss: 0.00001078
Iteration 189/1000 | Loss: 0.00001078
Iteration 190/1000 | Loss: 0.00001078
Iteration 191/1000 | Loss: 0.00001078
Iteration 192/1000 | Loss: 0.00001077
Iteration 193/1000 | Loss: 0.00001077
Iteration 194/1000 | Loss: 0.00001077
Iteration 195/1000 | Loss: 0.00001077
Iteration 196/1000 | Loss: 0.00001077
Iteration 197/1000 | Loss: 0.00001077
Iteration 198/1000 | Loss: 0.00001077
Iteration 199/1000 | Loss: 0.00001077
Iteration 200/1000 | Loss: 0.00001077
Iteration 201/1000 | Loss: 0.00001077
Iteration 202/1000 | Loss: 0.00001077
Iteration 203/1000 | Loss: 0.00001077
Iteration 204/1000 | Loss: 0.00001076
Iteration 205/1000 | Loss: 0.00001076
Iteration 206/1000 | Loss: 0.00001076
Iteration 207/1000 | Loss: 0.00001076
Iteration 208/1000 | Loss: 0.00001076
Iteration 209/1000 | Loss: 0.00001076
Iteration 210/1000 | Loss: 0.00001076
Iteration 211/1000 | Loss: 0.00001076
Iteration 212/1000 | Loss: 0.00001076
Iteration 213/1000 | Loss: 0.00001076
Iteration 214/1000 | Loss: 0.00001076
Iteration 215/1000 | Loss: 0.00001076
Iteration 216/1000 | Loss: 0.00001075
Iteration 217/1000 | Loss: 0.00001075
Iteration 218/1000 | Loss: 0.00001075
Iteration 219/1000 | Loss: 0.00001075
Iteration 220/1000 | Loss: 0.00001075
Iteration 221/1000 | Loss: 0.00001075
Iteration 222/1000 | Loss: 0.00001075
Iteration 223/1000 | Loss: 0.00001075
Iteration 224/1000 | Loss: 0.00001075
Iteration 225/1000 | Loss: 0.00001075
Iteration 226/1000 | Loss: 0.00001075
Iteration 227/1000 | Loss: 0.00001074
Iteration 228/1000 | Loss: 0.00001074
Iteration 229/1000 | Loss: 0.00001074
Iteration 230/1000 | Loss: 0.00001074
Iteration 231/1000 | Loss: 0.00001074
Iteration 232/1000 | Loss: 0.00001074
Iteration 233/1000 | Loss: 0.00001074
Iteration 234/1000 | Loss: 0.00001074
Iteration 235/1000 | Loss: 0.00001074
Iteration 236/1000 | Loss: 0.00001074
Iteration 237/1000 | Loss: 0.00001074
Iteration 238/1000 | Loss: 0.00001074
Iteration 239/1000 | Loss: 0.00001074
Iteration 240/1000 | Loss: 0.00001074
Iteration 241/1000 | Loss: 0.00001074
Iteration 242/1000 | Loss: 0.00001074
Iteration 243/1000 | Loss: 0.00001074
Iteration 244/1000 | Loss: 0.00001074
Iteration 245/1000 | Loss: 0.00001074
Iteration 246/1000 | Loss: 0.00001074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [1.0740062862168998e-05, 1.0740062862168998e-05, 1.0740062862168998e-05, 1.0740062862168998e-05, 1.0740062862168998e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0740062862168998e-05

Optimization complete. Final v2v error: 2.7698984146118164 mm

Highest mean error: 3.7887628078460693 mm for frame 66

Lowest mean error: 2.4613513946533203 mm for frame 142

Saving results

Total time: 43.989389419555664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025543
Iteration 2/25 | Loss: 0.01025543
Iteration 3/25 | Loss: 0.01025543
Iteration 4/25 | Loss: 0.01025542
Iteration 5/25 | Loss: 0.01025542
Iteration 6/25 | Loss: 0.01025542
Iteration 7/25 | Loss: 0.01025542
Iteration 8/25 | Loss: 0.01025542
Iteration 9/25 | Loss: 0.01025542
Iteration 10/25 | Loss: 0.01025542
Iteration 11/25 | Loss: 0.01025542
Iteration 12/25 | Loss: 0.01025542
Iteration 13/25 | Loss: 0.01025541
Iteration 14/25 | Loss: 0.01025541
Iteration 15/25 | Loss: 0.01025541
Iteration 16/25 | Loss: 0.01025541
Iteration 17/25 | Loss: 0.01025541
Iteration 18/25 | Loss: 0.01025541
Iteration 19/25 | Loss: 0.01025541
Iteration 20/25 | Loss: 0.01025541
Iteration 21/25 | Loss: 0.01025540
Iteration 22/25 | Loss: 0.01025540
Iteration 23/25 | Loss: 0.01025540
Iteration 24/25 | Loss: 0.01025540
Iteration 25/25 | Loss: 0.01025540

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.00194931
Iteration 2/25 | Loss: 0.11625329
Iteration 3/25 | Loss: 0.11281668
Iteration 4/25 | Loss: 0.11152536
Iteration 5/25 | Loss: 0.11074238
Iteration 6/25 | Loss: 0.11075903
Iteration 7/25 | Loss: 0.11071072
Iteration 8/25 | Loss: 0.11069107
Iteration 9/25 | Loss: 0.11069107
Iteration 10/25 | Loss: 0.11069105
Iteration 11/25 | Loss: 0.11069105
Iteration 12/25 | Loss: 0.11069105
Iteration 13/25 | Loss: 0.11069105
Iteration 14/25 | Loss: 0.11069105
Iteration 15/25 | Loss: 0.11069105
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.11069104820489883, 0.11069104820489883, 0.11069104820489883, 0.11069104820489883, 0.11069104820489883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11069104820489883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11069105
Iteration 2/1000 | Loss: 0.00130002
Iteration 3/1000 | Loss: 0.00158604
Iteration 4/1000 | Loss: 0.00040857
Iteration 5/1000 | Loss: 0.00013473
Iteration 6/1000 | Loss: 0.00018775
Iteration 7/1000 | Loss: 0.00012265
Iteration 8/1000 | Loss: 0.00047307
Iteration 9/1000 | Loss: 0.00005513
Iteration 10/1000 | Loss: 0.00009287
Iteration 11/1000 | Loss: 0.00005247
Iteration 12/1000 | Loss: 0.00006696
Iteration 13/1000 | Loss: 0.00005393
Iteration 14/1000 | Loss: 0.00028827
Iteration 15/1000 | Loss: 0.00003760
Iteration 16/1000 | Loss: 0.00011070
Iteration 17/1000 | Loss: 0.00003574
Iteration 18/1000 | Loss: 0.00003406
Iteration 19/1000 | Loss: 0.00004008
Iteration 20/1000 | Loss: 0.00003207
Iteration 21/1000 | Loss: 0.00012329
Iteration 22/1000 | Loss: 0.00008957
Iteration 23/1000 | Loss: 0.00004294
Iteration 24/1000 | Loss: 0.00008889
Iteration 25/1000 | Loss: 0.00003982
Iteration 26/1000 | Loss: 0.00010070
Iteration 27/1000 | Loss: 0.00003327
Iteration 28/1000 | Loss: 0.00023426
Iteration 29/1000 | Loss: 0.00002736
Iteration 30/1000 | Loss: 0.00002636
Iteration 31/1000 | Loss: 0.00002575
Iteration 32/1000 | Loss: 0.00003948
Iteration 33/1000 | Loss: 0.00002472
Iteration 34/1000 | Loss: 0.00020044
Iteration 35/1000 | Loss: 0.00004499
Iteration 36/1000 | Loss: 0.00002400
Iteration 37/1000 | Loss: 0.00002360
Iteration 38/1000 | Loss: 0.00004660
Iteration 39/1000 | Loss: 0.00002912
Iteration 40/1000 | Loss: 0.00002303
Iteration 41/1000 | Loss: 0.00003880
Iteration 42/1000 | Loss: 0.00002388
Iteration 43/1000 | Loss: 0.00002410
Iteration 44/1000 | Loss: 0.00002389
Iteration 45/1000 | Loss: 0.00002787
Iteration 46/1000 | Loss: 0.00002226
Iteration 47/1000 | Loss: 0.00002701
Iteration 48/1000 | Loss: 0.00002287
Iteration 49/1000 | Loss: 0.00002198
Iteration 50/1000 | Loss: 0.00002193
Iteration 51/1000 | Loss: 0.00002193
Iteration 52/1000 | Loss: 0.00002193
Iteration 53/1000 | Loss: 0.00002193
Iteration 54/1000 | Loss: 0.00002193
Iteration 55/1000 | Loss: 0.00002193
Iteration 56/1000 | Loss: 0.00002193
Iteration 57/1000 | Loss: 0.00002192
Iteration 58/1000 | Loss: 0.00002192
Iteration 59/1000 | Loss: 0.00002192
Iteration 60/1000 | Loss: 0.00002192
Iteration 61/1000 | Loss: 0.00002192
Iteration 62/1000 | Loss: 0.00002192
Iteration 63/1000 | Loss: 0.00002192
Iteration 64/1000 | Loss: 0.00002192
Iteration 65/1000 | Loss: 0.00002187
Iteration 66/1000 | Loss: 0.00002186
Iteration 67/1000 | Loss: 0.00003646
Iteration 68/1000 | Loss: 0.00002790
Iteration 69/1000 | Loss: 0.00002171
Iteration 70/1000 | Loss: 0.00002170
Iteration 71/1000 | Loss: 0.00002166
Iteration 72/1000 | Loss: 0.00002808
Iteration 73/1000 | Loss: 0.00004281
Iteration 74/1000 | Loss: 0.00002676
Iteration 75/1000 | Loss: 0.00002197
Iteration 76/1000 | Loss: 0.00002150
Iteration 77/1000 | Loss: 0.00002150
Iteration 78/1000 | Loss: 0.00002150
Iteration 79/1000 | Loss: 0.00002150
Iteration 80/1000 | Loss: 0.00002150
Iteration 81/1000 | Loss: 0.00002149
Iteration 82/1000 | Loss: 0.00002149
Iteration 83/1000 | Loss: 0.00002149
Iteration 84/1000 | Loss: 0.00002149
Iteration 85/1000 | Loss: 0.00002149
Iteration 86/1000 | Loss: 0.00002149
Iteration 87/1000 | Loss: 0.00002149
Iteration 88/1000 | Loss: 0.00002149
Iteration 89/1000 | Loss: 0.00002149
Iteration 90/1000 | Loss: 0.00002149
Iteration 91/1000 | Loss: 0.00002148
Iteration 92/1000 | Loss: 0.00002148
Iteration 93/1000 | Loss: 0.00002148
Iteration 94/1000 | Loss: 0.00002148
Iteration 95/1000 | Loss: 0.00002148
Iteration 96/1000 | Loss: 0.00002147
Iteration 97/1000 | Loss: 0.00002147
Iteration 98/1000 | Loss: 0.00002147
Iteration 99/1000 | Loss: 0.00002146
Iteration 100/1000 | Loss: 0.00002146
Iteration 101/1000 | Loss: 0.00002146
Iteration 102/1000 | Loss: 0.00002146
Iteration 103/1000 | Loss: 0.00002146
Iteration 104/1000 | Loss: 0.00002145
Iteration 105/1000 | Loss: 0.00002145
Iteration 106/1000 | Loss: 0.00002145
Iteration 107/1000 | Loss: 0.00002144
Iteration 108/1000 | Loss: 0.00002144
Iteration 109/1000 | Loss: 0.00002144
Iteration 110/1000 | Loss: 0.00002143
Iteration 111/1000 | Loss: 0.00002143
Iteration 112/1000 | Loss: 0.00002141
Iteration 113/1000 | Loss: 0.00002139
Iteration 114/1000 | Loss: 0.00002139
Iteration 115/1000 | Loss: 0.00002139
Iteration 116/1000 | Loss: 0.00002139
Iteration 117/1000 | Loss: 0.00002139
Iteration 118/1000 | Loss: 0.00002139
Iteration 119/1000 | Loss: 0.00002139
Iteration 120/1000 | Loss: 0.00002139
Iteration 121/1000 | Loss: 0.00002139
Iteration 122/1000 | Loss: 0.00002139
Iteration 123/1000 | Loss: 0.00002139
Iteration 124/1000 | Loss: 0.00003021
Iteration 125/1000 | Loss: 0.00002135
Iteration 126/1000 | Loss: 0.00002134
Iteration 127/1000 | Loss: 0.00002134
Iteration 128/1000 | Loss: 0.00002132
Iteration 129/1000 | Loss: 0.00002132
Iteration 130/1000 | Loss: 0.00002132
Iteration 131/1000 | Loss: 0.00002132
Iteration 132/1000 | Loss: 0.00002132
Iteration 133/1000 | Loss: 0.00002132
Iteration 134/1000 | Loss: 0.00002132
Iteration 135/1000 | Loss: 0.00002131
Iteration 136/1000 | Loss: 0.00002131
Iteration 137/1000 | Loss: 0.00002131
Iteration 138/1000 | Loss: 0.00002131
Iteration 139/1000 | Loss: 0.00002130
Iteration 140/1000 | Loss: 0.00002130
Iteration 141/1000 | Loss: 0.00002130
Iteration 142/1000 | Loss: 0.00002129
Iteration 143/1000 | Loss: 0.00002129
Iteration 144/1000 | Loss: 0.00002129
Iteration 145/1000 | Loss: 0.00002129
Iteration 146/1000 | Loss: 0.00002128
Iteration 147/1000 | Loss: 0.00002128
Iteration 148/1000 | Loss: 0.00002128
Iteration 149/1000 | Loss: 0.00002128
Iteration 150/1000 | Loss: 0.00002127
Iteration 151/1000 | Loss: 0.00002127
Iteration 152/1000 | Loss: 0.00002127
Iteration 153/1000 | Loss: 0.00002127
Iteration 154/1000 | Loss: 0.00002127
Iteration 155/1000 | Loss: 0.00002127
Iteration 156/1000 | Loss: 0.00002127
Iteration 157/1000 | Loss: 0.00002127
Iteration 158/1000 | Loss: 0.00002127
Iteration 159/1000 | Loss: 0.00002127
Iteration 160/1000 | Loss: 0.00002127
Iteration 161/1000 | Loss: 0.00002127
Iteration 162/1000 | Loss: 0.00002126
Iteration 163/1000 | Loss: 0.00002996
Iteration 164/1000 | Loss: 0.00002124
Iteration 165/1000 | Loss: 0.00002122
Iteration 166/1000 | Loss: 0.00002121
Iteration 167/1000 | Loss: 0.00002121
Iteration 168/1000 | Loss: 0.00002121
Iteration 169/1000 | Loss: 0.00002121
Iteration 170/1000 | Loss: 0.00002121
Iteration 171/1000 | Loss: 0.00002121
Iteration 172/1000 | Loss: 0.00002121
Iteration 173/1000 | Loss: 0.00002121
Iteration 174/1000 | Loss: 0.00002121
Iteration 175/1000 | Loss: 0.00002121
Iteration 176/1000 | Loss: 0.00002121
Iteration 177/1000 | Loss: 0.00002121
Iteration 178/1000 | Loss: 0.00002121
Iteration 179/1000 | Loss: 0.00002121
Iteration 180/1000 | Loss: 0.00002121
Iteration 181/1000 | Loss: 0.00002121
Iteration 182/1000 | Loss: 0.00002120
Iteration 183/1000 | Loss: 0.00002120
Iteration 184/1000 | Loss: 0.00002120
Iteration 185/1000 | Loss: 0.00002120
Iteration 186/1000 | Loss: 0.00002120
Iteration 187/1000 | Loss: 0.00002120
Iteration 188/1000 | Loss: 0.00002120
Iteration 189/1000 | Loss: 0.00002120
Iteration 190/1000 | Loss: 0.00002120
Iteration 191/1000 | Loss: 0.00002120
Iteration 192/1000 | Loss: 0.00002120
Iteration 193/1000 | Loss: 0.00002120
Iteration 194/1000 | Loss: 0.00002120
Iteration 195/1000 | Loss: 0.00002120
Iteration 196/1000 | Loss: 0.00002120
Iteration 197/1000 | Loss: 0.00002120
Iteration 198/1000 | Loss: 0.00002120
Iteration 199/1000 | Loss: 0.00002120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.1202316929702647e-05, 2.1202316929702647e-05, 2.1202316929702647e-05, 2.1202316929702647e-05, 2.1202316929702647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1202316929702647e-05

Optimization complete. Final v2v error: 3.853248357772827 mm

Highest mean error: 5.347458839416504 mm for frame 192

Lowest mean error: 2.9093079566955566 mm for frame 47

Saving results

Total time: 112.85723280906677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838112
Iteration 2/25 | Loss: 0.00163118
Iteration 3/25 | Loss: 0.00136749
Iteration 4/25 | Loss: 0.00133894
Iteration 5/25 | Loss: 0.00132544
Iteration 6/25 | Loss: 0.00131453
Iteration 7/25 | Loss: 0.00132983
Iteration 8/25 | Loss: 0.00129588
Iteration 9/25 | Loss: 0.00127719
Iteration 10/25 | Loss: 0.00127703
Iteration 11/25 | Loss: 0.00127395
Iteration 12/25 | Loss: 0.00126605
Iteration 13/25 | Loss: 0.00126828
Iteration 14/25 | Loss: 0.00126005
Iteration 15/25 | Loss: 0.00126276
Iteration 16/25 | Loss: 0.00126005
Iteration 17/25 | Loss: 0.00125292
Iteration 18/25 | Loss: 0.00125183
Iteration 19/25 | Loss: 0.00125174
Iteration 20/25 | Loss: 0.00125166
Iteration 21/25 | Loss: 0.00125129
Iteration 22/25 | Loss: 0.00125545
Iteration 23/25 | Loss: 0.00124807
Iteration 24/25 | Loss: 0.00124687
Iteration 25/25 | Loss: 0.00124681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92049837
Iteration 2/25 | Loss: 0.00063775
Iteration 3/25 | Loss: 0.00063774
Iteration 4/25 | Loss: 0.00063774
Iteration 5/25 | Loss: 0.00063774
Iteration 6/25 | Loss: 0.00063774
Iteration 7/25 | Loss: 0.00063774
Iteration 8/25 | Loss: 0.00063774
Iteration 9/25 | Loss: 0.00063774
Iteration 10/25 | Loss: 0.00063774
Iteration 11/25 | Loss: 0.00063774
Iteration 12/25 | Loss: 0.00063774
Iteration 13/25 | Loss: 0.00063774
Iteration 14/25 | Loss: 0.00063774
Iteration 15/25 | Loss: 0.00063774
Iteration 16/25 | Loss: 0.00063774
Iteration 17/25 | Loss: 0.00063774
Iteration 18/25 | Loss: 0.00063774
Iteration 19/25 | Loss: 0.00063774
Iteration 20/25 | Loss: 0.00063774
Iteration 21/25 | Loss: 0.00063774
Iteration 22/25 | Loss: 0.00063774
Iteration 23/25 | Loss: 0.00063774
Iteration 24/25 | Loss: 0.00063774
Iteration 25/25 | Loss: 0.00063774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063774
Iteration 2/1000 | Loss: 0.00003879
Iteration 3/1000 | Loss: 0.00003031
Iteration 4/1000 | Loss: 0.00002726
Iteration 5/1000 | Loss: 0.00002628
Iteration 6/1000 | Loss: 0.00002527
Iteration 7/1000 | Loss: 0.00002449
Iteration 8/1000 | Loss: 0.00002383
Iteration 9/1000 | Loss: 0.00002342
Iteration 10/1000 | Loss: 0.00002296
Iteration 11/1000 | Loss: 0.00002258
Iteration 12/1000 | Loss: 0.00042434
Iteration 13/1000 | Loss: 0.00002653
Iteration 14/1000 | Loss: 0.00002362
Iteration 15/1000 | Loss: 0.00002171
Iteration 16/1000 | Loss: 0.00002077
Iteration 17/1000 | Loss: 0.00002028
Iteration 18/1000 | Loss: 0.00001998
Iteration 19/1000 | Loss: 0.00001988
Iteration 20/1000 | Loss: 0.00001986
Iteration 21/1000 | Loss: 0.00001986
Iteration 22/1000 | Loss: 0.00001983
Iteration 23/1000 | Loss: 0.00001983
Iteration 24/1000 | Loss: 0.00001982
Iteration 25/1000 | Loss: 0.00001982
Iteration 26/1000 | Loss: 0.00001982
Iteration 27/1000 | Loss: 0.00001981
Iteration 28/1000 | Loss: 0.00001981
Iteration 29/1000 | Loss: 0.00001980
Iteration 30/1000 | Loss: 0.00001979
Iteration 31/1000 | Loss: 0.00001979
Iteration 32/1000 | Loss: 0.00001979
Iteration 33/1000 | Loss: 0.00001978
Iteration 34/1000 | Loss: 0.00001978
Iteration 35/1000 | Loss: 0.00001977
Iteration 36/1000 | Loss: 0.00001976
Iteration 37/1000 | Loss: 0.00001975
Iteration 38/1000 | Loss: 0.00001975
Iteration 39/1000 | Loss: 0.00001974
Iteration 40/1000 | Loss: 0.00001974
Iteration 41/1000 | Loss: 0.00001974
Iteration 42/1000 | Loss: 0.00001974
Iteration 43/1000 | Loss: 0.00001973
Iteration 44/1000 | Loss: 0.00001973
Iteration 45/1000 | Loss: 0.00001973
Iteration 46/1000 | Loss: 0.00001972
Iteration 47/1000 | Loss: 0.00001972
Iteration 48/1000 | Loss: 0.00001972
Iteration 49/1000 | Loss: 0.00001971
Iteration 50/1000 | Loss: 0.00001971
Iteration 51/1000 | Loss: 0.00001970
Iteration 52/1000 | Loss: 0.00001970
Iteration 53/1000 | Loss: 0.00001970
Iteration 54/1000 | Loss: 0.00001969
Iteration 55/1000 | Loss: 0.00001969
Iteration 56/1000 | Loss: 0.00001969
Iteration 57/1000 | Loss: 0.00001967
Iteration 58/1000 | Loss: 0.00001967
Iteration 59/1000 | Loss: 0.00001966
Iteration 60/1000 | Loss: 0.00001966
Iteration 61/1000 | Loss: 0.00001966
Iteration 62/1000 | Loss: 0.00001966
Iteration 63/1000 | Loss: 0.00001966
Iteration 64/1000 | Loss: 0.00001966
Iteration 65/1000 | Loss: 0.00001966
Iteration 66/1000 | Loss: 0.00001966
Iteration 67/1000 | Loss: 0.00001966
Iteration 68/1000 | Loss: 0.00001965
Iteration 69/1000 | Loss: 0.00001965
Iteration 70/1000 | Loss: 0.00001965
Iteration 71/1000 | Loss: 0.00001965
Iteration 72/1000 | Loss: 0.00001965
Iteration 73/1000 | Loss: 0.00001965
Iteration 74/1000 | Loss: 0.00001965
Iteration 75/1000 | Loss: 0.00001965
Iteration 76/1000 | Loss: 0.00001964
Iteration 77/1000 | Loss: 0.00001964
Iteration 78/1000 | Loss: 0.00001963
Iteration 79/1000 | Loss: 0.00001963
Iteration 80/1000 | Loss: 0.00001963
Iteration 81/1000 | Loss: 0.00001963
Iteration 82/1000 | Loss: 0.00001963
Iteration 83/1000 | Loss: 0.00001963
Iteration 84/1000 | Loss: 0.00001963
Iteration 85/1000 | Loss: 0.00001962
Iteration 86/1000 | Loss: 0.00001962
Iteration 87/1000 | Loss: 0.00001962
Iteration 88/1000 | Loss: 0.00001962
Iteration 89/1000 | Loss: 0.00001962
Iteration 90/1000 | Loss: 0.00001962
Iteration 91/1000 | Loss: 0.00001962
Iteration 92/1000 | Loss: 0.00001962
Iteration 93/1000 | Loss: 0.00001962
Iteration 94/1000 | Loss: 0.00001962
Iteration 95/1000 | Loss: 0.00001962
Iteration 96/1000 | Loss: 0.00001962
Iteration 97/1000 | Loss: 0.00001961
Iteration 98/1000 | Loss: 0.00001961
Iteration 99/1000 | Loss: 0.00001961
Iteration 100/1000 | Loss: 0.00001961
Iteration 101/1000 | Loss: 0.00001960
Iteration 102/1000 | Loss: 0.00001960
Iteration 103/1000 | Loss: 0.00001960
Iteration 104/1000 | Loss: 0.00001960
Iteration 105/1000 | Loss: 0.00001960
Iteration 106/1000 | Loss: 0.00001960
Iteration 107/1000 | Loss: 0.00001960
Iteration 108/1000 | Loss: 0.00001960
Iteration 109/1000 | Loss: 0.00001959
Iteration 110/1000 | Loss: 0.00001959
Iteration 111/1000 | Loss: 0.00001959
Iteration 112/1000 | Loss: 0.00001959
Iteration 113/1000 | Loss: 0.00001959
Iteration 114/1000 | Loss: 0.00001959
Iteration 115/1000 | Loss: 0.00001959
Iteration 116/1000 | Loss: 0.00001959
Iteration 117/1000 | Loss: 0.00001959
Iteration 118/1000 | Loss: 0.00001959
Iteration 119/1000 | Loss: 0.00001959
Iteration 120/1000 | Loss: 0.00001959
Iteration 121/1000 | Loss: 0.00001959
Iteration 122/1000 | Loss: 0.00001959
Iteration 123/1000 | Loss: 0.00001959
Iteration 124/1000 | Loss: 0.00001959
Iteration 125/1000 | Loss: 0.00001959
Iteration 126/1000 | Loss: 0.00001959
Iteration 127/1000 | Loss: 0.00001959
Iteration 128/1000 | Loss: 0.00001959
Iteration 129/1000 | Loss: 0.00001958
Iteration 130/1000 | Loss: 0.00001958
Iteration 131/1000 | Loss: 0.00001958
Iteration 132/1000 | Loss: 0.00001958
Iteration 133/1000 | Loss: 0.00001958
Iteration 134/1000 | Loss: 0.00001958
Iteration 135/1000 | Loss: 0.00001958
Iteration 136/1000 | Loss: 0.00001958
Iteration 137/1000 | Loss: 0.00001958
Iteration 138/1000 | Loss: 0.00001958
Iteration 139/1000 | Loss: 0.00001958
Iteration 140/1000 | Loss: 0.00001958
Iteration 141/1000 | Loss: 0.00001958
Iteration 142/1000 | Loss: 0.00001958
Iteration 143/1000 | Loss: 0.00001958
Iteration 144/1000 | Loss: 0.00001958
Iteration 145/1000 | Loss: 0.00001958
Iteration 146/1000 | Loss: 0.00001958
Iteration 147/1000 | Loss: 0.00001958
Iteration 148/1000 | Loss: 0.00001958
Iteration 149/1000 | Loss: 0.00001958
Iteration 150/1000 | Loss: 0.00001958
Iteration 151/1000 | Loss: 0.00001958
Iteration 152/1000 | Loss: 0.00001958
Iteration 153/1000 | Loss: 0.00001958
Iteration 154/1000 | Loss: 0.00001958
Iteration 155/1000 | Loss: 0.00001958
Iteration 156/1000 | Loss: 0.00001958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.9584334950195625e-05, 1.9584334950195625e-05, 1.9584334950195625e-05, 1.9584334950195625e-05, 1.9584334950195625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9584334950195625e-05

Optimization complete. Final v2v error: 3.734593629837036 mm

Highest mean error: 4.334065914154053 mm for frame 137

Lowest mean error: 3.5927441120147705 mm for frame 0

Saving results

Total time: 76.11109519004822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00732856
Iteration 2/25 | Loss: 0.00171820
Iteration 3/25 | Loss: 0.00134809
Iteration 4/25 | Loss: 0.00130797
Iteration 5/25 | Loss: 0.00130565
Iteration 6/25 | Loss: 0.00130032
Iteration 7/25 | Loss: 0.00129969
Iteration 8/25 | Loss: 0.00130317
Iteration 9/25 | Loss: 0.00129833
Iteration 10/25 | Loss: 0.00129744
Iteration 11/25 | Loss: 0.00130001
Iteration 12/25 | Loss: 0.00129938
Iteration 13/25 | Loss: 0.00129774
Iteration 14/25 | Loss: 0.00129619
Iteration 15/25 | Loss: 0.00129590
Iteration 16/25 | Loss: 0.00129590
Iteration 17/25 | Loss: 0.00129590
Iteration 18/25 | Loss: 0.00129590
Iteration 19/25 | Loss: 0.00129590
Iteration 20/25 | Loss: 0.00129590
Iteration 21/25 | Loss: 0.00129590
Iteration 22/25 | Loss: 0.00129589
Iteration 23/25 | Loss: 0.00129589
Iteration 24/25 | Loss: 0.00129589
Iteration 25/25 | Loss: 0.00129589

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.93859601
Iteration 2/25 | Loss: 0.00078228
Iteration 3/25 | Loss: 0.00078226
Iteration 4/25 | Loss: 0.00078226
Iteration 5/25 | Loss: 0.00078226
Iteration 6/25 | Loss: 0.00078226
Iteration 7/25 | Loss: 0.00078225
Iteration 8/25 | Loss: 0.00078225
Iteration 9/25 | Loss: 0.00078225
Iteration 10/25 | Loss: 0.00078225
Iteration 11/25 | Loss: 0.00078225
Iteration 12/25 | Loss: 0.00078225
Iteration 13/25 | Loss: 0.00078225
Iteration 14/25 | Loss: 0.00078225
Iteration 15/25 | Loss: 0.00078225
Iteration 16/25 | Loss: 0.00078225
Iteration 17/25 | Loss: 0.00078225
Iteration 18/25 | Loss: 0.00078225
Iteration 19/25 | Loss: 0.00078225
Iteration 20/25 | Loss: 0.00078225
Iteration 21/25 | Loss: 0.00078225
Iteration 22/25 | Loss: 0.00078225
Iteration 23/25 | Loss: 0.00078225
Iteration 24/25 | Loss: 0.00078225
Iteration 25/25 | Loss: 0.00078225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078225
Iteration 2/1000 | Loss: 0.00003342
Iteration 3/1000 | Loss: 0.00002636
Iteration 4/1000 | Loss: 0.00002438
Iteration 5/1000 | Loss: 0.00002327
Iteration 6/1000 | Loss: 0.00002246
Iteration 7/1000 | Loss: 0.00002177
Iteration 8/1000 | Loss: 0.00002118
Iteration 9/1000 | Loss: 0.00002092
Iteration 10/1000 | Loss: 0.00002069
Iteration 11/1000 | Loss: 0.00002054
Iteration 12/1000 | Loss: 0.00002052
Iteration 13/1000 | Loss: 0.00002051
Iteration 14/1000 | Loss: 0.00002051
Iteration 15/1000 | Loss: 0.00002051
Iteration 16/1000 | Loss: 0.00002049
Iteration 17/1000 | Loss: 0.00002049
Iteration 18/1000 | Loss: 0.00002048
Iteration 19/1000 | Loss: 0.00002047
Iteration 20/1000 | Loss: 0.00002046
Iteration 21/1000 | Loss: 0.00002045
Iteration 22/1000 | Loss: 0.00002041
Iteration 23/1000 | Loss: 0.00002040
Iteration 24/1000 | Loss: 0.00002040
Iteration 25/1000 | Loss: 0.00002038
Iteration 26/1000 | Loss: 0.00002038
Iteration 27/1000 | Loss: 0.00002037
Iteration 28/1000 | Loss: 0.00002037
Iteration 29/1000 | Loss: 0.00002037
Iteration 30/1000 | Loss: 0.00002036
Iteration 31/1000 | Loss: 0.00002036
Iteration 32/1000 | Loss: 0.00002036
Iteration 33/1000 | Loss: 0.00002036
Iteration 34/1000 | Loss: 0.00002036
Iteration 35/1000 | Loss: 0.00002036
Iteration 36/1000 | Loss: 0.00002036
Iteration 37/1000 | Loss: 0.00002035
Iteration 38/1000 | Loss: 0.00002035
Iteration 39/1000 | Loss: 0.00002035
Iteration 40/1000 | Loss: 0.00002035
Iteration 41/1000 | Loss: 0.00002035
Iteration 42/1000 | Loss: 0.00002034
Iteration 43/1000 | Loss: 0.00002034
Iteration 44/1000 | Loss: 0.00002034
Iteration 45/1000 | Loss: 0.00002034
Iteration 46/1000 | Loss: 0.00002034
Iteration 47/1000 | Loss: 0.00002034
Iteration 48/1000 | Loss: 0.00002034
Iteration 49/1000 | Loss: 0.00002033
Iteration 50/1000 | Loss: 0.00002032
Iteration 51/1000 | Loss: 0.00002032
Iteration 52/1000 | Loss: 0.00002032
Iteration 53/1000 | Loss: 0.00002032
Iteration 54/1000 | Loss: 0.00002032
Iteration 55/1000 | Loss: 0.00002032
Iteration 56/1000 | Loss: 0.00002032
Iteration 57/1000 | Loss: 0.00002032
Iteration 58/1000 | Loss: 0.00002032
Iteration 59/1000 | Loss: 0.00002031
Iteration 60/1000 | Loss: 0.00002031
Iteration 61/1000 | Loss: 0.00002031
Iteration 62/1000 | Loss: 0.00002031
Iteration 63/1000 | Loss: 0.00002031
Iteration 64/1000 | Loss: 0.00002030
Iteration 65/1000 | Loss: 0.00002030
Iteration 66/1000 | Loss: 0.00002029
Iteration 67/1000 | Loss: 0.00002029
Iteration 68/1000 | Loss: 0.00002029
Iteration 69/1000 | Loss: 0.00002028
Iteration 70/1000 | Loss: 0.00002028
Iteration 71/1000 | Loss: 0.00002028
Iteration 72/1000 | Loss: 0.00002028
Iteration 73/1000 | Loss: 0.00002028
Iteration 74/1000 | Loss: 0.00002027
Iteration 75/1000 | Loss: 0.00002027
Iteration 76/1000 | Loss: 0.00002027
Iteration 77/1000 | Loss: 0.00002027
Iteration 78/1000 | Loss: 0.00002027
Iteration 79/1000 | Loss: 0.00002027
Iteration 80/1000 | Loss: 0.00002027
Iteration 81/1000 | Loss: 0.00002026
Iteration 82/1000 | Loss: 0.00002026
Iteration 83/1000 | Loss: 0.00002026
Iteration 84/1000 | Loss: 0.00002026
Iteration 85/1000 | Loss: 0.00002026
Iteration 86/1000 | Loss: 0.00002026
Iteration 87/1000 | Loss: 0.00002026
Iteration 88/1000 | Loss: 0.00002026
Iteration 89/1000 | Loss: 0.00002026
Iteration 90/1000 | Loss: 0.00002026
Iteration 91/1000 | Loss: 0.00002026
Iteration 92/1000 | Loss: 0.00002026
Iteration 93/1000 | Loss: 0.00002026
Iteration 94/1000 | Loss: 0.00002026
Iteration 95/1000 | Loss: 0.00002026
Iteration 96/1000 | Loss: 0.00002026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.026039692282211e-05, 2.026039692282211e-05, 2.026039692282211e-05, 2.026039692282211e-05, 2.026039692282211e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.026039692282211e-05

Optimization complete. Final v2v error: 3.7555325031280518 mm

Highest mean error: 5.252601623535156 mm for frame 202

Lowest mean error: 3.3427369594573975 mm for frame 215

Saving results

Total time: 54.27525448799133
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800260
Iteration 2/25 | Loss: 0.00197848
Iteration 3/25 | Loss: 0.00153991
Iteration 4/25 | Loss: 0.00147647
Iteration 5/25 | Loss: 0.00145352
Iteration 6/25 | Loss: 0.00142976
Iteration 7/25 | Loss: 0.00141635
Iteration 8/25 | Loss: 0.00138870
Iteration 9/25 | Loss: 0.00138372
Iteration 10/25 | Loss: 0.00137580
Iteration 11/25 | Loss: 0.00137152
Iteration 12/25 | Loss: 0.00136752
Iteration 13/25 | Loss: 0.00136709
Iteration 14/25 | Loss: 0.00136701
Iteration 15/25 | Loss: 0.00136697
Iteration 16/25 | Loss: 0.00136697
Iteration 17/25 | Loss: 0.00136697
Iteration 18/25 | Loss: 0.00136696
Iteration 19/25 | Loss: 0.00136696
Iteration 20/25 | Loss: 0.00136696
Iteration 21/25 | Loss: 0.00136696
Iteration 22/25 | Loss: 0.00136696
Iteration 23/25 | Loss: 0.00136696
Iteration 24/25 | Loss: 0.00136696
Iteration 25/25 | Loss: 0.00136696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56605530
Iteration 2/25 | Loss: 0.00093508
Iteration 3/25 | Loss: 0.00093506
Iteration 4/25 | Loss: 0.00093506
Iteration 5/25 | Loss: 0.00093506
Iteration 6/25 | Loss: 0.00093506
Iteration 7/25 | Loss: 0.00093505
Iteration 8/25 | Loss: 0.00093505
Iteration 9/25 | Loss: 0.00093505
Iteration 10/25 | Loss: 0.00093505
Iteration 11/25 | Loss: 0.00093505
Iteration 12/25 | Loss: 0.00093505
Iteration 13/25 | Loss: 0.00093505
Iteration 14/25 | Loss: 0.00093505
Iteration 15/25 | Loss: 0.00093505
Iteration 16/25 | Loss: 0.00093505
Iteration 17/25 | Loss: 0.00093505
Iteration 18/25 | Loss: 0.00093505
Iteration 19/25 | Loss: 0.00093505
Iteration 20/25 | Loss: 0.00093505
Iteration 21/25 | Loss: 0.00093505
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009350533946417272, 0.0009350533946417272, 0.0009350533946417272, 0.0009350533946417272, 0.0009350533946417272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009350533946417272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093505
Iteration 2/1000 | Loss: 0.00004948
Iteration 3/1000 | Loss: 0.00003500
Iteration 4/1000 | Loss: 0.00003212
Iteration 5/1000 | Loss: 0.00003059
Iteration 6/1000 | Loss: 0.00002944
Iteration 7/1000 | Loss: 0.00002879
Iteration 8/1000 | Loss: 0.00002836
Iteration 9/1000 | Loss: 0.00002795
Iteration 10/1000 | Loss: 0.00002769
Iteration 11/1000 | Loss: 0.00002757
Iteration 12/1000 | Loss: 0.00002741
Iteration 13/1000 | Loss: 0.00002739
Iteration 14/1000 | Loss: 0.00002734
Iteration 15/1000 | Loss: 0.00002732
Iteration 16/1000 | Loss: 0.00002731
Iteration 17/1000 | Loss: 0.00002730
Iteration 18/1000 | Loss: 0.00002726
Iteration 19/1000 | Loss: 0.00002719
Iteration 20/1000 | Loss: 0.00002718
Iteration 21/1000 | Loss: 0.00002718
Iteration 22/1000 | Loss: 0.00002717
Iteration 23/1000 | Loss: 0.00002716
Iteration 24/1000 | Loss: 0.00002713
Iteration 25/1000 | Loss: 0.00002713
Iteration 26/1000 | Loss: 0.00002711
Iteration 27/1000 | Loss: 0.00002707
Iteration 28/1000 | Loss: 0.00002705
Iteration 29/1000 | Loss: 0.00002705
Iteration 30/1000 | Loss: 0.00002702
Iteration 31/1000 | Loss: 0.00002702
Iteration 32/1000 | Loss: 0.00002702
Iteration 33/1000 | Loss: 0.00002702
Iteration 34/1000 | Loss: 0.00002702
Iteration 35/1000 | Loss: 0.00002702
Iteration 36/1000 | Loss: 0.00002702
Iteration 37/1000 | Loss: 0.00002701
Iteration 38/1000 | Loss: 0.00002701
Iteration 39/1000 | Loss: 0.00002701
Iteration 40/1000 | Loss: 0.00002700
Iteration 41/1000 | Loss: 0.00002700
Iteration 42/1000 | Loss: 0.00002699
Iteration 43/1000 | Loss: 0.00002699
Iteration 44/1000 | Loss: 0.00002699
Iteration 45/1000 | Loss: 0.00002699
Iteration 46/1000 | Loss: 0.00002699
Iteration 47/1000 | Loss: 0.00002699
Iteration 48/1000 | Loss: 0.00002699
Iteration 49/1000 | Loss: 0.00002699
Iteration 50/1000 | Loss: 0.00002699
Iteration 51/1000 | Loss: 0.00002699
Iteration 52/1000 | Loss: 0.00002699
Iteration 53/1000 | Loss: 0.00002699
Iteration 54/1000 | Loss: 0.00002698
Iteration 55/1000 | Loss: 0.00002698
Iteration 56/1000 | Loss: 0.00002697
Iteration 57/1000 | Loss: 0.00002697
Iteration 58/1000 | Loss: 0.00002697
Iteration 59/1000 | Loss: 0.00002696
Iteration 60/1000 | Loss: 0.00002696
Iteration 61/1000 | Loss: 0.00002696
Iteration 62/1000 | Loss: 0.00002695
Iteration 63/1000 | Loss: 0.00002695
Iteration 64/1000 | Loss: 0.00002695
Iteration 65/1000 | Loss: 0.00002694
Iteration 66/1000 | Loss: 0.00002693
Iteration 67/1000 | Loss: 0.00002693
Iteration 68/1000 | Loss: 0.00002692
Iteration 69/1000 | Loss: 0.00002692
Iteration 70/1000 | Loss: 0.00002692
Iteration 71/1000 | Loss: 0.00002692
Iteration 72/1000 | Loss: 0.00002692
Iteration 73/1000 | Loss: 0.00002692
Iteration 74/1000 | Loss: 0.00002691
Iteration 75/1000 | Loss: 0.00002691
Iteration 76/1000 | Loss: 0.00002691
Iteration 77/1000 | Loss: 0.00002691
Iteration 78/1000 | Loss: 0.00002691
Iteration 79/1000 | Loss: 0.00002691
Iteration 80/1000 | Loss: 0.00002691
Iteration 81/1000 | Loss: 0.00002691
Iteration 82/1000 | Loss: 0.00002690
Iteration 83/1000 | Loss: 0.00002690
Iteration 84/1000 | Loss: 0.00002689
Iteration 85/1000 | Loss: 0.00002688
Iteration 86/1000 | Loss: 0.00002688
Iteration 87/1000 | Loss: 0.00002688
Iteration 88/1000 | Loss: 0.00002688
Iteration 89/1000 | Loss: 0.00002688
Iteration 90/1000 | Loss: 0.00002688
Iteration 91/1000 | Loss: 0.00002688
Iteration 92/1000 | Loss: 0.00002688
Iteration 93/1000 | Loss: 0.00002688
Iteration 94/1000 | Loss: 0.00002688
Iteration 95/1000 | Loss: 0.00002687
Iteration 96/1000 | Loss: 0.00002687
Iteration 97/1000 | Loss: 0.00002687
Iteration 98/1000 | Loss: 0.00002687
Iteration 99/1000 | Loss: 0.00002687
Iteration 100/1000 | Loss: 0.00002687
Iteration 101/1000 | Loss: 0.00002687
Iteration 102/1000 | Loss: 0.00002686
Iteration 103/1000 | Loss: 0.00002686
Iteration 104/1000 | Loss: 0.00002686
Iteration 105/1000 | Loss: 0.00002686
Iteration 106/1000 | Loss: 0.00002686
Iteration 107/1000 | Loss: 0.00002685
Iteration 108/1000 | Loss: 0.00002684
Iteration 109/1000 | Loss: 0.00002684
Iteration 110/1000 | Loss: 0.00002683
Iteration 111/1000 | Loss: 0.00002683
Iteration 112/1000 | Loss: 0.00002683
Iteration 113/1000 | Loss: 0.00002682
Iteration 114/1000 | Loss: 0.00002682
Iteration 115/1000 | Loss: 0.00002682
Iteration 116/1000 | Loss: 0.00002682
Iteration 117/1000 | Loss: 0.00002681
Iteration 118/1000 | Loss: 0.00002681
Iteration 119/1000 | Loss: 0.00002681
Iteration 120/1000 | Loss: 0.00002681
Iteration 121/1000 | Loss: 0.00002681
Iteration 122/1000 | Loss: 0.00002681
Iteration 123/1000 | Loss: 0.00002681
Iteration 124/1000 | Loss: 0.00002680
Iteration 125/1000 | Loss: 0.00002680
Iteration 126/1000 | Loss: 0.00002679
Iteration 127/1000 | Loss: 0.00002679
Iteration 128/1000 | Loss: 0.00002679
Iteration 129/1000 | Loss: 0.00002679
Iteration 130/1000 | Loss: 0.00002679
Iteration 131/1000 | Loss: 0.00002679
Iteration 132/1000 | Loss: 0.00002678
Iteration 133/1000 | Loss: 0.00002678
Iteration 134/1000 | Loss: 0.00002678
Iteration 135/1000 | Loss: 0.00002678
Iteration 136/1000 | Loss: 0.00002678
Iteration 137/1000 | Loss: 0.00002677
Iteration 138/1000 | Loss: 0.00002677
Iteration 139/1000 | Loss: 0.00002677
Iteration 140/1000 | Loss: 0.00002677
Iteration 141/1000 | Loss: 0.00002677
Iteration 142/1000 | Loss: 0.00002677
Iteration 143/1000 | Loss: 0.00002676
Iteration 144/1000 | Loss: 0.00002676
Iteration 145/1000 | Loss: 0.00002676
Iteration 146/1000 | Loss: 0.00002676
Iteration 147/1000 | Loss: 0.00002676
Iteration 148/1000 | Loss: 0.00002676
Iteration 149/1000 | Loss: 0.00002676
Iteration 150/1000 | Loss: 0.00002676
Iteration 151/1000 | Loss: 0.00002676
Iteration 152/1000 | Loss: 0.00002676
Iteration 153/1000 | Loss: 0.00002676
Iteration 154/1000 | Loss: 0.00002676
Iteration 155/1000 | Loss: 0.00002675
Iteration 156/1000 | Loss: 0.00002675
Iteration 157/1000 | Loss: 0.00002675
Iteration 158/1000 | Loss: 0.00002675
Iteration 159/1000 | Loss: 0.00002675
Iteration 160/1000 | Loss: 0.00002675
Iteration 161/1000 | Loss: 0.00002675
Iteration 162/1000 | Loss: 0.00002675
Iteration 163/1000 | Loss: 0.00002674
Iteration 164/1000 | Loss: 0.00002674
Iteration 165/1000 | Loss: 0.00002674
Iteration 166/1000 | Loss: 0.00002674
Iteration 167/1000 | Loss: 0.00002674
Iteration 168/1000 | Loss: 0.00002674
Iteration 169/1000 | Loss: 0.00002674
Iteration 170/1000 | Loss: 0.00002674
Iteration 171/1000 | Loss: 0.00002674
Iteration 172/1000 | Loss: 0.00002674
Iteration 173/1000 | Loss: 0.00002674
Iteration 174/1000 | Loss: 0.00002674
Iteration 175/1000 | Loss: 0.00002674
Iteration 176/1000 | Loss: 0.00002674
Iteration 177/1000 | Loss: 0.00002674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.6741035981103778e-05, 2.6741035981103778e-05, 2.6741035981103778e-05, 2.6741035981103778e-05, 2.6741035981103778e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6741035981103778e-05

Optimization complete. Final v2v error: 4.191659450531006 mm

Highest mean error: 6.05798864364624 mm for frame 188

Lowest mean error: 3.0589940547943115 mm for frame 110

Saving results

Total time: 62.790536880493164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814083
Iteration 2/25 | Loss: 0.00146586
Iteration 3/25 | Loss: 0.00128173
Iteration 4/25 | Loss: 0.00126544
Iteration 5/25 | Loss: 0.00126459
Iteration 6/25 | Loss: 0.00126459
Iteration 7/25 | Loss: 0.00126459
Iteration 8/25 | Loss: 0.00126459
Iteration 9/25 | Loss: 0.00126459
Iteration 10/25 | Loss: 0.00126459
Iteration 11/25 | Loss: 0.00126459
Iteration 12/25 | Loss: 0.00126459
Iteration 13/25 | Loss: 0.00126459
Iteration 14/25 | Loss: 0.00126459
Iteration 15/25 | Loss: 0.00126459
Iteration 16/25 | Loss: 0.00126459
Iteration 17/25 | Loss: 0.00126459
Iteration 18/25 | Loss: 0.00126459
Iteration 19/25 | Loss: 0.00126459
Iteration 20/25 | Loss: 0.00126459
Iteration 21/25 | Loss: 0.00126459
Iteration 22/25 | Loss: 0.00126459
Iteration 23/25 | Loss: 0.00126459
Iteration 24/25 | Loss: 0.00126459
Iteration 25/25 | Loss: 0.00126459

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97037524
Iteration 2/25 | Loss: 0.00061353
Iteration 3/25 | Loss: 0.00061352
Iteration 4/25 | Loss: 0.00061352
Iteration 5/25 | Loss: 0.00061352
Iteration 6/25 | Loss: 0.00061352
Iteration 7/25 | Loss: 0.00061352
Iteration 8/25 | Loss: 0.00061352
Iteration 9/25 | Loss: 0.00061352
Iteration 10/25 | Loss: 0.00061352
Iteration 11/25 | Loss: 0.00061352
Iteration 12/25 | Loss: 0.00061352
Iteration 13/25 | Loss: 0.00061352
Iteration 14/25 | Loss: 0.00061352
Iteration 15/25 | Loss: 0.00061352
Iteration 16/25 | Loss: 0.00061352
Iteration 17/25 | Loss: 0.00061352
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006135209114290774, 0.0006135209114290774, 0.0006135209114290774, 0.0006135209114290774, 0.0006135209114290774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006135209114290774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061352
Iteration 2/1000 | Loss: 0.00003314
Iteration 3/1000 | Loss: 0.00002617
Iteration 4/1000 | Loss: 0.00002415
Iteration 5/1000 | Loss: 0.00002316
Iteration 6/1000 | Loss: 0.00002254
Iteration 7/1000 | Loss: 0.00002201
Iteration 8/1000 | Loss: 0.00002163
Iteration 9/1000 | Loss: 0.00002143
Iteration 10/1000 | Loss: 0.00002118
Iteration 11/1000 | Loss: 0.00002111
Iteration 12/1000 | Loss: 0.00002103
Iteration 13/1000 | Loss: 0.00002099
Iteration 14/1000 | Loss: 0.00002084
Iteration 15/1000 | Loss: 0.00002072
Iteration 16/1000 | Loss: 0.00002065
Iteration 17/1000 | Loss: 0.00002060
Iteration 18/1000 | Loss: 0.00002060
Iteration 19/1000 | Loss: 0.00002060
Iteration 20/1000 | Loss: 0.00002058
Iteration 21/1000 | Loss: 0.00002057
Iteration 22/1000 | Loss: 0.00002056
Iteration 23/1000 | Loss: 0.00002056
Iteration 24/1000 | Loss: 0.00002056
Iteration 25/1000 | Loss: 0.00002056
Iteration 26/1000 | Loss: 0.00002056
Iteration 27/1000 | Loss: 0.00002055
Iteration 28/1000 | Loss: 0.00002055
Iteration 29/1000 | Loss: 0.00002055
Iteration 30/1000 | Loss: 0.00002055
Iteration 31/1000 | Loss: 0.00002054
Iteration 32/1000 | Loss: 0.00002052
Iteration 33/1000 | Loss: 0.00002051
Iteration 34/1000 | Loss: 0.00002051
Iteration 35/1000 | Loss: 0.00002051
Iteration 36/1000 | Loss: 0.00002050
Iteration 37/1000 | Loss: 0.00002049
Iteration 38/1000 | Loss: 0.00002049
Iteration 39/1000 | Loss: 0.00002048
Iteration 40/1000 | Loss: 0.00002048
Iteration 41/1000 | Loss: 0.00002046
Iteration 42/1000 | Loss: 0.00002046
Iteration 43/1000 | Loss: 0.00002046
Iteration 44/1000 | Loss: 0.00002046
Iteration 45/1000 | Loss: 0.00002046
Iteration 46/1000 | Loss: 0.00002045
Iteration 47/1000 | Loss: 0.00002045
Iteration 48/1000 | Loss: 0.00002045
Iteration 49/1000 | Loss: 0.00002045
Iteration 50/1000 | Loss: 0.00002045
Iteration 51/1000 | Loss: 0.00002045
Iteration 52/1000 | Loss: 0.00002045
Iteration 53/1000 | Loss: 0.00002045
Iteration 54/1000 | Loss: 0.00002044
Iteration 55/1000 | Loss: 0.00002044
Iteration 56/1000 | Loss: 0.00002043
Iteration 57/1000 | Loss: 0.00002043
Iteration 58/1000 | Loss: 0.00002042
Iteration 59/1000 | Loss: 0.00002042
Iteration 60/1000 | Loss: 0.00002042
Iteration 61/1000 | Loss: 0.00002041
Iteration 62/1000 | Loss: 0.00002041
Iteration 63/1000 | Loss: 0.00002041
Iteration 64/1000 | Loss: 0.00002040
Iteration 65/1000 | Loss: 0.00002039
Iteration 66/1000 | Loss: 0.00002039
Iteration 67/1000 | Loss: 0.00002037
Iteration 68/1000 | Loss: 0.00002037
Iteration 69/1000 | Loss: 0.00002037
Iteration 70/1000 | Loss: 0.00002036
Iteration 71/1000 | Loss: 0.00002036
Iteration 72/1000 | Loss: 0.00002036
Iteration 73/1000 | Loss: 0.00002036
Iteration 74/1000 | Loss: 0.00002036
Iteration 75/1000 | Loss: 0.00002035
Iteration 76/1000 | Loss: 0.00002034
Iteration 77/1000 | Loss: 0.00002034
Iteration 78/1000 | Loss: 0.00002034
Iteration 79/1000 | Loss: 0.00002034
Iteration 80/1000 | Loss: 0.00002034
Iteration 81/1000 | Loss: 0.00002034
Iteration 82/1000 | Loss: 0.00002034
Iteration 83/1000 | Loss: 0.00002033
Iteration 84/1000 | Loss: 0.00002033
Iteration 85/1000 | Loss: 0.00002033
Iteration 86/1000 | Loss: 0.00002033
Iteration 87/1000 | Loss: 0.00002032
Iteration 88/1000 | Loss: 0.00002031
Iteration 89/1000 | Loss: 0.00002031
Iteration 90/1000 | Loss: 0.00002031
Iteration 91/1000 | Loss: 0.00002031
Iteration 92/1000 | Loss: 0.00002031
Iteration 93/1000 | Loss: 0.00002031
Iteration 94/1000 | Loss: 0.00002031
Iteration 95/1000 | Loss: 0.00002031
Iteration 96/1000 | Loss: 0.00002030
Iteration 97/1000 | Loss: 0.00002030
Iteration 98/1000 | Loss: 0.00002030
Iteration 99/1000 | Loss: 0.00002030
Iteration 100/1000 | Loss: 0.00002030
Iteration 101/1000 | Loss: 0.00002030
Iteration 102/1000 | Loss: 0.00002030
Iteration 103/1000 | Loss: 0.00002030
Iteration 104/1000 | Loss: 0.00002030
Iteration 105/1000 | Loss: 0.00002030
Iteration 106/1000 | Loss: 0.00002030
Iteration 107/1000 | Loss: 0.00002030
Iteration 108/1000 | Loss: 0.00002030
Iteration 109/1000 | Loss: 0.00002030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.0299079551477917e-05, 2.0299079551477917e-05, 2.0299079551477917e-05, 2.0299079551477917e-05, 2.0299079551477917e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0299079551477917e-05

Optimization complete. Final v2v error: 3.7198967933654785 mm

Highest mean error: 3.8507890701293945 mm for frame 106

Lowest mean error: 3.5888097286224365 mm for frame 35

Saving results

Total time: 35.01902723312378
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411370
Iteration 2/25 | Loss: 0.00143337
Iteration 3/25 | Loss: 0.00124995
Iteration 4/25 | Loss: 0.00122531
Iteration 5/25 | Loss: 0.00122197
Iteration 6/25 | Loss: 0.00122106
Iteration 7/25 | Loss: 0.00122106
Iteration 8/25 | Loss: 0.00122106
Iteration 9/25 | Loss: 0.00122106
Iteration 10/25 | Loss: 0.00122106
Iteration 11/25 | Loss: 0.00122106
Iteration 12/25 | Loss: 0.00122106
Iteration 13/25 | Loss: 0.00122106
Iteration 14/25 | Loss: 0.00122106
Iteration 15/25 | Loss: 0.00122106
Iteration 16/25 | Loss: 0.00122106
Iteration 17/25 | Loss: 0.00122106
Iteration 18/25 | Loss: 0.00122106
Iteration 19/25 | Loss: 0.00122106
Iteration 20/25 | Loss: 0.00122106
Iteration 21/25 | Loss: 0.00122106
Iteration 22/25 | Loss: 0.00122106
Iteration 23/25 | Loss: 0.00122106
Iteration 24/25 | Loss: 0.00122106
Iteration 25/25 | Loss: 0.00122106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35338938
Iteration 2/25 | Loss: 0.00099713
Iteration 3/25 | Loss: 0.00099713
Iteration 4/25 | Loss: 0.00099713
Iteration 5/25 | Loss: 0.00099713
Iteration 6/25 | Loss: 0.00099713
Iteration 7/25 | Loss: 0.00099713
Iteration 8/25 | Loss: 0.00099713
Iteration 9/25 | Loss: 0.00099713
Iteration 10/25 | Loss: 0.00099713
Iteration 11/25 | Loss: 0.00099713
Iteration 12/25 | Loss: 0.00099713
Iteration 13/25 | Loss: 0.00099713
Iteration 14/25 | Loss: 0.00099713
Iteration 15/25 | Loss: 0.00099713
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000997126684524119, 0.000997126684524119, 0.000997126684524119, 0.000997126684524119, 0.000997126684524119]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000997126684524119

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099713
Iteration 2/1000 | Loss: 0.00002993
Iteration 3/1000 | Loss: 0.00001890
Iteration 4/1000 | Loss: 0.00001653
Iteration 5/1000 | Loss: 0.00001521
Iteration 6/1000 | Loss: 0.00001420
Iteration 7/1000 | Loss: 0.00001357
Iteration 8/1000 | Loss: 0.00001319
Iteration 9/1000 | Loss: 0.00001288
Iteration 10/1000 | Loss: 0.00001257
Iteration 11/1000 | Loss: 0.00001236
Iteration 12/1000 | Loss: 0.00001218
Iteration 13/1000 | Loss: 0.00001216
Iteration 14/1000 | Loss: 0.00001216
Iteration 15/1000 | Loss: 0.00001213
Iteration 16/1000 | Loss: 0.00001211
Iteration 17/1000 | Loss: 0.00001209
Iteration 18/1000 | Loss: 0.00001208
Iteration 19/1000 | Loss: 0.00001207
Iteration 20/1000 | Loss: 0.00001206
Iteration 21/1000 | Loss: 0.00001202
Iteration 22/1000 | Loss: 0.00001198
Iteration 23/1000 | Loss: 0.00001197
Iteration 24/1000 | Loss: 0.00001197
Iteration 25/1000 | Loss: 0.00001196
Iteration 26/1000 | Loss: 0.00001196
Iteration 27/1000 | Loss: 0.00001195
Iteration 28/1000 | Loss: 0.00001195
Iteration 29/1000 | Loss: 0.00001194
Iteration 30/1000 | Loss: 0.00001193
Iteration 31/1000 | Loss: 0.00001192
Iteration 32/1000 | Loss: 0.00001192
Iteration 33/1000 | Loss: 0.00001192
Iteration 34/1000 | Loss: 0.00001191
Iteration 35/1000 | Loss: 0.00001191
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001189
Iteration 38/1000 | Loss: 0.00001189
Iteration 39/1000 | Loss: 0.00001188
Iteration 40/1000 | Loss: 0.00001188
Iteration 41/1000 | Loss: 0.00001188
Iteration 42/1000 | Loss: 0.00001187
Iteration 43/1000 | Loss: 0.00001187
Iteration 44/1000 | Loss: 0.00001186
Iteration 45/1000 | Loss: 0.00001186
Iteration 46/1000 | Loss: 0.00001186
Iteration 47/1000 | Loss: 0.00001185
Iteration 48/1000 | Loss: 0.00001185
Iteration 49/1000 | Loss: 0.00001184
Iteration 50/1000 | Loss: 0.00001184
Iteration 51/1000 | Loss: 0.00001184
Iteration 52/1000 | Loss: 0.00001184
Iteration 53/1000 | Loss: 0.00001184
Iteration 54/1000 | Loss: 0.00001183
Iteration 55/1000 | Loss: 0.00001183
Iteration 56/1000 | Loss: 0.00001183
Iteration 57/1000 | Loss: 0.00001183
Iteration 58/1000 | Loss: 0.00001181
Iteration 59/1000 | Loss: 0.00001180
Iteration 60/1000 | Loss: 0.00001180
Iteration 61/1000 | Loss: 0.00001179
Iteration 62/1000 | Loss: 0.00001178
Iteration 63/1000 | Loss: 0.00001178
Iteration 64/1000 | Loss: 0.00001177
Iteration 65/1000 | Loss: 0.00001177
Iteration 66/1000 | Loss: 0.00001177
Iteration 67/1000 | Loss: 0.00001177
Iteration 68/1000 | Loss: 0.00001176
Iteration 69/1000 | Loss: 0.00001176
Iteration 70/1000 | Loss: 0.00001176
Iteration 71/1000 | Loss: 0.00001175
Iteration 72/1000 | Loss: 0.00001175
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001173
Iteration 75/1000 | Loss: 0.00001173
Iteration 76/1000 | Loss: 0.00001172
Iteration 77/1000 | Loss: 0.00001172
Iteration 78/1000 | Loss: 0.00001172
Iteration 79/1000 | Loss: 0.00001171
Iteration 80/1000 | Loss: 0.00001171
Iteration 81/1000 | Loss: 0.00001170
Iteration 82/1000 | Loss: 0.00001170
Iteration 83/1000 | Loss: 0.00001170
Iteration 84/1000 | Loss: 0.00001170
Iteration 85/1000 | Loss: 0.00001170
Iteration 86/1000 | Loss: 0.00001169
Iteration 87/1000 | Loss: 0.00001169
Iteration 88/1000 | Loss: 0.00001169
Iteration 89/1000 | Loss: 0.00001168
Iteration 90/1000 | Loss: 0.00001168
Iteration 91/1000 | Loss: 0.00001168
Iteration 92/1000 | Loss: 0.00001167
Iteration 93/1000 | Loss: 0.00001167
Iteration 94/1000 | Loss: 0.00001167
Iteration 95/1000 | Loss: 0.00001166
Iteration 96/1000 | Loss: 0.00001166
Iteration 97/1000 | Loss: 0.00001166
Iteration 98/1000 | Loss: 0.00001166
Iteration 99/1000 | Loss: 0.00001166
Iteration 100/1000 | Loss: 0.00001166
Iteration 101/1000 | Loss: 0.00001166
Iteration 102/1000 | Loss: 0.00001165
Iteration 103/1000 | Loss: 0.00001165
Iteration 104/1000 | Loss: 0.00001165
Iteration 105/1000 | Loss: 0.00001164
Iteration 106/1000 | Loss: 0.00001164
Iteration 107/1000 | Loss: 0.00001164
Iteration 108/1000 | Loss: 0.00001164
Iteration 109/1000 | Loss: 0.00001164
Iteration 110/1000 | Loss: 0.00001164
Iteration 111/1000 | Loss: 0.00001164
Iteration 112/1000 | Loss: 0.00001163
Iteration 113/1000 | Loss: 0.00001163
Iteration 114/1000 | Loss: 0.00001163
Iteration 115/1000 | Loss: 0.00001163
Iteration 116/1000 | Loss: 0.00001163
Iteration 117/1000 | Loss: 0.00001163
Iteration 118/1000 | Loss: 0.00001163
Iteration 119/1000 | Loss: 0.00001163
Iteration 120/1000 | Loss: 0.00001162
Iteration 121/1000 | Loss: 0.00001162
Iteration 122/1000 | Loss: 0.00001162
Iteration 123/1000 | Loss: 0.00001162
Iteration 124/1000 | Loss: 0.00001162
Iteration 125/1000 | Loss: 0.00001162
Iteration 126/1000 | Loss: 0.00001162
Iteration 127/1000 | Loss: 0.00001162
Iteration 128/1000 | Loss: 0.00001162
Iteration 129/1000 | Loss: 0.00001161
Iteration 130/1000 | Loss: 0.00001161
Iteration 131/1000 | Loss: 0.00001161
Iteration 132/1000 | Loss: 0.00001161
Iteration 133/1000 | Loss: 0.00001161
Iteration 134/1000 | Loss: 0.00001160
Iteration 135/1000 | Loss: 0.00001160
Iteration 136/1000 | Loss: 0.00001160
Iteration 137/1000 | Loss: 0.00001159
Iteration 138/1000 | Loss: 0.00001159
Iteration 139/1000 | Loss: 0.00001158
Iteration 140/1000 | Loss: 0.00001158
Iteration 141/1000 | Loss: 0.00001158
Iteration 142/1000 | Loss: 0.00001156
Iteration 143/1000 | Loss: 0.00001156
Iteration 144/1000 | Loss: 0.00001156
Iteration 145/1000 | Loss: 0.00001156
Iteration 146/1000 | Loss: 0.00001156
Iteration 147/1000 | Loss: 0.00001156
Iteration 148/1000 | Loss: 0.00001156
Iteration 149/1000 | Loss: 0.00001156
Iteration 150/1000 | Loss: 0.00001155
Iteration 151/1000 | Loss: 0.00001155
Iteration 152/1000 | Loss: 0.00001155
Iteration 153/1000 | Loss: 0.00001155
Iteration 154/1000 | Loss: 0.00001155
Iteration 155/1000 | Loss: 0.00001155
Iteration 156/1000 | Loss: 0.00001155
Iteration 157/1000 | Loss: 0.00001155
Iteration 158/1000 | Loss: 0.00001155
Iteration 159/1000 | Loss: 0.00001155
Iteration 160/1000 | Loss: 0.00001155
Iteration 161/1000 | Loss: 0.00001154
Iteration 162/1000 | Loss: 0.00001154
Iteration 163/1000 | Loss: 0.00001154
Iteration 164/1000 | Loss: 0.00001154
Iteration 165/1000 | Loss: 0.00001154
Iteration 166/1000 | Loss: 0.00001154
Iteration 167/1000 | Loss: 0.00001154
Iteration 168/1000 | Loss: 0.00001154
Iteration 169/1000 | Loss: 0.00001153
Iteration 170/1000 | Loss: 0.00001153
Iteration 171/1000 | Loss: 0.00001153
Iteration 172/1000 | Loss: 0.00001153
Iteration 173/1000 | Loss: 0.00001153
Iteration 174/1000 | Loss: 0.00001153
Iteration 175/1000 | Loss: 0.00001153
Iteration 176/1000 | Loss: 0.00001153
Iteration 177/1000 | Loss: 0.00001152
Iteration 178/1000 | Loss: 0.00001152
Iteration 179/1000 | Loss: 0.00001152
Iteration 180/1000 | Loss: 0.00001152
Iteration 181/1000 | Loss: 0.00001152
Iteration 182/1000 | Loss: 0.00001152
Iteration 183/1000 | Loss: 0.00001152
Iteration 184/1000 | Loss: 0.00001152
Iteration 185/1000 | Loss: 0.00001152
Iteration 186/1000 | Loss: 0.00001151
Iteration 187/1000 | Loss: 0.00001151
Iteration 188/1000 | Loss: 0.00001151
Iteration 189/1000 | Loss: 0.00001151
Iteration 190/1000 | Loss: 0.00001151
Iteration 191/1000 | Loss: 0.00001151
Iteration 192/1000 | Loss: 0.00001151
Iteration 193/1000 | Loss: 0.00001151
Iteration 194/1000 | Loss: 0.00001151
Iteration 195/1000 | Loss: 0.00001151
Iteration 196/1000 | Loss: 0.00001151
Iteration 197/1000 | Loss: 0.00001151
Iteration 198/1000 | Loss: 0.00001150
Iteration 199/1000 | Loss: 0.00001150
Iteration 200/1000 | Loss: 0.00001150
Iteration 201/1000 | Loss: 0.00001150
Iteration 202/1000 | Loss: 0.00001150
Iteration 203/1000 | Loss: 0.00001150
Iteration 204/1000 | Loss: 0.00001150
Iteration 205/1000 | Loss: 0.00001150
Iteration 206/1000 | Loss: 0.00001150
Iteration 207/1000 | Loss: 0.00001150
Iteration 208/1000 | Loss: 0.00001150
Iteration 209/1000 | Loss: 0.00001150
Iteration 210/1000 | Loss: 0.00001149
Iteration 211/1000 | Loss: 0.00001149
Iteration 212/1000 | Loss: 0.00001149
Iteration 213/1000 | Loss: 0.00001149
Iteration 214/1000 | Loss: 0.00001149
Iteration 215/1000 | Loss: 0.00001149
Iteration 216/1000 | Loss: 0.00001149
Iteration 217/1000 | Loss: 0.00001149
Iteration 218/1000 | Loss: 0.00001149
Iteration 219/1000 | Loss: 0.00001148
Iteration 220/1000 | Loss: 0.00001148
Iteration 221/1000 | Loss: 0.00001148
Iteration 222/1000 | Loss: 0.00001148
Iteration 223/1000 | Loss: 0.00001148
Iteration 224/1000 | Loss: 0.00001148
Iteration 225/1000 | Loss: 0.00001148
Iteration 226/1000 | Loss: 0.00001148
Iteration 227/1000 | Loss: 0.00001148
Iteration 228/1000 | Loss: 0.00001147
Iteration 229/1000 | Loss: 0.00001147
Iteration 230/1000 | Loss: 0.00001147
Iteration 231/1000 | Loss: 0.00001147
Iteration 232/1000 | Loss: 0.00001147
Iteration 233/1000 | Loss: 0.00001147
Iteration 234/1000 | Loss: 0.00001147
Iteration 235/1000 | Loss: 0.00001147
Iteration 236/1000 | Loss: 0.00001147
Iteration 237/1000 | Loss: 0.00001147
Iteration 238/1000 | Loss: 0.00001146
Iteration 239/1000 | Loss: 0.00001146
Iteration 240/1000 | Loss: 0.00001146
Iteration 241/1000 | Loss: 0.00001146
Iteration 242/1000 | Loss: 0.00001146
Iteration 243/1000 | Loss: 0.00001146
Iteration 244/1000 | Loss: 0.00001146
Iteration 245/1000 | Loss: 0.00001146
Iteration 246/1000 | Loss: 0.00001146
Iteration 247/1000 | Loss: 0.00001146
Iteration 248/1000 | Loss: 0.00001146
Iteration 249/1000 | Loss: 0.00001146
Iteration 250/1000 | Loss: 0.00001146
Iteration 251/1000 | Loss: 0.00001146
Iteration 252/1000 | Loss: 0.00001146
Iteration 253/1000 | Loss: 0.00001146
Iteration 254/1000 | Loss: 0.00001146
Iteration 255/1000 | Loss: 0.00001146
Iteration 256/1000 | Loss: 0.00001146
Iteration 257/1000 | Loss: 0.00001146
Iteration 258/1000 | Loss: 0.00001146
Iteration 259/1000 | Loss: 0.00001146
Iteration 260/1000 | Loss: 0.00001146
Iteration 261/1000 | Loss: 0.00001146
Iteration 262/1000 | Loss: 0.00001146
Iteration 263/1000 | Loss: 0.00001146
Iteration 264/1000 | Loss: 0.00001146
Iteration 265/1000 | Loss: 0.00001146
Iteration 266/1000 | Loss: 0.00001146
Iteration 267/1000 | Loss: 0.00001146
Iteration 268/1000 | Loss: 0.00001146
Iteration 269/1000 | Loss: 0.00001146
Iteration 270/1000 | Loss: 0.00001146
Iteration 271/1000 | Loss: 0.00001146
Iteration 272/1000 | Loss: 0.00001146
Iteration 273/1000 | Loss: 0.00001146
Iteration 274/1000 | Loss: 0.00001146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 274. Stopping optimization.
Last 5 losses: [1.1461743270047009e-05, 1.1461743270047009e-05, 1.1461743270047009e-05, 1.1461743270047009e-05, 1.1461743270047009e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1461743270047009e-05

Optimization complete. Final v2v error: 2.907090902328491 mm

Highest mean error: 3.542093515396118 mm for frame 76

Lowest mean error: 2.682500123977661 mm for frame 150

Saving results

Total time: 46.17922616004944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410350
Iteration 2/25 | Loss: 0.00128736
Iteration 3/25 | Loss: 0.00122530
Iteration 4/25 | Loss: 0.00121660
Iteration 5/25 | Loss: 0.00121299
Iteration 6/25 | Loss: 0.00121248
Iteration 7/25 | Loss: 0.00121248
Iteration 8/25 | Loss: 0.00121248
Iteration 9/25 | Loss: 0.00121248
Iteration 10/25 | Loss: 0.00121248
Iteration 11/25 | Loss: 0.00121248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012124840868636966, 0.0012124840868636966, 0.0012124840868636966, 0.0012124840868636966, 0.0012124840868636966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012124840868636966

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39454639
Iteration 2/25 | Loss: 0.00126049
Iteration 3/25 | Loss: 0.00126049
Iteration 4/25 | Loss: 0.00126049
Iteration 5/25 | Loss: 0.00126049
Iteration 6/25 | Loss: 0.00126049
Iteration 7/25 | Loss: 0.00126049
Iteration 8/25 | Loss: 0.00126049
Iteration 9/25 | Loss: 0.00126049
Iteration 10/25 | Loss: 0.00126049
Iteration 11/25 | Loss: 0.00126049
Iteration 12/25 | Loss: 0.00126049
Iteration 13/25 | Loss: 0.00126049
Iteration 14/25 | Loss: 0.00126049
Iteration 15/25 | Loss: 0.00126049
Iteration 16/25 | Loss: 0.00126049
Iteration 17/25 | Loss: 0.00126049
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012604888761416078, 0.0012604888761416078, 0.0012604888761416078, 0.0012604888761416078, 0.0012604888761416078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012604888761416078

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126049
Iteration 2/1000 | Loss: 0.00002687
Iteration 3/1000 | Loss: 0.00001787
Iteration 4/1000 | Loss: 0.00001526
Iteration 5/1000 | Loss: 0.00001428
Iteration 6/1000 | Loss: 0.00001381
Iteration 7/1000 | Loss: 0.00001352
Iteration 8/1000 | Loss: 0.00001332
Iteration 9/1000 | Loss: 0.00001312
Iteration 10/1000 | Loss: 0.00001290
Iteration 11/1000 | Loss: 0.00001270
Iteration 12/1000 | Loss: 0.00001267
Iteration 13/1000 | Loss: 0.00001265
Iteration 14/1000 | Loss: 0.00001264
Iteration 15/1000 | Loss: 0.00001262
Iteration 16/1000 | Loss: 0.00001261
Iteration 17/1000 | Loss: 0.00001260
Iteration 18/1000 | Loss: 0.00001259
Iteration 19/1000 | Loss: 0.00001259
Iteration 20/1000 | Loss: 0.00001257
Iteration 21/1000 | Loss: 0.00001256
Iteration 22/1000 | Loss: 0.00001251
Iteration 23/1000 | Loss: 0.00001250
Iteration 24/1000 | Loss: 0.00001250
Iteration 25/1000 | Loss: 0.00001249
Iteration 26/1000 | Loss: 0.00001249
Iteration 27/1000 | Loss: 0.00001248
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001247
Iteration 31/1000 | Loss: 0.00001247
Iteration 32/1000 | Loss: 0.00001246
Iteration 33/1000 | Loss: 0.00001246
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001245
Iteration 36/1000 | Loss: 0.00001245
Iteration 37/1000 | Loss: 0.00001244
Iteration 38/1000 | Loss: 0.00001244
Iteration 39/1000 | Loss: 0.00001244
Iteration 40/1000 | Loss: 0.00001243
Iteration 41/1000 | Loss: 0.00001243
Iteration 42/1000 | Loss: 0.00001243
Iteration 43/1000 | Loss: 0.00001243
Iteration 44/1000 | Loss: 0.00001243
Iteration 45/1000 | Loss: 0.00001242
Iteration 46/1000 | Loss: 0.00001242
Iteration 47/1000 | Loss: 0.00001242
Iteration 48/1000 | Loss: 0.00001242
Iteration 49/1000 | Loss: 0.00001241
Iteration 50/1000 | Loss: 0.00001241
Iteration 51/1000 | Loss: 0.00001241
Iteration 52/1000 | Loss: 0.00001241
Iteration 53/1000 | Loss: 0.00001241
Iteration 54/1000 | Loss: 0.00001241
Iteration 55/1000 | Loss: 0.00001241
Iteration 56/1000 | Loss: 0.00001241
Iteration 57/1000 | Loss: 0.00001240
Iteration 58/1000 | Loss: 0.00001240
Iteration 59/1000 | Loss: 0.00001240
Iteration 60/1000 | Loss: 0.00001240
Iteration 61/1000 | Loss: 0.00001239
Iteration 62/1000 | Loss: 0.00001239
Iteration 63/1000 | Loss: 0.00001238
Iteration 64/1000 | Loss: 0.00001238
Iteration 65/1000 | Loss: 0.00001238
Iteration 66/1000 | Loss: 0.00001238
Iteration 67/1000 | Loss: 0.00001238
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001237
Iteration 70/1000 | Loss: 0.00001237
Iteration 71/1000 | Loss: 0.00001237
Iteration 72/1000 | Loss: 0.00001237
Iteration 73/1000 | Loss: 0.00001237
Iteration 74/1000 | Loss: 0.00001237
Iteration 75/1000 | Loss: 0.00001237
Iteration 76/1000 | Loss: 0.00001237
Iteration 77/1000 | Loss: 0.00001237
Iteration 78/1000 | Loss: 0.00001237
Iteration 79/1000 | Loss: 0.00001237
Iteration 80/1000 | Loss: 0.00001236
Iteration 81/1000 | Loss: 0.00001236
Iteration 82/1000 | Loss: 0.00001236
Iteration 83/1000 | Loss: 0.00001236
Iteration 84/1000 | Loss: 0.00001236
Iteration 85/1000 | Loss: 0.00001236
Iteration 86/1000 | Loss: 0.00001236
Iteration 87/1000 | Loss: 0.00001236
Iteration 88/1000 | Loss: 0.00001234
Iteration 89/1000 | Loss: 0.00001234
Iteration 90/1000 | Loss: 0.00001234
Iteration 91/1000 | Loss: 0.00001234
Iteration 92/1000 | Loss: 0.00001233
Iteration 93/1000 | Loss: 0.00001233
Iteration 94/1000 | Loss: 0.00001233
Iteration 95/1000 | Loss: 0.00001233
Iteration 96/1000 | Loss: 0.00001233
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001232
Iteration 99/1000 | Loss: 0.00001232
Iteration 100/1000 | Loss: 0.00001232
Iteration 101/1000 | Loss: 0.00001232
Iteration 102/1000 | Loss: 0.00001232
Iteration 103/1000 | Loss: 0.00001232
Iteration 104/1000 | Loss: 0.00001232
Iteration 105/1000 | Loss: 0.00001231
Iteration 106/1000 | Loss: 0.00001231
Iteration 107/1000 | Loss: 0.00001231
Iteration 108/1000 | Loss: 0.00001231
Iteration 109/1000 | Loss: 0.00001231
Iteration 110/1000 | Loss: 0.00001231
Iteration 111/1000 | Loss: 0.00001230
Iteration 112/1000 | Loss: 0.00001230
Iteration 113/1000 | Loss: 0.00001230
Iteration 114/1000 | Loss: 0.00001230
Iteration 115/1000 | Loss: 0.00001230
Iteration 116/1000 | Loss: 0.00001230
Iteration 117/1000 | Loss: 0.00001229
Iteration 118/1000 | Loss: 0.00001229
Iteration 119/1000 | Loss: 0.00001229
Iteration 120/1000 | Loss: 0.00001229
Iteration 121/1000 | Loss: 0.00001229
Iteration 122/1000 | Loss: 0.00001228
Iteration 123/1000 | Loss: 0.00001228
Iteration 124/1000 | Loss: 0.00001228
Iteration 125/1000 | Loss: 0.00001227
Iteration 126/1000 | Loss: 0.00001227
Iteration 127/1000 | Loss: 0.00001227
Iteration 128/1000 | Loss: 0.00001227
Iteration 129/1000 | Loss: 0.00001226
Iteration 130/1000 | Loss: 0.00001226
Iteration 131/1000 | Loss: 0.00001226
Iteration 132/1000 | Loss: 0.00001226
Iteration 133/1000 | Loss: 0.00001225
Iteration 134/1000 | Loss: 0.00001225
Iteration 135/1000 | Loss: 0.00001225
Iteration 136/1000 | Loss: 0.00001225
Iteration 137/1000 | Loss: 0.00001224
Iteration 138/1000 | Loss: 0.00001224
Iteration 139/1000 | Loss: 0.00001224
Iteration 140/1000 | Loss: 0.00001223
Iteration 141/1000 | Loss: 0.00001223
Iteration 142/1000 | Loss: 0.00001223
Iteration 143/1000 | Loss: 0.00001223
Iteration 144/1000 | Loss: 0.00001223
Iteration 145/1000 | Loss: 0.00001222
Iteration 146/1000 | Loss: 0.00001222
Iteration 147/1000 | Loss: 0.00001222
Iteration 148/1000 | Loss: 0.00001222
Iteration 149/1000 | Loss: 0.00001222
Iteration 150/1000 | Loss: 0.00001221
Iteration 151/1000 | Loss: 0.00001221
Iteration 152/1000 | Loss: 0.00001221
Iteration 153/1000 | Loss: 0.00001221
Iteration 154/1000 | Loss: 0.00001221
Iteration 155/1000 | Loss: 0.00001221
Iteration 156/1000 | Loss: 0.00001221
Iteration 157/1000 | Loss: 0.00001221
Iteration 158/1000 | Loss: 0.00001221
Iteration 159/1000 | Loss: 0.00001221
Iteration 160/1000 | Loss: 0.00001221
Iteration 161/1000 | Loss: 0.00001221
Iteration 162/1000 | Loss: 0.00001221
Iteration 163/1000 | Loss: 0.00001221
Iteration 164/1000 | Loss: 0.00001221
Iteration 165/1000 | Loss: 0.00001221
Iteration 166/1000 | Loss: 0.00001221
Iteration 167/1000 | Loss: 0.00001221
Iteration 168/1000 | Loss: 0.00001221
Iteration 169/1000 | Loss: 0.00001221
Iteration 170/1000 | Loss: 0.00001221
Iteration 171/1000 | Loss: 0.00001221
Iteration 172/1000 | Loss: 0.00001221
Iteration 173/1000 | Loss: 0.00001221
Iteration 174/1000 | Loss: 0.00001221
Iteration 175/1000 | Loss: 0.00001221
Iteration 176/1000 | Loss: 0.00001221
Iteration 177/1000 | Loss: 0.00001221
Iteration 178/1000 | Loss: 0.00001221
Iteration 179/1000 | Loss: 0.00001221
Iteration 180/1000 | Loss: 0.00001221
Iteration 181/1000 | Loss: 0.00001221
Iteration 182/1000 | Loss: 0.00001221
Iteration 183/1000 | Loss: 0.00001221
Iteration 184/1000 | Loss: 0.00001221
Iteration 185/1000 | Loss: 0.00001221
Iteration 186/1000 | Loss: 0.00001221
Iteration 187/1000 | Loss: 0.00001221
Iteration 188/1000 | Loss: 0.00001221
Iteration 189/1000 | Loss: 0.00001221
Iteration 190/1000 | Loss: 0.00001221
Iteration 191/1000 | Loss: 0.00001221
Iteration 192/1000 | Loss: 0.00001221
Iteration 193/1000 | Loss: 0.00001221
Iteration 194/1000 | Loss: 0.00001221
Iteration 195/1000 | Loss: 0.00001221
Iteration 196/1000 | Loss: 0.00001221
Iteration 197/1000 | Loss: 0.00001221
Iteration 198/1000 | Loss: 0.00001221
Iteration 199/1000 | Loss: 0.00001221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.22063947856077e-05, 1.22063947856077e-05, 1.22063947856077e-05, 1.22063947856077e-05, 1.22063947856077e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.22063947856077e-05

Optimization complete. Final v2v error: 2.961033344268799 mm

Highest mean error: 3.070652961730957 mm for frame 9

Lowest mean error: 2.8396453857421875 mm for frame 78

Saving results

Total time: 36.21090078353882
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816632
Iteration 2/25 | Loss: 0.00131778
Iteration 3/25 | Loss: 0.00124769
Iteration 4/25 | Loss: 0.00123682
Iteration 5/25 | Loss: 0.00123292
Iteration 6/25 | Loss: 0.00123292
Iteration 7/25 | Loss: 0.00123292
Iteration 8/25 | Loss: 0.00123292
Iteration 9/25 | Loss: 0.00123292
Iteration 10/25 | Loss: 0.00123292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012329218443483114, 0.0012329218443483114, 0.0012329218443483114, 0.0012329218443483114, 0.0012329218443483114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012329218443483114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43334889
Iteration 2/25 | Loss: 0.00089148
Iteration 3/25 | Loss: 0.00089147
Iteration 4/25 | Loss: 0.00089147
Iteration 5/25 | Loss: 0.00089147
Iteration 6/25 | Loss: 0.00089147
Iteration 7/25 | Loss: 0.00089147
Iteration 8/25 | Loss: 0.00089147
Iteration 9/25 | Loss: 0.00089147
Iteration 10/25 | Loss: 0.00089147
Iteration 11/25 | Loss: 0.00089147
Iteration 12/25 | Loss: 0.00089147
Iteration 13/25 | Loss: 0.00089147
Iteration 14/25 | Loss: 0.00089147
Iteration 15/25 | Loss: 0.00089147
Iteration 16/25 | Loss: 0.00089147
Iteration 17/25 | Loss: 0.00089147
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008914701174944639, 0.0008914701174944639, 0.0008914701174944639, 0.0008914701174944639, 0.0008914701174944639]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008914701174944639

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089147
Iteration 2/1000 | Loss: 0.00002631
Iteration 3/1000 | Loss: 0.00001981
Iteration 4/1000 | Loss: 0.00001821
Iteration 5/1000 | Loss: 0.00001728
Iteration 6/1000 | Loss: 0.00001643
Iteration 7/1000 | Loss: 0.00001601
Iteration 8/1000 | Loss: 0.00001574
Iteration 9/1000 | Loss: 0.00001539
Iteration 10/1000 | Loss: 0.00001518
Iteration 11/1000 | Loss: 0.00001518
Iteration 12/1000 | Loss: 0.00001511
Iteration 13/1000 | Loss: 0.00001506
Iteration 14/1000 | Loss: 0.00001505
Iteration 15/1000 | Loss: 0.00001496
Iteration 16/1000 | Loss: 0.00001491
Iteration 17/1000 | Loss: 0.00001490
Iteration 18/1000 | Loss: 0.00001487
Iteration 19/1000 | Loss: 0.00001486
Iteration 20/1000 | Loss: 0.00001483
Iteration 21/1000 | Loss: 0.00001478
Iteration 22/1000 | Loss: 0.00001477
Iteration 23/1000 | Loss: 0.00001477
Iteration 24/1000 | Loss: 0.00001476
Iteration 25/1000 | Loss: 0.00001476
Iteration 26/1000 | Loss: 0.00001475
Iteration 27/1000 | Loss: 0.00001475
Iteration 28/1000 | Loss: 0.00001471
Iteration 29/1000 | Loss: 0.00001471
Iteration 30/1000 | Loss: 0.00001470
Iteration 31/1000 | Loss: 0.00001469
Iteration 32/1000 | Loss: 0.00001469
Iteration 33/1000 | Loss: 0.00001464
Iteration 34/1000 | Loss: 0.00001463
Iteration 35/1000 | Loss: 0.00001462
Iteration 36/1000 | Loss: 0.00001462
Iteration 37/1000 | Loss: 0.00001460
Iteration 38/1000 | Loss: 0.00001460
Iteration 39/1000 | Loss: 0.00001460
Iteration 40/1000 | Loss: 0.00001459
Iteration 41/1000 | Loss: 0.00001458
Iteration 42/1000 | Loss: 0.00001458
Iteration 43/1000 | Loss: 0.00001458
Iteration 44/1000 | Loss: 0.00001458
Iteration 45/1000 | Loss: 0.00001457
Iteration 46/1000 | Loss: 0.00001457
Iteration 47/1000 | Loss: 0.00001456
Iteration 48/1000 | Loss: 0.00001453
Iteration 49/1000 | Loss: 0.00001452
Iteration 50/1000 | Loss: 0.00001446
Iteration 51/1000 | Loss: 0.00001446
Iteration 52/1000 | Loss: 0.00001446
Iteration 53/1000 | Loss: 0.00001446
Iteration 54/1000 | Loss: 0.00001446
Iteration 55/1000 | Loss: 0.00001445
Iteration 56/1000 | Loss: 0.00001445
Iteration 57/1000 | Loss: 0.00001445
Iteration 58/1000 | Loss: 0.00001445
Iteration 59/1000 | Loss: 0.00001445
Iteration 60/1000 | Loss: 0.00001445
Iteration 61/1000 | Loss: 0.00001445
Iteration 62/1000 | Loss: 0.00001445
Iteration 63/1000 | Loss: 0.00001445
Iteration 64/1000 | Loss: 0.00001445
Iteration 65/1000 | Loss: 0.00001445
Iteration 66/1000 | Loss: 0.00001445
Iteration 67/1000 | Loss: 0.00001444
Iteration 68/1000 | Loss: 0.00001444
Iteration 69/1000 | Loss: 0.00001444
Iteration 70/1000 | Loss: 0.00001443
Iteration 71/1000 | Loss: 0.00001443
Iteration 72/1000 | Loss: 0.00001443
Iteration 73/1000 | Loss: 0.00001443
Iteration 74/1000 | Loss: 0.00001442
Iteration 75/1000 | Loss: 0.00001442
Iteration 76/1000 | Loss: 0.00001442
Iteration 77/1000 | Loss: 0.00001442
Iteration 78/1000 | Loss: 0.00001442
Iteration 79/1000 | Loss: 0.00001441
Iteration 80/1000 | Loss: 0.00001441
Iteration 81/1000 | Loss: 0.00001440
Iteration 82/1000 | Loss: 0.00001440
Iteration 83/1000 | Loss: 0.00001439
Iteration 84/1000 | Loss: 0.00001439
Iteration 85/1000 | Loss: 0.00001438
Iteration 86/1000 | Loss: 0.00001438
Iteration 87/1000 | Loss: 0.00001437
Iteration 88/1000 | Loss: 0.00001437
Iteration 89/1000 | Loss: 0.00001437
Iteration 90/1000 | Loss: 0.00001436
Iteration 91/1000 | Loss: 0.00001436
Iteration 92/1000 | Loss: 0.00001435
Iteration 93/1000 | Loss: 0.00001435
Iteration 94/1000 | Loss: 0.00001435
Iteration 95/1000 | Loss: 0.00001435
Iteration 96/1000 | Loss: 0.00001434
Iteration 97/1000 | Loss: 0.00001434
Iteration 98/1000 | Loss: 0.00001433
Iteration 99/1000 | Loss: 0.00001433
Iteration 100/1000 | Loss: 0.00001432
Iteration 101/1000 | Loss: 0.00001432
Iteration 102/1000 | Loss: 0.00001432
Iteration 103/1000 | Loss: 0.00001432
Iteration 104/1000 | Loss: 0.00001431
Iteration 105/1000 | Loss: 0.00001431
Iteration 106/1000 | Loss: 0.00001431
Iteration 107/1000 | Loss: 0.00001431
Iteration 108/1000 | Loss: 0.00001431
Iteration 109/1000 | Loss: 0.00001431
Iteration 110/1000 | Loss: 0.00001430
Iteration 111/1000 | Loss: 0.00001430
Iteration 112/1000 | Loss: 0.00001430
Iteration 113/1000 | Loss: 0.00001430
Iteration 114/1000 | Loss: 0.00001429
Iteration 115/1000 | Loss: 0.00001429
Iteration 116/1000 | Loss: 0.00001429
Iteration 117/1000 | Loss: 0.00001429
Iteration 118/1000 | Loss: 0.00001429
Iteration 119/1000 | Loss: 0.00001429
Iteration 120/1000 | Loss: 0.00001429
Iteration 121/1000 | Loss: 0.00001429
Iteration 122/1000 | Loss: 0.00001428
Iteration 123/1000 | Loss: 0.00001428
Iteration 124/1000 | Loss: 0.00001428
Iteration 125/1000 | Loss: 0.00001428
Iteration 126/1000 | Loss: 0.00001428
Iteration 127/1000 | Loss: 0.00001428
Iteration 128/1000 | Loss: 0.00001428
Iteration 129/1000 | Loss: 0.00001428
Iteration 130/1000 | Loss: 0.00001428
Iteration 131/1000 | Loss: 0.00001428
Iteration 132/1000 | Loss: 0.00001428
Iteration 133/1000 | Loss: 0.00001428
Iteration 134/1000 | Loss: 0.00001427
Iteration 135/1000 | Loss: 0.00001427
Iteration 136/1000 | Loss: 0.00001427
Iteration 137/1000 | Loss: 0.00001427
Iteration 138/1000 | Loss: 0.00001427
Iteration 139/1000 | Loss: 0.00001426
Iteration 140/1000 | Loss: 0.00001426
Iteration 141/1000 | Loss: 0.00001426
Iteration 142/1000 | Loss: 0.00001426
Iteration 143/1000 | Loss: 0.00001426
Iteration 144/1000 | Loss: 0.00001426
Iteration 145/1000 | Loss: 0.00001426
Iteration 146/1000 | Loss: 0.00001426
Iteration 147/1000 | Loss: 0.00001426
Iteration 148/1000 | Loss: 0.00001426
Iteration 149/1000 | Loss: 0.00001426
Iteration 150/1000 | Loss: 0.00001426
Iteration 151/1000 | Loss: 0.00001426
Iteration 152/1000 | Loss: 0.00001426
Iteration 153/1000 | Loss: 0.00001426
Iteration 154/1000 | Loss: 0.00001426
Iteration 155/1000 | Loss: 0.00001426
Iteration 156/1000 | Loss: 0.00001426
Iteration 157/1000 | Loss: 0.00001426
Iteration 158/1000 | Loss: 0.00001426
Iteration 159/1000 | Loss: 0.00001426
Iteration 160/1000 | Loss: 0.00001426
Iteration 161/1000 | Loss: 0.00001426
Iteration 162/1000 | Loss: 0.00001426
Iteration 163/1000 | Loss: 0.00001426
Iteration 164/1000 | Loss: 0.00001426
Iteration 165/1000 | Loss: 0.00001426
Iteration 166/1000 | Loss: 0.00001426
Iteration 167/1000 | Loss: 0.00001426
Iteration 168/1000 | Loss: 0.00001426
Iteration 169/1000 | Loss: 0.00001426
Iteration 170/1000 | Loss: 0.00001426
Iteration 171/1000 | Loss: 0.00001426
Iteration 172/1000 | Loss: 0.00001426
Iteration 173/1000 | Loss: 0.00001426
Iteration 174/1000 | Loss: 0.00001426
Iteration 175/1000 | Loss: 0.00001426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.4258532246458344e-05, 1.4258532246458344e-05, 1.4258532246458344e-05, 1.4258532246458344e-05, 1.4258532246458344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4258532246458344e-05

Optimization complete. Final v2v error: 3.2020645141601562 mm

Highest mean error: 3.5618324279785156 mm for frame 176

Lowest mean error: 2.8834612369537354 mm for frame 10

Saving results

Total time: 39.48578238487244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051706
Iteration 2/25 | Loss: 0.01051706
Iteration 3/25 | Loss: 0.01051706
Iteration 4/25 | Loss: 0.00241703
Iteration 5/25 | Loss: 0.00178456
Iteration 6/25 | Loss: 0.00137701
Iteration 7/25 | Loss: 0.00133750
Iteration 8/25 | Loss: 0.00131124
Iteration 9/25 | Loss: 0.00130645
Iteration 10/25 | Loss: 0.00130094
Iteration 11/25 | Loss: 0.00129490
Iteration 12/25 | Loss: 0.00129448
Iteration 13/25 | Loss: 0.00129448
Iteration 14/25 | Loss: 0.00129395
Iteration 15/25 | Loss: 0.00129394
Iteration 16/25 | Loss: 0.00129394
Iteration 17/25 | Loss: 0.00129394
Iteration 18/25 | Loss: 0.00129394
Iteration 19/25 | Loss: 0.00129394
Iteration 20/25 | Loss: 0.00129394
Iteration 21/25 | Loss: 0.00129394
Iteration 22/25 | Loss: 0.00129394
Iteration 23/25 | Loss: 0.00129394
Iteration 24/25 | Loss: 0.00129393
Iteration 25/25 | Loss: 0.00129393

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33837807
Iteration 2/25 | Loss: 0.00133337
Iteration 3/25 | Loss: 0.00131118
Iteration 4/25 | Loss: 0.00131118
Iteration 5/25 | Loss: 0.00131118
Iteration 6/25 | Loss: 0.00131117
Iteration 7/25 | Loss: 0.00131117
Iteration 8/25 | Loss: 0.00131117
Iteration 9/25 | Loss: 0.00131117
Iteration 10/25 | Loss: 0.00131117
Iteration 11/25 | Loss: 0.00131117
Iteration 12/25 | Loss: 0.00131117
Iteration 13/25 | Loss: 0.00131117
Iteration 14/25 | Loss: 0.00131117
Iteration 15/25 | Loss: 0.00131117
Iteration 16/25 | Loss: 0.00131117
Iteration 17/25 | Loss: 0.00131117
Iteration 18/25 | Loss: 0.00131117
Iteration 19/25 | Loss: 0.00131117
Iteration 20/25 | Loss: 0.00131117
Iteration 21/25 | Loss: 0.00131117
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013111737789586186, 0.0013111737789586186, 0.0013111737789586186, 0.0013111737789586186, 0.0013111737789586186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013111737789586186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131117
Iteration 2/1000 | Loss: 0.00010009
Iteration 3/1000 | Loss: 0.00006178
Iteration 4/1000 | Loss: 0.00004356
Iteration 5/1000 | Loss: 0.00003396
Iteration 6/1000 | Loss: 0.00004928
Iteration 7/1000 | Loss: 0.00003139
Iteration 8/1000 | Loss: 0.00003077
Iteration 9/1000 | Loss: 0.00003004
Iteration 10/1000 | Loss: 0.00002967
Iteration 11/1000 | Loss: 0.00019336
Iteration 12/1000 | Loss: 0.00009816
Iteration 13/1000 | Loss: 0.00003044
Iteration 14/1000 | Loss: 0.00002909
Iteration 15/1000 | Loss: 0.00003324
Iteration 16/1000 | Loss: 0.00017031
Iteration 17/1000 | Loss: 0.00017031
Iteration 18/1000 | Loss: 0.00017830
Iteration 19/1000 | Loss: 0.00003839
Iteration 20/1000 | Loss: 0.00002911
Iteration 21/1000 | Loss: 0.00003013
Iteration 22/1000 | Loss: 0.00002870
Iteration 23/1000 | Loss: 0.00002927
Iteration 24/1000 | Loss: 0.00018439
Iteration 25/1000 | Loss: 0.00003121
Iteration 26/1000 | Loss: 0.00005701
Iteration 27/1000 | Loss: 0.00003553
Iteration 28/1000 | Loss: 0.00002860
Iteration 29/1000 | Loss: 0.00002845
Iteration 30/1000 | Loss: 0.00003367
Iteration 31/1000 | Loss: 0.00003170
Iteration 32/1000 | Loss: 0.00002825
Iteration 33/1000 | Loss: 0.00002824
Iteration 34/1000 | Loss: 0.00002823
Iteration 35/1000 | Loss: 0.00002821
Iteration 36/1000 | Loss: 0.00002820
Iteration 37/1000 | Loss: 0.00002819
Iteration 38/1000 | Loss: 0.00002815
Iteration 39/1000 | Loss: 0.00002815
Iteration 40/1000 | Loss: 0.00002815
Iteration 41/1000 | Loss: 0.00002814
Iteration 42/1000 | Loss: 0.00002814
Iteration 43/1000 | Loss: 0.00002813
Iteration 44/1000 | Loss: 0.00002813
Iteration 45/1000 | Loss: 0.00002813
Iteration 46/1000 | Loss: 0.00002812
Iteration 47/1000 | Loss: 0.00002812
Iteration 48/1000 | Loss: 0.00002812
Iteration 49/1000 | Loss: 0.00002811
Iteration 50/1000 | Loss: 0.00002811
Iteration 51/1000 | Loss: 0.00002811
Iteration 52/1000 | Loss: 0.00002810
Iteration 53/1000 | Loss: 0.00002810
Iteration 54/1000 | Loss: 0.00002808
Iteration 55/1000 | Loss: 0.00002804
Iteration 56/1000 | Loss: 0.00002804
Iteration 57/1000 | Loss: 0.00002801
Iteration 58/1000 | Loss: 0.00002801
Iteration 59/1000 | Loss: 0.00002800
Iteration 60/1000 | Loss: 0.00002800
Iteration 61/1000 | Loss: 0.00002799
Iteration 62/1000 | Loss: 0.00002799
Iteration 63/1000 | Loss: 0.00002798
Iteration 64/1000 | Loss: 0.00002798
Iteration 65/1000 | Loss: 0.00002797
Iteration 66/1000 | Loss: 0.00002797
Iteration 67/1000 | Loss: 0.00002797
Iteration 68/1000 | Loss: 0.00002797
Iteration 69/1000 | Loss: 0.00002797
Iteration 70/1000 | Loss: 0.00002797
Iteration 71/1000 | Loss: 0.00002796
Iteration 72/1000 | Loss: 0.00002796
Iteration 73/1000 | Loss: 0.00002794
Iteration 74/1000 | Loss: 0.00002794
Iteration 75/1000 | Loss: 0.00002793
Iteration 76/1000 | Loss: 0.00002793
Iteration 77/1000 | Loss: 0.00002793
Iteration 78/1000 | Loss: 0.00002793
Iteration 79/1000 | Loss: 0.00002793
Iteration 80/1000 | Loss: 0.00002793
Iteration 81/1000 | Loss: 0.00002793
Iteration 82/1000 | Loss: 0.00002793
Iteration 83/1000 | Loss: 0.00002793
Iteration 84/1000 | Loss: 0.00002793
Iteration 85/1000 | Loss: 0.00002793
Iteration 86/1000 | Loss: 0.00002793
Iteration 87/1000 | Loss: 0.00002792
Iteration 88/1000 | Loss: 0.00002792
Iteration 89/1000 | Loss: 0.00002792
Iteration 90/1000 | Loss: 0.00002792
Iteration 91/1000 | Loss: 0.00002792
Iteration 92/1000 | Loss: 0.00002792
Iteration 93/1000 | Loss: 0.00002792
Iteration 94/1000 | Loss: 0.00002792
Iteration 95/1000 | Loss: 0.00002792
Iteration 96/1000 | Loss: 0.00002792
Iteration 97/1000 | Loss: 0.00002792
Iteration 98/1000 | Loss: 0.00002792
Iteration 99/1000 | Loss: 0.00002792
Iteration 100/1000 | Loss: 0.00002791
Iteration 101/1000 | Loss: 0.00002791
Iteration 102/1000 | Loss: 0.00002791
Iteration 103/1000 | Loss: 0.00002791
Iteration 104/1000 | Loss: 0.00002791
Iteration 105/1000 | Loss: 0.00002791
Iteration 106/1000 | Loss: 0.00002791
Iteration 107/1000 | Loss: 0.00002791
Iteration 108/1000 | Loss: 0.00002791
Iteration 109/1000 | Loss: 0.00002791
Iteration 110/1000 | Loss: 0.00002790
Iteration 111/1000 | Loss: 0.00002790
Iteration 112/1000 | Loss: 0.00002790
Iteration 113/1000 | Loss: 0.00002790
Iteration 114/1000 | Loss: 0.00002790
Iteration 115/1000 | Loss: 0.00002790
Iteration 116/1000 | Loss: 0.00002790
Iteration 117/1000 | Loss: 0.00002790
Iteration 118/1000 | Loss: 0.00002790
Iteration 119/1000 | Loss: 0.00002790
Iteration 120/1000 | Loss: 0.00002789
Iteration 121/1000 | Loss: 0.00002789
Iteration 122/1000 | Loss: 0.00002789
Iteration 123/1000 | Loss: 0.00002789
Iteration 124/1000 | Loss: 0.00002789
Iteration 125/1000 | Loss: 0.00002789
Iteration 126/1000 | Loss: 0.00002789
Iteration 127/1000 | Loss: 0.00002789
Iteration 128/1000 | Loss: 0.00002789
Iteration 129/1000 | Loss: 0.00002788
Iteration 130/1000 | Loss: 0.00002788
Iteration 131/1000 | Loss: 0.00002788
Iteration 132/1000 | Loss: 0.00002788
Iteration 133/1000 | Loss: 0.00002788
Iteration 134/1000 | Loss: 0.00021643
Iteration 135/1000 | Loss: 0.00002808
Iteration 136/1000 | Loss: 0.00002792
Iteration 137/1000 | Loss: 0.00002792
Iteration 138/1000 | Loss: 0.00002791
Iteration 139/1000 | Loss: 0.00002791
Iteration 140/1000 | Loss: 0.00002791
Iteration 141/1000 | Loss: 0.00002786
Iteration 142/1000 | Loss: 0.00002786
Iteration 143/1000 | Loss: 0.00002785
Iteration 144/1000 | Loss: 0.00002785
Iteration 145/1000 | Loss: 0.00002784
Iteration 146/1000 | Loss: 0.00002784
Iteration 147/1000 | Loss: 0.00002784
Iteration 148/1000 | Loss: 0.00002784
Iteration 149/1000 | Loss: 0.00002784
Iteration 150/1000 | Loss: 0.00002783
Iteration 151/1000 | Loss: 0.00002783
Iteration 152/1000 | Loss: 0.00002783
Iteration 153/1000 | Loss: 0.00002783
Iteration 154/1000 | Loss: 0.00002783
Iteration 155/1000 | Loss: 0.00002783
Iteration 156/1000 | Loss: 0.00002783
Iteration 157/1000 | Loss: 0.00002783
Iteration 158/1000 | Loss: 0.00002783
Iteration 159/1000 | Loss: 0.00002783
Iteration 160/1000 | Loss: 0.00002783
Iteration 161/1000 | Loss: 0.00002783
Iteration 162/1000 | Loss: 0.00002783
Iteration 163/1000 | Loss: 0.00002783
Iteration 164/1000 | Loss: 0.00002783
Iteration 165/1000 | Loss: 0.00002783
Iteration 166/1000 | Loss: 0.00002783
Iteration 167/1000 | Loss: 0.00002783
Iteration 168/1000 | Loss: 0.00002783
Iteration 169/1000 | Loss: 0.00002783
Iteration 170/1000 | Loss: 0.00002783
Iteration 171/1000 | Loss: 0.00002783
Iteration 172/1000 | Loss: 0.00002783
Iteration 173/1000 | Loss: 0.00002783
Iteration 174/1000 | Loss: 0.00002783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.7831465558847412e-05, 2.7831465558847412e-05, 2.7831465558847412e-05, 2.7831465558847412e-05, 2.7831465558847412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7831465558847412e-05

Optimization complete. Final v2v error: 4.486471652984619 mm

Highest mean error: 6.198968410491943 mm for frame 77

Lowest mean error: 3.3920674324035645 mm for frame 0

Saving results

Total time: 79.89025712013245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001804
Iteration 2/25 | Loss: 0.00293059
Iteration 3/25 | Loss: 0.00215705
Iteration 4/25 | Loss: 0.00197991
Iteration 5/25 | Loss: 0.00191365
Iteration 6/25 | Loss: 0.00177470
Iteration 7/25 | Loss: 0.00168149
Iteration 8/25 | Loss: 0.00157319
Iteration 9/25 | Loss: 0.00154781
Iteration 10/25 | Loss: 0.00146488
Iteration 11/25 | Loss: 0.00144153
Iteration 12/25 | Loss: 0.00143260
Iteration 13/25 | Loss: 0.00142189
Iteration 14/25 | Loss: 0.00141550
Iteration 15/25 | Loss: 0.00141358
Iteration 16/25 | Loss: 0.00141795
Iteration 17/25 | Loss: 0.00141718
Iteration 18/25 | Loss: 0.00141323
Iteration 19/25 | Loss: 0.00140975
Iteration 20/25 | Loss: 0.00140852
Iteration 21/25 | Loss: 0.00140805
Iteration 22/25 | Loss: 0.00141145
Iteration 23/25 | Loss: 0.00140944
Iteration 24/25 | Loss: 0.00140728
Iteration 25/25 | Loss: 0.00140682

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46065784
Iteration 2/25 | Loss: 0.00359449
Iteration 3/25 | Loss: 0.00306980
Iteration 4/25 | Loss: 0.00306980
Iteration 5/25 | Loss: 0.00306979
Iteration 6/25 | Loss: 0.00306979
Iteration 7/25 | Loss: 0.00306979
Iteration 8/25 | Loss: 0.00306979
Iteration 9/25 | Loss: 0.00306979
Iteration 10/25 | Loss: 0.00306979
Iteration 11/25 | Loss: 0.00306979
Iteration 12/25 | Loss: 0.00306979
Iteration 13/25 | Loss: 0.00306979
Iteration 14/25 | Loss: 0.00306979
Iteration 15/25 | Loss: 0.00306979
Iteration 16/25 | Loss: 0.00306979
Iteration 17/25 | Loss: 0.00306979
Iteration 18/25 | Loss: 0.00306979
Iteration 19/25 | Loss: 0.00306979
Iteration 20/25 | Loss: 0.00306979
Iteration 21/25 | Loss: 0.00306979
Iteration 22/25 | Loss: 0.00306979
Iteration 23/25 | Loss: 0.00306979
Iteration 24/25 | Loss: 0.00306979
Iteration 25/25 | Loss: 0.00306979

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00306979
Iteration 2/1000 | Loss: 0.00078050
Iteration 3/1000 | Loss: 0.00016947
Iteration 4/1000 | Loss: 0.00071519
Iteration 5/1000 | Loss: 0.00029463
Iteration 6/1000 | Loss: 0.00014535
Iteration 7/1000 | Loss: 0.00055944
Iteration 8/1000 | Loss: 0.00108196
Iteration 9/1000 | Loss: 0.00014489
Iteration 10/1000 | Loss: 0.00064124
Iteration 11/1000 | Loss: 0.00013282
Iteration 12/1000 | Loss: 0.00019698
Iteration 13/1000 | Loss: 0.00015379
Iteration 14/1000 | Loss: 0.00017675
Iteration 15/1000 | Loss: 0.00011290
Iteration 16/1000 | Loss: 0.00011070
Iteration 17/1000 | Loss: 0.00058203
Iteration 18/1000 | Loss: 0.00391070
Iteration 19/1000 | Loss: 0.00128679
Iteration 20/1000 | Loss: 0.00107006
Iteration 21/1000 | Loss: 0.00115143
Iteration 22/1000 | Loss: 0.00074824
Iteration 23/1000 | Loss: 0.00040837
Iteration 24/1000 | Loss: 0.00074026
Iteration 25/1000 | Loss: 0.00033322
Iteration 26/1000 | Loss: 0.00011909
Iteration 27/1000 | Loss: 0.00010617
Iteration 28/1000 | Loss: 0.00070729
Iteration 29/1000 | Loss: 0.00059037
Iteration 30/1000 | Loss: 0.00011113
Iteration 31/1000 | Loss: 0.00022254
Iteration 32/1000 | Loss: 0.00009931
Iteration 33/1000 | Loss: 0.00033462
Iteration 34/1000 | Loss: 0.00009798
Iteration 35/1000 | Loss: 0.00009597
Iteration 36/1000 | Loss: 0.00009466
Iteration 37/1000 | Loss: 0.00056792
Iteration 38/1000 | Loss: 0.00132556
Iteration 39/1000 | Loss: 0.00033494
Iteration 40/1000 | Loss: 0.00009382
Iteration 41/1000 | Loss: 0.00035276
Iteration 42/1000 | Loss: 0.00012691
Iteration 43/1000 | Loss: 0.00018186
Iteration 44/1000 | Loss: 0.00035539
Iteration 45/1000 | Loss: 0.00035970
Iteration 46/1000 | Loss: 0.00011863
Iteration 47/1000 | Loss: 0.00016529
Iteration 48/1000 | Loss: 0.00008699
Iteration 49/1000 | Loss: 0.00008622
Iteration 50/1000 | Loss: 0.00008533
Iteration 51/1000 | Loss: 0.00008467
Iteration 52/1000 | Loss: 0.00057796
Iteration 53/1000 | Loss: 0.00013157
Iteration 54/1000 | Loss: 0.00016295
Iteration 55/1000 | Loss: 0.00008429
Iteration 56/1000 | Loss: 0.00040809
Iteration 57/1000 | Loss: 0.00008923
Iteration 58/1000 | Loss: 0.00008553
Iteration 59/1000 | Loss: 0.00008392
Iteration 60/1000 | Loss: 0.00008301
Iteration 61/1000 | Loss: 0.00008240
Iteration 62/1000 | Loss: 0.00008213
Iteration 63/1000 | Loss: 0.00008195
Iteration 64/1000 | Loss: 0.00008181
Iteration 65/1000 | Loss: 0.00008175
Iteration 66/1000 | Loss: 0.00039823
Iteration 67/1000 | Loss: 0.00041988
Iteration 68/1000 | Loss: 0.00011760
Iteration 69/1000 | Loss: 0.00017365
Iteration 70/1000 | Loss: 0.00008180
Iteration 71/1000 | Loss: 0.00008159
Iteration 72/1000 | Loss: 0.00008147
Iteration 73/1000 | Loss: 0.00008144
Iteration 74/1000 | Loss: 0.00008142
Iteration 75/1000 | Loss: 0.00008138
Iteration 76/1000 | Loss: 0.00008138
Iteration 77/1000 | Loss: 0.00008137
Iteration 78/1000 | Loss: 0.00008137
Iteration 79/1000 | Loss: 0.00008136
Iteration 80/1000 | Loss: 0.00008135
Iteration 81/1000 | Loss: 0.00008135
Iteration 82/1000 | Loss: 0.00008134
Iteration 83/1000 | Loss: 0.00008130
Iteration 84/1000 | Loss: 0.00008123
Iteration 85/1000 | Loss: 0.00008123
Iteration 86/1000 | Loss: 0.00008123
Iteration 87/1000 | Loss: 0.00008121
Iteration 88/1000 | Loss: 0.00008121
Iteration 89/1000 | Loss: 0.00008121
Iteration 90/1000 | Loss: 0.00008121
Iteration 91/1000 | Loss: 0.00008121
Iteration 92/1000 | Loss: 0.00008120
Iteration 93/1000 | Loss: 0.00008120
Iteration 94/1000 | Loss: 0.00008120
Iteration 95/1000 | Loss: 0.00008120
Iteration 96/1000 | Loss: 0.00008120
Iteration 97/1000 | Loss: 0.00008120
Iteration 98/1000 | Loss: 0.00008120
Iteration 99/1000 | Loss: 0.00008120
Iteration 100/1000 | Loss: 0.00008120
Iteration 101/1000 | Loss: 0.00008120
Iteration 102/1000 | Loss: 0.00008119
Iteration 103/1000 | Loss: 0.00008119
Iteration 104/1000 | Loss: 0.00008119
Iteration 105/1000 | Loss: 0.00008116
Iteration 106/1000 | Loss: 0.00008116
Iteration 107/1000 | Loss: 0.00008115
Iteration 108/1000 | Loss: 0.00008114
Iteration 109/1000 | Loss: 0.00008114
Iteration 110/1000 | Loss: 0.00008114
Iteration 111/1000 | Loss: 0.00008114
Iteration 112/1000 | Loss: 0.00008113
Iteration 113/1000 | Loss: 0.00008113
Iteration 114/1000 | Loss: 0.00008113
Iteration 115/1000 | Loss: 0.00008112
Iteration 116/1000 | Loss: 0.00008112
Iteration 117/1000 | Loss: 0.00008111
Iteration 118/1000 | Loss: 0.00008110
Iteration 119/1000 | Loss: 0.00008110
Iteration 120/1000 | Loss: 0.00008110
Iteration 121/1000 | Loss: 0.00008110
Iteration 122/1000 | Loss: 0.00008110
Iteration 123/1000 | Loss: 0.00008110
Iteration 124/1000 | Loss: 0.00008110
Iteration 125/1000 | Loss: 0.00008110
Iteration 126/1000 | Loss: 0.00008109
Iteration 127/1000 | Loss: 0.00008109
Iteration 128/1000 | Loss: 0.00008109
Iteration 129/1000 | Loss: 0.00008109
Iteration 130/1000 | Loss: 0.00008109
Iteration 131/1000 | Loss: 0.00008108
Iteration 132/1000 | Loss: 0.00008108
Iteration 133/1000 | Loss: 0.00008108
Iteration 134/1000 | Loss: 0.00008108
Iteration 135/1000 | Loss: 0.00008108
Iteration 136/1000 | Loss: 0.00008108
Iteration 137/1000 | Loss: 0.00008107
Iteration 138/1000 | Loss: 0.00008107
Iteration 139/1000 | Loss: 0.00008107
Iteration 140/1000 | Loss: 0.00008107
Iteration 141/1000 | Loss: 0.00008107
Iteration 142/1000 | Loss: 0.00008107
Iteration 143/1000 | Loss: 0.00008107
Iteration 144/1000 | Loss: 0.00008107
Iteration 145/1000 | Loss: 0.00008107
Iteration 146/1000 | Loss: 0.00008107
Iteration 147/1000 | Loss: 0.00008107
Iteration 148/1000 | Loss: 0.00008107
Iteration 149/1000 | Loss: 0.00008107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [8.106999302981421e-05, 8.106999302981421e-05, 8.106999302981421e-05, 8.106999302981421e-05, 8.106999302981421e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.106999302981421e-05

Optimization complete. Final v2v error: 4.902559757232666 mm

Highest mean error: 11.847114562988281 mm for frame 21

Lowest mean error: 3.318589448928833 mm for frame 10

Saving results

Total time: 147.99678349494934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816520
Iteration 2/25 | Loss: 0.00128356
Iteration 3/25 | Loss: 0.00119308
Iteration 4/25 | Loss: 0.00118093
Iteration 5/25 | Loss: 0.00117810
Iteration 6/25 | Loss: 0.00117810
Iteration 7/25 | Loss: 0.00117810
Iteration 8/25 | Loss: 0.00117810
Iteration 9/25 | Loss: 0.00117810
Iteration 10/25 | Loss: 0.00117810
Iteration 11/25 | Loss: 0.00117810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011781016364693642, 0.0011781016364693642, 0.0011781016364693642, 0.0011781016364693642, 0.0011781016364693642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011781016364693642

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34899724
Iteration 2/25 | Loss: 0.00091739
Iteration 3/25 | Loss: 0.00091738
Iteration 4/25 | Loss: 0.00091738
Iteration 5/25 | Loss: 0.00091738
Iteration 6/25 | Loss: 0.00091738
Iteration 7/25 | Loss: 0.00091738
Iteration 8/25 | Loss: 0.00091738
Iteration 9/25 | Loss: 0.00091738
Iteration 10/25 | Loss: 0.00091738
Iteration 11/25 | Loss: 0.00091738
Iteration 12/25 | Loss: 0.00091738
Iteration 13/25 | Loss: 0.00091738
Iteration 14/25 | Loss: 0.00091738
Iteration 15/25 | Loss: 0.00091738
Iteration 16/25 | Loss: 0.00091738
Iteration 17/25 | Loss: 0.00091738
Iteration 18/25 | Loss: 0.00091738
Iteration 19/25 | Loss: 0.00091738
Iteration 20/25 | Loss: 0.00091738
Iteration 21/25 | Loss: 0.00091738
Iteration 22/25 | Loss: 0.00091738
Iteration 23/25 | Loss: 0.00091738
Iteration 24/25 | Loss: 0.00091738
Iteration 25/25 | Loss: 0.00091738

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091738
Iteration 2/1000 | Loss: 0.00002058
Iteration 3/1000 | Loss: 0.00001516
Iteration 4/1000 | Loss: 0.00001355
Iteration 5/1000 | Loss: 0.00001253
Iteration 6/1000 | Loss: 0.00001194
Iteration 7/1000 | Loss: 0.00001155
Iteration 8/1000 | Loss: 0.00001117
Iteration 9/1000 | Loss: 0.00001097
Iteration 10/1000 | Loss: 0.00001090
Iteration 11/1000 | Loss: 0.00001089
Iteration 12/1000 | Loss: 0.00001089
Iteration 13/1000 | Loss: 0.00001083
Iteration 14/1000 | Loss: 0.00001083
Iteration 15/1000 | Loss: 0.00001082
Iteration 16/1000 | Loss: 0.00001078
Iteration 17/1000 | Loss: 0.00001077
Iteration 18/1000 | Loss: 0.00001077
Iteration 19/1000 | Loss: 0.00001076
Iteration 20/1000 | Loss: 0.00001076
Iteration 21/1000 | Loss: 0.00001074
Iteration 22/1000 | Loss: 0.00001073
Iteration 23/1000 | Loss: 0.00001072
Iteration 24/1000 | Loss: 0.00001072
Iteration 25/1000 | Loss: 0.00001072
Iteration 26/1000 | Loss: 0.00001071
Iteration 27/1000 | Loss: 0.00001071
Iteration 28/1000 | Loss: 0.00001071
Iteration 29/1000 | Loss: 0.00001070
Iteration 30/1000 | Loss: 0.00001069
Iteration 31/1000 | Loss: 0.00001068
Iteration 32/1000 | Loss: 0.00001068
Iteration 33/1000 | Loss: 0.00001067
Iteration 34/1000 | Loss: 0.00001067
Iteration 35/1000 | Loss: 0.00001066
Iteration 36/1000 | Loss: 0.00001066
Iteration 37/1000 | Loss: 0.00001066
Iteration 38/1000 | Loss: 0.00001065
Iteration 39/1000 | Loss: 0.00001065
Iteration 40/1000 | Loss: 0.00001065
Iteration 41/1000 | Loss: 0.00001064
Iteration 42/1000 | Loss: 0.00001063
Iteration 43/1000 | Loss: 0.00001063
Iteration 44/1000 | Loss: 0.00001062
Iteration 45/1000 | Loss: 0.00001062
Iteration 46/1000 | Loss: 0.00001062
Iteration 47/1000 | Loss: 0.00001062
Iteration 48/1000 | Loss: 0.00001061
Iteration 49/1000 | Loss: 0.00001060
Iteration 50/1000 | Loss: 0.00001058
Iteration 51/1000 | Loss: 0.00001055
Iteration 52/1000 | Loss: 0.00001051
Iteration 53/1000 | Loss: 0.00001049
Iteration 54/1000 | Loss: 0.00001044
Iteration 55/1000 | Loss: 0.00001042
Iteration 56/1000 | Loss: 0.00001041
Iteration 57/1000 | Loss: 0.00001040
Iteration 58/1000 | Loss: 0.00001040
Iteration 59/1000 | Loss: 0.00001040
Iteration 60/1000 | Loss: 0.00001040
Iteration 61/1000 | Loss: 0.00001040
Iteration 62/1000 | Loss: 0.00001040
Iteration 63/1000 | Loss: 0.00001040
Iteration 64/1000 | Loss: 0.00001040
Iteration 65/1000 | Loss: 0.00001040
Iteration 66/1000 | Loss: 0.00001040
Iteration 67/1000 | Loss: 0.00001040
Iteration 68/1000 | Loss: 0.00001040
Iteration 69/1000 | Loss: 0.00001040
Iteration 70/1000 | Loss: 0.00001040
Iteration 71/1000 | Loss: 0.00001040
Iteration 72/1000 | Loss: 0.00001040
Iteration 73/1000 | Loss: 0.00001040
Iteration 74/1000 | Loss: 0.00001040
Iteration 75/1000 | Loss: 0.00001040
Iteration 76/1000 | Loss: 0.00001040
Iteration 77/1000 | Loss: 0.00001040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.0396393008704763e-05, 1.0396393008704763e-05, 1.0396393008704763e-05, 1.0396393008704763e-05, 1.0396393008704763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0396393008704763e-05

Optimization complete. Final v2v error: 2.780986785888672 mm

Highest mean error: 2.9946916103363037 mm for frame 104

Lowest mean error: 2.610997438430786 mm for frame 26

Saving results

Total time: 32.56759309768677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875456
Iteration 2/25 | Loss: 0.00144674
Iteration 3/25 | Loss: 0.00129926
Iteration 4/25 | Loss: 0.00127625
Iteration 5/25 | Loss: 0.00127151
Iteration 6/25 | Loss: 0.00126614
Iteration 7/25 | Loss: 0.00126346
Iteration 8/25 | Loss: 0.00126247
Iteration 9/25 | Loss: 0.00126202
Iteration 10/25 | Loss: 0.00126191
Iteration 11/25 | Loss: 0.00126191
Iteration 12/25 | Loss: 0.00126191
Iteration 13/25 | Loss: 0.00126191
Iteration 14/25 | Loss: 0.00126191
Iteration 15/25 | Loss: 0.00126190
Iteration 16/25 | Loss: 0.00126190
Iteration 17/25 | Loss: 0.00126190
Iteration 18/25 | Loss: 0.00126190
Iteration 19/25 | Loss: 0.00126190
Iteration 20/25 | Loss: 0.00126190
Iteration 21/25 | Loss: 0.00126190
Iteration 22/25 | Loss: 0.00126190
Iteration 23/25 | Loss: 0.00126190
Iteration 24/25 | Loss: 0.00126190
Iteration 25/25 | Loss: 0.00126190

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.31681037
Iteration 2/25 | Loss: 0.00115967
Iteration 3/25 | Loss: 0.00104241
Iteration 4/25 | Loss: 0.00104241
Iteration 5/25 | Loss: 0.00104241
Iteration 6/25 | Loss: 0.00104241
Iteration 7/25 | Loss: 0.00104241
Iteration 8/25 | Loss: 0.00104241
Iteration 9/25 | Loss: 0.00104241
Iteration 10/25 | Loss: 0.00104241
Iteration 11/25 | Loss: 0.00104241
Iteration 12/25 | Loss: 0.00104241
Iteration 13/25 | Loss: 0.00104241
Iteration 14/25 | Loss: 0.00104241
Iteration 15/25 | Loss: 0.00104241
Iteration 16/25 | Loss: 0.00104241
Iteration 17/25 | Loss: 0.00104241
Iteration 18/25 | Loss: 0.00104241
Iteration 19/25 | Loss: 0.00104241
Iteration 20/25 | Loss: 0.00104241
Iteration 21/25 | Loss: 0.00104241
Iteration 22/25 | Loss: 0.00104241
Iteration 23/25 | Loss: 0.00104241
Iteration 24/25 | Loss: 0.00104241
Iteration 25/25 | Loss: 0.00104241

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104241
Iteration 2/1000 | Loss: 0.00003198
Iteration 3/1000 | Loss: 0.00002190
Iteration 4/1000 | Loss: 0.00002003
Iteration 5/1000 | Loss: 0.00001878
Iteration 6/1000 | Loss: 0.00001772
Iteration 7/1000 | Loss: 0.00001732
Iteration 8/1000 | Loss: 0.00001681
Iteration 9/1000 | Loss: 0.00001654
Iteration 10/1000 | Loss: 0.00001612
Iteration 11/1000 | Loss: 0.00012013
Iteration 12/1000 | Loss: 0.00002877
Iteration 13/1000 | Loss: 0.00002041
Iteration 14/1000 | Loss: 0.00001603
Iteration 15/1000 | Loss: 0.00001573
Iteration 16/1000 | Loss: 0.00006988
Iteration 17/1000 | Loss: 0.00001581
Iteration 18/1000 | Loss: 0.00001562
Iteration 19/1000 | Loss: 0.00001561
Iteration 20/1000 | Loss: 0.00001561
Iteration 21/1000 | Loss: 0.00001560
Iteration 22/1000 | Loss: 0.00005671
Iteration 23/1000 | Loss: 0.00002643
Iteration 24/1000 | Loss: 0.00003214
Iteration 25/1000 | Loss: 0.00004765
Iteration 26/1000 | Loss: 0.00004768
Iteration 27/1000 | Loss: 0.00001572
Iteration 28/1000 | Loss: 0.00001545
Iteration 29/1000 | Loss: 0.00001543
Iteration 30/1000 | Loss: 0.00001543
Iteration 31/1000 | Loss: 0.00001542
Iteration 32/1000 | Loss: 0.00001541
Iteration 33/1000 | Loss: 0.00001540
Iteration 34/1000 | Loss: 0.00001540
Iteration 35/1000 | Loss: 0.00001540
Iteration 36/1000 | Loss: 0.00001539
Iteration 37/1000 | Loss: 0.00001539
Iteration 38/1000 | Loss: 0.00001538
Iteration 39/1000 | Loss: 0.00001538
Iteration 40/1000 | Loss: 0.00001537
Iteration 41/1000 | Loss: 0.00001537
Iteration 42/1000 | Loss: 0.00001536
Iteration 43/1000 | Loss: 0.00001535
Iteration 44/1000 | Loss: 0.00001535
Iteration 45/1000 | Loss: 0.00001535
Iteration 46/1000 | Loss: 0.00009316
Iteration 47/1000 | Loss: 0.00001561
Iteration 48/1000 | Loss: 0.00001532
Iteration 49/1000 | Loss: 0.00001531
Iteration 50/1000 | Loss: 0.00001530
Iteration 51/1000 | Loss: 0.00001528
Iteration 52/1000 | Loss: 0.00001527
Iteration 53/1000 | Loss: 0.00001527
Iteration 54/1000 | Loss: 0.00001526
Iteration 55/1000 | Loss: 0.00001526
Iteration 56/1000 | Loss: 0.00001524
Iteration 57/1000 | Loss: 0.00001524
Iteration 58/1000 | Loss: 0.00001524
Iteration 59/1000 | Loss: 0.00001524
Iteration 60/1000 | Loss: 0.00001524
Iteration 61/1000 | Loss: 0.00001524
Iteration 62/1000 | Loss: 0.00001524
Iteration 63/1000 | Loss: 0.00001524
Iteration 64/1000 | Loss: 0.00001524
Iteration 65/1000 | Loss: 0.00001524
Iteration 66/1000 | Loss: 0.00001524
Iteration 67/1000 | Loss: 0.00001524
Iteration 68/1000 | Loss: 0.00001524
Iteration 69/1000 | Loss: 0.00001524
Iteration 70/1000 | Loss: 0.00001524
Iteration 71/1000 | Loss: 0.00001524
Iteration 72/1000 | Loss: 0.00001524
Iteration 73/1000 | Loss: 0.00001524
Iteration 74/1000 | Loss: 0.00001524
Iteration 75/1000 | Loss: 0.00001524
Iteration 76/1000 | Loss: 0.00001524
Iteration 77/1000 | Loss: 0.00001524
Iteration 78/1000 | Loss: 0.00001524
Iteration 79/1000 | Loss: 0.00001523
Iteration 80/1000 | Loss: 0.00001523
Iteration 81/1000 | Loss: 0.00001523
Iteration 82/1000 | Loss: 0.00001523
Iteration 83/1000 | Loss: 0.00001523
Iteration 84/1000 | Loss: 0.00001523
Iteration 85/1000 | Loss: 0.00001523
Iteration 86/1000 | Loss: 0.00001523
Iteration 87/1000 | Loss: 0.00001523
Iteration 88/1000 | Loss: 0.00001523
Iteration 89/1000 | Loss: 0.00001523
Iteration 90/1000 | Loss: 0.00001523
Iteration 91/1000 | Loss: 0.00001523
Iteration 92/1000 | Loss: 0.00001523
Iteration 93/1000 | Loss: 0.00001523
Iteration 94/1000 | Loss: 0.00001523
Iteration 95/1000 | Loss: 0.00001523
Iteration 96/1000 | Loss: 0.00001523
Iteration 97/1000 | Loss: 0.00001523
Iteration 98/1000 | Loss: 0.00001523
Iteration 99/1000 | Loss: 0.00001523
Iteration 100/1000 | Loss: 0.00001523
Iteration 101/1000 | Loss: 0.00001523
Iteration 102/1000 | Loss: 0.00001523
Iteration 103/1000 | Loss: 0.00001523
Iteration 104/1000 | Loss: 0.00001523
Iteration 105/1000 | Loss: 0.00001523
Iteration 106/1000 | Loss: 0.00001523
Iteration 107/1000 | Loss: 0.00001523
Iteration 108/1000 | Loss: 0.00001523
Iteration 109/1000 | Loss: 0.00001523
Iteration 110/1000 | Loss: 0.00001523
Iteration 111/1000 | Loss: 0.00001523
Iteration 112/1000 | Loss: 0.00001523
Iteration 113/1000 | Loss: 0.00001523
Iteration 114/1000 | Loss: 0.00001523
Iteration 115/1000 | Loss: 0.00001523
Iteration 116/1000 | Loss: 0.00001523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.5231809811666608e-05, 1.5231809811666608e-05, 1.5231809811666608e-05, 1.5231809811666608e-05, 1.5231809811666608e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5231809811666608e-05

Optimization complete. Final v2v error: 3.278595447540283 mm

Highest mean error: 4.029780864715576 mm for frame 183

Lowest mean error: 2.8408689498901367 mm for frame 204

Saving results

Total time: 67.77984309196472
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047562
Iteration 2/25 | Loss: 0.00252602
Iteration 3/25 | Loss: 0.00197338
Iteration 4/25 | Loss: 0.00222160
Iteration 5/25 | Loss: 0.00225945
Iteration 6/25 | Loss: 0.00204879
Iteration 7/25 | Loss: 0.00192023
Iteration 8/25 | Loss: 0.00178385
Iteration 9/25 | Loss: 0.00169688
Iteration 10/25 | Loss: 0.00160041
Iteration 11/25 | Loss: 0.00157426
Iteration 12/25 | Loss: 0.00153592
Iteration 13/25 | Loss: 0.00150919
Iteration 14/25 | Loss: 0.00145528
Iteration 15/25 | Loss: 0.00144237
Iteration 16/25 | Loss: 0.00143034
Iteration 17/25 | Loss: 0.00142109
Iteration 18/25 | Loss: 0.00142555
Iteration 19/25 | Loss: 0.00142259
Iteration 20/25 | Loss: 0.00142412
Iteration 21/25 | Loss: 0.00141725
Iteration 22/25 | Loss: 0.00142243
Iteration 23/25 | Loss: 0.00142015
Iteration 24/25 | Loss: 0.00141567
Iteration 25/25 | Loss: 0.00141312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47189462
Iteration 2/25 | Loss: 0.00118271
Iteration 3/25 | Loss: 0.00118271
Iteration 4/25 | Loss: 0.00118271
Iteration 5/25 | Loss: 0.00118271
Iteration 6/25 | Loss: 0.00118271
Iteration 7/25 | Loss: 0.00118271
Iteration 8/25 | Loss: 0.00118271
Iteration 9/25 | Loss: 0.00118270
Iteration 10/25 | Loss: 0.00118270
Iteration 11/25 | Loss: 0.00118270
Iteration 12/25 | Loss: 0.00118270
Iteration 13/25 | Loss: 0.00118270
Iteration 14/25 | Loss: 0.00118270
Iteration 15/25 | Loss: 0.00118270
Iteration 16/25 | Loss: 0.00118270
Iteration 17/25 | Loss: 0.00118270
Iteration 18/25 | Loss: 0.00118270
Iteration 19/25 | Loss: 0.00118270
Iteration 20/25 | Loss: 0.00118270
Iteration 21/25 | Loss: 0.00118270
Iteration 22/25 | Loss: 0.00118270
Iteration 23/25 | Loss: 0.00118270
Iteration 24/25 | Loss: 0.00118270
Iteration 25/25 | Loss: 0.00118270

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118270
Iteration 2/1000 | Loss: 0.00024828
Iteration 3/1000 | Loss: 0.00035686
Iteration 4/1000 | Loss: 0.00021026
Iteration 5/1000 | Loss: 0.00035899
Iteration 6/1000 | Loss: 0.00030058
Iteration 7/1000 | Loss: 0.00027640
Iteration 8/1000 | Loss: 0.00032767
Iteration 9/1000 | Loss: 0.00029777
Iteration 10/1000 | Loss: 0.00025669
Iteration 11/1000 | Loss: 0.00028083
Iteration 12/1000 | Loss: 0.00021780
Iteration 13/1000 | Loss: 0.00025254
Iteration 14/1000 | Loss: 0.00030045
Iteration 15/1000 | Loss: 0.00029264
Iteration 16/1000 | Loss: 0.00032733
Iteration 17/1000 | Loss: 0.00033858
Iteration 18/1000 | Loss: 0.00034403
Iteration 19/1000 | Loss: 0.00035402
Iteration 20/1000 | Loss: 0.00036271
Iteration 21/1000 | Loss: 0.00034936
Iteration 22/1000 | Loss: 0.00028373
Iteration 23/1000 | Loss: 0.00034424
Iteration 24/1000 | Loss: 0.00035569
Iteration 25/1000 | Loss: 0.00035344
Iteration 26/1000 | Loss: 0.00030570
Iteration 27/1000 | Loss: 0.00028796
Iteration 28/1000 | Loss: 0.00028911
Iteration 29/1000 | Loss: 0.00037790
Iteration 30/1000 | Loss: 0.00036190
Iteration 31/1000 | Loss: 0.00036334
Iteration 32/1000 | Loss: 0.00036716
Iteration 33/1000 | Loss: 0.00033806
Iteration 34/1000 | Loss: 0.00026618
Iteration 35/1000 | Loss: 0.00034125
Iteration 36/1000 | Loss: 0.00032438
Iteration 37/1000 | Loss: 0.00030738
Iteration 38/1000 | Loss: 0.00031861
Iteration 39/1000 | Loss: 0.00033650
Iteration 40/1000 | Loss: 0.00033056
Iteration 41/1000 | Loss: 0.00036103
Iteration 42/1000 | Loss: 0.00033328
Iteration 43/1000 | Loss: 0.00034601
Iteration 44/1000 | Loss: 0.00033240
Iteration 45/1000 | Loss: 0.00024655
Iteration 46/1000 | Loss: 0.00021901
Iteration 47/1000 | Loss: 0.00027453
Iteration 48/1000 | Loss: 0.00022421
Iteration 49/1000 | Loss: 0.00021801
Iteration 50/1000 | Loss: 0.00032759
Iteration 51/1000 | Loss: 0.00032770
Iteration 52/1000 | Loss: 0.00033985
Iteration 53/1000 | Loss: 0.00045291
Iteration 54/1000 | Loss: 0.00040681
Iteration 55/1000 | Loss: 0.00048982
Iteration 56/1000 | Loss: 0.00016427
Iteration 57/1000 | Loss: 0.00019310
Iteration 58/1000 | Loss: 0.00026386
Iteration 59/1000 | Loss: 0.00020667
Iteration 60/1000 | Loss: 0.00017707
Iteration 61/1000 | Loss: 0.00017547
Iteration 62/1000 | Loss: 0.00015364
Iteration 63/1000 | Loss: 0.00023519
Iteration 64/1000 | Loss: 0.00020184
Iteration 65/1000 | Loss: 0.00023006
Iteration 66/1000 | Loss: 0.00022468
Iteration 67/1000 | Loss: 0.00026794
Iteration 68/1000 | Loss: 0.00024754
Iteration 69/1000 | Loss: 0.00030647
Iteration 70/1000 | Loss: 0.00050591
Iteration 71/1000 | Loss: 0.00018648
Iteration 72/1000 | Loss: 0.00020590
Iteration 73/1000 | Loss: 0.00020420
Iteration 74/1000 | Loss: 0.00019227
Iteration 75/1000 | Loss: 0.00016147
Iteration 76/1000 | Loss: 0.00017973
Iteration 77/1000 | Loss: 0.00026952
Iteration 78/1000 | Loss: 0.00022002
Iteration 79/1000 | Loss: 0.00023643
Iteration 80/1000 | Loss: 0.00027027
Iteration 81/1000 | Loss: 0.00020976
Iteration 82/1000 | Loss: 0.00023446
Iteration 83/1000 | Loss: 0.00023685
Iteration 84/1000 | Loss: 0.00017004
Iteration 85/1000 | Loss: 0.00021103
Iteration 86/1000 | Loss: 0.00021835
Iteration 87/1000 | Loss: 0.00020680
Iteration 88/1000 | Loss: 0.00022481
Iteration 89/1000 | Loss: 0.00021931
Iteration 90/1000 | Loss: 0.00023923
Iteration 91/1000 | Loss: 0.00019344
Iteration 92/1000 | Loss: 0.00022247
Iteration 93/1000 | Loss: 0.00020902
Iteration 94/1000 | Loss: 0.00024228
Iteration 95/1000 | Loss: 0.00024298
Iteration 96/1000 | Loss: 0.00026653
Iteration 97/1000 | Loss: 0.00022945
Iteration 98/1000 | Loss: 0.00037385
Iteration 99/1000 | Loss: 0.00024999
Iteration 100/1000 | Loss: 0.00024426
Iteration 101/1000 | Loss: 0.00022556
Iteration 102/1000 | Loss: 0.00048744
Iteration 103/1000 | Loss: 0.00023019
Iteration 104/1000 | Loss: 0.00039322
Iteration 105/1000 | Loss: 0.00025295
Iteration 106/1000 | Loss: 0.00024584
Iteration 107/1000 | Loss: 0.00024901
Iteration 108/1000 | Loss: 0.00039168
Iteration 109/1000 | Loss: 0.00019196
Iteration 110/1000 | Loss: 0.00022144
Iteration 111/1000 | Loss: 0.00024366
Iteration 112/1000 | Loss: 0.00023987
Iteration 113/1000 | Loss: 0.00033722
Iteration 114/1000 | Loss: 0.00020687
Iteration 115/1000 | Loss: 0.00017822
Iteration 116/1000 | Loss: 0.00018176
Iteration 117/1000 | Loss: 0.00015819
Iteration 118/1000 | Loss: 0.00018601
Iteration 119/1000 | Loss: 0.00020209
Iteration 120/1000 | Loss: 0.00018624
Iteration 121/1000 | Loss: 0.00018243
Iteration 122/1000 | Loss: 0.00022496
Iteration 123/1000 | Loss: 0.00021021
Iteration 124/1000 | Loss: 0.00023883
Iteration 125/1000 | Loss: 0.00023925
Iteration 126/1000 | Loss: 0.00023105
Iteration 127/1000 | Loss: 0.00023067
Iteration 128/1000 | Loss: 0.00022831
Iteration 129/1000 | Loss: 0.00018970
Iteration 130/1000 | Loss: 0.00020983
Iteration 131/1000 | Loss: 0.00020955
Iteration 132/1000 | Loss: 0.00024766
Iteration 133/1000 | Loss: 0.00016993
Iteration 134/1000 | Loss: 0.00023562
Iteration 135/1000 | Loss: 0.00020828
Iteration 136/1000 | Loss: 0.00022242
Iteration 137/1000 | Loss: 0.00020170
Iteration 138/1000 | Loss: 0.00021009
Iteration 139/1000 | Loss: 0.00019988
Iteration 140/1000 | Loss: 0.00017417
Iteration 141/1000 | Loss: 0.00024183
Iteration 142/1000 | Loss: 0.00025436
Iteration 143/1000 | Loss: 0.00020604
Iteration 144/1000 | Loss: 0.00034033
Iteration 145/1000 | Loss: 0.00016995
Iteration 146/1000 | Loss: 0.00019486
Iteration 147/1000 | Loss: 0.00017600
Iteration 148/1000 | Loss: 0.00008883
Iteration 149/1000 | Loss: 0.00005421
Iteration 150/1000 | Loss: 0.00004566
Iteration 151/1000 | Loss: 0.00006405
Iteration 152/1000 | Loss: 0.00003756
Iteration 153/1000 | Loss: 0.00003322
Iteration 154/1000 | Loss: 0.00002967
Iteration 155/1000 | Loss: 0.00002795
Iteration 156/1000 | Loss: 0.00002675
Iteration 157/1000 | Loss: 0.00002594
Iteration 158/1000 | Loss: 0.00002463
Iteration 159/1000 | Loss: 0.00002371
Iteration 160/1000 | Loss: 0.00002318
Iteration 161/1000 | Loss: 0.00002266
Iteration 162/1000 | Loss: 0.00002214
Iteration 163/1000 | Loss: 0.00002150
Iteration 164/1000 | Loss: 0.00018838
Iteration 165/1000 | Loss: 0.00029558
Iteration 166/1000 | Loss: 0.00004080
Iteration 167/1000 | Loss: 0.00003158
Iteration 168/1000 | Loss: 0.00002794
Iteration 169/1000 | Loss: 0.00002558
Iteration 170/1000 | Loss: 0.00061405
Iteration 171/1000 | Loss: 0.00002577
Iteration 172/1000 | Loss: 0.00002282
Iteration 173/1000 | Loss: 0.00002189
Iteration 174/1000 | Loss: 0.00002142
Iteration 175/1000 | Loss: 0.00002094
Iteration 176/1000 | Loss: 0.00002055
Iteration 177/1000 | Loss: 0.00002024
Iteration 178/1000 | Loss: 0.00002001
Iteration 179/1000 | Loss: 0.00001978
Iteration 180/1000 | Loss: 0.00001974
Iteration 181/1000 | Loss: 0.00001955
Iteration 182/1000 | Loss: 0.00001948
Iteration 183/1000 | Loss: 0.00001944
Iteration 184/1000 | Loss: 0.00001943
Iteration 185/1000 | Loss: 0.00001943
Iteration 186/1000 | Loss: 0.00001942
Iteration 187/1000 | Loss: 0.00001942
Iteration 188/1000 | Loss: 0.00001942
Iteration 189/1000 | Loss: 0.00001941
Iteration 190/1000 | Loss: 0.00001937
Iteration 191/1000 | Loss: 0.00001937
Iteration 192/1000 | Loss: 0.00001937
Iteration 193/1000 | Loss: 0.00001937
Iteration 194/1000 | Loss: 0.00001936
Iteration 195/1000 | Loss: 0.00001936
Iteration 196/1000 | Loss: 0.00001935
Iteration 197/1000 | Loss: 0.00001935
Iteration 198/1000 | Loss: 0.00001935
Iteration 199/1000 | Loss: 0.00001934
Iteration 200/1000 | Loss: 0.00001934
Iteration 201/1000 | Loss: 0.00001934
Iteration 202/1000 | Loss: 0.00001934
Iteration 203/1000 | Loss: 0.00001934
Iteration 204/1000 | Loss: 0.00001934
Iteration 205/1000 | Loss: 0.00001934
Iteration 206/1000 | Loss: 0.00001934
Iteration 207/1000 | Loss: 0.00001934
Iteration 208/1000 | Loss: 0.00001934
Iteration 209/1000 | Loss: 0.00001934
Iteration 210/1000 | Loss: 0.00001934
Iteration 211/1000 | Loss: 0.00001934
Iteration 212/1000 | Loss: 0.00001934
Iteration 213/1000 | Loss: 0.00001934
Iteration 214/1000 | Loss: 0.00001934
Iteration 215/1000 | Loss: 0.00001934
Iteration 216/1000 | Loss: 0.00001934
Iteration 217/1000 | Loss: 0.00001934
Iteration 218/1000 | Loss: 0.00001934
Iteration 219/1000 | Loss: 0.00001934
Iteration 220/1000 | Loss: 0.00001934
Iteration 221/1000 | Loss: 0.00001934
Iteration 222/1000 | Loss: 0.00001934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.9336057448526844e-05, 1.9336057448526844e-05, 1.9336057448526844e-05, 1.9336057448526844e-05, 1.9336057448526844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9336057448526844e-05

Optimization complete. Final v2v error: 3.5953710079193115 mm

Highest mean error: 4.937142372131348 mm for frame 80

Lowest mean error: 3.2059497833251953 mm for frame 147

Saving results

Total time: 298.75331807136536
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874877
Iteration 2/25 | Loss: 0.00177723
Iteration 3/25 | Loss: 0.00141071
Iteration 4/25 | Loss: 0.00135077
Iteration 5/25 | Loss: 0.00133509
Iteration 6/25 | Loss: 0.00132480
Iteration 7/25 | Loss: 0.00130861
Iteration 8/25 | Loss: 0.00131101
Iteration 9/25 | Loss: 0.00130895
Iteration 10/25 | Loss: 0.00130082
Iteration 11/25 | Loss: 0.00129877
Iteration 12/25 | Loss: 0.00129846
Iteration 13/25 | Loss: 0.00130482
Iteration 14/25 | Loss: 0.00130346
Iteration 15/25 | Loss: 0.00129783
Iteration 16/25 | Loss: 0.00129599
Iteration 17/25 | Loss: 0.00129533
Iteration 18/25 | Loss: 0.00129498
Iteration 19/25 | Loss: 0.00129494
Iteration 20/25 | Loss: 0.00129494
Iteration 21/25 | Loss: 0.00129494
Iteration 22/25 | Loss: 0.00129494
Iteration 23/25 | Loss: 0.00129494
Iteration 24/25 | Loss: 0.00129494
Iteration 25/25 | Loss: 0.00129494

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.70626068
Iteration 2/25 | Loss: 0.00090140
Iteration 3/25 | Loss: 0.00090140
Iteration 4/25 | Loss: 0.00090140
Iteration 5/25 | Loss: 0.00090140
Iteration 6/25 | Loss: 0.00090140
Iteration 7/25 | Loss: 0.00090140
Iteration 8/25 | Loss: 0.00090140
Iteration 9/25 | Loss: 0.00090140
Iteration 10/25 | Loss: 0.00090140
Iteration 11/25 | Loss: 0.00090140
Iteration 12/25 | Loss: 0.00090140
Iteration 13/25 | Loss: 0.00090140
Iteration 14/25 | Loss: 0.00090140
Iteration 15/25 | Loss: 0.00090140
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009013952221721411, 0.0009013952221721411, 0.0009013952221721411, 0.0009013952221721411, 0.0009013952221721411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009013952221721411

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090140
Iteration 2/1000 | Loss: 0.00033339
Iteration 3/1000 | Loss: 0.00021187
Iteration 4/1000 | Loss: 0.00024233
Iteration 5/1000 | Loss: 0.00016178
Iteration 6/1000 | Loss: 0.00018063
Iteration 7/1000 | Loss: 0.00002910
Iteration 8/1000 | Loss: 0.00002691
Iteration 9/1000 | Loss: 0.00002474
Iteration 10/1000 | Loss: 0.00021641
Iteration 11/1000 | Loss: 0.00003029
Iteration 12/1000 | Loss: 0.00002753
Iteration 13/1000 | Loss: 0.00002609
Iteration 14/1000 | Loss: 0.00002530
Iteration 15/1000 | Loss: 0.00002494
Iteration 16/1000 | Loss: 0.00002447
Iteration 17/1000 | Loss: 0.00010368
Iteration 18/1000 | Loss: 0.00077555
Iteration 19/1000 | Loss: 0.00019825
Iteration 20/1000 | Loss: 0.00002924
Iteration 21/1000 | Loss: 0.00002502
Iteration 22/1000 | Loss: 0.00002364
Iteration 23/1000 | Loss: 0.00005250
Iteration 24/1000 | Loss: 0.00013810
Iteration 25/1000 | Loss: 0.00002210
Iteration 26/1000 | Loss: 0.00002141
Iteration 27/1000 | Loss: 0.00002095
Iteration 28/1000 | Loss: 0.00002052
Iteration 29/1000 | Loss: 0.00014738
Iteration 30/1000 | Loss: 0.00002109
Iteration 31/1000 | Loss: 0.00002021
Iteration 32/1000 | Loss: 0.00001986
Iteration 33/1000 | Loss: 0.00001962
Iteration 34/1000 | Loss: 0.00005741
Iteration 35/1000 | Loss: 0.00001952
Iteration 36/1000 | Loss: 0.00003114
Iteration 37/1000 | Loss: 0.00001938
Iteration 38/1000 | Loss: 0.00005248
Iteration 39/1000 | Loss: 0.00001929
Iteration 40/1000 | Loss: 0.00001923
Iteration 41/1000 | Loss: 0.00001923
Iteration 42/1000 | Loss: 0.00001923
Iteration 43/1000 | Loss: 0.00001923
Iteration 44/1000 | Loss: 0.00001923
Iteration 45/1000 | Loss: 0.00001923
Iteration 46/1000 | Loss: 0.00001923
Iteration 47/1000 | Loss: 0.00001923
Iteration 48/1000 | Loss: 0.00001922
Iteration 49/1000 | Loss: 0.00001922
Iteration 50/1000 | Loss: 0.00001922
Iteration 51/1000 | Loss: 0.00001922
Iteration 52/1000 | Loss: 0.00001922
Iteration 53/1000 | Loss: 0.00001922
Iteration 54/1000 | Loss: 0.00001922
Iteration 55/1000 | Loss: 0.00001922
Iteration 56/1000 | Loss: 0.00001922
Iteration 57/1000 | Loss: 0.00001922
Iteration 58/1000 | Loss: 0.00001922
Iteration 59/1000 | Loss: 0.00001922
Iteration 60/1000 | Loss: 0.00001922
Iteration 61/1000 | Loss: 0.00001921
Iteration 62/1000 | Loss: 0.00001918
Iteration 63/1000 | Loss: 0.00001918
Iteration 64/1000 | Loss: 0.00001917
Iteration 65/1000 | Loss: 0.00001916
Iteration 66/1000 | Loss: 0.00001916
Iteration 67/1000 | Loss: 0.00001916
Iteration 68/1000 | Loss: 0.00001915
Iteration 69/1000 | Loss: 0.00001915
Iteration 70/1000 | Loss: 0.00001915
Iteration 71/1000 | Loss: 0.00001914
Iteration 72/1000 | Loss: 0.00001914
Iteration 73/1000 | Loss: 0.00001914
Iteration 74/1000 | Loss: 0.00001914
Iteration 75/1000 | Loss: 0.00001914
Iteration 76/1000 | Loss: 0.00001914
Iteration 77/1000 | Loss: 0.00001914
Iteration 78/1000 | Loss: 0.00001914
Iteration 79/1000 | Loss: 0.00001914
Iteration 80/1000 | Loss: 0.00001913
Iteration 81/1000 | Loss: 0.00001913
Iteration 82/1000 | Loss: 0.00001913
Iteration 83/1000 | Loss: 0.00001913
Iteration 84/1000 | Loss: 0.00001913
Iteration 85/1000 | Loss: 0.00001913
Iteration 86/1000 | Loss: 0.00001913
Iteration 87/1000 | Loss: 0.00001913
Iteration 88/1000 | Loss: 0.00001912
Iteration 89/1000 | Loss: 0.00001912
Iteration 90/1000 | Loss: 0.00001912
Iteration 91/1000 | Loss: 0.00001912
Iteration 92/1000 | Loss: 0.00001912
Iteration 93/1000 | Loss: 0.00001912
Iteration 94/1000 | Loss: 0.00001912
Iteration 95/1000 | Loss: 0.00001911
Iteration 96/1000 | Loss: 0.00001911
Iteration 97/1000 | Loss: 0.00001911
Iteration 98/1000 | Loss: 0.00001911
Iteration 99/1000 | Loss: 0.00001911
Iteration 100/1000 | Loss: 0.00001911
Iteration 101/1000 | Loss: 0.00001911
Iteration 102/1000 | Loss: 0.00001911
Iteration 103/1000 | Loss: 0.00001910
Iteration 104/1000 | Loss: 0.00001910
Iteration 105/1000 | Loss: 0.00001910
Iteration 106/1000 | Loss: 0.00001910
Iteration 107/1000 | Loss: 0.00001910
Iteration 108/1000 | Loss: 0.00001910
Iteration 109/1000 | Loss: 0.00001910
Iteration 110/1000 | Loss: 0.00001910
Iteration 111/1000 | Loss: 0.00001909
Iteration 112/1000 | Loss: 0.00001909
Iteration 113/1000 | Loss: 0.00001909
Iteration 114/1000 | Loss: 0.00001909
Iteration 115/1000 | Loss: 0.00001909
Iteration 116/1000 | Loss: 0.00001909
Iteration 117/1000 | Loss: 0.00001909
Iteration 118/1000 | Loss: 0.00001909
Iteration 119/1000 | Loss: 0.00001909
Iteration 120/1000 | Loss: 0.00001909
Iteration 121/1000 | Loss: 0.00001909
Iteration 122/1000 | Loss: 0.00001909
Iteration 123/1000 | Loss: 0.00001908
Iteration 124/1000 | Loss: 0.00001908
Iteration 125/1000 | Loss: 0.00001908
Iteration 126/1000 | Loss: 0.00001908
Iteration 127/1000 | Loss: 0.00001908
Iteration 128/1000 | Loss: 0.00001908
Iteration 129/1000 | Loss: 0.00001908
Iteration 130/1000 | Loss: 0.00001908
Iteration 131/1000 | Loss: 0.00001908
Iteration 132/1000 | Loss: 0.00001908
Iteration 133/1000 | Loss: 0.00001908
Iteration 134/1000 | Loss: 0.00001908
Iteration 135/1000 | Loss: 0.00001908
Iteration 136/1000 | Loss: 0.00001908
Iteration 137/1000 | Loss: 0.00001908
Iteration 138/1000 | Loss: 0.00001907
Iteration 139/1000 | Loss: 0.00001907
Iteration 140/1000 | Loss: 0.00001907
Iteration 141/1000 | Loss: 0.00001907
Iteration 142/1000 | Loss: 0.00001907
Iteration 143/1000 | Loss: 0.00001907
Iteration 144/1000 | Loss: 0.00001907
Iteration 145/1000 | Loss: 0.00001907
Iteration 146/1000 | Loss: 0.00001907
Iteration 147/1000 | Loss: 0.00001907
Iteration 148/1000 | Loss: 0.00001907
Iteration 149/1000 | Loss: 0.00001907
Iteration 150/1000 | Loss: 0.00001907
Iteration 151/1000 | Loss: 0.00001907
Iteration 152/1000 | Loss: 0.00001907
Iteration 153/1000 | Loss: 0.00001907
Iteration 154/1000 | Loss: 0.00001907
Iteration 155/1000 | Loss: 0.00001907
Iteration 156/1000 | Loss: 0.00001907
Iteration 157/1000 | Loss: 0.00001907
Iteration 158/1000 | Loss: 0.00001907
Iteration 159/1000 | Loss: 0.00001907
Iteration 160/1000 | Loss: 0.00001907
Iteration 161/1000 | Loss: 0.00001907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.906807483464945e-05, 1.906807483464945e-05, 1.906807483464945e-05, 1.906807483464945e-05, 1.906807483464945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.906807483464945e-05

Optimization complete. Final v2v error: 3.6337811946868896 mm

Highest mean error: 5.664459705352783 mm for frame 98

Lowest mean error: 2.9755451679229736 mm for frame 18

Saving results

Total time: 94.50900053977966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874561
Iteration 2/25 | Loss: 0.00135811
Iteration 3/25 | Loss: 0.00130140
Iteration 4/25 | Loss: 0.00129406
Iteration 5/25 | Loss: 0.00129203
Iteration 6/25 | Loss: 0.00129203
Iteration 7/25 | Loss: 0.00129203
Iteration 8/25 | Loss: 0.00129203
Iteration 9/25 | Loss: 0.00129203
Iteration 10/25 | Loss: 0.00129203
Iteration 11/25 | Loss: 0.00129203
Iteration 12/25 | Loss: 0.00129203
Iteration 13/25 | Loss: 0.00129203
Iteration 14/25 | Loss: 0.00129203
Iteration 15/25 | Loss: 0.00129203
Iteration 16/25 | Loss: 0.00129203
Iteration 17/25 | Loss: 0.00129203
Iteration 18/25 | Loss: 0.00129203
Iteration 19/25 | Loss: 0.00129203
Iteration 20/25 | Loss: 0.00129203
Iteration 21/25 | Loss: 0.00129203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001292029395699501, 0.001292029395699501, 0.001292029395699501, 0.001292029395699501, 0.001292029395699501]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001292029395699501

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30433631
Iteration 2/25 | Loss: 0.00106412
Iteration 3/25 | Loss: 0.00106412
Iteration 4/25 | Loss: 0.00106412
Iteration 5/25 | Loss: 0.00106412
Iteration 6/25 | Loss: 0.00106412
Iteration 7/25 | Loss: 0.00106412
Iteration 8/25 | Loss: 0.00106412
Iteration 9/25 | Loss: 0.00106412
Iteration 10/25 | Loss: 0.00106412
Iteration 11/25 | Loss: 0.00106412
Iteration 12/25 | Loss: 0.00106412
Iteration 13/25 | Loss: 0.00106412
Iteration 14/25 | Loss: 0.00106412
Iteration 15/25 | Loss: 0.00106412
Iteration 16/25 | Loss: 0.00106412
Iteration 17/25 | Loss: 0.00106412
Iteration 18/25 | Loss: 0.00106412
Iteration 19/25 | Loss: 0.00106411
Iteration 20/25 | Loss: 0.00106412
Iteration 21/25 | Loss: 0.00106411
Iteration 22/25 | Loss: 0.00106412
Iteration 23/25 | Loss: 0.00106411
Iteration 24/25 | Loss: 0.00106411
Iteration 25/25 | Loss: 0.00106411

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106411
Iteration 2/1000 | Loss: 0.00002983
Iteration 3/1000 | Loss: 0.00001965
Iteration 4/1000 | Loss: 0.00001825
Iteration 5/1000 | Loss: 0.00001820
Iteration 6/1000 | Loss: 0.00001766
Iteration 7/1000 | Loss: 0.00001722
Iteration 8/1000 | Loss: 0.00001697
Iteration 9/1000 | Loss: 0.00001687
Iteration 10/1000 | Loss: 0.00001660
Iteration 11/1000 | Loss: 0.00001637
Iteration 12/1000 | Loss: 0.00001623
Iteration 13/1000 | Loss: 0.00001613
Iteration 14/1000 | Loss: 0.00001607
Iteration 15/1000 | Loss: 0.00001606
Iteration 16/1000 | Loss: 0.00001606
Iteration 17/1000 | Loss: 0.00001606
Iteration 18/1000 | Loss: 0.00001604
Iteration 19/1000 | Loss: 0.00001604
Iteration 20/1000 | Loss: 0.00001603
Iteration 21/1000 | Loss: 0.00001603
Iteration 22/1000 | Loss: 0.00001601
Iteration 23/1000 | Loss: 0.00001599
Iteration 24/1000 | Loss: 0.00001597
Iteration 25/1000 | Loss: 0.00001585
Iteration 26/1000 | Loss: 0.00001584
Iteration 27/1000 | Loss: 0.00001582
Iteration 28/1000 | Loss: 0.00001582
Iteration 29/1000 | Loss: 0.00001579
Iteration 30/1000 | Loss: 0.00001579
Iteration 31/1000 | Loss: 0.00001573
Iteration 32/1000 | Loss: 0.00001573
Iteration 33/1000 | Loss: 0.00001572
Iteration 34/1000 | Loss: 0.00001570
Iteration 35/1000 | Loss: 0.00001569
Iteration 36/1000 | Loss: 0.00001569
Iteration 37/1000 | Loss: 0.00001568
Iteration 38/1000 | Loss: 0.00001568
Iteration 39/1000 | Loss: 0.00001568
Iteration 40/1000 | Loss: 0.00001568
Iteration 41/1000 | Loss: 0.00001567
Iteration 42/1000 | Loss: 0.00001567
Iteration 43/1000 | Loss: 0.00001566
Iteration 44/1000 | Loss: 0.00001566
Iteration 45/1000 | Loss: 0.00001566
Iteration 46/1000 | Loss: 0.00001565
Iteration 47/1000 | Loss: 0.00001565
Iteration 48/1000 | Loss: 0.00001565
Iteration 49/1000 | Loss: 0.00001565
Iteration 50/1000 | Loss: 0.00001565
Iteration 51/1000 | Loss: 0.00001565
Iteration 52/1000 | Loss: 0.00001565
Iteration 53/1000 | Loss: 0.00001565
Iteration 54/1000 | Loss: 0.00001565
Iteration 55/1000 | Loss: 0.00001564
Iteration 56/1000 | Loss: 0.00001564
Iteration 57/1000 | Loss: 0.00001564
Iteration 58/1000 | Loss: 0.00001564
Iteration 59/1000 | Loss: 0.00001564
Iteration 60/1000 | Loss: 0.00001564
Iteration 61/1000 | Loss: 0.00001564
Iteration 62/1000 | Loss: 0.00001564
Iteration 63/1000 | Loss: 0.00001563
Iteration 64/1000 | Loss: 0.00001563
Iteration 65/1000 | Loss: 0.00001562
Iteration 66/1000 | Loss: 0.00001562
Iteration 67/1000 | Loss: 0.00001562
Iteration 68/1000 | Loss: 0.00001562
Iteration 69/1000 | Loss: 0.00001562
Iteration 70/1000 | Loss: 0.00001562
Iteration 71/1000 | Loss: 0.00001562
Iteration 72/1000 | Loss: 0.00001562
Iteration 73/1000 | Loss: 0.00001562
Iteration 74/1000 | Loss: 0.00001561
Iteration 75/1000 | Loss: 0.00001561
Iteration 76/1000 | Loss: 0.00001561
Iteration 77/1000 | Loss: 0.00001561
Iteration 78/1000 | Loss: 0.00001561
Iteration 79/1000 | Loss: 0.00001561
Iteration 80/1000 | Loss: 0.00001561
Iteration 81/1000 | Loss: 0.00001561
Iteration 82/1000 | Loss: 0.00001561
Iteration 83/1000 | Loss: 0.00001561
Iteration 84/1000 | Loss: 0.00001561
Iteration 85/1000 | Loss: 0.00001561
Iteration 86/1000 | Loss: 0.00001561
Iteration 87/1000 | Loss: 0.00001561
Iteration 88/1000 | Loss: 0.00001561
Iteration 89/1000 | Loss: 0.00001561
Iteration 90/1000 | Loss: 0.00001560
Iteration 91/1000 | Loss: 0.00001560
Iteration 92/1000 | Loss: 0.00001560
Iteration 93/1000 | Loss: 0.00001560
Iteration 94/1000 | Loss: 0.00001560
Iteration 95/1000 | Loss: 0.00001560
Iteration 96/1000 | Loss: 0.00001560
Iteration 97/1000 | Loss: 0.00001560
Iteration 98/1000 | Loss: 0.00001559
Iteration 99/1000 | Loss: 0.00001559
Iteration 100/1000 | Loss: 0.00001559
Iteration 101/1000 | Loss: 0.00001559
Iteration 102/1000 | Loss: 0.00001559
Iteration 103/1000 | Loss: 0.00001559
Iteration 104/1000 | Loss: 0.00001558
Iteration 105/1000 | Loss: 0.00001558
Iteration 106/1000 | Loss: 0.00001558
Iteration 107/1000 | Loss: 0.00001558
Iteration 108/1000 | Loss: 0.00001558
Iteration 109/1000 | Loss: 0.00001558
Iteration 110/1000 | Loss: 0.00001557
Iteration 111/1000 | Loss: 0.00001557
Iteration 112/1000 | Loss: 0.00001557
Iteration 113/1000 | Loss: 0.00001557
Iteration 114/1000 | Loss: 0.00001556
Iteration 115/1000 | Loss: 0.00001556
Iteration 116/1000 | Loss: 0.00001556
Iteration 117/1000 | Loss: 0.00001556
Iteration 118/1000 | Loss: 0.00001556
Iteration 119/1000 | Loss: 0.00001556
Iteration 120/1000 | Loss: 0.00001556
Iteration 121/1000 | Loss: 0.00001556
Iteration 122/1000 | Loss: 0.00001555
Iteration 123/1000 | Loss: 0.00001555
Iteration 124/1000 | Loss: 0.00001555
Iteration 125/1000 | Loss: 0.00001555
Iteration 126/1000 | Loss: 0.00001555
Iteration 127/1000 | Loss: 0.00001554
Iteration 128/1000 | Loss: 0.00001554
Iteration 129/1000 | Loss: 0.00001553
Iteration 130/1000 | Loss: 0.00001553
Iteration 131/1000 | Loss: 0.00001552
Iteration 132/1000 | Loss: 0.00001552
Iteration 133/1000 | Loss: 0.00001552
Iteration 134/1000 | Loss: 0.00001552
Iteration 135/1000 | Loss: 0.00001552
Iteration 136/1000 | Loss: 0.00001552
Iteration 137/1000 | Loss: 0.00001552
Iteration 138/1000 | Loss: 0.00001551
Iteration 139/1000 | Loss: 0.00001551
Iteration 140/1000 | Loss: 0.00001551
Iteration 141/1000 | Loss: 0.00001551
Iteration 142/1000 | Loss: 0.00001551
Iteration 143/1000 | Loss: 0.00001551
Iteration 144/1000 | Loss: 0.00001551
Iteration 145/1000 | Loss: 0.00001550
Iteration 146/1000 | Loss: 0.00001550
Iteration 147/1000 | Loss: 0.00001550
Iteration 148/1000 | Loss: 0.00001550
Iteration 149/1000 | Loss: 0.00001550
Iteration 150/1000 | Loss: 0.00001549
Iteration 151/1000 | Loss: 0.00001549
Iteration 152/1000 | Loss: 0.00001549
Iteration 153/1000 | Loss: 0.00001548
Iteration 154/1000 | Loss: 0.00001548
Iteration 155/1000 | Loss: 0.00001548
Iteration 156/1000 | Loss: 0.00001548
Iteration 157/1000 | Loss: 0.00001548
Iteration 158/1000 | Loss: 0.00001548
Iteration 159/1000 | Loss: 0.00001548
Iteration 160/1000 | Loss: 0.00001547
Iteration 161/1000 | Loss: 0.00001547
Iteration 162/1000 | Loss: 0.00001547
Iteration 163/1000 | Loss: 0.00001547
Iteration 164/1000 | Loss: 0.00001547
Iteration 165/1000 | Loss: 0.00001547
Iteration 166/1000 | Loss: 0.00001547
Iteration 167/1000 | Loss: 0.00001547
Iteration 168/1000 | Loss: 0.00001546
Iteration 169/1000 | Loss: 0.00001546
Iteration 170/1000 | Loss: 0.00001546
Iteration 171/1000 | Loss: 0.00001546
Iteration 172/1000 | Loss: 0.00001546
Iteration 173/1000 | Loss: 0.00001546
Iteration 174/1000 | Loss: 0.00001546
Iteration 175/1000 | Loss: 0.00001546
Iteration 176/1000 | Loss: 0.00001546
Iteration 177/1000 | Loss: 0.00001546
Iteration 178/1000 | Loss: 0.00001546
Iteration 179/1000 | Loss: 0.00001546
Iteration 180/1000 | Loss: 0.00001546
Iteration 181/1000 | Loss: 0.00001545
Iteration 182/1000 | Loss: 0.00001545
Iteration 183/1000 | Loss: 0.00001545
Iteration 184/1000 | Loss: 0.00001545
Iteration 185/1000 | Loss: 0.00001545
Iteration 186/1000 | Loss: 0.00001545
Iteration 187/1000 | Loss: 0.00001545
Iteration 188/1000 | Loss: 0.00001544
Iteration 189/1000 | Loss: 0.00001544
Iteration 190/1000 | Loss: 0.00001544
Iteration 191/1000 | Loss: 0.00001544
Iteration 192/1000 | Loss: 0.00001544
Iteration 193/1000 | Loss: 0.00001544
Iteration 194/1000 | Loss: 0.00001544
Iteration 195/1000 | Loss: 0.00001544
Iteration 196/1000 | Loss: 0.00001544
Iteration 197/1000 | Loss: 0.00001543
Iteration 198/1000 | Loss: 0.00001543
Iteration 199/1000 | Loss: 0.00001543
Iteration 200/1000 | Loss: 0.00001543
Iteration 201/1000 | Loss: 0.00001542
Iteration 202/1000 | Loss: 0.00001542
Iteration 203/1000 | Loss: 0.00001542
Iteration 204/1000 | Loss: 0.00001542
Iteration 205/1000 | Loss: 0.00001542
Iteration 206/1000 | Loss: 0.00001542
Iteration 207/1000 | Loss: 0.00001541
Iteration 208/1000 | Loss: 0.00001541
Iteration 209/1000 | Loss: 0.00001541
Iteration 210/1000 | Loss: 0.00001541
Iteration 211/1000 | Loss: 0.00001541
Iteration 212/1000 | Loss: 0.00001541
Iteration 213/1000 | Loss: 0.00001540
Iteration 214/1000 | Loss: 0.00001540
Iteration 215/1000 | Loss: 0.00001540
Iteration 216/1000 | Loss: 0.00001540
Iteration 217/1000 | Loss: 0.00001540
Iteration 218/1000 | Loss: 0.00001540
Iteration 219/1000 | Loss: 0.00001540
Iteration 220/1000 | Loss: 0.00001540
Iteration 221/1000 | Loss: 0.00001539
Iteration 222/1000 | Loss: 0.00001539
Iteration 223/1000 | Loss: 0.00001539
Iteration 224/1000 | Loss: 0.00001539
Iteration 225/1000 | Loss: 0.00001539
Iteration 226/1000 | Loss: 0.00001539
Iteration 227/1000 | Loss: 0.00001538
Iteration 228/1000 | Loss: 0.00001538
Iteration 229/1000 | Loss: 0.00001538
Iteration 230/1000 | Loss: 0.00001538
Iteration 231/1000 | Loss: 0.00001538
Iteration 232/1000 | Loss: 0.00001538
Iteration 233/1000 | Loss: 0.00001538
Iteration 234/1000 | Loss: 0.00001538
Iteration 235/1000 | Loss: 0.00001538
Iteration 236/1000 | Loss: 0.00001538
Iteration 237/1000 | Loss: 0.00001538
Iteration 238/1000 | Loss: 0.00001538
Iteration 239/1000 | Loss: 0.00001537
Iteration 240/1000 | Loss: 0.00001537
Iteration 241/1000 | Loss: 0.00001537
Iteration 242/1000 | Loss: 0.00001537
Iteration 243/1000 | Loss: 0.00001537
Iteration 244/1000 | Loss: 0.00001537
Iteration 245/1000 | Loss: 0.00001537
Iteration 246/1000 | Loss: 0.00001537
Iteration 247/1000 | Loss: 0.00001537
Iteration 248/1000 | Loss: 0.00001537
Iteration 249/1000 | Loss: 0.00001537
Iteration 250/1000 | Loss: 0.00001536
Iteration 251/1000 | Loss: 0.00001536
Iteration 252/1000 | Loss: 0.00001536
Iteration 253/1000 | Loss: 0.00001536
Iteration 254/1000 | Loss: 0.00001536
Iteration 255/1000 | Loss: 0.00001536
Iteration 256/1000 | Loss: 0.00001536
Iteration 257/1000 | Loss: 0.00001536
Iteration 258/1000 | Loss: 0.00001536
Iteration 259/1000 | Loss: 0.00001536
Iteration 260/1000 | Loss: 0.00001535
Iteration 261/1000 | Loss: 0.00001535
Iteration 262/1000 | Loss: 0.00001535
Iteration 263/1000 | Loss: 0.00001535
Iteration 264/1000 | Loss: 0.00001535
Iteration 265/1000 | Loss: 0.00001534
Iteration 266/1000 | Loss: 0.00001534
Iteration 267/1000 | Loss: 0.00001534
Iteration 268/1000 | Loss: 0.00001534
Iteration 269/1000 | Loss: 0.00001533
Iteration 270/1000 | Loss: 0.00001533
Iteration 271/1000 | Loss: 0.00001533
Iteration 272/1000 | Loss: 0.00001533
Iteration 273/1000 | Loss: 0.00001533
Iteration 274/1000 | Loss: 0.00001533
Iteration 275/1000 | Loss: 0.00001533
Iteration 276/1000 | Loss: 0.00001533
Iteration 277/1000 | Loss: 0.00001533
Iteration 278/1000 | Loss: 0.00001533
Iteration 279/1000 | Loss: 0.00001533
Iteration 280/1000 | Loss: 0.00001533
Iteration 281/1000 | Loss: 0.00001533
Iteration 282/1000 | Loss: 0.00001532
Iteration 283/1000 | Loss: 0.00001532
Iteration 284/1000 | Loss: 0.00001532
Iteration 285/1000 | Loss: 0.00001532
Iteration 286/1000 | Loss: 0.00001532
Iteration 287/1000 | Loss: 0.00001532
Iteration 288/1000 | Loss: 0.00001532
Iteration 289/1000 | Loss: 0.00001532
Iteration 290/1000 | Loss: 0.00001532
Iteration 291/1000 | Loss: 0.00001531
Iteration 292/1000 | Loss: 0.00001531
Iteration 293/1000 | Loss: 0.00001531
Iteration 294/1000 | Loss: 0.00001531
Iteration 295/1000 | Loss: 0.00001531
Iteration 296/1000 | Loss: 0.00001531
Iteration 297/1000 | Loss: 0.00001531
Iteration 298/1000 | Loss: 0.00001531
Iteration 299/1000 | Loss: 0.00001531
Iteration 300/1000 | Loss: 0.00001531
Iteration 301/1000 | Loss: 0.00001531
Iteration 302/1000 | Loss: 0.00001531
Iteration 303/1000 | Loss: 0.00001531
Iteration 304/1000 | Loss: 0.00001531
Iteration 305/1000 | Loss: 0.00001531
Iteration 306/1000 | Loss: 0.00001531
Iteration 307/1000 | Loss: 0.00001531
Iteration 308/1000 | Loss: 0.00001531
Iteration 309/1000 | Loss: 0.00001531
Iteration 310/1000 | Loss: 0.00001531
Iteration 311/1000 | Loss: 0.00001530
Iteration 312/1000 | Loss: 0.00001530
Iteration 313/1000 | Loss: 0.00001530
Iteration 314/1000 | Loss: 0.00001530
Iteration 315/1000 | Loss: 0.00001530
Iteration 316/1000 | Loss: 0.00001530
Iteration 317/1000 | Loss: 0.00001530
Iteration 318/1000 | Loss: 0.00001530
Iteration 319/1000 | Loss: 0.00001530
Iteration 320/1000 | Loss: 0.00001530
Iteration 321/1000 | Loss: 0.00001530
Iteration 322/1000 | Loss: 0.00001530
Iteration 323/1000 | Loss: 0.00001530
Iteration 324/1000 | Loss: 0.00001530
Iteration 325/1000 | Loss: 0.00001530
Iteration 326/1000 | Loss: 0.00001530
Iteration 327/1000 | Loss: 0.00001530
Iteration 328/1000 | Loss: 0.00001530
Iteration 329/1000 | Loss: 0.00001530
Iteration 330/1000 | Loss: 0.00001530
Iteration 331/1000 | Loss: 0.00001530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 331. Stopping optimization.
Last 5 losses: [1.5302553947549313e-05, 1.5302553947549313e-05, 1.5302553947549313e-05, 1.5302553947549313e-05, 1.5302553947549313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5302553947549313e-05

Optimization complete. Final v2v error: 3.284414291381836 mm

Highest mean error: 3.4595069885253906 mm for frame 44

Lowest mean error: 3.1511452198028564 mm for frame 196

Saving results

Total time: 47.286473751068115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394331
Iteration 2/25 | Loss: 0.00146432
Iteration 3/25 | Loss: 0.00124577
Iteration 4/25 | Loss: 0.00121946
Iteration 5/25 | Loss: 0.00121689
Iteration 6/25 | Loss: 0.00121666
Iteration 7/25 | Loss: 0.00121666
Iteration 8/25 | Loss: 0.00121666
Iteration 9/25 | Loss: 0.00121666
Iteration 10/25 | Loss: 0.00121666
Iteration 11/25 | Loss: 0.00121666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012166565284132957, 0.0012166565284132957, 0.0012166565284132957, 0.0012166565284132957, 0.0012166565284132957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012166565284132957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35289454
Iteration 2/25 | Loss: 0.00106257
Iteration 3/25 | Loss: 0.00106257
Iteration 4/25 | Loss: 0.00106257
Iteration 5/25 | Loss: 0.00106257
Iteration 6/25 | Loss: 0.00106257
Iteration 7/25 | Loss: 0.00106257
Iteration 8/25 | Loss: 0.00106257
Iteration 9/25 | Loss: 0.00106257
Iteration 10/25 | Loss: 0.00106257
Iteration 11/25 | Loss: 0.00106257
Iteration 12/25 | Loss: 0.00106257
Iteration 13/25 | Loss: 0.00106257
Iteration 14/25 | Loss: 0.00106257
Iteration 15/25 | Loss: 0.00106257
Iteration 16/25 | Loss: 0.00106257
Iteration 17/25 | Loss: 0.00106257
Iteration 18/25 | Loss: 0.00106257
Iteration 19/25 | Loss: 0.00106257
Iteration 20/25 | Loss: 0.00106257
Iteration 21/25 | Loss: 0.00106257
Iteration 22/25 | Loss: 0.00106257
Iteration 23/25 | Loss: 0.00106257
Iteration 24/25 | Loss: 0.00106257
Iteration 25/25 | Loss: 0.00106257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106257
Iteration 2/1000 | Loss: 0.00003236
Iteration 3/1000 | Loss: 0.00001889
Iteration 4/1000 | Loss: 0.00001642
Iteration 5/1000 | Loss: 0.00001482
Iteration 6/1000 | Loss: 0.00001397
Iteration 7/1000 | Loss: 0.00001332
Iteration 8/1000 | Loss: 0.00001294
Iteration 9/1000 | Loss: 0.00001264
Iteration 10/1000 | Loss: 0.00001236
Iteration 11/1000 | Loss: 0.00001219
Iteration 12/1000 | Loss: 0.00001209
Iteration 13/1000 | Loss: 0.00001209
Iteration 14/1000 | Loss: 0.00001199
Iteration 15/1000 | Loss: 0.00001188
Iteration 16/1000 | Loss: 0.00001186
Iteration 17/1000 | Loss: 0.00001186
Iteration 18/1000 | Loss: 0.00001186
Iteration 19/1000 | Loss: 0.00001186
Iteration 20/1000 | Loss: 0.00001185
Iteration 21/1000 | Loss: 0.00001185
Iteration 22/1000 | Loss: 0.00001184
Iteration 23/1000 | Loss: 0.00001183
Iteration 24/1000 | Loss: 0.00001183
Iteration 25/1000 | Loss: 0.00001182
Iteration 26/1000 | Loss: 0.00001180
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001179
Iteration 29/1000 | Loss: 0.00001179
Iteration 30/1000 | Loss: 0.00001178
Iteration 31/1000 | Loss: 0.00001178
Iteration 32/1000 | Loss: 0.00001178
Iteration 33/1000 | Loss: 0.00001177
Iteration 34/1000 | Loss: 0.00001177
Iteration 35/1000 | Loss: 0.00001176
Iteration 36/1000 | Loss: 0.00001176
Iteration 37/1000 | Loss: 0.00001175
Iteration 38/1000 | Loss: 0.00001172
Iteration 39/1000 | Loss: 0.00001172
Iteration 40/1000 | Loss: 0.00001172
Iteration 41/1000 | Loss: 0.00001172
Iteration 42/1000 | Loss: 0.00001172
Iteration 43/1000 | Loss: 0.00001171
Iteration 44/1000 | Loss: 0.00001171
Iteration 45/1000 | Loss: 0.00001171
Iteration 46/1000 | Loss: 0.00001169
Iteration 47/1000 | Loss: 0.00001168
Iteration 48/1000 | Loss: 0.00001168
Iteration 49/1000 | Loss: 0.00001168
Iteration 50/1000 | Loss: 0.00001167
Iteration 51/1000 | Loss: 0.00001167
Iteration 52/1000 | Loss: 0.00001167
Iteration 53/1000 | Loss: 0.00001167
Iteration 54/1000 | Loss: 0.00001166
Iteration 55/1000 | Loss: 0.00001166
Iteration 56/1000 | Loss: 0.00001165
Iteration 57/1000 | Loss: 0.00001164
Iteration 58/1000 | Loss: 0.00001163
Iteration 59/1000 | Loss: 0.00001162
Iteration 60/1000 | Loss: 0.00001162
Iteration 61/1000 | Loss: 0.00001162
Iteration 62/1000 | Loss: 0.00001161
Iteration 63/1000 | Loss: 0.00001161
Iteration 64/1000 | Loss: 0.00001161
Iteration 65/1000 | Loss: 0.00001160
Iteration 66/1000 | Loss: 0.00001160
Iteration 67/1000 | Loss: 0.00001160
Iteration 68/1000 | Loss: 0.00001158
Iteration 69/1000 | Loss: 0.00001158
Iteration 70/1000 | Loss: 0.00001158
Iteration 71/1000 | Loss: 0.00001158
Iteration 72/1000 | Loss: 0.00001158
Iteration 73/1000 | Loss: 0.00001157
Iteration 74/1000 | Loss: 0.00001157
Iteration 75/1000 | Loss: 0.00001157
Iteration 76/1000 | Loss: 0.00001156
Iteration 77/1000 | Loss: 0.00001154
Iteration 78/1000 | Loss: 0.00001154
Iteration 79/1000 | Loss: 0.00001153
Iteration 80/1000 | Loss: 0.00001153
Iteration 81/1000 | Loss: 0.00001153
Iteration 82/1000 | Loss: 0.00001153
Iteration 83/1000 | Loss: 0.00001152
Iteration 84/1000 | Loss: 0.00001152
Iteration 85/1000 | Loss: 0.00001152
Iteration 86/1000 | Loss: 0.00001152
Iteration 87/1000 | Loss: 0.00001151
Iteration 88/1000 | Loss: 0.00001151
Iteration 89/1000 | Loss: 0.00001151
Iteration 90/1000 | Loss: 0.00001151
Iteration 91/1000 | Loss: 0.00001151
Iteration 92/1000 | Loss: 0.00001151
Iteration 93/1000 | Loss: 0.00001151
Iteration 94/1000 | Loss: 0.00001151
Iteration 95/1000 | Loss: 0.00001151
Iteration 96/1000 | Loss: 0.00001151
Iteration 97/1000 | Loss: 0.00001151
Iteration 98/1000 | Loss: 0.00001151
Iteration 99/1000 | Loss: 0.00001150
Iteration 100/1000 | Loss: 0.00001150
Iteration 101/1000 | Loss: 0.00001150
Iteration 102/1000 | Loss: 0.00001149
Iteration 103/1000 | Loss: 0.00001149
Iteration 104/1000 | Loss: 0.00001149
Iteration 105/1000 | Loss: 0.00001148
Iteration 106/1000 | Loss: 0.00001147
Iteration 107/1000 | Loss: 0.00001147
Iteration 108/1000 | Loss: 0.00001146
Iteration 109/1000 | Loss: 0.00001146
Iteration 110/1000 | Loss: 0.00001146
Iteration 111/1000 | Loss: 0.00001145
Iteration 112/1000 | Loss: 0.00001145
Iteration 113/1000 | Loss: 0.00001145
Iteration 114/1000 | Loss: 0.00001145
Iteration 115/1000 | Loss: 0.00001145
Iteration 116/1000 | Loss: 0.00001144
Iteration 117/1000 | Loss: 0.00001144
Iteration 118/1000 | Loss: 0.00001144
Iteration 119/1000 | Loss: 0.00001144
Iteration 120/1000 | Loss: 0.00001143
Iteration 121/1000 | Loss: 0.00001143
Iteration 122/1000 | Loss: 0.00001143
Iteration 123/1000 | Loss: 0.00001143
Iteration 124/1000 | Loss: 0.00001142
Iteration 125/1000 | Loss: 0.00001142
Iteration 126/1000 | Loss: 0.00001142
Iteration 127/1000 | Loss: 0.00001142
Iteration 128/1000 | Loss: 0.00001142
Iteration 129/1000 | Loss: 0.00001142
Iteration 130/1000 | Loss: 0.00001141
Iteration 131/1000 | Loss: 0.00001141
Iteration 132/1000 | Loss: 0.00001141
Iteration 133/1000 | Loss: 0.00001140
Iteration 134/1000 | Loss: 0.00001140
Iteration 135/1000 | Loss: 0.00001140
Iteration 136/1000 | Loss: 0.00001140
Iteration 137/1000 | Loss: 0.00001140
Iteration 138/1000 | Loss: 0.00001140
Iteration 139/1000 | Loss: 0.00001140
Iteration 140/1000 | Loss: 0.00001140
Iteration 141/1000 | Loss: 0.00001140
Iteration 142/1000 | Loss: 0.00001140
Iteration 143/1000 | Loss: 0.00001140
Iteration 144/1000 | Loss: 0.00001139
Iteration 145/1000 | Loss: 0.00001139
Iteration 146/1000 | Loss: 0.00001139
Iteration 147/1000 | Loss: 0.00001139
Iteration 148/1000 | Loss: 0.00001139
Iteration 149/1000 | Loss: 0.00001139
Iteration 150/1000 | Loss: 0.00001139
Iteration 151/1000 | Loss: 0.00001139
Iteration 152/1000 | Loss: 0.00001139
Iteration 153/1000 | Loss: 0.00001139
Iteration 154/1000 | Loss: 0.00001139
Iteration 155/1000 | Loss: 0.00001139
Iteration 156/1000 | Loss: 0.00001138
Iteration 157/1000 | Loss: 0.00001138
Iteration 158/1000 | Loss: 0.00001138
Iteration 159/1000 | Loss: 0.00001138
Iteration 160/1000 | Loss: 0.00001138
Iteration 161/1000 | Loss: 0.00001138
Iteration 162/1000 | Loss: 0.00001138
Iteration 163/1000 | Loss: 0.00001138
Iteration 164/1000 | Loss: 0.00001138
Iteration 165/1000 | Loss: 0.00001137
Iteration 166/1000 | Loss: 0.00001137
Iteration 167/1000 | Loss: 0.00001137
Iteration 168/1000 | Loss: 0.00001137
Iteration 169/1000 | Loss: 0.00001137
Iteration 170/1000 | Loss: 0.00001137
Iteration 171/1000 | Loss: 0.00001136
Iteration 172/1000 | Loss: 0.00001136
Iteration 173/1000 | Loss: 0.00001136
Iteration 174/1000 | Loss: 0.00001136
Iteration 175/1000 | Loss: 0.00001136
Iteration 176/1000 | Loss: 0.00001136
Iteration 177/1000 | Loss: 0.00001136
Iteration 178/1000 | Loss: 0.00001136
Iteration 179/1000 | Loss: 0.00001136
Iteration 180/1000 | Loss: 0.00001136
Iteration 181/1000 | Loss: 0.00001135
Iteration 182/1000 | Loss: 0.00001135
Iteration 183/1000 | Loss: 0.00001135
Iteration 184/1000 | Loss: 0.00001134
Iteration 185/1000 | Loss: 0.00001134
Iteration 186/1000 | Loss: 0.00001134
Iteration 187/1000 | Loss: 0.00001134
Iteration 188/1000 | Loss: 0.00001133
Iteration 189/1000 | Loss: 0.00001133
Iteration 190/1000 | Loss: 0.00001133
Iteration 191/1000 | Loss: 0.00001133
Iteration 192/1000 | Loss: 0.00001133
Iteration 193/1000 | Loss: 0.00001133
Iteration 194/1000 | Loss: 0.00001132
Iteration 195/1000 | Loss: 0.00001132
Iteration 196/1000 | Loss: 0.00001132
Iteration 197/1000 | Loss: 0.00001132
Iteration 198/1000 | Loss: 0.00001132
Iteration 199/1000 | Loss: 0.00001132
Iteration 200/1000 | Loss: 0.00001131
Iteration 201/1000 | Loss: 0.00001131
Iteration 202/1000 | Loss: 0.00001131
Iteration 203/1000 | Loss: 0.00001131
Iteration 204/1000 | Loss: 0.00001131
Iteration 205/1000 | Loss: 0.00001131
Iteration 206/1000 | Loss: 0.00001131
Iteration 207/1000 | Loss: 0.00001131
Iteration 208/1000 | Loss: 0.00001131
Iteration 209/1000 | Loss: 0.00001131
Iteration 210/1000 | Loss: 0.00001131
Iteration 211/1000 | Loss: 0.00001131
Iteration 212/1000 | Loss: 0.00001131
Iteration 213/1000 | Loss: 0.00001131
Iteration 214/1000 | Loss: 0.00001131
Iteration 215/1000 | Loss: 0.00001130
Iteration 216/1000 | Loss: 0.00001130
Iteration 217/1000 | Loss: 0.00001130
Iteration 218/1000 | Loss: 0.00001130
Iteration 219/1000 | Loss: 0.00001130
Iteration 220/1000 | Loss: 0.00001129
Iteration 221/1000 | Loss: 0.00001129
Iteration 222/1000 | Loss: 0.00001129
Iteration 223/1000 | Loss: 0.00001129
Iteration 224/1000 | Loss: 0.00001129
Iteration 225/1000 | Loss: 0.00001129
Iteration 226/1000 | Loss: 0.00001128
Iteration 227/1000 | Loss: 0.00001128
Iteration 228/1000 | Loss: 0.00001128
Iteration 229/1000 | Loss: 0.00001128
Iteration 230/1000 | Loss: 0.00001128
Iteration 231/1000 | Loss: 0.00001128
Iteration 232/1000 | Loss: 0.00001128
Iteration 233/1000 | Loss: 0.00001128
Iteration 234/1000 | Loss: 0.00001128
Iteration 235/1000 | Loss: 0.00001128
Iteration 236/1000 | Loss: 0.00001128
Iteration 237/1000 | Loss: 0.00001127
Iteration 238/1000 | Loss: 0.00001127
Iteration 239/1000 | Loss: 0.00001127
Iteration 240/1000 | Loss: 0.00001127
Iteration 241/1000 | Loss: 0.00001127
Iteration 242/1000 | Loss: 0.00001127
Iteration 243/1000 | Loss: 0.00001127
Iteration 244/1000 | Loss: 0.00001127
Iteration 245/1000 | Loss: 0.00001127
Iteration 246/1000 | Loss: 0.00001126
Iteration 247/1000 | Loss: 0.00001126
Iteration 248/1000 | Loss: 0.00001126
Iteration 249/1000 | Loss: 0.00001126
Iteration 250/1000 | Loss: 0.00001126
Iteration 251/1000 | Loss: 0.00001126
Iteration 252/1000 | Loss: 0.00001126
Iteration 253/1000 | Loss: 0.00001126
Iteration 254/1000 | Loss: 0.00001126
Iteration 255/1000 | Loss: 0.00001126
Iteration 256/1000 | Loss: 0.00001126
Iteration 257/1000 | Loss: 0.00001126
Iteration 258/1000 | Loss: 0.00001126
Iteration 259/1000 | Loss: 0.00001126
Iteration 260/1000 | Loss: 0.00001126
Iteration 261/1000 | Loss: 0.00001126
Iteration 262/1000 | Loss: 0.00001126
Iteration 263/1000 | Loss: 0.00001126
Iteration 264/1000 | Loss: 0.00001126
Iteration 265/1000 | Loss: 0.00001126
Iteration 266/1000 | Loss: 0.00001126
Iteration 267/1000 | Loss: 0.00001126
Iteration 268/1000 | Loss: 0.00001126
Iteration 269/1000 | Loss: 0.00001126
Iteration 270/1000 | Loss: 0.00001126
Iteration 271/1000 | Loss: 0.00001126
Iteration 272/1000 | Loss: 0.00001126
Iteration 273/1000 | Loss: 0.00001126
Iteration 274/1000 | Loss: 0.00001126
Iteration 275/1000 | Loss: 0.00001126
Iteration 276/1000 | Loss: 0.00001126
Iteration 277/1000 | Loss: 0.00001126
Iteration 278/1000 | Loss: 0.00001126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 278. Stopping optimization.
Last 5 losses: [1.1261014151386917e-05, 1.1261014151386917e-05, 1.1261014151386917e-05, 1.1261014151386917e-05, 1.1261014151386917e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1261014151386917e-05

Optimization complete. Final v2v error: 2.8678338527679443 mm

Highest mean error: 3.447176933288574 mm for frame 80

Lowest mean error: 2.6190242767333984 mm for frame 206

Saving results

Total time: 49.99239206314087
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780594
Iteration 2/25 | Loss: 0.00155367
Iteration 3/25 | Loss: 0.00128598
Iteration 4/25 | Loss: 0.00125607
Iteration 5/25 | Loss: 0.00124834
Iteration 6/25 | Loss: 0.00124479
Iteration 7/25 | Loss: 0.00124427
Iteration 8/25 | Loss: 0.00124412
Iteration 9/25 | Loss: 0.00124401
Iteration 10/25 | Loss: 0.00124397
Iteration 11/25 | Loss: 0.00124397
Iteration 12/25 | Loss: 0.00124397
Iteration 13/25 | Loss: 0.00124397
Iteration 14/25 | Loss: 0.00124397
Iteration 15/25 | Loss: 0.00124397
Iteration 16/25 | Loss: 0.00124397
Iteration 17/25 | Loss: 0.00124397
Iteration 18/25 | Loss: 0.00124397
Iteration 19/25 | Loss: 0.00124397
Iteration 20/25 | Loss: 0.00124397
Iteration 21/25 | Loss: 0.00124397
Iteration 22/25 | Loss: 0.00124397
Iteration 23/25 | Loss: 0.00124397
Iteration 24/25 | Loss: 0.00124397
Iteration 25/25 | Loss: 0.00124397

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13838553
Iteration 2/25 | Loss: 0.00109899
Iteration 3/25 | Loss: 0.00109899
Iteration 4/25 | Loss: 0.00109899
Iteration 5/25 | Loss: 0.00109899
Iteration 6/25 | Loss: 0.00109899
Iteration 7/25 | Loss: 0.00109899
Iteration 8/25 | Loss: 0.00109899
Iteration 9/25 | Loss: 0.00109899
Iteration 10/25 | Loss: 0.00109899
Iteration 11/25 | Loss: 0.00109899
Iteration 12/25 | Loss: 0.00109899
Iteration 13/25 | Loss: 0.00109899
Iteration 14/25 | Loss: 0.00109899
Iteration 15/25 | Loss: 0.00109899
Iteration 16/25 | Loss: 0.00109899
Iteration 17/25 | Loss: 0.00109899
Iteration 18/25 | Loss: 0.00109899
Iteration 19/25 | Loss: 0.00109899
Iteration 20/25 | Loss: 0.00109899
Iteration 21/25 | Loss: 0.00109899
Iteration 22/25 | Loss: 0.00109899
Iteration 23/25 | Loss: 0.00109899
Iteration 24/25 | Loss: 0.00109899
Iteration 25/25 | Loss: 0.00109899

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109899
Iteration 2/1000 | Loss: 0.00002465
Iteration 3/1000 | Loss: 0.00002403
Iteration 4/1000 | Loss: 0.00001904
Iteration 5/1000 | Loss: 0.00001814
Iteration 6/1000 | Loss: 0.00001763
Iteration 7/1000 | Loss: 0.00001724
Iteration 8/1000 | Loss: 0.00001958
Iteration 9/1000 | Loss: 0.00001651
Iteration 10/1000 | Loss: 0.00001624
Iteration 11/1000 | Loss: 0.00001603
Iteration 12/1000 | Loss: 0.00001602
Iteration 13/1000 | Loss: 0.00001602
Iteration 14/1000 | Loss: 0.00001600
Iteration 15/1000 | Loss: 0.00001594
Iteration 16/1000 | Loss: 0.00001589
Iteration 17/1000 | Loss: 0.00001587
Iteration 18/1000 | Loss: 0.00001586
Iteration 19/1000 | Loss: 0.00001586
Iteration 20/1000 | Loss: 0.00001586
Iteration 21/1000 | Loss: 0.00001584
Iteration 22/1000 | Loss: 0.00001583
Iteration 23/1000 | Loss: 0.00001583
Iteration 24/1000 | Loss: 0.00001579
Iteration 25/1000 | Loss: 0.00001571
Iteration 26/1000 | Loss: 0.00001570
Iteration 27/1000 | Loss: 0.00001569
Iteration 28/1000 | Loss: 0.00001569
Iteration 29/1000 | Loss: 0.00001568
Iteration 30/1000 | Loss: 0.00001566
Iteration 31/1000 | Loss: 0.00001566
Iteration 32/1000 | Loss: 0.00001566
Iteration 33/1000 | Loss: 0.00001565
Iteration 34/1000 | Loss: 0.00001565
Iteration 35/1000 | Loss: 0.00001565
Iteration 36/1000 | Loss: 0.00001564
Iteration 37/1000 | Loss: 0.00001564
Iteration 38/1000 | Loss: 0.00001564
Iteration 39/1000 | Loss: 0.00001563
Iteration 40/1000 | Loss: 0.00001563
Iteration 41/1000 | Loss: 0.00001563
Iteration 42/1000 | Loss: 0.00001563
Iteration 43/1000 | Loss: 0.00001563
Iteration 44/1000 | Loss: 0.00001562
Iteration 45/1000 | Loss: 0.00001562
Iteration 46/1000 | Loss: 0.00001562
Iteration 47/1000 | Loss: 0.00001562
Iteration 48/1000 | Loss: 0.00001562
Iteration 49/1000 | Loss: 0.00001562
Iteration 50/1000 | Loss: 0.00001562
Iteration 51/1000 | Loss: 0.00001561
Iteration 52/1000 | Loss: 0.00001561
Iteration 53/1000 | Loss: 0.00001560
Iteration 54/1000 | Loss: 0.00001560
Iteration 55/1000 | Loss: 0.00001560
Iteration 56/1000 | Loss: 0.00001559
Iteration 57/1000 | Loss: 0.00001559
Iteration 58/1000 | Loss: 0.00001559
Iteration 59/1000 | Loss: 0.00001558
Iteration 60/1000 | Loss: 0.00001558
Iteration 61/1000 | Loss: 0.00001558
Iteration 62/1000 | Loss: 0.00001558
Iteration 63/1000 | Loss: 0.00001558
Iteration 64/1000 | Loss: 0.00001558
Iteration 65/1000 | Loss: 0.00001558
Iteration 66/1000 | Loss: 0.00001558
Iteration 67/1000 | Loss: 0.00001558
Iteration 68/1000 | Loss: 0.00001558
Iteration 69/1000 | Loss: 0.00001557
Iteration 70/1000 | Loss: 0.00001557
Iteration 71/1000 | Loss: 0.00001557
Iteration 72/1000 | Loss: 0.00001557
Iteration 73/1000 | Loss: 0.00001557
Iteration 74/1000 | Loss: 0.00001556
Iteration 75/1000 | Loss: 0.00001556
Iteration 76/1000 | Loss: 0.00001556
Iteration 77/1000 | Loss: 0.00001556
Iteration 78/1000 | Loss: 0.00001556
Iteration 79/1000 | Loss: 0.00001556
Iteration 80/1000 | Loss: 0.00001556
Iteration 81/1000 | Loss: 0.00001556
Iteration 82/1000 | Loss: 0.00001556
Iteration 83/1000 | Loss: 0.00001556
Iteration 84/1000 | Loss: 0.00001555
Iteration 85/1000 | Loss: 0.00001555
Iteration 86/1000 | Loss: 0.00001555
Iteration 87/1000 | Loss: 0.00001555
Iteration 88/1000 | Loss: 0.00001555
Iteration 89/1000 | Loss: 0.00001555
Iteration 90/1000 | Loss: 0.00001554
Iteration 91/1000 | Loss: 0.00001554
Iteration 92/1000 | Loss: 0.00001554
Iteration 93/1000 | Loss: 0.00001554
Iteration 94/1000 | Loss: 0.00001554
Iteration 95/1000 | Loss: 0.00001554
Iteration 96/1000 | Loss: 0.00001554
Iteration 97/1000 | Loss: 0.00001553
Iteration 98/1000 | Loss: 0.00001553
Iteration 99/1000 | Loss: 0.00001553
Iteration 100/1000 | Loss: 0.00001553
Iteration 101/1000 | Loss: 0.00001553
Iteration 102/1000 | Loss: 0.00001553
Iteration 103/1000 | Loss: 0.00001552
Iteration 104/1000 | Loss: 0.00001552
Iteration 105/1000 | Loss: 0.00001552
Iteration 106/1000 | Loss: 0.00001551
Iteration 107/1000 | Loss: 0.00001551
Iteration 108/1000 | Loss: 0.00001551
Iteration 109/1000 | Loss: 0.00001551
Iteration 110/1000 | Loss: 0.00001551
Iteration 111/1000 | Loss: 0.00001551
Iteration 112/1000 | Loss: 0.00001551
Iteration 113/1000 | Loss: 0.00001551
Iteration 114/1000 | Loss: 0.00001551
Iteration 115/1000 | Loss: 0.00001550
Iteration 116/1000 | Loss: 0.00001550
Iteration 117/1000 | Loss: 0.00001550
Iteration 118/1000 | Loss: 0.00001550
Iteration 119/1000 | Loss: 0.00001549
Iteration 120/1000 | Loss: 0.00001549
Iteration 121/1000 | Loss: 0.00001549
Iteration 122/1000 | Loss: 0.00001549
Iteration 123/1000 | Loss: 0.00001549
Iteration 124/1000 | Loss: 0.00001549
Iteration 125/1000 | Loss: 0.00001549
Iteration 126/1000 | Loss: 0.00001549
Iteration 127/1000 | Loss: 0.00001548
Iteration 128/1000 | Loss: 0.00001548
Iteration 129/1000 | Loss: 0.00001548
Iteration 130/1000 | Loss: 0.00001548
Iteration 131/1000 | Loss: 0.00001548
Iteration 132/1000 | Loss: 0.00001548
Iteration 133/1000 | Loss: 0.00001548
Iteration 134/1000 | Loss: 0.00001547
Iteration 135/1000 | Loss: 0.00001547
Iteration 136/1000 | Loss: 0.00001547
Iteration 137/1000 | Loss: 0.00001547
Iteration 138/1000 | Loss: 0.00001547
Iteration 139/1000 | Loss: 0.00001547
Iteration 140/1000 | Loss: 0.00001546
Iteration 141/1000 | Loss: 0.00001546
Iteration 142/1000 | Loss: 0.00001546
Iteration 143/1000 | Loss: 0.00001546
Iteration 144/1000 | Loss: 0.00001546
Iteration 145/1000 | Loss: 0.00001545
Iteration 146/1000 | Loss: 0.00001545
Iteration 147/1000 | Loss: 0.00001545
Iteration 148/1000 | Loss: 0.00001545
Iteration 149/1000 | Loss: 0.00001545
Iteration 150/1000 | Loss: 0.00001545
Iteration 151/1000 | Loss: 0.00001545
Iteration 152/1000 | Loss: 0.00001545
Iteration 153/1000 | Loss: 0.00001545
Iteration 154/1000 | Loss: 0.00001544
Iteration 155/1000 | Loss: 0.00001544
Iteration 156/1000 | Loss: 0.00001544
Iteration 157/1000 | Loss: 0.00001544
Iteration 158/1000 | Loss: 0.00001544
Iteration 159/1000 | Loss: 0.00001544
Iteration 160/1000 | Loss: 0.00001544
Iteration 161/1000 | Loss: 0.00001544
Iteration 162/1000 | Loss: 0.00001544
Iteration 163/1000 | Loss: 0.00001544
Iteration 164/1000 | Loss: 0.00001544
Iteration 165/1000 | Loss: 0.00001544
Iteration 166/1000 | Loss: 0.00001544
Iteration 167/1000 | Loss: 0.00001544
Iteration 168/1000 | Loss: 0.00001543
Iteration 169/1000 | Loss: 0.00001543
Iteration 170/1000 | Loss: 0.00001543
Iteration 171/1000 | Loss: 0.00001543
Iteration 172/1000 | Loss: 0.00001543
Iteration 173/1000 | Loss: 0.00001543
Iteration 174/1000 | Loss: 0.00001543
Iteration 175/1000 | Loss: 0.00001543
Iteration 176/1000 | Loss: 0.00001543
Iteration 177/1000 | Loss: 0.00001543
Iteration 178/1000 | Loss: 0.00001543
Iteration 179/1000 | Loss: 0.00001543
Iteration 180/1000 | Loss: 0.00001543
Iteration 181/1000 | Loss: 0.00001543
Iteration 182/1000 | Loss: 0.00001543
Iteration 183/1000 | Loss: 0.00001543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.543271355330944e-05, 1.543271355330944e-05, 1.543271355330944e-05, 1.543271355330944e-05, 1.543271355330944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.543271355330944e-05

Optimization complete. Final v2v error: 3.2528316974639893 mm

Highest mean error: 5.361496448516846 mm for frame 48

Lowest mean error: 2.9144675731658936 mm for frame 150

Saving results

Total time: 52.00921154022217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00566023
Iteration 2/25 | Loss: 0.00141881
Iteration 3/25 | Loss: 0.00131493
Iteration 4/25 | Loss: 0.00130455
Iteration 5/25 | Loss: 0.00130321
Iteration 6/25 | Loss: 0.00130321
Iteration 7/25 | Loss: 0.00130321
Iteration 8/25 | Loss: 0.00130217
Iteration 9/25 | Loss: 0.00130217
Iteration 10/25 | Loss: 0.00130217
Iteration 11/25 | Loss: 0.00130217
Iteration 12/25 | Loss: 0.00130217
Iteration 13/25 | Loss: 0.00130217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013021695194765925, 0.0013021695194765925, 0.0013021695194765925, 0.0013021695194765925, 0.0013021695194765925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013021695194765925

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62418830
Iteration 2/25 | Loss: 0.00096194
Iteration 3/25 | Loss: 0.00096194
Iteration 4/25 | Loss: 0.00096193
Iteration 5/25 | Loss: 0.00096193
Iteration 6/25 | Loss: 0.00096193
Iteration 7/25 | Loss: 0.00096193
Iteration 8/25 | Loss: 0.00096193
Iteration 9/25 | Loss: 0.00096193
Iteration 10/25 | Loss: 0.00096193
Iteration 11/25 | Loss: 0.00096193
Iteration 12/25 | Loss: 0.00096193
Iteration 13/25 | Loss: 0.00096193
Iteration 14/25 | Loss: 0.00096193
Iteration 15/25 | Loss: 0.00096193
Iteration 16/25 | Loss: 0.00096193
Iteration 17/25 | Loss: 0.00096193
Iteration 18/25 | Loss: 0.00096193
Iteration 19/25 | Loss: 0.00096193
Iteration 20/25 | Loss: 0.00096193
Iteration 21/25 | Loss: 0.00096193
Iteration 22/25 | Loss: 0.00096193
Iteration 23/25 | Loss: 0.00096193
Iteration 24/25 | Loss: 0.00096193
Iteration 25/25 | Loss: 0.00096193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096193
Iteration 2/1000 | Loss: 0.00003603
Iteration 3/1000 | Loss: 0.00002754
Iteration 4/1000 | Loss: 0.00002612
Iteration 5/1000 | Loss: 0.00002490
Iteration 6/1000 | Loss: 0.00002406
Iteration 7/1000 | Loss: 0.00002350
Iteration 8/1000 | Loss: 0.00002295
Iteration 9/1000 | Loss: 0.00002239
Iteration 10/1000 | Loss: 0.00002189
Iteration 11/1000 | Loss: 0.00002145
Iteration 12/1000 | Loss: 0.00002122
Iteration 13/1000 | Loss: 0.00002094
Iteration 14/1000 | Loss: 0.00002081
Iteration 15/1000 | Loss: 0.00002066
Iteration 16/1000 | Loss: 0.00002049
Iteration 17/1000 | Loss: 0.00002045
Iteration 18/1000 | Loss: 0.00002041
Iteration 19/1000 | Loss: 0.00002034
Iteration 20/1000 | Loss: 0.00002034
Iteration 21/1000 | Loss: 0.00002033
Iteration 22/1000 | Loss: 0.00002033
Iteration 23/1000 | Loss: 0.00002021
Iteration 24/1000 | Loss: 0.00002019
Iteration 25/1000 | Loss: 0.00002017
Iteration 26/1000 | Loss: 0.00002016
Iteration 27/1000 | Loss: 0.00002016
Iteration 28/1000 | Loss: 0.00002014
Iteration 29/1000 | Loss: 0.00002013
Iteration 30/1000 | Loss: 0.00002012
Iteration 31/1000 | Loss: 0.00002010
Iteration 32/1000 | Loss: 0.00002010
Iteration 33/1000 | Loss: 0.00002010
Iteration 34/1000 | Loss: 0.00002009
Iteration 35/1000 | Loss: 0.00002009
Iteration 36/1000 | Loss: 0.00002009
Iteration 37/1000 | Loss: 0.00002009
Iteration 38/1000 | Loss: 0.00002009
Iteration 39/1000 | Loss: 0.00002009
Iteration 40/1000 | Loss: 0.00002008
Iteration 41/1000 | Loss: 0.00002008
Iteration 42/1000 | Loss: 0.00002008
Iteration 43/1000 | Loss: 0.00002008
Iteration 44/1000 | Loss: 0.00002007
Iteration 45/1000 | Loss: 0.00002007
Iteration 46/1000 | Loss: 0.00002007
Iteration 47/1000 | Loss: 0.00002007
Iteration 48/1000 | Loss: 0.00002007
Iteration 49/1000 | Loss: 0.00002007
Iteration 50/1000 | Loss: 0.00002007
Iteration 51/1000 | Loss: 0.00002007
Iteration 52/1000 | Loss: 0.00002007
Iteration 53/1000 | Loss: 0.00002007
Iteration 54/1000 | Loss: 0.00002006
Iteration 55/1000 | Loss: 0.00002005
Iteration 56/1000 | Loss: 0.00002002
Iteration 57/1000 | Loss: 0.00002002
Iteration 58/1000 | Loss: 0.00002001
Iteration 59/1000 | Loss: 0.00002001
Iteration 60/1000 | Loss: 0.00002001
Iteration 61/1000 | Loss: 0.00002001
Iteration 62/1000 | Loss: 0.00002001
Iteration 63/1000 | Loss: 0.00002001
Iteration 64/1000 | Loss: 0.00002000
Iteration 65/1000 | Loss: 0.00002000
Iteration 66/1000 | Loss: 0.00002000
Iteration 67/1000 | Loss: 0.00002000
Iteration 68/1000 | Loss: 0.00002000
Iteration 69/1000 | Loss: 0.00002000
Iteration 70/1000 | Loss: 0.00002000
Iteration 71/1000 | Loss: 0.00001999
Iteration 72/1000 | Loss: 0.00001999
Iteration 73/1000 | Loss: 0.00001998
Iteration 74/1000 | Loss: 0.00001997
Iteration 75/1000 | Loss: 0.00001996
Iteration 76/1000 | Loss: 0.00001996
Iteration 77/1000 | Loss: 0.00001996
Iteration 78/1000 | Loss: 0.00001996
Iteration 79/1000 | Loss: 0.00001996
Iteration 80/1000 | Loss: 0.00001996
Iteration 81/1000 | Loss: 0.00001996
Iteration 82/1000 | Loss: 0.00001996
Iteration 83/1000 | Loss: 0.00001996
Iteration 84/1000 | Loss: 0.00001995
Iteration 85/1000 | Loss: 0.00001995
Iteration 86/1000 | Loss: 0.00001995
Iteration 87/1000 | Loss: 0.00001995
Iteration 88/1000 | Loss: 0.00001995
Iteration 89/1000 | Loss: 0.00001995
Iteration 90/1000 | Loss: 0.00001994
Iteration 91/1000 | Loss: 0.00001994
Iteration 92/1000 | Loss: 0.00001994
Iteration 93/1000 | Loss: 0.00001994
Iteration 94/1000 | Loss: 0.00001994
Iteration 95/1000 | Loss: 0.00001994
Iteration 96/1000 | Loss: 0.00001994
Iteration 97/1000 | Loss: 0.00001994
Iteration 98/1000 | Loss: 0.00001994
Iteration 99/1000 | Loss: 0.00001994
Iteration 100/1000 | Loss: 0.00001994
Iteration 101/1000 | Loss: 0.00001994
Iteration 102/1000 | Loss: 0.00001994
Iteration 103/1000 | Loss: 0.00001994
Iteration 104/1000 | Loss: 0.00001994
Iteration 105/1000 | Loss: 0.00001994
Iteration 106/1000 | Loss: 0.00001994
Iteration 107/1000 | Loss: 0.00001994
Iteration 108/1000 | Loss: 0.00001994
Iteration 109/1000 | Loss: 0.00001994
Iteration 110/1000 | Loss: 0.00001994
Iteration 111/1000 | Loss: 0.00001994
Iteration 112/1000 | Loss: 0.00001994
Iteration 113/1000 | Loss: 0.00001994
Iteration 114/1000 | Loss: 0.00001994
Iteration 115/1000 | Loss: 0.00001994
Iteration 116/1000 | Loss: 0.00001994
Iteration 117/1000 | Loss: 0.00001994
Iteration 118/1000 | Loss: 0.00001994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.9943581719417125e-05, 1.9943581719417125e-05, 1.9943581719417125e-05, 1.9943581719417125e-05, 1.9943581719417125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9943581719417125e-05

Optimization complete. Final v2v error: 3.627045154571533 mm

Highest mean error: 3.7334539890289307 mm for frame 23

Lowest mean error: 3.561732292175293 mm for frame 221

Saving results

Total time: 44.724891901016235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390201
Iteration 2/25 | Loss: 0.00129523
Iteration 3/25 | Loss: 0.00120908
Iteration 4/25 | Loss: 0.00119556
Iteration 5/25 | Loss: 0.00119127
Iteration 6/25 | Loss: 0.00119037
Iteration 7/25 | Loss: 0.00119037
Iteration 8/25 | Loss: 0.00119037
Iteration 9/25 | Loss: 0.00119037
Iteration 10/25 | Loss: 0.00119037
Iteration 11/25 | Loss: 0.00119037
Iteration 12/25 | Loss: 0.00119037
Iteration 13/25 | Loss: 0.00119037
Iteration 14/25 | Loss: 0.00119037
Iteration 15/25 | Loss: 0.00119037
Iteration 16/25 | Loss: 0.00119037
Iteration 17/25 | Loss: 0.00119037
Iteration 18/25 | Loss: 0.00119037
Iteration 19/25 | Loss: 0.00119037
Iteration 20/25 | Loss: 0.00119037
Iteration 21/25 | Loss: 0.00119037
Iteration 22/25 | Loss: 0.00119037
Iteration 23/25 | Loss: 0.00119037
Iteration 24/25 | Loss: 0.00119037
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011903665727004409, 0.0011903665727004409, 0.0011903665727004409, 0.0011903665727004409, 0.0011903665727004409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011903665727004409

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33757806
Iteration 2/25 | Loss: 0.00111081
Iteration 3/25 | Loss: 0.00111080
Iteration 4/25 | Loss: 0.00111080
Iteration 5/25 | Loss: 0.00111080
Iteration 6/25 | Loss: 0.00111080
Iteration 7/25 | Loss: 0.00111080
Iteration 8/25 | Loss: 0.00111080
Iteration 9/25 | Loss: 0.00111080
Iteration 10/25 | Loss: 0.00111080
Iteration 11/25 | Loss: 0.00111080
Iteration 12/25 | Loss: 0.00111080
Iteration 13/25 | Loss: 0.00111080
Iteration 14/25 | Loss: 0.00111080
Iteration 15/25 | Loss: 0.00111080
Iteration 16/25 | Loss: 0.00111080
Iteration 17/25 | Loss: 0.00111080
Iteration 18/25 | Loss: 0.00111080
Iteration 19/25 | Loss: 0.00111080
Iteration 20/25 | Loss: 0.00111080
Iteration 21/25 | Loss: 0.00111080
Iteration 22/25 | Loss: 0.00111080
Iteration 23/25 | Loss: 0.00111080
Iteration 24/25 | Loss: 0.00111080
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001110798679292202, 0.001110798679292202, 0.001110798679292202, 0.001110798679292202, 0.001110798679292202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001110798679292202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111080
Iteration 2/1000 | Loss: 0.00004711
Iteration 3/1000 | Loss: 0.00003048
Iteration 4/1000 | Loss: 0.00002345
Iteration 5/1000 | Loss: 0.00002079
Iteration 6/1000 | Loss: 0.00001939
Iteration 7/1000 | Loss: 0.00001789
Iteration 8/1000 | Loss: 0.00001716
Iteration 9/1000 | Loss: 0.00001663
Iteration 10/1000 | Loss: 0.00001622
Iteration 11/1000 | Loss: 0.00001600
Iteration 12/1000 | Loss: 0.00001589
Iteration 13/1000 | Loss: 0.00001567
Iteration 14/1000 | Loss: 0.00001557
Iteration 15/1000 | Loss: 0.00001552
Iteration 16/1000 | Loss: 0.00001544
Iteration 17/1000 | Loss: 0.00001540
Iteration 18/1000 | Loss: 0.00001538
Iteration 19/1000 | Loss: 0.00001535
Iteration 20/1000 | Loss: 0.00001531
Iteration 21/1000 | Loss: 0.00001525
Iteration 22/1000 | Loss: 0.00001517
Iteration 23/1000 | Loss: 0.00001515
Iteration 24/1000 | Loss: 0.00001514
Iteration 25/1000 | Loss: 0.00001513
Iteration 26/1000 | Loss: 0.00001512
Iteration 27/1000 | Loss: 0.00001512
Iteration 28/1000 | Loss: 0.00001511
Iteration 29/1000 | Loss: 0.00001511
Iteration 30/1000 | Loss: 0.00001510
Iteration 31/1000 | Loss: 0.00001510
Iteration 32/1000 | Loss: 0.00001510
Iteration 33/1000 | Loss: 0.00001509
Iteration 34/1000 | Loss: 0.00001508
Iteration 35/1000 | Loss: 0.00001508
Iteration 36/1000 | Loss: 0.00001508
Iteration 37/1000 | Loss: 0.00001508
Iteration 38/1000 | Loss: 0.00001508
Iteration 39/1000 | Loss: 0.00001508
Iteration 40/1000 | Loss: 0.00001508
Iteration 41/1000 | Loss: 0.00001507
Iteration 42/1000 | Loss: 0.00001507
Iteration 43/1000 | Loss: 0.00001507
Iteration 44/1000 | Loss: 0.00001507
Iteration 45/1000 | Loss: 0.00001507
Iteration 46/1000 | Loss: 0.00001507
Iteration 47/1000 | Loss: 0.00001507
Iteration 48/1000 | Loss: 0.00001507
Iteration 49/1000 | Loss: 0.00001507
Iteration 50/1000 | Loss: 0.00001507
Iteration 51/1000 | Loss: 0.00001507
Iteration 52/1000 | Loss: 0.00001506
Iteration 53/1000 | Loss: 0.00001506
Iteration 54/1000 | Loss: 0.00001506
Iteration 55/1000 | Loss: 0.00001505
Iteration 56/1000 | Loss: 0.00001505
Iteration 57/1000 | Loss: 0.00001505
Iteration 58/1000 | Loss: 0.00001505
Iteration 59/1000 | Loss: 0.00001505
Iteration 60/1000 | Loss: 0.00001505
Iteration 61/1000 | Loss: 0.00001505
Iteration 62/1000 | Loss: 0.00001505
Iteration 63/1000 | Loss: 0.00001505
Iteration 64/1000 | Loss: 0.00001505
Iteration 65/1000 | Loss: 0.00001505
Iteration 66/1000 | Loss: 0.00001504
Iteration 67/1000 | Loss: 0.00001504
Iteration 68/1000 | Loss: 0.00001504
Iteration 69/1000 | Loss: 0.00001504
Iteration 70/1000 | Loss: 0.00001504
Iteration 71/1000 | Loss: 0.00001504
Iteration 72/1000 | Loss: 0.00001504
Iteration 73/1000 | Loss: 0.00001504
Iteration 74/1000 | Loss: 0.00001504
Iteration 75/1000 | Loss: 0.00001504
Iteration 76/1000 | Loss: 0.00001504
Iteration 77/1000 | Loss: 0.00001504
Iteration 78/1000 | Loss: 0.00001504
Iteration 79/1000 | Loss: 0.00001504
Iteration 80/1000 | Loss: 0.00001504
Iteration 81/1000 | Loss: 0.00001504
Iteration 82/1000 | Loss: 0.00001504
Iteration 83/1000 | Loss: 0.00001504
Iteration 84/1000 | Loss: 0.00001504
Iteration 85/1000 | Loss: 0.00001504
Iteration 86/1000 | Loss: 0.00001504
Iteration 87/1000 | Loss: 0.00001504
Iteration 88/1000 | Loss: 0.00001504
Iteration 89/1000 | Loss: 0.00001504
Iteration 90/1000 | Loss: 0.00001504
Iteration 91/1000 | Loss: 0.00001504
Iteration 92/1000 | Loss: 0.00001504
Iteration 93/1000 | Loss: 0.00001504
Iteration 94/1000 | Loss: 0.00001504
Iteration 95/1000 | Loss: 0.00001504
Iteration 96/1000 | Loss: 0.00001504
Iteration 97/1000 | Loss: 0.00001504
Iteration 98/1000 | Loss: 0.00001504
Iteration 99/1000 | Loss: 0.00001504
Iteration 100/1000 | Loss: 0.00001504
Iteration 101/1000 | Loss: 0.00001504
Iteration 102/1000 | Loss: 0.00001504
Iteration 103/1000 | Loss: 0.00001504
Iteration 104/1000 | Loss: 0.00001504
Iteration 105/1000 | Loss: 0.00001504
Iteration 106/1000 | Loss: 0.00001504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.5036619515740313e-05, 1.5036619515740313e-05, 1.5036619515740313e-05, 1.5036619515740313e-05, 1.5036619515740313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5036619515740313e-05

Optimization complete. Final v2v error: 3.2776615619659424 mm

Highest mean error: 4.128631591796875 mm for frame 23

Lowest mean error: 2.7761268615722656 mm for frame 11

Saving results

Total time: 35.08030676841736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010859
Iteration 2/25 | Loss: 0.00210972
Iteration 3/25 | Loss: 0.00151504
Iteration 4/25 | Loss: 0.00145210
Iteration 5/25 | Loss: 0.00147133
Iteration 6/25 | Loss: 0.00148936
Iteration 7/25 | Loss: 0.00135508
Iteration 8/25 | Loss: 0.00135673
Iteration 9/25 | Loss: 0.00128031
Iteration 10/25 | Loss: 0.00126759
Iteration 11/25 | Loss: 0.00126303
Iteration 12/25 | Loss: 0.00125299
Iteration 13/25 | Loss: 0.00124433
Iteration 14/25 | Loss: 0.00124219
Iteration 15/25 | Loss: 0.00124113
Iteration 16/25 | Loss: 0.00124216
Iteration 17/25 | Loss: 0.00124095
Iteration 18/25 | Loss: 0.00124143
Iteration 19/25 | Loss: 0.00124092
Iteration 20/25 | Loss: 0.00124148
Iteration 21/25 | Loss: 0.00124184
Iteration 22/25 | Loss: 0.00124054
Iteration 23/25 | Loss: 0.00124100
Iteration 24/25 | Loss: 0.00124050
Iteration 25/25 | Loss: 0.00124067

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41324568
Iteration 2/25 | Loss: 0.00114099
Iteration 3/25 | Loss: 0.00114099
Iteration 4/25 | Loss: 0.00114099
Iteration 5/25 | Loss: 0.00114099
Iteration 6/25 | Loss: 0.00114098
Iteration 7/25 | Loss: 0.00114098
Iteration 8/25 | Loss: 0.00114098
Iteration 9/25 | Loss: 0.00114098
Iteration 10/25 | Loss: 0.00114098
Iteration 11/25 | Loss: 0.00114098
Iteration 12/25 | Loss: 0.00114098
Iteration 13/25 | Loss: 0.00114098
Iteration 14/25 | Loss: 0.00114098
Iteration 15/25 | Loss: 0.00114098
Iteration 16/25 | Loss: 0.00114098
Iteration 17/25 | Loss: 0.00114098
Iteration 18/25 | Loss: 0.00114098
Iteration 19/25 | Loss: 0.00114098
Iteration 20/25 | Loss: 0.00114098
Iteration 21/25 | Loss: 0.00114098
Iteration 22/25 | Loss: 0.00114098
Iteration 23/25 | Loss: 0.00114098
Iteration 24/25 | Loss: 0.00114098
Iteration 25/25 | Loss: 0.00114098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114098
Iteration 2/1000 | Loss: 0.00003366
Iteration 3/1000 | Loss: 0.00003291
Iteration 4/1000 | Loss: 0.00004358
Iteration 5/1000 | Loss: 0.00002935
Iteration 6/1000 | Loss: 0.00002269
Iteration 7/1000 | Loss: 0.00002056
Iteration 8/1000 | Loss: 0.00003321
Iteration 9/1000 | Loss: 0.00003474
Iteration 10/1000 | Loss: 0.00008707
Iteration 11/1000 | Loss: 0.00338106
Iteration 12/1000 | Loss: 0.00026864
Iteration 13/1000 | Loss: 0.00042438
Iteration 14/1000 | Loss: 0.00006279
Iteration 15/1000 | Loss: 0.00003810
Iteration 16/1000 | Loss: 0.00003348
Iteration 17/1000 | Loss: 0.00005074
Iteration 18/1000 | Loss: 0.00004602
Iteration 19/1000 | Loss: 0.00001773
Iteration 20/1000 | Loss: 0.00001650
Iteration 21/1000 | Loss: 0.00001568
Iteration 22/1000 | Loss: 0.00001511
Iteration 23/1000 | Loss: 0.00001478
Iteration 24/1000 | Loss: 0.00001445
Iteration 25/1000 | Loss: 0.00001424
Iteration 26/1000 | Loss: 0.00001396
Iteration 27/1000 | Loss: 0.00001369
Iteration 28/1000 | Loss: 0.00007784
Iteration 29/1000 | Loss: 0.00001358
Iteration 30/1000 | Loss: 0.00001351
Iteration 31/1000 | Loss: 0.00001346
Iteration 32/1000 | Loss: 0.00001341
Iteration 33/1000 | Loss: 0.00001340
Iteration 34/1000 | Loss: 0.00001337
Iteration 35/1000 | Loss: 0.00001335
Iteration 36/1000 | Loss: 0.00001334
Iteration 37/1000 | Loss: 0.00025508
Iteration 38/1000 | Loss: 0.00022606
Iteration 39/1000 | Loss: 0.00002302
Iteration 40/1000 | Loss: 0.00012645
Iteration 41/1000 | Loss: 0.00001453
Iteration 42/1000 | Loss: 0.00001361
Iteration 43/1000 | Loss: 0.00001339
Iteration 44/1000 | Loss: 0.00023465
Iteration 45/1000 | Loss: 0.00002562
Iteration 46/1000 | Loss: 0.00001815
Iteration 47/1000 | Loss: 0.00001581
Iteration 48/1000 | Loss: 0.00001458
Iteration 49/1000 | Loss: 0.00001358
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001313
Iteration 52/1000 | Loss: 0.00001311
Iteration 53/1000 | Loss: 0.00001289
Iteration 54/1000 | Loss: 0.00007097
Iteration 55/1000 | Loss: 0.00001277
Iteration 56/1000 | Loss: 0.00001272
Iteration 57/1000 | Loss: 0.00001272
Iteration 58/1000 | Loss: 0.00001268
Iteration 59/1000 | Loss: 0.00001265
Iteration 60/1000 | Loss: 0.00001265
Iteration 61/1000 | Loss: 0.00001263
Iteration 62/1000 | Loss: 0.00001259
Iteration 63/1000 | Loss: 0.00001259
Iteration 64/1000 | Loss: 0.00001258
Iteration 65/1000 | Loss: 0.00005756
Iteration 66/1000 | Loss: 0.00001276
Iteration 67/1000 | Loss: 0.00001255
Iteration 68/1000 | Loss: 0.00001255
Iteration 69/1000 | Loss: 0.00001255
Iteration 70/1000 | Loss: 0.00001255
Iteration 71/1000 | Loss: 0.00001255
Iteration 72/1000 | Loss: 0.00001254
Iteration 73/1000 | Loss: 0.00001254
Iteration 74/1000 | Loss: 0.00001254
Iteration 75/1000 | Loss: 0.00001254
Iteration 76/1000 | Loss: 0.00001254
Iteration 77/1000 | Loss: 0.00001254
Iteration 78/1000 | Loss: 0.00001254
Iteration 79/1000 | Loss: 0.00001253
Iteration 80/1000 | Loss: 0.00001253
Iteration 81/1000 | Loss: 0.00001253
Iteration 82/1000 | Loss: 0.00001253
Iteration 83/1000 | Loss: 0.00001253
Iteration 84/1000 | Loss: 0.00001253
Iteration 85/1000 | Loss: 0.00001253
Iteration 86/1000 | Loss: 0.00001252
Iteration 87/1000 | Loss: 0.00001252
Iteration 88/1000 | Loss: 0.00001252
Iteration 89/1000 | Loss: 0.00001252
Iteration 90/1000 | Loss: 0.00001251
Iteration 91/1000 | Loss: 0.00001251
Iteration 92/1000 | Loss: 0.00001251
Iteration 93/1000 | Loss: 0.00001251
Iteration 94/1000 | Loss: 0.00001251
Iteration 95/1000 | Loss: 0.00001250
Iteration 96/1000 | Loss: 0.00001250
Iteration 97/1000 | Loss: 0.00001250
Iteration 98/1000 | Loss: 0.00001250
Iteration 99/1000 | Loss: 0.00001250
Iteration 100/1000 | Loss: 0.00001249
Iteration 101/1000 | Loss: 0.00001249
Iteration 102/1000 | Loss: 0.00001249
Iteration 103/1000 | Loss: 0.00001249
Iteration 104/1000 | Loss: 0.00001249
Iteration 105/1000 | Loss: 0.00001249
Iteration 106/1000 | Loss: 0.00001249
Iteration 107/1000 | Loss: 0.00001249
Iteration 108/1000 | Loss: 0.00001249
Iteration 109/1000 | Loss: 0.00001248
Iteration 110/1000 | Loss: 0.00001248
Iteration 111/1000 | Loss: 0.00001248
Iteration 112/1000 | Loss: 0.00001248
Iteration 113/1000 | Loss: 0.00001248
Iteration 114/1000 | Loss: 0.00001248
Iteration 115/1000 | Loss: 0.00001247
Iteration 116/1000 | Loss: 0.00001247
Iteration 117/1000 | Loss: 0.00001247
Iteration 118/1000 | Loss: 0.00001247
Iteration 119/1000 | Loss: 0.00001247
Iteration 120/1000 | Loss: 0.00001247
Iteration 121/1000 | Loss: 0.00001246
Iteration 122/1000 | Loss: 0.00001246
Iteration 123/1000 | Loss: 0.00001246
Iteration 124/1000 | Loss: 0.00001246
Iteration 125/1000 | Loss: 0.00001246
Iteration 126/1000 | Loss: 0.00001246
Iteration 127/1000 | Loss: 0.00001246
Iteration 128/1000 | Loss: 0.00001246
Iteration 129/1000 | Loss: 0.00001245
Iteration 130/1000 | Loss: 0.00001245
Iteration 131/1000 | Loss: 0.00001245
Iteration 132/1000 | Loss: 0.00001245
Iteration 133/1000 | Loss: 0.00001245
Iteration 134/1000 | Loss: 0.00001245
Iteration 135/1000 | Loss: 0.00001245
Iteration 136/1000 | Loss: 0.00001245
Iteration 137/1000 | Loss: 0.00001245
Iteration 138/1000 | Loss: 0.00001245
Iteration 139/1000 | Loss: 0.00001244
Iteration 140/1000 | Loss: 0.00001244
Iteration 141/1000 | Loss: 0.00001244
Iteration 142/1000 | Loss: 0.00001244
Iteration 143/1000 | Loss: 0.00001244
Iteration 144/1000 | Loss: 0.00001244
Iteration 145/1000 | Loss: 0.00001243
Iteration 146/1000 | Loss: 0.00001243
Iteration 147/1000 | Loss: 0.00001243
Iteration 148/1000 | Loss: 0.00001243
Iteration 149/1000 | Loss: 0.00001243
Iteration 150/1000 | Loss: 0.00001243
Iteration 151/1000 | Loss: 0.00001243
Iteration 152/1000 | Loss: 0.00001243
Iteration 153/1000 | Loss: 0.00001243
Iteration 154/1000 | Loss: 0.00001243
Iteration 155/1000 | Loss: 0.00001243
Iteration 156/1000 | Loss: 0.00001242
Iteration 157/1000 | Loss: 0.00001242
Iteration 158/1000 | Loss: 0.00001242
Iteration 159/1000 | Loss: 0.00001242
Iteration 160/1000 | Loss: 0.00001242
Iteration 161/1000 | Loss: 0.00001242
Iteration 162/1000 | Loss: 0.00001242
Iteration 163/1000 | Loss: 0.00001242
Iteration 164/1000 | Loss: 0.00001242
Iteration 165/1000 | Loss: 0.00001242
Iteration 166/1000 | Loss: 0.00001242
Iteration 167/1000 | Loss: 0.00001242
Iteration 168/1000 | Loss: 0.00001242
Iteration 169/1000 | Loss: 0.00001242
Iteration 170/1000 | Loss: 0.00001242
Iteration 171/1000 | Loss: 0.00001242
Iteration 172/1000 | Loss: 0.00001242
Iteration 173/1000 | Loss: 0.00001241
Iteration 174/1000 | Loss: 0.00001241
Iteration 175/1000 | Loss: 0.00001241
Iteration 176/1000 | Loss: 0.00001241
Iteration 177/1000 | Loss: 0.00001241
Iteration 178/1000 | Loss: 0.00001241
Iteration 179/1000 | Loss: 0.00001241
Iteration 180/1000 | Loss: 0.00001241
Iteration 181/1000 | Loss: 0.00001241
Iteration 182/1000 | Loss: 0.00001241
Iteration 183/1000 | Loss: 0.00001241
Iteration 184/1000 | Loss: 0.00001241
Iteration 185/1000 | Loss: 0.00001241
Iteration 186/1000 | Loss: 0.00001241
Iteration 187/1000 | Loss: 0.00001241
Iteration 188/1000 | Loss: 0.00001241
Iteration 189/1000 | Loss: 0.00001241
Iteration 190/1000 | Loss: 0.00001241
Iteration 191/1000 | Loss: 0.00001241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.2411396710376721e-05, 1.2411396710376721e-05, 1.2411396710376721e-05, 1.2411396710376721e-05, 1.2411396710376721e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2411396710376721e-05

Optimization complete. Final v2v error: 2.951066017150879 mm

Highest mean error: 4.114931106567383 mm for frame 64

Lowest mean error: 2.5427420139312744 mm for frame 148

Saving results

Total time: 120.8067672252655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00721531
Iteration 2/25 | Loss: 0.00152006
Iteration 3/25 | Loss: 0.00133656
Iteration 4/25 | Loss: 0.00130148
Iteration 5/25 | Loss: 0.00129125
Iteration 6/25 | Loss: 0.00128823
Iteration 7/25 | Loss: 0.00128811
Iteration 8/25 | Loss: 0.00128811
Iteration 9/25 | Loss: 0.00128811
Iteration 10/25 | Loss: 0.00128811
Iteration 11/25 | Loss: 0.00128811
Iteration 12/25 | Loss: 0.00128811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012881065485998988, 0.0012881065485998988, 0.0012881065485998988, 0.0012881065485998988, 0.0012881065485998988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012881065485998988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56448567
Iteration 2/25 | Loss: 0.00136117
Iteration 3/25 | Loss: 0.00136117
Iteration 4/25 | Loss: 0.00136117
Iteration 5/25 | Loss: 0.00136117
Iteration 6/25 | Loss: 0.00136117
Iteration 7/25 | Loss: 0.00136117
Iteration 8/25 | Loss: 0.00136117
Iteration 9/25 | Loss: 0.00136117
Iteration 10/25 | Loss: 0.00136117
Iteration 11/25 | Loss: 0.00136117
Iteration 12/25 | Loss: 0.00136117
Iteration 13/25 | Loss: 0.00136117
Iteration 14/25 | Loss: 0.00136117
Iteration 15/25 | Loss: 0.00136117
Iteration 16/25 | Loss: 0.00136117
Iteration 17/25 | Loss: 0.00136117
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013611686881631613, 0.0013611686881631613, 0.0013611686881631613, 0.0013611686881631613, 0.0013611686881631613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013611686881631613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136117
Iteration 2/1000 | Loss: 0.00006608
Iteration 3/1000 | Loss: 0.00003841
Iteration 4/1000 | Loss: 0.00003025
Iteration 5/1000 | Loss: 0.00002755
Iteration 6/1000 | Loss: 0.00002563
Iteration 7/1000 | Loss: 0.00002446
Iteration 8/1000 | Loss: 0.00002358
Iteration 9/1000 | Loss: 0.00002287
Iteration 10/1000 | Loss: 0.00002249
Iteration 11/1000 | Loss: 0.00002214
Iteration 12/1000 | Loss: 0.00002181
Iteration 13/1000 | Loss: 0.00002165
Iteration 14/1000 | Loss: 0.00002151
Iteration 15/1000 | Loss: 0.00002150
Iteration 16/1000 | Loss: 0.00002132
Iteration 17/1000 | Loss: 0.00002126
Iteration 18/1000 | Loss: 0.00002125
Iteration 19/1000 | Loss: 0.00002120
Iteration 20/1000 | Loss: 0.00002119
Iteration 21/1000 | Loss: 0.00002119
Iteration 22/1000 | Loss: 0.00002119
Iteration 23/1000 | Loss: 0.00002118
Iteration 24/1000 | Loss: 0.00002118
Iteration 25/1000 | Loss: 0.00002117
Iteration 26/1000 | Loss: 0.00002117
Iteration 27/1000 | Loss: 0.00002116
Iteration 28/1000 | Loss: 0.00002116
Iteration 29/1000 | Loss: 0.00002115
Iteration 30/1000 | Loss: 0.00002114
Iteration 31/1000 | Loss: 0.00002113
Iteration 32/1000 | Loss: 0.00002113
Iteration 33/1000 | Loss: 0.00002112
Iteration 34/1000 | Loss: 0.00002112
Iteration 35/1000 | Loss: 0.00002111
Iteration 36/1000 | Loss: 0.00002109
Iteration 37/1000 | Loss: 0.00002109
Iteration 38/1000 | Loss: 0.00002108
Iteration 39/1000 | Loss: 0.00002107
Iteration 40/1000 | Loss: 0.00002107
Iteration 41/1000 | Loss: 0.00002106
Iteration 42/1000 | Loss: 0.00002106
Iteration 43/1000 | Loss: 0.00002106
Iteration 44/1000 | Loss: 0.00002105
Iteration 45/1000 | Loss: 0.00002105
Iteration 46/1000 | Loss: 0.00002104
Iteration 47/1000 | Loss: 0.00002104
Iteration 48/1000 | Loss: 0.00002103
Iteration 49/1000 | Loss: 0.00002103
Iteration 50/1000 | Loss: 0.00002103
Iteration 51/1000 | Loss: 0.00002102
Iteration 52/1000 | Loss: 0.00002101
Iteration 53/1000 | Loss: 0.00002101
Iteration 54/1000 | Loss: 0.00002101
Iteration 55/1000 | Loss: 0.00002101
Iteration 56/1000 | Loss: 0.00002101
Iteration 57/1000 | Loss: 0.00002100
Iteration 58/1000 | Loss: 0.00002100
Iteration 59/1000 | Loss: 0.00002099
Iteration 60/1000 | Loss: 0.00002099
Iteration 61/1000 | Loss: 0.00002099
Iteration 62/1000 | Loss: 0.00002098
Iteration 63/1000 | Loss: 0.00002098
Iteration 64/1000 | Loss: 0.00002098
Iteration 65/1000 | Loss: 0.00002097
Iteration 66/1000 | Loss: 0.00002097
Iteration 67/1000 | Loss: 0.00002097
Iteration 68/1000 | Loss: 0.00002096
Iteration 69/1000 | Loss: 0.00002096
Iteration 70/1000 | Loss: 0.00002096
Iteration 71/1000 | Loss: 0.00002095
Iteration 72/1000 | Loss: 0.00002095
Iteration 73/1000 | Loss: 0.00002095
Iteration 74/1000 | Loss: 0.00002095
Iteration 75/1000 | Loss: 0.00002095
Iteration 76/1000 | Loss: 0.00002094
Iteration 77/1000 | Loss: 0.00002094
Iteration 78/1000 | Loss: 0.00002094
Iteration 79/1000 | Loss: 0.00002093
Iteration 80/1000 | Loss: 0.00002093
Iteration 81/1000 | Loss: 0.00002093
Iteration 82/1000 | Loss: 0.00002093
Iteration 83/1000 | Loss: 0.00002092
Iteration 84/1000 | Loss: 0.00002092
Iteration 85/1000 | Loss: 0.00002092
Iteration 86/1000 | Loss: 0.00002092
Iteration 87/1000 | Loss: 0.00002091
Iteration 88/1000 | Loss: 0.00002091
Iteration 89/1000 | Loss: 0.00002091
Iteration 90/1000 | Loss: 0.00002091
Iteration 91/1000 | Loss: 0.00002091
Iteration 92/1000 | Loss: 0.00002090
Iteration 93/1000 | Loss: 0.00002090
Iteration 94/1000 | Loss: 0.00002090
Iteration 95/1000 | Loss: 0.00002089
Iteration 96/1000 | Loss: 0.00002089
Iteration 97/1000 | Loss: 0.00002089
Iteration 98/1000 | Loss: 0.00002089
Iteration 99/1000 | Loss: 0.00002088
Iteration 100/1000 | Loss: 0.00002088
Iteration 101/1000 | Loss: 0.00002088
Iteration 102/1000 | Loss: 0.00002088
Iteration 103/1000 | Loss: 0.00002088
Iteration 104/1000 | Loss: 0.00002088
Iteration 105/1000 | Loss: 0.00002087
Iteration 106/1000 | Loss: 0.00002087
Iteration 107/1000 | Loss: 0.00002087
Iteration 108/1000 | Loss: 0.00002087
Iteration 109/1000 | Loss: 0.00002086
Iteration 110/1000 | Loss: 0.00002086
Iteration 111/1000 | Loss: 0.00002086
Iteration 112/1000 | Loss: 0.00002085
Iteration 113/1000 | Loss: 0.00002085
Iteration 114/1000 | Loss: 0.00002085
Iteration 115/1000 | Loss: 0.00002084
Iteration 116/1000 | Loss: 0.00002084
Iteration 117/1000 | Loss: 0.00002084
Iteration 118/1000 | Loss: 0.00002084
Iteration 119/1000 | Loss: 0.00002083
Iteration 120/1000 | Loss: 0.00002083
Iteration 121/1000 | Loss: 0.00002083
Iteration 122/1000 | Loss: 0.00002083
Iteration 123/1000 | Loss: 0.00002083
Iteration 124/1000 | Loss: 0.00002082
Iteration 125/1000 | Loss: 0.00002082
Iteration 126/1000 | Loss: 0.00002082
Iteration 127/1000 | Loss: 0.00002082
Iteration 128/1000 | Loss: 0.00002082
Iteration 129/1000 | Loss: 0.00002082
Iteration 130/1000 | Loss: 0.00002082
Iteration 131/1000 | Loss: 0.00002082
Iteration 132/1000 | Loss: 0.00002081
Iteration 133/1000 | Loss: 0.00002081
Iteration 134/1000 | Loss: 0.00002081
Iteration 135/1000 | Loss: 0.00002081
Iteration 136/1000 | Loss: 0.00002081
Iteration 137/1000 | Loss: 0.00002081
Iteration 138/1000 | Loss: 0.00002081
Iteration 139/1000 | Loss: 0.00002081
Iteration 140/1000 | Loss: 0.00002081
Iteration 141/1000 | Loss: 0.00002081
Iteration 142/1000 | Loss: 0.00002081
Iteration 143/1000 | Loss: 0.00002080
Iteration 144/1000 | Loss: 0.00002080
Iteration 145/1000 | Loss: 0.00002080
Iteration 146/1000 | Loss: 0.00002080
Iteration 147/1000 | Loss: 0.00002080
Iteration 148/1000 | Loss: 0.00002080
Iteration 149/1000 | Loss: 0.00002080
Iteration 150/1000 | Loss: 0.00002080
Iteration 151/1000 | Loss: 0.00002080
Iteration 152/1000 | Loss: 0.00002080
Iteration 153/1000 | Loss: 0.00002080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.0800125639652833e-05, 2.0800125639652833e-05, 2.0800125639652833e-05, 2.0800125639652833e-05, 2.0800125639652833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0800125639652833e-05

Optimization complete. Final v2v error: 3.865535020828247 mm

Highest mean error: 5.2189836502075195 mm for frame 65

Lowest mean error: 3.055443525314331 mm for frame 50

Saving results

Total time: 46.42763018608093
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487917
Iteration 2/25 | Loss: 0.00139822
Iteration 3/25 | Loss: 0.00128339
Iteration 4/25 | Loss: 0.00127111
Iteration 5/25 | Loss: 0.00126782
Iteration 6/25 | Loss: 0.00126733
Iteration 7/25 | Loss: 0.00126733
Iteration 8/25 | Loss: 0.00126733
Iteration 9/25 | Loss: 0.00126733
Iteration 10/25 | Loss: 0.00126733
Iteration 11/25 | Loss: 0.00126733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012673343298956752, 0.0012673343298956752, 0.0012673343298956752, 0.0012673343298956752, 0.0012673343298956752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012673343298956752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35982728
Iteration 2/25 | Loss: 0.00099102
Iteration 3/25 | Loss: 0.00099100
Iteration 4/25 | Loss: 0.00099100
Iteration 5/25 | Loss: 0.00099100
Iteration 6/25 | Loss: 0.00099100
Iteration 7/25 | Loss: 0.00099100
Iteration 8/25 | Loss: 0.00099100
Iteration 9/25 | Loss: 0.00099100
Iteration 10/25 | Loss: 0.00099100
Iteration 11/25 | Loss: 0.00099100
Iteration 12/25 | Loss: 0.00099100
Iteration 13/25 | Loss: 0.00099100
Iteration 14/25 | Loss: 0.00099100
Iteration 15/25 | Loss: 0.00099100
Iteration 16/25 | Loss: 0.00099100
Iteration 17/25 | Loss: 0.00099100
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009909990476444364, 0.0009909990476444364, 0.0009909990476444364, 0.0009909990476444364, 0.0009909990476444364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009909990476444364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099100
Iteration 2/1000 | Loss: 0.00004827
Iteration 3/1000 | Loss: 0.00002952
Iteration 4/1000 | Loss: 0.00002425
Iteration 5/1000 | Loss: 0.00002242
Iteration 6/1000 | Loss: 0.00002072
Iteration 7/1000 | Loss: 0.00001966
Iteration 8/1000 | Loss: 0.00001889
Iteration 9/1000 | Loss: 0.00001838
Iteration 10/1000 | Loss: 0.00001795
Iteration 11/1000 | Loss: 0.00001764
Iteration 12/1000 | Loss: 0.00001737
Iteration 13/1000 | Loss: 0.00001727
Iteration 14/1000 | Loss: 0.00001709
Iteration 15/1000 | Loss: 0.00001694
Iteration 16/1000 | Loss: 0.00001693
Iteration 17/1000 | Loss: 0.00001692
Iteration 18/1000 | Loss: 0.00001692
Iteration 19/1000 | Loss: 0.00001691
Iteration 20/1000 | Loss: 0.00001690
Iteration 21/1000 | Loss: 0.00001689
Iteration 22/1000 | Loss: 0.00001684
Iteration 23/1000 | Loss: 0.00001681
Iteration 24/1000 | Loss: 0.00001680
Iteration 25/1000 | Loss: 0.00001678
Iteration 26/1000 | Loss: 0.00001677
Iteration 27/1000 | Loss: 0.00001676
Iteration 28/1000 | Loss: 0.00001672
Iteration 29/1000 | Loss: 0.00001672
Iteration 30/1000 | Loss: 0.00001671
Iteration 31/1000 | Loss: 0.00001665
Iteration 32/1000 | Loss: 0.00001664
Iteration 33/1000 | Loss: 0.00001663
Iteration 34/1000 | Loss: 0.00001663
Iteration 35/1000 | Loss: 0.00001663
Iteration 36/1000 | Loss: 0.00001663
Iteration 37/1000 | Loss: 0.00001662
Iteration 38/1000 | Loss: 0.00001662
Iteration 39/1000 | Loss: 0.00001661
Iteration 40/1000 | Loss: 0.00001661
Iteration 41/1000 | Loss: 0.00001660
Iteration 42/1000 | Loss: 0.00001660
Iteration 43/1000 | Loss: 0.00001658
Iteration 44/1000 | Loss: 0.00001657
Iteration 45/1000 | Loss: 0.00001656
Iteration 46/1000 | Loss: 0.00001656
Iteration 47/1000 | Loss: 0.00001656
Iteration 48/1000 | Loss: 0.00001656
Iteration 49/1000 | Loss: 0.00001655
Iteration 50/1000 | Loss: 0.00001655
Iteration 51/1000 | Loss: 0.00001654
Iteration 52/1000 | Loss: 0.00001654
Iteration 53/1000 | Loss: 0.00001654
Iteration 54/1000 | Loss: 0.00001654
Iteration 55/1000 | Loss: 0.00001654
Iteration 56/1000 | Loss: 0.00001653
Iteration 57/1000 | Loss: 0.00001653
Iteration 58/1000 | Loss: 0.00001653
Iteration 59/1000 | Loss: 0.00001653
Iteration 60/1000 | Loss: 0.00001652
Iteration 61/1000 | Loss: 0.00001652
Iteration 62/1000 | Loss: 0.00001652
Iteration 63/1000 | Loss: 0.00001650
Iteration 64/1000 | Loss: 0.00001650
Iteration 65/1000 | Loss: 0.00001650
Iteration 66/1000 | Loss: 0.00001649
Iteration 67/1000 | Loss: 0.00001649
Iteration 68/1000 | Loss: 0.00001648
Iteration 69/1000 | Loss: 0.00001648
Iteration 70/1000 | Loss: 0.00001647
Iteration 71/1000 | Loss: 0.00001647
Iteration 72/1000 | Loss: 0.00001647
Iteration 73/1000 | Loss: 0.00001647
Iteration 74/1000 | Loss: 0.00001646
Iteration 75/1000 | Loss: 0.00001645
Iteration 76/1000 | Loss: 0.00001645
Iteration 77/1000 | Loss: 0.00001645
Iteration 78/1000 | Loss: 0.00001645
Iteration 79/1000 | Loss: 0.00001645
Iteration 80/1000 | Loss: 0.00001644
Iteration 81/1000 | Loss: 0.00001644
Iteration 82/1000 | Loss: 0.00001644
Iteration 83/1000 | Loss: 0.00001644
Iteration 84/1000 | Loss: 0.00001643
Iteration 85/1000 | Loss: 0.00001643
Iteration 86/1000 | Loss: 0.00001643
Iteration 87/1000 | Loss: 0.00001643
Iteration 88/1000 | Loss: 0.00001643
Iteration 89/1000 | Loss: 0.00001643
Iteration 90/1000 | Loss: 0.00001642
Iteration 91/1000 | Loss: 0.00001642
Iteration 92/1000 | Loss: 0.00001642
Iteration 93/1000 | Loss: 0.00001642
Iteration 94/1000 | Loss: 0.00001642
Iteration 95/1000 | Loss: 0.00001642
Iteration 96/1000 | Loss: 0.00001641
Iteration 97/1000 | Loss: 0.00001641
Iteration 98/1000 | Loss: 0.00001641
Iteration 99/1000 | Loss: 0.00001641
Iteration 100/1000 | Loss: 0.00001640
Iteration 101/1000 | Loss: 0.00001640
Iteration 102/1000 | Loss: 0.00001640
Iteration 103/1000 | Loss: 0.00001640
Iteration 104/1000 | Loss: 0.00001639
Iteration 105/1000 | Loss: 0.00001639
Iteration 106/1000 | Loss: 0.00001639
Iteration 107/1000 | Loss: 0.00001639
Iteration 108/1000 | Loss: 0.00001639
Iteration 109/1000 | Loss: 0.00001639
Iteration 110/1000 | Loss: 0.00001639
Iteration 111/1000 | Loss: 0.00001639
Iteration 112/1000 | Loss: 0.00001638
Iteration 113/1000 | Loss: 0.00001638
Iteration 114/1000 | Loss: 0.00001638
Iteration 115/1000 | Loss: 0.00001638
Iteration 116/1000 | Loss: 0.00001638
Iteration 117/1000 | Loss: 0.00001638
Iteration 118/1000 | Loss: 0.00001638
Iteration 119/1000 | Loss: 0.00001637
Iteration 120/1000 | Loss: 0.00001637
Iteration 121/1000 | Loss: 0.00001637
Iteration 122/1000 | Loss: 0.00001637
Iteration 123/1000 | Loss: 0.00001637
Iteration 124/1000 | Loss: 0.00001637
Iteration 125/1000 | Loss: 0.00001637
Iteration 126/1000 | Loss: 0.00001637
Iteration 127/1000 | Loss: 0.00001637
Iteration 128/1000 | Loss: 0.00001636
Iteration 129/1000 | Loss: 0.00001636
Iteration 130/1000 | Loss: 0.00001636
Iteration 131/1000 | Loss: 0.00001636
Iteration 132/1000 | Loss: 0.00001636
Iteration 133/1000 | Loss: 0.00001636
Iteration 134/1000 | Loss: 0.00001635
Iteration 135/1000 | Loss: 0.00001635
Iteration 136/1000 | Loss: 0.00001635
Iteration 137/1000 | Loss: 0.00001635
Iteration 138/1000 | Loss: 0.00001635
Iteration 139/1000 | Loss: 0.00001635
Iteration 140/1000 | Loss: 0.00001635
Iteration 141/1000 | Loss: 0.00001634
Iteration 142/1000 | Loss: 0.00001634
Iteration 143/1000 | Loss: 0.00001634
Iteration 144/1000 | Loss: 0.00001634
Iteration 145/1000 | Loss: 0.00001634
Iteration 146/1000 | Loss: 0.00001633
Iteration 147/1000 | Loss: 0.00001633
Iteration 148/1000 | Loss: 0.00001633
Iteration 149/1000 | Loss: 0.00001633
Iteration 150/1000 | Loss: 0.00001632
Iteration 151/1000 | Loss: 0.00001632
Iteration 152/1000 | Loss: 0.00001632
Iteration 153/1000 | Loss: 0.00001632
Iteration 154/1000 | Loss: 0.00001632
Iteration 155/1000 | Loss: 0.00001632
Iteration 156/1000 | Loss: 0.00001632
Iteration 157/1000 | Loss: 0.00001632
Iteration 158/1000 | Loss: 0.00001631
Iteration 159/1000 | Loss: 0.00001631
Iteration 160/1000 | Loss: 0.00001631
Iteration 161/1000 | Loss: 0.00001631
Iteration 162/1000 | Loss: 0.00001631
Iteration 163/1000 | Loss: 0.00001631
Iteration 164/1000 | Loss: 0.00001631
Iteration 165/1000 | Loss: 0.00001631
Iteration 166/1000 | Loss: 0.00001631
Iteration 167/1000 | Loss: 0.00001630
Iteration 168/1000 | Loss: 0.00001630
Iteration 169/1000 | Loss: 0.00001630
Iteration 170/1000 | Loss: 0.00001630
Iteration 171/1000 | Loss: 0.00001630
Iteration 172/1000 | Loss: 0.00001630
Iteration 173/1000 | Loss: 0.00001630
Iteration 174/1000 | Loss: 0.00001630
Iteration 175/1000 | Loss: 0.00001630
Iteration 176/1000 | Loss: 0.00001630
Iteration 177/1000 | Loss: 0.00001630
Iteration 178/1000 | Loss: 0.00001630
Iteration 179/1000 | Loss: 0.00001630
Iteration 180/1000 | Loss: 0.00001630
Iteration 181/1000 | Loss: 0.00001630
Iteration 182/1000 | Loss: 0.00001630
Iteration 183/1000 | Loss: 0.00001630
Iteration 184/1000 | Loss: 0.00001630
Iteration 185/1000 | Loss: 0.00001630
Iteration 186/1000 | Loss: 0.00001630
Iteration 187/1000 | Loss: 0.00001630
Iteration 188/1000 | Loss: 0.00001630
Iteration 189/1000 | Loss: 0.00001630
Iteration 190/1000 | Loss: 0.00001630
Iteration 191/1000 | Loss: 0.00001630
Iteration 192/1000 | Loss: 0.00001630
Iteration 193/1000 | Loss: 0.00001630
Iteration 194/1000 | Loss: 0.00001630
Iteration 195/1000 | Loss: 0.00001630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.6295576642733067e-05, 1.6295576642733067e-05, 1.6295576642733067e-05, 1.6295576642733067e-05, 1.6295576642733067e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6295576642733067e-05

Optimization complete. Final v2v error: 3.414024591445923 mm

Highest mean error: 4.238094806671143 mm for frame 135

Lowest mean error: 2.9586844444274902 mm for frame 90

Saving results

Total time: 44.16843795776367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845776
Iteration 2/25 | Loss: 0.00133566
Iteration 3/25 | Loss: 0.00126072
Iteration 4/25 | Loss: 0.00125349
Iteration 5/25 | Loss: 0.00125349
Iteration 6/25 | Loss: 0.00125349
Iteration 7/25 | Loss: 0.00125349
Iteration 8/25 | Loss: 0.00125349
Iteration 9/25 | Loss: 0.00125349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.001253489637747407, 0.001253489637747407, 0.001253489637747407, 0.001253489637747407, 0.001253489637747407]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001253489637747407

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28938985
Iteration 2/25 | Loss: 0.00085805
Iteration 3/25 | Loss: 0.00085795
Iteration 4/25 | Loss: 0.00085795
Iteration 5/25 | Loss: 0.00085795
Iteration 6/25 | Loss: 0.00085795
Iteration 7/25 | Loss: 0.00085795
Iteration 8/25 | Loss: 0.00085795
Iteration 9/25 | Loss: 0.00085795
Iteration 10/25 | Loss: 0.00085795
Iteration 11/25 | Loss: 0.00085795
Iteration 12/25 | Loss: 0.00085795
Iteration 13/25 | Loss: 0.00085795
Iteration 14/25 | Loss: 0.00085795
Iteration 15/25 | Loss: 0.00085795
Iteration 16/25 | Loss: 0.00085795
Iteration 17/25 | Loss: 0.00085795
Iteration 18/25 | Loss: 0.00085795
Iteration 19/25 | Loss: 0.00085795
Iteration 20/25 | Loss: 0.00085795
Iteration 21/25 | Loss: 0.00085795
Iteration 22/25 | Loss: 0.00085795
Iteration 23/25 | Loss: 0.00085795
Iteration 24/25 | Loss: 0.00085795
Iteration 25/25 | Loss: 0.00085795
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008579500718042254, 0.0008579500718042254, 0.0008579500718042254, 0.0008579500718042254, 0.0008579500718042254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008579500718042254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085795
Iteration 2/1000 | Loss: 0.00003058
Iteration 3/1000 | Loss: 0.00002156
Iteration 4/1000 | Loss: 0.00001856
Iteration 5/1000 | Loss: 0.00001748
Iteration 6/1000 | Loss: 0.00001644
Iteration 7/1000 | Loss: 0.00001586
Iteration 8/1000 | Loss: 0.00001550
Iteration 9/1000 | Loss: 0.00001505
Iteration 10/1000 | Loss: 0.00001473
Iteration 11/1000 | Loss: 0.00001472
Iteration 12/1000 | Loss: 0.00001468
Iteration 13/1000 | Loss: 0.00001467
Iteration 14/1000 | Loss: 0.00001447
Iteration 15/1000 | Loss: 0.00001426
Iteration 16/1000 | Loss: 0.00001406
Iteration 17/1000 | Loss: 0.00001401
Iteration 18/1000 | Loss: 0.00001391
Iteration 19/1000 | Loss: 0.00001380
Iteration 20/1000 | Loss: 0.00001376
Iteration 21/1000 | Loss: 0.00001375
Iteration 22/1000 | Loss: 0.00001372
Iteration 23/1000 | Loss: 0.00001371
Iteration 24/1000 | Loss: 0.00001367
Iteration 25/1000 | Loss: 0.00001366
Iteration 26/1000 | Loss: 0.00001365
Iteration 27/1000 | Loss: 0.00001365
Iteration 28/1000 | Loss: 0.00001365
Iteration 29/1000 | Loss: 0.00001364
Iteration 30/1000 | Loss: 0.00001364
Iteration 31/1000 | Loss: 0.00001364
Iteration 32/1000 | Loss: 0.00001363
Iteration 33/1000 | Loss: 0.00001363
Iteration 34/1000 | Loss: 0.00001362
Iteration 35/1000 | Loss: 0.00001362
Iteration 36/1000 | Loss: 0.00001361
Iteration 37/1000 | Loss: 0.00001361
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001360
Iteration 40/1000 | Loss: 0.00001360
Iteration 41/1000 | Loss: 0.00001359
Iteration 42/1000 | Loss: 0.00001359
Iteration 43/1000 | Loss: 0.00001358
Iteration 44/1000 | Loss: 0.00001357
Iteration 45/1000 | Loss: 0.00001357
Iteration 46/1000 | Loss: 0.00001355
Iteration 47/1000 | Loss: 0.00001354
Iteration 48/1000 | Loss: 0.00001353
Iteration 49/1000 | Loss: 0.00001353
Iteration 50/1000 | Loss: 0.00001352
Iteration 51/1000 | Loss: 0.00001352
Iteration 52/1000 | Loss: 0.00001352
Iteration 53/1000 | Loss: 0.00001352
Iteration 54/1000 | Loss: 0.00001351
Iteration 55/1000 | Loss: 0.00001351
Iteration 56/1000 | Loss: 0.00001351
Iteration 57/1000 | Loss: 0.00001351
Iteration 58/1000 | Loss: 0.00001351
Iteration 59/1000 | Loss: 0.00001351
Iteration 60/1000 | Loss: 0.00001350
Iteration 61/1000 | Loss: 0.00001350
Iteration 62/1000 | Loss: 0.00001350
Iteration 63/1000 | Loss: 0.00001350
Iteration 64/1000 | Loss: 0.00001350
Iteration 65/1000 | Loss: 0.00001350
Iteration 66/1000 | Loss: 0.00001349
Iteration 67/1000 | Loss: 0.00001349
Iteration 68/1000 | Loss: 0.00001349
Iteration 69/1000 | Loss: 0.00001349
Iteration 70/1000 | Loss: 0.00001348
Iteration 71/1000 | Loss: 0.00001348
Iteration 72/1000 | Loss: 0.00001348
Iteration 73/1000 | Loss: 0.00001347
Iteration 74/1000 | Loss: 0.00001347
Iteration 75/1000 | Loss: 0.00001347
Iteration 76/1000 | Loss: 0.00001347
Iteration 77/1000 | Loss: 0.00001347
Iteration 78/1000 | Loss: 0.00001346
Iteration 79/1000 | Loss: 0.00001346
Iteration 80/1000 | Loss: 0.00001345
Iteration 81/1000 | Loss: 0.00001345
Iteration 82/1000 | Loss: 0.00001345
Iteration 83/1000 | Loss: 0.00001344
Iteration 84/1000 | Loss: 0.00001344
Iteration 85/1000 | Loss: 0.00001344
Iteration 86/1000 | Loss: 0.00001343
Iteration 87/1000 | Loss: 0.00001343
Iteration 88/1000 | Loss: 0.00001342
Iteration 89/1000 | Loss: 0.00001342
Iteration 90/1000 | Loss: 0.00001342
Iteration 91/1000 | Loss: 0.00001342
Iteration 92/1000 | Loss: 0.00001342
Iteration 93/1000 | Loss: 0.00001341
Iteration 94/1000 | Loss: 0.00001341
Iteration 95/1000 | Loss: 0.00001340
Iteration 96/1000 | Loss: 0.00001340
Iteration 97/1000 | Loss: 0.00001340
Iteration 98/1000 | Loss: 0.00001339
Iteration 99/1000 | Loss: 0.00001339
Iteration 100/1000 | Loss: 0.00001339
Iteration 101/1000 | Loss: 0.00001339
Iteration 102/1000 | Loss: 0.00001338
Iteration 103/1000 | Loss: 0.00001338
Iteration 104/1000 | Loss: 0.00001338
Iteration 105/1000 | Loss: 0.00001338
Iteration 106/1000 | Loss: 0.00001338
Iteration 107/1000 | Loss: 0.00001338
Iteration 108/1000 | Loss: 0.00001338
Iteration 109/1000 | Loss: 0.00001338
Iteration 110/1000 | Loss: 0.00001337
Iteration 111/1000 | Loss: 0.00001336
Iteration 112/1000 | Loss: 0.00001336
Iteration 113/1000 | Loss: 0.00001336
Iteration 114/1000 | Loss: 0.00001335
Iteration 115/1000 | Loss: 0.00001335
Iteration 116/1000 | Loss: 0.00001334
Iteration 117/1000 | Loss: 0.00001333
Iteration 118/1000 | Loss: 0.00001332
Iteration 119/1000 | Loss: 0.00001332
Iteration 120/1000 | Loss: 0.00001332
Iteration 121/1000 | Loss: 0.00001332
Iteration 122/1000 | Loss: 0.00001332
Iteration 123/1000 | Loss: 0.00001331
Iteration 124/1000 | Loss: 0.00001331
Iteration 125/1000 | Loss: 0.00001331
Iteration 126/1000 | Loss: 0.00001331
Iteration 127/1000 | Loss: 0.00001331
Iteration 128/1000 | Loss: 0.00001331
Iteration 129/1000 | Loss: 0.00001331
Iteration 130/1000 | Loss: 0.00001331
Iteration 131/1000 | Loss: 0.00001331
Iteration 132/1000 | Loss: 0.00001331
Iteration 133/1000 | Loss: 0.00001331
Iteration 134/1000 | Loss: 0.00001330
Iteration 135/1000 | Loss: 0.00001330
Iteration 136/1000 | Loss: 0.00001330
Iteration 137/1000 | Loss: 0.00001330
Iteration 138/1000 | Loss: 0.00001329
Iteration 139/1000 | Loss: 0.00001329
Iteration 140/1000 | Loss: 0.00001329
Iteration 141/1000 | Loss: 0.00001329
Iteration 142/1000 | Loss: 0.00001329
Iteration 143/1000 | Loss: 0.00001329
Iteration 144/1000 | Loss: 0.00001329
Iteration 145/1000 | Loss: 0.00001329
Iteration 146/1000 | Loss: 0.00001329
Iteration 147/1000 | Loss: 0.00001329
Iteration 148/1000 | Loss: 0.00001329
Iteration 149/1000 | Loss: 0.00001328
Iteration 150/1000 | Loss: 0.00001328
Iteration 151/1000 | Loss: 0.00001328
Iteration 152/1000 | Loss: 0.00001328
Iteration 153/1000 | Loss: 0.00001328
Iteration 154/1000 | Loss: 0.00001328
Iteration 155/1000 | Loss: 0.00001328
Iteration 156/1000 | Loss: 0.00001328
Iteration 157/1000 | Loss: 0.00001328
Iteration 158/1000 | Loss: 0.00001328
Iteration 159/1000 | Loss: 0.00001328
Iteration 160/1000 | Loss: 0.00001328
Iteration 161/1000 | Loss: 0.00001328
Iteration 162/1000 | Loss: 0.00001327
Iteration 163/1000 | Loss: 0.00001327
Iteration 164/1000 | Loss: 0.00001327
Iteration 165/1000 | Loss: 0.00001327
Iteration 166/1000 | Loss: 0.00001327
Iteration 167/1000 | Loss: 0.00001327
Iteration 168/1000 | Loss: 0.00001327
Iteration 169/1000 | Loss: 0.00001327
Iteration 170/1000 | Loss: 0.00001327
Iteration 171/1000 | Loss: 0.00001326
Iteration 172/1000 | Loss: 0.00001326
Iteration 173/1000 | Loss: 0.00001326
Iteration 174/1000 | Loss: 0.00001326
Iteration 175/1000 | Loss: 0.00001326
Iteration 176/1000 | Loss: 0.00001326
Iteration 177/1000 | Loss: 0.00001326
Iteration 178/1000 | Loss: 0.00001326
Iteration 179/1000 | Loss: 0.00001326
Iteration 180/1000 | Loss: 0.00001326
Iteration 181/1000 | Loss: 0.00001326
Iteration 182/1000 | Loss: 0.00001326
Iteration 183/1000 | Loss: 0.00001326
Iteration 184/1000 | Loss: 0.00001326
Iteration 185/1000 | Loss: 0.00001326
Iteration 186/1000 | Loss: 0.00001326
Iteration 187/1000 | Loss: 0.00001326
Iteration 188/1000 | Loss: 0.00001326
Iteration 189/1000 | Loss: 0.00001326
Iteration 190/1000 | Loss: 0.00001326
Iteration 191/1000 | Loss: 0.00001326
Iteration 192/1000 | Loss: 0.00001326
Iteration 193/1000 | Loss: 0.00001326
Iteration 194/1000 | Loss: 0.00001325
Iteration 195/1000 | Loss: 0.00001325
Iteration 196/1000 | Loss: 0.00001325
Iteration 197/1000 | Loss: 0.00001325
Iteration 198/1000 | Loss: 0.00001325
Iteration 199/1000 | Loss: 0.00001325
Iteration 200/1000 | Loss: 0.00001325
Iteration 201/1000 | Loss: 0.00001325
Iteration 202/1000 | Loss: 0.00001325
Iteration 203/1000 | Loss: 0.00001325
Iteration 204/1000 | Loss: 0.00001324
Iteration 205/1000 | Loss: 0.00001324
Iteration 206/1000 | Loss: 0.00001324
Iteration 207/1000 | Loss: 0.00001324
Iteration 208/1000 | Loss: 0.00001324
Iteration 209/1000 | Loss: 0.00001324
Iteration 210/1000 | Loss: 0.00001324
Iteration 211/1000 | Loss: 0.00001324
Iteration 212/1000 | Loss: 0.00001324
Iteration 213/1000 | Loss: 0.00001324
Iteration 214/1000 | Loss: 0.00001324
Iteration 215/1000 | Loss: 0.00001324
Iteration 216/1000 | Loss: 0.00001324
Iteration 217/1000 | Loss: 0.00001324
Iteration 218/1000 | Loss: 0.00001324
Iteration 219/1000 | Loss: 0.00001324
Iteration 220/1000 | Loss: 0.00001324
Iteration 221/1000 | Loss: 0.00001323
Iteration 222/1000 | Loss: 0.00001323
Iteration 223/1000 | Loss: 0.00001323
Iteration 224/1000 | Loss: 0.00001323
Iteration 225/1000 | Loss: 0.00001323
Iteration 226/1000 | Loss: 0.00001323
Iteration 227/1000 | Loss: 0.00001323
Iteration 228/1000 | Loss: 0.00001323
Iteration 229/1000 | Loss: 0.00001323
Iteration 230/1000 | Loss: 0.00001323
Iteration 231/1000 | Loss: 0.00001323
Iteration 232/1000 | Loss: 0.00001323
Iteration 233/1000 | Loss: 0.00001323
Iteration 234/1000 | Loss: 0.00001323
Iteration 235/1000 | Loss: 0.00001323
Iteration 236/1000 | Loss: 0.00001323
Iteration 237/1000 | Loss: 0.00001323
Iteration 238/1000 | Loss: 0.00001323
Iteration 239/1000 | Loss: 0.00001323
Iteration 240/1000 | Loss: 0.00001323
Iteration 241/1000 | Loss: 0.00001323
Iteration 242/1000 | Loss: 0.00001323
Iteration 243/1000 | Loss: 0.00001323
Iteration 244/1000 | Loss: 0.00001323
Iteration 245/1000 | Loss: 0.00001323
Iteration 246/1000 | Loss: 0.00001323
Iteration 247/1000 | Loss: 0.00001323
Iteration 248/1000 | Loss: 0.00001323
Iteration 249/1000 | Loss: 0.00001323
Iteration 250/1000 | Loss: 0.00001323
Iteration 251/1000 | Loss: 0.00001323
Iteration 252/1000 | Loss: 0.00001323
Iteration 253/1000 | Loss: 0.00001323
Iteration 254/1000 | Loss: 0.00001323
Iteration 255/1000 | Loss: 0.00001323
Iteration 256/1000 | Loss: 0.00001323
Iteration 257/1000 | Loss: 0.00001323
Iteration 258/1000 | Loss: 0.00001323
Iteration 259/1000 | Loss: 0.00001323
Iteration 260/1000 | Loss: 0.00001323
Iteration 261/1000 | Loss: 0.00001323
Iteration 262/1000 | Loss: 0.00001323
Iteration 263/1000 | Loss: 0.00001323
Iteration 264/1000 | Loss: 0.00001323
Iteration 265/1000 | Loss: 0.00001323
Iteration 266/1000 | Loss: 0.00001323
Iteration 267/1000 | Loss: 0.00001323
Iteration 268/1000 | Loss: 0.00001323
Iteration 269/1000 | Loss: 0.00001323
Iteration 270/1000 | Loss: 0.00001323
Iteration 271/1000 | Loss: 0.00001323
Iteration 272/1000 | Loss: 0.00001323
Iteration 273/1000 | Loss: 0.00001323
Iteration 274/1000 | Loss: 0.00001323
Iteration 275/1000 | Loss: 0.00001323
Iteration 276/1000 | Loss: 0.00001323
Iteration 277/1000 | Loss: 0.00001323
Iteration 278/1000 | Loss: 0.00001323
Iteration 279/1000 | Loss: 0.00001323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [1.3225317161413841e-05, 1.3225317161413841e-05, 1.3225317161413841e-05, 1.3225317161413841e-05, 1.3225317161413841e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3225317161413841e-05

Optimization complete. Final v2v error: 3.112351655960083 mm

Highest mean error: 3.2878901958465576 mm for frame 68

Lowest mean error: 2.975283145904541 mm for frame 160

Saving results

Total time: 50.786020040512085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480232
Iteration 2/25 | Loss: 0.00128661
Iteration 3/25 | Loss: 0.00120854
Iteration 4/25 | Loss: 0.00119908
Iteration 5/25 | Loss: 0.00119641
Iteration 6/25 | Loss: 0.00119604
Iteration 7/25 | Loss: 0.00119604
Iteration 8/25 | Loss: 0.00119604
Iteration 9/25 | Loss: 0.00119604
Iteration 10/25 | Loss: 0.00119604
Iteration 11/25 | Loss: 0.00119604
Iteration 12/25 | Loss: 0.00119604
Iteration 13/25 | Loss: 0.00119604
Iteration 14/25 | Loss: 0.00119604
Iteration 15/25 | Loss: 0.00119604
Iteration 16/25 | Loss: 0.00119604
Iteration 17/25 | Loss: 0.00119604
Iteration 18/25 | Loss: 0.00119604
Iteration 19/25 | Loss: 0.00119604
Iteration 20/25 | Loss: 0.00119604
Iteration 21/25 | Loss: 0.00119604
Iteration 22/25 | Loss: 0.00119604
Iteration 23/25 | Loss: 0.00119604
Iteration 24/25 | Loss: 0.00119604
Iteration 25/25 | Loss: 0.00119604

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.59846544
Iteration 2/25 | Loss: 0.00097210
Iteration 3/25 | Loss: 0.00097210
Iteration 4/25 | Loss: 0.00097209
Iteration 5/25 | Loss: 0.00097209
Iteration 6/25 | Loss: 0.00097209
Iteration 7/25 | Loss: 0.00097209
Iteration 8/25 | Loss: 0.00097209
Iteration 9/25 | Loss: 0.00097209
Iteration 10/25 | Loss: 0.00097209
Iteration 11/25 | Loss: 0.00097209
Iteration 12/25 | Loss: 0.00097209
Iteration 13/25 | Loss: 0.00097209
Iteration 14/25 | Loss: 0.00097209
Iteration 15/25 | Loss: 0.00097209
Iteration 16/25 | Loss: 0.00097209
Iteration 17/25 | Loss: 0.00097209
Iteration 18/25 | Loss: 0.00097209
Iteration 19/25 | Loss: 0.00097209
Iteration 20/25 | Loss: 0.00097209
Iteration 21/25 | Loss: 0.00097209
Iteration 22/25 | Loss: 0.00097209
Iteration 23/25 | Loss: 0.00097209
Iteration 24/25 | Loss: 0.00097209
Iteration 25/25 | Loss: 0.00097209

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097209
Iteration 2/1000 | Loss: 0.00002205
Iteration 3/1000 | Loss: 0.00001734
Iteration 4/1000 | Loss: 0.00001593
Iteration 5/1000 | Loss: 0.00001493
Iteration 6/1000 | Loss: 0.00001415
Iteration 7/1000 | Loss: 0.00001356
Iteration 8/1000 | Loss: 0.00001313
Iteration 9/1000 | Loss: 0.00001291
Iteration 10/1000 | Loss: 0.00001268
Iteration 11/1000 | Loss: 0.00001250
Iteration 12/1000 | Loss: 0.00001250
Iteration 13/1000 | Loss: 0.00001245
Iteration 14/1000 | Loss: 0.00001230
Iteration 15/1000 | Loss: 0.00001228
Iteration 16/1000 | Loss: 0.00001222
Iteration 17/1000 | Loss: 0.00001221
Iteration 18/1000 | Loss: 0.00001220
Iteration 19/1000 | Loss: 0.00001220
Iteration 20/1000 | Loss: 0.00001214
Iteration 21/1000 | Loss: 0.00001210
Iteration 22/1000 | Loss: 0.00001209
Iteration 23/1000 | Loss: 0.00001209
Iteration 24/1000 | Loss: 0.00001208
Iteration 25/1000 | Loss: 0.00001208
Iteration 26/1000 | Loss: 0.00001206
Iteration 27/1000 | Loss: 0.00001205
Iteration 28/1000 | Loss: 0.00001203
Iteration 29/1000 | Loss: 0.00001203
Iteration 30/1000 | Loss: 0.00001202
Iteration 31/1000 | Loss: 0.00001201
Iteration 32/1000 | Loss: 0.00001199
Iteration 33/1000 | Loss: 0.00001196
Iteration 34/1000 | Loss: 0.00001196
Iteration 35/1000 | Loss: 0.00001196
Iteration 36/1000 | Loss: 0.00001195
Iteration 37/1000 | Loss: 0.00001195
Iteration 38/1000 | Loss: 0.00001194
Iteration 39/1000 | Loss: 0.00001193
Iteration 40/1000 | Loss: 0.00001193
Iteration 41/1000 | Loss: 0.00001192
Iteration 42/1000 | Loss: 0.00001192
Iteration 43/1000 | Loss: 0.00001192
Iteration 44/1000 | Loss: 0.00001192
Iteration 45/1000 | Loss: 0.00001190
Iteration 46/1000 | Loss: 0.00001189
Iteration 47/1000 | Loss: 0.00001189
Iteration 48/1000 | Loss: 0.00001189
Iteration 49/1000 | Loss: 0.00001189
Iteration 50/1000 | Loss: 0.00001189
Iteration 51/1000 | Loss: 0.00001188
Iteration 52/1000 | Loss: 0.00001188
Iteration 53/1000 | Loss: 0.00001188
Iteration 54/1000 | Loss: 0.00001187
Iteration 55/1000 | Loss: 0.00001187
Iteration 56/1000 | Loss: 0.00001187
Iteration 57/1000 | Loss: 0.00001187
Iteration 58/1000 | Loss: 0.00001187
Iteration 59/1000 | Loss: 0.00001186
Iteration 60/1000 | Loss: 0.00001186
Iteration 61/1000 | Loss: 0.00001186
Iteration 62/1000 | Loss: 0.00001185
Iteration 63/1000 | Loss: 0.00001185
Iteration 64/1000 | Loss: 0.00001185
Iteration 65/1000 | Loss: 0.00001185
Iteration 66/1000 | Loss: 0.00001185
Iteration 67/1000 | Loss: 0.00001184
Iteration 68/1000 | Loss: 0.00001182
Iteration 69/1000 | Loss: 0.00001179
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001176
Iteration 72/1000 | Loss: 0.00001176
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001174
Iteration 75/1000 | Loss: 0.00001174
Iteration 76/1000 | Loss: 0.00001173
Iteration 77/1000 | Loss: 0.00001173
Iteration 78/1000 | Loss: 0.00001172
Iteration 79/1000 | Loss: 0.00001172
Iteration 80/1000 | Loss: 0.00001172
Iteration 81/1000 | Loss: 0.00001171
Iteration 82/1000 | Loss: 0.00001171
Iteration 83/1000 | Loss: 0.00001170
Iteration 84/1000 | Loss: 0.00001170
Iteration 85/1000 | Loss: 0.00001170
Iteration 86/1000 | Loss: 0.00001170
Iteration 87/1000 | Loss: 0.00001170
Iteration 88/1000 | Loss: 0.00001169
Iteration 89/1000 | Loss: 0.00001169
Iteration 90/1000 | Loss: 0.00001169
Iteration 91/1000 | Loss: 0.00001168
Iteration 92/1000 | Loss: 0.00001168
Iteration 93/1000 | Loss: 0.00001168
Iteration 94/1000 | Loss: 0.00001167
Iteration 95/1000 | Loss: 0.00001167
Iteration 96/1000 | Loss: 0.00001167
Iteration 97/1000 | Loss: 0.00001167
Iteration 98/1000 | Loss: 0.00001167
Iteration 99/1000 | Loss: 0.00001167
Iteration 100/1000 | Loss: 0.00001167
Iteration 101/1000 | Loss: 0.00001167
Iteration 102/1000 | Loss: 0.00001167
Iteration 103/1000 | Loss: 0.00001166
Iteration 104/1000 | Loss: 0.00001166
Iteration 105/1000 | Loss: 0.00001166
Iteration 106/1000 | Loss: 0.00001166
Iteration 107/1000 | Loss: 0.00001166
Iteration 108/1000 | Loss: 0.00001165
Iteration 109/1000 | Loss: 0.00001165
Iteration 110/1000 | Loss: 0.00001165
Iteration 111/1000 | Loss: 0.00001165
Iteration 112/1000 | Loss: 0.00001165
Iteration 113/1000 | Loss: 0.00001164
Iteration 114/1000 | Loss: 0.00001164
Iteration 115/1000 | Loss: 0.00001164
Iteration 116/1000 | Loss: 0.00001164
Iteration 117/1000 | Loss: 0.00001163
Iteration 118/1000 | Loss: 0.00001163
Iteration 119/1000 | Loss: 0.00001163
Iteration 120/1000 | Loss: 0.00001163
Iteration 121/1000 | Loss: 0.00001163
Iteration 122/1000 | Loss: 0.00001163
Iteration 123/1000 | Loss: 0.00001163
Iteration 124/1000 | Loss: 0.00001163
Iteration 125/1000 | Loss: 0.00001163
Iteration 126/1000 | Loss: 0.00001163
Iteration 127/1000 | Loss: 0.00001163
Iteration 128/1000 | Loss: 0.00001162
Iteration 129/1000 | Loss: 0.00001162
Iteration 130/1000 | Loss: 0.00001162
Iteration 131/1000 | Loss: 0.00001162
Iteration 132/1000 | Loss: 0.00001162
Iteration 133/1000 | Loss: 0.00001162
Iteration 134/1000 | Loss: 0.00001162
Iteration 135/1000 | Loss: 0.00001162
Iteration 136/1000 | Loss: 0.00001162
Iteration 137/1000 | Loss: 0.00001162
Iteration 138/1000 | Loss: 0.00001162
Iteration 139/1000 | Loss: 0.00001161
Iteration 140/1000 | Loss: 0.00001161
Iteration 141/1000 | Loss: 0.00001161
Iteration 142/1000 | Loss: 0.00001161
Iteration 143/1000 | Loss: 0.00001161
Iteration 144/1000 | Loss: 0.00001161
Iteration 145/1000 | Loss: 0.00001161
Iteration 146/1000 | Loss: 0.00001161
Iteration 147/1000 | Loss: 0.00001161
Iteration 148/1000 | Loss: 0.00001161
Iteration 149/1000 | Loss: 0.00001161
Iteration 150/1000 | Loss: 0.00001161
Iteration 151/1000 | Loss: 0.00001161
Iteration 152/1000 | Loss: 0.00001161
Iteration 153/1000 | Loss: 0.00001161
Iteration 154/1000 | Loss: 0.00001161
Iteration 155/1000 | Loss: 0.00001161
Iteration 156/1000 | Loss: 0.00001161
Iteration 157/1000 | Loss: 0.00001160
Iteration 158/1000 | Loss: 0.00001160
Iteration 159/1000 | Loss: 0.00001160
Iteration 160/1000 | Loss: 0.00001160
Iteration 161/1000 | Loss: 0.00001160
Iteration 162/1000 | Loss: 0.00001160
Iteration 163/1000 | Loss: 0.00001160
Iteration 164/1000 | Loss: 0.00001160
Iteration 165/1000 | Loss: 0.00001160
Iteration 166/1000 | Loss: 0.00001159
Iteration 167/1000 | Loss: 0.00001159
Iteration 168/1000 | Loss: 0.00001159
Iteration 169/1000 | Loss: 0.00001159
Iteration 170/1000 | Loss: 0.00001159
Iteration 171/1000 | Loss: 0.00001159
Iteration 172/1000 | Loss: 0.00001159
Iteration 173/1000 | Loss: 0.00001159
Iteration 174/1000 | Loss: 0.00001159
Iteration 175/1000 | Loss: 0.00001159
Iteration 176/1000 | Loss: 0.00001159
Iteration 177/1000 | Loss: 0.00001159
Iteration 178/1000 | Loss: 0.00001159
Iteration 179/1000 | Loss: 0.00001159
Iteration 180/1000 | Loss: 0.00001159
Iteration 181/1000 | Loss: 0.00001159
Iteration 182/1000 | Loss: 0.00001159
Iteration 183/1000 | Loss: 0.00001159
Iteration 184/1000 | Loss: 0.00001159
Iteration 185/1000 | Loss: 0.00001159
Iteration 186/1000 | Loss: 0.00001159
Iteration 187/1000 | Loss: 0.00001159
Iteration 188/1000 | Loss: 0.00001159
Iteration 189/1000 | Loss: 0.00001159
Iteration 190/1000 | Loss: 0.00001159
Iteration 191/1000 | Loss: 0.00001159
Iteration 192/1000 | Loss: 0.00001159
Iteration 193/1000 | Loss: 0.00001159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.1590290341700893e-05, 1.1590290341700893e-05, 1.1590290341700893e-05, 1.1590290341700893e-05, 1.1590290341700893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1590290341700893e-05

Optimization complete. Final v2v error: 2.9255013465881348 mm

Highest mean error: 3.3829751014709473 mm for frame 62

Lowest mean error: 2.6519734859466553 mm for frame 133

Saving results

Total time: 42.04954481124878
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486437
Iteration 2/25 | Loss: 0.00133099
Iteration 3/25 | Loss: 0.00127202
Iteration 4/25 | Loss: 0.00126313
Iteration 5/25 | Loss: 0.00125921
Iteration 6/25 | Loss: 0.00125911
Iteration 7/25 | Loss: 0.00125911
Iteration 8/25 | Loss: 0.00125911
Iteration 9/25 | Loss: 0.00125911
Iteration 10/25 | Loss: 0.00125911
Iteration 11/25 | Loss: 0.00125911
Iteration 12/25 | Loss: 0.00125911
Iteration 13/25 | Loss: 0.00125911
Iteration 14/25 | Loss: 0.00125911
Iteration 15/25 | Loss: 0.00125911
Iteration 16/25 | Loss: 0.00125911
Iteration 17/25 | Loss: 0.00125911
Iteration 18/25 | Loss: 0.00125911
Iteration 19/25 | Loss: 0.00125911
Iteration 20/25 | Loss: 0.00125911
Iteration 21/25 | Loss: 0.00125911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012591095874086022, 0.0012591095874086022, 0.0012591095874086022, 0.0012591095874086022, 0.0012591095874086022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012591095874086022

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42511499
Iteration 2/25 | Loss: 0.00096500
Iteration 3/25 | Loss: 0.00096499
Iteration 4/25 | Loss: 0.00096498
Iteration 5/25 | Loss: 0.00096498
Iteration 6/25 | Loss: 0.00096498
Iteration 7/25 | Loss: 0.00096498
Iteration 8/25 | Loss: 0.00096498
Iteration 9/25 | Loss: 0.00096498
Iteration 10/25 | Loss: 0.00096498
Iteration 11/25 | Loss: 0.00096498
Iteration 12/25 | Loss: 0.00096498
Iteration 13/25 | Loss: 0.00096498
Iteration 14/25 | Loss: 0.00096498
Iteration 15/25 | Loss: 0.00096498
Iteration 16/25 | Loss: 0.00096498
Iteration 17/25 | Loss: 0.00096498
Iteration 18/25 | Loss: 0.00096498
Iteration 19/25 | Loss: 0.00096498
Iteration 20/25 | Loss: 0.00096498
Iteration 21/25 | Loss: 0.00096498
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009649822022765875, 0.0009649822022765875, 0.0009649822022765875, 0.0009649822022765875, 0.0009649822022765875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009649822022765875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096498
Iteration 2/1000 | Loss: 0.00002761
Iteration 3/1000 | Loss: 0.00001925
Iteration 4/1000 | Loss: 0.00001754
Iteration 5/1000 | Loss: 0.00001644
Iteration 6/1000 | Loss: 0.00001590
Iteration 7/1000 | Loss: 0.00001552
Iteration 8/1000 | Loss: 0.00001550
Iteration 9/1000 | Loss: 0.00001539
Iteration 10/1000 | Loss: 0.00001510
Iteration 11/1000 | Loss: 0.00001505
Iteration 12/1000 | Loss: 0.00001480
Iteration 13/1000 | Loss: 0.00001467
Iteration 14/1000 | Loss: 0.00001463
Iteration 15/1000 | Loss: 0.00001455
Iteration 16/1000 | Loss: 0.00001445
Iteration 17/1000 | Loss: 0.00001443
Iteration 18/1000 | Loss: 0.00001437
Iteration 19/1000 | Loss: 0.00001436
Iteration 20/1000 | Loss: 0.00001436
Iteration 21/1000 | Loss: 0.00001435
Iteration 22/1000 | Loss: 0.00001425
Iteration 23/1000 | Loss: 0.00001425
Iteration 24/1000 | Loss: 0.00001423
Iteration 25/1000 | Loss: 0.00001423
Iteration 26/1000 | Loss: 0.00001423
Iteration 27/1000 | Loss: 0.00001423
Iteration 28/1000 | Loss: 0.00001423
Iteration 29/1000 | Loss: 0.00001422
Iteration 30/1000 | Loss: 0.00001422
Iteration 31/1000 | Loss: 0.00001419
Iteration 32/1000 | Loss: 0.00001419
Iteration 33/1000 | Loss: 0.00001419
Iteration 34/1000 | Loss: 0.00001418
Iteration 35/1000 | Loss: 0.00001418
Iteration 36/1000 | Loss: 0.00001417
Iteration 37/1000 | Loss: 0.00001416
Iteration 38/1000 | Loss: 0.00001413
Iteration 39/1000 | Loss: 0.00001413
Iteration 40/1000 | Loss: 0.00001412
Iteration 41/1000 | Loss: 0.00001410
Iteration 42/1000 | Loss: 0.00001407
Iteration 43/1000 | Loss: 0.00001406
Iteration 44/1000 | Loss: 0.00001406
Iteration 45/1000 | Loss: 0.00001401
Iteration 46/1000 | Loss: 0.00001399
Iteration 47/1000 | Loss: 0.00001398
Iteration 48/1000 | Loss: 0.00001398
Iteration 49/1000 | Loss: 0.00001395
Iteration 50/1000 | Loss: 0.00001395
Iteration 51/1000 | Loss: 0.00001394
Iteration 52/1000 | Loss: 0.00001394
Iteration 53/1000 | Loss: 0.00001394
Iteration 54/1000 | Loss: 0.00001393
Iteration 55/1000 | Loss: 0.00001393
Iteration 56/1000 | Loss: 0.00001393
Iteration 57/1000 | Loss: 0.00001392
Iteration 58/1000 | Loss: 0.00001392
Iteration 59/1000 | Loss: 0.00001392
Iteration 60/1000 | Loss: 0.00001391
Iteration 61/1000 | Loss: 0.00001391
Iteration 62/1000 | Loss: 0.00001391
Iteration 63/1000 | Loss: 0.00001390
Iteration 64/1000 | Loss: 0.00001390
Iteration 65/1000 | Loss: 0.00001390
Iteration 66/1000 | Loss: 0.00001390
Iteration 67/1000 | Loss: 0.00001390
Iteration 68/1000 | Loss: 0.00001390
Iteration 69/1000 | Loss: 0.00001390
Iteration 70/1000 | Loss: 0.00001389
Iteration 71/1000 | Loss: 0.00001389
Iteration 72/1000 | Loss: 0.00001389
Iteration 73/1000 | Loss: 0.00001389
Iteration 74/1000 | Loss: 0.00001389
Iteration 75/1000 | Loss: 0.00001388
Iteration 76/1000 | Loss: 0.00001388
Iteration 77/1000 | Loss: 0.00001388
Iteration 78/1000 | Loss: 0.00001387
Iteration 79/1000 | Loss: 0.00001387
Iteration 80/1000 | Loss: 0.00001387
Iteration 81/1000 | Loss: 0.00001387
Iteration 82/1000 | Loss: 0.00001387
Iteration 83/1000 | Loss: 0.00001387
Iteration 84/1000 | Loss: 0.00001387
Iteration 85/1000 | Loss: 0.00001387
Iteration 86/1000 | Loss: 0.00001387
Iteration 87/1000 | Loss: 0.00001386
Iteration 88/1000 | Loss: 0.00001386
Iteration 89/1000 | Loss: 0.00001385
Iteration 90/1000 | Loss: 0.00001385
Iteration 91/1000 | Loss: 0.00001385
Iteration 92/1000 | Loss: 0.00001385
Iteration 93/1000 | Loss: 0.00001385
Iteration 94/1000 | Loss: 0.00001385
Iteration 95/1000 | Loss: 0.00001385
Iteration 96/1000 | Loss: 0.00001385
Iteration 97/1000 | Loss: 0.00001384
Iteration 98/1000 | Loss: 0.00001384
Iteration 99/1000 | Loss: 0.00001384
Iteration 100/1000 | Loss: 0.00001384
Iteration 101/1000 | Loss: 0.00001384
Iteration 102/1000 | Loss: 0.00001384
Iteration 103/1000 | Loss: 0.00001384
Iteration 104/1000 | Loss: 0.00001384
Iteration 105/1000 | Loss: 0.00001384
Iteration 106/1000 | Loss: 0.00001383
Iteration 107/1000 | Loss: 0.00001383
Iteration 108/1000 | Loss: 0.00001383
Iteration 109/1000 | Loss: 0.00001383
Iteration 110/1000 | Loss: 0.00001383
Iteration 111/1000 | Loss: 0.00001382
Iteration 112/1000 | Loss: 0.00001382
Iteration 113/1000 | Loss: 0.00001382
Iteration 114/1000 | Loss: 0.00001382
Iteration 115/1000 | Loss: 0.00001382
Iteration 116/1000 | Loss: 0.00001381
Iteration 117/1000 | Loss: 0.00001381
Iteration 118/1000 | Loss: 0.00001381
Iteration 119/1000 | Loss: 0.00001381
Iteration 120/1000 | Loss: 0.00001381
Iteration 121/1000 | Loss: 0.00001381
Iteration 122/1000 | Loss: 0.00001381
Iteration 123/1000 | Loss: 0.00001381
Iteration 124/1000 | Loss: 0.00001381
Iteration 125/1000 | Loss: 0.00001381
Iteration 126/1000 | Loss: 0.00001381
Iteration 127/1000 | Loss: 0.00001380
Iteration 128/1000 | Loss: 0.00001380
Iteration 129/1000 | Loss: 0.00001380
Iteration 130/1000 | Loss: 0.00001380
Iteration 131/1000 | Loss: 0.00001380
Iteration 132/1000 | Loss: 0.00001380
Iteration 133/1000 | Loss: 0.00001380
Iteration 134/1000 | Loss: 0.00001380
Iteration 135/1000 | Loss: 0.00001380
Iteration 136/1000 | Loss: 0.00001380
Iteration 137/1000 | Loss: 0.00001380
Iteration 138/1000 | Loss: 0.00001380
Iteration 139/1000 | Loss: 0.00001380
Iteration 140/1000 | Loss: 0.00001379
Iteration 141/1000 | Loss: 0.00001379
Iteration 142/1000 | Loss: 0.00001379
Iteration 143/1000 | Loss: 0.00001379
Iteration 144/1000 | Loss: 0.00001379
Iteration 145/1000 | Loss: 0.00001379
Iteration 146/1000 | Loss: 0.00001379
Iteration 147/1000 | Loss: 0.00001379
Iteration 148/1000 | Loss: 0.00001379
Iteration 149/1000 | Loss: 0.00001379
Iteration 150/1000 | Loss: 0.00001379
Iteration 151/1000 | Loss: 0.00001379
Iteration 152/1000 | Loss: 0.00001378
Iteration 153/1000 | Loss: 0.00001378
Iteration 154/1000 | Loss: 0.00001378
Iteration 155/1000 | Loss: 0.00001378
Iteration 156/1000 | Loss: 0.00001378
Iteration 157/1000 | Loss: 0.00001378
Iteration 158/1000 | Loss: 0.00001378
Iteration 159/1000 | Loss: 0.00001377
Iteration 160/1000 | Loss: 0.00001377
Iteration 161/1000 | Loss: 0.00001377
Iteration 162/1000 | Loss: 0.00001377
Iteration 163/1000 | Loss: 0.00001377
Iteration 164/1000 | Loss: 0.00001377
Iteration 165/1000 | Loss: 0.00001377
Iteration 166/1000 | Loss: 0.00001377
Iteration 167/1000 | Loss: 0.00001377
Iteration 168/1000 | Loss: 0.00001377
Iteration 169/1000 | Loss: 0.00001376
Iteration 170/1000 | Loss: 0.00001376
Iteration 171/1000 | Loss: 0.00001376
Iteration 172/1000 | Loss: 0.00001376
Iteration 173/1000 | Loss: 0.00001376
Iteration 174/1000 | Loss: 0.00001376
Iteration 175/1000 | Loss: 0.00001376
Iteration 176/1000 | Loss: 0.00001376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.376486943627242e-05, 1.376486943627242e-05, 1.376486943627242e-05, 1.376486943627242e-05, 1.376486943627242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.376486943627242e-05

Optimization complete. Final v2v error: 3.1421358585357666 mm

Highest mean error: 3.6111648082733154 mm for frame 105

Lowest mean error: 2.9245173931121826 mm for frame 225

Saving results

Total time: 45.83092212677002
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00735527
Iteration 2/25 | Loss: 0.00154428
Iteration 3/25 | Loss: 0.00134336
Iteration 4/25 | Loss: 0.00132633
Iteration 5/25 | Loss: 0.00132414
Iteration 6/25 | Loss: 0.00132414
Iteration 7/25 | Loss: 0.00132414
Iteration 8/25 | Loss: 0.00132414
Iteration 9/25 | Loss: 0.00132414
Iteration 10/25 | Loss: 0.00132414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013241448905318975, 0.0013241448905318975, 0.0013241448905318975, 0.0013241448905318975, 0.0013241448905318975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013241448905318975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.57385969
Iteration 2/25 | Loss: 0.00104558
Iteration 3/25 | Loss: 0.00104558
Iteration 4/25 | Loss: 0.00104558
Iteration 5/25 | Loss: 0.00104558
Iteration 6/25 | Loss: 0.00104557
Iteration 7/25 | Loss: 0.00104557
Iteration 8/25 | Loss: 0.00104557
Iteration 9/25 | Loss: 0.00104557
Iteration 10/25 | Loss: 0.00104557
Iteration 11/25 | Loss: 0.00104557
Iteration 12/25 | Loss: 0.00104557
Iteration 13/25 | Loss: 0.00104557
Iteration 14/25 | Loss: 0.00104557
Iteration 15/25 | Loss: 0.00104557
Iteration 16/25 | Loss: 0.00104557
Iteration 17/25 | Loss: 0.00104557
Iteration 18/25 | Loss: 0.00104557
Iteration 19/25 | Loss: 0.00104557
Iteration 20/25 | Loss: 0.00104557
Iteration 21/25 | Loss: 0.00104557
Iteration 22/25 | Loss: 0.00104557
Iteration 23/25 | Loss: 0.00104557
Iteration 24/25 | Loss: 0.00104557
Iteration 25/25 | Loss: 0.00104557

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104557
Iteration 2/1000 | Loss: 0.00003605
Iteration 3/1000 | Loss: 0.00002650
Iteration 4/1000 | Loss: 0.00002311
Iteration 5/1000 | Loss: 0.00002187
Iteration 6/1000 | Loss: 0.00002121
Iteration 7/1000 | Loss: 0.00002070
Iteration 8/1000 | Loss: 0.00002040
Iteration 9/1000 | Loss: 0.00001999
Iteration 10/1000 | Loss: 0.00001971
Iteration 11/1000 | Loss: 0.00001966
Iteration 12/1000 | Loss: 0.00001965
Iteration 13/1000 | Loss: 0.00001945
Iteration 14/1000 | Loss: 0.00001940
Iteration 15/1000 | Loss: 0.00001940
Iteration 16/1000 | Loss: 0.00001931
Iteration 17/1000 | Loss: 0.00001922
Iteration 18/1000 | Loss: 0.00001916
Iteration 19/1000 | Loss: 0.00001914
Iteration 20/1000 | Loss: 0.00001913
Iteration 21/1000 | Loss: 0.00001911
Iteration 22/1000 | Loss: 0.00001896
Iteration 23/1000 | Loss: 0.00001894
Iteration 24/1000 | Loss: 0.00001891
Iteration 25/1000 | Loss: 0.00001890
Iteration 26/1000 | Loss: 0.00001889
Iteration 27/1000 | Loss: 0.00001887
Iteration 28/1000 | Loss: 0.00001883
Iteration 29/1000 | Loss: 0.00001880
Iteration 30/1000 | Loss: 0.00001878
Iteration 31/1000 | Loss: 0.00001878
Iteration 32/1000 | Loss: 0.00001877
Iteration 33/1000 | Loss: 0.00001877
Iteration 34/1000 | Loss: 0.00001877
Iteration 35/1000 | Loss: 0.00001876
Iteration 36/1000 | Loss: 0.00001876
Iteration 37/1000 | Loss: 0.00001876
Iteration 38/1000 | Loss: 0.00001876
Iteration 39/1000 | Loss: 0.00001875
Iteration 40/1000 | Loss: 0.00001874
Iteration 41/1000 | Loss: 0.00001874
Iteration 42/1000 | Loss: 0.00001873
Iteration 43/1000 | Loss: 0.00001873
Iteration 44/1000 | Loss: 0.00001873
Iteration 45/1000 | Loss: 0.00001867
Iteration 46/1000 | Loss: 0.00001867
Iteration 47/1000 | Loss: 0.00001867
Iteration 48/1000 | Loss: 0.00001867
Iteration 49/1000 | Loss: 0.00001867
Iteration 50/1000 | Loss: 0.00001867
Iteration 51/1000 | Loss: 0.00001867
Iteration 52/1000 | Loss: 0.00001867
Iteration 53/1000 | Loss: 0.00001867
Iteration 54/1000 | Loss: 0.00001867
Iteration 55/1000 | Loss: 0.00001867
Iteration 56/1000 | Loss: 0.00001867
Iteration 57/1000 | Loss: 0.00001867
Iteration 58/1000 | Loss: 0.00001864
Iteration 59/1000 | Loss: 0.00001864
Iteration 60/1000 | Loss: 0.00001863
Iteration 61/1000 | Loss: 0.00001863
Iteration 62/1000 | Loss: 0.00001862
Iteration 63/1000 | Loss: 0.00001861
Iteration 64/1000 | Loss: 0.00001861
Iteration 65/1000 | Loss: 0.00001861
Iteration 66/1000 | Loss: 0.00001861
Iteration 67/1000 | Loss: 0.00001861
Iteration 68/1000 | Loss: 0.00001860
Iteration 69/1000 | Loss: 0.00001859
Iteration 70/1000 | Loss: 0.00001859
Iteration 71/1000 | Loss: 0.00001859
Iteration 72/1000 | Loss: 0.00001858
Iteration 73/1000 | Loss: 0.00001858
Iteration 74/1000 | Loss: 0.00001857
Iteration 75/1000 | Loss: 0.00001857
Iteration 76/1000 | Loss: 0.00001856
Iteration 77/1000 | Loss: 0.00001856
Iteration 78/1000 | Loss: 0.00001855
Iteration 79/1000 | Loss: 0.00001855
Iteration 80/1000 | Loss: 0.00001854
Iteration 81/1000 | Loss: 0.00001854
Iteration 82/1000 | Loss: 0.00001853
Iteration 83/1000 | Loss: 0.00001853
Iteration 84/1000 | Loss: 0.00001853
Iteration 85/1000 | Loss: 0.00001853
Iteration 86/1000 | Loss: 0.00001853
Iteration 87/1000 | Loss: 0.00001853
Iteration 88/1000 | Loss: 0.00001853
Iteration 89/1000 | Loss: 0.00001853
Iteration 90/1000 | Loss: 0.00001853
Iteration 91/1000 | Loss: 0.00001853
Iteration 92/1000 | Loss: 0.00001853
Iteration 93/1000 | Loss: 0.00001853
Iteration 94/1000 | Loss: 0.00001853
Iteration 95/1000 | Loss: 0.00001853
Iteration 96/1000 | Loss: 0.00001853
Iteration 97/1000 | Loss: 0.00001853
Iteration 98/1000 | Loss: 0.00001853
Iteration 99/1000 | Loss: 0.00001853
Iteration 100/1000 | Loss: 0.00001853
Iteration 101/1000 | Loss: 0.00001853
Iteration 102/1000 | Loss: 0.00001853
Iteration 103/1000 | Loss: 0.00001853
Iteration 104/1000 | Loss: 0.00001853
Iteration 105/1000 | Loss: 0.00001853
Iteration 106/1000 | Loss: 0.00001853
Iteration 107/1000 | Loss: 0.00001853
Iteration 108/1000 | Loss: 0.00001853
Iteration 109/1000 | Loss: 0.00001853
Iteration 110/1000 | Loss: 0.00001853
Iteration 111/1000 | Loss: 0.00001853
Iteration 112/1000 | Loss: 0.00001853
Iteration 113/1000 | Loss: 0.00001853
Iteration 114/1000 | Loss: 0.00001853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.8525846826378256e-05, 1.8525846826378256e-05, 1.8525846826378256e-05, 1.8525846826378256e-05, 1.8525846826378256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8525846826378256e-05

Optimization complete. Final v2v error: 3.523162603378296 mm

Highest mean error: 4.549972057342529 mm for frame 188

Lowest mean error: 2.9133872985839844 mm for frame 16

Saving results

Total time: 41.108505964279175
