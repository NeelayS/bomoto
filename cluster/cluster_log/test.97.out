Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=97, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 5432-5487
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417904
Iteration 2/25 | Loss: 0.00131653
Iteration 3/25 | Loss: 0.00126915
Iteration 4/25 | Loss: 0.00126282
Iteration 5/25 | Loss: 0.00126117
Iteration 6/25 | Loss: 0.00126117
Iteration 7/25 | Loss: 0.00126117
Iteration 8/25 | Loss: 0.00126117
Iteration 9/25 | Loss: 0.00126117
Iteration 10/25 | Loss: 0.00126117
Iteration 11/25 | Loss: 0.00126117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00126117211766541, 0.00126117211766541, 0.00126117211766541, 0.00126117211766541, 0.00126117211766541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00126117211766541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.11050844
Iteration 2/25 | Loss: 0.00132857
Iteration 3/25 | Loss: 0.00132857
Iteration 4/25 | Loss: 0.00132857
Iteration 5/25 | Loss: 0.00132857
Iteration 6/25 | Loss: 0.00132857
Iteration 7/25 | Loss: 0.00132857
Iteration 8/25 | Loss: 0.00132857
Iteration 9/25 | Loss: 0.00132857
Iteration 10/25 | Loss: 0.00132857
Iteration 11/25 | Loss: 0.00132856
Iteration 12/25 | Loss: 0.00132856
Iteration 13/25 | Loss: 0.00132856
Iteration 14/25 | Loss: 0.00132856
Iteration 15/25 | Loss: 0.00132856
Iteration 16/25 | Loss: 0.00132856
Iteration 17/25 | Loss: 0.00132856
Iteration 18/25 | Loss: 0.00132856
Iteration 19/25 | Loss: 0.00132856
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001328564714640379, 0.001328564714640379, 0.001328564714640379, 0.001328564714640379, 0.001328564714640379]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001328564714640379

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132856
Iteration 2/1000 | Loss: 0.00002166
Iteration 3/1000 | Loss: 0.00001813
Iteration 4/1000 | Loss: 0.00001640
Iteration 5/1000 | Loss: 0.00001538
Iteration 6/1000 | Loss: 0.00001492
Iteration 7/1000 | Loss: 0.00001452
Iteration 8/1000 | Loss: 0.00001407
Iteration 9/1000 | Loss: 0.00001376
Iteration 10/1000 | Loss: 0.00001375
Iteration 11/1000 | Loss: 0.00001374
Iteration 12/1000 | Loss: 0.00001362
Iteration 13/1000 | Loss: 0.00001347
Iteration 14/1000 | Loss: 0.00001336
Iteration 15/1000 | Loss: 0.00001335
Iteration 16/1000 | Loss: 0.00001328
Iteration 17/1000 | Loss: 0.00001318
Iteration 18/1000 | Loss: 0.00001310
Iteration 19/1000 | Loss: 0.00001294
Iteration 20/1000 | Loss: 0.00001288
Iteration 21/1000 | Loss: 0.00001287
Iteration 22/1000 | Loss: 0.00001285
Iteration 23/1000 | Loss: 0.00001284
Iteration 24/1000 | Loss: 0.00001284
Iteration 25/1000 | Loss: 0.00001284
Iteration 26/1000 | Loss: 0.00001283
Iteration 27/1000 | Loss: 0.00001282
Iteration 28/1000 | Loss: 0.00001278
Iteration 29/1000 | Loss: 0.00001278
Iteration 30/1000 | Loss: 0.00001275
Iteration 31/1000 | Loss: 0.00001274
Iteration 32/1000 | Loss: 0.00001274
Iteration 33/1000 | Loss: 0.00001274
Iteration 34/1000 | Loss: 0.00001273
Iteration 35/1000 | Loss: 0.00001273
Iteration 36/1000 | Loss: 0.00001273
Iteration 37/1000 | Loss: 0.00001272
Iteration 38/1000 | Loss: 0.00001271
Iteration 39/1000 | Loss: 0.00001271
Iteration 40/1000 | Loss: 0.00001267
Iteration 41/1000 | Loss: 0.00001267
Iteration 42/1000 | Loss: 0.00001267
Iteration 43/1000 | Loss: 0.00001263
Iteration 44/1000 | Loss: 0.00001262
Iteration 45/1000 | Loss: 0.00001262
Iteration 46/1000 | Loss: 0.00001262
Iteration 47/1000 | Loss: 0.00001262
Iteration 48/1000 | Loss: 0.00001261
Iteration 49/1000 | Loss: 0.00001260
Iteration 50/1000 | Loss: 0.00001258
Iteration 51/1000 | Loss: 0.00001257
Iteration 52/1000 | Loss: 0.00001257
Iteration 53/1000 | Loss: 0.00001256
Iteration 54/1000 | Loss: 0.00001256
Iteration 55/1000 | Loss: 0.00001256
Iteration 56/1000 | Loss: 0.00001255
Iteration 57/1000 | Loss: 0.00001255
Iteration 58/1000 | Loss: 0.00001253
Iteration 59/1000 | Loss: 0.00001253
Iteration 60/1000 | Loss: 0.00001252
Iteration 61/1000 | Loss: 0.00001252
Iteration 62/1000 | Loss: 0.00001252
Iteration 63/1000 | Loss: 0.00001251
Iteration 64/1000 | Loss: 0.00001250
Iteration 65/1000 | Loss: 0.00001250
Iteration 66/1000 | Loss: 0.00001250
Iteration 67/1000 | Loss: 0.00001250
Iteration 68/1000 | Loss: 0.00001250
Iteration 69/1000 | Loss: 0.00001250
Iteration 70/1000 | Loss: 0.00001250
Iteration 71/1000 | Loss: 0.00001250
Iteration 72/1000 | Loss: 0.00001250
Iteration 73/1000 | Loss: 0.00001250
Iteration 74/1000 | Loss: 0.00001250
Iteration 75/1000 | Loss: 0.00001250
Iteration 76/1000 | Loss: 0.00001250
Iteration 77/1000 | Loss: 0.00001250
Iteration 78/1000 | Loss: 0.00001250
Iteration 79/1000 | Loss: 0.00001250
Iteration 80/1000 | Loss: 0.00001250
Iteration 81/1000 | Loss: 0.00001250
Iteration 82/1000 | Loss: 0.00001250
Iteration 83/1000 | Loss: 0.00001250
Iteration 84/1000 | Loss: 0.00001250
Iteration 85/1000 | Loss: 0.00001250
Iteration 86/1000 | Loss: 0.00001250
Iteration 87/1000 | Loss: 0.00001250
Iteration 88/1000 | Loss: 0.00001250
Iteration 89/1000 | Loss: 0.00001250
Iteration 90/1000 | Loss: 0.00001250
Iteration 91/1000 | Loss: 0.00001250
Iteration 92/1000 | Loss: 0.00001250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.2497528587118722e-05, 1.2497528587118722e-05, 1.2497528587118722e-05, 1.2497528587118722e-05, 1.2497528587118722e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2497528587118722e-05

Optimization complete. Final v2v error: 3.0765480995178223 mm

Highest mean error: 3.3095545768737793 mm for frame 118

Lowest mean error: 2.9052703380584717 mm for frame 12

Saving results

Total time: 39.5997154712677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812146
Iteration 2/25 | Loss: 0.00153928
Iteration 3/25 | Loss: 0.00136582
Iteration 4/25 | Loss: 0.00133785
Iteration 5/25 | Loss: 0.00132378
Iteration 6/25 | Loss: 0.00132011
Iteration 7/25 | Loss: 0.00132129
Iteration 8/25 | Loss: 0.00131487
Iteration 9/25 | Loss: 0.00131307
Iteration 10/25 | Loss: 0.00131258
Iteration 11/25 | Loss: 0.00131253
Iteration 12/25 | Loss: 0.00131253
Iteration 13/25 | Loss: 0.00131251
Iteration 14/25 | Loss: 0.00131250
Iteration 15/25 | Loss: 0.00131249
Iteration 16/25 | Loss: 0.00131249
Iteration 17/25 | Loss: 0.00131249
Iteration 18/25 | Loss: 0.00131249
Iteration 19/25 | Loss: 0.00131249
Iteration 20/25 | Loss: 0.00131249
Iteration 21/25 | Loss: 0.00131248
Iteration 22/25 | Loss: 0.00131247
Iteration 23/25 | Loss: 0.00131247
Iteration 24/25 | Loss: 0.00131247
Iteration 25/25 | Loss: 0.00131247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87111616
Iteration 2/25 | Loss: 0.00161931
Iteration 3/25 | Loss: 0.00161931
Iteration 4/25 | Loss: 0.00161930
Iteration 5/25 | Loss: 0.00161930
Iteration 6/25 | Loss: 0.00161930
Iteration 7/25 | Loss: 0.00161930
Iteration 8/25 | Loss: 0.00161930
Iteration 9/25 | Loss: 0.00161930
Iteration 10/25 | Loss: 0.00161930
Iteration 11/25 | Loss: 0.00161930
Iteration 12/25 | Loss: 0.00161930
Iteration 13/25 | Loss: 0.00161930
Iteration 14/25 | Loss: 0.00161930
Iteration 15/25 | Loss: 0.00161930
Iteration 16/25 | Loss: 0.00161930
Iteration 17/25 | Loss: 0.00161930
Iteration 18/25 | Loss: 0.00161930
Iteration 19/25 | Loss: 0.00161930
Iteration 20/25 | Loss: 0.00161930
Iteration 21/25 | Loss: 0.00161930
Iteration 22/25 | Loss: 0.00161930
Iteration 23/25 | Loss: 0.00161930
Iteration 24/25 | Loss: 0.00161930
Iteration 25/25 | Loss: 0.00161930

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161930
Iteration 2/1000 | Loss: 0.00003433
Iteration 3/1000 | Loss: 0.00002271
Iteration 4/1000 | Loss: 0.00001957
Iteration 5/1000 | Loss: 0.00001829
Iteration 6/1000 | Loss: 0.00001742
Iteration 7/1000 | Loss: 0.00001691
Iteration 8/1000 | Loss: 0.00001654
Iteration 9/1000 | Loss: 0.00001630
Iteration 10/1000 | Loss: 0.00001616
Iteration 11/1000 | Loss: 0.00001594
Iteration 12/1000 | Loss: 0.00001579
Iteration 13/1000 | Loss: 0.00001562
Iteration 14/1000 | Loss: 0.00001553
Iteration 15/1000 | Loss: 0.00001550
Iteration 16/1000 | Loss: 0.00001541
Iteration 17/1000 | Loss: 0.00001538
Iteration 18/1000 | Loss: 0.00001531
Iteration 19/1000 | Loss: 0.00001526
Iteration 20/1000 | Loss: 0.00001523
Iteration 21/1000 | Loss: 0.00001520
Iteration 22/1000 | Loss: 0.00001520
Iteration 23/1000 | Loss: 0.00001519
Iteration 24/1000 | Loss: 0.00001519
Iteration 25/1000 | Loss: 0.00001518
Iteration 26/1000 | Loss: 0.00001517
Iteration 27/1000 | Loss: 0.00001516
Iteration 28/1000 | Loss: 0.00001515
Iteration 29/1000 | Loss: 0.00001515
Iteration 30/1000 | Loss: 0.00001515
Iteration 31/1000 | Loss: 0.00001514
Iteration 32/1000 | Loss: 0.00001514
Iteration 33/1000 | Loss: 0.00001513
Iteration 34/1000 | Loss: 0.00001513
Iteration 35/1000 | Loss: 0.00001513
Iteration 36/1000 | Loss: 0.00001513
Iteration 37/1000 | Loss: 0.00001512
Iteration 38/1000 | Loss: 0.00001512
Iteration 39/1000 | Loss: 0.00001512
Iteration 40/1000 | Loss: 0.00001512
Iteration 41/1000 | Loss: 0.00001511
Iteration 42/1000 | Loss: 0.00001511
Iteration 43/1000 | Loss: 0.00001511
Iteration 44/1000 | Loss: 0.00001511
Iteration 45/1000 | Loss: 0.00001511
Iteration 46/1000 | Loss: 0.00001510
Iteration 47/1000 | Loss: 0.00001510
Iteration 48/1000 | Loss: 0.00001510
Iteration 49/1000 | Loss: 0.00001509
Iteration 50/1000 | Loss: 0.00001509
Iteration 51/1000 | Loss: 0.00001509
Iteration 52/1000 | Loss: 0.00001509
Iteration 53/1000 | Loss: 0.00001508
Iteration 54/1000 | Loss: 0.00001508
Iteration 55/1000 | Loss: 0.00001508
Iteration 56/1000 | Loss: 0.00001508
Iteration 57/1000 | Loss: 0.00001507
Iteration 58/1000 | Loss: 0.00001507
Iteration 59/1000 | Loss: 0.00001507
Iteration 60/1000 | Loss: 0.00001507
Iteration 61/1000 | Loss: 0.00001507
Iteration 62/1000 | Loss: 0.00001507
Iteration 63/1000 | Loss: 0.00001507
Iteration 64/1000 | Loss: 0.00001506
Iteration 65/1000 | Loss: 0.00001506
Iteration 66/1000 | Loss: 0.00001506
Iteration 67/1000 | Loss: 0.00001506
Iteration 68/1000 | Loss: 0.00001505
Iteration 69/1000 | Loss: 0.00001505
Iteration 70/1000 | Loss: 0.00001505
Iteration 71/1000 | Loss: 0.00001505
Iteration 72/1000 | Loss: 0.00001505
Iteration 73/1000 | Loss: 0.00001504
Iteration 74/1000 | Loss: 0.00001504
Iteration 75/1000 | Loss: 0.00001503
Iteration 76/1000 | Loss: 0.00001503
Iteration 77/1000 | Loss: 0.00001503
Iteration 78/1000 | Loss: 0.00001502
Iteration 79/1000 | Loss: 0.00001502
Iteration 80/1000 | Loss: 0.00001502
Iteration 81/1000 | Loss: 0.00001502
Iteration 82/1000 | Loss: 0.00001501
Iteration 83/1000 | Loss: 0.00001501
Iteration 84/1000 | Loss: 0.00001501
Iteration 85/1000 | Loss: 0.00001501
Iteration 86/1000 | Loss: 0.00001500
Iteration 87/1000 | Loss: 0.00001500
Iteration 88/1000 | Loss: 0.00001500
Iteration 89/1000 | Loss: 0.00001500
Iteration 90/1000 | Loss: 0.00001499
Iteration 91/1000 | Loss: 0.00001499
Iteration 92/1000 | Loss: 0.00001499
Iteration 93/1000 | Loss: 0.00001498
Iteration 94/1000 | Loss: 0.00001498
Iteration 95/1000 | Loss: 0.00001498
Iteration 96/1000 | Loss: 0.00001498
Iteration 97/1000 | Loss: 0.00001498
Iteration 98/1000 | Loss: 0.00001498
Iteration 99/1000 | Loss: 0.00001497
Iteration 100/1000 | Loss: 0.00001497
Iteration 101/1000 | Loss: 0.00001497
Iteration 102/1000 | Loss: 0.00001497
Iteration 103/1000 | Loss: 0.00001497
Iteration 104/1000 | Loss: 0.00001497
Iteration 105/1000 | Loss: 0.00001496
Iteration 106/1000 | Loss: 0.00001496
Iteration 107/1000 | Loss: 0.00001496
Iteration 108/1000 | Loss: 0.00001496
Iteration 109/1000 | Loss: 0.00001496
Iteration 110/1000 | Loss: 0.00001496
Iteration 111/1000 | Loss: 0.00001496
Iteration 112/1000 | Loss: 0.00001496
Iteration 113/1000 | Loss: 0.00001496
Iteration 114/1000 | Loss: 0.00001496
Iteration 115/1000 | Loss: 0.00001496
Iteration 116/1000 | Loss: 0.00001496
Iteration 117/1000 | Loss: 0.00001496
Iteration 118/1000 | Loss: 0.00001496
Iteration 119/1000 | Loss: 0.00001496
Iteration 120/1000 | Loss: 0.00001496
Iteration 121/1000 | Loss: 0.00001496
Iteration 122/1000 | Loss: 0.00001496
Iteration 123/1000 | Loss: 0.00001496
Iteration 124/1000 | Loss: 0.00001496
Iteration 125/1000 | Loss: 0.00001496
Iteration 126/1000 | Loss: 0.00001496
Iteration 127/1000 | Loss: 0.00001496
Iteration 128/1000 | Loss: 0.00001496
Iteration 129/1000 | Loss: 0.00001496
Iteration 130/1000 | Loss: 0.00001496
Iteration 131/1000 | Loss: 0.00001496
Iteration 132/1000 | Loss: 0.00001496
Iteration 133/1000 | Loss: 0.00001496
Iteration 134/1000 | Loss: 0.00001496
Iteration 135/1000 | Loss: 0.00001496
Iteration 136/1000 | Loss: 0.00001496
Iteration 137/1000 | Loss: 0.00001496
Iteration 138/1000 | Loss: 0.00001496
Iteration 139/1000 | Loss: 0.00001496
Iteration 140/1000 | Loss: 0.00001496
Iteration 141/1000 | Loss: 0.00001496
Iteration 142/1000 | Loss: 0.00001496
Iteration 143/1000 | Loss: 0.00001496
Iteration 144/1000 | Loss: 0.00001496
Iteration 145/1000 | Loss: 0.00001496
Iteration 146/1000 | Loss: 0.00001496
Iteration 147/1000 | Loss: 0.00001496
Iteration 148/1000 | Loss: 0.00001496
Iteration 149/1000 | Loss: 0.00001496
Iteration 150/1000 | Loss: 0.00001496
Iteration 151/1000 | Loss: 0.00001496
Iteration 152/1000 | Loss: 0.00001496
Iteration 153/1000 | Loss: 0.00001496
Iteration 154/1000 | Loss: 0.00001496
Iteration 155/1000 | Loss: 0.00001496
Iteration 156/1000 | Loss: 0.00001496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.4959094187361188e-05, 1.4959094187361188e-05, 1.4959094187361188e-05, 1.4959094187361188e-05, 1.4959094187361188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4959094187361188e-05

Optimization complete. Final v2v error: 3.3088784217834473 mm

Highest mean error: 3.925543785095215 mm for frame 81

Lowest mean error: 2.7953603267669678 mm for frame 125

Saving results

Total time: 48.49974060058594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908130
Iteration 2/25 | Loss: 0.00150384
Iteration 3/25 | Loss: 0.00132241
Iteration 4/25 | Loss: 0.00130351
Iteration 5/25 | Loss: 0.00129854
Iteration 6/25 | Loss: 0.00129792
Iteration 7/25 | Loss: 0.00129792
Iteration 8/25 | Loss: 0.00129792
Iteration 9/25 | Loss: 0.00129792
Iteration 10/25 | Loss: 0.00129792
Iteration 11/25 | Loss: 0.00129792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012979196617379785, 0.0012979196617379785, 0.0012979196617379785, 0.0012979196617379785, 0.0012979196617379785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012979196617379785

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.41506457
Iteration 2/25 | Loss: 0.00147121
Iteration 3/25 | Loss: 0.00147121
Iteration 4/25 | Loss: 0.00147121
Iteration 5/25 | Loss: 0.00147121
Iteration 6/25 | Loss: 0.00147121
Iteration 7/25 | Loss: 0.00147121
Iteration 8/25 | Loss: 0.00147121
Iteration 9/25 | Loss: 0.00147121
Iteration 10/25 | Loss: 0.00147121
Iteration 11/25 | Loss: 0.00147121
Iteration 12/25 | Loss: 0.00147121
Iteration 13/25 | Loss: 0.00147121
Iteration 14/25 | Loss: 0.00147121
Iteration 15/25 | Loss: 0.00147121
Iteration 16/25 | Loss: 0.00147121
Iteration 17/25 | Loss: 0.00147121
Iteration 18/25 | Loss: 0.00147121
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014712064294144511, 0.0014712064294144511, 0.0014712064294144511, 0.0014712064294144511, 0.0014712064294144511]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014712064294144511

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147121
Iteration 2/1000 | Loss: 0.00003060
Iteration 3/1000 | Loss: 0.00002295
Iteration 4/1000 | Loss: 0.00001910
Iteration 5/1000 | Loss: 0.00001787
Iteration 6/1000 | Loss: 0.00001673
Iteration 7/1000 | Loss: 0.00001604
Iteration 8/1000 | Loss: 0.00001563
Iteration 9/1000 | Loss: 0.00001527
Iteration 10/1000 | Loss: 0.00001493
Iteration 11/1000 | Loss: 0.00001472
Iteration 12/1000 | Loss: 0.00001467
Iteration 13/1000 | Loss: 0.00001451
Iteration 14/1000 | Loss: 0.00001450
Iteration 15/1000 | Loss: 0.00001447
Iteration 16/1000 | Loss: 0.00001443
Iteration 17/1000 | Loss: 0.00001442
Iteration 18/1000 | Loss: 0.00001441
Iteration 19/1000 | Loss: 0.00001435
Iteration 20/1000 | Loss: 0.00001432
Iteration 21/1000 | Loss: 0.00001427
Iteration 22/1000 | Loss: 0.00001420
Iteration 23/1000 | Loss: 0.00001413
Iteration 24/1000 | Loss: 0.00001406
Iteration 25/1000 | Loss: 0.00001406
Iteration 26/1000 | Loss: 0.00001405
Iteration 27/1000 | Loss: 0.00001404
Iteration 28/1000 | Loss: 0.00001403
Iteration 29/1000 | Loss: 0.00001403
Iteration 30/1000 | Loss: 0.00001402
Iteration 31/1000 | Loss: 0.00001402
Iteration 32/1000 | Loss: 0.00001402
Iteration 33/1000 | Loss: 0.00001401
Iteration 34/1000 | Loss: 0.00001401
Iteration 35/1000 | Loss: 0.00001401
Iteration 36/1000 | Loss: 0.00001400
Iteration 37/1000 | Loss: 0.00001399
Iteration 38/1000 | Loss: 0.00001399
Iteration 39/1000 | Loss: 0.00001399
Iteration 40/1000 | Loss: 0.00001399
Iteration 41/1000 | Loss: 0.00001399
Iteration 42/1000 | Loss: 0.00001398
Iteration 43/1000 | Loss: 0.00001398
Iteration 44/1000 | Loss: 0.00001398
Iteration 45/1000 | Loss: 0.00001398
Iteration 46/1000 | Loss: 0.00001397
Iteration 47/1000 | Loss: 0.00001397
Iteration 48/1000 | Loss: 0.00001397
Iteration 49/1000 | Loss: 0.00001397
Iteration 50/1000 | Loss: 0.00001397
Iteration 51/1000 | Loss: 0.00001397
Iteration 52/1000 | Loss: 0.00001397
Iteration 53/1000 | Loss: 0.00001397
Iteration 54/1000 | Loss: 0.00001397
Iteration 55/1000 | Loss: 0.00001396
Iteration 56/1000 | Loss: 0.00001396
Iteration 57/1000 | Loss: 0.00001395
Iteration 58/1000 | Loss: 0.00001395
Iteration 59/1000 | Loss: 0.00001395
Iteration 60/1000 | Loss: 0.00001395
Iteration 61/1000 | Loss: 0.00001394
Iteration 62/1000 | Loss: 0.00001394
Iteration 63/1000 | Loss: 0.00001394
Iteration 64/1000 | Loss: 0.00001393
Iteration 65/1000 | Loss: 0.00001393
Iteration 66/1000 | Loss: 0.00001393
Iteration 67/1000 | Loss: 0.00001393
Iteration 68/1000 | Loss: 0.00001392
Iteration 69/1000 | Loss: 0.00001392
Iteration 70/1000 | Loss: 0.00001392
Iteration 71/1000 | Loss: 0.00001392
Iteration 72/1000 | Loss: 0.00001392
Iteration 73/1000 | Loss: 0.00001391
Iteration 74/1000 | Loss: 0.00001391
Iteration 75/1000 | Loss: 0.00001390
Iteration 76/1000 | Loss: 0.00001390
Iteration 77/1000 | Loss: 0.00001390
Iteration 78/1000 | Loss: 0.00001390
Iteration 79/1000 | Loss: 0.00001390
Iteration 80/1000 | Loss: 0.00001390
Iteration 81/1000 | Loss: 0.00001390
Iteration 82/1000 | Loss: 0.00001389
Iteration 83/1000 | Loss: 0.00001389
Iteration 84/1000 | Loss: 0.00001389
Iteration 85/1000 | Loss: 0.00001388
Iteration 86/1000 | Loss: 0.00001387
Iteration 87/1000 | Loss: 0.00001387
Iteration 88/1000 | Loss: 0.00001387
Iteration 89/1000 | Loss: 0.00001386
Iteration 90/1000 | Loss: 0.00001386
Iteration 91/1000 | Loss: 0.00001386
Iteration 92/1000 | Loss: 0.00001384
Iteration 93/1000 | Loss: 0.00001384
Iteration 94/1000 | Loss: 0.00001384
Iteration 95/1000 | Loss: 0.00001383
Iteration 96/1000 | Loss: 0.00001383
Iteration 97/1000 | Loss: 0.00001382
Iteration 98/1000 | Loss: 0.00001382
Iteration 99/1000 | Loss: 0.00001382
Iteration 100/1000 | Loss: 0.00001381
Iteration 101/1000 | Loss: 0.00001381
Iteration 102/1000 | Loss: 0.00001381
Iteration 103/1000 | Loss: 0.00001380
Iteration 104/1000 | Loss: 0.00001380
Iteration 105/1000 | Loss: 0.00001379
Iteration 106/1000 | Loss: 0.00001379
Iteration 107/1000 | Loss: 0.00001379
Iteration 108/1000 | Loss: 0.00001378
Iteration 109/1000 | Loss: 0.00001378
Iteration 110/1000 | Loss: 0.00001378
Iteration 111/1000 | Loss: 0.00001378
Iteration 112/1000 | Loss: 0.00001377
Iteration 113/1000 | Loss: 0.00001377
Iteration 114/1000 | Loss: 0.00001377
Iteration 115/1000 | Loss: 0.00001377
Iteration 116/1000 | Loss: 0.00001377
Iteration 117/1000 | Loss: 0.00001377
Iteration 118/1000 | Loss: 0.00001376
Iteration 119/1000 | Loss: 0.00001376
Iteration 120/1000 | Loss: 0.00001376
Iteration 121/1000 | Loss: 0.00001375
Iteration 122/1000 | Loss: 0.00001375
Iteration 123/1000 | Loss: 0.00001375
Iteration 124/1000 | Loss: 0.00001375
Iteration 125/1000 | Loss: 0.00001374
Iteration 126/1000 | Loss: 0.00001374
Iteration 127/1000 | Loss: 0.00001374
Iteration 128/1000 | Loss: 0.00001374
Iteration 129/1000 | Loss: 0.00001373
Iteration 130/1000 | Loss: 0.00001373
Iteration 131/1000 | Loss: 0.00001373
Iteration 132/1000 | Loss: 0.00001373
Iteration 133/1000 | Loss: 0.00001373
Iteration 134/1000 | Loss: 0.00001373
Iteration 135/1000 | Loss: 0.00001373
Iteration 136/1000 | Loss: 0.00001373
Iteration 137/1000 | Loss: 0.00001373
Iteration 138/1000 | Loss: 0.00001373
Iteration 139/1000 | Loss: 0.00001373
Iteration 140/1000 | Loss: 0.00001373
Iteration 141/1000 | Loss: 0.00001373
Iteration 142/1000 | Loss: 0.00001373
Iteration 143/1000 | Loss: 0.00001373
Iteration 144/1000 | Loss: 0.00001373
Iteration 145/1000 | Loss: 0.00001373
Iteration 146/1000 | Loss: 0.00001373
Iteration 147/1000 | Loss: 0.00001373
Iteration 148/1000 | Loss: 0.00001373
Iteration 149/1000 | Loss: 0.00001373
Iteration 150/1000 | Loss: 0.00001372
Iteration 151/1000 | Loss: 0.00001372
Iteration 152/1000 | Loss: 0.00001372
Iteration 153/1000 | Loss: 0.00001372
Iteration 154/1000 | Loss: 0.00001372
Iteration 155/1000 | Loss: 0.00001372
Iteration 156/1000 | Loss: 0.00001372
Iteration 157/1000 | Loss: 0.00001372
Iteration 158/1000 | Loss: 0.00001372
Iteration 159/1000 | Loss: 0.00001372
Iteration 160/1000 | Loss: 0.00001372
Iteration 161/1000 | Loss: 0.00001372
Iteration 162/1000 | Loss: 0.00001372
Iteration 163/1000 | Loss: 0.00001372
Iteration 164/1000 | Loss: 0.00001372
Iteration 165/1000 | Loss: 0.00001372
Iteration 166/1000 | Loss: 0.00001372
Iteration 167/1000 | Loss: 0.00001372
Iteration 168/1000 | Loss: 0.00001372
Iteration 169/1000 | Loss: 0.00001372
Iteration 170/1000 | Loss: 0.00001372
Iteration 171/1000 | Loss: 0.00001372
Iteration 172/1000 | Loss: 0.00001372
Iteration 173/1000 | Loss: 0.00001372
Iteration 174/1000 | Loss: 0.00001372
Iteration 175/1000 | Loss: 0.00001372
Iteration 176/1000 | Loss: 0.00001372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.37241786433151e-05, 1.37241786433151e-05, 1.37241786433151e-05, 1.37241786433151e-05, 1.37241786433151e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.37241786433151e-05

Optimization complete. Final v2v error: 3.1442575454711914 mm

Highest mean error: 3.695793390274048 mm for frame 5

Lowest mean error: 2.6719391345977783 mm for frame 175

Saving results

Total time: 47.77174782752991
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00693925
Iteration 2/25 | Loss: 0.00161740
Iteration 3/25 | Loss: 0.00137223
Iteration 4/25 | Loss: 0.00134973
Iteration 5/25 | Loss: 0.00133648
Iteration 6/25 | Loss: 0.00134607
Iteration 7/25 | Loss: 0.00134413
Iteration 8/25 | Loss: 0.00132853
Iteration 9/25 | Loss: 0.00132359
Iteration 10/25 | Loss: 0.00131601
Iteration 11/25 | Loss: 0.00131101
Iteration 12/25 | Loss: 0.00130880
Iteration 13/25 | Loss: 0.00130918
Iteration 14/25 | Loss: 0.00130748
Iteration 15/25 | Loss: 0.00130691
Iteration 16/25 | Loss: 0.00130809
Iteration 17/25 | Loss: 0.00130800
Iteration 18/25 | Loss: 0.00130626
Iteration 19/25 | Loss: 0.00130545
Iteration 20/25 | Loss: 0.00130520
Iteration 21/25 | Loss: 0.00130637
Iteration 22/25 | Loss: 0.00130509
Iteration 23/25 | Loss: 0.00130509
Iteration 24/25 | Loss: 0.00130508
Iteration 25/25 | Loss: 0.00130508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.96855164
Iteration 2/25 | Loss: 0.00145268
Iteration 3/25 | Loss: 0.00145266
Iteration 4/25 | Loss: 0.00145266
Iteration 5/25 | Loss: 0.00145266
Iteration 6/25 | Loss: 0.00145266
Iteration 7/25 | Loss: 0.00145266
Iteration 8/25 | Loss: 0.00145266
Iteration 9/25 | Loss: 0.00145266
Iteration 10/25 | Loss: 0.00145266
Iteration 11/25 | Loss: 0.00145266
Iteration 12/25 | Loss: 0.00145266
Iteration 13/25 | Loss: 0.00145266
Iteration 14/25 | Loss: 0.00145266
Iteration 15/25 | Loss: 0.00145266
Iteration 16/25 | Loss: 0.00145266
Iteration 17/25 | Loss: 0.00145266
Iteration 18/25 | Loss: 0.00145266
Iteration 19/25 | Loss: 0.00145266
Iteration 20/25 | Loss: 0.00145266
Iteration 21/25 | Loss: 0.00145266
Iteration 22/25 | Loss: 0.00145266
Iteration 23/25 | Loss: 0.00145266
Iteration 24/25 | Loss: 0.00145266
Iteration 25/25 | Loss: 0.00145266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145266
Iteration 2/1000 | Loss: 0.00002554
Iteration 3/1000 | Loss: 0.00002014
Iteration 4/1000 | Loss: 0.00002730
Iteration 5/1000 | Loss: 0.00001676
Iteration 6/1000 | Loss: 0.00001595
Iteration 7/1000 | Loss: 0.00001547
Iteration 8/1000 | Loss: 0.00001515
Iteration 9/1000 | Loss: 0.00001485
Iteration 10/1000 | Loss: 0.00001464
Iteration 11/1000 | Loss: 0.00001449
Iteration 12/1000 | Loss: 0.00001448
Iteration 13/1000 | Loss: 0.00001447
Iteration 14/1000 | Loss: 0.00001441
Iteration 15/1000 | Loss: 0.00001438
Iteration 16/1000 | Loss: 0.00001437
Iteration 17/1000 | Loss: 0.00001433
Iteration 18/1000 | Loss: 0.00001432
Iteration 19/1000 | Loss: 0.00001431
Iteration 20/1000 | Loss: 0.00001430
Iteration 21/1000 | Loss: 0.00001424
Iteration 22/1000 | Loss: 0.00001424
Iteration 23/1000 | Loss: 0.00001423
Iteration 24/1000 | Loss: 0.00001421
Iteration 25/1000 | Loss: 0.00001420
Iteration 26/1000 | Loss: 0.00001419
Iteration 27/1000 | Loss: 0.00001419
Iteration 28/1000 | Loss: 0.00001418
Iteration 29/1000 | Loss: 0.00001418
Iteration 30/1000 | Loss: 0.00001417
Iteration 31/1000 | Loss: 0.00001416
Iteration 32/1000 | Loss: 0.00001415
Iteration 33/1000 | Loss: 0.00001414
Iteration 34/1000 | Loss: 0.00001413
Iteration 35/1000 | Loss: 0.00001413
Iteration 36/1000 | Loss: 0.00001411
Iteration 37/1000 | Loss: 0.00001410
Iteration 38/1000 | Loss: 0.00001410
Iteration 39/1000 | Loss: 0.00001410
Iteration 40/1000 | Loss: 0.00001410
Iteration 41/1000 | Loss: 0.00001410
Iteration 42/1000 | Loss: 0.00001409
Iteration 43/1000 | Loss: 0.00001409
Iteration 44/1000 | Loss: 0.00001409
Iteration 45/1000 | Loss: 0.00001408
Iteration 46/1000 | Loss: 0.00001406
Iteration 47/1000 | Loss: 0.00001405
Iteration 48/1000 | Loss: 0.00001405
Iteration 49/1000 | Loss: 0.00001403
Iteration 50/1000 | Loss: 0.00001403
Iteration 51/1000 | Loss: 0.00001401
Iteration 52/1000 | Loss: 0.00001400
Iteration 53/1000 | Loss: 0.00001399
Iteration 54/1000 | Loss: 0.00001399
Iteration 55/1000 | Loss: 0.00001398
Iteration 56/1000 | Loss: 0.00001398
Iteration 57/1000 | Loss: 0.00001398
Iteration 58/1000 | Loss: 0.00001397
Iteration 59/1000 | Loss: 0.00001397
Iteration 60/1000 | Loss: 0.00001396
Iteration 61/1000 | Loss: 0.00001396
Iteration 62/1000 | Loss: 0.00001395
Iteration 63/1000 | Loss: 0.00001395
Iteration 64/1000 | Loss: 0.00001394
Iteration 65/1000 | Loss: 0.00001393
Iteration 66/1000 | Loss: 0.00001393
Iteration 67/1000 | Loss: 0.00001392
Iteration 68/1000 | Loss: 0.00001391
Iteration 69/1000 | Loss: 0.00001391
Iteration 70/1000 | Loss: 0.00001391
Iteration 71/1000 | Loss: 0.00001391
Iteration 72/1000 | Loss: 0.00001391
Iteration 73/1000 | Loss: 0.00001391
Iteration 74/1000 | Loss: 0.00001391
Iteration 75/1000 | Loss: 0.00001390
Iteration 76/1000 | Loss: 0.00001390
Iteration 77/1000 | Loss: 0.00001390
Iteration 78/1000 | Loss: 0.00001390
Iteration 79/1000 | Loss: 0.00001390
Iteration 80/1000 | Loss: 0.00001390
Iteration 81/1000 | Loss: 0.00001389
Iteration 82/1000 | Loss: 0.00001389
Iteration 83/1000 | Loss: 0.00001387
Iteration 84/1000 | Loss: 0.00001387
Iteration 85/1000 | Loss: 0.00001387
Iteration 86/1000 | Loss: 0.00001387
Iteration 87/1000 | Loss: 0.00001387
Iteration 88/1000 | Loss: 0.00001387
Iteration 89/1000 | Loss: 0.00001387
Iteration 90/1000 | Loss: 0.00001387
Iteration 91/1000 | Loss: 0.00001387
Iteration 92/1000 | Loss: 0.00001387
Iteration 93/1000 | Loss: 0.00001387
Iteration 94/1000 | Loss: 0.00001386
Iteration 95/1000 | Loss: 0.00001386
Iteration 96/1000 | Loss: 0.00001386
Iteration 97/1000 | Loss: 0.00001386
Iteration 98/1000 | Loss: 0.00001386
Iteration 99/1000 | Loss: 0.00001386
Iteration 100/1000 | Loss: 0.00001386
Iteration 101/1000 | Loss: 0.00001385
Iteration 102/1000 | Loss: 0.00001385
Iteration 103/1000 | Loss: 0.00001385
Iteration 104/1000 | Loss: 0.00001384
Iteration 105/1000 | Loss: 0.00001384
Iteration 106/1000 | Loss: 0.00001383
Iteration 107/1000 | Loss: 0.00001383
Iteration 108/1000 | Loss: 0.00001383
Iteration 109/1000 | Loss: 0.00001383
Iteration 110/1000 | Loss: 0.00001383
Iteration 111/1000 | Loss: 0.00001383
Iteration 112/1000 | Loss: 0.00001382
Iteration 113/1000 | Loss: 0.00001382
Iteration 114/1000 | Loss: 0.00001382
Iteration 115/1000 | Loss: 0.00001381
Iteration 116/1000 | Loss: 0.00001381
Iteration 117/1000 | Loss: 0.00001381
Iteration 118/1000 | Loss: 0.00001381
Iteration 119/1000 | Loss: 0.00001380
Iteration 120/1000 | Loss: 0.00001380
Iteration 121/1000 | Loss: 0.00001380
Iteration 122/1000 | Loss: 0.00001379
Iteration 123/1000 | Loss: 0.00001379
Iteration 124/1000 | Loss: 0.00001378
Iteration 125/1000 | Loss: 0.00001378
Iteration 126/1000 | Loss: 0.00001378
Iteration 127/1000 | Loss: 0.00001378
Iteration 128/1000 | Loss: 0.00001377
Iteration 129/1000 | Loss: 0.00001377
Iteration 130/1000 | Loss: 0.00001377
Iteration 131/1000 | Loss: 0.00001376
Iteration 132/1000 | Loss: 0.00001376
Iteration 133/1000 | Loss: 0.00001376
Iteration 134/1000 | Loss: 0.00001376
Iteration 135/1000 | Loss: 0.00001376
Iteration 136/1000 | Loss: 0.00001375
Iteration 137/1000 | Loss: 0.00001374
Iteration 138/1000 | Loss: 0.00001374
Iteration 139/1000 | Loss: 0.00001374
Iteration 140/1000 | Loss: 0.00001374
Iteration 141/1000 | Loss: 0.00001373
Iteration 142/1000 | Loss: 0.00001373
Iteration 143/1000 | Loss: 0.00001373
Iteration 144/1000 | Loss: 0.00001373
Iteration 145/1000 | Loss: 0.00001372
Iteration 146/1000 | Loss: 0.00001372
Iteration 147/1000 | Loss: 0.00001372
Iteration 148/1000 | Loss: 0.00001371
Iteration 149/1000 | Loss: 0.00001371
Iteration 150/1000 | Loss: 0.00001370
Iteration 151/1000 | Loss: 0.00001370
Iteration 152/1000 | Loss: 0.00001370
Iteration 153/1000 | Loss: 0.00001370
Iteration 154/1000 | Loss: 0.00001370
Iteration 155/1000 | Loss: 0.00001369
Iteration 156/1000 | Loss: 0.00001369
Iteration 157/1000 | Loss: 0.00001368
Iteration 158/1000 | Loss: 0.00001368
Iteration 159/1000 | Loss: 0.00001368
Iteration 160/1000 | Loss: 0.00001368
Iteration 161/1000 | Loss: 0.00001368
Iteration 162/1000 | Loss: 0.00001368
Iteration 163/1000 | Loss: 0.00001367
Iteration 164/1000 | Loss: 0.00001367
Iteration 165/1000 | Loss: 0.00001367
Iteration 166/1000 | Loss: 0.00001367
Iteration 167/1000 | Loss: 0.00001366
Iteration 168/1000 | Loss: 0.00001366
Iteration 169/1000 | Loss: 0.00001366
Iteration 170/1000 | Loss: 0.00001366
Iteration 171/1000 | Loss: 0.00001366
Iteration 172/1000 | Loss: 0.00001366
Iteration 173/1000 | Loss: 0.00001366
Iteration 174/1000 | Loss: 0.00001366
Iteration 175/1000 | Loss: 0.00001365
Iteration 176/1000 | Loss: 0.00001365
Iteration 177/1000 | Loss: 0.00001365
Iteration 178/1000 | Loss: 0.00001365
Iteration 179/1000 | Loss: 0.00001365
Iteration 180/1000 | Loss: 0.00001365
Iteration 181/1000 | Loss: 0.00001365
Iteration 182/1000 | Loss: 0.00001365
Iteration 183/1000 | Loss: 0.00001364
Iteration 184/1000 | Loss: 0.00001364
Iteration 185/1000 | Loss: 0.00001364
Iteration 186/1000 | Loss: 0.00001364
Iteration 187/1000 | Loss: 0.00001364
Iteration 188/1000 | Loss: 0.00001364
Iteration 189/1000 | Loss: 0.00001364
Iteration 190/1000 | Loss: 0.00001364
Iteration 191/1000 | Loss: 0.00001364
Iteration 192/1000 | Loss: 0.00001364
Iteration 193/1000 | Loss: 0.00001364
Iteration 194/1000 | Loss: 0.00001364
Iteration 195/1000 | Loss: 0.00001364
Iteration 196/1000 | Loss: 0.00001364
Iteration 197/1000 | Loss: 0.00001363
Iteration 198/1000 | Loss: 0.00001363
Iteration 199/1000 | Loss: 0.00001363
Iteration 200/1000 | Loss: 0.00001363
Iteration 201/1000 | Loss: 0.00001363
Iteration 202/1000 | Loss: 0.00001363
Iteration 203/1000 | Loss: 0.00001363
Iteration 204/1000 | Loss: 0.00001363
Iteration 205/1000 | Loss: 0.00001363
Iteration 206/1000 | Loss: 0.00001363
Iteration 207/1000 | Loss: 0.00001363
Iteration 208/1000 | Loss: 0.00001363
Iteration 209/1000 | Loss: 0.00001363
Iteration 210/1000 | Loss: 0.00001363
Iteration 211/1000 | Loss: 0.00001363
Iteration 212/1000 | Loss: 0.00001363
Iteration 213/1000 | Loss: 0.00001363
Iteration 214/1000 | Loss: 0.00001363
Iteration 215/1000 | Loss: 0.00001363
Iteration 216/1000 | Loss: 0.00001363
Iteration 217/1000 | Loss: 0.00001363
Iteration 218/1000 | Loss: 0.00001363
Iteration 219/1000 | Loss: 0.00001363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.363427236356074e-05, 1.363427236356074e-05, 1.363427236356074e-05, 1.363427236356074e-05, 1.363427236356074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.363427236356074e-05

Optimization complete. Final v2v error: 3.1220974922180176 mm

Highest mean error: 3.6612367630004883 mm for frame 122

Lowest mean error: 2.622203826904297 mm for frame 219

Saving results

Total time: 83.52475833892822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989481
Iteration 2/25 | Loss: 0.00467961
Iteration 3/25 | Loss: 0.00265855
Iteration 4/25 | Loss: 0.00239075
Iteration 5/25 | Loss: 0.00219157
Iteration 6/25 | Loss: 0.00193905
Iteration 7/25 | Loss: 0.00197327
Iteration 8/25 | Loss: 0.00178101
Iteration 9/25 | Loss: 0.00164741
Iteration 10/25 | Loss: 0.00153357
Iteration 11/25 | Loss: 0.00146785
Iteration 12/25 | Loss: 0.00145199
Iteration 13/25 | Loss: 0.00144550
Iteration 14/25 | Loss: 0.00144593
Iteration 15/25 | Loss: 0.00144447
Iteration 16/25 | Loss: 0.00144254
Iteration 17/25 | Loss: 0.00143988
Iteration 18/25 | Loss: 0.00143846
Iteration 19/25 | Loss: 0.00143663
Iteration 20/25 | Loss: 0.00143546
Iteration 21/25 | Loss: 0.00143869
Iteration 22/25 | Loss: 0.00143271
Iteration 23/25 | Loss: 0.00143153
Iteration 24/25 | Loss: 0.00143389
Iteration 25/25 | Loss: 0.00143107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27726197
Iteration 2/25 | Loss: 0.00136483
Iteration 3/25 | Loss: 0.00136482
Iteration 4/25 | Loss: 0.00136482
Iteration 5/25 | Loss: 0.00136482
Iteration 6/25 | Loss: 0.00136482
Iteration 7/25 | Loss: 0.00136482
Iteration 8/25 | Loss: 0.00136482
Iteration 9/25 | Loss: 0.00136482
Iteration 10/25 | Loss: 0.00136482
Iteration 11/25 | Loss: 0.00136482
Iteration 12/25 | Loss: 0.00136482
Iteration 13/25 | Loss: 0.00136482
Iteration 14/25 | Loss: 0.00136482
Iteration 15/25 | Loss: 0.00136482
Iteration 16/25 | Loss: 0.00136482
Iteration 17/25 | Loss: 0.00136482
Iteration 18/25 | Loss: 0.00136482
Iteration 19/25 | Loss: 0.00136482
Iteration 20/25 | Loss: 0.00136482
Iteration 21/25 | Loss: 0.00136482
Iteration 22/25 | Loss: 0.00136482
Iteration 23/25 | Loss: 0.00136482
Iteration 24/25 | Loss: 0.00136482
Iteration 25/25 | Loss: 0.00136482
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013648172607645392, 0.0013648172607645392, 0.0013648172607645392, 0.0013648172607645392, 0.0013648172607645392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013648172607645392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136482
Iteration 2/1000 | Loss: 0.00006082
Iteration 3/1000 | Loss: 0.00004913
Iteration 4/1000 | Loss: 0.00004569
Iteration 5/1000 | Loss: 0.00004353
Iteration 6/1000 | Loss: 0.00004215
Iteration 7/1000 | Loss: 0.00004132
Iteration 8/1000 | Loss: 0.00004025
Iteration 9/1000 | Loss: 0.00003931
Iteration 10/1000 | Loss: 0.00003871
Iteration 11/1000 | Loss: 0.00003823
Iteration 12/1000 | Loss: 0.00009323
Iteration 13/1000 | Loss: 0.00042661
Iteration 14/1000 | Loss: 0.00013410
Iteration 15/1000 | Loss: 0.00007411
Iteration 16/1000 | Loss: 0.00006716
Iteration 17/1000 | Loss: 0.00006720
Iteration 18/1000 | Loss: 0.00004222
Iteration 19/1000 | Loss: 0.00003746
Iteration 20/1000 | Loss: 0.00003312
Iteration 21/1000 | Loss: 0.00003096
Iteration 22/1000 | Loss: 0.00002923
Iteration 23/1000 | Loss: 0.00002829
Iteration 24/1000 | Loss: 0.00002760
Iteration 25/1000 | Loss: 0.00002703
Iteration 26/1000 | Loss: 0.00002669
Iteration 27/1000 | Loss: 0.00002654
Iteration 28/1000 | Loss: 0.00002632
Iteration 29/1000 | Loss: 0.00002625
Iteration 30/1000 | Loss: 0.00002614
Iteration 31/1000 | Loss: 0.00002612
Iteration 32/1000 | Loss: 0.00002604
Iteration 33/1000 | Loss: 0.00002599
Iteration 34/1000 | Loss: 0.00002598
Iteration 35/1000 | Loss: 0.00002598
Iteration 36/1000 | Loss: 0.00002597
Iteration 37/1000 | Loss: 0.00002596
Iteration 38/1000 | Loss: 0.00002595
Iteration 39/1000 | Loss: 0.00002594
Iteration 40/1000 | Loss: 0.00002588
Iteration 41/1000 | Loss: 0.00002587
Iteration 42/1000 | Loss: 0.00002586
Iteration 43/1000 | Loss: 0.00002581
Iteration 44/1000 | Loss: 0.00002581
Iteration 45/1000 | Loss: 0.00002580
Iteration 46/1000 | Loss: 0.00002580
Iteration 47/1000 | Loss: 0.00002580
Iteration 48/1000 | Loss: 0.00002579
Iteration 49/1000 | Loss: 0.00002579
Iteration 50/1000 | Loss: 0.00002579
Iteration 51/1000 | Loss: 0.00002579
Iteration 52/1000 | Loss: 0.00002579
Iteration 53/1000 | Loss: 0.00002579
Iteration 54/1000 | Loss: 0.00002578
Iteration 55/1000 | Loss: 0.00002578
Iteration 56/1000 | Loss: 0.00002578
Iteration 57/1000 | Loss: 0.00002578
Iteration 58/1000 | Loss: 0.00002578
Iteration 59/1000 | Loss: 0.00002578
Iteration 60/1000 | Loss: 0.00002578
Iteration 61/1000 | Loss: 0.00002578
Iteration 62/1000 | Loss: 0.00002578
Iteration 63/1000 | Loss: 0.00002578
Iteration 64/1000 | Loss: 0.00002578
Iteration 65/1000 | Loss: 0.00002578
Iteration 66/1000 | Loss: 0.00002578
Iteration 67/1000 | Loss: 0.00002578
Iteration 68/1000 | Loss: 0.00002578
Iteration 69/1000 | Loss: 0.00002578
Iteration 70/1000 | Loss: 0.00002578
Iteration 71/1000 | Loss: 0.00002578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [2.5782928787521087e-05, 2.5782928787521087e-05, 2.5782928787521087e-05, 2.5782928787521087e-05, 2.5782928787521087e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5782928787521087e-05

Optimization complete. Final v2v error: 4.343725204467773 mm

Highest mean error: 4.699859619140625 mm for frame 104

Lowest mean error: 4.219803810119629 mm for frame 100

Saving results

Total time: 101.94261431694031
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847908
Iteration 2/25 | Loss: 0.00135805
Iteration 3/25 | Loss: 0.00128106
Iteration 4/25 | Loss: 0.00127250
Iteration 5/25 | Loss: 0.00126962
Iteration 6/25 | Loss: 0.00126897
Iteration 7/25 | Loss: 0.00126897
Iteration 8/25 | Loss: 0.00126897
Iteration 9/25 | Loss: 0.00126897
Iteration 10/25 | Loss: 0.00126897
Iteration 11/25 | Loss: 0.00126897
Iteration 12/25 | Loss: 0.00126897
Iteration 13/25 | Loss: 0.00126897
Iteration 14/25 | Loss: 0.00126897
Iteration 15/25 | Loss: 0.00126897
Iteration 16/25 | Loss: 0.00126897
Iteration 17/25 | Loss: 0.00126897
Iteration 18/25 | Loss: 0.00126897
Iteration 19/25 | Loss: 0.00126897
Iteration 20/25 | Loss: 0.00126897
Iteration 21/25 | Loss: 0.00126897
Iteration 22/25 | Loss: 0.00126897
Iteration 23/25 | Loss: 0.00126897
Iteration 24/25 | Loss: 0.00126897
Iteration 25/25 | Loss: 0.00126897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72539616
Iteration 2/25 | Loss: 0.00151762
Iteration 3/25 | Loss: 0.00151761
Iteration 4/25 | Loss: 0.00151761
Iteration 5/25 | Loss: 0.00151761
Iteration 6/25 | Loss: 0.00151761
Iteration 7/25 | Loss: 0.00151761
Iteration 8/25 | Loss: 0.00151761
Iteration 9/25 | Loss: 0.00151761
Iteration 10/25 | Loss: 0.00151761
Iteration 11/25 | Loss: 0.00151761
Iteration 12/25 | Loss: 0.00151761
Iteration 13/25 | Loss: 0.00151761
Iteration 14/25 | Loss: 0.00151761
Iteration 15/25 | Loss: 0.00151761
Iteration 16/25 | Loss: 0.00151761
Iteration 17/25 | Loss: 0.00151761
Iteration 18/25 | Loss: 0.00151761
Iteration 19/25 | Loss: 0.00151761
Iteration 20/25 | Loss: 0.00151761
Iteration 21/25 | Loss: 0.00151761
Iteration 22/25 | Loss: 0.00151761
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0015176122542470694, 0.0015176122542470694, 0.0015176122542470694, 0.0015176122542470694, 0.0015176122542470694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015176122542470694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151761
Iteration 2/1000 | Loss: 0.00002077
Iteration 3/1000 | Loss: 0.00001563
Iteration 4/1000 | Loss: 0.00001405
Iteration 5/1000 | Loss: 0.00001334
Iteration 6/1000 | Loss: 0.00001277
Iteration 7/1000 | Loss: 0.00001218
Iteration 8/1000 | Loss: 0.00001188
Iteration 9/1000 | Loss: 0.00001164
Iteration 10/1000 | Loss: 0.00001141
Iteration 11/1000 | Loss: 0.00001124
Iteration 12/1000 | Loss: 0.00001113
Iteration 13/1000 | Loss: 0.00001106
Iteration 14/1000 | Loss: 0.00001102
Iteration 15/1000 | Loss: 0.00001102
Iteration 16/1000 | Loss: 0.00001100
Iteration 17/1000 | Loss: 0.00001099
Iteration 18/1000 | Loss: 0.00001088
Iteration 19/1000 | Loss: 0.00001084
Iteration 20/1000 | Loss: 0.00001083
Iteration 21/1000 | Loss: 0.00001080
Iteration 22/1000 | Loss: 0.00001080
Iteration 23/1000 | Loss: 0.00001077
Iteration 24/1000 | Loss: 0.00001074
Iteration 25/1000 | Loss: 0.00001074
Iteration 26/1000 | Loss: 0.00001073
Iteration 27/1000 | Loss: 0.00001072
Iteration 28/1000 | Loss: 0.00001072
Iteration 29/1000 | Loss: 0.00001071
Iteration 30/1000 | Loss: 0.00001070
Iteration 31/1000 | Loss: 0.00001070
Iteration 32/1000 | Loss: 0.00001069
Iteration 33/1000 | Loss: 0.00001066
Iteration 34/1000 | Loss: 0.00001065
Iteration 35/1000 | Loss: 0.00001064
Iteration 36/1000 | Loss: 0.00001064
Iteration 37/1000 | Loss: 0.00001063
Iteration 38/1000 | Loss: 0.00001063
Iteration 39/1000 | Loss: 0.00001063
Iteration 40/1000 | Loss: 0.00001062
Iteration 41/1000 | Loss: 0.00001062
Iteration 42/1000 | Loss: 0.00001062
Iteration 43/1000 | Loss: 0.00001061
Iteration 44/1000 | Loss: 0.00001061
Iteration 45/1000 | Loss: 0.00001061
Iteration 46/1000 | Loss: 0.00001061
Iteration 47/1000 | Loss: 0.00001060
Iteration 48/1000 | Loss: 0.00001060
Iteration 49/1000 | Loss: 0.00001060
Iteration 50/1000 | Loss: 0.00001060
Iteration 51/1000 | Loss: 0.00001059
Iteration 52/1000 | Loss: 0.00001059
Iteration 53/1000 | Loss: 0.00001058
Iteration 54/1000 | Loss: 0.00001058
Iteration 55/1000 | Loss: 0.00001058
Iteration 56/1000 | Loss: 0.00001057
Iteration 57/1000 | Loss: 0.00001057
Iteration 58/1000 | Loss: 0.00001057
Iteration 59/1000 | Loss: 0.00001056
Iteration 60/1000 | Loss: 0.00001056
Iteration 61/1000 | Loss: 0.00001056
Iteration 62/1000 | Loss: 0.00001056
Iteration 63/1000 | Loss: 0.00001055
Iteration 64/1000 | Loss: 0.00001054
Iteration 65/1000 | Loss: 0.00001054
Iteration 66/1000 | Loss: 0.00001053
Iteration 67/1000 | Loss: 0.00001053
Iteration 68/1000 | Loss: 0.00001052
Iteration 69/1000 | Loss: 0.00001052
Iteration 70/1000 | Loss: 0.00001052
Iteration 71/1000 | Loss: 0.00001052
Iteration 72/1000 | Loss: 0.00001052
Iteration 73/1000 | Loss: 0.00001051
Iteration 74/1000 | Loss: 0.00001051
Iteration 75/1000 | Loss: 0.00001051
Iteration 76/1000 | Loss: 0.00001050
Iteration 77/1000 | Loss: 0.00001050
Iteration 78/1000 | Loss: 0.00001050
Iteration 79/1000 | Loss: 0.00001049
Iteration 80/1000 | Loss: 0.00001049
Iteration 81/1000 | Loss: 0.00001049
Iteration 82/1000 | Loss: 0.00001049
Iteration 83/1000 | Loss: 0.00001049
Iteration 84/1000 | Loss: 0.00001049
Iteration 85/1000 | Loss: 0.00001048
Iteration 86/1000 | Loss: 0.00001048
Iteration 87/1000 | Loss: 0.00001048
Iteration 88/1000 | Loss: 0.00001048
Iteration 89/1000 | Loss: 0.00001048
Iteration 90/1000 | Loss: 0.00001047
Iteration 91/1000 | Loss: 0.00001047
Iteration 92/1000 | Loss: 0.00001047
Iteration 93/1000 | Loss: 0.00001047
Iteration 94/1000 | Loss: 0.00001047
Iteration 95/1000 | Loss: 0.00001047
Iteration 96/1000 | Loss: 0.00001047
Iteration 97/1000 | Loss: 0.00001047
Iteration 98/1000 | Loss: 0.00001047
Iteration 99/1000 | Loss: 0.00001047
Iteration 100/1000 | Loss: 0.00001047
Iteration 101/1000 | Loss: 0.00001047
Iteration 102/1000 | Loss: 0.00001047
Iteration 103/1000 | Loss: 0.00001046
Iteration 104/1000 | Loss: 0.00001046
Iteration 105/1000 | Loss: 0.00001046
Iteration 106/1000 | Loss: 0.00001046
Iteration 107/1000 | Loss: 0.00001046
Iteration 108/1000 | Loss: 0.00001046
Iteration 109/1000 | Loss: 0.00001046
Iteration 110/1000 | Loss: 0.00001046
Iteration 111/1000 | Loss: 0.00001046
Iteration 112/1000 | Loss: 0.00001046
Iteration 113/1000 | Loss: 0.00001046
Iteration 114/1000 | Loss: 0.00001046
Iteration 115/1000 | Loss: 0.00001046
Iteration 116/1000 | Loss: 0.00001046
Iteration 117/1000 | Loss: 0.00001045
Iteration 118/1000 | Loss: 0.00001045
Iteration 119/1000 | Loss: 0.00001045
Iteration 120/1000 | Loss: 0.00001045
Iteration 121/1000 | Loss: 0.00001045
Iteration 122/1000 | Loss: 0.00001045
Iteration 123/1000 | Loss: 0.00001045
Iteration 124/1000 | Loss: 0.00001045
Iteration 125/1000 | Loss: 0.00001045
Iteration 126/1000 | Loss: 0.00001044
Iteration 127/1000 | Loss: 0.00001044
Iteration 128/1000 | Loss: 0.00001044
Iteration 129/1000 | Loss: 0.00001044
Iteration 130/1000 | Loss: 0.00001044
Iteration 131/1000 | Loss: 0.00001044
Iteration 132/1000 | Loss: 0.00001044
Iteration 133/1000 | Loss: 0.00001044
Iteration 134/1000 | Loss: 0.00001044
Iteration 135/1000 | Loss: 0.00001044
Iteration 136/1000 | Loss: 0.00001044
Iteration 137/1000 | Loss: 0.00001044
Iteration 138/1000 | Loss: 0.00001044
Iteration 139/1000 | Loss: 0.00001043
Iteration 140/1000 | Loss: 0.00001043
Iteration 141/1000 | Loss: 0.00001043
Iteration 142/1000 | Loss: 0.00001043
Iteration 143/1000 | Loss: 0.00001043
Iteration 144/1000 | Loss: 0.00001043
Iteration 145/1000 | Loss: 0.00001043
Iteration 146/1000 | Loss: 0.00001043
Iteration 147/1000 | Loss: 0.00001043
Iteration 148/1000 | Loss: 0.00001043
Iteration 149/1000 | Loss: 0.00001043
Iteration 150/1000 | Loss: 0.00001042
Iteration 151/1000 | Loss: 0.00001042
Iteration 152/1000 | Loss: 0.00001042
Iteration 153/1000 | Loss: 0.00001041
Iteration 154/1000 | Loss: 0.00001041
Iteration 155/1000 | Loss: 0.00001041
Iteration 156/1000 | Loss: 0.00001041
Iteration 157/1000 | Loss: 0.00001041
Iteration 158/1000 | Loss: 0.00001041
Iteration 159/1000 | Loss: 0.00001040
Iteration 160/1000 | Loss: 0.00001040
Iteration 161/1000 | Loss: 0.00001040
Iteration 162/1000 | Loss: 0.00001040
Iteration 163/1000 | Loss: 0.00001040
Iteration 164/1000 | Loss: 0.00001040
Iteration 165/1000 | Loss: 0.00001039
Iteration 166/1000 | Loss: 0.00001039
Iteration 167/1000 | Loss: 0.00001039
Iteration 168/1000 | Loss: 0.00001039
Iteration 169/1000 | Loss: 0.00001039
Iteration 170/1000 | Loss: 0.00001039
Iteration 171/1000 | Loss: 0.00001039
Iteration 172/1000 | Loss: 0.00001039
Iteration 173/1000 | Loss: 0.00001039
Iteration 174/1000 | Loss: 0.00001039
Iteration 175/1000 | Loss: 0.00001039
Iteration 176/1000 | Loss: 0.00001039
Iteration 177/1000 | Loss: 0.00001039
Iteration 178/1000 | Loss: 0.00001039
Iteration 179/1000 | Loss: 0.00001039
Iteration 180/1000 | Loss: 0.00001039
Iteration 181/1000 | Loss: 0.00001039
Iteration 182/1000 | Loss: 0.00001039
Iteration 183/1000 | Loss: 0.00001039
Iteration 184/1000 | Loss: 0.00001039
Iteration 185/1000 | Loss: 0.00001039
Iteration 186/1000 | Loss: 0.00001039
Iteration 187/1000 | Loss: 0.00001039
Iteration 188/1000 | Loss: 0.00001039
Iteration 189/1000 | Loss: 0.00001039
Iteration 190/1000 | Loss: 0.00001039
Iteration 191/1000 | Loss: 0.00001039
Iteration 192/1000 | Loss: 0.00001039
Iteration 193/1000 | Loss: 0.00001039
Iteration 194/1000 | Loss: 0.00001039
Iteration 195/1000 | Loss: 0.00001039
Iteration 196/1000 | Loss: 0.00001039
Iteration 197/1000 | Loss: 0.00001039
Iteration 198/1000 | Loss: 0.00001039
Iteration 199/1000 | Loss: 0.00001039
Iteration 200/1000 | Loss: 0.00001039
Iteration 201/1000 | Loss: 0.00001039
Iteration 202/1000 | Loss: 0.00001039
Iteration 203/1000 | Loss: 0.00001039
Iteration 204/1000 | Loss: 0.00001039
Iteration 205/1000 | Loss: 0.00001039
Iteration 206/1000 | Loss: 0.00001039
Iteration 207/1000 | Loss: 0.00001039
Iteration 208/1000 | Loss: 0.00001039
Iteration 209/1000 | Loss: 0.00001039
Iteration 210/1000 | Loss: 0.00001039
Iteration 211/1000 | Loss: 0.00001039
Iteration 212/1000 | Loss: 0.00001039
Iteration 213/1000 | Loss: 0.00001039
Iteration 214/1000 | Loss: 0.00001039
Iteration 215/1000 | Loss: 0.00001039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.0390293937234674e-05, 1.0390293937234674e-05, 1.0390293937234674e-05, 1.0390293937234674e-05, 1.0390293937234674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0390293937234674e-05

Optimization complete. Final v2v error: 2.7751641273498535 mm

Highest mean error: 3.388310432434082 mm for frame 51

Lowest mean error: 2.5683541297912598 mm for frame 107

Saving results

Total time: 41.70359683036804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00529543
Iteration 2/25 | Loss: 0.00148784
Iteration 3/25 | Loss: 0.00141106
Iteration 4/25 | Loss: 0.00140131
Iteration 5/25 | Loss: 0.00139807
Iteration 6/25 | Loss: 0.00139807
Iteration 7/25 | Loss: 0.00139807
Iteration 8/25 | Loss: 0.00139807
Iteration 9/25 | Loss: 0.00139807
Iteration 10/25 | Loss: 0.00139807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013980650110170245, 0.0013980650110170245, 0.0013980650110170245, 0.0013980650110170245, 0.0013980650110170245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013980650110170245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.72255599
Iteration 2/25 | Loss: 0.00121452
Iteration 3/25 | Loss: 0.00121452
Iteration 4/25 | Loss: 0.00121452
Iteration 5/25 | Loss: 0.00121452
Iteration 6/25 | Loss: 0.00121452
Iteration 7/25 | Loss: 0.00121452
Iteration 8/25 | Loss: 0.00121452
Iteration 9/25 | Loss: 0.00121452
Iteration 10/25 | Loss: 0.00121452
Iteration 11/25 | Loss: 0.00121452
Iteration 12/25 | Loss: 0.00121452
Iteration 13/25 | Loss: 0.00121452
Iteration 14/25 | Loss: 0.00121452
Iteration 15/25 | Loss: 0.00121452
Iteration 16/25 | Loss: 0.00121451
Iteration 17/25 | Loss: 0.00121451
Iteration 18/25 | Loss: 0.00121451
Iteration 19/25 | Loss: 0.00121451
Iteration 20/25 | Loss: 0.00121451
Iteration 21/25 | Loss: 0.00121451
Iteration 22/25 | Loss: 0.00121451
Iteration 23/25 | Loss: 0.00121451
Iteration 24/25 | Loss: 0.00121451
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012145148357376456, 0.0012145148357376456, 0.0012145148357376456, 0.0012145148357376456, 0.0012145148357376456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012145148357376456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121451
Iteration 2/1000 | Loss: 0.00004448
Iteration 3/1000 | Loss: 0.00003892
Iteration 4/1000 | Loss: 0.00003694
Iteration 5/1000 | Loss: 0.00003551
Iteration 6/1000 | Loss: 0.00003505
Iteration 7/1000 | Loss: 0.00003442
Iteration 8/1000 | Loss: 0.00003403
Iteration 9/1000 | Loss: 0.00003359
Iteration 10/1000 | Loss: 0.00003327
Iteration 11/1000 | Loss: 0.00003303
Iteration 12/1000 | Loss: 0.00003271
Iteration 13/1000 | Loss: 0.00003246
Iteration 14/1000 | Loss: 0.00003232
Iteration 15/1000 | Loss: 0.00003216
Iteration 16/1000 | Loss: 0.00003215
Iteration 17/1000 | Loss: 0.00003213
Iteration 18/1000 | Loss: 0.00003212
Iteration 19/1000 | Loss: 0.00003200
Iteration 20/1000 | Loss: 0.00003199
Iteration 21/1000 | Loss: 0.00003199
Iteration 22/1000 | Loss: 0.00003198
Iteration 23/1000 | Loss: 0.00003198
Iteration 24/1000 | Loss: 0.00003198
Iteration 25/1000 | Loss: 0.00003198
Iteration 26/1000 | Loss: 0.00003198
Iteration 27/1000 | Loss: 0.00003197
Iteration 28/1000 | Loss: 0.00003197
Iteration 29/1000 | Loss: 0.00003194
Iteration 30/1000 | Loss: 0.00003194
Iteration 31/1000 | Loss: 0.00003193
Iteration 32/1000 | Loss: 0.00003192
Iteration 33/1000 | Loss: 0.00003192
Iteration 34/1000 | Loss: 0.00003191
Iteration 35/1000 | Loss: 0.00003190
Iteration 36/1000 | Loss: 0.00003190
Iteration 37/1000 | Loss: 0.00003190
Iteration 38/1000 | Loss: 0.00003189
Iteration 39/1000 | Loss: 0.00003189
Iteration 40/1000 | Loss: 0.00003189
Iteration 41/1000 | Loss: 0.00003189
Iteration 42/1000 | Loss: 0.00003189
Iteration 43/1000 | Loss: 0.00003189
Iteration 44/1000 | Loss: 0.00003189
Iteration 45/1000 | Loss: 0.00003189
Iteration 46/1000 | Loss: 0.00003188
Iteration 47/1000 | Loss: 0.00003188
Iteration 48/1000 | Loss: 0.00003185
Iteration 49/1000 | Loss: 0.00003185
Iteration 50/1000 | Loss: 0.00003185
Iteration 51/1000 | Loss: 0.00003185
Iteration 52/1000 | Loss: 0.00003184
Iteration 53/1000 | Loss: 0.00003184
Iteration 54/1000 | Loss: 0.00003184
Iteration 55/1000 | Loss: 0.00003184
Iteration 56/1000 | Loss: 0.00003184
Iteration 57/1000 | Loss: 0.00003184
Iteration 58/1000 | Loss: 0.00003184
Iteration 59/1000 | Loss: 0.00003183
Iteration 60/1000 | Loss: 0.00003183
Iteration 61/1000 | Loss: 0.00003182
Iteration 62/1000 | Loss: 0.00003182
Iteration 63/1000 | Loss: 0.00003181
Iteration 64/1000 | Loss: 0.00003181
Iteration 65/1000 | Loss: 0.00003181
Iteration 66/1000 | Loss: 0.00003181
Iteration 67/1000 | Loss: 0.00003181
Iteration 68/1000 | Loss: 0.00003181
Iteration 69/1000 | Loss: 0.00003180
Iteration 70/1000 | Loss: 0.00003180
Iteration 71/1000 | Loss: 0.00003180
Iteration 72/1000 | Loss: 0.00003180
Iteration 73/1000 | Loss: 0.00003179
Iteration 74/1000 | Loss: 0.00003179
Iteration 75/1000 | Loss: 0.00003179
Iteration 76/1000 | Loss: 0.00003178
Iteration 77/1000 | Loss: 0.00003178
Iteration 78/1000 | Loss: 0.00003178
Iteration 79/1000 | Loss: 0.00003178
Iteration 80/1000 | Loss: 0.00003178
Iteration 81/1000 | Loss: 0.00003178
Iteration 82/1000 | Loss: 0.00003178
Iteration 83/1000 | Loss: 0.00003178
Iteration 84/1000 | Loss: 0.00003178
Iteration 85/1000 | Loss: 0.00003178
Iteration 86/1000 | Loss: 0.00003178
Iteration 87/1000 | Loss: 0.00003178
Iteration 88/1000 | Loss: 0.00003178
Iteration 89/1000 | Loss: 0.00003178
Iteration 90/1000 | Loss: 0.00003178
Iteration 91/1000 | Loss: 0.00003178
Iteration 92/1000 | Loss: 0.00003178
Iteration 93/1000 | Loss: 0.00003178
Iteration 94/1000 | Loss: 0.00003178
Iteration 95/1000 | Loss: 0.00003178
Iteration 96/1000 | Loss: 0.00003178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [3.1779505661688745e-05, 3.1779505661688745e-05, 3.1779505661688745e-05, 3.1779505661688745e-05, 3.1779505661688745e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1779505661688745e-05

Optimization complete. Final v2v error: 4.476597309112549 mm

Highest mean error: 4.513430118560791 mm for frame 0

Lowest mean error: 4.435100555419922 mm for frame 124

Saving results

Total time: 40.35715699195862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764337
Iteration 2/25 | Loss: 0.00135217
Iteration 3/25 | Loss: 0.00126661
Iteration 4/25 | Loss: 0.00126197
Iteration 5/25 | Loss: 0.00126197
Iteration 6/25 | Loss: 0.00126197
Iteration 7/25 | Loss: 0.00126197
Iteration 8/25 | Loss: 0.00126197
Iteration 9/25 | Loss: 0.00126197
Iteration 10/25 | Loss: 0.00126197
Iteration 11/25 | Loss: 0.00126197
Iteration 12/25 | Loss: 0.00126197
Iteration 13/25 | Loss: 0.00126197
Iteration 14/25 | Loss: 0.00126197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012619708431884646, 0.0012619708431884646, 0.0012619708431884646, 0.0012619708431884646, 0.0012619708431884646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012619708431884646

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30138302
Iteration 2/25 | Loss: 0.00136601
Iteration 3/25 | Loss: 0.00136601
Iteration 4/25 | Loss: 0.00136601
Iteration 5/25 | Loss: 0.00136601
Iteration 6/25 | Loss: 0.00136601
Iteration 7/25 | Loss: 0.00136601
Iteration 8/25 | Loss: 0.00136601
Iteration 9/25 | Loss: 0.00136600
Iteration 10/25 | Loss: 0.00136600
Iteration 11/25 | Loss: 0.00136600
Iteration 12/25 | Loss: 0.00136600
Iteration 13/25 | Loss: 0.00136600
Iteration 14/25 | Loss: 0.00136600
Iteration 15/25 | Loss: 0.00136600
Iteration 16/25 | Loss: 0.00136600
Iteration 17/25 | Loss: 0.00136600
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013660043478012085, 0.0013660043478012085, 0.0013660043478012085, 0.0013660043478012085, 0.0013660043478012085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013660043478012085

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136600
Iteration 2/1000 | Loss: 0.00002172
Iteration 3/1000 | Loss: 0.00001650
Iteration 4/1000 | Loss: 0.00001501
Iteration 5/1000 | Loss: 0.00001398
Iteration 6/1000 | Loss: 0.00001331
Iteration 7/1000 | Loss: 0.00001293
Iteration 8/1000 | Loss: 0.00001247
Iteration 9/1000 | Loss: 0.00001208
Iteration 10/1000 | Loss: 0.00001186
Iteration 11/1000 | Loss: 0.00001180
Iteration 12/1000 | Loss: 0.00001177
Iteration 13/1000 | Loss: 0.00001165
Iteration 14/1000 | Loss: 0.00001160
Iteration 15/1000 | Loss: 0.00001157
Iteration 16/1000 | Loss: 0.00001156
Iteration 17/1000 | Loss: 0.00001155
Iteration 18/1000 | Loss: 0.00001154
Iteration 19/1000 | Loss: 0.00001150
Iteration 20/1000 | Loss: 0.00001144
Iteration 21/1000 | Loss: 0.00001143
Iteration 22/1000 | Loss: 0.00001141
Iteration 23/1000 | Loss: 0.00001141
Iteration 24/1000 | Loss: 0.00001130
Iteration 25/1000 | Loss: 0.00001123
Iteration 26/1000 | Loss: 0.00001119
Iteration 27/1000 | Loss: 0.00001117
Iteration 28/1000 | Loss: 0.00001110
Iteration 29/1000 | Loss: 0.00001104
Iteration 30/1000 | Loss: 0.00001104
Iteration 31/1000 | Loss: 0.00001104
Iteration 32/1000 | Loss: 0.00001103
Iteration 33/1000 | Loss: 0.00001102
Iteration 34/1000 | Loss: 0.00001101
Iteration 35/1000 | Loss: 0.00001101
Iteration 36/1000 | Loss: 0.00001100
Iteration 37/1000 | Loss: 0.00001100
Iteration 38/1000 | Loss: 0.00001100
Iteration 39/1000 | Loss: 0.00001100
Iteration 40/1000 | Loss: 0.00001099
Iteration 41/1000 | Loss: 0.00001099
Iteration 42/1000 | Loss: 0.00001097
Iteration 43/1000 | Loss: 0.00001096
Iteration 44/1000 | Loss: 0.00001096
Iteration 45/1000 | Loss: 0.00001095
Iteration 46/1000 | Loss: 0.00001095
Iteration 47/1000 | Loss: 0.00001094
Iteration 48/1000 | Loss: 0.00001093
Iteration 49/1000 | Loss: 0.00001092
Iteration 50/1000 | Loss: 0.00001091
Iteration 51/1000 | Loss: 0.00001091
Iteration 52/1000 | Loss: 0.00001091
Iteration 53/1000 | Loss: 0.00001091
Iteration 54/1000 | Loss: 0.00001089
Iteration 55/1000 | Loss: 0.00001088
Iteration 56/1000 | Loss: 0.00001087
Iteration 57/1000 | Loss: 0.00001086
Iteration 58/1000 | Loss: 0.00001085
Iteration 59/1000 | Loss: 0.00001085
Iteration 60/1000 | Loss: 0.00001085
Iteration 61/1000 | Loss: 0.00001085
Iteration 62/1000 | Loss: 0.00001084
Iteration 63/1000 | Loss: 0.00001083
Iteration 64/1000 | Loss: 0.00001082
Iteration 65/1000 | Loss: 0.00001082
Iteration 66/1000 | Loss: 0.00001082
Iteration 67/1000 | Loss: 0.00001081
Iteration 68/1000 | Loss: 0.00001081
Iteration 69/1000 | Loss: 0.00001081
Iteration 70/1000 | Loss: 0.00001081
Iteration 71/1000 | Loss: 0.00001080
Iteration 72/1000 | Loss: 0.00001080
Iteration 73/1000 | Loss: 0.00001079
Iteration 74/1000 | Loss: 0.00001079
Iteration 75/1000 | Loss: 0.00001078
Iteration 76/1000 | Loss: 0.00001078
Iteration 77/1000 | Loss: 0.00001077
Iteration 78/1000 | Loss: 0.00001077
Iteration 79/1000 | Loss: 0.00001077
Iteration 80/1000 | Loss: 0.00001077
Iteration 81/1000 | Loss: 0.00001077
Iteration 82/1000 | Loss: 0.00001077
Iteration 83/1000 | Loss: 0.00001077
Iteration 84/1000 | Loss: 0.00001077
Iteration 85/1000 | Loss: 0.00001076
Iteration 86/1000 | Loss: 0.00001076
Iteration 87/1000 | Loss: 0.00001076
Iteration 88/1000 | Loss: 0.00001076
Iteration 89/1000 | Loss: 0.00001076
Iteration 90/1000 | Loss: 0.00001076
Iteration 91/1000 | Loss: 0.00001075
Iteration 92/1000 | Loss: 0.00001075
Iteration 93/1000 | Loss: 0.00001075
Iteration 94/1000 | Loss: 0.00001074
Iteration 95/1000 | Loss: 0.00001074
Iteration 96/1000 | Loss: 0.00001074
Iteration 97/1000 | Loss: 0.00001074
Iteration 98/1000 | Loss: 0.00001073
Iteration 99/1000 | Loss: 0.00001073
Iteration 100/1000 | Loss: 0.00001072
Iteration 101/1000 | Loss: 0.00001072
Iteration 102/1000 | Loss: 0.00001072
Iteration 103/1000 | Loss: 0.00001072
Iteration 104/1000 | Loss: 0.00001072
Iteration 105/1000 | Loss: 0.00001072
Iteration 106/1000 | Loss: 0.00001072
Iteration 107/1000 | Loss: 0.00001072
Iteration 108/1000 | Loss: 0.00001071
Iteration 109/1000 | Loss: 0.00001071
Iteration 110/1000 | Loss: 0.00001071
Iteration 111/1000 | Loss: 0.00001071
Iteration 112/1000 | Loss: 0.00001071
Iteration 113/1000 | Loss: 0.00001071
Iteration 114/1000 | Loss: 0.00001070
Iteration 115/1000 | Loss: 0.00001070
Iteration 116/1000 | Loss: 0.00001070
Iteration 117/1000 | Loss: 0.00001070
Iteration 118/1000 | Loss: 0.00001070
Iteration 119/1000 | Loss: 0.00001069
Iteration 120/1000 | Loss: 0.00001069
Iteration 121/1000 | Loss: 0.00001069
Iteration 122/1000 | Loss: 0.00001069
Iteration 123/1000 | Loss: 0.00001068
Iteration 124/1000 | Loss: 0.00001068
Iteration 125/1000 | Loss: 0.00001068
Iteration 126/1000 | Loss: 0.00001067
Iteration 127/1000 | Loss: 0.00001067
Iteration 128/1000 | Loss: 0.00001067
Iteration 129/1000 | Loss: 0.00001067
Iteration 130/1000 | Loss: 0.00001067
Iteration 131/1000 | Loss: 0.00001067
Iteration 132/1000 | Loss: 0.00001067
Iteration 133/1000 | Loss: 0.00001067
Iteration 134/1000 | Loss: 0.00001067
Iteration 135/1000 | Loss: 0.00001067
Iteration 136/1000 | Loss: 0.00001066
Iteration 137/1000 | Loss: 0.00001066
Iteration 138/1000 | Loss: 0.00001066
Iteration 139/1000 | Loss: 0.00001066
Iteration 140/1000 | Loss: 0.00001066
Iteration 141/1000 | Loss: 0.00001066
Iteration 142/1000 | Loss: 0.00001066
Iteration 143/1000 | Loss: 0.00001065
Iteration 144/1000 | Loss: 0.00001065
Iteration 145/1000 | Loss: 0.00001065
Iteration 146/1000 | Loss: 0.00001064
Iteration 147/1000 | Loss: 0.00001064
Iteration 148/1000 | Loss: 0.00001064
Iteration 149/1000 | Loss: 0.00001064
Iteration 150/1000 | Loss: 0.00001064
Iteration 151/1000 | Loss: 0.00001064
Iteration 152/1000 | Loss: 0.00001064
Iteration 153/1000 | Loss: 0.00001063
Iteration 154/1000 | Loss: 0.00001063
Iteration 155/1000 | Loss: 0.00001063
Iteration 156/1000 | Loss: 0.00001062
Iteration 157/1000 | Loss: 0.00001062
Iteration 158/1000 | Loss: 0.00001062
Iteration 159/1000 | Loss: 0.00001062
Iteration 160/1000 | Loss: 0.00001062
Iteration 161/1000 | Loss: 0.00001062
Iteration 162/1000 | Loss: 0.00001062
Iteration 163/1000 | Loss: 0.00001062
Iteration 164/1000 | Loss: 0.00001062
Iteration 165/1000 | Loss: 0.00001061
Iteration 166/1000 | Loss: 0.00001061
Iteration 167/1000 | Loss: 0.00001061
Iteration 168/1000 | Loss: 0.00001061
Iteration 169/1000 | Loss: 0.00001061
Iteration 170/1000 | Loss: 0.00001061
Iteration 171/1000 | Loss: 0.00001061
Iteration 172/1000 | Loss: 0.00001061
Iteration 173/1000 | Loss: 0.00001061
Iteration 174/1000 | Loss: 0.00001061
Iteration 175/1000 | Loss: 0.00001061
Iteration 176/1000 | Loss: 0.00001060
Iteration 177/1000 | Loss: 0.00001060
Iteration 178/1000 | Loss: 0.00001060
Iteration 179/1000 | Loss: 0.00001060
Iteration 180/1000 | Loss: 0.00001060
Iteration 181/1000 | Loss: 0.00001060
Iteration 182/1000 | Loss: 0.00001060
Iteration 183/1000 | Loss: 0.00001060
Iteration 184/1000 | Loss: 0.00001060
Iteration 185/1000 | Loss: 0.00001060
Iteration 186/1000 | Loss: 0.00001060
Iteration 187/1000 | Loss: 0.00001060
Iteration 188/1000 | Loss: 0.00001060
Iteration 189/1000 | Loss: 0.00001060
Iteration 190/1000 | Loss: 0.00001060
Iteration 191/1000 | Loss: 0.00001060
Iteration 192/1000 | Loss: 0.00001060
Iteration 193/1000 | Loss: 0.00001060
Iteration 194/1000 | Loss: 0.00001060
Iteration 195/1000 | Loss: 0.00001060
Iteration 196/1000 | Loss: 0.00001060
Iteration 197/1000 | Loss: 0.00001060
Iteration 198/1000 | Loss: 0.00001060
Iteration 199/1000 | Loss: 0.00001060
Iteration 200/1000 | Loss: 0.00001060
Iteration 201/1000 | Loss: 0.00001060
Iteration 202/1000 | Loss: 0.00001060
Iteration 203/1000 | Loss: 0.00001060
Iteration 204/1000 | Loss: 0.00001060
Iteration 205/1000 | Loss: 0.00001060
Iteration 206/1000 | Loss: 0.00001060
Iteration 207/1000 | Loss: 0.00001060
Iteration 208/1000 | Loss: 0.00001060
Iteration 209/1000 | Loss: 0.00001060
Iteration 210/1000 | Loss: 0.00001060
Iteration 211/1000 | Loss: 0.00001060
Iteration 212/1000 | Loss: 0.00001060
Iteration 213/1000 | Loss: 0.00001060
Iteration 214/1000 | Loss: 0.00001060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.0598240805848036e-05, 1.0598240805848036e-05, 1.0598240805848036e-05, 1.0598240805848036e-05, 1.0598240805848036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0598240805848036e-05

Optimization complete. Final v2v error: 2.784684896469116 mm

Highest mean error: 2.957414150238037 mm for frame 80

Lowest mean error: 2.6141810417175293 mm for frame 251

Saving results

Total time: 49.51784563064575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974812
Iteration 2/25 | Loss: 0.00295765
Iteration 3/25 | Loss: 0.00194381
Iteration 4/25 | Loss: 0.00171227
Iteration 5/25 | Loss: 0.00165938
Iteration 6/25 | Loss: 0.00171236
Iteration 7/25 | Loss: 0.00161209
Iteration 8/25 | Loss: 0.00159659
Iteration 9/25 | Loss: 0.00156196
Iteration 10/25 | Loss: 0.00155403
Iteration 11/25 | Loss: 0.00154099
Iteration 12/25 | Loss: 0.00152422
Iteration 13/25 | Loss: 0.00150758
Iteration 14/25 | Loss: 0.00150797
Iteration 15/25 | Loss: 0.00150961
Iteration 16/25 | Loss: 0.00149536
Iteration 17/25 | Loss: 0.00149470
Iteration 18/25 | Loss: 0.00148400
Iteration 19/25 | Loss: 0.00148262
Iteration 20/25 | Loss: 0.00148247
Iteration 21/25 | Loss: 0.00148241
Iteration 22/25 | Loss: 0.00148240
Iteration 23/25 | Loss: 0.00148240
Iteration 24/25 | Loss: 0.00148240
Iteration 25/25 | Loss: 0.00148240

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26864898
Iteration 2/25 | Loss: 0.00294329
Iteration 3/25 | Loss: 0.00244897
Iteration 4/25 | Loss: 0.00244897
Iteration 5/25 | Loss: 0.00244896
Iteration 6/25 | Loss: 0.00244896
Iteration 7/25 | Loss: 0.00244896
Iteration 8/25 | Loss: 0.00244896
Iteration 9/25 | Loss: 0.00244896
Iteration 10/25 | Loss: 0.00244896
Iteration 11/25 | Loss: 0.00244896
Iteration 12/25 | Loss: 0.00244896
Iteration 13/25 | Loss: 0.00244896
Iteration 14/25 | Loss: 0.00244896
Iteration 15/25 | Loss: 0.00244896
Iteration 16/25 | Loss: 0.00244896
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0024489627685397863, 0.0024489627685397863, 0.0024489627685397863, 0.0024489627685397863, 0.0024489627685397863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024489627685397863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244896
Iteration 2/1000 | Loss: 0.00027411
Iteration 3/1000 | Loss: 0.00040574
Iteration 4/1000 | Loss: 0.00024729
Iteration 5/1000 | Loss: 0.00025510
Iteration 6/1000 | Loss: 0.00024712
Iteration 7/1000 | Loss: 0.00012009
Iteration 8/1000 | Loss: 0.00020956
Iteration 9/1000 | Loss: 0.00013191
Iteration 10/1000 | Loss: 0.00011377
Iteration 11/1000 | Loss: 0.00010857
Iteration 12/1000 | Loss: 0.00029618
Iteration 13/1000 | Loss: 0.00026244
Iteration 14/1000 | Loss: 0.00010620
Iteration 15/1000 | Loss: 0.00009064
Iteration 16/1000 | Loss: 0.00032336
Iteration 17/1000 | Loss: 0.00016251
Iteration 18/1000 | Loss: 0.00010459
Iteration 19/1000 | Loss: 0.00009383
Iteration 20/1000 | Loss: 0.00009397
Iteration 21/1000 | Loss: 0.00008187
Iteration 22/1000 | Loss: 0.00043312
Iteration 23/1000 | Loss: 0.00097844
Iteration 24/1000 | Loss: 0.00191742
Iteration 25/1000 | Loss: 0.00419980
Iteration 26/1000 | Loss: 0.00386499
Iteration 27/1000 | Loss: 0.00114216
Iteration 28/1000 | Loss: 0.00429722
Iteration 29/1000 | Loss: 0.00380282
Iteration 30/1000 | Loss: 0.00094023
Iteration 31/1000 | Loss: 0.00024774
Iteration 32/1000 | Loss: 0.00012781
Iteration 33/1000 | Loss: 0.00038154
Iteration 34/1000 | Loss: 0.00021590
Iteration 35/1000 | Loss: 0.00030643
Iteration 36/1000 | Loss: 0.00006569
Iteration 37/1000 | Loss: 0.00012647
Iteration 38/1000 | Loss: 0.00006276
Iteration 39/1000 | Loss: 0.00004655
Iteration 40/1000 | Loss: 0.00005082
Iteration 41/1000 | Loss: 0.00011799
Iteration 42/1000 | Loss: 0.00004121
Iteration 43/1000 | Loss: 0.00011628
Iteration 44/1000 | Loss: 0.00003924
Iteration 45/1000 | Loss: 0.00003731
Iteration 46/1000 | Loss: 0.00008976
Iteration 47/1000 | Loss: 0.00004650
Iteration 48/1000 | Loss: 0.00002630
Iteration 49/1000 | Loss: 0.00003902
Iteration 50/1000 | Loss: 0.00006457
Iteration 51/1000 | Loss: 0.00002636
Iteration 52/1000 | Loss: 0.00003186
Iteration 53/1000 | Loss: 0.00003179
Iteration 54/1000 | Loss: 0.00003377
Iteration 55/1000 | Loss: 0.00002142
Iteration 56/1000 | Loss: 0.00002714
Iteration 57/1000 | Loss: 0.00002028
Iteration 58/1000 | Loss: 0.00002613
Iteration 59/1000 | Loss: 0.00002119
Iteration 60/1000 | Loss: 0.00003755
Iteration 61/1000 | Loss: 0.00003344
Iteration 62/1000 | Loss: 0.00005525
Iteration 63/1000 | Loss: 0.00002367
Iteration 64/1000 | Loss: 0.00002560
Iteration 65/1000 | Loss: 0.00001983
Iteration 66/1000 | Loss: 0.00001981
Iteration 67/1000 | Loss: 0.00001981
Iteration 68/1000 | Loss: 0.00001982
Iteration 69/1000 | Loss: 0.00002196
Iteration 70/1000 | Loss: 0.00002415
Iteration 71/1000 | Loss: 0.00001978
Iteration 72/1000 | Loss: 0.00001978
Iteration 73/1000 | Loss: 0.00001978
Iteration 74/1000 | Loss: 0.00001976
Iteration 75/1000 | Loss: 0.00001975
Iteration 76/1000 | Loss: 0.00001975
Iteration 77/1000 | Loss: 0.00001975
Iteration 78/1000 | Loss: 0.00001975
Iteration 79/1000 | Loss: 0.00001975
Iteration 80/1000 | Loss: 0.00001975
Iteration 81/1000 | Loss: 0.00001975
Iteration 82/1000 | Loss: 0.00001975
Iteration 83/1000 | Loss: 0.00001975
Iteration 84/1000 | Loss: 0.00001975
Iteration 85/1000 | Loss: 0.00001975
Iteration 86/1000 | Loss: 0.00001975
Iteration 87/1000 | Loss: 0.00001975
Iteration 88/1000 | Loss: 0.00001975
Iteration 89/1000 | Loss: 0.00001975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.9750850697164424e-05, 1.9750850697164424e-05, 1.9750850697164424e-05, 1.9750850697164424e-05, 1.9750850697164424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9750850697164424e-05

Optimization complete. Final v2v error: 3.45133900642395 mm

Highest mean error: 10.305444717407227 mm for frame 184

Lowest mean error: 2.868454694747925 mm for frame 136

Saving results

Total time: 145.2209713459015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829802
Iteration 2/25 | Loss: 0.00140278
Iteration 3/25 | Loss: 0.00133563
Iteration 4/25 | Loss: 0.00131841
Iteration 5/25 | Loss: 0.00131207
Iteration 6/25 | Loss: 0.00131134
Iteration 7/25 | Loss: 0.00131134
Iteration 8/25 | Loss: 0.00131134
Iteration 9/25 | Loss: 0.00131134
Iteration 10/25 | Loss: 0.00131134
Iteration 11/25 | Loss: 0.00131134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013113418826833367, 0.0013113418826833367, 0.0013113418826833367, 0.0013113418826833367, 0.0013113418826833367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013113418826833367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28509629
Iteration 2/25 | Loss: 0.00172794
Iteration 3/25 | Loss: 0.00172794
Iteration 4/25 | Loss: 0.00172794
Iteration 5/25 | Loss: 0.00172794
Iteration 6/25 | Loss: 0.00172794
Iteration 7/25 | Loss: 0.00172794
Iteration 8/25 | Loss: 0.00172794
Iteration 9/25 | Loss: 0.00172794
Iteration 10/25 | Loss: 0.00172794
Iteration 11/25 | Loss: 0.00172794
Iteration 12/25 | Loss: 0.00172794
Iteration 13/25 | Loss: 0.00172794
Iteration 14/25 | Loss: 0.00172794
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001727940747514367, 0.001727940747514367, 0.001727940747514367, 0.001727940747514367, 0.001727940747514367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001727940747514367

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172794
Iteration 2/1000 | Loss: 0.00002692
Iteration 3/1000 | Loss: 0.00002161
Iteration 4/1000 | Loss: 0.00002065
Iteration 5/1000 | Loss: 0.00002022
Iteration 6/1000 | Loss: 0.00001989
Iteration 7/1000 | Loss: 0.00001959
Iteration 8/1000 | Loss: 0.00001922
Iteration 9/1000 | Loss: 0.00001913
Iteration 10/1000 | Loss: 0.00001911
Iteration 11/1000 | Loss: 0.00001889
Iteration 12/1000 | Loss: 0.00001874
Iteration 13/1000 | Loss: 0.00001867
Iteration 14/1000 | Loss: 0.00001867
Iteration 15/1000 | Loss: 0.00001866
Iteration 16/1000 | Loss: 0.00001856
Iteration 17/1000 | Loss: 0.00001855
Iteration 18/1000 | Loss: 0.00001854
Iteration 19/1000 | Loss: 0.00001854
Iteration 20/1000 | Loss: 0.00001849
Iteration 21/1000 | Loss: 0.00001848
Iteration 22/1000 | Loss: 0.00001846
Iteration 23/1000 | Loss: 0.00001843
Iteration 24/1000 | Loss: 0.00001843
Iteration 25/1000 | Loss: 0.00001842
Iteration 26/1000 | Loss: 0.00001838
Iteration 27/1000 | Loss: 0.00001835
Iteration 28/1000 | Loss: 0.00001834
Iteration 29/1000 | Loss: 0.00001833
Iteration 30/1000 | Loss: 0.00001832
Iteration 31/1000 | Loss: 0.00001832
Iteration 32/1000 | Loss: 0.00001831
Iteration 33/1000 | Loss: 0.00001831
Iteration 34/1000 | Loss: 0.00001831
Iteration 35/1000 | Loss: 0.00001830
Iteration 36/1000 | Loss: 0.00001830
Iteration 37/1000 | Loss: 0.00001830
Iteration 38/1000 | Loss: 0.00001829
Iteration 39/1000 | Loss: 0.00001829
Iteration 40/1000 | Loss: 0.00001828
Iteration 41/1000 | Loss: 0.00001828
Iteration 42/1000 | Loss: 0.00001828
Iteration 43/1000 | Loss: 0.00001828
Iteration 44/1000 | Loss: 0.00001828
Iteration 45/1000 | Loss: 0.00001827
Iteration 46/1000 | Loss: 0.00001827
Iteration 47/1000 | Loss: 0.00001827
Iteration 48/1000 | Loss: 0.00001827
Iteration 49/1000 | Loss: 0.00001827
Iteration 50/1000 | Loss: 0.00001827
Iteration 51/1000 | Loss: 0.00001827
Iteration 52/1000 | Loss: 0.00001827
Iteration 53/1000 | Loss: 0.00001827
Iteration 54/1000 | Loss: 0.00001827
Iteration 55/1000 | Loss: 0.00001827
Iteration 56/1000 | Loss: 0.00001827
Iteration 57/1000 | Loss: 0.00001826
Iteration 58/1000 | Loss: 0.00001826
Iteration 59/1000 | Loss: 0.00001826
Iteration 60/1000 | Loss: 0.00001825
Iteration 61/1000 | Loss: 0.00001825
Iteration 62/1000 | Loss: 0.00001824
Iteration 63/1000 | Loss: 0.00001824
Iteration 64/1000 | Loss: 0.00001824
Iteration 65/1000 | Loss: 0.00001823
Iteration 66/1000 | Loss: 0.00001823
Iteration 67/1000 | Loss: 0.00001823
Iteration 68/1000 | Loss: 0.00001822
Iteration 69/1000 | Loss: 0.00001822
Iteration 70/1000 | Loss: 0.00001822
Iteration 71/1000 | Loss: 0.00001821
Iteration 72/1000 | Loss: 0.00001821
Iteration 73/1000 | Loss: 0.00001820
Iteration 74/1000 | Loss: 0.00001820
Iteration 75/1000 | Loss: 0.00001820
Iteration 76/1000 | Loss: 0.00001819
Iteration 77/1000 | Loss: 0.00001819
Iteration 78/1000 | Loss: 0.00001819
Iteration 79/1000 | Loss: 0.00001819
Iteration 80/1000 | Loss: 0.00001819
Iteration 81/1000 | Loss: 0.00001819
Iteration 82/1000 | Loss: 0.00001818
Iteration 83/1000 | Loss: 0.00001818
Iteration 84/1000 | Loss: 0.00001818
Iteration 85/1000 | Loss: 0.00001818
Iteration 86/1000 | Loss: 0.00001817
Iteration 87/1000 | Loss: 0.00001817
Iteration 88/1000 | Loss: 0.00001817
Iteration 89/1000 | Loss: 0.00001817
Iteration 90/1000 | Loss: 0.00001817
Iteration 91/1000 | Loss: 0.00001817
Iteration 92/1000 | Loss: 0.00001817
Iteration 93/1000 | Loss: 0.00001817
Iteration 94/1000 | Loss: 0.00001816
Iteration 95/1000 | Loss: 0.00001816
Iteration 96/1000 | Loss: 0.00001815
Iteration 97/1000 | Loss: 0.00001815
Iteration 98/1000 | Loss: 0.00001815
Iteration 99/1000 | Loss: 0.00001814
Iteration 100/1000 | Loss: 0.00001814
Iteration 101/1000 | Loss: 0.00001814
Iteration 102/1000 | Loss: 0.00001813
Iteration 103/1000 | Loss: 0.00001813
Iteration 104/1000 | Loss: 0.00001813
Iteration 105/1000 | Loss: 0.00001813
Iteration 106/1000 | Loss: 0.00001813
Iteration 107/1000 | Loss: 0.00001813
Iteration 108/1000 | Loss: 0.00001812
Iteration 109/1000 | Loss: 0.00001812
Iteration 110/1000 | Loss: 0.00001812
Iteration 111/1000 | Loss: 0.00001812
Iteration 112/1000 | Loss: 0.00001812
Iteration 113/1000 | Loss: 0.00001812
Iteration 114/1000 | Loss: 0.00001812
Iteration 115/1000 | Loss: 0.00001812
Iteration 116/1000 | Loss: 0.00001812
Iteration 117/1000 | Loss: 0.00001812
Iteration 118/1000 | Loss: 0.00001812
Iteration 119/1000 | Loss: 0.00001812
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.8120910681318492e-05, 1.8120910681318492e-05, 1.8120910681318492e-05, 1.8120910681318492e-05, 1.8120910681318492e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8120910681318492e-05

Optimization complete. Final v2v error: 3.6049177646636963 mm

Highest mean error: 3.658634901046753 mm for frame 156

Lowest mean error: 3.422839879989624 mm for frame 1

Saving results

Total time: 37.19181799888611
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764762
Iteration 2/25 | Loss: 0.00172468
Iteration 3/25 | Loss: 0.00147457
Iteration 4/25 | Loss: 0.00144538
Iteration 5/25 | Loss: 0.00146734
Iteration 6/25 | Loss: 0.00141732
Iteration 7/25 | Loss: 0.00139849
Iteration 8/25 | Loss: 0.00139634
Iteration 9/25 | Loss: 0.00137994
Iteration 10/25 | Loss: 0.00137638
Iteration 11/25 | Loss: 0.00137535
Iteration 12/25 | Loss: 0.00137498
Iteration 13/25 | Loss: 0.00137429
Iteration 14/25 | Loss: 0.00137429
Iteration 15/25 | Loss: 0.00137429
Iteration 16/25 | Loss: 0.00137425
Iteration 17/25 | Loss: 0.00137425
Iteration 18/25 | Loss: 0.00137425
Iteration 19/25 | Loss: 0.00137425
Iteration 20/25 | Loss: 0.00137425
Iteration 21/25 | Loss: 0.00137425
Iteration 22/25 | Loss: 0.00137425
Iteration 23/25 | Loss: 0.00137425
Iteration 24/25 | Loss: 0.00137425
Iteration 25/25 | Loss: 0.00137425

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.47766566
Iteration 2/25 | Loss: 0.00107240
Iteration 3/25 | Loss: 0.00107235
Iteration 4/25 | Loss: 0.00107235
Iteration 5/25 | Loss: 0.00107235
Iteration 6/25 | Loss: 0.00107235
Iteration 7/25 | Loss: 0.00107235
Iteration 8/25 | Loss: 0.00107235
Iteration 9/25 | Loss: 0.00107235
Iteration 10/25 | Loss: 0.00107235
Iteration 11/25 | Loss: 0.00107235
Iteration 12/25 | Loss: 0.00107235
Iteration 13/25 | Loss: 0.00107235
Iteration 14/25 | Loss: 0.00107235
Iteration 15/25 | Loss: 0.00107235
Iteration 16/25 | Loss: 0.00107235
Iteration 17/25 | Loss: 0.00107235
Iteration 18/25 | Loss: 0.00107235
Iteration 19/25 | Loss: 0.00107235
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010723473969846964, 0.0010723473969846964, 0.0010723473969846964, 0.0010723473969846964, 0.0010723473969846964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010723473969846964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107235
Iteration 2/1000 | Loss: 0.00003108
Iteration 3/1000 | Loss: 0.00002343
Iteration 4/1000 | Loss: 0.00002161
Iteration 5/1000 | Loss: 0.00002086
Iteration 6/1000 | Loss: 0.00002027
Iteration 7/1000 | Loss: 0.00001971
Iteration 8/1000 | Loss: 0.00001923
Iteration 9/1000 | Loss: 0.00001893
Iteration 10/1000 | Loss: 0.00001878
Iteration 11/1000 | Loss: 0.00001862
Iteration 12/1000 | Loss: 0.00001858
Iteration 13/1000 | Loss: 0.00001855
Iteration 14/1000 | Loss: 0.00001847
Iteration 15/1000 | Loss: 0.00001844
Iteration 16/1000 | Loss: 0.00001843
Iteration 17/1000 | Loss: 0.00001841
Iteration 18/1000 | Loss: 0.00001840
Iteration 19/1000 | Loss: 0.00001839
Iteration 20/1000 | Loss: 0.00001839
Iteration 21/1000 | Loss: 0.00001838
Iteration 22/1000 | Loss: 0.00001837
Iteration 23/1000 | Loss: 0.00001836
Iteration 24/1000 | Loss: 0.00001835
Iteration 25/1000 | Loss: 0.00001835
Iteration 26/1000 | Loss: 0.00001834
Iteration 27/1000 | Loss: 0.00001834
Iteration 28/1000 | Loss: 0.00001833
Iteration 29/1000 | Loss: 0.00001833
Iteration 30/1000 | Loss: 0.00001833
Iteration 31/1000 | Loss: 0.00001832
Iteration 32/1000 | Loss: 0.00001832
Iteration 33/1000 | Loss: 0.00001831
Iteration 34/1000 | Loss: 0.00001831
Iteration 35/1000 | Loss: 0.00001831
Iteration 36/1000 | Loss: 0.00001831
Iteration 37/1000 | Loss: 0.00001831
Iteration 38/1000 | Loss: 0.00001831
Iteration 39/1000 | Loss: 0.00001831
Iteration 40/1000 | Loss: 0.00001831
Iteration 41/1000 | Loss: 0.00001831
Iteration 42/1000 | Loss: 0.00001830
Iteration 43/1000 | Loss: 0.00001830
Iteration 44/1000 | Loss: 0.00001830
Iteration 45/1000 | Loss: 0.00001829
Iteration 46/1000 | Loss: 0.00001829
Iteration 47/1000 | Loss: 0.00001829
Iteration 48/1000 | Loss: 0.00001828
Iteration 49/1000 | Loss: 0.00001828
Iteration 50/1000 | Loss: 0.00001828
Iteration 51/1000 | Loss: 0.00001828
Iteration 52/1000 | Loss: 0.00001828
Iteration 53/1000 | Loss: 0.00001828
Iteration 54/1000 | Loss: 0.00001827
Iteration 55/1000 | Loss: 0.00001827
Iteration 56/1000 | Loss: 0.00001827
Iteration 57/1000 | Loss: 0.00001826
Iteration 58/1000 | Loss: 0.00001824
Iteration 59/1000 | Loss: 0.00001823
Iteration 60/1000 | Loss: 0.00001822
Iteration 61/1000 | Loss: 0.00001821
Iteration 62/1000 | Loss: 0.00001821
Iteration 63/1000 | Loss: 0.00001821
Iteration 64/1000 | Loss: 0.00001821
Iteration 65/1000 | Loss: 0.00001821
Iteration 66/1000 | Loss: 0.00001820
Iteration 67/1000 | Loss: 0.00001820
Iteration 68/1000 | Loss: 0.00001817
Iteration 69/1000 | Loss: 0.00001817
Iteration 70/1000 | Loss: 0.00001817
Iteration 71/1000 | Loss: 0.00001816
Iteration 72/1000 | Loss: 0.00001816
Iteration 73/1000 | Loss: 0.00001816
Iteration 74/1000 | Loss: 0.00001815
Iteration 75/1000 | Loss: 0.00001815
Iteration 76/1000 | Loss: 0.00001815
Iteration 77/1000 | Loss: 0.00001814
Iteration 78/1000 | Loss: 0.00001814
Iteration 79/1000 | Loss: 0.00001813
Iteration 80/1000 | Loss: 0.00001813
Iteration 81/1000 | Loss: 0.00001812
Iteration 82/1000 | Loss: 0.00001812
Iteration 83/1000 | Loss: 0.00001812
Iteration 84/1000 | Loss: 0.00001811
Iteration 85/1000 | Loss: 0.00001811
Iteration 86/1000 | Loss: 0.00001811
Iteration 87/1000 | Loss: 0.00001811
Iteration 88/1000 | Loss: 0.00001811
Iteration 89/1000 | Loss: 0.00001811
Iteration 90/1000 | Loss: 0.00001810
Iteration 91/1000 | Loss: 0.00001810
Iteration 92/1000 | Loss: 0.00012224
Iteration 93/1000 | Loss: 0.00002668
Iteration 94/1000 | Loss: 0.00002098
Iteration 95/1000 | Loss: 0.00001833
Iteration 96/1000 | Loss: 0.00001806
Iteration 97/1000 | Loss: 0.00001806
Iteration 98/1000 | Loss: 0.00001806
Iteration 99/1000 | Loss: 0.00001805
Iteration 100/1000 | Loss: 0.00001805
Iteration 101/1000 | Loss: 0.00001805
Iteration 102/1000 | Loss: 0.00001805
Iteration 103/1000 | Loss: 0.00001805
Iteration 104/1000 | Loss: 0.00001803
Iteration 105/1000 | Loss: 0.00001803
Iteration 106/1000 | Loss: 0.00001802
Iteration 107/1000 | Loss: 0.00001802
Iteration 108/1000 | Loss: 0.00001801
Iteration 109/1000 | Loss: 0.00001801
Iteration 110/1000 | Loss: 0.00001801
Iteration 111/1000 | Loss: 0.00001801
Iteration 112/1000 | Loss: 0.00001801
Iteration 113/1000 | Loss: 0.00001801
Iteration 114/1000 | Loss: 0.00001801
Iteration 115/1000 | Loss: 0.00001801
Iteration 116/1000 | Loss: 0.00001801
Iteration 117/1000 | Loss: 0.00001801
Iteration 118/1000 | Loss: 0.00001801
Iteration 119/1000 | Loss: 0.00001801
Iteration 120/1000 | Loss: 0.00001801
Iteration 121/1000 | Loss: 0.00001801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.8006863683694974e-05, 1.8006863683694974e-05, 1.8006863683694974e-05, 1.8006863683694974e-05, 1.8006863683694974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8006863683694974e-05

Optimization complete. Final v2v error: 3.5373775959014893 mm

Highest mean error: 3.8246541023254395 mm for frame 233

Lowest mean error: 3.3014886379241943 mm for frame 134

Saving results

Total time: 63.04825782775879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399040
Iteration 2/25 | Loss: 0.00132229
Iteration 3/25 | Loss: 0.00125707
Iteration 4/25 | Loss: 0.00125193
Iteration 5/25 | Loss: 0.00125037
Iteration 6/25 | Loss: 0.00125030
Iteration 7/25 | Loss: 0.00125030
Iteration 8/25 | Loss: 0.00125030
Iteration 9/25 | Loss: 0.00125030
Iteration 10/25 | Loss: 0.00125030
Iteration 11/25 | Loss: 0.00125030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001250301138497889, 0.001250301138497889, 0.001250301138497889, 0.001250301138497889, 0.001250301138497889]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001250301138497889

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31072104
Iteration 2/25 | Loss: 0.00161441
Iteration 3/25 | Loss: 0.00161440
Iteration 4/25 | Loss: 0.00161440
Iteration 5/25 | Loss: 0.00161440
Iteration 6/25 | Loss: 0.00161440
Iteration 7/25 | Loss: 0.00161440
Iteration 8/25 | Loss: 0.00161440
Iteration 9/25 | Loss: 0.00161440
Iteration 10/25 | Loss: 0.00161440
Iteration 11/25 | Loss: 0.00161440
Iteration 12/25 | Loss: 0.00161440
Iteration 13/25 | Loss: 0.00161440
Iteration 14/25 | Loss: 0.00161440
Iteration 15/25 | Loss: 0.00161440
Iteration 16/25 | Loss: 0.00161440
Iteration 17/25 | Loss: 0.00161440
Iteration 18/25 | Loss: 0.00161440
Iteration 19/25 | Loss: 0.00161440
Iteration 20/25 | Loss: 0.00161440
Iteration 21/25 | Loss: 0.00161440
Iteration 22/25 | Loss: 0.00161440
Iteration 23/25 | Loss: 0.00161440
Iteration 24/25 | Loss: 0.00161440
Iteration 25/25 | Loss: 0.00161440

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161440
Iteration 2/1000 | Loss: 0.00002493
Iteration 3/1000 | Loss: 0.00001532
Iteration 4/1000 | Loss: 0.00001362
Iteration 5/1000 | Loss: 0.00001254
Iteration 6/1000 | Loss: 0.00001178
Iteration 7/1000 | Loss: 0.00001126
Iteration 8/1000 | Loss: 0.00001095
Iteration 9/1000 | Loss: 0.00001073
Iteration 10/1000 | Loss: 0.00001059
Iteration 11/1000 | Loss: 0.00001045
Iteration 12/1000 | Loss: 0.00001041
Iteration 13/1000 | Loss: 0.00001040
Iteration 14/1000 | Loss: 0.00001038
Iteration 15/1000 | Loss: 0.00001037
Iteration 16/1000 | Loss: 0.00001036
Iteration 17/1000 | Loss: 0.00001036
Iteration 18/1000 | Loss: 0.00001030
Iteration 19/1000 | Loss: 0.00001027
Iteration 20/1000 | Loss: 0.00001026
Iteration 21/1000 | Loss: 0.00001024
Iteration 22/1000 | Loss: 0.00001019
Iteration 23/1000 | Loss: 0.00001018
Iteration 24/1000 | Loss: 0.00001018
Iteration 25/1000 | Loss: 0.00001012
Iteration 26/1000 | Loss: 0.00001012
Iteration 27/1000 | Loss: 0.00001010
Iteration 28/1000 | Loss: 0.00001010
Iteration 29/1000 | Loss: 0.00001009
Iteration 30/1000 | Loss: 0.00001009
Iteration 31/1000 | Loss: 0.00001008
Iteration 32/1000 | Loss: 0.00001006
Iteration 33/1000 | Loss: 0.00001005
Iteration 34/1000 | Loss: 0.00001004
Iteration 35/1000 | Loss: 0.00001003
Iteration 36/1000 | Loss: 0.00001002
Iteration 37/1000 | Loss: 0.00001002
Iteration 38/1000 | Loss: 0.00001001
Iteration 39/1000 | Loss: 0.00001001
Iteration 40/1000 | Loss: 0.00001000
Iteration 41/1000 | Loss: 0.00001000
Iteration 42/1000 | Loss: 0.00001000
Iteration 43/1000 | Loss: 0.00000999
Iteration 44/1000 | Loss: 0.00000998
Iteration 45/1000 | Loss: 0.00000997
Iteration 46/1000 | Loss: 0.00000997
Iteration 47/1000 | Loss: 0.00000993
Iteration 48/1000 | Loss: 0.00000993
Iteration 49/1000 | Loss: 0.00000990
Iteration 50/1000 | Loss: 0.00000989
Iteration 51/1000 | Loss: 0.00000989
Iteration 52/1000 | Loss: 0.00000988
Iteration 53/1000 | Loss: 0.00000988
Iteration 54/1000 | Loss: 0.00000987
Iteration 55/1000 | Loss: 0.00000985
Iteration 56/1000 | Loss: 0.00000985
Iteration 57/1000 | Loss: 0.00000983
Iteration 58/1000 | Loss: 0.00000983
Iteration 59/1000 | Loss: 0.00000983
Iteration 60/1000 | Loss: 0.00000982
Iteration 61/1000 | Loss: 0.00000982
Iteration 62/1000 | Loss: 0.00000982
Iteration 63/1000 | Loss: 0.00000981
Iteration 64/1000 | Loss: 0.00000981
Iteration 65/1000 | Loss: 0.00000981
Iteration 66/1000 | Loss: 0.00000980
Iteration 67/1000 | Loss: 0.00000980
Iteration 68/1000 | Loss: 0.00000980
Iteration 69/1000 | Loss: 0.00000980
Iteration 70/1000 | Loss: 0.00000979
Iteration 71/1000 | Loss: 0.00000979
Iteration 72/1000 | Loss: 0.00000979
Iteration 73/1000 | Loss: 0.00000978
Iteration 74/1000 | Loss: 0.00000978
Iteration 75/1000 | Loss: 0.00000978
Iteration 76/1000 | Loss: 0.00000978
Iteration 77/1000 | Loss: 0.00000977
Iteration 78/1000 | Loss: 0.00000977
Iteration 79/1000 | Loss: 0.00000977
Iteration 80/1000 | Loss: 0.00000977
Iteration 81/1000 | Loss: 0.00000976
Iteration 82/1000 | Loss: 0.00000976
Iteration 83/1000 | Loss: 0.00000976
Iteration 84/1000 | Loss: 0.00000975
Iteration 85/1000 | Loss: 0.00000975
Iteration 86/1000 | Loss: 0.00000975
Iteration 87/1000 | Loss: 0.00000975
Iteration 88/1000 | Loss: 0.00000974
Iteration 89/1000 | Loss: 0.00000974
Iteration 90/1000 | Loss: 0.00000974
Iteration 91/1000 | Loss: 0.00000973
Iteration 92/1000 | Loss: 0.00000973
Iteration 93/1000 | Loss: 0.00000973
Iteration 94/1000 | Loss: 0.00000973
Iteration 95/1000 | Loss: 0.00000972
Iteration 96/1000 | Loss: 0.00000972
Iteration 97/1000 | Loss: 0.00000972
Iteration 98/1000 | Loss: 0.00000971
Iteration 99/1000 | Loss: 0.00000971
Iteration 100/1000 | Loss: 0.00000971
Iteration 101/1000 | Loss: 0.00000971
Iteration 102/1000 | Loss: 0.00000970
Iteration 103/1000 | Loss: 0.00000970
Iteration 104/1000 | Loss: 0.00000970
Iteration 105/1000 | Loss: 0.00000969
Iteration 106/1000 | Loss: 0.00000969
Iteration 107/1000 | Loss: 0.00000969
Iteration 108/1000 | Loss: 0.00000968
Iteration 109/1000 | Loss: 0.00000968
Iteration 110/1000 | Loss: 0.00000968
Iteration 111/1000 | Loss: 0.00000968
Iteration 112/1000 | Loss: 0.00000968
Iteration 113/1000 | Loss: 0.00000967
Iteration 114/1000 | Loss: 0.00000967
Iteration 115/1000 | Loss: 0.00000967
Iteration 116/1000 | Loss: 0.00000967
Iteration 117/1000 | Loss: 0.00000967
Iteration 118/1000 | Loss: 0.00000967
Iteration 119/1000 | Loss: 0.00000967
Iteration 120/1000 | Loss: 0.00000966
Iteration 121/1000 | Loss: 0.00000966
Iteration 122/1000 | Loss: 0.00000966
Iteration 123/1000 | Loss: 0.00000966
Iteration 124/1000 | Loss: 0.00000966
Iteration 125/1000 | Loss: 0.00000966
Iteration 126/1000 | Loss: 0.00000966
Iteration 127/1000 | Loss: 0.00000966
Iteration 128/1000 | Loss: 0.00000966
Iteration 129/1000 | Loss: 0.00000966
Iteration 130/1000 | Loss: 0.00000966
Iteration 131/1000 | Loss: 0.00000966
Iteration 132/1000 | Loss: 0.00000966
Iteration 133/1000 | Loss: 0.00000966
Iteration 134/1000 | Loss: 0.00000965
Iteration 135/1000 | Loss: 0.00000965
Iteration 136/1000 | Loss: 0.00000965
Iteration 137/1000 | Loss: 0.00000965
Iteration 138/1000 | Loss: 0.00000965
Iteration 139/1000 | Loss: 0.00000964
Iteration 140/1000 | Loss: 0.00000964
Iteration 141/1000 | Loss: 0.00000964
Iteration 142/1000 | Loss: 0.00000964
Iteration 143/1000 | Loss: 0.00000964
Iteration 144/1000 | Loss: 0.00000964
Iteration 145/1000 | Loss: 0.00000964
Iteration 146/1000 | Loss: 0.00000964
Iteration 147/1000 | Loss: 0.00000964
Iteration 148/1000 | Loss: 0.00000964
Iteration 149/1000 | Loss: 0.00000964
Iteration 150/1000 | Loss: 0.00000964
Iteration 151/1000 | Loss: 0.00000964
Iteration 152/1000 | Loss: 0.00000963
Iteration 153/1000 | Loss: 0.00000963
Iteration 154/1000 | Loss: 0.00000963
Iteration 155/1000 | Loss: 0.00000963
Iteration 156/1000 | Loss: 0.00000963
Iteration 157/1000 | Loss: 0.00000963
Iteration 158/1000 | Loss: 0.00000963
Iteration 159/1000 | Loss: 0.00000963
Iteration 160/1000 | Loss: 0.00000963
Iteration 161/1000 | Loss: 0.00000963
Iteration 162/1000 | Loss: 0.00000962
Iteration 163/1000 | Loss: 0.00000962
Iteration 164/1000 | Loss: 0.00000962
Iteration 165/1000 | Loss: 0.00000962
Iteration 166/1000 | Loss: 0.00000962
Iteration 167/1000 | Loss: 0.00000962
Iteration 168/1000 | Loss: 0.00000962
Iteration 169/1000 | Loss: 0.00000962
Iteration 170/1000 | Loss: 0.00000962
Iteration 171/1000 | Loss: 0.00000962
Iteration 172/1000 | Loss: 0.00000961
Iteration 173/1000 | Loss: 0.00000961
Iteration 174/1000 | Loss: 0.00000961
Iteration 175/1000 | Loss: 0.00000961
Iteration 176/1000 | Loss: 0.00000961
Iteration 177/1000 | Loss: 0.00000961
Iteration 178/1000 | Loss: 0.00000961
Iteration 179/1000 | Loss: 0.00000961
Iteration 180/1000 | Loss: 0.00000960
Iteration 181/1000 | Loss: 0.00000960
Iteration 182/1000 | Loss: 0.00000960
Iteration 183/1000 | Loss: 0.00000960
Iteration 184/1000 | Loss: 0.00000960
Iteration 185/1000 | Loss: 0.00000960
Iteration 186/1000 | Loss: 0.00000960
Iteration 187/1000 | Loss: 0.00000959
Iteration 188/1000 | Loss: 0.00000959
Iteration 189/1000 | Loss: 0.00000959
Iteration 190/1000 | Loss: 0.00000959
Iteration 191/1000 | Loss: 0.00000959
Iteration 192/1000 | Loss: 0.00000959
Iteration 193/1000 | Loss: 0.00000959
Iteration 194/1000 | Loss: 0.00000959
Iteration 195/1000 | Loss: 0.00000958
Iteration 196/1000 | Loss: 0.00000958
Iteration 197/1000 | Loss: 0.00000958
Iteration 198/1000 | Loss: 0.00000958
Iteration 199/1000 | Loss: 0.00000958
Iteration 200/1000 | Loss: 0.00000958
Iteration 201/1000 | Loss: 0.00000958
Iteration 202/1000 | Loss: 0.00000958
Iteration 203/1000 | Loss: 0.00000958
Iteration 204/1000 | Loss: 0.00000958
Iteration 205/1000 | Loss: 0.00000957
Iteration 206/1000 | Loss: 0.00000957
Iteration 207/1000 | Loss: 0.00000957
Iteration 208/1000 | Loss: 0.00000957
Iteration 209/1000 | Loss: 0.00000957
Iteration 210/1000 | Loss: 0.00000957
Iteration 211/1000 | Loss: 0.00000957
Iteration 212/1000 | Loss: 0.00000957
Iteration 213/1000 | Loss: 0.00000957
Iteration 214/1000 | Loss: 0.00000957
Iteration 215/1000 | Loss: 0.00000957
Iteration 216/1000 | Loss: 0.00000957
Iteration 217/1000 | Loss: 0.00000957
Iteration 218/1000 | Loss: 0.00000957
Iteration 219/1000 | Loss: 0.00000957
Iteration 220/1000 | Loss: 0.00000957
Iteration 221/1000 | Loss: 0.00000957
Iteration 222/1000 | Loss: 0.00000956
Iteration 223/1000 | Loss: 0.00000956
Iteration 224/1000 | Loss: 0.00000956
Iteration 225/1000 | Loss: 0.00000956
Iteration 226/1000 | Loss: 0.00000956
Iteration 227/1000 | Loss: 0.00000956
Iteration 228/1000 | Loss: 0.00000956
Iteration 229/1000 | Loss: 0.00000956
Iteration 230/1000 | Loss: 0.00000956
Iteration 231/1000 | Loss: 0.00000956
Iteration 232/1000 | Loss: 0.00000955
Iteration 233/1000 | Loss: 0.00000955
Iteration 234/1000 | Loss: 0.00000955
Iteration 235/1000 | Loss: 0.00000955
Iteration 236/1000 | Loss: 0.00000955
Iteration 237/1000 | Loss: 0.00000955
Iteration 238/1000 | Loss: 0.00000955
Iteration 239/1000 | Loss: 0.00000955
Iteration 240/1000 | Loss: 0.00000955
Iteration 241/1000 | Loss: 0.00000955
Iteration 242/1000 | Loss: 0.00000955
Iteration 243/1000 | Loss: 0.00000955
Iteration 244/1000 | Loss: 0.00000955
Iteration 245/1000 | Loss: 0.00000955
Iteration 246/1000 | Loss: 0.00000954
Iteration 247/1000 | Loss: 0.00000954
Iteration 248/1000 | Loss: 0.00000954
Iteration 249/1000 | Loss: 0.00000954
Iteration 250/1000 | Loss: 0.00000954
Iteration 251/1000 | Loss: 0.00000954
Iteration 252/1000 | Loss: 0.00000954
Iteration 253/1000 | Loss: 0.00000954
Iteration 254/1000 | Loss: 0.00000954
Iteration 255/1000 | Loss: 0.00000954
Iteration 256/1000 | Loss: 0.00000954
Iteration 257/1000 | Loss: 0.00000954
Iteration 258/1000 | Loss: 0.00000954
Iteration 259/1000 | Loss: 0.00000954
Iteration 260/1000 | Loss: 0.00000954
Iteration 261/1000 | Loss: 0.00000954
Iteration 262/1000 | Loss: 0.00000954
Iteration 263/1000 | Loss: 0.00000953
Iteration 264/1000 | Loss: 0.00000953
Iteration 265/1000 | Loss: 0.00000953
Iteration 266/1000 | Loss: 0.00000953
Iteration 267/1000 | Loss: 0.00000953
Iteration 268/1000 | Loss: 0.00000953
Iteration 269/1000 | Loss: 0.00000953
Iteration 270/1000 | Loss: 0.00000953
Iteration 271/1000 | Loss: 0.00000953
Iteration 272/1000 | Loss: 0.00000953
Iteration 273/1000 | Loss: 0.00000953
Iteration 274/1000 | Loss: 0.00000953
Iteration 275/1000 | Loss: 0.00000953
Iteration 276/1000 | Loss: 0.00000953
Iteration 277/1000 | Loss: 0.00000953
Iteration 278/1000 | Loss: 0.00000953
Iteration 279/1000 | Loss: 0.00000952
Iteration 280/1000 | Loss: 0.00000952
Iteration 281/1000 | Loss: 0.00000952
Iteration 282/1000 | Loss: 0.00000952
Iteration 283/1000 | Loss: 0.00000952
Iteration 284/1000 | Loss: 0.00000952
Iteration 285/1000 | Loss: 0.00000952
Iteration 286/1000 | Loss: 0.00000952
Iteration 287/1000 | Loss: 0.00000952
Iteration 288/1000 | Loss: 0.00000952
Iteration 289/1000 | Loss: 0.00000952
Iteration 290/1000 | Loss: 0.00000952
Iteration 291/1000 | Loss: 0.00000952
Iteration 292/1000 | Loss: 0.00000952
Iteration 293/1000 | Loss: 0.00000952
Iteration 294/1000 | Loss: 0.00000952
Iteration 295/1000 | Loss: 0.00000952
Iteration 296/1000 | Loss: 0.00000952
Iteration 297/1000 | Loss: 0.00000952
Iteration 298/1000 | Loss: 0.00000952
Iteration 299/1000 | Loss: 0.00000952
Iteration 300/1000 | Loss: 0.00000952
Iteration 301/1000 | Loss: 0.00000952
Iteration 302/1000 | Loss: 0.00000952
Iteration 303/1000 | Loss: 0.00000952
Iteration 304/1000 | Loss: 0.00000952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 304. Stopping optimization.
Last 5 losses: [9.516969839751255e-06, 9.516969839751255e-06, 9.516969839751255e-06, 9.516969839751255e-06, 9.516969839751255e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.516969839751255e-06

Optimization complete. Final v2v error: 2.6345789432525635 mm

Highest mean error: 3.530927896499634 mm for frame 71

Lowest mean error: 2.4713709354400635 mm for frame 129

Saving results

Total time: 47.33103036880493
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00514609
Iteration 2/25 | Loss: 0.00151759
Iteration 3/25 | Loss: 0.00135186
Iteration 4/25 | Loss: 0.00133981
Iteration 5/25 | Loss: 0.00133679
Iteration 6/25 | Loss: 0.00133564
Iteration 7/25 | Loss: 0.00133564
Iteration 8/25 | Loss: 0.00133564
Iteration 9/25 | Loss: 0.00133564
Iteration 10/25 | Loss: 0.00133564
Iteration 11/25 | Loss: 0.00133564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001335635781288147, 0.001335635781288147, 0.001335635781288147, 0.001335635781288147, 0.001335635781288147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001335635781288147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30378258
Iteration 2/25 | Loss: 0.00171335
Iteration 3/25 | Loss: 0.00171334
Iteration 4/25 | Loss: 0.00171334
Iteration 5/25 | Loss: 0.00171334
Iteration 6/25 | Loss: 0.00171334
Iteration 7/25 | Loss: 0.00171334
Iteration 8/25 | Loss: 0.00171334
Iteration 9/25 | Loss: 0.00171334
Iteration 10/25 | Loss: 0.00171334
Iteration 11/25 | Loss: 0.00171334
Iteration 12/25 | Loss: 0.00171334
Iteration 13/25 | Loss: 0.00171333
Iteration 14/25 | Loss: 0.00171333
Iteration 15/25 | Loss: 0.00171333
Iteration 16/25 | Loss: 0.00171333
Iteration 17/25 | Loss: 0.00171333
Iteration 18/25 | Loss: 0.00171333
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0017133349319919944, 0.0017133349319919944, 0.0017133349319919944, 0.0017133349319919944, 0.0017133349319919944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017133349319919944

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171333
Iteration 2/1000 | Loss: 0.00004324
Iteration 3/1000 | Loss: 0.00003084
Iteration 4/1000 | Loss: 0.00002791
Iteration 5/1000 | Loss: 0.00002629
Iteration 6/1000 | Loss: 0.00002511
Iteration 7/1000 | Loss: 0.00002423
Iteration 8/1000 | Loss: 0.00002366
Iteration 9/1000 | Loss: 0.00002328
Iteration 10/1000 | Loss: 0.00002285
Iteration 11/1000 | Loss: 0.00002251
Iteration 12/1000 | Loss: 0.00002232
Iteration 13/1000 | Loss: 0.00002214
Iteration 14/1000 | Loss: 0.00002202
Iteration 15/1000 | Loss: 0.00002192
Iteration 16/1000 | Loss: 0.00002191
Iteration 17/1000 | Loss: 0.00002191
Iteration 18/1000 | Loss: 0.00002187
Iteration 19/1000 | Loss: 0.00002186
Iteration 20/1000 | Loss: 0.00002179
Iteration 21/1000 | Loss: 0.00002179
Iteration 22/1000 | Loss: 0.00002177
Iteration 23/1000 | Loss: 0.00002176
Iteration 24/1000 | Loss: 0.00002174
Iteration 25/1000 | Loss: 0.00002174
Iteration 26/1000 | Loss: 0.00002173
Iteration 27/1000 | Loss: 0.00002173
Iteration 28/1000 | Loss: 0.00002173
Iteration 29/1000 | Loss: 0.00002172
Iteration 30/1000 | Loss: 0.00002172
Iteration 31/1000 | Loss: 0.00002172
Iteration 32/1000 | Loss: 0.00002172
Iteration 33/1000 | Loss: 0.00002172
Iteration 34/1000 | Loss: 0.00002172
Iteration 35/1000 | Loss: 0.00002172
Iteration 36/1000 | Loss: 0.00002171
Iteration 37/1000 | Loss: 0.00002171
Iteration 38/1000 | Loss: 0.00002171
Iteration 39/1000 | Loss: 0.00002170
Iteration 40/1000 | Loss: 0.00002170
Iteration 41/1000 | Loss: 0.00002170
Iteration 42/1000 | Loss: 0.00002169
Iteration 43/1000 | Loss: 0.00002169
Iteration 44/1000 | Loss: 0.00002169
Iteration 45/1000 | Loss: 0.00002168
Iteration 46/1000 | Loss: 0.00002168
Iteration 47/1000 | Loss: 0.00002167
Iteration 48/1000 | Loss: 0.00002166
Iteration 49/1000 | Loss: 0.00002165
Iteration 50/1000 | Loss: 0.00002163
Iteration 51/1000 | Loss: 0.00002161
Iteration 52/1000 | Loss: 0.00002160
Iteration 53/1000 | Loss: 0.00002160
Iteration 54/1000 | Loss: 0.00002160
Iteration 55/1000 | Loss: 0.00002160
Iteration 56/1000 | Loss: 0.00002159
Iteration 57/1000 | Loss: 0.00002159
Iteration 58/1000 | Loss: 0.00002158
Iteration 59/1000 | Loss: 0.00002158
Iteration 60/1000 | Loss: 0.00002157
Iteration 61/1000 | Loss: 0.00002157
Iteration 62/1000 | Loss: 0.00002157
Iteration 63/1000 | Loss: 0.00002156
Iteration 64/1000 | Loss: 0.00002156
Iteration 65/1000 | Loss: 0.00002156
Iteration 66/1000 | Loss: 0.00002155
Iteration 67/1000 | Loss: 0.00002155
Iteration 68/1000 | Loss: 0.00002155
Iteration 69/1000 | Loss: 0.00002155
Iteration 70/1000 | Loss: 0.00002155
Iteration 71/1000 | Loss: 0.00002154
Iteration 72/1000 | Loss: 0.00002154
Iteration 73/1000 | Loss: 0.00002154
Iteration 74/1000 | Loss: 0.00002154
Iteration 75/1000 | Loss: 0.00002154
Iteration 76/1000 | Loss: 0.00002154
Iteration 77/1000 | Loss: 0.00002153
Iteration 78/1000 | Loss: 0.00002153
Iteration 79/1000 | Loss: 0.00002153
Iteration 80/1000 | Loss: 0.00002153
Iteration 81/1000 | Loss: 0.00002153
Iteration 82/1000 | Loss: 0.00002153
Iteration 83/1000 | Loss: 0.00002153
Iteration 84/1000 | Loss: 0.00002153
Iteration 85/1000 | Loss: 0.00002153
Iteration 86/1000 | Loss: 0.00002153
Iteration 87/1000 | Loss: 0.00002152
Iteration 88/1000 | Loss: 0.00002152
Iteration 89/1000 | Loss: 0.00002152
Iteration 90/1000 | Loss: 0.00002151
Iteration 91/1000 | Loss: 0.00002151
Iteration 92/1000 | Loss: 0.00002151
Iteration 93/1000 | Loss: 0.00002151
Iteration 94/1000 | Loss: 0.00002151
Iteration 95/1000 | Loss: 0.00002151
Iteration 96/1000 | Loss: 0.00002151
Iteration 97/1000 | Loss: 0.00002151
Iteration 98/1000 | Loss: 0.00002151
Iteration 99/1000 | Loss: 0.00002151
Iteration 100/1000 | Loss: 0.00002150
Iteration 101/1000 | Loss: 0.00002150
Iteration 102/1000 | Loss: 0.00002150
Iteration 103/1000 | Loss: 0.00002150
Iteration 104/1000 | Loss: 0.00002150
Iteration 105/1000 | Loss: 0.00002150
Iteration 106/1000 | Loss: 0.00002149
Iteration 107/1000 | Loss: 0.00002149
Iteration 108/1000 | Loss: 0.00002149
Iteration 109/1000 | Loss: 0.00002148
Iteration 110/1000 | Loss: 0.00002148
Iteration 111/1000 | Loss: 0.00002148
Iteration 112/1000 | Loss: 0.00002148
Iteration 113/1000 | Loss: 0.00002148
Iteration 114/1000 | Loss: 0.00002147
Iteration 115/1000 | Loss: 0.00002147
Iteration 116/1000 | Loss: 0.00002147
Iteration 117/1000 | Loss: 0.00002147
Iteration 118/1000 | Loss: 0.00002147
Iteration 119/1000 | Loss: 0.00002147
Iteration 120/1000 | Loss: 0.00002147
Iteration 121/1000 | Loss: 0.00002146
Iteration 122/1000 | Loss: 0.00002146
Iteration 123/1000 | Loss: 0.00002146
Iteration 124/1000 | Loss: 0.00002145
Iteration 125/1000 | Loss: 0.00002145
Iteration 126/1000 | Loss: 0.00002145
Iteration 127/1000 | Loss: 0.00002145
Iteration 128/1000 | Loss: 0.00002145
Iteration 129/1000 | Loss: 0.00002145
Iteration 130/1000 | Loss: 0.00002145
Iteration 131/1000 | Loss: 0.00002145
Iteration 132/1000 | Loss: 0.00002145
Iteration 133/1000 | Loss: 0.00002145
Iteration 134/1000 | Loss: 0.00002145
Iteration 135/1000 | Loss: 0.00002145
Iteration 136/1000 | Loss: 0.00002144
Iteration 137/1000 | Loss: 0.00002144
Iteration 138/1000 | Loss: 0.00002144
Iteration 139/1000 | Loss: 0.00002144
Iteration 140/1000 | Loss: 0.00002144
Iteration 141/1000 | Loss: 0.00002144
Iteration 142/1000 | Loss: 0.00002144
Iteration 143/1000 | Loss: 0.00002144
Iteration 144/1000 | Loss: 0.00002143
Iteration 145/1000 | Loss: 0.00002143
Iteration 146/1000 | Loss: 0.00002143
Iteration 147/1000 | Loss: 0.00002143
Iteration 148/1000 | Loss: 0.00002143
Iteration 149/1000 | Loss: 0.00002143
Iteration 150/1000 | Loss: 0.00002143
Iteration 151/1000 | Loss: 0.00002143
Iteration 152/1000 | Loss: 0.00002143
Iteration 153/1000 | Loss: 0.00002143
Iteration 154/1000 | Loss: 0.00002143
Iteration 155/1000 | Loss: 0.00002143
Iteration 156/1000 | Loss: 0.00002142
Iteration 157/1000 | Loss: 0.00002142
Iteration 158/1000 | Loss: 0.00002142
Iteration 159/1000 | Loss: 0.00002142
Iteration 160/1000 | Loss: 0.00002142
Iteration 161/1000 | Loss: 0.00002142
Iteration 162/1000 | Loss: 0.00002142
Iteration 163/1000 | Loss: 0.00002142
Iteration 164/1000 | Loss: 0.00002142
Iteration 165/1000 | Loss: 0.00002142
Iteration 166/1000 | Loss: 0.00002142
Iteration 167/1000 | Loss: 0.00002142
Iteration 168/1000 | Loss: 0.00002142
Iteration 169/1000 | Loss: 0.00002141
Iteration 170/1000 | Loss: 0.00002141
Iteration 171/1000 | Loss: 0.00002141
Iteration 172/1000 | Loss: 0.00002141
Iteration 173/1000 | Loss: 0.00002141
Iteration 174/1000 | Loss: 0.00002141
Iteration 175/1000 | Loss: 0.00002141
Iteration 176/1000 | Loss: 0.00002141
Iteration 177/1000 | Loss: 0.00002141
Iteration 178/1000 | Loss: 0.00002141
Iteration 179/1000 | Loss: 0.00002141
Iteration 180/1000 | Loss: 0.00002141
Iteration 181/1000 | Loss: 0.00002141
Iteration 182/1000 | Loss: 0.00002141
Iteration 183/1000 | Loss: 0.00002141
Iteration 184/1000 | Loss: 0.00002141
Iteration 185/1000 | Loss: 0.00002141
Iteration 186/1000 | Loss: 0.00002141
Iteration 187/1000 | Loss: 0.00002141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [2.140514516213443e-05, 2.140514516213443e-05, 2.140514516213443e-05, 2.140514516213443e-05, 2.140514516213443e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.140514516213443e-05

Optimization complete. Final v2v error: 3.7103896141052246 mm

Highest mean error: 4.2891845703125 mm for frame 11

Lowest mean error: 2.889848232269287 mm for frame 63

Saving results

Total time: 45.81287980079651
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804061
Iteration 2/25 | Loss: 0.00181573
Iteration 3/25 | Loss: 0.00146123
Iteration 4/25 | Loss: 0.00143849
Iteration 5/25 | Loss: 0.00143245
Iteration 6/25 | Loss: 0.00143093
Iteration 7/25 | Loss: 0.00143067
Iteration 8/25 | Loss: 0.00143067
Iteration 9/25 | Loss: 0.00143067
Iteration 10/25 | Loss: 0.00143067
Iteration 11/25 | Loss: 0.00143067
Iteration 12/25 | Loss: 0.00143067
Iteration 13/25 | Loss: 0.00143067
Iteration 14/25 | Loss: 0.00143067
Iteration 15/25 | Loss: 0.00143067
Iteration 16/25 | Loss: 0.00143067
Iteration 17/25 | Loss: 0.00143067
Iteration 18/25 | Loss: 0.00143067
Iteration 19/25 | Loss: 0.00143067
Iteration 20/25 | Loss: 0.00143067
Iteration 21/25 | Loss: 0.00143067
Iteration 22/25 | Loss: 0.00143067
Iteration 23/25 | Loss: 0.00143067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014306666562333703, 0.0014306666562333703, 0.0014306666562333703, 0.0014306666562333703, 0.0014306666562333703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014306666562333703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.35136721
Iteration 2/25 | Loss: 0.00117784
Iteration 3/25 | Loss: 0.00117782
Iteration 4/25 | Loss: 0.00117782
Iteration 5/25 | Loss: 0.00117782
Iteration 6/25 | Loss: 0.00117782
Iteration 7/25 | Loss: 0.00117782
Iteration 8/25 | Loss: 0.00117782
Iteration 9/25 | Loss: 0.00117782
Iteration 10/25 | Loss: 0.00117782
Iteration 11/25 | Loss: 0.00117782
Iteration 12/25 | Loss: 0.00117782
Iteration 13/25 | Loss: 0.00117782
Iteration 14/25 | Loss: 0.00117782
Iteration 15/25 | Loss: 0.00117782
Iteration 16/25 | Loss: 0.00117782
Iteration 17/25 | Loss: 0.00117782
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001177815138362348, 0.001177815138362348, 0.001177815138362348, 0.001177815138362348, 0.001177815138362348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001177815138362348

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117782
Iteration 2/1000 | Loss: 0.00010904
Iteration 3/1000 | Loss: 0.00005332
Iteration 4/1000 | Loss: 0.00004158
Iteration 5/1000 | Loss: 0.00003847
Iteration 6/1000 | Loss: 0.00003708
Iteration 7/1000 | Loss: 0.00003617
Iteration 8/1000 | Loss: 0.00003534
Iteration 9/1000 | Loss: 0.00003455
Iteration 10/1000 | Loss: 0.00003394
Iteration 11/1000 | Loss: 0.00003339
Iteration 12/1000 | Loss: 0.00003310
Iteration 13/1000 | Loss: 0.00003275
Iteration 14/1000 | Loss: 0.00003248
Iteration 15/1000 | Loss: 0.00003222
Iteration 16/1000 | Loss: 0.00003204
Iteration 17/1000 | Loss: 0.00003188
Iteration 18/1000 | Loss: 0.00003177
Iteration 19/1000 | Loss: 0.00003171
Iteration 20/1000 | Loss: 0.00003154
Iteration 21/1000 | Loss: 0.00003150
Iteration 22/1000 | Loss: 0.00003149
Iteration 23/1000 | Loss: 0.00003149
Iteration 24/1000 | Loss: 0.00003148
Iteration 25/1000 | Loss: 0.00003147
Iteration 26/1000 | Loss: 0.00003146
Iteration 27/1000 | Loss: 0.00003144
Iteration 28/1000 | Loss: 0.00003141
Iteration 29/1000 | Loss: 0.00003141
Iteration 30/1000 | Loss: 0.00003135
Iteration 31/1000 | Loss: 0.00003134
Iteration 32/1000 | Loss: 0.00003133
Iteration 33/1000 | Loss: 0.00003133
Iteration 34/1000 | Loss: 0.00003132
Iteration 35/1000 | Loss: 0.00003128
Iteration 36/1000 | Loss: 0.00003127
Iteration 37/1000 | Loss: 0.00003126
Iteration 38/1000 | Loss: 0.00003126
Iteration 39/1000 | Loss: 0.00003125
Iteration 40/1000 | Loss: 0.00003125
Iteration 41/1000 | Loss: 0.00003124
Iteration 42/1000 | Loss: 0.00003124
Iteration 43/1000 | Loss: 0.00003124
Iteration 44/1000 | Loss: 0.00003124
Iteration 45/1000 | Loss: 0.00003124
Iteration 46/1000 | Loss: 0.00003123
Iteration 47/1000 | Loss: 0.00003122
Iteration 48/1000 | Loss: 0.00003122
Iteration 49/1000 | Loss: 0.00003121
Iteration 50/1000 | Loss: 0.00003121
Iteration 51/1000 | Loss: 0.00003121
Iteration 52/1000 | Loss: 0.00003120
Iteration 53/1000 | Loss: 0.00003120
Iteration 54/1000 | Loss: 0.00003119
Iteration 55/1000 | Loss: 0.00003119
Iteration 56/1000 | Loss: 0.00003119
Iteration 57/1000 | Loss: 0.00003119
Iteration 58/1000 | Loss: 0.00003119
Iteration 59/1000 | Loss: 0.00003119
Iteration 60/1000 | Loss: 0.00003119
Iteration 61/1000 | Loss: 0.00003119
Iteration 62/1000 | Loss: 0.00003119
Iteration 63/1000 | Loss: 0.00003118
Iteration 64/1000 | Loss: 0.00003118
Iteration 65/1000 | Loss: 0.00003118
Iteration 66/1000 | Loss: 0.00003118
Iteration 67/1000 | Loss: 0.00003118
Iteration 68/1000 | Loss: 0.00003118
Iteration 69/1000 | Loss: 0.00003117
Iteration 70/1000 | Loss: 0.00003117
Iteration 71/1000 | Loss: 0.00003117
Iteration 72/1000 | Loss: 0.00003116
Iteration 73/1000 | Loss: 0.00003116
Iteration 74/1000 | Loss: 0.00003116
Iteration 75/1000 | Loss: 0.00003116
Iteration 76/1000 | Loss: 0.00003116
Iteration 77/1000 | Loss: 0.00003115
Iteration 78/1000 | Loss: 0.00003115
Iteration 79/1000 | Loss: 0.00003115
Iteration 80/1000 | Loss: 0.00003115
Iteration 81/1000 | Loss: 0.00003114
Iteration 82/1000 | Loss: 0.00003114
Iteration 83/1000 | Loss: 0.00003114
Iteration 84/1000 | Loss: 0.00003114
Iteration 85/1000 | Loss: 0.00003114
Iteration 86/1000 | Loss: 0.00003114
Iteration 87/1000 | Loss: 0.00003114
Iteration 88/1000 | Loss: 0.00003114
Iteration 89/1000 | Loss: 0.00003113
Iteration 90/1000 | Loss: 0.00003113
Iteration 91/1000 | Loss: 0.00003113
Iteration 92/1000 | Loss: 0.00003113
Iteration 93/1000 | Loss: 0.00003113
Iteration 94/1000 | Loss: 0.00003113
Iteration 95/1000 | Loss: 0.00003113
Iteration 96/1000 | Loss: 0.00003113
Iteration 97/1000 | Loss: 0.00003113
Iteration 98/1000 | Loss: 0.00003113
Iteration 99/1000 | Loss: 0.00003113
Iteration 100/1000 | Loss: 0.00003113
Iteration 101/1000 | Loss: 0.00003112
Iteration 102/1000 | Loss: 0.00003112
Iteration 103/1000 | Loss: 0.00003112
Iteration 104/1000 | Loss: 0.00003112
Iteration 105/1000 | Loss: 0.00003112
Iteration 106/1000 | Loss: 0.00003112
Iteration 107/1000 | Loss: 0.00003112
Iteration 108/1000 | Loss: 0.00003111
Iteration 109/1000 | Loss: 0.00003111
Iteration 110/1000 | Loss: 0.00003111
Iteration 111/1000 | Loss: 0.00003111
Iteration 112/1000 | Loss: 0.00003111
Iteration 113/1000 | Loss: 0.00003111
Iteration 114/1000 | Loss: 0.00003110
Iteration 115/1000 | Loss: 0.00003110
Iteration 116/1000 | Loss: 0.00003110
Iteration 117/1000 | Loss: 0.00003110
Iteration 118/1000 | Loss: 0.00003110
Iteration 119/1000 | Loss: 0.00003110
Iteration 120/1000 | Loss: 0.00003109
Iteration 121/1000 | Loss: 0.00003109
Iteration 122/1000 | Loss: 0.00003109
Iteration 123/1000 | Loss: 0.00003108
Iteration 124/1000 | Loss: 0.00003108
Iteration 125/1000 | Loss: 0.00003108
Iteration 126/1000 | Loss: 0.00003107
Iteration 127/1000 | Loss: 0.00003107
Iteration 128/1000 | Loss: 0.00003107
Iteration 129/1000 | Loss: 0.00003107
Iteration 130/1000 | Loss: 0.00003107
Iteration 131/1000 | Loss: 0.00003107
Iteration 132/1000 | Loss: 0.00003106
Iteration 133/1000 | Loss: 0.00003106
Iteration 134/1000 | Loss: 0.00003106
Iteration 135/1000 | Loss: 0.00003106
Iteration 136/1000 | Loss: 0.00003106
Iteration 137/1000 | Loss: 0.00003105
Iteration 138/1000 | Loss: 0.00003105
Iteration 139/1000 | Loss: 0.00003105
Iteration 140/1000 | Loss: 0.00003105
Iteration 141/1000 | Loss: 0.00003105
Iteration 142/1000 | Loss: 0.00003105
Iteration 143/1000 | Loss: 0.00003105
Iteration 144/1000 | Loss: 0.00003104
Iteration 145/1000 | Loss: 0.00003104
Iteration 146/1000 | Loss: 0.00003104
Iteration 147/1000 | Loss: 0.00003104
Iteration 148/1000 | Loss: 0.00003104
Iteration 149/1000 | Loss: 0.00003104
Iteration 150/1000 | Loss: 0.00003104
Iteration 151/1000 | Loss: 0.00003104
Iteration 152/1000 | Loss: 0.00003104
Iteration 153/1000 | Loss: 0.00003103
Iteration 154/1000 | Loss: 0.00003103
Iteration 155/1000 | Loss: 0.00003103
Iteration 156/1000 | Loss: 0.00003103
Iteration 157/1000 | Loss: 0.00003103
Iteration 158/1000 | Loss: 0.00003103
Iteration 159/1000 | Loss: 0.00003103
Iteration 160/1000 | Loss: 0.00003103
Iteration 161/1000 | Loss: 0.00003102
Iteration 162/1000 | Loss: 0.00003102
Iteration 163/1000 | Loss: 0.00003102
Iteration 164/1000 | Loss: 0.00003102
Iteration 165/1000 | Loss: 0.00003102
Iteration 166/1000 | Loss: 0.00003102
Iteration 167/1000 | Loss: 0.00003102
Iteration 168/1000 | Loss: 0.00003102
Iteration 169/1000 | Loss: 0.00003102
Iteration 170/1000 | Loss: 0.00003102
Iteration 171/1000 | Loss: 0.00003102
Iteration 172/1000 | Loss: 0.00003102
Iteration 173/1000 | Loss: 0.00003102
Iteration 174/1000 | Loss: 0.00003102
Iteration 175/1000 | Loss: 0.00003102
Iteration 176/1000 | Loss: 0.00003101
Iteration 177/1000 | Loss: 0.00003101
Iteration 178/1000 | Loss: 0.00003101
Iteration 179/1000 | Loss: 0.00003101
Iteration 180/1000 | Loss: 0.00003101
Iteration 181/1000 | Loss: 0.00003101
Iteration 182/1000 | Loss: 0.00003101
Iteration 183/1000 | Loss: 0.00003101
Iteration 184/1000 | Loss: 0.00003101
Iteration 185/1000 | Loss: 0.00003101
Iteration 186/1000 | Loss: 0.00003101
Iteration 187/1000 | Loss: 0.00003101
Iteration 188/1000 | Loss: 0.00003101
Iteration 189/1000 | Loss: 0.00003101
Iteration 190/1000 | Loss: 0.00003101
Iteration 191/1000 | Loss: 0.00003101
Iteration 192/1000 | Loss: 0.00003101
Iteration 193/1000 | Loss: 0.00003101
Iteration 194/1000 | Loss: 0.00003101
Iteration 195/1000 | Loss: 0.00003101
Iteration 196/1000 | Loss: 0.00003101
Iteration 197/1000 | Loss: 0.00003101
Iteration 198/1000 | Loss: 0.00003101
Iteration 199/1000 | Loss: 0.00003101
Iteration 200/1000 | Loss: 0.00003101
Iteration 201/1000 | Loss: 0.00003101
Iteration 202/1000 | Loss: 0.00003101
Iteration 203/1000 | Loss: 0.00003101
Iteration 204/1000 | Loss: 0.00003101
Iteration 205/1000 | Loss: 0.00003101
Iteration 206/1000 | Loss: 0.00003101
Iteration 207/1000 | Loss: 0.00003101
Iteration 208/1000 | Loss: 0.00003101
Iteration 209/1000 | Loss: 0.00003101
Iteration 210/1000 | Loss: 0.00003101
Iteration 211/1000 | Loss: 0.00003101
Iteration 212/1000 | Loss: 0.00003101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [3.101247057202272e-05, 3.101247057202272e-05, 3.101247057202272e-05, 3.101247057202272e-05, 3.101247057202272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.101247057202272e-05

Optimization complete. Final v2v error: 4.4655375480651855 mm

Highest mean error: 5.917117595672607 mm for frame 59

Lowest mean error: 3.1911990642547607 mm for frame 97

Saving results

Total time: 53.88422250747681
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00290121
Iteration 2/25 | Loss: 0.00145433
Iteration 3/25 | Loss: 0.00129943
Iteration 4/25 | Loss: 0.00127346
Iteration 5/25 | Loss: 0.00126430
Iteration 6/25 | Loss: 0.00126155
Iteration 7/25 | Loss: 0.00126126
Iteration 8/25 | Loss: 0.00126126
Iteration 9/25 | Loss: 0.00126126
Iteration 10/25 | Loss: 0.00126126
Iteration 11/25 | Loss: 0.00126126
Iteration 12/25 | Loss: 0.00126126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012612624559551477, 0.0012612624559551477, 0.0012612624559551477, 0.0012612624559551477, 0.0012612624559551477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012612624559551477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27609944
Iteration 2/25 | Loss: 0.00185196
Iteration 3/25 | Loss: 0.00185196
Iteration 4/25 | Loss: 0.00185196
Iteration 5/25 | Loss: 0.00185195
Iteration 6/25 | Loss: 0.00185195
Iteration 7/25 | Loss: 0.00185195
Iteration 8/25 | Loss: 0.00185195
Iteration 9/25 | Loss: 0.00185195
Iteration 10/25 | Loss: 0.00185195
Iteration 11/25 | Loss: 0.00185195
Iteration 12/25 | Loss: 0.00185195
Iteration 13/25 | Loss: 0.00185195
Iteration 14/25 | Loss: 0.00185195
Iteration 15/25 | Loss: 0.00185195
Iteration 16/25 | Loss: 0.00185195
Iteration 17/25 | Loss: 0.00185195
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001851953100413084, 0.001851953100413084, 0.001851953100413084, 0.001851953100413084, 0.001851953100413084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001851953100413084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185195
Iteration 2/1000 | Loss: 0.00004599
Iteration 3/1000 | Loss: 0.00002485
Iteration 4/1000 | Loss: 0.00002125
Iteration 5/1000 | Loss: 0.00001958
Iteration 6/1000 | Loss: 0.00001830
Iteration 7/1000 | Loss: 0.00001759
Iteration 8/1000 | Loss: 0.00001710
Iteration 9/1000 | Loss: 0.00001659
Iteration 10/1000 | Loss: 0.00001615
Iteration 11/1000 | Loss: 0.00001590
Iteration 12/1000 | Loss: 0.00001569
Iteration 13/1000 | Loss: 0.00001564
Iteration 14/1000 | Loss: 0.00001561
Iteration 15/1000 | Loss: 0.00001561
Iteration 16/1000 | Loss: 0.00001554
Iteration 17/1000 | Loss: 0.00001553
Iteration 18/1000 | Loss: 0.00001552
Iteration 19/1000 | Loss: 0.00001552
Iteration 20/1000 | Loss: 0.00001551
Iteration 21/1000 | Loss: 0.00001547
Iteration 22/1000 | Loss: 0.00001544
Iteration 23/1000 | Loss: 0.00001544
Iteration 24/1000 | Loss: 0.00001544
Iteration 25/1000 | Loss: 0.00001543
Iteration 26/1000 | Loss: 0.00001543
Iteration 27/1000 | Loss: 0.00001539
Iteration 28/1000 | Loss: 0.00001537
Iteration 29/1000 | Loss: 0.00001536
Iteration 30/1000 | Loss: 0.00001536
Iteration 31/1000 | Loss: 0.00001533
Iteration 32/1000 | Loss: 0.00001533
Iteration 33/1000 | Loss: 0.00001531
Iteration 34/1000 | Loss: 0.00001531
Iteration 35/1000 | Loss: 0.00001530
Iteration 36/1000 | Loss: 0.00001530
Iteration 37/1000 | Loss: 0.00001530
Iteration 38/1000 | Loss: 0.00001529
Iteration 39/1000 | Loss: 0.00001528
Iteration 40/1000 | Loss: 0.00001527
Iteration 41/1000 | Loss: 0.00001526
Iteration 42/1000 | Loss: 0.00001526
Iteration 43/1000 | Loss: 0.00001526
Iteration 44/1000 | Loss: 0.00001524
Iteration 45/1000 | Loss: 0.00001524
Iteration 46/1000 | Loss: 0.00001524
Iteration 47/1000 | Loss: 0.00001523
Iteration 48/1000 | Loss: 0.00001523
Iteration 49/1000 | Loss: 0.00001523
Iteration 50/1000 | Loss: 0.00001522
Iteration 51/1000 | Loss: 0.00001522
Iteration 52/1000 | Loss: 0.00001521
Iteration 53/1000 | Loss: 0.00001521
Iteration 54/1000 | Loss: 0.00001521
Iteration 55/1000 | Loss: 0.00001521
Iteration 56/1000 | Loss: 0.00001521
Iteration 57/1000 | Loss: 0.00001520
Iteration 58/1000 | Loss: 0.00001520
Iteration 59/1000 | Loss: 0.00001520
Iteration 60/1000 | Loss: 0.00001520
Iteration 61/1000 | Loss: 0.00001520
Iteration 62/1000 | Loss: 0.00001520
Iteration 63/1000 | Loss: 0.00001519
Iteration 64/1000 | Loss: 0.00001519
Iteration 65/1000 | Loss: 0.00001519
Iteration 66/1000 | Loss: 0.00001519
Iteration 67/1000 | Loss: 0.00001518
Iteration 68/1000 | Loss: 0.00001518
Iteration 69/1000 | Loss: 0.00001518
Iteration 70/1000 | Loss: 0.00001518
Iteration 71/1000 | Loss: 0.00001517
Iteration 72/1000 | Loss: 0.00001517
Iteration 73/1000 | Loss: 0.00001517
Iteration 74/1000 | Loss: 0.00001517
Iteration 75/1000 | Loss: 0.00001517
Iteration 76/1000 | Loss: 0.00001517
Iteration 77/1000 | Loss: 0.00001517
Iteration 78/1000 | Loss: 0.00001517
Iteration 79/1000 | Loss: 0.00001517
Iteration 80/1000 | Loss: 0.00001517
Iteration 81/1000 | Loss: 0.00001517
Iteration 82/1000 | Loss: 0.00001517
Iteration 83/1000 | Loss: 0.00001516
Iteration 84/1000 | Loss: 0.00001516
Iteration 85/1000 | Loss: 0.00001516
Iteration 86/1000 | Loss: 0.00001516
Iteration 87/1000 | Loss: 0.00001515
Iteration 88/1000 | Loss: 0.00001515
Iteration 89/1000 | Loss: 0.00001515
Iteration 90/1000 | Loss: 0.00001515
Iteration 91/1000 | Loss: 0.00001515
Iteration 92/1000 | Loss: 0.00001515
Iteration 93/1000 | Loss: 0.00001515
Iteration 94/1000 | Loss: 0.00001514
Iteration 95/1000 | Loss: 0.00001514
Iteration 96/1000 | Loss: 0.00001514
Iteration 97/1000 | Loss: 0.00001514
Iteration 98/1000 | Loss: 0.00001514
Iteration 99/1000 | Loss: 0.00001514
Iteration 100/1000 | Loss: 0.00001514
Iteration 101/1000 | Loss: 0.00001514
Iteration 102/1000 | Loss: 0.00001513
Iteration 103/1000 | Loss: 0.00001513
Iteration 104/1000 | Loss: 0.00001513
Iteration 105/1000 | Loss: 0.00001513
Iteration 106/1000 | Loss: 0.00001513
Iteration 107/1000 | Loss: 0.00001513
Iteration 108/1000 | Loss: 0.00001513
Iteration 109/1000 | Loss: 0.00001512
Iteration 110/1000 | Loss: 0.00001512
Iteration 111/1000 | Loss: 0.00001512
Iteration 112/1000 | Loss: 0.00001512
Iteration 113/1000 | Loss: 0.00001511
Iteration 114/1000 | Loss: 0.00001511
Iteration 115/1000 | Loss: 0.00001511
Iteration 116/1000 | Loss: 0.00001511
Iteration 117/1000 | Loss: 0.00001510
Iteration 118/1000 | Loss: 0.00001510
Iteration 119/1000 | Loss: 0.00001510
Iteration 120/1000 | Loss: 0.00001510
Iteration 121/1000 | Loss: 0.00001509
Iteration 122/1000 | Loss: 0.00001509
Iteration 123/1000 | Loss: 0.00001509
Iteration 124/1000 | Loss: 0.00001509
Iteration 125/1000 | Loss: 0.00001508
Iteration 126/1000 | Loss: 0.00001508
Iteration 127/1000 | Loss: 0.00001508
Iteration 128/1000 | Loss: 0.00001508
Iteration 129/1000 | Loss: 0.00001508
Iteration 130/1000 | Loss: 0.00001507
Iteration 131/1000 | Loss: 0.00001507
Iteration 132/1000 | Loss: 0.00001507
Iteration 133/1000 | Loss: 0.00001507
Iteration 134/1000 | Loss: 0.00001507
Iteration 135/1000 | Loss: 0.00001507
Iteration 136/1000 | Loss: 0.00001507
Iteration 137/1000 | Loss: 0.00001507
Iteration 138/1000 | Loss: 0.00001506
Iteration 139/1000 | Loss: 0.00001506
Iteration 140/1000 | Loss: 0.00001506
Iteration 141/1000 | Loss: 0.00001506
Iteration 142/1000 | Loss: 0.00001506
Iteration 143/1000 | Loss: 0.00001506
Iteration 144/1000 | Loss: 0.00001506
Iteration 145/1000 | Loss: 0.00001506
Iteration 146/1000 | Loss: 0.00001506
Iteration 147/1000 | Loss: 0.00001506
Iteration 148/1000 | Loss: 0.00001506
Iteration 149/1000 | Loss: 0.00001505
Iteration 150/1000 | Loss: 0.00001505
Iteration 151/1000 | Loss: 0.00001505
Iteration 152/1000 | Loss: 0.00001505
Iteration 153/1000 | Loss: 0.00001505
Iteration 154/1000 | Loss: 0.00001505
Iteration 155/1000 | Loss: 0.00001505
Iteration 156/1000 | Loss: 0.00001505
Iteration 157/1000 | Loss: 0.00001505
Iteration 158/1000 | Loss: 0.00001505
Iteration 159/1000 | Loss: 0.00001505
Iteration 160/1000 | Loss: 0.00001505
Iteration 161/1000 | Loss: 0.00001504
Iteration 162/1000 | Loss: 0.00001504
Iteration 163/1000 | Loss: 0.00001504
Iteration 164/1000 | Loss: 0.00001504
Iteration 165/1000 | Loss: 0.00001504
Iteration 166/1000 | Loss: 0.00001504
Iteration 167/1000 | Loss: 0.00001504
Iteration 168/1000 | Loss: 0.00001504
Iteration 169/1000 | Loss: 0.00001504
Iteration 170/1000 | Loss: 0.00001503
Iteration 171/1000 | Loss: 0.00001503
Iteration 172/1000 | Loss: 0.00001503
Iteration 173/1000 | Loss: 0.00001503
Iteration 174/1000 | Loss: 0.00001503
Iteration 175/1000 | Loss: 0.00001503
Iteration 176/1000 | Loss: 0.00001503
Iteration 177/1000 | Loss: 0.00001503
Iteration 178/1000 | Loss: 0.00001502
Iteration 179/1000 | Loss: 0.00001502
Iteration 180/1000 | Loss: 0.00001502
Iteration 181/1000 | Loss: 0.00001502
Iteration 182/1000 | Loss: 0.00001502
Iteration 183/1000 | Loss: 0.00001502
Iteration 184/1000 | Loss: 0.00001502
Iteration 185/1000 | Loss: 0.00001501
Iteration 186/1000 | Loss: 0.00001501
Iteration 187/1000 | Loss: 0.00001501
Iteration 188/1000 | Loss: 0.00001501
Iteration 189/1000 | Loss: 0.00001501
Iteration 190/1000 | Loss: 0.00001501
Iteration 191/1000 | Loss: 0.00001500
Iteration 192/1000 | Loss: 0.00001500
Iteration 193/1000 | Loss: 0.00001500
Iteration 194/1000 | Loss: 0.00001500
Iteration 195/1000 | Loss: 0.00001499
Iteration 196/1000 | Loss: 0.00001499
Iteration 197/1000 | Loss: 0.00001499
Iteration 198/1000 | Loss: 0.00001499
Iteration 199/1000 | Loss: 0.00001499
Iteration 200/1000 | Loss: 0.00001499
Iteration 201/1000 | Loss: 0.00001499
Iteration 202/1000 | Loss: 0.00001499
Iteration 203/1000 | Loss: 0.00001499
Iteration 204/1000 | Loss: 0.00001499
Iteration 205/1000 | Loss: 0.00001499
Iteration 206/1000 | Loss: 0.00001499
Iteration 207/1000 | Loss: 0.00001499
Iteration 208/1000 | Loss: 0.00001499
Iteration 209/1000 | Loss: 0.00001498
Iteration 210/1000 | Loss: 0.00001498
Iteration 211/1000 | Loss: 0.00001498
Iteration 212/1000 | Loss: 0.00001498
Iteration 213/1000 | Loss: 0.00001498
Iteration 214/1000 | Loss: 0.00001498
Iteration 215/1000 | Loss: 0.00001498
Iteration 216/1000 | Loss: 0.00001498
Iteration 217/1000 | Loss: 0.00001498
Iteration 218/1000 | Loss: 0.00001498
Iteration 219/1000 | Loss: 0.00001498
Iteration 220/1000 | Loss: 0.00001498
Iteration 221/1000 | Loss: 0.00001498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.4983457731432281e-05, 1.4983457731432281e-05, 1.4983457731432281e-05, 1.4983457731432281e-05, 1.4983457731432281e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4983457731432281e-05

Optimization complete. Final v2v error: 3.2672038078308105 mm

Highest mean error: 3.8798201084136963 mm for frame 57

Lowest mean error: 2.9245564937591553 mm for frame 0

Saving results

Total time: 50.701701641082764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00368777
Iteration 2/25 | Loss: 0.00128039
Iteration 3/25 | Loss: 0.00123232
Iteration 4/25 | Loss: 0.00122603
Iteration 5/25 | Loss: 0.00122510
Iteration 6/25 | Loss: 0.00122510
Iteration 7/25 | Loss: 0.00122510
Iteration 8/25 | Loss: 0.00122510
Iteration 9/25 | Loss: 0.00122510
Iteration 10/25 | Loss: 0.00122510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012251021107658744, 0.0012251021107658744, 0.0012251021107658744, 0.0012251021107658744, 0.0012251021107658744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012251021107658744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38871038
Iteration 2/25 | Loss: 0.00142428
Iteration 3/25 | Loss: 0.00142428
Iteration 4/25 | Loss: 0.00142428
Iteration 5/25 | Loss: 0.00142428
Iteration 6/25 | Loss: 0.00142428
Iteration 7/25 | Loss: 0.00142428
Iteration 8/25 | Loss: 0.00142427
Iteration 9/25 | Loss: 0.00142427
Iteration 10/25 | Loss: 0.00142427
Iteration 11/25 | Loss: 0.00142427
Iteration 12/25 | Loss: 0.00142427
Iteration 13/25 | Loss: 0.00142427
Iteration 14/25 | Loss: 0.00142427
Iteration 15/25 | Loss: 0.00142427
Iteration 16/25 | Loss: 0.00142427
Iteration 17/25 | Loss: 0.00142427
Iteration 18/25 | Loss: 0.00142427
Iteration 19/25 | Loss: 0.00142427
Iteration 20/25 | Loss: 0.00142427
Iteration 21/25 | Loss: 0.00142427
Iteration 22/25 | Loss: 0.00142427
Iteration 23/25 | Loss: 0.00142427
Iteration 24/25 | Loss: 0.00142427
Iteration 25/25 | Loss: 0.00142427

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142427
Iteration 2/1000 | Loss: 0.00002271
Iteration 3/1000 | Loss: 0.00001597
Iteration 4/1000 | Loss: 0.00001376
Iteration 5/1000 | Loss: 0.00001249
Iteration 6/1000 | Loss: 0.00001168
Iteration 7/1000 | Loss: 0.00001108
Iteration 8/1000 | Loss: 0.00001066
Iteration 9/1000 | Loss: 0.00001054
Iteration 10/1000 | Loss: 0.00001028
Iteration 11/1000 | Loss: 0.00001028
Iteration 12/1000 | Loss: 0.00001007
Iteration 13/1000 | Loss: 0.00001004
Iteration 14/1000 | Loss: 0.00001001
Iteration 15/1000 | Loss: 0.00000999
Iteration 16/1000 | Loss: 0.00000993
Iteration 17/1000 | Loss: 0.00000992
Iteration 18/1000 | Loss: 0.00000992
Iteration 19/1000 | Loss: 0.00000976
Iteration 20/1000 | Loss: 0.00000971
Iteration 21/1000 | Loss: 0.00000964
Iteration 22/1000 | Loss: 0.00000963
Iteration 23/1000 | Loss: 0.00000961
Iteration 24/1000 | Loss: 0.00000959
Iteration 25/1000 | Loss: 0.00000959
Iteration 26/1000 | Loss: 0.00000955
Iteration 27/1000 | Loss: 0.00000953
Iteration 28/1000 | Loss: 0.00000953
Iteration 29/1000 | Loss: 0.00000952
Iteration 30/1000 | Loss: 0.00000952
Iteration 31/1000 | Loss: 0.00000951
Iteration 32/1000 | Loss: 0.00000949
Iteration 33/1000 | Loss: 0.00000949
Iteration 34/1000 | Loss: 0.00000946
Iteration 35/1000 | Loss: 0.00000945
Iteration 36/1000 | Loss: 0.00000945
Iteration 37/1000 | Loss: 0.00000944
Iteration 38/1000 | Loss: 0.00000944
Iteration 39/1000 | Loss: 0.00000941
Iteration 40/1000 | Loss: 0.00000935
Iteration 41/1000 | Loss: 0.00000932
Iteration 42/1000 | Loss: 0.00000931
Iteration 43/1000 | Loss: 0.00000930
Iteration 44/1000 | Loss: 0.00000929
Iteration 45/1000 | Loss: 0.00000928
Iteration 46/1000 | Loss: 0.00000923
Iteration 47/1000 | Loss: 0.00000922
Iteration 48/1000 | Loss: 0.00000920
Iteration 49/1000 | Loss: 0.00000920
Iteration 50/1000 | Loss: 0.00000919
Iteration 51/1000 | Loss: 0.00000918
Iteration 52/1000 | Loss: 0.00000918
Iteration 53/1000 | Loss: 0.00000918
Iteration 54/1000 | Loss: 0.00000917
Iteration 55/1000 | Loss: 0.00000917
Iteration 56/1000 | Loss: 0.00000917
Iteration 57/1000 | Loss: 0.00000916
Iteration 58/1000 | Loss: 0.00000916
Iteration 59/1000 | Loss: 0.00000916
Iteration 60/1000 | Loss: 0.00000915
Iteration 61/1000 | Loss: 0.00000915
Iteration 62/1000 | Loss: 0.00000914
Iteration 63/1000 | Loss: 0.00000914
Iteration 64/1000 | Loss: 0.00000914
Iteration 65/1000 | Loss: 0.00000914
Iteration 66/1000 | Loss: 0.00000914
Iteration 67/1000 | Loss: 0.00000914
Iteration 68/1000 | Loss: 0.00000914
Iteration 69/1000 | Loss: 0.00000913
Iteration 70/1000 | Loss: 0.00000912
Iteration 71/1000 | Loss: 0.00000911
Iteration 72/1000 | Loss: 0.00000911
Iteration 73/1000 | Loss: 0.00000911
Iteration 74/1000 | Loss: 0.00000911
Iteration 75/1000 | Loss: 0.00000911
Iteration 76/1000 | Loss: 0.00000910
Iteration 77/1000 | Loss: 0.00000910
Iteration 78/1000 | Loss: 0.00000910
Iteration 79/1000 | Loss: 0.00000910
Iteration 80/1000 | Loss: 0.00000910
Iteration 81/1000 | Loss: 0.00000909
Iteration 82/1000 | Loss: 0.00000909
Iteration 83/1000 | Loss: 0.00000909
Iteration 84/1000 | Loss: 0.00000908
Iteration 85/1000 | Loss: 0.00000908
Iteration 86/1000 | Loss: 0.00000908
Iteration 87/1000 | Loss: 0.00000908
Iteration 88/1000 | Loss: 0.00000908
Iteration 89/1000 | Loss: 0.00000908
Iteration 90/1000 | Loss: 0.00000908
Iteration 91/1000 | Loss: 0.00000908
Iteration 92/1000 | Loss: 0.00000908
Iteration 93/1000 | Loss: 0.00000907
Iteration 94/1000 | Loss: 0.00000907
Iteration 95/1000 | Loss: 0.00000907
Iteration 96/1000 | Loss: 0.00000906
Iteration 97/1000 | Loss: 0.00000906
Iteration 98/1000 | Loss: 0.00000906
Iteration 99/1000 | Loss: 0.00000906
Iteration 100/1000 | Loss: 0.00000906
Iteration 101/1000 | Loss: 0.00000906
Iteration 102/1000 | Loss: 0.00000905
Iteration 103/1000 | Loss: 0.00000905
Iteration 104/1000 | Loss: 0.00000905
Iteration 105/1000 | Loss: 0.00000905
Iteration 106/1000 | Loss: 0.00000905
Iteration 107/1000 | Loss: 0.00000905
Iteration 108/1000 | Loss: 0.00000905
Iteration 109/1000 | Loss: 0.00000905
Iteration 110/1000 | Loss: 0.00000905
Iteration 111/1000 | Loss: 0.00000905
Iteration 112/1000 | Loss: 0.00000905
Iteration 113/1000 | Loss: 0.00000905
Iteration 114/1000 | Loss: 0.00000905
Iteration 115/1000 | Loss: 0.00000905
Iteration 116/1000 | Loss: 0.00000904
Iteration 117/1000 | Loss: 0.00000904
Iteration 118/1000 | Loss: 0.00000904
Iteration 119/1000 | Loss: 0.00000904
Iteration 120/1000 | Loss: 0.00000904
Iteration 121/1000 | Loss: 0.00000904
Iteration 122/1000 | Loss: 0.00000904
Iteration 123/1000 | Loss: 0.00000904
Iteration 124/1000 | Loss: 0.00000904
Iteration 125/1000 | Loss: 0.00000904
Iteration 126/1000 | Loss: 0.00000904
Iteration 127/1000 | Loss: 0.00000904
Iteration 128/1000 | Loss: 0.00000904
Iteration 129/1000 | Loss: 0.00000904
Iteration 130/1000 | Loss: 0.00000903
Iteration 131/1000 | Loss: 0.00000903
Iteration 132/1000 | Loss: 0.00000903
Iteration 133/1000 | Loss: 0.00000903
Iteration 134/1000 | Loss: 0.00000903
Iteration 135/1000 | Loss: 0.00000903
Iteration 136/1000 | Loss: 0.00000903
Iteration 137/1000 | Loss: 0.00000903
Iteration 138/1000 | Loss: 0.00000902
Iteration 139/1000 | Loss: 0.00000902
Iteration 140/1000 | Loss: 0.00000902
Iteration 141/1000 | Loss: 0.00000902
Iteration 142/1000 | Loss: 0.00000902
Iteration 143/1000 | Loss: 0.00000902
Iteration 144/1000 | Loss: 0.00000901
Iteration 145/1000 | Loss: 0.00000901
Iteration 146/1000 | Loss: 0.00000901
Iteration 147/1000 | Loss: 0.00000901
Iteration 148/1000 | Loss: 0.00000901
Iteration 149/1000 | Loss: 0.00000901
Iteration 150/1000 | Loss: 0.00000901
Iteration 151/1000 | Loss: 0.00000900
Iteration 152/1000 | Loss: 0.00000900
Iteration 153/1000 | Loss: 0.00000900
Iteration 154/1000 | Loss: 0.00000900
Iteration 155/1000 | Loss: 0.00000900
Iteration 156/1000 | Loss: 0.00000900
Iteration 157/1000 | Loss: 0.00000900
Iteration 158/1000 | Loss: 0.00000899
Iteration 159/1000 | Loss: 0.00000899
Iteration 160/1000 | Loss: 0.00000899
Iteration 161/1000 | Loss: 0.00000898
Iteration 162/1000 | Loss: 0.00000898
Iteration 163/1000 | Loss: 0.00000898
Iteration 164/1000 | Loss: 0.00000898
Iteration 165/1000 | Loss: 0.00000897
Iteration 166/1000 | Loss: 0.00000897
Iteration 167/1000 | Loss: 0.00000897
Iteration 168/1000 | Loss: 0.00000897
Iteration 169/1000 | Loss: 0.00000897
Iteration 170/1000 | Loss: 0.00000897
Iteration 171/1000 | Loss: 0.00000897
Iteration 172/1000 | Loss: 0.00000897
Iteration 173/1000 | Loss: 0.00000896
Iteration 174/1000 | Loss: 0.00000896
Iteration 175/1000 | Loss: 0.00000896
Iteration 176/1000 | Loss: 0.00000896
Iteration 177/1000 | Loss: 0.00000896
Iteration 178/1000 | Loss: 0.00000896
Iteration 179/1000 | Loss: 0.00000895
Iteration 180/1000 | Loss: 0.00000895
Iteration 181/1000 | Loss: 0.00000895
Iteration 182/1000 | Loss: 0.00000895
Iteration 183/1000 | Loss: 0.00000894
Iteration 184/1000 | Loss: 0.00000894
Iteration 185/1000 | Loss: 0.00000894
Iteration 186/1000 | Loss: 0.00000894
Iteration 187/1000 | Loss: 0.00000894
Iteration 188/1000 | Loss: 0.00000894
Iteration 189/1000 | Loss: 0.00000894
Iteration 190/1000 | Loss: 0.00000894
Iteration 191/1000 | Loss: 0.00000894
Iteration 192/1000 | Loss: 0.00000894
Iteration 193/1000 | Loss: 0.00000894
Iteration 194/1000 | Loss: 0.00000894
Iteration 195/1000 | Loss: 0.00000894
Iteration 196/1000 | Loss: 0.00000894
Iteration 197/1000 | Loss: 0.00000894
Iteration 198/1000 | Loss: 0.00000894
Iteration 199/1000 | Loss: 0.00000894
Iteration 200/1000 | Loss: 0.00000893
Iteration 201/1000 | Loss: 0.00000893
Iteration 202/1000 | Loss: 0.00000893
Iteration 203/1000 | Loss: 0.00000893
Iteration 204/1000 | Loss: 0.00000893
Iteration 205/1000 | Loss: 0.00000893
Iteration 206/1000 | Loss: 0.00000893
Iteration 207/1000 | Loss: 0.00000893
Iteration 208/1000 | Loss: 0.00000893
Iteration 209/1000 | Loss: 0.00000893
Iteration 210/1000 | Loss: 0.00000893
Iteration 211/1000 | Loss: 0.00000892
Iteration 212/1000 | Loss: 0.00000892
Iteration 213/1000 | Loss: 0.00000892
Iteration 214/1000 | Loss: 0.00000892
Iteration 215/1000 | Loss: 0.00000892
Iteration 216/1000 | Loss: 0.00000892
Iteration 217/1000 | Loss: 0.00000892
Iteration 218/1000 | Loss: 0.00000892
Iteration 219/1000 | Loss: 0.00000892
Iteration 220/1000 | Loss: 0.00000892
Iteration 221/1000 | Loss: 0.00000892
Iteration 222/1000 | Loss: 0.00000892
Iteration 223/1000 | Loss: 0.00000892
Iteration 224/1000 | Loss: 0.00000892
Iteration 225/1000 | Loss: 0.00000892
Iteration 226/1000 | Loss: 0.00000892
Iteration 227/1000 | Loss: 0.00000892
Iteration 228/1000 | Loss: 0.00000892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [8.917059858504217e-06, 8.917059858504217e-06, 8.917059858504217e-06, 8.917059858504217e-06, 8.917059858504217e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.917059858504217e-06

Optimization complete. Final v2v error: 2.5846364498138428 mm

Highest mean error: 2.7394206523895264 mm for frame 110

Lowest mean error: 2.538156747817993 mm for frame 88

Saving results

Total time: 43.58979368209839
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993957
Iteration 2/25 | Loss: 0.00310794
Iteration 3/25 | Loss: 0.00214983
Iteration 4/25 | Loss: 0.00195736
Iteration 5/25 | Loss: 0.00188864
Iteration 6/25 | Loss: 0.00191821
Iteration 7/25 | Loss: 0.00177247
Iteration 8/25 | Loss: 0.00170499
Iteration 9/25 | Loss: 0.00165747
Iteration 10/25 | Loss: 0.00163410
Iteration 11/25 | Loss: 0.00160410
Iteration 12/25 | Loss: 0.00158087
Iteration 13/25 | Loss: 0.00157086
Iteration 14/25 | Loss: 0.00156865
Iteration 15/25 | Loss: 0.00157166
Iteration 16/25 | Loss: 0.00157313
Iteration 17/25 | Loss: 0.00157399
Iteration 18/25 | Loss: 0.00156883
Iteration 19/25 | Loss: 0.00156004
Iteration 20/25 | Loss: 0.00155651
Iteration 21/25 | Loss: 0.00155737
Iteration 22/25 | Loss: 0.00155683
Iteration 23/25 | Loss: 0.00155692
Iteration 24/25 | Loss: 0.00155735
Iteration 25/25 | Loss: 0.00155968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25871277
Iteration 2/25 | Loss: 0.00425966
Iteration 3/25 | Loss: 0.00233329
Iteration 4/25 | Loss: 0.00233328
Iteration 5/25 | Loss: 0.00233328
Iteration 6/25 | Loss: 0.00233328
Iteration 7/25 | Loss: 0.00227436
Iteration 8/25 | Loss: 0.00227432
Iteration 9/25 | Loss: 0.00227432
Iteration 10/25 | Loss: 0.00227432
Iteration 11/25 | Loss: 0.00227432
Iteration 12/25 | Loss: 0.00227432
Iteration 13/25 | Loss: 0.00227432
Iteration 14/25 | Loss: 0.00227432
Iteration 15/25 | Loss: 0.00227432
Iteration 16/25 | Loss: 0.00227432
Iteration 17/25 | Loss: 0.00227432
Iteration 18/25 | Loss: 0.00227432
Iteration 19/25 | Loss: 0.00227432
Iteration 20/25 | Loss: 0.00227432
Iteration 21/25 | Loss: 0.00227432
Iteration 22/25 | Loss: 0.00227432
Iteration 23/25 | Loss: 0.00227432
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0022743199951946735, 0.0022743199951946735, 0.0022743199951946735, 0.0022743199951946735, 0.0022743199951946735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022743199951946735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227432
Iteration 2/1000 | Loss: 0.00255323
Iteration 3/1000 | Loss: 0.00024944
Iteration 4/1000 | Loss: 0.00026066
Iteration 5/1000 | Loss: 0.00023109
Iteration 6/1000 | Loss: 0.00014452
Iteration 7/1000 | Loss: 0.00044108
Iteration 8/1000 | Loss: 0.00014286
Iteration 9/1000 | Loss: 0.00012494
Iteration 10/1000 | Loss: 0.00025604
Iteration 11/1000 | Loss: 0.00045766
Iteration 12/1000 | Loss: 0.00022914
Iteration 13/1000 | Loss: 0.00017500
Iteration 14/1000 | Loss: 0.00017151
Iteration 15/1000 | Loss: 0.00039434
Iteration 16/1000 | Loss: 0.00025485
Iteration 17/1000 | Loss: 0.00066089
Iteration 18/1000 | Loss: 0.00102197
Iteration 19/1000 | Loss: 0.00219092
Iteration 20/1000 | Loss: 0.00026369
Iteration 21/1000 | Loss: 0.00020163
Iteration 22/1000 | Loss: 0.00015274
Iteration 23/1000 | Loss: 0.00064282
Iteration 24/1000 | Loss: 0.00021403
Iteration 25/1000 | Loss: 0.00110241
Iteration 26/1000 | Loss: 0.00015546
Iteration 27/1000 | Loss: 0.00009117
Iteration 28/1000 | Loss: 0.00015342
Iteration 29/1000 | Loss: 0.00053807
Iteration 30/1000 | Loss: 0.00009615
Iteration 31/1000 | Loss: 0.00006923
Iteration 32/1000 | Loss: 0.00005739
Iteration 33/1000 | Loss: 0.00039642
Iteration 34/1000 | Loss: 0.00004724
Iteration 35/1000 | Loss: 0.00004197
Iteration 36/1000 | Loss: 0.00046478
Iteration 37/1000 | Loss: 0.00005457
Iteration 38/1000 | Loss: 0.00006532
Iteration 39/1000 | Loss: 0.00003667
Iteration 40/1000 | Loss: 0.00022996
Iteration 41/1000 | Loss: 0.00012495
Iteration 42/1000 | Loss: 0.00021132
Iteration 43/1000 | Loss: 0.00004341
Iteration 44/1000 | Loss: 0.00002986
Iteration 45/1000 | Loss: 0.00002700
Iteration 46/1000 | Loss: 0.00002464
Iteration 47/1000 | Loss: 0.00002331
Iteration 48/1000 | Loss: 0.00002264
Iteration 49/1000 | Loss: 0.00002213
Iteration 50/1000 | Loss: 0.00002171
Iteration 51/1000 | Loss: 0.00002135
Iteration 52/1000 | Loss: 0.00002109
Iteration 53/1000 | Loss: 0.00002088
Iteration 54/1000 | Loss: 0.00002082
Iteration 55/1000 | Loss: 0.00002079
Iteration 56/1000 | Loss: 0.00002074
Iteration 57/1000 | Loss: 0.00002070
Iteration 58/1000 | Loss: 0.00002070
Iteration 59/1000 | Loss: 0.00002070
Iteration 60/1000 | Loss: 0.00002070
Iteration 61/1000 | Loss: 0.00002070
Iteration 62/1000 | Loss: 0.00002069
Iteration 63/1000 | Loss: 0.00002069
Iteration 64/1000 | Loss: 0.00002069
Iteration 65/1000 | Loss: 0.00002069
Iteration 66/1000 | Loss: 0.00002068
Iteration 67/1000 | Loss: 0.00002068
Iteration 68/1000 | Loss: 0.00002067
Iteration 69/1000 | Loss: 0.00002067
Iteration 70/1000 | Loss: 0.00002067
Iteration 71/1000 | Loss: 0.00002066
Iteration 72/1000 | Loss: 0.00002066
Iteration 73/1000 | Loss: 0.00002066
Iteration 74/1000 | Loss: 0.00002066
Iteration 75/1000 | Loss: 0.00002066
Iteration 76/1000 | Loss: 0.00002066
Iteration 77/1000 | Loss: 0.00002065
Iteration 78/1000 | Loss: 0.00002061
Iteration 79/1000 | Loss: 0.00002061
Iteration 80/1000 | Loss: 0.00002059
Iteration 81/1000 | Loss: 0.00002059
Iteration 82/1000 | Loss: 0.00002059
Iteration 83/1000 | Loss: 0.00002058
Iteration 84/1000 | Loss: 0.00002057
Iteration 85/1000 | Loss: 0.00002056
Iteration 86/1000 | Loss: 0.00002056
Iteration 87/1000 | Loss: 0.00002055
Iteration 88/1000 | Loss: 0.00002054
Iteration 89/1000 | Loss: 0.00002054
Iteration 90/1000 | Loss: 0.00002052
Iteration 91/1000 | Loss: 0.00002051
Iteration 92/1000 | Loss: 0.00002051
Iteration 93/1000 | Loss: 0.00002050
Iteration 94/1000 | Loss: 0.00002050
Iteration 95/1000 | Loss: 0.00002050
Iteration 96/1000 | Loss: 0.00002050
Iteration 97/1000 | Loss: 0.00002049
Iteration 98/1000 | Loss: 0.00002049
Iteration 99/1000 | Loss: 0.00002049
Iteration 100/1000 | Loss: 0.00002049
Iteration 101/1000 | Loss: 0.00002049
Iteration 102/1000 | Loss: 0.00002049
Iteration 103/1000 | Loss: 0.00002049
Iteration 104/1000 | Loss: 0.00002049
Iteration 105/1000 | Loss: 0.00002049
Iteration 106/1000 | Loss: 0.00002049
Iteration 107/1000 | Loss: 0.00002049
Iteration 108/1000 | Loss: 0.00002048
Iteration 109/1000 | Loss: 0.00002048
Iteration 110/1000 | Loss: 0.00002048
Iteration 111/1000 | Loss: 0.00002048
Iteration 112/1000 | Loss: 0.00002048
Iteration 113/1000 | Loss: 0.00002048
Iteration 114/1000 | Loss: 0.00002048
Iteration 115/1000 | Loss: 0.00002048
Iteration 116/1000 | Loss: 0.00002048
Iteration 117/1000 | Loss: 0.00002047
Iteration 118/1000 | Loss: 0.00002047
Iteration 119/1000 | Loss: 0.00002047
Iteration 120/1000 | Loss: 0.00002047
Iteration 121/1000 | Loss: 0.00002047
Iteration 122/1000 | Loss: 0.00002047
Iteration 123/1000 | Loss: 0.00002047
Iteration 124/1000 | Loss: 0.00002047
Iteration 125/1000 | Loss: 0.00002046
Iteration 126/1000 | Loss: 0.00002046
Iteration 127/1000 | Loss: 0.00002046
Iteration 128/1000 | Loss: 0.00002046
Iteration 129/1000 | Loss: 0.00002046
Iteration 130/1000 | Loss: 0.00002046
Iteration 131/1000 | Loss: 0.00002046
Iteration 132/1000 | Loss: 0.00002045
Iteration 133/1000 | Loss: 0.00002045
Iteration 134/1000 | Loss: 0.00002045
Iteration 135/1000 | Loss: 0.00002045
Iteration 136/1000 | Loss: 0.00002045
Iteration 137/1000 | Loss: 0.00002045
Iteration 138/1000 | Loss: 0.00002045
Iteration 139/1000 | Loss: 0.00002045
Iteration 140/1000 | Loss: 0.00002045
Iteration 141/1000 | Loss: 0.00002045
Iteration 142/1000 | Loss: 0.00002045
Iteration 143/1000 | Loss: 0.00002045
Iteration 144/1000 | Loss: 0.00002045
Iteration 145/1000 | Loss: 0.00002045
Iteration 146/1000 | Loss: 0.00002045
Iteration 147/1000 | Loss: 0.00002045
Iteration 148/1000 | Loss: 0.00002045
Iteration 149/1000 | Loss: 0.00002045
Iteration 150/1000 | Loss: 0.00002045
Iteration 151/1000 | Loss: 0.00002045
Iteration 152/1000 | Loss: 0.00002045
Iteration 153/1000 | Loss: 0.00002045
Iteration 154/1000 | Loss: 0.00002045
Iteration 155/1000 | Loss: 0.00002045
Iteration 156/1000 | Loss: 0.00002045
Iteration 157/1000 | Loss: 0.00002045
Iteration 158/1000 | Loss: 0.00002045
Iteration 159/1000 | Loss: 0.00002045
Iteration 160/1000 | Loss: 0.00002045
Iteration 161/1000 | Loss: 0.00002045
Iteration 162/1000 | Loss: 0.00002045
Iteration 163/1000 | Loss: 0.00002045
Iteration 164/1000 | Loss: 0.00002045
Iteration 165/1000 | Loss: 0.00002045
Iteration 166/1000 | Loss: 0.00002045
Iteration 167/1000 | Loss: 0.00002045
Iteration 168/1000 | Loss: 0.00002045
Iteration 169/1000 | Loss: 0.00002045
Iteration 170/1000 | Loss: 0.00002045
Iteration 171/1000 | Loss: 0.00002045
Iteration 172/1000 | Loss: 0.00002045
Iteration 173/1000 | Loss: 0.00002045
Iteration 174/1000 | Loss: 0.00002045
Iteration 175/1000 | Loss: 0.00002045
Iteration 176/1000 | Loss: 0.00002045
Iteration 177/1000 | Loss: 0.00002045
Iteration 178/1000 | Loss: 0.00002045
Iteration 179/1000 | Loss: 0.00002045
Iteration 180/1000 | Loss: 0.00002045
Iteration 181/1000 | Loss: 0.00002045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.0449051589821465e-05, 2.0449051589821465e-05, 2.0449051589821465e-05, 2.0449051589821465e-05, 2.0449051589821465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0449051589821465e-05

Optimization complete. Final v2v error: 3.5874736309051514 mm

Highest mean error: 11.129756927490234 mm for frame 96

Lowest mean error: 3.2359042167663574 mm for frame 234

Saving results

Total time: 150.95962572097778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_005/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_005/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789077
Iteration 2/25 | Loss: 0.00142855
Iteration 3/25 | Loss: 0.00128043
Iteration 4/25 | Loss: 0.00125987
Iteration 5/25 | Loss: 0.00125570
Iteration 6/25 | Loss: 0.00125570
Iteration 7/25 | Loss: 0.00125570
Iteration 8/25 | Loss: 0.00125570
Iteration 9/25 | Loss: 0.00125570
Iteration 10/25 | Loss: 0.00125570
Iteration 11/25 | Loss: 0.00125570
Iteration 12/25 | Loss: 0.00125570
Iteration 13/25 | Loss: 0.00125570
Iteration 14/25 | Loss: 0.00125570
Iteration 15/25 | Loss: 0.00125570
Iteration 16/25 | Loss: 0.00125570
Iteration 17/25 | Loss: 0.00125570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012556978035718203, 0.0012556978035718203, 0.0012556978035718203, 0.0012556978035718203, 0.0012556978035718203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012556978035718203

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28774941
Iteration 2/25 | Loss: 0.00156296
Iteration 3/25 | Loss: 0.00156296
Iteration 4/25 | Loss: 0.00156296
Iteration 5/25 | Loss: 0.00156296
Iteration 6/25 | Loss: 0.00156296
Iteration 7/25 | Loss: 0.00156295
Iteration 8/25 | Loss: 0.00156295
Iteration 9/25 | Loss: 0.00156295
Iteration 10/25 | Loss: 0.00156295
Iteration 11/25 | Loss: 0.00156295
Iteration 12/25 | Loss: 0.00156295
Iteration 13/25 | Loss: 0.00156295
Iteration 14/25 | Loss: 0.00156295
Iteration 15/25 | Loss: 0.00156295
Iteration 16/25 | Loss: 0.00156295
Iteration 17/25 | Loss: 0.00156295
Iteration 18/25 | Loss: 0.00156295
Iteration 19/25 | Loss: 0.00156295
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0015629533445462584, 0.0015629533445462584, 0.0015629533445462584, 0.0015629533445462584, 0.0015629533445462584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015629533445462584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156295
Iteration 2/1000 | Loss: 0.00003034
Iteration 3/1000 | Loss: 0.00002048
Iteration 4/1000 | Loss: 0.00001648
Iteration 5/1000 | Loss: 0.00001470
Iteration 6/1000 | Loss: 0.00001363
Iteration 7/1000 | Loss: 0.00001279
Iteration 8/1000 | Loss: 0.00001232
Iteration 9/1000 | Loss: 0.00001189
Iteration 10/1000 | Loss: 0.00001149
Iteration 11/1000 | Loss: 0.00001112
Iteration 12/1000 | Loss: 0.00001094
Iteration 13/1000 | Loss: 0.00001086
Iteration 14/1000 | Loss: 0.00001074
Iteration 15/1000 | Loss: 0.00001073
Iteration 16/1000 | Loss: 0.00001066
Iteration 17/1000 | Loss: 0.00001066
Iteration 18/1000 | Loss: 0.00001060
Iteration 19/1000 | Loss: 0.00001060
Iteration 20/1000 | Loss: 0.00001058
Iteration 21/1000 | Loss: 0.00001057
Iteration 22/1000 | Loss: 0.00001057
Iteration 23/1000 | Loss: 0.00001056
Iteration 24/1000 | Loss: 0.00001056
Iteration 25/1000 | Loss: 0.00001055
Iteration 26/1000 | Loss: 0.00001054
Iteration 27/1000 | Loss: 0.00001054
Iteration 28/1000 | Loss: 0.00001052
Iteration 29/1000 | Loss: 0.00001051
Iteration 30/1000 | Loss: 0.00001048
Iteration 31/1000 | Loss: 0.00001047
Iteration 32/1000 | Loss: 0.00001045
Iteration 33/1000 | Loss: 0.00001044
Iteration 34/1000 | Loss: 0.00001044
Iteration 35/1000 | Loss: 0.00001042
Iteration 36/1000 | Loss: 0.00001040
Iteration 37/1000 | Loss: 0.00001040
Iteration 38/1000 | Loss: 0.00001040
Iteration 39/1000 | Loss: 0.00001040
Iteration 40/1000 | Loss: 0.00001040
Iteration 41/1000 | Loss: 0.00001040
Iteration 42/1000 | Loss: 0.00001040
Iteration 43/1000 | Loss: 0.00001039
Iteration 44/1000 | Loss: 0.00001039
Iteration 45/1000 | Loss: 0.00001039
Iteration 46/1000 | Loss: 0.00001039
Iteration 47/1000 | Loss: 0.00001039
Iteration 48/1000 | Loss: 0.00001039
Iteration 49/1000 | Loss: 0.00001038
Iteration 50/1000 | Loss: 0.00001038
Iteration 51/1000 | Loss: 0.00001038
Iteration 52/1000 | Loss: 0.00001038
Iteration 53/1000 | Loss: 0.00001037
Iteration 54/1000 | Loss: 0.00001037
Iteration 55/1000 | Loss: 0.00001036
Iteration 56/1000 | Loss: 0.00001036
Iteration 57/1000 | Loss: 0.00001036
Iteration 58/1000 | Loss: 0.00001035
Iteration 59/1000 | Loss: 0.00001035
Iteration 60/1000 | Loss: 0.00001035
Iteration 61/1000 | Loss: 0.00001034
Iteration 62/1000 | Loss: 0.00001034
Iteration 63/1000 | Loss: 0.00001034
Iteration 64/1000 | Loss: 0.00001033
Iteration 65/1000 | Loss: 0.00001033
Iteration 66/1000 | Loss: 0.00001033
Iteration 67/1000 | Loss: 0.00001033
Iteration 68/1000 | Loss: 0.00001032
Iteration 69/1000 | Loss: 0.00001032
Iteration 70/1000 | Loss: 0.00001032
Iteration 71/1000 | Loss: 0.00001032
Iteration 72/1000 | Loss: 0.00001031
Iteration 73/1000 | Loss: 0.00001031
Iteration 74/1000 | Loss: 0.00001030
Iteration 75/1000 | Loss: 0.00001030
Iteration 76/1000 | Loss: 0.00001030
Iteration 77/1000 | Loss: 0.00001030
Iteration 78/1000 | Loss: 0.00001029
Iteration 79/1000 | Loss: 0.00001029
Iteration 80/1000 | Loss: 0.00001028
Iteration 81/1000 | Loss: 0.00001028
Iteration 82/1000 | Loss: 0.00001028
Iteration 83/1000 | Loss: 0.00001028
Iteration 84/1000 | Loss: 0.00001028
Iteration 85/1000 | Loss: 0.00001028
Iteration 86/1000 | Loss: 0.00001028
Iteration 87/1000 | Loss: 0.00001028
Iteration 88/1000 | Loss: 0.00001027
Iteration 89/1000 | Loss: 0.00001027
Iteration 90/1000 | Loss: 0.00001027
Iteration 91/1000 | Loss: 0.00001027
Iteration 92/1000 | Loss: 0.00001027
Iteration 93/1000 | Loss: 0.00001027
Iteration 94/1000 | Loss: 0.00001027
Iteration 95/1000 | Loss: 0.00001027
Iteration 96/1000 | Loss: 0.00001027
Iteration 97/1000 | Loss: 0.00001027
Iteration 98/1000 | Loss: 0.00001027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.0274517080688383e-05, 1.0274517080688383e-05, 1.0274517080688383e-05, 1.0274517080688383e-05, 1.0274517080688383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0274517080688383e-05

Optimization complete. Final v2v error: 2.773489475250244 mm

Highest mean error: 2.951014995574951 mm for frame 75

Lowest mean error: 2.6461315155029297 mm for frame 128

Saving results

Total time: 40.258737564086914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404729
Iteration 2/25 | Loss: 0.00111518
Iteration 3/25 | Loss: 0.00102744
Iteration 4/25 | Loss: 0.00101612
Iteration 5/25 | Loss: 0.00100774
Iteration 6/25 | Loss: 0.00100634
Iteration 7/25 | Loss: 0.00100634
Iteration 8/25 | Loss: 0.00100634
Iteration 9/25 | Loss: 0.00100634
Iteration 10/25 | Loss: 0.00100634
Iteration 11/25 | Loss: 0.00100634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010063365334644914, 0.0010063365334644914, 0.0010063365334644914, 0.0010063365334644914, 0.0010063365334644914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010063365334644914

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66881788
Iteration 2/25 | Loss: 0.00096367
Iteration 3/25 | Loss: 0.00096362
Iteration 4/25 | Loss: 0.00096362
Iteration 5/25 | Loss: 0.00096362
Iteration 6/25 | Loss: 0.00096361
Iteration 7/25 | Loss: 0.00096361
Iteration 8/25 | Loss: 0.00096361
Iteration 9/25 | Loss: 0.00096361
Iteration 10/25 | Loss: 0.00096361
Iteration 11/25 | Loss: 0.00096361
Iteration 12/25 | Loss: 0.00096361
Iteration 13/25 | Loss: 0.00096361
Iteration 14/25 | Loss: 0.00096361
Iteration 15/25 | Loss: 0.00096361
Iteration 16/25 | Loss: 0.00096361
Iteration 17/25 | Loss: 0.00096361
Iteration 18/25 | Loss: 0.00096361
Iteration 19/25 | Loss: 0.00096361
Iteration 20/25 | Loss: 0.00096361
Iteration 21/25 | Loss: 0.00096361
Iteration 22/25 | Loss: 0.00096361
Iteration 23/25 | Loss: 0.00096361
Iteration 24/25 | Loss: 0.00096361
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009636139729991555, 0.0009636139729991555, 0.0009636139729991555, 0.0009636139729991555, 0.0009636139729991555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009636139729991555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096361
Iteration 2/1000 | Loss: 0.00003257
Iteration 3/1000 | Loss: 0.00002356
Iteration 4/1000 | Loss: 0.00002132
Iteration 5/1000 | Loss: 0.00002062
Iteration 6/1000 | Loss: 0.00001989
Iteration 7/1000 | Loss: 0.00001933
Iteration 8/1000 | Loss: 0.00001904
Iteration 9/1000 | Loss: 0.00001890
Iteration 10/1000 | Loss: 0.00001883
Iteration 11/1000 | Loss: 0.00001882
Iteration 12/1000 | Loss: 0.00001882
Iteration 13/1000 | Loss: 0.00001881
Iteration 14/1000 | Loss: 0.00001880
Iteration 15/1000 | Loss: 0.00001874
Iteration 16/1000 | Loss: 0.00001873
Iteration 17/1000 | Loss: 0.00001871
Iteration 18/1000 | Loss: 0.00001871
Iteration 19/1000 | Loss: 0.00001869
Iteration 20/1000 | Loss: 0.00001867
Iteration 21/1000 | Loss: 0.00001849
Iteration 22/1000 | Loss: 0.00001843
Iteration 23/1000 | Loss: 0.00001842
Iteration 24/1000 | Loss: 0.00001842
Iteration 25/1000 | Loss: 0.00001841
Iteration 26/1000 | Loss: 0.00001840
Iteration 27/1000 | Loss: 0.00001835
Iteration 28/1000 | Loss: 0.00001835
Iteration 29/1000 | Loss: 0.00001834
Iteration 30/1000 | Loss: 0.00001834
Iteration 31/1000 | Loss: 0.00001833
Iteration 32/1000 | Loss: 0.00001832
Iteration 33/1000 | Loss: 0.00001832
Iteration 34/1000 | Loss: 0.00001832
Iteration 35/1000 | Loss: 0.00001831
Iteration 36/1000 | Loss: 0.00001831
Iteration 37/1000 | Loss: 0.00001831
Iteration 38/1000 | Loss: 0.00001830
Iteration 39/1000 | Loss: 0.00001830
Iteration 40/1000 | Loss: 0.00001829
Iteration 41/1000 | Loss: 0.00001829
Iteration 42/1000 | Loss: 0.00001829
Iteration 43/1000 | Loss: 0.00001829
Iteration 44/1000 | Loss: 0.00001829
Iteration 45/1000 | Loss: 0.00001829
Iteration 46/1000 | Loss: 0.00001829
Iteration 47/1000 | Loss: 0.00001829
Iteration 48/1000 | Loss: 0.00001828
Iteration 49/1000 | Loss: 0.00001828
Iteration 50/1000 | Loss: 0.00001828
Iteration 51/1000 | Loss: 0.00001828
Iteration 52/1000 | Loss: 0.00001828
Iteration 53/1000 | Loss: 0.00001828
Iteration 54/1000 | Loss: 0.00001827
Iteration 55/1000 | Loss: 0.00001827
Iteration 56/1000 | Loss: 0.00001827
Iteration 57/1000 | Loss: 0.00001827
Iteration 58/1000 | Loss: 0.00001827
Iteration 59/1000 | Loss: 0.00001827
Iteration 60/1000 | Loss: 0.00001827
Iteration 61/1000 | Loss: 0.00001827
Iteration 62/1000 | Loss: 0.00001827
Iteration 63/1000 | Loss: 0.00001826
Iteration 64/1000 | Loss: 0.00001826
Iteration 65/1000 | Loss: 0.00001826
Iteration 66/1000 | Loss: 0.00001826
Iteration 67/1000 | Loss: 0.00001826
Iteration 68/1000 | Loss: 0.00001826
Iteration 69/1000 | Loss: 0.00001826
Iteration 70/1000 | Loss: 0.00001826
Iteration 71/1000 | Loss: 0.00001826
Iteration 72/1000 | Loss: 0.00001826
Iteration 73/1000 | Loss: 0.00001826
Iteration 74/1000 | Loss: 0.00001826
Iteration 75/1000 | Loss: 0.00001826
Iteration 76/1000 | Loss: 0.00001826
Iteration 77/1000 | Loss: 0.00001826
Iteration 78/1000 | Loss: 0.00001826
Iteration 79/1000 | Loss: 0.00001825
Iteration 80/1000 | Loss: 0.00001825
Iteration 81/1000 | Loss: 0.00001825
Iteration 82/1000 | Loss: 0.00001825
Iteration 83/1000 | Loss: 0.00001825
Iteration 84/1000 | Loss: 0.00001825
Iteration 85/1000 | Loss: 0.00001825
Iteration 86/1000 | Loss: 0.00001825
Iteration 87/1000 | Loss: 0.00001825
Iteration 88/1000 | Loss: 0.00001824
Iteration 89/1000 | Loss: 0.00001824
Iteration 90/1000 | Loss: 0.00001824
Iteration 91/1000 | Loss: 0.00001824
Iteration 92/1000 | Loss: 0.00001824
Iteration 93/1000 | Loss: 0.00001824
Iteration 94/1000 | Loss: 0.00001824
Iteration 95/1000 | Loss: 0.00001823
Iteration 96/1000 | Loss: 0.00001823
Iteration 97/1000 | Loss: 0.00001823
Iteration 98/1000 | Loss: 0.00001823
Iteration 99/1000 | Loss: 0.00001822
Iteration 100/1000 | Loss: 0.00001822
Iteration 101/1000 | Loss: 0.00001822
Iteration 102/1000 | Loss: 0.00001822
Iteration 103/1000 | Loss: 0.00001821
Iteration 104/1000 | Loss: 0.00001821
Iteration 105/1000 | Loss: 0.00001821
Iteration 106/1000 | Loss: 0.00001821
Iteration 107/1000 | Loss: 0.00001821
Iteration 108/1000 | Loss: 0.00001821
Iteration 109/1000 | Loss: 0.00001821
Iteration 110/1000 | Loss: 0.00001821
Iteration 111/1000 | Loss: 0.00001821
Iteration 112/1000 | Loss: 0.00001821
Iteration 113/1000 | Loss: 0.00001821
Iteration 114/1000 | Loss: 0.00001821
Iteration 115/1000 | Loss: 0.00001821
Iteration 116/1000 | Loss: 0.00001820
Iteration 117/1000 | Loss: 0.00001820
Iteration 118/1000 | Loss: 0.00001820
Iteration 119/1000 | Loss: 0.00001820
Iteration 120/1000 | Loss: 0.00001820
Iteration 121/1000 | Loss: 0.00001820
Iteration 122/1000 | Loss: 0.00001820
Iteration 123/1000 | Loss: 0.00001820
Iteration 124/1000 | Loss: 0.00001820
Iteration 125/1000 | Loss: 0.00001820
Iteration 126/1000 | Loss: 0.00001820
Iteration 127/1000 | Loss: 0.00001820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.8199882106273435e-05, 1.8199882106273435e-05, 1.8199882106273435e-05, 1.8199882106273435e-05, 1.8199882106273435e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8199882106273435e-05

Optimization complete. Final v2v error: 3.7448062896728516 mm

Highest mean error: 4.19813871383667 mm for frame 70

Lowest mean error: 3.433894395828247 mm for frame 205

Saving results

Total time: 37.861801624298096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00932098
Iteration 2/25 | Loss: 0.00117834
Iteration 3/25 | Loss: 0.00103705
Iteration 4/25 | Loss: 0.00101907
Iteration 5/25 | Loss: 0.00101275
Iteration 6/25 | Loss: 0.00101101
Iteration 7/25 | Loss: 0.00101084
Iteration 8/25 | Loss: 0.00101084
Iteration 9/25 | Loss: 0.00101084
Iteration 10/25 | Loss: 0.00101084
Iteration 11/25 | Loss: 0.00101084
Iteration 12/25 | Loss: 0.00101084
Iteration 13/25 | Loss: 0.00101084
Iteration 14/25 | Loss: 0.00101084
Iteration 15/25 | Loss: 0.00101084
Iteration 16/25 | Loss: 0.00101084
Iteration 17/25 | Loss: 0.00101084
Iteration 18/25 | Loss: 0.00101084
Iteration 19/25 | Loss: 0.00101084
Iteration 20/25 | Loss: 0.00101084
Iteration 21/25 | Loss: 0.00101084
Iteration 22/25 | Loss: 0.00101084
Iteration 23/25 | Loss: 0.00101084
Iteration 24/25 | Loss: 0.00101084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00101084029302001, 0.00101084029302001, 0.00101084029302001, 0.00101084029302001, 0.00101084029302001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00101084029302001

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48490906
Iteration 2/25 | Loss: 0.00083902
Iteration 3/25 | Loss: 0.00083900
Iteration 4/25 | Loss: 0.00083900
Iteration 5/25 | Loss: 0.00083900
Iteration 6/25 | Loss: 0.00083900
Iteration 7/25 | Loss: 0.00083900
Iteration 8/25 | Loss: 0.00083900
Iteration 9/25 | Loss: 0.00083900
Iteration 10/25 | Loss: 0.00083899
Iteration 11/25 | Loss: 0.00083899
Iteration 12/25 | Loss: 0.00083899
Iteration 13/25 | Loss: 0.00083899
Iteration 14/25 | Loss: 0.00083899
Iteration 15/25 | Loss: 0.00083899
Iteration 16/25 | Loss: 0.00083899
Iteration 17/25 | Loss: 0.00083899
Iteration 18/25 | Loss: 0.00083899
Iteration 19/25 | Loss: 0.00083899
Iteration 20/25 | Loss: 0.00083899
Iteration 21/25 | Loss: 0.00083899
Iteration 22/25 | Loss: 0.00083899
Iteration 23/25 | Loss: 0.00083899
Iteration 24/25 | Loss: 0.00083899
Iteration 25/25 | Loss: 0.00083899

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083899
Iteration 2/1000 | Loss: 0.00003903
Iteration 3/1000 | Loss: 0.00003207
Iteration 4/1000 | Loss: 0.00002668
Iteration 5/1000 | Loss: 0.00002492
Iteration 6/1000 | Loss: 0.00002378
Iteration 7/1000 | Loss: 0.00002301
Iteration 8/1000 | Loss: 0.00002252
Iteration 9/1000 | Loss: 0.00002219
Iteration 10/1000 | Loss: 0.00002191
Iteration 11/1000 | Loss: 0.00002182
Iteration 12/1000 | Loss: 0.00002174
Iteration 13/1000 | Loss: 0.00002173
Iteration 14/1000 | Loss: 0.00002172
Iteration 15/1000 | Loss: 0.00002171
Iteration 16/1000 | Loss: 0.00002170
Iteration 17/1000 | Loss: 0.00002170
Iteration 18/1000 | Loss: 0.00002170
Iteration 19/1000 | Loss: 0.00002170
Iteration 20/1000 | Loss: 0.00002168
Iteration 21/1000 | Loss: 0.00002167
Iteration 22/1000 | Loss: 0.00002166
Iteration 23/1000 | Loss: 0.00002165
Iteration 24/1000 | Loss: 0.00002165
Iteration 25/1000 | Loss: 0.00002165
Iteration 26/1000 | Loss: 0.00002165
Iteration 27/1000 | Loss: 0.00002164
Iteration 28/1000 | Loss: 0.00002164
Iteration 29/1000 | Loss: 0.00002164
Iteration 30/1000 | Loss: 0.00002163
Iteration 31/1000 | Loss: 0.00002163
Iteration 32/1000 | Loss: 0.00002163
Iteration 33/1000 | Loss: 0.00002163
Iteration 34/1000 | Loss: 0.00002163
Iteration 35/1000 | Loss: 0.00002162
Iteration 36/1000 | Loss: 0.00002162
Iteration 37/1000 | Loss: 0.00002162
Iteration 38/1000 | Loss: 0.00002162
Iteration 39/1000 | Loss: 0.00002161
Iteration 40/1000 | Loss: 0.00002161
Iteration 41/1000 | Loss: 0.00002161
Iteration 42/1000 | Loss: 0.00002161
Iteration 43/1000 | Loss: 0.00002161
Iteration 44/1000 | Loss: 0.00002161
Iteration 45/1000 | Loss: 0.00002161
Iteration 46/1000 | Loss: 0.00002161
Iteration 47/1000 | Loss: 0.00002160
Iteration 48/1000 | Loss: 0.00002160
Iteration 49/1000 | Loss: 0.00002160
Iteration 50/1000 | Loss: 0.00002160
Iteration 51/1000 | Loss: 0.00002160
Iteration 52/1000 | Loss: 0.00002160
Iteration 53/1000 | Loss: 0.00002159
Iteration 54/1000 | Loss: 0.00002159
Iteration 55/1000 | Loss: 0.00002159
Iteration 56/1000 | Loss: 0.00002159
Iteration 57/1000 | Loss: 0.00002159
Iteration 58/1000 | Loss: 0.00002159
Iteration 59/1000 | Loss: 0.00002159
Iteration 60/1000 | Loss: 0.00002159
Iteration 61/1000 | Loss: 0.00002159
Iteration 62/1000 | Loss: 0.00002159
Iteration 63/1000 | Loss: 0.00002159
Iteration 64/1000 | Loss: 0.00002159
Iteration 65/1000 | Loss: 0.00002159
Iteration 66/1000 | Loss: 0.00002159
Iteration 67/1000 | Loss: 0.00002159
Iteration 68/1000 | Loss: 0.00002159
Iteration 69/1000 | Loss: 0.00002159
Iteration 70/1000 | Loss: 0.00002159
Iteration 71/1000 | Loss: 0.00002159
Iteration 72/1000 | Loss: 0.00002159
Iteration 73/1000 | Loss: 0.00002159
Iteration 74/1000 | Loss: 0.00002159
Iteration 75/1000 | Loss: 0.00002159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.1591515178442933e-05, 2.1591515178442933e-05, 2.1591515178442933e-05, 2.1591515178442933e-05, 2.1591515178442933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1591515178442933e-05

Optimization complete. Final v2v error: 4.066442489624023 mm

Highest mean error: 4.233481407165527 mm for frame 25

Lowest mean error: 3.8657753467559814 mm for frame 12

Saving results

Total time: 29.39412808418274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01108091
Iteration 2/25 | Loss: 0.00293199
Iteration 3/25 | Loss: 0.00131701
Iteration 4/25 | Loss: 0.00112659
Iteration 5/25 | Loss: 0.00106800
Iteration 6/25 | Loss: 0.00107995
Iteration 7/25 | Loss: 0.00109535
Iteration 8/25 | Loss: 0.00107178
Iteration 9/25 | Loss: 0.00105865
Iteration 10/25 | Loss: 0.00104396
Iteration 11/25 | Loss: 0.00104545
Iteration 12/25 | Loss: 0.00104010
Iteration 13/25 | Loss: 0.00103614
Iteration 14/25 | Loss: 0.00103832
Iteration 15/25 | Loss: 0.00103296
Iteration 16/25 | Loss: 0.00103256
Iteration 17/25 | Loss: 0.00103137
Iteration 18/25 | Loss: 0.00102931
Iteration 19/25 | Loss: 0.00102889
Iteration 20/25 | Loss: 0.00102308
Iteration 21/25 | Loss: 0.00102191
Iteration 22/25 | Loss: 0.00102527
Iteration 23/25 | Loss: 0.00102761
Iteration 24/25 | Loss: 0.00102474
Iteration 25/25 | Loss: 0.00103046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55060339
Iteration 2/25 | Loss: 0.00089137
Iteration 3/25 | Loss: 0.00089137
Iteration 4/25 | Loss: 0.00089137
Iteration 5/25 | Loss: 0.00089137
Iteration 6/25 | Loss: 0.00089137
Iteration 7/25 | Loss: 0.00089137
Iteration 8/25 | Loss: 0.00089137
Iteration 9/25 | Loss: 0.00089137
Iteration 10/25 | Loss: 0.00089137
Iteration 11/25 | Loss: 0.00089137
Iteration 12/25 | Loss: 0.00089137
Iteration 13/25 | Loss: 0.00089137
Iteration 14/25 | Loss: 0.00089137
Iteration 15/25 | Loss: 0.00089137
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008913676720112562, 0.0008913676720112562, 0.0008913676720112562, 0.0008913676720112562, 0.0008913676720112562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008913676720112562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089137
Iteration 2/1000 | Loss: 0.00013159
Iteration 3/1000 | Loss: 0.00005140
Iteration 4/1000 | Loss: 0.00017514
Iteration 5/1000 | Loss: 0.00037485
Iteration 6/1000 | Loss: 0.00011583
Iteration 7/1000 | Loss: 0.00006659
Iteration 8/1000 | Loss: 0.00013347
Iteration 9/1000 | Loss: 0.00007705
Iteration 10/1000 | Loss: 0.00012647
Iteration 11/1000 | Loss: 0.00004660
Iteration 12/1000 | Loss: 0.00005976
Iteration 13/1000 | Loss: 0.00008598
Iteration 14/1000 | Loss: 0.00009380
Iteration 15/1000 | Loss: 0.00009672
Iteration 16/1000 | Loss: 0.00020979
Iteration 17/1000 | Loss: 0.00006389
Iteration 18/1000 | Loss: 0.00003181
Iteration 19/1000 | Loss: 0.00003042
Iteration 20/1000 | Loss: 0.00002962
Iteration 21/1000 | Loss: 0.00022239
Iteration 22/1000 | Loss: 0.00004457
Iteration 23/1000 | Loss: 0.00011665
Iteration 24/1000 | Loss: 0.00021087
Iteration 25/1000 | Loss: 0.00026884
Iteration 26/1000 | Loss: 0.00003393
Iteration 27/1000 | Loss: 0.00003062
Iteration 28/1000 | Loss: 0.00002917
Iteration 29/1000 | Loss: 0.00002806
Iteration 30/1000 | Loss: 0.00002746
Iteration 31/1000 | Loss: 0.00002710
Iteration 32/1000 | Loss: 0.00003834
Iteration 33/1000 | Loss: 0.00002815
Iteration 34/1000 | Loss: 0.00002675
Iteration 35/1000 | Loss: 0.00002610
Iteration 36/1000 | Loss: 0.00002583
Iteration 37/1000 | Loss: 0.00002577
Iteration 38/1000 | Loss: 0.00002575
Iteration 39/1000 | Loss: 0.00002574
Iteration 40/1000 | Loss: 0.00002573
Iteration 41/1000 | Loss: 0.00002571
Iteration 42/1000 | Loss: 0.00002570
Iteration 43/1000 | Loss: 0.00002556
Iteration 44/1000 | Loss: 0.00002555
Iteration 45/1000 | Loss: 0.00002555
Iteration 46/1000 | Loss: 0.00002552
Iteration 47/1000 | Loss: 0.00002552
Iteration 48/1000 | Loss: 0.00002549
Iteration 49/1000 | Loss: 0.00002548
Iteration 50/1000 | Loss: 0.00002548
Iteration 51/1000 | Loss: 0.00002548
Iteration 52/1000 | Loss: 0.00002547
Iteration 53/1000 | Loss: 0.00002547
Iteration 54/1000 | Loss: 0.00002547
Iteration 55/1000 | Loss: 0.00002547
Iteration 56/1000 | Loss: 0.00002547
Iteration 57/1000 | Loss: 0.00002546
Iteration 58/1000 | Loss: 0.00002546
Iteration 59/1000 | Loss: 0.00002546
Iteration 60/1000 | Loss: 0.00002546
Iteration 61/1000 | Loss: 0.00002546
Iteration 62/1000 | Loss: 0.00002546
Iteration 63/1000 | Loss: 0.00002546
Iteration 64/1000 | Loss: 0.00002546
Iteration 65/1000 | Loss: 0.00002546
Iteration 66/1000 | Loss: 0.00002546
Iteration 67/1000 | Loss: 0.00002545
Iteration 68/1000 | Loss: 0.00002545
Iteration 69/1000 | Loss: 0.00002545
Iteration 70/1000 | Loss: 0.00002545
Iteration 71/1000 | Loss: 0.00002545
Iteration 72/1000 | Loss: 0.00002545
Iteration 73/1000 | Loss: 0.00002545
Iteration 74/1000 | Loss: 0.00002545
Iteration 75/1000 | Loss: 0.00002544
Iteration 76/1000 | Loss: 0.00002544
Iteration 77/1000 | Loss: 0.00002544
Iteration 78/1000 | Loss: 0.00002544
Iteration 79/1000 | Loss: 0.00002544
Iteration 80/1000 | Loss: 0.00002544
Iteration 81/1000 | Loss: 0.00002544
Iteration 82/1000 | Loss: 0.00002544
Iteration 83/1000 | Loss: 0.00002544
Iteration 84/1000 | Loss: 0.00002544
Iteration 85/1000 | Loss: 0.00002544
Iteration 86/1000 | Loss: 0.00002544
Iteration 87/1000 | Loss: 0.00002544
Iteration 88/1000 | Loss: 0.00002544
Iteration 89/1000 | Loss: 0.00002543
Iteration 90/1000 | Loss: 0.00002543
Iteration 91/1000 | Loss: 0.00002543
Iteration 92/1000 | Loss: 0.00002543
Iteration 93/1000 | Loss: 0.00002543
Iteration 94/1000 | Loss: 0.00002543
Iteration 95/1000 | Loss: 0.00002543
Iteration 96/1000 | Loss: 0.00002542
Iteration 97/1000 | Loss: 0.00002542
Iteration 98/1000 | Loss: 0.00002542
Iteration 99/1000 | Loss: 0.00002542
Iteration 100/1000 | Loss: 0.00002542
Iteration 101/1000 | Loss: 0.00002542
Iteration 102/1000 | Loss: 0.00002542
Iteration 103/1000 | Loss: 0.00002542
Iteration 104/1000 | Loss: 0.00002542
Iteration 105/1000 | Loss: 0.00002542
Iteration 106/1000 | Loss: 0.00002542
Iteration 107/1000 | Loss: 0.00002542
Iteration 108/1000 | Loss: 0.00002542
Iteration 109/1000 | Loss: 0.00002542
Iteration 110/1000 | Loss: 0.00002542
Iteration 111/1000 | Loss: 0.00002542
Iteration 112/1000 | Loss: 0.00002542
Iteration 113/1000 | Loss: 0.00002542
Iteration 114/1000 | Loss: 0.00002542
Iteration 115/1000 | Loss: 0.00002541
Iteration 116/1000 | Loss: 0.00002541
Iteration 117/1000 | Loss: 0.00002541
Iteration 118/1000 | Loss: 0.00002541
Iteration 119/1000 | Loss: 0.00002541
Iteration 120/1000 | Loss: 0.00002541
Iteration 121/1000 | Loss: 0.00002541
Iteration 122/1000 | Loss: 0.00002541
Iteration 123/1000 | Loss: 0.00002540
Iteration 124/1000 | Loss: 0.00002540
Iteration 125/1000 | Loss: 0.00002540
Iteration 126/1000 | Loss: 0.00002540
Iteration 127/1000 | Loss: 0.00002540
Iteration 128/1000 | Loss: 0.00002540
Iteration 129/1000 | Loss: 0.00002540
Iteration 130/1000 | Loss: 0.00002540
Iteration 131/1000 | Loss: 0.00002540
Iteration 132/1000 | Loss: 0.00002539
Iteration 133/1000 | Loss: 0.00002539
Iteration 134/1000 | Loss: 0.00002539
Iteration 135/1000 | Loss: 0.00002539
Iteration 136/1000 | Loss: 0.00002539
Iteration 137/1000 | Loss: 0.00002539
Iteration 138/1000 | Loss: 0.00002539
Iteration 139/1000 | Loss: 0.00002539
Iteration 140/1000 | Loss: 0.00002539
Iteration 141/1000 | Loss: 0.00002539
Iteration 142/1000 | Loss: 0.00002539
Iteration 143/1000 | Loss: 0.00002539
Iteration 144/1000 | Loss: 0.00002539
Iteration 145/1000 | Loss: 0.00002539
Iteration 146/1000 | Loss: 0.00002539
Iteration 147/1000 | Loss: 0.00002539
Iteration 148/1000 | Loss: 0.00002539
Iteration 149/1000 | Loss: 0.00002539
Iteration 150/1000 | Loss: 0.00002539
Iteration 151/1000 | Loss: 0.00002539
Iteration 152/1000 | Loss: 0.00002539
Iteration 153/1000 | Loss: 0.00002539
Iteration 154/1000 | Loss: 0.00002539
Iteration 155/1000 | Loss: 0.00002539
Iteration 156/1000 | Loss: 0.00002539
Iteration 157/1000 | Loss: 0.00002539
Iteration 158/1000 | Loss: 0.00002539
Iteration 159/1000 | Loss: 0.00002539
Iteration 160/1000 | Loss: 0.00002539
Iteration 161/1000 | Loss: 0.00002539
Iteration 162/1000 | Loss: 0.00002539
Iteration 163/1000 | Loss: 0.00002539
Iteration 164/1000 | Loss: 0.00002539
Iteration 165/1000 | Loss: 0.00002539
Iteration 166/1000 | Loss: 0.00002539
Iteration 167/1000 | Loss: 0.00002539
Iteration 168/1000 | Loss: 0.00002539
Iteration 169/1000 | Loss: 0.00002539
Iteration 170/1000 | Loss: 0.00002539
Iteration 171/1000 | Loss: 0.00002539
Iteration 172/1000 | Loss: 0.00002539
Iteration 173/1000 | Loss: 0.00002539
Iteration 174/1000 | Loss: 0.00002539
Iteration 175/1000 | Loss: 0.00002539
Iteration 176/1000 | Loss: 0.00002539
Iteration 177/1000 | Loss: 0.00002539
Iteration 178/1000 | Loss: 0.00002539
Iteration 179/1000 | Loss: 0.00002539
Iteration 180/1000 | Loss: 0.00002539
Iteration 181/1000 | Loss: 0.00002539
Iteration 182/1000 | Loss: 0.00002539
Iteration 183/1000 | Loss: 0.00002539
Iteration 184/1000 | Loss: 0.00002539
Iteration 185/1000 | Loss: 0.00002539
Iteration 186/1000 | Loss: 0.00002539
Iteration 187/1000 | Loss: 0.00002539
Iteration 188/1000 | Loss: 0.00002539
Iteration 189/1000 | Loss: 0.00002539
Iteration 190/1000 | Loss: 0.00002539
Iteration 191/1000 | Loss: 0.00002539
Iteration 192/1000 | Loss: 0.00002539
Iteration 193/1000 | Loss: 0.00002539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [2.5387565983692184e-05, 2.5387565983692184e-05, 2.5387565983692184e-05, 2.5387565983692184e-05, 2.5387565983692184e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5387565983692184e-05

Optimization complete. Final v2v error: 4.28538703918457 mm

Highest mean error: 6.982213497161865 mm for frame 92

Lowest mean error: 3.8633837699890137 mm for frame 209

Saving results

Total time: 123.61412954330444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802081
Iteration 2/25 | Loss: 0.00167533
Iteration 3/25 | Loss: 0.00113030
Iteration 4/25 | Loss: 0.00105889
Iteration 5/25 | Loss: 0.00105411
Iteration 6/25 | Loss: 0.00103859
Iteration 7/25 | Loss: 0.00104012
Iteration 8/25 | Loss: 0.00104410
Iteration 9/25 | Loss: 0.00104224
Iteration 10/25 | Loss: 0.00103634
Iteration 11/25 | Loss: 0.00103432
Iteration 12/25 | Loss: 0.00103370
Iteration 13/25 | Loss: 0.00103365
Iteration 14/25 | Loss: 0.00103365
Iteration 15/25 | Loss: 0.00103365
Iteration 16/25 | Loss: 0.00103365
Iteration 17/25 | Loss: 0.00103365
Iteration 18/25 | Loss: 0.00103365
Iteration 19/25 | Loss: 0.00103365
Iteration 20/25 | Loss: 0.00103365
Iteration 21/25 | Loss: 0.00103365
Iteration 22/25 | Loss: 0.00103365
Iteration 23/25 | Loss: 0.00103365
Iteration 24/25 | Loss: 0.00103365
Iteration 25/25 | Loss: 0.00103365

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.13304901
Iteration 2/25 | Loss: 0.00091018
Iteration 3/25 | Loss: 0.00091015
Iteration 4/25 | Loss: 0.00091015
Iteration 5/25 | Loss: 0.00091015
Iteration 6/25 | Loss: 0.00091015
Iteration 7/25 | Loss: 0.00091015
Iteration 8/25 | Loss: 0.00091015
Iteration 9/25 | Loss: 0.00091015
Iteration 10/25 | Loss: 0.00091015
Iteration 11/25 | Loss: 0.00091015
Iteration 12/25 | Loss: 0.00091015
Iteration 13/25 | Loss: 0.00091015
Iteration 14/25 | Loss: 0.00091015
Iteration 15/25 | Loss: 0.00091015
Iteration 16/25 | Loss: 0.00091015
Iteration 17/25 | Loss: 0.00091015
Iteration 18/25 | Loss: 0.00091015
Iteration 19/25 | Loss: 0.00091015
Iteration 20/25 | Loss: 0.00091015
Iteration 21/25 | Loss: 0.00091015
Iteration 22/25 | Loss: 0.00091015
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009101470932364464, 0.0009101470932364464, 0.0009101470932364464, 0.0009101470932364464, 0.0009101470932364464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009101470932364464

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091015
Iteration 2/1000 | Loss: 0.00004477
Iteration 3/1000 | Loss: 0.00003692
Iteration 4/1000 | Loss: 0.00003429
Iteration 5/1000 | Loss: 0.00003291
Iteration 6/1000 | Loss: 0.00003214
Iteration 7/1000 | Loss: 0.00003161
Iteration 8/1000 | Loss: 0.00003136
Iteration 9/1000 | Loss: 0.00003120
Iteration 10/1000 | Loss: 0.00003104
Iteration 11/1000 | Loss: 0.00003092
Iteration 12/1000 | Loss: 0.00003089
Iteration 13/1000 | Loss: 0.00003089
Iteration 14/1000 | Loss: 0.00003088
Iteration 15/1000 | Loss: 0.00003088
Iteration 16/1000 | Loss: 0.00003087
Iteration 17/1000 | Loss: 0.00003086
Iteration 18/1000 | Loss: 0.00003086
Iteration 19/1000 | Loss: 0.00003086
Iteration 20/1000 | Loss: 0.00003086
Iteration 21/1000 | Loss: 0.00003086
Iteration 22/1000 | Loss: 0.00003086
Iteration 23/1000 | Loss: 0.00003085
Iteration 24/1000 | Loss: 0.00003085
Iteration 25/1000 | Loss: 0.00003085
Iteration 26/1000 | Loss: 0.00003085
Iteration 27/1000 | Loss: 0.00003085
Iteration 28/1000 | Loss: 0.00003085
Iteration 29/1000 | Loss: 0.00003085
Iteration 30/1000 | Loss: 0.00003085
Iteration 31/1000 | Loss: 0.00003085
Iteration 32/1000 | Loss: 0.00003085
Iteration 33/1000 | Loss: 0.00003085
Iteration 34/1000 | Loss: 0.00003084
Iteration 35/1000 | Loss: 0.00003083
Iteration 36/1000 | Loss: 0.00003083
Iteration 37/1000 | Loss: 0.00003083
Iteration 38/1000 | Loss: 0.00003083
Iteration 39/1000 | Loss: 0.00003083
Iteration 40/1000 | Loss: 0.00003083
Iteration 41/1000 | Loss: 0.00003083
Iteration 42/1000 | Loss: 0.00003082
Iteration 43/1000 | Loss: 0.00003082
Iteration 44/1000 | Loss: 0.00003082
Iteration 45/1000 | Loss: 0.00003082
Iteration 46/1000 | Loss: 0.00003082
Iteration 47/1000 | Loss: 0.00003082
Iteration 48/1000 | Loss: 0.00003082
Iteration 49/1000 | Loss: 0.00003082
Iteration 50/1000 | Loss: 0.00003081
Iteration 51/1000 | Loss: 0.00003081
Iteration 52/1000 | Loss: 0.00003081
Iteration 53/1000 | Loss: 0.00003081
Iteration 54/1000 | Loss: 0.00003081
Iteration 55/1000 | Loss: 0.00003081
Iteration 56/1000 | Loss: 0.00003081
Iteration 57/1000 | Loss: 0.00003080
Iteration 58/1000 | Loss: 0.00003080
Iteration 59/1000 | Loss: 0.00003080
Iteration 60/1000 | Loss: 0.00003080
Iteration 61/1000 | Loss: 0.00003080
Iteration 62/1000 | Loss: 0.00003080
Iteration 63/1000 | Loss: 0.00003080
Iteration 64/1000 | Loss: 0.00003080
Iteration 65/1000 | Loss: 0.00003080
Iteration 66/1000 | Loss: 0.00003080
Iteration 67/1000 | Loss: 0.00003080
Iteration 68/1000 | Loss: 0.00003080
Iteration 69/1000 | Loss: 0.00003080
Iteration 70/1000 | Loss: 0.00003080
Iteration 71/1000 | Loss: 0.00003080
Iteration 72/1000 | Loss: 0.00003080
Iteration 73/1000 | Loss: 0.00003079
Iteration 74/1000 | Loss: 0.00003079
Iteration 75/1000 | Loss: 0.00003079
Iteration 76/1000 | Loss: 0.00003079
Iteration 77/1000 | Loss: 0.00003079
Iteration 78/1000 | Loss: 0.00003079
Iteration 79/1000 | Loss: 0.00003079
Iteration 80/1000 | Loss: 0.00003079
Iteration 81/1000 | Loss: 0.00003079
Iteration 82/1000 | Loss: 0.00003078
Iteration 83/1000 | Loss: 0.00003078
Iteration 84/1000 | Loss: 0.00003078
Iteration 85/1000 | Loss: 0.00003078
Iteration 86/1000 | Loss: 0.00003078
Iteration 87/1000 | Loss: 0.00003078
Iteration 88/1000 | Loss: 0.00003078
Iteration 89/1000 | Loss: 0.00003078
Iteration 90/1000 | Loss: 0.00003078
Iteration 91/1000 | Loss: 0.00003078
Iteration 92/1000 | Loss: 0.00003078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [3.0781237001065165e-05, 3.0781237001065165e-05, 3.0781237001065165e-05, 3.0781237001065165e-05, 3.0781237001065165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0781237001065165e-05

Optimization complete. Final v2v error: 4.824091911315918 mm

Highest mean error: 5.255942344665527 mm for frame 12

Lowest mean error: 4.3233184814453125 mm for frame 1

Saving results

Total time: 47.75710892677307
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00716433
Iteration 2/25 | Loss: 0.00125533
Iteration 3/25 | Loss: 0.00113140
Iteration 4/25 | Loss: 0.00109503
Iteration 5/25 | Loss: 0.00108482
Iteration 6/25 | Loss: 0.00108252
Iteration 7/25 | Loss: 0.00108140
Iteration 8/25 | Loss: 0.00108140
Iteration 9/25 | Loss: 0.00108140
Iteration 10/25 | Loss: 0.00108140
Iteration 11/25 | Loss: 0.00108140
Iteration 12/25 | Loss: 0.00108140
Iteration 13/25 | Loss: 0.00108140
Iteration 14/25 | Loss: 0.00108140
Iteration 15/25 | Loss: 0.00108140
Iteration 16/25 | Loss: 0.00108140
Iteration 17/25 | Loss: 0.00108140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010813985718414187, 0.0010813985718414187, 0.0010813985718414187, 0.0010813985718414187, 0.0010813985718414187]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010813985718414187

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48225296
Iteration 2/25 | Loss: 0.00093381
Iteration 3/25 | Loss: 0.00093380
Iteration 4/25 | Loss: 0.00093380
Iteration 5/25 | Loss: 0.00093380
Iteration 6/25 | Loss: 0.00093380
Iteration 7/25 | Loss: 0.00093380
Iteration 8/25 | Loss: 0.00093380
Iteration 9/25 | Loss: 0.00093380
Iteration 10/25 | Loss: 0.00093380
Iteration 11/25 | Loss: 0.00093380
Iteration 12/25 | Loss: 0.00093380
Iteration 13/25 | Loss: 0.00093380
Iteration 14/25 | Loss: 0.00093380
Iteration 15/25 | Loss: 0.00093380
Iteration 16/25 | Loss: 0.00093380
Iteration 17/25 | Loss: 0.00093380
Iteration 18/25 | Loss: 0.00093380
Iteration 19/25 | Loss: 0.00093380
Iteration 20/25 | Loss: 0.00093380
Iteration 21/25 | Loss: 0.00093380
Iteration 22/25 | Loss: 0.00093380
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009337987285107374, 0.0009337987285107374, 0.0009337987285107374, 0.0009337987285107374, 0.0009337987285107374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009337987285107374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093380
Iteration 2/1000 | Loss: 0.00006687
Iteration 3/1000 | Loss: 0.00004973
Iteration 4/1000 | Loss: 0.00004601
Iteration 5/1000 | Loss: 0.00004406
Iteration 6/1000 | Loss: 0.00004246
Iteration 7/1000 | Loss: 0.00004122
Iteration 8/1000 | Loss: 0.00004028
Iteration 9/1000 | Loss: 0.00003977
Iteration 10/1000 | Loss: 0.00003942
Iteration 11/1000 | Loss: 0.00003918
Iteration 12/1000 | Loss: 0.00003910
Iteration 13/1000 | Loss: 0.00003907
Iteration 14/1000 | Loss: 0.00003906
Iteration 15/1000 | Loss: 0.00003906
Iteration 16/1000 | Loss: 0.00003905
Iteration 17/1000 | Loss: 0.00003905
Iteration 18/1000 | Loss: 0.00003905
Iteration 19/1000 | Loss: 0.00003904
Iteration 20/1000 | Loss: 0.00003904
Iteration 21/1000 | Loss: 0.00003904
Iteration 22/1000 | Loss: 0.00003903
Iteration 23/1000 | Loss: 0.00003903
Iteration 24/1000 | Loss: 0.00003903
Iteration 25/1000 | Loss: 0.00003902
Iteration 26/1000 | Loss: 0.00003902
Iteration 27/1000 | Loss: 0.00003902
Iteration 28/1000 | Loss: 0.00003901
Iteration 29/1000 | Loss: 0.00003901
Iteration 30/1000 | Loss: 0.00003900
Iteration 31/1000 | Loss: 0.00003900
Iteration 32/1000 | Loss: 0.00003899
Iteration 33/1000 | Loss: 0.00003899
Iteration 34/1000 | Loss: 0.00003899
Iteration 35/1000 | Loss: 0.00003898
Iteration 36/1000 | Loss: 0.00003898
Iteration 37/1000 | Loss: 0.00003897
Iteration 38/1000 | Loss: 0.00003897
Iteration 39/1000 | Loss: 0.00003897
Iteration 40/1000 | Loss: 0.00003896
Iteration 41/1000 | Loss: 0.00003896
Iteration 42/1000 | Loss: 0.00003896
Iteration 43/1000 | Loss: 0.00003895
Iteration 44/1000 | Loss: 0.00003895
Iteration 45/1000 | Loss: 0.00003895
Iteration 46/1000 | Loss: 0.00003895
Iteration 47/1000 | Loss: 0.00003895
Iteration 48/1000 | Loss: 0.00003895
Iteration 49/1000 | Loss: 0.00003894
Iteration 50/1000 | Loss: 0.00003894
Iteration 51/1000 | Loss: 0.00003894
Iteration 52/1000 | Loss: 0.00003894
Iteration 53/1000 | Loss: 0.00003893
Iteration 54/1000 | Loss: 0.00003893
Iteration 55/1000 | Loss: 0.00003893
Iteration 56/1000 | Loss: 0.00003893
Iteration 57/1000 | Loss: 0.00003893
Iteration 58/1000 | Loss: 0.00003893
Iteration 59/1000 | Loss: 0.00003893
Iteration 60/1000 | Loss: 0.00003893
Iteration 61/1000 | Loss: 0.00003892
Iteration 62/1000 | Loss: 0.00003892
Iteration 63/1000 | Loss: 0.00003892
Iteration 64/1000 | Loss: 0.00003891
Iteration 65/1000 | Loss: 0.00003891
Iteration 66/1000 | Loss: 0.00003891
Iteration 67/1000 | Loss: 0.00003890
Iteration 68/1000 | Loss: 0.00003889
Iteration 69/1000 | Loss: 0.00003889
Iteration 70/1000 | Loss: 0.00003889
Iteration 71/1000 | Loss: 0.00003888
Iteration 72/1000 | Loss: 0.00003888
Iteration 73/1000 | Loss: 0.00003888
Iteration 74/1000 | Loss: 0.00003888
Iteration 75/1000 | Loss: 0.00003887
Iteration 76/1000 | Loss: 0.00003887
Iteration 77/1000 | Loss: 0.00003887
Iteration 78/1000 | Loss: 0.00003886
Iteration 79/1000 | Loss: 0.00003886
Iteration 80/1000 | Loss: 0.00003886
Iteration 81/1000 | Loss: 0.00003886
Iteration 82/1000 | Loss: 0.00003886
Iteration 83/1000 | Loss: 0.00003886
Iteration 84/1000 | Loss: 0.00003886
Iteration 85/1000 | Loss: 0.00003886
Iteration 86/1000 | Loss: 0.00003886
Iteration 87/1000 | Loss: 0.00003886
Iteration 88/1000 | Loss: 0.00003886
Iteration 89/1000 | Loss: 0.00003886
Iteration 90/1000 | Loss: 0.00003886
Iteration 91/1000 | Loss: 0.00003886
Iteration 92/1000 | Loss: 0.00003886
Iteration 93/1000 | Loss: 0.00003886
Iteration 94/1000 | Loss: 0.00003886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [3.886039485223591e-05, 3.886039485223591e-05, 3.886039485223591e-05, 3.886039485223591e-05, 3.886039485223591e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.886039485223591e-05

Optimization complete. Final v2v error: 5.2391581535339355 mm

Highest mean error: 5.481532573699951 mm for frame 122

Lowest mean error: 4.906242370605469 mm for frame 228

Saving results

Total time: 37.5272102355957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00534371
Iteration 2/25 | Loss: 0.00134634
Iteration 3/25 | Loss: 0.00111327
Iteration 4/25 | Loss: 0.00107619
Iteration 5/25 | Loss: 0.00106874
Iteration 6/25 | Loss: 0.00106731
Iteration 7/25 | Loss: 0.00106725
Iteration 8/25 | Loss: 0.00106725
Iteration 9/25 | Loss: 0.00106725
Iteration 10/25 | Loss: 0.00106725
Iteration 11/25 | Loss: 0.00106725
Iteration 12/25 | Loss: 0.00106725
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001067249570041895, 0.001067249570041895, 0.001067249570041895, 0.001067249570041895, 0.001067249570041895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001067249570041895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49640417
Iteration 2/25 | Loss: 0.00097468
Iteration 3/25 | Loss: 0.00097467
Iteration 4/25 | Loss: 0.00097467
Iteration 5/25 | Loss: 0.00097467
Iteration 6/25 | Loss: 0.00097467
Iteration 7/25 | Loss: 0.00097467
Iteration 8/25 | Loss: 0.00097467
Iteration 9/25 | Loss: 0.00097467
Iteration 10/25 | Loss: 0.00097467
Iteration 11/25 | Loss: 0.00097467
Iteration 12/25 | Loss: 0.00097467
Iteration 13/25 | Loss: 0.00097467
Iteration 14/25 | Loss: 0.00097467
Iteration 15/25 | Loss: 0.00097467
Iteration 16/25 | Loss: 0.00097467
Iteration 17/25 | Loss: 0.00097467
Iteration 18/25 | Loss: 0.00097467
Iteration 19/25 | Loss: 0.00097467
Iteration 20/25 | Loss: 0.00097467
Iteration 21/25 | Loss: 0.00097467
Iteration 22/25 | Loss: 0.00097467
Iteration 23/25 | Loss: 0.00097467
Iteration 24/25 | Loss: 0.00097467
Iteration 25/25 | Loss: 0.00097467

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097467
Iteration 2/1000 | Loss: 0.00006022
Iteration 3/1000 | Loss: 0.00004342
Iteration 4/1000 | Loss: 0.00004010
Iteration 5/1000 | Loss: 0.00003809
Iteration 6/1000 | Loss: 0.00003694
Iteration 7/1000 | Loss: 0.00003616
Iteration 8/1000 | Loss: 0.00003551
Iteration 9/1000 | Loss: 0.00003515
Iteration 10/1000 | Loss: 0.00003495
Iteration 11/1000 | Loss: 0.00003485
Iteration 12/1000 | Loss: 0.00003480
Iteration 13/1000 | Loss: 0.00003477
Iteration 14/1000 | Loss: 0.00003477
Iteration 15/1000 | Loss: 0.00003476
Iteration 16/1000 | Loss: 0.00003475
Iteration 17/1000 | Loss: 0.00003474
Iteration 18/1000 | Loss: 0.00003473
Iteration 19/1000 | Loss: 0.00003473
Iteration 20/1000 | Loss: 0.00003473
Iteration 21/1000 | Loss: 0.00003472
Iteration 22/1000 | Loss: 0.00003472
Iteration 23/1000 | Loss: 0.00003472
Iteration 24/1000 | Loss: 0.00003472
Iteration 25/1000 | Loss: 0.00003471
Iteration 26/1000 | Loss: 0.00003471
Iteration 27/1000 | Loss: 0.00003469
Iteration 28/1000 | Loss: 0.00003469
Iteration 29/1000 | Loss: 0.00003469
Iteration 30/1000 | Loss: 0.00003467
Iteration 31/1000 | Loss: 0.00003466
Iteration 32/1000 | Loss: 0.00003466
Iteration 33/1000 | Loss: 0.00003466
Iteration 34/1000 | Loss: 0.00003466
Iteration 35/1000 | Loss: 0.00003465
Iteration 36/1000 | Loss: 0.00003465
Iteration 37/1000 | Loss: 0.00003464
Iteration 38/1000 | Loss: 0.00003464
Iteration 39/1000 | Loss: 0.00003463
Iteration 40/1000 | Loss: 0.00003463
Iteration 41/1000 | Loss: 0.00003463
Iteration 42/1000 | Loss: 0.00003462
Iteration 43/1000 | Loss: 0.00003462
Iteration 44/1000 | Loss: 0.00003462
Iteration 45/1000 | Loss: 0.00003462
Iteration 46/1000 | Loss: 0.00003462
Iteration 47/1000 | Loss: 0.00003462
Iteration 48/1000 | Loss: 0.00003462
Iteration 49/1000 | Loss: 0.00003462
Iteration 50/1000 | Loss: 0.00003462
Iteration 51/1000 | Loss: 0.00003462
Iteration 52/1000 | Loss: 0.00003462
Iteration 53/1000 | Loss: 0.00003462
Iteration 54/1000 | Loss: 0.00003462
Iteration 55/1000 | Loss: 0.00003462
Iteration 56/1000 | Loss: 0.00003461
Iteration 57/1000 | Loss: 0.00003461
Iteration 58/1000 | Loss: 0.00003461
Iteration 59/1000 | Loss: 0.00003461
Iteration 60/1000 | Loss: 0.00003460
Iteration 61/1000 | Loss: 0.00003460
Iteration 62/1000 | Loss: 0.00003460
Iteration 63/1000 | Loss: 0.00003459
Iteration 64/1000 | Loss: 0.00003459
Iteration 65/1000 | Loss: 0.00003459
Iteration 66/1000 | Loss: 0.00003459
Iteration 67/1000 | Loss: 0.00003459
Iteration 68/1000 | Loss: 0.00003459
Iteration 69/1000 | Loss: 0.00003459
Iteration 70/1000 | Loss: 0.00003459
Iteration 71/1000 | Loss: 0.00003459
Iteration 72/1000 | Loss: 0.00003459
Iteration 73/1000 | Loss: 0.00003458
Iteration 74/1000 | Loss: 0.00003457
Iteration 75/1000 | Loss: 0.00003456
Iteration 76/1000 | Loss: 0.00003456
Iteration 77/1000 | Loss: 0.00003455
Iteration 78/1000 | Loss: 0.00003455
Iteration 79/1000 | Loss: 0.00003455
Iteration 80/1000 | Loss: 0.00003455
Iteration 81/1000 | Loss: 0.00003455
Iteration 82/1000 | Loss: 0.00003455
Iteration 83/1000 | Loss: 0.00003455
Iteration 84/1000 | Loss: 0.00003455
Iteration 85/1000 | Loss: 0.00003455
Iteration 86/1000 | Loss: 0.00003455
Iteration 87/1000 | Loss: 0.00003455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [3.454517718637362e-05, 3.454517718637362e-05, 3.454517718637362e-05, 3.454517718637362e-05, 3.454517718637362e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.454517718637362e-05

Optimization complete. Final v2v error: 4.8878865242004395 mm

Highest mean error: 5.549788475036621 mm for frame 197

Lowest mean error: 4.277460098266602 mm for frame 226

Saving results

Total time: 35.76586675643921
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039515
Iteration 2/25 | Loss: 0.00205696
Iteration 3/25 | Loss: 0.00121518
Iteration 4/25 | Loss: 0.00117393
Iteration 5/25 | Loss: 0.00116181
Iteration 6/25 | Loss: 0.00115820
Iteration 7/25 | Loss: 0.00115715
Iteration 8/25 | Loss: 0.00115691
Iteration 9/25 | Loss: 0.00115691
Iteration 10/25 | Loss: 0.00115691
Iteration 11/25 | Loss: 0.00115691
Iteration 12/25 | Loss: 0.00115691
Iteration 13/25 | Loss: 0.00115691
Iteration 14/25 | Loss: 0.00115691
Iteration 15/25 | Loss: 0.00115691
Iteration 16/25 | Loss: 0.00115691
Iteration 17/25 | Loss: 0.00115691
Iteration 18/25 | Loss: 0.00115691
Iteration 19/25 | Loss: 0.00115691
Iteration 20/25 | Loss: 0.00115691
Iteration 21/25 | Loss: 0.00115691
Iteration 22/25 | Loss: 0.00115691
Iteration 23/25 | Loss: 0.00115691
Iteration 24/25 | Loss: 0.00115691
Iteration 25/25 | Loss: 0.00115691

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64633965
Iteration 2/25 | Loss: 0.00064282
Iteration 3/25 | Loss: 0.00064282
Iteration 4/25 | Loss: 0.00064282
Iteration 5/25 | Loss: 0.00064282
Iteration 6/25 | Loss: 0.00064282
Iteration 7/25 | Loss: 0.00064282
Iteration 8/25 | Loss: 0.00064282
Iteration 9/25 | Loss: 0.00064282
Iteration 10/25 | Loss: 0.00064282
Iteration 11/25 | Loss: 0.00064282
Iteration 12/25 | Loss: 0.00064282
Iteration 13/25 | Loss: 0.00064282
Iteration 14/25 | Loss: 0.00064282
Iteration 15/25 | Loss: 0.00064282
Iteration 16/25 | Loss: 0.00064282
Iteration 17/25 | Loss: 0.00064282
Iteration 18/25 | Loss: 0.00064282
Iteration 19/25 | Loss: 0.00064282
Iteration 20/25 | Loss: 0.00064282
Iteration 21/25 | Loss: 0.00064282
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006428172928281128, 0.0006428172928281128, 0.0006428172928281128, 0.0006428172928281128, 0.0006428172928281128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006428172928281128

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064282
Iteration 2/1000 | Loss: 0.00007470
Iteration 3/1000 | Loss: 0.00006042
Iteration 4/1000 | Loss: 0.00005158
Iteration 5/1000 | Loss: 0.00004876
Iteration 6/1000 | Loss: 0.00004685
Iteration 7/1000 | Loss: 0.00004553
Iteration 8/1000 | Loss: 0.00004450
Iteration 9/1000 | Loss: 0.00004386
Iteration 10/1000 | Loss: 0.00004341
Iteration 11/1000 | Loss: 0.00004299
Iteration 12/1000 | Loss: 0.00004273
Iteration 13/1000 | Loss: 0.00004252
Iteration 14/1000 | Loss: 0.00004232
Iteration 15/1000 | Loss: 0.00004221
Iteration 16/1000 | Loss: 0.00004211
Iteration 17/1000 | Loss: 0.00004205
Iteration 18/1000 | Loss: 0.00004188
Iteration 19/1000 | Loss: 0.00004170
Iteration 20/1000 | Loss: 0.00004162
Iteration 21/1000 | Loss: 0.00004156
Iteration 22/1000 | Loss: 0.00004148
Iteration 23/1000 | Loss: 0.00004141
Iteration 24/1000 | Loss: 0.00004141
Iteration 25/1000 | Loss: 0.00004138
Iteration 26/1000 | Loss: 0.00004138
Iteration 27/1000 | Loss: 0.00004138
Iteration 28/1000 | Loss: 0.00004136
Iteration 29/1000 | Loss: 0.00004136
Iteration 30/1000 | Loss: 0.00004135
Iteration 31/1000 | Loss: 0.00004135
Iteration 32/1000 | Loss: 0.00004135
Iteration 33/1000 | Loss: 0.00004135
Iteration 34/1000 | Loss: 0.00004133
Iteration 35/1000 | Loss: 0.00004133
Iteration 36/1000 | Loss: 0.00004133
Iteration 37/1000 | Loss: 0.00004133
Iteration 38/1000 | Loss: 0.00004133
Iteration 39/1000 | Loss: 0.00004133
Iteration 40/1000 | Loss: 0.00004133
Iteration 41/1000 | Loss: 0.00004133
Iteration 42/1000 | Loss: 0.00004133
Iteration 43/1000 | Loss: 0.00004132
Iteration 44/1000 | Loss: 0.00004132
Iteration 45/1000 | Loss: 0.00004131
Iteration 46/1000 | Loss: 0.00004131
Iteration 47/1000 | Loss: 0.00004130
Iteration 48/1000 | Loss: 0.00004130
Iteration 49/1000 | Loss: 0.00004130
Iteration 50/1000 | Loss: 0.00004129
Iteration 51/1000 | Loss: 0.00004129
Iteration 52/1000 | Loss: 0.00004129
Iteration 53/1000 | Loss: 0.00004126
Iteration 54/1000 | Loss: 0.00004125
Iteration 55/1000 | Loss: 0.00004125
Iteration 56/1000 | Loss: 0.00004123
Iteration 57/1000 | Loss: 0.00004123
Iteration 58/1000 | Loss: 0.00004123
Iteration 59/1000 | Loss: 0.00004123
Iteration 60/1000 | Loss: 0.00004123
Iteration 61/1000 | Loss: 0.00004123
Iteration 62/1000 | Loss: 0.00004123
Iteration 63/1000 | Loss: 0.00004122
Iteration 64/1000 | Loss: 0.00004122
Iteration 65/1000 | Loss: 0.00004122
Iteration 66/1000 | Loss: 0.00004122
Iteration 67/1000 | Loss: 0.00004121
Iteration 68/1000 | Loss: 0.00004121
Iteration 69/1000 | Loss: 0.00004121
Iteration 70/1000 | Loss: 0.00004120
Iteration 71/1000 | Loss: 0.00004120
Iteration 72/1000 | Loss: 0.00004120
Iteration 73/1000 | Loss: 0.00004120
Iteration 74/1000 | Loss: 0.00004120
Iteration 75/1000 | Loss: 0.00004120
Iteration 76/1000 | Loss: 0.00004120
Iteration 77/1000 | Loss: 0.00004120
Iteration 78/1000 | Loss: 0.00004120
Iteration 79/1000 | Loss: 0.00004118
Iteration 80/1000 | Loss: 0.00004118
Iteration 81/1000 | Loss: 0.00004116
Iteration 82/1000 | Loss: 0.00004116
Iteration 83/1000 | Loss: 0.00004116
Iteration 84/1000 | Loss: 0.00004116
Iteration 85/1000 | Loss: 0.00004115
Iteration 86/1000 | Loss: 0.00004115
Iteration 87/1000 | Loss: 0.00004115
Iteration 88/1000 | Loss: 0.00004115
Iteration 89/1000 | Loss: 0.00004115
Iteration 90/1000 | Loss: 0.00004114
Iteration 91/1000 | Loss: 0.00004114
Iteration 92/1000 | Loss: 0.00004114
Iteration 93/1000 | Loss: 0.00004114
Iteration 94/1000 | Loss: 0.00004114
Iteration 95/1000 | Loss: 0.00004114
Iteration 96/1000 | Loss: 0.00004114
Iteration 97/1000 | Loss: 0.00004114
Iteration 98/1000 | Loss: 0.00004114
Iteration 99/1000 | Loss: 0.00004114
Iteration 100/1000 | Loss: 0.00004114
Iteration 101/1000 | Loss: 0.00004114
Iteration 102/1000 | Loss: 0.00004114
Iteration 103/1000 | Loss: 0.00004114
Iteration 104/1000 | Loss: 0.00004114
Iteration 105/1000 | Loss: 0.00004114
Iteration 106/1000 | Loss: 0.00004113
Iteration 107/1000 | Loss: 0.00004113
Iteration 108/1000 | Loss: 0.00004113
Iteration 109/1000 | Loss: 0.00004113
Iteration 110/1000 | Loss: 0.00004113
Iteration 111/1000 | Loss: 0.00004113
Iteration 112/1000 | Loss: 0.00004113
Iteration 113/1000 | Loss: 0.00004113
Iteration 114/1000 | Loss: 0.00004113
Iteration 115/1000 | Loss: 0.00004112
Iteration 116/1000 | Loss: 0.00004112
Iteration 117/1000 | Loss: 0.00004112
Iteration 118/1000 | Loss: 0.00004112
Iteration 119/1000 | Loss: 0.00004110
Iteration 120/1000 | Loss: 0.00004110
Iteration 121/1000 | Loss: 0.00004109
Iteration 122/1000 | Loss: 0.00004109
Iteration 123/1000 | Loss: 0.00004109
Iteration 124/1000 | Loss: 0.00004109
Iteration 125/1000 | Loss: 0.00004109
Iteration 126/1000 | Loss: 0.00004109
Iteration 127/1000 | Loss: 0.00004109
Iteration 128/1000 | Loss: 0.00004108
Iteration 129/1000 | Loss: 0.00004108
Iteration 130/1000 | Loss: 0.00004108
Iteration 131/1000 | Loss: 0.00004108
Iteration 132/1000 | Loss: 0.00004108
Iteration 133/1000 | Loss: 0.00004108
Iteration 134/1000 | Loss: 0.00004108
Iteration 135/1000 | Loss: 0.00004108
Iteration 136/1000 | Loss: 0.00004108
Iteration 137/1000 | Loss: 0.00004108
Iteration 138/1000 | Loss: 0.00004108
Iteration 139/1000 | Loss: 0.00004107
Iteration 140/1000 | Loss: 0.00004107
Iteration 141/1000 | Loss: 0.00004107
Iteration 142/1000 | Loss: 0.00004107
Iteration 143/1000 | Loss: 0.00004106
Iteration 144/1000 | Loss: 0.00004106
Iteration 145/1000 | Loss: 0.00004106
Iteration 146/1000 | Loss: 0.00004106
Iteration 147/1000 | Loss: 0.00004106
Iteration 148/1000 | Loss: 0.00004106
Iteration 149/1000 | Loss: 0.00004105
Iteration 150/1000 | Loss: 0.00004105
Iteration 151/1000 | Loss: 0.00004105
Iteration 152/1000 | Loss: 0.00004105
Iteration 153/1000 | Loss: 0.00004105
Iteration 154/1000 | Loss: 0.00004105
Iteration 155/1000 | Loss: 0.00004105
Iteration 156/1000 | Loss: 0.00004105
Iteration 157/1000 | Loss: 0.00004104
Iteration 158/1000 | Loss: 0.00004104
Iteration 159/1000 | Loss: 0.00004104
Iteration 160/1000 | Loss: 0.00004104
Iteration 161/1000 | Loss: 0.00004104
Iteration 162/1000 | Loss: 0.00004104
Iteration 163/1000 | Loss: 0.00004103
Iteration 164/1000 | Loss: 0.00004103
Iteration 165/1000 | Loss: 0.00004103
Iteration 166/1000 | Loss: 0.00004103
Iteration 167/1000 | Loss: 0.00004103
Iteration 168/1000 | Loss: 0.00004103
Iteration 169/1000 | Loss: 0.00004102
Iteration 170/1000 | Loss: 0.00004102
Iteration 171/1000 | Loss: 0.00004102
Iteration 172/1000 | Loss: 0.00004102
Iteration 173/1000 | Loss: 0.00004102
Iteration 174/1000 | Loss: 0.00004102
Iteration 175/1000 | Loss: 0.00004102
Iteration 176/1000 | Loss: 0.00004102
Iteration 177/1000 | Loss: 0.00004102
Iteration 178/1000 | Loss: 0.00004102
Iteration 179/1000 | Loss: 0.00004102
Iteration 180/1000 | Loss: 0.00004102
Iteration 181/1000 | Loss: 0.00004102
Iteration 182/1000 | Loss: 0.00004102
Iteration 183/1000 | Loss: 0.00004102
Iteration 184/1000 | Loss: 0.00004102
Iteration 185/1000 | Loss: 0.00004102
Iteration 186/1000 | Loss: 0.00004102
Iteration 187/1000 | Loss: 0.00004102
Iteration 188/1000 | Loss: 0.00004102
Iteration 189/1000 | Loss: 0.00004102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [4.101661397726275e-05, 4.101661397726275e-05, 4.101661397726275e-05, 4.101661397726275e-05, 4.101661397726275e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.101661397726275e-05

Optimization complete. Final v2v error: 5.32521390914917 mm

Highest mean error: 6.2166428565979 mm for frame 101

Lowest mean error: 4.8786492347717285 mm for frame 88

Saving results

Total time: 52.31772255897522
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01122803
Iteration 2/25 | Loss: 0.00238761
Iteration 3/25 | Loss: 0.00329745
Iteration 4/25 | Loss: 0.00175955
Iteration 5/25 | Loss: 0.00146015
Iteration 6/25 | Loss: 0.00121889
Iteration 7/25 | Loss: 0.00118790
Iteration 8/25 | Loss: 0.00118564
Iteration 9/25 | Loss: 0.00116667
Iteration 10/25 | Loss: 0.00112990
Iteration 11/25 | Loss: 0.00107879
Iteration 12/25 | Loss: 0.00106225
Iteration 13/25 | Loss: 0.00105298
Iteration 14/25 | Loss: 0.00105017
Iteration 15/25 | Loss: 0.00104710
Iteration 16/25 | Loss: 0.00104250
Iteration 17/25 | Loss: 0.00103700
Iteration 18/25 | Loss: 0.00103744
Iteration 19/25 | Loss: 0.00103803
Iteration 20/25 | Loss: 0.00103850
Iteration 21/25 | Loss: 0.00103940
Iteration 22/25 | Loss: 0.00103846
Iteration 23/25 | Loss: 0.00103879
Iteration 24/25 | Loss: 0.00103816
Iteration 25/25 | Loss: 0.00107054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55768657
Iteration 2/25 | Loss: 0.00147816
Iteration 3/25 | Loss: 0.00090215
Iteration 4/25 | Loss: 0.00090215
Iteration 5/25 | Loss: 0.00090215
Iteration 6/25 | Loss: 0.00090215
Iteration 7/25 | Loss: 0.00090214
Iteration 8/25 | Loss: 0.00090214
Iteration 9/25 | Loss: 0.00090214
Iteration 10/25 | Loss: 0.00090214
Iteration 11/25 | Loss: 0.00090214
Iteration 12/25 | Loss: 0.00090214
Iteration 13/25 | Loss: 0.00090214
Iteration 14/25 | Loss: 0.00090214
Iteration 15/25 | Loss: 0.00090214
Iteration 16/25 | Loss: 0.00090214
Iteration 17/25 | Loss: 0.00090214
Iteration 18/25 | Loss: 0.00090214
Iteration 19/25 | Loss: 0.00090214
Iteration 20/25 | Loss: 0.00090214
Iteration 21/25 | Loss: 0.00090214
Iteration 22/25 | Loss: 0.00090214
Iteration 23/25 | Loss: 0.00090214
Iteration 24/25 | Loss: 0.00090214
Iteration 25/25 | Loss: 0.00090214

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090214
Iteration 2/1000 | Loss: 0.00017377
Iteration 3/1000 | Loss: 0.00023615
Iteration 4/1000 | Loss: 0.00013377
Iteration 5/1000 | Loss: 0.00021233
Iteration 6/1000 | Loss: 0.00025114
Iteration 7/1000 | Loss: 0.00011314
Iteration 8/1000 | Loss: 0.00007818
Iteration 9/1000 | Loss: 0.00129420
Iteration 10/1000 | Loss: 0.00190909
Iteration 11/1000 | Loss: 0.00261951
Iteration 12/1000 | Loss: 0.00145167
Iteration 13/1000 | Loss: 0.00068708
Iteration 14/1000 | Loss: 0.00197209
Iteration 15/1000 | Loss: 0.00071635
Iteration 16/1000 | Loss: 0.00103438
Iteration 17/1000 | Loss: 0.00105479
Iteration 18/1000 | Loss: 0.00096742
Iteration 19/1000 | Loss: 0.00064578
Iteration 20/1000 | Loss: 0.00072711
Iteration 21/1000 | Loss: 0.00153521
Iteration 22/1000 | Loss: 0.00048319
Iteration 23/1000 | Loss: 0.00177764
Iteration 24/1000 | Loss: 0.00064199
Iteration 25/1000 | Loss: 0.00109798
Iteration 26/1000 | Loss: 0.00090003
Iteration 27/1000 | Loss: 0.00067219
Iteration 28/1000 | Loss: 0.00091624
Iteration 29/1000 | Loss: 0.00111967
Iteration 30/1000 | Loss: 0.00081174
Iteration 31/1000 | Loss: 0.00074770
Iteration 32/1000 | Loss: 0.00007186
Iteration 33/1000 | Loss: 0.00005632
Iteration 34/1000 | Loss: 0.00071235
Iteration 35/1000 | Loss: 0.00145103
Iteration 36/1000 | Loss: 0.00116816
Iteration 37/1000 | Loss: 0.00078965
Iteration 38/1000 | Loss: 0.00122136
Iteration 39/1000 | Loss: 0.00119568
Iteration 40/1000 | Loss: 0.00063226
Iteration 41/1000 | Loss: 0.00124204
Iteration 42/1000 | Loss: 0.00099446
Iteration 43/1000 | Loss: 0.00051853
Iteration 44/1000 | Loss: 0.00104095
Iteration 45/1000 | Loss: 0.00120904
Iteration 46/1000 | Loss: 0.00015082
Iteration 47/1000 | Loss: 0.00107584
Iteration 48/1000 | Loss: 0.00069136
Iteration 49/1000 | Loss: 0.00105698
Iteration 50/1000 | Loss: 0.00093728
Iteration 51/1000 | Loss: 0.00041209
Iteration 52/1000 | Loss: 0.00062210
Iteration 53/1000 | Loss: 0.00083588
Iteration 54/1000 | Loss: 0.00062287
Iteration 55/1000 | Loss: 0.00108203
Iteration 56/1000 | Loss: 0.00107559
Iteration 57/1000 | Loss: 0.00087093
Iteration 58/1000 | Loss: 0.00079110
Iteration 59/1000 | Loss: 0.00056467
Iteration 60/1000 | Loss: 0.00081886
Iteration 61/1000 | Loss: 0.00045924
Iteration 62/1000 | Loss: 0.00082969
Iteration 63/1000 | Loss: 0.00069657
Iteration 64/1000 | Loss: 0.00081306
Iteration 65/1000 | Loss: 0.00049351
Iteration 66/1000 | Loss: 0.00080805
Iteration 67/1000 | Loss: 0.00085878
Iteration 68/1000 | Loss: 0.00102519
Iteration 69/1000 | Loss: 0.00096349
Iteration 70/1000 | Loss: 0.00148663
Iteration 71/1000 | Loss: 0.00086196
Iteration 72/1000 | Loss: 0.00135807
Iteration 73/1000 | Loss: 0.00013144
Iteration 74/1000 | Loss: 0.00098043
Iteration 75/1000 | Loss: 0.00045594
Iteration 76/1000 | Loss: 0.00008651
Iteration 77/1000 | Loss: 0.00005378
Iteration 78/1000 | Loss: 0.00005585
Iteration 79/1000 | Loss: 0.00098361
Iteration 80/1000 | Loss: 0.00054748
Iteration 81/1000 | Loss: 0.00068318
Iteration 82/1000 | Loss: 0.00088096
Iteration 83/1000 | Loss: 0.00103308
Iteration 84/1000 | Loss: 0.00118788
Iteration 85/1000 | Loss: 0.00105724
Iteration 86/1000 | Loss: 0.00112975
Iteration 87/1000 | Loss: 0.00113983
Iteration 88/1000 | Loss: 0.00137335
Iteration 89/1000 | Loss: 0.00025505
Iteration 90/1000 | Loss: 0.00006300
Iteration 91/1000 | Loss: 0.00004843
Iteration 92/1000 | Loss: 0.00004215
Iteration 93/1000 | Loss: 0.00004616
Iteration 94/1000 | Loss: 0.00030726
Iteration 95/1000 | Loss: 0.00015452
Iteration 96/1000 | Loss: 0.00021851
Iteration 97/1000 | Loss: 0.00049204
Iteration 98/1000 | Loss: 0.00065349
Iteration 99/1000 | Loss: 0.00004924
Iteration 100/1000 | Loss: 0.00003735
Iteration 101/1000 | Loss: 0.00003424
Iteration 102/1000 | Loss: 0.00017158
Iteration 103/1000 | Loss: 0.00003702
Iteration 104/1000 | Loss: 0.00012436
Iteration 105/1000 | Loss: 0.00013291
Iteration 106/1000 | Loss: 0.00003684
Iteration 107/1000 | Loss: 0.00003166
Iteration 108/1000 | Loss: 0.00002981
Iteration 109/1000 | Loss: 0.00002913
Iteration 110/1000 | Loss: 0.00007352
Iteration 111/1000 | Loss: 0.00007078
Iteration 112/1000 | Loss: 0.00004957
Iteration 113/1000 | Loss: 0.00002985
Iteration 114/1000 | Loss: 0.00004136
Iteration 115/1000 | Loss: 0.00003530
Iteration 116/1000 | Loss: 0.00004412
Iteration 117/1000 | Loss: 0.00005182
Iteration 118/1000 | Loss: 0.00004858
Iteration 119/1000 | Loss: 0.00005950
Iteration 120/1000 | Loss: 0.00012663
Iteration 121/1000 | Loss: 0.00010887
Iteration 122/1000 | Loss: 0.00003688
Iteration 123/1000 | Loss: 0.00006107
Iteration 124/1000 | Loss: 0.00004431
Iteration 125/1000 | Loss: 0.00005979
Iteration 126/1000 | Loss: 0.00004120
Iteration 127/1000 | Loss: 0.00005906
Iteration 128/1000 | Loss: 0.00003972
Iteration 129/1000 | Loss: 0.00005699
Iteration 130/1000 | Loss: 0.00003363
Iteration 131/1000 | Loss: 0.00003007
Iteration 132/1000 | Loss: 0.00002916
Iteration 133/1000 | Loss: 0.00003881
Iteration 134/1000 | Loss: 0.00003174
Iteration 135/1000 | Loss: 0.00003832
Iteration 136/1000 | Loss: 0.00002887
Iteration 137/1000 | Loss: 0.00011047
Iteration 138/1000 | Loss: 0.00003352
Iteration 139/1000 | Loss: 0.00003090
Iteration 140/1000 | Loss: 0.00013356
Iteration 141/1000 | Loss: 0.00003517
Iteration 142/1000 | Loss: 0.00009740
Iteration 143/1000 | Loss: 0.00013522
Iteration 144/1000 | Loss: 0.00004828
Iteration 145/1000 | Loss: 0.00012747
Iteration 146/1000 | Loss: 0.00012305
Iteration 147/1000 | Loss: 0.00017008
Iteration 148/1000 | Loss: 0.00008663
Iteration 149/1000 | Loss: 0.00010850
Iteration 150/1000 | Loss: 0.00015310
Iteration 151/1000 | Loss: 0.00012335
Iteration 152/1000 | Loss: 0.00009913
Iteration 153/1000 | Loss: 0.00013426
Iteration 154/1000 | Loss: 0.00019666
Iteration 155/1000 | Loss: 0.00009092
Iteration 156/1000 | Loss: 0.00006837
Iteration 157/1000 | Loss: 0.00011450
Iteration 158/1000 | Loss: 0.00019750
Iteration 159/1000 | Loss: 0.00003699
Iteration 160/1000 | Loss: 0.00010188
Iteration 161/1000 | Loss: 0.00015090
Iteration 162/1000 | Loss: 0.00008302
Iteration 163/1000 | Loss: 0.00009215
Iteration 164/1000 | Loss: 0.00015703
Iteration 165/1000 | Loss: 0.00008385
Iteration 166/1000 | Loss: 0.00006756
Iteration 167/1000 | Loss: 0.00018922
Iteration 168/1000 | Loss: 0.00009792
Iteration 169/1000 | Loss: 0.00007274
Iteration 170/1000 | Loss: 0.00018432
Iteration 171/1000 | Loss: 0.00007005
Iteration 172/1000 | Loss: 0.00016642
Iteration 173/1000 | Loss: 0.00014765
Iteration 174/1000 | Loss: 0.00009587
Iteration 175/1000 | Loss: 0.00004873
Iteration 176/1000 | Loss: 0.00015623
Iteration 177/1000 | Loss: 0.00014875
Iteration 178/1000 | Loss: 0.00013836
Iteration 179/1000 | Loss: 0.00015546
Iteration 180/1000 | Loss: 0.00009012
Iteration 181/1000 | Loss: 0.00013990
Iteration 182/1000 | Loss: 0.00013160
Iteration 183/1000 | Loss: 0.00016495
Iteration 184/1000 | Loss: 0.00009777
Iteration 185/1000 | Loss: 0.00015378
Iteration 186/1000 | Loss: 0.00005840
Iteration 187/1000 | Loss: 0.00012302
Iteration 188/1000 | Loss: 0.00011394
Iteration 189/1000 | Loss: 0.00004651
Iteration 190/1000 | Loss: 0.00003480
Iteration 191/1000 | Loss: 0.00003368
Iteration 192/1000 | Loss: 0.00002922
Iteration 193/1000 | Loss: 0.00002831
Iteration 194/1000 | Loss: 0.00002775
Iteration 195/1000 | Loss: 0.00003288
Iteration 196/1000 | Loss: 0.00002931
Iteration 197/1000 | Loss: 0.00002836
Iteration 198/1000 | Loss: 0.00002777
Iteration 199/1000 | Loss: 0.00003263
Iteration 200/1000 | Loss: 0.00003093
Iteration 201/1000 | Loss: 0.00003273
Iteration 202/1000 | Loss: 0.00003083
Iteration 203/1000 | Loss: 0.00002823
Iteration 204/1000 | Loss: 0.00003156
Iteration 205/1000 | Loss: 0.00003073
Iteration 206/1000 | Loss: 0.00003146
Iteration 207/1000 | Loss: 0.00003062
Iteration 208/1000 | Loss: 0.00003139
Iteration 209/1000 | Loss: 0.00003139
Iteration 210/1000 | Loss: 0.00003057
Iteration 211/1000 | Loss: 0.00003130
Iteration 212/1000 | Loss: 0.00003037
Iteration 213/1000 | Loss: 0.00003175
Iteration 214/1000 | Loss: 0.00003549
Iteration 215/1000 | Loss: 0.00002984
Iteration 216/1000 | Loss: 0.00002798
Iteration 217/1000 | Loss: 0.00002704
Iteration 218/1000 | Loss: 0.00002663
Iteration 219/1000 | Loss: 0.00002654
Iteration 220/1000 | Loss: 0.00002637
Iteration 221/1000 | Loss: 0.00002636
Iteration 222/1000 | Loss: 0.00002630
Iteration 223/1000 | Loss: 0.00002621
Iteration 224/1000 | Loss: 0.00002617
Iteration 225/1000 | Loss: 0.00002617
Iteration 226/1000 | Loss: 0.00002617
Iteration 227/1000 | Loss: 0.00002617
Iteration 228/1000 | Loss: 0.00002617
Iteration 229/1000 | Loss: 0.00002617
Iteration 230/1000 | Loss: 0.00002617
Iteration 231/1000 | Loss: 0.00002617
Iteration 232/1000 | Loss: 0.00002616
Iteration 233/1000 | Loss: 0.00002616
Iteration 234/1000 | Loss: 0.00002616
Iteration 235/1000 | Loss: 0.00002616
Iteration 236/1000 | Loss: 0.00002616
Iteration 237/1000 | Loss: 0.00002615
Iteration 238/1000 | Loss: 0.00002615
Iteration 239/1000 | Loss: 0.00002613
Iteration 240/1000 | Loss: 0.00002613
Iteration 241/1000 | Loss: 0.00002613
Iteration 242/1000 | Loss: 0.00002612
Iteration 243/1000 | Loss: 0.00002612
Iteration 244/1000 | Loss: 0.00002612
Iteration 245/1000 | Loss: 0.00002611
Iteration 246/1000 | Loss: 0.00002610
Iteration 247/1000 | Loss: 0.00002610
Iteration 248/1000 | Loss: 0.00002610
Iteration 249/1000 | Loss: 0.00002610
Iteration 250/1000 | Loss: 0.00002610
Iteration 251/1000 | Loss: 0.00002609
Iteration 252/1000 | Loss: 0.00002609
Iteration 253/1000 | Loss: 0.00002609
Iteration 254/1000 | Loss: 0.00002609
Iteration 255/1000 | Loss: 0.00002609
Iteration 256/1000 | Loss: 0.00002609
Iteration 257/1000 | Loss: 0.00002609
Iteration 258/1000 | Loss: 0.00002609
Iteration 259/1000 | Loss: 0.00002609
Iteration 260/1000 | Loss: 0.00002609
Iteration 261/1000 | Loss: 0.00002608
Iteration 262/1000 | Loss: 0.00002608
Iteration 263/1000 | Loss: 0.00002608
Iteration 264/1000 | Loss: 0.00002608
Iteration 265/1000 | Loss: 0.00002608
Iteration 266/1000 | Loss: 0.00002608
Iteration 267/1000 | Loss: 0.00002608
Iteration 268/1000 | Loss: 0.00002608
Iteration 269/1000 | Loss: 0.00002608
Iteration 270/1000 | Loss: 0.00002608
Iteration 271/1000 | Loss: 0.00002608
Iteration 272/1000 | Loss: 0.00002608
Iteration 273/1000 | Loss: 0.00002607
Iteration 274/1000 | Loss: 0.00002607
Iteration 275/1000 | Loss: 0.00002607
Iteration 276/1000 | Loss: 0.00002607
Iteration 277/1000 | Loss: 0.00002607
Iteration 278/1000 | Loss: 0.00002607
Iteration 279/1000 | Loss: 0.00002607
Iteration 280/1000 | Loss: 0.00002607
Iteration 281/1000 | Loss: 0.00002606
Iteration 282/1000 | Loss: 0.00002606
Iteration 283/1000 | Loss: 0.00002606
Iteration 284/1000 | Loss: 0.00002606
Iteration 285/1000 | Loss: 0.00002606
Iteration 286/1000 | Loss: 0.00002606
Iteration 287/1000 | Loss: 0.00002606
Iteration 288/1000 | Loss: 0.00002606
Iteration 289/1000 | Loss: 0.00002606
Iteration 290/1000 | Loss: 0.00002606
Iteration 291/1000 | Loss: 0.00002606
Iteration 292/1000 | Loss: 0.00002606
Iteration 293/1000 | Loss: 0.00002606
Iteration 294/1000 | Loss: 0.00002606
Iteration 295/1000 | Loss: 0.00002606
Iteration 296/1000 | Loss: 0.00002606
Iteration 297/1000 | Loss: 0.00002606
Iteration 298/1000 | Loss: 0.00002606
Iteration 299/1000 | Loss: 0.00002606
Iteration 300/1000 | Loss: 0.00002605
Iteration 301/1000 | Loss: 0.00002605
Iteration 302/1000 | Loss: 0.00002605
Iteration 303/1000 | Loss: 0.00002605
Iteration 304/1000 | Loss: 0.00002605
Iteration 305/1000 | Loss: 0.00002605
Iteration 306/1000 | Loss: 0.00002605
Iteration 307/1000 | Loss: 0.00002605
Iteration 308/1000 | Loss: 0.00002605
Iteration 309/1000 | Loss: 0.00002605
Iteration 310/1000 | Loss: 0.00002605
Iteration 311/1000 | Loss: 0.00002605
Iteration 312/1000 | Loss: 0.00002605
Iteration 313/1000 | Loss: 0.00002605
Iteration 314/1000 | Loss: 0.00002605
Iteration 315/1000 | Loss: 0.00002605
Iteration 316/1000 | Loss: 0.00002605
Iteration 317/1000 | Loss: 0.00002605
Iteration 318/1000 | Loss: 0.00002604
Iteration 319/1000 | Loss: 0.00002604
Iteration 320/1000 | Loss: 0.00002604
Iteration 321/1000 | Loss: 0.00002604
Iteration 322/1000 | Loss: 0.00002604
Iteration 323/1000 | Loss: 0.00002604
Iteration 324/1000 | Loss: 0.00002604
Iteration 325/1000 | Loss: 0.00002604
Iteration 326/1000 | Loss: 0.00002604
Iteration 327/1000 | Loss: 0.00002604
Iteration 328/1000 | Loss: 0.00002604
Iteration 329/1000 | Loss: 0.00002604
Iteration 330/1000 | Loss: 0.00002604
Iteration 331/1000 | Loss: 0.00002604
Iteration 332/1000 | Loss: 0.00002604
Iteration 333/1000 | Loss: 0.00002604
Iteration 334/1000 | Loss: 0.00002604
Iteration 335/1000 | Loss: 0.00002604
Iteration 336/1000 | Loss: 0.00002604
Iteration 337/1000 | Loss: 0.00002604
Iteration 338/1000 | Loss: 0.00002604
Iteration 339/1000 | Loss: 0.00002604
Iteration 340/1000 | Loss: 0.00002604
Iteration 341/1000 | Loss: 0.00002604
Iteration 342/1000 | Loss: 0.00002604
Iteration 343/1000 | Loss: 0.00002604
Iteration 344/1000 | Loss: 0.00002604
Iteration 345/1000 | Loss: 0.00002604
Iteration 346/1000 | Loss: 0.00002604
Iteration 347/1000 | Loss: 0.00002604
Iteration 348/1000 | Loss: 0.00002604
Iteration 349/1000 | Loss: 0.00002604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 349. Stopping optimization.
Last 5 losses: [2.6044415790238418e-05, 2.6044415790238418e-05, 2.6044415790238418e-05, 2.6044415790238418e-05, 2.6044415790238418e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6044415790238418e-05

Optimization complete. Final v2v error: 4.269436359405518 mm

Highest mean error: 9.733162879943848 mm for frame 238

Lowest mean error: 3.6064860820770264 mm for frame 204

Saving results

Total time: 411.7143828868866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00914446
Iteration 2/25 | Loss: 0.00152925
Iteration 3/25 | Loss: 0.00124600
Iteration 4/25 | Loss: 0.00118745
Iteration 5/25 | Loss: 0.00117198
Iteration 6/25 | Loss: 0.00116848
Iteration 7/25 | Loss: 0.00116724
Iteration 8/25 | Loss: 0.00116713
Iteration 9/25 | Loss: 0.00116713
Iteration 10/25 | Loss: 0.00116713
Iteration 11/25 | Loss: 0.00116713
Iteration 12/25 | Loss: 0.00116713
Iteration 13/25 | Loss: 0.00116713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011671294923871756, 0.0011671294923871756, 0.0011671294923871756, 0.0011671294923871756, 0.0011671294923871756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011671294923871756

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04704106
Iteration 2/25 | Loss: 0.00102909
Iteration 3/25 | Loss: 0.00102908
Iteration 4/25 | Loss: 0.00102908
Iteration 5/25 | Loss: 0.00102908
Iteration 6/25 | Loss: 0.00102908
Iteration 7/25 | Loss: 0.00102908
Iteration 8/25 | Loss: 0.00102907
Iteration 9/25 | Loss: 0.00102907
Iteration 10/25 | Loss: 0.00102907
Iteration 11/25 | Loss: 0.00102907
Iteration 12/25 | Loss: 0.00102907
Iteration 13/25 | Loss: 0.00102907
Iteration 14/25 | Loss: 0.00102907
Iteration 15/25 | Loss: 0.00102907
Iteration 16/25 | Loss: 0.00102907
Iteration 17/25 | Loss: 0.00102907
Iteration 18/25 | Loss: 0.00102907
Iteration 19/25 | Loss: 0.00102907
Iteration 20/25 | Loss: 0.00102907
Iteration 21/25 | Loss: 0.00102907
Iteration 22/25 | Loss: 0.00102907
Iteration 23/25 | Loss: 0.00102907
Iteration 24/25 | Loss: 0.00102907
Iteration 25/25 | Loss: 0.00102907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102907
Iteration 2/1000 | Loss: 0.00009281
Iteration 3/1000 | Loss: 0.00005926
Iteration 4/1000 | Loss: 0.00005115
Iteration 5/1000 | Loss: 0.00004658
Iteration 6/1000 | Loss: 0.00004356
Iteration 7/1000 | Loss: 0.00004183
Iteration 8/1000 | Loss: 0.00004039
Iteration 9/1000 | Loss: 0.00004001
Iteration 10/1000 | Loss: 0.00003944
Iteration 11/1000 | Loss: 0.00003903
Iteration 12/1000 | Loss: 0.00003884
Iteration 13/1000 | Loss: 0.00003875
Iteration 14/1000 | Loss: 0.00003875
Iteration 15/1000 | Loss: 0.00003875
Iteration 16/1000 | Loss: 0.00003875
Iteration 17/1000 | Loss: 0.00003875
Iteration 18/1000 | Loss: 0.00003874
Iteration 19/1000 | Loss: 0.00003874
Iteration 20/1000 | Loss: 0.00003874
Iteration 21/1000 | Loss: 0.00003874
Iteration 22/1000 | Loss: 0.00003874
Iteration 23/1000 | Loss: 0.00003873
Iteration 24/1000 | Loss: 0.00003872
Iteration 25/1000 | Loss: 0.00003871
Iteration 26/1000 | Loss: 0.00003870
Iteration 27/1000 | Loss: 0.00003869
Iteration 28/1000 | Loss: 0.00003868
Iteration 29/1000 | Loss: 0.00003868
Iteration 30/1000 | Loss: 0.00003867
Iteration 31/1000 | Loss: 0.00003867
Iteration 32/1000 | Loss: 0.00003867
Iteration 33/1000 | Loss: 0.00003863
Iteration 34/1000 | Loss: 0.00003863
Iteration 35/1000 | Loss: 0.00003863
Iteration 36/1000 | Loss: 0.00003862
Iteration 37/1000 | Loss: 0.00003862
Iteration 38/1000 | Loss: 0.00003862
Iteration 39/1000 | Loss: 0.00003862
Iteration 40/1000 | Loss: 0.00003861
Iteration 41/1000 | Loss: 0.00003861
Iteration 42/1000 | Loss: 0.00003861
Iteration 43/1000 | Loss: 0.00003860
Iteration 44/1000 | Loss: 0.00003860
Iteration 45/1000 | Loss: 0.00003860
Iteration 46/1000 | Loss: 0.00003860
Iteration 47/1000 | Loss: 0.00003860
Iteration 48/1000 | Loss: 0.00003860
Iteration 49/1000 | Loss: 0.00003859
Iteration 50/1000 | Loss: 0.00003859
Iteration 51/1000 | Loss: 0.00003859
Iteration 52/1000 | Loss: 0.00003859
Iteration 53/1000 | Loss: 0.00003859
Iteration 54/1000 | Loss: 0.00003859
Iteration 55/1000 | Loss: 0.00003859
Iteration 56/1000 | Loss: 0.00003858
Iteration 57/1000 | Loss: 0.00003858
Iteration 58/1000 | Loss: 0.00003858
Iteration 59/1000 | Loss: 0.00003858
Iteration 60/1000 | Loss: 0.00003858
Iteration 61/1000 | Loss: 0.00003858
Iteration 62/1000 | Loss: 0.00003858
Iteration 63/1000 | Loss: 0.00003858
Iteration 64/1000 | Loss: 0.00003858
Iteration 65/1000 | Loss: 0.00003858
Iteration 66/1000 | Loss: 0.00003858
Iteration 67/1000 | Loss: 0.00003858
Iteration 68/1000 | Loss: 0.00003858
Iteration 69/1000 | Loss: 0.00003858
Iteration 70/1000 | Loss: 0.00003858
Iteration 71/1000 | Loss: 0.00003858
Iteration 72/1000 | Loss: 0.00003858
Iteration 73/1000 | Loss: 0.00003858
Iteration 74/1000 | Loss: 0.00003858
Iteration 75/1000 | Loss: 0.00003858
Iteration 76/1000 | Loss: 0.00003858
Iteration 77/1000 | Loss: 0.00003858
Iteration 78/1000 | Loss: 0.00003858
Iteration 79/1000 | Loss: 0.00003858
Iteration 80/1000 | Loss: 0.00003858
Iteration 81/1000 | Loss: 0.00003858
Iteration 82/1000 | Loss: 0.00003858
Iteration 83/1000 | Loss: 0.00003858
Iteration 84/1000 | Loss: 0.00003858
Iteration 85/1000 | Loss: 0.00003858
Iteration 86/1000 | Loss: 0.00003858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [3.857767296722159e-05, 3.857767296722159e-05, 3.857767296722159e-05, 3.857767296722159e-05, 3.857767296722159e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.857767296722159e-05

Optimization complete. Final v2v error: 5.076157093048096 mm

Highest mean error: 5.745265007019043 mm for frame 131

Lowest mean error: 4.665615081787109 mm for frame 28

Saving results

Total time: 33.11311459541321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883105
Iteration 2/25 | Loss: 0.00129681
Iteration 3/25 | Loss: 0.00109782
Iteration 4/25 | Loss: 0.00106602
Iteration 5/25 | Loss: 0.00105599
Iteration 6/25 | Loss: 0.00105387
Iteration 7/25 | Loss: 0.00105387
Iteration 8/25 | Loss: 0.00105387
Iteration 9/25 | Loss: 0.00105387
Iteration 10/25 | Loss: 0.00105387
Iteration 11/25 | Loss: 0.00105387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010538717033341527, 0.0010538717033341527, 0.0010538717033341527, 0.0010538717033341527, 0.0010538717033341527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010538717033341527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45573020
Iteration 2/25 | Loss: 0.00089234
Iteration 3/25 | Loss: 0.00089229
Iteration 4/25 | Loss: 0.00089229
Iteration 5/25 | Loss: 0.00089229
Iteration 6/25 | Loss: 0.00089229
Iteration 7/25 | Loss: 0.00089229
Iteration 8/25 | Loss: 0.00089229
Iteration 9/25 | Loss: 0.00089229
Iteration 10/25 | Loss: 0.00089229
Iteration 11/25 | Loss: 0.00089229
Iteration 12/25 | Loss: 0.00089229
Iteration 13/25 | Loss: 0.00089229
Iteration 14/25 | Loss: 0.00089229
Iteration 15/25 | Loss: 0.00089229
Iteration 16/25 | Loss: 0.00089229
Iteration 17/25 | Loss: 0.00089229
Iteration 18/25 | Loss: 0.00089229
Iteration 19/25 | Loss: 0.00089229
Iteration 20/25 | Loss: 0.00089229
Iteration 21/25 | Loss: 0.00089229
Iteration 22/25 | Loss: 0.00089229
Iteration 23/25 | Loss: 0.00089229
Iteration 24/25 | Loss: 0.00089229
Iteration 25/25 | Loss: 0.00089229

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089229
Iteration 2/1000 | Loss: 0.00005514
Iteration 3/1000 | Loss: 0.00003816
Iteration 4/1000 | Loss: 0.00002940
Iteration 5/1000 | Loss: 0.00002786
Iteration 6/1000 | Loss: 0.00002635
Iteration 7/1000 | Loss: 0.00002527
Iteration 8/1000 | Loss: 0.00002457
Iteration 9/1000 | Loss: 0.00002405
Iteration 10/1000 | Loss: 0.00002362
Iteration 11/1000 | Loss: 0.00002338
Iteration 12/1000 | Loss: 0.00002330
Iteration 13/1000 | Loss: 0.00002323
Iteration 14/1000 | Loss: 0.00002321
Iteration 15/1000 | Loss: 0.00002320
Iteration 16/1000 | Loss: 0.00002313
Iteration 17/1000 | Loss: 0.00002312
Iteration 18/1000 | Loss: 0.00002312
Iteration 19/1000 | Loss: 0.00002303
Iteration 20/1000 | Loss: 0.00002300
Iteration 21/1000 | Loss: 0.00002299
Iteration 22/1000 | Loss: 0.00002298
Iteration 23/1000 | Loss: 0.00002298
Iteration 24/1000 | Loss: 0.00002295
Iteration 25/1000 | Loss: 0.00002290
Iteration 26/1000 | Loss: 0.00002286
Iteration 27/1000 | Loss: 0.00002286
Iteration 28/1000 | Loss: 0.00002286
Iteration 29/1000 | Loss: 0.00002285
Iteration 30/1000 | Loss: 0.00002285
Iteration 31/1000 | Loss: 0.00002285
Iteration 32/1000 | Loss: 0.00002285
Iteration 33/1000 | Loss: 0.00002285
Iteration 34/1000 | Loss: 0.00002285
Iteration 35/1000 | Loss: 0.00002285
Iteration 36/1000 | Loss: 0.00002285
Iteration 37/1000 | Loss: 0.00002285
Iteration 38/1000 | Loss: 0.00002285
Iteration 39/1000 | Loss: 0.00002285
Iteration 40/1000 | Loss: 0.00002284
Iteration 41/1000 | Loss: 0.00002284
Iteration 42/1000 | Loss: 0.00002283
Iteration 43/1000 | Loss: 0.00002282
Iteration 44/1000 | Loss: 0.00002282
Iteration 45/1000 | Loss: 0.00002282
Iteration 46/1000 | Loss: 0.00002281
Iteration 47/1000 | Loss: 0.00002281
Iteration 48/1000 | Loss: 0.00002281
Iteration 49/1000 | Loss: 0.00002280
Iteration 50/1000 | Loss: 0.00002280
Iteration 51/1000 | Loss: 0.00002280
Iteration 52/1000 | Loss: 0.00002280
Iteration 53/1000 | Loss: 0.00002280
Iteration 54/1000 | Loss: 0.00002280
Iteration 55/1000 | Loss: 0.00002280
Iteration 56/1000 | Loss: 0.00002279
Iteration 57/1000 | Loss: 0.00002279
Iteration 58/1000 | Loss: 0.00002279
Iteration 59/1000 | Loss: 0.00002279
Iteration 60/1000 | Loss: 0.00002279
Iteration 61/1000 | Loss: 0.00002278
Iteration 62/1000 | Loss: 0.00002278
Iteration 63/1000 | Loss: 0.00002278
Iteration 64/1000 | Loss: 0.00002278
Iteration 65/1000 | Loss: 0.00002278
Iteration 66/1000 | Loss: 0.00002278
Iteration 67/1000 | Loss: 0.00002277
Iteration 68/1000 | Loss: 0.00002277
Iteration 69/1000 | Loss: 0.00002277
Iteration 70/1000 | Loss: 0.00002276
Iteration 71/1000 | Loss: 0.00002276
Iteration 72/1000 | Loss: 0.00002276
Iteration 73/1000 | Loss: 0.00002276
Iteration 74/1000 | Loss: 0.00002276
Iteration 75/1000 | Loss: 0.00002276
Iteration 76/1000 | Loss: 0.00002276
Iteration 77/1000 | Loss: 0.00002275
Iteration 78/1000 | Loss: 0.00002275
Iteration 79/1000 | Loss: 0.00002275
Iteration 80/1000 | Loss: 0.00002275
Iteration 81/1000 | Loss: 0.00002275
Iteration 82/1000 | Loss: 0.00002275
Iteration 83/1000 | Loss: 0.00002275
Iteration 84/1000 | Loss: 0.00002275
Iteration 85/1000 | Loss: 0.00002275
Iteration 86/1000 | Loss: 0.00002275
Iteration 87/1000 | Loss: 0.00002275
Iteration 88/1000 | Loss: 0.00002275
Iteration 89/1000 | Loss: 0.00002275
Iteration 90/1000 | Loss: 0.00002274
Iteration 91/1000 | Loss: 0.00002274
Iteration 92/1000 | Loss: 0.00002274
Iteration 93/1000 | Loss: 0.00002274
Iteration 94/1000 | Loss: 0.00002274
Iteration 95/1000 | Loss: 0.00002274
Iteration 96/1000 | Loss: 0.00002274
Iteration 97/1000 | Loss: 0.00002274
Iteration 98/1000 | Loss: 0.00002273
Iteration 99/1000 | Loss: 0.00002273
Iteration 100/1000 | Loss: 0.00002273
Iteration 101/1000 | Loss: 0.00002273
Iteration 102/1000 | Loss: 0.00002273
Iteration 103/1000 | Loss: 0.00002273
Iteration 104/1000 | Loss: 0.00002273
Iteration 105/1000 | Loss: 0.00002272
Iteration 106/1000 | Loss: 0.00002272
Iteration 107/1000 | Loss: 0.00002272
Iteration 108/1000 | Loss: 0.00002272
Iteration 109/1000 | Loss: 0.00002272
Iteration 110/1000 | Loss: 0.00002272
Iteration 111/1000 | Loss: 0.00002272
Iteration 112/1000 | Loss: 0.00002271
Iteration 113/1000 | Loss: 0.00002271
Iteration 114/1000 | Loss: 0.00002271
Iteration 115/1000 | Loss: 0.00002271
Iteration 116/1000 | Loss: 0.00002271
Iteration 117/1000 | Loss: 0.00002271
Iteration 118/1000 | Loss: 0.00002271
Iteration 119/1000 | Loss: 0.00002271
Iteration 120/1000 | Loss: 0.00002271
Iteration 121/1000 | Loss: 0.00002271
Iteration 122/1000 | Loss: 0.00002271
Iteration 123/1000 | Loss: 0.00002271
Iteration 124/1000 | Loss: 0.00002271
Iteration 125/1000 | Loss: 0.00002271
Iteration 126/1000 | Loss: 0.00002271
Iteration 127/1000 | Loss: 0.00002271
Iteration 128/1000 | Loss: 0.00002271
Iteration 129/1000 | Loss: 0.00002270
Iteration 130/1000 | Loss: 0.00002270
Iteration 131/1000 | Loss: 0.00002270
Iteration 132/1000 | Loss: 0.00002270
Iteration 133/1000 | Loss: 0.00002270
Iteration 134/1000 | Loss: 0.00002270
Iteration 135/1000 | Loss: 0.00002270
Iteration 136/1000 | Loss: 0.00002270
Iteration 137/1000 | Loss: 0.00002270
Iteration 138/1000 | Loss: 0.00002269
Iteration 139/1000 | Loss: 0.00002269
Iteration 140/1000 | Loss: 0.00002269
Iteration 141/1000 | Loss: 0.00002269
Iteration 142/1000 | Loss: 0.00002269
Iteration 143/1000 | Loss: 0.00002269
Iteration 144/1000 | Loss: 0.00002269
Iteration 145/1000 | Loss: 0.00002269
Iteration 146/1000 | Loss: 0.00002269
Iteration 147/1000 | Loss: 0.00002269
Iteration 148/1000 | Loss: 0.00002269
Iteration 149/1000 | Loss: 0.00002269
Iteration 150/1000 | Loss: 0.00002269
Iteration 151/1000 | Loss: 0.00002269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.2690444893669337e-05, 2.2690444893669337e-05, 2.2690444893669337e-05, 2.2690444893669337e-05, 2.2690444893669337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2690444893669337e-05

Optimization complete. Final v2v error: 4.124992370605469 mm

Highest mean error: 5.106021881103516 mm for frame 133

Lowest mean error: 3.717179298400879 mm for frame 86

Saving results

Total time: 43.209757566452026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00900945
Iteration 2/25 | Loss: 0.00197176
Iteration 3/25 | Loss: 0.00113402
Iteration 4/25 | Loss: 0.00102254
Iteration 5/25 | Loss: 0.00100580
Iteration 6/25 | Loss: 0.00100172
Iteration 7/25 | Loss: 0.00100131
Iteration 8/25 | Loss: 0.00100131
Iteration 9/25 | Loss: 0.00100131
Iteration 10/25 | Loss: 0.00100131
Iteration 11/25 | Loss: 0.00100131
Iteration 12/25 | Loss: 0.00100131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010013149585574865, 0.0010013149585574865, 0.0010013149585574865, 0.0010013149585574865, 0.0010013149585574865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010013149585574865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85320318
Iteration 2/25 | Loss: 0.00090585
Iteration 3/25 | Loss: 0.00090585
Iteration 4/25 | Loss: 0.00090585
Iteration 5/25 | Loss: 0.00090585
Iteration 6/25 | Loss: 0.00090585
Iteration 7/25 | Loss: 0.00090585
Iteration 8/25 | Loss: 0.00090585
Iteration 9/25 | Loss: 0.00090585
Iteration 10/25 | Loss: 0.00090585
Iteration 11/25 | Loss: 0.00090585
Iteration 12/25 | Loss: 0.00090585
Iteration 13/25 | Loss: 0.00090585
Iteration 14/25 | Loss: 0.00090585
Iteration 15/25 | Loss: 0.00090585
Iteration 16/25 | Loss: 0.00090585
Iteration 17/25 | Loss: 0.00090585
Iteration 18/25 | Loss: 0.00090585
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009058477589860559, 0.0009058477589860559, 0.0009058477589860559, 0.0009058477589860559, 0.0009058477589860559]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009058477589860559

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090585
Iteration 2/1000 | Loss: 0.00002862
Iteration 3/1000 | Loss: 0.00002266
Iteration 4/1000 | Loss: 0.00002025
Iteration 5/1000 | Loss: 0.00001902
Iteration 6/1000 | Loss: 0.00001843
Iteration 7/1000 | Loss: 0.00001807
Iteration 8/1000 | Loss: 0.00001785
Iteration 9/1000 | Loss: 0.00001773
Iteration 10/1000 | Loss: 0.00001765
Iteration 11/1000 | Loss: 0.00001759
Iteration 12/1000 | Loss: 0.00001754
Iteration 13/1000 | Loss: 0.00001747
Iteration 14/1000 | Loss: 0.00001745
Iteration 15/1000 | Loss: 0.00001745
Iteration 16/1000 | Loss: 0.00001737
Iteration 17/1000 | Loss: 0.00001736
Iteration 18/1000 | Loss: 0.00001734
Iteration 19/1000 | Loss: 0.00001734
Iteration 20/1000 | Loss: 0.00001734
Iteration 21/1000 | Loss: 0.00001734
Iteration 22/1000 | Loss: 0.00001734
Iteration 23/1000 | Loss: 0.00001734
Iteration 24/1000 | Loss: 0.00001734
Iteration 25/1000 | Loss: 0.00001733
Iteration 26/1000 | Loss: 0.00001733
Iteration 27/1000 | Loss: 0.00001732
Iteration 28/1000 | Loss: 0.00001731
Iteration 29/1000 | Loss: 0.00001731
Iteration 30/1000 | Loss: 0.00001731
Iteration 31/1000 | Loss: 0.00001730
Iteration 32/1000 | Loss: 0.00001730
Iteration 33/1000 | Loss: 0.00001730
Iteration 34/1000 | Loss: 0.00001729
Iteration 35/1000 | Loss: 0.00001729
Iteration 36/1000 | Loss: 0.00001727
Iteration 37/1000 | Loss: 0.00001727
Iteration 38/1000 | Loss: 0.00001727
Iteration 39/1000 | Loss: 0.00001726
Iteration 40/1000 | Loss: 0.00001726
Iteration 41/1000 | Loss: 0.00001726
Iteration 42/1000 | Loss: 0.00001726
Iteration 43/1000 | Loss: 0.00001726
Iteration 44/1000 | Loss: 0.00001725
Iteration 45/1000 | Loss: 0.00001725
Iteration 46/1000 | Loss: 0.00001724
Iteration 47/1000 | Loss: 0.00001724
Iteration 48/1000 | Loss: 0.00001723
Iteration 49/1000 | Loss: 0.00001723
Iteration 50/1000 | Loss: 0.00001721
Iteration 51/1000 | Loss: 0.00001721
Iteration 52/1000 | Loss: 0.00001721
Iteration 53/1000 | Loss: 0.00001721
Iteration 54/1000 | Loss: 0.00001721
Iteration 55/1000 | Loss: 0.00001721
Iteration 56/1000 | Loss: 0.00001720
Iteration 57/1000 | Loss: 0.00001720
Iteration 58/1000 | Loss: 0.00001720
Iteration 59/1000 | Loss: 0.00001720
Iteration 60/1000 | Loss: 0.00001720
Iteration 61/1000 | Loss: 0.00001720
Iteration 62/1000 | Loss: 0.00001720
Iteration 63/1000 | Loss: 0.00001720
Iteration 64/1000 | Loss: 0.00001720
Iteration 65/1000 | Loss: 0.00001720
Iteration 66/1000 | Loss: 0.00001720
Iteration 67/1000 | Loss: 0.00001720
Iteration 68/1000 | Loss: 0.00001719
Iteration 69/1000 | Loss: 0.00001719
Iteration 70/1000 | Loss: 0.00001719
Iteration 71/1000 | Loss: 0.00001719
Iteration 72/1000 | Loss: 0.00001719
Iteration 73/1000 | Loss: 0.00001719
Iteration 74/1000 | Loss: 0.00001719
Iteration 75/1000 | Loss: 0.00001718
Iteration 76/1000 | Loss: 0.00001718
Iteration 77/1000 | Loss: 0.00001718
Iteration 78/1000 | Loss: 0.00001718
Iteration 79/1000 | Loss: 0.00001717
Iteration 80/1000 | Loss: 0.00001717
Iteration 81/1000 | Loss: 0.00001717
Iteration 82/1000 | Loss: 0.00001717
Iteration 83/1000 | Loss: 0.00001717
Iteration 84/1000 | Loss: 0.00001717
Iteration 85/1000 | Loss: 0.00001717
Iteration 86/1000 | Loss: 0.00001717
Iteration 87/1000 | Loss: 0.00001717
Iteration 88/1000 | Loss: 0.00001716
Iteration 89/1000 | Loss: 0.00001716
Iteration 90/1000 | Loss: 0.00001716
Iteration 91/1000 | Loss: 0.00001716
Iteration 92/1000 | Loss: 0.00001716
Iteration 93/1000 | Loss: 0.00001716
Iteration 94/1000 | Loss: 0.00001715
Iteration 95/1000 | Loss: 0.00001715
Iteration 96/1000 | Loss: 0.00001714
Iteration 97/1000 | Loss: 0.00001714
Iteration 98/1000 | Loss: 0.00001714
Iteration 99/1000 | Loss: 0.00001714
Iteration 100/1000 | Loss: 0.00001714
Iteration 101/1000 | Loss: 0.00001713
Iteration 102/1000 | Loss: 0.00001713
Iteration 103/1000 | Loss: 0.00001713
Iteration 104/1000 | Loss: 0.00001713
Iteration 105/1000 | Loss: 0.00001713
Iteration 106/1000 | Loss: 0.00001713
Iteration 107/1000 | Loss: 0.00001713
Iteration 108/1000 | Loss: 0.00001713
Iteration 109/1000 | Loss: 0.00001712
Iteration 110/1000 | Loss: 0.00001712
Iteration 111/1000 | Loss: 0.00001712
Iteration 112/1000 | Loss: 0.00001712
Iteration 113/1000 | Loss: 0.00001712
Iteration 114/1000 | Loss: 0.00001712
Iteration 115/1000 | Loss: 0.00001712
Iteration 116/1000 | Loss: 0.00001711
Iteration 117/1000 | Loss: 0.00001711
Iteration 118/1000 | Loss: 0.00001711
Iteration 119/1000 | Loss: 0.00001711
Iteration 120/1000 | Loss: 0.00001711
Iteration 121/1000 | Loss: 0.00001711
Iteration 122/1000 | Loss: 0.00001711
Iteration 123/1000 | Loss: 0.00001711
Iteration 124/1000 | Loss: 0.00001711
Iteration 125/1000 | Loss: 0.00001711
Iteration 126/1000 | Loss: 0.00001711
Iteration 127/1000 | Loss: 0.00001711
Iteration 128/1000 | Loss: 0.00001711
Iteration 129/1000 | Loss: 0.00001711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.711136610538233e-05, 1.711136610538233e-05, 1.711136610538233e-05, 1.711136610538233e-05, 1.711136610538233e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.711136610538233e-05

Optimization complete. Final v2v error: 3.6371171474456787 mm

Highest mean error: 3.9835410118103027 mm for frame 171

Lowest mean error: 3.41817569732666 mm for frame 33

Saving results

Total time: 38.44906830787659
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843177
Iteration 2/25 | Loss: 0.00163339
Iteration 3/25 | Loss: 0.00118459
Iteration 4/25 | Loss: 0.00113443
Iteration 5/25 | Loss: 0.00111882
Iteration 6/25 | Loss: 0.00111556
Iteration 7/25 | Loss: 0.00111504
Iteration 8/25 | Loss: 0.00111504
Iteration 9/25 | Loss: 0.00111504
Iteration 10/25 | Loss: 0.00111504
Iteration 11/25 | Loss: 0.00111504
Iteration 12/25 | Loss: 0.00111504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011150402715429664, 0.0011150402715429664, 0.0011150402715429664, 0.0011150402715429664, 0.0011150402715429664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011150402715429664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46536720
Iteration 2/25 | Loss: 0.00058234
Iteration 3/25 | Loss: 0.00058230
Iteration 4/25 | Loss: 0.00058230
Iteration 5/25 | Loss: 0.00058230
Iteration 6/25 | Loss: 0.00058230
Iteration 7/25 | Loss: 0.00058230
Iteration 8/25 | Loss: 0.00058230
Iteration 9/25 | Loss: 0.00058230
Iteration 10/25 | Loss: 0.00058230
Iteration 11/25 | Loss: 0.00058230
Iteration 12/25 | Loss: 0.00058230
Iteration 13/25 | Loss: 0.00058230
Iteration 14/25 | Loss: 0.00058230
Iteration 15/25 | Loss: 0.00058230
Iteration 16/25 | Loss: 0.00058230
Iteration 17/25 | Loss: 0.00058230
Iteration 18/25 | Loss: 0.00058230
Iteration 19/25 | Loss: 0.00058230
Iteration 20/25 | Loss: 0.00058230
Iteration 21/25 | Loss: 0.00058230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005823015817441046, 0.0005823015817441046, 0.0005823015817441046, 0.0005823015817441046, 0.0005823015817441046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005823015817441046

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058230
Iteration 2/1000 | Loss: 0.00006123
Iteration 3/1000 | Loss: 0.00004722
Iteration 4/1000 | Loss: 0.00004321
Iteration 5/1000 | Loss: 0.00004106
Iteration 6/1000 | Loss: 0.00003995
Iteration 7/1000 | Loss: 0.00003918
Iteration 8/1000 | Loss: 0.00003848
Iteration 9/1000 | Loss: 0.00003804
Iteration 10/1000 | Loss: 0.00003782
Iteration 11/1000 | Loss: 0.00003777
Iteration 12/1000 | Loss: 0.00003774
Iteration 13/1000 | Loss: 0.00003772
Iteration 14/1000 | Loss: 0.00003768
Iteration 15/1000 | Loss: 0.00003767
Iteration 16/1000 | Loss: 0.00003766
Iteration 17/1000 | Loss: 0.00003765
Iteration 18/1000 | Loss: 0.00003764
Iteration 19/1000 | Loss: 0.00003763
Iteration 20/1000 | Loss: 0.00003762
Iteration 21/1000 | Loss: 0.00003762
Iteration 22/1000 | Loss: 0.00003761
Iteration 23/1000 | Loss: 0.00003759
Iteration 24/1000 | Loss: 0.00003759
Iteration 25/1000 | Loss: 0.00003759
Iteration 26/1000 | Loss: 0.00003759
Iteration 27/1000 | Loss: 0.00003758
Iteration 28/1000 | Loss: 0.00003758
Iteration 29/1000 | Loss: 0.00003758
Iteration 30/1000 | Loss: 0.00003758
Iteration 31/1000 | Loss: 0.00003758
Iteration 32/1000 | Loss: 0.00003758
Iteration 33/1000 | Loss: 0.00003758
Iteration 34/1000 | Loss: 0.00003758
Iteration 35/1000 | Loss: 0.00003757
Iteration 36/1000 | Loss: 0.00003757
Iteration 37/1000 | Loss: 0.00003757
Iteration 38/1000 | Loss: 0.00003756
Iteration 39/1000 | Loss: 0.00003756
Iteration 40/1000 | Loss: 0.00003756
Iteration 41/1000 | Loss: 0.00003756
Iteration 42/1000 | Loss: 0.00003756
Iteration 43/1000 | Loss: 0.00003756
Iteration 44/1000 | Loss: 0.00003755
Iteration 45/1000 | Loss: 0.00003755
Iteration 46/1000 | Loss: 0.00003755
Iteration 47/1000 | Loss: 0.00003752
Iteration 48/1000 | Loss: 0.00003752
Iteration 49/1000 | Loss: 0.00003752
Iteration 50/1000 | Loss: 0.00003752
Iteration 51/1000 | Loss: 0.00003752
Iteration 52/1000 | Loss: 0.00003752
Iteration 53/1000 | Loss: 0.00003752
Iteration 54/1000 | Loss: 0.00003750
Iteration 55/1000 | Loss: 0.00003750
Iteration 56/1000 | Loss: 0.00003749
Iteration 57/1000 | Loss: 0.00003749
Iteration 58/1000 | Loss: 0.00003749
Iteration 59/1000 | Loss: 0.00003749
Iteration 60/1000 | Loss: 0.00003749
Iteration 61/1000 | Loss: 0.00003749
Iteration 62/1000 | Loss: 0.00003749
Iteration 63/1000 | Loss: 0.00003748
Iteration 64/1000 | Loss: 0.00003748
Iteration 65/1000 | Loss: 0.00003748
Iteration 66/1000 | Loss: 0.00003748
Iteration 67/1000 | Loss: 0.00003747
Iteration 68/1000 | Loss: 0.00003747
Iteration 69/1000 | Loss: 0.00003747
Iteration 70/1000 | Loss: 0.00003747
Iteration 71/1000 | Loss: 0.00003747
Iteration 72/1000 | Loss: 0.00003747
Iteration 73/1000 | Loss: 0.00003747
Iteration 74/1000 | Loss: 0.00003746
Iteration 75/1000 | Loss: 0.00003746
Iteration 76/1000 | Loss: 0.00003746
Iteration 77/1000 | Loss: 0.00003746
Iteration 78/1000 | Loss: 0.00003746
Iteration 79/1000 | Loss: 0.00003746
Iteration 80/1000 | Loss: 0.00003746
Iteration 81/1000 | Loss: 0.00003746
Iteration 82/1000 | Loss: 0.00003746
Iteration 83/1000 | Loss: 0.00003746
Iteration 84/1000 | Loss: 0.00003745
Iteration 85/1000 | Loss: 0.00003745
Iteration 86/1000 | Loss: 0.00003745
Iteration 87/1000 | Loss: 0.00003745
Iteration 88/1000 | Loss: 0.00003744
Iteration 89/1000 | Loss: 0.00003744
Iteration 90/1000 | Loss: 0.00003744
Iteration 91/1000 | Loss: 0.00003743
Iteration 92/1000 | Loss: 0.00003743
Iteration 93/1000 | Loss: 0.00003743
Iteration 94/1000 | Loss: 0.00003743
Iteration 95/1000 | Loss: 0.00003743
Iteration 96/1000 | Loss: 0.00003743
Iteration 97/1000 | Loss: 0.00003743
Iteration 98/1000 | Loss: 0.00003743
Iteration 99/1000 | Loss: 0.00003743
Iteration 100/1000 | Loss: 0.00003742
Iteration 101/1000 | Loss: 0.00003740
Iteration 102/1000 | Loss: 0.00003740
Iteration 103/1000 | Loss: 0.00003740
Iteration 104/1000 | Loss: 0.00003739
Iteration 105/1000 | Loss: 0.00003739
Iteration 106/1000 | Loss: 0.00003738
Iteration 107/1000 | Loss: 0.00003738
Iteration 108/1000 | Loss: 0.00003738
Iteration 109/1000 | Loss: 0.00003737
Iteration 110/1000 | Loss: 0.00003737
Iteration 111/1000 | Loss: 0.00003737
Iteration 112/1000 | Loss: 0.00003736
Iteration 113/1000 | Loss: 0.00003736
Iteration 114/1000 | Loss: 0.00003736
Iteration 115/1000 | Loss: 0.00003736
Iteration 116/1000 | Loss: 0.00003736
Iteration 117/1000 | Loss: 0.00003736
Iteration 118/1000 | Loss: 0.00003736
Iteration 119/1000 | Loss: 0.00003736
Iteration 120/1000 | Loss: 0.00003736
Iteration 121/1000 | Loss: 0.00003736
Iteration 122/1000 | Loss: 0.00003735
Iteration 123/1000 | Loss: 0.00003735
Iteration 124/1000 | Loss: 0.00003733
Iteration 125/1000 | Loss: 0.00003733
Iteration 126/1000 | Loss: 0.00003733
Iteration 127/1000 | Loss: 0.00003733
Iteration 128/1000 | Loss: 0.00003733
Iteration 129/1000 | Loss: 0.00003733
Iteration 130/1000 | Loss: 0.00003732
Iteration 131/1000 | Loss: 0.00003732
Iteration 132/1000 | Loss: 0.00003732
Iteration 133/1000 | Loss: 0.00003732
Iteration 134/1000 | Loss: 0.00003732
Iteration 135/1000 | Loss: 0.00003732
Iteration 136/1000 | Loss: 0.00003732
Iteration 137/1000 | Loss: 0.00003731
Iteration 138/1000 | Loss: 0.00003731
Iteration 139/1000 | Loss: 0.00003731
Iteration 140/1000 | Loss: 0.00003731
Iteration 141/1000 | Loss: 0.00003731
Iteration 142/1000 | Loss: 0.00003731
Iteration 143/1000 | Loss: 0.00003730
Iteration 144/1000 | Loss: 0.00003730
Iteration 145/1000 | Loss: 0.00003730
Iteration 146/1000 | Loss: 0.00003730
Iteration 147/1000 | Loss: 0.00003730
Iteration 148/1000 | Loss: 0.00003730
Iteration 149/1000 | Loss: 0.00003730
Iteration 150/1000 | Loss: 0.00003730
Iteration 151/1000 | Loss: 0.00003730
Iteration 152/1000 | Loss: 0.00003730
Iteration 153/1000 | Loss: 0.00003730
Iteration 154/1000 | Loss: 0.00003729
Iteration 155/1000 | Loss: 0.00003729
Iteration 156/1000 | Loss: 0.00003729
Iteration 157/1000 | Loss: 0.00003729
Iteration 158/1000 | Loss: 0.00003728
Iteration 159/1000 | Loss: 0.00003728
Iteration 160/1000 | Loss: 0.00003728
Iteration 161/1000 | Loss: 0.00003728
Iteration 162/1000 | Loss: 0.00003728
Iteration 163/1000 | Loss: 0.00003727
Iteration 164/1000 | Loss: 0.00003727
Iteration 165/1000 | Loss: 0.00003727
Iteration 166/1000 | Loss: 0.00003727
Iteration 167/1000 | Loss: 0.00003727
Iteration 168/1000 | Loss: 0.00003727
Iteration 169/1000 | Loss: 0.00003727
Iteration 170/1000 | Loss: 0.00003727
Iteration 171/1000 | Loss: 0.00003727
Iteration 172/1000 | Loss: 0.00003726
Iteration 173/1000 | Loss: 0.00003726
Iteration 174/1000 | Loss: 0.00003726
Iteration 175/1000 | Loss: 0.00003726
Iteration 176/1000 | Loss: 0.00003726
Iteration 177/1000 | Loss: 0.00003726
Iteration 178/1000 | Loss: 0.00003725
Iteration 179/1000 | Loss: 0.00003725
Iteration 180/1000 | Loss: 0.00003725
Iteration 181/1000 | Loss: 0.00003725
Iteration 182/1000 | Loss: 0.00003725
Iteration 183/1000 | Loss: 0.00003724
Iteration 184/1000 | Loss: 0.00003724
Iteration 185/1000 | Loss: 0.00003724
Iteration 186/1000 | Loss: 0.00003724
Iteration 187/1000 | Loss: 0.00003723
Iteration 188/1000 | Loss: 0.00003723
Iteration 189/1000 | Loss: 0.00003723
Iteration 190/1000 | Loss: 0.00003723
Iteration 191/1000 | Loss: 0.00003723
Iteration 192/1000 | Loss: 0.00003723
Iteration 193/1000 | Loss: 0.00003723
Iteration 194/1000 | Loss: 0.00003723
Iteration 195/1000 | Loss: 0.00003723
Iteration 196/1000 | Loss: 0.00003723
Iteration 197/1000 | Loss: 0.00003723
Iteration 198/1000 | Loss: 0.00003723
Iteration 199/1000 | Loss: 0.00003723
Iteration 200/1000 | Loss: 0.00003723
Iteration 201/1000 | Loss: 0.00003723
Iteration 202/1000 | Loss: 0.00003722
Iteration 203/1000 | Loss: 0.00003722
Iteration 204/1000 | Loss: 0.00003722
Iteration 205/1000 | Loss: 0.00003722
Iteration 206/1000 | Loss: 0.00003722
Iteration 207/1000 | Loss: 0.00003722
Iteration 208/1000 | Loss: 0.00003722
Iteration 209/1000 | Loss: 0.00003722
Iteration 210/1000 | Loss: 0.00003722
Iteration 211/1000 | Loss: 0.00003722
Iteration 212/1000 | Loss: 0.00003722
Iteration 213/1000 | Loss: 0.00003722
Iteration 214/1000 | Loss: 0.00003722
Iteration 215/1000 | Loss: 0.00003722
Iteration 216/1000 | Loss: 0.00003721
Iteration 217/1000 | Loss: 0.00003721
Iteration 218/1000 | Loss: 0.00003721
Iteration 219/1000 | Loss: 0.00003721
Iteration 220/1000 | Loss: 0.00003721
Iteration 221/1000 | Loss: 0.00003721
Iteration 222/1000 | Loss: 0.00003721
Iteration 223/1000 | Loss: 0.00003721
Iteration 224/1000 | Loss: 0.00003721
Iteration 225/1000 | Loss: 0.00003721
Iteration 226/1000 | Loss: 0.00003721
Iteration 227/1000 | Loss: 0.00003721
Iteration 228/1000 | Loss: 0.00003721
Iteration 229/1000 | Loss: 0.00003721
Iteration 230/1000 | Loss: 0.00003721
Iteration 231/1000 | Loss: 0.00003721
Iteration 232/1000 | Loss: 0.00003721
Iteration 233/1000 | Loss: 0.00003721
Iteration 234/1000 | Loss: 0.00003721
Iteration 235/1000 | Loss: 0.00003721
Iteration 236/1000 | Loss: 0.00003721
Iteration 237/1000 | Loss: 0.00003721
Iteration 238/1000 | Loss: 0.00003721
Iteration 239/1000 | Loss: 0.00003721
Iteration 240/1000 | Loss: 0.00003721
Iteration 241/1000 | Loss: 0.00003721
Iteration 242/1000 | Loss: 0.00003721
Iteration 243/1000 | Loss: 0.00003720
Iteration 244/1000 | Loss: 0.00003720
Iteration 245/1000 | Loss: 0.00003720
Iteration 246/1000 | Loss: 0.00003720
Iteration 247/1000 | Loss: 0.00003720
Iteration 248/1000 | Loss: 0.00003720
Iteration 249/1000 | Loss: 0.00003720
Iteration 250/1000 | Loss: 0.00003720
Iteration 251/1000 | Loss: 0.00003720
Iteration 252/1000 | Loss: 0.00003720
Iteration 253/1000 | Loss: 0.00003720
Iteration 254/1000 | Loss: 0.00003719
Iteration 255/1000 | Loss: 0.00003719
Iteration 256/1000 | Loss: 0.00003719
Iteration 257/1000 | Loss: 0.00003719
Iteration 258/1000 | Loss: 0.00003719
Iteration 259/1000 | Loss: 0.00003719
Iteration 260/1000 | Loss: 0.00003719
Iteration 261/1000 | Loss: 0.00003719
Iteration 262/1000 | Loss: 0.00003719
Iteration 263/1000 | Loss: 0.00003719
Iteration 264/1000 | Loss: 0.00003719
Iteration 265/1000 | Loss: 0.00003719
Iteration 266/1000 | Loss: 0.00003719
Iteration 267/1000 | Loss: 0.00003719
Iteration 268/1000 | Loss: 0.00003719
Iteration 269/1000 | Loss: 0.00003719
Iteration 270/1000 | Loss: 0.00003718
Iteration 271/1000 | Loss: 0.00003718
Iteration 272/1000 | Loss: 0.00003718
Iteration 273/1000 | Loss: 0.00003718
Iteration 274/1000 | Loss: 0.00003718
Iteration 275/1000 | Loss: 0.00003718
Iteration 276/1000 | Loss: 0.00003718
Iteration 277/1000 | Loss: 0.00003718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [3.718430525623262e-05, 3.718430525623262e-05, 3.718430525623262e-05, 3.718430525623262e-05, 3.718430525623262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.718430525623262e-05

Optimization complete. Final v2v error: 4.976844310760498 mm

Highest mean error: 5.3597846031188965 mm for frame 69

Lowest mean error: 3.961289405822754 mm for frame 3

Saving results

Total time: 43.47930335998535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109022
Iteration 2/25 | Loss: 0.00298337
Iteration 3/25 | Loss: 0.00186331
Iteration 4/25 | Loss: 0.00175709
Iteration 5/25 | Loss: 0.00174053
Iteration 6/25 | Loss: 0.00152981
Iteration 7/25 | Loss: 0.00151737
Iteration 8/25 | Loss: 0.00145251
Iteration 9/25 | Loss: 0.00134467
Iteration 10/25 | Loss: 0.00131148
Iteration 11/25 | Loss: 0.00134308
Iteration 12/25 | Loss: 0.00131688
Iteration 13/25 | Loss: 0.00130208
Iteration 14/25 | Loss: 0.00130789
Iteration 15/25 | Loss: 0.00130038
Iteration 16/25 | Loss: 0.00129415
Iteration 17/25 | Loss: 0.00129365
Iteration 18/25 | Loss: 0.00128269
Iteration 19/25 | Loss: 0.00128285
Iteration 20/25 | Loss: 0.00127985
Iteration 21/25 | Loss: 0.00128010
Iteration 22/25 | Loss: 0.00127207
Iteration 23/25 | Loss: 0.00127407
Iteration 24/25 | Loss: 0.00127293
Iteration 25/25 | Loss: 0.00127098

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50385737
Iteration 2/25 | Loss: 0.00377154
Iteration 3/25 | Loss: 0.00323489
Iteration 4/25 | Loss: 0.00323489
Iteration 5/25 | Loss: 0.00323489
Iteration 6/25 | Loss: 0.00323489
Iteration 7/25 | Loss: 0.00323489
Iteration 8/25 | Loss: 0.00323489
Iteration 9/25 | Loss: 0.00323489
Iteration 10/25 | Loss: 0.00323489
Iteration 11/25 | Loss: 0.00323489
Iteration 12/25 | Loss: 0.00323489
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0032348879612982273, 0.0032348879612982273, 0.0032348879612982273, 0.0032348879612982273, 0.0032348879612982273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032348879612982273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00323489
Iteration 2/1000 | Loss: 0.00241767
Iteration 3/1000 | Loss: 0.00203323
Iteration 4/1000 | Loss: 0.00154571
Iteration 5/1000 | Loss: 0.00160842
Iteration 6/1000 | Loss: 0.00032244
Iteration 7/1000 | Loss: 0.00027386
Iteration 8/1000 | Loss: 0.00035629
Iteration 9/1000 | Loss: 0.00031692
Iteration 10/1000 | Loss: 0.00027676
Iteration 11/1000 | Loss: 0.00054260
Iteration 12/1000 | Loss: 0.00022250
Iteration 13/1000 | Loss: 0.00015065
Iteration 14/1000 | Loss: 0.00018361
Iteration 15/1000 | Loss: 0.00062199
Iteration 16/1000 | Loss: 0.00023314
Iteration 17/1000 | Loss: 0.00023567
Iteration 18/1000 | Loss: 0.00017387
Iteration 19/1000 | Loss: 0.00043480
Iteration 20/1000 | Loss: 0.00012938
Iteration 21/1000 | Loss: 0.00009765
Iteration 22/1000 | Loss: 0.00025551
Iteration 23/1000 | Loss: 0.00015049
Iteration 24/1000 | Loss: 0.00013120
Iteration 25/1000 | Loss: 0.00037175
Iteration 26/1000 | Loss: 0.00028335
Iteration 27/1000 | Loss: 0.00012678
Iteration 28/1000 | Loss: 0.00035486
Iteration 29/1000 | Loss: 0.00019324
Iteration 30/1000 | Loss: 0.00005779
Iteration 31/1000 | Loss: 0.00005125
Iteration 32/1000 | Loss: 0.00004947
Iteration 33/1000 | Loss: 0.00011774
Iteration 34/1000 | Loss: 0.00021654
Iteration 35/1000 | Loss: 0.00018289
Iteration 36/1000 | Loss: 0.00011044
Iteration 37/1000 | Loss: 0.00017821
Iteration 38/1000 | Loss: 0.00008689
Iteration 39/1000 | Loss: 0.00009924
Iteration 40/1000 | Loss: 0.00007808
Iteration 41/1000 | Loss: 0.00007045
Iteration 42/1000 | Loss: 0.00004475
Iteration 43/1000 | Loss: 0.00016550
Iteration 44/1000 | Loss: 0.00008353
Iteration 45/1000 | Loss: 0.00029482
Iteration 46/1000 | Loss: 0.00019991
Iteration 47/1000 | Loss: 0.00008163
Iteration 48/1000 | Loss: 0.00004498
Iteration 49/1000 | Loss: 0.00008701
Iteration 50/1000 | Loss: 0.00007902
Iteration 51/1000 | Loss: 0.00011157
Iteration 52/1000 | Loss: 0.00008094
Iteration 53/1000 | Loss: 0.00009665
Iteration 54/1000 | Loss: 0.00007577
Iteration 55/1000 | Loss: 0.00046782
Iteration 56/1000 | Loss: 0.00116765
Iteration 57/1000 | Loss: 0.00010810
Iteration 58/1000 | Loss: 0.00011939
Iteration 59/1000 | Loss: 0.00020481
Iteration 60/1000 | Loss: 0.00004362
Iteration 61/1000 | Loss: 0.00010280
Iteration 62/1000 | Loss: 0.00004226
Iteration 63/1000 | Loss: 0.00007703
Iteration 64/1000 | Loss: 0.00006855
Iteration 65/1000 | Loss: 0.00003707
Iteration 66/1000 | Loss: 0.00004058
Iteration 67/1000 | Loss: 0.00003631
Iteration 68/1000 | Loss: 0.00003618
Iteration 69/1000 | Loss: 0.00007508
Iteration 70/1000 | Loss: 0.00005485
Iteration 71/1000 | Loss: 0.00003601
Iteration 72/1000 | Loss: 0.00005950
Iteration 73/1000 | Loss: 0.00003587
Iteration 74/1000 | Loss: 0.00011181
Iteration 75/1000 | Loss: 0.00004567
Iteration 76/1000 | Loss: 0.00003574
Iteration 77/1000 | Loss: 0.00007938
Iteration 78/1000 | Loss: 0.00003568
Iteration 79/1000 | Loss: 0.00003564
Iteration 80/1000 | Loss: 0.00003564
Iteration 81/1000 | Loss: 0.00003564
Iteration 82/1000 | Loss: 0.00003564
Iteration 83/1000 | Loss: 0.00003563
Iteration 84/1000 | Loss: 0.00003563
Iteration 85/1000 | Loss: 0.00003563
Iteration 86/1000 | Loss: 0.00003563
Iteration 87/1000 | Loss: 0.00003563
Iteration 88/1000 | Loss: 0.00003563
Iteration 89/1000 | Loss: 0.00003563
Iteration 90/1000 | Loss: 0.00003563
Iteration 91/1000 | Loss: 0.00003563
Iteration 92/1000 | Loss: 0.00003562
Iteration 93/1000 | Loss: 0.00010235
Iteration 94/1000 | Loss: 0.00003579
Iteration 95/1000 | Loss: 0.00008224
Iteration 96/1000 | Loss: 0.00004151
Iteration 97/1000 | Loss: 0.00004592
Iteration 98/1000 | Loss: 0.00003553
Iteration 99/1000 | Loss: 0.00003553
Iteration 100/1000 | Loss: 0.00003553
Iteration 101/1000 | Loss: 0.00003552
Iteration 102/1000 | Loss: 0.00003552
Iteration 103/1000 | Loss: 0.00003552
Iteration 104/1000 | Loss: 0.00003552
Iteration 105/1000 | Loss: 0.00003552
Iteration 106/1000 | Loss: 0.00003552
Iteration 107/1000 | Loss: 0.00003552
Iteration 108/1000 | Loss: 0.00003552
Iteration 109/1000 | Loss: 0.00003551
Iteration 110/1000 | Loss: 0.00003551
Iteration 111/1000 | Loss: 0.00003550
Iteration 112/1000 | Loss: 0.00003550
Iteration 113/1000 | Loss: 0.00003550
Iteration 114/1000 | Loss: 0.00003550
Iteration 115/1000 | Loss: 0.00003550
Iteration 116/1000 | Loss: 0.00003549
Iteration 117/1000 | Loss: 0.00003549
Iteration 118/1000 | Loss: 0.00003549
Iteration 119/1000 | Loss: 0.00003549
Iteration 120/1000 | Loss: 0.00003549
Iteration 121/1000 | Loss: 0.00003549
Iteration 122/1000 | Loss: 0.00003549
Iteration 123/1000 | Loss: 0.00003549
Iteration 124/1000 | Loss: 0.00003548
Iteration 125/1000 | Loss: 0.00003548
Iteration 126/1000 | Loss: 0.00003547
Iteration 127/1000 | Loss: 0.00003547
Iteration 128/1000 | Loss: 0.00003547
Iteration 129/1000 | Loss: 0.00003547
Iteration 130/1000 | Loss: 0.00003546
Iteration 131/1000 | Loss: 0.00003546
Iteration 132/1000 | Loss: 0.00003546
Iteration 133/1000 | Loss: 0.00003546
Iteration 134/1000 | Loss: 0.00003546
Iteration 135/1000 | Loss: 0.00003546
Iteration 136/1000 | Loss: 0.00003546
Iteration 137/1000 | Loss: 0.00003546
Iteration 138/1000 | Loss: 0.00003546
Iteration 139/1000 | Loss: 0.00003546
Iteration 140/1000 | Loss: 0.00003546
Iteration 141/1000 | Loss: 0.00003545
Iteration 142/1000 | Loss: 0.00003545
Iteration 143/1000 | Loss: 0.00003544
Iteration 144/1000 | Loss: 0.00003544
Iteration 145/1000 | Loss: 0.00003544
Iteration 146/1000 | Loss: 0.00003543
Iteration 147/1000 | Loss: 0.00003543
Iteration 148/1000 | Loss: 0.00003542
Iteration 149/1000 | Loss: 0.00007110
Iteration 150/1000 | Loss: 0.00007510
Iteration 151/1000 | Loss: 0.00007510
Iteration 152/1000 | Loss: 0.00007510
Iteration 153/1000 | Loss: 0.00007510
Iteration 154/1000 | Loss: 0.00007510
Iteration 155/1000 | Loss: 0.00007510
Iteration 156/1000 | Loss: 0.00007510
Iteration 157/1000 | Loss: 0.00007510
Iteration 158/1000 | Loss: 0.00007510
Iteration 159/1000 | Loss: 0.00007510
Iteration 160/1000 | Loss: 0.00007510
Iteration 161/1000 | Loss: 0.00007510
Iteration 162/1000 | Loss: 0.00007510
Iteration 163/1000 | Loss: 0.00007510
Iteration 164/1000 | Loss: 0.00007510
Iteration 165/1000 | Loss: 0.00007510
Iteration 166/1000 | Loss: 0.00007510
Iteration 167/1000 | Loss: 0.00007510
Iteration 168/1000 | Loss: 0.00007510
Iteration 169/1000 | Loss: 0.00007510
Iteration 170/1000 | Loss: 0.00007510
Iteration 171/1000 | Loss: 0.00007510
Iteration 172/1000 | Loss: 0.00007510
Iteration 173/1000 | Loss: 0.00007510
Iteration 174/1000 | Loss: 0.00007510
Iteration 175/1000 | Loss: 0.00007510
Iteration 176/1000 | Loss: 0.00007510
Iteration 177/1000 | Loss: 0.00007510
Iteration 178/1000 | Loss: 0.00007510
Iteration 179/1000 | Loss: 0.00007510
Iteration 180/1000 | Loss: 0.00007510
Iteration 181/1000 | Loss: 0.00007510
Iteration 182/1000 | Loss: 0.00007510
Iteration 183/1000 | Loss: 0.00007510
Iteration 184/1000 | Loss: 0.00007510
Iteration 185/1000 | Loss: 0.00007510
Iteration 186/1000 | Loss: 0.00007510
Iteration 187/1000 | Loss: 0.00007510
Iteration 188/1000 | Loss: 0.00007510
Iteration 189/1000 | Loss: 0.00007510
Iteration 190/1000 | Loss: 0.00007510
Iteration 191/1000 | Loss: 0.00007510
Iteration 192/1000 | Loss: 0.00007510
Iteration 193/1000 | Loss: 0.00007510
Iteration 194/1000 | Loss: 0.00007510
Iteration 195/1000 | Loss: 0.00007510
Iteration 196/1000 | Loss: 0.00007510
Iteration 197/1000 | Loss: 0.00007510
Iteration 198/1000 | Loss: 0.00007510
Iteration 199/1000 | Loss: 0.00007510
Iteration 200/1000 | Loss: 0.00007510
Iteration 201/1000 | Loss: 0.00007510
Iteration 202/1000 | Loss: 0.00007510
Iteration 203/1000 | Loss: 0.00007510
Iteration 204/1000 | Loss: 0.00007510
Iteration 205/1000 | Loss: 0.00007510
Iteration 206/1000 | Loss: 0.00007510
Iteration 207/1000 | Loss: 0.00007510
Iteration 208/1000 | Loss: 0.00007510
Iteration 209/1000 | Loss: 0.00007510
Iteration 210/1000 | Loss: 0.00007510
Iteration 211/1000 | Loss: 0.00007510
Iteration 212/1000 | Loss: 0.00007510
Iteration 213/1000 | Loss: 0.00007510
Iteration 214/1000 | Loss: 0.00007510
Iteration 215/1000 | Loss: 0.00007510
Iteration 216/1000 | Loss: 0.00007510
Iteration 217/1000 | Loss: 0.00007510
Iteration 218/1000 | Loss: 0.00007510
Iteration 219/1000 | Loss: 0.00007510
Iteration 220/1000 | Loss: 0.00007510
Iteration 221/1000 | Loss: 0.00007510
Iteration 222/1000 | Loss: 0.00007510
Iteration 223/1000 | Loss: 0.00007510
Iteration 224/1000 | Loss: 0.00007510
Iteration 225/1000 | Loss: 0.00007510
Iteration 226/1000 | Loss: 0.00007510
Iteration 227/1000 | Loss: 0.00007510
Iteration 228/1000 | Loss: 0.00007510
Iteration 229/1000 | Loss: 0.00007510
Iteration 230/1000 | Loss: 0.00007510
Iteration 231/1000 | Loss: 0.00007510
Iteration 232/1000 | Loss: 0.00007510
Iteration 233/1000 | Loss: 0.00007510
Iteration 234/1000 | Loss: 0.00007510
Iteration 235/1000 | Loss: 0.00007510
Iteration 236/1000 | Loss: 0.00007510
Iteration 237/1000 | Loss: 0.00007510
Iteration 238/1000 | Loss: 0.00007510
Iteration 239/1000 | Loss: 0.00007510
Iteration 240/1000 | Loss: 0.00007510
Iteration 241/1000 | Loss: 0.00007510
Iteration 242/1000 | Loss: 0.00007510
Iteration 243/1000 | Loss: 0.00075817
Iteration 244/1000 | Loss: 0.00005086
Iteration 245/1000 | Loss: 0.00004776
Iteration 246/1000 | Loss: 0.00005746
Iteration 247/1000 | Loss: 0.00003545
Iteration 248/1000 | Loss: 0.00003543
Iteration 249/1000 | Loss: 0.00005333
Iteration 250/1000 | Loss: 0.00003542
Iteration 251/1000 | Loss: 0.00009962
Iteration 252/1000 | Loss: 0.00008385
Iteration 253/1000 | Loss: 0.00003559
Iteration 254/1000 | Loss: 0.00003538
Iteration 255/1000 | Loss: 0.00003538
Iteration 256/1000 | Loss: 0.00003536
Iteration 257/1000 | Loss: 0.00003535
Iteration 258/1000 | Loss: 0.00003535
Iteration 259/1000 | Loss: 0.00003535
Iteration 260/1000 | Loss: 0.00003535
Iteration 261/1000 | Loss: 0.00003535
Iteration 262/1000 | Loss: 0.00003535
Iteration 263/1000 | Loss: 0.00003535
Iteration 264/1000 | Loss: 0.00003535
Iteration 265/1000 | Loss: 0.00003535
Iteration 266/1000 | Loss: 0.00003535
Iteration 267/1000 | Loss: 0.00006169
Iteration 268/1000 | Loss: 0.00004237
Iteration 269/1000 | Loss: 0.00006836
Iteration 270/1000 | Loss: 0.00018682
Iteration 271/1000 | Loss: 0.00004479
Iteration 272/1000 | Loss: 0.00003549
Iteration 273/1000 | Loss: 0.00005697
Iteration 274/1000 | Loss: 0.00003733
Iteration 275/1000 | Loss: 0.00003669
Iteration 276/1000 | Loss: 0.00003542
Iteration 277/1000 | Loss: 0.00003957
Iteration 278/1000 | Loss: 0.00007052
Iteration 279/1000 | Loss: 0.00007052
Iteration 280/1000 | Loss: 0.00005018
Iteration 281/1000 | Loss: 0.00003756
Iteration 282/1000 | Loss: 0.00005192
Iteration 283/1000 | Loss: 0.00003887
Iteration 284/1000 | Loss: 0.00003543
Iteration 285/1000 | Loss: 0.00003538
Iteration 286/1000 | Loss: 0.00003538
Iteration 287/1000 | Loss: 0.00003538
Iteration 288/1000 | Loss: 0.00003538
Iteration 289/1000 | Loss: 0.00003537
Iteration 290/1000 | Loss: 0.00003537
Iteration 291/1000 | Loss: 0.00003537
Iteration 292/1000 | Loss: 0.00003537
Iteration 293/1000 | Loss: 0.00003537
Iteration 294/1000 | Loss: 0.00003537
Iteration 295/1000 | Loss: 0.00003537
Iteration 296/1000 | Loss: 0.00003537
Iteration 297/1000 | Loss: 0.00003537
Iteration 298/1000 | Loss: 0.00003537
Iteration 299/1000 | Loss: 0.00003537
Iteration 300/1000 | Loss: 0.00003537
Iteration 301/1000 | Loss: 0.00003537
Iteration 302/1000 | Loss: 0.00003536
Iteration 303/1000 | Loss: 0.00003536
Iteration 304/1000 | Loss: 0.00003536
Iteration 305/1000 | Loss: 0.00003535
Iteration 306/1000 | Loss: 0.00003915
Iteration 307/1000 | Loss: 0.00003535
Iteration 308/1000 | Loss: 0.00003535
Iteration 309/1000 | Loss: 0.00003534
Iteration 310/1000 | Loss: 0.00003534
Iteration 311/1000 | Loss: 0.00003534
Iteration 312/1000 | Loss: 0.00003534
Iteration 313/1000 | Loss: 0.00003534
Iteration 314/1000 | Loss: 0.00003534
Iteration 315/1000 | Loss: 0.00003534
Iteration 316/1000 | Loss: 0.00003534
Iteration 317/1000 | Loss: 0.00003534
Iteration 318/1000 | Loss: 0.00003534
Iteration 319/1000 | Loss: 0.00003534
Iteration 320/1000 | Loss: 0.00003533
Iteration 321/1000 | Loss: 0.00003533
Iteration 322/1000 | Loss: 0.00003533
Iteration 323/1000 | Loss: 0.00003533
Iteration 324/1000 | Loss: 0.00004207
Iteration 325/1000 | Loss: 0.00003533
Iteration 326/1000 | Loss: 0.00003533
Iteration 327/1000 | Loss: 0.00003533
Iteration 328/1000 | Loss: 0.00003533
Iteration 329/1000 | Loss: 0.00003533
Iteration 330/1000 | Loss: 0.00003533
Iteration 331/1000 | Loss: 0.00003533
Iteration 332/1000 | Loss: 0.00003533
Iteration 333/1000 | Loss: 0.00003533
Iteration 334/1000 | Loss: 0.00003533
Iteration 335/1000 | Loss: 0.00003533
Iteration 336/1000 | Loss: 0.00003533
Iteration 337/1000 | Loss: 0.00003533
Iteration 338/1000 | Loss: 0.00003533
Iteration 339/1000 | Loss: 0.00003533
Iteration 340/1000 | Loss: 0.00003533
Iteration 341/1000 | Loss: 0.00003533
Iteration 342/1000 | Loss: 0.00003533
Iteration 343/1000 | Loss: 0.00003533
Iteration 344/1000 | Loss: 0.00003533
Iteration 345/1000 | Loss: 0.00003533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 345. Stopping optimization.
Last 5 losses: [3.532678238116205e-05, 3.532678238116205e-05, 3.532678238116205e-05, 3.532678238116205e-05, 3.532678238116205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.532678238116205e-05

Optimization complete. Final v2v error: 4.510890483856201 mm

Highest mean error: 13.002946853637695 mm for frame 155

Lowest mean error: 4.023514747619629 mm for frame 18

Saving results

Total time: 237.10887694358826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470631
Iteration 2/25 | Loss: 0.00109280
Iteration 3/25 | Loss: 0.00096433
Iteration 4/25 | Loss: 0.00094636
Iteration 5/25 | Loss: 0.00093901
Iteration 6/25 | Loss: 0.00093699
Iteration 7/25 | Loss: 0.00093635
Iteration 8/25 | Loss: 0.00093635
Iteration 9/25 | Loss: 0.00093635
Iteration 10/25 | Loss: 0.00093635
Iteration 11/25 | Loss: 0.00093635
Iteration 12/25 | Loss: 0.00093635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00093634839868173, 0.00093634839868173, 0.00093634839868173, 0.00093634839868173, 0.00093634839868173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00093634839868173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49920738
Iteration 2/25 | Loss: 0.00073851
Iteration 3/25 | Loss: 0.00073850
Iteration 4/25 | Loss: 0.00073850
Iteration 5/25 | Loss: 0.00073850
Iteration 6/25 | Loss: 0.00073850
Iteration 7/25 | Loss: 0.00073850
Iteration 8/25 | Loss: 0.00073850
Iteration 9/25 | Loss: 0.00073850
Iteration 10/25 | Loss: 0.00073850
Iteration 11/25 | Loss: 0.00073850
Iteration 12/25 | Loss: 0.00073850
Iteration 13/25 | Loss: 0.00073850
Iteration 14/25 | Loss: 0.00073850
Iteration 15/25 | Loss: 0.00073850
Iteration 16/25 | Loss: 0.00073850
Iteration 17/25 | Loss: 0.00073850
Iteration 18/25 | Loss: 0.00073850
Iteration 19/25 | Loss: 0.00073850
Iteration 20/25 | Loss: 0.00073850
Iteration 21/25 | Loss: 0.00073850
Iteration 22/25 | Loss: 0.00073850
Iteration 23/25 | Loss: 0.00073850
Iteration 24/25 | Loss: 0.00073850
Iteration 25/25 | Loss: 0.00073850

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073850
Iteration 2/1000 | Loss: 0.00002481
Iteration 3/1000 | Loss: 0.00001968
Iteration 4/1000 | Loss: 0.00001813
Iteration 5/1000 | Loss: 0.00001754
Iteration 6/1000 | Loss: 0.00001715
Iteration 7/1000 | Loss: 0.00001685
Iteration 8/1000 | Loss: 0.00001660
Iteration 9/1000 | Loss: 0.00001648
Iteration 10/1000 | Loss: 0.00001648
Iteration 11/1000 | Loss: 0.00001647
Iteration 12/1000 | Loss: 0.00001646
Iteration 13/1000 | Loss: 0.00001646
Iteration 14/1000 | Loss: 0.00001645
Iteration 15/1000 | Loss: 0.00001645
Iteration 16/1000 | Loss: 0.00001644
Iteration 17/1000 | Loss: 0.00001644
Iteration 18/1000 | Loss: 0.00001643
Iteration 19/1000 | Loss: 0.00001643
Iteration 20/1000 | Loss: 0.00001642
Iteration 21/1000 | Loss: 0.00001642
Iteration 22/1000 | Loss: 0.00001642
Iteration 23/1000 | Loss: 0.00001642
Iteration 24/1000 | Loss: 0.00001642
Iteration 25/1000 | Loss: 0.00001641
Iteration 26/1000 | Loss: 0.00001640
Iteration 27/1000 | Loss: 0.00001640
Iteration 28/1000 | Loss: 0.00001639
Iteration 29/1000 | Loss: 0.00001638
Iteration 30/1000 | Loss: 0.00001637
Iteration 31/1000 | Loss: 0.00001637
Iteration 32/1000 | Loss: 0.00001634
Iteration 33/1000 | Loss: 0.00001633
Iteration 34/1000 | Loss: 0.00001631
Iteration 35/1000 | Loss: 0.00001631
Iteration 36/1000 | Loss: 0.00001630
Iteration 37/1000 | Loss: 0.00001630
Iteration 38/1000 | Loss: 0.00001629
Iteration 39/1000 | Loss: 0.00001629
Iteration 40/1000 | Loss: 0.00001629
Iteration 41/1000 | Loss: 0.00001628
Iteration 42/1000 | Loss: 0.00001628
Iteration 43/1000 | Loss: 0.00001628
Iteration 44/1000 | Loss: 0.00001628
Iteration 45/1000 | Loss: 0.00001627
Iteration 46/1000 | Loss: 0.00001627
Iteration 47/1000 | Loss: 0.00001627
Iteration 48/1000 | Loss: 0.00001627
Iteration 49/1000 | Loss: 0.00001626
Iteration 50/1000 | Loss: 0.00001626
Iteration 51/1000 | Loss: 0.00001625
Iteration 52/1000 | Loss: 0.00001625
Iteration 53/1000 | Loss: 0.00001625
Iteration 54/1000 | Loss: 0.00001625
Iteration 55/1000 | Loss: 0.00001625
Iteration 56/1000 | Loss: 0.00001625
Iteration 57/1000 | Loss: 0.00001625
Iteration 58/1000 | Loss: 0.00001625
Iteration 59/1000 | Loss: 0.00001625
Iteration 60/1000 | Loss: 0.00001625
Iteration 61/1000 | Loss: 0.00001624
Iteration 62/1000 | Loss: 0.00001624
Iteration 63/1000 | Loss: 0.00001624
Iteration 64/1000 | Loss: 0.00001624
Iteration 65/1000 | Loss: 0.00001623
Iteration 66/1000 | Loss: 0.00001623
Iteration 67/1000 | Loss: 0.00001623
Iteration 68/1000 | Loss: 0.00001623
Iteration 69/1000 | Loss: 0.00001623
Iteration 70/1000 | Loss: 0.00001622
Iteration 71/1000 | Loss: 0.00001622
Iteration 72/1000 | Loss: 0.00001622
Iteration 73/1000 | Loss: 0.00001622
Iteration 74/1000 | Loss: 0.00001622
Iteration 75/1000 | Loss: 0.00001622
Iteration 76/1000 | Loss: 0.00001622
Iteration 77/1000 | Loss: 0.00001622
Iteration 78/1000 | Loss: 0.00001622
Iteration 79/1000 | Loss: 0.00001622
Iteration 80/1000 | Loss: 0.00001622
Iteration 81/1000 | Loss: 0.00001622
Iteration 82/1000 | Loss: 0.00001622
Iteration 83/1000 | Loss: 0.00001622
Iteration 84/1000 | Loss: 0.00001621
Iteration 85/1000 | Loss: 0.00001621
Iteration 86/1000 | Loss: 0.00001621
Iteration 87/1000 | Loss: 0.00001621
Iteration 88/1000 | Loss: 0.00001621
Iteration 89/1000 | Loss: 0.00001621
Iteration 90/1000 | Loss: 0.00001621
Iteration 91/1000 | Loss: 0.00001621
Iteration 92/1000 | Loss: 0.00001621
Iteration 93/1000 | Loss: 0.00001621
Iteration 94/1000 | Loss: 0.00001621
Iteration 95/1000 | Loss: 0.00001621
Iteration 96/1000 | Loss: 0.00001621
Iteration 97/1000 | Loss: 0.00001621
Iteration 98/1000 | Loss: 0.00001621
Iteration 99/1000 | Loss: 0.00001621
Iteration 100/1000 | Loss: 0.00001621
Iteration 101/1000 | Loss: 0.00001621
Iteration 102/1000 | Loss: 0.00001620
Iteration 103/1000 | Loss: 0.00001620
Iteration 104/1000 | Loss: 0.00001620
Iteration 105/1000 | Loss: 0.00001620
Iteration 106/1000 | Loss: 0.00001620
Iteration 107/1000 | Loss: 0.00001620
Iteration 108/1000 | Loss: 0.00001620
Iteration 109/1000 | Loss: 0.00001620
Iteration 110/1000 | Loss: 0.00001620
Iteration 111/1000 | Loss: 0.00001620
Iteration 112/1000 | Loss: 0.00001620
Iteration 113/1000 | Loss: 0.00001620
Iteration 114/1000 | Loss: 0.00001620
Iteration 115/1000 | Loss: 0.00001620
Iteration 116/1000 | Loss: 0.00001620
Iteration 117/1000 | Loss: 0.00001620
Iteration 118/1000 | Loss: 0.00001620
Iteration 119/1000 | Loss: 0.00001619
Iteration 120/1000 | Loss: 0.00001619
Iteration 121/1000 | Loss: 0.00001619
Iteration 122/1000 | Loss: 0.00001619
Iteration 123/1000 | Loss: 0.00001619
Iteration 124/1000 | Loss: 0.00001619
Iteration 125/1000 | Loss: 0.00001619
Iteration 126/1000 | Loss: 0.00001619
Iteration 127/1000 | Loss: 0.00001619
Iteration 128/1000 | Loss: 0.00001619
Iteration 129/1000 | Loss: 0.00001619
Iteration 130/1000 | Loss: 0.00001619
Iteration 131/1000 | Loss: 0.00001619
Iteration 132/1000 | Loss: 0.00001618
Iteration 133/1000 | Loss: 0.00001618
Iteration 134/1000 | Loss: 0.00001618
Iteration 135/1000 | Loss: 0.00001618
Iteration 136/1000 | Loss: 0.00001618
Iteration 137/1000 | Loss: 0.00001618
Iteration 138/1000 | Loss: 0.00001618
Iteration 139/1000 | Loss: 0.00001618
Iteration 140/1000 | Loss: 0.00001618
Iteration 141/1000 | Loss: 0.00001618
Iteration 142/1000 | Loss: 0.00001618
Iteration 143/1000 | Loss: 0.00001617
Iteration 144/1000 | Loss: 0.00001617
Iteration 145/1000 | Loss: 0.00001617
Iteration 146/1000 | Loss: 0.00001617
Iteration 147/1000 | Loss: 0.00001617
Iteration 148/1000 | Loss: 0.00001617
Iteration 149/1000 | Loss: 0.00001617
Iteration 150/1000 | Loss: 0.00001617
Iteration 151/1000 | Loss: 0.00001617
Iteration 152/1000 | Loss: 0.00001617
Iteration 153/1000 | Loss: 0.00001617
Iteration 154/1000 | Loss: 0.00001617
Iteration 155/1000 | Loss: 0.00001617
Iteration 156/1000 | Loss: 0.00001617
Iteration 157/1000 | Loss: 0.00001617
Iteration 158/1000 | Loss: 0.00001617
Iteration 159/1000 | Loss: 0.00001617
Iteration 160/1000 | Loss: 0.00001616
Iteration 161/1000 | Loss: 0.00001616
Iteration 162/1000 | Loss: 0.00001616
Iteration 163/1000 | Loss: 0.00001616
Iteration 164/1000 | Loss: 0.00001616
Iteration 165/1000 | Loss: 0.00001616
Iteration 166/1000 | Loss: 0.00001616
Iteration 167/1000 | Loss: 0.00001616
Iteration 168/1000 | Loss: 0.00001615
Iteration 169/1000 | Loss: 0.00001615
Iteration 170/1000 | Loss: 0.00001615
Iteration 171/1000 | Loss: 0.00001615
Iteration 172/1000 | Loss: 0.00001615
Iteration 173/1000 | Loss: 0.00001615
Iteration 174/1000 | Loss: 0.00001615
Iteration 175/1000 | Loss: 0.00001615
Iteration 176/1000 | Loss: 0.00001615
Iteration 177/1000 | Loss: 0.00001615
Iteration 178/1000 | Loss: 0.00001615
Iteration 179/1000 | Loss: 0.00001615
Iteration 180/1000 | Loss: 0.00001615
Iteration 181/1000 | Loss: 0.00001615
Iteration 182/1000 | Loss: 0.00001615
Iteration 183/1000 | Loss: 0.00001615
Iteration 184/1000 | Loss: 0.00001614
Iteration 185/1000 | Loss: 0.00001614
Iteration 186/1000 | Loss: 0.00001614
Iteration 187/1000 | Loss: 0.00001614
Iteration 188/1000 | Loss: 0.00001614
Iteration 189/1000 | Loss: 0.00001614
Iteration 190/1000 | Loss: 0.00001614
Iteration 191/1000 | Loss: 0.00001614
Iteration 192/1000 | Loss: 0.00001614
Iteration 193/1000 | Loss: 0.00001614
Iteration 194/1000 | Loss: 0.00001614
Iteration 195/1000 | Loss: 0.00001614
Iteration 196/1000 | Loss: 0.00001614
Iteration 197/1000 | Loss: 0.00001614
Iteration 198/1000 | Loss: 0.00001614
Iteration 199/1000 | Loss: 0.00001614
Iteration 200/1000 | Loss: 0.00001614
Iteration 201/1000 | Loss: 0.00001614
Iteration 202/1000 | Loss: 0.00001613
Iteration 203/1000 | Loss: 0.00001613
Iteration 204/1000 | Loss: 0.00001613
Iteration 205/1000 | Loss: 0.00001613
Iteration 206/1000 | Loss: 0.00001613
Iteration 207/1000 | Loss: 0.00001613
Iteration 208/1000 | Loss: 0.00001613
Iteration 209/1000 | Loss: 0.00001613
Iteration 210/1000 | Loss: 0.00001613
Iteration 211/1000 | Loss: 0.00001613
Iteration 212/1000 | Loss: 0.00001613
Iteration 213/1000 | Loss: 0.00001613
Iteration 214/1000 | Loss: 0.00001613
Iteration 215/1000 | Loss: 0.00001613
Iteration 216/1000 | Loss: 0.00001613
Iteration 217/1000 | Loss: 0.00001613
Iteration 218/1000 | Loss: 0.00001613
Iteration 219/1000 | Loss: 0.00001613
Iteration 220/1000 | Loss: 0.00001613
Iteration 221/1000 | Loss: 0.00001613
Iteration 222/1000 | Loss: 0.00001613
Iteration 223/1000 | Loss: 0.00001613
Iteration 224/1000 | Loss: 0.00001613
Iteration 225/1000 | Loss: 0.00001612
Iteration 226/1000 | Loss: 0.00001612
Iteration 227/1000 | Loss: 0.00001612
Iteration 228/1000 | Loss: 0.00001612
Iteration 229/1000 | Loss: 0.00001612
Iteration 230/1000 | Loss: 0.00001612
Iteration 231/1000 | Loss: 0.00001612
Iteration 232/1000 | Loss: 0.00001612
Iteration 233/1000 | Loss: 0.00001612
Iteration 234/1000 | Loss: 0.00001612
Iteration 235/1000 | Loss: 0.00001612
Iteration 236/1000 | Loss: 0.00001612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.6122789020300843e-05, 1.6122789020300843e-05, 1.6122789020300843e-05, 1.6122789020300843e-05, 1.6122789020300843e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6122789020300843e-05

Optimization complete. Final v2v error: 3.45151424407959 mm

Highest mean error: 3.9157071113586426 mm for frame 38

Lowest mean error: 3.004082202911377 mm for frame 127

Saving results

Total time: 37.319008588790894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00922111
Iteration 2/25 | Loss: 0.00180538
Iteration 3/25 | Loss: 0.00122702
Iteration 4/25 | Loss: 0.00119934
Iteration 5/25 | Loss: 0.00118908
Iteration 6/25 | Loss: 0.00118637
Iteration 7/25 | Loss: 0.00118619
Iteration 8/25 | Loss: 0.00118619
Iteration 9/25 | Loss: 0.00118619
Iteration 10/25 | Loss: 0.00118619
Iteration 11/25 | Loss: 0.00118619
Iteration 12/25 | Loss: 0.00118619
Iteration 13/25 | Loss: 0.00118619
Iteration 14/25 | Loss: 0.00118619
Iteration 15/25 | Loss: 0.00118619
Iteration 16/25 | Loss: 0.00118619
Iteration 17/25 | Loss: 0.00118619
Iteration 18/25 | Loss: 0.00118619
Iteration 19/25 | Loss: 0.00118619
Iteration 20/25 | Loss: 0.00118619
Iteration 21/25 | Loss: 0.00118619
Iteration 22/25 | Loss: 0.00118619
Iteration 23/25 | Loss: 0.00118619
Iteration 24/25 | Loss: 0.00118619
Iteration 25/25 | Loss: 0.00118619

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.52734405
Iteration 2/25 | Loss: 0.00048368
Iteration 3/25 | Loss: 0.00048368
Iteration 4/25 | Loss: 0.00048368
Iteration 5/25 | Loss: 0.00048368
Iteration 6/25 | Loss: 0.00048368
Iteration 7/25 | Loss: 0.00048368
Iteration 8/25 | Loss: 0.00048368
Iteration 9/25 | Loss: 0.00048368
Iteration 10/25 | Loss: 0.00048368
Iteration 11/25 | Loss: 0.00048368
Iteration 12/25 | Loss: 0.00048368
Iteration 13/25 | Loss: 0.00048368
Iteration 14/25 | Loss: 0.00048368
Iteration 15/25 | Loss: 0.00048368
Iteration 16/25 | Loss: 0.00048368
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00048367647104896605, 0.00048367647104896605, 0.00048367647104896605, 0.00048367647104896605, 0.00048367647104896605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00048367647104896605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048368
Iteration 2/1000 | Loss: 0.00007400
Iteration 3/1000 | Loss: 0.00006357
Iteration 4/1000 | Loss: 0.00005562
Iteration 5/1000 | Loss: 0.00005296
Iteration 6/1000 | Loss: 0.00005133
Iteration 7/1000 | Loss: 0.00005047
Iteration 8/1000 | Loss: 0.00004933
Iteration 9/1000 | Loss: 0.00004849
Iteration 10/1000 | Loss: 0.00004795
Iteration 11/1000 | Loss: 0.00004748
Iteration 12/1000 | Loss: 0.00004701
Iteration 13/1000 | Loss: 0.00004662
Iteration 14/1000 | Loss: 0.00004635
Iteration 15/1000 | Loss: 0.00004609
Iteration 16/1000 | Loss: 0.00004585
Iteration 17/1000 | Loss: 0.00004562
Iteration 18/1000 | Loss: 0.00004539
Iteration 19/1000 | Loss: 0.00004526
Iteration 20/1000 | Loss: 0.00004514
Iteration 21/1000 | Loss: 0.00004510
Iteration 22/1000 | Loss: 0.00004510
Iteration 23/1000 | Loss: 0.00004509
Iteration 24/1000 | Loss: 0.00004508
Iteration 25/1000 | Loss: 0.00004506
Iteration 26/1000 | Loss: 0.00004503
Iteration 27/1000 | Loss: 0.00004501
Iteration 28/1000 | Loss: 0.00004500
Iteration 29/1000 | Loss: 0.00004500
Iteration 30/1000 | Loss: 0.00004499
Iteration 31/1000 | Loss: 0.00004497
Iteration 32/1000 | Loss: 0.00004494
Iteration 33/1000 | Loss: 0.00004493
Iteration 34/1000 | Loss: 0.00004491
Iteration 35/1000 | Loss: 0.00004491
Iteration 36/1000 | Loss: 0.00004491
Iteration 37/1000 | Loss: 0.00004491
Iteration 38/1000 | Loss: 0.00004491
Iteration 39/1000 | Loss: 0.00004491
Iteration 40/1000 | Loss: 0.00004491
Iteration 41/1000 | Loss: 0.00004491
Iteration 42/1000 | Loss: 0.00004491
Iteration 43/1000 | Loss: 0.00004490
Iteration 44/1000 | Loss: 0.00004490
Iteration 45/1000 | Loss: 0.00004490
Iteration 46/1000 | Loss: 0.00004489
Iteration 47/1000 | Loss: 0.00004488
Iteration 48/1000 | Loss: 0.00004488
Iteration 49/1000 | Loss: 0.00004487
Iteration 50/1000 | Loss: 0.00004487
Iteration 51/1000 | Loss: 0.00004487
Iteration 52/1000 | Loss: 0.00004486
Iteration 53/1000 | Loss: 0.00004486
Iteration 54/1000 | Loss: 0.00004486
Iteration 55/1000 | Loss: 0.00004486
Iteration 56/1000 | Loss: 0.00004486
Iteration 57/1000 | Loss: 0.00004486
Iteration 58/1000 | Loss: 0.00004486
Iteration 59/1000 | Loss: 0.00004486
Iteration 60/1000 | Loss: 0.00004484
Iteration 61/1000 | Loss: 0.00004484
Iteration 62/1000 | Loss: 0.00004482
Iteration 63/1000 | Loss: 0.00004481
Iteration 64/1000 | Loss: 0.00004480
Iteration 65/1000 | Loss: 0.00004480
Iteration 66/1000 | Loss: 0.00004480
Iteration 67/1000 | Loss: 0.00004480
Iteration 68/1000 | Loss: 0.00004480
Iteration 69/1000 | Loss: 0.00004480
Iteration 70/1000 | Loss: 0.00004480
Iteration 71/1000 | Loss: 0.00004480
Iteration 72/1000 | Loss: 0.00004480
Iteration 73/1000 | Loss: 0.00004479
Iteration 74/1000 | Loss: 0.00004478
Iteration 75/1000 | Loss: 0.00004478
Iteration 76/1000 | Loss: 0.00004478
Iteration 77/1000 | Loss: 0.00004477
Iteration 78/1000 | Loss: 0.00004476
Iteration 79/1000 | Loss: 0.00004473
Iteration 80/1000 | Loss: 0.00004465
Iteration 81/1000 | Loss: 0.00004465
Iteration 82/1000 | Loss: 0.00004465
Iteration 83/1000 | Loss: 0.00004464
Iteration 84/1000 | Loss: 0.00004464
Iteration 85/1000 | Loss: 0.00004464
Iteration 86/1000 | Loss: 0.00004464
Iteration 87/1000 | Loss: 0.00004464
Iteration 88/1000 | Loss: 0.00004464
Iteration 89/1000 | Loss: 0.00004464
Iteration 90/1000 | Loss: 0.00004464
Iteration 91/1000 | Loss: 0.00004464
Iteration 92/1000 | Loss: 0.00004464
Iteration 93/1000 | Loss: 0.00004464
Iteration 94/1000 | Loss: 0.00004464
Iteration 95/1000 | Loss: 0.00004463
Iteration 96/1000 | Loss: 0.00004463
Iteration 97/1000 | Loss: 0.00004463
Iteration 98/1000 | Loss: 0.00004463
Iteration 99/1000 | Loss: 0.00004462
Iteration 100/1000 | Loss: 0.00004462
Iteration 101/1000 | Loss: 0.00004461
Iteration 102/1000 | Loss: 0.00004461
Iteration 103/1000 | Loss: 0.00004461
Iteration 104/1000 | Loss: 0.00004461
Iteration 105/1000 | Loss: 0.00004460
Iteration 106/1000 | Loss: 0.00004460
Iteration 107/1000 | Loss: 0.00004460
Iteration 108/1000 | Loss: 0.00004460
Iteration 109/1000 | Loss: 0.00004460
Iteration 110/1000 | Loss: 0.00004460
Iteration 111/1000 | Loss: 0.00004459
Iteration 112/1000 | Loss: 0.00004459
Iteration 113/1000 | Loss: 0.00004459
Iteration 114/1000 | Loss: 0.00004458
Iteration 115/1000 | Loss: 0.00004458
Iteration 116/1000 | Loss: 0.00004458
Iteration 117/1000 | Loss: 0.00004458
Iteration 118/1000 | Loss: 0.00004458
Iteration 119/1000 | Loss: 0.00004457
Iteration 120/1000 | Loss: 0.00004457
Iteration 121/1000 | Loss: 0.00004457
Iteration 122/1000 | Loss: 0.00004457
Iteration 123/1000 | Loss: 0.00004457
Iteration 124/1000 | Loss: 0.00004457
Iteration 125/1000 | Loss: 0.00004457
Iteration 126/1000 | Loss: 0.00004457
Iteration 127/1000 | Loss: 0.00004457
Iteration 128/1000 | Loss: 0.00004457
Iteration 129/1000 | Loss: 0.00004457
Iteration 130/1000 | Loss: 0.00004457
Iteration 131/1000 | Loss: 0.00004456
Iteration 132/1000 | Loss: 0.00004456
Iteration 133/1000 | Loss: 0.00004456
Iteration 134/1000 | Loss: 0.00004456
Iteration 135/1000 | Loss: 0.00004456
Iteration 136/1000 | Loss: 0.00004455
Iteration 137/1000 | Loss: 0.00004455
Iteration 138/1000 | Loss: 0.00004455
Iteration 139/1000 | Loss: 0.00004455
Iteration 140/1000 | Loss: 0.00004455
Iteration 141/1000 | Loss: 0.00004455
Iteration 142/1000 | Loss: 0.00004455
Iteration 143/1000 | Loss: 0.00004455
Iteration 144/1000 | Loss: 0.00004455
Iteration 145/1000 | Loss: 0.00004455
Iteration 146/1000 | Loss: 0.00004455
Iteration 147/1000 | Loss: 0.00004455
Iteration 148/1000 | Loss: 0.00004454
Iteration 149/1000 | Loss: 0.00004454
Iteration 150/1000 | Loss: 0.00004454
Iteration 151/1000 | Loss: 0.00004454
Iteration 152/1000 | Loss: 0.00004454
Iteration 153/1000 | Loss: 0.00004454
Iteration 154/1000 | Loss: 0.00004453
Iteration 155/1000 | Loss: 0.00004453
Iteration 156/1000 | Loss: 0.00004453
Iteration 157/1000 | Loss: 0.00004453
Iteration 158/1000 | Loss: 0.00004453
Iteration 159/1000 | Loss: 0.00004452
Iteration 160/1000 | Loss: 0.00004452
Iteration 161/1000 | Loss: 0.00004452
Iteration 162/1000 | Loss: 0.00004452
Iteration 163/1000 | Loss: 0.00004452
Iteration 164/1000 | Loss: 0.00004451
Iteration 165/1000 | Loss: 0.00004451
Iteration 166/1000 | Loss: 0.00004451
Iteration 167/1000 | Loss: 0.00004451
Iteration 168/1000 | Loss: 0.00004451
Iteration 169/1000 | Loss: 0.00004451
Iteration 170/1000 | Loss: 0.00004451
Iteration 171/1000 | Loss: 0.00004451
Iteration 172/1000 | Loss: 0.00004451
Iteration 173/1000 | Loss: 0.00004451
Iteration 174/1000 | Loss: 0.00004451
Iteration 175/1000 | Loss: 0.00004451
Iteration 176/1000 | Loss: 0.00004451
Iteration 177/1000 | Loss: 0.00004451
Iteration 178/1000 | Loss: 0.00004451
Iteration 179/1000 | Loss: 0.00004451
Iteration 180/1000 | Loss: 0.00004451
Iteration 181/1000 | Loss: 0.00004450
Iteration 182/1000 | Loss: 0.00004450
Iteration 183/1000 | Loss: 0.00004450
Iteration 184/1000 | Loss: 0.00004450
Iteration 185/1000 | Loss: 0.00004450
Iteration 186/1000 | Loss: 0.00004450
Iteration 187/1000 | Loss: 0.00004450
Iteration 188/1000 | Loss: 0.00004450
Iteration 189/1000 | Loss: 0.00004449
Iteration 190/1000 | Loss: 0.00004449
Iteration 191/1000 | Loss: 0.00004449
Iteration 192/1000 | Loss: 0.00004449
Iteration 193/1000 | Loss: 0.00004449
Iteration 194/1000 | Loss: 0.00004448
Iteration 195/1000 | Loss: 0.00004448
Iteration 196/1000 | Loss: 0.00004448
Iteration 197/1000 | Loss: 0.00004448
Iteration 198/1000 | Loss: 0.00004448
Iteration 199/1000 | Loss: 0.00004448
Iteration 200/1000 | Loss: 0.00004448
Iteration 201/1000 | Loss: 0.00004448
Iteration 202/1000 | Loss: 0.00004448
Iteration 203/1000 | Loss: 0.00004448
Iteration 204/1000 | Loss: 0.00004448
Iteration 205/1000 | Loss: 0.00004448
Iteration 206/1000 | Loss: 0.00004447
Iteration 207/1000 | Loss: 0.00004447
Iteration 208/1000 | Loss: 0.00004447
Iteration 209/1000 | Loss: 0.00004447
Iteration 210/1000 | Loss: 0.00004447
Iteration 211/1000 | Loss: 0.00004447
Iteration 212/1000 | Loss: 0.00004447
Iteration 213/1000 | Loss: 0.00004447
Iteration 214/1000 | Loss: 0.00004447
Iteration 215/1000 | Loss: 0.00004446
Iteration 216/1000 | Loss: 0.00004446
Iteration 217/1000 | Loss: 0.00004446
Iteration 218/1000 | Loss: 0.00004446
Iteration 219/1000 | Loss: 0.00004446
Iteration 220/1000 | Loss: 0.00004446
Iteration 221/1000 | Loss: 0.00004446
Iteration 222/1000 | Loss: 0.00004446
Iteration 223/1000 | Loss: 0.00004446
Iteration 224/1000 | Loss: 0.00004446
Iteration 225/1000 | Loss: 0.00004446
Iteration 226/1000 | Loss: 0.00004446
Iteration 227/1000 | Loss: 0.00004445
Iteration 228/1000 | Loss: 0.00004445
Iteration 229/1000 | Loss: 0.00004445
Iteration 230/1000 | Loss: 0.00004445
Iteration 231/1000 | Loss: 0.00004445
Iteration 232/1000 | Loss: 0.00004445
Iteration 233/1000 | Loss: 0.00004445
Iteration 234/1000 | Loss: 0.00004445
Iteration 235/1000 | Loss: 0.00004445
Iteration 236/1000 | Loss: 0.00004445
Iteration 237/1000 | Loss: 0.00004445
Iteration 238/1000 | Loss: 0.00004445
Iteration 239/1000 | Loss: 0.00004445
Iteration 240/1000 | Loss: 0.00004444
Iteration 241/1000 | Loss: 0.00004444
Iteration 242/1000 | Loss: 0.00004444
Iteration 243/1000 | Loss: 0.00004444
Iteration 244/1000 | Loss: 0.00004444
Iteration 245/1000 | Loss: 0.00004444
Iteration 246/1000 | Loss: 0.00004444
Iteration 247/1000 | Loss: 0.00004444
Iteration 248/1000 | Loss: 0.00004444
Iteration 249/1000 | Loss: 0.00004444
Iteration 250/1000 | Loss: 0.00004444
Iteration 251/1000 | Loss: 0.00004444
Iteration 252/1000 | Loss: 0.00004444
Iteration 253/1000 | Loss: 0.00004444
Iteration 254/1000 | Loss: 0.00004444
Iteration 255/1000 | Loss: 0.00004444
Iteration 256/1000 | Loss: 0.00004444
Iteration 257/1000 | Loss: 0.00004444
Iteration 258/1000 | Loss: 0.00004444
Iteration 259/1000 | Loss: 0.00004444
Iteration 260/1000 | Loss: 0.00004444
Iteration 261/1000 | Loss: 0.00004444
Iteration 262/1000 | Loss: 0.00004444
Iteration 263/1000 | Loss: 0.00004444
Iteration 264/1000 | Loss: 0.00004444
Iteration 265/1000 | Loss: 0.00004444
Iteration 266/1000 | Loss: 0.00004444
Iteration 267/1000 | Loss: 0.00004444
Iteration 268/1000 | Loss: 0.00004444
Iteration 269/1000 | Loss: 0.00004444
Iteration 270/1000 | Loss: 0.00004444
Iteration 271/1000 | Loss: 0.00004444
Iteration 272/1000 | Loss: 0.00004444
Iteration 273/1000 | Loss: 0.00004444
Iteration 274/1000 | Loss: 0.00004444
Iteration 275/1000 | Loss: 0.00004444
Iteration 276/1000 | Loss: 0.00004444
Iteration 277/1000 | Loss: 0.00004444
Iteration 278/1000 | Loss: 0.00004444
Iteration 279/1000 | Loss: 0.00004444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [4.444103979039937e-05, 4.444103979039937e-05, 4.444103979039937e-05, 4.444103979039937e-05, 4.444103979039937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.444103979039937e-05

Optimization complete. Final v2v error: 5.471864700317383 mm

Highest mean error: 6.668346405029297 mm for frame 143

Lowest mean error: 5.223620414733887 mm for frame 9

Saving results

Total time: 57.0235595703125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453011
Iteration 2/25 | Loss: 0.00121671
Iteration 3/25 | Loss: 0.00108701
Iteration 4/25 | Loss: 0.00105781
Iteration 5/25 | Loss: 0.00104561
Iteration 6/25 | Loss: 0.00104341
Iteration 7/25 | Loss: 0.00104269
Iteration 8/25 | Loss: 0.00104269
Iteration 9/25 | Loss: 0.00104269
Iteration 10/25 | Loss: 0.00104269
Iteration 11/25 | Loss: 0.00104269
Iteration 12/25 | Loss: 0.00104269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010426851222291589, 0.0010426851222291589, 0.0010426851222291589, 0.0010426851222291589, 0.0010426851222291589]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010426851222291589

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48274314
Iteration 2/25 | Loss: 0.00070468
Iteration 3/25 | Loss: 0.00070468
Iteration 4/25 | Loss: 0.00070468
Iteration 5/25 | Loss: 0.00070467
Iteration 6/25 | Loss: 0.00070467
Iteration 7/25 | Loss: 0.00070467
Iteration 8/25 | Loss: 0.00070467
Iteration 9/25 | Loss: 0.00070467
Iteration 10/25 | Loss: 0.00070467
Iteration 11/25 | Loss: 0.00070467
Iteration 12/25 | Loss: 0.00070467
Iteration 13/25 | Loss: 0.00070467
Iteration 14/25 | Loss: 0.00070467
Iteration 15/25 | Loss: 0.00070467
Iteration 16/25 | Loss: 0.00070467
Iteration 17/25 | Loss: 0.00070467
Iteration 18/25 | Loss: 0.00070467
Iteration 19/25 | Loss: 0.00070467
Iteration 20/25 | Loss: 0.00070467
Iteration 21/25 | Loss: 0.00070467
Iteration 22/25 | Loss: 0.00070467
Iteration 23/25 | Loss: 0.00070467
Iteration 24/25 | Loss: 0.00070467
Iteration 25/25 | Loss: 0.00070467
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007046734681352973, 0.0007046734681352973, 0.0007046734681352973, 0.0007046734681352973, 0.0007046734681352973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007046734681352973

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070467
Iteration 2/1000 | Loss: 0.00005747
Iteration 3/1000 | Loss: 0.00004665
Iteration 4/1000 | Loss: 0.00004392
Iteration 5/1000 | Loss: 0.00004224
Iteration 6/1000 | Loss: 0.00004076
Iteration 7/1000 | Loss: 0.00003954
Iteration 8/1000 | Loss: 0.00003893
Iteration 9/1000 | Loss: 0.00003855
Iteration 10/1000 | Loss: 0.00003830
Iteration 11/1000 | Loss: 0.00003817
Iteration 12/1000 | Loss: 0.00003817
Iteration 13/1000 | Loss: 0.00003816
Iteration 14/1000 | Loss: 0.00003816
Iteration 15/1000 | Loss: 0.00003816
Iteration 16/1000 | Loss: 0.00003816
Iteration 17/1000 | Loss: 0.00003815
Iteration 18/1000 | Loss: 0.00003815
Iteration 19/1000 | Loss: 0.00003815
Iteration 20/1000 | Loss: 0.00003814
Iteration 21/1000 | Loss: 0.00003813
Iteration 22/1000 | Loss: 0.00003813
Iteration 23/1000 | Loss: 0.00003812
Iteration 24/1000 | Loss: 0.00003812
Iteration 25/1000 | Loss: 0.00003812
Iteration 26/1000 | Loss: 0.00003811
Iteration 27/1000 | Loss: 0.00003811
Iteration 28/1000 | Loss: 0.00003811
Iteration 29/1000 | Loss: 0.00003811
Iteration 30/1000 | Loss: 0.00003811
Iteration 31/1000 | Loss: 0.00003811
Iteration 32/1000 | Loss: 0.00003811
Iteration 33/1000 | Loss: 0.00003811
Iteration 34/1000 | Loss: 0.00003810
Iteration 35/1000 | Loss: 0.00003810
Iteration 36/1000 | Loss: 0.00003810
Iteration 37/1000 | Loss: 0.00003809
Iteration 38/1000 | Loss: 0.00003809
Iteration 39/1000 | Loss: 0.00003809
Iteration 40/1000 | Loss: 0.00003809
Iteration 41/1000 | Loss: 0.00003809
Iteration 42/1000 | Loss: 0.00003808
Iteration 43/1000 | Loss: 0.00003808
Iteration 44/1000 | Loss: 0.00003808
Iteration 45/1000 | Loss: 0.00003807
Iteration 46/1000 | Loss: 0.00003807
Iteration 47/1000 | Loss: 0.00003807
Iteration 48/1000 | Loss: 0.00003807
Iteration 49/1000 | Loss: 0.00003807
Iteration 50/1000 | Loss: 0.00003807
Iteration 51/1000 | Loss: 0.00003807
Iteration 52/1000 | Loss: 0.00003806
Iteration 53/1000 | Loss: 0.00003806
Iteration 54/1000 | Loss: 0.00003806
Iteration 55/1000 | Loss: 0.00003806
Iteration 56/1000 | Loss: 0.00003806
Iteration 57/1000 | Loss: 0.00003805
Iteration 58/1000 | Loss: 0.00003805
Iteration 59/1000 | Loss: 0.00003805
Iteration 60/1000 | Loss: 0.00003805
Iteration 61/1000 | Loss: 0.00003804
Iteration 62/1000 | Loss: 0.00003804
Iteration 63/1000 | Loss: 0.00003804
Iteration 64/1000 | Loss: 0.00003804
Iteration 65/1000 | Loss: 0.00003804
Iteration 66/1000 | Loss: 0.00003804
Iteration 67/1000 | Loss: 0.00003803
Iteration 68/1000 | Loss: 0.00003803
Iteration 69/1000 | Loss: 0.00003803
Iteration 70/1000 | Loss: 0.00003803
Iteration 71/1000 | Loss: 0.00003803
Iteration 72/1000 | Loss: 0.00003803
Iteration 73/1000 | Loss: 0.00003803
Iteration 74/1000 | Loss: 0.00003803
Iteration 75/1000 | Loss: 0.00003803
Iteration 76/1000 | Loss: 0.00003803
Iteration 77/1000 | Loss: 0.00003803
Iteration 78/1000 | Loss: 0.00003802
Iteration 79/1000 | Loss: 0.00003802
Iteration 80/1000 | Loss: 0.00003802
Iteration 81/1000 | Loss: 0.00003802
Iteration 82/1000 | Loss: 0.00003802
Iteration 83/1000 | Loss: 0.00003802
Iteration 84/1000 | Loss: 0.00003802
Iteration 85/1000 | Loss: 0.00003802
Iteration 86/1000 | Loss: 0.00003802
Iteration 87/1000 | Loss: 0.00003801
Iteration 88/1000 | Loss: 0.00003801
Iteration 89/1000 | Loss: 0.00003801
Iteration 90/1000 | Loss: 0.00003800
Iteration 91/1000 | Loss: 0.00003800
Iteration 92/1000 | Loss: 0.00003800
Iteration 93/1000 | Loss: 0.00003800
Iteration 94/1000 | Loss: 0.00003800
Iteration 95/1000 | Loss: 0.00003800
Iteration 96/1000 | Loss: 0.00003800
Iteration 97/1000 | Loss: 0.00003800
Iteration 98/1000 | Loss: 0.00003800
Iteration 99/1000 | Loss: 0.00003799
Iteration 100/1000 | Loss: 0.00003799
Iteration 101/1000 | Loss: 0.00003799
Iteration 102/1000 | Loss: 0.00003799
Iteration 103/1000 | Loss: 0.00003799
Iteration 104/1000 | Loss: 0.00003798
Iteration 105/1000 | Loss: 0.00003798
Iteration 106/1000 | Loss: 0.00003798
Iteration 107/1000 | Loss: 0.00003798
Iteration 108/1000 | Loss: 0.00003798
Iteration 109/1000 | Loss: 0.00003798
Iteration 110/1000 | Loss: 0.00003798
Iteration 111/1000 | Loss: 0.00003798
Iteration 112/1000 | Loss: 0.00003798
Iteration 113/1000 | Loss: 0.00003798
Iteration 114/1000 | Loss: 0.00003798
Iteration 115/1000 | Loss: 0.00003798
Iteration 116/1000 | Loss: 0.00003797
Iteration 117/1000 | Loss: 0.00003797
Iteration 118/1000 | Loss: 0.00003797
Iteration 119/1000 | Loss: 0.00003797
Iteration 120/1000 | Loss: 0.00003797
Iteration 121/1000 | Loss: 0.00003797
Iteration 122/1000 | Loss: 0.00003797
Iteration 123/1000 | Loss: 0.00003797
Iteration 124/1000 | Loss: 0.00003797
Iteration 125/1000 | Loss: 0.00003796
Iteration 126/1000 | Loss: 0.00003796
Iteration 127/1000 | Loss: 0.00003796
Iteration 128/1000 | Loss: 0.00003796
Iteration 129/1000 | Loss: 0.00003796
Iteration 130/1000 | Loss: 0.00003796
Iteration 131/1000 | Loss: 0.00003796
Iteration 132/1000 | Loss: 0.00003796
Iteration 133/1000 | Loss: 0.00003796
Iteration 134/1000 | Loss: 0.00003796
Iteration 135/1000 | Loss: 0.00003796
Iteration 136/1000 | Loss: 0.00003796
Iteration 137/1000 | Loss: 0.00003796
Iteration 138/1000 | Loss: 0.00003796
Iteration 139/1000 | Loss: 0.00003796
Iteration 140/1000 | Loss: 0.00003795
Iteration 141/1000 | Loss: 0.00003795
Iteration 142/1000 | Loss: 0.00003795
Iteration 143/1000 | Loss: 0.00003795
Iteration 144/1000 | Loss: 0.00003795
Iteration 145/1000 | Loss: 0.00003795
Iteration 146/1000 | Loss: 0.00003795
Iteration 147/1000 | Loss: 0.00003795
Iteration 148/1000 | Loss: 0.00003795
Iteration 149/1000 | Loss: 0.00003795
Iteration 150/1000 | Loss: 0.00003795
Iteration 151/1000 | Loss: 0.00003795
Iteration 152/1000 | Loss: 0.00003795
Iteration 153/1000 | Loss: 0.00003795
Iteration 154/1000 | Loss: 0.00003795
Iteration 155/1000 | Loss: 0.00003794
Iteration 156/1000 | Loss: 0.00003794
Iteration 157/1000 | Loss: 0.00003794
Iteration 158/1000 | Loss: 0.00003794
Iteration 159/1000 | Loss: 0.00003794
Iteration 160/1000 | Loss: 0.00003794
Iteration 161/1000 | Loss: 0.00003794
Iteration 162/1000 | Loss: 0.00003794
Iteration 163/1000 | Loss: 0.00003794
Iteration 164/1000 | Loss: 0.00003794
Iteration 165/1000 | Loss: 0.00003794
Iteration 166/1000 | Loss: 0.00003793
Iteration 167/1000 | Loss: 0.00003793
Iteration 168/1000 | Loss: 0.00003793
Iteration 169/1000 | Loss: 0.00003793
Iteration 170/1000 | Loss: 0.00003793
Iteration 171/1000 | Loss: 0.00003793
Iteration 172/1000 | Loss: 0.00003793
Iteration 173/1000 | Loss: 0.00003793
Iteration 174/1000 | Loss: 0.00003793
Iteration 175/1000 | Loss: 0.00003793
Iteration 176/1000 | Loss: 0.00003793
Iteration 177/1000 | Loss: 0.00003793
Iteration 178/1000 | Loss: 0.00003793
Iteration 179/1000 | Loss: 0.00003793
Iteration 180/1000 | Loss: 0.00003793
Iteration 181/1000 | Loss: 0.00003793
Iteration 182/1000 | Loss: 0.00003792
Iteration 183/1000 | Loss: 0.00003792
Iteration 184/1000 | Loss: 0.00003792
Iteration 185/1000 | Loss: 0.00003792
Iteration 186/1000 | Loss: 0.00003792
Iteration 187/1000 | Loss: 0.00003792
Iteration 188/1000 | Loss: 0.00003792
Iteration 189/1000 | Loss: 0.00003792
Iteration 190/1000 | Loss: 0.00003792
Iteration 191/1000 | Loss: 0.00003792
Iteration 192/1000 | Loss: 0.00003792
Iteration 193/1000 | Loss: 0.00003792
Iteration 194/1000 | Loss: 0.00003792
Iteration 195/1000 | Loss: 0.00003792
Iteration 196/1000 | Loss: 0.00003792
Iteration 197/1000 | Loss: 0.00003792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [3.792230927501805e-05, 3.792230927501805e-05, 3.792230927501805e-05, 3.792230927501805e-05, 3.792230927501805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.792230927501805e-05

Optimization complete. Final v2v error: 5.099590301513672 mm

Highest mean error: 5.523673057556152 mm for frame 18

Lowest mean error: 4.289493560791016 mm for frame 39

Saving results

Total time: 36.27053165435791
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402076
Iteration 2/25 | Loss: 0.00110644
Iteration 3/25 | Loss: 0.00097967
Iteration 4/25 | Loss: 0.00096706
Iteration 5/25 | Loss: 0.00095977
Iteration 6/25 | Loss: 0.00095693
Iteration 7/25 | Loss: 0.00095611
Iteration 8/25 | Loss: 0.00095607
Iteration 9/25 | Loss: 0.00095607
Iteration 10/25 | Loss: 0.00095607
Iteration 11/25 | Loss: 0.00095607
Iteration 12/25 | Loss: 0.00095607
Iteration 13/25 | Loss: 0.00095607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009560682810842991, 0.0009560682810842991, 0.0009560682810842991, 0.0009560682810842991, 0.0009560682810842991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009560682810842991

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65072775
Iteration 2/25 | Loss: 0.00082843
Iteration 3/25 | Loss: 0.00082843
Iteration 4/25 | Loss: 0.00082843
Iteration 5/25 | Loss: 0.00082843
Iteration 6/25 | Loss: 0.00082843
Iteration 7/25 | Loss: 0.00082843
Iteration 8/25 | Loss: 0.00082843
Iteration 9/25 | Loss: 0.00082843
Iteration 10/25 | Loss: 0.00082843
Iteration 11/25 | Loss: 0.00082843
Iteration 12/25 | Loss: 0.00082843
Iteration 13/25 | Loss: 0.00082843
Iteration 14/25 | Loss: 0.00082843
Iteration 15/25 | Loss: 0.00082843
Iteration 16/25 | Loss: 0.00082843
Iteration 17/25 | Loss: 0.00082843
Iteration 18/25 | Loss: 0.00082843
Iteration 19/25 | Loss: 0.00082843
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008284259238280356, 0.0008284259238280356, 0.0008284259238280356, 0.0008284259238280356, 0.0008284259238280356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008284259238280356

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082843
Iteration 2/1000 | Loss: 0.00002771
Iteration 3/1000 | Loss: 0.00002199
Iteration 4/1000 | Loss: 0.00001911
Iteration 5/1000 | Loss: 0.00001832
Iteration 6/1000 | Loss: 0.00001769
Iteration 7/1000 | Loss: 0.00001728
Iteration 8/1000 | Loss: 0.00001698
Iteration 9/1000 | Loss: 0.00001676
Iteration 10/1000 | Loss: 0.00001673
Iteration 11/1000 | Loss: 0.00001670
Iteration 12/1000 | Loss: 0.00001663
Iteration 13/1000 | Loss: 0.00001657
Iteration 14/1000 | Loss: 0.00001657
Iteration 15/1000 | Loss: 0.00001656
Iteration 16/1000 | Loss: 0.00001655
Iteration 17/1000 | Loss: 0.00001652
Iteration 18/1000 | Loss: 0.00001651
Iteration 19/1000 | Loss: 0.00001650
Iteration 20/1000 | Loss: 0.00001647
Iteration 21/1000 | Loss: 0.00001646
Iteration 22/1000 | Loss: 0.00001645
Iteration 23/1000 | Loss: 0.00001644
Iteration 24/1000 | Loss: 0.00001642
Iteration 25/1000 | Loss: 0.00001641
Iteration 26/1000 | Loss: 0.00001640
Iteration 27/1000 | Loss: 0.00001640
Iteration 28/1000 | Loss: 0.00001640
Iteration 29/1000 | Loss: 0.00001638
Iteration 30/1000 | Loss: 0.00001635
Iteration 31/1000 | Loss: 0.00001635
Iteration 32/1000 | Loss: 0.00001635
Iteration 33/1000 | Loss: 0.00001635
Iteration 34/1000 | Loss: 0.00001635
Iteration 35/1000 | Loss: 0.00001635
Iteration 36/1000 | Loss: 0.00001635
Iteration 37/1000 | Loss: 0.00001635
Iteration 38/1000 | Loss: 0.00001635
Iteration 39/1000 | Loss: 0.00001635
Iteration 40/1000 | Loss: 0.00001634
Iteration 41/1000 | Loss: 0.00001634
Iteration 42/1000 | Loss: 0.00001634
Iteration 43/1000 | Loss: 0.00001633
Iteration 44/1000 | Loss: 0.00001633
Iteration 45/1000 | Loss: 0.00001632
Iteration 46/1000 | Loss: 0.00001632
Iteration 47/1000 | Loss: 0.00001632
Iteration 48/1000 | Loss: 0.00001631
Iteration 49/1000 | Loss: 0.00001631
Iteration 50/1000 | Loss: 0.00001631
Iteration 51/1000 | Loss: 0.00001631
Iteration 52/1000 | Loss: 0.00001631
Iteration 53/1000 | Loss: 0.00001630
Iteration 54/1000 | Loss: 0.00001630
Iteration 55/1000 | Loss: 0.00001630
Iteration 56/1000 | Loss: 0.00001629
Iteration 57/1000 | Loss: 0.00001629
Iteration 58/1000 | Loss: 0.00001627
Iteration 59/1000 | Loss: 0.00001627
Iteration 60/1000 | Loss: 0.00001627
Iteration 61/1000 | Loss: 0.00001627
Iteration 62/1000 | Loss: 0.00001627
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001626
Iteration 65/1000 | Loss: 0.00001626
Iteration 66/1000 | Loss: 0.00001626
Iteration 67/1000 | Loss: 0.00001626
Iteration 68/1000 | Loss: 0.00001626
Iteration 69/1000 | Loss: 0.00001626
Iteration 70/1000 | Loss: 0.00001626
Iteration 71/1000 | Loss: 0.00001625
Iteration 72/1000 | Loss: 0.00001625
Iteration 73/1000 | Loss: 0.00001625
Iteration 74/1000 | Loss: 0.00001625
Iteration 75/1000 | Loss: 0.00001625
Iteration 76/1000 | Loss: 0.00001625
Iteration 77/1000 | Loss: 0.00001624
Iteration 78/1000 | Loss: 0.00001624
Iteration 79/1000 | Loss: 0.00001624
Iteration 80/1000 | Loss: 0.00001624
Iteration 81/1000 | Loss: 0.00001624
Iteration 82/1000 | Loss: 0.00001624
Iteration 83/1000 | Loss: 0.00001624
Iteration 84/1000 | Loss: 0.00001624
Iteration 85/1000 | Loss: 0.00001624
Iteration 86/1000 | Loss: 0.00001624
Iteration 87/1000 | Loss: 0.00001624
Iteration 88/1000 | Loss: 0.00001624
Iteration 89/1000 | Loss: 0.00001623
Iteration 90/1000 | Loss: 0.00001623
Iteration 91/1000 | Loss: 0.00001623
Iteration 92/1000 | Loss: 0.00001623
Iteration 93/1000 | Loss: 0.00001623
Iteration 94/1000 | Loss: 0.00001623
Iteration 95/1000 | Loss: 0.00001623
Iteration 96/1000 | Loss: 0.00001623
Iteration 97/1000 | Loss: 0.00001623
Iteration 98/1000 | Loss: 0.00001623
Iteration 99/1000 | Loss: 0.00001623
Iteration 100/1000 | Loss: 0.00001623
Iteration 101/1000 | Loss: 0.00001623
Iteration 102/1000 | Loss: 0.00001623
Iteration 103/1000 | Loss: 0.00001622
Iteration 104/1000 | Loss: 0.00001622
Iteration 105/1000 | Loss: 0.00001622
Iteration 106/1000 | Loss: 0.00001622
Iteration 107/1000 | Loss: 0.00001622
Iteration 108/1000 | Loss: 0.00001622
Iteration 109/1000 | Loss: 0.00001622
Iteration 110/1000 | Loss: 0.00001622
Iteration 111/1000 | Loss: 0.00001622
Iteration 112/1000 | Loss: 0.00001622
Iteration 113/1000 | Loss: 0.00001622
Iteration 114/1000 | Loss: 0.00001622
Iteration 115/1000 | Loss: 0.00001622
Iteration 116/1000 | Loss: 0.00001622
Iteration 117/1000 | Loss: 0.00001622
Iteration 118/1000 | Loss: 0.00001622
Iteration 119/1000 | Loss: 0.00001622
Iteration 120/1000 | Loss: 0.00001622
Iteration 121/1000 | Loss: 0.00001621
Iteration 122/1000 | Loss: 0.00001621
Iteration 123/1000 | Loss: 0.00001621
Iteration 124/1000 | Loss: 0.00001621
Iteration 125/1000 | Loss: 0.00001621
Iteration 126/1000 | Loss: 0.00001621
Iteration 127/1000 | Loss: 0.00001621
Iteration 128/1000 | Loss: 0.00001621
Iteration 129/1000 | Loss: 0.00001621
Iteration 130/1000 | Loss: 0.00001621
Iteration 131/1000 | Loss: 0.00001621
Iteration 132/1000 | Loss: 0.00001621
Iteration 133/1000 | Loss: 0.00001621
Iteration 134/1000 | Loss: 0.00001621
Iteration 135/1000 | Loss: 0.00001621
Iteration 136/1000 | Loss: 0.00001621
Iteration 137/1000 | Loss: 0.00001620
Iteration 138/1000 | Loss: 0.00001620
Iteration 139/1000 | Loss: 0.00001620
Iteration 140/1000 | Loss: 0.00001620
Iteration 141/1000 | Loss: 0.00001620
Iteration 142/1000 | Loss: 0.00001620
Iteration 143/1000 | Loss: 0.00001620
Iteration 144/1000 | Loss: 0.00001620
Iteration 145/1000 | Loss: 0.00001620
Iteration 146/1000 | Loss: 0.00001620
Iteration 147/1000 | Loss: 0.00001620
Iteration 148/1000 | Loss: 0.00001620
Iteration 149/1000 | Loss: 0.00001620
Iteration 150/1000 | Loss: 0.00001620
Iteration 151/1000 | Loss: 0.00001620
Iteration 152/1000 | Loss: 0.00001620
Iteration 153/1000 | Loss: 0.00001620
Iteration 154/1000 | Loss: 0.00001620
Iteration 155/1000 | Loss: 0.00001619
Iteration 156/1000 | Loss: 0.00001619
Iteration 157/1000 | Loss: 0.00001619
Iteration 158/1000 | Loss: 0.00001619
Iteration 159/1000 | Loss: 0.00001619
Iteration 160/1000 | Loss: 0.00001619
Iteration 161/1000 | Loss: 0.00001619
Iteration 162/1000 | Loss: 0.00001619
Iteration 163/1000 | Loss: 0.00001619
Iteration 164/1000 | Loss: 0.00001619
Iteration 165/1000 | Loss: 0.00001619
Iteration 166/1000 | Loss: 0.00001619
Iteration 167/1000 | Loss: 0.00001619
Iteration 168/1000 | Loss: 0.00001619
Iteration 169/1000 | Loss: 0.00001619
Iteration 170/1000 | Loss: 0.00001619
Iteration 171/1000 | Loss: 0.00001619
Iteration 172/1000 | Loss: 0.00001619
Iteration 173/1000 | Loss: 0.00001619
Iteration 174/1000 | Loss: 0.00001619
Iteration 175/1000 | Loss: 0.00001619
Iteration 176/1000 | Loss: 0.00001619
Iteration 177/1000 | Loss: 0.00001619
Iteration 178/1000 | Loss: 0.00001619
Iteration 179/1000 | Loss: 0.00001619
Iteration 180/1000 | Loss: 0.00001619
Iteration 181/1000 | Loss: 0.00001619
Iteration 182/1000 | Loss: 0.00001619
Iteration 183/1000 | Loss: 0.00001619
Iteration 184/1000 | Loss: 0.00001619
Iteration 185/1000 | Loss: 0.00001619
Iteration 186/1000 | Loss: 0.00001619
Iteration 187/1000 | Loss: 0.00001619
Iteration 188/1000 | Loss: 0.00001619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.6186057109734975e-05, 1.6186057109734975e-05, 1.6186057109734975e-05, 1.6186057109734975e-05, 1.6186057109734975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6186057109734975e-05

Optimization complete. Final v2v error: 3.492302894592285 mm

Highest mean error: 4.000904083251953 mm for frame 127

Lowest mean error: 2.9756078720092773 mm for frame 2

Saving results

Total time: 43.362497091293335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042897
Iteration 2/25 | Loss: 0.00187934
Iteration 3/25 | Loss: 0.00122997
Iteration 4/25 | Loss: 0.00119610
Iteration 5/25 | Loss: 0.00118061
Iteration 6/25 | Loss: 0.00117597
Iteration 7/25 | Loss: 0.00117529
Iteration 8/25 | Loss: 0.00117529
Iteration 9/25 | Loss: 0.00117529
Iteration 10/25 | Loss: 0.00117529
Iteration 11/25 | Loss: 0.00117529
Iteration 12/25 | Loss: 0.00117529
Iteration 13/25 | Loss: 0.00117529
Iteration 14/25 | Loss: 0.00117529
Iteration 15/25 | Loss: 0.00117529
Iteration 16/25 | Loss: 0.00117529
Iteration 17/25 | Loss: 0.00117529
Iteration 18/25 | Loss: 0.00117529
Iteration 19/25 | Loss: 0.00117529
Iteration 20/25 | Loss: 0.00117529
Iteration 21/25 | Loss: 0.00117529
Iteration 22/25 | Loss: 0.00117529
Iteration 23/25 | Loss: 0.00117529
Iteration 24/25 | Loss: 0.00117529
Iteration 25/25 | Loss: 0.00117529

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92214167
Iteration 2/25 | Loss: 0.00069826
Iteration 3/25 | Loss: 0.00069825
Iteration 4/25 | Loss: 0.00069825
Iteration 5/25 | Loss: 0.00069825
Iteration 6/25 | Loss: 0.00069825
Iteration 7/25 | Loss: 0.00069825
Iteration 8/25 | Loss: 0.00069825
Iteration 9/25 | Loss: 0.00069825
Iteration 10/25 | Loss: 0.00069825
Iteration 11/25 | Loss: 0.00069825
Iteration 12/25 | Loss: 0.00069825
Iteration 13/25 | Loss: 0.00069825
Iteration 14/25 | Loss: 0.00069825
Iteration 15/25 | Loss: 0.00069825
Iteration 16/25 | Loss: 0.00069825
Iteration 17/25 | Loss: 0.00069825
Iteration 18/25 | Loss: 0.00069825
Iteration 19/25 | Loss: 0.00069825
Iteration 20/25 | Loss: 0.00069825
Iteration 21/25 | Loss: 0.00069825
Iteration 22/25 | Loss: 0.00069825
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006982519989833236, 0.0006982519989833236, 0.0006982519989833236, 0.0006982519989833236, 0.0006982519989833236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006982519989833236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069825
Iteration 2/1000 | Loss: 0.00008117
Iteration 3/1000 | Loss: 0.00006223
Iteration 4/1000 | Loss: 0.00005808
Iteration 5/1000 | Loss: 0.00005541
Iteration 6/1000 | Loss: 0.00005381
Iteration 7/1000 | Loss: 0.00005254
Iteration 8/1000 | Loss: 0.00005184
Iteration 9/1000 | Loss: 0.00005115
Iteration 10/1000 | Loss: 0.00005060
Iteration 11/1000 | Loss: 0.00005028
Iteration 12/1000 | Loss: 0.00005010
Iteration 13/1000 | Loss: 0.00004997
Iteration 14/1000 | Loss: 0.00004982
Iteration 15/1000 | Loss: 0.00004975
Iteration 16/1000 | Loss: 0.00004969
Iteration 17/1000 | Loss: 0.00004966
Iteration 18/1000 | Loss: 0.00004964
Iteration 19/1000 | Loss: 0.00004964
Iteration 20/1000 | Loss: 0.00004963
Iteration 21/1000 | Loss: 0.00004963
Iteration 22/1000 | Loss: 0.00004963
Iteration 23/1000 | Loss: 0.00004963
Iteration 24/1000 | Loss: 0.00004963
Iteration 25/1000 | Loss: 0.00004963
Iteration 26/1000 | Loss: 0.00004963
Iteration 27/1000 | Loss: 0.00004963
Iteration 28/1000 | Loss: 0.00004963
Iteration 29/1000 | Loss: 0.00004963
Iteration 30/1000 | Loss: 0.00004962
Iteration 31/1000 | Loss: 0.00004962
Iteration 32/1000 | Loss: 0.00004962
Iteration 33/1000 | Loss: 0.00004960
Iteration 34/1000 | Loss: 0.00004960
Iteration 35/1000 | Loss: 0.00004957
Iteration 36/1000 | Loss: 0.00004957
Iteration 37/1000 | Loss: 0.00004953
Iteration 38/1000 | Loss: 0.00004953
Iteration 39/1000 | Loss: 0.00004953
Iteration 40/1000 | Loss: 0.00004952
Iteration 41/1000 | Loss: 0.00004952
Iteration 42/1000 | Loss: 0.00004951
Iteration 43/1000 | Loss: 0.00004951
Iteration 44/1000 | Loss: 0.00004951
Iteration 45/1000 | Loss: 0.00004950
Iteration 46/1000 | Loss: 0.00004950
Iteration 47/1000 | Loss: 0.00004950
Iteration 48/1000 | Loss: 0.00004950
Iteration 49/1000 | Loss: 0.00004950
Iteration 50/1000 | Loss: 0.00004950
Iteration 51/1000 | Loss: 0.00004950
Iteration 52/1000 | Loss: 0.00004949
Iteration 53/1000 | Loss: 0.00004949
Iteration 54/1000 | Loss: 0.00004949
Iteration 55/1000 | Loss: 0.00004949
Iteration 56/1000 | Loss: 0.00004949
Iteration 57/1000 | Loss: 0.00004949
Iteration 58/1000 | Loss: 0.00004949
Iteration 59/1000 | Loss: 0.00004948
Iteration 60/1000 | Loss: 0.00004948
Iteration 61/1000 | Loss: 0.00004948
Iteration 62/1000 | Loss: 0.00004948
Iteration 63/1000 | Loss: 0.00004948
Iteration 64/1000 | Loss: 0.00004948
Iteration 65/1000 | Loss: 0.00004948
Iteration 66/1000 | Loss: 0.00004948
Iteration 67/1000 | Loss: 0.00004948
Iteration 68/1000 | Loss: 0.00004948
Iteration 69/1000 | Loss: 0.00004948
Iteration 70/1000 | Loss: 0.00004947
Iteration 71/1000 | Loss: 0.00004947
Iteration 72/1000 | Loss: 0.00004947
Iteration 73/1000 | Loss: 0.00004947
Iteration 74/1000 | Loss: 0.00004947
Iteration 75/1000 | Loss: 0.00004946
Iteration 76/1000 | Loss: 0.00004946
Iteration 77/1000 | Loss: 0.00004946
Iteration 78/1000 | Loss: 0.00004946
Iteration 79/1000 | Loss: 0.00004946
Iteration 80/1000 | Loss: 0.00004946
Iteration 81/1000 | Loss: 0.00004946
Iteration 82/1000 | Loss: 0.00004946
Iteration 83/1000 | Loss: 0.00004946
Iteration 84/1000 | Loss: 0.00004946
Iteration 85/1000 | Loss: 0.00004945
Iteration 86/1000 | Loss: 0.00004945
Iteration 87/1000 | Loss: 0.00004945
Iteration 88/1000 | Loss: 0.00004945
Iteration 89/1000 | Loss: 0.00004945
Iteration 90/1000 | Loss: 0.00004945
Iteration 91/1000 | Loss: 0.00004945
Iteration 92/1000 | Loss: 0.00004945
Iteration 93/1000 | Loss: 0.00004945
Iteration 94/1000 | Loss: 0.00004945
Iteration 95/1000 | Loss: 0.00004945
Iteration 96/1000 | Loss: 0.00004945
Iteration 97/1000 | Loss: 0.00004945
Iteration 98/1000 | Loss: 0.00004945
Iteration 99/1000 | Loss: 0.00004945
Iteration 100/1000 | Loss: 0.00004945
Iteration 101/1000 | Loss: 0.00004945
Iteration 102/1000 | Loss: 0.00004945
Iteration 103/1000 | Loss: 0.00004945
Iteration 104/1000 | Loss: 0.00004945
Iteration 105/1000 | Loss: 0.00004945
Iteration 106/1000 | Loss: 0.00004945
Iteration 107/1000 | Loss: 0.00004945
Iteration 108/1000 | Loss: 0.00004945
Iteration 109/1000 | Loss: 0.00004945
Iteration 110/1000 | Loss: 0.00004945
Iteration 111/1000 | Loss: 0.00004945
Iteration 112/1000 | Loss: 0.00004945
Iteration 113/1000 | Loss: 0.00004945
Iteration 114/1000 | Loss: 0.00004945
Iteration 115/1000 | Loss: 0.00004945
Iteration 116/1000 | Loss: 0.00004945
Iteration 117/1000 | Loss: 0.00004945
Iteration 118/1000 | Loss: 0.00004945
Iteration 119/1000 | Loss: 0.00004945
Iteration 120/1000 | Loss: 0.00004945
Iteration 121/1000 | Loss: 0.00004945
Iteration 122/1000 | Loss: 0.00004945
Iteration 123/1000 | Loss: 0.00004945
Iteration 124/1000 | Loss: 0.00004945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [4.945267937728204e-05, 4.945267937728204e-05, 4.945267937728204e-05, 4.945267937728204e-05, 4.945267937728204e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.945267937728204e-05

Optimization complete. Final v2v error: 5.7712907791137695 mm

Highest mean error: 6.912559509277344 mm for frame 96

Lowest mean error: 4.50015115737915 mm for frame 1

Saving results

Total time: 45.343017578125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426941
Iteration 2/25 | Loss: 0.00107944
Iteration 3/25 | Loss: 0.00096220
Iteration 4/25 | Loss: 0.00094858
Iteration 5/25 | Loss: 0.00094620
Iteration 6/25 | Loss: 0.00094493
Iteration 7/25 | Loss: 0.00094469
Iteration 8/25 | Loss: 0.00094469
Iteration 9/25 | Loss: 0.00094469
Iteration 10/25 | Loss: 0.00094469
Iteration 11/25 | Loss: 0.00094469
Iteration 12/25 | Loss: 0.00094469
Iteration 13/25 | Loss: 0.00094469
Iteration 14/25 | Loss: 0.00094469
Iteration 15/25 | Loss: 0.00094469
Iteration 16/25 | Loss: 0.00094469
Iteration 17/25 | Loss: 0.00094469
Iteration 18/25 | Loss: 0.00094469
Iteration 19/25 | Loss: 0.00094469
Iteration 20/25 | Loss: 0.00094469
Iteration 21/25 | Loss: 0.00094469
Iteration 22/25 | Loss: 0.00094469
Iteration 23/25 | Loss: 0.00094469
Iteration 24/25 | Loss: 0.00094469
Iteration 25/25 | Loss: 0.00094469

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48463416
Iteration 2/25 | Loss: 0.00082905
Iteration 3/25 | Loss: 0.00082902
Iteration 4/25 | Loss: 0.00082902
Iteration 5/25 | Loss: 0.00082902
Iteration 6/25 | Loss: 0.00082902
Iteration 7/25 | Loss: 0.00082902
Iteration 8/25 | Loss: 0.00082902
Iteration 9/25 | Loss: 0.00082902
Iteration 10/25 | Loss: 0.00082902
Iteration 11/25 | Loss: 0.00082902
Iteration 12/25 | Loss: 0.00082902
Iteration 13/25 | Loss: 0.00082902
Iteration 14/25 | Loss: 0.00082902
Iteration 15/25 | Loss: 0.00082902
Iteration 16/25 | Loss: 0.00082902
Iteration 17/25 | Loss: 0.00082902
Iteration 18/25 | Loss: 0.00082902
Iteration 19/25 | Loss: 0.00082902
Iteration 20/25 | Loss: 0.00082902
Iteration 21/25 | Loss: 0.00082902
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008290175464935601, 0.0008290175464935601, 0.0008290175464935601, 0.0008290175464935601, 0.0008290175464935601]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008290175464935601

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082902
Iteration 2/1000 | Loss: 0.00003207
Iteration 3/1000 | Loss: 0.00002130
Iteration 4/1000 | Loss: 0.00001832
Iteration 5/1000 | Loss: 0.00001734
Iteration 6/1000 | Loss: 0.00001663
Iteration 7/1000 | Loss: 0.00001625
Iteration 8/1000 | Loss: 0.00001586
Iteration 9/1000 | Loss: 0.00001575
Iteration 10/1000 | Loss: 0.00001575
Iteration 11/1000 | Loss: 0.00001571
Iteration 12/1000 | Loss: 0.00001567
Iteration 13/1000 | Loss: 0.00001566
Iteration 14/1000 | Loss: 0.00001566
Iteration 15/1000 | Loss: 0.00001565
Iteration 16/1000 | Loss: 0.00001564
Iteration 17/1000 | Loss: 0.00001563
Iteration 18/1000 | Loss: 0.00001562
Iteration 19/1000 | Loss: 0.00001561
Iteration 20/1000 | Loss: 0.00001560
Iteration 21/1000 | Loss: 0.00001560
Iteration 22/1000 | Loss: 0.00001559
Iteration 23/1000 | Loss: 0.00001559
Iteration 24/1000 | Loss: 0.00001559
Iteration 25/1000 | Loss: 0.00001558
Iteration 26/1000 | Loss: 0.00001556
Iteration 27/1000 | Loss: 0.00001555
Iteration 28/1000 | Loss: 0.00001555
Iteration 29/1000 | Loss: 0.00001555
Iteration 30/1000 | Loss: 0.00001555
Iteration 31/1000 | Loss: 0.00001555
Iteration 32/1000 | Loss: 0.00001554
Iteration 33/1000 | Loss: 0.00001554
Iteration 34/1000 | Loss: 0.00001554
Iteration 35/1000 | Loss: 0.00001553
Iteration 36/1000 | Loss: 0.00001553
Iteration 37/1000 | Loss: 0.00001552
Iteration 38/1000 | Loss: 0.00001551
Iteration 39/1000 | Loss: 0.00001550
Iteration 40/1000 | Loss: 0.00001550
Iteration 41/1000 | Loss: 0.00001549
Iteration 42/1000 | Loss: 0.00001548
Iteration 43/1000 | Loss: 0.00001548
Iteration 44/1000 | Loss: 0.00001548
Iteration 45/1000 | Loss: 0.00001547
Iteration 46/1000 | Loss: 0.00001547
Iteration 47/1000 | Loss: 0.00001547
Iteration 48/1000 | Loss: 0.00001545
Iteration 49/1000 | Loss: 0.00001545
Iteration 50/1000 | Loss: 0.00001545
Iteration 51/1000 | Loss: 0.00001545
Iteration 52/1000 | Loss: 0.00001545
Iteration 53/1000 | Loss: 0.00001545
Iteration 54/1000 | Loss: 0.00001545
Iteration 55/1000 | Loss: 0.00001545
Iteration 56/1000 | Loss: 0.00001545
Iteration 57/1000 | Loss: 0.00001545
Iteration 58/1000 | Loss: 0.00001544
Iteration 59/1000 | Loss: 0.00001544
Iteration 60/1000 | Loss: 0.00001543
Iteration 61/1000 | Loss: 0.00001543
Iteration 62/1000 | Loss: 0.00001543
Iteration 63/1000 | Loss: 0.00001543
Iteration 64/1000 | Loss: 0.00001543
Iteration 65/1000 | Loss: 0.00001542
Iteration 66/1000 | Loss: 0.00001542
Iteration 67/1000 | Loss: 0.00001541
Iteration 68/1000 | Loss: 0.00001541
Iteration 69/1000 | Loss: 0.00001541
Iteration 70/1000 | Loss: 0.00001541
Iteration 71/1000 | Loss: 0.00001540
Iteration 72/1000 | Loss: 0.00001540
Iteration 73/1000 | Loss: 0.00001540
Iteration 74/1000 | Loss: 0.00001540
Iteration 75/1000 | Loss: 0.00001539
Iteration 76/1000 | Loss: 0.00001539
Iteration 77/1000 | Loss: 0.00001539
Iteration 78/1000 | Loss: 0.00001538
Iteration 79/1000 | Loss: 0.00001538
Iteration 80/1000 | Loss: 0.00001538
Iteration 81/1000 | Loss: 0.00001538
Iteration 82/1000 | Loss: 0.00001538
Iteration 83/1000 | Loss: 0.00001538
Iteration 84/1000 | Loss: 0.00001537
Iteration 85/1000 | Loss: 0.00001537
Iteration 86/1000 | Loss: 0.00001537
Iteration 87/1000 | Loss: 0.00001537
Iteration 88/1000 | Loss: 0.00001537
Iteration 89/1000 | Loss: 0.00001536
Iteration 90/1000 | Loss: 0.00001536
Iteration 91/1000 | Loss: 0.00001536
Iteration 92/1000 | Loss: 0.00001536
Iteration 93/1000 | Loss: 0.00001536
Iteration 94/1000 | Loss: 0.00001536
Iteration 95/1000 | Loss: 0.00001536
Iteration 96/1000 | Loss: 0.00001535
Iteration 97/1000 | Loss: 0.00001535
Iteration 98/1000 | Loss: 0.00001535
Iteration 99/1000 | Loss: 0.00001535
Iteration 100/1000 | Loss: 0.00001535
Iteration 101/1000 | Loss: 0.00001535
Iteration 102/1000 | Loss: 0.00001535
Iteration 103/1000 | Loss: 0.00001535
Iteration 104/1000 | Loss: 0.00001535
Iteration 105/1000 | Loss: 0.00001535
Iteration 106/1000 | Loss: 0.00001535
Iteration 107/1000 | Loss: 0.00001535
Iteration 108/1000 | Loss: 0.00001535
Iteration 109/1000 | Loss: 0.00001535
Iteration 110/1000 | Loss: 0.00001535
Iteration 111/1000 | Loss: 0.00001535
Iteration 112/1000 | Loss: 0.00001535
Iteration 113/1000 | Loss: 0.00001535
Iteration 114/1000 | Loss: 0.00001535
Iteration 115/1000 | Loss: 0.00001535
Iteration 116/1000 | Loss: 0.00001535
Iteration 117/1000 | Loss: 0.00001535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.5345092833740637e-05, 1.5345092833740637e-05, 1.5345092833740637e-05, 1.5345092833740637e-05, 1.5345092833740637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5345092833740637e-05

Optimization complete. Final v2v error: 3.472637891769409 mm

Highest mean error: 3.684236764907837 mm for frame 98

Lowest mean error: 3.1971256732940674 mm for frame 123

Saving results

Total time: 30.481071710586548
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01124331
Iteration 2/25 | Loss: 0.00193056
Iteration 3/25 | Loss: 0.00154330
Iteration 4/25 | Loss: 0.00144228
Iteration 5/25 | Loss: 0.00136723
Iteration 6/25 | Loss: 0.00125978
Iteration 7/25 | Loss: 0.00126468
Iteration 8/25 | Loss: 0.00125277
Iteration 9/25 | Loss: 0.00120585
Iteration 10/25 | Loss: 0.00109789
Iteration 11/25 | Loss: 0.00106324
Iteration 12/25 | Loss: 0.00103950
Iteration 13/25 | Loss: 0.00102392
Iteration 14/25 | Loss: 0.00101390
Iteration 15/25 | Loss: 0.00101002
Iteration 16/25 | Loss: 0.00100409
Iteration 17/25 | Loss: 0.00099508
Iteration 18/25 | Loss: 0.00100013
Iteration 19/25 | Loss: 0.00100681
Iteration 20/25 | Loss: 0.00099576
Iteration 21/25 | Loss: 0.00098629
Iteration 22/25 | Loss: 0.00098356
Iteration 23/25 | Loss: 0.00098767
Iteration 24/25 | Loss: 0.00098646
Iteration 25/25 | Loss: 0.00098669

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54148197
Iteration 2/25 | Loss: 0.00150043
Iteration 3/25 | Loss: 0.00114105
Iteration 4/25 | Loss: 0.00114105
Iteration 5/25 | Loss: 0.00114105
Iteration 6/25 | Loss: 0.00114105
Iteration 7/25 | Loss: 0.00114105
Iteration 8/25 | Loss: 0.00114105
Iteration 9/25 | Loss: 0.00114105
Iteration 10/25 | Loss: 0.00114105
Iteration 11/25 | Loss: 0.00114105
Iteration 12/25 | Loss: 0.00114105
Iteration 13/25 | Loss: 0.00114105
Iteration 14/25 | Loss: 0.00114105
Iteration 15/25 | Loss: 0.00114105
Iteration 16/25 | Loss: 0.00114105
Iteration 17/25 | Loss: 0.00114105
Iteration 18/25 | Loss: 0.00114105
Iteration 19/25 | Loss: 0.00114105
Iteration 20/25 | Loss: 0.00114105
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001141048502177, 0.001141048502177, 0.001141048502177, 0.001141048502177, 0.001141048502177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001141048502177

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114105
Iteration 2/1000 | Loss: 0.00031197
Iteration 3/1000 | Loss: 0.00024887
Iteration 4/1000 | Loss: 0.00024249
Iteration 5/1000 | Loss: 0.00088937
Iteration 6/1000 | Loss: 0.00029892
Iteration 7/1000 | Loss: 0.00022984
Iteration 8/1000 | Loss: 0.00053015
Iteration 9/1000 | Loss: 0.00050507
Iteration 10/1000 | Loss: 0.00035235
Iteration 11/1000 | Loss: 0.00006046
Iteration 12/1000 | Loss: 0.00088714
Iteration 13/1000 | Loss: 0.00041141
Iteration 14/1000 | Loss: 0.00047406
Iteration 15/1000 | Loss: 0.00019918
Iteration 16/1000 | Loss: 0.00005227
Iteration 17/1000 | Loss: 0.00003841
Iteration 18/1000 | Loss: 0.00018989
Iteration 19/1000 | Loss: 0.00026800
Iteration 20/1000 | Loss: 0.00091523
Iteration 21/1000 | Loss: 0.00024229
Iteration 22/1000 | Loss: 0.00053650
Iteration 23/1000 | Loss: 0.00023838
Iteration 24/1000 | Loss: 0.00027146
Iteration 25/1000 | Loss: 0.00003087
Iteration 26/1000 | Loss: 0.00083096
Iteration 27/1000 | Loss: 0.00023170
Iteration 28/1000 | Loss: 0.00002893
Iteration 29/1000 | Loss: 0.00019056
Iteration 30/1000 | Loss: 0.00002381
Iteration 31/1000 | Loss: 0.00002208
Iteration 32/1000 | Loss: 0.00002776
Iteration 33/1000 | Loss: 0.00002080
Iteration 34/1000 | Loss: 0.00001997
Iteration 35/1000 | Loss: 0.00001943
Iteration 36/1000 | Loss: 0.00001898
Iteration 37/1000 | Loss: 0.00001855
Iteration 38/1000 | Loss: 0.00035017
Iteration 39/1000 | Loss: 0.00044487
Iteration 40/1000 | Loss: 0.00008837
Iteration 41/1000 | Loss: 0.00033183
Iteration 42/1000 | Loss: 0.00027816
Iteration 43/1000 | Loss: 0.00022284
Iteration 44/1000 | Loss: 0.00046942
Iteration 45/1000 | Loss: 0.00042230
Iteration 46/1000 | Loss: 0.00016327
Iteration 47/1000 | Loss: 0.00033096
Iteration 48/1000 | Loss: 0.00016428
Iteration 49/1000 | Loss: 0.00027623
Iteration 50/1000 | Loss: 0.00020789
Iteration 51/1000 | Loss: 0.00025580
Iteration 52/1000 | Loss: 0.00005302
Iteration 53/1000 | Loss: 0.00004008
Iteration 54/1000 | Loss: 0.00039061
Iteration 55/1000 | Loss: 0.00027215
Iteration 56/1000 | Loss: 0.00036456
Iteration 57/1000 | Loss: 0.00026040
Iteration 58/1000 | Loss: 0.00053677
Iteration 59/1000 | Loss: 0.00084525
Iteration 60/1000 | Loss: 0.00038098
Iteration 61/1000 | Loss: 0.00025262
Iteration 62/1000 | Loss: 0.00033666
Iteration 63/1000 | Loss: 0.00029753
Iteration 64/1000 | Loss: 0.00018301
Iteration 65/1000 | Loss: 0.00024142
Iteration 66/1000 | Loss: 0.00076326
Iteration 67/1000 | Loss: 0.00020194
Iteration 68/1000 | Loss: 0.00067223
Iteration 69/1000 | Loss: 0.00009231
Iteration 70/1000 | Loss: 0.00041575
Iteration 71/1000 | Loss: 0.00063731
Iteration 72/1000 | Loss: 0.00054346
Iteration 73/1000 | Loss: 0.00023968
Iteration 74/1000 | Loss: 0.00003487
Iteration 75/1000 | Loss: 0.00002604
Iteration 76/1000 | Loss: 0.00002197
Iteration 77/1000 | Loss: 0.00001952
Iteration 78/1000 | Loss: 0.00001877
Iteration 79/1000 | Loss: 0.00001830
Iteration 80/1000 | Loss: 0.00001780
Iteration 81/1000 | Loss: 0.00001744
Iteration 82/1000 | Loss: 0.00001708
Iteration 83/1000 | Loss: 0.00001689
Iteration 84/1000 | Loss: 0.00001675
Iteration 85/1000 | Loss: 0.00001673
Iteration 86/1000 | Loss: 0.00001666
Iteration 87/1000 | Loss: 0.00001642
Iteration 88/1000 | Loss: 0.00001641
Iteration 89/1000 | Loss: 0.00001952
Iteration 90/1000 | Loss: 0.00001706
Iteration 91/1000 | Loss: 0.00001591
Iteration 92/1000 | Loss: 0.00001553
Iteration 93/1000 | Loss: 0.00001545
Iteration 94/1000 | Loss: 0.00001534
Iteration 95/1000 | Loss: 0.00001533
Iteration 96/1000 | Loss: 0.00001533
Iteration 97/1000 | Loss: 0.00001532
Iteration 98/1000 | Loss: 0.00001532
Iteration 99/1000 | Loss: 0.00001531
Iteration 100/1000 | Loss: 0.00001528
Iteration 101/1000 | Loss: 0.00001526
Iteration 102/1000 | Loss: 0.00001526
Iteration 103/1000 | Loss: 0.00001526
Iteration 104/1000 | Loss: 0.00001526
Iteration 105/1000 | Loss: 0.00001525
Iteration 106/1000 | Loss: 0.00001525
Iteration 107/1000 | Loss: 0.00001525
Iteration 108/1000 | Loss: 0.00001525
Iteration 109/1000 | Loss: 0.00001524
Iteration 110/1000 | Loss: 0.00001524
Iteration 111/1000 | Loss: 0.00001523
Iteration 112/1000 | Loss: 0.00001523
Iteration 113/1000 | Loss: 0.00001523
Iteration 114/1000 | Loss: 0.00001522
Iteration 115/1000 | Loss: 0.00001522
Iteration 116/1000 | Loss: 0.00001522
Iteration 117/1000 | Loss: 0.00001522
Iteration 118/1000 | Loss: 0.00001522
Iteration 119/1000 | Loss: 0.00001521
Iteration 120/1000 | Loss: 0.00001521
Iteration 121/1000 | Loss: 0.00001521
Iteration 122/1000 | Loss: 0.00001521
Iteration 123/1000 | Loss: 0.00001520
Iteration 124/1000 | Loss: 0.00001520
Iteration 125/1000 | Loss: 0.00001520
Iteration 126/1000 | Loss: 0.00001520
Iteration 127/1000 | Loss: 0.00001519
Iteration 128/1000 | Loss: 0.00001519
Iteration 129/1000 | Loss: 0.00001519
Iteration 130/1000 | Loss: 0.00001519
Iteration 131/1000 | Loss: 0.00001519
Iteration 132/1000 | Loss: 0.00001519
Iteration 133/1000 | Loss: 0.00001519
Iteration 134/1000 | Loss: 0.00001519
Iteration 135/1000 | Loss: 0.00001519
Iteration 136/1000 | Loss: 0.00001519
Iteration 137/1000 | Loss: 0.00001518
Iteration 138/1000 | Loss: 0.00001518
Iteration 139/1000 | Loss: 0.00001518
Iteration 140/1000 | Loss: 0.00001518
Iteration 141/1000 | Loss: 0.00001518
Iteration 142/1000 | Loss: 0.00001518
Iteration 143/1000 | Loss: 0.00001518
Iteration 144/1000 | Loss: 0.00001518
Iteration 145/1000 | Loss: 0.00001518
Iteration 146/1000 | Loss: 0.00001518
Iteration 147/1000 | Loss: 0.00001518
Iteration 148/1000 | Loss: 0.00001517
Iteration 149/1000 | Loss: 0.00001517
Iteration 150/1000 | Loss: 0.00001517
Iteration 151/1000 | Loss: 0.00001517
Iteration 152/1000 | Loss: 0.00001517
Iteration 153/1000 | Loss: 0.00001517
Iteration 154/1000 | Loss: 0.00001517
Iteration 155/1000 | Loss: 0.00001517
Iteration 156/1000 | Loss: 0.00001516
Iteration 157/1000 | Loss: 0.00001516
Iteration 158/1000 | Loss: 0.00001516
Iteration 159/1000 | Loss: 0.00001516
Iteration 160/1000 | Loss: 0.00001516
Iteration 161/1000 | Loss: 0.00001516
Iteration 162/1000 | Loss: 0.00001516
Iteration 163/1000 | Loss: 0.00001516
Iteration 164/1000 | Loss: 0.00001516
Iteration 165/1000 | Loss: 0.00001516
Iteration 166/1000 | Loss: 0.00001516
Iteration 167/1000 | Loss: 0.00001516
Iteration 168/1000 | Loss: 0.00001516
Iteration 169/1000 | Loss: 0.00001516
Iteration 170/1000 | Loss: 0.00001516
Iteration 171/1000 | Loss: 0.00001516
Iteration 172/1000 | Loss: 0.00001516
Iteration 173/1000 | Loss: 0.00001516
Iteration 174/1000 | Loss: 0.00001516
Iteration 175/1000 | Loss: 0.00001516
Iteration 176/1000 | Loss: 0.00001516
Iteration 177/1000 | Loss: 0.00001515
Iteration 178/1000 | Loss: 0.00001515
Iteration 179/1000 | Loss: 0.00001515
Iteration 180/1000 | Loss: 0.00001515
Iteration 181/1000 | Loss: 0.00001515
Iteration 182/1000 | Loss: 0.00001515
Iteration 183/1000 | Loss: 0.00001515
Iteration 184/1000 | Loss: 0.00001515
Iteration 185/1000 | Loss: 0.00001515
Iteration 186/1000 | Loss: 0.00001515
Iteration 187/1000 | Loss: 0.00001515
Iteration 188/1000 | Loss: 0.00001515
Iteration 189/1000 | Loss: 0.00001515
Iteration 190/1000 | Loss: 0.00001515
Iteration 191/1000 | Loss: 0.00001515
Iteration 192/1000 | Loss: 0.00001515
Iteration 193/1000 | Loss: 0.00001515
Iteration 194/1000 | Loss: 0.00001515
Iteration 195/1000 | Loss: 0.00001515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.5151899788179435e-05, 1.5151899788179435e-05, 1.5151899788179435e-05, 1.5151899788179435e-05, 1.5151899788179435e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5151899788179435e-05

Optimization complete. Final v2v error: 3.3121373653411865 mm

Highest mean error: 4.415309906005859 mm for frame 71

Lowest mean error: 2.8869283199310303 mm for frame 0

Saving results

Total time: 179.43809843063354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00516467
Iteration 2/25 | Loss: 0.00112768
Iteration 3/25 | Loss: 0.00102238
Iteration 4/25 | Loss: 0.00100065
Iteration 5/25 | Loss: 0.00099171
Iteration 6/25 | Loss: 0.00099014
Iteration 7/25 | Loss: 0.00098976
Iteration 8/25 | Loss: 0.00098976
Iteration 9/25 | Loss: 0.00098976
Iteration 10/25 | Loss: 0.00098976
Iteration 11/25 | Loss: 0.00098976
Iteration 12/25 | Loss: 0.00098976
Iteration 13/25 | Loss: 0.00098976
Iteration 14/25 | Loss: 0.00098976
Iteration 15/25 | Loss: 0.00098976
Iteration 16/25 | Loss: 0.00098976
Iteration 17/25 | Loss: 0.00098976
Iteration 18/25 | Loss: 0.00098976
Iteration 19/25 | Loss: 0.00098976
Iteration 20/25 | Loss: 0.00098976
Iteration 21/25 | Loss: 0.00098976
Iteration 22/25 | Loss: 0.00098976
Iteration 23/25 | Loss: 0.00098976
Iteration 24/25 | Loss: 0.00098976
Iteration 25/25 | Loss: 0.00098976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009897599229589105, 0.0009897599229589105, 0.0009897599229589105, 0.0009897599229589105, 0.0009897599229589105]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009897599229589105

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49306726
Iteration 2/25 | Loss: 0.00082343
Iteration 3/25 | Loss: 0.00082339
Iteration 4/25 | Loss: 0.00082339
Iteration 5/25 | Loss: 0.00082339
Iteration 6/25 | Loss: 0.00082339
Iteration 7/25 | Loss: 0.00082339
Iteration 8/25 | Loss: 0.00082339
Iteration 9/25 | Loss: 0.00082339
Iteration 10/25 | Loss: 0.00082339
Iteration 11/25 | Loss: 0.00082339
Iteration 12/25 | Loss: 0.00082339
Iteration 13/25 | Loss: 0.00082339
Iteration 14/25 | Loss: 0.00082339
Iteration 15/25 | Loss: 0.00082339
Iteration 16/25 | Loss: 0.00082339
Iteration 17/25 | Loss: 0.00082339
Iteration 18/25 | Loss: 0.00082339
Iteration 19/25 | Loss: 0.00082339
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008233917760662735, 0.0008233917760662735, 0.0008233917760662735, 0.0008233917760662735, 0.0008233917760662735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008233917760662735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082339
Iteration 2/1000 | Loss: 0.00003496
Iteration 3/1000 | Loss: 0.00002722
Iteration 4/1000 | Loss: 0.00002429
Iteration 5/1000 | Loss: 0.00002293
Iteration 6/1000 | Loss: 0.00002216
Iteration 7/1000 | Loss: 0.00002154
Iteration 8/1000 | Loss: 0.00002110
Iteration 9/1000 | Loss: 0.00002080
Iteration 10/1000 | Loss: 0.00002074
Iteration 11/1000 | Loss: 0.00002073
Iteration 12/1000 | Loss: 0.00002072
Iteration 13/1000 | Loss: 0.00002066
Iteration 14/1000 | Loss: 0.00002066
Iteration 15/1000 | Loss: 0.00002064
Iteration 16/1000 | Loss: 0.00002064
Iteration 17/1000 | Loss: 0.00002063
Iteration 18/1000 | Loss: 0.00002061
Iteration 19/1000 | Loss: 0.00002061
Iteration 20/1000 | Loss: 0.00002060
Iteration 21/1000 | Loss: 0.00002060
Iteration 22/1000 | Loss: 0.00002060
Iteration 23/1000 | Loss: 0.00002059
Iteration 24/1000 | Loss: 0.00002059
Iteration 25/1000 | Loss: 0.00002058
Iteration 26/1000 | Loss: 0.00002058
Iteration 27/1000 | Loss: 0.00002056
Iteration 28/1000 | Loss: 0.00002055
Iteration 29/1000 | Loss: 0.00002055
Iteration 30/1000 | Loss: 0.00002055
Iteration 31/1000 | Loss: 0.00002055
Iteration 32/1000 | Loss: 0.00002054
Iteration 33/1000 | Loss: 0.00002053
Iteration 34/1000 | Loss: 0.00002053
Iteration 35/1000 | Loss: 0.00002053
Iteration 36/1000 | Loss: 0.00002053
Iteration 37/1000 | Loss: 0.00002053
Iteration 38/1000 | Loss: 0.00002053
Iteration 39/1000 | Loss: 0.00002052
Iteration 40/1000 | Loss: 0.00002052
Iteration 41/1000 | Loss: 0.00002052
Iteration 42/1000 | Loss: 0.00002052
Iteration 43/1000 | Loss: 0.00002051
Iteration 44/1000 | Loss: 0.00002051
Iteration 45/1000 | Loss: 0.00002050
Iteration 46/1000 | Loss: 0.00002050
Iteration 47/1000 | Loss: 0.00002050
Iteration 48/1000 | Loss: 0.00002050
Iteration 49/1000 | Loss: 0.00002050
Iteration 50/1000 | Loss: 0.00002050
Iteration 51/1000 | Loss: 0.00002050
Iteration 52/1000 | Loss: 0.00002049
Iteration 53/1000 | Loss: 0.00002049
Iteration 54/1000 | Loss: 0.00002049
Iteration 55/1000 | Loss: 0.00002049
Iteration 56/1000 | Loss: 0.00002049
Iteration 57/1000 | Loss: 0.00002049
Iteration 58/1000 | Loss: 0.00002049
Iteration 59/1000 | Loss: 0.00002049
Iteration 60/1000 | Loss: 0.00002049
Iteration 61/1000 | Loss: 0.00002048
Iteration 62/1000 | Loss: 0.00002048
Iteration 63/1000 | Loss: 0.00002048
Iteration 64/1000 | Loss: 0.00002048
Iteration 65/1000 | Loss: 0.00002048
Iteration 66/1000 | Loss: 0.00002048
Iteration 67/1000 | Loss: 0.00002048
Iteration 68/1000 | Loss: 0.00002047
Iteration 69/1000 | Loss: 0.00002047
Iteration 70/1000 | Loss: 0.00002047
Iteration 71/1000 | Loss: 0.00002047
Iteration 72/1000 | Loss: 0.00002047
Iteration 73/1000 | Loss: 0.00002047
Iteration 74/1000 | Loss: 0.00002047
Iteration 75/1000 | Loss: 0.00002047
Iteration 76/1000 | Loss: 0.00002047
Iteration 77/1000 | Loss: 0.00002046
Iteration 78/1000 | Loss: 0.00002046
Iteration 79/1000 | Loss: 0.00002046
Iteration 80/1000 | Loss: 0.00002046
Iteration 81/1000 | Loss: 0.00002046
Iteration 82/1000 | Loss: 0.00002046
Iteration 83/1000 | Loss: 0.00002046
Iteration 84/1000 | Loss: 0.00002046
Iteration 85/1000 | Loss: 0.00002046
Iteration 86/1000 | Loss: 0.00002046
Iteration 87/1000 | Loss: 0.00002046
Iteration 88/1000 | Loss: 0.00002046
Iteration 89/1000 | Loss: 0.00002045
Iteration 90/1000 | Loss: 0.00002045
Iteration 91/1000 | Loss: 0.00002045
Iteration 92/1000 | Loss: 0.00002045
Iteration 93/1000 | Loss: 0.00002044
Iteration 94/1000 | Loss: 0.00002044
Iteration 95/1000 | Loss: 0.00002044
Iteration 96/1000 | Loss: 0.00002044
Iteration 97/1000 | Loss: 0.00002044
Iteration 98/1000 | Loss: 0.00002044
Iteration 99/1000 | Loss: 0.00002044
Iteration 100/1000 | Loss: 0.00002044
Iteration 101/1000 | Loss: 0.00002044
Iteration 102/1000 | Loss: 0.00002043
Iteration 103/1000 | Loss: 0.00002043
Iteration 104/1000 | Loss: 0.00002043
Iteration 105/1000 | Loss: 0.00002042
Iteration 106/1000 | Loss: 0.00002042
Iteration 107/1000 | Loss: 0.00002042
Iteration 108/1000 | Loss: 0.00002042
Iteration 109/1000 | Loss: 0.00002042
Iteration 110/1000 | Loss: 0.00002042
Iteration 111/1000 | Loss: 0.00002042
Iteration 112/1000 | Loss: 0.00002042
Iteration 113/1000 | Loss: 0.00002041
Iteration 114/1000 | Loss: 0.00002041
Iteration 115/1000 | Loss: 0.00002041
Iteration 116/1000 | Loss: 0.00002041
Iteration 117/1000 | Loss: 0.00002041
Iteration 118/1000 | Loss: 0.00002041
Iteration 119/1000 | Loss: 0.00002041
Iteration 120/1000 | Loss: 0.00002041
Iteration 121/1000 | Loss: 0.00002041
Iteration 122/1000 | Loss: 0.00002041
Iteration 123/1000 | Loss: 0.00002041
Iteration 124/1000 | Loss: 0.00002041
Iteration 125/1000 | Loss: 0.00002041
Iteration 126/1000 | Loss: 0.00002041
Iteration 127/1000 | Loss: 0.00002041
Iteration 128/1000 | Loss: 0.00002041
Iteration 129/1000 | Loss: 0.00002041
Iteration 130/1000 | Loss: 0.00002040
Iteration 131/1000 | Loss: 0.00002040
Iteration 132/1000 | Loss: 0.00002040
Iteration 133/1000 | Loss: 0.00002040
Iteration 134/1000 | Loss: 0.00002040
Iteration 135/1000 | Loss: 0.00002040
Iteration 136/1000 | Loss: 0.00002040
Iteration 137/1000 | Loss: 0.00002040
Iteration 138/1000 | Loss: 0.00002040
Iteration 139/1000 | Loss: 0.00002040
Iteration 140/1000 | Loss: 0.00002040
Iteration 141/1000 | Loss: 0.00002040
Iteration 142/1000 | Loss: 0.00002040
Iteration 143/1000 | Loss: 0.00002040
Iteration 144/1000 | Loss: 0.00002040
Iteration 145/1000 | Loss: 0.00002040
Iteration 146/1000 | Loss: 0.00002040
Iteration 147/1000 | Loss: 0.00002040
Iteration 148/1000 | Loss: 0.00002039
Iteration 149/1000 | Loss: 0.00002039
Iteration 150/1000 | Loss: 0.00002039
Iteration 151/1000 | Loss: 0.00002039
Iteration 152/1000 | Loss: 0.00002039
Iteration 153/1000 | Loss: 0.00002039
Iteration 154/1000 | Loss: 0.00002039
Iteration 155/1000 | Loss: 0.00002039
Iteration 156/1000 | Loss: 0.00002039
Iteration 157/1000 | Loss: 0.00002039
Iteration 158/1000 | Loss: 0.00002039
Iteration 159/1000 | Loss: 0.00002038
Iteration 160/1000 | Loss: 0.00002038
Iteration 161/1000 | Loss: 0.00002038
Iteration 162/1000 | Loss: 0.00002038
Iteration 163/1000 | Loss: 0.00002038
Iteration 164/1000 | Loss: 0.00002038
Iteration 165/1000 | Loss: 0.00002038
Iteration 166/1000 | Loss: 0.00002038
Iteration 167/1000 | Loss: 0.00002038
Iteration 168/1000 | Loss: 0.00002038
Iteration 169/1000 | Loss: 0.00002038
Iteration 170/1000 | Loss: 0.00002038
Iteration 171/1000 | Loss: 0.00002038
Iteration 172/1000 | Loss: 0.00002038
Iteration 173/1000 | Loss: 0.00002038
Iteration 174/1000 | Loss: 0.00002038
Iteration 175/1000 | Loss: 0.00002038
Iteration 176/1000 | Loss: 0.00002038
Iteration 177/1000 | Loss: 0.00002038
Iteration 178/1000 | Loss: 0.00002038
Iteration 179/1000 | Loss: 0.00002038
Iteration 180/1000 | Loss: 0.00002037
Iteration 181/1000 | Loss: 0.00002037
Iteration 182/1000 | Loss: 0.00002037
Iteration 183/1000 | Loss: 0.00002037
Iteration 184/1000 | Loss: 0.00002037
Iteration 185/1000 | Loss: 0.00002037
Iteration 186/1000 | Loss: 0.00002037
Iteration 187/1000 | Loss: 0.00002037
Iteration 188/1000 | Loss: 0.00002037
Iteration 189/1000 | Loss: 0.00002037
Iteration 190/1000 | Loss: 0.00002037
Iteration 191/1000 | Loss: 0.00002037
Iteration 192/1000 | Loss: 0.00002037
Iteration 193/1000 | Loss: 0.00002037
Iteration 194/1000 | Loss: 0.00002036
Iteration 195/1000 | Loss: 0.00002036
Iteration 196/1000 | Loss: 0.00002036
Iteration 197/1000 | Loss: 0.00002036
Iteration 198/1000 | Loss: 0.00002036
Iteration 199/1000 | Loss: 0.00002036
Iteration 200/1000 | Loss: 0.00002036
Iteration 201/1000 | Loss: 0.00002036
Iteration 202/1000 | Loss: 0.00002036
Iteration 203/1000 | Loss: 0.00002036
Iteration 204/1000 | Loss: 0.00002036
Iteration 205/1000 | Loss: 0.00002036
Iteration 206/1000 | Loss: 0.00002036
Iteration 207/1000 | Loss: 0.00002036
Iteration 208/1000 | Loss: 0.00002036
Iteration 209/1000 | Loss: 0.00002036
Iteration 210/1000 | Loss: 0.00002036
Iteration 211/1000 | Loss: 0.00002036
Iteration 212/1000 | Loss: 0.00002036
Iteration 213/1000 | Loss: 0.00002036
Iteration 214/1000 | Loss: 0.00002036
Iteration 215/1000 | Loss: 0.00002036
Iteration 216/1000 | Loss: 0.00002036
Iteration 217/1000 | Loss: 0.00002036
Iteration 218/1000 | Loss: 0.00002036
Iteration 219/1000 | Loss: 0.00002036
Iteration 220/1000 | Loss: 0.00002036
Iteration 221/1000 | Loss: 0.00002036
Iteration 222/1000 | Loss: 0.00002036
Iteration 223/1000 | Loss: 0.00002036
Iteration 224/1000 | Loss: 0.00002036
Iteration 225/1000 | Loss: 0.00002036
Iteration 226/1000 | Loss: 0.00002036
Iteration 227/1000 | Loss: 0.00002036
Iteration 228/1000 | Loss: 0.00002036
Iteration 229/1000 | Loss: 0.00002036
Iteration 230/1000 | Loss: 0.00002036
Iteration 231/1000 | Loss: 0.00002036
Iteration 232/1000 | Loss: 0.00002036
Iteration 233/1000 | Loss: 0.00002036
Iteration 234/1000 | Loss: 0.00002036
Iteration 235/1000 | Loss: 0.00002036
Iteration 236/1000 | Loss: 0.00002036
Iteration 237/1000 | Loss: 0.00002036
Iteration 238/1000 | Loss: 0.00002036
Iteration 239/1000 | Loss: 0.00002036
Iteration 240/1000 | Loss: 0.00002036
Iteration 241/1000 | Loss: 0.00002036
Iteration 242/1000 | Loss: 0.00002036
Iteration 243/1000 | Loss: 0.00002036
Iteration 244/1000 | Loss: 0.00002036
Iteration 245/1000 | Loss: 0.00002036
Iteration 246/1000 | Loss: 0.00002036
Iteration 247/1000 | Loss: 0.00002036
Iteration 248/1000 | Loss: 0.00002036
Iteration 249/1000 | Loss: 0.00002036
Iteration 250/1000 | Loss: 0.00002036
Iteration 251/1000 | Loss: 0.00002036
Iteration 252/1000 | Loss: 0.00002036
Iteration 253/1000 | Loss: 0.00002036
Iteration 254/1000 | Loss: 0.00002036
Iteration 255/1000 | Loss: 0.00002036
Iteration 256/1000 | Loss: 0.00002036
Iteration 257/1000 | Loss: 0.00002036
Iteration 258/1000 | Loss: 0.00002036
Iteration 259/1000 | Loss: 0.00002036
Iteration 260/1000 | Loss: 0.00002036
Iteration 261/1000 | Loss: 0.00002036
Iteration 262/1000 | Loss: 0.00002036
Iteration 263/1000 | Loss: 0.00002036
Iteration 264/1000 | Loss: 0.00002036
Iteration 265/1000 | Loss: 0.00002036
Iteration 266/1000 | Loss: 0.00002036
Iteration 267/1000 | Loss: 0.00002036
Iteration 268/1000 | Loss: 0.00002036
Iteration 269/1000 | Loss: 0.00002036
Iteration 270/1000 | Loss: 0.00002036
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [2.035988109128084e-05, 2.035988109128084e-05, 2.035988109128084e-05, 2.035988109128084e-05, 2.035988109128084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.035988109128084e-05

Optimization complete. Final v2v error: 3.8852336406707764 mm

Highest mean error: 4.642120838165283 mm for frame 50

Lowest mean error: 3.490873098373413 mm for frame 11

Saving results

Total time: 38.84771466255188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992342
Iteration 2/25 | Loss: 0.00159305
Iteration 3/25 | Loss: 0.00116939
Iteration 4/25 | Loss: 0.00112282
Iteration 5/25 | Loss: 0.00109907
Iteration 6/25 | Loss: 0.00109160
Iteration 7/25 | Loss: 0.00109075
Iteration 8/25 | Loss: 0.00109075
Iteration 9/25 | Loss: 0.00109075
Iteration 10/25 | Loss: 0.00109075
Iteration 11/25 | Loss: 0.00109075
Iteration 12/25 | Loss: 0.00109075
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010907523101195693, 0.0010907523101195693, 0.0010907523101195693, 0.0010907523101195693, 0.0010907523101195693]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010907523101195693

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13939142
Iteration 2/25 | Loss: 0.00072982
Iteration 3/25 | Loss: 0.00072981
Iteration 4/25 | Loss: 0.00072981
Iteration 5/25 | Loss: 0.00072981
Iteration 6/25 | Loss: 0.00072980
Iteration 7/25 | Loss: 0.00072980
Iteration 8/25 | Loss: 0.00072980
Iteration 9/25 | Loss: 0.00072980
Iteration 10/25 | Loss: 0.00072980
Iteration 11/25 | Loss: 0.00072980
Iteration 12/25 | Loss: 0.00072980
Iteration 13/25 | Loss: 0.00072980
Iteration 14/25 | Loss: 0.00072980
Iteration 15/25 | Loss: 0.00072980
Iteration 16/25 | Loss: 0.00072980
Iteration 17/25 | Loss: 0.00072980
Iteration 18/25 | Loss: 0.00072980
Iteration 19/25 | Loss: 0.00072980
Iteration 20/25 | Loss: 0.00072980
Iteration 21/25 | Loss: 0.00072980
Iteration 22/25 | Loss: 0.00072980
Iteration 23/25 | Loss: 0.00072980
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007298034033738077, 0.0007298034033738077, 0.0007298034033738077, 0.0007298034033738077, 0.0007298034033738077]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007298034033738077

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072980
Iteration 2/1000 | Loss: 0.00006181
Iteration 3/1000 | Loss: 0.00004813
Iteration 4/1000 | Loss: 0.00004242
Iteration 5/1000 | Loss: 0.00004007
Iteration 6/1000 | Loss: 0.00003827
Iteration 7/1000 | Loss: 0.00003706
Iteration 8/1000 | Loss: 0.00003627
Iteration 9/1000 | Loss: 0.00003579
Iteration 10/1000 | Loss: 0.00003550
Iteration 11/1000 | Loss: 0.00003533
Iteration 12/1000 | Loss: 0.00003531
Iteration 13/1000 | Loss: 0.00003524
Iteration 14/1000 | Loss: 0.00003523
Iteration 15/1000 | Loss: 0.00003521
Iteration 16/1000 | Loss: 0.00003519
Iteration 17/1000 | Loss: 0.00003519
Iteration 18/1000 | Loss: 0.00003519
Iteration 19/1000 | Loss: 0.00003518
Iteration 20/1000 | Loss: 0.00003518
Iteration 21/1000 | Loss: 0.00003518
Iteration 22/1000 | Loss: 0.00003518
Iteration 23/1000 | Loss: 0.00003518
Iteration 24/1000 | Loss: 0.00003518
Iteration 25/1000 | Loss: 0.00003518
Iteration 26/1000 | Loss: 0.00003518
Iteration 27/1000 | Loss: 0.00003516
Iteration 28/1000 | Loss: 0.00003516
Iteration 29/1000 | Loss: 0.00003516
Iteration 30/1000 | Loss: 0.00003515
Iteration 31/1000 | Loss: 0.00003515
Iteration 32/1000 | Loss: 0.00003515
Iteration 33/1000 | Loss: 0.00003514
Iteration 34/1000 | Loss: 0.00003512
Iteration 35/1000 | Loss: 0.00003510
Iteration 36/1000 | Loss: 0.00003509
Iteration 37/1000 | Loss: 0.00003506
Iteration 38/1000 | Loss: 0.00003505
Iteration 39/1000 | Loss: 0.00003505
Iteration 40/1000 | Loss: 0.00003505
Iteration 41/1000 | Loss: 0.00003504
Iteration 42/1000 | Loss: 0.00003504
Iteration 43/1000 | Loss: 0.00003503
Iteration 44/1000 | Loss: 0.00003503
Iteration 45/1000 | Loss: 0.00003503
Iteration 46/1000 | Loss: 0.00003502
Iteration 47/1000 | Loss: 0.00003502
Iteration 48/1000 | Loss: 0.00003502
Iteration 49/1000 | Loss: 0.00003502
Iteration 50/1000 | Loss: 0.00003502
Iteration 51/1000 | Loss: 0.00003502
Iteration 52/1000 | Loss: 0.00003502
Iteration 53/1000 | Loss: 0.00003502
Iteration 54/1000 | Loss: 0.00003501
Iteration 55/1000 | Loss: 0.00003501
Iteration 56/1000 | Loss: 0.00003501
Iteration 57/1000 | Loss: 0.00003498
Iteration 58/1000 | Loss: 0.00003498
Iteration 59/1000 | Loss: 0.00003498
Iteration 60/1000 | Loss: 0.00003497
Iteration 61/1000 | Loss: 0.00003497
Iteration 62/1000 | Loss: 0.00003496
Iteration 63/1000 | Loss: 0.00003495
Iteration 64/1000 | Loss: 0.00003494
Iteration 65/1000 | Loss: 0.00003494
Iteration 66/1000 | Loss: 0.00003494
Iteration 67/1000 | Loss: 0.00003493
Iteration 68/1000 | Loss: 0.00003493
Iteration 69/1000 | Loss: 0.00003492
Iteration 70/1000 | Loss: 0.00003492
Iteration 71/1000 | Loss: 0.00003491
Iteration 72/1000 | Loss: 0.00003491
Iteration 73/1000 | Loss: 0.00003491
Iteration 74/1000 | Loss: 0.00003490
Iteration 75/1000 | Loss: 0.00003490
Iteration 76/1000 | Loss: 0.00003490
Iteration 77/1000 | Loss: 0.00003489
Iteration 78/1000 | Loss: 0.00003488
Iteration 79/1000 | Loss: 0.00003488
Iteration 80/1000 | Loss: 0.00003488
Iteration 81/1000 | Loss: 0.00003488
Iteration 82/1000 | Loss: 0.00003488
Iteration 83/1000 | Loss: 0.00003488
Iteration 84/1000 | Loss: 0.00003487
Iteration 85/1000 | Loss: 0.00003487
Iteration 86/1000 | Loss: 0.00003486
Iteration 87/1000 | Loss: 0.00003486
Iteration 88/1000 | Loss: 0.00003486
Iteration 89/1000 | Loss: 0.00003486
Iteration 90/1000 | Loss: 0.00003486
Iteration 91/1000 | Loss: 0.00003485
Iteration 92/1000 | Loss: 0.00003485
Iteration 93/1000 | Loss: 0.00003485
Iteration 94/1000 | Loss: 0.00003484
Iteration 95/1000 | Loss: 0.00003484
Iteration 96/1000 | Loss: 0.00003484
Iteration 97/1000 | Loss: 0.00003483
Iteration 98/1000 | Loss: 0.00003483
Iteration 99/1000 | Loss: 0.00003483
Iteration 100/1000 | Loss: 0.00003482
Iteration 101/1000 | Loss: 0.00003481
Iteration 102/1000 | Loss: 0.00003481
Iteration 103/1000 | Loss: 0.00003481
Iteration 104/1000 | Loss: 0.00003481
Iteration 105/1000 | Loss: 0.00003481
Iteration 106/1000 | Loss: 0.00003481
Iteration 107/1000 | Loss: 0.00003481
Iteration 108/1000 | Loss: 0.00003481
Iteration 109/1000 | Loss: 0.00003481
Iteration 110/1000 | Loss: 0.00003481
Iteration 111/1000 | Loss: 0.00003481
Iteration 112/1000 | Loss: 0.00003481
Iteration 113/1000 | Loss: 0.00003480
Iteration 114/1000 | Loss: 0.00003480
Iteration 115/1000 | Loss: 0.00003479
Iteration 116/1000 | Loss: 0.00003479
Iteration 117/1000 | Loss: 0.00003478
Iteration 118/1000 | Loss: 0.00003478
Iteration 119/1000 | Loss: 0.00003477
Iteration 120/1000 | Loss: 0.00003477
Iteration 121/1000 | Loss: 0.00003477
Iteration 122/1000 | Loss: 0.00003475
Iteration 123/1000 | Loss: 0.00003475
Iteration 124/1000 | Loss: 0.00003475
Iteration 125/1000 | Loss: 0.00003475
Iteration 126/1000 | Loss: 0.00003475
Iteration 127/1000 | Loss: 0.00003475
Iteration 128/1000 | Loss: 0.00003474
Iteration 129/1000 | Loss: 0.00003474
Iteration 130/1000 | Loss: 0.00003473
Iteration 131/1000 | Loss: 0.00003472
Iteration 132/1000 | Loss: 0.00003472
Iteration 133/1000 | Loss: 0.00003472
Iteration 134/1000 | Loss: 0.00003472
Iteration 135/1000 | Loss: 0.00003472
Iteration 136/1000 | Loss: 0.00003472
Iteration 137/1000 | Loss: 0.00003472
Iteration 138/1000 | Loss: 0.00003471
Iteration 139/1000 | Loss: 0.00003471
Iteration 140/1000 | Loss: 0.00003471
Iteration 141/1000 | Loss: 0.00003470
Iteration 142/1000 | Loss: 0.00003470
Iteration 143/1000 | Loss: 0.00003470
Iteration 144/1000 | Loss: 0.00003470
Iteration 145/1000 | Loss: 0.00003470
Iteration 146/1000 | Loss: 0.00003470
Iteration 147/1000 | Loss: 0.00003470
Iteration 148/1000 | Loss: 0.00003470
Iteration 149/1000 | Loss: 0.00003469
Iteration 150/1000 | Loss: 0.00003469
Iteration 151/1000 | Loss: 0.00003469
Iteration 152/1000 | Loss: 0.00003469
Iteration 153/1000 | Loss: 0.00003469
Iteration 154/1000 | Loss: 0.00003469
Iteration 155/1000 | Loss: 0.00003469
Iteration 156/1000 | Loss: 0.00003469
Iteration 157/1000 | Loss: 0.00003469
Iteration 158/1000 | Loss: 0.00003469
Iteration 159/1000 | Loss: 0.00003469
Iteration 160/1000 | Loss: 0.00003469
Iteration 161/1000 | Loss: 0.00003468
Iteration 162/1000 | Loss: 0.00003468
Iteration 163/1000 | Loss: 0.00003468
Iteration 164/1000 | Loss: 0.00003468
Iteration 165/1000 | Loss: 0.00003467
Iteration 166/1000 | Loss: 0.00003467
Iteration 167/1000 | Loss: 0.00003467
Iteration 168/1000 | Loss: 0.00003467
Iteration 169/1000 | Loss: 0.00003467
Iteration 170/1000 | Loss: 0.00003467
Iteration 171/1000 | Loss: 0.00003467
Iteration 172/1000 | Loss: 0.00003466
Iteration 173/1000 | Loss: 0.00003466
Iteration 174/1000 | Loss: 0.00003466
Iteration 175/1000 | Loss: 0.00003466
Iteration 176/1000 | Loss: 0.00003466
Iteration 177/1000 | Loss: 0.00003466
Iteration 178/1000 | Loss: 0.00003466
Iteration 179/1000 | Loss: 0.00003466
Iteration 180/1000 | Loss: 0.00003466
Iteration 181/1000 | Loss: 0.00003466
Iteration 182/1000 | Loss: 0.00003466
Iteration 183/1000 | Loss: 0.00003466
Iteration 184/1000 | Loss: 0.00003465
Iteration 185/1000 | Loss: 0.00003465
Iteration 186/1000 | Loss: 0.00003465
Iteration 187/1000 | Loss: 0.00003465
Iteration 188/1000 | Loss: 0.00003465
Iteration 189/1000 | Loss: 0.00003465
Iteration 190/1000 | Loss: 0.00003465
Iteration 191/1000 | Loss: 0.00003465
Iteration 192/1000 | Loss: 0.00003465
Iteration 193/1000 | Loss: 0.00003465
Iteration 194/1000 | Loss: 0.00003465
Iteration 195/1000 | Loss: 0.00003465
Iteration 196/1000 | Loss: 0.00003465
Iteration 197/1000 | Loss: 0.00003464
Iteration 198/1000 | Loss: 0.00003464
Iteration 199/1000 | Loss: 0.00003464
Iteration 200/1000 | Loss: 0.00003464
Iteration 201/1000 | Loss: 0.00003464
Iteration 202/1000 | Loss: 0.00003464
Iteration 203/1000 | Loss: 0.00003464
Iteration 204/1000 | Loss: 0.00003464
Iteration 205/1000 | Loss: 0.00003464
Iteration 206/1000 | Loss: 0.00003464
Iteration 207/1000 | Loss: 0.00003464
Iteration 208/1000 | Loss: 0.00003463
Iteration 209/1000 | Loss: 0.00003463
Iteration 210/1000 | Loss: 0.00003463
Iteration 211/1000 | Loss: 0.00003463
Iteration 212/1000 | Loss: 0.00003463
Iteration 213/1000 | Loss: 0.00003463
Iteration 214/1000 | Loss: 0.00003463
Iteration 215/1000 | Loss: 0.00003463
Iteration 216/1000 | Loss: 0.00003463
Iteration 217/1000 | Loss: 0.00003463
Iteration 218/1000 | Loss: 0.00003463
Iteration 219/1000 | Loss: 0.00003462
Iteration 220/1000 | Loss: 0.00003462
Iteration 221/1000 | Loss: 0.00003462
Iteration 222/1000 | Loss: 0.00003462
Iteration 223/1000 | Loss: 0.00003462
Iteration 224/1000 | Loss: 0.00003462
Iteration 225/1000 | Loss: 0.00003462
Iteration 226/1000 | Loss: 0.00003462
Iteration 227/1000 | Loss: 0.00003462
Iteration 228/1000 | Loss: 0.00003461
Iteration 229/1000 | Loss: 0.00003461
Iteration 230/1000 | Loss: 0.00003461
Iteration 231/1000 | Loss: 0.00003461
Iteration 232/1000 | Loss: 0.00003461
Iteration 233/1000 | Loss: 0.00003460
Iteration 234/1000 | Loss: 0.00003460
Iteration 235/1000 | Loss: 0.00003460
Iteration 236/1000 | Loss: 0.00003460
Iteration 237/1000 | Loss: 0.00003460
Iteration 238/1000 | Loss: 0.00003460
Iteration 239/1000 | Loss: 0.00003460
Iteration 240/1000 | Loss: 0.00003460
Iteration 241/1000 | Loss: 0.00003460
Iteration 242/1000 | Loss: 0.00003460
Iteration 243/1000 | Loss: 0.00003459
Iteration 244/1000 | Loss: 0.00003459
Iteration 245/1000 | Loss: 0.00003459
Iteration 246/1000 | Loss: 0.00003459
Iteration 247/1000 | Loss: 0.00003459
Iteration 248/1000 | Loss: 0.00003459
Iteration 249/1000 | Loss: 0.00003459
Iteration 250/1000 | Loss: 0.00003459
Iteration 251/1000 | Loss: 0.00003459
Iteration 252/1000 | Loss: 0.00003459
Iteration 253/1000 | Loss: 0.00003459
Iteration 254/1000 | Loss: 0.00003458
Iteration 255/1000 | Loss: 0.00003458
Iteration 256/1000 | Loss: 0.00003458
Iteration 257/1000 | Loss: 0.00003458
Iteration 258/1000 | Loss: 0.00003458
Iteration 259/1000 | Loss: 0.00003458
Iteration 260/1000 | Loss: 0.00003458
Iteration 261/1000 | Loss: 0.00003458
Iteration 262/1000 | Loss: 0.00003458
Iteration 263/1000 | Loss: 0.00003458
Iteration 264/1000 | Loss: 0.00003458
Iteration 265/1000 | Loss: 0.00003458
Iteration 266/1000 | Loss: 0.00003458
Iteration 267/1000 | Loss: 0.00003458
Iteration 268/1000 | Loss: 0.00003458
Iteration 269/1000 | Loss: 0.00003458
Iteration 270/1000 | Loss: 0.00003458
Iteration 271/1000 | Loss: 0.00003458
Iteration 272/1000 | Loss: 0.00003458
Iteration 273/1000 | Loss: 0.00003458
Iteration 274/1000 | Loss: 0.00003458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 274. Stopping optimization.
Last 5 losses: [3.4579697967274114e-05, 3.4579697967274114e-05, 3.4579697967274114e-05, 3.4579697967274114e-05, 3.4579697967274114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4579697967274114e-05

Optimization complete. Final v2v error: 4.7433624267578125 mm

Highest mean error: 5.863412380218506 mm for frame 102

Lowest mean error: 3.744861602783203 mm for frame 38

Saving results

Total time: 47.800095558166504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435147
Iteration 2/25 | Loss: 0.00117730
Iteration 3/25 | Loss: 0.00102556
Iteration 4/25 | Loss: 0.00100094
Iteration 5/25 | Loss: 0.00099493
Iteration 6/25 | Loss: 0.00099264
Iteration 7/25 | Loss: 0.00099206
Iteration 8/25 | Loss: 0.00099206
Iteration 9/25 | Loss: 0.00099206
Iteration 10/25 | Loss: 0.00099206
Iteration 11/25 | Loss: 0.00099206
Iteration 12/25 | Loss: 0.00099206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000992064829915762, 0.000992064829915762, 0.000992064829915762, 0.000992064829915762, 0.000992064829915762]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000992064829915762

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48189223
Iteration 2/25 | Loss: 0.00083313
Iteration 3/25 | Loss: 0.00083313
Iteration 4/25 | Loss: 0.00083313
Iteration 5/25 | Loss: 0.00083313
Iteration 6/25 | Loss: 0.00083313
Iteration 7/25 | Loss: 0.00083313
Iteration 8/25 | Loss: 0.00083313
Iteration 9/25 | Loss: 0.00083313
Iteration 10/25 | Loss: 0.00083313
Iteration 11/25 | Loss: 0.00083313
Iteration 12/25 | Loss: 0.00083313
Iteration 13/25 | Loss: 0.00083313
Iteration 14/25 | Loss: 0.00083313
Iteration 15/25 | Loss: 0.00083313
Iteration 16/25 | Loss: 0.00083313
Iteration 17/25 | Loss: 0.00083313
Iteration 18/25 | Loss: 0.00083313
Iteration 19/25 | Loss: 0.00083313
Iteration 20/25 | Loss: 0.00083313
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008331304416060448, 0.0008331304416060448, 0.0008331304416060448, 0.0008331304416060448, 0.0008331304416060448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008331304416060448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083313
Iteration 2/1000 | Loss: 0.00003481
Iteration 3/1000 | Loss: 0.00002718
Iteration 4/1000 | Loss: 0.00002489
Iteration 5/1000 | Loss: 0.00002390
Iteration 6/1000 | Loss: 0.00002293
Iteration 7/1000 | Loss: 0.00002217
Iteration 8/1000 | Loss: 0.00002173
Iteration 9/1000 | Loss: 0.00002154
Iteration 10/1000 | Loss: 0.00002139
Iteration 11/1000 | Loss: 0.00002138
Iteration 12/1000 | Loss: 0.00002138
Iteration 13/1000 | Loss: 0.00002133
Iteration 14/1000 | Loss: 0.00002127
Iteration 15/1000 | Loss: 0.00002125
Iteration 16/1000 | Loss: 0.00002117
Iteration 17/1000 | Loss: 0.00002117
Iteration 18/1000 | Loss: 0.00002112
Iteration 19/1000 | Loss: 0.00002111
Iteration 20/1000 | Loss: 0.00002111
Iteration 21/1000 | Loss: 0.00002111
Iteration 22/1000 | Loss: 0.00002111
Iteration 23/1000 | Loss: 0.00002107
Iteration 24/1000 | Loss: 0.00002104
Iteration 25/1000 | Loss: 0.00002102
Iteration 26/1000 | Loss: 0.00002102
Iteration 27/1000 | Loss: 0.00002102
Iteration 28/1000 | Loss: 0.00002101
Iteration 29/1000 | Loss: 0.00002101
Iteration 30/1000 | Loss: 0.00002100
Iteration 31/1000 | Loss: 0.00002100
Iteration 32/1000 | Loss: 0.00002100
Iteration 33/1000 | Loss: 0.00002100
Iteration 34/1000 | Loss: 0.00002099
Iteration 35/1000 | Loss: 0.00002099
Iteration 36/1000 | Loss: 0.00002099
Iteration 37/1000 | Loss: 0.00002099
Iteration 38/1000 | Loss: 0.00002098
Iteration 39/1000 | Loss: 0.00002098
Iteration 40/1000 | Loss: 0.00002098
Iteration 41/1000 | Loss: 0.00002098
Iteration 42/1000 | Loss: 0.00002098
Iteration 43/1000 | Loss: 0.00002098
Iteration 44/1000 | Loss: 0.00002097
Iteration 45/1000 | Loss: 0.00002097
Iteration 46/1000 | Loss: 0.00002097
Iteration 47/1000 | Loss: 0.00002096
Iteration 48/1000 | Loss: 0.00002096
Iteration 49/1000 | Loss: 0.00002095
Iteration 50/1000 | Loss: 0.00002095
Iteration 51/1000 | Loss: 0.00002095
Iteration 52/1000 | Loss: 0.00002095
Iteration 53/1000 | Loss: 0.00002094
Iteration 54/1000 | Loss: 0.00002093
Iteration 55/1000 | Loss: 0.00002093
Iteration 56/1000 | Loss: 0.00002093
Iteration 57/1000 | Loss: 0.00002093
Iteration 58/1000 | Loss: 0.00002093
Iteration 59/1000 | Loss: 0.00002092
Iteration 60/1000 | Loss: 0.00002092
Iteration 61/1000 | Loss: 0.00002092
Iteration 62/1000 | Loss: 0.00002092
Iteration 63/1000 | Loss: 0.00002092
Iteration 64/1000 | Loss: 0.00002092
Iteration 65/1000 | Loss: 0.00002092
Iteration 66/1000 | Loss: 0.00002092
Iteration 67/1000 | Loss: 0.00002092
Iteration 68/1000 | Loss: 0.00002092
Iteration 69/1000 | Loss: 0.00002092
Iteration 70/1000 | Loss: 0.00002091
Iteration 71/1000 | Loss: 0.00002091
Iteration 72/1000 | Loss: 0.00002091
Iteration 73/1000 | Loss: 0.00002091
Iteration 74/1000 | Loss: 0.00002090
Iteration 75/1000 | Loss: 0.00002090
Iteration 76/1000 | Loss: 0.00002090
Iteration 77/1000 | Loss: 0.00002090
Iteration 78/1000 | Loss: 0.00002090
Iteration 79/1000 | Loss: 0.00002090
Iteration 80/1000 | Loss: 0.00002090
Iteration 81/1000 | Loss: 0.00002090
Iteration 82/1000 | Loss: 0.00002090
Iteration 83/1000 | Loss: 0.00002090
Iteration 84/1000 | Loss: 0.00002090
Iteration 85/1000 | Loss: 0.00002089
Iteration 86/1000 | Loss: 0.00002089
Iteration 87/1000 | Loss: 0.00002089
Iteration 88/1000 | Loss: 0.00002089
Iteration 89/1000 | Loss: 0.00002089
Iteration 90/1000 | Loss: 0.00002089
Iteration 91/1000 | Loss: 0.00002088
Iteration 92/1000 | Loss: 0.00002088
Iteration 93/1000 | Loss: 0.00002088
Iteration 94/1000 | Loss: 0.00002087
Iteration 95/1000 | Loss: 0.00002087
Iteration 96/1000 | Loss: 0.00002087
Iteration 97/1000 | Loss: 0.00002087
Iteration 98/1000 | Loss: 0.00002087
Iteration 99/1000 | Loss: 0.00002087
Iteration 100/1000 | Loss: 0.00002086
Iteration 101/1000 | Loss: 0.00002086
Iteration 102/1000 | Loss: 0.00002086
Iteration 103/1000 | Loss: 0.00002086
Iteration 104/1000 | Loss: 0.00002086
Iteration 105/1000 | Loss: 0.00002086
Iteration 106/1000 | Loss: 0.00002086
Iteration 107/1000 | Loss: 0.00002085
Iteration 108/1000 | Loss: 0.00002085
Iteration 109/1000 | Loss: 0.00002085
Iteration 110/1000 | Loss: 0.00002085
Iteration 111/1000 | Loss: 0.00002085
Iteration 112/1000 | Loss: 0.00002085
Iteration 113/1000 | Loss: 0.00002085
Iteration 114/1000 | Loss: 0.00002085
Iteration 115/1000 | Loss: 0.00002085
Iteration 116/1000 | Loss: 0.00002085
Iteration 117/1000 | Loss: 0.00002085
Iteration 118/1000 | Loss: 0.00002085
Iteration 119/1000 | Loss: 0.00002085
Iteration 120/1000 | Loss: 0.00002085
Iteration 121/1000 | Loss: 0.00002084
Iteration 122/1000 | Loss: 0.00002084
Iteration 123/1000 | Loss: 0.00002084
Iteration 124/1000 | Loss: 0.00002084
Iteration 125/1000 | Loss: 0.00002084
Iteration 126/1000 | Loss: 0.00002084
Iteration 127/1000 | Loss: 0.00002084
Iteration 128/1000 | Loss: 0.00002084
Iteration 129/1000 | Loss: 0.00002084
Iteration 130/1000 | Loss: 0.00002084
Iteration 131/1000 | Loss: 0.00002083
Iteration 132/1000 | Loss: 0.00002083
Iteration 133/1000 | Loss: 0.00002083
Iteration 134/1000 | Loss: 0.00002083
Iteration 135/1000 | Loss: 0.00002083
Iteration 136/1000 | Loss: 0.00002083
Iteration 137/1000 | Loss: 0.00002083
Iteration 138/1000 | Loss: 0.00002083
Iteration 139/1000 | Loss: 0.00002083
Iteration 140/1000 | Loss: 0.00002083
Iteration 141/1000 | Loss: 0.00002083
Iteration 142/1000 | Loss: 0.00002083
Iteration 143/1000 | Loss: 0.00002083
Iteration 144/1000 | Loss: 0.00002083
Iteration 145/1000 | Loss: 0.00002083
Iteration 146/1000 | Loss: 0.00002083
Iteration 147/1000 | Loss: 0.00002083
Iteration 148/1000 | Loss: 0.00002082
Iteration 149/1000 | Loss: 0.00002082
Iteration 150/1000 | Loss: 0.00002082
Iteration 151/1000 | Loss: 0.00002082
Iteration 152/1000 | Loss: 0.00002082
Iteration 153/1000 | Loss: 0.00002082
Iteration 154/1000 | Loss: 0.00002082
Iteration 155/1000 | Loss: 0.00002082
Iteration 156/1000 | Loss: 0.00002082
Iteration 157/1000 | Loss: 0.00002082
Iteration 158/1000 | Loss: 0.00002082
Iteration 159/1000 | Loss: 0.00002082
Iteration 160/1000 | Loss: 0.00002082
Iteration 161/1000 | Loss: 0.00002082
Iteration 162/1000 | Loss: 0.00002082
Iteration 163/1000 | Loss: 0.00002082
Iteration 164/1000 | Loss: 0.00002082
Iteration 165/1000 | Loss: 0.00002082
Iteration 166/1000 | Loss: 0.00002082
Iteration 167/1000 | Loss: 0.00002082
Iteration 168/1000 | Loss: 0.00002082
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.0818742996198125e-05, 2.0818742996198125e-05, 2.0818742996198125e-05, 2.0818742996198125e-05, 2.0818742996198125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0818742996198125e-05

Optimization complete. Final v2v error: 3.8522567749023438 mm

Highest mean error: 4.430886268615723 mm for frame 96

Lowest mean error: 3.484015464782715 mm for frame 24

Saving results

Total time: 37.44016432762146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925275
Iteration 2/25 | Loss: 0.00128224
Iteration 3/25 | Loss: 0.00101274
Iteration 4/25 | Loss: 0.00096867
Iteration 5/25 | Loss: 0.00096020
Iteration 6/25 | Loss: 0.00095707
Iteration 7/25 | Loss: 0.00095643
Iteration 8/25 | Loss: 0.00095643
Iteration 9/25 | Loss: 0.00095643
Iteration 10/25 | Loss: 0.00095643
Iteration 11/25 | Loss: 0.00095643
Iteration 12/25 | Loss: 0.00095643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009564338251948357, 0.0009564338251948357, 0.0009564338251948357, 0.0009564338251948357, 0.0009564338251948357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009564338251948357

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50777733
Iteration 2/25 | Loss: 0.00076150
Iteration 3/25 | Loss: 0.00076150
Iteration 4/25 | Loss: 0.00076150
Iteration 5/25 | Loss: 0.00076150
Iteration 6/25 | Loss: 0.00076150
Iteration 7/25 | Loss: 0.00076150
Iteration 8/25 | Loss: 0.00076150
Iteration 9/25 | Loss: 0.00076150
Iteration 10/25 | Loss: 0.00076150
Iteration 11/25 | Loss: 0.00076150
Iteration 12/25 | Loss: 0.00076150
Iteration 13/25 | Loss: 0.00076150
Iteration 14/25 | Loss: 0.00076150
Iteration 15/25 | Loss: 0.00076150
Iteration 16/25 | Loss: 0.00076150
Iteration 17/25 | Loss: 0.00076150
Iteration 18/25 | Loss: 0.00076150
Iteration 19/25 | Loss: 0.00076150
Iteration 20/25 | Loss: 0.00076150
Iteration 21/25 | Loss: 0.00076150
Iteration 22/25 | Loss: 0.00076150
Iteration 23/25 | Loss: 0.00076150
Iteration 24/25 | Loss: 0.00076150
Iteration 25/25 | Loss: 0.00076150

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076150
Iteration 2/1000 | Loss: 0.00002664
Iteration 3/1000 | Loss: 0.00002185
Iteration 4/1000 | Loss: 0.00002043
Iteration 5/1000 | Loss: 0.00001951
Iteration 6/1000 | Loss: 0.00001870
Iteration 7/1000 | Loss: 0.00001823
Iteration 8/1000 | Loss: 0.00001780
Iteration 9/1000 | Loss: 0.00001755
Iteration 10/1000 | Loss: 0.00001741
Iteration 11/1000 | Loss: 0.00001739
Iteration 12/1000 | Loss: 0.00001736
Iteration 13/1000 | Loss: 0.00001736
Iteration 14/1000 | Loss: 0.00001735
Iteration 15/1000 | Loss: 0.00001735
Iteration 16/1000 | Loss: 0.00001735
Iteration 17/1000 | Loss: 0.00001733
Iteration 18/1000 | Loss: 0.00001733
Iteration 19/1000 | Loss: 0.00001732
Iteration 20/1000 | Loss: 0.00001732
Iteration 21/1000 | Loss: 0.00001732
Iteration 22/1000 | Loss: 0.00001730
Iteration 23/1000 | Loss: 0.00001729
Iteration 24/1000 | Loss: 0.00001729
Iteration 25/1000 | Loss: 0.00001728
Iteration 26/1000 | Loss: 0.00001727
Iteration 27/1000 | Loss: 0.00001727
Iteration 28/1000 | Loss: 0.00001726
Iteration 29/1000 | Loss: 0.00001725
Iteration 30/1000 | Loss: 0.00001725
Iteration 31/1000 | Loss: 0.00001724
Iteration 32/1000 | Loss: 0.00001724
Iteration 33/1000 | Loss: 0.00001723
Iteration 34/1000 | Loss: 0.00001723
Iteration 35/1000 | Loss: 0.00001723
Iteration 36/1000 | Loss: 0.00001722
Iteration 37/1000 | Loss: 0.00001722
Iteration 38/1000 | Loss: 0.00001721
Iteration 39/1000 | Loss: 0.00001721
Iteration 40/1000 | Loss: 0.00001720
Iteration 41/1000 | Loss: 0.00001720
Iteration 42/1000 | Loss: 0.00001720
Iteration 43/1000 | Loss: 0.00001720
Iteration 44/1000 | Loss: 0.00001720
Iteration 45/1000 | Loss: 0.00001720
Iteration 46/1000 | Loss: 0.00001719
Iteration 47/1000 | Loss: 0.00001719
Iteration 48/1000 | Loss: 0.00001719
Iteration 49/1000 | Loss: 0.00001719
Iteration 50/1000 | Loss: 0.00001719
Iteration 51/1000 | Loss: 0.00001719
Iteration 52/1000 | Loss: 0.00001719
Iteration 53/1000 | Loss: 0.00001719
Iteration 54/1000 | Loss: 0.00001719
Iteration 55/1000 | Loss: 0.00001718
Iteration 56/1000 | Loss: 0.00001717
Iteration 57/1000 | Loss: 0.00001717
Iteration 58/1000 | Loss: 0.00001717
Iteration 59/1000 | Loss: 0.00001716
Iteration 60/1000 | Loss: 0.00001716
Iteration 61/1000 | Loss: 0.00001716
Iteration 62/1000 | Loss: 0.00001715
Iteration 63/1000 | Loss: 0.00001715
Iteration 64/1000 | Loss: 0.00001714
Iteration 65/1000 | Loss: 0.00001714
Iteration 66/1000 | Loss: 0.00001714
Iteration 67/1000 | Loss: 0.00001713
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001712
Iteration 70/1000 | Loss: 0.00001712
Iteration 71/1000 | Loss: 0.00001712
Iteration 72/1000 | Loss: 0.00001712
Iteration 73/1000 | Loss: 0.00001712
Iteration 74/1000 | Loss: 0.00001712
Iteration 75/1000 | Loss: 0.00001712
Iteration 76/1000 | Loss: 0.00001712
Iteration 77/1000 | Loss: 0.00001712
Iteration 78/1000 | Loss: 0.00001712
Iteration 79/1000 | Loss: 0.00001711
Iteration 80/1000 | Loss: 0.00001711
Iteration 81/1000 | Loss: 0.00001710
Iteration 82/1000 | Loss: 0.00001710
Iteration 83/1000 | Loss: 0.00001710
Iteration 84/1000 | Loss: 0.00001710
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001709
Iteration 87/1000 | Loss: 0.00001709
Iteration 88/1000 | Loss: 0.00001709
Iteration 89/1000 | Loss: 0.00001709
Iteration 90/1000 | Loss: 0.00001709
Iteration 91/1000 | Loss: 0.00001709
Iteration 92/1000 | Loss: 0.00001709
Iteration 93/1000 | Loss: 0.00001709
Iteration 94/1000 | Loss: 0.00001709
Iteration 95/1000 | Loss: 0.00001709
Iteration 96/1000 | Loss: 0.00001709
Iteration 97/1000 | Loss: 0.00001709
Iteration 98/1000 | Loss: 0.00001709
Iteration 99/1000 | Loss: 0.00001709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.7089047105400823e-05, 1.7089047105400823e-05, 1.7089047105400823e-05, 1.7089047105400823e-05, 1.7089047105400823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7089047105400823e-05

Optimization complete. Final v2v error: 3.5511205196380615 mm

Highest mean error: 3.796302080154419 mm for frame 89

Lowest mean error: 3.268798589706421 mm for frame 188

Saving results

Total time: 35.41639709472656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1552/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1552/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01126848
Iteration 2/25 | Loss: 0.00220109
Iteration 3/25 | Loss: 0.00144622
Iteration 4/25 | Loss: 0.00129454
Iteration 5/25 | Loss: 0.00131779
Iteration 6/25 | Loss: 0.00134577
Iteration 7/25 | Loss: 0.00128427
Iteration 8/25 | Loss: 0.00119064
Iteration 9/25 | Loss: 0.00117443
Iteration 10/25 | Loss: 0.00115131
Iteration 11/25 | Loss: 0.00111053
Iteration 12/25 | Loss: 0.00107289
Iteration 13/25 | Loss: 0.00103895
Iteration 14/25 | Loss: 0.00101657
Iteration 15/25 | Loss: 0.00100252
Iteration 16/25 | Loss: 0.00099954
Iteration 17/25 | Loss: 0.00099597
Iteration 18/25 | Loss: 0.00098857
Iteration 19/25 | Loss: 0.00098527
Iteration 20/25 | Loss: 0.00098391
Iteration 21/25 | Loss: 0.00098491
Iteration 22/25 | Loss: 0.00098449
Iteration 23/25 | Loss: 0.00098193
Iteration 24/25 | Loss: 0.00098488
Iteration 25/25 | Loss: 0.00097970

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50980270
Iteration 2/25 | Loss: 0.00134150
Iteration 3/25 | Loss: 0.00118236
Iteration 4/25 | Loss: 0.00118236
Iteration 5/25 | Loss: 0.00118235
Iteration 6/25 | Loss: 0.00118235
Iteration 7/25 | Loss: 0.00118235
Iteration 8/25 | Loss: 0.00118235
Iteration 9/25 | Loss: 0.00118235
Iteration 10/25 | Loss: 0.00118235
Iteration 11/25 | Loss: 0.00118235
Iteration 12/25 | Loss: 0.00118235
Iteration 13/25 | Loss: 0.00118235
Iteration 14/25 | Loss: 0.00118235
Iteration 15/25 | Loss: 0.00118235
Iteration 16/25 | Loss: 0.00118235
Iteration 17/25 | Loss: 0.00118235
Iteration 18/25 | Loss: 0.00118235
Iteration 19/25 | Loss: 0.00118235
Iteration 20/25 | Loss: 0.00118235
Iteration 21/25 | Loss: 0.00118235
Iteration 22/25 | Loss: 0.00118235
Iteration 23/25 | Loss: 0.00118235
Iteration 24/25 | Loss: 0.00118235
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011823540553450584, 0.0011823540553450584, 0.0011823540553450584, 0.0011823540553450584, 0.0011823540553450584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011823540553450584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118235
Iteration 2/1000 | Loss: 0.00069647
Iteration 3/1000 | Loss: 0.00089415
Iteration 4/1000 | Loss: 0.00039763
Iteration 5/1000 | Loss: 0.00040406
Iteration 6/1000 | Loss: 0.00065724
Iteration 7/1000 | Loss: 0.00088444
Iteration 8/1000 | Loss: 0.00051945
Iteration 9/1000 | Loss: 0.00036696
Iteration 10/1000 | Loss: 0.00041194
Iteration 11/1000 | Loss: 0.00056334
Iteration 12/1000 | Loss: 0.00052935
Iteration 13/1000 | Loss: 0.00068925
Iteration 14/1000 | Loss: 0.00048692
Iteration 15/1000 | Loss: 0.00045494
Iteration 16/1000 | Loss: 0.00067040
Iteration 17/1000 | Loss: 0.00048507
Iteration 18/1000 | Loss: 0.00027318
Iteration 19/1000 | Loss: 0.00035076
Iteration 20/1000 | Loss: 0.00030845
Iteration 21/1000 | Loss: 0.00038869
Iteration 22/1000 | Loss: 0.00042282
Iteration 23/1000 | Loss: 0.00028705
Iteration 24/1000 | Loss: 0.00034864
Iteration 25/1000 | Loss: 0.00053591
Iteration 26/1000 | Loss: 0.00021741
Iteration 27/1000 | Loss: 0.00059733
Iteration 28/1000 | Loss: 0.00038307
Iteration 29/1000 | Loss: 0.00035107
Iteration 30/1000 | Loss: 0.00065123
Iteration 31/1000 | Loss: 0.00058919
Iteration 32/1000 | Loss: 0.00010511
Iteration 33/1000 | Loss: 0.00011103
Iteration 34/1000 | Loss: 0.00062755
Iteration 35/1000 | Loss: 0.00077742
Iteration 36/1000 | Loss: 0.00072481
Iteration 37/1000 | Loss: 0.00093068
Iteration 38/1000 | Loss: 0.00031936
Iteration 39/1000 | Loss: 0.00042042
Iteration 40/1000 | Loss: 0.00029084
Iteration 41/1000 | Loss: 0.00050266
Iteration 42/1000 | Loss: 0.00036620
Iteration 43/1000 | Loss: 0.00044485
Iteration 44/1000 | Loss: 0.00039781
Iteration 45/1000 | Loss: 0.00021871
Iteration 46/1000 | Loss: 0.00004686
Iteration 47/1000 | Loss: 0.00005270
Iteration 48/1000 | Loss: 0.00031903
Iteration 49/1000 | Loss: 0.00024280
Iteration 50/1000 | Loss: 0.00030138
Iteration 51/1000 | Loss: 0.00017215
Iteration 52/1000 | Loss: 0.00015913
Iteration 53/1000 | Loss: 0.00005985
Iteration 54/1000 | Loss: 0.00016960
Iteration 55/1000 | Loss: 0.00029466
Iteration 56/1000 | Loss: 0.00028391
Iteration 57/1000 | Loss: 0.00020087
Iteration 58/1000 | Loss: 0.00016522
Iteration 59/1000 | Loss: 0.00012473
Iteration 60/1000 | Loss: 0.00015419
Iteration 61/1000 | Loss: 0.00008712
Iteration 62/1000 | Loss: 0.00048833
Iteration 63/1000 | Loss: 0.00100115
Iteration 64/1000 | Loss: 0.00048323
Iteration 65/1000 | Loss: 0.00033813
Iteration 66/1000 | Loss: 0.00005967
Iteration 67/1000 | Loss: 0.00007342
Iteration 68/1000 | Loss: 0.00023544
Iteration 69/1000 | Loss: 0.00019596
Iteration 70/1000 | Loss: 0.00017423
Iteration 71/1000 | Loss: 0.00011105
Iteration 72/1000 | Loss: 0.00006140
Iteration 73/1000 | Loss: 0.00009650
Iteration 74/1000 | Loss: 0.00008989
Iteration 75/1000 | Loss: 0.00015737
Iteration 76/1000 | Loss: 0.00003731
Iteration 77/1000 | Loss: 0.00003107
Iteration 78/1000 | Loss: 0.00022466
Iteration 79/1000 | Loss: 0.00007334
Iteration 80/1000 | Loss: 0.00017621
Iteration 81/1000 | Loss: 0.00003260
Iteration 82/1000 | Loss: 0.00015525
Iteration 83/1000 | Loss: 0.00009447
Iteration 84/1000 | Loss: 0.00007664
Iteration 85/1000 | Loss: 0.00007478
Iteration 86/1000 | Loss: 0.00006809
Iteration 87/1000 | Loss: 0.00016435
Iteration 88/1000 | Loss: 0.00011228
Iteration 89/1000 | Loss: 0.00014386
Iteration 90/1000 | Loss: 0.00010474
Iteration 91/1000 | Loss: 0.00002689
Iteration 92/1000 | Loss: 0.00002334
Iteration 93/1000 | Loss: 0.00002207
Iteration 94/1000 | Loss: 0.00019086
Iteration 95/1000 | Loss: 0.00016591
Iteration 96/1000 | Loss: 0.00002550
Iteration 97/1000 | Loss: 0.00002205
Iteration 98/1000 | Loss: 0.00002033
Iteration 99/1000 | Loss: 0.00001979
Iteration 100/1000 | Loss: 0.00001931
Iteration 101/1000 | Loss: 0.00001881
Iteration 102/1000 | Loss: 0.00001847
Iteration 103/1000 | Loss: 0.00001845
Iteration 104/1000 | Loss: 0.00001831
Iteration 105/1000 | Loss: 0.00001817
Iteration 106/1000 | Loss: 0.00001815
Iteration 107/1000 | Loss: 0.00001807
Iteration 108/1000 | Loss: 0.00001806
Iteration 109/1000 | Loss: 0.00001804
Iteration 110/1000 | Loss: 0.00001804
Iteration 111/1000 | Loss: 0.00001803
Iteration 112/1000 | Loss: 0.00001803
Iteration 113/1000 | Loss: 0.00001800
Iteration 114/1000 | Loss: 0.00001798
Iteration 115/1000 | Loss: 0.00001798
Iteration 116/1000 | Loss: 0.00001796
Iteration 117/1000 | Loss: 0.00001793
Iteration 118/1000 | Loss: 0.00001790
Iteration 119/1000 | Loss: 0.00001789
Iteration 120/1000 | Loss: 0.00001786
Iteration 121/1000 | Loss: 0.00001786
Iteration 122/1000 | Loss: 0.00001786
Iteration 123/1000 | Loss: 0.00001786
Iteration 124/1000 | Loss: 0.00001785
Iteration 125/1000 | Loss: 0.00001785
Iteration 126/1000 | Loss: 0.00001785
Iteration 127/1000 | Loss: 0.00001785
Iteration 128/1000 | Loss: 0.00001785
Iteration 129/1000 | Loss: 0.00001785
Iteration 130/1000 | Loss: 0.00001784
Iteration 131/1000 | Loss: 0.00001783
Iteration 132/1000 | Loss: 0.00001783
Iteration 133/1000 | Loss: 0.00001783
Iteration 134/1000 | Loss: 0.00001783
Iteration 135/1000 | Loss: 0.00001783
Iteration 136/1000 | Loss: 0.00001783
Iteration 137/1000 | Loss: 0.00001783
Iteration 138/1000 | Loss: 0.00001783
Iteration 139/1000 | Loss: 0.00001783
Iteration 140/1000 | Loss: 0.00001783
Iteration 141/1000 | Loss: 0.00001783
Iteration 142/1000 | Loss: 0.00001783
Iteration 143/1000 | Loss: 0.00001783
Iteration 144/1000 | Loss: 0.00001783
Iteration 145/1000 | Loss: 0.00001782
Iteration 146/1000 | Loss: 0.00001782
Iteration 147/1000 | Loss: 0.00001782
Iteration 148/1000 | Loss: 0.00001782
Iteration 149/1000 | Loss: 0.00001782
Iteration 150/1000 | Loss: 0.00001782
Iteration 151/1000 | Loss: 0.00001782
Iteration 152/1000 | Loss: 0.00001782
Iteration 153/1000 | Loss: 0.00001782
Iteration 154/1000 | Loss: 0.00001782
Iteration 155/1000 | Loss: 0.00001781
Iteration 156/1000 | Loss: 0.00001781
Iteration 157/1000 | Loss: 0.00001781
Iteration 158/1000 | Loss: 0.00001781
Iteration 159/1000 | Loss: 0.00001781
Iteration 160/1000 | Loss: 0.00001781
Iteration 161/1000 | Loss: 0.00001781
Iteration 162/1000 | Loss: 0.00001781
Iteration 163/1000 | Loss: 0.00001781
Iteration 164/1000 | Loss: 0.00001781
Iteration 165/1000 | Loss: 0.00001780
Iteration 166/1000 | Loss: 0.00001780
Iteration 167/1000 | Loss: 0.00001779
Iteration 168/1000 | Loss: 0.00001779
Iteration 169/1000 | Loss: 0.00001779
Iteration 170/1000 | Loss: 0.00001779
Iteration 171/1000 | Loss: 0.00001779
Iteration 172/1000 | Loss: 0.00001779
Iteration 173/1000 | Loss: 0.00001779
Iteration 174/1000 | Loss: 0.00001779
Iteration 175/1000 | Loss: 0.00001779
Iteration 176/1000 | Loss: 0.00001779
Iteration 177/1000 | Loss: 0.00001779
Iteration 178/1000 | Loss: 0.00001778
Iteration 179/1000 | Loss: 0.00001778
Iteration 180/1000 | Loss: 0.00001778
Iteration 181/1000 | Loss: 0.00001778
Iteration 182/1000 | Loss: 0.00001778
Iteration 183/1000 | Loss: 0.00001778
Iteration 184/1000 | Loss: 0.00001778
Iteration 185/1000 | Loss: 0.00001778
Iteration 186/1000 | Loss: 0.00001778
Iteration 187/1000 | Loss: 0.00001778
Iteration 188/1000 | Loss: 0.00001778
Iteration 189/1000 | Loss: 0.00001778
Iteration 190/1000 | Loss: 0.00001777
Iteration 191/1000 | Loss: 0.00001777
Iteration 192/1000 | Loss: 0.00001777
Iteration 193/1000 | Loss: 0.00001777
Iteration 194/1000 | Loss: 0.00001776
Iteration 195/1000 | Loss: 0.00001776
Iteration 196/1000 | Loss: 0.00001776
Iteration 197/1000 | Loss: 0.00001776
Iteration 198/1000 | Loss: 0.00001776
Iteration 199/1000 | Loss: 0.00001776
Iteration 200/1000 | Loss: 0.00001776
Iteration 201/1000 | Loss: 0.00001776
Iteration 202/1000 | Loss: 0.00001776
Iteration 203/1000 | Loss: 0.00001776
Iteration 204/1000 | Loss: 0.00001776
Iteration 205/1000 | Loss: 0.00001775
Iteration 206/1000 | Loss: 0.00001775
Iteration 207/1000 | Loss: 0.00001775
Iteration 208/1000 | Loss: 0.00001775
Iteration 209/1000 | Loss: 0.00001775
Iteration 210/1000 | Loss: 0.00001775
Iteration 211/1000 | Loss: 0.00001775
Iteration 212/1000 | Loss: 0.00001775
Iteration 213/1000 | Loss: 0.00001775
Iteration 214/1000 | Loss: 0.00001775
Iteration 215/1000 | Loss: 0.00001775
Iteration 216/1000 | Loss: 0.00001775
Iteration 217/1000 | Loss: 0.00001775
Iteration 218/1000 | Loss: 0.00001775
Iteration 219/1000 | Loss: 0.00001775
Iteration 220/1000 | Loss: 0.00001775
Iteration 221/1000 | Loss: 0.00001775
Iteration 222/1000 | Loss: 0.00001775
Iteration 223/1000 | Loss: 0.00001775
Iteration 224/1000 | Loss: 0.00001775
Iteration 225/1000 | Loss: 0.00001775
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.775026430550497e-05, 1.775026430550497e-05, 1.775026430550497e-05, 1.775026430550497e-05, 1.775026430550497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.775026430550497e-05

Optimization complete. Final v2v error: 3.5420758724212646 mm

Highest mean error: 9.614418983459473 mm for frame 173

Lowest mean error: 3.1453983783721924 mm for frame 236

Saving results

Total time: 228.2500705718994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032545
Iteration 2/25 | Loss: 0.00393121
Iteration 3/25 | Loss: 0.00284597
Iteration 4/25 | Loss: 0.00242052
Iteration 5/25 | Loss: 0.00220414
Iteration 6/25 | Loss: 0.00209908
Iteration 7/25 | Loss: 0.00200544
Iteration 8/25 | Loss: 0.00197864
Iteration 9/25 | Loss: 0.00197351
Iteration 10/25 | Loss: 0.00196872
Iteration 11/25 | Loss: 0.00196377
Iteration 12/25 | Loss: 0.00196397
Iteration 13/25 | Loss: 0.00196373
Iteration 14/25 | Loss: 0.00196377
Iteration 15/25 | Loss: 0.00196343
Iteration 16/25 | Loss: 0.00196343
Iteration 17/25 | Loss: 0.00196343
Iteration 18/25 | Loss: 0.00196343
Iteration 19/25 | Loss: 0.00196343
Iteration 20/25 | Loss: 0.00196343
Iteration 21/25 | Loss: 0.00196343
Iteration 22/25 | Loss: 0.00196343
Iteration 23/25 | Loss: 0.00196343
Iteration 24/25 | Loss: 0.00196343
Iteration 25/25 | Loss: 0.00196343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19720459
Iteration 2/25 | Loss: 0.00588790
Iteration 3/25 | Loss: 0.00581599
Iteration 4/25 | Loss: 0.00581599
Iteration 5/25 | Loss: 0.00581599
Iteration 6/25 | Loss: 0.00581599
Iteration 7/25 | Loss: 0.00581599
Iteration 8/25 | Loss: 0.00581599
Iteration 9/25 | Loss: 0.00581599
Iteration 10/25 | Loss: 0.00581599
Iteration 11/25 | Loss: 0.00581599
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.005815986078232527, 0.005815986078232527, 0.005815986078232527, 0.005815986078232527, 0.005815986078232527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005815986078232527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00581599
Iteration 2/1000 | Loss: 0.00132208
Iteration 3/1000 | Loss: 0.00088358
Iteration 4/1000 | Loss: 0.00544643
Iteration 5/1000 | Loss: 0.00080193
Iteration 6/1000 | Loss: 0.00113704
Iteration 7/1000 | Loss: 0.00087845
Iteration 8/1000 | Loss: 0.00052763
Iteration 9/1000 | Loss: 0.00065828
Iteration 10/1000 | Loss: 0.00160614
Iteration 11/1000 | Loss: 0.00104699
Iteration 12/1000 | Loss: 0.00066138
Iteration 13/1000 | Loss: 0.00041511
Iteration 14/1000 | Loss: 0.00043116
Iteration 15/1000 | Loss: 0.00040456
Iteration 16/1000 | Loss: 0.00078429
Iteration 17/1000 | Loss: 0.03580862
Iteration 18/1000 | Loss: 0.02694039
Iteration 19/1000 | Loss: 0.01031844
Iteration 20/1000 | Loss: 0.01075296
Iteration 21/1000 | Loss: 0.00600301
Iteration 22/1000 | Loss: 0.00247132
Iteration 23/1000 | Loss: 0.00226239
Iteration 24/1000 | Loss: 0.00249365
Iteration 25/1000 | Loss: 0.00160063
Iteration 26/1000 | Loss: 0.00154389
Iteration 27/1000 | Loss: 0.00192046
Iteration 28/1000 | Loss: 0.00158817
Iteration 29/1000 | Loss: 0.00219948
Iteration 30/1000 | Loss: 0.00086376
Iteration 31/1000 | Loss: 0.00141833
Iteration 32/1000 | Loss: 0.00124534
Iteration 33/1000 | Loss: 0.00097076
Iteration 34/1000 | Loss: 0.00148128
Iteration 35/1000 | Loss: 0.00070022
Iteration 36/1000 | Loss: 0.00078974
Iteration 37/1000 | Loss: 0.00205994
Iteration 38/1000 | Loss: 0.00131128
Iteration 39/1000 | Loss: 0.00184033
Iteration 40/1000 | Loss: 0.00107261
Iteration 41/1000 | Loss: 0.00144806
Iteration 42/1000 | Loss: 0.00148674
Iteration 43/1000 | Loss: 0.00188795
Iteration 44/1000 | Loss: 0.00149901
Iteration 45/1000 | Loss: 0.00400601
Iteration 46/1000 | Loss: 0.00329231
Iteration 47/1000 | Loss: 0.00191574
Iteration 48/1000 | Loss: 0.00198361
Iteration 49/1000 | Loss: 0.00278084
Iteration 50/1000 | Loss: 0.00257025
Iteration 51/1000 | Loss: 0.00214707
Iteration 52/1000 | Loss: 0.00209022
Iteration 53/1000 | Loss: 0.00173893
Iteration 54/1000 | Loss: 0.00189082
Iteration 55/1000 | Loss: 0.00217467
Iteration 56/1000 | Loss: 0.00517966
Iteration 57/1000 | Loss: 0.00188876
Iteration 58/1000 | Loss: 0.00214511
Iteration 59/1000 | Loss: 0.00190001
Iteration 60/1000 | Loss: 0.00338992
Iteration 61/1000 | Loss: 0.00288002
Iteration 62/1000 | Loss: 0.00192649
Iteration 63/1000 | Loss: 0.00117561
Iteration 64/1000 | Loss: 0.00180854
Iteration 65/1000 | Loss: 0.00119368
Iteration 66/1000 | Loss: 0.00164380
Iteration 67/1000 | Loss: 0.00156228
Iteration 68/1000 | Loss: 0.00158687
Iteration 69/1000 | Loss: 0.00149069
Iteration 70/1000 | Loss: 0.00133671
Iteration 71/1000 | Loss: 0.00128185
Iteration 72/1000 | Loss: 0.00167640
Iteration 73/1000 | Loss: 0.00115064
Iteration 74/1000 | Loss: 0.00267778
Iteration 75/1000 | Loss: 0.00146231
Iteration 76/1000 | Loss: 0.00149854
Iteration 77/1000 | Loss: 0.00205194
Iteration 78/1000 | Loss: 0.00053805
Iteration 79/1000 | Loss: 0.00034570
Iteration 80/1000 | Loss: 0.00054147
Iteration 81/1000 | Loss: 0.00015886
Iteration 82/1000 | Loss: 0.00041413
Iteration 83/1000 | Loss: 0.00012785
Iteration 84/1000 | Loss: 0.00022767
Iteration 85/1000 | Loss: 0.00026135
Iteration 86/1000 | Loss: 0.00046378
Iteration 87/1000 | Loss: 0.00030374
Iteration 88/1000 | Loss: 0.00033709
Iteration 89/1000 | Loss: 0.00036615
Iteration 90/1000 | Loss: 0.00008939
Iteration 91/1000 | Loss: 0.00050962
Iteration 92/1000 | Loss: 0.00070205
Iteration 93/1000 | Loss: 0.00065246
Iteration 94/1000 | Loss: 0.00020409
Iteration 95/1000 | Loss: 0.00007928
Iteration 96/1000 | Loss: 0.00061471
Iteration 97/1000 | Loss: 0.00068857
Iteration 98/1000 | Loss: 0.00010648
Iteration 99/1000 | Loss: 0.00041187
Iteration 100/1000 | Loss: 0.00007040
Iteration 101/1000 | Loss: 0.00006377
Iteration 102/1000 | Loss: 0.00006400
Iteration 103/1000 | Loss: 0.00018568
Iteration 104/1000 | Loss: 0.00009354
Iteration 105/1000 | Loss: 0.00006944
Iteration 106/1000 | Loss: 0.00005812
Iteration 107/1000 | Loss: 0.00053882
Iteration 108/1000 | Loss: 0.00063071
Iteration 109/1000 | Loss: 0.00063644
Iteration 110/1000 | Loss: 0.00059286
Iteration 111/1000 | Loss: 0.00048519
Iteration 112/1000 | Loss: 0.00040980
Iteration 113/1000 | Loss: 0.00039118
Iteration 114/1000 | Loss: 0.00004018
Iteration 115/1000 | Loss: 0.00005204
Iteration 116/1000 | Loss: 0.00014931
Iteration 117/1000 | Loss: 0.00011776
Iteration 118/1000 | Loss: 0.00012933
Iteration 119/1000 | Loss: 0.00010486
Iteration 120/1000 | Loss: 0.00017342
Iteration 121/1000 | Loss: 0.00032685
Iteration 122/1000 | Loss: 0.00060518
Iteration 123/1000 | Loss: 0.00028003
Iteration 124/1000 | Loss: 0.00031474
Iteration 125/1000 | Loss: 0.00022700
Iteration 126/1000 | Loss: 0.00023767
Iteration 127/1000 | Loss: 0.00020374
Iteration 128/1000 | Loss: 0.00035304
Iteration 129/1000 | Loss: 0.00022845
Iteration 130/1000 | Loss: 0.00040918
Iteration 131/1000 | Loss: 0.00004722
Iteration 132/1000 | Loss: 0.00004926
Iteration 133/1000 | Loss: 0.00012037
Iteration 134/1000 | Loss: 0.00013799
Iteration 135/1000 | Loss: 0.00004589
Iteration 136/1000 | Loss: 0.00029241
Iteration 137/1000 | Loss: 0.00006826
Iteration 138/1000 | Loss: 0.00072327
Iteration 139/1000 | Loss: 0.00042996
Iteration 140/1000 | Loss: 0.00007519
Iteration 141/1000 | Loss: 0.00073031
Iteration 142/1000 | Loss: 0.00020745
Iteration 143/1000 | Loss: 0.00031314
Iteration 144/1000 | Loss: 0.00025444
Iteration 145/1000 | Loss: 0.00030102
Iteration 146/1000 | Loss: 0.00020674
Iteration 147/1000 | Loss: 0.00020746
Iteration 148/1000 | Loss: 0.00025954
Iteration 149/1000 | Loss: 0.00019290
Iteration 150/1000 | Loss: 0.00024697
Iteration 151/1000 | Loss: 0.00006675
Iteration 152/1000 | Loss: 0.00004014
Iteration 153/1000 | Loss: 0.00003646
Iteration 154/1000 | Loss: 0.00005439
Iteration 155/1000 | Loss: 0.00003282
Iteration 156/1000 | Loss: 0.00002929
Iteration 157/1000 | Loss: 0.00060912
Iteration 158/1000 | Loss: 0.00029432
Iteration 159/1000 | Loss: 0.00010096
Iteration 160/1000 | Loss: 0.00003327
Iteration 161/1000 | Loss: 0.00004273
Iteration 162/1000 | Loss: 0.00060327
Iteration 163/1000 | Loss: 0.00040780
Iteration 164/1000 | Loss: 0.00013623
Iteration 165/1000 | Loss: 0.00004216
Iteration 166/1000 | Loss: 0.00014067
Iteration 167/1000 | Loss: 0.00018973
Iteration 168/1000 | Loss: 0.00011712
Iteration 169/1000 | Loss: 0.00003282
Iteration 170/1000 | Loss: 0.00002357
Iteration 171/1000 | Loss: 0.00002530
Iteration 172/1000 | Loss: 0.00002325
Iteration 173/1000 | Loss: 0.00001887
Iteration 174/1000 | Loss: 0.00002181
Iteration 175/1000 | Loss: 0.00001715
Iteration 176/1000 | Loss: 0.00001684
Iteration 177/1000 | Loss: 0.00002083
Iteration 178/1000 | Loss: 0.00003782
Iteration 179/1000 | Loss: 0.00001796
Iteration 180/1000 | Loss: 0.00001726
Iteration 181/1000 | Loss: 0.00001636
Iteration 182/1000 | Loss: 0.00001635
Iteration 183/1000 | Loss: 0.00001635
Iteration 184/1000 | Loss: 0.00001633
Iteration 185/1000 | Loss: 0.00001666
Iteration 186/1000 | Loss: 0.00001673
Iteration 187/1000 | Loss: 0.00001627
Iteration 188/1000 | Loss: 0.00001627
Iteration 189/1000 | Loss: 0.00001625
Iteration 190/1000 | Loss: 0.00001625
Iteration 191/1000 | Loss: 0.00001624
Iteration 192/1000 | Loss: 0.00001624
Iteration 193/1000 | Loss: 0.00001624
Iteration 194/1000 | Loss: 0.00001623
Iteration 195/1000 | Loss: 0.00001623
Iteration 196/1000 | Loss: 0.00001894
Iteration 197/1000 | Loss: 0.00001621
Iteration 198/1000 | Loss: 0.00001621
Iteration 199/1000 | Loss: 0.00001621
Iteration 200/1000 | Loss: 0.00001620
Iteration 201/1000 | Loss: 0.00001620
Iteration 202/1000 | Loss: 0.00001620
Iteration 203/1000 | Loss: 0.00001618
Iteration 204/1000 | Loss: 0.00001618
Iteration 205/1000 | Loss: 0.00031416
Iteration 206/1000 | Loss: 0.00005807
Iteration 207/1000 | Loss: 0.00002667
Iteration 208/1000 | Loss: 0.00003390
Iteration 209/1000 | Loss: 0.00002005
Iteration 210/1000 | Loss: 0.00002937
Iteration 211/1000 | Loss: 0.00002300
Iteration 212/1000 | Loss: 0.00002549
Iteration 213/1000 | Loss: 0.00002144
Iteration 214/1000 | Loss: 0.00001432
Iteration 215/1000 | Loss: 0.00001666
Iteration 216/1000 | Loss: 0.00001590
Iteration 217/1000 | Loss: 0.00001718
Iteration 218/1000 | Loss: 0.00001366
Iteration 219/1000 | Loss: 0.00001544
Iteration 220/1000 | Loss: 0.00001363
Iteration 221/1000 | Loss: 0.00001362
Iteration 222/1000 | Loss: 0.00001362
Iteration 223/1000 | Loss: 0.00001362
Iteration 224/1000 | Loss: 0.00001362
Iteration 225/1000 | Loss: 0.00001361
Iteration 226/1000 | Loss: 0.00001361
Iteration 227/1000 | Loss: 0.00001359
Iteration 228/1000 | Loss: 0.00001359
Iteration 229/1000 | Loss: 0.00001359
Iteration 230/1000 | Loss: 0.00001359
Iteration 231/1000 | Loss: 0.00001359
Iteration 232/1000 | Loss: 0.00001359
Iteration 233/1000 | Loss: 0.00001359
Iteration 234/1000 | Loss: 0.00001359
Iteration 235/1000 | Loss: 0.00001359
Iteration 236/1000 | Loss: 0.00001359
Iteration 237/1000 | Loss: 0.00001358
Iteration 238/1000 | Loss: 0.00001358
Iteration 239/1000 | Loss: 0.00001358
Iteration 240/1000 | Loss: 0.00001402
Iteration 241/1000 | Loss: 0.00001353
Iteration 242/1000 | Loss: 0.00001353
Iteration 243/1000 | Loss: 0.00001353
Iteration 244/1000 | Loss: 0.00001352
Iteration 245/1000 | Loss: 0.00001352
Iteration 246/1000 | Loss: 0.00001352
Iteration 247/1000 | Loss: 0.00001352
Iteration 248/1000 | Loss: 0.00001352
Iteration 249/1000 | Loss: 0.00001455
Iteration 250/1000 | Loss: 0.00001454
Iteration 251/1000 | Loss: 0.00001615
Iteration 252/1000 | Loss: 0.00001349
Iteration 253/1000 | Loss: 0.00001547
Iteration 254/1000 | Loss: 0.00001342
Iteration 255/1000 | Loss: 0.00001342
Iteration 256/1000 | Loss: 0.00001342
Iteration 257/1000 | Loss: 0.00001342
Iteration 258/1000 | Loss: 0.00001342
Iteration 259/1000 | Loss: 0.00001342
Iteration 260/1000 | Loss: 0.00001342
Iteration 261/1000 | Loss: 0.00001342
Iteration 262/1000 | Loss: 0.00001342
Iteration 263/1000 | Loss: 0.00001342
Iteration 264/1000 | Loss: 0.00001342
Iteration 265/1000 | Loss: 0.00001342
Iteration 266/1000 | Loss: 0.00001342
Iteration 267/1000 | Loss: 0.00001342
Iteration 268/1000 | Loss: 0.00001341
Iteration 269/1000 | Loss: 0.00001341
Iteration 270/1000 | Loss: 0.00001341
Iteration 271/1000 | Loss: 0.00001341
Iteration 272/1000 | Loss: 0.00001341
Iteration 273/1000 | Loss: 0.00001341
Iteration 274/1000 | Loss: 0.00001341
Iteration 275/1000 | Loss: 0.00001341
Iteration 276/1000 | Loss: 0.00001341
Iteration 277/1000 | Loss: 0.00001341
Iteration 278/1000 | Loss: 0.00001341
Iteration 279/1000 | Loss: 0.00001341
Iteration 280/1000 | Loss: 0.00001340
Iteration 281/1000 | Loss: 0.00001340
Iteration 282/1000 | Loss: 0.00001340
Iteration 283/1000 | Loss: 0.00001340
Iteration 284/1000 | Loss: 0.00001340
Iteration 285/1000 | Loss: 0.00001340
Iteration 286/1000 | Loss: 0.00001340
Iteration 287/1000 | Loss: 0.00001340
Iteration 288/1000 | Loss: 0.00001340
Iteration 289/1000 | Loss: 0.00001340
Iteration 290/1000 | Loss: 0.00001340
Iteration 291/1000 | Loss: 0.00001340
Iteration 292/1000 | Loss: 0.00001340
Iteration 293/1000 | Loss: 0.00001340
Iteration 294/1000 | Loss: 0.00001340
Iteration 295/1000 | Loss: 0.00001340
Iteration 296/1000 | Loss: 0.00001340
Iteration 297/1000 | Loss: 0.00001340
Iteration 298/1000 | Loss: 0.00001340
Iteration 299/1000 | Loss: 0.00001340
Iteration 300/1000 | Loss: 0.00001340
Iteration 301/1000 | Loss: 0.00001340
Iteration 302/1000 | Loss: 0.00001340
Iteration 303/1000 | Loss: 0.00001340
Iteration 304/1000 | Loss: 0.00001340
Iteration 305/1000 | Loss: 0.00001340
Iteration 306/1000 | Loss: 0.00001340
Iteration 307/1000 | Loss: 0.00001340
Iteration 308/1000 | Loss: 0.00001340
Iteration 309/1000 | Loss: 0.00001340
Iteration 310/1000 | Loss: 0.00001340
Iteration 311/1000 | Loss: 0.00001340
Iteration 312/1000 | Loss: 0.00001340
Iteration 313/1000 | Loss: 0.00001340
Iteration 314/1000 | Loss: 0.00001340
Iteration 315/1000 | Loss: 0.00001340
Iteration 316/1000 | Loss: 0.00001340
Iteration 317/1000 | Loss: 0.00001340
Iteration 318/1000 | Loss: 0.00001340
Iteration 319/1000 | Loss: 0.00001340
Iteration 320/1000 | Loss: 0.00001340
Iteration 321/1000 | Loss: 0.00001340
Iteration 322/1000 | Loss: 0.00001340
Iteration 323/1000 | Loss: 0.00001340
Iteration 324/1000 | Loss: 0.00001340
Iteration 325/1000 | Loss: 0.00001340
Iteration 326/1000 | Loss: 0.00001340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 326. Stopping optimization.
Last 5 losses: [1.3404980563791469e-05, 1.3404980563791469e-05, 1.3404980563791469e-05, 1.3404980563791469e-05, 1.3404980563791469e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3404980563791469e-05

Optimization complete. Final v2v error: 3.191694498062134 mm

Highest mean error: 3.756197214126587 mm for frame 134

Lowest mean error: 2.964566230773926 mm for frame 15

Saving results

Total time: 305.5967378616333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447365
Iteration 2/25 | Loss: 0.00128259
Iteration 3/25 | Loss: 0.00118972
Iteration 4/25 | Loss: 0.00117469
Iteration 5/25 | Loss: 0.00116981
Iteration 6/25 | Loss: 0.00116882
Iteration 7/25 | Loss: 0.00116880
Iteration 8/25 | Loss: 0.00116880
Iteration 9/25 | Loss: 0.00116880
Iteration 10/25 | Loss: 0.00116880
Iteration 11/25 | Loss: 0.00116880
Iteration 12/25 | Loss: 0.00116880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011687991209328175, 0.0011687991209328175, 0.0011687991209328175, 0.0011687991209328175, 0.0011687991209328175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011687991209328175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83936012
Iteration 2/25 | Loss: 0.00105085
Iteration 3/25 | Loss: 0.00105084
Iteration 4/25 | Loss: 0.00105084
Iteration 5/25 | Loss: 0.00105084
Iteration 6/25 | Loss: 0.00105084
Iteration 7/25 | Loss: 0.00105084
Iteration 8/25 | Loss: 0.00105084
Iteration 9/25 | Loss: 0.00105084
Iteration 10/25 | Loss: 0.00105084
Iteration 11/25 | Loss: 0.00105084
Iteration 12/25 | Loss: 0.00105084
Iteration 13/25 | Loss: 0.00105084
Iteration 14/25 | Loss: 0.00105084
Iteration 15/25 | Loss: 0.00105084
Iteration 16/25 | Loss: 0.00105084
Iteration 17/25 | Loss: 0.00105084
Iteration 18/25 | Loss: 0.00105084
Iteration 19/25 | Loss: 0.00105084
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010508401319384575, 0.0010508401319384575, 0.0010508401319384575, 0.0010508401319384575, 0.0010508401319384575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010508401319384575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105084
Iteration 2/1000 | Loss: 0.00003618
Iteration 3/1000 | Loss: 0.00002631
Iteration 4/1000 | Loss: 0.00002367
Iteration 5/1000 | Loss: 0.00002237
Iteration 6/1000 | Loss: 0.00002149
Iteration 7/1000 | Loss: 0.00002094
Iteration 8/1000 | Loss: 0.00002052
Iteration 9/1000 | Loss: 0.00002022
Iteration 10/1000 | Loss: 0.00001999
Iteration 11/1000 | Loss: 0.00001997
Iteration 12/1000 | Loss: 0.00001996
Iteration 13/1000 | Loss: 0.00001994
Iteration 14/1000 | Loss: 0.00001993
Iteration 15/1000 | Loss: 0.00001992
Iteration 16/1000 | Loss: 0.00001989
Iteration 17/1000 | Loss: 0.00001989
Iteration 18/1000 | Loss: 0.00001982
Iteration 19/1000 | Loss: 0.00001981
Iteration 20/1000 | Loss: 0.00001979
Iteration 21/1000 | Loss: 0.00001975
Iteration 22/1000 | Loss: 0.00001974
Iteration 23/1000 | Loss: 0.00001974
Iteration 24/1000 | Loss: 0.00001974
Iteration 25/1000 | Loss: 0.00001973
Iteration 26/1000 | Loss: 0.00001973
Iteration 27/1000 | Loss: 0.00001972
Iteration 28/1000 | Loss: 0.00001972
Iteration 29/1000 | Loss: 0.00001971
Iteration 30/1000 | Loss: 0.00001971
Iteration 31/1000 | Loss: 0.00001971
Iteration 32/1000 | Loss: 0.00001970
Iteration 33/1000 | Loss: 0.00001970
Iteration 34/1000 | Loss: 0.00001969
Iteration 35/1000 | Loss: 0.00001969
Iteration 36/1000 | Loss: 0.00001969
Iteration 37/1000 | Loss: 0.00001968
Iteration 38/1000 | Loss: 0.00001968
Iteration 39/1000 | Loss: 0.00001968
Iteration 40/1000 | Loss: 0.00001968
Iteration 41/1000 | Loss: 0.00001967
Iteration 42/1000 | Loss: 0.00001967
Iteration 43/1000 | Loss: 0.00001967
Iteration 44/1000 | Loss: 0.00001967
Iteration 45/1000 | Loss: 0.00001967
Iteration 46/1000 | Loss: 0.00001967
Iteration 47/1000 | Loss: 0.00001967
Iteration 48/1000 | Loss: 0.00001966
Iteration 49/1000 | Loss: 0.00001966
Iteration 50/1000 | Loss: 0.00001966
Iteration 51/1000 | Loss: 0.00001966
Iteration 52/1000 | Loss: 0.00001966
Iteration 53/1000 | Loss: 0.00001966
Iteration 54/1000 | Loss: 0.00001965
Iteration 55/1000 | Loss: 0.00001965
Iteration 56/1000 | Loss: 0.00001965
Iteration 57/1000 | Loss: 0.00001965
Iteration 58/1000 | Loss: 0.00001965
Iteration 59/1000 | Loss: 0.00001964
Iteration 60/1000 | Loss: 0.00001964
Iteration 61/1000 | Loss: 0.00001964
Iteration 62/1000 | Loss: 0.00001963
Iteration 63/1000 | Loss: 0.00001963
Iteration 64/1000 | Loss: 0.00001963
Iteration 65/1000 | Loss: 0.00001963
Iteration 66/1000 | Loss: 0.00001963
Iteration 67/1000 | Loss: 0.00001963
Iteration 68/1000 | Loss: 0.00001963
Iteration 69/1000 | Loss: 0.00001963
Iteration 70/1000 | Loss: 0.00001963
Iteration 71/1000 | Loss: 0.00001963
Iteration 72/1000 | Loss: 0.00001963
Iteration 73/1000 | Loss: 0.00001962
Iteration 74/1000 | Loss: 0.00001962
Iteration 75/1000 | Loss: 0.00001962
Iteration 76/1000 | Loss: 0.00001962
Iteration 77/1000 | Loss: 0.00001962
Iteration 78/1000 | Loss: 0.00001962
Iteration 79/1000 | Loss: 0.00001962
Iteration 80/1000 | Loss: 0.00001962
Iteration 81/1000 | Loss: 0.00001962
Iteration 82/1000 | Loss: 0.00001962
Iteration 83/1000 | Loss: 0.00001962
Iteration 84/1000 | Loss: 0.00001962
Iteration 85/1000 | Loss: 0.00001961
Iteration 86/1000 | Loss: 0.00001961
Iteration 87/1000 | Loss: 0.00001961
Iteration 88/1000 | Loss: 0.00001961
Iteration 89/1000 | Loss: 0.00001961
Iteration 90/1000 | Loss: 0.00001961
Iteration 91/1000 | Loss: 0.00001961
Iteration 92/1000 | Loss: 0.00001961
Iteration 93/1000 | Loss: 0.00001961
Iteration 94/1000 | Loss: 0.00001961
Iteration 95/1000 | Loss: 0.00001961
Iteration 96/1000 | Loss: 0.00001961
Iteration 97/1000 | Loss: 0.00001961
Iteration 98/1000 | Loss: 0.00001961
Iteration 99/1000 | Loss: 0.00001960
Iteration 100/1000 | Loss: 0.00001960
Iteration 101/1000 | Loss: 0.00001960
Iteration 102/1000 | Loss: 0.00001960
Iteration 103/1000 | Loss: 0.00001960
Iteration 104/1000 | Loss: 0.00001960
Iteration 105/1000 | Loss: 0.00001960
Iteration 106/1000 | Loss: 0.00001960
Iteration 107/1000 | Loss: 0.00001960
Iteration 108/1000 | Loss: 0.00001960
Iteration 109/1000 | Loss: 0.00001960
Iteration 110/1000 | Loss: 0.00001960
Iteration 111/1000 | Loss: 0.00001960
Iteration 112/1000 | Loss: 0.00001960
Iteration 113/1000 | Loss: 0.00001960
Iteration 114/1000 | Loss: 0.00001959
Iteration 115/1000 | Loss: 0.00001959
Iteration 116/1000 | Loss: 0.00001959
Iteration 117/1000 | Loss: 0.00001959
Iteration 118/1000 | Loss: 0.00001959
Iteration 119/1000 | Loss: 0.00001959
Iteration 120/1000 | Loss: 0.00001959
Iteration 121/1000 | Loss: 0.00001959
Iteration 122/1000 | Loss: 0.00001959
Iteration 123/1000 | Loss: 0.00001959
Iteration 124/1000 | Loss: 0.00001959
Iteration 125/1000 | Loss: 0.00001959
Iteration 126/1000 | Loss: 0.00001959
Iteration 127/1000 | Loss: 0.00001959
Iteration 128/1000 | Loss: 0.00001959
Iteration 129/1000 | Loss: 0.00001959
Iteration 130/1000 | Loss: 0.00001959
Iteration 131/1000 | Loss: 0.00001959
Iteration 132/1000 | Loss: 0.00001959
Iteration 133/1000 | Loss: 0.00001959
Iteration 134/1000 | Loss: 0.00001959
Iteration 135/1000 | Loss: 0.00001959
Iteration 136/1000 | Loss: 0.00001959
Iteration 137/1000 | Loss: 0.00001959
Iteration 138/1000 | Loss: 0.00001959
Iteration 139/1000 | Loss: 0.00001959
Iteration 140/1000 | Loss: 0.00001959
Iteration 141/1000 | Loss: 0.00001959
Iteration 142/1000 | Loss: 0.00001959
Iteration 143/1000 | Loss: 0.00001959
Iteration 144/1000 | Loss: 0.00001959
Iteration 145/1000 | Loss: 0.00001959
Iteration 146/1000 | Loss: 0.00001959
Iteration 147/1000 | Loss: 0.00001959
Iteration 148/1000 | Loss: 0.00001959
Iteration 149/1000 | Loss: 0.00001959
Iteration 150/1000 | Loss: 0.00001959
Iteration 151/1000 | Loss: 0.00001959
Iteration 152/1000 | Loss: 0.00001959
Iteration 153/1000 | Loss: 0.00001959
Iteration 154/1000 | Loss: 0.00001959
Iteration 155/1000 | Loss: 0.00001959
Iteration 156/1000 | Loss: 0.00001959
Iteration 157/1000 | Loss: 0.00001959
Iteration 158/1000 | Loss: 0.00001959
Iteration 159/1000 | Loss: 0.00001959
Iteration 160/1000 | Loss: 0.00001959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.959357177838683e-05, 1.959357177838683e-05, 1.959357177838683e-05, 1.959357177838683e-05, 1.959357177838683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.959357177838683e-05

Optimization complete. Final v2v error: 3.7421398162841797 mm

Highest mean error: 4.133447647094727 mm for frame 105

Lowest mean error: 3.3517134189605713 mm for frame 35

Saving results

Total time: 32.59326148033142
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483725
Iteration 2/25 | Loss: 0.00141534
Iteration 3/25 | Loss: 0.00121960
Iteration 4/25 | Loss: 0.00120077
Iteration 5/25 | Loss: 0.00119901
Iteration 6/25 | Loss: 0.00119901
Iteration 7/25 | Loss: 0.00119901
Iteration 8/25 | Loss: 0.00119901
Iteration 9/25 | Loss: 0.00119901
Iteration 10/25 | Loss: 0.00119901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001199006219394505, 0.001199006219394505, 0.001199006219394505, 0.001199006219394505, 0.001199006219394505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001199006219394505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25933421
Iteration 2/25 | Loss: 0.00102435
Iteration 3/25 | Loss: 0.00102434
Iteration 4/25 | Loss: 0.00102434
Iteration 5/25 | Loss: 0.00102434
Iteration 6/25 | Loss: 0.00102434
Iteration 7/25 | Loss: 0.00102434
Iteration 8/25 | Loss: 0.00102434
Iteration 9/25 | Loss: 0.00102434
Iteration 10/25 | Loss: 0.00102434
Iteration 11/25 | Loss: 0.00102434
Iteration 12/25 | Loss: 0.00102434
Iteration 13/25 | Loss: 0.00102434
Iteration 14/25 | Loss: 0.00102434
Iteration 15/25 | Loss: 0.00102434
Iteration 16/25 | Loss: 0.00102434
Iteration 17/25 | Loss: 0.00102434
Iteration 18/25 | Loss: 0.00102434
Iteration 19/25 | Loss: 0.00102434
Iteration 20/25 | Loss: 0.00102434
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010243385331705213, 0.0010243385331705213, 0.0010243385331705213, 0.0010243385331705213, 0.0010243385331705213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010243385331705213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102434
Iteration 2/1000 | Loss: 0.00004478
Iteration 3/1000 | Loss: 0.00002465
Iteration 4/1000 | Loss: 0.00002152
Iteration 5/1000 | Loss: 0.00002014
Iteration 6/1000 | Loss: 0.00001942
Iteration 7/1000 | Loss: 0.00001907
Iteration 8/1000 | Loss: 0.00001881
Iteration 9/1000 | Loss: 0.00001852
Iteration 10/1000 | Loss: 0.00001832
Iteration 11/1000 | Loss: 0.00001822
Iteration 12/1000 | Loss: 0.00001807
Iteration 13/1000 | Loss: 0.00001807
Iteration 14/1000 | Loss: 0.00001803
Iteration 15/1000 | Loss: 0.00001803
Iteration 16/1000 | Loss: 0.00001803
Iteration 17/1000 | Loss: 0.00001802
Iteration 18/1000 | Loss: 0.00001802
Iteration 19/1000 | Loss: 0.00001801
Iteration 20/1000 | Loss: 0.00001801
Iteration 21/1000 | Loss: 0.00001800
Iteration 22/1000 | Loss: 0.00001800
Iteration 23/1000 | Loss: 0.00001800
Iteration 24/1000 | Loss: 0.00001799
Iteration 25/1000 | Loss: 0.00001798
Iteration 26/1000 | Loss: 0.00001798
Iteration 27/1000 | Loss: 0.00001797
Iteration 28/1000 | Loss: 0.00001796
Iteration 29/1000 | Loss: 0.00001796
Iteration 30/1000 | Loss: 0.00001794
Iteration 31/1000 | Loss: 0.00001794
Iteration 32/1000 | Loss: 0.00001794
Iteration 33/1000 | Loss: 0.00001794
Iteration 34/1000 | Loss: 0.00001793
Iteration 35/1000 | Loss: 0.00001793
Iteration 36/1000 | Loss: 0.00001793
Iteration 37/1000 | Loss: 0.00001793
Iteration 38/1000 | Loss: 0.00001793
Iteration 39/1000 | Loss: 0.00001793
Iteration 40/1000 | Loss: 0.00001793
Iteration 41/1000 | Loss: 0.00001793
Iteration 42/1000 | Loss: 0.00001793
Iteration 43/1000 | Loss: 0.00001793
Iteration 44/1000 | Loss: 0.00001791
Iteration 45/1000 | Loss: 0.00001791
Iteration 46/1000 | Loss: 0.00001790
Iteration 47/1000 | Loss: 0.00001790
Iteration 48/1000 | Loss: 0.00001790
Iteration 49/1000 | Loss: 0.00001790
Iteration 50/1000 | Loss: 0.00001790
Iteration 51/1000 | Loss: 0.00001790
Iteration 52/1000 | Loss: 0.00001790
Iteration 53/1000 | Loss: 0.00001790
Iteration 54/1000 | Loss: 0.00001790
Iteration 55/1000 | Loss: 0.00001790
Iteration 56/1000 | Loss: 0.00001790
Iteration 57/1000 | Loss: 0.00001790
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [1.789523048500996e-05, 1.789523048500996e-05, 1.789523048500996e-05, 1.789523048500996e-05, 1.789523048500996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.789523048500996e-05

Optimization complete. Final v2v error: 3.5545570850372314 mm

Highest mean error: 3.9059255123138428 mm for frame 188

Lowest mean error: 3.2065420150756836 mm for frame 239

Saving results

Total time: 30.714900970458984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00914274
Iteration 2/25 | Loss: 0.00138964
Iteration 3/25 | Loss: 0.00118849
Iteration 4/25 | Loss: 0.00117752
Iteration 5/25 | Loss: 0.00117422
Iteration 6/25 | Loss: 0.00117394
Iteration 7/25 | Loss: 0.00117394
Iteration 8/25 | Loss: 0.00117394
Iteration 9/25 | Loss: 0.00117394
Iteration 10/25 | Loss: 0.00117394
Iteration 11/25 | Loss: 0.00117394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011739392066374421, 0.0011739392066374421, 0.0011739392066374421, 0.0011739392066374421, 0.0011739392066374421]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011739392066374421

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24383509
Iteration 2/25 | Loss: 0.00093883
Iteration 3/25 | Loss: 0.00093883
Iteration 4/25 | Loss: 0.00093883
Iteration 5/25 | Loss: 0.00093883
Iteration 6/25 | Loss: 0.00093883
Iteration 7/25 | Loss: 0.00093883
Iteration 8/25 | Loss: 0.00093883
Iteration 9/25 | Loss: 0.00093883
Iteration 10/25 | Loss: 0.00093883
Iteration 11/25 | Loss: 0.00093883
Iteration 12/25 | Loss: 0.00093883
Iteration 13/25 | Loss: 0.00093883
Iteration 14/25 | Loss: 0.00093883
Iteration 15/25 | Loss: 0.00093883
Iteration 16/25 | Loss: 0.00093883
Iteration 17/25 | Loss: 0.00093883
Iteration 18/25 | Loss: 0.00093883
Iteration 19/25 | Loss: 0.00093883
Iteration 20/25 | Loss: 0.00093883
Iteration 21/25 | Loss: 0.00093883
Iteration 22/25 | Loss: 0.00093883
Iteration 23/25 | Loss: 0.00093883
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009388256585225463, 0.0009388256585225463, 0.0009388256585225463, 0.0009388256585225463, 0.0009388256585225463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009388256585225463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093883
Iteration 2/1000 | Loss: 0.00004772
Iteration 3/1000 | Loss: 0.00003160
Iteration 4/1000 | Loss: 0.00002781
Iteration 5/1000 | Loss: 0.00002626
Iteration 6/1000 | Loss: 0.00002487
Iteration 7/1000 | Loss: 0.00002404
Iteration 8/1000 | Loss: 0.00002331
Iteration 9/1000 | Loss: 0.00002275
Iteration 10/1000 | Loss: 0.00002250
Iteration 11/1000 | Loss: 0.00002242
Iteration 12/1000 | Loss: 0.00002237
Iteration 13/1000 | Loss: 0.00002235
Iteration 14/1000 | Loss: 0.00002234
Iteration 15/1000 | Loss: 0.00002232
Iteration 16/1000 | Loss: 0.00002223
Iteration 17/1000 | Loss: 0.00002217
Iteration 18/1000 | Loss: 0.00002208
Iteration 19/1000 | Loss: 0.00002207
Iteration 20/1000 | Loss: 0.00002203
Iteration 21/1000 | Loss: 0.00002203
Iteration 22/1000 | Loss: 0.00002203
Iteration 23/1000 | Loss: 0.00002203
Iteration 24/1000 | Loss: 0.00002202
Iteration 25/1000 | Loss: 0.00002202
Iteration 26/1000 | Loss: 0.00002202
Iteration 27/1000 | Loss: 0.00002202
Iteration 28/1000 | Loss: 0.00002201
Iteration 29/1000 | Loss: 0.00002201
Iteration 30/1000 | Loss: 0.00002201
Iteration 31/1000 | Loss: 0.00002201
Iteration 32/1000 | Loss: 0.00002200
Iteration 33/1000 | Loss: 0.00002200
Iteration 34/1000 | Loss: 0.00002199
Iteration 35/1000 | Loss: 0.00002199
Iteration 36/1000 | Loss: 0.00002199
Iteration 37/1000 | Loss: 0.00002198
Iteration 38/1000 | Loss: 0.00002198
Iteration 39/1000 | Loss: 0.00002198
Iteration 40/1000 | Loss: 0.00002198
Iteration 41/1000 | Loss: 0.00002198
Iteration 42/1000 | Loss: 0.00002197
Iteration 43/1000 | Loss: 0.00002197
Iteration 44/1000 | Loss: 0.00002195
Iteration 45/1000 | Loss: 0.00002195
Iteration 46/1000 | Loss: 0.00002195
Iteration 47/1000 | Loss: 0.00002195
Iteration 48/1000 | Loss: 0.00002195
Iteration 49/1000 | Loss: 0.00002194
Iteration 50/1000 | Loss: 0.00002194
Iteration 51/1000 | Loss: 0.00002194
Iteration 52/1000 | Loss: 0.00002194
Iteration 53/1000 | Loss: 0.00002194
Iteration 54/1000 | Loss: 0.00002194
Iteration 55/1000 | Loss: 0.00002193
Iteration 56/1000 | Loss: 0.00002193
Iteration 57/1000 | Loss: 0.00002193
Iteration 58/1000 | Loss: 0.00002193
Iteration 59/1000 | Loss: 0.00002193
Iteration 60/1000 | Loss: 0.00002193
Iteration 61/1000 | Loss: 0.00002193
Iteration 62/1000 | Loss: 0.00002193
Iteration 63/1000 | Loss: 0.00002193
Iteration 64/1000 | Loss: 0.00002193
Iteration 65/1000 | Loss: 0.00002193
Iteration 66/1000 | Loss: 0.00002193
Iteration 67/1000 | Loss: 0.00002193
Iteration 68/1000 | Loss: 0.00002192
Iteration 69/1000 | Loss: 0.00002192
Iteration 70/1000 | Loss: 0.00002192
Iteration 71/1000 | Loss: 0.00002192
Iteration 72/1000 | Loss: 0.00002192
Iteration 73/1000 | Loss: 0.00002192
Iteration 74/1000 | Loss: 0.00002191
Iteration 75/1000 | Loss: 0.00002191
Iteration 76/1000 | Loss: 0.00002191
Iteration 77/1000 | Loss: 0.00002191
Iteration 78/1000 | Loss: 0.00002191
Iteration 79/1000 | Loss: 0.00002191
Iteration 80/1000 | Loss: 0.00002191
Iteration 81/1000 | Loss: 0.00002191
Iteration 82/1000 | Loss: 0.00002190
Iteration 83/1000 | Loss: 0.00002190
Iteration 84/1000 | Loss: 0.00002190
Iteration 85/1000 | Loss: 0.00002190
Iteration 86/1000 | Loss: 0.00002190
Iteration 87/1000 | Loss: 0.00002190
Iteration 88/1000 | Loss: 0.00002190
Iteration 89/1000 | Loss: 0.00002189
Iteration 90/1000 | Loss: 0.00002189
Iteration 91/1000 | Loss: 0.00002189
Iteration 92/1000 | Loss: 0.00002189
Iteration 93/1000 | Loss: 0.00002189
Iteration 94/1000 | Loss: 0.00002189
Iteration 95/1000 | Loss: 0.00002189
Iteration 96/1000 | Loss: 0.00002188
Iteration 97/1000 | Loss: 0.00002188
Iteration 98/1000 | Loss: 0.00002187
Iteration 99/1000 | Loss: 0.00002187
Iteration 100/1000 | Loss: 0.00002187
Iteration 101/1000 | Loss: 0.00002187
Iteration 102/1000 | Loss: 0.00002187
Iteration 103/1000 | Loss: 0.00002187
Iteration 104/1000 | Loss: 0.00002186
Iteration 105/1000 | Loss: 0.00002186
Iteration 106/1000 | Loss: 0.00002186
Iteration 107/1000 | Loss: 0.00002185
Iteration 108/1000 | Loss: 0.00002185
Iteration 109/1000 | Loss: 0.00002185
Iteration 110/1000 | Loss: 0.00002185
Iteration 111/1000 | Loss: 0.00002184
Iteration 112/1000 | Loss: 0.00002184
Iteration 113/1000 | Loss: 0.00002184
Iteration 114/1000 | Loss: 0.00002184
Iteration 115/1000 | Loss: 0.00002184
Iteration 116/1000 | Loss: 0.00002184
Iteration 117/1000 | Loss: 0.00002184
Iteration 118/1000 | Loss: 0.00002184
Iteration 119/1000 | Loss: 0.00002183
Iteration 120/1000 | Loss: 0.00002183
Iteration 121/1000 | Loss: 0.00002183
Iteration 122/1000 | Loss: 0.00002183
Iteration 123/1000 | Loss: 0.00002183
Iteration 124/1000 | Loss: 0.00002183
Iteration 125/1000 | Loss: 0.00002183
Iteration 126/1000 | Loss: 0.00002182
Iteration 127/1000 | Loss: 0.00002182
Iteration 128/1000 | Loss: 0.00002182
Iteration 129/1000 | Loss: 0.00002182
Iteration 130/1000 | Loss: 0.00002182
Iteration 131/1000 | Loss: 0.00002181
Iteration 132/1000 | Loss: 0.00002181
Iteration 133/1000 | Loss: 0.00002181
Iteration 134/1000 | Loss: 0.00002181
Iteration 135/1000 | Loss: 0.00002180
Iteration 136/1000 | Loss: 0.00002180
Iteration 137/1000 | Loss: 0.00002180
Iteration 138/1000 | Loss: 0.00002180
Iteration 139/1000 | Loss: 0.00002180
Iteration 140/1000 | Loss: 0.00002180
Iteration 141/1000 | Loss: 0.00002180
Iteration 142/1000 | Loss: 0.00002180
Iteration 143/1000 | Loss: 0.00002179
Iteration 144/1000 | Loss: 0.00002179
Iteration 145/1000 | Loss: 0.00002179
Iteration 146/1000 | Loss: 0.00002179
Iteration 147/1000 | Loss: 0.00002179
Iteration 148/1000 | Loss: 0.00002179
Iteration 149/1000 | Loss: 0.00002179
Iteration 150/1000 | Loss: 0.00002178
Iteration 151/1000 | Loss: 0.00002178
Iteration 152/1000 | Loss: 0.00002178
Iteration 153/1000 | Loss: 0.00002178
Iteration 154/1000 | Loss: 0.00002178
Iteration 155/1000 | Loss: 0.00002177
Iteration 156/1000 | Loss: 0.00002177
Iteration 157/1000 | Loss: 0.00002177
Iteration 158/1000 | Loss: 0.00002177
Iteration 159/1000 | Loss: 0.00002176
Iteration 160/1000 | Loss: 0.00002176
Iteration 161/1000 | Loss: 0.00002176
Iteration 162/1000 | Loss: 0.00002176
Iteration 163/1000 | Loss: 0.00002176
Iteration 164/1000 | Loss: 0.00002176
Iteration 165/1000 | Loss: 0.00002176
Iteration 166/1000 | Loss: 0.00002175
Iteration 167/1000 | Loss: 0.00002175
Iteration 168/1000 | Loss: 0.00002175
Iteration 169/1000 | Loss: 0.00002175
Iteration 170/1000 | Loss: 0.00002174
Iteration 171/1000 | Loss: 0.00002174
Iteration 172/1000 | Loss: 0.00002174
Iteration 173/1000 | Loss: 0.00002174
Iteration 174/1000 | Loss: 0.00002173
Iteration 175/1000 | Loss: 0.00002173
Iteration 176/1000 | Loss: 0.00002173
Iteration 177/1000 | Loss: 0.00002173
Iteration 178/1000 | Loss: 0.00002173
Iteration 179/1000 | Loss: 0.00002173
Iteration 180/1000 | Loss: 0.00002173
Iteration 181/1000 | Loss: 0.00002172
Iteration 182/1000 | Loss: 0.00002172
Iteration 183/1000 | Loss: 0.00002172
Iteration 184/1000 | Loss: 0.00002172
Iteration 185/1000 | Loss: 0.00002172
Iteration 186/1000 | Loss: 0.00002172
Iteration 187/1000 | Loss: 0.00002172
Iteration 188/1000 | Loss: 0.00002172
Iteration 189/1000 | Loss: 0.00002171
Iteration 190/1000 | Loss: 0.00002171
Iteration 191/1000 | Loss: 0.00002171
Iteration 192/1000 | Loss: 0.00002171
Iteration 193/1000 | Loss: 0.00002171
Iteration 194/1000 | Loss: 0.00002171
Iteration 195/1000 | Loss: 0.00002171
Iteration 196/1000 | Loss: 0.00002171
Iteration 197/1000 | Loss: 0.00002171
Iteration 198/1000 | Loss: 0.00002171
Iteration 199/1000 | Loss: 0.00002171
Iteration 200/1000 | Loss: 0.00002170
Iteration 201/1000 | Loss: 0.00002170
Iteration 202/1000 | Loss: 0.00002170
Iteration 203/1000 | Loss: 0.00002170
Iteration 204/1000 | Loss: 0.00002170
Iteration 205/1000 | Loss: 0.00002170
Iteration 206/1000 | Loss: 0.00002170
Iteration 207/1000 | Loss: 0.00002170
Iteration 208/1000 | Loss: 0.00002170
Iteration 209/1000 | Loss: 0.00002170
Iteration 210/1000 | Loss: 0.00002169
Iteration 211/1000 | Loss: 0.00002169
Iteration 212/1000 | Loss: 0.00002169
Iteration 213/1000 | Loss: 0.00002169
Iteration 214/1000 | Loss: 0.00002169
Iteration 215/1000 | Loss: 0.00002169
Iteration 216/1000 | Loss: 0.00002169
Iteration 217/1000 | Loss: 0.00002169
Iteration 218/1000 | Loss: 0.00002169
Iteration 219/1000 | Loss: 0.00002169
Iteration 220/1000 | Loss: 0.00002169
Iteration 221/1000 | Loss: 0.00002169
Iteration 222/1000 | Loss: 0.00002169
Iteration 223/1000 | Loss: 0.00002169
Iteration 224/1000 | Loss: 0.00002169
Iteration 225/1000 | Loss: 0.00002169
Iteration 226/1000 | Loss: 0.00002169
Iteration 227/1000 | Loss: 0.00002168
Iteration 228/1000 | Loss: 0.00002168
Iteration 229/1000 | Loss: 0.00002168
Iteration 230/1000 | Loss: 0.00002168
Iteration 231/1000 | Loss: 0.00002168
Iteration 232/1000 | Loss: 0.00002168
Iteration 233/1000 | Loss: 0.00002168
Iteration 234/1000 | Loss: 0.00002168
Iteration 235/1000 | Loss: 0.00002168
Iteration 236/1000 | Loss: 0.00002168
Iteration 237/1000 | Loss: 0.00002168
Iteration 238/1000 | Loss: 0.00002168
Iteration 239/1000 | Loss: 0.00002168
Iteration 240/1000 | Loss: 0.00002168
Iteration 241/1000 | Loss: 0.00002168
Iteration 242/1000 | Loss: 0.00002168
Iteration 243/1000 | Loss: 0.00002168
Iteration 244/1000 | Loss: 0.00002168
Iteration 245/1000 | Loss: 0.00002168
Iteration 246/1000 | Loss: 0.00002168
Iteration 247/1000 | Loss: 0.00002168
Iteration 248/1000 | Loss: 0.00002168
Iteration 249/1000 | Loss: 0.00002168
Iteration 250/1000 | Loss: 0.00002168
Iteration 251/1000 | Loss: 0.00002168
Iteration 252/1000 | Loss: 0.00002168
Iteration 253/1000 | Loss: 0.00002168
Iteration 254/1000 | Loss: 0.00002168
Iteration 255/1000 | Loss: 0.00002168
Iteration 256/1000 | Loss: 0.00002168
Iteration 257/1000 | Loss: 0.00002168
Iteration 258/1000 | Loss: 0.00002168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [2.1684367311536334e-05, 2.1684367311536334e-05, 2.1684367311536334e-05, 2.1684367311536334e-05, 2.1684367311536334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1684367311536334e-05

Optimization complete. Final v2v error: 3.772127151489258 mm

Highest mean error: 4.718032360076904 mm for frame 232

Lowest mean error: 3.1517298221588135 mm for frame 158

Saving results

Total time: 47.77147817611694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00562593
Iteration 2/25 | Loss: 0.00126796
Iteration 3/25 | Loss: 0.00118276
Iteration 4/25 | Loss: 0.00117151
Iteration 5/25 | Loss: 0.00116737
Iteration 6/25 | Loss: 0.00116647
Iteration 7/25 | Loss: 0.00116647
Iteration 8/25 | Loss: 0.00116647
Iteration 9/25 | Loss: 0.00116645
Iteration 10/25 | Loss: 0.00116645
Iteration 11/25 | Loss: 0.00116645
Iteration 12/25 | Loss: 0.00116645
Iteration 13/25 | Loss: 0.00116645
Iteration 14/25 | Loss: 0.00116645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011664547491818666, 0.0011664547491818666, 0.0011664547491818666, 0.0011664547491818666, 0.0011664547491818666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011664547491818666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.49614811
Iteration 2/25 | Loss: 0.00104307
Iteration 3/25 | Loss: 0.00104305
Iteration 4/25 | Loss: 0.00104305
Iteration 5/25 | Loss: 0.00104305
Iteration 6/25 | Loss: 0.00104305
Iteration 7/25 | Loss: 0.00104305
Iteration 8/25 | Loss: 0.00104305
Iteration 9/25 | Loss: 0.00104305
Iteration 10/25 | Loss: 0.00104305
Iteration 11/25 | Loss: 0.00104305
Iteration 12/25 | Loss: 0.00104305
Iteration 13/25 | Loss: 0.00104305
Iteration 14/25 | Loss: 0.00104305
Iteration 15/25 | Loss: 0.00104305
Iteration 16/25 | Loss: 0.00104305
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001043046242557466, 0.001043046242557466, 0.001043046242557466, 0.001043046242557466, 0.001043046242557466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001043046242557466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104305
Iteration 2/1000 | Loss: 0.00003131
Iteration 3/1000 | Loss: 0.00002192
Iteration 4/1000 | Loss: 0.00001994
Iteration 5/1000 | Loss: 0.00001893
Iteration 6/1000 | Loss: 0.00001838
Iteration 7/1000 | Loss: 0.00001803
Iteration 8/1000 | Loss: 0.00001791
Iteration 9/1000 | Loss: 0.00001754
Iteration 10/1000 | Loss: 0.00001729
Iteration 11/1000 | Loss: 0.00001721
Iteration 12/1000 | Loss: 0.00001718
Iteration 13/1000 | Loss: 0.00001716
Iteration 14/1000 | Loss: 0.00001715
Iteration 15/1000 | Loss: 0.00001712
Iteration 16/1000 | Loss: 0.00001708
Iteration 17/1000 | Loss: 0.00001704
Iteration 18/1000 | Loss: 0.00001704
Iteration 19/1000 | Loss: 0.00001703
Iteration 20/1000 | Loss: 0.00001703
Iteration 21/1000 | Loss: 0.00001702
Iteration 22/1000 | Loss: 0.00001702
Iteration 23/1000 | Loss: 0.00001701
Iteration 24/1000 | Loss: 0.00001701
Iteration 25/1000 | Loss: 0.00001701
Iteration 26/1000 | Loss: 0.00001701
Iteration 27/1000 | Loss: 0.00001700
Iteration 28/1000 | Loss: 0.00001700
Iteration 29/1000 | Loss: 0.00001700
Iteration 30/1000 | Loss: 0.00001699
Iteration 31/1000 | Loss: 0.00001698
Iteration 32/1000 | Loss: 0.00001698
Iteration 33/1000 | Loss: 0.00001698
Iteration 34/1000 | Loss: 0.00001697
Iteration 35/1000 | Loss: 0.00001697
Iteration 36/1000 | Loss: 0.00001696
Iteration 37/1000 | Loss: 0.00001695
Iteration 38/1000 | Loss: 0.00001695
Iteration 39/1000 | Loss: 0.00001695
Iteration 40/1000 | Loss: 0.00001695
Iteration 41/1000 | Loss: 0.00001695
Iteration 42/1000 | Loss: 0.00001695
Iteration 43/1000 | Loss: 0.00001695
Iteration 44/1000 | Loss: 0.00001694
Iteration 45/1000 | Loss: 0.00001694
Iteration 46/1000 | Loss: 0.00001694
Iteration 47/1000 | Loss: 0.00001693
Iteration 48/1000 | Loss: 0.00001693
Iteration 49/1000 | Loss: 0.00001693
Iteration 50/1000 | Loss: 0.00001692
Iteration 51/1000 | Loss: 0.00001692
Iteration 52/1000 | Loss: 0.00001692
Iteration 53/1000 | Loss: 0.00001691
Iteration 54/1000 | Loss: 0.00001691
Iteration 55/1000 | Loss: 0.00001691
Iteration 56/1000 | Loss: 0.00001691
Iteration 57/1000 | Loss: 0.00001691
Iteration 58/1000 | Loss: 0.00001691
Iteration 59/1000 | Loss: 0.00001691
Iteration 60/1000 | Loss: 0.00001691
Iteration 61/1000 | Loss: 0.00001691
Iteration 62/1000 | Loss: 0.00001690
Iteration 63/1000 | Loss: 0.00001690
Iteration 64/1000 | Loss: 0.00001690
Iteration 65/1000 | Loss: 0.00001689
Iteration 66/1000 | Loss: 0.00001689
Iteration 67/1000 | Loss: 0.00001689
Iteration 68/1000 | Loss: 0.00001689
Iteration 69/1000 | Loss: 0.00001689
Iteration 70/1000 | Loss: 0.00001689
Iteration 71/1000 | Loss: 0.00001689
Iteration 72/1000 | Loss: 0.00001688
Iteration 73/1000 | Loss: 0.00001688
Iteration 74/1000 | Loss: 0.00001688
Iteration 75/1000 | Loss: 0.00001688
Iteration 76/1000 | Loss: 0.00001688
Iteration 77/1000 | Loss: 0.00001688
Iteration 78/1000 | Loss: 0.00001687
Iteration 79/1000 | Loss: 0.00001687
Iteration 80/1000 | Loss: 0.00001687
Iteration 81/1000 | Loss: 0.00001687
Iteration 82/1000 | Loss: 0.00001687
Iteration 83/1000 | Loss: 0.00001687
Iteration 84/1000 | Loss: 0.00001687
Iteration 85/1000 | Loss: 0.00001687
Iteration 86/1000 | Loss: 0.00001686
Iteration 87/1000 | Loss: 0.00001686
Iteration 88/1000 | Loss: 0.00001686
Iteration 89/1000 | Loss: 0.00001686
Iteration 90/1000 | Loss: 0.00001686
Iteration 91/1000 | Loss: 0.00001686
Iteration 92/1000 | Loss: 0.00001686
Iteration 93/1000 | Loss: 0.00001686
Iteration 94/1000 | Loss: 0.00001685
Iteration 95/1000 | Loss: 0.00001685
Iteration 96/1000 | Loss: 0.00001685
Iteration 97/1000 | Loss: 0.00001685
Iteration 98/1000 | Loss: 0.00001685
Iteration 99/1000 | Loss: 0.00001685
Iteration 100/1000 | Loss: 0.00001684
Iteration 101/1000 | Loss: 0.00001684
Iteration 102/1000 | Loss: 0.00001684
Iteration 103/1000 | Loss: 0.00001684
Iteration 104/1000 | Loss: 0.00001684
Iteration 105/1000 | Loss: 0.00001684
Iteration 106/1000 | Loss: 0.00001684
Iteration 107/1000 | Loss: 0.00001684
Iteration 108/1000 | Loss: 0.00001684
Iteration 109/1000 | Loss: 0.00001683
Iteration 110/1000 | Loss: 0.00001683
Iteration 111/1000 | Loss: 0.00001683
Iteration 112/1000 | Loss: 0.00001683
Iteration 113/1000 | Loss: 0.00001683
Iteration 114/1000 | Loss: 0.00001682
Iteration 115/1000 | Loss: 0.00001682
Iteration 116/1000 | Loss: 0.00001682
Iteration 117/1000 | Loss: 0.00001682
Iteration 118/1000 | Loss: 0.00001682
Iteration 119/1000 | Loss: 0.00001681
Iteration 120/1000 | Loss: 0.00001681
Iteration 121/1000 | Loss: 0.00001681
Iteration 122/1000 | Loss: 0.00001681
Iteration 123/1000 | Loss: 0.00001681
Iteration 124/1000 | Loss: 0.00001681
Iteration 125/1000 | Loss: 0.00001681
Iteration 126/1000 | Loss: 0.00001681
Iteration 127/1000 | Loss: 0.00001681
Iteration 128/1000 | Loss: 0.00001681
Iteration 129/1000 | Loss: 0.00001681
Iteration 130/1000 | Loss: 0.00001681
Iteration 131/1000 | Loss: 0.00001681
Iteration 132/1000 | Loss: 0.00001681
Iteration 133/1000 | Loss: 0.00001681
Iteration 134/1000 | Loss: 0.00001680
Iteration 135/1000 | Loss: 0.00001680
Iteration 136/1000 | Loss: 0.00001680
Iteration 137/1000 | Loss: 0.00001680
Iteration 138/1000 | Loss: 0.00001680
Iteration 139/1000 | Loss: 0.00001680
Iteration 140/1000 | Loss: 0.00001680
Iteration 141/1000 | Loss: 0.00001680
Iteration 142/1000 | Loss: 0.00001680
Iteration 143/1000 | Loss: 0.00001680
Iteration 144/1000 | Loss: 0.00001680
Iteration 145/1000 | Loss: 0.00001680
Iteration 146/1000 | Loss: 0.00001680
Iteration 147/1000 | Loss: 0.00001680
Iteration 148/1000 | Loss: 0.00001680
Iteration 149/1000 | Loss: 0.00001680
Iteration 150/1000 | Loss: 0.00001680
Iteration 151/1000 | Loss: 0.00001680
Iteration 152/1000 | Loss: 0.00001680
Iteration 153/1000 | Loss: 0.00001680
Iteration 154/1000 | Loss: 0.00001680
Iteration 155/1000 | Loss: 0.00001680
Iteration 156/1000 | Loss: 0.00001680
Iteration 157/1000 | Loss: 0.00001680
Iteration 158/1000 | Loss: 0.00001680
Iteration 159/1000 | Loss: 0.00001680
Iteration 160/1000 | Loss: 0.00001680
Iteration 161/1000 | Loss: 0.00001680
Iteration 162/1000 | Loss: 0.00001680
Iteration 163/1000 | Loss: 0.00001680
Iteration 164/1000 | Loss: 0.00001680
Iteration 165/1000 | Loss: 0.00001680
Iteration 166/1000 | Loss: 0.00001680
Iteration 167/1000 | Loss: 0.00001680
Iteration 168/1000 | Loss: 0.00001680
Iteration 169/1000 | Loss: 0.00001680
Iteration 170/1000 | Loss: 0.00001680
Iteration 171/1000 | Loss: 0.00001680
Iteration 172/1000 | Loss: 0.00001680
Iteration 173/1000 | Loss: 0.00001680
Iteration 174/1000 | Loss: 0.00001680
Iteration 175/1000 | Loss: 0.00001680
Iteration 176/1000 | Loss: 0.00001680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.680003697401844e-05, 1.680003697401844e-05, 1.680003697401844e-05, 1.680003697401844e-05, 1.680003697401844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.680003697401844e-05

Optimization complete. Final v2v error: 3.5644643306732178 mm

Highest mean error: 4.2159600257873535 mm for frame 93

Lowest mean error: 3.112989664077759 mm for frame 0

Saving results

Total time: 34.69105362892151
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01111171
Iteration 2/25 | Loss: 0.00198304
Iteration 3/25 | Loss: 0.00144884
Iteration 4/25 | Loss: 0.00132623
Iteration 5/25 | Loss: 0.00124953
Iteration 6/25 | Loss: 0.00123670
Iteration 7/25 | Loss: 0.00114338
Iteration 8/25 | Loss: 0.00114367
Iteration 9/25 | Loss: 0.00113055
Iteration 10/25 | Loss: 0.00114713
Iteration 11/25 | Loss: 0.00118087
Iteration 12/25 | Loss: 0.00114055
Iteration 13/25 | Loss: 0.00112533
Iteration 14/25 | Loss: 0.00111307
Iteration 15/25 | Loss: 0.00113407
Iteration 16/25 | Loss: 0.00110268
Iteration 17/25 | Loss: 0.00110144
Iteration 18/25 | Loss: 0.00110106
Iteration 19/25 | Loss: 0.00110084
Iteration 20/25 | Loss: 0.00110071
Iteration 21/25 | Loss: 0.00110070
Iteration 22/25 | Loss: 0.00110070
Iteration 23/25 | Loss: 0.00110070
Iteration 24/25 | Loss: 0.00110070
Iteration 25/25 | Loss: 0.00110070

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22896123
Iteration 2/25 | Loss: 0.00097754
Iteration 3/25 | Loss: 0.00097754
Iteration 4/25 | Loss: 0.00097754
Iteration 5/25 | Loss: 0.00097754
Iteration 6/25 | Loss: 0.00097754
Iteration 7/25 | Loss: 0.00097754
Iteration 8/25 | Loss: 0.00097754
Iteration 9/25 | Loss: 0.00097754
Iteration 10/25 | Loss: 0.00097754
Iteration 11/25 | Loss: 0.00097754
Iteration 12/25 | Loss: 0.00097754
Iteration 13/25 | Loss: 0.00097754
Iteration 14/25 | Loss: 0.00097754
Iteration 15/25 | Loss: 0.00097754
Iteration 16/25 | Loss: 0.00097754
Iteration 17/25 | Loss: 0.00097754
Iteration 18/25 | Loss: 0.00097754
Iteration 19/25 | Loss: 0.00097754
Iteration 20/25 | Loss: 0.00097754
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009775370126590133, 0.0009775370126590133, 0.0009775370126590133, 0.0009775370126590133, 0.0009775370126590133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009775370126590133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097754
Iteration 2/1000 | Loss: 0.00003957
Iteration 3/1000 | Loss: 0.00003016
Iteration 4/1000 | Loss: 0.00002777
Iteration 5/1000 | Loss: 0.00002626
Iteration 6/1000 | Loss: 0.00002531
Iteration 7/1000 | Loss: 0.00002457
Iteration 8/1000 | Loss: 0.00002415
Iteration 9/1000 | Loss: 0.00002383
Iteration 10/1000 | Loss: 0.00002359
Iteration 11/1000 | Loss: 0.00002358
Iteration 12/1000 | Loss: 0.00002353
Iteration 13/1000 | Loss: 0.00002352
Iteration 14/1000 | Loss: 0.00002352
Iteration 15/1000 | Loss: 0.00002352
Iteration 16/1000 | Loss: 0.00002350
Iteration 17/1000 | Loss: 0.00002350
Iteration 18/1000 | Loss: 0.00002350
Iteration 19/1000 | Loss: 0.00002350
Iteration 20/1000 | Loss: 0.00002349
Iteration 21/1000 | Loss: 0.00002349
Iteration 22/1000 | Loss: 0.00002349
Iteration 23/1000 | Loss: 0.00002349
Iteration 24/1000 | Loss: 0.00002345
Iteration 25/1000 | Loss: 0.00002345
Iteration 26/1000 | Loss: 0.00002345
Iteration 27/1000 | Loss: 0.00002345
Iteration 28/1000 | Loss: 0.00002345
Iteration 29/1000 | Loss: 0.00002345
Iteration 30/1000 | Loss: 0.00002345
Iteration 31/1000 | Loss: 0.00002345
Iteration 32/1000 | Loss: 0.00002344
Iteration 33/1000 | Loss: 0.00002344
Iteration 34/1000 | Loss: 0.00002344
Iteration 35/1000 | Loss: 0.00002344
Iteration 36/1000 | Loss: 0.00002344
Iteration 37/1000 | Loss: 0.00002344
Iteration 38/1000 | Loss: 0.00002344
Iteration 39/1000 | Loss: 0.00002342
Iteration 40/1000 | Loss: 0.00002341
Iteration 41/1000 | Loss: 0.00002340
Iteration 42/1000 | Loss: 0.00002339
Iteration 43/1000 | Loss: 0.00002339
Iteration 44/1000 | Loss: 0.00002339
Iteration 45/1000 | Loss: 0.00002338
Iteration 46/1000 | Loss: 0.00002336
Iteration 47/1000 | Loss: 0.00002336
Iteration 48/1000 | Loss: 0.00002336
Iteration 49/1000 | Loss: 0.00002336
Iteration 50/1000 | Loss: 0.00002336
Iteration 51/1000 | Loss: 0.00002336
Iteration 52/1000 | Loss: 0.00002336
Iteration 53/1000 | Loss: 0.00002336
Iteration 54/1000 | Loss: 0.00002336
Iteration 55/1000 | Loss: 0.00002336
Iteration 56/1000 | Loss: 0.00002336
Iteration 57/1000 | Loss: 0.00002336
Iteration 58/1000 | Loss: 0.00002336
Iteration 59/1000 | Loss: 0.00002336
Iteration 60/1000 | Loss: 0.00002335
Iteration 61/1000 | Loss: 0.00002335
Iteration 62/1000 | Loss: 0.00002335
Iteration 63/1000 | Loss: 0.00002334
Iteration 64/1000 | Loss: 0.00002334
Iteration 65/1000 | Loss: 0.00002334
Iteration 66/1000 | Loss: 0.00002334
Iteration 67/1000 | Loss: 0.00002333
Iteration 68/1000 | Loss: 0.00002333
Iteration 69/1000 | Loss: 0.00002333
Iteration 70/1000 | Loss: 0.00002333
Iteration 71/1000 | Loss: 0.00002333
Iteration 72/1000 | Loss: 0.00002333
Iteration 73/1000 | Loss: 0.00002332
Iteration 74/1000 | Loss: 0.00002332
Iteration 75/1000 | Loss: 0.00002332
Iteration 76/1000 | Loss: 0.00002332
Iteration 77/1000 | Loss: 0.00002331
Iteration 78/1000 | Loss: 0.00002331
Iteration 79/1000 | Loss: 0.00002330
Iteration 80/1000 | Loss: 0.00002330
Iteration 81/1000 | Loss: 0.00002330
Iteration 82/1000 | Loss: 0.00002330
Iteration 83/1000 | Loss: 0.00002330
Iteration 84/1000 | Loss: 0.00002329
Iteration 85/1000 | Loss: 0.00002329
Iteration 86/1000 | Loss: 0.00002329
Iteration 87/1000 | Loss: 0.00002329
Iteration 88/1000 | Loss: 0.00002328
Iteration 89/1000 | Loss: 0.00002328
Iteration 90/1000 | Loss: 0.00002328
Iteration 91/1000 | Loss: 0.00002328
Iteration 92/1000 | Loss: 0.00002328
Iteration 93/1000 | Loss: 0.00002327
Iteration 94/1000 | Loss: 0.00002327
Iteration 95/1000 | Loss: 0.00002327
Iteration 96/1000 | Loss: 0.00002327
Iteration 97/1000 | Loss: 0.00002327
Iteration 98/1000 | Loss: 0.00002326
Iteration 99/1000 | Loss: 0.00002326
Iteration 100/1000 | Loss: 0.00002326
Iteration 101/1000 | Loss: 0.00002326
Iteration 102/1000 | Loss: 0.00002326
Iteration 103/1000 | Loss: 0.00002326
Iteration 104/1000 | Loss: 0.00002325
Iteration 105/1000 | Loss: 0.00002325
Iteration 106/1000 | Loss: 0.00002325
Iteration 107/1000 | Loss: 0.00002325
Iteration 108/1000 | Loss: 0.00002325
Iteration 109/1000 | Loss: 0.00002325
Iteration 110/1000 | Loss: 0.00002325
Iteration 111/1000 | Loss: 0.00002325
Iteration 112/1000 | Loss: 0.00002325
Iteration 113/1000 | Loss: 0.00002325
Iteration 114/1000 | Loss: 0.00002325
Iteration 115/1000 | Loss: 0.00002325
Iteration 116/1000 | Loss: 0.00002325
Iteration 117/1000 | Loss: 0.00002325
Iteration 118/1000 | Loss: 0.00002325
Iteration 119/1000 | Loss: 0.00002325
Iteration 120/1000 | Loss: 0.00002325
Iteration 121/1000 | Loss: 0.00002325
Iteration 122/1000 | Loss: 0.00002325
Iteration 123/1000 | Loss: 0.00002325
Iteration 124/1000 | Loss: 0.00002325
Iteration 125/1000 | Loss: 0.00002325
Iteration 126/1000 | Loss: 0.00002325
Iteration 127/1000 | Loss: 0.00002325
Iteration 128/1000 | Loss: 0.00002325
Iteration 129/1000 | Loss: 0.00002324
Iteration 130/1000 | Loss: 0.00002324
Iteration 131/1000 | Loss: 0.00002324
Iteration 132/1000 | Loss: 0.00002324
Iteration 133/1000 | Loss: 0.00002324
Iteration 134/1000 | Loss: 0.00002324
Iteration 135/1000 | Loss: 0.00002324
Iteration 136/1000 | Loss: 0.00002324
Iteration 137/1000 | Loss: 0.00002324
Iteration 138/1000 | Loss: 0.00002324
Iteration 139/1000 | Loss: 0.00002324
Iteration 140/1000 | Loss: 0.00002324
Iteration 141/1000 | Loss: 0.00002324
Iteration 142/1000 | Loss: 0.00002324
Iteration 143/1000 | Loss: 0.00002324
Iteration 144/1000 | Loss: 0.00002324
Iteration 145/1000 | Loss: 0.00002324
Iteration 146/1000 | Loss: 0.00002324
Iteration 147/1000 | Loss: 0.00002324
Iteration 148/1000 | Loss: 0.00002324
Iteration 149/1000 | Loss: 0.00002324
Iteration 150/1000 | Loss: 0.00002324
Iteration 151/1000 | Loss: 0.00002324
Iteration 152/1000 | Loss: 0.00002324
Iteration 153/1000 | Loss: 0.00002324
Iteration 154/1000 | Loss: 0.00002324
Iteration 155/1000 | Loss: 0.00002324
Iteration 156/1000 | Loss: 0.00002324
Iteration 157/1000 | Loss: 0.00002324
Iteration 158/1000 | Loss: 0.00002324
Iteration 159/1000 | Loss: 0.00002324
Iteration 160/1000 | Loss: 0.00002324
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.3236209017341025e-05, 2.3236209017341025e-05, 2.3236209017341025e-05, 2.3236209017341025e-05, 2.3236209017341025e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3236209017341025e-05

Optimization complete. Final v2v error: 3.920530080795288 mm

Highest mean error: 9.227655410766602 mm for frame 47

Lowest mean error: 3.592186689376831 mm for frame 3

Saving results

Total time: 67.37485480308533
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001122
Iteration 2/25 | Loss: 0.00209332
Iteration 3/25 | Loss: 0.00153665
Iteration 4/25 | Loss: 0.00143680
Iteration 5/25 | Loss: 0.00131922
Iteration 6/25 | Loss: 0.00127030
Iteration 7/25 | Loss: 0.00125134
Iteration 8/25 | Loss: 0.00134562
Iteration 9/25 | Loss: 0.00127626
Iteration 10/25 | Loss: 0.00119907
Iteration 11/25 | Loss: 0.00117326
Iteration 12/25 | Loss: 0.00114651
Iteration 13/25 | Loss: 0.00113899
Iteration 14/25 | Loss: 0.00113497
Iteration 15/25 | Loss: 0.00113365
Iteration 16/25 | Loss: 0.00113186
Iteration 17/25 | Loss: 0.00113132
Iteration 18/25 | Loss: 0.00113023
Iteration 19/25 | Loss: 0.00113008
Iteration 20/25 | Loss: 0.00113036
Iteration 21/25 | Loss: 0.00113013
Iteration 22/25 | Loss: 0.00112985
Iteration 23/25 | Loss: 0.00112982
Iteration 24/25 | Loss: 0.00112928
Iteration 25/25 | Loss: 0.00112972

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33886123
Iteration 2/25 | Loss: 0.00150656
Iteration 3/25 | Loss: 0.00120731
Iteration 4/25 | Loss: 0.00120731
Iteration 5/25 | Loss: 0.00120731
Iteration 6/25 | Loss: 0.00120731
Iteration 7/25 | Loss: 0.00120731
Iteration 8/25 | Loss: 0.00120731
Iteration 9/25 | Loss: 0.00120731
Iteration 10/25 | Loss: 0.00120731
Iteration 11/25 | Loss: 0.00120731
Iteration 12/25 | Loss: 0.00120731
Iteration 13/25 | Loss: 0.00120731
Iteration 14/25 | Loss: 0.00120731
Iteration 15/25 | Loss: 0.00120731
Iteration 16/25 | Loss: 0.00120731
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012073091929778457, 0.0012073091929778457, 0.0012073091929778457, 0.0012073091929778457, 0.0012073091929778457]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012073091929778457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120731
Iteration 2/1000 | Loss: 0.00023309
Iteration 3/1000 | Loss: 0.00018449
Iteration 4/1000 | Loss: 0.00005084
Iteration 5/1000 | Loss: 0.00004848
Iteration 6/1000 | Loss: 0.00003535
Iteration 7/1000 | Loss: 0.00005468
Iteration 8/1000 | Loss: 0.00003313
Iteration 9/1000 | Loss: 0.00005073
Iteration 10/1000 | Loss: 0.00003742
Iteration 11/1000 | Loss: 0.00013379
Iteration 12/1000 | Loss: 0.00003714
Iteration 13/1000 | Loss: 0.00004753
Iteration 14/1000 | Loss: 0.00004741
Iteration 15/1000 | Loss: 0.00004269
Iteration 16/1000 | Loss: 0.00004523
Iteration 17/1000 | Loss: 0.00004027
Iteration 18/1000 | Loss: 0.00003875
Iteration 19/1000 | Loss: 0.00003799
Iteration 20/1000 | Loss: 0.00003855
Iteration 21/1000 | Loss: 0.00004087
Iteration 22/1000 | Loss: 0.00038580
Iteration 23/1000 | Loss: 0.00003970
Iteration 24/1000 | Loss: 0.00003904
Iteration 25/1000 | Loss: 0.00005282
Iteration 26/1000 | Loss: 0.00003401
Iteration 27/1000 | Loss: 0.00003893
Iteration 28/1000 | Loss: 0.00003805
Iteration 29/1000 | Loss: 0.00003697
Iteration 30/1000 | Loss: 0.00004320
Iteration 31/1000 | Loss: 0.00004674
Iteration 32/1000 | Loss: 0.00004420
Iteration 33/1000 | Loss: 0.00004479
Iteration 34/1000 | Loss: 0.00003795
Iteration 35/1000 | Loss: 0.00003655
Iteration 36/1000 | Loss: 0.00004089
Iteration 37/1000 | Loss: 0.00003576
Iteration 38/1000 | Loss: 0.00003832
Iteration 39/1000 | Loss: 0.00004669
Iteration 40/1000 | Loss: 0.00003705
Iteration 41/1000 | Loss: 0.00004372
Iteration 42/1000 | Loss: 0.00003848
Iteration 43/1000 | Loss: 0.00004545
Iteration 44/1000 | Loss: 0.00003891
Iteration 45/1000 | Loss: 0.00005738
Iteration 46/1000 | Loss: 0.00004418
Iteration 47/1000 | Loss: 0.00004476
Iteration 48/1000 | Loss: 0.00004879
Iteration 49/1000 | Loss: 0.00004558
Iteration 50/1000 | Loss: 0.00003357
Iteration 51/1000 | Loss: 0.00003197
Iteration 52/1000 | Loss: 0.00003843
Iteration 53/1000 | Loss: 0.00004225
Iteration 54/1000 | Loss: 0.00003361
Iteration 55/1000 | Loss: 0.00003700
Iteration 56/1000 | Loss: 0.00005989
Iteration 57/1000 | Loss: 0.00003086
Iteration 58/1000 | Loss: 0.00002742
Iteration 59/1000 | Loss: 0.00002610
Iteration 60/1000 | Loss: 0.00002550
Iteration 61/1000 | Loss: 0.00002510
Iteration 62/1000 | Loss: 0.00002486
Iteration 63/1000 | Loss: 0.00002484
Iteration 64/1000 | Loss: 0.00002474
Iteration 65/1000 | Loss: 0.00002470
Iteration 66/1000 | Loss: 0.00002459
Iteration 67/1000 | Loss: 0.00002456
Iteration 68/1000 | Loss: 0.00002456
Iteration 69/1000 | Loss: 0.00002456
Iteration 70/1000 | Loss: 0.00002455
Iteration 71/1000 | Loss: 0.00002455
Iteration 72/1000 | Loss: 0.00002455
Iteration 73/1000 | Loss: 0.00002453
Iteration 74/1000 | Loss: 0.00002453
Iteration 75/1000 | Loss: 0.00002452
Iteration 76/1000 | Loss: 0.00002451
Iteration 77/1000 | Loss: 0.00002451
Iteration 78/1000 | Loss: 0.00002451
Iteration 79/1000 | Loss: 0.00002450
Iteration 80/1000 | Loss: 0.00002448
Iteration 81/1000 | Loss: 0.00002448
Iteration 82/1000 | Loss: 0.00002447
Iteration 83/1000 | Loss: 0.00002447
Iteration 84/1000 | Loss: 0.00002445
Iteration 85/1000 | Loss: 0.00002445
Iteration 86/1000 | Loss: 0.00002445
Iteration 87/1000 | Loss: 0.00002445
Iteration 88/1000 | Loss: 0.00002445
Iteration 89/1000 | Loss: 0.00002445
Iteration 90/1000 | Loss: 0.00002444
Iteration 91/1000 | Loss: 0.00002444
Iteration 92/1000 | Loss: 0.00002444
Iteration 93/1000 | Loss: 0.00002444
Iteration 94/1000 | Loss: 0.00002444
Iteration 95/1000 | Loss: 0.00002444
Iteration 96/1000 | Loss: 0.00002443
Iteration 97/1000 | Loss: 0.00002443
Iteration 98/1000 | Loss: 0.00002442
Iteration 99/1000 | Loss: 0.00002442
Iteration 100/1000 | Loss: 0.00002442
Iteration 101/1000 | Loss: 0.00002441
Iteration 102/1000 | Loss: 0.00002441
Iteration 103/1000 | Loss: 0.00002440
Iteration 104/1000 | Loss: 0.00002440
Iteration 105/1000 | Loss: 0.00002440
Iteration 106/1000 | Loss: 0.00002438
Iteration 107/1000 | Loss: 0.00002437
Iteration 108/1000 | Loss: 0.00002437
Iteration 109/1000 | Loss: 0.00002437
Iteration 110/1000 | Loss: 0.00002437
Iteration 111/1000 | Loss: 0.00002437
Iteration 112/1000 | Loss: 0.00002436
Iteration 113/1000 | Loss: 0.00002436
Iteration 114/1000 | Loss: 0.00002435
Iteration 115/1000 | Loss: 0.00002434
Iteration 116/1000 | Loss: 0.00002433
Iteration 117/1000 | Loss: 0.00002432
Iteration 118/1000 | Loss: 0.00002432
Iteration 119/1000 | Loss: 0.00002432
Iteration 120/1000 | Loss: 0.00002432
Iteration 121/1000 | Loss: 0.00002432
Iteration 122/1000 | Loss: 0.00002431
Iteration 123/1000 | Loss: 0.00002431
Iteration 124/1000 | Loss: 0.00002431
Iteration 125/1000 | Loss: 0.00002431
Iteration 126/1000 | Loss: 0.00002431
Iteration 127/1000 | Loss: 0.00002431
Iteration 128/1000 | Loss: 0.00002430
Iteration 129/1000 | Loss: 0.00002430
Iteration 130/1000 | Loss: 0.00002426
Iteration 131/1000 | Loss: 0.00002426
Iteration 132/1000 | Loss: 0.00002425
Iteration 133/1000 | Loss: 0.00002425
Iteration 134/1000 | Loss: 0.00002425
Iteration 135/1000 | Loss: 0.00002425
Iteration 136/1000 | Loss: 0.00002425
Iteration 137/1000 | Loss: 0.00002425
Iteration 138/1000 | Loss: 0.00002425
Iteration 139/1000 | Loss: 0.00002425
Iteration 140/1000 | Loss: 0.00002425
Iteration 141/1000 | Loss: 0.00002425
Iteration 142/1000 | Loss: 0.00002424
Iteration 143/1000 | Loss: 0.00002424
Iteration 144/1000 | Loss: 0.00002424
Iteration 145/1000 | Loss: 0.00002424
Iteration 146/1000 | Loss: 0.00002424
Iteration 147/1000 | Loss: 0.00002424
Iteration 148/1000 | Loss: 0.00002424
Iteration 149/1000 | Loss: 0.00002424
Iteration 150/1000 | Loss: 0.00002423
Iteration 151/1000 | Loss: 0.00002423
Iteration 152/1000 | Loss: 0.00002423
Iteration 153/1000 | Loss: 0.00002422
Iteration 154/1000 | Loss: 0.00002422
Iteration 155/1000 | Loss: 0.00002422
Iteration 156/1000 | Loss: 0.00002422
Iteration 157/1000 | Loss: 0.00002422
Iteration 158/1000 | Loss: 0.00002422
Iteration 159/1000 | Loss: 0.00002421
Iteration 160/1000 | Loss: 0.00002421
Iteration 161/1000 | Loss: 0.00002421
Iteration 162/1000 | Loss: 0.00002420
Iteration 163/1000 | Loss: 0.00002420
Iteration 164/1000 | Loss: 0.00002420
Iteration 165/1000 | Loss: 0.00002427
Iteration 166/1000 | Loss: 0.00002426
Iteration 167/1000 | Loss: 0.00002426
Iteration 168/1000 | Loss: 0.00002426
Iteration 169/1000 | Loss: 0.00002426
Iteration 170/1000 | Loss: 0.00002426
Iteration 171/1000 | Loss: 0.00002426
Iteration 172/1000 | Loss: 0.00002426
Iteration 173/1000 | Loss: 0.00002425
Iteration 174/1000 | Loss: 0.00002425
Iteration 175/1000 | Loss: 0.00002425
Iteration 176/1000 | Loss: 0.00002425
Iteration 177/1000 | Loss: 0.00002425
Iteration 178/1000 | Loss: 0.00002424
Iteration 179/1000 | Loss: 0.00002424
Iteration 180/1000 | Loss: 0.00002424
Iteration 181/1000 | Loss: 0.00002424
Iteration 182/1000 | Loss: 0.00002423
Iteration 183/1000 | Loss: 0.00002423
Iteration 184/1000 | Loss: 0.00002423
Iteration 185/1000 | Loss: 0.00002423
Iteration 186/1000 | Loss: 0.00002422
Iteration 187/1000 | Loss: 0.00002421
Iteration 188/1000 | Loss: 0.00002421
Iteration 189/1000 | Loss: 0.00002421
Iteration 190/1000 | Loss: 0.00002421
Iteration 191/1000 | Loss: 0.00002421
Iteration 192/1000 | Loss: 0.00002421
Iteration 193/1000 | Loss: 0.00002421
Iteration 194/1000 | Loss: 0.00002421
Iteration 195/1000 | Loss: 0.00002421
Iteration 196/1000 | Loss: 0.00002421
Iteration 197/1000 | Loss: 0.00002421
Iteration 198/1000 | Loss: 0.00002421
Iteration 199/1000 | Loss: 0.00002421
Iteration 200/1000 | Loss: 0.00002421
Iteration 201/1000 | Loss: 0.00002421
Iteration 202/1000 | Loss: 0.00002421
Iteration 203/1000 | Loss: 0.00002421
Iteration 204/1000 | Loss: 0.00002421
Iteration 205/1000 | Loss: 0.00002421
Iteration 206/1000 | Loss: 0.00002421
Iteration 207/1000 | Loss: 0.00002421
Iteration 208/1000 | Loss: 0.00002421
Iteration 209/1000 | Loss: 0.00002421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.420632154098712e-05, 2.420632154098712e-05, 2.420632154098712e-05, 2.420632154098712e-05, 2.420632154098712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.420632154098712e-05

Optimization complete. Final v2v error: 3.6902637481689453 mm

Highest mean error: 22.87603759765625 mm for frame 23

Lowest mean error: 3.21284818649292 mm for frame 60

Saving results

Total time: 152.6205952167511
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00910161
Iteration 2/25 | Loss: 0.00187857
Iteration 3/25 | Loss: 0.00154614
Iteration 4/25 | Loss: 0.00150369
Iteration 5/25 | Loss: 0.00149382
Iteration 6/25 | Loss: 0.00149335
Iteration 7/25 | Loss: 0.00149582
Iteration 8/25 | Loss: 0.00149010
Iteration 9/25 | Loss: 0.00148061
Iteration 10/25 | Loss: 0.00147255
Iteration 11/25 | Loss: 0.00146845
Iteration 12/25 | Loss: 0.00147211
Iteration 13/25 | Loss: 0.00146502
Iteration 14/25 | Loss: 0.00146299
Iteration 15/25 | Loss: 0.00146253
Iteration 16/25 | Loss: 0.00146224
Iteration 17/25 | Loss: 0.00146214
Iteration 18/25 | Loss: 0.00146211
Iteration 19/25 | Loss: 0.00146210
Iteration 20/25 | Loss: 0.00146210
Iteration 21/25 | Loss: 0.00146210
Iteration 22/25 | Loss: 0.00146210
Iteration 23/25 | Loss: 0.00146210
Iteration 24/25 | Loss: 0.00146210
Iteration 25/25 | Loss: 0.00146209

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20119035
Iteration 2/25 | Loss: 0.00243116
Iteration 3/25 | Loss: 0.00243110
Iteration 4/25 | Loss: 0.00243110
Iteration 5/25 | Loss: 0.00243110
Iteration 6/25 | Loss: 0.00243110
Iteration 7/25 | Loss: 0.00243110
Iteration 8/25 | Loss: 0.00243110
Iteration 9/25 | Loss: 0.00243110
Iteration 10/25 | Loss: 0.00243110
Iteration 11/25 | Loss: 0.00243110
Iteration 12/25 | Loss: 0.00243110
Iteration 13/25 | Loss: 0.00243110
Iteration 14/25 | Loss: 0.00243110
Iteration 15/25 | Loss: 0.00243110
Iteration 16/25 | Loss: 0.00243110
Iteration 17/25 | Loss: 0.00243110
Iteration 18/25 | Loss: 0.00243110
Iteration 19/25 | Loss: 0.00243110
Iteration 20/25 | Loss: 0.00243110
Iteration 21/25 | Loss: 0.00243110
Iteration 22/25 | Loss: 0.00243110
Iteration 23/25 | Loss: 0.00243110
Iteration 24/25 | Loss: 0.00243110
Iteration 25/25 | Loss: 0.00243110

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243110
Iteration 2/1000 | Loss: 0.00037633
Iteration 3/1000 | Loss: 0.00026113
Iteration 4/1000 | Loss: 0.00022682
Iteration 5/1000 | Loss: 0.00020440
Iteration 6/1000 | Loss: 0.00038005
Iteration 7/1000 | Loss: 0.00112623
Iteration 8/1000 | Loss: 0.00032741
Iteration 9/1000 | Loss: 0.00017130
Iteration 10/1000 | Loss: 0.00028941
Iteration 11/1000 | Loss: 0.00012633
Iteration 12/1000 | Loss: 0.00021929
Iteration 13/1000 | Loss: 0.00009375
Iteration 14/1000 | Loss: 0.00008813
Iteration 15/1000 | Loss: 0.00008119
Iteration 16/1000 | Loss: 0.00007658
Iteration 17/1000 | Loss: 0.00007325
Iteration 18/1000 | Loss: 0.00007024
Iteration 19/1000 | Loss: 0.00006838
Iteration 20/1000 | Loss: 0.00006709
Iteration 21/1000 | Loss: 0.00038166
Iteration 22/1000 | Loss: 0.00006937
Iteration 23/1000 | Loss: 0.00006164
Iteration 24/1000 | Loss: 0.00006006
Iteration 25/1000 | Loss: 0.00005889
Iteration 26/1000 | Loss: 0.00005809
Iteration 27/1000 | Loss: 0.00005729
Iteration 28/1000 | Loss: 0.00005675
Iteration 29/1000 | Loss: 0.00005628
Iteration 30/1000 | Loss: 0.00005588
Iteration 31/1000 | Loss: 0.00005566
Iteration 32/1000 | Loss: 0.00005544
Iteration 33/1000 | Loss: 0.00005520
Iteration 34/1000 | Loss: 0.00092487
Iteration 35/1000 | Loss: 0.00163974
Iteration 36/1000 | Loss: 0.00155761
Iteration 37/1000 | Loss: 0.00141343
Iteration 38/1000 | Loss: 0.00148273
Iteration 39/1000 | Loss: 0.00009365
Iteration 40/1000 | Loss: 0.00006773
Iteration 41/1000 | Loss: 0.00005320
Iteration 42/1000 | Loss: 0.00004525
Iteration 43/1000 | Loss: 0.00003944
Iteration 44/1000 | Loss: 0.00003437
Iteration 45/1000 | Loss: 0.00003110
Iteration 46/1000 | Loss: 0.00002915
Iteration 47/1000 | Loss: 0.00002786
Iteration 48/1000 | Loss: 0.00002688
Iteration 49/1000 | Loss: 0.00002644
Iteration 50/1000 | Loss: 0.00002614
Iteration 51/1000 | Loss: 0.00002614
Iteration 52/1000 | Loss: 0.00002603
Iteration 53/1000 | Loss: 0.00002586
Iteration 54/1000 | Loss: 0.00002583
Iteration 55/1000 | Loss: 0.00002575
Iteration 56/1000 | Loss: 0.00002570
Iteration 57/1000 | Loss: 0.00002569
Iteration 58/1000 | Loss: 0.00002564
Iteration 59/1000 | Loss: 0.00002564
Iteration 60/1000 | Loss: 0.00002564
Iteration 61/1000 | Loss: 0.00002564
Iteration 62/1000 | Loss: 0.00002564
Iteration 63/1000 | Loss: 0.00002564
Iteration 64/1000 | Loss: 0.00002564
Iteration 65/1000 | Loss: 0.00002563
Iteration 66/1000 | Loss: 0.00002563
Iteration 67/1000 | Loss: 0.00002563
Iteration 68/1000 | Loss: 0.00002549
Iteration 69/1000 | Loss: 0.00002549
Iteration 70/1000 | Loss: 0.00002547
Iteration 71/1000 | Loss: 0.00002547
Iteration 72/1000 | Loss: 0.00002542
Iteration 73/1000 | Loss: 0.00002541
Iteration 74/1000 | Loss: 0.00002538
Iteration 75/1000 | Loss: 0.00002537
Iteration 76/1000 | Loss: 0.00002537
Iteration 77/1000 | Loss: 0.00002537
Iteration 78/1000 | Loss: 0.00002537
Iteration 79/1000 | Loss: 0.00002537
Iteration 80/1000 | Loss: 0.00002536
Iteration 81/1000 | Loss: 0.00002536
Iteration 82/1000 | Loss: 0.00002536
Iteration 83/1000 | Loss: 0.00002536
Iteration 84/1000 | Loss: 0.00002536
Iteration 85/1000 | Loss: 0.00002536
Iteration 86/1000 | Loss: 0.00002536
Iteration 87/1000 | Loss: 0.00002536
Iteration 88/1000 | Loss: 0.00002536
Iteration 89/1000 | Loss: 0.00002535
Iteration 90/1000 | Loss: 0.00002535
Iteration 91/1000 | Loss: 0.00002532
Iteration 92/1000 | Loss: 0.00002532
Iteration 93/1000 | Loss: 0.00002532
Iteration 94/1000 | Loss: 0.00002532
Iteration 95/1000 | Loss: 0.00002532
Iteration 96/1000 | Loss: 0.00002532
Iteration 97/1000 | Loss: 0.00002532
Iteration 98/1000 | Loss: 0.00002532
Iteration 99/1000 | Loss: 0.00002532
Iteration 100/1000 | Loss: 0.00002531
Iteration 101/1000 | Loss: 0.00002531
Iteration 102/1000 | Loss: 0.00002531
Iteration 103/1000 | Loss: 0.00002531
Iteration 104/1000 | Loss: 0.00002530
Iteration 105/1000 | Loss: 0.00002529
Iteration 106/1000 | Loss: 0.00002529
Iteration 107/1000 | Loss: 0.00002529
Iteration 108/1000 | Loss: 0.00002529
Iteration 109/1000 | Loss: 0.00002529
Iteration 110/1000 | Loss: 0.00002529
Iteration 111/1000 | Loss: 0.00002529
Iteration 112/1000 | Loss: 0.00002528
Iteration 113/1000 | Loss: 0.00002528
Iteration 114/1000 | Loss: 0.00002528
Iteration 115/1000 | Loss: 0.00002528
Iteration 116/1000 | Loss: 0.00002527
Iteration 117/1000 | Loss: 0.00002527
Iteration 118/1000 | Loss: 0.00002526
Iteration 119/1000 | Loss: 0.00002526
Iteration 120/1000 | Loss: 0.00002525
Iteration 121/1000 | Loss: 0.00002525
Iteration 122/1000 | Loss: 0.00002525
Iteration 123/1000 | Loss: 0.00002525
Iteration 124/1000 | Loss: 0.00002524
Iteration 125/1000 | Loss: 0.00002524
Iteration 126/1000 | Loss: 0.00002524
Iteration 127/1000 | Loss: 0.00002523
Iteration 128/1000 | Loss: 0.00002523
Iteration 129/1000 | Loss: 0.00002522
Iteration 130/1000 | Loss: 0.00002522
Iteration 131/1000 | Loss: 0.00002522
Iteration 132/1000 | Loss: 0.00002522
Iteration 133/1000 | Loss: 0.00002522
Iteration 134/1000 | Loss: 0.00002522
Iteration 135/1000 | Loss: 0.00002522
Iteration 136/1000 | Loss: 0.00002522
Iteration 137/1000 | Loss: 0.00002522
Iteration 138/1000 | Loss: 0.00002522
Iteration 139/1000 | Loss: 0.00002522
Iteration 140/1000 | Loss: 0.00002522
Iteration 141/1000 | Loss: 0.00002521
Iteration 142/1000 | Loss: 0.00002521
Iteration 143/1000 | Loss: 0.00002521
Iteration 144/1000 | Loss: 0.00002521
Iteration 145/1000 | Loss: 0.00002521
Iteration 146/1000 | Loss: 0.00002521
Iteration 147/1000 | Loss: 0.00002521
Iteration 148/1000 | Loss: 0.00002521
Iteration 149/1000 | Loss: 0.00002521
Iteration 150/1000 | Loss: 0.00002520
Iteration 151/1000 | Loss: 0.00002520
Iteration 152/1000 | Loss: 0.00002520
Iteration 153/1000 | Loss: 0.00002520
Iteration 154/1000 | Loss: 0.00002520
Iteration 155/1000 | Loss: 0.00002520
Iteration 156/1000 | Loss: 0.00002520
Iteration 157/1000 | Loss: 0.00002520
Iteration 158/1000 | Loss: 0.00002520
Iteration 159/1000 | Loss: 0.00002520
Iteration 160/1000 | Loss: 0.00002520
Iteration 161/1000 | Loss: 0.00002519
Iteration 162/1000 | Loss: 0.00002519
Iteration 163/1000 | Loss: 0.00002519
Iteration 164/1000 | Loss: 0.00002519
Iteration 165/1000 | Loss: 0.00002519
Iteration 166/1000 | Loss: 0.00002519
Iteration 167/1000 | Loss: 0.00002519
Iteration 168/1000 | Loss: 0.00002519
Iteration 169/1000 | Loss: 0.00002519
Iteration 170/1000 | Loss: 0.00002519
Iteration 171/1000 | Loss: 0.00002519
Iteration 172/1000 | Loss: 0.00002519
Iteration 173/1000 | Loss: 0.00002518
Iteration 174/1000 | Loss: 0.00002518
Iteration 175/1000 | Loss: 0.00002518
Iteration 176/1000 | Loss: 0.00002518
Iteration 177/1000 | Loss: 0.00002518
Iteration 178/1000 | Loss: 0.00002518
Iteration 179/1000 | Loss: 0.00002518
Iteration 180/1000 | Loss: 0.00002518
Iteration 181/1000 | Loss: 0.00002518
Iteration 182/1000 | Loss: 0.00002517
Iteration 183/1000 | Loss: 0.00002517
Iteration 184/1000 | Loss: 0.00002517
Iteration 185/1000 | Loss: 0.00002517
Iteration 186/1000 | Loss: 0.00002517
Iteration 187/1000 | Loss: 0.00002517
Iteration 188/1000 | Loss: 0.00002517
Iteration 189/1000 | Loss: 0.00002517
Iteration 190/1000 | Loss: 0.00002517
Iteration 191/1000 | Loss: 0.00002517
Iteration 192/1000 | Loss: 0.00002516
Iteration 193/1000 | Loss: 0.00002516
Iteration 194/1000 | Loss: 0.00002516
Iteration 195/1000 | Loss: 0.00002516
Iteration 196/1000 | Loss: 0.00002516
Iteration 197/1000 | Loss: 0.00002516
Iteration 198/1000 | Loss: 0.00002516
Iteration 199/1000 | Loss: 0.00002516
Iteration 200/1000 | Loss: 0.00002516
Iteration 201/1000 | Loss: 0.00002515
Iteration 202/1000 | Loss: 0.00002515
Iteration 203/1000 | Loss: 0.00002515
Iteration 204/1000 | Loss: 0.00002515
Iteration 205/1000 | Loss: 0.00002515
Iteration 206/1000 | Loss: 0.00002515
Iteration 207/1000 | Loss: 0.00002515
Iteration 208/1000 | Loss: 0.00002515
Iteration 209/1000 | Loss: 0.00002515
Iteration 210/1000 | Loss: 0.00002515
Iteration 211/1000 | Loss: 0.00002515
Iteration 212/1000 | Loss: 0.00002515
Iteration 213/1000 | Loss: 0.00002514
Iteration 214/1000 | Loss: 0.00002514
Iteration 215/1000 | Loss: 0.00002514
Iteration 216/1000 | Loss: 0.00002514
Iteration 217/1000 | Loss: 0.00002514
Iteration 218/1000 | Loss: 0.00002514
Iteration 219/1000 | Loss: 0.00002514
Iteration 220/1000 | Loss: 0.00002514
Iteration 221/1000 | Loss: 0.00002514
Iteration 222/1000 | Loss: 0.00002514
Iteration 223/1000 | Loss: 0.00002514
Iteration 224/1000 | Loss: 0.00002514
Iteration 225/1000 | Loss: 0.00002514
Iteration 226/1000 | Loss: 0.00002514
Iteration 227/1000 | Loss: 0.00002513
Iteration 228/1000 | Loss: 0.00002513
Iteration 229/1000 | Loss: 0.00002513
Iteration 230/1000 | Loss: 0.00002513
Iteration 231/1000 | Loss: 0.00002513
Iteration 232/1000 | Loss: 0.00002513
Iteration 233/1000 | Loss: 0.00002513
Iteration 234/1000 | Loss: 0.00002513
Iteration 235/1000 | Loss: 0.00002513
Iteration 236/1000 | Loss: 0.00002513
Iteration 237/1000 | Loss: 0.00002513
Iteration 238/1000 | Loss: 0.00002513
Iteration 239/1000 | Loss: 0.00002513
Iteration 240/1000 | Loss: 0.00002513
Iteration 241/1000 | Loss: 0.00002513
Iteration 242/1000 | Loss: 0.00002513
Iteration 243/1000 | Loss: 0.00002513
Iteration 244/1000 | Loss: 0.00002513
Iteration 245/1000 | Loss: 0.00002513
Iteration 246/1000 | Loss: 0.00002513
Iteration 247/1000 | Loss: 0.00002513
Iteration 248/1000 | Loss: 0.00002513
Iteration 249/1000 | Loss: 0.00002513
Iteration 250/1000 | Loss: 0.00002513
Iteration 251/1000 | Loss: 0.00002513
Iteration 252/1000 | Loss: 0.00002513
Iteration 253/1000 | Loss: 0.00002513
Iteration 254/1000 | Loss: 0.00002513
Iteration 255/1000 | Loss: 0.00002513
Iteration 256/1000 | Loss: 0.00002513
Iteration 257/1000 | Loss: 0.00002513
Iteration 258/1000 | Loss: 0.00002513
Iteration 259/1000 | Loss: 0.00002513
Iteration 260/1000 | Loss: 0.00002513
Iteration 261/1000 | Loss: 0.00002513
Iteration 262/1000 | Loss: 0.00002513
Iteration 263/1000 | Loss: 0.00002513
Iteration 264/1000 | Loss: 0.00002513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [2.5132103473879397e-05, 2.5132103473879397e-05, 2.5132103473879397e-05, 2.5132103473879397e-05, 2.5132103473879397e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5132103473879397e-05

Optimization complete. Final v2v error: 4.285406589508057 mm

Highest mean error: 4.777361869812012 mm for frame 100

Lowest mean error: 3.5428550243377686 mm for frame 9

Saving results

Total time: 117.542551279068
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00754529
Iteration 2/25 | Loss: 0.00143373
Iteration 3/25 | Loss: 0.00120616
Iteration 4/25 | Loss: 0.00115865
Iteration 5/25 | Loss: 0.00115219
Iteration 6/25 | Loss: 0.00114364
Iteration 7/25 | Loss: 0.00114440
Iteration 8/25 | Loss: 0.00114292
Iteration 9/25 | Loss: 0.00114193
Iteration 10/25 | Loss: 0.00114124
Iteration 11/25 | Loss: 0.00114085
Iteration 12/25 | Loss: 0.00114057
Iteration 13/25 | Loss: 0.00114049
Iteration 14/25 | Loss: 0.00114049
Iteration 15/25 | Loss: 0.00114048
Iteration 16/25 | Loss: 0.00114048
Iteration 17/25 | Loss: 0.00114048
Iteration 18/25 | Loss: 0.00114048
Iteration 19/25 | Loss: 0.00114048
Iteration 20/25 | Loss: 0.00114048
Iteration 21/25 | Loss: 0.00114048
Iteration 22/25 | Loss: 0.00114048
Iteration 23/25 | Loss: 0.00114048
Iteration 24/25 | Loss: 0.00114047
Iteration 25/25 | Loss: 0.00114047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78537536
Iteration 2/25 | Loss: 0.00116658
Iteration 3/25 | Loss: 0.00116658
Iteration 4/25 | Loss: 0.00116658
Iteration 5/25 | Loss: 0.00116658
Iteration 6/25 | Loss: 0.00116657
Iteration 7/25 | Loss: 0.00116657
Iteration 8/25 | Loss: 0.00116657
Iteration 9/25 | Loss: 0.00116657
Iteration 10/25 | Loss: 0.00116657
Iteration 11/25 | Loss: 0.00116657
Iteration 12/25 | Loss: 0.00116657
Iteration 13/25 | Loss: 0.00116657
Iteration 14/25 | Loss: 0.00116657
Iteration 15/25 | Loss: 0.00116657
Iteration 16/25 | Loss: 0.00116657
Iteration 17/25 | Loss: 0.00116657
Iteration 18/25 | Loss: 0.00116657
Iteration 19/25 | Loss: 0.00116657
Iteration 20/25 | Loss: 0.00116657
Iteration 21/25 | Loss: 0.00116657
Iteration 22/25 | Loss: 0.00116657
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001166574307717383, 0.001166574307717383, 0.001166574307717383, 0.001166574307717383, 0.001166574307717383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001166574307717383

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116657
Iteration 2/1000 | Loss: 0.00002699
Iteration 3/1000 | Loss: 0.00002087
Iteration 4/1000 | Loss: 0.00001879
Iteration 5/1000 | Loss: 0.00001787
Iteration 6/1000 | Loss: 0.00001705
Iteration 7/1000 | Loss: 0.00001664
Iteration 8/1000 | Loss: 0.00001619
Iteration 9/1000 | Loss: 0.00001592
Iteration 10/1000 | Loss: 0.00001580
Iteration 11/1000 | Loss: 0.00001578
Iteration 12/1000 | Loss: 0.00001577
Iteration 13/1000 | Loss: 0.00001573
Iteration 14/1000 | Loss: 0.00001568
Iteration 15/1000 | Loss: 0.00001563
Iteration 16/1000 | Loss: 0.00001562
Iteration 17/1000 | Loss: 0.00001551
Iteration 18/1000 | Loss: 0.00001548
Iteration 19/1000 | Loss: 0.00001547
Iteration 20/1000 | Loss: 0.00001545
Iteration 21/1000 | Loss: 0.00001545
Iteration 22/1000 | Loss: 0.00001545
Iteration 23/1000 | Loss: 0.00001545
Iteration 24/1000 | Loss: 0.00001545
Iteration 25/1000 | Loss: 0.00001544
Iteration 26/1000 | Loss: 0.00001543
Iteration 27/1000 | Loss: 0.00001540
Iteration 28/1000 | Loss: 0.00001540
Iteration 29/1000 | Loss: 0.00001540
Iteration 30/1000 | Loss: 0.00001539
Iteration 31/1000 | Loss: 0.00001539
Iteration 32/1000 | Loss: 0.00001539
Iteration 33/1000 | Loss: 0.00001538
Iteration 34/1000 | Loss: 0.00001538
Iteration 35/1000 | Loss: 0.00001537
Iteration 36/1000 | Loss: 0.00001537
Iteration 37/1000 | Loss: 0.00001537
Iteration 38/1000 | Loss: 0.00001537
Iteration 39/1000 | Loss: 0.00001537
Iteration 40/1000 | Loss: 0.00001536
Iteration 41/1000 | Loss: 0.00001536
Iteration 42/1000 | Loss: 0.00001536
Iteration 43/1000 | Loss: 0.00001536
Iteration 44/1000 | Loss: 0.00001536
Iteration 45/1000 | Loss: 0.00001536
Iteration 46/1000 | Loss: 0.00001536
Iteration 47/1000 | Loss: 0.00001535
Iteration 48/1000 | Loss: 0.00001534
Iteration 49/1000 | Loss: 0.00001534
Iteration 50/1000 | Loss: 0.00001533
Iteration 51/1000 | Loss: 0.00001533
Iteration 52/1000 | Loss: 0.00001533
Iteration 53/1000 | Loss: 0.00001533
Iteration 54/1000 | Loss: 0.00001533
Iteration 55/1000 | Loss: 0.00001533
Iteration 56/1000 | Loss: 0.00001532
Iteration 57/1000 | Loss: 0.00001532
Iteration 58/1000 | Loss: 0.00001532
Iteration 59/1000 | Loss: 0.00001532
Iteration 60/1000 | Loss: 0.00001531
Iteration 61/1000 | Loss: 0.00001530
Iteration 62/1000 | Loss: 0.00001530
Iteration 63/1000 | Loss: 0.00001530
Iteration 64/1000 | Loss: 0.00001530
Iteration 65/1000 | Loss: 0.00001530
Iteration 66/1000 | Loss: 0.00001530
Iteration 67/1000 | Loss: 0.00001530
Iteration 68/1000 | Loss: 0.00001530
Iteration 69/1000 | Loss: 0.00001529
Iteration 70/1000 | Loss: 0.00001529
Iteration 71/1000 | Loss: 0.00001529
Iteration 72/1000 | Loss: 0.00001529
Iteration 73/1000 | Loss: 0.00001529
Iteration 74/1000 | Loss: 0.00001529
Iteration 75/1000 | Loss: 0.00001529
Iteration 76/1000 | Loss: 0.00001529
Iteration 77/1000 | Loss: 0.00001529
Iteration 78/1000 | Loss: 0.00001529
Iteration 79/1000 | Loss: 0.00001529
Iteration 80/1000 | Loss: 0.00001529
Iteration 81/1000 | Loss: 0.00001529
Iteration 82/1000 | Loss: 0.00001529
Iteration 83/1000 | Loss: 0.00001529
Iteration 84/1000 | Loss: 0.00001529
Iteration 85/1000 | Loss: 0.00001529
Iteration 86/1000 | Loss: 0.00001529
Iteration 87/1000 | Loss: 0.00001529
Iteration 88/1000 | Loss: 0.00001529
Iteration 89/1000 | Loss: 0.00001529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.5291592717403546e-05, 1.5291592717403546e-05, 1.5291592717403546e-05, 1.5291592717403546e-05, 1.5291592717403546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5291592717403546e-05

Optimization complete. Final v2v error: 3.3329272270202637 mm

Highest mean error: 3.7261881828308105 mm for frame 141

Lowest mean error: 3.019442081451416 mm for frame 21

Saving results

Total time: 51.08945345878601
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854558
Iteration 2/25 | Loss: 0.00147448
Iteration 3/25 | Loss: 0.00122780
Iteration 4/25 | Loss: 0.00119611
Iteration 5/25 | Loss: 0.00119127
Iteration 6/25 | Loss: 0.00118994
Iteration 7/25 | Loss: 0.00118982
Iteration 8/25 | Loss: 0.00118982
Iteration 9/25 | Loss: 0.00118982
Iteration 10/25 | Loss: 0.00118982
Iteration 11/25 | Loss: 0.00118982
Iteration 12/25 | Loss: 0.00118982
Iteration 13/25 | Loss: 0.00118982
Iteration 14/25 | Loss: 0.00118982
Iteration 15/25 | Loss: 0.00118982
Iteration 16/25 | Loss: 0.00118982
Iteration 17/25 | Loss: 0.00118982
Iteration 18/25 | Loss: 0.00118982
Iteration 19/25 | Loss: 0.00118982
Iteration 20/25 | Loss: 0.00118982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011898220982402563, 0.0011898220982402563, 0.0011898220982402563, 0.0011898220982402563, 0.0011898220982402563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011898220982402563

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23937559
Iteration 2/25 | Loss: 0.00117672
Iteration 3/25 | Loss: 0.00117672
Iteration 4/25 | Loss: 0.00117672
Iteration 5/25 | Loss: 0.00117672
Iteration 6/25 | Loss: 0.00117672
Iteration 7/25 | Loss: 0.00117672
Iteration 8/25 | Loss: 0.00117672
Iteration 9/25 | Loss: 0.00117672
Iteration 10/25 | Loss: 0.00117672
Iteration 11/25 | Loss: 0.00117672
Iteration 12/25 | Loss: 0.00117672
Iteration 13/25 | Loss: 0.00117672
Iteration 14/25 | Loss: 0.00117672
Iteration 15/25 | Loss: 0.00117672
Iteration 16/25 | Loss: 0.00117672
Iteration 17/25 | Loss: 0.00117672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001176720019429922, 0.001176720019429922, 0.001176720019429922, 0.001176720019429922, 0.001176720019429922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001176720019429922

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117672
Iteration 2/1000 | Loss: 0.00005571
Iteration 3/1000 | Loss: 0.00003309
Iteration 4/1000 | Loss: 0.00002757
Iteration 5/1000 | Loss: 0.00002593
Iteration 6/1000 | Loss: 0.00002437
Iteration 7/1000 | Loss: 0.00002350
Iteration 8/1000 | Loss: 0.00002276
Iteration 9/1000 | Loss: 0.00002227
Iteration 10/1000 | Loss: 0.00002181
Iteration 11/1000 | Loss: 0.00002151
Iteration 12/1000 | Loss: 0.00002133
Iteration 13/1000 | Loss: 0.00002131
Iteration 14/1000 | Loss: 0.00002127
Iteration 15/1000 | Loss: 0.00002127
Iteration 16/1000 | Loss: 0.00002125
Iteration 17/1000 | Loss: 0.00002120
Iteration 18/1000 | Loss: 0.00002119
Iteration 19/1000 | Loss: 0.00002119
Iteration 20/1000 | Loss: 0.00002118
Iteration 21/1000 | Loss: 0.00002117
Iteration 22/1000 | Loss: 0.00002116
Iteration 23/1000 | Loss: 0.00002115
Iteration 24/1000 | Loss: 0.00002115
Iteration 25/1000 | Loss: 0.00002115
Iteration 26/1000 | Loss: 0.00002115
Iteration 27/1000 | Loss: 0.00002114
Iteration 28/1000 | Loss: 0.00002114
Iteration 29/1000 | Loss: 0.00002114
Iteration 30/1000 | Loss: 0.00002114
Iteration 31/1000 | Loss: 0.00002114
Iteration 32/1000 | Loss: 0.00002114
Iteration 33/1000 | Loss: 0.00002114
Iteration 34/1000 | Loss: 0.00002114
Iteration 35/1000 | Loss: 0.00002113
Iteration 36/1000 | Loss: 0.00002113
Iteration 37/1000 | Loss: 0.00002113
Iteration 38/1000 | Loss: 0.00002113
Iteration 39/1000 | Loss: 0.00002113
Iteration 40/1000 | Loss: 0.00002113
Iteration 41/1000 | Loss: 0.00002112
Iteration 42/1000 | Loss: 0.00002111
Iteration 43/1000 | Loss: 0.00002111
Iteration 44/1000 | Loss: 0.00002111
Iteration 45/1000 | Loss: 0.00002111
Iteration 46/1000 | Loss: 0.00002110
Iteration 47/1000 | Loss: 0.00002110
Iteration 48/1000 | Loss: 0.00002109
Iteration 49/1000 | Loss: 0.00002109
Iteration 50/1000 | Loss: 0.00002109
Iteration 51/1000 | Loss: 0.00002109
Iteration 52/1000 | Loss: 0.00002108
Iteration 53/1000 | Loss: 0.00002108
Iteration 54/1000 | Loss: 0.00002108
Iteration 55/1000 | Loss: 0.00002108
Iteration 56/1000 | Loss: 0.00002108
Iteration 57/1000 | Loss: 0.00002108
Iteration 58/1000 | Loss: 0.00002107
Iteration 59/1000 | Loss: 0.00002107
Iteration 60/1000 | Loss: 0.00002107
Iteration 61/1000 | Loss: 0.00002107
Iteration 62/1000 | Loss: 0.00002107
Iteration 63/1000 | Loss: 0.00002107
Iteration 64/1000 | Loss: 0.00002107
Iteration 65/1000 | Loss: 0.00002107
Iteration 66/1000 | Loss: 0.00002106
Iteration 67/1000 | Loss: 0.00002106
Iteration 68/1000 | Loss: 0.00002106
Iteration 69/1000 | Loss: 0.00002105
Iteration 70/1000 | Loss: 0.00002105
Iteration 71/1000 | Loss: 0.00002105
Iteration 72/1000 | Loss: 0.00002105
Iteration 73/1000 | Loss: 0.00002104
Iteration 74/1000 | Loss: 0.00002104
Iteration 75/1000 | Loss: 0.00002104
Iteration 76/1000 | Loss: 0.00002104
Iteration 77/1000 | Loss: 0.00002104
Iteration 78/1000 | Loss: 0.00002104
Iteration 79/1000 | Loss: 0.00002104
Iteration 80/1000 | Loss: 0.00002104
Iteration 81/1000 | Loss: 0.00002103
Iteration 82/1000 | Loss: 0.00002103
Iteration 83/1000 | Loss: 0.00002103
Iteration 84/1000 | Loss: 0.00002103
Iteration 85/1000 | Loss: 0.00002102
Iteration 86/1000 | Loss: 0.00002102
Iteration 87/1000 | Loss: 0.00002102
Iteration 88/1000 | Loss: 0.00002102
Iteration 89/1000 | Loss: 0.00002102
Iteration 90/1000 | Loss: 0.00002102
Iteration 91/1000 | Loss: 0.00002102
Iteration 92/1000 | Loss: 0.00002102
Iteration 93/1000 | Loss: 0.00002102
Iteration 94/1000 | Loss: 0.00002102
Iteration 95/1000 | Loss: 0.00002102
Iteration 96/1000 | Loss: 0.00002102
Iteration 97/1000 | Loss: 0.00002101
Iteration 98/1000 | Loss: 0.00002101
Iteration 99/1000 | Loss: 0.00002101
Iteration 100/1000 | Loss: 0.00002101
Iteration 101/1000 | Loss: 0.00002101
Iteration 102/1000 | Loss: 0.00002101
Iteration 103/1000 | Loss: 0.00002101
Iteration 104/1000 | Loss: 0.00002101
Iteration 105/1000 | Loss: 0.00002101
Iteration 106/1000 | Loss: 0.00002101
Iteration 107/1000 | Loss: 0.00002101
Iteration 108/1000 | Loss: 0.00002101
Iteration 109/1000 | Loss: 0.00002101
Iteration 110/1000 | Loss: 0.00002100
Iteration 111/1000 | Loss: 0.00002100
Iteration 112/1000 | Loss: 0.00002100
Iteration 113/1000 | Loss: 0.00002100
Iteration 114/1000 | Loss: 0.00002100
Iteration 115/1000 | Loss: 0.00002100
Iteration 116/1000 | Loss: 0.00002100
Iteration 117/1000 | Loss: 0.00002100
Iteration 118/1000 | Loss: 0.00002100
Iteration 119/1000 | Loss: 0.00002100
Iteration 120/1000 | Loss: 0.00002100
Iteration 121/1000 | Loss: 0.00002100
Iteration 122/1000 | Loss: 0.00002100
Iteration 123/1000 | Loss: 0.00002100
Iteration 124/1000 | Loss: 0.00002100
Iteration 125/1000 | Loss: 0.00002100
Iteration 126/1000 | Loss: 0.00002100
Iteration 127/1000 | Loss: 0.00002100
Iteration 128/1000 | Loss: 0.00002100
Iteration 129/1000 | Loss: 0.00002100
Iteration 130/1000 | Loss: 0.00002100
Iteration 131/1000 | Loss: 0.00002100
Iteration 132/1000 | Loss: 0.00002100
Iteration 133/1000 | Loss: 0.00002100
Iteration 134/1000 | Loss: 0.00002100
Iteration 135/1000 | Loss: 0.00002100
Iteration 136/1000 | Loss: 0.00002100
Iteration 137/1000 | Loss: 0.00002100
Iteration 138/1000 | Loss: 0.00002100
Iteration 139/1000 | Loss: 0.00002100
Iteration 140/1000 | Loss: 0.00002100
Iteration 141/1000 | Loss: 0.00002100
Iteration 142/1000 | Loss: 0.00002100
Iteration 143/1000 | Loss: 0.00002100
Iteration 144/1000 | Loss: 0.00002100
Iteration 145/1000 | Loss: 0.00002100
Iteration 146/1000 | Loss: 0.00002100
Iteration 147/1000 | Loss: 0.00002100
Iteration 148/1000 | Loss: 0.00002100
Iteration 149/1000 | Loss: 0.00002100
Iteration 150/1000 | Loss: 0.00002100
Iteration 151/1000 | Loss: 0.00002100
Iteration 152/1000 | Loss: 0.00002100
Iteration 153/1000 | Loss: 0.00002100
Iteration 154/1000 | Loss: 0.00002100
Iteration 155/1000 | Loss: 0.00002100
Iteration 156/1000 | Loss: 0.00002100
Iteration 157/1000 | Loss: 0.00002100
Iteration 158/1000 | Loss: 0.00002100
Iteration 159/1000 | Loss: 0.00002100
Iteration 160/1000 | Loss: 0.00002100
Iteration 161/1000 | Loss: 0.00002100
Iteration 162/1000 | Loss: 0.00002100
Iteration 163/1000 | Loss: 0.00002100
Iteration 164/1000 | Loss: 0.00002100
Iteration 165/1000 | Loss: 0.00002100
Iteration 166/1000 | Loss: 0.00002100
Iteration 167/1000 | Loss: 0.00002100
Iteration 168/1000 | Loss: 0.00002100
Iteration 169/1000 | Loss: 0.00002100
Iteration 170/1000 | Loss: 0.00002100
Iteration 171/1000 | Loss: 0.00002100
Iteration 172/1000 | Loss: 0.00002100
Iteration 173/1000 | Loss: 0.00002100
Iteration 174/1000 | Loss: 0.00002100
Iteration 175/1000 | Loss: 0.00002100
Iteration 176/1000 | Loss: 0.00002100
Iteration 177/1000 | Loss: 0.00002100
Iteration 178/1000 | Loss: 0.00002100
Iteration 179/1000 | Loss: 0.00002100
Iteration 180/1000 | Loss: 0.00002100
Iteration 181/1000 | Loss: 0.00002100
Iteration 182/1000 | Loss: 0.00002100
Iteration 183/1000 | Loss: 0.00002100
Iteration 184/1000 | Loss: 0.00002100
Iteration 185/1000 | Loss: 0.00002100
Iteration 186/1000 | Loss: 0.00002100
Iteration 187/1000 | Loss: 0.00002100
Iteration 188/1000 | Loss: 0.00002100
Iteration 189/1000 | Loss: 0.00002100
Iteration 190/1000 | Loss: 0.00002100
Iteration 191/1000 | Loss: 0.00002100
Iteration 192/1000 | Loss: 0.00002100
Iteration 193/1000 | Loss: 0.00002100
Iteration 194/1000 | Loss: 0.00002100
Iteration 195/1000 | Loss: 0.00002100
Iteration 196/1000 | Loss: 0.00002100
Iteration 197/1000 | Loss: 0.00002100
Iteration 198/1000 | Loss: 0.00002100
Iteration 199/1000 | Loss: 0.00002100
Iteration 200/1000 | Loss: 0.00002100
Iteration 201/1000 | Loss: 0.00002100
Iteration 202/1000 | Loss: 0.00002100
Iteration 203/1000 | Loss: 0.00002100
Iteration 204/1000 | Loss: 0.00002100
Iteration 205/1000 | Loss: 0.00002100
Iteration 206/1000 | Loss: 0.00002100
Iteration 207/1000 | Loss: 0.00002100
Iteration 208/1000 | Loss: 0.00002100
Iteration 209/1000 | Loss: 0.00002100
Iteration 210/1000 | Loss: 0.00002100
Iteration 211/1000 | Loss: 0.00002100
Iteration 212/1000 | Loss: 0.00002100
Iteration 213/1000 | Loss: 0.00002100
Iteration 214/1000 | Loss: 0.00002100
Iteration 215/1000 | Loss: 0.00002100
Iteration 216/1000 | Loss: 0.00002100
Iteration 217/1000 | Loss: 0.00002100
Iteration 218/1000 | Loss: 0.00002100
Iteration 219/1000 | Loss: 0.00002100
Iteration 220/1000 | Loss: 0.00002100
Iteration 221/1000 | Loss: 0.00002100
Iteration 222/1000 | Loss: 0.00002100
Iteration 223/1000 | Loss: 0.00002100
Iteration 224/1000 | Loss: 0.00002100
Iteration 225/1000 | Loss: 0.00002100
Iteration 226/1000 | Loss: 0.00002100
Iteration 227/1000 | Loss: 0.00002100
Iteration 228/1000 | Loss: 0.00002100
Iteration 229/1000 | Loss: 0.00002100
Iteration 230/1000 | Loss: 0.00002100
Iteration 231/1000 | Loss: 0.00002100
Iteration 232/1000 | Loss: 0.00002100
Iteration 233/1000 | Loss: 0.00002100
Iteration 234/1000 | Loss: 0.00002100
Iteration 235/1000 | Loss: 0.00002100
Iteration 236/1000 | Loss: 0.00002100
Iteration 237/1000 | Loss: 0.00002100
Iteration 238/1000 | Loss: 0.00002100
Iteration 239/1000 | Loss: 0.00002100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [2.099987250403501e-05, 2.099987250403501e-05, 2.099987250403501e-05, 2.099987250403501e-05, 2.099987250403501e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.099987250403501e-05

Optimization complete. Final v2v error: 3.782618522644043 mm

Highest mean error: 4.10895299911499 mm for frame 88

Lowest mean error: 3.336945056915283 mm for frame 4

Saving results

Total time: 37.231237173080444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428860
Iteration 2/25 | Loss: 0.00137219
Iteration 3/25 | Loss: 0.00121167
Iteration 4/25 | Loss: 0.00118313
Iteration 5/25 | Loss: 0.00117472
Iteration 6/25 | Loss: 0.00117202
Iteration 7/25 | Loss: 0.00117144
Iteration 8/25 | Loss: 0.00117134
Iteration 9/25 | Loss: 0.00117134
Iteration 10/25 | Loss: 0.00117134
Iteration 11/25 | Loss: 0.00117134
Iteration 12/25 | Loss: 0.00117134
Iteration 13/25 | Loss: 0.00117134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011713390704244375, 0.0011713390704244375, 0.0011713390704244375, 0.0011713390704244375, 0.0011713390704244375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011713390704244375

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33042967
Iteration 2/25 | Loss: 0.00113683
Iteration 3/25 | Loss: 0.00113682
Iteration 4/25 | Loss: 0.00113682
Iteration 5/25 | Loss: 0.00113682
Iteration 6/25 | Loss: 0.00113682
Iteration 7/25 | Loss: 0.00113682
Iteration 8/25 | Loss: 0.00113682
Iteration 9/25 | Loss: 0.00113682
Iteration 10/25 | Loss: 0.00113682
Iteration 11/25 | Loss: 0.00113682
Iteration 12/25 | Loss: 0.00113682
Iteration 13/25 | Loss: 0.00113682
Iteration 14/25 | Loss: 0.00113682
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001136819482780993, 0.001136819482780993, 0.001136819482780993, 0.001136819482780993, 0.001136819482780993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001136819482780993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113682
Iteration 2/1000 | Loss: 0.00004875
Iteration 3/1000 | Loss: 0.00002739
Iteration 4/1000 | Loss: 0.00002218
Iteration 5/1000 | Loss: 0.00001989
Iteration 6/1000 | Loss: 0.00001878
Iteration 7/1000 | Loss: 0.00001796
Iteration 8/1000 | Loss: 0.00001749
Iteration 9/1000 | Loss: 0.00001719
Iteration 10/1000 | Loss: 0.00001697
Iteration 11/1000 | Loss: 0.00001682
Iteration 12/1000 | Loss: 0.00001673
Iteration 13/1000 | Loss: 0.00001663
Iteration 14/1000 | Loss: 0.00001649
Iteration 15/1000 | Loss: 0.00001646
Iteration 16/1000 | Loss: 0.00001643
Iteration 17/1000 | Loss: 0.00001642
Iteration 18/1000 | Loss: 0.00001641
Iteration 19/1000 | Loss: 0.00001640
Iteration 20/1000 | Loss: 0.00001635
Iteration 21/1000 | Loss: 0.00001635
Iteration 22/1000 | Loss: 0.00001634
Iteration 23/1000 | Loss: 0.00001633
Iteration 24/1000 | Loss: 0.00001633
Iteration 25/1000 | Loss: 0.00001632
Iteration 26/1000 | Loss: 0.00001632
Iteration 27/1000 | Loss: 0.00001632
Iteration 28/1000 | Loss: 0.00001632
Iteration 29/1000 | Loss: 0.00001631
Iteration 30/1000 | Loss: 0.00001631
Iteration 31/1000 | Loss: 0.00001630
Iteration 32/1000 | Loss: 0.00001630
Iteration 33/1000 | Loss: 0.00001630
Iteration 34/1000 | Loss: 0.00001630
Iteration 35/1000 | Loss: 0.00001629
Iteration 36/1000 | Loss: 0.00001629
Iteration 37/1000 | Loss: 0.00001628
Iteration 38/1000 | Loss: 0.00001628
Iteration 39/1000 | Loss: 0.00001627
Iteration 40/1000 | Loss: 0.00001627
Iteration 41/1000 | Loss: 0.00001627
Iteration 42/1000 | Loss: 0.00001626
Iteration 43/1000 | Loss: 0.00001626
Iteration 44/1000 | Loss: 0.00001626
Iteration 45/1000 | Loss: 0.00001626
Iteration 46/1000 | Loss: 0.00001626
Iteration 47/1000 | Loss: 0.00001625
Iteration 48/1000 | Loss: 0.00001625
Iteration 49/1000 | Loss: 0.00001625
Iteration 50/1000 | Loss: 0.00001625
Iteration 51/1000 | Loss: 0.00001624
Iteration 52/1000 | Loss: 0.00001624
Iteration 53/1000 | Loss: 0.00001624
Iteration 54/1000 | Loss: 0.00001624
Iteration 55/1000 | Loss: 0.00001624
Iteration 56/1000 | Loss: 0.00001624
Iteration 57/1000 | Loss: 0.00001624
Iteration 58/1000 | Loss: 0.00001623
Iteration 59/1000 | Loss: 0.00001623
Iteration 60/1000 | Loss: 0.00001622
Iteration 61/1000 | Loss: 0.00001622
Iteration 62/1000 | Loss: 0.00001622
Iteration 63/1000 | Loss: 0.00001622
Iteration 64/1000 | Loss: 0.00001622
Iteration 65/1000 | Loss: 0.00001622
Iteration 66/1000 | Loss: 0.00001622
Iteration 67/1000 | Loss: 0.00001622
Iteration 68/1000 | Loss: 0.00001622
Iteration 69/1000 | Loss: 0.00001622
Iteration 70/1000 | Loss: 0.00001622
Iteration 71/1000 | Loss: 0.00001621
Iteration 72/1000 | Loss: 0.00001621
Iteration 73/1000 | Loss: 0.00001621
Iteration 74/1000 | Loss: 0.00001621
Iteration 75/1000 | Loss: 0.00001620
Iteration 76/1000 | Loss: 0.00001620
Iteration 77/1000 | Loss: 0.00001620
Iteration 78/1000 | Loss: 0.00001620
Iteration 79/1000 | Loss: 0.00001620
Iteration 80/1000 | Loss: 0.00001619
Iteration 81/1000 | Loss: 0.00001619
Iteration 82/1000 | Loss: 0.00001619
Iteration 83/1000 | Loss: 0.00001619
Iteration 84/1000 | Loss: 0.00001619
Iteration 85/1000 | Loss: 0.00001619
Iteration 86/1000 | Loss: 0.00001619
Iteration 87/1000 | Loss: 0.00001619
Iteration 88/1000 | Loss: 0.00001619
Iteration 89/1000 | Loss: 0.00001619
Iteration 90/1000 | Loss: 0.00001619
Iteration 91/1000 | Loss: 0.00001618
Iteration 92/1000 | Loss: 0.00001618
Iteration 93/1000 | Loss: 0.00001618
Iteration 94/1000 | Loss: 0.00001618
Iteration 95/1000 | Loss: 0.00001618
Iteration 96/1000 | Loss: 0.00001618
Iteration 97/1000 | Loss: 0.00001618
Iteration 98/1000 | Loss: 0.00001618
Iteration 99/1000 | Loss: 0.00001618
Iteration 100/1000 | Loss: 0.00001618
Iteration 101/1000 | Loss: 0.00001618
Iteration 102/1000 | Loss: 0.00001618
Iteration 103/1000 | Loss: 0.00001618
Iteration 104/1000 | Loss: 0.00001617
Iteration 105/1000 | Loss: 0.00001617
Iteration 106/1000 | Loss: 0.00001617
Iteration 107/1000 | Loss: 0.00001617
Iteration 108/1000 | Loss: 0.00001617
Iteration 109/1000 | Loss: 0.00001617
Iteration 110/1000 | Loss: 0.00001617
Iteration 111/1000 | Loss: 0.00001617
Iteration 112/1000 | Loss: 0.00001617
Iteration 113/1000 | Loss: 0.00001617
Iteration 114/1000 | Loss: 0.00001617
Iteration 115/1000 | Loss: 0.00001616
Iteration 116/1000 | Loss: 0.00001616
Iteration 117/1000 | Loss: 0.00001616
Iteration 118/1000 | Loss: 0.00001616
Iteration 119/1000 | Loss: 0.00001616
Iteration 120/1000 | Loss: 0.00001616
Iteration 121/1000 | Loss: 0.00001616
Iteration 122/1000 | Loss: 0.00001615
Iteration 123/1000 | Loss: 0.00001615
Iteration 124/1000 | Loss: 0.00001615
Iteration 125/1000 | Loss: 0.00001614
Iteration 126/1000 | Loss: 0.00001614
Iteration 127/1000 | Loss: 0.00001614
Iteration 128/1000 | Loss: 0.00001614
Iteration 129/1000 | Loss: 0.00001613
Iteration 130/1000 | Loss: 0.00001613
Iteration 131/1000 | Loss: 0.00001613
Iteration 132/1000 | Loss: 0.00001613
Iteration 133/1000 | Loss: 0.00001613
Iteration 134/1000 | Loss: 0.00001612
Iteration 135/1000 | Loss: 0.00001612
Iteration 136/1000 | Loss: 0.00001612
Iteration 137/1000 | Loss: 0.00001612
Iteration 138/1000 | Loss: 0.00001611
Iteration 139/1000 | Loss: 0.00001611
Iteration 140/1000 | Loss: 0.00001611
Iteration 141/1000 | Loss: 0.00001611
Iteration 142/1000 | Loss: 0.00001611
Iteration 143/1000 | Loss: 0.00001611
Iteration 144/1000 | Loss: 0.00001611
Iteration 145/1000 | Loss: 0.00001611
Iteration 146/1000 | Loss: 0.00001611
Iteration 147/1000 | Loss: 0.00001610
Iteration 148/1000 | Loss: 0.00001610
Iteration 149/1000 | Loss: 0.00001610
Iteration 150/1000 | Loss: 0.00001610
Iteration 151/1000 | Loss: 0.00001610
Iteration 152/1000 | Loss: 0.00001610
Iteration 153/1000 | Loss: 0.00001610
Iteration 154/1000 | Loss: 0.00001610
Iteration 155/1000 | Loss: 0.00001610
Iteration 156/1000 | Loss: 0.00001610
Iteration 157/1000 | Loss: 0.00001610
Iteration 158/1000 | Loss: 0.00001610
Iteration 159/1000 | Loss: 0.00001609
Iteration 160/1000 | Loss: 0.00001609
Iteration 161/1000 | Loss: 0.00001609
Iteration 162/1000 | Loss: 0.00001609
Iteration 163/1000 | Loss: 0.00001609
Iteration 164/1000 | Loss: 0.00001609
Iteration 165/1000 | Loss: 0.00001609
Iteration 166/1000 | Loss: 0.00001609
Iteration 167/1000 | Loss: 0.00001609
Iteration 168/1000 | Loss: 0.00001609
Iteration 169/1000 | Loss: 0.00001609
Iteration 170/1000 | Loss: 0.00001609
Iteration 171/1000 | Loss: 0.00001609
Iteration 172/1000 | Loss: 0.00001609
Iteration 173/1000 | Loss: 0.00001609
Iteration 174/1000 | Loss: 0.00001609
Iteration 175/1000 | Loss: 0.00001609
Iteration 176/1000 | Loss: 0.00001609
Iteration 177/1000 | Loss: 0.00001609
Iteration 178/1000 | Loss: 0.00001609
Iteration 179/1000 | Loss: 0.00001608
Iteration 180/1000 | Loss: 0.00001608
Iteration 181/1000 | Loss: 0.00001608
Iteration 182/1000 | Loss: 0.00001608
Iteration 183/1000 | Loss: 0.00001608
Iteration 184/1000 | Loss: 0.00001608
Iteration 185/1000 | Loss: 0.00001608
Iteration 186/1000 | Loss: 0.00001608
Iteration 187/1000 | Loss: 0.00001608
Iteration 188/1000 | Loss: 0.00001608
Iteration 189/1000 | Loss: 0.00001608
Iteration 190/1000 | Loss: 0.00001608
Iteration 191/1000 | Loss: 0.00001608
Iteration 192/1000 | Loss: 0.00001608
Iteration 193/1000 | Loss: 0.00001608
Iteration 194/1000 | Loss: 0.00001608
Iteration 195/1000 | Loss: 0.00001608
Iteration 196/1000 | Loss: 0.00001608
Iteration 197/1000 | Loss: 0.00001607
Iteration 198/1000 | Loss: 0.00001607
Iteration 199/1000 | Loss: 0.00001607
Iteration 200/1000 | Loss: 0.00001607
Iteration 201/1000 | Loss: 0.00001607
Iteration 202/1000 | Loss: 0.00001607
Iteration 203/1000 | Loss: 0.00001607
Iteration 204/1000 | Loss: 0.00001607
Iteration 205/1000 | Loss: 0.00001607
Iteration 206/1000 | Loss: 0.00001607
Iteration 207/1000 | Loss: 0.00001607
Iteration 208/1000 | Loss: 0.00001607
Iteration 209/1000 | Loss: 0.00001607
Iteration 210/1000 | Loss: 0.00001607
Iteration 211/1000 | Loss: 0.00001607
Iteration 212/1000 | Loss: 0.00001607
Iteration 213/1000 | Loss: 0.00001607
Iteration 214/1000 | Loss: 0.00001607
Iteration 215/1000 | Loss: 0.00001607
Iteration 216/1000 | Loss: 0.00001607
Iteration 217/1000 | Loss: 0.00001607
Iteration 218/1000 | Loss: 0.00001607
Iteration 219/1000 | Loss: 0.00001607
Iteration 220/1000 | Loss: 0.00001607
Iteration 221/1000 | Loss: 0.00001607
Iteration 222/1000 | Loss: 0.00001607
Iteration 223/1000 | Loss: 0.00001607
Iteration 224/1000 | Loss: 0.00001607
Iteration 225/1000 | Loss: 0.00001607
Iteration 226/1000 | Loss: 0.00001607
Iteration 227/1000 | Loss: 0.00001607
Iteration 228/1000 | Loss: 0.00001607
Iteration 229/1000 | Loss: 0.00001607
Iteration 230/1000 | Loss: 0.00001607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.6066074749687687e-05, 1.6066074749687687e-05, 1.6066074749687687e-05, 1.6066074749687687e-05, 1.6066074749687687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6066074749687687e-05

Optimization complete. Final v2v error: 3.3849427700042725 mm

Highest mean error: 4.269540786743164 mm for frame 44

Lowest mean error: 2.949977397918701 mm for frame 89

Saving results

Total time: 42.66313672065735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00623365
Iteration 2/25 | Loss: 0.00127196
Iteration 3/25 | Loss: 0.00117180
Iteration 4/25 | Loss: 0.00115747
Iteration 5/25 | Loss: 0.00115161
Iteration 6/25 | Loss: 0.00115023
Iteration 7/25 | Loss: 0.00115023
Iteration 8/25 | Loss: 0.00115023
Iteration 9/25 | Loss: 0.00115023
Iteration 10/25 | Loss: 0.00115023
Iteration 11/25 | Loss: 0.00115023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011502283159643412, 0.0011502283159643412, 0.0011502283159643412, 0.0011502283159643412, 0.0011502283159643412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011502283159643412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61918116
Iteration 2/25 | Loss: 0.00117679
Iteration 3/25 | Loss: 0.00117678
Iteration 4/25 | Loss: 0.00117678
Iteration 5/25 | Loss: 0.00117678
Iteration 6/25 | Loss: 0.00117678
Iteration 7/25 | Loss: 0.00117678
Iteration 8/25 | Loss: 0.00117678
Iteration 9/25 | Loss: 0.00117678
Iteration 10/25 | Loss: 0.00117678
Iteration 11/25 | Loss: 0.00117678
Iteration 12/25 | Loss: 0.00117678
Iteration 13/25 | Loss: 0.00117678
Iteration 14/25 | Loss: 0.00117678
Iteration 15/25 | Loss: 0.00117678
Iteration 16/25 | Loss: 0.00117678
Iteration 17/25 | Loss: 0.00117678
Iteration 18/25 | Loss: 0.00117678
Iteration 19/25 | Loss: 0.00117678
Iteration 20/25 | Loss: 0.00117678
Iteration 21/25 | Loss: 0.00117678
Iteration 22/25 | Loss: 0.00117678
Iteration 23/25 | Loss: 0.00117678
Iteration 24/25 | Loss: 0.00117678
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011767821852117777, 0.0011767821852117777, 0.0011767821852117777, 0.0011767821852117777, 0.0011767821852117777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011767821852117777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117678
Iteration 2/1000 | Loss: 0.00003079
Iteration 3/1000 | Loss: 0.00001938
Iteration 4/1000 | Loss: 0.00001759
Iteration 5/1000 | Loss: 0.00001638
Iteration 6/1000 | Loss: 0.00001592
Iteration 7/1000 | Loss: 0.00001565
Iteration 8/1000 | Loss: 0.00001538
Iteration 9/1000 | Loss: 0.00001522
Iteration 10/1000 | Loss: 0.00001521
Iteration 11/1000 | Loss: 0.00001516
Iteration 12/1000 | Loss: 0.00001513
Iteration 13/1000 | Loss: 0.00001512
Iteration 14/1000 | Loss: 0.00001511
Iteration 15/1000 | Loss: 0.00001505
Iteration 16/1000 | Loss: 0.00001504
Iteration 17/1000 | Loss: 0.00001503
Iteration 18/1000 | Loss: 0.00001503
Iteration 19/1000 | Loss: 0.00001502
Iteration 20/1000 | Loss: 0.00001502
Iteration 21/1000 | Loss: 0.00001502
Iteration 22/1000 | Loss: 0.00001500
Iteration 23/1000 | Loss: 0.00001500
Iteration 24/1000 | Loss: 0.00001500
Iteration 25/1000 | Loss: 0.00001499
Iteration 26/1000 | Loss: 0.00001499
Iteration 27/1000 | Loss: 0.00001499
Iteration 28/1000 | Loss: 0.00001499
Iteration 29/1000 | Loss: 0.00001499
Iteration 30/1000 | Loss: 0.00001499
Iteration 31/1000 | Loss: 0.00001499
Iteration 32/1000 | Loss: 0.00001499
Iteration 33/1000 | Loss: 0.00001499
Iteration 34/1000 | Loss: 0.00001498
Iteration 35/1000 | Loss: 0.00001497
Iteration 36/1000 | Loss: 0.00001497
Iteration 37/1000 | Loss: 0.00001496
Iteration 38/1000 | Loss: 0.00001495
Iteration 39/1000 | Loss: 0.00001495
Iteration 40/1000 | Loss: 0.00001495
Iteration 41/1000 | Loss: 0.00001495
Iteration 42/1000 | Loss: 0.00001494
Iteration 43/1000 | Loss: 0.00001494
Iteration 44/1000 | Loss: 0.00001494
Iteration 45/1000 | Loss: 0.00001494
Iteration 46/1000 | Loss: 0.00001494
Iteration 47/1000 | Loss: 0.00001494
Iteration 48/1000 | Loss: 0.00001494
Iteration 49/1000 | Loss: 0.00001494
Iteration 50/1000 | Loss: 0.00001493
Iteration 51/1000 | Loss: 0.00001493
Iteration 52/1000 | Loss: 0.00001493
Iteration 53/1000 | Loss: 0.00001493
Iteration 54/1000 | Loss: 0.00001493
Iteration 55/1000 | Loss: 0.00001493
Iteration 56/1000 | Loss: 0.00001493
Iteration 57/1000 | Loss: 0.00001493
Iteration 58/1000 | Loss: 0.00001493
Iteration 59/1000 | Loss: 0.00001493
Iteration 60/1000 | Loss: 0.00001493
Iteration 61/1000 | Loss: 0.00001493
Iteration 62/1000 | Loss: 0.00001493
Iteration 63/1000 | Loss: 0.00001493
Iteration 64/1000 | Loss: 0.00001492
Iteration 65/1000 | Loss: 0.00001492
Iteration 66/1000 | Loss: 0.00001492
Iteration 67/1000 | Loss: 0.00001491
Iteration 68/1000 | Loss: 0.00001491
Iteration 69/1000 | Loss: 0.00001490
Iteration 70/1000 | Loss: 0.00001490
Iteration 71/1000 | Loss: 0.00001490
Iteration 72/1000 | Loss: 0.00001489
Iteration 73/1000 | Loss: 0.00001489
Iteration 74/1000 | Loss: 0.00001489
Iteration 75/1000 | Loss: 0.00001489
Iteration 76/1000 | Loss: 0.00001489
Iteration 77/1000 | Loss: 0.00001489
Iteration 78/1000 | Loss: 0.00001489
Iteration 79/1000 | Loss: 0.00001489
Iteration 80/1000 | Loss: 0.00001489
Iteration 81/1000 | Loss: 0.00001489
Iteration 82/1000 | Loss: 0.00001489
Iteration 83/1000 | Loss: 0.00001488
Iteration 84/1000 | Loss: 0.00001488
Iteration 85/1000 | Loss: 0.00001487
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00001487
Iteration 88/1000 | Loss: 0.00001486
Iteration 89/1000 | Loss: 0.00001486
Iteration 90/1000 | Loss: 0.00001486
Iteration 91/1000 | Loss: 0.00001486
Iteration 92/1000 | Loss: 0.00001486
Iteration 93/1000 | Loss: 0.00001486
Iteration 94/1000 | Loss: 0.00001485
Iteration 95/1000 | Loss: 0.00001485
Iteration 96/1000 | Loss: 0.00001485
Iteration 97/1000 | Loss: 0.00001485
Iteration 98/1000 | Loss: 0.00001485
Iteration 99/1000 | Loss: 0.00001484
Iteration 100/1000 | Loss: 0.00001484
Iteration 101/1000 | Loss: 0.00001484
Iteration 102/1000 | Loss: 0.00001484
Iteration 103/1000 | Loss: 0.00001484
Iteration 104/1000 | Loss: 0.00001484
Iteration 105/1000 | Loss: 0.00001484
Iteration 106/1000 | Loss: 0.00001484
Iteration 107/1000 | Loss: 0.00001484
Iteration 108/1000 | Loss: 0.00001484
Iteration 109/1000 | Loss: 0.00001484
Iteration 110/1000 | Loss: 0.00001484
Iteration 111/1000 | Loss: 0.00001484
Iteration 112/1000 | Loss: 0.00001484
Iteration 113/1000 | Loss: 0.00001484
Iteration 114/1000 | Loss: 0.00001484
Iteration 115/1000 | Loss: 0.00001484
Iteration 116/1000 | Loss: 0.00001483
Iteration 117/1000 | Loss: 0.00001483
Iteration 118/1000 | Loss: 0.00001483
Iteration 119/1000 | Loss: 0.00001483
Iteration 120/1000 | Loss: 0.00001483
Iteration 121/1000 | Loss: 0.00001482
Iteration 122/1000 | Loss: 0.00001482
Iteration 123/1000 | Loss: 0.00001482
Iteration 124/1000 | Loss: 0.00001482
Iteration 125/1000 | Loss: 0.00001482
Iteration 126/1000 | Loss: 0.00001482
Iteration 127/1000 | Loss: 0.00001482
Iteration 128/1000 | Loss: 0.00001482
Iteration 129/1000 | Loss: 0.00001482
Iteration 130/1000 | Loss: 0.00001482
Iteration 131/1000 | Loss: 0.00001482
Iteration 132/1000 | Loss: 0.00001482
Iteration 133/1000 | Loss: 0.00001482
Iteration 134/1000 | Loss: 0.00001482
Iteration 135/1000 | Loss: 0.00001482
Iteration 136/1000 | Loss: 0.00001482
Iteration 137/1000 | Loss: 0.00001481
Iteration 138/1000 | Loss: 0.00001481
Iteration 139/1000 | Loss: 0.00001481
Iteration 140/1000 | Loss: 0.00001481
Iteration 141/1000 | Loss: 0.00001481
Iteration 142/1000 | Loss: 0.00001481
Iteration 143/1000 | Loss: 0.00001481
Iteration 144/1000 | Loss: 0.00001481
Iteration 145/1000 | Loss: 0.00001481
Iteration 146/1000 | Loss: 0.00001481
Iteration 147/1000 | Loss: 0.00001481
Iteration 148/1000 | Loss: 0.00001481
Iteration 149/1000 | Loss: 0.00001481
Iteration 150/1000 | Loss: 0.00001481
Iteration 151/1000 | Loss: 0.00001481
Iteration 152/1000 | Loss: 0.00001481
Iteration 153/1000 | Loss: 0.00001481
Iteration 154/1000 | Loss: 0.00001481
Iteration 155/1000 | Loss: 0.00001481
Iteration 156/1000 | Loss: 0.00001481
Iteration 157/1000 | Loss: 0.00001481
Iteration 158/1000 | Loss: 0.00001481
Iteration 159/1000 | Loss: 0.00001481
Iteration 160/1000 | Loss: 0.00001481
Iteration 161/1000 | Loss: 0.00001481
Iteration 162/1000 | Loss: 0.00001481
Iteration 163/1000 | Loss: 0.00001481
Iteration 164/1000 | Loss: 0.00001481
Iteration 165/1000 | Loss: 0.00001481
Iteration 166/1000 | Loss: 0.00001481
Iteration 167/1000 | Loss: 0.00001481
Iteration 168/1000 | Loss: 0.00001481
Iteration 169/1000 | Loss: 0.00001481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.4812444533163216e-05, 1.4812444533163216e-05, 1.4812444533163216e-05, 1.4812444533163216e-05, 1.4812444533163216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4812444533163216e-05

Optimization complete. Final v2v error: 3.3079330921173096 mm

Highest mean error: 3.6001665592193604 mm for frame 112

Lowest mean error: 3.1017796993255615 mm for frame 12

Saving results

Total time: 32.2697491645813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5394/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5394/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889752
Iteration 2/25 | Loss: 0.00163197
Iteration 3/25 | Loss: 0.00126758
Iteration 4/25 | Loss: 0.00122874
Iteration 5/25 | Loss: 0.00122229
Iteration 6/25 | Loss: 0.00122149
Iteration 7/25 | Loss: 0.00122149
Iteration 8/25 | Loss: 0.00122149
Iteration 9/25 | Loss: 0.00122149
Iteration 10/25 | Loss: 0.00122149
Iteration 11/25 | Loss: 0.00122149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012214886955916882, 0.0012214886955916882, 0.0012214886955916882, 0.0012214886955916882, 0.0012214886955916882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012214886955916882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23467898
Iteration 2/25 | Loss: 0.00112692
Iteration 3/25 | Loss: 0.00112692
Iteration 4/25 | Loss: 0.00112692
Iteration 5/25 | Loss: 0.00112692
Iteration 6/25 | Loss: 0.00112692
Iteration 7/25 | Loss: 0.00112692
Iteration 8/25 | Loss: 0.00112692
Iteration 9/25 | Loss: 0.00112692
Iteration 10/25 | Loss: 0.00112692
Iteration 11/25 | Loss: 0.00112692
Iteration 12/25 | Loss: 0.00112692
Iteration 13/25 | Loss: 0.00112692
Iteration 14/25 | Loss: 0.00112692
Iteration 15/25 | Loss: 0.00112692
Iteration 16/25 | Loss: 0.00112692
Iteration 17/25 | Loss: 0.00112692
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011269161477684975, 0.0011269161477684975, 0.0011269161477684975, 0.0011269161477684975, 0.0011269161477684975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011269161477684975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112692
Iteration 2/1000 | Loss: 0.00005474
Iteration 3/1000 | Loss: 0.00003004
Iteration 4/1000 | Loss: 0.00002485
Iteration 5/1000 | Loss: 0.00002100
Iteration 6/1000 | Loss: 0.00001956
Iteration 7/1000 | Loss: 0.00001873
Iteration 8/1000 | Loss: 0.00001820
Iteration 9/1000 | Loss: 0.00001778
Iteration 10/1000 | Loss: 0.00001745
Iteration 11/1000 | Loss: 0.00001714
Iteration 12/1000 | Loss: 0.00001691
Iteration 13/1000 | Loss: 0.00001676
Iteration 14/1000 | Loss: 0.00001676
Iteration 15/1000 | Loss: 0.00001675
Iteration 16/1000 | Loss: 0.00001675
Iteration 17/1000 | Loss: 0.00001674
Iteration 18/1000 | Loss: 0.00001670
Iteration 19/1000 | Loss: 0.00001670
Iteration 20/1000 | Loss: 0.00001670
Iteration 21/1000 | Loss: 0.00001666
Iteration 22/1000 | Loss: 0.00001666
Iteration 23/1000 | Loss: 0.00001665
Iteration 24/1000 | Loss: 0.00001665
Iteration 25/1000 | Loss: 0.00001664
Iteration 26/1000 | Loss: 0.00001663
Iteration 27/1000 | Loss: 0.00001662
Iteration 28/1000 | Loss: 0.00001662
Iteration 29/1000 | Loss: 0.00001662
Iteration 30/1000 | Loss: 0.00001661
Iteration 31/1000 | Loss: 0.00001660
Iteration 32/1000 | Loss: 0.00001660
Iteration 33/1000 | Loss: 0.00001660
Iteration 34/1000 | Loss: 0.00001660
Iteration 35/1000 | Loss: 0.00001660
Iteration 36/1000 | Loss: 0.00001659
Iteration 37/1000 | Loss: 0.00001659
Iteration 38/1000 | Loss: 0.00001659
Iteration 39/1000 | Loss: 0.00001658
Iteration 40/1000 | Loss: 0.00001658
Iteration 41/1000 | Loss: 0.00001658
Iteration 42/1000 | Loss: 0.00001658
Iteration 43/1000 | Loss: 0.00001658
Iteration 44/1000 | Loss: 0.00001657
Iteration 45/1000 | Loss: 0.00001657
Iteration 46/1000 | Loss: 0.00001657
Iteration 47/1000 | Loss: 0.00001656
Iteration 48/1000 | Loss: 0.00001656
Iteration 49/1000 | Loss: 0.00001656
Iteration 50/1000 | Loss: 0.00001656
Iteration 51/1000 | Loss: 0.00001656
Iteration 52/1000 | Loss: 0.00001656
Iteration 53/1000 | Loss: 0.00001655
Iteration 54/1000 | Loss: 0.00001655
Iteration 55/1000 | Loss: 0.00001655
Iteration 56/1000 | Loss: 0.00001655
Iteration 57/1000 | Loss: 0.00001655
Iteration 58/1000 | Loss: 0.00001655
Iteration 59/1000 | Loss: 0.00001655
Iteration 60/1000 | Loss: 0.00001655
Iteration 61/1000 | Loss: 0.00001655
Iteration 62/1000 | Loss: 0.00001655
Iteration 63/1000 | Loss: 0.00001655
Iteration 64/1000 | Loss: 0.00001655
Iteration 65/1000 | Loss: 0.00001654
Iteration 66/1000 | Loss: 0.00001654
Iteration 67/1000 | Loss: 0.00001654
Iteration 68/1000 | Loss: 0.00001654
Iteration 69/1000 | Loss: 0.00001654
Iteration 70/1000 | Loss: 0.00001654
Iteration 71/1000 | Loss: 0.00001654
Iteration 72/1000 | Loss: 0.00001654
Iteration 73/1000 | Loss: 0.00001654
Iteration 74/1000 | Loss: 0.00001654
Iteration 75/1000 | Loss: 0.00001654
Iteration 76/1000 | Loss: 0.00001654
Iteration 77/1000 | Loss: 0.00001654
Iteration 78/1000 | Loss: 0.00001654
Iteration 79/1000 | Loss: 0.00001654
Iteration 80/1000 | Loss: 0.00001654
Iteration 81/1000 | Loss: 0.00001654
Iteration 82/1000 | Loss: 0.00001654
Iteration 83/1000 | Loss: 0.00001654
Iteration 84/1000 | Loss: 0.00001654
Iteration 85/1000 | Loss: 0.00001654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.6544912796234712e-05, 1.6544912796234712e-05, 1.6544912796234712e-05, 1.6544912796234712e-05, 1.6544912796234712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6544912796234712e-05

Optimization complete. Final v2v error: 3.492826461791992 mm

Highest mean error: 4.161500453948975 mm for frame 196

Lowest mean error: 3.1107797622680664 mm for frame 106

Saving results

Total time: 35.79601526260376
