Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=120, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 6720-6775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069528
Iteration 2/25 | Loss: 0.00132399
Iteration 3/25 | Loss: 0.00079367
Iteration 4/25 | Loss: 0.00071078
Iteration 5/25 | Loss: 0.00067565
Iteration 6/25 | Loss: 0.00066424
Iteration 7/25 | Loss: 0.00066925
Iteration 8/25 | Loss: 0.00065694
Iteration 9/25 | Loss: 0.00065816
Iteration 10/25 | Loss: 0.00065220
Iteration 11/25 | Loss: 0.00065167
Iteration 12/25 | Loss: 0.00065149
Iteration 13/25 | Loss: 0.00065144
Iteration 14/25 | Loss: 0.00065143
Iteration 15/25 | Loss: 0.00065143
Iteration 16/25 | Loss: 0.00065143
Iteration 17/25 | Loss: 0.00065143
Iteration 18/25 | Loss: 0.00065143
Iteration 19/25 | Loss: 0.00065143
Iteration 20/25 | Loss: 0.00065143
Iteration 21/25 | Loss: 0.00065143
Iteration 22/25 | Loss: 0.00065142
Iteration 23/25 | Loss: 0.00065142
Iteration 24/25 | Loss: 0.00065142
Iteration 25/25 | Loss: 0.00065142

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.66258812
Iteration 2/25 | Loss: 0.00034106
Iteration 3/25 | Loss: 0.00029471
Iteration 4/25 | Loss: 0.00029471
Iteration 5/25 | Loss: 0.00029471
Iteration 6/25 | Loss: 0.00029470
Iteration 7/25 | Loss: 0.00029470
Iteration 8/25 | Loss: 0.00029470
Iteration 9/25 | Loss: 0.00029470
Iteration 10/25 | Loss: 0.00029470
Iteration 11/25 | Loss: 0.00029470
Iteration 12/25 | Loss: 0.00029470
Iteration 13/25 | Loss: 0.00029470
Iteration 14/25 | Loss: 0.00029470
Iteration 15/25 | Loss: 0.00029470
Iteration 16/25 | Loss: 0.00029470
Iteration 17/25 | Loss: 0.00029470
Iteration 18/25 | Loss: 0.00029470
Iteration 19/25 | Loss: 0.00029470
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0002947044267784804, 0.0002947044267784804, 0.0002947044267784804, 0.0002947044267784804, 0.0002947044267784804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002947044267784804

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029470
Iteration 2/1000 | Loss: 0.00002583
Iteration 3/1000 | Loss: 0.00001777
Iteration 4/1000 | Loss: 0.00001658
Iteration 5/1000 | Loss: 0.00001560
Iteration 6/1000 | Loss: 0.00001519
Iteration 7/1000 | Loss: 0.00006920
Iteration 8/1000 | Loss: 0.00002488
Iteration 9/1000 | Loss: 0.00001612
Iteration 10/1000 | Loss: 0.00001446
Iteration 11/1000 | Loss: 0.00001433
Iteration 12/1000 | Loss: 0.00008479
Iteration 13/1000 | Loss: 0.00038442
Iteration 14/1000 | Loss: 0.00001472
Iteration 15/1000 | Loss: 0.00001417
Iteration 16/1000 | Loss: 0.00007985
Iteration 17/1000 | Loss: 0.00004184
Iteration 18/1000 | Loss: 0.00001726
Iteration 19/1000 | Loss: 0.00001408
Iteration 20/1000 | Loss: 0.00001408
Iteration 21/1000 | Loss: 0.00001407
Iteration 22/1000 | Loss: 0.00001407
Iteration 23/1000 | Loss: 0.00001407
Iteration 24/1000 | Loss: 0.00001407
Iteration 25/1000 | Loss: 0.00001406
Iteration 26/1000 | Loss: 0.00001406
Iteration 27/1000 | Loss: 0.00001403
Iteration 28/1000 | Loss: 0.00001400
Iteration 29/1000 | Loss: 0.00001398
Iteration 30/1000 | Loss: 0.00001397
Iteration 31/1000 | Loss: 0.00001397
Iteration 32/1000 | Loss: 0.00001396
Iteration 33/1000 | Loss: 0.00001396
Iteration 34/1000 | Loss: 0.00001395
Iteration 35/1000 | Loss: 0.00001393
Iteration 36/1000 | Loss: 0.00001392
Iteration 37/1000 | Loss: 0.00001392
Iteration 38/1000 | Loss: 0.00001391
Iteration 39/1000 | Loss: 0.00001391
Iteration 40/1000 | Loss: 0.00001391
Iteration 41/1000 | Loss: 0.00001390
Iteration 42/1000 | Loss: 0.00001390
Iteration 43/1000 | Loss: 0.00001390
Iteration 44/1000 | Loss: 0.00001390
Iteration 45/1000 | Loss: 0.00001390
Iteration 46/1000 | Loss: 0.00001389
Iteration 47/1000 | Loss: 0.00001389
Iteration 48/1000 | Loss: 0.00001389
Iteration 49/1000 | Loss: 0.00001389
Iteration 50/1000 | Loss: 0.00001389
Iteration 51/1000 | Loss: 0.00001389
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001388
Iteration 54/1000 | Loss: 0.00001388
Iteration 55/1000 | Loss: 0.00001388
Iteration 56/1000 | Loss: 0.00001388
Iteration 57/1000 | Loss: 0.00001387
Iteration 58/1000 | Loss: 0.00001387
Iteration 59/1000 | Loss: 0.00001387
Iteration 60/1000 | Loss: 0.00001386
Iteration 61/1000 | Loss: 0.00001386
Iteration 62/1000 | Loss: 0.00001386
Iteration 63/1000 | Loss: 0.00001385
Iteration 64/1000 | Loss: 0.00001385
Iteration 65/1000 | Loss: 0.00001384
Iteration 66/1000 | Loss: 0.00001384
Iteration 67/1000 | Loss: 0.00001383
Iteration 68/1000 | Loss: 0.00001383
Iteration 69/1000 | Loss: 0.00001382
Iteration 70/1000 | Loss: 0.00001382
Iteration 71/1000 | Loss: 0.00001382
Iteration 72/1000 | Loss: 0.00001381
Iteration 73/1000 | Loss: 0.00001381
Iteration 74/1000 | Loss: 0.00001381
Iteration 75/1000 | Loss: 0.00001381
Iteration 76/1000 | Loss: 0.00001380
Iteration 77/1000 | Loss: 0.00001380
Iteration 78/1000 | Loss: 0.00001380
Iteration 79/1000 | Loss: 0.00001380
Iteration 80/1000 | Loss: 0.00001380
Iteration 81/1000 | Loss: 0.00001380
Iteration 82/1000 | Loss: 0.00001380
Iteration 83/1000 | Loss: 0.00001379
Iteration 84/1000 | Loss: 0.00001379
Iteration 85/1000 | Loss: 0.00001379
Iteration 86/1000 | Loss: 0.00001379
Iteration 87/1000 | Loss: 0.00001379
Iteration 88/1000 | Loss: 0.00001379
Iteration 89/1000 | Loss: 0.00001379
Iteration 90/1000 | Loss: 0.00001378
Iteration 91/1000 | Loss: 0.00001378
Iteration 92/1000 | Loss: 0.00001378
Iteration 93/1000 | Loss: 0.00001378
Iteration 94/1000 | Loss: 0.00001378
Iteration 95/1000 | Loss: 0.00001378
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00001377
Iteration 98/1000 | Loss: 0.00001377
Iteration 99/1000 | Loss: 0.00001377
Iteration 100/1000 | Loss: 0.00001377
Iteration 101/1000 | Loss: 0.00001377
Iteration 102/1000 | Loss: 0.00001376
Iteration 103/1000 | Loss: 0.00001376
Iteration 104/1000 | Loss: 0.00001376
Iteration 105/1000 | Loss: 0.00001376
Iteration 106/1000 | Loss: 0.00001376
Iteration 107/1000 | Loss: 0.00001375
Iteration 108/1000 | Loss: 0.00001375
Iteration 109/1000 | Loss: 0.00001375
Iteration 110/1000 | Loss: 0.00001375
Iteration 111/1000 | Loss: 0.00001375
Iteration 112/1000 | Loss: 0.00001375
Iteration 113/1000 | Loss: 0.00001374
Iteration 114/1000 | Loss: 0.00001374
Iteration 115/1000 | Loss: 0.00001374
Iteration 116/1000 | Loss: 0.00001374
Iteration 117/1000 | Loss: 0.00001374
Iteration 118/1000 | Loss: 0.00001374
Iteration 119/1000 | Loss: 0.00001374
Iteration 120/1000 | Loss: 0.00001374
Iteration 121/1000 | Loss: 0.00001374
Iteration 122/1000 | Loss: 0.00001374
Iteration 123/1000 | Loss: 0.00001374
Iteration 124/1000 | Loss: 0.00001374
Iteration 125/1000 | Loss: 0.00001374
Iteration 126/1000 | Loss: 0.00001374
Iteration 127/1000 | Loss: 0.00001374
Iteration 128/1000 | Loss: 0.00001373
Iteration 129/1000 | Loss: 0.00001373
Iteration 130/1000 | Loss: 0.00001373
Iteration 131/1000 | Loss: 0.00001373
Iteration 132/1000 | Loss: 0.00001373
Iteration 133/1000 | Loss: 0.00001373
Iteration 134/1000 | Loss: 0.00001372
Iteration 135/1000 | Loss: 0.00001372
Iteration 136/1000 | Loss: 0.00001372
Iteration 137/1000 | Loss: 0.00001371
Iteration 138/1000 | Loss: 0.00001371
Iteration 139/1000 | Loss: 0.00001371
Iteration 140/1000 | Loss: 0.00001371
Iteration 141/1000 | Loss: 0.00001370
Iteration 142/1000 | Loss: 0.00001370
Iteration 143/1000 | Loss: 0.00001370
Iteration 144/1000 | Loss: 0.00001370
Iteration 145/1000 | Loss: 0.00001369
Iteration 146/1000 | Loss: 0.00001369
Iteration 147/1000 | Loss: 0.00001369
Iteration 148/1000 | Loss: 0.00001369
Iteration 149/1000 | Loss: 0.00001369
Iteration 150/1000 | Loss: 0.00001369
Iteration 151/1000 | Loss: 0.00001369
Iteration 152/1000 | Loss: 0.00001368
Iteration 153/1000 | Loss: 0.00001368
Iteration 154/1000 | Loss: 0.00001368
Iteration 155/1000 | Loss: 0.00001368
Iteration 156/1000 | Loss: 0.00001368
Iteration 157/1000 | Loss: 0.00001368
Iteration 158/1000 | Loss: 0.00001368
Iteration 159/1000 | Loss: 0.00001368
Iteration 160/1000 | Loss: 0.00001368
Iteration 161/1000 | Loss: 0.00001368
Iteration 162/1000 | Loss: 0.00001368
Iteration 163/1000 | Loss: 0.00001368
Iteration 164/1000 | Loss: 0.00001368
Iteration 165/1000 | Loss: 0.00001368
Iteration 166/1000 | Loss: 0.00001368
Iteration 167/1000 | Loss: 0.00001367
Iteration 168/1000 | Loss: 0.00001367
Iteration 169/1000 | Loss: 0.00001367
Iteration 170/1000 | Loss: 0.00001367
Iteration 171/1000 | Loss: 0.00001367
Iteration 172/1000 | Loss: 0.00001367
Iteration 173/1000 | Loss: 0.00001367
Iteration 174/1000 | Loss: 0.00001367
Iteration 175/1000 | Loss: 0.00001367
Iteration 176/1000 | Loss: 0.00001367
Iteration 177/1000 | Loss: 0.00001367
Iteration 178/1000 | Loss: 0.00001367
Iteration 179/1000 | Loss: 0.00001367
Iteration 180/1000 | Loss: 0.00001366
Iteration 181/1000 | Loss: 0.00001366
Iteration 182/1000 | Loss: 0.00001366
Iteration 183/1000 | Loss: 0.00001366
Iteration 184/1000 | Loss: 0.00001366
Iteration 185/1000 | Loss: 0.00001366
Iteration 186/1000 | Loss: 0.00001366
Iteration 187/1000 | Loss: 0.00006154
Iteration 188/1000 | Loss: 0.00001583
Iteration 189/1000 | Loss: 0.00001429
Iteration 190/1000 | Loss: 0.00001375
Iteration 191/1000 | Loss: 0.00001375
Iteration 192/1000 | Loss: 0.00001365
Iteration 193/1000 | Loss: 0.00001364
Iteration 194/1000 | Loss: 0.00001363
Iteration 195/1000 | Loss: 0.00001363
Iteration 196/1000 | Loss: 0.00001363
Iteration 197/1000 | Loss: 0.00001362
Iteration 198/1000 | Loss: 0.00001362
Iteration 199/1000 | Loss: 0.00001362
Iteration 200/1000 | Loss: 0.00001362
Iteration 201/1000 | Loss: 0.00001362
Iteration 202/1000 | Loss: 0.00001362
Iteration 203/1000 | Loss: 0.00001362
Iteration 204/1000 | Loss: 0.00001362
Iteration 205/1000 | Loss: 0.00001362
Iteration 206/1000 | Loss: 0.00001362
Iteration 207/1000 | Loss: 0.00001362
Iteration 208/1000 | Loss: 0.00001362
Iteration 209/1000 | Loss: 0.00001362
Iteration 210/1000 | Loss: 0.00001362
Iteration 211/1000 | Loss: 0.00001362
Iteration 212/1000 | Loss: 0.00001362
Iteration 213/1000 | Loss: 0.00001362
Iteration 214/1000 | Loss: 0.00001362
Iteration 215/1000 | Loss: 0.00001362
Iteration 216/1000 | Loss: 0.00001362
Iteration 217/1000 | Loss: 0.00001362
Iteration 218/1000 | Loss: 0.00001362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.3617326658277307e-05, 1.3617326658277307e-05, 1.3617326658277307e-05, 1.3617326658277307e-05, 1.3617326658277307e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3617326658277307e-05

Optimization complete. Final v2v error: 3.0957958698272705 mm

Highest mean error: 8.372171401977539 mm for frame 237

Lowest mean error: 2.822999954223633 mm for frame 8

Saving results

Total time: 79.31317257881165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080689
Iteration 2/25 | Loss: 0.00107860
Iteration 3/25 | Loss: 0.00073593
Iteration 4/25 | Loss: 0.00067085
Iteration 5/25 | Loss: 0.00065570
Iteration 6/25 | Loss: 0.00065218
Iteration 7/25 | Loss: 0.00065110
Iteration 8/25 | Loss: 0.00065766
Iteration 9/25 | Loss: 0.00066070
Iteration 10/25 | Loss: 0.00066590
Iteration 11/25 | Loss: 0.00066297
Iteration 12/25 | Loss: 0.00065850
Iteration 13/25 | Loss: 0.00065377
Iteration 14/25 | Loss: 0.00065194
Iteration 15/25 | Loss: 0.00065090
Iteration 16/25 | Loss: 0.00064994
Iteration 17/25 | Loss: 0.00064795
Iteration 18/25 | Loss: 0.00064892
Iteration 19/25 | Loss: 0.00064652
Iteration 20/25 | Loss: 0.00064615
Iteration 21/25 | Loss: 0.00064587
Iteration 22/25 | Loss: 0.00064577
Iteration 23/25 | Loss: 0.00064576
Iteration 24/25 | Loss: 0.00064575
Iteration 25/25 | Loss: 0.00064575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.51138067
Iteration 2/25 | Loss: 0.00024172
Iteration 3/25 | Loss: 0.00024172
Iteration 4/25 | Loss: 0.00024172
Iteration 5/25 | Loss: 0.00024172
Iteration 6/25 | Loss: 0.00024172
Iteration 7/25 | Loss: 0.00024172
Iteration 8/25 | Loss: 0.00024172
Iteration 9/25 | Loss: 0.00024172
Iteration 10/25 | Loss: 0.00024172
Iteration 11/25 | Loss: 0.00024172
Iteration 12/25 | Loss: 0.00024172
Iteration 13/25 | Loss: 0.00024172
Iteration 14/25 | Loss: 0.00024172
Iteration 15/25 | Loss: 0.00024172
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0002417204377707094, 0.0002417204377707094, 0.0002417204377707094, 0.0002417204377707094, 0.0002417204377707094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002417204377707094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024172
Iteration 2/1000 | Loss: 0.00002473
Iteration 3/1000 | Loss: 0.00001780
Iteration 4/1000 | Loss: 0.00001620
Iteration 5/1000 | Loss: 0.00001511
Iteration 6/1000 | Loss: 0.00001471
Iteration 7/1000 | Loss: 0.00001432
Iteration 8/1000 | Loss: 0.00001390
Iteration 9/1000 | Loss: 0.00001363
Iteration 10/1000 | Loss: 0.00001344
Iteration 11/1000 | Loss: 0.00001339
Iteration 12/1000 | Loss: 0.00001337
Iteration 13/1000 | Loss: 0.00001336
Iteration 14/1000 | Loss: 0.00001336
Iteration 15/1000 | Loss: 0.00001329
Iteration 16/1000 | Loss: 0.00001328
Iteration 17/1000 | Loss: 0.00001327
Iteration 18/1000 | Loss: 0.00001326
Iteration 19/1000 | Loss: 0.00001326
Iteration 20/1000 | Loss: 0.00001326
Iteration 21/1000 | Loss: 0.00001326
Iteration 22/1000 | Loss: 0.00001325
Iteration 23/1000 | Loss: 0.00001325
Iteration 24/1000 | Loss: 0.00001325
Iteration 25/1000 | Loss: 0.00001325
Iteration 26/1000 | Loss: 0.00001325
Iteration 27/1000 | Loss: 0.00001325
Iteration 28/1000 | Loss: 0.00001325
Iteration 29/1000 | Loss: 0.00001324
Iteration 30/1000 | Loss: 0.00001324
Iteration 31/1000 | Loss: 0.00001322
Iteration 32/1000 | Loss: 0.00001322
Iteration 33/1000 | Loss: 0.00001322
Iteration 34/1000 | Loss: 0.00001322
Iteration 35/1000 | Loss: 0.00001322
Iteration 36/1000 | Loss: 0.00001322
Iteration 37/1000 | Loss: 0.00001322
Iteration 38/1000 | Loss: 0.00001322
Iteration 39/1000 | Loss: 0.00001321
Iteration 40/1000 | Loss: 0.00001321
Iteration 41/1000 | Loss: 0.00001321
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001321
Iteration 44/1000 | Loss: 0.00001321
Iteration 45/1000 | Loss: 0.00001321
Iteration 46/1000 | Loss: 0.00001319
Iteration 47/1000 | Loss: 0.00001319
Iteration 48/1000 | Loss: 0.00001319
Iteration 49/1000 | Loss: 0.00001318
Iteration 50/1000 | Loss: 0.00001318
Iteration 51/1000 | Loss: 0.00001318
Iteration 52/1000 | Loss: 0.00001318
Iteration 53/1000 | Loss: 0.00001318
Iteration 54/1000 | Loss: 0.00001318
Iteration 55/1000 | Loss: 0.00001318
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001317
Iteration 58/1000 | Loss: 0.00001317
Iteration 59/1000 | Loss: 0.00001317
Iteration 60/1000 | Loss: 0.00001317
Iteration 61/1000 | Loss: 0.00001316
Iteration 62/1000 | Loss: 0.00001316
Iteration 63/1000 | Loss: 0.00001316
Iteration 64/1000 | Loss: 0.00001316
Iteration 65/1000 | Loss: 0.00001316
Iteration 66/1000 | Loss: 0.00001316
Iteration 67/1000 | Loss: 0.00001316
Iteration 68/1000 | Loss: 0.00001316
Iteration 69/1000 | Loss: 0.00001316
Iteration 70/1000 | Loss: 0.00001316
Iteration 71/1000 | Loss: 0.00001316
Iteration 72/1000 | Loss: 0.00001316
Iteration 73/1000 | Loss: 0.00001316
Iteration 74/1000 | Loss: 0.00001316
Iteration 75/1000 | Loss: 0.00001316
Iteration 76/1000 | Loss: 0.00001316
Iteration 77/1000 | Loss: 0.00001316
Iteration 78/1000 | Loss: 0.00001316
Iteration 79/1000 | Loss: 0.00001316
Iteration 80/1000 | Loss: 0.00001316
Iteration 81/1000 | Loss: 0.00001316
Iteration 82/1000 | Loss: 0.00001316
Iteration 83/1000 | Loss: 0.00001316
Iteration 84/1000 | Loss: 0.00001316
Iteration 85/1000 | Loss: 0.00001316
Iteration 86/1000 | Loss: 0.00001316
Iteration 87/1000 | Loss: 0.00001316
Iteration 88/1000 | Loss: 0.00001316
Iteration 89/1000 | Loss: 0.00001316
Iteration 90/1000 | Loss: 0.00001316
Iteration 91/1000 | Loss: 0.00001316
Iteration 92/1000 | Loss: 0.00001316
Iteration 93/1000 | Loss: 0.00001316
Iteration 94/1000 | Loss: 0.00001316
Iteration 95/1000 | Loss: 0.00001316
Iteration 96/1000 | Loss: 0.00001316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.315898225584533e-05, 1.315898225584533e-05, 1.315898225584533e-05, 1.315898225584533e-05, 1.315898225584533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.315898225584533e-05

Optimization complete. Final v2v error: 3.077918767929077 mm

Highest mean error: 3.35764479637146 mm for frame 1

Lowest mean error: 2.879642963409424 mm for frame 125

Saving results

Total time: 68.07684540748596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829298
Iteration 2/25 | Loss: 0.00074038
Iteration 3/25 | Loss: 0.00063506
Iteration 4/25 | Loss: 0.00061632
Iteration 5/25 | Loss: 0.00060949
Iteration 6/25 | Loss: 0.00060817
Iteration 7/25 | Loss: 0.00060786
Iteration 8/25 | Loss: 0.00060786
Iteration 9/25 | Loss: 0.00060786
Iteration 10/25 | Loss: 0.00060786
Iteration 11/25 | Loss: 0.00060786
Iteration 12/25 | Loss: 0.00060786
Iteration 13/25 | Loss: 0.00060786
Iteration 14/25 | Loss: 0.00060786
Iteration 15/25 | Loss: 0.00060786
Iteration 16/25 | Loss: 0.00060786
Iteration 17/25 | Loss: 0.00060786
Iteration 18/25 | Loss: 0.00060786
Iteration 19/25 | Loss: 0.00060786
Iteration 20/25 | Loss: 0.00060786
Iteration 21/25 | Loss: 0.00060786
Iteration 22/25 | Loss: 0.00060786
Iteration 23/25 | Loss: 0.00060786
Iteration 24/25 | Loss: 0.00060786
Iteration 25/25 | Loss: 0.00060786

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46626115
Iteration 2/25 | Loss: 0.00033424
Iteration 3/25 | Loss: 0.00033424
Iteration 4/25 | Loss: 0.00033424
Iteration 5/25 | Loss: 0.00033424
Iteration 6/25 | Loss: 0.00033424
Iteration 7/25 | Loss: 0.00033424
Iteration 8/25 | Loss: 0.00033424
Iteration 9/25 | Loss: 0.00033424
Iteration 10/25 | Loss: 0.00033424
Iteration 11/25 | Loss: 0.00033424
Iteration 12/25 | Loss: 0.00033424
Iteration 13/25 | Loss: 0.00033424
Iteration 14/25 | Loss: 0.00033424
Iteration 15/25 | Loss: 0.00033424
Iteration 16/25 | Loss: 0.00033424
Iteration 17/25 | Loss: 0.00033424
Iteration 18/25 | Loss: 0.00033424
Iteration 19/25 | Loss: 0.00033424
Iteration 20/25 | Loss: 0.00033424
Iteration 21/25 | Loss: 0.00033424
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00033423607237637043, 0.00033423607237637043, 0.00033423607237637043, 0.00033423607237637043, 0.00033423607237637043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033423607237637043

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033424
Iteration 2/1000 | Loss: 0.00002298
Iteration 3/1000 | Loss: 0.00001540
Iteration 4/1000 | Loss: 0.00001408
Iteration 5/1000 | Loss: 0.00001346
Iteration 6/1000 | Loss: 0.00001307
Iteration 7/1000 | Loss: 0.00001277
Iteration 8/1000 | Loss: 0.00001273
Iteration 9/1000 | Loss: 0.00001264
Iteration 10/1000 | Loss: 0.00001263
Iteration 11/1000 | Loss: 0.00001258
Iteration 12/1000 | Loss: 0.00001257
Iteration 13/1000 | Loss: 0.00001257
Iteration 14/1000 | Loss: 0.00001256
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001254
Iteration 17/1000 | Loss: 0.00001251
Iteration 18/1000 | Loss: 0.00001251
Iteration 19/1000 | Loss: 0.00001250
Iteration 20/1000 | Loss: 0.00001250
Iteration 21/1000 | Loss: 0.00001250
Iteration 22/1000 | Loss: 0.00001250
Iteration 23/1000 | Loss: 0.00001249
Iteration 24/1000 | Loss: 0.00001247
Iteration 25/1000 | Loss: 0.00001247
Iteration 26/1000 | Loss: 0.00001246
Iteration 27/1000 | Loss: 0.00001246
Iteration 28/1000 | Loss: 0.00001242
Iteration 29/1000 | Loss: 0.00001242
Iteration 30/1000 | Loss: 0.00001241
Iteration 31/1000 | Loss: 0.00001238
Iteration 32/1000 | Loss: 0.00001237
Iteration 33/1000 | Loss: 0.00001237
Iteration 34/1000 | Loss: 0.00001236
Iteration 35/1000 | Loss: 0.00001236
Iteration 36/1000 | Loss: 0.00001235
Iteration 37/1000 | Loss: 0.00001233
Iteration 38/1000 | Loss: 0.00001232
Iteration 39/1000 | Loss: 0.00001232
Iteration 40/1000 | Loss: 0.00001232
Iteration 41/1000 | Loss: 0.00001232
Iteration 42/1000 | Loss: 0.00001231
Iteration 43/1000 | Loss: 0.00001231
Iteration 44/1000 | Loss: 0.00001231
Iteration 45/1000 | Loss: 0.00001231
Iteration 46/1000 | Loss: 0.00001231
Iteration 47/1000 | Loss: 0.00001231
Iteration 48/1000 | Loss: 0.00001231
Iteration 49/1000 | Loss: 0.00001230
Iteration 50/1000 | Loss: 0.00001230
Iteration 51/1000 | Loss: 0.00001230
Iteration 52/1000 | Loss: 0.00001229
Iteration 53/1000 | Loss: 0.00001229
Iteration 54/1000 | Loss: 0.00001229
Iteration 55/1000 | Loss: 0.00001229
Iteration 56/1000 | Loss: 0.00001229
Iteration 57/1000 | Loss: 0.00001229
Iteration 58/1000 | Loss: 0.00001229
Iteration 59/1000 | Loss: 0.00001228
Iteration 60/1000 | Loss: 0.00001228
Iteration 61/1000 | Loss: 0.00001228
Iteration 62/1000 | Loss: 0.00001228
Iteration 63/1000 | Loss: 0.00001227
Iteration 64/1000 | Loss: 0.00001227
Iteration 65/1000 | Loss: 0.00001226
Iteration 66/1000 | Loss: 0.00001226
Iteration 67/1000 | Loss: 0.00001225
Iteration 68/1000 | Loss: 0.00001225
Iteration 69/1000 | Loss: 0.00001225
Iteration 70/1000 | Loss: 0.00001225
Iteration 71/1000 | Loss: 0.00001224
Iteration 72/1000 | Loss: 0.00001224
Iteration 73/1000 | Loss: 0.00001224
Iteration 74/1000 | Loss: 0.00001224
Iteration 75/1000 | Loss: 0.00001223
Iteration 76/1000 | Loss: 0.00001223
Iteration 77/1000 | Loss: 0.00001223
Iteration 78/1000 | Loss: 0.00001223
Iteration 79/1000 | Loss: 0.00001222
Iteration 80/1000 | Loss: 0.00001222
Iteration 81/1000 | Loss: 0.00001222
Iteration 82/1000 | Loss: 0.00001222
Iteration 83/1000 | Loss: 0.00001222
Iteration 84/1000 | Loss: 0.00001222
Iteration 85/1000 | Loss: 0.00001222
Iteration 86/1000 | Loss: 0.00001222
Iteration 87/1000 | Loss: 0.00001221
Iteration 88/1000 | Loss: 0.00001221
Iteration 89/1000 | Loss: 0.00001221
Iteration 90/1000 | Loss: 0.00001221
Iteration 91/1000 | Loss: 0.00001221
Iteration 92/1000 | Loss: 0.00001221
Iteration 93/1000 | Loss: 0.00001221
Iteration 94/1000 | Loss: 0.00001220
Iteration 95/1000 | Loss: 0.00001220
Iteration 96/1000 | Loss: 0.00001220
Iteration 97/1000 | Loss: 0.00001220
Iteration 98/1000 | Loss: 0.00001220
Iteration 99/1000 | Loss: 0.00001220
Iteration 100/1000 | Loss: 0.00001220
Iteration 101/1000 | Loss: 0.00001220
Iteration 102/1000 | Loss: 0.00001220
Iteration 103/1000 | Loss: 0.00001220
Iteration 104/1000 | Loss: 0.00001220
Iteration 105/1000 | Loss: 0.00001220
Iteration 106/1000 | Loss: 0.00001220
Iteration 107/1000 | Loss: 0.00001220
Iteration 108/1000 | Loss: 0.00001220
Iteration 109/1000 | Loss: 0.00001219
Iteration 110/1000 | Loss: 0.00001219
Iteration 111/1000 | Loss: 0.00001219
Iteration 112/1000 | Loss: 0.00001219
Iteration 113/1000 | Loss: 0.00001219
Iteration 114/1000 | Loss: 0.00001219
Iteration 115/1000 | Loss: 0.00001219
Iteration 116/1000 | Loss: 0.00001219
Iteration 117/1000 | Loss: 0.00001219
Iteration 118/1000 | Loss: 0.00001219
Iteration 119/1000 | Loss: 0.00001219
Iteration 120/1000 | Loss: 0.00001219
Iteration 121/1000 | Loss: 0.00001219
Iteration 122/1000 | Loss: 0.00001218
Iteration 123/1000 | Loss: 0.00001218
Iteration 124/1000 | Loss: 0.00001218
Iteration 125/1000 | Loss: 0.00001218
Iteration 126/1000 | Loss: 0.00001218
Iteration 127/1000 | Loss: 0.00001218
Iteration 128/1000 | Loss: 0.00001218
Iteration 129/1000 | Loss: 0.00001218
Iteration 130/1000 | Loss: 0.00001218
Iteration 131/1000 | Loss: 0.00001218
Iteration 132/1000 | Loss: 0.00001218
Iteration 133/1000 | Loss: 0.00001218
Iteration 134/1000 | Loss: 0.00001218
Iteration 135/1000 | Loss: 0.00001218
Iteration 136/1000 | Loss: 0.00001218
Iteration 137/1000 | Loss: 0.00001217
Iteration 138/1000 | Loss: 0.00001217
Iteration 139/1000 | Loss: 0.00001216
Iteration 140/1000 | Loss: 0.00001216
Iteration 141/1000 | Loss: 0.00001216
Iteration 142/1000 | Loss: 0.00001216
Iteration 143/1000 | Loss: 0.00001216
Iteration 144/1000 | Loss: 0.00001216
Iteration 145/1000 | Loss: 0.00001216
Iteration 146/1000 | Loss: 0.00001215
Iteration 147/1000 | Loss: 0.00001215
Iteration 148/1000 | Loss: 0.00001215
Iteration 149/1000 | Loss: 0.00001215
Iteration 150/1000 | Loss: 0.00001215
Iteration 151/1000 | Loss: 0.00001215
Iteration 152/1000 | Loss: 0.00001215
Iteration 153/1000 | Loss: 0.00001215
Iteration 154/1000 | Loss: 0.00001215
Iteration 155/1000 | Loss: 0.00001215
Iteration 156/1000 | Loss: 0.00001214
Iteration 157/1000 | Loss: 0.00001214
Iteration 158/1000 | Loss: 0.00001214
Iteration 159/1000 | Loss: 0.00001214
Iteration 160/1000 | Loss: 0.00001214
Iteration 161/1000 | Loss: 0.00001214
Iteration 162/1000 | Loss: 0.00001214
Iteration 163/1000 | Loss: 0.00001214
Iteration 164/1000 | Loss: 0.00001213
Iteration 165/1000 | Loss: 0.00001213
Iteration 166/1000 | Loss: 0.00001213
Iteration 167/1000 | Loss: 0.00001213
Iteration 168/1000 | Loss: 0.00001213
Iteration 169/1000 | Loss: 0.00001212
Iteration 170/1000 | Loss: 0.00001212
Iteration 171/1000 | Loss: 0.00001212
Iteration 172/1000 | Loss: 0.00001212
Iteration 173/1000 | Loss: 0.00001212
Iteration 174/1000 | Loss: 0.00001212
Iteration 175/1000 | Loss: 0.00001212
Iteration 176/1000 | Loss: 0.00001212
Iteration 177/1000 | Loss: 0.00001211
Iteration 178/1000 | Loss: 0.00001211
Iteration 179/1000 | Loss: 0.00001211
Iteration 180/1000 | Loss: 0.00001211
Iteration 181/1000 | Loss: 0.00001211
Iteration 182/1000 | Loss: 0.00001211
Iteration 183/1000 | Loss: 0.00001211
Iteration 184/1000 | Loss: 0.00001211
Iteration 185/1000 | Loss: 0.00001211
Iteration 186/1000 | Loss: 0.00001211
Iteration 187/1000 | Loss: 0.00001211
Iteration 188/1000 | Loss: 0.00001211
Iteration 189/1000 | Loss: 0.00001210
Iteration 190/1000 | Loss: 0.00001210
Iteration 191/1000 | Loss: 0.00001210
Iteration 192/1000 | Loss: 0.00001210
Iteration 193/1000 | Loss: 0.00001210
Iteration 194/1000 | Loss: 0.00001210
Iteration 195/1000 | Loss: 0.00001210
Iteration 196/1000 | Loss: 0.00001210
Iteration 197/1000 | Loss: 0.00001210
Iteration 198/1000 | Loss: 0.00001210
Iteration 199/1000 | Loss: 0.00001210
Iteration 200/1000 | Loss: 0.00001210
Iteration 201/1000 | Loss: 0.00001210
Iteration 202/1000 | Loss: 0.00001210
Iteration 203/1000 | Loss: 0.00001209
Iteration 204/1000 | Loss: 0.00001209
Iteration 205/1000 | Loss: 0.00001209
Iteration 206/1000 | Loss: 0.00001209
Iteration 207/1000 | Loss: 0.00001209
Iteration 208/1000 | Loss: 0.00001209
Iteration 209/1000 | Loss: 0.00001209
Iteration 210/1000 | Loss: 0.00001209
Iteration 211/1000 | Loss: 0.00001209
Iteration 212/1000 | Loss: 0.00001209
Iteration 213/1000 | Loss: 0.00001209
Iteration 214/1000 | Loss: 0.00001209
Iteration 215/1000 | Loss: 0.00001209
Iteration 216/1000 | Loss: 0.00001209
Iteration 217/1000 | Loss: 0.00001209
Iteration 218/1000 | Loss: 0.00001209
Iteration 219/1000 | Loss: 0.00001209
Iteration 220/1000 | Loss: 0.00001209
Iteration 221/1000 | Loss: 0.00001209
Iteration 222/1000 | Loss: 0.00001209
Iteration 223/1000 | Loss: 0.00001209
Iteration 224/1000 | Loss: 0.00001209
Iteration 225/1000 | Loss: 0.00001209
Iteration 226/1000 | Loss: 0.00001209
Iteration 227/1000 | Loss: 0.00001209
Iteration 228/1000 | Loss: 0.00001209
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.2088873518223409e-05, 1.2088873518223409e-05, 1.2088873518223409e-05, 1.2088873518223409e-05, 1.2088873518223409e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2088873518223409e-05

Optimization complete. Final v2v error: 2.9401791095733643 mm

Highest mean error: 3.038431406021118 mm for frame 81

Lowest mean error: 2.817831039428711 mm for frame 90

Saving results

Total time: 36.52112293243408
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808357
Iteration 2/25 | Loss: 0.00081838
Iteration 3/25 | Loss: 0.00063328
Iteration 4/25 | Loss: 0.00060590
Iteration 5/25 | Loss: 0.00060117
Iteration 6/25 | Loss: 0.00060044
Iteration 7/25 | Loss: 0.00060044
Iteration 8/25 | Loss: 0.00060044
Iteration 9/25 | Loss: 0.00060044
Iteration 10/25 | Loss: 0.00060044
Iteration 11/25 | Loss: 0.00060044
Iteration 12/25 | Loss: 0.00060043
Iteration 13/25 | Loss: 0.00060043
Iteration 14/25 | Loss: 0.00060043
Iteration 15/25 | Loss: 0.00060043
Iteration 16/25 | Loss: 0.00060043
Iteration 17/25 | Loss: 0.00060043
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006004276219755411, 0.0006004276219755411, 0.0006004276219755411, 0.0006004276219755411, 0.0006004276219755411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006004276219755411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46266675
Iteration 2/25 | Loss: 0.00029041
Iteration 3/25 | Loss: 0.00029041
Iteration 4/25 | Loss: 0.00029041
Iteration 5/25 | Loss: 0.00029041
Iteration 6/25 | Loss: 0.00029041
Iteration 7/25 | Loss: 0.00029041
Iteration 8/25 | Loss: 0.00029041
Iteration 9/25 | Loss: 0.00029041
Iteration 10/25 | Loss: 0.00029041
Iteration 11/25 | Loss: 0.00029041
Iteration 12/25 | Loss: 0.00029041
Iteration 13/25 | Loss: 0.00029041
Iteration 14/25 | Loss: 0.00029041
Iteration 15/25 | Loss: 0.00029041
Iteration 16/25 | Loss: 0.00029041
Iteration 17/25 | Loss: 0.00029041
Iteration 18/25 | Loss: 0.00029041
Iteration 19/25 | Loss: 0.00029041
Iteration 20/25 | Loss: 0.00029041
Iteration 21/25 | Loss: 0.00029041
Iteration 22/25 | Loss: 0.00029041
Iteration 23/25 | Loss: 0.00029041
Iteration 24/25 | Loss: 0.00029041
Iteration 25/25 | Loss: 0.00029041
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0002904111170209944, 0.0002904111170209944, 0.0002904111170209944, 0.0002904111170209944, 0.0002904111170209944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002904111170209944

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029041
Iteration 2/1000 | Loss: 0.00001895
Iteration 3/1000 | Loss: 0.00001278
Iteration 4/1000 | Loss: 0.00001192
Iteration 5/1000 | Loss: 0.00001126
Iteration 6/1000 | Loss: 0.00001102
Iteration 7/1000 | Loss: 0.00001075
Iteration 8/1000 | Loss: 0.00001067
Iteration 9/1000 | Loss: 0.00001065
Iteration 10/1000 | Loss: 0.00001061
Iteration 11/1000 | Loss: 0.00001060
Iteration 12/1000 | Loss: 0.00001059
Iteration 13/1000 | Loss: 0.00001057
Iteration 14/1000 | Loss: 0.00001057
Iteration 15/1000 | Loss: 0.00001056
Iteration 16/1000 | Loss: 0.00001056
Iteration 17/1000 | Loss: 0.00001056
Iteration 18/1000 | Loss: 0.00001055
Iteration 19/1000 | Loss: 0.00001053
Iteration 20/1000 | Loss: 0.00001052
Iteration 21/1000 | Loss: 0.00001052
Iteration 22/1000 | Loss: 0.00001051
Iteration 23/1000 | Loss: 0.00001051
Iteration 24/1000 | Loss: 0.00001050
Iteration 25/1000 | Loss: 0.00001049
Iteration 26/1000 | Loss: 0.00001048
Iteration 27/1000 | Loss: 0.00001048
Iteration 28/1000 | Loss: 0.00001047
Iteration 29/1000 | Loss: 0.00001047
Iteration 30/1000 | Loss: 0.00001047
Iteration 31/1000 | Loss: 0.00001047
Iteration 32/1000 | Loss: 0.00001047
Iteration 33/1000 | Loss: 0.00001047
Iteration 34/1000 | Loss: 0.00001046
Iteration 35/1000 | Loss: 0.00001046
Iteration 36/1000 | Loss: 0.00001046
Iteration 37/1000 | Loss: 0.00001043
Iteration 38/1000 | Loss: 0.00001042
Iteration 39/1000 | Loss: 0.00001042
Iteration 40/1000 | Loss: 0.00001041
Iteration 41/1000 | Loss: 0.00001040
Iteration 42/1000 | Loss: 0.00001039
Iteration 43/1000 | Loss: 0.00001038
Iteration 44/1000 | Loss: 0.00001038
Iteration 45/1000 | Loss: 0.00001037
Iteration 46/1000 | Loss: 0.00001037
Iteration 47/1000 | Loss: 0.00001037
Iteration 48/1000 | Loss: 0.00001037
Iteration 49/1000 | Loss: 0.00001037
Iteration 50/1000 | Loss: 0.00001036
Iteration 51/1000 | Loss: 0.00001036
Iteration 52/1000 | Loss: 0.00001035
Iteration 53/1000 | Loss: 0.00001034
Iteration 54/1000 | Loss: 0.00001032
Iteration 55/1000 | Loss: 0.00001032
Iteration 56/1000 | Loss: 0.00001032
Iteration 57/1000 | Loss: 0.00001031
Iteration 58/1000 | Loss: 0.00001031
Iteration 59/1000 | Loss: 0.00001030
Iteration 60/1000 | Loss: 0.00001029
Iteration 61/1000 | Loss: 0.00001028
Iteration 62/1000 | Loss: 0.00001028
Iteration 63/1000 | Loss: 0.00001028
Iteration 64/1000 | Loss: 0.00001027
Iteration 65/1000 | Loss: 0.00001027
Iteration 66/1000 | Loss: 0.00001027
Iteration 67/1000 | Loss: 0.00001027
Iteration 68/1000 | Loss: 0.00001027
Iteration 69/1000 | Loss: 0.00001027
Iteration 70/1000 | Loss: 0.00001027
Iteration 71/1000 | Loss: 0.00001027
Iteration 72/1000 | Loss: 0.00001026
Iteration 73/1000 | Loss: 0.00001026
Iteration 74/1000 | Loss: 0.00001026
Iteration 75/1000 | Loss: 0.00001026
Iteration 76/1000 | Loss: 0.00001026
Iteration 77/1000 | Loss: 0.00001026
Iteration 78/1000 | Loss: 0.00001025
Iteration 79/1000 | Loss: 0.00001025
Iteration 80/1000 | Loss: 0.00001025
Iteration 81/1000 | Loss: 0.00001025
Iteration 82/1000 | Loss: 0.00001025
Iteration 83/1000 | Loss: 0.00001025
Iteration 84/1000 | Loss: 0.00001025
Iteration 85/1000 | Loss: 0.00001025
Iteration 86/1000 | Loss: 0.00001025
Iteration 87/1000 | Loss: 0.00001024
Iteration 88/1000 | Loss: 0.00001024
Iteration 89/1000 | Loss: 0.00001024
Iteration 90/1000 | Loss: 0.00001024
Iteration 91/1000 | Loss: 0.00001024
Iteration 92/1000 | Loss: 0.00001024
Iteration 93/1000 | Loss: 0.00001024
Iteration 94/1000 | Loss: 0.00001024
Iteration 95/1000 | Loss: 0.00001023
Iteration 96/1000 | Loss: 0.00001023
Iteration 97/1000 | Loss: 0.00001023
Iteration 98/1000 | Loss: 0.00001023
Iteration 99/1000 | Loss: 0.00001023
Iteration 100/1000 | Loss: 0.00001023
Iteration 101/1000 | Loss: 0.00001023
Iteration 102/1000 | Loss: 0.00001022
Iteration 103/1000 | Loss: 0.00001022
Iteration 104/1000 | Loss: 0.00001022
Iteration 105/1000 | Loss: 0.00001021
Iteration 106/1000 | Loss: 0.00001021
Iteration 107/1000 | Loss: 0.00001021
Iteration 108/1000 | Loss: 0.00001021
Iteration 109/1000 | Loss: 0.00001021
Iteration 110/1000 | Loss: 0.00001021
Iteration 111/1000 | Loss: 0.00001021
Iteration 112/1000 | Loss: 0.00001021
Iteration 113/1000 | Loss: 0.00001021
Iteration 114/1000 | Loss: 0.00001021
Iteration 115/1000 | Loss: 0.00001021
Iteration 116/1000 | Loss: 0.00001020
Iteration 117/1000 | Loss: 0.00001020
Iteration 118/1000 | Loss: 0.00001020
Iteration 119/1000 | Loss: 0.00001020
Iteration 120/1000 | Loss: 0.00001020
Iteration 121/1000 | Loss: 0.00001020
Iteration 122/1000 | Loss: 0.00001020
Iteration 123/1000 | Loss: 0.00001020
Iteration 124/1000 | Loss: 0.00001020
Iteration 125/1000 | Loss: 0.00001020
Iteration 126/1000 | Loss: 0.00001020
Iteration 127/1000 | Loss: 0.00001020
Iteration 128/1000 | Loss: 0.00001020
Iteration 129/1000 | Loss: 0.00001020
Iteration 130/1000 | Loss: 0.00001020
Iteration 131/1000 | Loss: 0.00001020
Iteration 132/1000 | Loss: 0.00001019
Iteration 133/1000 | Loss: 0.00001019
Iteration 134/1000 | Loss: 0.00001019
Iteration 135/1000 | Loss: 0.00001019
Iteration 136/1000 | Loss: 0.00001019
Iteration 137/1000 | Loss: 0.00001018
Iteration 138/1000 | Loss: 0.00001018
Iteration 139/1000 | Loss: 0.00001018
Iteration 140/1000 | Loss: 0.00001018
Iteration 141/1000 | Loss: 0.00001018
Iteration 142/1000 | Loss: 0.00001018
Iteration 143/1000 | Loss: 0.00001018
Iteration 144/1000 | Loss: 0.00001017
Iteration 145/1000 | Loss: 0.00001017
Iteration 146/1000 | Loss: 0.00001017
Iteration 147/1000 | Loss: 0.00001017
Iteration 148/1000 | Loss: 0.00001017
Iteration 149/1000 | Loss: 0.00001017
Iteration 150/1000 | Loss: 0.00001017
Iteration 151/1000 | Loss: 0.00001017
Iteration 152/1000 | Loss: 0.00001016
Iteration 153/1000 | Loss: 0.00001016
Iteration 154/1000 | Loss: 0.00001016
Iteration 155/1000 | Loss: 0.00001016
Iteration 156/1000 | Loss: 0.00001016
Iteration 157/1000 | Loss: 0.00001016
Iteration 158/1000 | Loss: 0.00001016
Iteration 159/1000 | Loss: 0.00001016
Iteration 160/1000 | Loss: 0.00001016
Iteration 161/1000 | Loss: 0.00001016
Iteration 162/1000 | Loss: 0.00001016
Iteration 163/1000 | Loss: 0.00001015
Iteration 164/1000 | Loss: 0.00001015
Iteration 165/1000 | Loss: 0.00001015
Iteration 166/1000 | Loss: 0.00001015
Iteration 167/1000 | Loss: 0.00001015
Iteration 168/1000 | Loss: 0.00001015
Iteration 169/1000 | Loss: 0.00001015
Iteration 170/1000 | Loss: 0.00001015
Iteration 171/1000 | Loss: 0.00001015
Iteration 172/1000 | Loss: 0.00001015
Iteration 173/1000 | Loss: 0.00001014
Iteration 174/1000 | Loss: 0.00001014
Iteration 175/1000 | Loss: 0.00001014
Iteration 176/1000 | Loss: 0.00001014
Iteration 177/1000 | Loss: 0.00001014
Iteration 178/1000 | Loss: 0.00001014
Iteration 179/1000 | Loss: 0.00001014
Iteration 180/1000 | Loss: 0.00001014
Iteration 181/1000 | Loss: 0.00001014
Iteration 182/1000 | Loss: 0.00001014
Iteration 183/1000 | Loss: 0.00001014
Iteration 184/1000 | Loss: 0.00001014
Iteration 185/1000 | Loss: 0.00001014
Iteration 186/1000 | Loss: 0.00001014
Iteration 187/1000 | Loss: 0.00001014
Iteration 188/1000 | Loss: 0.00001014
Iteration 189/1000 | Loss: 0.00001014
Iteration 190/1000 | Loss: 0.00001014
Iteration 191/1000 | Loss: 0.00001014
Iteration 192/1000 | Loss: 0.00001014
Iteration 193/1000 | Loss: 0.00001014
Iteration 194/1000 | Loss: 0.00001014
Iteration 195/1000 | Loss: 0.00001013
Iteration 196/1000 | Loss: 0.00001013
Iteration 197/1000 | Loss: 0.00001013
Iteration 198/1000 | Loss: 0.00001013
Iteration 199/1000 | Loss: 0.00001013
Iteration 200/1000 | Loss: 0.00001013
Iteration 201/1000 | Loss: 0.00001013
Iteration 202/1000 | Loss: 0.00001013
Iteration 203/1000 | Loss: 0.00001013
Iteration 204/1000 | Loss: 0.00001013
Iteration 205/1000 | Loss: 0.00001013
Iteration 206/1000 | Loss: 0.00001013
Iteration 207/1000 | Loss: 0.00001013
Iteration 208/1000 | Loss: 0.00001013
Iteration 209/1000 | Loss: 0.00001013
Iteration 210/1000 | Loss: 0.00001013
Iteration 211/1000 | Loss: 0.00001013
Iteration 212/1000 | Loss: 0.00001013
Iteration 213/1000 | Loss: 0.00001013
Iteration 214/1000 | Loss: 0.00001013
Iteration 215/1000 | Loss: 0.00001013
Iteration 216/1000 | Loss: 0.00001013
Iteration 217/1000 | Loss: 0.00001013
Iteration 218/1000 | Loss: 0.00001013
Iteration 219/1000 | Loss: 0.00001013
Iteration 220/1000 | Loss: 0.00001013
Iteration 221/1000 | Loss: 0.00001013
Iteration 222/1000 | Loss: 0.00001013
Iteration 223/1000 | Loss: 0.00001013
Iteration 224/1000 | Loss: 0.00001013
Iteration 225/1000 | Loss: 0.00001013
Iteration 226/1000 | Loss: 0.00001013
Iteration 227/1000 | Loss: 0.00001013
Iteration 228/1000 | Loss: 0.00001013
Iteration 229/1000 | Loss: 0.00001013
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.0125921107828617e-05, 1.0125921107828617e-05, 1.0125921107828617e-05, 1.0125921107828617e-05, 1.0125921107828617e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0125921107828617e-05

Optimization complete. Final v2v error: 2.6829047203063965 mm

Highest mean error: 2.8349952697753906 mm for frame 93

Lowest mean error: 2.5601539611816406 mm for frame 146

Saving results

Total time: 37.09170198440552
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_006/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_006/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00936785
Iteration 2/25 | Loss: 0.00122993
Iteration 3/25 | Loss: 0.00078961
Iteration 4/25 | Loss: 0.00073570
Iteration 5/25 | Loss: 0.00072201
Iteration 6/25 | Loss: 0.00072031
Iteration 7/25 | Loss: 0.00072011
Iteration 8/25 | Loss: 0.00072011
Iteration 9/25 | Loss: 0.00072011
Iteration 10/25 | Loss: 0.00072011
Iteration 11/25 | Loss: 0.00072011
Iteration 12/25 | Loss: 0.00072011
Iteration 13/25 | Loss: 0.00072011
Iteration 14/25 | Loss: 0.00072011
Iteration 15/25 | Loss: 0.00072011
Iteration 16/25 | Loss: 0.00072011
Iteration 17/25 | Loss: 0.00072011
Iteration 18/25 | Loss: 0.00072011
Iteration 19/25 | Loss: 0.00072011
Iteration 20/25 | Loss: 0.00072011
Iteration 21/25 | Loss: 0.00072011
Iteration 22/25 | Loss: 0.00072011
Iteration 23/25 | Loss: 0.00072011
Iteration 24/25 | Loss: 0.00072011
Iteration 25/25 | Loss: 0.00072011

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75580436
Iteration 2/25 | Loss: 0.00020398
Iteration 3/25 | Loss: 0.00020398
Iteration 4/25 | Loss: 0.00020397
Iteration 5/25 | Loss: 0.00020397
Iteration 6/25 | Loss: 0.00020397
Iteration 7/25 | Loss: 0.00020397
Iteration 8/25 | Loss: 0.00020397
Iteration 9/25 | Loss: 0.00020397
Iteration 10/25 | Loss: 0.00020397
Iteration 11/25 | Loss: 0.00020397
Iteration 12/25 | Loss: 0.00020397
Iteration 13/25 | Loss: 0.00020397
Iteration 14/25 | Loss: 0.00020397
Iteration 15/25 | Loss: 0.00020397
Iteration 16/25 | Loss: 0.00020397
Iteration 17/25 | Loss: 0.00020397
Iteration 18/25 | Loss: 0.00020397
Iteration 19/25 | Loss: 0.00020397
Iteration 20/25 | Loss: 0.00020397
Iteration 21/25 | Loss: 0.00020397
Iteration 22/25 | Loss: 0.00020397
Iteration 23/25 | Loss: 0.00020397
Iteration 24/25 | Loss: 0.00020397
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0002039741084445268, 0.0002039741084445268, 0.0002039741084445268, 0.0002039741084445268, 0.0002039741084445268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002039741084445268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020397
Iteration 2/1000 | Loss: 0.00003765
Iteration 3/1000 | Loss: 0.00002866
Iteration 4/1000 | Loss: 0.00002566
Iteration 5/1000 | Loss: 0.00002436
Iteration 6/1000 | Loss: 0.00002355
Iteration 7/1000 | Loss: 0.00002288
Iteration 8/1000 | Loss: 0.00002238
Iteration 9/1000 | Loss: 0.00002208
Iteration 10/1000 | Loss: 0.00002189
Iteration 11/1000 | Loss: 0.00002167
Iteration 12/1000 | Loss: 0.00002145
Iteration 13/1000 | Loss: 0.00002129
Iteration 14/1000 | Loss: 0.00002122
Iteration 15/1000 | Loss: 0.00002120
Iteration 16/1000 | Loss: 0.00002119
Iteration 17/1000 | Loss: 0.00002119
Iteration 18/1000 | Loss: 0.00002119
Iteration 19/1000 | Loss: 0.00002119
Iteration 20/1000 | Loss: 0.00002119
Iteration 21/1000 | Loss: 0.00002119
Iteration 22/1000 | Loss: 0.00002119
Iteration 23/1000 | Loss: 0.00002118
Iteration 24/1000 | Loss: 0.00002118
Iteration 25/1000 | Loss: 0.00002118
Iteration 26/1000 | Loss: 0.00002118
Iteration 27/1000 | Loss: 0.00002118
Iteration 28/1000 | Loss: 0.00002118
Iteration 29/1000 | Loss: 0.00002118
Iteration 30/1000 | Loss: 0.00002117
Iteration 31/1000 | Loss: 0.00002116
Iteration 32/1000 | Loss: 0.00002116
Iteration 33/1000 | Loss: 0.00002115
Iteration 34/1000 | Loss: 0.00002115
Iteration 35/1000 | Loss: 0.00002114
Iteration 36/1000 | Loss: 0.00002114
Iteration 37/1000 | Loss: 0.00002114
Iteration 38/1000 | Loss: 0.00002114
Iteration 39/1000 | Loss: 0.00002114
Iteration 40/1000 | Loss: 0.00002113
Iteration 41/1000 | Loss: 0.00002113
Iteration 42/1000 | Loss: 0.00002113
Iteration 43/1000 | Loss: 0.00002113
Iteration 44/1000 | Loss: 0.00002113
Iteration 45/1000 | Loss: 0.00002112
Iteration 46/1000 | Loss: 0.00002110
Iteration 47/1000 | Loss: 0.00002110
Iteration 48/1000 | Loss: 0.00002110
Iteration 49/1000 | Loss: 0.00002110
Iteration 50/1000 | Loss: 0.00002110
Iteration 51/1000 | Loss: 0.00002110
Iteration 52/1000 | Loss: 0.00002110
Iteration 53/1000 | Loss: 0.00002110
Iteration 54/1000 | Loss: 0.00002110
Iteration 55/1000 | Loss: 0.00002109
Iteration 56/1000 | Loss: 0.00002109
Iteration 57/1000 | Loss: 0.00002109
Iteration 58/1000 | Loss: 0.00002109
Iteration 59/1000 | Loss: 0.00002109
Iteration 60/1000 | Loss: 0.00002109
Iteration 61/1000 | Loss: 0.00002108
Iteration 62/1000 | Loss: 0.00002107
Iteration 63/1000 | Loss: 0.00002105
Iteration 64/1000 | Loss: 0.00002105
Iteration 65/1000 | Loss: 0.00002105
Iteration 66/1000 | Loss: 0.00002105
Iteration 67/1000 | Loss: 0.00002105
Iteration 68/1000 | Loss: 0.00002104
Iteration 69/1000 | Loss: 0.00002104
Iteration 70/1000 | Loss: 0.00002104
Iteration 71/1000 | Loss: 0.00002104
Iteration 72/1000 | Loss: 0.00002103
Iteration 73/1000 | Loss: 0.00002101
Iteration 74/1000 | Loss: 0.00002099
Iteration 75/1000 | Loss: 0.00002099
Iteration 76/1000 | Loss: 0.00002099
Iteration 77/1000 | Loss: 0.00002099
Iteration 78/1000 | Loss: 0.00002099
Iteration 79/1000 | Loss: 0.00002099
Iteration 80/1000 | Loss: 0.00002099
Iteration 81/1000 | Loss: 0.00002099
Iteration 82/1000 | Loss: 0.00002098
Iteration 83/1000 | Loss: 0.00002096
Iteration 84/1000 | Loss: 0.00002095
Iteration 85/1000 | Loss: 0.00002095
Iteration 86/1000 | Loss: 0.00002095
Iteration 87/1000 | Loss: 0.00002095
Iteration 88/1000 | Loss: 0.00002095
Iteration 89/1000 | Loss: 0.00002094
Iteration 90/1000 | Loss: 0.00002094
Iteration 91/1000 | Loss: 0.00002094
Iteration 92/1000 | Loss: 0.00002094
Iteration 93/1000 | Loss: 0.00002094
Iteration 94/1000 | Loss: 0.00002094
Iteration 95/1000 | Loss: 0.00002094
Iteration 96/1000 | Loss: 0.00002094
Iteration 97/1000 | Loss: 0.00002094
Iteration 98/1000 | Loss: 0.00002093
Iteration 99/1000 | Loss: 0.00002092
Iteration 100/1000 | Loss: 0.00002092
Iteration 101/1000 | Loss: 0.00002091
Iteration 102/1000 | Loss: 0.00002091
Iteration 103/1000 | Loss: 0.00002091
Iteration 104/1000 | Loss: 0.00002091
Iteration 105/1000 | Loss: 0.00002091
Iteration 106/1000 | Loss: 0.00002090
Iteration 107/1000 | Loss: 0.00002090
Iteration 108/1000 | Loss: 0.00002090
Iteration 109/1000 | Loss: 0.00002090
Iteration 110/1000 | Loss: 0.00002090
Iteration 111/1000 | Loss: 0.00002090
Iteration 112/1000 | Loss: 0.00002090
Iteration 113/1000 | Loss: 0.00002090
Iteration 114/1000 | Loss: 0.00002090
Iteration 115/1000 | Loss: 0.00002089
Iteration 116/1000 | Loss: 0.00002089
Iteration 117/1000 | Loss: 0.00002089
Iteration 118/1000 | Loss: 0.00002089
Iteration 119/1000 | Loss: 0.00002088
Iteration 120/1000 | Loss: 0.00002088
Iteration 121/1000 | Loss: 0.00002087
Iteration 122/1000 | Loss: 0.00002086
Iteration 123/1000 | Loss: 0.00002085
Iteration 124/1000 | Loss: 0.00002085
Iteration 125/1000 | Loss: 0.00002084
Iteration 126/1000 | Loss: 0.00002084
Iteration 127/1000 | Loss: 0.00002083
Iteration 128/1000 | Loss: 0.00002083
Iteration 129/1000 | Loss: 0.00002082
Iteration 130/1000 | Loss: 0.00002082
Iteration 131/1000 | Loss: 0.00002081
Iteration 132/1000 | Loss: 0.00002081
Iteration 133/1000 | Loss: 0.00002081
Iteration 134/1000 | Loss: 0.00002080
Iteration 135/1000 | Loss: 0.00002080
Iteration 136/1000 | Loss: 0.00002079
Iteration 137/1000 | Loss: 0.00002079
Iteration 138/1000 | Loss: 0.00002079
Iteration 139/1000 | Loss: 0.00002078
Iteration 140/1000 | Loss: 0.00002078
Iteration 141/1000 | Loss: 0.00002077
Iteration 142/1000 | Loss: 0.00002077
Iteration 143/1000 | Loss: 0.00002077
Iteration 144/1000 | Loss: 0.00002077
Iteration 145/1000 | Loss: 0.00002077
Iteration 146/1000 | Loss: 0.00002077
Iteration 147/1000 | Loss: 0.00002076
Iteration 148/1000 | Loss: 0.00002076
Iteration 149/1000 | Loss: 0.00002076
Iteration 150/1000 | Loss: 0.00002076
Iteration 151/1000 | Loss: 0.00002076
Iteration 152/1000 | Loss: 0.00002076
Iteration 153/1000 | Loss: 0.00002074
Iteration 154/1000 | Loss: 0.00002072
Iteration 155/1000 | Loss: 0.00002072
Iteration 156/1000 | Loss: 0.00002072
Iteration 157/1000 | Loss: 0.00002072
Iteration 158/1000 | Loss: 0.00002072
Iteration 159/1000 | Loss: 0.00002072
Iteration 160/1000 | Loss: 0.00002072
Iteration 161/1000 | Loss: 0.00002072
Iteration 162/1000 | Loss: 0.00002072
Iteration 163/1000 | Loss: 0.00002072
Iteration 164/1000 | Loss: 0.00002071
Iteration 165/1000 | Loss: 0.00002071
Iteration 166/1000 | Loss: 0.00002071
Iteration 167/1000 | Loss: 0.00002071
Iteration 168/1000 | Loss: 0.00002071
Iteration 169/1000 | Loss: 0.00002071
Iteration 170/1000 | Loss: 0.00002071
Iteration 171/1000 | Loss: 0.00002071
Iteration 172/1000 | Loss: 0.00002071
Iteration 173/1000 | Loss: 0.00002071
Iteration 174/1000 | Loss: 0.00002071
Iteration 175/1000 | Loss: 0.00002071
Iteration 176/1000 | Loss: 0.00002071
Iteration 177/1000 | Loss: 0.00002071
Iteration 178/1000 | Loss: 0.00002071
Iteration 179/1000 | Loss: 0.00002070
Iteration 180/1000 | Loss: 0.00002070
Iteration 181/1000 | Loss: 0.00002070
Iteration 182/1000 | Loss: 0.00002070
Iteration 183/1000 | Loss: 0.00002069
Iteration 184/1000 | Loss: 0.00002069
Iteration 185/1000 | Loss: 0.00002069
Iteration 186/1000 | Loss: 0.00002069
Iteration 187/1000 | Loss: 0.00002069
Iteration 188/1000 | Loss: 0.00002069
Iteration 189/1000 | Loss: 0.00002069
Iteration 190/1000 | Loss: 0.00002069
Iteration 191/1000 | Loss: 0.00002069
Iteration 192/1000 | Loss: 0.00002069
Iteration 193/1000 | Loss: 0.00002069
Iteration 194/1000 | Loss: 0.00002069
Iteration 195/1000 | Loss: 0.00002069
Iteration 196/1000 | Loss: 0.00002069
Iteration 197/1000 | Loss: 0.00002068
Iteration 198/1000 | Loss: 0.00002068
Iteration 199/1000 | Loss: 0.00002068
Iteration 200/1000 | Loss: 0.00002068
Iteration 201/1000 | Loss: 0.00002068
Iteration 202/1000 | Loss: 0.00002068
Iteration 203/1000 | Loss: 0.00002067
Iteration 204/1000 | Loss: 0.00002067
Iteration 205/1000 | Loss: 0.00002067
Iteration 206/1000 | Loss: 0.00002067
Iteration 207/1000 | Loss: 0.00002067
Iteration 208/1000 | Loss: 0.00002067
Iteration 209/1000 | Loss: 0.00002067
Iteration 210/1000 | Loss: 0.00002067
Iteration 211/1000 | Loss: 0.00002067
Iteration 212/1000 | Loss: 0.00002066
Iteration 213/1000 | Loss: 0.00002066
Iteration 214/1000 | Loss: 0.00002066
Iteration 215/1000 | Loss: 0.00002066
Iteration 216/1000 | Loss: 0.00002065
Iteration 217/1000 | Loss: 0.00002065
Iteration 218/1000 | Loss: 0.00002065
Iteration 219/1000 | Loss: 0.00002065
Iteration 220/1000 | Loss: 0.00002065
Iteration 221/1000 | Loss: 0.00002065
Iteration 222/1000 | Loss: 0.00002065
Iteration 223/1000 | Loss: 0.00002065
Iteration 224/1000 | Loss: 0.00002065
Iteration 225/1000 | Loss: 0.00002065
Iteration 226/1000 | Loss: 0.00002065
Iteration 227/1000 | Loss: 0.00002065
Iteration 228/1000 | Loss: 0.00002065
Iteration 229/1000 | Loss: 0.00002065
Iteration 230/1000 | Loss: 0.00002065
Iteration 231/1000 | Loss: 0.00002065
Iteration 232/1000 | Loss: 0.00002065
Iteration 233/1000 | Loss: 0.00002064
Iteration 234/1000 | Loss: 0.00002064
Iteration 235/1000 | Loss: 0.00002064
Iteration 236/1000 | Loss: 0.00002063
Iteration 237/1000 | Loss: 0.00002063
Iteration 238/1000 | Loss: 0.00002063
Iteration 239/1000 | Loss: 0.00002063
Iteration 240/1000 | Loss: 0.00002063
Iteration 241/1000 | Loss: 0.00002063
Iteration 242/1000 | Loss: 0.00002063
Iteration 243/1000 | Loss: 0.00002063
Iteration 244/1000 | Loss: 0.00002062
Iteration 245/1000 | Loss: 0.00002062
Iteration 246/1000 | Loss: 0.00002062
Iteration 247/1000 | Loss: 0.00002062
Iteration 248/1000 | Loss: 0.00002062
Iteration 249/1000 | Loss: 0.00002062
Iteration 250/1000 | Loss: 0.00002062
Iteration 251/1000 | Loss: 0.00002062
Iteration 252/1000 | Loss: 0.00002062
Iteration 253/1000 | Loss: 0.00002062
Iteration 254/1000 | Loss: 0.00002062
Iteration 255/1000 | Loss: 0.00002062
Iteration 256/1000 | Loss: 0.00002062
Iteration 257/1000 | Loss: 0.00002062
Iteration 258/1000 | Loss: 0.00002062
Iteration 259/1000 | Loss: 0.00002062
Iteration 260/1000 | Loss: 0.00002062
Iteration 261/1000 | Loss: 0.00002062
Iteration 262/1000 | Loss: 0.00002062
Iteration 263/1000 | Loss: 0.00002062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [2.0615059838746674e-05, 2.0615059838746674e-05, 2.0615059838746674e-05, 2.0615059838746674e-05, 2.0615059838746674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0615059838746674e-05

Optimization complete. Final v2v error: 3.8045434951782227 mm

Highest mean error: 4.040183067321777 mm for frame 60

Lowest mean error: 3.5336568355560303 mm for frame 162

Saving results

Total time: 47.38316035270691
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788312
Iteration 2/25 | Loss: 0.00100870
Iteration 3/25 | Loss: 0.00089529
Iteration 4/25 | Loss: 0.00086479
Iteration 5/25 | Loss: 0.00085472
Iteration 6/25 | Loss: 0.00085276
Iteration 7/25 | Loss: 0.00085192
Iteration 8/25 | Loss: 0.00085190
Iteration 9/25 | Loss: 0.00085190
Iteration 10/25 | Loss: 0.00085190
Iteration 11/25 | Loss: 0.00085190
Iteration 12/25 | Loss: 0.00085190
Iteration 13/25 | Loss: 0.00085190
Iteration 14/25 | Loss: 0.00085190
Iteration 15/25 | Loss: 0.00085190
Iteration 16/25 | Loss: 0.00085190
Iteration 17/25 | Loss: 0.00085190
Iteration 18/25 | Loss: 0.00085190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008519018301740289, 0.0008519018301740289, 0.0008519018301740289, 0.0008519018301740289, 0.0008519018301740289]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008519018301740289

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64845014
Iteration 2/25 | Loss: 0.00202179
Iteration 3/25 | Loss: 0.00202179
Iteration 4/25 | Loss: 0.00202179
Iteration 5/25 | Loss: 0.00202179
Iteration 6/25 | Loss: 0.00202179
Iteration 7/25 | Loss: 0.00202179
Iteration 8/25 | Loss: 0.00202179
Iteration 9/25 | Loss: 0.00202179
Iteration 10/25 | Loss: 0.00202179
Iteration 11/25 | Loss: 0.00202179
Iteration 12/25 | Loss: 0.00202179
Iteration 13/25 | Loss: 0.00202179
Iteration 14/25 | Loss: 0.00202179
Iteration 15/25 | Loss: 0.00202179
Iteration 16/25 | Loss: 0.00202179
Iteration 17/25 | Loss: 0.00202179
Iteration 18/25 | Loss: 0.00202179
Iteration 19/25 | Loss: 0.00202179
Iteration 20/25 | Loss: 0.00202179
Iteration 21/25 | Loss: 0.00202179
Iteration 22/25 | Loss: 0.00202179
Iteration 23/25 | Loss: 0.00202179
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0020217886194586754, 0.0020217886194586754, 0.0020217886194586754, 0.0020217886194586754, 0.0020217886194586754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020217886194586754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202179
Iteration 2/1000 | Loss: 0.00004143
Iteration 3/1000 | Loss: 0.00002797
Iteration 4/1000 | Loss: 0.00002511
Iteration 5/1000 | Loss: 0.00002416
Iteration 6/1000 | Loss: 0.00002353
Iteration 7/1000 | Loss: 0.00002310
Iteration 8/1000 | Loss: 0.00002277
Iteration 9/1000 | Loss: 0.00002265
Iteration 10/1000 | Loss: 0.00002264
Iteration 11/1000 | Loss: 0.00002258
Iteration 12/1000 | Loss: 0.00002255
Iteration 13/1000 | Loss: 0.00002254
Iteration 14/1000 | Loss: 0.00002254
Iteration 15/1000 | Loss: 0.00002248
Iteration 16/1000 | Loss: 0.00002239
Iteration 17/1000 | Loss: 0.00002237
Iteration 18/1000 | Loss: 0.00002237
Iteration 19/1000 | Loss: 0.00002236
Iteration 20/1000 | Loss: 0.00002236
Iteration 21/1000 | Loss: 0.00002235
Iteration 22/1000 | Loss: 0.00002234
Iteration 23/1000 | Loss: 0.00002234
Iteration 24/1000 | Loss: 0.00002234
Iteration 25/1000 | Loss: 0.00002233
Iteration 26/1000 | Loss: 0.00002233
Iteration 27/1000 | Loss: 0.00002233
Iteration 28/1000 | Loss: 0.00002232
Iteration 29/1000 | Loss: 0.00002232
Iteration 30/1000 | Loss: 0.00002232
Iteration 31/1000 | Loss: 0.00002232
Iteration 32/1000 | Loss: 0.00002232
Iteration 33/1000 | Loss: 0.00002232
Iteration 34/1000 | Loss: 0.00002232
Iteration 35/1000 | Loss: 0.00002232
Iteration 36/1000 | Loss: 0.00002232
Iteration 37/1000 | Loss: 0.00002232
Iteration 38/1000 | Loss: 0.00002231
Iteration 39/1000 | Loss: 0.00002231
Iteration 40/1000 | Loss: 0.00002231
Iteration 41/1000 | Loss: 0.00002231
Iteration 42/1000 | Loss: 0.00002230
Iteration 43/1000 | Loss: 0.00002230
Iteration 44/1000 | Loss: 0.00002230
Iteration 45/1000 | Loss: 0.00002230
Iteration 46/1000 | Loss: 0.00002229
Iteration 47/1000 | Loss: 0.00002229
Iteration 48/1000 | Loss: 0.00002229
Iteration 49/1000 | Loss: 0.00002229
Iteration 50/1000 | Loss: 0.00002229
Iteration 51/1000 | Loss: 0.00002229
Iteration 52/1000 | Loss: 0.00002228
Iteration 53/1000 | Loss: 0.00002228
Iteration 54/1000 | Loss: 0.00002228
Iteration 55/1000 | Loss: 0.00002228
Iteration 56/1000 | Loss: 0.00002228
Iteration 57/1000 | Loss: 0.00002228
Iteration 58/1000 | Loss: 0.00002228
Iteration 59/1000 | Loss: 0.00002228
Iteration 60/1000 | Loss: 0.00002228
Iteration 61/1000 | Loss: 0.00002227
Iteration 62/1000 | Loss: 0.00002227
Iteration 63/1000 | Loss: 0.00002227
Iteration 64/1000 | Loss: 0.00002227
Iteration 65/1000 | Loss: 0.00002227
Iteration 66/1000 | Loss: 0.00002227
Iteration 67/1000 | Loss: 0.00002227
Iteration 68/1000 | Loss: 0.00002227
Iteration 69/1000 | Loss: 0.00002227
Iteration 70/1000 | Loss: 0.00002226
Iteration 71/1000 | Loss: 0.00002226
Iteration 72/1000 | Loss: 0.00002226
Iteration 73/1000 | Loss: 0.00002226
Iteration 74/1000 | Loss: 0.00002225
Iteration 75/1000 | Loss: 0.00002225
Iteration 76/1000 | Loss: 0.00002225
Iteration 77/1000 | Loss: 0.00002225
Iteration 78/1000 | Loss: 0.00002225
Iteration 79/1000 | Loss: 0.00002225
Iteration 80/1000 | Loss: 0.00002224
Iteration 81/1000 | Loss: 0.00002224
Iteration 82/1000 | Loss: 0.00002224
Iteration 83/1000 | Loss: 0.00002224
Iteration 84/1000 | Loss: 0.00002223
Iteration 85/1000 | Loss: 0.00002223
Iteration 86/1000 | Loss: 0.00002223
Iteration 87/1000 | Loss: 0.00002223
Iteration 88/1000 | Loss: 0.00002222
Iteration 89/1000 | Loss: 0.00002222
Iteration 90/1000 | Loss: 0.00002222
Iteration 91/1000 | Loss: 0.00002221
Iteration 92/1000 | Loss: 0.00002221
Iteration 93/1000 | Loss: 0.00002220
Iteration 94/1000 | Loss: 0.00002220
Iteration 95/1000 | Loss: 0.00002219
Iteration 96/1000 | Loss: 0.00002219
Iteration 97/1000 | Loss: 0.00002219
Iteration 98/1000 | Loss: 0.00002219
Iteration 99/1000 | Loss: 0.00002219
Iteration 100/1000 | Loss: 0.00002219
Iteration 101/1000 | Loss: 0.00002218
Iteration 102/1000 | Loss: 0.00002218
Iteration 103/1000 | Loss: 0.00002217
Iteration 104/1000 | Loss: 0.00002217
Iteration 105/1000 | Loss: 0.00002217
Iteration 106/1000 | Loss: 0.00002217
Iteration 107/1000 | Loss: 0.00002217
Iteration 108/1000 | Loss: 0.00002217
Iteration 109/1000 | Loss: 0.00002216
Iteration 110/1000 | Loss: 0.00002216
Iteration 111/1000 | Loss: 0.00002216
Iteration 112/1000 | Loss: 0.00002216
Iteration 113/1000 | Loss: 0.00002216
Iteration 114/1000 | Loss: 0.00002215
Iteration 115/1000 | Loss: 0.00002214
Iteration 116/1000 | Loss: 0.00002214
Iteration 117/1000 | Loss: 0.00002214
Iteration 118/1000 | Loss: 0.00002214
Iteration 119/1000 | Loss: 0.00002214
Iteration 120/1000 | Loss: 0.00002214
Iteration 121/1000 | Loss: 0.00002214
Iteration 122/1000 | Loss: 0.00002214
Iteration 123/1000 | Loss: 0.00002213
Iteration 124/1000 | Loss: 0.00002213
Iteration 125/1000 | Loss: 0.00002213
Iteration 126/1000 | Loss: 0.00002213
Iteration 127/1000 | Loss: 0.00002213
Iteration 128/1000 | Loss: 0.00002212
Iteration 129/1000 | Loss: 0.00002212
Iteration 130/1000 | Loss: 0.00002212
Iteration 131/1000 | Loss: 0.00002212
Iteration 132/1000 | Loss: 0.00002211
Iteration 133/1000 | Loss: 0.00002211
Iteration 134/1000 | Loss: 0.00002211
Iteration 135/1000 | Loss: 0.00002211
Iteration 136/1000 | Loss: 0.00002211
Iteration 137/1000 | Loss: 0.00002211
Iteration 138/1000 | Loss: 0.00002211
Iteration 139/1000 | Loss: 0.00002211
Iteration 140/1000 | Loss: 0.00002211
Iteration 141/1000 | Loss: 0.00002211
Iteration 142/1000 | Loss: 0.00002211
Iteration 143/1000 | Loss: 0.00002211
Iteration 144/1000 | Loss: 0.00002211
Iteration 145/1000 | Loss: 0.00002211
Iteration 146/1000 | Loss: 0.00002211
Iteration 147/1000 | Loss: 0.00002211
Iteration 148/1000 | Loss: 0.00002211
Iteration 149/1000 | Loss: 0.00002211
Iteration 150/1000 | Loss: 0.00002211
Iteration 151/1000 | Loss: 0.00002211
Iteration 152/1000 | Loss: 0.00002211
Iteration 153/1000 | Loss: 0.00002211
Iteration 154/1000 | Loss: 0.00002211
Iteration 155/1000 | Loss: 0.00002211
Iteration 156/1000 | Loss: 0.00002211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.211035643995274e-05, 2.211035643995274e-05, 2.211035643995274e-05, 2.211035643995274e-05, 2.211035643995274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.211035643995274e-05

Optimization complete. Final v2v error: 3.994645118713379 mm

Highest mean error: 4.526119232177734 mm for frame 114

Lowest mean error: 3.7128653526306152 mm for frame 19

Saving results

Total time: 35.9208037853241
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01076021
Iteration 2/25 | Loss: 0.00131721
Iteration 3/25 | Loss: 0.00099716
Iteration 4/25 | Loss: 0.00089184
Iteration 5/25 | Loss: 0.00087439
Iteration 6/25 | Loss: 0.00087322
Iteration 7/25 | Loss: 0.00087607
Iteration 8/25 | Loss: 0.00087653
Iteration 9/25 | Loss: 0.00086909
Iteration 10/25 | Loss: 0.00086845
Iteration 11/25 | Loss: 0.00086814
Iteration 12/25 | Loss: 0.00086800
Iteration 13/25 | Loss: 0.00086788
Iteration 14/25 | Loss: 0.00086788
Iteration 15/25 | Loss: 0.00086787
Iteration 16/25 | Loss: 0.00086787
Iteration 17/25 | Loss: 0.00086787
Iteration 18/25 | Loss: 0.00086787
Iteration 19/25 | Loss: 0.00086787
Iteration 20/25 | Loss: 0.00086787
Iteration 21/25 | Loss: 0.00086787
Iteration 22/25 | Loss: 0.00086787
Iteration 23/25 | Loss: 0.00086787
Iteration 24/25 | Loss: 0.00086787
Iteration 25/25 | Loss: 0.00086787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.34814310
Iteration 2/25 | Loss: 0.00198911
Iteration 3/25 | Loss: 0.00198910
Iteration 4/25 | Loss: 0.00198910
Iteration 5/25 | Loss: 0.00198910
Iteration 6/25 | Loss: 0.00198910
Iteration 7/25 | Loss: 0.00198910
Iteration 8/25 | Loss: 0.00198910
Iteration 9/25 | Loss: 0.00198910
Iteration 10/25 | Loss: 0.00198910
Iteration 11/25 | Loss: 0.00198910
Iteration 12/25 | Loss: 0.00198910
Iteration 13/25 | Loss: 0.00198910
Iteration 14/25 | Loss: 0.00198910
Iteration 15/25 | Loss: 0.00198910
Iteration 16/25 | Loss: 0.00198910
Iteration 17/25 | Loss: 0.00198910
Iteration 18/25 | Loss: 0.00198910
Iteration 19/25 | Loss: 0.00198910
Iteration 20/25 | Loss: 0.00198910
Iteration 21/25 | Loss: 0.00198910
Iteration 22/25 | Loss: 0.00198910
Iteration 23/25 | Loss: 0.00198910
Iteration 24/25 | Loss: 0.00198910
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0019891008269041777, 0.0019891008269041777, 0.0019891008269041777, 0.0019891008269041777, 0.0019891008269041777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019891008269041777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198910
Iteration 2/1000 | Loss: 0.00002714
Iteration 3/1000 | Loss: 0.00002001
Iteration 4/1000 | Loss: 0.00001813
Iteration 5/1000 | Loss: 0.00001734
Iteration 6/1000 | Loss: 0.00001690
Iteration 7/1000 | Loss: 0.00001649
Iteration 8/1000 | Loss: 0.00001628
Iteration 9/1000 | Loss: 0.00001623
Iteration 10/1000 | Loss: 0.00001622
Iteration 11/1000 | Loss: 0.00001614
Iteration 12/1000 | Loss: 0.00001607
Iteration 13/1000 | Loss: 0.00001602
Iteration 14/1000 | Loss: 0.00001600
Iteration 15/1000 | Loss: 0.00001600
Iteration 16/1000 | Loss: 0.00001599
Iteration 17/1000 | Loss: 0.00001597
Iteration 18/1000 | Loss: 0.00001596
Iteration 19/1000 | Loss: 0.00001596
Iteration 20/1000 | Loss: 0.00001595
Iteration 21/1000 | Loss: 0.00001595
Iteration 22/1000 | Loss: 0.00001595
Iteration 23/1000 | Loss: 0.00001595
Iteration 24/1000 | Loss: 0.00001595
Iteration 25/1000 | Loss: 0.00001594
Iteration 26/1000 | Loss: 0.00001594
Iteration 27/1000 | Loss: 0.00001591
Iteration 28/1000 | Loss: 0.00001591
Iteration 29/1000 | Loss: 0.00001590
Iteration 30/1000 | Loss: 0.00001590
Iteration 31/1000 | Loss: 0.00001590
Iteration 32/1000 | Loss: 0.00001589
Iteration 33/1000 | Loss: 0.00001589
Iteration 34/1000 | Loss: 0.00001588
Iteration 35/1000 | Loss: 0.00001587
Iteration 36/1000 | Loss: 0.00001587
Iteration 37/1000 | Loss: 0.00001587
Iteration 38/1000 | Loss: 0.00001587
Iteration 39/1000 | Loss: 0.00001587
Iteration 40/1000 | Loss: 0.00001587
Iteration 41/1000 | Loss: 0.00001586
Iteration 42/1000 | Loss: 0.00001586
Iteration 43/1000 | Loss: 0.00001586
Iteration 44/1000 | Loss: 0.00001586
Iteration 45/1000 | Loss: 0.00001586
Iteration 46/1000 | Loss: 0.00001586
Iteration 47/1000 | Loss: 0.00001586
Iteration 48/1000 | Loss: 0.00001586
Iteration 49/1000 | Loss: 0.00001586
Iteration 50/1000 | Loss: 0.00001586
Iteration 51/1000 | Loss: 0.00001585
Iteration 52/1000 | Loss: 0.00001585
Iteration 53/1000 | Loss: 0.00001585
Iteration 54/1000 | Loss: 0.00001585
Iteration 55/1000 | Loss: 0.00001584
Iteration 56/1000 | Loss: 0.00001584
Iteration 57/1000 | Loss: 0.00001584
Iteration 58/1000 | Loss: 0.00001584
Iteration 59/1000 | Loss: 0.00001584
Iteration 60/1000 | Loss: 0.00001584
Iteration 61/1000 | Loss: 0.00001584
Iteration 62/1000 | Loss: 0.00001583
Iteration 63/1000 | Loss: 0.00001583
Iteration 64/1000 | Loss: 0.00001583
Iteration 65/1000 | Loss: 0.00001583
Iteration 66/1000 | Loss: 0.00001583
Iteration 67/1000 | Loss: 0.00001583
Iteration 68/1000 | Loss: 0.00001583
Iteration 69/1000 | Loss: 0.00001583
Iteration 70/1000 | Loss: 0.00001583
Iteration 71/1000 | Loss: 0.00001583
Iteration 72/1000 | Loss: 0.00001583
Iteration 73/1000 | Loss: 0.00001583
Iteration 74/1000 | Loss: 0.00001583
Iteration 75/1000 | Loss: 0.00001583
Iteration 76/1000 | Loss: 0.00001582
Iteration 77/1000 | Loss: 0.00001582
Iteration 78/1000 | Loss: 0.00001582
Iteration 79/1000 | Loss: 0.00001582
Iteration 80/1000 | Loss: 0.00001581
Iteration 81/1000 | Loss: 0.00001581
Iteration 82/1000 | Loss: 0.00001581
Iteration 83/1000 | Loss: 0.00001581
Iteration 84/1000 | Loss: 0.00001581
Iteration 85/1000 | Loss: 0.00001580
Iteration 86/1000 | Loss: 0.00001580
Iteration 87/1000 | Loss: 0.00001580
Iteration 88/1000 | Loss: 0.00001580
Iteration 89/1000 | Loss: 0.00001579
Iteration 90/1000 | Loss: 0.00001579
Iteration 91/1000 | Loss: 0.00001579
Iteration 92/1000 | Loss: 0.00001579
Iteration 93/1000 | Loss: 0.00001579
Iteration 94/1000 | Loss: 0.00001579
Iteration 95/1000 | Loss: 0.00001579
Iteration 96/1000 | Loss: 0.00001579
Iteration 97/1000 | Loss: 0.00001578
Iteration 98/1000 | Loss: 0.00001578
Iteration 99/1000 | Loss: 0.00001578
Iteration 100/1000 | Loss: 0.00001578
Iteration 101/1000 | Loss: 0.00001578
Iteration 102/1000 | Loss: 0.00001578
Iteration 103/1000 | Loss: 0.00001578
Iteration 104/1000 | Loss: 0.00001578
Iteration 105/1000 | Loss: 0.00001578
Iteration 106/1000 | Loss: 0.00001578
Iteration 107/1000 | Loss: 0.00001578
Iteration 108/1000 | Loss: 0.00001578
Iteration 109/1000 | Loss: 0.00001578
Iteration 110/1000 | Loss: 0.00001578
Iteration 111/1000 | Loss: 0.00001578
Iteration 112/1000 | Loss: 0.00001578
Iteration 113/1000 | Loss: 0.00001578
Iteration 114/1000 | Loss: 0.00001578
Iteration 115/1000 | Loss: 0.00001578
Iteration 116/1000 | Loss: 0.00001578
Iteration 117/1000 | Loss: 0.00001578
Iteration 118/1000 | Loss: 0.00001578
Iteration 119/1000 | Loss: 0.00001578
Iteration 120/1000 | Loss: 0.00001578
Iteration 121/1000 | Loss: 0.00001578
Iteration 122/1000 | Loss: 0.00001578
Iteration 123/1000 | Loss: 0.00001578
Iteration 124/1000 | Loss: 0.00001578
Iteration 125/1000 | Loss: 0.00001578
Iteration 126/1000 | Loss: 0.00001578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.577585680934135e-05, 1.577585680934135e-05, 1.577585680934135e-05, 1.577585680934135e-05, 1.577585680934135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.577585680934135e-05

Optimization complete. Final v2v error: 3.275146722793579 mm

Highest mean error: 3.7607414722442627 mm for frame 172

Lowest mean error: 2.8258097171783447 mm for frame 196

Saving results

Total time: 47.720545291900635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480392
Iteration 2/25 | Loss: 0.00094571
Iteration 3/25 | Loss: 0.00082601
Iteration 4/25 | Loss: 0.00081190
Iteration 5/25 | Loss: 0.00080751
Iteration 6/25 | Loss: 0.00080606
Iteration 7/25 | Loss: 0.00080566
Iteration 8/25 | Loss: 0.00080566
Iteration 9/25 | Loss: 0.00080566
Iteration 10/25 | Loss: 0.00080566
Iteration 11/25 | Loss: 0.00080566
Iteration 12/25 | Loss: 0.00080566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008056566002778709, 0.0008056566002778709, 0.0008056566002778709, 0.0008056566002778709, 0.0008056566002778709]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008056566002778709

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65525734
Iteration 2/25 | Loss: 0.00179476
Iteration 3/25 | Loss: 0.00179474
Iteration 4/25 | Loss: 0.00179474
Iteration 5/25 | Loss: 0.00179474
Iteration 6/25 | Loss: 0.00179474
Iteration 7/25 | Loss: 0.00179474
Iteration 8/25 | Loss: 0.00179474
Iteration 9/25 | Loss: 0.00179474
Iteration 10/25 | Loss: 0.00179474
Iteration 11/25 | Loss: 0.00179474
Iteration 12/25 | Loss: 0.00179474
Iteration 13/25 | Loss: 0.00179474
Iteration 14/25 | Loss: 0.00179474
Iteration 15/25 | Loss: 0.00179474
Iteration 16/25 | Loss: 0.00179474
Iteration 17/25 | Loss: 0.00179474
Iteration 18/25 | Loss: 0.00179474
Iteration 19/25 | Loss: 0.00179474
Iteration 20/25 | Loss: 0.00179474
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001794740790501237, 0.001794740790501237, 0.001794740790501237, 0.001794740790501237, 0.001794740790501237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001794740790501237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179474
Iteration 2/1000 | Loss: 0.00003801
Iteration 3/1000 | Loss: 0.00002373
Iteration 4/1000 | Loss: 0.00001816
Iteration 5/1000 | Loss: 0.00001685
Iteration 6/1000 | Loss: 0.00001613
Iteration 7/1000 | Loss: 0.00001581
Iteration 8/1000 | Loss: 0.00001549
Iteration 9/1000 | Loss: 0.00001540
Iteration 10/1000 | Loss: 0.00001538
Iteration 11/1000 | Loss: 0.00001538
Iteration 12/1000 | Loss: 0.00001534
Iteration 13/1000 | Loss: 0.00001534
Iteration 14/1000 | Loss: 0.00001534
Iteration 15/1000 | Loss: 0.00001533
Iteration 16/1000 | Loss: 0.00001533
Iteration 17/1000 | Loss: 0.00001533
Iteration 18/1000 | Loss: 0.00001532
Iteration 19/1000 | Loss: 0.00001531
Iteration 20/1000 | Loss: 0.00001530
Iteration 21/1000 | Loss: 0.00001530
Iteration 22/1000 | Loss: 0.00001529
Iteration 23/1000 | Loss: 0.00001529
Iteration 24/1000 | Loss: 0.00001529
Iteration 25/1000 | Loss: 0.00001528
Iteration 26/1000 | Loss: 0.00001527
Iteration 27/1000 | Loss: 0.00001526
Iteration 28/1000 | Loss: 0.00001526
Iteration 29/1000 | Loss: 0.00001526
Iteration 30/1000 | Loss: 0.00001525
Iteration 31/1000 | Loss: 0.00001525
Iteration 32/1000 | Loss: 0.00001525
Iteration 33/1000 | Loss: 0.00001525
Iteration 34/1000 | Loss: 0.00001525
Iteration 35/1000 | Loss: 0.00001525
Iteration 36/1000 | Loss: 0.00001525
Iteration 37/1000 | Loss: 0.00001525
Iteration 38/1000 | Loss: 0.00001524
Iteration 39/1000 | Loss: 0.00001524
Iteration 40/1000 | Loss: 0.00001524
Iteration 41/1000 | Loss: 0.00001523
Iteration 42/1000 | Loss: 0.00001523
Iteration 43/1000 | Loss: 0.00001523
Iteration 44/1000 | Loss: 0.00001523
Iteration 45/1000 | Loss: 0.00001523
Iteration 46/1000 | Loss: 0.00001523
Iteration 47/1000 | Loss: 0.00001523
Iteration 48/1000 | Loss: 0.00001522
Iteration 49/1000 | Loss: 0.00001522
Iteration 50/1000 | Loss: 0.00001522
Iteration 51/1000 | Loss: 0.00001522
Iteration 52/1000 | Loss: 0.00001522
Iteration 53/1000 | Loss: 0.00001521
Iteration 54/1000 | Loss: 0.00001521
Iteration 55/1000 | Loss: 0.00001521
Iteration 56/1000 | Loss: 0.00001520
Iteration 57/1000 | Loss: 0.00001520
Iteration 58/1000 | Loss: 0.00001520
Iteration 59/1000 | Loss: 0.00001520
Iteration 60/1000 | Loss: 0.00001520
Iteration 61/1000 | Loss: 0.00001520
Iteration 62/1000 | Loss: 0.00001519
Iteration 63/1000 | Loss: 0.00001519
Iteration 64/1000 | Loss: 0.00001519
Iteration 65/1000 | Loss: 0.00001519
Iteration 66/1000 | Loss: 0.00001518
Iteration 67/1000 | Loss: 0.00001518
Iteration 68/1000 | Loss: 0.00001518
Iteration 69/1000 | Loss: 0.00001517
Iteration 70/1000 | Loss: 0.00001517
Iteration 71/1000 | Loss: 0.00001516
Iteration 72/1000 | Loss: 0.00001516
Iteration 73/1000 | Loss: 0.00001516
Iteration 74/1000 | Loss: 0.00001516
Iteration 75/1000 | Loss: 0.00001516
Iteration 76/1000 | Loss: 0.00001516
Iteration 77/1000 | Loss: 0.00001516
Iteration 78/1000 | Loss: 0.00001516
Iteration 79/1000 | Loss: 0.00001516
Iteration 80/1000 | Loss: 0.00001515
Iteration 81/1000 | Loss: 0.00001515
Iteration 82/1000 | Loss: 0.00001514
Iteration 83/1000 | Loss: 0.00001514
Iteration 84/1000 | Loss: 0.00001514
Iteration 85/1000 | Loss: 0.00001513
Iteration 86/1000 | Loss: 0.00001513
Iteration 87/1000 | Loss: 0.00001513
Iteration 88/1000 | Loss: 0.00001512
Iteration 89/1000 | Loss: 0.00001512
Iteration 90/1000 | Loss: 0.00001512
Iteration 91/1000 | Loss: 0.00001512
Iteration 92/1000 | Loss: 0.00001511
Iteration 93/1000 | Loss: 0.00001511
Iteration 94/1000 | Loss: 0.00001511
Iteration 95/1000 | Loss: 0.00001510
Iteration 96/1000 | Loss: 0.00001510
Iteration 97/1000 | Loss: 0.00001510
Iteration 98/1000 | Loss: 0.00001510
Iteration 99/1000 | Loss: 0.00001510
Iteration 100/1000 | Loss: 0.00001510
Iteration 101/1000 | Loss: 0.00001510
Iteration 102/1000 | Loss: 0.00001509
Iteration 103/1000 | Loss: 0.00001509
Iteration 104/1000 | Loss: 0.00001509
Iteration 105/1000 | Loss: 0.00001509
Iteration 106/1000 | Loss: 0.00001509
Iteration 107/1000 | Loss: 0.00001509
Iteration 108/1000 | Loss: 0.00001509
Iteration 109/1000 | Loss: 0.00001509
Iteration 110/1000 | Loss: 0.00001509
Iteration 111/1000 | Loss: 0.00001509
Iteration 112/1000 | Loss: 0.00001509
Iteration 113/1000 | Loss: 0.00001509
Iteration 114/1000 | Loss: 0.00001509
Iteration 115/1000 | Loss: 0.00001509
Iteration 116/1000 | Loss: 0.00001509
Iteration 117/1000 | Loss: 0.00001509
Iteration 118/1000 | Loss: 0.00001509
Iteration 119/1000 | Loss: 0.00001509
Iteration 120/1000 | Loss: 0.00001509
Iteration 121/1000 | Loss: 0.00001509
Iteration 122/1000 | Loss: 0.00001509
Iteration 123/1000 | Loss: 0.00001509
Iteration 124/1000 | Loss: 0.00001509
Iteration 125/1000 | Loss: 0.00001509
Iteration 126/1000 | Loss: 0.00001509
Iteration 127/1000 | Loss: 0.00001509
Iteration 128/1000 | Loss: 0.00001509
Iteration 129/1000 | Loss: 0.00001509
Iteration 130/1000 | Loss: 0.00001509
Iteration 131/1000 | Loss: 0.00001509
Iteration 132/1000 | Loss: 0.00001509
Iteration 133/1000 | Loss: 0.00001509
Iteration 134/1000 | Loss: 0.00001509
Iteration 135/1000 | Loss: 0.00001509
Iteration 136/1000 | Loss: 0.00001509
Iteration 137/1000 | Loss: 0.00001509
Iteration 138/1000 | Loss: 0.00001509
Iteration 139/1000 | Loss: 0.00001509
Iteration 140/1000 | Loss: 0.00001509
Iteration 141/1000 | Loss: 0.00001509
Iteration 142/1000 | Loss: 0.00001509
Iteration 143/1000 | Loss: 0.00001509
Iteration 144/1000 | Loss: 0.00001509
Iteration 145/1000 | Loss: 0.00001509
Iteration 146/1000 | Loss: 0.00001509
Iteration 147/1000 | Loss: 0.00001509
Iteration 148/1000 | Loss: 0.00001509
Iteration 149/1000 | Loss: 0.00001509
Iteration 150/1000 | Loss: 0.00001509
Iteration 151/1000 | Loss: 0.00001509
Iteration 152/1000 | Loss: 0.00001509
Iteration 153/1000 | Loss: 0.00001509
Iteration 154/1000 | Loss: 0.00001509
Iteration 155/1000 | Loss: 0.00001509
Iteration 156/1000 | Loss: 0.00001509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.5085339327924885e-05, 1.5085339327924885e-05, 1.5085339327924885e-05, 1.5085339327924885e-05, 1.5085339327924885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5085339327924885e-05

Optimization complete. Final v2v error: 3.1857879161834717 mm

Highest mean error: 3.767765760421753 mm for frame 59

Lowest mean error: 2.8145835399627686 mm for frame 19

Saving results

Total time: 32.57346510887146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00914356
Iteration 2/25 | Loss: 0.00141840
Iteration 3/25 | Loss: 0.00100293
Iteration 4/25 | Loss: 0.00094495
Iteration 5/25 | Loss: 0.00093818
Iteration 6/25 | Loss: 0.00093756
Iteration 7/25 | Loss: 0.00093756
Iteration 8/25 | Loss: 0.00093756
Iteration 9/25 | Loss: 0.00093756
Iteration 10/25 | Loss: 0.00093756
Iteration 11/25 | Loss: 0.00093756
Iteration 12/25 | Loss: 0.00093756
Iteration 13/25 | Loss: 0.00093756
Iteration 14/25 | Loss: 0.00093756
Iteration 15/25 | Loss: 0.00093756
Iteration 16/25 | Loss: 0.00093756
Iteration 17/25 | Loss: 0.00093756
Iteration 18/25 | Loss: 0.00093756
Iteration 19/25 | Loss: 0.00093756
Iteration 20/25 | Loss: 0.00093756
Iteration 21/25 | Loss: 0.00093756
Iteration 22/25 | Loss: 0.00093756
Iteration 23/25 | Loss: 0.00093756
Iteration 24/25 | Loss: 0.00093756
Iteration 25/25 | Loss: 0.00093756

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.28767872
Iteration 2/25 | Loss: 0.00198081
Iteration 3/25 | Loss: 0.00198079
Iteration 4/25 | Loss: 0.00198079
Iteration 5/25 | Loss: 0.00198079
Iteration 6/25 | Loss: 0.00198079
Iteration 7/25 | Loss: 0.00198079
Iteration 8/25 | Loss: 0.00198079
Iteration 9/25 | Loss: 0.00198079
Iteration 10/25 | Loss: 0.00198079
Iteration 11/25 | Loss: 0.00198079
Iteration 12/25 | Loss: 0.00198079
Iteration 13/25 | Loss: 0.00198079
Iteration 14/25 | Loss: 0.00198079
Iteration 15/25 | Loss: 0.00198079
Iteration 16/25 | Loss: 0.00198079
Iteration 17/25 | Loss: 0.00198079
Iteration 18/25 | Loss: 0.00198079
Iteration 19/25 | Loss: 0.00198079
Iteration 20/25 | Loss: 0.00198079
Iteration 21/25 | Loss: 0.00198079
Iteration 22/25 | Loss: 0.00198079
Iteration 23/25 | Loss: 0.00198079
Iteration 24/25 | Loss: 0.00198079
Iteration 25/25 | Loss: 0.00198079

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198079
Iteration 2/1000 | Loss: 0.00004316
Iteration 3/1000 | Loss: 0.00002675
Iteration 4/1000 | Loss: 0.00002233
Iteration 5/1000 | Loss: 0.00002061
Iteration 6/1000 | Loss: 0.00001922
Iteration 7/1000 | Loss: 0.00001879
Iteration 8/1000 | Loss: 0.00001840
Iteration 9/1000 | Loss: 0.00001823
Iteration 10/1000 | Loss: 0.00001815
Iteration 11/1000 | Loss: 0.00001813
Iteration 12/1000 | Loss: 0.00001808
Iteration 13/1000 | Loss: 0.00001804
Iteration 14/1000 | Loss: 0.00001799
Iteration 15/1000 | Loss: 0.00001795
Iteration 16/1000 | Loss: 0.00001794
Iteration 17/1000 | Loss: 0.00001794
Iteration 18/1000 | Loss: 0.00001793
Iteration 19/1000 | Loss: 0.00001791
Iteration 20/1000 | Loss: 0.00001788
Iteration 21/1000 | Loss: 0.00001787
Iteration 22/1000 | Loss: 0.00001787
Iteration 23/1000 | Loss: 0.00001786
Iteration 24/1000 | Loss: 0.00001786
Iteration 25/1000 | Loss: 0.00001786
Iteration 26/1000 | Loss: 0.00001786
Iteration 27/1000 | Loss: 0.00001786
Iteration 28/1000 | Loss: 0.00001786
Iteration 29/1000 | Loss: 0.00001785
Iteration 30/1000 | Loss: 0.00001785
Iteration 31/1000 | Loss: 0.00001785
Iteration 32/1000 | Loss: 0.00001785
Iteration 33/1000 | Loss: 0.00001785
Iteration 34/1000 | Loss: 0.00001785
Iteration 35/1000 | Loss: 0.00001785
Iteration 36/1000 | Loss: 0.00001784
Iteration 37/1000 | Loss: 0.00001784
Iteration 38/1000 | Loss: 0.00001784
Iteration 39/1000 | Loss: 0.00001784
Iteration 40/1000 | Loss: 0.00001784
Iteration 41/1000 | Loss: 0.00001784
Iteration 42/1000 | Loss: 0.00001784
Iteration 43/1000 | Loss: 0.00001784
Iteration 44/1000 | Loss: 0.00001784
Iteration 45/1000 | Loss: 0.00001783
Iteration 46/1000 | Loss: 0.00001783
Iteration 47/1000 | Loss: 0.00001783
Iteration 48/1000 | Loss: 0.00001783
Iteration 49/1000 | Loss: 0.00001783
Iteration 50/1000 | Loss: 0.00001783
Iteration 51/1000 | Loss: 0.00001783
Iteration 52/1000 | Loss: 0.00001783
Iteration 53/1000 | Loss: 0.00001783
Iteration 54/1000 | Loss: 0.00001783
Iteration 55/1000 | Loss: 0.00001783
Iteration 56/1000 | Loss: 0.00001783
Iteration 57/1000 | Loss: 0.00001782
Iteration 58/1000 | Loss: 0.00001782
Iteration 59/1000 | Loss: 0.00001782
Iteration 60/1000 | Loss: 0.00001782
Iteration 61/1000 | Loss: 0.00001782
Iteration 62/1000 | Loss: 0.00001782
Iteration 63/1000 | Loss: 0.00001781
Iteration 64/1000 | Loss: 0.00001781
Iteration 65/1000 | Loss: 0.00001781
Iteration 66/1000 | Loss: 0.00001781
Iteration 67/1000 | Loss: 0.00001781
Iteration 68/1000 | Loss: 0.00001780
Iteration 69/1000 | Loss: 0.00001780
Iteration 70/1000 | Loss: 0.00001780
Iteration 71/1000 | Loss: 0.00001780
Iteration 72/1000 | Loss: 0.00001780
Iteration 73/1000 | Loss: 0.00001780
Iteration 74/1000 | Loss: 0.00001780
Iteration 75/1000 | Loss: 0.00001780
Iteration 76/1000 | Loss: 0.00001780
Iteration 77/1000 | Loss: 0.00001780
Iteration 78/1000 | Loss: 0.00001780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.779878766683396e-05, 1.779878766683396e-05, 1.779878766683396e-05, 1.779878766683396e-05, 1.779878766683396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.779878766683396e-05

Optimization complete. Final v2v error: 3.5588691234588623 mm

Highest mean error: 4.107883930206299 mm for frame 0

Lowest mean error: 3.2463836669921875 mm for frame 230

Saving results

Total time: 32.61352729797363
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411540
Iteration 2/25 | Loss: 0.00104532
Iteration 3/25 | Loss: 0.00086635
Iteration 4/25 | Loss: 0.00085012
Iteration 5/25 | Loss: 0.00084246
Iteration 6/25 | Loss: 0.00084028
Iteration 7/25 | Loss: 0.00084024
Iteration 8/25 | Loss: 0.00084024
Iteration 9/25 | Loss: 0.00084024
Iteration 10/25 | Loss: 0.00084024
Iteration 11/25 | Loss: 0.00084024
Iteration 12/25 | Loss: 0.00084024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008402411476708949, 0.0008402411476708949, 0.0008402411476708949, 0.0008402411476708949, 0.0008402411476708949]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008402411476708949

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94180274
Iteration 2/25 | Loss: 0.00223277
Iteration 3/25 | Loss: 0.00223276
Iteration 4/25 | Loss: 0.00223276
Iteration 5/25 | Loss: 0.00223276
Iteration 6/25 | Loss: 0.00223276
Iteration 7/25 | Loss: 0.00223275
Iteration 8/25 | Loss: 0.00223275
Iteration 9/25 | Loss: 0.00223275
Iteration 10/25 | Loss: 0.00223275
Iteration 11/25 | Loss: 0.00223275
Iteration 12/25 | Loss: 0.00223275
Iteration 13/25 | Loss: 0.00223275
Iteration 14/25 | Loss: 0.00223275
Iteration 15/25 | Loss: 0.00223275
Iteration 16/25 | Loss: 0.00223275
Iteration 17/25 | Loss: 0.00223275
Iteration 18/25 | Loss: 0.00223275
Iteration 19/25 | Loss: 0.00223275
Iteration 20/25 | Loss: 0.00223275
Iteration 21/25 | Loss: 0.00223275
Iteration 22/25 | Loss: 0.00223275
Iteration 23/25 | Loss: 0.00223275
Iteration 24/25 | Loss: 0.00223275
Iteration 25/25 | Loss: 0.00223275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223275
Iteration 2/1000 | Loss: 0.00003055
Iteration 3/1000 | Loss: 0.00001623
Iteration 4/1000 | Loss: 0.00001437
Iteration 5/1000 | Loss: 0.00001335
Iteration 6/1000 | Loss: 0.00001284
Iteration 7/1000 | Loss: 0.00001265
Iteration 8/1000 | Loss: 0.00001233
Iteration 9/1000 | Loss: 0.00001220
Iteration 10/1000 | Loss: 0.00001218
Iteration 11/1000 | Loss: 0.00001211
Iteration 12/1000 | Loss: 0.00001209
Iteration 13/1000 | Loss: 0.00001209
Iteration 14/1000 | Loss: 0.00001208
Iteration 15/1000 | Loss: 0.00001199
Iteration 16/1000 | Loss: 0.00001195
Iteration 17/1000 | Loss: 0.00001194
Iteration 18/1000 | Loss: 0.00001194
Iteration 19/1000 | Loss: 0.00001193
Iteration 20/1000 | Loss: 0.00001193
Iteration 21/1000 | Loss: 0.00001188
Iteration 22/1000 | Loss: 0.00001188
Iteration 23/1000 | Loss: 0.00001188
Iteration 24/1000 | Loss: 0.00001187
Iteration 25/1000 | Loss: 0.00001186
Iteration 26/1000 | Loss: 0.00001185
Iteration 27/1000 | Loss: 0.00001185
Iteration 28/1000 | Loss: 0.00001185
Iteration 29/1000 | Loss: 0.00001184
Iteration 30/1000 | Loss: 0.00001184
Iteration 31/1000 | Loss: 0.00001183
Iteration 32/1000 | Loss: 0.00001183
Iteration 33/1000 | Loss: 0.00001183
Iteration 34/1000 | Loss: 0.00001183
Iteration 35/1000 | Loss: 0.00001182
Iteration 36/1000 | Loss: 0.00001181
Iteration 37/1000 | Loss: 0.00001181
Iteration 38/1000 | Loss: 0.00001181
Iteration 39/1000 | Loss: 0.00001180
Iteration 40/1000 | Loss: 0.00001180
Iteration 41/1000 | Loss: 0.00001179
Iteration 42/1000 | Loss: 0.00001179
Iteration 43/1000 | Loss: 0.00001179
Iteration 44/1000 | Loss: 0.00001178
Iteration 45/1000 | Loss: 0.00001178
Iteration 46/1000 | Loss: 0.00001178
Iteration 47/1000 | Loss: 0.00001177
Iteration 48/1000 | Loss: 0.00001177
Iteration 49/1000 | Loss: 0.00001176
Iteration 50/1000 | Loss: 0.00001176
Iteration 51/1000 | Loss: 0.00001175
Iteration 52/1000 | Loss: 0.00001175
Iteration 53/1000 | Loss: 0.00001175
Iteration 54/1000 | Loss: 0.00001174
Iteration 55/1000 | Loss: 0.00001174
Iteration 56/1000 | Loss: 0.00001172
Iteration 57/1000 | Loss: 0.00001172
Iteration 58/1000 | Loss: 0.00001172
Iteration 59/1000 | Loss: 0.00001171
Iteration 60/1000 | Loss: 0.00001171
Iteration 61/1000 | Loss: 0.00001171
Iteration 62/1000 | Loss: 0.00001170
Iteration 63/1000 | Loss: 0.00001170
Iteration 64/1000 | Loss: 0.00001170
Iteration 65/1000 | Loss: 0.00001170
Iteration 66/1000 | Loss: 0.00001169
Iteration 67/1000 | Loss: 0.00001169
Iteration 68/1000 | Loss: 0.00001169
Iteration 69/1000 | Loss: 0.00001169
Iteration 70/1000 | Loss: 0.00001168
Iteration 71/1000 | Loss: 0.00001168
Iteration 72/1000 | Loss: 0.00001168
Iteration 73/1000 | Loss: 0.00001168
Iteration 74/1000 | Loss: 0.00001168
Iteration 75/1000 | Loss: 0.00001168
Iteration 76/1000 | Loss: 0.00001168
Iteration 77/1000 | Loss: 0.00001168
Iteration 78/1000 | Loss: 0.00001167
Iteration 79/1000 | Loss: 0.00001167
Iteration 80/1000 | Loss: 0.00001167
Iteration 81/1000 | Loss: 0.00001167
Iteration 82/1000 | Loss: 0.00001166
Iteration 83/1000 | Loss: 0.00001164
Iteration 84/1000 | Loss: 0.00001164
Iteration 85/1000 | Loss: 0.00001164
Iteration 86/1000 | Loss: 0.00001164
Iteration 87/1000 | Loss: 0.00001164
Iteration 88/1000 | Loss: 0.00001164
Iteration 89/1000 | Loss: 0.00001163
Iteration 90/1000 | Loss: 0.00001163
Iteration 91/1000 | Loss: 0.00001163
Iteration 92/1000 | Loss: 0.00001163
Iteration 93/1000 | Loss: 0.00001163
Iteration 94/1000 | Loss: 0.00001163
Iteration 95/1000 | Loss: 0.00001162
Iteration 96/1000 | Loss: 0.00001162
Iteration 97/1000 | Loss: 0.00001162
Iteration 98/1000 | Loss: 0.00001162
Iteration 99/1000 | Loss: 0.00001161
Iteration 100/1000 | Loss: 0.00001161
Iteration 101/1000 | Loss: 0.00001161
Iteration 102/1000 | Loss: 0.00001161
Iteration 103/1000 | Loss: 0.00001161
Iteration 104/1000 | Loss: 0.00001160
Iteration 105/1000 | Loss: 0.00001160
Iteration 106/1000 | Loss: 0.00001160
Iteration 107/1000 | Loss: 0.00001160
Iteration 108/1000 | Loss: 0.00001160
Iteration 109/1000 | Loss: 0.00001160
Iteration 110/1000 | Loss: 0.00001160
Iteration 111/1000 | Loss: 0.00001160
Iteration 112/1000 | Loss: 0.00001160
Iteration 113/1000 | Loss: 0.00001160
Iteration 114/1000 | Loss: 0.00001159
Iteration 115/1000 | Loss: 0.00001159
Iteration 116/1000 | Loss: 0.00001159
Iteration 117/1000 | Loss: 0.00001159
Iteration 118/1000 | Loss: 0.00001159
Iteration 119/1000 | Loss: 0.00001159
Iteration 120/1000 | Loss: 0.00001159
Iteration 121/1000 | Loss: 0.00001159
Iteration 122/1000 | Loss: 0.00001159
Iteration 123/1000 | Loss: 0.00001159
Iteration 124/1000 | Loss: 0.00001159
Iteration 125/1000 | Loss: 0.00001159
Iteration 126/1000 | Loss: 0.00001159
Iteration 127/1000 | Loss: 0.00001159
Iteration 128/1000 | Loss: 0.00001159
Iteration 129/1000 | Loss: 0.00001159
Iteration 130/1000 | Loss: 0.00001159
Iteration 131/1000 | Loss: 0.00001159
Iteration 132/1000 | Loss: 0.00001159
Iteration 133/1000 | Loss: 0.00001159
Iteration 134/1000 | Loss: 0.00001159
Iteration 135/1000 | Loss: 0.00001159
Iteration 136/1000 | Loss: 0.00001159
Iteration 137/1000 | Loss: 0.00001159
Iteration 138/1000 | Loss: 0.00001159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.1585426364035811e-05, 1.1585426364035811e-05, 1.1585426364035811e-05, 1.1585426364035811e-05, 1.1585426364035811e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1585426364035811e-05

Optimization complete. Final v2v error: 2.9221723079681396 mm

Highest mean error: 3.16403865814209 mm for frame 18

Lowest mean error: 2.7310640811920166 mm for frame 89

Saving results

Total time: 39.942527532577515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863461
Iteration 2/25 | Loss: 0.00134658
Iteration 3/25 | Loss: 0.00095134
Iteration 4/25 | Loss: 0.00087896
Iteration 5/25 | Loss: 0.00086946
Iteration 6/25 | Loss: 0.00086888
Iteration 7/25 | Loss: 0.00086888
Iteration 8/25 | Loss: 0.00086888
Iteration 9/25 | Loss: 0.00086888
Iteration 10/25 | Loss: 0.00086888
Iteration 11/25 | Loss: 0.00086888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008688775706104934, 0.0008688775706104934, 0.0008688775706104934, 0.0008688775706104934, 0.0008688775706104934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008688775706104934

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67380464
Iteration 2/25 | Loss: 0.00229156
Iteration 3/25 | Loss: 0.00229156
Iteration 4/25 | Loss: 0.00229156
Iteration 5/25 | Loss: 0.00229156
Iteration 6/25 | Loss: 0.00229156
Iteration 7/25 | Loss: 0.00229156
Iteration 8/25 | Loss: 0.00229156
Iteration 9/25 | Loss: 0.00229156
Iteration 10/25 | Loss: 0.00229156
Iteration 11/25 | Loss: 0.00229156
Iteration 12/25 | Loss: 0.00229156
Iteration 13/25 | Loss: 0.00229156
Iteration 14/25 | Loss: 0.00229156
Iteration 15/25 | Loss: 0.00229156
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0022915552835911512, 0.0022915552835911512, 0.0022915552835911512, 0.0022915552835911512, 0.0022915552835911512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022915552835911512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00229156
Iteration 2/1000 | Loss: 0.00004258
Iteration 3/1000 | Loss: 0.00002972
Iteration 4/1000 | Loss: 0.00002657
Iteration 5/1000 | Loss: 0.00002485
Iteration 6/1000 | Loss: 0.00002365
Iteration 7/1000 | Loss: 0.00002245
Iteration 8/1000 | Loss: 0.00002178
Iteration 9/1000 | Loss: 0.00002123
Iteration 10/1000 | Loss: 0.00002086
Iteration 11/1000 | Loss: 0.00002076
Iteration 12/1000 | Loss: 0.00002054
Iteration 13/1000 | Loss: 0.00002037
Iteration 14/1000 | Loss: 0.00002033
Iteration 15/1000 | Loss: 0.00002029
Iteration 16/1000 | Loss: 0.00002028
Iteration 17/1000 | Loss: 0.00002024
Iteration 18/1000 | Loss: 0.00002024
Iteration 19/1000 | Loss: 0.00002023
Iteration 20/1000 | Loss: 0.00002021
Iteration 21/1000 | Loss: 0.00002020
Iteration 22/1000 | Loss: 0.00002019
Iteration 23/1000 | Loss: 0.00002019
Iteration 24/1000 | Loss: 0.00002016
Iteration 25/1000 | Loss: 0.00002010
Iteration 26/1000 | Loss: 0.00002006
Iteration 27/1000 | Loss: 0.00002005
Iteration 28/1000 | Loss: 0.00002005
Iteration 29/1000 | Loss: 0.00002004
Iteration 30/1000 | Loss: 0.00002004
Iteration 31/1000 | Loss: 0.00002003
Iteration 32/1000 | Loss: 0.00002003
Iteration 33/1000 | Loss: 0.00002003
Iteration 34/1000 | Loss: 0.00002002
Iteration 35/1000 | Loss: 0.00002002
Iteration 36/1000 | Loss: 0.00002002
Iteration 37/1000 | Loss: 0.00002001
Iteration 38/1000 | Loss: 0.00002001
Iteration 39/1000 | Loss: 0.00002001
Iteration 40/1000 | Loss: 0.00002001
Iteration 41/1000 | Loss: 0.00002001
Iteration 42/1000 | Loss: 0.00002001
Iteration 43/1000 | Loss: 0.00002001
Iteration 44/1000 | Loss: 0.00002000
Iteration 45/1000 | Loss: 0.00002000
Iteration 46/1000 | Loss: 0.00002000
Iteration 47/1000 | Loss: 0.00002000
Iteration 48/1000 | Loss: 0.00002000
Iteration 49/1000 | Loss: 0.00001999
Iteration 50/1000 | Loss: 0.00001999
Iteration 51/1000 | Loss: 0.00001999
Iteration 52/1000 | Loss: 0.00001999
Iteration 53/1000 | Loss: 0.00001999
Iteration 54/1000 | Loss: 0.00001999
Iteration 55/1000 | Loss: 0.00001999
Iteration 56/1000 | Loss: 0.00001999
Iteration 57/1000 | Loss: 0.00001999
Iteration 58/1000 | Loss: 0.00001999
Iteration 59/1000 | Loss: 0.00001999
Iteration 60/1000 | Loss: 0.00001999
Iteration 61/1000 | Loss: 0.00001998
Iteration 62/1000 | Loss: 0.00001998
Iteration 63/1000 | Loss: 0.00001998
Iteration 64/1000 | Loss: 0.00001998
Iteration 65/1000 | Loss: 0.00001998
Iteration 66/1000 | Loss: 0.00001998
Iteration 67/1000 | Loss: 0.00001998
Iteration 68/1000 | Loss: 0.00001998
Iteration 69/1000 | Loss: 0.00001998
Iteration 70/1000 | Loss: 0.00001997
Iteration 71/1000 | Loss: 0.00001997
Iteration 72/1000 | Loss: 0.00001997
Iteration 73/1000 | Loss: 0.00001997
Iteration 74/1000 | Loss: 0.00001997
Iteration 75/1000 | Loss: 0.00001997
Iteration 76/1000 | Loss: 0.00001997
Iteration 77/1000 | Loss: 0.00001997
Iteration 78/1000 | Loss: 0.00001997
Iteration 79/1000 | Loss: 0.00001997
Iteration 80/1000 | Loss: 0.00001997
Iteration 81/1000 | Loss: 0.00001997
Iteration 82/1000 | Loss: 0.00001997
Iteration 83/1000 | Loss: 0.00001997
Iteration 84/1000 | Loss: 0.00001997
Iteration 85/1000 | Loss: 0.00001997
Iteration 86/1000 | Loss: 0.00001997
Iteration 87/1000 | Loss: 0.00001997
Iteration 88/1000 | Loss: 0.00001997
Iteration 89/1000 | Loss: 0.00001997
Iteration 90/1000 | Loss: 0.00001997
Iteration 91/1000 | Loss: 0.00001997
Iteration 92/1000 | Loss: 0.00001997
Iteration 93/1000 | Loss: 0.00001997
Iteration 94/1000 | Loss: 0.00001997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.9965051251347177e-05, 1.9965051251347177e-05, 1.9965051251347177e-05, 1.9965051251347177e-05, 1.9965051251347177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9965051251347177e-05

Optimization complete. Final v2v error: 3.9026198387145996 mm

Highest mean error: 4.242271900177002 mm for frame 129

Lowest mean error: 3.4328725337982178 mm for frame 83

Saving results

Total time: 39.64406871795654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957795
Iteration 2/25 | Loss: 0.00159137
Iteration 3/25 | Loss: 0.00108964
Iteration 4/25 | Loss: 0.00103620
Iteration 5/25 | Loss: 0.00102984
Iteration 6/25 | Loss: 0.00102866
Iteration 7/25 | Loss: 0.00102866
Iteration 8/25 | Loss: 0.00102866
Iteration 9/25 | Loss: 0.00102866
Iteration 10/25 | Loss: 0.00102866
Iteration 11/25 | Loss: 0.00102866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010286588221788406, 0.0010286588221788406, 0.0010286588221788406, 0.0010286588221788406, 0.0010286588221788406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010286588221788406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.69592929
Iteration 2/25 | Loss: 0.00166703
Iteration 3/25 | Loss: 0.00166703
Iteration 4/25 | Loss: 0.00166702
Iteration 5/25 | Loss: 0.00166702
Iteration 6/25 | Loss: 0.00166702
Iteration 7/25 | Loss: 0.00166702
Iteration 8/25 | Loss: 0.00166702
Iteration 9/25 | Loss: 0.00166702
Iteration 10/25 | Loss: 0.00166702
Iteration 11/25 | Loss: 0.00166702
Iteration 12/25 | Loss: 0.00166702
Iteration 13/25 | Loss: 0.00166702
Iteration 14/25 | Loss: 0.00166702
Iteration 15/25 | Loss: 0.00166702
Iteration 16/25 | Loss: 0.00166702
Iteration 17/25 | Loss: 0.00166702
Iteration 18/25 | Loss: 0.00166702
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016670237528160214, 0.0016670237528160214, 0.0016670237528160214, 0.0016670237528160214, 0.0016670237528160214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016670237528160214

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166702
Iteration 2/1000 | Loss: 0.00005564
Iteration 3/1000 | Loss: 0.00003516
Iteration 4/1000 | Loss: 0.00003212
Iteration 5/1000 | Loss: 0.00003029
Iteration 6/1000 | Loss: 0.00002870
Iteration 7/1000 | Loss: 0.00002827
Iteration 8/1000 | Loss: 0.00002808
Iteration 9/1000 | Loss: 0.00002792
Iteration 10/1000 | Loss: 0.00002769
Iteration 11/1000 | Loss: 0.00002750
Iteration 12/1000 | Loss: 0.00002749
Iteration 13/1000 | Loss: 0.00002745
Iteration 14/1000 | Loss: 0.00002745
Iteration 15/1000 | Loss: 0.00002744
Iteration 16/1000 | Loss: 0.00002744
Iteration 17/1000 | Loss: 0.00002744
Iteration 18/1000 | Loss: 0.00002744
Iteration 19/1000 | Loss: 0.00002744
Iteration 20/1000 | Loss: 0.00002744
Iteration 21/1000 | Loss: 0.00002743
Iteration 22/1000 | Loss: 0.00002737
Iteration 23/1000 | Loss: 0.00002737
Iteration 24/1000 | Loss: 0.00002736
Iteration 25/1000 | Loss: 0.00002736
Iteration 26/1000 | Loss: 0.00002735
Iteration 27/1000 | Loss: 0.00002733
Iteration 28/1000 | Loss: 0.00002732
Iteration 29/1000 | Loss: 0.00002732
Iteration 30/1000 | Loss: 0.00002732
Iteration 31/1000 | Loss: 0.00002732
Iteration 32/1000 | Loss: 0.00002729
Iteration 33/1000 | Loss: 0.00002729
Iteration 34/1000 | Loss: 0.00002729
Iteration 35/1000 | Loss: 0.00002728
Iteration 36/1000 | Loss: 0.00002728
Iteration 37/1000 | Loss: 0.00002728
Iteration 38/1000 | Loss: 0.00002728
Iteration 39/1000 | Loss: 0.00002728
Iteration 40/1000 | Loss: 0.00002728
Iteration 41/1000 | Loss: 0.00002728
Iteration 42/1000 | Loss: 0.00002728
Iteration 43/1000 | Loss: 0.00002728
Iteration 44/1000 | Loss: 0.00002728
Iteration 45/1000 | Loss: 0.00002727
Iteration 46/1000 | Loss: 0.00002727
Iteration 47/1000 | Loss: 0.00002727
Iteration 48/1000 | Loss: 0.00002727
Iteration 49/1000 | Loss: 0.00002727
Iteration 50/1000 | Loss: 0.00002727
Iteration 51/1000 | Loss: 0.00002726
Iteration 52/1000 | Loss: 0.00002726
Iteration 53/1000 | Loss: 0.00002726
Iteration 54/1000 | Loss: 0.00002726
Iteration 55/1000 | Loss: 0.00002726
Iteration 56/1000 | Loss: 0.00002725
Iteration 57/1000 | Loss: 0.00002725
Iteration 58/1000 | Loss: 0.00002725
Iteration 59/1000 | Loss: 0.00002725
Iteration 60/1000 | Loss: 0.00002724
Iteration 61/1000 | Loss: 0.00002724
Iteration 62/1000 | Loss: 0.00002724
Iteration 63/1000 | Loss: 0.00002724
Iteration 64/1000 | Loss: 0.00002724
Iteration 65/1000 | Loss: 0.00002724
Iteration 66/1000 | Loss: 0.00002723
Iteration 67/1000 | Loss: 0.00002723
Iteration 68/1000 | Loss: 0.00002723
Iteration 69/1000 | Loss: 0.00002722
Iteration 70/1000 | Loss: 0.00002722
Iteration 71/1000 | Loss: 0.00002722
Iteration 72/1000 | Loss: 0.00002722
Iteration 73/1000 | Loss: 0.00002722
Iteration 74/1000 | Loss: 0.00002721
Iteration 75/1000 | Loss: 0.00002721
Iteration 76/1000 | Loss: 0.00002721
Iteration 77/1000 | Loss: 0.00002721
Iteration 78/1000 | Loss: 0.00002720
Iteration 79/1000 | Loss: 0.00002720
Iteration 80/1000 | Loss: 0.00002719
Iteration 81/1000 | Loss: 0.00002719
Iteration 82/1000 | Loss: 0.00002719
Iteration 83/1000 | Loss: 0.00002719
Iteration 84/1000 | Loss: 0.00002719
Iteration 85/1000 | Loss: 0.00002719
Iteration 86/1000 | Loss: 0.00002719
Iteration 87/1000 | Loss: 0.00002718
Iteration 88/1000 | Loss: 0.00002718
Iteration 89/1000 | Loss: 0.00002718
Iteration 90/1000 | Loss: 0.00002718
Iteration 91/1000 | Loss: 0.00002718
Iteration 92/1000 | Loss: 0.00002718
Iteration 93/1000 | Loss: 0.00002718
Iteration 94/1000 | Loss: 0.00002718
Iteration 95/1000 | Loss: 0.00002718
Iteration 96/1000 | Loss: 0.00002718
Iteration 97/1000 | Loss: 0.00002718
Iteration 98/1000 | Loss: 0.00002718
Iteration 99/1000 | Loss: 0.00002717
Iteration 100/1000 | Loss: 0.00002717
Iteration 101/1000 | Loss: 0.00002717
Iteration 102/1000 | Loss: 0.00002717
Iteration 103/1000 | Loss: 0.00002717
Iteration 104/1000 | Loss: 0.00002717
Iteration 105/1000 | Loss: 0.00002717
Iteration 106/1000 | Loss: 0.00002717
Iteration 107/1000 | Loss: 0.00002717
Iteration 108/1000 | Loss: 0.00002716
Iteration 109/1000 | Loss: 0.00002716
Iteration 110/1000 | Loss: 0.00002716
Iteration 111/1000 | Loss: 0.00002716
Iteration 112/1000 | Loss: 0.00002716
Iteration 113/1000 | Loss: 0.00002715
Iteration 114/1000 | Loss: 0.00002715
Iteration 115/1000 | Loss: 0.00002715
Iteration 116/1000 | Loss: 0.00002715
Iteration 117/1000 | Loss: 0.00002715
Iteration 118/1000 | Loss: 0.00002714
Iteration 119/1000 | Loss: 0.00002714
Iteration 120/1000 | Loss: 0.00002714
Iteration 121/1000 | Loss: 0.00002714
Iteration 122/1000 | Loss: 0.00002714
Iteration 123/1000 | Loss: 0.00002714
Iteration 124/1000 | Loss: 0.00002714
Iteration 125/1000 | Loss: 0.00002714
Iteration 126/1000 | Loss: 0.00002714
Iteration 127/1000 | Loss: 0.00002714
Iteration 128/1000 | Loss: 0.00002714
Iteration 129/1000 | Loss: 0.00002714
Iteration 130/1000 | Loss: 0.00002714
Iteration 131/1000 | Loss: 0.00002714
Iteration 132/1000 | Loss: 0.00002714
Iteration 133/1000 | Loss: 0.00002714
Iteration 134/1000 | Loss: 0.00002714
Iteration 135/1000 | Loss: 0.00002713
Iteration 136/1000 | Loss: 0.00002713
Iteration 137/1000 | Loss: 0.00002713
Iteration 138/1000 | Loss: 0.00002713
Iteration 139/1000 | Loss: 0.00002713
Iteration 140/1000 | Loss: 0.00002713
Iteration 141/1000 | Loss: 0.00002713
Iteration 142/1000 | Loss: 0.00002713
Iteration 143/1000 | Loss: 0.00002712
Iteration 144/1000 | Loss: 0.00002712
Iteration 145/1000 | Loss: 0.00002712
Iteration 146/1000 | Loss: 0.00002712
Iteration 147/1000 | Loss: 0.00002712
Iteration 148/1000 | Loss: 0.00002712
Iteration 149/1000 | Loss: 0.00002712
Iteration 150/1000 | Loss: 0.00002711
Iteration 151/1000 | Loss: 0.00002711
Iteration 152/1000 | Loss: 0.00002711
Iteration 153/1000 | Loss: 0.00002711
Iteration 154/1000 | Loss: 0.00002711
Iteration 155/1000 | Loss: 0.00002711
Iteration 156/1000 | Loss: 0.00002710
Iteration 157/1000 | Loss: 0.00002710
Iteration 158/1000 | Loss: 0.00002710
Iteration 159/1000 | Loss: 0.00002710
Iteration 160/1000 | Loss: 0.00002710
Iteration 161/1000 | Loss: 0.00002710
Iteration 162/1000 | Loss: 0.00002710
Iteration 163/1000 | Loss: 0.00002710
Iteration 164/1000 | Loss: 0.00002710
Iteration 165/1000 | Loss: 0.00002710
Iteration 166/1000 | Loss: 0.00002710
Iteration 167/1000 | Loss: 0.00002709
Iteration 168/1000 | Loss: 0.00002709
Iteration 169/1000 | Loss: 0.00002709
Iteration 170/1000 | Loss: 0.00002709
Iteration 171/1000 | Loss: 0.00002709
Iteration 172/1000 | Loss: 0.00002709
Iteration 173/1000 | Loss: 0.00002709
Iteration 174/1000 | Loss: 0.00002709
Iteration 175/1000 | Loss: 0.00002709
Iteration 176/1000 | Loss: 0.00002709
Iteration 177/1000 | Loss: 0.00002709
Iteration 178/1000 | Loss: 0.00002709
Iteration 179/1000 | Loss: 0.00002709
Iteration 180/1000 | Loss: 0.00002709
Iteration 181/1000 | Loss: 0.00002709
Iteration 182/1000 | Loss: 0.00002709
Iteration 183/1000 | Loss: 0.00002709
Iteration 184/1000 | Loss: 0.00002709
Iteration 185/1000 | Loss: 0.00002709
Iteration 186/1000 | Loss: 0.00002709
Iteration 187/1000 | Loss: 0.00002709
Iteration 188/1000 | Loss: 0.00002709
Iteration 189/1000 | Loss: 0.00002709
Iteration 190/1000 | Loss: 0.00002709
Iteration 191/1000 | Loss: 0.00002709
Iteration 192/1000 | Loss: 0.00002709
Iteration 193/1000 | Loss: 0.00002709
Iteration 194/1000 | Loss: 0.00002709
Iteration 195/1000 | Loss: 0.00002709
Iteration 196/1000 | Loss: 0.00002709
Iteration 197/1000 | Loss: 0.00002709
Iteration 198/1000 | Loss: 0.00002709
Iteration 199/1000 | Loss: 0.00002709
Iteration 200/1000 | Loss: 0.00002709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [2.70914715656545e-05, 2.70914715656545e-05, 2.70914715656545e-05, 2.70914715656545e-05, 2.70914715656545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.70914715656545e-05

Optimization complete. Final v2v error: 4.474685192108154 mm

Highest mean error: 4.992463111877441 mm for frame 29

Lowest mean error: 4.198658466339111 mm for frame 182

Saving results

Total time: 39.69651198387146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032496
Iteration 2/25 | Loss: 0.00230985
Iteration 3/25 | Loss: 0.00161886
Iteration 4/25 | Loss: 0.00140407
Iteration 5/25 | Loss: 0.00131211
Iteration 6/25 | Loss: 0.00124665
Iteration 7/25 | Loss: 0.00123029
Iteration 8/25 | Loss: 0.00123899
Iteration 9/25 | Loss: 0.00122434
Iteration 10/25 | Loss: 0.00120463
Iteration 11/25 | Loss: 0.00120695
Iteration 12/25 | Loss: 0.00123855
Iteration 13/25 | Loss: 0.00119648
Iteration 14/25 | Loss: 0.00115429
Iteration 15/25 | Loss: 0.00112938
Iteration 16/25 | Loss: 0.00111419
Iteration 17/25 | Loss: 0.00110204
Iteration 18/25 | Loss: 0.00109499
Iteration 19/25 | Loss: 0.00108608
Iteration 20/25 | Loss: 0.00108435
Iteration 21/25 | Loss: 0.00108250
Iteration 22/25 | Loss: 0.00109027
Iteration 23/25 | Loss: 0.00108266
Iteration 24/25 | Loss: 0.00107648
Iteration 25/25 | Loss: 0.00107556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72691894
Iteration 2/25 | Loss: 0.00328637
Iteration 3/25 | Loss: 0.00322397
Iteration 4/25 | Loss: 0.00322397
Iteration 5/25 | Loss: 0.00322397
Iteration 6/25 | Loss: 0.00322397
Iteration 7/25 | Loss: 0.00322397
Iteration 8/25 | Loss: 0.00322397
Iteration 9/25 | Loss: 0.00322397
Iteration 10/25 | Loss: 0.00322397
Iteration 11/25 | Loss: 0.00322397
Iteration 12/25 | Loss: 0.00322397
Iteration 13/25 | Loss: 0.00322397
Iteration 14/25 | Loss: 0.00322397
Iteration 15/25 | Loss: 0.00322397
Iteration 16/25 | Loss: 0.00322397
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003223967971280217, 0.003223967971280217, 0.003223967971280217, 0.003223967971280217, 0.003223967971280217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003223967971280217

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00322397
Iteration 2/1000 | Loss: 0.00021238
Iteration 3/1000 | Loss: 0.00024388
Iteration 4/1000 | Loss: 0.00017663
Iteration 5/1000 | Loss: 0.00035956
Iteration 6/1000 | Loss: 0.00012261
Iteration 7/1000 | Loss: 0.00010143
Iteration 8/1000 | Loss: 0.00013408
Iteration 9/1000 | Loss: 0.00011225
Iteration 10/1000 | Loss: 0.00013091
Iteration 11/1000 | Loss: 0.00012206
Iteration 12/1000 | Loss: 0.00013408
Iteration 13/1000 | Loss: 0.00008544
Iteration 14/1000 | Loss: 0.00012474
Iteration 15/1000 | Loss: 0.00009538
Iteration 16/1000 | Loss: 0.00011356
Iteration 17/1000 | Loss: 0.00011757
Iteration 18/1000 | Loss: 0.00010250
Iteration 19/1000 | Loss: 0.00009424
Iteration 20/1000 | Loss: 0.00011553
Iteration 21/1000 | Loss: 0.00009034
Iteration 22/1000 | Loss: 0.00012195
Iteration 23/1000 | Loss: 0.00010068
Iteration 24/1000 | Loss: 0.00012885
Iteration 25/1000 | Loss: 0.00011590
Iteration 26/1000 | Loss: 0.00015455
Iteration 27/1000 | Loss: 0.00011716
Iteration 28/1000 | Loss: 0.00014564
Iteration 29/1000 | Loss: 0.00011418
Iteration 30/1000 | Loss: 0.00013453
Iteration 31/1000 | Loss: 0.00010328
Iteration 32/1000 | Loss: 0.00009558
Iteration 33/1000 | Loss: 0.00009763
Iteration 34/1000 | Loss: 0.00012769
Iteration 35/1000 | Loss: 0.00013624
Iteration 36/1000 | Loss: 0.00012297
Iteration 37/1000 | Loss: 0.00013432
Iteration 38/1000 | Loss: 0.00010334
Iteration 39/1000 | Loss: 0.00105314
Iteration 40/1000 | Loss: 0.00156900
Iteration 41/1000 | Loss: 0.00141571
Iteration 42/1000 | Loss: 0.00135891
Iteration 43/1000 | Loss: 0.00021605
Iteration 44/1000 | Loss: 0.00347455
Iteration 45/1000 | Loss: 0.00209186
Iteration 46/1000 | Loss: 0.00239135
Iteration 47/1000 | Loss: 0.00057459
Iteration 48/1000 | Loss: 0.00009039
Iteration 49/1000 | Loss: 0.00024120
Iteration 50/1000 | Loss: 0.00007844
Iteration 51/1000 | Loss: 0.00006077
Iteration 52/1000 | Loss: 0.00005826
Iteration 53/1000 | Loss: 0.00005150
Iteration 54/1000 | Loss: 0.00004840
Iteration 55/1000 | Loss: 0.00004657
Iteration 56/1000 | Loss: 0.00004912
Iteration 57/1000 | Loss: 0.00004604
Iteration 58/1000 | Loss: 0.00004431
Iteration 59/1000 | Loss: 0.00004316
Iteration 60/1000 | Loss: 0.00004255
Iteration 61/1000 | Loss: 0.00004199
Iteration 62/1000 | Loss: 0.00004347
Iteration 63/1000 | Loss: 0.00004102
Iteration 64/1000 | Loss: 0.00004770
Iteration 65/1000 | Loss: 0.00004025
Iteration 66/1000 | Loss: 0.00005453
Iteration 67/1000 | Loss: 0.00004540
Iteration 68/1000 | Loss: 0.00003955
Iteration 69/1000 | Loss: 0.00003925
Iteration 70/1000 | Loss: 0.00003897
Iteration 71/1000 | Loss: 0.00007322
Iteration 72/1000 | Loss: 0.00003865
Iteration 73/1000 | Loss: 0.00003842
Iteration 74/1000 | Loss: 0.00108797
Iteration 75/1000 | Loss: 0.00146766
Iteration 76/1000 | Loss: 0.00016417
Iteration 77/1000 | Loss: 0.00006621
Iteration 78/1000 | Loss: 0.00004378
Iteration 79/1000 | Loss: 0.00006953
Iteration 80/1000 | Loss: 0.00003959
Iteration 81/1000 | Loss: 0.00005087
Iteration 82/1000 | Loss: 0.00003827
Iteration 83/1000 | Loss: 0.00003756
Iteration 84/1000 | Loss: 0.00004682
Iteration 85/1000 | Loss: 0.00003710
Iteration 86/1000 | Loss: 0.00006035
Iteration 87/1000 | Loss: 0.00003668
Iteration 88/1000 | Loss: 0.00003705
Iteration 89/1000 | Loss: 0.00003704
Iteration 90/1000 | Loss: 0.00003637
Iteration 91/1000 | Loss: 0.00003636
Iteration 92/1000 | Loss: 0.00003635
Iteration 93/1000 | Loss: 0.00003665
Iteration 94/1000 | Loss: 0.00003607
Iteration 95/1000 | Loss: 0.00003607
Iteration 96/1000 | Loss: 0.00003605
Iteration 97/1000 | Loss: 0.00003598
Iteration 98/1000 | Loss: 0.00003597
Iteration 99/1000 | Loss: 0.00003590
Iteration 100/1000 | Loss: 0.00003590
Iteration 101/1000 | Loss: 0.00003590
Iteration 102/1000 | Loss: 0.00003589
Iteration 103/1000 | Loss: 0.00003588
Iteration 104/1000 | Loss: 0.00003587
Iteration 105/1000 | Loss: 0.00003586
Iteration 106/1000 | Loss: 0.00003585
Iteration 107/1000 | Loss: 0.00003585
Iteration 108/1000 | Loss: 0.00003585
Iteration 109/1000 | Loss: 0.00003585
Iteration 110/1000 | Loss: 0.00003585
Iteration 111/1000 | Loss: 0.00003584
Iteration 112/1000 | Loss: 0.00003584
Iteration 113/1000 | Loss: 0.00003584
Iteration 114/1000 | Loss: 0.00003584
Iteration 115/1000 | Loss: 0.00003584
Iteration 116/1000 | Loss: 0.00003584
Iteration 117/1000 | Loss: 0.00003584
Iteration 118/1000 | Loss: 0.00003584
Iteration 119/1000 | Loss: 0.00003584
Iteration 120/1000 | Loss: 0.00003583
Iteration 121/1000 | Loss: 0.00003583
Iteration 122/1000 | Loss: 0.00003583
Iteration 123/1000 | Loss: 0.00003583
Iteration 124/1000 | Loss: 0.00003583
Iteration 125/1000 | Loss: 0.00003582
Iteration 126/1000 | Loss: 0.00003582
Iteration 127/1000 | Loss: 0.00003582
Iteration 128/1000 | Loss: 0.00003581
Iteration 129/1000 | Loss: 0.00003581
Iteration 130/1000 | Loss: 0.00003581
Iteration 131/1000 | Loss: 0.00003581
Iteration 132/1000 | Loss: 0.00003580
Iteration 133/1000 | Loss: 0.00003580
Iteration 134/1000 | Loss: 0.00003580
Iteration 135/1000 | Loss: 0.00003579
Iteration 136/1000 | Loss: 0.00003579
Iteration 137/1000 | Loss: 0.00003578
Iteration 138/1000 | Loss: 0.00003578
Iteration 139/1000 | Loss: 0.00003577
Iteration 140/1000 | Loss: 0.00003576
Iteration 141/1000 | Loss: 0.00003576
Iteration 142/1000 | Loss: 0.00003576
Iteration 143/1000 | Loss: 0.00003576
Iteration 144/1000 | Loss: 0.00003575
Iteration 145/1000 | Loss: 0.00003575
Iteration 146/1000 | Loss: 0.00003575
Iteration 147/1000 | Loss: 0.00003574
Iteration 148/1000 | Loss: 0.00003574
Iteration 149/1000 | Loss: 0.00003574
Iteration 150/1000 | Loss: 0.00003574
Iteration 151/1000 | Loss: 0.00003574
Iteration 152/1000 | Loss: 0.00003573
Iteration 153/1000 | Loss: 0.00003573
Iteration 154/1000 | Loss: 0.00003573
Iteration 155/1000 | Loss: 0.00003573
Iteration 156/1000 | Loss: 0.00003573
Iteration 157/1000 | Loss: 0.00003573
Iteration 158/1000 | Loss: 0.00003573
Iteration 159/1000 | Loss: 0.00003573
Iteration 160/1000 | Loss: 0.00003573
Iteration 161/1000 | Loss: 0.00003572
Iteration 162/1000 | Loss: 0.00003572
Iteration 163/1000 | Loss: 0.00003572
Iteration 164/1000 | Loss: 0.00003572
Iteration 165/1000 | Loss: 0.00003572
Iteration 166/1000 | Loss: 0.00003572
Iteration 167/1000 | Loss: 0.00003572
Iteration 168/1000 | Loss: 0.00003572
Iteration 169/1000 | Loss: 0.00003572
Iteration 170/1000 | Loss: 0.00003572
Iteration 171/1000 | Loss: 0.00003572
Iteration 172/1000 | Loss: 0.00003572
Iteration 173/1000 | Loss: 0.00003572
Iteration 174/1000 | Loss: 0.00003572
Iteration 175/1000 | Loss: 0.00003572
Iteration 176/1000 | Loss: 0.00003572
Iteration 177/1000 | Loss: 0.00003572
Iteration 178/1000 | Loss: 0.00003572
Iteration 179/1000 | Loss: 0.00003571
Iteration 180/1000 | Loss: 0.00003571
Iteration 181/1000 | Loss: 0.00003571
Iteration 182/1000 | Loss: 0.00003571
Iteration 183/1000 | Loss: 0.00003571
Iteration 184/1000 | Loss: 0.00003571
Iteration 185/1000 | Loss: 0.00003571
Iteration 186/1000 | Loss: 0.00003571
Iteration 187/1000 | Loss: 0.00003571
Iteration 188/1000 | Loss: 0.00003571
Iteration 189/1000 | Loss: 0.00003571
Iteration 190/1000 | Loss: 0.00003571
Iteration 191/1000 | Loss: 0.00003571
Iteration 192/1000 | Loss: 0.00003571
Iteration 193/1000 | Loss: 0.00003571
Iteration 194/1000 | Loss: 0.00003571
Iteration 195/1000 | Loss: 0.00003571
Iteration 196/1000 | Loss: 0.00003571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [3.571081833797507e-05, 3.571081833797507e-05, 3.571081833797507e-05, 3.571081833797507e-05, 3.571081833797507e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.571081833797507e-05

Optimization complete. Final v2v error: 4.342628002166748 mm

Highest mean error: 12.34553337097168 mm for frame 4

Lowest mean error: 2.9753103256225586 mm for frame 1

Saving results

Total time: 183.64619493484497
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00901886
Iteration 2/25 | Loss: 0.00152580
Iteration 3/25 | Loss: 0.00108266
Iteration 4/25 | Loss: 0.00099940
Iteration 5/25 | Loss: 0.00096980
Iteration 6/25 | Loss: 0.00096093
Iteration 7/25 | Loss: 0.00095971
Iteration 8/25 | Loss: 0.00095971
Iteration 9/25 | Loss: 0.00095971
Iteration 10/25 | Loss: 0.00095971
Iteration 11/25 | Loss: 0.00095971
Iteration 12/25 | Loss: 0.00095971
Iteration 13/25 | Loss: 0.00095971
Iteration 14/25 | Loss: 0.00095971
Iteration 15/25 | Loss: 0.00095971
Iteration 16/25 | Loss: 0.00095971
Iteration 17/25 | Loss: 0.00095971
Iteration 18/25 | Loss: 0.00095971
Iteration 19/25 | Loss: 0.00095971
Iteration 20/25 | Loss: 0.00095971
Iteration 21/25 | Loss: 0.00095971
Iteration 22/25 | Loss: 0.00095971
Iteration 23/25 | Loss: 0.00095971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009597106836736202, 0.0009597106836736202, 0.0009597106836736202, 0.0009597106836736202, 0.0009597106836736202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009597106836736202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.96487451
Iteration 2/25 | Loss: 0.00229123
Iteration 3/25 | Loss: 0.00229122
Iteration 4/25 | Loss: 0.00229122
Iteration 5/25 | Loss: 0.00229122
Iteration 6/25 | Loss: 0.00229122
Iteration 7/25 | Loss: 0.00229122
Iteration 8/25 | Loss: 0.00229122
Iteration 9/25 | Loss: 0.00229122
Iteration 10/25 | Loss: 0.00229122
Iteration 11/25 | Loss: 0.00229122
Iteration 12/25 | Loss: 0.00229122
Iteration 13/25 | Loss: 0.00229122
Iteration 14/25 | Loss: 0.00229122
Iteration 15/25 | Loss: 0.00229122
Iteration 16/25 | Loss: 0.00229122
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0022912202402949333, 0.0022912202402949333, 0.0022912202402949333, 0.0022912202402949333, 0.0022912202402949333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022912202402949333

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00229122
Iteration 2/1000 | Loss: 0.00003970
Iteration 3/1000 | Loss: 0.00003227
Iteration 4/1000 | Loss: 0.00002815
Iteration 5/1000 | Loss: 0.00002659
Iteration 6/1000 | Loss: 0.00002561
Iteration 7/1000 | Loss: 0.00002504
Iteration 8/1000 | Loss: 0.00002459
Iteration 9/1000 | Loss: 0.00002429
Iteration 10/1000 | Loss: 0.00002411
Iteration 11/1000 | Loss: 0.00002395
Iteration 12/1000 | Loss: 0.00002392
Iteration 13/1000 | Loss: 0.00002391
Iteration 14/1000 | Loss: 0.00002389
Iteration 15/1000 | Loss: 0.00002388
Iteration 16/1000 | Loss: 0.00002388
Iteration 17/1000 | Loss: 0.00002387
Iteration 18/1000 | Loss: 0.00002383
Iteration 19/1000 | Loss: 0.00002381
Iteration 20/1000 | Loss: 0.00002380
Iteration 21/1000 | Loss: 0.00002379
Iteration 22/1000 | Loss: 0.00002379
Iteration 23/1000 | Loss: 0.00002378
Iteration 24/1000 | Loss: 0.00002378
Iteration 25/1000 | Loss: 0.00002378
Iteration 26/1000 | Loss: 0.00002377
Iteration 27/1000 | Loss: 0.00002377
Iteration 28/1000 | Loss: 0.00002376
Iteration 29/1000 | Loss: 0.00002376
Iteration 30/1000 | Loss: 0.00002376
Iteration 31/1000 | Loss: 0.00002375
Iteration 32/1000 | Loss: 0.00002375
Iteration 33/1000 | Loss: 0.00002374
Iteration 34/1000 | Loss: 0.00002374
Iteration 35/1000 | Loss: 0.00002374
Iteration 36/1000 | Loss: 0.00002374
Iteration 37/1000 | Loss: 0.00002373
Iteration 38/1000 | Loss: 0.00002373
Iteration 39/1000 | Loss: 0.00002373
Iteration 40/1000 | Loss: 0.00002372
Iteration 41/1000 | Loss: 0.00002372
Iteration 42/1000 | Loss: 0.00002372
Iteration 43/1000 | Loss: 0.00002372
Iteration 44/1000 | Loss: 0.00002372
Iteration 45/1000 | Loss: 0.00002372
Iteration 46/1000 | Loss: 0.00002371
Iteration 47/1000 | Loss: 0.00002371
Iteration 48/1000 | Loss: 0.00002371
Iteration 49/1000 | Loss: 0.00002371
Iteration 50/1000 | Loss: 0.00002371
Iteration 51/1000 | Loss: 0.00002371
Iteration 52/1000 | Loss: 0.00002371
Iteration 53/1000 | Loss: 0.00002371
Iteration 54/1000 | Loss: 0.00002370
Iteration 55/1000 | Loss: 0.00002370
Iteration 56/1000 | Loss: 0.00002370
Iteration 57/1000 | Loss: 0.00002370
Iteration 58/1000 | Loss: 0.00002370
Iteration 59/1000 | Loss: 0.00002370
Iteration 60/1000 | Loss: 0.00002370
Iteration 61/1000 | Loss: 0.00002370
Iteration 62/1000 | Loss: 0.00002369
Iteration 63/1000 | Loss: 0.00002369
Iteration 64/1000 | Loss: 0.00002369
Iteration 65/1000 | Loss: 0.00002369
Iteration 66/1000 | Loss: 0.00002369
Iteration 67/1000 | Loss: 0.00002368
Iteration 68/1000 | Loss: 0.00002368
Iteration 69/1000 | Loss: 0.00002368
Iteration 70/1000 | Loss: 0.00002368
Iteration 71/1000 | Loss: 0.00002368
Iteration 72/1000 | Loss: 0.00002368
Iteration 73/1000 | Loss: 0.00002367
Iteration 74/1000 | Loss: 0.00002367
Iteration 75/1000 | Loss: 0.00002367
Iteration 76/1000 | Loss: 0.00002367
Iteration 77/1000 | Loss: 0.00002367
Iteration 78/1000 | Loss: 0.00002367
Iteration 79/1000 | Loss: 0.00002367
Iteration 80/1000 | Loss: 0.00002367
Iteration 81/1000 | Loss: 0.00002367
Iteration 82/1000 | Loss: 0.00002367
Iteration 83/1000 | Loss: 0.00002366
Iteration 84/1000 | Loss: 0.00002366
Iteration 85/1000 | Loss: 0.00002366
Iteration 86/1000 | Loss: 0.00002366
Iteration 87/1000 | Loss: 0.00002366
Iteration 88/1000 | Loss: 0.00002366
Iteration 89/1000 | Loss: 0.00002366
Iteration 90/1000 | Loss: 0.00002366
Iteration 91/1000 | Loss: 0.00002365
Iteration 92/1000 | Loss: 0.00002365
Iteration 93/1000 | Loss: 0.00002365
Iteration 94/1000 | Loss: 0.00002365
Iteration 95/1000 | Loss: 0.00002364
Iteration 96/1000 | Loss: 0.00002364
Iteration 97/1000 | Loss: 0.00002364
Iteration 98/1000 | Loss: 0.00002364
Iteration 99/1000 | Loss: 0.00002364
Iteration 100/1000 | Loss: 0.00002364
Iteration 101/1000 | Loss: 0.00002364
Iteration 102/1000 | Loss: 0.00002364
Iteration 103/1000 | Loss: 0.00002364
Iteration 104/1000 | Loss: 0.00002364
Iteration 105/1000 | Loss: 0.00002364
Iteration 106/1000 | Loss: 0.00002364
Iteration 107/1000 | Loss: 0.00002364
Iteration 108/1000 | Loss: 0.00002363
Iteration 109/1000 | Loss: 0.00002363
Iteration 110/1000 | Loss: 0.00002363
Iteration 111/1000 | Loss: 0.00002363
Iteration 112/1000 | Loss: 0.00002363
Iteration 113/1000 | Loss: 0.00002363
Iteration 114/1000 | Loss: 0.00002363
Iteration 115/1000 | Loss: 0.00002363
Iteration 116/1000 | Loss: 0.00002363
Iteration 117/1000 | Loss: 0.00002363
Iteration 118/1000 | Loss: 0.00002363
Iteration 119/1000 | Loss: 0.00002363
Iteration 120/1000 | Loss: 0.00002363
Iteration 121/1000 | Loss: 0.00002362
Iteration 122/1000 | Loss: 0.00002362
Iteration 123/1000 | Loss: 0.00002362
Iteration 124/1000 | Loss: 0.00002362
Iteration 125/1000 | Loss: 0.00002362
Iteration 126/1000 | Loss: 0.00002362
Iteration 127/1000 | Loss: 0.00002362
Iteration 128/1000 | Loss: 0.00002362
Iteration 129/1000 | Loss: 0.00002362
Iteration 130/1000 | Loss: 0.00002362
Iteration 131/1000 | Loss: 0.00002362
Iteration 132/1000 | Loss: 0.00002362
Iteration 133/1000 | Loss: 0.00002362
Iteration 134/1000 | Loss: 0.00002362
Iteration 135/1000 | Loss: 0.00002362
Iteration 136/1000 | Loss: 0.00002362
Iteration 137/1000 | Loss: 0.00002362
Iteration 138/1000 | Loss: 0.00002362
Iteration 139/1000 | Loss: 0.00002362
Iteration 140/1000 | Loss: 0.00002362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.3624819732503965e-05, 2.3624819732503965e-05, 2.3624819732503965e-05, 2.3624819732503965e-05, 2.3624819732503965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3624819732503965e-05

Optimization complete. Final v2v error: 4.092536449432373 mm

Highest mean error: 4.469898223876953 mm for frame 82

Lowest mean error: 3.6302239894866943 mm for frame 113

Saving results

Total time: 34.94756054878235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00956784
Iteration 2/25 | Loss: 0.00150335
Iteration 3/25 | Loss: 0.00114379
Iteration 4/25 | Loss: 0.00105506
Iteration 5/25 | Loss: 0.00103431
Iteration 6/25 | Loss: 0.00102130
Iteration 7/25 | Loss: 0.00101735
Iteration 8/25 | Loss: 0.00100737
Iteration 9/25 | Loss: 0.00100190
Iteration 10/25 | Loss: 0.00098891
Iteration 11/25 | Loss: 0.00098228
Iteration 12/25 | Loss: 0.00097964
Iteration 13/25 | Loss: 0.00097691
Iteration 14/25 | Loss: 0.00096698
Iteration 15/25 | Loss: 0.00096974
Iteration 16/25 | Loss: 0.00096946
Iteration 17/25 | Loss: 0.00096920
Iteration 18/25 | Loss: 0.00096789
Iteration 19/25 | Loss: 0.00096533
Iteration 20/25 | Loss: 0.00096428
Iteration 21/25 | Loss: 0.00096328
Iteration 22/25 | Loss: 0.00096327
Iteration 23/25 | Loss: 0.00095779
Iteration 24/25 | Loss: 0.00095708
Iteration 25/25 | Loss: 0.00096124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.38717365
Iteration 2/25 | Loss: 0.00226294
Iteration 3/25 | Loss: 0.00226290
Iteration 4/25 | Loss: 0.00226290
Iteration 5/25 | Loss: 0.00226290
Iteration 6/25 | Loss: 0.00226290
Iteration 7/25 | Loss: 0.00226290
Iteration 8/25 | Loss: 0.00226290
Iteration 9/25 | Loss: 0.00226290
Iteration 10/25 | Loss: 0.00226290
Iteration 11/25 | Loss: 0.00226290
Iteration 12/25 | Loss: 0.00226290
Iteration 13/25 | Loss: 0.00226290
Iteration 14/25 | Loss: 0.00226290
Iteration 15/25 | Loss: 0.00226290
Iteration 16/25 | Loss: 0.00226290
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0022628959268331528, 0.0022628959268331528, 0.0022628959268331528, 0.0022628959268331528, 0.0022628959268331528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022628959268331528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226290
Iteration 2/1000 | Loss: 0.00045806
Iteration 3/1000 | Loss: 0.00025642
Iteration 4/1000 | Loss: 0.00019627
Iteration 5/1000 | Loss: 0.00008222
Iteration 6/1000 | Loss: 0.00005112
Iteration 7/1000 | Loss: 0.00004317
Iteration 8/1000 | Loss: 0.00003847
Iteration 9/1000 | Loss: 0.00028575
Iteration 10/1000 | Loss: 0.00005649
Iteration 11/1000 | Loss: 0.00004837
Iteration 12/1000 | Loss: 0.00004596
Iteration 13/1000 | Loss: 0.00004427
Iteration 14/1000 | Loss: 0.00004305
Iteration 15/1000 | Loss: 0.00034212
Iteration 16/1000 | Loss: 0.00006227
Iteration 17/1000 | Loss: 0.00003922
Iteration 18/1000 | Loss: 0.00003472
Iteration 19/1000 | Loss: 0.00003333
Iteration 20/1000 | Loss: 0.00003231
Iteration 21/1000 | Loss: 0.00003108
Iteration 22/1000 | Loss: 0.00002980
Iteration 23/1000 | Loss: 0.00002914
Iteration 24/1000 | Loss: 0.00002870
Iteration 25/1000 | Loss: 0.00002813
Iteration 26/1000 | Loss: 0.00002750
Iteration 27/1000 | Loss: 0.00002720
Iteration 28/1000 | Loss: 0.00002702
Iteration 29/1000 | Loss: 0.00002689
Iteration 30/1000 | Loss: 0.00002682
Iteration 31/1000 | Loss: 0.00002682
Iteration 32/1000 | Loss: 0.00002680
Iteration 33/1000 | Loss: 0.00002680
Iteration 34/1000 | Loss: 0.00002679
Iteration 35/1000 | Loss: 0.00002679
Iteration 36/1000 | Loss: 0.00002678
Iteration 37/1000 | Loss: 0.00002677
Iteration 38/1000 | Loss: 0.00002676
Iteration 39/1000 | Loss: 0.00002676
Iteration 40/1000 | Loss: 0.00002675
Iteration 41/1000 | Loss: 0.00002675
Iteration 42/1000 | Loss: 0.00002674
Iteration 43/1000 | Loss: 0.00002674
Iteration 44/1000 | Loss: 0.00002674
Iteration 45/1000 | Loss: 0.00002673
Iteration 46/1000 | Loss: 0.00002673
Iteration 47/1000 | Loss: 0.00002673
Iteration 48/1000 | Loss: 0.00002672
Iteration 49/1000 | Loss: 0.00002672
Iteration 50/1000 | Loss: 0.00002672
Iteration 51/1000 | Loss: 0.00002672
Iteration 52/1000 | Loss: 0.00002672
Iteration 53/1000 | Loss: 0.00002672
Iteration 54/1000 | Loss: 0.00002672
Iteration 55/1000 | Loss: 0.00002672
Iteration 56/1000 | Loss: 0.00002672
Iteration 57/1000 | Loss: 0.00002671
Iteration 58/1000 | Loss: 0.00002671
Iteration 59/1000 | Loss: 0.00002671
Iteration 60/1000 | Loss: 0.00002671
Iteration 61/1000 | Loss: 0.00002671
Iteration 62/1000 | Loss: 0.00002671
Iteration 63/1000 | Loss: 0.00002671
Iteration 64/1000 | Loss: 0.00002671
Iteration 65/1000 | Loss: 0.00002671
Iteration 66/1000 | Loss: 0.00002671
Iteration 67/1000 | Loss: 0.00002671
Iteration 68/1000 | Loss: 0.00002671
Iteration 69/1000 | Loss: 0.00002670
Iteration 70/1000 | Loss: 0.00002670
Iteration 71/1000 | Loss: 0.00002670
Iteration 72/1000 | Loss: 0.00002670
Iteration 73/1000 | Loss: 0.00002670
Iteration 74/1000 | Loss: 0.00002670
Iteration 75/1000 | Loss: 0.00002670
Iteration 76/1000 | Loss: 0.00002670
Iteration 77/1000 | Loss: 0.00002670
Iteration 78/1000 | Loss: 0.00002670
Iteration 79/1000 | Loss: 0.00002670
Iteration 80/1000 | Loss: 0.00002670
Iteration 81/1000 | Loss: 0.00002670
Iteration 82/1000 | Loss: 0.00002670
Iteration 83/1000 | Loss: 0.00002670
Iteration 84/1000 | Loss: 0.00002670
Iteration 85/1000 | Loss: 0.00002670
Iteration 86/1000 | Loss: 0.00002670
Iteration 87/1000 | Loss: 0.00002670
Iteration 88/1000 | Loss: 0.00002670
Iteration 89/1000 | Loss: 0.00002670
Iteration 90/1000 | Loss: 0.00002670
Iteration 91/1000 | Loss: 0.00002670
Iteration 92/1000 | Loss: 0.00002670
Iteration 93/1000 | Loss: 0.00002670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [2.6703739422373474e-05, 2.6703739422373474e-05, 2.6703739422373474e-05, 2.6703739422373474e-05, 2.6703739422373474e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6703739422373474e-05

Optimization complete. Final v2v error: 4.27410888671875 mm

Highest mean error: 7.812682628631592 mm for frame 95

Lowest mean error: 3.529386281967163 mm for frame 186

Saving results

Total time: 94.09429740905762
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431841
Iteration 2/25 | Loss: 0.00097122
Iteration 3/25 | Loss: 0.00088289
Iteration 4/25 | Loss: 0.00087112
Iteration 5/25 | Loss: 0.00086753
Iteration 6/25 | Loss: 0.00086686
Iteration 7/25 | Loss: 0.00086686
Iteration 8/25 | Loss: 0.00086686
Iteration 9/25 | Loss: 0.00086686
Iteration 10/25 | Loss: 0.00086686
Iteration 11/25 | Loss: 0.00086686
Iteration 12/25 | Loss: 0.00086686
Iteration 13/25 | Loss: 0.00086686
Iteration 14/25 | Loss: 0.00086686
Iteration 15/25 | Loss: 0.00086686
Iteration 16/25 | Loss: 0.00086686
Iteration 17/25 | Loss: 0.00086686
Iteration 18/25 | Loss: 0.00086686
Iteration 19/25 | Loss: 0.00086686
Iteration 20/25 | Loss: 0.00086686
Iteration 21/25 | Loss: 0.00086686
Iteration 22/25 | Loss: 0.00086686
Iteration 23/25 | Loss: 0.00086686
Iteration 24/25 | Loss: 0.00086686
Iteration 25/25 | Loss: 0.00086686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66401982
Iteration 2/25 | Loss: 0.00211627
Iteration 3/25 | Loss: 0.00211626
Iteration 4/25 | Loss: 0.00211626
Iteration 5/25 | Loss: 0.00211626
Iteration 6/25 | Loss: 0.00211626
Iteration 7/25 | Loss: 0.00211626
Iteration 8/25 | Loss: 0.00211626
Iteration 9/25 | Loss: 0.00211626
Iteration 10/25 | Loss: 0.00211626
Iteration 11/25 | Loss: 0.00211626
Iteration 12/25 | Loss: 0.00211626
Iteration 13/25 | Loss: 0.00211626
Iteration 14/25 | Loss: 0.00211626
Iteration 15/25 | Loss: 0.00211626
Iteration 16/25 | Loss: 0.00211626
Iteration 17/25 | Loss: 0.00211626
Iteration 18/25 | Loss: 0.00211626
Iteration 19/25 | Loss: 0.00211626
Iteration 20/25 | Loss: 0.00211626
Iteration 21/25 | Loss: 0.00211626
Iteration 22/25 | Loss: 0.00211626
Iteration 23/25 | Loss: 0.00211626
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0021162626799196005, 0.0021162626799196005, 0.0021162626799196005, 0.0021162626799196005, 0.0021162626799196005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021162626799196005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211626
Iteration 2/1000 | Loss: 0.00002909
Iteration 3/1000 | Loss: 0.00002257
Iteration 4/1000 | Loss: 0.00002125
Iteration 5/1000 | Loss: 0.00002050
Iteration 6/1000 | Loss: 0.00002012
Iteration 7/1000 | Loss: 0.00001981
Iteration 8/1000 | Loss: 0.00001978
Iteration 9/1000 | Loss: 0.00001975
Iteration 10/1000 | Loss: 0.00001971
Iteration 11/1000 | Loss: 0.00001971
Iteration 12/1000 | Loss: 0.00001970
Iteration 13/1000 | Loss: 0.00001970
Iteration 14/1000 | Loss: 0.00001970
Iteration 15/1000 | Loss: 0.00001968
Iteration 16/1000 | Loss: 0.00001968
Iteration 17/1000 | Loss: 0.00001968
Iteration 18/1000 | Loss: 0.00001968
Iteration 19/1000 | Loss: 0.00001967
Iteration 20/1000 | Loss: 0.00001959
Iteration 21/1000 | Loss: 0.00001959
Iteration 22/1000 | Loss: 0.00001959
Iteration 23/1000 | Loss: 0.00001958
Iteration 24/1000 | Loss: 0.00001958
Iteration 25/1000 | Loss: 0.00001958
Iteration 26/1000 | Loss: 0.00001957
Iteration 27/1000 | Loss: 0.00001957
Iteration 28/1000 | Loss: 0.00001955
Iteration 29/1000 | Loss: 0.00001955
Iteration 30/1000 | Loss: 0.00001954
Iteration 31/1000 | Loss: 0.00001953
Iteration 32/1000 | Loss: 0.00001952
Iteration 33/1000 | Loss: 0.00001952
Iteration 34/1000 | Loss: 0.00001951
Iteration 35/1000 | Loss: 0.00001951
Iteration 36/1000 | Loss: 0.00001951
Iteration 37/1000 | Loss: 0.00001951
Iteration 38/1000 | Loss: 0.00001951
Iteration 39/1000 | Loss: 0.00001951
Iteration 40/1000 | Loss: 0.00001951
Iteration 41/1000 | Loss: 0.00001950
Iteration 42/1000 | Loss: 0.00001950
Iteration 43/1000 | Loss: 0.00001950
Iteration 44/1000 | Loss: 0.00001950
Iteration 45/1000 | Loss: 0.00001950
Iteration 46/1000 | Loss: 0.00001950
Iteration 47/1000 | Loss: 0.00001950
Iteration 48/1000 | Loss: 0.00001950
Iteration 49/1000 | Loss: 0.00001950
Iteration 50/1000 | Loss: 0.00001950
Iteration 51/1000 | Loss: 0.00001950
Iteration 52/1000 | Loss: 0.00001949
Iteration 53/1000 | Loss: 0.00001949
Iteration 54/1000 | Loss: 0.00001949
Iteration 55/1000 | Loss: 0.00001949
Iteration 56/1000 | Loss: 0.00001949
Iteration 57/1000 | Loss: 0.00001949
Iteration 58/1000 | Loss: 0.00001949
Iteration 59/1000 | Loss: 0.00001949
Iteration 60/1000 | Loss: 0.00001949
Iteration 61/1000 | Loss: 0.00001948
Iteration 62/1000 | Loss: 0.00001948
Iteration 63/1000 | Loss: 0.00001948
Iteration 64/1000 | Loss: 0.00001948
Iteration 65/1000 | Loss: 0.00001948
Iteration 66/1000 | Loss: 0.00001948
Iteration 67/1000 | Loss: 0.00001948
Iteration 68/1000 | Loss: 0.00001948
Iteration 69/1000 | Loss: 0.00001948
Iteration 70/1000 | Loss: 0.00001948
Iteration 71/1000 | Loss: 0.00001948
Iteration 72/1000 | Loss: 0.00001947
Iteration 73/1000 | Loss: 0.00001947
Iteration 74/1000 | Loss: 0.00001947
Iteration 75/1000 | Loss: 0.00001947
Iteration 76/1000 | Loss: 0.00001947
Iteration 77/1000 | Loss: 0.00001946
Iteration 78/1000 | Loss: 0.00001946
Iteration 79/1000 | Loss: 0.00001946
Iteration 80/1000 | Loss: 0.00001946
Iteration 81/1000 | Loss: 0.00001946
Iteration 82/1000 | Loss: 0.00001945
Iteration 83/1000 | Loss: 0.00001945
Iteration 84/1000 | Loss: 0.00001945
Iteration 85/1000 | Loss: 0.00001945
Iteration 86/1000 | Loss: 0.00001945
Iteration 87/1000 | Loss: 0.00001945
Iteration 88/1000 | Loss: 0.00001945
Iteration 89/1000 | Loss: 0.00001945
Iteration 90/1000 | Loss: 0.00001945
Iteration 91/1000 | Loss: 0.00001945
Iteration 92/1000 | Loss: 0.00001945
Iteration 93/1000 | Loss: 0.00001945
Iteration 94/1000 | Loss: 0.00001945
Iteration 95/1000 | Loss: 0.00001945
Iteration 96/1000 | Loss: 0.00001945
Iteration 97/1000 | Loss: 0.00001945
Iteration 98/1000 | Loss: 0.00001945
Iteration 99/1000 | Loss: 0.00001945
Iteration 100/1000 | Loss: 0.00001945
Iteration 101/1000 | Loss: 0.00001945
Iteration 102/1000 | Loss: 0.00001945
Iteration 103/1000 | Loss: 0.00001945
Iteration 104/1000 | Loss: 0.00001945
Iteration 105/1000 | Loss: 0.00001945
Iteration 106/1000 | Loss: 0.00001945
Iteration 107/1000 | Loss: 0.00001945
Iteration 108/1000 | Loss: 0.00001945
Iteration 109/1000 | Loss: 0.00001945
Iteration 110/1000 | Loss: 0.00001945
Iteration 111/1000 | Loss: 0.00001945
Iteration 112/1000 | Loss: 0.00001945
Iteration 113/1000 | Loss: 0.00001945
Iteration 114/1000 | Loss: 0.00001945
Iteration 115/1000 | Loss: 0.00001945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.945110307133291e-05, 1.945110307133291e-05, 1.945110307133291e-05, 1.945110307133291e-05, 1.945110307133291e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.945110307133291e-05

Optimization complete. Final v2v error: 3.7954623699188232 mm

Highest mean error: 4.098067760467529 mm for frame 46

Lowest mean error: 3.550177812576294 mm for frame 87

Saving results

Total time: 29.880244970321655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410844
Iteration 2/25 | Loss: 0.00090856
Iteration 3/25 | Loss: 0.00082169
Iteration 4/25 | Loss: 0.00080496
Iteration 5/25 | Loss: 0.00079809
Iteration 6/25 | Loss: 0.00079665
Iteration 7/25 | Loss: 0.00079652
Iteration 8/25 | Loss: 0.00079652
Iteration 9/25 | Loss: 0.00079652
Iteration 10/25 | Loss: 0.00079652
Iteration 11/25 | Loss: 0.00079652
Iteration 12/25 | Loss: 0.00079652
Iteration 13/25 | Loss: 0.00079652
Iteration 14/25 | Loss: 0.00079652
Iteration 15/25 | Loss: 0.00079652
Iteration 16/25 | Loss: 0.00079652
Iteration 17/25 | Loss: 0.00079652
Iteration 18/25 | Loss: 0.00079652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007965245167724788, 0.0007965245167724788, 0.0007965245167724788, 0.0007965245167724788, 0.0007965245167724788]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007965245167724788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62856126
Iteration 2/25 | Loss: 0.00194488
Iteration 3/25 | Loss: 0.00194487
Iteration 4/25 | Loss: 0.00194487
Iteration 5/25 | Loss: 0.00194487
Iteration 6/25 | Loss: 0.00194487
Iteration 7/25 | Loss: 0.00194487
Iteration 8/25 | Loss: 0.00194487
Iteration 9/25 | Loss: 0.00194487
Iteration 10/25 | Loss: 0.00194487
Iteration 11/25 | Loss: 0.00194487
Iteration 12/25 | Loss: 0.00194487
Iteration 13/25 | Loss: 0.00194487
Iteration 14/25 | Loss: 0.00194487
Iteration 15/25 | Loss: 0.00194487
Iteration 16/25 | Loss: 0.00194487
Iteration 17/25 | Loss: 0.00194487
Iteration 18/25 | Loss: 0.00194487
Iteration 19/25 | Loss: 0.00194487
Iteration 20/25 | Loss: 0.00194487
Iteration 21/25 | Loss: 0.00194487
Iteration 22/25 | Loss: 0.00194487
Iteration 23/25 | Loss: 0.00194487
Iteration 24/25 | Loss: 0.00194487
Iteration 25/25 | Loss: 0.00194487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194487
Iteration 2/1000 | Loss: 0.00002879
Iteration 3/1000 | Loss: 0.00001984
Iteration 4/1000 | Loss: 0.00001759
Iteration 5/1000 | Loss: 0.00001689
Iteration 6/1000 | Loss: 0.00001640
Iteration 7/1000 | Loss: 0.00001604
Iteration 8/1000 | Loss: 0.00001602
Iteration 9/1000 | Loss: 0.00001582
Iteration 10/1000 | Loss: 0.00001577
Iteration 11/1000 | Loss: 0.00001571
Iteration 12/1000 | Loss: 0.00001569
Iteration 13/1000 | Loss: 0.00001568
Iteration 14/1000 | Loss: 0.00001567
Iteration 15/1000 | Loss: 0.00001567
Iteration 16/1000 | Loss: 0.00001560
Iteration 17/1000 | Loss: 0.00001560
Iteration 18/1000 | Loss: 0.00001557
Iteration 19/1000 | Loss: 0.00001556
Iteration 20/1000 | Loss: 0.00001556
Iteration 21/1000 | Loss: 0.00001555
Iteration 22/1000 | Loss: 0.00001555
Iteration 23/1000 | Loss: 0.00001554
Iteration 24/1000 | Loss: 0.00001554
Iteration 25/1000 | Loss: 0.00001553
Iteration 26/1000 | Loss: 0.00001553
Iteration 27/1000 | Loss: 0.00001553
Iteration 28/1000 | Loss: 0.00001553
Iteration 29/1000 | Loss: 0.00001553
Iteration 30/1000 | Loss: 0.00001553
Iteration 31/1000 | Loss: 0.00001553
Iteration 32/1000 | Loss: 0.00001553
Iteration 33/1000 | Loss: 0.00001553
Iteration 34/1000 | Loss: 0.00001553
Iteration 35/1000 | Loss: 0.00001553
Iteration 36/1000 | Loss: 0.00001552
Iteration 37/1000 | Loss: 0.00001552
Iteration 38/1000 | Loss: 0.00001552
Iteration 39/1000 | Loss: 0.00001552
Iteration 40/1000 | Loss: 0.00001552
Iteration 41/1000 | Loss: 0.00001551
Iteration 42/1000 | Loss: 0.00001551
Iteration 43/1000 | Loss: 0.00001550
Iteration 44/1000 | Loss: 0.00001550
Iteration 45/1000 | Loss: 0.00001550
Iteration 46/1000 | Loss: 0.00001550
Iteration 47/1000 | Loss: 0.00001550
Iteration 48/1000 | Loss: 0.00001549
Iteration 49/1000 | Loss: 0.00001549
Iteration 50/1000 | Loss: 0.00001549
Iteration 51/1000 | Loss: 0.00001549
Iteration 52/1000 | Loss: 0.00001548
Iteration 53/1000 | Loss: 0.00001548
Iteration 54/1000 | Loss: 0.00001548
Iteration 55/1000 | Loss: 0.00001548
Iteration 56/1000 | Loss: 0.00001548
Iteration 57/1000 | Loss: 0.00001548
Iteration 58/1000 | Loss: 0.00001548
Iteration 59/1000 | Loss: 0.00001548
Iteration 60/1000 | Loss: 0.00001548
Iteration 61/1000 | Loss: 0.00001548
Iteration 62/1000 | Loss: 0.00001547
Iteration 63/1000 | Loss: 0.00001547
Iteration 64/1000 | Loss: 0.00001547
Iteration 65/1000 | Loss: 0.00001547
Iteration 66/1000 | Loss: 0.00001546
Iteration 67/1000 | Loss: 0.00001546
Iteration 68/1000 | Loss: 0.00001546
Iteration 69/1000 | Loss: 0.00001546
Iteration 70/1000 | Loss: 0.00001546
Iteration 71/1000 | Loss: 0.00001546
Iteration 72/1000 | Loss: 0.00001546
Iteration 73/1000 | Loss: 0.00001546
Iteration 74/1000 | Loss: 0.00001545
Iteration 75/1000 | Loss: 0.00001545
Iteration 76/1000 | Loss: 0.00001545
Iteration 77/1000 | Loss: 0.00001544
Iteration 78/1000 | Loss: 0.00001544
Iteration 79/1000 | Loss: 0.00001544
Iteration 80/1000 | Loss: 0.00001544
Iteration 81/1000 | Loss: 0.00001544
Iteration 82/1000 | Loss: 0.00001544
Iteration 83/1000 | Loss: 0.00001544
Iteration 84/1000 | Loss: 0.00001544
Iteration 85/1000 | Loss: 0.00001543
Iteration 86/1000 | Loss: 0.00001543
Iteration 87/1000 | Loss: 0.00001543
Iteration 88/1000 | Loss: 0.00001543
Iteration 89/1000 | Loss: 0.00001543
Iteration 90/1000 | Loss: 0.00001543
Iteration 91/1000 | Loss: 0.00001543
Iteration 92/1000 | Loss: 0.00001543
Iteration 93/1000 | Loss: 0.00001543
Iteration 94/1000 | Loss: 0.00001543
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001542
Iteration 97/1000 | Loss: 0.00001542
Iteration 98/1000 | Loss: 0.00001542
Iteration 99/1000 | Loss: 0.00001542
Iteration 100/1000 | Loss: 0.00001542
Iteration 101/1000 | Loss: 0.00001542
Iteration 102/1000 | Loss: 0.00001542
Iteration 103/1000 | Loss: 0.00001542
Iteration 104/1000 | Loss: 0.00001542
Iteration 105/1000 | Loss: 0.00001541
Iteration 106/1000 | Loss: 0.00001541
Iteration 107/1000 | Loss: 0.00001541
Iteration 108/1000 | Loss: 0.00001540
Iteration 109/1000 | Loss: 0.00001540
Iteration 110/1000 | Loss: 0.00001540
Iteration 111/1000 | Loss: 0.00001540
Iteration 112/1000 | Loss: 0.00001540
Iteration 113/1000 | Loss: 0.00001540
Iteration 114/1000 | Loss: 0.00001540
Iteration 115/1000 | Loss: 0.00001540
Iteration 116/1000 | Loss: 0.00001540
Iteration 117/1000 | Loss: 0.00001540
Iteration 118/1000 | Loss: 0.00001540
Iteration 119/1000 | Loss: 0.00001539
Iteration 120/1000 | Loss: 0.00001539
Iteration 121/1000 | Loss: 0.00001539
Iteration 122/1000 | Loss: 0.00001539
Iteration 123/1000 | Loss: 0.00001539
Iteration 124/1000 | Loss: 0.00001539
Iteration 125/1000 | Loss: 0.00001539
Iteration 126/1000 | Loss: 0.00001539
Iteration 127/1000 | Loss: 0.00001539
Iteration 128/1000 | Loss: 0.00001539
Iteration 129/1000 | Loss: 0.00001539
Iteration 130/1000 | Loss: 0.00001538
Iteration 131/1000 | Loss: 0.00001538
Iteration 132/1000 | Loss: 0.00001538
Iteration 133/1000 | Loss: 0.00001537
Iteration 134/1000 | Loss: 0.00001537
Iteration 135/1000 | Loss: 0.00001537
Iteration 136/1000 | Loss: 0.00001537
Iteration 137/1000 | Loss: 0.00001537
Iteration 138/1000 | Loss: 0.00001536
Iteration 139/1000 | Loss: 0.00001536
Iteration 140/1000 | Loss: 0.00001536
Iteration 141/1000 | Loss: 0.00001536
Iteration 142/1000 | Loss: 0.00001536
Iteration 143/1000 | Loss: 0.00001536
Iteration 144/1000 | Loss: 0.00001535
Iteration 145/1000 | Loss: 0.00001535
Iteration 146/1000 | Loss: 0.00001535
Iteration 147/1000 | Loss: 0.00001535
Iteration 148/1000 | Loss: 0.00001534
Iteration 149/1000 | Loss: 0.00001534
Iteration 150/1000 | Loss: 0.00001534
Iteration 151/1000 | Loss: 0.00001534
Iteration 152/1000 | Loss: 0.00001534
Iteration 153/1000 | Loss: 0.00001534
Iteration 154/1000 | Loss: 0.00001534
Iteration 155/1000 | Loss: 0.00001533
Iteration 156/1000 | Loss: 0.00001533
Iteration 157/1000 | Loss: 0.00001533
Iteration 158/1000 | Loss: 0.00001533
Iteration 159/1000 | Loss: 0.00001533
Iteration 160/1000 | Loss: 0.00001533
Iteration 161/1000 | Loss: 0.00001533
Iteration 162/1000 | Loss: 0.00001533
Iteration 163/1000 | Loss: 0.00001533
Iteration 164/1000 | Loss: 0.00001533
Iteration 165/1000 | Loss: 0.00001533
Iteration 166/1000 | Loss: 0.00001532
Iteration 167/1000 | Loss: 0.00001532
Iteration 168/1000 | Loss: 0.00001532
Iteration 169/1000 | Loss: 0.00001532
Iteration 170/1000 | Loss: 0.00001532
Iteration 171/1000 | Loss: 0.00001532
Iteration 172/1000 | Loss: 0.00001532
Iteration 173/1000 | Loss: 0.00001532
Iteration 174/1000 | Loss: 0.00001532
Iteration 175/1000 | Loss: 0.00001532
Iteration 176/1000 | Loss: 0.00001532
Iteration 177/1000 | Loss: 0.00001532
Iteration 178/1000 | Loss: 0.00001532
Iteration 179/1000 | Loss: 0.00001532
Iteration 180/1000 | Loss: 0.00001532
Iteration 181/1000 | Loss: 0.00001532
Iteration 182/1000 | Loss: 0.00001532
Iteration 183/1000 | Loss: 0.00001532
Iteration 184/1000 | Loss: 0.00001532
Iteration 185/1000 | Loss: 0.00001532
Iteration 186/1000 | Loss: 0.00001532
Iteration 187/1000 | Loss: 0.00001532
Iteration 188/1000 | Loss: 0.00001532
Iteration 189/1000 | Loss: 0.00001532
Iteration 190/1000 | Loss: 0.00001532
Iteration 191/1000 | Loss: 0.00001532
Iteration 192/1000 | Loss: 0.00001532
Iteration 193/1000 | Loss: 0.00001532
Iteration 194/1000 | Loss: 0.00001532
Iteration 195/1000 | Loss: 0.00001532
Iteration 196/1000 | Loss: 0.00001532
Iteration 197/1000 | Loss: 0.00001532
Iteration 198/1000 | Loss: 0.00001532
Iteration 199/1000 | Loss: 0.00001532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.5317344150389545e-05, 1.5317344150389545e-05, 1.5317344150389545e-05, 1.5317344150389545e-05, 1.5317344150389545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5317344150389545e-05

Optimization complete. Final v2v error: 3.3215396404266357 mm

Highest mean error: 3.55757737159729 mm for frame 122

Lowest mean error: 2.979665517807007 mm for frame 0

Saving results

Total time: 39.979286670684814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399931
Iteration 2/25 | Loss: 0.00107501
Iteration 3/25 | Loss: 0.00092858
Iteration 4/25 | Loss: 0.00089385
Iteration 5/25 | Loss: 0.00088347
Iteration 6/25 | Loss: 0.00088112
Iteration 7/25 | Loss: 0.00088034
Iteration 8/25 | Loss: 0.00088022
Iteration 9/25 | Loss: 0.00088022
Iteration 10/25 | Loss: 0.00088022
Iteration 11/25 | Loss: 0.00088022
Iteration 12/25 | Loss: 0.00088022
Iteration 13/25 | Loss: 0.00088022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008802152588032186, 0.0008802152588032186, 0.0008802152588032186, 0.0008802152588032186, 0.0008802152588032186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008802152588032186

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62304926
Iteration 2/25 | Loss: 0.00252000
Iteration 3/25 | Loss: 0.00252000
Iteration 4/25 | Loss: 0.00252000
Iteration 5/25 | Loss: 0.00252000
Iteration 6/25 | Loss: 0.00252000
Iteration 7/25 | Loss: 0.00252000
Iteration 8/25 | Loss: 0.00252000
Iteration 9/25 | Loss: 0.00252000
Iteration 10/25 | Loss: 0.00252000
Iteration 11/25 | Loss: 0.00252000
Iteration 12/25 | Loss: 0.00252000
Iteration 13/25 | Loss: 0.00252000
Iteration 14/25 | Loss: 0.00252000
Iteration 15/25 | Loss: 0.00252000
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0025199968367815018, 0.0025199968367815018, 0.0025199968367815018, 0.0025199968367815018, 0.0025199968367815018]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025199968367815018

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00252000
Iteration 2/1000 | Loss: 0.00005764
Iteration 3/1000 | Loss: 0.00003772
Iteration 4/1000 | Loss: 0.00003194
Iteration 5/1000 | Loss: 0.00002842
Iteration 6/1000 | Loss: 0.00002693
Iteration 7/1000 | Loss: 0.00002601
Iteration 8/1000 | Loss: 0.00002537
Iteration 9/1000 | Loss: 0.00002495
Iteration 10/1000 | Loss: 0.00002445
Iteration 11/1000 | Loss: 0.00002411
Iteration 12/1000 | Loss: 0.00002391
Iteration 13/1000 | Loss: 0.00002374
Iteration 14/1000 | Loss: 0.00002371
Iteration 15/1000 | Loss: 0.00002368
Iteration 16/1000 | Loss: 0.00002364
Iteration 17/1000 | Loss: 0.00002358
Iteration 18/1000 | Loss: 0.00002354
Iteration 19/1000 | Loss: 0.00002354
Iteration 20/1000 | Loss: 0.00002353
Iteration 21/1000 | Loss: 0.00002351
Iteration 22/1000 | Loss: 0.00002350
Iteration 23/1000 | Loss: 0.00002349
Iteration 24/1000 | Loss: 0.00002346
Iteration 25/1000 | Loss: 0.00002345
Iteration 26/1000 | Loss: 0.00002344
Iteration 27/1000 | Loss: 0.00002342
Iteration 28/1000 | Loss: 0.00002342
Iteration 29/1000 | Loss: 0.00002340
Iteration 30/1000 | Loss: 0.00002340
Iteration 31/1000 | Loss: 0.00002339
Iteration 32/1000 | Loss: 0.00002339
Iteration 33/1000 | Loss: 0.00002338
Iteration 34/1000 | Loss: 0.00002338
Iteration 35/1000 | Loss: 0.00002336
Iteration 36/1000 | Loss: 0.00002336
Iteration 37/1000 | Loss: 0.00002335
Iteration 38/1000 | Loss: 0.00002335
Iteration 39/1000 | Loss: 0.00002334
Iteration 40/1000 | Loss: 0.00002334
Iteration 41/1000 | Loss: 0.00002334
Iteration 42/1000 | Loss: 0.00002333
Iteration 43/1000 | Loss: 0.00002333
Iteration 44/1000 | Loss: 0.00002332
Iteration 45/1000 | Loss: 0.00002332
Iteration 46/1000 | Loss: 0.00002332
Iteration 47/1000 | Loss: 0.00002331
Iteration 48/1000 | Loss: 0.00002331
Iteration 49/1000 | Loss: 0.00002331
Iteration 50/1000 | Loss: 0.00002331
Iteration 51/1000 | Loss: 0.00002331
Iteration 52/1000 | Loss: 0.00002331
Iteration 53/1000 | Loss: 0.00002331
Iteration 54/1000 | Loss: 0.00002330
Iteration 55/1000 | Loss: 0.00002330
Iteration 56/1000 | Loss: 0.00002330
Iteration 57/1000 | Loss: 0.00002330
Iteration 58/1000 | Loss: 0.00002330
Iteration 59/1000 | Loss: 0.00002330
Iteration 60/1000 | Loss: 0.00002330
Iteration 61/1000 | Loss: 0.00002330
Iteration 62/1000 | Loss: 0.00002329
Iteration 63/1000 | Loss: 0.00002329
Iteration 64/1000 | Loss: 0.00002329
Iteration 65/1000 | Loss: 0.00002329
Iteration 66/1000 | Loss: 0.00002328
Iteration 67/1000 | Loss: 0.00002328
Iteration 68/1000 | Loss: 0.00002328
Iteration 69/1000 | Loss: 0.00002328
Iteration 70/1000 | Loss: 0.00002328
Iteration 71/1000 | Loss: 0.00002328
Iteration 72/1000 | Loss: 0.00002327
Iteration 73/1000 | Loss: 0.00002327
Iteration 74/1000 | Loss: 0.00002327
Iteration 75/1000 | Loss: 0.00002326
Iteration 76/1000 | Loss: 0.00002326
Iteration 77/1000 | Loss: 0.00002326
Iteration 78/1000 | Loss: 0.00002326
Iteration 79/1000 | Loss: 0.00002325
Iteration 80/1000 | Loss: 0.00002325
Iteration 81/1000 | Loss: 0.00002325
Iteration 82/1000 | Loss: 0.00002325
Iteration 83/1000 | Loss: 0.00002325
Iteration 84/1000 | Loss: 0.00002325
Iteration 85/1000 | Loss: 0.00002325
Iteration 86/1000 | Loss: 0.00002324
Iteration 87/1000 | Loss: 0.00002324
Iteration 88/1000 | Loss: 0.00002324
Iteration 89/1000 | Loss: 0.00002324
Iteration 90/1000 | Loss: 0.00002324
Iteration 91/1000 | Loss: 0.00002323
Iteration 92/1000 | Loss: 0.00002323
Iteration 93/1000 | Loss: 0.00002323
Iteration 94/1000 | Loss: 0.00002323
Iteration 95/1000 | Loss: 0.00002323
Iteration 96/1000 | Loss: 0.00002323
Iteration 97/1000 | Loss: 0.00002323
Iteration 98/1000 | Loss: 0.00002323
Iteration 99/1000 | Loss: 0.00002323
Iteration 100/1000 | Loss: 0.00002323
Iteration 101/1000 | Loss: 0.00002323
Iteration 102/1000 | Loss: 0.00002323
Iteration 103/1000 | Loss: 0.00002323
Iteration 104/1000 | Loss: 0.00002322
Iteration 105/1000 | Loss: 0.00002322
Iteration 106/1000 | Loss: 0.00002322
Iteration 107/1000 | Loss: 0.00002322
Iteration 108/1000 | Loss: 0.00002322
Iteration 109/1000 | Loss: 0.00002322
Iteration 110/1000 | Loss: 0.00002321
Iteration 111/1000 | Loss: 0.00002321
Iteration 112/1000 | Loss: 0.00002321
Iteration 113/1000 | Loss: 0.00002321
Iteration 114/1000 | Loss: 0.00002320
Iteration 115/1000 | Loss: 0.00002320
Iteration 116/1000 | Loss: 0.00002320
Iteration 117/1000 | Loss: 0.00002320
Iteration 118/1000 | Loss: 0.00002320
Iteration 119/1000 | Loss: 0.00002320
Iteration 120/1000 | Loss: 0.00002320
Iteration 121/1000 | Loss: 0.00002320
Iteration 122/1000 | Loss: 0.00002319
Iteration 123/1000 | Loss: 0.00002319
Iteration 124/1000 | Loss: 0.00002319
Iteration 125/1000 | Loss: 0.00002319
Iteration 126/1000 | Loss: 0.00002319
Iteration 127/1000 | Loss: 0.00002319
Iteration 128/1000 | Loss: 0.00002319
Iteration 129/1000 | Loss: 0.00002319
Iteration 130/1000 | Loss: 0.00002319
Iteration 131/1000 | Loss: 0.00002319
Iteration 132/1000 | Loss: 0.00002319
Iteration 133/1000 | Loss: 0.00002319
Iteration 134/1000 | Loss: 0.00002318
Iteration 135/1000 | Loss: 0.00002318
Iteration 136/1000 | Loss: 0.00002318
Iteration 137/1000 | Loss: 0.00002318
Iteration 138/1000 | Loss: 0.00002318
Iteration 139/1000 | Loss: 0.00002318
Iteration 140/1000 | Loss: 0.00002318
Iteration 141/1000 | Loss: 0.00002318
Iteration 142/1000 | Loss: 0.00002318
Iteration 143/1000 | Loss: 0.00002318
Iteration 144/1000 | Loss: 0.00002318
Iteration 145/1000 | Loss: 0.00002318
Iteration 146/1000 | Loss: 0.00002318
Iteration 147/1000 | Loss: 0.00002318
Iteration 148/1000 | Loss: 0.00002318
Iteration 149/1000 | Loss: 0.00002318
Iteration 150/1000 | Loss: 0.00002318
Iteration 151/1000 | Loss: 0.00002318
Iteration 152/1000 | Loss: 0.00002318
Iteration 153/1000 | Loss: 0.00002318
Iteration 154/1000 | Loss: 0.00002318
Iteration 155/1000 | Loss: 0.00002318
Iteration 156/1000 | Loss: 0.00002318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.317962935194373e-05, 2.317962935194373e-05, 2.317962935194373e-05, 2.317962935194373e-05, 2.317962935194373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.317962935194373e-05

Optimization complete. Final v2v error: 3.9507546424865723 mm

Highest mean error: 4.5000410079956055 mm for frame 92

Lowest mean error: 3.2459163665771484 mm for frame 80

Saving results

Total time: 40.840731620788574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897944
Iteration 2/25 | Loss: 0.00126136
Iteration 3/25 | Loss: 0.00107780
Iteration 4/25 | Loss: 0.00103291
Iteration 5/25 | Loss: 0.00102264
Iteration 6/25 | Loss: 0.00102228
Iteration 7/25 | Loss: 0.00102228
Iteration 8/25 | Loss: 0.00102228
Iteration 9/25 | Loss: 0.00102228
Iteration 10/25 | Loss: 0.00102228
Iteration 11/25 | Loss: 0.00102228
Iteration 12/25 | Loss: 0.00102228
Iteration 13/25 | Loss: 0.00102228
Iteration 14/25 | Loss: 0.00102228
Iteration 15/25 | Loss: 0.00102228
Iteration 16/25 | Loss: 0.00102228
Iteration 17/25 | Loss: 0.00102228
Iteration 18/25 | Loss: 0.00102228
Iteration 19/25 | Loss: 0.00102228
Iteration 20/25 | Loss: 0.00102228
Iteration 21/25 | Loss: 0.00102228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010222842684015632, 0.0010222842684015632, 0.0010222842684015632, 0.0010222842684015632, 0.0010222842684015632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010222842684015632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16963172
Iteration 2/25 | Loss: 0.00231233
Iteration 3/25 | Loss: 0.00231232
Iteration 4/25 | Loss: 0.00231231
Iteration 5/25 | Loss: 0.00231231
Iteration 6/25 | Loss: 0.00231231
Iteration 7/25 | Loss: 0.00231231
Iteration 8/25 | Loss: 0.00231231
Iteration 9/25 | Loss: 0.00231231
Iteration 10/25 | Loss: 0.00231231
Iteration 11/25 | Loss: 0.00231231
Iteration 12/25 | Loss: 0.00231231
Iteration 13/25 | Loss: 0.00231231
Iteration 14/25 | Loss: 0.00231231
Iteration 15/25 | Loss: 0.00231231
Iteration 16/25 | Loss: 0.00231231
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002312311204150319, 0.002312311204150319, 0.002312311204150319, 0.002312311204150319, 0.002312311204150319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002312311204150319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231231
Iteration 2/1000 | Loss: 0.00005040
Iteration 3/1000 | Loss: 0.00003812
Iteration 4/1000 | Loss: 0.00003518
Iteration 5/1000 | Loss: 0.00003265
Iteration 6/1000 | Loss: 0.00003155
Iteration 7/1000 | Loss: 0.00003065
Iteration 8/1000 | Loss: 0.00003012
Iteration 9/1000 | Loss: 0.00002982
Iteration 10/1000 | Loss: 0.00002960
Iteration 11/1000 | Loss: 0.00002941
Iteration 12/1000 | Loss: 0.00002941
Iteration 13/1000 | Loss: 0.00002941
Iteration 14/1000 | Loss: 0.00002940
Iteration 15/1000 | Loss: 0.00002940
Iteration 16/1000 | Loss: 0.00002929
Iteration 17/1000 | Loss: 0.00002928
Iteration 18/1000 | Loss: 0.00002928
Iteration 19/1000 | Loss: 0.00002928
Iteration 20/1000 | Loss: 0.00002927
Iteration 21/1000 | Loss: 0.00002927
Iteration 22/1000 | Loss: 0.00002927
Iteration 23/1000 | Loss: 0.00002927
Iteration 24/1000 | Loss: 0.00002927
Iteration 25/1000 | Loss: 0.00002926
Iteration 26/1000 | Loss: 0.00002926
Iteration 27/1000 | Loss: 0.00002926
Iteration 28/1000 | Loss: 0.00002926
Iteration 29/1000 | Loss: 0.00002926
Iteration 30/1000 | Loss: 0.00002926
Iteration 31/1000 | Loss: 0.00002926
Iteration 32/1000 | Loss: 0.00002926
Iteration 33/1000 | Loss: 0.00002926
Iteration 34/1000 | Loss: 0.00002925
Iteration 35/1000 | Loss: 0.00002925
Iteration 36/1000 | Loss: 0.00002925
Iteration 37/1000 | Loss: 0.00002925
Iteration 38/1000 | Loss: 0.00002925
Iteration 39/1000 | Loss: 0.00002925
Iteration 40/1000 | Loss: 0.00002925
Iteration 41/1000 | Loss: 0.00002925
Iteration 42/1000 | Loss: 0.00002925
Iteration 43/1000 | Loss: 0.00002925
Iteration 44/1000 | Loss: 0.00002924
Iteration 45/1000 | Loss: 0.00002924
Iteration 46/1000 | Loss: 0.00002924
Iteration 47/1000 | Loss: 0.00002924
Iteration 48/1000 | Loss: 0.00002924
Iteration 49/1000 | Loss: 0.00002924
Iteration 50/1000 | Loss: 0.00002923
Iteration 51/1000 | Loss: 0.00002923
Iteration 52/1000 | Loss: 0.00002923
Iteration 53/1000 | Loss: 0.00002923
Iteration 54/1000 | Loss: 0.00002923
Iteration 55/1000 | Loss: 0.00002922
Iteration 56/1000 | Loss: 0.00002922
Iteration 57/1000 | Loss: 0.00002922
Iteration 58/1000 | Loss: 0.00002922
Iteration 59/1000 | Loss: 0.00002921
Iteration 60/1000 | Loss: 0.00002921
Iteration 61/1000 | Loss: 0.00002921
Iteration 62/1000 | Loss: 0.00002921
Iteration 63/1000 | Loss: 0.00002921
Iteration 64/1000 | Loss: 0.00002921
Iteration 65/1000 | Loss: 0.00002921
Iteration 66/1000 | Loss: 0.00002921
Iteration 67/1000 | Loss: 0.00002921
Iteration 68/1000 | Loss: 0.00002921
Iteration 69/1000 | Loss: 0.00002921
Iteration 70/1000 | Loss: 0.00002921
Iteration 71/1000 | Loss: 0.00002921
Iteration 72/1000 | Loss: 0.00002921
Iteration 73/1000 | Loss: 0.00002921
Iteration 74/1000 | Loss: 0.00002921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [2.920543738582637e-05, 2.920543738582637e-05, 2.920543738582637e-05, 2.920543738582637e-05, 2.920543738582637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.920543738582637e-05

Optimization complete. Final v2v error: 4.492228984832764 mm

Highest mean error: 4.775940418243408 mm for frame 239

Lowest mean error: 4.245671272277832 mm for frame 4

Saving results

Total time: 32.06113243103027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01135912
Iteration 2/25 | Loss: 0.01135912
Iteration 3/25 | Loss: 0.01135912
Iteration 4/25 | Loss: 0.01135912
Iteration 5/25 | Loss: 0.01135912
Iteration 6/25 | Loss: 0.01135912
Iteration 7/25 | Loss: 0.01135911
Iteration 8/25 | Loss: 0.01135911
Iteration 9/25 | Loss: 0.01135911
Iteration 10/25 | Loss: 0.01135911
Iteration 11/25 | Loss: 0.01135911
Iteration 12/25 | Loss: 0.01135911
Iteration 13/25 | Loss: 0.01135911
Iteration 14/25 | Loss: 0.01135911
Iteration 15/25 | Loss: 0.01135910
Iteration 16/25 | Loss: 0.01135910
Iteration 17/25 | Loss: 0.01135910
Iteration 18/25 | Loss: 0.01135910
Iteration 19/25 | Loss: 0.01135910
Iteration 20/25 | Loss: 0.01135910
Iteration 21/25 | Loss: 0.01135910
Iteration 22/25 | Loss: 0.01135910
Iteration 23/25 | Loss: 0.01135910
Iteration 24/25 | Loss: 0.01135910
Iteration 25/25 | Loss: 0.01135909

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60705626
Iteration 2/25 | Loss: 0.16359483
Iteration 3/25 | Loss: 0.16213782
Iteration 4/25 | Loss: 0.16208898
Iteration 5/25 | Loss: 0.16208895
Iteration 6/25 | Loss: 0.16208895
Iteration 7/25 | Loss: 0.16208893
Iteration 8/25 | Loss: 0.16208893
Iteration 9/25 | Loss: 0.16208893
Iteration 10/25 | Loss: 0.16208893
Iteration 11/25 | Loss: 0.16208893
Iteration 12/25 | Loss: 0.16208893
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.16208893060684204, 0.16208893060684204, 0.16208893060684204, 0.16208893060684204, 0.16208893060684204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.16208893060684204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16208893
Iteration 2/1000 | Loss: 0.00325175
Iteration 3/1000 | Loss: 0.00313957
Iteration 4/1000 | Loss: 0.00123948
Iteration 5/1000 | Loss: 0.00339589
Iteration 6/1000 | Loss: 0.00013349
Iteration 7/1000 | Loss: 0.00062207
Iteration 8/1000 | Loss: 0.00294717
Iteration 9/1000 | Loss: 0.00041953
Iteration 10/1000 | Loss: 0.00070814
Iteration 11/1000 | Loss: 0.00054754
Iteration 12/1000 | Loss: 0.00061807
Iteration 13/1000 | Loss: 0.00052333
Iteration 14/1000 | Loss: 0.00006137
Iteration 15/1000 | Loss: 0.00005211
Iteration 16/1000 | Loss: 0.00059249
Iteration 17/1000 | Loss: 0.00004227
Iteration 18/1000 | Loss: 0.00003671
Iteration 19/1000 | Loss: 0.00003441
Iteration 20/1000 | Loss: 0.00021387
Iteration 21/1000 | Loss: 0.00003202
Iteration 22/1000 | Loss: 0.00016005
Iteration 23/1000 | Loss: 0.00003287
Iteration 24/1000 | Loss: 0.00007834
Iteration 25/1000 | Loss: 0.00002959
Iteration 26/1000 | Loss: 0.00002815
Iteration 27/1000 | Loss: 0.00002743
Iteration 28/1000 | Loss: 0.00020770
Iteration 29/1000 | Loss: 0.00003463
Iteration 30/1000 | Loss: 0.00003380
Iteration 31/1000 | Loss: 0.00002642
Iteration 32/1000 | Loss: 0.00002595
Iteration 33/1000 | Loss: 0.00002545
Iteration 34/1000 | Loss: 0.00002512
Iteration 35/1000 | Loss: 0.00002485
Iteration 36/1000 | Loss: 0.00002464
Iteration 37/1000 | Loss: 0.00020837
Iteration 38/1000 | Loss: 0.00002504
Iteration 39/1000 | Loss: 0.00002451
Iteration 40/1000 | Loss: 0.00002441
Iteration 41/1000 | Loss: 0.00002440
Iteration 42/1000 | Loss: 0.00002440
Iteration 43/1000 | Loss: 0.00002439
Iteration 44/1000 | Loss: 0.00002439
Iteration 45/1000 | Loss: 0.00002439
Iteration 46/1000 | Loss: 0.00002439
Iteration 47/1000 | Loss: 0.00002439
Iteration 48/1000 | Loss: 0.00002438
Iteration 49/1000 | Loss: 0.00002438
Iteration 50/1000 | Loss: 0.00002438
Iteration 51/1000 | Loss: 0.00002438
Iteration 52/1000 | Loss: 0.00002438
Iteration 53/1000 | Loss: 0.00002437
Iteration 54/1000 | Loss: 0.00002437
Iteration 55/1000 | Loss: 0.00002437
Iteration 56/1000 | Loss: 0.00002436
Iteration 57/1000 | Loss: 0.00002436
Iteration 58/1000 | Loss: 0.00002436
Iteration 59/1000 | Loss: 0.00002435
Iteration 60/1000 | Loss: 0.00002435
Iteration 61/1000 | Loss: 0.00002434
Iteration 62/1000 | Loss: 0.00002433
Iteration 63/1000 | Loss: 0.00002433
Iteration 64/1000 | Loss: 0.00002433
Iteration 65/1000 | Loss: 0.00002433
Iteration 66/1000 | Loss: 0.00002433
Iteration 67/1000 | Loss: 0.00002432
Iteration 68/1000 | Loss: 0.00002432
Iteration 69/1000 | Loss: 0.00002432
Iteration 70/1000 | Loss: 0.00002431
Iteration 71/1000 | Loss: 0.00002431
Iteration 72/1000 | Loss: 0.00002431
Iteration 73/1000 | Loss: 0.00002430
Iteration 74/1000 | Loss: 0.00002429
Iteration 75/1000 | Loss: 0.00002429
Iteration 76/1000 | Loss: 0.00002428
Iteration 77/1000 | Loss: 0.00002427
Iteration 78/1000 | Loss: 0.00002426
Iteration 79/1000 | Loss: 0.00002424
Iteration 80/1000 | Loss: 0.00002423
Iteration 81/1000 | Loss: 0.00002423
Iteration 82/1000 | Loss: 0.00002423
Iteration 83/1000 | Loss: 0.00002422
Iteration 84/1000 | Loss: 0.00002422
Iteration 85/1000 | Loss: 0.00002422
Iteration 86/1000 | Loss: 0.00002421
Iteration 87/1000 | Loss: 0.00002421
Iteration 88/1000 | Loss: 0.00002420
Iteration 89/1000 | Loss: 0.00002420
Iteration 90/1000 | Loss: 0.00002420
Iteration 91/1000 | Loss: 0.00002420
Iteration 92/1000 | Loss: 0.00002420
Iteration 93/1000 | Loss: 0.00002420
Iteration 94/1000 | Loss: 0.00002420
Iteration 95/1000 | Loss: 0.00002419
Iteration 96/1000 | Loss: 0.00002419
Iteration 97/1000 | Loss: 0.00002419
Iteration 98/1000 | Loss: 0.00002419
Iteration 99/1000 | Loss: 0.00002419
Iteration 100/1000 | Loss: 0.00002419
Iteration 101/1000 | Loss: 0.00002419
Iteration 102/1000 | Loss: 0.00002419
Iteration 103/1000 | Loss: 0.00002419
Iteration 104/1000 | Loss: 0.00002419
Iteration 105/1000 | Loss: 0.00002419
Iteration 106/1000 | Loss: 0.00002419
Iteration 107/1000 | Loss: 0.00002419
Iteration 108/1000 | Loss: 0.00002419
Iteration 109/1000 | Loss: 0.00002418
Iteration 110/1000 | Loss: 0.00002418
Iteration 111/1000 | Loss: 0.00002418
Iteration 112/1000 | Loss: 0.00002418
Iteration 113/1000 | Loss: 0.00002418
Iteration 114/1000 | Loss: 0.00002418
Iteration 115/1000 | Loss: 0.00002418
Iteration 116/1000 | Loss: 0.00002418
Iteration 117/1000 | Loss: 0.00002418
Iteration 118/1000 | Loss: 0.00002418
Iteration 119/1000 | Loss: 0.00002417
Iteration 120/1000 | Loss: 0.00002417
Iteration 121/1000 | Loss: 0.00002417
Iteration 122/1000 | Loss: 0.00002417
Iteration 123/1000 | Loss: 0.00002417
Iteration 124/1000 | Loss: 0.00002417
Iteration 125/1000 | Loss: 0.00002417
Iteration 126/1000 | Loss: 0.00002417
Iteration 127/1000 | Loss: 0.00002417
Iteration 128/1000 | Loss: 0.00002417
Iteration 129/1000 | Loss: 0.00002417
Iteration 130/1000 | Loss: 0.00002417
Iteration 131/1000 | Loss: 0.00002417
Iteration 132/1000 | Loss: 0.00002417
Iteration 133/1000 | Loss: 0.00002417
Iteration 134/1000 | Loss: 0.00002417
Iteration 135/1000 | Loss: 0.00002417
Iteration 136/1000 | Loss: 0.00002417
Iteration 137/1000 | Loss: 0.00002417
Iteration 138/1000 | Loss: 0.00002416
Iteration 139/1000 | Loss: 0.00002416
Iteration 140/1000 | Loss: 0.00002416
Iteration 141/1000 | Loss: 0.00002416
Iteration 142/1000 | Loss: 0.00002416
Iteration 143/1000 | Loss: 0.00002416
Iteration 144/1000 | Loss: 0.00002416
Iteration 145/1000 | Loss: 0.00002416
Iteration 146/1000 | Loss: 0.00002416
Iteration 147/1000 | Loss: 0.00002416
Iteration 148/1000 | Loss: 0.00002416
Iteration 149/1000 | Loss: 0.00002416
Iteration 150/1000 | Loss: 0.00002416
Iteration 151/1000 | Loss: 0.00002416
Iteration 152/1000 | Loss: 0.00002416
Iteration 153/1000 | Loss: 0.00002416
Iteration 154/1000 | Loss: 0.00002416
Iteration 155/1000 | Loss: 0.00002416
Iteration 156/1000 | Loss: 0.00002416
Iteration 157/1000 | Loss: 0.00002415
Iteration 158/1000 | Loss: 0.00002415
Iteration 159/1000 | Loss: 0.00002415
Iteration 160/1000 | Loss: 0.00002415
Iteration 161/1000 | Loss: 0.00002415
Iteration 162/1000 | Loss: 0.00002415
Iteration 163/1000 | Loss: 0.00002415
Iteration 164/1000 | Loss: 0.00002415
Iteration 165/1000 | Loss: 0.00002415
Iteration 166/1000 | Loss: 0.00002415
Iteration 167/1000 | Loss: 0.00002415
Iteration 168/1000 | Loss: 0.00002415
Iteration 169/1000 | Loss: 0.00002415
Iteration 170/1000 | Loss: 0.00002415
Iteration 171/1000 | Loss: 0.00002415
Iteration 172/1000 | Loss: 0.00002415
Iteration 173/1000 | Loss: 0.00002415
Iteration 174/1000 | Loss: 0.00002414
Iteration 175/1000 | Loss: 0.00002414
Iteration 176/1000 | Loss: 0.00002414
Iteration 177/1000 | Loss: 0.00002414
Iteration 178/1000 | Loss: 0.00002414
Iteration 179/1000 | Loss: 0.00002414
Iteration 180/1000 | Loss: 0.00002414
Iteration 181/1000 | Loss: 0.00002414
Iteration 182/1000 | Loss: 0.00002413
Iteration 183/1000 | Loss: 0.00002413
Iteration 184/1000 | Loss: 0.00002413
Iteration 185/1000 | Loss: 0.00002413
Iteration 186/1000 | Loss: 0.00002413
Iteration 187/1000 | Loss: 0.00002413
Iteration 188/1000 | Loss: 0.00002413
Iteration 189/1000 | Loss: 0.00002413
Iteration 190/1000 | Loss: 0.00002413
Iteration 191/1000 | Loss: 0.00002413
Iteration 192/1000 | Loss: 0.00002413
Iteration 193/1000 | Loss: 0.00002413
Iteration 194/1000 | Loss: 0.00002413
Iteration 195/1000 | Loss: 0.00002412
Iteration 196/1000 | Loss: 0.00002412
Iteration 197/1000 | Loss: 0.00002412
Iteration 198/1000 | Loss: 0.00002412
Iteration 199/1000 | Loss: 0.00002412
Iteration 200/1000 | Loss: 0.00002412
Iteration 201/1000 | Loss: 0.00002412
Iteration 202/1000 | Loss: 0.00002412
Iteration 203/1000 | Loss: 0.00002412
Iteration 204/1000 | Loss: 0.00002412
Iteration 205/1000 | Loss: 0.00002412
Iteration 206/1000 | Loss: 0.00002412
Iteration 207/1000 | Loss: 0.00002412
Iteration 208/1000 | Loss: 0.00002412
Iteration 209/1000 | Loss: 0.00002412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.412221692793537e-05, 2.412221692793537e-05, 2.412221692793537e-05, 2.412221692793537e-05, 2.412221692793537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.412221692793537e-05

Optimization complete. Final v2v error: 4.120353698730469 mm

Highest mean error: 10.044445037841797 mm for frame 3

Lowest mean error: 3.367597818374634 mm for frame 1

Saving results

Total time: 81.7073564529419
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01153903
Iteration 2/25 | Loss: 0.00217774
Iteration 3/25 | Loss: 0.00171373
Iteration 4/25 | Loss: 0.00133961
Iteration 5/25 | Loss: 0.00113292
Iteration 6/25 | Loss: 0.00109007
Iteration 7/25 | Loss: 0.00108376
Iteration 8/25 | Loss: 0.00108280
Iteration 9/25 | Loss: 0.00108280
Iteration 10/25 | Loss: 0.00108280
Iteration 11/25 | Loss: 0.00108280
Iteration 12/25 | Loss: 0.00108280
Iteration 13/25 | Loss: 0.00108280
Iteration 14/25 | Loss: 0.00108280
Iteration 15/25 | Loss: 0.00108280
Iteration 16/25 | Loss: 0.00108280
Iteration 17/25 | Loss: 0.00108280
Iteration 18/25 | Loss: 0.00108280
Iteration 19/25 | Loss: 0.00108280
Iteration 20/25 | Loss: 0.00108280
Iteration 21/25 | Loss: 0.00108280
Iteration 22/25 | Loss: 0.00108280
Iteration 23/25 | Loss: 0.00108280
Iteration 24/25 | Loss: 0.00108280
Iteration 25/25 | Loss: 0.00108280

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47743642
Iteration 2/25 | Loss: 0.00163857
Iteration 3/25 | Loss: 0.00163856
Iteration 4/25 | Loss: 0.00163856
Iteration 5/25 | Loss: 0.00163856
Iteration 6/25 | Loss: 0.00163856
Iteration 7/25 | Loss: 0.00163856
Iteration 8/25 | Loss: 0.00163856
Iteration 9/25 | Loss: 0.00163856
Iteration 10/25 | Loss: 0.00163856
Iteration 11/25 | Loss: 0.00163856
Iteration 12/25 | Loss: 0.00163856
Iteration 13/25 | Loss: 0.00163855
Iteration 14/25 | Loss: 0.00163855
Iteration 15/25 | Loss: 0.00163855
Iteration 16/25 | Loss: 0.00163856
Iteration 17/25 | Loss: 0.00163855
Iteration 18/25 | Loss: 0.00163855
Iteration 19/25 | Loss: 0.00163855
Iteration 20/25 | Loss: 0.00163855
Iteration 21/25 | Loss: 0.00163855
Iteration 22/25 | Loss: 0.00163855
Iteration 23/25 | Loss: 0.00163855
Iteration 24/25 | Loss: 0.00163855
Iteration 25/25 | Loss: 0.00163855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163855
Iteration 2/1000 | Loss: 0.00005765
Iteration 3/1000 | Loss: 0.00004430
Iteration 4/1000 | Loss: 0.00003982
Iteration 5/1000 | Loss: 0.00003773
Iteration 6/1000 | Loss: 0.00003675
Iteration 7/1000 | Loss: 0.00003619
Iteration 8/1000 | Loss: 0.00003581
Iteration 9/1000 | Loss: 0.00003555
Iteration 10/1000 | Loss: 0.00003524
Iteration 11/1000 | Loss: 0.00003505
Iteration 12/1000 | Loss: 0.00003498
Iteration 13/1000 | Loss: 0.00003496
Iteration 14/1000 | Loss: 0.00003495
Iteration 15/1000 | Loss: 0.00003486
Iteration 16/1000 | Loss: 0.00003478
Iteration 17/1000 | Loss: 0.00003478
Iteration 18/1000 | Loss: 0.00003476
Iteration 19/1000 | Loss: 0.00003472
Iteration 20/1000 | Loss: 0.00003470
Iteration 21/1000 | Loss: 0.00003466
Iteration 22/1000 | Loss: 0.00003466
Iteration 23/1000 | Loss: 0.00003465
Iteration 24/1000 | Loss: 0.00003459
Iteration 25/1000 | Loss: 0.00003456
Iteration 26/1000 | Loss: 0.00003456
Iteration 27/1000 | Loss: 0.00003456
Iteration 28/1000 | Loss: 0.00003456
Iteration 29/1000 | Loss: 0.00003456
Iteration 30/1000 | Loss: 0.00003456
Iteration 31/1000 | Loss: 0.00003456
Iteration 32/1000 | Loss: 0.00003456
Iteration 33/1000 | Loss: 0.00003455
Iteration 34/1000 | Loss: 0.00003454
Iteration 35/1000 | Loss: 0.00003454
Iteration 36/1000 | Loss: 0.00003454
Iteration 37/1000 | Loss: 0.00003454
Iteration 38/1000 | Loss: 0.00003454
Iteration 39/1000 | Loss: 0.00003453
Iteration 40/1000 | Loss: 0.00003453
Iteration 41/1000 | Loss: 0.00003453
Iteration 42/1000 | Loss: 0.00003453
Iteration 43/1000 | Loss: 0.00003453
Iteration 44/1000 | Loss: 0.00003453
Iteration 45/1000 | Loss: 0.00003453
Iteration 46/1000 | Loss: 0.00003453
Iteration 47/1000 | Loss: 0.00003453
Iteration 48/1000 | Loss: 0.00003453
Iteration 49/1000 | Loss: 0.00003453
Iteration 50/1000 | Loss: 0.00003453
Iteration 51/1000 | Loss: 0.00003453
Iteration 52/1000 | Loss: 0.00003452
Iteration 53/1000 | Loss: 0.00003452
Iteration 54/1000 | Loss: 0.00003451
Iteration 55/1000 | Loss: 0.00003451
Iteration 56/1000 | Loss: 0.00003451
Iteration 57/1000 | Loss: 0.00003450
Iteration 58/1000 | Loss: 0.00003450
Iteration 59/1000 | Loss: 0.00003450
Iteration 60/1000 | Loss: 0.00003450
Iteration 61/1000 | Loss: 0.00003450
Iteration 62/1000 | Loss: 0.00003449
Iteration 63/1000 | Loss: 0.00003449
Iteration 64/1000 | Loss: 0.00003449
Iteration 65/1000 | Loss: 0.00003449
Iteration 66/1000 | Loss: 0.00003449
Iteration 67/1000 | Loss: 0.00003449
Iteration 68/1000 | Loss: 0.00003449
Iteration 69/1000 | Loss: 0.00003448
Iteration 70/1000 | Loss: 0.00003448
Iteration 71/1000 | Loss: 0.00003448
Iteration 72/1000 | Loss: 0.00003448
Iteration 73/1000 | Loss: 0.00003447
Iteration 74/1000 | Loss: 0.00003447
Iteration 75/1000 | Loss: 0.00003447
Iteration 76/1000 | Loss: 0.00003447
Iteration 77/1000 | Loss: 0.00003447
Iteration 78/1000 | Loss: 0.00003447
Iteration 79/1000 | Loss: 0.00003447
Iteration 80/1000 | Loss: 0.00003447
Iteration 81/1000 | Loss: 0.00003447
Iteration 82/1000 | Loss: 0.00003447
Iteration 83/1000 | Loss: 0.00003447
Iteration 84/1000 | Loss: 0.00003447
Iteration 85/1000 | Loss: 0.00003447
Iteration 86/1000 | Loss: 0.00003447
Iteration 87/1000 | Loss: 0.00003446
Iteration 88/1000 | Loss: 0.00003446
Iteration 89/1000 | Loss: 0.00003446
Iteration 90/1000 | Loss: 0.00003446
Iteration 91/1000 | Loss: 0.00003445
Iteration 92/1000 | Loss: 0.00003445
Iteration 93/1000 | Loss: 0.00003445
Iteration 94/1000 | Loss: 0.00003445
Iteration 95/1000 | Loss: 0.00003445
Iteration 96/1000 | Loss: 0.00003445
Iteration 97/1000 | Loss: 0.00003445
Iteration 98/1000 | Loss: 0.00003445
Iteration 99/1000 | Loss: 0.00003445
Iteration 100/1000 | Loss: 0.00003445
Iteration 101/1000 | Loss: 0.00003445
Iteration 102/1000 | Loss: 0.00003445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [3.4451815736247227e-05, 3.4451815736247227e-05, 3.4451815736247227e-05, 3.4451815736247227e-05, 3.4451815736247227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4451815736247227e-05

Optimization complete. Final v2v error: 4.831747055053711 mm

Highest mean error: 5.143426418304443 mm for frame 102

Lowest mean error: 4.461482524871826 mm for frame 152

Saving results

Total time: 43.188392877578735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082121
Iteration 2/25 | Loss: 0.00214469
Iteration 3/25 | Loss: 0.00166343
Iteration 4/25 | Loss: 0.00198613
Iteration 5/25 | Loss: 0.00112832
Iteration 6/25 | Loss: 0.00101059
Iteration 7/25 | Loss: 0.00098913
Iteration 8/25 | Loss: 0.00096632
Iteration 9/25 | Loss: 0.00094985
Iteration 10/25 | Loss: 0.00095511
Iteration 11/25 | Loss: 0.00095274
Iteration 12/25 | Loss: 0.00094563
Iteration 13/25 | Loss: 0.00094450
Iteration 14/25 | Loss: 0.00094426
Iteration 15/25 | Loss: 0.00094413
Iteration 16/25 | Loss: 0.00094391
Iteration 17/25 | Loss: 0.00094379
Iteration 18/25 | Loss: 0.00094371
Iteration 19/25 | Loss: 0.00094364
Iteration 20/25 | Loss: 0.00094361
Iteration 21/25 | Loss: 0.00094361
Iteration 22/25 | Loss: 0.00094353
Iteration 23/25 | Loss: 0.00094341
Iteration 24/25 | Loss: 0.00094910
Iteration 25/25 | Loss: 0.00094045

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64415205
Iteration 2/25 | Loss: 0.00227122
Iteration 3/25 | Loss: 0.00227122
Iteration 4/25 | Loss: 0.00227121
Iteration 5/25 | Loss: 0.00227121
Iteration 6/25 | Loss: 0.00227121
Iteration 7/25 | Loss: 0.00227121
Iteration 8/25 | Loss: 0.00227121
Iteration 9/25 | Loss: 0.00227121
Iteration 10/25 | Loss: 0.00227121
Iteration 11/25 | Loss: 0.00227121
Iteration 12/25 | Loss: 0.00227121
Iteration 13/25 | Loss: 0.00227121
Iteration 14/25 | Loss: 0.00227121
Iteration 15/25 | Loss: 0.00227121
Iteration 16/25 | Loss: 0.00227121
Iteration 17/25 | Loss: 0.00227121
Iteration 18/25 | Loss: 0.00227121
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0022712128702551126, 0.0022712128702551126, 0.0022712128702551126, 0.0022712128702551126, 0.0022712128702551126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022712128702551126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227121
Iteration 2/1000 | Loss: 0.00005063
Iteration 3/1000 | Loss: 0.00003928
Iteration 4/1000 | Loss: 0.00003317
Iteration 5/1000 | Loss: 0.00003044
Iteration 6/1000 | Loss: 0.00002883
Iteration 7/1000 | Loss: 0.00002785
Iteration 8/1000 | Loss: 0.00002710
Iteration 9/1000 | Loss: 0.00002651
Iteration 10/1000 | Loss: 0.00040579
Iteration 11/1000 | Loss: 0.00002733
Iteration 12/1000 | Loss: 0.00002538
Iteration 13/1000 | Loss: 0.00002378
Iteration 14/1000 | Loss: 0.00002302
Iteration 15/1000 | Loss: 0.00002269
Iteration 16/1000 | Loss: 0.00002252
Iteration 17/1000 | Loss: 0.00002246
Iteration 18/1000 | Loss: 0.00002245
Iteration 19/1000 | Loss: 0.00002240
Iteration 20/1000 | Loss: 0.00002240
Iteration 21/1000 | Loss: 0.00002239
Iteration 22/1000 | Loss: 0.00002239
Iteration 23/1000 | Loss: 0.00002238
Iteration 24/1000 | Loss: 0.00002238
Iteration 25/1000 | Loss: 0.00002238
Iteration 26/1000 | Loss: 0.00002234
Iteration 27/1000 | Loss: 0.00002233
Iteration 28/1000 | Loss: 0.00002232
Iteration 29/1000 | Loss: 0.00002232
Iteration 30/1000 | Loss: 0.00002232
Iteration 31/1000 | Loss: 0.00002232
Iteration 32/1000 | Loss: 0.00002232
Iteration 33/1000 | Loss: 0.00002232
Iteration 34/1000 | Loss: 0.00002231
Iteration 35/1000 | Loss: 0.00002231
Iteration 36/1000 | Loss: 0.00002231
Iteration 37/1000 | Loss: 0.00002231
Iteration 38/1000 | Loss: 0.00002231
Iteration 39/1000 | Loss: 0.00002231
Iteration 40/1000 | Loss: 0.00002231
Iteration 41/1000 | Loss: 0.00002231
Iteration 42/1000 | Loss: 0.00002231
Iteration 43/1000 | Loss: 0.00002231
Iteration 44/1000 | Loss: 0.00002231
Iteration 45/1000 | Loss: 0.00002231
Iteration 46/1000 | Loss: 0.00002230
Iteration 47/1000 | Loss: 0.00002230
Iteration 48/1000 | Loss: 0.00002230
Iteration 49/1000 | Loss: 0.00002230
Iteration 50/1000 | Loss: 0.00002230
Iteration 51/1000 | Loss: 0.00002230
Iteration 52/1000 | Loss: 0.00002229
Iteration 53/1000 | Loss: 0.00002229
Iteration 54/1000 | Loss: 0.00002229
Iteration 55/1000 | Loss: 0.00002228
Iteration 56/1000 | Loss: 0.00002228
Iteration 57/1000 | Loss: 0.00002228
Iteration 58/1000 | Loss: 0.00002227
Iteration 59/1000 | Loss: 0.00002227
Iteration 60/1000 | Loss: 0.00002227
Iteration 61/1000 | Loss: 0.00002227
Iteration 62/1000 | Loss: 0.00002226
Iteration 63/1000 | Loss: 0.00002226
Iteration 64/1000 | Loss: 0.00002226
Iteration 65/1000 | Loss: 0.00002226
Iteration 66/1000 | Loss: 0.00002226
Iteration 67/1000 | Loss: 0.00002226
Iteration 68/1000 | Loss: 0.00002226
Iteration 69/1000 | Loss: 0.00002226
Iteration 70/1000 | Loss: 0.00002225
Iteration 71/1000 | Loss: 0.00002225
Iteration 72/1000 | Loss: 0.00002225
Iteration 73/1000 | Loss: 0.00002225
Iteration 74/1000 | Loss: 0.00002224
Iteration 75/1000 | Loss: 0.00002224
Iteration 76/1000 | Loss: 0.00002224
Iteration 77/1000 | Loss: 0.00002224
Iteration 78/1000 | Loss: 0.00002224
Iteration 79/1000 | Loss: 0.00002224
Iteration 80/1000 | Loss: 0.00002224
Iteration 81/1000 | Loss: 0.00002224
Iteration 82/1000 | Loss: 0.00002224
Iteration 83/1000 | Loss: 0.00002224
Iteration 84/1000 | Loss: 0.00002224
Iteration 85/1000 | Loss: 0.00002224
Iteration 86/1000 | Loss: 0.00002224
Iteration 87/1000 | Loss: 0.00002224
Iteration 88/1000 | Loss: 0.00002224
Iteration 89/1000 | Loss: 0.00002224
Iteration 90/1000 | Loss: 0.00002224
Iteration 91/1000 | Loss: 0.00002224
Iteration 92/1000 | Loss: 0.00002224
Iteration 93/1000 | Loss: 0.00002224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [2.2244219508138485e-05, 2.2244219508138485e-05, 2.2244219508138485e-05, 2.2244219508138485e-05, 2.2244219508138485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2244219508138485e-05

Optimization complete. Final v2v error: 3.893446207046509 mm

Highest mean error: 4.883871078491211 mm for frame 1

Lowest mean error: 3.5346107482910156 mm for frame 95

Saving results

Total time: 69.42638301849365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429222
Iteration 2/25 | Loss: 0.00105437
Iteration 3/25 | Loss: 0.00094269
Iteration 4/25 | Loss: 0.00091067
Iteration 5/25 | Loss: 0.00090110
Iteration 6/25 | Loss: 0.00089942
Iteration 7/25 | Loss: 0.00089886
Iteration 8/25 | Loss: 0.00089885
Iteration 9/25 | Loss: 0.00089885
Iteration 10/25 | Loss: 0.00089885
Iteration 11/25 | Loss: 0.00089885
Iteration 12/25 | Loss: 0.00089885
Iteration 13/25 | Loss: 0.00089885
Iteration 14/25 | Loss: 0.00089885
Iteration 15/25 | Loss: 0.00089885
Iteration 16/25 | Loss: 0.00089885
Iteration 17/25 | Loss: 0.00089885
Iteration 18/25 | Loss: 0.00089885
Iteration 19/25 | Loss: 0.00089885
Iteration 20/25 | Loss: 0.00089885
Iteration 21/25 | Loss: 0.00089885
Iteration 22/25 | Loss: 0.00089885
Iteration 23/25 | Loss: 0.00089885
Iteration 24/25 | Loss: 0.00089885
Iteration 25/25 | Loss: 0.00089885

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.23162746
Iteration 2/25 | Loss: 0.00215521
Iteration 3/25 | Loss: 0.00215521
Iteration 4/25 | Loss: 0.00215521
Iteration 5/25 | Loss: 0.00215521
Iteration 6/25 | Loss: 0.00215521
Iteration 7/25 | Loss: 0.00215521
Iteration 8/25 | Loss: 0.00215521
Iteration 9/25 | Loss: 0.00215521
Iteration 10/25 | Loss: 0.00215521
Iteration 11/25 | Loss: 0.00215521
Iteration 12/25 | Loss: 0.00215521
Iteration 13/25 | Loss: 0.00215521
Iteration 14/25 | Loss: 0.00215521
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0021552059333771467, 0.0021552059333771467, 0.0021552059333771467, 0.0021552059333771467, 0.0021552059333771467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021552059333771467

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215521
Iteration 2/1000 | Loss: 0.00005560
Iteration 3/1000 | Loss: 0.00003572
Iteration 4/1000 | Loss: 0.00003220
Iteration 5/1000 | Loss: 0.00003069
Iteration 6/1000 | Loss: 0.00002963
Iteration 7/1000 | Loss: 0.00002880
Iteration 8/1000 | Loss: 0.00002821
Iteration 9/1000 | Loss: 0.00002792
Iteration 10/1000 | Loss: 0.00002789
Iteration 11/1000 | Loss: 0.00002768
Iteration 12/1000 | Loss: 0.00002764
Iteration 13/1000 | Loss: 0.00002755
Iteration 14/1000 | Loss: 0.00002754
Iteration 15/1000 | Loss: 0.00002754
Iteration 16/1000 | Loss: 0.00002753
Iteration 17/1000 | Loss: 0.00002752
Iteration 18/1000 | Loss: 0.00002745
Iteration 19/1000 | Loss: 0.00002741
Iteration 20/1000 | Loss: 0.00002741
Iteration 21/1000 | Loss: 0.00002738
Iteration 22/1000 | Loss: 0.00002737
Iteration 23/1000 | Loss: 0.00002737
Iteration 24/1000 | Loss: 0.00002736
Iteration 25/1000 | Loss: 0.00002736
Iteration 26/1000 | Loss: 0.00002735
Iteration 27/1000 | Loss: 0.00002735
Iteration 28/1000 | Loss: 0.00002735
Iteration 29/1000 | Loss: 0.00002735
Iteration 30/1000 | Loss: 0.00002735
Iteration 31/1000 | Loss: 0.00002735
Iteration 32/1000 | Loss: 0.00002734
Iteration 33/1000 | Loss: 0.00002734
Iteration 34/1000 | Loss: 0.00002734
Iteration 35/1000 | Loss: 0.00002734
Iteration 36/1000 | Loss: 0.00002734
Iteration 37/1000 | Loss: 0.00002734
Iteration 38/1000 | Loss: 0.00002734
Iteration 39/1000 | Loss: 0.00002734
Iteration 40/1000 | Loss: 0.00002734
Iteration 41/1000 | Loss: 0.00002734
Iteration 42/1000 | Loss: 0.00002734
Iteration 43/1000 | Loss: 0.00002734
Iteration 44/1000 | Loss: 0.00002734
Iteration 45/1000 | Loss: 0.00002733
Iteration 46/1000 | Loss: 0.00002733
Iteration 47/1000 | Loss: 0.00002733
Iteration 48/1000 | Loss: 0.00002733
Iteration 49/1000 | Loss: 0.00002733
Iteration 50/1000 | Loss: 0.00002733
Iteration 51/1000 | Loss: 0.00002733
Iteration 52/1000 | Loss: 0.00002732
Iteration 53/1000 | Loss: 0.00002732
Iteration 54/1000 | Loss: 0.00002732
Iteration 55/1000 | Loss: 0.00002732
Iteration 56/1000 | Loss: 0.00002732
Iteration 57/1000 | Loss: 0.00002732
Iteration 58/1000 | Loss: 0.00002731
Iteration 59/1000 | Loss: 0.00002731
Iteration 60/1000 | Loss: 0.00002731
Iteration 61/1000 | Loss: 0.00002731
Iteration 62/1000 | Loss: 0.00002730
Iteration 63/1000 | Loss: 0.00002730
Iteration 64/1000 | Loss: 0.00002729
Iteration 65/1000 | Loss: 0.00002729
Iteration 66/1000 | Loss: 0.00002729
Iteration 67/1000 | Loss: 0.00002729
Iteration 68/1000 | Loss: 0.00002728
Iteration 69/1000 | Loss: 0.00002728
Iteration 70/1000 | Loss: 0.00002728
Iteration 71/1000 | Loss: 0.00002728
Iteration 72/1000 | Loss: 0.00002728
Iteration 73/1000 | Loss: 0.00002728
Iteration 74/1000 | Loss: 0.00002728
Iteration 75/1000 | Loss: 0.00002728
Iteration 76/1000 | Loss: 0.00002728
Iteration 77/1000 | Loss: 0.00002727
Iteration 78/1000 | Loss: 0.00002727
Iteration 79/1000 | Loss: 0.00002727
Iteration 80/1000 | Loss: 0.00002726
Iteration 81/1000 | Loss: 0.00002725
Iteration 82/1000 | Loss: 0.00002725
Iteration 83/1000 | Loss: 0.00002725
Iteration 84/1000 | Loss: 0.00002725
Iteration 85/1000 | Loss: 0.00002724
Iteration 86/1000 | Loss: 0.00002724
Iteration 87/1000 | Loss: 0.00002724
Iteration 88/1000 | Loss: 0.00002724
Iteration 89/1000 | Loss: 0.00002723
Iteration 90/1000 | Loss: 0.00002723
Iteration 91/1000 | Loss: 0.00002723
Iteration 92/1000 | Loss: 0.00002723
Iteration 93/1000 | Loss: 0.00002723
Iteration 94/1000 | Loss: 0.00002723
Iteration 95/1000 | Loss: 0.00002722
Iteration 96/1000 | Loss: 0.00002722
Iteration 97/1000 | Loss: 0.00002722
Iteration 98/1000 | Loss: 0.00002722
Iteration 99/1000 | Loss: 0.00002722
Iteration 100/1000 | Loss: 0.00002722
Iteration 101/1000 | Loss: 0.00002721
Iteration 102/1000 | Loss: 0.00002721
Iteration 103/1000 | Loss: 0.00002721
Iteration 104/1000 | Loss: 0.00002721
Iteration 105/1000 | Loss: 0.00002721
Iteration 106/1000 | Loss: 0.00002721
Iteration 107/1000 | Loss: 0.00002721
Iteration 108/1000 | Loss: 0.00002721
Iteration 109/1000 | Loss: 0.00002721
Iteration 110/1000 | Loss: 0.00002721
Iteration 111/1000 | Loss: 0.00002720
Iteration 112/1000 | Loss: 0.00002720
Iteration 113/1000 | Loss: 0.00002720
Iteration 114/1000 | Loss: 0.00002720
Iteration 115/1000 | Loss: 0.00002720
Iteration 116/1000 | Loss: 0.00002720
Iteration 117/1000 | Loss: 0.00002719
Iteration 118/1000 | Loss: 0.00002719
Iteration 119/1000 | Loss: 0.00002719
Iteration 120/1000 | Loss: 0.00002719
Iteration 121/1000 | Loss: 0.00002719
Iteration 122/1000 | Loss: 0.00002719
Iteration 123/1000 | Loss: 0.00002719
Iteration 124/1000 | Loss: 0.00002718
Iteration 125/1000 | Loss: 0.00002718
Iteration 126/1000 | Loss: 0.00002718
Iteration 127/1000 | Loss: 0.00002718
Iteration 128/1000 | Loss: 0.00002718
Iteration 129/1000 | Loss: 0.00002718
Iteration 130/1000 | Loss: 0.00002718
Iteration 131/1000 | Loss: 0.00002718
Iteration 132/1000 | Loss: 0.00002718
Iteration 133/1000 | Loss: 0.00002718
Iteration 134/1000 | Loss: 0.00002718
Iteration 135/1000 | Loss: 0.00002718
Iteration 136/1000 | Loss: 0.00002718
Iteration 137/1000 | Loss: 0.00002718
Iteration 138/1000 | Loss: 0.00002717
Iteration 139/1000 | Loss: 0.00002717
Iteration 140/1000 | Loss: 0.00002717
Iteration 141/1000 | Loss: 0.00002717
Iteration 142/1000 | Loss: 0.00002717
Iteration 143/1000 | Loss: 0.00002717
Iteration 144/1000 | Loss: 0.00002717
Iteration 145/1000 | Loss: 0.00002717
Iteration 146/1000 | Loss: 0.00002717
Iteration 147/1000 | Loss: 0.00002717
Iteration 148/1000 | Loss: 0.00002717
Iteration 149/1000 | Loss: 0.00002717
Iteration 150/1000 | Loss: 0.00002716
Iteration 151/1000 | Loss: 0.00002716
Iteration 152/1000 | Loss: 0.00002716
Iteration 153/1000 | Loss: 0.00002716
Iteration 154/1000 | Loss: 0.00002716
Iteration 155/1000 | Loss: 0.00002716
Iteration 156/1000 | Loss: 0.00002716
Iteration 157/1000 | Loss: 0.00002716
Iteration 158/1000 | Loss: 0.00002716
Iteration 159/1000 | Loss: 0.00002715
Iteration 160/1000 | Loss: 0.00002715
Iteration 161/1000 | Loss: 0.00002715
Iteration 162/1000 | Loss: 0.00002715
Iteration 163/1000 | Loss: 0.00002715
Iteration 164/1000 | Loss: 0.00002715
Iteration 165/1000 | Loss: 0.00002715
Iteration 166/1000 | Loss: 0.00002715
Iteration 167/1000 | Loss: 0.00002715
Iteration 168/1000 | Loss: 0.00002715
Iteration 169/1000 | Loss: 0.00002715
Iteration 170/1000 | Loss: 0.00002715
Iteration 171/1000 | Loss: 0.00002715
Iteration 172/1000 | Loss: 0.00002715
Iteration 173/1000 | Loss: 0.00002715
Iteration 174/1000 | Loss: 0.00002715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.714865331654437e-05, 2.714865331654437e-05, 2.714865331654437e-05, 2.714865331654437e-05, 2.714865331654437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.714865331654437e-05

Optimization complete. Final v2v error: 4.243794918060303 mm

Highest mean error: 4.628024578094482 mm for frame 113

Lowest mean error: 3.761636257171631 mm for frame 39

Saving results

Total time: 39.153141498565674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402396
Iteration 2/25 | Loss: 0.00110286
Iteration 3/25 | Loss: 0.00094083
Iteration 4/25 | Loss: 0.00089904
Iteration 5/25 | Loss: 0.00088613
Iteration 6/25 | Loss: 0.00088234
Iteration 7/25 | Loss: 0.00088089
Iteration 8/25 | Loss: 0.00088035
Iteration 9/25 | Loss: 0.00088035
Iteration 10/25 | Loss: 0.00088035
Iteration 11/25 | Loss: 0.00088035
Iteration 12/25 | Loss: 0.00088035
Iteration 13/25 | Loss: 0.00088035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008803526870906353, 0.0008803526870906353, 0.0008803526870906353, 0.0008803526870906353, 0.0008803526870906353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008803526870906353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62378359
Iteration 2/25 | Loss: 0.00244711
Iteration 3/25 | Loss: 0.00244711
Iteration 4/25 | Loss: 0.00244711
Iteration 5/25 | Loss: 0.00244711
Iteration 6/25 | Loss: 0.00244711
Iteration 7/25 | Loss: 0.00244711
Iteration 8/25 | Loss: 0.00244711
Iteration 9/25 | Loss: 0.00244711
Iteration 10/25 | Loss: 0.00244711
Iteration 11/25 | Loss: 0.00244711
Iteration 12/25 | Loss: 0.00244711
Iteration 13/25 | Loss: 0.00244711
Iteration 14/25 | Loss: 0.00244711
Iteration 15/25 | Loss: 0.00244711
Iteration 16/25 | Loss: 0.00244711
Iteration 17/25 | Loss: 0.00244711
Iteration 18/25 | Loss: 0.00244711
Iteration 19/25 | Loss: 0.00244711
Iteration 20/25 | Loss: 0.00244711
Iteration 21/25 | Loss: 0.00244711
Iteration 22/25 | Loss: 0.00244711
Iteration 23/25 | Loss: 0.00244711
Iteration 24/25 | Loss: 0.00244711
Iteration 25/25 | Loss: 0.00244711

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244711
Iteration 2/1000 | Loss: 0.00006475
Iteration 3/1000 | Loss: 0.00004043
Iteration 4/1000 | Loss: 0.00003342
Iteration 5/1000 | Loss: 0.00003006
Iteration 6/1000 | Loss: 0.00002820
Iteration 7/1000 | Loss: 0.00002732
Iteration 8/1000 | Loss: 0.00002669
Iteration 9/1000 | Loss: 0.00002614
Iteration 10/1000 | Loss: 0.00002578
Iteration 11/1000 | Loss: 0.00002540
Iteration 12/1000 | Loss: 0.00002520
Iteration 13/1000 | Loss: 0.00002502
Iteration 14/1000 | Loss: 0.00002484
Iteration 15/1000 | Loss: 0.00002482
Iteration 16/1000 | Loss: 0.00002479
Iteration 17/1000 | Loss: 0.00002474
Iteration 18/1000 | Loss: 0.00002473
Iteration 19/1000 | Loss: 0.00002470
Iteration 20/1000 | Loss: 0.00002469
Iteration 21/1000 | Loss: 0.00002469
Iteration 22/1000 | Loss: 0.00002468
Iteration 23/1000 | Loss: 0.00002468
Iteration 24/1000 | Loss: 0.00002464
Iteration 25/1000 | Loss: 0.00002464
Iteration 26/1000 | Loss: 0.00002463
Iteration 27/1000 | Loss: 0.00002463
Iteration 28/1000 | Loss: 0.00002462
Iteration 29/1000 | Loss: 0.00002462
Iteration 30/1000 | Loss: 0.00002456
Iteration 31/1000 | Loss: 0.00002456
Iteration 32/1000 | Loss: 0.00002455
Iteration 33/1000 | Loss: 0.00002454
Iteration 34/1000 | Loss: 0.00002454
Iteration 35/1000 | Loss: 0.00002454
Iteration 36/1000 | Loss: 0.00002453
Iteration 37/1000 | Loss: 0.00002453
Iteration 38/1000 | Loss: 0.00002453
Iteration 39/1000 | Loss: 0.00002452
Iteration 40/1000 | Loss: 0.00002452
Iteration 41/1000 | Loss: 0.00002452
Iteration 42/1000 | Loss: 0.00002451
Iteration 43/1000 | Loss: 0.00002451
Iteration 44/1000 | Loss: 0.00002451
Iteration 45/1000 | Loss: 0.00002450
Iteration 46/1000 | Loss: 0.00002450
Iteration 47/1000 | Loss: 0.00002450
Iteration 48/1000 | Loss: 0.00002450
Iteration 49/1000 | Loss: 0.00002450
Iteration 50/1000 | Loss: 0.00002450
Iteration 51/1000 | Loss: 0.00002450
Iteration 52/1000 | Loss: 0.00002450
Iteration 53/1000 | Loss: 0.00002450
Iteration 54/1000 | Loss: 0.00002450
Iteration 55/1000 | Loss: 0.00002450
Iteration 56/1000 | Loss: 0.00002450
Iteration 57/1000 | Loss: 0.00002449
Iteration 58/1000 | Loss: 0.00002449
Iteration 59/1000 | Loss: 0.00002449
Iteration 60/1000 | Loss: 0.00002449
Iteration 61/1000 | Loss: 0.00002449
Iteration 62/1000 | Loss: 0.00002449
Iteration 63/1000 | Loss: 0.00002448
Iteration 64/1000 | Loss: 0.00002448
Iteration 65/1000 | Loss: 0.00002448
Iteration 66/1000 | Loss: 0.00002447
Iteration 67/1000 | Loss: 0.00002447
Iteration 68/1000 | Loss: 0.00002447
Iteration 69/1000 | Loss: 0.00002447
Iteration 70/1000 | Loss: 0.00002447
Iteration 71/1000 | Loss: 0.00002446
Iteration 72/1000 | Loss: 0.00002446
Iteration 73/1000 | Loss: 0.00002446
Iteration 74/1000 | Loss: 0.00002446
Iteration 75/1000 | Loss: 0.00002446
Iteration 76/1000 | Loss: 0.00002445
Iteration 77/1000 | Loss: 0.00002445
Iteration 78/1000 | Loss: 0.00002445
Iteration 79/1000 | Loss: 0.00002445
Iteration 80/1000 | Loss: 0.00002444
Iteration 81/1000 | Loss: 0.00002444
Iteration 82/1000 | Loss: 0.00002444
Iteration 83/1000 | Loss: 0.00002444
Iteration 84/1000 | Loss: 0.00002444
Iteration 85/1000 | Loss: 0.00002444
Iteration 86/1000 | Loss: 0.00002443
Iteration 87/1000 | Loss: 0.00002443
Iteration 88/1000 | Loss: 0.00002443
Iteration 89/1000 | Loss: 0.00002443
Iteration 90/1000 | Loss: 0.00002442
Iteration 91/1000 | Loss: 0.00002442
Iteration 92/1000 | Loss: 0.00002442
Iteration 93/1000 | Loss: 0.00002441
Iteration 94/1000 | Loss: 0.00002441
Iteration 95/1000 | Loss: 0.00002441
Iteration 96/1000 | Loss: 0.00002440
Iteration 97/1000 | Loss: 0.00002440
Iteration 98/1000 | Loss: 0.00002440
Iteration 99/1000 | Loss: 0.00002440
Iteration 100/1000 | Loss: 0.00002439
Iteration 101/1000 | Loss: 0.00002439
Iteration 102/1000 | Loss: 0.00002439
Iteration 103/1000 | Loss: 0.00002438
Iteration 104/1000 | Loss: 0.00002438
Iteration 105/1000 | Loss: 0.00002438
Iteration 106/1000 | Loss: 0.00002438
Iteration 107/1000 | Loss: 0.00002438
Iteration 108/1000 | Loss: 0.00002438
Iteration 109/1000 | Loss: 0.00002438
Iteration 110/1000 | Loss: 0.00002438
Iteration 111/1000 | Loss: 0.00002438
Iteration 112/1000 | Loss: 0.00002438
Iteration 113/1000 | Loss: 0.00002438
Iteration 114/1000 | Loss: 0.00002438
Iteration 115/1000 | Loss: 0.00002438
Iteration 116/1000 | Loss: 0.00002438
Iteration 117/1000 | Loss: 0.00002438
Iteration 118/1000 | Loss: 0.00002438
Iteration 119/1000 | Loss: 0.00002438
Iteration 120/1000 | Loss: 0.00002438
Iteration 121/1000 | Loss: 0.00002438
Iteration 122/1000 | Loss: 0.00002438
Iteration 123/1000 | Loss: 0.00002438
Iteration 124/1000 | Loss: 0.00002438
Iteration 125/1000 | Loss: 0.00002438
Iteration 126/1000 | Loss: 0.00002438
Iteration 127/1000 | Loss: 0.00002438
Iteration 128/1000 | Loss: 0.00002438
Iteration 129/1000 | Loss: 0.00002438
Iteration 130/1000 | Loss: 0.00002438
Iteration 131/1000 | Loss: 0.00002438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.437779403408058e-05, 2.437779403408058e-05, 2.437779403408058e-05, 2.437779403408058e-05, 2.437779403408058e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.437779403408058e-05

Optimization complete. Final v2v error: 4.050404071807861 mm

Highest mean error: 4.618111610412598 mm for frame 92

Lowest mean error: 3.297534227371216 mm for frame 80

Saving results

Total time: 42.899428606033325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808888
Iteration 2/25 | Loss: 0.00138609
Iteration 3/25 | Loss: 0.00096223
Iteration 4/25 | Loss: 0.00088656
Iteration 5/25 | Loss: 0.00087027
Iteration 6/25 | Loss: 0.00088077
Iteration 7/25 | Loss: 0.00086224
Iteration 8/25 | Loss: 0.00085246
Iteration 9/25 | Loss: 0.00084571
Iteration 10/25 | Loss: 0.00083807
Iteration 11/25 | Loss: 0.00083415
Iteration 12/25 | Loss: 0.00083337
Iteration 13/25 | Loss: 0.00083288
Iteration 14/25 | Loss: 0.00083258
Iteration 15/25 | Loss: 0.00083243
Iteration 16/25 | Loss: 0.00083238
Iteration 17/25 | Loss: 0.00083238
Iteration 18/25 | Loss: 0.00083238
Iteration 19/25 | Loss: 0.00083238
Iteration 20/25 | Loss: 0.00083238
Iteration 21/25 | Loss: 0.00083237
Iteration 22/25 | Loss: 0.00083231
Iteration 23/25 | Loss: 0.00083230
Iteration 24/25 | Loss: 0.00083230
Iteration 25/25 | Loss: 0.00083233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.24309206
Iteration 2/25 | Loss: 0.00183399
Iteration 3/25 | Loss: 0.00183399
Iteration 4/25 | Loss: 0.00183398
Iteration 5/25 | Loss: 0.00183398
Iteration 6/25 | Loss: 0.00183398
Iteration 7/25 | Loss: 0.00183398
Iteration 8/25 | Loss: 0.00183398
Iteration 9/25 | Loss: 0.00183398
Iteration 10/25 | Loss: 0.00183398
Iteration 11/25 | Loss: 0.00183398
Iteration 12/25 | Loss: 0.00183398
Iteration 13/25 | Loss: 0.00183398
Iteration 14/25 | Loss: 0.00183398
Iteration 15/25 | Loss: 0.00183398
Iteration 16/25 | Loss: 0.00183398
Iteration 17/25 | Loss: 0.00183398
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0018339818343520164, 0.0018339818343520164, 0.0018339818343520164, 0.0018339818343520164, 0.0018339818343520164]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018339818343520164

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183398
Iteration 2/1000 | Loss: 0.00003127
Iteration 3/1000 | Loss: 0.00002145
Iteration 4/1000 | Loss: 0.00001867
Iteration 5/1000 | Loss: 0.00001771
Iteration 6/1000 | Loss: 0.00001720
Iteration 7/1000 | Loss: 0.00001682
Iteration 8/1000 | Loss: 0.00001663
Iteration 9/1000 | Loss: 0.00001645
Iteration 10/1000 | Loss: 0.00001635
Iteration 11/1000 | Loss: 0.00001626
Iteration 12/1000 | Loss: 0.00001625
Iteration 13/1000 | Loss: 0.00001624
Iteration 14/1000 | Loss: 0.00001620
Iteration 15/1000 | Loss: 0.00001619
Iteration 16/1000 | Loss: 0.00001619
Iteration 17/1000 | Loss: 0.00001618
Iteration 18/1000 | Loss: 0.00001618
Iteration 19/1000 | Loss: 0.00001615
Iteration 20/1000 | Loss: 0.00001614
Iteration 21/1000 | Loss: 0.00001614
Iteration 22/1000 | Loss: 0.00001614
Iteration 23/1000 | Loss: 0.00001613
Iteration 24/1000 | Loss: 0.00001613
Iteration 25/1000 | Loss: 0.00001612
Iteration 26/1000 | Loss: 0.00001612
Iteration 27/1000 | Loss: 0.00001611
Iteration 28/1000 | Loss: 0.00001611
Iteration 29/1000 | Loss: 0.00001610
Iteration 30/1000 | Loss: 0.00001610
Iteration 31/1000 | Loss: 0.00001610
Iteration 32/1000 | Loss: 0.00001609
Iteration 33/1000 | Loss: 0.00001609
Iteration 34/1000 | Loss: 0.00001609
Iteration 35/1000 | Loss: 0.00001608
Iteration 36/1000 | Loss: 0.00001608
Iteration 37/1000 | Loss: 0.00001608
Iteration 38/1000 | Loss: 0.00001607
Iteration 39/1000 | Loss: 0.00001607
Iteration 40/1000 | Loss: 0.00001606
Iteration 41/1000 | Loss: 0.00001606
Iteration 42/1000 | Loss: 0.00001606
Iteration 43/1000 | Loss: 0.00001606
Iteration 44/1000 | Loss: 0.00001605
Iteration 45/1000 | Loss: 0.00001605
Iteration 46/1000 | Loss: 0.00001605
Iteration 47/1000 | Loss: 0.00001604
Iteration 48/1000 | Loss: 0.00001604
Iteration 49/1000 | Loss: 0.00001604
Iteration 50/1000 | Loss: 0.00001603
Iteration 51/1000 | Loss: 0.00001603
Iteration 52/1000 | Loss: 0.00001603
Iteration 53/1000 | Loss: 0.00001602
Iteration 54/1000 | Loss: 0.00001602
Iteration 55/1000 | Loss: 0.00001602
Iteration 56/1000 | Loss: 0.00001602
Iteration 57/1000 | Loss: 0.00001601
Iteration 58/1000 | Loss: 0.00001601
Iteration 59/1000 | Loss: 0.00001600
Iteration 60/1000 | Loss: 0.00001600
Iteration 61/1000 | Loss: 0.00001600
Iteration 62/1000 | Loss: 0.00001600
Iteration 63/1000 | Loss: 0.00001600
Iteration 64/1000 | Loss: 0.00001600
Iteration 65/1000 | Loss: 0.00001600
Iteration 66/1000 | Loss: 0.00001600
Iteration 67/1000 | Loss: 0.00001600
Iteration 68/1000 | Loss: 0.00001600
Iteration 69/1000 | Loss: 0.00001599
Iteration 70/1000 | Loss: 0.00001599
Iteration 71/1000 | Loss: 0.00001599
Iteration 72/1000 | Loss: 0.00001599
Iteration 73/1000 | Loss: 0.00001598
Iteration 74/1000 | Loss: 0.00001598
Iteration 75/1000 | Loss: 0.00001598
Iteration 76/1000 | Loss: 0.00001598
Iteration 77/1000 | Loss: 0.00001598
Iteration 78/1000 | Loss: 0.00001597
Iteration 79/1000 | Loss: 0.00001597
Iteration 80/1000 | Loss: 0.00001597
Iteration 81/1000 | Loss: 0.00001597
Iteration 82/1000 | Loss: 0.00001596
Iteration 83/1000 | Loss: 0.00001596
Iteration 84/1000 | Loss: 0.00001596
Iteration 85/1000 | Loss: 0.00001595
Iteration 86/1000 | Loss: 0.00001595
Iteration 87/1000 | Loss: 0.00001595
Iteration 88/1000 | Loss: 0.00001595
Iteration 89/1000 | Loss: 0.00001594
Iteration 90/1000 | Loss: 0.00001594
Iteration 91/1000 | Loss: 0.00001594
Iteration 92/1000 | Loss: 0.00001594
Iteration 93/1000 | Loss: 0.00001593
Iteration 94/1000 | Loss: 0.00001593
Iteration 95/1000 | Loss: 0.00001593
Iteration 96/1000 | Loss: 0.00001593
Iteration 97/1000 | Loss: 0.00001593
Iteration 98/1000 | Loss: 0.00001593
Iteration 99/1000 | Loss: 0.00001593
Iteration 100/1000 | Loss: 0.00001593
Iteration 101/1000 | Loss: 0.00001592
Iteration 102/1000 | Loss: 0.00001592
Iteration 103/1000 | Loss: 0.00001592
Iteration 104/1000 | Loss: 0.00001592
Iteration 105/1000 | Loss: 0.00001592
Iteration 106/1000 | Loss: 0.00001592
Iteration 107/1000 | Loss: 0.00001592
Iteration 108/1000 | Loss: 0.00001591
Iteration 109/1000 | Loss: 0.00001591
Iteration 110/1000 | Loss: 0.00001591
Iteration 111/1000 | Loss: 0.00001591
Iteration 112/1000 | Loss: 0.00001590
Iteration 113/1000 | Loss: 0.00001590
Iteration 114/1000 | Loss: 0.00001590
Iteration 115/1000 | Loss: 0.00001590
Iteration 116/1000 | Loss: 0.00001590
Iteration 117/1000 | Loss: 0.00001590
Iteration 118/1000 | Loss: 0.00001590
Iteration 119/1000 | Loss: 0.00001590
Iteration 120/1000 | Loss: 0.00001590
Iteration 121/1000 | Loss: 0.00001590
Iteration 122/1000 | Loss: 0.00001590
Iteration 123/1000 | Loss: 0.00001590
Iteration 124/1000 | Loss: 0.00001590
Iteration 125/1000 | Loss: 0.00001590
Iteration 126/1000 | Loss: 0.00001590
Iteration 127/1000 | Loss: 0.00001590
Iteration 128/1000 | Loss: 0.00001590
Iteration 129/1000 | Loss: 0.00001590
Iteration 130/1000 | Loss: 0.00001590
Iteration 131/1000 | Loss: 0.00001590
Iteration 132/1000 | Loss: 0.00001590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.5900921425782144e-05, 1.5900921425782144e-05, 1.5900921425782144e-05, 1.5900921425782144e-05, 1.5900921425782144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5900921425782144e-05

Optimization complete. Final v2v error: 3.320237636566162 mm

Highest mean error: 9.36249828338623 mm for frame 100

Lowest mean error: 2.9197816848754883 mm for frame 201

Saving results

Total time: 61.622185468673706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842595
Iteration 2/25 | Loss: 0.00156419
Iteration 3/25 | Loss: 0.00099848
Iteration 4/25 | Loss: 0.00091549
Iteration 5/25 | Loss: 0.00090491
Iteration 6/25 | Loss: 0.00090409
Iteration 7/25 | Loss: 0.00090409
Iteration 8/25 | Loss: 0.00090409
Iteration 9/25 | Loss: 0.00090409
Iteration 10/25 | Loss: 0.00090409
Iteration 11/25 | Loss: 0.00090409
Iteration 12/25 | Loss: 0.00090409
Iteration 13/25 | Loss: 0.00090409
Iteration 14/25 | Loss: 0.00090409
Iteration 15/25 | Loss: 0.00090409
Iteration 16/25 | Loss: 0.00090409
Iteration 17/25 | Loss: 0.00090409
Iteration 18/25 | Loss: 0.00090409
Iteration 19/25 | Loss: 0.00090409
Iteration 20/25 | Loss: 0.00090409
Iteration 21/25 | Loss: 0.00090409
Iteration 22/25 | Loss: 0.00090409
Iteration 23/25 | Loss: 0.00090409
Iteration 24/25 | Loss: 0.00090409
Iteration 25/25 | Loss: 0.00090409

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59636128
Iteration 2/25 | Loss: 0.00202814
Iteration 3/25 | Loss: 0.00202812
Iteration 4/25 | Loss: 0.00202812
Iteration 5/25 | Loss: 0.00202812
Iteration 6/25 | Loss: 0.00202812
Iteration 7/25 | Loss: 0.00202812
Iteration 8/25 | Loss: 0.00202812
Iteration 9/25 | Loss: 0.00202812
Iteration 10/25 | Loss: 0.00202812
Iteration 11/25 | Loss: 0.00202812
Iteration 12/25 | Loss: 0.00202812
Iteration 13/25 | Loss: 0.00202812
Iteration 14/25 | Loss: 0.00202812
Iteration 15/25 | Loss: 0.00202812
Iteration 16/25 | Loss: 0.00202812
Iteration 17/25 | Loss: 0.00202812
Iteration 18/25 | Loss: 0.00202812
Iteration 19/25 | Loss: 0.00202812
Iteration 20/25 | Loss: 0.00202812
Iteration 21/25 | Loss: 0.00202812
Iteration 22/25 | Loss: 0.00202812
Iteration 23/25 | Loss: 0.00202812
Iteration 24/25 | Loss: 0.00202812
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002028119284659624, 0.002028119284659624, 0.002028119284659624, 0.002028119284659624, 0.002028119284659624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002028119284659624

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202812
Iteration 2/1000 | Loss: 0.00003591
Iteration 3/1000 | Loss: 0.00002417
Iteration 4/1000 | Loss: 0.00002223
Iteration 5/1000 | Loss: 0.00002109
Iteration 6/1000 | Loss: 0.00002031
Iteration 7/1000 | Loss: 0.00001984
Iteration 8/1000 | Loss: 0.00001952
Iteration 9/1000 | Loss: 0.00001937
Iteration 10/1000 | Loss: 0.00001921
Iteration 11/1000 | Loss: 0.00001916
Iteration 12/1000 | Loss: 0.00001916
Iteration 13/1000 | Loss: 0.00001910
Iteration 14/1000 | Loss: 0.00001901
Iteration 15/1000 | Loss: 0.00001901
Iteration 16/1000 | Loss: 0.00001900
Iteration 17/1000 | Loss: 0.00001899
Iteration 18/1000 | Loss: 0.00001899
Iteration 19/1000 | Loss: 0.00001899
Iteration 20/1000 | Loss: 0.00001898
Iteration 21/1000 | Loss: 0.00001898
Iteration 22/1000 | Loss: 0.00001897
Iteration 23/1000 | Loss: 0.00001897
Iteration 24/1000 | Loss: 0.00001897
Iteration 25/1000 | Loss: 0.00001896
Iteration 26/1000 | Loss: 0.00001896
Iteration 27/1000 | Loss: 0.00001896
Iteration 28/1000 | Loss: 0.00001896
Iteration 29/1000 | Loss: 0.00001896
Iteration 30/1000 | Loss: 0.00001896
Iteration 31/1000 | Loss: 0.00001896
Iteration 32/1000 | Loss: 0.00001896
Iteration 33/1000 | Loss: 0.00001896
Iteration 34/1000 | Loss: 0.00001896
Iteration 35/1000 | Loss: 0.00001896
Iteration 36/1000 | Loss: 0.00001895
Iteration 37/1000 | Loss: 0.00001895
Iteration 38/1000 | Loss: 0.00001895
Iteration 39/1000 | Loss: 0.00001895
Iteration 40/1000 | Loss: 0.00001894
Iteration 41/1000 | Loss: 0.00001894
Iteration 42/1000 | Loss: 0.00001894
Iteration 43/1000 | Loss: 0.00001894
Iteration 44/1000 | Loss: 0.00001894
Iteration 45/1000 | Loss: 0.00001893
Iteration 46/1000 | Loss: 0.00001893
Iteration 47/1000 | Loss: 0.00001893
Iteration 48/1000 | Loss: 0.00001893
Iteration 49/1000 | Loss: 0.00001892
Iteration 50/1000 | Loss: 0.00001891
Iteration 51/1000 | Loss: 0.00001891
Iteration 52/1000 | Loss: 0.00001891
Iteration 53/1000 | Loss: 0.00001890
Iteration 54/1000 | Loss: 0.00001890
Iteration 55/1000 | Loss: 0.00001889
Iteration 56/1000 | Loss: 0.00001889
Iteration 57/1000 | Loss: 0.00001889
Iteration 58/1000 | Loss: 0.00001889
Iteration 59/1000 | Loss: 0.00001888
Iteration 60/1000 | Loss: 0.00001888
Iteration 61/1000 | Loss: 0.00001888
Iteration 62/1000 | Loss: 0.00001888
Iteration 63/1000 | Loss: 0.00001888
Iteration 64/1000 | Loss: 0.00001887
Iteration 65/1000 | Loss: 0.00001887
Iteration 66/1000 | Loss: 0.00001887
Iteration 67/1000 | Loss: 0.00001887
Iteration 68/1000 | Loss: 0.00001887
Iteration 69/1000 | Loss: 0.00001886
Iteration 70/1000 | Loss: 0.00001886
Iteration 71/1000 | Loss: 0.00001886
Iteration 72/1000 | Loss: 0.00001886
Iteration 73/1000 | Loss: 0.00001886
Iteration 74/1000 | Loss: 0.00001885
Iteration 75/1000 | Loss: 0.00001885
Iteration 76/1000 | Loss: 0.00001885
Iteration 77/1000 | Loss: 0.00001885
Iteration 78/1000 | Loss: 0.00001885
Iteration 79/1000 | Loss: 0.00001885
Iteration 80/1000 | Loss: 0.00001885
Iteration 81/1000 | Loss: 0.00001885
Iteration 82/1000 | Loss: 0.00001885
Iteration 83/1000 | Loss: 0.00001885
Iteration 84/1000 | Loss: 0.00001884
Iteration 85/1000 | Loss: 0.00001884
Iteration 86/1000 | Loss: 0.00001883
Iteration 87/1000 | Loss: 0.00001883
Iteration 88/1000 | Loss: 0.00001883
Iteration 89/1000 | Loss: 0.00001883
Iteration 90/1000 | Loss: 0.00001883
Iteration 91/1000 | Loss: 0.00001883
Iteration 92/1000 | Loss: 0.00001883
Iteration 93/1000 | Loss: 0.00001883
Iteration 94/1000 | Loss: 0.00001882
Iteration 95/1000 | Loss: 0.00001882
Iteration 96/1000 | Loss: 0.00001882
Iteration 97/1000 | Loss: 0.00001882
Iteration 98/1000 | Loss: 0.00001882
Iteration 99/1000 | Loss: 0.00001882
Iteration 100/1000 | Loss: 0.00001882
Iteration 101/1000 | Loss: 0.00001882
Iteration 102/1000 | Loss: 0.00001882
Iteration 103/1000 | Loss: 0.00001882
Iteration 104/1000 | Loss: 0.00001882
Iteration 105/1000 | Loss: 0.00001881
Iteration 106/1000 | Loss: 0.00001881
Iteration 107/1000 | Loss: 0.00001881
Iteration 108/1000 | Loss: 0.00001881
Iteration 109/1000 | Loss: 0.00001881
Iteration 110/1000 | Loss: 0.00001881
Iteration 111/1000 | Loss: 0.00001881
Iteration 112/1000 | Loss: 0.00001881
Iteration 113/1000 | Loss: 0.00001881
Iteration 114/1000 | Loss: 0.00001881
Iteration 115/1000 | Loss: 0.00001881
Iteration 116/1000 | Loss: 0.00001881
Iteration 117/1000 | Loss: 0.00001881
Iteration 118/1000 | Loss: 0.00001881
Iteration 119/1000 | Loss: 0.00001881
Iteration 120/1000 | Loss: 0.00001881
Iteration 121/1000 | Loss: 0.00001881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.8811600966728292e-05, 1.8811600966728292e-05, 1.8811600966728292e-05, 1.8811600966728292e-05, 1.8811600966728292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8811600966728292e-05

Optimization complete. Final v2v error: 3.685744524002075 mm

Highest mean error: 4.37230920791626 mm for frame 181

Lowest mean error: 3.4766104221343994 mm for frame 104

Saving results

Total time: 37.04201650619507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084195
Iteration 2/25 | Loss: 0.00181229
Iteration 3/25 | Loss: 0.00139917
Iteration 4/25 | Loss: 0.00136725
Iteration 5/25 | Loss: 0.00134712
Iteration 6/25 | Loss: 0.00130237
Iteration 7/25 | Loss: 0.00115571
Iteration 8/25 | Loss: 0.00113449
Iteration 9/25 | Loss: 0.00113475
Iteration 10/25 | Loss: 0.00114374
Iteration 11/25 | Loss: 0.00110875
Iteration 12/25 | Loss: 0.00113154
Iteration 13/25 | Loss: 0.00112796
Iteration 14/25 | Loss: 0.00112269
Iteration 15/25 | Loss: 0.00110842
Iteration 16/25 | Loss: 0.00104497
Iteration 17/25 | Loss: 0.00103745
Iteration 18/25 | Loss: 0.00103594
Iteration 19/25 | Loss: 0.00103509
Iteration 20/25 | Loss: 0.00103474
Iteration 21/25 | Loss: 0.00103467
Iteration 22/25 | Loss: 0.00103467
Iteration 23/25 | Loss: 0.00103467
Iteration 24/25 | Loss: 0.00103466
Iteration 25/25 | Loss: 0.00103466

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.09569502
Iteration 2/25 | Loss: 0.00352228
Iteration 3/25 | Loss: 0.00352225
Iteration 4/25 | Loss: 0.00352225
Iteration 5/25 | Loss: 0.00352225
Iteration 6/25 | Loss: 0.00352225
Iteration 7/25 | Loss: 0.00352224
Iteration 8/25 | Loss: 0.00352224
Iteration 9/25 | Loss: 0.00352224
Iteration 10/25 | Loss: 0.00352224
Iteration 11/25 | Loss: 0.00352224
Iteration 12/25 | Loss: 0.00352224
Iteration 13/25 | Loss: 0.00352224
Iteration 14/25 | Loss: 0.00352224
Iteration 15/25 | Loss: 0.00352224
Iteration 16/25 | Loss: 0.00352224
Iteration 17/25 | Loss: 0.00352224
Iteration 18/25 | Loss: 0.00352224
Iteration 19/25 | Loss: 0.00352224
Iteration 20/25 | Loss: 0.00352224
Iteration 21/25 | Loss: 0.00352224
Iteration 22/25 | Loss: 0.00352224
Iteration 23/25 | Loss: 0.00352224
Iteration 24/25 | Loss: 0.00352224
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0035222433507442474, 0.0035222433507442474, 0.0035222433507442474, 0.0035222433507442474, 0.0035222433507442474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035222433507442474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00352224
Iteration 2/1000 | Loss: 0.00022163
Iteration 3/1000 | Loss: 0.00016399
Iteration 4/1000 | Loss: 0.00300005
Iteration 5/1000 | Loss: 0.00039701
Iteration 6/1000 | Loss: 0.00017953
Iteration 7/1000 | Loss: 0.00014482
Iteration 8/1000 | Loss: 0.00150633
Iteration 9/1000 | Loss: 0.00208355
Iteration 10/1000 | Loss: 0.00145394
Iteration 11/1000 | Loss: 0.00017106
Iteration 12/1000 | Loss: 0.00111804
Iteration 13/1000 | Loss: 0.00243197
Iteration 14/1000 | Loss: 0.00233848
Iteration 15/1000 | Loss: 0.00149396
Iteration 16/1000 | Loss: 0.00190261
Iteration 17/1000 | Loss: 0.00144430
Iteration 18/1000 | Loss: 0.00156153
Iteration 19/1000 | Loss: 0.00116396
Iteration 20/1000 | Loss: 0.00071816
Iteration 21/1000 | Loss: 0.00076170
Iteration 22/1000 | Loss: 0.00156610
Iteration 23/1000 | Loss: 0.00074146
Iteration 24/1000 | Loss: 0.00103741
Iteration 25/1000 | Loss: 0.00029749
Iteration 26/1000 | Loss: 0.00049683
Iteration 27/1000 | Loss: 0.00028203
Iteration 28/1000 | Loss: 0.00062525
Iteration 29/1000 | Loss: 0.00087737
Iteration 30/1000 | Loss: 0.00064871
Iteration 31/1000 | Loss: 0.00073727
Iteration 32/1000 | Loss: 0.00129780
Iteration 33/1000 | Loss: 0.00173837
Iteration 34/1000 | Loss: 0.00176378
Iteration 35/1000 | Loss: 0.00318214
Iteration 36/1000 | Loss: 0.00074834
Iteration 37/1000 | Loss: 0.00191321
Iteration 38/1000 | Loss: 0.00087356
Iteration 39/1000 | Loss: 0.00080473
Iteration 40/1000 | Loss: 0.00031754
Iteration 41/1000 | Loss: 0.00180820
Iteration 42/1000 | Loss: 0.00087179
Iteration 43/1000 | Loss: 0.00080135
Iteration 44/1000 | Loss: 0.00047644
Iteration 45/1000 | Loss: 0.00053487
Iteration 46/1000 | Loss: 0.00080495
Iteration 47/1000 | Loss: 0.00114544
Iteration 48/1000 | Loss: 0.00109300
Iteration 49/1000 | Loss: 0.00076251
Iteration 50/1000 | Loss: 0.00138889
Iteration 51/1000 | Loss: 0.00130371
Iteration 52/1000 | Loss: 0.00050840
Iteration 53/1000 | Loss: 0.00061970
Iteration 54/1000 | Loss: 0.00023074
Iteration 55/1000 | Loss: 0.00043230
Iteration 56/1000 | Loss: 0.00022387
Iteration 57/1000 | Loss: 0.00048201
Iteration 58/1000 | Loss: 0.00052834
Iteration 59/1000 | Loss: 0.00043437
Iteration 60/1000 | Loss: 0.00006775
Iteration 61/1000 | Loss: 0.00073013
Iteration 62/1000 | Loss: 0.00124963
Iteration 63/1000 | Loss: 0.00032996
Iteration 64/1000 | Loss: 0.00057413
Iteration 65/1000 | Loss: 0.00122783
Iteration 66/1000 | Loss: 0.00172629
Iteration 67/1000 | Loss: 0.00077621
Iteration 68/1000 | Loss: 0.00014862
Iteration 69/1000 | Loss: 0.00059497
Iteration 70/1000 | Loss: 0.00047203
Iteration 71/1000 | Loss: 0.00057552
Iteration 72/1000 | Loss: 0.00101995
Iteration 73/1000 | Loss: 0.00052218
Iteration 74/1000 | Loss: 0.00016107
Iteration 75/1000 | Loss: 0.00065065
Iteration 76/1000 | Loss: 0.00074641
Iteration 77/1000 | Loss: 0.00024606
Iteration 78/1000 | Loss: 0.00075024
Iteration 79/1000 | Loss: 0.00005334
Iteration 80/1000 | Loss: 0.00086202
Iteration 81/1000 | Loss: 0.00136230
Iteration 82/1000 | Loss: 0.00095019
Iteration 83/1000 | Loss: 0.00039888
Iteration 84/1000 | Loss: 0.00127020
Iteration 85/1000 | Loss: 0.00077155
Iteration 86/1000 | Loss: 0.00041280
Iteration 87/1000 | Loss: 0.00018120
Iteration 88/1000 | Loss: 0.00005964
Iteration 89/1000 | Loss: 0.00081849
Iteration 90/1000 | Loss: 0.00040801
Iteration 91/1000 | Loss: 0.00070521
Iteration 92/1000 | Loss: 0.00035363
Iteration 93/1000 | Loss: 0.00101144
Iteration 94/1000 | Loss: 0.00089237
Iteration 95/1000 | Loss: 0.00090892
Iteration 96/1000 | Loss: 0.00113028
Iteration 97/1000 | Loss: 0.00104521
Iteration 98/1000 | Loss: 0.00112440
Iteration 99/1000 | Loss: 0.00145143
Iteration 100/1000 | Loss: 0.00052025
Iteration 101/1000 | Loss: 0.00070303
Iteration 102/1000 | Loss: 0.00149894
Iteration 103/1000 | Loss: 0.00063708
Iteration 104/1000 | Loss: 0.00082613
Iteration 105/1000 | Loss: 0.00066352
Iteration 106/1000 | Loss: 0.00025855
Iteration 107/1000 | Loss: 0.00063408
Iteration 108/1000 | Loss: 0.00046522
Iteration 109/1000 | Loss: 0.00005486
Iteration 110/1000 | Loss: 0.00057876
Iteration 111/1000 | Loss: 0.00007101
Iteration 112/1000 | Loss: 0.00039925
Iteration 113/1000 | Loss: 0.00013253
Iteration 114/1000 | Loss: 0.00027302
Iteration 115/1000 | Loss: 0.00107788
Iteration 116/1000 | Loss: 0.00053217
Iteration 117/1000 | Loss: 0.00004662
Iteration 118/1000 | Loss: 0.00113822
Iteration 119/1000 | Loss: 0.00013068
Iteration 120/1000 | Loss: 0.00004438
Iteration 121/1000 | Loss: 0.00045815
Iteration 122/1000 | Loss: 0.00100340
Iteration 123/1000 | Loss: 0.00091175
Iteration 124/1000 | Loss: 0.00040979
Iteration 125/1000 | Loss: 0.00005247
Iteration 126/1000 | Loss: 0.00039089
Iteration 127/1000 | Loss: 0.00006287
Iteration 128/1000 | Loss: 0.00004520
Iteration 129/1000 | Loss: 0.00003798
Iteration 130/1000 | Loss: 0.00003450
Iteration 131/1000 | Loss: 0.00003236
Iteration 132/1000 | Loss: 0.00003073
Iteration 133/1000 | Loss: 0.00002950
Iteration 134/1000 | Loss: 0.00002808
Iteration 135/1000 | Loss: 0.00002679
Iteration 136/1000 | Loss: 0.00002574
Iteration 137/1000 | Loss: 0.00002501
Iteration 138/1000 | Loss: 0.00002467
Iteration 139/1000 | Loss: 0.00002436
Iteration 140/1000 | Loss: 0.00002417
Iteration 141/1000 | Loss: 0.00002413
Iteration 142/1000 | Loss: 0.00002409
Iteration 143/1000 | Loss: 0.00002406
Iteration 144/1000 | Loss: 0.00002406
Iteration 145/1000 | Loss: 0.00002400
Iteration 146/1000 | Loss: 0.00002400
Iteration 147/1000 | Loss: 0.00002399
Iteration 148/1000 | Loss: 0.00002393
Iteration 149/1000 | Loss: 0.00002389
Iteration 150/1000 | Loss: 0.00002384
Iteration 151/1000 | Loss: 0.00002379
Iteration 152/1000 | Loss: 0.00002379
Iteration 153/1000 | Loss: 0.00002379
Iteration 154/1000 | Loss: 0.00002378
Iteration 155/1000 | Loss: 0.00002378
Iteration 156/1000 | Loss: 0.00002378
Iteration 157/1000 | Loss: 0.00002378
Iteration 158/1000 | Loss: 0.00002377
Iteration 159/1000 | Loss: 0.00002377
Iteration 160/1000 | Loss: 0.00002377
Iteration 161/1000 | Loss: 0.00002377
Iteration 162/1000 | Loss: 0.00002377
Iteration 163/1000 | Loss: 0.00002377
Iteration 164/1000 | Loss: 0.00002377
Iteration 165/1000 | Loss: 0.00002376
Iteration 166/1000 | Loss: 0.00002376
Iteration 167/1000 | Loss: 0.00002376
Iteration 168/1000 | Loss: 0.00002376
Iteration 169/1000 | Loss: 0.00002376
Iteration 170/1000 | Loss: 0.00002376
Iteration 171/1000 | Loss: 0.00002376
Iteration 172/1000 | Loss: 0.00002376
Iteration 173/1000 | Loss: 0.00002376
Iteration 174/1000 | Loss: 0.00002375
Iteration 175/1000 | Loss: 0.00002375
Iteration 176/1000 | Loss: 0.00002375
Iteration 177/1000 | Loss: 0.00002374
Iteration 178/1000 | Loss: 0.00002374
Iteration 179/1000 | Loss: 0.00002374
Iteration 180/1000 | Loss: 0.00002374
Iteration 181/1000 | Loss: 0.00002373
Iteration 182/1000 | Loss: 0.00002373
Iteration 183/1000 | Loss: 0.00002373
Iteration 184/1000 | Loss: 0.00002372
Iteration 185/1000 | Loss: 0.00002372
Iteration 186/1000 | Loss: 0.00002372
Iteration 187/1000 | Loss: 0.00002372
Iteration 188/1000 | Loss: 0.00002371
Iteration 189/1000 | Loss: 0.00002371
Iteration 190/1000 | Loss: 0.00002371
Iteration 191/1000 | Loss: 0.00002371
Iteration 192/1000 | Loss: 0.00002371
Iteration 193/1000 | Loss: 0.00002371
Iteration 194/1000 | Loss: 0.00002371
Iteration 195/1000 | Loss: 0.00002371
Iteration 196/1000 | Loss: 0.00002371
Iteration 197/1000 | Loss: 0.00002371
Iteration 198/1000 | Loss: 0.00002371
Iteration 199/1000 | Loss: 0.00002370
Iteration 200/1000 | Loss: 0.00002370
Iteration 201/1000 | Loss: 0.00002370
Iteration 202/1000 | Loss: 0.00002370
Iteration 203/1000 | Loss: 0.00002370
Iteration 204/1000 | Loss: 0.00002370
Iteration 205/1000 | Loss: 0.00002370
Iteration 206/1000 | Loss: 0.00002370
Iteration 207/1000 | Loss: 0.00002370
Iteration 208/1000 | Loss: 0.00002370
Iteration 209/1000 | Loss: 0.00002370
Iteration 210/1000 | Loss: 0.00002370
Iteration 211/1000 | Loss: 0.00002370
Iteration 212/1000 | Loss: 0.00002370
Iteration 213/1000 | Loss: 0.00002369
Iteration 214/1000 | Loss: 0.00002369
Iteration 215/1000 | Loss: 0.00002369
Iteration 216/1000 | Loss: 0.00002369
Iteration 217/1000 | Loss: 0.00002369
Iteration 218/1000 | Loss: 0.00002369
Iteration 219/1000 | Loss: 0.00002369
Iteration 220/1000 | Loss: 0.00002369
Iteration 221/1000 | Loss: 0.00002369
Iteration 222/1000 | Loss: 0.00002369
Iteration 223/1000 | Loss: 0.00002369
Iteration 224/1000 | Loss: 0.00002369
Iteration 225/1000 | Loss: 0.00002369
Iteration 226/1000 | Loss: 0.00002369
Iteration 227/1000 | Loss: 0.00002369
Iteration 228/1000 | Loss: 0.00002369
Iteration 229/1000 | Loss: 0.00002368
Iteration 230/1000 | Loss: 0.00002368
Iteration 231/1000 | Loss: 0.00002368
Iteration 232/1000 | Loss: 0.00002368
Iteration 233/1000 | Loss: 0.00002368
Iteration 234/1000 | Loss: 0.00002368
Iteration 235/1000 | Loss: 0.00002368
Iteration 236/1000 | Loss: 0.00002368
Iteration 237/1000 | Loss: 0.00002368
Iteration 238/1000 | Loss: 0.00002368
Iteration 239/1000 | Loss: 0.00002368
Iteration 240/1000 | Loss: 0.00002368
Iteration 241/1000 | Loss: 0.00002368
Iteration 242/1000 | Loss: 0.00002368
Iteration 243/1000 | Loss: 0.00002367
Iteration 244/1000 | Loss: 0.00002367
Iteration 245/1000 | Loss: 0.00002367
Iteration 246/1000 | Loss: 0.00002367
Iteration 247/1000 | Loss: 0.00002367
Iteration 248/1000 | Loss: 0.00002367
Iteration 249/1000 | Loss: 0.00002367
Iteration 250/1000 | Loss: 0.00002367
Iteration 251/1000 | Loss: 0.00002367
Iteration 252/1000 | Loss: 0.00002367
Iteration 253/1000 | Loss: 0.00002366
Iteration 254/1000 | Loss: 0.00002366
Iteration 255/1000 | Loss: 0.00002366
Iteration 256/1000 | Loss: 0.00002366
Iteration 257/1000 | Loss: 0.00002366
Iteration 258/1000 | Loss: 0.00002366
Iteration 259/1000 | Loss: 0.00002366
Iteration 260/1000 | Loss: 0.00002366
Iteration 261/1000 | Loss: 0.00002366
Iteration 262/1000 | Loss: 0.00002366
Iteration 263/1000 | Loss: 0.00002366
Iteration 264/1000 | Loss: 0.00002366
Iteration 265/1000 | Loss: 0.00002366
Iteration 266/1000 | Loss: 0.00002366
Iteration 267/1000 | Loss: 0.00002366
Iteration 268/1000 | Loss: 0.00002365
Iteration 269/1000 | Loss: 0.00002365
Iteration 270/1000 | Loss: 0.00002365
Iteration 271/1000 | Loss: 0.00002365
Iteration 272/1000 | Loss: 0.00002365
Iteration 273/1000 | Loss: 0.00002365
Iteration 274/1000 | Loss: 0.00002365
Iteration 275/1000 | Loss: 0.00002365
Iteration 276/1000 | Loss: 0.00002365
Iteration 277/1000 | Loss: 0.00002365
Iteration 278/1000 | Loss: 0.00002365
Iteration 279/1000 | Loss: 0.00002365
Iteration 280/1000 | Loss: 0.00002365
Iteration 281/1000 | Loss: 0.00002365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [2.3654651158722118e-05, 2.3654651158722118e-05, 2.3654651158722118e-05, 2.3654651158722118e-05, 2.3654651158722118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3654651158722118e-05

Optimization complete. Final v2v error: 3.4383609294891357 mm

Highest mean error: 20.159574508666992 mm for frame 60

Lowest mean error: 2.56709361076355 mm for frame 0

Saving results

Total time: 247.2205352783203
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00541101
Iteration 2/25 | Loss: 0.00131520
Iteration 3/25 | Loss: 0.00102321
Iteration 4/25 | Loss: 0.00096486
Iteration 5/25 | Loss: 0.00094738
Iteration 6/25 | Loss: 0.00094344
Iteration 7/25 | Loss: 0.00094185
Iteration 8/25 | Loss: 0.00094185
Iteration 9/25 | Loss: 0.00094185
Iteration 10/25 | Loss: 0.00094185
Iteration 11/25 | Loss: 0.00094185
Iteration 12/25 | Loss: 0.00094185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009418473928235471, 0.0009418473928235471, 0.0009418473928235471, 0.0009418473928235471, 0.0009418473928235471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009418473928235471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86454165
Iteration 2/25 | Loss: 0.00224705
Iteration 3/25 | Loss: 0.00224705
Iteration 4/25 | Loss: 0.00224705
Iteration 5/25 | Loss: 0.00224705
Iteration 6/25 | Loss: 0.00224705
Iteration 7/25 | Loss: 0.00224705
Iteration 8/25 | Loss: 0.00224705
Iteration 9/25 | Loss: 0.00224705
Iteration 10/25 | Loss: 0.00224705
Iteration 11/25 | Loss: 0.00224705
Iteration 12/25 | Loss: 0.00224705
Iteration 13/25 | Loss: 0.00224705
Iteration 14/25 | Loss: 0.00224705
Iteration 15/25 | Loss: 0.00224705
Iteration 16/25 | Loss: 0.00224705
Iteration 17/25 | Loss: 0.00224705
Iteration 18/25 | Loss: 0.00224705
Iteration 19/25 | Loss: 0.00224705
Iteration 20/25 | Loss: 0.00224705
Iteration 21/25 | Loss: 0.00224705
Iteration 22/25 | Loss: 0.00224705
Iteration 23/25 | Loss: 0.00224705
Iteration 24/25 | Loss: 0.00224705
Iteration 25/25 | Loss: 0.00224705

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224705
Iteration 2/1000 | Loss: 0.00004643
Iteration 3/1000 | Loss: 0.00003602
Iteration 4/1000 | Loss: 0.00003262
Iteration 5/1000 | Loss: 0.00003046
Iteration 6/1000 | Loss: 0.00002914
Iteration 7/1000 | Loss: 0.00002849
Iteration 8/1000 | Loss: 0.00002798
Iteration 9/1000 | Loss: 0.00002766
Iteration 10/1000 | Loss: 0.00002741
Iteration 11/1000 | Loss: 0.00002721
Iteration 12/1000 | Loss: 0.00002707
Iteration 13/1000 | Loss: 0.00002701
Iteration 14/1000 | Loss: 0.00002701
Iteration 15/1000 | Loss: 0.00002701
Iteration 16/1000 | Loss: 0.00002701
Iteration 17/1000 | Loss: 0.00002701
Iteration 18/1000 | Loss: 0.00002701
Iteration 19/1000 | Loss: 0.00002698
Iteration 20/1000 | Loss: 0.00002698
Iteration 21/1000 | Loss: 0.00002697
Iteration 22/1000 | Loss: 0.00002696
Iteration 23/1000 | Loss: 0.00002696
Iteration 24/1000 | Loss: 0.00002696
Iteration 25/1000 | Loss: 0.00002696
Iteration 26/1000 | Loss: 0.00002696
Iteration 27/1000 | Loss: 0.00002696
Iteration 28/1000 | Loss: 0.00002695
Iteration 29/1000 | Loss: 0.00002695
Iteration 30/1000 | Loss: 0.00002695
Iteration 31/1000 | Loss: 0.00002695
Iteration 32/1000 | Loss: 0.00002695
Iteration 33/1000 | Loss: 0.00002695
Iteration 34/1000 | Loss: 0.00002695
Iteration 35/1000 | Loss: 0.00002695
Iteration 36/1000 | Loss: 0.00002695
Iteration 37/1000 | Loss: 0.00002695
Iteration 38/1000 | Loss: 0.00002694
Iteration 39/1000 | Loss: 0.00002690
Iteration 40/1000 | Loss: 0.00002689
Iteration 41/1000 | Loss: 0.00002687
Iteration 42/1000 | Loss: 0.00002685
Iteration 43/1000 | Loss: 0.00002685
Iteration 44/1000 | Loss: 0.00002685
Iteration 45/1000 | Loss: 0.00002685
Iteration 46/1000 | Loss: 0.00002685
Iteration 47/1000 | Loss: 0.00002685
Iteration 48/1000 | Loss: 0.00002685
Iteration 49/1000 | Loss: 0.00002685
Iteration 50/1000 | Loss: 0.00002685
Iteration 51/1000 | Loss: 0.00002685
Iteration 52/1000 | Loss: 0.00002684
Iteration 53/1000 | Loss: 0.00002684
Iteration 54/1000 | Loss: 0.00002682
Iteration 55/1000 | Loss: 0.00002682
Iteration 56/1000 | Loss: 0.00002681
Iteration 57/1000 | Loss: 0.00002681
Iteration 58/1000 | Loss: 0.00002680
Iteration 59/1000 | Loss: 0.00002680
Iteration 60/1000 | Loss: 0.00002679
Iteration 61/1000 | Loss: 0.00002676
Iteration 62/1000 | Loss: 0.00002676
Iteration 63/1000 | Loss: 0.00002676
Iteration 64/1000 | Loss: 0.00002676
Iteration 65/1000 | Loss: 0.00002676
Iteration 66/1000 | Loss: 0.00002676
Iteration 67/1000 | Loss: 0.00002676
Iteration 68/1000 | Loss: 0.00002676
Iteration 69/1000 | Loss: 0.00002675
Iteration 70/1000 | Loss: 0.00002675
Iteration 71/1000 | Loss: 0.00002675
Iteration 72/1000 | Loss: 0.00002675
Iteration 73/1000 | Loss: 0.00002674
Iteration 74/1000 | Loss: 0.00002674
Iteration 75/1000 | Loss: 0.00002673
Iteration 76/1000 | Loss: 0.00002671
Iteration 77/1000 | Loss: 0.00002671
Iteration 78/1000 | Loss: 0.00002671
Iteration 79/1000 | Loss: 0.00002671
Iteration 80/1000 | Loss: 0.00002671
Iteration 81/1000 | Loss: 0.00002671
Iteration 82/1000 | Loss: 0.00002671
Iteration 83/1000 | Loss: 0.00002671
Iteration 84/1000 | Loss: 0.00002670
Iteration 85/1000 | Loss: 0.00002670
Iteration 86/1000 | Loss: 0.00002670
Iteration 87/1000 | Loss: 0.00002670
Iteration 88/1000 | Loss: 0.00002670
Iteration 89/1000 | Loss: 0.00002670
Iteration 90/1000 | Loss: 0.00002670
Iteration 91/1000 | Loss: 0.00002670
Iteration 92/1000 | Loss: 0.00002670
Iteration 93/1000 | Loss: 0.00002670
Iteration 94/1000 | Loss: 0.00002670
Iteration 95/1000 | Loss: 0.00002670
Iteration 96/1000 | Loss: 0.00002670
Iteration 97/1000 | Loss: 0.00002670
Iteration 98/1000 | Loss: 0.00002669
Iteration 99/1000 | Loss: 0.00002669
Iteration 100/1000 | Loss: 0.00002669
Iteration 101/1000 | Loss: 0.00002669
Iteration 102/1000 | Loss: 0.00002668
Iteration 103/1000 | Loss: 0.00002668
Iteration 104/1000 | Loss: 0.00002668
Iteration 105/1000 | Loss: 0.00002668
Iteration 106/1000 | Loss: 0.00002667
Iteration 107/1000 | Loss: 0.00002667
Iteration 108/1000 | Loss: 0.00002667
Iteration 109/1000 | Loss: 0.00002667
Iteration 110/1000 | Loss: 0.00002667
Iteration 111/1000 | Loss: 0.00002667
Iteration 112/1000 | Loss: 0.00002667
Iteration 113/1000 | Loss: 0.00002666
Iteration 114/1000 | Loss: 0.00002666
Iteration 115/1000 | Loss: 0.00002666
Iteration 116/1000 | Loss: 0.00002666
Iteration 117/1000 | Loss: 0.00002666
Iteration 118/1000 | Loss: 0.00002666
Iteration 119/1000 | Loss: 0.00002666
Iteration 120/1000 | Loss: 0.00002666
Iteration 121/1000 | Loss: 0.00002665
Iteration 122/1000 | Loss: 0.00002665
Iteration 123/1000 | Loss: 0.00002665
Iteration 124/1000 | Loss: 0.00002665
Iteration 125/1000 | Loss: 0.00002665
Iteration 126/1000 | Loss: 0.00002664
Iteration 127/1000 | Loss: 0.00002664
Iteration 128/1000 | Loss: 0.00002664
Iteration 129/1000 | Loss: 0.00002664
Iteration 130/1000 | Loss: 0.00002664
Iteration 131/1000 | Loss: 0.00002664
Iteration 132/1000 | Loss: 0.00002664
Iteration 133/1000 | Loss: 0.00002664
Iteration 134/1000 | Loss: 0.00002664
Iteration 135/1000 | Loss: 0.00002664
Iteration 136/1000 | Loss: 0.00002664
Iteration 137/1000 | Loss: 0.00002664
Iteration 138/1000 | Loss: 0.00002664
Iteration 139/1000 | Loss: 0.00002663
Iteration 140/1000 | Loss: 0.00002663
Iteration 141/1000 | Loss: 0.00002663
Iteration 142/1000 | Loss: 0.00002663
Iteration 143/1000 | Loss: 0.00002663
Iteration 144/1000 | Loss: 0.00002663
Iteration 145/1000 | Loss: 0.00002663
Iteration 146/1000 | Loss: 0.00002663
Iteration 147/1000 | Loss: 0.00002663
Iteration 148/1000 | Loss: 0.00002663
Iteration 149/1000 | Loss: 0.00002663
Iteration 150/1000 | Loss: 0.00002663
Iteration 151/1000 | Loss: 0.00002663
Iteration 152/1000 | Loss: 0.00002662
Iteration 153/1000 | Loss: 0.00002662
Iteration 154/1000 | Loss: 0.00002662
Iteration 155/1000 | Loss: 0.00002662
Iteration 156/1000 | Loss: 0.00002662
Iteration 157/1000 | Loss: 0.00002662
Iteration 158/1000 | Loss: 0.00002662
Iteration 159/1000 | Loss: 0.00002662
Iteration 160/1000 | Loss: 0.00002662
Iteration 161/1000 | Loss: 0.00002662
Iteration 162/1000 | Loss: 0.00002662
Iteration 163/1000 | Loss: 0.00002662
Iteration 164/1000 | Loss: 0.00002662
Iteration 165/1000 | Loss: 0.00002662
Iteration 166/1000 | Loss: 0.00002662
Iteration 167/1000 | Loss: 0.00002661
Iteration 168/1000 | Loss: 0.00002661
Iteration 169/1000 | Loss: 0.00002661
Iteration 170/1000 | Loss: 0.00002661
Iteration 171/1000 | Loss: 0.00002661
Iteration 172/1000 | Loss: 0.00002661
Iteration 173/1000 | Loss: 0.00002660
Iteration 174/1000 | Loss: 0.00002660
Iteration 175/1000 | Loss: 0.00002660
Iteration 176/1000 | Loss: 0.00002660
Iteration 177/1000 | Loss: 0.00002660
Iteration 178/1000 | Loss: 0.00002660
Iteration 179/1000 | Loss: 0.00002660
Iteration 180/1000 | Loss: 0.00002660
Iteration 181/1000 | Loss: 0.00002660
Iteration 182/1000 | Loss: 0.00002660
Iteration 183/1000 | Loss: 0.00002660
Iteration 184/1000 | Loss: 0.00002660
Iteration 185/1000 | Loss: 0.00002660
Iteration 186/1000 | Loss: 0.00002659
Iteration 187/1000 | Loss: 0.00002659
Iteration 188/1000 | Loss: 0.00002659
Iteration 189/1000 | Loss: 0.00002659
Iteration 190/1000 | Loss: 0.00002659
Iteration 191/1000 | Loss: 0.00002659
Iteration 192/1000 | Loss: 0.00002659
Iteration 193/1000 | Loss: 0.00002659
Iteration 194/1000 | Loss: 0.00002658
Iteration 195/1000 | Loss: 0.00002658
Iteration 196/1000 | Loss: 0.00002657
Iteration 197/1000 | Loss: 0.00002657
Iteration 198/1000 | Loss: 0.00002657
Iteration 199/1000 | Loss: 0.00002657
Iteration 200/1000 | Loss: 0.00002657
Iteration 201/1000 | Loss: 0.00002657
Iteration 202/1000 | Loss: 0.00002657
Iteration 203/1000 | Loss: 0.00002657
Iteration 204/1000 | Loss: 0.00002657
Iteration 205/1000 | Loss: 0.00002656
Iteration 206/1000 | Loss: 0.00002656
Iteration 207/1000 | Loss: 0.00002656
Iteration 208/1000 | Loss: 0.00002655
Iteration 209/1000 | Loss: 0.00002655
Iteration 210/1000 | Loss: 0.00002655
Iteration 211/1000 | Loss: 0.00002655
Iteration 212/1000 | Loss: 0.00002655
Iteration 213/1000 | Loss: 0.00002655
Iteration 214/1000 | Loss: 0.00002654
Iteration 215/1000 | Loss: 0.00002654
Iteration 216/1000 | Loss: 0.00002654
Iteration 217/1000 | Loss: 0.00002654
Iteration 218/1000 | Loss: 0.00002654
Iteration 219/1000 | Loss: 0.00002653
Iteration 220/1000 | Loss: 0.00002653
Iteration 221/1000 | Loss: 0.00002653
Iteration 222/1000 | Loss: 0.00002653
Iteration 223/1000 | Loss: 0.00002653
Iteration 224/1000 | Loss: 0.00002653
Iteration 225/1000 | Loss: 0.00002653
Iteration 226/1000 | Loss: 0.00002653
Iteration 227/1000 | Loss: 0.00002653
Iteration 228/1000 | Loss: 0.00002653
Iteration 229/1000 | Loss: 0.00002653
Iteration 230/1000 | Loss: 0.00002653
Iteration 231/1000 | Loss: 0.00002652
Iteration 232/1000 | Loss: 0.00002652
Iteration 233/1000 | Loss: 0.00002652
Iteration 234/1000 | Loss: 0.00002652
Iteration 235/1000 | Loss: 0.00002652
Iteration 236/1000 | Loss: 0.00002652
Iteration 237/1000 | Loss: 0.00002652
Iteration 238/1000 | Loss: 0.00002652
Iteration 239/1000 | Loss: 0.00002652
Iteration 240/1000 | Loss: 0.00002652
Iteration 241/1000 | Loss: 0.00002652
Iteration 242/1000 | Loss: 0.00002651
Iteration 243/1000 | Loss: 0.00002651
Iteration 244/1000 | Loss: 0.00002651
Iteration 245/1000 | Loss: 0.00002651
Iteration 246/1000 | Loss: 0.00002651
Iteration 247/1000 | Loss: 0.00002650
Iteration 248/1000 | Loss: 0.00002650
Iteration 249/1000 | Loss: 0.00002650
Iteration 250/1000 | Loss: 0.00002650
Iteration 251/1000 | Loss: 0.00002650
Iteration 252/1000 | Loss: 0.00002650
Iteration 253/1000 | Loss: 0.00002650
Iteration 254/1000 | Loss: 0.00002649
Iteration 255/1000 | Loss: 0.00002649
Iteration 256/1000 | Loss: 0.00002649
Iteration 257/1000 | Loss: 0.00002649
Iteration 258/1000 | Loss: 0.00002649
Iteration 259/1000 | Loss: 0.00002649
Iteration 260/1000 | Loss: 0.00002649
Iteration 261/1000 | Loss: 0.00002648
Iteration 262/1000 | Loss: 0.00002648
Iteration 263/1000 | Loss: 0.00002648
Iteration 264/1000 | Loss: 0.00002648
Iteration 265/1000 | Loss: 0.00002648
Iteration 266/1000 | Loss: 0.00002648
Iteration 267/1000 | Loss: 0.00002648
Iteration 268/1000 | Loss: 0.00002648
Iteration 269/1000 | Loss: 0.00002647
Iteration 270/1000 | Loss: 0.00002647
Iteration 271/1000 | Loss: 0.00002647
Iteration 272/1000 | Loss: 0.00002647
Iteration 273/1000 | Loss: 0.00002647
Iteration 274/1000 | Loss: 0.00002647
Iteration 275/1000 | Loss: 0.00002647
Iteration 276/1000 | Loss: 0.00002647
Iteration 277/1000 | Loss: 0.00002646
Iteration 278/1000 | Loss: 0.00002646
Iteration 279/1000 | Loss: 0.00002646
Iteration 280/1000 | Loss: 0.00002646
Iteration 281/1000 | Loss: 0.00002646
Iteration 282/1000 | Loss: 0.00002646
Iteration 283/1000 | Loss: 0.00002646
Iteration 284/1000 | Loss: 0.00002646
Iteration 285/1000 | Loss: 0.00002646
Iteration 286/1000 | Loss: 0.00002646
Iteration 287/1000 | Loss: 0.00002645
Iteration 288/1000 | Loss: 0.00002645
Iteration 289/1000 | Loss: 0.00002645
Iteration 290/1000 | Loss: 0.00002645
Iteration 291/1000 | Loss: 0.00002645
Iteration 292/1000 | Loss: 0.00002645
Iteration 293/1000 | Loss: 0.00002645
Iteration 294/1000 | Loss: 0.00002645
Iteration 295/1000 | Loss: 0.00002645
Iteration 296/1000 | Loss: 0.00002645
Iteration 297/1000 | Loss: 0.00002645
Iteration 298/1000 | Loss: 0.00002645
Iteration 299/1000 | Loss: 0.00002645
Iteration 300/1000 | Loss: 0.00002645
Iteration 301/1000 | Loss: 0.00002645
Iteration 302/1000 | Loss: 0.00002644
Iteration 303/1000 | Loss: 0.00002644
Iteration 304/1000 | Loss: 0.00002644
Iteration 305/1000 | Loss: 0.00002644
Iteration 306/1000 | Loss: 0.00002644
Iteration 307/1000 | Loss: 0.00002644
Iteration 308/1000 | Loss: 0.00002644
Iteration 309/1000 | Loss: 0.00002644
Iteration 310/1000 | Loss: 0.00002644
Iteration 311/1000 | Loss: 0.00002644
Iteration 312/1000 | Loss: 0.00002644
Iteration 313/1000 | Loss: 0.00002644
Iteration 314/1000 | Loss: 0.00002644
Iteration 315/1000 | Loss: 0.00002644
Iteration 316/1000 | Loss: 0.00002644
Iteration 317/1000 | Loss: 0.00002644
Iteration 318/1000 | Loss: 0.00002644
Iteration 319/1000 | Loss: 0.00002644
Iteration 320/1000 | Loss: 0.00002643
Iteration 321/1000 | Loss: 0.00002643
Iteration 322/1000 | Loss: 0.00002643
Iteration 323/1000 | Loss: 0.00002643
Iteration 324/1000 | Loss: 0.00002643
Iteration 325/1000 | Loss: 0.00002643
Iteration 326/1000 | Loss: 0.00002643
Iteration 327/1000 | Loss: 0.00002643
Iteration 328/1000 | Loss: 0.00002643
Iteration 329/1000 | Loss: 0.00002643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 329. Stopping optimization.
Last 5 losses: [2.6433539460413158e-05, 2.6433539460413158e-05, 2.6433539460413158e-05, 2.6433539460413158e-05, 2.6433539460413158e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6433539460413158e-05

Optimization complete. Final v2v error: 4.358121871948242 mm

Highest mean error: 4.6120734214782715 mm for frame 106

Lowest mean error: 4.079657554626465 mm for frame 41

Saving results

Total time: 59.96554708480835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082946
Iteration 2/25 | Loss: 0.00238417
Iteration 3/25 | Loss: 0.00192255
Iteration 4/25 | Loss: 0.00168409
Iteration 5/25 | Loss: 0.00147669
Iteration 6/25 | Loss: 0.00123251
Iteration 7/25 | Loss: 0.00113157
Iteration 8/25 | Loss: 0.00107308
Iteration 9/25 | Loss: 0.00103955
Iteration 10/25 | Loss: 0.00102885
Iteration 11/25 | Loss: 0.00101125
Iteration 12/25 | Loss: 0.00100097
Iteration 13/25 | Loss: 0.00099842
Iteration 14/25 | Loss: 0.00099776
Iteration 15/25 | Loss: 0.00099598
Iteration 16/25 | Loss: 0.00099571
Iteration 17/25 | Loss: 0.00099556
Iteration 18/25 | Loss: 0.00099549
Iteration 19/25 | Loss: 0.00099549
Iteration 20/25 | Loss: 0.00099549
Iteration 21/25 | Loss: 0.00099549
Iteration 22/25 | Loss: 0.00099549
Iteration 23/25 | Loss: 0.00099548
Iteration 24/25 | Loss: 0.00099548
Iteration 25/25 | Loss: 0.00099548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63771713
Iteration 2/25 | Loss: 0.00256925
Iteration 3/25 | Loss: 0.00256924
Iteration 4/25 | Loss: 0.00256924
Iteration 5/25 | Loss: 0.00256924
Iteration 6/25 | Loss: 0.00256924
Iteration 7/25 | Loss: 0.00256924
Iteration 8/25 | Loss: 0.00256924
Iteration 9/25 | Loss: 0.00256924
Iteration 10/25 | Loss: 0.00256924
Iteration 11/25 | Loss: 0.00256924
Iteration 12/25 | Loss: 0.00256924
Iteration 13/25 | Loss: 0.00256924
Iteration 14/25 | Loss: 0.00256924
Iteration 15/25 | Loss: 0.00256924
Iteration 16/25 | Loss: 0.00256924
Iteration 17/25 | Loss: 0.00256924
Iteration 18/25 | Loss: 0.00256924
Iteration 19/25 | Loss: 0.00256924
Iteration 20/25 | Loss: 0.00256924
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002569240750744939, 0.002569240750744939, 0.002569240750744939, 0.002569240750744939, 0.002569240750744939]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002569240750744939

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00256924
Iteration 2/1000 | Loss: 0.00009379
Iteration 3/1000 | Loss: 0.00006606
Iteration 4/1000 | Loss: 0.00005755
Iteration 5/1000 | Loss: 0.00005094
Iteration 6/1000 | Loss: 0.00004805
Iteration 7/1000 | Loss: 0.00004569
Iteration 8/1000 | Loss: 0.00004445
Iteration 9/1000 | Loss: 0.00004364
Iteration 10/1000 | Loss: 0.00004289
Iteration 11/1000 | Loss: 0.00004245
Iteration 12/1000 | Loss: 0.00004206
Iteration 13/1000 | Loss: 0.00004180
Iteration 14/1000 | Loss: 0.00004152
Iteration 15/1000 | Loss: 0.00004151
Iteration 16/1000 | Loss: 0.00021070
Iteration 17/1000 | Loss: 0.00161074
Iteration 18/1000 | Loss: 0.00010168
Iteration 19/1000 | Loss: 0.00006858
Iteration 20/1000 | Loss: 0.00005507
Iteration 21/1000 | Loss: 0.00004184
Iteration 22/1000 | Loss: 0.00003184
Iteration 23/1000 | Loss: 0.00002859
Iteration 24/1000 | Loss: 0.00002692
Iteration 25/1000 | Loss: 0.00002577
Iteration 26/1000 | Loss: 0.00002503
Iteration 27/1000 | Loss: 0.00002441
Iteration 28/1000 | Loss: 0.00002404
Iteration 29/1000 | Loss: 0.00002379
Iteration 30/1000 | Loss: 0.00002351
Iteration 31/1000 | Loss: 0.00002347
Iteration 32/1000 | Loss: 0.00002340
Iteration 33/1000 | Loss: 0.00002333
Iteration 34/1000 | Loss: 0.00002331
Iteration 35/1000 | Loss: 0.00002331
Iteration 36/1000 | Loss: 0.00002330
Iteration 37/1000 | Loss: 0.00002330
Iteration 38/1000 | Loss: 0.00002329
Iteration 39/1000 | Loss: 0.00002328
Iteration 40/1000 | Loss: 0.00002328
Iteration 41/1000 | Loss: 0.00002328
Iteration 42/1000 | Loss: 0.00002328
Iteration 43/1000 | Loss: 0.00002328
Iteration 44/1000 | Loss: 0.00002328
Iteration 45/1000 | Loss: 0.00002328
Iteration 46/1000 | Loss: 0.00002328
Iteration 47/1000 | Loss: 0.00002328
Iteration 48/1000 | Loss: 0.00002327
Iteration 49/1000 | Loss: 0.00002327
Iteration 50/1000 | Loss: 0.00002327
Iteration 51/1000 | Loss: 0.00002327
Iteration 52/1000 | Loss: 0.00002326
Iteration 53/1000 | Loss: 0.00002326
Iteration 54/1000 | Loss: 0.00002326
Iteration 55/1000 | Loss: 0.00002326
Iteration 56/1000 | Loss: 0.00002326
Iteration 57/1000 | Loss: 0.00002326
Iteration 58/1000 | Loss: 0.00002326
Iteration 59/1000 | Loss: 0.00002325
Iteration 60/1000 | Loss: 0.00002325
Iteration 61/1000 | Loss: 0.00002325
Iteration 62/1000 | Loss: 0.00002324
Iteration 63/1000 | Loss: 0.00002324
Iteration 64/1000 | Loss: 0.00002324
Iteration 65/1000 | Loss: 0.00002324
Iteration 66/1000 | Loss: 0.00002324
Iteration 67/1000 | Loss: 0.00002324
Iteration 68/1000 | Loss: 0.00002324
Iteration 69/1000 | Loss: 0.00002323
Iteration 70/1000 | Loss: 0.00002323
Iteration 71/1000 | Loss: 0.00002323
Iteration 72/1000 | Loss: 0.00002323
Iteration 73/1000 | Loss: 0.00002323
Iteration 74/1000 | Loss: 0.00002323
Iteration 75/1000 | Loss: 0.00002323
Iteration 76/1000 | Loss: 0.00002323
Iteration 77/1000 | Loss: 0.00002323
Iteration 78/1000 | Loss: 0.00002323
Iteration 79/1000 | Loss: 0.00002323
Iteration 80/1000 | Loss: 0.00002323
Iteration 81/1000 | Loss: 0.00002323
Iteration 82/1000 | Loss: 0.00002323
Iteration 83/1000 | Loss: 0.00002323
Iteration 84/1000 | Loss: 0.00002323
Iteration 85/1000 | Loss: 0.00002323
Iteration 86/1000 | Loss: 0.00002323
Iteration 87/1000 | Loss: 0.00002323
Iteration 88/1000 | Loss: 0.00002323
Iteration 89/1000 | Loss: 0.00002323
Iteration 90/1000 | Loss: 0.00002323
Iteration 91/1000 | Loss: 0.00002323
Iteration 92/1000 | Loss: 0.00002323
Iteration 93/1000 | Loss: 0.00002323
Iteration 94/1000 | Loss: 0.00002323
Iteration 95/1000 | Loss: 0.00002323
Iteration 96/1000 | Loss: 0.00002323
Iteration 97/1000 | Loss: 0.00002323
Iteration 98/1000 | Loss: 0.00002323
Iteration 99/1000 | Loss: 0.00002323
Iteration 100/1000 | Loss: 0.00002323
Iteration 101/1000 | Loss: 0.00002323
Iteration 102/1000 | Loss: 0.00002323
Iteration 103/1000 | Loss: 0.00002323
Iteration 104/1000 | Loss: 0.00002323
Iteration 105/1000 | Loss: 0.00002323
Iteration 106/1000 | Loss: 0.00002323
Iteration 107/1000 | Loss: 0.00002323
Iteration 108/1000 | Loss: 0.00002323
Iteration 109/1000 | Loss: 0.00002323
Iteration 110/1000 | Loss: 0.00002323
Iteration 111/1000 | Loss: 0.00002323
Iteration 112/1000 | Loss: 0.00002323
Iteration 113/1000 | Loss: 0.00002323
Iteration 114/1000 | Loss: 0.00002323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.3226912162499502e-05, 2.3226912162499502e-05, 2.3226912162499502e-05, 2.3226912162499502e-05, 2.3226912162499502e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3226912162499502e-05

Optimization complete. Final v2v error: 3.9740140438079834 mm

Highest mean error: 10.50196647644043 mm for frame 63

Lowest mean error: 3.7038135528564453 mm for frame 106

Saving results

Total time: 88.97124767303467
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_us_1402/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_us_1402/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443668
Iteration 2/25 | Loss: 0.00113508
Iteration 3/25 | Loss: 0.00087155
Iteration 4/25 | Loss: 0.00082495
Iteration 5/25 | Loss: 0.00081281
Iteration 6/25 | Loss: 0.00080918
Iteration 7/25 | Loss: 0.00080806
Iteration 8/25 | Loss: 0.00080798
Iteration 9/25 | Loss: 0.00080798
Iteration 10/25 | Loss: 0.00080798
Iteration 11/25 | Loss: 0.00080798
Iteration 12/25 | Loss: 0.00080798
Iteration 13/25 | Loss: 0.00080798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008079786202870309, 0.0008079786202870309, 0.0008079786202870309, 0.0008079786202870309, 0.0008079786202870309]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008079786202870309

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64094949
Iteration 2/25 | Loss: 0.00197397
Iteration 3/25 | Loss: 0.00197397
Iteration 4/25 | Loss: 0.00197397
Iteration 5/25 | Loss: 0.00197397
Iteration 6/25 | Loss: 0.00197397
Iteration 7/25 | Loss: 0.00197397
Iteration 8/25 | Loss: 0.00197397
Iteration 9/25 | Loss: 0.00197397
Iteration 10/25 | Loss: 0.00197397
Iteration 11/25 | Loss: 0.00197397
Iteration 12/25 | Loss: 0.00197397
Iteration 13/25 | Loss: 0.00197397
Iteration 14/25 | Loss: 0.00197397
Iteration 15/25 | Loss: 0.00197397
Iteration 16/25 | Loss: 0.00197397
Iteration 17/25 | Loss: 0.00197397
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019739666022360325, 0.0019739666022360325, 0.0019739666022360325, 0.0019739666022360325, 0.0019739666022360325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019739666022360325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197397
Iteration 2/1000 | Loss: 0.00002475
Iteration 3/1000 | Loss: 0.00001681
Iteration 4/1000 | Loss: 0.00001477
Iteration 5/1000 | Loss: 0.00001403
Iteration 6/1000 | Loss: 0.00001340
Iteration 7/1000 | Loss: 0.00001305
Iteration 8/1000 | Loss: 0.00001285
Iteration 9/1000 | Loss: 0.00001267
Iteration 10/1000 | Loss: 0.00001263
Iteration 11/1000 | Loss: 0.00001263
Iteration 12/1000 | Loss: 0.00001259
Iteration 13/1000 | Loss: 0.00001259
Iteration 14/1000 | Loss: 0.00001258
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001257
Iteration 17/1000 | Loss: 0.00001257
Iteration 18/1000 | Loss: 0.00001255
Iteration 19/1000 | Loss: 0.00001254
Iteration 20/1000 | Loss: 0.00001253
Iteration 21/1000 | Loss: 0.00001253
Iteration 22/1000 | Loss: 0.00001252
Iteration 23/1000 | Loss: 0.00001252
Iteration 24/1000 | Loss: 0.00001250
Iteration 25/1000 | Loss: 0.00001248
Iteration 26/1000 | Loss: 0.00001248
Iteration 27/1000 | Loss: 0.00001248
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001248
Iteration 30/1000 | Loss: 0.00001248
Iteration 31/1000 | Loss: 0.00001248
Iteration 32/1000 | Loss: 0.00001248
Iteration 33/1000 | Loss: 0.00001246
Iteration 34/1000 | Loss: 0.00001246
Iteration 35/1000 | Loss: 0.00001246
Iteration 36/1000 | Loss: 0.00001246
Iteration 37/1000 | Loss: 0.00001245
Iteration 38/1000 | Loss: 0.00001245
Iteration 39/1000 | Loss: 0.00001245
Iteration 40/1000 | Loss: 0.00001245
Iteration 41/1000 | Loss: 0.00001245
Iteration 42/1000 | Loss: 0.00001244
Iteration 43/1000 | Loss: 0.00001244
Iteration 44/1000 | Loss: 0.00001244
Iteration 45/1000 | Loss: 0.00001243
Iteration 46/1000 | Loss: 0.00001243
Iteration 47/1000 | Loss: 0.00001242
Iteration 48/1000 | Loss: 0.00001241
Iteration 49/1000 | Loss: 0.00001241
Iteration 50/1000 | Loss: 0.00001241
Iteration 51/1000 | Loss: 0.00001240
Iteration 52/1000 | Loss: 0.00001240
Iteration 53/1000 | Loss: 0.00001240
Iteration 54/1000 | Loss: 0.00001240
Iteration 55/1000 | Loss: 0.00001239
Iteration 56/1000 | Loss: 0.00001239
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001238
Iteration 61/1000 | Loss: 0.00001238
Iteration 62/1000 | Loss: 0.00001237
Iteration 63/1000 | Loss: 0.00001237
Iteration 64/1000 | Loss: 0.00001237
Iteration 65/1000 | Loss: 0.00001236
Iteration 66/1000 | Loss: 0.00001236
Iteration 67/1000 | Loss: 0.00001236
Iteration 68/1000 | Loss: 0.00001235
Iteration 69/1000 | Loss: 0.00001235
Iteration 70/1000 | Loss: 0.00001235
Iteration 71/1000 | Loss: 0.00001234
Iteration 72/1000 | Loss: 0.00001234
Iteration 73/1000 | Loss: 0.00001234
Iteration 74/1000 | Loss: 0.00001234
Iteration 75/1000 | Loss: 0.00001234
Iteration 76/1000 | Loss: 0.00001234
Iteration 77/1000 | Loss: 0.00001233
Iteration 78/1000 | Loss: 0.00001233
Iteration 79/1000 | Loss: 0.00001233
Iteration 80/1000 | Loss: 0.00001233
Iteration 81/1000 | Loss: 0.00001232
Iteration 82/1000 | Loss: 0.00001232
Iteration 83/1000 | Loss: 0.00001232
Iteration 84/1000 | Loss: 0.00001232
Iteration 85/1000 | Loss: 0.00001232
Iteration 86/1000 | Loss: 0.00001232
Iteration 87/1000 | Loss: 0.00001232
Iteration 88/1000 | Loss: 0.00001232
Iteration 89/1000 | Loss: 0.00001232
Iteration 90/1000 | Loss: 0.00001232
Iteration 91/1000 | Loss: 0.00001231
Iteration 92/1000 | Loss: 0.00001231
Iteration 93/1000 | Loss: 0.00001231
Iteration 94/1000 | Loss: 0.00001231
Iteration 95/1000 | Loss: 0.00001231
Iteration 96/1000 | Loss: 0.00001230
Iteration 97/1000 | Loss: 0.00001230
Iteration 98/1000 | Loss: 0.00001230
Iteration 99/1000 | Loss: 0.00001230
Iteration 100/1000 | Loss: 0.00001230
Iteration 101/1000 | Loss: 0.00001230
Iteration 102/1000 | Loss: 0.00001230
Iteration 103/1000 | Loss: 0.00001230
Iteration 104/1000 | Loss: 0.00001230
Iteration 105/1000 | Loss: 0.00001230
Iteration 106/1000 | Loss: 0.00001230
Iteration 107/1000 | Loss: 0.00001230
Iteration 108/1000 | Loss: 0.00001230
Iteration 109/1000 | Loss: 0.00001230
Iteration 110/1000 | Loss: 0.00001230
Iteration 111/1000 | Loss: 0.00001230
Iteration 112/1000 | Loss: 0.00001229
Iteration 113/1000 | Loss: 0.00001229
Iteration 114/1000 | Loss: 0.00001229
Iteration 115/1000 | Loss: 0.00001229
Iteration 116/1000 | Loss: 0.00001229
Iteration 117/1000 | Loss: 0.00001229
Iteration 118/1000 | Loss: 0.00001228
Iteration 119/1000 | Loss: 0.00001228
Iteration 120/1000 | Loss: 0.00001228
Iteration 121/1000 | Loss: 0.00001228
Iteration 122/1000 | Loss: 0.00001228
Iteration 123/1000 | Loss: 0.00001228
Iteration 124/1000 | Loss: 0.00001228
Iteration 125/1000 | Loss: 0.00001228
Iteration 126/1000 | Loss: 0.00001228
Iteration 127/1000 | Loss: 0.00001228
Iteration 128/1000 | Loss: 0.00001228
Iteration 129/1000 | Loss: 0.00001228
Iteration 130/1000 | Loss: 0.00001228
Iteration 131/1000 | Loss: 0.00001228
Iteration 132/1000 | Loss: 0.00001227
Iteration 133/1000 | Loss: 0.00001227
Iteration 134/1000 | Loss: 0.00001227
Iteration 135/1000 | Loss: 0.00001227
Iteration 136/1000 | Loss: 0.00001227
Iteration 137/1000 | Loss: 0.00001227
Iteration 138/1000 | Loss: 0.00001227
Iteration 139/1000 | Loss: 0.00001227
Iteration 140/1000 | Loss: 0.00001227
Iteration 141/1000 | Loss: 0.00001227
Iteration 142/1000 | Loss: 0.00001227
Iteration 143/1000 | Loss: 0.00001227
Iteration 144/1000 | Loss: 0.00001227
Iteration 145/1000 | Loss: 0.00001226
Iteration 146/1000 | Loss: 0.00001226
Iteration 147/1000 | Loss: 0.00001226
Iteration 148/1000 | Loss: 0.00001226
Iteration 149/1000 | Loss: 0.00001226
Iteration 150/1000 | Loss: 0.00001226
Iteration 151/1000 | Loss: 0.00001226
Iteration 152/1000 | Loss: 0.00001226
Iteration 153/1000 | Loss: 0.00001226
Iteration 154/1000 | Loss: 0.00001226
Iteration 155/1000 | Loss: 0.00001226
Iteration 156/1000 | Loss: 0.00001226
Iteration 157/1000 | Loss: 0.00001226
Iteration 158/1000 | Loss: 0.00001226
Iteration 159/1000 | Loss: 0.00001225
Iteration 160/1000 | Loss: 0.00001225
Iteration 161/1000 | Loss: 0.00001225
Iteration 162/1000 | Loss: 0.00001225
Iteration 163/1000 | Loss: 0.00001225
Iteration 164/1000 | Loss: 0.00001225
Iteration 165/1000 | Loss: 0.00001225
Iteration 166/1000 | Loss: 0.00001225
Iteration 167/1000 | Loss: 0.00001225
Iteration 168/1000 | Loss: 0.00001225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.2252372471266426e-05, 1.2252372471266426e-05, 1.2252372471266426e-05, 1.2252372471266426e-05, 1.2252372471266426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2252372471266426e-05

Optimization complete. Final v2v error: 2.9981689453125 mm

Highest mean error: 3.706240653991699 mm for frame 91

Lowest mean error: 2.7298238277435303 mm for frame 24

Saving results

Total time: 39.85672211647034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438472
Iteration 2/25 | Loss: 0.00140701
Iteration 3/25 | Loss: 0.00131282
Iteration 4/25 | Loss: 0.00129963
Iteration 5/25 | Loss: 0.00129613
Iteration 6/25 | Loss: 0.00129524
Iteration 7/25 | Loss: 0.00129494
Iteration 8/25 | Loss: 0.00129494
Iteration 9/25 | Loss: 0.00129494
Iteration 10/25 | Loss: 0.00129494
Iteration 11/25 | Loss: 0.00129494
Iteration 12/25 | Loss: 0.00129494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001294936053454876, 0.001294936053454876, 0.001294936053454876, 0.001294936053454876, 0.001294936053454876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001294936053454876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50580025
Iteration 2/25 | Loss: 0.00088583
Iteration 3/25 | Loss: 0.00088583
Iteration 4/25 | Loss: 0.00088583
Iteration 5/25 | Loss: 0.00088583
Iteration 6/25 | Loss: 0.00088583
Iteration 7/25 | Loss: 0.00088583
Iteration 8/25 | Loss: 0.00088583
Iteration 9/25 | Loss: 0.00088583
Iteration 10/25 | Loss: 0.00088583
Iteration 11/25 | Loss: 0.00088583
Iteration 12/25 | Loss: 0.00088583
Iteration 13/25 | Loss: 0.00088583
Iteration 14/25 | Loss: 0.00088583
Iteration 15/25 | Loss: 0.00088583
Iteration 16/25 | Loss: 0.00088583
Iteration 17/25 | Loss: 0.00088583
Iteration 18/25 | Loss: 0.00088583
Iteration 19/25 | Loss: 0.00088583
Iteration 20/25 | Loss: 0.00088583
Iteration 21/25 | Loss: 0.00088583
Iteration 22/25 | Loss: 0.00088583
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008858251967467368, 0.0008858251967467368, 0.0008858251967467368, 0.0008858251967467368, 0.0008858251967467368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008858251967467368

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088583
Iteration 2/1000 | Loss: 0.00004329
Iteration 3/1000 | Loss: 0.00002796
Iteration 4/1000 | Loss: 0.00002241
Iteration 5/1000 | Loss: 0.00002052
Iteration 6/1000 | Loss: 0.00001982
Iteration 7/1000 | Loss: 0.00001915
Iteration 8/1000 | Loss: 0.00001872
Iteration 9/1000 | Loss: 0.00001822
Iteration 10/1000 | Loss: 0.00001786
Iteration 11/1000 | Loss: 0.00001763
Iteration 12/1000 | Loss: 0.00001743
Iteration 13/1000 | Loss: 0.00001717
Iteration 14/1000 | Loss: 0.00001701
Iteration 15/1000 | Loss: 0.00001684
Iteration 16/1000 | Loss: 0.00001682
Iteration 17/1000 | Loss: 0.00001676
Iteration 18/1000 | Loss: 0.00001672
Iteration 19/1000 | Loss: 0.00001670
Iteration 20/1000 | Loss: 0.00001669
Iteration 21/1000 | Loss: 0.00001668
Iteration 22/1000 | Loss: 0.00001667
Iteration 23/1000 | Loss: 0.00001662
Iteration 24/1000 | Loss: 0.00001658
Iteration 25/1000 | Loss: 0.00001656
Iteration 26/1000 | Loss: 0.00001655
Iteration 27/1000 | Loss: 0.00001655
Iteration 28/1000 | Loss: 0.00001653
Iteration 29/1000 | Loss: 0.00001652
Iteration 30/1000 | Loss: 0.00001652
Iteration 31/1000 | Loss: 0.00001651
Iteration 32/1000 | Loss: 0.00001651
Iteration 33/1000 | Loss: 0.00001650
Iteration 34/1000 | Loss: 0.00001650
Iteration 35/1000 | Loss: 0.00001649
Iteration 36/1000 | Loss: 0.00001649
Iteration 37/1000 | Loss: 0.00001648
Iteration 38/1000 | Loss: 0.00001647
Iteration 39/1000 | Loss: 0.00001646
Iteration 40/1000 | Loss: 0.00001645
Iteration 41/1000 | Loss: 0.00001645
Iteration 42/1000 | Loss: 0.00001644
Iteration 43/1000 | Loss: 0.00001644
Iteration 44/1000 | Loss: 0.00001642
Iteration 45/1000 | Loss: 0.00001642
Iteration 46/1000 | Loss: 0.00001639
Iteration 47/1000 | Loss: 0.00001638
Iteration 48/1000 | Loss: 0.00001638
Iteration 49/1000 | Loss: 0.00001637
Iteration 50/1000 | Loss: 0.00001637
Iteration 51/1000 | Loss: 0.00001637
Iteration 52/1000 | Loss: 0.00001636
Iteration 53/1000 | Loss: 0.00001636
Iteration 54/1000 | Loss: 0.00001635
Iteration 55/1000 | Loss: 0.00001635
Iteration 56/1000 | Loss: 0.00001635
Iteration 57/1000 | Loss: 0.00001634
Iteration 58/1000 | Loss: 0.00001634
Iteration 59/1000 | Loss: 0.00001634
Iteration 60/1000 | Loss: 0.00001634
Iteration 61/1000 | Loss: 0.00001633
Iteration 62/1000 | Loss: 0.00001633
Iteration 63/1000 | Loss: 0.00001632
Iteration 64/1000 | Loss: 0.00001631
Iteration 65/1000 | Loss: 0.00001631
Iteration 66/1000 | Loss: 0.00001631
Iteration 67/1000 | Loss: 0.00001631
Iteration 68/1000 | Loss: 0.00001631
Iteration 69/1000 | Loss: 0.00001630
Iteration 70/1000 | Loss: 0.00001630
Iteration 71/1000 | Loss: 0.00001629
Iteration 72/1000 | Loss: 0.00001629
Iteration 73/1000 | Loss: 0.00001628
Iteration 74/1000 | Loss: 0.00001628
Iteration 75/1000 | Loss: 0.00001628
Iteration 76/1000 | Loss: 0.00001628
Iteration 77/1000 | Loss: 0.00001627
Iteration 78/1000 | Loss: 0.00001627
Iteration 79/1000 | Loss: 0.00001626
Iteration 80/1000 | Loss: 0.00001626
Iteration 81/1000 | Loss: 0.00001626
Iteration 82/1000 | Loss: 0.00001625
Iteration 83/1000 | Loss: 0.00001625
Iteration 84/1000 | Loss: 0.00001625
Iteration 85/1000 | Loss: 0.00001624
Iteration 86/1000 | Loss: 0.00001624
Iteration 87/1000 | Loss: 0.00001624
Iteration 88/1000 | Loss: 0.00001623
Iteration 89/1000 | Loss: 0.00001623
Iteration 90/1000 | Loss: 0.00001623
Iteration 91/1000 | Loss: 0.00001623
Iteration 92/1000 | Loss: 0.00001623
Iteration 93/1000 | Loss: 0.00001623
Iteration 94/1000 | Loss: 0.00001623
Iteration 95/1000 | Loss: 0.00001622
Iteration 96/1000 | Loss: 0.00001621
Iteration 97/1000 | Loss: 0.00001621
Iteration 98/1000 | Loss: 0.00001621
Iteration 99/1000 | Loss: 0.00001621
Iteration 100/1000 | Loss: 0.00001621
Iteration 101/1000 | Loss: 0.00001621
Iteration 102/1000 | Loss: 0.00001620
Iteration 103/1000 | Loss: 0.00001620
Iteration 104/1000 | Loss: 0.00001620
Iteration 105/1000 | Loss: 0.00001620
Iteration 106/1000 | Loss: 0.00001620
Iteration 107/1000 | Loss: 0.00001620
Iteration 108/1000 | Loss: 0.00001619
Iteration 109/1000 | Loss: 0.00001619
Iteration 110/1000 | Loss: 0.00001619
Iteration 111/1000 | Loss: 0.00001619
Iteration 112/1000 | Loss: 0.00001618
Iteration 113/1000 | Loss: 0.00001618
Iteration 114/1000 | Loss: 0.00001618
Iteration 115/1000 | Loss: 0.00001618
Iteration 116/1000 | Loss: 0.00001618
Iteration 117/1000 | Loss: 0.00001618
Iteration 118/1000 | Loss: 0.00001618
Iteration 119/1000 | Loss: 0.00001617
Iteration 120/1000 | Loss: 0.00001617
Iteration 121/1000 | Loss: 0.00001617
Iteration 122/1000 | Loss: 0.00001617
Iteration 123/1000 | Loss: 0.00001617
Iteration 124/1000 | Loss: 0.00001617
Iteration 125/1000 | Loss: 0.00001617
Iteration 126/1000 | Loss: 0.00001616
Iteration 127/1000 | Loss: 0.00001616
Iteration 128/1000 | Loss: 0.00001616
Iteration 129/1000 | Loss: 0.00001616
Iteration 130/1000 | Loss: 0.00001616
Iteration 131/1000 | Loss: 0.00001616
Iteration 132/1000 | Loss: 0.00001616
Iteration 133/1000 | Loss: 0.00001616
Iteration 134/1000 | Loss: 0.00001616
Iteration 135/1000 | Loss: 0.00001616
Iteration 136/1000 | Loss: 0.00001616
Iteration 137/1000 | Loss: 0.00001615
Iteration 138/1000 | Loss: 0.00001615
Iteration 139/1000 | Loss: 0.00001615
Iteration 140/1000 | Loss: 0.00001615
Iteration 141/1000 | Loss: 0.00001615
Iteration 142/1000 | Loss: 0.00001615
Iteration 143/1000 | Loss: 0.00001615
Iteration 144/1000 | Loss: 0.00001614
Iteration 145/1000 | Loss: 0.00001614
Iteration 146/1000 | Loss: 0.00001614
Iteration 147/1000 | Loss: 0.00001614
Iteration 148/1000 | Loss: 0.00001614
Iteration 149/1000 | Loss: 0.00001614
Iteration 150/1000 | Loss: 0.00001614
Iteration 151/1000 | Loss: 0.00001614
Iteration 152/1000 | Loss: 0.00001614
Iteration 153/1000 | Loss: 0.00001614
Iteration 154/1000 | Loss: 0.00001614
Iteration 155/1000 | Loss: 0.00001614
Iteration 156/1000 | Loss: 0.00001614
Iteration 157/1000 | Loss: 0.00001614
Iteration 158/1000 | Loss: 0.00001614
Iteration 159/1000 | Loss: 0.00001614
Iteration 160/1000 | Loss: 0.00001614
Iteration 161/1000 | Loss: 0.00001614
Iteration 162/1000 | Loss: 0.00001614
Iteration 163/1000 | Loss: 0.00001614
Iteration 164/1000 | Loss: 0.00001614
Iteration 165/1000 | Loss: 0.00001614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.6138463251991197e-05, 1.6138463251991197e-05, 1.6138463251991197e-05, 1.6138463251991197e-05, 1.6138463251991197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6138463251991197e-05

Optimization complete. Final v2v error: 3.3910305500030518 mm

Highest mean error: 4.683370590209961 mm for frame 46

Lowest mean error: 3.047159433364868 mm for frame 86

Saving results

Total time: 44.33667802810669
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967683
Iteration 2/25 | Loss: 0.00250392
Iteration 3/25 | Loss: 0.00213417
Iteration 4/25 | Loss: 0.00198275
Iteration 5/25 | Loss: 0.00238744
Iteration 6/25 | Loss: 0.00167028
Iteration 7/25 | Loss: 0.00148325
Iteration 8/25 | Loss: 0.00145798
Iteration 9/25 | Loss: 0.00145208
Iteration 10/25 | Loss: 0.00144848
Iteration 11/25 | Loss: 0.00144701
Iteration 12/25 | Loss: 0.00144491
Iteration 13/25 | Loss: 0.00144506
Iteration 14/25 | Loss: 0.00144581
Iteration 15/25 | Loss: 0.00144598
Iteration 16/25 | Loss: 0.00144537
Iteration 17/25 | Loss: 0.00144436
Iteration 18/25 | Loss: 0.00144522
Iteration 19/25 | Loss: 0.00144180
Iteration 20/25 | Loss: 0.00144441
Iteration 21/25 | Loss: 0.00144559
Iteration 22/25 | Loss: 0.00144499
Iteration 23/25 | Loss: 0.00144596
Iteration 24/25 | Loss: 0.00144466
Iteration 25/25 | Loss: 0.00144558

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39288986
Iteration 2/25 | Loss: 0.00102896
Iteration 3/25 | Loss: 0.00102895
Iteration 4/25 | Loss: 0.00102895
Iteration 5/25 | Loss: 0.00102895
Iteration 6/25 | Loss: 0.00102895
Iteration 7/25 | Loss: 0.00102895
Iteration 8/25 | Loss: 0.00102895
Iteration 9/25 | Loss: 0.00102895
Iteration 10/25 | Loss: 0.00102895
Iteration 11/25 | Loss: 0.00102895
Iteration 12/25 | Loss: 0.00102895
Iteration 13/25 | Loss: 0.00102895
Iteration 14/25 | Loss: 0.00102895
Iteration 15/25 | Loss: 0.00102895
Iteration 16/25 | Loss: 0.00102895
Iteration 17/25 | Loss: 0.00102895
Iteration 18/25 | Loss: 0.00102895
Iteration 19/25 | Loss: 0.00102895
Iteration 20/25 | Loss: 0.00102895
Iteration 21/25 | Loss: 0.00102895
Iteration 22/25 | Loss: 0.00102895
Iteration 23/25 | Loss: 0.00102895
Iteration 24/25 | Loss: 0.00102895
Iteration 25/25 | Loss: 0.00102895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102895
Iteration 2/1000 | Loss: 0.00018704
Iteration 3/1000 | Loss: 0.00012115
Iteration 4/1000 | Loss: 0.00007048
Iteration 5/1000 | Loss: 0.00009569
Iteration 6/1000 | Loss: 0.00006038
Iteration 7/1000 | Loss: 0.00009974
Iteration 8/1000 | Loss: 0.00009438
Iteration 9/1000 | Loss: 0.00009161
Iteration 10/1000 | Loss: 0.00009038
Iteration 11/1000 | Loss: 0.00009980
Iteration 12/1000 | Loss: 0.00005772
Iteration 13/1000 | Loss: 0.00008660
Iteration 14/1000 | Loss: 0.00008957
Iteration 15/1000 | Loss: 0.00013413
Iteration 16/1000 | Loss: 0.00008419
Iteration 17/1000 | Loss: 0.00008825
Iteration 18/1000 | Loss: 0.00007418
Iteration 19/1000 | Loss: 0.00009003
Iteration 20/1000 | Loss: 0.00008873
Iteration 21/1000 | Loss: 0.00008475
Iteration 22/1000 | Loss: 0.00009232
Iteration 23/1000 | Loss: 0.00008430
Iteration 24/1000 | Loss: 0.00009236
Iteration 25/1000 | Loss: 0.00007888
Iteration 26/1000 | Loss: 0.00010349
Iteration 27/1000 | Loss: 0.00008922
Iteration 28/1000 | Loss: 0.00014380
Iteration 29/1000 | Loss: 0.00006778
Iteration 30/1000 | Loss: 0.00004896
Iteration 31/1000 | Loss: 0.00004501
Iteration 32/1000 | Loss: 0.00004366
Iteration 33/1000 | Loss: 0.00004268
Iteration 34/1000 | Loss: 0.00029304
Iteration 35/1000 | Loss: 0.00028377
Iteration 36/1000 | Loss: 0.00005723
Iteration 37/1000 | Loss: 0.00004681
Iteration 38/1000 | Loss: 0.00004046
Iteration 39/1000 | Loss: 0.00003606
Iteration 40/1000 | Loss: 0.00003367
Iteration 41/1000 | Loss: 0.00003197
Iteration 42/1000 | Loss: 0.00003072
Iteration 43/1000 | Loss: 0.00002969
Iteration 44/1000 | Loss: 0.00002879
Iteration 45/1000 | Loss: 0.00002810
Iteration 46/1000 | Loss: 0.00002760
Iteration 47/1000 | Loss: 0.00002716
Iteration 48/1000 | Loss: 0.00002691
Iteration 49/1000 | Loss: 0.00002676
Iteration 50/1000 | Loss: 0.00002667
Iteration 51/1000 | Loss: 0.00002666
Iteration 52/1000 | Loss: 0.00002666
Iteration 53/1000 | Loss: 0.00002665
Iteration 54/1000 | Loss: 0.00002664
Iteration 55/1000 | Loss: 0.00002659
Iteration 56/1000 | Loss: 0.00002657
Iteration 57/1000 | Loss: 0.00002657
Iteration 58/1000 | Loss: 0.00002656
Iteration 59/1000 | Loss: 0.00002655
Iteration 60/1000 | Loss: 0.00002655
Iteration 61/1000 | Loss: 0.00002654
Iteration 62/1000 | Loss: 0.00002653
Iteration 63/1000 | Loss: 0.00002652
Iteration 64/1000 | Loss: 0.00002651
Iteration 65/1000 | Loss: 0.00002651
Iteration 66/1000 | Loss: 0.00002651
Iteration 67/1000 | Loss: 0.00002651
Iteration 68/1000 | Loss: 0.00002651
Iteration 69/1000 | Loss: 0.00002651
Iteration 70/1000 | Loss: 0.00002651
Iteration 71/1000 | Loss: 0.00002651
Iteration 72/1000 | Loss: 0.00002651
Iteration 73/1000 | Loss: 0.00002651
Iteration 74/1000 | Loss: 0.00002650
Iteration 75/1000 | Loss: 0.00002648
Iteration 76/1000 | Loss: 0.00002648
Iteration 77/1000 | Loss: 0.00002647
Iteration 78/1000 | Loss: 0.00002647
Iteration 79/1000 | Loss: 0.00002646
Iteration 80/1000 | Loss: 0.00002644
Iteration 81/1000 | Loss: 0.00002644
Iteration 82/1000 | Loss: 0.00002644
Iteration 83/1000 | Loss: 0.00002644
Iteration 84/1000 | Loss: 0.00002644
Iteration 85/1000 | Loss: 0.00002643
Iteration 86/1000 | Loss: 0.00002643
Iteration 87/1000 | Loss: 0.00002643
Iteration 88/1000 | Loss: 0.00002643
Iteration 89/1000 | Loss: 0.00002643
Iteration 90/1000 | Loss: 0.00002643
Iteration 91/1000 | Loss: 0.00002643
Iteration 92/1000 | Loss: 0.00002643
Iteration 93/1000 | Loss: 0.00002643
Iteration 94/1000 | Loss: 0.00002642
Iteration 95/1000 | Loss: 0.00002642
Iteration 96/1000 | Loss: 0.00002642
Iteration 97/1000 | Loss: 0.00002642
Iteration 98/1000 | Loss: 0.00002641
Iteration 99/1000 | Loss: 0.00002641
Iteration 100/1000 | Loss: 0.00002641
Iteration 101/1000 | Loss: 0.00002641
Iteration 102/1000 | Loss: 0.00002640
Iteration 103/1000 | Loss: 0.00002640
Iteration 104/1000 | Loss: 0.00002640
Iteration 105/1000 | Loss: 0.00002639
Iteration 106/1000 | Loss: 0.00002639
Iteration 107/1000 | Loss: 0.00002639
Iteration 108/1000 | Loss: 0.00002639
Iteration 109/1000 | Loss: 0.00002638
Iteration 110/1000 | Loss: 0.00002638
Iteration 111/1000 | Loss: 0.00002637
Iteration 112/1000 | Loss: 0.00002637
Iteration 113/1000 | Loss: 0.00002637
Iteration 114/1000 | Loss: 0.00002636
Iteration 115/1000 | Loss: 0.00002636
Iteration 116/1000 | Loss: 0.00002636
Iteration 117/1000 | Loss: 0.00002636
Iteration 118/1000 | Loss: 0.00002636
Iteration 119/1000 | Loss: 0.00002636
Iteration 120/1000 | Loss: 0.00002635
Iteration 121/1000 | Loss: 0.00002635
Iteration 122/1000 | Loss: 0.00002635
Iteration 123/1000 | Loss: 0.00002635
Iteration 124/1000 | Loss: 0.00002635
Iteration 125/1000 | Loss: 0.00002635
Iteration 126/1000 | Loss: 0.00002635
Iteration 127/1000 | Loss: 0.00002635
Iteration 128/1000 | Loss: 0.00002635
Iteration 129/1000 | Loss: 0.00002634
Iteration 130/1000 | Loss: 0.00002634
Iteration 131/1000 | Loss: 0.00002634
Iteration 132/1000 | Loss: 0.00002634
Iteration 133/1000 | Loss: 0.00002634
Iteration 134/1000 | Loss: 0.00002633
Iteration 135/1000 | Loss: 0.00002633
Iteration 136/1000 | Loss: 0.00002633
Iteration 137/1000 | Loss: 0.00002633
Iteration 138/1000 | Loss: 0.00002633
Iteration 139/1000 | Loss: 0.00002633
Iteration 140/1000 | Loss: 0.00002633
Iteration 141/1000 | Loss: 0.00002633
Iteration 142/1000 | Loss: 0.00002633
Iteration 143/1000 | Loss: 0.00002633
Iteration 144/1000 | Loss: 0.00002633
Iteration 145/1000 | Loss: 0.00002633
Iteration 146/1000 | Loss: 0.00002633
Iteration 147/1000 | Loss: 0.00002633
Iteration 148/1000 | Loss: 0.00002632
Iteration 149/1000 | Loss: 0.00002632
Iteration 150/1000 | Loss: 0.00002632
Iteration 151/1000 | Loss: 0.00002632
Iteration 152/1000 | Loss: 0.00002632
Iteration 153/1000 | Loss: 0.00002632
Iteration 154/1000 | Loss: 0.00002632
Iteration 155/1000 | Loss: 0.00002632
Iteration 156/1000 | Loss: 0.00002632
Iteration 157/1000 | Loss: 0.00002632
Iteration 158/1000 | Loss: 0.00002632
Iteration 159/1000 | Loss: 0.00002632
Iteration 160/1000 | Loss: 0.00002632
Iteration 161/1000 | Loss: 0.00002632
Iteration 162/1000 | Loss: 0.00002632
Iteration 163/1000 | Loss: 0.00002632
Iteration 164/1000 | Loss: 0.00002632
Iteration 165/1000 | Loss: 0.00002632
Iteration 166/1000 | Loss: 0.00002631
Iteration 167/1000 | Loss: 0.00002631
Iteration 168/1000 | Loss: 0.00002631
Iteration 169/1000 | Loss: 0.00002631
Iteration 170/1000 | Loss: 0.00002631
Iteration 171/1000 | Loss: 0.00002631
Iteration 172/1000 | Loss: 0.00002631
Iteration 173/1000 | Loss: 0.00002631
Iteration 174/1000 | Loss: 0.00002631
Iteration 175/1000 | Loss: 0.00002631
Iteration 176/1000 | Loss: 0.00002631
Iteration 177/1000 | Loss: 0.00002631
Iteration 178/1000 | Loss: 0.00002631
Iteration 179/1000 | Loss: 0.00002631
Iteration 180/1000 | Loss: 0.00002631
Iteration 181/1000 | Loss: 0.00002631
Iteration 182/1000 | Loss: 0.00002631
Iteration 183/1000 | Loss: 0.00002631
Iteration 184/1000 | Loss: 0.00002631
Iteration 185/1000 | Loss: 0.00002631
Iteration 186/1000 | Loss: 0.00002630
Iteration 187/1000 | Loss: 0.00002630
Iteration 188/1000 | Loss: 0.00002630
Iteration 189/1000 | Loss: 0.00002630
Iteration 190/1000 | Loss: 0.00002630
Iteration 191/1000 | Loss: 0.00002630
Iteration 192/1000 | Loss: 0.00002630
Iteration 193/1000 | Loss: 0.00002630
Iteration 194/1000 | Loss: 0.00002630
Iteration 195/1000 | Loss: 0.00002630
Iteration 196/1000 | Loss: 0.00002630
Iteration 197/1000 | Loss: 0.00002630
Iteration 198/1000 | Loss: 0.00002630
Iteration 199/1000 | Loss: 0.00002630
Iteration 200/1000 | Loss: 0.00002630
Iteration 201/1000 | Loss: 0.00002630
Iteration 202/1000 | Loss: 0.00002630
Iteration 203/1000 | Loss: 0.00002630
Iteration 204/1000 | Loss: 0.00002630
Iteration 205/1000 | Loss: 0.00002630
Iteration 206/1000 | Loss: 0.00002630
Iteration 207/1000 | Loss: 0.00002630
Iteration 208/1000 | Loss: 0.00002630
Iteration 209/1000 | Loss: 0.00002630
Iteration 210/1000 | Loss: 0.00002630
Iteration 211/1000 | Loss: 0.00002630
Iteration 212/1000 | Loss: 0.00002630
Iteration 213/1000 | Loss: 0.00002630
Iteration 214/1000 | Loss: 0.00002630
Iteration 215/1000 | Loss: 0.00002630
Iteration 216/1000 | Loss: 0.00002630
Iteration 217/1000 | Loss: 0.00002630
Iteration 218/1000 | Loss: 0.00002630
Iteration 219/1000 | Loss: 0.00002630
Iteration 220/1000 | Loss: 0.00002630
Iteration 221/1000 | Loss: 0.00002630
Iteration 222/1000 | Loss: 0.00002630
Iteration 223/1000 | Loss: 0.00002630
Iteration 224/1000 | Loss: 0.00002630
Iteration 225/1000 | Loss: 0.00002630
Iteration 226/1000 | Loss: 0.00002630
Iteration 227/1000 | Loss: 0.00002630
Iteration 228/1000 | Loss: 0.00002630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [2.630236849654466e-05, 2.630236849654466e-05, 2.630236849654466e-05, 2.630236849654466e-05, 2.630236849654466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.630236849654466e-05

Optimization complete. Final v2v error: 4.349922180175781 mm

Highest mean error: 4.589704990386963 mm for frame 57

Lowest mean error: 4.112986087799072 mm for frame 0

Saving results

Total time: 124.82596850395203
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783493
Iteration 2/25 | Loss: 0.00146686
Iteration 3/25 | Loss: 0.00133879
Iteration 4/25 | Loss: 0.00132256
Iteration 5/25 | Loss: 0.00131521
Iteration 6/25 | Loss: 0.00131471
Iteration 7/25 | Loss: 0.00131471
Iteration 8/25 | Loss: 0.00131471
Iteration 9/25 | Loss: 0.00131471
Iteration 10/25 | Loss: 0.00131471
Iteration 11/25 | Loss: 0.00131471
Iteration 12/25 | Loss: 0.00131471
Iteration 13/25 | Loss: 0.00131471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013147067511454225, 0.0013147067511454225, 0.0013147067511454225, 0.0013147067511454225, 0.0013147067511454225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013147067511454225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78012800
Iteration 2/25 | Loss: 0.00068194
Iteration 3/25 | Loss: 0.00068194
Iteration 4/25 | Loss: 0.00068194
Iteration 5/25 | Loss: 0.00068194
Iteration 6/25 | Loss: 0.00068194
Iteration 7/25 | Loss: 0.00068194
Iteration 8/25 | Loss: 0.00068194
Iteration 9/25 | Loss: 0.00068194
Iteration 10/25 | Loss: 0.00068194
Iteration 11/25 | Loss: 0.00068194
Iteration 12/25 | Loss: 0.00068194
Iteration 13/25 | Loss: 0.00068194
Iteration 14/25 | Loss: 0.00068194
Iteration 15/25 | Loss: 0.00068194
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006819382542744279, 0.0006819382542744279, 0.0006819382542744279, 0.0006819382542744279, 0.0006819382542744279]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006819382542744279

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068194
Iteration 2/1000 | Loss: 0.00004383
Iteration 3/1000 | Loss: 0.00003125
Iteration 4/1000 | Loss: 0.00002722
Iteration 5/1000 | Loss: 0.00002566
Iteration 6/1000 | Loss: 0.00002419
Iteration 7/1000 | Loss: 0.00002354
Iteration 8/1000 | Loss: 0.00002305
Iteration 9/1000 | Loss: 0.00002267
Iteration 10/1000 | Loss: 0.00002230
Iteration 11/1000 | Loss: 0.00002193
Iteration 12/1000 | Loss: 0.00002171
Iteration 13/1000 | Loss: 0.00002167
Iteration 14/1000 | Loss: 0.00002157
Iteration 15/1000 | Loss: 0.00002143
Iteration 16/1000 | Loss: 0.00002135
Iteration 17/1000 | Loss: 0.00002125
Iteration 18/1000 | Loss: 0.00002123
Iteration 19/1000 | Loss: 0.00002123
Iteration 20/1000 | Loss: 0.00002122
Iteration 21/1000 | Loss: 0.00002122
Iteration 22/1000 | Loss: 0.00002121
Iteration 23/1000 | Loss: 0.00002121
Iteration 24/1000 | Loss: 0.00002120
Iteration 25/1000 | Loss: 0.00002120
Iteration 26/1000 | Loss: 0.00002119
Iteration 27/1000 | Loss: 0.00002119
Iteration 28/1000 | Loss: 0.00002119
Iteration 29/1000 | Loss: 0.00002119
Iteration 30/1000 | Loss: 0.00002118
Iteration 31/1000 | Loss: 0.00002118
Iteration 32/1000 | Loss: 0.00002118
Iteration 33/1000 | Loss: 0.00002118
Iteration 34/1000 | Loss: 0.00002117
Iteration 35/1000 | Loss: 0.00002117
Iteration 36/1000 | Loss: 0.00002117
Iteration 37/1000 | Loss: 0.00002117
Iteration 38/1000 | Loss: 0.00002117
Iteration 39/1000 | Loss: 0.00002117
Iteration 40/1000 | Loss: 0.00002117
Iteration 41/1000 | Loss: 0.00002116
Iteration 42/1000 | Loss: 0.00002116
Iteration 43/1000 | Loss: 0.00002115
Iteration 44/1000 | Loss: 0.00002115
Iteration 45/1000 | Loss: 0.00002115
Iteration 46/1000 | Loss: 0.00002115
Iteration 47/1000 | Loss: 0.00002114
Iteration 48/1000 | Loss: 0.00002114
Iteration 49/1000 | Loss: 0.00002114
Iteration 50/1000 | Loss: 0.00002114
Iteration 51/1000 | Loss: 0.00002114
Iteration 52/1000 | Loss: 0.00002114
Iteration 53/1000 | Loss: 0.00002114
Iteration 54/1000 | Loss: 0.00002114
Iteration 55/1000 | Loss: 0.00002114
Iteration 56/1000 | Loss: 0.00002114
Iteration 57/1000 | Loss: 0.00002113
Iteration 58/1000 | Loss: 0.00002113
Iteration 59/1000 | Loss: 0.00002113
Iteration 60/1000 | Loss: 0.00002113
Iteration 61/1000 | Loss: 0.00002113
Iteration 62/1000 | Loss: 0.00002113
Iteration 63/1000 | Loss: 0.00002113
Iteration 64/1000 | Loss: 0.00002113
Iteration 65/1000 | Loss: 0.00002112
Iteration 66/1000 | Loss: 0.00002112
Iteration 67/1000 | Loss: 0.00002112
Iteration 68/1000 | Loss: 0.00002112
Iteration 69/1000 | Loss: 0.00002112
Iteration 70/1000 | Loss: 0.00002111
Iteration 71/1000 | Loss: 0.00002111
Iteration 72/1000 | Loss: 0.00002111
Iteration 73/1000 | Loss: 0.00002111
Iteration 74/1000 | Loss: 0.00002111
Iteration 75/1000 | Loss: 0.00002111
Iteration 76/1000 | Loss: 0.00002111
Iteration 77/1000 | Loss: 0.00002111
Iteration 78/1000 | Loss: 0.00002111
Iteration 79/1000 | Loss: 0.00002110
Iteration 80/1000 | Loss: 0.00002110
Iteration 81/1000 | Loss: 0.00002110
Iteration 82/1000 | Loss: 0.00002109
Iteration 83/1000 | Loss: 0.00002109
Iteration 84/1000 | Loss: 0.00002109
Iteration 85/1000 | Loss: 0.00002109
Iteration 86/1000 | Loss: 0.00002109
Iteration 87/1000 | Loss: 0.00002108
Iteration 88/1000 | Loss: 0.00002108
Iteration 89/1000 | Loss: 0.00002108
Iteration 90/1000 | Loss: 0.00002108
Iteration 91/1000 | Loss: 0.00002108
Iteration 92/1000 | Loss: 0.00002108
Iteration 93/1000 | Loss: 0.00002108
Iteration 94/1000 | Loss: 0.00002108
Iteration 95/1000 | Loss: 0.00002107
Iteration 96/1000 | Loss: 0.00002107
Iteration 97/1000 | Loss: 0.00002107
Iteration 98/1000 | Loss: 0.00002107
Iteration 99/1000 | Loss: 0.00002107
Iteration 100/1000 | Loss: 0.00002106
Iteration 101/1000 | Loss: 0.00002106
Iteration 102/1000 | Loss: 0.00002106
Iteration 103/1000 | Loss: 0.00002106
Iteration 104/1000 | Loss: 0.00002106
Iteration 105/1000 | Loss: 0.00002106
Iteration 106/1000 | Loss: 0.00002106
Iteration 107/1000 | Loss: 0.00002106
Iteration 108/1000 | Loss: 0.00002106
Iteration 109/1000 | Loss: 0.00002106
Iteration 110/1000 | Loss: 0.00002106
Iteration 111/1000 | Loss: 0.00002106
Iteration 112/1000 | Loss: 0.00002105
Iteration 113/1000 | Loss: 0.00002105
Iteration 114/1000 | Loss: 0.00002105
Iteration 115/1000 | Loss: 0.00002105
Iteration 116/1000 | Loss: 0.00002105
Iteration 117/1000 | Loss: 0.00002104
Iteration 118/1000 | Loss: 0.00002104
Iteration 119/1000 | Loss: 0.00002104
Iteration 120/1000 | Loss: 0.00002104
Iteration 121/1000 | Loss: 0.00002104
Iteration 122/1000 | Loss: 0.00002104
Iteration 123/1000 | Loss: 0.00002104
Iteration 124/1000 | Loss: 0.00002104
Iteration 125/1000 | Loss: 0.00002104
Iteration 126/1000 | Loss: 0.00002103
Iteration 127/1000 | Loss: 0.00002103
Iteration 128/1000 | Loss: 0.00002103
Iteration 129/1000 | Loss: 0.00002103
Iteration 130/1000 | Loss: 0.00002103
Iteration 131/1000 | Loss: 0.00002103
Iteration 132/1000 | Loss: 0.00002103
Iteration 133/1000 | Loss: 0.00002103
Iteration 134/1000 | Loss: 0.00002103
Iteration 135/1000 | Loss: 0.00002103
Iteration 136/1000 | Loss: 0.00002102
Iteration 137/1000 | Loss: 0.00002102
Iteration 138/1000 | Loss: 0.00002102
Iteration 139/1000 | Loss: 0.00002102
Iteration 140/1000 | Loss: 0.00002102
Iteration 141/1000 | Loss: 0.00002102
Iteration 142/1000 | Loss: 0.00002102
Iteration 143/1000 | Loss: 0.00002102
Iteration 144/1000 | Loss: 0.00002102
Iteration 145/1000 | Loss: 0.00002102
Iteration 146/1000 | Loss: 0.00002102
Iteration 147/1000 | Loss: 0.00002102
Iteration 148/1000 | Loss: 0.00002102
Iteration 149/1000 | Loss: 0.00002102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.102095641021151e-05, 2.102095641021151e-05, 2.102095641021151e-05, 2.102095641021151e-05, 2.102095641021151e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.102095641021151e-05

Optimization complete. Final v2v error: 3.905897855758667 mm

Highest mean error: 4.262922286987305 mm for frame 130

Lowest mean error: 3.6078174114227295 mm for frame 73

Saving results

Total time: 44.53148531913757
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419570
Iteration 2/25 | Loss: 0.00134199
Iteration 3/25 | Loss: 0.00128885
Iteration 4/25 | Loss: 0.00127890
Iteration 5/25 | Loss: 0.00127701
Iteration 6/25 | Loss: 0.00127664
Iteration 7/25 | Loss: 0.00127664
Iteration 8/25 | Loss: 0.00127664
Iteration 9/25 | Loss: 0.00127664
Iteration 10/25 | Loss: 0.00127664
Iteration 11/25 | Loss: 0.00127664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012766439467668533, 0.0012766439467668533, 0.0012766439467668533, 0.0012766439467668533, 0.0012766439467668533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012766439467668533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46863842
Iteration 2/25 | Loss: 0.00084788
Iteration 3/25 | Loss: 0.00084788
Iteration 4/25 | Loss: 0.00084788
Iteration 5/25 | Loss: 0.00084788
Iteration 6/25 | Loss: 0.00084788
Iteration 7/25 | Loss: 0.00084788
Iteration 8/25 | Loss: 0.00084788
Iteration 9/25 | Loss: 0.00084788
Iteration 10/25 | Loss: 0.00084788
Iteration 11/25 | Loss: 0.00084788
Iteration 12/25 | Loss: 0.00084788
Iteration 13/25 | Loss: 0.00084788
Iteration 14/25 | Loss: 0.00084788
Iteration 15/25 | Loss: 0.00084788
Iteration 16/25 | Loss: 0.00084788
Iteration 17/25 | Loss: 0.00084788
Iteration 18/25 | Loss: 0.00084788
Iteration 19/25 | Loss: 0.00084788
Iteration 20/25 | Loss: 0.00084788
Iteration 21/25 | Loss: 0.00084788
Iteration 22/25 | Loss: 0.00084788
Iteration 23/25 | Loss: 0.00084788
Iteration 24/25 | Loss: 0.00084788
Iteration 25/25 | Loss: 0.00084788

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084788
Iteration 2/1000 | Loss: 0.00002552
Iteration 3/1000 | Loss: 0.00001853
Iteration 4/1000 | Loss: 0.00001728
Iteration 5/1000 | Loss: 0.00001660
Iteration 6/1000 | Loss: 0.00001618
Iteration 7/1000 | Loss: 0.00001574
Iteration 8/1000 | Loss: 0.00001548
Iteration 9/1000 | Loss: 0.00001547
Iteration 10/1000 | Loss: 0.00001520
Iteration 11/1000 | Loss: 0.00001498
Iteration 12/1000 | Loss: 0.00001484
Iteration 13/1000 | Loss: 0.00001471
Iteration 14/1000 | Loss: 0.00001469
Iteration 15/1000 | Loss: 0.00001469
Iteration 16/1000 | Loss: 0.00001468
Iteration 17/1000 | Loss: 0.00001467
Iteration 18/1000 | Loss: 0.00001466
Iteration 19/1000 | Loss: 0.00001461
Iteration 20/1000 | Loss: 0.00001461
Iteration 21/1000 | Loss: 0.00001460
Iteration 22/1000 | Loss: 0.00001453
Iteration 23/1000 | Loss: 0.00001453
Iteration 24/1000 | Loss: 0.00001449
Iteration 25/1000 | Loss: 0.00001449
Iteration 26/1000 | Loss: 0.00001449
Iteration 27/1000 | Loss: 0.00001448
Iteration 28/1000 | Loss: 0.00001448
Iteration 29/1000 | Loss: 0.00001448
Iteration 30/1000 | Loss: 0.00001448
Iteration 31/1000 | Loss: 0.00001448
Iteration 32/1000 | Loss: 0.00001448
Iteration 33/1000 | Loss: 0.00001447
Iteration 34/1000 | Loss: 0.00001447
Iteration 35/1000 | Loss: 0.00001447
Iteration 36/1000 | Loss: 0.00001447
Iteration 37/1000 | Loss: 0.00001447
Iteration 38/1000 | Loss: 0.00001447
Iteration 39/1000 | Loss: 0.00001447
Iteration 40/1000 | Loss: 0.00001447
Iteration 41/1000 | Loss: 0.00001447
Iteration 42/1000 | Loss: 0.00001447
Iteration 43/1000 | Loss: 0.00001447
Iteration 44/1000 | Loss: 0.00001447
Iteration 45/1000 | Loss: 0.00001447
Iteration 46/1000 | Loss: 0.00001447
Iteration 47/1000 | Loss: 0.00001447
Iteration 48/1000 | Loss: 0.00001447
Iteration 49/1000 | Loss: 0.00001447
Iteration 50/1000 | Loss: 0.00001447
Iteration 51/1000 | Loss: 0.00001447
Iteration 52/1000 | Loss: 0.00001445
Iteration 53/1000 | Loss: 0.00001444
Iteration 54/1000 | Loss: 0.00001444
Iteration 55/1000 | Loss: 0.00001443
Iteration 56/1000 | Loss: 0.00001443
Iteration 57/1000 | Loss: 0.00001443
Iteration 58/1000 | Loss: 0.00001442
Iteration 59/1000 | Loss: 0.00001442
Iteration 60/1000 | Loss: 0.00001442
Iteration 61/1000 | Loss: 0.00001442
Iteration 62/1000 | Loss: 0.00001442
Iteration 63/1000 | Loss: 0.00001441
Iteration 64/1000 | Loss: 0.00001441
Iteration 65/1000 | Loss: 0.00001439
Iteration 66/1000 | Loss: 0.00001438
Iteration 67/1000 | Loss: 0.00001438
Iteration 68/1000 | Loss: 0.00001438
Iteration 69/1000 | Loss: 0.00001438
Iteration 70/1000 | Loss: 0.00001438
Iteration 71/1000 | Loss: 0.00001437
Iteration 72/1000 | Loss: 0.00001437
Iteration 73/1000 | Loss: 0.00001436
Iteration 74/1000 | Loss: 0.00001432
Iteration 75/1000 | Loss: 0.00001432
Iteration 76/1000 | Loss: 0.00001429
Iteration 77/1000 | Loss: 0.00001429
Iteration 78/1000 | Loss: 0.00001428
Iteration 79/1000 | Loss: 0.00001428
Iteration 80/1000 | Loss: 0.00001427
Iteration 81/1000 | Loss: 0.00001427
Iteration 82/1000 | Loss: 0.00001427
Iteration 83/1000 | Loss: 0.00001427
Iteration 84/1000 | Loss: 0.00001427
Iteration 85/1000 | Loss: 0.00001427
Iteration 86/1000 | Loss: 0.00001427
Iteration 87/1000 | Loss: 0.00001427
Iteration 88/1000 | Loss: 0.00001426
Iteration 89/1000 | Loss: 0.00001426
Iteration 90/1000 | Loss: 0.00001426
Iteration 91/1000 | Loss: 0.00001426
Iteration 92/1000 | Loss: 0.00001425
Iteration 93/1000 | Loss: 0.00001425
Iteration 94/1000 | Loss: 0.00001425
Iteration 95/1000 | Loss: 0.00001424
Iteration 96/1000 | Loss: 0.00001424
Iteration 97/1000 | Loss: 0.00001423
Iteration 98/1000 | Loss: 0.00001423
Iteration 99/1000 | Loss: 0.00001423
Iteration 100/1000 | Loss: 0.00001422
Iteration 101/1000 | Loss: 0.00001422
Iteration 102/1000 | Loss: 0.00001422
Iteration 103/1000 | Loss: 0.00001422
Iteration 104/1000 | Loss: 0.00001422
Iteration 105/1000 | Loss: 0.00001422
Iteration 106/1000 | Loss: 0.00001422
Iteration 107/1000 | Loss: 0.00001422
Iteration 108/1000 | Loss: 0.00001422
Iteration 109/1000 | Loss: 0.00001422
Iteration 110/1000 | Loss: 0.00001422
Iteration 111/1000 | Loss: 0.00001422
Iteration 112/1000 | Loss: 0.00001422
Iteration 113/1000 | Loss: 0.00001422
Iteration 114/1000 | Loss: 0.00001422
Iteration 115/1000 | Loss: 0.00001422
Iteration 116/1000 | Loss: 0.00001422
Iteration 117/1000 | Loss: 0.00001422
Iteration 118/1000 | Loss: 0.00001422
Iteration 119/1000 | Loss: 0.00001422
Iteration 120/1000 | Loss: 0.00001422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.4217835087038111e-05, 1.4217835087038111e-05, 1.4217835087038111e-05, 1.4217835087038111e-05, 1.4217835087038111e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4217835087038111e-05

Optimization complete. Final v2v error: 3.2325551509857178 mm

Highest mean error: 3.609328269958496 mm for frame 80

Lowest mean error: 3.0551745891571045 mm for frame 103

Saving results

Total time: 35.659682750701904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403241
Iteration 2/25 | Loss: 0.00131608
Iteration 3/25 | Loss: 0.00125652
Iteration 4/25 | Loss: 0.00124717
Iteration 5/25 | Loss: 0.00124497
Iteration 6/25 | Loss: 0.00124497
Iteration 7/25 | Loss: 0.00124497
Iteration 8/25 | Loss: 0.00124497
Iteration 9/25 | Loss: 0.00124497
Iteration 10/25 | Loss: 0.00124497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012449725763872266, 0.0012449725763872266, 0.0012449725763872266, 0.0012449725763872266, 0.0012449725763872266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012449725763872266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.31835365
Iteration 2/25 | Loss: 0.00083988
Iteration 3/25 | Loss: 0.00083988
Iteration 4/25 | Loss: 0.00083988
Iteration 5/25 | Loss: 0.00083988
Iteration 6/25 | Loss: 0.00083988
Iteration 7/25 | Loss: 0.00083988
Iteration 8/25 | Loss: 0.00083988
Iteration 9/25 | Loss: 0.00083988
Iteration 10/25 | Loss: 0.00083988
Iteration 11/25 | Loss: 0.00083988
Iteration 12/25 | Loss: 0.00083988
Iteration 13/25 | Loss: 0.00083988
Iteration 14/25 | Loss: 0.00083988
Iteration 15/25 | Loss: 0.00083988
Iteration 16/25 | Loss: 0.00083988
Iteration 17/25 | Loss: 0.00083988
Iteration 18/25 | Loss: 0.00083988
Iteration 19/25 | Loss: 0.00083988
Iteration 20/25 | Loss: 0.00083988
Iteration 21/25 | Loss: 0.00083988
Iteration 22/25 | Loss: 0.00083988
Iteration 23/25 | Loss: 0.00083988
Iteration 24/25 | Loss: 0.00083988
Iteration 25/25 | Loss: 0.00083988

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083988
Iteration 2/1000 | Loss: 0.00002148
Iteration 3/1000 | Loss: 0.00001707
Iteration 4/1000 | Loss: 0.00001564
Iteration 5/1000 | Loss: 0.00001458
Iteration 6/1000 | Loss: 0.00001380
Iteration 7/1000 | Loss: 0.00001346
Iteration 8/1000 | Loss: 0.00001311
Iteration 9/1000 | Loss: 0.00001276
Iteration 10/1000 | Loss: 0.00001264
Iteration 11/1000 | Loss: 0.00001260
Iteration 12/1000 | Loss: 0.00001259
Iteration 13/1000 | Loss: 0.00001259
Iteration 14/1000 | Loss: 0.00001236
Iteration 15/1000 | Loss: 0.00001234
Iteration 16/1000 | Loss: 0.00001232
Iteration 17/1000 | Loss: 0.00001225
Iteration 18/1000 | Loss: 0.00001211
Iteration 19/1000 | Loss: 0.00001209
Iteration 20/1000 | Loss: 0.00001208
Iteration 21/1000 | Loss: 0.00001204
Iteration 22/1000 | Loss: 0.00001203
Iteration 23/1000 | Loss: 0.00001201
Iteration 24/1000 | Loss: 0.00001195
Iteration 25/1000 | Loss: 0.00001195
Iteration 26/1000 | Loss: 0.00001192
Iteration 27/1000 | Loss: 0.00001185
Iteration 28/1000 | Loss: 0.00001183
Iteration 29/1000 | Loss: 0.00001172
Iteration 30/1000 | Loss: 0.00001170
Iteration 31/1000 | Loss: 0.00001169
Iteration 32/1000 | Loss: 0.00001168
Iteration 33/1000 | Loss: 0.00001168
Iteration 34/1000 | Loss: 0.00001167
Iteration 35/1000 | Loss: 0.00001167
Iteration 36/1000 | Loss: 0.00001167
Iteration 37/1000 | Loss: 0.00001166
Iteration 38/1000 | Loss: 0.00001165
Iteration 39/1000 | Loss: 0.00001164
Iteration 40/1000 | Loss: 0.00001163
Iteration 41/1000 | Loss: 0.00001162
Iteration 42/1000 | Loss: 0.00001162
Iteration 43/1000 | Loss: 0.00001162
Iteration 44/1000 | Loss: 0.00001161
Iteration 45/1000 | Loss: 0.00001161
Iteration 46/1000 | Loss: 0.00001161
Iteration 47/1000 | Loss: 0.00001160
Iteration 48/1000 | Loss: 0.00001160
Iteration 49/1000 | Loss: 0.00001160
Iteration 50/1000 | Loss: 0.00001159
Iteration 51/1000 | Loss: 0.00001159
Iteration 52/1000 | Loss: 0.00001156
Iteration 53/1000 | Loss: 0.00001156
Iteration 54/1000 | Loss: 0.00001153
Iteration 55/1000 | Loss: 0.00001151
Iteration 56/1000 | Loss: 0.00001151
Iteration 57/1000 | Loss: 0.00001150
Iteration 58/1000 | Loss: 0.00001150
Iteration 59/1000 | Loss: 0.00001150
Iteration 60/1000 | Loss: 0.00001150
Iteration 61/1000 | Loss: 0.00001150
Iteration 62/1000 | Loss: 0.00001150
Iteration 63/1000 | Loss: 0.00001150
Iteration 64/1000 | Loss: 0.00001150
Iteration 65/1000 | Loss: 0.00001150
Iteration 66/1000 | Loss: 0.00001150
Iteration 67/1000 | Loss: 0.00001149
Iteration 68/1000 | Loss: 0.00001147
Iteration 69/1000 | Loss: 0.00001147
Iteration 70/1000 | Loss: 0.00001147
Iteration 71/1000 | Loss: 0.00001147
Iteration 72/1000 | Loss: 0.00001147
Iteration 73/1000 | Loss: 0.00001146
Iteration 74/1000 | Loss: 0.00001146
Iteration 75/1000 | Loss: 0.00001146
Iteration 76/1000 | Loss: 0.00001146
Iteration 77/1000 | Loss: 0.00001146
Iteration 78/1000 | Loss: 0.00001146
Iteration 79/1000 | Loss: 0.00001146
Iteration 80/1000 | Loss: 0.00001145
Iteration 81/1000 | Loss: 0.00001145
Iteration 82/1000 | Loss: 0.00001144
Iteration 83/1000 | Loss: 0.00001143
Iteration 84/1000 | Loss: 0.00001143
Iteration 85/1000 | Loss: 0.00001143
Iteration 86/1000 | Loss: 0.00001143
Iteration 87/1000 | Loss: 0.00001142
Iteration 88/1000 | Loss: 0.00001142
Iteration 89/1000 | Loss: 0.00001142
Iteration 90/1000 | Loss: 0.00001142
Iteration 91/1000 | Loss: 0.00001141
Iteration 92/1000 | Loss: 0.00001141
Iteration 93/1000 | Loss: 0.00001141
Iteration 94/1000 | Loss: 0.00001141
Iteration 95/1000 | Loss: 0.00001140
Iteration 96/1000 | Loss: 0.00001140
Iteration 97/1000 | Loss: 0.00001140
Iteration 98/1000 | Loss: 0.00001140
Iteration 99/1000 | Loss: 0.00001140
Iteration 100/1000 | Loss: 0.00001139
Iteration 101/1000 | Loss: 0.00001139
Iteration 102/1000 | Loss: 0.00001139
Iteration 103/1000 | Loss: 0.00001139
Iteration 104/1000 | Loss: 0.00001138
Iteration 105/1000 | Loss: 0.00001138
Iteration 106/1000 | Loss: 0.00001138
Iteration 107/1000 | Loss: 0.00001137
Iteration 108/1000 | Loss: 0.00001137
Iteration 109/1000 | Loss: 0.00001137
Iteration 110/1000 | Loss: 0.00001136
Iteration 111/1000 | Loss: 0.00001136
Iteration 112/1000 | Loss: 0.00001136
Iteration 113/1000 | Loss: 0.00001136
Iteration 114/1000 | Loss: 0.00001135
Iteration 115/1000 | Loss: 0.00001135
Iteration 116/1000 | Loss: 0.00001135
Iteration 117/1000 | Loss: 0.00001135
Iteration 118/1000 | Loss: 0.00001134
Iteration 119/1000 | Loss: 0.00001134
Iteration 120/1000 | Loss: 0.00001134
Iteration 121/1000 | Loss: 0.00001134
Iteration 122/1000 | Loss: 0.00001134
Iteration 123/1000 | Loss: 0.00001134
Iteration 124/1000 | Loss: 0.00001134
Iteration 125/1000 | Loss: 0.00001133
Iteration 126/1000 | Loss: 0.00001133
Iteration 127/1000 | Loss: 0.00001133
Iteration 128/1000 | Loss: 0.00001133
Iteration 129/1000 | Loss: 0.00001133
Iteration 130/1000 | Loss: 0.00001133
Iteration 131/1000 | Loss: 0.00001133
Iteration 132/1000 | Loss: 0.00001133
Iteration 133/1000 | Loss: 0.00001133
Iteration 134/1000 | Loss: 0.00001132
Iteration 135/1000 | Loss: 0.00001132
Iteration 136/1000 | Loss: 0.00001132
Iteration 137/1000 | Loss: 0.00001132
Iteration 138/1000 | Loss: 0.00001132
Iteration 139/1000 | Loss: 0.00001132
Iteration 140/1000 | Loss: 0.00001132
Iteration 141/1000 | Loss: 0.00001132
Iteration 142/1000 | Loss: 0.00001132
Iteration 143/1000 | Loss: 0.00001132
Iteration 144/1000 | Loss: 0.00001132
Iteration 145/1000 | Loss: 0.00001132
Iteration 146/1000 | Loss: 0.00001132
Iteration 147/1000 | Loss: 0.00001132
Iteration 148/1000 | Loss: 0.00001132
Iteration 149/1000 | Loss: 0.00001132
Iteration 150/1000 | Loss: 0.00001132
Iteration 151/1000 | Loss: 0.00001131
Iteration 152/1000 | Loss: 0.00001131
Iteration 153/1000 | Loss: 0.00001131
Iteration 154/1000 | Loss: 0.00001131
Iteration 155/1000 | Loss: 0.00001131
Iteration 156/1000 | Loss: 0.00001131
Iteration 157/1000 | Loss: 0.00001131
Iteration 158/1000 | Loss: 0.00001131
Iteration 159/1000 | Loss: 0.00001131
Iteration 160/1000 | Loss: 0.00001131
Iteration 161/1000 | Loss: 0.00001131
Iteration 162/1000 | Loss: 0.00001131
Iteration 163/1000 | Loss: 0.00001131
Iteration 164/1000 | Loss: 0.00001131
Iteration 165/1000 | Loss: 0.00001131
Iteration 166/1000 | Loss: 0.00001131
Iteration 167/1000 | Loss: 0.00001131
Iteration 168/1000 | Loss: 0.00001131
Iteration 169/1000 | Loss: 0.00001131
Iteration 170/1000 | Loss: 0.00001130
Iteration 171/1000 | Loss: 0.00001130
Iteration 172/1000 | Loss: 0.00001130
Iteration 173/1000 | Loss: 0.00001130
Iteration 174/1000 | Loss: 0.00001130
Iteration 175/1000 | Loss: 0.00001130
Iteration 176/1000 | Loss: 0.00001130
Iteration 177/1000 | Loss: 0.00001130
Iteration 178/1000 | Loss: 0.00001130
Iteration 179/1000 | Loss: 0.00001130
Iteration 180/1000 | Loss: 0.00001130
Iteration 181/1000 | Loss: 0.00001130
Iteration 182/1000 | Loss: 0.00001130
Iteration 183/1000 | Loss: 0.00001130
Iteration 184/1000 | Loss: 0.00001130
Iteration 185/1000 | Loss: 0.00001130
Iteration 186/1000 | Loss: 0.00001130
Iteration 187/1000 | Loss: 0.00001129
Iteration 188/1000 | Loss: 0.00001129
Iteration 189/1000 | Loss: 0.00001129
Iteration 190/1000 | Loss: 0.00001129
Iteration 191/1000 | Loss: 0.00001129
Iteration 192/1000 | Loss: 0.00001129
Iteration 193/1000 | Loss: 0.00001129
Iteration 194/1000 | Loss: 0.00001128
Iteration 195/1000 | Loss: 0.00001128
Iteration 196/1000 | Loss: 0.00001128
Iteration 197/1000 | Loss: 0.00001128
Iteration 198/1000 | Loss: 0.00001128
Iteration 199/1000 | Loss: 0.00001128
Iteration 200/1000 | Loss: 0.00001128
Iteration 201/1000 | Loss: 0.00001128
Iteration 202/1000 | Loss: 0.00001128
Iteration 203/1000 | Loss: 0.00001128
Iteration 204/1000 | Loss: 0.00001128
Iteration 205/1000 | Loss: 0.00001128
Iteration 206/1000 | Loss: 0.00001128
Iteration 207/1000 | Loss: 0.00001128
Iteration 208/1000 | Loss: 0.00001128
Iteration 209/1000 | Loss: 0.00001128
Iteration 210/1000 | Loss: 0.00001128
Iteration 211/1000 | Loss: 0.00001128
Iteration 212/1000 | Loss: 0.00001128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.1281132174190134e-05, 1.1281132174190134e-05, 1.1281132174190134e-05, 1.1281132174190134e-05, 1.1281132174190134e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1281132174190134e-05

Optimization complete. Final v2v error: 2.914982557296753 mm

Highest mean error: 3.062493085861206 mm for frame 126

Lowest mean error: 2.8307485580444336 mm for frame 161

Saving results

Total time: 48.366352558135986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040338
Iteration 2/25 | Loss: 0.01040338
Iteration 3/25 | Loss: 0.01040338
Iteration 4/25 | Loss: 0.01040338
Iteration 5/25 | Loss: 0.01040338
Iteration 6/25 | Loss: 0.01040338
Iteration 7/25 | Loss: 0.01040337
Iteration 8/25 | Loss: 0.01040337
Iteration 9/25 | Loss: 0.01040337
Iteration 10/25 | Loss: 0.01040337
Iteration 11/25 | Loss: 0.01040337
Iteration 12/25 | Loss: 0.01040337
Iteration 13/25 | Loss: 0.01040337
Iteration 14/25 | Loss: 0.01040337
Iteration 15/25 | Loss: 0.01040337
Iteration 16/25 | Loss: 0.01040337
Iteration 17/25 | Loss: 0.01040337
Iteration 18/25 | Loss: 0.01040336
Iteration 19/25 | Loss: 0.01040336
Iteration 20/25 | Loss: 0.01040336
Iteration 21/25 | Loss: 0.01040336
Iteration 22/25 | Loss: 0.01040336
Iteration 23/25 | Loss: 0.01040336
Iteration 24/25 | Loss: 0.01040336
Iteration 25/25 | Loss: 0.01040336

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86194766
Iteration 2/25 | Loss: 0.06748611
Iteration 3/25 | Loss: 0.06747813
Iteration 4/25 | Loss: 0.06747811
Iteration 5/25 | Loss: 0.06747811
Iteration 6/25 | Loss: 0.06747810
Iteration 7/25 | Loss: 0.06747810
Iteration 8/25 | Loss: 0.06747810
Iteration 9/25 | Loss: 0.06747810
Iteration 10/25 | Loss: 0.06747810
Iteration 11/25 | Loss: 0.06747810
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.06747809797525406, 0.06747809797525406, 0.06747809797525406, 0.06747809797525406, 0.06747809797525406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06747809797525406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06747809
Iteration 2/1000 | Loss: 0.00568035
Iteration 3/1000 | Loss: 0.00156998
Iteration 4/1000 | Loss: 0.00059408
Iteration 5/1000 | Loss: 0.01634525
Iteration 6/1000 | Loss: 0.00352646
Iteration 7/1000 | Loss: 0.00274870
Iteration 8/1000 | Loss: 0.00456412
Iteration 9/1000 | Loss: 0.00059549
Iteration 10/1000 | Loss: 0.00029127
Iteration 11/1000 | Loss: 0.00015341
Iteration 12/1000 | Loss: 0.00009861
Iteration 13/1000 | Loss: 0.00044455
Iteration 14/1000 | Loss: 0.00008991
Iteration 15/1000 | Loss: 0.00005988
Iteration 16/1000 | Loss: 0.00196423
Iteration 17/1000 | Loss: 0.00284144
Iteration 18/1000 | Loss: 0.00009777
Iteration 19/1000 | Loss: 0.00039233
Iteration 20/1000 | Loss: 0.00023200
Iteration 21/1000 | Loss: 0.00004887
Iteration 22/1000 | Loss: 0.00083525
Iteration 23/1000 | Loss: 0.00188367
Iteration 24/1000 | Loss: 0.00110758
Iteration 25/1000 | Loss: 0.00004072
Iteration 26/1000 | Loss: 0.00003822
Iteration 27/1000 | Loss: 0.00011501
Iteration 28/1000 | Loss: 0.00003817
Iteration 29/1000 | Loss: 0.00020945
Iteration 30/1000 | Loss: 0.00021370
Iteration 31/1000 | Loss: 0.00113811
Iteration 32/1000 | Loss: 0.00008167
Iteration 33/1000 | Loss: 0.00033462
Iteration 34/1000 | Loss: 0.00003237
Iteration 35/1000 | Loss: 0.00226472
Iteration 36/1000 | Loss: 0.00968506
Iteration 37/1000 | Loss: 0.00085201
Iteration 38/1000 | Loss: 0.00014721
Iteration 39/1000 | Loss: 0.00013083
Iteration 40/1000 | Loss: 0.00007111
Iteration 41/1000 | Loss: 0.00003394
Iteration 42/1000 | Loss: 0.00021904
Iteration 43/1000 | Loss: 0.00004598
Iteration 44/1000 | Loss: 0.00005683
Iteration 45/1000 | Loss: 0.00002911
Iteration 46/1000 | Loss: 0.00003074
Iteration 47/1000 | Loss: 0.00002816
Iteration 48/1000 | Loss: 0.00010838
Iteration 49/1000 | Loss: 0.00003081
Iteration 50/1000 | Loss: 0.00006165
Iteration 51/1000 | Loss: 0.00002705
Iteration 52/1000 | Loss: 0.00024751
Iteration 53/1000 | Loss: 0.00022347
Iteration 54/1000 | Loss: 0.00099521
Iteration 55/1000 | Loss: 0.00013907
Iteration 56/1000 | Loss: 0.00009648
Iteration 57/1000 | Loss: 0.00002573
Iteration 58/1000 | Loss: 0.00007325
Iteration 59/1000 | Loss: 0.00003504
Iteration 60/1000 | Loss: 0.00002631
Iteration 61/1000 | Loss: 0.00002538
Iteration 62/1000 | Loss: 0.00010869
Iteration 63/1000 | Loss: 0.00017727
Iteration 64/1000 | Loss: 0.00002859
Iteration 65/1000 | Loss: 0.00003066
Iteration 66/1000 | Loss: 0.00009503
Iteration 67/1000 | Loss: 0.00002432
Iteration 68/1000 | Loss: 0.00002402
Iteration 69/1000 | Loss: 0.00002363
Iteration 70/1000 | Loss: 0.00004899
Iteration 71/1000 | Loss: 0.00004902
Iteration 72/1000 | Loss: 0.00002321
Iteration 73/1000 | Loss: 0.00002309
Iteration 74/1000 | Loss: 0.00002302
Iteration 75/1000 | Loss: 0.00002300
Iteration 76/1000 | Loss: 0.00002299
Iteration 77/1000 | Loss: 0.00002298
Iteration 78/1000 | Loss: 0.00002298
Iteration 79/1000 | Loss: 0.00002298
Iteration 80/1000 | Loss: 0.00002298
Iteration 81/1000 | Loss: 0.00002297
Iteration 82/1000 | Loss: 0.00002295
Iteration 83/1000 | Loss: 0.00007460
Iteration 84/1000 | Loss: 0.00004437
Iteration 85/1000 | Loss: 0.00007877
Iteration 86/1000 | Loss: 0.00006628
Iteration 87/1000 | Loss: 0.00009848
Iteration 88/1000 | Loss: 0.00002928
Iteration 89/1000 | Loss: 0.00004683
Iteration 90/1000 | Loss: 0.00002489
Iteration 91/1000 | Loss: 0.00002493
Iteration 92/1000 | Loss: 0.00002322
Iteration 93/1000 | Loss: 0.00002282
Iteration 94/1000 | Loss: 0.00002282
Iteration 95/1000 | Loss: 0.00002282
Iteration 96/1000 | Loss: 0.00002282
Iteration 97/1000 | Loss: 0.00002282
Iteration 98/1000 | Loss: 0.00002282
Iteration 99/1000 | Loss: 0.00002282
Iteration 100/1000 | Loss: 0.00002282
Iteration 101/1000 | Loss: 0.00002281
Iteration 102/1000 | Loss: 0.00002281
Iteration 103/1000 | Loss: 0.00002281
Iteration 104/1000 | Loss: 0.00002279
Iteration 105/1000 | Loss: 0.00002279
Iteration 106/1000 | Loss: 0.00002279
Iteration 107/1000 | Loss: 0.00002279
Iteration 108/1000 | Loss: 0.00002279
Iteration 109/1000 | Loss: 0.00002279
Iteration 110/1000 | Loss: 0.00002279
Iteration 111/1000 | Loss: 0.00002278
Iteration 112/1000 | Loss: 0.00002278
Iteration 113/1000 | Loss: 0.00002278
Iteration 114/1000 | Loss: 0.00002278
Iteration 115/1000 | Loss: 0.00002278
Iteration 116/1000 | Loss: 0.00004382
Iteration 117/1000 | Loss: 0.00002285
Iteration 118/1000 | Loss: 0.00002273
Iteration 119/1000 | Loss: 0.00002272
Iteration 120/1000 | Loss: 0.00002271
Iteration 121/1000 | Loss: 0.00002270
Iteration 122/1000 | Loss: 0.00002270
Iteration 123/1000 | Loss: 0.00002270
Iteration 124/1000 | Loss: 0.00002270
Iteration 125/1000 | Loss: 0.00002269
Iteration 126/1000 | Loss: 0.00002269
Iteration 127/1000 | Loss: 0.00002269
Iteration 128/1000 | Loss: 0.00002269
Iteration 129/1000 | Loss: 0.00002269
Iteration 130/1000 | Loss: 0.00002269
Iteration 131/1000 | Loss: 0.00002268
Iteration 132/1000 | Loss: 0.00002268
Iteration 133/1000 | Loss: 0.00002268
Iteration 134/1000 | Loss: 0.00002267
Iteration 135/1000 | Loss: 0.00003078
Iteration 136/1000 | Loss: 0.00002294
Iteration 137/1000 | Loss: 0.00002267
Iteration 138/1000 | Loss: 0.00002267
Iteration 139/1000 | Loss: 0.00002267
Iteration 140/1000 | Loss: 0.00002267
Iteration 141/1000 | Loss: 0.00002267
Iteration 142/1000 | Loss: 0.00002267
Iteration 143/1000 | Loss: 0.00002267
Iteration 144/1000 | Loss: 0.00002267
Iteration 145/1000 | Loss: 0.00002267
Iteration 146/1000 | Loss: 0.00002267
Iteration 147/1000 | Loss: 0.00002267
Iteration 148/1000 | Loss: 0.00002267
Iteration 149/1000 | Loss: 0.00002267
Iteration 150/1000 | Loss: 0.00002267
Iteration 151/1000 | Loss: 0.00002267
Iteration 152/1000 | Loss: 0.00002267
Iteration 153/1000 | Loss: 0.00002267
Iteration 154/1000 | Loss: 0.00002267
Iteration 155/1000 | Loss: 0.00002267
Iteration 156/1000 | Loss: 0.00002267
Iteration 157/1000 | Loss: 0.00002267
Iteration 158/1000 | Loss: 0.00002267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.2670694306725636e-05, 2.2670694306725636e-05, 2.2670694306725636e-05, 2.2670694306725636e-05, 2.2670694306725636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2670694306725636e-05

Optimization complete. Final v2v error: 3.9230849742889404 mm

Highest mean error: 5.044436931610107 mm for frame 234

Lowest mean error: 3.2077622413635254 mm for frame 137

Saving results

Total time: 149.53028893470764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00700564
Iteration 2/25 | Loss: 0.00143470
Iteration 3/25 | Loss: 0.00132772
Iteration 4/25 | Loss: 0.00129738
Iteration 5/25 | Loss: 0.00128916
Iteration 6/25 | Loss: 0.00128641
Iteration 7/25 | Loss: 0.00128503
Iteration 8/25 | Loss: 0.00128367
Iteration 9/25 | Loss: 0.00128574
Iteration 10/25 | Loss: 0.00128305
Iteration 11/25 | Loss: 0.00128284
Iteration 12/25 | Loss: 0.00128439
Iteration 13/25 | Loss: 0.00128269
Iteration 14/25 | Loss: 0.00128268
Iteration 15/25 | Loss: 0.00128268
Iteration 16/25 | Loss: 0.00128268
Iteration 17/25 | Loss: 0.00128268
Iteration 18/25 | Loss: 0.00128268
Iteration 19/25 | Loss: 0.00128268
Iteration 20/25 | Loss: 0.00128268
Iteration 21/25 | Loss: 0.00128268
Iteration 22/25 | Loss: 0.00128267
Iteration 23/25 | Loss: 0.00128267
Iteration 24/25 | Loss: 0.00128267
Iteration 25/25 | Loss: 0.00128267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.60965538
Iteration 2/25 | Loss: 0.00085731
Iteration 3/25 | Loss: 0.00085730
Iteration 4/25 | Loss: 0.00085730
Iteration 5/25 | Loss: 0.00085730
Iteration 6/25 | Loss: 0.00085730
Iteration 7/25 | Loss: 0.00085730
Iteration 8/25 | Loss: 0.00085730
Iteration 9/25 | Loss: 0.00085730
Iteration 10/25 | Loss: 0.00085730
Iteration 11/25 | Loss: 0.00085730
Iteration 12/25 | Loss: 0.00085730
Iteration 13/25 | Loss: 0.00085730
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008573024533689022, 0.0008573024533689022, 0.0008573024533689022, 0.0008573024533689022, 0.0008573024533689022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008573024533689022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085730
Iteration 2/1000 | Loss: 0.00002464
Iteration 3/1000 | Loss: 0.00001994
Iteration 4/1000 | Loss: 0.00001859
Iteration 5/1000 | Loss: 0.00001775
Iteration 6/1000 | Loss: 0.00001714
Iteration 7/1000 | Loss: 0.00001688
Iteration 8/1000 | Loss: 0.00029220
Iteration 9/1000 | Loss: 0.00001707
Iteration 10/1000 | Loss: 0.00001588
Iteration 11/1000 | Loss: 0.00001532
Iteration 12/1000 | Loss: 0.00001494
Iteration 13/1000 | Loss: 0.00001491
Iteration 14/1000 | Loss: 0.00001482
Iteration 15/1000 | Loss: 0.00001479
Iteration 16/1000 | Loss: 0.00001469
Iteration 17/1000 | Loss: 0.00001466
Iteration 18/1000 | Loss: 0.00001462
Iteration 19/1000 | Loss: 0.00001462
Iteration 20/1000 | Loss: 0.00001462
Iteration 21/1000 | Loss: 0.00001462
Iteration 22/1000 | Loss: 0.00001462
Iteration 23/1000 | Loss: 0.00001462
Iteration 24/1000 | Loss: 0.00001462
Iteration 25/1000 | Loss: 0.00001461
Iteration 26/1000 | Loss: 0.00001460
Iteration 27/1000 | Loss: 0.00001460
Iteration 28/1000 | Loss: 0.00001456
Iteration 29/1000 | Loss: 0.00001455
Iteration 30/1000 | Loss: 0.00001453
Iteration 31/1000 | Loss: 0.00001448
Iteration 32/1000 | Loss: 0.00001437
Iteration 33/1000 | Loss: 0.00001433
Iteration 34/1000 | Loss: 0.00001431
Iteration 35/1000 | Loss: 0.00001429
Iteration 36/1000 | Loss: 0.00001429
Iteration 37/1000 | Loss: 0.00001428
Iteration 38/1000 | Loss: 0.00001427
Iteration 39/1000 | Loss: 0.00001426
Iteration 40/1000 | Loss: 0.00001425
Iteration 41/1000 | Loss: 0.00001424
Iteration 42/1000 | Loss: 0.00001423
Iteration 43/1000 | Loss: 0.00001423
Iteration 44/1000 | Loss: 0.00001423
Iteration 45/1000 | Loss: 0.00001422
Iteration 46/1000 | Loss: 0.00001422
Iteration 47/1000 | Loss: 0.00001421
Iteration 48/1000 | Loss: 0.00001421
Iteration 49/1000 | Loss: 0.00001420
Iteration 50/1000 | Loss: 0.00001420
Iteration 51/1000 | Loss: 0.00001420
Iteration 52/1000 | Loss: 0.00001419
Iteration 53/1000 | Loss: 0.00001419
Iteration 54/1000 | Loss: 0.00001419
Iteration 55/1000 | Loss: 0.00001418
Iteration 56/1000 | Loss: 0.00001418
Iteration 57/1000 | Loss: 0.00001418
Iteration 58/1000 | Loss: 0.00001417
Iteration 59/1000 | Loss: 0.00001416
Iteration 60/1000 | Loss: 0.00001416
Iteration 61/1000 | Loss: 0.00001415
Iteration 62/1000 | Loss: 0.00001415
Iteration 63/1000 | Loss: 0.00001413
Iteration 64/1000 | Loss: 0.00001413
Iteration 65/1000 | Loss: 0.00001411
Iteration 66/1000 | Loss: 0.00001410
Iteration 67/1000 | Loss: 0.00001410
Iteration 68/1000 | Loss: 0.00001410
Iteration 69/1000 | Loss: 0.00001410
Iteration 70/1000 | Loss: 0.00001410
Iteration 71/1000 | Loss: 0.00001409
Iteration 72/1000 | Loss: 0.00001409
Iteration 73/1000 | Loss: 0.00001409
Iteration 74/1000 | Loss: 0.00001408
Iteration 75/1000 | Loss: 0.00001408
Iteration 76/1000 | Loss: 0.00001407
Iteration 77/1000 | Loss: 0.00001407
Iteration 78/1000 | Loss: 0.00001407
Iteration 79/1000 | Loss: 0.00001406
Iteration 80/1000 | Loss: 0.00001406
Iteration 81/1000 | Loss: 0.00001406
Iteration 82/1000 | Loss: 0.00001406
Iteration 83/1000 | Loss: 0.00001406
Iteration 84/1000 | Loss: 0.00001406
Iteration 85/1000 | Loss: 0.00001406
Iteration 86/1000 | Loss: 0.00001406
Iteration 87/1000 | Loss: 0.00001406
Iteration 88/1000 | Loss: 0.00001405
Iteration 89/1000 | Loss: 0.00001405
Iteration 90/1000 | Loss: 0.00001405
Iteration 91/1000 | Loss: 0.00001405
Iteration 92/1000 | Loss: 0.00001405
Iteration 93/1000 | Loss: 0.00001405
Iteration 94/1000 | Loss: 0.00001404
Iteration 95/1000 | Loss: 0.00001404
Iteration 96/1000 | Loss: 0.00001404
Iteration 97/1000 | Loss: 0.00001403
Iteration 98/1000 | Loss: 0.00001403
Iteration 99/1000 | Loss: 0.00001403
Iteration 100/1000 | Loss: 0.00001403
Iteration 101/1000 | Loss: 0.00001403
Iteration 102/1000 | Loss: 0.00001403
Iteration 103/1000 | Loss: 0.00001403
Iteration 104/1000 | Loss: 0.00001403
Iteration 105/1000 | Loss: 0.00001403
Iteration 106/1000 | Loss: 0.00001403
Iteration 107/1000 | Loss: 0.00001403
Iteration 108/1000 | Loss: 0.00001403
Iteration 109/1000 | Loss: 0.00001403
Iteration 110/1000 | Loss: 0.00001402
Iteration 111/1000 | Loss: 0.00001402
Iteration 112/1000 | Loss: 0.00001402
Iteration 113/1000 | Loss: 0.00001402
Iteration 114/1000 | Loss: 0.00001402
Iteration 115/1000 | Loss: 0.00001402
Iteration 116/1000 | Loss: 0.00001402
Iteration 117/1000 | Loss: 0.00001402
Iteration 118/1000 | Loss: 0.00001402
Iteration 119/1000 | Loss: 0.00001402
Iteration 120/1000 | Loss: 0.00001402
Iteration 121/1000 | Loss: 0.00001402
Iteration 122/1000 | Loss: 0.00001402
Iteration 123/1000 | Loss: 0.00001402
Iteration 124/1000 | Loss: 0.00001402
Iteration 125/1000 | Loss: 0.00001402
Iteration 126/1000 | Loss: 0.00001401
Iteration 127/1000 | Loss: 0.00001401
Iteration 128/1000 | Loss: 0.00001401
Iteration 129/1000 | Loss: 0.00001401
Iteration 130/1000 | Loss: 0.00001401
Iteration 131/1000 | Loss: 0.00001401
Iteration 132/1000 | Loss: 0.00001401
Iteration 133/1000 | Loss: 0.00001401
Iteration 134/1000 | Loss: 0.00001401
Iteration 135/1000 | Loss: 0.00001401
Iteration 136/1000 | Loss: 0.00001401
Iteration 137/1000 | Loss: 0.00001401
Iteration 138/1000 | Loss: 0.00001401
Iteration 139/1000 | Loss: 0.00001401
Iteration 140/1000 | Loss: 0.00001401
Iteration 141/1000 | Loss: 0.00001401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.4011703569849487e-05, 1.4011703569849487e-05, 1.4011703569849487e-05, 1.4011703569849487e-05, 1.4011703569849487e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4011703569849487e-05

Optimization complete. Final v2v error: 3.2027249336242676 mm

Highest mean error: 4.11933708190918 mm for frame 140

Lowest mean error: 2.9665427207946777 mm for frame 4

Saving results

Total time: 62.766576051712036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428246
Iteration 2/25 | Loss: 0.00132037
Iteration 3/25 | Loss: 0.00125533
Iteration 4/25 | Loss: 0.00124676
Iteration 5/25 | Loss: 0.00124417
Iteration 6/25 | Loss: 0.00124399
Iteration 7/25 | Loss: 0.00124399
Iteration 8/25 | Loss: 0.00124399
Iteration 9/25 | Loss: 0.00124399
Iteration 10/25 | Loss: 0.00124399
Iteration 11/25 | Loss: 0.00124399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012439896818250418, 0.0012439896818250418, 0.0012439896818250418, 0.0012439896818250418, 0.0012439896818250418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012439896818250418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.08137584
Iteration 2/25 | Loss: 0.00083545
Iteration 3/25 | Loss: 0.00083545
Iteration 4/25 | Loss: 0.00083545
Iteration 5/25 | Loss: 0.00083545
Iteration 6/25 | Loss: 0.00083545
Iteration 7/25 | Loss: 0.00083545
Iteration 8/25 | Loss: 0.00083545
Iteration 9/25 | Loss: 0.00083545
Iteration 10/25 | Loss: 0.00083544
Iteration 11/25 | Loss: 0.00083544
Iteration 12/25 | Loss: 0.00083544
Iteration 13/25 | Loss: 0.00083544
Iteration 14/25 | Loss: 0.00083544
Iteration 15/25 | Loss: 0.00083544
Iteration 16/25 | Loss: 0.00083544
Iteration 17/25 | Loss: 0.00083544
Iteration 18/25 | Loss: 0.00083544
Iteration 19/25 | Loss: 0.00083544
Iteration 20/25 | Loss: 0.00083544
Iteration 21/25 | Loss: 0.00083544
Iteration 22/25 | Loss: 0.00083544
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008354440215043724, 0.0008354440215043724, 0.0008354440215043724, 0.0008354440215043724, 0.0008354440215043724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008354440215043724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083544
Iteration 2/1000 | Loss: 0.00002500
Iteration 3/1000 | Loss: 0.00001881
Iteration 4/1000 | Loss: 0.00001635
Iteration 5/1000 | Loss: 0.00001541
Iteration 6/1000 | Loss: 0.00001463
Iteration 7/1000 | Loss: 0.00001419
Iteration 8/1000 | Loss: 0.00001393
Iteration 9/1000 | Loss: 0.00001362
Iteration 10/1000 | Loss: 0.00001344
Iteration 11/1000 | Loss: 0.00001339
Iteration 12/1000 | Loss: 0.00001337
Iteration 13/1000 | Loss: 0.00001336
Iteration 14/1000 | Loss: 0.00001335
Iteration 15/1000 | Loss: 0.00001333
Iteration 16/1000 | Loss: 0.00001333
Iteration 17/1000 | Loss: 0.00001314
Iteration 18/1000 | Loss: 0.00001303
Iteration 19/1000 | Loss: 0.00001303
Iteration 20/1000 | Loss: 0.00001295
Iteration 21/1000 | Loss: 0.00001292
Iteration 22/1000 | Loss: 0.00001290
Iteration 23/1000 | Loss: 0.00001288
Iteration 24/1000 | Loss: 0.00001287
Iteration 25/1000 | Loss: 0.00001286
Iteration 26/1000 | Loss: 0.00001286
Iteration 27/1000 | Loss: 0.00001284
Iteration 28/1000 | Loss: 0.00001282
Iteration 29/1000 | Loss: 0.00001276
Iteration 30/1000 | Loss: 0.00001276
Iteration 31/1000 | Loss: 0.00001272
Iteration 32/1000 | Loss: 0.00001271
Iteration 33/1000 | Loss: 0.00001270
Iteration 34/1000 | Loss: 0.00001270
Iteration 35/1000 | Loss: 0.00001269
Iteration 36/1000 | Loss: 0.00001268
Iteration 37/1000 | Loss: 0.00001268
Iteration 38/1000 | Loss: 0.00001268
Iteration 39/1000 | Loss: 0.00001267
Iteration 40/1000 | Loss: 0.00001267
Iteration 41/1000 | Loss: 0.00001267
Iteration 42/1000 | Loss: 0.00001267
Iteration 43/1000 | Loss: 0.00001267
Iteration 44/1000 | Loss: 0.00001267
Iteration 45/1000 | Loss: 0.00001266
Iteration 46/1000 | Loss: 0.00001266
Iteration 47/1000 | Loss: 0.00001266
Iteration 48/1000 | Loss: 0.00001266
Iteration 49/1000 | Loss: 0.00001266
Iteration 50/1000 | Loss: 0.00001265
Iteration 51/1000 | Loss: 0.00001265
Iteration 52/1000 | Loss: 0.00001264
Iteration 53/1000 | Loss: 0.00001264
Iteration 54/1000 | Loss: 0.00001264
Iteration 55/1000 | Loss: 0.00001264
Iteration 56/1000 | Loss: 0.00001263
Iteration 57/1000 | Loss: 0.00001263
Iteration 58/1000 | Loss: 0.00001263
Iteration 59/1000 | Loss: 0.00001262
Iteration 60/1000 | Loss: 0.00001261
Iteration 61/1000 | Loss: 0.00001260
Iteration 62/1000 | Loss: 0.00001260
Iteration 63/1000 | Loss: 0.00001259
Iteration 64/1000 | Loss: 0.00001259
Iteration 65/1000 | Loss: 0.00001259
Iteration 66/1000 | Loss: 0.00001259
Iteration 67/1000 | Loss: 0.00001258
Iteration 68/1000 | Loss: 0.00001258
Iteration 69/1000 | Loss: 0.00001257
Iteration 70/1000 | Loss: 0.00001257
Iteration 71/1000 | Loss: 0.00001256
Iteration 72/1000 | Loss: 0.00001255
Iteration 73/1000 | Loss: 0.00001255
Iteration 74/1000 | Loss: 0.00001254
Iteration 75/1000 | Loss: 0.00001254
Iteration 76/1000 | Loss: 0.00001253
Iteration 77/1000 | Loss: 0.00001253
Iteration 78/1000 | Loss: 0.00001252
Iteration 79/1000 | Loss: 0.00001252
Iteration 80/1000 | Loss: 0.00001252
Iteration 81/1000 | Loss: 0.00001251
Iteration 82/1000 | Loss: 0.00001251
Iteration 83/1000 | Loss: 0.00001250
Iteration 84/1000 | Loss: 0.00001249
Iteration 85/1000 | Loss: 0.00001249
Iteration 86/1000 | Loss: 0.00001249
Iteration 87/1000 | Loss: 0.00001249
Iteration 88/1000 | Loss: 0.00001249
Iteration 89/1000 | Loss: 0.00001248
Iteration 90/1000 | Loss: 0.00001248
Iteration 91/1000 | Loss: 0.00001248
Iteration 92/1000 | Loss: 0.00001248
Iteration 93/1000 | Loss: 0.00001248
Iteration 94/1000 | Loss: 0.00001248
Iteration 95/1000 | Loss: 0.00001248
Iteration 96/1000 | Loss: 0.00001248
Iteration 97/1000 | Loss: 0.00001248
Iteration 98/1000 | Loss: 0.00001248
Iteration 99/1000 | Loss: 0.00001247
Iteration 100/1000 | Loss: 0.00001247
Iteration 101/1000 | Loss: 0.00001247
Iteration 102/1000 | Loss: 0.00001247
Iteration 103/1000 | Loss: 0.00001247
Iteration 104/1000 | Loss: 0.00001247
Iteration 105/1000 | Loss: 0.00001247
Iteration 106/1000 | Loss: 0.00001247
Iteration 107/1000 | Loss: 0.00001247
Iteration 108/1000 | Loss: 0.00001247
Iteration 109/1000 | Loss: 0.00001247
Iteration 110/1000 | Loss: 0.00001246
Iteration 111/1000 | Loss: 0.00001246
Iteration 112/1000 | Loss: 0.00001246
Iteration 113/1000 | Loss: 0.00001246
Iteration 114/1000 | Loss: 0.00001246
Iteration 115/1000 | Loss: 0.00001245
Iteration 116/1000 | Loss: 0.00001245
Iteration 117/1000 | Loss: 0.00001245
Iteration 118/1000 | Loss: 0.00001245
Iteration 119/1000 | Loss: 0.00001244
Iteration 120/1000 | Loss: 0.00001244
Iteration 121/1000 | Loss: 0.00001244
Iteration 122/1000 | Loss: 0.00001244
Iteration 123/1000 | Loss: 0.00001244
Iteration 124/1000 | Loss: 0.00001244
Iteration 125/1000 | Loss: 0.00001244
Iteration 126/1000 | Loss: 0.00001244
Iteration 127/1000 | Loss: 0.00001244
Iteration 128/1000 | Loss: 0.00001244
Iteration 129/1000 | Loss: 0.00001244
Iteration 130/1000 | Loss: 0.00001243
Iteration 131/1000 | Loss: 0.00001243
Iteration 132/1000 | Loss: 0.00001243
Iteration 133/1000 | Loss: 0.00001243
Iteration 134/1000 | Loss: 0.00001243
Iteration 135/1000 | Loss: 0.00001243
Iteration 136/1000 | Loss: 0.00001243
Iteration 137/1000 | Loss: 0.00001243
Iteration 138/1000 | Loss: 0.00001243
Iteration 139/1000 | Loss: 0.00001242
Iteration 140/1000 | Loss: 0.00001242
Iteration 141/1000 | Loss: 0.00001242
Iteration 142/1000 | Loss: 0.00001242
Iteration 143/1000 | Loss: 0.00001242
Iteration 144/1000 | Loss: 0.00001242
Iteration 145/1000 | Loss: 0.00001241
Iteration 146/1000 | Loss: 0.00001241
Iteration 147/1000 | Loss: 0.00001241
Iteration 148/1000 | Loss: 0.00001241
Iteration 149/1000 | Loss: 0.00001241
Iteration 150/1000 | Loss: 0.00001241
Iteration 151/1000 | Loss: 0.00001241
Iteration 152/1000 | Loss: 0.00001240
Iteration 153/1000 | Loss: 0.00001240
Iteration 154/1000 | Loss: 0.00001240
Iteration 155/1000 | Loss: 0.00001240
Iteration 156/1000 | Loss: 0.00001240
Iteration 157/1000 | Loss: 0.00001240
Iteration 158/1000 | Loss: 0.00001240
Iteration 159/1000 | Loss: 0.00001240
Iteration 160/1000 | Loss: 0.00001240
Iteration 161/1000 | Loss: 0.00001240
Iteration 162/1000 | Loss: 0.00001240
Iteration 163/1000 | Loss: 0.00001240
Iteration 164/1000 | Loss: 0.00001239
Iteration 165/1000 | Loss: 0.00001239
Iteration 166/1000 | Loss: 0.00001239
Iteration 167/1000 | Loss: 0.00001239
Iteration 168/1000 | Loss: 0.00001239
Iteration 169/1000 | Loss: 0.00001238
Iteration 170/1000 | Loss: 0.00001238
Iteration 171/1000 | Loss: 0.00001238
Iteration 172/1000 | Loss: 0.00001238
Iteration 173/1000 | Loss: 0.00001238
Iteration 174/1000 | Loss: 0.00001238
Iteration 175/1000 | Loss: 0.00001238
Iteration 176/1000 | Loss: 0.00001238
Iteration 177/1000 | Loss: 0.00001238
Iteration 178/1000 | Loss: 0.00001237
Iteration 179/1000 | Loss: 0.00001237
Iteration 180/1000 | Loss: 0.00001237
Iteration 181/1000 | Loss: 0.00001237
Iteration 182/1000 | Loss: 0.00001237
Iteration 183/1000 | Loss: 0.00001237
Iteration 184/1000 | Loss: 0.00001237
Iteration 185/1000 | Loss: 0.00001237
Iteration 186/1000 | Loss: 0.00001236
Iteration 187/1000 | Loss: 0.00001236
Iteration 188/1000 | Loss: 0.00001236
Iteration 189/1000 | Loss: 0.00001236
Iteration 190/1000 | Loss: 0.00001236
Iteration 191/1000 | Loss: 0.00001236
Iteration 192/1000 | Loss: 0.00001236
Iteration 193/1000 | Loss: 0.00001236
Iteration 194/1000 | Loss: 0.00001236
Iteration 195/1000 | Loss: 0.00001236
Iteration 196/1000 | Loss: 0.00001236
Iteration 197/1000 | Loss: 0.00001236
Iteration 198/1000 | Loss: 0.00001236
Iteration 199/1000 | Loss: 0.00001236
Iteration 200/1000 | Loss: 0.00001236
Iteration 201/1000 | Loss: 0.00001236
Iteration 202/1000 | Loss: 0.00001236
Iteration 203/1000 | Loss: 0.00001236
Iteration 204/1000 | Loss: 0.00001236
Iteration 205/1000 | Loss: 0.00001236
Iteration 206/1000 | Loss: 0.00001236
Iteration 207/1000 | Loss: 0.00001236
Iteration 208/1000 | Loss: 0.00001236
Iteration 209/1000 | Loss: 0.00001236
Iteration 210/1000 | Loss: 0.00001236
Iteration 211/1000 | Loss: 0.00001236
Iteration 212/1000 | Loss: 0.00001236
Iteration 213/1000 | Loss: 0.00001236
Iteration 214/1000 | Loss: 0.00001236
Iteration 215/1000 | Loss: 0.00001236
Iteration 216/1000 | Loss: 0.00001236
Iteration 217/1000 | Loss: 0.00001236
Iteration 218/1000 | Loss: 0.00001236
Iteration 219/1000 | Loss: 0.00001236
Iteration 220/1000 | Loss: 0.00001236
Iteration 221/1000 | Loss: 0.00001236
Iteration 222/1000 | Loss: 0.00001236
Iteration 223/1000 | Loss: 0.00001236
Iteration 224/1000 | Loss: 0.00001236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.2358765161479823e-05, 1.2358765161479823e-05, 1.2358765161479823e-05, 1.2358765161479823e-05, 1.2358765161479823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2358765161479823e-05

Optimization complete. Final v2v error: 3.0313873291015625 mm

Highest mean error: 3.245421886444092 mm for frame 107

Lowest mean error: 2.882810354232788 mm for frame 177

Saving results

Total time: 44.38267993927002
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818813
Iteration 2/25 | Loss: 0.00219329
Iteration 3/25 | Loss: 0.00163619
Iteration 4/25 | Loss: 0.00156723
Iteration 5/25 | Loss: 0.00156379
Iteration 6/25 | Loss: 0.00153661
Iteration 7/25 | Loss: 0.00151783
Iteration 8/25 | Loss: 0.00151702
Iteration 9/25 | Loss: 0.00151684
Iteration 10/25 | Loss: 0.00151143
Iteration 11/25 | Loss: 0.00150903
Iteration 12/25 | Loss: 0.00150802
Iteration 13/25 | Loss: 0.00150703
Iteration 14/25 | Loss: 0.00150691
Iteration 15/25 | Loss: 0.00150652
Iteration 16/25 | Loss: 0.00150674
Iteration 17/25 | Loss: 0.00150671
Iteration 18/25 | Loss: 0.00150638
Iteration 19/25 | Loss: 0.00150661
Iteration 20/25 | Loss: 0.00150633
Iteration 21/25 | Loss: 0.00150664
Iteration 22/25 | Loss: 0.00150643
Iteration 23/25 | Loss: 0.00150618
Iteration 24/25 | Loss: 0.00150579
Iteration 25/25 | Loss: 0.00150681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23955059
Iteration 2/25 | Loss: 0.00101988
Iteration 3/25 | Loss: 0.00101985
Iteration 4/25 | Loss: 0.00101985
Iteration 5/25 | Loss: 0.00101985
Iteration 6/25 | Loss: 0.00101985
Iteration 7/25 | Loss: 0.00101985
Iteration 8/25 | Loss: 0.00101985
Iteration 9/25 | Loss: 0.00101985
Iteration 10/25 | Loss: 0.00101985
Iteration 11/25 | Loss: 0.00101985
Iteration 12/25 | Loss: 0.00101985
Iteration 13/25 | Loss: 0.00101985
Iteration 14/25 | Loss: 0.00101985
Iteration 15/25 | Loss: 0.00101985
Iteration 16/25 | Loss: 0.00101985
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010198499076068401, 0.0010198499076068401, 0.0010198499076068401, 0.0010198499076068401, 0.0010198499076068401]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010198499076068401

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101985
Iteration 2/1000 | Loss: 0.00006155
Iteration 3/1000 | Loss: 0.00003892
Iteration 4/1000 | Loss: 0.00004022
Iteration 5/1000 | Loss: 0.00003645
Iteration 6/1000 | Loss: 0.00003444
Iteration 7/1000 | Loss: 0.00004008
Iteration 8/1000 | Loss: 0.00004251
Iteration 9/1000 | Loss: 0.00004096
Iteration 10/1000 | Loss: 0.00003556
Iteration 11/1000 | Loss: 0.00003385
Iteration 12/1000 | Loss: 0.00003278
Iteration 13/1000 | Loss: 0.00003200
Iteration 14/1000 | Loss: 0.00003139
Iteration 15/1000 | Loss: 0.00003101
Iteration 16/1000 | Loss: 0.00003083
Iteration 17/1000 | Loss: 0.00003057
Iteration 18/1000 | Loss: 0.00003042
Iteration 19/1000 | Loss: 0.00003039
Iteration 20/1000 | Loss: 0.00003038
Iteration 21/1000 | Loss: 0.00003038
Iteration 22/1000 | Loss: 0.00003037
Iteration 23/1000 | Loss: 0.00003037
Iteration 24/1000 | Loss: 0.00003033
Iteration 25/1000 | Loss: 0.00003032
Iteration 26/1000 | Loss: 0.00003032
Iteration 27/1000 | Loss: 0.00003032
Iteration 28/1000 | Loss: 0.00003030
Iteration 29/1000 | Loss: 0.00003030
Iteration 30/1000 | Loss: 0.00003030
Iteration 31/1000 | Loss: 0.00003030
Iteration 32/1000 | Loss: 0.00003030
Iteration 33/1000 | Loss: 0.00003029
Iteration 34/1000 | Loss: 0.00003029
Iteration 35/1000 | Loss: 0.00003029
Iteration 36/1000 | Loss: 0.00003029
Iteration 37/1000 | Loss: 0.00003029
Iteration 38/1000 | Loss: 0.00003029
Iteration 39/1000 | Loss: 0.00003029
Iteration 40/1000 | Loss: 0.00003029
Iteration 41/1000 | Loss: 0.00003028
Iteration 42/1000 | Loss: 0.00003027
Iteration 43/1000 | Loss: 0.00003027
Iteration 44/1000 | Loss: 0.00003026
Iteration 45/1000 | Loss: 0.00003026
Iteration 46/1000 | Loss: 0.00003026
Iteration 47/1000 | Loss: 0.00003026
Iteration 48/1000 | Loss: 0.00003026
Iteration 49/1000 | Loss: 0.00003026
Iteration 50/1000 | Loss: 0.00003026
Iteration 51/1000 | Loss: 0.00003026
Iteration 52/1000 | Loss: 0.00003026
Iteration 53/1000 | Loss: 0.00003025
Iteration 54/1000 | Loss: 0.00003025
Iteration 55/1000 | Loss: 0.00003025
Iteration 56/1000 | Loss: 0.00003025
Iteration 57/1000 | Loss: 0.00003025
Iteration 58/1000 | Loss: 0.00003025
Iteration 59/1000 | Loss: 0.00003025
Iteration 60/1000 | Loss: 0.00003025
Iteration 61/1000 | Loss: 0.00003025
Iteration 62/1000 | Loss: 0.00003025
Iteration 63/1000 | Loss: 0.00003025
Iteration 64/1000 | Loss: 0.00003025
Iteration 65/1000 | Loss: 0.00003025
Iteration 66/1000 | Loss: 0.00003024
Iteration 67/1000 | Loss: 0.00003024
Iteration 68/1000 | Loss: 0.00003024
Iteration 69/1000 | Loss: 0.00003024
Iteration 70/1000 | Loss: 0.00003024
Iteration 71/1000 | Loss: 0.00003024
Iteration 72/1000 | Loss: 0.00003024
Iteration 73/1000 | Loss: 0.00003024
Iteration 74/1000 | Loss: 0.00003024
Iteration 75/1000 | Loss: 0.00003024
Iteration 76/1000 | Loss: 0.00003024
Iteration 77/1000 | Loss: 0.00003024
Iteration 78/1000 | Loss: 0.00003023
Iteration 79/1000 | Loss: 0.00003023
Iteration 80/1000 | Loss: 0.00003023
Iteration 81/1000 | Loss: 0.00003023
Iteration 82/1000 | Loss: 0.00003023
Iteration 83/1000 | Loss: 0.00003023
Iteration 84/1000 | Loss: 0.00003023
Iteration 85/1000 | Loss: 0.00003023
Iteration 86/1000 | Loss: 0.00003023
Iteration 87/1000 | Loss: 0.00003023
Iteration 88/1000 | Loss: 0.00003023
Iteration 89/1000 | Loss: 0.00003023
Iteration 90/1000 | Loss: 0.00003023
Iteration 91/1000 | Loss: 0.00003023
Iteration 92/1000 | Loss: 0.00003023
Iteration 93/1000 | Loss: 0.00003022
Iteration 94/1000 | Loss: 0.00003022
Iteration 95/1000 | Loss: 0.00003022
Iteration 96/1000 | Loss: 0.00003022
Iteration 97/1000 | Loss: 0.00003022
Iteration 98/1000 | Loss: 0.00003022
Iteration 99/1000 | Loss: 0.00003022
Iteration 100/1000 | Loss: 0.00003022
Iteration 101/1000 | Loss: 0.00003022
Iteration 102/1000 | Loss: 0.00003022
Iteration 103/1000 | Loss: 0.00003022
Iteration 104/1000 | Loss: 0.00003022
Iteration 105/1000 | Loss: 0.00003022
Iteration 106/1000 | Loss: 0.00003022
Iteration 107/1000 | Loss: 0.00003022
Iteration 108/1000 | Loss: 0.00003021
Iteration 109/1000 | Loss: 0.00003021
Iteration 110/1000 | Loss: 0.00003021
Iteration 111/1000 | Loss: 0.00003021
Iteration 112/1000 | Loss: 0.00003021
Iteration 113/1000 | Loss: 0.00003021
Iteration 114/1000 | Loss: 0.00003021
Iteration 115/1000 | Loss: 0.00003020
Iteration 116/1000 | Loss: 0.00003020
Iteration 117/1000 | Loss: 0.00003020
Iteration 118/1000 | Loss: 0.00003020
Iteration 119/1000 | Loss: 0.00003020
Iteration 120/1000 | Loss: 0.00003020
Iteration 121/1000 | Loss: 0.00003019
Iteration 122/1000 | Loss: 0.00003019
Iteration 123/1000 | Loss: 0.00003019
Iteration 124/1000 | Loss: 0.00003019
Iteration 125/1000 | Loss: 0.00003019
Iteration 126/1000 | Loss: 0.00003019
Iteration 127/1000 | Loss: 0.00003019
Iteration 128/1000 | Loss: 0.00003019
Iteration 129/1000 | Loss: 0.00003019
Iteration 130/1000 | Loss: 0.00003019
Iteration 131/1000 | Loss: 0.00003019
Iteration 132/1000 | Loss: 0.00003019
Iteration 133/1000 | Loss: 0.00003019
Iteration 134/1000 | Loss: 0.00003018
Iteration 135/1000 | Loss: 0.00003018
Iteration 136/1000 | Loss: 0.00003018
Iteration 137/1000 | Loss: 0.00003018
Iteration 138/1000 | Loss: 0.00003018
Iteration 139/1000 | Loss: 0.00003018
Iteration 140/1000 | Loss: 0.00003018
Iteration 141/1000 | Loss: 0.00003018
Iteration 142/1000 | Loss: 0.00003018
Iteration 143/1000 | Loss: 0.00003018
Iteration 144/1000 | Loss: 0.00003018
Iteration 145/1000 | Loss: 0.00003017
Iteration 146/1000 | Loss: 0.00003017
Iteration 147/1000 | Loss: 0.00003017
Iteration 148/1000 | Loss: 0.00003017
Iteration 149/1000 | Loss: 0.00003017
Iteration 150/1000 | Loss: 0.00003017
Iteration 151/1000 | Loss: 0.00003017
Iteration 152/1000 | Loss: 0.00003017
Iteration 153/1000 | Loss: 0.00003017
Iteration 154/1000 | Loss: 0.00003017
Iteration 155/1000 | Loss: 0.00003017
Iteration 156/1000 | Loss: 0.00003017
Iteration 157/1000 | Loss: 0.00003017
Iteration 158/1000 | Loss: 0.00003017
Iteration 159/1000 | Loss: 0.00003017
Iteration 160/1000 | Loss: 0.00003017
Iteration 161/1000 | Loss: 0.00003016
Iteration 162/1000 | Loss: 0.00003016
Iteration 163/1000 | Loss: 0.00003016
Iteration 164/1000 | Loss: 0.00003016
Iteration 165/1000 | Loss: 0.00003016
Iteration 166/1000 | Loss: 0.00003016
Iteration 167/1000 | Loss: 0.00003015
Iteration 168/1000 | Loss: 0.00003015
Iteration 169/1000 | Loss: 0.00003015
Iteration 170/1000 | Loss: 0.00003015
Iteration 171/1000 | Loss: 0.00003015
Iteration 172/1000 | Loss: 0.00003015
Iteration 173/1000 | Loss: 0.00003015
Iteration 174/1000 | Loss: 0.00003015
Iteration 175/1000 | Loss: 0.00003015
Iteration 176/1000 | Loss: 0.00003015
Iteration 177/1000 | Loss: 0.00003015
Iteration 178/1000 | Loss: 0.00003015
Iteration 179/1000 | Loss: 0.00003015
Iteration 180/1000 | Loss: 0.00003015
Iteration 181/1000 | Loss: 0.00003015
Iteration 182/1000 | Loss: 0.00003015
Iteration 183/1000 | Loss: 0.00003015
Iteration 184/1000 | Loss: 0.00003015
Iteration 185/1000 | Loss: 0.00003015
Iteration 186/1000 | Loss: 0.00003015
Iteration 187/1000 | Loss: 0.00003015
Iteration 188/1000 | Loss: 0.00003015
Iteration 189/1000 | Loss: 0.00003015
Iteration 190/1000 | Loss: 0.00003015
Iteration 191/1000 | Loss: 0.00003015
Iteration 192/1000 | Loss: 0.00003015
Iteration 193/1000 | Loss: 0.00003015
Iteration 194/1000 | Loss: 0.00003015
Iteration 195/1000 | Loss: 0.00003015
Iteration 196/1000 | Loss: 0.00003015
Iteration 197/1000 | Loss: 0.00003015
Iteration 198/1000 | Loss: 0.00003015
Iteration 199/1000 | Loss: 0.00003015
Iteration 200/1000 | Loss: 0.00003015
Iteration 201/1000 | Loss: 0.00003015
Iteration 202/1000 | Loss: 0.00003015
Iteration 203/1000 | Loss: 0.00003015
Iteration 204/1000 | Loss: 0.00003015
Iteration 205/1000 | Loss: 0.00003015
Iteration 206/1000 | Loss: 0.00003015
Iteration 207/1000 | Loss: 0.00003015
Iteration 208/1000 | Loss: 0.00003015
Iteration 209/1000 | Loss: 0.00003015
Iteration 210/1000 | Loss: 0.00003015
Iteration 211/1000 | Loss: 0.00003015
Iteration 212/1000 | Loss: 0.00003015
Iteration 213/1000 | Loss: 0.00003015
Iteration 214/1000 | Loss: 0.00003015
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [3.0149209123919718e-05, 3.0149209123919718e-05, 3.0149209123919718e-05, 3.0149209123919718e-05, 3.0149209123919718e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0149209123919718e-05

Optimization complete. Final v2v error: 4.47151517868042 mm

Highest mean error: 5.23262882232666 mm for frame 178

Lowest mean error: 4.30911111831665 mm for frame 54

Saving results

Total time: 94.38405990600586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470202
Iteration 2/25 | Loss: 0.00143531
Iteration 3/25 | Loss: 0.00134501
Iteration 4/25 | Loss: 0.00132438
Iteration 5/25 | Loss: 0.00131682
Iteration 6/25 | Loss: 0.00131630
Iteration 7/25 | Loss: 0.00131630
Iteration 8/25 | Loss: 0.00131630
Iteration 9/25 | Loss: 0.00131630
Iteration 10/25 | Loss: 0.00131630
Iteration 11/25 | Loss: 0.00131630
Iteration 12/25 | Loss: 0.00131630
Iteration 13/25 | Loss: 0.00131630
Iteration 14/25 | Loss: 0.00131630
Iteration 15/25 | Loss: 0.00131630
Iteration 16/25 | Loss: 0.00131630
Iteration 17/25 | Loss: 0.00131630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013163044350221753, 0.0013163044350221753, 0.0013163044350221753, 0.0013163044350221753, 0.0013163044350221753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013163044350221753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.82470942
Iteration 2/25 | Loss: 0.00099986
Iteration 3/25 | Loss: 0.00099985
Iteration 4/25 | Loss: 0.00099985
Iteration 5/25 | Loss: 0.00099985
Iteration 6/25 | Loss: 0.00099985
Iteration 7/25 | Loss: 0.00099985
Iteration 8/25 | Loss: 0.00099985
Iteration 9/25 | Loss: 0.00099985
Iteration 10/25 | Loss: 0.00099985
Iteration 11/25 | Loss: 0.00099985
Iteration 12/25 | Loss: 0.00099985
Iteration 13/25 | Loss: 0.00099985
Iteration 14/25 | Loss: 0.00099985
Iteration 15/25 | Loss: 0.00099985
Iteration 16/25 | Loss: 0.00099985
Iteration 17/25 | Loss: 0.00099985
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009998503373935819, 0.0009998503373935819, 0.0009998503373935819, 0.0009998503373935819, 0.0009998503373935819]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009998503373935819

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099985
Iteration 2/1000 | Loss: 0.00002990
Iteration 3/1000 | Loss: 0.00002470
Iteration 4/1000 | Loss: 0.00002354
Iteration 5/1000 | Loss: 0.00002274
Iteration 6/1000 | Loss: 0.00002215
Iteration 7/1000 | Loss: 0.00002180
Iteration 8/1000 | Loss: 0.00002134
Iteration 9/1000 | Loss: 0.00002093
Iteration 10/1000 | Loss: 0.00002071
Iteration 11/1000 | Loss: 0.00002051
Iteration 12/1000 | Loss: 0.00002044
Iteration 13/1000 | Loss: 0.00002043
Iteration 14/1000 | Loss: 0.00002036
Iteration 15/1000 | Loss: 0.00002028
Iteration 16/1000 | Loss: 0.00002028
Iteration 17/1000 | Loss: 0.00002028
Iteration 18/1000 | Loss: 0.00002021
Iteration 19/1000 | Loss: 0.00002014
Iteration 20/1000 | Loss: 0.00002014
Iteration 21/1000 | Loss: 0.00002009
Iteration 22/1000 | Loss: 0.00002009
Iteration 23/1000 | Loss: 0.00002008
Iteration 24/1000 | Loss: 0.00002007
Iteration 25/1000 | Loss: 0.00002007
Iteration 26/1000 | Loss: 0.00002006
Iteration 27/1000 | Loss: 0.00002006
Iteration 28/1000 | Loss: 0.00002005
Iteration 29/1000 | Loss: 0.00002004
Iteration 30/1000 | Loss: 0.00002004
Iteration 31/1000 | Loss: 0.00002004
Iteration 32/1000 | Loss: 0.00002003
Iteration 33/1000 | Loss: 0.00002003
Iteration 34/1000 | Loss: 0.00002003
Iteration 35/1000 | Loss: 0.00002003
Iteration 36/1000 | Loss: 0.00002003
Iteration 37/1000 | Loss: 0.00002003
Iteration 38/1000 | Loss: 0.00002003
Iteration 39/1000 | Loss: 0.00002003
Iteration 40/1000 | Loss: 0.00002003
Iteration 41/1000 | Loss: 0.00002002
Iteration 42/1000 | Loss: 0.00002002
Iteration 43/1000 | Loss: 0.00002002
Iteration 44/1000 | Loss: 0.00002000
Iteration 45/1000 | Loss: 0.00002000
Iteration 46/1000 | Loss: 0.00002000
Iteration 47/1000 | Loss: 0.00001999
Iteration 48/1000 | Loss: 0.00001999
Iteration 49/1000 | Loss: 0.00001999
Iteration 50/1000 | Loss: 0.00001998
Iteration 51/1000 | Loss: 0.00001998
Iteration 52/1000 | Loss: 0.00001997
Iteration 53/1000 | Loss: 0.00001997
Iteration 54/1000 | Loss: 0.00001996
Iteration 55/1000 | Loss: 0.00001996
Iteration 56/1000 | Loss: 0.00001995
Iteration 57/1000 | Loss: 0.00001995
Iteration 58/1000 | Loss: 0.00001994
Iteration 59/1000 | Loss: 0.00001994
Iteration 60/1000 | Loss: 0.00001994
Iteration 61/1000 | Loss: 0.00001994
Iteration 62/1000 | Loss: 0.00001994
Iteration 63/1000 | Loss: 0.00001994
Iteration 64/1000 | Loss: 0.00001994
Iteration 65/1000 | Loss: 0.00001993
Iteration 66/1000 | Loss: 0.00001992
Iteration 67/1000 | Loss: 0.00001991
Iteration 68/1000 | Loss: 0.00001990
Iteration 69/1000 | Loss: 0.00001990
Iteration 70/1000 | Loss: 0.00001990
Iteration 71/1000 | Loss: 0.00001989
Iteration 72/1000 | Loss: 0.00001989
Iteration 73/1000 | Loss: 0.00001988
Iteration 74/1000 | Loss: 0.00001988
Iteration 75/1000 | Loss: 0.00001988
Iteration 76/1000 | Loss: 0.00001987
Iteration 77/1000 | Loss: 0.00001987
Iteration 78/1000 | Loss: 0.00001986
Iteration 79/1000 | Loss: 0.00001985
Iteration 80/1000 | Loss: 0.00001985
Iteration 81/1000 | Loss: 0.00001985
Iteration 82/1000 | Loss: 0.00001985
Iteration 83/1000 | Loss: 0.00001984
Iteration 84/1000 | Loss: 0.00001984
Iteration 85/1000 | Loss: 0.00001983
Iteration 86/1000 | Loss: 0.00001983
Iteration 87/1000 | Loss: 0.00001983
Iteration 88/1000 | Loss: 0.00001982
Iteration 89/1000 | Loss: 0.00001982
Iteration 90/1000 | Loss: 0.00001981
Iteration 91/1000 | Loss: 0.00001980
Iteration 92/1000 | Loss: 0.00001980
Iteration 93/1000 | Loss: 0.00001979
Iteration 94/1000 | Loss: 0.00001978
Iteration 95/1000 | Loss: 0.00001978
Iteration 96/1000 | Loss: 0.00001978
Iteration 97/1000 | Loss: 0.00001977
Iteration 98/1000 | Loss: 0.00001977
Iteration 99/1000 | Loss: 0.00001977
Iteration 100/1000 | Loss: 0.00001976
Iteration 101/1000 | Loss: 0.00001976
Iteration 102/1000 | Loss: 0.00001976
Iteration 103/1000 | Loss: 0.00001975
Iteration 104/1000 | Loss: 0.00001975
Iteration 105/1000 | Loss: 0.00001975
Iteration 106/1000 | Loss: 0.00001975
Iteration 107/1000 | Loss: 0.00001975
Iteration 108/1000 | Loss: 0.00001975
Iteration 109/1000 | Loss: 0.00001975
Iteration 110/1000 | Loss: 0.00001975
Iteration 111/1000 | Loss: 0.00001975
Iteration 112/1000 | Loss: 0.00001974
Iteration 113/1000 | Loss: 0.00001974
Iteration 114/1000 | Loss: 0.00001974
Iteration 115/1000 | Loss: 0.00001974
Iteration 116/1000 | Loss: 0.00001974
Iteration 117/1000 | Loss: 0.00001974
Iteration 118/1000 | Loss: 0.00001974
Iteration 119/1000 | Loss: 0.00001974
Iteration 120/1000 | Loss: 0.00001974
Iteration 121/1000 | Loss: 0.00001974
Iteration 122/1000 | Loss: 0.00001974
Iteration 123/1000 | Loss: 0.00001974
Iteration 124/1000 | Loss: 0.00001974
Iteration 125/1000 | Loss: 0.00001974
Iteration 126/1000 | Loss: 0.00001974
Iteration 127/1000 | Loss: 0.00001974
Iteration 128/1000 | Loss: 0.00001974
Iteration 129/1000 | Loss: 0.00001974
Iteration 130/1000 | Loss: 0.00001974
Iteration 131/1000 | Loss: 0.00001974
Iteration 132/1000 | Loss: 0.00001974
Iteration 133/1000 | Loss: 0.00001974
Iteration 134/1000 | Loss: 0.00001974
Iteration 135/1000 | Loss: 0.00001974
Iteration 136/1000 | Loss: 0.00001974
Iteration 137/1000 | Loss: 0.00001974
Iteration 138/1000 | Loss: 0.00001974
Iteration 139/1000 | Loss: 0.00001974
Iteration 140/1000 | Loss: 0.00001974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.9736304238904268e-05, 1.9736304238904268e-05, 1.9736304238904268e-05, 1.9736304238904268e-05, 1.9736304238904268e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9736304238904268e-05

Optimization complete. Final v2v error: 3.7656259536743164 mm

Highest mean error: 4.3103718757629395 mm for frame 227

Lowest mean error: 3.3879003524780273 mm for frame 251

Saving results

Total time: 43.65159487724304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037663
Iteration 2/25 | Loss: 0.00231880
Iteration 3/25 | Loss: 0.00179192
Iteration 4/25 | Loss: 0.00165062
Iteration 5/25 | Loss: 0.00154742
Iteration 6/25 | Loss: 0.00150778
Iteration 7/25 | Loss: 0.00145799
Iteration 8/25 | Loss: 0.00148378
Iteration 9/25 | Loss: 0.00137950
Iteration 10/25 | Loss: 0.00135601
Iteration 11/25 | Loss: 0.00133714
Iteration 12/25 | Loss: 0.00133023
Iteration 13/25 | Loss: 0.00133217
Iteration 14/25 | Loss: 0.00131928
Iteration 15/25 | Loss: 0.00131077
Iteration 16/25 | Loss: 0.00131148
Iteration 17/25 | Loss: 0.00131047
Iteration 18/25 | Loss: 0.00130708
Iteration 19/25 | Loss: 0.00131315
Iteration 20/25 | Loss: 0.00131031
Iteration 21/25 | Loss: 0.00130845
Iteration 22/25 | Loss: 0.00130203
Iteration 23/25 | Loss: 0.00130171
Iteration 24/25 | Loss: 0.00130163
Iteration 25/25 | Loss: 0.00130162

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52380228
Iteration 2/25 | Loss: 0.00170121
Iteration 3/25 | Loss: 0.00106336
Iteration 4/25 | Loss: 0.00106333
Iteration 5/25 | Loss: 0.00106333
Iteration 6/25 | Loss: 0.00106333
Iteration 7/25 | Loss: 0.00106333
Iteration 8/25 | Loss: 0.00106332
Iteration 9/25 | Loss: 0.00106332
Iteration 10/25 | Loss: 0.00106332
Iteration 11/25 | Loss: 0.00106332
Iteration 12/25 | Loss: 0.00106332
Iteration 13/25 | Loss: 0.00106332
Iteration 14/25 | Loss: 0.00106332
Iteration 15/25 | Loss: 0.00106332
Iteration 16/25 | Loss: 0.00106332
Iteration 17/25 | Loss: 0.00106332
Iteration 18/25 | Loss: 0.00106332
Iteration 19/25 | Loss: 0.00106332
Iteration 20/25 | Loss: 0.00106332
Iteration 21/25 | Loss: 0.00106332
Iteration 22/25 | Loss: 0.00106332
Iteration 23/25 | Loss: 0.00106332
Iteration 24/25 | Loss: 0.00106332
Iteration 25/25 | Loss: 0.00106332

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106332
Iteration 2/1000 | Loss: 0.00075048
Iteration 3/1000 | Loss: 0.00109260
Iteration 4/1000 | Loss: 0.00005807
Iteration 5/1000 | Loss: 0.00010316
Iteration 6/1000 | Loss: 0.00032483
Iteration 7/1000 | Loss: 0.00004498
Iteration 8/1000 | Loss: 0.00009542
Iteration 9/1000 | Loss: 0.00002392
Iteration 10/1000 | Loss: 0.00002251
Iteration 11/1000 | Loss: 0.00002163
Iteration 12/1000 | Loss: 0.00002340
Iteration 13/1000 | Loss: 0.00009899
Iteration 14/1000 | Loss: 0.00002073
Iteration 15/1000 | Loss: 0.00005304
Iteration 16/1000 | Loss: 0.00002053
Iteration 17/1000 | Loss: 0.00002121
Iteration 18/1000 | Loss: 0.00003252
Iteration 19/1000 | Loss: 0.00002099
Iteration 20/1000 | Loss: 0.00002005
Iteration 21/1000 | Loss: 0.00002005
Iteration 22/1000 | Loss: 0.00001998
Iteration 23/1000 | Loss: 0.00001998
Iteration 24/1000 | Loss: 0.00001998
Iteration 25/1000 | Loss: 0.00001997
Iteration 26/1000 | Loss: 0.00001997
Iteration 27/1000 | Loss: 0.00001997
Iteration 28/1000 | Loss: 0.00002001
Iteration 29/1000 | Loss: 0.00001997
Iteration 30/1000 | Loss: 0.00001997
Iteration 31/1000 | Loss: 0.00001997
Iteration 32/1000 | Loss: 0.00001997
Iteration 33/1000 | Loss: 0.00001997
Iteration 34/1000 | Loss: 0.00001996
Iteration 35/1000 | Loss: 0.00001993
Iteration 36/1000 | Loss: 0.00002034
Iteration 37/1000 | Loss: 0.00001990
Iteration 38/1000 | Loss: 0.00001984
Iteration 39/1000 | Loss: 0.00001984
Iteration 40/1000 | Loss: 0.00001984
Iteration 41/1000 | Loss: 0.00002014
Iteration 42/1000 | Loss: 0.00001980
Iteration 43/1000 | Loss: 0.00001979
Iteration 44/1000 | Loss: 0.00001979
Iteration 45/1000 | Loss: 0.00001978
Iteration 46/1000 | Loss: 0.00001977
Iteration 47/1000 | Loss: 0.00001977
Iteration 48/1000 | Loss: 0.00001977
Iteration 49/1000 | Loss: 0.00001976
Iteration 50/1000 | Loss: 0.00001976
Iteration 51/1000 | Loss: 0.00001976
Iteration 52/1000 | Loss: 0.00001974
Iteration 53/1000 | Loss: 0.00001973
Iteration 54/1000 | Loss: 0.00001972
Iteration 55/1000 | Loss: 0.00001972
Iteration 56/1000 | Loss: 0.00001972
Iteration 57/1000 | Loss: 0.00001971
Iteration 58/1000 | Loss: 0.00001971
Iteration 59/1000 | Loss: 0.00001970
Iteration 60/1000 | Loss: 0.00001970
Iteration 61/1000 | Loss: 0.00001970
Iteration 62/1000 | Loss: 0.00001966
Iteration 63/1000 | Loss: 0.00001965
Iteration 64/1000 | Loss: 0.00009941
Iteration 65/1000 | Loss: 0.00003220
Iteration 66/1000 | Loss: 0.00002790
Iteration 67/1000 | Loss: 0.00001957
Iteration 68/1000 | Loss: 0.00001956
Iteration 69/1000 | Loss: 0.00002015
Iteration 70/1000 | Loss: 0.00001955
Iteration 71/1000 | Loss: 0.00001955
Iteration 72/1000 | Loss: 0.00001954
Iteration 73/1000 | Loss: 0.00001954
Iteration 74/1000 | Loss: 0.00002623
Iteration 75/1000 | Loss: 0.00001954
Iteration 76/1000 | Loss: 0.00001954
Iteration 77/1000 | Loss: 0.00001954
Iteration 78/1000 | Loss: 0.00001954
Iteration 79/1000 | Loss: 0.00001954
Iteration 80/1000 | Loss: 0.00001954
Iteration 81/1000 | Loss: 0.00001954
Iteration 82/1000 | Loss: 0.00001954
Iteration 83/1000 | Loss: 0.00001954
Iteration 84/1000 | Loss: 0.00001953
Iteration 85/1000 | Loss: 0.00001953
Iteration 86/1000 | Loss: 0.00001953
Iteration 87/1000 | Loss: 0.00001952
Iteration 88/1000 | Loss: 0.00001952
Iteration 89/1000 | Loss: 0.00001952
Iteration 90/1000 | Loss: 0.00001952
Iteration 91/1000 | Loss: 0.00001952
Iteration 92/1000 | Loss: 0.00001952
Iteration 93/1000 | Loss: 0.00001952
Iteration 94/1000 | Loss: 0.00001952
Iteration 95/1000 | Loss: 0.00001952
Iteration 96/1000 | Loss: 0.00001952
Iteration 97/1000 | Loss: 0.00001952
Iteration 98/1000 | Loss: 0.00001952
Iteration 99/1000 | Loss: 0.00001951
Iteration 100/1000 | Loss: 0.00001951
Iteration 101/1000 | Loss: 0.00001951
Iteration 102/1000 | Loss: 0.00001951
Iteration 103/1000 | Loss: 0.00001950
Iteration 104/1000 | Loss: 0.00001950
Iteration 105/1000 | Loss: 0.00001949
Iteration 106/1000 | Loss: 0.00001949
Iteration 107/1000 | Loss: 0.00001949
Iteration 108/1000 | Loss: 0.00001949
Iteration 109/1000 | Loss: 0.00001948
Iteration 110/1000 | Loss: 0.00001948
Iteration 111/1000 | Loss: 0.00001948
Iteration 112/1000 | Loss: 0.00001948
Iteration 113/1000 | Loss: 0.00001948
Iteration 114/1000 | Loss: 0.00001948
Iteration 115/1000 | Loss: 0.00001947
Iteration 116/1000 | Loss: 0.00001947
Iteration 117/1000 | Loss: 0.00001946
Iteration 118/1000 | Loss: 0.00002093
Iteration 119/1000 | Loss: 0.00001995
Iteration 120/1000 | Loss: 0.00001944
Iteration 121/1000 | Loss: 0.00001944
Iteration 122/1000 | Loss: 0.00001944
Iteration 123/1000 | Loss: 0.00001944
Iteration 124/1000 | Loss: 0.00001944
Iteration 125/1000 | Loss: 0.00001944
Iteration 126/1000 | Loss: 0.00001944
Iteration 127/1000 | Loss: 0.00001944
Iteration 128/1000 | Loss: 0.00001944
Iteration 129/1000 | Loss: 0.00001944
Iteration 130/1000 | Loss: 0.00001944
Iteration 131/1000 | Loss: 0.00001944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.944312680279836e-05, 1.944312680279836e-05, 1.944312680279836e-05, 1.944312680279836e-05, 1.944312680279836e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.944312680279836e-05

Optimization complete. Final v2v error: 3.6539955139160156 mm

Highest mean error: 5.965121269226074 mm for frame 197

Lowest mean error: 2.985454559326172 mm for frame 15

Saving results

Total time: 102.76336288452148
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401596
Iteration 2/25 | Loss: 0.00135557
Iteration 3/25 | Loss: 0.00128056
Iteration 4/25 | Loss: 0.00127711
Iteration 5/25 | Loss: 0.00127711
Iteration 6/25 | Loss: 0.00127711
Iteration 7/25 | Loss: 0.00127711
Iteration 8/25 | Loss: 0.00127711
Iteration 9/25 | Loss: 0.00127711
Iteration 10/25 | Loss: 0.00127711
Iteration 11/25 | Loss: 0.00127711
Iteration 12/25 | Loss: 0.00127711
Iteration 13/25 | Loss: 0.00127711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001277109025977552, 0.001277109025977552, 0.001277109025977552, 0.001277109025977552, 0.001277109025977552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001277109025977552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64537442
Iteration 2/25 | Loss: 0.00069378
Iteration 3/25 | Loss: 0.00069378
Iteration 4/25 | Loss: 0.00069378
Iteration 5/25 | Loss: 0.00069378
Iteration 6/25 | Loss: 0.00069378
Iteration 7/25 | Loss: 0.00069378
Iteration 8/25 | Loss: 0.00069378
Iteration 9/25 | Loss: 0.00069378
Iteration 10/25 | Loss: 0.00069378
Iteration 11/25 | Loss: 0.00069378
Iteration 12/25 | Loss: 0.00069378
Iteration 13/25 | Loss: 0.00069378
Iteration 14/25 | Loss: 0.00069378
Iteration 15/25 | Loss: 0.00069378
Iteration 16/25 | Loss: 0.00069378
Iteration 17/25 | Loss: 0.00069378
Iteration 18/25 | Loss: 0.00069378
Iteration 19/25 | Loss: 0.00069378
Iteration 20/25 | Loss: 0.00069378
Iteration 21/25 | Loss: 0.00069378
Iteration 22/25 | Loss: 0.00069378
Iteration 23/25 | Loss: 0.00069378
Iteration 24/25 | Loss: 0.00069378
Iteration 25/25 | Loss: 0.00069378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069378
Iteration 2/1000 | Loss: 0.00003415
Iteration 3/1000 | Loss: 0.00002000
Iteration 4/1000 | Loss: 0.00001760
Iteration 5/1000 | Loss: 0.00001656
Iteration 6/1000 | Loss: 0.00001586
Iteration 7/1000 | Loss: 0.00001537
Iteration 8/1000 | Loss: 0.00001505
Iteration 9/1000 | Loss: 0.00001461
Iteration 10/1000 | Loss: 0.00001419
Iteration 11/1000 | Loss: 0.00001407
Iteration 12/1000 | Loss: 0.00001398
Iteration 13/1000 | Loss: 0.00001394
Iteration 14/1000 | Loss: 0.00001372
Iteration 15/1000 | Loss: 0.00001366
Iteration 16/1000 | Loss: 0.00001364
Iteration 17/1000 | Loss: 0.00001348
Iteration 18/1000 | Loss: 0.00001342
Iteration 19/1000 | Loss: 0.00001337
Iteration 20/1000 | Loss: 0.00001337
Iteration 21/1000 | Loss: 0.00001336
Iteration 22/1000 | Loss: 0.00001334
Iteration 23/1000 | Loss: 0.00001333
Iteration 24/1000 | Loss: 0.00001327
Iteration 25/1000 | Loss: 0.00001323
Iteration 26/1000 | Loss: 0.00001318
Iteration 27/1000 | Loss: 0.00001318
Iteration 28/1000 | Loss: 0.00001313
Iteration 29/1000 | Loss: 0.00001310
Iteration 30/1000 | Loss: 0.00001310
Iteration 31/1000 | Loss: 0.00001306
Iteration 32/1000 | Loss: 0.00001305
Iteration 33/1000 | Loss: 0.00001305
Iteration 34/1000 | Loss: 0.00001304
Iteration 35/1000 | Loss: 0.00001304
Iteration 36/1000 | Loss: 0.00001303
Iteration 37/1000 | Loss: 0.00001303
Iteration 38/1000 | Loss: 0.00001302
Iteration 39/1000 | Loss: 0.00001302
Iteration 40/1000 | Loss: 0.00001302
Iteration 41/1000 | Loss: 0.00001301
Iteration 42/1000 | Loss: 0.00001301
Iteration 43/1000 | Loss: 0.00001300
Iteration 44/1000 | Loss: 0.00001300
Iteration 45/1000 | Loss: 0.00001299
Iteration 46/1000 | Loss: 0.00001299
Iteration 47/1000 | Loss: 0.00001298
Iteration 48/1000 | Loss: 0.00001298
Iteration 49/1000 | Loss: 0.00001298
Iteration 50/1000 | Loss: 0.00001297
Iteration 51/1000 | Loss: 0.00001297
Iteration 52/1000 | Loss: 0.00001296
Iteration 53/1000 | Loss: 0.00001296
Iteration 54/1000 | Loss: 0.00001295
Iteration 55/1000 | Loss: 0.00001295
Iteration 56/1000 | Loss: 0.00001294
Iteration 57/1000 | Loss: 0.00001294
Iteration 58/1000 | Loss: 0.00001294
Iteration 59/1000 | Loss: 0.00001293
Iteration 60/1000 | Loss: 0.00001293
Iteration 61/1000 | Loss: 0.00001293
Iteration 62/1000 | Loss: 0.00001292
Iteration 63/1000 | Loss: 0.00001292
Iteration 64/1000 | Loss: 0.00001291
Iteration 65/1000 | Loss: 0.00001291
Iteration 66/1000 | Loss: 0.00001290
Iteration 67/1000 | Loss: 0.00001289
Iteration 68/1000 | Loss: 0.00001288
Iteration 69/1000 | Loss: 0.00001288
Iteration 70/1000 | Loss: 0.00001288
Iteration 71/1000 | Loss: 0.00001288
Iteration 72/1000 | Loss: 0.00001287
Iteration 73/1000 | Loss: 0.00001287
Iteration 74/1000 | Loss: 0.00001286
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001286
Iteration 77/1000 | Loss: 0.00001285
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001284
Iteration 80/1000 | Loss: 0.00001284
Iteration 81/1000 | Loss: 0.00001283
Iteration 82/1000 | Loss: 0.00001283
Iteration 83/1000 | Loss: 0.00001283
Iteration 84/1000 | Loss: 0.00001283
Iteration 85/1000 | Loss: 0.00001282
Iteration 86/1000 | Loss: 0.00001281
Iteration 87/1000 | Loss: 0.00001281
Iteration 88/1000 | Loss: 0.00001281
Iteration 89/1000 | Loss: 0.00001281
Iteration 90/1000 | Loss: 0.00001281
Iteration 91/1000 | Loss: 0.00001281
Iteration 92/1000 | Loss: 0.00001281
Iteration 93/1000 | Loss: 0.00001281
Iteration 94/1000 | Loss: 0.00001281
Iteration 95/1000 | Loss: 0.00001280
Iteration 96/1000 | Loss: 0.00001280
Iteration 97/1000 | Loss: 0.00001279
Iteration 98/1000 | Loss: 0.00001279
Iteration 99/1000 | Loss: 0.00001279
Iteration 100/1000 | Loss: 0.00001279
Iteration 101/1000 | Loss: 0.00001278
Iteration 102/1000 | Loss: 0.00001278
Iteration 103/1000 | Loss: 0.00001278
Iteration 104/1000 | Loss: 0.00001278
Iteration 105/1000 | Loss: 0.00001278
Iteration 106/1000 | Loss: 0.00001277
Iteration 107/1000 | Loss: 0.00001277
Iteration 108/1000 | Loss: 0.00001277
Iteration 109/1000 | Loss: 0.00001277
Iteration 110/1000 | Loss: 0.00001277
Iteration 111/1000 | Loss: 0.00001277
Iteration 112/1000 | Loss: 0.00001277
Iteration 113/1000 | Loss: 0.00001277
Iteration 114/1000 | Loss: 0.00001277
Iteration 115/1000 | Loss: 0.00001277
Iteration 116/1000 | Loss: 0.00001276
Iteration 117/1000 | Loss: 0.00001276
Iteration 118/1000 | Loss: 0.00001276
Iteration 119/1000 | Loss: 0.00001276
Iteration 120/1000 | Loss: 0.00001276
Iteration 121/1000 | Loss: 0.00001276
Iteration 122/1000 | Loss: 0.00001275
Iteration 123/1000 | Loss: 0.00001275
Iteration 124/1000 | Loss: 0.00001275
Iteration 125/1000 | Loss: 0.00001275
Iteration 126/1000 | Loss: 0.00001274
Iteration 127/1000 | Loss: 0.00001274
Iteration 128/1000 | Loss: 0.00001274
Iteration 129/1000 | Loss: 0.00001273
Iteration 130/1000 | Loss: 0.00001273
Iteration 131/1000 | Loss: 0.00001273
Iteration 132/1000 | Loss: 0.00001273
Iteration 133/1000 | Loss: 0.00001272
Iteration 134/1000 | Loss: 0.00001272
Iteration 135/1000 | Loss: 0.00001272
Iteration 136/1000 | Loss: 0.00001272
Iteration 137/1000 | Loss: 0.00001272
Iteration 138/1000 | Loss: 0.00001271
Iteration 139/1000 | Loss: 0.00001271
Iteration 140/1000 | Loss: 0.00001270
Iteration 141/1000 | Loss: 0.00001270
Iteration 142/1000 | Loss: 0.00001269
Iteration 143/1000 | Loss: 0.00001269
Iteration 144/1000 | Loss: 0.00001269
Iteration 145/1000 | Loss: 0.00001269
Iteration 146/1000 | Loss: 0.00001269
Iteration 147/1000 | Loss: 0.00001269
Iteration 148/1000 | Loss: 0.00001269
Iteration 149/1000 | Loss: 0.00001269
Iteration 150/1000 | Loss: 0.00001268
Iteration 151/1000 | Loss: 0.00001268
Iteration 152/1000 | Loss: 0.00001267
Iteration 153/1000 | Loss: 0.00001267
Iteration 154/1000 | Loss: 0.00001267
Iteration 155/1000 | Loss: 0.00001267
Iteration 156/1000 | Loss: 0.00001267
Iteration 157/1000 | Loss: 0.00001267
Iteration 158/1000 | Loss: 0.00001266
Iteration 159/1000 | Loss: 0.00001266
Iteration 160/1000 | Loss: 0.00001266
Iteration 161/1000 | Loss: 0.00001266
Iteration 162/1000 | Loss: 0.00001265
Iteration 163/1000 | Loss: 0.00001265
Iteration 164/1000 | Loss: 0.00001265
Iteration 165/1000 | Loss: 0.00001265
Iteration 166/1000 | Loss: 0.00001265
Iteration 167/1000 | Loss: 0.00001265
Iteration 168/1000 | Loss: 0.00001265
Iteration 169/1000 | Loss: 0.00001265
Iteration 170/1000 | Loss: 0.00001265
Iteration 171/1000 | Loss: 0.00001265
Iteration 172/1000 | Loss: 0.00001265
Iteration 173/1000 | Loss: 0.00001265
Iteration 174/1000 | Loss: 0.00001265
Iteration 175/1000 | Loss: 0.00001265
Iteration 176/1000 | Loss: 0.00001265
Iteration 177/1000 | Loss: 0.00001265
Iteration 178/1000 | Loss: 0.00001265
Iteration 179/1000 | Loss: 0.00001265
Iteration 180/1000 | Loss: 0.00001265
Iteration 181/1000 | Loss: 0.00001265
Iteration 182/1000 | Loss: 0.00001265
Iteration 183/1000 | Loss: 0.00001265
Iteration 184/1000 | Loss: 0.00001265
Iteration 185/1000 | Loss: 0.00001265
Iteration 186/1000 | Loss: 0.00001265
Iteration 187/1000 | Loss: 0.00001265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.2650416465476155e-05, 1.2650416465476155e-05, 1.2650416465476155e-05, 1.2650416465476155e-05, 1.2650416465476155e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2650416465476155e-05

Optimization complete. Final v2v error: 3.016313314437866 mm

Highest mean error: 3.341365098953247 mm for frame 117

Lowest mean error: 2.7624857425689697 mm for frame 260

Saving results

Total time: 50.2552924156189
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00646279
Iteration 2/25 | Loss: 0.00142918
Iteration 3/25 | Loss: 0.00133652
Iteration 4/25 | Loss: 0.00132943
Iteration 5/25 | Loss: 0.00132780
Iteration 6/25 | Loss: 0.00132780
Iteration 7/25 | Loss: 0.00132780
Iteration 8/25 | Loss: 0.00132780
Iteration 9/25 | Loss: 0.00132780
Iteration 10/25 | Loss: 0.00132780
Iteration 11/25 | Loss: 0.00132780
Iteration 12/25 | Loss: 0.00132780
Iteration 13/25 | Loss: 0.00132780
Iteration 14/25 | Loss: 0.00132780
Iteration 15/25 | Loss: 0.00132780
Iteration 16/25 | Loss: 0.00132780
Iteration 17/25 | Loss: 0.00132780
Iteration 18/25 | Loss: 0.00132780
Iteration 19/25 | Loss: 0.00132780
Iteration 20/25 | Loss: 0.00132780
Iteration 21/25 | Loss: 0.00132780
Iteration 22/25 | Loss: 0.00132780
Iteration 23/25 | Loss: 0.00132780
Iteration 24/25 | Loss: 0.00132780
Iteration 25/25 | Loss: 0.00132780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013278010301291943, 0.0013278010301291943, 0.0013278010301291943, 0.0013278010301291943, 0.0013278010301291943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013278010301291943

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.11908293
Iteration 2/25 | Loss: 0.00084079
Iteration 3/25 | Loss: 0.00084079
Iteration 4/25 | Loss: 0.00084078
Iteration 5/25 | Loss: 0.00084078
Iteration 6/25 | Loss: 0.00084078
Iteration 7/25 | Loss: 0.00084078
Iteration 8/25 | Loss: 0.00084078
Iteration 9/25 | Loss: 0.00084078
Iteration 10/25 | Loss: 0.00084078
Iteration 11/25 | Loss: 0.00084078
Iteration 12/25 | Loss: 0.00084078
Iteration 13/25 | Loss: 0.00084078
Iteration 14/25 | Loss: 0.00084078
Iteration 15/25 | Loss: 0.00084078
Iteration 16/25 | Loss: 0.00084078
Iteration 17/25 | Loss: 0.00084078
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008407824207097292, 0.0008407824207097292, 0.0008407824207097292, 0.0008407824207097292, 0.0008407824207097292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008407824207097292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084078
Iteration 2/1000 | Loss: 0.00004586
Iteration 3/1000 | Loss: 0.00002818
Iteration 4/1000 | Loss: 0.00002442
Iteration 5/1000 | Loss: 0.00002297
Iteration 6/1000 | Loss: 0.00002193
Iteration 7/1000 | Loss: 0.00002101
Iteration 8/1000 | Loss: 0.00002038
Iteration 9/1000 | Loss: 0.00002001
Iteration 10/1000 | Loss: 0.00001969
Iteration 11/1000 | Loss: 0.00001920
Iteration 12/1000 | Loss: 0.00001895
Iteration 13/1000 | Loss: 0.00001880
Iteration 14/1000 | Loss: 0.00001869
Iteration 15/1000 | Loss: 0.00001865
Iteration 16/1000 | Loss: 0.00001847
Iteration 17/1000 | Loss: 0.00001839
Iteration 18/1000 | Loss: 0.00001834
Iteration 19/1000 | Loss: 0.00001832
Iteration 20/1000 | Loss: 0.00001831
Iteration 21/1000 | Loss: 0.00001823
Iteration 22/1000 | Loss: 0.00001822
Iteration 23/1000 | Loss: 0.00001822
Iteration 24/1000 | Loss: 0.00001819
Iteration 25/1000 | Loss: 0.00001816
Iteration 26/1000 | Loss: 0.00001816
Iteration 27/1000 | Loss: 0.00001815
Iteration 28/1000 | Loss: 0.00001814
Iteration 29/1000 | Loss: 0.00001808
Iteration 30/1000 | Loss: 0.00001808
Iteration 31/1000 | Loss: 0.00001807
Iteration 32/1000 | Loss: 0.00001807
Iteration 33/1000 | Loss: 0.00001807
Iteration 34/1000 | Loss: 0.00001806
Iteration 35/1000 | Loss: 0.00001806
Iteration 36/1000 | Loss: 0.00001805
Iteration 37/1000 | Loss: 0.00001803
Iteration 38/1000 | Loss: 0.00001802
Iteration 39/1000 | Loss: 0.00001802
Iteration 40/1000 | Loss: 0.00001802
Iteration 41/1000 | Loss: 0.00001801
Iteration 42/1000 | Loss: 0.00001801
Iteration 43/1000 | Loss: 0.00001801
Iteration 44/1000 | Loss: 0.00001800
Iteration 45/1000 | Loss: 0.00001800
Iteration 46/1000 | Loss: 0.00001799
Iteration 47/1000 | Loss: 0.00001799
Iteration 48/1000 | Loss: 0.00001798
Iteration 49/1000 | Loss: 0.00001798
Iteration 50/1000 | Loss: 0.00001797
Iteration 51/1000 | Loss: 0.00001797
Iteration 52/1000 | Loss: 0.00001797
Iteration 53/1000 | Loss: 0.00001797
Iteration 54/1000 | Loss: 0.00001796
Iteration 55/1000 | Loss: 0.00001796
Iteration 56/1000 | Loss: 0.00001796
Iteration 57/1000 | Loss: 0.00001795
Iteration 58/1000 | Loss: 0.00001795
Iteration 59/1000 | Loss: 0.00001794
Iteration 60/1000 | Loss: 0.00001794
Iteration 61/1000 | Loss: 0.00001793
Iteration 62/1000 | Loss: 0.00001793
Iteration 63/1000 | Loss: 0.00001793
Iteration 64/1000 | Loss: 0.00001792
Iteration 65/1000 | Loss: 0.00001791
Iteration 66/1000 | Loss: 0.00001791
Iteration 67/1000 | Loss: 0.00001791
Iteration 68/1000 | Loss: 0.00001791
Iteration 69/1000 | Loss: 0.00001790
Iteration 70/1000 | Loss: 0.00001790
Iteration 71/1000 | Loss: 0.00001790
Iteration 72/1000 | Loss: 0.00001790
Iteration 73/1000 | Loss: 0.00001790
Iteration 74/1000 | Loss: 0.00001790
Iteration 75/1000 | Loss: 0.00001789
Iteration 76/1000 | Loss: 0.00001789
Iteration 77/1000 | Loss: 0.00001788
Iteration 78/1000 | Loss: 0.00001788
Iteration 79/1000 | Loss: 0.00001787
Iteration 80/1000 | Loss: 0.00001787
Iteration 81/1000 | Loss: 0.00001787
Iteration 82/1000 | Loss: 0.00001787
Iteration 83/1000 | Loss: 0.00001786
Iteration 84/1000 | Loss: 0.00001786
Iteration 85/1000 | Loss: 0.00001786
Iteration 86/1000 | Loss: 0.00001786
Iteration 87/1000 | Loss: 0.00001785
Iteration 88/1000 | Loss: 0.00001785
Iteration 89/1000 | Loss: 0.00001785
Iteration 90/1000 | Loss: 0.00001785
Iteration 91/1000 | Loss: 0.00001785
Iteration 92/1000 | Loss: 0.00001785
Iteration 93/1000 | Loss: 0.00001784
Iteration 94/1000 | Loss: 0.00001784
Iteration 95/1000 | Loss: 0.00001784
Iteration 96/1000 | Loss: 0.00001784
Iteration 97/1000 | Loss: 0.00001783
Iteration 98/1000 | Loss: 0.00001783
Iteration 99/1000 | Loss: 0.00001783
Iteration 100/1000 | Loss: 0.00001782
Iteration 101/1000 | Loss: 0.00001782
Iteration 102/1000 | Loss: 0.00001781
Iteration 103/1000 | Loss: 0.00001781
Iteration 104/1000 | Loss: 0.00001781
Iteration 105/1000 | Loss: 0.00001781
Iteration 106/1000 | Loss: 0.00001781
Iteration 107/1000 | Loss: 0.00001781
Iteration 108/1000 | Loss: 0.00001781
Iteration 109/1000 | Loss: 0.00001781
Iteration 110/1000 | Loss: 0.00001780
Iteration 111/1000 | Loss: 0.00001780
Iteration 112/1000 | Loss: 0.00001780
Iteration 113/1000 | Loss: 0.00001780
Iteration 114/1000 | Loss: 0.00001779
Iteration 115/1000 | Loss: 0.00001778
Iteration 116/1000 | Loss: 0.00001778
Iteration 117/1000 | Loss: 0.00001778
Iteration 118/1000 | Loss: 0.00001777
Iteration 119/1000 | Loss: 0.00001777
Iteration 120/1000 | Loss: 0.00001777
Iteration 121/1000 | Loss: 0.00001777
Iteration 122/1000 | Loss: 0.00001777
Iteration 123/1000 | Loss: 0.00001777
Iteration 124/1000 | Loss: 0.00001776
Iteration 125/1000 | Loss: 0.00001776
Iteration 126/1000 | Loss: 0.00001776
Iteration 127/1000 | Loss: 0.00001776
Iteration 128/1000 | Loss: 0.00001776
Iteration 129/1000 | Loss: 0.00001776
Iteration 130/1000 | Loss: 0.00001776
Iteration 131/1000 | Loss: 0.00001776
Iteration 132/1000 | Loss: 0.00001776
Iteration 133/1000 | Loss: 0.00001776
Iteration 134/1000 | Loss: 0.00001775
Iteration 135/1000 | Loss: 0.00001775
Iteration 136/1000 | Loss: 0.00001775
Iteration 137/1000 | Loss: 0.00001775
Iteration 138/1000 | Loss: 0.00001774
Iteration 139/1000 | Loss: 0.00001774
Iteration 140/1000 | Loss: 0.00001774
Iteration 141/1000 | Loss: 0.00001774
Iteration 142/1000 | Loss: 0.00001773
Iteration 143/1000 | Loss: 0.00001773
Iteration 144/1000 | Loss: 0.00001773
Iteration 145/1000 | Loss: 0.00001773
Iteration 146/1000 | Loss: 0.00001773
Iteration 147/1000 | Loss: 0.00001772
Iteration 148/1000 | Loss: 0.00001772
Iteration 149/1000 | Loss: 0.00001772
Iteration 150/1000 | Loss: 0.00001772
Iteration 151/1000 | Loss: 0.00001772
Iteration 152/1000 | Loss: 0.00001771
Iteration 153/1000 | Loss: 0.00001771
Iteration 154/1000 | Loss: 0.00001771
Iteration 155/1000 | Loss: 0.00001771
Iteration 156/1000 | Loss: 0.00001771
Iteration 157/1000 | Loss: 0.00001770
Iteration 158/1000 | Loss: 0.00001770
Iteration 159/1000 | Loss: 0.00001770
Iteration 160/1000 | Loss: 0.00001770
Iteration 161/1000 | Loss: 0.00001770
Iteration 162/1000 | Loss: 0.00001770
Iteration 163/1000 | Loss: 0.00001770
Iteration 164/1000 | Loss: 0.00001770
Iteration 165/1000 | Loss: 0.00001769
Iteration 166/1000 | Loss: 0.00001769
Iteration 167/1000 | Loss: 0.00001769
Iteration 168/1000 | Loss: 0.00001769
Iteration 169/1000 | Loss: 0.00001769
Iteration 170/1000 | Loss: 0.00001769
Iteration 171/1000 | Loss: 0.00001768
Iteration 172/1000 | Loss: 0.00001768
Iteration 173/1000 | Loss: 0.00001768
Iteration 174/1000 | Loss: 0.00001768
Iteration 175/1000 | Loss: 0.00001768
Iteration 176/1000 | Loss: 0.00001768
Iteration 177/1000 | Loss: 0.00001767
Iteration 178/1000 | Loss: 0.00001767
Iteration 179/1000 | Loss: 0.00001767
Iteration 180/1000 | Loss: 0.00001767
Iteration 181/1000 | Loss: 0.00001767
Iteration 182/1000 | Loss: 0.00001767
Iteration 183/1000 | Loss: 0.00001766
Iteration 184/1000 | Loss: 0.00001766
Iteration 185/1000 | Loss: 0.00001766
Iteration 186/1000 | Loss: 0.00001766
Iteration 187/1000 | Loss: 0.00001766
Iteration 188/1000 | Loss: 0.00001766
Iteration 189/1000 | Loss: 0.00001766
Iteration 190/1000 | Loss: 0.00001766
Iteration 191/1000 | Loss: 0.00001766
Iteration 192/1000 | Loss: 0.00001766
Iteration 193/1000 | Loss: 0.00001766
Iteration 194/1000 | Loss: 0.00001765
Iteration 195/1000 | Loss: 0.00001765
Iteration 196/1000 | Loss: 0.00001765
Iteration 197/1000 | Loss: 0.00001765
Iteration 198/1000 | Loss: 0.00001765
Iteration 199/1000 | Loss: 0.00001765
Iteration 200/1000 | Loss: 0.00001765
Iteration 201/1000 | Loss: 0.00001765
Iteration 202/1000 | Loss: 0.00001765
Iteration 203/1000 | Loss: 0.00001765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.7653983377385885e-05, 1.7653983377385885e-05, 1.7653983377385885e-05, 1.7653983377385885e-05, 1.7653983377385885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7653983377385885e-05

Optimization complete. Final v2v error: 3.4867608547210693 mm

Highest mean error: 3.9676573276519775 mm for frame 195

Lowest mean error: 3.066326379776001 mm for frame 88

Saving results

Total time: 52.08152103424072
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489932
Iteration 2/25 | Loss: 0.00143790
Iteration 3/25 | Loss: 0.00134134
Iteration 4/25 | Loss: 0.00132456
Iteration 5/25 | Loss: 0.00131950
Iteration 6/25 | Loss: 0.00131950
Iteration 7/25 | Loss: 0.00131950
Iteration 8/25 | Loss: 0.00131950
Iteration 9/25 | Loss: 0.00131950
Iteration 10/25 | Loss: 0.00131950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013194993371143937, 0.0013194993371143937, 0.0013194993371143937, 0.0013194993371143937, 0.0013194993371143937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013194993371143937

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44796336
Iteration 2/25 | Loss: 0.00096069
Iteration 3/25 | Loss: 0.00096068
Iteration 4/25 | Loss: 0.00096068
Iteration 5/25 | Loss: 0.00096067
Iteration 6/25 | Loss: 0.00096067
Iteration 7/25 | Loss: 0.00096067
Iteration 8/25 | Loss: 0.00096067
Iteration 9/25 | Loss: 0.00096067
Iteration 10/25 | Loss: 0.00096067
Iteration 11/25 | Loss: 0.00096067
Iteration 12/25 | Loss: 0.00096067
Iteration 13/25 | Loss: 0.00096067
Iteration 14/25 | Loss: 0.00096067
Iteration 15/25 | Loss: 0.00096067
Iteration 16/25 | Loss: 0.00096067
Iteration 17/25 | Loss: 0.00096067
Iteration 18/25 | Loss: 0.00096067
Iteration 19/25 | Loss: 0.00096067
Iteration 20/25 | Loss: 0.00096067
Iteration 21/25 | Loss: 0.00096067
Iteration 22/25 | Loss: 0.00096067
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009606719249859452, 0.0009606719249859452, 0.0009606719249859452, 0.0009606719249859452, 0.0009606719249859452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009606719249859452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096067
Iteration 2/1000 | Loss: 0.00003718
Iteration 3/1000 | Loss: 0.00002487
Iteration 4/1000 | Loss: 0.00002213
Iteration 5/1000 | Loss: 0.00002100
Iteration 6/1000 | Loss: 0.00001998
Iteration 7/1000 | Loss: 0.00001945
Iteration 8/1000 | Loss: 0.00001904
Iteration 9/1000 | Loss: 0.00001869
Iteration 10/1000 | Loss: 0.00001833
Iteration 11/1000 | Loss: 0.00001816
Iteration 12/1000 | Loss: 0.00001813
Iteration 13/1000 | Loss: 0.00001805
Iteration 14/1000 | Loss: 0.00001804
Iteration 15/1000 | Loss: 0.00001788
Iteration 16/1000 | Loss: 0.00001784
Iteration 17/1000 | Loss: 0.00001775
Iteration 18/1000 | Loss: 0.00001771
Iteration 19/1000 | Loss: 0.00001766
Iteration 20/1000 | Loss: 0.00001759
Iteration 21/1000 | Loss: 0.00001758
Iteration 22/1000 | Loss: 0.00001753
Iteration 23/1000 | Loss: 0.00001750
Iteration 24/1000 | Loss: 0.00001750
Iteration 25/1000 | Loss: 0.00001749
Iteration 26/1000 | Loss: 0.00001748
Iteration 27/1000 | Loss: 0.00001748
Iteration 28/1000 | Loss: 0.00001747
Iteration 29/1000 | Loss: 0.00001747
Iteration 30/1000 | Loss: 0.00001746
Iteration 31/1000 | Loss: 0.00001746
Iteration 32/1000 | Loss: 0.00001746
Iteration 33/1000 | Loss: 0.00001746
Iteration 34/1000 | Loss: 0.00001745
Iteration 35/1000 | Loss: 0.00001745
Iteration 36/1000 | Loss: 0.00001745
Iteration 37/1000 | Loss: 0.00001744
Iteration 38/1000 | Loss: 0.00001744
Iteration 39/1000 | Loss: 0.00001744
Iteration 40/1000 | Loss: 0.00001743
Iteration 41/1000 | Loss: 0.00001743
Iteration 42/1000 | Loss: 0.00001743
Iteration 43/1000 | Loss: 0.00001742
Iteration 44/1000 | Loss: 0.00001742
Iteration 45/1000 | Loss: 0.00001742
Iteration 46/1000 | Loss: 0.00001741
Iteration 47/1000 | Loss: 0.00001739
Iteration 48/1000 | Loss: 0.00001738
Iteration 49/1000 | Loss: 0.00001737
Iteration 50/1000 | Loss: 0.00001737
Iteration 51/1000 | Loss: 0.00001736
Iteration 52/1000 | Loss: 0.00001735
Iteration 53/1000 | Loss: 0.00001735
Iteration 54/1000 | Loss: 0.00001735
Iteration 55/1000 | Loss: 0.00001733
Iteration 56/1000 | Loss: 0.00001733
Iteration 57/1000 | Loss: 0.00001733
Iteration 58/1000 | Loss: 0.00001733
Iteration 59/1000 | Loss: 0.00001733
Iteration 60/1000 | Loss: 0.00001732
Iteration 61/1000 | Loss: 0.00001732
Iteration 62/1000 | Loss: 0.00001731
Iteration 63/1000 | Loss: 0.00001731
Iteration 64/1000 | Loss: 0.00001731
Iteration 65/1000 | Loss: 0.00001730
Iteration 66/1000 | Loss: 0.00001729
Iteration 67/1000 | Loss: 0.00001729
Iteration 68/1000 | Loss: 0.00001728
Iteration 69/1000 | Loss: 0.00001728
Iteration 70/1000 | Loss: 0.00001728
Iteration 71/1000 | Loss: 0.00001728
Iteration 72/1000 | Loss: 0.00001727
Iteration 73/1000 | Loss: 0.00001727
Iteration 74/1000 | Loss: 0.00001727
Iteration 75/1000 | Loss: 0.00001727
Iteration 76/1000 | Loss: 0.00001727
Iteration 77/1000 | Loss: 0.00001726
Iteration 78/1000 | Loss: 0.00001726
Iteration 79/1000 | Loss: 0.00001726
Iteration 80/1000 | Loss: 0.00001725
Iteration 81/1000 | Loss: 0.00001725
Iteration 82/1000 | Loss: 0.00001724
Iteration 83/1000 | Loss: 0.00001724
Iteration 84/1000 | Loss: 0.00001724
Iteration 85/1000 | Loss: 0.00001724
Iteration 86/1000 | Loss: 0.00001723
Iteration 87/1000 | Loss: 0.00001723
Iteration 88/1000 | Loss: 0.00001723
Iteration 89/1000 | Loss: 0.00001723
Iteration 90/1000 | Loss: 0.00001723
Iteration 91/1000 | Loss: 0.00001723
Iteration 92/1000 | Loss: 0.00001722
Iteration 93/1000 | Loss: 0.00001722
Iteration 94/1000 | Loss: 0.00001722
Iteration 95/1000 | Loss: 0.00001721
Iteration 96/1000 | Loss: 0.00001721
Iteration 97/1000 | Loss: 0.00001720
Iteration 98/1000 | Loss: 0.00001720
Iteration 99/1000 | Loss: 0.00001720
Iteration 100/1000 | Loss: 0.00001719
Iteration 101/1000 | Loss: 0.00001719
Iteration 102/1000 | Loss: 0.00001719
Iteration 103/1000 | Loss: 0.00001719
Iteration 104/1000 | Loss: 0.00001719
Iteration 105/1000 | Loss: 0.00001719
Iteration 106/1000 | Loss: 0.00001718
Iteration 107/1000 | Loss: 0.00001718
Iteration 108/1000 | Loss: 0.00001718
Iteration 109/1000 | Loss: 0.00001718
Iteration 110/1000 | Loss: 0.00001718
Iteration 111/1000 | Loss: 0.00001718
Iteration 112/1000 | Loss: 0.00001718
Iteration 113/1000 | Loss: 0.00001718
Iteration 114/1000 | Loss: 0.00001718
Iteration 115/1000 | Loss: 0.00001718
Iteration 116/1000 | Loss: 0.00001718
Iteration 117/1000 | Loss: 0.00001718
Iteration 118/1000 | Loss: 0.00001718
Iteration 119/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.7175538232550025e-05, 1.7175538232550025e-05, 1.7175538232550025e-05, 1.7175538232550025e-05, 1.7175538232550025e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7175538232550025e-05

Optimization complete. Final v2v error: 3.4695823192596436 mm

Highest mean error: 3.949070453643799 mm for frame 116

Lowest mean error: 2.9800169467926025 mm for frame 41

Saving results

Total time: 42.83912110328674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00983077
Iteration 2/25 | Loss: 0.00983077
Iteration 3/25 | Loss: 0.00983076
Iteration 4/25 | Loss: 0.00235225
Iteration 5/25 | Loss: 0.00192143
Iteration 6/25 | Loss: 0.00187092
Iteration 7/25 | Loss: 0.00180839
Iteration 8/25 | Loss: 0.00177322
Iteration 9/25 | Loss: 0.00174470
Iteration 10/25 | Loss: 0.00172670
Iteration 11/25 | Loss: 0.00171449
Iteration 12/25 | Loss: 0.00170141
Iteration 13/25 | Loss: 0.00170733
Iteration 14/25 | Loss: 0.00167917
Iteration 15/25 | Loss: 0.00167145
Iteration 16/25 | Loss: 0.00166725
Iteration 17/25 | Loss: 0.00166344
Iteration 18/25 | Loss: 0.00166242
Iteration 19/25 | Loss: 0.00166362
Iteration 20/25 | Loss: 0.00165996
Iteration 21/25 | Loss: 0.00166219
Iteration 22/25 | Loss: 0.00165838
Iteration 23/25 | Loss: 0.00165784
Iteration 24/25 | Loss: 0.00165778
Iteration 25/25 | Loss: 0.00165778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36489379
Iteration 2/25 | Loss: 0.00287154
Iteration 3/25 | Loss: 0.00278694
Iteration 4/25 | Loss: 0.00278694
Iteration 5/25 | Loss: 0.00278694
Iteration 6/25 | Loss: 0.00278694
Iteration 7/25 | Loss: 0.00278694
Iteration 8/25 | Loss: 0.00278694
Iteration 9/25 | Loss: 0.00278694
Iteration 10/25 | Loss: 0.00278694
Iteration 11/25 | Loss: 0.00278694
Iteration 12/25 | Loss: 0.00278694
Iteration 13/25 | Loss: 0.00278694
Iteration 14/25 | Loss: 0.00278694
Iteration 15/25 | Loss: 0.00278694
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0027869350742548704, 0.0027869350742548704, 0.0027869350742548704, 0.0027869350742548704, 0.0027869350742548704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027869350742548704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00278694
Iteration 2/1000 | Loss: 0.00044098
Iteration 3/1000 | Loss: 0.00028699
Iteration 4/1000 | Loss: 0.00047706
Iteration 5/1000 | Loss: 0.00024356
Iteration 6/1000 | Loss: 0.00023408
Iteration 7/1000 | Loss: 0.00022689
Iteration 8/1000 | Loss: 0.00030587
Iteration 9/1000 | Loss: 0.00022838
Iteration 10/1000 | Loss: 0.00018345
Iteration 11/1000 | Loss: 0.00017639
Iteration 12/1000 | Loss: 0.00017205
Iteration 13/1000 | Loss: 0.00016732
Iteration 14/1000 | Loss: 0.00016397
Iteration 15/1000 | Loss: 0.00035173
Iteration 16/1000 | Loss: 0.00128135
Iteration 17/1000 | Loss: 0.00900294
Iteration 18/1000 | Loss: 0.00039194
Iteration 19/1000 | Loss: 0.00025787
Iteration 20/1000 | Loss: 0.00018809
Iteration 21/1000 | Loss: 0.00012668
Iteration 22/1000 | Loss: 0.00008539
Iteration 23/1000 | Loss: 0.00006537
Iteration 24/1000 | Loss: 0.00004823
Iteration 25/1000 | Loss: 0.00003818
Iteration 26/1000 | Loss: 0.00003345
Iteration 27/1000 | Loss: 0.00002894
Iteration 28/1000 | Loss: 0.00002589
Iteration 29/1000 | Loss: 0.00002331
Iteration 30/1000 | Loss: 0.00002121
Iteration 31/1000 | Loss: 0.00001992
Iteration 32/1000 | Loss: 0.00001823
Iteration 33/1000 | Loss: 0.00001707
Iteration 34/1000 | Loss: 0.00001629
Iteration 35/1000 | Loss: 0.00001565
Iteration 36/1000 | Loss: 0.00001523
Iteration 37/1000 | Loss: 0.00001494
Iteration 38/1000 | Loss: 0.00001465
Iteration 39/1000 | Loss: 0.00001453
Iteration 40/1000 | Loss: 0.00001451
Iteration 41/1000 | Loss: 0.00001451
Iteration 42/1000 | Loss: 0.00001447
Iteration 43/1000 | Loss: 0.00001444
Iteration 44/1000 | Loss: 0.00001444
Iteration 45/1000 | Loss: 0.00001444
Iteration 46/1000 | Loss: 0.00001443
Iteration 47/1000 | Loss: 0.00001441
Iteration 48/1000 | Loss: 0.00001441
Iteration 49/1000 | Loss: 0.00001441
Iteration 50/1000 | Loss: 0.00001441
Iteration 51/1000 | Loss: 0.00001441
Iteration 52/1000 | Loss: 0.00001441
Iteration 53/1000 | Loss: 0.00001440
Iteration 54/1000 | Loss: 0.00001439
Iteration 55/1000 | Loss: 0.00001438
Iteration 56/1000 | Loss: 0.00001438
Iteration 57/1000 | Loss: 0.00001438
Iteration 58/1000 | Loss: 0.00001438
Iteration 59/1000 | Loss: 0.00001438
Iteration 60/1000 | Loss: 0.00001438
Iteration 61/1000 | Loss: 0.00001438
Iteration 62/1000 | Loss: 0.00001438
Iteration 63/1000 | Loss: 0.00001438
Iteration 64/1000 | Loss: 0.00001437
Iteration 65/1000 | Loss: 0.00001437
Iteration 66/1000 | Loss: 0.00001437
Iteration 67/1000 | Loss: 0.00001436
Iteration 68/1000 | Loss: 0.00001436
Iteration 69/1000 | Loss: 0.00001435
Iteration 70/1000 | Loss: 0.00001435
Iteration 71/1000 | Loss: 0.00001435
Iteration 72/1000 | Loss: 0.00001435
Iteration 73/1000 | Loss: 0.00001435
Iteration 74/1000 | Loss: 0.00001435
Iteration 75/1000 | Loss: 0.00001435
Iteration 76/1000 | Loss: 0.00001435
Iteration 77/1000 | Loss: 0.00001435
Iteration 78/1000 | Loss: 0.00001435
Iteration 79/1000 | Loss: 0.00001433
Iteration 80/1000 | Loss: 0.00001433
Iteration 81/1000 | Loss: 0.00001431
Iteration 82/1000 | Loss: 0.00001431
Iteration 83/1000 | Loss: 0.00001430
Iteration 84/1000 | Loss: 0.00001430
Iteration 85/1000 | Loss: 0.00001430
Iteration 86/1000 | Loss: 0.00001429
Iteration 87/1000 | Loss: 0.00001429
Iteration 88/1000 | Loss: 0.00001429
Iteration 89/1000 | Loss: 0.00001429
Iteration 90/1000 | Loss: 0.00001429
Iteration 91/1000 | Loss: 0.00001429
Iteration 92/1000 | Loss: 0.00001429
Iteration 93/1000 | Loss: 0.00001428
Iteration 94/1000 | Loss: 0.00001428
Iteration 95/1000 | Loss: 0.00001428
Iteration 96/1000 | Loss: 0.00001428
Iteration 97/1000 | Loss: 0.00001428
Iteration 98/1000 | Loss: 0.00001428
Iteration 99/1000 | Loss: 0.00001428
Iteration 100/1000 | Loss: 0.00001427
Iteration 101/1000 | Loss: 0.00001427
Iteration 102/1000 | Loss: 0.00001427
Iteration 103/1000 | Loss: 0.00001427
Iteration 104/1000 | Loss: 0.00001426
Iteration 105/1000 | Loss: 0.00001425
Iteration 106/1000 | Loss: 0.00001425
Iteration 107/1000 | Loss: 0.00001424
Iteration 108/1000 | Loss: 0.00001424
Iteration 109/1000 | Loss: 0.00001424
Iteration 110/1000 | Loss: 0.00001424
Iteration 111/1000 | Loss: 0.00001424
Iteration 112/1000 | Loss: 0.00001424
Iteration 113/1000 | Loss: 0.00001424
Iteration 114/1000 | Loss: 0.00001424
Iteration 115/1000 | Loss: 0.00001424
Iteration 116/1000 | Loss: 0.00001424
Iteration 117/1000 | Loss: 0.00001423
Iteration 118/1000 | Loss: 0.00001423
Iteration 119/1000 | Loss: 0.00001423
Iteration 120/1000 | Loss: 0.00001422
Iteration 121/1000 | Loss: 0.00001422
Iteration 122/1000 | Loss: 0.00001422
Iteration 123/1000 | Loss: 0.00001421
Iteration 124/1000 | Loss: 0.00001421
Iteration 125/1000 | Loss: 0.00001421
Iteration 126/1000 | Loss: 0.00001421
Iteration 127/1000 | Loss: 0.00001421
Iteration 128/1000 | Loss: 0.00001421
Iteration 129/1000 | Loss: 0.00001421
Iteration 130/1000 | Loss: 0.00001421
Iteration 131/1000 | Loss: 0.00001421
Iteration 132/1000 | Loss: 0.00001421
Iteration 133/1000 | Loss: 0.00001421
Iteration 134/1000 | Loss: 0.00001421
Iteration 135/1000 | Loss: 0.00001421
Iteration 136/1000 | Loss: 0.00001421
Iteration 137/1000 | Loss: 0.00001421
Iteration 138/1000 | Loss: 0.00001421
Iteration 139/1000 | Loss: 0.00001421
Iteration 140/1000 | Loss: 0.00001421
Iteration 141/1000 | Loss: 0.00001421
Iteration 142/1000 | Loss: 0.00001421
Iteration 143/1000 | Loss: 0.00001421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.4206129890226293e-05, 1.4206129890226293e-05, 1.4206129890226293e-05, 1.4206129890226293e-05, 1.4206129890226293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4206129890226293e-05

Optimization complete. Final v2v error: 3.2534494400024414 mm

Highest mean error: 3.4494402408599854 mm for frame 190

Lowest mean error: 3.0911405086517334 mm for frame 47

Saving results

Total time: 115.11554646492004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00741812
Iteration 2/25 | Loss: 0.00201157
Iteration 3/25 | Loss: 0.00174139
Iteration 4/25 | Loss: 0.00170819
Iteration 5/25 | Loss: 0.00164593
Iteration 6/25 | Loss: 0.00163774
Iteration 7/25 | Loss: 0.00163889
Iteration 8/25 | Loss: 0.00163545
Iteration 9/25 | Loss: 0.00162419
Iteration 10/25 | Loss: 0.00161715
Iteration 11/25 | Loss: 0.00161379
Iteration 12/25 | Loss: 0.00161389
Iteration 13/25 | Loss: 0.00161796
Iteration 14/25 | Loss: 0.00161012
Iteration 15/25 | Loss: 0.00160703
Iteration 16/25 | Loss: 0.00160656
Iteration 17/25 | Loss: 0.00160621
Iteration 18/25 | Loss: 0.00160611
Iteration 19/25 | Loss: 0.00160595
Iteration 20/25 | Loss: 0.00160606
Iteration 21/25 | Loss: 0.00160621
Iteration 22/25 | Loss: 0.00160552
Iteration 23/25 | Loss: 0.00160656
Iteration 24/25 | Loss: 0.00160597
Iteration 25/25 | Loss: 0.00160577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70841122
Iteration 2/25 | Loss: 0.00188968
Iteration 3/25 | Loss: 0.00188936
Iteration 4/25 | Loss: 0.00188936
Iteration 5/25 | Loss: 0.00188936
Iteration 6/25 | Loss: 0.00188935
Iteration 7/25 | Loss: 0.00188935
Iteration 8/25 | Loss: 0.00188935
Iteration 9/25 | Loss: 0.00188935
Iteration 10/25 | Loss: 0.00188935
Iteration 11/25 | Loss: 0.00188935
Iteration 12/25 | Loss: 0.00188935
Iteration 13/25 | Loss: 0.00188935
Iteration 14/25 | Loss: 0.00188935
Iteration 15/25 | Loss: 0.00188935
Iteration 16/25 | Loss: 0.00188935
Iteration 17/25 | Loss: 0.00188935
Iteration 18/25 | Loss: 0.00188935
Iteration 19/25 | Loss: 0.00188935
Iteration 20/25 | Loss: 0.00188935
Iteration 21/25 | Loss: 0.00188935
Iteration 22/25 | Loss: 0.00188935
Iteration 23/25 | Loss: 0.00188935
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001889352803118527, 0.001889352803118527, 0.001889352803118527, 0.001889352803118527, 0.001889352803118527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001889352803118527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00188935
Iteration 2/1000 | Loss: 0.00337498
Iteration 3/1000 | Loss: 0.00013481
Iteration 4/1000 | Loss: 0.00007938
Iteration 5/1000 | Loss: 0.00006189
Iteration 6/1000 | Loss: 0.00005453
Iteration 7/1000 | Loss: 0.00005211
Iteration 8/1000 | Loss: 0.00004790
Iteration 9/1000 | Loss: 0.00004887
Iteration 10/1000 | Loss: 0.00004720
Iteration 11/1000 | Loss: 0.00004698
Iteration 12/1000 | Loss: 0.00004356
Iteration 13/1000 | Loss: 0.00004294
Iteration 14/1000 | Loss: 0.00004321
Iteration 15/1000 | Loss: 0.00004139
Iteration 16/1000 | Loss: 0.00004041
Iteration 17/1000 | Loss: 0.00003963
Iteration 18/1000 | Loss: 0.00003919
Iteration 19/1000 | Loss: 0.00004154
Iteration 20/1000 | Loss: 0.00004192
Iteration 21/1000 | Loss: 0.00004212
Iteration 22/1000 | Loss: 0.00004285
Iteration 23/1000 | Loss: 0.00004065
Iteration 24/1000 | Loss: 0.00003946
Iteration 25/1000 | Loss: 0.00004008
Iteration 26/1000 | Loss: 0.00004147
Iteration 27/1000 | Loss: 0.00004103
Iteration 28/1000 | Loss: 0.00003972
Iteration 29/1000 | Loss: 0.00004112
Iteration 30/1000 | Loss: 0.00003963
Iteration 31/1000 | Loss: 0.00004044
Iteration 32/1000 | Loss: 0.00004030
Iteration 33/1000 | Loss: 0.00004108
Iteration 34/1000 | Loss: 0.00004059
Iteration 35/1000 | Loss: 0.00004092
Iteration 36/1000 | Loss: 0.00004016
Iteration 37/1000 | Loss: 0.00004087
Iteration 38/1000 | Loss: 0.00004048
Iteration 39/1000 | Loss: 0.00005761
Iteration 40/1000 | Loss: 0.00005415
Iteration 41/1000 | Loss: 0.00004322
Iteration 42/1000 | Loss: 0.00004560
Iteration 43/1000 | Loss: 0.00004294
Iteration 44/1000 | Loss: 0.00004674
Iteration 45/1000 | Loss: 0.00004318
Iteration 46/1000 | Loss: 0.00004693
Iteration 47/1000 | Loss: 0.00004309
Iteration 48/1000 | Loss: 0.00004309
Iteration 49/1000 | Loss: 0.00004309
Iteration 50/1000 | Loss: 0.00004309
Iteration 51/1000 | Loss: 0.00005342
Iteration 52/1000 | Loss: 0.00004793
Iteration 53/1000 | Loss: 0.00006522
Iteration 54/1000 | Loss: 0.00005331
Iteration 55/1000 | Loss: 0.00005672
Iteration 56/1000 | Loss: 0.00004205
Iteration 57/1000 | Loss: 0.00003924
Iteration 58/1000 | Loss: 0.00003852
Iteration 59/1000 | Loss: 0.00003841
Iteration 60/1000 | Loss: 0.00004103
Iteration 61/1000 | Loss: 0.00003769
Iteration 62/1000 | Loss: 0.00003979
Iteration 63/1000 | Loss: 0.00004186
Iteration 64/1000 | Loss: 0.00003815
Iteration 65/1000 | Loss: 0.00003761
Iteration 66/1000 | Loss: 0.00003740
Iteration 67/1000 | Loss: 0.00003735
Iteration 68/1000 | Loss: 0.00004034
Iteration 69/1000 | Loss: 0.00003847
Iteration 70/1000 | Loss: 0.00004031
Iteration 71/1000 | Loss: 0.00003877
Iteration 72/1000 | Loss: 0.00003718
Iteration 73/1000 | Loss: 0.00003781
Iteration 74/1000 | Loss: 0.00003738
Iteration 75/1000 | Loss: 0.00003793
Iteration 76/1000 | Loss: 0.00003730
Iteration 77/1000 | Loss: 0.00003710
Iteration 78/1000 | Loss: 0.00003709
Iteration 79/1000 | Loss: 0.00003709
Iteration 80/1000 | Loss: 0.00003709
Iteration 81/1000 | Loss: 0.00003709
Iteration 82/1000 | Loss: 0.00003709
Iteration 83/1000 | Loss: 0.00003708
Iteration 84/1000 | Loss: 0.00003708
Iteration 85/1000 | Loss: 0.00003708
Iteration 86/1000 | Loss: 0.00003708
Iteration 87/1000 | Loss: 0.00003708
Iteration 88/1000 | Loss: 0.00003807
Iteration 89/1000 | Loss: 0.00003727
Iteration 90/1000 | Loss: 0.00003725
Iteration 91/1000 | Loss: 0.00003705
Iteration 92/1000 | Loss: 0.00003704
Iteration 93/1000 | Loss: 0.00003704
Iteration 94/1000 | Loss: 0.00003704
Iteration 95/1000 | Loss: 0.00003704
Iteration 96/1000 | Loss: 0.00003704
Iteration 97/1000 | Loss: 0.00003704
Iteration 98/1000 | Loss: 0.00003703
Iteration 99/1000 | Loss: 0.00003703
Iteration 100/1000 | Loss: 0.00003703
Iteration 101/1000 | Loss: 0.00003702
Iteration 102/1000 | Loss: 0.00003702
Iteration 103/1000 | Loss: 0.00003702
Iteration 104/1000 | Loss: 0.00003702
Iteration 105/1000 | Loss: 0.00003702
Iteration 106/1000 | Loss: 0.00003702
Iteration 107/1000 | Loss: 0.00003702
Iteration 108/1000 | Loss: 0.00003701
Iteration 109/1000 | Loss: 0.00003701
Iteration 110/1000 | Loss: 0.00003700
Iteration 111/1000 | Loss: 0.00003700
Iteration 112/1000 | Loss: 0.00003700
Iteration 113/1000 | Loss: 0.00003699
Iteration 114/1000 | Loss: 0.00003699
Iteration 115/1000 | Loss: 0.00003699
Iteration 116/1000 | Loss: 0.00003698
Iteration 117/1000 | Loss: 0.00003698
Iteration 118/1000 | Loss: 0.00003698
Iteration 119/1000 | Loss: 0.00003698
Iteration 120/1000 | Loss: 0.00003697
Iteration 121/1000 | Loss: 0.00003697
Iteration 122/1000 | Loss: 0.00003696
Iteration 123/1000 | Loss: 0.00003696
Iteration 124/1000 | Loss: 0.00003695
Iteration 125/1000 | Loss: 0.00003695
Iteration 126/1000 | Loss: 0.00003695
Iteration 127/1000 | Loss: 0.00003694
Iteration 128/1000 | Loss: 0.00003692
Iteration 129/1000 | Loss: 0.00003692
Iteration 130/1000 | Loss: 0.00003691
Iteration 131/1000 | Loss: 0.00003691
Iteration 132/1000 | Loss: 0.00003691
Iteration 133/1000 | Loss: 0.00003690
Iteration 134/1000 | Loss: 0.00003690
Iteration 135/1000 | Loss: 0.00003690
Iteration 136/1000 | Loss: 0.00003690
Iteration 137/1000 | Loss: 0.00003689
Iteration 138/1000 | Loss: 0.00003689
Iteration 139/1000 | Loss: 0.00003689
Iteration 140/1000 | Loss: 0.00003689
Iteration 141/1000 | Loss: 0.00003688
Iteration 142/1000 | Loss: 0.00003688
Iteration 143/1000 | Loss: 0.00003688
Iteration 144/1000 | Loss: 0.00003688
Iteration 145/1000 | Loss: 0.00003687
Iteration 146/1000 | Loss: 0.00003687
Iteration 147/1000 | Loss: 0.00003687
Iteration 148/1000 | Loss: 0.00003686
Iteration 149/1000 | Loss: 0.00003686
Iteration 150/1000 | Loss: 0.00003686
Iteration 151/1000 | Loss: 0.00003686
Iteration 152/1000 | Loss: 0.00003686
Iteration 153/1000 | Loss: 0.00003685
Iteration 154/1000 | Loss: 0.00003685
Iteration 155/1000 | Loss: 0.00003685
Iteration 156/1000 | Loss: 0.00003685
Iteration 157/1000 | Loss: 0.00003685
Iteration 158/1000 | Loss: 0.00003685
Iteration 159/1000 | Loss: 0.00003685
Iteration 160/1000 | Loss: 0.00003685
Iteration 161/1000 | Loss: 0.00003685
Iteration 162/1000 | Loss: 0.00003685
Iteration 163/1000 | Loss: 0.00003685
Iteration 164/1000 | Loss: 0.00003685
Iteration 165/1000 | Loss: 0.00003685
Iteration 166/1000 | Loss: 0.00003685
Iteration 167/1000 | Loss: 0.00003685
Iteration 168/1000 | Loss: 0.00003685
Iteration 169/1000 | Loss: 0.00003685
Iteration 170/1000 | Loss: 0.00003684
Iteration 171/1000 | Loss: 0.00003684
Iteration 172/1000 | Loss: 0.00003684
Iteration 173/1000 | Loss: 0.00003684
Iteration 174/1000 | Loss: 0.00003684
Iteration 175/1000 | Loss: 0.00003684
Iteration 176/1000 | Loss: 0.00003684
Iteration 177/1000 | Loss: 0.00003684
Iteration 178/1000 | Loss: 0.00003684
Iteration 179/1000 | Loss: 0.00003684
Iteration 180/1000 | Loss: 0.00003684
Iteration 181/1000 | Loss: 0.00003684
Iteration 182/1000 | Loss: 0.00003684
Iteration 183/1000 | Loss: 0.00003684
Iteration 184/1000 | Loss: 0.00003684
Iteration 185/1000 | Loss: 0.00003683
Iteration 186/1000 | Loss: 0.00003683
Iteration 187/1000 | Loss: 0.00003683
Iteration 188/1000 | Loss: 0.00003683
Iteration 189/1000 | Loss: 0.00003683
Iteration 190/1000 | Loss: 0.00003683
Iteration 191/1000 | Loss: 0.00003683
Iteration 192/1000 | Loss: 0.00003683
Iteration 193/1000 | Loss: 0.00003682
Iteration 194/1000 | Loss: 0.00003682
Iteration 195/1000 | Loss: 0.00003682
Iteration 196/1000 | Loss: 0.00003682
Iteration 197/1000 | Loss: 0.00003682
Iteration 198/1000 | Loss: 0.00003682
Iteration 199/1000 | Loss: 0.00003682
Iteration 200/1000 | Loss: 0.00003681
Iteration 201/1000 | Loss: 0.00003681
Iteration 202/1000 | Loss: 0.00003681
Iteration 203/1000 | Loss: 0.00003681
Iteration 204/1000 | Loss: 0.00003681
Iteration 205/1000 | Loss: 0.00003681
Iteration 206/1000 | Loss: 0.00003681
Iteration 207/1000 | Loss: 0.00003680
Iteration 208/1000 | Loss: 0.00003680
Iteration 209/1000 | Loss: 0.00003680
Iteration 210/1000 | Loss: 0.00003680
Iteration 211/1000 | Loss: 0.00003680
Iteration 212/1000 | Loss: 0.00003680
Iteration 213/1000 | Loss: 0.00003680
Iteration 214/1000 | Loss: 0.00003680
Iteration 215/1000 | Loss: 0.00003680
Iteration 216/1000 | Loss: 0.00003680
Iteration 217/1000 | Loss: 0.00003680
Iteration 218/1000 | Loss: 0.00003680
Iteration 219/1000 | Loss: 0.00003680
Iteration 220/1000 | Loss: 0.00003680
Iteration 221/1000 | Loss: 0.00003680
Iteration 222/1000 | Loss: 0.00003679
Iteration 223/1000 | Loss: 0.00003679
Iteration 224/1000 | Loss: 0.00003679
Iteration 225/1000 | Loss: 0.00003679
Iteration 226/1000 | Loss: 0.00003679
Iteration 227/1000 | Loss: 0.00003679
Iteration 228/1000 | Loss: 0.00003679
Iteration 229/1000 | Loss: 0.00003679
Iteration 230/1000 | Loss: 0.00003679
Iteration 231/1000 | Loss: 0.00003679
Iteration 232/1000 | Loss: 0.00003679
Iteration 233/1000 | Loss: 0.00003679
Iteration 234/1000 | Loss: 0.00003679
Iteration 235/1000 | Loss: 0.00003679
Iteration 236/1000 | Loss: 0.00003679
Iteration 237/1000 | Loss: 0.00003679
Iteration 238/1000 | Loss: 0.00003679
Iteration 239/1000 | Loss: 0.00003679
Iteration 240/1000 | Loss: 0.00003679
Iteration 241/1000 | Loss: 0.00003679
Iteration 242/1000 | Loss: 0.00003678
Iteration 243/1000 | Loss: 0.00003678
Iteration 244/1000 | Loss: 0.00003678
Iteration 245/1000 | Loss: 0.00003678
Iteration 246/1000 | Loss: 0.00003678
Iteration 247/1000 | Loss: 0.00003678
Iteration 248/1000 | Loss: 0.00003678
Iteration 249/1000 | Loss: 0.00003678
Iteration 250/1000 | Loss: 0.00003678
Iteration 251/1000 | Loss: 0.00003678
Iteration 252/1000 | Loss: 0.00003678
Iteration 253/1000 | Loss: 0.00003678
Iteration 254/1000 | Loss: 0.00003678
Iteration 255/1000 | Loss: 0.00003678
Iteration 256/1000 | Loss: 0.00003678
Iteration 257/1000 | Loss: 0.00003678
Iteration 258/1000 | Loss: 0.00003678
Iteration 259/1000 | Loss: 0.00003678
Iteration 260/1000 | Loss: 0.00003678
Iteration 261/1000 | Loss: 0.00003678
Iteration 262/1000 | Loss: 0.00003678
Iteration 263/1000 | Loss: 0.00003678
Iteration 264/1000 | Loss: 0.00003678
Iteration 265/1000 | Loss: 0.00003678
Iteration 266/1000 | Loss: 0.00003678
Iteration 267/1000 | Loss: 0.00003678
Iteration 268/1000 | Loss: 0.00003678
Iteration 269/1000 | Loss: 0.00003678
Iteration 270/1000 | Loss: 0.00003678
Iteration 271/1000 | Loss: 0.00003678
Iteration 272/1000 | Loss: 0.00003678
Iteration 273/1000 | Loss: 0.00003678
Iteration 274/1000 | Loss: 0.00003678
Iteration 275/1000 | Loss: 0.00003678
Iteration 276/1000 | Loss: 0.00003678
Iteration 277/1000 | Loss: 0.00003678
Iteration 278/1000 | Loss: 0.00003678
Iteration 279/1000 | Loss: 0.00003678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [3.6779518268303946e-05, 3.6779518268303946e-05, 3.6779518268303946e-05, 3.6779518268303946e-05, 3.6779518268303946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6779518268303946e-05

Optimization complete. Final v2v error: 4.833430290222168 mm

Highest mean error: 7.73157262802124 mm for frame 178

Lowest mean error: 3.2805092334747314 mm for frame 239

Saving results

Total time: 182.67320680618286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970452
Iteration 2/25 | Loss: 0.00970452
Iteration 3/25 | Loss: 0.00970452
Iteration 4/25 | Loss: 0.00970452
Iteration 5/25 | Loss: 0.00970452
Iteration 6/25 | Loss: 0.00970451
Iteration 7/25 | Loss: 0.00970451
Iteration 8/25 | Loss: 0.00970451
Iteration 9/25 | Loss: 0.00970451
Iteration 10/25 | Loss: 0.00970450
Iteration 11/25 | Loss: 0.00970450
Iteration 12/25 | Loss: 0.00970450
Iteration 13/25 | Loss: 0.00970450
Iteration 14/25 | Loss: 0.00970450
Iteration 15/25 | Loss: 0.00970449
Iteration 16/25 | Loss: 0.00970449
Iteration 17/25 | Loss: 0.00970449
Iteration 18/25 | Loss: 0.00970449
Iteration 19/25 | Loss: 0.00970449
Iteration 20/25 | Loss: 0.00970448
Iteration 21/25 | Loss: 0.00970448
Iteration 22/25 | Loss: 0.00970448
Iteration 23/25 | Loss: 0.00970448
Iteration 24/25 | Loss: 0.00970448
Iteration 25/25 | Loss: 0.00970448

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53585780
Iteration 2/25 | Loss: 0.16841388
Iteration 3/25 | Loss: 0.16841105
Iteration 4/25 | Loss: 0.16841100
Iteration 5/25 | Loss: 0.16841100
Iteration 6/25 | Loss: 0.16841099
Iteration 7/25 | Loss: 0.16841099
Iteration 8/25 | Loss: 0.16841096
Iteration 9/25 | Loss: 0.16841096
Iteration 10/25 | Loss: 0.16841096
Iteration 11/25 | Loss: 0.16841096
Iteration 12/25 | Loss: 0.16841096
Iteration 13/25 | Loss: 0.16841096
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.16841095685958862, 0.16841095685958862, 0.16841095685958862, 0.16841095685958862, 0.16841095685958862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.16841095685958862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16841096
Iteration 2/1000 | Loss: 0.00280259
Iteration 3/1000 | Loss: 0.00088763
Iteration 4/1000 | Loss: 0.00053153
Iteration 5/1000 | Loss: 0.00127235
Iteration 6/1000 | Loss: 0.00016380
Iteration 7/1000 | Loss: 0.00120875
Iteration 8/1000 | Loss: 0.00014323
Iteration 9/1000 | Loss: 0.00010850
Iteration 10/1000 | Loss: 0.00007723
Iteration 11/1000 | Loss: 0.00039486
Iteration 12/1000 | Loss: 0.00010742
Iteration 13/1000 | Loss: 0.00007650
Iteration 14/1000 | Loss: 0.00021467
Iteration 15/1000 | Loss: 0.00012055
Iteration 16/1000 | Loss: 0.00004169
Iteration 17/1000 | Loss: 0.00003494
Iteration 18/1000 | Loss: 0.00002926
Iteration 19/1000 | Loss: 0.00029062
Iteration 20/1000 | Loss: 0.00005498
Iteration 21/1000 | Loss: 0.00024669
Iteration 22/1000 | Loss: 0.00007283
Iteration 23/1000 | Loss: 0.00003243
Iteration 24/1000 | Loss: 0.00002538
Iteration 25/1000 | Loss: 0.00010787
Iteration 26/1000 | Loss: 0.00002809
Iteration 27/1000 | Loss: 0.00003373
Iteration 28/1000 | Loss: 0.00002351
Iteration 29/1000 | Loss: 0.00011837
Iteration 30/1000 | Loss: 0.00004194
Iteration 31/1000 | Loss: 0.00002223
Iteration 32/1000 | Loss: 0.00009431
Iteration 33/1000 | Loss: 0.00018478
Iteration 34/1000 | Loss: 0.00002290
Iteration 35/1000 | Loss: 0.00004057
Iteration 36/1000 | Loss: 0.00002141
Iteration 37/1000 | Loss: 0.00005534
Iteration 38/1000 | Loss: 0.00003994
Iteration 39/1000 | Loss: 0.00003432
Iteration 40/1000 | Loss: 0.00002282
Iteration 41/1000 | Loss: 0.00002120
Iteration 42/1000 | Loss: 0.00002120
Iteration 43/1000 | Loss: 0.00002120
Iteration 44/1000 | Loss: 0.00002120
Iteration 45/1000 | Loss: 0.00002120
Iteration 46/1000 | Loss: 0.00002119
Iteration 47/1000 | Loss: 0.00002119
Iteration 48/1000 | Loss: 0.00002137
Iteration 49/1000 | Loss: 0.00002137
Iteration 50/1000 | Loss: 0.00002118
Iteration 51/1000 | Loss: 0.00002117
Iteration 52/1000 | Loss: 0.00002117
Iteration 53/1000 | Loss: 0.00002117
Iteration 54/1000 | Loss: 0.00002117
Iteration 55/1000 | Loss: 0.00002117
Iteration 56/1000 | Loss: 0.00002116
Iteration 57/1000 | Loss: 0.00002116
Iteration 58/1000 | Loss: 0.00002116
Iteration 59/1000 | Loss: 0.00002116
Iteration 60/1000 | Loss: 0.00002116
Iteration 61/1000 | Loss: 0.00002116
Iteration 62/1000 | Loss: 0.00002116
Iteration 63/1000 | Loss: 0.00002116
Iteration 64/1000 | Loss: 0.00002116
Iteration 65/1000 | Loss: 0.00002116
Iteration 66/1000 | Loss: 0.00002116
Iteration 67/1000 | Loss: 0.00002116
Iteration 68/1000 | Loss: 0.00002116
Iteration 69/1000 | Loss: 0.00002116
Iteration 70/1000 | Loss: 0.00002116
Iteration 71/1000 | Loss: 0.00002116
Iteration 72/1000 | Loss: 0.00002116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [2.1160085452720523e-05, 2.1160085452720523e-05, 2.1160085452720523e-05, 2.1160085452720523e-05, 2.1160085452720523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1160085452720523e-05

Optimization complete. Final v2v error: 3.8814430236816406 mm

Highest mean error: 4.073413848876953 mm for frame 227

Lowest mean error: 3.8197739124298096 mm for frame 20

Saving results

Total time: 73.24357032775879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01146949
Iteration 2/25 | Loss: 0.00441997
Iteration 3/25 | Loss: 0.00307954
Iteration 4/25 | Loss: 0.00319442
Iteration 5/25 | Loss: 0.00256856
Iteration 6/25 | Loss: 0.00242094
Iteration 7/25 | Loss: 0.00225977
Iteration 8/25 | Loss: 0.00228065
Iteration 9/25 | Loss: 0.00217742
Iteration 10/25 | Loss: 0.00216021
Iteration 11/25 | Loss: 0.00217174
Iteration 12/25 | Loss: 0.00214839
Iteration 13/25 | Loss: 0.00213498
Iteration 14/25 | Loss: 0.00212926
Iteration 15/25 | Loss: 0.00212760
Iteration 16/25 | Loss: 0.00212928
Iteration 17/25 | Loss: 0.00212850
Iteration 18/25 | Loss: 0.00212783
Iteration 19/25 | Loss: 0.00212858
Iteration 20/25 | Loss: 0.00212700
Iteration 21/25 | Loss: 0.00212702
Iteration 22/25 | Loss: 0.00212896
Iteration 23/25 | Loss: 0.00212518
Iteration 24/25 | Loss: 0.00212297
Iteration 25/25 | Loss: 0.00212281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.51884699
Iteration 2/25 | Loss: 0.03596781
Iteration 3/25 | Loss: 0.00675772
Iteration 4/25 | Loss: 0.00675772
Iteration 5/25 | Loss: 0.00675772
Iteration 6/25 | Loss: 0.00675771
Iteration 7/25 | Loss: 0.00675771
Iteration 8/25 | Loss: 0.00675771
Iteration 9/25 | Loss: 0.00675771
Iteration 10/25 | Loss: 0.00675771
Iteration 11/25 | Loss: 0.00675771
Iteration 12/25 | Loss: 0.00675771
Iteration 13/25 | Loss: 0.00675771
Iteration 14/25 | Loss: 0.00675771
Iteration 15/25 | Loss: 0.00675771
Iteration 16/25 | Loss: 0.00675771
Iteration 17/25 | Loss: 0.00675771
Iteration 18/25 | Loss: 0.00675771
Iteration 19/25 | Loss: 0.00675771
Iteration 20/25 | Loss: 0.00675771
Iteration 21/25 | Loss: 0.00675771
Iteration 22/25 | Loss: 0.00675771
Iteration 23/25 | Loss: 0.00675771
Iteration 24/25 | Loss: 0.00675771
Iteration 25/25 | Loss: 0.00675771

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00675771
Iteration 2/1000 | Loss: 0.00081158
Iteration 3/1000 | Loss: 0.00064430
Iteration 4/1000 | Loss: 0.00044822
Iteration 5/1000 | Loss: 0.00040131
Iteration 6/1000 | Loss: 0.00036051
Iteration 7/1000 | Loss: 0.00032977
Iteration 8/1000 | Loss: 0.00089405
Iteration 9/1000 | Loss: 0.00747921
Iteration 10/1000 | Loss: 0.02115270
Iteration 11/1000 | Loss: 0.00895567
Iteration 12/1000 | Loss: 0.00494843
Iteration 13/1000 | Loss: 0.00208852
Iteration 14/1000 | Loss: 0.00032165
Iteration 15/1000 | Loss: 0.00079982
Iteration 16/1000 | Loss: 0.00097157
Iteration 17/1000 | Loss: 0.00014848
Iteration 18/1000 | Loss: 0.00012250
Iteration 19/1000 | Loss: 0.00010231
Iteration 20/1000 | Loss: 0.00009119
Iteration 21/1000 | Loss: 0.00008170
Iteration 22/1000 | Loss: 0.00007536
Iteration 23/1000 | Loss: 0.00006989
Iteration 24/1000 | Loss: 0.00006577
Iteration 25/1000 | Loss: 0.00006299
Iteration 26/1000 | Loss: 0.00006078
Iteration 27/1000 | Loss: 0.00005924
Iteration 28/1000 | Loss: 0.00005831
Iteration 29/1000 | Loss: 0.00005733
Iteration 30/1000 | Loss: 0.00005677
Iteration 31/1000 | Loss: 0.00005635
Iteration 32/1000 | Loss: 0.00005606
Iteration 33/1000 | Loss: 0.00005575
Iteration 34/1000 | Loss: 0.00005548
Iteration 35/1000 | Loss: 0.00005532
Iteration 36/1000 | Loss: 0.00005529
Iteration 37/1000 | Loss: 0.00005521
Iteration 38/1000 | Loss: 0.00005506
Iteration 39/1000 | Loss: 0.00014319
Iteration 40/1000 | Loss: 0.00005638
Iteration 41/1000 | Loss: 0.00005511
Iteration 42/1000 | Loss: 0.00005450
Iteration 43/1000 | Loss: 0.00005399
Iteration 44/1000 | Loss: 0.00005350
Iteration 45/1000 | Loss: 0.00005318
Iteration 46/1000 | Loss: 0.00005294
Iteration 47/1000 | Loss: 0.00005277
Iteration 48/1000 | Loss: 0.00005273
Iteration 49/1000 | Loss: 0.00005260
Iteration 50/1000 | Loss: 0.00005249
Iteration 51/1000 | Loss: 0.00005248
Iteration 52/1000 | Loss: 0.00005247
Iteration 53/1000 | Loss: 0.00005247
Iteration 54/1000 | Loss: 0.00005246
Iteration 55/1000 | Loss: 0.00005244
Iteration 56/1000 | Loss: 0.00005244
Iteration 57/1000 | Loss: 0.00005243
Iteration 58/1000 | Loss: 0.00005242
Iteration 59/1000 | Loss: 0.00005241
Iteration 60/1000 | Loss: 0.00005240
Iteration 61/1000 | Loss: 0.00005240
Iteration 62/1000 | Loss: 0.00005239
Iteration 63/1000 | Loss: 0.00005235
Iteration 64/1000 | Loss: 0.00005235
Iteration 65/1000 | Loss: 0.00005234
Iteration 66/1000 | Loss: 0.00005233
Iteration 67/1000 | Loss: 0.00005232
Iteration 68/1000 | Loss: 0.00005232
Iteration 69/1000 | Loss: 0.00005229
Iteration 70/1000 | Loss: 0.00005228
Iteration 71/1000 | Loss: 0.00005227
Iteration 72/1000 | Loss: 0.00005227
Iteration 73/1000 | Loss: 0.00005227
Iteration 74/1000 | Loss: 0.00005226
Iteration 75/1000 | Loss: 0.00005226
Iteration 76/1000 | Loss: 0.00005226
Iteration 77/1000 | Loss: 0.00005225
Iteration 78/1000 | Loss: 0.00005225
Iteration 79/1000 | Loss: 0.00005225
Iteration 80/1000 | Loss: 0.00005225
Iteration 81/1000 | Loss: 0.00005224
Iteration 82/1000 | Loss: 0.00005224
Iteration 83/1000 | Loss: 0.00005224
Iteration 84/1000 | Loss: 0.00005223
Iteration 85/1000 | Loss: 0.00005223
Iteration 86/1000 | Loss: 0.00005223
Iteration 87/1000 | Loss: 0.00005223
Iteration 88/1000 | Loss: 0.00005223
Iteration 89/1000 | Loss: 0.00005223
Iteration 90/1000 | Loss: 0.00005223
Iteration 91/1000 | Loss: 0.00005223
Iteration 92/1000 | Loss: 0.00005223
Iteration 93/1000 | Loss: 0.00005223
Iteration 94/1000 | Loss: 0.00005223
Iteration 95/1000 | Loss: 0.00005223
Iteration 96/1000 | Loss: 0.00005223
Iteration 97/1000 | Loss: 0.00005223
Iteration 98/1000 | Loss: 0.00005223
Iteration 99/1000 | Loss: 0.00005223
Iteration 100/1000 | Loss: 0.00005222
Iteration 101/1000 | Loss: 0.00005222
Iteration 102/1000 | Loss: 0.00005222
Iteration 103/1000 | Loss: 0.00005222
Iteration 104/1000 | Loss: 0.00005222
Iteration 105/1000 | Loss: 0.00005222
Iteration 106/1000 | Loss: 0.00005222
Iteration 107/1000 | Loss: 0.00005222
Iteration 108/1000 | Loss: 0.00005222
Iteration 109/1000 | Loss: 0.00005222
Iteration 110/1000 | Loss: 0.00005222
Iteration 111/1000 | Loss: 0.00005222
Iteration 112/1000 | Loss: 0.00005222
Iteration 113/1000 | Loss: 0.00005222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [5.221978426561691e-05, 5.221978426561691e-05, 5.221978426561691e-05, 5.221978426561691e-05, 5.221978426561691e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.221978426561691e-05

Optimization complete. Final v2v error: 5.560903072357178 mm

Highest mean error: 11.477822303771973 mm for frame 239

Lowest mean error: 3.9975028038024902 mm for frame 4

Saving results

Total time: 138.94158148765564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803888
Iteration 2/25 | Loss: 0.00134313
Iteration 3/25 | Loss: 0.00127124
Iteration 4/25 | Loss: 0.00126269
Iteration 5/25 | Loss: 0.00126061
Iteration 6/25 | Loss: 0.00126061
Iteration 7/25 | Loss: 0.00126061
Iteration 8/25 | Loss: 0.00126061
Iteration 9/25 | Loss: 0.00126061
Iteration 10/25 | Loss: 0.00126061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012606051750481129, 0.0012606051750481129, 0.0012606051750481129, 0.0012606051750481129, 0.0012606051750481129]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012606051750481129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40320778
Iteration 2/25 | Loss: 0.00080134
Iteration 3/25 | Loss: 0.00080134
Iteration 4/25 | Loss: 0.00080134
Iteration 5/25 | Loss: 0.00080134
Iteration 6/25 | Loss: 0.00080134
Iteration 7/25 | Loss: 0.00080134
Iteration 8/25 | Loss: 0.00080134
Iteration 9/25 | Loss: 0.00080134
Iteration 10/25 | Loss: 0.00080134
Iteration 11/25 | Loss: 0.00080134
Iteration 12/25 | Loss: 0.00080134
Iteration 13/25 | Loss: 0.00080134
Iteration 14/25 | Loss: 0.00080134
Iteration 15/25 | Loss: 0.00080134
Iteration 16/25 | Loss: 0.00080134
Iteration 17/25 | Loss: 0.00080134
Iteration 18/25 | Loss: 0.00080134
Iteration 19/25 | Loss: 0.00080134
Iteration 20/25 | Loss: 0.00080134
Iteration 21/25 | Loss: 0.00080134
Iteration 22/25 | Loss: 0.00080134
Iteration 23/25 | Loss: 0.00080134
Iteration 24/25 | Loss: 0.00080134
Iteration 25/25 | Loss: 0.00080134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080134
Iteration 2/1000 | Loss: 0.00002585
Iteration 3/1000 | Loss: 0.00001824
Iteration 4/1000 | Loss: 0.00001591
Iteration 5/1000 | Loss: 0.00001513
Iteration 6/1000 | Loss: 0.00001431
Iteration 7/1000 | Loss: 0.00001364
Iteration 8/1000 | Loss: 0.00001334
Iteration 9/1000 | Loss: 0.00001311
Iteration 10/1000 | Loss: 0.00001284
Iteration 11/1000 | Loss: 0.00001284
Iteration 12/1000 | Loss: 0.00001277
Iteration 13/1000 | Loss: 0.00001276
Iteration 14/1000 | Loss: 0.00001273
Iteration 15/1000 | Loss: 0.00001261
Iteration 16/1000 | Loss: 0.00001260
Iteration 17/1000 | Loss: 0.00001258
Iteration 18/1000 | Loss: 0.00001255
Iteration 19/1000 | Loss: 0.00001253
Iteration 20/1000 | Loss: 0.00001252
Iteration 21/1000 | Loss: 0.00001249
Iteration 22/1000 | Loss: 0.00001243
Iteration 23/1000 | Loss: 0.00001241
Iteration 24/1000 | Loss: 0.00001240
Iteration 25/1000 | Loss: 0.00001240
Iteration 26/1000 | Loss: 0.00001239
Iteration 27/1000 | Loss: 0.00001239
Iteration 28/1000 | Loss: 0.00001238
Iteration 29/1000 | Loss: 0.00001237
Iteration 30/1000 | Loss: 0.00001236
Iteration 31/1000 | Loss: 0.00001235
Iteration 32/1000 | Loss: 0.00001235
Iteration 33/1000 | Loss: 0.00001235
Iteration 34/1000 | Loss: 0.00001235
Iteration 35/1000 | Loss: 0.00001235
Iteration 36/1000 | Loss: 0.00001235
Iteration 37/1000 | Loss: 0.00001235
Iteration 38/1000 | Loss: 0.00001235
Iteration 39/1000 | Loss: 0.00001235
Iteration 40/1000 | Loss: 0.00001234
Iteration 41/1000 | Loss: 0.00001234
Iteration 42/1000 | Loss: 0.00001234
Iteration 43/1000 | Loss: 0.00001233
Iteration 44/1000 | Loss: 0.00001231
Iteration 45/1000 | Loss: 0.00001231
Iteration 46/1000 | Loss: 0.00001231
Iteration 47/1000 | Loss: 0.00001230
Iteration 48/1000 | Loss: 0.00001230
Iteration 49/1000 | Loss: 0.00001230
Iteration 50/1000 | Loss: 0.00001229
Iteration 51/1000 | Loss: 0.00001229
Iteration 52/1000 | Loss: 0.00001228
Iteration 53/1000 | Loss: 0.00001226
Iteration 54/1000 | Loss: 0.00001225
Iteration 55/1000 | Loss: 0.00001223
Iteration 56/1000 | Loss: 0.00001219
Iteration 57/1000 | Loss: 0.00001215
Iteration 58/1000 | Loss: 0.00001215
Iteration 59/1000 | Loss: 0.00001215
Iteration 60/1000 | Loss: 0.00001214
Iteration 61/1000 | Loss: 0.00001214
Iteration 62/1000 | Loss: 0.00001214
Iteration 63/1000 | Loss: 0.00001214
Iteration 64/1000 | Loss: 0.00001214
Iteration 65/1000 | Loss: 0.00001214
Iteration 66/1000 | Loss: 0.00001214
Iteration 67/1000 | Loss: 0.00001214
Iteration 68/1000 | Loss: 0.00001213
Iteration 69/1000 | Loss: 0.00001212
Iteration 70/1000 | Loss: 0.00001211
Iteration 71/1000 | Loss: 0.00001211
Iteration 72/1000 | Loss: 0.00001210
Iteration 73/1000 | Loss: 0.00001210
Iteration 74/1000 | Loss: 0.00001209
Iteration 75/1000 | Loss: 0.00001209
Iteration 76/1000 | Loss: 0.00001208
Iteration 77/1000 | Loss: 0.00001208
Iteration 78/1000 | Loss: 0.00001208
Iteration 79/1000 | Loss: 0.00001208
Iteration 80/1000 | Loss: 0.00001208
Iteration 81/1000 | Loss: 0.00001207
Iteration 82/1000 | Loss: 0.00001207
Iteration 83/1000 | Loss: 0.00001207
Iteration 84/1000 | Loss: 0.00001206
Iteration 85/1000 | Loss: 0.00001205
Iteration 86/1000 | Loss: 0.00001204
Iteration 87/1000 | Loss: 0.00001204
Iteration 88/1000 | Loss: 0.00001204
Iteration 89/1000 | Loss: 0.00001204
Iteration 90/1000 | Loss: 0.00001204
Iteration 91/1000 | Loss: 0.00001204
Iteration 92/1000 | Loss: 0.00001203
Iteration 93/1000 | Loss: 0.00001203
Iteration 94/1000 | Loss: 0.00001202
Iteration 95/1000 | Loss: 0.00001202
Iteration 96/1000 | Loss: 0.00001202
Iteration 97/1000 | Loss: 0.00001202
Iteration 98/1000 | Loss: 0.00001202
Iteration 99/1000 | Loss: 0.00001202
Iteration 100/1000 | Loss: 0.00001201
Iteration 101/1000 | Loss: 0.00001201
Iteration 102/1000 | Loss: 0.00001200
Iteration 103/1000 | Loss: 0.00001200
Iteration 104/1000 | Loss: 0.00001200
Iteration 105/1000 | Loss: 0.00001200
Iteration 106/1000 | Loss: 0.00001200
Iteration 107/1000 | Loss: 0.00001200
Iteration 108/1000 | Loss: 0.00001200
Iteration 109/1000 | Loss: 0.00001200
Iteration 110/1000 | Loss: 0.00001200
Iteration 111/1000 | Loss: 0.00001199
Iteration 112/1000 | Loss: 0.00001199
Iteration 113/1000 | Loss: 0.00001199
Iteration 114/1000 | Loss: 0.00001198
Iteration 115/1000 | Loss: 0.00001198
Iteration 116/1000 | Loss: 0.00001198
Iteration 117/1000 | Loss: 0.00001197
Iteration 118/1000 | Loss: 0.00001197
Iteration 119/1000 | Loss: 0.00001197
Iteration 120/1000 | Loss: 0.00001197
Iteration 121/1000 | Loss: 0.00001197
Iteration 122/1000 | Loss: 0.00001196
Iteration 123/1000 | Loss: 0.00001196
Iteration 124/1000 | Loss: 0.00001196
Iteration 125/1000 | Loss: 0.00001196
Iteration 126/1000 | Loss: 0.00001196
Iteration 127/1000 | Loss: 0.00001196
Iteration 128/1000 | Loss: 0.00001195
Iteration 129/1000 | Loss: 0.00001195
Iteration 130/1000 | Loss: 0.00001195
Iteration 131/1000 | Loss: 0.00001195
Iteration 132/1000 | Loss: 0.00001195
Iteration 133/1000 | Loss: 0.00001195
Iteration 134/1000 | Loss: 0.00001195
Iteration 135/1000 | Loss: 0.00001195
Iteration 136/1000 | Loss: 0.00001194
Iteration 137/1000 | Loss: 0.00001194
Iteration 138/1000 | Loss: 0.00001194
Iteration 139/1000 | Loss: 0.00001194
Iteration 140/1000 | Loss: 0.00001194
Iteration 141/1000 | Loss: 0.00001194
Iteration 142/1000 | Loss: 0.00001194
Iteration 143/1000 | Loss: 0.00001194
Iteration 144/1000 | Loss: 0.00001194
Iteration 145/1000 | Loss: 0.00001194
Iteration 146/1000 | Loss: 0.00001193
Iteration 147/1000 | Loss: 0.00001193
Iteration 148/1000 | Loss: 0.00001193
Iteration 149/1000 | Loss: 0.00001193
Iteration 150/1000 | Loss: 0.00001193
Iteration 151/1000 | Loss: 0.00001193
Iteration 152/1000 | Loss: 0.00001193
Iteration 153/1000 | Loss: 0.00001193
Iteration 154/1000 | Loss: 0.00001193
Iteration 155/1000 | Loss: 0.00001193
Iteration 156/1000 | Loss: 0.00001193
Iteration 157/1000 | Loss: 0.00001192
Iteration 158/1000 | Loss: 0.00001192
Iteration 159/1000 | Loss: 0.00001192
Iteration 160/1000 | Loss: 0.00001192
Iteration 161/1000 | Loss: 0.00001192
Iteration 162/1000 | Loss: 0.00001192
Iteration 163/1000 | Loss: 0.00001192
Iteration 164/1000 | Loss: 0.00001191
Iteration 165/1000 | Loss: 0.00001191
Iteration 166/1000 | Loss: 0.00001191
Iteration 167/1000 | Loss: 0.00001191
Iteration 168/1000 | Loss: 0.00001191
Iteration 169/1000 | Loss: 0.00001191
Iteration 170/1000 | Loss: 0.00001191
Iteration 171/1000 | Loss: 0.00001191
Iteration 172/1000 | Loss: 0.00001190
Iteration 173/1000 | Loss: 0.00001190
Iteration 174/1000 | Loss: 0.00001190
Iteration 175/1000 | Loss: 0.00001189
Iteration 176/1000 | Loss: 0.00001189
Iteration 177/1000 | Loss: 0.00001189
Iteration 178/1000 | Loss: 0.00001189
Iteration 179/1000 | Loss: 0.00001189
Iteration 180/1000 | Loss: 0.00001189
Iteration 181/1000 | Loss: 0.00001188
Iteration 182/1000 | Loss: 0.00001188
Iteration 183/1000 | Loss: 0.00001188
Iteration 184/1000 | Loss: 0.00001188
Iteration 185/1000 | Loss: 0.00001188
Iteration 186/1000 | Loss: 0.00001188
Iteration 187/1000 | Loss: 0.00001188
Iteration 188/1000 | Loss: 0.00001188
Iteration 189/1000 | Loss: 0.00001188
Iteration 190/1000 | Loss: 0.00001187
Iteration 191/1000 | Loss: 0.00001187
Iteration 192/1000 | Loss: 0.00001187
Iteration 193/1000 | Loss: 0.00001187
Iteration 194/1000 | Loss: 0.00001186
Iteration 195/1000 | Loss: 0.00001186
Iteration 196/1000 | Loss: 0.00001186
Iteration 197/1000 | Loss: 0.00001186
Iteration 198/1000 | Loss: 0.00001186
Iteration 199/1000 | Loss: 0.00001186
Iteration 200/1000 | Loss: 0.00001186
Iteration 201/1000 | Loss: 0.00001186
Iteration 202/1000 | Loss: 0.00001186
Iteration 203/1000 | Loss: 0.00001186
Iteration 204/1000 | Loss: 0.00001186
Iteration 205/1000 | Loss: 0.00001186
Iteration 206/1000 | Loss: 0.00001186
Iteration 207/1000 | Loss: 0.00001186
Iteration 208/1000 | Loss: 0.00001185
Iteration 209/1000 | Loss: 0.00001185
Iteration 210/1000 | Loss: 0.00001185
Iteration 211/1000 | Loss: 0.00001185
Iteration 212/1000 | Loss: 0.00001185
Iteration 213/1000 | Loss: 0.00001185
Iteration 214/1000 | Loss: 0.00001185
Iteration 215/1000 | Loss: 0.00001185
Iteration 216/1000 | Loss: 0.00001185
Iteration 217/1000 | Loss: 0.00001185
Iteration 218/1000 | Loss: 0.00001185
Iteration 219/1000 | Loss: 0.00001185
Iteration 220/1000 | Loss: 0.00001185
Iteration 221/1000 | Loss: 0.00001185
Iteration 222/1000 | Loss: 0.00001185
Iteration 223/1000 | Loss: 0.00001185
Iteration 224/1000 | Loss: 0.00001185
Iteration 225/1000 | Loss: 0.00001185
Iteration 226/1000 | Loss: 0.00001185
Iteration 227/1000 | Loss: 0.00001185
Iteration 228/1000 | Loss: 0.00001185
Iteration 229/1000 | Loss: 0.00001185
Iteration 230/1000 | Loss: 0.00001185
Iteration 231/1000 | Loss: 0.00001185
Iteration 232/1000 | Loss: 0.00001185
Iteration 233/1000 | Loss: 0.00001185
Iteration 234/1000 | Loss: 0.00001185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [1.184502434625756e-05, 1.184502434625756e-05, 1.184502434625756e-05, 1.184502434625756e-05, 1.184502434625756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.184502434625756e-05

Optimization complete. Final v2v error: 2.932140827178955 mm

Highest mean error: 3.149545669555664 mm for frame 67

Lowest mean error: 2.785891532897949 mm for frame 169

Saving results

Total time: 44.89309477806091
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00709968
Iteration 2/25 | Loss: 0.00155454
Iteration 3/25 | Loss: 0.00142361
Iteration 4/25 | Loss: 0.00140883
Iteration 5/25 | Loss: 0.00140530
Iteration 6/25 | Loss: 0.00140530
Iteration 7/25 | Loss: 0.00140530
Iteration 8/25 | Loss: 0.00140530
Iteration 9/25 | Loss: 0.00140530
Iteration 10/25 | Loss: 0.00140530
Iteration 11/25 | Loss: 0.00140530
Iteration 12/25 | Loss: 0.00140530
Iteration 13/25 | Loss: 0.00140530
Iteration 14/25 | Loss: 0.00140530
Iteration 15/25 | Loss: 0.00140530
Iteration 16/25 | Loss: 0.00140530
Iteration 17/25 | Loss: 0.00140530
Iteration 18/25 | Loss: 0.00140530
Iteration 19/25 | Loss: 0.00140530
Iteration 20/25 | Loss: 0.00140530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014053038321435452, 0.0014053038321435452, 0.0014053038321435452, 0.0014053038321435452, 0.0014053038321435452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014053038321435452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.17112827
Iteration 2/25 | Loss: 0.00115712
Iteration 3/25 | Loss: 0.00115709
Iteration 4/25 | Loss: 0.00115709
Iteration 5/25 | Loss: 0.00115709
Iteration 6/25 | Loss: 0.00115709
Iteration 7/25 | Loss: 0.00115709
Iteration 8/25 | Loss: 0.00115709
Iteration 9/25 | Loss: 0.00115709
Iteration 10/25 | Loss: 0.00115709
Iteration 11/25 | Loss: 0.00115709
Iteration 12/25 | Loss: 0.00115709
Iteration 13/25 | Loss: 0.00115709
Iteration 14/25 | Loss: 0.00115709
Iteration 15/25 | Loss: 0.00115709
Iteration 16/25 | Loss: 0.00115709
Iteration 17/25 | Loss: 0.00115709
Iteration 18/25 | Loss: 0.00115709
Iteration 19/25 | Loss: 0.00115709
Iteration 20/25 | Loss: 0.00115709
Iteration 21/25 | Loss: 0.00115709
Iteration 22/25 | Loss: 0.00115709
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001157086226157844, 0.001157086226157844, 0.001157086226157844, 0.001157086226157844, 0.001157086226157844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001157086226157844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115709
Iteration 2/1000 | Loss: 0.00004919
Iteration 3/1000 | Loss: 0.00003297
Iteration 4/1000 | Loss: 0.00003006
Iteration 5/1000 | Loss: 0.00002827
Iteration 6/1000 | Loss: 0.00002705
Iteration 7/1000 | Loss: 0.00002659
Iteration 8/1000 | Loss: 0.00002622
Iteration 9/1000 | Loss: 0.00002596
Iteration 10/1000 | Loss: 0.00002574
Iteration 11/1000 | Loss: 0.00002557
Iteration 12/1000 | Loss: 0.00002553
Iteration 13/1000 | Loss: 0.00002540
Iteration 14/1000 | Loss: 0.00002534
Iteration 15/1000 | Loss: 0.00002534
Iteration 16/1000 | Loss: 0.00002529
Iteration 17/1000 | Loss: 0.00002529
Iteration 18/1000 | Loss: 0.00002529
Iteration 19/1000 | Loss: 0.00002529
Iteration 20/1000 | Loss: 0.00002528
Iteration 21/1000 | Loss: 0.00002528
Iteration 22/1000 | Loss: 0.00002527
Iteration 23/1000 | Loss: 0.00002527
Iteration 24/1000 | Loss: 0.00002526
Iteration 25/1000 | Loss: 0.00002526
Iteration 26/1000 | Loss: 0.00002526
Iteration 27/1000 | Loss: 0.00002525
Iteration 28/1000 | Loss: 0.00002525
Iteration 29/1000 | Loss: 0.00002524
Iteration 30/1000 | Loss: 0.00002524
Iteration 31/1000 | Loss: 0.00002524
Iteration 32/1000 | Loss: 0.00002524
Iteration 33/1000 | Loss: 0.00002524
Iteration 34/1000 | Loss: 0.00002523
Iteration 35/1000 | Loss: 0.00002523
Iteration 36/1000 | Loss: 0.00002523
Iteration 37/1000 | Loss: 0.00002521
Iteration 38/1000 | Loss: 0.00002521
Iteration 39/1000 | Loss: 0.00002521
Iteration 40/1000 | Loss: 0.00002521
Iteration 41/1000 | Loss: 0.00002521
Iteration 42/1000 | Loss: 0.00002521
Iteration 43/1000 | Loss: 0.00002521
Iteration 44/1000 | Loss: 0.00002520
Iteration 45/1000 | Loss: 0.00002519
Iteration 46/1000 | Loss: 0.00002519
Iteration 47/1000 | Loss: 0.00002518
Iteration 48/1000 | Loss: 0.00002518
Iteration 49/1000 | Loss: 0.00002517
Iteration 50/1000 | Loss: 0.00002517
Iteration 51/1000 | Loss: 0.00002517
Iteration 52/1000 | Loss: 0.00002517
Iteration 53/1000 | Loss: 0.00002517
Iteration 54/1000 | Loss: 0.00002517
Iteration 55/1000 | Loss: 0.00002517
Iteration 56/1000 | Loss: 0.00002517
Iteration 57/1000 | Loss: 0.00002517
Iteration 58/1000 | Loss: 0.00002517
Iteration 59/1000 | Loss: 0.00002517
Iteration 60/1000 | Loss: 0.00002516
Iteration 61/1000 | Loss: 0.00002516
Iteration 62/1000 | Loss: 0.00002516
Iteration 63/1000 | Loss: 0.00002516
Iteration 64/1000 | Loss: 0.00002515
Iteration 65/1000 | Loss: 0.00002515
Iteration 66/1000 | Loss: 0.00002514
Iteration 67/1000 | Loss: 0.00002513
Iteration 68/1000 | Loss: 0.00002513
Iteration 69/1000 | Loss: 0.00002513
Iteration 70/1000 | Loss: 0.00002513
Iteration 71/1000 | Loss: 0.00002513
Iteration 72/1000 | Loss: 0.00002513
Iteration 73/1000 | Loss: 0.00002512
Iteration 74/1000 | Loss: 0.00002511
Iteration 75/1000 | Loss: 0.00002510
Iteration 76/1000 | Loss: 0.00002510
Iteration 77/1000 | Loss: 0.00002509
Iteration 78/1000 | Loss: 0.00002508
Iteration 79/1000 | Loss: 0.00002508
Iteration 80/1000 | Loss: 0.00002507
Iteration 81/1000 | Loss: 0.00002506
Iteration 82/1000 | Loss: 0.00002505
Iteration 83/1000 | Loss: 0.00002504
Iteration 84/1000 | Loss: 0.00002504
Iteration 85/1000 | Loss: 0.00002504
Iteration 86/1000 | Loss: 0.00002500
Iteration 87/1000 | Loss: 0.00002499
Iteration 88/1000 | Loss: 0.00002499
Iteration 89/1000 | Loss: 0.00002498
Iteration 90/1000 | Loss: 0.00002496
Iteration 91/1000 | Loss: 0.00002496
Iteration 92/1000 | Loss: 0.00002494
Iteration 93/1000 | Loss: 0.00002494
Iteration 94/1000 | Loss: 0.00002494
Iteration 95/1000 | Loss: 0.00002494
Iteration 96/1000 | Loss: 0.00002494
Iteration 97/1000 | Loss: 0.00002494
Iteration 98/1000 | Loss: 0.00002494
Iteration 99/1000 | Loss: 0.00002494
Iteration 100/1000 | Loss: 0.00002494
Iteration 101/1000 | Loss: 0.00002494
Iteration 102/1000 | Loss: 0.00002493
Iteration 103/1000 | Loss: 0.00002493
Iteration 104/1000 | Loss: 0.00002493
Iteration 105/1000 | Loss: 0.00002493
Iteration 106/1000 | Loss: 0.00002493
Iteration 107/1000 | Loss: 0.00002493
Iteration 108/1000 | Loss: 0.00002493
Iteration 109/1000 | Loss: 0.00002493
Iteration 110/1000 | Loss: 0.00002493
Iteration 111/1000 | Loss: 0.00002493
Iteration 112/1000 | Loss: 0.00002493
Iteration 113/1000 | Loss: 0.00002493
Iteration 114/1000 | Loss: 0.00002491
Iteration 115/1000 | Loss: 0.00002491
Iteration 116/1000 | Loss: 0.00002491
Iteration 117/1000 | Loss: 0.00002490
Iteration 118/1000 | Loss: 0.00002490
Iteration 119/1000 | Loss: 0.00002490
Iteration 120/1000 | Loss: 0.00002490
Iteration 121/1000 | Loss: 0.00002489
Iteration 122/1000 | Loss: 0.00002489
Iteration 123/1000 | Loss: 0.00002489
Iteration 124/1000 | Loss: 0.00002488
Iteration 125/1000 | Loss: 0.00002488
Iteration 126/1000 | Loss: 0.00002488
Iteration 127/1000 | Loss: 0.00002487
Iteration 128/1000 | Loss: 0.00002487
Iteration 129/1000 | Loss: 0.00002486
Iteration 130/1000 | Loss: 0.00002486
Iteration 131/1000 | Loss: 0.00002486
Iteration 132/1000 | Loss: 0.00002486
Iteration 133/1000 | Loss: 0.00002486
Iteration 134/1000 | Loss: 0.00002486
Iteration 135/1000 | Loss: 0.00002486
Iteration 136/1000 | Loss: 0.00002486
Iteration 137/1000 | Loss: 0.00002486
Iteration 138/1000 | Loss: 0.00002486
Iteration 139/1000 | Loss: 0.00002486
Iteration 140/1000 | Loss: 0.00002486
Iteration 141/1000 | Loss: 0.00002486
Iteration 142/1000 | Loss: 0.00002486
Iteration 143/1000 | Loss: 0.00002485
Iteration 144/1000 | Loss: 0.00002485
Iteration 145/1000 | Loss: 0.00002485
Iteration 146/1000 | Loss: 0.00002484
Iteration 147/1000 | Loss: 0.00002484
Iteration 148/1000 | Loss: 0.00002484
Iteration 149/1000 | Loss: 0.00002483
Iteration 150/1000 | Loss: 0.00002483
Iteration 151/1000 | Loss: 0.00002483
Iteration 152/1000 | Loss: 0.00002483
Iteration 153/1000 | Loss: 0.00002483
Iteration 154/1000 | Loss: 0.00002483
Iteration 155/1000 | Loss: 0.00002482
Iteration 156/1000 | Loss: 0.00002482
Iteration 157/1000 | Loss: 0.00002482
Iteration 158/1000 | Loss: 0.00002482
Iteration 159/1000 | Loss: 0.00002482
Iteration 160/1000 | Loss: 0.00002482
Iteration 161/1000 | Loss: 0.00002482
Iteration 162/1000 | Loss: 0.00002482
Iteration 163/1000 | Loss: 0.00002482
Iteration 164/1000 | Loss: 0.00002482
Iteration 165/1000 | Loss: 0.00002482
Iteration 166/1000 | Loss: 0.00002482
Iteration 167/1000 | Loss: 0.00002482
Iteration 168/1000 | Loss: 0.00002482
Iteration 169/1000 | Loss: 0.00002482
Iteration 170/1000 | Loss: 0.00002482
Iteration 171/1000 | Loss: 0.00002482
Iteration 172/1000 | Loss: 0.00002482
Iteration 173/1000 | Loss: 0.00002482
Iteration 174/1000 | Loss: 0.00002482
Iteration 175/1000 | Loss: 0.00002482
Iteration 176/1000 | Loss: 0.00002482
Iteration 177/1000 | Loss: 0.00002482
Iteration 178/1000 | Loss: 0.00002482
Iteration 179/1000 | Loss: 0.00002482
Iteration 180/1000 | Loss: 0.00002482
Iteration 181/1000 | Loss: 0.00002482
Iteration 182/1000 | Loss: 0.00002482
Iteration 183/1000 | Loss: 0.00002482
Iteration 184/1000 | Loss: 0.00002482
Iteration 185/1000 | Loss: 0.00002482
Iteration 186/1000 | Loss: 0.00002482
Iteration 187/1000 | Loss: 0.00002482
Iteration 188/1000 | Loss: 0.00002482
Iteration 189/1000 | Loss: 0.00002482
Iteration 190/1000 | Loss: 0.00002482
Iteration 191/1000 | Loss: 0.00002482
Iteration 192/1000 | Loss: 0.00002482
Iteration 193/1000 | Loss: 0.00002482
Iteration 194/1000 | Loss: 0.00002482
Iteration 195/1000 | Loss: 0.00002482
Iteration 196/1000 | Loss: 0.00002482
Iteration 197/1000 | Loss: 0.00002482
Iteration 198/1000 | Loss: 0.00002482
Iteration 199/1000 | Loss: 0.00002482
Iteration 200/1000 | Loss: 0.00002482
Iteration 201/1000 | Loss: 0.00002482
Iteration 202/1000 | Loss: 0.00002482
Iteration 203/1000 | Loss: 0.00002482
Iteration 204/1000 | Loss: 0.00002482
Iteration 205/1000 | Loss: 0.00002482
Iteration 206/1000 | Loss: 0.00002482
Iteration 207/1000 | Loss: 0.00002482
Iteration 208/1000 | Loss: 0.00002482
Iteration 209/1000 | Loss: 0.00002482
Iteration 210/1000 | Loss: 0.00002482
Iteration 211/1000 | Loss: 0.00002482
Iteration 212/1000 | Loss: 0.00002482
Iteration 213/1000 | Loss: 0.00002482
Iteration 214/1000 | Loss: 0.00002482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.4821703846100718e-05, 2.4821703846100718e-05, 2.4821703846100718e-05, 2.4821703846100718e-05, 2.4821703846100718e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4821703846100718e-05

Optimization complete. Final v2v error: 4.153061389923096 mm

Highest mean error: 4.537021160125732 mm for frame 59

Lowest mean error: 3.6594278812408447 mm for frame 239

Saving results

Total time: 46.122613191604614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00945154
Iteration 2/25 | Loss: 0.00199461
Iteration 3/25 | Loss: 0.00158714
Iteration 4/25 | Loss: 0.00157393
Iteration 5/25 | Loss: 0.00157106
Iteration 6/25 | Loss: 0.00157072
Iteration 7/25 | Loss: 0.00157072
Iteration 8/25 | Loss: 0.00157072
Iteration 9/25 | Loss: 0.00157072
Iteration 10/25 | Loss: 0.00157072
Iteration 11/25 | Loss: 0.00157072
Iteration 12/25 | Loss: 0.00157072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015707205748185515, 0.0015707205748185515, 0.0015707205748185515, 0.0015707205748185515, 0.0015707205748185515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015707205748185515

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23718774
Iteration 2/25 | Loss: 0.00114230
Iteration 3/25 | Loss: 0.00114230
Iteration 4/25 | Loss: 0.00114230
Iteration 5/25 | Loss: 0.00114230
Iteration 6/25 | Loss: 0.00114230
Iteration 7/25 | Loss: 0.00114230
Iteration 8/25 | Loss: 0.00114230
Iteration 9/25 | Loss: 0.00114230
Iteration 10/25 | Loss: 0.00114230
Iteration 11/25 | Loss: 0.00114230
Iteration 12/25 | Loss: 0.00114230
Iteration 13/25 | Loss: 0.00114230
Iteration 14/25 | Loss: 0.00114230
Iteration 15/25 | Loss: 0.00114230
Iteration 16/25 | Loss: 0.00114230
Iteration 17/25 | Loss: 0.00114230
Iteration 18/25 | Loss: 0.00114230
Iteration 19/25 | Loss: 0.00114230
Iteration 20/25 | Loss: 0.00114230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011422964744269848, 0.0011422964744269848, 0.0011422964744269848, 0.0011422964744269848, 0.0011422964744269848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011422964744269848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114230
Iteration 2/1000 | Loss: 0.00007834
Iteration 3/1000 | Loss: 0.00005558
Iteration 4/1000 | Loss: 0.00004497
Iteration 5/1000 | Loss: 0.00004149
Iteration 6/1000 | Loss: 0.00004020
Iteration 7/1000 | Loss: 0.00003917
Iteration 8/1000 | Loss: 0.00003831
Iteration 9/1000 | Loss: 0.00003744
Iteration 10/1000 | Loss: 0.00003696
Iteration 11/1000 | Loss: 0.00003626
Iteration 12/1000 | Loss: 0.00003588
Iteration 13/1000 | Loss: 0.00003559
Iteration 14/1000 | Loss: 0.00003525
Iteration 15/1000 | Loss: 0.00003492
Iteration 16/1000 | Loss: 0.00003470
Iteration 17/1000 | Loss: 0.00003448
Iteration 18/1000 | Loss: 0.00003429
Iteration 19/1000 | Loss: 0.00003415
Iteration 20/1000 | Loss: 0.00003412
Iteration 21/1000 | Loss: 0.00003407
Iteration 22/1000 | Loss: 0.00003402
Iteration 23/1000 | Loss: 0.00003388
Iteration 24/1000 | Loss: 0.00003381
Iteration 25/1000 | Loss: 0.00003371
Iteration 26/1000 | Loss: 0.00003362
Iteration 27/1000 | Loss: 0.00003353
Iteration 28/1000 | Loss: 0.00003352
Iteration 29/1000 | Loss: 0.00003352
Iteration 30/1000 | Loss: 0.00003351
Iteration 31/1000 | Loss: 0.00003351
Iteration 32/1000 | Loss: 0.00003350
Iteration 33/1000 | Loss: 0.00003350
Iteration 34/1000 | Loss: 0.00003350
Iteration 35/1000 | Loss: 0.00003349
Iteration 36/1000 | Loss: 0.00003349
Iteration 37/1000 | Loss: 0.00003349
Iteration 38/1000 | Loss: 0.00003349
Iteration 39/1000 | Loss: 0.00003349
Iteration 40/1000 | Loss: 0.00003349
Iteration 41/1000 | Loss: 0.00003349
Iteration 42/1000 | Loss: 0.00003346
Iteration 43/1000 | Loss: 0.00003345
Iteration 44/1000 | Loss: 0.00003344
Iteration 45/1000 | Loss: 0.00003341
Iteration 46/1000 | Loss: 0.00003341
Iteration 47/1000 | Loss: 0.00003341
Iteration 48/1000 | Loss: 0.00003341
Iteration 49/1000 | Loss: 0.00003341
Iteration 50/1000 | Loss: 0.00003341
Iteration 51/1000 | Loss: 0.00003341
Iteration 52/1000 | Loss: 0.00003340
Iteration 53/1000 | Loss: 0.00003339
Iteration 54/1000 | Loss: 0.00003339
Iteration 55/1000 | Loss: 0.00003339
Iteration 56/1000 | Loss: 0.00003339
Iteration 57/1000 | Loss: 0.00003339
Iteration 58/1000 | Loss: 0.00003339
Iteration 59/1000 | Loss: 0.00003339
Iteration 60/1000 | Loss: 0.00003339
Iteration 61/1000 | Loss: 0.00003338
Iteration 62/1000 | Loss: 0.00003338
Iteration 63/1000 | Loss: 0.00003338
Iteration 64/1000 | Loss: 0.00003338
Iteration 65/1000 | Loss: 0.00003337
Iteration 66/1000 | Loss: 0.00003337
Iteration 67/1000 | Loss: 0.00003336
Iteration 68/1000 | Loss: 0.00003336
Iteration 69/1000 | Loss: 0.00003335
Iteration 70/1000 | Loss: 0.00003335
Iteration 71/1000 | Loss: 0.00003335
Iteration 72/1000 | Loss: 0.00003335
Iteration 73/1000 | Loss: 0.00003335
Iteration 74/1000 | Loss: 0.00003334
Iteration 75/1000 | Loss: 0.00003334
Iteration 76/1000 | Loss: 0.00003333
Iteration 77/1000 | Loss: 0.00003333
Iteration 78/1000 | Loss: 0.00003333
Iteration 79/1000 | Loss: 0.00003332
Iteration 80/1000 | Loss: 0.00003332
Iteration 81/1000 | Loss: 0.00003332
Iteration 82/1000 | Loss: 0.00003331
Iteration 83/1000 | Loss: 0.00003331
Iteration 84/1000 | Loss: 0.00003331
Iteration 85/1000 | Loss: 0.00003331
Iteration 86/1000 | Loss: 0.00003330
Iteration 87/1000 | Loss: 0.00003330
Iteration 88/1000 | Loss: 0.00003330
Iteration 89/1000 | Loss: 0.00003330
Iteration 90/1000 | Loss: 0.00003330
Iteration 91/1000 | Loss: 0.00003330
Iteration 92/1000 | Loss: 0.00003329
Iteration 93/1000 | Loss: 0.00003329
Iteration 94/1000 | Loss: 0.00003329
Iteration 95/1000 | Loss: 0.00003329
Iteration 96/1000 | Loss: 0.00003329
Iteration 97/1000 | Loss: 0.00003328
Iteration 98/1000 | Loss: 0.00003328
Iteration 99/1000 | Loss: 0.00003327
Iteration 100/1000 | Loss: 0.00003327
Iteration 101/1000 | Loss: 0.00003326
Iteration 102/1000 | Loss: 0.00003326
Iteration 103/1000 | Loss: 0.00003326
Iteration 104/1000 | Loss: 0.00003326
Iteration 105/1000 | Loss: 0.00003326
Iteration 106/1000 | Loss: 0.00003326
Iteration 107/1000 | Loss: 0.00003326
Iteration 108/1000 | Loss: 0.00003325
Iteration 109/1000 | Loss: 0.00003325
Iteration 110/1000 | Loss: 0.00003325
Iteration 111/1000 | Loss: 0.00003325
Iteration 112/1000 | Loss: 0.00003324
Iteration 113/1000 | Loss: 0.00003324
Iteration 114/1000 | Loss: 0.00003324
Iteration 115/1000 | Loss: 0.00003324
Iteration 116/1000 | Loss: 0.00003324
Iteration 117/1000 | Loss: 0.00003323
Iteration 118/1000 | Loss: 0.00003323
Iteration 119/1000 | Loss: 0.00003323
Iteration 120/1000 | Loss: 0.00003323
Iteration 121/1000 | Loss: 0.00003323
Iteration 122/1000 | Loss: 0.00003323
Iteration 123/1000 | Loss: 0.00003323
Iteration 124/1000 | Loss: 0.00003323
Iteration 125/1000 | Loss: 0.00003323
Iteration 126/1000 | Loss: 0.00003322
Iteration 127/1000 | Loss: 0.00003322
Iteration 128/1000 | Loss: 0.00003322
Iteration 129/1000 | Loss: 0.00003322
Iteration 130/1000 | Loss: 0.00003322
Iteration 131/1000 | Loss: 0.00003322
Iteration 132/1000 | Loss: 0.00003322
Iteration 133/1000 | Loss: 0.00003322
Iteration 134/1000 | Loss: 0.00003322
Iteration 135/1000 | Loss: 0.00003322
Iteration 136/1000 | Loss: 0.00003322
Iteration 137/1000 | Loss: 0.00003322
Iteration 138/1000 | Loss: 0.00003322
Iteration 139/1000 | Loss: 0.00003321
Iteration 140/1000 | Loss: 0.00003321
Iteration 141/1000 | Loss: 0.00003321
Iteration 142/1000 | Loss: 0.00003321
Iteration 143/1000 | Loss: 0.00003321
Iteration 144/1000 | Loss: 0.00003321
Iteration 145/1000 | Loss: 0.00003321
Iteration 146/1000 | Loss: 0.00003321
Iteration 147/1000 | Loss: 0.00003321
Iteration 148/1000 | Loss: 0.00003321
Iteration 149/1000 | Loss: 0.00003321
Iteration 150/1000 | Loss: 0.00003321
Iteration 151/1000 | Loss: 0.00003321
Iteration 152/1000 | Loss: 0.00003321
Iteration 153/1000 | Loss: 0.00003321
Iteration 154/1000 | Loss: 0.00003320
Iteration 155/1000 | Loss: 0.00003320
Iteration 156/1000 | Loss: 0.00003320
Iteration 157/1000 | Loss: 0.00003320
Iteration 158/1000 | Loss: 0.00003320
Iteration 159/1000 | Loss: 0.00003320
Iteration 160/1000 | Loss: 0.00003319
Iteration 161/1000 | Loss: 0.00003319
Iteration 162/1000 | Loss: 0.00003319
Iteration 163/1000 | Loss: 0.00003319
Iteration 164/1000 | Loss: 0.00003319
Iteration 165/1000 | Loss: 0.00003319
Iteration 166/1000 | Loss: 0.00003319
Iteration 167/1000 | Loss: 0.00003319
Iteration 168/1000 | Loss: 0.00003319
Iteration 169/1000 | Loss: 0.00003318
Iteration 170/1000 | Loss: 0.00003318
Iteration 171/1000 | Loss: 0.00003318
Iteration 172/1000 | Loss: 0.00003318
Iteration 173/1000 | Loss: 0.00003318
Iteration 174/1000 | Loss: 0.00003318
Iteration 175/1000 | Loss: 0.00003318
Iteration 176/1000 | Loss: 0.00003318
Iteration 177/1000 | Loss: 0.00003318
Iteration 178/1000 | Loss: 0.00003318
Iteration 179/1000 | Loss: 0.00003318
Iteration 180/1000 | Loss: 0.00003318
Iteration 181/1000 | Loss: 0.00003318
Iteration 182/1000 | Loss: 0.00003318
Iteration 183/1000 | Loss: 0.00003318
Iteration 184/1000 | Loss: 0.00003318
Iteration 185/1000 | Loss: 0.00003318
Iteration 186/1000 | Loss: 0.00003318
Iteration 187/1000 | Loss: 0.00003318
Iteration 188/1000 | Loss: 0.00003318
Iteration 189/1000 | Loss: 0.00003318
Iteration 190/1000 | Loss: 0.00003318
Iteration 191/1000 | Loss: 0.00003318
Iteration 192/1000 | Loss: 0.00003318
Iteration 193/1000 | Loss: 0.00003318
Iteration 194/1000 | Loss: 0.00003318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [3.317883238196373e-05, 3.317883238196373e-05, 3.317883238196373e-05, 3.317883238196373e-05, 3.317883238196373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.317883238196373e-05

Optimization complete. Final v2v error: 4.741705894470215 mm

Highest mean error: 5.41849422454834 mm for frame 132

Lowest mean error: 4.3969950675964355 mm for frame 60

Saving results

Total time: 54.82738280296326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028631
Iteration 2/25 | Loss: 0.00199951
Iteration 3/25 | Loss: 0.00154117
Iteration 4/25 | Loss: 0.00152172
Iteration 5/25 | Loss: 0.00144366
Iteration 6/25 | Loss: 0.00142575
Iteration 7/25 | Loss: 0.00142783
Iteration 8/25 | Loss: 0.00142160
Iteration 9/25 | Loss: 0.00142045
Iteration 10/25 | Loss: 0.00142006
Iteration 11/25 | Loss: 0.00142493
Iteration 12/25 | Loss: 0.00141878
Iteration 13/25 | Loss: 0.00141679
Iteration 14/25 | Loss: 0.00141622
Iteration 15/25 | Loss: 0.00141613
Iteration 16/25 | Loss: 0.00141606
Iteration 17/25 | Loss: 0.00141606
Iteration 18/25 | Loss: 0.00141606
Iteration 19/25 | Loss: 0.00141605
Iteration 20/25 | Loss: 0.00141605
Iteration 21/25 | Loss: 0.00141605
Iteration 22/25 | Loss: 0.00141605
Iteration 23/25 | Loss: 0.00141605
Iteration 24/25 | Loss: 0.00141605
Iteration 25/25 | Loss: 0.00141605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90931827
Iteration 2/25 | Loss: 0.00085384
Iteration 3/25 | Loss: 0.00085384
Iteration 4/25 | Loss: 0.00085384
Iteration 5/25 | Loss: 0.00085384
Iteration 6/25 | Loss: 0.00085384
Iteration 7/25 | Loss: 0.00085384
Iteration 8/25 | Loss: 0.00085384
Iteration 9/25 | Loss: 0.00085384
Iteration 10/25 | Loss: 0.00085384
Iteration 11/25 | Loss: 0.00085384
Iteration 12/25 | Loss: 0.00085383
Iteration 13/25 | Loss: 0.00085383
Iteration 14/25 | Loss: 0.00085383
Iteration 15/25 | Loss: 0.00085383
Iteration 16/25 | Loss: 0.00085383
Iteration 17/25 | Loss: 0.00085383
Iteration 18/25 | Loss: 0.00085383
Iteration 19/25 | Loss: 0.00085383
Iteration 20/25 | Loss: 0.00085383
Iteration 21/25 | Loss: 0.00085383
Iteration 22/25 | Loss: 0.00085383
Iteration 23/25 | Loss: 0.00085383
Iteration 24/25 | Loss: 0.00085383
Iteration 25/25 | Loss: 0.00085383

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085383
Iteration 2/1000 | Loss: 0.00175325
Iteration 3/1000 | Loss: 0.00006987
Iteration 4/1000 | Loss: 0.00004444
Iteration 5/1000 | Loss: 0.00003800
Iteration 6/1000 | Loss: 0.00003524
Iteration 7/1000 | Loss: 0.00003357
Iteration 8/1000 | Loss: 0.00003248
Iteration 9/1000 | Loss: 0.00003127
Iteration 10/1000 | Loss: 0.00003061
Iteration 11/1000 | Loss: 0.00003006
Iteration 12/1000 | Loss: 0.00002967
Iteration 13/1000 | Loss: 0.00002928
Iteration 14/1000 | Loss: 0.00002896
Iteration 15/1000 | Loss: 0.00002872
Iteration 16/1000 | Loss: 0.00002849
Iteration 17/1000 | Loss: 0.00002833
Iteration 18/1000 | Loss: 0.00002832
Iteration 19/1000 | Loss: 0.00002818
Iteration 20/1000 | Loss: 0.00002805
Iteration 21/1000 | Loss: 0.00002803
Iteration 22/1000 | Loss: 0.00002800
Iteration 23/1000 | Loss: 0.00002797
Iteration 24/1000 | Loss: 0.00002794
Iteration 25/1000 | Loss: 0.00002788
Iteration 26/1000 | Loss: 0.00002788
Iteration 27/1000 | Loss: 0.00002787
Iteration 28/1000 | Loss: 0.00002786
Iteration 29/1000 | Loss: 0.00002784
Iteration 30/1000 | Loss: 0.00002784
Iteration 31/1000 | Loss: 0.00002784
Iteration 32/1000 | Loss: 0.00002784
Iteration 33/1000 | Loss: 0.00002784
Iteration 34/1000 | Loss: 0.00002784
Iteration 35/1000 | Loss: 0.00002783
Iteration 36/1000 | Loss: 0.00002783
Iteration 37/1000 | Loss: 0.00002783
Iteration 38/1000 | Loss: 0.00002783
Iteration 39/1000 | Loss: 0.00002783
Iteration 40/1000 | Loss: 0.00002783
Iteration 41/1000 | Loss: 0.00002783
Iteration 42/1000 | Loss: 0.00002783
Iteration 43/1000 | Loss: 0.00002780
Iteration 44/1000 | Loss: 0.00002780
Iteration 45/1000 | Loss: 0.00002779
Iteration 46/1000 | Loss: 0.00002776
Iteration 47/1000 | Loss: 0.00002774
Iteration 48/1000 | Loss: 0.00002774
Iteration 49/1000 | Loss: 0.00002773
Iteration 50/1000 | Loss: 0.00002772
Iteration 51/1000 | Loss: 0.00002772
Iteration 52/1000 | Loss: 0.00002772
Iteration 53/1000 | Loss: 0.00002772
Iteration 54/1000 | Loss: 0.00002772
Iteration 55/1000 | Loss: 0.00002772
Iteration 56/1000 | Loss: 0.00002772
Iteration 57/1000 | Loss: 0.00002772
Iteration 58/1000 | Loss: 0.00002771
Iteration 59/1000 | Loss: 0.00002771
Iteration 60/1000 | Loss: 0.00002771
Iteration 61/1000 | Loss: 0.00002771
Iteration 62/1000 | Loss: 0.00002770
Iteration 63/1000 | Loss: 0.00002770
Iteration 64/1000 | Loss: 0.00002770
Iteration 65/1000 | Loss: 0.00002770
Iteration 66/1000 | Loss: 0.00002769
Iteration 67/1000 | Loss: 0.00002769
Iteration 68/1000 | Loss: 0.00002769
Iteration 69/1000 | Loss: 0.00002769
Iteration 70/1000 | Loss: 0.00002768
Iteration 71/1000 | Loss: 0.00002768
Iteration 72/1000 | Loss: 0.00002767
Iteration 73/1000 | Loss: 0.00002767
Iteration 74/1000 | Loss: 0.00002767
Iteration 75/1000 | Loss: 0.00002767
Iteration 76/1000 | Loss: 0.00002767
Iteration 77/1000 | Loss: 0.00002766
Iteration 78/1000 | Loss: 0.00002766
Iteration 79/1000 | Loss: 0.00002766
Iteration 80/1000 | Loss: 0.00002766
Iteration 81/1000 | Loss: 0.00002766
Iteration 82/1000 | Loss: 0.00002766
Iteration 83/1000 | Loss: 0.00002765
Iteration 84/1000 | Loss: 0.00002765
Iteration 85/1000 | Loss: 0.00002765
Iteration 86/1000 | Loss: 0.00002765
Iteration 87/1000 | Loss: 0.00002764
Iteration 88/1000 | Loss: 0.00002764
Iteration 89/1000 | Loss: 0.00002764
Iteration 90/1000 | Loss: 0.00002764
Iteration 91/1000 | Loss: 0.00002764
Iteration 92/1000 | Loss: 0.00002764
Iteration 93/1000 | Loss: 0.00002763
Iteration 94/1000 | Loss: 0.00002763
Iteration 95/1000 | Loss: 0.00002763
Iteration 96/1000 | Loss: 0.00002763
Iteration 97/1000 | Loss: 0.00002763
Iteration 98/1000 | Loss: 0.00002763
Iteration 99/1000 | Loss: 0.00002762
Iteration 100/1000 | Loss: 0.00002762
Iteration 101/1000 | Loss: 0.00002762
Iteration 102/1000 | Loss: 0.00002761
Iteration 103/1000 | Loss: 0.00002761
Iteration 104/1000 | Loss: 0.00002761
Iteration 105/1000 | Loss: 0.00002761
Iteration 106/1000 | Loss: 0.00002761
Iteration 107/1000 | Loss: 0.00002761
Iteration 108/1000 | Loss: 0.00002761
Iteration 109/1000 | Loss: 0.00002760
Iteration 110/1000 | Loss: 0.00002760
Iteration 111/1000 | Loss: 0.00002760
Iteration 112/1000 | Loss: 0.00002760
Iteration 113/1000 | Loss: 0.00002759
Iteration 114/1000 | Loss: 0.00002759
Iteration 115/1000 | Loss: 0.00002759
Iteration 116/1000 | Loss: 0.00002759
Iteration 117/1000 | Loss: 0.00002759
Iteration 118/1000 | Loss: 0.00002759
Iteration 119/1000 | Loss: 0.00002759
Iteration 120/1000 | Loss: 0.00002759
Iteration 121/1000 | Loss: 0.00002759
Iteration 122/1000 | Loss: 0.00002759
Iteration 123/1000 | Loss: 0.00002758
Iteration 124/1000 | Loss: 0.00002758
Iteration 125/1000 | Loss: 0.00002758
Iteration 126/1000 | Loss: 0.00002758
Iteration 127/1000 | Loss: 0.00002758
Iteration 128/1000 | Loss: 0.00002758
Iteration 129/1000 | Loss: 0.00002757
Iteration 130/1000 | Loss: 0.00002757
Iteration 131/1000 | Loss: 0.00002757
Iteration 132/1000 | Loss: 0.00002757
Iteration 133/1000 | Loss: 0.00002757
Iteration 134/1000 | Loss: 0.00002757
Iteration 135/1000 | Loss: 0.00002757
Iteration 136/1000 | Loss: 0.00002757
Iteration 137/1000 | Loss: 0.00002757
Iteration 138/1000 | Loss: 0.00002756
Iteration 139/1000 | Loss: 0.00002755
Iteration 140/1000 | Loss: 0.00002755
Iteration 141/1000 | Loss: 0.00002755
Iteration 142/1000 | Loss: 0.00002755
Iteration 143/1000 | Loss: 0.00002755
Iteration 144/1000 | Loss: 0.00002755
Iteration 145/1000 | Loss: 0.00002755
Iteration 146/1000 | Loss: 0.00002755
Iteration 147/1000 | Loss: 0.00002755
Iteration 148/1000 | Loss: 0.00002755
Iteration 149/1000 | Loss: 0.00002755
Iteration 150/1000 | Loss: 0.00002755
Iteration 151/1000 | Loss: 0.00002754
Iteration 152/1000 | Loss: 0.00002754
Iteration 153/1000 | Loss: 0.00002754
Iteration 154/1000 | Loss: 0.00002754
Iteration 155/1000 | Loss: 0.00002754
Iteration 156/1000 | Loss: 0.00002754
Iteration 157/1000 | Loss: 0.00002754
Iteration 158/1000 | Loss: 0.00002754
Iteration 159/1000 | Loss: 0.00002754
Iteration 160/1000 | Loss: 0.00002754
Iteration 161/1000 | Loss: 0.00002754
Iteration 162/1000 | Loss: 0.00002754
Iteration 163/1000 | Loss: 0.00002754
Iteration 164/1000 | Loss: 0.00002754
Iteration 165/1000 | Loss: 0.00002754
Iteration 166/1000 | Loss: 0.00002754
Iteration 167/1000 | Loss: 0.00002754
Iteration 168/1000 | Loss: 0.00002754
Iteration 169/1000 | Loss: 0.00002754
Iteration 170/1000 | Loss: 0.00002754
Iteration 171/1000 | Loss: 0.00002754
Iteration 172/1000 | Loss: 0.00002754
Iteration 173/1000 | Loss: 0.00002754
Iteration 174/1000 | Loss: 0.00002754
Iteration 175/1000 | Loss: 0.00002754
Iteration 176/1000 | Loss: 0.00002754
Iteration 177/1000 | Loss: 0.00002754
Iteration 178/1000 | Loss: 0.00002754
Iteration 179/1000 | Loss: 0.00002754
Iteration 180/1000 | Loss: 0.00002754
Iteration 181/1000 | Loss: 0.00002754
Iteration 182/1000 | Loss: 0.00002754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.753979424596764e-05, 2.753979424596764e-05, 2.753979424596764e-05, 2.753979424596764e-05, 2.753979424596764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.753979424596764e-05

Optimization complete. Final v2v error: 4.3369221687316895 mm

Highest mean error: 5.0364274978637695 mm for frame 164

Lowest mean error: 3.841210126876831 mm for frame 136

Saving results

Total time: 70.80302047729492
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008303
Iteration 2/25 | Loss: 0.00267187
Iteration 3/25 | Loss: 0.00222720
Iteration 4/25 | Loss: 0.00206727
Iteration 5/25 | Loss: 0.00201820
Iteration 6/25 | Loss: 0.00157509
Iteration 7/25 | Loss: 0.00148802
Iteration 8/25 | Loss: 0.00145948
Iteration 9/25 | Loss: 0.00143562
Iteration 10/25 | Loss: 0.00142723
Iteration 11/25 | Loss: 0.00142496
Iteration 12/25 | Loss: 0.00142458
Iteration 13/25 | Loss: 0.00142500
Iteration 14/25 | Loss: 0.00142151
Iteration 15/25 | Loss: 0.00142042
Iteration 16/25 | Loss: 0.00142020
Iteration 17/25 | Loss: 0.00142018
Iteration 18/25 | Loss: 0.00142018
Iteration 19/25 | Loss: 0.00142018
Iteration 20/25 | Loss: 0.00142018
Iteration 21/25 | Loss: 0.00142017
Iteration 22/25 | Loss: 0.00142017
Iteration 23/25 | Loss: 0.00142017
Iteration 24/25 | Loss: 0.00142017
Iteration 25/25 | Loss: 0.00142017

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43850195
Iteration 2/25 | Loss: 0.00074747
Iteration 3/25 | Loss: 0.00074747
Iteration 4/25 | Loss: 0.00074747
Iteration 5/25 | Loss: 0.00074747
Iteration 6/25 | Loss: 0.00074747
Iteration 7/25 | Loss: 0.00074747
Iteration 8/25 | Loss: 0.00074747
Iteration 9/25 | Loss: 0.00074747
Iteration 10/25 | Loss: 0.00074747
Iteration 11/25 | Loss: 0.00074747
Iteration 12/25 | Loss: 0.00074747
Iteration 13/25 | Loss: 0.00074747
Iteration 14/25 | Loss: 0.00074747
Iteration 15/25 | Loss: 0.00074747
Iteration 16/25 | Loss: 0.00074747
Iteration 17/25 | Loss: 0.00074747
Iteration 18/25 | Loss: 0.00074747
Iteration 19/25 | Loss: 0.00074747
Iteration 20/25 | Loss: 0.00074747
Iteration 21/25 | Loss: 0.00074747
Iteration 22/25 | Loss: 0.00074747
Iteration 23/25 | Loss: 0.00074747
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007474652375094593, 0.0007474652375094593, 0.0007474652375094593, 0.0007474652375094593, 0.0007474652375094593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007474652375094593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074747
Iteration 2/1000 | Loss: 0.00024925
Iteration 3/1000 | Loss: 0.00004304
Iteration 4/1000 | Loss: 0.00003688
Iteration 5/1000 | Loss: 0.00003446
Iteration 6/1000 | Loss: 0.00003258
Iteration 7/1000 | Loss: 0.00003175
Iteration 8/1000 | Loss: 0.00015479
Iteration 9/1000 | Loss: 0.00008660
Iteration 10/1000 | Loss: 0.00014290
Iteration 11/1000 | Loss: 0.00009091
Iteration 12/1000 | Loss: 0.00014368
Iteration 13/1000 | Loss: 0.00029046
Iteration 14/1000 | Loss: 0.00016812
Iteration 15/1000 | Loss: 0.00004925
Iteration 16/1000 | Loss: 0.00003921
Iteration 17/1000 | Loss: 0.00003856
Iteration 18/1000 | Loss: 0.00003411
Iteration 19/1000 | Loss: 0.00003351
Iteration 20/1000 | Loss: 0.00003632
Iteration 21/1000 | Loss: 0.00003205
Iteration 22/1000 | Loss: 0.00003140
Iteration 23/1000 | Loss: 0.00003100
Iteration 24/1000 | Loss: 0.00003064
Iteration 25/1000 | Loss: 0.00003034
Iteration 26/1000 | Loss: 0.00003011
Iteration 27/1000 | Loss: 0.00002993
Iteration 28/1000 | Loss: 0.00002976
Iteration 29/1000 | Loss: 0.00002961
Iteration 30/1000 | Loss: 0.00002937
Iteration 31/1000 | Loss: 0.00020650
Iteration 32/1000 | Loss: 0.00031623
Iteration 33/1000 | Loss: 0.00014735
Iteration 34/1000 | Loss: 0.00020962
Iteration 35/1000 | Loss: 0.00016373
Iteration 36/1000 | Loss: 0.00017945
Iteration 37/1000 | Loss: 0.00027971
Iteration 38/1000 | Loss: 0.00003702
Iteration 39/1000 | Loss: 0.00020130
Iteration 40/1000 | Loss: 0.00028471
Iteration 41/1000 | Loss: 0.00019343
Iteration 42/1000 | Loss: 0.00027836
Iteration 43/1000 | Loss: 0.00030228
Iteration 44/1000 | Loss: 0.00017003
Iteration 45/1000 | Loss: 0.00018404
Iteration 46/1000 | Loss: 0.00013684
Iteration 47/1000 | Loss: 0.00011903
Iteration 48/1000 | Loss: 0.00013557
Iteration 49/1000 | Loss: 0.00003524
Iteration 50/1000 | Loss: 0.00003248
Iteration 51/1000 | Loss: 0.00003177
Iteration 52/1000 | Loss: 0.00003124
Iteration 53/1000 | Loss: 0.00003093
Iteration 54/1000 | Loss: 0.00015325
Iteration 55/1000 | Loss: 0.00026920
Iteration 56/1000 | Loss: 0.00008905
Iteration 57/1000 | Loss: 0.00003924
Iteration 58/1000 | Loss: 0.00003320
Iteration 59/1000 | Loss: 0.00003138
Iteration 60/1000 | Loss: 0.00003065
Iteration 61/1000 | Loss: 0.00003035
Iteration 62/1000 | Loss: 0.00025229
Iteration 63/1000 | Loss: 0.00003990
Iteration 64/1000 | Loss: 0.00003491
Iteration 65/1000 | Loss: 0.00003263
Iteration 66/1000 | Loss: 0.00003191
Iteration 67/1000 | Loss: 0.00003132
Iteration 68/1000 | Loss: 0.00003075
Iteration 69/1000 | Loss: 0.00003013
Iteration 70/1000 | Loss: 0.00002909
Iteration 71/1000 | Loss: 0.00002827
Iteration 72/1000 | Loss: 0.00002801
Iteration 73/1000 | Loss: 0.00002789
Iteration 74/1000 | Loss: 0.00002787
Iteration 75/1000 | Loss: 0.00002787
Iteration 76/1000 | Loss: 0.00002787
Iteration 77/1000 | Loss: 0.00002787
Iteration 78/1000 | Loss: 0.00002787
Iteration 79/1000 | Loss: 0.00002786
Iteration 80/1000 | Loss: 0.00002786
Iteration 81/1000 | Loss: 0.00002786
Iteration 82/1000 | Loss: 0.00002785
Iteration 83/1000 | Loss: 0.00002785
Iteration 84/1000 | Loss: 0.00002784
Iteration 85/1000 | Loss: 0.00002784
Iteration 86/1000 | Loss: 0.00002784
Iteration 87/1000 | Loss: 0.00002784
Iteration 88/1000 | Loss: 0.00002784
Iteration 89/1000 | Loss: 0.00002784
Iteration 90/1000 | Loss: 0.00002784
Iteration 91/1000 | Loss: 0.00002783
Iteration 92/1000 | Loss: 0.00002783
Iteration 93/1000 | Loss: 0.00002782
Iteration 94/1000 | Loss: 0.00002782
Iteration 95/1000 | Loss: 0.00002782
Iteration 96/1000 | Loss: 0.00002781
Iteration 97/1000 | Loss: 0.00002781
Iteration 98/1000 | Loss: 0.00002780
Iteration 99/1000 | Loss: 0.00002780
Iteration 100/1000 | Loss: 0.00002780
Iteration 101/1000 | Loss: 0.00002780
Iteration 102/1000 | Loss: 0.00002779
Iteration 103/1000 | Loss: 0.00002779
Iteration 104/1000 | Loss: 0.00002779
Iteration 105/1000 | Loss: 0.00002778
Iteration 106/1000 | Loss: 0.00002778
Iteration 107/1000 | Loss: 0.00002778
Iteration 108/1000 | Loss: 0.00002778
Iteration 109/1000 | Loss: 0.00002778
Iteration 110/1000 | Loss: 0.00002777
Iteration 111/1000 | Loss: 0.00002777
Iteration 112/1000 | Loss: 0.00002777
Iteration 113/1000 | Loss: 0.00002777
Iteration 114/1000 | Loss: 0.00002775
Iteration 115/1000 | Loss: 0.00002775
Iteration 116/1000 | Loss: 0.00002773
Iteration 117/1000 | Loss: 0.00002773
Iteration 118/1000 | Loss: 0.00002772
Iteration 119/1000 | Loss: 0.00002772
Iteration 120/1000 | Loss: 0.00002772
Iteration 121/1000 | Loss: 0.00002772
Iteration 122/1000 | Loss: 0.00002772
Iteration 123/1000 | Loss: 0.00002772
Iteration 124/1000 | Loss: 0.00002772
Iteration 125/1000 | Loss: 0.00002772
Iteration 126/1000 | Loss: 0.00002772
Iteration 127/1000 | Loss: 0.00002772
Iteration 128/1000 | Loss: 0.00002772
Iteration 129/1000 | Loss: 0.00002772
Iteration 130/1000 | Loss: 0.00002772
Iteration 131/1000 | Loss: 0.00002772
Iteration 132/1000 | Loss: 0.00002772
Iteration 133/1000 | Loss: 0.00002772
Iteration 134/1000 | Loss: 0.00002772
Iteration 135/1000 | Loss: 0.00002772
Iteration 136/1000 | Loss: 0.00002772
Iteration 137/1000 | Loss: 0.00002772
Iteration 138/1000 | Loss: 0.00002772
Iteration 139/1000 | Loss: 0.00002772
Iteration 140/1000 | Loss: 0.00002772
Iteration 141/1000 | Loss: 0.00002772
Iteration 142/1000 | Loss: 0.00002772
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.7717167540686205e-05, 2.7717167540686205e-05, 2.7717167540686205e-05, 2.7717167540686205e-05, 2.7717167540686205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7717167540686205e-05

Optimization complete. Final v2v error: 4.485952854156494 mm

Highest mean error: 5.639421463012695 mm for frame 2

Lowest mean error: 3.9722979068756104 mm for frame 6

Saving results

Total time: 141.36542558670044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401350
Iteration 2/25 | Loss: 0.00135540
Iteration 3/25 | Loss: 0.00127401
Iteration 4/25 | Loss: 0.00126513
Iteration 5/25 | Loss: 0.00126234
Iteration 6/25 | Loss: 0.00126187
Iteration 7/25 | Loss: 0.00126187
Iteration 8/25 | Loss: 0.00126187
Iteration 9/25 | Loss: 0.00126187
Iteration 10/25 | Loss: 0.00126187
Iteration 11/25 | Loss: 0.00126187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012618741020560265, 0.0012618741020560265, 0.0012618741020560265, 0.0012618741020560265, 0.0012618741020560265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012618741020560265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40462208
Iteration 2/25 | Loss: 0.00087848
Iteration 3/25 | Loss: 0.00087847
Iteration 4/25 | Loss: 0.00087847
Iteration 5/25 | Loss: 0.00087847
Iteration 6/25 | Loss: 0.00087847
Iteration 7/25 | Loss: 0.00087847
Iteration 8/25 | Loss: 0.00087847
Iteration 9/25 | Loss: 0.00087847
Iteration 10/25 | Loss: 0.00087847
Iteration 11/25 | Loss: 0.00087847
Iteration 12/25 | Loss: 0.00087847
Iteration 13/25 | Loss: 0.00087847
Iteration 14/25 | Loss: 0.00087847
Iteration 15/25 | Loss: 0.00087847
Iteration 16/25 | Loss: 0.00087847
Iteration 17/25 | Loss: 0.00087847
Iteration 18/25 | Loss: 0.00087847
Iteration 19/25 | Loss: 0.00087847
Iteration 20/25 | Loss: 0.00087847
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008784694946371019, 0.0008784694946371019, 0.0008784694946371019, 0.0008784694946371019, 0.0008784694946371019]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008784694946371019

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087847
Iteration 2/1000 | Loss: 0.00003088
Iteration 3/1000 | Loss: 0.00001956
Iteration 4/1000 | Loss: 0.00001729
Iteration 5/1000 | Loss: 0.00001604
Iteration 6/1000 | Loss: 0.00001530
Iteration 7/1000 | Loss: 0.00001464
Iteration 8/1000 | Loss: 0.00001421
Iteration 9/1000 | Loss: 0.00001394
Iteration 10/1000 | Loss: 0.00001371
Iteration 11/1000 | Loss: 0.00001354
Iteration 12/1000 | Loss: 0.00001341
Iteration 13/1000 | Loss: 0.00001341
Iteration 14/1000 | Loss: 0.00001335
Iteration 15/1000 | Loss: 0.00001334
Iteration 16/1000 | Loss: 0.00001332
Iteration 17/1000 | Loss: 0.00001332
Iteration 18/1000 | Loss: 0.00001332
Iteration 19/1000 | Loss: 0.00001332
Iteration 20/1000 | Loss: 0.00001332
Iteration 21/1000 | Loss: 0.00001332
Iteration 22/1000 | Loss: 0.00001331
Iteration 23/1000 | Loss: 0.00001331
Iteration 24/1000 | Loss: 0.00001331
Iteration 25/1000 | Loss: 0.00001331
Iteration 26/1000 | Loss: 0.00001331
Iteration 27/1000 | Loss: 0.00001331
Iteration 28/1000 | Loss: 0.00001331
Iteration 29/1000 | Loss: 0.00001331
Iteration 30/1000 | Loss: 0.00001331
Iteration 31/1000 | Loss: 0.00001331
Iteration 32/1000 | Loss: 0.00001330
Iteration 33/1000 | Loss: 0.00001330
Iteration 34/1000 | Loss: 0.00001327
Iteration 35/1000 | Loss: 0.00001327
Iteration 36/1000 | Loss: 0.00001326
Iteration 37/1000 | Loss: 0.00001326
Iteration 38/1000 | Loss: 0.00001325
Iteration 39/1000 | Loss: 0.00001325
Iteration 40/1000 | Loss: 0.00001323
Iteration 41/1000 | Loss: 0.00001322
Iteration 42/1000 | Loss: 0.00001322
Iteration 43/1000 | Loss: 0.00001321
Iteration 44/1000 | Loss: 0.00001321
Iteration 45/1000 | Loss: 0.00001321
Iteration 46/1000 | Loss: 0.00001320
Iteration 47/1000 | Loss: 0.00001320
Iteration 48/1000 | Loss: 0.00001319
Iteration 49/1000 | Loss: 0.00001318
Iteration 50/1000 | Loss: 0.00001318
Iteration 51/1000 | Loss: 0.00001317
Iteration 52/1000 | Loss: 0.00001317
Iteration 53/1000 | Loss: 0.00001316
Iteration 54/1000 | Loss: 0.00001316
Iteration 55/1000 | Loss: 0.00001315
Iteration 56/1000 | Loss: 0.00001315
Iteration 57/1000 | Loss: 0.00001315
Iteration 58/1000 | Loss: 0.00001314
Iteration 59/1000 | Loss: 0.00001314
Iteration 60/1000 | Loss: 0.00001314
Iteration 61/1000 | Loss: 0.00001314
Iteration 62/1000 | Loss: 0.00001314
Iteration 63/1000 | Loss: 0.00001314
Iteration 64/1000 | Loss: 0.00001314
Iteration 65/1000 | Loss: 0.00001313
Iteration 66/1000 | Loss: 0.00001313
Iteration 67/1000 | Loss: 0.00001312
Iteration 68/1000 | Loss: 0.00001311
Iteration 69/1000 | Loss: 0.00001311
Iteration 70/1000 | Loss: 0.00001310
Iteration 71/1000 | Loss: 0.00001310
Iteration 72/1000 | Loss: 0.00001310
Iteration 73/1000 | Loss: 0.00001310
Iteration 74/1000 | Loss: 0.00001309
Iteration 75/1000 | Loss: 0.00001309
Iteration 76/1000 | Loss: 0.00001309
Iteration 77/1000 | Loss: 0.00001308
Iteration 78/1000 | Loss: 0.00001308
Iteration 79/1000 | Loss: 0.00001307
Iteration 80/1000 | Loss: 0.00001307
Iteration 81/1000 | Loss: 0.00001307
Iteration 82/1000 | Loss: 0.00001306
Iteration 83/1000 | Loss: 0.00001306
Iteration 84/1000 | Loss: 0.00001306
Iteration 85/1000 | Loss: 0.00001306
Iteration 86/1000 | Loss: 0.00001305
Iteration 87/1000 | Loss: 0.00001305
Iteration 88/1000 | Loss: 0.00001305
Iteration 89/1000 | Loss: 0.00001305
Iteration 90/1000 | Loss: 0.00001305
Iteration 91/1000 | Loss: 0.00001305
Iteration 92/1000 | Loss: 0.00001304
Iteration 93/1000 | Loss: 0.00001304
Iteration 94/1000 | Loss: 0.00001303
Iteration 95/1000 | Loss: 0.00001303
Iteration 96/1000 | Loss: 0.00001303
Iteration 97/1000 | Loss: 0.00001303
Iteration 98/1000 | Loss: 0.00001302
Iteration 99/1000 | Loss: 0.00001302
Iteration 100/1000 | Loss: 0.00001301
Iteration 101/1000 | Loss: 0.00001301
Iteration 102/1000 | Loss: 0.00001300
Iteration 103/1000 | Loss: 0.00001300
Iteration 104/1000 | Loss: 0.00001300
Iteration 105/1000 | Loss: 0.00001299
Iteration 106/1000 | Loss: 0.00001299
Iteration 107/1000 | Loss: 0.00001299
Iteration 108/1000 | Loss: 0.00001298
Iteration 109/1000 | Loss: 0.00001298
Iteration 110/1000 | Loss: 0.00001297
Iteration 111/1000 | Loss: 0.00001297
Iteration 112/1000 | Loss: 0.00001296
Iteration 113/1000 | Loss: 0.00001296
Iteration 114/1000 | Loss: 0.00001295
Iteration 115/1000 | Loss: 0.00001295
Iteration 116/1000 | Loss: 0.00001295
Iteration 117/1000 | Loss: 0.00001295
Iteration 118/1000 | Loss: 0.00001294
Iteration 119/1000 | Loss: 0.00001294
Iteration 120/1000 | Loss: 0.00001294
Iteration 121/1000 | Loss: 0.00001294
Iteration 122/1000 | Loss: 0.00001294
Iteration 123/1000 | Loss: 0.00001293
Iteration 124/1000 | Loss: 0.00001293
Iteration 125/1000 | Loss: 0.00001293
Iteration 126/1000 | Loss: 0.00001293
Iteration 127/1000 | Loss: 0.00001292
Iteration 128/1000 | Loss: 0.00001292
Iteration 129/1000 | Loss: 0.00001292
Iteration 130/1000 | Loss: 0.00001291
Iteration 131/1000 | Loss: 0.00001291
Iteration 132/1000 | Loss: 0.00001291
Iteration 133/1000 | Loss: 0.00001291
Iteration 134/1000 | Loss: 0.00001290
Iteration 135/1000 | Loss: 0.00001290
Iteration 136/1000 | Loss: 0.00001289
Iteration 137/1000 | Loss: 0.00001289
Iteration 138/1000 | Loss: 0.00001289
Iteration 139/1000 | Loss: 0.00001289
Iteration 140/1000 | Loss: 0.00001288
Iteration 141/1000 | Loss: 0.00001288
Iteration 142/1000 | Loss: 0.00001288
Iteration 143/1000 | Loss: 0.00001288
Iteration 144/1000 | Loss: 0.00001287
Iteration 145/1000 | Loss: 0.00001287
Iteration 146/1000 | Loss: 0.00001287
Iteration 147/1000 | Loss: 0.00001287
Iteration 148/1000 | Loss: 0.00001286
Iteration 149/1000 | Loss: 0.00001286
Iteration 150/1000 | Loss: 0.00001285
Iteration 151/1000 | Loss: 0.00001285
Iteration 152/1000 | Loss: 0.00001285
Iteration 153/1000 | Loss: 0.00001285
Iteration 154/1000 | Loss: 0.00001285
Iteration 155/1000 | Loss: 0.00001285
Iteration 156/1000 | Loss: 0.00001285
Iteration 157/1000 | Loss: 0.00001284
Iteration 158/1000 | Loss: 0.00001284
Iteration 159/1000 | Loss: 0.00001284
Iteration 160/1000 | Loss: 0.00001284
Iteration 161/1000 | Loss: 0.00001284
Iteration 162/1000 | Loss: 0.00001284
Iteration 163/1000 | Loss: 0.00001284
Iteration 164/1000 | Loss: 0.00001284
Iteration 165/1000 | Loss: 0.00001284
Iteration 166/1000 | Loss: 0.00001283
Iteration 167/1000 | Loss: 0.00001283
Iteration 168/1000 | Loss: 0.00001283
Iteration 169/1000 | Loss: 0.00001283
Iteration 170/1000 | Loss: 0.00001283
Iteration 171/1000 | Loss: 0.00001283
Iteration 172/1000 | Loss: 0.00001283
Iteration 173/1000 | Loss: 0.00001283
Iteration 174/1000 | Loss: 0.00001282
Iteration 175/1000 | Loss: 0.00001282
Iteration 176/1000 | Loss: 0.00001282
Iteration 177/1000 | Loss: 0.00001282
Iteration 178/1000 | Loss: 0.00001282
Iteration 179/1000 | Loss: 0.00001282
Iteration 180/1000 | Loss: 0.00001282
Iteration 181/1000 | Loss: 0.00001282
Iteration 182/1000 | Loss: 0.00001282
Iteration 183/1000 | Loss: 0.00001282
Iteration 184/1000 | Loss: 0.00001282
Iteration 185/1000 | Loss: 0.00001282
Iteration 186/1000 | Loss: 0.00001282
Iteration 187/1000 | Loss: 0.00001282
Iteration 188/1000 | Loss: 0.00001282
Iteration 189/1000 | Loss: 0.00001282
Iteration 190/1000 | Loss: 0.00001281
Iteration 191/1000 | Loss: 0.00001281
Iteration 192/1000 | Loss: 0.00001281
Iteration 193/1000 | Loss: 0.00001281
Iteration 194/1000 | Loss: 0.00001281
Iteration 195/1000 | Loss: 0.00001281
Iteration 196/1000 | Loss: 0.00001281
Iteration 197/1000 | Loss: 0.00001281
Iteration 198/1000 | Loss: 0.00001281
Iteration 199/1000 | Loss: 0.00001281
Iteration 200/1000 | Loss: 0.00001281
Iteration 201/1000 | Loss: 0.00001281
Iteration 202/1000 | Loss: 0.00001280
Iteration 203/1000 | Loss: 0.00001280
Iteration 204/1000 | Loss: 0.00001280
Iteration 205/1000 | Loss: 0.00001280
Iteration 206/1000 | Loss: 0.00001280
Iteration 207/1000 | Loss: 0.00001280
Iteration 208/1000 | Loss: 0.00001280
Iteration 209/1000 | Loss: 0.00001280
Iteration 210/1000 | Loss: 0.00001280
Iteration 211/1000 | Loss: 0.00001280
Iteration 212/1000 | Loss: 0.00001280
Iteration 213/1000 | Loss: 0.00001280
Iteration 214/1000 | Loss: 0.00001280
Iteration 215/1000 | Loss: 0.00001280
Iteration 216/1000 | Loss: 0.00001280
Iteration 217/1000 | Loss: 0.00001280
Iteration 218/1000 | Loss: 0.00001280
Iteration 219/1000 | Loss: 0.00001280
Iteration 220/1000 | Loss: 0.00001280
Iteration 221/1000 | Loss: 0.00001280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.2800009244529065e-05, 1.2800009244529065e-05, 1.2800009244529065e-05, 1.2800009244529065e-05, 1.2800009244529065e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2800009244529065e-05

Optimization complete. Final v2v error: 3.046520233154297 mm

Highest mean error: 4.049529552459717 mm for frame 88

Lowest mean error: 2.8138227462768555 mm for frame 177

Saving results

Total time: 44.0044584274292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002897
Iteration 2/25 | Loss: 0.00178846
Iteration 3/25 | Loss: 0.00150224
Iteration 4/25 | Loss: 0.00144594
Iteration 5/25 | Loss: 0.00142864
Iteration 6/25 | Loss: 0.00142402
Iteration 7/25 | Loss: 0.00142355
Iteration 8/25 | Loss: 0.00142355
Iteration 9/25 | Loss: 0.00142355
Iteration 10/25 | Loss: 0.00142355
Iteration 11/25 | Loss: 0.00142355
Iteration 12/25 | Loss: 0.00142355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014235511189326644, 0.0014235511189326644, 0.0014235511189326644, 0.0014235511189326644, 0.0014235511189326644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014235511189326644

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92709118
Iteration 2/25 | Loss: 0.00109711
Iteration 3/25 | Loss: 0.00109691
Iteration 4/25 | Loss: 0.00109691
Iteration 5/25 | Loss: 0.00109691
Iteration 6/25 | Loss: 0.00109691
Iteration 7/25 | Loss: 0.00109691
Iteration 8/25 | Loss: 0.00109691
Iteration 9/25 | Loss: 0.00109691
Iteration 10/25 | Loss: 0.00109691
Iteration 11/25 | Loss: 0.00109691
Iteration 12/25 | Loss: 0.00109691
Iteration 13/25 | Loss: 0.00109691
Iteration 14/25 | Loss: 0.00109691
Iteration 15/25 | Loss: 0.00109691
Iteration 16/25 | Loss: 0.00109691
Iteration 17/25 | Loss: 0.00109691
Iteration 18/25 | Loss: 0.00109691
Iteration 19/25 | Loss: 0.00109691
Iteration 20/25 | Loss: 0.00109691
Iteration 21/25 | Loss: 0.00109691
Iteration 22/25 | Loss: 0.00109691
Iteration 23/25 | Loss: 0.00109691
Iteration 24/25 | Loss: 0.00109691
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001096907421015203, 0.001096907421015203, 0.001096907421015203, 0.001096907421015203, 0.001096907421015203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001096907421015203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109691
Iteration 2/1000 | Loss: 0.00009785
Iteration 3/1000 | Loss: 0.00006941
Iteration 4/1000 | Loss: 0.00005995
Iteration 5/1000 | Loss: 0.00005657
Iteration 6/1000 | Loss: 0.00005451
Iteration 7/1000 | Loss: 0.00005268
Iteration 8/1000 | Loss: 0.00005120
Iteration 9/1000 | Loss: 0.00005006
Iteration 10/1000 | Loss: 0.00004916
Iteration 11/1000 | Loss: 0.00004841
Iteration 12/1000 | Loss: 0.00004776
Iteration 13/1000 | Loss: 0.00004725
Iteration 14/1000 | Loss: 0.00004681
Iteration 15/1000 | Loss: 0.00004646
Iteration 16/1000 | Loss: 0.00004612
Iteration 17/1000 | Loss: 0.00004591
Iteration 18/1000 | Loss: 0.00004571
Iteration 19/1000 | Loss: 0.00004557
Iteration 20/1000 | Loss: 0.00004542
Iteration 21/1000 | Loss: 0.00004542
Iteration 22/1000 | Loss: 0.00004534
Iteration 23/1000 | Loss: 0.00004519
Iteration 24/1000 | Loss: 0.00004514
Iteration 25/1000 | Loss: 0.00004511
Iteration 26/1000 | Loss: 0.00004506
Iteration 27/1000 | Loss: 0.00004501
Iteration 28/1000 | Loss: 0.00004500
Iteration 29/1000 | Loss: 0.00004500
Iteration 30/1000 | Loss: 0.00004500
Iteration 31/1000 | Loss: 0.00004495
Iteration 32/1000 | Loss: 0.00004495
Iteration 33/1000 | Loss: 0.00004492
Iteration 34/1000 | Loss: 0.00004491
Iteration 35/1000 | Loss: 0.00004491
Iteration 36/1000 | Loss: 0.00004491
Iteration 37/1000 | Loss: 0.00004491
Iteration 38/1000 | Loss: 0.00004491
Iteration 39/1000 | Loss: 0.00004491
Iteration 40/1000 | Loss: 0.00004491
Iteration 41/1000 | Loss: 0.00004491
Iteration 42/1000 | Loss: 0.00004491
Iteration 43/1000 | Loss: 0.00004490
Iteration 44/1000 | Loss: 0.00004486
Iteration 45/1000 | Loss: 0.00004484
Iteration 46/1000 | Loss: 0.00004483
Iteration 47/1000 | Loss: 0.00004483
Iteration 48/1000 | Loss: 0.00004477
Iteration 49/1000 | Loss: 0.00004476
Iteration 50/1000 | Loss: 0.00004476
Iteration 51/1000 | Loss: 0.00004476
Iteration 52/1000 | Loss: 0.00004476
Iteration 53/1000 | Loss: 0.00004476
Iteration 54/1000 | Loss: 0.00004476
Iteration 55/1000 | Loss: 0.00004476
Iteration 56/1000 | Loss: 0.00004476
Iteration 57/1000 | Loss: 0.00004476
Iteration 58/1000 | Loss: 0.00004476
Iteration 59/1000 | Loss: 0.00004476
Iteration 60/1000 | Loss: 0.00004476
Iteration 61/1000 | Loss: 0.00004476
Iteration 62/1000 | Loss: 0.00004475
Iteration 63/1000 | Loss: 0.00004475
Iteration 64/1000 | Loss: 0.00004475
Iteration 65/1000 | Loss: 0.00004475
Iteration 66/1000 | Loss: 0.00004474
Iteration 67/1000 | Loss: 0.00004474
Iteration 68/1000 | Loss: 0.00004474
Iteration 69/1000 | Loss: 0.00004474
Iteration 70/1000 | Loss: 0.00004474
Iteration 71/1000 | Loss: 0.00004474
Iteration 72/1000 | Loss: 0.00004474
Iteration 73/1000 | Loss: 0.00004473
Iteration 74/1000 | Loss: 0.00004473
Iteration 75/1000 | Loss: 0.00004473
Iteration 76/1000 | Loss: 0.00004473
Iteration 77/1000 | Loss: 0.00004473
Iteration 78/1000 | Loss: 0.00004473
Iteration 79/1000 | Loss: 0.00004473
Iteration 80/1000 | Loss: 0.00004473
Iteration 81/1000 | Loss: 0.00004473
Iteration 82/1000 | Loss: 0.00004473
Iteration 83/1000 | Loss: 0.00004473
Iteration 84/1000 | Loss: 0.00004473
Iteration 85/1000 | Loss: 0.00004472
Iteration 86/1000 | Loss: 0.00004472
Iteration 87/1000 | Loss: 0.00004472
Iteration 88/1000 | Loss: 0.00004472
Iteration 89/1000 | Loss: 0.00004472
Iteration 90/1000 | Loss: 0.00004471
Iteration 91/1000 | Loss: 0.00004471
Iteration 92/1000 | Loss: 0.00004471
Iteration 93/1000 | Loss: 0.00004470
Iteration 94/1000 | Loss: 0.00004470
Iteration 95/1000 | Loss: 0.00004470
Iteration 96/1000 | Loss: 0.00004470
Iteration 97/1000 | Loss: 0.00004469
Iteration 98/1000 | Loss: 0.00004469
Iteration 99/1000 | Loss: 0.00004469
Iteration 100/1000 | Loss: 0.00004468
Iteration 101/1000 | Loss: 0.00004468
Iteration 102/1000 | Loss: 0.00004468
Iteration 103/1000 | Loss: 0.00004467
Iteration 104/1000 | Loss: 0.00004467
Iteration 105/1000 | Loss: 0.00004467
Iteration 106/1000 | Loss: 0.00004467
Iteration 107/1000 | Loss: 0.00004467
Iteration 108/1000 | Loss: 0.00004466
Iteration 109/1000 | Loss: 0.00004466
Iteration 110/1000 | Loss: 0.00004466
Iteration 111/1000 | Loss: 0.00004465
Iteration 112/1000 | Loss: 0.00004465
Iteration 113/1000 | Loss: 0.00004465
Iteration 114/1000 | Loss: 0.00004464
Iteration 115/1000 | Loss: 0.00004464
Iteration 116/1000 | Loss: 0.00004464
Iteration 117/1000 | Loss: 0.00004463
Iteration 118/1000 | Loss: 0.00004463
Iteration 119/1000 | Loss: 0.00004463
Iteration 120/1000 | Loss: 0.00004463
Iteration 121/1000 | Loss: 0.00004462
Iteration 122/1000 | Loss: 0.00004462
Iteration 123/1000 | Loss: 0.00004462
Iteration 124/1000 | Loss: 0.00004461
Iteration 125/1000 | Loss: 0.00004461
Iteration 126/1000 | Loss: 0.00004461
Iteration 127/1000 | Loss: 0.00004461
Iteration 128/1000 | Loss: 0.00004461
Iteration 129/1000 | Loss: 0.00004461
Iteration 130/1000 | Loss: 0.00004461
Iteration 131/1000 | Loss: 0.00004461
Iteration 132/1000 | Loss: 0.00004461
Iteration 133/1000 | Loss: 0.00004460
Iteration 134/1000 | Loss: 0.00004460
Iteration 135/1000 | Loss: 0.00004460
Iteration 136/1000 | Loss: 0.00004460
Iteration 137/1000 | Loss: 0.00004460
Iteration 138/1000 | Loss: 0.00004460
Iteration 139/1000 | Loss: 0.00004460
Iteration 140/1000 | Loss: 0.00004460
Iteration 141/1000 | Loss: 0.00004459
Iteration 142/1000 | Loss: 0.00004459
Iteration 143/1000 | Loss: 0.00004459
Iteration 144/1000 | Loss: 0.00004459
Iteration 145/1000 | Loss: 0.00004459
Iteration 146/1000 | Loss: 0.00004459
Iteration 147/1000 | Loss: 0.00004459
Iteration 148/1000 | Loss: 0.00004459
Iteration 149/1000 | Loss: 0.00004459
Iteration 150/1000 | Loss: 0.00004458
Iteration 151/1000 | Loss: 0.00004458
Iteration 152/1000 | Loss: 0.00004458
Iteration 153/1000 | Loss: 0.00004458
Iteration 154/1000 | Loss: 0.00004458
Iteration 155/1000 | Loss: 0.00004458
Iteration 156/1000 | Loss: 0.00004458
Iteration 157/1000 | Loss: 0.00004458
Iteration 158/1000 | Loss: 0.00004458
Iteration 159/1000 | Loss: 0.00004458
Iteration 160/1000 | Loss: 0.00004457
Iteration 161/1000 | Loss: 0.00004457
Iteration 162/1000 | Loss: 0.00004457
Iteration 163/1000 | Loss: 0.00004457
Iteration 164/1000 | Loss: 0.00004457
Iteration 165/1000 | Loss: 0.00004457
Iteration 166/1000 | Loss: 0.00004457
Iteration 167/1000 | Loss: 0.00004457
Iteration 168/1000 | Loss: 0.00004457
Iteration 169/1000 | Loss: 0.00004457
Iteration 170/1000 | Loss: 0.00004457
Iteration 171/1000 | Loss: 0.00004457
Iteration 172/1000 | Loss: 0.00004457
Iteration 173/1000 | Loss: 0.00004457
Iteration 174/1000 | Loss: 0.00004457
Iteration 175/1000 | Loss: 0.00004457
Iteration 176/1000 | Loss: 0.00004457
Iteration 177/1000 | Loss: 0.00004457
Iteration 178/1000 | Loss: 0.00004457
Iteration 179/1000 | Loss: 0.00004457
Iteration 180/1000 | Loss: 0.00004457
Iteration 181/1000 | Loss: 0.00004457
Iteration 182/1000 | Loss: 0.00004457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [4.457197792362422e-05, 4.457197792362422e-05, 4.457197792362422e-05, 4.457197792362422e-05, 4.457197792362422e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.457197792362422e-05

Optimization complete. Final v2v error: 5.499298572540283 mm

Highest mean error: 5.888979911804199 mm for frame 11

Lowest mean error: 5.065914630889893 mm for frame 150

Saving results

Total time: 62.62673902511597
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414725
Iteration 2/25 | Loss: 0.00134367
Iteration 3/25 | Loss: 0.00128230
Iteration 4/25 | Loss: 0.00127226
Iteration 5/25 | Loss: 0.00126933
Iteration 6/25 | Loss: 0.00126888
Iteration 7/25 | Loss: 0.00126888
Iteration 8/25 | Loss: 0.00126888
Iteration 9/25 | Loss: 0.00126888
Iteration 10/25 | Loss: 0.00126888
Iteration 11/25 | Loss: 0.00126888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012688819551840425, 0.0012688819551840425, 0.0012688819551840425, 0.0012688819551840425, 0.0012688819551840425]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012688819551840425

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40356803
Iteration 2/25 | Loss: 0.00086373
Iteration 3/25 | Loss: 0.00086373
Iteration 4/25 | Loss: 0.00086373
Iteration 5/25 | Loss: 0.00086373
Iteration 6/25 | Loss: 0.00086373
Iteration 7/25 | Loss: 0.00086373
Iteration 8/25 | Loss: 0.00086373
Iteration 9/25 | Loss: 0.00086373
Iteration 10/25 | Loss: 0.00086373
Iteration 11/25 | Loss: 0.00086373
Iteration 12/25 | Loss: 0.00086373
Iteration 13/25 | Loss: 0.00086373
Iteration 14/25 | Loss: 0.00086373
Iteration 15/25 | Loss: 0.00086373
Iteration 16/25 | Loss: 0.00086373
Iteration 17/25 | Loss: 0.00086373
Iteration 18/25 | Loss: 0.00086373
Iteration 19/25 | Loss: 0.00086373
Iteration 20/25 | Loss: 0.00086373
Iteration 21/25 | Loss: 0.00086373
Iteration 22/25 | Loss: 0.00086373
Iteration 23/25 | Loss: 0.00086373
Iteration 24/25 | Loss: 0.00086373
Iteration 25/25 | Loss: 0.00086373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086373
Iteration 2/1000 | Loss: 0.00002576
Iteration 3/1000 | Loss: 0.00001809
Iteration 4/1000 | Loss: 0.00001637
Iteration 5/1000 | Loss: 0.00001558
Iteration 6/1000 | Loss: 0.00001500
Iteration 7/1000 | Loss: 0.00001459
Iteration 8/1000 | Loss: 0.00001439
Iteration 9/1000 | Loss: 0.00001423
Iteration 10/1000 | Loss: 0.00001407
Iteration 11/1000 | Loss: 0.00001406
Iteration 12/1000 | Loss: 0.00001404
Iteration 13/1000 | Loss: 0.00001389
Iteration 14/1000 | Loss: 0.00001383
Iteration 15/1000 | Loss: 0.00001376
Iteration 16/1000 | Loss: 0.00001376
Iteration 17/1000 | Loss: 0.00001375
Iteration 18/1000 | Loss: 0.00001371
Iteration 19/1000 | Loss: 0.00001367
Iteration 20/1000 | Loss: 0.00001367
Iteration 21/1000 | Loss: 0.00001366
Iteration 22/1000 | Loss: 0.00001366
Iteration 23/1000 | Loss: 0.00001364
Iteration 24/1000 | Loss: 0.00001363
Iteration 25/1000 | Loss: 0.00001362
Iteration 26/1000 | Loss: 0.00001358
Iteration 27/1000 | Loss: 0.00001357
Iteration 28/1000 | Loss: 0.00001355
Iteration 29/1000 | Loss: 0.00001355
Iteration 30/1000 | Loss: 0.00001354
Iteration 31/1000 | Loss: 0.00001354
Iteration 32/1000 | Loss: 0.00001354
Iteration 33/1000 | Loss: 0.00001353
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001352
Iteration 36/1000 | Loss: 0.00001351
Iteration 37/1000 | Loss: 0.00001351
Iteration 38/1000 | Loss: 0.00001350
Iteration 39/1000 | Loss: 0.00001350
Iteration 40/1000 | Loss: 0.00001349
Iteration 41/1000 | Loss: 0.00001349
Iteration 42/1000 | Loss: 0.00001348
Iteration 43/1000 | Loss: 0.00001348
Iteration 44/1000 | Loss: 0.00001347
Iteration 45/1000 | Loss: 0.00001347
Iteration 46/1000 | Loss: 0.00001347
Iteration 47/1000 | Loss: 0.00001347
Iteration 48/1000 | Loss: 0.00001347
Iteration 49/1000 | Loss: 0.00001347
Iteration 50/1000 | Loss: 0.00001347
Iteration 51/1000 | Loss: 0.00001346
Iteration 52/1000 | Loss: 0.00001346
Iteration 53/1000 | Loss: 0.00001345
Iteration 54/1000 | Loss: 0.00001345
Iteration 55/1000 | Loss: 0.00001345
Iteration 56/1000 | Loss: 0.00001345
Iteration 57/1000 | Loss: 0.00001345
Iteration 58/1000 | Loss: 0.00001345
Iteration 59/1000 | Loss: 0.00001344
Iteration 60/1000 | Loss: 0.00001344
Iteration 61/1000 | Loss: 0.00001344
Iteration 62/1000 | Loss: 0.00001343
Iteration 63/1000 | Loss: 0.00001343
Iteration 64/1000 | Loss: 0.00001343
Iteration 65/1000 | Loss: 0.00001343
Iteration 66/1000 | Loss: 0.00001343
Iteration 67/1000 | Loss: 0.00001343
Iteration 68/1000 | Loss: 0.00001343
Iteration 69/1000 | Loss: 0.00001342
Iteration 70/1000 | Loss: 0.00001342
Iteration 71/1000 | Loss: 0.00001342
Iteration 72/1000 | Loss: 0.00001341
Iteration 73/1000 | Loss: 0.00001340
Iteration 74/1000 | Loss: 0.00001340
Iteration 75/1000 | Loss: 0.00001340
Iteration 76/1000 | Loss: 0.00001340
Iteration 77/1000 | Loss: 0.00001340
Iteration 78/1000 | Loss: 0.00001340
Iteration 79/1000 | Loss: 0.00001339
Iteration 80/1000 | Loss: 0.00001339
Iteration 81/1000 | Loss: 0.00001339
Iteration 82/1000 | Loss: 0.00001339
Iteration 83/1000 | Loss: 0.00001338
Iteration 84/1000 | Loss: 0.00001338
Iteration 85/1000 | Loss: 0.00001337
Iteration 86/1000 | Loss: 0.00001337
Iteration 87/1000 | Loss: 0.00001337
Iteration 88/1000 | Loss: 0.00001337
Iteration 89/1000 | Loss: 0.00001336
Iteration 90/1000 | Loss: 0.00001336
Iteration 91/1000 | Loss: 0.00001336
Iteration 92/1000 | Loss: 0.00001336
Iteration 93/1000 | Loss: 0.00001336
Iteration 94/1000 | Loss: 0.00001336
Iteration 95/1000 | Loss: 0.00001335
Iteration 96/1000 | Loss: 0.00001335
Iteration 97/1000 | Loss: 0.00001335
Iteration 98/1000 | Loss: 0.00001334
Iteration 99/1000 | Loss: 0.00001334
Iteration 100/1000 | Loss: 0.00001334
Iteration 101/1000 | Loss: 0.00001334
Iteration 102/1000 | Loss: 0.00001334
Iteration 103/1000 | Loss: 0.00001334
Iteration 104/1000 | Loss: 0.00001334
Iteration 105/1000 | Loss: 0.00001333
Iteration 106/1000 | Loss: 0.00001333
Iteration 107/1000 | Loss: 0.00001333
Iteration 108/1000 | Loss: 0.00001333
Iteration 109/1000 | Loss: 0.00001333
Iteration 110/1000 | Loss: 0.00001333
Iteration 111/1000 | Loss: 0.00001333
Iteration 112/1000 | Loss: 0.00001333
Iteration 113/1000 | Loss: 0.00001333
Iteration 114/1000 | Loss: 0.00001333
Iteration 115/1000 | Loss: 0.00001332
Iteration 116/1000 | Loss: 0.00001332
Iteration 117/1000 | Loss: 0.00001332
Iteration 118/1000 | Loss: 0.00001332
Iteration 119/1000 | Loss: 0.00001332
Iteration 120/1000 | Loss: 0.00001331
Iteration 121/1000 | Loss: 0.00001331
Iteration 122/1000 | Loss: 0.00001331
Iteration 123/1000 | Loss: 0.00001331
Iteration 124/1000 | Loss: 0.00001331
Iteration 125/1000 | Loss: 0.00001331
Iteration 126/1000 | Loss: 0.00001331
Iteration 127/1000 | Loss: 0.00001330
Iteration 128/1000 | Loss: 0.00001330
Iteration 129/1000 | Loss: 0.00001330
Iteration 130/1000 | Loss: 0.00001330
Iteration 131/1000 | Loss: 0.00001330
Iteration 132/1000 | Loss: 0.00001330
Iteration 133/1000 | Loss: 0.00001330
Iteration 134/1000 | Loss: 0.00001330
Iteration 135/1000 | Loss: 0.00001330
Iteration 136/1000 | Loss: 0.00001329
Iteration 137/1000 | Loss: 0.00001329
Iteration 138/1000 | Loss: 0.00001329
Iteration 139/1000 | Loss: 0.00001329
Iteration 140/1000 | Loss: 0.00001329
Iteration 141/1000 | Loss: 0.00001329
Iteration 142/1000 | Loss: 0.00001329
Iteration 143/1000 | Loss: 0.00001329
Iteration 144/1000 | Loss: 0.00001329
Iteration 145/1000 | Loss: 0.00001329
Iteration 146/1000 | Loss: 0.00001329
Iteration 147/1000 | Loss: 0.00001329
Iteration 148/1000 | Loss: 0.00001329
Iteration 149/1000 | Loss: 0.00001329
Iteration 150/1000 | Loss: 0.00001329
Iteration 151/1000 | Loss: 0.00001329
Iteration 152/1000 | Loss: 0.00001329
Iteration 153/1000 | Loss: 0.00001329
Iteration 154/1000 | Loss: 0.00001329
Iteration 155/1000 | Loss: 0.00001329
Iteration 156/1000 | Loss: 0.00001329
Iteration 157/1000 | Loss: 0.00001329
Iteration 158/1000 | Loss: 0.00001329
Iteration 159/1000 | Loss: 0.00001329
Iteration 160/1000 | Loss: 0.00001329
Iteration 161/1000 | Loss: 0.00001329
Iteration 162/1000 | Loss: 0.00001329
Iteration 163/1000 | Loss: 0.00001329
Iteration 164/1000 | Loss: 0.00001329
Iteration 165/1000 | Loss: 0.00001329
Iteration 166/1000 | Loss: 0.00001329
Iteration 167/1000 | Loss: 0.00001329
Iteration 168/1000 | Loss: 0.00001329
Iteration 169/1000 | Loss: 0.00001329
Iteration 170/1000 | Loss: 0.00001329
Iteration 171/1000 | Loss: 0.00001329
Iteration 172/1000 | Loss: 0.00001329
Iteration 173/1000 | Loss: 0.00001329
Iteration 174/1000 | Loss: 0.00001329
Iteration 175/1000 | Loss: 0.00001329
Iteration 176/1000 | Loss: 0.00001329
Iteration 177/1000 | Loss: 0.00001329
Iteration 178/1000 | Loss: 0.00001329
Iteration 179/1000 | Loss: 0.00001329
Iteration 180/1000 | Loss: 0.00001329
Iteration 181/1000 | Loss: 0.00001329
Iteration 182/1000 | Loss: 0.00001329
Iteration 183/1000 | Loss: 0.00001329
Iteration 184/1000 | Loss: 0.00001329
Iteration 185/1000 | Loss: 0.00001329
Iteration 186/1000 | Loss: 0.00001329
Iteration 187/1000 | Loss: 0.00001329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.3286625289765652e-05, 1.3286625289765652e-05, 1.3286625289765652e-05, 1.3286625289765652e-05, 1.3286625289765652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3286625289765652e-05

Optimization complete. Final v2v error: 3.119239091873169 mm

Highest mean error: 3.3176109790802 mm for frame 103

Lowest mean error: 2.9697680473327637 mm for frame 88

Saving results

Total time: 37.03652858734131
