Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=77, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 4312-4367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828029
Iteration 2/25 | Loss: 0.00133349
Iteration 3/25 | Loss: 0.00125293
Iteration 4/25 | Loss: 0.00123618
Iteration 5/25 | Loss: 0.00123176
Iteration 6/25 | Loss: 0.00123091
Iteration 7/25 | Loss: 0.00123091
Iteration 8/25 | Loss: 0.00123091
Iteration 9/25 | Loss: 0.00123091
Iteration 10/25 | Loss: 0.00123091
Iteration 11/25 | Loss: 0.00123091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012309134472161531, 0.0012309134472161531, 0.0012309134472161531, 0.0012309134472161531, 0.0012309134472161531]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012309134472161531

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84122598
Iteration 2/25 | Loss: 0.00182770
Iteration 3/25 | Loss: 0.00182769
Iteration 4/25 | Loss: 0.00182768
Iteration 5/25 | Loss: 0.00182768
Iteration 6/25 | Loss: 0.00182768
Iteration 7/25 | Loss: 0.00182768
Iteration 8/25 | Loss: 0.00182768
Iteration 9/25 | Loss: 0.00182768
Iteration 10/25 | Loss: 0.00182768
Iteration 11/25 | Loss: 0.00182768
Iteration 12/25 | Loss: 0.00182768
Iteration 13/25 | Loss: 0.00182768
Iteration 14/25 | Loss: 0.00182768
Iteration 15/25 | Loss: 0.00182768
Iteration 16/25 | Loss: 0.00182768
Iteration 17/25 | Loss: 0.00182768
Iteration 18/25 | Loss: 0.00182768
Iteration 19/25 | Loss: 0.00182768
Iteration 20/25 | Loss: 0.00182768
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0018276822520419955, 0.0018276822520419955, 0.0018276822520419955, 0.0018276822520419955, 0.0018276822520419955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018276822520419955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182768
Iteration 2/1000 | Loss: 0.00004869
Iteration 3/1000 | Loss: 0.00003014
Iteration 4/1000 | Loss: 0.00002431
Iteration 5/1000 | Loss: 0.00002290
Iteration 6/1000 | Loss: 0.00002206
Iteration 7/1000 | Loss: 0.00002141
Iteration 8/1000 | Loss: 0.00002099
Iteration 9/1000 | Loss: 0.00002054
Iteration 10/1000 | Loss: 0.00002023
Iteration 11/1000 | Loss: 0.00002022
Iteration 12/1000 | Loss: 0.00002004
Iteration 13/1000 | Loss: 0.00001990
Iteration 14/1000 | Loss: 0.00001977
Iteration 15/1000 | Loss: 0.00001962
Iteration 16/1000 | Loss: 0.00001957
Iteration 17/1000 | Loss: 0.00001956
Iteration 18/1000 | Loss: 0.00001955
Iteration 19/1000 | Loss: 0.00001955
Iteration 20/1000 | Loss: 0.00001953
Iteration 21/1000 | Loss: 0.00001952
Iteration 22/1000 | Loss: 0.00001942
Iteration 23/1000 | Loss: 0.00001936
Iteration 24/1000 | Loss: 0.00001933
Iteration 25/1000 | Loss: 0.00001933
Iteration 26/1000 | Loss: 0.00001932
Iteration 27/1000 | Loss: 0.00001932
Iteration 28/1000 | Loss: 0.00001931
Iteration 29/1000 | Loss: 0.00001931
Iteration 30/1000 | Loss: 0.00001930
Iteration 31/1000 | Loss: 0.00001930
Iteration 32/1000 | Loss: 0.00001929
Iteration 33/1000 | Loss: 0.00001927
Iteration 34/1000 | Loss: 0.00001926
Iteration 35/1000 | Loss: 0.00001926
Iteration 36/1000 | Loss: 0.00001925
Iteration 37/1000 | Loss: 0.00001924
Iteration 38/1000 | Loss: 0.00001924
Iteration 39/1000 | Loss: 0.00001923
Iteration 40/1000 | Loss: 0.00001923
Iteration 41/1000 | Loss: 0.00001922
Iteration 42/1000 | Loss: 0.00001922
Iteration 43/1000 | Loss: 0.00001922
Iteration 44/1000 | Loss: 0.00001922
Iteration 45/1000 | Loss: 0.00001921
Iteration 46/1000 | Loss: 0.00001921
Iteration 47/1000 | Loss: 0.00001921
Iteration 48/1000 | Loss: 0.00001920
Iteration 49/1000 | Loss: 0.00001920
Iteration 50/1000 | Loss: 0.00001919
Iteration 51/1000 | Loss: 0.00001919
Iteration 52/1000 | Loss: 0.00001919
Iteration 53/1000 | Loss: 0.00001919
Iteration 54/1000 | Loss: 0.00001918
Iteration 55/1000 | Loss: 0.00001918
Iteration 56/1000 | Loss: 0.00001918
Iteration 57/1000 | Loss: 0.00001918
Iteration 58/1000 | Loss: 0.00001918
Iteration 59/1000 | Loss: 0.00001917
Iteration 60/1000 | Loss: 0.00001917
Iteration 61/1000 | Loss: 0.00001917
Iteration 62/1000 | Loss: 0.00001917
Iteration 63/1000 | Loss: 0.00001917
Iteration 64/1000 | Loss: 0.00001917
Iteration 65/1000 | Loss: 0.00001916
Iteration 66/1000 | Loss: 0.00001916
Iteration 67/1000 | Loss: 0.00001916
Iteration 68/1000 | Loss: 0.00001916
Iteration 69/1000 | Loss: 0.00001915
Iteration 70/1000 | Loss: 0.00001915
Iteration 71/1000 | Loss: 0.00001915
Iteration 72/1000 | Loss: 0.00001915
Iteration 73/1000 | Loss: 0.00001914
Iteration 74/1000 | Loss: 0.00001914
Iteration 75/1000 | Loss: 0.00001914
Iteration 76/1000 | Loss: 0.00001914
Iteration 77/1000 | Loss: 0.00001914
Iteration 78/1000 | Loss: 0.00001913
Iteration 79/1000 | Loss: 0.00001913
Iteration 80/1000 | Loss: 0.00001913
Iteration 81/1000 | Loss: 0.00001913
Iteration 82/1000 | Loss: 0.00001913
Iteration 83/1000 | Loss: 0.00001912
Iteration 84/1000 | Loss: 0.00001912
Iteration 85/1000 | Loss: 0.00001912
Iteration 86/1000 | Loss: 0.00001911
Iteration 87/1000 | Loss: 0.00001911
Iteration 88/1000 | Loss: 0.00001911
Iteration 89/1000 | Loss: 0.00001911
Iteration 90/1000 | Loss: 0.00001911
Iteration 91/1000 | Loss: 0.00001910
Iteration 92/1000 | Loss: 0.00001910
Iteration 93/1000 | Loss: 0.00001910
Iteration 94/1000 | Loss: 0.00001910
Iteration 95/1000 | Loss: 0.00001910
Iteration 96/1000 | Loss: 0.00001910
Iteration 97/1000 | Loss: 0.00001909
Iteration 98/1000 | Loss: 0.00001909
Iteration 99/1000 | Loss: 0.00001909
Iteration 100/1000 | Loss: 0.00001909
Iteration 101/1000 | Loss: 0.00001909
Iteration 102/1000 | Loss: 0.00001909
Iteration 103/1000 | Loss: 0.00001909
Iteration 104/1000 | Loss: 0.00001909
Iteration 105/1000 | Loss: 0.00001909
Iteration 106/1000 | Loss: 0.00001908
Iteration 107/1000 | Loss: 0.00001908
Iteration 108/1000 | Loss: 0.00001908
Iteration 109/1000 | Loss: 0.00001908
Iteration 110/1000 | Loss: 0.00001908
Iteration 111/1000 | Loss: 0.00001908
Iteration 112/1000 | Loss: 0.00001907
Iteration 113/1000 | Loss: 0.00001907
Iteration 114/1000 | Loss: 0.00001907
Iteration 115/1000 | Loss: 0.00001907
Iteration 116/1000 | Loss: 0.00001907
Iteration 117/1000 | Loss: 0.00001907
Iteration 118/1000 | Loss: 0.00001907
Iteration 119/1000 | Loss: 0.00001907
Iteration 120/1000 | Loss: 0.00001907
Iteration 121/1000 | Loss: 0.00001907
Iteration 122/1000 | Loss: 0.00001907
Iteration 123/1000 | Loss: 0.00001907
Iteration 124/1000 | Loss: 0.00001907
Iteration 125/1000 | Loss: 0.00001907
Iteration 126/1000 | Loss: 0.00001907
Iteration 127/1000 | Loss: 0.00001907
Iteration 128/1000 | Loss: 0.00001906
Iteration 129/1000 | Loss: 0.00001906
Iteration 130/1000 | Loss: 0.00001906
Iteration 131/1000 | Loss: 0.00001906
Iteration 132/1000 | Loss: 0.00001906
Iteration 133/1000 | Loss: 0.00001906
Iteration 134/1000 | Loss: 0.00001906
Iteration 135/1000 | Loss: 0.00001906
Iteration 136/1000 | Loss: 0.00001906
Iteration 137/1000 | Loss: 0.00001906
Iteration 138/1000 | Loss: 0.00001906
Iteration 139/1000 | Loss: 0.00001906
Iteration 140/1000 | Loss: 0.00001906
Iteration 141/1000 | Loss: 0.00001906
Iteration 142/1000 | Loss: 0.00001906
Iteration 143/1000 | Loss: 0.00001906
Iteration 144/1000 | Loss: 0.00001906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.9063727449974976e-05, 1.9063727449974976e-05, 1.9063727449974976e-05, 1.9063727449974976e-05, 1.9063727449974976e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9063727449974976e-05

Optimization complete. Final v2v error: 3.684326410293579 mm

Highest mean error: 4.135909080505371 mm for frame 48

Lowest mean error: 3.390075922012329 mm for frame 130

Saving results

Total time: 40.93094444274902
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781864
Iteration 2/25 | Loss: 0.00134281
Iteration 3/25 | Loss: 0.00120646
Iteration 4/25 | Loss: 0.00119261
Iteration 5/25 | Loss: 0.00119023
Iteration 6/25 | Loss: 0.00119023
Iteration 7/25 | Loss: 0.00119023
Iteration 8/25 | Loss: 0.00119023
Iteration 9/25 | Loss: 0.00119023
Iteration 10/25 | Loss: 0.00119023
Iteration 11/25 | Loss: 0.00119023
Iteration 12/25 | Loss: 0.00119023
Iteration 13/25 | Loss: 0.00119023
Iteration 14/25 | Loss: 0.00119023
Iteration 15/25 | Loss: 0.00119023
Iteration 16/25 | Loss: 0.00119023
Iteration 17/25 | Loss: 0.00119023
Iteration 18/25 | Loss: 0.00119023
Iteration 19/25 | Loss: 0.00119023
Iteration 20/25 | Loss: 0.00119023
Iteration 21/25 | Loss: 0.00119023
Iteration 22/25 | Loss: 0.00119023
Iteration 23/25 | Loss: 0.00119023
Iteration 24/25 | Loss: 0.00119023
Iteration 25/25 | Loss: 0.00119023

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28730452
Iteration 2/25 | Loss: 0.00149660
Iteration 3/25 | Loss: 0.00149660
Iteration 4/25 | Loss: 0.00149660
Iteration 5/25 | Loss: 0.00149660
Iteration 6/25 | Loss: 0.00149660
Iteration 7/25 | Loss: 0.00149660
Iteration 8/25 | Loss: 0.00149660
Iteration 9/25 | Loss: 0.00149660
Iteration 10/25 | Loss: 0.00149660
Iteration 11/25 | Loss: 0.00149660
Iteration 12/25 | Loss: 0.00149660
Iteration 13/25 | Loss: 0.00149660
Iteration 14/25 | Loss: 0.00149660
Iteration 15/25 | Loss: 0.00149660
Iteration 16/25 | Loss: 0.00149660
Iteration 17/25 | Loss: 0.00149660
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014965971931815147, 0.0014965971931815147, 0.0014965971931815147, 0.0014965971931815147, 0.0014965971931815147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014965971931815147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149660
Iteration 2/1000 | Loss: 0.00002628
Iteration 3/1000 | Loss: 0.00001753
Iteration 4/1000 | Loss: 0.00001363
Iteration 5/1000 | Loss: 0.00001273
Iteration 6/1000 | Loss: 0.00001178
Iteration 7/1000 | Loss: 0.00001117
Iteration 8/1000 | Loss: 0.00001081
Iteration 9/1000 | Loss: 0.00001053
Iteration 10/1000 | Loss: 0.00001023
Iteration 11/1000 | Loss: 0.00001001
Iteration 12/1000 | Loss: 0.00000998
Iteration 13/1000 | Loss: 0.00000996
Iteration 14/1000 | Loss: 0.00000988
Iteration 15/1000 | Loss: 0.00000988
Iteration 16/1000 | Loss: 0.00000987
Iteration 17/1000 | Loss: 0.00000987
Iteration 18/1000 | Loss: 0.00000985
Iteration 19/1000 | Loss: 0.00000981
Iteration 20/1000 | Loss: 0.00000979
Iteration 21/1000 | Loss: 0.00000977
Iteration 22/1000 | Loss: 0.00000975
Iteration 23/1000 | Loss: 0.00000974
Iteration 24/1000 | Loss: 0.00000972
Iteration 25/1000 | Loss: 0.00000971
Iteration 26/1000 | Loss: 0.00000971
Iteration 27/1000 | Loss: 0.00000971
Iteration 28/1000 | Loss: 0.00000970
Iteration 29/1000 | Loss: 0.00000969
Iteration 30/1000 | Loss: 0.00000968
Iteration 31/1000 | Loss: 0.00000968
Iteration 32/1000 | Loss: 0.00000967
Iteration 33/1000 | Loss: 0.00000965
Iteration 34/1000 | Loss: 0.00000961
Iteration 35/1000 | Loss: 0.00000960
Iteration 36/1000 | Loss: 0.00000958
Iteration 37/1000 | Loss: 0.00000958
Iteration 38/1000 | Loss: 0.00000957
Iteration 39/1000 | Loss: 0.00000956
Iteration 40/1000 | Loss: 0.00000951
Iteration 41/1000 | Loss: 0.00000948
Iteration 42/1000 | Loss: 0.00000947
Iteration 43/1000 | Loss: 0.00000947
Iteration 44/1000 | Loss: 0.00000946
Iteration 45/1000 | Loss: 0.00000945
Iteration 46/1000 | Loss: 0.00000945
Iteration 47/1000 | Loss: 0.00000944
Iteration 48/1000 | Loss: 0.00000944
Iteration 49/1000 | Loss: 0.00000941
Iteration 50/1000 | Loss: 0.00000941
Iteration 51/1000 | Loss: 0.00000941
Iteration 52/1000 | Loss: 0.00000941
Iteration 53/1000 | Loss: 0.00000941
Iteration 54/1000 | Loss: 0.00000939
Iteration 55/1000 | Loss: 0.00000939
Iteration 56/1000 | Loss: 0.00000935
Iteration 57/1000 | Loss: 0.00000933
Iteration 58/1000 | Loss: 0.00000933
Iteration 59/1000 | Loss: 0.00000933
Iteration 60/1000 | Loss: 0.00000932
Iteration 61/1000 | Loss: 0.00000932
Iteration 62/1000 | Loss: 0.00000932
Iteration 63/1000 | Loss: 0.00000931
Iteration 64/1000 | Loss: 0.00000931
Iteration 65/1000 | Loss: 0.00000931
Iteration 66/1000 | Loss: 0.00000930
Iteration 67/1000 | Loss: 0.00000930
Iteration 68/1000 | Loss: 0.00000930
Iteration 69/1000 | Loss: 0.00000930
Iteration 70/1000 | Loss: 0.00000930
Iteration 71/1000 | Loss: 0.00000930
Iteration 72/1000 | Loss: 0.00000929
Iteration 73/1000 | Loss: 0.00000929
Iteration 74/1000 | Loss: 0.00000928
Iteration 75/1000 | Loss: 0.00000928
Iteration 76/1000 | Loss: 0.00000928
Iteration 77/1000 | Loss: 0.00000928
Iteration 78/1000 | Loss: 0.00000928
Iteration 79/1000 | Loss: 0.00000928
Iteration 80/1000 | Loss: 0.00000928
Iteration 81/1000 | Loss: 0.00000927
Iteration 82/1000 | Loss: 0.00000926
Iteration 83/1000 | Loss: 0.00000926
Iteration 84/1000 | Loss: 0.00000926
Iteration 85/1000 | Loss: 0.00000926
Iteration 86/1000 | Loss: 0.00000925
Iteration 87/1000 | Loss: 0.00000925
Iteration 88/1000 | Loss: 0.00000925
Iteration 89/1000 | Loss: 0.00000925
Iteration 90/1000 | Loss: 0.00000925
Iteration 91/1000 | Loss: 0.00000924
Iteration 92/1000 | Loss: 0.00000924
Iteration 93/1000 | Loss: 0.00000924
Iteration 94/1000 | Loss: 0.00000924
Iteration 95/1000 | Loss: 0.00000924
Iteration 96/1000 | Loss: 0.00000924
Iteration 97/1000 | Loss: 0.00000924
Iteration 98/1000 | Loss: 0.00000924
Iteration 99/1000 | Loss: 0.00000924
Iteration 100/1000 | Loss: 0.00000924
Iteration 101/1000 | Loss: 0.00000923
Iteration 102/1000 | Loss: 0.00000923
Iteration 103/1000 | Loss: 0.00000923
Iteration 104/1000 | Loss: 0.00000922
Iteration 105/1000 | Loss: 0.00000922
Iteration 106/1000 | Loss: 0.00000922
Iteration 107/1000 | Loss: 0.00000922
Iteration 108/1000 | Loss: 0.00000921
Iteration 109/1000 | Loss: 0.00000921
Iteration 110/1000 | Loss: 0.00000920
Iteration 111/1000 | Loss: 0.00000920
Iteration 112/1000 | Loss: 0.00000919
Iteration 113/1000 | Loss: 0.00000919
Iteration 114/1000 | Loss: 0.00000919
Iteration 115/1000 | Loss: 0.00000919
Iteration 116/1000 | Loss: 0.00000918
Iteration 117/1000 | Loss: 0.00000918
Iteration 118/1000 | Loss: 0.00000918
Iteration 119/1000 | Loss: 0.00000918
Iteration 120/1000 | Loss: 0.00000918
Iteration 121/1000 | Loss: 0.00000917
Iteration 122/1000 | Loss: 0.00000917
Iteration 123/1000 | Loss: 0.00000917
Iteration 124/1000 | Loss: 0.00000917
Iteration 125/1000 | Loss: 0.00000917
Iteration 126/1000 | Loss: 0.00000917
Iteration 127/1000 | Loss: 0.00000917
Iteration 128/1000 | Loss: 0.00000916
Iteration 129/1000 | Loss: 0.00000916
Iteration 130/1000 | Loss: 0.00000916
Iteration 131/1000 | Loss: 0.00000916
Iteration 132/1000 | Loss: 0.00000916
Iteration 133/1000 | Loss: 0.00000916
Iteration 134/1000 | Loss: 0.00000916
Iteration 135/1000 | Loss: 0.00000916
Iteration 136/1000 | Loss: 0.00000916
Iteration 137/1000 | Loss: 0.00000915
Iteration 138/1000 | Loss: 0.00000915
Iteration 139/1000 | Loss: 0.00000915
Iteration 140/1000 | Loss: 0.00000915
Iteration 141/1000 | Loss: 0.00000915
Iteration 142/1000 | Loss: 0.00000915
Iteration 143/1000 | Loss: 0.00000915
Iteration 144/1000 | Loss: 0.00000915
Iteration 145/1000 | Loss: 0.00000915
Iteration 146/1000 | Loss: 0.00000915
Iteration 147/1000 | Loss: 0.00000915
Iteration 148/1000 | Loss: 0.00000915
Iteration 149/1000 | Loss: 0.00000915
Iteration 150/1000 | Loss: 0.00000915
Iteration 151/1000 | Loss: 0.00000914
Iteration 152/1000 | Loss: 0.00000914
Iteration 153/1000 | Loss: 0.00000914
Iteration 154/1000 | Loss: 0.00000914
Iteration 155/1000 | Loss: 0.00000914
Iteration 156/1000 | Loss: 0.00000914
Iteration 157/1000 | Loss: 0.00000914
Iteration 158/1000 | Loss: 0.00000914
Iteration 159/1000 | Loss: 0.00000914
Iteration 160/1000 | Loss: 0.00000913
Iteration 161/1000 | Loss: 0.00000913
Iteration 162/1000 | Loss: 0.00000913
Iteration 163/1000 | Loss: 0.00000913
Iteration 164/1000 | Loss: 0.00000913
Iteration 165/1000 | Loss: 0.00000913
Iteration 166/1000 | Loss: 0.00000913
Iteration 167/1000 | Loss: 0.00000913
Iteration 168/1000 | Loss: 0.00000913
Iteration 169/1000 | Loss: 0.00000913
Iteration 170/1000 | Loss: 0.00000913
Iteration 171/1000 | Loss: 0.00000913
Iteration 172/1000 | Loss: 0.00000912
Iteration 173/1000 | Loss: 0.00000912
Iteration 174/1000 | Loss: 0.00000912
Iteration 175/1000 | Loss: 0.00000912
Iteration 176/1000 | Loss: 0.00000912
Iteration 177/1000 | Loss: 0.00000911
Iteration 178/1000 | Loss: 0.00000911
Iteration 179/1000 | Loss: 0.00000911
Iteration 180/1000 | Loss: 0.00000911
Iteration 181/1000 | Loss: 0.00000911
Iteration 182/1000 | Loss: 0.00000911
Iteration 183/1000 | Loss: 0.00000911
Iteration 184/1000 | Loss: 0.00000911
Iteration 185/1000 | Loss: 0.00000911
Iteration 186/1000 | Loss: 0.00000910
Iteration 187/1000 | Loss: 0.00000910
Iteration 188/1000 | Loss: 0.00000910
Iteration 189/1000 | Loss: 0.00000910
Iteration 190/1000 | Loss: 0.00000910
Iteration 191/1000 | Loss: 0.00000910
Iteration 192/1000 | Loss: 0.00000910
Iteration 193/1000 | Loss: 0.00000910
Iteration 194/1000 | Loss: 0.00000910
Iteration 195/1000 | Loss: 0.00000910
Iteration 196/1000 | Loss: 0.00000910
Iteration 197/1000 | Loss: 0.00000910
Iteration 198/1000 | Loss: 0.00000910
Iteration 199/1000 | Loss: 0.00000910
Iteration 200/1000 | Loss: 0.00000910
Iteration 201/1000 | Loss: 0.00000910
Iteration 202/1000 | Loss: 0.00000910
Iteration 203/1000 | Loss: 0.00000910
Iteration 204/1000 | Loss: 0.00000910
Iteration 205/1000 | Loss: 0.00000910
Iteration 206/1000 | Loss: 0.00000910
Iteration 207/1000 | Loss: 0.00000910
Iteration 208/1000 | Loss: 0.00000910
Iteration 209/1000 | Loss: 0.00000910
Iteration 210/1000 | Loss: 0.00000910
Iteration 211/1000 | Loss: 0.00000910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [9.10064500203589e-06, 9.10064500203589e-06, 9.10064500203589e-06, 9.10064500203589e-06, 9.10064500203589e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.10064500203589e-06

Optimization complete. Final v2v error: 2.5880613327026367 mm

Highest mean error: 3.0445282459259033 mm for frame 101

Lowest mean error: 2.3797199726104736 mm for frame 142

Saving results

Total time: 46.38354182243347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00696810
Iteration 2/25 | Loss: 0.00154313
Iteration 3/25 | Loss: 0.00128780
Iteration 4/25 | Loss: 0.00124814
Iteration 5/25 | Loss: 0.00123803
Iteration 6/25 | Loss: 0.00123403
Iteration 7/25 | Loss: 0.00123131
Iteration 8/25 | Loss: 0.00122961
Iteration 9/25 | Loss: 0.00122828
Iteration 10/25 | Loss: 0.00122784
Iteration 11/25 | Loss: 0.00122899
Iteration 12/25 | Loss: 0.00122778
Iteration 13/25 | Loss: 0.00122764
Iteration 14/25 | Loss: 0.00122727
Iteration 15/25 | Loss: 0.00122762
Iteration 16/25 | Loss: 0.00122707
Iteration 17/25 | Loss: 0.00122751
Iteration 18/25 | Loss: 0.00122672
Iteration 19/25 | Loss: 0.00122809
Iteration 20/25 | Loss: 0.00122662
Iteration 21/25 | Loss: 0.00122662
Iteration 22/25 | Loss: 0.00122713
Iteration 23/25 | Loss: 0.00122684
Iteration 24/25 | Loss: 0.00122755
Iteration 25/25 | Loss: 0.00122654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.43602514
Iteration 2/25 | Loss: 0.00147161
Iteration 3/25 | Loss: 0.00147161
Iteration 4/25 | Loss: 0.00147161
Iteration 5/25 | Loss: 0.00147160
Iteration 6/25 | Loss: 0.00147160
Iteration 7/25 | Loss: 0.00147160
Iteration 8/25 | Loss: 0.00147160
Iteration 9/25 | Loss: 0.00147160
Iteration 10/25 | Loss: 0.00147160
Iteration 11/25 | Loss: 0.00147160
Iteration 12/25 | Loss: 0.00147160
Iteration 13/25 | Loss: 0.00147160
Iteration 14/25 | Loss: 0.00147160
Iteration 15/25 | Loss: 0.00147160
Iteration 16/25 | Loss: 0.00147160
Iteration 17/25 | Loss: 0.00147160
Iteration 18/25 | Loss: 0.00147160
Iteration 19/25 | Loss: 0.00147160
Iteration 20/25 | Loss: 0.00147160
Iteration 21/25 | Loss: 0.00147160
Iteration 22/25 | Loss: 0.00147160
Iteration 23/25 | Loss: 0.00147160
Iteration 24/25 | Loss: 0.00147160
Iteration 25/25 | Loss: 0.00147160
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014716028235852718, 0.0014716028235852718, 0.0014716028235852718, 0.0014716028235852718, 0.0014716028235852718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014716028235852718

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147160
Iteration 2/1000 | Loss: 0.00005124
Iteration 3/1000 | Loss: 0.00011902
Iteration 4/1000 | Loss: 0.00008214
Iteration 5/1000 | Loss: 0.00002671
Iteration 6/1000 | Loss: 0.00002732
Iteration 7/1000 | Loss: 0.00002549
Iteration 8/1000 | Loss: 0.00002256
Iteration 9/1000 | Loss: 0.00002985
Iteration 10/1000 | Loss: 0.00002303
Iteration 11/1000 | Loss: 0.00002070
Iteration 12/1000 | Loss: 0.00001938
Iteration 13/1000 | Loss: 0.00001876
Iteration 14/1000 | Loss: 0.00012141
Iteration 15/1000 | Loss: 0.00001947
Iteration 16/1000 | Loss: 0.00002092
Iteration 17/1000 | Loss: 0.00004710
Iteration 18/1000 | Loss: 0.00002472
Iteration 19/1000 | Loss: 0.00002135
Iteration 20/1000 | Loss: 0.00002152
Iteration 21/1000 | Loss: 0.00007055
Iteration 22/1000 | Loss: 0.00013152
Iteration 23/1000 | Loss: 0.00002178
Iteration 24/1000 | Loss: 0.00002768
Iteration 25/1000 | Loss: 0.00008643
Iteration 26/1000 | Loss: 0.00002682
Iteration 27/1000 | Loss: 0.00002626
Iteration 28/1000 | Loss: 0.00002208
Iteration 29/1000 | Loss: 0.00004212
Iteration 30/1000 | Loss: 0.00002762
Iteration 31/1000 | Loss: 0.00002608
Iteration 32/1000 | Loss: 0.00002713
Iteration 33/1000 | Loss: 0.00002713
Iteration 34/1000 | Loss: 0.00002628
Iteration 35/1000 | Loss: 0.00002725
Iteration 36/1000 | Loss: 0.00002451
Iteration 37/1000 | Loss: 0.00002092
Iteration 38/1000 | Loss: 0.00002334
Iteration 39/1000 | Loss: 0.00002189
Iteration 40/1000 | Loss: 0.00002510
Iteration 41/1000 | Loss: 0.00002458
Iteration 42/1000 | Loss: 0.00002417
Iteration 43/1000 | Loss: 0.00002246
Iteration 44/1000 | Loss: 0.00002279
Iteration 45/1000 | Loss: 0.00002511
Iteration 46/1000 | Loss: 0.00004885
Iteration 47/1000 | Loss: 0.00003278
Iteration 48/1000 | Loss: 0.00002991
Iteration 49/1000 | Loss: 0.00003415
Iteration 50/1000 | Loss: 0.00003340
Iteration 51/1000 | Loss: 0.00003825
Iteration 52/1000 | Loss: 0.00002665
Iteration 53/1000 | Loss: 0.00002090
Iteration 54/1000 | Loss: 0.00002600
Iteration 55/1000 | Loss: 0.00002235
Iteration 56/1000 | Loss: 0.00002832
Iteration 57/1000 | Loss: 0.00002162
Iteration 58/1000 | Loss: 0.00002207
Iteration 59/1000 | Loss: 0.00002643
Iteration 60/1000 | Loss: 0.00002061
Iteration 61/1000 | Loss: 0.00001722
Iteration 62/1000 | Loss: 0.00001719
Iteration 63/1000 | Loss: 0.00001719
Iteration 64/1000 | Loss: 0.00001717
Iteration 65/1000 | Loss: 0.00001716
Iteration 66/1000 | Loss: 0.00001716
Iteration 67/1000 | Loss: 0.00001714
Iteration 68/1000 | Loss: 0.00001703
Iteration 69/1000 | Loss: 0.00002274
Iteration 70/1000 | Loss: 0.00004097
Iteration 71/1000 | Loss: 0.00002411
Iteration 72/1000 | Loss: 0.00002003
Iteration 73/1000 | Loss: 0.00002312
Iteration 74/1000 | Loss: 0.00002942
Iteration 75/1000 | Loss: 0.00002252
Iteration 76/1000 | Loss: 0.00001985
Iteration 77/1000 | Loss: 0.00002600
Iteration 78/1000 | Loss: 0.00004424
Iteration 79/1000 | Loss: 0.00003319
Iteration 80/1000 | Loss: 0.00002357
Iteration 81/1000 | Loss: 0.00020315
Iteration 82/1000 | Loss: 0.00029981
Iteration 83/1000 | Loss: 0.00003869
Iteration 84/1000 | Loss: 0.00002743
Iteration 85/1000 | Loss: 0.00002348
Iteration 86/1000 | Loss: 0.00004011
Iteration 87/1000 | Loss: 0.00001819
Iteration 88/1000 | Loss: 0.00001757
Iteration 89/1000 | Loss: 0.00001724
Iteration 90/1000 | Loss: 0.00007869
Iteration 91/1000 | Loss: 0.00001697
Iteration 92/1000 | Loss: 0.00001695
Iteration 93/1000 | Loss: 0.00001676
Iteration 94/1000 | Loss: 0.00001674
Iteration 95/1000 | Loss: 0.00001671
Iteration 96/1000 | Loss: 0.00001670
Iteration 97/1000 | Loss: 0.00001664
Iteration 98/1000 | Loss: 0.00001662
Iteration 99/1000 | Loss: 0.00001661
Iteration 100/1000 | Loss: 0.00001661
Iteration 101/1000 | Loss: 0.00001660
Iteration 102/1000 | Loss: 0.00001660
Iteration 103/1000 | Loss: 0.00001659
Iteration 104/1000 | Loss: 0.00001659
Iteration 105/1000 | Loss: 0.00001658
Iteration 106/1000 | Loss: 0.00001658
Iteration 107/1000 | Loss: 0.00001656
Iteration 108/1000 | Loss: 0.00001655
Iteration 109/1000 | Loss: 0.00006208
Iteration 110/1000 | Loss: 0.00001656
Iteration 111/1000 | Loss: 0.00001650
Iteration 112/1000 | Loss: 0.00001650
Iteration 113/1000 | Loss: 0.00001650
Iteration 114/1000 | Loss: 0.00001650
Iteration 115/1000 | Loss: 0.00001650
Iteration 116/1000 | Loss: 0.00001649
Iteration 117/1000 | Loss: 0.00001649
Iteration 118/1000 | Loss: 0.00001649
Iteration 119/1000 | Loss: 0.00001649
Iteration 120/1000 | Loss: 0.00001649
Iteration 121/1000 | Loss: 0.00001649
Iteration 122/1000 | Loss: 0.00001649
Iteration 123/1000 | Loss: 0.00001649
Iteration 124/1000 | Loss: 0.00001649
Iteration 125/1000 | Loss: 0.00001649
Iteration 126/1000 | Loss: 0.00001649
Iteration 127/1000 | Loss: 0.00001649
Iteration 128/1000 | Loss: 0.00001649
Iteration 129/1000 | Loss: 0.00001649
Iteration 130/1000 | Loss: 0.00001649
Iteration 131/1000 | Loss: 0.00001649
Iteration 132/1000 | Loss: 0.00001649
Iteration 133/1000 | Loss: 0.00001649
Iteration 134/1000 | Loss: 0.00001649
Iteration 135/1000 | Loss: 0.00001649
Iteration 136/1000 | Loss: 0.00001649
Iteration 137/1000 | Loss: 0.00001649
Iteration 138/1000 | Loss: 0.00001649
Iteration 139/1000 | Loss: 0.00001649
Iteration 140/1000 | Loss: 0.00001649
Iteration 141/1000 | Loss: 0.00001649
Iteration 142/1000 | Loss: 0.00001649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.6493124348926358e-05, 1.6493124348926358e-05, 1.6493124348926358e-05, 1.6493124348926358e-05, 1.6493124348926358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6493124348926358e-05

Optimization complete. Final v2v error: 3.3440873622894287 mm

Highest mean error: 5.438605308532715 mm for frame 67

Lowest mean error: 2.5109212398529053 mm for frame 196

Saving results

Total time: 188.41061186790466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818550
Iteration 2/25 | Loss: 0.00148425
Iteration 3/25 | Loss: 0.00130561
Iteration 4/25 | Loss: 0.00129159
Iteration 5/25 | Loss: 0.00128985
Iteration 6/25 | Loss: 0.00128985
Iteration 7/25 | Loss: 0.00128985
Iteration 8/25 | Loss: 0.00128985
Iteration 9/25 | Loss: 0.00128985
Iteration 10/25 | Loss: 0.00128985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012898534769192338, 0.0012898534769192338, 0.0012898534769192338, 0.0012898534769192338, 0.0012898534769192338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012898534769192338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92990071
Iteration 2/25 | Loss: 0.00091499
Iteration 3/25 | Loss: 0.00091499
Iteration 4/25 | Loss: 0.00091499
Iteration 5/25 | Loss: 0.00091499
Iteration 6/25 | Loss: 0.00091499
Iteration 7/25 | Loss: 0.00091499
Iteration 8/25 | Loss: 0.00091499
Iteration 9/25 | Loss: 0.00091499
Iteration 10/25 | Loss: 0.00091499
Iteration 11/25 | Loss: 0.00091499
Iteration 12/25 | Loss: 0.00091499
Iteration 13/25 | Loss: 0.00091499
Iteration 14/25 | Loss: 0.00091499
Iteration 15/25 | Loss: 0.00091499
Iteration 16/25 | Loss: 0.00091499
Iteration 17/25 | Loss: 0.00091499
Iteration 18/25 | Loss: 0.00091499
Iteration 19/25 | Loss: 0.00091499
Iteration 20/25 | Loss: 0.00091499
Iteration 21/25 | Loss: 0.00091499
Iteration 22/25 | Loss: 0.00091499
Iteration 23/25 | Loss: 0.00091499
Iteration 24/25 | Loss: 0.00091499
Iteration 25/25 | Loss: 0.00091499

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091499
Iteration 2/1000 | Loss: 0.00004135
Iteration 3/1000 | Loss: 0.00002984
Iteration 4/1000 | Loss: 0.00002713
Iteration 5/1000 | Loss: 0.00002542
Iteration 6/1000 | Loss: 0.00002480
Iteration 7/1000 | Loss: 0.00002400
Iteration 8/1000 | Loss: 0.00002351
Iteration 9/1000 | Loss: 0.00002324
Iteration 10/1000 | Loss: 0.00002286
Iteration 11/1000 | Loss: 0.00002252
Iteration 12/1000 | Loss: 0.00002216
Iteration 13/1000 | Loss: 0.00002182
Iteration 14/1000 | Loss: 0.00002165
Iteration 15/1000 | Loss: 0.00002164
Iteration 16/1000 | Loss: 0.00002157
Iteration 17/1000 | Loss: 0.00002148
Iteration 18/1000 | Loss: 0.00002142
Iteration 19/1000 | Loss: 0.00002125
Iteration 20/1000 | Loss: 0.00002123
Iteration 21/1000 | Loss: 0.00002115
Iteration 22/1000 | Loss: 0.00002107
Iteration 23/1000 | Loss: 0.00002097
Iteration 24/1000 | Loss: 0.00002092
Iteration 25/1000 | Loss: 0.00002092
Iteration 26/1000 | Loss: 0.00002092
Iteration 27/1000 | Loss: 0.00002092
Iteration 28/1000 | Loss: 0.00002092
Iteration 29/1000 | Loss: 0.00002092
Iteration 30/1000 | Loss: 0.00002092
Iteration 31/1000 | Loss: 0.00002092
Iteration 32/1000 | Loss: 0.00002092
Iteration 33/1000 | Loss: 0.00002092
Iteration 34/1000 | Loss: 0.00002092
Iteration 35/1000 | Loss: 0.00002092
Iteration 36/1000 | Loss: 0.00002092
Iteration 37/1000 | Loss: 0.00002092
Iteration 38/1000 | Loss: 0.00002091
Iteration 39/1000 | Loss: 0.00002091
Iteration 40/1000 | Loss: 0.00002091
Iteration 41/1000 | Loss: 0.00002090
Iteration 42/1000 | Loss: 0.00002090
Iteration 43/1000 | Loss: 0.00002090
Iteration 44/1000 | Loss: 0.00002090
Iteration 45/1000 | Loss: 0.00002090
Iteration 46/1000 | Loss: 0.00002090
Iteration 47/1000 | Loss: 0.00002089
Iteration 48/1000 | Loss: 0.00002089
Iteration 49/1000 | Loss: 0.00002089
Iteration 50/1000 | Loss: 0.00002089
Iteration 51/1000 | Loss: 0.00002088
Iteration 52/1000 | Loss: 0.00002088
Iteration 53/1000 | Loss: 0.00002088
Iteration 54/1000 | Loss: 0.00002088
Iteration 55/1000 | Loss: 0.00002088
Iteration 56/1000 | Loss: 0.00002088
Iteration 57/1000 | Loss: 0.00002087
Iteration 58/1000 | Loss: 0.00002087
Iteration 59/1000 | Loss: 0.00002087
Iteration 60/1000 | Loss: 0.00002087
Iteration 61/1000 | Loss: 0.00002087
Iteration 62/1000 | Loss: 0.00002087
Iteration 63/1000 | Loss: 0.00002087
Iteration 64/1000 | Loss: 0.00002087
Iteration 65/1000 | Loss: 0.00002087
Iteration 66/1000 | Loss: 0.00002087
Iteration 67/1000 | Loss: 0.00002087
Iteration 68/1000 | Loss: 0.00002087
Iteration 69/1000 | Loss: 0.00002086
Iteration 70/1000 | Loss: 0.00002086
Iteration 71/1000 | Loss: 0.00002086
Iteration 72/1000 | Loss: 0.00002086
Iteration 73/1000 | Loss: 0.00002086
Iteration 74/1000 | Loss: 0.00002085
Iteration 75/1000 | Loss: 0.00002085
Iteration 76/1000 | Loss: 0.00002085
Iteration 77/1000 | Loss: 0.00002085
Iteration 78/1000 | Loss: 0.00002085
Iteration 79/1000 | Loss: 0.00002085
Iteration 80/1000 | Loss: 0.00002085
Iteration 81/1000 | Loss: 0.00002085
Iteration 82/1000 | Loss: 0.00002085
Iteration 83/1000 | Loss: 0.00002085
Iteration 84/1000 | Loss: 0.00002085
Iteration 85/1000 | Loss: 0.00002085
Iteration 86/1000 | Loss: 0.00002085
Iteration 87/1000 | Loss: 0.00002085
Iteration 88/1000 | Loss: 0.00002085
Iteration 89/1000 | Loss: 0.00002085
Iteration 90/1000 | Loss: 0.00002085
Iteration 91/1000 | Loss: 0.00002085
Iteration 92/1000 | Loss: 0.00002085
Iteration 93/1000 | Loss: 0.00002085
Iteration 94/1000 | Loss: 0.00002085
Iteration 95/1000 | Loss: 0.00002085
Iteration 96/1000 | Loss: 0.00002084
Iteration 97/1000 | Loss: 0.00002084
Iteration 98/1000 | Loss: 0.00002084
Iteration 99/1000 | Loss: 0.00002084
Iteration 100/1000 | Loss: 0.00002084
Iteration 101/1000 | Loss: 0.00002084
Iteration 102/1000 | Loss: 0.00002084
Iteration 103/1000 | Loss: 0.00002084
Iteration 104/1000 | Loss: 0.00002084
Iteration 105/1000 | Loss: 0.00002084
Iteration 106/1000 | Loss: 0.00002083
Iteration 107/1000 | Loss: 0.00002083
Iteration 108/1000 | Loss: 0.00002083
Iteration 109/1000 | Loss: 0.00002083
Iteration 110/1000 | Loss: 0.00002083
Iteration 111/1000 | Loss: 0.00002083
Iteration 112/1000 | Loss: 0.00002083
Iteration 113/1000 | Loss: 0.00002083
Iteration 114/1000 | Loss: 0.00002083
Iteration 115/1000 | Loss: 0.00002082
Iteration 116/1000 | Loss: 0.00002082
Iteration 117/1000 | Loss: 0.00002082
Iteration 118/1000 | Loss: 0.00002081
Iteration 119/1000 | Loss: 0.00002081
Iteration 120/1000 | Loss: 0.00002081
Iteration 121/1000 | Loss: 0.00002080
Iteration 122/1000 | Loss: 0.00002079
Iteration 123/1000 | Loss: 0.00002079
Iteration 124/1000 | Loss: 0.00002079
Iteration 125/1000 | Loss: 0.00002078
Iteration 126/1000 | Loss: 0.00002078
Iteration 127/1000 | Loss: 0.00002078
Iteration 128/1000 | Loss: 0.00002077
Iteration 129/1000 | Loss: 0.00002077
Iteration 130/1000 | Loss: 0.00002077
Iteration 131/1000 | Loss: 0.00002077
Iteration 132/1000 | Loss: 0.00002077
Iteration 133/1000 | Loss: 0.00002077
Iteration 134/1000 | Loss: 0.00002077
Iteration 135/1000 | Loss: 0.00002076
Iteration 136/1000 | Loss: 0.00002076
Iteration 137/1000 | Loss: 0.00002076
Iteration 138/1000 | Loss: 0.00002076
Iteration 139/1000 | Loss: 0.00002076
Iteration 140/1000 | Loss: 0.00002076
Iteration 141/1000 | Loss: 0.00002076
Iteration 142/1000 | Loss: 0.00002076
Iteration 143/1000 | Loss: 0.00002076
Iteration 144/1000 | Loss: 0.00002076
Iteration 145/1000 | Loss: 0.00002076
Iteration 146/1000 | Loss: 0.00002075
Iteration 147/1000 | Loss: 0.00002075
Iteration 148/1000 | Loss: 0.00002075
Iteration 149/1000 | Loss: 0.00002075
Iteration 150/1000 | Loss: 0.00002075
Iteration 151/1000 | Loss: 0.00002075
Iteration 152/1000 | Loss: 0.00002075
Iteration 153/1000 | Loss: 0.00002075
Iteration 154/1000 | Loss: 0.00002075
Iteration 155/1000 | Loss: 0.00002075
Iteration 156/1000 | Loss: 0.00002075
Iteration 157/1000 | Loss: 0.00002074
Iteration 158/1000 | Loss: 0.00002074
Iteration 159/1000 | Loss: 0.00002074
Iteration 160/1000 | Loss: 0.00002074
Iteration 161/1000 | Loss: 0.00002074
Iteration 162/1000 | Loss: 0.00002074
Iteration 163/1000 | Loss: 0.00002074
Iteration 164/1000 | Loss: 0.00002073
Iteration 165/1000 | Loss: 0.00002073
Iteration 166/1000 | Loss: 0.00002073
Iteration 167/1000 | Loss: 0.00002073
Iteration 168/1000 | Loss: 0.00002073
Iteration 169/1000 | Loss: 0.00002073
Iteration 170/1000 | Loss: 0.00002073
Iteration 171/1000 | Loss: 0.00002073
Iteration 172/1000 | Loss: 0.00002073
Iteration 173/1000 | Loss: 0.00002073
Iteration 174/1000 | Loss: 0.00002073
Iteration 175/1000 | Loss: 0.00002073
Iteration 176/1000 | Loss: 0.00002073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [2.0733587007271126e-05, 2.0733587007271126e-05, 2.0733587007271126e-05, 2.0733587007271126e-05, 2.0733587007271126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0733587007271126e-05

Optimization complete. Final v2v error: 3.848615884780884 mm

Highest mean error: 3.887127637863159 mm for frame 141

Lowest mean error: 3.815484046936035 mm for frame 78

Saving results

Total time: 42.752044677734375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421460
Iteration 2/25 | Loss: 0.00130152
Iteration 3/25 | Loss: 0.00123152
Iteration 4/25 | Loss: 0.00122264
Iteration 5/25 | Loss: 0.00122056
Iteration 6/25 | Loss: 0.00121971
Iteration 7/25 | Loss: 0.00121971
Iteration 8/25 | Loss: 0.00121971
Iteration 9/25 | Loss: 0.00121971
Iteration 10/25 | Loss: 0.00121971
Iteration 11/25 | Loss: 0.00121971
Iteration 12/25 | Loss: 0.00121971
Iteration 13/25 | Loss: 0.00121971
Iteration 14/25 | Loss: 0.00121971
Iteration 15/25 | Loss: 0.00121971
Iteration 16/25 | Loss: 0.00121971
Iteration 17/25 | Loss: 0.00121971
Iteration 18/25 | Loss: 0.00121971
Iteration 19/25 | Loss: 0.00121971
Iteration 20/25 | Loss: 0.00121971
Iteration 21/25 | Loss: 0.00121971
Iteration 22/25 | Loss: 0.00121971
Iteration 23/25 | Loss: 0.00121971
Iteration 24/25 | Loss: 0.00121971
Iteration 25/25 | Loss: 0.00121971

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29398012
Iteration 2/25 | Loss: 0.00164699
Iteration 3/25 | Loss: 0.00164699
Iteration 4/25 | Loss: 0.00164699
Iteration 5/25 | Loss: 0.00164699
Iteration 6/25 | Loss: 0.00164699
Iteration 7/25 | Loss: 0.00164699
Iteration 8/25 | Loss: 0.00164699
Iteration 9/25 | Loss: 0.00164699
Iteration 10/25 | Loss: 0.00164699
Iteration 11/25 | Loss: 0.00164699
Iteration 12/25 | Loss: 0.00164699
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0016469857655465603, 0.0016469857655465603, 0.0016469857655465603, 0.0016469857655465603, 0.0016469857655465603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016469857655465603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164699
Iteration 2/1000 | Loss: 0.00002663
Iteration 3/1000 | Loss: 0.00002018
Iteration 4/1000 | Loss: 0.00001837
Iteration 5/1000 | Loss: 0.00001764
Iteration 6/1000 | Loss: 0.00001715
Iteration 7/1000 | Loss: 0.00001655
Iteration 8/1000 | Loss: 0.00001631
Iteration 9/1000 | Loss: 0.00001606
Iteration 10/1000 | Loss: 0.00001578
Iteration 11/1000 | Loss: 0.00001572
Iteration 12/1000 | Loss: 0.00001567
Iteration 13/1000 | Loss: 0.00001565
Iteration 14/1000 | Loss: 0.00001564
Iteration 15/1000 | Loss: 0.00001558
Iteration 16/1000 | Loss: 0.00001558
Iteration 17/1000 | Loss: 0.00001555
Iteration 18/1000 | Loss: 0.00001551
Iteration 19/1000 | Loss: 0.00001551
Iteration 20/1000 | Loss: 0.00001550
Iteration 21/1000 | Loss: 0.00001549
Iteration 22/1000 | Loss: 0.00001549
Iteration 23/1000 | Loss: 0.00001549
Iteration 24/1000 | Loss: 0.00001548
Iteration 25/1000 | Loss: 0.00001546
Iteration 26/1000 | Loss: 0.00001545
Iteration 27/1000 | Loss: 0.00001544
Iteration 28/1000 | Loss: 0.00001541
Iteration 29/1000 | Loss: 0.00001541
Iteration 30/1000 | Loss: 0.00001540
Iteration 31/1000 | Loss: 0.00001538
Iteration 32/1000 | Loss: 0.00001537
Iteration 33/1000 | Loss: 0.00001536
Iteration 34/1000 | Loss: 0.00001533
Iteration 35/1000 | Loss: 0.00001530
Iteration 36/1000 | Loss: 0.00001530
Iteration 37/1000 | Loss: 0.00001530
Iteration 38/1000 | Loss: 0.00001529
Iteration 39/1000 | Loss: 0.00001529
Iteration 40/1000 | Loss: 0.00001529
Iteration 41/1000 | Loss: 0.00001528
Iteration 42/1000 | Loss: 0.00001528
Iteration 43/1000 | Loss: 0.00001528
Iteration 44/1000 | Loss: 0.00001527
Iteration 45/1000 | Loss: 0.00001526
Iteration 46/1000 | Loss: 0.00001526
Iteration 47/1000 | Loss: 0.00001526
Iteration 48/1000 | Loss: 0.00001526
Iteration 49/1000 | Loss: 0.00001525
Iteration 50/1000 | Loss: 0.00001525
Iteration 51/1000 | Loss: 0.00001525
Iteration 52/1000 | Loss: 0.00001525
Iteration 53/1000 | Loss: 0.00001525
Iteration 54/1000 | Loss: 0.00001525
Iteration 55/1000 | Loss: 0.00001525
Iteration 56/1000 | Loss: 0.00001525
Iteration 57/1000 | Loss: 0.00001525
Iteration 58/1000 | Loss: 0.00001525
Iteration 59/1000 | Loss: 0.00001524
Iteration 60/1000 | Loss: 0.00001524
Iteration 61/1000 | Loss: 0.00001524
Iteration 62/1000 | Loss: 0.00001524
Iteration 63/1000 | Loss: 0.00001524
Iteration 64/1000 | Loss: 0.00001524
Iteration 65/1000 | Loss: 0.00001523
Iteration 66/1000 | Loss: 0.00001523
Iteration 67/1000 | Loss: 0.00001522
Iteration 68/1000 | Loss: 0.00001522
Iteration 69/1000 | Loss: 0.00001522
Iteration 70/1000 | Loss: 0.00001522
Iteration 71/1000 | Loss: 0.00001522
Iteration 72/1000 | Loss: 0.00001522
Iteration 73/1000 | Loss: 0.00001521
Iteration 74/1000 | Loss: 0.00001521
Iteration 75/1000 | Loss: 0.00001521
Iteration 76/1000 | Loss: 0.00001521
Iteration 77/1000 | Loss: 0.00001521
Iteration 78/1000 | Loss: 0.00001521
Iteration 79/1000 | Loss: 0.00001521
Iteration 80/1000 | Loss: 0.00001520
Iteration 81/1000 | Loss: 0.00001519
Iteration 82/1000 | Loss: 0.00001519
Iteration 83/1000 | Loss: 0.00001518
Iteration 84/1000 | Loss: 0.00001518
Iteration 85/1000 | Loss: 0.00001518
Iteration 86/1000 | Loss: 0.00001518
Iteration 87/1000 | Loss: 0.00001518
Iteration 88/1000 | Loss: 0.00001518
Iteration 89/1000 | Loss: 0.00001518
Iteration 90/1000 | Loss: 0.00001518
Iteration 91/1000 | Loss: 0.00001518
Iteration 92/1000 | Loss: 0.00001518
Iteration 93/1000 | Loss: 0.00001518
Iteration 94/1000 | Loss: 0.00001518
Iteration 95/1000 | Loss: 0.00001518
Iteration 96/1000 | Loss: 0.00001517
Iteration 97/1000 | Loss: 0.00001516
Iteration 98/1000 | Loss: 0.00001516
Iteration 99/1000 | Loss: 0.00001516
Iteration 100/1000 | Loss: 0.00001516
Iteration 101/1000 | Loss: 0.00001516
Iteration 102/1000 | Loss: 0.00001516
Iteration 103/1000 | Loss: 0.00001516
Iteration 104/1000 | Loss: 0.00001516
Iteration 105/1000 | Loss: 0.00001516
Iteration 106/1000 | Loss: 0.00001516
Iteration 107/1000 | Loss: 0.00001516
Iteration 108/1000 | Loss: 0.00001516
Iteration 109/1000 | Loss: 0.00001515
Iteration 110/1000 | Loss: 0.00001515
Iteration 111/1000 | Loss: 0.00001515
Iteration 112/1000 | Loss: 0.00001515
Iteration 113/1000 | Loss: 0.00001515
Iteration 114/1000 | Loss: 0.00001515
Iteration 115/1000 | Loss: 0.00001514
Iteration 116/1000 | Loss: 0.00001514
Iteration 117/1000 | Loss: 0.00001514
Iteration 118/1000 | Loss: 0.00001513
Iteration 119/1000 | Loss: 0.00001513
Iteration 120/1000 | Loss: 0.00001513
Iteration 121/1000 | Loss: 0.00001513
Iteration 122/1000 | Loss: 0.00001513
Iteration 123/1000 | Loss: 0.00001513
Iteration 124/1000 | Loss: 0.00001513
Iteration 125/1000 | Loss: 0.00001513
Iteration 126/1000 | Loss: 0.00001513
Iteration 127/1000 | Loss: 0.00001513
Iteration 128/1000 | Loss: 0.00001513
Iteration 129/1000 | Loss: 0.00001513
Iteration 130/1000 | Loss: 0.00001512
Iteration 131/1000 | Loss: 0.00001512
Iteration 132/1000 | Loss: 0.00001511
Iteration 133/1000 | Loss: 0.00001511
Iteration 134/1000 | Loss: 0.00001511
Iteration 135/1000 | Loss: 0.00001511
Iteration 136/1000 | Loss: 0.00001511
Iteration 137/1000 | Loss: 0.00001510
Iteration 138/1000 | Loss: 0.00001510
Iteration 139/1000 | Loss: 0.00001510
Iteration 140/1000 | Loss: 0.00001510
Iteration 141/1000 | Loss: 0.00001510
Iteration 142/1000 | Loss: 0.00001510
Iteration 143/1000 | Loss: 0.00001510
Iteration 144/1000 | Loss: 0.00001510
Iteration 145/1000 | Loss: 0.00001509
Iteration 146/1000 | Loss: 0.00001509
Iteration 147/1000 | Loss: 0.00001509
Iteration 148/1000 | Loss: 0.00001509
Iteration 149/1000 | Loss: 0.00001508
Iteration 150/1000 | Loss: 0.00001508
Iteration 151/1000 | Loss: 0.00001508
Iteration 152/1000 | Loss: 0.00001508
Iteration 153/1000 | Loss: 0.00001508
Iteration 154/1000 | Loss: 0.00001508
Iteration 155/1000 | Loss: 0.00001508
Iteration 156/1000 | Loss: 0.00001508
Iteration 157/1000 | Loss: 0.00001508
Iteration 158/1000 | Loss: 0.00001508
Iteration 159/1000 | Loss: 0.00001508
Iteration 160/1000 | Loss: 0.00001508
Iteration 161/1000 | Loss: 0.00001508
Iteration 162/1000 | Loss: 0.00001508
Iteration 163/1000 | Loss: 0.00001508
Iteration 164/1000 | Loss: 0.00001507
Iteration 165/1000 | Loss: 0.00001507
Iteration 166/1000 | Loss: 0.00001507
Iteration 167/1000 | Loss: 0.00001507
Iteration 168/1000 | Loss: 0.00001507
Iteration 169/1000 | Loss: 0.00001507
Iteration 170/1000 | Loss: 0.00001507
Iteration 171/1000 | Loss: 0.00001507
Iteration 172/1000 | Loss: 0.00001507
Iteration 173/1000 | Loss: 0.00001507
Iteration 174/1000 | Loss: 0.00001507
Iteration 175/1000 | Loss: 0.00001507
Iteration 176/1000 | Loss: 0.00001507
Iteration 177/1000 | Loss: 0.00001507
Iteration 178/1000 | Loss: 0.00001507
Iteration 179/1000 | Loss: 0.00001507
Iteration 180/1000 | Loss: 0.00001506
Iteration 181/1000 | Loss: 0.00001506
Iteration 182/1000 | Loss: 0.00001506
Iteration 183/1000 | Loss: 0.00001506
Iteration 184/1000 | Loss: 0.00001506
Iteration 185/1000 | Loss: 0.00001506
Iteration 186/1000 | Loss: 0.00001506
Iteration 187/1000 | Loss: 0.00001506
Iteration 188/1000 | Loss: 0.00001506
Iteration 189/1000 | Loss: 0.00001505
Iteration 190/1000 | Loss: 0.00001505
Iteration 191/1000 | Loss: 0.00001505
Iteration 192/1000 | Loss: 0.00001505
Iteration 193/1000 | Loss: 0.00001505
Iteration 194/1000 | Loss: 0.00001505
Iteration 195/1000 | Loss: 0.00001505
Iteration 196/1000 | Loss: 0.00001505
Iteration 197/1000 | Loss: 0.00001505
Iteration 198/1000 | Loss: 0.00001505
Iteration 199/1000 | Loss: 0.00001505
Iteration 200/1000 | Loss: 0.00001505
Iteration 201/1000 | Loss: 0.00001505
Iteration 202/1000 | Loss: 0.00001505
Iteration 203/1000 | Loss: 0.00001505
Iteration 204/1000 | Loss: 0.00001505
Iteration 205/1000 | Loss: 0.00001505
Iteration 206/1000 | Loss: 0.00001505
Iteration 207/1000 | Loss: 0.00001505
Iteration 208/1000 | Loss: 0.00001505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.5051494301587809e-05, 1.5051494301587809e-05, 1.5051494301587809e-05, 1.5051494301587809e-05, 1.5051494301587809e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5051494301587809e-05

Optimization complete. Final v2v error: 3.269805669784546 mm

Highest mean error: 3.768542528152466 mm for frame 21

Lowest mean error: 3.040649652481079 mm for frame 72

Saving results

Total time: 40.146294832229614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403480
Iteration 2/25 | Loss: 0.00126369
Iteration 3/25 | Loss: 0.00119591
Iteration 4/25 | Loss: 0.00118746
Iteration 5/25 | Loss: 0.00118543
Iteration 6/25 | Loss: 0.00118492
Iteration 7/25 | Loss: 0.00118486
Iteration 8/25 | Loss: 0.00118486
Iteration 9/25 | Loss: 0.00118486
Iteration 10/25 | Loss: 0.00118486
Iteration 11/25 | Loss: 0.00118486
Iteration 12/25 | Loss: 0.00118486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011848646681755781, 0.0011848646681755781, 0.0011848646681755781, 0.0011848646681755781, 0.0011848646681755781]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011848646681755781

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38233936
Iteration 2/25 | Loss: 0.00134566
Iteration 3/25 | Loss: 0.00134566
Iteration 4/25 | Loss: 0.00134566
Iteration 5/25 | Loss: 0.00134566
Iteration 6/25 | Loss: 0.00134566
Iteration 7/25 | Loss: 0.00134566
Iteration 8/25 | Loss: 0.00134566
Iteration 9/25 | Loss: 0.00134566
Iteration 10/25 | Loss: 0.00134565
Iteration 11/25 | Loss: 0.00134565
Iteration 12/25 | Loss: 0.00134565
Iteration 13/25 | Loss: 0.00134565
Iteration 14/25 | Loss: 0.00134565
Iteration 15/25 | Loss: 0.00134565
Iteration 16/25 | Loss: 0.00134565
Iteration 17/25 | Loss: 0.00134565
Iteration 18/25 | Loss: 0.00134565
Iteration 19/25 | Loss: 0.00134565
Iteration 20/25 | Loss: 0.00134565
Iteration 21/25 | Loss: 0.00134565
Iteration 22/25 | Loss: 0.00134565
Iteration 23/25 | Loss: 0.00134565
Iteration 24/25 | Loss: 0.00134565
Iteration 25/25 | Loss: 0.00134565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134565
Iteration 2/1000 | Loss: 0.00002095
Iteration 3/1000 | Loss: 0.00001403
Iteration 4/1000 | Loss: 0.00001278
Iteration 5/1000 | Loss: 0.00001183
Iteration 6/1000 | Loss: 0.00001129
Iteration 7/1000 | Loss: 0.00001086
Iteration 8/1000 | Loss: 0.00001058
Iteration 9/1000 | Loss: 0.00001038
Iteration 10/1000 | Loss: 0.00001033
Iteration 11/1000 | Loss: 0.00001024
Iteration 12/1000 | Loss: 0.00001012
Iteration 13/1000 | Loss: 0.00000993
Iteration 14/1000 | Loss: 0.00000989
Iteration 15/1000 | Loss: 0.00000974
Iteration 16/1000 | Loss: 0.00000967
Iteration 17/1000 | Loss: 0.00000964
Iteration 18/1000 | Loss: 0.00000963
Iteration 19/1000 | Loss: 0.00000962
Iteration 20/1000 | Loss: 0.00000960
Iteration 21/1000 | Loss: 0.00000959
Iteration 22/1000 | Loss: 0.00000958
Iteration 23/1000 | Loss: 0.00000958
Iteration 24/1000 | Loss: 0.00000955
Iteration 25/1000 | Loss: 0.00000955
Iteration 26/1000 | Loss: 0.00000952
Iteration 27/1000 | Loss: 0.00000952
Iteration 28/1000 | Loss: 0.00000951
Iteration 29/1000 | Loss: 0.00000950
Iteration 30/1000 | Loss: 0.00000950
Iteration 31/1000 | Loss: 0.00000949
Iteration 32/1000 | Loss: 0.00000949
Iteration 33/1000 | Loss: 0.00000948
Iteration 34/1000 | Loss: 0.00000948
Iteration 35/1000 | Loss: 0.00000948
Iteration 36/1000 | Loss: 0.00000948
Iteration 37/1000 | Loss: 0.00000947
Iteration 38/1000 | Loss: 0.00000947
Iteration 39/1000 | Loss: 0.00000947
Iteration 40/1000 | Loss: 0.00000947
Iteration 41/1000 | Loss: 0.00000946
Iteration 42/1000 | Loss: 0.00000946
Iteration 43/1000 | Loss: 0.00000946
Iteration 44/1000 | Loss: 0.00000946
Iteration 45/1000 | Loss: 0.00000945
Iteration 46/1000 | Loss: 0.00000945
Iteration 47/1000 | Loss: 0.00000945
Iteration 48/1000 | Loss: 0.00000945
Iteration 49/1000 | Loss: 0.00000945
Iteration 50/1000 | Loss: 0.00000944
Iteration 51/1000 | Loss: 0.00000944
Iteration 52/1000 | Loss: 0.00000944
Iteration 53/1000 | Loss: 0.00000943
Iteration 54/1000 | Loss: 0.00000943
Iteration 55/1000 | Loss: 0.00000943
Iteration 56/1000 | Loss: 0.00000943
Iteration 57/1000 | Loss: 0.00000942
Iteration 58/1000 | Loss: 0.00000942
Iteration 59/1000 | Loss: 0.00000942
Iteration 60/1000 | Loss: 0.00000942
Iteration 61/1000 | Loss: 0.00000942
Iteration 62/1000 | Loss: 0.00000941
Iteration 63/1000 | Loss: 0.00000941
Iteration 64/1000 | Loss: 0.00000941
Iteration 65/1000 | Loss: 0.00000941
Iteration 66/1000 | Loss: 0.00000941
Iteration 67/1000 | Loss: 0.00000941
Iteration 68/1000 | Loss: 0.00000940
Iteration 69/1000 | Loss: 0.00000940
Iteration 70/1000 | Loss: 0.00000940
Iteration 71/1000 | Loss: 0.00000940
Iteration 72/1000 | Loss: 0.00000939
Iteration 73/1000 | Loss: 0.00000939
Iteration 74/1000 | Loss: 0.00000938
Iteration 75/1000 | Loss: 0.00000938
Iteration 76/1000 | Loss: 0.00000938
Iteration 77/1000 | Loss: 0.00000937
Iteration 78/1000 | Loss: 0.00000937
Iteration 79/1000 | Loss: 0.00000937
Iteration 80/1000 | Loss: 0.00000937
Iteration 81/1000 | Loss: 0.00000937
Iteration 82/1000 | Loss: 0.00000937
Iteration 83/1000 | Loss: 0.00000937
Iteration 84/1000 | Loss: 0.00000936
Iteration 85/1000 | Loss: 0.00000936
Iteration 86/1000 | Loss: 0.00000936
Iteration 87/1000 | Loss: 0.00000936
Iteration 88/1000 | Loss: 0.00000936
Iteration 89/1000 | Loss: 0.00000936
Iteration 90/1000 | Loss: 0.00000936
Iteration 91/1000 | Loss: 0.00000936
Iteration 92/1000 | Loss: 0.00000935
Iteration 93/1000 | Loss: 0.00000935
Iteration 94/1000 | Loss: 0.00000935
Iteration 95/1000 | Loss: 0.00000935
Iteration 96/1000 | Loss: 0.00000935
Iteration 97/1000 | Loss: 0.00000935
Iteration 98/1000 | Loss: 0.00000935
Iteration 99/1000 | Loss: 0.00000934
Iteration 100/1000 | Loss: 0.00000934
Iteration 101/1000 | Loss: 0.00000934
Iteration 102/1000 | Loss: 0.00000934
Iteration 103/1000 | Loss: 0.00000934
Iteration 104/1000 | Loss: 0.00000934
Iteration 105/1000 | Loss: 0.00000934
Iteration 106/1000 | Loss: 0.00000934
Iteration 107/1000 | Loss: 0.00000934
Iteration 108/1000 | Loss: 0.00000934
Iteration 109/1000 | Loss: 0.00000934
Iteration 110/1000 | Loss: 0.00000933
Iteration 111/1000 | Loss: 0.00000933
Iteration 112/1000 | Loss: 0.00000933
Iteration 113/1000 | Loss: 0.00000933
Iteration 114/1000 | Loss: 0.00000933
Iteration 115/1000 | Loss: 0.00000933
Iteration 116/1000 | Loss: 0.00000933
Iteration 117/1000 | Loss: 0.00000933
Iteration 118/1000 | Loss: 0.00000933
Iteration 119/1000 | Loss: 0.00000932
Iteration 120/1000 | Loss: 0.00000932
Iteration 121/1000 | Loss: 0.00000932
Iteration 122/1000 | Loss: 0.00000932
Iteration 123/1000 | Loss: 0.00000932
Iteration 124/1000 | Loss: 0.00000932
Iteration 125/1000 | Loss: 0.00000932
Iteration 126/1000 | Loss: 0.00000931
Iteration 127/1000 | Loss: 0.00000931
Iteration 128/1000 | Loss: 0.00000931
Iteration 129/1000 | Loss: 0.00000931
Iteration 130/1000 | Loss: 0.00000931
Iteration 131/1000 | Loss: 0.00000931
Iteration 132/1000 | Loss: 0.00000931
Iteration 133/1000 | Loss: 0.00000931
Iteration 134/1000 | Loss: 0.00000930
Iteration 135/1000 | Loss: 0.00000930
Iteration 136/1000 | Loss: 0.00000930
Iteration 137/1000 | Loss: 0.00000930
Iteration 138/1000 | Loss: 0.00000930
Iteration 139/1000 | Loss: 0.00000930
Iteration 140/1000 | Loss: 0.00000930
Iteration 141/1000 | Loss: 0.00000929
Iteration 142/1000 | Loss: 0.00000929
Iteration 143/1000 | Loss: 0.00000929
Iteration 144/1000 | Loss: 0.00000929
Iteration 145/1000 | Loss: 0.00000929
Iteration 146/1000 | Loss: 0.00000929
Iteration 147/1000 | Loss: 0.00000928
Iteration 148/1000 | Loss: 0.00000928
Iteration 149/1000 | Loss: 0.00000928
Iteration 150/1000 | Loss: 0.00000928
Iteration 151/1000 | Loss: 0.00000928
Iteration 152/1000 | Loss: 0.00000928
Iteration 153/1000 | Loss: 0.00000928
Iteration 154/1000 | Loss: 0.00000927
Iteration 155/1000 | Loss: 0.00000927
Iteration 156/1000 | Loss: 0.00000927
Iteration 157/1000 | Loss: 0.00000926
Iteration 158/1000 | Loss: 0.00000926
Iteration 159/1000 | Loss: 0.00000926
Iteration 160/1000 | Loss: 0.00000926
Iteration 161/1000 | Loss: 0.00000926
Iteration 162/1000 | Loss: 0.00000926
Iteration 163/1000 | Loss: 0.00000926
Iteration 164/1000 | Loss: 0.00000926
Iteration 165/1000 | Loss: 0.00000926
Iteration 166/1000 | Loss: 0.00000926
Iteration 167/1000 | Loss: 0.00000925
Iteration 168/1000 | Loss: 0.00000925
Iteration 169/1000 | Loss: 0.00000925
Iteration 170/1000 | Loss: 0.00000925
Iteration 171/1000 | Loss: 0.00000925
Iteration 172/1000 | Loss: 0.00000925
Iteration 173/1000 | Loss: 0.00000925
Iteration 174/1000 | Loss: 0.00000925
Iteration 175/1000 | Loss: 0.00000925
Iteration 176/1000 | Loss: 0.00000925
Iteration 177/1000 | Loss: 0.00000925
Iteration 178/1000 | Loss: 0.00000925
Iteration 179/1000 | Loss: 0.00000925
Iteration 180/1000 | Loss: 0.00000925
Iteration 181/1000 | Loss: 0.00000925
Iteration 182/1000 | Loss: 0.00000924
Iteration 183/1000 | Loss: 0.00000924
Iteration 184/1000 | Loss: 0.00000924
Iteration 185/1000 | Loss: 0.00000924
Iteration 186/1000 | Loss: 0.00000924
Iteration 187/1000 | Loss: 0.00000924
Iteration 188/1000 | Loss: 0.00000924
Iteration 189/1000 | Loss: 0.00000924
Iteration 190/1000 | Loss: 0.00000924
Iteration 191/1000 | Loss: 0.00000924
Iteration 192/1000 | Loss: 0.00000924
Iteration 193/1000 | Loss: 0.00000924
Iteration 194/1000 | Loss: 0.00000924
Iteration 195/1000 | Loss: 0.00000924
Iteration 196/1000 | Loss: 0.00000924
Iteration 197/1000 | Loss: 0.00000924
Iteration 198/1000 | Loss: 0.00000924
Iteration 199/1000 | Loss: 0.00000924
Iteration 200/1000 | Loss: 0.00000924
Iteration 201/1000 | Loss: 0.00000924
Iteration 202/1000 | Loss: 0.00000924
Iteration 203/1000 | Loss: 0.00000923
Iteration 204/1000 | Loss: 0.00000923
Iteration 205/1000 | Loss: 0.00000923
Iteration 206/1000 | Loss: 0.00000923
Iteration 207/1000 | Loss: 0.00000923
Iteration 208/1000 | Loss: 0.00000923
Iteration 209/1000 | Loss: 0.00000923
Iteration 210/1000 | Loss: 0.00000923
Iteration 211/1000 | Loss: 0.00000923
Iteration 212/1000 | Loss: 0.00000923
Iteration 213/1000 | Loss: 0.00000923
Iteration 214/1000 | Loss: 0.00000923
Iteration 215/1000 | Loss: 0.00000923
Iteration 216/1000 | Loss: 0.00000923
Iteration 217/1000 | Loss: 0.00000923
Iteration 218/1000 | Loss: 0.00000923
Iteration 219/1000 | Loss: 0.00000923
Iteration 220/1000 | Loss: 0.00000923
Iteration 221/1000 | Loss: 0.00000923
Iteration 222/1000 | Loss: 0.00000923
Iteration 223/1000 | Loss: 0.00000923
Iteration 224/1000 | Loss: 0.00000923
Iteration 225/1000 | Loss: 0.00000923
Iteration 226/1000 | Loss: 0.00000923
Iteration 227/1000 | Loss: 0.00000922
Iteration 228/1000 | Loss: 0.00000922
Iteration 229/1000 | Loss: 0.00000922
Iteration 230/1000 | Loss: 0.00000922
Iteration 231/1000 | Loss: 0.00000922
Iteration 232/1000 | Loss: 0.00000922
Iteration 233/1000 | Loss: 0.00000922
Iteration 234/1000 | Loss: 0.00000922
Iteration 235/1000 | Loss: 0.00000922
Iteration 236/1000 | Loss: 0.00000922
Iteration 237/1000 | Loss: 0.00000922
Iteration 238/1000 | Loss: 0.00000922
Iteration 239/1000 | Loss: 0.00000922
Iteration 240/1000 | Loss: 0.00000922
Iteration 241/1000 | Loss: 0.00000922
Iteration 242/1000 | Loss: 0.00000922
Iteration 243/1000 | Loss: 0.00000922
Iteration 244/1000 | Loss: 0.00000922
Iteration 245/1000 | Loss: 0.00000922
Iteration 246/1000 | Loss: 0.00000922
Iteration 247/1000 | Loss: 0.00000922
Iteration 248/1000 | Loss: 0.00000922
Iteration 249/1000 | Loss: 0.00000922
Iteration 250/1000 | Loss: 0.00000921
Iteration 251/1000 | Loss: 0.00000921
Iteration 252/1000 | Loss: 0.00000921
Iteration 253/1000 | Loss: 0.00000921
Iteration 254/1000 | Loss: 0.00000921
Iteration 255/1000 | Loss: 0.00000921
Iteration 256/1000 | Loss: 0.00000921
Iteration 257/1000 | Loss: 0.00000921
Iteration 258/1000 | Loss: 0.00000921
Iteration 259/1000 | Loss: 0.00000921
Iteration 260/1000 | Loss: 0.00000921
Iteration 261/1000 | Loss: 0.00000921
Iteration 262/1000 | Loss: 0.00000921
Iteration 263/1000 | Loss: 0.00000921
Iteration 264/1000 | Loss: 0.00000921
Iteration 265/1000 | Loss: 0.00000921
Iteration 266/1000 | Loss: 0.00000921
Iteration 267/1000 | Loss: 0.00000921
Iteration 268/1000 | Loss: 0.00000921
Iteration 269/1000 | Loss: 0.00000921
Iteration 270/1000 | Loss: 0.00000921
Iteration 271/1000 | Loss: 0.00000921
Iteration 272/1000 | Loss: 0.00000921
Iteration 273/1000 | Loss: 0.00000921
Iteration 274/1000 | Loss: 0.00000921
Iteration 275/1000 | Loss: 0.00000921
Iteration 276/1000 | Loss: 0.00000921
Iteration 277/1000 | Loss: 0.00000921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [9.211449651047587e-06, 9.211449651047587e-06, 9.211449651047587e-06, 9.211449651047587e-06, 9.211449651047587e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.211449651047587e-06

Optimization complete. Final v2v error: 2.5795881748199463 mm

Highest mean error: 3.421903610229492 mm for frame 74

Lowest mean error: 2.3767168521881104 mm for frame 107

Saving results

Total time: 44.170485734939575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886805
Iteration 2/25 | Loss: 0.00270820
Iteration 3/25 | Loss: 0.00186080
Iteration 4/25 | Loss: 0.00152111
Iteration 5/25 | Loss: 0.00148960
Iteration 6/25 | Loss: 0.00148645
Iteration 7/25 | Loss: 0.00145582
Iteration 8/25 | Loss: 0.00144693
Iteration 9/25 | Loss: 0.00144727
Iteration 10/25 | Loss: 0.00144225
Iteration 11/25 | Loss: 0.00144375
Iteration 12/25 | Loss: 0.00143760
Iteration 13/25 | Loss: 0.00144127
Iteration 14/25 | Loss: 0.00143770
Iteration 15/25 | Loss: 0.00144525
Iteration 16/25 | Loss: 0.00143145
Iteration 17/25 | Loss: 0.00143373
Iteration 18/25 | Loss: 0.00143422
Iteration 19/25 | Loss: 0.00143610
Iteration 20/25 | Loss: 0.00143368
Iteration 21/25 | Loss: 0.00143635
Iteration 22/25 | Loss: 0.00143340
Iteration 23/25 | Loss: 0.00143824
Iteration 24/25 | Loss: 0.00143470
Iteration 25/25 | Loss: 0.00143635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.41227245
Iteration 2/25 | Loss: 0.00430201
Iteration 3/25 | Loss: 0.00301291
Iteration 4/25 | Loss: 0.00301291
Iteration 5/25 | Loss: 0.00301291
Iteration 6/25 | Loss: 0.00301290
Iteration 7/25 | Loss: 0.00301290
Iteration 8/25 | Loss: 0.00301290
Iteration 9/25 | Loss: 0.00301290
Iteration 10/25 | Loss: 0.00301290
Iteration 11/25 | Loss: 0.00301290
Iteration 12/25 | Loss: 0.00301290
Iteration 13/25 | Loss: 0.00301290
Iteration 14/25 | Loss: 0.00301290
Iteration 15/25 | Loss: 0.00301290
Iteration 16/25 | Loss: 0.00301290
Iteration 17/25 | Loss: 0.00301290
Iteration 18/25 | Loss: 0.00301290
Iteration 19/25 | Loss: 0.00301290
Iteration 20/25 | Loss: 0.00301290
Iteration 21/25 | Loss: 0.00301290
Iteration 22/25 | Loss: 0.00301290
Iteration 23/25 | Loss: 0.00301290
Iteration 24/25 | Loss: 0.00301290
Iteration 25/25 | Loss: 0.00301290

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00301290
Iteration 2/1000 | Loss: 0.00176071
Iteration 3/1000 | Loss: 0.00237745
Iteration 4/1000 | Loss: 0.00207350
Iteration 5/1000 | Loss: 0.00093425
Iteration 6/1000 | Loss: 0.00056242
Iteration 7/1000 | Loss: 0.00163498
Iteration 8/1000 | Loss: 0.00233820
Iteration 9/1000 | Loss: 0.00066180
Iteration 10/1000 | Loss: 0.00147502
Iteration 11/1000 | Loss: 0.00084560
Iteration 12/1000 | Loss: 0.00121863
Iteration 13/1000 | Loss: 0.00099254
Iteration 14/1000 | Loss: 0.00062811
Iteration 15/1000 | Loss: 0.00074170
Iteration 16/1000 | Loss: 0.00128567
Iteration 17/1000 | Loss: 0.00090354
Iteration 18/1000 | Loss: 0.00221926
Iteration 19/1000 | Loss: 0.00093086
Iteration 20/1000 | Loss: 0.00037795
Iteration 21/1000 | Loss: 0.00036440
Iteration 22/1000 | Loss: 0.00131454
Iteration 23/1000 | Loss: 0.00288588
Iteration 24/1000 | Loss: 0.00426816
Iteration 25/1000 | Loss: 0.00381289
Iteration 26/1000 | Loss: 0.00341669
Iteration 27/1000 | Loss: 0.00237581
Iteration 28/1000 | Loss: 0.00177623
Iteration 29/1000 | Loss: 0.00216938
Iteration 30/1000 | Loss: 0.00070389
Iteration 31/1000 | Loss: 0.00059060
Iteration 32/1000 | Loss: 0.00023389
Iteration 33/1000 | Loss: 0.00023748
Iteration 34/1000 | Loss: 0.00018881
Iteration 35/1000 | Loss: 0.00052668
Iteration 36/1000 | Loss: 0.00070993
Iteration 37/1000 | Loss: 0.00018884
Iteration 38/1000 | Loss: 0.00023735
Iteration 39/1000 | Loss: 0.00017229
Iteration 40/1000 | Loss: 0.00025937
Iteration 41/1000 | Loss: 0.00010823
Iteration 42/1000 | Loss: 0.00006044
Iteration 43/1000 | Loss: 0.00093295
Iteration 44/1000 | Loss: 0.00093385
Iteration 45/1000 | Loss: 0.00036779
Iteration 46/1000 | Loss: 0.00010163
Iteration 47/1000 | Loss: 0.00034591
Iteration 48/1000 | Loss: 0.00064819
Iteration 49/1000 | Loss: 0.00035363
Iteration 50/1000 | Loss: 0.00014991
Iteration 51/1000 | Loss: 0.00023284
Iteration 52/1000 | Loss: 0.00011931
Iteration 53/1000 | Loss: 0.00004343
Iteration 54/1000 | Loss: 0.00021097
Iteration 55/1000 | Loss: 0.00061269
Iteration 56/1000 | Loss: 0.00027578
Iteration 57/1000 | Loss: 0.00032395
Iteration 58/1000 | Loss: 0.00026499
Iteration 59/1000 | Loss: 0.00021938
Iteration 60/1000 | Loss: 0.00083911
Iteration 61/1000 | Loss: 0.00004414
Iteration 62/1000 | Loss: 0.00003858
Iteration 63/1000 | Loss: 0.00035503
Iteration 64/1000 | Loss: 0.00003655
Iteration 65/1000 | Loss: 0.00003424
Iteration 66/1000 | Loss: 0.00088260
Iteration 67/1000 | Loss: 0.00044762
Iteration 68/1000 | Loss: 0.00010966
Iteration 69/1000 | Loss: 0.00059386
Iteration 70/1000 | Loss: 0.00034868
Iteration 71/1000 | Loss: 0.00046045
Iteration 72/1000 | Loss: 0.00027200
Iteration 73/1000 | Loss: 0.00045314
Iteration 74/1000 | Loss: 0.00025824
Iteration 75/1000 | Loss: 0.00052476
Iteration 76/1000 | Loss: 0.00025746
Iteration 77/1000 | Loss: 0.00025253
Iteration 78/1000 | Loss: 0.00020887
Iteration 79/1000 | Loss: 0.00059365
Iteration 80/1000 | Loss: 0.00003306
Iteration 81/1000 | Loss: 0.00006318
Iteration 82/1000 | Loss: 0.00015006
Iteration 83/1000 | Loss: 0.00005195
Iteration 84/1000 | Loss: 0.00042864
Iteration 85/1000 | Loss: 0.00027130
Iteration 86/1000 | Loss: 0.00050119
Iteration 87/1000 | Loss: 0.00128622
Iteration 88/1000 | Loss: 0.00030552
Iteration 89/1000 | Loss: 0.00029692
Iteration 90/1000 | Loss: 0.00031683
Iteration 91/1000 | Loss: 0.00040067
Iteration 92/1000 | Loss: 0.00047614
Iteration 93/1000 | Loss: 0.00057925
Iteration 94/1000 | Loss: 0.00037989
Iteration 95/1000 | Loss: 0.00107687
Iteration 96/1000 | Loss: 0.00054888
Iteration 97/1000 | Loss: 0.00045521
Iteration 98/1000 | Loss: 0.00004441
Iteration 99/1000 | Loss: 0.00012595
Iteration 100/1000 | Loss: 0.00003408
Iteration 101/1000 | Loss: 0.00003008
Iteration 102/1000 | Loss: 0.00002762
Iteration 103/1000 | Loss: 0.00002593
Iteration 104/1000 | Loss: 0.00002460
Iteration 105/1000 | Loss: 0.00002383
Iteration 106/1000 | Loss: 0.00024311
Iteration 107/1000 | Loss: 0.00002405
Iteration 108/1000 | Loss: 0.00002322
Iteration 109/1000 | Loss: 0.00002293
Iteration 110/1000 | Loss: 0.00002266
Iteration 111/1000 | Loss: 0.00002238
Iteration 112/1000 | Loss: 0.00002213
Iteration 113/1000 | Loss: 0.00002212
Iteration 114/1000 | Loss: 0.00002191
Iteration 115/1000 | Loss: 0.00002182
Iteration 116/1000 | Loss: 0.00002180
Iteration 117/1000 | Loss: 0.00002161
Iteration 118/1000 | Loss: 0.00002137
Iteration 119/1000 | Loss: 0.00002119
Iteration 120/1000 | Loss: 0.00002101
Iteration 121/1000 | Loss: 0.00002094
Iteration 122/1000 | Loss: 0.00002082
Iteration 123/1000 | Loss: 0.00002080
Iteration 124/1000 | Loss: 0.00002080
Iteration 125/1000 | Loss: 0.00002079
Iteration 126/1000 | Loss: 0.00002079
Iteration 127/1000 | Loss: 0.00002079
Iteration 128/1000 | Loss: 0.00002078
Iteration 129/1000 | Loss: 0.00002078
Iteration 130/1000 | Loss: 0.00002077
Iteration 131/1000 | Loss: 0.00002077
Iteration 132/1000 | Loss: 0.00002076
Iteration 133/1000 | Loss: 0.00002074
Iteration 134/1000 | Loss: 0.00002073
Iteration 135/1000 | Loss: 0.00002069
Iteration 136/1000 | Loss: 0.00002066
Iteration 137/1000 | Loss: 0.00002065
Iteration 138/1000 | Loss: 0.00002065
Iteration 139/1000 | Loss: 0.00002065
Iteration 140/1000 | Loss: 0.00002064
Iteration 141/1000 | Loss: 0.00002064
Iteration 142/1000 | Loss: 0.00002063
Iteration 143/1000 | Loss: 0.00002063
Iteration 144/1000 | Loss: 0.00002063
Iteration 145/1000 | Loss: 0.00002063
Iteration 146/1000 | Loss: 0.00002062
Iteration 147/1000 | Loss: 0.00002062
Iteration 148/1000 | Loss: 0.00002062
Iteration 149/1000 | Loss: 0.00002061
Iteration 150/1000 | Loss: 0.00002061
Iteration 151/1000 | Loss: 0.00002061
Iteration 152/1000 | Loss: 0.00002060
Iteration 153/1000 | Loss: 0.00002060
Iteration 154/1000 | Loss: 0.00002060
Iteration 155/1000 | Loss: 0.00002059
Iteration 156/1000 | Loss: 0.00002059
Iteration 157/1000 | Loss: 0.00002057
Iteration 158/1000 | Loss: 0.00002057
Iteration 159/1000 | Loss: 0.00002057
Iteration 160/1000 | Loss: 0.00002053
Iteration 161/1000 | Loss: 0.00002053
Iteration 162/1000 | Loss: 0.00002051
Iteration 163/1000 | Loss: 0.00002051
Iteration 164/1000 | Loss: 0.00002051
Iteration 165/1000 | Loss: 0.00002051
Iteration 166/1000 | Loss: 0.00002050
Iteration 167/1000 | Loss: 0.00002050
Iteration 168/1000 | Loss: 0.00002050
Iteration 169/1000 | Loss: 0.00002050
Iteration 170/1000 | Loss: 0.00002050
Iteration 171/1000 | Loss: 0.00002050
Iteration 172/1000 | Loss: 0.00002050
Iteration 173/1000 | Loss: 0.00002050
Iteration 174/1000 | Loss: 0.00002050
Iteration 175/1000 | Loss: 0.00002050
Iteration 176/1000 | Loss: 0.00002050
Iteration 177/1000 | Loss: 0.00002050
Iteration 178/1000 | Loss: 0.00002050
Iteration 179/1000 | Loss: 0.00002049
Iteration 180/1000 | Loss: 0.00002049
Iteration 181/1000 | Loss: 0.00002049
Iteration 182/1000 | Loss: 0.00002049
Iteration 183/1000 | Loss: 0.00002049
Iteration 184/1000 | Loss: 0.00002049
Iteration 185/1000 | Loss: 0.00002049
Iteration 186/1000 | Loss: 0.00002049
Iteration 187/1000 | Loss: 0.00002049
Iteration 188/1000 | Loss: 0.00002049
Iteration 189/1000 | Loss: 0.00002049
Iteration 190/1000 | Loss: 0.00002049
Iteration 191/1000 | Loss: 0.00002049
Iteration 192/1000 | Loss: 0.00002049
Iteration 193/1000 | Loss: 0.00002049
Iteration 194/1000 | Loss: 0.00002049
Iteration 195/1000 | Loss: 0.00002049
Iteration 196/1000 | Loss: 0.00002049
Iteration 197/1000 | Loss: 0.00002049
Iteration 198/1000 | Loss: 0.00002049
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [2.0494660930125974e-05, 2.0494660930125974e-05, 2.0494660930125974e-05, 2.0494660930125974e-05, 2.0494660930125974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0494660930125974e-05

Optimization complete. Final v2v error: 3.27121901512146 mm

Highest mean error: 12.865030288696289 mm for frame 56

Lowest mean error: 2.618579626083374 mm for frame 20

Saving results

Total time: 223.65330004692078
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930066
Iteration 2/25 | Loss: 0.00256866
Iteration 3/25 | Loss: 0.00213737
Iteration 4/25 | Loss: 0.00179662
Iteration 5/25 | Loss: 0.00199870
Iteration 6/25 | Loss: 0.00200346
Iteration 7/25 | Loss: 0.00167849
Iteration 8/25 | Loss: 0.00153231
Iteration 9/25 | Loss: 0.00144852
Iteration 10/25 | Loss: 0.00135360
Iteration 11/25 | Loss: 0.00132505
Iteration 12/25 | Loss: 0.00131617
Iteration 13/25 | Loss: 0.00133804
Iteration 14/25 | Loss: 0.00130941
Iteration 15/25 | Loss: 0.00128402
Iteration 16/25 | Loss: 0.00127816
Iteration 17/25 | Loss: 0.00128936
Iteration 18/25 | Loss: 0.00127492
Iteration 19/25 | Loss: 0.00126810
Iteration 20/25 | Loss: 0.00127409
Iteration 21/25 | Loss: 0.00127376
Iteration 22/25 | Loss: 0.00126484
Iteration 23/25 | Loss: 0.00126231
Iteration 24/25 | Loss: 0.00126181
Iteration 25/25 | Loss: 0.00126168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28004718
Iteration 2/25 | Loss: 0.00095227
Iteration 3/25 | Loss: 0.00095227
Iteration 4/25 | Loss: 0.00095227
Iteration 5/25 | Loss: 0.00095227
Iteration 6/25 | Loss: 0.00095227
Iteration 7/25 | Loss: 0.00095227
Iteration 8/25 | Loss: 0.00095226
Iteration 9/25 | Loss: 0.00095226
Iteration 10/25 | Loss: 0.00095226
Iteration 11/25 | Loss: 0.00095226
Iteration 12/25 | Loss: 0.00095226
Iteration 13/25 | Loss: 0.00095226
Iteration 14/25 | Loss: 0.00095226
Iteration 15/25 | Loss: 0.00095226
Iteration 16/25 | Loss: 0.00095226
Iteration 17/25 | Loss: 0.00095226
Iteration 18/25 | Loss: 0.00095226
Iteration 19/25 | Loss: 0.00095226
Iteration 20/25 | Loss: 0.00095226
Iteration 21/25 | Loss: 0.00095226
Iteration 22/25 | Loss: 0.00095226
Iteration 23/25 | Loss: 0.00095226
Iteration 24/25 | Loss: 0.00095226
Iteration 25/25 | Loss: 0.00095226
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009522640029899776, 0.0009522640029899776, 0.0009522640029899776, 0.0009522640029899776, 0.0009522640029899776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009522640029899776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095226
Iteration 2/1000 | Loss: 0.00003856
Iteration 3/1000 | Loss: 0.00002554
Iteration 4/1000 | Loss: 0.00002252
Iteration 5/1000 | Loss: 0.00002150
Iteration 6/1000 | Loss: 0.00002079
Iteration 7/1000 | Loss: 0.00002015
Iteration 8/1000 | Loss: 0.00001970
Iteration 9/1000 | Loss: 0.00001930
Iteration 10/1000 | Loss: 0.00001904
Iteration 11/1000 | Loss: 0.00001878
Iteration 12/1000 | Loss: 0.00001874
Iteration 13/1000 | Loss: 0.00001854
Iteration 14/1000 | Loss: 0.00001853
Iteration 15/1000 | Loss: 0.00001849
Iteration 16/1000 | Loss: 0.00001830
Iteration 17/1000 | Loss: 0.00046413
Iteration 18/1000 | Loss: 0.00002245
Iteration 19/1000 | Loss: 0.00001992
Iteration 20/1000 | Loss: 0.00001811
Iteration 21/1000 | Loss: 0.00001690
Iteration 22/1000 | Loss: 0.00001617
Iteration 23/1000 | Loss: 0.00001581
Iteration 24/1000 | Loss: 0.00001559
Iteration 25/1000 | Loss: 0.00001554
Iteration 26/1000 | Loss: 0.00001548
Iteration 27/1000 | Loss: 0.00001546
Iteration 28/1000 | Loss: 0.00001546
Iteration 29/1000 | Loss: 0.00001545
Iteration 30/1000 | Loss: 0.00001545
Iteration 31/1000 | Loss: 0.00001544
Iteration 32/1000 | Loss: 0.00001544
Iteration 33/1000 | Loss: 0.00001543
Iteration 34/1000 | Loss: 0.00001533
Iteration 35/1000 | Loss: 0.00001532
Iteration 36/1000 | Loss: 0.00001532
Iteration 37/1000 | Loss: 0.00001531
Iteration 38/1000 | Loss: 0.00001530
Iteration 39/1000 | Loss: 0.00001528
Iteration 40/1000 | Loss: 0.00001528
Iteration 41/1000 | Loss: 0.00001527
Iteration 42/1000 | Loss: 0.00001526
Iteration 43/1000 | Loss: 0.00001526
Iteration 44/1000 | Loss: 0.00001525
Iteration 45/1000 | Loss: 0.00001525
Iteration 46/1000 | Loss: 0.00001525
Iteration 47/1000 | Loss: 0.00001524
Iteration 48/1000 | Loss: 0.00001524
Iteration 49/1000 | Loss: 0.00001523
Iteration 50/1000 | Loss: 0.00001523
Iteration 51/1000 | Loss: 0.00001523
Iteration 52/1000 | Loss: 0.00001522
Iteration 53/1000 | Loss: 0.00001522
Iteration 54/1000 | Loss: 0.00001521
Iteration 55/1000 | Loss: 0.00001521
Iteration 56/1000 | Loss: 0.00001521
Iteration 57/1000 | Loss: 0.00001521
Iteration 58/1000 | Loss: 0.00001521
Iteration 59/1000 | Loss: 0.00001521
Iteration 60/1000 | Loss: 0.00001521
Iteration 61/1000 | Loss: 0.00001521
Iteration 62/1000 | Loss: 0.00001520
Iteration 63/1000 | Loss: 0.00001520
Iteration 64/1000 | Loss: 0.00001520
Iteration 65/1000 | Loss: 0.00001519
Iteration 66/1000 | Loss: 0.00001519
Iteration 67/1000 | Loss: 0.00001519
Iteration 68/1000 | Loss: 0.00001518
Iteration 69/1000 | Loss: 0.00001518
Iteration 70/1000 | Loss: 0.00001518
Iteration 71/1000 | Loss: 0.00001518
Iteration 72/1000 | Loss: 0.00001518
Iteration 73/1000 | Loss: 0.00001518
Iteration 74/1000 | Loss: 0.00001518
Iteration 75/1000 | Loss: 0.00001517
Iteration 76/1000 | Loss: 0.00001517
Iteration 77/1000 | Loss: 0.00001517
Iteration 78/1000 | Loss: 0.00001517
Iteration 79/1000 | Loss: 0.00001517
Iteration 80/1000 | Loss: 0.00001517
Iteration 81/1000 | Loss: 0.00001517
Iteration 82/1000 | Loss: 0.00001516
Iteration 83/1000 | Loss: 0.00001515
Iteration 84/1000 | Loss: 0.00001515
Iteration 85/1000 | Loss: 0.00001515
Iteration 86/1000 | Loss: 0.00001515
Iteration 87/1000 | Loss: 0.00001515
Iteration 88/1000 | Loss: 0.00001515
Iteration 89/1000 | Loss: 0.00001515
Iteration 90/1000 | Loss: 0.00001514
Iteration 91/1000 | Loss: 0.00001514
Iteration 92/1000 | Loss: 0.00001514
Iteration 93/1000 | Loss: 0.00001514
Iteration 94/1000 | Loss: 0.00001513
Iteration 95/1000 | Loss: 0.00001513
Iteration 96/1000 | Loss: 0.00001513
Iteration 97/1000 | Loss: 0.00001513
Iteration 98/1000 | Loss: 0.00001513
Iteration 99/1000 | Loss: 0.00001513
Iteration 100/1000 | Loss: 0.00001513
Iteration 101/1000 | Loss: 0.00001513
Iteration 102/1000 | Loss: 0.00001513
Iteration 103/1000 | Loss: 0.00001513
Iteration 104/1000 | Loss: 0.00001513
Iteration 105/1000 | Loss: 0.00001513
Iteration 106/1000 | Loss: 0.00001512
Iteration 107/1000 | Loss: 0.00001512
Iteration 108/1000 | Loss: 0.00001512
Iteration 109/1000 | Loss: 0.00001512
Iteration 110/1000 | Loss: 0.00001512
Iteration 111/1000 | Loss: 0.00001512
Iteration 112/1000 | Loss: 0.00001512
Iteration 113/1000 | Loss: 0.00001512
Iteration 114/1000 | Loss: 0.00001512
Iteration 115/1000 | Loss: 0.00001512
Iteration 116/1000 | Loss: 0.00001512
Iteration 117/1000 | Loss: 0.00001512
Iteration 118/1000 | Loss: 0.00001512
Iteration 119/1000 | Loss: 0.00001512
Iteration 120/1000 | Loss: 0.00001512
Iteration 121/1000 | Loss: 0.00001512
Iteration 122/1000 | Loss: 0.00001511
Iteration 123/1000 | Loss: 0.00001511
Iteration 124/1000 | Loss: 0.00001511
Iteration 125/1000 | Loss: 0.00001511
Iteration 126/1000 | Loss: 0.00001511
Iteration 127/1000 | Loss: 0.00001511
Iteration 128/1000 | Loss: 0.00001511
Iteration 129/1000 | Loss: 0.00001511
Iteration 130/1000 | Loss: 0.00001511
Iteration 131/1000 | Loss: 0.00001510
Iteration 132/1000 | Loss: 0.00001510
Iteration 133/1000 | Loss: 0.00001510
Iteration 134/1000 | Loss: 0.00001510
Iteration 135/1000 | Loss: 0.00001510
Iteration 136/1000 | Loss: 0.00001510
Iteration 137/1000 | Loss: 0.00001510
Iteration 138/1000 | Loss: 0.00001510
Iteration 139/1000 | Loss: 0.00001510
Iteration 140/1000 | Loss: 0.00001510
Iteration 141/1000 | Loss: 0.00001510
Iteration 142/1000 | Loss: 0.00001510
Iteration 143/1000 | Loss: 0.00001510
Iteration 144/1000 | Loss: 0.00001510
Iteration 145/1000 | Loss: 0.00001510
Iteration 146/1000 | Loss: 0.00001510
Iteration 147/1000 | Loss: 0.00001510
Iteration 148/1000 | Loss: 0.00001510
Iteration 149/1000 | Loss: 0.00001510
Iteration 150/1000 | Loss: 0.00001510
Iteration 151/1000 | Loss: 0.00001510
Iteration 152/1000 | Loss: 0.00001510
Iteration 153/1000 | Loss: 0.00001510
Iteration 154/1000 | Loss: 0.00001510
Iteration 155/1000 | Loss: 0.00001510
Iteration 156/1000 | Loss: 0.00001510
Iteration 157/1000 | Loss: 0.00001510
Iteration 158/1000 | Loss: 0.00001510
Iteration 159/1000 | Loss: 0.00001510
Iteration 160/1000 | Loss: 0.00001510
Iteration 161/1000 | Loss: 0.00001510
Iteration 162/1000 | Loss: 0.00001510
Iteration 163/1000 | Loss: 0.00001510
Iteration 164/1000 | Loss: 0.00001510
Iteration 165/1000 | Loss: 0.00001510
Iteration 166/1000 | Loss: 0.00001510
Iteration 167/1000 | Loss: 0.00001510
Iteration 168/1000 | Loss: 0.00001510
Iteration 169/1000 | Loss: 0.00001510
Iteration 170/1000 | Loss: 0.00001510
Iteration 171/1000 | Loss: 0.00001510
Iteration 172/1000 | Loss: 0.00001510
Iteration 173/1000 | Loss: 0.00001510
Iteration 174/1000 | Loss: 0.00001510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.5099748452485073e-05, 1.5099748452485073e-05, 1.5099748452485073e-05, 1.5099748452485073e-05, 1.5099748452485073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5099748452485073e-05

Optimization complete. Final v2v error: 3.343679904937744 mm

Highest mean error: 4.258935928344727 mm for frame 2

Lowest mean error: 3.2475123405456543 mm for frame 50

Saving results

Total time: 84.04040718078613
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993233
Iteration 2/25 | Loss: 0.00418642
Iteration 3/25 | Loss: 0.00285757
Iteration 4/25 | Loss: 0.00220919
Iteration 5/25 | Loss: 0.00209212
Iteration 6/25 | Loss: 0.00164934
Iteration 7/25 | Loss: 0.00140549
Iteration 8/25 | Loss: 0.00126992
Iteration 9/25 | Loss: 0.00122324
Iteration 10/25 | Loss: 0.00120144
Iteration 11/25 | Loss: 0.00118766
Iteration 12/25 | Loss: 0.00118401
Iteration 13/25 | Loss: 0.00118742
Iteration 14/25 | Loss: 0.00118094
Iteration 15/25 | Loss: 0.00117897
Iteration 16/25 | Loss: 0.00117851
Iteration 17/25 | Loss: 0.00117835
Iteration 18/25 | Loss: 0.00117828
Iteration 19/25 | Loss: 0.00117827
Iteration 20/25 | Loss: 0.00117827
Iteration 21/25 | Loss: 0.00117827
Iteration 22/25 | Loss: 0.00117827
Iteration 23/25 | Loss: 0.00117827
Iteration 24/25 | Loss: 0.00117827
Iteration 25/25 | Loss: 0.00117826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29562616
Iteration 2/25 | Loss: 0.00126611
Iteration 3/25 | Loss: 0.00126611
Iteration 4/25 | Loss: 0.00126611
Iteration 5/25 | Loss: 0.00126611
Iteration 6/25 | Loss: 0.00126611
Iteration 7/25 | Loss: 0.00126611
Iteration 8/25 | Loss: 0.00126611
Iteration 9/25 | Loss: 0.00126611
Iteration 10/25 | Loss: 0.00126611
Iteration 11/25 | Loss: 0.00126611
Iteration 12/25 | Loss: 0.00126611
Iteration 13/25 | Loss: 0.00126611
Iteration 14/25 | Loss: 0.00126611
Iteration 15/25 | Loss: 0.00126611
Iteration 16/25 | Loss: 0.00126611
Iteration 17/25 | Loss: 0.00126611
Iteration 18/25 | Loss: 0.00126611
Iteration 19/25 | Loss: 0.00126611
Iteration 20/25 | Loss: 0.00126611
Iteration 21/25 | Loss: 0.00126611
Iteration 22/25 | Loss: 0.00126611
Iteration 23/25 | Loss: 0.00126611
Iteration 24/25 | Loss: 0.00126611
Iteration 25/25 | Loss: 0.00126611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126611
Iteration 2/1000 | Loss: 0.00003474
Iteration 3/1000 | Loss: 0.00002659
Iteration 4/1000 | Loss: 0.00002378
Iteration 5/1000 | Loss: 0.00002234
Iteration 6/1000 | Loss: 0.00002156
Iteration 7/1000 | Loss: 0.00002071
Iteration 8/1000 | Loss: 0.00002035
Iteration 9/1000 | Loss: 0.00001984
Iteration 10/1000 | Loss: 0.00001957
Iteration 11/1000 | Loss: 0.00248362
Iteration 12/1000 | Loss: 0.00003399
Iteration 13/1000 | Loss: 0.00006908
Iteration 14/1000 | Loss: 0.00001957
Iteration 15/1000 | Loss: 0.00001672
Iteration 16/1000 | Loss: 0.00002098
Iteration 17/1000 | Loss: 0.00001484
Iteration 18/1000 | Loss: 0.00001222
Iteration 19/1000 | Loss: 0.00001161
Iteration 20/1000 | Loss: 0.00001113
Iteration 21/1000 | Loss: 0.00001083
Iteration 22/1000 | Loss: 0.00001056
Iteration 23/1000 | Loss: 0.00001018
Iteration 24/1000 | Loss: 0.00000995
Iteration 25/1000 | Loss: 0.00000989
Iteration 26/1000 | Loss: 0.00000989
Iteration 27/1000 | Loss: 0.00000976
Iteration 28/1000 | Loss: 0.00000971
Iteration 29/1000 | Loss: 0.00000971
Iteration 30/1000 | Loss: 0.00000969
Iteration 31/1000 | Loss: 0.00000968
Iteration 32/1000 | Loss: 0.00000963
Iteration 33/1000 | Loss: 0.00000962
Iteration 34/1000 | Loss: 0.00000961
Iteration 35/1000 | Loss: 0.00000961
Iteration 36/1000 | Loss: 0.00000960
Iteration 37/1000 | Loss: 0.00000960
Iteration 38/1000 | Loss: 0.00000960
Iteration 39/1000 | Loss: 0.00000959
Iteration 40/1000 | Loss: 0.00000959
Iteration 41/1000 | Loss: 0.00000958
Iteration 42/1000 | Loss: 0.00000958
Iteration 43/1000 | Loss: 0.00000958
Iteration 44/1000 | Loss: 0.00000958
Iteration 45/1000 | Loss: 0.00000958
Iteration 46/1000 | Loss: 0.00000957
Iteration 47/1000 | Loss: 0.00000957
Iteration 48/1000 | Loss: 0.00000957
Iteration 49/1000 | Loss: 0.00000956
Iteration 50/1000 | Loss: 0.00000956
Iteration 51/1000 | Loss: 0.00000955
Iteration 52/1000 | Loss: 0.00000955
Iteration 53/1000 | Loss: 0.00000955
Iteration 54/1000 | Loss: 0.00000954
Iteration 55/1000 | Loss: 0.00000954
Iteration 56/1000 | Loss: 0.00000953
Iteration 57/1000 | Loss: 0.00000953
Iteration 58/1000 | Loss: 0.00000953
Iteration 59/1000 | Loss: 0.00000953
Iteration 60/1000 | Loss: 0.00000952
Iteration 61/1000 | Loss: 0.00000952
Iteration 62/1000 | Loss: 0.00000952
Iteration 63/1000 | Loss: 0.00000952
Iteration 64/1000 | Loss: 0.00000952
Iteration 65/1000 | Loss: 0.00000952
Iteration 66/1000 | Loss: 0.00000951
Iteration 67/1000 | Loss: 0.00000951
Iteration 68/1000 | Loss: 0.00000951
Iteration 69/1000 | Loss: 0.00000951
Iteration 70/1000 | Loss: 0.00000951
Iteration 71/1000 | Loss: 0.00000951
Iteration 72/1000 | Loss: 0.00000951
Iteration 73/1000 | Loss: 0.00000951
Iteration 74/1000 | Loss: 0.00000951
Iteration 75/1000 | Loss: 0.00000951
Iteration 76/1000 | Loss: 0.00000951
Iteration 77/1000 | Loss: 0.00000951
Iteration 78/1000 | Loss: 0.00000951
Iteration 79/1000 | Loss: 0.00000951
Iteration 80/1000 | Loss: 0.00000950
Iteration 81/1000 | Loss: 0.00000950
Iteration 82/1000 | Loss: 0.00000950
Iteration 83/1000 | Loss: 0.00000950
Iteration 84/1000 | Loss: 0.00000950
Iteration 85/1000 | Loss: 0.00000950
Iteration 86/1000 | Loss: 0.00000950
Iteration 87/1000 | Loss: 0.00000950
Iteration 88/1000 | Loss: 0.00000949
Iteration 89/1000 | Loss: 0.00000949
Iteration 90/1000 | Loss: 0.00000949
Iteration 91/1000 | Loss: 0.00000949
Iteration 92/1000 | Loss: 0.00000949
Iteration 93/1000 | Loss: 0.00000949
Iteration 94/1000 | Loss: 0.00000949
Iteration 95/1000 | Loss: 0.00000949
Iteration 96/1000 | Loss: 0.00000949
Iteration 97/1000 | Loss: 0.00000949
Iteration 98/1000 | Loss: 0.00000948
Iteration 99/1000 | Loss: 0.00000948
Iteration 100/1000 | Loss: 0.00000948
Iteration 101/1000 | Loss: 0.00000948
Iteration 102/1000 | Loss: 0.00000948
Iteration 103/1000 | Loss: 0.00000948
Iteration 104/1000 | Loss: 0.00000948
Iteration 105/1000 | Loss: 0.00000948
Iteration 106/1000 | Loss: 0.00000947
Iteration 107/1000 | Loss: 0.00000947
Iteration 108/1000 | Loss: 0.00000947
Iteration 109/1000 | Loss: 0.00000947
Iteration 110/1000 | Loss: 0.00000947
Iteration 111/1000 | Loss: 0.00000947
Iteration 112/1000 | Loss: 0.00000947
Iteration 113/1000 | Loss: 0.00000947
Iteration 114/1000 | Loss: 0.00000947
Iteration 115/1000 | Loss: 0.00000947
Iteration 116/1000 | Loss: 0.00000947
Iteration 117/1000 | Loss: 0.00000947
Iteration 118/1000 | Loss: 0.00000947
Iteration 119/1000 | Loss: 0.00000947
Iteration 120/1000 | Loss: 0.00000947
Iteration 121/1000 | Loss: 0.00000947
Iteration 122/1000 | Loss: 0.00000947
Iteration 123/1000 | Loss: 0.00000947
Iteration 124/1000 | Loss: 0.00000947
Iteration 125/1000 | Loss: 0.00000947
Iteration 126/1000 | Loss: 0.00000947
Iteration 127/1000 | Loss: 0.00000947
Iteration 128/1000 | Loss: 0.00000947
Iteration 129/1000 | Loss: 0.00000947
Iteration 130/1000 | Loss: 0.00000947
Iteration 131/1000 | Loss: 0.00000947
Iteration 132/1000 | Loss: 0.00000947
Iteration 133/1000 | Loss: 0.00000947
Iteration 134/1000 | Loss: 0.00000947
Iteration 135/1000 | Loss: 0.00000947
Iteration 136/1000 | Loss: 0.00000947
Iteration 137/1000 | Loss: 0.00000947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [9.471978955843952e-06, 9.471978955843952e-06, 9.471978955843952e-06, 9.471978955843952e-06, 9.471978955843952e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.471978955843952e-06

Optimization complete. Final v2v error: 2.682154417037964 mm

Highest mean error: 3.151660203933716 mm for frame 137

Lowest mean error: 2.5629630088806152 mm for frame 115

Saving results

Total time: 73.95187568664551
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415500
Iteration 2/25 | Loss: 0.00123616
Iteration 3/25 | Loss: 0.00116570
Iteration 4/25 | Loss: 0.00115757
Iteration 5/25 | Loss: 0.00115507
Iteration 6/25 | Loss: 0.00115454
Iteration 7/25 | Loss: 0.00115454
Iteration 8/25 | Loss: 0.00115454
Iteration 9/25 | Loss: 0.00115454
Iteration 10/25 | Loss: 0.00115454
Iteration 11/25 | Loss: 0.00115454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011545444140210748, 0.0011545444140210748, 0.0011545444140210748, 0.0011545444140210748, 0.0011545444140210748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011545444140210748

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.70270753
Iteration 2/25 | Loss: 0.00138929
Iteration 3/25 | Loss: 0.00138929
Iteration 4/25 | Loss: 0.00138929
Iteration 5/25 | Loss: 0.00138929
Iteration 6/25 | Loss: 0.00138929
Iteration 7/25 | Loss: 0.00138929
Iteration 8/25 | Loss: 0.00138929
Iteration 9/25 | Loss: 0.00138929
Iteration 10/25 | Loss: 0.00138929
Iteration 11/25 | Loss: 0.00138929
Iteration 12/25 | Loss: 0.00138929
Iteration 13/25 | Loss: 0.00138929
Iteration 14/25 | Loss: 0.00138929
Iteration 15/25 | Loss: 0.00138929
Iteration 16/25 | Loss: 0.00138929
Iteration 17/25 | Loss: 0.00138929
Iteration 18/25 | Loss: 0.00138929
Iteration 19/25 | Loss: 0.00138929
Iteration 20/25 | Loss: 0.00138929
Iteration 21/25 | Loss: 0.00138929
Iteration 22/25 | Loss: 0.00138929
Iteration 23/25 | Loss: 0.00138929
Iteration 24/25 | Loss: 0.00138929
Iteration 25/25 | Loss: 0.00138929

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138929
Iteration 2/1000 | Loss: 0.00001889
Iteration 3/1000 | Loss: 0.00001439
Iteration 4/1000 | Loss: 0.00001300
Iteration 5/1000 | Loss: 0.00001225
Iteration 6/1000 | Loss: 0.00001157
Iteration 7/1000 | Loss: 0.00001116
Iteration 8/1000 | Loss: 0.00001089
Iteration 9/1000 | Loss: 0.00001055
Iteration 10/1000 | Loss: 0.00001038
Iteration 11/1000 | Loss: 0.00001037
Iteration 12/1000 | Loss: 0.00001037
Iteration 13/1000 | Loss: 0.00001024
Iteration 14/1000 | Loss: 0.00001012
Iteration 15/1000 | Loss: 0.00001007
Iteration 16/1000 | Loss: 0.00001003
Iteration 17/1000 | Loss: 0.00001002
Iteration 18/1000 | Loss: 0.00001002
Iteration 19/1000 | Loss: 0.00000996
Iteration 20/1000 | Loss: 0.00000995
Iteration 21/1000 | Loss: 0.00000988
Iteration 22/1000 | Loss: 0.00000988
Iteration 23/1000 | Loss: 0.00000980
Iteration 24/1000 | Loss: 0.00000975
Iteration 25/1000 | Loss: 0.00000974
Iteration 26/1000 | Loss: 0.00000974
Iteration 27/1000 | Loss: 0.00000974
Iteration 28/1000 | Loss: 0.00000973
Iteration 29/1000 | Loss: 0.00000973
Iteration 30/1000 | Loss: 0.00000973
Iteration 31/1000 | Loss: 0.00000972
Iteration 32/1000 | Loss: 0.00000972
Iteration 33/1000 | Loss: 0.00000971
Iteration 34/1000 | Loss: 0.00000971
Iteration 35/1000 | Loss: 0.00000970
Iteration 36/1000 | Loss: 0.00000970
Iteration 37/1000 | Loss: 0.00000969
Iteration 38/1000 | Loss: 0.00000969
Iteration 39/1000 | Loss: 0.00000969
Iteration 40/1000 | Loss: 0.00000969
Iteration 41/1000 | Loss: 0.00000969
Iteration 42/1000 | Loss: 0.00000969
Iteration 43/1000 | Loss: 0.00000968
Iteration 44/1000 | Loss: 0.00000968
Iteration 45/1000 | Loss: 0.00000968
Iteration 46/1000 | Loss: 0.00000968
Iteration 47/1000 | Loss: 0.00000968
Iteration 48/1000 | Loss: 0.00000968
Iteration 49/1000 | Loss: 0.00000968
Iteration 50/1000 | Loss: 0.00000968
Iteration 51/1000 | Loss: 0.00000968
Iteration 52/1000 | Loss: 0.00000967
Iteration 53/1000 | Loss: 0.00000967
Iteration 54/1000 | Loss: 0.00000967
Iteration 55/1000 | Loss: 0.00000966
Iteration 56/1000 | Loss: 0.00000966
Iteration 57/1000 | Loss: 0.00000966
Iteration 58/1000 | Loss: 0.00000966
Iteration 59/1000 | Loss: 0.00000966
Iteration 60/1000 | Loss: 0.00000966
Iteration 61/1000 | Loss: 0.00000966
Iteration 62/1000 | Loss: 0.00000966
Iteration 63/1000 | Loss: 0.00000966
Iteration 64/1000 | Loss: 0.00000966
Iteration 65/1000 | Loss: 0.00000966
Iteration 66/1000 | Loss: 0.00000965
Iteration 67/1000 | Loss: 0.00000965
Iteration 68/1000 | Loss: 0.00000964
Iteration 69/1000 | Loss: 0.00000964
Iteration 70/1000 | Loss: 0.00000964
Iteration 71/1000 | Loss: 0.00000963
Iteration 72/1000 | Loss: 0.00000963
Iteration 73/1000 | Loss: 0.00000963
Iteration 74/1000 | Loss: 0.00000962
Iteration 75/1000 | Loss: 0.00000962
Iteration 76/1000 | Loss: 0.00000962
Iteration 77/1000 | Loss: 0.00000962
Iteration 78/1000 | Loss: 0.00000962
Iteration 79/1000 | Loss: 0.00000961
Iteration 80/1000 | Loss: 0.00000961
Iteration 81/1000 | Loss: 0.00000960
Iteration 82/1000 | Loss: 0.00000960
Iteration 83/1000 | Loss: 0.00000960
Iteration 84/1000 | Loss: 0.00000959
Iteration 85/1000 | Loss: 0.00000959
Iteration 86/1000 | Loss: 0.00000959
Iteration 87/1000 | Loss: 0.00000959
Iteration 88/1000 | Loss: 0.00000958
Iteration 89/1000 | Loss: 0.00000958
Iteration 90/1000 | Loss: 0.00000958
Iteration 91/1000 | Loss: 0.00000958
Iteration 92/1000 | Loss: 0.00000958
Iteration 93/1000 | Loss: 0.00000958
Iteration 94/1000 | Loss: 0.00000958
Iteration 95/1000 | Loss: 0.00000957
Iteration 96/1000 | Loss: 0.00000957
Iteration 97/1000 | Loss: 0.00000957
Iteration 98/1000 | Loss: 0.00000956
Iteration 99/1000 | Loss: 0.00000955
Iteration 100/1000 | Loss: 0.00000955
Iteration 101/1000 | Loss: 0.00000955
Iteration 102/1000 | Loss: 0.00000955
Iteration 103/1000 | Loss: 0.00000954
Iteration 104/1000 | Loss: 0.00000954
Iteration 105/1000 | Loss: 0.00000954
Iteration 106/1000 | Loss: 0.00000953
Iteration 107/1000 | Loss: 0.00000953
Iteration 108/1000 | Loss: 0.00000953
Iteration 109/1000 | Loss: 0.00000953
Iteration 110/1000 | Loss: 0.00000952
Iteration 111/1000 | Loss: 0.00000952
Iteration 112/1000 | Loss: 0.00000952
Iteration 113/1000 | Loss: 0.00000952
Iteration 114/1000 | Loss: 0.00000952
Iteration 115/1000 | Loss: 0.00000952
Iteration 116/1000 | Loss: 0.00000951
Iteration 117/1000 | Loss: 0.00000951
Iteration 118/1000 | Loss: 0.00000951
Iteration 119/1000 | Loss: 0.00000951
Iteration 120/1000 | Loss: 0.00000951
Iteration 121/1000 | Loss: 0.00000951
Iteration 122/1000 | Loss: 0.00000950
Iteration 123/1000 | Loss: 0.00000950
Iteration 124/1000 | Loss: 0.00000950
Iteration 125/1000 | Loss: 0.00000950
Iteration 126/1000 | Loss: 0.00000950
Iteration 127/1000 | Loss: 0.00000949
Iteration 128/1000 | Loss: 0.00000949
Iteration 129/1000 | Loss: 0.00000949
Iteration 130/1000 | Loss: 0.00000949
Iteration 131/1000 | Loss: 0.00000949
Iteration 132/1000 | Loss: 0.00000949
Iteration 133/1000 | Loss: 0.00000948
Iteration 134/1000 | Loss: 0.00000948
Iteration 135/1000 | Loss: 0.00000948
Iteration 136/1000 | Loss: 0.00000948
Iteration 137/1000 | Loss: 0.00000947
Iteration 138/1000 | Loss: 0.00000947
Iteration 139/1000 | Loss: 0.00000947
Iteration 140/1000 | Loss: 0.00000947
Iteration 141/1000 | Loss: 0.00000947
Iteration 142/1000 | Loss: 0.00000947
Iteration 143/1000 | Loss: 0.00000947
Iteration 144/1000 | Loss: 0.00000947
Iteration 145/1000 | Loss: 0.00000947
Iteration 146/1000 | Loss: 0.00000947
Iteration 147/1000 | Loss: 0.00000947
Iteration 148/1000 | Loss: 0.00000947
Iteration 149/1000 | Loss: 0.00000946
Iteration 150/1000 | Loss: 0.00000946
Iteration 151/1000 | Loss: 0.00000946
Iteration 152/1000 | Loss: 0.00000945
Iteration 153/1000 | Loss: 0.00000945
Iteration 154/1000 | Loss: 0.00000944
Iteration 155/1000 | Loss: 0.00000944
Iteration 156/1000 | Loss: 0.00000944
Iteration 157/1000 | Loss: 0.00000944
Iteration 158/1000 | Loss: 0.00000944
Iteration 159/1000 | Loss: 0.00000944
Iteration 160/1000 | Loss: 0.00000944
Iteration 161/1000 | Loss: 0.00000944
Iteration 162/1000 | Loss: 0.00000944
Iteration 163/1000 | Loss: 0.00000944
Iteration 164/1000 | Loss: 0.00000944
Iteration 165/1000 | Loss: 0.00000944
Iteration 166/1000 | Loss: 0.00000944
Iteration 167/1000 | Loss: 0.00000944
Iteration 168/1000 | Loss: 0.00000943
Iteration 169/1000 | Loss: 0.00000943
Iteration 170/1000 | Loss: 0.00000943
Iteration 171/1000 | Loss: 0.00000943
Iteration 172/1000 | Loss: 0.00000943
Iteration 173/1000 | Loss: 0.00000943
Iteration 174/1000 | Loss: 0.00000942
Iteration 175/1000 | Loss: 0.00000942
Iteration 176/1000 | Loss: 0.00000942
Iteration 177/1000 | Loss: 0.00000942
Iteration 178/1000 | Loss: 0.00000942
Iteration 179/1000 | Loss: 0.00000942
Iteration 180/1000 | Loss: 0.00000942
Iteration 181/1000 | Loss: 0.00000942
Iteration 182/1000 | Loss: 0.00000942
Iteration 183/1000 | Loss: 0.00000942
Iteration 184/1000 | Loss: 0.00000942
Iteration 185/1000 | Loss: 0.00000942
Iteration 186/1000 | Loss: 0.00000942
Iteration 187/1000 | Loss: 0.00000942
Iteration 188/1000 | Loss: 0.00000942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [9.421239155926742e-06, 9.421239155926742e-06, 9.421239155926742e-06, 9.421239155926742e-06, 9.421239155926742e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.421239155926742e-06

Optimization complete. Final v2v error: 2.686204433441162 mm

Highest mean error: 2.9118082523345947 mm for frame 76

Lowest mean error: 2.5152828693389893 mm for frame 161

Saving results

Total time: 41.35911965370178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845033
Iteration 2/25 | Loss: 0.00158264
Iteration 3/25 | Loss: 0.00131048
Iteration 4/25 | Loss: 0.00127209
Iteration 5/25 | Loss: 0.00126577
Iteration 6/25 | Loss: 0.00126452
Iteration 7/25 | Loss: 0.00126452
Iteration 8/25 | Loss: 0.00126452
Iteration 9/25 | Loss: 0.00126452
Iteration 10/25 | Loss: 0.00126452
Iteration 11/25 | Loss: 0.00126452
Iteration 12/25 | Loss: 0.00126452
Iteration 13/25 | Loss: 0.00126452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012645231327041984, 0.0012645231327041984, 0.0012645231327041984, 0.0012645231327041984, 0.0012645231327041984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012645231327041984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18731904
Iteration 2/25 | Loss: 0.00109280
Iteration 3/25 | Loss: 0.00109276
Iteration 4/25 | Loss: 0.00109276
Iteration 5/25 | Loss: 0.00109276
Iteration 6/25 | Loss: 0.00109276
Iteration 7/25 | Loss: 0.00109276
Iteration 8/25 | Loss: 0.00109276
Iteration 9/25 | Loss: 0.00109276
Iteration 10/25 | Loss: 0.00109276
Iteration 11/25 | Loss: 0.00109276
Iteration 12/25 | Loss: 0.00109276
Iteration 13/25 | Loss: 0.00109276
Iteration 14/25 | Loss: 0.00109276
Iteration 15/25 | Loss: 0.00109276
Iteration 16/25 | Loss: 0.00109276
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010927565163001418, 0.0010927565163001418, 0.0010927565163001418, 0.0010927565163001418, 0.0010927565163001418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010927565163001418

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109276
Iteration 2/1000 | Loss: 0.00006142
Iteration 3/1000 | Loss: 0.00004224
Iteration 4/1000 | Loss: 0.00003412
Iteration 5/1000 | Loss: 0.00003113
Iteration 6/1000 | Loss: 0.00002958
Iteration 7/1000 | Loss: 0.00002858
Iteration 8/1000 | Loss: 0.00002789
Iteration 9/1000 | Loss: 0.00002747
Iteration 10/1000 | Loss: 0.00002707
Iteration 11/1000 | Loss: 0.00002675
Iteration 12/1000 | Loss: 0.00002638
Iteration 13/1000 | Loss: 0.00002605
Iteration 14/1000 | Loss: 0.00002582
Iteration 15/1000 | Loss: 0.00002562
Iteration 16/1000 | Loss: 0.00002541
Iteration 17/1000 | Loss: 0.00002522
Iteration 18/1000 | Loss: 0.00002514
Iteration 19/1000 | Loss: 0.00002513
Iteration 20/1000 | Loss: 0.00002512
Iteration 21/1000 | Loss: 0.00002512
Iteration 22/1000 | Loss: 0.00002506
Iteration 23/1000 | Loss: 0.00002506
Iteration 24/1000 | Loss: 0.00002503
Iteration 25/1000 | Loss: 0.00002503
Iteration 26/1000 | Loss: 0.00002502
Iteration 27/1000 | Loss: 0.00002501
Iteration 28/1000 | Loss: 0.00002500
Iteration 29/1000 | Loss: 0.00002500
Iteration 30/1000 | Loss: 0.00002499
Iteration 31/1000 | Loss: 0.00002499
Iteration 32/1000 | Loss: 0.00002498
Iteration 33/1000 | Loss: 0.00002496
Iteration 34/1000 | Loss: 0.00002495
Iteration 35/1000 | Loss: 0.00002494
Iteration 36/1000 | Loss: 0.00002494
Iteration 37/1000 | Loss: 0.00002494
Iteration 38/1000 | Loss: 0.00002493
Iteration 39/1000 | Loss: 0.00002493
Iteration 40/1000 | Loss: 0.00002492
Iteration 41/1000 | Loss: 0.00002492
Iteration 42/1000 | Loss: 0.00002490
Iteration 43/1000 | Loss: 0.00002489
Iteration 44/1000 | Loss: 0.00002489
Iteration 45/1000 | Loss: 0.00002489
Iteration 46/1000 | Loss: 0.00002489
Iteration 47/1000 | Loss: 0.00002489
Iteration 48/1000 | Loss: 0.00002488
Iteration 49/1000 | Loss: 0.00002488
Iteration 50/1000 | Loss: 0.00002486
Iteration 51/1000 | Loss: 0.00002484
Iteration 52/1000 | Loss: 0.00002484
Iteration 53/1000 | Loss: 0.00002484
Iteration 54/1000 | Loss: 0.00002483
Iteration 55/1000 | Loss: 0.00002483
Iteration 56/1000 | Loss: 0.00002483
Iteration 57/1000 | Loss: 0.00002483
Iteration 58/1000 | Loss: 0.00002483
Iteration 59/1000 | Loss: 0.00002483
Iteration 60/1000 | Loss: 0.00002481
Iteration 61/1000 | Loss: 0.00002478
Iteration 62/1000 | Loss: 0.00002478
Iteration 63/1000 | Loss: 0.00002477
Iteration 64/1000 | Loss: 0.00002477
Iteration 65/1000 | Loss: 0.00002476
Iteration 66/1000 | Loss: 0.00002475
Iteration 67/1000 | Loss: 0.00002475
Iteration 68/1000 | Loss: 0.00002473
Iteration 69/1000 | Loss: 0.00002472
Iteration 70/1000 | Loss: 0.00002471
Iteration 71/1000 | Loss: 0.00002471
Iteration 72/1000 | Loss: 0.00002470
Iteration 73/1000 | Loss: 0.00002470
Iteration 74/1000 | Loss: 0.00002469
Iteration 75/1000 | Loss: 0.00002468
Iteration 76/1000 | Loss: 0.00002468
Iteration 77/1000 | Loss: 0.00002468
Iteration 78/1000 | Loss: 0.00002468
Iteration 79/1000 | Loss: 0.00002467
Iteration 80/1000 | Loss: 0.00002466
Iteration 81/1000 | Loss: 0.00002466
Iteration 82/1000 | Loss: 0.00002465
Iteration 83/1000 | Loss: 0.00002465
Iteration 84/1000 | Loss: 0.00002465
Iteration 85/1000 | Loss: 0.00002465
Iteration 86/1000 | Loss: 0.00002465
Iteration 87/1000 | Loss: 0.00002465
Iteration 88/1000 | Loss: 0.00002464
Iteration 89/1000 | Loss: 0.00002464
Iteration 90/1000 | Loss: 0.00002464
Iteration 91/1000 | Loss: 0.00002464
Iteration 92/1000 | Loss: 0.00002464
Iteration 93/1000 | Loss: 0.00002464
Iteration 94/1000 | Loss: 0.00002464
Iteration 95/1000 | Loss: 0.00002464
Iteration 96/1000 | Loss: 0.00002464
Iteration 97/1000 | Loss: 0.00002463
Iteration 98/1000 | Loss: 0.00002463
Iteration 99/1000 | Loss: 0.00002463
Iteration 100/1000 | Loss: 0.00002463
Iteration 101/1000 | Loss: 0.00002462
Iteration 102/1000 | Loss: 0.00002462
Iteration 103/1000 | Loss: 0.00002462
Iteration 104/1000 | Loss: 0.00002462
Iteration 105/1000 | Loss: 0.00002462
Iteration 106/1000 | Loss: 0.00002462
Iteration 107/1000 | Loss: 0.00002462
Iteration 108/1000 | Loss: 0.00002462
Iteration 109/1000 | Loss: 0.00002461
Iteration 110/1000 | Loss: 0.00002461
Iteration 111/1000 | Loss: 0.00002461
Iteration 112/1000 | Loss: 0.00002461
Iteration 113/1000 | Loss: 0.00002461
Iteration 114/1000 | Loss: 0.00002460
Iteration 115/1000 | Loss: 0.00002460
Iteration 116/1000 | Loss: 0.00002460
Iteration 117/1000 | Loss: 0.00002460
Iteration 118/1000 | Loss: 0.00002460
Iteration 119/1000 | Loss: 0.00002460
Iteration 120/1000 | Loss: 0.00002460
Iteration 121/1000 | Loss: 0.00002459
Iteration 122/1000 | Loss: 0.00002459
Iteration 123/1000 | Loss: 0.00002459
Iteration 124/1000 | Loss: 0.00002459
Iteration 125/1000 | Loss: 0.00002459
Iteration 126/1000 | Loss: 0.00002459
Iteration 127/1000 | Loss: 0.00002459
Iteration 128/1000 | Loss: 0.00002459
Iteration 129/1000 | Loss: 0.00002459
Iteration 130/1000 | Loss: 0.00002459
Iteration 131/1000 | Loss: 0.00002459
Iteration 132/1000 | Loss: 0.00002459
Iteration 133/1000 | Loss: 0.00002459
Iteration 134/1000 | Loss: 0.00002459
Iteration 135/1000 | Loss: 0.00002459
Iteration 136/1000 | Loss: 0.00002459
Iteration 137/1000 | Loss: 0.00002459
Iteration 138/1000 | Loss: 0.00002459
Iteration 139/1000 | Loss: 0.00002458
Iteration 140/1000 | Loss: 0.00002458
Iteration 141/1000 | Loss: 0.00002458
Iteration 142/1000 | Loss: 0.00002458
Iteration 143/1000 | Loss: 0.00002458
Iteration 144/1000 | Loss: 0.00002458
Iteration 145/1000 | Loss: 0.00002458
Iteration 146/1000 | Loss: 0.00002458
Iteration 147/1000 | Loss: 0.00002458
Iteration 148/1000 | Loss: 0.00002458
Iteration 149/1000 | Loss: 0.00002458
Iteration 150/1000 | Loss: 0.00002457
Iteration 151/1000 | Loss: 0.00002457
Iteration 152/1000 | Loss: 0.00002457
Iteration 153/1000 | Loss: 0.00002457
Iteration 154/1000 | Loss: 0.00002457
Iteration 155/1000 | Loss: 0.00002457
Iteration 156/1000 | Loss: 0.00002457
Iteration 157/1000 | Loss: 0.00002457
Iteration 158/1000 | Loss: 0.00002457
Iteration 159/1000 | Loss: 0.00002457
Iteration 160/1000 | Loss: 0.00002457
Iteration 161/1000 | Loss: 0.00002457
Iteration 162/1000 | Loss: 0.00002457
Iteration 163/1000 | Loss: 0.00002457
Iteration 164/1000 | Loss: 0.00002457
Iteration 165/1000 | Loss: 0.00002457
Iteration 166/1000 | Loss: 0.00002457
Iteration 167/1000 | Loss: 0.00002457
Iteration 168/1000 | Loss: 0.00002456
Iteration 169/1000 | Loss: 0.00002456
Iteration 170/1000 | Loss: 0.00002456
Iteration 171/1000 | Loss: 0.00002456
Iteration 172/1000 | Loss: 0.00002456
Iteration 173/1000 | Loss: 0.00002456
Iteration 174/1000 | Loss: 0.00002456
Iteration 175/1000 | Loss: 0.00002456
Iteration 176/1000 | Loss: 0.00002456
Iteration 177/1000 | Loss: 0.00002456
Iteration 178/1000 | Loss: 0.00002456
Iteration 179/1000 | Loss: 0.00002456
Iteration 180/1000 | Loss: 0.00002456
Iteration 181/1000 | Loss: 0.00002456
Iteration 182/1000 | Loss: 0.00002456
Iteration 183/1000 | Loss: 0.00002456
Iteration 184/1000 | Loss: 0.00002456
Iteration 185/1000 | Loss: 0.00002455
Iteration 186/1000 | Loss: 0.00002455
Iteration 187/1000 | Loss: 0.00002455
Iteration 188/1000 | Loss: 0.00002455
Iteration 189/1000 | Loss: 0.00002455
Iteration 190/1000 | Loss: 0.00002455
Iteration 191/1000 | Loss: 0.00002455
Iteration 192/1000 | Loss: 0.00002455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [2.455465801176615e-05, 2.455465801176615e-05, 2.455465801176615e-05, 2.455465801176615e-05, 2.455465801176615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.455465801176615e-05

Optimization complete. Final v2v error: 4.102845191955566 mm

Highest mean error: 4.440415382385254 mm for frame 3

Lowest mean error: 3.901171922683716 mm for frame 136

Saving results

Total time: 48.02113127708435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047802
Iteration 2/25 | Loss: 0.00202067
Iteration 3/25 | Loss: 0.00146210
Iteration 4/25 | Loss: 0.00139785
Iteration 5/25 | Loss: 0.00139008
Iteration 6/25 | Loss: 0.00138912
Iteration 7/25 | Loss: 0.00138912
Iteration 8/25 | Loss: 0.00138912
Iteration 9/25 | Loss: 0.00138912
Iteration 10/25 | Loss: 0.00138912
Iteration 11/25 | Loss: 0.00138912
Iteration 12/25 | Loss: 0.00138912
Iteration 13/25 | Loss: 0.00138912
Iteration 14/25 | Loss: 0.00138912
Iteration 15/25 | Loss: 0.00138912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013891165144741535, 0.0013891165144741535, 0.0013891165144741535, 0.0013891165144741535, 0.0013891165144741535]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013891165144741535

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84504294
Iteration 2/25 | Loss: 0.00087541
Iteration 3/25 | Loss: 0.00087540
Iteration 4/25 | Loss: 0.00087540
Iteration 5/25 | Loss: 0.00087540
Iteration 6/25 | Loss: 0.00087540
Iteration 7/25 | Loss: 0.00087540
Iteration 8/25 | Loss: 0.00087540
Iteration 9/25 | Loss: 0.00087540
Iteration 10/25 | Loss: 0.00087540
Iteration 11/25 | Loss: 0.00087540
Iteration 12/25 | Loss: 0.00087540
Iteration 13/25 | Loss: 0.00087540
Iteration 14/25 | Loss: 0.00087540
Iteration 15/25 | Loss: 0.00087540
Iteration 16/25 | Loss: 0.00087540
Iteration 17/25 | Loss: 0.00087540
Iteration 18/25 | Loss: 0.00087540
Iteration 19/25 | Loss: 0.00087540
Iteration 20/25 | Loss: 0.00087540
Iteration 21/25 | Loss: 0.00087540
Iteration 22/25 | Loss: 0.00087540
Iteration 23/25 | Loss: 0.00087540
Iteration 24/25 | Loss: 0.00087540
Iteration 25/25 | Loss: 0.00087540

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087540
Iteration 2/1000 | Loss: 0.00006446
Iteration 3/1000 | Loss: 0.00004264
Iteration 4/1000 | Loss: 0.00003812
Iteration 5/1000 | Loss: 0.00003654
Iteration 6/1000 | Loss: 0.00003543
Iteration 7/1000 | Loss: 0.00003471
Iteration 8/1000 | Loss: 0.00003409
Iteration 9/1000 | Loss: 0.00003353
Iteration 10/1000 | Loss: 0.00003309
Iteration 11/1000 | Loss: 0.00003270
Iteration 12/1000 | Loss: 0.00003236
Iteration 13/1000 | Loss: 0.00003206
Iteration 14/1000 | Loss: 0.00003187
Iteration 15/1000 | Loss: 0.00003170
Iteration 16/1000 | Loss: 0.00003167
Iteration 17/1000 | Loss: 0.00003160
Iteration 18/1000 | Loss: 0.00003155
Iteration 19/1000 | Loss: 0.00003149
Iteration 20/1000 | Loss: 0.00003149
Iteration 21/1000 | Loss: 0.00003147
Iteration 22/1000 | Loss: 0.00003142
Iteration 23/1000 | Loss: 0.00003141
Iteration 24/1000 | Loss: 0.00003136
Iteration 25/1000 | Loss: 0.00003132
Iteration 26/1000 | Loss: 0.00003129
Iteration 27/1000 | Loss: 0.00003125
Iteration 28/1000 | Loss: 0.00003124
Iteration 29/1000 | Loss: 0.00003123
Iteration 30/1000 | Loss: 0.00003123
Iteration 31/1000 | Loss: 0.00003122
Iteration 32/1000 | Loss: 0.00003120
Iteration 33/1000 | Loss: 0.00003120
Iteration 34/1000 | Loss: 0.00003120
Iteration 35/1000 | Loss: 0.00003120
Iteration 36/1000 | Loss: 0.00003120
Iteration 37/1000 | Loss: 0.00003120
Iteration 38/1000 | Loss: 0.00003120
Iteration 39/1000 | Loss: 0.00003119
Iteration 40/1000 | Loss: 0.00003119
Iteration 41/1000 | Loss: 0.00003119
Iteration 42/1000 | Loss: 0.00003118
Iteration 43/1000 | Loss: 0.00003117
Iteration 44/1000 | Loss: 0.00003116
Iteration 45/1000 | Loss: 0.00003116
Iteration 46/1000 | Loss: 0.00003116
Iteration 47/1000 | Loss: 0.00003116
Iteration 48/1000 | Loss: 0.00003116
Iteration 49/1000 | Loss: 0.00003115
Iteration 50/1000 | Loss: 0.00003115
Iteration 51/1000 | Loss: 0.00003115
Iteration 52/1000 | Loss: 0.00003115
Iteration 53/1000 | Loss: 0.00003115
Iteration 54/1000 | Loss: 0.00003115
Iteration 55/1000 | Loss: 0.00003115
Iteration 56/1000 | Loss: 0.00003115
Iteration 57/1000 | Loss: 0.00003115
Iteration 58/1000 | Loss: 0.00003115
Iteration 59/1000 | Loss: 0.00003115
Iteration 60/1000 | Loss: 0.00003115
Iteration 61/1000 | Loss: 0.00003115
Iteration 62/1000 | Loss: 0.00003115
Iteration 63/1000 | Loss: 0.00003115
Iteration 64/1000 | Loss: 0.00003115
Iteration 65/1000 | Loss: 0.00003115
Iteration 66/1000 | Loss: 0.00003115
Iteration 67/1000 | Loss: 0.00003115
Iteration 68/1000 | Loss: 0.00003115
Iteration 69/1000 | Loss: 0.00003115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [3.1152529118116945e-05, 3.1152529118116945e-05, 3.1152529118116945e-05, 3.1152529118116945e-05, 3.1152529118116945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1152529118116945e-05

Optimization complete. Final v2v error: 4.582964897155762 mm

Highest mean error: 5.062385559082031 mm for frame 100

Lowest mean error: 4.1279449462890625 mm for frame 221

Saving results

Total time: 42.02024006843567
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00348930
Iteration 2/25 | Loss: 0.00122013
Iteration 3/25 | Loss: 0.00115591
Iteration 4/25 | Loss: 0.00114643
Iteration 5/25 | Loss: 0.00114284
Iteration 6/25 | Loss: 0.00114157
Iteration 7/25 | Loss: 0.00114157
Iteration 8/25 | Loss: 0.00114157
Iteration 9/25 | Loss: 0.00114157
Iteration 10/25 | Loss: 0.00114157
Iteration 11/25 | Loss: 0.00114157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011415693443268538, 0.0011415693443268538, 0.0011415693443268538, 0.0011415693443268538, 0.0011415693443268538]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011415693443268538

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30107450
Iteration 2/25 | Loss: 0.00162671
Iteration 3/25 | Loss: 0.00162671
Iteration 4/25 | Loss: 0.00162671
Iteration 5/25 | Loss: 0.00162671
Iteration 6/25 | Loss: 0.00162671
Iteration 7/25 | Loss: 0.00162671
Iteration 8/25 | Loss: 0.00162671
Iteration 9/25 | Loss: 0.00162671
Iteration 10/25 | Loss: 0.00162671
Iteration 11/25 | Loss: 0.00162671
Iteration 12/25 | Loss: 0.00162671
Iteration 13/25 | Loss: 0.00162671
Iteration 14/25 | Loss: 0.00162671
Iteration 15/25 | Loss: 0.00162671
Iteration 16/25 | Loss: 0.00162671
Iteration 17/25 | Loss: 0.00162671
Iteration 18/25 | Loss: 0.00162671
Iteration 19/25 | Loss: 0.00162671
Iteration 20/25 | Loss: 0.00162671
Iteration 21/25 | Loss: 0.00162671
Iteration 22/25 | Loss: 0.00162671
Iteration 23/25 | Loss: 0.00162671
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016267072642222047, 0.0016267072642222047, 0.0016267072642222047, 0.0016267072642222047, 0.0016267072642222047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016267072642222047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162671
Iteration 2/1000 | Loss: 0.00002645
Iteration 3/1000 | Loss: 0.00001687
Iteration 4/1000 | Loss: 0.00001424
Iteration 5/1000 | Loss: 0.00001311
Iteration 6/1000 | Loss: 0.00001266
Iteration 7/1000 | Loss: 0.00001220
Iteration 8/1000 | Loss: 0.00001190
Iteration 9/1000 | Loss: 0.00001171
Iteration 10/1000 | Loss: 0.00001149
Iteration 11/1000 | Loss: 0.00001137
Iteration 12/1000 | Loss: 0.00001130
Iteration 13/1000 | Loss: 0.00001129
Iteration 14/1000 | Loss: 0.00001129
Iteration 15/1000 | Loss: 0.00001128
Iteration 16/1000 | Loss: 0.00001128
Iteration 17/1000 | Loss: 0.00001127
Iteration 18/1000 | Loss: 0.00001126
Iteration 19/1000 | Loss: 0.00001124
Iteration 20/1000 | Loss: 0.00001121
Iteration 21/1000 | Loss: 0.00001116
Iteration 22/1000 | Loss: 0.00001101
Iteration 23/1000 | Loss: 0.00001099
Iteration 24/1000 | Loss: 0.00001098
Iteration 25/1000 | Loss: 0.00001097
Iteration 26/1000 | Loss: 0.00001097
Iteration 27/1000 | Loss: 0.00001096
Iteration 28/1000 | Loss: 0.00001095
Iteration 29/1000 | Loss: 0.00001094
Iteration 30/1000 | Loss: 0.00001093
Iteration 31/1000 | Loss: 0.00001092
Iteration 32/1000 | Loss: 0.00001092
Iteration 33/1000 | Loss: 0.00001092
Iteration 34/1000 | Loss: 0.00001091
Iteration 35/1000 | Loss: 0.00001091
Iteration 36/1000 | Loss: 0.00001091
Iteration 37/1000 | Loss: 0.00001090
Iteration 38/1000 | Loss: 0.00001090
Iteration 39/1000 | Loss: 0.00001090
Iteration 40/1000 | Loss: 0.00001090
Iteration 41/1000 | Loss: 0.00001089
Iteration 42/1000 | Loss: 0.00001088
Iteration 43/1000 | Loss: 0.00001088
Iteration 44/1000 | Loss: 0.00001087
Iteration 45/1000 | Loss: 0.00001087
Iteration 46/1000 | Loss: 0.00001087
Iteration 47/1000 | Loss: 0.00001086
Iteration 48/1000 | Loss: 0.00001086
Iteration 49/1000 | Loss: 0.00001085
Iteration 50/1000 | Loss: 0.00001085
Iteration 51/1000 | Loss: 0.00001084
Iteration 52/1000 | Loss: 0.00001083
Iteration 53/1000 | Loss: 0.00001083
Iteration 54/1000 | Loss: 0.00001083
Iteration 55/1000 | Loss: 0.00001082
Iteration 56/1000 | Loss: 0.00001082
Iteration 57/1000 | Loss: 0.00001082
Iteration 58/1000 | Loss: 0.00001082
Iteration 59/1000 | Loss: 0.00001082
Iteration 60/1000 | Loss: 0.00001081
Iteration 61/1000 | Loss: 0.00001081
Iteration 62/1000 | Loss: 0.00001081
Iteration 63/1000 | Loss: 0.00001080
Iteration 64/1000 | Loss: 0.00001080
Iteration 65/1000 | Loss: 0.00001080
Iteration 66/1000 | Loss: 0.00001080
Iteration 67/1000 | Loss: 0.00001080
Iteration 68/1000 | Loss: 0.00001080
Iteration 69/1000 | Loss: 0.00001080
Iteration 70/1000 | Loss: 0.00001079
Iteration 71/1000 | Loss: 0.00001079
Iteration 72/1000 | Loss: 0.00001078
Iteration 73/1000 | Loss: 0.00001078
Iteration 74/1000 | Loss: 0.00001078
Iteration 75/1000 | Loss: 0.00001078
Iteration 76/1000 | Loss: 0.00001078
Iteration 77/1000 | Loss: 0.00001077
Iteration 78/1000 | Loss: 0.00001076
Iteration 79/1000 | Loss: 0.00001076
Iteration 80/1000 | Loss: 0.00001075
Iteration 81/1000 | Loss: 0.00001074
Iteration 82/1000 | Loss: 0.00001074
Iteration 83/1000 | Loss: 0.00001073
Iteration 84/1000 | Loss: 0.00001072
Iteration 85/1000 | Loss: 0.00001072
Iteration 86/1000 | Loss: 0.00001072
Iteration 87/1000 | Loss: 0.00001072
Iteration 88/1000 | Loss: 0.00001072
Iteration 89/1000 | Loss: 0.00001072
Iteration 90/1000 | Loss: 0.00001072
Iteration 91/1000 | Loss: 0.00001072
Iteration 92/1000 | Loss: 0.00001071
Iteration 93/1000 | Loss: 0.00001071
Iteration 94/1000 | Loss: 0.00001071
Iteration 95/1000 | Loss: 0.00001070
Iteration 96/1000 | Loss: 0.00001070
Iteration 97/1000 | Loss: 0.00001069
Iteration 98/1000 | Loss: 0.00001069
Iteration 99/1000 | Loss: 0.00001068
Iteration 100/1000 | Loss: 0.00001068
Iteration 101/1000 | Loss: 0.00001068
Iteration 102/1000 | Loss: 0.00001068
Iteration 103/1000 | Loss: 0.00001068
Iteration 104/1000 | Loss: 0.00001068
Iteration 105/1000 | Loss: 0.00001068
Iteration 106/1000 | Loss: 0.00001068
Iteration 107/1000 | Loss: 0.00001068
Iteration 108/1000 | Loss: 0.00001068
Iteration 109/1000 | Loss: 0.00001068
Iteration 110/1000 | Loss: 0.00001067
Iteration 111/1000 | Loss: 0.00001067
Iteration 112/1000 | Loss: 0.00001067
Iteration 113/1000 | Loss: 0.00001067
Iteration 114/1000 | Loss: 0.00001067
Iteration 115/1000 | Loss: 0.00001067
Iteration 116/1000 | Loss: 0.00001066
Iteration 117/1000 | Loss: 0.00001065
Iteration 118/1000 | Loss: 0.00001065
Iteration 119/1000 | Loss: 0.00001064
Iteration 120/1000 | Loss: 0.00001063
Iteration 121/1000 | Loss: 0.00001063
Iteration 122/1000 | Loss: 0.00001063
Iteration 123/1000 | Loss: 0.00001063
Iteration 124/1000 | Loss: 0.00001063
Iteration 125/1000 | Loss: 0.00001063
Iteration 126/1000 | Loss: 0.00001062
Iteration 127/1000 | Loss: 0.00001062
Iteration 128/1000 | Loss: 0.00001061
Iteration 129/1000 | Loss: 0.00001061
Iteration 130/1000 | Loss: 0.00001060
Iteration 131/1000 | Loss: 0.00001060
Iteration 132/1000 | Loss: 0.00001060
Iteration 133/1000 | Loss: 0.00001059
Iteration 134/1000 | Loss: 0.00001059
Iteration 135/1000 | Loss: 0.00001058
Iteration 136/1000 | Loss: 0.00001058
Iteration 137/1000 | Loss: 0.00001058
Iteration 138/1000 | Loss: 0.00001057
Iteration 139/1000 | Loss: 0.00001057
Iteration 140/1000 | Loss: 0.00001057
Iteration 141/1000 | Loss: 0.00001057
Iteration 142/1000 | Loss: 0.00001057
Iteration 143/1000 | Loss: 0.00001057
Iteration 144/1000 | Loss: 0.00001057
Iteration 145/1000 | Loss: 0.00001056
Iteration 146/1000 | Loss: 0.00001056
Iteration 147/1000 | Loss: 0.00001056
Iteration 148/1000 | Loss: 0.00001056
Iteration 149/1000 | Loss: 0.00001056
Iteration 150/1000 | Loss: 0.00001056
Iteration 151/1000 | Loss: 0.00001055
Iteration 152/1000 | Loss: 0.00001055
Iteration 153/1000 | Loss: 0.00001055
Iteration 154/1000 | Loss: 0.00001055
Iteration 155/1000 | Loss: 0.00001055
Iteration 156/1000 | Loss: 0.00001054
Iteration 157/1000 | Loss: 0.00001054
Iteration 158/1000 | Loss: 0.00001054
Iteration 159/1000 | Loss: 0.00001054
Iteration 160/1000 | Loss: 0.00001054
Iteration 161/1000 | Loss: 0.00001054
Iteration 162/1000 | Loss: 0.00001054
Iteration 163/1000 | Loss: 0.00001054
Iteration 164/1000 | Loss: 0.00001054
Iteration 165/1000 | Loss: 0.00001054
Iteration 166/1000 | Loss: 0.00001054
Iteration 167/1000 | Loss: 0.00001053
Iteration 168/1000 | Loss: 0.00001053
Iteration 169/1000 | Loss: 0.00001053
Iteration 170/1000 | Loss: 0.00001053
Iteration 171/1000 | Loss: 0.00001053
Iteration 172/1000 | Loss: 0.00001053
Iteration 173/1000 | Loss: 0.00001053
Iteration 174/1000 | Loss: 0.00001053
Iteration 175/1000 | Loss: 0.00001052
Iteration 176/1000 | Loss: 0.00001052
Iteration 177/1000 | Loss: 0.00001052
Iteration 178/1000 | Loss: 0.00001052
Iteration 179/1000 | Loss: 0.00001052
Iteration 180/1000 | Loss: 0.00001052
Iteration 181/1000 | Loss: 0.00001052
Iteration 182/1000 | Loss: 0.00001052
Iteration 183/1000 | Loss: 0.00001052
Iteration 184/1000 | Loss: 0.00001052
Iteration 185/1000 | Loss: 0.00001052
Iteration 186/1000 | Loss: 0.00001052
Iteration 187/1000 | Loss: 0.00001052
Iteration 188/1000 | Loss: 0.00001052
Iteration 189/1000 | Loss: 0.00001052
Iteration 190/1000 | Loss: 0.00001052
Iteration 191/1000 | Loss: 0.00001052
Iteration 192/1000 | Loss: 0.00001052
Iteration 193/1000 | Loss: 0.00001052
Iteration 194/1000 | Loss: 0.00001051
Iteration 195/1000 | Loss: 0.00001051
Iteration 196/1000 | Loss: 0.00001051
Iteration 197/1000 | Loss: 0.00001051
Iteration 198/1000 | Loss: 0.00001051
Iteration 199/1000 | Loss: 0.00001051
Iteration 200/1000 | Loss: 0.00001051
Iteration 201/1000 | Loss: 0.00001050
Iteration 202/1000 | Loss: 0.00001050
Iteration 203/1000 | Loss: 0.00001050
Iteration 204/1000 | Loss: 0.00001050
Iteration 205/1000 | Loss: 0.00001050
Iteration 206/1000 | Loss: 0.00001050
Iteration 207/1000 | Loss: 0.00001050
Iteration 208/1000 | Loss: 0.00001050
Iteration 209/1000 | Loss: 0.00001050
Iteration 210/1000 | Loss: 0.00001050
Iteration 211/1000 | Loss: 0.00001049
Iteration 212/1000 | Loss: 0.00001049
Iteration 213/1000 | Loss: 0.00001049
Iteration 214/1000 | Loss: 0.00001049
Iteration 215/1000 | Loss: 0.00001049
Iteration 216/1000 | Loss: 0.00001049
Iteration 217/1000 | Loss: 0.00001049
Iteration 218/1000 | Loss: 0.00001049
Iteration 219/1000 | Loss: 0.00001049
Iteration 220/1000 | Loss: 0.00001049
Iteration 221/1000 | Loss: 0.00001049
Iteration 222/1000 | Loss: 0.00001049
Iteration 223/1000 | Loss: 0.00001049
Iteration 224/1000 | Loss: 0.00001049
Iteration 225/1000 | Loss: 0.00001049
Iteration 226/1000 | Loss: 0.00001049
Iteration 227/1000 | Loss: 0.00001048
Iteration 228/1000 | Loss: 0.00001048
Iteration 229/1000 | Loss: 0.00001048
Iteration 230/1000 | Loss: 0.00001048
Iteration 231/1000 | Loss: 0.00001048
Iteration 232/1000 | Loss: 0.00001048
Iteration 233/1000 | Loss: 0.00001048
Iteration 234/1000 | Loss: 0.00001048
Iteration 235/1000 | Loss: 0.00001048
Iteration 236/1000 | Loss: 0.00001047
Iteration 237/1000 | Loss: 0.00001047
Iteration 238/1000 | Loss: 0.00001047
Iteration 239/1000 | Loss: 0.00001047
Iteration 240/1000 | Loss: 0.00001047
Iteration 241/1000 | Loss: 0.00001047
Iteration 242/1000 | Loss: 0.00001047
Iteration 243/1000 | Loss: 0.00001047
Iteration 244/1000 | Loss: 0.00001046
Iteration 245/1000 | Loss: 0.00001046
Iteration 246/1000 | Loss: 0.00001046
Iteration 247/1000 | Loss: 0.00001046
Iteration 248/1000 | Loss: 0.00001046
Iteration 249/1000 | Loss: 0.00001046
Iteration 250/1000 | Loss: 0.00001046
Iteration 251/1000 | Loss: 0.00001046
Iteration 252/1000 | Loss: 0.00001046
Iteration 253/1000 | Loss: 0.00001046
Iteration 254/1000 | Loss: 0.00001046
Iteration 255/1000 | Loss: 0.00001046
Iteration 256/1000 | Loss: 0.00001046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [1.0461287274665665e-05, 1.0461287274665665e-05, 1.0461287274665665e-05, 1.0461287274665665e-05, 1.0461287274665665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0461287274665665e-05

Optimization complete. Final v2v error: 2.7454888820648193 mm

Highest mean error: 3.5767452716827393 mm for frame 25

Lowest mean error: 2.293424129486084 mm for frame 132

Saving results

Total time: 44.506332874298096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451182
Iteration 2/25 | Loss: 0.00080691
Iteration 3/25 | Loss: 0.00064745
Iteration 4/25 | Loss: 0.00061870
Iteration 5/25 | Loss: 0.00061126
Iteration 6/25 | Loss: 0.00060984
Iteration 7/25 | Loss: 0.00060968
Iteration 8/25 | Loss: 0.00060968
Iteration 9/25 | Loss: 0.00060968
Iteration 10/25 | Loss: 0.00060968
Iteration 11/25 | Loss: 0.00060968
Iteration 12/25 | Loss: 0.00060968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006096814176999032, 0.0006096814176999032, 0.0006096814176999032, 0.0006096814176999032, 0.0006096814176999032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006096814176999032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.81781864
Iteration 2/25 | Loss: 0.00025453
Iteration 3/25 | Loss: 0.00025452
Iteration 4/25 | Loss: 0.00025452
Iteration 5/25 | Loss: 0.00025451
Iteration 6/25 | Loss: 0.00025451
Iteration 7/25 | Loss: 0.00025451
Iteration 8/25 | Loss: 0.00025451
Iteration 9/25 | Loss: 0.00025451
Iteration 10/25 | Loss: 0.00025451
Iteration 11/25 | Loss: 0.00025451
Iteration 12/25 | Loss: 0.00025451
Iteration 13/25 | Loss: 0.00025451
Iteration 14/25 | Loss: 0.00025451
Iteration 15/25 | Loss: 0.00025451
Iteration 16/25 | Loss: 0.00025451
Iteration 17/25 | Loss: 0.00025451
Iteration 18/25 | Loss: 0.00025451
Iteration 19/25 | Loss: 0.00025451
Iteration 20/25 | Loss: 0.00025451
Iteration 21/25 | Loss: 0.00025451
Iteration 22/25 | Loss: 0.00025451
Iteration 23/25 | Loss: 0.00025451
Iteration 24/25 | Loss: 0.00025451
Iteration 25/25 | Loss: 0.00025451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025451
Iteration 2/1000 | Loss: 0.00002475
Iteration 3/1000 | Loss: 0.00001511
Iteration 4/1000 | Loss: 0.00001409
Iteration 5/1000 | Loss: 0.00001313
Iteration 6/1000 | Loss: 0.00001280
Iteration 7/1000 | Loss: 0.00001269
Iteration 8/1000 | Loss: 0.00001255
Iteration 9/1000 | Loss: 0.00001250
Iteration 10/1000 | Loss: 0.00001237
Iteration 11/1000 | Loss: 0.00001229
Iteration 12/1000 | Loss: 0.00001226
Iteration 13/1000 | Loss: 0.00001225
Iteration 14/1000 | Loss: 0.00001217
Iteration 15/1000 | Loss: 0.00001216
Iteration 16/1000 | Loss: 0.00001216
Iteration 17/1000 | Loss: 0.00001215
Iteration 18/1000 | Loss: 0.00001214
Iteration 19/1000 | Loss: 0.00001211
Iteration 20/1000 | Loss: 0.00001211
Iteration 21/1000 | Loss: 0.00001211
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001210
Iteration 24/1000 | Loss: 0.00001204
Iteration 25/1000 | Loss: 0.00001203
Iteration 26/1000 | Loss: 0.00001202
Iteration 27/1000 | Loss: 0.00001202
Iteration 28/1000 | Loss: 0.00001201
Iteration 29/1000 | Loss: 0.00001201
Iteration 30/1000 | Loss: 0.00001201
Iteration 31/1000 | Loss: 0.00001201
Iteration 32/1000 | Loss: 0.00001201
Iteration 33/1000 | Loss: 0.00001201
Iteration 34/1000 | Loss: 0.00001201
Iteration 35/1000 | Loss: 0.00001201
Iteration 36/1000 | Loss: 0.00001201
Iteration 37/1000 | Loss: 0.00001198
Iteration 38/1000 | Loss: 0.00001198
Iteration 39/1000 | Loss: 0.00001198
Iteration 40/1000 | Loss: 0.00001198
Iteration 41/1000 | Loss: 0.00001198
Iteration 42/1000 | Loss: 0.00001198
Iteration 43/1000 | Loss: 0.00001198
Iteration 44/1000 | Loss: 0.00001197
Iteration 45/1000 | Loss: 0.00001197
Iteration 46/1000 | Loss: 0.00001196
Iteration 47/1000 | Loss: 0.00001196
Iteration 48/1000 | Loss: 0.00001195
Iteration 49/1000 | Loss: 0.00001195
Iteration 50/1000 | Loss: 0.00001195
Iteration 51/1000 | Loss: 0.00001195
Iteration 52/1000 | Loss: 0.00001195
Iteration 53/1000 | Loss: 0.00001194
Iteration 54/1000 | Loss: 0.00001194
Iteration 55/1000 | Loss: 0.00001194
Iteration 56/1000 | Loss: 0.00001194
Iteration 57/1000 | Loss: 0.00001193
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001193
Iteration 61/1000 | Loss: 0.00001193
Iteration 62/1000 | Loss: 0.00001192
Iteration 63/1000 | Loss: 0.00001192
Iteration 64/1000 | Loss: 0.00001192
Iteration 65/1000 | Loss: 0.00001192
Iteration 66/1000 | Loss: 0.00001192
Iteration 67/1000 | Loss: 0.00001192
Iteration 68/1000 | Loss: 0.00001192
Iteration 69/1000 | Loss: 0.00001192
Iteration 70/1000 | Loss: 0.00001191
Iteration 71/1000 | Loss: 0.00001191
Iteration 72/1000 | Loss: 0.00001191
Iteration 73/1000 | Loss: 0.00001190
Iteration 74/1000 | Loss: 0.00001190
Iteration 75/1000 | Loss: 0.00001190
Iteration 76/1000 | Loss: 0.00001190
Iteration 77/1000 | Loss: 0.00001190
Iteration 78/1000 | Loss: 0.00001190
Iteration 79/1000 | Loss: 0.00001190
Iteration 80/1000 | Loss: 0.00001190
Iteration 81/1000 | Loss: 0.00001190
Iteration 82/1000 | Loss: 0.00001190
Iteration 83/1000 | Loss: 0.00001190
Iteration 84/1000 | Loss: 0.00001190
Iteration 85/1000 | Loss: 0.00001190
Iteration 86/1000 | Loss: 0.00001190
Iteration 87/1000 | Loss: 0.00001190
Iteration 88/1000 | Loss: 0.00001190
Iteration 89/1000 | Loss: 0.00001190
Iteration 90/1000 | Loss: 0.00001190
Iteration 91/1000 | Loss: 0.00001190
Iteration 92/1000 | Loss: 0.00001189
Iteration 93/1000 | Loss: 0.00001189
Iteration 94/1000 | Loss: 0.00001189
Iteration 95/1000 | Loss: 0.00001189
Iteration 96/1000 | Loss: 0.00001189
Iteration 97/1000 | Loss: 0.00001188
Iteration 98/1000 | Loss: 0.00001188
Iteration 99/1000 | Loss: 0.00001188
Iteration 100/1000 | Loss: 0.00001188
Iteration 101/1000 | Loss: 0.00001187
Iteration 102/1000 | Loss: 0.00001187
Iteration 103/1000 | Loss: 0.00001187
Iteration 104/1000 | Loss: 0.00001187
Iteration 105/1000 | Loss: 0.00001187
Iteration 106/1000 | Loss: 0.00001187
Iteration 107/1000 | Loss: 0.00001187
Iteration 108/1000 | Loss: 0.00001187
Iteration 109/1000 | Loss: 0.00001187
Iteration 110/1000 | Loss: 0.00001187
Iteration 111/1000 | Loss: 0.00001186
Iteration 112/1000 | Loss: 0.00001186
Iteration 113/1000 | Loss: 0.00001186
Iteration 114/1000 | Loss: 0.00001185
Iteration 115/1000 | Loss: 0.00001185
Iteration 116/1000 | Loss: 0.00001185
Iteration 117/1000 | Loss: 0.00001185
Iteration 118/1000 | Loss: 0.00001185
Iteration 119/1000 | Loss: 0.00001185
Iteration 120/1000 | Loss: 0.00001185
Iteration 121/1000 | Loss: 0.00001185
Iteration 122/1000 | Loss: 0.00001185
Iteration 123/1000 | Loss: 0.00001185
Iteration 124/1000 | Loss: 0.00001185
Iteration 125/1000 | Loss: 0.00001185
Iteration 126/1000 | Loss: 0.00001184
Iteration 127/1000 | Loss: 0.00001184
Iteration 128/1000 | Loss: 0.00001184
Iteration 129/1000 | Loss: 0.00001183
Iteration 130/1000 | Loss: 0.00001183
Iteration 131/1000 | Loss: 0.00001183
Iteration 132/1000 | Loss: 0.00001182
Iteration 133/1000 | Loss: 0.00001182
Iteration 134/1000 | Loss: 0.00001182
Iteration 135/1000 | Loss: 0.00001182
Iteration 136/1000 | Loss: 0.00001182
Iteration 137/1000 | Loss: 0.00001182
Iteration 138/1000 | Loss: 0.00001182
Iteration 139/1000 | Loss: 0.00001181
Iteration 140/1000 | Loss: 0.00001181
Iteration 141/1000 | Loss: 0.00001181
Iteration 142/1000 | Loss: 0.00001180
Iteration 143/1000 | Loss: 0.00001180
Iteration 144/1000 | Loss: 0.00001180
Iteration 145/1000 | Loss: 0.00001180
Iteration 146/1000 | Loss: 0.00001180
Iteration 147/1000 | Loss: 0.00001179
Iteration 148/1000 | Loss: 0.00001179
Iteration 149/1000 | Loss: 0.00001179
Iteration 150/1000 | Loss: 0.00001179
Iteration 151/1000 | Loss: 0.00001179
Iteration 152/1000 | Loss: 0.00001179
Iteration 153/1000 | Loss: 0.00001179
Iteration 154/1000 | Loss: 0.00001179
Iteration 155/1000 | Loss: 0.00001179
Iteration 156/1000 | Loss: 0.00001179
Iteration 157/1000 | Loss: 0.00001179
Iteration 158/1000 | Loss: 0.00001179
Iteration 159/1000 | Loss: 0.00001179
Iteration 160/1000 | Loss: 0.00001179
Iteration 161/1000 | Loss: 0.00001179
Iteration 162/1000 | Loss: 0.00001179
Iteration 163/1000 | Loss: 0.00001179
Iteration 164/1000 | Loss: 0.00001179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.1792718396463897e-05, 1.1792718396463897e-05, 1.1792718396463897e-05, 1.1792718396463897e-05, 1.1792718396463897e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1792718396463897e-05

Optimization complete. Final v2v error: 2.923691511154175 mm

Highest mean error: 3.270132064819336 mm for frame 133

Lowest mean error: 2.6850154399871826 mm for frame 155

Saving results

Total time: 38.092660665512085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00754046
Iteration 2/25 | Loss: 0.00088341
Iteration 3/25 | Loss: 0.00069942
Iteration 4/25 | Loss: 0.00065205
Iteration 5/25 | Loss: 0.00064472
Iteration 6/25 | Loss: 0.00064375
Iteration 7/25 | Loss: 0.00064375
Iteration 8/25 | Loss: 0.00064375
Iteration 9/25 | Loss: 0.00064375
Iteration 10/25 | Loss: 0.00064375
Iteration 11/25 | Loss: 0.00064375
Iteration 12/25 | Loss: 0.00064375
Iteration 13/25 | Loss: 0.00064375
Iteration 14/25 | Loss: 0.00064375
Iteration 15/25 | Loss: 0.00064375
Iteration 16/25 | Loss: 0.00064375
Iteration 17/25 | Loss: 0.00064375
Iteration 18/25 | Loss: 0.00064375
Iteration 19/25 | Loss: 0.00064375
Iteration 20/25 | Loss: 0.00064375
Iteration 21/25 | Loss: 0.00064375
Iteration 22/25 | Loss: 0.00064375
Iteration 23/25 | Loss: 0.00064375
Iteration 24/25 | Loss: 0.00064375
Iteration 25/25 | Loss: 0.00064375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006437537376768887, 0.0006437537376768887, 0.0006437537376768887, 0.0006437537376768887, 0.0006437537376768887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006437537376768887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47124124
Iteration 2/25 | Loss: 0.00017398
Iteration 3/25 | Loss: 0.00017391
Iteration 4/25 | Loss: 0.00017391
Iteration 5/25 | Loss: 0.00017391
Iteration 6/25 | Loss: 0.00017391
Iteration 7/25 | Loss: 0.00017391
Iteration 8/25 | Loss: 0.00017391
Iteration 9/25 | Loss: 0.00017391
Iteration 10/25 | Loss: 0.00017391
Iteration 11/25 | Loss: 0.00017391
Iteration 12/25 | Loss: 0.00017391
Iteration 13/25 | Loss: 0.00017391
Iteration 14/25 | Loss: 0.00017391
Iteration 15/25 | Loss: 0.00017391
Iteration 16/25 | Loss: 0.00017391
Iteration 17/25 | Loss: 0.00017391
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00017391055007465184, 0.00017391055007465184, 0.00017391055007465184, 0.00017391055007465184, 0.00017391055007465184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00017391055007465184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017391
Iteration 2/1000 | Loss: 0.00002425
Iteration 3/1000 | Loss: 0.00001798
Iteration 4/1000 | Loss: 0.00001646
Iteration 5/1000 | Loss: 0.00001545
Iteration 6/1000 | Loss: 0.00001462
Iteration 7/1000 | Loss: 0.00001407
Iteration 8/1000 | Loss: 0.00001380
Iteration 9/1000 | Loss: 0.00001349
Iteration 10/1000 | Loss: 0.00001349
Iteration 11/1000 | Loss: 0.00001348
Iteration 12/1000 | Loss: 0.00001335
Iteration 13/1000 | Loss: 0.00001328
Iteration 14/1000 | Loss: 0.00001327
Iteration 15/1000 | Loss: 0.00001326
Iteration 16/1000 | Loss: 0.00001324
Iteration 17/1000 | Loss: 0.00001323
Iteration 18/1000 | Loss: 0.00001317
Iteration 19/1000 | Loss: 0.00001317
Iteration 20/1000 | Loss: 0.00001316
Iteration 21/1000 | Loss: 0.00001315
Iteration 22/1000 | Loss: 0.00001314
Iteration 23/1000 | Loss: 0.00001309
Iteration 24/1000 | Loss: 0.00001303
Iteration 25/1000 | Loss: 0.00001303
Iteration 26/1000 | Loss: 0.00001303
Iteration 27/1000 | Loss: 0.00001303
Iteration 28/1000 | Loss: 0.00001300
Iteration 29/1000 | Loss: 0.00001300
Iteration 30/1000 | Loss: 0.00001299
Iteration 31/1000 | Loss: 0.00001299
Iteration 32/1000 | Loss: 0.00001299
Iteration 33/1000 | Loss: 0.00001299
Iteration 34/1000 | Loss: 0.00001299
Iteration 35/1000 | Loss: 0.00001299
Iteration 36/1000 | Loss: 0.00001299
Iteration 37/1000 | Loss: 0.00001299
Iteration 38/1000 | Loss: 0.00001299
Iteration 39/1000 | Loss: 0.00001299
Iteration 40/1000 | Loss: 0.00001299
Iteration 41/1000 | Loss: 0.00001298
Iteration 42/1000 | Loss: 0.00001297
Iteration 43/1000 | Loss: 0.00001297
Iteration 44/1000 | Loss: 0.00001296
Iteration 45/1000 | Loss: 0.00001296
Iteration 46/1000 | Loss: 0.00001296
Iteration 47/1000 | Loss: 0.00001296
Iteration 48/1000 | Loss: 0.00001295
Iteration 49/1000 | Loss: 0.00001295
Iteration 50/1000 | Loss: 0.00001295
Iteration 51/1000 | Loss: 0.00001294
Iteration 52/1000 | Loss: 0.00001294
Iteration 53/1000 | Loss: 0.00001293
Iteration 54/1000 | Loss: 0.00001293
Iteration 55/1000 | Loss: 0.00001293
Iteration 56/1000 | Loss: 0.00001292
Iteration 57/1000 | Loss: 0.00001292
Iteration 58/1000 | Loss: 0.00001292
Iteration 59/1000 | Loss: 0.00001292
Iteration 60/1000 | Loss: 0.00001292
Iteration 61/1000 | Loss: 0.00001292
Iteration 62/1000 | Loss: 0.00001292
Iteration 63/1000 | Loss: 0.00001292
Iteration 64/1000 | Loss: 0.00001292
Iteration 65/1000 | Loss: 0.00001292
Iteration 66/1000 | Loss: 0.00001292
Iteration 67/1000 | Loss: 0.00001291
Iteration 68/1000 | Loss: 0.00001291
Iteration 69/1000 | Loss: 0.00001290
Iteration 70/1000 | Loss: 0.00001290
Iteration 71/1000 | Loss: 0.00001290
Iteration 72/1000 | Loss: 0.00001290
Iteration 73/1000 | Loss: 0.00001289
Iteration 74/1000 | Loss: 0.00001289
Iteration 75/1000 | Loss: 0.00001289
Iteration 76/1000 | Loss: 0.00001289
Iteration 77/1000 | Loss: 0.00001289
Iteration 78/1000 | Loss: 0.00001289
Iteration 79/1000 | Loss: 0.00001289
Iteration 80/1000 | Loss: 0.00001289
Iteration 81/1000 | Loss: 0.00001289
Iteration 82/1000 | Loss: 0.00001289
Iteration 83/1000 | Loss: 0.00001288
Iteration 84/1000 | Loss: 0.00001288
Iteration 85/1000 | Loss: 0.00001288
Iteration 86/1000 | Loss: 0.00001288
Iteration 87/1000 | Loss: 0.00001288
Iteration 88/1000 | Loss: 0.00001288
Iteration 89/1000 | Loss: 0.00001288
Iteration 90/1000 | Loss: 0.00001287
Iteration 91/1000 | Loss: 0.00001287
Iteration 92/1000 | Loss: 0.00001287
Iteration 93/1000 | Loss: 0.00001287
Iteration 94/1000 | Loss: 0.00001287
Iteration 95/1000 | Loss: 0.00001287
Iteration 96/1000 | Loss: 0.00001287
Iteration 97/1000 | Loss: 0.00001287
Iteration 98/1000 | Loss: 0.00001287
Iteration 99/1000 | Loss: 0.00001287
Iteration 100/1000 | Loss: 0.00001287
Iteration 101/1000 | Loss: 0.00001287
Iteration 102/1000 | Loss: 0.00001287
Iteration 103/1000 | Loss: 0.00001287
Iteration 104/1000 | Loss: 0.00001287
Iteration 105/1000 | Loss: 0.00001286
Iteration 106/1000 | Loss: 0.00001286
Iteration 107/1000 | Loss: 0.00001286
Iteration 108/1000 | Loss: 0.00001286
Iteration 109/1000 | Loss: 0.00001286
Iteration 110/1000 | Loss: 0.00001286
Iteration 111/1000 | Loss: 0.00001285
Iteration 112/1000 | Loss: 0.00001285
Iteration 113/1000 | Loss: 0.00001285
Iteration 114/1000 | Loss: 0.00001285
Iteration 115/1000 | Loss: 0.00001285
Iteration 116/1000 | Loss: 0.00001285
Iteration 117/1000 | Loss: 0.00001285
Iteration 118/1000 | Loss: 0.00001285
Iteration 119/1000 | Loss: 0.00001285
Iteration 120/1000 | Loss: 0.00001285
Iteration 121/1000 | Loss: 0.00001285
Iteration 122/1000 | Loss: 0.00001285
Iteration 123/1000 | Loss: 0.00001284
Iteration 124/1000 | Loss: 0.00001284
Iteration 125/1000 | Loss: 0.00001284
Iteration 126/1000 | Loss: 0.00001284
Iteration 127/1000 | Loss: 0.00001284
Iteration 128/1000 | Loss: 0.00001284
Iteration 129/1000 | Loss: 0.00001284
Iteration 130/1000 | Loss: 0.00001284
Iteration 131/1000 | Loss: 0.00001283
Iteration 132/1000 | Loss: 0.00001283
Iteration 133/1000 | Loss: 0.00001283
Iteration 134/1000 | Loss: 0.00001283
Iteration 135/1000 | Loss: 0.00001283
Iteration 136/1000 | Loss: 0.00001283
Iteration 137/1000 | Loss: 0.00001283
Iteration 138/1000 | Loss: 0.00001283
Iteration 139/1000 | Loss: 0.00001283
Iteration 140/1000 | Loss: 0.00001283
Iteration 141/1000 | Loss: 0.00001283
Iteration 142/1000 | Loss: 0.00001283
Iteration 143/1000 | Loss: 0.00001282
Iteration 144/1000 | Loss: 0.00001282
Iteration 145/1000 | Loss: 0.00001282
Iteration 146/1000 | Loss: 0.00001282
Iteration 147/1000 | Loss: 0.00001282
Iteration 148/1000 | Loss: 0.00001282
Iteration 149/1000 | Loss: 0.00001282
Iteration 150/1000 | Loss: 0.00001282
Iteration 151/1000 | Loss: 0.00001282
Iteration 152/1000 | Loss: 0.00001282
Iteration 153/1000 | Loss: 0.00001282
Iteration 154/1000 | Loss: 0.00001282
Iteration 155/1000 | Loss: 0.00001282
Iteration 156/1000 | Loss: 0.00001282
Iteration 157/1000 | Loss: 0.00001282
Iteration 158/1000 | Loss: 0.00001282
Iteration 159/1000 | Loss: 0.00001282
Iteration 160/1000 | Loss: 0.00001282
Iteration 161/1000 | Loss: 0.00001282
Iteration 162/1000 | Loss: 0.00001282
Iteration 163/1000 | Loss: 0.00001282
Iteration 164/1000 | Loss: 0.00001282
Iteration 165/1000 | Loss: 0.00001282
Iteration 166/1000 | Loss: 0.00001282
Iteration 167/1000 | Loss: 0.00001282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.2818737559427973e-05, 1.2818737559427973e-05, 1.2818737559427973e-05, 1.2818737559427973e-05, 1.2818737559427973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2818737559427973e-05

Optimization complete. Final v2v error: 3.0945069789886475 mm

Highest mean error: 3.3097727298736572 mm for frame 72

Lowest mean error: 2.887651205062866 mm for frame 138

Saving results

Total time: 42.21973180770874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450488
Iteration 2/25 | Loss: 0.00084867
Iteration 3/25 | Loss: 0.00069115
Iteration 4/25 | Loss: 0.00065223
Iteration 5/25 | Loss: 0.00064448
Iteration 6/25 | Loss: 0.00064328
Iteration 7/25 | Loss: 0.00064328
Iteration 8/25 | Loss: 0.00064328
Iteration 9/25 | Loss: 0.00064328
Iteration 10/25 | Loss: 0.00064328
Iteration 11/25 | Loss: 0.00064328
Iteration 12/25 | Loss: 0.00064328
Iteration 13/25 | Loss: 0.00064328
Iteration 14/25 | Loss: 0.00064328
Iteration 15/25 | Loss: 0.00064328
Iteration 16/25 | Loss: 0.00064328
Iteration 17/25 | Loss: 0.00064328
Iteration 18/25 | Loss: 0.00064328
Iteration 19/25 | Loss: 0.00064328
Iteration 20/25 | Loss: 0.00064328
Iteration 21/25 | Loss: 0.00064328
Iteration 22/25 | Loss: 0.00064328
Iteration 23/25 | Loss: 0.00064328
Iteration 24/25 | Loss: 0.00064328
Iteration 25/25 | Loss: 0.00064328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.28804350
Iteration 2/25 | Loss: 0.00030971
Iteration 3/25 | Loss: 0.00030967
Iteration 4/25 | Loss: 0.00030967
Iteration 5/25 | Loss: 0.00030967
Iteration 6/25 | Loss: 0.00030967
Iteration 7/25 | Loss: 0.00030967
Iteration 8/25 | Loss: 0.00030967
Iteration 9/25 | Loss: 0.00030967
Iteration 10/25 | Loss: 0.00030967
Iteration 11/25 | Loss: 0.00030967
Iteration 12/25 | Loss: 0.00030967
Iteration 13/25 | Loss: 0.00030967
Iteration 14/25 | Loss: 0.00030967
Iteration 15/25 | Loss: 0.00030967
Iteration 16/25 | Loss: 0.00030967
Iteration 17/25 | Loss: 0.00030967
Iteration 18/25 | Loss: 0.00030967
Iteration 19/25 | Loss: 0.00030967
Iteration 20/25 | Loss: 0.00030967
Iteration 21/25 | Loss: 0.00030967
Iteration 22/25 | Loss: 0.00030967
Iteration 23/25 | Loss: 0.00030967
Iteration 24/25 | Loss: 0.00030967
Iteration 25/25 | Loss: 0.00030967

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030967
Iteration 2/1000 | Loss: 0.00002649
Iteration 3/1000 | Loss: 0.00001942
Iteration 4/1000 | Loss: 0.00001788
Iteration 5/1000 | Loss: 0.00001689
Iteration 6/1000 | Loss: 0.00001623
Iteration 7/1000 | Loss: 0.00001578
Iteration 8/1000 | Loss: 0.00001546
Iteration 9/1000 | Loss: 0.00001540
Iteration 10/1000 | Loss: 0.00001520
Iteration 11/1000 | Loss: 0.00001519
Iteration 12/1000 | Loss: 0.00001517
Iteration 13/1000 | Loss: 0.00001513
Iteration 14/1000 | Loss: 0.00001512
Iteration 15/1000 | Loss: 0.00001511
Iteration 16/1000 | Loss: 0.00001510
Iteration 17/1000 | Loss: 0.00001510
Iteration 18/1000 | Loss: 0.00001503
Iteration 19/1000 | Loss: 0.00001500
Iteration 20/1000 | Loss: 0.00001500
Iteration 21/1000 | Loss: 0.00001499
Iteration 22/1000 | Loss: 0.00001499
Iteration 23/1000 | Loss: 0.00001498
Iteration 24/1000 | Loss: 0.00001498
Iteration 25/1000 | Loss: 0.00001497
Iteration 26/1000 | Loss: 0.00001496
Iteration 27/1000 | Loss: 0.00001496
Iteration 28/1000 | Loss: 0.00001495
Iteration 29/1000 | Loss: 0.00001493
Iteration 30/1000 | Loss: 0.00001493
Iteration 31/1000 | Loss: 0.00001492
Iteration 32/1000 | Loss: 0.00001492
Iteration 33/1000 | Loss: 0.00001492
Iteration 34/1000 | Loss: 0.00001491
Iteration 35/1000 | Loss: 0.00001491
Iteration 36/1000 | Loss: 0.00001491
Iteration 37/1000 | Loss: 0.00001490
Iteration 38/1000 | Loss: 0.00001490
Iteration 39/1000 | Loss: 0.00001490
Iteration 40/1000 | Loss: 0.00001490
Iteration 41/1000 | Loss: 0.00001489
Iteration 42/1000 | Loss: 0.00001489
Iteration 43/1000 | Loss: 0.00001489
Iteration 44/1000 | Loss: 0.00001489
Iteration 45/1000 | Loss: 0.00001489
Iteration 46/1000 | Loss: 0.00001488
Iteration 47/1000 | Loss: 0.00001488
Iteration 48/1000 | Loss: 0.00001488
Iteration 49/1000 | Loss: 0.00001487
Iteration 50/1000 | Loss: 0.00001487
Iteration 51/1000 | Loss: 0.00001487
Iteration 52/1000 | Loss: 0.00001487
Iteration 53/1000 | Loss: 0.00001486
Iteration 54/1000 | Loss: 0.00001486
Iteration 55/1000 | Loss: 0.00001486
Iteration 56/1000 | Loss: 0.00001486
Iteration 57/1000 | Loss: 0.00001486
Iteration 58/1000 | Loss: 0.00001486
Iteration 59/1000 | Loss: 0.00001485
Iteration 60/1000 | Loss: 0.00001485
Iteration 61/1000 | Loss: 0.00001485
Iteration 62/1000 | Loss: 0.00001485
Iteration 63/1000 | Loss: 0.00001484
Iteration 64/1000 | Loss: 0.00001483
Iteration 65/1000 | Loss: 0.00001483
Iteration 66/1000 | Loss: 0.00001483
Iteration 67/1000 | Loss: 0.00001483
Iteration 68/1000 | Loss: 0.00001483
Iteration 69/1000 | Loss: 0.00001483
Iteration 70/1000 | Loss: 0.00001483
Iteration 71/1000 | Loss: 0.00001483
Iteration 72/1000 | Loss: 0.00001483
Iteration 73/1000 | Loss: 0.00001483
Iteration 74/1000 | Loss: 0.00001482
Iteration 75/1000 | Loss: 0.00001482
Iteration 76/1000 | Loss: 0.00001482
Iteration 77/1000 | Loss: 0.00001482
Iteration 78/1000 | Loss: 0.00001482
Iteration 79/1000 | Loss: 0.00001482
Iteration 80/1000 | Loss: 0.00001482
Iteration 81/1000 | Loss: 0.00001481
Iteration 82/1000 | Loss: 0.00001481
Iteration 83/1000 | Loss: 0.00001481
Iteration 84/1000 | Loss: 0.00001481
Iteration 85/1000 | Loss: 0.00001481
Iteration 86/1000 | Loss: 0.00001480
Iteration 87/1000 | Loss: 0.00001480
Iteration 88/1000 | Loss: 0.00001480
Iteration 89/1000 | Loss: 0.00001480
Iteration 90/1000 | Loss: 0.00001480
Iteration 91/1000 | Loss: 0.00001480
Iteration 92/1000 | Loss: 0.00001480
Iteration 93/1000 | Loss: 0.00001480
Iteration 94/1000 | Loss: 0.00001480
Iteration 95/1000 | Loss: 0.00001480
Iteration 96/1000 | Loss: 0.00001480
Iteration 97/1000 | Loss: 0.00001480
Iteration 98/1000 | Loss: 0.00001480
Iteration 99/1000 | Loss: 0.00001480
Iteration 100/1000 | Loss: 0.00001480
Iteration 101/1000 | Loss: 0.00001480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.4797537005506456e-05, 1.4797537005506456e-05, 1.4797537005506456e-05, 1.4797537005506456e-05, 1.4797537005506456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4797537005506456e-05

Optimization complete. Final v2v error: 3.2672793865203857 mm

Highest mean error: 3.6294093132019043 mm for frame 83

Lowest mean error: 2.861664295196533 mm for frame 13

Saving results

Total time: 36.110652923583984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435430
Iteration 2/25 | Loss: 0.00083581
Iteration 3/25 | Loss: 0.00064957
Iteration 4/25 | Loss: 0.00062830
Iteration 5/25 | Loss: 0.00062399
Iteration 6/25 | Loss: 0.00062296
Iteration 7/25 | Loss: 0.00062272
Iteration 8/25 | Loss: 0.00062272
Iteration 9/25 | Loss: 0.00062272
Iteration 10/25 | Loss: 0.00062272
Iteration 11/25 | Loss: 0.00062272
Iteration 12/25 | Loss: 0.00062272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006227180710993707, 0.0006227180710993707, 0.0006227180710993707, 0.0006227180710993707, 0.0006227180710993707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006227180710993707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.64976168
Iteration 2/25 | Loss: 0.00026780
Iteration 3/25 | Loss: 0.00026778
Iteration 4/25 | Loss: 0.00026778
Iteration 5/25 | Loss: 0.00026778
Iteration 6/25 | Loss: 0.00026778
Iteration 7/25 | Loss: 0.00026778
Iteration 8/25 | Loss: 0.00026778
Iteration 9/25 | Loss: 0.00026778
Iteration 10/25 | Loss: 0.00026778
Iteration 11/25 | Loss: 0.00026778
Iteration 12/25 | Loss: 0.00026778
Iteration 13/25 | Loss: 0.00026778
Iteration 14/25 | Loss: 0.00026778
Iteration 15/25 | Loss: 0.00026778
Iteration 16/25 | Loss: 0.00026778
Iteration 17/25 | Loss: 0.00026778
Iteration 18/25 | Loss: 0.00026778
Iteration 19/25 | Loss: 0.00026778
Iteration 20/25 | Loss: 0.00026778
Iteration 21/25 | Loss: 0.00026778
Iteration 22/25 | Loss: 0.00026778
Iteration 23/25 | Loss: 0.00026778
Iteration 24/25 | Loss: 0.00026778
Iteration 25/25 | Loss: 0.00026778

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026778
Iteration 2/1000 | Loss: 0.00002589
Iteration 3/1000 | Loss: 0.00001667
Iteration 4/1000 | Loss: 0.00001572
Iteration 5/1000 | Loss: 0.00001492
Iteration 6/1000 | Loss: 0.00001454
Iteration 7/1000 | Loss: 0.00001418
Iteration 8/1000 | Loss: 0.00001399
Iteration 9/1000 | Loss: 0.00001398
Iteration 10/1000 | Loss: 0.00001390
Iteration 11/1000 | Loss: 0.00001381
Iteration 12/1000 | Loss: 0.00001380
Iteration 13/1000 | Loss: 0.00001380
Iteration 14/1000 | Loss: 0.00001378
Iteration 15/1000 | Loss: 0.00001376
Iteration 16/1000 | Loss: 0.00001375
Iteration 17/1000 | Loss: 0.00001375
Iteration 18/1000 | Loss: 0.00001375
Iteration 19/1000 | Loss: 0.00001375
Iteration 20/1000 | Loss: 0.00001374
Iteration 21/1000 | Loss: 0.00001374
Iteration 22/1000 | Loss: 0.00001373
Iteration 23/1000 | Loss: 0.00001372
Iteration 24/1000 | Loss: 0.00001369
Iteration 25/1000 | Loss: 0.00001365
Iteration 26/1000 | Loss: 0.00001362
Iteration 27/1000 | Loss: 0.00001361
Iteration 28/1000 | Loss: 0.00001361
Iteration 29/1000 | Loss: 0.00001361
Iteration 30/1000 | Loss: 0.00001360
Iteration 31/1000 | Loss: 0.00001357
Iteration 32/1000 | Loss: 0.00001357
Iteration 33/1000 | Loss: 0.00001356
Iteration 34/1000 | Loss: 0.00001356
Iteration 35/1000 | Loss: 0.00001355
Iteration 36/1000 | Loss: 0.00001354
Iteration 37/1000 | Loss: 0.00001354
Iteration 38/1000 | Loss: 0.00001354
Iteration 39/1000 | Loss: 0.00001354
Iteration 40/1000 | Loss: 0.00001354
Iteration 41/1000 | Loss: 0.00001354
Iteration 42/1000 | Loss: 0.00001354
Iteration 43/1000 | Loss: 0.00001353
Iteration 44/1000 | Loss: 0.00001353
Iteration 45/1000 | Loss: 0.00001353
Iteration 46/1000 | Loss: 0.00001353
Iteration 47/1000 | Loss: 0.00001353
Iteration 48/1000 | Loss: 0.00001353
Iteration 49/1000 | Loss: 0.00001352
Iteration 50/1000 | Loss: 0.00001352
Iteration 51/1000 | Loss: 0.00001352
Iteration 52/1000 | Loss: 0.00001352
Iteration 53/1000 | Loss: 0.00001352
Iteration 54/1000 | Loss: 0.00001351
Iteration 55/1000 | Loss: 0.00001351
Iteration 56/1000 | Loss: 0.00001351
Iteration 57/1000 | Loss: 0.00001350
Iteration 58/1000 | Loss: 0.00001350
Iteration 59/1000 | Loss: 0.00001350
Iteration 60/1000 | Loss: 0.00001349
Iteration 61/1000 | Loss: 0.00001349
Iteration 62/1000 | Loss: 0.00001349
Iteration 63/1000 | Loss: 0.00001348
Iteration 64/1000 | Loss: 0.00001348
Iteration 65/1000 | Loss: 0.00001348
Iteration 66/1000 | Loss: 0.00001348
Iteration 67/1000 | Loss: 0.00001347
Iteration 68/1000 | Loss: 0.00001347
Iteration 69/1000 | Loss: 0.00001347
Iteration 70/1000 | Loss: 0.00001347
Iteration 71/1000 | Loss: 0.00001346
Iteration 72/1000 | Loss: 0.00001346
Iteration 73/1000 | Loss: 0.00001346
Iteration 74/1000 | Loss: 0.00001345
Iteration 75/1000 | Loss: 0.00001345
Iteration 76/1000 | Loss: 0.00001345
Iteration 77/1000 | Loss: 0.00001344
Iteration 78/1000 | Loss: 0.00001344
Iteration 79/1000 | Loss: 0.00001344
Iteration 80/1000 | Loss: 0.00001344
Iteration 81/1000 | Loss: 0.00001344
Iteration 82/1000 | Loss: 0.00001344
Iteration 83/1000 | Loss: 0.00001344
Iteration 84/1000 | Loss: 0.00001343
Iteration 85/1000 | Loss: 0.00001343
Iteration 86/1000 | Loss: 0.00001343
Iteration 87/1000 | Loss: 0.00001343
Iteration 88/1000 | Loss: 0.00001343
Iteration 89/1000 | Loss: 0.00001343
Iteration 90/1000 | Loss: 0.00001342
Iteration 91/1000 | Loss: 0.00001342
Iteration 92/1000 | Loss: 0.00001342
Iteration 93/1000 | Loss: 0.00001342
Iteration 94/1000 | Loss: 0.00001341
Iteration 95/1000 | Loss: 0.00001341
Iteration 96/1000 | Loss: 0.00001341
Iteration 97/1000 | Loss: 0.00001340
Iteration 98/1000 | Loss: 0.00001340
Iteration 99/1000 | Loss: 0.00001340
Iteration 100/1000 | Loss: 0.00001340
Iteration 101/1000 | Loss: 0.00001340
Iteration 102/1000 | Loss: 0.00001339
Iteration 103/1000 | Loss: 0.00001339
Iteration 104/1000 | Loss: 0.00001339
Iteration 105/1000 | Loss: 0.00001339
Iteration 106/1000 | Loss: 0.00001338
Iteration 107/1000 | Loss: 0.00001338
Iteration 108/1000 | Loss: 0.00001338
Iteration 109/1000 | Loss: 0.00001338
Iteration 110/1000 | Loss: 0.00001338
Iteration 111/1000 | Loss: 0.00001338
Iteration 112/1000 | Loss: 0.00001338
Iteration 113/1000 | Loss: 0.00001338
Iteration 114/1000 | Loss: 0.00001338
Iteration 115/1000 | Loss: 0.00001337
Iteration 116/1000 | Loss: 0.00001337
Iteration 117/1000 | Loss: 0.00001337
Iteration 118/1000 | Loss: 0.00001337
Iteration 119/1000 | Loss: 0.00001337
Iteration 120/1000 | Loss: 0.00001337
Iteration 121/1000 | Loss: 0.00001337
Iteration 122/1000 | Loss: 0.00001337
Iteration 123/1000 | Loss: 0.00001337
Iteration 124/1000 | Loss: 0.00001337
Iteration 125/1000 | Loss: 0.00001337
Iteration 126/1000 | Loss: 0.00001337
Iteration 127/1000 | Loss: 0.00001337
Iteration 128/1000 | Loss: 0.00001337
Iteration 129/1000 | Loss: 0.00001337
Iteration 130/1000 | Loss: 0.00001337
Iteration 131/1000 | Loss: 0.00001337
Iteration 132/1000 | Loss: 0.00001337
Iteration 133/1000 | Loss: 0.00001337
Iteration 134/1000 | Loss: 0.00001337
Iteration 135/1000 | Loss: 0.00001337
Iteration 136/1000 | Loss: 0.00001337
Iteration 137/1000 | Loss: 0.00001337
Iteration 138/1000 | Loss: 0.00001336
Iteration 139/1000 | Loss: 0.00001336
Iteration 140/1000 | Loss: 0.00001336
Iteration 141/1000 | Loss: 0.00001336
Iteration 142/1000 | Loss: 0.00001336
Iteration 143/1000 | Loss: 0.00001336
Iteration 144/1000 | Loss: 0.00001336
Iteration 145/1000 | Loss: 0.00001336
Iteration 146/1000 | Loss: 0.00001336
Iteration 147/1000 | Loss: 0.00001336
Iteration 148/1000 | Loss: 0.00001336
Iteration 149/1000 | Loss: 0.00001336
Iteration 150/1000 | Loss: 0.00001336
Iteration 151/1000 | Loss: 0.00001336
Iteration 152/1000 | Loss: 0.00001336
Iteration 153/1000 | Loss: 0.00001336
Iteration 154/1000 | Loss: 0.00001335
Iteration 155/1000 | Loss: 0.00001335
Iteration 156/1000 | Loss: 0.00001335
Iteration 157/1000 | Loss: 0.00001335
Iteration 158/1000 | Loss: 0.00001335
Iteration 159/1000 | Loss: 0.00001335
Iteration 160/1000 | Loss: 0.00001335
Iteration 161/1000 | Loss: 0.00001335
Iteration 162/1000 | Loss: 0.00001335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.3353048416320235e-05, 1.3353048416320235e-05, 1.3353048416320235e-05, 1.3353048416320235e-05, 1.3353048416320235e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3353048416320235e-05

Optimization complete. Final v2v error: 3.086899757385254 mm

Highest mean error: 3.787977933883667 mm for frame 53

Lowest mean error: 2.7285826206207275 mm for frame 37

Saving results

Total time: 36.75125980377197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075185
Iteration 2/25 | Loss: 0.00216411
Iteration 3/25 | Loss: 0.00124426
Iteration 4/25 | Loss: 0.00102079
Iteration 5/25 | Loss: 0.00112649
Iteration 6/25 | Loss: 0.00114574
Iteration 7/25 | Loss: 0.00103601
Iteration 8/25 | Loss: 0.00107991
Iteration 9/25 | Loss: 0.00093390
Iteration 10/25 | Loss: 0.00090683
Iteration 11/25 | Loss: 0.00088022
Iteration 12/25 | Loss: 0.00085891
Iteration 13/25 | Loss: 0.00083387
Iteration 14/25 | Loss: 0.00081695
Iteration 15/25 | Loss: 0.00079764
Iteration 16/25 | Loss: 0.00077932
Iteration 17/25 | Loss: 0.00076597
Iteration 18/25 | Loss: 0.00076097
Iteration 19/25 | Loss: 0.00076940
Iteration 20/25 | Loss: 0.00079867
Iteration 21/25 | Loss: 0.00075790
Iteration 22/25 | Loss: 0.00074206
Iteration 23/25 | Loss: 0.00073403
Iteration 24/25 | Loss: 0.00073307
Iteration 25/25 | Loss: 0.00073242

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71513426
Iteration 2/25 | Loss: 0.00108057
Iteration 3/25 | Loss: 0.00108057
Iteration 4/25 | Loss: 0.00108057
Iteration 5/25 | Loss: 0.00108057
Iteration 6/25 | Loss: 0.00108057
Iteration 7/25 | Loss: 0.00108057
Iteration 8/25 | Loss: 0.00108057
Iteration 9/25 | Loss: 0.00108057
Iteration 10/25 | Loss: 0.00108057
Iteration 11/25 | Loss: 0.00108057
Iteration 12/25 | Loss: 0.00108057
Iteration 13/25 | Loss: 0.00108057
Iteration 14/25 | Loss: 0.00108057
Iteration 15/25 | Loss: 0.00108057
Iteration 16/25 | Loss: 0.00108057
Iteration 17/25 | Loss: 0.00108057
Iteration 18/25 | Loss: 0.00108057
Iteration 19/25 | Loss: 0.00108057
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010805717902258039, 0.0010805717902258039, 0.0010805717902258039, 0.0010805717902258039, 0.0010805717902258039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010805717902258039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108057
Iteration 2/1000 | Loss: 0.00336370
Iteration 3/1000 | Loss: 0.00239129
Iteration 4/1000 | Loss: 0.00058717
Iteration 5/1000 | Loss: 0.00032932
Iteration 6/1000 | Loss: 0.00062666
Iteration 7/1000 | Loss: 0.00393186
Iteration 8/1000 | Loss: 0.00360695
Iteration 9/1000 | Loss: 0.00290589
Iteration 10/1000 | Loss: 0.00045530
Iteration 11/1000 | Loss: 0.00431542
Iteration 12/1000 | Loss: 0.00038435
Iteration 13/1000 | Loss: 0.00086168
Iteration 14/1000 | Loss: 0.00068346
Iteration 15/1000 | Loss: 0.00074831
Iteration 16/1000 | Loss: 0.00084240
Iteration 17/1000 | Loss: 0.00062850
Iteration 18/1000 | Loss: 0.00111871
Iteration 19/1000 | Loss: 0.00106560
Iteration 20/1000 | Loss: 0.00080384
Iteration 21/1000 | Loss: 0.00033977
Iteration 22/1000 | Loss: 0.00050742
Iteration 23/1000 | Loss: 0.00014220
Iteration 24/1000 | Loss: 0.00023470
Iteration 25/1000 | Loss: 0.00089568
Iteration 26/1000 | Loss: 0.00049592
Iteration 27/1000 | Loss: 0.00039570
Iteration 28/1000 | Loss: 0.00037056
Iteration 29/1000 | Loss: 0.00063779
Iteration 30/1000 | Loss: 0.00026686
Iteration 31/1000 | Loss: 0.00053147
Iteration 32/1000 | Loss: 0.00024012
Iteration 33/1000 | Loss: 0.00048860
Iteration 34/1000 | Loss: 0.00022116
Iteration 35/1000 | Loss: 0.00030048
Iteration 36/1000 | Loss: 0.00030195
Iteration 37/1000 | Loss: 0.00027166
Iteration 38/1000 | Loss: 0.00033314
Iteration 39/1000 | Loss: 0.00035126
Iteration 40/1000 | Loss: 0.00033004
Iteration 41/1000 | Loss: 0.00030701
Iteration 42/1000 | Loss: 0.00106526
Iteration 43/1000 | Loss: 0.00073785
Iteration 44/1000 | Loss: 0.00062601
Iteration 45/1000 | Loss: 0.00038474
Iteration 46/1000 | Loss: 0.00027966
Iteration 47/1000 | Loss: 0.00030666
Iteration 48/1000 | Loss: 0.00007029
Iteration 49/1000 | Loss: 0.00018233
Iteration 50/1000 | Loss: 0.00018771
Iteration 51/1000 | Loss: 0.00016460
Iteration 52/1000 | Loss: 0.00048292
Iteration 53/1000 | Loss: 0.00035215
Iteration 54/1000 | Loss: 0.00024989
Iteration 55/1000 | Loss: 0.00038856
Iteration 56/1000 | Loss: 0.00022291
Iteration 57/1000 | Loss: 0.00021446
Iteration 58/1000 | Loss: 0.00022658
Iteration 59/1000 | Loss: 0.00008087
Iteration 60/1000 | Loss: 0.00079767
Iteration 61/1000 | Loss: 0.00129473
Iteration 62/1000 | Loss: 0.00109540
Iteration 63/1000 | Loss: 0.00055916
Iteration 64/1000 | Loss: 0.00043050
Iteration 65/1000 | Loss: 0.00028403
Iteration 66/1000 | Loss: 0.00044890
Iteration 67/1000 | Loss: 0.00039146
Iteration 68/1000 | Loss: 0.00076328
Iteration 69/1000 | Loss: 0.00068587
Iteration 70/1000 | Loss: 0.00062189
Iteration 71/1000 | Loss: 0.00022625
Iteration 72/1000 | Loss: 0.00045911
Iteration 73/1000 | Loss: 0.00079563
Iteration 74/1000 | Loss: 0.00030279
Iteration 75/1000 | Loss: 0.00014753
Iteration 76/1000 | Loss: 0.00004738
Iteration 77/1000 | Loss: 0.00003326
Iteration 78/1000 | Loss: 0.00002999
Iteration 79/1000 | Loss: 0.00031164
Iteration 80/1000 | Loss: 0.00028276
Iteration 81/1000 | Loss: 0.00004000
Iteration 82/1000 | Loss: 0.00019290
Iteration 83/1000 | Loss: 0.00002524
Iteration 84/1000 | Loss: 0.00002299
Iteration 85/1000 | Loss: 0.00002159
Iteration 86/1000 | Loss: 0.00002082
Iteration 87/1000 | Loss: 0.00002028
Iteration 88/1000 | Loss: 0.00001986
Iteration 89/1000 | Loss: 0.00051915
Iteration 90/1000 | Loss: 0.00002131
Iteration 91/1000 | Loss: 0.00001870
Iteration 92/1000 | Loss: 0.00001753
Iteration 93/1000 | Loss: 0.00001669
Iteration 94/1000 | Loss: 0.00001626
Iteration 95/1000 | Loss: 0.00001598
Iteration 96/1000 | Loss: 0.00001571
Iteration 97/1000 | Loss: 0.00001559
Iteration 98/1000 | Loss: 0.00001553
Iteration 99/1000 | Loss: 0.00001551
Iteration 100/1000 | Loss: 0.00001549
Iteration 101/1000 | Loss: 0.00001549
Iteration 102/1000 | Loss: 0.00001548
Iteration 103/1000 | Loss: 0.00001548
Iteration 104/1000 | Loss: 0.00001548
Iteration 105/1000 | Loss: 0.00001548
Iteration 106/1000 | Loss: 0.00001547
Iteration 107/1000 | Loss: 0.00001547
Iteration 108/1000 | Loss: 0.00001546
Iteration 109/1000 | Loss: 0.00001546
Iteration 110/1000 | Loss: 0.00001546
Iteration 111/1000 | Loss: 0.00001546
Iteration 112/1000 | Loss: 0.00001665
Iteration 113/1000 | Loss: 0.00001533
Iteration 114/1000 | Loss: 0.00001521
Iteration 115/1000 | Loss: 0.00001515
Iteration 116/1000 | Loss: 0.00001506
Iteration 117/1000 | Loss: 0.00001504
Iteration 118/1000 | Loss: 0.00001492
Iteration 119/1000 | Loss: 0.00001473
Iteration 120/1000 | Loss: 0.00001469
Iteration 121/1000 | Loss: 0.00001459
Iteration 122/1000 | Loss: 0.00001456
Iteration 123/1000 | Loss: 0.00001456
Iteration 124/1000 | Loss: 0.00001454
Iteration 125/1000 | Loss: 0.00001450
Iteration 126/1000 | Loss: 0.00001450
Iteration 127/1000 | Loss: 0.00001447
Iteration 128/1000 | Loss: 0.00001446
Iteration 129/1000 | Loss: 0.00001446
Iteration 130/1000 | Loss: 0.00001446
Iteration 131/1000 | Loss: 0.00001445
Iteration 132/1000 | Loss: 0.00001445
Iteration 133/1000 | Loss: 0.00001444
Iteration 134/1000 | Loss: 0.00001444
Iteration 135/1000 | Loss: 0.00001444
Iteration 136/1000 | Loss: 0.00001444
Iteration 137/1000 | Loss: 0.00001444
Iteration 138/1000 | Loss: 0.00001443
Iteration 139/1000 | Loss: 0.00001443
Iteration 140/1000 | Loss: 0.00001442
Iteration 141/1000 | Loss: 0.00001442
Iteration 142/1000 | Loss: 0.00001442
Iteration 143/1000 | Loss: 0.00001442
Iteration 144/1000 | Loss: 0.00001442
Iteration 145/1000 | Loss: 0.00001442
Iteration 146/1000 | Loss: 0.00001441
Iteration 147/1000 | Loss: 0.00001441
Iteration 148/1000 | Loss: 0.00001441
Iteration 149/1000 | Loss: 0.00001441
Iteration 150/1000 | Loss: 0.00001441
Iteration 151/1000 | Loss: 0.00001441
Iteration 152/1000 | Loss: 0.00001441
Iteration 153/1000 | Loss: 0.00001441
Iteration 154/1000 | Loss: 0.00001441
Iteration 155/1000 | Loss: 0.00001441
Iteration 156/1000 | Loss: 0.00001441
Iteration 157/1000 | Loss: 0.00001440
Iteration 158/1000 | Loss: 0.00001440
Iteration 159/1000 | Loss: 0.00001440
Iteration 160/1000 | Loss: 0.00001440
Iteration 161/1000 | Loss: 0.00001440
Iteration 162/1000 | Loss: 0.00001440
Iteration 163/1000 | Loss: 0.00001440
Iteration 164/1000 | Loss: 0.00001440
Iteration 165/1000 | Loss: 0.00001440
Iteration 166/1000 | Loss: 0.00001440
Iteration 167/1000 | Loss: 0.00001440
Iteration 168/1000 | Loss: 0.00001440
Iteration 169/1000 | Loss: 0.00001440
Iteration 170/1000 | Loss: 0.00001440
Iteration 171/1000 | Loss: 0.00001440
Iteration 172/1000 | Loss: 0.00001439
Iteration 173/1000 | Loss: 0.00001439
Iteration 174/1000 | Loss: 0.00001439
Iteration 175/1000 | Loss: 0.00001439
Iteration 176/1000 | Loss: 0.00001439
Iteration 177/1000 | Loss: 0.00001439
Iteration 178/1000 | Loss: 0.00001439
Iteration 179/1000 | Loss: 0.00001439
Iteration 180/1000 | Loss: 0.00001439
Iteration 181/1000 | Loss: 0.00001439
Iteration 182/1000 | Loss: 0.00001439
Iteration 183/1000 | Loss: 0.00001438
Iteration 184/1000 | Loss: 0.00001438
Iteration 185/1000 | Loss: 0.00001438
Iteration 186/1000 | Loss: 0.00001438
Iteration 187/1000 | Loss: 0.00001438
Iteration 188/1000 | Loss: 0.00001438
Iteration 189/1000 | Loss: 0.00001438
Iteration 190/1000 | Loss: 0.00001438
Iteration 191/1000 | Loss: 0.00001438
Iteration 192/1000 | Loss: 0.00001438
Iteration 193/1000 | Loss: 0.00001438
Iteration 194/1000 | Loss: 0.00001438
Iteration 195/1000 | Loss: 0.00001438
Iteration 196/1000 | Loss: 0.00001438
Iteration 197/1000 | Loss: 0.00001438
Iteration 198/1000 | Loss: 0.00001438
Iteration 199/1000 | Loss: 0.00001438
Iteration 200/1000 | Loss: 0.00001438
Iteration 201/1000 | Loss: 0.00001438
Iteration 202/1000 | Loss: 0.00001438
Iteration 203/1000 | Loss: 0.00001438
Iteration 204/1000 | Loss: 0.00001438
Iteration 205/1000 | Loss: 0.00001438
Iteration 206/1000 | Loss: 0.00001438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.4381516848516185e-05, 1.4381516848516185e-05, 1.4381516848516185e-05, 1.4381516848516185e-05, 1.4381516848516185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4381516848516185e-05

Optimization complete. Final v2v error: 3.1373136043548584 mm

Highest mean error: 5.3604021072387695 mm for frame 97

Lowest mean error: 2.7524242401123047 mm for frame 19

Saving results

Total time: 200.6658787727356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818502
Iteration 2/25 | Loss: 0.00175870
Iteration 3/25 | Loss: 0.00091016
Iteration 4/25 | Loss: 0.00079009
Iteration 5/25 | Loss: 0.00074038
Iteration 6/25 | Loss: 0.00071378
Iteration 7/25 | Loss: 0.00071518
Iteration 8/25 | Loss: 0.00072862
Iteration 9/25 | Loss: 0.00072818
Iteration 10/25 | Loss: 0.00070702
Iteration 11/25 | Loss: 0.00070927
Iteration 12/25 | Loss: 0.00070224
Iteration 13/25 | Loss: 0.00070194
Iteration 14/25 | Loss: 0.00070190
Iteration 15/25 | Loss: 0.00070190
Iteration 16/25 | Loss: 0.00070190
Iteration 17/25 | Loss: 0.00070190
Iteration 18/25 | Loss: 0.00070190
Iteration 19/25 | Loss: 0.00070190
Iteration 20/25 | Loss: 0.00070190
Iteration 21/25 | Loss: 0.00070190
Iteration 22/25 | Loss: 0.00070190
Iteration 23/25 | Loss: 0.00070190
Iteration 24/25 | Loss: 0.00070190
Iteration 25/25 | Loss: 0.00070190

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.79662323
Iteration 2/25 | Loss: 0.00037347
Iteration 3/25 | Loss: 0.00037343
Iteration 4/25 | Loss: 0.00037343
Iteration 5/25 | Loss: 0.00037343
Iteration 6/25 | Loss: 0.00037343
Iteration 7/25 | Loss: 0.00037343
Iteration 8/25 | Loss: 0.00037343
Iteration 9/25 | Loss: 0.00037343
Iteration 10/25 | Loss: 0.00037343
Iteration 11/25 | Loss: 0.00037343
Iteration 12/25 | Loss: 0.00037343
Iteration 13/25 | Loss: 0.00037343
Iteration 14/25 | Loss: 0.00037343
Iteration 15/25 | Loss: 0.00037343
Iteration 16/25 | Loss: 0.00037343
Iteration 17/25 | Loss: 0.00037343
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003734290658030659, 0.0003734290658030659, 0.0003734290658030659, 0.0003734290658030659, 0.0003734290658030659]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003734290658030659

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037343
Iteration 2/1000 | Loss: 0.00002633
Iteration 3/1000 | Loss: 0.00002113
Iteration 4/1000 | Loss: 0.00002013
Iteration 5/1000 | Loss: 0.00001941
Iteration 6/1000 | Loss: 0.00001895
Iteration 7/1000 | Loss: 0.00001862
Iteration 8/1000 | Loss: 0.00001829
Iteration 9/1000 | Loss: 0.00001822
Iteration 10/1000 | Loss: 0.00001818
Iteration 11/1000 | Loss: 0.00001815
Iteration 12/1000 | Loss: 0.00001811
Iteration 13/1000 | Loss: 0.00001803
Iteration 14/1000 | Loss: 0.00001803
Iteration 15/1000 | Loss: 0.00001801
Iteration 16/1000 | Loss: 0.00001801
Iteration 17/1000 | Loss: 0.00001798
Iteration 18/1000 | Loss: 0.00001798
Iteration 19/1000 | Loss: 0.00001798
Iteration 20/1000 | Loss: 0.00001798
Iteration 21/1000 | Loss: 0.00001798
Iteration 22/1000 | Loss: 0.00001797
Iteration 23/1000 | Loss: 0.00001797
Iteration 24/1000 | Loss: 0.00001797
Iteration 25/1000 | Loss: 0.00001797
Iteration 26/1000 | Loss: 0.00001797
Iteration 27/1000 | Loss: 0.00001797
Iteration 28/1000 | Loss: 0.00001797
Iteration 29/1000 | Loss: 0.00001797
Iteration 30/1000 | Loss: 0.00001797
Iteration 31/1000 | Loss: 0.00001797
Iteration 32/1000 | Loss: 0.00001797
Iteration 33/1000 | Loss: 0.00001796
Iteration 34/1000 | Loss: 0.00001795
Iteration 35/1000 | Loss: 0.00001794
Iteration 36/1000 | Loss: 0.00001792
Iteration 37/1000 | Loss: 0.00001792
Iteration 38/1000 | Loss: 0.00001792
Iteration 39/1000 | Loss: 0.00001792
Iteration 40/1000 | Loss: 0.00001792
Iteration 41/1000 | Loss: 0.00001792
Iteration 42/1000 | Loss: 0.00001791
Iteration 43/1000 | Loss: 0.00001791
Iteration 44/1000 | Loss: 0.00001791
Iteration 45/1000 | Loss: 0.00001791
Iteration 46/1000 | Loss: 0.00001791
Iteration 47/1000 | Loss: 0.00001791
Iteration 48/1000 | Loss: 0.00001791
Iteration 49/1000 | Loss: 0.00001791
Iteration 50/1000 | Loss: 0.00001790
Iteration 51/1000 | Loss: 0.00001789
Iteration 52/1000 | Loss: 0.00001789
Iteration 53/1000 | Loss: 0.00001788
Iteration 54/1000 | Loss: 0.00001788
Iteration 55/1000 | Loss: 0.00001788
Iteration 56/1000 | Loss: 0.00001788
Iteration 57/1000 | Loss: 0.00001787
Iteration 58/1000 | Loss: 0.00001787
Iteration 59/1000 | Loss: 0.00001787
Iteration 60/1000 | Loss: 0.00001787
Iteration 61/1000 | Loss: 0.00001787
Iteration 62/1000 | Loss: 0.00001786
Iteration 63/1000 | Loss: 0.00001786
Iteration 64/1000 | Loss: 0.00001786
Iteration 65/1000 | Loss: 0.00001786
Iteration 66/1000 | Loss: 0.00001786
Iteration 67/1000 | Loss: 0.00001786
Iteration 68/1000 | Loss: 0.00001786
Iteration 69/1000 | Loss: 0.00001786
Iteration 70/1000 | Loss: 0.00001786
Iteration 71/1000 | Loss: 0.00001785
Iteration 72/1000 | Loss: 0.00001785
Iteration 73/1000 | Loss: 0.00001785
Iteration 74/1000 | Loss: 0.00001785
Iteration 75/1000 | Loss: 0.00001785
Iteration 76/1000 | Loss: 0.00001785
Iteration 77/1000 | Loss: 0.00001785
Iteration 78/1000 | Loss: 0.00001785
Iteration 79/1000 | Loss: 0.00001785
Iteration 80/1000 | Loss: 0.00001785
Iteration 81/1000 | Loss: 0.00001784
Iteration 82/1000 | Loss: 0.00001784
Iteration 83/1000 | Loss: 0.00001784
Iteration 84/1000 | Loss: 0.00001784
Iteration 85/1000 | Loss: 0.00001784
Iteration 86/1000 | Loss: 0.00001784
Iteration 87/1000 | Loss: 0.00001784
Iteration 88/1000 | Loss: 0.00001784
Iteration 89/1000 | Loss: 0.00001784
Iteration 90/1000 | Loss: 0.00001784
Iteration 91/1000 | Loss: 0.00001784
Iteration 92/1000 | Loss: 0.00001783
Iteration 93/1000 | Loss: 0.00001783
Iteration 94/1000 | Loss: 0.00001783
Iteration 95/1000 | Loss: 0.00001783
Iteration 96/1000 | Loss: 0.00001783
Iteration 97/1000 | Loss: 0.00001782
Iteration 98/1000 | Loss: 0.00001782
Iteration 99/1000 | Loss: 0.00001782
Iteration 100/1000 | Loss: 0.00001782
Iteration 101/1000 | Loss: 0.00001782
Iteration 102/1000 | Loss: 0.00001782
Iteration 103/1000 | Loss: 0.00001782
Iteration 104/1000 | Loss: 0.00001782
Iteration 105/1000 | Loss: 0.00001782
Iteration 106/1000 | Loss: 0.00001782
Iteration 107/1000 | Loss: 0.00001782
Iteration 108/1000 | Loss: 0.00001782
Iteration 109/1000 | Loss: 0.00001782
Iteration 110/1000 | Loss: 0.00001782
Iteration 111/1000 | Loss: 0.00001782
Iteration 112/1000 | Loss: 0.00001782
Iteration 113/1000 | Loss: 0.00001782
Iteration 114/1000 | Loss: 0.00001782
Iteration 115/1000 | Loss: 0.00001782
Iteration 116/1000 | Loss: 0.00001782
Iteration 117/1000 | Loss: 0.00001781
Iteration 118/1000 | Loss: 0.00001781
Iteration 119/1000 | Loss: 0.00001781
Iteration 120/1000 | Loss: 0.00001781
Iteration 121/1000 | Loss: 0.00001781
Iteration 122/1000 | Loss: 0.00001781
Iteration 123/1000 | Loss: 0.00001781
Iteration 124/1000 | Loss: 0.00001781
Iteration 125/1000 | Loss: 0.00001781
Iteration 126/1000 | Loss: 0.00001781
Iteration 127/1000 | Loss: 0.00001781
Iteration 128/1000 | Loss: 0.00001781
Iteration 129/1000 | Loss: 0.00001781
Iteration 130/1000 | Loss: 0.00001781
Iteration 131/1000 | Loss: 0.00001781
Iteration 132/1000 | Loss: 0.00001781
Iteration 133/1000 | Loss: 0.00001781
Iteration 134/1000 | Loss: 0.00001781
Iteration 135/1000 | Loss: 0.00001781
Iteration 136/1000 | Loss: 0.00001781
Iteration 137/1000 | Loss: 0.00001781
Iteration 138/1000 | Loss: 0.00001781
Iteration 139/1000 | Loss: 0.00001781
Iteration 140/1000 | Loss: 0.00001781
Iteration 141/1000 | Loss: 0.00001781
Iteration 142/1000 | Loss: 0.00001781
Iteration 143/1000 | Loss: 0.00001781
Iteration 144/1000 | Loss: 0.00001781
Iteration 145/1000 | Loss: 0.00001781
Iteration 146/1000 | Loss: 0.00001781
Iteration 147/1000 | Loss: 0.00001781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.7808142729336396e-05, 1.7808142729336396e-05, 1.7808142729336396e-05, 1.7808142729336396e-05, 1.7808142729336396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7808142729336396e-05

Optimization complete. Final v2v error: 3.598968029022217 mm

Highest mean error: 4.052933692932129 mm for frame 8

Lowest mean error: 3.176999092102051 mm for frame 49

Saving results

Total time: 47.07348394393921
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436587
Iteration 2/25 | Loss: 0.00091323
Iteration 3/25 | Loss: 0.00074178
Iteration 4/25 | Loss: 0.00071074
Iteration 5/25 | Loss: 0.00070092
Iteration 6/25 | Loss: 0.00069913
Iteration 7/25 | Loss: 0.00069880
Iteration 8/25 | Loss: 0.00069880
Iteration 9/25 | Loss: 0.00069880
Iteration 10/25 | Loss: 0.00069880
Iteration 11/25 | Loss: 0.00069880
Iteration 12/25 | Loss: 0.00069880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006987956585362554, 0.0006987956585362554, 0.0006987956585362554, 0.0006987956585362554, 0.0006987956585362554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006987956585362554

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41381216
Iteration 2/25 | Loss: 0.00028413
Iteration 3/25 | Loss: 0.00028412
Iteration 4/25 | Loss: 0.00028412
Iteration 5/25 | Loss: 0.00028412
Iteration 6/25 | Loss: 0.00028412
Iteration 7/25 | Loss: 0.00028412
Iteration 8/25 | Loss: 0.00028412
Iteration 9/25 | Loss: 0.00028412
Iteration 10/25 | Loss: 0.00028412
Iteration 11/25 | Loss: 0.00028412
Iteration 12/25 | Loss: 0.00028412
Iteration 13/25 | Loss: 0.00028412
Iteration 14/25 | Loss: 0.00028412
Iteration 15/25 | Loss: 0.00028412
Iteration 16/25 | Loss: 0.00028412
Iteration 17/25 | Loss: 0.00028412
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00028412044048309326, 0.00028412044048309326, 0.00028412044048309326, 0.00028412044048309326, 0.00028412044048309326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028412044048309326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028412
Iteration 2/1000 | Loss: 0.00004290
Iteration 3/1000 | Loss: 0.00003102
Iteration 4/1000 | Loss: 0.00002910
Iteration 5/1000 | Loss: 0.00002751
Iteration 6/1000 | Loss: 0.00002654
Iteration 7/1000 | Loss: 0.00002561
Iteration 8/1000 | Loss: 0.00002510
Iteration 9/1000 | Loss: 0.00002494
Iteration 10/1000 | Loss: 0.00002477
Iteration 11/1000 | Loss: 0.00002459
Iteration 12/1000 | Loss: 0.00002457
Iteration 13/1000 | Loss: 0.00002448
Iteration 14/1000 | Loss: 0.00002440
Iteration 15/1000 | Loss: 0.00002436
Iteration 16/1000 | Loss: 0.00002436
Iteration 17/1000 | Loss: 0.00002436
Iteration 18/1000 | Loss: 0.00002436
Iteration 19/1000 | Loss: 0.00002434
Iteration 20/1000 | Loss: 0.00002427
Iteration 21/1000 | Loss: 0.00002427
Iteration 22/1000 | Loss: 0.00002427
Iteration 23/1000 | Loss: 0.00002427
Iteration 24/1000 | Loss: 0.00002426
Iteration 25/1000 | Loss: 0.00002425
Iteration 26/1000 | Loss: 0.00002425
Iteration 27/1000 | Loss: 0.00002424
Iteration 28/1000 | Loss: 0.00002424
Iteration 29/1000 | Loss: 0.00002424
Iteration 30/1000 | Loss: 0.00002424
Iteration 31/1000 | Loss: 0.00002424
Iteration 32/1000 | Loss: 0.00002424
Iteration 33/1000 | Loss: 0.00002424
Iteration 34/1000 | Loss: 0.00002424
Iteration 35/1000 | Loss: 0.00002424
Iteration 36/1000 | Loss: 0.00002424
Iteration 37/1000 | Loss: 0.00002424
Iteration 38/1000 | Loss: 0.00002423
Iteration 39/1000 | Loss: 0.00002423
Iteration 40/1000 | Loss: 0.00002423
Iteration 41/1000 | Loss: 0.00002423
Iteration 42/1000 | Loss: 0.00002423
Iteration 43/1000 | Loss: 0.00002423
Iteration 44/1000 | Loss: 0.00002423
Iteration 45/1000 | Loss: 0.00002422
Iteration 46/1000 | Loss: 0.00002422
Iteration 47/1000 | Loss: 0.00002422
Iteration 48/1000 | Loss: 0.00002422
Iteration 49/1000 | Loss: 0.00002422
Iteration 50/1000 | Loss: 0.00002422
Iteration 51/1000 | Loss: 0.00002422
Iteration 52/1000 | Loss: 0.00002422
Iteration 53/1000 | Loss: 0.00002422
Iteration 54/1000 | Loss: 0.00002422
Iteration 55/1000 | Loss: 0.00002421
Iteration 56/1000 | Loss: 0.00002421
Iteration 57/1000 | Loss: 0.00002421
Iteration 58/1000 | Loss: 0.00002421
Iteration 59/1000 | Loss: 0.00002421
Iteration 60/1000 | Loss: 0.00002421
Iteration 61/1000 | Loss: 0.00002421
Iteration 62/1000 | Loss: 0.00002420
Iteration 63/1000 | Loss: 0.00002420
Iteration 64/1000 | Loss: 0.00002420
Iteration 65/1000 | Loss: 0.00002420
Iteration 66/1000 | Loss: 0.00002420
Iteration 67/1000 | Loss: 0.00002420
Iteration 68/1000 | Loss: 0.00002420
Iteration 69/1000 | Loss: 0.00002419
Iteration 70/1000 | Loss: 0.00002419
Iteration 71/1000 | Loss: 0.00002419
Iteration 72/1000 | Loss: 0.00002419
Iteration 73/1000 | Loss: 0.00002419
Iteration 74/1000 | Loss: 0.00002419
Iteration 75/1000 | Loss: 0.00002418
Iteration 76/1000 | Loss: 0.00002417
Iteration 77/1000 | Loss: 0.00002417
Iteration 78/1000 | Loss: 0.00002417
Iteration 79/1000 | Loss: 0.00002417
Iteration 80/1000 | Loss: 0.00002417
Iteration 81/1000 | Loss: 0.00002416
Iteration 82/1000 | Loss: 0.00002416
Iteration 83/1000 | Loss: 0.00002416
Iteration 84/1000 | Loss: 0.00002416
Iteration 85/1000 | Loss: 0.00002416
Iteration 86/1000 | Loss: 0.00002416
Iteration 87/1000 | Loss: 0.00002415
Iteration 88/1000 | Loss: 0.00002415
Iteration 89/1000 | Loss: 0.00002415
Iteration 90/1000 | Loss: 0.00002414
Iteration 91/1000 | Loss: 0.00002414
Iteration 92/1000 | Loss: 0.00002414
Iteration 93/1000 | Loss: 0.00002413
Iteration 94/1000 | Loss: 0.00002413
Iteration 95/1000 | Loss: 0.00002413
Iteration 96/1000 | Loss: 0.00002413
Iteration 97/1000 | Loss: 0.00002412
Iteration 98/1000 | Loss: 0.00002411
Iteration 99/1000 | Loss: 0.00002411
Iteration 100/1000 | Loss: 0.00002411
Iteration 101/1000 | Loss: 0.00002411
Iteration 102/1000 | Loss: 0.00002410
Iteration 103/1000 | Loss: 0.00002410
Iteration 104/1000 | Loss: 0.00002410
Iteration 105/1000 | Loss: 0.00002409
Iteration 106/1000 | Loss: 0.00002409
Iteration 107/1000 | Loss: 0.00002409
Iteration 108/1000 | Loss: 0.00002409
Iteration 109/1000 | Loss: 0.00002408
Iteration 110/1000 | Loss: 0.00002408
Iteration 111/1000 | Loss: 0.00002408
Iteration 112/1000 | Loss: 0.00002408
Iteration 113/1000 | Loss: 0.00002408
Iteration 114/1000 | Loss: 0.00002408
Iteration 115/1000 | Loss: 0.00002408
Iteration 116/1000 | Loss: 0.00002407
Iteration 117/1000 | Loss: 0.00002407
Iteration 118/1000 | Loss: 0.00002407
Iteration 119/1000 | Loss: 0.00002407
Iteration 120/1000 | Loss: 0.00002407
Iteration 121/1000 | Loss: 0.00002407
Iteration 122/1000 | Loss: 0.00002407
Iteration 123/1000 | Loss: 0.00002407
Iteration 124/1000 | Loss: 0.00002407
Iteration 125/1000 | Loss: 0.00002407
Iteration 126/1000 | Loss: 0.00002406
Iteration 127/1000 | Loss: 0.00002406
Iteration 128/1000 | Loss: 0.00002405
Iteration 129/1000 | Loss: 0.00002405
Iteration 130/1000 | Loss: 0.00002405
Iteration 131/1000 | Loss: 0.00002405
Iteration 132/1000 | Loss: 0.00002405
Iteration 133/1000 | Loss: 0.00002405
Iteration 134/1000 | Loss: 0.00002405
Iteration 135/1000 | Loss: 0.00002405
Iteration 136/1000 | Loss: 0.00002405
Iteration 137/1000 | Loss: 0.00002405
Iteration 138/1000 | Loss: 0.00002405
Iteration 139/1000 | Loss: 0.00002405
Iteration 140/1000 | Loss: 0.00002405
Iteration 141/1000 | Loss: 0.00002405
Iteration 142/1000 | Loss: 0.00002404
Iteration 143/1000 | Loss: 0.00002404
Iteration 144/1000 | Loss: 0.00002404
Iteration 145/1000 | Loss: 0.00002404
Iteration 146/1000 | Loss: 0.00002404
Iteration 147/1000 | Loss: 0.00002404
Iteration 148/1000 | Loss: 0.00002404
Iteration 149/1000 | Loss: 0.00002404
Iteration 150/1000 | Loss: 0.00002404
Iteration 151/1000 | Loss: 0.00002404
Iteration 152/1000 | Loss: 0.00002404
Iteration 153/1000 | Loss: 0.00002403
Iteration 154/1000 | Loss: 0.00002403
Iteration 155/1000 | Loss: 0.00002403
Iteration 156/1000 | Loss: 0.00002403
Iteration 157/1000 | Loss: 0.00002403
Iteration 158/1000 | Loss: 0.00002403
Iteration 159/1000 | Loss: 0.00002403
Iteration 160/1000 | Loss: 0.00002403
Iteration 161/1000 | Loss: 0.00002403
Iteration 162/1000 | Loss: 0.00002403
Iteration 163/1000 | Loss: 0.00002403
Iteration 164/1000 | Loss: 0.00002403
Iteration 165/1000 | Loss: 0.00002403
Iteration 166/1000 | Loss: 0.00002403
Iteration 167/1000 | Loss: 0.00002402
Iteration 168/1000 | Loss: 0.00002402
Iteration 169/1000 | Loss: 0.00002402
Iteration 170/1000 | Loss: 0.00002402
Iteration 171/1000 | Loss: 0.00002402
Iteration 172/1000 | Loss: 0.00002402
Iteration 173/1000 | Loss: 0.00002402
Iteration 174/1000 | Loss: 0.00002402
Iteration 175/1000 | Loss: 0.00002402
Iteration 176/1000 | Loss: 0.00002402
Iteration 177/1000 | Loss: 0.00002402
Iteration 178/1000 | Loss: 0.00002402
Iteration 179/1000 | Loss: 0.00002402
Iteration 180/1000 | Loss: 0.00002402
Iteration 181/1000 | Loss: 0.00002402
Iteration 182/1000 | Loss: 0.00002402
Iteration 183/1000 | Loss: 0.00002402
Iteration 184/1000 | Loss: 0.00002401
Iteration 185/1000 | Loss: 0.00002401
Iteration 186/1000 | Loss: 0.00002401
Iteration 187/1000 | Loss: 0.00002401
Iteration 188/1000 | Loss: 0.00002401
Iteration 189/1000 | Loss: 0.00002401
Iteration 190/1000 | Loss: 0.00002401
Iteration 191/1000 | Loss: 0.00002401
Iteration 192/1000 | Loss: 0.00002401
Iteration 193/1000 | Loss: 0.00002401
Iteration 194/1000 | Loss: 0.00002401
Iteration 195/1000 | Loss: 0.00002401
Iteration 196/1000 | Loss: 0.00002401
Iteration 197/1000 | Loss: 0.00002401
Iteration 198/1000 | Loss: 0.00002401
Iteration 199/1000 | Loss: 0.00002401
Iteration 200/1000 | Loss: 0.00002400
Iteration 201/1000 | Loss: 0.00002400
Iteration 202/1000 | Loss: 0.00002400
Iteration 203/1000 | Loss: 0.00002400
Iteration 204/1000 | Loss: 0.00002400
Iteration 205/1000 | Loss: 0.00002400
Iteration 206/1000 | Loss: 0.00002400
Iteration 207/1000 | Loss: 0.00002400
Iteration 208/1000 | Loss: 0.00002400
Iteration 209/1000 | Loss: 0.00002400
Iteration 210/1000 | Loss: 0.00002400
Iteration 211/1000 | Loss: 0.00002400
Iteration 212/1000 | Loss: 0.00002400
Iteration 213/1000 | Loss: 0.00002400
Iteration 214/1000 | Loss: 0.00002400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.4004128135857172e-05, 2.4004128135857172e-05, 2.4004128135857172e-05, 2.4004128135857172e-05, 2.4004128135857172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4004128135857172e-05

Optimization complete. Final v2v error: 4.089609622955322 mm

Highest mean error: 4.489438056945801 mm for frame 176

Lowest mean error: 3.706606149673462 mm for frame 214

Saving results

Total time: 47.09111785888672
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00609743
Iteration 2/25 | Loss: 0.00075455
Iteration 3/25 | Loss: 0.00065111
Iteration 4/25 | Loss: 0.00062872
Iteration 5/25 | Loss: 0.00061797
Iteration 6/25 | Loss: 0.00061605
Iteration 7/25 | Loss: 0.00061568
Iteration 8/25 | Loss: 0.00061568
Iteration 9/25 | Loss: 0.00061568
Iteration 10/25 | Loss: 0.00061568
Iteration 11/25 | Loss: 0.00061568
Iteration 12/25 | Loss: 0.00061568
Iteration 13/25 | Loss: 0.00061568
Iteration 14/25 | Loss: 0.00061568
Iteration 15/25 | Loss: 0.00061568
Iteration 16/25 | Loss: 0.00061568
Iteration 17/25 | Loss: 0.00061568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006156843737699091, 0.0006156843737699091, 0.0006156843737699091, 0.0006156843737699091, 0.0006156843737699091]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006156843737699091

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.98370504
Iteration 2/25 | Loss: 0.00029607
Iteration 3/25 | Loss: 0.00029607
Iteration 4/25 | Loss: 0.00029607
Iteration 5/25 | Loss: 0.00029607
Iteration 6/25 | Loss: 0.00029607
Iteration 7/25 | Loss: 0.00029607
Iteration 8/25 | Loss: 0.00029607
Iteration 9/25 | Loss: 0.00029607
Iteration 10/25 | Loss: 0.00029606
Iteration 11/25 | Loss: 0.00029606
Iteration 12/25 | Loss: 0.00029606
Iteration 13/25 | Loss: 0.00029606
Iteration 14/25 | Loss: 0.00029606
Iteration 15/25 | Loss: 0.00029606
Iteration 16/25 | Loss: 0.00029606
Iteration 17/25 | Loss: 0.00029606
Iteration 18/25 | Loss: 0.00029606
Iteration 19/25 | Loss: 0.00029606
Iteration 20/25 | Loss: 0.00029606
Iteration 21/25 | Loss: 0.00029606
Iteration 22/25 | Loss: 0.00029606
Iteration 23/25 | Loss: 0.00029606
Iteration 24/25 | Loss: 0.00029606
Iteration 25/25 | Loss: 0.00029606

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029606
Iteration 2/1000 | Loss: 0.00002593
Iteration 3/1000 | Loss: 0.00001628
Iteration 4/1000 | Loss: 0.00001513
Iteration 5/1000 | Loss: 0.00001445
Iteration 6/1000 | Loss: 0.00001405
Iteration 7/1000 | Loss: 0.00001371
Iteration 8/1000 | Loss: 0.00001360
Iteration 9/1000 | Loss: 0.00001345
Iteration 10/1000 | Loss: 0.00001334
Iteration 11/1000 | Loss: 0.00001333
Iteration 12/1000 | Loss: 0.00001332
Iteration 13/1000 | Loss: 0.00001329
Iteration 14/1000 | Loss: 0.00001328
Iteration 15/1000 | Loss: 0.00001324
Iteration 16/1000 | Loss: 0.00001321
Iteration 17/1000 | Loss: 0.00001309
Iteration 18/1000 | Loss: 0.00001309
Iteration 19/1000 | Loss: 0.00001306
Iteration 20/1000 | Loss: 0.00001305
Iteration 21/1000 | Loss: 0.00001301
Iteration 22/1000 | Loss: 0.00001301
Iteration 23/1000 | Loss: 0.00001301
Iteration 24/1000 | Loss: 0.00001301
Iteration 25/1000 | Loss: 0.00001301
Iteration 26/1000 | Loss: 0.00001301
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001301
Iteration 29/1000 | Loss: 0.00001301
Iteration 30/1000 | Loss: 0.00001301
Iteration 31/1000 | Loss: 0.00001300
Iteration 32/1000 | Loss: 0.00001300
Iteration 33/1000 | Loss: 0.00001300
Iteration 34/1000 | Loss: 0.00001300
Iteration 35/1000 | Loss: 0.00001300
Iteration 36/1000 | Loss: 0.00001300
Iteration 37/1000 | Loss: 0.00001300
Iteration 38/1000 | Loss: 0.00001300
Iteration 39/1000 | Loss: 0.00001298
Iteration 40/1000 | Loss: 0.00001298
Iteration 41/1000 | Loss: 0.00001297
Iteration 42/1000 | Loss: 0.00001297
Iteration 43/1000 | Loss: 0.00001297
Iteration 44/1000 | Loss: 0.00001296
Iteration 45/1000 | Loss: 0.00001295
Iteration 46/1000 | Loss: 0.00001295
Iteration 47/1000 | Loss: 0.00001294
Iteration 48/1000 | Loss: 0.00001294
Iteration 49/1000 | Loss: 0.00001294
Iteration 50/1000 | Loss: 0.00001293
Iteration 51/1000 | Loss: 0.00001293
Iteration 52/1000 | Loss: 0.00001293
Iteration 53/1000 | Loss: 0.00001293
Iteration 54/1000 | Loss: 0.00001293
Iteration 55/1000 | Loss: 0.00001293
Iteration 56/1000 | Loss: 0.00001293
Iteration 57/1000 | Loss: 0.00001293
Iteration 58/1000 | Loss: 0.00001293
Iteration 59/1000 | Loss: 0.00001292
Iteration 60/1000 | Loss: 0.00001292
Iteration 61/1000 | Loss: 0.00001291
Iteration 62/1000 | Loss: 0.00001290
Iteration 63/1000 | Loss: 0.00001290
Iteration 64/1000 | Loss: 0.00001290
Iteration 65/1000 | Loss: 0.00001289
Iteration 66/1000 | Loss: 0.00001289
Iteration 67/1000 | Loss: 0.00001289
Iteration 68/1000 | Loss: 0.00001289
Iteration 69/1000 | Loss: 0.00001289
Iteration 70/1000 | Loss: 0.00001289
Iteration 71/1000 | Loss: 0.00001289
Iteration 72/1000 | Loss: 0.00001289
Iteration 73/1000 | Loss: 0.00001289
Iteration 74/1000 | Loss: 0.00001289
Iteration 75/1000 | Loss: 0.00001289
Iteration 76/1000 | Loss: 0.00001289
Iteration 77/1000 | Loss: 0.00001289
Iteration 78/1000 | Loss: 0.00001289
Iteration 79/1000 | Loss: 0.00001289
Iteration 80/1000 | Loss: 0.00001289
Iteration 81/1000 | Loss: 0.00001289
Iteration 82/1000 | Loss: 0.00001289
Iteration 83/1000 | Loss: 0.00001289
Iteration 84/1000 | Loss: 0.00001289
Iteration 85/1000 | Loss: 0.00001289
Iteration 86/1000 | Loss: 0.00001289
Iteration 87/1000 | Loss: 0.00001289
Iteration 88/1000 | Loss: 0.00001289
Iteration 89/1000 | Loss: 0.00001289
Iteration 90/1000 | Loss: 0.00001289
Iteration 91/1000 | Loss: 0.00001289
Iteration 92/1000 | Loss: 0.00001289
Iteration 93/1000 | Loss: 0.00001289
Iteration 94/1000 | Loss: 0.00001289
Iteration 95/1000 | Loss: 0.00001289
Iteration 96/1000 | Loss: 0.00001289
Iteration 97/1000 | Loss: 0.00001289
Iteration 98/1000 | Loss: 0.00001289
Iteration 99/1000 | Loss: 0.00001289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.2891504411527421e-05, 1.2891504411527421e-05, 1.2891504411527421e-05, 1.2891504411527421e-05, 1.2891504411527421e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2891504411527421e-05

Optimization complete. Final v2v error: 3.0582447052001953 mm

Highest mean error: 3.2296881675720215 mm for frame 30

Lowest mean error: 2.9169504642486572 mm for frame 108

Saving results

Total time: 32.98297929763794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046938
Iteration 2/25 | Loss: 0.00505523
Iteration 3/25 | Loss: 0.00378493
Iteration 4/25 | Loss: 0.00385429
Iteration 5/25 | Loss: 0.00225831
Iteration 6/25 | Loss: 0.00197302
Iteration 7/25 | Loss: 0.00187584
Iteration 8/25 | Loss: 0.00177679
Iteration 9/25 | Loss: 0.00170031
Iteration 10/25 | Loss: 0.00165754
Iteration 11/25 | Loss: 0.00162631
Iteration 12/25 | Loss: 0.00161505
Iteration 13/25 | Loss: 0.00160382
Iteration 14/25 | Loss: 0.00159189
Iteration 15/25 | Loss: 0.00159314
Iteration 16/25 | Loss: 0.00159129
Iteration 17/25 | Loss: 0.00158340
Iteration 18/25 | Loss: 0.00157991
Iteration 19/25 | Loss: 0.00157455
Iteration 20/25 | Loss: 0.00157216
Iteration 21/25 | Loss: 0.00156681
Iteration 22/25 | Loss: 0.00156544
Iteration 23/25 | Loss: 0.00156481
Iteration 24/25 | Loss: 0.00156449
Iteration 25/25 | Loss: 0.00156370

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37185633
Iteration 2/25 | Loss: 0.00894642
Iteration 3/25 | Loss: 0.00869212
Iteration 4/25 | Loss: 0.00869211
Iteration 5/25 | Loss: 0.00869211
Iteration 6/25 | Loss: 0.00869211
Iteration 7/25 | Loss: 0.00869211
Iteration 8/25 | Loss: 0.00869211
Iteration 9/25 | Loss: 0.00869211
Iteration 10/25 | Loss: 0.00869211
Iteration 11/25 | Loss: 0.00869211
Iteration 12/25 | Loss: 0.00869211
Iteration 13/25 | Loss: 0.00869211
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.008692112751305103, 0.008692112751305103, 0.008692112751305103, 0.008692112751305103, 0.008692112751305103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008692112751305103

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00869211
Iteration 2/1000 | Loss: 0.00410205
Iteration 3/1000 | Loss: 0.00421407
Iteration 4/1000 | Loss: 0.00269637
Iteration 5/1000 | Loss: 0.00275519
Iteration 6/1000 | Loss: 0.00164047
Iteration 7/1000 | Loss: 0.00127399
Iteration 8/1000 | Loss: 0.00097406
Iteration 9/1000 | Loss: 0.00078204
Iteration 10/1000 | Loss: 0.00055324
Iteration 11/1000 | Loss: 0.00031436
Iteration 12/1000 | Loss: 0.00070093
Iteration 13/1000 | Loss: 0.00019520
Iteration 14/1000 | Loss: 0.00058543
Iteration 15/1000 | Loss: 0.00082345
Iteration 16/1000 | Loss: 0.00114057
Iteration 17/1000 | Loss: 0.00082111
Iteration 18/1000 | Loss: 0.00050096
Iteration 19/1000 | Loss: 0.00011394
Iteration 20/1000 | Loss: 0.00019298
Iteration 21/1000 | Loss: 0.00056744
Iteration 22/1000 | Loss: 0.00054102
Iteration 23/1000 | Loss: 0.00060192
Iteration 24/1000 | Loss: 0.00115270
Iteration 25/1000 | Loss: 0.00261778
Iteration 26/1000 | Loss: 0.00113014
Iteration 27/1000 | Loss: 0.00026413
Iteration 28/1000 | Loss: 0.00012266
Iteration 29/1000 | Loss: 0.00009319
Iteration 30/1000 | Loss: 0.00006853
Iteration 31/1000 | Loss: 0.00004797
Iteration 32/1000 | Loss: 0.00039847
Iteration 33/1000 | Loss: 0.00004340
Iteration 34/1000 | Loss: 0.00010235
Iteration 35/1000 | Loss: 0.00003320
Iteration 36/1000 | Loss: 0.00003016
Iteration 37/1000 | Loss: 0.00002778
Iteration 38/1000 | Loss: 0.00002619
Iteration 39/1000 | Loss: 0.00025834
Iteration 40/1000 | Loss: 0.00022674
Iteration 41/1000 | Loss: 0.00013225
Iteration 42/1000 | Loss: 0.00016186
Iteration 43/1000 | Loss: 0.00002935
Iteration 44/1000 | Loss: 0.00002598
Iteration 45/1000 | Loss: 0.00002368
Iteration 46/1000 | Loss: 0.00002169
Iteration 47/1000 | Loss: 0.00002082
Iteration 48/1000 | Loss: 0.00002034
Iteration 49/1000 | Loss: 0.00002006
Iteration 50/1000 | Loss: 0.00001986
Iteration 51/1000 | Loss: 0.00001984
Iteration 52/1000 | Loss: 0.00001979
Iteration 53/1000 | Loss: 0.00001979
Iteration 54/1000 | Loss: 0.00001972
Iteration 55/1000 | Loss: 0.00001966
Iteration 56/1000 | Loss: 0.00001966
Iteration 57/1000 | Loss: 0.00001956
Iteration 58/1000 | Loss: 0.00001954
Iteration 59/1000 | Loss: 0.00001954
Iteration 60/1000 | Loss: 0.00001951
Iteration 61/1000 | Loss: 0.00001950
Iteration 62/1000 | Loss: 0.00001949
Iteration 63/1000 | Loss: 0.00001949
Iteration 64/1000 | Loss: 0.00001949
Iteration 65/1000 | Loss: 0.00001948
Iteration 66/1000 | Loss: 0.00001948
Iteration 67/1000 | Loss: 0.00001948
Iteration 68/1000 | Loss: 0.00001947
Iteration 69/1000 | Loss: 0.00001947
Iteration 70/1000 | Loss: 0.00001947
Iteration 71/1000 | Loss: 0.00001946
Iteration 72/1000 | Loss: 0.00001944
Iteration 73/1000 | Loss: 0.00001944
Iteration 74/1000 | Loss: 0.00001944
Iteration 75/1000 | Loss: 0.00001944
Iteration 76/1000 | Loss: 0.00001942
Iteration 77/1000 | Loss: 0.00001941
Iteration 78/1000 | Loss: 0.00001941
Iteration 79/1000 | Loss: 0.00001941
Iteration 80/1000 | Loss: 0.00001941
Iteration 81/1000 | Loss: 0.00001941
Iteration 82/1000 | Loss: 0.00001941
Iteration 83/1000 | Loss: 0.00001941
Iteration 84/1000 | Loss: 0.00001940
Iteration 85/1000 | Loss: 0.00001940
Iteration 86/1000 | Loss: 0.00001939
Iteration 87/1000 | Loss: 0.00001938
Iteration 88/1000 | Loss: 0.00001937
Iteration 89/1000 | Loss: 0.00001937
Iteration 90/1000 | Loss: 0.00001937
Iteration 91/1000 | Loss: 0.00001937
Iteration 92/1000 | Loss: 0.00001936
Iteration 93/1000 | Loss: 0.00001936
Iteration 94/1000 | Loss: 0.00001936
Iteration 95/1000 | Loss: 0.00001935
Iteration 96/1000 | Loss: 0.00001934
Iteration 97/1000 | Loss: 0.00001934
Iteration 98/1000 | Loss: 0.00001934
Iteration 99/1000 | Loss: 0.00001933
Iteration 100/1000 | Loss: 0.00001933
Iteration 101/1000 | Loss: 0.00001932
Iteration 102/1000 | Loss: 0.00001932
Iteration 103/1000 | Loss: 0.00001932
Iteration 104/1000 | Loss: 0.00001931
Iteration 105/1000 | Loss: 0.00001931
Iteration 106/1000 | Loss: 0.00001931
Iteration 107/1000 | Loss: 0.00001931
Iteration 108/1000 | Loss: 0.00001931
Iteration 109/1000 | Loss: 0.00001930
Iteration 110/1000 | Loss: 0.00001930
Iteration 111/1000 | Loss: 0.00001930
Iteration 112/1000 | Loss: 0.00001930
Iteration 113/1000 | Loss: 0.00001930
Iteration 114/1000 | Loss: 0.00001930
Iteration 115/1000 | Loss: 0.00001930
Iteration 116/1000 | Loss: 0.00001929
Iteration 117/1000 | Loss: 0.00001929
Iteration 118/1000 | Loss: 0.00001929
Iteration 119/1000 | Loss: 0.00001929
Iteration 120/1000 | Loss: 0.00001929
Iteration 121/1000 | Loss: 0.00001929
Iteration 122/1000 | Loss: 0.00001929
Iteration 123/1000 | Loss: 0.00001929
Iteration 124/1000 | Loss: 0.00001929
Iteration 125/1000 | Loss: 0.00001929
Iteration 126/1000 | Loss: 0.00001929
Iteration 127/1000 | Loss: 0.00001928
Iteration 128/1000 | Loss: 0.00001928
Iteration 129/1000 | Loss: 0.00001928
Iteration 130/1000 | Loss: 0.00001928
Iteration 131/1000 | Loss: 0.00001928
Iteration 132/1000 | Loss: 0.00001928
Iteration 133/1000 | Loss: 0.00001928
Iteration 134/1000 | Loss: 0.00001928
Iteration 135/1000 | Loss: 0.00001928
Iteration 136/1000 | Loss: 0.00001928
Iteration 137/1000 | Loss: 0.00001927
Iteration 138/1000 | Loss: 0.00001927
Iteration 139/1000 | Loss: 0.00001927
Iteration 140/1000 | Loss: 0.00001927
Iteration 141/1000 | Loss: 0.00001927
Iteration 142/1000 | Loss: 0.00001927
Iteration 143/1000 | Loss: 0.00001927
Iteration 144/1000 | Loss: 0.00001927
Iteration 145/1000 | Loss: 0.00001926
Iteration 146/1000 | Loss: 0.00001926
Iteration 147/1000 | Loss: 0.00001926
Iteration 148/1000 | Loss: 0.00001926
Iteration 149/1000 | Loss: 0.00001926
Iteration 150/1000 | Loss: 0.00001926
Iteration 151/1000 | Loss: 0.00001926
Iteration 152/1000 | Loss: 0.00001926
Iteration 153/1000 | Loss: 0.00001926
Iteration 154/1000 | Loss: 0.00001926
Iteration 155/1000 | Loss: 0.00001926
Iteration 156/1000 | Loss: 0.00001926
Iteration 157/1000 | Loss: 0.00001925
Iteration 158/1000 | Loss: 0.00001925
Iteration 159/1000 | Loss: 0.00001925
Iteration 160/1000 | Loss: 0.00001925
Iteration 161/1000 | Loss: 0.00001925
Iteration 162/1000 | Loss: 0.00001925
Iteration 163/1000 | Loss: 0.00001925
Iteration 164/1000 | Loss: 0.00001925
Iteration 165/1000 | Loss: 0.00001925
Iteration 166/1000 | Loss: 0.00001925
Iteration 167/1000 | Loss: 0.00001925
Iteration 168/1000 | Loss: 0.00001924
Iteration 169/1000 | Loss: 0.00001924
Iteration 170/1000 | Loss: 0.00001924
Iteration 171/1000 | Loss: 0.00001924
Iteration 172/1000 | Loss: 0.00001924
Iteration 173/1000 | Loss: 0.00001924
Iteration 174/1000 | Loss: 0.00001923
Iteration 175/1000 | Loss: 0.00001923
Iteration 176/1000 | Loss: 0.00001923
Iteration 177/1000 | Loss: 0.00001923
Iteration 178/1000 | Loss: 0.00001923
Iteration 179/1000 | Loss: 0.00001923
Iteration 180/1000 | Loss: 0.00001923
Iteration 181/1000 | Loss: 0.00001923
Iteration 182/1000 | Loss: 0.00001923
Iteration 183/1000 | Loss: 0.00001923
Iteration 184/1000 | Loss: 0.00001923
Iteration 185/1000 | Loss: 0.00001923
Iteration 186/1000 | Loss: 0.00001923
Iteration 187/1000 | Loss: 0.00001923
Iteration 188/1000 | Loss: 0.00001923
Iteration 189/1000 | Loss: 0.00001923
Iteration 190/1000 | Loss: 0.00001923
Iteration 191/1000 | Loss: 0.00001922
Iteration 192/1000 | Loss: 0.00001922
Iteration 193/1000 | Loss: 0.00001922
Iteration 194/1000 | Loss: 0.00001922
Iteration 195/1000 | Loss: 0.00001922
Iteration 196/1000 | Loss: 0.00001922
Iteration 197/1000 | Loss: 0.00001922
Iteration 198/1000 | Loss: 0.00001922
Iteration 199/1000 | Loss: 0.00001922
Iteration 200/1000 | Loss: 0.00001922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.922310548252426e-05, 1.922310548252426e-05, 1.922310548252426e-05, 1.922310548252426e-05, 1.922310548252426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.922310548252426e-05

Optimization complete. Final v2v error: 3.5687317848205566 mm

Highest mean error: 12.57228946685791 mm for frame 230

Lowest mean error: 3.289109230041504 mm for frame 100

Saving results

Total time: 148.30977177619934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879262
Iteration 2/25 | Loss: 0.00144385
Iteration 3/25 | Loss: 0.00111210
Iteration 4/25 | Loss: 0.00104646
Iteration 5/25 | Loss: 0.00102865
Iteration 6/25 | Loss: 0.00102135
Iteration 7/25 | Loss: 0.00102796
Iteration 8/25 | Loss: 0.00102631
Iteration 9/25 | Loss: 0.00101915
Iteration 10/25 | Loss: 0.00101696
Iteration 11/25 | Loss: 0.00101153
Iteration 12/25 | Loss: 0.00100561
Iteration 13/25 | Loss: 0.00100070
Iteration 14/25 | Loss: 0.00099426
Iteration 15/25 | Loss: 0.00099624
Iteration 16/25 | Loss: 0.00099313
Iteration 17/25 | Loss: 0.00098959
Iteration 18/25 | Loss: 0.00098403
Iteration 19/25 | Loss: 0.00098321
Iteration 20/25 | Loss: 0.00098304
Iteration 21/25 | Loss: 0.00098292
Iteration 22/25 | Loss: 0.00098284
Iteration 23/25 | Loss: 0.00098283
Iteration 24/25 | Loss: 0.00098283
Iteration 25/25 | Loss: 0.00098283

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39344311
Iteration 2/25 | Loss: 0.00201119
Iteration 3/25 | Loss: 0.00201112
Iteration 4/25 | Loss: 0.00201112
Iteration 5/25 | Loss: 0.00201112
Iteration 6/25 | Loss: 0.00201112
Iteration 7/25 | Loss: 0.00201112
Iteration 8/25 | Loss: 0.00201112
Iteration 9/25 | Loss: 0.00201112
Iteration 10/25 | Loss: 0.00201112
Iteration 11/25 | Loss: 0.00201112
Iteration 12/25 | Loss: 0.00201112
Iteration 13/25 | Loss: 0.00201112
Iteration 14/25 | Loss: 0.00201112
Iteration 15/25 | Loss: 0.00201112
Iteration 16/25 | Loss: 0.00201112
Iteration 17/25 | Loss: 0.00201112
Iteration 18/25 | Loss: 0.00201112
Iteration 19/25 | Loss: 0.00201112
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002011119155213237, 0.002011119155213237, 0.002011119155213237, 0.002011119155213237, 0.002011119155213237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002011119155213237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201112
Iteration 2/1000 | Loss: 0.00042070
Iteration 3/1000 | Loss: 0.00031468
Iteration 4/1000 | Loss: 0.00025737
Iteration 5/1000 | Loss: 0.00051066
Iteration 6/1000 | Loss: 0.00038540
Iteration 7/1000 | Loss: 0.00018731
Iteration 8/1000 | Loss: 0.00035852
Iteration 9/1000 | Loss: 0.00117485
Iteration 10/1000 | Loss: 0.00011687
Iteration 11/1000 | Loss: 0.00010055
Iteration 12/1000 | Loss: 0.00008903
Iteration 13/1000 | Loss: 0.00068295
Iteration 14/1000 | Loss: 0.00168747
Iteration 15/1000 | Loss: 0.00156321
Iteration 16/1000 | Loss: 0.00052199
Iteration 17/1000 | Loss: 0.00009497
Iteration 18/1000 | Loss: 0.00007687
Iteration 19/1000 | Loss: 0.00007093
Iteration 20/1000 | Loss: 0.00006769
Iteration 21/1000 | Loss: 0.00006530
Iteration 22/1000 | Loss: 0.00006335
Iteration 23/1000 | Loss: 0.00006188
Iteration 24/1000 | Loss: 0.00087467
Iteration 25/1000 | Loss: 0.00006589
Iteration 26/1000 | Loss: 0.00006008
Iteration 27/1000 | Loss: 0.00005694
Iteration 28/1000 | Loss: 0.00070943
Iteration 29/1000 | Loss: 0.00300383
Iteration 30/1000 | Loss: 0.00006888
Iteration 31/1000 | Loss: 0.00005344
Iteration 32/1000 | Loss: 0.00004719
Iteration 33/1000 | Loss: 0.00004208
Iteration 34/1000 | Loss: 0.00003934
Iteration 35/1000 | Loss: 0.00097140
Iteration 36/1000 | Loss: 0.00093413
Iteration 37/1000 | Loss: 0.00061847
Iteration 38/1000 | Loss: 0.00079325
Iteration 39/1000 | Loss: 0.00004339
Iteration 40/1000 | Loss: 0.00003697
Iteration 41/1000 | Loss: 0.00003398
Iteration 42/1000 | Loss: 0.00003189
Iteration 43/1000 | Loss: 0.00094848
Iteration 44/1000 | Loss: 0.00003262
Iteration 45/1000 | Loss: 0.00002971
Iteration 46/1000 | Loss: 0.00002808
Iteration 47/1000 | Loss: 0.00002667
Iteration 48/1000 | Loss: 0.00002581
Iteration 49/1000 | Loss: 0.00002521
Iteration 50/1000 | Loss: 0.00002494
Iteration 51/1000 | Loss: 0.00002463
Iteration 52/1000 | Loss: 0.00002450
Iteration 53/1000 | Loss: 0.00002447
Iteration 54/1000 | Loss: 0.00002444
Iteration 55/1000 | Loss: 0.00002443
Iteration 56/1000 | Loss: 0.00002442
Iteration 57/1000 | Loss: 0.00002442
Iteration 58/1000 | Loss: 0.00002438
Iteration 59/1000 | Loss: 0.00002436
Iteration 60/1000 | Loss: 0.00002432
Iteration 61/1000 | Loss: 0.00002428
Iteration 62/1000 | Loss: 0.00002427
Iteration 63/1000 | Loss: 0.00002426
Iteration 64/1000 | Loss: 0.00002425
Iteration 65/1000 | Loss: 0.00002425
Iteration 66/1000 | Loss: 0.00002424
Iteration 67/1000 | Loss: 0.00002424
Iteration 68/1000 | Loss: 0.00002424
Iteration 69/1000 | Loss: 0.00002423
Iteration 70/1000 | Loss: 0.00002423
Iteration 71/1000 | Loss: 0.00002423
Iteration 72/1000 | Loss: 0.00002422
Iteration 73/1000 | Loss: 0.00002421
Iteration 74/1000 | Loss: 0.00002421
Iteration 75/1000 | Loss: 0.00002421
Iteration 76/1000 | Loss: 0.00002421
Iteration 77/1000 | Loss: 0.00002420
Iteration 78/1000 | Loss: 0.00002420
Iteration 79/1000 | Loss: 0.00002419
Iteration 80/1000 | Loss: 0.00002419
Iteration 81/1000 | Loss: 0.00002418
Iteration 82/1000 | Loss: 0.00002418
Iteration 83/1000 | Loss: 0.00002414
Iteration 84/1000 | Loss: 0.00002413
Iteration 85/1000 | Loss: 0.00002413
Iteration 86/1000 | Loss: 0.00002412
Iteration 87/1000 | Loss: 0.00002412
Iteration 88/1000 | Loss: 0.00002412
Iteration 89/1000 | Loss: 0.00002411
Iteration 90/1000 | Loss: 0.00002410
Iteration 91/1000 | Loss: 0.00002410
Iteration 92/1000 | Loss: 0.00002410
Iteration 93/1000 | Loss: 0.00002409
Iteration 94/1000 | Loss: 0.00002409
Iteration 95/1000 | Loss: 0.00002409
Iteration 96/1000 | Loss: 0.00002408
Iteration 97/1000 | Loss: 0.00002408
Iteration 98/1000 | Loss: 0.00002408
Iteration 99/1000 | Loss: 0.00002407
Iteration 100/1000 | Loss: 0.00002407
Iteration 101/1000 | Loss: 0.00002406
Iteration 102/1000 | Loss: 0.00002406
Iteration 103/1000 | Loss: 0.00002406
Iteration 104/1000 | Loss: 0.00002406
Iteration 105/1000 | Loss: 0.00002405
Iteration 106/1000 | Loss: 0.00002405
Iteration 107/1000 | Loss: 0.00002405
Iteration 108/1000 | Loss: 0.00002405
Iteration 109/1000 | Loss: 0.00002405
Iteration 110/1000 | Loss: 0.00002404
Iteration 111/1000 | Loss: 0.00002404
Iteration 112/1000 | Loss: 0.00002404
Iteration 113/1000 | Loss: 0.00002403
Iteration 114/1000 | Loss: 0.00002403
Iteration 115/1000 | Loss: 0.00002403
Iteration 116/1000 | Loss: 0.00002403
Iteration 117/1000 | Loss: 0.00002402
Iteration 118/1000 | Loss: 0.00002402
Iteration 119/1000 | Loss: 0.00002402
Iteration 120/1000 | Loss: 0.00002401
Iteration 121/1000 | Loss: 0.00002401
Iteration 122/1000 | Loss: 0.00002401
Iteration 123/1000 | Loss: 0.00002401
Iteration 124/1000 | Loss: 0.00002401
Iteration 125/1000 | Loss: 0.00002400
Iteration 126/1000 | Loss: 0.00002400
Iteration 127/1000 | Loss: 0.00002400
Iteration 128/1000 | Loss: 0.00002400
Iteration 129/1000 | Loss: 0.00002400
Iteration 130/1000 | Loss: 0.00002400
Iteration 131/1000 | Loss: 0.00002400
Iteration 132/1000 | Loss: 0.00002400
Iteration 133/1000 | Loss: 0.00002400
Iteration 134/1000 | Loss: 0.00002399
Iteration 135/1000 | Loss: 0.00002399
Iteration 136/1000 | Loss: 0.00002399
Iteration 137/1000 | Loss: 0.00002399
Iteration 138/1000 | Loss: 0.00002399
Iteration 139/1000 | Loss: 0.00002399
Iteration 140/1000 | Loss: 0.00002398
Iteration 141/1000 | Loss: 0.00002398
Iteration 142/1000 | Loss: 0.00002398
Iteration 143/1000 | Loss: 0.00002398
Iteration 144/1000 | Loss: 0.00002397
Iteration 145/1000 | Loss: 0.00002397
Iteration 146/1000 | Loss: 0.00002397
Iteration 147/1000 | Loss: 0.00002396
Iteration 148/1000 | Loss: 0.00002396
Iteration 149/1000 | Loss: 0.00002396
Iteration 150/1000 | Loss: 0.00002396
Iteration 151/1000 | Loss: 0.00002395
Iteration 152/1000 | Loss: 0.00002395
Iteration 153/1000 | Loss: 0.00002395
Iteration 154/1000 | Loss: 0.00002394
Iteration 155/1000 | Loss: 0.00002394
Iteration 156/1000 | Loss: 0.00002394
Iteration 157/1000 | Loss: 0.00002394
Iteration 158/1000 | Loss: 0.00002394
Iteration 159/1000 | Loss: 0.00002394
Iteration 160/1000 | Loss: 0.00002394
Iteration 161/1000 | Loss: 0.00002394
Iteration 162/1000 | Loss: 0.00002394
Iteration 163/1000 | Loss: 0.00002394
Iteration 164/1000 | Loss: 0.00002394
Iteration 165/1000 | Loss: 0.00002394
Iteration 166/1000 | Loss: 0.00002394
Iteration 167/1000 | Loss: 0.00002394
Iteration 168/1000 | Loss: 0.00002394
Iteration 169/1000 | Loss: 0.00002394
Iteration 170/1000 | Loss: 0.00002394
Iteration 171/1000 | Loss: 0.00002394
Iteration 172/1000 | Loss: 0.00002394
Iteration 173/1000 | Loss: 0.00002394
Iteration 174/1000 | Loss: 0.00002394
Iteration 175/1000 | Loss: 0.00002394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.394062903476879e-05, 2.394062903476879e-05, 2.394062903476879e-05, 2.394062903476879e-05, 2.394062903476879e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.394062903476879e-05

Optimization complete. Final v2v error: 4.0341596603393555 mm

Highest mean error: 4.7885260581970215 mm for frame 48

Lowest mean error: 2.9361331462860107 mm for frame 8

Saving results

Total time: 122.79766154289246
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01147213
Iteration 2/25 | Loss: 0.00132328
Iteration 3/25 | Loss: 0.00083327
Iteration 4/25 | Loss: 0.00078675
Iteration 5/25 | Loss: 0.00077812
Iteration 6/25 | Loss: 0.00077029
Iteration 7/25 | Loss: 0.00076928
Iteration 8/25 | Loss: 0.00076923
Iteration 9/25 | Loss: 0.00076923
Iteration 10/25 | Loss: 0.00076923
Iteration 11/25 | Loss: 0.00076923
Iteration 12/25 | Loss: 0.00076923
Iteration 13/25 | Loss: 0.00076923
Iteration 14/25 | Loss: 0.00076922
Iteration 15/25 | Loss: 0.00076922
Iteration 16/25 | Loss: 0.00076922
Iteration 17/25 | Loss: 0.00076922
Iteration 18/25 | Loss: 0.00076922
Iteration 19/25 | Loss: 0.00076922
Iteration 20/25 | Loss: 0.00076922
Iteration 21/25 | Loss: 0.00076922
Iteration 22/25 | Loss: 0.00076922
Iteration 23/25 | Loss: 0.00076922
Iteration 24/25 | Loss: 0.00076922
Iteration 25/25 | Loss: 0.00076922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87575567
Iteration 2/25 | Loss: 0.00023339
Iteration 3/25 | Loss: 0.00023337
Iteration 4/25 | Loss: 0.00023337
Iteration 5/25 | Loss: 0.00023337
Iteration 6/25 | Loss: 0.00023337
Iteration 7/25 | Loss: 0.00023337
Iteration 8/25 | Loss: 0.00023337
Iteration 9/25 | Loss: 0.00023337
Iteration 10/25 | Loss: 0.00023336
Iteration 11/25 | Loss: 0.00023336
Iteration 12/25 | Loss: 0.00023336
Iteration 13/25 | Loss: 0.00023336
Iteration 14/25 | Loss: 0.00023336
Iteration 15/25 | Loss: 0.00023336
Iteration 16/25 | Loss: 0.00023336
Iteration 17/25 | Loss: 0.00023336
Iteration 18/25 | Loss: 0.00023336
Iteration 19/25 | Loss: 0.00023336
Iteration 20/25 | Loss: 0.00023336
Iteration 21/25 | Loss: 0.00023336
Iteration 22/25 | Loss: 0.00023336
Iteration 23/25 | Loss: 0.00023336
Iteration 24/25 | Loss: 0.00023336
Iteration 25/25 | Loss: 0.00023336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023336
Iteration 2/1000 | Loss: 0.00012404
Iteration 3/1000 | Loss: 0.00002718
Iteration 4/1000 | Loss: 0.00009707
Iteration 5/1000 | Loss: 0.00002416
Iteration 6/1000 | Loss: 0.00002366
Iteration 7/1000 | Loss: 0.00002312
Iteration 8/1000 | Loss: 0.00009012
Iteration 9/1000 | Loss: 0.00024818
Iteration 10/1000 | Loss: 0.00009603
Iteration 11/1000 | Loss: 0.00002264
Iteration 12/1000 | Loss: 0.00002238
Iteration 13/1000 | Loss: 0.00002216
Iteration 14/1000 | Loss: 0.00002214
Iteration 15/1000 | Loss: 0.00002213
Iteration 16/1000 | Loss: 0.00002213
Iteration 17/1000 | Loss: 0.00002205
Iteration 18/1000 | Loss: 0.00002205
Iteration 19/1000 | Loss: 0.00002205
Iteration 20/1000 | Loss: 0.00002205
Iteration 21/1000 | Loss: 0.00002204
Iteration 22/1000 | Loss: 0.00002204
Iteration 23/1000 | Loss: 0.00002204
Iteration 24/1000 | Loss: 0.00002204
Iteration 25/1000 | Loss: 0.00002204
Iteration 26/1000 | Loss: 0.00002204
Iteration 27/1000 | Loss: 0.00002204
Iteration 28/1000 | Loss: 0.00002203
Iteration 29/1000 | Loss: 0.00002197
Iteration 30/1000 | Loss: 0.00002197
Iteration 31/1000 | Loss: 0.00002194
Iteration 32/1000 | Loss: 0.00002193
Iteration 33/1000 | Loss: 0.00002193
Iteration 34/1000 | Loss: 0.00002193
Iteration 35/1000 | Loss: 0.00002192
Iteration 36/1000 | Loss: 0.00002192
Iteration 37/1000 | Loss: 0.00002192
Iteration 38/1000 | Loss: 0.00002192
Iteration 39/1000 | Loss: 0.00002192
Iteration 40/1000 | Loss: 0.00002191
Iteration 41/1000 | Loss: 0.00002191
Iteration 42/1000 | Loss: 0.00002191
Iteration 43/1000 | Loss: 0.00002190
Iteration 44/1000 | Loss: 0.00002190
Iteration 45/1000 | Loss: 0.00002190
Iteration 46/1000 | Loss: 0.00002189
Iteration 47/1000 | Loss: 0.00002187
Iteration 48/1000 | Loss: 0.00002187
Iteration 49/1000 | Loss: 0.00002187
Iteration 50/1000 | Loss: 0.00002186
Iteration 51/1000 | Loss: 0.00002186
Iteration 52/1000 | Loss: 0.00002186
Iteration 53/1000 | Loss: 0.00002186
Iteration 54/1000 | Loss: 0.00002186
Iteration 55/1000 | Loss: 0.00002185
Iteration 56/1000 | Loss: 0.00002185
Iteration 57/1000 | Loss: 0.00002185
Iteration 58/1000 | Loss: 0.00002185
Iteration 59/1000 | Loss: 0.00002185
Iteration 60/1000 | Loss: 0.00002184
Iteration 61/1000 | Loss: 0.00002184
Iteration 62/1000 | Loss: 0.00002184
Iteration 63/1000 | Loss: 0.00002184
Iteration 64/1000 | Loss: 0.00002184
Iteration 65/1000 | Loss: 0.00002184
Iteration 66/1000 | Loss: 0.00002184
Iteration 67/1000 | Loss: 0.00002184
Iteration 68/1000 | Loss: 0.00002184
Iteration 69/1000 | Loss: 0.00002184
Iteration 70/1000 | Loss: 0.00002183
Iteration 71/1000 | Loss: 0.00002183
Iteration 72/1000 | Loss: 0.00002183
Iteration 73/1000 | Loss: 0.00002183
Iteration 74/1000 | Loss: 0.00002183
Iteration 75/1000 | Loss: 0.00002183
Iteration 76/1000 | Loss: 0.00002183
Iteration 77/1000 | Loss: 0.00002183
Iteration 78/1000 | Loss: 0.00002183
Iteration 79/1000 | Loss: 0.00002183
Iteration 80/1000 | Loss: 0.00002183
Iteration 81/1000 | Loss: 0.00002183
Iteration 82/1000 | Loss: 0.00002183
Iteration 83/1000 | Loss: 0.00002183
Iteration 84/1000 | Loss: 0.00002183
Iteration 85/1000 | Loss: 0.00002183
Iteration 86/1000 | Loss: 0.00002183
Iteration 87/1000 | Loss: 0.00002183
Iteration 88/1000 | Loss: 0.00002183
Iteration 89/1000 | Loss: 0.00002183
Iteration 90/1000 | Loss: 0.00002183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [2.1833089704159647e-05, 2.1833089704159647e-05, 2.1833089704159647e-05, 2.1833089704159647e-05, 2.1833089704159647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1833089704159647e-05

Optimization complete. Final v2v error: 3.91640043258667 mm

Highest mean error: 4.423427581787109 mm for frame 138

Lowest mean error: 3.544797658920288 mm for frame 60

Saving results

Total time: 38.38302969932556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997682
Iteration 2/25 | Loss: 0.00422960
Iteration 3/25 | Loss: 0.00270014
Iteration 4/25 | Loss: 0.00221889
Iteration 5/25 | Loss: 0.00187422
Iteration 6/25 | Loss: 0.00168679
Iteration 7/25 | Loss: 0.00176880
Iteration 8/25 | Loss: 0.00156350
Iteration 9/25 | Loss: 0.00143131
Iteration 10/25 | Loss: 0.00135578
Iteration 11/25 | Loss: 0.00132513
Iteration 12/25 | Loss: 0.00130319
Iteration 13/25 | Loss: 0.00125857
Iteration 14/25 | Loss: 0.00120069
Iteration 15/25 | Loss: 0.00117481
Iteration 16/25 | Loss: 0.00115572
Iteration 17/25 | Loss: 0.00115346
Iteration 18/25 | Loss: 0.00114396
Iteration 19/25 | Loss: 0.00114141
Iteration 20/25 | Loss: 0.00113879
Iteration 21/25 | Loss: 0.00112665
Iteration 22/25 | Loss: 0.00113267
Iteration 23/25 | Loss: 0.00111862
Iteration 24/25 | Loss: 0.00110485
Iteration 25/25 | Loss: 0.00110120

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45298874
Iteration 2/25 | Loss: 0.00288277
Iteration 3/25 | Loss: 0.00288276
Iteration 4/25 | Loss: 0.00288276
Iteration 5/25 | Loss: 0.00288276
Iteration 6/25 | Loss: 0.00288276
Iteration 7/25 | Loss: 0.00288276
Iteration 8/25 | Loss: 0.00288276
Iteration 9/25 | Loss: 0.00288276
Iteration 10/25 | Loss: 0.00288276
Iteration 11/25 | Loss: 0.00288276
Iteration 12/25 | Loss: 0.00288276
Iteration 13/25 | Loss: 0.00288276
Iteration 14/25 | Loss: 0.00288276
Iteration 15/25 | Loss: 0.00288276
Iteration 16/25 | Loss: 0.00288276
Iteration 17/25 | Loss: 0.00288276
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002882756292819977, 0.002882756292819977, 0.002882756292819977, 0.002882756292819977, 0.002882756292819977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002882756292819977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288276
Iteration 2/1000 | Loss: 0.00513057
Iteration 3/1000 | Loss: 0.00168983
Iteration 4/1000 | Loss: 0.00091479
Iteration 5/1000 | Loss: 0.00043623
Iteration 6/1000 | Loss: 0.00031290
Iteration 7/1000 | Loss: 0.00112179
Iteration 8/1000 | Loss: 0.00453719
Iteration 9/1000 | Loss: 0.00267833
Iteration 10/1000 | Loss: 0.00106401
Iteration 11/1000 | Loss: 0.00088961
Iteration 12/1000 | Loss: 0.00070173
Iteration 13/1000 | Loss: 0.00122970
Iteration 14/1000 | Loss: 0.00055539
Iteration 15/1000 | Loss: 0.00177821
Iteration 16/1000 | Loss: 0.00335848
Iteration 17/1000 | Loss: 0.00178401
Iteration 18/1000 | Loss: 0.00103683
Iteration 19/1000 | Loss: 0.00090628
Iteration 20/1000 | Loss: 0.00133272
Iteration 21/1000 | Loss: 0.00089319
Iteration 22/1000 | Loss: 0.00119593
Iteration 23/1000 | Loss: 0.00058207
Iteration 24/1000 | Loss: 0.00146449
Iteration 25/1000 | Loss: 0.00074705
Iteration 26/1000 | Loss: 0.00092287
Iteration 27/1000 | Loss: 0.00129696
Iteration 28/1000 | Loss: 0.00061673
Iteration 29/1000 | Loss: 0.00081202
Iteration 30/1000 | Loss: 0.00030364
Iteration 31/1000 | Loss: 0.00042991
Iteration 32/1000 | Loss: 0.00033283
Iteration 33/1000 | Loss: 0.00046163
Iteration 34/1000 | Loss: 0.00052332
Iteration 35/1000 | Loss: 0.00027683
Iteration 36/1000 | Loss: 0.00053432
Iteration 37/1000 | Loss: 0.00024488
Iteration 38/1000 | Loss: 0.00030102
Iteration 39/1000 | Loss: 0.00022210
Iteration 40/1000 | Loss: 0.00020319
Iteration 41/1000 | Loss: 0.00021046
Iteration 42/1000 | Loss: 0.00019028
Iteration 43/1000 | Loss: 0.00011847
Iteration 44/1000 | Loss: 0.00011707
Iteration 45/1000 | Loss: 0.00012627
Iteration 46/1000 | Loss: 0.00016536
Iteration 47/1000 | Loss: 0.00028258
Iteration 48/1000 | Loss: 0.00020121
Iteration 49/1000 | Loss: 0.00016223
Iteration 50/1000 | Loss: 0.00014064
Iteration 51/1000 | Loss: 0.00020791
Iteration 52/1000 | Loss: 0.00017385
Iteration 53/1000 | Loss: 0.00035506
Iteration 54/1000 | Loss: 0.00025061
Iteration 55/1000 | Loss: 0.00012669
Iteration 56/1000 | Loss: 0.00014196
Iteration 57/1000 | Loss: 0.00024849
Iteration 58/1000 | Loss: 0.00017412
Iteration 59/1000 | Loss: 0.00021338
Iteration 60/1000 | Loss: 0.00014928
Iteration 61/1000 | Loss: 0.00015607
Iteration 62/1000 | Loss: 0.00014224
Iteration 63/1000 | Loss: 0.00021245
Iteration 64/1000 | Loss: 0.00007125
Iteration 65/1000 | Loss: 0.00016251
Iteration 66/1000 | Loss: 0.00011355
Iteration 67/1000 | Loss: 0.00007196
Iteration 68/1000 | Loss: 0.00014674
Iteration 69/1000 | Loss: 0.00013543
Iteration 70/1000 | Loss: 0.00015631
Iteration 71/1000 | Loss: 0.00014869
Iteration 72/1000 | Loss: 0.00007452
Iteration 73/1000 | Loss: 0.00008890
Iteration 74/1000 | Loss: 0.00007610
Iteration 75/1000 | Loss: 0.00007276
Iteration 76/1000 | Loss: 0.00006144
Iteration 77/1000 | Loss: 0.00006560
Iteration 78/1000 | Loss: 0.00004603
Iteration 79/1000 | Loss: 0.00004897
Iteration 80/1000 | Loss: 0.00006110
Iteration 81/1000 | Loss: 0.00006101
Iteration 82/1000 | Loss: 0.00007448
Iteration 83/1000 | Loss: 0.00008631
Iteration 84/1000 | Loss: 0.00023333
Iteration 85/1000 | Loss: 0.00014138
Iteration 86/1000 | Loss: 0.00021668
Iteration 87/1000 | Loss: 0.00014165
Iteration 88/1000 | Loss: 0.00007983
Iteration 89/1000 | Loss: 0.00005908
Iteration 90/1000 | Loss: 0.00017922
Iteration 91/1000 | Loss: 0.00015839
Iteration 92/1000 | Loss: 0.00009202
Iteration 93/1000 | Loss: 0.00022270
Iteration 94/1000 | Loss: 0.00009416
Iteration 95/1000 | Loss: 0.00015803
Iteration 96/1000 | Loss: 0.00013795
Iteration 97/1000 | Loss: 0.00007904
Iteration 98/1000 | Loss: 0.00013779
Iteration 99/1000 | Loss: 0.00012311
Iteration 100/1000 | Loss: 0.00016078
Iteration 101/1000 | Loss: 0.00018580
Iteration 102/1000 | Loss: 0.00012630
Iteration 103/1000 | Loss: 0.00008987
Iteration 104/1000 | Loss: 0.00008809
Iteration 105/1000 | Loss: 0.00019603
Iteration 106/1000 | Loss: 0.00013566
Iteration 107/1000 | Loss: 0.00006268
Iteration 108/1000 | Loss: 0.00010760
Iteration 109/1000 | Loss: 0.00028126
Iteration 110/1000 | Loss: 0.00013583
Iteration 111/1000 | Loss: 0.00020866
Iteration 112/1000 | Loss: 0.00006981
Iteration 113/1000 | Loss: 0.00004647
Iteration 114/1000 | Loss: 0.00004477
Iteration 115/1000 | Loss: 0.00003460
Iteration 116/1000 | Loss: 0.00004323
Iteration 117/1000 | Loss: 0.00004005
Iteration 118/1000 | Loss: 0.00009250
Iteration 119/1000 | Loss: 0.00006737
Iteration 120/1000 | Loss: 0.00003944
Iteration 121/1000 | Loss: 0.00003946
Iteration 122/1000 | Loss: 0.00003950
Iteration 123/1000 | Loss: 0.00004373
Iteration 124/1000 | Loss: 0.00003581
Iteration 125/1000 | Loss: 0.00003197
Iteration 126/1000 | Loss: 0.00004275
Iteration 127/1000 | Loss: 0.00003518
Iteration 128/1000 | Loss: 0.00003567
Iteration 129/1000 | Loss: 0.00003353
Iteration 130/1000 | Loss: 0.00003986
Iteration 131/1000 | Loss: 0.00003199
Iteration 132/1000 | Loss: 0.00004322
Iteration 133/1000 | Loss: 0.00003018
Iteration 134/1000 | Loss: 0.00004485
Iteration 135/1000 | Loss: 0.00002829
Iteration 136/1000 | Loss: 0.00003932
Iteration 137/1000 | Loss: 0.00003062
Iteration 138/1000 | Loss: 0.00002751
Iteration 139/1000 | Loss: 0.00002828
Iteration 140/1000 | Loss: 0.00002922
Iteration 141/1000 | Loss: 0.00002731
Iteration 142/1000 | Loss: 0.00002971
Iteration 143/1000 | Loss: 0.00003483
Iteration 144/1000 | Loss: 0.00003364
Iteration 145/1000 | Loss: 0.00003694
Iteration 146/1000 | Loss: 0.00003425
Iteration 147/1000 | Loss: 0.00003229
Iteration 148/1000 | Loss: 0.00006259
Iteration 149/1000 | Loss: 0.00003481
Iteration 150/1000 | Loss: 0.00005631
Iteration 151/1000 | Loss: 0.00003149
Iteration 152/1000 | Loss: 0.00002584
Iteration 153/1000 | Loss: 0.00002496
Iteration 154/1000 | Loss: 0.00002466
Iteration 155/1000 | Loss: 0.00002418
Iteration 156/1000 | Loss: 0.00002413
Iteration 157/1000 | Loss: 0.00002391
Iteration 158/1000 | Loss: 0.00056900
Iteration 159/1000 | Loss: 0.00020467
Iteration 160/1000 | Loss: 0.00003892
Iteration 161/1000 | Loss: 0.00002826
Iteration 162/1000 | Loss: 0.00002407
Iteration 163/1000 | Loss: 0.00002164
Iteration 164/1000 | Loss: 0.00001940
Iteration 165/1000 | Loss: 0.00001826
Iteration 166/1000 | Loss: 0.00001776
Iteration 167/1000 | Loss: 0.00001735
Iteration 168/1000 | Loss: 0.00001704
Iteration 169/1000 | Loss: 0.00001690
Iteration 170/1000 | Loss: 0.00001689
Iteration 171/1000 | Loss: 0.00001676
Iteration 172/1000 | Loss: 0.00001672
Iteration 173/1000 | Loss: 0.00001672
Iteration 174/1000 | Loss: 0.00001671
Iteration 175/1000 | Loss: 0.00001657
Iteration 176/1000 | Loss: 0.00001653
Iteration 177/1000 | Loss: 0.00001653
Iteration 178/1000 | Loss: 0.00001651
Iteration 179/1000 | Loss: 0.00001650
Iteration 180/1000 | Loss: 0.00001650
Iteration 181/1000 | Loss: 0.00001649
Iteration 182/1000 | Loss: 0.00001647
Iteration 183/1000 | Loss: 0.00001647
Iteration 184/1000 | Loss: 0.00001647
Iteration 185/1000 | Loss: 0.00001647
Iteration 186/1000 | Loss: 0.00001647
Iteration 187/1000 | Loss: 0.00001647
Iteration 188/1000 | Loss: 0.00001647
Iteration 189/1000 | Loss: 0.00001647
Iteration 190/1000 | Loss: 0.00001647
Iteration 191/1000 | Loss: 0.00001647
Iteration 192/1000 | Loss: 0.00001647
Iteration 193/1000 | Loss: 0.00001647
Iteration 194/1000 | Loss: 0.00001646
Iteration 195/1000 | Loss: 0.00001646
Iteration 196/1000 | Loss: 0.00001646
Iteration 197/1000 | Loss: 0.00001646
Iteration 198/1000 | Loss: 0.00001646
Iteration 199/1000 | Loss: 0.00001646
Iteration 200/1000 | Loss: 0.00001645
Iteration 201/1000 | Loss: 0.00001645
Iteration 202/1000 | Loss: 0.00001645
Iteration 203/1000 | Loss: 0.00001645
Iteration 204/1000 | Loss: 0.00001645
Iteration 205/1000 | Loss: 0.00001645
Iteration 206/1000 | Loss: 0.00001645
Iteration 207/1000 | Loss: 0.00001645
Iteration 208/1000 | Loss: 0.00001645
Iteration 209/1000 | Loss: 0.00001645
Iteration 210/1000 | Loss: 0.00001645
Iteration 211/1000 | Loss: 0.00001645
Iteration 212/1000 | Loss: 0.00001645
Iteration 213/1000 | Loss: 0.00001645
Iteration 214/1000 | Loss: 0.00001644
Iteration 215/1000 | Loss: 0.00001644
Iteration 216/1000 | Loss: 0.00001644
Iteration 217/1000 | Loss: 0.00001644
Iteration 218/1000 | Loss: 0.00001643
Iteration 219/1000 | Loss: 0.00001643
Iteration 220/1000 | Loss: 0.00001643
Iteration 221/1000 | Loss: 0.00001642
Iteration 222/1000 | Loss: 0.00001642
Iteration 223/1000 | Loss: 0.00001641
Iteration 224/1000 | Loss: 0.00001641
Iteration 225/1000 | Loss: 0.00001641
Iteration 226/1000 | Loss: 0.00001641
Iteration 227/1000 | Loss: 0.00001641
Iteration 228/1000 | Loss: 0.00001641
Iteration 229/1000 | Loss: 0.00001641
Iteration 230/1000 | Loss: 0.00001641
Iteration 231/1000 | Loss: 0.00001641
Iteration 232/1000 | Loss: 0.00001640
Iteration 233/1000 | Loss: 0.00001640
Iteration 234/1000 | Loss: 0.00001640
Iteration 235/1000 | Loss: 0.00001640
Iteration 236/1000 | Loss: 0.00001640
Iteration 237/1000 | Loss: 0.00001640
Iteration 238/1000 | Loss: 0.00001640
Iteration 239/1000 | Loss: 0.00001640
Iteration 240/1000 | Loss: 0.00001640
Iteration 241/1000 | Loss: 0.00001640
Iteration 242/1000 | Loss: 0.00001640
Iteration 243/1000 | Loss: 0.00001640
Iteration 244/1000 | Loss: 0.00001640
Iteration 245/1000 | Loss: 0.00001640
Iteration 246/1000 | Loss: 0.00001640
Iteration 247/1000 | Loss: 0.00001640
Iteration 248/1000 | Loss: 0.00001640
Iteration 249/1000 | Loss: 0.00001639
Iteration 250/1000 | Loss: 0.00001639
Iteration 251/1000 | Loss: 0.00001639
Iteration 252/1000 | Loss: 0.00001639
Iteration 253/1000 | Loss: 0.00001639
Iteration 254/1000 | Loss: 0.00001639
Iteration 255/1000 | Loss: 0.00001639
Iteration 256/1000 | Loss: 0.00001639
Iteration 257/1000 | Loss: 0.00001639
Iteration 258/1000 | Loss: 0.00001639
Iteration 259/1000 | Loss: 0.00001639
Iteration 260/1000 | Loss: 0.00001638
Iteration 261/1000 | Loss: 0.00001638
Iteration 262/1000 | Loss: 0.00001638
Iteration 263/1000 | Loss: 0.00001638
Iteration 264/1000 | Loss: 0.00001638
Iteration 265/1000 | Loss: 0.00001638
Iteration 266/1000 | Loss: 0.00001638
Iteration 267/1000 | Loss: 0.00001638
Iteration 268/1000 | Loss: 0.00001638
Iteration 269/1000 | Loss: 0.00001638
Iteration 270/1000 | Loss: 0.00001638
Iteration 271/1000 | Loss: 0.00001638
Iteration 272/1000 | Loss: 0.00001637
Iteration 273/1000 | Loss: 0.00001637
Iteration 274/1000 | Loss: 0.00001637
Iteration 275/1000 | Loss: 0.00001637
Iteration 276/1000 | Loss: 0.00001637
Iteration 277/1000 | Loss: 0.00001637
Iteration 278/1000 | Loss: 0.00001637
Iteration 279/1000 | Loss: 0.00001637
Iteration 280/1000 | Loss: 0.00001637
Iteration 281/1000 | Loss: 0.00001637
Iteration 282/1000 | Loss: 0.00001637
Iteration 283/1000 | Loss: 0.00001637
Iteration 284/1000 | Loss: 0.00001637
Iteration 285/1000 | Loss: 0.00001636
Iteration 286/1000 | Loss: 0.00001636
Iteration 287/1000 | Loss: 0.00001636
Iteration 288/1000 | Loss: 0.00001636
Iteration 289/1000 | Loss: 0.00001636
Iteration 290/1000 | Loss: 0.00001636
Iteration 291/1000 | Loss: 0.00001636
Iteration 292/1000 | Loss: 0.00001636
Iteration 293/1000 | Loss: 0.00001636
Iteration 294/1000 | Loss: 0.00001636
Iteration 295/1000 | Loss: 0.00001636
Iteration 296/1000 | Loss: 0.00001635
Iteration 297/1000 | Loss: 0.00001635
Iteration 298/1000 | Loss: 0.00001635
Iteration 299/1000 | Loss: 0.00001635
Iteration 300/1000 | Loss: 0.00001635
Iteration 301/1000 | Loss: 0.00001635
Iteration 302/1000 | Loss: 0.00001635
Iteration 303/1000 | Loss: 0.00001635
Iteration 304/1000 | Loss: 0.00001635
Iteration 305/1000 | Loss: 0.00001635
Iteration 306/1000 | Loss: 0.00001635
Iteration 307/1000 | Loss: 0.00001635
Iteration 308/1000 | Loss: 0.00001634
Iteration 309/1000 | Loss: 0.00001634
Iteration 310/1000 | Loss: 0.00001634
Iteration 311/1000 | Loss: 0.00001634
Iteration 312/1000 | Loss: 0.00001634
Iteration 313/1000 | Loss: 0.00001634
Iteration 314/1000 | Loss: 0.00001634
Iteration 315/1000 | Loss: 0.00001634
Iteration 316/1000 | Loss: 0.00001634
Iteration 317/1000 | Loss: 0.00001634
Iteration 318/1000 | Loss: 0.00001634
Iteration 319/1000 | Loss: 0.00001634
Iteration 320/1000 | Loss: 0.00001634
Iteration 321/1000 | Loss: 0.00001634
Iteration 322/1000 | Loss: 0.00001634
Iteration 323/1000 | Loss: 0.00001634
Iteration 324/1000 | Loss: 0.00001634
Iteration 325/1000 | Loss: 0.00001634
Iteration 326/1000 | Loss: 0.00001634
Iteration 327/1000 | Loss: 0.00001634
Iteration 328/1000 | Loss: 0.00001634
Iteration 329/1000 | Loss: 0.00001634
Iteration 330/1000 | Loss: 0.00001634
Iteration 331/1000 | Loss: 0.00001634
Iteration 332/1000 | Loss: 0.00001634
Iteration 333/1000 | Loss: 0.00001634
Iteration 334/1000 | Loss: 0.00001634
Iteration 335/1000 | Loss: 0.00001634
Iteration 336/1000 | Loss: 0.00001634
Iteration 337/1000 | Loss: 0.00001634
Iteration 338/1000 | Loss: 0.00001634
Iteration 339/1000 | Loss: 0.00001634
Iteration 340/1000 | Loss: 0.00001634
Iteration 341/1000 | Loss: 0.00001634
Iteration 342/1000 | Loss: 0.00001634
Iteration 343/1000 | Loss: 0.00001634
Iteration 344/1000 | Loss: 0.00001634
Iteration 345/1000 | Loss: 0.00001634
Iteration 346/1000 | Loss: 0.00001634
Iteration 347/1000 | Loss: 0.00001634
Iteration 348/1000 | Loss: 0.00001634
Iteration 349/1000 | Loss: 0.00001634
Iteration 350/1000 | Loss: 0.00001634
Iteration 351/1000 | Loss: 0.00001634
Iteration 352/1000 | Loss: 0.00001634
Iteration 353/1000 | Loss: 0.00001634
Iteration 354/1000 | Loss: 0.00001634
Iteration 355/1000 | Loss: 0.00001634
Iteration 356/1000 | Loss: 0.00001634
Iteration 357/1000 | Loss: 0.00001634
Iteration 358/1000 | Loss: 0.00001634
Iteration 359/1000 | Loss: 0.00001634
Iteration 360/1000 | Loss: 0.00001634
Iteration 361/1000 | Loss: 0.00001634
Iteration 362/1000 | Loss: 0.00001634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 362. Stopping optimization.
Last 5 losses: [1.634333784750197e-05, 1.634333784750197e-05, 1.634333784750197e-05, 1.634333784750197e-05, 1.634333784750197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.634333784750197e-05

Optimization complete. Final v2v error: 3.3803293704986572 mm

Highest mean error: 3.696192979812622 mm for frame 147

Lowest mean error: 3.1151416301727295 mm for frame 80

Saving results

Total time: 336.56993532180786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840218
Iteration 2/25 | Loss: 0.00109980
Iteration 3/25 | Loss: 0.00068816
Iteration 4/25 | Loss: 0.00073558
Iteration 5/25 | Loss: 0.00068190
Iteration 6/25 | Loss: 0.00062550
Iteration 7/25 | Loss: 0.00062296
Iteration 8/25 | Loss: 0.00062236
Iteration 9/25 | Loss: 0.00062223
Iteration 10/25 | Loss: 0.00062223
Iteration 11/25 | Loss: 0.00062223
Iteration 12/25 | Loss: 0.00062223
Iteration 13/25 | Loss: 0.00062223
Iteration 14/25 | Loss: 0.00062223
Iteration 15/25 | Loss: 0.00062223
Iteration 16/25 | Loss: 0.00062223
Iteration 17/25 | Loss: 0.00062223
Iteration 18/25 | Loss: 0.00062223
Iteration 19/25 | Loss: 0.00062223
Iteration 20/25 | Loss: 0.00062223
Iteration 21/25 | Loss: 0.00062223
Iteration 22/25 | Loss: 0.00062223
Iteration 23/25 | Loss: 0.00062223
Iteration 24/25 | Loss: 0.00062223
Iteration 25/25 | Loss: 0.00062223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.82589483
Iteration 2/25 | Loss: 0.00031903
Iteration 3/25 | Loss: 0.00031903
Iteration 4/25 | Loss: 0.00031903
Iteration 5/25 | Loss: 0.00031903
Iteration 6/25 | Loss: 0.00031903
Iteration 7/25 | Loss: 0.00031903
Iteration 8/25 | Loss: 0.00031903
Iteration 9/25 | Loss: 0.00031903
Iteration 10/25 | Loss: 0.00031903
Iteration 11/25 | Loss: 0.00031903
Iteration 12/25 | Loss: 0.00031903
Iteration 13/25 | Loss: 0.00031903
Iteration 14/25 | Loss: 0.00031903
Iteration 15/25 | Loss: 0.00031903
Iteration 16/25 | Loss: 0.00031903
Iteration 17/25 | Loss: 0.00031903
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00031902670161798596, 0.00031902670161798596, 0.00031902670161798596, 0.00031902670161798596, 0.00031902670161798596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031902670161798596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031903
Iteration 2/1000 | Loss: 0.00002576
Iteration 3/1000 | Loss: 0.00001587
Iteration 4/1000 | Loss: 0.00001473
Iteration 5/1000 | Loss: 0.00001404
Iteration 6/1000 | Loss: 0.00001370
Iteration 7/1000 | Loss: 0.00001346
Iteration 8/1000 | Loss: 0.00001339
Iteration 9/1000 | Loss: 0.00001317
Iteration 10/1000 | Loss: 0.00001301
Iteration 11/1000 | Loss: 0.00001288
Iteration 12/1000 | Loss: 0.00001287
Iteration 13/1000 | Loss: 0.00001287
Iteration 14/1000 | Loss: 0.00001286
Iteration 15/1000 | Loss: 0.00001283
Iteration 16/1000 | Loss: 0.00001280
Iteration 17/1000 | Loss: 0.00001279
Iteration 18/1000 | Loss: 0.00001277
Iteration 19/1000 | Loss: 0.00001276
Iteration 20/1000 | Loss: 0.00001274
Iteration 21/1000 | Loss: 0.00001269
Iteration 22/1000 | Loss: 0.00001268
Iteration 23/1000 | Loss: 0.00001267
Iteration 24/1000 | Loss: 0.00001266
Iteration 25/1000 | Loss: 0.00001266
Iteration 26/1000 | Loss: 0.00001265
Iteration 27/1000 | Loss: 0.00001265
Iteration 28/1000 | Loss: 0.00001264
Iteration 29/1000 | Loss: 0.00001264
Iteration 30/1000 | Loss: 0.00001264
Iteration 31/1000 | Loss: 0.00001263
Iteration 32/1000 | Loss: 0.00001263
Iteration 33/1000 | Loss: 0.00001263
Iteration 34/1000 | Loss: 0.00001262
Iteration 35/1000 | Loss: 0.00001262
Iteration 36/1000 | Loss: 0.00001262
Iteration 37/1000 | Loss: 0.00001261
Iteration 38/1000 | Loss: 0.00001261
Iteration 39/1000 | Loss: 0.00001261
Iteration 40/1000 | Loss: 0.00001261
Iteration 41/1000 | Loss: 0.00001260
Iteration 42/1000 | Loss: 0.00001260
Iteration 43/1000 | Loss: 0.00001260
Iteration 44/1000 | Loss: 0.00001259
Iteration 45/1000 | Loss: 0.00001258
Iteration 46/1000 | Loss: 0.00001258
Iteration 47/1000 | Loss: 0.00001258
Iteration 48/1000 | Loss: 0.00001258
Iteration 49/1000 | Loss: 0.00001258
Iteration 50/1000 | Loss: 0.00001258
Iteration 51/1000 | Loss: 0.00001258
Iteration 52/1000 | Loss: 0.00001258
Iteration 53/1000 | Loss: 0.00001257
Iteration 54/1000 | Loss: 0.00001257
Iteration 55/1000 | Loss: 0.00001256
Iteration 56/1000 | Loss: 0.00001256
Iteration 57/1000 | Loss: 0.00001256
Iteration 58/1000 | Loss: 0.00001256
Iteration 59/1000 | Loss: 0.00001255
Iteration 60/1000 | Loss: 0.00001255
Iteration 61/1000 | Loss: 0.00001255
Iteration 62/1000 | Loss: 0.00001255
Iteration 63/1000 | Loss: 0.00001255
Iteration 64/1000 | Loss: 0.00001255
Iteration 65/1000 | Loss: 0.00001254
Iteration 66/1000 | Loss: 0.00001253
Iteration 67/1000 | Loss: 0.00001253
Iteration 68/1000 | Loss: 0.00001253
Iteration 69/1000 | Loss: 0.00001252
Iteration 70/1000 | Loss: 0.00001251
Iteration 71/1000 | Loss: 0.00001251
Iteration 72/1000 | Loss: 0.00001251
Iteration 73/1000 | Loss: 0.00001251
Iteration 74/1000 | Loss: 0.00001251
Iteration 75/1000 | Loss: 0.00001250
Iteration 76/1000 | Loss: 0.00001250
Iteration 77/1000 | Loss: 0.00001250
Iteration 78/1000 | Loss: 0.00001250
Iteration 79/1000 | Loss: 0.00001250
Iteration 80/1000 | Loss: 0.00001249
Iteration 81/1000 | Loss: 0.00001249
Iteration 82/1000 | Loss: 0.00001249
Iteration 83/1000 | Loss: 0.00001249
Iteration 84/1000 | Loss: 0.00001249
Iteration 85/1000 | Loss: 0.00001248
Iteration 86/1000 | Loss: 0.00001248
Iteration 87/1000 | Loss: 0.00001248
Iteration 88/1000 | Loss: 0.00001248
Iteration 89/1000 | Loss: 0.00001248
Iteration 90/1000 | Loss: 0.00001248
Iteration 91/1000 | Loss: 0.00001247
Iteration 92/1000 | Loss: 0.00001247
Iteration 93/1000 | Loss: 0.00001247
Iteration 94/1000 | Loss: 0.00001247
Iteration 95/1000 | Loss: 0.00001247
Iteration 96/1000 | Loss: 0.00001247
Iteration 97/1000 | Loss: 0.00001247
Iteration 98/1000 | Loss: 0.00001246
Iteration 99/1000 | Loss: 0.00001246
Iteration 100/1000 | Loss: 0.00001246
Iteration 101/1000 | Loss: 0.00001246
Iteration 102/1000 | Loss: 0.00001246
Iteration 103/1000 | Loss: 0.00001246
Iteration 104/1000 | Loss: 0.00001246
Iteration 105/1000 | Loss: 0.00001246
Iteration 106/1000 | Loss: 0.00001246
Iteration 107/1000 | Loss: 0.00001246
Iteration 108/1000 | Loss: 0.00001246
Iteration 109/1000 | Loss: 0.00001246
Iteration 110/1000 | Loss: 0.00001245
Iteration 111/1000 | Loss: 0.00001245
Iteration 112/1000 | Loss: 0.00001245
Iteration 113/1000 | Loss: 0.00001245
Iteration 114/1000 | Loss: 0.00001245
Iteration 115/1000 | Loss: 0.00001245
Iteration 116/1000 | Loss: 0.00001245
Iteration 117/1000 | Loss: 0.00001245
Iteration 118/1000 | Loss: 0.00001245
Iteration 119/1000 | Loss: 0.00001245
Iteration 120/1000 | Loss: 0.00001245
Iteration 121/1000 | Loss: 0.00001245
Iteration 122/1000 | Loss: 0.00001245
Iteration 123/1000 | Loss: 0.00001245
Iteration 124/1000 | Loss: 0.00001245
Iteration 125/1000 | Loss: 0.00001245
Iteration 126/1000 | Loss: 0.00001245
Iteration 127/1000 | Loss: 0.00001245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.245021394424839e-05, 1.245021394424839e-05, 1.245021394424839e-05, 1.245021394424839e-05, 1.245021394424839e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.245021394424839e-05

Optimization complete. Final v2v error: 3.00512957572937 mm

Highest mean error: 3.1905128955841064 mm for frame 197

Lowest mean error: 2.81242036819458 mm for frame 80

Saving results

Total time: 43.354191064834595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075082
Iteration 2/25 | Loss: 0.00133509
Iteration 3/25 | Loss: 0.00093486
Iteration 4/25 | Loss: 0.00119105
Iteration 5/25 | Loss: 0.00083879
Iteration 6/25 | Loss: 0.00074807
Iteration 7/25 | Loss: 0.00073239
Iteration 8/25 | Loss: 0.00072634
Iteration 9/25 | Loss: 0.00072259
Iteration 10/25 | Loss: 0.00071970
Iteration 11/25 | Loss: 0.00071720
Iteration 12/25 | Loss: 0.00071571
Iteration 13/25 | Loss: 0.00074367
Iteration 14/25 | Loss: 0.00076909
Iteration 15/25 | Loss: 0.00072642
Iteration 16/25 | Loss: 0.00068726
Iteration 17/25 | Loss: 0.00067598
Iteration 18/25 | Loss: 0.00067389
Iteration 19/25 | Loss: 0.00067376
Iteration 20/25 | Loss: 0.00067376
Iteration 21/25 | Loss: 0.00067376
Iteration 22/25 | Loss: 0.00067376
Iteration 23/25 | Loss: 0.00067376
Iteration 24/25 | Loss: 0.00067376
Iteration 25/25 | Loss: 0.00067376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.68121243
Iteration 2/25 | Loss: 0.00043489
Iteration 3/25 | Loss: 0.00043488
Iteration 4/25 | Loss: 0.00043488
Iteration 5/25 | Loss: 0.00043488
Iteration 6/25 | Loss: 0.00043488
Iteration 7/25 | Loss: 0.00043488
Iteration 8/25 | Loss: 0.00043488
Iteration 9/25 | Loss: 0.00043488
Iteration 10/25 | Loss: 0.00043488
Iteration 11/25 | Loss: 0.00043488
Iteration 12/25 | Loss: 0.00043488
Iteration 13/25 | Loss: 0.00043488
Iteration 14/25 | Loss: 0.00043488
Iteration 15/25 | Loss: 0.00043488
Iteration 16/25 | Loss: 0.00043488
Iteration 17/25 | Loss: 0.00043488
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00043487921357154846, 0.00043487921357154846, 0.00043487921357154846, 0.00043487921357154846, 0.00043487921357154846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00043487921357154846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043488
Iteration 2/1000 | Loss: 0.00162353
Iteration 3/1000 | Loss: 0.00151129
Iteration 4/1000 | Loss: 0.00003461
Iteration 5/1000 | Loss: 0.00002488
Iteration 6/1000 | Loss: 0.00018676
Iteration 7/1000 | Loss: 0.00002798
Iteration 8/1000 | Loss: 0.00002236
Iteration 9/1000 | Loss: 0.00002013
Iteration 10/1000 | Loss: 0.00001896
Iteration 11/1000 | Loss: 0.00001805
Iteration 12/1000 | Loss: 0.00001756
Iteration 13/1000 | Loss: 0.00045103
Iteration 14/1000 | Loss: 0.00034684
Iteration 15/1000 | Loss: 0.00002308
Iteration 16/1000 | Loss: 0.00001875
Iteration 17/1000 | Loss: 0.00001692
Iteration 18/1000 | Loss: 0.00001468
Iteration 19/1000 | Loss: 0.00001364
Iteration 20/1000 | Loss: 0.00001304
Iteration 21/1000 | Loss: 0.00001288
Iteration 22/1000 | Loss: 0.00001268
Iteration 23/1000 | Loss: 0.00001266
Iteration 24/1000 | Loss: 0.00001262
Iteration 25/1000 | Loss: 0.00001261
Iteration 26/1000 | Loss: 0.00001260
Iteration 27/1000 | Loss: 0.00001256
Iteration 28/1000 | Loss: 0.00001240
Iteration 29/1000 | Loss: 0.00001235
Iteration 30/1000 | Loss: 0.00001228
Iteration 31/1000 | Loss: 0.00001226
Iteration 32/1000 | Loss: 0.00001223
Iteration 33/1000 | Loss: 0.00001223
Iteration 34/1000 | Loss: 0.00001220
Iteration 35/1000 | Loss: 0.00001220
Iteration 36/1000 | Loss: 0.00001219
Iteration 37/1000 | Loss: 0.00001219
Iteration 38/1000 | Loss: 0.00001219
Iteration 39/1000 | Loss: 0.00001218
Iteration 40/1000 | Loss: 0.00001214
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00001213
Iteration 43/1000 | Loss: 0.00001213
Iteration 44/1000 | Loss: 0.00001213
Iteration 45/1000 | Loss: 0.00001213
Iteration 46/1000 | Loss: 0.00001213
Iteration 47/1000 | Loss: 0.00001213
Iteration 48/1000 | Loss: 0.00001213
Iteration 49/1000 | Loss: 0.00001213
Iteration 50/1000 | Loss: 0.00001213
Iteration 51/1000 | Loss: 0.00001213
Iteration 52/1000 | Loss: 0.00001211
Iteration 53/1000 | Loss: 0.00001210
Iteration 54/1000 | Loss: 0.00001210
Iteration 55/1000 | Loss: 0.00001210
Iteration 56/1000 | Loss: 0.00001210
Iteration 57/1000 | Loss: 0.00001210
Iteration 58/1000 | Loss: 0.00001210
Iteration 59/1000 | Loss: 0.00001210
Iteration 60/1000 | Loss: 0.00001209
Iteration 61/1000 | Loss: 0.00001209
Iteration 62/1000 | Loss: 0.00001209
Iteration 63/1000 | Loss: 0.00001208
Iteration 64/1000 | Loss: 0.00001208
Iteration 65/1000 | Loss: 0.00001208
Iteration 66/1000 | Loss: 0.00001208
Iteration 67/1000 | Loss: 0.00001208
Iteration 68/1000 | Loss: 0.00001208
Iteration 69/1000 | Loss: 0.00001207
Iteration 70/1000 | Loss: 0.00001207
Iteration 71/1000 | Loss: 0.00001207
Iteration 72/1000 | Loss: 0.00001207
Iteration 73/1000 | Loss: 0.00001206
Iteration 74/1000 | Loss: 0.00001206
Iteration 75/1000 | Loss: 0.00001206
Iteration 76/1000 | Loss: 0.00001206
Iteration 77/1000 | Loss: 0.00001206
Iteration 78/1000 | Loss: 0.00001205
Iteration 79/1000 | Loss: 0.00001205
Iteration 80/1000 | Loss: 0.00001205
Iteration 81/1000 | Loss: 0.00001205
Iteration 82/1000 | Loss: 0.00001204
Iteration 83/1000 | Loss: 0.00001204
Iteration 84/1000 | Loss: 0.00001204
Iteration 85/1000 | Loss: 0.00001204
Iteration 86/1000 | Loss: 0.00001203
Iteration 87/1000 | Loss: 0.00001203
Iteration 88/1000 | Loss: 0.00001203
Iteration 89/1000 | Loss: 0.00001203
Iteration 90/1000 | Loss: 0.00001202
Iteration 91/1000 | Loss: 0.00001202
Iteration 92/1000 | Loss: 0.00001202
Iteration 93/1000 | Loss: 0.00001202
Iteration 94/1000 | Loss: 0.00001202
Iteration 95/1000 | Loss: 0.00001202
Iteration 96/1000 | Loss: 0.00001201
Iteration 97/1000 | Loss: 0.00001201
Iteration 98/1000 | Loss: 0.00001201
Iteration 99/1000 | Loss: 0.00001201
Iteration 100/1000 | Loss: 0.00001201
Iteration 101/1000 | Loss: 0.00001201
Iteration 102/1000 | Loss: 0.00001201
Iteration 103/1000 | Loss: 0.00001201
Iteration 104/1000 | Loss: 0.00001201
Iteration 105/1000 | Loss: 0.00001201
Iteration 106/1000 | Loss: 0.00001201
Iteration 107/1000 | Loss: 0.00001200
Iteration 108/1000 | Loss: 0.00001200
Iteration 109/1000 | Loss: 0.00001200
Iteration 110/1000 | Loss: 0.00001200
Iteration 111/1000 | Loss: 0.00001200
Iteration 112/1000 | Loss: 0.00001200
Iteration 113/1000 | Loss: 0.00001200
Iteration 114/1000 | Loss: 0.00001200
Iteration 115/1000 | Loss: 0.00001200
Iteration 116/1000 | Loss: 0.00001200
Iteration 117/1000 | Loss: 0.00001200
Iteration 118/1000 | Loss: 0.00001200
Iteration 119/1000 | Loss: 0.00001200
Iteration 120/1000 | Loss: 0.00001200
Iteration 121/1000 | Loss: 0.00001200
Iteration 122/1000 | Loss: 0.00001200
Iteration 123/1000 | Loss: 0.00001200
Iteration 124/1000 | Loss: 0.00001200
Iteration 125/1000 | Loss: 0.00001199
Iteration 126/1000 | Loss: 0.00001199
Iteration 127/1000 | Loss: 0.00001199
Iteration 128/1000 | Loss: 0.00001199
Iteration 129/1000 | Loss: 0.00001199
Iteration 130/1000 | Loss: 0.00001199
Iteration 131/1000 | Loss: 0.00001199
Iteration 132/1000 | Loss: 0.00001199
Iteration 133/1000 | Loss: 0.00001199
Iteration 134/1000 | Loss: 0.00001199
Iteration 135/1000 | Loss: 0.00001199
Iteration 136/1000 | Loss: 0.00001199
Iteration 137/1000 | Loss: 0.00001199
Iteration 138/1000 | Loss: 0.00001199
Iteration 139/1000 | Loss: 0.00001199
Iteration 140/1000 | Loss: 0.00001199
Iteration 141/1000 | Loss: 0.00001199
Iteration 142/1000 | Loss: 0.00001199
Iteration 143/1000 | Loss: 0.00001199
Iteration 144/1000 | Loss: 0.00001199
Iteration 145/1000 | Loss: 0.00001199
Iteration 146/1000 | Loss: 0.00001199
Iteration 147/1000 | Loss: 0.00001199
Iteration 148/1000 | Loss: 0.00001198
Iteration 149/1000 | Loss: 0.00001198
Iteration 150/1000 | Loss: 0.00001198
Iteration 151/1000 | Loss: 0.00001198
Iteration 152/1000 | Loss: 0.00001198
Iteration 153/1000 | Loss: 0.00001198
Iteration 154/1000 | Loss: 0.00001198
Iteration 155/1000 | Loss: 0.00001198
Iteration 156/1000 | Loss: 0.00001198
Iteration 157/1000 | Loss: 0.00001198
Iteration 158/1000 | Loss: 0.00001198
Iteration 159/1000 | Loss: 0.00001198
Iteration 160/1000 | Loss: 0.00001198
Iteration 161/1000 | Loss: 0.00001198
Iteration 162/1000 | Loss: 0.00001198
Iteration 163/1000 | Loss: 0.00001198
Iteration 164/1000 | Loss: 0.00001198
Iteration 165/1000 | Loss: 0.00001198
Iteration 166/1000 | Loss: 0.00001198
Iteration 167/1000 | Loss: 0.00001198
Iteration 168/1000 | Loss: 0.00001198
Iteration 169/1000 | Loss: 0.00001198
Iteration 170/1000 | Loss: 0.00001198
Iteration 171/1000 | Loss: 0.00001198
Iteration 172/1000 | Loss: 0.00001198
Iteration 173/1000 | Loss: 0.00001198
Iteration 174/1000 | Loss: 0.00001198
Iteration 175/1000 | Loss: 0.00001198
Iteration 176/1000 | Loss: 0.00001198
Iteration 177/1000 | Loss: 0.00001198
Iteration 178/1000 | Loss: 0.00001198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.1978449037997052e-05, 1.1978449037997052e-05, 1.1978449037997052e-05, 1.1978449037997052e-05, 1.1978449037997052e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1978449037997052e-05

Optimization complete. Final v2v error: 2.9028337001800537 mm

Highest mean error: 4.126933574676514 mm for frame 0

Lowest mean error: 2.5348429679870605 mm for frame 99

Saving results

Total time: 78.95756959915161
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01097761
Iteration 2/25 | Loss: 0.01097761
Iteration 3/25 | Loss: 0.01097761
Iteration 4/25 | Loss: 0.01097761
Iteration 5/25 | Loss: 0.00257122
Iteration 6/25 | Loss: 0.00150856
Iteration 7/25 | Loss: 0.00144079
Iteration 8/25 | Loss: 0.00129490
Iteration 9/25 | Loss: 0.00115390
Iteration 10/25 | Loss: 0.00113827
Iteration 11/25 | Loss: 0.00104160
Iteration 12/25 | Loss: 0.00104605
Iteration 13/25 | Loss: 0.00099304
Iteration 14/25 | Loss: 0.00099548
Iteration 15/25 | Loss: 0.00096893
Iteration 16/25 | Loss: 0.00095430
Iteration 17/25 | Loss: 0.00092295
Iteration 18/25 | Loss: 0.00089004
Iteration 19/25 | Loss: 0.00089077
Iteration 20/25 | Loss: 0.00085703
Iteration 21/25 | Loss: 0.00086887
Iteration 22/25 | Loss: 0.00084999
Iteration 23/25 | Loss: 0.00082102
Iteration 24/25 | Loss: 0.00081475
Iteration 25/25 | Loss: 0.00081266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54447126
Iteration 2/25 | Loss: 0.00593532
Iteration 3/25 | Loss: 0.00180985
Iteration 4/25 | Loss: 0.00180982
Iteration 5/25 | Loss: 0.00180982
Iteration 6/25 | Loss: 0.00180981
Iteration 7/25 | Loss: 0.00180981
Iteration 8/25 | Loss: 0.00180981
Iteration 9/25 | Loss: 0.00180981
Iteration 10/25 | Loss: 0.00180981
Iteration 11/25 | Loss: 0.00180981
Iteration 12/25 | Loss: 0.00180981
Iteration 13/25 | Loss: 0.00180981
Iteration 14/25 | Loss: 0.00180981
Iteration 15/25 | Loss: 0.00180981
Iteration 16/25 | Loss: 0.00180981
Iteration 17/25 | Loss: 0.00180981
Iteration 18/25 | Loss: 0.00180981
Iteration 19/25 | Loss: 0.00180981
Iteration 20/25 | Loss: 0.00180981
Iteration 21/25 | Loss: 0.00180981
Iteration 22/25 | Loss: 0.00180981
Iteration 23/25 | Loss: 0.00180981
Iteration 24/25 | Loss: 0.00180981
Iteration 25/25 | Loss: 0.00180981

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180981
Iteration 2/1000 | Loss: 0.00253999
Iteration 3/1000 | Loss: 0.00481500
Iteration 4/1000 | Loss: 0.00260202
Iteration 5/1000 | Loss: 0.00261700
Iteration 6/1000 | Loss: 0.00113460
Iteration 7/1000 | Loss: 0.00115032
Iteration 8/1000 | Loss: 0.00077660
Iteration 9/1000 | Loss: 0.00151919
Iteration 10/1000 | Loss: 0.00134266
Iteration 11/1000 | Loss: 0.00096186
Iteration 12/1000 | Loss: 0.00124410
Iteration 13/1000 | Loss: 0.00083609
Iteration 14/1000 | Loss: 0.00059135
Iteration 15/1000 | Loss: 0.00227744
Iteration 16/1000 | Loss: 0.00231942
Iteration 17/1000 | Loss: 0.00079227
Iteration 18/1000 | Loss: 0.00227952
Iteration 19/1000 | Loss: 0.00069253
Iteration 20/1000 | Loss: 0.00303344
Iteration 21/1000 | Loss: 0.00069978
Iteration 22/1000 | Loss: 0.00055560
Iteration 23/1000 | Loss: 0.00107101
Iteration 24/1000 | Loss: 0.00074073
Iteration 25/1000 | Loss: 0.00080700
Iteration 26/1000 | Loss: 0.00063904
Iteration 27/1000 | Loss: 0.00075858
Iteration 28/1000 | Loss: 0.00051983
Iteration 29/1000 | Loss: 0.00224908
Iteration 30/1000 | Loss: 0.00123919
Iteration 31/1000 | Loss: 0.00049963
Iteration 32/1000 | Loss: 0.00042601
Iteration 33/1000 | Loss: 0.00045228
Iteration 34/1000 | Loss: 0.00092595
Iteration 35/1000 | Loss: 0.00076198
Iteration 36/1000 | Loss: 0.00059043
Iteration 37/1000 | Loss: 0.00054478
Iteration 38/1000 | Loss: 0.00061807
Iteration 39/1000 | Loss: 0.00068124
Iteration 40/1000 | Loss: 0.00113539
Iteration 41/1000 | Loss: 0.00085307
Iteration 42/1000 | Loss: 0.00049593
Iteration 43/1000 | Loss: 0.00069433
Iteration 44/1000 | Loss: 0.00127207
Iteration 45/1000 | Loss: 0.00074978
Iteration 46/1000 | Loss: 0.00071645
Iteration 47/1000 | Loss: 0.00144775
Iteration 48/1000 | Loss: 0.00068474
Iteration 49/1000 | Loss: 0.00326707
Iteration 50/1000 | Loss: 0.00097647
Iteration 51/1000 | Loss: 0.00109528
Iteration 52/1000 | Loss: 0.00068571
Iteration 53/1000 | Loss: 0.00068580
Iteration 54/1000 | Loss: 0.00170749
Iteration 55/1000 | Loss: 0.00065192
Iteration 56/1000 | Loss: 0.00126466
Iteration 57/1000 | Loss: 0.00107137
Iteration 58/1000 | Loss: 0.00167990
Iteration 59/1000 | Loss: 0.00050873
Iteration 60/1000 | Loss: 0.00069143
Iteration 61/1000 | Loss: 0.00065740
Iteration 62/1000 | Loss: 0.00110616
Iteration 63/1000 | Loss: 0.00075339
Iteration 64/1000 | Loss: 0.00080867
Iteration 65/1000 | Loss: 0.00091879
Iteration 66/1000 | Loss: 0.00068016
Iteration 67/1000 | Loss: 0.00077797
Iteration 68/1000 | Loss: 0.00157595
Iteration 69/1000 | Loss: 0.00067141
Iteration 70/1000 | Loss: 0.00064689
Iteration 71/1000 | Loss: 0.00086662
Iteration 72/1000 | Loss: 0.00070100
Iteration 73/1000 | Loss: 0.00150434
Iteration 74/1000 | Loss: 0.00128284
Iteration 75/1000 | Loss: 0.00081379
Iteration 76/1000 | Loss: 0.00123977
Iteration 77/1000 | Loss: 0.00054806
Iteration 78/1000 | Loss: 0.00070114
Iteration 79/1000 | Loss: 0.00042684
Iteration 80/1000 | Loss: 0.00054830
Iteration 81/1000 | Loss: 0.00062970
Iteration 82/1000 | Loss: 0.00165538
Iteration 83/1000 | Loss: 0.00118140
Iteration 84/1000 | Loss: 0.00077228
Iteration 85/1000 | Loss: 0.00080151
Iteration 86/1000 | Loss: 0.00086253
Iteration 87/1000 | Loss: 0.00088632
Iteration 88/1000 | Loss: 0.00134324
Iteration 89/1000 | Loss: 0.00088802
Iteration 90/1000 | Loss: 0.00078815
Iteration 91/1000 | Loss: 0.00099578
Iteration 92/1000 | Loss: 0.00076733
Iteration 93/1000 | Loss: 0.00085778
Iteration 94/1000 | Loss: 0.00126924
Iteration 95/1000 | Loss: 0.00107276
Iteration 96/1000 | Loss: 0.00114593
Iteration 97/1000 | Loss: 0.00067240
Iteration 98/1000 | Loss: 0.00072597
Iteration 99/1000 | Loss: 0.00066839
Iteration 100/1000 | Loss: 0.00095843
Iteration 101/1000 | Loss: 0.00093062
Iteration 102/1000 | Loss: 0.00117667
Iteration 103/1000 | Loss: 0.00085129
Iteration 104/1000 | Loss: 0.00076745
Iteration 105/1000 | Loss: 0.00066848
Iteration 106/1000 | Loss: 0.00102998
Iteration 107/1000 | Loss: 0.00080640
Iteration 108/1000 | Loss: 0.00083686
Iteration 109/1000 | Loss: 0.00086465
Iteration 110/1000 | Loss: 0.00062625
Iteration 111/1000 | Loss: 0.00066842
Iteration 112/1000 | Loss: 0.00083208
Iteration 113/1000 | Loss: 0.00086959
Iteration 114/1000 | Loss: 0.00070113
Iteration 115/1000 | Loss: 0.00110360
Iteration 116/1000 | Loss: 0.00073384
Iteration 117/1000 | Loss: 0.00069323
Iteration 118/1000 | Loss: 0.00078118
Iteration 119/1000 | Loss: 0.00072615
Iteration 120/1000 | Loss: 0.00076785
Iteration 121/1000 | Loss: 0.00074913
Iteration 122/1000 | Loss: 0.00068608
Iteration 123/1000 | Loss: 0.00089678
Iteration 124/1000 | Loss: 0.00101941
Iteration 125/1000 | Loss: 0.00131488
Iteration 126/1000 | Loss: 0.00104461
Iteration 127/1000 | Loss: 0.00077270
Iteration 128/1000 | Loss: 0.00080570
Iteration 129/1000 | Loss: 0.00040729
Iteration 130/1000 | Loss: 0.00065844
Iteration 131/1000 | Loss: 0.00110803
Iteration 132/1000 | Loss: 0.00117550
Iteration 133/1000 | Loss: 0.00094329
Iteration 134/1000 | Loss: 0.00127993
Iteration 135/1000 | Loss: 0.00046489
Iteration 136/1000 | Loss: 0.00125640
Iteration 137/1000 | Loss: 0.00071144
Iteration 138/1000 | Loss: 0.00070410
Iteration 139/1000 | Loss: 0.00095749
Iteration 140/1000 | Loss: 0.00146813
Iteration 141/1000 | Loss: 0.00158911
Iteration 142/1000 | Loss: 0.00086055
Iteration 143/1000 | Loss: 0.00033912
Iteration 144/1000 | Loss: 0.00020071
Iteration 145/1000 | Loss: 0.00010941
Iteration 146/1000 | Loss: 0.00017756
Iteration 147/1000 | Loss: 0.00037479
Iteration 148/1000 | Loss: 0.00085847
Iteration 149/1000 | Loss: 0.00036665
Iteration 150/1000 | Loss: 0.00105118
Iteration 151/1000 | Loss: 0.00025558
Iteration 152/1000 | Loss: 0.00005624
Iteration 153/1000 | Loss: 0.00042037
Iteration 154/1000 | Loss: 0.00009024
Iteration 155/1000 | Loss: 0.00028289
Iteration 156/1000 | Loss: 0.00015664
Iteration 157/1000 | Loss: 0.00019112
Iteration 158/1000 | Loss: 0.00017068
Iteration 159/1000 | Loss: 0.00027318
Iteration 160/1000 | Loss: 0.00016475
Iteration 161/1000 | Loss: 0.00014296
Iteration 162/1000 | Loss: 0.00018910
Iteration 163/1000 | Loss: 0.00037147
Iteration 164/1000 | Loss: 0.00007805
Iteration 165/1000 | Loss: 0.00036525
Iteration 166/1000 | Loss: 0.00019456
Iteration 167/1000 | Loss: 0.00056142
Iteration 168/1000 | Loss: 0.00046703
Iteration 169/1000 | Loss: 0.00034991
Iteration 170/1000 | Loss: 0.00056074
Iteration 171/1000 | Loss: 0.00075304
Iteration 172/1000 | Loss: 0.00077995
Iteration 173/1000 | Loss: 0.00079143
Iteration 174/1000 | Loss: 0.00084630
Iteration 175/1000 | Loss: 0.00057438
Iteration 176/1000 | Loss: 0.00052330
Iteration 177/1000 | Loss: 0.00063699
Iteration 178/1000 | Loss: 0.00021122
Iteration 179/1000 | Loss: 0.00011281
Iteration 180/1000 | Loss: 0.00012846
Iteration 181/1000 | Loss: 0.00010989
Iteration 182/1000 | Loss: 0.00012169
Iteration 183/1000 | Loss: 0.00012296
Iteration 184/1000 | Loss: 0.00023694
Iteration 185/1000 | Loss: 0.00019306
Iteration 186/1000 | Loss: 0.00020838
Iteration 187/1000 | Loss: 0.00019183
Iteration 188/1000 | Loss: 0.00021741
Iteration 189/1000 | Loss: 0.00016337
Iteration 190/1000 | Loss: 0.00086151
Iteration 191/1000 | Loss: 0.00021656
Iteration 192/1000 | Loss: 0.00012297
Iteration 193/1000 | Loss: 0.00052612
Iteration 194/1000 | Loss: 0.00063447
Iteration 195/1000 | Loss: 0.00010872
Iteration 196/1000 | Loss: 0.00007516
Iteration 197/1000 | Loss: 0.00049521
Iteration 198/1000 | Loss: 0.00083432
Iteration 199/1000 | Loss: 0.00052246
Iteration 200/1000 | Loss: 0.00051678
Iteration 201/1000 | Loss: 0.00080174
Iteration 202/1000 | Loss: 0.00014981
Iteration 203/1000 | Loss: 0.00058499
Iteration 204/1000 | Loss: 0.00021090
Iteration 205/1000 | Loss: 0.00031636
Iteration 206/1000 | Loss: 0.00017858
Iteration 207/1000 | Loss: 0.00019170
Iteration 208/1000 | Loss: 0.00018092
Iteration 209/1000 | Loss: 0.00035681
Iteration 210/1000 | Loss: 0.00064590
Iteration 211/1000 | Loss: 0.00094737
Iteration 212/1000 | Loss: 0.00055179
Iteration 213/1000 | Loss: 0.00018004
Iteration 214/1000 | Loss: 0.00029598
Iteration 215/1000 | Loss: 0.00009002
Iteration 216/1000 | Loss: 0.00013456
Iteration 217/1000 | Loss: 0.00009876
Iteration 218/1000 | Loss: 0.00010222
Iteration 219/1000 | Loss: 0.00008164
Iteration 220/1000 | Loss: 0.00017136
Iteration 221/1000 | Loss: 0.00010779
Iteration 222/1000 | Loss: 0.00015052
Iteration 223/1000 | Loss: 0.00007004
Iteration 224/1000 | Loss: 0.00009284
Iteration 225/1000 | Loss: 0.00008303
Iteration 226/1000 | Loss: 0.00021504
Iteration 227/1000 | Loss: 0.00009433
Iteration 228/1000 | Loss: 0.00012635
Iteration 229/1000 | Loss: 0.00007996
Iteration 230/1000 | Loss: 0.00019968
Iteration 231/1000 | Loss: 0.00007796
Iteration 232/1000 | Loss: 0.00012128
Iteration 233/1000 | Loss: 0.00006867
Iteration 234/1000 | Loss: 0.00014522
Iteration 235/1000 | Loss: 0.00024349
Iteration 236/1000 | Loss: 0.00055433
Iteration 237/1000 | Loss: 0.00009793
Iteration 238/1000 | Loss: 0.00007111
Iteration 239/1000 | Loss: 0.00014069
Iteration 240/1000 | Loss: 0.00013161
Iteration 241/1000 | Loss: 0.00021061
Iteration 242/1000 | Loss: 0.00016876
Iteration 243/1000 | Loss: 0.00050629
Iteration 244/1000 | Loss: 0.00031039
Iteration 245/1000 | Loss: 0.00086901
Iteration 246/1000 | Loss: 0.00035618
Iteration 247/1000 | Loss: 0.00004024
Iteration 248/1000 | Loss: 0.00005550
Iteration 249/1000 | Loss: 0.00052898
Iteration 250/1000 | Loss: 0.00031240
Iteration 251/1000 | Loss: 0.00003549
Iteration 252/1000 | Loss: 0.00002987
Iteration 253/1000 | Loss: 0.00006831
Iteration 254/1000 | Loss: 0.00051195
Iteration 255/1000 | Loss: 0.00022841
Iteration 256/1000 | Loss: 0.00003051
Iteration 257/1000 | Loss: 0.00006462
Iteration 258/1000 | Loss: 0.00042979
Iteration 259/1000 | Loss: 0.00058088
Iteration 260/1000 | Loss: 0.00118016
Iteration 261/1000 | Loss: 0.00007079
Iteration 262/1000 | Loss: 0.00003022
Iteration 263/1000 | Loss: 0.00018742
Iteration 264/1000 | Loss: 0.00002590
Iteration 265/1000 | Loss: 0.00005126
Iteration 266/1000 | Loss: 0.00008171
Iteration 267/1000 | Loss: 0.00008336
Iteration 268/1000 | Loss: 0.00006257
Iteration 269/1000 | Loss: 0.00046473
Iteration 270/1000 | Loss: 0.00012217
Iteration 271/1000 | Loss: 0.00003279
Iteration 272/1000 | Loss: 0.00002701
Iteration 273/1000 | Loss: 0.00017939
Iteration 274/1000 | Loss: 0.00032982
Iteration 275/1000 | Loss: 0.00003253
Iteration 276/1000 | Loss: 0.00002915
Iteration 277/1000 | Loss: 0.00004123
Iteration 278/1000 | Loss: 0.00002553
Iteration 279/1000 | Loss: 0.00003245
Iteration 280/1000 | Loss: 0.00002415
Iteration 281/1000 | Loss: 0.00005314
Iteration 282/1000 | Loss: 0.00023188
Iteration 283/1000 | Loss: 0.00009588
Iteration 284/1000 | Loss: 0.00006552
Iteration 285/1000 | Loss: 0.00020329
Iteration 286/1000 | Loss: 0.00003226
Iteration 287/1000 | Loss: 0.00002672
Iteration 288/1000 | Loss: 0.00002479
Iteration 289/1000 | Loss: 0.00003028
Iteration 290/1000 | Loss: 0.00002204
Iteration 291/1000 | Loss: 0.00004871
Iteration 292/1000 | Loss: 0.00002179
Iteration 293/1000 | Loss: 0.00002173
Iteration 294/1000 | Loss: 0.00002189
Iteration 295/1000 | Loss: 0.00003112
Iteration 296/1000 | Loss: 0.00007169
Iteration 297/1000 | Loss: 0.00002203
Iteration 298/1000 | Loss: 0.00002179
Iteration 299/1000 | Loss: 0.00002162
Iteration 300/1000 | Loss: 0.00002162
Iteration 301/1000 | Loss: 0.00002162
Iteration 302/1000 | Loss: 0.00003371
Iteration 303/1000 | Loss: 0.00005417
Iteration 304/1000 | Loss: 0.00002157
Iteration 305/1000 | Loss: 0.00002540
Iteration 306/1000 | Loss: 0.00002135
Iteration 307/1000 | Loss: 0.00002134
Iteration 308/1000 | Loss: 0.00003217
Iteration 309/1000 | Loss: 0.00002138
Iteration 310/1000 | Loss: 0.00002134
Iteration 311/1000 | Loss: 0.00002134
Iteration 312/1000 | Loss: 0.00002121
Iteration 313/1000 | Loss: 0.00002120
Iteration 314/1000 | Loss: 0.00002120
Iteration 315/1000 | Loss: 0.00002120
Iteration 316/1000 | Loss: 0.00002120
Iteration 317/1000 | Loss: 0.00002119
Iteration 318/1000 | Loss: 0.00002119
Iteration 319/1000 | Loss: 0.00004813
Iteration 320/1000 | Loss: 0.00002945
Iteration 321/1000 | Loss: 0.00003187
Iteration 322/1000 | Loss: 0.00002305
Iteration 323/1000 | Loss: 0.00002116
Iteration 324/1000 | Loss: 0.00004694
Iteration 325/1000 | Loss: 0.00002915
Iteration 326/1000 | Loss: 0.00002256
Iteration 327/1000 | Loss: 0.00002758
Iteration 328/1000 | Loss: 0.00002106
Iteration 329/1000 | Loss: 0.00002102
Iteration 330/1000 | Loss: 0.00002102
Iteration 331/1000 | Loss: 0.00002101
Iteration 332/1000 | Loss: 0.00002101
Iteration 333/1000 | Loss: 0.00002101
Iteration 334/1000 | Loss: 0.00002100
Iteration 335/1000 | Loss: 0.00002100
Iteration 336/1000 | Loss: 0.00002106
Iteration 337/1000 | Loss: 0.00002106
Iteration 338/1000 | Loss: 0.00002106
Iteration 339/1000 | Loss: 0.00003014
Iteration 340/1000 | Loss: 0.00004011
Iteration 341/1000 | Loss: 0.00002108
Iteration 342/1000 | Loss: 0.00002096
Iteration 343/1000 | Loss: 0.00002566
Iteration 344/1000 | Loss: 0.00002174
Iteration 345/1000 | Loss: 0.00002265
Iteration 346/1000 | Loss: 0.00002100
Iteration 347/1000 | Loss: 0.00002464
Iteration 348/1000 | Loss: 0.00002136
Iteration 349/1000 | Loss: 0.00002104
Iteration 350/1000 | Loss: 0.00002363
Iteration 351/1000 | Loss: 0.00002363
Iteration 352/1000 | Loss: 0.00004468
Iteration 353/1000 | Loss: 0.00002106
Iteration 354/1000 | Loss: 0.00002106
Iteration 355/1000 | Loss: 0.00002366
Iteration 356/1000 | Loss: 0.00002366
Iteration 357/1000 | Loss: 0.00002366
Iteration 358/1000 | Loss: 0.00005211
Iteration 359/1000 | Loss: 0.00017032
Iteration 360/1000 | Loss: 0.00002110
Iteration 361/1000 | Loss: 0.00002094
Iteration 362/1000 | Loss: 0.00002094
Iteration 363/1000 | Loss: 0.00002093
Iteration 364/1000 | Loss: 0.00002093
Iteration 365/1000 | Loss: 0.00002093
Iteration 366/1000 | Loss: 0.00002093
Iteration 367/1000 | Loss: 0.00002093
Iteration 368/1000 | Loss: 0.00002093
Iteration 369/1000 | Loss: 0.00002093
Iteration 370/1000 | Loss: 0.00002093
Iteration 371/1000 | Loss: 0.00002093
Iteration 372/1000 | Loss: 0.00002100
Iteration 373/1000 | Loss: 0.00002203
Iteration 374/1000 | Loss: 0.00002098
Iteration 375/1000 | Loss: 0.00002097
Iteration 376/1000 | Loss: 0.00002096
Iteration 377/1000 | Loss: 0.00002096
Iteration 378/1000 | Loss: 0.00002096
Iteration 379/1000 | Loss: 0.00002096
Iteration 380/1000 | Loss: 0.00002096
Iteration 381/1000 | Loss: 0.00002096
Iteration 382/1000 | Loss: 0.00002096
Iteration 383/1000 | Loss: 0.00002096
Iteration 384/1000 | Loss: 0.00002097
Iteration 385/1000 | Loss: 0.00002097
Iteration 386/1000 | Loss: 0.00002097
Iteration 387/1000 | Loss: 0.00002096
Iteration 388/1000 | Loss: 0.00002096
Iteration 389/1000 | Loss: 0.00002096
Iteration 390/1000 | Loss: 0.00002096
Iteration 391/1000 | Loss: 0.00002096
Iteration 392/1000 | Loss: 0.00002093
Iteration 393/1000 | Loss: 0.00002093
Iteration 394/1000 | Loss: 0.00002093
Iteration 395/1000 | Loss: 0.00002095
Iteration 396/1000 | Loss: 0.00002095
Iteration 397/1000 | Loss: 0.00002095
Iteration 398/1000 | Loss: 0.00002095
Iteration 399/1000 | Loss: 0.00002095
Iteration 400/1000 | Loss: 0.00002095
Iteration 401/1000 | Loss: 0.00002095
Iteration 402/1000 | Loss: 0.00002095
Iteration 403/1000 | Loss: 0.00002095
Iteration 404/1000 | Loss: 0.00002095
Iteration 405/1000 | Loss: 0.00002095
Iteration 406/1000 | Loss: 0.00002095
Iteration 407/1000 | Loss: 0.00002095
Iteration 408/1000 | Loss: 0.00002095
Iteration 409/1000 | Loss: 0.00002095
Iteration 410/1000 | Loss: 0.00002095
Iteration 411/1000 | Loss: 0.00002095
Iteration 412/1000 | Loss: 0.00002095
Iteration 413/1000 | Loss: 0.00002095
Iteration 414/1000 | Loss: 0.00002095
Iteration 415/1000 | Loss: 0.00002095
Iteration 416/1000 | Loss: 0.00002095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 416. Stopping optimization.
Last 5 losses: [2.094878254865762e-05, 2.094878254865762e-05, 2.094878254865762e-05, 2.094878254865762e-05, 2.094878254865762e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.094878254865762e-05

Optimization complete. Final v2v error: 3.6841914653778076 mm

Highest mean error: 15.677059173583984 mm for frame 67

Lowest mean error: 2.790303945541382 mm for frame 237

Saving results

Total time: 574.9787182807922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068304
Iteration 2/25 | Loss: 0.00154156
Iteration 3/25 | Loss: 0.00083735
Iteration 4/25 | Loss: 0.00075285
Iteration 5/25 | Loss: 0.00072334
Iteration 6/25 | Loss: 0.00071245
Iteration 7/25 | Loss: 0.00069822
Iteration 8/25 | Loss: 0.00069203
Iteration 9/25 | Loss: 0.00069340
Iteration 10/25 | Loss: 0.00068940
Iteration 11/25 | Loss: 0.00068927
Iteration 12/25 | Loss: 0.00068927
Iteration 13/25 | Loss: 0.00068927
Iteration 14/25 | Loss: 0.00068927
Iteration 15/25 | Loss: 0.00068926
Iteration 16/25 | Loss: 0.00068926
Iteration 17/25 | Loss: 0.00068926
Iteration 18/25 | Loss: 0.00068926
Iteration 19/25 | Loss: 0.00068926
Iteration 20/25 | Loss: 0.00068926
Iteration 21/25 | Loss: 0.00068926
Iteration 22/25 | Loss: 0.00068926
Iteration 23/25 | Loss: 0.00068926
Iteration 24/25 | Loss: 0.00068926
Iteration 25/25 | Loss: 0.00068926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.46844625
Iteration 2/25 | Loss: 0.00031508
Iteration 3/25 | Loss: 0.00031507
Iteration 4/25 | Loss: 0.00031507
Iteration 5/25 | Loss: 0.00031507
Iteration 6/25 | Loss: 0.00031507
Iteration 7/25 | Loss: 0.00031507
Iteration 8/25 | Loss: 0.00031507
Iteration 9/25 | Loss: 0.00031507
Iteration 10/25 | Loss: 0.00031507
Iteration 11/25 | Loss: 0.00031507
Iteration 12/25 | Loss: 0.00031507
Iteration 13/25 | Loss: 0.00031507
Iteration 14/25 | Loss: 0.00031507
Iteration 15/25 | Loss: 0.00031507
Iteration 16/25 | Loss: 0.00031507
Iteration 17/25 | Loss: 0.00031507
Iteration 18/25 | Loss: 0.00031507
Iteration 19/25 | Loss: 0.00031507
Iteration 20/25 | Loss: 0.00031507
Iteration 21/25 | Loss: 0.00031507
Iteration 22/25 | Loss: 0.00031507
Iteration 23/25 | Loss: 0.00031507
Iteration 24/25 | Loss: 0.00031507
Iteration 25/25 | Loss: 0.00031507

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031507
Iteration 2/1000 | Loss: 0.00002761
Iteration 3/1000 | Loss: 0.00001893
Iteration 4/1000 | Loss: 0.00001752
Iteration 5/1000 | Loss: 0.00001664
Iteration 6/1000 | Loss: 0.00001597
Iteration 7/1000 | Loss: 0.00001554
Iteration 8/1000 | Loss: 0.00001527
Iteration 9/1000 | Loss: 0.00001504
Iteration 10/1000 | Loss: 0.00001497
Iteration 11/1000 | Loss: 0.00001496
Iteration 12/1000 | Loss: 0.00001495
Iteration 13/1000 | Loss: 0.00001494
Iteration 14/1000 | Loss: 0.00001494
Iteration 15/1000 | Loss: 0.00001490
Iteration 16/1000 | Loss: 0.00001489
Iteration 17/1000 | Loss: 0.00001489
Iteration 18/1000 | Loss: 0.00001488
Iteration 19/1000 | Loss: 0.00001482
Iteration 20/1000 | Loss: 0.00001478
Iteration 21/1000 | Loss: 0.00001477
Iteration 22/1000 | Loss: 0.00001477
Iteration 23/1000 | Loss: 0.00001474
Iteration 24/1000 | Loss: 0.00001473
Iteration 25/1000 | Loss: 0.00001472
Iteration 26/1000 | Loss: 0.00001472
Iteration 27/1000 | Loss: 0.00001471
Iteration 28/1000 | Loss: 0.00001471
Iteration 29/1000 | Loss: 0.00001470
Iteration 30/1000 | Loss: 0.00001469
Iteration 31/1000 | Loss: 0.00001469
Iteration 32/1000 | Loss: 0.00001469
Iteration 33/1000 | Loss: 0.00001468
Iteration 34/1000 | Loss: 0.00001468
Iteration 35/1000 | Loss: 0.00001468
Iteration 36/1000 | Loss: 0.00001468
Iteration 37/1000 | Loss: 0.00001468
Iteration 38/1000 | Loss: 0.00001468
Iteration 39/1000 | Loss: 0.00001467
Iteration 40/1000 | Loss: 0.00001466
Iteration 41/1000 | Loss: 0.00001465
Iteration 42/1000 | Loss: 0.00001465
Iteration 43/1000 | Loss: 0.00001464
Iteration 44/1000 | Loss: 0.00001464
Iteration 45/1000 | Loss: 0.00001464
Iteration 46/1000 | Loss: 0.00001464
Iteration 47/1000 | Loss: 0.00001464
Iteration 48/1000 | Loss: 0.00001464
Iteration 49/1000 | Loss: 0.00001464
Iteration 50/1000 | Loss: 0.00001464
Iteration 51/1000 | Loss: 0.00001464
Iteration 52/1000 | Loss: 0.00001463
Iteration 53/1000 | Loss: 0.00001463
Iteration 54/1000 | Loss: 0.00001463
Iteration 55/1000 | Loss: 0.00001462
Iteration 56/1000 | Loss: 0.00001462
Iteration 57/1000 | Loss: 0.00001462
Iteration 58/1000 | Loss: 0.00001461
Iteration 59/1000 | Loss: 0.00001461
Iteration 60/1000 | Loss: 0.00001461
Iteration 61/1000 | Loss: 0.00001461
Iteration 62/1000 | Loss: 0.00001460
Iteration 63/1000 | Loss: 0.00001460
Iteration 64/1000 | Loss: 0.00001460
Iteration 65/1000 | Loss: 0.00001459
Iteration 66/1000 | Loss: 0.00001459
Iteration 67/1000 | Loss: 0.00001459
Iteration 68/1000 | Loss: 0.00001458
Iteration 69/1000 | Loss: 0.00001458
Iteration 70/1000 | Loss: 0.00001458
Iteration 71/1000 | Loss: 0.00001458
Iteration 72/1000 | Loss: 0.00001458
Iteration 73/1000 | Loss: 0.00001458
Iteration 74/1000 | Loss: 0.00001458
Iteration 75/1000 | Loss: 0.00001458
Iteration 76/1000 | Loss: 0.00001457
Iteration 77/1000 | Loss: 0.00001457
Iteration 78/1000 | Loss: 0.00001457
Iteration 79/1000 | Loss: 0.00001457
Iteration 80/1000 | Loss: 0.00001456
Iteration 81/1000 | Loss: 0.00001456
Iteration 82/1000 | Loss: 0.00001456
Iteration 83/1000 | Loss: 0.00001455
Iteration 84/1000 | Loss: 0.00001455
Iteration 85/1000 | Loss: 0.00001455
Iteration 86/1000 | Loss: 0.00001455
Iteration 87/1000 | Loss: 0.00001455
Iteration 88/1000 | Loss: 0.00001454
Iteration 89/1000 | Loss: 0.00001454
Iteration 90/1000 | Loss: 0.00001454
Iteration 91/1000 | Loss: 0.00001454
Iteration 92/1000 | Loss: 0.00001454
Iteration 93/1000 | Loss: 0.00001453
Iteration 94/1000 | Loss: 0.00001453
Iteration 95/1000 | Loss: 0.00001453
Iteration 96/1000 | Loss: 0.00001453
Iteration 97/1000 | Loss: 0.00001452
Iteration 98/1000 | Loss: 0.00001452
Iteration 99/1000 | Loss: 0.00001452
Iteration 100/1000 | Loss: 0.00001452
Iteration 101/1000 | Loss: 0.00001451
Iteration 102/1000 | Loss: 0.00001451
Iteration 103/1000 | Loss: 0.00001451
Iteration 104/1000 | Loss: 0.00001451
Iteration 105/1000 | Loss: 0.00001450
Iteration 106/1000 | Loss: 0.00001450
Iteration 107/1000 | Loss: 0.00001450
Iteration 108/1000 | Loss: 0.00001450
Iteration 109/1000 | Loss: 0.00001450
Iteration 110/1000 | Loss: 0.00001449
Iteration 111/1000 | Loss: 0.00001449
Iteration 112/1000 | Loss: 0.00001449
Iteration 113/1000 | Loss: 0.00001449
Iteration 114/1000 | Loss: 0.00001449
Iteration 115/1000 | Loss: 0.00001449
Iteration 116/1000 | Loss: 0.00001448
Iteration 117/1000 | Loss: 0.00001448
Iteration 118/1000 | Loss: 0.00001448
Iteration 119/1000 | Loss: 0.00001448
Iteration 120/1000 | Loss: 0.00001448
Iteration 121/1000 | Loss: 0.00001448
Iteration 122/1000 | Loss: 0.00001448
Iteration 123/1000 | Loss: 0.00001448
Iteration 124/1000 | Loss: 0.00001448
Iteration 125/1000 | Loss: 0.00001448
Iteration 126/1000 | Loss: 0.00001448
Iteration 127/1000 | Loss: 0.00001447
Iteration 128/1000 | Loss: 0.00001447
Iteration 129/1000 | Loss: 0.00001447
Iteration 130/1000 | Loss: 0.00001447
Iteration 131/1000 | Loss: 0.00001447
Iteration 132/1000 | Loss: 0.00001447
Iteration 133/1000 | Loss: 0.00001447
Iteration 134/1000 | Loss: 0.00001447
Iteration 135/1000 | Loss: 0.00001447
Iteration 136/1000 | Loss: 0.00001447
Iteration 137/1000 | Loss: 0.00001447
Iteration 138/1000 | Loss: 0.00001447
Iteration 139/1000 | Loss: 0.00001447
Iteration 140/1000 | Loss: 0.00001446
Iteration 141/1000 | Loss: 0.00001446
Iteration 142/1000 | Loss: 0.00001446
Iteration 143/1000 | Loss: 0.00001446
Iteration 144/1000 | Loss: 0.00001446
Iteration 145/1000 | Loss: 0.00001446
Iteration 146/1000 | Loss: 0.00001446
Iteration 147/1000 | Loss: 0.00001446
Iteration 148/1000 | Loss: 0.00001445
Iteration 149/1000 | Loss: 0.00001445
Iteration 150/1000 | Loss: 0.00001445
Iteration 151/1000 | Loss: 0.00001445
Iteration 152/1000 | Loss: 0.00001445
Iteration 153/1000 | Loss: 0.00001445
Iteration 154/1000 | Loss: 0.00001445
Iteration 155/1000 | Loss: 0.00001445
Iteration 156/1000 | Loss: 0.00001445
Iteration 157/1000 | Loss: 0.00001445
Iteration 158/1000 | Loss: 0.00001445
Iteration 159/1000 | Loss: 0.00001444
Iteration 160/1000 | Loss: 0.00001444
Iteration 161/1000 | Loss: 0.00001444
Iteration 162/1000 | Loss: 0.00001444
Iteration 163/1000 | Loss: 0.00001444
Iteration 164/1000 | Loss: 0.00001444
Iteration 165/1000 | Loss: 0.00001444
Iteration 166/1000 | Loss: 0.00001444
Iteration 167/1000 | Loss: 0.00001444
Iteration 168/1000 | Loss: 0.00001444
Iteration 169/1000 | Loss: 0.00001444
Iteration 170/1000 | Loss: 0.00001444
Iteration 171/1000 | Loss: 0.00001444
Iteration 172/1000 | Loss: 0.00001444
Iteration 173/1000 | Loss: 0.00001444
Iteration 174/1000 | Loss: 0.00001444
Iteration 175/1000 | Loss: 0.00001444
Iteration 176/1000 | Loss: 0.00001443
Iteration 177/1000 | Loss: 0.00001443
Iteration 178/1000 | Loss: 0.00001443
Iteration 179/1000 | Loss: 0.00001443
Iteration 180/1000 | Loss: 0.00001443
Iteration 181/1000 | Loss: 0.00001443
Iteration 182/1000 | Loss: 0.00001443
Iteration 183/1000 | Loss: 0.00001443
Iteration 184/1000 | Loss: 0.00001443
Iteration 185/1000 | Loss: 0.00001443
Iteration 186/1000 | Loss: 0.00001443
Iteration 187/1000 | Loss: 0.00001443
Iteration 188/1000 | Loss: 0.00001442
Iteration 189/1000 | Loss: 0.00001442
Iteration 190/1000 | Loss: 0.00001442
Iteration 191/1000 | Loss: 0.00001442
Iteration 192/1000 | Loss: 0.00001442
Iteration 193/1000 | Loss: 0.00001442
Iteration 194/1000 | Loss: 0.00001442
Iteration 195/1000 | Loss: 0.00001442
Iteration 196/1000 | Loss: 0.00001442
Iteration 197/1000 | Loss: 0.00001442
Iteration 198/1000 | Loss: 0.00001442
Iteration 199/1000 | Loss: 0.00001442
Iteration 200/1000 | Loss: 0.00001442
Iteration 201/1000 | Loss: 0.00001442
Iteration 202/1000 | Loss: 0.00001442
Iteration 203/1000 | Loss: 0.00001442
Iteration 204/1000 | Loss: 0.00001442
Iteration 205/1000 | Loss: 0.00001442
Iteration 206/1000 | Loss: 0.00001442
Iteration 207/1000 | Loss: 0.00001442
Iteration 208/1000 | Loss: 0.00001442
Iteration 209/1000 | Loss: 0.00001442
Iteration 210/1000 | Loss: 0.00001442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.4422372260014527e-05, 1.4422372260014527e-05, 1.4422372260014527e-05, 1.4422372260014527e-05, 1.4422372260014527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4422372260014527e-05

Optimization complete. Final v2v error: 3.152219295501709 mm

Highest mean error: 8.305688858032227 mm for frame 186

Lowest mean error: 2.839906692504883 mm for frame 10

Saving results

Total time: 51.133445501327515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890612
Iteration 2/25 | Loss: 0.00079929
Iteration 3/25 | Loss: 0.00064516
Iteration 4/25 | Loss: 0.00062307
Iteration 5/25 | Loss: 0.00061555
Iteration 6/25 | Loss: 0.00061416
Iteration 7/25 | Loss: 0.00061387
Iteration 8/25 | Loss: 0.00061387
Iteration 9/25 | Loss: 0.00061387
Iteration 10/25 | Loss: 0.00061387
Iteration 11/25 | Loss: 0.00061387
Iteration 12/25 | Loss: 0.00061387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006138719618320465, 0.0006138719618320465, 0.0006138719618320465, 0.0006138719618320465, 0.0006138719618320465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006138719618320465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.88721800
Iteration 2/25 | Loss: 0.00029775
Iteration 3/25 | Loss: 0.00029775
Iteration 4/25 | Loss: 0.00029775
Iteration 5/25 | Loss: 0.00029775
Iteration 6/25 | Loss: 0.00029775
Iteration 7/25 | Loss: 0.00029775
Iteration 8/25 | Loss: 0.00029775
Iteration 9/25 | Loss: 0.00029775
Iteration 10/25 | Loss: 0.00029775
Iteration 11/25 | Loss: 0.00029775
Iteration 12/25 | Loss: 0.00029775
Iteration 13/25 | Loss: 0.00029775
Iteration 14/25 | Loss: 0.00029775
Iteration 15/25 | Loss: 0.00029775
Iteration 16/25 | Loss: 0.00029775
Iteration 17/25 | Loss: 0.00029775
Iteration 18/25 | Loss: 0.00029775
Iteration 19/25 | Loss: 0.00029775
Iteration 20/25 | Loss: 0.00029775
Iteration 21/25 | Loss: 0.00029775
Iteration 22/25 | Loss: 0.00029775
Iteration 23/25 | Loss: 0.00029775
Iteration 24/25 | Loss: 0.00029775
Iteration 25/25 | Loss: 0.00029775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029775
Iteration 2/1000 | Loss: 0.00002557
Iteration 3/1000 | Loss: 0.00001768
Iteration 4/1000 | Loss: 0.00001415
Iteration 5/1000 | Loss: 0.00001360
Iteration 6/1000 | Loss: 0.00001315
Iteration 7/1000 | Loss: 0.00001287
Iteration 8/1000 | Loss: 0.00001264
Iteration 9/1000 | Loss: 0.00001253
Iteration 10/1000 | Loss: 0.00001253
Iteration 11/1000 | Loss: 0.00001252
Iteration 12/1000 | Loss: 0.00001252
Iteration 13/1000 | Loss: 0.00001250
Iteration 14/1000 | Loss: 0.00001250
Iteration 15/1000 | Loss: 0.00001248
Iteration 16/1000 | Loss: 0.00001242
Iteration 17/1000 | Loss: 0.00001234
Iteration 18/1000 | Loss: 0.00001234
Iteration 19/1000 | Loss: 0.00001232
Iteration 20/1000 | Loss: 0.00001230
Iteration 21/1000 | Loss: 0.00001229
Iteration 22/1000 | Loss: 0.00001229
Iteration 23/1000 | Loss: 0.00001228
Iteration 24/1000 | Loss: 0.00001228
Iteration 25/1000 | Loss: 0.00001227
Iteration 26/1000 | Loss: 0.00001227
Iteration 27/1000 | Loss: 0.00001224
Iteration 28/1000 | Loss: 0.00001224
Iteration 29/1000 | Loss: 0.00001223
Iteration 30/1000 | Loss: 0.00001222
Iteration 31/1000 | Loss: 0.00001221
Iteration 32/1000 | Loss: 0.00001221
Iteration 33/1000 | Loss: 0.00001220
Iteration 34/1000 | Loss: 0.00001219
Iteration 35/1000 | Loss: 0.00001218
Iteration 36/1000 | Loss: 0.00001217
Iteration 37/1000 | Loss: 0.00001217
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001215
Iteration 40/1000 | Loss: 0.00001214
Iteration 41/1000 | Loss: 0.00001213
Iteration 42/1000 | Loss: 0.00001212
Iteration 43/1000 | Loss: 0.00001212
Iteration 44/1000 | Loss: 0.00001211
Iteration 45/1000 | Loss: 0.00001209
Iteration 46/1000 | Loss: 0.00001209
Iteration 47/1000 | Loss: 0.00001208
Iteration 48/1000 | Loss: 0.00001208
Iteration 49/1000 | Loss: 0.00001208
Iteration 50/1000 | Loss: 0.00001208
Iteration 51/1000 | Loss: 0.00001208
Iteration 52/1000 | Loss: 0.00001207
Iteration 53/1000 | Loss: 0.00001207
Iteration 54/1000 | Loss: 0.00001207
Iteration 55/1000 | Loss: 0.00001207
Iteration 56/1000 | Loss: 0.00001207
Iteration 57/1000 | Loss: 0.00001207
Iteration 58/1000 | Loss: 0.00001206
Iteration 59/1000 | Loss: 0.00001206
Iteration 60/1000 | Loss: 0.00001206
Iteration 61/1000 | Loss: 0.00001205
Iteration 62/1000 | Loss: 0.00001205
Iteration 63/1000 | Loss: 0.00001205
Iteration 64/1000 | Loss: 0.00001205
Iteration 65/1000 | Loss: 0.00001205
Iteration 66/1000 | Loss: 0.00001204
Iteration 67/1000 | Loss: 0.00001204
Iteration 68/1000 | Loss: 0.00001204
Iteration 69/1000 | Loss: 0.00001204
Iteration 70/1000 | Loss: 0.00001203
Iteration 71/1000 | Loss: 0.00001203
Iteration 72/1000 | Loss: 0.00001203
Iteration 73/1000 | Loss: 0.00001203
Iteration 74/1000 | Loss: 0.00001203
Iteration 75/1000 | Loss: 0.00001203
Iteration 76/1000 | Loss: 0.00001203
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001202
Iteration 80/1000 | Loss: 0.00001202
Iteration 81/1000 | Loss: 0.00001202
Iteration 82/1000 | Loss: 0.00001202
Iteration 83/1000 | Loss: 0.00001202
Iteration 84/1000 | Loss: 0.00001202
Iteration 85/1000 | Loss: 0.00001202
Iteration 86/1000 | Loss: 0.00001202
Iteration 87/1000 | Loss: 0.00001201
Iteration 88/1000 | Loss: 0.00001200
Iteration 89/1000 | Loss: 0.00001200
Iteration 90/1000 | Loss: 0.00001199
Iteration 91/1000 | Loss: 0.00001199
Iteration 92/1000 | Loss: 0.00001199
Iteration 93/1000 | Loss: 0.00001199
Iteration 94/1000 | Loss: 0.00001198
Iteration 95/1000 | Loss: 0.00001198
Iteration 96/1000 | Loss: 0.00001198
Iteration 97/1000 | Loss: 0.00001198
Iteration 98/1000 | Loss: 0.00001198
Iteration 99/1000 | Loss: 0.00001198
Iteration 100/1000 | Loss: 0.00001197
Iteration 101/1000 | Loss: 0.00001196
Iteration 102/1000 | Loss: 0.00001196
Iteration 103/1000 | Loss: 0.00001196
Iteration 104/1000 | Loss: 0.00001195
Iteration 105/1000 | Loss: 0.00001195
Iteration 106/1000 | Loss: 0.00001195
Iteration 107/1000 | Loss: 0.00001195
Iteration 108/1000 | Loss: 0.00001195
Iteration 109/1000 | Loss: 0.00001195
Iteration 110/1000 | Loss: 0.00001194
Iteration 111/1000 | Loss: 0.00001194
Iteration 112/1000 | Loss: 0.00001194
Iteration 113/1000 | Loss: 0.00001194
Iteration 114/1000 | Loss: 0.00001194
Iteration 115/1000 | Loss: 0.00001194
Iteration 116/1000 | Loss: 0.00001194
Iteration 117/1000 | Loss: 0.00001194
Iteration 118/1000 | Loss: 0.00001194
Iteration 119/1000 | Loss: 0.00001193
Iteration 120/1000 | Loss: 0.00001193
Iteration 121/1000 | Loss: 0.00001193
Iteration 122/1000 | Loss: 0.00001193
Iteration 123/1000 | Loss: 0.00001193
Iteration 124/1000 | Loss: 0.00001193
Iteration 125/1000 | Loss: 0.00001193
Iteration 126/1000 | Loss: 0.00001193
Iteration 127/1000 | Loss: 0.00001193
Iteration 128/1000 | Loss: 0.00001193
Iteration 129/1000 | Loss: 0.00001193
Iteration 130/1000 | Loss: 0.00001192
Iteration 131/1000 | Loss: 0.00001192
Iteration 132/1000 | Loss: 0.00001192
Iteration 133/1000 | Loss: 0.00001192
Iteration 134/1000 | Loss: 0.00001192
Iteration 135/1000 | Loss: 0.00001192
Iteration 136/1000 | Loss: 0.00001192
Iteration 137/1000 | Loss: 0.00001192
Iteration 138/1000 | Loss: 0.00001192
Iteration 139/1000 | Loss: 0.00001192
Iteration 140/1000 | Loss: 0.00001192
Iteration 141/1000 | Loss: 0.00001192
Iteration 142/1000 | Loss: 0.00001192
Iteration 143/1000 | Loss: 0.00001191
Iteration 144/1000 | Loss: 0.00001191
Iteration 145/1000 | Loss: 0.00001191
Iteration 146/1000 | Loss: 0.00001191
Iteration 147/1000 | Loss: 0.00001191
Iteration 148/1000 | Loss: 0.00001191
Iteration 149/1000 | Loss: 0.00001191
Iteration 150/1000 | Loss: 0.00001191
Iteration 151/1000 | Loss: 0.00001191
Iteration 152/1000 | Loss: 0.00001191
Iteration 153/1000 | Loss: 0.00001191
Iteration 154/1000 | Loss: 0.00001191
Iteration 155/1000 | Loss: 0.00001191
Iteration 156/1000 | Loss: 0.00001191
Iteration 157/1000 | Loss: 0.00001191
Iteration 158/1000 | Loss: 0.00001191
Iteration 159/1000 | Loss: 0.00001191
Iteration 160/1000 | Loss: 0.00001190
Iteration 161/1000 | Loss: 0.00001190
Iteration 162/1000 | Loss: 0.00001190
Iteration 163/1000 | Loss: 0.00001190
Iteration 164/1000 | Loss: 0.00001190
Iteration 165/1000 | Loss: 0.00001190
Iteration 166/1000 | Loss: 0.00001190
Iteration 167/1000 | Loss: 0.00001190
Iteration 168/1000 | Loss: 0.00001190
Iteration 169/1000 | Loss: 0.00001190
Iteration 170/1000 | Loss: 0.00001190
Iteration 171/1000 | Loss: 0.00001190
Iteration 172/1000 | Loss: 0.00001190
Iteration 173/1000 | Loss: 0.00001190
Iteration 174/1000 | Loss: 0.00001189
Iteration 175/1000 | Loss: 0.00001189
Iteration 176/1000 | Loss: 0.00001189
Iteration 177/1000 | Loss: 0.00001189
Iteration 178/1000 | Loss: 0.00001189
Iteration 179/1000 | Loss: 0.00001189
Iteration 180/1000 | Loss: 0.00001189
Iteration 181/1000 | Loss: 0.00001189
Iteration 182/1000 | Loss: 0.00001189
Iteration 183/1000 | Loss: 0.00001189
Iteration 184/1000 | Loss: 0.00001189
Iteration 185/1000 | Loss: 0.00001189
Iteration 186/1000 | Loss: 0.00001189
Iteration 187/1000 | Loss: 0.00001189
Iteration 188/1000 | Loss: 0.00001188
Iteration 189/1000 | Loss: 0.00001188
Iteration 190/1000 | Loss: 0.00001188
Iteration 191/1000 | Loss: 0.00001188
Iteration 192/1000 | Loss: 0.00001188
Iteration 193/1000 | Loss: 0.00001188
Iteration 194/1000 | Loss: 0.00001188
Iteration 195/1000 | Loss: 0.00001188
Iteration 196/1000 | Loss: 0.00001188
Iteration 197/1000 | Loss: 0.00001188
Iteration 198/1000 | Loss: 0.00001188
Iteration 199/1000 | Loss: 0.00001188
Iteration 200/1000 | Loss: 0.00001188
Iteration 201/1000 | Loss: 0.00001188
Iteration 202/1000 | Loss: 0.00001188
Iteration 203/1000 | Loss: 0.00001188
Iteration 204/1000 | Loss: 0.00001188
Iteration 205/1000 | Loss: 0.00001188
Iteration 206/1000 | Loss: 0.00001188
Iteration 207/1000 | Loss: 0.00001188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.188183614431182e-05, 1.188183614431182e-05, 1.188183614431182e-05, 1.188183614431182e-05, 1.188183614431182e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.188183614431182e-05

Optimization complete. Final v2v error: 2.907937526702881 mm

Highest mean error: 3.4064857959747314 mm for frame 50

Lowest mean error: 2.7502543926239014 mm for frame 72

Saving results

Total time: 37.44940233230591
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01136644
Iteration 2/25 | Loss: 0.00332935
Iteration 3/25 | Loss: 0.00265348
Iteration 4/25 | Loss: 0.00167629
Iteration 5/25 | Loss: 0.00132298
Iteration 6/25 | Loss: 0.00138226
Iteration 7/25 | Loss: 0.00124255
Iteration 8/25 | Loss: 0.00112116
Iteration 9/25 | Loss: 0.00100737
Iteration 10/25 | Loss: 0.00094682
Iteration 11/25 | Loss: 0.00092618
Iteration 12/25 | Loss: 0.00089717
Iteration 13/25 | Loss: 0.00089197
Iteration 14/25 | Loss: 0.00087948
Iteration 15/25 | Loss: 0.00086643
Iteration 16/25 | Loss: 0.00086012
Iteration 17/25 | Loss: 0.00085864
Iteration 18/25 | Loss: 0.00084265
Iteration 19/25 | Loss: 0.00084438
Iteration 20/25 | Loss: 0.00083678
Iteration 21/25 | Loss: 0.00083342
Iteration 22/25 | Loss: 0.00083266
Iteration 23/25 | Loss: 0.00082815
Iteration 24/25 | Loss: 0.00082848
Iteration 25/25 | Loss: 0.00082868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84704542
Iteration 2/25 | Loss: 0.00270314
Iteration 3/25 | Loss: 0.00049264
Iteration 4/25 | Loss: 0.00049259
Iteration 5/25 | Loss: 0.00049259
Iteration 6/25 | Loss: 0.00049259
Iteration 7/25 | Loss: 0.00049259
Iteration 8/25 | Loss: 0.00049259
Iteration 9/25 | Loss: 0.00049259
Iteration 10/25 | Loss: 0.00049259
Iteration 11/25 | Loss: 0.00049259
Iteration 12/25 | Loss: 0.00049259
Iteration 13/25 | Loss: 0.00049259
Iteration 14/25 | Loss: 0.00049259
Iteration 15/25 | Loss: 0.00049259
Iteration 16/25 | Loss: 0.00049259
Iteration 17/25 | Loss: 0.00049258
Iteration 18/25 | Loss: 0.00049258
Iteration 19/25 | Loss: 0.00049258
Iteration 20/25 | Loss: 0.00049258
Iteration 21/25 | Loss: 0.00049258
Iteration 22/25 | Loss: 0.00049258
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000492584949824959, 0.000492584949824959, 0.000492584949824959, 0.000492584949824959, 0.000492584949824959]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000492584949824959

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049258
Iteration 2/1000 | Loss: 0.00026025
Iteration 3/1000 | Loss: 0.00004729
Iteration 4/1000 | Loss: 0.00070865
Iteration 5/1000 | Loss: 0.00022878
Iteration 6/1000 | Loss: 0.00005308
Iteration 7/1000 | Loss: 0.00003930
Iteration 8/1000 | Loss: 0.00003493
Iteration 9/1000 | Loss: 0.00036850
Iteration 10/1000 | Loss: 0.00148756
Iteration 11/1000 | Loss: 0.00228181
Iteration 12/1000 | Loss: 0.00060666
Iteration 13/1000 | Loss: 0.00037384
Iteration 14/1000 | Loss: 0.00007415
Iteration 15/1000 | Loss: 0.00006722
Iteration 16/1000 | Loss: 0.00049264
Iteration 17/1000 | Loss: 0.00016962
Iteration 18/1000 | Loss: 0.00014911
Iteration 19/1000 | Loss: 0.00009758
Iteration 20/1000 | Loss: 0.00021779
Iteration 21/1000 | Loss: 0.00019400
Iteration 22/1000 | Loss: 0.00046857
Iteration 23/1000 | Loss: 0.00053134
Iteration 24/1000 | Loss: 0.00046358
Iteration 25/1000 | Loss: 0.00064632
Iteration 26/1000 | Loss: 0.00049655
Iteration 27/1000 | Loss: 0.00011314
Iteration 28/1000 | Loss: 0.00005569
Iteration 29/1000 | Loss: 0.00047127
Iteration 30/1000 | Loss: 0.00020636
Iteration 31/1000 | Loss: 0.00018078
Iteration 32/1000 | Loss: 0.00004627
Iteration 33/1000 | Loss: 0.00003648
Iteration 34/1000 | Loss: 0.00004452
Iteration 35/1000 | Loss: 0.00003318
Iteration 36/1000 | Loss: 0.00003095
Iteration 37/1000 | Loss: 0.00019207
Iteration 38/1000 | Loss: 0.00081254
Iteration 39/1000 | Loss: 0.00027565
Iteration 40/1000 | Loss: 0.00004763
Iteration 41/1000 | Loss: 0.00003413
Iteration 42/1000 | Loss: 0.00088401
Iteration 43/1000 | Loss: 0.00009647
Iteration 44/1000 | Loss: 0.00004464
Iteration 45/1000 | Loss: 0.00003398
Iteration 46/1000 | Loss: 0.00003093
Iteration 47/1000 | Loss: 0.00002886
Iteration 48/1000 | Loss: 0.00029014
Iteration 49/1000 | Loss: 0.00006294
Iteration 50/1000 | Loss: 0.00010986
Iteration 51/1000 | Loss: 0.00002546
Iteration 52/1000 | Loss: 0.00002469
Iteration 53/1000 | Loss: 0.00002410
Iteration 54/1000 | Loss: 0.00002366
Iteration 55/1000 | Loss: 0.00002337
Iteration 56/1000 | Loss: 0.00002317
Iteration 57/1000 | Loss: 0.00002306
Iteration 58/1000 | Loss: 0.00002291
Iteration 59/1000 | Loss: 0.00023793
Iteration 60/1000 | Loss: 0.00016498
Iteration 61/1000 | Loss: 0.00003367
Iteration 62/1000 | Loss: 0.00002745
Iteration 63/1000 | Loss: 0.00002518
Iteration 64/1000 | Loss: 0.00002347
Iteration 65/1000 | Loss: 0.00002823
Iteration 66/1000 | Loss: 0.00002314
Iteration 67/1000 | Loss: 0.00002575
Iteration 68/1000 | Loss: 0.00002211
Iteration 69/1000 | Loss: 0.00002196
Iteration 70/1000 | Loss: 0.00002190
Iteration 71/1000 | Loss: 0.00002187
Iteration 72/1000 | Loss: 0.00002187
Iteration 73/1000 | Loss: 0.00002178
Iteration 74/1000 | Loss: 0.00002172
Iteration 75/1000 | Loss: 0.00002171
Iteration 76/1000 | Loss: 0.00002158
Iteration 77/1000 | Loss: 0.00002158
Iteration 78/1000 | Loss: 0.00002156
Iteration 79/1000 | Loss: 0.00002156
Iteration 80/1000 | Loss: 0.00002155
Iteration 81/1000 | Loss: 0.00002155
Iteration 82/1000 | Loss: 0.00002155
Iteration 83/1000 | Loss: 0.00002154
Iteration 84/1000 | Loss: 0.00002152
Iteration 85/1000 | Loss: 0.00002152
Iteration 86/1000 | Loss: 0.00002152
Iteration 87/1000 | Loss: 0.00002152
Iteration 88/1000 | Loss: 0.00002151
Iteration 89/1000 | Loss: 0.00002151
Iteration 90/1000 | Loss: 0.00002150
Iteration 91/1000 | Loss: 0.00002150
Iteration 92/1000 | Loss: 0.00002149
Iteration 93/1000 | Loss: 0.00002149
Iteration 94/1000 | Loss: 0.00002149
Iteration 95/1000 | Loss: 0.00002149
Iteration 96/1000 | Loss: 0.00002149
Iteration 97/1000 | Loss: 0.00002149
Iteration 98/1000 | Loss: 0.00002148
Iteration 99/1000 | Loss: 0.00002148
Iteration 100/1000 | Loss: 0.00002148
Iteration 101/1000 | Loss: 0.00002148
Iteration 102/1000 | Loss: 0.00002148
Iteration 103/1000 | Loss: 0.00002148
Iteration 104/1000 | Loss: 0.00002147
Iteration 105/1000 | Loss: 0.00002147
Iteration 106/1000 | Loss: 0.00002147
Iteration 107/1000 | Loss: 0.00002147
Iteration 108/1000 | Loss: 0.00002147
Iteration 109/1000 | Loss: 0.00002147
Iteration 110/1000 | Loss: 0.00002147
Iteration 111/1000 | Loss: 0.00002147
Iteration 112/1000 | Loss: 0.00002146
Iteration 113/1000 | Loss: 0.00002146
Iteration 114/1000 | Loss: 0.00002146
Iteration 115/1000 | Loss: 0.00002146
Iteration 116/1000 | Loss: 0.00002146
Iteration 117/1000 | Loss: 0.00002146
Iteration 118/1000 | Loss: 0.00002146
Iteration 119/1000 | Loss: 0.00002146
Iteration 120/1000 | Loss: 0.00002146
Iteration 121/1000 | Loss: 0.00002146
Iteration 122/1000 | Loss: 0.00002146
Iteration 123/1000 | Loss: 0.00002146
Iteration 124/1000 | Loss: 0.00002146
Iteration 125/1000 | Loss: 0.00002146
Iteration 126/1000 | Loss: 0.00002146
Iteration 127/1000 | Loss: 0.00002146
Iteration 128/1000 | Loss: 0.00002145
Iteration 129/1000 | Loss: 0.00002145
Iteration 130/1000 | Loss: 0.00002145
Iteration 131/1000 | Loss: 0.00002145
Iteration 132/1000 | Loss: 0.00002145
Iteration 133/1000 | Loss: 0.00002145
Iteration 134/1000 | Loss: 0.00002145
Iteration 135/1000 | Loss: 0.00002145
Iteration 136/1000 | Loss: 0.00002145
Iteration 137/1000 | Loss: 0.00002145
Iteration 138/1000 | Loss: 0.00002145
Iteration 139/1000 | Loss: 0.00002145
Iteration 140/1000 | Loss: 0.00002145
Iteration 141/1000 | Loss: 0.00002145
Iteration 142/1000 | Loss: 0.00002145
Iteration 143/1000 | Loss: 0.00002145
Iteration 144/1000 | Loss: 0.00002145
Iteration 145/1000 | Loss: 0.00002145
Iteration 146/1000 | Loss: 0.00002145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.1450414351420477e-05, 2.1450414351420477e-05, 2.1450414351420477e-05, 2.1450414351420477e-05, 2.1450414351420477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1450414351420477e-05

Optimization complete. Final v2v error: 3.806358575820923 mm

Highest mean error: 5.209066867828369 mm for frame 47

Lowest mean error: 2.980708599090576 mm for frame 80

Saving results

Total time: 175.1152720451355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380305
Iteration 2/25 | Loss: 0.00084175
Iteration 3/25 | Loss: 0.00066037
Iteration 4/25 | Loss: 0.00064617
Iteration 5/25 | Loss: 0.00063722
Iteration 6/25 | Loss: 0.00063533
Iteration 7/25 | Loss: 0.00063517
Iteration 8/25 | Loss: 0.00063517
Iteration 9/25 | Loss: 0.00063517
Iteration 10/25 | Loss: 0.00063517
Iteration 11/25 | Loss: 0.00063517
Iteration 12/25 | Loss: 0.00063517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006351670599542558, 0.0006351670599542558, 0.0006351670599542558, 0.0006351670599542558, 0.0006351670599542558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006351670599542558

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61461926
Iteration 2/25 | Loss: 0.00032704
Iteration 3/25 | Loss: 0.00032704
Iteration 4/25 | Loss: 0.00032704
Iteration 5/25 | Loss: 0.00032704
Iteration 6/25 | Loss: 0.00032704
Iteration 7/25 | Loss: 0.00032704
Iteration 8/25 | Loss: 0.00032704
Iteration 9/25 | Loss: 0.00032703
Iteration 10/25 | Loss: 0.00032703
Iteration 11/25 | Loss: 0.00032703
Iteration 12/25 | Loss: 0.00032703
Iteration 13/25 | Loss: 0.00032703
Iteration 14/25 | Loss: 0.00032703
Iteration 15/25 | Loss: 0.00032703
Iteration 16/25 | Loss: 0.00032703
Iteration 17/25 | Loss: 0.00032703
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00032703462056815624, 0.00032703462056815624, 0.00032703462056815624, 0.00032703462056815624, 0.00032703462056815624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032703462056815624

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032703
Iteration 2/1000 | Loss: 0.00002398
Iteration 3/1000 | Loss: 0.00001582
Iteration 4/1000 | Loss: 0.00001445
Iteration 5/1000 | Loss: 0.00001366
Iteration 6/1000 | Loss: 0.00001322
Iteration 7/1000 | Loss: 0.00001295
Iteration 8/1000 | Loss: 0.00001272
Iteration 9/1000 | Loss: 0.00001254
Iteration 10/1000 | Loss: 0.00001237
Iteration 11/1000 | Loss: 0.00001236
Iteration 12/1000 | Loss: 0.00001235
Iteration 13/1000 | Loss: 0.00001235
Iteration 14/1000 | Loss: 0.00001234
Iteration 15/1000 | Loss: 0.00001234
Iteration 16/1000 | Loss: 0.00001234
Iteration 17/1000 | Loss: 0.00001232
Iteration 18/1000 | Loss: 0.00001231
Iteration 19/1000 | Loss: 0.00001230
Iteration 20/1000 | Loss: 0.00001229
Iteration 21/1000 | Loss: 0.00001228
Iteration 22/1000 | Loss: 0.00001223
Iteration 23/1000 | Loss: 0.00001223
Iteration 24/1000 | Loss: 0.00001223
Iteration 25/1000 | Loss: 0.00001223
Iteration 26/1000 | Loss: 0.00001223
Iteration 27/1000 | Loss: 0.00001222
Iteration 28/1000 | Loss: 0.00001222
Iteration 29/1000 | Loss: 0.00001222
Iteration 30/1000 | Loss: 0.00001222
Iteration 31/1000 | Loss: 0.00001222
Iteration 32/1000 | Loss: 0.00001222
Iteration 33/1000 | Loss: 0.00001220
Iteration 34/1000 | Loss: 0.00001220
Iteration 35/1000 | Loss: 0.00001220
Iteration 36/1000 | Loss: 0.00001219
Iteration 37/1000 | Loss: 0.00001219
Iteration 38/1000 | Loss: 0.00001219
Iteration 39/1000 | Loss: 0.00001218
Iteration 40/1000 | Loss: 0.00001217
Iteration 41/1000 | Loss: 0.00001216
Iteration 42/1000 | Loss: 0.00001216
Iteration 43/1000 | Loss: 0.00001216
Iteration 44/1000 | Loss: 0.00001215
Iteration 45/1000 | Loss: 0.00001215
Iteration 46/1000 | Loss: 0.00001215
Iteration 47/1000 | Loss: 0.00001215
Iteration 48/1000 | Loss: 0.00001214
Iteration 49/1000 | Loss: 0.00001214
Iteration 50/1000 | Loss: 0.00001213
Iteration 51/1000 | Loss: 0.00001212
Iteration 52/1000 | Loss: 0.00001212
Iteration 53/1000 | Loss: 0.00001212
Iteration 54/1000 | Loss: 0.00001212
Iteration 55/1000 | Loss: 0.00001211
Iteration 56/1000 | Loss: 0.00001211
Iteration 57/1000 | Loss: 0.00001211
Iteration 58/1000 | Loss: 0.00001210
Iteration 59/1000 | Loss: 0.00001210
Iteration 60/1000 | Loss: 0.00001209
Iteration 61/1000 | Loss: 0.00001209
Iteration 62/1000 | Loss: 0.00001209
Iteration 63/1000 | Loss: 0.00001209
Iteration 64/1000 | Loss: 0.00001209
Iteration 65/1000 | Loss: 0.00001209
Iteration 66/1000 | Loss: 0.00001209
Iteration 67/1000 | Loss: 0.00001208
Iteration 68/1000 | Loss: 0.00001208
Iteration 69/1000 | Loss: 0.00001208
Iteration 70/1000 | Loss: 0.00001208
Iteration 71/1000 | Loss: 0.00001208
Iteration 72/1000 | Loss: 0.00001208
Iteration 73/1000 | Loss: 0.00001208
Iteration 74/1000 | Loss: 0.00001208
Iteration 75/1000 | Loss: 0.00001208
Iteration 76/1000 | Loss: 0.00001208
Iteration 77/1000 | Loss: 0.00001208
Iteration 78/1000 | Loss: 0.00001207
Iteration 79/1000 | Loss: 0.00001207
Iteration 80/1000 | Loss: 0.00001207
Iteration 81/1000 | Loss: 0.00001207
Iteration 82/1000 | Loss: 0.00001207
Iteration 83/1000 | Loss: 0.00001207
Iteration 84/1000 | Loss: 0.00001207
Iteration 85/1000 | Loss: 0.00001207
Iteration 86/1000 | Loss: 0.00001207
Iteration 87/1000 | Loss: 0.00001207
Iteration 88/1000 | Loss: 0.00001207
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001206
Iteration 91/1000 | Loss: 0.00001206
Iteration 92/1000 | Loss: 0.00001206
Iteration 93/1000 | Loss: 0.00001206
Iteration 94/1000 | Loss: 0.00001206
Iteration 95/1000 | Loss: 0.00001206
Iteration 96/1000 | Loss: 0.00001206
Iteration 97/1000 | Loss: 0.00001206
Iteration 98/1000 | Loss: 0.00001206
Iteration 99/1000 | Loss: 0.00001206
Iteration 100/1000 | Loss: 0.00001206
Iteration 101/1000 | Loss: 0.00001206
Iteration 102/1000 | Loss: 0.00001206
Iteration 103/1000 | Loss: 0.00001206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.2063468602718785e-05, 1.2063468602718785e-05, 1.2063468602718785e-05, 1.2063468602718785e-05, 1.2063468602718785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2063468602718785e-05

Optimization complete. Final v2v error: 2.9556264877319336 mm

Highest mean error: 3.180663824081421 mm for frame 85

Lowest mean error: 2.808892250061035 mm for frame 227

Saving results

Total time: 35.02204346656799
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394619
Iteration 2/25 | Loss: 0.00086413
Iteration 3/25 | Loss: 0.00065092
Iteration 4/25 | Loss: 0.00062383
Iteration 5/25 | Loss: 0.00061400
Iteration 6/25 | Loss: 0.00061111
Iteration 7/25 | Loss: 0.00061028
Iteration 8/25 | Loss: 0.00061024
Iteration 9/25 | Loss: 0.00061024
Iteration 10/25 | Loss: 0.00061024
Iteration 11/25 | Loss: 0.00061024
Iteration 12/25 | Loss: 0.00061024
Iteration 13/25 | Loss: 0.00061024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006102393963374197, 0.0006102393963374197, 0.0006102393963374197, 0.0006102393963374197, 0.0006102393963374197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006102393963374197

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35794711
Iteration 2/25 | Loss: 0.00028178
Iteration 3/25 | Loss: 0.00028176
Iteration 4/25 | Loss: 0.00028176
Iteration 5/25 | Loss: 0.00028176
Iteration 6/25 | Loss: 0.00028175
Iteration 7/25 | Loss: 0.00028175
Iteration 8/25 | Loss: 0.00028175
Iteration 9/25 | Loss: 0.00028175
Iteration 10/25 | Loss: 0.00028175
Iteration 11/25 | Loss: 0.00028175
Iteration 12/25 | Loss: 0.00028175
Iteration 13/25 | Loss: 0.00028175
Iteration 14/25 | Loss: 0.00028175
Iteration 15/25 | Loss: 0.00028175
Iteration 16/25 | Loss: 0.00028175
Iteration 17/25 | Loss: 0.00028175
Iteration 18/25 | Loss: 0.00028175
Iteration 19/25 | Loss: 0.00028175
Iteration 20/25 | Loss: 0.00028175
Iteration 21/25 | Loss: 0.00028175
Iteration 22/25 | Loss: 0.00028175
Iteration 23/25 | Loss: 0.00028175
Iteration 24/25 | Loss: 0.00028175
Iteration 25/25 | Loss: 0.00028175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028175
Iteration 2/1000 | Loss: 0.00003196
Iteration 3/1000 | Loss: 0.00002108
Iteration 4/1000 | Loss: 0.00001656
Iteration 5/1000 | Loss: 0.00001563
Iteration 6/1000 | Loss: 0.00001496
Iteration 7/1000 | Loss: 0.00001446
Iteration 8/1000 | Loss: 0.00001418
Iteration 9/1000 | Loss: 0.00001390
Iteration 10/1000 | Loss: 0.00001373
Iteration 11/1000 | Loss: 0.00001369
Iteration 12/1000 | Loss: 0.00001366
Iteration 13/1000 | Loss: 0.00001366
Iteration 14/1000 | Loss: 0.00001363
Iteration 15/1000 | Loss: 0.00001361
Iteration 16/1000 | Loss: 0.00001359
Iteration 17/1000 | Loss: 0.00001358
Iteration 18/1000 | Loss: 0.00001357
Iteration 19/1000 | Loss: 0.00001357
Iteration 20/1000 | Loss: 0.00001356
Iteration 21/1000 | Loss: 0.00001356
Iteration 22/1000 | Loss: 0.00001356
Iteration 23/1000 | Loss: 0.00001355
Iteration 24/1000 | Loss: 0.00001352
Iteration 25/1000 | Loss: 0.00001348
Iteration 26/1000 | Loss: 0.00001348
Iteration 27/1000 | Loss: 0.00001347
Iteration 28/1000 | Loss: 0.00001347
Iteration 29/1000 | Loss: 0.00001343
Iteration 30/1000 | Loss: 0.00001343
Iteration 31/1000 | Loss: 0.00001341
Iteration 32/1000 | Loss: 0.00001339
Iteration 33/1000 | Loss: 0.00001339
Iteration 34/1000 | Loss: 0.00001338
Iteration 35/1000 | Loss: 0.00001338
Iteration 36/1000 | Loss: 0.00001337
Iteration 37/1000 | Loss: 0.00001336
Iteration 38/1000 | Loss: 0.00001331
Iteration 39/1000 | Loss: 0.00001331
Iteration 40/1000 | Loss: 0.00001330
Iteration 41/1000 | Loss: 0.00001330
Iteration 42/1000 | Loss: 0.00001330
Iteration 43/1000 | Loss: 0.00001328
Iteration 44/1000 | Loss: 0.00001328
Iteration 45/1000 | Loss: 0.00001328
Iteration 46/1000 | Loss: 0.00001328
Iteration 47/1000 | Loss: 0.00001328
Iteration 48/1000 | Loss: 0.00001328
Iteration 49/1000 | Loss: 0.00001328
Iteration 50/1000 | Loss: 0.00001328
Iteration 51/1000 | Loss: 0.00001328
Iteration 52/1000 | Loss: 0.00001328
Iteration 53/1000 | Loss: 0.00001327
Iteration 54/1000 | Loss: 0.00001327
Iteration 55/1000 | Loss: 0.00001327
Iteration 56/1000 | Loss: 0.00001327
Iteration 57/1000 | Loss: 0.00001326
Iteration 58/1000 | Loss: 0.00001326
Iteration 59/1000 | Loss: 0.00001326
Iteration 60/1000 | Loss: 0.00001325
Iteration 61/1000 | Loss: 0.00001325
Iteration 62/1000 | Loss: 0.00001324
Iteration 63/1000 | Loss: 0.00001324
Iteration 64/1000 | Loss: 0.00001323
Iteration 65/1000 | Loss: 0.00001323
Iteration 66/1000 | Loss: 0.00001322
Iteration 67/1000 | Loss: 0.00001322
Iteration 68/1000 | Loss: 0.00001322
Iteration 69/1000 | Loss: 0.00001322
Iteration 70/1000 | Loss: 0.00001322
Iteration 71/1000 | Loss: 0.00001321
Iteration 72/1000 | Loss: 0.00001321
Iteration 73/1000 | Loss: 0.00001321
Iteration 74/1000 | Loss: 0.00001321
Iteration 75/1000 | Loss: 0.00001321
Iteration 76/1000 | Loss: 0.00001321
Iteration 77/1000 | Loss: 0.00001321
Iteration 78/1000 | Loss: 0.00001321
Iteration 79/1000 | Loss: 0.00001320
Iteration 80/1000 | Loss: 0.00001320
Iteration 81/1000 | Loss: 0.00001320
Iteration 82/1000 | Loss: 0.00001319
Iteration 83/1000 | Loss: 0.00001319
Iteration 84/1000 | Loss: 0.00001319
Iteration 85/1000 | Loss: 0.00001319
Iteration 86/1000 | Loss: 0.00001318
Iteration 87/1000 | Loss: 0.00001318
Iteration 88/1000 | Loss: 0.00001318
Iteration 89/1000 | Loss: 0.00001318
Iteration 90/1000 | Loss: 0.00001318
Iteration 91/1000 | Loss: 0.00001318
Iteration 92/1000 | Loss: 0.00001317
Iteration 93/1000 | Loss: 0.00001317
Iteration 94/1000 | Loss: 0.00001317
Iteration 95/1000 | Loss: 0.00001317
Iteration 96/1000 | Loss: 0.00001317
Iteration 97/1000 | Loss: 0.00001317
Iteration 98/1000 | Loss: 0.00001316
Iteration 99/1000 | Loss: 0.00001316
Iteration 100/1000 | Loss: 0.00001316
Iteration 101/1000 | Loss: 0.00001316
Iteration 102/1000 | Loss: 0.00001316
Iteration 103/1000 | Loss: 0.00001316
Iteration 104/1000 | Loss: 0.00001316
Iteration 105/1000 | Loss: 0.00001316
Iteration 106/1000 | Loss: 0.00001316
Iteration 107/1000 | Loss: 0.00001316
Iteration 108/1000 | Loss: 0.00001315
Iteration 109/1000 | Loss: 0.00001315
Iteration 110/1000 | Loss: 0.00001315
Iteration 111/1000 | Loss: 0.00001314
Iteration 112/1000 | Loss: 0.00001314
Iteration 113/1000 | Loss: 0.00001314
Iteration 114/1000 | Loss: 0.00001314
Iteration 115/1000 | Loss: 0.00001314
Iteration 116/1000 | Loss: 0.00001314
Iteration 117/1000 | Loss: 0.00001313
Iteration 118/1000 | Loss: 0.00001313
Iteration 119/1000 | Loss: 0.00001313
Iteration 120/1000 | Loss: 0.00001313
Iteration 121/1000 | Loss: 0.00001313
Iteration 122/1000 | Loss: 0.00001313
Iteration 123/1000 | Loss: 0.00001313
Iteration 124/1000 | Loss: 0.00001313
Iteration 125/1000 | Loss: 0.00001313
Iteration 126/1000 | Loss: 0.00001313
Iteration 127/1000 | Loss: 0.00001313
Iteration 128/1000 | Loss: 0.00001313
Iteration 129/1000 | Loss: 0.00001313
Iteration 130/1000 | Loss: 0.00001313
Iteration 131/1000 | Loss: 0.00001313
Iteration 132/1000 | Loss: 0.00001313
Iteration 133/1000 | Loss: 0.00001313
Iteration 134/1000 | Loss: 0.00001313
Iteration 135/1000 | Loss: 0.00001313
Iteration 136/1000 | Loss: 0.00001313
Iteration 137/1000 | Loss: 0.00001313
Iteration 138/1000 | Loss: 0.00001313
Iteration 139/1000 | Loss: 0.00001313
Iteration 140/1000 | Loss: 0.00001313
Iteration 141/1000 | Loss: 0.00001313
Iteration 142/1000 | Loss: 0.00001313
Iteration 143/1000 | Loss: 0.00001313
Iteration 144/1000 | Loss: 0.00001313
Iteration 145/1000 | Loss: 0.00001313
Iteration 146/1000 | Loss: 0.00001313
Iteration 147/1000 | Loss: 0.00001313
Iteration 148/1000 | Loss: 0.00001313
Iteration 149/1000 | Loss: 0.00001313
Iteration 150/1000 | Loss: 0.00001313
Iteration 151/1000 | Loss: 0.00001313
Iteration 152/1000 | Loss: 0.00001313
Iteration 153/1000 | Loss: 0.00001313
Iteration 154/1000 | Loss: 0.00001313
Iteration 155/1000 | Loss: 0.00001313
Iteration 156/1000 | Loss: 0.00001313
Iteration 157/1000 | Loss: 0.00001313
Iteration 158/1000 | Loss: 0.00001313
Iteration 159/1000 | Loss: 0.00001313
Iteration 160/1000 | Loss: 0.00001313
Iteration 161/1000 | Loss: 0.00001313
Iteration 162/1000 | Loss: 0.00001313
Iteration 163/1000 | Loss: 0.00001313
Iteration 164/1000 | Loss: 0.00001313
Iteration 165/1000 | Loss: 0.00001313
Iteration 166/1000 | Loss: 0.00001313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.3126124031259678e-05, 1.3126124031259678e-05, 1.3126124031259678e-05, 1.3126124031259678e-05, 1.3126124031259678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3126124031259678e-05

Optimization complete. Final v2v error: 2.9001011848449707 mm

Highest mean error: 5.066575527191162 mm for frame 74

Lowest mean error: 2.3050453662872314 mm for frame 110

Saving results

Total time: 40.00415635108948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533560
Iteration 2/25 | Loss: 0.00103291
Iteration 3/25 | Loss: 0.00067207
Iteration 4/25 | Loss: 0.00064614
Iteration 5/25 | Loss: 0.00064082
Iteration 6/25 | Loss: 0.00063873
Iteration 7/25 | Loss: 0.00063833
Iteration 8/25 | Loss: 0.00063833
Iteration 9/25 | Loss: 0.00063833
Iteration 10/25 | Loss: 0.00063833
Iteration 11/25 | Loss: 0.00063833
Iteration 12/25 | Loss: 0.00063833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006383324507623911, 0.0006383324507623911, 0.0006383324507623911, 0.0006383324507623911, 0.0006383324507623911]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006383324507623911

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80271095
Iteration 2/25 | Loss: 0.00019014
Iteration 3/25 | Loss: 0.00019013
Iteration 4/25 | Loss: 0.00019013
Iteration 5/25 | Loss: 0.00019013
Iteration 6/25 | Loss: 0.00019013
Iteration 7/25 | Loss: 0.00019013
Iteration 8/25 | Loss: 0.00019013
Iteration 9/25 | Loss: 0.00019013
Iteration 10/25 | Loss: 0.00019013
Iteration 11/25 | Loss: 0.00019013
Iteration 12/25 | Loss: 0.00019013
Iteration 13/25 | Loss: 0.00019013
Iteration 14/25 | Loss: 0.00019013
Iteration 15/25 | Loss: 0.00019013
Iteration 16/25 | Loss: 0.00019013
Iteration 17/25 | Loss: 0.00019013
Iteration 18/25 | Loss: 0.00019013
Iteration 19/25 | Loss: 0.00019013
Iteration 20/25 | Loss: 0.00019013
Iteration 21/25 | Loss: 0.00019013
Iteration 22/25 | Loss: 0.00019013
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0001901325595099479, 0.0001901325595099479, 0.0001901325595099479, 0.0001901325595099479, 0.0001901325595099479]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001901325595099479

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019013
Iteration 2/1000 | Loss: 0.00002744
Iteration 3/1000 | Loss: 0.00001445
Iteration 4/1000 | Loss: 0.00001289
Iteration 5/1000 | Loss: 0.00001244
Iteration 6/1000 | Loss: 0.00001196
Iteration 7/1000 | Loss: 0.00001168
Iteration 8/1000 | Loss: 0.00001145
Iteration 9/1000 | Loss: 0.00001128
Iteration 10/1000 | Loss: 0.00001127
Iteration 11/1000 | Loss: 0.00001112
Iteration 12/1000 | Loss: 0.00001109
Iteration 13/1000 | Loss: 0.00001099
Iteration 14/1000 | Loss: 0.00001091
Iteration 15/1000 | Loss: 0.00001081
Iteration 16/1000 | Loss: 0.00001081
Iteration 17/1000 | Loss: 0.00001081
Iteration 18/1000 | Loss: 0.00001079
Iteration 19/1000 | Loss: 0.00001078
Iteration 20/1000 | Loss: 0.00001078
Iteration 21/1000 | Loss: 0.00001078
Iteration 22/1000 | Loss: 0.00001078
Iteration 23/1000 | Loss: 0.00001077
Iteration 24/1000 | Loss: 0.00001077
Iteration 25/1000 | Loss: 0.00001077
Iteration 26/1000 | Loss: 0.00001074
Iteration 27/1000 | Loss: 0.00001074
Iteration 28/1000 | Loss: 0.00001074
Iteration 29/1000 | Loss: 0.00001074
Iteration 30/1000 | Loss: 0.00001074
Iteration 31/1000 | Loss: 0.00001074
Iteration 32/1000 | Loss: 0.00001074
Iteration 33/1000 | Loss: 0.00001074
Iteration 34/1000 | Loss: 0.00001074
Iteration 35/1000 | Loss: 0.00001074
Iteration 36/1000 | Loss: 0.00001074
Iteration 37/1000 | Loss: 0.00001073
Iteration 38/1000 | Loss: 0.00001073
Iteration 39/1000 | Loss: 0.00001072
Iteration 40/1000 | Loss: 0.00001072
Iteration 41/1000 | Loss: 0.00001071
Iteration 42/1000 | Loss: 0.00001071
Iteration 43/1000 | Loss: 0.00001071
Iteration 44/1000 | Loss: 0.00001071
Iteration 45/1000 | Loss: 0.00001071
Iteration 46/1000 | Loss: 0.00001070
Iteration 47/1000 | Loss: 0.00001070
Iteration 48/1000 | Loss: 0.00001070
Iteration 49/1000 | Loss: 0.00001069
Iteration 50/1000 | Loss: 0.00001069
Iteration 51/1000 | Loss: 0.00001068
Iteration 52/1000 | Loss: 0.00001068
Iteration 53/1000 | Loss: 0.00001068
Iteration 54/1000 | Loss: 0.00001068
Iteration 55/1000 | Loss: 0.00001068
Iteration 56/1000 | Loss: 0.00001068
Iteration 57/1000 | Loss: 0.00001067
Iteration 58/1000 | Loss: 0.00001067
Iteration 59/1000 | Loss: 0.00001067
Iteration 60/1000 | Loss: 0.00001067
Iteration 61/1000 | Loss: 0.00001067
Iteration 62/1000 | Loss: 0.00001067
Iteration 63/1000 | Loss: 0.00001067
Iteration 64/1000 | Loss: 0.00001066
Iteration 65/1000 | Loss: 0.00001065
Iteration 66/1000 | Loss: 0.00001065
Iteration 67/1000 | Loss: 0.00001065
Iteration 68/1000 | Loss: 0.00001065
Iteration 69/1000 | Loss: 0.00001064
Iteration 70/1000 | Loss: 0.00001063
Iteration 71/1000 | Loss: 0.00001063
Iteration 72/1000 | Loss: 0.00001062
Iteration 73/1000 | Loss: 0.00001062
Iteration 74/1000 | Loss: 0.00001062
Iteration 75/1000 | Loss: 0.00001061
Iteration 76/1000 | Loss: 0.00001061
Iteration 77/1000 | Loss: 0.00001061
Iteration 78/1000 | Loss: 0.00001061
Iteration 79/1000 | Loss: 0.00001061
Iteration 80/1000 | Loss: 0.00001060
Iteration 81/1000 | Loss: 0.00001060
Iteration 82/1000 | Loss: 0.00001058
Iteration 83/1000 | Loss: 0.00001058
Iteration 84/1000 | Loss: 0.00001058
Iteration 85/1000 | Loss: 0.00001058
Iteration 86/1000 | Loss: 0.00001058
Iteration 87/1000 | Loss: 0.00001058
Iteration 88/1000 | Loss: 0.00001058
Iteration 89/1000 | Loss: 0.00001058
Iteration 90/1000 | Loss: 0.00001057
Iteration 91/1000 | Loss: 0.00001057
Iteration 92/1000 | Loss: 0.00001057
Iteration 93/1000 | Loss: 0.00001056
Iteration 94/1000 | Loss: 0.00001056
Iteration 95/1000 | Loss: 0.00001055
Iteration 96/1000 | Loss: 0.00001055
Iteration 97/1000 | Loss: 0.00001055
Iteration 98/1000 | Loss: 0.00001055
Iteration 99/1000 | Loss: 0.00001054
Iteration 100/1000 | Loss: 0.00001054
Iteration 101/1000 | Loss: 0.00001053
Iteration 102/1000 | Loss: 0.00001053
Iteration 103/1000 | Loss: 0.00001053
Iteration 104/1000 | Loss: 0.00001053
Iteration 105/1000 | Loss: 0.00001052
Iteration 106/1000 | Loss: 0.00001052
Iteration 107/1000 | Loss: 0.00001052
Iteration 108/1000 | Loss: 0.00001052
Iteration 109/1000 | Loss: 0.00001052
Iteration 110/1000 | Loss: 0.00001052
Iteration 111/1000 | Loss: 0.00001052
Iteration 112/1000 | Loss: 0.00001052
Iteration 113/1000 | Loss: 0.00001052
Iteration 114/1000 | Loss: 0.00001052
Iteration 115/1000 | Loss: 0.00001051
Iteration 116/1000 | Loss: 0.00001051
Iteration 117/1000 | Loss: 0.00001051
Iteration 118/1000 | Loss: 0.00001051
Iteration 119/1000 | Loss: 0.00001051
Iteration 120/1000 | Loss: 0.00001051
Iteration 121/1000 | Loss: 0.00001051
Iteration 122/1000 | Loss: 0.00001051
Iteration 123/1000 | Loss: 0.00001051
Iteration 124/1000 | Loss: 0.00001051
Iteration 125/1000 | Loss: 0.00001051
Iteration 126/1000 | Loss: 0.00001051
Iteration 127/1000 | Loss: 0.00001050
Iteration 128/1000 | Loss: 0.00001050
Iteration 129/1000 | Loss: 0.00001050
Iteration 130/1000 | Loss: 0.00001050
Iteration 131/1000 | Loss: 0.00001050
Iteration 132/1000 | Loss: 0.00001050
Iteration 133/1000 | Loss: 0.00001049
Iteration 134/1000 | Loss: 0.00001049
Iteration 135/1000 | Loss: 0.00001049
Iteration 136/1000 | Loss: 0.00001049
Iteration 137/1000 | Loss: 0.00001049
Iteration 138/1000 | Loss: 0.00001049
Iteration 139/1000 | Loss: 0.00001049
Iteration 140/1000 | Loss: 0.00001049
Iteration 141/1000 | Loss: 0.00001049
Iteration 142/1000 | Loss: 0.00001048
Iteration 143/1000 | Loss: 0.00001048
Iteration 144/1000 | Loss: 0.00001048
Iteration 145/1000 | Loss: 0.00001048
Iteration 146/1000 | Loss: 0.00001047
Iteration 147/1000 | Loss: 0.00001047
Iteration 148/1000 | Loss: 0.00001047
Iteration 149/1000 | Loss: 0.00001047
Iteration 150/1000 | Loss: 0.00001047
Iteration 151/1000 | Loss: 0.00001046
Iteration 152/1000 | Loss: 0.00001046
Iteration 153/1000 | Loss: 0.00001046
Iteration 154/1000 | Loss: 0.00001046
Iteration 155/1000 | Loss: 0.00001046
Iteration 156/1000 | Loss: 0.00001046
Iteration 157/1000 | Loss: 0.00001046
Iteration 158/1000 | Loss: 0.00001045
Iteration 159/1000 | Loss: 0.00001045
Iteration 160/1000 | Loss: 0.00001045
Iteration 161/1000 | Loss: 0.00001045
Iteration 162/1000 | Loss: 0.00001045
Iteration 163/1000 | Loss: 0.00001045
Iteration 164/1000 | Loss: 0.00001044
Iteration 165/1000 | Loss: 0.00001044
Iteration 166/1000 | Loss: 0.00001044
Iteration 167/1000 | Loss: 0.00001044
Iteration 168/1000 | Loss: 0.00001044
Iteration 169/1000 | Loss: 0.00001043
Iteration 170/1000 | Loss: 0.00001043
Iteration 171/1000 | Loss: 0.00001043
Iteration 172/1000 | Loss: 0.00001043
Iteration 173/1000 | Loss: 0.00001043
Iteration 174/1000 | Loss: 0.00001043
Iteration 175/1000 | Loss: 0.00001043
Iteration 176/1000 | Loss: 0.00001043
Iteration 177/1000 | Loss: 0.00001043
Iteration 178/1000 | Loss: 0.00001043
Iteration 179/1000 | Loss: 0.00001043
Iteration 180/1000 | Loss: 0.00001042
Iteration 181/1000 | Loss: 0.00001042
Iteration 182/1000 | Loss: 0.00001042
Iteration 183/1000 | Loss: 0.00001042
Iteration 184/1000 | Loss: 0.00001042
Iteration 185/1000 | Loss: 0.00001042
Iteration 186/1000 | Loss: 0.00001042
Iteration 187/1000 | Loss: 0.00001042
Iteration 188/1000 | Loss: 0.00001042
Iteration 189/1000 | Loss: 0.00001042
Iteration 190/1000 | Loss: 0.00001042
Iteration 191/1000 | Loss: 0.00001042
Iteration 192/1000 | Loss: 0.00001042
Iteration 193/1000 | Loss: 0.00001042
Iteration 194/1000 | Loss: 0.00001042
Iteration 195/1000 | Loss: 0.00001042
Iteration 196/1000 | Loss: 0.00001042
Iteration 197/1000 | Loss: 0.00001042
Iteration 198/1000 | Loss: 0.00001042
Iteration 199/1000 | Loss: 0.00001042
Iteration 200/1000 | Loss: 0.00001042
Iteration 201/1000 | Loss: 0.00001042
Iteration 202/1000 | Loss: 0.00001042
Iteration 203/1000 | Loss: 0.00001042
Iteration 204/1000 | Loss: 0.00001042
Iteration 205/1000 | Loss: 0.00001042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.0415943506814074e-05, 1.0415943506814074e-05, 1.0415943506814074e-05, 1.0415943506814074e-05, 1.0415943506814074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0415943506814074e-05

Optimization complete. Final v2v error: 2.698620557785034 mm

Highest mean error: 3.370081663131714 mm for frame 15

Lowest mean error: 2.468380928039551 mm for frame 60

Saving results

Total time: 41.43528127670288
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850203
Iteration 2/25 | Loss: 0.00077712
Iteration 3/25 | Loss: 0.00067075
Iteration 4/25 | Loss: 0.00064352
Iteration 5/25 | Loss: 0.00064041
Iteration 6/25 | Loss: 0.00063940
Iteration 7/25 | Loss: 0.00063917
Iteration 8/25 | Loss: 0.00063917
Iteration 9/25 | Loss: 0.00063917
Iteration 10/25 | Loss: 0.00063917
Iteration 11/25 | Loss: 0.00063917
Iteration 12/25 | Loss: 0.00063917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006391743081621826, 0.0006391743081621826, 0.0006391743081621826, 0.0006391743081621826, 0.0006391743081621826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006391743081621826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47169113
Iteration 2/25 | Loss: 0.00027069
Iteration 3/25 | Loss: 0.00027068
Iteration 4/25 | Loss: 0.00027068
Iteration 5/25 | Loss: 0.00027068
Iteration 6/25 | Loss: 0.00027068
Iteration 7/25 | Loss: 0.00027068
Iteration 8/25 | Loss: 0.00027068
Iteration 9/25 | Loss: 0.00027068
Iteration 10/25 | Loss: 0.00027068
Iteration 11/25 | Loss: 0.00027068
Iteration 12/25 | Loss: 0.00027068
Iteration 13/25 | Loss: 0.00027068
Iteration 14/25 | Loss: 0.00027068
Iteration 15/25 | Loss: 0.00027068
Iteration 16/25 | Loss: 0.00027068
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00027068197960034013, 0.00027068197960034013, 0.00027068197960034013, 0.00027068197960034013, 0.00027068197960034013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027068197960034013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027068
Iteration 2/1000 | Loss: 0.00002851
Iteration 3/1000 | Loss: 0.00002059
Iteration 4/1000 | Loss: 0.00001957
Iteration 5/1000 | Loss: 0.00001834
Iteration 6/1000 | Loss: 0.00001781
Iteration 7/1000 | Loss: 0.00001722
Iteration 8/1000 | Loss: 0.00001695
Iteration 9/1000 | Loss: 0.00001682
Iteration 10/1000 | Loss: 0.00001672
Iteration 11/1000 | Loss: 0.00001672
Iteration 12/1000 | Loss: 0.00001671
Iteration 13/1000 | Loss: 0.00001670
Iteration 14/1000 | Loss: 0.00001667
Iteration 15/1000 | Loss: 0.00001666
Iteration 16/1000 | Loss: 0.00001665
Iteration 17/1000 | Loss: 0.00001665
Iteration 18/1000 | Loss: 0.00001664
Iteration 19/1000 | Loss: 0.00001664
Iteration 20/1000 | Loss: 0.00001662
Iteration 21/1000 | Loss: 0.00001661
Iteration 22/1000 | Loss: 0.00001661
Iteration 23/1000 | Loss: 0.00001661
Iteration 24/1000 | Loss: 0.00001660
Iteration 25/1000 | Loss: 0.00001660
Iteration 26/1000 | Loss: 0.00001659
Iteration 27/1000 | Loss: 0.00001659
Iteration 28/1000 | Loss: 0.00001659
Iteration 29/1000 | Loss: 0.00001659
Iteration 30/1000 | Loss: 0.00001659
Iteration 31/1000 | Loss: 0.00001659
Iteration 32/1000 | Loss: 0.00001659
Iteration 33/1000 | Loss: 0.00001658
Iteration 34/1000 | Loss: 0.00001658
Iteration 35/1000 | Loss: 0.00001658
Iteration 36/1000 | Loss: 0.00001658
Iteration 37/1000 | Loss: 0.00001657
Iteration 38/1000 | Loss: 0.00001657
Iteration 39/1000 | Loss: 0.00001657
Iteration 40/1000 | Loss: 0.00001656
Iteration 41/1000 | Loss: 0.00001656
Iteration 42/1000 | Loss: 0.00001656
Iteration 43/1000 | Loss: 0.00001656
Iteration 44/1000 | Loss: 0.00001656
Iteration 45/1000 | Loss: 0.00001655
Iteration 46/1000 | Loss: 0.00001655
Iteration 47/1000 | Loss: 0.00001655
Iteration 48/1000 | Loss: 0.00001655
Iteration 49/1000 | Loss: 0.00001654
Iteration 50/1000 | Loss: 0.00001654
Iteration 51/1000 | Loss: 0.00001654
Iteration 52/1000 | Loss: 0.00001654
Iteration 53/1000 | Loss: 0.00001654
Iteration 54/1000 | Loss: 0.00001654
Iteration 55/1000 | Loss: 0.00001654
Iteration 56/1000 | Loss: 0.00001654
Iteration 57/1000 | Loss: 0.00001653
Iteration 58/1000 | Loss: 0.00001653
Iteration 59/1000 | Loss: 0.00001653
Iteration 60/1000 | Loss: 0.00001652
Iteration 61/1000 | Loss: 0.00001652
Iteration 62/1000 | Loss: 0.00001652
Iteration 63/1000 | Loss: 0.00001652
Iteration 64/1000 | Loss: 0.00001652
Iteration 65/1000 | Loss: 0.00001651
Iteration 66/1000 | Loss: 0.00001651
Iteration 67/1000 | Loss: 0.00001651
Iteration 68/1000 | Loss: 0.00001651
Iteration 69/1000 | Loss: 0.00001650
Iteration 70/1000 | Loss: 0.00001650
Iteration 71/1000 | Loss: 0.00001650
Iteration 72/1000 | Loss: 0.00001649
Iteration 73/1000 | Loss: 0.00001649
Iteration 74/1000 | Loss: 0.00001649
Iteration 75/1000 | Loss: 0.00001649
Iteration 76/1000 | Loss: 0.00001649
Iteration 77/1000 | Loss: 0.00001649
Iteration 78/1000 | Loss: 0.00001649
Iteration 79/1000 | Loss: 0.00001649
Iteration 80/1000 | Loss: 0.00001649
Iteration 81/1000 | Loss: 0.00001649
Iteration 82/1000 | Loss: 0.00001649
Iteration 83/1000 | Loss: 0.00001649
Iteration 84/1000 | Loss: 0.00001649
Iteration 85/1000 | Loss: 0.00001649
Iteration 86/1000 | Loss: 0.00001648
Iteration 87/1000 | Loss: 0.00001648
Iteration 88/1000 | Loss: 0.00001648
Iteration 89/1000 | Loss: 0.00001648
Iteration 90/1000 | Loss: 0.00001648
Iteration 91/1000 | Loss: 0.00001648
Iteration 92/1000 | Loss: 0.00001648
Iteration 93/1000 | Loss: 0.00001648
Iteration 94/1000 | Loss: 0.00001648
Iteration 95/1000 | Loss: 0.00001648
Iteration 96/1000 | Loss: 0.00001647
Iteration 97/1000 | Loss: 0.00001647
Iteration 98/1000 | Loss: 0.00001647
Iteration 99/1000 | Loss: 0.00001646
Iteration 100/1000 | Loss: 0.00001646
Iteration 101/1000 | Loss: 0.00001646
Iteration 102/1000 | Loss: 0.00001646
Iteration 103/1000 | Loss: 0.00001646
Iteration 104/1000 | Loss: 0.00001646
Iteration 105/1000 | Loss: 0.00001645
Iteration 106/1000 | Loss: 0.00001645
Iteration 107/1000 | Loss: 0.00001645
Iteration 108/1000 | Loss: 0.00001645
Iteration 109/1000 | Loss: 0.00001644
Iteration 110/1000 | Loss: 0.00001644
Iteration 111/1000 | Loss: 0.00001644
Iteration 112/1000 | Loss: 0.00001644
Iteration 113/1000 | Loss: 0.00001643
Iteration 114/1000 | Loss: 0.00001643
Iteration 115/1000 | Loss: 0.00001642
Iteration 116/1000 | Loss: 0.00001642
Iteration 117/1000 | Loss: 0.00001641
Iteration 118/1000 | Loss: 0.00001640
Iteration 119/1000 | Loss: 0.00001640
Iteration 120/1000 | Loss: 0.00001640
Iteration 121/1000 | Loss: 0.00001639
Iteration 122/1000 | Loss: 0.00001639
Iteration 123/1000 | Loss: 0.00001638
Iteration 124/1000 | Loss: 0.00001637
Iteration 125/1000 | Loss: 0.00001637
Iteration 126/1000 | Loss: 0.00001636
Iteration 127/1000 | Loss: 0.00001636
Iteration 128/1000 | Loss: 0.00001635
Iteration 129/1000 | Loss: 0.00001635
Iteration 130/1000 | Loss: 0.00001635
Iteration 131/1000 | Loss: 0.00001635
Iteration 132/1000 | Loss: 0.00001635
Iteration 133/1000 | Loss: 0.00001635
Iteration 134/1000 | Loss: 0.00001634
Iteration 135/1000 | Loss: 0.00001634
Iteration 136/1000 | Loss: 0.00001633
Iteration 137/1000 | Loss: 0.00001633
Iteration 138/1000 | Loss: 0.00001633
Iteration 139/1000 | Loss: 0.00001632
Iteration 140/1000 | Loss: 0.00001632
Iteration 141/1000 | Loss: 0.00001632
Iteration 142/1000 | Loss: 0.00001632
Iteration 143/1000 | Loss: 0.00001632
Iteration 144/1000 | Loss: 0.00001632
Iteration 145/1000 | Loss: 0.00001632
Iteration 146/1000 | Loss: 0.00001631
Iteration 147/1000 | Loss: 0.00001631
Iteration 148/1000 | Loss: 0.00001631
Iteration 149/1000 | Loss: 0.00001631
Iteration 150/1000 | Loss: 0.00001631
Iteration 151/1000 | Loss: 0.00001631
Iteration 152/1000 | Loss: 0.00001631
Iteration 153/1000 | Loss: 0.00001631
Iteration 154/1000 | Loss: 0.00001631
Iteration 155/1000 | Loss: 0.00001631
Iteration 156/1000 | Loss: 0.00001631
Iteration 157/1000 | Loss: 0.00001630
Iteration 158/1000 | Loss: 0.00001630
Iteration 159/1000 | Loss: 0.00001630
Iteration 160/1000 | Loss: 0.00001630
Iteration 161/1000 | Loss: 0.00001630
Iteration 162/1000 | Loss: 0.00001630
Iteration 163/1000 | Loss: 0.00001630
Iteration 164/1000 | Loss: 0.00001630
Iteration 165/1000 | Loss: 0.00001630
Iteration 166/1000 | Loss: 0.00001630
Iteration 167/1000 | Loss: 0.00001630
Iteration 168/1000 | Loss: 0.00001630
Iteration 169/1000 | Loss: 0.00001630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.630418591958005e-05, 1.630418591958005e-05, 1.630418591958005e-05, 1.630418591958005e-05, 1.630418591958005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.630418591958005e-05

Optimization complete. Final v2v error: 3.3896100521087646 mm

Highest mean error: 3.476956605911255 mm for frame 68

Lowest mean error: 3.2975077629089355 mm for frame 58

Saving results

Total time: 34.8589403629303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867008
Iteration 2/25 | Loss: 0.00091174
Iteration 3/25 | Loss: 0.00066339
Iteration 4/25 | Loss: 0.00063455
Iteration 5/25 | Loss: 0.00062729
Iteration 6/25 | Loss: 0.00062500
Iteration 7/25 | Loss: 0.00062446
Iteration 8/25 | Loss: 0.00062446
Iteration 9/25 | Loss: 0.00062446
Iteration 10/25 | Loss: 0.00062446
Iteration 11/25 | Loss: 0.00062446
Iteration 12/25 | Loss: 0.00062446
Iteration 13/25 | Loss: 0.00062446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006244583055377007, 0.0006244583055377007, 0.0006244583055377007, 0.0006244583055377007, 0.0006244583055377007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006244583055377007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56530535
Iteration 2/25 | Loss: 0.00038677
Iteration 3/25 | Loss: 0.00038677
Iteration 4/25 | Loss: 0.00038677
Iteration 5/25 | Loss: 0.00038677
Iteration 6/25 | Loss: 0.00038677
Iteration 7/25 | Loss: 0.00038677
Iteration 8/25 | Loss: 0.00038677
Iteration 9/25 | Loss: 0.00038677
Iteration 10/25 | Loss: 0.00038677
Iteration 11/25 | Loss: 0.00038677
Iteration 12/25 | Loss: 0.00038677
Iteration 13/25 | Loss: 0.00038677
Iteration 14/25 | Loss: 0.00038677
Iteration 15/25 | Loss: 0.00038677
Iteration 16/25 | Loss: 0.00038677
Iteration 17/25 | Loss: 0.00038677
Iteration 18/25 | Loss: 0.00038677
Iteration 19/25 | Loss: 0.00038677
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00038676519761793315, 0.00038676519761793315, 0.00038676519761793315, 0.00038676519761793315, 0.00038676519761793315]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038676519761793315

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038677
Iteration 2/1000 | Loss: 0.00002820
Iteration 3/1000 | Loss: 0.00001412
Iteration 4/1000 | Loss: 0.00001238
Iteration 5/1000 | Loss: 0.00001185
Iteration 6/1000 | Loss: 0.00001138
Iteration 7/1000 | Loss: 0.00001111
Iteration 8/1000 | Loss: 0.00001104
Iteration 9/1000 | Loss: 0.00001082
Iteration 10/1000 | Loss: 0.00001080
Iteration 11/1000 | Loss: 0.00001079
Iteration 12/1000 | Loss: 0.00001063
Iteration 13/1000 | Loss: 0.00001063
Iteration 14/1000 | Loss: 0.00001056
Iteration 15/1000 | Loss: 0.00001056
Iteration 16/1000 | Loss: 0.00001055
Iteration 17/1000 | Loss: 0.00001054
Iteration 18/1000 | Loss: 0.00001052
Iteration 19/1000 | Loss: 0.00001051
Iteration 20/1000 | Loss: 0.00001051
Iteration 21/1000 | Loss: 0.00001051
Iteration 22/1000 | Loss: 0.00001050
Iteration 23/1000 | Loss: 0.00001050
Iteration 24/1000 | Loss: 0.00001050
Iteration 25/1000 | Loss: 0.00001050
Iteration 26/1000 | Loss: 0.00001049
Iteration 27/1000 | Loss: 0.00001049
Iteration 28/1000 | Loss: 0.00001048
Iteration 29/1000 | Loss: 0.00001048
Iteration 30/1000 | Loss: 0.00001048
Iteration 31/1000 | Loss: 0.00001047
Iteration 32/1000 | Loss: 0.00001047
Iteration 33/1000 | Loss: 0.00001046
Iteration 34/1000 | Loss: 0.00001046
Iteration 35/1000 | Loss: 0.00001045
Iteration 36/1000 | Loss: 0.00001045
Iteration 37/1000 | Loss: 0.00001044
Iteration 38/1000 | Loss: 0.00001044
Iteration 39/1000 | Loss: 0.00001043
Iteration 40/1000 | Loss: 0.00001043
Iteration 41/1000 | Loss: 0.00001042
Iteration 42/1000 | Loss: 0.00001041
Iteration 43/1000 | Loss: 0.00001041
Iteration 44/1000 | Loss: 0.00001040
Iteration 45/1000 | Loss: 0.00001039
Iteration 46/1000 | Loss: 0.00001038
Iteration 47/1000 | Loss: 0.00001038
Iteration 48/1000 | Loss: 0.00001038
Iteration 49/1000 | Loss: 0.00001038
Iteration 50/1000 | Loss: 0.00001038
Iteration 51/1000 | Loss: 0.00001038
Iteration 52/1000 | Loss: 0.00001038
Iteration 53/1000 | Loss: 0.00001037
Iteration 54/1000 | Loss: 0.00001037
Iteration 55/1000 | Loss: 0.00001037
Iteration 56/1000 | Loss: 0.00001037
Iteration 57/1000 | Loss: 0.00001036
Iteration 58/1000 | Loss: 0.00001036
Iteration 59/1000 | Loss: 0.00001036
Iteration 60/1000 | Loss: 0.00001035
Iteration 61/1000 | Loss: 0.00001035
Iteration 62/1000 | Loss: 0.00001034
Iteration 63/1000 | Loss: 0.00001034
Iteration 64/1000 | Loss: 0.00001033
Iteration 65/1000 | Loss: 0.00001033
Iteration 66/1000 | Loss: 0.00001032
Iteration 67/1000 | Loss: 0.00001032
Iteration 68/1000 | Loss: 0.00001031
Iteration 69/1000 | Loss: 0.00001031
Iteration 70/1000 | Loss: 0.00001031
Iteration 71/1000 | Loss: 0.00001030
Iteration 72/1000 | Loss: 0.00001030
Iteration 73/1000 | Loss: 0.00001029
Iteration 74/1000 | Loss: 0.00001029
Iteration 75/1000 | Loss: 0.00001029
Iteration 76/1000 | Loss: 0.00001029
Iteration 77/1000 | Loss: 0.00001029
Iteration 78/1000 | Loss: 0.00001029
Iteration 79/1000 | Loss: 0.00001029
Iteration 80/1000 | Loss: 0.00001028
Iteration 81/1000 | Loss: 0.00001028
Iteration 82/1000 | Loss: 0.00001028
Iteration 83/1000 | Loss: 0.00001028
Iteration 84/1000 | Loss: 0.00001028
Iteration 85/1000 | Loss: 0.00001028
Iteration 86/1000 | Loss: 0.00001028
Iteration 87/1000 | Loss: 0.00001028
Iteration 88/1000 | Loss: 0.00001028
Iteration 89/1000 | Loss: 0.00001027
Iteration 90/1000 | Loss: 0.00001027
Iteration 91/1000 | Loss: 0.00001027
Iteration 92/1000 | Loss: 0.00001026
Iteration 93/1000 | Loss: 0.00001026
Iteration 94/1000 | Loss: 0.00001026
Iteration 95/1000 | Loss: 0.00001026
Iteration 96/1000 | Loss: 0.00001026
Iteration 97/1000 | Loss: 0.00001025
Iteration 98/1000 | Loss: 0.00001025
Iteration 99/1000 | Loss: 0.00001025
Iteration 100/1000 | Loss: 0.00001025
Iteration 101/1000 | Loss: 0.00001025
Iteration 102/1000 | Loss: 0.00001024
Iteration 103/1000 | Loss: 0.00001024
Iteration 104/1000 | Loss: 0.00001024
Iteration 105/1000 | Loss: 0.00001024
Iteration 106/1000 | Loss: 0.00001024
Iteration 107/1000 | Loss: 0.00001024
Iteration 108/1000 | Loss: 0.00001024
Iteration 109/1000 | Loss: 0.00001024
Iteration 110/1000 | Loss: 0.00001024
Iteration 111/1000 | Loss: 0.00001024
Iteration 112/1000 | Loss: 0.00001024
Iteration 113/1000 | Loss: 0.00001024
Iteration 114/1000 | Loss: 0.00001024
Iteration 115/1000 | Loss: 0.00001023
Iteration 116/1000 | Loss: 0.00001023
Iteration 117/1000 | Loss: 0.00001023
Iteration 118/1000 | Loss: 0.00001023
Iteration 119/1000 | Loss: 0.00001023
Iteration 120/1000 | Loss: 0.00001022
Iteration 121/1000 | Loss: 0.00001022
Iteration 122/1000 | Loss: 0.00001022
Iteration 123/1000 | Loss: 0.00001022
Iteration 124/1000 | Loss: 0.00001022
Iteration 125/1000 | Loss: 0.00001022
Iteration 126/1000 | Loss: 0.00001022
Iteration 127/1000 | Loss: 0.00001022
Iteration 128/1000 | Loss: 0.00001022
Iteration 129/1000 | Loss: 0.00001022
Iteration 130/1000 | Loss: 0.00001022
Iteration 131/1000 | Loss: 0.00001022
Iteration 132/1000 | Loss: 0.00001022
Iteration 133/1000 | Loss: 0.00001022
Iteration 134/1000 | Loss: 0.00001021
Iteration 135/1000 | Loss: 0.00001021
Iteration 136/1000 | Loss: 0.00001021
Iteration 137/1000 | Loss: 0.00001021
Iteration 138/1000 | Loss: 0.00001021
Iteration 139/1000 | Loss: 0.00001021
Iteration 140/1000 | Loss: 0.00001021
Iteration 141/1000 | Loss: 0.00001021
Iteration 142/1000 | Loss: 0.00001021
Iteration 143/1000 | Loss: 0.00001021
Iteration 144/1000 | Loss: 0.00001021
Iteration 145/1000 | Loss: 0.00001021
Iteration 146/1000 | Loss: 0.00001021
Iteration 147/1000 | Loss: 0.00001021
Iteration 148/1000 | Loss: 0.00001021
Iteration 149/1000 | Loss: 0.00001021
Iteration 150/1000 | Loss: 0.00001021
Iteration 151/1000 | Loss: 0.00001021
Iteration 152/1000 | Loss: 0.00001021
Iteration 153/1000 | Loss: 0.00001021
Iteration 154/1000 | Loss: 0.00001021
Iteration 155/1000 | Loss: 0.00001021
Iteration 156/1000 | Loss: 0.00001021
Iteration 157/1000 | Loss: 0.00001021
Iteration 158/1000 | Loss: 0.00001021
Iteration 159/1000 | Loss: 0.00001021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.020816034724703e-05, 1.020816034724703e-05, 1.020816034724703e-05, 1.020816034724703e-05, 1.020816034724703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.020816034724703e-05

Optimization complete. Final v2v error: 2.729295015335083 mm

Highest mean error: 3.7546768188476562 mm for frame 112

Lowest mean error: 2.4655585289001465 mm for frame 68

Saving results

Total time: 37.183223724365234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00515893
Iteration 2/25 | Loss: 0.00086168
Iteration 3/25 | Loss: 0.00073782
Iteration 4/25 | Loss: 0.00070103
Iteration 5/25 | Loss: 0.00068746
Iteration 6/25 | Loss: 0.00068486
Iteration 7/25 | Loss: 0.00068374
Iteration 8/25 | Loss: 0.00068343
Iteration 9/25 | Loss: 0.00068343
Iteration 10/25 | Loss: 0.00068343
Iteration 11/25 | Loss: 0.00068343
Iteration 12/25 | Loss: 0.00068343
Iteration 13/25 | Loss: 0.00068343
Iteration 14/25 | Loss: 0.00068343
Iteration 15/25 | Loss: 0.00068343
Iteration 16/25 | Loss: 0.00068343
Iteration 17/25 | Loss: 0.00068343
Iteration 18/25 | Loss: 0.00068343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006834325613453984, 0.0006834325613453984, 0.0006834325613453984, 0.0006834325613453984, 0.0006834325613453984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006834325613453984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.11703205
Iteration 2/25 | Loss: 0.00026756
Iteration 3/25 | Loss: 0.00026756
Iteration 4/25 | Loss: 0.00026756
Iteration 5/25 | Loss: 0.00026756
Iteration 6/25 | Loss: 0.00026756
Iteration 7/25 | Loss: 0.00026756
Iteration 8/25 | Loss: 0.00026756
Iteration 9/25 | Loss: 0.00026756
Iteration 10/25 | Loss: 0.00026756
Iteration 11/25 | Loss: 0.00026756
Iteration 12/25 | Loss: 0.00026756
Iteration 13/25 | Loss: 0.00026756
Iteration 14/25 | Loss: 0.00026756
Iteration 15/25 | Loss: 0.00026756
Iteration 16/25 | Loss: 0.00026756
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0002675559662748128, 0.0002675559662748128, 0.0002675559662748128, 0.0002675559662748128, 0.0002675559662748128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002675559662748128

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026756
Iteration 2/1000 | Loss: 0.00004215
Iteration 3/1000 | Loss: 0.00003053
Iteration 4/1000 | Loss: 0.00002627
Iteration 5/1000 | Loss: 0.00002534
Iteration 6/1000 | Loss: 0.00002448
Iteration 7/1000 | Loss: 0.00002395
Iteration 8/1000 | Loss: 0.00002333
Iteration 9/1000 | Loss: 0.00002301
Iteration 10/1000 | Loss: 0.00002285
Iteration 11/1000 | Loss: 0.00002271
Iteration 12/1000 | Loss: 0.00002257
Iteration 13/1000 | Loss: 0.00002255
Iteration 14/1000 | Loss: 0.00002253
Iteration 15/1000 | Loss: 0.00002252
Iteration 16/1000 | Loss: 0.00002252
Iteration 17/1000 | Loss: 0.00002251
Iteration 18/1000 | Loss: 0.00002251
Iteration 19/1000 | Loss: 0.00002250
Iteration 20/1000 | Loss: 0.00002249
Iteration 21/1000 | Loss: 0.00002248
Iteration 22/1000 | Loss: 0.00002247
Iteration 23/1000 | Loss: 0.00002246
Iteration 24/1000 | Loss: 0.00002245
Iteration 25/1000 | Loss: 0.00002244
Iteration 26/1000 | Loss: 0.00002240
Iteration 27/1000 | Loss: 0.00002239
Iteration 28/1000 | Loss: 0.00002237
Iteration 29/1000 | Loss: 0.00002233
Iteration 30/1000 | Loss: 0.00002233
Iteration 31/1000 | Loss: 0.00002231
Iteration 32/1000 | Loss: 0.00002231
Iteration 33/1000 | Loss: 0.00002231
Iteration 34/1000 | Loss: 0.00002230
Iteration 35/1000 | Loss: 0.00002230
Iteration 36/1000 | Loss: 0.00002230
Iteration 37/1000 | Loss: 0.00002229
Iteration 38/1000 | Loss: 0.00002229
Iteration 39/1000 | Loss: 0.00002229
Iteration 40/1000 | Loss: 0.00002229
Iteration 41/1000 | Loss: 0.00002228
Iteration 42/1000 | Loss: 0.00002228
Iteration 43/1000 | Loss: 0.00002228
Iteration 44/1000 | Loss: 0.00002228
Iteration 45/1000 | Loss: 0.00002228
Iteration 46/1000 | Loss: 0.00002228
Iteration 47/1000 | Loss: 0.00002228
Iteration 48/1000 | Loss: 0.00002228
Iteration 49/1000 | Loss: 0.00002227
Iteration 50/1000 | Loss: 0.00002227
Iteration 51/1000 | Loss: 0.00002227
Iteration 52/1000 | Loss: 0.00002227
Iteration 53/1000 | Loss: 0.00002226
Iteration 54/1000 | Loss: 0.00002226
Iteration 55/1000 | Loss: 0.00002226
Iteration 56/1000 | Loss: 0.00002225
Iteration 57/1000 | Loss: 0.00002225
Iteration 58/1000 | Loss: 0.00002225
Iteration 59/1000 | Loss: 0.00002224
Iteration 60/1000 | Loss: 0.00002224
Iteration 61/1000 | Loss: 0.00002224
Iteration 62/1000 | Loss: 0.00002224
Iteration 63/1000 | Loss: 0.00002223
Iteration 64/1000 | Loss: 0.00002223
Iteration 65/1000 | Loss: 0.00002222
Iteration 66/1000 | Loss: 0.00002222
Iteration 67/1000 | Loss: 0.00002222
Iteration 68/1000 | Loss: 0.00002221
Iteration 69/1000 | Loss: 0.00002221
Iteration 70/1000 | Loss: 0.00002221
Iteration 71/1000 | Loss: 0.00002220
Iteration 72/1000 | Loss: 0.00002220
Iteration 73/1000 | Loss: 0.00002220
Iteration 74/1000 | Loss: 0.00002219
Iteration 75/1000 | Loss: 0.00002219
Iteration 76/1000 | Loss: 0.00002219
Iteration 77/1000 | Loss: 0.00002218
Iteration 78/1000 | Loss: 0.00002218
Iteration 79/1000 | Loss: 0.00002218
Iteration 80/1000 | Loss: 0.00002217
Iteration 81/1000 | Loss: 0.00002217
Iteration 82/1000 | Loss: 0.00002216
Iteration 83/1000 | Loss: 0.00002216
Iteration 84/1000 | Loss: 0.00002216
Iteration 85/1000 | Loss: 0.00002215
Iteration 86/1000 | Loss: 0.00002215
Iteration 87/1000 | Loss: 0.00002215
Iteration 88/1000 | Loss: 0.00002214
Iteration 89/1000 | Loss: 0.00002214
Iteration 90/1000 | Loss: 0.00002213
Iteration 91/1000 | Loss: 0.00002213
Iteration 92/1000 | Loss: 0.00002213
Iteration 93/1000 | Loss: 0.00002213
Iteration 94/1000 | Loss: 0.00002212
Iteration 95/1000 | Loss: 0.00002212
Iteration 96/1000 | Loss: 0.00002212
Iteration 97/1000 | Loss: 0.00002211
Iteration 98/1000 | Loss: 0.00002211
Iteration 99/1000 | Loss: 0.00002211
Iteration 100/1000 | Loss: 0.00002210
Iteration 101/1000 | Loss: 0.00002210
Iteration 102/1000 | Loss: 0.00002210
Iteration 103/1000 | Loss: 0.00002209
Iteration 104/1000 | Loss: 0.00002209
Iteration 105/1000 | Loss: 0.00002209
Iteration 106/1000 | Loss: 0.00002209
Iteration 107/1000 | Loss: 0.00002208
Iteration 108/1000 | Loss: 0.00002208
Iteration 109/1000 | Loss: 0.00002208
Iteration 110/1000 | Loss: 0.00002208
Iteration 111/1000 | Loss: 0.00002208
Iteration 112/1000 | Loss: 0.00002208
Iteration 113/1000 | Loss: 0.00002207
Iteration 114/1000 | Loss: 0.00002207
Iteration 115/1000 | Loss: 0.00002207
Iteration 116/1000 | Loss: 0.00002206
Iteration 117/1000 | Loss: 0.00002206
Iteration 118/1000 | Loss: 0.00002206
Iteration 119/1000 | Loss: 0.00002205
Iteration 120/1000 | Loss: 0.00002205
Iteration 121/1000 | Loss: 0.00002205
Iteration 122/1000 | Loss: 0.00002205
Iteration 123/1000 | Loss: 0.00002205
Iteration 124/1000 | Loss: 0.00002204
Iteration 125/1000 | Loss: 0.00002204
Iteration 126/1000 | Loss: 0.00002204
Iteration 127/1000 | Loss: 0.00002204
Iteration 128/1000 | Loss: 0.00002204
Iteration 129/1000 | Loss: 0.00002203
Iteration 130/1000 | Loss: 0.00002203
Iteration 131/1000 | Loss: 0.00002203
Iteration 132/1000 | Loss: 0.00002203
Iteration 133/1000 | Loss: 0.00002203
Iteration 134/1000 | Loss: 0.00002203
Iteration 135/1000 | Loss: 0.00002203
Iteration 136/1000 | Loss: 0.00002202
Iteration 137/1000 | Loss: 0.00002202
Iteration 138/1000 | Loss: 0.00002202
Iteration 139/1000 | Loss: 0.00002202
Iteration 140/1000 | Loss: 0.00002202
Iteration 141/1000 | Loss: 0.00002202
Iteration 142/1000 | Loss: 0.00002202
Iteration 143/1000 | Loss: 0.00002202
Iteration 144/1000 | Loss: 0.00002202
Iteration 145/1000 | Loss: 0.00002202
Iteration 146/1000 | Loss: 0.00002202
Iteration 147/1000 | Loss: 0.00002202
Iteration 148/1000 | Loss: 0.00002202
Iteration 149/1000 | Loss: 0.00002202
Iteration 150/1000 | Loss: 0.00002202
Iteration 151/1000 | Loss: 0.00002202
Iteration 152/1000 | Loss: 0.00002202
Iteration 153/1000 | Loss: 0.00002202
Iteration 154/1000 | Loss: 0.00002202
Iteration 155/1000 | Loss: 0.00002202
Iteration 156/1000 | Loss: 0.00002202
Iteration 157/1000 | Loss: 0.00002202
Iteration 158/1000 | Loss: 0.00002202
Iteration 159/1000 | Loss: 0.00002202
Iteration 160/1000 | Loss: 0.00002202
Iteration 161/1000 | Loss: 0.00002202
Iteration 162/1000 | Loss: 0.00002202
Iteration 163/1000 | Loss: 0.00002202
Iteration 164/1000 | Loss: 0.00002202
Iteration 165/1000 | Loss: 0.00002202
Iteration 166/1000 | Loss: 0.00002202
Iteration 167/1000 | Loss: 0.00002202
Iteration 168/1000 | Loss: 0.00002202
Iteration 169/1000 | Loss: 0.00002202
Iteration 170/1000 | Loss: 0.00002202
Iteration 171/1000 | Loss: 0.00002202
Iteration 172/1000 | Loss: 0.00002202
Iteration 173/1000 | Loss: 0.00002202
Iteration 174/1000 | Loss: 0.00002202
Iteration 175/1000 | Loss: 0.00002202
Iteration 176/1000 | Loss: 0.00002202
Iteration 177/1000 | Loss: 0.00002202
Iteration 178/1000 | Loss: 0.00002202
Iteration 179/1000 | Loss: 0.00002202
Iteration 180/1000 | Loss: 0.00002202
Iteration 181/1000 | Loss: 0.00002202
Iteration 182/1000 | Loss: 0.00002202
Iteration 183/1000 | Loss: 0.00002202
Iteration 184/1000 | Loss: 0.00002202
Iteration 185/1000 | Loss: 0.00002202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.201535608037375e-05, 2.201535608037375e-05, 2.201535608037375e-05, 2.201535608037375e-05, 2.201535608037375e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.201535608037375e-05

Optimization complete. Final v2v error: 3.920565366744995 mm

Highest mean error: 4.7995734214782715 mm for frame 68

Lowest mean error: 3.544175148010254 mm for frame 127

Saving results

Total time: 40.092949867248535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055070
Iteration 2/25 | Loss: 0.00191043
Iteration 3/25 | Loss: 0.00112672
Iteration 4/25 | Loss: 0.00097570
Iteration 5/25 | Loss: 0.00091821
Iteration 6/25 | Loss: 0.00087431
Iteration 7/25 | Loss: 0.00084408
Iteration 8/25 | Loss: 0.00083096
Iteration 9/25 | Loss: 0.00082719
Iteration 10/25 | Loss: 0.00082227
Iteration 11/25 | Loss: 0.00081976
Iteration 12/25 | Loss: 0.00081929
Iteration 13/25 | Loss: 0.00081818
Iteration 14/25 | Loss: 0.00082006
Iteration 15/25 | Loss: 0.00082089
Iteration 16/25 | Loss: 0.00081918
Iteration 17/25 | Loss: 0.00082223
Iteration 18/25 | Loss: 0.00082118
Iteration 19/25 | Loss: 0.00082288
Iteration 20/25 | Loss: 0.00081926
Iteration 21/25 | Loss: 0.00081856
Iteration 22/25 | Loss: 0.00081929
Iteration 23/25 | Loss: 0.00080518
Iteration 24/25 | Loss: 0.00080361
Iteration 25/25 | Loss: 0.00080191

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.92335892
Iteration 2/25 | Loss: 0.00055197
Iteration 3/25 | Loss: 0.00055058
Iteration 4/25 | Loss: 0.00055057
Iteration 5/25 | Loss: 0.00055057
Iteration 6/25 | Loss: 0.00055057
Iteration 7/25 | Loss: 0.00055057
Iteration 8/25 | Loss: 0.00055057
Iteration 9/25 | Loss: 0.00055057
Iteration 10/25 | Loss: 0.00055057
Iteration 11/25 | Loss: 0.00055057
Iteration 12/25 | Loss: 0.00055057
Iteration 13/25 | Loss: 0.00055057
Iteration 14/25 | Loss: 0.00055057
Iteration 15/25 | Loss: 0.00055057
Iteration 16/25 | Loss: 0.00055057
Iteration 17/25 | Loss: 0.00055057
Iteration 18/25 | Loss: 0.00055057
Iteration 19/25 | Loss: 0.00055057
Iteration 20/25 | Loss: 0.00055057
Iteration 21/25 | Loss: 0.00055057
Iteration 22/25 | Loss: 0.00055057
Iteration 23/25 | Loss: 0.00055057
Iteration 24/25 | Loss: 0.00055057
Iteration 25/25 | Loss: 0.00055057

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055057
Iteration 2/1000 | Loss: 0.00036448
Iteration 3/1000 | Loss: 0.00029573
Iteration 4/1000 | Loss: 0.00043176
Iteration 5/1000 | Loss: 0.00038177
Iteration 6/1000 | Loss: 0.00035869
Iteration 7/1000 | Loss: 0.00043246
Iteration 8/1000 | Loss: 0.00042082
Iteration 9/1000 | Loss: 0.00023219
Iteration 10/1000 | Loss: 0.00022259
Iteration 11/1000 | Loss: 0.00031981
Iteration 12/1000 | Loss: 0.00039070
Iteration 13/1000 | Loss: 0.00026426
Iteration 14/1000 | Loss: 0.00036089
Iteration 15/1000 | Loss: 0.00082820
Iteration 16/1000 | Loss: 0.00030128
Iteration 17/1000 | Loss: 0.00047411
Iteration 18/1000 | Loss: 0.00062965
Iteration 19/1000 | Loss: 0.00056811
Iteration 20/1000 | Loss: 0.00004271
Iteration 21/1000 | Loss: 0.00003939
Iteration 22/1000 | Loss: 0.00003781
Iteration 23/1000 | Loss: 0.00003672
Iteration 24/1000 | Loss: 0.00003455
Iteration 25/1000 | Loss: 0.00003332
Iteration 26/1000 | Loss: 0.00003217
Iteration 27/1000 | Loss: 0.00003158
Iteration 28/1000 | Loss: 0.00003121
Iteration 29/1000 | Loss: 0.00003086
Iteration 30/1000 | Loss: 0.00003051
Iteration 31/1000 | Loss: 0.00003025
Iteration 32/1000 | Loss: 0.00004409
Iteration 33/1000 | Loss: 0.00003357
Iteration 34/1000 | Loss: 0.00003134
Iteration 35/1000 | Loss: 0.00003043
Iteration 36/1000 | Loss: 0.00002988
Iteration 37/1000 | Loss: 0.00002950
Iteration 38/1000 | Loss: 0.00002941
Iteration 39/1000 | Loss: 0.00002924
Iteration 40/1000 | Loss: 0.00002923
Iteration 41/1000 | Loss: 0.00002923
Iteration 42/1000 | Loss: 0.00002911
Iteration 43/1000 | Loss: 0.00002908
Iteration 44/1000 | Loss: 0.00002907
Iteration 45/1000 | Loss: 0.00002898
Iteration 46/1000 | Loss: 0.00002897
Iteration 47/1000 | Loss: 0.00002897
Iteration 48/1000 | Loss: 0.00002896
Iteration 49/1000 | Loss: 0.00002896
Iteration 50/1000 | Loss: 0.00002896
Iteration 51/1000 | Loss: 0.00002895
Iteration 52/1000 | Loss: 0.00002895
Iteration 53/1000 | Loss: 0.00002894
Iteration 54/1000 | Loss: 0.00002894
Iteration 55/1000 | Loss: 0.00002893
Iteration 56/1000 | Loss: 0.00002893
Iteration 57/1000 | Loss: 0.00002892
Iteration 58/1000 | Loss: 0.00002892
Iteration 59/1000 | Loss: 0.00002892
Iteration 60/1000 | Loss: 0.00002891
Iteration 61/1000 | Loss: 0.00002891
Iteration 62/1000 | Loss: 0.00002891
Iteration 63/1000 | Loss: 0.00002891
Iteration 64/1000 | Loss: 0.00002890
Iteration 65/1000 | Loss: 0.00002890
Iteration 66/1000 | Loss: 0.00002890
Iteration 67/1000 | Loss: 0.00002890
Iteration 68/1000 | Loss: 0.00002890
Iteration 69/1000 | Loss: 0.00002889
Iteration 70/1000 | Loss: 0.00002889
Iteration 71/1000 | Loss: 0.00002889
Iteration 72/1000 | Loss: 0.00002889
Iteration 73/1000 | Loss: 0.00002889
Iteration 74/1000 | Loss: 0.00002889
Iteration 75/1000 | Loss: 0.00002888
Iteration 76/1000 | Loss: 0.00002888
Iteration 77/1000 | Loss: 0.00002887
Iteration 78/1000 | Loss: 0.00002887
Iteration 79/1000 | Loss: 0.00002886
Iteration 80/1000 | Loss: 0.00002886
Iteration 81/1000 | Loss: 0.00002886
Iteration 82/1000 | Loss: 0.00002885
Iteration 83/1000 | Loss: 0.00002885
Iteration 84/1000 | Loss: 0.00002885
Iteration 85/1000 | Loss: 0.00002884
Iteration 86/1000 | Loss: 0.00002884
Iteration 87/1000 | Loss: 0.00002884
Iteration 88/1000 | Loss: 0.00002883
Iteration 89/1000 | Loss: 0.00002883
Iteration 90/1000 | Loss: 0.00002883
Iteration 91/1000 | Loss: 0.00002883
Iteration 92/1000 | Loss: 0.00002883
Iteration 93/1000 | Loss: 0.00002882
Iteration 94/1000 | Loss: 0.00002882
Iteration 95/1000 | Loss: 0.00002882
Iteration 96/1000 | Loss: 0.00002882
Iteration 97/1000 | Loss: 0.00002882
Iteration 98/1000 | Loss: 0.00002882
Iteration 99/1000 | Loss: 0.00002882
Iteration 100/1000 | Loss: 0.00002882
Iteration 101/1000 | Loss: 0.00002882
Iteration 102/1000 | Loss: 0.00002882
Iteration 103/1000 | Loss: 0.00002882
Iteration 104/1000 | Loss: 0.00002882
Iteration 105/1000 | Loss: 0.00002881
Iteration 106/1000 | Loss: 0.00002881
Iteration 107/1000 | Loss: 0.00002881
Iteration 108/1000 | Loss: 0.00002881
Iteration 109/1000 | Loss: 0.00002881
Iteration 110/1000 | Loss: 0.00002881
Iteration 111/1000 | Loss: 0.00002881
Iteration 112/1000 | Loss: 0.00002881
Iteration 113/1000 | Loss: 0.00002881
Iteration 114/1000 | Loss: 0.00002881
Iteration 115/1000 | Loss: 0.00002881
Iteration 116/1000 | Loss: 0.00002881
Iteration 117/1000 | Loss: 0.00002881
Iteration 118/1000 | Loss: 0.00002881
Iteration 119/1000 | Loss: 0.00002881
Iteration 120/1000 | Loss: 0.00002881
Iteration 121/1000 | Loss: 0.00002881
Iteration 122/1000 | Loss: 0.00002881
Iteration 123/1000 | Loss: 0.00002881
Iteration 124/1000 | Loss: 0.00002881
Iteration 125/1000 | Loss: 0.00002881
Iteration 126/1000 | Loss: 0.00002881
Iteration 127/1000 | Loss: 0.00002881
Iteration 128/1000 | Loss: 0.00002881
Iteration 129/1000 | Loss: 0.00002881
Iteration 130/1000 | Loss: 0.00002881
Iteration 131/1000 | Loss: 0.00002881
Iteration 132/1000 | Loss: 0.00002881
Iteration 133/1000 | Loss: 0.00002881
Iteration 134/1000 | Loss: 0.00002881
Iteration 135/1000 | Loss: 0.00002881
Iteration 136/1000 | Loss: 0.00002881
Iteration 137/1000 | Loss: 0.00002881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.8806769478251226e-05, 2.8806769478251226e-05, 2.8806769478251226e-05, 2.8806769478251226e-05, 2.8806769478251226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8806769478251226e-05

Optimization complete. Final v2v error: 4.408917427062988 mm

Highest mean error: 6.554773807525635 mm for frame 122

Lowest mean error: 3.211146593093872 mm for frame 98

Saving results

Total time: 110.99838161468506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011722
Iteration 2/25 | Loss: 0.00144780
Iteration 3/25 | Loss: 0.00093848
Iteration 4/25 | Loss: 0.00085935
Iteration 5/25 | Loss: 0.00083923
Iteration 6/25 | Loss: 0.00083487
Iteration 7/25 | Loss: 0.00083456
Iteration 8/25 | Loss: 0.00083456
Iteration 9/25 | Loss: 0.00083456
Iteration 10/25 | Loss: 0.00083456
Iteration 11/25 | Loss: 0.00083456
Iteration 12/25 | Loss: 0.00083456
Iteration 13/25 | Loss: 0.00083456
Iteration 14/25 | Loss: 0.00083456
Iteration 15/25 | Loss: 0.00083456
Iteration 16/25 | Loss: 0.00083456
Iteration 17/25 | Loss: 0.00083456
Iteration 18/25 | Loss: 0.00083456
Iteration 19/25 | Loss: 0.00083456
Iteration 20/25 | Loss: 0.00083456
Iteration 21/25 | Loss: 0.00083456
Iteration 22/25 | Loss: 0.00083456
Iteration 23/25 | Loss: 0.00083456
Iteration 24/25 | Loss: 0.00083456
Iteration 25/25 | Loss: 0.00083456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11462116
Iteration 2/25 | Loss: 0.00031321
Iteration 3/25 | Loss: 0.00031313
Iteration 4/25 | Loss: 0.00031313
Iteration 5/25 | Loss: 0.00031313
Iteration 6/25 | Loss: 0.00031313
Iteration 7/25 | Loss: 0.00031313
Iteration 8/25 | Loss: 0.00031313
Iteration 9/25 | Loss: 0.00031313
Iteration 10/25 | Loss: 0.00031313
Iteration 11/25 | Loss: 0.00031313
Iteration 12/25 | Loss: 0.00031313
Iteration 13/25 | Loss: 0.00031313
Iteration 14/25 | Loss: 0.00031313
Iteration 15/25 | Loss: 0.00031313
Iteration 16/25 | Loss: 0.00031313
Iteration 17/25 | Loss: 0.00031313
Iteration 18/25 | Loss: 0.00031313
Iteration 19/25 | Loss: 0.00031313
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0003131318953819573, 0.0003131318953819573, 0.0003131318953819573, 0.0003131318953819573, 0.0003131318953819573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003131318953819573

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031313
Iteration 2/1000 | Loss: 0.00005388
Iteration 3/1000 | Loss: 0.00004015
Iteration 4/1000 | Loss: 0.00003646
Iteration 5/1000 | Loss: 0.00003467
Iteration 6/1000 | Loss: 0.00003368
Iteration 7/1000 | Loss: 0.00003273
Iteration 8/1000 | Loss: 0.00003181
Iteration 9/1000 | Loss: 0.00003125
Iteration 10/1000 | Loss: 0.00003082
Iteration 11/1000 | Loss: 0.00003051
Iteration 12/1000 | Loss: 0.00003024
Iteration 13/1000 | Loss: 0.00003004
Iteration 14/1000 | Loss: 0.00002996
Iteration 15/1000 | Loss: 0.00002995
Iteration 16/1000 | Loss: 0.00002985
Iteration 17/1000 | Loss: 0.00002983
Iteration 18/1000 | Loss: 0.00002978
Iteration 19/1000 | Loss: 0.00002976
Iteration 20/1000 | Loss: 0.00002975
Iteration 21/1000 | Loss: 0.00002975
Iteration 22/1000 | Loss: 0.00002974
Iteration 23/1000 | Loss: 0.00002973
Iteration 24/1000 | Loss: 0.00002973
Iteration 25/1000 | Loss: 0.00002972
Iteration 26/1000 | Loss: 0.00002972
Iteration 27/1000 | Loss: 0.00002972
Iteration 28/1000 | Loss: 0.00002971
Iteration 29/1000 | Loss: 0.00002971
Iteration 30/1000 | Loss: 0.00002970
Iteration 31/1000 | Loss: 0.00002970
Iteration 32/1000 | Loss: 0.00002970
Iteration 33/1000 | Loss: 0.00002969
Iteration 34/1000 | Loss: 0.00002968
Iteration 35/1000 | Loss: 0.00002968
Iteration 36/1000 | Loss: 0.00002967
Iteration 37/1000 | Loss: 0.00002967
Iteration 38/1000 | Loss: 0.00002967
Iteration 39/1000 | Loss: 0.00002967
Iteration 40/1000 | Loss: 0.00002967
Iteration 41/1000 | Loss: 0.00002967
Iteration 42/1000 | Loss: 0.00002967
Iteration 43/1000 | Loss: 0.00002966
Iteration 44/1000 | Loss: 0.00002966
Iteration 45/1000 | Loss: 0.00002966
Iteration 46/1000 | Loss: 0.00002965
Iteration 47/1000 | Loss: 0.00002965
Iteration 48/1000 | Loss: 0.00002964
Iteration 49/1000 | Loss: 0.00002964
Iteration 50/1000 | Loss: 0.00002964
Iteration 51/1000 | Loss: 0.00002964
Iteration 52/1000 | Loss: 0.00002964
Iteration 53/1000 | Loss: 0.00002963
Iteration 54/1000 | Loss: 0.00002963
Iteration 55/1000 | Loss: 0.00002963
Iteration 56/1000 | Loss: 0.00002962
Iteration 57/1000 | Loss: 0.00002962
Iteration 58/1000 | Loss: 0.00002962
Iteration 59/1000 | Loss: 0.00002961
Iteration 60/1000 | Loss: 0.00002961
Iteration 61/1000 | Loss: 0.00002961
Iteration 62/1000 | Loss: 0.00002960
Iteration 63/1000 | Loss: 0.00002960
Iteration 64/1000 | Loss: 0.00002960
Iteration 65/1000 | Loss: 0.00002959
Iteration 66/1000 | Loss: 0.00002959
Iteration 67/1000 | Loss: 0.00002959
Iteration 68/1000 | Loss: 0.00002958
Iteration 69/1000 | Loss: 0.00002958
Iteration 70/1000 | Loss: 0.00002958
Iteration 71/1000 | Loss: 0.00002957
Iteration 72/1000 | Loss: 0.00002957
Iteration 73/1000 | Loss: 0.00002957
Iteration 74/1000 | Loss: 0.00002957
Iteration 75/1000 | Loss: 0.00002956
Iteration 76/1000 | Loss: 0.00002956
Iteration 77/1000 | Loss: 0.00002956
Iteration 78/1000 | Loss: 0.00002956
Iteration 79/1000 | Loss: 0.00002956
Iteration 80/1000 | Loss: 0.00002955
Iteration 81/1000 | Loss: 0.00002955
Iteration 82/1000 | Loss: 0.00002955
Iteration 83/1000 | Loss: 0.00002955
Iteration 84/1000 | Loss: 0.00002955
Iteration 85/1000 | Loss: 0.00002955
Iteration 86/1000 | Loss: 0.00002955
Iteration 87/1000 | Loss: 0.00002955
Iteration 88/1000 | Loss: 0.00002955
Iteration 89/1000 | Loss: 0.00002954
Iteration 90/1000 | Loss: 0.00002954
Iteration 91/1000 | Loss: 0.00002954
Iteration 92/1000 | Loss: 0.00002954
Iteration 93/1000 | Loss: 0.00002954
Iteration 94/1000 | Loss: 0.00002954
Iteration 95/1000 | Loss: 0.00002954
Iteration 96/1000 | Loss: 0.00002954
Iteration 97/1000 | Loss: 0.00002953
Iteration 98/1000 | Loss: 0.00002953
Iteration 99/1000 | Loss: 0.00002953
Iteration 100/1000 | Loss: 0.00002953
Iteration 101/1000 | Loss: 0.00002953
Iteration 102/1000 | Loss: 0.00002953
Iteration 103/1000 | Loss: 0.00002953
Iteration 104/1000 | Loss: 0.00002953
Iteration 105/1000 | Loss: 0.00002953
Iteration 106/1000 | Loss: 0.00002953
Iteration 107/1000 | Loss: 0.00002953
Iteration 108/1000 | Loss: 0.00002953
Iteration 109/1000 | Loss: 0.00002953
Iteration 110/1000 | Loss: 0.00002953
Iteration 111/1000 | Loss: 0.00002953
Iteration 112/1000 | Loss: 0.00002953
Iteration 113/1000 | Loss: 0.00002953
Iteration 114/1000 | Loss: 0.00002953
Iteration 115/1000 | Loss: 0.00002953
Iteration 116/1000 | Loss: 0.00002953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.953119110316038e-05, 2.953119110316038e-05, 2.953119110316038e-05, 2.953119110316038e-05, 2.953119110316038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.953119110316038e-05

Optimization complete. Final v2v error: 4.398988723754883 mm

Highest mean error: 5.308735370635986 mm for frame 80

Lowest mean error: 3.327232599258423 mm for frame 215

Saving results

Total time: 42.184962034225464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00621756
Iteration 2/25 | Loss: 0.00097270
Iteration 3/25 | Loss: 0.00081764
Iteration 4/25 | Loss: 0.00078131
Iteration 5/25 | Loss: 0.00077682
Iteration 6/25 | Loss: 0.00077522
Iteration 7/25 | Loss: 0.00077486
Iteration 8/25 | Loss: 0.00077486
Iteration 9/25 | Loss: 0.00077486
Iteration 10/25 | Loss: 0.00077486
Iteration 11/25 | Loss: 0.00077486
Iteration 12/25 | Loss: 0.00077486
Iteration 13/25 | Loss: 0.00077486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007748588104732335, 0.0007748588104732335, 0.0007748588104732335, 0.0007748588104732335, 0.0007748588104732335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007748588104732335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.64999747
Iteration 2/25 | Loss: 0.00045846
Iteration 3/25 | Loss: 0.00045845
Iteration 4/25 | Loss: 0.00045845
Iteration 5/25 | Loss: 0.00045845
Iteration 6/25 | Loss: 0.00045845
Iteration 7/25 | Loss: 0.00045845
Iteration 8/25 | Loss: 0.00045845
Iteration 9/25 | Loss: 0.00045845
Iteration 10/25 | Loss: 0.00045845
Iteration 11/25 | Loss: 0.00045845
Iteration 12/25 | Loss: 0.00045845
Iteration 13/25 | Loss: 0.00045845
Iteration 14/25 | Loss: 0.00045845
Iteration 15/25 | Loss: 0.00045845
Iteration 16/25 | Loss: 0.00045845
Iteration 17/25 | Loss: 0.00045845
Iteration 18/25 | Loss: 0.00045845
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004584500566124916, 0.0004584500566124916, 0.0004584500566124916, 0.0004584500566124916, 0.0004584500566124916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004584500566124916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045845
Iteration 2/1000 | Loss: 0.00006068
Iteration 3/1000 | Loss: 0.00003488
Iteration 4/1000 | Loss: 0.00003105
Iteration 5/1000 | Loss: 0.00002958
Iteration 6/1000 | Loss: 0.00002847
Iteration 7/1000 | Loss: 0.00002803
Iteration 8/1000 | Loss: 0.00002771
Iteration 9/1000 | Loss: 0.00002763
Iteration 10/1000 | Loss: 0.00002739
Iteration 11/1000 | Loss: 0.00002726
Iteration 12/1000 | Loss: 0.00002719
Iteration 13/1000 | Loss: 0.00002714
Iteration 14/1000 | Loss: 0.00002712
Iteration 15/1000 | Loss: 0.00002711
Iteration 16/1000 | Loss: 0.00002710
Iteration 17/1000 | Loss: 0.00002710
Iteration 18/1000 | Loss: 0.00002709
Iteration 19/1000 | Loss: 0.00002709
Iteration 20/1000 | Loss: 0.00002703
Iteration 21/1000 | Loss: 0.00002703
Iteration 22/1000 | Loss: 0.00002703
Iteration 23/1000 | Loss: 0.00002703
Iteration 24/1000 | Loss: 0.00002703
Iteration 25/1000 | Loss: 0.00002703
Iteration 26/1000 | Loss: 0.00002703
Iteration 27/1000 | Loss: 0.00002703
Iteration 28/1000 | Loss: 0.00002702
Iteration 29/1000 | Loss: 0.00002702
Iteration 30/1000 | Loss: 0.00002702
Iteration 31/1000 | Loss: 0.00002702
Iteration 32/1000 | Loss: 0.00002702
Iteration 33/1000 | Loss: 0.00002702
Iteration 34/1000 | Loss: 0.00002699
Iteration 35/1000 | Loss: 0.00002699
Iteration 36/1000 | Loss: 0.00002699
Iteration 37/1000 | Loss: 0.00002699
Iteration 38/1000 | Loss: 0.00002699
Iteration 39/1000 | Loss: 0.00002699
Iteration 40/1000 | Loss: 0.00002699
Iteration 41/1000 | Loss: 0.00002699
Iteration 42/1000 | Loss: 0.00002699
Iteration 43/1000 | Loss: 0.00002699
Iteration 44/1000 | Loss: 0.00002699
Iteration 45/1000 | Loss: 0.00002698
Iteration 46/1000 | Loss: 0.00002698
Iteration 47/1000 | Loss: 0.00002698
Iteration 48/1000 | Loss: 0.00002697
Iteration 49/1000 | Loss: 0.00002697
Iteration 50/1000 | Loss: 0.00002697
Iteration 51/1000 | Loss: 0.00002697
Iteration 52/1000 | Loss: 0.00002696
Iteration 53/1000 | Loss: 0.00002696
Iteration 54/1000 | Loss: 0.00002696
Iteration 55/1000 | Loss: 0.00002696
Iteration 56/1000 | Loss: 0.00002696
Iteration 57/1000 | Loss: 0.00002696
Iteration 58/1000 | Loss: 0.00002696
Iteration 59/1000 | Loss: 0.00002696
Iteration 60/1000 | Loss: 0.00002696
Iteration 61/1000 | Loss: 0.00002695
Iteration 62/1000 | Loss: 0.00002695
Iteration 63/1000 | Loss: 0.00002695
Iteration 64/1000 | Loss: 0.00002695
Iteration 65/1000 | Loss: 0.00002694
Iteration 66/1000 | Loss: 0.00002694
Iteration 67/1000 | Loss: 0.00002694
Iteration 68/1000 | Loss: 0.00002693
Iteration 69/1000 | Loss: 0.00002693
Iteration 70/1000 | Loss: 0.00002693
Iteration 71/1000 | Loss: 0.00002693
Iteration 72/1000 | Loss: 0.00002692
Iteration 73/1000 | Loss: 0.00002692
Iteration 74/1000 | Loss: 0.00002692
Iteration 75/1000 | Loss: 0.00002692
Iteration 76/1000 | Loss: 0.00002691
Iteration 77/1000 | Loss: 0.00002691
Iteration 78/1000 | Loss: 0.00002691
Iteration 79/1000 | Loss: 0.00002691
Iteration 80/1000 | Loss: 0.00002690
Iteration 81/1000 | Loss: 0.00002690
Iteration 82/1000 | Loss: 0.00002690
Iteration 83/1000 | Loss: 0.00002688
Iteration 84/1000 | Loss: 0.00002688
Iteration 85/1000 | Loss: 0.00002687
Iteration 86/1000 | Loss: 0.00002687
Iteration 87/1000 | Loss: 0.00002687
Iteration 88/1000 | Loss: 0.00002686
Iteration 89/1000 | Loss: 0.00002686
Iteration 90/1000 | Loss: 0.00002686
Iteration 91/1000 | Loss: 0.00002685
Iteration 92/1000 | Loss: 0.00002685
Iteration 93/1000 | Loss: 0.00002685
Iteration 94/1000 | Loss: 0.00002685
Iteration 95/1000 | Loss: 0.00002685
Iteration 96/1000 | Loss: 0.00002685
Iteration 97/1000 | Loss: 0.00002685
Iteration 98/1000 | Loss: 0.00002685
Iteration 99/1000 | Loss: 0.00002684
Iteration 100/1000 | Loss: 0.00002684
Iteration 101/1000 | Loss: 0.00002684
Iteration 102/1000 | Loss: 0.00002684
Iteration 103/1000 | Loss: 0.00002684
Iteration 104/1000 | Loss: 0.00002684
Iteration 105/1000 | Loss: 0.00002684
Iteration 106/1000 | Loss: 0.00002684
Iteration 107/1000 | Loss: 0.00002684
Iteration 108/1000 | Loss: 0.00002684
Iteration 109/1000 | Loss: 0.00002684
Iteration 110/1000 | Loss: 0.00002684
Iteration 111/1000 | Loss: 0.00002684
Iteration 112/1000 | Loss: 0.00002684
Iteration 113/1000 | Loss: 0.00002683
Iteration 114/1000 | Loss: 0.00002683
Iteration 115/1000 | Loss: 0.00002683
Iteration 116/1000 | Loss: 0.00002682
Iteration 117/1000 | Loss: 0.00002682
Iteration 118/1000 | Loss: 0.00002682
Iteration 119/1000 | Loss: 0.00002682
Iteration 120/1000 | Loss: 0.00002682
Iteration 121/1000 | Loss: 0.00002682
Iteration 122/1000 | Loss: 0.00002682
Iteration 123/1000 | Loss: 0.00002682
Iteration 124/1000 | Loss: 0.00002682
Iteration 125/1000 | Loss: 0.00002681
Iteration 126/1000 | Loss: 0.00002681
Iteration 127/1000 | Loss: 0.00002681
Iteration 128/1000 | Loss: 0.00002681
Iteration 129/1000 | Loss: 0.00002681
Iteration 130/1000 | Loss: 0.00002681
Iteration 131/1000 | Loss: 0.00002681
Iteration 132/1000 | Loss: 0.00002681
Iteration 133/1000 | Loss: 0.00002681
Iteration 134/1000 | Loss: 0.00002681
Iteration 135/1000 | Loss: 0.00002680
Iteration 136/1000 | Loss: 0.00002680
Iteration 137/1000 | Loss: 0.00002680
Iteration 138/1000 | Loss: 0.00002680
Iteration 139/1000 | Loss: 0.00002680
Iteration 140/1000 | Loss: 0.00002680
Iteration 141/1000 | Loss: 0.00002679
Iteration 142/1000 | Loss: 0.00002679
Iteration 143/1000 | Loss: 0.00002679
Iteration 144/1000 | Loss: 0.00002679
Iteration 145/1000 | Loss: 0.00002679
Iteration 146/1000 | Loss: 0.00002679
Iteration 147/1000 | Loss: 0.00002679
Iteration 148/1000 | Loss: 0.00002679
Iteration 149/1000 | Loss: 0.00002679
Iteration 150/1000 | Loss: 0.00002679
Iteration 151/1000 | Loss: 0.00002679
Iteration 152/1000 | Loss: 0.00002679
Iteration 153/1000 | Loss: 0.00002679
Iteration 154/1000 | Loss: 0.00002679
Iteration 155/1000 | Loss: 0.00002679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.6788475224748254e-05, 2.6788475224748254e-05, 2.6788475224748254e-05, 2.6788475224748254e-05, 2.6788475224748254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6788475224748254e-05

Optimization complete. Final v2v error: 4.365732192993164 mm

Highest mean error: 4.534066677093506 mm for frame 65

Lowest mean error: 4.2656378746032715 mm for frame 116

Saving results

Total time: 35.001582860946655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033382
Iteration 2/25 | Loss: 0.00218525
Iteration 3/25 | Loss: 0.00136048
Iteration 4/25 | Loss: 0.00124064
Iteration 5/25 | Loss: 0.00112609
Iteration 6/25 | Loss: 0.00118918
Iteration 7/25 | Loss: 0.00096817
Iteration 8/25 | Loss: 0.00088621
Iteration 9/25 | Loss: 0.00080141
Iteration 10/25 | Loss: 0.00075853
Iteration 11/25 | Loss: 0.00072267
Iteration 12/25 | Loss: 0.00071302
Iteration 13/25 | Loss: 0.00070108
Iteration 14/25 | Loss: 0.00069294
Iteration 15/25 | Loss: 0.00069319
Iteration 16/25 | Loss: 0.00068709
Iteration 17/25 | Loss: 0.00068469
Iteration 18/25 | Loss: 0.00068048
Iteration 19/25 | Loss: 0.00067945
Iteration 20/25 | Loss: 0.00068244
Iteration 21/25 | Loss: 0.00067856
Iteration 22/25 | Loss: 0.00067166
Iteration 23/25 | Loss: 0.00067018
Iteration 24/25 | Loss: 0.00066801
Iteration 25/25 | Loss: 0.00066667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51031566
Iteration 2/25 | Loss: 0.00039107
Iteration 3/25 | Loss: 0.00039107
Iteration 4/25 | Loss: 0.00039106
Iteration 5/25 | Loss: 0.00039106
Iteration 6/25 | Loss: 0.00039106
Iteration 7/25 | Loss: 0.00039106
Iteration 8/25 | Loss: 0.00039106
Iteration 9/25 | Loss: 0.00039106
Iteration 10/25 | Loss: 0.00039106
Iteration 11/25 | Loss: 0.00039106
Iteration 12/25 | Loss: 0.00039106
Iteration 13/25 | Loss: 0.00039106
Iteration 14/25 | Loss: 0.00039106
Iteration 15/25 | Loss: 0.00039106
Iteration 16/25 | Loss: 0.00039106
Iteration 17/25 | Loss: 0.00039106
Iteration 18/25 | Loss: 0.00039106
Iteration 19/25 | Loss: 0.00039106
Iteration 20/25 | Loss: 0.00039106
Iteration 21/25 | Loss: 0.00039106
Iteration 22/25 | Loss: 0.00039106
Iteration 23/25 | Loss: 0.00039106
Iteration 24/25 | Loss: 0.00039106
Iteration 25/25 | Loss: 0.00039106

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039106
Iteration 2/1000 | Loss: 0.00005903
Iteration 3/1000 | Loss: 0.00005554
Iteration 4/1000 | Loss: 0.00003902
Iteration 5/1000 | Loss: 0.00003104
Iteration 6/1000 | Loss: 0.00002510
Iteration 7/1000 | Loss: 0.00002082
Iteration 8/1000 | Loss: 0.00002866
Iteration 9/1000 | Loss: 0.00002993
Iteration 10/1000 | Loss: 0.00003436
Iteration 11/1000 | Loss: 0.00003958
Iteration 12/1000 | Loss: 0.00003714
Iteration 13/1000 | Loss: 0.00011527
Iteration 14/1000 | Loss: 0.00003123
Iteration 15/1000 | Loss: 0.00002152
Iteration 16/1000 | Loss: 0.00001920
Iteration 17/1000 | Loss: 0.00001757
Iteration 18/1000 | Loss: 0.00004349
Iteration 19/1000 | Loss: 0.00001611
Iteration 20/1000 | Loss: 0.00002884
Iteration 21/1000 | Loss: 0.00001534
Iteration 22/1000 | Loss: 0.00001509
Iteration 23/1000 | Loss: 0.00001484
Iteration 24/1000 | Loss: 0.00001472
Iteration 25/1000 | Loss: 0.00001450
Iteration 26/1000 | Loss: 0.00001449
Iteration 27/1000 | Loss: 0.00001448
Iteration 28/1000 | Loss: 0.00001443
Iteration 29/1000 | Loss: 0.00001436
Iteration 30/1000 | Loss: 0.00001432
Iteration 31/1000 | Loss: 0.00001431
Iteration 32/1000 | Loss: 0.00001431
Iteration 33/1000 | Loss: 0.00001430
Iteration 34/1000 | Loss: 0.00001430
Iteration 35/1000 | Loss: 0.00001430
Iteration 36/1000 | Loss: 0.00001429
Iteration 37/1000 | Loss: 0.00001429
Iteration 38/1000 | Loss: 0.00001429
Iteration 39/1000 | Loss: 0.00001429
Iteration 40/1000 | Loss: 0.00001429
Iteration 41/1000 | Loss: 0.00001428
Iteration 42/1000 | Loss: 0.00001428
Iteration 43/1000 | Loss: 0.00001428
Iteration 44/1000 | Loss: 0.00001428
Iteration 45/1000 | Loss: 0.00001428
Iteration 46/1000 | Loss: 0.00001427
Iteration 47/1000 | Loss: 0.00004930
Iteration 48/1000 | Loss: 0.00001435
Iteration 49/1000 | Loss: 0.00001425
Iteration 50/1000 | Loss: 0.00001425
Iteration 51/1000 | Loss: 0.00001424
Iteration 52/1000 | Loss: 0.00001424
Iteration 53/1000 | Loss: 0.00001424
Iteration 54/1000 | Loss: 0.00001424
Iteration 55/1000 | Loss: 0.00001424
Iteration 56/1000 | Loss: 0.00001424
Iteration 57/1000 | Loss: 0.00001424
Iteration 58/1000 | Loss: 0.00001424
Iteration 59/1000 | Loss: 0.00001424
Iteration 60/1000 | Loss: 0.00001423
Iteration 61/1000 | Loss: 0.00001423
Iteration 62/1000 | Loss: 0.00001422
Iteration 63/1000 | Loss: 0.00001422
Iteration 64/1000 | Loss: 0.00001421
Iteration 65/1000 | Loss: 0.00001421
Iteration 66/1000 | Loss: 0.00001420
Iteration 67/1000 | Loss: 0.00001420
Iteration 68/1000 | Loss: 0.00001419
Iteration 69/1000 | Loss: 0.00001419
Iteration 70/1000 | Loss: 0.00001419
Iteration 71/1000 | Loss: 0.00001419
Iteration 72/1000 | Loss: 0.00001418
Iteration 73/1000 | Loss: 0.00001418
Iteration 74/1000 | Loss: 0.00001418
Iteration 75/1000 | Loss: 0.00001418
Iteration 76/1000 | Loss: 0.00001418
Iteration 77/1000 | Loss: 0.00001418
Iteration 78/1000 | Loss: 0.00001418
Iteration 79/1000 | Loss: 0.00001418
Iteration 80/1000 | Loss: 0.00001418
Iteration 81/1000 | Loss: 0.00001418
Iteration 82/1000 | Loss: 0.00001418
Iteration 83/1000 | Loss: 0.00001418
Iteration 84/1000 | Loss: 0.00001418
Iteration 85/1000 | Loss: 0.00001418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.418284864485031e-05, 1.418284864485031e-05, 1.418284864485031e-05, 1.418284864485031e-05, 1.418284864485031e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.418284864485031e-05

Optimization complete. Final v2v error: 3.0993258953094482 mm

Highest mean error: 5.072793006896973 mm for frame 37

Lowest mean error: 2.5556201934814453 mm for frame 147

Saving results

Total time: 88.46175646781921
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957058
Iteration 2/25 | Loss: 0.00130290
Iteration 3/25 | Loss: 0.00085019
Iteration 4/25 | Loss: 0.00076634
Iteration 5/25 | Loss: 0.00070058
Iteration 6/25 | Loss: 0.00070738
Iteration 7/25 | Loss: 0.00069263
Iteration 8/25 | Loss: 0.00069750
Iteration 9/25 | Loss: 0.00069223
Iteration 10/25 | Loss: 0.00070046
Iteration 11/25 | Loss: 0.00069504
Iteration 12/25 | Loss: 0.00069375
Iteration 13/25 | Loss: 0.00069001
Iteration 14/25 | Loss: 0.00068975
Iteration 15/25 | Loss: 0.00068986
Iteration 16/25 | Loss: 0.00068721
Iteration 17/25 | Loss: 0.00068721
Iteration 18/25 | Loss: 0.00068721
Iteration 19/25 | Loss: 0.00068721
Iteration 20/25 | Loss: 0.00068721
Iteration 21/25 | Loss: 0.00068720
Iteration 22/25 | Loss: 0.00068720
Iteration 23/25 | Loss: 0.00068720
Iteration 24/25 | Loss: 0.00068720
Iteration 25/25 | Loss: 0.00068720

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57260239
Iteration 2/25 | Loss: 0.00040706
Iteration 3/25 | Loss: 0.00029797
Iteration 4/25 | Loss: 0.00029797
Iteration 5/25 | Loss: 0.00029797
Iteration 6/25 | Loss: 0.00029797
Iteration 7/25 | Loss: 0.00029797
Iteration 8/25 | Loss: 0.00029797
Iteration 9/25 | Loss: 0.00029797
Iteration 10/25 | Loss: 0.00029797
Iteration 11/25 | Loss: 0.00029797
Iteration 12/25 | Loss: 0.00029797
Iteration 13/25 | Loss: 0.00029797
Iteration 14/25 | Loss: 0.00029797
Iteration 15/25 | Loss: 0.00029797
Iteration 16/25 | Loss: 0.00029797
Iteration 17/25 | Loss: 0.00029797
Iteration 18/25 | Loss: 0.00029797
Iteration 19/25 | Loss: 0.00029797
Iteration 20/25 | Loss: 0.00029797
Iteration 21/25 | Loss: 0.00029797
Iteration 22/25 | Loss: 0.00029797
Iteration 23/25 | Loss: 0.00029797
Iteration 24/25 | Loss: 0.00029797
Iteration 25/25 | Loss: 0.00029797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029797
Iteration 2/1000 | Loss: 0.00003742
Iteration 3/1000 | Loss: 0.00002741
Iteration 4/1000 | Loss: 0.00002431
Iteration 5/1000 | Loss: 0.00002317
Iteration 6/1000 | Loss: 0.00002217
Iteration 7/1000 | Loss: 0.00002158
Iteration 8/1000 | Loss: 0.00002099
Iteration 9/1000 | Loss: 0.00002063
Iteration 10/1000 | Loss: 0.00002041
Iteration 11/1000 | Loss: 0.00002022
Iteration 12/1000 | Loss: 0.00002010
Iteration 13/1000 | Loss: 0.00002003
Iteration 14/1000 | Loss: 0.00001998
Iteration 15/1000 | Loss: 0.00001998
Iteration 16/1000 | Loss: 0.00001996
Iteration 17/1000 | Loss: 0.00001996
Iteration 18/1000 | Loss: 0.00001996
Iteration 19/1000 | Loss: 0.00001996
Iteration 20/1000 | Loss: 0.00001996
Iteration 21/1000 | Loss: 0.00001996
Iteration 22/1000 | Loss: 0.00001996
Iteration 23/1000 | Loss: 0.00001996
Iteration 24/1000 | Loss: 0.00001996
Iteration 25/1000 | Loss: 0.00001995
Iteration 26/1000 | Loss: 0.00001995
Iteration 27/1000 | Loss: 0.00001995
Iteration 28/1000 | Loss: 0.00001995
Iteration 29/1000 | Loss: 0.00001995
Iteration 30/1000 | Loss: 0.00001995
Iteration 31/1000 | Loss: 0.00001995
Iteration 32/1000 | Loss: 0.00001994
Iteration 33/1000 | Loss: 0.00001994
Iteration 34/1000 | Loss: 0.00001993
Iteration 35/1000 | Loss: 0.00001992
Iteration 36/1000 | Loss: 0.00001992
Iteration 37/1000 | Loss: 0.00001992
Iteration 38/1000 | Loss: 0.00001991
Iteration 39/1000 | Loss: 0.00001991
Iteration 40/1000 | Loss: 0.00001990
Iteration 41/1000 | Loss: 0.00001990
Iteration 42/1000 | Loss: 0.00001990
Iteration 43/1000 | Loss: 0.00001989
Iteration 44/1000 | Loss: 0.00001989
Iteration 45/1000 | Loss: 0.00001989
Iteration 46/1000 | Loss: 0.00001989
Iteration 47/1000 | Loss: 0.00001988
Iteration 48/1000 | Loss: 0.00001988
Iteration 49/1000 | Loss: 0.00001988
Iteration 50/1000 | Loss: 0.00001988
Iteration 51/1000 | Loss: 0.00001988
Iteration 52/1000 | Loss: 0.00001988
Iteration 53/1000 | Loss: 0.00001987
Iteration 54/1000 | Loss: 0.00001987
Iteration 55/1000 | Loss: 0.00001987
Iteration 56/1000 | Loss: 0.00001987
Iteration 57/1000 | Loss: 0.00001987
Iteration 58/1000 | Loss: 0.00001986
Iteration 59/1000 | Loss: 0.00001986
Iteration 60/1000 | Loss: 0.00001986
Iteration 61/1000 | Loss: 0.00001986
Iteration 62/1000 | Loss: 0.00001985
Iteration 63/1000 | Loss: 0.00001985
Iteration 64/1000 | Loss: 0.00001985
Iteration 65/1000 | Loss: 0.00001985
Iteration 66/1000 | Loss: 0.00001985
Iteration 67/1000 | Loss: 0.00001985
Iteration 68/1000 | Loss: 0.00001985
Iteration 69/1000 | Loss: 0.00001984
Iteration 70/1000 | Loss: 0.00001984
Iteration 71/1000 | Loss: 0.00001984
Iteration 72/1000 | Loss: 0.00001984
Iteration 73/1000 | Loss: 0.00001984
Iteration 74/1000 | Loss: 0.00001984
Iteration 75/1000 | Loss: 0.00001984
Iteration 76/1000 | Loss: 0.00001984
Iteration 77/1000 | Loss: 0.00001984
Iteration 78/1000 | Loss: 0.00001984
Iteration 79/1000 | Loss: 0.00001984
Iteration 80/1000 | Loss: 0.00001983
Iteration 81/1000 | Loss: 0.00001983
Iteration 82/1000 | Loss: 0.00001983
Iteration 83/1000 | Loss: 0.00001983
Iteration 84/1000 | Loss: 0.00001983
Iteration 85/1000 | Loss: 0.00001983
Iteration 86/1000 | Loss: 0.00001983
Iteration 87/1000 | Loss: 0.00001983
Iteration 88/1000 | Loss: 0.00001983
Iteration 89/1000 | Loss: 0.00001983
Iteration 90/1000 | Loss: 0.00001983
Iteration 91/1000 | Loss: 0.00001983
Iteration 92/1000 | Loss: 0.00001983
Iteration 93/1000 | Loss: 0.00001983
Iteration 94/1000 | Loss: 0.00001983
Iteration 95/1000 | Loss: 0.00001983
Iteration 96/1000 | Loss: 0.00001983
Iteration 97/1000 | Loss: 0.00001983
Iteration 98/1000 | Loss: 0.00001983
Iteration 99/1000 | Loss: 0.00001982
Iteration 100/1000 | Loss: 0.00001982
Iteration 101/1000 | Loss: 0.00001982
Iteration 102/1000 | Loss: 0.00001982
Iteration 103/1000 | Loss: 0.00001982
Iteration 104/1000 | Loss: 0.00001982
Iteration 105/1000 | Loss: 0.00001982
Iteration 106/1000 | Loss: 0.00001982
Iteration 107/1000 | Loss: 0.00001982
Iteration 108/1000 | Loss: 0.00001982
Iteration 109/1000 | Loss: 0.00001982
Iteration 110/1000 | Loss: 0.00001982
Iteration 111/1000 | Loss: 0.00001982
Iteration 112/1000 | Loss: 0.00001982
Iteration 113/1000 | Loss: 0.00001982
Iteration 114/1000 | Loss: 0.00001982
Iteration 115/1000 | Loss: 0.00001982
Iteration 116/1000 | Loss: 0.00001982
Iteration 117/1000 | Loss: 0.00001982
Iteration 118/1000 | Loss: 0.00001982
Iteration 119/1000 | Loss: 0.00001982
Iteration 120/1000 | Loss: 0.00001982
Iteration 121/1000 | Loss: 0.00001982
Iteration 122/1000 | Loss: 0.00001982
Iteration 123/1000 | Loss: 0.00001982
Iteration 124/1000 | Loss: 0.00001982
Iteration 125/1000 | Loss: 0.00001982
Iteration 126/1000 | Loss: 0.00001982
Iteration 127/1000 | Loss: 0.00001982
Iteration 128/1000 | Loss: 0.00001982
Iteration 129/1000 | Loss: 0.00001982
Iteration 130/1000 | Loss: 0.00001982
Iteration 131/1000 | Loss: 0.00001982
Iteration 132/1000 | Loss: 0.00001982
Iteration 133/1000 | Loss: 0.00001982
Iteration 134/1000 | Loss: 0.00001982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.9817120119114406e-05, 1.9817120119114406e-05, 1.9817120119114406e-05, 1.9817120119114406e-05, 1.9817120119114406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9817120119114406e-05

Optimization complete. Final v2v error: 3.7676773071289062 mm

Highest mean error: 4.788272857666016 mm for frame 39

Lowest mean error: 3.2887518405914307 mm for frame 121

Saving results

Total time: 53.733776569366455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396676
Iteration 2/25 | Loss: 0.00085829
Iteration 3/25 | Loss: 0.00065335
Iteration 4/25 | Loss: 0.00062179
Iteration 5/25 | Loss: 0.00061083
Iteration 6/25 | Loss: 0.00060782
Iteration 7/25 | Loss: 0.00060694
Iteration 8/25 | Loss: 0.00060686
Iteration 9/25 | Loss: 0.00060686
Iteration 10/25 | Loss: 0.00060686
Iteration 11/25 | Loss: 0.00060686
Iteration 12/25 | Loss: 0.00060686
Iteration 13/25 | Loss: 0.00060686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000606857764068991, 0.000606857764068991, 0.000606857764068991, 0.000606857764068991, 0.000606857764068991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000606857764068991

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36209786
Iteration 2/25 | Loss: 0.00026990
Iteration 3/25 | Loss: 0.00026988
Iteration 4/25 | Loss: 0.00026988
Iteration 5/25 | Loss: 0.00026988
Iteration 6/25 | Loss: 0.00026988
Iteration 7/25 | Loss: 0.00026988
Iteration 8/25 | Loss: 0.00026988
Iteration 9/25 | Loss: 0.00026988
Iteration 10/25 | Loss: 0.00026988
Iteration 11/25 | Loss: 0.00026988
Iteration 12/25 | Loss: 0.00026988
Iteration 13/25 | Loss: 0.00026988
Iteration 14/25 | Loss: 0.00026988
Iteration 15/25 | Loss: 0.00026988
Iteration 16/25 | Loss: 0.00026988
Iteration 17/25 | Loss: 0.00026988
Iteration 18/25 | Loss: 0.00026988
Iteration 19/25 | Loss: 0.00026988
Iteration 20/25 | Loss: 0.00026988
Iteration 21/25 | Loss: 0.00026988
Iteration 22/25 | Loss: 0.00026988
Iteration 23/25 | Loss: 0.00026988
Iteration 24/25 | Loss: 0.00026988
Iteration 25/25 | Loss: 0.00026988

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026988
Iteration 2/1000 | Loss: 0.00002939
Iteration 3/1000 | Loss: 0.00001935
Iteration 4/1000 | Loss: 0.00001579
Iteration 5/1000 | Loss: 0.00001479
Iteration 6/1000 | Loss: 0.00001433
Iteration 7/1000 | Loss: 0.00001381
Iteration 8/1000 | Loss: 0.00001358
Iteration 9/1000 | Loss: 0.00001330
Iteration 10/1000 | Loss: 0.00001317
Iteration 11/1000 | Loss: 0.00001313
Iteration 12/1000 | Loss: 0.00001313
Iteration 13/1000 | Loss: 0.00001312
Iteration 14/1000 | Loss: 0.00001306
Iteration 15/1000 | Loss: 0.00001303
Iteration 16/1000 | Loss: 0.00001302
Iteration 17/1000 | Loss: 0.00001300
Iteration 18/1000 | Loss: 0.00001300
Iteration 19/1000 | Loss: 0.00001299
Iteration 20/1000 | Loss: 0.00001298
Iteration 21/1000 | Loss: 0.00001292
Iteration 22/1000 | Loss: 0.00001290
Iteration 23/1000 | Loss: 0.00001289
Iteration 24/1000 | Loss: 0.00001288
Iteration 25/1000 | Loss: 0.00001288
Iteration 26/1000 | Loss: 0.00001288
Iteration 27/1000 | Loss: 0.00001288
Iteration 28/1000 | Loss: 0.00001288
Iteration 29/1000 | Loss: 0.00001288
Iteration 30/1000 | Loss: 0.00001287
Iteration 31/1000 | Loss: 0.00001287
Iteration 32/1000 | Loss: 0.00001286
Iteration 33/1000 | Loss: 0.00001286
Iteration 34/1000 | Loss: 0.00001285
Iteration 35/1000 | Loss: 0.00001285
Iteration 36/1000 | Loss: 0.00001285
Iteration 37/1000 | Loss: 0.00001284
Iteration 38/1000 | Loss: 0.00001284
Iteration 39/1000 | Loss: 0.00001284
Iteration 40/1000 | Loss: 0.00001283
Iteration 41/1000 | Loss: 0.00001282
Iteration 42/1000 | Loss: 0.00001282
Iteration 43/1000 | Loss: 0.00001282
Iteration 44/1000 | Loss: 0.00001281
Iteration 45/1000 | Loss: 0.00001281
Iteration 46/1000 | Loss: 0.00001281
Iteration 47/1000 | Loss: 0.00001279
Iteration 48/1000 | Loss: 0.00001275
Iteration 49/1000 | Loss: 0.00001275
Iteration 50/1000 | Loss: 0.00001274
Iteration 51/1000 | Loss: 0.00001274
Iteration 52/1000 | Loss: 0.00001274
Iteration 53/1000 | Loss: 0.00001274
Iteration 54/1000 | Loss: 0.00001274
Iteration 55/1000 | Loss: 0.00001274
Iteration 56/1000 | Loss: 0.00001274
Iteration 57/1000 | Loss: 0.00001274
Iteration 58/1000 | Loss: 0.00001274
Iteration 59/1000 | Loss: 0.00001274
Iteration 60/1000 | Loss: 0.00001274
Iteration 61/1000 | Loss: 0.00001273
Iteration 62/1000 | Loss: 0.00001273
Iteration 63/1000 | Loss: 0.00001272
Iteration 64/1000 | Loss: 0.00001272
Iteration 65/1000 | Loss: 0.00001272
Iteration 66/1000 | Loss: 0.00001272
Iteration 67/1000 | Loss: 0.00001271
Iteration 68/1000 | Loss: 0.00001271
Iteration 69/1000 | Loss: 0.00001270
Iteration 70/1000 | Loss: 0.00001270
Iteration 71/1000 | Loss: 0.00001270
Iteration 72/1000 | Loss: 0.00001270
Iteration 73/1000 | Loss: 0.00001269
Iteration 74/1000 | Loss: 0.00001269
Iteration 75/1000 | Loss: 0.00001269
Iteration 76/1000 | Loss: 0.00001268
Iteration 77/1000 | Loss: 0.00001268
Iteration 78/1000 | Loss: 0.00001264
Iteration 79/1000 | Loss: 0.00001264
Iteration 80/1000 | Loss: 0.00001264
Iteration 81/1000 | Loss: 0.00001264
Iteration 82/1000 | Loss: 0.00001264
Iteration 83/1000 | Loss: 0.00001264
Iteration 84/1000 | Loss: 0.00001264
Iteration 85/1000 | Loss: 0.00001264
Iteration 86/1000 | Loss: 0.00001264
Iteration 87/1000 | Loss: 0.00001264
Iteration 88/1000 | Loss: 0.00001264
Iteration 89/1000 | Loss: 0.00001264
Iteration 90/1000 | Loss: 0.00001264
Iteration 91/1000 | Loss: 0.00001263
Iteration 92/1000 | Loss: 0.00001263
Iteration 93/1000 | Loss: 0.00001262
Iteration 94/1000 | Loss: 0.00001261
Iteration 95/1000 | Loss: 0.00001261
Iteration 96/1000 | Loss: 0.00001260
Iteration 97/1000 | Loss: 0.00001260
Iteration 98/1000 | Loss: 0.00001260
Iteration 99/1000 | Loss: 0.00001259
Iteration 100/1000 | Loss: 0.00001259
Iteration 101/1000 | Loss: 0.00001259
Iteration 102/1000 | Loss: 0.00001258
Iteration 103/1000 | Loss: 0.00001258
Iteration 104/1000 | Loss: 0.00001258
Iteration 105/1000 | Loss: 0.00001258
Iteration 106/1000 | Loss: 0.00001258
Iteration 107/1000 | Loss: 0.00001258
Iteration 108/1000 | Loss: 0.00001257
Iteration 109/1000 | Loss: 0.00001257
Iteration 110/1000 | Loss: 0.00001256
Iteration 111/1000 | Loss: 0.00001256
Iteration 112/1000 | Loss: 0.00001256
Iteration 113/1000 | Loss: 0.00001255
Iteration 114/1000 | Loss: 0.00001255
Iteration 115/1000 | Loss: 0.00001255
Iteration 116/1000 | Loss: 0.00001255
Iteration 117/1000 | Loss: 0.00001255
Iteration 118/1000 | Loss: 0.00001255
Iteration 119/1000 | Loss: 0.00001255
Iteration 120/1000 | Loss: 0.00001254
Iteration 121/1000 | Loss: 0.00001254
Iteration 122/1000 | Loss: 0.00001254
Iteration 123/1000 | Loss: 0.00001254
Iteration 124/1000 | Loss: 0.00001254
Iteration 125/1000 | Loss: 0.00001254
Iteration 126/1000 | Loss: 0.00001254
Iteration 127/1000 | Loss: 0.00001254
Iteration 128/1000 | Loss: 0.00001254
Iteration 129/1000 | Loss: 0.00001254
Iteration 130/1000 | Loss: 0.00001253
Iteration 131/1000 | Loss: 0.00001253
Iteration 132/1000 | Loss: 0.00001253
Iteration 133/1000 | Loss: 0.00001253
Iteration 134/1000 | Loss: 0.00001253
Iteration 135/1000 | Loss: 0.00001253
Iteration 136/1000 | Loss: 0.00001253
Iteration 137/1000 | Loss: 0.00001253
Iteration 138/1000 | Loss: 0.00001253
Iteration 139/1000 | Loss: 0.00001253
Iteration 140/1000 | Loss: 0.00001253
Iteration 141/1000 | Loss: 0.00001253
Iteration 142/1000 | Loss: 0.00001252
Iteration 143/1000 | Loss: 0.00001252
Iteration 144/1000 | Loss: 0.00001252
Iteration 145/1000 | Loss: 0.00001252
Iteration 146/1000 | Loss: 0.00001252
Iteration 147/1000 | Loss: 0.00001252
Iteration 148/1000 | Loss: 0.00001252
Iteration 149/1000 | Loss: 0.00001252
Iteration 150/1000 | Loss: 0.00001252
Iteration 151/1000 | Loss: 0.00001252
Iteration 152/1000 | Loss: 0.00001252
Iteration 153/1000 | Loss: 0.00001252
Iteration 154/1000 | Loss: 0.00001252
Iteration 155/1000 | Loss: 0.00001252
Iteration 156/1000 | Loss: 0.00001252
Iteration 157/1000 | Loss: 0.00001252
Iteration 158/1000 | Loss: 0.00001252
Iteration 159/1000 | Loss: 0.00001252
Iteration 160/1000 | Loss: 0.00001252
Iteration 161/1000 | Loss: 0.00001252
Iteration 162/1000 | Loss: 0.00001251
Iteration 163/1000 | Loss: 0.00001251
Iteration 164/1000 | Loss: 0.00001251
Iteration 165/1000 | Loss: 0.00001251
Iteration 166/1000 | Loss: 0.00001251
Iteration 167/1000 | Loss: 0.00001250
Iteration 168/1000 | Loss: 0.00001250
Iteration 169/1000 | Loss: 0.00001250
Iteration 170/1000 | Loss: 0.00001250
Iteration 171/1000 | Loss: 0.00001250
Iteration 172/1000 | Loss: 0.00001250
Iteration 173/1000 | Loss: 0.00001250
Iteration 174/1000 | Loss: 0.00001250
Iteration 175/1000 | Loss: 0.00001250
Iteration 176/1000 | Loss: 0.00001249
Iteration 177/1000 | Loss: 0.00001249
Iteration 178/1000 | Loss: 0.00001249
Iteration 179/1000 | Loss: 0.00001249
Iteration 180/1000 | Loss: 0.00001249
Iteration 181/1000 | Loss: 0.00001249
Iteration 182/1000 | Loss: 0.00001249
Iteration 183/1000 | Loss: 0.00001249
Iteration 184/1000 | Loss: 0.00001249
Iteration 185/1000 | Loss: 0.00001249
Iteration 186/1000 | Loss: 0.00001249
Iteration 187/1000 | Loss: 0.00001249
Iteration 188/1000 | Loss: 0.00001249
Iteration 189/1000 | Loss: 0.00001249
Iteration 190/1000 | Loss: 0.00001249
Iteration 191/1000 | Loss: 0.00001249
Iteration 192/1000 | Loss: 0.00001249
Iteration 193/1000 | Loss: 0.00001249
Iteration 194/1000 | Loss: 0.00001249
Iteration 195/1000 | Loss: 0.00001249
Iteration 196/1000 | Loss: 0.00001249
Iteration 197/1000 | Loss: 0.00001249
Iteration 198/1000 | Loss: 0.00001249
Iteration 199/1000 | Loss: 0.00001249
Iteration 200/1000 | Loss: 0.00001249
Iteration 201/1000 | Loss: 0.00001249
Iteration 202/1000 | Loss: 0.00001249
Iteration 203/1000 | Loss: 0.00001249
Iteration 204/1000 | Loss: 0.00001249
Iteration 205/1000 | Loss: 0.00001249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.2492374480643775e-05, 1.2492374480643775e-05, 1.2492374480643775e-05, 1.2492374480643775e-05, 1.2492374480643775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2492374480643775e-05

Optimization complete. Final v2v error: 2.876197576522827 mm

Highest mean error: 4.848387718200684 mm for frame 84

Lowest mean error: 2.345531702041626 mm for frame 117

Saving results

Total time: 41.20886421203613
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998559
Iteration 2/25 | Loss: 0.00307073
Iteration 3/25 | Loss: 0.00199370
Iteration 4/25 | Loss: 0.00188791
Iteration 5/25 | Loss: 0.00174374
Iteration 6/25 | Loss: 0.00167678
Iteration 7/25 | Loss: 0.00153578
Iteration 8/25 | Loss: 0.00138921
Iteration 9/25 | Loss: 0.00176186
Iteration 10/25 | Loss: 0.00191173
Iteration 11/25 | Loss: 0.00136164
Iteration 12/25 | Loss: 0.00090147
Iteration 13/25 | Loss: 0.00082919
Iteration 14/25 | Loss: 0.00082112
Iteration 15/25 | Loss: 0.00082767
Iteration 16/25 | Loss: 0.00082678
Iteration 17/25 | Loss: 0.00082658
Iteration 18/25 | Loss: 0.00081091
Iteration 19/25 | Loss: 0.00081110
Iteration 20/25 | Loss: 0.00080500
Iteration 21/25 | Loss: 0.00080251
Iteration 22/25 | Loss: 0.00080071
Iteration 23/25 | Loss: 0.00080017
Iteration 24/25 | Loss: 0.00080008
Iteration 25/25 | Loss: 0.00080007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.14395046
Iteration 2/25 | Loss: 0.00121099
Iteration 3/25 | Loss: 0.00037805
Iteration 4/25 | Loss: 0.00037805
Iteration 5/25 | Loss: 0.00037805
Iteration 6/25 | Loss: 0.00037805
Iteration 7/25 | Loss: 0.00037805
Iteration 8/25 | Loss: 0.00037805
Iteration 9/25 | Loss: 0.00037805
Iteration 10/25 | Loss: 0.00037805
Iteration 11/25 | Loss: 0.00037805
Iteration 12/25 | Loss: 0.00037805
Iteration 13/25 | Loss: 0.00037805
Iteration 14/25 | Loss: 0.00037805
Iteration 15/25 | Loss: 0.00037805
Iteration 16/25 | Loss: 0.00037805
Iteration 17/25 | Loss: 0.00037805
Iteration 18/25 | Loss: 0.00037805
Iteration 19/25 | Loss: 0.00037805
Iteration 20/25 | Loss: 0.00037805
Iteration 21/25 | Loss: 0.00037805
Iteration 22/25 | Loss: 0.00037805
Iteration 23/25 | Loss: 0.00037805
Iteration 24/25 | Loss: 0.00037805
Iteration 25/25 | Loss: 0.00037805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037805
Iteration 2/1000 | Loss: 0.00005195
Iteration 3/1000 | Loss: 0.00004175
Iteration 4/1000 | Loss: 0.00003840
Iteration 5/1000 | Loss: 0.00047227
Iteration 6/1000 | Loss: 0.00004321
Iteration 7/1000 | Loss: 0.00003254
Iteration 8/1000 | Loss: 0.00002935
Iteration 9/1000 | Loss: 0.00002732
Iteration 10/1000 | Loss: 0.00002644
Iteration 11/1000 | Loss: 0.00002579
Iteration 12/1000 | Loss: 0.00002560
Iteration 13/1000 | Loss: 0.00002530
Iteration 14/1000 | Loss: 0.00002506
Iteration 15/1000 | Loss: 0.00002495
Iteration 16/1000 | Loss: 0.00002483
Iteration 17/1000 | Loss: 0.00002475
Iteration 18/1000 | Loss: 0.00002475
Iteration 19/1000 | Loss: 0.00002475
Iteration 20/1000 | Loss: 0.00002474
Iteration 21/1000 | Loss: 0.00002474
Iteration 22/1000 | Loss: 0.00002473
Iteration 23/1000 | Loss: 0.00002472
Iteration 24/1000 | Loss: 0.00002471
Iteration 25/1000 | Loss: 0.00002470
Iteration 26/1000 | Loss: 0.00002470
Iteration 27/1000 | Loss: 0.00002470
Iteration 28/1000 | Loss: 0.00002469
Iteration 29/1000 | Loss: 0.00002469
Iteration 30/1000 | Loss: 0.00002469
Iteration 31/1000 | Loss: 0.00002468
Iteration 32/1000 | Loss: 0.00002468
Iteration 33/1000 | Loss: 0.00002468
Iteration 34/1000 | Loss: 0.00002468
Iteration 35/1000 | Loss: 0.00002468
Iteration 36/1000 | Loss: 0.00002468
Iteration 37/1000 | Loss: 0.00002467
Iteration 38/1000 | Loss: 0.00002467
Iteration 39/1000 | Loss: 0.00002466
Iteration 40/1000 | Loss: 0.00002466
Iteration 41/1000 | Loss: 0.00002466
Iteration 42/1000 | Loss: 0.00002466
Iteration 43/1000 | Loss: 0.00002466
Iteration 44/1000 | Loss: 0.00002466
Iteration 45/1000 | Loss: 0.00002466
Iteration 46/1000 | Loss: 0.00002466
Iteration 47/1000 | Loss: 0.00002465
Iteration 48/1000 | Loss: 0.00002465
Iteration 49/1000 | Loss: 0.00002465
Iteration 50/1000 | Loss: 0.00002465
Iteration 51/1000 | Loss: 0.00002465
Iteration 52/1000 | Loss: 0.00002465
Iteration 53/1000 | Loss: 0.00002465
Iteration 54/1000 | Loss: 0.00002465
Iteration 55/1000 | Loss: 0.00002465
Iteration 56/1000 | Loss: 0.00002465
Iteration 57/1000 | Loss: 0.00002465
Iteration 58/1000 | Loss: 0.00002465
Iteration 59/1000 | Loss: 0.00002465
Iteration 60/1000 | Loss: 0.00002465
Iteration 61/1000 | Loss: 0.00002465
Iteration 62/1000 | Loss: 0.00002465
Iteration 63/1000 | Loss: 0.00002465
Iteration 64/1000 | Loss: 0.00002465
Iteration 65/1000 | Loss: 0.00002465
Iteration 66/1000 | Loss: 0.00002465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [2.4650900741107762e-05, 2.4650900741107762e-05, 2.4650900741107762e-05, 2.4650900741107762e-05, 2.4650900741107762e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4650900741107762e-05

Optimization complete. Final v2v error: 4.133144855499268 mm

Highest mean error: 9.649843215942383 mm for frame 47

Lowest mean error: 3.6866860389709473 mm for frame 24

Saving results

Total time: 66.34458303451538
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944209
Iteration 2/25 | Loss: 0.00106782
Iteration 3/25 | Loss: 0.00082171
Iteration 4/25 | Loss: 0.00077920
Iteration 5/25 | Loss: 0.00075814
Iteration 6/25 | Loss: 0.00075232
Iteration 7/25 | Loss: 0.00075086
Iteration 8/25 | Loss: 0.00075073
Iteration 9/25 | Loss: 0.00075073
Iteration 10/25 | Loss: 0.00075073
Iteration 11/25 | Loss: 0.00075073
Iteration 12/25 | Loss: 0.00075073
Iteration 13/25 | Loss: 0.00075073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007507327245548368, 0.0007507327245548368, 0.0007507327245548368, 0.0007507327245548368, 0.0007507327245548368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007507327245548368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45993161
Iteration 2/25 | Loss: 0.00028552
Iteration 3/25 | Loss: 0.00028550
Iteration 4/25 | Loss: 0.00028550
Iteration 5/25 | Loss: 0.00028550
Iteration 6/25 | Loss: 0.00028550
Iteration 7/25 | Loss: 0.00028550
Iteration 8/25 | Loss: 0.00028550
Iteration 9/25 | Loss: 0.00028550
Iteration 10/25 | Loss: 0.00028550
Iteration 11/25 | Loss: 0.00028550
Iteration 12/25 | Loss: 0.00028550
Iteration 13/25 | Loss: 0.00028550
Iteration 14/25 | Loss: 0.00028550
Iteration 15/25 | Loss: 0.00028550
Iteration 16/25 | Loss: 0.00028550
Iteration 17/25 | Loss: 0.00028550
Iteration 18/25 | Loss: 0.00028550
Iteration 19/25 | Loss: 0.00028550
Iteration 20/25 | Loss: 0.00028550
Iteration 21/25 | Loss: 0.00028550
Iteration 22/25 | Loss: 0.00028550
Iteration 23/25 | Loss: 0.00028550
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002854993217624724, 0.0002854993217624724, 0.0002854993217624724, 0.0002854993217624724, 0.0002854993217624724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002854993217624724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028550
Iteration 2/1000 | Loss: 0.00005545
Iteration 3/1000 | Loss: 0.00003969
Iteration 4/1000 | Loss: 0.00003561
Iteration 5/1000 | Loss: 0.00003395
Iteration 6/1000 | Loss: 0.00003248
Iteration 7/1000 | Loss: 0.00003142
Iteration 8/1000 | Loss: 0.00003053
Iteration 9/1000 | Loss: 0.00003010
Iteration 10/1000 | Loss: 0.00002977
Iteration 11/1000 | Loss: 0.00002956
Iteration 12/1000 | Loss: 0.00002941
Iteration 13/1000 | Loss: 0.00002933
Iteration 14/1000 | Loss: 0.00002932
Iteration 15/1000 | Loss: 0.00002917
Iteration 16/1000 | Loss: 0.00002914
Iteration 17/1000 | Loss: 0.00002912
Iteration 18/1000 | Loss: 0.00002911
Iteration 19/1000 | Loss: 0.00002911
Iteration 20/1000 | Loss: 0.00002911
Iteration 21/1000 | Loss: 0.00002911
Iteration 22/1000 | Loss: 0.00002911
Iteration 23/1000 | Loss: 0.00002911
Iteration 24/1000 | Loss: 0.00002911
Iteration 25/1000 | Loss: 0.00002911
Iteration 26/1000 | Loss: 0.00002911
Iteration 27/1000 | Loss: 0.00002909
Iteration 28/1000 | Loss: 0.00002908
Iteration 29/1000 | Loss: 0.00002908
Iteration 30/1000 | Loss: 0.00002907
Iteration 31/1000 | Loss: 0.00002907
Iteration 32/1000 | Loss: 0.00002907
Iteration 33/1000 | Loss: 0.00002907
Iteration 34/1000 | Loss: 0.00002907
Iteration 35/1000 | Loss: 0.00002906
Iteration 36/1000 | Loss: 0.00002906
Iteration 37/1000 | Loss: 0.00002906
Iteration 38/1000 | Loss: 0.00002906
Iteration 39/1000 | Loss: 0.00002904
Iteration 40/1000 | Loss: 0.00002904
Iteration 41/1000 | Loss: 0.00002903
Iteration 42/1000 | Loss: 0.00002902
Iteration 43/1000 | Loss: 0.00002902
Iteration 44/1000 | Loss: 0.00002902
Iteration 45/1000 | Loss: 0.00002902
Iteration 46/1000 | Loss: 0.00002899
Iteration 47/1000 | Loss: 0.00002899
Iteration 48/1000 | Loss: 0.00002898
Iteration 49/1000 | Loss: 0.00002898
Iteration 50/1000 | Loss: 0.00002896
Iteration 51/1000 | Loss: 0.00002896
Iteration 52/1000 | Loss: 0.00002895
Iteration 53/1000 | Loss: 0.00002895
Iteration 54/1000 | Loss: 0.00002895
Iteration 55/1000 | Loss: 0.00002895
Iteration 56/1000 | Loss: 0.00002894
Iteration 57/1000 | Loss: 0.00002894
Iteration 58/1000 | Loss: 0.00002894
Iteration 59/1000 | Loss: 0.00002894
Iteration 60/1000 | Loss: 0.00002894
Iteration 61/1000 | Loss: 0.00002893
Iteration 62/1000 | Loss: 0.00002893
Iteration 63/1000 | Loss: 0.00002892
Iteration 64/1000 | Loss: 0.00002892
Iteration 65/1000 | Loss: 0.00002892
Iteration 66/1000 | Loss: 0.00002891
Iteration 67/1000 | Loss: 0.00002891
Iteration 68/1000 | Loss: 0.00002891
Iteration 69/1000 | Loss: 0.00002891
Iteration 70/1000 | Loss: 0.00002891
Iteration 71/1000 | Loss: 0.00002891
Iteration 72/1000 | Loss: 0.00002891
Iteration 73/1000 | Loss: 0.00002891
Iteration 74/1000 | Loss: 0.00002891
Iteration 75/1000 | Loss: 0.00002891
Iteration 76/1000 | Loss: 0.00002890
Iteration 77/1000 | Loss: 0.00002890
Iteration 78/1000 | Loss: 0.00002890
Iteration 79/1000 | Loss: 0.00002890
Iteration 80/1000 | Loss: 0.00002890
Iteration 81/1000 | Loss: 0.00002889
Iteration 82/1000 | Loss: 0.00002889
Iteration 83/1000 | Loss: 0.00002889
Iteration 84/1000 | Loss: 0.00002889
Iteration 85/1000 | Loss: 0.00002889
Iteration 86/1000 | Loss: 0.00002889
Iteration 87/1000 | Loss: 0.00002889
Iteration 88/1000 | Loss: 0.00002889
Iteration 89/1000 | Loss: 0.00002888
Iteration 90/1000 | Loss: 0.00002888
Iteration 91/1000 | Loss: 0.00002888
Iteration 92/1000 | Loss: 0.00002888
Iteration 93/1000 | Loss: 0.00002888
Iteration 94/1000 | Loss: 0.00002888
Iteration 95/1000 | Loss: 0.00002888
Iteration 96/1000 | Loss: 0.00002887
Iteration 97/1000 | Loss: 0.00002887
Iteration 98/1000 | Loss: 0.00002887
Iteration 99/1000 | Loss: 0.00002887
Iteration 100/1000 | Loss: 0.00002886
Iteration 101/1000 | Loss: 0.00002886
Iteration 102/1000 | Loss: 0.00002886
Iteration 103/1000 | Loss: 0.00002885
Iteration 104/1000 | Loss: 0.00002885
Iteration 105/1000 | Loss: 0.00002885
Iteration 106/1000 | Loss: 0.00002885
Iteration 107/1000 | Loss: 0.00002885
Iteration 108/1000 | Loss: 0.00002885
Iteration 109/1000 | Loss: 0.00002885
Iteration 110/1000 | Loss: 0.00002885
Iteration 111/1000 | Loss: 0.00002885
Iteration 112/1000 | Loss: 0.00002885
Iteration 113/1000 | Loss: 0.00002885
Iteration 114/1000 | Loss: 0.00002884
Iteration 115/1000 | Loss: 0.00002884
Iteration 116/1000 | Loss: 0.00002884
Iteration 117/1000 | Loss: 0.00002884
Iteration 118/1000 | Loss: 0.00002884
Iteration 119/1000 | Loss: 0.00002884
Iteration 120/1000 | Loss: 0.00002884
Iteration 121/1000 | Loss: 0.00002884
Iteration 122/1000 | Loss: 0.00002884
Iteration 123/1000 | Loss: 0.00002883
Iteration 124/1000 | Loss: 0.00002883
Iteration 125/1000 | Loss: 0.00002883
Iteration 126/1000 | Loss: 0.00002883
Iteration 127/1000 | Loss: 0.00002883
Iteration 128/1000 | Loss: 0.00002883
Iteration 129/1000 | Loss: 0.00002883
Iteration 130/1000 | Loss: 0.00002883
Iteration 131/1000 | Loss: 0.00002883
Iteration 132/1000 | Loss: 0.00002883
Iteration 133/1000 | Loss: 0.00002883
Iteration 134/1000 | Loss: 0.00002883
Iteration 135/1000 | Loss: 0.00002883
Iteration 136/1000 | Loss: 0.00002883
Iteration 137/1000 | Loss: 0.00002883
Iteration 138/1000 | Loss: 0.00002883
Iteration 139/1000 | Loss: 0.00002883
Iteration 140/1000 | Loss: 0.00002883
Iteration 141/1000 | Loss: 0.00002883
Iteration 142/1000 | Loss: 0.00002882
Iteration 143/1000 | Loss: 0.00002882
Iteration 144/1000 | Loss: 0.00002882
Iteration 145/1000 | Loss: 0.00002882
Iteration 146/1000 | Loss: 0.00002882
Iteration 147/1000 | Loss: 0.00002882
Iteration 148/1000 | Loss: 0.00002882
Iteration 149/1000 | Loss: 0.00002882
Iteration 150/1000 | Loss: 0.00002882
Iteration 151/1000 | Loss: 0.00002882
Iteration 152/1000 | Loss: 0.00002882
Iteration 153/1000 | Loss: 0.00002882
Iteration 154/1000 | Loss: 0.00002881
Iteration 155/1000 | Loss: 0.00002881
Iteration 156/1000 | Loss: 0.00002881
Iteration 157/1000 | Loss: 0.00002881
Iteration 158/1000 | Loss: 0.00002881
Iteration 159/1000 | Loss: 0.00002881
Iteration 160/1000 | Loss: 0.00002881
Iteration 161/1000 | Loss: 0.00002881
Iteration 162/1000 | Loss: 0.00002881
Iteration 163/1000 | Loss: 0.00002881
Iteration 164/1000 | Loss: 0.00002881
Iteration 165/1000 | Loss: 0.00002881
Iteration 166/1000 | Loss: 0.00002881
Iteration 167/1000 | Loss: 0.00002881
Iteration 168/1000 | Loss: 0.00002881
Iteration 169/1000 | Loss: 0.00002881
Iteration 170/1000 | Loss: 0.00002881
Iteration 171/1000 | Loss: 0.00002881
Iteration 172/1000 | Loss: 0.00002881
Iteration 173/1000 | Loss: 0.00002881
Iteration 174/1000 | Loss: 0.00002881
Iteration 175/1000 | Loss: 0.00002881
Iteration 176/1000 | Loss: 0.00002881
Iteration 177/1000 | Loss: 0.00002881
Iteration 178/1000 | Loss: 0.00002881
Iteration 179/1000 | Loss: 0.00002881
Iteration 180/1000 | Loss: 0.00002881
Iteration 181/1000 | Loss: 0.00002881
Iteration 182/1000 | Loss: 0.00002881
Iteration 183/1000 | Loss: 0.00002881
Iteration 184/1000 | Loss: 0.00002881
Iteration 185/1000 | Loss: 0.00002881
Iteration 186/1000 | Loss: 0.00002881
Iteration 187/1000 | Loss: 0.00002881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [2.8807467970182188e-05, 2.8807467970182188e-05, 2.8807467970182188e-05, 2.8807467970182188e-05, 2.8807467970182188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8807467970182188e-05

Optimization complete. Final v2v error: 4.477470874786377 mm

Highest mean error: 6.029843807220459 mm for frame 67

Lowest mean error: 3.832226514816284 mm for frame 44

Saving results

Total time: 41.135618686676025
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806090
Iteration 2/25 | Loss: 0.00113906
Iteration 3/25 | Loss: 0.00080129
Iteration 4/25 | Loss: 0.00074638
Iteration 5/25 | Loss: 0.00073667
Iteration 6/25 | Loss: 0.00073599
Iteration 7/25 | Loss: 0.00073599
Iteration 8/25 | Loss: 0.00073599
Iteration 9/25 | Loss: 0.00073599
Iteration 10/25 | Loss: 0.00073599
Iteration 11/25 | Loss: 0.00073599
Iteration 12/25 | Loss: 0.00073599
Iteration 13/25 | Loss: 0.00073599
Iteration 14/25 | Loss: 0.00073599
Iteration 15/25 | Loss: 0.00073599
Iteration 16/25 | Loss: 0.00073599
Iteration 17/25 | Loss: 0.00073599
Iteration 18/25 | Loss: 0.00073599
Iteration 19/25 | Loss: 0.00073599
Iteration 20/25 | Loss: 0.00073599
Iteration 21/25 | Loss: 0.00073599
Iteration 22/25 | Loss: 0.00073599
Iteration 23/25 | Loss: 0.00073599
Iteration 24/25 | Loss: 0.00073599
Iteration 25/25 | Loss: 0.00073599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007359857554547489, 0.0007359857554547489, 0.0007359857554547489, 0.0007359857554547489, 0.0007359857554547489]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007359857554547489

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45328391
Iteration 2/25 | Loss: 0.00039730
Iteration 3/25 | Loss: 0.00039729
Iteration 4/25 | Loss: 0.00039729
Iteration 5/25 | Loss: 0.00039728
Iteration 6/25 | Loss: 0.00039728
Iteration 7/25 | Loss: 0.00039728
Iteration 8/25 | Loss: 0.00039728
Iteration 9/25 | Loss: 0.00039728
Iteration 10/25 | Loss: 0.00039728
Iteration 11/25 | Loss: 0.00039728
Iteration 12/25 | Loss: 0.00039728
Iteration 13/25 | Loss: 0.00039728
Iteration 14/25 | Loss: 0.00039728
Iteration 15/25 | Loss: 0.00039728
Iteration 16/25 | Loss: 0.00039728
Iteration 17/25 | Loss: 0.00039728
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00039728335104882717, 0.00039728335104882717, 0.00039728335104882717, 0.00039728335104882717, 0.00039728335104882717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039728335104882717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039728
Iteration 2/1000 | Loss: 0.00002997
Iteration 3/1000 | Loss: 0.00002415
Iteration 4/1000 | Loss: 0.00002225
Iteration 5/1000 | Loss: 0.00002142
Iteration 6/1000 | Loss: 0.00002091
Iteration 7/1000 | Loss: 0.00002048
Iteration 8/1000 | Loss: 0.00002007
Iteration 9/1000 | Loss: 0.00002006
Iteration 10/1000 | Loss: 0.00001990
Iteration 11/1000 | Loss: 0.00001989
Iteration 12/1000 | Loss: 0.00001988
Iteration 13/1000 | Loss: 0.00001985
Iteration 14/1000 | Loss: 0.00001985
Iteration 15/1000 | Loss: 0.00001982
Iteration 16/1000 | Loss: 0.00001981
Iteration 17/1000 | Loss: 0.00001980
Iteration 18/1000 | Loss: 0.00001980
Iteration 19/1000 | Loss: 0.00001979
Iteration 20/1000 | Loss: 0.00001979
Iteration 21/1000 | Loss: 0.00001979
Iteration 22/1000 | Loss: 0.00001978
Iteration 23/1000 | Loss: 0.00001978
Iteration 24/1000 | Loss: 0.00001978
Iteration 25/1000 | Loss: 0.00001977
Iteration 26/1000 | Loss: 0.00001974
Iteration 27/1000 | Loss: 0.00001973
Iteration 28/1000 | Loss: 0.00001972
Iteration 29/1000 | Loss: 0.00001967
Iteration 30/1000 | Loss: 0.00001966
Iteration 31/1000 | Loss: 0.00001965
Iteration 32/1000 | Loss: 0.00001965
Iteration 33/1000 | Loss: 0.00001965
Iteration 34/1000 | Loss: 0.00001965
Iteration 35/1000 | Loss: 0.00001964
Iteration 36/1000 | Loss: 0.00001964
Iteration 37/1000 | Loss: 0.00001964
Iteration 38/1000 | Loss: 0.00001964
Iteration 39/1000 | Loss: 0.00001963
Iteration 40/1000 | Loss: 0.00001963
Iteration 41/1000 | Loss: 0.00001963
Iteration 42/1000 | Loss: 0.00001962
Iteration 43/1000 | Loss: 0.00001962
Iteration 44/1000 | Loss: 0.00001962
Iteration 45/1000 | Loss: 0.00001962
Iteration 46/1000 | Loss: 0.00001962
Iteration 47/1000 | Loss: 0.00001962
Iteration 48/1000 | Loss: 0.00001961
Iteration 49/1000 | Loss: 0.00001961
Iteration 50/1000 | Loss: 0.00001961
Iteration 51/1000 | Loss: 0.00001961
Iteration 52/1000 | Loss: 0.00001961
Iteration 53/1000 | Loss: 0.00001960
Iteration 54/1000 | Loss: 0.00001960
Iteration 55/1000 | Loss: 0.00001960
Iteration 56/1000 | Loss: 0.00001960
Iteration 57/1000 | Loss: 0.00001960
Iteration 58/1000 | Loss: 0.00001960
Iteration 59/1000 | Loss: 0.00001960
Iteration 60/1000 | Loss: 0.00001960
Iteration 61/1000 | Loss: 0.00001960
Iteration 62/1000 | Loss: 0.00001960
Iteration 63/1000 | Loss: 0.00001959
Iteration 64/1000 | Loss: 0.00001959
Iteration 65/1000 | Loss: 0.00001959
Iteration 66/1000 | Loss: 0.00001959
Iteration 67/1000 | Loss: 0.00001959
Iteration 68/1000 | Loss: 0.00001959
Iteration 69/1000 | Loss: 0.00001959
Iteration 70/1000 | Loss: 0.00001959
Iteration 71/1000 | Loss: 0.00001958
Iteration 72/1000 | Loss: 0.00001958
Iteration 73/1000 | Loss: 0.00001958
Iteration 74/1000 | Loss: 0.00001958
Iteration 75/1000 | Loss: 0.00001958
Iteration 76/1000 | Loss: 0.00001958
Iteration 77/1000 | Loss: 0.00001958
Iteration 78/1000 | Loss: 0.00001958
Iteration 79/1000 | Loss: 0.00001957
Iteration 80/1000 | Loss: 0.00001957
Iteration 81/1000 | Loss: 0.00001957
Iteration 82/1000 | Loss: 0.00001957
Iteration 83/1000 | Loss: 0.00001957
Iteration 84/1000 | Loss: 0.00001957
Iteration 85/1000 | Loss: 0.00001957
Iteration 86/1000 | Loss: 0.00001957
Iteration 87/1000 | Loss: 0.00001957
Iteration 88/1000 | Loss: 0.00001957
Iteration 89/1000 | Loss: 0.00001957
Iteration 90/1000 | Loss: 0.00001957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.9569459254853427e-05, 1.9569459254853427e-05, 1.9569459254853427e-05, 1.9569459254853427e-05, 1.9569459254853427e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9569459254853427e-05

Optimization complete. Final v2v error: 3.678921699523926 mm

Highest mean error: 3.8118927478790283 mm for frame 25

Lowest mean error: 3.5728161334991455 mm for frame 90

Saving results

Total time: 32.36475229263306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899135
Iteration 2/25 | Loss: 0.00089103
Iteration 3/25 | Loss: 0.00069906
Iteration 4/25 | Loss: 0.00066294
Iteration 5/25 | Loss: 0.00065574
Iteration 6/25 | Loss: 0.00065434
Iteration 7/25 | Loss: 0.00065402
Iteration 8/25 | Loss: 0.00065402
Iteration 9/25 | Loss: 0.00065402
Iteration 10/25 | Loss: 0.00065402
Iteration 11/25 | Loss: 0.00065402
Iteration 12/25 | Loss: 0.00065402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006540227914229035, 0.0006540227914229035, 0.0006540227914229035, 0.0006540227914229035, 0.0006540227914229035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006540227914229035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.63673854
Iteration 2/25 | Loss: 0.00029490
Iteration 3/25 | Loss: 0.00029488
Iteration 4/25 | Loss: 0.00029488
Iteration 5/25 | Loss: 0.00029488
Iteration 6/25 | Loss: 0.00029488
Iteration 7/25 | Loss: 0.00029488
Iteration 8/25 | Loss: 0.00029488
Iteration 9/25 | Loss: 0.00029487
Iteration 10/25 | Loss: 0.00029487
Iteration 11/25 | Loss: 0.00029487
Iteration 12/25 | Loss: 0.00029487
Iteration 13/25 | Loss: 0.00029487
Iteration 14/25 | Loss: 0.00029487
Iteration 15/25 | Loss: 0.00029487
Iteration 16/25 | Loss: 0.00029487
Iteration 17/25 | Loss: 0.00029487
Iteration 18/25 | Loss: 0.00029487
Iteration 19/25 | Loss: 0.00029487
Iteration 20/25 | Loss: 0.00029487
Iteration 21/25 | Loss: 0.00029487
Iteration 22/25 | Loss: 0.00029487
Iteration 23/25 | Loss: 0.00029487
Iteration 24/25 | Loss: 0.00029487
Iteration 25/25 | Loss: 0.00029487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029487
Iteration 2/1000 | Loss: 0.00002835
Iteration 3/1000 | Loss: 0.00001917
Iteration 4/1000 | Loss: 0.00001801
Iteration 5/1000 | Loss: 0.00001721
Iteration 6/1000 | Loss: 0.00001683
Iteration 7/1000 | Loss: 0.00001647
Iteration 8/1000 | Loss: 0.00001619
Iteration 9/1000 | Loss: 0.00001617
Iteration 10/1000 | Loss: 0.00001601
Iteration 11/1000 | Loss: 0.00001594
Iteration 12/1000 | Loss: 0.00001593
Iteration 13/1000 | Loss: 0.00001584
Iteration 14/1000 | Loss: 0.00001581
Iteration 15/1000 | Loss: 0.00001581
Iteration 16/1000 | Loss: 0.00001580
Iteration 17/1000 | Loss: 0.00001580
Iteration 18/1000 | Loss: 0.00001580
Iteration 19/1000 | Loss: 0.00001578
Iteration 20/1000 | Loss: 0.00001574
Iteration 21/1000 | Loss: 0.00001572
Iteration 22/1000 | Loss: 0.00001572
Iteration 23/1000 | Loss: 0.00001572
Iteration 24/1000 | Loss: 0.00001571
Iteration 25/1000 | Loss: 0.00001571
Iteration 26/1000 | Loss: 0.00001570
Iteration 27/1000 | Loss: 0.00001569
Iteration 28/1000 | Loss: 0.00001569
Iteration 29/1000 | Loss: 0.00001567
Iteration 30/1000 | Loss: 0.00001566
Iteration 31/1000 | Loss: 0.00001566
Iteration 32/1000 | Loss: 0.00001566
Iteration 33/1000 | Loss: 0.00001565
Iteration 34/1000 | Loss: 0.00001564
Iteration 35/1000 | Loss: 0.00001563
Iteration 36/1000 | Loss: 0.00001562
Iteration 37/1000 | Loss: 0.00001562
Iteration 38/1000 | Loss: 0.00001562
Iteration 39/1000 | Loss: 0.00001562
Iteration 40/1000 | Loss: 0.00001561
Iteration 41/1000 | Loss: 0.00001561
Iteration 42/1000 | Loss: 0.00001561
Iteration 43/1000 | Loss: 0.00001561
Iteration 44/1000 | Loss: 0.00001560
Iteration 45/1000 | Loss: 0.00001560
Iteration 46/1000 | Loss: 0.00001559
Iteration 47/1000 | Loss: 0.00001559
Iteration 48/1000 | Loss: 0.00001558
Iteration 49/1000 | Loss: 0.00001558
Iteration 50/1000 | Loss: 0.00001558
Iteration 51/1000 | Loss: 0.00001557
Iteration 52/1000 | Loss: 0.00001557
Iteration 53/1000 | Loss: 0.00001557
Iteration 54/1000 | Loss: 0.00001556
Iteration 55/1000 | Loss: 0.00001556
Iteration 56/1000 | Loss: 0.00001555
Iteration 57/1000 | Loss: 0.00001554
Iteration 58/1000 | Loss: 0.00001554
Iteration 59/1000 | Loss: 0.00001552
Iteration 60/1000 | Loss: 0.00001552
Iteration 61/1000 | Loss: 0.00001549
Iteration 62/1000 | Loss: 0.00001548
Iteration 63/1000 | Loss: 0.00001548
Iteration 64/1000 | Loss: 0.00001547
Iteration 65/1000 | Loss: 0.00001547
Iteration 66/1000 | Loss: 0.00001546
Iteration 67/1000 | Loss: 0.00001546
Iteration 68/1000 | Loss: 0.00001546
Iteration 69/1000 | Loss: 0.00001545
Iteration 70/1000 | Loss: 0.00001545
Iteration 71/1000 | Loss: 0.00001545
Iteration 72/1000 | Loss: 0.00001545
Iteration 73/1000 | Loss: 0.00001545
Iteration 74/1000 | Loss: 0.00001544
Iteration 75/1000 | Loss: 0.00001544
Iteration 76/1000 | Loss: 0.00001544
Iteration 77/1000 | Loss: 0.00001544
Iteration 78/1000 | Loss: 0.00001544
Iteration 79/1000 | Loss: 0.00001544
Iteration 80/1000 | Loss: 0.00001544
Iteration 81/1000 | Loss: 0.00001544
Iteration 82/1000 | Loss: 0.00001544
Iteration 83/1000 | Loss: 0.00001544
Iteration 84/1000 | Loss: 0.00001543
Iteration 85/1000 | Loss: 0.00001543
Iteration 86/1000 | Loss: 0.00001543
Iteration 87/1000 | Loss: 0.00001543
Iteration 88/1000 | Loss: 0.00001543
Iteration 89/1000 | Loss: 0.00001543
Iteration 90/1000 | Loss: 0.00001543
Iteration 91/1000 | Loss: 0.00001543
Iteration 92/1000 | Loss: 0.00001543
Iteration 93/1000 | Loss: 0.00001543
Iteration 94/1000 | Loss: 0.00001543
Iteration 95/1000 | Loss: 0.00001543
Iteration 96/1000 | Loss: 0.00001543
Iteration 97/1000 | Loss: 0.00001543
Iteration 98/1000 | Loss: 0.00001542
Iteration 99/1000 | Loss: 0.00001542
Iteration 100/1000 | Loss: 0.00001542
Iteration 101/1000 | Loss: 0.00001542
Iteration 102/1000 | Loss: 0.00001541
Iteration 103/1000 | Loss: 0.00001541
Iteration 104/1000 | Loss: 0.00001541
Iteration 105/1000 | Loss: 0.00001541
Iteration 106/1000 | Loss: 0.00001541
Iteration 107/1000 | Loss: 0.00001541
Iteration 108/1000 | Loss: 0.00001541
Iteration 109/1000 | Loss: 0.00001541
Iteration 110/1000 | Loss: 0.00001541
Iteration 111/1000 | Loss: 0.00001541
Iteration 112/1000 | Loss: 0.00001541
Iteration 113/1000 | Loss: 0.00001541
Iteration 114/1000 | Loss: 0.00001541
Iteration 115/1000 | Loss: 0.00001541
Iteration 116/1000 | Loss: 0.00001541
Iteration 117/1000 | Loss: 0.00001541
Iteration 118/1000 | Loss: 0.00001541
Iteration 119/1000 | Loss: 0.00001541
Iteration 120/1000 | Loss: 0.00001541
Iteration 121/1000 | Loss: 0.00001541
Iteration 122/1000 | Loss: 0.00001541
Iteration 123/1000 | Loss: 0.00001541
Iteration 124/1000 | Loss: 0.00001541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.5408508261316456e-05, 1.5408508261316456e-05, 1.5408508261316456e-05, 1.5408508261316456e-05, 1.5408508261316456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5408508261316456e-05

Optimization complete. Final v2v error: 3.3465096950531006 mm

Highest mean error: 3.697157382965088 mm for frame 32

Lowest mean error: 3.081604480743408 mm for frame 12

Saving results

Total time: 33.463929176330566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020124
Iteration 2/25 | Loss: 0.00212754
Iteration 3/25 | Loss: 0.00148402
Iteration 4/25 | Loss: 0.00131720
Iteration 5/25 | Loss: 0.00124798
Iteration 6/25 | Loss: 0.00129272
Iteration 7/25 | Loss: 0.00123976
Iteration 8/25 | Loss: 0.00117118
Iteration 9/25 | Loss: 0.00113402
Iteration 10/25 | Loss: 0.00112292
Iteration 11/25 | Loss: 0.00112545
Iteration 12/25 | Loss: 0.00111264
Iteration 13/25 | Loss: 0.00110155
Iteration 14/25 | Loss: 0.00109310
Iteration 15/25 | Loss: 0.00109043
Iteration 16/25 | Loss: 0.00108990
Iteration 17/25 | Loss: 0.00108956
Iteration 18/25 | Loss: 0.00108922
Iteration 19/25 | Loss: 0.00108826
Iteration 20/25 | Loss: 0.00108675
Iteration 21/25 | Loss: 0.00109249
Iteration 22/25 | Loss: 0.00108341
Iteration 23/25 | Loss: 0.00108060
Iteration 24/25 | Loss: 0.00107976
Iteration 25/25 | Loss: 0.00108575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43130267
Iteration 2/25 | Loss: 0.00317820
Iteration 3/25 | Loss: 0.00317819
Iteration 4/25 | Loss: 0.00317819
Iteration 5/25 | Loss: 0.00317819
Iteration 6/25 | Loss: 0.00317819
Iteration 7/25 | Loss: 0.00317819
Iteration 8/25 | Loss: 0.00317819
Iteration 9/25 | Loss: 0.00317819
Iteration 10/25 | Loss: 0.00317819
Iteration 11/25 | Loss: 0.00317819
Iteration 12/25 | Loss: 0.00317819
Iteration 13/25 | Loss: 0.00317819
Iteration 14/25 | Loss: 0.00317819
Iteration 15/25 | Loss: 0.00317819
Iteration 16/25 | Loss: 0.00317819
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003178190905600786, 0.003178190905600786, 0.003178190905600786, 0.003178190905600786, 0.003178190905600786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003178190905600786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00317819
Iteration 2/1000 | Loss: 0.00052118
Iteration 3/1000 | Loss: 0.00038307
Iteration 4/1000 | Loss: 0.00031993
Iteration 5/1000 | Loss: 0.00027810
Iteration 6/1000 | Loss: 0.00025713
Iteration 7/1000 | Loss: 0.00023998
Iteration 8/1000 | Loss: 0.00022756
Iteration 9/1000 | Loss: 0.00041873
Iteration 10/1000 | Loss: 0.00078808
Iteration 11/1000 | Loss: 0.00087884
Iteration 12/1000 | Loss: 0.00162131
Iteration 13/1000 | Loss: 0.00468280
Iteration 14/1000 | Loss: 0.00799980
Iteration 15/1000 | Loss: 0.00240831
Iteration 16/1000 | Loss: 0.00048814
Iteration 17/1000 | Loss: 0.00032121
Iteration 18/1000 | Loss: 0.00048728
Iteration 19/1000 | Loss: 0.00027676
Iteration 20/1000 | Loss: 0.00054336
Iteration 21/1000 | Loss: 0.00040075
Iteration 22/1000 | Loss: 0.00009682
Iteration 23/1000 | Loss: 0.00053872
Iteration 24/1000 | Loss: 0.00006944
Iteration 25/1000 | Loss: 0.00005893
Iteration 26/1000 | Loss: 0.00004878
Iteration 27/1000 | Loss: 0.00004250
Iteration 28/1000 | Loss: 0.00028661
Iteration 29/1000 | Loss: 0.00004431
Iteration 30/1000 | Loss: 0.00003793
Iteration 31/1000 | Loss: 0.00003302
Iteration 32/1000 | Loss: 0.00003026
Iteration 33/1000 | Loss: 0.00002810
Iteration 34/1000 | Loss: 0.00002559
Iteration 35/1000 | Loss: 0.00053349
Iteration 36/1000 | Loss: 0.00004854
Iteration 37/1000 | Loss: 0.00003523
Iteration 38/1000 | Loss: 0.00003078
Iteration 39/1000 | Loss: 0.00002832
Iteration 40/1000 | Loss: 0.00002372
Iteration 41/1000 | Loss: 0.00002062
Iteration 42/1000 | Loss: 0.00001901
Iteration 43/1000 | Loss: 0.00001815
Iteration 44/1000 | Loss: 0.00001762
Iteration 45/1000 | Loss: 0.00001718
Iteration 46/1000 | Loss: 0.00001691
Iteration 47/1000 | Loss: 0.00001669
Iteration 48/1000 | Loss: 0.00001653
Iteration 49/1000 | Loss: 0.00001643
Iteration 50/1000 | Loss: 0.00001625
Iteration 51/1000 | Loss: 0.00001619
Iteration 52/1000 | Loss: 0.00001615
Iteration 53/1000 | Loss: 0.00046331
Iteration 54/1000 | Loss: 0.00003191
Iteration 55/1000 | Loss: 0.00002115
Iteration 56/1000 | Loss: 0.00001941
Iteration 57/1000 | Loss: 0.00001847
Iteration 58/1000 | Loss: 0.00001780
Iteration 59/1000 | Loss: 0.00001741
Iteration 60/1000 | Loss: 0.00043190
Iteration 61/1000 | Loss: 0.00031043
Iteration 62/1000 | Loss: 0.00031507
Iteration 63/1000 | Loss: 0.00003468
Iteration 64/1000 | Loss: 0.00002383
Iteration 65/1000 | Loss: 0.00002101
Iteration 66/1000 | Loss: 0.00001967
Iteration 67/1000 | Loss: 0.00001900
Iteration 68/1000 | Loss: 0.00039590
Iteration 69/1000 | Loss: 0.00011646
Iteration 70/1000 | Loss: 0.00018995
Iteration 71/1000 | Loss: 0.00003446
Iteration 72/1000 | Loss: 0.00002299
Iteration 73/1000 | Loss: 0.00002044
Iteration 74/1000 | Loss: 0.00001909
Iteration 75/1000 | Loss: 0.00012497
Iteration 76/1000 | Loss: 0.00001861
Iteration 77/1000 | Loss: 0.00001678
Iteration 78/1000 | Loss: 0.00001570
Iteration 79/1000 | Loss: 0.00001529
Iteration 80/1000 | Loss: 0.00001510
Iteration 81/1000 | Loss: 0.00001509
Iteration 82/1000 | Loss: 0.00001509
Iteration 83/1000 | Loss: 0.00001498
Iteration 84/1000 | Loss: 0.00001497
Iteration 85/1000 | Loss: 0.00001497
Iteration 86/1000 | Loss: 0.00001497
Iteration 87/1000 | Loss: 0.00001496
Iteration 88/1000 | Loss: 0.00001496
Iteration 89/1000 | Loss: 0.00001495
Iteration 90/1000 | Loss: 0.00001494
Iteration 91/1000 | Loss: 0.00001494
Iteration 92/1000 | Loss: 0.00001494
Iteration 93/1000 | Loss: 0.00001494
Iteration 94/1000 | Loss: 0.00001493
Iteration 95/1000 | Loss: 0.00001492
Iteration 96/1000 | Loss: 0.00001492
Iteration 97/1000 | Loss: 0.00001489
Iteration 98/1000 | Loss: 0.00001488
Iteration 99/1000 | Loss: 0.00001488
Iteration 100/1000 | Loss: 0.00001488
Iteration 101/1000 | Loss: 0.00001488
Iteration 102/1000 | Loss: 0.00001488
Iteration 103/1000 | Loss: 0.00001488
Iteration 104/1000 | Loss: 0.00001488
Iteration 105/1000 | Loss: 0.00001488
Iteration 106/1000 | Loss: 0.00001488
Iteration 107/1000 | Loss: 0.00001488
Iteration 108/1000 | Loss: 0.00001488
Iteration 109/1000 | Loss: 0.00001488
Iteration 110/1000 | Loss: 0.00001487
Iteration 111/1000 | Loss: 0.00001487
Iteration 112/1000 | Loss: 0.00001487
Iteration 113/1000 | Loss: 0.00001487
Iteration 114/1000 | Loss: 0.00001487
Iteration 115/1000 | Loss: 0.00001487
Iteration 116/1000 | Loss: 0.00001487
Iteration 117/1000 | Loss: 0.00001487
Iteration 118/1000 | Loss: 0.00001487
Iteration 119/1000 | Loss: 0.00001487
Iteration 120/1000 | Loss: 0.00001487
Iteration 121/1000 | Loss: 0.00001487
Iteration 122/1000 | Loss: 0.00001487
Iteration 123/1000 | Loss: 0.00001487
Iteration 124/1000 | Loss: 0.00001487
Iteration 125/1000 | Loss: 0.00001487
Iteration 126/1000 | Loss: 0.00001487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.4873960026307032e-05, 1.4873960026307032e-05, 1.4873960026307032e-05, 1.4873960026307032e-05, 1.4873960026307032e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4873960026307032e-05

Optimization complete. Final v2v error: 3.224233865737915 mm

Highest mean error: 4.879875659942627 mm for frame 53

Lowest mean error: 3.039820432662964 mm for frame 55

Saving results

Total time: 152.19325184822083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829225
Iteration 2/25 | Loss: 0.00075388
Iteration 3/25 | Loss: 0.00061989
Iteration 4/25 | Loss: 0.00059696
Iteration 5/25 | Loss: 0.00059309
Iteration 6/25 | Loss: 0.00059228
Iteration 7/25 | Loss: 0.00059223
Iteration 8/25 | Loss: 0.00059223
Iteration 9/25 | Loss: 0.00059223
Iteration 10/25 | Loss: 0.00059223
Iteration 11/25 | Loss: 0.00059223
Iteration 12/25 | Loss: 0.00059223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005922314012423158, 0.0005922314012423158, 0.0005922314012423158, 0.0005922314012423158, 0.0005922314012423158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005922314012423158

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46294749
Iteration 2/25 | Loss: 0.00027842
Iteration 3/25 | Loss: 0.00027842
Iteration 4/25 | Loss: 0.00027842
Iteration 5/25 | Loss: 0.00027842
Iteration 6/25 | Loss: 0.00027842
Iteration 7/25 | Loss: 0.00027842
Iteration 8/25 | Loss: 0.00027842
Iteration 9/25 | Loss: 0.00027842
Iteration 10/25 | Loss: 0.00027842
Iteration 11/25 | Loss: 0.00027842
Iteration 12/25 | Loss: 0.00027842
Iteration 13/25 | Loss: 0.00027842
Iteration 14/25 | Loss: 0.00027842
Iteration 15/25 | Loss: 0.00027842
Iteration 16/25 | Loss: 0.00027842
Iteration 17/25 | Loss: 0.00027842
Iteration 18/25 | Loss: 0.00027842
Iteration 19/25 | Loss: 0.00027842
Iteration 20/25 | Loss: 0.00027842
Iteration 21/25 | Loss: 0.00027842
Iteration 22/25 | Loss: 0.00027842
Iteration 23/25 | Loss: 0.00027842
Iteration 24/25 | Loss: 0.00027842
Iteration 25/25 | Loss: 0.00027842

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027842
Iteration 2/1000 | Loss: 0.00001791
Iteration 3/1000 | Loss: 0.00001218
Iteration 4/1000 | Loss: 0.00001142
Iteration 5/1000 | Loss: 0.00001083
Iteration 6/1000 | Loss: 0.00001073
Iteration 7/1000 | Loss: 0.00001045
Iteration 8/1000 | Loss: 0.00001035
Iteration 9/1000 | Loss: 0.00001034
Iteration 10/1000 | Loss: 0.00001033
Iteration 11/1000 | Loss: 0.00001033
Iteration 12/1000 | Loss: 0.00001033
Iteration 13/1000 | Loss: 0.00001032
Iteration 14/1000 | Loss: 0.00001032
Iteration 15/1000 | Loss: 0.00001032
Iteration 16/1000 | Loss: 0.00001031
Iteration 17/1000 | Loss: 0.00001029
Iteration 18/1000 | Loss: 0.00001025
Iteration 19/1000 | Loss: 0.00001022
Iteration 20/1000 | Loss: 0.00001021
Iteration 21/1000 | Loss: 0.00001021
Iteration 22/1000 | Loss: 0.00001020
Iteration 23/1000 | Loss: 0.00001020
Iteration 24/1000 | Loss: 0.00001019
Iteration 25/1000 | Loss: 0.00001019
Iteration 26/1000 | Loss: 0.00001019
Iteration 27/1000 | Loss: 0.00001019
Iteration 28/1000 | Loss: 0.00001018
Iteration 29/1000 | Loss: 0.00001018
Iteration 30/1000 | Loss: 0.00001017
Iteration 31/1000 | Loss: 0.00001016
Iteration 32/1000 | Loss: 0.00001016
Iteration 33/1000 | Loss: 0.00001016
Iteration 34/1000 | Loss: 0.00001012
Iteration 35/1000 | Loss: 0.00001009
Iteration 36/1000 | Loss: 0.00001008
Iteration 37/1000 | Loss: 0.00001007
Iteration 38/1000 | Loss: 0.00001007
Iteration 39/1000 | Loss: 0.00001007
Iteration 40/1000 | Loss: 0.00001007
Iteration 41/1000 | Loss: 0.00001007
Iteration 42/1000 | Loss: 0.00001007
Iteration 43/1000 | Loss: 0.00001007
Iteration 44/1000 | Loss: 0.00001007
Iteration 45/1000 | Loss: 0.00001007
Iteration 46/1000 | Loss: 0.00001007
Iteration 47/1000 | Loss: 0.00001007
Iteration 48/1000 | Loss: 0.00001006
Iteration 49/1000 | Loss: 0.00001006
Iteration 50/1000 | Loss: 0.00001006
Iteration 51/1000 | Loss: 0.00001006
Iteration 52/1000 | Loss: 0.00001006
Iteration 53/1000 | Loss: 0.00001005
Iteration 54/1000 | Loss: 0.00001004
Iteration 55/1000 | Loss: 0.00001004
Iteration 56/1000 | Loss: 0.00001003
Iteration 57/1000 | Loss: 0.00001003
Iteration 58/1000 | Loss: 0.00001002
Iteration 59/1000 | Loss: 0.00001001
Iteration 60/1000 | Loss: 0.00001000
Iteration 61/1000 | Loss: 0.00001000
Iteration 62/1000 | Loss: 0.00001000
Iteration 63/1000 | Loss: 0.00000999
Iteration 64/1000 | Loss: 0.00000999
Iteration 65/1000 | Loss: 0.00000998
Iteration 66/1000 | Loss: 0.00000998
Iteration 67/1000 | Loss: 0.00000998
Iteration 68/1000 | Loss: 0.00000997
Iteration 69/1000 | Loss: 0.00000997
Iteration 70/1000 | Loss: 0.00000996
Iteration 71/1000 | Loss: 0.00000996
Iteration 72/1000 | Loss: 0.00000995
Iteration 73/1000 | Loss: 0.00000995
Iteration 74/1000 | Loss: 0.00000995
Iteration 75/1000 | Loss: 0.00000995
Iteration 76/1000 | Loss: 0.00000994
Iteration 77/1000 | Loss: 0.00000994
Iteration 78/1000 | Loss: 0.00000993
Iteration 79/1000 | Loss: 0.00000991
Iteration 80/1000 | Loss: 0.00000990
Iteration 81/1000 | Loss: 0.00000990
Iteration 82/1000 | Loss: 0.00000990
Iteration 83/1000 | Loss: 0.00000990
Iteration 84/1000 | Loss: 0.00000990
Iteration 85/1000 | Loss: 0.00000990
Iteration 86/1000 | Loss: 0.00000989
Iteration 87/1000 | Loss: 0.00000989
Iteration 88/1000 | Loss: 0.00000989
Iteration 89/1000 | Loss: 0.00000989
Iteration 90/1000 | Loss: 0.00000988
Iteration 91/1000 | Loss: 0.00000988
Iteration 92/1000 | Loss: 0.00000988
Iteration 93/1000 | Loss: 0.00000988
Iteration 94/1000 | Loss: 0.00000987
Iteration 95/1000 | Loss: 0.00000987
Iteration 96/1000 | Loss: 0.00000987
Iteration 97/1000 | Loss: 0.00000987
Iteration 98/1000 | Loss: 0.00000987
Iteration 99/1000 | Loss: 0.00000987
Iteration 100/1000 | Loss: 0.00000987
Iteration 101/1000 | Loss: 0.00000987
Iteration 102/1000 | Loss: 0.00000987
Iteration 103/1000 | Loss: 0.00000986
Iteration 104/1000 | Loss: 0.00000986
Iteration 105/1000 | Loss: 0.00000986
Iteration 106/1000 | Loss: 0.00000985
Iteration 107/1000 | Loss: 0.00000985
Iteration 108/1000 | Loss: 0.00000985
Iteration 109/1000 | Loss: 0.00000985
Iteration 110/1000 | Loss: 0.00000985
Iteration 111/1000 | Loss: 0.00000985
Iteration 112/1000 | Loss: 0.00000985
Iteration 113/1000 | Loss: 0.00000985
Iteration 114/1000 | Loss: 0.00000985
Iteration 115/1000 | Loss: 0.00000985
Iteration 116/1000 | Loss: 0.00000985
Iteration 117/1000 | Loss: 0.00000985
Iteration 118/1000 | Loss: 0.00000984
Iteration 119/1000 | Loss: 0.00000984
Iteration 120/1000 | Loss: 0.00000984
Iteration 121/1000 | Loss: 0.00000984
Iteration 122/1000 | Loss: 0.00000983
Iteration 123/1000 | Loss: 0.00000983
Iteration 124/1000 | Loss: 0.00000983
Iteration 125/1000 | Loss: 0.00000983
Iteration 126/1000 | Loss: 0.00000982
Iteration 127/1000 | Loss: 0.00000982
Iteration 128/1000 | Loss: 0.00000982
Iteration 129/1000 | Loss: 0.00000982
Iteration 130/1000 | Loss: 0.00000982
Iteration 131/1000 | Loss: 0.00000982
Iteration 132/1000 | Loss: 0.00000982
Iteration 133/1000 | Loss: 0.00000982
Iteration 134/1000 | Loss: 0.00000982
Iteration 135/1000 | Loss: 0.00000982
Iteration 136/1000 | Loss: 0.00000981
Iteration 137/1000 | Loss: 0.00000981
Iteration 138/1000 | Loss: 0.00000981
Iteration 139/1000 | Loss: 0.00000981
Iteration 140/1000 | Loss: 0.00000981
Iteration 141/1000 | Loss: 0.00000981
Iteration 142/1000 | Loss: 0.00000981
Iteration 143/1000 | Loss: 0.00000981
Iteration 144/1000 | Loss: 0.00000981
Iteration 145/1000 | Loss: 0.00000981
Iteration 146/1000 | Loss: 0.00000980
Iteration 147/1000 | Loss: 0.00000980
Iteration 148/1000 | Loss: 0.00000980
Iteration 149/1000 | Loss: 0.00000980
Iteration 150/1000 | Loss: 0.00000980
Iteration 151/1000 | Loss: 0.00000980
Iteration 152/1000 | Loss: 0.00000979
Iteration 153/1000 | Loss: 0.00000979
Iteration 154/1000 | Loss: 0.00000979
Iteration 155/1000 | Loss: 0.00000979
Iteration 156/1000 | Loss: 0.00000979
Iteration 157/1000 | Loss: 0.00000979
Iteration 158/1000 | Loss: 0.00000979
Iteration 159/1000 | Loss: 0.00000978
Iteration 160/1000 | Loss: 0.00000978
Iteration 161/1000 | Loss: 0.00000978
Iteration 162/1000 | Loss: 0.00000978
Iteration 163/1000 | Loss: 0.00000978
Iteration 164/1000 | Loss: 0.00000978
Iteration 165/1000 | Loss: 0.00000978
Iteration 166/1000 | Loss: 0.00000978
Iteration 167/1000 | Loss: 0.00000978
Iteration 168/1000 | Loss: 0.00000977
Iteration 169/1000 | Loss: 0.00000977
Iteration 170/1000 | Loss: 0.00000977
Iteration 171/1000 | Loss: 0.00000977
Iteration 172/1000 | Loss: 0.00000977
Iteration 173/1000 | Loss: 0.00000976
Iteration 174/1000 | Loss: 0.00000976
Iteration 175/1000 | Loss: 0.00000976
Iteration 176/1000 | Loss: 0.00000976
Iteration 177/1000 | Loss: 0.00000976
Iteration 178/1000 | Loss: 0.00000976
Iteration 179/1000 | Loss: 0.00000976
Iteration 180/1000 | Loss: 0.00000976
Iteration 181/1000 | Loss: 0.00000976
Iteration 182/1000 | Loss: 0.00000976
Iteration 183/1000 | Loss: 0.00000976
Iteration 184/1000 | Loss: 0.00000976
Iteration 185/1000 | Loss: 0.00000976
Iteration 186/1000 | Loss: 0.00000976
Iteration 187/1000 | Loss: 0.00000976
Iteration 188/1000 | Loss: 0.00000976
Iteration 189/1000 | Loss: 0.00000976
Iteration 190/1000 | Loss: 0.00000976
Iteration 191/1000 | Loss: 0.00000976
Iteration 192/1000 | Loss: 0.00000976
Iteration 193/1000 | Loss: 0.00000976
Iteration 194/1000 | Loss: 0.00000975
Iteration 195/1000 | Loss: 0.00000975
Iteration 196/1000 | Loss: 0.00000975
Iteration 197/1000 | Loss: 0.00000975
Iteration 198/1000 | Loss: 0.00000975
Iteration 199/1000 | Loss: 0.00000975
Iteration 200/1000 | Loss: 0.00000975
Iteration 201/1000 | Loss: 0.00000975
Iteration 202/1000 | Loss: 0.00000975
Iteration 203/1000 | Loss: 0.00000975
Iteration 204/1000 | Loss: 0.00000975
Iteration 205/1000 | Loss: 0.00000975
Iteration 206/1000 | Loss: 0.00000975
Iteration 207/1000 | Loss: 0.00000975
Iteration 208/1000 | Loss: 0.00000975
Iteration 209/1000 | Loss: 0.00000975
Iteration 210/1000 | Loss: 0.00000975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [9.748247975949198e-06, 9.748247975949198e-06, 9.748247975949198e-06, 9.748247975949198e-06, 9.748247975949198e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.748247975949198e-06

Optimization complete. Final v2v error: 2.6275315284729004 mm

Highest mean error: 2.791198968887329 mm for frame 31

Lowest mean error: 2.5256073474884033 mm for frame 127

Saving results

Total time: 35.39253282546997
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393771
Iteration 2/25 | Loss: 0.00083797
Iteration 3/25 | Loss: 0.00066519
Iteration 4/25 | Loss: 0.00062810
Iteration 5/25 | Loss: 0.00061938
Iteration 6/25 | Loss: 0.00061598
Iteration 7/25 | Loss: 0.00061532
Iteration 8/25 | Loss: 0.00061532
Iteration 9/25 | Loss: 0.00061532
Iteration 10/25 | Loss: 0.00061532
Iteration 11/25 | Loss: 0.00061532
Iteration 12/25 | Loss: 0.00061532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006153184222057462, 0.0006153184222057462, 0.0006153184222057462, 0.0006153184222057462, 0.0006153184222057462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006153184222057462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45093024
Iteration 2/25 | Loss: 0.00025216
Iteration 3/25 | Loss: 0.00025215
Iteration 4/25 | Loss: 0.00025215
Iteration 5/25 | Loss: 0.00025215
Iteration 6/25 | Loss: 0.00025215
Iteration 7/25 | Loss: 0.00025215
Iteration 8/25 | Loss: 0.00025215
Iteration 9/25 | Loss: 0.00025215
Iteration 10/25 | Loss: 0.00025215
Iteration 11/25 | Loss: 0.00025215
Iteration 12/25 | Loss: 0.00025215
Iteration 13/25 | Loss: 0.00025215
Iteration 14/25 | Loss: 0.00025215
Iteration 15/25 | Loss: 0.00025215
Iteration 16/25 | Loss: 0.00025215
Iteration 17/25 | Loss: 0.00025215
Iteration 18/25 | Loss: 0.00025215
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00025215005734935403, 0.00025215005734935403, 0.00025215005734935403, 0.00025215005734935403, 0.00025215005734935403]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025215005734935403

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025215
Iteration 2/1000 | Loss: 0.00002975
Iteration 3/1000 | Loss: 0.00002222
Iteration 4/1000 | Loss: 0.00001882
Iteration 5/1000 | Loss: 0.00001742
Iteration 6/1000 | Loss: 0.00001650
Iteration 7/1000 | Loss: 0.00001604
Iteration 8/1000 | Loss: 0.00001564
Iteration 9/1000 | Loss: 0.00001556
Iteration 10/1000 | Loss: 0.00001541
Iteration 11/1000 | Loss: 0.00001514
Iteration 12/1000 | Loss: 0.00001507
Iteration 13/1000 | Loss: 0.00001497
Iteration 14/1000 | Loss: 0.00001489
Iteration 15/1000 | Loss: 0.00001486
Iteration 16/1000 | Loss: 0.00001482
Iteration 17/1000 | Loss: 0.00001476
Iteration 18/1000 | Loss: 0.00001476
Iteration 19/1000 | Loss: 0.00001472
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00001471
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001470
Iteration 24/1000 | Loss: 0.00001469
Iteration 25/1000 | Loss: 0.00001468
Iteration 26/1000 | Loss: 0.00001468
Iteration 27/1000 | Loss: 0.00001468
Iteration 28/1000 | Loss: 0.00001467
Iteration 29/1000 | Loss: 0.00001467
Iteration 30/1000 | Loss: 0.00001467
Iteration 31/1000 | Loss: 0.00001467
Iteration 32/1000 | Loss: 0.00001466
Iteration 33/1000 | Loss: 0.00001466
Iteration 34/1000 | Loss: 0.00001466
Iteration 35/1000 | Loss: 0.00001466
Iteration 36/1000 | Loss: 0.00001465
Iteration 37/1000 | Loss: 0.00001465
Iteration 38/1000 | Loss: 0.00001465
Iteration 39/1000 | Loss: 0.00001464
Iteration 40/1000 | Loss: 0.00001464
Iteration 41/1000 | Loss: 0.00001464
Iteration 42/1000 | Loss: 0.00001464
Iteration 43/1000 | Loss: 0.00001464
Iteration 44/1000 | Loss: 0.00001464
Iteration 45/1000 | Loss: 0.00001463
Iteration 46/1000 | Loss: 0.00001463
Iteration 47/1000 | Loss: 0.00001461
Iteration 48/1000 | Loss: 0.00001460
Iteration 49/1000 | Loss: 0.00001459
Iteration 50/1000 | Loss: 0.00001459
Iteration 51/1000 | Loss: 0.00001458
Iteration 52/1000 | Loss: 0.00001458
Iteration 53/1000 | Loss: 0.00001458
Iteration 54/1000 | Loss: 0.00001457
Iteration 55/1000 | Loss: 0.00001457
Iteration 56/1000 | Loss: 0.00001457
Iteration 57/1000 | Loss: 0.00001457
Iteration 58/1000 | Loss: 0.00001457
Iteration 59/1000 | Loss: 0.00001456
Iteration 60/1000 | Loss: 0.00001456
Iteration 61/1000 | Loss: 0.00001456
Iteration 62/1000 | Loss: 0.00001456
Iteration 63/1000 | Loss: 0.00001456
Iteration 64/1000 | Loss: 0.00001456
Iteration 65/1000 | Loss: 0.00001454
Iteration 66/1000 | Loss: 0.00001454
Iteration 67/1000 | Loss: 0.00001454
Iteration 68/1000 | Loss: 0.00001454
Iteration 69/1000 | Loss: 0.00001454
Iteration 70/1000 | Loss: 0.00001454
Iteration 71/1000 | Loss: 0.00001454
Iteration 72/1000 | Loss: 0.00001453
Iteration 73/1000 | Loss: 0.00001453
Iteration 74/1000 | Loss: 0.00001453
Iteration 75/1000 | Loss: 0.00001453
Iteration 76/1000 | Loss: 0.00001453
Iteration 77/1000 | Loss: 0.00001453
Iteration 78/1000 | Loss: 0.00001453
Iteration 79/1000 | Loss: 0.00001452
Iteration 80/1000 | Loss: 0.00001452
Iteration 81/1000 | Loss: 0.00001452
Iteration 82/1000 | Loss: 0.00001452
Iteration 83/1000 | Loss: 0.00001452
Iteration 84/1000 | Loss: 0.00001452
Iteration 85/1000 | Loss: 0.00001452
Iteration 86/1000 | Loss: 0.00001452
Iteration 87/1000 | Loss: 0.00001451
Iteration 88/1000 | Loss: 0.00001451
Iteration 89/1000 | Loss: 0.00001451
Iteration 90/1000 | Loss: 0.00001451
Iteration 91/1000 | Loss: 0.00001451
Iteration 92/1000 | Loss: 0.00001451
Iteration 93/1000 | Loss: 0.00001450
Iteration 94/1000 | Loss: 0.00001450
Iteration 95/1000 | Loss: 0.00001450
Iteration 96/1000 | Loss: 0.00001450
Iteration 97/1000 | Loss: 0.00001449
Iteration 98/1000 | Loss: 0.00001449
Iteration 99/1000 | Loss: 0.00001449
Iteration 100/1000 | Loss: 0.00001449
Iteration 101/1000 | Loss: 0.00001449
Iteration 102/1000 | Loss: 0.00001449
Iteration 103/1000 | Loss: 0.00001449
Iteration 104/1000 | Loss: 0.00001448
Iteration 105/1000 | Loss: 0.00001448
Iteration 106/1000 | Loss: 0.00001448
Iteration 107/1000 | Loss: 0.00001448
Iteration 108/1000 | Loss: 0.00001448
Iteration 109/1000 | Loss: 0.00001448
Iteration 110/1000 | Loss: 0.00001448
Iteration 111/1000 | Loss: 0.00001448
Iteration 112/1000 | Loss: 0.00001448
Iteration 113/1000 | Loss: 0.00001447
Iteration 114/1000 | Loss: 0.00001447
Iteration 115/1000 | Loss: 0.00001447
Iteration 116/1000 | Loss: 0.00001447
Iteration 117/1000 | Loss: 0.00001447
Iteration 118/1000 | Loss: 0.00001447
Iteration 119/1000 | Loss: 0.00001447
Iteration 120/1000 | Loss: 0.00001446
Iteration 121/1000 | Loss: 0.00001446
Iteration 122/1000 | Loss: 0.00001446
Iteration 123/1000 | Loss: 0.00001446
Iteration 124/1000 | Loss: 0.00001446
Iteration 125/1000 | Loss: 0.00001446
Iteration 126/1000 | Loss: 0.00001446
Iteration 127/1000 | Loss: 0.00001445
Iteration 128/1000 | Loss: 0.00001445
Iteration 129/1000 | Loss: 0.00001445
Iteration 130/1000 | Loss: 0.00001445
Iteration 131/1000 | Loss: 0.00001445
Iteration 132/1000 | Loss: 0.00001445
Iteration 133/1000 | Loss: 0.00001445
Iteration 134/1000 | Loss: 0.00001445
Iteration 135/1000 | Loss: 0.00001445
Iteration 136/1000 | Loss: 0.00001445
Iteration 137/1000 | Loss: 0.00001445
Iteration 138/1000 | Loss: 0.00001445
Iteration 139/1000 | Loss: 0.00001445
Iteration 140/1000 | Loss: 0.00001445
Iteration 141/1000 | Loss: 0.00001445
Iteration 142/1000 | Loss: 0.00001445
Iteration 143/1000 | Loss: 0.00001445
Iteration 144/1000 | Loss: 0.00001445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.4453898074862082e-05, 1.4453898074862082e-05, 1.4453898074862082e-05, 1.4453898074862082e-05, 1.4453898074862082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4453898074862082e-05

Optimization complete. Final v2v error: 3.174143075942993 mm

Highest mean error: 3.5607752799987793 mm for frame 107

Lowest mean error: 2.870988368988037 mm for frame 17

Saving results

Total time: 38.08360481262207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502125
Iteration 2/25 | Loss: 0.00104797
Iteration 3/25 | Loss: 0.00070686
Iteration 4/25 | Loss: 0.00064329
Iteration 5/25 | Loss: 0.00062873
Iteration 6/25 | Loss: 0.00062576
Iteration 7/25 | Loss: 0.00062438
Iteration 8/25 | Loss: 0.00062428
Iteration 9/25 | Loss: 0.00062428
Iteration 10/25 | Loss: 0.00062428
Iteration 11/25 | Loss: 0.00062428
Iteration 12/25 | Loss: 0.00062428
Iteration 13/25 | Loss: 0.00062428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006242769304662943, 0.0006242769304662943, 0.0006242769304662943, 0.0006242769304662943, 0.0006242769304662943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006242769304662943

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47340751
Iteration 2/25 | Loss: 0.00027050
Iteration 3/25 | Loss: 0.00027050
Iteration 4/25 | Loss: 0.00027050
Iteration 5/25 | Loss: 0.00027050
Iteration 6/25 | Loss: 0.00027050
Iteration 7/25 | Loss: 0.00027050
Iteration 8/25 | Loss: 0.00027049
Iteration 9/25 | Loss: 0.00027049
Iteration 10/25 | Loss: 0.00027049
Iteration 11/25 | Loss: 0.00027049
Iteration 12/25 | Loss: 0.00027049
Iteration 13/25 | Loss: 0.00027049
Iteration 14/25 | Loss: 0.00027049
Iteration 15/25 | Loss: 0.00027049
Iteration 16/25 | Loss: 0.00027049
Iteration 17/25 | Loss: 0.00027049
Iteration 18/25 | Loss: 0.00027049
Iteration 19/25 | Loss: 0.00027049
Iteration 20/25 | Loss: 0.00027049
Iteration 21/25 | Loss: 0.00027049
Iteration 22/25 | Loss: 0.00027049
Iteration 23/25 | Loss: 0.00027049
Iteration 24/25 | Loss: 0.00027049
Iteration 25/25 | Loss: 0.00027049

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027049
Iteration 2/1000 | Loss: 0.00002218
Iteration 3/1000 | Loss: 0.00001606
Iteration 4/1000 | Loss: 0.00001504
Iteration 5/1000 | Loss: 0.00001433
Iteration 6/1000 | Loss: 0.00001398
Iteration 7/1000 | Loss: 0.00001370
Iteration 8/1000 | Loss: 0.00001349
Iteration 9/1000 | Loss: 0.00001333
Iteration 10/1000 | Loss: 0.00001329
Iteration 11/1000 | Loss: 0.00001329
Iteration 12/1000 | Loss: 0.00001327
Iteration 13/1000 | Loss: 0.00001322
Iteration 14/1000 | Loss: 0.00001318
Iteration 15/1000 | Loss: 0.00001317
Iteration 16/1000 | Loss: 0.00001316
Iteration 17/1000 | Loss: 0.00001314
Iteration 18/1000 | Loss: 0.00001313
Iteration 19/1000 | Loss: 0.00001313
Iteration 20/1000 | Loss: 0.00001313
Iteration 21/1000 | Loss: 0.00001312
Iteration 22/1000 | Loss: 0.00001311
Iteration 23/1000 | Loss: 0.00001311
Iteration 24/1000 | Loss: 0.00001310
Iteration 25/1000 | Loss: 0.00001310
Iteration 26/1000 | Loss: 0.00001309
Iteration 27/1000 | Loss: 0.00001309
Iteration 28/1000 | Loss: 0.00001309
Iteration 29/1000 | Loss: 0.00001305
Iteration 30/1000 | Loss: 0.00001305
Iteration 31/1000 | Loss: 0.00001302
Iteration 32/1000 | Loss: 0.00001302
Iteration 33/1000 | Loss: 0.00001302
Iteration 34/1000 | Loss: 0.00001302
Iteration 35/1000 | Loss: 0.00001301
Iteration 36/1000 | Loss: 0.00001301
Iteration 37/1000 | Loss: 0.00001301
Iteration 38/1000 | Loss: 0.00001301
Iteration 39/1000 | Loss: 0.00001301
Iteration 40/1000 | Loss: 0.00001300
Iteration 41/1000 | Loss: 0.00001300
Iteration 42/1000 | Loss: 0.00001300
Iteration 43/1000 | Loss: 0.00001299
Iteration 44/1000 | Loss: 0.00001299
Iteration 45/1000 | Loss: 0.00001299
Iteration 46/1000 | Loss: 0.00001299
Iteration 47/1000 | Loss: 0.00001298
Iteration 48/1000 | Loss: 0.00001298
Iteration 49/1000 | Loss: 0.00001298
Iteration 50/1000 | Loss: 0.00001298
Iteration 51/1000 | Loss: 0.00001298
Iteration 52/1000 | Loss: 0.00001298
Iteration 53/1000 | Loss: 0.00001298
Iteration 54/1000 | Loss: 0.00001298
Iteration 55/1000 | Loss: 0.00001298
Iteration 56/1000 | Loss: 0.00001298
Iteration 57/1000 | Loss: 0.00001298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [1.2980879546375945e-05, 1.2980879546375945e-05, 1.2980879546375945e-05, 1.2980879546375945e-05, 1.2980879546375945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2980879546375945e-05

Optimization complete. Final v2v error: 2.846242904663086 mm

Highest mean error: 4.6859002113342285 mm for frame 84

Lowest mean error: 2.3904805183410645 mm for frame 160

Saving results

Total time: 33.232110261917114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824506
Iteration 2/25 | Loss: 0.00149420
Iteration 3/25 | Loss: 0.00095653
Iteration 4/25 | Loss: 0.00081782
Iteration 5/25 | Loss: 0.00077278
Iteration 6/25 | Loss: 0.00076038
Iteration 7/25 | Loss: 0.00073758
Iteration 8/25 | Loss: 0.00076903
Iteration 9/25 | Loss: 0.00071748
Iteration 10/25 | Loss: 0.00072867
Iteration 11/25 | Loss: 0.00070936
Iteration 12/25 | Loss: 0.00070704
Iteration 13/25 | Loss: 0.00070699
Iteration 14/25 | Loss: 0.00070719
Iteration 15/25 | Loss: 0.00070373
Iteration 16/25 | Loss: 0.00070351
Iteration 17/25 | Loss: 0.00070336
Iteration 18/25 | Loss: 0.00070334
Iteration 19/25 | Loss: 0.00070334
Iteration 20/25 | Loss: 0.00070334
Iteration 21/25 | Loss: 0.00070334
Iteration 22/25 | Loss: 0.00070334
Iteration 23/25 | Loss: 0.00070333
Iteration 24/25 | Loss: 0.00070333
Iteration 25/25 | Loss: 0.00070333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90372860
Iteration 2/25 | Loss: 0.00050869
Iteration 3/25 | Loss: 0.00030418
Iteration 4/25 | Loss: 0.00030412
Iteration 5/25 | Loss: 0.00030412
Iteration 6/25 | Loss: 0.00030412
Iteration 7/25 | Loss: 0.00030412
Iteration 8/25 | Loss: 0.00030412
Iteration 9/25 | Loss: 0.00030412
Iteration 10/25 | Loss: 0.00030412
Iteration 11/25 | Loss: 0.00030412
Iteration 12/25 | Loss: 0.00030412
Iteration 13/25 | Loss: 0.00030412
Iteration 14/25 | Loss: 0.00030412
Iteration 15/25 | Loss: 0.00030412
Iteration 16/25 | Loss: 0.00030412
Iteration 17/25 | Loss: 0.00030412
Iteration 18/25 | Loss: 0.00030412
Iteration 19/25 | Loss: 0.00030412
Iteration 20/25 | Loss: 0.00030412
Iteration 21/25 | Loss: 0.00030412
Iteration 22/25 | Loss: 0.00030412
Iteration 23/25 | Loss: 0.00030412
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00030411710031330585, 0.00030411710031330585, 0.00030411710031330585, 0.00030411710031330585, 0.00030411710031330585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030411710031330585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030412
Iteration 2/1000 | Loss: 0.00018367
Iteration 3/1000 | Loss: 0.00027662
Iteration 4/1000 | Loss: 0.00003148
Iteration 5/1000 | Loss: 0.00002834
Iteration 6/1000 | Loss: 0.00015324
Iteration 7/1000 | Loss: 0.00002677
Iteration 8/1000 | Loss: 0.00025468
Iteration 9/1000 | Loss: 0.00002555
Iteration 10/1000 | Loss: 0.00002510
Iteration 11/1000 | Loss: 0.00017369
Iteration 12/1000 | Loss: 0.00002468
Iteration 13/1000 | Loss: 0.00002427
Iteration 14/1000 | Loss: 0.00002422
Iteration 15/1000 | Loss: 0.00002406
Iteration 16/1000 | Loss: 0.00002405
Iteration 17/1000 | Loss: 0.00002398
Iteration 18/1000 | Loss: 0.00002389
Iteration 19/1000 | Loss: 0.00002386
Iteration 20/1000 | Loss: 0.00002385
Iteration 21/1000 | Loss: 0.00002385
Iteration 22/1000 | Loss: 0.00002385
Iteration 23/1000 | Loss: 0.00002384
Iteration 24/1000 | Loss: 0.00002383
Iteration 25/1000 | Loss: 0.00002381
Iteration 26/1000 | Loss: 0.00002381
Iteration 27/1000 | Loss: 0.00002380
Iteration 28/1000 | Loss: 0.00002379
Iteration 29/1000 | Loss: 0.00007224
Iteration 30/1000 | Loss: 0.00021650
Iteration 31/1000 | Loss: 0.00011165
Iteration 32/1000 | Loss: 0.00002391
Iteration 33/1000 | Loss: 0.00002374
Iteration 34/1000 | Loss: 0.00002372
Iteration 35/1000 | Loss: 0.00002372
Iteration 36/1000 | Loss: 0.00002370
Iteration 37/1000 | Loss: 0.00002369
Iteration 38/1000 | Loss: 0.00002369
Iteration 39/1000 | Loss: 0.00002369
Iteration 40/1000 | Loss: 0.00002369
Iteration 41/1000 | Loss: 0.00002369
Iteration 42/1000 | Loss: 0.00002369
Iteration 43/1000 | Loss: 0.00002369
Iteration 44/1000 | Loss: 0.00002369
Iteration 45/1000 | Loss: 0.00002368
Iteration 46/1000 | Loss: 0.00002368
Iteration 47/1000 | Loss: 0.00002368
Iteration 48/1000 | Loss: 0.00002368
Iteration 49/1000 | Loss: 0.00002367
Iteration 50/1000 | Loss: 0.00002367
Iteration 51/1000 | Loss: 0.00002366
Iteration 52/1000 | Loss: 0.00002366
Iteration 53/1000 | Loss: 0.00002365
Iteration 54/1000 | Loss: 0.00002364
Iteration 55/1000 | Loss: 0.00002364
Iteration 56/1000 | Loss: 0.00002363
Iteration 57/1000 | Loss: 0.00002363
Iteration 58/1000 | Loss: 0.00002363
Iteration 59/1000 | Loss: 0.00002363
Iteration 60/1000 | Loss: 0.00002362
Iteration 61/1000 | Loss: 0.00002362
Iteration 62/1000 | Loss: 0.00002362
Iteration 63/1000 | Loss: 0.00002362
Iteration 64/1000 | Loss: 0.00002362
Iteration 65/1000 | Loss: 0.00002361
Iteration 66/1000 | Loss: 0.00002360
Iteration 67/1000 | Loss: 0.00002360
Iteration 68/1000 | Loss: 0.00002359
Iteration 69/1000 | Loss: 0.00002359
Iteration 70/1000 | Loss: 0.00002359
Iteration 71/1000 | Loss: 0.00002359
Iteration 72/1000 | Loss: 0.00002359
Iteration 73/1000 | Loss: 0.00002359
Iteration 74/1000 | Loss: 0.00002359
Iteration 75/1000 | Loss: 0.00002359
Iteration 76/1000 | Loss: 0.00002358
Iteration 77/1000 | Loss: 0.00002358
Iteration 78/1000 | Loss: 0.00002358
Iteration 79/1000 | Loss: 0.00002358
Iteration 80/1000 | Loss: 0.00002358
Iteration 81/1000 | Loss: 0.00002358
Iteration 82/1000 | Loss: 0.00002358
Iteration 83/1000 | Loss: 0.00002358
Iteration 84/1000 | Loss: 0.00002357
Iteration 85/1000 | Loss: 0.00002357
Iteration 86/1000 | Loss: 0.00002357
Iteration 87/1000 | Loss: 0.00002357
Iteration 88/1000 | Loss: 0.00002356
Iteration 89/1000 | Loss: 0.00002356
Iteration 90/1000 | Loss: 0.00002356
Iteration 91/1000 | Loss: 0.00002356
Iteration 92/1000 | Loss: 0.00002356
Iteration 93/1000 | Loss: 0.00002356
Iteration 94/1000 | Loss: 0.00002356
Iteration 95/1000 | Loss: 0.00002356
Iteration 96/1000 | Loss: 0.00002356
Iteration 97/1000 | Loss: 0.00002356
Iteration 98/1000 | Loss: 0.00002356
Iteration 99/1000 | Loss: 0.00002356
Iteration 100/1000 | Loss: 0.00002356
Iteration 101/1000 | Loss: 0.00002356
Iteration 102/1000 | Loss: 0.00002355
Iteration 103/1000 | Loss: 0.00002355
Iteration 104/1000 | Loss: 0.00002355
Iteration 105/1000 | Loss: 0.00002355
Iteration 106/1000 | Loss: 0.00002355
Iteration 107/1000 | Loss: 0.00002355
Iteration 108/1000 | Loss: 0.00002355
Iteration 109/1000 | Loss: 0.00002355
Iteration 110/1000 | Loss: 0.00002355
Iteration 111/1000 | Loss: 0.00002355
Iteration 112/1000 | Loss: 0.00002355
Iteration 113/1000 | Loss: 0.00002355
Iteration 114/1000 | Loss: 0.00002355
Iteration 115/1000 | Loss: 0.00002355
Iteration 116/1000 | Loss: 0.00002355
Iteration 117/1000 | Loss: 0.00002355
Iteration 118/1000 | Loss: 0.00002355
Iteration 119/1000 | Loss: 0.00002355
Iteration 120/1000 | Loss: 0.00002355
Iteration 121/1000 | Loss: 0.00002355
Iteration 122/1000 | Loss: 0.00002355
Iteration 123/1000 | Loss: 0.00002355
Iteration 124/1000 | Loss: 0.00002355
Iteration 125/1000 | Loss: 0.00002355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.355247488594614e-05, 2.355247488594614e-05, 2.355247488594614e-05, 2.355247488594614e-05, 2.355247488594614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.355247488594614e-05

Optimization complete. Final v2v error: 4.036559104919434 mm

Highest mean error: 4.3552093505859375 mm for frame 73

Lowest mean error: 3.583176612854004 mm for frame 132

Saving results

Total time: 66.21708273887634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908681
Iteration 2/25 | Loss: 0.00100769
Iteration 3/25 | Loss: 0.00070774
Iteration 4/25 | Loss: 0.00066816
Iteration 5/25 | Loss: 0.00065843
Iteration 6/25 | Loss: 0.00065592
Iteration 7/25 | Loss: 0.00065536
Iteration 8/25 | Loss: 0.00065530
Iteration 9/25 | Loss: 0.00065530
Iteration 10/25 | Loss: 0.00065530
Iteration 11/25 | Loss: 0.00065530
Iteration 12/25 | Loss: 0.00065530
Iteration 13/25 | Loss: 0.00065530
Iteration 14/25 | Loss: 0.00065530
Iteration 15/25 | Loss: 0.00065530
Iteration 16/25 | Loss: 0.00065530
Iteration 17/25 | Loss: 0.00065530
Iteration 18/25 | Loss: 0.00065530
Iteration 19/25 | Loss: 0.00065530
Iteration 20/25 | Loss: 0.00065530
Iteration 21/25 | Loss: 0.00065530
Iteration 22/25 | Loss: 0.00065530
Iteration 23/25 | Loss: 0.00065530
Iteration 24/25 | Loss: 0.00065530
Iteration 25/25 | Loss: 0.00065530

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52870202
Iteration 2/25 | Loss: 0.00030630
Iteration 3/25 | Loss: 0.00030630
Iteration 4/25 | Loss: 0.00030630
Iteration 5/25 | Loss: 0.00030630
Iteration 6/25 | Loss: 0.00030630
Iteration 7/25 | Loss: 0.00030630
Iteration 8/25 | Loss: 0.00030630
Iteration 9/25 | Loss: 0.00030630
Iteration 10/25 | Loss: 0.00030629
Iteration 11/25 | Loss: 0.00030629
Iteration 12/25 | Loss: 0.00030629
Iteration 13/25 | Loss: 0.00030629
Iteration 14/25 | Loss: 0.00030629
Iteration 15/25 | Loss: 0.00030629
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00030629493994638324, 0.00030629493994638324, 0.00030629493994638324, 0.00030629493994638324, 0.00030629493994638324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030629493994638324

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030629
Iteration 2/1000 | Loss: 0.00003461
Iteration 3/1000 | Loss: 0.00002467
Iteration 4/1000 | Loss: 0.00002297
Iteration 5/1000 | Loss: 0.00002215
Iteration 6/1000 | Loss: 0.00002136
Iteration 7/1000 | Loss: 0.00002084
Iteration 8/1000 | Loss: 0.00002044
Iteration 9/1000 | Loss: 0.00002014
Iteration 10/1000 | Loss: 0.00001990
Iteration 11/1000 | Loss: 0.00001975
Iteration 12/1000 | Loss: 0.00001971
Iteration 13/1000 | Loss: 0.00001967
Iteration 14/1000 | Loss: 0.00001963
Iteration 15/1000 | Loss: 0.00001955
Iteration 16/1000 | Loss: 0.00001955
Iteration 17/1000 | Loss: 0.00001952
Iteration 18/1000 | Loss: 0.00001952
Iteration 19/1000 | Loss: 0.00001951
Iteration 20/1000 | Loss: 0.00001950
Iteration 21/1000 | Loss: 0.00001949
Iteration 22/1000 | Loss: 0.00001949
Iteration 23/1000 | Loss: 0.00001949
Iteration 24/1000 | Loss: 0.00001949
Iteration 25/1000 | Loss: 0.00001949
Iteration 26/1000 | Loss: 0.00001949
Iteration 27/1000 | Loss: 0.00001949
Iteration 28/1000 | Loss: 0.00001949
Iteration 29/1000 | Loss: 0.00001948
Iteration 30/1000 | Loss: 0.00001945
Iteration 31/1000 | Loss: 0.00001945
Iteration 32/1000 | Loss: 0.00001945
Iteration 33/1000 | Loss: 0.00001945
Iteration 34/1000 | Loss: 0.00001945
Iteration 35/1000 | Loss: 0.00001944
Iteration 36/1000 | Loss: 0.00001944
Iteration 37/1000 | Loss: 0.00001942
Iteration 38/1000 | Loss: 0.00001938
Iteration 39/1000 | Loss: 0.00001938
Iteration 40/1000 | Loss: 0.00001933
Iteration 41/1000 | Loss: 0.00001933
Iteration 42/1000 | Loss: 0.00001933
Iteration 43/1000 | Loss: 0.00001932
Iteration 44/1000 | Loss: 0.00001932
Iteration 45/1000 | Loss: 0.00001932
Iteration 46/1000 | Loss: 0.00001932
Iteration 47/1000 | Loss: 0.00001932
Iteration 48/1000 | Loss: 0.00001930
Iteration 49/1000 | Loss: 0.00001930
Iteration 50/1000 | Loss: 0.00001929
Iteration 51/1000 | Loss: 0.00001929
Iteration 52/1000 | Loss: 0.00001929
Iteration 53/1000 | Loss: 0.00001929
Iteration 54/1000 | Loss: 0.00001929
Iteration 55/1000 | Loss: 0.00001929
Iteration 56/1000 | Loss: 0.00001929
Iteration 57/1000 | Loss: 0.00001928
Iteration 58/1000 | Loss: 0.00001928
Iteration 59/1000 | Loss: 0.00001928
Iteration 60/1000 | Loss: 0.00001928
Iteration 61/1000 | Loss: 0.00001928
Iteration 62/1000 | Loss: 0.00001927
Iteration 63/1000 | Loss: 0.00001927
Iteration 64/1000 | Loss: 0.00001927
Iteration 65/1000 | Loss: 0.00001926
Iteration 66/1000 | Loss: 0.00001926
Iteration 67/1000 | Loss: 0.00001926
Iteration 68/1000 | Loss: 0.00001926
Iteration 69/1000 | Loss: 0.00001926
Iteration 70/1000 | Loss: 0.00001926
Iteration 71/1000 | Loss: 0.00001926
Iteration 72/1000 | Loss: 0.00001925
Iteration 73/1000 | Loss: 0.00001925
Iteration 74/1000 | Loss: 0.00001925
Iteration 75/1000 | Loss: 0.00001925
Iteration 76/1000 | Loss: 0.00001925
Iteration 77/1000 | Loss: 0.00001925
Iteration 78/1000 | Loss: 0.00001924
Iteration 79/1000 | Loss: 0.00001924
Iteration 80/1000 | Loss: 0.00001924
Iteration 81/1000 | Loss: 0.00001924
Iteration 82/1000 | Loss: 0.00001924
Iteration 83/1000 | Loss: 0.00001924
Iteration 84/1000 | Loss: 0.00001924
Iteration 85/1000 | Loss: 0.00001924
Iteration 86/1000 | Loss: 0.00001923
Iteration 87/1000 | Loss: 0.00001923
Iteration 88/1000 | Loss: 0.00001923
Iteration 89/1000 | Loss: 0.00001923
Iteration 90/1000 | Loss: 0.00001923
Iteration 91/1000 | Loss: 0.00001923
Iteration 92/1000 | Loss: 0.00001923
Iteration 93/1000 | Loss: 0.00001923
Iteration 94/1000 | Loss: 0.00001923
Iteration 95/1000 | Loss: 0.00001923
Iteration 96/1000 | Loss: 0.00001922
Iteration 97/1000 | Loss: 0.00001922
Iteration 98/1000 | Loss: 0.00001922
Iteration 99/1000 | Loss: 0.00001922
Iteration 100/1000 | Loss: 0.00001922
Iteration 101/1000 | Loss: 0.00001922
Iteration 102/1000 | Loss: 0.00001921
Iteration 103/1000 | Loss: 0.00001921
Iteration 104/1000 | Loss: 0.00001921
Iteration 105/1000 | Loss: 0.00001921
Iteration 106/1000 | Loss: 0.00001921
Iteration 107/1000 | Loss: 0.00001921
Iteration 108/1000 | Loss: 0.00001921
Iteration 109/1000 | Loss: 0.00001920
Iteration 110/1000 | Loss: 0.00001920
Iteration 111/1000 | Loss: 0.00001920
Iteration 112/1000 | Loss: 0.00001919
Iteration 113/1000 | Loss: 0.00001919
Iteration 114/1000 | Loss: 0.00001919
Iteration 115/1000 | Loss: 0.00001919
Iteration 116/1000 | Loss: 0.00001919
Iteration 117/1000 | Loss: 0.00001919
Iteration 118/1000 | Loss: 0.00001919
Iteration 119/1000 | Loss: 0.00001918
Iteration 120/1000 | Loss: 0.00001918
Iteration 121/1000 | Loss: 0.00001918
Iteration 122/1000 | Loss: 0.00001918
Iteration 123/1000 | Loss: 0.00001918
Iteration 124/1000 | Loss: 0.00001918
Iteration 125/1000 | Loss: 0.00001918
Iteration 126/1000 | Loss: 0.00001917
Iteration 127/1000 | Loss: 0.00001917
Iteration 128/1000 | Loss: 0.00001917
Iteration 129/1000 | Loss: 0.00001917
Iteration 130/1000 | Loss: 0.00001917
Iteration 131/1000 | Loss: 0.00001917
Iteration 132/1000 | Loss: 0.00001917
Iteration 133/1000 | Loss: 0.00001916
Iteration 134/1000 | Loss: 0.00001916
Iteration 135/1000 | Loss: 0.00001916
Iteration 136/1000 | Loss: 0.00001916
Iteration 137/1000 | Loss: 0.00001916
Iteration 138/1000 | Loss: 0.00001916
Iteration 139/1000 | Loss: 0.00001916
Iteration 140/1000 | Loss: 0.00001916
Iteration 141/1000 | Loss: 0.00001916
Iteration 142/1000 | Loss: 0.00001916
Iteration 143/1000 | Loss: 0.00001916
Iteration 144/1000 | Loss: 0.00001916
Iteration 145/1000 | Loss: 0.00001916
Iteration 146/1000 | Loss: 0.00001916
Iteration 147/1000 | Loss: 0.00001916
Iteration 148/1000 | Loss: 0.00001916
Iteration 149/1000 | Loss: 0.00001916
Iteration 150/1000 | Loss: 0.00001916
Iteration 151/1000 | Loss: 0.00001916
Iteration 152/1000 | Loss: 0.00001916
Iteration 153/1000 | Loss: 0.00001916
Iteration 154/1000 | Loss: 0.00001916
Iteration 155/1000 | Loss: 0.00001916
Iteration 156/1000 | Loss: 0.00001916
Iteration 157/1000 | Loss: 0.00001916
Iteration 158/1000 | Loss: 0.00001916
Iteration 159/1000 | Loss: 0.00001916
Iteration 160/1000 | Loss: 0.00001916
Iteration 161/1000 | Loss: 0.00001916
Iteration 162/1000 | Loss: 0.00001916
Iteration 163/1000 | Loss: 0.00001916
Iteration 164/1000 | Loss: 0.00001916
Iteration 165/1000 | Loss: 0.00001916
Iteration 166/1000 | Loss: 0.00001916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.9164610421285033e-05, 1.9164610421285033e-05, 1.9164610421285033e-05, 1.9164610421285033e-05, 1.9164610421285033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9164610421285033e-05

Optimization complete. Final v2v error: 3.5784850120544434 mm

Highest mean error: 5.1969170570373535 mm for frame 55

Lowest mean error: 3.0668342113494873 mm for frame 103

Saving results

Total time: 39.2456476688385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472641
Iteration 2/25 | Loss: 0.00082525
Iteration 3/25 | Loss: 0.00067106
Iteration 4/25 | Loss: 0.00065136
Iteration 5/25 | Loss: 0.00064536
Iteration 6/25 | Loss: 0.00064372
Iteration 7/25 | Loss: 0.00064357
Iteration 8/25 | Loss: 0.00064357
Iteration 9/25 | Loss: 0.00064357
Iteration 10/25 | Loss: 0.00064357
Iteration 11/25 | Loss: 0.00064357
Iteration 12/25 | Loss: 0.00064357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006435724208131433, 0.0006435724208131433, 0.0006435724208131433, 0.0006435724208131433, 0.0006435724208131433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006435724208131433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45559227
Iteration 2/25 | Loss: 0.00028034
Iteration 3/25 | Loss: 0.00028034
Iteration 4/25 | Loss: 0.00028034
Iteration 5/25 | Loss: 0.00028034
Iteration 6/25 | Loss: 0.00028034
Iteration 7/25 | Loss: 0.00028033
Iteration 8/25 | Loss: 0.00028033
Iteration 9/25 | Loss: 0.00028033
Iteration 10/25 | Loss: 0.00028033
Iteration 11/25 | Loss: 0.00028033
Iteration 12/25 | Loss: 0.00028033
Iteration 13/25 | Loss: 0.00028033
Iteration 14/25 | Loss: 0.00028033
Iteration 15/25 | Loss: 0.00028033
Iteration 16/25 | Loss: 0.00028033
Iteration 17/25 | Loss: 0.00028033
Iteration 18/25 | Loss: 0.00028033
Iteration 19/25 | Loss: 0.00028033
Iteration 20/25 | Loss: 0.00028033
Iteration 21/25 | Loss: 0.00028033
Iteration 22/25 | Loss: 0.00028033
Iteration 23/25 | Loss: 0.00028033
Iteration 24/25 | Loss: 0.00028033
Iteration 25/25 | Loss: 0.00028033

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028033
Iteration 2/1000 | Loss: 0.00002279
Iteration 3/1000 | Loss: 0.00001618
Iteration 4/1000 | Loss: 0.00001510
Iteration 5/1000 | Loss: 0.00001421
Iteration 6/1000 | Loss: 0.00001378
Iteration 7/1000 | Loss: 0.00001363
Iteration 8/1000 | Loss: 0.00001343
Iteration 9/1000 | Loss: 0.00001340
Iteration 10/1000 | Loss: 0.00001340
Iteration 11/1000 | Loss: 0.00001339
Iteration 12/1000 | Loss: 0.00001332
Iteration 13/1000 | Loss: 0.00001327
Iteration 14/1000 | Loss: 0.00001320
Iteration 15/1000 | Loss: 0.00001318
Iteration 16/1000 | Loss: 0.00001317
Iteration 17/1000 | Loss: 0.00001317
Iteration 18/1000 | Loss: 0.00001316
Iteration 19/1000 | Loss: 0.00001315
Iteration 20/1000 | Loss: 0.00001315
Iteration 21/1000 | Loss: 0.00001315
Iteration 22/1000 | Loss: 0.00001313
Iteration 23/1000 | Loss: 0.00001310
Iteration 24/1000 | Loss: 0.00001309
Iteration 25/1000 | Loss: 0.00001309
Iteration 26/1000 | Loss: 0.00001309
Iteration 27/1000 | Loss: 0.00001309
Iteration 28/1000 | Loss: 0.00001307
Iteration 29/1000 | Loss: 0.00001307
Iteration 30/1000 | Loss: 0.00001306
Iteration 31/1000 | Loss: 0.00001306
Iteration 32/1000 | Loss: 0.00001305
Iteration 33/1000 | Loss: 0.00001305
Iteration 34/1000 | Loss: 0.00001304
Iteration 35/1000 | Loss: 0.00001304
Iteration 36/1000 | Loss: 0.00001304
Iteration 37/1000 | Loss: 0.00001303
Iteration 38/1000 | Loss: 0.00001303
Iteration 39/1000 | Loss: 0.00001303
Iteration 40/1000 | Loss: 0.00001302
Iteration 41/1000 | Loss: 0.00001302
Iteration 42/1000 | Loss: 0.00001302
Iteration 43/1000 | Loss: 0.00001301
Iteration 44/1000 | Loss: 0.00001301
Iteration 45/1000 | Loss: 0.00001301
Iteration 46/1000 | Loss: 0.00001301
Iteration 47/1000 | Loss: 0.00001300
Iteration 48/1000 | Loss: 0.00001300
Iteration 49/1000 | Loss: 0.00001300
Iteration 50/1000 | Loss: 0.00001299
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001299
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00001299
Iteration 57/1000 | Loss: 0.00001299
Iteration 58/1000 | Loss: 0.00001299
Iteration 59/1000 | Loss: 0.00001298
Iteration 60/1000 | Loss: 0.00001298
Iteration 61/1000 | Loss: 0.00001298
Iteration 62/1000 | Loss: 0.00001298
Iteration 63/1000 | Loss: 0.00001298
Iteration 64/1000 | Loss: 0.00001298
Iteration 65/1000 | Loss: 0.00001298
Iteration 66/1000 | Loss: 0.00001298
Iteration 67/1000 | Loss: 0.00001298
Iteration 68/1000 | Loss: 0.00001297
Iteration 69/1000 | Loss: 0.00001297
Iteration 70/1000 | Loss: 0.00001297
Iteration 71/1000 | Loss: 0.00001297
Iteration 72/1000 | Loss: 0.00001296
Iteration 73/1000 | Loss: 0.00001296
Iteration 74/1000 | Loss: 0.00001296
Iteration 75/1000 | Loss: 0.00001296
Iteration 76/1000 | Loss: 0.00001296
Iteration 77/1000 | Loss: 0.00001296
Iteration 78/1000 | Loss: 0.00001296
Iteration 79/1000 | Loss: 0.00001296
Iteration 80/1000 | Loss: 0.00001296
Iteration 81/1000 | Loss: 0.00001295
Iteration 82/1000 | Loss: 0.00001295
Iteration 83/1000 | Loss: 0.00001295
Iteration 84/1000 | Loss: 0.00001295
Iteration 85/1000 | Loss: 0.00001295
Iteration 86/1000 | Loss: 0.00001295
Iteration 87/1000 | Loss: 0.00001295
Iteration 88/1000 | Loss: 0.00001295
Iteration 89/1000 | Loss: 0.00001295
Iteration 90/1000 | Loss: 0.00001295
Iteration 91/1000 | Loss: 0.00001295
Iteration 92/1000 | Loss: 0.00001295
Iteration 93/1000 | Loss: 0.00001295
Iteration 94/1000 | Loss: 0.00001295
Iteration 95/1000 | Loss: 0.00001295
Iteration 96/1000 | Loss: 0.00001295
Iteration 97/1000 | Loss: 0.00001295
Iteration 98/1000 | Loss: 0.00001295
Iteration 99/1000 | Loss: 0.00001295
Iteration 100/1000 | Loss: 0.00001295
Iteration 101/1000 | Loss: 0.00001295
Iteration 102/1000 | Loss: 0.00001295
Iteration 103/1000 | Loss: 0.00001295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.2950370546604972e-05, 1.2950370546604972e-05, 1.2950370546604972e-05, 1.2950370546604972e-05, 1.2950370546604972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2950370546604972e-05

Optimization complete. Final v2v error: 3.012249231338501 mm

Highest mean error: 3.359066963195801 mm for frame 62

Lowest mean error: 2.8200583457946777 mm for frame 34

Saving results

Total time: 29.77369260787964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01012016
Iteration 2/25 | Loss: 0.00161761
Iteration 3/25 | Loss: 0.00085027
Iteration 4/25 | Loss: 0.00075336
Iteration 5/25 | Loss: 0.00072384
Iteration 6/25 | Loss: 0.00071407
Iteration 7/25 | Loss: 0.00069747
Iteration 8/25 | Loss: 0.00069589
Iteration 9/25 | Loss: 0.00068943
Iteration 10/25 | Loss: 0.00067006
Iteration 11/25 | Loss: 0.00066121
Iteration 12/25 | Loss: 0.00065932
Iteration 13/25 | Loss: 0.00065743
Iteration 14/25 | Loss: 0.00065488
Iteration 15/25 | Loss: 0.00064554
Iteration 16/25 | Loss: 0.00064569
Iteration 17/25 | Loss: 0.00064550
Iteration 18/25 | Loss: 0.00064401
Iteration 19/25 | Loss: 0.00064363
Iteration 20/25 | Loss: 0.00064363
Iteration 21/25 | Loss: 0.00064363
Iteration 22/25 | Loss: 0.00064363
Iteration 23/25 | Loss: 0.00064363
Iteration 24/25 | Loss: 0.00064363
Iteration 25/25 | Loss: 0.00064363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94531274
Iteration 2/25 | Loss: 0.00030225
Iteration 3/25 | Loss: 0.00026553
Iteration 4/25 | Loss: 0.00026553
Iteration 5/25 | Loss: 0.00026553
Iteration 6/25 | Loss: 0.00026553
Iteration 7/25 | Loss: 0.00026553
Iteration 8/25 | Loss: 0.00026553
Iteration 9/25 | Loss: 0.00026553
Iteration 10/25 | Loss: 0.00026553
Iteration 11/25 | Loss: 0.00026553
Iteration 12/25 | Loss: 0.00026553
Iteration 13/25 | Loss: 0.00026553
Iteration 14/25 | Loss: 0.00026553
Iteration 15/25 | Loss: 0.00026553
Iteration 16/25 | Loss: 0.00026553
Iteration 17/25 | Loss: 0.00026553
Iteration 18/25 | Loss: 0.00026553
Iteration 19/25 | Loss: 0.00026553
Iteration 20/25 | Loss: 0.00026553
Iteration 21/25 | Loss: 0.00026553
Iteration 22/25 | Loss: 0.00026553
Iteration 23/25 | Loss: 0.00026553
Iteration 24/25 | Loss: 0.00026553
Iteration 25/25 | Loss: 0.00026553
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0002655282733030617, 0.0002655282733030617, 0.0002655282733030617, 0.0002655282733030617, 0.0002655282733030617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002655282733030617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026553
Iteration 2/1000 | Loss: 0.00005850
Iteration 3/1000 | Loss: 0.00001925
Iteration 4/1000 | Loss: 0.00001811
Iteration 5/1000 | Loss: 0.00002935
Iteration 6/1000 | Loss: 0.00003422
Iteration 7/1000 | Loss: 0.00003344
Iteration 8/1000 | Loss: 0.00002751
Iteration 9/1000 | Loss: 0.00001568
Iteration 10/1000 | Loss: 0.00006010
Iteration 11/1000 | Loss: 0.00004235
Iteration 12/1000 | Loss: 0.00001858
Iteration 13/1000 | Loss: 0.00001507
Iteration 14/1000 | Loss: 0.00001962
Iteration 15/1000 | Loss: 0.00002281
Iteration 16/1000 | Loss: 0.00003376
Iteration 17/1000 | Loss: 0.00005431
Iteration 18/1000 | Loss: 0.00001497
Iteration 19/1000 | Loss: 0.00002100
Iteration 20/1000 | Loss: 0.00001481
Iteration 21/1000 | Loss: 0.00001481
Iteration 22/1000 | Loss: 0.00001481
Iteration 23/1000 | Loss: 0.00001481
Iteration 24/1000 | Loss: 0.00001481
Iteration 25/1000 | Loss: 0.00001480
Iteration 26/1000 | Loss: 0.00001480
Iteration 27/1000 | Loss: 0.00001480
Iteration 28/1000 | Loss: 0.00001479
Iteration 29/1000 | Loss: 0.00001479
Iteration 30/1000 | Loss: 0.00001479
Iteration 31/1000 | Loss: 0.00001475
Iteration 32/1000 | Loss: 0.00001472
Iteration 33/1000 | Loss: 0.00001471
Iteration 34/1000 | Loss: 0.00001471
Iteration 35/1000 | Loss: 0.00001470
Iteration 36/1000 | Loss: 0.00001470
Iteration 37/1000 | Loss: 0.00002928
Iteration 38/1000 | Loss: 0.00001469
Iteration 39/1000 | Loss: 0.00001466
Iteration 40/1000 | Loss: 0.00001466
Iteration 41/1000 | Loss: 0.00001466
Iteration 42/1000 | Loss: 0.00001466
Iteration 43/1000 | Loss: 0.00001465
Iteration 44/1000 | Loss: 0.00001465
Iteration 45/1000 | Loss: 0.00001465
Iteration 46/1000 | Loss: 0.00001465
Iteration 47/1000 | Loss: 0.00001465
Iteration 48/1000 | Loss: 0.00001465
Iteration 49/1000 | Loss: 0.00001465
Iteration 50/1000 | Loss: 0.00001464
Iteration 51/1000 | Loss: 0.00001464
Iteration 52/1000 | Loss: 0.00001464
Iteration 53/1000 | Loss: 0.00001464
Iteration 54/1000 | Loss: 0.00001464
Iteration 55/1000 | Loss: 0.00001464
Iteration 56/1000 | Loss: 0.00001463
Iteration 57/1000 | Loss: 0.00001463
Iteration 58/1000 | Loss: 0.00001463
Iteration 59/1000 | Loss: 0.00001463
Iteration 60/1000 | Loss: 0.00002558
Iteration 61/1000 | Loss: 0.00001461
Iteration 62/1000 | Loss: 0.00001461
Iteration 63/1000 | Loss: 0.00001460
Iteration 64/1000 | Loss: 0.00001460
Iteration 65/1000 | Loss: 0.00001460
Iteration 66/1000 | Loss: 0.00001460
Iteration 67/1000 | Loss: 0.00001460
Iteration 68/1000 | Loss: 0.00001460
Iteration 69/1000 | Loss: 0.00001460
Iteration 70/1000 | Loss: 0.00001460
Iteration 71/1000 | Loss: 0.00001460
Iteration 72/1000 | Loss: 0.00001460
Iteration 73/1000 | Loss: 0.00001460
Iteration 74/1000 | Loss: 0.00001460
Iteration 75/1000 | Loss: 0.00001460
Iteration 76/1000 | Loss: 0.00001460
Iteration 77/1000 | Loss: 0.00001460
Iteration 78/1000 | Loss: 0.00001460
Iteration 79/1000 | Loss: 0.00001460
Iteration 80/1000 | Loss: 0.00001459
Iteration 81/1000 | Loss: 0.00001459
Iteration 82/1000 | Loss: 0.00001459
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001458
Iteration 85/1000 | Loss: 0.00001458
Iteration 86/1000 | Loss: 0.00001458
Iteration 87/1000 | Loss: 0.00001458
Iteration 88/1000 | Loss: 0.00001458
Iteration 89/1000 | Loss: 0.00001458
Iteration 90/1000 | Loss: 0.00001457
Iteration 91/1000 | Loss: 0.00001457
Iteration 92/1000 | Loss: 0.00001457
Iteration 93/1000 | Loss: 0.00002361
Iteration 94/1000 | Loss: 0.00001456
Iteration 95/1000 | Loss: 0.00001456
Iteration 96/1000 | Loss: 0.00001456
Iteration 97/1000 | Loss: 0.00001456
Iteration 98/1000 | Loss: 0.00001456
Iteration 99/1000 | Loss: 0.00001456
Iteration 100/1000 | Loss: 0.00001456
Iteration 101/1000 | Loss: 0.00001456
Iteration 102/1000 | Loss: 0.00001456
Iteration 103/1000 | Loss: 0.00001456
Iteration 104/1000 | Loss: 0.00001456
Iteration 105/1000 | Loss: 0.00001456
Iteration 106/1000 | Loss: 0.00001456
Iteration 107/1000 | Loss: 0.00001456
Iteration 108/1000 | Loss: 0.00001456
Iteration 109/1000 | Loss: 0.00001456
Iteration 110/1000 | Loss: 0.00001456
Iteration 111/1000 | Loss: 0.00001456
Iteration 112/1000 | Loss: 0.00001456
Iteration 113/1000 | Loss: 0.00001455
Iteration 114/1000 | Loss: 0.00001455
Iteration 115/1000 | Loss: 0.00001455
Iteration 116/1000 | Loss: 0.00001454
Iteration 117/1000 | Loss: 0.00001454
Iteration 118/1000 | Loss: 0.00001454
Iteration 119/1000 | Loss: 0.00001453
Iteration 120/1000 | Loss: 0.00001453
Iteration 121/1000 | Loss: 0.00001453
Iteration 122/1000 | Loss: 0.00001453
Iteration 123/1000 | Loss: 0.00001453
Iteration 124/1000 | Loss: 0.00001453
Iteration 125/1000 | Loss: 0.00001453
Iteration 126/1000 | Loss: 0.00001453
Iteration 127/1000 | Loss: 0.00001453
Iteration 128/1000 | Loss: 0.00001452
Iteration 129/1000 | Loss: 0.00001452
Iteration 130/1000 | Loss: 0.00001452
Iteration 131/1000 | Loss: 0.00001452
Iteration 132/1000 | Loss: 0.00001452
Iteration 133/1000 | Loss: 0.00001452
Iteration 134/1000 | Loss: 0.00001452
Iteration 135/1000 | Loss: 0.00001451
Iteration 136/1000 | Loss: 0.00001451
Iteration 137/1000 | Loss: 0.00001451
Iteration 138/1000 | Loss: 0.00001451
Iteration 139/1000 | Loss: 0.00001451
Iteration 140/1000 | Loss: 0.00001451
Iteration 141/1000 | Loss: 0.00001450
Iteration 142/1000 | Loss: 0.00003011
Iteration 143/1000 | Loss: 0.00001449
Iteration 144/1000 | Loss: 0.00001449
Iteration 145/1000 | Loss: 0.00001448
Iteration 146/1000 | Loss: 0.00001448
Iteration 147/1000 | Loss: 0.00001448
Iteration 148/1000 | Loss: 0.00001448
Iteration 149/1000 | Loss: 0.00001448
Iteration 150/1000 | Loss: 0.00001448
Iteration 151/1000 | Loss: 0.00001448
Iteration 152/1000 | Loss: 0.00001448
Iteration 153/1000 | Loss: 0.00001448
Iteration 154/1000 | Loss: 0.00001448
Iteration 155/1000 | Loss: 0.00001448
Iteration 156/1000 | Loss: 0.00001448
Iteration 157/1000 | Loss: 0.00001447
Iteration 158/1000 | Loss: 0.00001447
Iteration 159/1000 | Loss: 0.00001447
Iteration 160/1000 | Loss: 0.00001447
Iteration 161/1000 | Loss: 0.00001447
Iteration 162/1000 | Loss: 0.00001447
Iteration 163/1000 | Loss: 0.00001447
Iteration 164/1000 | Loss: 0.00001447
Iteration 165/1000 | Loss: 0.00001447
Iteration 166/1000 | Loss: 0.00001447
Iteration 167/1000 | Loss: 0.00001447
Iteration 168/1000 | Loss: 0.00001447
Iteration 169/1000 | Loss: 0.00001447
Iteration 170/1000 | Loss: 0.00001447
Iteration 171/1000 | Loss: 0.00001447
Iteration 172/1000 | Loss: 0.00001447
Iteration 173/1000 | Loss: 0.00001447
Iteration 174/1000 | Loss: 0.00001447
Iteration 175/1000 | Loss: 0.00001447
Iteration 176/1000 | Loss: 0.00001446
Iteration 177/1000 | Loss: 0.00001446
Iteration 178/1000 | Loss: 0.00001446
Iteration 179/1000 | Loss: 0.00001446
Iteration 180/1000 | Loss: 0.00001446
Iteration 181/1000 | Loss: 0.00001446
Iteration 182/1000 | Loss: 0.00001446
Iteration 183/1000 | Loss: 0.00001446
Iteration 184/1000 | Loss: 0.00001446
Iteration 185/1000 | Loss: 0.00001446
Iteration 186/1000 | Loss: 0.00001446
Iteration 187/1000 | Loss: 0.00001446
Iteration 188/1000 | Loss: 0.00001446
Iteration 189/1000 | Loss: 0.00001446
Iteration 190/1000 | Loss: 0.00001446
Iteration 191/1000 | Loss: 0.00001446
Iteration 192/1000 | Loss: 0.00001446
Iteration 193/1000 | Loss: 0.00001446
Iteration 194/1000 | Loss: 0.00001446
Iteration 195/1000 | Loss: 0.00001446
Iteration 196/1000 | Loss: 0.00001445
Iteration 197/1000 | Loss: 0.00001445
Iteration 198/1000 | Loss: 0.00001445
Iteration 199/1000 | Loss: 0.00001445
Iteration 200/1000 | Loss: 0.00001445
Iteration 201/1000 | Loss: 0.00001445
Iteration 202/1000 | Loss: 0.00001445
Iteration 203/1000 | Loss: 0.00001445
Iteration 204/1000 | Loss: 0.00001445
Iteration 205/1000 | Loss: 0.00001445
Iteration 206/1000 | Loss: 0.00001445
Iteration 207/1000 | Loss: 0.00001444
Iteration 208/1000 | Loss: 0.00001444
Iteration 209/1000 | Loss: 0.00001444
Iteration 210/1000 | Loss: 0.00001444
Iteration 211/1000 | Loss: 0.00001444
Iteration 212/1000 | Loss: 0.00001444
Iteration 213/1000 | Loss: 0.00001444
Iteration 214/1000 | Loss: 0.00001444
Iteration 215/1000 | Loss: 0.00001444
Iteration 216/1000 | Loss: 0.00001443
Iteration 217/1000 | Loss: 0.00001443
Iteration 218/1000 | Loss: 0.00001443
Iteration 219/1000 | Loss: 0.00001443
Iteration 220/1000 | Loss: 0.00001443
Iteration 221/1000 | Loss: 0.00004053
Iteration 222/1000 | Loss: 0.00003103
Iteration 223/1000 | Loss: 0.00001553
Iteration 224/1000 | Loss: 0.00002434
Iteration 225/1000 | Loss: 0.00001666
Iteration 226/1000 | Loss: 0.00001443
Iteration 227/1000 | Loss: 0.00001442
Iteration 228/1000 | Loss: 0.00001442
Iteration 229/1000 | Loss: 0.00001442
Iteration 230/1000 | Loss: 0.00001442
Iteration 231/1000 | Loss: 0.00001442
Iteration 232/1000 | Loss: 0.00001442
Iteration 233/1000 | Loss: 0.00001442
Iteration 234/1000 | Loss: 0.00001441
Iteration 235/1000 | Loss: 0.00001441
Iteration 236/1000 | Loss: 0.00001441
Iteration 237/1000 | Loss: 0.00001441
Iteration 238/1000 | Loss: 0.00001441
Iteration 239/1000 | Loss: 0.00001441
Iteration 240/1000 | Loss: 0.00001441
Iteration 241/1000 | Loss: 0.00001441
Iteration 242/1000 | Loss: 0.00001441
Iteration 243/1000 | Loss: 0.00001441
Iteration 244/1000 | Loss: 0.00001441
Iteration 245/1000 | Loss: 0.00001441
Iteration 246/1000 | Loss: 0.00001441
Iteration 247/1000 | Loss: 0.00001441
Iteration 248/1000 | Loss: 0.00001441
Iteration 249/1000 | Loss: 0.00001441
Iteration 250/1000 | Loss: 0.00001441
Iteration 251/1000 | Loss: 0.00001441
Iteration 252/1000 | Loss: 0.00001441
Iteration 253/1000 | Loss: 0.00001441
Iteration 254/1000 | Loss: 0.00001441
Iteration 255/1000 | Loss: 0.00001441
Iteration 256/1000 | Loss: 0.00001441
Iteration 257/1000 | Loss: 0.00001441
Iteration 258/1000 | Loss: 0.00001441
Iteration 259/1000 | Loss: 0.00001441
Iteration 260/1000 | Loss: 0.00001441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [1.4406964510271791e-05, 1.4406964510271791e-05, 1.4406964510271791e-05, 1.4406964510271791e-05, 1.4406964510271791e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4406964510271791e-05

Optimization complete. Final v2v error: 3.2331595420837402 mm

Highest mean error: 3.6640145778656006 mm for frame 118

Lowest mean error: 2.8273725509643555 mm for frame 234

Saving results

Total time: 95.28642797470093
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01110208
Iteration 2/25 | Loss: 0.01110208
Iteration 3/25 | Loss: 0.01110207
Iteration 4/25 | Loss: 0.01110207
Iteration 5/25 | Loss: 0.01110207
Iteration 6/25 | Loss: 0.01110207
Iteration 7/25 | Loss: 0.01110207
Iteration 8/25 | Loss: 0.01110206
Iteration 9/25 | Loss: 0.01110206
Iteration 10/25 | Loss: 0.01110206
Iteration 11/25 | Loss: 0.01110206
Iteration 12/25 | Loss: 0.01110206
Iteration 13/25 | Loss: 0.01110206
Iteration 14/25 | Loss: 0.01110205
Iteration 15/25 | Loss: 0.01110205
Iteration 16/25 | Loss: 0.01110204
Iteration 17/25 | Loss: 0.01110204
Iteration 18/25 | Loss: 0.01110204
Iteration 19/25 | Loss: 0.01110203
Iteration 20/25 | Loss: 0.01110203
Iteration 21/25 | Loss: 0.01110203
Iteration 22/25 | Loss: 0.01110203
Iteration 23/25 | Loss: 0.01110203
Iteration 24/25 | Loss: 0.01110202
Iteration 25/25 | Loss: 0.01110202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71314061
Iteration 2/25 | Loss: 0.05732772
Iteration 3/25 | Loss: 0.05735131
Iteration 4/25 | Loss: 0.05717759
Iteration 5/25 | Loss: 0.05701655
Iteration 6/25 | Loss: 0.05700312
Iteration 7/25 | Loss: 0.05700311
Iteration 8/25 | Loss: 0.05700311
Iteration 9/25 | Loss: 0.05700311
Iteration 10/25 | Loss: 0.05700311
Iteration 11/25 | Loss: 0.05700311
Iteration 12/25 | Loss: 0.05700310
Iteration 13/25 | Loss: 0.05700310
Iteration 14/25 | Loss: 0.05700310
Iteration 15/25 | Loss: 0.05700310
Iteration 16/25 | Loss: 0.05700310
Iteration 17/25 | Loss: 0.05700310
Iteration 18/25 | Loss: 0.05700310
Iteration 19/25 | Loss: 0.05700310
Iteration 20/25 | Loss: 0.05700310
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.05700309947133064, 0.05700309947133064, 0.05700309947133064, 0.05700309947133064, 0.05700309947133064]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.05700309947133064

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05700310
Iteration 2/1000 | Loss: 0.01150891
Iteration 3/1000 | Loss: 0.00381453
Iteration 4/1000 | Loss: 0.00316424
Iteration 5/1000 | Loss: 0.00386299
Iteration 6/1000 | Loss: 0.00096735
Iteration 7/1000 | Loss: 0.00064769
Iteration 8/1000 | Loss: 0.00049398
Iteration 9/1000 | Loss: 0.00029926
Iteration 10/1000 | Loss: 0.00068168
Iteration 11/1000 | Loss: 0.00045524
Iteration 12/1000 | Loss: 0.00027507
Iteration 13/1000 | Loss: 0.00044949
Iteration 14/1000 | Loss: 0.00279270
Iteration 15/1000 | Loss: 0.00278127
Iteration 16/1000 | Loss: 0.00080227
Iteration 17/1000 | Loss: 0.00347328
Iteration 18/1000 | Loss: 0.00372343
Iteration 19/1000 | Loss: 0.00313322
Iteration 20/1000 | Loss: 0.00147409
Iteration 21/1000 | Loss: 0.00049177
Iteration 22/1000 | Loss: 0.00019112
Iteration 23/1000 | Loss: 0.00034473
Iteration 24/1000 | Loss: 0.00013322
Iteration 25/1000 | Loss: 0.00019251
Iteration 26/1000 | Loss: 0.00018519
Iteration 27/1000 | Loss: 0.00018788
Iteration 28/1000 | Loss: 0.00052942
Iteration 29/1000 | Loss: 0.00016914
Iteration 30/1000 | Loss: 0.00036537
Iteration 31/1000 | Loss: 0.00022235
Iteration 32/1000 | Loss: 0.00004530
Iteration 33/1000 | Loss: 0.00024477
Iteration 34/1000 | Loss: 0.00022584
Iteration 35/1000 | Loss: 0.00009316
Iteration 36/1000 | Loss: 0.00027250
Iteration 37/1000 | Loss: 0.00051163
Iteration 38/1000 | Loss: 0.00011652
Iteration 39/1000 | Loss: 0.00010618
Iteration 40/1000 | Loss: 0.00012671
Iteration 41/1000 | Loss: 0.00004628
Iteration 42/1000 | Loss: 0.00008975
Iteration 43/1000 | Loss: 0.00003226
Iteration 44/1000 | Loss: 0.00036082
Iteration 45/1000 | Loss: 0.00004357
Iteration 46/1000 | Loss: 0.00029696
Iteration 47/1000 | Loss: 0.00033514
Iteration 48/1000 | Loss: 0.00045051
Iteration 49/1000 | Loss: 0.00013553
Iteration 50/1000 | Loss: 0.00004200
Iteration 51/1000 | Loss: 0.00018165
Iteration 52/1000 | Loss: 0.00029936
Iteration 53/1000 | Loss: 0.00007203
Iteration 54/1000 | Loss: 0.00005000
Iteration 55/1000 | Loss: 0.00039128
Iteration 56/1000 | Loss: 0.00175079
Iteration 57/1000 | Loss: 0.00124545
Iteration 58/1000 | Loss: 0.00102153
Iteration 59/1000 | Loss: 0.00024596
Iteration 60/1000 | Loss: 0.00042982
Iteration 61/1000 | Loss: 0.00024187
Iteration 62/1000 | Loss: 0.00043560
Iteration 63/1000 | Loss: 0.00032309
Iteration 64/1000 | Loss: 0.00021138
Iteration 65/1000 | Loss: 0.00006128
Iteration 66/1000 | Loss: 0.00009173
Iteration 67/1000 | Loss: 0.00003235
Iteration 68/1000 | Loss: 0.00002984
Iteration 69/1000 | Loss: 0.00002588
Iteration 70/1000 | Loss: 0.00007661
Iteration 71/1000 | Loss: 0.00002934
Iteration 72/1000 | Loss: 0.00002529
Iteration 73/1000 | Loss: 0.00017096
Iteration 74/1000 | Loss: 0.00080545
Iteration 75/1000 | Loss: 0.00059945
Iteration 76/1000 | Loss: 0.00007723
Iteration 77/1000 | Loss: 0.00023853
Iteration 78/1000 | Loss: 0.00014171
Iteration 79/1000 | Loss: 0.00003853
Iteration 80/1000 | Loss: 0.00010658
Iteration 81/1000 | Loss: 0.00002490
Iteration 82/1000 | Loss: 0.00004685
Iteration 83/1000 | Loss: 0.00011091
Iteration 84/1000 | Loss: 0.00005652
Iteration 85/1000 | Loss: 0.00002804
Iteration 86/1000 | Loss: 0.00002354
Iteration 87/1000 | Loss: 0.00002672
Iteration 88/1000 | Loss: 0.00040203
Iteration 89/1000 | Loss: 0.00048909
Iteration 90/1000 | Loss: 0.00012460
Iteration 91/1000 | Loss: 0.00021324
Iteration 92/1000 | Loss: 0.00023503
Iteration 93/1000 | Loss: 0.00062363
Iteration 94/1000 | Loss: 0.00007377
Iteration 95/1000 | Loss: 0.00066470
Iteration 96/1000 | Loss: 0.00003616
Iteration 97/1000 | Loss: 0.00004308
Iteration 98/1000 | Loss: 0.00024725
Iteration 99/1000 | Loss: 0.00064146
Iteration 100/1000 | Loss: 0.00003106
Iteration 101/1000 | Loss: 0.00047227
Iteration 102/1000 | Loss: 0.00002423
Iteration 103/1000 | Loss: 0.00002371
Iteration 104/1000 | Loss: 0.00010119
Iteration 105/1000 | Loss: 0.00002636
Iteration 106/1000 | Loss: 0.00023380
Iteration 107/1000 | Loss: 0.00006343
Iteration 108/1000 | Loss: 0.00003894
Iteration 109/1000 | Loss: 0.00004045
Iteration 110/1000 | Loss: 0.00031504
Iteration 111/1000 | Loss: 0.00028944
Iteration 112/1000 | Loss: 0.00003666
Iteration 113/1000 | Loss: 0.00002491
Iteration 114/1000 | Loss: 0.00002193
Iteration 115/1000 | Loss: 0.00002150
Iteration 116/1000 | Loss: 0.00023329
Iteration 117/1000 | Loss: 0.00030212
Iteration 118/1000 | Loss: 0.00032824
Iteration 119/1000 | Loss: 0.00004420
Iteration 120/1000 | Loss: 0.00002169
Iteration 121/1000 | Loss: 0.00002124
Iteration 122/1000 | Loss: 0.00031165
Iteration 123/1000 | Loss: 0.00032940
Iteration 124/1000 | Loss: 0.00023192
Iteration 125/1000 | Loss: 0.00004399
Iteration 126/1000 | Loss: 0.00006284
Iteration 127/1000 | Loss: 0.00006043
Iteration 128/1000 | Loss: 0.00002556
Iteration 129/1000 | Loss: 0.00012878
Iteration 130/1000 | Loss: 0.00002385
Iteration 131/1000 | Loss: 0.00013626
Iteration 132/1000 | Loss: 0.00021323
Iteration 133/1000 | Loss: 0.00032828
Iteration 134/1000 | Loss: 0.00004709
Iteration 135/1000 | Loss: 0.00028392
Iteration 136/1000 | Loss: 0.00002779
Iteration 137/1000 | Loss: 0.00002951
Iteration 138/1000 | Loss: 0.00021466
Iteration 139/1000 | Loss: 0.00009105
Iteration 140/1000 | Loss: 0.00005959
Iteration 141/1000 | Loss: 0.00021578
Iteration 142/1000 | Loss: 0.00012305
Iteration 143/1000 | Loss: 0.00004088
Iteration 144/1000 | Loss: 0.00021132
Iteration 145/1000 | Loss: 0.00020317
Iteration 146/1000 | Loss: 0.00037786
Iteration 147/1000 | Loss: 0.00011672
Iteration 148/1000 | Loss: 0.00003314
Iteration 149/1000 | Loss: 0.00005661
Iteration 150/1000 | Loss: 0.00003290
Iteration 151/1000 | Loss: 0.00002169
Iteration 152/1000 | Loss: 0.00002119
Iteration 153/1000 | Loss: 0.00030470
Iteration 154/1000 | Loss: 0.00014213
Iteration 155/1000 | Loss: 0.00039210
Iteration 156/1000 | Loss: 0.00006470
Iteration 157/1000 | Loss: 0.00003051
Iteration 158/1000 | Loss: 0.00002657
Iteration 159/1000 | Loss: 0.00002093
Iteration 160/1000 | Loss: 0.00002079
Iteration 161/1000 | Loss: 0.00002078
Iteration 162/1000 | Loss: 0.00002074
Iteration 163/1000 | Loss: 0.00002074
Iteration 164/1000 | Loss: 0.00002074
Iteration 165/1000 | Loss: 0.00002073
Iteration 166/1000 | Loss: 0.00002073
Iteration 167/1000 | Loss: 0.00002071
Iteration 168/1000 | Loss: 0.00002068
Iteration 169/1000 | Loss: 0.00002067
Iteration 170/1000 | Loss: 0.00002066
Iteration 171/1000 | Loss: 0.00002066
Iteration 172/1000 | Loss: 0.00002066
Iteration 173/1000 | Loss: 0.00002066
Iteration 174/1000 | Loss: 0.00002066
Iteration 175/1000 | Loss: 0.00002066
Iteration 176/1000 | Loss: 0.00002065
Iteration 177/1000 | Loss: 0.00002065
Iteration 178/1000 | Loss: 0.00002065
Iteration 179/1000 | Loss: 0.00002065
Iteration 180/1000 | Loss: 0.00002064
Iteration 181/1000 | Loss: 0.00002064
Iteration 182/1000 | Loss: 0.00002064
Iteration 183/1000 | Loss: 0.00002064
Iteration 184/1000 | Loss: 0.00002064
Iteration 185/1000 | Loss: 0.00002064
Iteration 186/1000 | Loss: 0.00023977
Iteration 187/1000 | Loss: 0.00011746
Iteration 188/1000 | Loss: 0.00002559
Iteration 189/1000 | Loss: 0.00020647
Iteration 190/1000 | Loss: 0.00013871
Iteration 191/1000 | Loss: 0.00002431
Iteration 192/1000 | Loss: 0.00020721
Iteration 193/1000 | Loss: 0.00007677
Iteration 194/1000 | Loss: 0.00002727
Iteration 195/1000 | Loss: 0.00002104
Iteration 196/1000 | Loss: 0.00002128
Iteration 197/1000 | Loss: 0.00019200
Iteration 198/1000 | Loss: 0.00004185
Iteration 199/1000 | Loss: 0.00004689
Iteration 200/1000 | Loss: 0.00002824
Iteration 201/1000 | Loss: 0.00002072
Iteration 202/1000 | Loss: 0.00002063
Iteration 203/1000 | Loss: 0.00019105
Iteration 204/1000 | Loss: 0.00020349
Iteration 205/1000 | Loss: 0.00003719
Iteration 206/1000 | Loss: 0.00003151
Iteration 207/1000 | Loss: 0.00002276
Iteration 208/1000 | Loss: 0.00002124
Iteration 209/1000 | Loss: 0.00002100
Iteration 210/1000 | Loss: 0.00002090
Iteration 211/1000 | Loss: 0.00002082
Iteration 212/1000 | Loss: 0.00002082
Iteration 213/1000 | Loss: 0.00005702
Iteration 214/1000 | Loss: 0.00002107
Iteration 215/1000 | Loss: 0.00002076
Iteration 216/1000 | Loss: 0.00002076
Iteration 217/1000 | Loss: 0.00002072
Iteration 218/1000 | Loss: 0.00003300
Iteration 219/1000 | Loss: 0.00002069
Iteration 220/1000 | Loss: 0.00012442
Iteration 221/1000 | Loss: 0.00003118
Iteration 222/1000 | Loss: 0.00003898
Iteration 223/1000 | Loss: 0.00002072
Iteration 224/1000 | Loss: 0.00002537
Iteration 225/1000 | Loss: 0.00002055
Iteration 226/1000 | Loss: 0.00002048
Iteration 227/1000 | Loss: 0.00002047
Iteration 228/1000 | Loss: 0.00002047
Iteration 229/1000 | Loss: 0.00002047
Iteration 230/1000 | Loss: 0.00002046
Iteration 231/1000 | Loss: 0.00002046
Iteration 232/1000 | Loss: 0.00002046
Iteration 233/1000 | Loss: 0.00002045
Iteration 234/1000 | Loss: 0.00002045
Iteration 235/1000 | Loss: 0.00002045
Iteration 236/1000 | Loss: 0.00002045
Iteration 237/1000 | Loss: 0.00002044
Iteration 238/1000 | Loss: 0.00002044
Iteration 239/1000 | Loss: 0.00002043
Iteration 240/1000 | Loss: 0.00002043
Iteration 241/1000 | Loss: 0.00002043
Iteration 242/1000 | Loss: 0.00002042
Iteration 243/1000 | Loss: 0.00002042
Iteration 244/1000 | Loss: 0.00002041
Iteration 245/1000 | Loss: 0.00002041
Iteration 246/1000 | Loss: 0.00002041
Iteration 247/1000 | Loss: 0.00002040
Iteration 248/1000 | Loss: 0.00002040
Iteration 249/1000 | Loss: 0.00002040
Iteration 250/1000 | Loss: 0.00002039
Iteration 251/1000 | Loss: 0.00002039
Iteration 252/1000 | Loss: 0.00002038
Iteration 253/1000 | Loss: 0.00010887
Iteration 254/1000 | Loss: 0.00002071
Iteration 255/1000 | Loss: 0.00005378
Iteration 256/1000 | Loss: 0.00002045
Iteration 257/1000 | Loss: 0.00002035
Iteration 258/1000 | Loss: 0.00002035
Iteration 259/1000 | Loss: 0.00002035
Iteration 260/1000 | Loss: 0.00002035
Iteration 261/1000 | Loss: 0.00002035
Iteration 262/1000 | Loss: 0.00002035
Iteration 263/1000 | Loss: 0.00002035
Iteration 264/1000 | Loss: 0.00002035
Iteration 265/1000 | Loss: 0.00002035
Iteration 266/1000 | Loss: 0.00002034
Iteration 267/1000 | Loss: 0.00002034
Iteration 268/1000 | Loss: 0.00002034
Iteration 269/1000 | Loss: 0.00002034
Iteration 270/1000 | Loss: 0.00002034
Iteration 271/1000 | Loss: 0.00002034
Iteration 272/1000 | Loss: 0.00002034
Iteration 273/1000 | Loss: 0.00002034
Iteration 274/1000 | Loss: 0.00002034
Iteration 275/1000 | Loss: 0.00002034
Iteration 276/1000 | Loss: 0.00002034
Iteration 277/1000 | Loss: 0.00002034
Iteration 278/1000 | Loss: 0.00002034
Iteration 279/1000 | Loss: 0.00002033
Iteration 280/1000 | Loss: 0.00002033
Iteration 281/1000 | Loss: 0.00002033
Iteration 282/1000 | Loss: 0.00002032
Iteration 283/1000 | Loss: 0.00002032
Iteration 284/1000 | Loss: 0.00002032
Iteration 285/1000 | Loss: 0.00002032
Iteration 286/1000 | Loss: 0.00002032
Iteration 287/1000 | Loss: 0.00002032
Iteration 288/1000 | Loss: 0.00002032
Iteration 289/1000 | Loss: 0.00002032
Iteration 290/1000 | Loss: 0.00002032
Iteration 291/1000 | Loss: 0.00002032
Iteration 292/1000 | Loss: 0.00002032
Iteration 293/1000 | Loss: 0.00002032
Iteration 294/1000 | Loss: 0.00002032
Iteration 295/1000 | Loss: 0.00002032
Iteration 296/1000 | Loss: 0.00002032
Iteration 297/1000 | Loss: 0.00002032
Iteration 298/1000 | Loss: 0.00002032
Iteration 299/1000 | Loss: 0.00002032
Iteration 300/1000 | Loss: 0.00002032
Iteration 301/1000 | Loss: 0.00002032
Iteration 302/1000 | Loss: 0.00002032
Iteration 303/1000 | Loss: 0.00002032
Iteration 304/1000 | Loss: 0.00002032
Iteration 305/1000 | Loss: 0.00002032
Iteration 306/1000 | Loss: 0.00002032
Iteration 307/1000 | Loss: 0.00002032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 307. Stopping optimization.
Last 5 losses: [2.031547592196148e-05, 2.031547592196148e-05, 2.031547592196148e-05, 2.031547592196148e-05, 2.031547592196148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.031547592196148e-05

Optimization complete. Final v2v error: 3.4836697578430176 mm

Highest mean error: 15.511592864990234 mm for frame 51

Lowest mean error: 2.700465679168701 mm for frame 23

Saving results

Total time: 331.09752202033997
