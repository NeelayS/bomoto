Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=177, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 9912-9967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085460
Iteration 2/25 | Loss: 0.01085460
Iteration 3/25 | Loss: 0.01085460
Iteration 4/25 | Loss: 0.01085460
Iteration 5/25 | Loss: 0.01085460
Iteration 6/25 | Loss: 0.01085459
Iteration 7/25 | Loss: 0.01085459
Iteration 8/25 | Loss: 0.01085459
Iteration 9/25 | Loss: 0.01085459
Iteration 10/25 | Loss: 0.01085459
Iteration 11/25 | Loss: 0.01085459
Iteration 12/25 | Loss: 0.01085458
Iteration 13/25 | Loss: 0.01085458
Iteration 14/25 | Loss: 0.01085458
Iteration 15/25 | Loss: 0.01085458
Iteration 16/25 | Loss: 0.01085458
Iteration 17/25 | Loss: 0.01085458
Iteration 18/25 | Loss: 0.01085458
Iteration 19/25 | Loss: 0.01085457
Iteration 20/25 | Loss: 0.01085457
Iteration 21/25 | Loss: 0.01085457
Iteration 22/25 | Loss: 0.01085457
Iteration 23/25 | Loss: 0.01085457
Iteration 24/25 | Loss: 0.01085457
Iteration 25/25 | Loss: 0.01085456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68334103
Iteration 2/25 | Loss: 0.08046552
Iteration 3/25 | Loss: 0.08045255
Iteration 4/25 | Loss: 0.08045253
Iteration 5/25 | Loss: 0.08045253
Iteration 6/25 | Loss: 0.08045252
Iteration 7/25 | Loss: 0.08045252
Iteration 8/25 | Loss: 0.08045252
Iteration 9/25 | Loss: 0.08045252
Iteration 10/25 | Loss: 0.08045252
Iteration 11/25 | Loss: 0.08045252
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.08045251667499542, 0.08045251667499542, 0.08045251667499542, 0.08045251667499542, 0.08045251667499542]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08045251667499542

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08045252
Iteration 2/1000 | Loss: 0.00158151
Iteration 3/1000 | Loss: 0.00055201
Iteration 4/1000 | Loss: 0.00136872
Iteration 5/1000 | Loss: 0.00744155
Iteration 6/1000 | Loss: 0.00012244
Iteration 7/1000 | Loss: 0.00007327
Iteration 8/1000 | Loss: 0.00004988
Iteration 9/1000 | Loss: 0.00006816
Iteration 10/1000 | Loss: 0.00004350
Iteration 11/1000 | Loss: 0.00003106
Iteration 12/1000 | Loss: 0.00003330
Iteration 13/1000 | Loss: 0.00008576
Iteration 14/1000 | Loss: 0.00002279
Iteration 15/1000 | Loss: 0.00002090
Iteration 16/1000 | Loss: 0.00001991
Iteration 17/1000 | Loss: 0.00001905
Iteration 18/1000 | Loss: 0.00002057
Iteration 19/1000 | Loss: 0.00001823
Iteration 20/1000 | Loss: 0.00001690
Iteration 21/1000 | Loss: 0.00004384
Iteration 22/1000 | Loss: 0.00001548
Iteration 23/1000 | Loss: 0.00001489
Iteration 24/1000 | Loss: 0.00002966
Iteration 25/1000 | Loss: 0.00001910
Iteration 26/1000 | Loss: 0.00001366
Iteration 27/1000 | Loss: 0.00001323
Iteration 28/1000 | Loss: 0.00001295
Iteration 29/1000 | Loss: 0.00003220
Iteration 30/1000 | Loss: 0.00004584
Iteration 31/1000 | Loss: 0.00001899
Iteration 32/1000 | Loss: 0.00002672
Iteration 33/1000 | Loss: 0.00003570
Iteration 34/1000 | Loss: 0.00001347
Iteration 35/1000 | Loss: 0.00001285
Iteration 36/1000 | Loss: 0.00002609
Iteration 37/1000 | Loss: 0.00001232
Iteration 38/1000 | Loss: 0.00001209
Iteration 39/1000 | Loss: 0.00001203
Iteration 40/1000 | Loss: 0.00001203
Iteration 41/1000 | Loss: 0.00001187
Iteration 42/1000 | Loss: 0.00001171
Iteration 43/1000 | Loss: 0.00001165
Iteration 44/1000 | Loss: 0.00001165
Iteration 45/1000 | Loss: 0.00001163
Iteration 46/1000 | Loss: 0.00001162
Iteration 47/1000 | Loss: 0.00001162
Iteration 48/1000 | Loss: 0.00001162
Iteration 49/1000 | Loss: 0.00001162
Iteration 50/1000 | Loss: 0.00001161
Iteration 51/1000 | Loss: 0.00001160
Iteration 52/1000 | Loss: 0.00001159
Iteration 53/1000 | Loss: 0.00001158
Iteration 54/1000 | Loss: 0.00001154
Iteration 55/1000 | Loss: 0.00001152
Iteration 56/1000 | Loss: 0.00001151
Iteration 57/1000 | Loss: 0.00001151
Iteration 58/1000 | Loss: 0.00001150
Iteration 59/1000 | Loss: 0.00001150
Iteration 60/1000 | Loss: 0.00001149
Iteration 61/1000 | Loss: 0.00001149
Iteration 62/1000 | Loss: 0.00001149
Iteration 63/1000 | Loss: 0.00001149
Iteration 64/1000 | Loss: 0.00001149
Iteration 65/1000 | Loss: 0.00001149
Iteration 66/1000 | Loss: 0.00001148
Iteration 67/1000 | Loss: 0.00001148
Iteration 68/1000 | Loss: 0.00001147
Iteration 69/1000 | Loss: 0.00001147
Iteration 70/1000 | Loss: 0.00001145
Iteration 71/1000 | Loss: 0.00001145
Iteration 72/1000 | Loss: 0.00001145
Iteration 73/1000 | Loss: 0.00001145
Iteration 74/1000 | Loss: 0.00001145
Iteration 75/1000 | Loss: 0.00001145
Iteration 76/1000 | Loss: 0.00001145
Iteration 77/1000 | Loss: 0.00001144
Iteration 78/1000 | Loss: 0.00001144
Iteration 79/1000 | Loss: 0.00001143
Iteration 80/1000 | Loss: 0.00001141
Iteration 81/1000 | Loss: 0.00001141
Iteration 82/1000 | Loss: 0.00001140
Iteration 83/1000 | Loss: 0.00001140
Iteration 84/1000 | Loss: 0.00001140
Iteration 85/1000 | Loss: 0.00001140
Iteration 86/1000 | Loss: 0.00001140
Iteration 87/1000 | Loss: 0.00001140
Iteration 88/1000 | Loss: 0.00001139
Iteration 89/1000 | Loss: 0.00001138
Iteration 90/1000 | Loss: 0.00001138
Iteration 91/1000 | Loss: 0.00001138
Iteration 92/1000 | Loss: 0.00001137
Iteration 93/1000 | Loss: 0.00001137
Iteration 94/1000 | Loss: 0.00001137
Iteration 95/1000 | Loss: 0.00001136
Iteration 96/1000 | Loss: 0.00001136
Iteration 97/1000 | Loss: 0.00001136
Iteration 98/1000 | Loss: 0.00001135
Iteration 99/1000 | Loss: 0.00001135
Iteration 100/1000 | Loss: 0.00001135
Iteration 101/1000 | Loss: 0.00001135
Iteration 102/1000 | Loss: 0.00001134
Iteration 103/1000 | Loss: 0.00001134
Iteration 104/1000 | Loss: 0.00001134
Iteration 105/1000 | Loss: 0.00001134
Iteration 106/1000 | Loss: 0.00001134
Iteration 107/1000 | Loss: 0.00001134
Iteration 108/1000 | Loss: 0.00001134
Iteration 109/1000 | Loss: 0.00001134
Iteration 110/1000 | Loss: 0.00002244
Iteration 111/1000 | Loss: 0.00001137
Iteration 112/1000 | Loss: 0.00001312
Iteration 113/1000 | Loss: 0.00001133
Iteration 114/1000 | Loss: 0.00001133
Iteration 115/1000 | Loss: 0.00001133
Iteration 116/1000 | Loss: 0.00001133
Iteration 117/1000 | Loss: 0.00001133
Iteration 118/1000 | Loss: 0.00001133
Iteration 119/1000 | Loss: 0.00001133
Iteration 120/1000 | Loss: 0.00001133
Iteration 121/1000 | Loss: 0.00001133
Iteration 122/1000 | Loss: 0.00001133
Iteration 123/1000 | Loss: 0.00001132
Iteration 124/1000 | Loss: 0.00001132
Iteration 125/1000 | Loss: 0.00001132
Iteration 126/1000 | Loss: 0.00001132
Iteration 127/1000 | Loss: 0.00001132
Iteration 128/1000 | Loss: 0.00001132
Iteration 129/1000 | Loss: 0.00001131
Iteration 130/1000 | Loss: 0.00001131
Iteration 131/1000 | Loss: 0.00001131
Iteration 132/1000 | Loss: 0.00001131
Iteration 133/1000 | Loss: 0.00001131
Iteration 134/1000 | Loss: 0.00001131
Iteration 135/1000 | Loss: 0.00001131
Iteration 136/1000 | Loss: 0.00001131
Iteration 137/1000 | Loss: 0.00001131
Iteration 138/1000 | Loss: 0.00001131
Iteration 139/1000 | Loss: 0.00001131
Iteration 140/1000 | Loss: 0.00001131
Iteration 141/1000 | Loss: 0.00001130
Iteration 142/1000 | Loss: 0.00001130
Iteration 143/1000 | Loss: 0.00001130
Iteration 144/1000 | Loss: 0.00001130
Iteration 145/1000 | Loss: 0.00001130
Iteration 146/1000 | Loss: 0.00001130
Iteration 147/1000 | Loss: 0.00001130
Iteration 148/1000 | Loss: 0.00001130
Iteration 149/1000 | Loss: 0.00001130
Iteration 150/1000 | Loss: 0.00001130
Iteration 151/1000 | Loss: 0.00001130
Iteration 152/1000 | Loss: 0.00001130
Iteration 153/1000 | Loss: 0.00001130
Iteration 154/1000 | Loss: 0.00001130
Iteration 155/1000 | Loss: 0.00001130
Iteration 156/1000 | Loss: 0.00001130
Iteration 157/1000 | Loss: 0.00001130
Iteration 158/1000 | Loss: 0.00001130
Iteration 159/1000 | Loss: 0.00001130
Iteration 160/1000 | Loss: 0.00001130
Iteration 161/1000 | Loss: 0.00001130
Iteration 162/1000 | Loss: 0.00001130
Iteration 163/1000 | Loss: 0.00001130
Iteration 164/1000 | Loss: 0.00001130
Iteration 165/1000 | Loss: 0.00001130
Iteration 166/1000 | Loss: 0.00001130
Iteration 167/1000 | Loss: 0.00001130
Iteration 168/1000 | Loss: 0.00001130
Iteration 169/1000 | Loss: 0.00001130
Iteration 170/1000 | Loss: 0.00001130
Iteration 171/1000 | Loss: 0.00001130
Iteration 172/1000 | Loss: 0.00001130
Iteration 173/1000 | Loss: 0.00001130
Iteration 174/1000 | Loss: 0.00001130
Iteration 175/1000 | Loss: 0.00001130
Iteration 176/1000 | Loss: 0.00001130
Iteration 177/1000 | Loss: 0.00001130
Iteration 178/1000 | Loss: 0.00001130
Iteration 179/1000 | Loss: 0.00001130
Iteration 180/1000 | Loss: 0.00001130
Iteration 181/1000 | Loss: 0.00001130
Iteration 182/1000 | Loss: 0.00001130
Iteration 183/1000 | Loss: 0.00001130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.1295009244349785e-05, 1.1295009244349785e-05, 1.1295009244349785e-05, 1.1295009244349785e-05, 1.1295009244349785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1295009244349785e-05

Optimization complete. Final v2v error: 2.7708916664123535 mm

Highest mean error: 9.193469047546387 mm for frame 131

Lowest mean error: 2.5482795238494873 mm for frame 160

Saving results

Total time: 82.1383798122406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415258
Iteration 2/25 | Loss: 0.00105659
Iteration 3/25 | Loss: 0.00090264
Iteration 4/25 | Loss: 0.00088849
Iteration 5/25 | Loss: 0.00088655
Iteration 6/25 | Loss: 0.00088611
Iteration 7/25 | Loss: 0.00088611
Iteration 8/25 | Loss: 0.00088611
Iteration 9/25 | Loss: 0.00088611
Iteration 10/25 | Loss: 0.00088611
Iteration 11/25 | Loss: 0.00088611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008861128590069711, 0.0008861128590069711, 0.0008861128590069711, 0.0008861128590069711, 0.0008861128590069711]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008861128590069711

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.16065121
Iteration 2/25 | Loss: 0.00048232
Iteration 3/25 | Loss: 0.00048231
Iteration 4/25 | Loss: 0.00048231
Iteration 5/25 | Loss: 0.00048231
Iteration 6/25 | Loss: 0.00048230
Iteration 7/25 | Loss: 0.00048230
Iteration 8/25 | Loss: 0.00048230
Iteration 9/25 | Loss: 0.00048230
Iteration 10/25 | Loss: 0.00048230
Iteration 11/25 | Loss: 0.00048230
Iteration 12/25 | Loss: 0.00048230
Iteration 13/25 | Loss: 0.00048230
Iteration 14/25 | Loss: 0.00048230
Iteration 15/25 | Loss: 0.00048230
Iteration 16/25 | Loss: 0.00048230
Iteration 17/25 | Loss: 0.00048230
Iteration 18/25 | Loss: 0.00048230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004823031777050346, 0.0004823031777050346, 0.0004823031777050346, 0.0004823031777050346, 0.0004823031777050346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004823031777050346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048230
Iteration 2/1000 | Loss: 0.00002320
Iteration 3/1000 | Loss: 0.00001567
Iteration 4/1000 | Loss: 0.00001354
Iteration 5/1000 | Loss: 0.00001260
Iteration 6/1000 | Loss: 0.00001222
Iteration 7/1000 | Loss: 0.00001216
Iteration 8/1000 | Loss: 0.00001180
Iteration 9/1000 | Loss: 0.00001158
Iteration 10/1000 | Loss: 0.00001140
Iteration 11/1000 | Loss: 0.00001137
Iteration 12/1000 | Loss: 0.00001134
Iteration 13/1000 | Loss: 0.00001133
Iteration 14/1000 | Loss: 0.00001123
Iteration 15/1000 | Loss: 0.00001120
Iteration 16/1000 | Loss: 0.00001120
Iteration 17/1000 | Loss: 0.00001119
Iteration 18/1000 | Loss: 0.00001119
Iteration 19/1000 | Loss: 0.00001115
Iteration 20/1000 | Loss: 0.00001115
Iteration 21/1000 | Loss: 0.00001115
Iteration 22/1000 | Loss: 0.00001115
Iteration 23/1000 | Loss: 0.00001115
Iteration 24/1000 | Loss: 0.00001115
Iteration 25/1000 | Loss: 0.00001115
Iteration 26/1000 | Loss: 0.00001112
Iteration 27/1000 | Loss: 0.00001112
Iteration 28/1000 | Loss: 0.00001111
Iteration 29/1000 | Loss: 0.00001111
Iteration 30/1000 | Loss: 0.00001111
Iteration 31/1000 | Loss: 0.00001110
Iteration 32/1000 | Loss: 0.00001110
Iteration 33/1000 | Loss: 0.00001109
Iteration 34/1000 | Loss: 0.00001107
Iteration 35/1000 | Loss: 0.00001107
Iteration 36/1000 | Loss: 0.00001107
Iteration 37/1000 | Loss: 0.00001107
Iteration 38/1000 | Loss: 0.00001107
Iteration 39/1000 | Loss: 0.00001106
Iteration 40/1000 | Loss: 0.00001106
Iteration 41/1000 | Loss: 0.00001105
Iteration 42/1000 | Loss: 0.00001105
Iteration 43/1000 | Loss: 0.00001104
Iteration 44/1000 | Loss: 0.00001104
Iteration 45/1000 | Loss: 0.00001104
Iteration 46/1000 | Loss: 0.00001104
Iteration 47/1000 | Loss: 0.00001104
Iteration 48/1000 | Loss: 0.00001103
Iteration 49/1000 | Loss: 0.00001103
Iteration 50/1000 | Loss: 0.00001103
Iteration 51/1000 | Loss: 0.00001103
Iteration 52/1000 | Loss: 0.00001103
Iteration 53/1000 | Loss: 0.00001103
Iteration 54/1000 | Loss: 0.00001103
Iteration 55/1000 | Loss: 0.00001103
Iteration 56/1000 | Loss: 0.00001103
Iteration 57/1000 | Loss: 0.00001102
Iteration 58/1000 | Loss: 0.00001102
Iteration 59/1000 | Loss: 0.00001102
Iteration 60/1000 | Loss: 0.00001101
Iteration 61/1000 | Loss: 0.00001101
Iteration 62/1000 | Loss: 0.00001101
Iteration 63/1000 | Loss: 0.00001101
Iteration 64/1000 | Loss: 0.00001100
Iteration 65/1000 | Loss: 0.00001100
Iteration 66/1000 | Loss: 0.00001100
Iteration 67/1000 | Loss: 0.00001100
Iteration 68/1000 | Loss: 0.00001099
Iteration 69/1000 | Loss: 0.00001099
Iteration 70/1000 | Loss: 0.00001099
Iteration 71/1000 | Loss: 0.00001098
Iteration 72/1000 | Loss: 0.00001098
Iteration 73/1000 | Loss: 0.00001097
Iteration 74/1000 | Loss: 0.00001097
Iteration 75/1000 | Loss: 0.00001097
Iteration 76/1000 | Loss: 0.00001096
Iteration 77/1000 | Loss: 0.00001096
Iteration 78/1000 | Loss: 0.00001096
Iteration 79/1000 | Loss: 0.00001096
Iteration 80/1000 | Loss: 0.00001096
Iteration 81/1000 | Loss: 0.00001096
Iteration 82/1000 | Loss: 0.00001096
Iteration 83/1000 | Loss: 0.00001095
Iteration 84/1000 | Loss: 0.00001095
Iteration 85/1000 | Loss: 0.00001095
Iteration 86/1000 | Loss: 0.00001095
Iteration 87/1000 | Loss: 0.00001095
Iteration 88/1000 | Loss: 0.00001095
Iteration 89/1000 | Loss: 0.00001095
Iteration 90/1000 | Loss: 0.00001094
Iteration 91/1000 | Loss: 0.00001094
Iteration 92/1000 | Loss: 0.00001094
Iteration 93/1000 | Loss: 0.00001094
Iteration 94/1000 | Loss: 0.00001094
Iteration 95/1000 | Loss: 0.00001093
Iteration 96/1000 | Loss: 0.00001093
Iteration 97/1000 | Loss: 0.00001093
Iteration 98/1000 | Loss: 0.00001093
Iteration 99/1000 | Loss: 0.00001093
Iteration 100/1000 | Loss: 0.00001093
Iteration 101/1000 | Loss: 0.00001092
Iteration 102/1000 | Loss: 0.00001092
Iteration 103/1000 | Loss: 0.00001092
Iteration 104/1000 | Loss: 0.00001092
Iteration 105/1000 | Loss: 0.00001092
Iteration 106/1000 | Loss: 0.00001092
Iteration 107/1000 | Loss: 0.00001092
Iteration 108/1000 | Loss: 0.00001091
Iteration 109/1000 | Loss: 0.00001091
Iteration 110/1000 | Loss: 0.00001091
Iteration 111/1000 | Loss: 0.00001091
Iteration 112/1000 | Loss: 0.00001091
Iteration 113/1000 | Loss: 0.00001091
Iteration 114/1000 | Loss: 0.00001091
Iteration 115/1000 | Loss: 0.00001091
Iteration 116/1000 | Loss: 0.00001091
Iteration 117/1000 | Loss: 0.00001091
Iteration 118/1000 | Loss: 0.00001091
Iteration 119/1000 | Loss: 0.00001091
Iteration 120/1000 | Loss: 0.00001091
Iteration 121/1000 | Loss: 0.00001091
Iteration 122/1000 | Loss: 0.00001091
Iteration 123/1000 | Loss: 0.00001091
Iteration 124/1000 | Loss: 0.00001091
Iteration 125/1000 | Loss: 0.00001091
Iteration 126/1000 | Loss: 0.00001091
Iteration 127/1000 | Loss: 0.00001091
Iteration 128/1000 | Loss: 0.00001091
Iteration 129/1000 | Loss: 0.00001091
Iteration 130/1000 | Loss: 0.00001091
Iteration 131/1000 | Loss: 0.00001091
Iteration 132/1000 | Loss: 0.00001091
Iteration 133/1000 | Loss: 0.00001091
Iteration 134/1000 | Loss: 0.00001091
Iteration 135/1000 | Loss: 0.00001091
Iteration 136/1000 | Loss: 0.00001091
Iteration 137/1000 | Loss: 0.00001091
Iteration 138/1000 | Loss: 0.00001091
Iteration 139/1000 | Loss: 0.00001091
Iteration 140/1000 | Loss: 0.00001091
Iteration 141/1000 | Loss: 0.00001091
Iteration 142/1000 | Loss: 0.00001091
Iteration 143/1000 | Loss: 0.00001091
Iteration 144/1000 | Loss: 0.00001091
Iteration 145/1000 | Loss: 0.00001091
Iteration 146/1000 | Loss: 0.00001091
Iteration 147/1000 | Loss: 0.00001091
Iteration 148/1000 | Loss: 0.00001091
Iteration 149/1000 | Loss: 0.00001091
Iteration 150/1000 | Loss: 0.00001091
Iteration 151/1000 | Loss: 0.00001091
Iteration 152/1000 | Loss: 0.00001091
Iteration 153/1000 | Loss: 0.00001091
Iteration 154/1000 | Loss: 0.00001091
Iteration 155/1000 | Loss: 0.00001091
Iteration 156/1000 | Loss: 0.00001091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.0905268936767243e-05, 1.0905268936767243e-05, 1.0905268936767243e-05, 1.0905268936767243e-05, 1.0905268936767243e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0905268936767243e-05

Optimization complete. Final v2v error: 2.796401262283325 mm

Highest mean error: 3.325040578842163 mm for frame 40

Lowest mean error: 2.3265469074249268 mm for frame 18

Saving results

Total time: 32.65134072303772
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049798
Iteration 2/25 | Loss: 0.00412981
Iteration 3/25 | Loss: 0.00293319
Iteration 4/25 | Loss: 0.00254347
Iteration 5/25 | Loss: 0.00203359
Iteration 6/25 | Loss: 0.00192633
Iteration 7/25 | Loss: 0.00184491
Iteration 8/25 | Loss: 0.00179568
Iteration 9/25 | Loss: 0.00176904
Iteration 10/25 | Loss: 0.00170672
Iteration 11/25 | Loss: 0.00168488
Iteration 12/25 | Loss: 0.00167277
Iteration 13/25 | Loss: 0.00167137
Iteration 14/25 | Loss: 0.00166267
Iteration 15/25 | Loss: 0.00166354
Iteration 16/25 | Loss: 0.00166276
Iteration 17/25 | Loss: 0.00164891
Iteration 18/25 | Loss: 0.00164690
Iteration 19/25 | Loss: 0.00164700
Iteration 20/25 | Loss: 0.00164700
Iteration 21/25 | Loss: 0.00164686
Iteration 22/25 | Loss: 0.00164363
Iteration 23/25 | Loss: 0.00164259
Iteration 24/25 | Loss: 0.00164236
Iteration 25/25 | Loss: 0.00164221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32854140
Iteration 2/25 | Loss: 0.00867815
Iteration 3/25 | Loss: 0.00732919
Iteration 4/25 | Loss: 0.00732919
Iteration 5/25 | Loss: 0.00732919
Iteration 6/25 | Loss: 0.00732919
Iteration 7/25 | Loss: 0.00732918
Iteration 8/25 | Loss: 0.00732918
Iteration 9/25 | Loss: 0.00732918
Iteration 10/25 | Loss: 0.00732918
Iteration 11/25 | Loss: 0.00732918
Iteration 12/25 | Loss: 0.00732918
Iteration 13/25 | Loss: 0.00732918
Iteration 14/25 | Loss: 0.00732918
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.007329184096306562, 0.007329184096306562, 0.007329184096306562, 0.007329184096306562, 0.007329184096306562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007329184096306562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00732918
Iteration 2/1000 | Loss: 0.00194872
Iteration 3/1000 | Loss: 0.00193845
Iteration 4/1000 | Loss: 0.00065798
Iteration 5/1000 | Loss: 0.00235727
Iteration 6/1000 | Loss: 0.00096480
Iteration 7/1000 | Loss: 0.00211536
Iteration 8/1000 | Loss: 0.00059789
Iteration 9/1000 | Loss: 0.00065990
Iteration 10/1000 | Loss: 0.00055584
Iteration 11/1000 | Loss: 0.00077176
Iteration 12/1000 | Loss: 0.00052266
Iteration 13/1000 | Loss: 0.00055390
Iteration 14/1000 | Loss: 0.00049232
Iteration 15/1000 | Loss: 0.00049403
Iteration 16/1000 | Loss: 0.00054837
Iteration 17/1000 | Loss: 0.00058368
Iteration 18/1000 | Loss: 0.00064635
Iteration 19/1000 | Loss: 0.00076806
Iteration 20/1000 | Loss: 0.00037803
Iteration 21/1000 | Loss: 0.00037587
Iteration 22/1000 | Loss: 0.00069908
Iteration 23/1000 | Loss: 0.00163655
Iteration 24/1000 | Loss: 0.00156561
Iteration 25/1000 | Loss: 0.00099124
Iteration 26/1000 | Loss: 0.00065363
Iteration 27/1000 | Loss: 0.00044881
Iteration 28/1000 | Loss: 0.00036638
Iteration 29/1000 | Loss: 0.00047570
Iteration 30/1000 | Loss: 0.00036944
Iteration 31/1000 | Loss: 0.00058176
Iteration 32/1000 | Loss: 0.00053837
Iteration 33/1000 | Loss: 0.00039249
Iteration 34/1000 | Loss: 0.00038271
Iteration 35/1000 | Loss: 0.00035054
Iteration 36/1000 | Loss: 0.00040042
Iteration 37/1000 | Loss: 0.00040596
Iteration 38/1000 | Loss: 0.00051100
Iteration 39/1000 | Loss: 0.00042495
Iteration 40/1000 | Loss: 0.00059402
Iteration 41/1000 | Loss: 0.00035799
Iteration 42/1000 | Loss: 0.00071238
Iteration 43/1000 | Loss: 0.00079821
Iteration 44/1000 | Loss: 0.00048143
Iteration 45/1000 | Loss: 0.00060728
Iteration 46/1000 | Loss: 0.00040665
Iteration 47/1000 | Loss: 0.00036598
Iteration 48/1000 | Loss: 0.00032128
Iteration 49/1000 | Loss: 0.00044548
Iteration 50/1000 | Loss: 0.00031664
Iteration 51/1000 | Loss: 0.00030966
Iteration 52/1000 | Loss: 0.00090446
Iteration 53/1000 | Loss: 0.00030588
Iteration 54/1000 | Loss: 0.00042638
Iteration 55/1000 | Loss: 0.00079576
Iteration 56/1000 | Loss: 0.00065425
Iteration 57/1000 | Loss: 0.00157799
Iteration 58/1000 | Loss: 0.00032869
Iteration 59/1000 | Loss: 0.00060540
Iteration 60/1000 | Loss: 0.00029291
Iteration 61/1000 | Loss: 0.00031910
Iteration 62/1000 | Loss: 0.00032583
Iteration 63/1000 | Loss: 0.00037058
Iteration 64/1000 | Loss: 0.00071761
Iteration 65/1000 | Loss: 0.00028617
Iteration 66/1000 | Loss: 0.00028661
Iteration 67/1000 | Loss: 0.00035779
Iteration 68/1000 | Loss: 0.00033305
Iteration 69/1000 | Loss: 0.00028125
Iteration 70/1000 | Loss: 0.00029305
Iteration 71/1000 | Loss: 0.00027969
Iteration 72/1000 | Loss: 0.00038946
Iteration 73/1000 | Loss: 0.00028065
Iteration 74/1000 | Loss: 0.00027856
Iteration 75/1000 | Loss: 0.00027833
Iteration 76/1000 | Loss: 0.00040111
Iteration 77/1000 | Loss: 0.00027792
Iteration 78/1000 | Loss: 0.00029192
Iteration 79/1000 | Loss: 0.00027769
Iteration 80/1000 | Loss: 0.00028180
Iteration 81/1000 | Loss: 0.00027740
Iteration 82/1000 | Loss: 0.00027736
Iteration 83/1000 | Loss: 0.00027735
Iteration 84/1000 | Loss: 0.00027733
Iteration 85/1000 | Loss: 0.00027733
Iteration 86/1000 | Loss: 0.00027730
Iteration 87/1000 | Loss: 0.00027723
Iteration 88/1000 | Loss: 0.00027723
Iteration 89/1000 | Loss: 0.00027723
Iteration 90/1000 | Loss: 0.00027723
Iteration 91/1000 | Loss: 0.00027723
Iteration 92/1000 | Loss: 0.00027723
Iteration 93/1000 | Loss: 0.00027722
Iteration 94/1000 | Loss: 0.00027722
Iteration 95/1000 | Loss: 0.00027722
Iteration 96/1000 | Loss: 0.00029860
Iteration 97/1000 | Loss: 0.00027722
Iteration 98/1000 | Loss: 0.00027711
Iteration 99/1000 | Loss: 0.00027710
Iteration 100/1000 | Loss: 0.00027710
Iteration 101/1000 | Loss: 0.00027710
Iteration 102/1000 | Loss: 0.00027710
Iteration 103/1000 | Loss: 0.00027710
Iteration 104/1000 | Loss: 0.00027710
Iteration 105/1000 | Loss: 0.00027710
Iteration 106/1000 | Loss: 0.00027710
Iteration 107/1000 | Loss: 0.00027710
Iteration 108/1000 | Loss: 0.00027710
Iteration 109/1000 | Loss: 0.00027709
Iteration 110/1000 | Loss: 0.00027709
Iteration 111/1000 | Loss: 0.00027708
Iteration 112/1000 | Loss: 0.00027708
Iteration 113/1000 | Loss: 0.00027708
Iteration 114/1000 | Loss: 0.00027708
Iteration 115/1000 | Loss: 0.00027708
Iteration 116/1000 | Loss: 0.00027708
Iteration 117/1000 | Loss: 0.00027708
Iteration 118/1000 | Loss: 0.00027708
Iteration 119/1000 | Loss: 0.00027708
Iteration 120/1000 | Loss: 0.00027708
Iteration 121/1000 | Loss: 0.00027707
Iteration 122/1000 | Loss: 0.00027707
Iteration 123/1000 | Loss: 0.00027707
Iteration 124/1000 | Loss: 0.00027706
Iteration 125/1000 | Loss: 0.00027706
Iteration 126/1000 | Loss: 0.00027706
Iteration 127/1000 | Loss: 0.00027706
Iteration 128/1000 | Loss: 0.00027706
Iteration 129/1000 | Loss: 0.00027706
Iteration 130/1000 | Loss: 0.00027706
Iteration 131/1000 | Loss: 0.00027705
Iteration 132/1000 | Loss: 0.00027705
Iteration 133/1000 | Loss: 0.00027705
Iteration 134/1000 | Loss: 0.00027705
Iteration 135/1000 | Loss: 0.00027705
Iteration 136/1000 | Loss: 0.00027704
Iteration 137/1000 | Loss: 0.00027704
Iteration 138/1000 | Loss: 0.00027704
Iteration 139/1000 | Loss: 0.00027704
Iteration 140/1000 | Loss: 0.00027703
Iteration 141/1000 | Loss: 0.00027703
Iteration 142/1000 | Loss: 0.00027703
Iteration 143/1000 | Loss: 0.00027703
Iteration 144/1000 | Loss: 0.00027703
Iteration 145/1000 | Loss: 0.00027703
Iteration 146/1000 | Loss: 0.00027703
Iteration 147/1000 | Loss: 0.00027703
Iteration 148/1000 | Loss: 0.00027703
Iteration 149/1000 | Loss: 0.00027703
Iteration 150/1000 | Loss: 0.00027703
Iteration 151/1000 | Loss: 0.00027703
Iteration 152/1000 | Loss: 0.00027703
Iteration 153/1000 | Loss: 0.00029029
Iteration 154/1000 | Loss: 0.00027776
Iteration 155/1000 | Loss: 0.00027859
Iteration 156/1000 | Loss: 0.00027728
Iteration 157/1000 | Loss: 0.00028135
Iteration 158/1000 | Loss: 0.00027725
Iteration 159/1000 | Loss: 0.00027879
Iteration 160/1000 | Loss: 0.00027879
Iteration 161/1000 | Loss: 0.00031383
Iteration 162/1000 | Loss: 0.00027723
Iteration 163/1000 | Loss: 0.00027695
Iteration 164/1000 | Loss: 0.00027695
Iteration 165/1000 | Loss: 0.00027694
Iteration 166/1000 | Loss: 0.00027694
Iteration 167/1000 | Loss: 0.00027694
Iteration 168/1000 | Loss: 0.00027694
Iteration 169/1000 | Loss: 0.00027694
Iteration 170/1000 | Loss: 0.00027694
Iteration 171/1000 | Loss: 0.00027694
Iteration 172/1000 | Loss: 0.00027694
Iteration 173/1000 | Loss: 0.00027694
Iteration 174/1000 | Loss: 0.00027694
Iteration 175/1000 | Loss: 0.00027694
Iteration 176/1000 | Loss: 0.00027694
Iteration 177/1000 | Loss: 0.00027694
Iteration 178/1000 | Loss: 0.00027693
Iteration 179/1000 | Loss: 0.00027693
Iteration 180/1000 | Loss: 0.00027693
Iteration 181/1000 | Loss: 0.00027693
Iteration 182/1000 | Loss: 0.00027693
Iteration 183/1000 | Loss: 0.00027693
Iteration 184/1000 | Loss: 0.00027693
Iteration 185/1000 | Loss: 0.00027693
Iteration 186/1000 | Loss: 0.00027692
Iteration 187/1000 | Loss: 0.00027692
Iteration 188/1000 | Loss: 0.00027692
Iteration 189/1000 | Loss: 0.00027692
Iteration 190/1000 | Loss: 0.00027692
Iteration 191/1000 | Loss: 0.00027692
Iteration 192/1000 | Loss: 0.00027691
Iteration 193/1000 | Loss: 0.00027691
Iteration 194/1000 | Loss: 0.00027691
Iteration 195/1000 | Loss: 0.00027691
Iteration 196/1000 | Loss: 0.00027691
Iteration 197/1000 | Loss: 0.00027691
Iteration 198/1000 | Loss: 0.00027691
Iteration 199/1000 | Loss: 0.00027691
Iteration 200/1000 | Loss: 0.00027691
Iteration 201/1000 | Loss: 0.00027691
Iteration 202/1000 | Loss: 0.00027691
Iteration 203/1000 | Loss: 0.00027691
Iteration 204/1000 | Loss: 0.00027690
Iteration 205/1000 | Loss: 0.00027690
Iteration 206/1000 | Loss: 0.00027690
Iteration 207/1000 | Loss: 0.00027690
Iteration 208/1000 | Loss: 0.00027690
Iteration 209/1000 | Loss: 0.00027690
Iteration 210/1000 | Loss: 0.00027690
Iteration 211/1000 | Loss: 0.00027690
Iteration 212/1000 | Loss: 0.00027690
Iteration 213/1000 | Loss: 0.00027690
Iteration 214/1000 | Loss: 0.00027690
Iteration 215/1000 | Loss: 0.00027690
Iteration 216/1000 | Loss: 0.00027690
Iteration 217/1000 | Loss: 0.00027690
Iteration 218/1000 | Loss: 0.00027690
Iteration 219/1000 | Loss: 0.00027982
Iteration 220/1000 | Loss: 0.00027703
Iteration 221/1000 | Loss: 0.00027691
Iteration 222/1000 | Loss: 0.00027691
Iteration 223/1000 | Loss: 0.00027691
Iteration 224/1000 | Loss: 0.00027691
Iteration 225/1000 | Loss: 0.00027691
Iteration 226/1000 | Loss: 0.00027691
Iteration 227/1000 | Loss: 0.00027691
Iteration 228/1000 | Loss: 0.00027691
Iteration 229/1000 | Loss: 0.00027691
Iteration 230/1000 | Loss: 0.00027691
Iteration 231/1000 | Loss: 0.00027691
Iteration 232/1000 | Loss: 0.00027691
Iteration 233/1000 | Loss: 0.00027691
Iteration 234/1000 | Loss: 0.00027691
Iteration 235/1000 | Loss: 0.00027691
Iteration 236/1000 | Loss: 0.00027691
Iteration 237/1000 | Loss: 0.00027691
Iteration 238/1000 | Loss: 0.00027691
Iteration 239/1000 | Loss: 0.00027691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [0.00027690778370015323, 0.00027690778370015323, 0.00027690778370015323, 0.00027690778370015323, 0.00027690778370015323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027690778370015323

Optimization complete. Final v2v error: 9.166854858398438 mm

Highest mean error: 11.292882919311523 mm for frame 169

Lowest mean error: 4.9539031982421875 mm for frame 0

Saving results

Total time: 209.51100778579712
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813813
Iteration 2/25 | Loss: 0.00150599
Iteration 3/25 | Loss: 0.00103816
Iteration 4/25 | Loss: 0.00091160
Iteration 5/25 | Loss: 0.00089623
Iteration 6/25 | Loss: 0.00090644
Iteration 7/25 | Loss: 0.00088540
Iteration 8/25 | Loss: 0.00089828
Iteration 9/25 | Loss: 0.00087092
Iteration 10/25 | Loss: 0.00085491
Iteration 11/25 | Loss: 0.00084908
Iteration 12/25 | Loss: 0.00085112
Iteration 13/25 | Loss: 0.00084712
Iteration 14/25 | Loss: 0.00084686
Iteration 15/25 | Loss: 0.00084679
Iteration 16/25 | Loss: 0.00084679
Iteration 17/25 | Loss: 0.00084679
Iteration 18/25 | Loss: 0.00084679
Iteration 19/25 | Loss: 0.00084678
Iteration 20/25 | Loss: 0.00084678
Iteration 21/25 | Loss: 0.00084678
Iteration 22/25 | Loss: 0.00084678
Iteration 23/25 | Loss: 0.00084678
Iteration 24/25 | Loss: 0.00084678
Iteration 25/25 | Loss: 0.00084678

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91855669
Iteration 2/25 | Loss: 0.00062106
Iteration 3/25 | Loss: 0.00057995
Iteration 4/25 | Loss: 0.00057995
Iteration 5/25 | Loss: 0.00057995
Iteration 6/25 | Loss: 0.00057995
Iteration 7/25 | Loss: 0.00057995
Iteration 8/25 | Loss: 0.00057995
Iteration 9/25 | Loss: 0.00057995
Iteration 10/25 | Loss: 0.00057995
Iteration 11/25 | Loss: 0.00057995
Iteration 12/25 | Loss: 0.00057995
Iteration 13/25 | Loss: 0.00057995
Iteration 14/25 | Loss: 0.00057995
Iteration 15/25 | Loss: 0.00057995
Iteration 16/25 | Loss: 0.00057995
Iteration 17/25 | Loss: 0.00057995
Iteration 18/25 | Loss: 0.00057995
Iteration 19/25 | Loss: 0.00057995
Iteration 20/25 | Loss: 0.00057995
Iteration 21/25 | Loss: 0.00057995
Iteration 22/25 | Loss: 0.00057995
Iteration 23/25 | Loss: 0.00057995
Iteration 24/25 | Loss: 0.00057995
Iteration 25/25 | Loss: 0.00057995

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057995
Iteration 2/1000 | Loss: 0.00005957
Iteration 3/1000 | Loss: 0.00002813
Iteration 4/1000 | Loss: 0.00001149
Iteration 5/1000 | Loss: 0.00001016
Iteration 6/1000 | Loss: 0.00011200
Iteration 7/1000 | Loss: 0.00080116
Iteration 8/1000 | Loss: 0.00001042
Iteration 9/1000 | Loss: 0.00000938
Iteration 10/1000 | Loss: 0.00000903
Iteration 11/1000 | Loss: 0.00000897
Iteration 12/1000 | Loss: 0.00000894
Iteration 13/1000 | Loss: 0.00005676
Iteration 14/1000 | Loss: 0.00000882
Iteration 15/1000 | Loss: 0.00000882
Iteration 16/1000 | Loss: 0.00000880
Iteration 17/1000 | Loss: 0.00000878
Iteration 18/1000 | Loss: 0.00000878
Iteration 19/1000 | Loss: 0.00000878
Iteration 20/1000 | Loss: 0.00000878
Iteration 21/1000 | Loss: 0.00000877
Iteration 22/1000 | Loss: 0.00000877
Iteration 23/1000 | Loss: 0.00000876
Iteration 24/1000 | Loss: 0.00000875
Iteration 25/1000 | Loss: 0.00000873
Iteration 26/1000 | Loss: 0.00000871
Iteration 27/1000 | Loss: 0.00000868
Iteration 28/1000 | Loss: 0.00000867
Iteration 29/1000 | Loss: 0.00000865
Iteration 30/1000 | Loss: 0.00000864
Iteration 31/1000 | Loss: 0.00000864
Iteration 32/1000 | Loss: 0.00000864
Iteration 33/1000 | Loss: 0.00000864
Iteration 34/1000 | Loss: 0.00000863
Iteration 35/1000 | Loss: 0.00000863
Iteration 36/1000 | Loss: 0.00000861
Iteration 37/1000 | Loss: 0.00000861
Iteration 38/1000 | Loss: 0.00000861
Iteration 39/1000 | Loss: 0.00000861
Iteration 40/1000 | Loss: 0.00000861
Iteration 41/1000 | Loss: 0.00000860
Iteration 42/1000 | Loss: 0.00000860
Iteration 43/1000 | Loss: 0.00000860
Iteration 44/1000 | Loss: 0.00000860
Iteration 45/1000 | Loss: 0.00000860
Iteration 46/1000 | Loss: 0.00000860
Iteration 47/1000 | Loss: 0.00000860
Iteration 48/1000 | Loss: 0.00000860
Iteration 49/1000 | Loss: 0.00000859
Iteration 50/1000 | Loss: 0.00000859
Iteration 51/1000 | Loss: 0.00000859
Iteration 52/1000 | Loss: 0.00000858
Iteration 53/1000 | Loss: 0.00004131
Iteration 54/1000 | Loss: 0.00000855
Iteration 55/1000 | Loss: 0.00000855
Iteration 56/1000 | Loss: 0.00000854
Iteration 57/1000 | Loss: 0.00000854
Iteration 58/1000 | Loss: 0.00000854
Iteration 59/1000 | Loss: 0.00000854
Iteration 60/1000 | Loss: 0.00000854
Iteration 61/1000 | Loss: 0.00000854
Iteration 62/1000 | Loss: 0.00000854
Iteration 63/1000 | Loss: 0.00000854
Iteration 64/1000 | Loss: 0.00000854
Iteration 65/1000 | Loss: 0.00000854
Iteration 66/1000 | Loss: 0.00000854
Iteration 67/1000 | Loss: 0.00000854
Iteration 68/1000 | Loss: 0.00000854
Iteration 69/1000 | Loss: 0.00000854
Iteration 70/1000 | Loss: 0.00000853
Iteration 71/1000 | Loss: 0.00000853
Iteration 72/1000 | Loss: 0.00000853
Iteration 73/1000 | Loss: 0.00000853
Iteration 74/1000 | Loss: 0.00000853
Iteration 75/1000 | Loss: 0.00000853
Iteration 76/1000 | Loss: 0.00000853
Iteration 77/1000 | Loss: 0.00000853
Iteration 78/1000 | Loss: 0.00000853
Iteration 79/1000 | Loss: 0.00000853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [8.534582775610033e-06, 8.534582775610033e-06, 8.534582775610033e-06, 8.534582775610033e-06, 8.534582775610033e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.534582775610033e-06

Optimization complete. Final v2v error: 2.460919141769409 mm

Highest mean error: 3.326754570007324 mm for frame 88

Lowest mean error: 1.8923283815383911 mm for frame 12

Saving results

Total time: 50.9322566986084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435002
Iteration 2/25 | Loss: 0.00117471
Iteration 3/25 | Loss: 0.00092445
Iteration 4/25 | Loss: 0.00088992
Iteration 5/25 | Loss: 0.00088223
Iteration 6/25 | Loss: 0.00088026
Iteration 7/25 | Loss: 0.00088010
Iteration 8/25 | Loss: 0.00088010
Iteration 9/25 | Loss: 0.00088010
Iteration 10/25 | Loss: 0.00088010
Iteration 11/25 | Loss: 0.00088010
Iteration 12/25 | Loss: 0.00088010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008800987852737308, 0.0008800987852737308, 0.0008800987852737308, 0.0008800987852737308, 0.0008800987852737308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008800987852737308

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33807087
Iteration 2/25 | Loss: 0.00056292
Iteration 3/25 | Loss: 0.00056291
Iteration 4/25 | Loss: 0.00056291
Iteration 5/25 | Loss: 0.00056291
Iteration 6/25 | Loss: 0.00056291
Iteration 7/25 | Loss: 0.00056291
Iteration 8/25 | Loss: 0.00056291
Iteration 9/25 | Loss: 0.00056291
Iteration 10/25 | Loss: 0.00056291
Iteration 11/25 | Loss: 0.00056291
Iteration 12/25 | Loss: 0.00056291
Iteration 13/25 | Loss: 0.00056291
Iteration 14/25 | Loss: 0.00056291
Iteration 15/25 | Loss: 0.00056291
Iteration 16/25 | Loss: 0.00056291
Iteration 17/25 | Loss: 0.00056291
Iteration 18/25 | Loss: 0.00056291
Iteration 19/25 | Loss: 0.00056291
Iteration 20/25 | Loss: 0.00056291
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005629084771499038, 0.0005629084771499038, 0.0005629084771499038, 0.0005629084771499038, 0.0005629084771499038]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005629084771499038

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056291
Iteration 2/1000 | Loss: 0.00002692
Iteration 3/1000 | Loss: 0.00001609
Iteration 4/1000 | Loss: 0.00001323
Iteration 5/1000 | Loss: 0.00001238
Iteration 6/1000 | Loss: 0.00001180
Iteration 7/1000 | Loss: 0.00001150
Iteration 8/1000 | Loss: 0.00001125
Iteration 9/1000 | Loss: 0.00001108
Iteration 10/1000 | Loss: 0.00001106
Iteration 11/1000 | Loss: 0.00001102
Iteration 12/1000 | Loss: 0.00001096
Iteration 13/1000 | Loss: 0.00001094
Iteration 14/1000 | Loss: 0.00001093
Iteration 15/1000 | Loss: 0.00001092
Iteration 16/1000 | Loss: 0.00001091
Iteration 17/1000 | Loss: 0.00001089
Iteration 18/1000 | Loss: 0.00001088
Iteration 19/1000 | Loss: 0.00001087
Iteration 20/1000 | Loss: 0.00001086
Iteration 21/1000 | Loss: 0.00001086
Iteration 22/1000 | Loss: 0.00001085
Iteration 23/1000 | Loss: 0.00001085
Iteration 24/1000 | Loss: 0.00001084
Iteration 25/1000 | Loss: 0.00001084
Iteration 26/1000 | Loss: 0.00001083
Iteration 27/1000 | Loss: 0.00001083
Iteration 28/1000 | Loss: 0.00001082
Iteration 29/1000 | Loss: 0.00001081
Iteration 30/1000 | Loss: 0.00001081
Iteration 31/1000 | Loss: 0.00001081
Iteration 32/1000 | Loss: 0.00001080
Iteration 33/1000 | Loss: 0.00001080
Iteration 34/1000 | Loss: 0.00001079
Iteration 35/1000 | Loss: 0.00001079
Iteration 36/1000 | Loss: 0.00001079
Iteration 37/1000 | Loss: 0.00001078
Iteration 38/1000 | Loss: 0.00001078
Iteration 39/1000 | Loss: 0.00001078
Iteration 40/1000 | Loss: 0.00001077
Iteration 41/1000 | Loss: 0.00001077
Iteration 42/1000 | Loss: 0.00001076
Iteration 43/1000 | Loss: 0.00001076
Iteration 44/1000 | Loss: 0.00001075
Iteration 45/1000 | Loss: 0.00001075
Iteration 46/1000 | Loss: 0.00001075
Iteration 47/1000 | Loss: 0.00001074
Iteration 48/1000 | Loss: 0.00001074
Iteration 49/1000 | Loss: 0.00001073
Iteration 50/1000 | Loss: 0.00001073
Iteration 51/1000 | Loss: 0.00001073
Iteration 52/1000 | Loss: 0.00001072
Iteration 53/1000 | Loss: 0.00001072
Iteration 54/1000 | Loss: 0.00001071
Iteration 55/1000 | Loss: 0.00001071
Iteration 56/1000 | Loss: 0.00001071
Iteration 57/1000 | Loss: 0.00001070
Iteration 58/1000 | Loss: 0.00001070
Iteration 59/1000 | Loss: 0.00001069
Iteration 60/1000 | Loss: 0.00001069
Iteration 61/1000 | Loss: 0.00001069
Iteration 62/1000 | Loss: 0.00001069
Iteration 63/1000 | Loss: 0.00001068
Iteration 64/1000 | Loss: 0.00001068
Iteration 65/1000 | Loss: 0.00001067
Iteration 66/1000 | Loss: 0.00001067
Iteration 67/1000 | Loss: 0.00001067
Iteration 68/1000 | Loss: 0.00001066
Iteration 69/1000 | Loss: 0.00001066
Iteration 70/1000 | Loss: 0.00001066
Iteration 71/1000 | Loss: 0.00001065
Iteration 72/1000 | Loss: 0.00001065
Iteration 73/1000 | Loss: 0.00001065
Iteration 74/1000 | Loss: 0.00001065
Iteration 75/1000 | Loss: 0.00001065
Iteration 76/1000 | Loss: 0.00001064
Iteration 77/1000 | Loss: 0.00001064
Iteration 78/1000 | Loss: 0.00001064
Iteration 79/1000 | Loss: 0.00001064
Iteration 80/1000 | Loss: 0.00001064
Iteration 81/1000 | Loss: 0.00001064
Iteration 82/1000 | Loss: 0.00001064
Iteration 83/1000 | Loss: 0.00001064
Iteration 84/1000 | Loss: 0.00001064
Iteration 85/1000 | Loss: 0.00001064
Iteration 86/1000 | Loss: 0.00001064
Iteration 87/1000 | Loss: 0.00001064
Iteration 88/1000 | Loss: 0.00001064
Iteration 89/1000 | Loss: 0.00001063
Iteration 90/1000 | Loss: 0.00001063
Iteration 91/1000 | Loss: 0.00001063
Iteration 92/1000 | Loss: 0.00001063
Iteration 93/1000 | Loss: 0.00001063
Iteration 94/1000 | Loss: 0.00001063
Iteration 95/1000 | Loss: 0.00001063
Iteration 96/1000 | Loss: 0.00001063
Iteration 97/1000 | Loss: 0.00001062
Iteration 98/1000 | Loss: 0.00001062
Iteration 99/1000 | Loss: 0.00001062
Iteration 100/1000 | Loss: 0.00001062
Iteration 101/1000 | Loss: 0.00001062
Iteration 102/1000 | Loss: 0.00001062
Iteration 103/1000 | Loss: 0.00001062
Iteration 104/1000 | Loss: 0.00001062
Iteration 105/1000 | Loss: 0.00001062
Iteration 106/1000 | Loss: 0.00001061
Iteration 107/1000 | Loss: 0.00001061
Iteration 108/1000 | Loss: 0.00001061
Iteration 109/1000 | Loss: 0.00001061
Iteration 110/1000 | Loss: 0.00001061
Iteration 111/1000 | Loss: 0.00001061
Iteration 112/1000 | Loss: 0.00001061
Iteration 113/1000 | Loss: 0.00001061
Iteration 114/1000 | Loss: 0.00001061
Iteration 115/1000 | Loss: 0.00001061
Iteration 116/1000 | Loss: 0.00001060
Iteration 117/1000 | Loss: 0.00001060
Iteration 118/1000 | Loss: 0.00001060
Iteration 119/1000 | Loss: 0.00001060
Iteration 120/1000 | Loss: 0.00001060
Iteration 121/1000 | Loss: 0.00001060
Iteration 122/1000 | Loss: 0.00001060
Iteration 123/1000 | Loss: 0.00001060
Iteration 124/1000 | Loss: 0.00001059
Iteration 125/1000 | Loss: 0.00001059
Iteration 126/1000 | Loss: 0.00001059
Iteration 127/1000 | Loss: 0.00001059
Iteration 128/1000 | Loss: 0.00001059
Iteration 129/1000 | Loss: 0.00001059
Iteration 130/1000 | Loss: 0.00001059
Iteration 131/1000 | Loss: 0.00001059
Iteration 132/1000 | Loss: 0.00001059
Iteration 133/1000 | Loss: 0.00001059
Iteration 134/1000 | Loss: 0.00001059
Iteration 135/1000 | Loss: 0.00001059
Iteration 136/1000 | Loss: 0.00001059
Iteration 137/1000 | Loss: 0.00001058
Iteration 138/1000 | Loss: 0.00001058
Iteration 139/1000 | Loss: 0.00001058
Iteration 140/1000 | Loss: 0.00001058
Iteration 141/1000 | Loss: 0.00001058
Iteration 142/1000 | Loss: 0.00001058
Iteration 143/1000 | Loss: 0.00001058
Iteration 144/1000 | Loss: 0.00001057
Iteration 145/1000 | Loss: 0.00001057
Iteration 146/1000 | Loss: 0.00001057
Iteration 147/1000 | Loss: 0.00001057
Iteration 148/1000 | Loss: 0.00001057
Iteration 149/1000 | Loss: 0.00001057
Iteration 150/1000 | Loss: 0.00001057
Iteration 151/1000 | Loss: 0.00001057
Iteration 152/1000 | Loss: 0.00001057
Iteration 153/1000 | Loss: 0.00001056
Iteration 154/1000 | Loss: 0.00001056
Iteration 155/1000 | Loss: 0.00001056
Iteration 156/1000 | Loss: 0.00001056
Iteration 157/1000 | Loss: 0.00001056
Iteration 158/1000 | Loss: 0.00001056
Iteration 159/1000 | Loss: 0.00001056
Iteration 160/1000 | Loss: 0.00001056
Iteration 161/1000 | Loss: 0.00001056
Iteration 162/1000 | Loss: 0.00001056
Iteration 163/1000 | Loss: 0.00001055
Iteration 164/1000 | Loss: 0.00001055
Iteration 165/1000 | Loss: 0.00001055
Iteration 166/1000 | Loss: 0.00001055
Iteration 167/1000 | Loss: 0.00001055
Iteration 168/1000 | Loss: 0.00001055
Iteration 169/1000 | Loss: 0.00001055
Iteration 170/1000 | Loss: 0.00001055
Iteration 171/1000 | Loss: 0.00001055
Iteration 172/1000 | Loss: 0.00001055
Iteration 173/1000 | Loss: 0.00001055
Iteration 174/1000 | Loss: 0.00001055
Iteration 175/1000 | Loss: 0.00001055
Iteration 176/1000 | Loss: 0.00001055
Iteration 177/1000 | Loss: 0.00001055
Iteration 178/1000 | Loss: 0.00001055
Iteration 179/1000 | Loss: 0.00001055
Iteration 180/1000 | Loss: 0.00001055
Iteration 181/1000 | Loss: 0.00001055
Iteration 182/1000 | Loss: 0.00001055
Iteration 183/1000 | Loss: 0.00001055
Iteration 184/1000 | Loss: 0.00001055
Iteration 185/1000 | Loss: 0.00001055
Iteration 186/1000 | Loss: 0.00001055
Iteration 187/1000 | Loss: 0.00001055
Iteration 188/1000 | Loss: 0.00001055
Iteration 189/1000 | Loss: 0.00001055
Iteration 190/1000 | Loss: 0.00001055
Iteration 191/1000 | Loss: 0.00001055
Iteration 192/1000 | Loss: 0.00001055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.0550749721005559e-05, 1.0550749721005559e-05, 1.0550749721005559e-05, 1.0550749721005559e-05, 1.0550749721005559e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0550749721005559e-05

Optimization complete. Final v2v error: 2.663701057434082 mm

Highest mean error: 3.658000946044922 mm for frame 202

Lowest mean error: 2.126739025115967 mm for frame 107

Saving results

Total time: 40.40497016906738
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00597277
Iteration 2/25 | Loss: 0.00092070
Iteration 3/25 | Loss: 0.00083986
Iteration 4/25 | Loss: 0.00083018
Iteration 5/25 | Loss: 0.00082769
Iteration 6/25 | Loss: 0.00082739
Iteration 7/25 | Loss: 0.00082739
Iteration 8/25 | Loss: 0.00082739
Iteration 9/25 | Loss: 0.00082739
Iteration 10/25 | Loss: 0.00082739
Iteration 11/25 | Loss: 0.00082739
Iteration 12/25 | Loss: 0.00082739
Iteration 13/25 | Loss: 0.00082739
Iteration 14/25 | Loss: 0.00082739
Iteration 15/25 | Loss: 0.00082739
Iteration 16/25 | Loss: 0.00082739
Iteration 17/25 | Loss: 0.00082739
Iteration 18/25 | Loss: 0.00082739
Iteration 19/25 | Loss: 0.00082739
Iteration 20/25 | Loss: 0.00082739
Iteration 21/25 | Loss: 0.00082739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008273927960544825, 0.0008273927960544825, 0.0008273927960544825, 0.0008273927960544825, 0.0008273927960544825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008273927960544825

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.11091065
Iteration 2/25 | Loss: 0.00052280
Iteration 3/25 | Loss: 0.00052280
Iteration 4/25 | Loss: 0.00052280
Iteration 5/25 | Loss: 0.00052280
Iteration 6/25 | Loss: 0.00052280
Iteration 7/25 | Loss: 0.00052280
Iteration 8/25 | Loss: 0.00052280
Iteration 9/25 | Loss: 0.00052280
Iteration 10/25 | Loss: 0.00052280
Iteration 11/25 | Loss: 0.00052280
Iteration 12/25 | Loss: 0.00052280
Iteration 13/25 | Loss: 0.00052280
Iteration 14/25 | Loss: 0.00052280
Iteration 15/25 | Loss: 0.00052280
Iteration 16/25 | Loss: 0.00052280
Iteration 17/25 | Loss: 0.00052280
Iteration 18/25 | Loss: 0.00052280
Iteration 19/25 | Loss: 0.00052280
Iteration 20/25 | Loss: 0.00052280
Iteration 21/25 | Loss: 0.00052280
Iteration 22/25 | Loss: 0.00052280
Iteration 23/25 | Loss: 0.00052280
Iteration 24/25 | Loss: 0.00052280
Iteration 25/25 | Loss: 0.00052280

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052280
Iteration 2/1000 | Loss: 0.00001834
Iteration 3/1000 | Loss: 0.00001234
Iteration 4/1000 | Loss: 0.00001092
Iteration 5/1000 | Loss: 0.00001029
Iteration 6/1000 | Loss: 0.00000985
Iteration 7/1000 | Loss: 0.00000956
Iteration 8/1000 | Loss: 0.00000951
Iteration 9/1000 | Loss: 0.00000949
Iteration 10/1000 | Loss: 0.00000943
Iteration 11/1000 | Loss: 0.00000941
Iteration 12/1000 | Loss: 0.00000941
Iteration 13/1000 | Loss: 0.00000939
Iteration 14/1000 | Loss: 0.00000938
Iteration 15/1000 | Loss: 0.00000930
Iteration 16/1000 | Loss: 0.00000930
Iteration 17/1000 | Loss: 0.00000928
Iteration 18/1000 | Loss: 0.00000922
Iteration 19/1000 | Loss: 0.00000922
Iteration 20/1000 | Loss: 0.00000918
Iteration 21/1000 | Loss: 0.00000918
Iteration 22/1000 | Loss: 0.00000918
Iteration 23/1000 | Loss: 0.00000918
Iteration 24/1000 | Loss: 0.00000918
Iteration 25/1000 | Loss: 0.00000918
Iteration 26/1000 | Loss: 0.00000918
Iteration 27/1000 | Loss: 0.00000916
Iteration 28/1000 | Loss: 0.00000915
Iteration 29/1000 | Loss: 0.00000913
Iteration 30/1000 | Loss: 0.00000913
Iteration 31/1000 | Loss: 0.00000911
Iteration 32/1000 | Loss: 0.00000911
Iteration 33/1000 | Loss: 0.00000910
Iteration 34/1000 | Loss: 0.00000909
Iteration 35/1000 | Loss: 0.00000909
Iteration 36/1000 | Loss: 0.00000908
Iteration 37/1000 | Loss: 0.00000908
Iteration 38/1000 | Loss: 0.00000908
Iteration 39/1000 | Loss: 0.00000908
Iteration 40/1000 | Loss: 0.00000908
Iteration 41/1000 | Loss: 0.00000908
Iteration 42/1000 | Loss: 0.00000908
Iteration 43/1000 | Loss: 0.00000908
Iteration 44/1000 | Loss: 0.00000908
Iteration 45/1000 | Loss: 0.00000905
Iteration 46/1000 | Loss: 0.00000905
Iteration 47/1000 | Loss: 0.00000904
Iteration 48/1000 | Loss: 0.00000904
Iteration 49/1000 | Loss: 0.00000903
Iteration 50/1000 | Loss: 0.00000902
Iteration 51/1000 | Loss: 0.00000902
Iteration 52/1000 | Loss: 0.00000902
Iteration 53/1000 | Loss: 0.00000901
Iteration 54/1000 | Loss: 0.00000901
Iteration 55/1000 | Loss: 0.00000900
Iteration 56/1000 | Loss: 0.00000900
Iteration 57/1000 | Loss: 0.00000899
Iteration 58/1000 | Loss: 0.00000899
Iteration 59/1000 | Loss: 0.00000898
Iteration 60/1000 | Loss: 0.00000898
Iteration 61/1000 | Loss: 0.00000897
Iteration 62/1000 | Loss: 0.00000897
Iteration 63/1000 | Loss: 0.00000896
Iteration 64/1000 | Loss: 0.00000896
Iteration 65/1000 | Loss: 0.00000895
Iteration 66/1000 | Loss: 0.00000895
Iteration 67/1000 | Loss: 0.00000895
Iteration 68/1000 | Loss: 0.00000895
Iteration 69/1000 | Loss: 0.00000894
Iteration 70/1000 | Loss: 0.00000893
Iteration 71/1000 | Loss: 0.00000893
Iteration 72/1000 | Loss: 0.00000892
Iteration 73/1000 | Loss: 0.00000892
Iteration 74/1000 | Loss: 0.00000891
Iteration 75/1000 | Loss: 0.00000891
Iteration 76/1000 | Loss: 0.00000891
Iteration 77/1000 | Loss: 0.00000890
Iteration 78/1000 | Loss: 0.00000890
Iteration 79/1000 | Loss: 0.00000889
Iteration 80/1000 | Loss: 0.00000889
Iteration 81/1000 | Loss: 0.00000889
Iteration 82/1000 | Loss: 0.00000889
Iteration 83/1000 | Loss: 0.00000888
Iteration 84/1000 | Loss: 0.00000888
Iteration 85/1000 | Loss: 0.00000888
Iteration 86/1000 | Loss: 0.00000888
Iteration 87/1000 | Loss: 0.00000888
Iteration 88/1000 | Loss: 0.00000888
Iteration 89/1000 | Loss: 0.00000887
Iteration 90/1000 | Loss: 0.00000887
Iteration 91/1000 | Loss: 0.00000887
Iteration 92/1000 | Loss: 0.00000887
Iteration 93/1000 | Loss: 0.00000886
Iteration 94/1000 | Loss: 0.00000886
Iteration 95/1000 | Loss: 0.00000886
Iteration 96/1000 | Loss: 0.00000886
Iteration 97/1000 | Loss: 0.00000885
Iteration 98/1000 | Loss: 0.00000885
Iteration 99/1000 | Loss: 0.00000885
Iteration 100/1000 | Loss: 0.00000884
Iteration 101/1000 | Loss: 0.00000884
Iteration 102/1000 | Loss: 0.00000884
Iteration 103/1000 | Loss: 0.00000884
Iteration 104/1000 | Loss: 0.00000884
Iteration 105/1000 | Loss: 0.00000883
Iteration 106/1000 | Loss: 0.00000883
Iteration 107/1000 | Loss: 0.00000883
Iteration 108/1000 | Loss: 0.00000882
Iteration 109/1000 | Loss: 0.00000882
Iteration 110/1000 | Loss: 0.00000882
Iteration 111/1000 | Loss: 0.00000882
Iteration 112/1000 | Loss: 0.00000882
Iteration 113/1000 | Loss: 0.00000882
Iteration 114/1000 | Loss: 0.00000882
Iteration 115/1000 | Loss: 0.00000882
Iteration 116/1000 | Loss: 0.00000881
Iteration 117/1000 | Loss: 0.00000881
Iteration 118/1000 | Loss: 0.00000881
Iteration 119/1000 | Loss: 0.00000881
Iteration 120/1000 | Loss: 0.00000881
Iteration 121/1000 | Loss: 0.00000881
Iteration 122/1000 | Loss: 0.00000881
Iteration 123/1000 | Loss: 0.00000881
Iteration 124/1000 | Loss: 0.00000881
Iteration 125/1000 | Loss: 0.00000881
Iteration 126/1000 | Loss: 0.00000881
Iteration 127/1000 | Loss: 0.00000881
Iteration 128/1000 | Loss: 0.00000880
Iteration 129/1000 | Loss: 0.00000880
Iteration 130/1000 | Loss: 0.00000880
Iteration 131/1000 | Loss: 0.00000880
Iteration 132/1000 | Loss: 0.00000880
Iteration 133/1000 | Loss: 0.00000880
Iteration 134/1000 | Loss: 0.00000880
Iteration 135/1000 | Loss: 0.00000880
Iteration 136/1000 | Loss: 0.00000880
Iteration 137/1000 | Loss: 0.00000880
Iteration 138/1000 | Loss: 0.00000880
Iteration 139/1000 | Loss: 0.00000880
Iteration 140/1000 | Loss: 0.00000880
Iteration 141/1000 | Loss: 0.00000880
Iteration 142/1000 | Loss: 0.00000880
Iteration 143/1000 | Loss: 0.00000880
Iteration 144/1000 | Loss: 0.00000880
Iteration 145/1000 | Loss: 0.00000880
Iteration 146/1000 | Loss: 0.00000880
Iteration 147/1000 | Loss: 0.00000880
Iteration 148/1000 | Loss: 0.00000880
Iteration 149/1000 | Loss: 0.00000880
Iteration 150/1000 | Loss: 0.00000880
Iteration 151/1000 | Loss: 0.00000880
Iteration 152/1000 | Loss: 0.00000880
Iteration 153/1000 | Loss: 0.00000880
Iteration 154/1000 | Loss: 0.00000880
Iteration 155/1000 | Loss: 0.00000880
Iteration 156/1000 | Loss: 0.00000880
Iteration 157/1000 | Loss: 0.00000880
Iteration 158/1000 | Loss: 0.00000880
Iteration 159/1000 | Loss: 0.00000880
Iteration 160/1000 | Loss: 0.00000880
Iteration 161/1000 | Loss: 0.00000880
Iteration 162/1000 | Loss: 0.00000880
Iteration 163/1000 | Loss: 0.00000880
Iteration 164/1000 | Loss: 0.00000880
Iteration 165/1000 | Loss: 0.00000880
Iteration 166/1000 | Loss: 0.00000880
Iteration 167/1000 | Loss: 0.00000880
Iteration 168/1000 | Loss: 0.00000880
Iteration 169/1000 | Loss: 0.00000880
Iteration 170/1000 | Loss: 0.00000880
Iteration 171/1000 | Loss: 0.00000880
Iteration 172/1000 | Loss: 0.00000880
Iteration 173/1000 | Loss: 0.00000880
Iteration 174/1000 | Loss: 0.00000880
Iteration 175/1000 | Loss: 0.00000880
Iteration 176/1000 | Loss: 0.00000880
Iteration 177/1000 | Loss: 0.00000880
Iteration 178/1000 | Loss: 0.00000880
Iteration 179/1000 | Loss: 0.00000880
Iteration 180/1000 | Loss: 0.00000880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [8.801367584965192e-06, 8.801367584965192e-06, 8.801367584965192e-06, 8.801367584965192e-06, 8.801367584965192e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.801367584965192e-06

Optimization complete. Final v2v error: 2.5175998210906982 mm

Highest mean error: 2.8212411403656006 mm for frame 167

Lowest mean error: 2.162144660949707 mm for frame 0

Saving results

Total time: 34.81362533569336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037192
Iteration 2/25 | Loss: 0.00178117
Iteration 3/25 | Loss: 0.00133911
Iteration 4/25 | Loss: 0.00136740
Iteration 5/25 | Loss: 0.00121823
Iteration 6/25 | Loss: 0.00116327
Iteration 7/25 | Loss: 0.00109310
Iteration 8/25 | Loss: 0.00107812
Iteration 9/25 | Loss: 0.00103309
Iteration 10/25 | Loss: 0.00103374
Iteration 11/25 | Loss: 0.00101233
Iteration 12/25 | Loss: 0.00101129
Iteration 13/25 | Loss: 0.00099622
Iteration 14/25 | Loss: 0.00099918
Iteration 15/25 | Loss: 0.00099636
Iteration 16/25 | Loss: 0.00099094
Iteration 17/25 | Loss: 0.00098317
Iteration 18/25 | Loss: 0.00097705
Iteration 19/25 | Loss: 0.00097153
Iteration 20/25 | Loss: 0.00096279
Iteration 21/25 | Loss: 0.00096643
Iteration 22/25 | Loss: 0.00096757
Iteration 23/25 | Loss: 0.00096711
Iteration 24/25 | Loss: 0.00096580
Iteration 25/25 | Loss: 0.00096669

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36465561
Iteration 2/25 | Loss: 0.00083808
Iteration 3/25 | Loss: 0.00083807
Iteration 4/25 | Loss: 0.00083807
Iteration 5/25 | Loss: 0.00083807
Iteration 6/25 | Loss: 0.00083807
Iteration 7/25 | Loss: 0.00083807
Iteration 8/25 | Loss: 0.00083807
Iteration 9/25 | Loss: 0.00083807
Iteration 10/25 | Loss: 0.00083807
Iteration 11/25 | Loss: 0.00083807
Iteration 12/25 | Loss: 0.00083807
Iteration 13/25 | Loss: 0.00083807
Iteration 14/25 | Loss: 0.00083807
Iteration 15/25 | Loss: 0.00083807
Iteration 16/25 | Loss: 0.00083807
Iteration 17/25 | Loss: 0.00083807
Iteration 18/25 | Loss: 0.00083807
Iteration 19/25 | Loss: 0.00083807
Iteration 20/25 | Loss: 0.00083807
Iteration 21/25 | Loss: 0.00083807
Iteration 22/25 | Loss: 0.00083807
Iteration 23/25 | Loss: 0.00083807
Iteration 24/25 | Loss: 0.00083807
Iteration 25/25 | Loss: 0.00083807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083807
Iteration 2/1000 | Loss: 0.00006591
Iteration 3/1000 | Loss: 0.00025617
Iteration 4/1000 | Loss: 0.00056818
Iteration 5/1000 | Loss: 0.00037719
Iteration 6/1000 | Loss: 0.00018706
Iteration 7/1000 | Loss: 0.00033188
Iteration 8/1000 | Loss: 0.00022537
Iteration 9/1000 | Loss: 0.00023014
Iteration 10/1000 | Loss: 0.00015171
Iteration 11/1000 | Loss: 0.00006582
Iteration 12/1000 | Loss: 0.00018228
Iteration 13/1000 | Loss: 0.00013800
Iteration 14/1000 | Loss: 0.00016475
Iteration 15/1000 | Loss: 0.00019478
Iteration 16/1000 | Loss: 0.00016819
Iteration 17/1000 | Loss: 0.00011256
Iteration 18/1000 | Loss: 0.00015724
Iteration 19/1000 | Loss: 0.00019408
Iteration 20/1000 | Loss: 0.00012968
Iteration 21/1000 | Loss: 0.00010230
Iteration 22/1000 | Loss: 0.00010786
Iteration 23/1000 | Loss: 0.00009549
Iteration 24/1000 | Loss: 0.00017567
Iteration 25/1000 | Loss: 0.00010556
Iteration 26/1000 | Loss: 0.00015604
Iteration 27/1000 | Loss: 0.00010518
Iteration 28/1000 | Loss: 0.00014210
Iteration 29/1000 | Loss: 0.00042091
Iteration 30/1000 | Loss: 0.00019376
Iteration 31/1000 | Loss: 0.00024469
Iteration 32/1000 | Loss: 0.00022181
Iteration 33/1000 | Loss: 0.00016253
Iteration 34/1000 | Loss: 0.00021315
Iteration 35/1000 | Loss: 0.00006041
Iteration 36/1000 | Loss: 0.00018444
Iteration 37/1000 | Loss: 0.00028500
Iteration 38/1000 | Loss: 0.00009378
Iteration 39/1000 | Loss: 0.00003007
Iteration 40/1000 | Loss: 0.00020353
Iteration 41/1000 | Loss: 0.00016098
Iteration 42/1000 | Loss: 0.00008805
Iteration 43/1000 | Loss: 0.00014703
Iteration 44/1000 | Loss: 0.00012047
Iteration 45/1000 | Loss: 0.00008028
Iteration 46/1000 | Loss: 0.00009068
Iteration 47/1000 | Loss: 0.00024479
Iteration 48/1000 | Loss: 0.00010331
Iteration 49/1000 | Loss: 0.00030984
Iteration 50/1000 | Loss: 0.00020480
Iteration 51/1000 | Loss: 0.00002320
Iteration 52/1000 | Loss: 0.00004457
Iteration 53/1000 | Loss: 0.00016645
Iteration 54/1000 | Loss: 0.00030416
Iteration 55/1000 | Loss: 0.00022924
Iteration 56/1000 | Loss: 0.00017232
Iteration 57/1000 | Loss: 0.00011948
Iteration 58/1000 | Loss: 0.00031462
Iteration 59/1000 | Loss: 0.00020436
Iteration 60/1000 | Loss: 0.00019031
Iteration 61/1000 | Loss: 0.00020038
Iteration 62/1000 | Loss: 0.00013430
Iteration 63/1000 | Loss: 0.00014768
Iteration 64/1000 | Loss: 0.00019732
Iteration 65/1000 | Loss: 0.00014643
Iteration 66/1000 | Loss: 0.00014691
Iteration 67/1000 | Loss: 0.00009429
Iteration 68/1000 | Loss: 0.00028403
Iteration 69/1000 | Loss: 0.00029378
Iteration 70/1000 | Loss: 0.00014995
Iteration 71/1000 | Loss: 0.00002587
Iteration 72/1000 | Loss: 0.00013684
Iteration 73/1000 | Loss: 0.00011146
Iteration 74/1000 | Loss: 0.00013742
Iteration 75/1000 | Loss: 0.00018665
Iteration 76/1000 | Loss: 0.00014863
Iteration 77/1000 | Loss: 0.00017540
Iteration 78/1000 | Loss: 0.00014214
Iteration 79/1000 | Loss: 0.00028105
Iteration 80/1000 | Loss: 0.00019117
Iteration 81/1000 | Loss: 0.00002557
Iteration 82/1000 | Loss: 0.00017342
Iteration 83/1000 | Loss: 0.00024663
Iteration 84/1000 | Loss: 0.00012750
Iteration 85/1000 | Loss: 0.00018417
Iteration 86/1000 | Loss: 0.00022432
Iteration 87/1000 | Loss: 0.00022635
Iteration 88/1000 | Loss: 0.00015286
Iteration 89/1000 | Loss: 0.00025745
Iteration 90/1000 | Loss: 0.00017734
Iteration 91/1000 | Loss: 0.00021003
Iteration 92/1000 | Loss: 0.00017662
Iteration 93/1000 | Loss: 0.00020025
Iteration 94/1000 | Loss: 0.00003548
Iteration 95/1000 | Loss: 0.00012105
Iteration 96/1000 | Loss: 0.00005968
Iteration 97/1000 | Loss: 0.00022907
Iteration 98/1000 | Loss: 0.00003239
Iteration 99/1000 | Loss: 0.00024767
Iteration 100/1000 | Loss: 0.00023511
Iteration 101/1000 | Loss: 0.00012440
Iteration 102/1000 | Loss: 0.00013889
Iteration 103/1000 | Loss: 0.00024468
Iteration 104/1000 | Loss: 0.00081124
Iteration 105/1000 | Loss: 0.00021400
Iteration 106/1000 | Loss: 0.00040351
Iteration 107/1000 | Loss: 0.00008816
Iteration 108/1000 | Loss: 0.00056984
Iteration 109/1000 | Loss: 0.00025721
Iteration 110/1000 | Loss: 0.00002399
Iteration 111/1000 | Loss: 0.00022812
Iteration 112/1000 | Loss: 0.00032107
Iteration 113/1000 | Loss: 0.00035570
Iteration 114/1000 | Loss: 0.00026547
Iteration 115/1000 | Loss: 0.00030458
Iteration 116/1000 | Loss: 0.00027695
Iteration 117/1000 | Loss: 0.00002656
Iteration 118/1000 | Loss: 0.00025392
Iteration 119/1000 | Loss: 0.00001978
Iteration 120/1000 | Loss: 0.00001730
Iteration 121/1000 | Loss: 0.00009820
Iteration 122/1000 | Loss: 0.00001616
Iteration 123/1000 | Loss: 0.00001586
Iteration 124/1000 | Loss: 0.00001561
Iteration 125/1000 | Loss: 0.00001541
Iteration 126/1000 | Loss: 0.00001514
Iteration 127/1000 | Loss: 0.00063119
Iteration 128/1000 | Loss: 0.00031404
Iteration 129/1000 | Loss: 0.00011835
Iteration 130/1000 | Loss: 0.00049246
Iteration 131/1000 | Loss: 0.00006675
Iteration 132/1000 | Loss: 0.00004205
Iteration 133/1000 | Loss: 0.00007458
Iteration 134/1000 | Loss: 0.00003531
Iteration 135/1000 | Loss: 0.00003774
Iteration 136/1000 | Loss: 0.00002642
Iteration 137/1000 | Loss: 0.00001673
Iteration 138/1000 | Loss: 0.00001440
Iteration 139/1000 | Loss: 0.00001884
Iteration 140/1000 | Loss: 0.00006371
Iteration 141/1000 | Loss: 0.00001270
Iteration 142/1000 | Loss: 0.00001221
Iteration 143/1000 | Loss: 0.00001198
Iteration 144/1000 | Loss: 0.00001190
Iteration 145/1000 | Loss: 0.00001175
Iteration 146/1000 | Loss: 0.00001171
Iteration 147/1000 | Loss: 0.00001166
Iteration 148/1000 | Loss: 0.00001157
Iteration 149/1000 | Loss: 0.00001157
Iteration 150/1000 | Loss: 0.00001157
Iteration 151/1000 | Loss: 0.00001156
Iteration 152/1000 | Loss: 0.00001156
Iteration 153/1000 | Loss: 0.00001155
Iteration 154/1000 | Loss: 0.00001155
Iteration 155/1000 | Loss: 0.00001154
Iteration 156/1000 | Loss: 0.00001153
Iteration 157/1000 | Loss: 0.00001153
Iteration 158/1000 | Loss: 0.00001153
Iteration 159/1000 | Loss: 0.00001153
Iteration 160/1000 | Loss: 0.00001151
Iteration 161/1000 | Loss: 0.00001150
Iteration 162/1000 | Loss: 0.00011499
Iteration 163/1000 | Loss: 0.00002302
Iteration 164/1000 | Loss: 0.00001625
Iteration 165/1000 | Loss: 0.00004715
Iteration 166/1000 | Loss: 0.00001157
Iteration 167/1000 | Loss: 0.00002313
Iteration 168/1000 | Loss: 0.00001288
Iteration 169/1000 | Loss: 0.00001174
Iteration 170/1000 | Loss: 0.00001140
Iteration 171/1000 | Loss: 0.00001140
Iteration 172/1000 | Loss: 0.00001139
Iteration 173/1000 | Loss: 0.00001139
Iteration 174/1000 | Loss: 0.00001139
Iteration 175/1000 | Loss: 0.00001139
Iteration 176/1000 | Loss: 0.00001139
Iteration 177/1000 | Loss: 0.00001139
Iteration 178/1000 | Loss: 0.00001138
Iteration 179/1000 | Loss: 0.00001138
Iteration 180/1000 | Loss: 0.00001138
Iteration 181/1000 | Loss: 0.00001137
Iteration 182/1000 | Loss: 0.00001136
Iteration 183/1000 | Loss: 0.00001135
Iteration 184/1000 | Loss: 0.00001135
Iteration 185/1000 | Loss: 0.00001135
Iteration 186/1000 | Loss: 0.00001134
Iteration 187/1000 | Loss: 0.00001134
Iteration 188/1000 | Loss: 0.00001134
Iteration 189/1000 | Loss: 0.00001133
Iteration 190/1000 | Loss: 0.00001133
Iteration 191/1000 | Loss: 0.00001133
Iteration 192/1000 | Loss: 0.00001133
Iteration 193/1000 | Loss: 0.00001132
Iteration 194/1000 | Loss: 0.00001132
Iteration 195/1000 | Loss: 0.00001132
Iteration 196/1000 | Loss: 0.00001132
Iteration 197/1000 | Loss: 0.00001132
Iteration 198/1000 | Loss: 0.00001132
Iteration 199/1000 | Loss: 0.00001132
Iteration 200/1000 | Loss: 0.00001132
Iteration 201/1000 | Loss: 0.00001132
Iteration 202/1000 | Loss: 0.00001131
Iteration 203/1000 | Loss: 0.00001131
Iteration 204/1000 | Loss: 0.00001130
Iteration 205/1000 | Loss: 0.00001130
Iteration 206/1000 | Loss: 0.00001130
Iteration 207/1000 | Loss: 0.00001129
Iteration 208/1000 | Loss: 0.00001129
Iteration 209/1000 | Loss: 0.00001129
Iteration 210/1000 | Loss: 0.00001129
Iteration 211/1000 | Loss: 0.00001129
Iteration 212/1000 | Loss: 0.00001129
Iteration 213/1000 | Loss: 0.00001128
Iteration 214/1000 | Loss: 0.00001128
Iteration 215/1000 | Loss: 0.00001127
Iteration 216/1000 | Loss: 0.00001127
Iteration 217/1000 | Loss: 0.00001127
Iteration 218/1000 | Loss: 0.00001127
Iteration 219/1000 | Loss: 0.00001127
Iteration 220/1000 | Loss: 0.00001127
Iteration 221/1000 | Loss: 0.00001127
Iteration 222/1000 | Loss: 0.00001127
Iteration 223/1000 | Loss: 0.00001126
Iteration 224/1000 | Loss: 0.00001126
Iteration 225/1000 | Loss: 0.00001126
Iteration 226/1000 | Loss: 0.00001126
Iteration 227/1000 | Loss: 0.00001126
Iteration 228/1000 | Loss: 0.00001125
Iteration 229/1000 | Loss: 0.00001125
Iteration 230/1000 | Loss: 0.00001125
Iteration 231/1000 | Loss: 0.00001125
Iteration 232/1000 | Loss: 0.00001125
Iteration 233/1000 | Loss: 0.00001125
Iteration 234/1000 | Loss: 0.00001125
Iteration 235/1000 | Loss: 0.00001125
Iteration 236/1000 | Loss: 0.00001125
Iteration 237/1000 | Loss: 0.00001125
Iteration 238/1000 | Loss: 0.00001125
Iteration 239/1000 | Loss: 0.00001125
Iteration 240/1000 | Loss: 0.00001125
Iteration 241/1000 | Loss: 0.00001125
Iteration 242/1000 | Loss: 0.00001125
Iteration 243/1000 | Loss: 0.00001125
Iteration 244/1000 | Loss: 0.00001125
Iteration 245/1000 | Loss: 0.00001125
Iteration 246/1000 | Loss: 0.00001125
Iteration 247/1000 | Loss: 0.00001125
Iteration 248/1000 | Loss: 0.00001125
Iteration 249/1000 | Loss: 0.00001125
Iteration 250/1000 | Loss: 0.00001125
Iteration 251/1000 | Loss: 0.00001125
Iteration 252/1000 | Loss: 0.00001125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [1.1252670446992852e-05, 1.1252670446992852e-05, 1.1252670446992852e-05, 1.1252670446992852e-05, 1.1252670446992852e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1252670446992852e-05

Optimization complete. Final v2v error: 2.848860263824463 mm

Highest mean error: 3.8722214698791504 mm for frame 39

Lowest mean error: 2.378788709640503 mm for frame 15

Saving results

Total time: 263.1467046737671
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862787
Iteration 2/25 | Loss: 0.00144697
Iteration 3/25 | Loss: 0.00104792
Iteration 4/25 | Loss: 0.00100843
Iteration 5/25 | Loss: 0.00100278
Iteration 6/25 | Loss: 0.00100218
Iteration 7/25 | Loss: 0.00100218
Iteration 8/25 | Loss: 0.00100213
Iteration 9/25 | Loss: 0.00100213
Iteration 10/25 | Loss: 0.00100213
Iteration 11/25 | Loss: 0.00100211
Iteration 12/25 | Loss: 0.00100211
Iteration 13/25 | Loss: 0.00100211
Iteration 14/25 | Loss: 0.00100211
Iteration 15/25 | Loss: 0.00100211
Iteration 16/25 | Loss: 0.00100211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010021079797297716, 0.0010021079797297716, 0.0010021079797297716, 0.0010021079797297716, 0.0010021079797297716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010021079797297716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13974714
Iteration 2/25 | Loss: 0.00047647
Iteration 3/25 | Loss: 0.00047647
Iteration 4/25 | Loss: 0.00047647
Iteration 5/25 | Loss: 0.00047647
Iteration 6/25 | Loss: 0.00047647
Iteration 7/25 | Loss: 0.00047647
Iteration 8/25 | Loss: 0.00047647
Iteration 9/25 | Loss: 0.00047647
Iteration 10/25 | Loss: 0.00047647
Iteration 11/25 | Loss: 0.00047647
Iteration 12/25 | Loss: 0.00047647
Iteration 13/25 | Loss: 0.00047647
Iteration 14/25 | Loss: 0.00047647
Iteration 15/25 | Loss: 0.00047647
Iteration 16/25 | Loss: 0.00047647
Iteration 17/25 | Loss: 0.00047647
Iteration 18/25 | Loss: 0.00047647
Iteration 19/25 | Loss: 0.00047647
Iteration 20/25 | Loss: 0.00047647
Iteration 21/25 | Loss: 0.00047647
Iteration 22/25 | Loss: 0.00047647
Iteration 23/25 | Loss: 0.00047647
Iteration 24/25 | Loss: 0.00047647
Iteration 25/25 | Loss: 0.00047647

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047647
Iteration 2/1000 | Loss: 0.00004107
Iteration 3/1000 | Loss: 0.00002501
Iteration 4/1000 | Loss: 0.00002001
Iteration 5/1000 | Loss: 0.00001867
Iteration 6/1000 | Loss: 0.00001791
Iteration 7/1000 | Loss: 0.00001747
Iteration 8/1000 | Loss: 0.00001716
Iteration 9/1000 | Loss: 0.00001683
Iteration 10/1000 | Loss: 0.00001675
Iteration 11/1000 | Loss: 0.00001656
Iteration 12/1000 | Loss: 0.00001651
Iteration 13/1000 | Loss: 0.00001647
Iteration 14/1000 | Loss: 0.00001639
Iteration 15/1000 | Loss: 0.00001638
Iteration 16/1000 | Loss: 0.00001624
Iteration 17/1000 | Loss: 0.00001622
Iteration 18/1000 | Loss: 0.00001620
Iteration 19/1000 | Loss: 0.00001620
Iteration 20/1000 | Loss: 0.00001612
Iteration 21/1000 | Loss: 0.00001612
Iteration 22/1000 | Loss: 0.00001611
Iteration 23/1000 | Loss: 0.00001609
Iteration 24/1000 | Loss: 0.00001608
Iteration 25/1000 | Loss: 0.00001608
Iteration 26/1000 | Loss: 0.00001607
Iteration 27/1000 | Loss: 0.00001607
Iteration 28/1000 | Loss: 0.00001606
Iteration 29/1000 | Loss: 0.00001605
Iteration 30/1000 | Loss: 0.00001605
Iteration 31/1000 | Loss: 0.00001605
Iteration 32/1000 | Loss: 0.00001605
Iteration 33/1000 | Loss: 0.00001605
Iteration 34/1000 | Loss: 0.00001605
Iteration 35/1000 | Loss: 0.00001604
Iteration 36/1000 | Loss: 0.00001604
Iteration 37/1000 | Loss: 0.00001603
Iteration 38/1000 | Loss: 0.00001603
Iteration 39/1000 | Loss: 0.00001603
Iteration 40/1000 | Loss: 0.00001602
Iteration 41/1000 | Loss: 0.00001602
Iteration 42/1000 | Loss: 0.00001602
Iteration 43/1000 | Loss: 0.00001602
Iteration 44/1000 | Loss: 0.00001601
Iteration 45/1000 | Loss: 0.00001601
Iteration 46/1000 | Loss: 0.00001601
Iteration 47/1000 | Loss: 0.00001600
Iteration 48/1000 | Loss: 0.00001600
Iteration 49/1000 | Loss: 0.00001600
Iteration 50/1000 | Loss: 0.00001599
Iteration 51/1000 | Loss: 0.00001599
Iteration 52/1000 | Loss: 0.00001599
Iteration 53/1000 | Loss: 0.00001599
Iteration 54/1000 | Loss: 0.00001599
Iteration 55/1000 | Loss: 0.00001598
Iteration 56/1000 | Loss: 0.00001598
Iteration 57/1000 | Loss: 0.00001598
Iteration 58/1000 | Loss: 0.00001598
Iteration 59/1000 | Loss: 0.00001598
Iteration 60/1000 | Loss: 0.00001598
Iteration 61/1000 | Loss: 0.00001597
Iteration 62/1000 | Loss: 0.00001597
Iteration 63/1000 | Loss: 0.00001597
Iteration 64/1000 | Loss: 0.00001597
Iteration 65/1000 | Loss: 0.00001597
Iteration 66/1000 | Loss: 0.00001597
Iteration 67/1000 | Loss: 0.00001597
Iteration 68/1000 | Loss: 0.00001597
Iteration 69/1000 | Loss: 0.00001596
Iteration 70/1000 | Loss: 0.00001596
Iteration 71/1000 | Loss: 0.00001596
Iteration 72/1000 | Loss: 0.00001596
Iteration 73/1000 | Loss: 0.00001596
Iteration 74/1000 | Loss: 0.00001596
Iteration 75/1000 | Loss: 0.00001596
Iteration 76/1000 | Loss: 0.00001596
Iteration 77/1000 | Loss: 0.00001596
Iteration 78/1000 | Loss: 0.00001596
Iteration 79/1000 | Loss: 0.00001595
Iteration 80/1000 | Loss: 0.00001595
Iteration 81/1000 | Loss: 0.00001595
Iteration 82/1000 | Loss: 0.00001595
Iteration 83/1000 | Loss: 0.00001595
Iteration 84/1000 | Loss: 0.00001595
Iteration 85/1000 | Loss: 0.00001594
Iteration 86/1000 | Loss: 0.00001594
Iteration 87/1000 | Loss: 0.00001594
Iteration 88/1000 | Loss: 0.00001594
Iteration 89/1000 | Loss: 0.00001594
Iteration 90/1000 | Loss: 0.00001593
Iteration 91/1000 | Loss: 0.00001593
Iteration 92/1000 | Loss: 0.00001593
Iteration 93/1000 | Loss: 0.00001593
Iteration 94/1000 | Loss: 0.00001593
Iteration 95/1000 | Loss: 0.00001593
Iteration 96/1000 | Loss: 0.00001593
Iteration 97/1000 | Loss: 0.00001593
Iteration 98/1000 | Loss: 0.00001593
Iteration 99/1000 | Loss: 0.00001593
Iteration 100/1000 | Loss: 0.00001593
Iteration 101/1000 | Loss: 0.00001593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.5927354979794472e-05, 1.5927354979794472e-05, 1.5927354979794472e-05, 1.5927354979794472e-05, 1.5927354979794472e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5927354979794472e-05

Optimization complete. Final v2v error: 3.3754348754882812 mm

Highest mean error: 3.6835920810699463 mm for frame 100

Lowest mean error: 2.928925037384033 mm for frame 230

Saving results

Total time: 37.3302583694458
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417549
Iteration 2/25 | Loss: 0.00105745
Iteration 3/25 | Loss: 0.00094414
Iteration 4/25 | Loss: 0.00092708
Iteration 5/25 | Loss: 0.00092279
Iteration 6/25 | Loss: 0.00092127
Iteration 7/25 | Loss: 0.00092064
Iteration 8/25 | Loss: 0.00092054
Iteration 9/25 | Loss: 0.00092054
Iteration 10/25 | Loss: 0.00092054
Iteration 11/25 | Loss: 0.00092054
Iteration 12/25 | Loss: 0.00092054
Iteration 13/25 | Loss: 0.00092054
Iteration 14/25 | Loss: 0.00092054
Iteration 15/25 | Loss: 0.00092054
Iteration 16/25 | Loss: 0.00092054
Iteration 17/25 | Loss: 0.00092054
Iteration 18/25 | Loss: 0.00092054
Iteration 19/25 | Loss: 0.00092054
Iteration 20/25 | Loss: 0.00092054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000920542748644948, 0.000920542748644948, 0.000920542748644948, 0.000920542748644948, 0.000920542748644948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000920542748644948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34788144
Iteration 2/25 | Loss: 0.00080320
Iteration 3/25 | Loss: 0.00080320
Iteration 4/25 | Loss: 0.00080320
Iteration 5/25 | Loss: 0.00080320
Iteration 6/25 | Loss: 0.00080320
Iteration 7/25 | Loss: 0.00080320
Iteration 8/25 | Loss: 0.00080319
Iteration 9/25 | Loss: 0.00080319
Iteration 10/25 | Loss: 0.00080319
Iteration 11/25 | Loss: 0.00080319
Iteration 12/25 | Loss: 0.00080319
Iteration 13/25 | Loss: 0.00080319
Iteration 14/25 | Loss: 0.00080319
Iteration 15/25 | Loss: 0.00080319
Iteration 16/25 | Loss: 0.00080319
Iteration 17/25 | Loss: 0.00080319
Iteration 18/25 | Loss: 0.00080319
Iteration 19/25 | Loss: 0.00080319
Iteration 20/25 | Loss: 0.00080319
Iteration 21/25 | Loss: 0.00080319
Iteration 22/25 | Loss: 0.00080319
Iteration 23/25 | Loss: 0.00080319
Iteration 24/25 | Loss: 0.00080319
Iteration 25/25 | Loss: 0.00080319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080319
Iteration 2/1000 | Loss: 0.00003104
Iteration 3/1000 | Loss: 0.00001930
Iteration 4/1000 | Loss: 0.00001771
Iteration 5/1000 | Loss: 0.00001709
Iteration 6/1000 | Loss: 0.00001683
Iteration 7/1000 | Loss: 0.00001678
Iteration 8/1000 | Loss: 0.00001662
Iteration 9/1000 | Loss: 0.00001645
Iteration 10/1000 | Loss: 0.00001645
Iteration 11/1000 | Loss: 0.00001642
Iteration 12/1000 | Loss: 0.00001640
Iteration 13/1000 | Loss: 0.00001633
Iteration 14/1000 | Loss: 0.00001632
Iteration 15/1000 | Loss: 0.00001627
Iteration 16/1000 | Loss: 0.00001622
Iteration 17/1000 | Loss: 0.00001622
Iteration 18/1000 | Loss: 0.00001620
Iteration 19/1000 | Loss: 0.00001620
Iteration 20/1000 | Loss: 0.00001620
Iteration 21/1000 | Loss: 0.00001620
Iteration 22/1000 | Loss: 0.00001619
Iteration 23/1000 | Loss: 0.00001619
Iteration 24/1000 | Loss: 0.00001619
Iteration 25/1000 | Loss: 0.00001619
Iteration 26/1000 | Loss: 0.00001618
Iteration 27/1000 | Loss: 0.00001617
Iteration 28/1000 | Loss: 0.00001617
Iteration 29/1000 | Loss: 0.00001617
Iteration 30/1000 | Loss: 0.00001616
Iteration 31/1000 | Loss: 0.00001616
Iteration 32/1000 | Loss: 0.00001616
Iteration 33/1000 | Loss: 0.00001616
Iteration 34/1000 | Loss: 0.00001616
Iteration 35/1000 | Loss: 0.00001616
Iteration 36/1000 | Loss: 0.00001616
Iteration 37/1000 | Loss: 0.00001616
Iteration 38/1000 | Loss: 0.00001616
Iteration 39/1000 | Loss: 0.00001616
Iteration 40/1000 | Loss: 0.00001616
Iteration 41/1000 | Loss: 0.00001616
Iteration 42/1000 | Loss: 0.00001616
Iteration 43/1000 | Loss: 0.00001616
Iteration 44/1000 | Loss: 0.00001616
Iteration 45/1000 | Loss: 0.00001616
Iteration 46/1000 | Loss: 0.00001616
Iteration 47/1000 | Loss: 0.00001616
Iteration 48/1000 | Loss: 0.00001616
Iteration 49/1000 | Loss: 0.00001616
Iteration 50/1000 | Loss: 0.00001616
Iteration 51/1000 | Loss: 0.00001616
Iteration 52/1000 | Loss: 0.00001616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 52. Stopping optimization.
Last 5 losses: [1.6156845958903432e-05, 1.6156845958903432e-05, 1.6156845958903432e-05, 1.6156845958903432e-05, 1.6156845958903432e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6156845958903432e-05

Optimization complete. Final v2v error: 3.2931909561157227 mm

Highest mean error: 4.144693851470947 mm for frame 22

Lowest mean error: 2.874671697616577 mm for frame 73

Saving results

Total time: 27.15866255760193
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832570
Iteration 2/25 | Loss: 0.00094946
Iteration 3/25 | Loss: 0.00086255
Iteration 4/25 | Loss: 0.00083813
Iteration 5/25 | Loss: 0.00083000
Iteration 6/25 | Loss: 0.00082839
Iteration 7/25 | Loss: 0.00082839
Iteration 8/25 | Loss: 0.00082839
Iteration 9/25 | Loss: 0.00082839
Iteration 10/25 | Loss: 0.00082839
Iteration 11/25 | Loss: 0.00082839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000828391348477453, 0.000828391348477453, 0.000828391348477453, 0.000828391348477453, 0.000828391348477453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000828391348477453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.55120087
Iteration 2/25 | Loss: 0.00096052
Iteration 3/25 | Loss: 0.00096047
Iteration 4/25 | Loss: 0.00096047
Iteration 5/25 | Loss: 0.00096047
Iteration 6/25 | Loss: 0.00096047
Iteration 7/25 | Loss: 0.00096047
Iteration 8/25 | Loss: 0.00096047
Iteration 9/25 | Loss: 0.00096047
Iteration 10/25 | Loss: 0.00096047
Iteration 11/25 | Loss: 0.00096047
Iteration 12/25 | Loss: 0.00096047
Iteration 13/25 | Loss: 0.00096047
Iteration 14/25 | Loss: 0.00096047
Iteration 15/25 | Loss: 0.00096047
Iteration 16/25 | Loss: 0.00096047
Iteration 17/25 | Loss: 0.00096047
Iteration 18/25 | Loss: 0.00096047
Iteration 19/25 | Loss: 0.00096047
Iteration 20/25 | Loss: 0.00096047
Iteration 21/25 | Loss: 0.00096047
Iteration 22/25 | Loss: 0.00096047
Iteration 23/25 | Loss: 0.00096047
Iteration 24/25 | Loss: 0.00096047
Iteration 25/25 | Loss: 0.00096047
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009604690712876618, 0.0009604690712876618, 0.0009604690712876618, 0.0009604690712876618, 0.0009604690712876618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009604690712876618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096047
Iteration 2/1000 | Loss: 0.00002534
Iteration 3/1000 | Loss: 0.00001298
Iteration 4/1000 | Loss: 0.00001121
Iteration 5/1000 | Loss: 0.00001067
Iteration 6/1000 | Loss: 0.00001039
Iteration 7/1000 | Loss: 0.00001011
Iteration 8/1000 | Loss: 0.00000990
Iteration 9/1000 | Loss: 0.00000978
Iteration 10/1000 | Loss: 0.00000965
Iteration 11/1000 | Loss: 0.00000965
Iteration 12/1000 | Loss: 0.00000961
Iteration 13/1000 | Loss: 0.00000955
Iteration 14/1000 | Loss: 0.00000950
Iteration 15/1000 | Loss: 0.00000950
Iteration 16/1000 | Loss: 0.00000950
Iteration 17/1000 | Loss: 0.00000949
Iteration 18/1000 | Loss: 0.00000949
Iteration 19/1000 | Loss: 0.00000945
Iteration 20/1000 | Loss: 0.00000945
Iteration 21/1000 | Loss: 0.00000945
Iteration 22/1000 | Loss: 0.00000945
Iteration 23/1000 | Loss: 0.00000945
Iteration 24/1000 | Loss: 0.00000944
Iteration 25/1000 | Loss: 0.00000944
Iteration 26/1000 | Loss: 0.00000944
Iteration 27/1000 | Loss: 0.00000944
Iteration 28/1000 | Loss: 0.00000943
Iteration 29/1000 | Loss: 0.00000943
Iteration 30/1000 | Loss: 0.00000942
Iteration 31/1000 | Loss: 0.00000942
Iteration 32/1000 | Loss: 0.00000941
Iteration 33/1000 | Loss: 0.00000941
Iteration 34/1000 | Loss: 0.00000941
Iteration 35/1000 | Loss: 0.00000940
Iteration 36/1000 | Loss: 0.00000940
Iteration 37/1000 | Loss: 0.00000940
Iteration 38/1000 | Loss: 0.00000940
Iteration 39/1000 | Loss: 0.00000940
Iteration 40/1000 | Loss: 0.00000939
Iteration 41/1000 | Loss: 0.00000939
Iteration 42/1000 | Loss: 0.00000939
Iteration 43/1000 | Loss: 0.00000938
Iteration 44/1000 | Loss: 0.00000938
Iteration 45/1000 | Loss: 0.00000937
Iteration 46/1000 | Loss: 0.00000937
Iteration 47/1000 | Loss: 0.00000937
Iteration 48/1000 | Loss: 0.00000937
Iteration 49/1000 | Loss: 0.00000937
Iteration 50/1000 | Loss: 0.00000937
Iteration 51/1000 | Loss: 0.00000936
Iteration 52/1000 | Loss: 0.00000936
Iteration 53/1000 | Loss: 0.00000936
Iteration 54/1000 | Loss: 0.00000936
Iteration 55/1000 | Loss: 0.00000935
Iteration 56/1000 | Loss: 0.00000935
Iteration 57/1000 | Loss: 0.00000935
Iteration 58/1000 | Loss: 0.00000935
Iteration 59/1000 | Loss: 0.00000935
Iteration 60/1000 | Loss: 0.00000935
Iteration 61/1000 | Loss: 0.00000935
Iteration 62/1000 | Loss: 0.00000935
Iteration 63/1000 | Loss: 0.00000934
Iteration 64/1000 | Loss: 0.00000934
Iteration 65/1000 | Loss: 0.00000934
Iteration 66/1000 | Loss: 0.00000934
Iteration 67/1000 | Loss: 0.00000934
Iteration 68/1000 | Loss: 0.00000934
Iteration 69/1000 | Loss: 0.00000934
Iteration 70/1000 | Loss: 0.00000934
Iteration 71/1000 | Loss: 0.00000934
Iteration 72/1000 | Loss: 0.00000933
Iteration 73/1000 | Loss: 0.00000933
Iteration 74/1000 | Loss: 0.00000933
Iteration 75/1000 | Loss: 0.00000933
Iteration 76/1000 | Loss: 0.00000933
Iteration 77/1000 | Loss: 0.00000933
Iteration 78/1000 | Loss: 0.00000933
Iteration 79/1000 | Loss: 0.00000933
Iteration 80/1000 | Loss: 0.00000933
Iteration 81/1000 | Loss: 0.00000933
Iteration 82/1000 | Loss: 0.00000933
Iteration 83/1000 | Loss: 0.00000933
Iteration 84/1000 | Loss: 0.00000933
Iteration 85/1000 | Loss: 0.00000933
Iteration 86/1000 | Loss: 0.00000933
Iteration 87/1000 | Loss: 0.00000933
Iteration 88/1000 | Loss: 0.00000932
Iteration 89/1000 | Loss: 0.00000932
Iteration 90/1000 | Loss: 0.00000932
Iteration 91/1000 | Loss: 0.00000932
Iteration 92/1000 | Loss: 0.00000932
Iteration 93/1000 | Loss: 0.00000932
Iteration 94/1000 | Loss: 0.00000932
Iteration 95/1000 | Loss: 0.00000932
Iteration 96/1000 | Loss: 0.00000932
Iteration 97/1000 | Loss: 0.00000932
Iteration 98/1000 | Loss: 0.00000932
Iteration 99/1000 | Loss: 0.00000932
Iteration 100/1000 | Loss: 0.00000932
Iteration 101/1000 | Loss: 0.00000931
Iteration 102/1000 | Loss: 0.00000931
Iteration 103/1000 | Loss: 0.00000931
Iteration 104/1000 | Loss: 0.00000931
Iteration 105/1000 | Loss: 0.00000930
Iteration 106/1000 | Loss: 0.00000930
Iteration 107/1000 | Loss: 0.00000930
Iteration 108/1000 | Loss: 0.00000929
Iteration 109/1000 | Loss: 0.00000929
Iteration 110/1000 | Loss: 0.00000929
Iteration 111/1000 | Loss: 0.00000929
Iteration 112/1000 | Loss: 0.00000929
Iteration 113/1000 | Loss: 0.00000929
Iteration 114/1000 | Loss: 0.00000929
Iteration 115/1000 | Loss: 0.00000929
Iteration 116/1000 | Loss: 0.00000929
Iteration 117/1000 | Loss: 0.00000929
Iteration 118/1000 | Loss: 0.00000929
Iteration 119/1000 | Loss: 0.00000929
Iteration 120/1000 | Loss: 0.00000929
Iteration 121/1000 | Loss: 0.00000928
Iteration 122/1000 | Loss: 0.00000928
Iteration 123/1000 | Loss: 0.00000928
Iteration 124/1000 | Loss: 0.00000928
Iteration 125/1000 | Loss: 0.00000927
Iteration 126/1000 | Loss: 0.00000927
Iteration 127/1000 | Loss: 0.00000927
Iteration 128/1000 | Loss: 0.00000927
Iteration 129/1000 | Loss: 0.00000927
Iteration 130/1000 | Loss: 0.00000927
Iteration 131/1000 | Loss: 0.00000927
Iteration 132/1000 | Loss: 0.00000927
Iteration 133/1000 | Loss: 0.00000927
Iteration 134/1000 | Loss: 0.00000927
Iteration 135/1000 | Loss: 0.00000927
Iteration 136/1000 | Loss: 0.00000927
Iteration 137/1000 | Loss: 0.00000927
Iteration 138/1000 | Loss: 0.00000927
Iteration 139/1000 | Loss: 0.00000926
Iteration 140/1000 | Loss: 0.00000926
Iteration 141/1000 | Loss: 0.00000926
Iteration 142/1000 | Loss: 0.00000926
Iteration 143/1000 | Loss: 0.00000926
Iteration 144/1000 | Loss: 0.00000925
Iteration 145/1000 | Loss: 0.00000925
Iteration 146/1000 | Loss: 0.00000925
Iteration 147/1000 | Loss: 0.00000925
Iteration 148/1000 | Loss: 0.00000924
Iteration 149/1000 | Loss: 0.00000924
Iteration 150/1000 | Loss: 0.00000924
Iteration 151/1000 | Loss: 0.00000924
Iteration 152/1000 | Loss: 0.00000924
Iteration 153/1000 | Loss: 0.00000924
Iteration 154/1000 | Loss: 0.00000924
Iteration 155/1000 | Loss: 0.00000924
Iteration 156/1000 | Loss: 0.00000924
Iteration 157/1000 | Loss: 0.00000924
Iteration 158/1000 | Loss: 0.00000924
Iteration 159/1000 | Loss: 0.00000924
Iteration 160/1000 | Loss: 0.00000924
Iteration 161/1000 | Loss: 0.00000924
Iteration 162/1000 | Loss: 0.00000924
Iteration 163/1000 | Loss: 0.00000924
Iteration 164/1000 | Loss: 0.00000924
Iteration 165/1000 | Loss: 0.00000924
Iteration 166/1000 | Loss: 0.00000924
Iteration 167/1000 | Loss: 0.00000924
Iteration 168/1000 | Loss: 0.00000924
Iteration 169/1000 | Loss: 0.00000924
Iteration 170/1000 | Loss: 0.00000924
Iteration 171/1000 | Loss: 0.00000924
Iteration 172/1000 | Loss: 0.00000924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [9.243522072210908e-06, 9.243522072210908e-06, 9.243522072210908e-06, 9.243522072210908e-06, 9.243522072210908e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.243522072210908e-06

Optimization complete. Final v2v error: 2.5809526443481445 mm

Highest mean error: 2.965667963027954 mm for frame 208

Lowest mean error: 2.265157461166382 mm for frame 45

Saving results

Total time: 39.930314779281616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00760432
Iteration 2/25 | Loss: 0.00127581
Iteration 3/25 | Loss: 0.00100124
Iteration 4/25 | Loss: 0.00097560
Iteration 5/25 | Loss: 0.00097215
Iteration 6/25 | Loss: 0.00097160
Iteration 7/25 | Loss: 0.00097160
Iteration 8/25 | Loss: 0.00097160
Iteration 9/25 | Loss: 0.00097160
Iteration 10/25 | Loss: 0.00097160
Iteration 11/25 | Loss: 0.00097160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000971602916251868, 0.000971602916251868, 0.000971602916251868, 0.000971602916251868, 0.000971602916251868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000971602916251868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.55474639
Iteration 2/25 | Loss: 0.00059704
Iteration 3/25 | Loss: 0.00059704
Iteration 4/25 | Loss: 0.00059704
Iteration 5/25 | Loss: 0.00059704
Iteration 6/25 | Loss: 0.00059704
Iteration 7/25 | Loss: 0.00059704
Iteration 8/25 | Loss: 0.00059704
Iteration 9/25 | Loss: 0.00059704
Iteration 10/25 | Loss: 0.00059704
Iteration 11/25 | Loss: 0.00059704
Iteration 12/25 | Loss: 0.00059704
Iteration 13/25 | Loss: 0.00059704
Iteration 14/25 | Loss: 0.00059704
Iteration 15/25 | Loss: 0.00059704
Iteration 16/25 | Loss: 0.00059704
Iteration 17/25 | Loss: 0.00059704
Iteration 18/25 | Loss: 0.00059704
Iteration 19/25 | Loss: 0.00059704
Iteration 20/25 | Loss: 0.00059704
Iteration 21/25 | Loss: 0.00059704
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005970417405478656, 0.0005970417405478656, 0.0005970417405478656, 0.0005970417405478656, 0.0005970417405478656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005970417405478656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059704
Iteration 2/1000 | Loss: 0.00002672
Iteration 3/1000 | Loss: 0.00001929
Iteration 4/1000 | Loss: 0.00001670
Iteration 5/1000 | Loss: 0.00001575
Iteration 6/1000 | Loss: 0.00001533
Iteration 7/1000 | Loss: 0.00001506
Iteration 8/1000 | Loss: 0.00001481
Iteration 9/1000 | Loss: 0.00001458
Iteration 10/1000 | Loss: 0.00001441
Iteration 11/1000 | Loss: 0.00001440
Iteration 12/1000 | Loss: 0.00001436
Iteration 13/1000 | Loss: 0.00001433
Iteration 14/1000 | Loss: 0.00001432
Iteration 15/1000 | Loss: 0.00001432
Iteration 16/1000 | Loss: 0.00001431
Iteration 17/1000 | Loss: 0.00001431
Iteration 18/1000 | Loss: 0.00001431
Iteration 19/1000 | Loss: 0.00001430
Iteration 20/1000 | Loss: 0.00001430
Iteration 21/1000 | Loss: 0.00001428
Iteration 22/1000 | Loss: 0.00001428
Iteration 23/1000 | Loss: 0.00001428
Iteration 24/1000 | Loss: 0.00001427
Iteration 25/1000 | Loss: 0.00001427
Iteration 26/1000 | Loss: 0.00001427
Iteration 27/1000 | Loss: 0.00001427
Iteration 28/1000 | Loss: 0.00001427
Iteration 29/1000 | Loss: 0.00001427
Iteration 30/1000 | Loss: 0.00001427
Iteration 31/1000 | Loss: 0.00001427
Iteration 32/1000 | Loss: 0.00001427
Iteration 33/1000 | Loss: 0.00001427
Iteration 34/1000 | Loss: 0.00001427
Iteration 35/1000 | Loss: 0.00001427
Iteration 36/1000 | Loss: 0.00001427
Iteration 37/1000 | Loss: 0.00001426
Iteration 38/1000 | Loss: 0.00001426
Iteration 39/1000 | Loss: 0.00001425
Iteration 40/1000 | Loss: 0.00001425
Iteration 41/1000 | Loss: 0.00001425
Iteration 42/1000 | Loss: 0.00001424
Iteration 43/1000 | Loss: 0.00001424
Iteration 44/1000 | Loss: 0.00001424
Iteration 45/1000 | Loss: 0.00001424
Iteration 46/1000 | Loss: 0.00001424
Iteration 47/1000 | Loss: 0.00001423
Iteration 48/1000 | Loss: 0.00001423
Iteration 49/1000 | Loss: 0.00001423
Iteration 50/1000 | Loss: 0.00001422
Iteration 51/1000 | Loss: 0.00001422
Iteration 52/1000 | Loss: 0.00001422
Iteration 53/1000 | Loss: 0.00001421
Iteration 54/1000 | Loss: 0.00001421
Iteration 55/1000 | Loss: 0.00001421
Iteration 56/1000 | Loss: 0.00001421
Iteration 57/1000 | Loss: 0.00001421
Iteration 58/1000 | Loss: 0.00001420
Iteration 59/1000 | Loss: 0.00001420
Iteration 60/1000 | Loss: 0.00001420
Iteration 61/1000 | Loss: 0.00001420
Iteration 62/1000 | Loss: 0.00001420
Iteration 63/1000 | Loss: 0.00001420
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001420
Iteration 66/1000 | Loss: 0.00001420
Iteration 67/1000 | Loss: 0.00001420
Iteration 68/1000 | Loss: 0.00001420
Iteration 69/1000 | Loss: 0.00001420
Iteration 70/1000 | Loss: 0.00001420
Iteration 71/1000 | Loss: 0.00001420
Iteration 72/1000 | Loss: 0.00001420
Iteration 73/1000 | Loss: 0.00001420
Iteration 74/1000 | Loss: 0.00001420
Iteration 75/1000 | Loss: 0.00001420
Iteration 76/1000 | Loss: 0.00001420
Iteration 77/1000 | Loss: 0.00001420
Iteration 78/1000 | Loss: 0.00001420
Iteration 79/1000 | Loss: 0.00001420
Iteration 80/1000 | Loss: 0.00001420
Iteration 81/1000 | Loss: 0.00001420
Iteration 82/1000 | Loss: 0.00001420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.4197721611708403e-05, 1.4197721611708403e-05, 1.4197721611708403e-05, 1.4197721611708403e-05, 1.4197721611708403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4197721611708403e-05

Optimization complete. Final v2v error: 3.1057257652282715 mm

Highest mean error: 3.766939163208008 mm for frame 97

Lowest mean error: 2.281140089035034 mm for frame 5

Saving results

Total time: 29.459068775177002
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000910
Iteration 2/25 | Loss: 0.01000910
Iteration 3/25 | Loss: 0.01000910
Iteration 4/25 | Loss: 0.01000910
Iteration 5/25 | Loss: 0.01000910
Iteration 6/25 | Loss: 0.01000910
Iteration 7/25 | Loss: 0.01000910
Iteration 8/25 | Loss: 0.01000909
Iteration 9/25 | Loss: 0.01000909
Iteration 10/25 | Loss: 0.01000909
Iteration 11/25 | Loss: 0.01000909
Iteration 12/25 | Loss: 0.01000909
Iteration 13/25 | Loss: 0.01000908
Iteration 14/25 | Loss: 0.01000908
Iteration 15/25 | Loss: 0.01000908
Iteration 16/25 | Loss: 0.01000908
Iteration 17/25 | Loss: 0.01000908
Iteration 18/25 | Loss: 0.01000908
Iteration 19/25 | Loss: 0.01000907
Iteration 20/25 | Loss: 0.01000907
Iteration 21/25 | Loss: 0.01000907
Iteration 22/25 | Loss: 0.01000907
Iteration 23/25 | Loss: 0.01000906
Iteration 24/25 | Loss: 0.01000906
Iteration 25/25 | Loss: 0.01000906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36868894
Iteration 2/25 | Loss: 0.17764296
Iteration 3/25 | Loss: 0.17761305
Iteration 4/25 | Loss: 0.17761301
Iteration 5/25 | Loss: 0.17761299
Iteration 6/25 | Loss: 0.17761299
Iteration 7/25 | Loss: 0.17761299
Iteration 8/25 | Loss: 0.17761298
Iteration 9/25 | Loss: 0.17761298
Iteration 10/25 | Loss: 0.17761298
Iteration 11/25 | Loss: 0.17761298
Iteration 12/25 | Loss: 0.17761298
Iteration 13/25 | Loss: 0.17761295
Iteration 14/25 | Loss: 0.17761295
Iteration 15/25 | Loss: 0.17761295
Iteration 16/25 | Loss: 0.17761295
Iteration 17/25 | Loss: 0.17761295
Iteration 18/25 | Loss: 0.17761295
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.17761294543743134, 0.17761294543743134, 0.17761294543743134, 0.17761294543743134, 0.17761294543743134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17761294543743134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17761295
Iteration 2/1000 | Loss: 0.00323426
Iteration 3/1000 | Loss: 0.00045470
Iteration 4/1000 | Loss: 0.00027058
Iteration 5/1000 | Loss: 0.00079848
Iteration 6/1000 | Loss: 0.00871926
Iteration 7/1000 | Loss: 0.00093281
Iteration 8/1000 | Loss: 0.00075643
Iteration 9/1000 | Loss: 0.00440476
Iteration 10/1000 | Loss: 0.00071480
Iteration 11/1000 | Loss: 0.00096231
Iteration 12/1000 | Loss: 0.00023106
Iteration 13/1000 | Loss: 0.00011442
Iteration 14/1000 | Loss: 0.00025976
Iteration 15/1000 | Loss: 0.00389918
Iteration 16/1000 | Loss: 0.00005377
Iteration 17/1000 | Loss: 0.00005498
Iteration 18/1000 | Loss: 0.00007918
Iteration 19/1000 | Loss: 0.00011978
Iteration 20/1000 | Loss: 0.00178893
Iteration 21/1000 | Loss: 0.00321687
Iteration 22/1000 | Loss: 0.00318585
Iteration 23/1000 | Loss: 0.00246750
Iteration 24/1000 | Loss: 0.00006127
Iteration 25/1000 | Loss: 0.00002015
Iteration 26/1000 | Loss: 0.00001830
Iteration 27/1000 | Loss: 0.00020953
Iteration 28/1000 | Loss: 0.00258021
Iteration 29/1000 | Loss: 0.00027500
Iteration 30/1000 | Loss: 0.00006163
Iteration 31/1000 | Loss: 0.00067293
Iteration 32/1000 | Loss: 0.00026348
Iteration 33/1000 | Loss: 0.00002815
Iteration 34/1000 | Loss: 0.00006089
Iteration 35/1000 | Loss: 0.00002436
Iteration 36/1000 | Loss: 0.00006658
Iteration 37/1000 | Loss: 0.00019062
Iteration 38/1000 | Loss: 0.00003988
Iteration 39/1000 | Loss: 0.00038482
Iteration 40/1000 | Loss: 0.00027248
Iteration 41/1000 | Loss: 0.00008024
Iteration 42/1000 | Loss: 0.00005935
Iteration 43/1000 | Loss: 0.00012285
Iteration 44/1000 | Loss: 0.00003254
Iteration 45/1000 | Loss: 0.00002116
Iteration 46/1000 | Loss: 0.00006016
Iteration 47/1000 | Loss: 0.00030554
Iteration 48/1000 | Loss: 0.00005958
Iteration 49/1000 | Loss: 0.00002174
Iteration 50/1000 | Loss: 0.00006621
Iteration 51/1000 | Loss: 0.00001688
Iteration 52/1000 | Loss: 0.00001404
Iteration 53/1000 | Loss: 0.00003805
Iteration 54/1000 | Loss: 0.00001606
Iteration 55/1000 | Loss: 0.00005661
Iteration 56/1000 | Loss: 0.00001480
Iteration 57/1000 | Loss: 0.00004819
Iteration 58/1000 | Loss: 0.00001448
Iteration 59/1000 | Loss: 0.00002765
Iteration 60/1000 | Loss: 0.00002072
Iteration 61/1000 | Loss: 0.00001292
Iteration 62/1000 | Loss: 0.00001563
Iteration 63/1000 | Loss: 0.00003142
Iteration 64/1000 | Loss: 0.00019663
Iteration 65/1000 | Loss: 0.00001266
Iteration 66/1000 | Loss: 0.00006699
Iteration 67/1000 | Loss: 0.00001519
Iteration 68/1000 | Loss: 0.00001253
Iteration 69/1000 | Loss: 0.00002066
Iteration 70/1000 | Loss: 0.00001244
Iteration 71/1000 | Loss: 0.00001242
Iteration 72/1000 | Loss: 0.00001242
Iteration 73/1000 | Loss: 0.00001241
Iteration 74/1000 | Loss: 0.00001241
Iteration 75/1000 | Loss: 0.00001241
Iteration 76/1000 | Loss: 0.00001241
Iteration 77/1000 | Loss: 0.00001240
Iteration 78/1000 | Loss: 0.00001240
Iteration 79/1000 | Loss: 0.00001240
Iteration 80/1000 | Loss: 0.00001239
Iteration 81/1000 | Loss: 0.00001239
Iteration 82/1000 | Loss: 0.00001239
Iteration 83/1000 | Loss: 0.00001238
Iteration 84/1000 | Loss: 0.00001238
Iteration 85/1000 | Loss: 0.00001238
Iteration 86/1000 | Loss: 0.00001238
Iteration 87/1000 | Loss: 0.00001238
Iteration 88/1000 | Loss: 0.00001237
Iteration 89/1000 | Loss: 0.00001237
Iteration 90/1000 | Loss: 0.00001237
Iteration 91/1000 | Loss: 0.00001236
Iteration 92/1000 | Loss: 0.00002247
Iteration 93/1000 | Loss: 0.00001236
Iteration 94/1000 | Loss: 0.00001235
Iteration 95/1000 | Loss: 0.00001234
Iteration 96/1000 | Loss: 0.00001234
Iteration 97/1000 | Loss: 0.00001234
Iteration 98/1000 | Loss: 0.00001233
Iteration 99/1000 | Loss: 0.00001233
Iteration 100/1000 | Loss: 0.00001233
Iteration 101/1000 | Loss: 0.00001233
Iteration 102/1000 | Loss: 0.00001233
Iteration 103/1000 | Loss: 0.00001233
Iteration 104/1000 | Loss: 0.00001233
Iteration 105/1000 | Loss: 0.00001233
Iteration 106/1000 | Loss: 0.00001232
Iteration 107/1000 | Loss: 0.00001232
Iteration 108/1000 | Loss: 0.00001232
Iteration 109/1000 | Loss: 0.00001232
Iteration 110/1000 | Loss: 0.00001232
Iteration 111/1000 | Loss: 0.00001232
Iteration 112/1000 | Loss: 0.00001232
Iteration 113/1000 | Loss: 0.00001232
Iteration 114/1000 | Loss: 0.00001232
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001231
Iteration 117/1000 | Loss: 0.00001231
Iteration 118/1000 | Loss: 0.00001231
Iteration 119/1000 | Loss: 0.00001231
Iteration 120/1000 | Loss: 0.00001231
Iteration 121/1000 | Loss: 0.00001231
Iteration 122/1000 | Loss: 0.00001230
Iteration 123/1000 | Loss: 0.00001230
Iteration 124/1000 | Loss: 0.00001230
Iteration 125/1000 | Loss: 0.00001230
Iteration 126/1000 | Loss: 0.00001230
Iteration 127/1000 | Loss: 0.00001230
Iteration 128/1000 | Loss: 0.00001230
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001230
Iteration 131/1000 | Loss: 0.00001230
Iteration 132/1000 | Loss: 0.00001230
Iteration 133/1000 | Loss: 0.00001230
Iteration 134/1000 | Loss: 0.00001230
Iteration 135/1000 | Loss: 0.00001230
Iteration 136/1000 | Loss: 0.00001230
Iteration 137/1000 | Loss: 0.00001230
Iteration 138/1000 | Loss: 0.00001230
Iteration 139/1000 | Loss: 0.00001230
Iteration 140/1000 | Loss: 0.00001230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.229774807143258e-05, 1.229774807143258e-05, 1.229774807143258e-05, 1.229774807143258e-05, 1.229774807143258e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.229774807143258e-05

Optimization complete. Final v2v error: 3.073140859603882 mm

Highest mean error: 3.447045087814331 mm for frame 209

Lowest mean error: 2.6457676887512207 mm for frame 65

Saving results

Total time: 122.78205418586731
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824477
Iteration 2/25 | Loss: 0.00116761
Iteration 3/25 | Loss: 0.00104075
Iteration 4/25 | Loss: 0.00101030
Iteration 5/25 | Loss: 0.00099924
Iteration 6/25 | Loss: 0.00099639
Iteration 7/25 | Loss: 0.00099639
Iteration 8/25 | Loss: 0.00099639
Iteration 9/25 | Loss: 0.00099639
Iteration 10/25 | Loss: 0.00099639
Iteration 11/25 | Loss: 0.00099639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009963874472305179, 0.0009963874472305179, 0.0009963874472305179, 0.0009963874472305179, 0.0009963874472305179]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009963874472305179

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98526680
Iteration 2/25 | Loss: 0.00081453
Iteration 3/25 | Loss: 0.00081452
Iteration 4/25 | Loss: 0.00081452
Iteration 5/25 | Loss: 0.00081452
Iteration 6/25 | Loss: 0.00081452
Iteration 7/25 | Loss: 0.00081452
Iteration 8/25 | Loss: 0.00081452
Iteration 9/25 | Loss: 0.00081452
Iteration 10/25 | Loss: 0.00081452
Iteration 11/25 | Loss: 0.00081452
Iteration 12/25 | Loss: 0.00081452
Iteration 13/25 | Loss: 0.00081452
Iteration 14/25 | Loss: 0.00081452
Iteration 15/25 | Loss: 0.00081452
Iteration 16/25 | Loss: 0.00081452
Iteration 17/25 | Loss: 0.00081452
Iteration 18/25 | Loss: 0.00081452
Iteration 19/25 | Loss: 0.00081452
Iteration 20/25 | Loss: 0.00081452
Iteration 21/25 | Loss: 0.00081452
Iteration 22/25 | Loss: 0.00081452
Iteration 23/25 | Loss: 0.00081452
Iteration 24/25 | Loss: 0.00081452
Iteration 25/25 | Loss: 0.00081452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081452
Iteration 2/1000 | Loss: 0.00004285
Iteration 3/1000 | Loss: 0.00002908
Iteration 4/1000 | Loss: 0.00002643
Iteration 5/1000 | Loss: 0.00002516
Iteration 6/1000 | Loss: 0.00002463
Iteration 7/1000 | Loss: 0.00002419
Iteration 8/1000 | Loss: 0.00002396
Iteration 9/1000 | Loss: 0.00002371
Iteration 10/1000 | Loss: 0.00002361
Iteration 11/1000 | Loss: 0.00002360
Iteration 12/1000 | Loss: 0.00002350
Iteration 13/1000 | Loss: 0.00002350
Iteration 14/1000 | Loss: 0.00002350
Iteration 15/1000 | Loss: 0.00002350
Iteration 16/1000 | Loss: 0.00002350
Iteration 17/1000 | Loss: 0.00002347
Iteration 18/1000 | Loss: 0.00002346
Iteration 19/1000 | Loss: 0.00002346
Iteration 20/1000 | Loss: 0.00002345
Iteration 21/1000 | Loss: 0.00002344
Iteration 22/1000 | Loss: 0.00002343
Iteration 23/1000 | Loss: 0.00002343
Iteration 24/1000 | Loss: 0.00002340
Iteration 25/1000 | Loss: 0.00002340
Iteration 26/1000 | Loss: 0.00002339
Iteration 27/1000 | Loss: 0.00002339
Iteration 28/1000 | Loss: 0.00002339
Iteration 29/1000 | Loss: 0.00002338
Iteration 30/1000 | Loss: 0.00002337
Iteration 31/1000 | Loss: 0.00002336
Iteration 32/1000 | Loss: 0.00002336
Iteration 33/1000 | Loss: 0.00002335
Iteration 34/1000 | Loss: 0.00002335
Iteration 35/1000 | Loss: 0.00002335
Iteration 36/1000 | Loss: 0.00002335
Iteration 37/1000 | Loss: 0.00002335
Iteration 38/1000 | Loss: 0.00002335
Iteration 39/1000 | Loss: 0.00002335
Iteration 40/1000 | Loss: 0.00002334
Iteration 41/1000 | Loss: 0.00002334
Iteration 42/1000 | Loss: 0.00002334
Iteration 43/1000 | Loss: 0.00002334
Iteration 44/1000 | Loss: 0.00002333
Iteration 45/1000 | Loss: 0.00002333
Iteration 46/1000 | Loss: 0.00002333
Iteration 47/1000 | Loss: 0.00002333
Iteration 48/1000 | Loss: 0.00002332
Iteration 49/1000 | Loss: 0.00002332
Iteration 50/1000 | Loss: 0.00002332
Iteration 51/1000 | Loss: 0.00002332
Iteration 52/1000 | Loss: 0.00002332
Iteration 53/1000 | Loss: 0.00002331
Iteration 54/1000 | Loss: 0.00002331
Iteration 55/1000 | Loss: 0.00002331
Iteration 56/1000 | Loss: 0.00002330
Iteration 57/1000 | Loss: 0.00002330
Iteration 58/1000 | Loss: 0.00002330
Iteration 59/1000 | Loss: 0.00002329
Iteration 60/1000 | Loss: 0.00002329
Iteration 61/1000 | Loss: 0.00002329
Iteration 62/1000 | Loss: 0.00002329
Iteration 63/1000 | Loss: 0.00002329
Iteration 64/1000 | Loss: 0.00002328
Iteration 65/1000 | Loss: 0.00002327
Iteration 66/1000 | Loss: 0.00002327
Iteration 67/1000 | Loss: 0.00002327
Iteration 68/1000 | Loss: 0.00002327
Iteration 69/1000 | Loss: 0.00002327
Iteration 70/1000 | Loss: 0.00002327
Iteration 71/1000 | Loss: 0.00002327
Iteration 72/1000 | Loss: 0.00002326
Iteration 73/1000 | Loss: 0.00002326
Iteration 74/1000 | Loss: 0.00002326
Iteration 75/1000 | Loss: 0.00002326
Iteration 76/1000 | Loss: 0.00002326
Iteration 77/1000 | Loss: 0.00002326
Iteration 78/1000 | Loss: 0.00002325
Iteration 79/1000 | Loss: 0.00002325
Iteration 80/1000 | Loss: 0.00002325
Iteration 81/1000 | Loss: 0.00002325
Iteration 82/1000 | Loss: 0.00002325
Iteration 83/1000 | Loss: 0.00002325
Iteration 84/1000 | Loss: 0.00002325
Iteration 85/1000 | Loss: 0.00002325
Iteration 86/1000 | Loss: 0.00002324
Iteration 87/1000 | Loss: 0.00002324
Iteration 88/1000 | Loss: 0.00002324
Iteration 89/1000 | Loss: 0.00002324
Iteration 90/1000 | Loss: 0.00002324
Iteration 91/1000 | Loss: 0.00002323
Iteration 92/1000 | Loss: 0.00002323
Iteration 93/1000 | Loss: 0.00002323
Iteration 94/1000 | Loss: 0.00002323
Iteration 95/1000 | Loss: 0.00002323
Iteration 96/1000 | Loss: 0.00002323
Iteration 97/1000 | Loss: 0.00002323
Iteration 98/1000 | Loss: 0.00002323
Iteration 99/1000 | Loss: 0.00002323
Iteration 100/1000 | Loss: 0.00002323
Iteration 101/1000 | Loss: 0.00002323
Iteration 102/1000 | Loss: 0.00002322
Iteration 103/1000 | Loss: 0.00002322
Iteration 104/1000 | Loss: 0.00002322
Iteration 105/1000 | Loss: 0.00002322
Iteration 106/1000 | Loss: 0.00002322
Iteration 107/1000 | Loss: 0.00002322
Iteration 108/1000 | Loss: 0.00002322
Iteration 109/1000 | Loss: 0.00002322
Iteration 110/1000 | Loss: 0.00002322
Iteration 111/1000 | Loss: 0.00002322
Iteration 112/1000 | Loss: 0.00002322
Iteration 113/1000 | Loss: 0.00002322
Iteration 114/1000 | Loss: 0.00002322
Iteration 115/1000 | Loss: 0.00002321
Iteration 116/1000 | Loss: 0.00002321
Iteration 117/1000 | Loss: 0.00002321
Iteration 118/1000 | Loss: 0.00002321
Iteration 119/1000 | Loss: 0.00002321
Iteration 120/1000 | Loss: 0.00002321
Iteration 121/1000 | Loss: 0.00002321
Iteration 122/1000 | Loss: 0.00002320
Iteration 123/1000 | Loss: 0.00002320
Iteration 124/1000 | Loss: 0.00002320
Iteration 125/1000 | Loss: 0.00002320
Iteration 126/1000 | Loss: 0.00002320
Iteration 127/1000 | Loss: 0.00002320
Iteration 128/1000 | Loss: 0.00002320
Iteration 129/1000 | Loss: 0.00002320
Iteration 130/1000 | Loss: 0.00002320
Iteration 131/1000 | Loss: 0.00002320
Iteration 132/1000 | Loss: 0.00002320
Iteration 133/1000 | Loss: 0.00002320
Iteration 134/1000 | Loss: 0.00002320
Iteration 135/1000 | Loss: 0.00002320
Iteration 136/1000 | Loss: 0.00002320
Iteration 137/1000 | Loss: 0.00002320
Iteration 138/1000 | Loss: 0.00002320
Iteration 139/1000 | Loss: 0.00002320
Iteration 140/1000 | Loss: 0.00002320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.3197266273200512e-05, 2.3197266273200512e-05, 2.3197266273200512e-05, 2.3197266273200512e-05, 2.3197266273200512e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3197266273200512e-05

Optimization complete. Final v2v error: 3.8785016536712646 mm

Highest mean error: 4.430031776428223 mm for frame 140

Lowest mean error: 3.2975733280181885 mm for frame 127

Saving results

Total time: 38.045483112335205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035262
Iteration 2/25 | Loss: 0.00180226
Iteration 3/25 | Loss: 0.00157643
Iteration 4/25 | Loss: 0.00151676
Iteration 5/25 | Loss: 0.00148947
Iteration 6/25 | Loss: 0.00148190
Iteration 7/25 | Loss: 0.00147949
Iteration 8/25 | Loss: 0.00147923
Iteration 9/25 | Loss: 0.00147923
Iteration 10/25 | Loss: 0.00147923
Iteration 11/25 | Loss: 0.00147923
Iteration 12/25 | Loss: 0.00147923
Iteration 13/25 | Loss: 0.00147923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001479234080761671, 0.001479234080761671, 0.001479234080761671, 0.001479234080761671, 0.001479234080761671]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001479234080761671

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26150393
Iteration 2/25 | Loss: 0.00169812
Iteration 3/25 | Loss: 0.00169808
Iteration 4/25 | Loss: 0.00169808
Iteration 5/25 | Loss: 0.00169807
Iteration 6/25 | Loss: 0.00169807
Iteration 7/25 | Loss: 0.00169807
Iteration 8/25 | Loss: 0.00169807
Iteration 9/25 | Loss: 0.00169807
Iteration 10/25 | Loss: 0.00169807
Iteration 11/25 | Loss: 0.00169807
Iteration 12/25 | Loss: 0.00169807
Iteration 13/25 | Loss: 0.00169807
Iteration 14/25 | Loss: 0.00169807
Iteration 15/25 | Loss: 0.00169807
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0016980733489617705, 0.0016980733489617705, 0.0016980733489617705, 0.0016980733489617705, 0.0016980733489617705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016980733489617705

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169807
Iteration 2/1000 | Loss: 0.00010351
Iteration 3/1000 | Loss: 0.00007142
Iteration 4/1000 | Loss: 0.00059180
Iteration 5/1000 | Loss: 0.00006667
Iteration 6/1000 | Loss: 0.00005631
Iteration 7/1000 | Loss: 0.00005189
Iteration 8/1000 | Loss: 0.00004914
Iteration 9/1000 | Loss: 0.00004758
Iteration 10/1000 | Loss: 0.00004663
Iteration 11/1000 | Loss: 0.00004588
Iteration 12/1000 | Loss: 0.00004517
Iteration 13/1000 | Loss: 0.00004461
Iteration 14/1000 | Loss: 0.00004409
Iteration 15/1000 | Loss: 0.00004377
Iteration 16/1000 | Loss: 0.00004336
Iteration 17/1000 | Loss: 0.00004316
Iteration 18/1000 | Loss: 0.00004299
Iteration 19/1000 | Loss: 0.00004289
Iteration 20/1000 | Loss: 0.00004287
Iteration 21/1000 | Loss: 0.00004275
Iteration 22/1000 | Loss: 0.00004270
Iteration 23/1000 | Loss: 0.00004263
Iteration 24/1000 | Loss: 0.00004261
Iteration 25/1000 | Loss: 0.00004260
Iteration 26/1000 | Loss: 0.00004258
Iteration 27/1000 | Loss: 0.00004257
Iteration 28/1000 | Loss: 0.00004256
Iteration 29/1000 | Loss: 0.00004255
Iteration 30/1000 | Loss: 0.00004255
Iteration 31/1000 | Loss: 0.00004255
Iteration 32/1000 | Loss: 0.00004255
Iteration 33/1000 | Loss: 0.00004254
Iteration 34/1000 | Loss: 0.00004253
Iteration 35/1000 | Loss: 0.00004253
Iteration 36/1000 | Loss: 0.00004252
Iteration 37/1000 | Loss: 0.00004252
Iteration 38/1000 | Loss: 0.00004252
Iteration 39/1000 | Loss: 0.00004252
Iteration 40/1000 | Loss: 0.00004251
Iteration 41/1000 | Loss: 0.00004251
Iteration 42/1000 | Loss: 0.00004251
Iteration 43/1000 | Loss: 0.00004250
Iteration 44/1000 | Loss: 0.00004250
Iteration 45/1000 | Loss: 0.00004249
Iteration 46/1000 | Loss: 0.00004249
Iteration 47/1000 | Loss: 0.00004248
Iteration 48/1000 | Loss: 0.00004248
Iteration 49/1000 | Loss: 0.00004247
Iteration 50/1000 | Loss: 0.00004247
Iteration 51/1000 | Loss: 0.00004247
Iteration 52/1000 | Loss: 0.00004247
Iteration 53/1000 | Loss: 0.00004247
Iteration 54/1000 | Loss: 0.00004247
Iteration 55/1000 | Loss: 0.00004247
Iteration 56/1000 | Loss: 0.00004246
Iteration 57/1000 | Loss: 0.00004246
Iteration 58/1000 | Loss: 0.00004246
Iteration 59/1000 | Loss: 0.00004246
Iteration 60/1000 | Loss: 0.00004246
Iteration 61/1000 | Loss: 0.00004245
Iteration 62/1000 | Loss: 0.00004245
Iteration 63/1000 | Loss: 0.00004245
Iteration 64/1000 | Loss: 0.00004245
Iteration 65/1000 | Loss: 0.00004245
Iteration 66/1000 | Loss: 0.00004245
Iteration 67/1000 | Loss: 0.00004244
Iteration 68/1000 | Loss: 0.00004244
Iteration 69/1000 | Loss: 0.00004244
Iteration 70/1000 | Loss: 0.00004244
Iteration 71/1000 | Loss: 0.00004244
Iteration 72/1000 | Loss: 0.00004244
Iteration 73/1000 | Loss: 0.00004244
Iteration 74/1000 | Loss: 0.00004244
Iteration 75/1000 | Loss: 0.00004244
Iteration 76/1000 | Loss: 0.00004244
Iteration 77/1000 | Loss: 0.00004243
Iteration 78/1000 | Loss: 0.00004243
Iteration 79/1000 | Loss: 0.00004243
Iteration 80/1000 | Loss: 0.00004243
Iteration 81/1000 | Loss: 0.00004243
Iteration 82/1000 | Loss: 0.00004243
Iteration 83/1000 | Loss: 0.00004242
Iteration 84/1000 | Loss: 0.00004242
Iteration 85/1000 | Loss: 0.00004242
Iteration 86/1000 | Loss: 0.00004242
Iteration 87/1000 | Loss: 0.00004242
Iteration 88/1000 | Loss: 0.00004242
Iteration 89/1000 | Loss: 0.00004242
Iteration 90/1000 | Loss: 0.00004241
Iteration 91/1000 | Loss: 0.00004241
Iteration 92/1000 | Loss: 0.00004241
Iteration 93/1000 | Loss: 0.00004241
Iteration 94/1000 | Loss: 0.00004241
Iteration 95/1000 | Loss: 0.00004240
Iteration 96/1000 | Loss: 0.00004240
Iteration 97/1000 | Loss: 0.00004240
Iteration 98/1000 | Loss: 0.00004240
Iteration 99/1000 | Loss: 0.00004240
Iteration 100/1000 | Loss: 0.00004239
Iteration 101/1000 | Loss: 0.00004239
Iteration 102/1000 | Loss: 0.00004239
Iteration 103/1000 | Loss: 0.00004238
Iteration 104/1000 | Loss: 0.00004238
Iteration 105/1000 | Loss: 0.00004238
Iteration 106/1000 | Loss: 0.00004238
Iteration 107/1000 | Loss: 0.00004238
Iteration 108/1000 | Loss: 0.00004238
Iteration 109/1000 | Loss: 0.00004238
Iteration 110/1000 | Loss: 0.00004238
Iteration 111/1000 | Loss: 0.00004238
Iteration 112/1000 | Loss: 0.00004238
Iteration 113/1000 | Loss: 0.00004237
Iteration 114/1000 | Loss: 0.00004236
Iteration 115/1000 | Loss: 0.00004236
Iteration 116/1000 | Loss: 0.00004236
Iteration 117/1000 | Loss: 0.00004235
Iteration 118/1000 | Loss: 0.00004235
Iteration 119/1000 | Loss: 0.00004235
Iteration 120/1000 | Loss: 0.00004235
Iteration 121/1000 | Loss: 0.00004235
Iteration 122/1000 | Loss: 0.00004235
Iteration 123/1000 | Loss: 0.00004234
Iteration 124/1000 | Loss: 0.00004234
Iteration 125/1000 | Loss: 0.00004234
Iteration 126/1000 | Loss: 0.00004234
Iteration 127/1000 | Loss: 0.00004234
Iteration 128/1000 | Loss: 0.00004234
Iteration 129/1000 | Loss: 0.00004233
Iteration 130/1000 | Loss: 0.00004233
Iteration 131/1000 | Loss: 0.00004233
Iteration 132/1000 | Loss: 0.00004233
Iteration 133/1000 | Loss: 0.00004233
Iteration 134/1000 | Loss: 0.00004233
Iteration 135/1000 | Loss: 0.00004233
Iteration 136/1000 | Loss: 0.00004232
Iteration 137/1000 | Loss: 0.00004232
Iteration 138/1000 | Loss: 0.00004232
Iteration 139/1000 | Loss: 0.00004232
Iteration 140/1000 | Loss: 0.00004232
Iteration 141/1000 | Loss: 0.00004232
Iteration 142/1000 | Loss: 0.00004232
Iteration 143/1000 | Loss: 0.00004232
Iteration 144/1000 | Loss: 0.00004232
Iteration 145/1000 | Loss: 0.00004232
Iteration 146/1000 | Loss: 0.00004232
Iteration 147/1000 | Loss: 0.00004232
Iteration 148/1000 | Loss: 0.00004232
Iteration 149/1000 | Loss: 0.00004231
Iteration 150/1000 | Loss: 0.00004231
Iteration 151/1000 | Loss: 0.00004231
Iteration 152/1000 | Loss: 0.00004231
Iteration 153/1000 | Loss: 0.00004231
Iteration 154/1000 | Loss: 0.00004231
Iteration 155/1000 | Loss: 0.00004231
Iteration 156/1000 | Loss: 0.00004231
Iteration 157/1000 | Loss: 0.00004231
Iteration 158/1000 | Loss: 0.00004231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [4.230752165312879e-05, 4.230752165312879e-05, 4.230752165312879e-05, 4.230752165312879e-05, 4.230752165312879e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.230752165312879e-05

Optimization complete. Final v2v error: 5.459031581878662 mm

Highest mean error: 6.757627010345459 mm for frame 133

Lowest mean error: 3.9313583374023438 mm for frame 23

Saving results

Total time: 58.40143823623657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874134
Iteration 2/25 | Loss: 0.00178060
Iteration 3/25 | Loss: 0.00151890
Iteration 4/25 | Loss: 0.00147158
Iteration 5/25 | Loss: 0.00145218
Iteration 6/25 | Loss: 0.00144826
Iteration 7/25 | Loss: 0.00144689
Iteration 8/25 | Loss: 0.00144627
Iteration 9/25 | Loss: 0.00144583
Iteration 10/25 | Loss: 0.00144556
Iteration 11/25 | Loss: 0.00144555
Iteration 12/25 | Loss: 0.00144554
Iteration 13/25 | Loss: 0.00144554
Iteration 14/25 | Loss: 0.00144554
Iteration 15/25 | Loss: 0.00144554
Iteration 16/25 | Loss: 0.00144554
Iteration 17/25 | Loss: 0.00144554
Iteration 18/25 | Loss: 0.00144553
Iteration 19/25 | Loss: 0.00144553
Iteration 20/25 | Loss: 0.00144553
Iteration 21/25 | Loss: 0.00144553
Iteration 22/25 | Loss: 0.00144553
Iteration 23/25 | Loss: 0.00144553
Iteration 24/25 | Loss: 0.00144553
Iteration 25/25 | Loss: 0.00144553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014455339405685663, 0.0014455339405685663, 0.0014455339405685663, 0.0014455339405685663, 0.0014455339405685663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014455339405685663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.76612234
Iteration 2/25 | Loss: 0.00225204
Iteration 3/25 | Loss: 0.00225200
Iteration 4/25 | Loss: 0.00225200
Iteration 5/25 | Loss: 0.00225200
Iteration 6/25 | Loss: 0.00225200
Iteration 7/25 | Loss: 0.00225200
Iteration 8/25 | Loss: 0.00225200
Iteration 9/25 | Loss: 0.00225200
Iteration 10/25 | Loss: 0.00225200
Iteration 11/25 | Loss: 0.00225200
Iteration 12/25 | Loss: 0.00225200
Iteration 13/25 | Loss: 0.00225200
Iteration 14/25 | Loss: 0.00225200
Iteration 15/25 | Loss: 0.00225200
Iteration 16/25 | Loss: 0.00225200
Iteration 17/25 | Loss: 0.00225200
Iteration 18/25 | Loss: 0.00225200
Iteration 19/25 | Loss: 0.00225200
Iteration 20/25 | Loss: 0.00225200
Iteration 21/25 | Loss: 0.00225200
Iteration 22/25 | Loss: 0.00225200
Iteration 23/25 | Loss: 0.00225200
Iteration 24/25 | Loss: 0.00225200
Iteration 25/25 | Loss: 0.00225200

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225200
Iteration 2/1000 | Loss: 0.00007390
Iteration 3/1000 | Loss: 0.00004908
Iteration 4/1000 | Loss: 0.00003936
Iteration 5/1000 | Loss: 0.00003533
Iteration 6/1000 | Loss: 0.00003276
Iteration 7/1000 | Loss: 0.00014777
Iteration 8/1000 | Loss: 0.00003099
Iteration 9/1000 | Loss: 0.00003033
Iteration 10/1000 | Loss: 0.00002980
Iteration 11/1000 | Loss: 0.00002948
Iteration 12/1000 | Loss: 0.00002913
Iteration 13/1000 | Loss: 0.00002891
Iteration 14/1000 | Loss: 0.00002874
Iteration 15/1000 | Loss: 0.00002872
Iteration 16/1000 | Loss: 0.00002864
Iteration 17/1000 | Loss: 0.00002864
Iteration 18/1000 | Loss: 0.00002861
Iteration 19/1000 | Loss: 0.00002860
Iteration 20/1000 | Loss: 0.00002860
Iteration 21/1000 | Loss: 0.00002859
Iteration 22/1000 | Loss: 0.00002859
Iteration 23/1000 | Loss: 0.00002859
Iteration 24/1000 | Loss: 0.00002858
Iteration 25/1000 | Loss: 0.00002857
Iteration 26/1000 | Loss: 0.00002857
Iteration 27/1000 | Loss: 0.00002857
Iteration 28/1000 | Loss: 0.00002856
Iteration 29/1000 | Loss: 0.00002856
Iteration 30/1000 | Loss: 0.00002856
Iteration 31/1000 | Loss: 0.00002856
Iteration 32/1000 | Loss: 0.00002855
Iteration 33/1000 | Loss: 0.00002855
Iteration 34/1000 | Loss: 0.00002855
Iteration 35/1000 | Loss: 0.00002855
Iteration 36/1000 | Loss: 0.00002854
Iteration 37/1000 | Loss: 0.00002854
Iteration 38/1000 | Loss: 0.00002854
Iteration 39/1000 | Loss: 0.00002854
Iteration 40/1000 | Loss: 0.00002854
Iteration 41/1000 | Loss: 0.00002854
Iteration 42/1000 | Loss: 0.00002854
Iteration 43/1000 | Loss: 0.00002853
Iteration 44/1000 | Loss: 0.00002853
Iteration 45/1000 | Loss: 0.00002853
Iteration 46/1000 | Loss: 0.00002853
Iteration 47/1000 | Loss: 0.00002853
Iteration 48/1000 | Loss: 0.00002853
Iteration 49/1000 | Loss: 0.00002853
Iteration 50/1000 | Loss: 0.00002853
Iteration 51/1000 | Loss: 0.00002853
Iteration 52/1000 | Loss: 0.00002853
Iteration 53/1000 | Loss: 0.00002853
Iteration 54/1000 | Loss: 0.00002853
Iteration 55/1000 | Loss: 0.00002853
Iteration 56/1000 | Loss: 0.00002852
Iteration 57/1000 | Loss: 0.00002852
Iteration 58/1000 | Loss: 0.00002851
Iteration 59/1000 | Loss: 0.00002851
Iteration 60/1000 | Loss: 0.00002851
Iteration 61/1000 | Loss: 0.00002851
Iteration 62/1000 | Loss: 0.00002851
Iteration 63/1000 | Loss: 0.00002850
Iteration 64/1000 | Loss: 0.00002850
Iteration 65/1000 | Loss: 0.00002850
Iteration 66/1000 | Loss: 0.00002850
Iteration 67/1000 | Loss: 0.00002849
Iteration 68/1000 | Loss: 0.00002849
Iteration 69/1000 | Loss: 0.00002849
Iteration 70/1000 | Loss: 0.00002849
Iteration 71/1000 | Loss: 0.00002849
Iteration 72/1000 | Loss: 0.00002849
Iteration 73/1000 | Loss: 0.00002849
Iteration 74/1000 | Loss: 0.00002849
Iteration 75/1000 | Loss: 0.00002849
Iteration 76/1000 | Loss: 0.00002849
Iteration 77/1000 | Loss: 0.00002849
Iteration 78/1000 | Loss: 0.00002848
Iteration 79/1000 | Loss: 0.00002848
Iteration 80/1000 | Loss: 0.00002848
Iteration 81/1000 | Loss: 0.00002848
Iteration 82/1000 | Loss: 0.00002848
Iteration 83/1000 | Loss: 0.00002847
Iteration 84/1000 | Loss: 0.00002847
Iteration 85/1000 | Loss: 0.00002847
Iteration 86/1000 | Loss: 0.00002847
Iteration 87/1000 | Loss: 0.00002847
Iteration 88/1000 | Loss: 0.00002847
Iteration 89/1000 | Loss: 0.00002847
Iteration 90/1000 | Loss: 0.00002847
Iteration 91/1000 | Loss: 0.00002847
Iteration 92/1000 | Loss: 0.00002847
Iteration 93/1000 | Loss: 0.00002847
Iteration 94/1000 | Loss: 0.00002847
Iteration 95/1000 | Loss: 0.00002847
Iteration 96/1000 | Loss: 0.00002847
Iteration 97/1000 | Loss: 0.00002847
Iteration 98/1000 | Loss: 0.00002847
Iteration 99/1000 | Loss: 0.00002846
Iteration 100/1000 | Loss: 0.00002846
Iteration 101/1000 | Loss: 0.00002846
Iteration 102/1000 | Loss: 0.00002846
Iteration 103/1000 | Loss: 0.00002846
Iteration 104/1000 | Loss: 0.00002846
Iteration 105/1000 | Loss: 0.00002846
Iteration 106/1000 | Loss: 0.00002846
Iteration 107/1000 | Loss: 0.00002846
Iteration 108/1000 | Loss: 0.00002846
Iteration 109/1000 | Loss: 0.00002846
Iteration 110/1000 | Loss: 0.00002846
Iteration 111/1000 | Loss: 0.00002846
Iteration 112/1000 | Loss: 0.00002846
Iteration 113/1000 | Loss: 0.00002846
Iteration 114/1000 | Loss: 0.00002846
Iteration 115/1000 | Loss: 0.00002846
Iteration 116/1000 | Loss: 0.00002846
Iteration 117/1000 | Loss: 0.00002846
Iteration 118/1000 | Loss: 0.00002846
Iteration 119/1000 | Loss: 0.00002846
Iteration 120/1000 | Loss: 0.00002846
Iteration 121/1000 | Loss: 0.00002846
Iteration 122/1000 | Loss: 0.00002846
Iteration 123/1000 | Loss: 0.00002846
Iteration 124/1000 | Loss: 0.00002846
Iteration 125/1000 | Loss: 0.00002846
Iteration 126/1000 | Loss: 0.00002846
Iteration 127/1000 | Loss: 0.00002846
Iteration 128/1000 | Loss: 0.00002846
Iteration 129/1000 | Loss: 0.00002846
Iteration 130/1000 | Loss: 0.00002846
Iteration 131/1000 | Loss: 0.00002846
Iteration 132/1000 | Loss: 0.00002846
Iteration 133/1000 | Loss: 0.00002846
Iteration 134/1000 | Loss: 0.00002846
Iteration 135/1000 | Loss: 0.00002846
Iteration 136/1000 | Loss: 0.00002846
Iteration 137/1000 | Loss: 0.00002846
Iteration 138/1000 | Loss: 0.00002846
Iteration 139/1000 | Loss: 0.00002846
Iteration 140/1000 | Loss: 0.00002846
Iteration 141/1000 | Loss: 0.00002846
Iteration 142/1000 | Loss: 0.00002846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.8464964998420328e-05, 2.8464964998420328e-05, 2.8464964998420328e-05, 2.8464964998420328e-05, 2.8464964998420328e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8464964998420328e-05

Optimization complete. Final v2v error: 4.389216899871826 mm

Highest mean error: 14.14295768737793 mm for frame 77

Lowest mean error: 3.8217670917510986 mm for frame 132

Saving results

Total time: 44.39071440696716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01100473
Iteration 2/25 | Loss: 0.00217278
Iteration 3/25 | Loss: 0.00186008
Iteration 4/25 | Loss: 0.00156178
Iteration 5/25 | Loss: 0.00151379
Iteration 6/25 | Loss: 0.00156002
Iteration 7/25 | Loss: 0.00152293
Iteration 8/25 | Loss: 0.00149968
Iteration 9/25 | Loss: 0.00149771
Iteration 10/25 | Loss: 0.00148635
Iteration 11/25 | Loss: 0.00148796
Iteration 12/25 | Loss: 0.00147136
Iteration 13/25 | Loss: 0.00146621
Iteration 14/25 | Loss: 0.00147047
Iteration 15/25 | Loss: 0.00146489
Iteration 16/25 | Loss: 0.00145948
Iteration 17/25 | Loss: 0.00145400
Iteration 18/25 | Loss: 0.00144366
Iteration 19/25 | Loss: 0.00144977
Iteration 20/25 | Loss: 0.00144535
Iteration 21/25 | Loss: 0.00144670
Iteration 22/25 | Loss: 0.00144035
Iteration 23/25 | Loss: 0.00143749
Iteration 24/25 | Loss: 0.00143988
Iteration 25/25 | Loss: 0.00143689

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27588868
Iteration 2/25 | Loss: 0.00242363
Iteration 3/25 | Loss: 0.00211300
Iteration 4/25 | Loss: 0.00211300
Iteration 5/25 | Loss: 0.00211300
Iteration 6/25 | Loss: 0.00211300
Iteration 7/25 | Loss: 0.00211300
Iteration 8/25 | Loss: 0.00211300
Iteration 9/25 | Loss: 0.00211299
Iteration 10/25 | Loss: 0.00211299
Iteration 11/25 | Loss: 0.00211299
Iteration 12/25 | Loss: 0.00211299
Iteration 13/25 | Loss: 0.00211299
Iteration 14/25 | Loss: 0.00211299
Iteration 15/25 | Loss: 0.00211299
Iteration 16/25 | Loss: 0.00211299
Iteration 17/25 | Loss: 0.00211299
Iteration 18/25 | Loss: 0.00211299
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002112994436174631, 0.002112994436174631, 0.002112994436174631, 0.002112994436174631, 0.002112994436174631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002112994436174631

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211299
Iteration 2/1000 | Loss: 0.00062050
Iteration 3/1000 | Loss: 0.00510845
Iteration 4/1000 | Loss: 0.00547401
Iteration 5/1000 | Loss: 0.00203530
Iteration 6/1000 | Loss: 0.00381787
Iteration 7/1000 | Loss: 0.00182957
Iteration 8/1000 | Loss: 0.00139451
Iteration 9/1000 | Loss: 0.00188472
Iteration 10/1000 | Loss: 0.00143900
Iteration 11/1000 | Loss: 0.00124839
Iteration 12/1000 | Loss: 0.00199406
Iteration 13/1000 | Loss: 0.00195062
Iteration 14/1000 | Loss: 0.00109570
Iteration 15/1000 | Loss: 0.00131463
Iteration 16/1000 | Loss: 0.00147036
Iteration 17/1000 | Loss: 0.00107228
Iteration 18/1000 | Loss: 0.00129901
Iteration 19/1000 | Loss: 0.00060171
Iteration 20/1000 | Loss: 0.00023629
Iteration 21/1000 | Loss: 0.00021533
Iteration 22/1000 | Loss: 0.00026999
Iteration 23/1000 | Loss: 0.00027396
Iteration 24/1000 | Loss: 0.00097112
Iteration 25/1000 | Loss: 0.00045163
Iteration 26/1000 | Loss: 0.00068805
Iteration 27/1000 | Loss: 0.00066910
Iteration 28/1000 | Loss: 0.00051698
Iteration 29/1000 | Loss: 0.00027750
Iteration 30/1000 | Loss: 0.00101155
Iteration 31/1000 | Loss: 0.00191773
Iteration 32/1000 | Loss: 0.00087225
Iteration 33/1000 | Loss: 0.00172594
Iteration 34/1000 | Loss: 0.00097209
Iteration 35/1000 | Loss: 0.00125978
Iteration 36/1000 | Loss: 0.00176149
Iteration 37/1000 | Loss: 0.00148484
Iteration 38/1000 | Loss: 0.00178091
Iteration 39/1000 | Loss: 0.00102925
Iteration 40/1000 | Loss: 0.00096042
Iteration 41/1000 | Loss: 0.00092932
Iteration 42/1000 | Loss: 0.00125752
Iteration 43/1000 | Loss: 0.00114559
Iteration 44/1000 | Loss: 0.00090723
Iteration 45/1000 | Loss: 0.00109684
Iteration 46/1000 | Loss: 0.00128154
Iteration 47/1000 | Loss: 0.00166498
Iteration 48/1000 | Loss: 0.00065380
Iteration 49/1000 | Loss: 0.00107445
Iteration 50/1000 | Loss: 0.00121445
Iteration 51/1000 | Loss: 0.00051611
Iteration 52/1000 | Loss: 0.00015229
Iteration 53/1000 | Loss: 0.00149155
Iteration 54/1000 | Loss: 0.00068104
Iteration 55/1000 | Loss: 0.00049670
Iteration 56/1000 | Loss: 0.00020539
Iteration 57/1000 | Loss: 0.00062519
Iteration 58/1000 | Loss: 0.00042716
Iteration 59/1000 | Loss: 0.00125443
Iteration 60/1000 | Loss: 0.00020744
Iteration 61/1000 | Loss: 0.00012177
Iteration 62/1000 | Loss: 0.00025517
Iteration 63/1000 | Loss: 0.00007221
Iteration 64/1000 | Loss: 0.00047947
Iteration 65/1000 | Loss: 0.00028977
Iteration 66/1000 | Loss: 0.00034459
Iteration 67/1000 | Loss: 0.00084181
Iteration 68/1000 | Loss: 0.00072050
Iteration 69/1000 | Loss: 0.00074016
Iteration 70/1000 | Loss: 0.00051250
Iteration 71/1000 | Loss: 0.00016133
Iteration 72/1000 | Loss: 0.00071705
Iteration 73/1000 | Loss: 0.00064170
Iteration 74/1000 | Loss: 0.00056465
Iteration 75/1000 | Loss: 0.00049737
Iteration 76/1000 | Loss: 0.00042425
Iteration 77/1000 | Loss: 0.00081044
Iteration 78/1000 | Loss: 0.00072629
Iteration 79/1000 | Loss: 0.00092146
Iteration 80/1000 | Loss: 0.00061649
Iteration 81/1000 | Loss: 0.00081823
Iteration 82/1000 | Loss: 0.00035972
Iteration 83/1000 | Loss: 0.00097199
Iteration 84/1000 | Loss: 0.00065943
Iteration 85/1000 | Loss: 0.00100408
Iteration 86/1000 | Loss: 0.00075956
Iteration 87/1000 | Loss: 0.00075906
Iteration 88/1000 | Loss: 0.00031844
Iteration 89/1000 | Loss: 0.00027842
Iteration 90/1000 | Loss: 0.00032543
Iteration 91/1000 | Loss: 0.00038643
Iteration 92/1000 | Loss: 0.00016730
Iteration 93/1000 | Loss: 0.00047549
Iteration 94/1000 | Loss: 0.00041258
Iteration 95/1000 | Loss: 0.00037713
Iteration 96/1000 | Loss: 0.00028902
Iteration 97/1000 | Loss: 0.00057926
Iteration 98/1000 | Loss: 0.00072846
Iteration 99/1000 | Loss: 0.00060672
Iteration 100/1000 | Loss: 0.00007392
Iteration 101/1000 | Loss: 0.00039740
Iteration 102/1000 | Loss: 0.00021779
Iteration 103/1000 | Loss: 0.00025795
Iteration 104/1000 | Loss: 0.00038357
Iteration 105/1000 | Loss: 0.00075955
Iteration 106/1000 | Loss: 0.00022512
Iteration 107/1000 | Loss: 0.00048703
Iteration 108/1000 | Loss: 0.00045327
Iteration 109/1000 | Loss: 0.00081195
Iteration 110/1000 | Loss: 0.00165507
Iteration 111/1000 | Loss: 0.00025754
Iteration 112/1000 | Loss: 0.00011804
Iteration 113/1000 | Loss: 0.00073429
Iteration 114/1000 | Loss: 0.00016994
Iteration 115/1000 | Loss: 0.00012222
Iteration 116/1000 | Loss: 0.00010457
Iteration 117/1000 | Loss: 0.00014785
Iteration 118/1000 | Loss: 0.00017856
Iteration 119/1000 | Loss: 0.00005499
Iteration 120/1000 | Loss: 0.00004992
Iteration 121/1000 | Loss: 0.00091994
Iteration 122/1000 | Loss: 0.00050414
Iteration 123/1000 | Loss: 0.00012294
Iteration 124/1000 | Loss: 0.00010699
Iteration 125/1000 | Loss: 0.00011987
Iteration 126/1000 | Loss: 0.00089870
Iteration 127/1000 | Loss: 0.00032304
Iteration 128/1000 | Loss: 0.00008505
Iteration 129/1000 | Loss: 0.00090144
Iteration 130/1000 | Loss: 0.00034300
Iteration 131/1000 | Loss: 0.00083889
Iteration 132/1000 | Loss: 0.00037317
Iteration 133/1000 | Loss: 0.00023999
Iteration 134/1000 | Loss: 0.00008107
Iteration 135/1000 | Loss: 0.00015602
Iteration 136/1000 | Loss: 0.00010262
Iteration 137/1000 | Loss: 0.00016055
Iteration 138/1000 | Loss: 0.00015398
Iteration 139/1000 | Loss: 0.00015056
Iteration 140/1000 | Loss: 0.00014946
Iteration 141/1000 | Loss: 0.00015577
Iteration 142/1000 | Loss: 0.00081162
Iteration 143/1000 | Loss: 0.00058240
Iteration 144/1000 | Loss: 0.00011987
Iteration 145/1000 | Loss: 0.00013196
Iteration 146/1000 | Loss: 0.00011723
Iteration 147/1000 | Loss: 0.00012647
Iteration 148/1000 | Loss: 0.00084585
Iteration 149/1000 | Loss: 0.00021856
Iteration 150/1000 | Loss: 0.00012373
Iteration 151/1000 | Loss: 0.00012046
Iteration 152/1000 | Loss: 0.00017259
Iteration 153/1000 | Loss: 0.00018209
Iteration 154/1000 | Loss: 0.00087886
Iteration 155/1000 | Loss: 0.00029171
Iteration 156/1000 | Loss: 0.00011161
Iteration 157/1000 | Loss: 0.00091154
Iteration 158/1000 | Loss: 0.00070897
Iteration 159/1000 | Loss: 0.00015853
Iteration 160/1000 | Loss: 0.00003817
Iteration 161/1000 | Loss: 0.00004210
Iteration 162/1000 | Loss: 0.00003143
Iteration 163/1000 | Loss: 0.00003352
Iteration 164/1000 | Loss: 0.00011847
Iteration 165/1000 | Loss: 0.00015534
Iteration 166/1000 | Loss: 0.00003666
Iteration 167/1000 | Loss: 0.00018935
Iteration 168/1000 | Loss: 0.00016174
Iteration 169/1000 | Loss: 0.00018400
Iteration 170/1000 | Loss: 0.00014305
Iteration 171/1000 | Loss: 0.00013520
Iteration 172/1000 | Loss: 0.00053760
Iteration 173/1000 | Loss: 0.00014766
Iteration 174/1000 | Loss: 0.00017297
Iteration 175/1000 | Loss: 0.00005340
Iteration 176/1000 | Loss: 0.00006507
Iteration 177/1000 | Loss: 0.00048661
Iteration 178/1000 | Loss: 0.00025044
Iteration 179/1000 | Loss: 0.00038022
Iteration 180/1000 | Loss: 0.00018719
Iteration 181/1000 | Loss: 0.00033538
Iteration 182/1000 | Loss: 0.00029255
Iteration 183/1000 | Loss: 0.00053751
Iteration 184/1000 | Loss: 0.00023739
Iteration 185/1000 | Loss: 0.00060730
Iteration 186/1000 | Loss: 0.00010143
Iteration 187/1000 | Loss: 0.00045625
Iteration 188/1000 | Loss: 0.00010743
Iteration 189/1000 | Loss: 0.00006237
Iteration 190/1000 | Loss: 0.00011071
Iteration 191/1000 | Loss: 0.00004877
Iteration 192/1000 | Loss: 0.00011929
Iteration 193/1000 | Loss: 0.00014114
Iteration 194/1000 | Loss: 0.00015046
Iteration 195/1000 | Loss: 0.00014668
Iteration 196/1000 | Loss: 0.00015036
Iteration 197/1000 | Loss: 0.00009330
Iteration 198/1000 | Loss: 0.00014890
Iteration 199/1000 | Loss: 0.00023051
Iteration 200/1000 | Loss: 0.00015908
Iteration 201/1000 | Loss: 0.00004020
Iteration 202/1000 | Loss: 0.00005915
Iteration 203/1000 | Loss: 0.00009474
Iteration 204/1000 | Loss: 0.00012020
Iteration 205/1000 | Loss: 0.00008768
Iteration 206/1000 | Loss: 0.00011187
Iteration 207/1000 | Loss: 0.00009001
Iteration 208/1000 | Loss: 0.00003753
Iteration 209/1000 | Loss: 0.00010188
Iteration 210/1000 | Loss: 0.00021781
Iteration 211/1000 | Loss: 0.00004827
Iteration 212/1000 | Loss: 0.00003946
Iteration 213/1000 | Loss: 0.00013139
Iteration 214/1000 | Loss: 0.00011286
Iteration 215/1000 | Loss: 0.00014141
Iteration 216/1000 | Loss: 0.00017600
Iteration 217/1000 | Loss: 0.00018594
Iteration 218/1000 | Loss: 0.00018406
Iteration 219/1000 | Loss: 0.00006103
Iteration 220/1000 | Loss: 0.00020875
Iteration 221/1000 | Loss: 0.00025019
Iteration 222/1000 | Loss: 0.00019661
Iteration 223/1000 | Loss: 0.00010039
Iteration 224/1000 | Loss: 0.00022428
Iteration 225/1000 | Loss: 0.00004748
Iteration 226/1000 | Loss: 0.00021499
Iteration 227/1000 | Loss: 0.00010142
Iteration 228/1000 | Loss: 0.00005259
Iteration 229/1000 | Loss: 0.00005648
Iteration 230/1000 | Loss: 0.00004627
Iteration 231/1000 | Loss: 0.00004495
Iteration 232/1000 | Loss: 0.00004349
Iteration 233/1000 | Loss: 0.00004165
Iteration 234/1000 | Loss: 0.00004051
Iteration 235/1000 | Loss: 0.00004315
Iteration 236/1000 | Loss: 0.00004179
Iteration 237/1000 | Loss: 0.00003633
Iteration 238/1000 | Loss: 0.00003686
Iteration 239/1000 | Loss: 0.00003264
Iteration 240/1000 | Loss: 0.00004192
Iteration 241/1000 | Loss: 0.00004090
Iteration 242/1000 | Loss: 0.00006521
Iteration 243/1000 | Loss: 0.00004394
Iteration 244/1000 | Loss: 0.00004381
Iteration 245/1000 | Loss: 0.00003805
Iteration 246/1000 | Loss: 0.00035469
Iteration 247/1000 | Loss: 0.00019877
Iteration 248/1000 | Loss: 0.00004696
Iteration 249/1000 | Loss: 0.00003906
Iteration 250/1000 | Loss: 0.00004153
Iteration 251/1000 | Loss: 0.00002904
Iteration 252/1000 | Loss: 0.00004641
Iteration 253/1000 | Loss: 0.00004851
Iteration 254/1000 | Loss: 0.00005034
Iteration 255/1000 | Loss: 0.00003358
Iteration 256/1000 | Loss: 0.00002784
Iteration 257/1000 | Loss: 0.00002611
Iteration 258/1000 | Loss: 0.00002517
Iteration 259/1000 | Loss: 0.00002454
Iteration 260/1000 | Loss: 0.00015225
Iteration 261/1000 | Loss: 0.00075351
Iteration 262/1000 | Loss: 0.00007595
Iteration 263/1000 | Loss: 0.00064199
Iteration 264/1000 | Loss: 0.00075301
Iteration 265/1000 | Loss: 0.00056935
Iteration 266/1000 | Loss: 0.00003624
Iteration 267/1000 | Loss: 0.00002783
Iteration 268/1000 | Loss: 0.00002430
Iteration 269/1000 | Loss: 0.00002290
Iteration 270/1000 | Loss: 0.00002209
Iteration 271/1000 | Loss: 0.00002166
Iteration 272/1000 | Loss: 0.00002135
Iteration 273/1000 | Loss: 0.00002112
Iteration 274/1000 | Loss: 0.00002093
Iteration 275/1000 | Loss: 0.00002091
Iteration 276/1000 | Loss: 0.00002089
Iteration 277/1000 | Loss: 0.00002089
Iteration 278/1000 | Loss: 0.00002085
Iteration 279/1000 | Loss: 0.00002084
Iteration 280/1000 | Loss: 0.00002084
Iteration 281/1000 | Loss: 0.00002083
Iteration 282/1000 | Loss: 0.00002083
Iteration 283/1000 | Loss: 0.00002081
Iteration 284/1000 | Loss: 0.00002080
Iteration 285/1000 | Loss: 0.00002080
Iteration 286/1000 | Loss: 0.00002078
Iteration 287/1000 | Loss: 0.00002077
Iteration 288/1000 | Loss: 0.00002076
Iteration 289/1000 | Loss: 0.00002076
Iteration 290/1000 | Loss: 0.00002076
Iteration 291/1000 | Loss: 0.00002075
Iteration 292/1000 | Loss: 0.00002075
Iteration 293/1000 | Loss: 0.00002074
Iteration 294/1000 | Loss: 0.00002074
Iteration 295/1000 | Loss: 0.00002073
Iteration 296/1000 | Loss: 0.00002073
Iteration 297/1000 | Loss: 0.00002072
Iteration 298/1000 | Loss: 0.00002070
Iteration 299/1000 | Loss: 0.00002070
Iteration 300/1000 | Loss: 0.00002069
Iteration 301/1000 | Loss: 0.00002069
Iteration 302/1000 | Loss: 0.00002068
Iteration 303/1000 | Loss: 0.00002064
Iteration 304/1000 | Loss: 0.00002063
Iteration 305/1000 | Loss: 0.00002062
Iteration 306/1000 | Loss: 0.00002061
Iteration 307/1000 | Loss: 0.00002059
Iteration 308/1000 | Loss: 0.00002058
Iteration 309/1000 | Loss: 0.00002058
Iteration 310/1000 | Loss: 0.00002058
Iteration 311/1000 | Loss: 0.00002058
Iteration 312/1000 | Loss: 0.00002057
Iteration 313/1000 | Loss: 0.00002057
Iteration 314/1000 | Loss: 0.00002056
Iteration 315/1000 | Loss: 0.00002056
Iteration 316/1000 | Loss: 0.00002056
Iteration 317/1000 | Loss: 0.00002055
Iteration 318/1000 | Loss: 0.00002055
Iteration 319/1000 | Loss: 0.00002055
Iteration 320/1000 | Loss: 0.00002055
Iteration 321/1000 | Loss: 0.00002055
Iteration 322/1000 | Loss: 0.00002054
Iteration 323/1000 | Loss: 0.00002054
Iteration 324/1000 | Loss: 0.00002054
Iteration 325/1000 | Loss: 0.00002053
Iteration 326/1000 | Loss: 0.00002053
Iteration 327/1000 | Loss: 0.00002052
Iteration 328/1000 | Loss: 0.00002052
Iteration 329/1000 | Loss: 0.00002051
Iteration 330/1000 | Loss: 0.00002051
Iteration 331/1000 | Loss: 0.00002050
Iteration 332/1000 | Loss: 0.00002050
Iteration 333/1000 | Loss: 0.00002049
Iteration 334/1000 | Loss: 0.00002048
Iteration 335/1000 | Loss: 0.00002048
Iteration 336/1000 | Loss: 0.00002047
Iteration 337/1000 | Loss: 0.00002047
Iteration 338/1000 | Loss: 0.00002047
Iteration 339/1000 | Loss: 0.00002047
Iteration 340/1000 | Loss: 0.00002046
Iteration 341/1000 | Loss: 0.00002046
Iteration 342/1000 | Loss: 0.00002046
Iteration 343/1000 | Loss: 0.00002046
Iteration 344/1000 | Loss: 0.00002045
Iteration 345/1000 | Loss: 0.00002045
Iteration 346/1000 | Loss: 0.00002045
Iteration 347/1000 | Loss: 0.00002045
Iteration 348/1000 | Loss: 0.00002045
Iteration 349/1000 | Loss: 0.00002045
Iteration 350/1000 | Loss: 0.00002044
Iteration 351/1000 | Loss: 0.00002044
Iteration 352/1000 | Loss: 0.00002044
Iteration 353/1000 | Loss: 0.00002044
Iteration 354/1000 | Loss: 0.00002044
Iteration 355/1000 | Loss: 0.00002044
Iteration 356/1000 | Loss: 0.00002044
Iteration 357/1000 | Loss: 0.00002044
Iteration 358/1000 | Loss: 0.00002043
Iteration 359/1000 | Loss: 0.00002043
Iteration 360/1000 | Loss: 0.00002043
Iteration 361/1000 | Loss: 0.00002043
Iteration 362/1000 | Loss: 0.00002043
Iteration 363/1000 | Loss: 0.00002043
Iteration 364/1000 | Loss: 0.00002043
Iteration 365/1000 | Loss: 0.00002043
Iteration 366/1000 | Loss: 0.00002043
Iteration 367/1000 | Loss: 0.00002043
Iteration 368/1000 | Loss: 0.00002043
Iteration 369/1000 | Loss: 0.00002043
Iteration 370/1000 | Loss: 0.00002043
Iteration 371/1000 | Loss: 0.00002043
Iteration 372/1000 | Loss: 0.00002043
Iteration 373/1000 | Loss: 0.00002043
Iteration 374/1000 | Loss: 0.00002043
Iteration 375/1000 | Loss: 0.00002043
Iteration 376/1000 | Loss: 0.00002043
Iteration 377/1000 | Loss: 0.00002043
Iteration 378/1000 | Loss: 0.00002043
Iteration 379/1000 | Loss: 0.00002043
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 379. Stopping optimization.
Last 5 losses: [2.042682172032073e-05, 2.042682172032073e-05, 2.042682172032073e-05, 2.042682172032073e-05, 2.042682172032073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.042682172032073e-05

Optimization complete. Final v2v error: 3.851118326187134 mm

Highest mean error: 9.140422821044922 mm for frame 107

Lowest mean error: 3.504183292388916 mm for frame 0

Saving results

Total time: 443.261159658432
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062864
Iteration 2/25 | Loss: 0.00353709
Iteration 3/25 | Loss: 0.00270373
Iteration 4/25 | Loss: 0.00241370
Iteration 5/25 | Loss: 0.00260384
Iteration 6/25 | Loss: 0.00221036
Iteration 7/25 | Loss: 0.00191133
Iteration 8/25 | Loss: 0.00157576
Iteration 9/25 | Loss: 0.00142738
Iteration 10/25 | Loss: 0.00140034
Iteration 11/25 | Loss: 0.00139743
Iteration 12/25 | Loss: 0.00139681
Iteration 13/25 | Loss: 0.00139666
Iteration 14/25 | Loss: 0.00139666
Iteration 15/25 | Loss: 0.00139666
Iteration 16/25 | Loss: 0.00139666
Iteration 17/25 | Loss: 0.00139666
Iteration 18/25 | Loss: 0.00139666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001396660809405148, 0.001396660809405148, 0.001396660809405148, 0.001396660809405148, 0.001396660809405148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001396660809405148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25184572
Iteration 2/25 | Loss: 0.00148504
Iteration 3/25 | Loss: 0.00148504
Iteration 4/25 | Loss: 0.00148504
Iteration 5/25 | Loss: 0.00148504
Iteration 6/25 | Loss: 0.00148503
Iteration 7/25 | Loss: 0.00148503
Iteration 8/25 | Loss: 0.00148503
Iteration 9/25 | Loss: 0.00148503
Iteration 10/25 | Loss: 0.00148503
Iteration 11/25 | Loss: 0.00148503
Iteration 12/25 | Loss: 0.00148503
Iteration 13/25 | Loss: 0.00148503
Iteration 14/25 | Loss: 0.00148503
Iteration 15/25 | Loss: 0.00148503
Iteration 16/25 | Loss: 0.00148503
Iteration 17/25 | Loss: 0.00148503
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014850333100184798, 0.0014850333100184798, 0.0014850333100184798, 0.0014850333100184798, 0.0014850333100184798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014850333100184798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148503
Iteration 2/1000 | Loss: 0.00004695
Iteration 3/1000 | Loss: 0.00003229
Iteration 4/1000 | Loss: 0.00002742
Iteration 5/1000 | Loss: 0.00002450
Iteration 6/1000 | Loss: 0.00002335
Iteration 7/1000 | Loss: 0.00002256
Iteration 8/1000 | Loss: 0.00002211
Iteration 9/1000 | Loss: 0.00002161
Iteration 10/1000 | Loss: 0.00002122
Iteration 11/1000 | Loss: 0.00002091
Iteration 12/1000 | Loss: 0.00002071
Iteration 13/1000 | Loss: 0.00002066
Iteration 14/1000 | Loss: 0.00002066
Iteration 15/1000 | Loss: 0.00002066
Iteration 16/1000 | Loss: 0.00002066
Iteration 17/1000 | Loss: 0.00002066
Iteration 18/1000 | Loss: 0.00002066
Iteration 19/1000 | Loss: 0.00002066
Iteration 20/1000 | Loss: 0.00002066
Iteration 21/1000 | Loss: 0.00002064
Iteration 22/1000 | Loss: 0.00002063
Iteration 23/1000 | Loss: 0.00002062
Iteration 24/1000 | Loss: 0.00002062
Iteration 25/1000 | Loss: 0.00002061
Iteration 26/1000 | Loss: 0.00002061
Iteration 27/1000 | Loss: 0.00002061
Iteration 28/1000 | Loss: 0.00002060
Iteration 29/1000 | Loss: 0.00002060
Iteration 30/1000 | Loss: 0.00002060
Iteration 31/1000 | Loss: 0.00002060
Iteration 32/1000 | Loss: 0.00002060
Iteration 33/1000 | Loss: 0.00002060
Iteration 34/1000 | Loss: 0.00002060
Iteration 35/1000 | Loss: 0.00002060
Iteration 36/1000 | Loss: 0.00002060
Iteration 37/1000 | Loss: 0.00002059
Iteration 38/1000 | Loss: 0.00002059
Iteration 39/1000 | Loss: 0.00002058
Iteration 40/1000 | Loss: 0.00002057
Iteration 41/1000 | Loss: 0.00002056
Iteration 42/1000 | Loss: 0.00002056
Iteration 43/1000 | Loss: 0.00002056
Iteration 44/1000 | Loss: 0.00002056
Iteration 45/1000 | Loss: 0.00002056
Iteration 46/1000 | Loss: 0.00002056
Iteration 47/1000 | Loss: 0.00002056
Iteration 48/1000 | Loss: 0.00002056
Iteration 49/1000 | Loss: 0.00002056
Iteration 50/1000 | Loss: 0.00002056
Iteration 51/1000 | Loss: 0.00002055
Iteration 52/1000 | Loss: 0.00002055
Iteration 53/1000 | Loss: 0.00002055
Iteration 54/1000 | Loss: 0.00002055
Iteration 55/1000 | Loss: 0.00002055
Iteration 56/1000 | Loss: 0.00002054
Iteration 57/1000 | Loss: 0.00002054
Iteration 58/1000 | Loss: 0.00002054
Iteration 59/1000 | Loss: 0.00002054
Iteration 60/1000 | Loss: 0.00002054
Iteration 61/1000 | Loss: 0.00002054
Iteration 62/1000 | Loss: 0.00002053
Iteration 63/1000 | Loss: 0.00002053
Iteration 64/1000 | Loss: 0.00002053
Iteration 65/1000 | Loss: 0.00002053
Iteration 66/1000 | Loss: 0.00002053
Iteration 67/1000 | Loss: 0.00002053
Iteration 68/1000 | Loss: 0.00002053
Iteration 69/1000 | Loss: 0.00002053
Iteration 70/1000 | Loss: 0.00002053
Iteration 71/1000 | Loss: 0.00002053
Iteration 72/1000 | Loss: 0.00002052
Iteration 73/1000 | Loss: 0.00002052
Iteration 74/1000 | Loss: 0.00002052
Iteration 75/1000 | Loss: 0.00002052
Iteration 76/1000 | Loss: 0.00002052
Iteration 77/1000 | Loss: 0.00002052
Iteration 78/1000 | Loss: 0.00002052
Iteration 79/1000 | Loss: 0.00002052
Iteration 80/1000 | Loss: 0.00002052
Iteration 81/1000 | Loss: 0.00002052
Iteration 82/1000 | Loss: 0.00002052
Iteration 83/1000 | Loss: 0.00002052
Iteration 84/1000 | Loss: 0.00002052
Iteration 85/1000 | Loss: 0.00002052
Iteration 86/1000 | Loss: 0.00002052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [2.052108357020188e-05, 2.052108357020188e-05, 2.052108357020188e-05, 2.052108357020188e-05, 2.052108357020188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.052108357020188e-05

Optimization complete. Final v2v error: 3.7831387519836426 mm

Highest mean error: 3.9569363594055176 mm for frame 139

Lowest mean error: 3.6436569690704346 mm for frame 102

Saving results

Total time: 42.364274978637695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00702903
Iteration 2/25 | Loss: 0.00161653
Iteration 3/25 | Loss: 0.00144556
Iteration 4/25 | Loss: 0.00142694
Iteration 5/25 | Loss: 0.00142451
Iteration 6/25 | Loss: 0.00142424
Iteration 7/25 | Loss: 0.00142424
Iteration 8/25 | Loss: 0.00142424
Iteration 9/25 | Loss: 0.00142424
Iteration 10/25 | Loss: 0.00142424
Iteration 11/25 | Loss: 0.00142424
Iteration 12/25 | Loss: 0.00142424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014242418110370636, 0.0014242418110370636, 0.0014242418110370636, 0.0014242418110370636, 0.0014242418110370636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014242418110370636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19495332
Iteration 2/25 | Loss: 0.00180393
Iteration 3/25 | Loss: 0.00180393
Iteration 4/25 | Loss: 0.00180393
Iteration 5/25 | Loss: 0.00180393
Iteration 6/25 | Loss: 0.00180393
Iteration 7/25 | Loss: 0.00180393
Iteration 8/25 | Loss: 0.00180393
Iteration 9/25 | Loss: 0.00180393
Iteration 10/25 | Loss: 0.00180393
Iteration 11/25 | Loss: 0.00180393
Iteration 12/25 | Loss: 0.00180393
Iteration 13/25 | Loss: 0.00180393
Iteration 14/25 | Loss: 0.00180393
Iteration 15/25 | Loss: 0.00180393
Iteration 16/25 | Loss: 0.00180393
Iteration 17/25 | Loss: 0.00180393
Iteration 18/25 | Loss: 0.00180393
Iteration 19/25 | Loss: 0.00180393
Iteration 20/25 | Loss: 0.00180393
Iteration 21/25 | Loss: 0.00180393
Iteration 22/25 | Loss: 0.00180393
Iteration 23/25 | Loss: 0.00180393
Iteration 24/25 | Loss: 0.00180393
Iteration 25/25 | Loss: 0.00180393

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180393
Iteration 2/1000 | Loss: 0.00006874
Iteration 3/1000 | Loss: 0.00004473
Iteration 4/1000 | Loss: 0.00003096
Iteration 5/1000 | Loss: 0.00002777
Iteration 6/1000 | Loss: 0.00002493
Iteration 7/1000 | Loss: 0.00002392
Iteration 8/1000 | Loss: 0.00002319
Iteration 9/1000 | Loss: 0.00002271
Iteration 10/1000 | Loss: 0.00002234
Iteration 11/1000 | Loss: 0.00002200
Iteration 12/1000 | Loss: 0.00002176
Iteration 13/1000 | Loss: 0.00002170
Iteration 14/1000 | Loss: 0.00002151
Iteration 15/1000 | Loss: 0.00002147
Iteration 16/1000 | Loss: 0.00002147
Iteration 17/1000 | Loss: 0.00002142
Iteration 18/1000 | Loss: 0.00002138
Iteration 19/1000 | Loss: 0.00002135
Iteration 20/1000 | Loss: 0.00002134
Iteration 21/1000 | Loss: 0.00002134
Iteration 22/1000 | Loss: 0.00002133
Iteration 23/1000 | Loss: 0.00002133
Iteration 24/1000 | Loss: 0.00002132
Iteration 25/1000 | Loss: 0.00002131
Iteration 26/1000 | Loss: 0.00002131
Iteration 27/1000 | Loss: 0.00002131
Iteration 28/1000 | Loss: 0.00002131
Iteration 29/1000 | Loss: 0.00002130
Iteration 30/1000 | Loss: 0.00002130
Iteration 31/1000 | Loss: 0.00002130
Iteration 32/1000 | Loss: 0.00002130
Iteration 33/1000 | Loss: 0.00002129
Iteration 34/1000 | Loss: 0.00002129
Iteration 35/1000 | Loss: 0.00002129
Iteration 36/1000 | Loss: 0.00002129
Iteration 37/1000 | Loss: 0.00002129
Iteration 38/1000 | Loss: 0.00002128
Iteration 39/1000 | Loss: 0.00002128
Iteration 40/1000 | Loss: 0.00002128
Iteration 41/1000 | Loss: 0.00002128
Iteration 42/1000 | Loss: 0.00002128
Iteration 43/1000 | Loss: 0.00002127
Iteration 44/1000 | Loss: 0.00002127
Iteration 45/1000 | Loss: 0.00002127
Iteration 46/1000 | Loss: 0.00002127
Iteration 47/1000 | Loss: 0.00002127
Iteration 48/1000 | Loss: 0.00002126
Iteration 49/1000 | Loss: 0.00002126
Iteration 50/1000 | Loss: 0.00002126
Iteration 51/1000 | Loss: 0.00002126
Iteration 52/1000 | Loss: 0.00002126
Iteration 53/1000 | Loss: 0.00002126
Iteration 54/1000 | Loss: 0.00002125
Iteration 55/1000 | Loss: 0.00002125
Iteration 56/1000 | Loss: 0.00002125
Iteration 57/1000 | Loss: 0.00002125
Iteration 58/1000 | Loss: 0.00002124
Iteration 59/1000 | Loss: 0.00002124
Iteration 60/1000 | Loss: 0.00002124
Iteration 61/1000 | Loss: 0.00002123
Iteration 62/1000 | Loss: 0.00002123
Iteration 63/1000 | Loss: 0.00002123
Iteration 64/1000 | Loss: 0.00002123
Iteration 65/1000 | Loss: 0.00002123
Iteration 66/1000 | Loss: 0.00002123
Iteration 67/1000 | Loss: 0.00002123
Iteration 68/1000 | Loss: 0.00002122
Iteration 69/1000 | Loss: 0.00002122
Iteration 70/1000 | Loss: 0.00002122
Iteration 71/1000 | Loss: 0.00002122
Iteration 72/1000 | Loss: 0.00002122
Iteration 73/1000 | Loss: 0.00002122
Iteration 74/1000 | Loss: 0.00002121
Iteration 75/1000 | Loss: 0.00002121
Iteration 76/1000 | Loss: 0.00002121
Iteration 77/1000 | Loss: 0.00002121
Iteration 78/1000 | Loss: 0.00002120
Iteration 79/1000 | Loss: 0.00002120
Iteration 80/1000 | Loss: 0.00002120
Iteration 81/1000 | Loss: 0.00002120
Iteration 82/1000 | Loss: 0.00002120
Iteration 83/1000 | Loss: 0.00002120
Iteration 84/1000 | Loss: 0.00002120
Iteration 85/1000 | Loss: 0.00002119
Iteration 86/1000 | Loss: 0.00002119
Iteration 87/1000 | Loss: 0.00002119
Iteration 88/1000 | Loss: 0.00002119
Iteration 89/1000 | Loss: 0.00002119
Iteration 90/1000 | Loss: 0.00002119
Iteration 91/1000 | Loss: 0.00002118
Iteration 92/1000 | Loss: 0.00002118
Iteration 93/1000 | Loss: 0.00002118
Iteration 94/1000 | Loss: 0.00002118
Iteration 95/1000 | Loss: 0.00002118
Iteration 96/1000 | Loss: 0.00002118
Iteration 97/1000 | Loss: 0.00002117
Iteration 98/1000 | Loss: 0.00002117
Iteration 99/1000 | Loss: 0.00002117
Iteration 100/1000 | Loss: 0.00002117
Iteration 101/1000 | Loss: 0.00002117
Iteration 102/1000 | Loss: 0.00002117
Iteration 103/1000 | Loss: 0.00002117
Iteration 104/1000 | Loss: 0.00002117
Iteration 105/1000 | Loss: 0.00002117
Iteration 106/1000 | Loss: 0.00002117
Iteration 107/1000 | Loss: 0.00002117
Iteration 108/1000 | Loss: 0.00002117
Iteration 109/1000 | Loss: 0.00002117
Iteration 110/1000 | Loss: 0.00002117
Iteration 111/1000 | Loss: 0.00002117
Iteration 112/1000 | Loss: 0.00002116
Iteration 113/1000 | Loss: 0.00002116
Iteration 114/1000 | Loss: 0.00002116
Iteration 115/1000 | Loss: 0.00002116
Iteration 116/1000 | Loss: 0.00002116
Iteration 117/1000 | Loss: 0.00002116
Iteration 118/1000 | Loss: 0.00002116
Iteration 119/1000 | Loss: 0.00002116
Iteration 120/1000 | Loss: 0.00002116
Iteration 121/1000 | Loss: 0.00002116
Iteration 122/1000 | Loss: 0.00002116
Iteration 123/1000 | Loss: 0.00002116
Iteration 124/1000 | Loss: 0.00002116
Iteration 125/1000 | Loss: 0.00002116
Iteration 126/1000 | Loss: 0.00002116
Iteration 127/1000 | Loss: 0.00002116
Iteration 128/1000 | Loss: 0.00002115
Iteration 129/1000 | Loss: 0.00002115
Iteration 130/1000 | Loss: 0.00002115
Iteration 131/1000 | Loss: 0.00002115
Iteration 132/1000 | Loss: 0.00002115
Iteration 133/1000 | Loss: 0.00002115
Iteration 134/1000 | Loss: 0.00002115
Iteration 135/1000 | Loss: 0.00002115
Iteration 136/1000 | Loss: 0.00002115
Iteration 137/1000 | Loss: 0.00002115
Iteration 138/1000 | Loss: 0.00002115
Iteration 139/1000 | Loss: 0.00002115
Iteration 140/1000 | Loss: 0.00002115
Iteration 141/1000 | Loss: 0.00002115
Iteration 142/1000 | Loss: 0.00002114
Iteration 143/1000 | Loss: 0.00002114
Iteration 144/1000 | Loss: 0.00002114
Iteration 145/1000 | Loss: 0.00002114
Iteration 146/1000 | Loss: 0.00002114
Iteration 147/1000 | Loss: 0.00002114
Iteration 148/1000 | Loss: 0.00002114
Iteration 149/1000 | Loss: 0.00002114
Iteration 150/1000 | Loss: 0.00002114
Iteration 151/1000 | Loss: 0.00002114
Iteration 152/1000 | Loss: 0.00002114
Iteration 153/1000 | Loss: 0.00002113
Iteration 154/1000 | Loss: 0.00002113
Iteration 155/1000 | Loss: 0.00002113
Iteration 156/1000 | Loss: 0.00002113
Iteration 157/1000 | Loss: 0.00002113
Iteration 158/1000 | Loss: 0.00002113
Iteration 159/1000 | Loss: 0.00002113
Iteration 160/1000 | Loss: 0.00002113
Iteration 161/1000 | Loss: 0.00002113
Iteration 162/1000 | Loss: 0.00002113
Iteration 163/1000 | Loss: 0.00002113
Iteration 164/1000 | Loss: 0.00002113
Iteration 165/1000 | Loss: 0.00002113
Iteration 166/1000 | Loss: 0.00002113
Iteration 167/1000 | Loss: 0.00002113
Iteration 168/1000 | Loss: 0.00002113
Iteration 169/1000 | Loss: 0.00002113
Iteration 170/1000 | Loss: 0.00002113
Iteration 171/1000 | Loss: 0.00002113
Iteration 172/1000 | Loss: 0.00002113
Iteration 173/1000 | Loss: 0.00002113
Iteration 174/1000 | Loss: 0.00002113
Iteration 175/1000 | Loss: 0.00002113
Iteration 176/1000 | Loss: 0.00002113
Iteration 177/1000 | Loss: 0.00002113
Iteration 178/1000 | Loss: 0.00002113
Iteration 179/1000 | Loss: 0.00002113
Iteration 180/1000 | Loss: 0.00002113
Iteration 181/1000 | Loss: 0.00002113
Iteration 182/1000 | Loss: 0.00002113
Iteration 183/1000 | Loss: 0.00002113
Iteration 184/1000 | Loss: 0.00002113
Iteration 185/1000 | Loss: 0.00002113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.1127019863342866e-05, 2.1127019863342866e-05, 2.1127019863342866e-05, 2.1127019863342866e-05, 2.1127019863342866e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1127019863342866e-05

Optimization complete. Final v2v error: 3.88346004486084 mm

Highest mean error: 4.247150897979736 mm for frame 126

Lowest mean error: 3.471090316772461 mm for frame 53

Saving results

Total time: 38.34265351295471
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047309
Iteration 2/25 | Loss: 0.00356894
Iteration 3/25 | Loss: 0.00276816
Iteration 4/25 | Loss: 0.00193860
Iteration 5/25 | Loss: 0.00173386
Iteration 6/25 | Loss: 0.00209734
Iteration 7/25 | Loss: 0.00214747
Iteration 8/25 | Loss: 0.00192803
Iteration 9/25 | Loss: 0.00173250
Iteration 10/25 | Loss: 0.00169212
Iteration 11/25 | Loss: 0.00167618
Iteration 12/25 | Loss: 0.00163275
Iteration 13/25 | Loss: 0.00159619
Iteration 14/25 | Loss: 0.00160280
Iteration 15/25 | Loss: 0.00160598
Iteration 16/25 | Loss: 0.00161715
Iteration 17/25 | Loss: 0.00165123
Iteration 18/25 | Loss: 0.00163884
Iteration 19/25 | Loss: 0.00162619
Iteration 20/25 | Loss: 0.00162337
Iteration 21/25 | Loss: 0.00161280
Iteration 22/25 | Loss: 0.00160034
Iteration 23/25 | Loss: 0.00157268
Iteration 24/25 | Loss: 0.00158283
Iteration 25/25 | Loss: 0.00155316

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21970582
Iteration 2/25 | Loss: 0.00200068
Iteration 3/25 | Loss: 0.00200068
Iteration 4/25 | Loss: 0.00200068
Iteration 5/25 | Loss: 0.00200068
Iteration 6/25 | Loss: 0.00200068
Iteration 7/25 | Loss: 0.00200068
Iteration 8/25 | Loss: 0.00200068
Iteration 9/25 | Loss: 0.00200068
Iteration 10/25 | Loss: 0.00200068
Iteration 11/25 | Loss: 0.00200068
Iteration 12/25 | Loss: 0.00200068
Iteration 13/25 | Loss: 0.00200068
Iteration 14/25 | Loss: 0.00200068
Iteration 15/25 | Loss: 0.00200068
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0020006767008453608, 0.0020006767008453608, 0.0020006767008453608, 0.0020006767008453608, 0.0020006767008453608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020006767008453608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200068
Iteration 2/1000 | Loss: 0.00015535
Iteration 3/1000 | Loss: 0.00030120
Iteration 4/1000 | Loss: 0.00010592
Iteration 5/1000 | Loss: 0.00010132
Iteration 6/1000 | Loss: 0.00025012
Iteration 7/1000 | Loss: 0.00026188
Iteration 8/1000 | Loss: 0.00007755
Iteration 9/1000 | Loss: 0.00008211
Iteration 10/1000 | Loss: 0.00026917
Iteration 11/1000 | Loss: 0.00017461
Iteration 12/1000 | Loss: 0.00008349
Iteration 13/1000 | Loss: 0.00008096
Iteration 14/1000 | Loss: 0.00006588
Iteration 15/1000 | Loss: 0.00008182
Iteration 16/1000 | Loss: 0.00007537
Iteration 17/1000 | Loss: 0.00007440
Iteration 18/1000 | Loss: 0.00009654
Iteration 19/1000 | Loss: 0.00009380
Iteration 20/1000 | Loss: 0.00011320
Iteration 21/1000 | Loss: 0.00011069
Iteration 22/1000 | Loss: 0.00011462
Iteration 23/1000 | Loss: 0.00012943
Iteration 24/1000 | Loss: 0.00012869
Iteration 25/1000 | Loss: 0.00012419
Iteration 26/1000 | Loss: 0.00012522
Iteration 27/1000 | Loss: 0.00011936
Iteration 28/1000 | Loss: 0.00013018
Iteration 29/1000 | Loss: 0.00009351
Iteration 30/1000 | Loss: 0.00012184
Iteration 31/1000 | Loss: 0.00011719
Iteration 32/1000 | Loss: 0.00012795
Iteration 33/1000 | Loss: 0.00011973
Iteration 34/1000 | Loss: 0.00010516
Iteration 35/1000 | Loss: 0.00013478
Iteration 36/1000 | Loss: 0.00010123
Iteration 37/1000 | Loss: 0.00011028
Iteration 38/1000 | Loss: 0.00009187
Iteration 39/1000 | Loss: 0.00008033
Iteration 40/1000 | Loss: 0.00009663
Iteration 41/1000 | Loss: 0.00009208
Iteration 42/1000 | Loss: 0.00009976
Iteration 43/1000 | Loss: 0.00010324
Iteration 44/1000 | Loss: 0.00010595
Iteration 45/1000 | Loss: 0.00015354
Iteration 46/1000 | Loss: 0.00032256
Iteration 47/1000 | Loss: 0.00031475
Iteration 48/1000 | Loss: 0.00032618
Iteration 49/1000 | Loss: 0.00025919
Iteration 50/1000 | Loss: 0.00044844
Iteration 51/1000 | Loss: 0.00043015
Iteration 52/1000 | Loss: 0.00032763
Iteration 53/1000 | Loss: 0.00022320
Iteration 54/1000 | Loss: 0.00030759
Iteration 55/1000 | Loss: 0.00015370
Iteration 56/1000 | Loss: 0.00005457
Iteration 57/1000 | Loss: 0.00034992
Iteration 58/1000 | Loss: 0.00014941
Iteration 59/1000 | Loss: 0.00027750
Iteration 60/1000 | Loss: 0.00043633
Iteration 61/1000 | Loss: 0.00007988
Iteration 62/1000 | Loss: 0.00006458
Iteration 63/1000 | Loss: 0.00025219
Iteration 64/1000 | Loss: 0.00025922
Iteration 65/1000 | Loss: 0.00020737
Iteration 66/1000 | Loss: 0.00033338
Iteration 67/1000 | Loss: 0.00024756
Iteration 68/1000 | Loss: 0.00007317
Iteration 69/1000 | Loss: 0.00005732
Iteration 70/1000 | Loss: 0.00026177
Iteration 71/1000 | Loss: 0.00019650
Iteration 72/1000 | Loss: 0.00013036
Iteration 73/1000 | Loss: 0.00013629
Iteration 74/1000 | Loss: 0.00012889
Iteration 75/1000 | Loss: 0.00005334
Iteration 76/1000 | Loss: 0.00005639
Iteration 77/1000 | Loss: 0.00005352
Iteration 78/1000 | Loss: 0.00004485
Iteration 79/1000 | Loss: 0.00005747
Iteration 80/1000 | Loss: 0.00025222
Iteration 81/1000 | Loss: 0.00098396
Iteration 82/1000 | Loss: 0.00020845
Iteration 83/1000 | Loss: 0.00053476
Iteration 84/1000 | Loss: 0.00071093
Iteration 85/1000 | Loss: 0.00015787
Iteration 86/1000 | Loss: 0.00054241
Iteration 87/1000 | Loss: 0.00029159
Iteration 88/1000 | Loss: 0.00026800
Iteration 89/1000 | Loss: 0.00021290
Iteration 90/1000 | Loss: 0.00037839
Iteration 91/1000 | Loss: 0.00049226
Iteration 92/1000 | Loss: 0.00046391
Iteration 93/1000 | Loss: 0.00089482
Iteration 94/1000 | Loss: 0.00024439
Iteration 95/1000 | Loss: 0.00047116
Iteration 96/1000 | Loss: 0.00030025
Iteration 97/1000 | Loss: 0.00017383
Iteration 98/1000 | Loss: 0.00012185
Iteration 99/1000 | Loss: 0.00050714
Iteration 100/1000 | Loss: 0.00019249
Iteration 101/1000 | Loss: 0.00033454
Iteration 102/1000 | Loss: 0.00047096
Iteration 103/1000 | Loss: 0.00047202
Iteration 104/1000 | Loss: 0.00024629
Iteration 105/1000 | Loss: 0.00032286
Iteration 106/1000 | Loss: 0.00028598
Iteration 107/1000 | Loss: 0.00009587
Iteration 108/1000 | Loss: 0.00024587
Iteration 109/1000 | Loss: 0.00026955
Iteration 110/1000 | Loss: 0.00040869
Iteration 111/1000 | Loss: 0.00064908
Iteration 112/1000 | Loss: 0.00022530
Iteration 113/1000 | Loss: 0.00008640
Iteration 114/1000 | Loss: 0.00007486
Iteration 115/1000 | Loss: 0.00010259
Iteration 116/1000 | Loss: 0.00008247
Iteration 117/1000 | Loss: 0.00006033
Iteration 118/1000 | Loss: 0.00011068
Iteration 119/1000 | Loss: 0.00009124
Iteration 120/1000 | Loss: 0.00010164
Iteration 121/1000 | Loss: 0.00007323
Iteration 122/1000 | Loss: 0.00006150
Iteration 123/1000 | Loss: 0.00026119
Iteration 124/1000 | Loss: 0.00067461
Iteration 125/1000 | Loss: 0.00022895
Iteration 126/1000 | Loss: 0.00010903
Iteration 127/1000 | Loss: 0.00009404
Iteration 128/1000 | Loss: 0.00024347
Iteration 129/1000 | Loss: 0.00021223
Iteration 130/1000 | Loss: 0.00007788
Iteration 131/1000 | Loss: 0.00023354
Iteration 132/1000 | Loss: 0.00018335
Iteration 133/1000 | Loss: 0.00008047
Iteration 134/1000 | Loss: 0.00017465
Iteration 135/1000 | Loss: 0.00027268
Iteration 136/1000 | Loss: 0.00025843
Iteration 137/1000 | Loss: 0.00020648
Iteration 138/1000 | Loss: 0.00012729
Iteration 139/1000 | Loss: 0.00013037
Iteration 140/1000 | Loss: 0.00013212
Iteration 141/1000 | Loss: 0.00014455
Iteration 142/1000 | Loss: 0.00007075
Iteration 143/1000 | Loss: 0.00007765
Iteration 144/1000 | Loss: 0.00006909
Iteration 145/1000 | Loss: 0.00006972
Iteration 146/1000 | Loss: 0.00008629
Iteration 147/1000 | Loss: 0.00006459
Iteration 148/1000 | Loss: 0.00006805
Iteration 149/1000 | Loss: 0.00007908
Iteration 150/1000 | Loss: 0.00006235
Iteration 151/1000 | Loss: 0.00006489
Iteration 152/1000 | Loss: 0.00006356
Iteration 153/1000 | Loss: 0.00007908
Iteration 154/1000 | Loss: 0.00007524
Iteration 155/1000 | Loss: 0.00007568
Iteration 156/1000 | Loss: 0.00007501
Iteration 157/1000 | Loss: 0.00007562
Iteration 158/1000 | Loss: 0.00006618
Iteration 159/1000 | Loss: 0.00008885
Iteration 160/1000 | Loss: 0.00008008
Iteration 161/1000 | Loss: 0.00006891
Iteration 162/1000 | Loss: 0.00008452
Iteration 163/1000 | Loss: 0.00007316
Iteration 164/1000 | Loss: 0.00008433
Iteration 165/1000 | Loss: 0.00007137
Iteration 166/1000 | Loss: 0.00007559
Iteration 167/1000 | Loss: 0.00006799
Iteration 168/1000 | Loss: 0.00008029
Iteration 169/1000 | Loss: 0.00006587
Iteration 170/1000 | Loss: 0.00005628
Iteration 171/1000 | Loss: 0.00006175
Iteration 172/1000 | Loss: 0.00005997
Iteration 173/1000 | Loss: 0.00006046
Iteration 174/1000 | Loss: 0.00005232
Iteration 175/1000 | Loss: 0.00005456
Iteration 176/1000 | Loss: 0.00005936
Iteration 177/1000 | Loss: 0.00006233
Iteration 178/1000 | Loss: 0.00006884
Iteration 179/1000 | Loss: 0.00007657
Iteration 180/1000 | Loss: 0.00005849
Iteration 181/1000 | Loss: 0.00023639
Iteration 182/1000 | Loss: 0.00015919
Iteration 183/1000 | Loss: 0.00005634
Iteration 184/1000 | Loss: 0.00004466
Iteration 185/1000 | Loss: 0.00003576
Iteration 186/1000 | Loss: 0.00004104
Iteration 187/1000 | Loss: 0.00004235
Iteration 188/1000 | Loss: 0.00004852
Iteration 189/1000 | Loss: 0.00005313
Iteration 190/1000 | Loss: 0.00004866
Iteration 191/1000 | Loss: 0.00005536
Iteration 192/1000 | Loss: 0.00004726
Iteration 193/1000 | Loss: 0.00004105
Iteration 194/1000 | Loss: 0.00004666
Iteration 195/1000 | Loss: 0.00003885
Iteration 196/1000 | Loss: 0.00003725
Iteration 197/1000 | Loss: 0.00004186
Iteration 198/1000 | Loss: 0.00004375
Iteration 199/1000 | Loss: 0.00004262
Iteration 200/1000 | Loss: 0.00004098
Iteration 201/1000 | Loss: 0.00004057
Iteration 202/1000 | Loss: 0.00004141
Iteration 203/1000 | Loss: 0.00004480
Iteration 204/1000 | Loss: 0.00005164
Iteration 205/1000 | Loss: 0.00004479
Iteration 206/1000 | Loss: 0.00004635
Iteration 207/1000 | Loss: 0.00005896
Iteration 208/1000 | Loss: 0.00005403
Iteration 209/1000 | Loss: 0.00004250
Iteration 210/1000 | Loss: 0.00003944
Iteration 211/1000 | Loss: 0.00003759
Iteration 212/1000 | Loss: 0.00003578
Iteration 213/1000 | Loss: 0.00003473
Iteration 214/1000 | Loss: 0.00003394
Iteration 215/1000 | Loss: 0.00003368
Iteration 216/1000 | Loss: 0.00003340
Iteration 217/1000 | Loss: 0.00022506
Iteration 218/1000 | Loss: 0.00014853
Iteration 219/1000 | Loss: 0.00004928
Iteration 220/1000 | Loss: 0.00044508
Iteration 221/1000 | Loss: 0.00019681
Iteration 222/1000 | Loss: 0.00007273
Iteration 223/1000 | Loss: 0.00031526
Iteration 224/1000 | Loss: 0.00022937
Iteration 225/1000 | Loss: 0.00006569
Iteration 226/1000 | Loss: 0.00003479
Iteration 227/1000 | Loss: 0.00003347
Iteration 228/1000 | Loss: 0.00004101
Iteration 229/1000 | Loss: 0.00081711
Iteration 230/1000 | Loss: 0.00054482
Iteration 231/1000 | Loss: 0.00009484
Iteration 232/1000 | Loss: 0.00019169
Iteration 233/1000 | Loss: 0.00011272
Iteration 234/1000 | Loss: 0.00009082
Iteration 235/1000 | Loss: 0.00013084
Iteration 236/1000 | Loss: 0.00010986
Iteration 237/1000 | Loss: 0.00006043
Iteration 238/1000 | Loss: 0.00014119
Iteration 239/1000 | Loss: 0.00004189
Iteration 240/1000 | Loss: 0.00005626
Iteration 241/1000 | Loss: 0.00020950
Iteration 242/1000 | Loss: 0.00005568
Iteration 243/1000 | Loss: 0.00004066
Iteration 244/1000 | Loss: 0.00003917
Iteration 245/1000 | Loss: 0.00003802
Iteration 246/1000 | Loss: 0.00024073
Iteration 247/1000 | Loss: 0.00022862
Iteration 248/1000 | Loss: 0.00017472
Iteration 249/1000 | Loss: 0.00005800
Iteration 250/1000 | Loss: 0.00005535
Iteration 251/1000 | Loss: 0.00004047
Iteration 252/1000 | Loss: 0.00003524
Iteration 253/1000 | Loss: 0.00003405
Iteration 254/1000 | Loss: 0.00004844
Iteration 255/1000 | Loss: 0.00003965
Iteration 256/1000 | Loss: 0.00003641
Iteration 257/1000 | Loss: 0.00003562
Iteration 258/1000 | Loss: 0.00004819
Iteration 259/1000 | Loss: 0.00004488
Iteration 260/1000 | Loss: 0.00003801
Iteration 261/1000 | Loss: 0.00005202
Iteration 262/1000 | Loss: 0.00004937
Iteration 263/1000 | Loss: 0.00005315
Iteration 264/1000 | Loss: 0.00004015
Iteration 265/1000 | Loss: 0.00004068
Iteration 266/1000 | Loss: 0.00004218
Iteration 267/1000 | Loss: 0.00004112
Iteration 268/1000 | Loss: 0.00004801
Iteration 269/1000 | Loss: 0.00003917
Iteration 270/1000 | Loss: 0.00003960
Iteration 271/1000 | Loss: 0.00003811
Iteration 272/1000 | Loss: 0.00004650
Iteration 273/1000 | Loss: 0.00004100
Iteration 274/1000 | Loss: 0.00005393
Iteration 275/1000 | Loss: 0.00003974
Iteration 276/1000 | Loss: 0.00004458
Iteration 277/1000 | Loss: 0.00003982
Iteration 278/1000 | Loss: 0.00004460
Iteration 279/1000 | Loss: 0.00003970
Iteration 280/1000 | Loss: 0.00005184
Iteration 281/1000 | Loss: 0.00003973
Iteration 282/1000 | Loss: 0.00004354
Iteration 283/1000 | Loss: 0.00004002
Iteration 284/1000 | Loss: 0.00005206
Iteration 285/1000 | Loss: 0.00004733
Iteration 286/1000 | Loss: 0.00003975
Iteration 287/1000 | Loss: 0.00004516
Iteration 288/1000 | Loss: 0.00004618
Iteration 289/1000 | Loss: 0.00004723
Iteration 290/1000 | Loss: 0.00004239
Iteration 291/1000 | Loss: 0.00005211
Iteration 292/1000 | Loss: 0.00004935
Iteration 293/1000 | Loss: 0.00005787
Iteration 294/1000 | Loss: 0.00004556
Iteration 295/1000 | Loss: 0.00004300
Iteration 296/1000 | Loss: 0.00004075
Iteration 297/1000 | Loss: 0.00004269
Iteration 298/1000 | Loss: 0.00004005
Iteration 299/1000 | Loss: 0.00005066
Iteration 300/1000 | Loss: 0.00006625
Iteration 301/1000 | Loss: 0.00004168
Iteration 302/1000 | Loss: 0.00004803
Iteration 303/1000 | Loss: 0.00004841
Iteration 304/1000 | Loss: 0.00005093
Iteration 305/1000 | Loss: 0.00003702
Iteration 306/1000 | Loss: 0.00003411
Iteration 307/1000 | Loss: 0.00003245
Iteration 308/1000 | Loss: 0.00003187
Iteration 309/1000 | Loss: 0.00003134
Iteration 310/1000 | Loss: 0.00003095
Iteration 311/1000 | Loss: 0.00003057
Iteration 312/1000 | Loss: 0.00003034
Iteration 313/1000 | Loss: 0.00003028
Iteration 314/1000 | Loss: 0.00003026
Iteration 315/1000 | Loss: 0.00003021
Iteration 316/1000 | Loss: 0.00003020
Iteration 317/1000 | Loss: 0.00003019
Iteration 318/1000 | Loss: 0.00003019
Iteration 319/1000 | Loss: 0.00003018
Iteration 320/1000 | Loss: 0.00003018
Iteration 321/1000 | Loss: 0.00003018
Iteration 322/1000 | Loss: 0.00003018
Iteration 323/1000 | Loss: 0.00003017
Iteration 324/1000 | Loss: 0.00003017
Iteration 325/1000 | Loss: 0.00003015
Iteration 326/1000 | Loss: 0.00003015
Iteration 327/1000 | Loss: 0.00003015
Iteration 328/1000 | Loss: 0.00003014
Iteration 329/1000 | Loss: 0.00003014
Iteration 330/1000 | Loss: 0.00003014
Iteration 331/1000 | Loss: 0.00003014
Iteration 332/1000 | Loss: 0.00003014
Iteration 333/1000 | Loss: 0.00003014
Iteration 334/1000 | Loss: 0.00003014
Iteration 335/1000 | Loss: 0.00003013
Iteration 336/1000 | Loss: 0.00003013
Iteration 337/1000 | Loss: 0.00003013
Iteration 338/1000 | Loss: 0.00003012
Iteration 339/1000 | Loss: 0.00003012
Iteration 340/1000 | Loss: 0.00003012
Iteration 341/1000 | Loss: 0.00003012
Iteration 342/1000 | Loss: 0.00003011
Iteration 343/1000 | Loss: 0.00003011
Iteration 344/1000 | Loss: 0.00003011
Iteration 345/1000 | Loss: 0.00003011
Iteration 346/1000 | Loss: 0.00003011
Iteration 347/1000 | Loss: 0.00003011
Iteration 348/1000 | Loss: 0.00003011
Iteration 349/1000 | Loss: 0.00003011
Iteration 350/1000 | Loss: 0.00003011
Iteration 351/1000 | Loss: 0.00003011
Iteration 352/1000 | Loss: 0.00003011
Iteration 353/1000 | Loss: 0.00003010
Iteration 354/1000 | Loss: 0.00003010
Iteration 355/1000 | Loss: 0.00003010
Iteration 356/1000 | Loss: 0.00003010
Iteration 357/1000 | Loss: 0.00003009
Iteration 358/1000 | Loss: 0.00003009
Iteration 359/1000 | Loss: 0.00003009
Iteration 360/1000 | Loss: 0.00003009
Iteration 361/1000 | Loss: 0.00003009
Iteration 362/1000 | Loss: 0.00003008
Iteration 363/1000 | Loss: 0.00003008
Iteration 364/1000 | Loss: 0.00003008
Iteration 365/1000 | Loss: 0.00003008
Iteration 366/1000 | Loss: 0.00003008
Iteration 367/1000 | Loss: 0.00003008
Iteration 368/1000 | Loss: 0.00003008
Iteration 369/1000 | Loss: 0.00003007
Iteration 370/1000 | Loss: 0.00003007
Iteration 371/1000 | Loss: 0.00003007
Iteration 372/1000 | Loss: 0.00003007
Iteration 373/1000 | Loss: 0.00003007
Iteration 374/1000 | Loss: 0.00003007
Iteration 375/1000 | Loss: 0.00003007
Iteration 376/1000 | Loss: 0.00003007
Iteration 377/1000 | Loss: 0.00003007
Iteration 378/1000 | Loss: 0.00003007
Iteration 379/1000 | Loss: 0.00003007
Iteration 380/1000 | Loss: 0.00003006
Iteration 381/1000 | Loss: 0.00003006
Iteration 382/1000 | Loss: 0.00003006
Iteration 383/1000 | Loss: 0.00003006
Iteration 384/1000 | Loss: 0.00003006
Iteration 385/1000 | Loss: 0.00003006
Iteration 386/1000 | Loss: 0.00003006
Iteration 387/1000 | Loss: 0.00003006
Iteration 388/1000 | Loss: 0.00003006
Iteration 389/1000 | Loss: 0.00003006
Iteration 390/1000 | Loss: 0.00003006
Iteration 391/1000 | Loss: 0.00003006
Iteration 392/1000 | Loss: 0.00003006
Iteration 393/1000 | Loss: 0.00003006
Iteration 394/1000 | Loss: 0.00003006
Iteration 395/1000 | Loss: 0.00003006
Iteration 396/1000 | Loss: 0.00003005
Iteration 397/1000 | Loss: 0.00003005
Iteration 398/1000 | Loss: 0.00003005
Iteration 399/1000 | Loss: 0.00003005
Iteration 400/1000 | Loss: 0.00003005
Iteration 401/1000 | Loss: 0.00003005
Iteration 402/1000 | Loss: 0.00003005
Iteration 403/1000 | Loss: 0.00003005
Iteration 404/1000 | Loss: 0.00003005
Iteration 405/1000 | Loss: 0.00003005
Iteration 406/1000 | Loss: 0.00003005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 406. Stopping optimization.
Last 5 losses: [3.005405051226262e-05, 3.005405051226262e-05, 3.005405051226262e-05, 3.005405051226262e-05, 3.005405051226262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.005405051226262e-05

Optimization complete. Final v2v error: 4.129175662994385 mm

Highest mean error: 22.309362411499023 mm for frame 53

Lowest mean error: 3.8287651538848877 mm for frame 52

Saving results

Total time: 489.3580904006958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00607824
Iteration 2/25 | Loss: 0.00143578
Iteration 3/25 | Loss: 0.00134525
Iteration 4/25 | Loss: 0.00133397
Iteration 5/25 | Loss: 0.00132969
Iteration 6/25 | Loss: 0.00132846
Iteration 7/25 | Loss: 0.00132846
Iteration 8/25 | Loss: 0.00132846
Iteration 9/25 | Loss: 0.00132846
Iteration 10/25 | Loss: 0.00132846
Iteration 11/25 | Loss: 0.00132846
Iteration 12/25 | Loss: 0.00132846
Iteration 13/25 | Loss: 0.00132846
Iteration 14/25 | Loss: 0.00132846
Iteration 15/25 | Loss: 0.00132846
Iteration 16/25 | Loss: 0.00132846
Iteration 17/25 | Loss: 0.00132846
Iteration 18/25 | Loss: 0.00132846
Iteration 19/25 | Loss: 0.00132846
Iteration 20/25 | Loss: 0.00132846
Iteration 21/25 | Loss: 0.00132846
Iteration 22/25 | Loss: 0.00132846
Iteration 23/25 | Loss: 0.00132846
Iteration 24/25 | Loss: 0.00132846
Iteration 25/25 | Loss: 0.00132846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001328462385572493, 0.001328462385572493, 0.001328462385572493, 0.001328462385572493, 0.001328462385572493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001328462385572493

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.69953012
Iteration 2/25 | Loss: 0.00180422
Iteration 3/25 | Loss: 0.00180422
Iteration 4/25 | Loss: 0.00180422
Iteration 5/25 | Loss: 0.00180422
Iteration 6/25 | Loss: 0.00180422
Iteration 7/25 | Loss: 0.00180422
Iteration 8/25 | Loss: 0.00180422
Iteration 9/25 | Loss: 0.00180422
Iteration 10/25 | Loss: 0.00180422
Iteration 11/25 | Loss: 0.00180421
Iteration 12/25 | Loss: 0.00180421
Iteration 13/25 | Loss: 0.00180421
Iteration 14/25 | Loss: 0.00180421
Iteration 15/25 | Loss: 0.00180421
Iteration 16/25 | Loss: 0.00180421
Iteration 17/25 | Loss: 0.00180421
Iteration 18/25 | Loss: 0.00180421
Iteration 19/25 | Loss: 0.00180421
Iteration 20/25 | Loss: 0.00180421
Iteration 21/25 | Loss: 0.00180421
Iteration 22/25 | Loss: 0.00180421
Iteration 23/25 | Loss: 0.00180421
Iteration 24/25 | Loss: 0.00180421
Iteration 25/25 | Loss: 0.00180421

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180421
Iteration 2/1000 | Loss: 0.00004417
Iteration 3/1000 | Loss: 0.00002921
Iteration 4/1000 | Loss: 0.00002540
Iteration 5/1000 | Loss: 0.00002375
Iteration 6/1000 | Loss: 0.00002288
Iteration 7/1000 | Loss: 0.00002240
Iteration 8/1000 | Loss: 0.00002200
Iteration 9/1000 | Loss: 0.00002172
Iteration 10/1000 | Loss: 0.00002154
Iteration 11/1000 | Loss: 0.00002153
Iteration 12/1000 | Loss: 0.00002146
Iteration 13/1000 | Loss: 0.00002143
Iteration 14/1000 | Loss: 0.00002139
Iteration 15/1000 | Loss: 0.00002139
Iteration 16/1000 | Loss: 0.00002138
Iteration 17/1000 | Loss: 0.00002136
Iteration 18/1000 | Loss: 0.00002136
Iteration 19/1000 | Loss: 0.00002136
Iteration 20/1000 | Loss: 0.00002136
Iteration 21/1000 | Loss: 0.00002135
Iteration 22/1000 | Loss: 0.00002135
Iteration 23/1000 | Loss: 0.00002134
Iteration 24/1000 | Loss: 0.00002134
Iteration 25/1000 | Loss: 0.00002132
Iteration 26/1000 | Loss: 0.00002131
Iteration 27/1000 | Loss: 0.00002130
Iteration 28/1000 | Loss: 0.00002130
Iteration 29/1000 | Loss: 0.00002130
Iteration 30/1000 | Loss: 0.00002130
Iteration 31/1000 | Loss: 0.00002129
Iteration 32/1000 | Loss: 0.00002129
Iteration 33/1000 | Loss: 0.00002129
Iteration 34/1000 | Loss: 0.00002128
Iteration 35/1000 | Loss: 0.00002128
Iteration 36/1000 | Loss: 0.00002128
Iteration 37/1000 | Loss: 0.00002128
Iteration 38/1000 | Loss: 0.00002128
Iteration 39/1000 | Loss: 0.00002128
Iteration 40/1000 | Loss: 0.00002128
Iteration 41/1000 | Loss: 0.00002127
Iteration 42/1000 | Loss: 0.00002127
Iteration 43/1000 | Loss: 0.00002127
Iteration 44/1000 | Loss: 0.00002127
Iteration 45/1000 | Loss: 0.00002127
Iteration 46/1000 | Loss: 0.00002127
Iteration 47/1000 | Loss: 0.00002127
Iteration 48/1000 | Loss: 0.00002127
Iteration 49/1000 | Loss: 0.00002127
Iteration 50/1000 | Loss: 0.00002126
Iteration 51/1000 | Loss: 0.00002126
Iteration 52/1000 | Loss: 0.00002126
Iteration 53/1000 | Loss: 0.00002126
Iteration 54/1000 | Loss: 0.00002126
Iteration 55/1000 | Loss: 0.00002125
Iteration 56/1000 | Loss: 0.00002125
Iteration 57/1000 | Loss: 0.00002125
Iteration 58/1000 | Loss: 0.00002125
Iteration 59/1000 | Loss: 0.00002124
Iteration 60/1000 | Loss: 0.00002124
Iteration 61/1000 | Loss: 0.00002124
Iteration 62/1000 | Loss: 0.00002124
Iteration 63/1000 | Loss: 0.00002124
Iteration 64/1000 | Loss: 0.00002124
Iteration 65/1000 | Loss: 0.00002124
Iteration 66/1000 | Loss: 0.00002123
Iteration 67/1000 | Loss: 0.00002123
Iteration 68/1000 | Loss: 0.00002123
Iteration 69/1000 | Loss: 0.00002123
Iteration 70/1000 | Loss: 0.00002123
Iteration 71/1000 | Loss: 0.00002123
Iteration 72/1000 | Loss: 0.00002123
Iteration 73/1000 | Loss: 0.00002123
Iteration 74/1000 | Loss: 0.00002123
Iteration 75/1000 | Loss: 0.00002123
Iteration 76/1000 | Loss: 0.00002123
Iteration 77/1000 | Loss: 0.00002123
Iteration 78/1000 | Loss: 0.00002123
Iteration 79/1000 | Loss: 0.00002123
Iteration 80/1000 | Loss: 0.00002122
Iteration 81/1000 | Loss: 0.00002122
Iteration 82/1000 | Loss: 0.00002122
Iteration 83/1000 | Loss: 0.00002122
Iteration 84/1000 | Loss: 0.00002122
Iteration 85/1000 | Loss: 0.00002122
Iteration 86/1000 | Loss: 0.00002122
Iteration 87/1000 | Loss: 0.00002122
Iteration 88/1000 | Loss: 0.00002122
Iteration 89/1000 | Loss: 0.00002121
Iteration 90/1000 | Loss: 0.00002121
Iteration 91/1000 | Loss: 0.00002121
Iteration 92/1000 | Loss: 0.00002121
Iteration 93/1000 | Loss: 0.00002121
Iteration 94/1000 | Loss: 0.00002120
Iteration 95/1000 | Loss: 0.00002120
Iteration 96/1000 | Loss: 0.00002120
Iteration 97/1000 | Loss: 0.00002120
Iteration 98/1000 | Loss: 0.00002119
Iteration 99/1000 | Loss: 0.00002119
Iteration 100/1000 | Loss: 0.00002119
Iteration 101/1000 | Loss: 0.00002118
Iteration 102/1000 | Loss: 0.00002118
Iteration 103/1000 | Loss: 0.00002118
Iteration 104/1000 | Loss: 0.00002118
Iteration 105/1000 | Loss: 0.00002118
Iteration 106/1000 | Loss: 0.00002118
Iteration 107/1000 | Loss: 0.00002117
Iteration 108/1000 | Loss: 0.00002117
Iteration 109/1000 | Loss: 0.00002117
Iteration 110/1000 | Loss: 0.00002117
Iteration 111/1000 | Loss: 0.00002117
Iteration 112/1000 | Loss: 0.00002117
Iteration 113/1000 | Loss: 0.00002117
Iteration 114/1000 | Loss: 0.00002117
Iteration 115/1000 | Loss: 0.00002117
Iteration 116/1000 | Loss: 0.00002117
Iteration 117/1000 | Loss: 0.00002117
Iteration 118/1000 | Loss: 0.00002116
Iteration 119/1000 | Loss: 0.00002116
Iteration 120/1000 | Loss: 0.00002116
Iteration 121/1000 | Loss: 0.00002116
Iteration 122/1000 | Loss: 0.00002116
Iteration 123/1000 | Loss: 0.00002116
Iteration 124/1000 | Loss: 0.00002115
Iteration 125/1000 | Loss: 0.00002115
Iteration 126/1000 | Loss: 0.00002115
Iteration 127/1000 | Loss: 0.00002114
Iteration 128/1000 | Loss: 0.00002114
Iteration 129/1000 | Loss: 0.00002113
Iteration 130/1000 | Loss: 0.00002113
Iteration 131/1000 | Loss: 0.00002113
Iteration 132/1000 | Loss: 0.00002113
Iteration 133/1000 | Loss: 0.00002113
Iteration 134/1000 | Loss: 0.00002113
Iteration 135/1000 | Loss: 0.00002113
Iteration 136/1000 | Loss: 0.00002113
Iteration 137/1000 | Loss: 0.00002113
Iteration 138/1000 | Loss: 0.00002113
Iteration 139/1000 | Loss: 0.00002112
Iteration 140/1000 | Loss: 0.00002112
Iteration 141/1000 | Loss: 0.00002112
Iteration 142/1000 | Loss: 0.00002112
Iteration 143/1000 | Loss: 0.00002112
Iteration 144/1000 | Loss: 0.00002112
Iteration 145/1000 | Loss: 0.00002112
Iteration 146/1000 | Loss: 0.00002112
Iteration 147/1000 | Loss: 0.00002112
Iteration 148/1000 | Loss: 0.00002112
Iteration 149/1000 | Loss: 0.00002112
Iteration 150/1000 | Loss: 0.00002112
Iteration 151/1000 | Loss: 0.00002112
Iteration 152/1000 | Loss: 0.00002111
Iteration 153/1000 | Loss: 0.00002111
Iteration 154/1000 | Loss: 0.00002111
Iteration 155/1000 | Loss: 0.00002111
Iteration 156/1000 | Loss: 0.00002111
Iteration 157/1000 | Loss: 0.00002111
Iteration 158/1000 | Loss: 0.00002111
Iteration 159/1000 | Loss: 0.00002111
Iteration 160/1000 | Loss: 0.00002111
Iteration 161/1000 | Loss: 0.00002111
Iteration 162/1000 | Loss: 0.00002111
Iteration 163/1000 | Loss: 0.00002111
Iteration 164/1000 | Loss: 0.00002111
Iteration 165/1000 | Loss: 0.00002111
Iteration 166/1000 | Loss: 0.00002111
Iteration 167/1000 | Loss: 0.00002111
Iteration 168/1000 | Loss: 0.00002111
Iteration 169/1000 | Loss: 0.00002110
Iteration 170/1000 | Loss: 0.00002110
Iteration 171/1000 | Loss: 0.00002110
Iteration 172/1000 | Loss: 0.00002110
Iteration 173/1000 | Loss: 0.00002110
Iteration 174/1000 | Loss: 0.00002110
Iteration 175/1000 | Loss: 0.00002110
Iteration 176/1000 | Loss: 0.00002110
Iteration 177/1000 | Loss: 0.00002110
Iteration 178/1000 | Loss: 0.00002110
Iteration 179/1000 | Loss: 0.00002110
Iteration 180/1000 | Loss: 0.00002110
Iteration 181/1000 | Loss: 0.00002110
Iteration 182/1000 | Loss: 0.00002110
Iteration 183/1000 | Loss: 0.00002110
Iteration 184/1000 | Loss: 0.00002110
Iteration 185/1000 | Loss: 0.00002110
Iteration 186/1000 | Loss: 0.00002110
Iteration 187/1000 | Loss: 0.00002110
Iteration 188/1000 | Loss: 0.00002110
Iteration 189/1000 | Loss: 0.00002110
Iteration 190/1000 | Loss: 0.00002110
Iteration 191/1000 | Loss: 0.00002110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.110214700223878e-05, 2.110214700223878e-05, 2.110214700223878e-05, 2.110214700223878e-05, 2.110214700223878e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.110214700223878e-05

Optimization complete. Final v2v error: 3.847309112548828 mm

Highest mean error: 4.134381294250488 mm for frame 79

Lowest mean error: 3.64694881439209 mm for frame 41

Saving results

Total time: 37.185529470443726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447570
Iteration 2/25 | Loss: 0.00150451
Iteration 3/25 | Loss: 0.00139886
Iteration 4/25 | Loss: 0.00138856
Iteration 5/25 | Loss: 0.00138571
Iteration 6/25 | Loss: 0.00138558
Iteration 7/25 | Loss: 0.00138558
Iteration 8/25 | Loss: 0.00138558
Iteration 9/25 | Loss: 0.00138558
Iteration 10/25 | Loss: 0.00138558
Iteration 11/25 | Loss: 0.00138558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013855794677510858, 0.0013855794677510858, 0.0013855794677510858, 0.0013855794677510858, 0.0013855794677510858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013855794677510858

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99331433
Iteration 2/25 | Loss: 0.00171543
Iteration 3/25 | Loss: 0.00171543
Iteration 4/25 | Loss: 0.00171543
Iteration 5/25 | Loss: 0.00171543
Iteration 6/25 | Loss: 0.00171543
Iteration 7/25 | Loss: 0.00171543
Iteration 8/25 | Loss: 0.00171543
Iteration 9/25 | Loss: 0.00171543
Iteration 10/25 | Loss: 0.00171543
Iteration 11/25 | Loss: 0.00171543
Iteration 12/25 | Loss: 0.00171543
Iteration 13/25 | Loss: 0.00171543
Iteration 14/25 | Loss: 0.00171543
Iteration 15/25 | Loss: 0.00171543
Iteration 16/25 | Loss: 0.00171543
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001715431921184063, 0.001715431921184063, 0.001715431921184063, 0.001715431921184063, 0.001715431921184063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001715431921184063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171543
Iteration 2/1000 | Loss: 0.00005656
Iteration 3/1000 | Loss: 0.00003453
Iteration 4/1000 | Loss: 0.00002808
Iteration 5/1000 | Loss: 0.00002429
Iteration 6/1000 | Loss: 0.00002349
Iteration 7/1000 | Loss: 0.00002304
Iteration 8/1000 | Loss: 0.00002277
Iteration 9/1000 | Loss: 0.00002269
Iteration 10/1000 | Loss: 0.00002252
Iteration 11/1000 | Loss: 0.00002250
Iteration 12/1000 | Loss: 0.00002235
Iteration 13/1000 | Loss: 0.00002231
Iteration 14/1000 | Loss: 0.00002231
Iteration 15/1000 | Loss: 0.00002231
Iteration 16/1000 | Loss: 0.00002221
Iteration 17/1000 | Loss: 0.00002218
Iteration 18/1000 | Loss: 0.00002217
Iteration 19/1000 | Loss: 0.00002213
Iteration 20/1000 | Loss: 0.00002213
Iteration 21/1000 | Loss: 0.00002213
Iteration 22/1000 | Loss: 0.00002213
Iteration 23/1000 | Loss: 0.00002213
Iteration 24/1000 | Loss: 0.00002212
Iteration 25/1000 | Loss: 0.00002212
Iteration 26/1000 | Loss: 0.00002212
Iteration 27/1000 | Loss: 0.00002212
Iteration 28/1000 | Loss: 0.00002212
Iteration 29/1000 | Loss: 0.00002212
Iteration 30/1000 | Loss: 0.00002211
Iteration 31/1000 | Loss: 0.00002206
Iteration 32/1000 | Loss: 0.00002206
Iteration 33/1000 | Loss: 0.00002206
Iteration 34/1000 | Loss: 0.00002205
Iteration 35/1000 | Loss: 0.00002205
Iteration 36/1000 | Loss: 0.00002203
Iteration 37/1000 | Loss: 0.00002203
Iteration 38/1000 | Loss: 0.00002202
Iteration 39/1000 | Loss: 0.00002202
Iteration 40/1000 | Loss: 0.00002202
Iteration 41/1000 | Loss: 0.00002202
Iteration 42/1000 | Loss: 0.00002202
Iteration 43/1000 | Loss: 0.00002202
Iteration 44/1000 | Loss: 0.00002202
Iteration 45/1000 | Loss: 0.00002202
Iteration 46/1000 | Loss: 0.00002202
Iteration 47/1000 | Loss: 0.00002202
Iteration 48/1000 | Loss: 0.00002202
Iteration 49/1000 | Loss: 0.00002201
Iteration 50/1000 | Loss: 0.00002201
Iteration 51/1000 | Loss: 0.00002201
Iteration 52/1000 | Loss: 0.00002201
Iteration 53/1000 | Loss: 0.00002200
Iteration 54/1000 | Loss: 0.00002200
Iteration 55/1000 | Loss: 0.00002200
Iteration 56/1000 | Loss: 0.00002200
Iteration 57/1000 | Loss: 0.00002199
Iteration 58/1000 | Loss: 0.00002199
Iteration 59/1000 | Loss: 0.00002199
Iteration 60/1000 | Loss: 0.00002199
Iteration 61/1000 | Loss: 0.00002199
Iteration 62/1000 | Loss: 0.00002198
Iteration 63/1000 | Loss: 0.00002198
Iteration 64/1000 | Loss: 0.00002195
Iteration 65/1000 | Loss: 0.00002195
Iteration 66/1000 | Loss: 0.00002195
Iteration 67/1000 | Loss: 0.00002194
Iteration 68/1000 | Loss: 0.00002194
Iteration 69/1000 | Loss: 0.00002194
Iteration 70/1000 | Loss: 0.00002194
Iteration 71/1000 | Loss: 0.00002194
Iteration 72/1000 | Loss: 0.00002193
Iteration 73/1000 | Loss: 0.00002193
Iteration 74/1000 | Loss: 0.00002193
Iteration 75/1000 | Loss: 0.00002193
Iteration 76/1000 | Loss: 0.00002193
Iteration 77/1000 | Loss: 0.00002193
Iteration 78/1000 | Loss: 0.00002193
Iteration 79/1000 | Loss: 0.00002193
Iteration 80/1000 | Loss: 0.00002193
Iteration 81/1000 | Loss: 0.00002193
Iteration 82/1000 | Loss: 0.00002193
Iteration 83/1000 | Loss: 0.00002192
Iteration 84/1000 | Loss: 0.00002192
Iteration 85/1000 | Loss: 0.00002192
Iteration 86/1000 | Loss: 0.00002192
Iteration 87/1000 | Loss: 0.00002192
Iteration 88/1000 | Loss: 0.00002192
Iteration 89/1000 | Loss: 0.00002192
Iteration 90/1000 | Loss: 0.00002192
Iteration 91/1000 | Loss: 0.00002192
Iteration 92/1000 | Loss: 0.00002191
Iteration 93/1000 | Loss: 0.00002191
Iteration 94/1000 | Loss: 0.00002190
Iteration 95/1000 | Loss: 0.00002190
Iteration 96/1000 | Loss: 0.00002189
Iteration 97/1000 | Loss: 0.00002189
Iteration 98/1000 | Loss: 0.00002188
Iteration 99/1000 | Loss: 0.00002188
Iteration 100/1000 | Loss: 0.00002186
Iteration 101/1000 | Loss: 0.00002186
Iteration 102/1000 | Loss: 0.00002185
Iteration 103/1000 | Loss: 0.00002185
Iteration 104/1000 | Loss: 0.00002182
Iteration 105/1000 | Loss: 0.00002182
Iteration 106/1000 | Loss: 0.00002182
Iteration 107/1000 | Loss: 0.00002182
Iteration 108/1000 | Loss: 0.00002182
Iteration 109/1000 | Loss: 0.00002182
Iteration 110/1000 | Loss: 0.00002182
Iteration 111/1000 | Loss: 0.00002182
Iteration 112/1000 | Loss: 0.00002182
Iteration 113/1000 | Loss: 0.00002182
Iteration 114/1000 | Loss: 0.00002181
Iteration 115/1000 | Loss: 0.00002181
Iteration 116/1000 | Loss: 0.00002181
Iteration 117/1000 | Loss: 0.00002181
Iteration 118/1000 | Loss: 0.00002181
Iteration 119/1000 | Loss: 0.00002181
Iteration 120/1000 | Loss: 0.00002181
Iteration 121/1000 | Loss: 0.00002181
Iteration 122/1000 | Loss: 0.00002181
Iteration 123/1000 | Loss: 0.00002181
Iteration 124/1000 | Loss: 0.00002179
Iteration 125/1000 | Loss: 0.00002179
Iteration 126/1000 | Loss: 0.00002179
Iteration 127/1000 | Loss: 0.00002178
Iteration 128/1000 | Loss: 0.00002178
Iteration 129/1000 | Loss: 0.00002177
Iteration 130/1000 | Loss: 0.00002177
Iteration 131/1000 | Loss: 0.00002177
Iteration 132/1000 | Loss: 0.00002176
Iteration 133/1000 | Loss: 0.00002176
Iteration 134/1000 | Loss: 0.00002175
Iteration 135/1000 | Loss: 0.00002175
Iteration 136/1000 | Loss: 0.00002175
Iteration 137/1000 | Loss: 0.00002174
Iteration 138/1000 | Loss: 0.00002174
Iteration 139/1000 | Loss: 0.00002174
Iteration 140/1000 | Loss: 0.00002173
Iteration 141/1000 | Loss: 0.00002173
Iteration 142/1000 | Loss: 0.00002173
Iteration 143/1000 | Loss: 0.00002173
Iteration 144/1000 | Loss: 0.00002172
Iteration 145/1000 | Loss: 0.00002172
Iteration 146/1000 | Loss: 0.00002172
Iteration 147/1000 | Loss: 0.00002172
Iteration 148/1000 | Loss: 0.00002171
Iteration 149/1000 | Loss: 0.00002171
Iteration 150/1000 | Loss: 0.00002170
Iteration 151/1000 | Loss: 0.00002170
Iteration 152/1000 | Loss: 0.00002170
Iteration 153/1000 | Loss: 0.00002170
Iteration 154/1000 | Loss: 0.00002170
Iteration 155/1000 | Loss: 0.00002169
Iteration 156/1000 | Loss: 0.00002169
Iteration 157/1000 | Loss: 0.00002169
Iteration 158/1000 | Loss: 0.00002169
Iteration 159/1000 | Loss: 0.00002169
Iteration 160/1000 | Loss: 0.00002169
Iteration 161/1000 | Loss: 0.00002169
Iteration 162/1000 | Loss: 0.00002167
Iteration 163/1000 | Loss: 0.00002167
Iteration 164/1000 | Loss: 0.00002167
Iteration 165/1000 | Loss: 0.00002167
Iteration 166/1000 | Loss: 0.00002167
Iteration 167/1000 | Loss: 0.00002167
Iteration 168/1000 | Loss: 0.00002167
Iteration 169/1000 | Loss: 0.00002167
Iteration 170/1000 | Loss: 0.00002167
Iteration 171/1000 | Loss: 0.00002167
Iteration 172/1000 | Loss: 0.00002167
Iteration 173/1000 | Loss: 0.00002167
Iteration 174/1000 | Loss: 0.00002167
Iteration 175/1000 | Loss: 0.00002167
Iteration 176/1000 | Loss: 0.00002167
Iteration 177/1000 | Loss: 0.00002166
Iteration 178/1000 | Loss: 0.00002166
Iteration 179/1000 | Loss: 0.00002166
Iteration 180/1000 | Loss: 0.00002166
Iteration 181/1000 | Loss: 0.00002165
Iteration 182/1000 | Loss: 0.00002165
Iteration 183/1000 | Loss: 0.00002165
Iteration 184/1000 | Loss: 0.00002165
Iteration 185/1000 | Loss: 0.00002164
Iteration 186/1000 | Loss: 0.00002164
Iteration 187/1000 | Loss: 0.00002164
Iteration 188/1000 | Loss: 0.00002164
Iteration 189/1000 | Loss: 0.00002164
Iteration 190/1000 | Loss: 0.00002164
Iteration 191/1000 | Loss: 0.00002164
Iteration 192/1000 | Loss: 0.00002164
Iteration 193/1000 | Loss: 0.00002164
Iteration 194/1000 | Loss: 0.00002164
Iteration 195/1000 | Loss: 0.00002164
Iteration 196/1000 | Loss: 0.00002164
Iteration 197/1000 | Loss: 0.00002164
Iteration 198/1000 | Loss: 0.00002164
Iteration 199/1000 | Loss: 0.00002164
Iteration 200/1000 | Loss: 0.00002164
Iteration 201/1000 | Loss: 0.00002164
Iteration 202/1000 | Loss: 0.00002163
Iteration 203/1000 | Loss: 0.00002163
Iteration 204/1000 | Loss: 0.00002163
Iteration 205/1000 | Loss: 0.00002163
Iteration 206/1000 | Loss: 0.00002163
Iteration 207/1000 | Loss: 0.00002163
Iteration 208/1000 | Loss: 0.00002163
Iteration 209/1000 | Loss: 0.00002163
Iteration 210/1000 | Loss: 0.00002163
Iteration 211/1000 | Loss: 0.00002163
Iteration 212/1000 | Loss: 0.00002163
Iteration 213/1000 | Loss: 0.00002163
Iteration 214/1000 | Loss: 0.00002163
Iteration 215/1000 | Loss: 0.00002163
Iteration 216/1000 | Loss: 0.00002163
Iteration 217/1000 | Loss: 0.00002163
Iteration 218/1000 | Loss: 0.00002163
Iteration 219/1000 | Loss: 0.00002163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [2.163333738280926e-05, 2.163333738280926e-05, 2.163333738280926e-05, 2.163333738280926e-05, 2.163333738280926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.163333738280926e-05

Optimization complete. Final v2v error: 3.903709650039673 mm

Highest mean error: 3.993727207183838 mm for frame 9

Lowest mean error: 3.8234965801239014 mm for frame 112

Saving results

Total time: 37.541181564331055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428130
Iteration 2/25 | Loss: 0.00148371
Iteration 3/25 | Loss: 0.00137940
Iteration 4/25 | Loss: 0.00137197
Iteration 5/25 | Loss: 0.00136995
Iteration 6/25 | Loss: 0.00136973
Iteration 7/25 | Loss: 0.00136973
Iteration 8/25 | Loss: 0.00136973
Iteration 9/25 | Loss: 0.00136973
Iteration 10/25 | Loss: 0.00136973
Iteration 11/25 | Loss: 0.00136973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013697345275431871, 0.0013697345275431871, 0.0013697345275431871, 0.0013697345275431871, 0.0013697345275431871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013697345275431871

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45648181
Iteration 2/25 | Loss: 0.00177780
Iteration 3/25 | Loss: 0.00177780
Iteration 4/25 | Loss: 0.00177780
Iteration 5/25 | Loss: 0.00177780
Iteration 6/25 | Loss: 0.00177780
Iteration 7/25 | Loss: 0.00177780
Iteration 8/25 | Loss: 0.00177780
Iteration 9/25 | Loss: 0.00177780
Iteration 10/25 | Loss: 0.00177780
Iteration 11/25 | Loss: 0.00177780
Iteration 12/25 | Loss: 0.00177780
Iteration 13/25 | Loss: 0.00177780
Iteration 14/25 | Loss: 0.00177780
Iteration 15/25 | Loss: 0.00177780
Iteration 16/25 | Loss: 0.00177780
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0017777967732399702, 0.0017777967732399702, 0.0017777967732399702, 0.0017777967732399702, 0.0017777967732399702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017777967732399702

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177780
Iteration 2/1000 | Loss: 0.00006156
Iteration 3/1000 | Loss: 0.00003367
Iteration 4/1000 | Loss: 0.00002786
Iteration 5/1000 | Loss: 0.00002460
Iteration 6/1000 | Loss: 0.00002306
Iteration 7/1000 | Loss: 0.00002200
Iteration 8/1000 | Loss: 0.00002132
Iteration 9/1000 | Loss: 0.00002091
Iteration 10/1000 | Loss: 0.00002055
Iteration 11/1000 | Loss: 0.00002043
Iteration 12/1000 | Loss: 0.00002021
Iteration 13/1000 | Loss: 0.00002001
Iteration 14/1000 | Loss: 0.00001999
Iteration 15/1000 | Loss: 0.00001992
Iteration 16/1000 | Loss: 0.00001991
Iteration 17/1000 | Loss: 0.00001986
Iteration 18/1000 | Loss: 0.00001986
Iteration 19/1000 | Loss: 0.00001984
Iteration 20/1000 | Loss: 0.00001984
Iteration 21/1000 | Loss: 0.00001984
Iteration 22/1000 | Loss: 0.00001982
Iteration 23/1000 | Loss: 0.00001982
Iteration 24/1000 | Loss: 0.00001981
Iteration 25/1000 | Loss: 0.00001981
Iteration 26/1000 | Loss: 0.00001981
Iteration 27/1000 | Loss: 0.00001981
Iteration 28/1000 | Loss: 0.00001980
Iteration 29/1000 | Loss: 0.00001980
Iteration 30/1000 | Loss: 0.00001980
Iteration 31/1000 | Loss: 0.00001980
Iteration 32/1000 | Loss: 0.00001979
Iteration 33/1000 | Loss: 0.00001979
Iteration 34/1000 | Loss: 0.00001979
Iteration 35/1000 | Loss: 0.00001979
Iteration 36/1000 | Loss: 0.00001979
Iteration 37/1000 | Loss: 0.00001978
Iteration 38/1000 | Loss: 0.00001978
Iteration 39/1000 | Loss: 0.00001978
Iteration 40/1000 | Loss: 0.00001977
Iteration 41/1000 | Loss: 0.00001977
Iteration 42/1000 | Loss: 0.00001977
Iteration 43/1000 | Loss: 0.00001977
Iteration 44/1000 | Loss: 0.00001977
Iteration 45/1000 | Loss: 0.00001977
Iteration 46/1000 | Loss: 0.00001976
Iteration 47/1000 | Loss: 0.00001976
Iteration 48/1000 | Loss: 0.00001976
Iteration 49/1000 | Loss: 0.00001976
Iteration 50/1000 | Loss: 0.00001976
Iteration 51/1000 | Loss: 0.00001975
Iteration 52/1000 | Loss: 0.00001975
Iteration 53/1000 | Loss: 0.00001975
Iteration 54/1000 | Loss: 0.00001975
Iteration 55/1000 | Loss: 0.00001975
Iteration 56/1000 | Loss: 0.00001975
Iteration 57/1000 | Loss: 0.00001974
Iteration 58/1000 | Loss: 0.00001974
Iteration 59/1000 | Loss: 0.00001974
Iteration 60/1000 | Loss: 0.00001974
Iteration 61/1000 | Loss: 0.00001974
Iteration 62/1000 | Loss: 0.00001973
Iteration 63/1000 | Loss: 0.00001973
Iteration 64/1000 | Loss: 0.00001973
Iteration 65/1000 | Loss: 0.00001973
Iteration 66/1000 | Loss: 0.00001972
Iteration 67/1000 | Loss: 0.00001972
Iteration 68/1000 | Loss: 0.00001972
Iteration 69/1000 | Loss: 0.00001972
Iteration 70/1000 | Loss: 0.00001972
Iteration 71/1000 | Loss: 0.00001972
Iteration 72/1000 | Loss: 0.00001972
Iteration 73/1000 | Loss: 0.00001972
Iteration 74/1000 | Loss: 0.00001972
Iteration 75/1000 | Loss: 0.00001972
Iteration 76/1000 | Loss: 0.00001971
Iteration 77/1000 | Loss: 0.00001971
Iteration 78/1000 | Loss: 0.00001971
Iteration 79/1000 | Loss: 0.00001971
Iteration 80/1000 | Loss: 0.00001971
Iteration 81/1000 | Loss: 0.00001971
Iteration 82/1000 | Loss: 0.00001971
Iteration 83/1000 | Loss: 0.00001971
Iteration 84/1000 | Loss: 0.00001970
Iteration 85/1000 | Loss: 0.00001970
Iteration 86/1000 | Loss: 0.00001970
Iteration 87/1000 | Loss: 0.00001970
Iteration 88/1000 | Loss: 0.00001970
Iteration 89/1000 | Loss: 0.00001970
Iteration 90/1000 | Loss: 0.00001970
Iteration 91/1000 | Loss: 0.00001970
Iteration 92/1000 | Loss: 0.00001970
Iteration 93/1000 | Loss: 0.00001970
Iteration 94/1000 | Loss: 0.00001970
Iteration 95/1000 | Loss: 0.00001970
Iteration 96/1000 | Loss: 0.00001970
Iteration 97/1000 | Loss: 0.00001970
Iteration 98/1000 | Loss: 0.00001970
Iteration 99/1000 | Loss: 0.00001970
Iteration 100/1000 | Loss: 0.00001970
Iteration 101/1000 | Loss: 0.00001970
Iteration 102/1000 | Loss: 0.00001970
Iteration 103/1000 | Loss: 0.00001970
Iteration 104/1000 | Loss: 0.00001970
Iteration 105/1000 | Loss: 0.00001970
Iteration 106/1000 | Loss: 0.00001970
Iteration 107/1000 | Loss: 0.00001970
Iteration 108/1000 | Loss: 0.00001970
Iteration 109/1000 | Loss: 0.00001970
Iteration 110/1000 | Loss: 0.00001970
Iteration 111/1000 | Loss: 0.00001970
Iteration 112/1000 | Loss: 0.00001970
Iteration 113/1000 | Loss: 0.00001970
Iteration 114/1000 | Loss: 0.00001970
Iteration 115/1000 | Loss: 0.00001970
Iteration 116/1000 | Loss: 0.00001970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.96950768440729e-05, 1.96950768440729e-05, 1.96950768440729e-05, 1.96950768440729e-05, 1.96950768440729e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.96950768440729e-05

Optimization complete. Final v2v error: 3.7801878452301025 mm

Highest mean error: 5.114327907562256 mm for frame 191

Lowest mean error: 3.336404800415039 mm for frame 96

Saving results

Total time: 38.79787802696228
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865728
Iteration 2/25 | Loss: 0.00155595
Iteration 3/25 | Loss: 0.00140068
Iteration 4/25 | Loss: 0.00137375
Iteration 5/25 | Loss: 0.00136979
Iteration 6/25 | Loss: 0.00136629
Iteration 7/25 | Loss: 0.00136469
Iteration 8/25 | Loss: 0.00136372
Iteration 9/25 | Loss: 0.00136325
Iteration 10/25 | Loss: 0.00136309
Iteration 11/25 | Loss: 0.00136302
Iteration 12/25 | Loss: 0.00136301
Iteration 13/25 | Loss: 0.00136301
Iteration 14/25 | Loss: 0.00136301
Iteration 15/25 | Loss: 0.00136301
Iteration 16/25 | Loss: 0.00136301
Iteration 17/25 | Loss: 0.00136301
Iteration 18/25 | Loss: 0.00136301
Iteration 19/25 | Loss: 0.00136301
Iteration 20/25 | Loss: 0.00136301
Iteration 21/25 | Loss: 0.00136301
Iteration 22/25 | Loss: 0.00136301
Iteration 23/25 | Loss: 0.00136301
Iteration 24/25 | Loss: 0.00136300
Iteration 25/25 | Loss: 0.00136300

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.43136239
Iteration 2/25 | Loss: 0.00182154
Iteration 3/25 | Loss: 0.00182153
Iteration 4/25 | Loss: 0.00182153
Iteration 5/25 | Loss: 0.00182153
Iteration 6/25 | Loss: 0.00182153
Iteration 7/25 | Loss: 0.00182153
Iteration 8/25 | Loss: 0.00182153
Iteration 9/25 | Loss: 0.00182153
Iteration 10/25 | Loss: 0.00182153
Iteration 11/25 | Loss: 0.00182153
Iteration 12/25 | Loss: 0.00182153
Iteration 13/25 | Loss: 0.00182153
Iteration 14/25 | Loss: 0.00182153
Iteration 15/25 | Loss: 0.00182153
Iteration 16/25 | Loss: 0.00182153
Iteration 17/25 | Loss: 0.00182153
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0018215308664366603, 0.0018215308664366603, 0.0018215308664366603, 0.0018215308664366603, 0.0018215308664366603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018215308664366603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182153
Iteration 2/1000 | Loss: 0.00004841
Iteration 3/1000 | Loss: 0.00003187
Iteration 4/1000 | Loss: 0.00002629
Iteration 5/1000 | Loss: 0.00002399
Iteration 6/1000 | Loss: 0.00002315
Iteration 7/1000 | Loss: 0.00002263
Iteration 8/1000 | Loss: 0.00002614
Iteration 9/1000 | Loss: 0.00002184
Iteration 10/1000 | Loss: 0.00002166
Iteration 11/1000 | Loss: 0.00002148
Iteration 12/1000 | Loss: 0.00002141
Iteration 13/1000 | Loss: 0.00002126
Iteration 14/1000 | Loss: 0.00002126
Iteration 15/1000 | Loss: 0.00002125
Iteration 16/1000 | Loss: 0.00002124
Iteration 17/1000 | Loss: 0.00002120
Iteration 18/1000 | Loss: 0.00002116
Iteration 19/1000 | Loss: 0.00002116
Iteration 20/1000 | Loss: 0.00002146
Iteration 21/1000 | Loss: 0.00002126
Iteration 22/1000 | Loss: 0.00002126
Iteration 23/1000 | Loss: 0.00002106
Iteration 24/1000 | Loss: 0.00002105
Iteration 25/1000 | Loss: 0.00002105
Iteration 26/1000 | Loss: 0.00002104
Iteration 27/1000 | Loss: 0.00002104
Iteration 28/1000 | Loss: 0.00002104
Iteration 29/1000 | Loss: 0.00002104
Iteration 30/1000 | Loss: 0.00002104
Iteration 31/1000 | Loss: 0.00002103
Iteration 32/1000 | Loss: 0.00002103
Iteration 33/1000 | Loss: 0.00002103
Iteration 34/1000 | Loss: 0.00002103
Iteration 35/1000 | Loss: 0.00002103
Iteration 36/1000 | Loss: 0.00002103
Iteration 37/1000 | Loss: 0.00002103
Iteration 38/1000 | Loss: 0.00002103
Iteration 39/1000 | Loss: 0.00002102
Iteration 40/1000 | Loss: 0.00002102
Iteration 41/1000 | Loss: 0.00002102
Iteration 42/1000 | Loss: 0.00002102
Iteration 43/1000 | Loss: 0.00002102
Iteration 44/1000 | Loss: 0.00002102
Iteration 45/1000 | Loss: 0.00002102
Iteration 46/1000 | Loss: 0.00002102
Iteration 47/1000 | Loss: 0.00002102
Iteration 48/1000 | Loss: 0.00002101
Iteration 49/1000 | Loss: 0.00002101
Iteration 50/1000 | Loss: 0.00002101
Iteration 51/1000 | Loss: 0.00002101
Iteration 52/1000 | Loss: 0.00002101
Iteration 53/1000 | Loss: 0.00002101
Iteration 54/1000 | Loss: 0.00002100
Iteration 55/1000 | Loss: 0.00002100
Iteration 56/1000 | Loss: 0.00002099
Iteration 57/1000 | Loss: 0.00002098
Iteration 58/1000 | Loss: 0.00002097
Iteration 59/1000 | Loss: 0.00002097
Iteration 60/1000 | Loss: 0.00002096
Iteration 61/1000 | Loss: 0.00002096
Iteration 62/1000 | Loss: 0.00002096
Iteration 63/1000 | Loss: 0.00002095
Iteration 64/1000 | Loss: 0.00002095
Iteration 65/1000 | Loss: 0.00002095
Iteration 66/1000 | Loss: 0.00002094
Iteration 67/1000 | Loss: 0.00002094
Iteration 68/1000 | Loss: 0.00002094
Iteration 69/1000 | Loss: 0.00002094
Iteration 70/1000 | Loss: 0.00002094
Iteration 71/1000 | Loss: 0.00002094
Iteration 72/1000 | Loss: 0.00002572
Iteration 73/1000 | Loss: 0.00002572
Iteration 74/1000 | Loss: 0.00002181
Iteration 75/1000 | Loss: 0.00002106
Iteration 76/1000 | Loss: 0.00002089
Iteration 77/1000 | Loss: 0.00002088
Iteration 78/1000 | Loss: 0.00002088
Iteration 79/1000 | Loss: 0.00002088
Iteration 80/1000 | Loss: 0.00002236
Iteration 81/1000 | Loss: 0.00002098
Iteration 82/1000 | Loss: 0.00002091
Iteration 83/1000 | Loss: 0.00002087
Iteration 84/1000 | Loss: 0.00002087
Iteration 85/1000 | Loss: 0.00002087
Iteration 86/1000 | Loss: 0.00002087
Iteration 87/1000 | Loss: 0.00002086
Iteration 88/1000 | Loss: 0.00002086
Iteration 89/1000 | Loss: 0.00002086
Iteration 90/1000 | Loss: 0.00002086
Iteration 91/1000 | Loss: 0.00002086
Iteration 92/1000 | Loss: 0.00002086
Iteration 93/1000 | Loss: 0.00002086
Iteration 94/1000 | Loss: 0.00002086
Iteration 95/1000 | Loss: 0.00002086
Iteration 96/1000 | Loss: 0.00002086
Iteration 97/1000 | Loss: 0.00002086
Iteration 98/1000 | Loss: 0.00002086
Iteration 99/1000 | Loss: 0.00002086
Iteration 100/1000 | Loss: 0.00002086
Iteration 101/1000 | Loss: 0.00002086
Iteration 102/1000 | Loss: 0.00002086
Iteration 103/1000 | Loss: 0.00002086
Iteration 104/1000 | Loss: 0.00002086
Iteration 105/1000 | Loss: 0.00002086
Iteration 106/1000 | Loss: 0.00002086
Iteration 107/1000 | Loss: 0.00002086
Iteration 108/1000 | Loss: 0.00002086
Iteration 109/1000 | Loss: 0.00002086
Iteration 110/1000 | Loss: 0.00002086
Iteration 111/1000 | Loss: 0.00002086
Iteration 112/1000 | Loss: 0.00002086
Iteration 113/1000 | Loss: 0.00002086
Iteration 114/1000 | Loss: 0.00002086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.085979576804675e-05, 2.085979576804675e-05, 2.085979576804675e-05, 2.085979576804675e-05, 2.085979576804675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.085979576804675e-05

Optimization complete. Final v2v error: 3.873229503631592 mm

Highest mean error: 4.580422878265381 mm for frame 229

Lowest mean error: 3.514198064804077 mm for frame 17

Saving results

Total time: 54.625141620635986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439557
Iteration 2/25 | Loss: 0.00157960
Iteration 3/25 | Loss: 0.00141888
Iteration 4/25 | Loss: 0.00139244
Iteration 5/25 | Loss: 0.00138647
Iteration 6/25 | Loss: 0.00138506
Iteration 7/25 | Loss: 0.00138506
Iteration 8/25 | Loss: 0.00138506
Iteration 9/25 | Loss: 0.00138506
Iteration 10/25 | Loss: 0.00138506
Iteration 11/25 | Loss: 0.00138506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013850624673068523, 0.0013850624673068523, 0.0013850624673068523, 0.0013850624673068523, 0.0013850624673068523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013850624673068523

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22388721
Iteration 2/25 | Loss: 0.00193887
Iteration 3/25 | Loss: 0.00193887
Iteration 4/25 | Loss: 0.00193887
Iteration 5/25 | Loss: 0.00193887
Iteration 6/25 | Loss: 0.00193887
Iteration 7/25 | Loss: 0.00193887
Iteration 8/25 | Loss: 0.00193887
Iteration 9/25 | Loss: 0.00193887
Iteration 10/25 | Loss: 0.00193887
Iteration 11/25 | Loss: 0.00193887
Iteration 12/25 | Loss: 0.00193887
Iteration 13/25 | Loss: 0.00193887
Iteration 14/25 | Loss: 0.00193887
Iteration 15/25 | Loss: 0.00193887
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0019388695945963264, 0.0019388695945963264, 0.0019388695945963264, 0.0019388695945963264, 0.0019388695945963264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019388695945963264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193887
Iteration 2/1000 | Loss: 0.00006391
Iteration 3/1000 | Loss: 0.00003759
Iteration 4/1000 | Loss: 0.00002896
Iteration 5/1000 | Loss: 0.00002576
Iteration 6/1000 | Loss: 0.00002366
Iteration 7/1000 | Loss: 0.00002268
Iteration 8/1000 | Loss: 0.00002199
Iteration 9/1000 | Loss: 0.00002143
Iteration 10/1000 | Loss: 0.00002096
Iteration 11/1000 | Loss: 0.00002063
Iteration 12/1000 | Loss: 0.00002034
Iteration 13/1000 | Loss: 0.00002033
Iteration 14/1000 | Loss: 0.00002014
Iteration 15/1000 | Loss: 0.00002007
Iteration 16/1000 | Loss: 0.00002003
Iteration 17/1000 | Loss: 0.00002003
Iteration 18/1000 | Loss: 0.00002002
Iteration 19/1000 | Loss: 0.00002001
Iteration 20/1000 | Loss: 0.00002000
Iteration 21/1000 | Loss: 0.00001997
Iteration 22/1000 | Loss: 0.00001996
Iteration 23/1000 | Loss: 0.00001996
Iteration 24/1000 | Loss: 0.00001996
Iteration 25/1000 | Loss: 0.00001996
Iteration 26/1000 | Loss: 0.00001996
Iteration 27/1000 | Loss: 0.00001996
Iteration 28/1000 | Loss: 0.00001996
Iteration 29/1000 | Loss: 0.00001996
Iteration 30/1000 | Loss: 0.00001996
Iteration 31/1000 | Loss: 0.00001996
Iteration 32/1000 | Loss: 0.00001995
Iteration 33/1000 | Loss: 0.00001995
Iteration 34/1000 | Loss: 0.00001995
Iteration 35/1000 | Loss: 0.00001995
Iteration 36/1000 | Loss: 0.00001995
Iteration 37/1000 | Loss: 0.00001995
Iteration 38/1000 | Loss: 0.00001995
Iteration 39/1000 | Loss: 0.00001993
Iteration 40/1000 | Loss: 0.00001992
Iteration 41/1000 | Loss: 0.00001992
Iteration 42/1000 | Loss: 0.00001991
Iteration 43/1000 | Loss: 0.00001991
Iteration 44/1000 | Loss: 0.00001991
Iteration 45/1000 | Loss: 0.00001991
Iteration 46/1000 | Loss: 0.00001991
Iteration 47/1000 | Loss: 0.00001991
Iteration 48/1000 | Loss: 0.00001991
Iteration 49/1000 | Loss: 0.00001991
Iteration 50/1000 | Loss: 0.00001991
Iteration 51/1000 | Loss: 0.00001991
Iteration 52/1000 | Loss: 0.00001990
Iteration 53/1000 | Loss: 0.00001990
Iteration 54/1000 | Loss: 0.00001989
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00001988
Iteration 57/1000 | Loss: 0.00001988
Iteration 58/1000 | Loss: 0.00001988
Iteration 59/1000 | Loss: 0.00001987
Iteration 60/1000 | Loss: 0.00001987
Iteration 61/1000 | Loss: 0.00001987
Iteration 62/1000 | Loss: 0.00001987
Iteration 63/1000 | Loss: 0.00001987
Iteration 64/1000 | Loss: 0.00001987
Iteration 65/1000 | Loss: 0.00001987
Iteration 66/1000 | Loss: 0.00001987
Iteration 67/1000 | Loss: 0.00001987
Iteration 68/1000 | Loss: 0.00001987
Iteration 69/1000 | Loss: 0.00001987
Iteration 70/1000 | Loss: 0.00001987
Iteration 71/1000 | Loss: 0.00001987
Iteration 72/1000 | Loss: 0.00001987
Iteration 73/1000 | Loss: 0.00001987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [1.9867278751917183e-05, 1.9867278751917183e-05, 1.9867278751917183e-05, 1.9867278751917183e-05, 1.9867278751917183e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9867278751917183e-05

Optimization complete. Final v2v error: 3.7330315113067627 mm

Highest mean error: 4.279595851898193 mm for frame 111

Lowest mean error: 3.4723072052001953 mm for frame 193

Saving results

Total time: 36.467554569244385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00564317
Iteration 2/25 | Loss: 0.00167544
Iteration 3/25 | Loss: 0.00144893
Iteration 4/25 | Loss: 0.00138507
Iteration 5/25 | Loss: 0.00136928
Iteration 6/25 | Loss: 0.00136332
Iteration 7/25 | Loss: 0.00137194
Iteration 8/25 | Loss: 0.00136627
Iteration 9/25 | Loss: 0.00135178
Iteration 10/25 | Loss: 0.00134765
Iteration 11/25 | Loss: 0.00134661
Iteration 12/25 | Loss: 0.00134620
Iteration 13/25 | Loss: 0.00134593
Iteration 14/25 | Loss: 0.00134568
Iteration 15/25 | Loss: 0.00134916
Iteration 16/25 | Loss: 0.00134559
Iteration 17/25 | Loss: 0.00134481
Iteration 18/25 | Loss: 0.00134845
Iteration 19/25 | Loss: 0.00134627
Iteration 20/25 | Loss: 0.00134821
Iteration 21/25 | Loss: 0.00134273
Iteration 22/25 | Loss: 0.00134166
Iteration 23/25 | Loss: 0.00134131
Iteration 24/25 | Loss: 0.00134126
Iteration 25/25 | Loss: 0.00134126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30101717
Iteration 2/25 | Loss: 0.00176893
Iteration 3/25 | Loss: 0.00176893
Iteration 4/25 | Loss: 0.00176893
Iteration 5/25 | Loss: 0.00176893
Iteration 6/25 | Loss: 0.00176893
Iteration 7/25 | Loss: 0.00176893
Iteration 8/25 | Loss: 0.00176893
Iteration 9/25 | Loss: 0.00176893
Iteration 10/25 | Loss: 0.00176893
Iteration 11/25 | Loss: 0.00176893
Iteration 12/25 | Loss: 0.00176893
Iteration 13/25 | Loss: 0.00176893
Iteration 14/25 | Loss: 0.00176893
Iteration 15/25 | Loss: 0.00176893
Iteration 16/25 | Loss: 0.00176893
Iteration 17/25 | Loss: 0.00176893
Iteration 18/25 | Loss: 0.00176893
Iteration 19/25 | Loss: 0.00176893
Iteration 20/25 | Loss: 0.00176893
Iteration 21/25 | Loss: 0.00176893
Iteration 22/25 | Loss: 0.00176893
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0017689266242086887, 0.0017689266242086887, 0.0017689266242086887, 0.0017689266242086887, 0.0017689266242086887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017689266242086887

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176893
Iteration 2/1000 | Loss: 0.00004769
Iteration 3/1000 | Loss: 0.00003292
Iteration 4/1000 | Loss: 0.00002904
Iteration 5/1000 | Loss: 0.00002627
Iteration 6/1000 | Loss: 0.00002536
Iteration 7/1000 | Loss: 0.00002463
Iteration 8/1000 | Loss: 0.00002424
Iteration 9/1000 | Loss: 0.00002390
Iteration 10/1000 | Loss: 0.00002364
Iteration 11/1000 | Loss: 0.00002347
Iteration 12/1000 | Loss: 0.00002339
Iteration 13/1000 | Loss: 0.00002335
Iteration 14/1000 | Loss: 0.00002334
Iteration 15/1000 | Loss: 0.00002333
Iteration 16/1000 | Loss: 0.00002333
Iteration 17/1000 | Loss: 0.00002332
Iteration 18/1000 | Loss: 0.00002332
Iteration 19/1000 | Loss: 0.00002331
Iteration 20/1000 | Loss: 0.00002329
Iteration 21/1000 | Loss: 0.00002329
Iteration 22/1000 | Loss: 0.00002328
Iteration 23/1000 | Loss: 0.00002328
Iteration 24/1000 | Loss: 0.00002327
Iteration 25/1000 | Loss: 0.00002327
Iteration 26/1000 | Loss: 0.00002327
Iteration 27/1000 | Loss: 0.00002327
Iteration 28/1000 | Loss: 0.00002327
Iteration 29/1000 | Loss: 0.00002327
Iteration 30/1000 | Loss: 0.00002326
Iteration 31/1000 | Loss: 0.00002326
Iteration 32/1000 | Loss: 0.00002325
Iteration 33/1000 | Loss: 0.00002325
Iteration 34/1000 | Loss: 0.00002324
Iteration 35/1000 | Loss: 0.00002324
Iteration 36/1000 | Loss: 0.00002324
Iteration 37/1000 | Loss: 0.00002323
Iteration 38/1000 | Loss: 0.00002323
Iteration 39/1000 | Loss: 0.00002322
Iteration 40/1000 | Loss: 0.00002322
Iteration 41/1000 | Loss: 0.00002322
Iteration 42/1000 | Loss: 0.00002322
Iteration 43/1000 | Loss: 0.00002322
Iteration 44/1000 | Loss: 0.00002322
Iteration 45/1000 | Loss: 0.00002321
Iteration 46/1000 | Loss: 0.00002321
Iteration 47/1000 | Loss: 0.00002321
Iteration 48/1000 | Loss: 0.00002321
Iteration 49/1000 | Loss: 0.00002320
Iteration 50/1000 | Loss: 0.00002320
Iteration 51/1000 | Loss: 0.00002320
Iteration 52/1000 | Loss: 0.00002320
Iteration 53/1000 | Loss: 0.00002320
Iteration 54/1000 | Loss: 0.00002319
Iteration 55/1000 | Loss: 0.00002319
Iteration 56/1000 | Loss: 0.00002319
Iteration 57/1000 | Loss: 0.00002319
Iteration 58/1000 | Loss: 0.00002319
Iteration 59/1000 | Loss: 0.00002319
Iteration 60/1000 | Loss: 0.00002318
Iteration 61/1000 | Loss: 0.00002318
Iteration 62/1000 | Loss: 0.00002318
Iteration 63/1000 | Loss: 0.00002318
Iteration 64/1000 | Loss: 0.00002317
Iteration 65/1000 | Loss: 0.00002317
Iteration 66/1000 | Loss: 0.00002317
Iteration 67/1000 | Loss: 0.00002317
Iteration 68/1000 | Loss: 0.00002317
Iteration 69/1000 | Loss: 0.00002317
Iteration 70/1000 | Loss: 0.00002316
Iteration 71/1000 | Loss: 0.00002316
Iteration 72/1000 | Loss: 0.00002316
Iteration 73/1000 | Loss: 0.00002316
Iteration 74/1000 | Loss: 0.00002316
Iteration 75/1000 | Loss: 0.00002315
Iteration 76/1000 | Loss: 0.00002315
Iteration 77/1000 | Loss: 0.00002315
Iteration 78/1000 | Loss: 0.00002314
Iteration 79/1000 | Loss: 0.00002314
Iteration 80/1000 | Loss: 0.00002314
Iteration 81/1000 | Loss: 0.00002314
Iteration 82/1000 | Loss: 0.00002313
Iteration 83/1000 | Loss: 0.00002313
Iteration 84/1000 | Loss: 0.00002313
Iteration 85/1000 | Loss: 0.00002313
Iteration 86/1000 | Loss: 0.00002313
Iteration 87/1000 | Loss: 0.00002312
Iteration 88/1000 | Loss: 0.00002312
Iteration 89/1000 | Loss: 0.00002311
Iteration 90/1000 | Loss: 0.00002311
Iteration 91/1000 | Loss: 0.00002311
Iteration 92/1000 | Loss: 0.00002311
Iteration 93/1000 | Loss: 0.00002311
Iteration 94/1000 | Loss: 0.00002311
Iteration 95/1000 | Loss: 0.00002311
Iteration 96/1000 | Loss: 0.00002311
Iteration 97/1000 | Loss: 0.00002311
Iteration 98/1000 | Loss: 0.00002311
Iteration 99/1000 | Loss: 0.00002310
Iteration 100/1000 | Loss: 0.00002310
Iteration 101/1000 | Loss: 0.00002310
Iteration 102/1000 | Loss: 0.00002309
Iteration 103/1000 | Loss: 0.00002309
Iteration 104/1000 | Loss: 0.00002309
Iteration 105/1000 | Loss: 0.00002309
Iteration 106/1000 | Loss: 0.00002309
Iteration 107/1000 | Loss: 0.00002309
Iteration 108/1000 | Loss: 0.00002308
Iteration 109/1000 | Loss: 0.00002308
Iteration 110/1000 | Loss: 0.00002308
Iteration 111/1000 | Loss: 0.00002308
Iteration 112/1000 | Loss: 0.00002308
Iteration 113/1000 | Loss: 0.00002308
Iteration 114/1000 | Loss: 0.00002308
Iteration 115/1000 | Loss: 0.00002308
Iteration 116/1000 | Loss: 0.00002308
Iteration 117/1000 | Loss: 0.00002308
Iteration 118/1000 | Loss: 0.00002308
Iteration 119/1000 | Loss: 0.00002308
Iteration 120/1000 | Loss: 0.00002308
Iteration 121/1000 | Loss: 0.00002308
Iteration 122/1000 | Loss: 0.00002308
Iteration 123/1000 | Loss: 0.00002308
Iteration 124/1000 | Loss: 0.00002308
Iteration 125/1000 | Loss: 0.00002308
Iteration 126/1000 | Loss: 0.00002308
Iteration 127/1000 | Loss: 0.00002308
Iteration 128/1000 | Loss: 0.00002308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.308041621290613e-05, 2.308041621290613e-05, 2.308041621290613e-05, 2.308041621290613e-05, 2.308041621290613e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.308041621290613e-05

Optimization complete. Final v2v error: 3.988189697265625 mm

Highest mean error: 4.430708408355713 mm for frame 69

Lowest mean error: 3.58685564994812 mm for frame 166

Saving results

Total time: 66.67715835571289
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872592
Iteration 2/25 | Loss: 0.00165731
Iteration 3/25 | Loss: 0.00141007
Iteration 4/25 | Loss: 0.00139443
Iteration 5/25 | Loss: 0.00138969
Iteration 6/25 | Loss: 0.00138787
Iteration 7/25 | Loss: 0.00138787
Iteration 8/25 | Loss: 0.00138787
Iteration 9/25 | Loss: 0.00138787
Iteration 10/25 | Loss: 0.00138787
Iteration 11/25 | Loss: 0.00138787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013878741301596165, 0.0013878741301596165, 0.0013878741301596165, 0.0013878741301596165, 0.0013878741301596165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013878741301596165

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15543878
Iteration 2/25 | Loss: 0.00191660
Iteration 3/25 | Loss: 0.00191660
Iteration 4/25 | Loss: 0.00191660
Iteration 5/25 | Loss: 0.00191660
Iteration 6/25 | Loss: 0.00191660
Iteration 7/25 | Loss: 0.00191660
Iteration 8/25 | Loss: 0.00191660
Iteration 9/25 | Loss: 0.00191660
Iteration 10/25 | Loss: 0.00191660
Iteration 11/25 | Loss: 0.00191660
Iteration 12/25 | Loss: 0.00191660
Iteration 13/25 | Loss: 0.00191660
Iteration 14/25 | Loss: 0.00191660
Iteration 15/25 | Loss: 0.00191660
Iteration 16/25 | Loss: 0.00191660
Iteration 17/25 | Loss: 0.00191660
Iteration 18/25 | Loss: 0.00191660
Iteration 19/25 | Loss: 0.00191660
Iteration 20/25 | Loss: 0.00191660
Iteration 21/25 | Loss: 0.00191660
Iteration 22/25 | Loss: 0.00191660
Iteration 23/25 | Loss: 0.00191660
Iteration 24/25 | Loss: 0.00191660
Iteration 25/25 | Loss: 0.00191660

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191660
Iteration 2/1000 | Loss: 0.00009088
Iteration 3/1000 | Loss: 0.00005476
Iteration 4/1000 | Loss: 0.00004370
Iteration 5/1000 | Loss: 0.00003956
Iteration 6/1000 | Loss: 0.00003576
Iteration 7/1000 | Loss: 0.00003344
Iteration 8/1000 | Loss: 0.00003256
Iteration 9/1000 | Loss: 0.00003186
Iteration 10/1000 | Loss: 0.00003143
Iteration 11/1000 | Loss: 0.00003107
Iteration 12/1000 | Loss: 0.00003071
Iteration 13/1000 | Loss: 0.00003042
Iteration 14/1000 | Loss: 0.00003016
Iteration 15/1000 | Loss: 0.00003001
Iteration 16/1000 | Loss: 0.00003000
Iteration 17/1000 | Loss: 0.00002999
Iteration 18/1000 | Loss: 0.00002994
Iteration 19/1000 | Loss: 0.00002994
Iteration 20/1000 | Loss: 0.00002994
Iteration 21/1000 | Loss: 0.00002994
Iteration 22/1000 | Loss: 0.00002994
Iteration 23/1000 | Loss: 0.00002994
Iteration 24/1000 | Loss: 0.00002994
Iteration 25/1000 | Loss: 0.00002994
Iteration 26/1000 | Loss: 0.00002993
Iteration 27/1000 | Loss: 0.00002991
Iteration 28/1000 | Loss: 0.00002991
Iteration 29/1000 | Loss: 0.00002991
Iteration 30/1000 | Loss: 0.00002991
Iteration 31/1000 | Loss: 0.00002991
Iteration 32/1000 | Loss: 0.00002991
Iteration 33/1000 | Loss: 0.00002991
Iteration 34/1000 | Loss: 0.00002991
Iteration 35/1000 | Loss: 0.00002991
Iteration 36/1000 | Loss: 0.00002991
Iteration 37/1000 | Loss: 0.00002991
Iteration 38/1000 | Loss: 0.00002991
Iteration 39/1000 | Loss: 0.00002991
Iteration 40/1000 | Loss: 0.00002991
Iteration 41/1000 | Loss: 0.00002991
Iteration 42/1000 | Loss: 0.00002991
Iteration 43/1000 | Loss: 0.00002991
Iteration 44/1000 | Loss: 0.00002991
Iteration 45/1000 | Loss: 0.00002991
Iteration 46/1000 | Loss: 0.00002991
Iteration 47/1000 | Loss: 0.00002991
Iteration 48/1000 | Loss: 0.00002991
Iteration 49/1000 | Loss: 0.00002991
Iteration 50/1000 | Loss: 0.00002991
Iteration 51/1000 | Loss: 0.00002991
Iteration 52/1000 | Loss: 0.00002991
Iteration 53/1000 | Loss: 0.00002991
Iteration 54/1000 | Loss: 0.00002991
Iteration 55/1000 | Loss: 0.00002991
Iteration 56/1000 | Loss: 0.00002991
Iteration 57/1000 | Loss: 0.00002991
Iteration 58/1000 | Loss: 0.00002991
Iteration 59/1000 | Loss: 0.00002991
Iteration 60/1000 | Loss: 0.00002991
Iteration 61/1000 | Loss: 0.00002991
Iteration 62/1000 | Loss: 0.00002991
Iteration 63/1000 | Loss: 0.00002991
Iteration 64/1000 | Loss: 0.00002991
Iteration 65/1000 | Loss: 0.00002991
Iteration 66/1000 | Loss: 0.00002991
Iteration 67/1000 | Loss: 0.00002991
Iteration 68/1000 | Loss: 0.00002991
Iteration 69/1000 | Loss: 0.00002991
Iteration 70/1000 | Loss: 0.00002991
Iteration 71/1000 | Loss: 0.00002991
Iteration 72/1000 | Loss: 0.00002991
Iteration 73/1000 | Loss: 0.00002991
Iteration 74/1000 | Loss: 0.00002991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [2.990606844832655e-05, 2.990606844832655e-05, 2.990606844832655e-05, 2.990606844832655e-05, 2.990606844832655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.990606844832655e-05

Optimization complete. Final v2v error: 4.4598708152771 mm

Highest mean error: 6.053659439086914 mm for frame 62

Lowest mean error: 3.6616811752319336 mm for frame 1

Saving results

Total time: 33.11415338516235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00654980
Iteration 2/25 | Loss: 0.00138180
Iteration 3/25 | Loss: 0.00131739
Iteration 4/25 | Loss: 0.00130994
Iteration 5/25 | Loss: 0.00130814
Iteration 6/25 | Loss: 0.00130814
Iteration 7/25 | Loss: 0.00130814
Iteration 8/25 | Loss: 0.00130814
Iteration 9/25 | Loss: 0.00130814
Iteration 10/25 | Loss: 0.00130814
Iteration 11/25 | Loss: 0.00130814
Iteration 12/25 | Loss: 0.00130814
Iteration 13/25 | Loss: 0.00130814
Iteration 14/25 | Loss: 0.00130814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013081353390589356, 0.0013081353390589356, 0.0013081353390589356, 0.0013081353390589356, 0.0013081353390589356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013081353390589356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.64040375
Iteration 2/25 | Loss: 0.00169143
Iteration 3/25 | Loss: 0.00169140
Iteration 4/25 | Loss: 0.00169140
Iteration 5/25 | Loss: 0.00169140
Iteration 6/25 | Loss: 0.00169140
Iteration 7/25 | Loss: 0.00169140
Iteration 8/25 | Loss: 0.00169140
Iteration 9/25 | Loss: 0.00169140
Iteration 10/25 | Loss: 0.00169140
Iteration 11/25 | Loss: 0.00169140
Iteration 12/25 | Loss: 0.00169140
Iteration 13/25 | Loss: 0.00169140
Iteration 14/25 | Loss: 0.00169140
Iteration 15/25 | Loss: 0.00169140
Iteration 16/25 | Loss: 0.00169140
Iteration 17/25 | Loss: 0.00169140
Iteration 18/25 | Loss: 0.00169140
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016913977451622486, 0.0016913977451622486, 0.0016913977451622486, 0.0016913977451622486, 0.0016913977451622486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016913977451622486

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169140
Iteration 2/1000 | Loss: 0.00004343
Iteration 3/1000 | Loss: 0.00002830
Iteration 4/1000 | Loss: 0.00002432
Iteration 5/1000 | Loss: 0.00002249
Iteration 6/1000 | Loss: 0.00002145
Iteration 7/1000 | Loss: 0.00002099
Iteration 8/1000 | Loss: 0.00002063
Iteration 9/1000 | Loss: 0.00002048
Iteration 10/1000 | Loss: 0.00002023
Iteration 11/1000 | Loss: 0.00002011
Iteration 12/1000 | Loss: 0.00002008
Iteration 13/1000 | Loss: 0.00001999
Iteration 14/1000 | Loss: 0.00001999
Iteration 15/1000 | Loss: 0.00001999
Iteration 16/1000 | Loss: 0.00001999
Iteration 17/1000 | Loss: 0.00001999
Iteration 18/1000 | Loss: 0.00001999
Iteration 19/1000 | Loss: 0.00001998
Iteration 20/1000 | Loss: 0.00001998
Iteration 21/1000 | Loss: 0.00001997
Iteration 22/1000 | Loss: 0.00001997
Iteration 23/1000 | Loss: 0.00001996
Iteration 24/1000 | Loss: 0.00001996
Iteration 25/1000 | Loss: 0.00001995
Iteration 26/1000 | Loss: 0.00001995
Iteration 27/1000 | Loss: 0.00001995
Iteration 28/1000 | Loss: 0.00001994
Iteration 29/1000 | Loss: 0.00001993
Iteration 30/1000 | Loss: 0.00001992
Iteration 31/1000 | Loss: 0.00001992
Iteration 32/1000 | Loss: 0.00001991
Iteration 33/1000 | Loss: 0.00001991
Iteration 34/1000 | Loss: 0.00001991
Iteration 35/1000 | Loss: 0.00001991
Iteration 36/1000 | Loss: 0.00001990
Iteration 37/1000 | Loss: 0.00001990
Iteration 38/1000 | Loss: 0.00001990
Iteration 39/1000 | Loss: 0.00001989
Iteration 40/1000 | Loss: 0.00001989
Iteration 41/1000 | Loss: 0.00001989
Iteration 42/1000 | Loss: 0.00001988
Iteration 43/1000 | Loss: 0.00001988
Iteration 44/1000 | Loss: 0.00001988
Iteration 45/1000 | Loss: 0.00001988
Iteration 46/1000 | Loss: 0.00001988
Iteration 47/1000 | Loss: 0.00001987
Iteration 48/1000 | Loss: 0.00001987
Iteration 49/1000 | Loss: 0.00001987
Iteration 50/1000 | Loss: 0.00001987
Iteration 51/1000 | Loss: 0.00001987
Iteration 52/1000 | Loss: 0.00001987
Iteration 53/1000 | Loss: 0.00001987
Iteration 54/1000 | Loss: 0.00001987
Iteration 55/1000 | Loss: 0.00001987
Iteration 56/1000 | Loss: 0.00001987
Iteration 57/1000 | Loss: 0.00001987
Iteration 58/1000 | Loss: 0.00001987
Iteration 59/1000 | Loss: 0.00001987
Iteration 60/1000 | Loss: 0.00001987
Iteration 61/1000 | Loss: 0.00001987
Iteration 62/1000 | Loss: 0.00001987
Iteration 63/1000 | Loss: 0.00001987
Iteration 64/1000 | Loss: 0.00001987
Iteration 65/1000 | Loss: 0.00001987
Iteration 66/1000 | Loss: 0.00001987
Iteration 67/1000 | Loss: 0.00001987
Iteration 68/1000 | Loss: 0.00001987
Iteration 69/1000 | Loss: 0.00001987
Iteration 70/1000 | Loss: 0.00001987
Iteration 71/1000 | Loss: 0.00001987
Iteration 72/1000 | Loss: 0.00001987
Iteration 73/1000 | Loss: 0.00001987
Iteration 74/1000 | Loss: 0.00001987
Iteration 75/1000 | Loss: 0.00001987
Iteration 76/1000 | Loss: 0.00001987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.9867249648086727e-05, 1.9867249648086727e-05, 1.9867249648086727e-05, 1.9867249648086727e-05, 1.9867249648086727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9867249648086727e-05

Optimization complete. Final v2v error: 3.671468496322632 mm

Highest mean error: 4.108846187591553 mm for frame 116

Lowest mean error: 3.3903744220733643 mm for frame 8

Saving results

Total time: 31.304969310760498
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481009
Iteration 2/25 | Loss: 0.00147147
Iteration 3/25 | Loss: 0.00138037
Iteration 4/25 | Loss: 0.00136826
Iteration 5/25 | Loss: 0.00136423
Iteration 6/25 | Loss: 0.00136369
Iteration 7/25 | Loss: 0.00136369
Iteration 8/25 | Loss: 0.00136369
Iteration 9/25 | Loss: 0.00136369
Iteration 10/25 | Loss: 0.00136369
Iteration 11/25 | Loss: 0.00136369
Iteration 12/25 | Loss: 0.00136369
Iteration 13/25 | Loss: 0.00136369
Iteration 14/25 | Loss: 0.00136369
Iteration 15/25 | Loss: 0.00136369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013636924559250474, 0.0013636924559250474, 0.0013636924559250474, 0.0013636924559250474, 0.0013636924559250474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013636924559250474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26162744
Iteration 2/25 | Loss: 0.00160984
Iteration 3/25 | Loss: 0.00160984
Iteration 4/25 | Loss: 0.00160983
Iteration 5/25 | Loss: 0.00160983
Iteration 6/25 | Loss: 0.00160983
Iteration 7/25 | Loss: 0.00160983
Iteration 8/25 | Loss: 0.00160983
Iteration 9/25 | Loss: 0.00160983
Iteration 10/25 | Loss: 0.00160983
Iteration 11/25 | Loss: 0.00160983
Iteration 12/25 | Loss: 0.00160983
Iteration 13/25 | Loss: 0.00160983
Iteration 14/25 | Loss: 0.00160983
Iteration 15/25 | Loss: 0.00160983
Iteration 16/25 | Loss: 0.00160983
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016098321648314595, 0.0016098321648314595, 0.0016098321648314595, 0.0016098321648314595, 0.0016098321648314595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016098321648314595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160983
Iteration 2/1000 | Loss: 0.00004292
Iteration 3/1000 | Loss: 0.00003202
Iteration 4/1000 | Loss: 0.00002851
Iteration 5/1000 | Loss: 0.00002592
Iteration 6/1000 | Loss: 0.00002507
Iteration 7/1000 | Loss: 0.00002448
Iteration 8/1000 | Loss: 0.00002413
Iteration 9/1000 | Loss: 0.00002387
Iteration 10/1000 | Loss: 0.00002366
Iteration 11/1000 | Loss: 0.00002364
Iteration 12/1000 | Loss: 0.00002363
Iteration 13/1000 | Loss: 0.00002355
Iteration 14/1000 | Loss: 0.00002354
Iteration 15/1000 | Loss: 0.00002354
Iteration 16/1000 | Loss: 0.00002349
Iteration 17/1000 | Loss: 0.00002347
Iteration 18/1000 | Loss: 0.00002343
Iteration 19/1000 | Loss: 0.00002342
Iteration 20/1000 | Loss: 0.00002341
Iteration 21/1000 | Loss: 0.00002340
Iteration 22/1000 | Loss: 0.00002339
Iteration 23/1000 | Loss: 0.00002338
Iteration 24/1000 | Loss: 0.00002337
Iteration 25/1000 | Loss: 0.00002337
Iteration 26/1000 | Loss: 0.00002336
Iteration 27/1000 | Loss: 0.00002336
Iteration 28/1000 | Loss: 0.00002335
Iteration 29/1000 | Loss: 0.00002335
Iteration 30/1000 | Loss: 0.00002335
Iteration 31/1000 | Loss: 0.00002335
Iteration 32/1000 | Loss: 0.00002334
Iteration 33/1000 | Loss: 0.00002334
Iteration 34/1000 | Loss: 0.00002334
Iteration 35/1000 | Loss: 0.00002333
Iteration 36/1000 | Loss: 0.00002333
Iteration 37/1000 | Loss: 0.00002333
Iteration 38/1000 | Loss: 0.00002333
Iteration 39/1000 | Loss: 0.00002333
Iteration 40/1000 | Loss: 0.00002333
Iteration 41/1000 | Loss: 0.00002332
Iteration 42/1000 | Loss: 0.00002332
Iteration 43/1000 | Loss: 0.00002332
Iteration 44/1000 | Loss: 0.00002332
Iteration 45/1000 | Loss: 0.00002331
Iteration 46/1000 | Loss: 0.00002331
Iteration 47/1000 | Loss: 0.00002331
Iteration 48/1000 | Loss: 0.00002331
Iteration 49/1000 | Loss: 0.00002331
Iteration 50/1000 | Loss: 0.00002331
Iteration 51/1000 | Loss: 0.00002330
Iteration 52/1000 | Loss: 0.00002330
Iteration 53/1000 | Loss: 0.00002330
Iteration 54/1000 | Loss: 0.00002330
Iteration 55/1000 | Loss: 0.00002330
Iteration 56/1000 | Loss: 0.00002330
Iteration 57/1000 | Loss: 0.00002330
Iteration 58/1000 | Loss: 0.00002330
Iteration 59/1000 | Loss: 0.00002330
Iteration 60/1000 | Loss: 0.00002330
Iteration 61/1000 | Loss: 0.00002330
Iteration 62/1000 | Loss: 0.00002330
Iteration 63/1000 | Loss: 0.00002330
Iteration 64/1000 | Loss: 0.00002330
Iteration 65/1000 | Loss: 0.00002330
Iteration 66/1000 | Loss: 0.00002330
Iteration 67/1000 | Loss: 0.00002330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [2.3298849555430934e-05, 2.3298849555430934e-05, 2.3298849555430934e-05, 2.3298849555430934e-05, 2.3298849555430934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3298849555430934e-05

Optimization complete. Final v2v error: 4.056757926940918 mm

Highest mean error: 4.30764627456665 mm for frame 121

Lowest mean error: 3.737344264984131 mm for frame 47

Saving results

Total time: 27.651817560195923
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01119400
Iteration 2/25 | Loss: 0.01119400
Iteration 3/25 | Loss: 0.00420337
Iteration 4/25 | Loss: 0.00269085
Iteration 5/25 | Loss: 0.00219752
Iteration 6/25 | Loss: 0.00210269
Iteration 7/25 | Loss: 0.00199418
Iteration 8/25 | Loss: 0.00193195
Iteration 9/25 | Loss: 0.00190508
Iteration 10/25 | Loss: 0.00182992
Iteration 11/25 | Loss: 0.00176811
Iteration 12/25 | Loss: 0.00172894
Iteration 13/25 | Loss: 0.00168095
Iteration 14/25 | Loss: 0.00167628
Iteration 15/25 | Loss: 0.00169083
Iteration 16/25 | Loss: 0.00167019
Iteration 17/25 | Loss: 0.00166205
Iteration 18/25 | Loss: 0.00166353
Iteration 19/25 | Loss: 0.00165614
Iteration 20/25 | Loss: 0.00166494
Iteration 21/25 | Loss: 0.00165299
Iteration 22/25 | Loss: 0.00166132
Iteration 23/25 | Loss: 0.00166135
Iteration 24/25 | Loss: 0.00165498
Iteration 25/25 | Loss: 0.00165748

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.51634705
Iteration 2/25 | Loss: 0.00361073
Iteration 3/25 | Loss: 0.00330758
Iteration 4/25 | Loss: 0.00330758
Iteration 5/25 | Loss: 0.00330758
Iteration 6/25 | Loss: 0.00330758
Iteration 7/25 | Loss: 0.00330758
Iteration 8/25 | Loss: 0.00330757
Iteration 9/25 | Loss: 0.00330757
Iteration 10/25 | Loss: 0.00330757
Iteration 11/25 | Loss: 0.00330757
Iteration 12/25 | Loss: 0.00330757
Iteration 13/25 | Loss: 0.00330757
Iteration 14/25 | Loss: 0.00330757
Iteration 15/25 | Loss: 0.00330757
Iteration 16/25 | Loss: 0.00330757
Iteration 17/25 | Loss: 0.00330757
Iteration 18/25 | Loss: 0.00330757
Iteration 19/25 | Loss: 0.00330757
Iteration 20/25 | Loss: 0.00330757
Iteration 21/25 | Loss: 0.00330757
Iteration 22/25 | Loss: 0.00330757
Iteration 23/25 | Loss: 0.00330757
Iteration 24/25 | Loss: 0.00330757
Iteration 25/25 | Loss: 0.00330757

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00330757
Iteration 2/1000 | Loss: 0.00073417
Iteration 3/1000 | Loss: 0.00218527
Iteration 4/1000 | Loss: 0.00039713
Iteration 5/1000 | Loss: 0.00201447
Iteration 6/1000 | Loss: 0.00028132
Iteration 7/1000 | Loss: 0.00034656
Iteration 8/1000 | Loss: 0.00025701
Iteration 9/1000 | Loss: 0.00023536
Iteration 10/1000 | Loss: 0.00023734
Iteration 11/1000 | Loss: 0.00029181
Iteration 12/1000 | Loss: 0.00021089
Iteration 13/1000 | Loss: 0.00028595
Iteration 14/1000 | Loss: 0.00069620
Iteration 15/1000 | Loss: 0.00201768
Iteration 16/1000 | Loss: 0.00076705
Iteration 17/1000 | Loss: 0.00034961
Iteration 18/1000 | Loss: 0.00093481
Iteration 19/1000 | Loss: 0.00040334
Iteration 20/1000 | Loss: 0.00026720
Iteration 21/1000 | Loss: 0.00023877
Iteration 22/1000 | Loss: 0.00025211
Iteration 23/1000 | Loss: 0.00032987
Iteration 24/1000 | Loss: 0.00070471
Iteration 25/1000 | Loss: 0.00052787
Iteration 26/1000 | Loss: 0.00062414
Iteration 27/1000 | Loss: 0.00055226
Iteration 28/1000 | Loss: 0.00043809
Iteration 29/1000 | Loss: 0.00034608
Iteration 30/1000 | Loss: 0.00039921
Iteration 31/1000 | Loss: 0.00081426
Iteration 32/1000 | Loss: 0.00031994
Iteration 33/1000 | Loss: 0.00023917
Iteration 34/1000 | Loss: 0.00038131
Iteration 35/1000 | Loss: 0.00044391
Iteration 36/1000 | Loss: 0.00022164
Iteration 37/1000 | Loss: 0.00023579
Iteration 38/1000 | Loss: 0.00041024
Iteration 39/1000 | Loss: 0.00096243
Iteration 40/1000 | Loss: 0.00200738
Iteration 41/1000 | Loss: 0.00234221
Iteration 42/1000 | Loss: 0.00178239
Iteration 43/1000 | Loss: 0.00170284
Iteration 44/1000 | Loss: 0.00040894
Iteration 45/1000 | Loss: 0.00092931
Iteration 46/1000 | Loss: 0.00069935
Iteration 47/1000 | Loss: 0.00031521
Iteration 48/1000 | Loss: 0.00048186
Iteration 49/1000 | Loss: 0.00101127
Iteration 50/1000 | Loss: 0.00013792
Iteration 51/1000 | Loss: 0.00016624
Iteration 52/1000 | Loss: 0.00042000
Iteration 53/1000 | Loss: 0.00018639
Iteration 54/1000 | Loss: 0.00012870
Iteration 55/1000 | Loss: 0.00028007
Iteration 56/1000 | Loss: 0.00016493
Iteration 57/1000 | Loss: 0.00016363
Iteration 58/1000 | Loss: 0.00005129
Iteration 59/1000 | Loss: 0.00032375
Iteration 60/1000 | Loss: 0.00005312
Iteration 61/1000 | Loss: 0.00005312
Iteration 62/1000 | Loss: 0.00020811
Iteration 63/1000 | Loss: 0.00004789
Iteration 64/1000 | Loss: 0.00010273
Iteration 65/1000 | Loss: 0.00031939
Iteration 66/1000 | Loss: 0.00008784
Iteration 67/1000 | Loss: 0.00005346
Iteration 68/1000 | Loss: 0.00009132
Iteration 69/1000 | Loss: 0.00004527
Iteration 70/1000 | Loss: 0.00003991
Iteration 71/1000 | Loss: 0.00006339
Iteration 72/1000 | Loss: 0.00005113
Iteration 73/1000 | Loss: 0.00003913
Iteration 74/1000 | Loss: 0.00005055
Iteration 75/1000 | Loss: 0.00003883
Iteration 76/1000 | Loss: 0.00003870
Iteration 77/1000 | Loss: 0.00003862
Iteration 78/1000 | Loss: 0.00003861
Iteration 79/1000 | Loss: 0.00003860
Iteration 80/1000 | Loss: 0.00003860
Iteration 81/1000 | Loss: 0.00003859
Iteration 82/1000 | Loss: 0.00003856
Iteration 83/1000 | Loss: 0.00003856
Iteration 84/1000 | Loss: 0.00003856
Iteration 85/1000 | Loss: 0.00003856
Iteration 86/1000 | Loss: 0.00003856
Iteration 87/1000 | Loss: 0.00003856
Iteration 88/1000 | Loss: 0.00003856
Iteration 89/1000 | Loss: 0.00003855
Iteration 90/1000 | Loss: 0.00003855
Iteration 91/1000 | Loss: 0.00003855
Iteration 92/1000 | Loss: 0.00003855
Iteration 93/1000 | Loss: 0.00003853
Iteration 94/1000 | Loss: 0.00003853
Iteration 95/1000 | Loss: 0.00003853
Iteration 96/1000 | Loss: 0.00003853
Iteration 97/1000 | Loss: 0.00003853
Iteration 98/1000 | Loss: 0.00003852
Iteration 99/1000 | Loss: 0.00003852
Iteration 100/1000 | Loss: 0.00003852
Iteration 101/1000 | Loss: 0.00003852
Iteration 102/1000 | Loss: 0.00003852
Iteration 103/1000 | Loss: 0.00003852
Iteration 104/1000 | Loss: 0.00003851
Iteration 105/1000 | Loss: 0.00003850
Iteration 106/1000 | Loss: 0.00003850
Iteration 107/1000 | Loss: 0.00003850
Iteration 108/1000 | Loss: 0.00003850
Iteration 109/1000 | Loss: 0.00003850
Iteration 110/1000 | Loss: 0.00003850
Iteration 111/1000 | Loss: 0.00003850
Iteration 112/1000 | Loss: 0.00003850
Iteration 113/1000 | Loss: 0.00003850
Iteration 114/1000 | Loss: 0.00003850
Iteration 115/1000 | Loss: 0.00003849
Iteration 116/1000 | Loss: 0.00003849
Iteration 117/1000 | Loss: 0.00003849
Iteration 118/1000 | Loss: 0.00003849
Iteration 119/1000 | Loss: 0.00003849
Iteration 120/1000 | Loss: 0.00003849
Iteration 121/1000 | Loss: 0.00003848
Iteration 122/1000 | Loss: 0.00003848
Iteration 123/1000 | Loss: 0.00003848
Iteration 124/1000 | Loss: 0.00003848
Iteration 125/1000 | Loss: 0.00003848
Iteration 126/1000 | Loss: 0.00003848
Iteration 127/1000 | Loss: 0.00003848
Iteration 128/1000 | Loss: 0.00003848
Iteration 129/1000 | Loss: 0.00003847
Iteration 130/1000 | Loss: 0.00003847
Iteration 131/1000 | Loss: 0.00003847
Iteration 132/1000 | Loss: 0.00003846
Iteration 133/1000 | Loss: 0.00003846
Iteration 134/1000 | Loss: 0.00003846
Iteration 135/1000 | Loss: 0.00003846
Iteration 136/1000 | Loss: 0.00003846
Iteration 137/1000 | Loss: 0.00003846
Iteration 138/1000 | Loss: 0.00003845
Iteration 139/1000 | Loss: 0.00003845
Iteration 140/1000 | Loss: 0.00003845
Iteration 141/1000 | Loss: 0.00003845
Iteration 142/1000 | Loss: 0.00003845
Iteration 143/1000 | Loss: 0.00003845
Iteration 144/1000 | Loss: 0.00003845
Iteration 145/1000 | Loss: 0.00003845
Iteration 146/1000 | Loss: 0.00003845
Iteration 147/1000 | Loss: 0.00003844
Iteration 148/1000 | Loss: 0.00003844
Iteration 149/1000 | Loss: 0.00003844
Iteration 150/1000 | Loss: 0.00003844
Iteration 151/1000 | Loss: 0.00003844
Iteration 152/1000 | Loss: 0.00003844
Iteration 153/1000 | Loss: 0.00003844
Iteration 154/1000 | Loss: 0.00003844
Iteration 155/1000 | Loss: 0.00003844
Iteration 156/1000 | Loss: 0.00003843
Iteration 157/1000 | Loss: 0.00003843
Iteration 158/1000 | Loss: 0.00003843
Iteration 159/1000 | Loss: 0.00003843
Iteration 160/1000 | Loss: 0.00003843
Iteration 161/1000 | Loss: 0.00003843
Iteration 162/1000 | Loss: 0.00003843
Iteration 163/1000 | Loss: 0.00003843
Iteration 164/1000 | Loss: 0.00003843
Iteration 165/1000 | Loss: 0.00003842
Iteration 166/1000 | Loss: 0.00003842
Iteration 167/1000 | Loss: 0.00003842
Iteration 168/1000 | Loss: 0.00003842
Iteration 169/1000 | Loss: 0.00003842
Iteration 170/1000 | Loss: 0.00003842
Iteration 171/1000 | Loss: 0.00003842
Iteration 172/1000 | Loss: 0.00003842
Iteration 173/1000 | Loss: 0.00003842
Iteration 174/1000 | Loss: 0.00003841
Iteration 175/1000 | Loss: 0.00003841
Iteration 176/1000 | Loss: 0.00003841
Iteration 177/1000 | Loss: 0.00003841
Iteration 178/1000 | Loss: 0.00003841
Iteration 179/1000 | Loss: 0.00003841
Iteration 180/1000 | Loss: 0.00003841
Iteration 181/1000 | Loss: 0.00003841
Iteration 182/1000 | Loss: 0.00003841
Iteration 183/1000 | Loss: 0.00003841
Iteration 184/1000 | Loss: 0.00003841
Iteration 185/1000 | Loss: 0.00003840
Iteration 186/1000 | Loss: 0.00003840
Iteration 187/1000 | Loss: 0.00003840
Iteration 188/1000 | Loss: 0.00003840
Iteration 189/1000 | Loss: 0.00003840
Iteration 190/1000 | Loss: 0.00003840
Iteration 191/1000 | Loss: 0.00003840
Iteration 192/1000 | Loss: 0.00003840
Iteration 193/1000 | Loss: 0.00003840
Iteration 194/1000 | Loss: 0.00003840
Iteration 195/1000 | Loss: 0.00003839
Iteration 196/1000 | Loss: 0.00003839
Iteration 197/1000 | Loss: 0.00003839
Iteration 198/1000 | Loss: 0.00003839
Iteration 199/1000 | Loss: 0.00003839
Iteration 200/1000 | Loss: 0.00003839
Iteration 201/1000 | Loss: 0.00003839
Iteration 202/1000 | Loss: 0.00003839
Iteration 203/1000 | Loss: 0.00003839
Iteration 204/1000 | Loss: 0.00003839
Iteration 205/1000 | Loss: 0.00003839
Iteration 206/1000 | Loss: 0.00003839
Iteration 207/1000 | Loss: 0.00003839
Iteration 208/1000 | Loss: 0.00003839
Iteration 209/1000 | Loss: 0.00003839
Iteration 210/1000 | Loss: 0.00003839
Iteration 211/1000 | Loss: 0.00003839
Iteration 212/1000 | Loss: 0.00003839
Iteration 213/1000 | Loss: 0.00003839
Iteration 214/1000 | Loss: 0.00003839
Iteration 215/1000 | Loss: 0.00003839
Iteration 216/1000 | Loss: 0.00003839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [3.838993870886043e-05, 3.838993870886043e-05, 3.838993870886043e-05, 3.838993870886043e-05, 3.838993870886043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.838993870886043e-05

Optimization complete. Final v2v error: 4.3343987464904785 mm

Highest mean error: 21.089942932128906 mm for frame 115

Lowest mean error: 3.6804468631744385 mm for frame 38

Saving results

Total time: 164.3220453262329
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050503
Iteration 2/25 | Loss: 0.00215305
Iteration 3/25 | Loss: 0.00173858
Iteration 4/25 | Loss: 0.00161807
Iteration 5/25 | Loss: 0.00161013
Iteration 6/25 | Loss: 0.00158410
Iteration 7/25 | Loss: 0.00151684
Iteration 8/25 | Loss: 0.00148509
Iteration 9/25 | Loss: 0.00146849
Iteration 10/25 | Loss: 0.00145907
Iteration 11/25 | Loss: 0.00146282
Iteration 12/25 | Loss: 0.00145976
Iteration 13/25 | Loss: 0.00145371
Iteration 14/25 | Loss: 0.00145065
Iteration 15/25 | Loss: 0.00145134
Iteration 16/25 | Loss: 0.00144911
Iteration 17/25 | Loss: 0.00145051
Iteration 18/25 | Loss: 0.00144841
Iteration 19/25 | Loss: 0.00144471
Iteration 20/25 | Loss: 0.00144257
Iteration 21/25 | Loss: 0.00144414
Iteration 22/25 | Loss: 0.00145025
Iteration 23/25 | Loss: 0.00144607
Iteration 24/25 | Loss: 0.00144458
Iteration 25/25 | Loss: 0.00144927

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14401281
Iteration 2/25 | Loss: 0.00206730
Iteration 3/25 | Loss: 0.00206730
Iteration 4/25 | Loss: 0.00206730
Iteration 5/25 | Loss: 0.00206730
Iteration 6/25 | Loss: 0.00206730
Iteration 7/25 | Loss: 0.00206730
Iteration 8/25 | Loss: 0.00206730
Iteration 9/25 | Loss: 0.00206729
Iteration 10/25 | Loss: 0.00206729
Iteration 11/25 | Loss: 0.00206729
Iteration 12/25 | Loss: 0.00206729
Iteration 13/25 | Loss: 0.00206729
Iteration 14/25 | Loss: 0.00206729
Iteration 15/25 | Loss: 0.00206729
Iteration 16/25 | Loss: 0.00206729
Iteration 17/25 | Loss: 0.00206729
Iteration 18/25 | Loss: 0.00206729
Iteration 19/25 | Loss: 0.00206729
Iteration 20/25 | Loss: 0.00206729
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0020672944374382496, 0.0020672944374382496, 0.0020672944374382496, 0.0020672944374382496, 0.0020672944374382496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020672944374382496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206729
Iteration 2/1000 | Loss: 0.00012407
Iteration 3/1000 | Loss: 0.00012904
Iteration 4/1000 | Loss: 0.00009974
Iteration 5/1000 | Loss: 0.00008227
Iteration 6/1000 | Loss: 0.00006780
Iteration 7/1000 | Loss: 0.00027299
Iteration 8/1000 | Loss: 0.00012277
Iteration 9/1000 | Loss: 0.00004961
Iteration 10/1000 | Loss: 0.00007083
Iteration 11/1000 | Loss: 0.00008976
Iteration 12/1000 | Loss: 0.00011008
Iteration 13/1000 | Loss: 0.00008593
Iteration 14/1000 | Loss: 0.00016290
Iteration 15/1000 | Loss: 0.00012205
Iteration 16/1000 | Loss: 0.00004982
Iteration 17/1000 | Loss: 0.00005747
Iteration 18/1000 | Loss: 0.00005041
Iteration 19/1000 | Loss: 0.00006604
Iteration 20/1000 | Loss: 0.00004119
Iteration 21/1000 | Loss: 0.00008157
Iteration 22/1000 | Loss: 0.00003979
Iteration 23/1000 | Loss: 0.00012333
Iteration 24/1000 | Loss: 0.00008429
Iteration 25/1000 | Loss: 0.00024527
Iteration 26/1000 | Loss: 0.00044591
Iteration 27/1000 | Loss: 0.00006000
Iteration 28/1000 | Loss: 0.00007946
Iteration 29/1000 | Loss: 0.00019763
Iteration 30/1000 | Loss: 0.00008384
Iteration 31/1000 | Loss: 0.00003925
Iteration 32/1000 | Loss: 0.00003679
Iteration 33/1000 | Loss: 0.00012656
Iteration 34/1000 | Loss: 0.00003978
Iteration 35/1000 | Loss: 0.00009428
Iteration 36/1000 | Loss: 0.00011125
Iteration 37/1000 | Loss: 0.00018312
Iteration 38/1000 | Loss: 0.00008032
Iteration 39/1000 | Loss: 0.00004252
Iteration 40/1000 | Loss: 0.00007318
Iteration 41/1000 | Loss: 0.00011804
Iteration 42/1000 | Loss: 0.00019504
Iteration 43/1000 | Loss: 0.00009707
Iteration 44/1000 | Loss: 0.00020198
Iteration 45/1000 | Loss: 0.00004684
Iteration 46/1000 | Loss: 0.00003787
Iteration 47/1000 | Loss: 0.00003554
Iteration 48/1000 | Loss: 0.00003293
Iteration 49/1000 | Loss: 0.00017881
Iteration 50/1000 | Loss: 0.00003717
Iteration 51/1000 | Loss: 0.00004030
Iteration 52/1000 | Loss: 0.00003393
Iteration 53/1000 | Loss: 0.00003281
Iteration 54/1000 | Loss: 0.00003180
Iteration 55/1000 | Loss: 0.00003071
Iteration 56/1000 | Loss: 0.00003008
Iteration 57/1000 | Loss: 0.00002955
Iteration 58/1000 | Loss: 0.00002936
Iteration 59/1000 | Loss: 0.00009315
Iteration 60/1000 | Loss: 0.00002915
Iteration 61/1000 | Loss: 0.00002892
Iteration 62/1000 | Loss: 0.00002875
Iteration 63/1000 | Loss: 0.00002875
Iteration 64/1000 | Loss: 0.00002874
Iteration 65/1000 | Loss: 0.00002869
Iteration 66/1000 | Loss: 0.00002869
Iteration 67/1000 | Loss: 0.00013061
Iteration 68/1000 | Loss: 0.00026426
Iteration 69/1000 | Loss: 0.00063747
Iteration 70/1000 | Loss: 0.00003408
Iteration 71/1000 | Loss: 0.00003173
Iteration 72/1000 | Loss: 0.00010999
Iteration 73/1000 | Loss: 0.00036244
Iteration 74/1000 | Loss: 0.00003107
Iteration 75/1000 | Loss: 0.00012248
Iteration 76/1000 | Loss: 0.00026968
Iteration 77/1000 | Loss: 0.00003019
Iteration 78/1000 | Loss: 0.00002978
Iteration 79/1000 | Loss: 0.00011678
Iteration 80/1000 | Loss: 0.00009428
Iteration 81/1000 | Loss: 0.00003003
Iteration 82/1000 | Loss: 0.00010750
Iteration 83/1000 | Loss: 0.00006283
Iteration 84/1000 | Loss: 0.00002947
Iteration 85/1000 | Loss: 0.00008594
Iteration 86/1000 | Loss: 0.00006590
Iteration 87/1000 | Loss: 0.00007461
Iteration 88/1000 | Loss: 0.00005824
Iteration 89/1000 | Loss: 0.00005687
Iteration 90/1000 | Loss: 0.00002911
Iteration 91/1000 | Loss: 0.00002897
Iteration 92/1000 | Loss: 0.00002894
Iteration 93/1000 | Loss: 0.00002894
Iteration 94/1000 | Loss: 0.00002884
Iteration 95/1000 | Loss: 0.00002877
Iteration 96/1000 | Loss: 0.00002871
Iteration 97/1000 | Loss: 0.00002870
Iteration 98/1000 | Loss: 0.00002866
Iteration 99/1000 | Loss: 0.00002866
Iteration 100/1000 | Loss: 0.00002865
Iteration 101/1000 | Loss: 0.00002865
Iteration 102/1000 | Loss: 0.00002864
Iteration 103/1000 | Loss: 0.00002862
Iteration 104/1000 | Loss: 0.00002851
Iteration 105/1000 | Loss: 0.00002846
Iteration 106/1000 | Loss: 0.00013391
Iteration 107/1000 | Loss: 0.00013293
Iteration 108/1000 | Loss: 0.00011316
Iteration 109/1000 | Loss: 0.00009905
Iteration 110/1000 | Loss: 0.00010839
Iteration 111/1000 | Loss: 0.00004296
Iteration 112/1000 | Loss: 0.00003144
Iteration 113/1000 | Loss: 0.00003051
Iteration 114/1000 | Loss: 0.00002991
Iteration 115/1000 | Loss: 0.00002959
Iteration 116/1000 | Loss: 0.00012912
Iteration 117/1000 | Loss: 0.00003425
Iteration 118/1000 | Loss: 0.00003184
Iteration 119/1000 | Loss: 0.00003051
Iteration 120/1000 | Loss: 0.00003020
Iteration 121/1000 | Loss: 0.00002989
Iteration 122/1000 | Loss: 0.00011793
Iteration 123/1000 | Loss: 0.00003309
Iteration 124/1000 | Loss: 0.00003190
Iteration 125/1000 | Loss: 0.00003067
Iteration 126/1000 | Loss: 0.00003015
Iteration 127/1000 | Loss: 0.00002974
Iteration 128/1000 | Loss: 0.00002944
Iteration 129/1000 | Loss: 0.00002907
Iteration 130/1000 | Loss: 0.00002890
Iteration 131/1000 | Loss: 0.00002886
Iteration 132/1000 | Loss: 0.00002879
Iteration 133/1000 | Loss: 0.00002873
Iteration 134/1000 | Loss: 0.00002870
Iteration 135/1000 | Loss: 0.00002870
Iteration 136/1000 | Loss: 0.00002868
Iteration 137/1000 | Loss: 0.00002868
Iteration 138/1000 | Loss: 0.00002866
Iteration 139/1000 | Loss: 0.00002865
Iteration 140/1000 | Loss: 0.00002865
Iteration 141/1000 | Loss: 0.00002864
Iteration 142/1000 | Loss: 0.00002863
Iteration 143/1000 | Loss: 0.00002861
Iteration 144/1000 | Loss: 0.00002861
Iteration 145/1000 | Loss: 0.00002861
Iteration 146/1000 | Loss: 0.00002858
Iteration 147/1000 | Loss: 0.00012593
Iteration 148/1000 | Loss: 0.00009985
Iteration 149/1000 | Loss: 0.00003607
Iteration 150/1000 | Loss: 0.00003404
Iteration 151/1000 | Loss: 0.00003209
Iteration 152/1000 | Loss: 0.00003129
Iteration 153/1000 | Loss: 0.00003059
Iteration 154/1000 | Loss: 0.00002969
Iteration 155/1000 | Loss: 0.00002918
Iteration 156/1000 | Loss: 0.00002886
Iteration 157/1000 | Loss: 0.00002877
Iteration 158/1000 | Loss: 0.00002868
Iteration 159/1000 | Loss: 0.00002866
Iteration 160/1000 | Loss: 0.00002864
Iteration 161/1000 | Loss: 0.00002864
Iteration 162/1000 | Loss: 0.00002863
Iteration 163/1000 | Loss: 0.00002863
Iteration 164/1000 | Loss: 0.00002862
Iteration 165/1000 | Loss: 0.00002862
Iteration 166/1000 | Loss: 0.00002861
Iteration 167/1000 | Loss: 0.00002860
Iteration 168/1000 | Loss: 0.00002857
Iteration 169/1000 | Loss: 0.00002857
Iteration 170/1000 | Loss: 0.00002855
Iteration 171/1000 | Loss: 0.00002855
Iteration 172/1000 | Loss: 0.00002855
Iteration 173/1000 | Loss: 0.00002854
Iteration 174/1000 | Loss: 0.00002853
Iteration 175/1000 | Loss: 0.00002853
Iteration 176/1000 | Loss: 0.00002853
Iteration 177/1000 | Loss: 0.00002853
Iteration 178/1000 | Loss: 0.00002852
Iteration 179/1000 | Loss: 0.00002850
Iteration 180/1000 | Loss: 0.00002849
Iteration 181/1000 | Loss: 0.00002849
Iteration 182/1000 | Loss: 0.00002848
Iteration 183/1000 | Loss: 0.00002848
Iteration 184/1000 | Loss: 0.00002847
Iteration 185/1000 | Loss: 0.00002843
Iteration 186/1000 | Loss: 0.00002842
Iteration 187/1000 | Loss: 0.00002841
Iteration 188/1000 | Loss: 0.00002838
Iteration 189/1000 | Loss: 0.00002829
Iteration 190/1000 | Loss: 0.00002828
Iteration 191/1000 | Loss: 0.00002828
Iteration 192/1000 | Loss: 0.00002826
Iteration 193/1000 | Loss: 0.00002825
Iteration 194/1000 | Loss: 0.00002825
Iteration 195/1000 | Loss: 0.00002824
Iteration 196/1000 | Loss: 0.00002824
Iteration 197/1000 | Loss: 0.00002823
Iteration 198/1000 | Loss: 0.00002823
Iteration 199/1000 | Loss: 0.00002822
Iteration 200/1000 | Loss: 0.00002822
Iteration 201/1000 | Loss: 0.00002822
Iteration 202/1000 | Loss: 0.00002822
Iteration 203/1000 | Loss: 0.00002821
Iteration 204/1000 | Loss: 0.00002821
Iteration 205/1000 | Loss: 0.00002821
Iteration 206/1000 | Loss: 0.00002820
Iteration 207/1000 | Loss: 0.00002820
Iteration 208/1000 | Loss: 0.00002820
Iteration 209/1000 | Loss: 0.00002820
Iteration 210/1000 | Loss: 0.00002820
Iteration 211/1000 | Loss: 0.00002820
Iteration 212/1000 | Loss: 0.00002820
Iteration 213/1000 | Loss: 0.00002820
Iteration 214/1000 | Loss: 0.00002820
Iteration 215/1000 | Loss: 0.00002819
Iteration 216/1000 | Loss: 0.00002819
Iteration 217/1000 | Loss: 0.00002819
Iteration 218/1000 | Loss: 0.00002819
Iteration 219/1000 | Loss: 0.00002819
Iteration 220/1000 | Loss: 0.00002818
Iteration 221/1000 | Loss: 0.00002818
Iteration 222/1000 | Loss: 0.00002818
Iteration 223/1000 | Loss: 0.00002818
Iteration 224/1000 | Loss: 0.00002818
Iteration 225/1000 | Loss: 0.00002818
Iteration 226/1000 | Loss: 0.00002818
Iteration 227/1000 | Loss: 0.00002817
Iteration 228/1000 | Loss: 0.00002817
Iteration 229/1000 | Loss: 0.00002817
Iteration 230/1000 | Loss: 0.00002816
Iteration 231/1000 | Loss: 0.00002816
Iteration 232/1000 | Loss: 0.00002816
Iteration 233/1000 | Loss: 0.00002816
Iteration 234/1000 | Loss: 0.00002816
Iteration 235/1000 | Loss: 0.00002816
Iteration 236/1000 | Loss: 0.00002816
Iteration 237/1000 | Loss: 0.00002816
Iteration 238/1000 | Loss: 0.00002816
Iteration 239/1000 | Loss: 0.00002815
Iteration 240/1000 | Loss: 0.00002815
Iteration 241/1000 | Loss: 0.00002815
Iteration 242/1000 | Loss: 0.00002815
Iteration 243/1000 | Loss: 0.00002814
Iteration 244/1000 | Loss: 0.00002814
Iteration 245/1000 | Loss: 0.00002814
Iteration 246/1000 | Loss: 0.00002814
Iteration 247/1000 | Loss: 0.00002813
Iteration 248/1000 | Loss: 0.00002813
Iteration 249/1000 | Loss: 0.00002813
Iteration 250/1000 | Loss: 0.00002813
Iteration 251/1000 | Loss: 0.00002813
Iteration 252/1000 | Loss: 0.00002813
Iteration 253/1000 | Loss: 0.00002813
Iteration 254/1000 | Loss: 0.00002813
Iteration 255/1000 | Loss: 0.00002813
Iteration 256/1000 | Loss: 0.00002813
Iteration 257/1000 | Loss: 0.00002813
Iteration 258/1000 | Loss: 0.00002813
Iteration 259/1000 | Loss: 0.00002813
Iteration 260/1000 | Loss: 0.00002813
Iteration 261/1000 | Loss: 0.00002813
Iteration 262/1000 | Loss: 0.00002813
Iteration 263/1000 | Loss: 0.00002812
Iteration 264/1000 | Loss: 0.00002812
Iteration 265/1000 | Loss: 0.00002812
Iteration 266/1000 | Loss: 0.00002812
Iteration 267/1000 | Loss: 0.00002812
Iteration 268/1000 | Loss: 0.00002811
Iteration 269/1000 | Loss: 0.00002811
Iteration 270/1000 | Loss: 0.00002811
Iteration 271/1000 | Loss: 0.00002811
Iteration 272/1000 | Loss: 0.00002811
Iteration 273/1000 | Loss: 0.00002811
Iteration 274/1000 | Loss: 0.00002811
Iteration 275/1000 | Loss: 0.00002811
Iteration 276/1000 | Loss: 0.00002811
Iteration 277/1000 | Loss: 0.00002811
Iteration 278/1000 | Loss: 0.00002811
Iteration 279/1000 | Loss: 0.00002811
Iteration 280/1000 | Loss: 0.00002811
Iteration 281/1000 | Loss: 0.00002811
Iteration 282/1000 | Loss: 0.00002811
Iteration 283/1000 | Loss: 0.00002811
Iteration 284/1000 | Loss: 0.00002811
Iteration 285/1000 | Loss: 0.00002811
Iteration 286/1000 | Loss: 0.00002811
Iteration 287/1000 | Loss: 0.00002811
Iteration 288/1000 | Loss: 0.00002811
Iteration 289/1000 | Loss: 0.00002810
Iteration 290/1000 | Loss: 0.00002810
Iteration 291/1000 | Loss: 0.00002810
Iteration 292/1000 | Loss: 0.00002810
Iteration 293/1000 | Loss: 0.00002810
Iteration 294/1000 | Loss: 0.00002810
Iteration 295/1000 | Loss: 0.00002810
Iteration 296/1000 | Loss: 0.00002810
Iteration 297/1000 | Loss: 0.00002810
Iteration 298/1000 | Loss: 0.00002810
Iteration 299/1000 | Loss: 0.00002810
Iteration 300/1000 | Loss: 0.00002810
Iteration 301/1000 | Loss: 0.00002809
Iteration 302/1000 | Loss: 0.00002809
Iteration 303/1000 | Loss: 0.00002809
Iteration 304/1000 | Loss: 0.00002809
Iteration 305/1000 | Loss: 0.00002809
Iteration 306/1000 | Loss: 0.00002809
Iteration 307/1000 | Loss: 0.00002809
Iteration 308/1000 | Loss: 0.00002809
Iteration 309/1000 | Loss: 0.00002809
Iteration 310/1000 | Loss: 0.00002809
Iteration 311/1000 | Loss: 0.00002809
Iteration 312/1000 | Loss: 0.00002809
Iteration 313/1000 | Loss: 0.00002809
Iteration 314/1000 | Loss: 0.00002809
Iteration 315/1000 | Loss: 0.00002809
Iteration 316/1000 | Loss: 0.00002809
Iteration 317/1000 | Loss: 0.00002809
Iteration 318/1000 | Loss: 0.00002809
Iteration 319/1000 | Loss: 0.00002809
Iteration 320/1000 | Loss: 0.00002809
Iteration 321/1000 | Loss: 0.00002809
Iteration 322/1000 | Loss: 0.00002809
Iteration 323/1000 | Loss: 0.00002809
Iteration 324/1000 | Loss: 0.00002809
Iteration 325/1000 | Loss: 0.00002809
Iteration 326/1000 | Loss: 0.00002809
Iteration 327/1000 | Loss: 0.00002809
Iteration 328/1000 | Loss: 0.00002809
Iteration 329/1000 | Loss: 0.00002809
Iteration 330/1000 | Loss: 0.00002809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 330. Stopping optimization.
Last 5 losses: [2.8090449632145464e-05, 2.8090449632145464e-05, 2.8090449632145464e-05, 2.8090449632145464e-05, 2.8090449632145464e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8090449632145464e-05

Optimization complete. Final v2v error: 4.355635166168213 mm

Highest mean error: 11.865690231323242 mm for frame 212

Lowest mean error: 3.658388137817383 mm for frame 38

Saving results

Total time: 284.37700724601746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005908
Iteration 2/25 | Loss: 0.00448492
Iteration 3/25 | Loss: 0.00223523
Iteration 4/25 | Loss: 0.00187168
Iteration 5/25 | Loss: 0.00181970
Iteration 6/25 | Loss: 0.00176111
Iteration 7/25 | Loss: 0.00169633
Iteration 8/25 | Loss: 0.00166611
Iteration 9/25 | Loss: 0.00164989
Iteration 10/25 | Loss: 0.00162235
Iteration 11/25 | Loss: 0.00161278
Iteration 12/25 | Loss: 0.00161298
Iteration 13/25 | Loss: 0.00160510
Iteration 14/25 | Loss: 0.00160277
Iteration 15/25 | Loss: 0.00160158
Iteration 16/25 | Loss: 0.00160104
Iteration 17/25 | Loss: 0.00160081
Iteration 18/25 | Loss: 0.00160080
Iteration 19/25 | Loss: 0.00160080
Iteration 20/25 | Loss: 0.00160080
Iteration 21/25 | Loss: 0.00160080
Iteration 22/25 | Loss: 0.00160080
Iteration 23/25 | Loss: 0.00160080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016008001985028386, 0.0016008001985028386, 0.0016008001985028386, 0.0016008001985028386, 0.0016008001985028386]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016008001985028386

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27447653
Iteration 2/25 | Loss: 0.00285770
Iteration 3/25 | Loss: 0.00285770
Iteration 4/25 | Loss: 0.00285770
Iteration 5/25 | Loss: 0.00285770
Iteration 6/25 | Loss: 0.00285770
Iteration 7/25 | Loss: 0.00285769
Iteration 8/25 | Loss: 0.00285769
Iteration 9/25 | Loss: 0.00285769
Iteration 10/25 | Loss: 0.00285769
Iteration 11/25 | Loss: 0.00285769
Iteration 12/25 | Loss: 0.00285769
Iteration 13/25 | Loss: 0.00285769
Iteration 14/25 | Loss: 0.00285769
Iteration 15/25 | Loss: 0.00285769
Iteration 16/25 | Loss: 0.00285769
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0028576937038451433, 0.0028576937038451433, 0.0028576937038451433, 0.0028576937038451433, 0.0028576937038451433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028576937038451433

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00285769
Iteration 2/1000 | Loss: 0.00030460
Iteration 3/1000 | Loss: 0.00022624
Iteration 4/1000 | Loss: 0.00018859
Iteration 5/1000 | Loss: 0.00017307
Iteration 6/1000 | Loss: 0.00016303
Iteration 7/1000 | Loss: 0.00015496
Iteration 8/1000 | Loss: 0.00015038
Iteration 9/1000 | Loss: 0.00037847
Iteration 10/1000 | Loss: 0.00339507
Iteration 11/1000 | Loss: 0.00995806
Iteration 12/1000 | Loss: 0.00041076
Iteration 13/1000 | Loss: 0.00019805
Iteration 14/1000 | Loss: 0.00011323
Iteration 15/1000 | Loss: 0.00008124
Iteration 16/1000 | Loss: 0.00005997
Iteration 17/1000 | Loss: 0.00004970
Iteration 18/1000 | Loss: 0.00004258
Iteration 19/1000 | Loss: 0.00003811
Iteration 20/1000 | Loss: 0.00003320
Iteration 21/1000 | Loss: 0.00003064
Iteration 22/1000 | Loss: 0.00002832
Iteration 23/1000 | Loss: 0.00002665
Iteration 24/1000 | Loss: 0.00002544
Iteration 25/1000 | Loss: 0.00002434
Iteration 26/1000 | Loss: 0.00002355
Iteration 27/1000 | Loss: 0.00002312
Iteration 28/1000 | Loss: 0.00002275
Iteration 29/1000 | Loss: 0.00002248
Iteration 30/1000 | Loss: 0.00002227
Iteration 31/1000 | Loss: 0.00002213
Iteration 32/1000 | Loss: 0.00002205
Iteration 33/1000 | Loss: 0.00002204
Iteration 34/1000 | Loss: 0.00002203
Iteration 35/1000 | Loss: 0.00002203
Iteration 36/1000 | Loss: 0.00002203
Iteration 37/1000 | Loss: 0.00002202
Iteration 38/1000 | Loss: 0.00002202
Iteration 39/1000 | Loss: 0.00002202
Iteration 40/1000 | Loss: 0.00002201
Iteration 41/1000 | Loss: 0.00002201
Iteration 42/1000 | Loss: 0.00002201
Iteration 43/1000 | Loss: 0.00002201
Iteration 44/1000 | Loss: 0.00002200
Iteration 45/1000 | Loss: 0.00002200
Iteration 46/1000 | Loss: 0.00002200
Iteration 47/1000 | Loss: 0.00002199
Iteration 48/1000 | Loss: 0.00002199
Iteration 49/1000 | Loss: 0.00002199
Iteration 50/1000 | Loss: 0.00002199
Iteration 51/1000 | Loss: 0.00002198
Iteration 52/1000 | Loss: 0.00002198
Iteration 53/1000 | Loss: 0.00002198
Iteration 54/1000 | Loss: 0.00002198
Iteration 55/1000 | Loss: 0.00002198
Iteration 56/1000 | Loss: 0.00002198
Iteration 57/1000 | Loss: 0.00002198
Iteration 58/1000 | Loss: 0.00002198
Iteration 59/1000 | Loss: 0.00002198
Iteration 60/1000 | Loss: 0.00002197
Iteration 61/1000 | Loss: 0.00002197
Iteration 62/1000 | Loss: 0.00002197
Iteration 63/1000 | Loss: 0.00002197
Iteration 64/1000 | Loss: 0.00002197
Iteration 65/1000 | Loss: 0.00002197
Iteration 66/1000 | Loss: 0.00002197
Iteration 67/1000 | Loss: 0.00002197
Iteration 68/1000 | Loss: 0.00002197
Iteration 69/1000 | Loss: 0.00002196
Iteration 70/1000 | Loss: 0.00002196
Iteration 71/1000 | Loss: 0.00002196
Iteration 72/1000 | Loss: 0.00002196
Iteration 73/1000 | Loss: 0.00002196
Iteration 74/1000 | Loss: 0.00002196
Iteration 75/1000 | Loss: 0.00002196
Iteration 76/1000 | Loss: 0.00002196
Iteration 77/1000 | Loss: 0.00002195
Iteration 78/1000 | Loss: 0.00002195
Iteration 79/1000 | Loss: 0.00002195
Iteration 80/1000 | Loss: 0.00002195
Iteration 81/1000 | Loss: 0.00002195
Iteration 82/1000 | Loss: 0.00002194
Iteration 83/1000 | Loss: 0.00002194
Iteration 84/1000 | Loss: 0.00002194
Iteration 85/1000 | Loss: 0.00002194
Iteration 86/1000 | Loss: 0.00002194
Iteration 87/1000 | Loss: 0.00002194
Iteration 88/1000 | Loss: 0.00002194
Iteration 89/1000 | Loss: 0.00002194
Iteration 90/1000 | Loss: 0.00002194
Iteration 91/1000 | Loss: 0.00002194
Iteration 92/1000 | Loss: 0.00002194
Iteration 93/1000 | Loss: 0.00002194
Iteration 94/1000 | Loss: 0.00002194
Iteration 95/1000 | Loss: 0.00002194
Iteration 96/1000 | Loss: 0.00002194
Iteration 97/1000 | Loss: 0.00002194
Iteration 98/1000 | Loss: 0.00002194
Iteration 99/1000 | Loss: 0.00002194
Iteration 100/1000 | Loss: 0.00002194
Iteration 101/1000 | Loss: 0.00002194
Iteration 102/1000 | Loss: 0.00002194
Iteration 103/1000 | Loss: 0.00002194
Iteration 104/1000 | Loss: 0.00002194
Iteration 105/1000 | Loss: 0.00002194
Iteration 106/1000 | Loss: 0.00002194
Iteration 107/1000 | Loss: 0.00002194
Iteration 108/1000 | Loss: 0.00002194
Iteration 109/1000 | Loss: 0.00002194
Iteration 110/1000 | Loss: 0.00002194
Iteration 111/1000 | Loss: 0.00002194
Iteration 112/1000 | Loss: 0.00002194
Iteration 113/1000 | Loss: 0.00002194
Iteration 114/1000 | Loss: 0.00002194
Iteration 115/1000 | Loss: 0.00002194
Iteration 116/1000 | Loss: 0.00002194
Iteration 117/1000 | Loss: 0.00002194
Iteration 118/1000 | Loss: 0.00002194
Iteration 119/1000 | Loss: 0.00002194
Iteration 120/1000 | Loss: 0.00002194
Iteration 121/1000 | Loss: 0.00002194
Iteration 122/1000 | Loss: 0.00002194
Iteration 123/1000 | Loss: 0.00002194
Iteration 124/1000 | Loss: 0.00002194
Iteration 125/1000 | Loss: 0.00002194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.1935113181825727e-05, 2.1935113181825727e-05, 2.1935113181825727e-05, 2.1935113181825727e-05, 2.1935113181825727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1935113181825727e-05

Optimization complete. Final v2v error: 3.9888229370117188 mm

Highest mean error: 4.562711715698242 mm for frame 53

Lowest mean error: 3.6782405376434326 mm for frame 82

Saving results

Total time: 80.24255013465881
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887414
Iteration 2/25 | Loss: 0.00167470
Iteration 3/25 | Loss: 0.00142675
Iteration 4/25 | Loss: 0.00140256
Iteration 5/25 | Loss: 0.00139879
Iteration 6/25 | Loss: 0.00139832
Iteration 7/25 | Loss: 0.00139829
Iteration 8/25 | Loss: 0.00139829
Iteration 9/25 | Loss: 0.00139829
Iteration 10/25 | Loss: 0.00139829
Iteration 11/25 | Loss: 0.00139829
Iteration 12/25 | Loss: 0.00139829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001398288644850254, 0.001398288644850254, 0.001398288644850254, 0.001398288644850254, 0.001398288644850254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001398288644850254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88744384
Iteration 2/25 | Loss: 0.00082544
Iteration 3/25 | Loss: 0.00082544
Iteration 4/25 | Loss: 0.00082544
Iteration 5/25 | Loss: 0.00082544
Iteration 6/25 | Loss: 0.00082544
Iteration 7/25 | Loss: 0.00082544
Iteration 8/25 | Loss: 0.00082544
Iteration 9/25 | Loss: 0.00082544
Iteration 10/25 | Loss: 0.00082544
Iteration 11/25 | Loss: 0.00082544
Iteration 12/25 | Loss: 0.00082544
Iteration 13/25 | Loss: 0.00082544
Iteration 14/25 | Loss: 0.00082544
Iteration 15/25 | Loss: 0.00082544
Iteration 16/25 | Loss: 0.00082544
Iteration 17/25 | Loss: 0.00082544
Iteration 18/25 | Loss: 0.00082544
Iteration 19/25 | Loss: 0.00082544
Iteration 20/25 | Loss: 0.00082544
Iteration 21/25 | Loss: 0.00082544
Iteration 22/25 | Loss: 0.00082544
Iteration 23/25 | Loss: 0.00082544
Iteration 24/25 | Loss: 0.00082544
Iteration 25/25 | Loss: 0.00082544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082544
Iteration 2/1000 | Loss: 0.00005087
Iteration 3/1000 | Loss: 0.00003501
Iteration 4/1000 | Loss: 0.00002941
Iteration 5/1000 | Loss: 0.00002701
Iteration 6/1000 | Loss: 0.00002629
Iteration 7/1000 | Loss: 0.00002584
Iteration 8/1000 | Loss: 0.00002559
Iteration 9/1000 | Loss: 0.00002542
Iteration 10/1000 | Loss: 0.00002532
Iteration 11/1000 | Loss: 0.00002518
Iteration 12/1000 | Loss: 0.00002509
Iteration 13/1000 | Loss: 0.00002508
Iteration 14/1000 | Loss: 0.00002507
Iteration 15/1000 | Loss: 0.00002506
Iteration 16/1000 | Loss: 0.00002506
Iteration 17/1000 | Loss: 0.00002506
Iteration 18/1000 | Loss: 0.00002506
Iteration 19/1000 | Loss: 0.00002506
Iteration 20/1000 | Loss: 0.00002505
Iteration 21/1000 | Loss: 0.00002505
Iteration 22/1000 | Loss: 0.00002505
Iteration 23/1000 | Loss: 0.00002504
Iteration 24/1000 | Loss: 0.00002504
Iteration 25/1000 | Loss: 0.00002504
Iteration 26/1000 | Loss: 0.00002504
Iteration 27/1000 | Loss: 0.00002504
Iteration 28/1000 | Loss: 0.00002503
Iteration 29/1000 | Loss: 0.00002503
Iteration 30/1000 | Loss: 0.00002503
Iteration 31/1000 | Loss: 0.00002502
Iteration 32/1000 | Loss: 0.00002502
Iteration 33/1000 | Loss: 0.00002501
Iteration 34/1000 | Loss: 0.00002501
Iteration 35/1000 | Loss: 0.00002501
Iteration 36/1000 | Loss: 0.00002500
Iteration 37/1000 | Loss: 0.00002499
Iteration 38/1000 | Loss: 0.00002498
Iteration 39/1000 | Loss: 0.00002498
Iteration 40/1000 | Loss: 0.00002497
Iteration 41/1000 | Loss: 0.00002497
Iteration 42/1000 | Loss: 0.00002497
Iteration 43/1000 | Loss: 0.00002496
Iteration 44/1000 | Loss: 0.00002496
Iteration 45/1000 | Loss: 0.00002496
Iteration 46/1000 | Loss: 0.00002496
Iteration 47/1000 | Loss: 0.00002495
Iteration 48/1000 | Loss: 0.00002495
Iteration 49/1000 | Loss: 0.00002495
Iteration 50/1000 | Loss: 0.00002495
Iteration 51/1000 | Loss: 0.00002495
Iteration 52/1000 | Loss: 0.00002494
Iteration 53/1000 | Loss: 0.00002494
Iteration 54/1000 | Loss: 0.00002494
Iteration 55/1000 | Loss: 0.00002494
Iteration 56/1000 | Loss: 0.00002494
Iteration 57/1000 | Loss: 0.00002494
Iteration 58/1000 | Loss: 0.00002494
Iteration 59/1000 | Loss: 0.00002494
Iteration 60/1000 | Loss: 0.00002494
Iteration 61/1000 | Loss: 0.00002493
Iteration 62/1000 | Loss: 0.00002493
Iteration 63/1000 | Loss: 0.00002493
Iteration 64/1000 | Loss: 0.00002493
Iteration 65/1000 | Loss: 0.00002493
Iteration 66/1000 | Loss: 0.00002493
Iteration 67/1000 | Loss: 0.00002493
Iteration 68/1000 | Loss: 0.00002493
Iteration 69/1000 | Loss: 0.00002493
Iteration 70/1000 | Loss: 0.00002493
Iteration 71/1000 | Loss: 0.00002493
Iteration 72/1000 | Loss: 0.00002493
Iteration 73/1000 | Loss: 0.00002493
Iteration 74/1000 | Loss: 0.00002493
Iteration 75/1000 | Loss: 0.00002493
Iteration 76/1000 | Loss: 0.00002493
Iteration 77/1000 | Loss: 0.00002493
Iteration 78/1000 | Loss: 0.00002493
Iteration 79/1000 | Loss: 0.00002493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [2.4925315301516093e-05, 2.4925315301516093e-05, 2.4925315301516093e-05, 2.4925315301516093e-05, 2.4925315301516093e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4925315301516093e-05

Optimization complete. Final v2v error: 4.244479656219482 mm

Highest mean error: 4.360393047332764 mm for frame 14

Lowest mean error: 4.05722713470459 mm for frame 73

Saving results

Total time: 29.609256267547607
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00735681
Iteration 2/25 | Loss: 0.00164620
Iteration 3/25 | Loss: 0.00150769
Iteration 4/25 | Loss: 0.00147990
Iteration 5/25 | Loss: 0.00147139
Iteration 6/25 | Loss: 0.00146608
Iteration 7/25 | Loss: 0.00146224
Iteration 8/25 | Loss: 0.00146131
Iteration 9/25 | Loss: 0.00146110
Iteration 10/25 | Loss: 0.00146107
Iteration 11/25 | Loss: 0.00146106
Iteration 12/25 | Loss: 0.00146106
Iteration 13/25 | Loss: 0.00146105
Iteration 14/25 | Loss: 0.00146104
Iteration 15/25 | Loss: 0.00146104
Iteration 16/25 | Loss: 0.00146104
Iteration 17/25 | Loss: 0.00146104
Iteration 18/25 | Loss: 0.00146104
Iteration 19/25 | Loss: 0.00146104
Iteration 20/25 | Loss: 0.00146104
Iteration 21/25 | Loss: 0.00146104
Iteration 22/25 | Loss: 0.00146104
Iteration 23/25 | Loss: 0.00146103
Iteration 24/25 | Loss: 0.00146103
Iteration 25/25 | Loss: 0.00146103

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30455482
Iteration 2/25 | Loss: 0.00278341
Iteration 3/25 | Loss: 0.00278341
Iteration 4/25 | Loss: 0.00278341
Iteration 5/25 | Loss: 0.00278341
Iteration 6/25 | Loss: 0.00278341
Iteration 7/25 | Loss: 0.00278341
Iteration 8/25 | Loss: 0.00278341
Iteration 9/25 | Loss: 0.00278341
Iteration 10/25 | Loss: 0.00278341
Iteration 11/25 | Loss: 0.00278341
Iteration 12/25 | Loss: 0.00278341
Iteration 13/25 | Loss: 0.00278341
Iteration 14/25 | Loss: 0.00278341
Iteration 15/25 | Loss: 0.00278341
Iteration 16/25 | Loss: 0.00278341
Iteration 17/25 | Loss: 0.00278341
Iteration 18/25 | Loss: 0.00278341
Iteration 19/25 | Loss: 0.00278341
Iteration 20/25 | Loss: 0.00278341
Iteration 21/25 | Loss: 0.00278341
Iteration 22/25 | Loss: 0.00278341
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0027834069915115833, 0.0027834069915115833, 0.0027834069915115833, 0.0027834069915115833, 0.0027834069915115833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027834069915115833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00278341
Iteration 2/1000 | Loss: 0.00010681
Iteration 3/1000 | Loss: 0.00006081
Iteration 4/1000 | Loss: 0.00004292
Iteration 5/1000 | Loss: 0.00003637
Iteration 6/1000 | Loss: 0.00003314
Iteration 7/1000 | Loss: 0.00003063
Iteration 8/1000 | Loss: 0.00002984
Iteration 9/1000 | Loss: 0.00002939
Iteration 10/1000 | Loss: 0.00002911
Iteration 11/1000 | Loss: 0.00002874
Iteration 12/1000 | Loss: 0.00002842
Iteration 13/1000 | Loss: 0.00002823
Iteration 14/1000 | Loss: 0.00002817
Iteration 15/1000 | Loss: 0.00002796
Iteration 16/1000 | Loss: 0.00002788
Iteration 17/1000 | Loss: 0.00002784
Iteration 18/1000 | Loss: 0.00002778
Iteration 19/1000 | Loss: 0.00002777
Iteration 20/1000 | Loss: 0.00002777
Iteration 21/1000 | Loss: 0.00002777
Iteration 22/1000 | Loss: 0.00002776
Iteration 23/1000 | Loss: 0.00002775
Iteration 24/1000 | Loss: 0.00002775
Iteration 25/1000 | Loss: 0.00002770
Iteration 26/1000 | Loss: 0.00002770
Iteration 27/1000 | Loss: 0.00002769
Iteration 28/1000 | Loss: 0.00002768
Iteration 29/1000 | Loss: 0.00002768
Iteration 30/1000 | Loss: 0.00002768
Iteration 31/1000 | Loss: 0.00002768
Iteration 32/1000 | Loss: 0.00002767
Iteration 33/1000 | Loss: 0.00002767
Iteration 34/1000 | Loss: 0.00002767
Iteration 35/1000 | Loss: 0.00002767
Iteration 36/1000 | Loss: 0.00002767
Iteration 37/1000 | Loss: 0.00002765
Iteration 38/1000 | Loss: 0.00002765
Iteration 39/1000 | Loss: 0.00002764
Iteration 40/1000 | Loss: 0.00002763
Iteration 41/1000 | Loss: 0.00002763
Iteration 42/1000 | Loss: 0.00002762
Iteration 43/1000 | Loss: 0.00002762
Iteration 44/1000 | Loss: 0.00002762
Iteration 45/1000 | Loss: 0.00002761
Iteration 46/1000 | Loss: 0.00002761
Iteration 47/1000 | Loss: 0.00002761
Iteration 48/1000 | Loss: 0.00002760
Iteration 49/1000 | Loss: 0.00002760
Iteration 50/1000 | Loss: 0.00002760
Iteration 51/1000 | Loss: 0.00002759
Iteration 52/1000 | Loss: 0.00002759
Iteration 53/1000 | Loss: 0.00002758
Iteration 54/1000 | Loss: 0.00002758
Iteration 55/1000 | Loss: 0.00002758
Iteration 56/1000 | Loss: 0.00002757
Iteration 57/1000 | Loss: 0.00002757
Iteration 58/1000 | Loss: 0.00002756
Iteration 59/1000 | Loss: 0.00002756
Iteration 60/1000 | Loss: 0.00002756
Iteration 61/1000 | Loss: 0.00002755
Iteration 62/1000 | Loss: 0.00002755
Iteration 63/1000 | Loss: 0.00002754
Iteration 64/1000 | Loss: 0.00002754
Iteration 65/1000 | Loss: 0.00002754
Iteration 66/1000 | Loss: 0.00002753
Iteration 67/1000 | Loss: 0.00002753
Iteration 68/1000 | Loss: 0.00002752
Iteration 69/1000 | Loss: 0.00002752
Iteration 70/1000 | Loss: 0.00002752
Iteration 71/1000 | Loss: 0.00002752
Iteration 72/1000 | Loss: 0.00002751
Iteration 73/1000 | Loss: 0.00002751
Iteration 74/1000 | Loss: 0.00002751
Iteration 75/1000 | Loss: 0.00002751
Iteration 76/1000 | Loss: 0.00002751
Iteration 77/1000 | Loss: 0.00002751
Iteration 78/1000 | Loss: 0.00002751
Iteration 79/1000 | Loss: 0.00002750
Iteration 80/1000 | Loss: 0.00002750
Iteration 81/1000 | Loss: 0.00002750
Iteration 82/1000 | Loss: 0.00002749
Iteration 83/1000 | Loss: 0.00002749
Iteration 84/1000 | Loss: 0.00002749
Iteration 85/1000 | Loss: 0.00002748
Iteration 86/1000 | Loss: 0.00002748
Iteration 87/1000 | Loss: 0.00002748
Iteration 88/1000 | Loss: 0.00002747
Iteration 89/1000 | Loss: 0.00002747
Iteration 90/1000 | Loss: 0.00002747
Iteration 91/1000 | Loss: 0.00002747
Iteration 92/1000 | Loss: 0.00002747
Iteration 93/1000 | Loss: 0.00002747
Iteration 94/1000 | Loss: 0.00002747
Iteration 95/1000 | Loss: 0.00002746
Iteration 96/1000 | Loss: 0.00002746
Iteration 97/1000 | Loss: 0.00002746
Iteration 98/1000 | Loss: 0.00002746
Iteration 99/1000 | Loss: 0.00002745
Iteration 100/1000 | Loss: 0.00002745
Iteration 101/1000 | Loss: 0.00002744
Iteration 102/1000 | Loss: 0.00002744
Iteration 103/1000 | Loss: 0.00002743
Iteration 104/1000 | Loss: 0.00002743
Iteration 105/1000 | Loss: 0.00002743
Iteration 106/1000 | Loss: 0.00002742
Iteration 107/1000 | Loss: 0.00002742
Iteration 108/1000 | Loss: 0.00002742
Iteration 109/1000 | Loss: 0.00002741
Iteration 110/1000 | Loss: 0.00002741
Iteration 111/1000 | Loss: 0.00002741
Iteration 112/1000 | Loss: 0.00002741
Iteration 113/1000 | Loss: 0.00002741
Iteration 114/1000 | Loss: 0.00002740
Iteration 115/1000 | Loss: 0.00002740
Iteration 116/1000 | Loss: 0.00002740
Iteration 117/1000 | Loss: 0.00002740
Iteration 118/1000 | Loss: 0.00002740
Iteration 119/1000 | Loss: 0.00002739
Iteration 120/1000 | Loss: 0.00002739
Iteration 121/1000 | Loss: 0.00002739
Iteration 122/1000 | Loss: 0.00002738
Iteration 123/1000 | Loss: 0.00002738
Iteration 124/1000 | Loss: 0.00002738
Iteration 125/1000 | Loss: 0.00002738
Iteration 126/1000 | Loss: 0.00002738
Iteration 127/1000 | Loss: 0.00002738
Iteration 128/1000 | Loss: 0.00002738
Iteration 129/1000 | Loss: 0.00002738
Iteration 130/1000 | Loss: 0.00002738
Iteration 131/1000 | Loss: 0.00002738
Iteration 132/1000 | Loss: 0.00002738
Iteration 133/1000 | Loss: 0.00002738
Iteration 134/1000 | Loss: 0.00002738
Iteration 135/1000 | Loss: 0.00002738
Iteration 136/1000 | Loss: 0.00002738
Iteration 137/1000 | Loss: 0.00002738
Iteration 138/1000 | Loss: 0.00002738
Iteration 139/1000 | Loss: 0.00002738
Iteration 140/1000 | Loss: 0.00002738
Iteration 141/1000 | Loss: 0.00002738
Iteration 142/1000 | Loss: 0.00002738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.737588329182472e-05, 2.737588329182472e-05, 2.737588329182472e-05, 2.737588329182472e-05, 2.737588329182472e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.737588329182472e-05

Optimization complete. Final v2v error: 4.480578422546387 mm

Highest mean error: 4.9491658210754395 mm for frame 72

Lowest mean error: 3.570155382156372 mm for frame 5

Saving results

Total time: 49.610342025756836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01106376
Iteration 2/25 | Loss: 0.00353485
Iteration 3/25 | Loss: 0.00311864
Iteration 4/25 | Loss: 0.00300338
Iteration 5/25 | Loss: 0.00282726
Iteration 6/25 | Loss: 0.00278203
Iteration 7/25 | Loss: 0.00267122
Iteration 8/25 | Loss: 0.00256699
Iteration 9/25 | Loss: 0.00249604
Iteration 10/25 | Loss: 0.00245277
Iteration 11/25 | Loss: 0.00243916
Iteration 12/25 | Loss: 0.00241687
Iteration 13/25 | Loss: 0.00238213
Iteration 14/25 | Loss: 0.00237768
Iteration 15/25 | Loss: 0.00236769
Iteration 16/25 | Loss: 0.00236203
Iteration 17/25 | Loss: 0.00236989
Iteration 18/25 | Loss: 0.00238976
Iteration 19/25 | Loss: 0.00236128
Iteration 20/25 | Loss: 0.00233147
Iteration 21/25 | Loss: 0.00231240
Iteration 22/25 | Loss: 0.00230663
Iteration 23/25 | Loss: 0.00229948
Iteration 24/25 | Loss: 0.00228751
Iteration 25/25 | Loss: 0.00227915

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08844745
Iteration 2/25 | Loss: 0.00580621
Iteration 3/25 | Loss: 0.00580620
Iteration 4/25 | Loss: 0.00580620
Iteration 5/25 | Loss: 0.00580620
Iteration 6/25 | Loss: 0.00580620
Iteration 7/25 | Loss: 0.00580620
Iteration 8/25 | Loss: 0.00580620
Iteration 9/25 | Loss: 0.00580620
Iteration 10/25 | Loss: 0.00580620
Iteration 11/25 | Loss: 0.00580620
Iteration 12/25 | Loss: 0.00580620
Iteration 13/25 | Loss: 0.00580620
Iteration 14/25 | Loss: 0.00580620
Iteration 15/25 | Loss: 0.00580620
Iteration 16/25 | Loss: 0.00580620
Iteration 17/25 | Loss: 0.00580620
Iteration 18/25 | Loss: 0.00580620
Iteration 19/25 | Loss: 0.00580620
Iteration 20/25 | Loss: 0.00580620
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0058061955496668816, 0.0058061955496668816, 0.0058061955496668816, 0.0058061955496668816, 0.0058061955496668816]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0058061955496668816

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00580620
Iteration 2/1000 | Loss: 0.00921835
Iteration 3/1000 | Loss: 0.00247770
Iteration 4/1000 | Loss: 0.00678065
Iteration 5/1000 | Loss: 0.00908084
Iteration 6/1000 | Loss: 0.00795277
Iteration 7/1000 | Loss: 0.00689926
Iteration 8/1000 | Loss: 0.00293144
Iteration 9/1000 | Loss: 0.00750974
Iteration 10/1000 | Loss: 0.00437180
Iteration 11/1000 | Loss: 0.00085022
Iteration 12/1000 | Loss: 0.00129363
Iteration 13/1000 | Loss: 0.00071279
Iteration 14/1000 | Loss: 0.00036279
Iteration 15/1000 | Loss: 0.00106334
Iteration 16/1000 | Loss: 0.00075297
Iteration 17/1000 | Loss: 0.00118527
Iteration 18/1000 | Loss: 0.00094167
Iteration 19/1000 | Loss: 0.00147099
Iteration 20/1000 | Loss: 0.00183517
Iteration 21/1000 | Loss: 0.00146862
Iteration 22/1000 | Loss: 0.00101344
Iteration 23/1000 | Loss: 0.00121851
Iteration 24/1000 | Loss: 0.00081045
Iteration 25/1000 | Loss: 0.00076369
Iteration 26/1000 | Loss: 0.00070335
Iteration 27/1000 | Loss: 0.00174075
Iteration 28/1000 | Loss: 0.00126564
Iteration 29/1000 | Loss: 0.00194334
Iteration 30/1000 | Loss: 0.00083595
Iteration 31/1000 | Loss: 0.00034056
Iteration 32/1000 | Loss: 0.00089286
Iteration 33/1000 | Loss: 0.00054675
Iteration 34/1000 | Loss: 0.00048655
Iteration 35/1000 | Loss: 0.00164632
Iteration 36/1000 | Loss: 0.00108508
Iteration 37/1000 | Loss: 0.00090278
Iteration 38/1000 | Loss: 0.00401634
Iteration 39/1000 | Loss: 0.00374100
Iteration 40/1000 | Loss: 0.00136625
Iteration 41/1000 | Loss: 0.00214966
Iteration 42/1000 | Loss: 0.00126029
Iteration 43/1000 | Loss: 0.00074514
Iteration 44/1000 | Loss: 0.00053902
Iteration 45/1000 | Loss: 0.00382958
Iteration 46/1000 | Loss: 0.00339207
Iteration 47/1000 | Loss: 0.00175481
Iteration 48/1000 | Loss: 0.00134801
Iteration 49/1000 | Loss: 0.00069810
Iteration 50/1000 | Loss: 0.00235321
Iteration 51/1000 | Loss: 0.00218061
Iteration 52/1000 | Loss: 0.00225221
Iteration 53/1000 | Loss: 0.00243753
Iteration 54/1000 | Loss: 0.00129867
Iteration 55/1000 | Loss: 0.00215237
Iteration 56/1000 | Loss: 0.00119695
Iteration 57/1000 | Loss: 0.00138989
Iteration 58/1000 | Loss: 0.00174856
Iteration 59/1000 | Loss: 0.00091818
Iteration 60/1000 | Loss: 0.00131059
Iteration 61/1000 | Loss: 0.00156491
Iteration 62/1000 | Loss: 0.00143101
Iteration 63/1000 | Loss: 0.00184006
Iteration 64/1000 | Loss: 0.00151776
Iteration 65/1000 | Loss: 0.00197403
Iteration 66/1000 | Loss: 0.00130339
Iteration 67/1000 | Loss: 0.00153054
Iteration 68/1000 | Loss: 0.00145076
Iteration 69/1000 | Loss: 0.00193553
Iteration 70/1000 | Loss: 0.00091386
Iteration 71/1000 | Loss: 0.00100359
Iteration 72/1000 | Loss: 0.00416863
Iteration 73/1000 | Loss: 0.00294585
Iteration 74/1000 | Loss: 0.00134908
Iteration 75/1000 | Loss: 0.00104643
Iteration 76/1000 | Loss: 0.00095149
Iteration 77/1000 | Loss: 0.00118402
Iteration 78/1000 | Loss: 0.00099467
Iteration 79/1000 | Loss: 0.00210143
Iteration 80/1000 | Loss: 0.00124012
Iteration 81/1000 | Loss: 0.00127675
Iteration 82/1000 | Loss: 0.00092784
Iteration 83/1000 | Loss: 0.00033278
Iteration 84/1000 | Loss: 0.00022522
Iteration 85/1000 | Loss: 0.00152855
Iteration 86/1000 | Loss: 0.00101205
Iteration 87/1000 | Loss: 0.00106693
Iteration 88/1000 | Loss: 0.00037235
Iteration 89/1000 | Loss: 0.00070572
Iteration 90/1000 | Loss: 0.00115328
Iteration 91/1000 | Loss: 0.00059859
Iteration 92/1000 | Loss: 0.00065471
Iteration 93/1000 | Loss: 0.00021004
Iteration 94/1000 | Loss: 0.00025069
Iteration 95/1000 | Loss: 0.00022030
Iteration 96/1000 | Loss: 0.00015805
Iteration 97/1000 | Loss: 0.00055496
Iteration 98/1000 | Loss: 0.00039271
Iteration 99/1000 | Loss: 0.00048837
Iteration 100/1000 | Loss: 0.00016249
Iteration 101/1000 | Loss: 0.00017825
Iteration 102/1000 | Loss: 0.00020711
Iteration 103/1000 | Loss: 0.00019323
Iteration 104/1000 | Loss: 0.00052723
Iteration 105/1000 | Loss: 0.00057705
Iteration 106/1000 | Loss: 0.00055830
Iteration 107/1000 | Loss: 0.00043758
Iteration 108/1000 | Loss: 0.00024388
Iteration 109/1000 | Loss: 0.00020952
Iteration 110/1000 | Loss: 0.00021380
Iteration 111/1000 | Loss: 0.00021309
Iteration 112/1000 | Loss: 0.00014825
Iteration 113/1000 | Loss: 0.00012976
Iteration 114/1000 | Loss: 0.00044590
Iteration 115/1000 | Loss: 0.00018596
Iteration 116/1000 | Loss: 0.00017617
Iteration 117/1000 | Loss: 0.00017351
Iteration 118/1000 | Loss: 0.00018664
Iteration 119/1000 | Loss: 0.00017948
Iteration 120/1000 | Loss: 0.00068782
Iteration 121/1000 | Loss: 0.00017984
Iteration 122/1000 | Loss: 0.00024906
Iteration 123/1000 | Loss: 0.00045184
Iteration 124/1000 | Loss: 0.00034910
Iteration 125/1000 | Loss: 0.00021090
Iteration 126/1000 | Loss: 0.00022192
Iteration 127/1000 | Loss: 0.00019122
Iteration 128/1000 | Loss: 0.00036470
Iteration 129/1000 | Loss: 0.00018953
Iteration 130/1000 | Loss: 0.00018561
Iteration 131/1000 | Loss: 0.00016088
Iteration 132/1000 | Loss: 0.00021443
Iteration 133/1000 | Loss: 0.00033793
Iteration 134/1000 | Loss: 0.00102118
Iteration 135/1000 | Loss: 0.00063245
Iteration 136/1000 | Loss: 0.00249656
Iteration 137/1000 | Loss: 0.00110171
Iteration 138/1000 | Loss: 0.00146618
Iteration 139/1000 | Loss: 0.00038065
Iteration 140/1000 | Loss: 0.00018634
Iteration 141/1000 | Loss: 0.00009965
Iteration 142/1000 | Loss: 0.00017053
Iteration 143/1000 | Loss: 0.00009186
Iteration 144/1000 | Loss: 0.00020299
Iteration 145/1000 | Loss: 0.00012642
Iteration 146/1000 | Loss: 0.00014711
Iteration 147/1000 | Loss: 0.00019829
Iteration 148/1000 | Loss: 0.00014725
Iteration 149/1000 | Loss: 0.00017718
Iteration 150/1000 | Loss: 0.00010104
Iteration 151/1000 | Loss: 0.00017635
Iteration 152/1000 | Loss: 0.00014501
Iteration 153/1000 | Loss: 0.00009814
Iteration 154/1000 | Loss: 0.00011184
Iteration 155/1000 | Loss: 0.00009933
Iteration 156/1000 | Loss: 0.00014971
Iteration 157/1000 | Loss: 0.00013686
Iteration 158/1000 | Loss: 0.00010861
Iteration 159/1000 | Loss: 0.00015430
Iteration 160/1000 | Loss: 0.00017193
Iteration 161/1000 | Loss: 0.00012801
Iteration 162/1000 | Loss: 0.00012745
Iteration 163/1000 | Loss: 0.00006008
Iteration 164/1000 | Loss: 0.00010450
Iteration 165/1000 | Loss: 0.00010146
Iteration 166/1000 | Loss: 0.00007406
Iteration 167/1000 | Loss: 0.00013710
Iteration 168/1000 | Loss: 0.00010407
Iteration 169/1000 | Loss: 0.00011225
Iteration 170/1000 | Loss: 0.00009689
Iteration 171/1000 | Loss: 0.00011975
Iteration 172/1000 | Loss: 0.00013096
Iteration 173/1000 | Loss: 0.00011934
Iteration 174/1000 | Loss: 0.00013795
Iteration 175/1000 | Loss: 0.00012873
Iteration 176/1000 | Loss: 0.00013767
Iteration 177/1000 | Loss: 0.00012129
Iteration 178/1000 | Loss: 0.00010864
Iteration 179/1000 | Loss: 0.00006170
Iteration 180/1000 | Loss: 0.00010230
Iteration 181/1000 | Loss: 0.00008856
Iteration 182/1000 | Loss: 0.00013813
Iteration 183/1000 | Loss: 0.00006841
Iteration 184/1000 | Loss: 0.00005896
Iteration 185/1000 | Loss: 0.00005723
Iteration 186/1000 | Loss: 0.00005603
Iteration 187/1000 | Loss: 0.00005504
Iteration 188/1000 | Loss: 0.00005447
Iteration 189/1000 | Loss: 0.00005406
Iteration 190/1000 | Loss: 0.00005372
Iteration 191/1000 | Loss: 0.00005342
Iteration 192/1000 | Loss: 0.00005311
Iteration 193/1000 | Loss: 0.00005281
Iteration 194/1000 | Loss: 0.00005254
Iteration 195/1000 | Loss: 0.00005234
Iteration 196/1000 | Loss: 0.00005232
Iteration 197/1000 | Loss: 0.00005224
Iteration 198/1000 | Loss: 0.00005220
Iteration 199/1000 | Loss: 0.00005218
Iteration 200/1000 | Loss: 0.00005216
Iteration 201/1000 | Loss: 0.00005216
Iteration 202/1000 | Loss: 0.00005216
Iteration 203/1000 | Loss: 0.00005216
Iteration 204/1000 | Loss: 0.00005215
Iteration 205/1000 | Loss: 0.00005214
Iteration 206/1000 | Loss: 0.00005213
Iteration 207/1000 | Loss: 0.00005213
Iteration 208/1000 | Loss: 0.00005213
Iteration 209/1000 | Loss: 0.00005212
Iteration 210/1000 | Loss: 0.00005212
Iteration 211/1000 | Loss: 0.00005211
Iteration 212/1000 | Loss: 0.00005211
Iteration 213/1000 | Loss: 0.00005210
Iteration 214/1000 | Loss: 0.00005210
Iteration 215/1000 | Loss: 0.00005210
Iteration 216/1000 | Loss: 0.00005210
Iteration 217/1000 | Loss: 0.00005210
Iteration 218/1000 | Loss: 0.00005210
Iteration 219/1000 | Loss: 0.00005210
Iteration 220/1000 | Loss: 0.00005210
Iteration 221/1000 | Loss: 0.00005209
Iteration 222/1000 | Loss: 0.00005209
Iteration 223/1000 | Loss: 0.00005209
Iteration 224/1000 | Loss: 0.00005209
Iteration 225/1000 | Loss: 0.00005209
Iteration 226/1000 | Loss: 0.00005209
Iteration 227/1000 | Loss: 0.00005209
Iteration 228/1000 | Loss: 0.00005208
Iteration 229/1000 | Loss: 0.00005208
Iteration 230/1000 | Loss: 0.00005208
Iteration 231/1000 | Loss: 0.00005208
Iteration 232/1000 | Loss: 0.00005208
Iteration 233/1000 | Loss: 0.00005208
Iteration 234/1000 | Loss: 0.00005208
Iteration 235/1000 | Loss: 0.00005208
Iteration 236/1000 | Loss: 0.00005207
Iteration 237/1000 | Loss: 0.00005207
Iteration 238/1000 | Loss: 0.00005207
Iteration 239/1000 | Loss: 0.00005207
Iteration 240/1000 | Loss: 0.00005207
Iteration 241/1000 | Loss: 0.00005207
Iteration 242/1000 | Loss: 0.00005207
Iteration 243/1000 | Loss: 0.00005207
Iteration 244/1000 | Loss: 0.00005207
Iteration 245/1000 | Loss: 0.00005207
Iteration 246/1000 | Loss: 0.00005207
Iteration 247/1000 | Loss: 0.00005207
Iteration 248/1000 | Loss: 0.00005206
Iteration 249/1000 | Loss: 0.00005206
Iteration 250/1000 | Loss: 0.00005206
Iteration 251/1000 | Loss: 0.00005206
Iteration 252/1000 | Loss: 0.00005206
Iteration 253/1000 | Loss: 0.00005206
Iteration 254/1000 | Loss: 0.00005206
Iteration 255/1000 | Loss: 0.00005206
Iteration 256/1000 | Loss: 0.00005206
Iteration 257/1000 | Loss: 0.00005206
Iteration 258/1000 | Loss: 0.00005206
Iteration 259/1000 | Loss: 0.00005206
Iteration 260/1000 | Loss: 0.00005206
Iteration 261/1000 | Loss: 0.00005206
Iteration 262/1000 | Loss: 0.00005206
Iteration 263/1000 | Loss: 0.00005206
Iteration 264/1000 | Loss: 0.00005206
Iteration 265/1000 | Loss: 0.00005206
Iteration 266/1000 | Loss: 0.00005206
Iteration 267/1000 | Loss: 0.00005206
Iteration 268/1000 | Loss: 0.00005205
Iteration 269/1000 | Loss: 0.00005205
Iteration 270/1000 | Loss: 0.00005205
Iteration 271/1000 | Loss: 0.00005205
Iteration 272/1000 | Loss: 0.00005205
Iteration 273/1000 | Loss: 0.00005205
Iteration 274/1000 | Loss: 0.00005205
Iteration 275/1000 | Loss: 0.00005205
Iteration 276/1000 | Loss: 0.00005205
Iteration 277/1000 | Loss: 0.00005205
Iteration 278/1000 | Loss: 0.00005205
Iteration 279/1000 | Loss: 0.00005205
Iteration 280/1000 | Loss: 0.00005205
Iteration 281/1000 | Loss: 0.00005205
Iteration 282/1000 | Loss: 0.00005205
Iteration 283/1000 | Loss: 0.00005205
Iteration 284/1000 | Loss: 0.00005205
Iteration 285/1000 | Loss: 0.00005205
Iteration 286/1000 | Loss: 0.00005205
Iteration 287/1000 | Loss: 0.00005205
Iteration 288/1000 | Loss: 0.00005205
Iteration 289/1000 | Loss: 0.00005205
Iteration 290/1000 | Loss: 0.00005205
Iteration 291/1000 | Loss: 0.00005205
Iteration 292/1000 | Loss: 0.00005205
Iteration 293/1000 | Loss: 0.00005205
Iteration 294/1000 | Loss: 0.00005205
Iteration 295/1000 | Loss: 0.00005205
Iteration 296/1000 | Loss: 0.00005205
Iteration 297/1000 | Loss: 0.00005205
Iteration 298/1000 | Loss: 0.00005205
Iteration 299/1000 | Loss: 0.00005205
Iteration 300/1000 | Loss: 0.00005205
Iteration 301/1000 | Loss: 0.00005205
Iteration 302/1000 | Loss: 0.00005205
Iteration 303/1000 | Loss: 0.00005205
Iteration 304/1000 | Loss: 0.00005205
Iteration 305/1000 | Loss: 0.00005205
Iteration 306/1000 | Loss: 0.00005205
Iteration 307/1000 | Loss: 0.00005205
Iteration 308/1000 | Loss: 0.00005205
Iteration 309/1000 | Loss: 0.00005205
Iteration 310/1000 | Loss: 0.00005205
Iteration 311/1000 | Loss: 0.00005205
Iteration 312/1000 | Loss: 0.00005205
Iteration 313/1000 | Loss: 0.00005205
Iteration 314/1000 | Loss: 0.00005205
Iteration 315/1000 | Loss: 0.00005205
Iteration 316/1000 | Loss: 0.00005205
Iteration 317/1000 | Loss: 0.00005205
Iteration 318/1000 | Loss: 0.00005205
Iteration 319/1000 | Loss: 0.00005205
Iteration 320/1000 | Loss: 0.00005205
Iteration 321/1000 | Loss: 0.00005205
Iteration 322/1000 | Loss: 0.00005205
Iteration 323/1000 | Loss: 0.00005205
Iteration 324/1000 | Loss: 0.00005205
Iteration 325/1000 | Loss: 0.00005205
Iteration 326/1000 | Loss: 0.00005205
Iteration 327/1000 | Loss: 0.00005205
Iteration 328/1000 | Loss: 0.00005205
Iteration 329/1000 | Loss: 0.00005205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 329. Stopping optimization.
Last 5 losses: [5.2045601478312165e-05, 5.2045601478312165e-05, 5.2045601478312165e-05, 5.2045601478312165e-05, 5.2045601478312165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.2045601478312165e-05

Optimization complete. Final v2v error: 5.880451202392578 mm

Highest mean error: 6.914135456085205 mm for frame 141

Lowest mean error: 4.9495625495910645 mm for frame 10

Saving results

Total time: 374.17571210861206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864227
Iteration 2/25 | Loss: 0.00195490
Iteration 3/25 | Loss: 0.00160045
Iteration 4/25 | Loss: 0.00155025
Iteration 5/25 | Loss: 0.00152651
Iteration 6/25 | Loss: 0.00153402
Iteration 7/25 | Loss: 0.00148320
Iteration 8/25 | Loss: 0.00147990
Iteration 9/25 | Loss: 0.00147877
Iteration 10/25 | Loss: 0.00148470
Iteration 11/25 | Loss: 0.00147855
Iteration 12/25 | Loss: 0.00148311
Iteration 13/25 | Loss: 0.00147797
Iteration 14/25 | Loss: 0.00147241
Iteration 15/25 | Loss: 0.00147369
Iteration 16/25 | Loss: 0.00147112
Iteration 17/25 | Loss: 0.00145794
Iteration 18/25 | Loss: 0.00145289
Iteration 19/25 | Loss: 0.00144216
Iteration 20/25 | Loss: 0.00143910
Iteration 21/25 | Loss: 0.00143852
Iteration 22/25 | Loss: 0.00143843
Iteration 23/25 | Loss: 0.00143843
Iteration 24/25 | Loss: 0.00143842
Iteration 25/25 | Loss: 0.00143842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.37940264
Iteration 2/25 | Loss: 0.00233940
Iteration 3/25 | Loss: 0.00233923
Iteration 4/25 | Loss: 0.00233923
Iteration 5/25 | Loss: 0.00233923
Iteration 6/25 | Loss: 0.00233923
Iteration 7/25 | Loss: 0.00233923
Iteration 8/25 | Loss: 0.00233922
Iteration 9/25 | Loss: 0.00233922
Iteration 10/25 | Loss: 0.00233922
Iteration 11/25 | Loss: 0.00233922
Iteration 12/25 | Loss: 0.00233922
Iteration 13/25 | Loss: 0.00233922
Iteration 14/25 | Loss: 0.00233922
Iteration 15/25 | Loss: 0.00233922
Iteration 16/25 | Loss: 0.00233922
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0023392236325889826, 0.0023392236325889826, 0.0023392236325889826, 0.0023392236325889826, 0.0023392236325889826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023392236325889826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233922
Iteration 2/1000 | Loss: 0.00015928
Iteration 3/1000 | Loss: 0.00009377
Iteration 4/1000 | Loss: 0.00007370
Iteration 5/1000 | Loss: 0.00006596
Iteration 6/1000 | Loss: 0.00140716
Iteration 7/1000 | Loss: 0.00070396
Iteration 8/1000 | Loss: 0.00121774
Iteration 9/1000 | Loss: 0.00104778
Iteration 10/1000 | Loss: 0.00009577
Iteration 11/1000 | Loss: 0.00036371
Iteration 12/1000 | Loss: 0.00007323
Iteration 13/1000 | Loss: 0.00005767
Iteration 14/1000 | Loss: 0.00005327
Iteration 15/1000 | Loss: 0.00005108
Iteration 16/1000 | Loss: 0.00004945
Iteration 17/1000 | Loss: 0.00004803
Iteration 18/1000 | Loss: 0.00004679
Iteration 19/1000 | Loss: 0.00004562
Iteration 20/1000 | Loss: 0.00004501
Iteration 21/1000 | Loss: 0.00004407
Iteration 22/1000 | Loss: 0.00004198
Iteration 23/1000 | Loss: 0.00004038
Iteration 24/1000 | Loss: 0.00003928
Iteration 25/1000 | Loss: 0.00003865
Iteration 26/1000 | Loss: 0.00003826
Iteration 27/1000 | Loss: 0.00003803
Iteration 28/1000 | Loss: 0.00003790
Iteration 29/1000 | Loss: 0.00003774
Iteration 30/1000 | Loss: 0.00003771
Iteration 31/1000 | Loss: 0.00003752
Iteration 32/1000 | Loss: 0.00003734
Iteration 33/1000 | Loss: 0.00003730
Iteration 34/1000 | Loss: 0.00003730
Iteration 35/1000 | Loss: 0.00003725
Iteration 36/1000 | Loss: 0.00003719
Iteration 37/1000 | Loss: 0.00003718
Iteration 38/1000 | Loss: 0.00003716
Iteration 39/1000 | Loss: 0.00003716
Iteration 40/1000 | Loss: 0.00003715
Iteration 41/1000 | Loss: 0.00003715
Iteration 42/1000 | Loss: 0.00003713
Iteration 43/1000 | Loss: 0.00003713
Iteration 44/1000 | Loss: 0.00003711
Iteration 45/1000 | Loss: 0.00003710
Iteration 46/1000 | Loss: 0.00003709
Iteration 47/1000 | Loss: 0.00003708
Iteration 48/1000 | Loss: 0.00003708
Iteration 49/1000 | Loss: 0.00003707
Iteration 50/1000 | Loss: 0.00003706
Iteration 51/1000 | Loss: 0.00003703
Iteration 52/1000 | Loss: 0.00003703
Iteration 53/1000 | Loss: 0.00003702
Iteration 54/1000 | Loss: 0.00003702
Iteration 55/1000 | Loss: 0.00003700
Iteration 56/1000 | Loss: 0.00003699
Iteration 57/1000 | Loss: 0.00003699
Iteration 58/1000 | Loss: 0.00003699
Iteration 59/1000 | Loss: 0.00003698
Iteration 60/1000 | Loss: 0.00003697
Iteration 61/1000 | Loss: 0.00003696
Iteration 62/1000 | Loss: 0.00003695
Iteration 63/1000 | Loss: 0.00003695
Iteration 64/1000 | Loss: 0.00003695
Iteration 65/1000 | Loss: 0.00003695
Iteration 66/1000 | Loss: 0.00003695
Iteration 67/1000 | Loss: 0.00003695
Iteration 68/1000 | Loss: 0.00003695
Iteration 69/1000 | Loss: 0.00003695
Iteration 70/1000 | Loss: 0.00003695
Iteration 71/1000 | Loss: 0.00003695
Iteration 72/1000 | Loss: 0.00003695
Iteration 73/1000 | Loss: 0.00003694
Iteration 74/1000 | Loss: 0.00003694
Iteration 75/1000 | Loss: 0.00003694
Iteration 76/1000 | Loss: 0.00003694
Iteration 77/1000 | Loss: 0.00003694
Iteration 78/1000 | Loss: 0.00003694
Iteration 79/1000 | Loss: 0.00003694
Iteration 80/1000 | Loss: 0.00003694
Iteration 81/1000 | Loss: 0.00003693
Iteration 82/1000 | Loss: 0.00003693
Iteration 83/1000 | Loss: 0.00003692
Iteration 84/1000 | Loss: 0.00003692
Iteration 85/1000 | Loss: 0.00003692
Iteration 86/1000 | Loss: 0.00003692
Iteration 87/1000 | Loss: 0.00003692
Iteration 88/1000 | Loss: 0.00003692
Iteration 89/1000 | Loss: 0.00003692
Iteration 90/1000 | Loss: 0.00003691
Iteration 91/1000 | Loss: 0.00003691
Iteration 92/1000 | Loss: 0.00003691
Iteration 93/1000 | Loss: 0.00003691
Iteration 94/1000 | Loss: 0.00003691
Iteration 95/1000 | Loss: 0.00003690
Iteration 96/1000 | Loss: 0.00003690
Iteration 97/1000 | Loss: 0.00003690
Iteration 98/1000 | Loss: 0.00003690
Iteration 99/1000 | Loss: 0.00003689
Iteration 100/1000 | Loss: 0.00003689
Iteration 101/1000 | Loss: 0.00003689
Iteration 102/1000 | Loss: 0.00003689
Iteration 103/1000 | Loss: 0.00003689
Iteration 104/1000 | Loss: 0.00003689
Iteration 105/1000 | Loss: 0.00003688
Iteration 106/1000 | Loss: 0.00003688
Iteration 107/1000 | Loss: 0.00003688
Iteration 108/1000 | Loss: 0.00003688
Iteration 109/1000 | Loss: 0.00003687
Iteration 110/1000 | Loss: 0.00003687
Iteration 111/1000 | Loss: 0.00003687
Iteration 112/1000 | Loss: 0.00003687
Iteration 113/1000 | Loss: 0.00003687
Iteration 114/1000 | Loss: 0.00003687
Iteration 115/1000 | Loss: 0.00003687
Iteration 116/1000 | Loss: 0.00003687
Iteration 117/1000 | Loss: 0.00003687
Iteration 118/1000 | Loss: 0.00003687
Iteration 119/1000 | Loss: 0.00003687
Iteration 120/1000 | Loss: 0.00003686
Iteration 121/1000 | Loss: 0.00003686
Iteration 122/1000 | Loss: 0.00003686
Iteration 123/1000 | Loss: 0.00003686
Iteration 124/1000 | Loss: 0.00003686
Iteration 125/1000 | Loss: 0.00003686
Iteration 126/1000 | Loss: 0.00003686
Iteration 127/1000 | Loss: 0.00003686
Iteration 128/1000 | Loss: 0.00003686
Iteration 129/1000 | Loss: 0.00003686
Iteration 130/1000 | Loss: 0.00003686
Iteration 131/1000 | Loss: 0.00003686
Iteration 132/1000 | Loss: 0.00003686
Iteration 133/1000 | Loss: 0.00003686
Iteration 134/1000 | Loss: 0.00003686
Iteration 135/1000 | Loss: 0.00003686
Iteration 136/1000 | Loss: 0.00003686
Iteration 137/1000 | Loss: 0.00003686
Iteration 138/1000 | Loss: 0.00003686
Iteration 139/1000 | Loss: 0.00003686
Iteration 140/1000 | Loss: 0.00003686
Iteration 141/1000 | Loss: 0.00003686
Iteration 142/1000 | Loss: 0.00003686
Iteration 143/1000 | Loss: 0.00003686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [3.686335185193457e-05, 3.686335185193457e-05, 3.686335185193457e-05, 3.686335185193457e-05, 3.686335185193457e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.686335185193457e-05

Optimization complete. Final v2v error: 4.852622032165527 mm

Highest mean error: 7.1785149574279785 mm for frame 70

Lowest mean error: 3.695924997329712 mm for frame 104

Saving results

Total time: 91.14089250564575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00579460
Iteration 2/25 | Loss: 0.00189930
Iteration 3/25 | Loss: 0.00147941
Iteration 4/25 | Loss: 0.00145387
Iteration 5/25 | Loss: 0.00144984
Iteration 6/25 | Loss: 0.00144903
Iteration 7/25 | Loss: 0.00144903
Iteration 8/25 | Loss: 0.00144903
Iteration 9/25 | Loss: 0.00144903
Iteration 10/25 | Loss: 0.00144903
Iteration 11/25 | Loss: 0.00144903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014490276807919145, 0.0014490276807919145, 0.0014490276807919145, 0.0014490276807919145, 0.0014490276807919145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014490276807919145

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98800957
Iteration 2/25 | Loss: 0.00147778
Iteration 3/25 | Loss: 0.00147777
Iteration 4/25 | Loss: 0.00147777
Iteration 5/25 | Loss: 0.00147777
Iteration 6/25 | Loss: 0.00147777
Iteration 7/25 | Loss: 0.00147777
Iteration 8/25 | Loss: 0.00147777
Iteration 9/25 | Loss: 0.00147777
Iteration 10/25 | Loss: 0.00147777
Iteration 11/25 | Loss: 0.00147777
Iteration 12/25 | Loss: 0.00147777
Iteration 13/25 | Loss: 0.00147777
Iteration 14/25 | Loss: 0.00147777
Iteration 15/25 | Loss: 0.00147777
Iteration 16/25 | Loss: 0.00147777
Iteration 17/25 | Loss: 0.00147777
Iteration 18/25 | Loss: 0.00147777
Iteration 19/25 | Loss: 0.00147777
Iteration 20/25 | Loss: 0.00147777
Iteration 21/25 | Loss: 0.00147777
Iteration 22/25 | Loss: 0.00147777
Iteration 23/25 | Loss: 0.00147777
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014777688775211573, 0.0014777688775211573, 0.0014777688775211573, 0.0014777688775211573, 0.0014777688775211573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014777688775211573

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147777
Iteration 2/1000 | Loss: 0.00010102
Iteration 3/1000 | Loss: 0.00007296
Iteration 4/1000 | Loss: 0.00006242
Iteration 5/1000 | Loss: 0.00005704
Iteration 6/1000 | Loss: 0.00005366
Iteration 7/1000 | Loss: 0.00005192
Iteration 8/1000 | Loss: 0.00005096
Iteration 9/1000 | Loss: 0.00005010
Iteration 10/1000 | Loss: 0.00004924
Iteration 11/1000 | Loss: 0.00004851
Iteration 12/1000 | Loss: 0.00004782
Iteration 13/1000 | Loss: 0.00004736
Iteration 14/1000 | Loss: 0.00004708
Iteration 15/1000 | Loss: 0.00004674
Iteration 16/1000 | Loss: 0.00004639
Iteration 17/1000 | Loss: 0.00004607
Iteration 18/1000 | Loss: 0.00004580
Iteration 19/1000 | Loss: 0.00004557
Iteration 20/1000 | Loss: 0.00004540
Iteration 21/1000 | Loss: 0.00004533
Iteration 22/1000 | Loss: 0.00004528
Iteration 23/1000 | Loss: 0.00004512
Iteration 24/1000 | Loss: 0.00004505
Iteration 25/1000 | Loss: 0.00004504
Iteration 26/1000 | Loss: 0.00004504
Iteration 27/1000 | Loss: 0.00004503
Iteration 28/1000 | Loss: 0.00004501
Iteration 29/1000 | Loss: 0.00004497
Iteration 30/1000 | Loss: 0.00004493
Iteration 31/1000 | Loss: 0.00004493
Iteration 32/1000 | Loss: 0.00004493
Iteration 33/1000 | Loss: 0.00004493
Iteration 34/1000 | Loss: 0.00004492
Iteration 35/1000 | Loss: 0.00004492
Iteration 36/1000 | Loss: 0.00004492
Iteration 37/1000 | Loss: 0.00004492
Iteration 38/1000 | Loss: 0.00004492
Iteration 39/1000 | Loss: 0.00004492
Iteration 40/1000 | Loss: 0.00004492
Iteration 41/1000 | Loss: 0.00004492
Iteration 42/1000 | Loss: 0.00004491
Iteration 43/1000 | Loss: 0.00004490
Iteration 44/1000 | Loss: 0.00004490
Iteration 45/1000 | Loss: 0.00004490
Iteration 46/1000 | Loss: 0.00004490
Iteration 47/1000 | Loss: 0.00004490
Iteration 48/1000 | Loss: 0.00004489
Iteration 49/1000 | Loss: 0.00004488
Iteration 50/1000 | Loss: 0.00004488
Iteration 51/1000 | Loss: 0.00004488
Iteration 52/1000 | Loss: 0.00004488
Iteration 53/1000 | Loss: 0.00004488
Iteration 54/1000 | Loss: 0.00004488
Iteration 55/1000 | Loss: 0.00004488
Iteration 56/1000 | Loss: 0.00004487
Iteration 57/1000 | Loss: 0.00004487
Iteration 58/1000 | Loss: 0.00004487
Iteration 59/1000 | Loss: 0.00004487
Iteration 60/1000 | Loss: 0.00004487
Iteration 61/1000 | Loss: 0.00004487
Iteration 62/1000 | Loss: 0.00004486
Iteration 63/1000 | Loss: 0.00004486
Iteration 64/1000 | Loss: 0.00004485
Iteration 65/1000 | Loss: 0.00004485
Iteration 66/1000 | Loss: 0.00004485
Iteration 67/1000 | Loss: 0.00004485
Iteration 68/1000 | Loss: 0.00004485
Iteration 69/1000 | Loss: 0.00004485
Iteration 70/1000 | Loss: 0.00004485
Iteration 71/1000 | Loss: 0.00004484
Iteration 72/1000 | Loss: 0.00004484
Iteration 73/1000 | Loss: 0.00004484
Iteration 74/1000 | Loss: 0.00004484
Iteration 75/1000 | Loss: 0.00004484
Iteration 76/1000 | Loss: 0.00004484
Iteration 77/1000 | Loss: 0.00004484
Iteration 78/1000 | Loss: 0.00004484
Iteration 79/1000 | Loss: 0.00004484
Iteration 80/1000 | Loss: 0.00004484
Iteration 81/1000 | Loss: 0.00004483
Iteration 82/1000 | Loss: 0.00004483
Iteration 83/1000 | Loss: 0.00004482
Iteration 84/1000 | Loss: 0.00004482
Iteration 85/1000 | Loss: 0.00004482
Iteration 86/1000 | Loss: 0.00004482
Iteration 87/1000 | Loss: 0.00004482
Iteration 88/1000 | Loss: 0.00004481
Iteration 89/1000 | Loss: 0.00004481
Iteration 90/1000 | Loss: 0.00004481
Iteration 91/1000 | Loss: 0.00004481
Iteration 92/1000 | Loss: 0.00004481
Iteration 93/1000 | Loss: 0.00004481
Iteration 94/1000 | Loss: 0.00004481
Iteration 95/1000 | Loss: 0.00004481
Iteration 96/1000 | Loss: 0.00004481
Iteration 97/1000 | Loss: 0.00004481
Iteration 98/1000 | Loss: 0.00004480
Iteration 99/1000 | Loss: 0.00004480
Iteration 100/1000 | Loss: 0.00004480
Iteration 101/1000 | Loss: 0.00004479
Iteration 102/1000 | Loss: 0.00004479
Iteration 103/1000 | Loss: 0.00004479
Iteration 104/1000 | Loss: 0.00004479
Iteration 105/1000 | Loss: 0.00004479
Iteration 106/1000 | Loss: 0.00004479
Iteration 107/1000 | Loss: 0.00004479
Iteration 108/1000 | Loss: 0.00004479
Iteration 109/1000 | Loss: 0.00004478
Iteration 110/1000 | Loss: 0.00004478
Iteration 111/1000 | Loss: 0.00004478
Iteration 112/1000 | Loss: 0.00004478
Iteration 113/1000 | Loss: 0.00004478
Iteration 114/1000 | Loss: 0.00004477
Iteration 115/1000 | Loss: 0.00004477
Iteration 116/1000 | Loss: 0.00004477
Iteration 117/1000 | Loss: 0.00004477
Iteration 118/1000 | Loss: 0.00004477
Iteration 119/1000 | Loss: 0.00004476
Iteration 120/1000 | Loss: 0.00004476
Iteration 121/1000 | Loss: 0.00004476
Iteration 122/1000 | Loss: 0.00004476
Iteration 123/1000 | Loss: 0.00004476
Iteration 124/1000 | Loss: 0.00004476
Iteration 125/1000 | Loss: 0.00004476
Iteration 126/1000 | Loss: 0.00004476
Iteration 127/1000 | Loss: 0.00004475
Iteration 128/1000 | Loss: 0.00004475
Iteration 129/1000 | Loss: 0.00004475
Iteration 130/1000 | Loss: 0.00004475
Iteration 131/1000 | Loss: 0.00004475
Iteration 132/1000 | Loss: 0.00004475
Iteration 133/1000 | Loss: 0.00004475
Iteration 134/1000 | Loss: 0.00004475
Iteration 135/1000 | Loss: 0.00004475
Iteration 136/1000 | Loss: 0.00004475
Iteration 137/1000 | Loss: 0.00004475
Iteration 138/1000 | Loss: 0.00004475
Iteration 139/1000 | Loss: 0.00004475
Iteration 140/1000 | Loss: 0.00004475
Iteration 141/1000 | Loss: 0.00004475
Iteration 142/1000 | Loss: 0.00004474
Iteration 143/1000 | Loss: 0.00004474
Iteration 144/1000 | Loss: 0.00004474
Iteration 145/1000 | Loss: 0.00004474
Iteration 146/1000 | Loss: 0.00004474
Iteration 147/1000 | Loss: 0.00004474
Iteration 148/1000 | Loss: 0.00004474
Iteration 149/1000 | Loss: 0.00004474
Iteration 150/1000 | Loss: 0.00004474
Iteration 151/1000 | Loss: 0.00004474
Iteration 152/1000 | Loss: 0.00004474
Iteration 153/1000 | Loss: 0.00004474
Iteration 154/1000 | Loss: 0.00004474
Iteration 155/1000 | Loss: 0.00004474
Iteration 156/1000 | Loss: 0.00004473
Iteration 157/1000 | Loss: 0.00004473
Iteration 158/1000 | Loss: 0.00004473
Iteration 159/1000 | Loss: 0.00004473
Iteration 160/1000 | Loss: 0.00004473
Iteration 161/1000 | Loss: 0.00004473
Iteration 162/1000 | Loss: 0.00004473
Iteration 163/1000 | Loss: 0.00004473
Iteration 164/1000 | Loss: 0.00004473
Iteration 165/1000 | Loss: 0.00004473
Iteration 166/1000 | Loss: 0.00004473
Iteration 167/1000 | Loss: 0.00004473
Iteration 168/1000 | Loss: 0.00004473
Iteration 169/1000 | Loss: 0.00004473
Iteration 170/1000 | Loss: 0.00004473
Iteration 171/1000 | Loss: 0.00004473
Iteration 172/1000 | Loss: 0.00004473
Iteration 173/1000 | Loss: 0.00004473
Iteration 174/1000 | Loss: 0.00004473
Iteration 175/1000 | Loss: 0.00004473
Iteration 176/1000 | Loss: 0.00004473
Iteration 177/1000 | Loss: 0.00004473
Iteration 178/1000 | Loss: 0.00004473
Iteration 179/1000 | Loss: 0.00004473
Iteration 180/1000 | Loss: 0.00004473
Iteration 181/1000 | Loss: 0.00004473
Iteration 182/1000 | Loss: 0.00004473
Iteration 183/1000 | Loss: 0.00004473
Iteration 184/1000 | Loss: 0.00004473
Iteration 185/1000 | Loss: 0.00004473
Iteration 186/1000 | Loss: 0.00004473
Iteration 187/1000 | Loss: 0.00004473
Iteration 188/1000 | Loss: 0.00004473
Iteration 189/1000 | Loss: 0.00004473
Iteration 190/1000 | Loss: 0.00004473
Iteration 191/1000 | Loss: 0.00004473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [4.472978980629705e-05, 4.472978980629705e-05, 4.472978980629705e-05, 4.472978980629705e-05, 4.472978980629705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.472978980629705e-05

Optimization complete. Final v2v error: 5.1141157150268555 mm

Highest mean error: 6.53040885925293 mm for frame 92

Lowest mean error: 3.7613637447357178 mm for frame 133

Saving results

Total time: 52.828351736068726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868534
Iteration 2/25 | Loss: 0.00148009
Iteration 3/25 | Loss: 0.00137443
Iteration 4/25 | Loss: 0.00135550
Iteration 5/25 | Loss: 0.00134847
Iteration 6/25 | Loss: 0.00134732
Iteration 7/25 | Loss: 0.00134732
Iteration 8/25 | Loss: 0.00134732
Iteration 9/25 | Loss: 0.00134732
Iteration 10/25 | Loss: 0.00134732
Iteration 11/25 | Loss: 0.00134732
Iteration 12/25 | Loss: 0.00134732
Iteration 13/25 | Loss: 0.00134732
Iteration 14/25 | Loss: 0.00134732
Iteration 15/25 | Loss: 0.00134732
Iteration 16/25 | Loss: 0.00134732
Iteration 17/25 | Loss: 0.00134732
Iteration 18/25 | Loss: 0.00134732
Iteration 19/25 | Loss: 0.00134732
Iteration 20/25 | Loss: 0.00134732
Iteration 21/25 | Loss: 0.00134732
Iteration 22/25 | Loss: 0.00134732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013473223662003875, 0.0013473223662003875, 0.0013473223662003875, 0.0013473223662003875, 0.0013473223662003875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013473223662003875

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32269871
Iteration 2/25 | Loss: 0.00191901
Iteration 3/25 | Loss: 0.00191901
Iteration 4/25 | Loss: 0.00191901
Iteration 5/25 | Loss: 0.00191901
Iteration 6/25 | Loss: 0.00191901
Iteration 7/25 | Loss: 0.00191901
Iteration 8/25 | Loss: 0.00191901
Iteration 9/25 | Loss: 0.00191901
Iteration 10/25 | Loss: 0.00191901
Iteration 11/25 | Loss: 0.00191901
Iteration 12/25 | Loss: 0.00191901
Iteration 13/25 | Loss: 0.00191901
Iteration 14/25 | Loss: 0.00191901
Iteration 15/25 | Loss: 0.00191901
Iteration 16/25 | Loss: 0.00191901
Iteration 17/25 | Loss: 0.00191901
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019190082093700767, 0.0019190082093700767, 0.0019190082093700767, 0.0019190082093700767, 0.0019190082093700767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019190082093700767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191901
Iteration 2/1000 | Loss: 0.00004859
Iteration 3/1000 | Loss: 0.00003449
Iteration 4/1000 | Loss: 0.00002947
Iteration 5/1000 | Loss: 0.00002675
Iteration 6/1000 | Loss: 0.00002480
Iteration 7/1000 | Loss: 0.00002397
Iteration 8/1000 | Loss: 0.00002324
Iteration 9/1000 | Loss: 0.00002273
Iteration 10/1000 | Loss: 0.00002226
Iteration 11/1000 | Loss: 0.00002192
Iteration 12/1000 | Loss: 0.00002169
Iteration 13/1000 | Loss: 0.00002160
Iteration 14/1000 | Loss: 0.00002144
Iteration 15/1000 | Loss: 0.00002141
Iteration 16/1000 | Loss: 0.00002140
Iteration 17/1000 | Loss: 0.00002134
Iteration 18/1000 | Loss: 0.00002134
Iteration 19/1000 | Loss: 0.00002134
Iteration 20/1000 | Loss: 0.00002133
Iteration 21/1000 | Loss: 0.00002133
Iteration 22/1000 | Loss: 0.00002133
Iteration 23/1000 | Loss: 0.00002131
Iteration 24/1000 | Loss: 0.00002131
Iteration 25/1000 | Loss: 0.00002130
Iteration 26/1000 | Loss: 0.00002130
Iteration 27/1000 | Loss: 0.00002129
Iteration 28/1000 | Loss: 0.00002128
Iteration 29/1000 | Loss: 0.00002128
Iteration 30/1000 | Loss: 0.00002127
Iteration 31/1000 | Loss: 0.00002127
Iteration 32/1000 | Loss: 0.00002125
Iteration 33/1000 | Loss: 0.00002124
Iteration 34/1000 | Loss: 0.00002124
Iteration 35/1000 | Loss: 0.00002124
Iteration 36/1000 | Loss: 0.00002124
Iteration 37/1000 | Loss: 0.00002124
Iteration 38/1000 | Loss: 0.00002123
Iteration 39/1000 | Loss: 0.00002123
Iteration 40/1000 | Loss: 0.00002122
Iteration 41/1000 | Loss: 0.00002122
Iteration 42/1000 | Loss: 0.00002122
Iteration 43/1000 | Loss: 0.00002122
Iteration 44/1000 | Loss: 0.00002121
Iteration 45/1000 | Loss: 0.00002121
Iteration 46/1000 | Loss: 0.00002120
Iteration 47/1000 | Loss: 0.00002120
Iteration 48/1000 | Loss: 0.00002120
Iteration 49/1000 | Loss: 0.00002119
Iteration 50/1000 | Loss: 0.00002119
Iteration 51/1000 | Loss: 0.00002119
Iteration 52/1000 | Loss: 0.00002119
Iteration 53/1000 | Loss: 0.00002118
Iteration 54/1000 | Loss: 0.00002118
Iteration 55/1000 | Loss: 0.00002118
Iteration 56/1000 | Loss: 0.00002118
Iteration 57/1000 | Loss: 0.00002117
Iteration 58/1000 | Loss: 0.00002117
Iteration 59/1000 | Loss: 0.00002116
Iteration 60/1000 | Loss: 0.00002116
Iteration 61/1000 | Loss: 0.00002116
Iteration 62/1000 | Loss: 0.00002116
Iteration 63/1000 | Loss: 0.00002115
Iteration 64/1000 | Loss: 0.00002115
Iteration 65/1000 | Loss: 0.00002115
Iteration 66/1000 | Loss: 0.00002114
Iteration 67/1000 | Loss: 0.00002114
Iteration 68/1000 | Loss: 0.00002114
Iteration 69/1000 | Loss: 0.00002114
Iteration 70/1000 | Loss: 0.00002114
Iteration 71/1000 | Loss: 0.00002114
Iteration 72/1000 | Loss: 0.00002114
Iteration 73/1000 | Loss: 0.00002114
Iteration 74/1000 | Loss: 0.00002114
Iteration 75/1000 | Loss: 0.00002114
Iteration 76/1000 | Loss: 0.00002114
Iteration 77/1000 | Loss: 0.00002114
Iteration 78/1000 | Loss: 0.00002113
Iteration 79/1000 | Loss: 0.00002113
Iteration 80/1000 | Loss: 0.00002113
Iteration 81/1000 | Loss: 0.00002112
Iteration 82/1000 | Loss: 0.00002112
Iteration 83/1000 | Loss: 0.00002112
Iteration 84/1000 | Loss: 0.00002112
Iteration 85/1000 | Loss: 0.00002112
Iteration 86/1000 | Loss: 0.00002112
Iteration 87/1000 | Loss: 0.00002111
Iteration 88/1000 | Loss: 0.00002111
Iteration 89/1000 | Loss: 0.00002111
Iteration 90/1000 | Loss: 0.00002111
Iteration 91/1000 | Loss: 0.00002111
Iteration 92/1000 | Loss: 0.00002111
Iteration 93/1000 | Loss: 0.00002111
Iteration 94/1000 | Loss: 0.00002111
Iteration 95/1000 | Loss: 0.00002111
Iteration 96/1000 | Loss: 0.00002111
Iteration 97/1000 | Loss: 0.00002111
Iteration 98/1000 | Loss: 0.00002111
Iteration 99/1000 | Loss: 0.00002111
Iteration 100/1000 | Loss: 0.00002111
Iteration 101/1000 | Loss: 0.00002111
Iteration 102/1000 | Loss: 0.00002111
Iteration 103/1000 | Loss: 0.00002111
Iteration 104/1000 | Loss: 0.00002111
Iteration 105/1000 | Loss: 0.00002110
Iteration 106/1000 | Loss: 0.00002110
Iteration 107/1000 | Loss: 0.00002110
Iteration 108/1000 | Loss: 0.00002110
Iteration 109/1000 | Loss: 0.00002110
Iteration 110/1000 | Loss: 0.00002110
Iteration 111/1000 | Loss: 0.00002110
Iteration 112/1000 | Loss: 0.00002110
Iteration 113/1000 | Loss: 0.00002110
Iteration 114/1000 | Loss: 0.00002110
Iteration 115/1000 | Loss: 0.00002109
Iteration 116/1000 | Loss: 0.00002109
Iteration 117/1000 | Loss: 0.00002109
Iteration 118/1000 | Loss: 0.00002109
Iteration 119/1000 | Loss: 0.00002109
Iteration 120/1000 | Loss: 0.00002109
Iteration 121/1000 | Loss: 0.00002109
Iteration 122/1000 | Loss: 0.00002109
Iteration 123/1000 | Loss: 0.00002109
Iteration 124/1000 | Loss: 0.00002109
Iteration 125/1000 | Loss: 0.00002108
Iteration 126/1000 | Loss: 0.00002108
Iteration 127/1000 | Loss: 0.00002108
Iteration 128/1000 | Loss: 0.00002108
Iteration 129/1000 | Loss: 0.00002108
Iteration 130/1000 | Loss: 0.00002108
Iteration 131/1000 | Loss: 0.00002108
Iteration 132/1000 | Loss: 0.00002108
Iteration 133/1000 | Loss: 0.00002108
Iteration 134/1000 | Loss: 0.00002108
Iteration 135/1000 | Loss: 0.00002108
Iteration 136/1000 | Loss: 0.00002108
Iteration 137/1000 | Loss: 0.00002108
Iteration 138/1000 | Loss: 0.00002108
Iteration 139/1000 | Loss: 0.00002108
Iteration 140/1000 | Loss: 0.00002108
Iteration 141/1000 | Loss: 0.00002108
Iteration 142/1000 | Loss: 0.00002108
Iteration 143/1000 | Loss: 0.00002108
Iteration 144/1000 | Loss: 0.00002107
Iteration 145/1000 | Loss: 0.00002107
Iteration 146/1000 | Loss: 0.00002107
Iteration 147/1000 | Loss: 0.00002107
Iteration 148/1000 | Loss: 0.00002107
Iteration 149/1000 | Loss: 0.00002107
Iteration 150/1000 | Loss: 0.00002107
Iteration 151/1000 | Loss: 0.00002107
Iteration 152/1000 | Loss: 0.00002107
Iteration 153/1000 | Loss: 0.00002107
Iteration 154/1000 | Loss: 0.00002107
Iteration 155/1000 | Loss: 0.00002107
Iteration 156/1000 | Loss: 0.00002107
Iteration 157/1000 | Loss: 0.00002106
Iteration 158/1000 | Loss: 0.00002106
Iteration 159/1000 | Loss: 0.00002106
Iteration 160/1000 | Loss: 0.00002106
Iteration 161/1000 | Loss: 0.00002106
Iteration 162/1000 | Loss: 0.00002106
Iteration 163/1000 | Loss: 0.00002106
Iteration 164/1000 | Loss: 0.00002106
Iteration 165/1000 | Loss: 0.00002106
Iteration 166/1000 | Loss: 0.00002106
Iteration 167/1000 | Loss: 0.00002106
Iteration 168/1000 | Loss: 0.00002106
Iteration 169/1000 | Loss: 0.00002106
Iteration 170/1000 | Loss: 0.00002106
Iteration 171/1000 | Loss: 0.00002106
Iteration 172/1000 | Loss: 0.00002106
Iteration 173/1000 | Loss: 0.00002106
Iteration 174/1000 | Loss: 0.00002106
Iteration 175/1000 | Loss: 0.00002106
Iteration 176/1000 | Loss: 0.00002106
Iteration 177/1000 | Loss: 0.00002106
Iteration 178/1000 | Loss: 0.00002106
Iteration 179/1000 | Loss: 0.00002106
Iteration 180/1000 | Loss: 0.00002106
Iteration 181/1000 | Loss: 0.00002106
Iteration 182/1000 | Loss: 0.00002106
Iteration 183/1000 | Loss: 0.00002106
Iteration 184/1000 | Loss: 0.00002106
Iteration 185/1000 | Loss: 0.00002106
Iteration 186/1000 | Loss: 0.00002106
Iteration 187/1000 | Loss: 0.00002106
Iteration 188/1000 | Loss: 0.00002106
Iteration 189/1000 | Loss: 0.00002106
Iteration 190/1000 | Loss: 0.00002106
Iteration 191/1000 | Loss: 0.00002106
Iteration 192/1000 | Loss: 0.00002106
Iteration 193/1000 | Loss: 0.00002106
Iteration 194/1000 | Loss: 0.00002106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [2.105877865687944e-05, 2.105877865687944e-05, 2.105877865687944e-05, 2.105877865687944e-05, 2.105877865687944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.105877865687944e-05

Optimization complete. Final v2v error: 3.8472111225128174 mm

Highest mean error: 4.410523414611816 mm for frame 184

Lowest mean error: 3.572711944580078 mm for frame 43

Saving results

Total time: 44.2025249004364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_nl_5778/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_nl_5778/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103232
Iteration 2/25 | Loss: 0.00244189
Iteration 3/25 | Loss: 0.00172604
Iteration 4/25 | Loss: 0.00163352
Iteration 5/25 | Loss: 0.00183787
Iteration 6/25 | Loss: 0.00168193
Iteration 7/25 | Loss: 0.00153534
Iteration 8/25 | Loss: 0.00147262
Iteration 9/25 | Loss: 0.00144059
Iteration 10/25 | Loss: 0.00141466
Iteration 11/25 | Loss: 0.00140663
Iteration 12/25 | Loss: 0.00138940
Iteration 13/25 | Loss: 0.00138418
Iteration 14/25 | Loss: 0.00137249
Iteration 15/25 | Loss: 0.00136481
Iteration 16/25 | Loss: 0.00137421
Iteration 17/25 | Loss: 0.00136630
Iteration 18/25 | Loss: 0.00135729
Iteration 19/25 | Loss: 0.00136151
Iteration 20/25 | Loss: 0.00135727
Iteration 21/25 | Loss: 0.00136058
Iteration 22/25 | Loss: 0.00135529
Iteration 23/25 | Loss: 0.00135090
Iteration 24/25 | Loss: 0.00134952
Iteration 25/25 | Loss: 0.00134483

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.92315149
Iteration 2/25 | Loss: 0.00210775
Iteration 3/25 | Loss: 0.00210775
Iteration 4/25 | Loss: 0.00210775
Iteration 5/25 | Loss: 0.00210775
Iteration 6/25 | Loss: 0.00210775
Iteration 7/25 | Loss: 0.00210775
Iteration 8/25 | Loss: 0.00210775
Iteration 9/25 | Loss: 0.00210775
Iteration 10/25 | Loss: 0.00210775
Iteration 11/25 | Loss: 0.00210775
Iteration 12/25 | Loss: 0.00210775
Iteration 13/25 | Loss: 0.00210775
Iteration 14/25 | Loss: 0.00210775
Iteration 15/25 | Loss: 0.00210775
Iteration 16/25 | Loss: 0.00210775
Iteration 17/25 | Loss: 0.00210775
Iteration 18/25 | Loss: 0.00210775
Iteration 19/25 | Loss: 0.00210775
Iteration 20/25 | Loss: 0.00210775
Iteration 21/25 | Loss: 0.00210775
Iteration 22/25 | Loss: 0.00210775
Iteration 23/25 | Loss: 0.00210775
Iteration 24/25 | Loss: 0.00210775
Iteration 25/25 | Loss: 0.00210775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210775
Iteration 2/1000 | Loss: 0.00023738
Iteration 3/1000 | Loss: 0.00027954
Iteration 4/1000 | Loss: 0.00041965
Iteration 5/1000 | Loss: 0.00026703
Iteration 6/1000 | Loss: 0.00035574
Iteration 7/1000 | Loss: 0.00033962
Iteration 8/1000 | Loss: 0.00052195
Iteration 9/1000 | Loss: 0.00077467
Iteration 10/1000 | Loss: 0.00087999
Iteration 11/1000 | Loss: 0.00032666
Iteration 12/1000 | Loss: 0.00025910
Iteration 13/1000 | Loss: 0.00037520
Iteration 14/1000 | Loss: 0.00018863
Iteration 15/1000 | Loss: 0.00051779
Iteration 16/1000 | Loss: 0.00029137
Iteration 17/1000 | Loss: 0.00019868
Iteration 18/1000 | Loss: 0.00015306
Iteration 19/1000 | Loss: 0.00018814
Iteration 20/1000 | Loss: 0.00028999
Iteration 21/1000 | Loss: 0.00016557
Iteration 22/1000 | Loss: 0.00042444
Iteration 23/1000 | Loss: 0.00039133
Iteration 24/1000 | Loss: 0.00033306
Iteration 25/1000 | Loss: 0.00026665
Iteration 26/1000 | Loss: 0.00033217
Iteration 27/1000 | Loss: 0.00040249
Iteration 28/1000 | Loss: 0.00036935
Iteration 29/1000 | Loss: 0.00048564
Iteration 30/1000 | Loss: 0.00043781
Iteration 31/1000 | Loss: 0.00040986
Iteration 32/1000 | Loss: 0.00044985
Iteration 33/1000 | Loss: 0.00041775
Iteration 34/1000 | Loss: 0.00030780
Iteration 35/1000 | Loss: 0.00022263
Iteration 36/1000 | Loss: 0.00037785
Iteration 37/1000 | Loss: 0.00047113
Iteration 38/1000 | Loss: 0.00051997
Iteration 39/1000 | Loss: 0.00049740
Iteration 40/1000 | Loss: 0.00042029
Iteration 41/1000 | Loss: 0.00038945
Iteration 42/1000 | Loss: 0.00022890
Iteration 43/1000 | Loss: 0.00039026
Iteration 44/1000 | Loss: 0.00025469
Iteration 45/1000 | Loss: 0.00040408
Iteration 46/1000 | Loss: 0.00065924
Iteration 47/1000 | Loss: 0.00027851
Iteration 48/1000 | Loss: 0.00052902
Iteration 49/1000 | Loss: 0.00037422
Iteration 50/1000 | Loss: 0.00055006
Iteration 51/1000 | Loss: 0.00021151
Iteration 52/1000 | Loss: 0.00018620
Iteration 53/1000 | Loss: 0.00019175
Iteration 54/1000 | Loss: 0.00017412
Iteration 55/1000 | Loss: 0.00019782
Iteration 56/1000 | Loss: 0.00040567
Iteration 57/1000 | Loss: 0.00039123
Iteration 58/1000 | Loss: 0.00043469
Iteration 59/1000 | Loss: 0.00040893
Iteration 60/1000 | Loss: 0.00024177
Iteration 61/1000 | Loss: 0.00056013
Iteration 62/1000 | Loss: 0.00051623
Iteration 63/1000 | Loss: 0.00070960
Iteration 64/1000 | Loss: 0.00042481
Iteration 65/1000 | Loss: 0.00019000
Iteration 66/1000 | Loss: 0.00030337
Iteration 67/1000 | Loss: 0.00049162
Iteration 68/1000 | Loss: 0.00042386
Iteration 69/1000 | Loss: 0.00034486
Iteration 70/1000 | Loss: 0.00006945
Iteration 71/1000 | Loss: 0.00031517
Iteration 72/1000 | Loss: 0.00029903
Iteration 73/1000 | Loss: 0.00040160
Iteration 74/1000 | Loss: 0.00040057
Iteration 75/1000 | Loss: 0.00038424
Iteration 76/1000 | Loss: 0.00041421
Iteration 77/1000 | Loss: 0.00019215
Iteration 78/1000 | Loss: 0.00026315
Iteration 79/1000 | Loss: 0.00009030
Iteration 80/1000 | Loss: 0.00008440
Iteration 81/1000 | Loss: 0.00014692
Iteration 82/1000 | Loss: 0.00014056
Iteration 83/1000 | Loss: 0.00014874
Iteration 84/1000 | Loss: 0.00018060
Iteration 85/1000 | Loss: 0.00010761
Iteration 86/1000 | Loss: 0.00010799
Iteration 87/1000 | Loss: 0.00062641
Iteration 88/1000 | Loss: 0.00052311
Iteration 89/1000 | Loss: 0.00040634
Iteration 90/1000 | Loss: 0.00025459
Iteration 91/1000 | Loss: 0.00031579
Iteration 92/1000 | Loss: 0.00029723
Iteration 93/1000 | Loss: 0.00019896
Iteration 94/1000 | Loss: 0.00019808
Iteration 95/1000 | Loss: 0.00005927
Iteration 96/1000 | Loss: 0.00007377
Iteration 97/1000 | Loss: 0.00005530
Iteration 98/1000 | Loss: 0.00024943
Iteration 99/1000 | Loss: 0.00032612
Iteration 100/1000 | Loss: 0.00022516
Iteration 101/1000 | Loss: 0.00019120
Iteration 102/1000 | Loss: 0.00018771
Iteration 103/1000 | Loss: 0.00021627
Iteration 104/1000 | Loss: 0.00021719
Iteration 105/1000 | Loss: 0.00061786
Iteration 106/1000 | Loss: 0.00019422
Iteration 107/1000 | Loss: 0.00018414
Iteration 108/1000 | Loss: 0.00036193
Iteration 109/1000 | Loss: 0.00019283
Iteration 110/1000 | Loss: 0.00005185
Iteration 111/1000 | Loss: 0.00007439
Iteration 112/1000 | Loss: 0.00003989
Iteration 113/1000 | Loss: 0.00006403
Iteration 114/1000 | Loss: 0.00005694
Iteration 115/1000 | Loss: 0.00005651
Iteration 116/1000 | Loss: 0.00005680
Iteration 117/1000 | Loss: 0.00003287
Iteration 118/1000 | Loss: 0.00004158
Iteration 119/1000 | Loss: 0.00003854
Iteration 120/1000 | Loss: 0.00003797
Iteration 121/1000 | Loss: 0.00003586
Iteration 122/1000 | Loss: 0.00003560
Iteration 123/1000 | Loss: 0.00003214
Iteration 124/1000 | Loss: 0.00004207
Iteration 125/1000 | Loss: 0.00003911
Iteration 126/1000 | Loss: 0.00003264
Iteration 127/1000 | Loss: 0.00003166
Iteration 128/1000 | Loss: 0.00003892
Iteration 129/1000 | Loss: 0.00003899
Iteration 130/1000 | Loss: 0.00003958
Iteration 131/1000 | Loss: 0.00003886
Iteration 132/1000 | Loss: 0.00003953
Iteration 133/1000 | Loss: 0.00003819
Iteration 134/1000 | Loss: 0.00006034
Iteration 135/1000 | Loss: 0.00003194
Iteration 136/1000 | Loss: 0.00003859
Iteration 137/1000 | Loss: 0.00003863
Iteration 138/1000 | Loss: 0.00003896
Iteration 139/1000 | Loss: 0.00003846
Iteration 140/1000 | Loss: 0.00003256
Iteration 141/1000 | Loss: 0.00003585
Iteration 142/1000 | Loss: 0.00003036
Iteration 143/1000 | Loss: 0.00004220
Iteration 144/1000 | Loss: 0.00003994
Iteration 145/1000 | Loss: 0.00004595
Iteration 146/1000 | Loss: 0.00004013
Iteration 147/1000 | Loss: 0.00003935
Iteration 148/1000 | Loss: 0.00003774
Iteration 149/1000 | Loss: 0.00003922
Iteration 150/1000 | Loss: 0.00004320
Iteration 151/1000 | Loss: 0.00003782
Iteration 152/1000 | Loss: 0.00003998
Iteration 153/1000 | Loss: 0.00003825
Iteration 154/1000 | Loss: 0.00003977
Iteration 155/1000 | Loss: 0.00004148
Iteration 156/1000 | Loss: 0.00004957
Iteration 157/1000 | Loss: 0.00005112
Iteration 158/1000 | Loss: 0.00004123
Iteration 159/1000 | Loss: 0.00004263
Iteration 160/1000 | Loss: 0.00004334
Iteration 161/1000 | Loss: 0.00004166
Iteration 162/1000 | Loss: 0.00003994
Iteration 163/1000 | Loss: 0.00003876
Iteration 164/1000 | Loss: 0.00004359
Iteration 165/1000 | Loss: 0.00005833
Iteration 166/1000 | Loss: 0.00004452
Iteration 167/1000 | Loss: 0.00004484
Iteration 168/1000 | Loss: 0.00003914
Iteration 169/1000 | Loss: 0.00004390
Iteration 170/1000 | Loss: 0.00004209
Iteration 171/1000 | Loss: 0.00003614
Iteration 172/1000 | Loss: 0.00004631
Iteration 173/1000 | Loss: 0.00003693
Iteration 174/1000 | Loss: 0.00005357
Iteration 175/1000 | Loss: 0.00004022
Iteration 176/1000 | Loss: 0.00004062
Iteration 177/1000 | Loss: 0.00003767
Iteration 178/1000 | Loss: 0.00003946
Iteration 179/1000 | Loss: 0.00003715
Iteration 180/1000 | Loss: 0.00003932
Iteration 181/1000 | Loss: 0.00003863
Iteration 182/1000 | Loss: 0.00003858
Iteration 183/1000 | Loss: 0.00003711
Iteration 184/1000 | Loss: 0.00003826
Iteration 185/1000 | Loss: 0.00003896
Iteration 186/1000 | Loss: 0.00004104
Iteration 187/1000 | Loss: 0.00003418
Iteration 188/1000 | Loss: 0.00002579
Iteration 189/1000 | Loss: 0.00004933
Iteration 190/1000 | Loss: 0.00003437
Iteration 191/1000 | Loss: 0.00006554
Iteration 192/1000 | Loss: 0.00003957
Iteration 193/1000 | Loss: 0.00003674
Iteration 194/1000 | Loss: 0.00003693
Iteration 195/1000 | Loss: 0.00006056
Iteration 196/1000 | Loss: 0.00003832
Iteration 197/1000 | Loss: 0.00003584
Iteration 198/1000 | Loss: 0.00003790
Iteration 199/1000 | Loss: 0.00003671
Iteration 200/1000 | Loss: 0.00004640
Iteration 201/1000 | Loss: 0.00003778
Iteration 202/1000 | Loss: 0.00003904
Iteration 203/1000 | Loss: 0.00003455
Iteration 204/1000 | Loss: 0.00004019
Iteration 205/1000 | Loss: 0.00003748
Iteration 206/1000 | Loss: 0.00003911
Iteration 207/1000 | Loss: 0.00003713
Iteration 208/1000 | Loss: 0.00004008
Iteration 209/1000 | Loss: 0.00003313
Iteration 210/1000 | Loss: 0.00004405
Iteration 211/1000 | Loss: 0.00004032
Iteration 212/1000 | Loss: 0.00003718
Iteration 213/1000 | Loss: 0.00004211
Iteration 214/1000 | Loss: 0.00004215
Iteration 215/1000 | Loss: 0.00003792
Iteration 216/1000 | Loss: 0.00004609
Iteration 217/1000 | Loss: 0.00003966
Iteration 218/1000 | Loss: 0.00003510
Iteration 219/1000 | Loss: 0.00002704
Iteration 220/1000 | Loss: 0.00002616
Iteration 221/1000 | Loss: 0.00004742
Iteration 222/1000 | Loss: 0.00004648
Iteration 223/1000 | Loss: 0.00003820
Iteration 224/1000 | Loss: 0.00003582
Iteration 225/1000 | Loss: 0.00003640
Iteration 226/1000 | Loss: 0.00003445
Iteration 227/1000 | Loss: 0.00003685
Iteration 228/1000 | Loss: 0.00003981
Iteration 229/1000 | Loss: 0.00004238
Iteration 230/1000 | Loss: 0.00003703
Iteration 231/1000 | Loss: 0.00003407
Iteration 232/1000 | Loss: 0.00004103
Iteration 233/1000 | Loss: 0.00004073
Iteration 234/1000 | Loss: 0.00003458
Iteration 235/1000 | Loss: 0.00004056
Iteration 236/1000 | Loss: 0.00003505
Iteration 237/1000 | Loss: 0.00003480
Iteration 238/1000 | Loss: 0.00003651
Iteration 239/1000 | Loss: 0.00003560
Iteration 240/1000 | Loss: 0.00003609
Iteration 241/1000 | Loss: 0.00003703
Iteration 242/1000 | Loss: 0.00003955
Iteration 243/1000 | Loss: 0.00002445
Iteration 244/1000 | Loss: 0.00002368
Iteration 245/1000 | Loss: 0.00002300
Iteration 246/1000 | Loss: 0.00002262
Iteration 247/1000 | Loss: 0.00002226
Iteration 248/1000 | Loss: 0.00002186
Iteration 249/1000 | Loss: 0.00002162
Iteration 250/1000 | Loss: 0.00002159
Iteration 251/1000 | Loss: 0.00002157
Iteration 252/1000 | Loss: 0.00002157
Iteration 253/1000 | Loss: 0.00002157
Iteration 254/1000 | Loss: 0.00002157
Iteration 255/1000 | Loss: 0.00002157
Iteration 256/1000 | Loss: 0.00002156
Iteration 257/1000 | Loss: 0.00002156
Iteration 258/1000 | Loss: 0.00002155
Iteration 259/1000 | Loss: 0.00002154
Iteration 260/1000 | Loss: 0.00002154
Iteration 261/1000 | Loss: 0.00002154
Iteration 262/1000 | Loss: 0.00002154
Iteration 263/1000 | Loss: 0.00002153
Iteration 264/1000 | Loss: 0.00002153
Iteration 265/1000 | Loss: 0.00002153
Iteration 266/1000 | Loss: 0.00002153
Iteration 267/1000 | Loss: 0.00002153
Iteration 268/1000 | Loss: 0.00002153
Iteration 269/1000 | Loss: 0.00002153
Iteration 270/1000 | Loss: 0.00002153
Iteration 271/1000 | Loss: 0.00002153
Iteration 272/1000 | Loss: 0.00002153
Iteration 273/1000 | Loss: 0.00002153
Iteration 274/1000 | Loss: 0.00002153
Iteration 275/1000 | Loss: 0.00002153
Iteration 276/1000 | Loss: 0.00002153
Iteration 277/1000 | Loss: 0.00002153
Iteration 278/1000 | Loss: 0.00002153
Iteration 279/1000 | Loss: 0.00002153
Iteration 280/1000 | Loss: 0.00002152
Iteration 281/1000 | Loss: 0.00002152
Iteration 282/1000 | Loss: 0.00002152
Iteration 283/1000 | Loss: 0.00002152
Iteration 284/1000 | Loss: 0.00002152
Iteration 285/1000 | Loss: 0.00002152
Iteration 286/1000 | Loss: 0.00002152
Iteration 287/1000 | Loss: 0.00002152
Iteration 288/1000 | Loss: 0.00002152
Iteration 289/1000 | Loss: 0.00002152
Iteration 290/1000 | Loss: 0.00002152
Iteration 291/1000 | Loss: 0.00002151
Iteration 292/1000 | Loss: 0.00002151
Iteration 293/1000 | Loss: 0.00002151
Iteration 294/1000 | Loss: 0.00002151
Iteration 295/1000 | Loss: 0.00002150
Iteration 296/1000 | Loss: 0.00002150
Iteration 297/1000 | Loss: 0.00002150
Iteration 298/1000 | Loss: 0.00002150
Iteration 299/1000 | Loss: 0.00002150
Iteration 300/1000 | Loss: 0.00002150
Iteration 301/1000 | Loss: 0.00002150
Iteration 302/1000 | Loss: 0.00002150
Iteration 303/1000 | Loss: 0.00002150
Iteration 304/1000 | Loss: 0.00002150
Iteration 305/1000 | Loss: 0.00002150
Iteration 306/1000 | Loss: 0.00002150
Iteration 307/1000 | Loss: 0.00002150
Iteration 308/1000 | Loss: 0.00002149
Iteration 309/1000 | Loss: 0.00002149
Iteration 310/1000 | Loss: 0.00002149
Iteration 311/1000 | Loss: 0.00002149
Iteration 312/1000 | Loss: 0.00002148
Iteration 313/1000 | Loss: 0.00002148
Iteration 314/1000 | Loss: 0.00002148
Iteration 315/1000 | Loss: 0.00002147
Iteration 316/1000 | Loss: 0.00002147
Iteration 317/1000 | Loss: 0.00002147
Iteration 318/1000 | Loss: 0.00002147
Iteration 319/1000 | Loss: 0.00002147
Iteration 320/1000 | Loss: 0.00002147
Iteration 321/1000 | Loss: 0.00002147
Iteration 322/1000 | Loss: 0.00002147
Iteration 323/1000 | Loss: 0.00002146
Iteration 324/1000 | Loss: 0.00002146
Iteration 325/1000 | Loss: 0.00002146
Iteration 326/1000 | Loss: 0.00002146
Iteration 327/1000 | Loss: 0.00002146
Iteration 328/1000 | Loss: 0.00002146
Iteration 329/1000 | Loss: 0.00002145
Iteration 330/1000 | Loss: 0.00002145
Iteration 331/1000 | Loss: 0.00002145
Iteration 332/1000 | Loss: 0.00002145
Iteration 333/1000 | Loss: 0.00002145
Iteration 334/1000 | Loss: 0.00002145
Iteration 335/1000 | Loss: 0.00002145
Iteration 336/1000 | Loss: 0.00002145
Iteration 337/1000 | Loss: 0.00002145
Iteration 338/1000 | Loss: 0.00002144
Iteration 339/1000 | Loss: 0.00002144
Iteration 340/1000 | Loss: 0.00002144
Iteration 341/1000 | Loss: 0.00002144
Iteration 342/1000 | Loss: 0.00002144
Iteration 343/1000 | Loss: 0.00002144
Iteration 344/1000 | Loss: 0.00002144
Iteration 345/1000 | Loss: 0.00002144
Iteration 346/1000 | Loss: 0.00002144
Iteration 347/1000 | Loss: 0.00002144
Iteration 348/1000 | Loss: 0.00002144
Iteration 349/1000 | Loss: 0.00002144
Iteration 350/1000 | Loss: 0.00002144
Iteration 351/1000 | Loss: 0.00002144
Iteration 352/1000 | Loss: 0.00002144
Iteration 353/1000 | Loss: 0.00002144
Iteration 354/1000 | Loss: 0.00002144
Iteration 355/1000 | Loss: 0.00002144
Iteration 356/1000 | Loss: 0.00002143
Iteration 357/1000 | Loss: 0.00002143
Iteration 358/1000 | Loss: 0.00002143
Iteration 359/1000 | Loss: 0.00002143
Iteration 360/1000 | Loss: 0.00002143
Iteration 361/1000 | Loss: 0.00002143
Iteration 362/1000 | Loss: 0.00002143
Iteration 363/1000 | Loss: 0.00002143
Iteration 364/1000 | Loss: 0.00002143
Iteration 365/1000 | Loss: 0.00002143
Iteration 366/1000 | Loss: 0.00002142
Iteration 367/1000 | Loss: 0.00002142
Iteration 368/1000 | Loss: 0.00002142
Iteration 369/1000 | Loss: 0.00002142
Iteration 370/1000 | Loss: 0.00002142
Iteration 371/1000 | Loss: 0.00002142
Iteration 372/1000 | Loss: 0.00002141
Iteration 373/1000 | Loss: 0.00002141
Iteration 374/1000 | Loss: 0.00002141
Iteration 375/1000 | Loss: 0.00002141
Iteration 376/1000 | Loss: 0.00002141
Iteration 377/1000 | Loss: 0.00002141
Iteration 378/1000 | Loss: 0.00002141
Iteration 379/1000 | Loss: 0.00002141
Iteration 380/1000 | Loss: 0.00002141
Iteration 381/1000 | Loss: 0.00002141
Iteration 382/1000 | Loss: 0.00002141
Iteration 383/1000 | Loss: 0.00002141
Iteration 384/1000 | Loss: 0.00002141
Iteration 385/1000 | Loss: 0.00002141
Iteration 386/1000 | Loss: 0.00002141
Iteration 387/1000 | Loss: 0.00002141
Iteration 388/1000 | Loss: 0.00002140
Iteration 389/1000 | Loss: 0.00002140
Iteration 390/1000 | Loss: 0.00002140
Iteration 391/1000 | Loss: 0.00002140
Iteration 392/1000 | Loss: 0.00002140
Iteration 393/1000 | Loss: 0.00002140
Iteration 394/1000 | Loss: 0.00002140
Iteration 395/1000 | Loss: 0.00002140
Iteration 396/1000 | Loss: 0.00002140
Iteration 397/1000 | Loss: 0.00002140
Iteration 398/1000 | Loss: 0.00002140
Iteration 399/1000 | Loss: 0.00002140
Iteration 400/1000 | Loss: 0.00002140
Iteration 401/1000 | Loss: 0.00002140
Iteration 402/1000 | Loss: 0.00002140
Iteration 403/1000 | Loss: 0.00002140
Iteration 404/1000 | Loss: 0.00002140
Iteration 405/1000 | Loss: 0.00002140
Iteration 406/1000 | Loss: 0.00002140
Iteration 407/1000 | Loss: 0.00002140
Iteration 408/1000 | Loss: 0.00002140
Iteration 409/1000 | Loss: 0.00002140
Iteration 410/1000 | Loss: 0.00002140
Iteration 411/1000 | Loss: 0.00002140
Iteration 412/1000 | Loss: 0.00002140
Iteration 413/1000 | Loss: 0.00002140
Iteration 414/1000 | Loss: 0.00002140
Iteration 415/1000 | Loss: 0.00002140
Iteration 416/1000 | Loss: 0.00002140
Iteration 417/1000 | Loss: 0.00002140
Iteration 418/1000 | Loss: 0.00002140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 418. Stopping optimization.
Last 5 losses: [2.140377910109237e-05, 2.140377910109237e-05, 2.140377910109237e-05, 2.140377910109237e-05, 2.140377910109237e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.140377910109237e-05

Optimization complete. Final v2v error: 3.8976798057556152 mm

Highest mean error: 9.201831817626953 mm for frame 6

Lowest mean error: 3.439728021621704 mm for frame 0

Saving results

Total time: 412.86580204963684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527112
Iteration 2/25 | Loss: 0.00129879
Iteration 3/25 | Loss: 0.00119599
Iteration 4/25 | Loss: 0.00117597
Iteration 5/25 | Loss: 0.00116898
Iteration 6/25 | Loss: 0.00116758
Iteration 7/25 | Loss: 0.00116741
Iteration 8/25 | Loss: 0.00116741
Iteration 9/25 | Loss: 0.00116741
Iteration 10/25 | Loss: 0.00116741
Iteration 11/25 | Loss: 0.00116741
Iteration 12/25 | Loss: 0.00116741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011674120323732495, 0.0011674120323732495, 0.0011674120323732495, 0.0011674120323732495, 0.0011674120323732495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011674120323732495

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35458601
Iteration 2/25 | Loss: 0.00110391
Iteration 3/25 | Loss: 0.00110390
Iteration 4/25 | Loss: 0.00110390
Iteration 5/25 | Loss: 0.00110390
Iteration 6/25 | Loss: 0.00110390
Iteration 7/25 | Loss: 0.00110390
Iteration 8/25 | Loss: 0.00110390
Iteration 9/25 | Loss: 0.00110390
Iteration 10/25 | Loss: 0.00110390
Iteration 11/25 | Loss: 0.00110390
Iteration 12/25 | Loss: 0.00110390
Iteration 13/25 | Loss: 0.00110390
Iteration 14/25 | Loss: 0.00110390
Iteration 15/25 | Loss: 0.00110390
Iteration 16/25 | Loss: 0.00110390
Iteration 17/25 | Loss: 0.00110390
Iteration 18/25 | Loss: 0.00110390
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011038986267521977, 0.0011038986267521977, 0.0011038986267521977, 0.0011038986267521977, 0.0011038986267521977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011038986267521977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110390
Iteration 2/1000 | Loss: 0.00006530
Iteration 3/1000 | Loss: 0.00003950
Iteration 4/1000 | Loss: 0.00003012
Iteration 5/1000 | Loss: 0.00002703
Iteration 6/1000 | Loss: 0.00002585
Iteration 7/1000 | Loss: 0.00002479
Iteration 8/1000 | Loss: 0.00002414
Iteration 9/1000 | Loss: 0.00002375
Iteration 10/1000 | Loss: 0.00002337
Iteration 11/1000 | Loss: 0.00002314
Iteration 12/1000 | Loss: 0.00002293
Iteration 13/1000 | Loss: 0.00002275
Iteration 14/1000 | Loss: 0.00002269
Iteration 15/1000 | Loss: 0.00002262
Iteration 16/1000 | Loss: 0.00002255
Iteration 17/1000 | Loss: 0.00002250
Iteration 18/1000 | Loss: 0.00002249
Iteration 19/1000 | Loss: 0.00002248
Iteration 20/1000 | Loss: 0.00002247
Iteration 21/1000 | Loss: 0.00002245
Iteration 22/1000 | Loss: 0.00002241
Iteration 23/1000 | Loss: 0.00002238
Iteration 24/1000 | Loss: 0.00002238
Iteration 25/1000 | Loss: 0.00002236
Iteration 26/1000 | Loss: 0.00002236
Iteration 27/1000 | Loss: 0.00002233
Iteration 28/1000 | Loss: 0.00002232
Iteration 29/1000 | Loss: 0.00002232
Iteration 30/1000 | Loss: 0.00002231
Iteration 31/1000 | Loss: 0.00002229
Iteration 32/1000 | Loss: 0.00002225
Iteration 33/1000 | Loss: 0.00002225
Iteration 34/1000 | Loss: 0.00002223
Iteration 35/1000 | Loss: 0.00002222
Iteration 36/1000 | Loss: 0.00002222
Iteration 37/1000 | Loss: 0.00002222
Iteration 38/1000 | Loss: 0.00002221
Iteration 39/1000 | Loss: 0.00002221
Iteration 40/1000 | Loss: 0.00002220
Iteration 41/1000 | Loss: 0.00002220
Iteration 42/1000 | Loss: 0.00002219
Iteration 43/1000 | Loss: 0.00002219
Iteration 44/1000 | Loss: 0.00002219
Iteration 45/1000 | Loss: 0.00002218
Iteration 46/1000 | Loss: 0.00002218
Iteration 47/1000 | Loss: 0.00002217
Iteration 48/1000 | Loss: 0.00002217
Iteration 49/1000 | Loss: 0.00002216
Iteration 50/1000 | Loss: 0.00002216
Iteration 51/1000 | Loss: 0.00002215
Iteration 52/1000 | Loss: 0.00002215
Iteration 53/1000 | Loss: 0.00002214
Iteration 54/1000 | Loss: 0.00002214
Iteration 55/1000 | Loss: 0.00002213
Iteration 56/1000 | Loss: 0.00002213
Iteration 57/1000 | Loss: 0.00002213
Iteration 58/1000 | Loss: 0.00002212
Iteration 59/1000 | Loss: 0.00002212
Iteration 60/1000 | Loss: 0.00002211
Iteration 61/1000 | Loss: 0.00002211
Iteration 62/1000 | Loss: 0.00002211
Iteration 63/1000 | Loss: 0.00002210
Iteration 64/1000 | Loss: 0.00002210
Iteration 65/1000 | Loss: 0.00002210
Iteration 66/1000 | Loss: 0.00002210
Iteration 67/1000 | Loss: 0.00002209
Iteration 68/1000 | Loss: 0.00002209
Iteration 69/1000 | Loss: 0.00002209
Iteration 70/1000 | Loss: 0.00002208
Iteration 71/1000 | Loss: 0.00002208
Iteration 72/1000 | Loss: 0.00002207
Iteration 73/1000 | Loss: 0.00002207
Iteration 74/1000 | Loss: 0.00002207
Iteration 75/1000 | Loss: 0.00002206
Iteration 76/1000 | Loss: 0.00002206
Iteration 77/1000 | Loss: 0.00002206
Iteration 78/1000 | Loss: 0.00002206
Iteration 79/1000 | Loss: 0.00002206
Iteration 80/1000 | Loss: 0.00002205
Iteration 81/1000 | Loss: 0.00002205
Iteration 82/1000 | Loss: 0.00002205
Iteration 83/1000 | Loss: 0.00002204
Iteration 84/1000 | Loss: 0.00002204
Iteration 85/1000 | Loss: 0.00002204
Iteration 86/1000 | Loss: 0.00002203
Iteration 87/1000 | Loss: 0.00002203
Iteration 88/1000 | Loss: 0.00002203
Iteration 89/1000 | Loss: 0.00002203
Iteration 90/1000 | Loss: 0.00002202
Iteration 91/1000 | Loss: 0.00002202
Iteration 92/1000 | Loss: 0.00002202
Iteration 93/1000 | Loss: 0.00002202
Iteration 94/1000 | Loss: 0.00002201
Iteration 95/1000 | Loss: 0.00002201
Iteration 96/1000 | Loss: 0.00002201
Iteration 97/1000 | Loss: 0.00002201
Iteration 98/1000 | Loss: 0.00002201
Iteration 99/1000 | Loss: 0.00002200
Iteration 100/1000 | Loss: 0.00002200
Iteration 101/1000 | Loss: 0.00002200
Iteration 102/1000 | Loss: 0.00002200
Iteration 103/1000 | Loss: 0.00002200
Iteration 104/1000 | Loss: 0.00002200
Iteration 105/1000 | Loss: 0.00002199
Iteration 106/1000 | Loss: 0.00002199
Iteration 107/1000 | Loss: 0.00002199
Iteration 108/1000 | Loss: 0.00002199
Iteration 109/1000 | Loss: 0.00002199
Iteration 110/1000 | Loss: 0.00002198
Iteration 111/1000 | Loss: 0.00002198
Iteration 112/1000 | Loss: 0.00002198
Iteration 113/1000 | Loss: 0.00002198
Iteration 114/1000 | Loss: 0.00002198
Iteration 115/1000 | Loss: 0.00002198
Iteration 116/1000 | Loss: 0.00002198
Iteration 117/1000 | Loss: 0.00002198
Iteration 118/1000 | Loss: 0.00002197
Iteration 119/1000 | Loss: 0.00002197
Iteration 120/1000 | Loss: 0.00002197
Iteration 121/1000 | Loss: 0.00002197
Iteration 122/1000 | Loss: 0.00002197
Iteration 123/1000 | Loss: 0.00002197
Iteration 124/1000 | Loss: 0.00002197
Iteration 125/1000 | Loss: 0.00002197
Iteration 126/1000 | Loss: 0.00002196
Iteration 127/1000 | Loss: 0.00002196
Iteration 128/1000 | Loss: 0.00002196
Iteration 129/1000 | Loss: 0.00002196
Iteration 130/1000 | Loss: 0.00002196
Iteration 131/1000 | Loss: 0.00002196
Iteration 132/1000 | Loss: 0.00002196
Iteration 133/1000 | Loss: 0.00002196
Iteration 134/1000 | Loss: 0.00002195
Iteration 135/1000 | Loss: 0.00002195
Iteration 136/1000 | Loss: 0.00002195
Iteration 137/1000 | Loss: 0.00002195
Iteration 138/1000 | Loss: 0.00002195
Iteration 139/1000 | Loss: 0.00002195
Iteration 140/1000 | Loss: 0.00002195
Iteration 141/1000 | Loss: 0.00002195
Iteration 142/1000 | Loss: 0.00002195
Iteration 143/1000 | Loss: 0.00002195
Iteration 144/1000 | Loss: 0.00002195
Iteration 145/1000 | Loss: 0.00002195
Iteration 146/1000 | Loss: 0.00002195
Iteration 147/1000 | Loss: 0.00002195
Iteration 148/1000 | Loss: 0.00002195
Iteration 149/1000 | Loss: 0.00002194
Iteration 150/1000 | Loss: 0.00002194
Iteration 151/1000 | Loss: 0.00002194
Iteration 152/1000 | Loss: 0.00002194
Iteration 153/1000 | Loss: 0.00002194
Iteration 154/1000 | Loss: 0.00002194
Iteration 155/1000 | Loss: 0.00002194
Iteration 156/1000 | Loss: 0.00002194
Iteration 157/1000 | Loss: 0.00002194
Iteration 158/1000 | Loss: 0.00002193
Iteration 159/1000 | Loss: 0.00002193
Iteration 160/1000 | Loss: 0.00002193
Iteration 161/1000 | Loss: 0.00002193
Iteration 162/1000 | Loss: 0.00002193
Iteration 163/1000 | Loss: 0.00002193
Iteration 164/1000 | Loss: 0.00002193
Iteration 165/1000 | Loss: 0.00002193
Iteration 166/1000 | Loss: 0.00002193
Iteration 167/1000 | Loss: 0.00002192
Iteration 168/1000 | Loss: 0.00002192
Iteration 169/1000 | Loss: 0.00002192
Iteration 170/1000 | Loss: 0.00002192
Iteration 171/1000 | Loss: 0.00002192
Iteration 172/1000 | Loss: 0.00002192
Iteration 173/1000 | Loss: 0.00002192
Iteration 174/1000 | Loss: 0.00002192
Iteration 175/1000 | Loss: 0.00002192
Iteration 176/1000 | Loss: 0.00002192
Iteration 177/1000 | Loss: 0.00002192
Iteration 178/1000 | Loss: 0.00002192
Iteration 179/1000 | Loss: 0.00002192
Iteration 180/1000 | Loss: 0.00002192
Iteration 181/1000 | Loss: 0.00002191
Iteration 182/1000 | Loss: 0.00002191
Iteration 183/1000 | Loss: 0.00002191
Iteration 184/1000 | Loss: 0.00002191
Iteration 185/1000 | Loss: 0.00002191
Iteration 186/1000 | Loss: 0.00002191
Iteration 187/1000 | Loss: 0.00002191
Iteration 188/1000 | Loss: 0.00002191
Iteration 189/1000 | Loss: 0.00002191
Iteration 190/1000 | Loss: 0.00002191
Iteration 191/1000 | Loss: 0.00002191
Iteration 192/1000 | Loss: 0.00002190
Iteration 193/1000 | Loss: 0.00002190
Iteration 194/1000 | Loss: 0.00002190
Iteration 195/1000 | Loss: 0.00002190
Iteration 196/1000 | Loss: 0.00002190
Iteration 197/1000 | Loss: 0.00002190
Iteration 198/1000 | Loss: 0.00002190
Iteration 199/1000 | Loss: 0.00002190
Iteration 200/1000 | Loss: 0.00002189
Iteration 201/1000 | Loss: 0.00002189
Iteration 202/1000 | Loss: 0.00002189
Iteration 203/1000 | Loss: 0.00002189
Iteration 204/1000 | Loss: 0.00002189
Iteration 205/1000 | Loss: 0.00002189
Iteration 206/1000 | Loss: 0.00002189
Iteration 207/1000 | Loss: 0.00002189
Iteration 208/1000 | Loss: 0.00002188
Iteration 209/1000 | Loss: 0.00002188
Iteration 210/1000 | Loss: 0.00002188
Iteration 211/1000 | Loss: 0.00002188
Iteration 212/1000 | Loss: 0.00002188
Iteration 213/1000 | Loss: 0.00002188
Iteration 214/1000 | Loss: 0.00002188
Iteration 215/1000 | Loss: 0.00002188
Iteration 216/1000 | Loss: 0.00002188
Iteration 217/1000 | Loss: 0.00002188
Iteration 218/1000 | Loss: 0.00002188
Iteration 219/1000 | Loss: 0.00002188
Iteration 220/1000 | Loss: 0.00002188
Iteration 221/1000 | Loss: 0.00002188
Iteration 222/1000 | Loss: 0.00002187
Iteration 223/1000 | Loss: 0.00002187
Iteration 224/1000 | Loss: 0.00002187
Iteration 225/1000 | Loss: 0.00002187
Iteration 226/1000 | Loss: 0.00002187
Iteration 227/1000 | Loss: 0.00002187
Iteration 228/1000 | Loss: 0.00002187
Iteration 229/1000 | Loss: 0.00002186
Iteration 230/1000 | Loss: 0.00002186
Iteration 231/1000 | Loss: 0.00002186
Iteration 232/1000 | Loss: 0.00002186
Iteration 233/1000 | Loss: 0.00002186
Iteration 234/1000 | Loss: 0.00002186
Iteration 235/1000 | Loss: 0.00002186
Iteration 236/1000 | Loss: 0.00002186
Iteration 237/1000 | Loss: 0.00002186
Iteration 238/1000 | Loss: 0.00002186
Iteration 239/1000 | Loss: 0.00002186
Iteration 240/1000 | Loss: 0.00002185
Iteration 241/1000 | Loss: 0.00002185
Iteration 242/1000 | Loss: 0.00002185
Iteration 243/1000 | Loss: 0.00002185
Iteration 244/1000 | Loss: 0.00002185
Iteration 245/1000 | Loss: 0.00002185
Iteration 246/1000 | Loss: 0.00002185
Iteration 247/1000 | Loss: 0.00002185
Iteration 248/1000 | Loss: 0.00002185
Iteration 249/1000 | Loss: 0.00002185
Iteration 250/1000 | Loss: 0.00002185
Iteration 251/1000 | Loss: 0.00002185
Iteration 252/1000 | Loss: 0.00002185
Iteration 253/1000 | Loss: 0.00002185
Iteration 254/1000 | Loss: 0.00002185
Iteration 255/1000 | Loss: 0.00002185
Iteration 256/1000 | Loss: 0.00002185
Iteration 257/1000 | Loss: 0.00002185
Iteration 258/1000 | Loss: 0.00002185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [2.1854537408216856e-05, 2.1854537408216856e-05, 2.1854537408216856e-05, 2.1854537408216856e-05, 2.1854537408216856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1854537408216856e-05

Optimization complete. Final v2v error: 3.705620050430298 mm

Highest mean error: 4.41788911819458 mm for frame 111

Lowest mean error: 2.7441956996917725 mm for frame 8

Saving results

Total time: 47.75719094276428
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415502
Iteration 2/25 | Loss: 0.00118171
Iteration 3/25 | Loss: 0.00110611
Iteration 4/25 | Loss: 0.00109998
Iteration 5/25 | Loss: 0.00109751
Iteration 6/25 | Loss: 0.00109751
Iteration 7/25 | Loss: 0.00109751
Iteration 8/25 | Loss: 0.00109751
Iteration 9/25 | Loss: 0.00109751
Iteration 10/25 | Loss: 0.00109751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010975071927532554, 0.0010975071927532554, 0.0010975071927532554, 0.0010975071927532554, 0.0010975071927532554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010975071927532554

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44476724
Iteration 2/25 | Loss: 0.00071658
Iteration 3/25 | Loss: 0.00071658
Iteration 4/25 | Loss: 0.00071658
Iteration 5/25 | Loss: 0.00071658
Iteration 6/25 | Loss: 0.00071658
Iteration 7/25 | Loss: 0.00071658
Iteration 8/25 | Loss: 0.00071658
Iteration 9/25 | Loss: 0.00071658
Iteration 10/25 | Loss: 0.00071658
Iteration 11/25 | Loss: 0.00071658
Iteration 12/25 | Loss: 0.00071658
Iteration 13/25 | Loss: 0.00071658
Iteration 14/25 | Loss: 0.00071658
Iteration 15/25 | Loss: 0.00071658
Iteration 16/25 | Loss: 0.00071658
Iteration 17/25 | Loss: 0.00071658
Iteration 18/25 | Loss: 0.00071658
Iteration 19/25 | Loss: 0.00071658
Iteration 20/25 | Loss: 0.00071658
Iteration 21/25 | Loss: 0.00071658
Iteration 22/25 | Loss: 0.00071658
Iteration 23/25 | Loss: 0.00071658
Iteration 24/25 | Loss: 0.00071658
Iteration 25/25 | Loss: 0.00071658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071658
Iteration 2/1000 | Loss: 0.00002043
Iteration 3/1000 | Loss: 0.00001285
Iteration 4/1000 | Loss: 0.00001150
Iteration 5/1000 | Loss: 0.00001091
Iteration 6/1000 | Loss: 0.00001051
Iteration 7/1000 | Loss: 0.00001035
Iteration 8/1000 | Loss: 0.00001013
Iteration 9/1000 | Loss: 0.00000986
Iteration 10/1000 | Loss: 0.00000963
Iteration 11/1000 | Loss: 0.00000961
Iteration 12/1000 | Loss: 0.00000960
Iteration 13/1000 | Loss: 0.00000956
Iteration 14/1000 | Loss: 0.00000941
Iteration 15/1000 | Loss: 0.00000938
Iteration 16/1000 | Loss: 0.00000937
Iteration 17/1000 | Loss: 0.00000937
Iteration 18/1000 | Loss: 0.00000933
Iteration 19/1000 | Loss: 0.00000931
Iteration 20/1000 | Loss: 0.00000931
Iteration 21/1000 | Loss: 0.00000930
Iteration 22/1000 | Loss: 0.00000929
Iteration 23/1000 | Loss: 0.00000928
Iteration 24/1000 | Loss: 0.00000928
Iteration 25/1000 | Loss: 0.00000928
Iteration 26/1000 | Loss: 0.00000927
Iteration 27/1000 | Loss: 0.00000926
Iteration 28/1000 | Loss: 0.00000926
Iteration 29/1000 | Loss: 0.00000926
Iteration 30/1000 | Loss: 0.00000925
Iteration 31/1000 | Loss: 0.00000925
Iteration 32/1000 | Loss: 0.00000925
Iteration 33/1000 | Loss: 0.00000924
Iteration 34/1000 | Loss: 0.00000924
Iteration 35/1000 | Loss: 0.00000924
Iteration 36/1000 | Loss: 0.00000923
Iteration 37/1000 | Loss: 0.00000923
Iteration 38/1000 | Loss: 0.00000922
Iteration 39/1000 | Loss: 0.00000922
Iteration 40/1000 | Loss: 0.00000922
Iteration 41/1000 | Loss: 0.00000921
Iteration 42/1000 | Loss: 0.00000921
Iteration 43/1000 | Loss: 0.00000921
Iteration 44/1000 | Loss: 0.00000921
Iteration 45/1000 | Loss: 0.00000921
Iteration 46/1000 | Loss: 0.00000920
Iteration 47/1000 | Loss: 0.00000920
Iteration 48/1000 | Loss: 0.00000919
Iteration 49/1000 | Loss: 0.00000919
Iteration 50/1000 | Loss: 0.00000918
Iteration 51/1000 | Loss: 0.00000918
Iteration 52/1000 | Loss: 0.00000918
Iteration 53/1000 | Loss: 0.00000918
Iteration 54/1000 | Loss: 0.00000918
Iteration 55/1000 | Loss: 0.00000918
Iteration 56/1000 | Loss: 0.00000917
Iteration 57/1000 | Loss: 0.00000916
Iteration 58/1000 | Loss: 0.00000915
Iteration 59/1000 | Loss: 0.00000915
Iteration 60/1000 | Loss: 0.00000915
Iteration 61/1000 | Loss: 0.00000915
Iteration 62/1000 | Loss: 0.00000915
Iteration 63/1000 | Loss: 0.00000915
Iteration 64/1000 | Loss: 0.00000914
Iteration 65/1000 | Loss: 0.00000914
Iteration 66/1000 | Loss: 0.00000914
Iteration 67/1000 | Loss: 0.00000914
Iteration 68/1000 | Loss: 0.00000914
Iteration 69/1000 | Loss: 0.00000914
Iteration 70/1000 | Loss: 0.00000913
Iteration 71/1000 | Loss: 0.00000913
Iteration 72/1000 | Loss: 0.00000913
Iteration 73/1000 | Loss: 0.00000913
Iteration 74/1000 | Loss: 0.00000913
Iteration 75/1000 | Loss: 0.00000913
Iteration 76/1000 | Loss: 0.00000913
Iteration 77/1000 | Loss: 0.00000913
Iteration 78/1000 | Loss: 0.00000913
Iteration 79/1000 | Loss: 0.00000913
Iteration 80/1000 | Loss: 0.00000913
Iteration 81/1000 | Loss: 0.00000912
Iteration 82/1000 | Loss: 0.00000912
Iteration 83/1000 | Loss: 0.00000912
Iteration 84/1000 | Loss: 0.00000912
Iteration 85/1000 | Loss: 0.00000912
Iteration 86/1000 | Loss: 0.00000912
Iteration 87/1000 | Loss: 0.00000912
Iteration 88/1000 | Loss: 0.00000912
Iteration 89/1000 | Loss: 0.00000912
Iteration 90/1000 | Loss: 0.00000912
Iteration 91/1000 | Loss: 0.00000912
Iteration 92/1000 | Loss: 0.00000912
Iteration 93/1000 | Loss: 0.00000912
Iteration 94/1000 | Loss: 0.00000912
Iteration 95/1000 | Loss: 0.00000912
Iteration 96/1000 | Loss: 0.00000912
Iteration 97/1000 | Loss: 0.00000912
Iteration 98/1000 | Loss: 0.00000912
Iteration 99/1000 | Loss: 0.00000912
Iteration 100/1000 | Loss: 0.00000912
Iteration 101/1000 | Loss: 0.00000912
Iteration 102/1000 | Loss: 0.00000912
Iteration 103/1000 | Loss: 0.00000912
Iteration 104/1000 | Loss: 0.00000912
Iteration 105/1000 | Loss: 0.00000912
Iteration 106/1000 | Loss: 0.00000912
Iteration 107/1000 | Loss: 0.00000912
Iteration 108/1000 | Loss: 0.00000912
Iteration 109/1000 | Loss: 0.00000912
Iteration 110/1000 | Loss: 0.00000912
Iteration 111/1000 | Loss: 0.00000912
Iteration 112/1000 | Loss: 0.00000912
Iteration 113/1000 | Loss: 0.00000912
Iteration 114/1000 | Loss: 0.00000912
Iteration 115/1000 | Loss: 0.00000912
Iteration 116/1000 | Loss: 0.00000912
Iteration 117/1000 | Loss: 0.00000912
Iteration 118/1000 | Loss: 0.00000912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [9.115531611314509e-06, 9.115531611314509e-06, 9.115531611314509e-06, 9.115531611314509e-06, 9.115531611314509e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.115531611314509e-06

Optimization complete. Final v2v error: 2.607020139694214 mm

Highest mean error: 2.809114456176758 mm for frame 163

Lowest mean error: 2.4660656452178955 mm for frame 14

Saving results

Total time: 33.902318716049194
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01092015
Iteration 2/25 | Loss: 0.01092015
Iteration 3/25 | Loss: 0.01092015
Iteration 4/25 | Loss: 0.01092015
Iteration 5/25 | Loss: 0.01092015
Iteration 6/25 | Loss: 0.01092015
Iteration 7/25 | Loss: 0.01092015
Iteration 8/25 | Loss: 0.01092015
Iteration 9/25 | Loss: 0.01092015
Iteration 10/25 | Loss: 0.01092015
Iteration 11/25 | Loss: 0.01092015
Iteration 12/25 | Loss: 0.01092015
Iteration 13/25 | Loss: 0.01092015
Iteration 14/25 | Loss: 0.01092014
Iteration 15/25 | Loss: 0.01092014
Iteration 16/25 | Loss: 0.01092014
Iteration 17/25 | Loss: 0.01092014
Iteration 18/25 | Loss: 0.01092014
Iteration 19/25 | Loss: 0.01092014
Iteration 20/25 | Loss: 0.01092014
Iteration 21/25 | Loss: 0.01092014
Iteration 22/25 | Loss: 0.01092014
Iteration 23/25 | Loss: 0.01092014
Iteration 24/25 | Loss: 0.01092014
Iteration 25/25 | Loss: 0.01092014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.30925751
Iteration 2/25 | Loss: 0.17713784
Iteration 3/25 | Loss: 0.17444734
Iteration 4/25 | Loss: 0.17401047
Iteration 5/25 | Loss: 0.17401041
Iteration 6/25 | Loss: 0.17401040
Iteration 7/25 | Loss: 0.17401040
Iteration 8/25 | Loss: 0.17401038
Iteration 9/25 | Loss: 0.17401038
Iteration 10/25 | Loss: 0.17401038
Iteration 11/25 | Loss: 0.17401038
Iteration 12/25 | Loss: 0.17401038
Iteration 13/25 | Loss: 0.17401038
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.17401038110256195, 0.17401038110256195, 0.17401038110256195, 0.17401038110256195, 0.17401038110256195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17401038110256195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17401038
Iteration 2/1000 | Loss: 0.00709636
Iteration 3/1000 | Loss: 0.00055781
Iteration 4/1000 | Loss: 0.00064047
Iteration 5/1000 | Loss: 0.00361243
Iteration 6/1000 | Loss: 0.00030801
Iteration 7/1000 | Loss: 0.00085252
Iteration 8/1000 | Loss: 0.00018576
Iteration 9/1000 | Loss: 0.00103539
Iteration 10/1000 | Loss: 0.00019166
Iteration 11/1000 | Loss: 0.00007086
Iteration 12/1000 | Loss: 0.00055960
Iteration 13/1000 | Loss: 0.00004421
Iteration 14/1000 | Loss: 0.00002274
Iteration 15/1000 | Loss: 0.00008718
Iteration 16/1000 | Loss: 0.00001966
Iteration 17/1000 | Loss: 0.00005977
Iteration 18/1000 | Loss: 0.00001793
Iteration 19/1000 | Loss: 0.00027981
Iteration 20/1000 | Loss: 0.00001984
Iteration 21/1000 | Loss: 0.00018482
Iteration 22/1000 | Loss: 0.00023665
Iteration 23/1000 | Loss: 0.00001637
Iteration 24/1000 | Loss: 0.00006713
Iteration 25/1000 | Loss: 0.00005530
Iteration 26/1000 | Loss: 0.00001529
Iteration 27/1000 | Loss: 0.00001487
Iteration 28/1000 | Loss: 0.00001439
Iteration 29/1000 | Loss: 0.00007204
Iteration 30/1000 | Loss: 0.00005972
Iteration 31/1000 | Loss: 0.00001408
Iteration 32/1000 | Loss: 0.00002193
Iteration 33/1000 | Loss: 0.00001345
Iteration 34/1000 | Loss: 0.00001308
Iteration 35/1000 | Loss: 0.00007646
Iteration 36/1000 | Loss: 0.00026672
Iteration 37/1000 | Loss: 0.00002754
Iteration 38/1000 | Loss: 0.00001292
Iteration 39/1000 | Loss: 0.00004517
Iteration 40/1000 | Loss: 0.00001535
Iteration 41/1000 | Loss: 0.00001546
Iteration 42/1000 | Loss: 0.00007614
Iteration 43/1000 | Loss: 0.00001254
Iteration 44/1000 | Loss: 0.00001229
Iteration 45/1000 | Loss: 0.00001226
Iteration 46/1000 | Loss: 0.00001225
Iteration 47/1000 | Loss: 0.00006237
Iteration 48/1000 | Loss: 0.00001216
Iteration 49/1000 | Loss: 0.00001204
Iteration 50/1000 | Loss: 0.00001203
Iteration 51/1000 | Loss: 0.00001203
Iteration 52/1000 | Loss: 0.00001202
Iteration 53/1000 | Loss: 0.00001202
Iteration 54/1000 | Loss: 0.00001202
Iteration 55/1000 | Loss: 0.00001201
Iteration 56/1000 | Loss: 0.00001201
Iteration 57/1000 | Loss: 0.00001200
Iteration 58/1000 | Loss: 0.00001200
Iteration 59/1000 | Loss: 0.00001200
Iteration 60/1000 | Loss: 0.00001200
Iteration 61/1000 | Loss: 0.00001199
Iteration 62/1000 | Loss: 0.00001199
Iteration 63/1000 | Loss: 0.00001198
Iteration 64/1000 | Loss: 0.00001184
Iteration 65/1000 | Loss: 0.00001172
Iteration 66/1000 | Loss: 0.00001171
Iteration 67/1000 | Loss: 0.00001170
Iteration 68/1000 | Loss: 0.00001170
Iteration 69/1000 | Loss: 0.00001168
Iteration 70/1000 | Loss: 0.00001165
Iteration 71/1000 | Loss: 0.00001164
Iteration 72/1000 | Loss: 0.00001164
Iteration 73/1000 | Loss: 0.00001163
Iteration 74/1000 | Loss: 0.00001163
Iteration 75/1000 | Loss: 0.00001162
Iteration 76/1000 | Loss: 0.00001162
Iteration 77/1000 | Loss: 0.00001161
Iteration 78/1000 | Loss: 0.00004710
Iteration 79/1000 | Loss: 0.00001166
Iteration 80/1000 | Loss: 0.00001158
Iteration 81/1000 | Loss: 0.00001158
Iteration 82/1000 | Loss: 0.00001158
Iteration 83/1000 | Loss: 0.00001158
Iteration 84/1000 | Loss: 0.00001158
Iteration 85/1000 | Loss: 0.00001158
Iteration 86/1000 | Loss: 0.00001158
Iteration 87/1000 | Loss: 0.00001158
Iteration 88/1000 | Loss: 0.00001157
Iteration 89/1000 | Loss: 0.00001157
Iteration 90/1000 | Loss: 0.00001157
Iteration 91/1000 | Loss: 0.00001156
Iteration 92/1000 | Loss: 0.00001156
Iteration 93/1000 | Loss: 0.00001156
Iteration 94/1000 | Loss: 0.00001156
Iteration 95/1000 | Loss: 0.00001156
Iteration 96/1000 | Loss: 0.00001155
Iteration 97/1000 | Loss: 0.00001155
Iteration 98/1000 | Loss: 0.00001155
Iteration 99/1000 | Loss: 0.00001155
Iteration 100/1000 | Loss: 0.00001155
Iteration 101/1000 | Loss: 0.00001155
Iteration 102/1000 | Loss: 0.00001155
Iteration 103/1000 | Loss: 0.00001154
Iteration 104/1000 | Loss: 0.00001154
Iteration 105/1000 | Loss: 0.00001154
Iteration 106/1000 | Loss: 0.00001154
Iteration 107/1000 | Loss: 0.00001153
Iteration 108/1000 | Loss: 0.00001153
Iteration 109/1000 | Loss: 0.00001153
Iteration 110/1000 | Loss: 0.00001153
Iteration 111/1000 | Loss: 0.00001152
Iteration 112/1000 | Loss: 0.00001152
Iteration 113/1000 | Loss: 0.00001151
Iteration 114/1000 | Loss: 0.00001151
Iteration 115/1000 | Loss: 0.00001151
Iteration 116/1000 | Loss: 0.00001151
Iteration 117/1000 | Loss: 0.00001151
Iteration 118/1000 | Loss: 0.00001151
Iteration 119/1000 | Loss: 0.00001150
Iteration 120/1000 | Loss: 0.00001150
Iteration 121/1000 | Loss: 0.00001150
Iteration 122/1000 | Loss: 0.00001150
Iteration 123/1000 | Loss: 0.00001150
Iteration 124/1000 | Loss: 0.00001150
Iteration 125/1000 | Loss: 0.00001150
Iteration 126/1000 | Loss: 0.00001150
Iteration 127/1000 | Loss: 0.00001150
Iteration 128/1000 | Loss: 0.00001150
Iteration 129/1000 | Loss: 0.00001150
Iteration 130/1000 | Loss: 0.00001149
Iteration 131/1000 | Loss: 0.00001149
Iteration 132/1000 | Loss: 0.00001149
Iteration 133/1000 | Loss: 0.00001148
Iteration 134/1000 | Loss: 0.00001148
Iteration 135/1000 | Loss: 0.00001148
Iteration 136/1000 | Loss: 0.00001147
Iteration 137/1000 | Loss: 0.00001147
Iteration 138/1000 | Loss: 0.00001147
Iteration 139/1000 | Loss: 0.00001147
Iteration 140/1000 | Loss: 0.00001147
Iteration 141/1000 | Loss: 0.00001147
Iteration 142/1000 | Loss: 0.00001146
Iteration 143/1000 | Loss: 0.00001146
Iteration 144/1000 | Loss: 0.00001146
Iteration 145/1000 | Loss: 0.00001146
Iteration 146/1000 | Loss: 0.00001146
Iteration 147/1000 | Loss: 0.00001146
Iteration 148/1000 | Loss: 0.00001146
Iteration 149/1000 | Loss: 0.00001146
Iteration 150/1000 | Loss: 0.00001146
Iteration 151/1000 | Loss: 0.00001145
Iteration 152/1000 | Loss: 0.00001145
Iteration 153/1000 | Loss: 0.00001145
Iteration 154/1000 | Loss: 0.00001145
Iteration 155/1000 | Loss: 0.00001145
Iteration 156/1000 | Loss: 0.00001145
Iteration 157/1000 | Loss: 0.00001145
Iteration 158/1000 | Loss: 0.00001145
Iteration 159/1000 | Loss: 0.00001145
Iteration 160/1000 | Loss: 0.00001145
Iteration 161/1000 | Loss: 0.00001145
Iteration 162/1000 | Loss: 0.00001145
Iteration 163/1000 | Loss: 0.00001145
Iteration 164/1000 | Loss: 0.00001145
Iteration 165/1000 | Loss: 0.00001145
Iteration 166/1000 | Loss: 0.00001145
Iteration 167/1000 | Loss: 0.00001145
Iteration 168/1000 | Loss: 0.00001145
Iteration 169/1000 | Loss: 0.00001145
Iteration 170/1000 | Loss: 0.00001145
Iteration 171/1000 | Loss: 0.00001145
Iteration 172/1000 | Loss: 0.00001145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.14524018499651e-05, 1.14524018499651e-05, 1.14524018499651e-05, 1.14524018499651e-05, 1.14524018499651e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.14524018499651e-05

Optimization complete. Final v2v error: 2.883533000946045 mm

Highest mean error: 3.217081069946289 mm for frame 121

Lowest mean error: 2.6369564533233643 mm for frame 10

Saving results

Total time: 95.04587531089783
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908202
Iteration 2/25 | Loss: 0.00130041
Iteration 3/25 | Loss: 0.00119523
Iteration 4/25 | Loss: 0.00118589
Iteration 5/25 | Loss: 0.00118345
Iteration 6/25 | Loss: 0.00118307
Iteration 7/25 | Loss: 0.00118307
Iteration 8/25 | Loss: 0.00118307
Iteration 9/25 | Loss: 0.00118307
Iteration 10/25 | Loss: 0.00118307
Iteration 11/25 | Loss: 0.00118307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011830712901428342, 0.0011830712901428342, 0.0011830712901428342, 0.0011830712901428342, 0.0011830712901428342]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011830712901428342

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20840085
Iteration 2/25 | Loss: 0.00093296
Iteration 3/25 | Loss: 0.00093294
Iteration 4/25 | Loss: 0.00093294
Iteration 5/25 | Loss: 0.00093294
Iteration 6/25 | Loss: 0.00093294
Iteration 7/25 | Loss: 0.00093294
Iteration 8/25 | Loss: 0.00093294
Iteration 9/25 | Loss: 0.00093294
Iteration 10/25 | Loss: 0.00093294
Iteration 11/25 | Loss: 0.00093294
Iteration 12/25 | Loss: 0.00093294
Iteration 13/25 | Loss: 0.00093294
Iteration 14/25 | Loss: 0.00093294
Iteration 15/25 | Loss: 0.00093294
Iteration 16/25 | Loss: 0.00093294
Iteration 17/25 | Loss: 0.00093294
Iteration 18/25 | Loss: 0.00093294
Iteration 19/25 | Loss: 0.00093294
Iteration 20/25 | Loss: 0.00093294
Iteration 21/25 | Loss: 0.00093294
Iteration 22/25 | Loss: 0.00093294
Iteration 23/25 | Loss: 0.00093294
Iteration 24/25 | Loss: 0.00093294
Iteration 25/25 | Loss: 0.00093293

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093293
Iteration 2/1000 | Loss: 0.00003207
Iteration 3/1000 | Loss: 0.00001962
Iteration 4/1000 | Loss: 0.00001740
Iteration 5/1000 | Loss: 0.00001676
Iteration 6/1000 | Loss: 0.00001625
Iteration 7/1000 | Loss: 0.00001593
Iteration 8/1000 | Loss: 0.00001574
Iteration 9/1000 | Loss: 0.00001545
Iteration 10/1000 | Loss: 0.00001534
Iteration 11/1000 | Loss: 0.00001529
Iteration 12/1000 | Loss: 0.00001511
Iteration 13/1000 | Loss: 0.00001511
Iteration 14/1000 | Loss: 0.00001504
Iteration 15/1000 | Loss: 0.00001497
Iteration 16/1000 | Loss: 0.00001496
Iteration 17/1000 | Loss: 0.00001495
Iteration 18/1000 | Loss: 0.00001494
Iteration 19/1000 | Loss: 0.00001490
Iteration 20/1000 | Loss: 0.00001490
Iteration 21/1000 | Loss: 0.00001489
Iteration 22/1000 | Loss: 0.00001485
Iteration 23/1000 | Loss: 0.00001482
Iteration 24/1000 | Loss: 0.00001477
Iteration 25/1000 | Loss: 0.00001477
Iteration 26/1000 | Loss: 0.00001477
Iteration 27/1000 | Loss: 0.00001474
Iteration 28/1000 | Loss: 0.00001473
Iteration 29/1000 | Loss: 0.00001471
Iteration 30/1000 | Loss: 0.00001471
Iteration 31/1000 | Loss: 0.00001469
Iteration 32/1000 | Loss: 0.00001467
Iteration 33/1000 | Loss: 0.00001466
Iteration 34/1000 | Loss: 0.00001466
Iteration 35/1000 | Loss: 0.00001465
Iteration 36/1000 | Loss: 0.00001464
Iteration 37/1000 | Loss: 0.00001464
Iteration 38/1000 | Loss: 0.00001464
Iteration 39/1000 | Loss: 0.00001464
Iteration 40/1000 | Loss: 0.00001464
Iteration 41/1000 | Loss: 0.00001464
Iteration 42/1000 | Loss: 0.00001464
Iteration 43/1000 | Loss: 0.00001464
Iteration 44/1000 | Loss: 0.00001464
Iteration 45/1000 | Loss: 0.00001463
Iteration 46/1000 | Loss: 0.00001463
Iteration 47/1000 | Loss: 0.00001462
Iteration 48/1000 | Loss: 0.00001462
Iteration 49/1000 | Loss: 0.00001462
Iteration 50/1000 | Loss: 0.00001462
Iteration 51/1000 | Loss: 0.00001461
Iteration 52/1000 | Loss: 0.00001461
Iteration 53/1000 | Loss: 0.00001461
Iteration 54/1000 | Loss: 0.00001461
Iteration 55/1000 | Loss: 0.00001461
Iteration 56/1000 | Loss: 0.00001461
Iteration 57/1000 | Loss: 0.00001460
Iteration 58/1000 | Loss: 0.00001460
Iteration 59/1000 | Loss: 0.00001460
Iteration 60/1000 | Loss: 0.00001460
Iteration 61/1000 | Loss: 0.00001460
Iteration 62/1000 | Loss: 0.00001460
Iteration 63/1000 | Loss: 0.00001460
Iteration 64/1000 | Loss: 0.00001459
Iteration 65/1000 | Loss: 0.00001459
Iteration 66/1000 | Loss: 0.00001459
Iteration 67/1000 | Loss: 0.00001458
Iteration 68/1000 | Loss: 0.00001458
Iteration 69/1000 | Loss: 0.00001458
Iteration 70/1000 | Loss: 0.00001458
Iteration 71/1000 | Loss: 0.00001458
Iteration 72/1000 | Loss: 0.00001457
Iteration 73/1000 | Loss: 0.00001457
Iteration 74/1000 | Loss: 0.00001457
Iteration 75/1000 | Loss: 0.00001457
Iteration 76/1000 | Loss: 0.00001457
Iteration 77/1000 | Loss: 0.00001457
Iteration 78/1000 | Loss: 0.00001456
Iteration 79/1000 | Loss: 0.00001456
Iteration 80/1000 | Loss: 0.00001456
Iteration 81/1000 | Loss: 0.00001456
Iteration 82/1000 | Loss: 0.00001456
Iteration 83/1000 | Loss: 0.00001455
Iteration 84/1000 | Loss: 0.00001455
Iteration 85/1000 | Loss: 0.00001455
Iteration 86/1000 | Loss: 0.00001454
Iteration 87/1000 | Loss: 0.00001454
Iteration 88/1000 | Loss: 0.00001454
Iteration 89/1000 | Loss: 0.00001454
Iteration 90/1000 | Loss: 0.00001454
Iteration 91/1000 | Loss: 0.00001453
Iteration 92/1000 | Loss: 0.00001453
Iteration 93/1000 | Loss: 0.00001453
Iteration 94/1000 | Loss: 0.00001453
Iteration 95/1000 | Loss: 0.00001453
Iteration 96/1000 | Loss: 0.00001453
Iteration 97/1000 | Loss: 0.00001453
Iteration 98/1000 | Loss: 0.00001452
Iteration 99/1000 | Loss: 0.00001452
Iteration 100/1000 | Loss: 0.00001452
Iteration 101/1000 | Loss: 0.00001451
Iteration 102/1000 | Loss: 0.00001451
Iteration 103/1000 | Loss: 0.00001451
Iteration 104/1000 | Loss: 0.00001451
Iteration 105/1000 | Loss: 0.00001451
Iteration 106/1000 | Loss: 0.00001451
Iteration 107/1000 | Loss: 0.00001450
Iteration 108/1000 | Loss: 0.00001450
Iteration 109/1000 | Loss: 0.00001450
Iteration 110/1000 | Loss: 0.00001449
Iteration 111/1000 | Loss: 0.00001449
Iteration 112/1000 | Loss: 0.00001449
Iteration 113/1000 | Loss: 0.00001448
Iteration 114/1000 | Loss: 0.00001448
Iteration 115/1000 | Loss: 0.00001448
Iteration 116/1000 | Loss: 0.00001447
Iteration 117/1000 | Loss: 0.00001446
Iteration 118/1000 | Loss: 0.00001446
Iteration 119/1000 | Loss: 0.00001446
Iteration 120/1000 | Loss: 0.00001446
Iteration 121/1000 | Loss: 0.00001445
Iteration 122/1000 | Loss: 0.00001445
Iteration 123/1000 | Loss: 0.00001445
Iteration 124/1000 | Loss: 0.00001444
Iteration 125/1000 | Loss: 0.00001444
Iteration 126/1000 | Loss: 0.00001444
Iteration 127/1000 | Loss: 0.00001443
Iteration 128/1000 | Loss: 0.00001443
Iteration 129/1000 | Loss: 0.00001442
Iteration 130/1000 | Loss: 0.00001442
Iteration 131/1000 | Loss: 0.00001442
Iteration 132/1000 | Loss: 0.00001442
Iteration 133/1000 | Loss: 0.00001442
Iteration 134/1000 | Loss: 0.00001442
Iteration 135/1000 | Loss: 0.00001442
Iteration 136/1000 | Loss: 0.00001442
Iteration 137/1000 | Loss: 0.00001442
Iteration 138/1000 | Loss: 0.00001442
Iteration 139/1000 | Loss: 0.00001442
Iteration 140/1000 | Loss: 0.00001442
Iteration 141/1000 | Loss: 0.00001442
Iteration 142/1000 | Loss: 0.00001442
Iteration 143/1000 | Loss: 0.00001442
Iteration 144/1000 | Loss: 0.00001442
Iteration 145/1000 | Loss: 0.00001441
Iteration 146/1000 | Loss: 0.00001441
Iteration 147/1000 | Loss: 0.00001441
Iteration 148/1000 | Loss: 0.00001441
Iteration 149/1000 | Loss: 0.00001441
Iteration 150/1000 | Loss: 0.00001441
Iteration 151/1000 | Loss: 0.00001440
Iteration 152/1000 | Loss: 0.00001440
Iteration 153/1000 | Loss: 0.00001440
Iteration 154/1000 | Loss: 0.00001440
Iteration 155/1000 | Loss: 0.00001440
Iteration 156/1000 | Loss: 0.00001440
Iteration 157/1000 | Loss: 0.00001439
Iteration 158/1000 | Loss: 0.00001439
Iteration 159/1000 | Loss: 0.00001439
Iteration 160/1000 | Loss: 0.00001439
Iteration 161/1000 | Loss: 0.00001439
Iteration 162/1000 | Loss: 0.00001439
Iteration 163/1000 | Loss: 0.00001439
Iteration 164/1000 | Loss: 0.00001439
Iteration 165/1000 | Loss: 0.00001439
Iteration 166/1000 | Loss: 0.00001439
Iteration 167/1000 | Loss: 0.00001438
Iteration 168/1000 | Loss: 0.00001438
Iteration 169/1000 | Loss: 0.00001438
Iteration 170/1000 | Loss: 0.00001437
Iteration 171/1000 | Loss: 0.00001437
Iteration 172/1000 | Loss: 0.00001437
Iteration 173/1000 | Loss: 0.00001437
Iteration 174/1000 | Loss: 0.00001437
Iteration 175/1000 | Loss: 0.00001437
Iteration 176/1000 | Loss: 0.00001437
Iteration 177/1000 | Loss: 0.00001437
Iteration 178/1000 | Loss: 0.00001437
Iteration 179/1000 | Loss: 0.00001436
Iteration 180/1000 | Loss: 0.00001436
Iteration 181/1000 | Loss: 0.00001436
Iteration 182/1000 | Loss: 0.00001436
Iteration 183/1000 | Loss: 0.00001436
Iteration 184/1000 | Loss: 0.00001436
Iteration 185/1000 | Loss: 0.00001436
Iteration 186/1000 | Loss: 0.00001436
Iteration 187/1000 | Loss: 0.00001436
Iteration 188/1000 | Loss: 0.00001436
Iteration 189/1000 | Loss: 0.00001435
Iteration 190/1000 | Loss: 0.00001435
Iteration 191/1000 | Loss: 0.00001435
Iteration 192/1000 | Loss: 0.00001435
Iteration 193/1000 | Loss: 0.00001434
Iteration 194/1000 | Loss: 0.00001434
Iteration 195/1000 | Loss: 0.00001434
Iteration 196/1000 | Loss: 0.00001434
Iteration 197/1000 | Loss: 0.00001434
Iteration 198/1000 | Loss: 0.00001434
Iteration 199/1000 | Loss: 0.00001434
Iteration 200/1000 | Loss: 0.00001434
Iteration 201/1000 | Loss: 0.00001434
Iteration 202/1000 | Loss: 0.00001434
Iteration 203/1000 | Loss: 0.00001433
Iteration 204/1000 | Loss: 0.00001433
Iteration 205/1000 | Loss: 0.00001433
Iteration 206/1000 | Loss: 0.00001433
Iteration 207/1000 | Loss: 0.00001433
Iteration 208/1000 | Loss: 0.00001433
Iteration 209/1000 | Loss: 0.00001433
Iteration 210/1000 | Loss: 0.00001433
Iteration 211/1000 | Loss: 0.00001433
Iteration 212/1000 | Loss: 0.00001432
Iteration 213/1000 | Loss: 0.00001432
Iteration 214/1000 | Loss: 0.00001432
Iteration 215/1000 | Loss: 0.00001432
Iteration 216/1000 | Loss: 0.00001432
Iteration 217/1000 | Loss: 0.00001432
Iteration 218/1000 | Loss: 0.00001432
Iteration 219/1000 | Loss: 0.00001432
Iteration 220/1000 | Loss: 0.00001432
Iteration 221/1000 | Loss: 0.00001432
Iteration 222/1000 | Loss: 0.00001431
Iteration 223/1000 | Loss: 0.00001431
Iteration 224/1000 | Loss: 0.00001431
Iteration 225/1000 | Loss: 0.00001431
Iteration 226/1000 | Loss: 0.00001431
Iteration 227/1000 | Loss: 0.00001431
Iteration 228/1000 | Loss: 0.00001431
Iteration 229/1000 | Loss: 0.00001430
Iteration 230/1000 | Loss: 0.00001430
Iteration 231/1000 | Loss: 0.00001430
Iteration 232/1000 | Loss: 0.00001430
Iteration 233/1000 | Loss: 0.00001430
Iteration 234/1000 | Loss: 0.00001430
Iteration 235/1000 | Loss: 0.00001429
Iteration 236/1000 | Loss: 0.00001429
Iteration 237/1000 | Loss: 0.00001429
Iteration 238/1000 | Loss: 0.00001429
Iteration 239/1000 | Loss: 0.00001429
Iteration 240/1000 | Loss: 0.00001429
Iteration 241/1000 | Loss: 0.00001429
Iteration 242/1000 | Loss: 0.00001429
Iteration 243/1000 | Loss: 0.00001429
Iteration 244/1000 | Loss: 0.00001429
Iteration 245/1000 | Loss: 0.00001429
Iteration 246/1000 | Loss: 0.00001429
Iteration 247/1000 | Loss: 0.00001429
Iteration 248/1000 | Loss: 0.00001428
Iteration 249/1000 | Loss: 0.00001428
Iteration 250/1000 | Loss: 0.00001428
Iteration 251/1000 | Loss: 0.00001428
Iteration 252/1000 | Loss: 0.00001428
Iteration 253/1000 | Loss: 0.00001428
Iteration 254/1000 | Loss: 0.00001428
Iteration 255/1000 | Loss: 0.00001428
Iteration 256/1000 | Loss: 0.00001428
Iteration 257/1000 | Loss: 0.00001428
Iteration 258/1000 | Loss: 0.00001428
Iteration 259/1000 | Loss: 0.00001428
Iteration 260/1000 | Loss: 0.00001428
Iteration 261/1000 | Loss: 0.00001428
Iteration 262/1000 | Loss: 0.00001428
Iteration 263/1000 | Loss: 0.00001428
Iteration 264/1000 | Loss: 0.00001428
Iteration 265/1000 | Loss: 0.00001428
Iteration 266/1000 | Loss: 0.00001428
Iteration 267/1000 | Loss: 0.00001428
Iteration 268/1000 | Loss: 0.00001428
Iteration 269/1000 | Loss: 0.00001428
Iteration 270/1000 | Loss: 0.00001428
Iteration 271/1000 | Loss: 0.00001428
Iteration 272/1000 | Loss: 0.00001428
Iteration 273/1000 | Loss: 0.00001428
Iteration 274/1000 | Loss: 0.00001428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 274. Stopping optimization.
Last 5 losses: [1.4278647540777456e-05, 1.4278647540777456e-05, 1.4278647540777456e-05, 1.4278647540777456e-05, 1.4278647540777456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4278647540777456e-05

Optimization complete. Final v2v error: 3.146310806274414 mm

Highest mean error: 3.2679691314697266 mm for frame 0

Lowest mean error: 2.9852969646453857 mm for frame 117

Saving results

Total time: 44.75941491127014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389983
Iteration 2/25 | Loss: 0.00113067
Iteration 3/25 | Loss: 0.00106113
Iteration 4/25 | Loss: 0.00105317
Iteration 5/25 | Loss: 0.00105117
Iteration 6/25 | Loss: 0.00105068
Iteration 7/25 | Loss: 0.00105068
Iteration 8/25 | Loss: 0.00105068
Iteration 9/25 | Loss: 0.00105068
Iteration 10/25 | Loss: 0.00105068
Iteration 11/25 | Loss: 0.00105068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001050683669745922, 0.001050683669745922, 0.001050683669745922, 0.001050683669745922, 0.001050683669745922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001050683669745922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34926784
Iteration 2/25 | Loss: 0.00092754
Iteration 3/25 | Loss: 0.00092753
Iteration 4/25 | Loss: 0.00092753
Iteration 5/25 | Loss: 0.00092753
Iteration 6/25 | Loss: 0.00092753
Iteration 7/25 | Loss: 0.00092753
Iteration 8/25 | Loss: 0.00092753
Iteration 9/25 | Loss: 0.00092753
Iteration 10/25 | Loss: 0.00092753
Iteration 11/25 | Loss: 0.00092752
Iteration 12/25 | Loss: 0.00092752
Iteration 13/25 | Loss: 0.00092752
Iteration 14/25 | Loss: 0.00092752
Iteration 15/25 | Loss: 0.00092752
Iteration 16/25 | Loss: 0.00092752
Iteration 17/25 | Loss: 0.00092752
Iteration 18/25 | Loss: 0.00092752
Iteration 19/25 | Loss: 0.00092752
Iteration 20/25 | Loss: 0.00092752
Iteration 21/25 | Loss: 0.00092752
Iteration 22/25 | Loss: 0.00092752
Iteration 23/25 | Loss: 0.00092752
Iteration 24/25 | Loss: 0.00092752
Iteration 25/25 | Loss: 0.00092752

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092752
Iteration 2/1000 | Loss: 0.00001728
Iteration 3/1000 | Loss: 0.00001068
Iteration 4/1000 | Loss: 0.00000963
Iteration 5/1000 | Loss: 0.00000908
Iteration 6/1000 | Loss: 0.00000876
Iteration 7/1000 | Loss: 0.00000842
Iteration 8/1000 | Loss: 0.00000826
Iteration 9/1000 | Loss: 0.00000825
Iteration 10/1000 | Loss: 0.00000824
Iteration 11/1000 | Loss: 0.00000824
Iteration 12/1000 | Loss: 0.00000823
Iteration 13/1000 | Loss: 0.00000823
Iteration 14/1000 | Loss: 0.00000822
Iteration 15/1000 | Loss: 0.00000820
Iteration 16/1000 | Loss: 0.00000820
Iteration 17/1000 | Loss: 0.00000819
Iteration 18/1000 | Loss: 0.00000818
Iteration 19/1000 | Loss: 0.00000817
Iteration 20/1000 | Loss: 0.00000817
Iteration 21/1000 | Loss: 0.00000816
Iteration 22/1000 | Loss: 0.00000811
Iteration 23/1000 | Loss: 0.00000809
Iteration 24/1000 | Loss: 0.00000801
Iteration 25/1000 | Loss: 0.00000801
Iteration 26/1000 | Loss: 0.00000798
Iteration 27/1000 | Loss: 0.00000795
Iteration 28/1000 | Loss: 0.00000794
Iteration 29/1000 | Loss: 0.00000793
Iteration 30/1000 | Loss: 0.00000789
Iteration 31/1000 | Loss: 0.00000787
Iteration 32/1000 | Loss: 0.00000787
Iteration 33/1000 | Loss: 0.00000787
Iteration 34/1000 | Loss: 0.00000787
Iteration 35/1000 | Loss: 0.00000786
Iteration 36/1000 | Loss: 0.00000786
Iteration 37/1000 | Loss: 0.00000786
Iteration 38/1000 | Loss: 0.00000785
Iteration 39/1000 | Loss: 0.00000785
Iteration 40/1000 | Loss: 0.00000785
Iteration 41/1000 | Loss: 0.00000784
Iteration 42/1000 | Loss: 0.00000784
Iteration 43/1000 | Loss: 0.00000782
Iteration 44/1000 | Loss: 0.00000778
Iteration 45/1000 | Loss: 0.00000777
Iteration 46/1000 | Loss: 0.00000777
Iteration 47/1000 | Loss: 0.00000776
Iteration 48/1000 | Loss: 0.00000776
Iteration 49/1000 | Loss: 0.00000776
Iteration 50/1000 | Loss: 0.00000776
Iteration 51/1000 | Loss: 0.00000775
Iteration 52/1000 | Loss: 0.00000775
Iteration 53/1000 | Loss: 0.00000775
Iteration 54/1000 | Loss: 0.00000775
Iteration 55/1000 | Loss: 0.00000775
Iteration 56/1000 | Loss: 0.00000774
Iteration 57/1000 | Loss: 0.00000774
Iteration 58/1000 | Loss: 0.00000774
Iteration 59/1000 | Loss: 0.00000773
Iteration 60/1000 | Loss: 0.00000773
Iteration 61/1000 | Loss: 0.00000772
Iteration 62/1000 | Loss: 0.00000772
Iteration 63/1000 | Loss: 0.00000771
Iteration 64/1000 | Loss: 0.00000771
Iteration 65/1000 | Loss: 0.00000771
Iteration 66/1000 | Loss: 0.00000770
Iteration 67/1000 | Loss: 0.00000769
Iteration 68/1000 | Loss: 0.00000769
Iteration 69/1000 | Loss: 0.00000769
Iteration 70/1000 | Loss: 0.00000768
Iteration 71/1000 | Loss: 0.00000768
Iteration 72/1000 | Loss: 0.00000768
Iteration 73/1000 | Loss: 0.00000767
Iteration 74/1000 | Loss: 0.00000767
Iteration 75/1000 | Loss: 0.00000767
Iteration 76/1000 | Loss: 0.00000767
Iteration 77/1000 | Loss: 0.00000767
Iteration 78/1000 | Loss: 0.00000766
Iteration 79/1000 | Loss: 0.00000766
Iteration 80/1000 | Loss: 0.00000766
Iteration 81/1000 | Loss: 0.00000766
Iteration 82/1000 | Loss: 0.00000765
Iteration 83/1000 | Loss: 0.00000765
Iteration 84/1000 | Loss: 0.00000765
Iteration 85/1000 | Loss: 0.00000765
Iteration 86/1000 | Loss: 0.00000764
Iteration 87/1000 | Loss: 0.00000764
Iteration 88/1000 | Loss: 0.00000764
Iteration 89/1000 | Loss: 0.00000764
Iteration 90/1000 | Loss: 0.00000763
Iteration 91/1000 | Loss: 0.00000763
Iteration 92/1000 | Loss: 0.00000762
Iteration 93/1000 | Loss: 0.00000762
Iteration 94/1000 | Loss: 0.00000762
Iteration 95/1000 | Loss: 0.00000762
Iteration 96/1000 | Loss: 0.00000762
Iteration 97/1000 | Loss: 0.00000762
Iteration 98/1000 | Loss: 0.00000762
Iteration 99/1000 | Loss: 0.00000762
Iteration 100/1000 | Loss: 0.00000762
Iteration 101/1000 | Loss: 0.00000762
Iteration 102/1000 | Loss: 0.00000762
Iteration 103/1000 | Loss: 0.00000762
Iteration 104/1000 | Loss: 0.00000762
Iteration 105/1000 | Loss: 0.00000762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [7.6242176874075085e-06, 7.6242176874075085e-06, 7.6242176874075085e-06, 7.6242176874075085e-06, 7.6242176874075085e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.6242176874075085e-06

Optimization complete. Final v2v error: 2.3930699825286865 mm

Highest mean error: 2.517642021179199 mm for frame 84

Lowest mean error: 2.3318824768066406 mm for frame 149

Saving results

Total time: 30.067444562911987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030693
Iteration 2/25 | Loss: 0.00164776
Iteration 3/25 | Loss: 0.00135213
Iteration 4/25 | Loss: 0.00131898
Iteration 5/25 | Loss: 0.00129457
Iteration 6/25 | Loss: 0.00125094
Iteration 7/25 | Loss: 0.00118253
Iteration 8/25 | Loss: 0.00117551
Iteration 9/25 | Loss: 0.00114335
Iteration 10/25 | Loss: 0.00112755
Iteration 11/25 | Loss: 0.00112508
Iteration 12/25 | Loss: 0.00111864
Iteration 13/25 | Loss: 0.00111423
Iteration 14/25 | Loss: 0.00111878
Iteration 15/25 | Loss: 0.00111946
Iteration 16/25 | Loss: 0.00111387
Iteration 17/25 | Loss: 0.00111489
Iteration 18/25 | Loss: 0.00111628
Iteration 19/25 | Loss: 0.00111309
Iteration 20/25 | Loss: 0.00110744
Iteration 21/25 | Loss: 0.00110645
Iteration 22/25 | Loss: 0.00110615
Iteration 23/25 | Loss: 0.00110612
Iteration 24/25 | Loss: 0.00110611
Iteration 25/25 | Loss: 0.00110611

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38211966
Iteration 2/25 | Loss: 0.00158357
Iteration 3/25 | Loss: 0.00112978
Iteration 4/25 | Loss: 0.00112977
Iteration 5/25 | Loss: 0.00112977
Iteration 6/25 | Loss: 0.00112977
Iteration 7/25 | Loss: 0.00112977
Iteration 8/25 | Loss: 0.00112977
Iteration 9/25 | Loss: 0.00112977
Iteration 10/25 | Loss: 0.00112977
Iteration 11/25 | Loss: 0.00112977
Iteration 12/25 | Loss: 0.00112977
Iteration 13/25 | Loss: 0.00112977
Iteration 14/25 | Loss: 0.00112977
Iteration 15/25 | Loss: 0.00112977
Iteration 16/25 | Loss: 0.00112977
Iteration 17/25 | Loss: 0.00112977
Iteration 18/25 | Loss: 0.00112977
Iteration 19/25 | Loss: 0.00112977
Iteration 20/25 | Loss: 0.00112977
Iteration 21/25 | Loss: 0.00112977
Iteration 22/25 | Loss: 0.00112977
Iteration 23/25 | Loss: 0.00112977
Iteration 24/25 | Loss: 0.00112977
Iteration 25/25 | Loss: 0.00112977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112977
Iteration 2/1000 | Loss: 0.00015774
Iteration 3/1000 | Loss: 0.00004309
Iteration 4/1000 | Loss: 0.00033547
Iteration 5/1000 | Loss: 0.00004501
Iteration 6/1000 | Loss: 0.00013589
Iteration 7/1000 | Loss: 0.00003045
Iteration 8/1000 | Loss: 0.00004313
Iteration 9/1000 | Loss: 0.00003629
Iteration 10/1000 | Loss: 0.00005286
Iteration 11/1000 | Loss: 0.00003810
Iteration 12/1000 | Loss: 0.00003500
Iteration 13/1000 | Loss: 0.00003558
Iteration 14/1000 | Loss: 0.00003851
Iteration 15/1000 | Loss: 0.00003671
Iteration 16/1000 | Loss: 0.00003156
Iteration 17/1000 | Loss: 0.00004987
Iteration 18/1000 | Loss: 0.00003454
Iteration 19/1000 | Loss: 0.00004813
Iteration 20/1000 | Loss: 0.00002634
Iteration 21/1000 | Loss: 0.00002566
Iteration 22/1000 | Loss: 0.00004015
Iteration 23/1000 | Loss: 0.00005352
Iteration 24/1000 | Loss: 0.00003885
Iteration 25/1000 | Loss: 0.00006161
Iteration 26/1000 | Loss: 0.00004904
Iteration 27/1000 | Loss: 0.00005124
Iteration 28/1000 | Loss: 0.00003938
Iteration 29/1000 | Loss: 0.00027184
Iteration 30/1000 | Loss: 0.00380874
Iteration 31/1000 | Loss: 0.00101295
Iteration 32/1000 | Loss: 0.00361344
Iteration 33/1000 | Loss: 0.00024476
Iteration 34/1000 | Loss: 0.00028257
Iteration 35/1000 | Loss: 0.00018182
Iteration 36/1000 | Loss: 0.00011304
Iteration 37/1000 | Loss: 0.00019082
Iteration 38/1000 | Loss: 0.00007518
Iteration 39/1000 | Loss: 0.00014502
Iteration 40/1000 | Loss: 0.00013993
Iteration 41/1000 | Loss: 0.00025051
Iteration 42/1000 | Loss: 0.00027652
Iteration 43/1000 | Loss: 0.00025328
Iteration 44/1000 | Loss: 0.00003597
Iteration 45/1000 | Loss: 0.00014818
Iteration 46/1000 | Loss: 0.00015396
Iteration 47/1000 | Loss: 0.00012672
Iteration 48/1000 | Loss: 0.00004121
Iteration 49/1000 | Loss: 0.00016693
Iteration 50/1000 | Loss: 0.00010657
Iteration 51/1000 | Loss: 0.00016982
Iteration 52/1000 | Loss: 0.00018076
Iteration 53/1000 | Loss: 0.00016112
Iteration 54/1000 | Loss: 0.00012432
Iteration 55/1000 | Loss: 0.00012689
Iteration 56/1000 | Loss: 0.00013579
Iteration 57/1000 | Loss: 0.00010465
Iteration 58/1000 | Loss: 0.00016754
Iteration 59/1000 | Loss: 0.00010828
Iteration 60/1000 | Loss: 0.00030634
Iteration 61/1000 | Loss: 0.00006185
Iteration 62/1000 | Loss: 0.00020611
Iteration 63/1000 | Loss: 0.00016338
Iteration 64/1000 | Loss: 0.00001557
Iteration 65/1000 | Loss: 0.00019064
Iteration 66/1000 | Loss: 0.00026810
Iteration 67/1000 | Loss: 0.00018611
Iteration 68/1000 | Loss: 0.00029278
Iteration 69/1000 | Loss: 0.00012874
Iteration 70/1000 | Loss: 0.00005884
Iteration 71/1000 | Loss: 0.00018066
Iteration 72/1000 | Loss: 0.00016502
Iteration 73/1000 | Loss: 0.00024578
Iteration 74/1000 | Loss: 0.00017769
Iteration 75/1000 | Loss: 0.00014394
Iteration 76/1000 | Loss: 0.00019138
Iteration 77/1000 | Loss: 0.00012329
Iteration 78/1000 | Loss: 0.00017570
Iteration 79/1000 | Loss: 0.00008979
Iteration 80/1000 | Loss: 0.00001574
Iteration 81/1000 | Loss: 0.00017739
Iteration 82/1000 | Loss: 0.00024109
Iteration 83/1000 | Loss: 0.00014060
Iteration 84/1000 | Loss: 0.00018545
Iteration 85/1000 | Loss: 0.00013107
Iteration 86/1000 | Loss: 0.00014128
Iteration 87/1000 | Loss: 0.00024209
Iteration 88/1000 | Loss: 0.00007125
Iteration 89/1000 | Loss: 0.00013134
Iteration 90/1000 | Loss: 0.00013962
Iteration 91/1000 | Loss: 0.00027553
Iteration 92/1000 | Loss: 0.00016694
Iteration 93/1000 | Loss: 0.00023576
Iteration 94/1000 | Loss: 0.00021138
Iteration 95/1000 | Loss: 0.00013155
Iteration 96/1000 | Loss: 0.00015809
Iteration 97/1000 | Loss: 0.00005494
Iteration 98/1000 | Loss: 0.00016521
Iteration 99/1000 | Loss: 0.00015541
Iteration 100/1000 | Loss: 0.00009846
Iteration 101/1000 | Loss: 0.00012408
Iteration 102/1000 | Loss: 0.00010635
Iteration 103/1000 | Loss: 0.00005695
Iteration 104/1000 | Loss: 0.00015072
Iteration 105/1000 | Loss: 0.00016594
Iteration 106/1000 | Loss: 0.00013992
Iteration 107/1000 | Loss: 0.00016175
Iteration 108/1000 | Loss: 0.00012306
Iteration 109/1000 | Loss: 0.00014951
Iteration 110/1000 | Loss: 0.00011424
Iteration 111/1000 | Loss: 0.00007952
Iteration 112/1000 | Loss: 0.00015566
Iteration 113/1000 | Loss: 0.00005792
Iteration 114/1000 | Loss: 0.00004121
Iteration 115/1000 | Loss: 0.00018001
Iteration 116/1000 | Loss: 0.00014568
Iteration 117/1000 | Loss: 0.00002708
Iteration 118/1000 | Loss: 0.00023326
Iteration 119/1000 | Loss: 0.00018561
Iteration 120/1000 | Loss: 0.00012346
Iteration 121/1000 | Loss: 0.00011347
Iteration 122/1000 | Loss: 0.00018062
Iteration 123/1000 | Loss: 0.00016290
Iteration 124/1000 | Loss: 0.00017319
Iteration 125/1000 | Loss: 0.00017696
Iteration 126/1000 | Loss: 0.00017724
Iteration 127/1000 | Loss: 0.00022822
Iteration 128/1000 | Loss: 0.00020809
Iteration 129/1000 | Loss: 0.00029606
Iteration 130/1000 | Loss: 0.00015196
Iteration 131/1000 | Loss: 0.00017759
Iteration 132/1000 | Loss: 0.00012821
Iteration 133/1000 | Loss: 0.00016940
Iteration 134/1000 | Loss: 0.00012478
Iteration 135/1000 | Loss: 0.00025013
Iteration 136/1000 | Loss: 0.00001973
Iteration 137/1000 | Loss: 0.00003299
Iteration 138/1000 | Loss: 0.00001934
Iteration 139/1000 | Loss: 0.00004435
Iteration 140/1000 | Loss: 0.00002537
Iteration 141/1000 | Loss: 0.00001438
Iteration 142/1000 | Loss: 0.00001310
Iteration 143/1000 | Loss: 0.00001195
Iteration 144/1000 | Loss: 0.00001090
Iteration 145/1000 | Loss: 0.00001042
Iteration 146/1000 | Loss: 0.00001030
Iteration 147/1000 | Loss: 0.00001005
Iteration 148/1000 | Loss: 0.00000987
Iteration 149/1000 | Loss: 0.00009481
Iteration 150/1000 | Loss: 0.00001053
Iteration 151/1000 | Loss: 0.00002498
Iteration 152/1000 | Loss: 0.00001347
Iteration 153/1000 | Loss: 0.00001426
Iteration 154/1000 | Loss: 0.00000975
Iteration 155/1000 | Loss: 0.00000975
Iteration 156/1000 | Loss: 0.00000975
Iteration 157/1000 | Loss: 0.00000975
Iteration 158/1000 | Loss: 0.00000975
Iteration 159/1000 | Loss: 0.00000974
Iteration 160/1000 | Loss: 0.00000972
Iteration 161/1000 | Loss: 0.00000970
Iteration 162/1000 | Loss: 0.00000965
Iteration 163/1000 | Loss: 0.00000964
Iteration 164/1000 | Loss: 0.00000960
Iteration 165/1000 | Loss: 0.00000960
Iteration 166/1000 | Loss: 0.00000959
Iteration 167/1000 | Loss: 0.00000959
Iteration 168/1000 | Loss: 0.00000958
Iteration 169/1000 | Loss: 0.00000957
Iteration 170/1000 | Loss: 0.00000956
Iteration 171/1000 | Loss: 0.00000955
Iteration 172/1000 | Loss: 0.00000955
Iteration 173/1000 | Loss: 0.00000955
Iteration 174/1000 | Loss: 0.00000955
Iteration 175/1000 | Loss: 0.00000954
Iteration 176/1000 | Loss: 0.00000954
Iteration 177/1000 | Loss: 0.00000954
Iteration 178/1000 | Loss: 0.00000954
Iteration 179/1000 | Loss: 0.00000954
Iteration 180/1000 | Loss: 0.00000954
Iteration 181/1000 | Loss: 0.00000954
Iteration 182/1000 | Loss: 0.00000954
Iteration 183/1000 | Loss: 0.00000953
Iteration 184/1000 | Loss: 0.00000952
Iteration 185/1000 | Loss: 0.00000952
Iteration 186/1000 | Loss: 0.00000952
Iteration 187/1000 | Loss: 0.00000952
Iteration 188/1000 | Loss: 0.00000952
Iteration 189/1000 | Loss: 0.00000952
Iteration 190/1000 | Loss: 0.00000951
Iteration 191/1000 | Loss: 0.00000951
Iteration 192/1000 | Loss: 0.00000951
Iteration 193/1000 | Loss: 0.00000951
Iteration 194/1000 | Loss: 0.00000951
Iteration 195/1000 | Loss: 0.00000951
Iteration 196/1000 | Loss: 0.00000951
Iteration 197/1000 | Loss: 0.00000951
Iteration 198/1000 | Loss: 0.00000950
Iteration 199/1000 | Loss: 0.00000950
Iteration 200/1000 | Loss: 0.00000950
Iteration 201/1000 | Loss: 0.00000950
Iteration 202/1000 | Loss: 0.00000950
Iteration 203/1000 | Loss: 0.00000950
Iteration 204/1000 | Loss: 0.00000950
Iteration 205/1000 | Loss: 0.00000950
Iteration 206/1000 | Loss: 0.00000949
Iteration 207/1000 | Loss: 0.00000949
Iteration 208/1000 | Loss: 0.00000949
Iteration 209/1000 | Loss: 0.00000949
Iteration 210/1000 | Loss: 0.00000949
Iteration 211/1000 | Loss: 0.00000949
Iteration 212/1000 | Loss: 0.00000949
Iteration 213/1000 | Loss: 0.00000949
Iteration 214/1000 | Loss: 0.00000949
Iteration 215/1000 | Loss: 0.00000949
Iteration 216/1000 | Loss: 0.00000949
Iteration 217/1000 | Loss: 0.00000949
Iteration 218/1000 | Loss: 0.00000949
Iteration 219/1000 | Loss: 0.00000949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [9.492915523878764e-06, 9.492915523878764e-06, 9.492915523878764e-06, 9.492915523878764e-06, 9.492915523878764e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.492915523878764e-06

Optimization complete. Final v2v error: 2.617642641067505 mm

Highest mean error: 3.4184365272521973 mm for frame 67

Lowest mean error: 2.405316114425659 mm for frame 32

Saving results

Total time: 259.9506161212921
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021794
Iteration 2/25 | Loss: 0.00220289
Iteration 3/25 | Loss: 0.00168396
Iteration 4/25 | Loss: 0.00148017
Iteration 5/25 | Loss: 0.00153701
Iteration 6/25 | Loss: 0.00135542
Iteration 7/25 | Loss: 0.00130771
Iteration 8/25 | Loss: 0.00129268
Iteration 9/25 | Loss: 0.00128404
Iteration 10/25 | Loss: 0.00127494
Iteration 11/25 | Loss: 0.00125957
Iteration 12/25 | Loss: 0.00125082
Iteration 13/25 | Loss: 0.00124384
Iteration 14/25 | Loss: 0.00124472
Iteration 15/25 | Loss: 0.00123857
Iteration 16/25 | Loss: 0.00123513
Iteration 17/25 | Loss: 0.00123931
Iteration 18/25 | Loss: 0.00126293
Iteration 19/25 | Loss: 0.00122671
Iteration 20/25 | Loss: 0.00123258
Iteration 21/25 | Loss: 0.00122919
Iteration 22/25 | Loss: 0.00122869
Iteration 23/25 | Loss: 0.00122640
Iteration 24/25 | Loss: 0.00122229
Iteration 25/25 | Loss: 0.00122307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34834051
Iteration 2/25 | Loss: 0.00177256
Iteration 3/25 | Loss: 0.00157155
Iteration 4/25 | Loss: 0.00156300
Iteration 5/25 | Loss: 0.00156300
Iteration 6/25 | Loss: 0.00156300
Iteration 7/25 | Loss: 0.00156300
Iteration 8/25 | Loss: 0.00156300
Iteration 9/25 | Loss: 0.00156300
Iteration 10/25 | Loss: 0.00156300
Iteration 11/25 | Loss: 0.00156300
Iteration 12/25 | Loss: 0.00156300
Iteration 13/25 | Loss: 0.00156300
Iteration 14/25 | Loss: 0.00156300
Iteration 15/25 | Loss: 0.00156299
Iteration 16/25 | Loss: 0.00156299
Iteration 17/25 | Loss: 0.00156299
Iteration 18/25 | Loss: 0.00156299
Iteration 19/25 | Loss: 0.00156299
Iteration 20/25 | Loss: 0.00156299
Iteration 21/25 | Loss: 0.00156299
Iteration 22/25 | Loss: 0.00156299
Iteration 23/25 | Loss: 0.00156299
Iteration 24/25 | Loss: 0.00156299
Iteration 25/25 | Loss: 0.00156299

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156299
Iteration 2/1000 | Loss: 0.00041733
Iteration 3/1000 | Loss: 0.00059639
Iteration 4/1000 | Loss: 0.00060311
Iteration 5/1000 | Loss: 0.00023645
Iteration 6/1000 | Loss: 0.00027068
Iteration 7/1000 | Loss: 0.00013363
Iteration 8/1000 | Loss: 0.00021859
Iteration 9/1000 | Loss: 0.00048657
Iteration 10/1000 | Loss: 0.00011762
Iteration 11/1000 | Loss: 0.00030545
Iteration 12/1000 | Loss: 0.00013632
Iteration 13/1000 | Loss: 0.00010187
Iteration 14/1000 | Loss: 0.00009034
Iteration 15/1000 | Loss: 0.00022467
Iteration 16/1000 | Loss: 0.00015410
Iteration 17/1000 | Loss: 0.00011691
Iteration 18/1000 | Loss: 0.00010719
Iteration 19/1000 | Loss: 0.00010031
Iteration 20/1000 | Loss: 0.00009772
Iteration 21/1000 | Loss: 0.00025274
Iteration 22/1000 | Loss: 0.00010707
Iteration 23/1000 | Loss: 0.00020980
Iteration 24/1000 | Loss: 0.00096564
Iteration 25/1000 | Loss: 0.00008610
Iteration 26/1000 | Loss: 0.00011004
Iteration 27/1000 | Loss: 0.00009076
Iteration 28/1000 | Loss: 0.00014509
Iteration 29/1000 | Loss: 0.00011384
Iteration 30/1000 | Loss: 0.00011364
Iteration 31/1000 | Loss: 0.00008304
Iteration 32/1000 | Loss: 0.00016053
Iteration 33/1000 | Loss: 0.00010451
Iteration 34/1000 | Loss: 0.00020810
Iteration 35/1000 | Loss: 0.00047443
Iteration 36/1000 | Loss: 0.00021221
Iteration 37/1000 | Loss: 0.00033760
Iteration 38/1000 | Loss: 0.00037989
Iteration 39/1000 | Loss: 0.00026911
Iteration 40/1000 | Loss: 0.00012666
Iteration 41/1000 | Loss: 0.00017009
Iteration 42/1000 | Loss: 0.00024003
Iteration 43/1000 | Loss: 0.00036161
Iteration 44/1000 | Loss: 0.00035303
Iteration 45/1000 | Loss: 0.00024972
Iteration 46/1000 | Loss: 0.00023488
Iteration 47/1000 | Loss: 0.00046250
Iteration 48/1000 | Loss: 0.00029918
Iteration 49/1000 | Loss: 0.00026799
Iteration 50/1000 | Loss: 0.00011276
Iteration 51/1000 | Loss: 0.00018516
Iteration 52/1000 | Loss: 0.00009734
Iteration 53/1000 | Loss: 0.00025941
Iteration 54/1000 | Loss: 0.00019410
Iteration 55/1000 | Loss: 0.00050555
Iteration 56/1000 | Loss: 0.00015086
Iteration 57/1000 | Loss: 0.00011477
Iteration 58/1000 | Loss: 0.00013364
Iteration 59/1000 | Loss: 0.00022595
Iteration 60/1000 | Loss: 0.00034290
Iteration 61/1000 | Loss: 0.00016214
Iteration 62/1000 | Loss: 0.00016436
Iteration 63/1000 | Loss: 0.00027778
Iteration 64/1000 | Loss: 0.00027503
Iteration 65/1000 | Loss: 0.00023414
Iteration 66/1000 | Loss: 0.00025633
Iteration 67/1000 | Loss: 0.00010824
Iteration 68/1000 | Loss: 0.00011691
Iteration 69/1000 | Loss: 0.00012090
Iteration 70/1000 | Loss: 0.00010708
Iteration 71/1000 | Loss: 0.00010298
Iteration 72/1000 | Loss: 0.00009848
Iteration 73/1000 | Loss: 0.00022160
Iteration 74/1000 | Loss: 0.00014113
Iteration 75/1000 | Loss: 0.00011419
Iteration 76/1000 | Loss: 0.00009696
Iteration 77/1000 | Loss: 0.00009033
Iteration 78/1000 | Loss: 0.00007503
Iteration 79/1000 | Loss: 0.00008857
Iteration 80/1000 | Loss: 0.00007142
Iteration 81/1000 | Loss: 0.00016335
Iteration 82/1000 | Loss: 0.00032518
Iteration 83/1000 | Loss: 0.00020865
Iteration 84/1000 | Loss: 0.00028738
Iteration 85/1000 | Loss: 0.00008299
Iteration 86/1000 | Loss: 0.00014237
Iteration 87/1000 | Loss: 0.00007675
Iteration 88/1000 | Loss: 0.00007689
Iteration 89/1000 | Loss: 0.00013735
Iteration 90/1000 | Loss: 0.00019437
Iteration 91/1000 | Loss: 0.00025607
Iteration 92/1000 | Loss: 0.00014365
Iteration 93/1000 | Loss: 0.00009790
Iteration 94/1000 | Loss: 0.00020702
Iteration 95/1000 | Loss: 0.00034301
Iteration 96/1000 | Loss: 0.00016572
Iteration 97/1000 | Loss: 0.00017159
Iteration 98/1000 | Loss: 0.00024212
Iteration 99/1000 | Loss: 0.00008957
Iteration 100/1000 | Loss: 0.00015568
Iteration 101/1000 | Loss: 0.00008054
Iteration 102/1000 | Loss: 0.00008514
Iteration 103/1000 | Loss: 0.00007166
Iteration 104/1000 | Loss: 0.00007729
Iteration 105/1000 | Loss: 0.00007576
Iteration 106/1000 | Loss: 0.00022812
Iteration 107/1000 | Loss: 0.00012714
Iteration 108/1000 | Loss: 0.00027662
Iteration 109/1000 | Loss: 0.00010975
Iteration 110/1000 | Loss: 0.00007575
Iteration 111/1000 | Loss: 0.00011442
Iteration 112/1000 | Loss: 0.00008855
Iteration 113/1000 | Loss: 0.00013968
Iteration 114/1000 | Loss: 0.00007077
Iteration 115/1000 | Loss: 0.00012943
Iteration 116/1000 | Loss: 0.00035285
Iteration 117/1000 | Loss: 0.00153967
Iteration 118/1000 | Loss: 0.00085146
Iteration 119/1000 | Loss: 0.00071921
Iteration 120/1000 | Loss: 0.00041828
Iteration 121/1000 | Loss: 0.00067024
Iteration 122/1000 | Loss: 0.00020680
Iteration 123/1000 | Loss: 0.00021911
Iteration 124/1000 | Loss: 0.00013075
Iteration 125/1000 | Loss: 0.00013367
Iteration 126/1000 | Loss: 0.00010482
Iteration 127/1000 | Loss: 0.00015849
Iteration 128/1000 | Loss: 0.00006090
Iteration 129/1000 | Loss: 0.00040769
Iteration 130/1000 | Loss: 0.00016075
Iteration 131/1000 | Loss: 0.00016693
Iteration 132/1000 | Loss: 0.00021025
Iteration 133/1000 | Loss: 0.00008103
Iteration 134/1000 | Loss: 0.00014430
Iteration 135/1000 | Loss: 0.00034666
Iteration 136/1000 | Loss: 0.00005435
Iteration 137/1000 | Loss: 0.00012240
Iteration 138/1000 | Loss: 0.00005097
Iteration 139/1000 | Loss: 0.00005019
Iteration 140/1000 | Loss: 0.00017317
Iteration 141/1000 | Loss: 0.00005476
Iteration 142/1000 | Loss: 0.00012756
Iteration 143/1000 | Loss: 0.00025986
Iteration 144/1000 | Loss: 0.00108145
Iteration 145/1000 | Loss: 0.00014778
Iteration 146/1000 | Loss: 0.00013081
Iteration 147/1000 | Loss: 0.00005578
Iteration 148/1000 | Loss: 0.00005077
Iteration 149/1000 | Loss: 0.00018785
Iteration 150/1000 | Loss: 0.00004743
Iteration 151/1000 | Loss: 0.00004634
Iteration 152/1000 | Loss: 0.00004520
Iteration 153/1000 | Loss: 0.00004470
Iteration 154/1000 | Loss: 0.00006891
Iteration 155/1000 | Loss: 0.00004412
Iteration 156/1000 | Loss: 0.00004406
Iteration 157/1000 | Loss: 0.00028899
Iteration 158/1000 | Loss: 0.00011786
Iteration 159/1000 | Loss: 0.00012312
Iteration 160/1000 | Loss: 0.00004545
Iteration 161/1000 | Loss: 0.00006152
Iteration 162/1000 | Loss: 0.00004391
Iteration 163/1000 | Loss: 0.00024053
Iteration 164/1000 | Loss: 0.00004757
Iteration 165/1000 | Loss: 0.00009578
Iteration 166/1000 | Loss: 0.00005171
Iteration 167/1000 | Loss: 0.00004403
Iteration 168/1000 | Loss: 0.00007946
Iteration 169/1000 | Loss: 0.00004263
Iteration 170/1000 | Loss: 0.00016427
Iteration 171/1000 | Loss: 0.00013858
Iteration 172/1000 | Loss: 0.00025396
Iteration 173/1000 | Loss: 0.00004463
Iteration 174/1000 | Loss: 0.00004307
Iteration 175/1000 | Loss: 0.00014063
Iteration 176/1000 | Loss: 0.00054640
Iteration 177/1000 | Loss: 0.00008609
Iteration 178/1000 | Loss: 0.00030204
Iteration 179/1000 | Loss: 0.00047250
Iteration 180/1000 | Loss: 0.00004380
Iteration 181/1000 | Loss: 0.00004167
Iteration 182/1000 | Loss: 0.00004108
Iteration 183/1000 | Loss: 0.00021729
Iteration 184/1000 | Loss: 0.00008783
Iteration 185/1000 | Loss: 0.00005578
Iteration 186/1000 | Loss: 0.00004061
Iteration 187/1000 | Loss: 0.00004058
Iteration 188/1000 | Loss: 0.00004055
Iteration 189/1000 | Loss: 0.00004053
Iteration 190/1000 | Loss: 0.00004053
Iteration 191/1000 | Loss: 0.00004052
Iteration 192/1000 | Loss: 0.00004052
Iteration 193/1000 | Loss: 0.00004052
Iteration 194/1000 | Loss: 0.00004052
Iteration 195/1000 | Loss: 0.00004052
Iteration 196/1000 | Loss: 0.00004052
Iteration 197/1000 | Loss: 0.00004051
Iteration 198/1000 | Loss: 0.00004051
Iteration 199/1000 | Loss: 0.00004051
Iteration 200/1000 | Loss: 0.00004050
Iteration 201/1000 | Loss: 0.00004050
Iteration 202/1000 | Loss: 0.00004050
Iteration 203/1000 | Loss: 0.00004049
Iteration 204/1000 | Loss: 0.00004049
Iteration 205/1000 | Loss: 0.00004048
Iteration 206/1000 | Loss: 0.00004045
Iteration 207/1000 | Loss: 0.00004045
Iteration 208/1000 | Loss: 0.00004044
Iteration 209/1000 | Loss: 0.00004043
Iteration 210/1000 | Loss: 0.00006623
Iteration 211/1000 | Loss: 0.00004051
Iteration 212/1000 | Loss: 0.00004029
Iteration 213/1000 | Loss: 0.00004029
Iteration 214/1000 | Loss: 0.00004029
Iteration 215/1000 | Loss: 0.00004029
Iteration 216/1000 | Loss: 0.00004029
Iteration 217/1000 | Loss: 0.00004029
Iteration 218/1000 | Loss: 0.00004029
Iteration 219/1000 | Loss: 0.00004029
Iteration 220/1000 | Loss: 0.00004029
Iteration 221/1000 | Loss: 0.00004029
Iteration 222/1000 | Loss: 0.00004028
Iteration 223/1000 | Loss: 0.00004028
Iteration 224/1000 | Loss: 0.00004028
Iteration 225/1000 | Loss: 0.00004027
Iteration 226/1000 | Loss: 0.00004027
Iteration 227/1000 | Loss: 0.00004027
Iteration 228/1000 | Loss: 0.00004027
Iteration 229/1000 | Loss: 0.00004027
Iteration 230/1000 | Loss: 0.00004027
Iteration 231/1000 | Loss: 0.00004027
Iteration 232/1000 | Loss: 0.00004027
Iteration 233/1000 | Loss: 0.00004027
Iteration 234/1000 | Loss: 0.00004027
Iteration 235/1000 | Loss: 0.00004026
Iteration 236/1000 | Loss: 0.00004026
Iteration 237/1000 | Loss: 0.00004026
Iteration 238/1000 | Loss: 0.00004026
Iteration 239/1000 | Loss: 0.00004026
Iteration 240/1000 | Loss: 0.00004026
Iteration 241/1000 | Loss: 0.00004026
Iteration 242/1000 | Loss: 0.00004026
Iteration 243/1000 | Loss: 0.00004025
Iteration 244/1000 | Loss: 0.00004025
Iteration 245/1000 | Loss: 0.00004025
Iteration 246/1000 | Loss: 0.00004025
Iteration 247/1000 | Loss: 0.00004025
Iteration 248/1000 | Loss: 0.00004024
Iteration 249/1000 | Loss: 0.00004024
Iteration 250/1000 | Loss: 0.00004024
Iteration 251/1000 | Loss: 0.00004024
Iteration 252/1000 | Loss: 0.00004023
Iteration 253/1000 | Loss: 0.00004023
Iteration 254/1000 | Loss: 0.00004023
Iteration 255/1000 | Loss: 0.00004023
Iteration 256/1000 | Loss: 0.00004023
Iteration 257/1000 | Loss: 0.00004023
Iteration 258/1000 | Loss: 0.00004023
Iteration 259/1000 | Loss: 0.00004023
Iteration 260/1000 | Loss: 0.00004022
Iteration 261/1000 | Loss: 0.00004022
Iteration 262/1000 | Loss: 0.00004022
Iteration 263/1000 | Loss: 0.00004022
Iteration 264/1000 | Loss: 0.00004022
Iteration 265/1000 | Loss: 0.00004022
Iteration 266/1000 | Loss: 0.00004022
Iteration 267/1000 | Loss: 0.00004021
Iteration 268/1000 | Loss: 0.00004021
Iteration 269/1000 | Loss: 0.00004021
Iteration 270/1000 | Loss: 0.00004020
Iteration 271/1000 | Loss: 0.00004020
Iteration 272/1000 | Loss: 0.00004020
Iteration 273/1000 | Loss: 0.00004019
Iteration 274/1000 | Loss: 0.00004019
Iteration 275/1000 | Loss: 0.00009017
Iteration 276/1000 | Loss: 0.00004029
Iteration 277/1000 | Loss: 0.00004018
Iteration 278/1000 | Loss: 0.00004014
Iteration 279/1000 | Loss: 0.00004014
Iteration 280/1000 | Loss: 0.00007017
Iteration 281/1000 | Loss: 0.00004457
Iteration 282/1000 | Loss: 0.00004013
Iteration 283/1000 | Loss: 0.00004013
Iteration 284/1000 | Loss: 0.00004013
Iteration 285/1000 | Loss: 0.00004013
Iteration 286/1000 | Loss: 0.00004013
Iteration 287/1000 | Loss: 0.00004013
Iteration 288/1000 | Loss: 0.00004013
Iteration 289/1000 | Loss: 0.00004013
Iteration 290/1000 | Loss: 0.00004013
Iteration 291/1000 | Loss: 0.00004013
Iteration 292/1000 | Loss: 0.00004012
Iteration 293/1000 | Loss: 0.00004012
Iteration 294/1000 | Loss: 0.00004012
Iteration 295/1000 | Loss: 0.00004012
Iteration 296/1000 | Loss: 0.00004012
Iteration 297/1000 | Loss: 0.00004951
Iteration 298/1000 | Loss: 0.00004013
Iteration 299/1000 | Loss: 0.00004013
Iteration 300/1000 | Loss: 0.00004013
Iteration 301/1000 | Loss: 0.00004012
Iteration 302/1000 | Loss: 0.00004012
Iteration 303/1000 | Loss: 0.00004012
Iteration 304/1000 | Loss: 0.00004012
Iteration 305/1000 | Loss: 0.00004012
Iteration 306/1000 | Loss: 0.00004012
Iteration 307/1000 | Loss: 0.00004012
Iteration 308/1000 | Loss: 0.00004012
Iteration 309/1000 | Loss: 0.00004012
Iteration 310/1000 | Loss: 0.00004012
Iteration 311/1000 | Loss: 0.00004012
Iteration 312/1000 | Loss: 0.00004012
Iteration 313/1000 | Loss: 0.00004012
Iteration 314/1000 | Loss: 0.00004012
Iteration 315/1000 | Loss: 0.00004012
Iteration 316/1000 | Loss: 0.00004012
Iteration 317/1000 | Loss: 0.00004012
Iteration 318/1000 | Loss: 0.00004012
Iteration 319/1000 | Loss: 0.00004012
Iteration 320/1000 | Loss: 0.00004012
Iteration 321/1000 | Loss: 0.00004012
Iteration 322/1000 | Loss: 0.00004012
Iteration 323/1000 | Loss: 0.00004012
Iteration 324/1000 | Loss: 0.00004012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 324. Stopping optimization.
Last 5 losses: [4.011977216578089e-05, 4.011977216578089e-05, 4.011977216578089e-05, 4.011977216578089e-05, 4.011977216578089e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.011977216578089e-05

Optimization complete. Final v2v error: 3.5205464363098145 mm

Highest mean error: 10.418449401855469 mm for frame 201

Lowest mean error: 2.589299201965332 mm for frame 37

Saving results

Total time: 372.1805000305176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00344343
Iteration 2/25 | Loss: 0.00139803
Iteration 3/25 | Loss: 0.00114125
Iteration 4/25 | Loss: 0.00110051
Iteration 5/25 | Loss: 0.00109530
Iteration 6/25 | Loss: 0.00109394
Iteration 7/25 | Loss: 0.00109364
Iteration 8/25 | Loss: 0.00109364
Iteration 9/25 | Loss: 0.00109364
Iteration 10/25 | Loss: 0.00109364
Iteration 11/25 | Loss: 0.00109364
Iteration 12/25 | Loss: 0.00109364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010936387116089463, 0.0010936387116089463, 0.0010936387116089463, 0.0010936387116089463, 0.0010936387116089463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010936387116089463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37746024
Iteration 2/25 | Loss: 0.00091158
Iteration 3/25 | Loss: 0.00091158
Iteration 4/25 | Loss: 0.00091158
Iteration 5/25 | Loss: 0.00091158
Iteration 6/25 | Loss: 0.00091158
Iteration 7/25 | Loss: 0.00091158
Iteration 8/25 | Loss: 0.00091158
Iteration 9/25 | Loss: 0.00091158
Iteration 10/25 | Loss: 0.00091158
Iteration 11/25 | Loss: 0.00091158
Iteration 12/25 | Loss: 0.00091158
Iteration 13/25 | Loss: 0.00091158
Iteration 14/25 | Loss: 0.00091158
Iteration 15/25 | Loss: 0.00091158
Iteration 16/25 | Loss: 0.00091158
Iteration 17/25 | Loss: 0.00091158
Iteration 18/25 | Loss: 0.00091158
Iteration 19/25 | Loss: 0.00091158
Iteration 20/25 | Loss: 0.00091158
Iteration 21/25 | Loss: 0.00091158
Iteration 22/25 | Loss: 0.00091158
Iteration 23/25 | Loss: 0.00091158
Iteration 24/25 | Loss: 0.00091158
Iteration 25/25 | Loss: 0.00091158

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091158
Iteration 2/1000 | Loss: 0.00004650
Iteration 3/1000 | Loss: 0.00002451
Iteration 4/1000 | Loss: 0.00001605
Iteration 5/1000 | Loss: 0.00001458
Iteration 6/1000 | Loss: 0.00001366
Iteration 7/1000 | Loss: 0.00001313
Iteration 8/1000 | Loss: 0.00001272
Iteration 9/1000 | Loss: 0.00001243
Iteration 10/1000 | Loss: 0.00001217
Iteration 11/1000 | Loss: 0.00001212
Iteration 12/1000 | Loss: 0.00001207
Iteration 13/1000 | Loss: 0.00001205
Iteration 14/1000 | Loss: 0.00001194
Iteration 15/1000 | Loss: 0.00001190
Iteration 16/1000 | Loss: 0.00001183
Iteration 17/1000 | Loss: 0.00001179
Iteration 18/1000 | Loss: 0.00001175
Iteration 19/1000 | Loss: 0.00001174
Iteration 20/1000 | Loss: 0.00001172
Iteration 21/1000 | Loss: 0.00001171
Iteration 22/1000 | Loss: 0.00001171
Iteration 23/1000 | Loss: 0.00001171
Iteration 24/1000 | Loss: 0.00001170
Iteration 25/1000 | Loss: 0.00001170
Iteration 26/1000 | Loss: 0.00001170
Iteration 27/1000 | Loss: 0.00001170
Iteration 28/1000 | Loss: 0.00001170
Iteration 29/1000 | Loss: 0.00001170
Iteration 30/1000 | Loss: 0.00001170
Iteration 31/1000 | Loss: 0.00001169
Iteration 32/1000 | Loss: 0.00001169
Iteration 33/1000 | Loss: 0.00001169
Iteration 34/1000 | Loss: 0.00001169
Iteration 35/1000 | Loss: 0.00001168
Iteration 36/1000 | Loss: 0.00001168
Iteration 37/1000 | Loss: 0.00001168
Iteration 38/1000 | Loss: 0.00001168
Iteration 39/1000 | Loss: 0.00001167
Iteration 40/1000 | Loss: 0.00001167
Iteration 41/1000 | Loss: 0.00001167
Iteration 42/1000 | Loss: 0.00001167
Iteration 43/1000 | Loss: 0.00001167
Iteration 44/1000 | Loss: 0.00001167
Iteration 45/1000 | Loss: 0.00001166
Iteration 46/1000 | Loss: 0.00001166
Iteration 47/1000 | Loss: 0.00001166
Iteration 48/1000 | Loss: 0.00001166
Iteration 49/1000 | Loss: 0.00001166
Iteration 50/1000 | Loss: 0.00001166
Iteration 51/1000 | Loss: 0.00001166
Iteration 52/1000 | Loss: 0.00001166
Iteration 53/1000 | Loss: 0.00001166
Iteration 54/1000 | Loss: 0.00001166
Iteration 55/1000 | Loss: 0.00001166
Iteration 56/1000 | Loss: 0.00001166
Iteration 57/1000 | Loss: 0.00001166
Iteration 58/1000 | Loss: 0.00001166
Iteration 59/1000 | Loss: 0.00001166
Iteration 60/1000 | Loss: 0.00001166
Iteration 61/1000 | Loss: 0.00001165
Iteration 62/1000 | Loss: 0.00001165
Iteration 63/1000 | Loss: 0.00001165
Iteration 64/1000 | Loss: 0.00001165
Iteration 65/1000 | Loss: 0.00001165
Iteration 66/1000 | Loss: 0.00001164
Iteration 67/1000 | Loss: 0.00001164
Iteration 68/1000 | Loss: 0.00001164
Iteration 69/1000 | Loss: 0.00001164
Iteration 70/1000 | Loss: 0.00001164
Iteration 71/1000 | Loss: 0.00001163
Iteration 72/1000 | Loss: 0.00001163
Iteration 73/1000 | Loss: 0.00001163
Iteration 74/1000 | Loss: 0.00001163
Iteration 75/1000 | Loss: 0.00001163
Iteration 76/1000 | Loss: 0.00001162
Iteration 77/1000 | Loss: 0.00001162
Iteration 78/1000 | Loss: 0.00001162
Iteration 79/1000 | Loss: 0.00001161
Iteration 80/1000 | Loss: 0.00001161
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001160
Iteration 85/1000 | Loss: 0.00001159
Iteration 86/1000 | Loss: 0.00001159
Iteration 87/1000 | Loss: 0.00001159
Iteration 88/1000 | Loss: 0.00001158
Iteration 89/1000 | Loss: 0.00001158
Iteration 90/1000 | Loss: 0.00001158
Iteration 91/1000 | Loss: 0.00001158
Iteration 92/1000 | Loss: 0.00001157
Iteration 93/1000 | Loss: 0.00001157
Iteration 94/1000 | Loss: 0.00001157
Iteration 95/1000 | Loss: 0.00001156
Iteration 96/1000 | Loss: 0.00001156
Iteration 97/1000 | Loss: 0.00001156
Iteration 98/1000 | Loss: 0.00001156
Iteration 99/1000 | Loss: 0.00001156
Iteration 100/1000 | Loss: 0.00001155
Iteration 101/1000 | Loss: 0.00001155
Iteration 102/1000 | Loss: 0.00001155
Iteration 103/1000 | Loss: 0.00001155
Iteration 104/1000 | Loss: 0.00001154
Iteration 105/1000 | Loss: 0.00001154
Iteration 106/1000 | Loss: 0.00001154
Iteration 107/1000 | Loss: 0.00001154
Iteration 108/1000 | Loss: 0.00001154
Iteration 109/1000 | Loss: 0.00001154
Iteration 110/1000 | Loss: 0.00001154
Iteration 111/1000 | Loss: 0.00001154
Iteration 112/1000 | Loss: 0.00001154
Iteration 113/1000 | Loss: 0.00001153
Iteration 114/1000 | Loss: 0.00001153
Iteration 115/1000 | Loss: 0.00001153
Iteration 116/1000 | Loss: 0.00001153
Iteration 117/1000 | Loss: 0.00001153
Iteration 118/1000 | Loss: 0.00001153
Iteration 119/1000 | Loss: 0.00001153
Iteration 120/1000 | Loss: 0.00001153
Iteration 121/1000 | Loss: 0.00001153
Iteration 122/1000 | Loss: 0.00001153
Iteration 123/1000 | Loss: 0.00001153
Iteration 124/1000 | Loss: 0.00001153
Iteration 125/1000 | Loss: 0.00001153
Iteration 126/1000 | Loss: 0.00001153
Iteration 127/1000 | Loss: 0.00001152
Iteration 128/1000 | Loss: 0.00001152
Iteration 129/1000 | Loss: 0.00001152
Iteration 130/1000 | Loss: 0.00001152
Iteration 131/1000 | Loss: 0.00001152
Iteration 132/1000 | Loss: 0.00001152
Iteration 133/1000 | Loss: 0.00001152
Iteration 134/1000 | Loss: 0.00001152
Iteration 135/1000 | Loss: 0.00001152
Iteration 136/1000 | Loss: 0.00001152
Iteration 137/1000 | Loss: 0.00001152
Iteration 138/1000 | Loss: 0.00001152
Iteration 139/1000 | Loss: 0.00001151
Iteration 140/1000 | Loss: 0.00001151
Iteration 141/1000 | Loss: 0.00001151
Iteration 142/1000 | Loss: 0.00001151
Iteration 143/1000 | Loss: 0.00001151
Iteration 144/1000 | Loss: 0.00001151
Iteration 145/1000 | Loss: 0.00001151
Iteration 146/1000 | Loss: 0.00001151
Iteration 147/1000 | Loss: 0.00001151
Iteration 148/1000 | Loss: 0.00001151
Iteration 149/1000 | Loss: 0.00001151
Iteration 150/1000 | Loss: 0.00001151
Iteration 151/1000 | Loss: 0.00001151
Iteration 152/1000 | Loss: 0.00001151
Iteration 153/1000 | Loss: 0.00001151
Iteration 154/1000 | Loss: 0.00001151
Iteration 155/1000 | Loss: 0.00001151
Iteration 156/1000 | Loss: 0.00001151
Iteration 157/1000 | Loss: 0.00001150
Iteration 158/1000 | Loss: 0.00001150
Iteration 159/1000 | Loss: 0.00001150
Iteration 160/1000 | Loss: 0.00001150
Iteration 161/1000 | Loss: 0.00001150
Iteration 162/1000 | Loss: 0.00001150
Iteration 163/1000 | Loss: 0.00001150
Iteration 164/1000 | Loss: 0.00001150
Iteration 165/1000 | Loss: 0.00001150
Iteration 166/1000 | Loss: 0.00001150
Iteration 167/1000 | Loss: 0.00001150
Iteration 168/1000 | Loss: 0.00001150
Iteration 169/1000 | Loss: 0.00001150
Iteration 170/1000 | Loss: 0.00001150
Iteration 171/1000 | Loss: 0.00001150
Iteration 172/1000 | Loss: 0.00001150
Iteration 173/1000 | Loss: 0.00001150
Iteration 174/1000 | Loss: 0.00001150
Iteration 175/1000 | Loss: 0.00001149
Iteration 176/1000 | Loss: 0.00001149
Iteration 177/1000 | Loss: 0.00001149
Iteration 178/1000 | Loss: 0.00001149
Iteration 179/1000 | Loss: 0.00001149
Iteration 180/1000 | Loss: 0.00001149
Iteration 181/1000 | Loss: 0.00001149
Iteration 182/1000 | Loss: 0.00001149
Iteration 183/1000 | Loss: 0.00001149
Iteration 184/1000 | Loss: 0.00001149
Iteration 185/1000 | Loss: 0.00001149
Iteration 186/1000 | Loss: 0.00001149
Iteration 187/1000 | Loss: 0.00001148
Iteration 188/1000 | Loss: 0.00001148
Iteration 189/1000 | Loss: 0.00001148
Iteration 190/1000 | Loss: 0.00001148
Iteration 191/1000 | Loss: 0.00001148
Iteration 192/1000 | Loss: 0.00001148
Iteration 193/1000 | Loss: 0.00001148
Iteration 194/1000 | Loss: 0.00001148
Iteration 195/1000 | Loss: 0.00001148
Iteration 196/1000 | Loss: 0.00001148
Iteration 197/1000 | Loss: 0.00001148
Iteration 198/1000 | Loss: 0.00001147
Iteration 199/1000 | Loss: 0.00001147
Iteration 200/1000 | Loss: 0.00001147
Iteration 201/1000 | Loss: 0.00001147
Iteration 202/1000 | Loss: 0.00001147
Iteration 203/1000 | Loss: 0.00001147
Iteration 204/1000 | Loss: 0.00001147
Iteration 205/1000 | Loss: 0.00001147
Iteration 206/1000 | Loss: 0.00001147
Iteration 207/1000 | Loss: 0.00001147
Iteration 208/1000 | Loss: 0.00001147
Iteration 209/1000 | Loss: 0.00001147
Iteration 210/1000 | Loss: 0.00001147
Iteration 211/1000 | Loss: 0.00001147
Iteration 212/1000 | Loss: 0.00001147
Iteration 213/1000 | Loss: 0.00001147
Iteration 214/1000 | Loss: 0.00001147
Iteration 215/1000 | Loss: 0.00001147
Iteration 216/1000 | Loss: 0.00001147
Iteration 217/1000 | Loss: 0.00001147
Iteration 218/1000 | Loss: 0.00001147
Iteration 219/1000 | Loss: 0.00001147
Iteration 220/1000 | Loss: 0.00001146
Iteration 221/1000 | Loss: 0.00001146
Iteration 222/1000 | Loss: 0.00001146
Iteration 223/1000 | Loss: 0.00001146
Iteration 224/1000 | Loss: 0.00001146
Iteration 225/1000 | Loss: 0.00001146
Iteration 226/1000 | Loss: 0.00001146
Iteration 227/1000 | Loss: 0.00001146
Iteration 228/1000 | Loss: 0.00001146
Iteration 229/1000 | Loss: 0.00001146
Iteration 230/1000 | Loss: 0.00001146
Iteration 231/1000 | Loss: 0.00001146
Iteration 232/1000 | Loss: 0.00001146
Iteration 233/1000 | Loss: 0.00001146
Iteration 234/1000 | Loss: 0.00001146
Iteration 235/1000 | Loss: 0.00001146
Iteration 236/1000 | Loss: 0.00001146
Iteration 237/1000 | Loss: 0.00001146
Iteration 238/1000 | Loss: 0.00001146
Iteration 239/1000 | Loss: 0.00001146
Iteration 240/1000 | Loss: 0.00001145
Iteration 241/1000 | Loss: 0.00001145
Iteration 242/1000 | Loss: 0.00001145
Iteration 243/1000 | Loss: 0.00001145
Iteration 244/1000 | Loss: 0.00001145
Iteration 245/1000 | Loss: 0.00001145
Iteration 246/1000 | Loss: 0.00001145
Iteration 247/1000 | Loss: 0.00001145
Iteration 248/1000 | Loss: 0.00001145
Iteration 249/1000 | Loss: 0.00001145
Iteration 250/1000 | Loss: 0.00001145
Iteration 251/1000 | Loss: 0.00001145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [1.1452810213086195e-05, 1.1452810213086195e-05, 1.1452810213086195e-05, 1.1452810213086195e-05, 1.1452810213086195e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1452810213086195e-05

Optimization complete. Final v2v error: 2.913637399673462 mm

Highest mean error: 3.317495346069336 mm for frame 28

Lowest mean error: 2.467691659927368 mm for frame 13

Saving results

Total time: 40.898253440856934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874598
Iteration 2/25 | Loss: 0.00130591
Iteration 3/25 | Loss: 0.00116681
Iteration 4/25 | Loss: 0.00114214
Iteration 5/25 | Loss: 0.00113378
Iteration 6/25 | Loss: 0.00113173
Iteration 7/25 | Loss: 0.00113153
Iteration 8/25 | Loss: 0.00113153
Iteration 9/25 | Loss: 0.00113153
Iteration 10/25 | Loss: 0.00113153
Iteration 11/25 | Loss: 0.00113153
Iteration 12/25 | Loss: 0.00113153
Iteration 13/25 | Loss: 0.00113153
Iteration 14/25 | Loss: 0.00113153
Iteration 15/25 | Loss: 0.00113153
Iteration 16/25 | Loss: 0.00113153
Iteration 17/25 | Loss: 0.00113153
Iteration 18/25 | Loss: 0.00113153
Iteration 19/25 | Loss: 0.00113153
Iteration 20/25 | Loss: 0.00113153
Iteration 21/25 | Loss: 0.00113153
Iteration 22/25 | Loss: 0.00113153
Iteration 23/25 | Loss: 0.00113153
Iteration 24/25 | Loss: 0.00113153
Iteration 25/25 | Loss: 0.00113153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36445343
Iteration 2/25 | Loss: 0.00081437
Iteration 3/25 | Loss: 0.00081434
Iteration 4/25 | Loss: 0.00081434
Iteration 5/25 | Loss: 0.00081434
Iteration 6/25 | Loss: 0.00081434
Iteration 7/25 | Loss: 0.00081434
Iteration 8/25 | Loss: 0.00081434
Iteration 9/25 | Loss: 0.00081434
Iteration 10/25 | Loss: 0.00081434
Iteration 11/25 | Loss: 0.00081434
Iteration 12/25 | Loss: 0.00081434
Iteration 13/25 | Loss: 0.00081434
Iteration 14/25 | Loss: 0.00081434
Iteration 15/25 | Loss: 0.00081434
Iteration 16/25 | Loss: 0.00081434
Iteration 17/25 | Loss: 0.00081434
Iteration 18/25 | Loss: 0.00081434
Iteration 19/25 | Loss: 0.00081434
Iteration 20/25 | Loss: 0.00081434
Iteration 21/25 | Loss: 0.00081434
Iteration 22/25 | Loss: 0.00081434
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008143390878103673, 0.0008143390878103673, 0.0008143390878103673, 0.0008143390878103673, 0.0008143390878103673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008143390878103673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081434
Iteration 2/1000 | Loss: 0.00004469
Iteration 3/1000 | Loss: 0.00002885
Iteration 4/1000 | Loss: 0.00002343
Iteration 5/1000 | Loss: 0.00002163
Iteration 6/1000 | Loss: 0.00002067
Iteration 7/1000 | Loss: 0.00001987
Iteration 8/1000 | Loss: 0.00001921
Iteration 9/1000 | Loss: 0.00001877
Iteration 10/1000 | Loss: 0.00001839
Iteration 11/1000 | Loss: 0.00001826
Iteration 12/1000 | Loss: 0.00001812
Iteration 13/1000 | Loss: 0.00001806
Iteration 14/1000 | Loss: 0.00001796
Iteration 15/1000 | Loss: 0.00001787
Iteration 16/1000 | Loss: 0.00001783
Iteration 17/1000 | Loss: 0.00001783
Iteration 18/1000 | Loss: 0.00001782
Iteration 19/1000 | Loss: 0.00001779
Iteration 20/1000 | Loss: 0.00001778
Iteration 21/1000 | Loss: 0.00001773
Iteration 22/1000 | Loss: 0.00001767
Iteration 23/1000 | Loss: 0.00001765
Iteration 24/1000 | Loss: 0.00001765
Iteration 25/1000 | Loss: 0.00001764
Iteration 26/1000 | Loss: 0.00001764
Iteration 27/1000 | Loss: 0.00001764
Iteration 28/1000 | Loss: 0.00001763
Iteration 29/1000 | Loss: 0.00001760
Iteration 30/1000 | Loss: 0.00001760
Iteration 31/1000 | Loss: 0.00001760
Iteration 32/1000 | Loss: 0.00001759
Iteration 33/1000 | Loss: 0.00001759
Iteration 34/1000 | Loss: 0.00001759
Iteration 35/1000 | Loss: 0.00001758
Iteration 36/1000 | Loss: 0.00001758
Iteration 37/1000 | Loss: 0.00001757
Iteration 38/1000 | Loss: 0.00001757
Iteration 39/1000 | Loss: 0.00001757
Iteration 40/1000 | Loss: 0.00001757
Iteration 41/1000 | Loss: 0.00001757
Iteration 42/1000 | Loss: 0.00001757
Iteration 43/1000 | Loss: 0.00001757
Iteration 44/1000 | Loss: 0.00001756
Iteration 45/1000 | Loss: 0.00001756
Iteration 46/1000 | Loss: 0.00001756
Iteration 47/1000 | Loss: 0.00001756
Iteration 48/1000 | Loss: 0.00001756
Iteration 49/1000 | Loss: 0.00001756
Iteration 50/1000 | Loss: 0.00001756
Iteration 51/1000 | Loss: 0.00001756
Iteration 52/1000 | Loss: 0.00001756
Iteration 53/1000 | Loss: 0.00001755
Iteration 54/1000 | Loss: 0.00001755
Iteration 55/1000 | Loss: 0.00001755
Iteration 56/1000 | Loss: 0.00001755
Iteration 57/1000 | Loss: 0.00001755
Iteration 58/1000 | Loss: 0.00001754
Iteration 59/1000 | Loss: 0.00001754
Iteration 60/1000 | Loss: 0.00001754
Iteration 61/1000 | Loss: 0.00001754
Iteration 62/1000 | Loss: 0.00001754
Iteration 63/1000 | Loss: 0.00001754
Iteration 64/1000 | Loss: 0.00001754
Iteration 65/1000 | Loss: 0.00001753
Iteration 66/1000 | Loss: 0.00001753
Iteration 67/1000 | Loss: 0.00001753
Iteration 68/1000 | Loss: 0.00001753
Iteration 69/1000 | Loss: 0.00001753
Iteration 70/1000 | Loss: 0.00001753
Iteration 71/1000 | Loss: 0.00001753
Iteration 72/1000 | Loss: 0.00001753
Iteration 73/1000 | Loss: 0.00001753
Iteration 74/1000 | Loss: 0.00001753
Iteration 75/1000 | Loss: 0.00001753
Iteration 76/1000 | Loss: 0.00001752
Iteration 77/1000 | Loss: 0.00001752
Iteration 78/1000 | Loss: 0.00001752
Iteration 79/1000 | Loss: 0.00001751
Iteration 80/1000 | Loss: 0.00001751
Iteration 81/1000 | Loss: 0.00001751
Iteration 82/1000 | Loss: 0.00001751
Iteration 83/1000 | Loss: 0.00001751
Iteration 84/1000 | Loss: 0.00001751
Iteration 85/1000 | Loss: 0.00001751
Iteration 86/1000 | Loss: 0.00001751
Iteration 87/1000 | Loss: 0.00001750
Iteration 88/1000 | Loss: 0.00001750
Iteration 89/1000 | Loss: 0.00001750
Iteration 90/1000 | Loss: 0.00001749
Iteration 91/1000 | Loss: 0.00001749
Iteration 92/1000 | Loss: 0.00001749
Iteration 93/1000 | Loss: 0.00001749
Iteration 94/1000 | Loss: 0.00001749
Iteration 95/1000 | Loss: 0.00001749
Iteration 96/1000 | Loss: 0.00001748
Iteration 97/1000 | Loss: 0.00001748
Iteration 98/1000 | Loss: 0.00001748
Iteration 99/1000 | Loss: 0.00001748
Iteration 100/1000 | Loss: 0.00001748
Iteration 101/1000 | Loss: 0.00001748
Iteration 102/1000 | Loss: 0.00001748
Iteration 103/1000 | Loss: 0.00001748
Iteration 104/1000 | Loss: 0.00001748
Iteration 105/1000 | Loss: 0.00001747
Iteration 106/1000 | Loss: 0.00001747
Iteration 107/1000 | Loss: 0.00001747
Iteration 108/1000 | Loss: 0.00001747
Iteration 109/1000 | Loss: 0.00001747
Iteration 110/1000 | Loss: 0.00001747
Iteration 111/1000 | Loss: 0.00001747
Iteration 112/1000 | Loss: 0.00001746
Iteration 113/1000 | Loss: 0.00001746
Iteration 114/1000 | Loss: 0.00001746
Iteration 115/1000 | Loss: 0.00001745
Iteration 116/1000 | Loss: 0.00001745
Iteration 117/1000 | Loss: 0.00001745
Iteration 118/1000 | Loss: 0.00001745
Iteration 119/1000 | Loss: 0.00001745
Iteration 120/1000 | Loss: 0.00001745
Iteration 121/1000 | Loss: 0.00001745
Iteration 122/1000 | Loss: 0.00001745
Iteration 123/1000 | Loss: 0.00001745
Iteration 124/1000 | Loss: 0.00001745
Iteration 125/1000 | Loss: 0.00001745
Iteration 126/1000 | Loss: 0.00001745
Iteration 127/1000 | Loss: 0.00001745
Iteration 128/1000 | Loss: 0.00001745
Iteration 129/1000 | Loss: 0.00001744
Iteration 130/1000 | Loss: 0.00001744
Iteration 131/1000 | Loss: 0.00001744
Iteration 132/1000 | Loss: 0.00001744
Iteration 133/1000 | Loss: 0.00001744
Iteration 134/1000 | Loss: 0.00001744
Iteration 135/1000 | Loss: 0.00001744
Iteration 136/1000 | Loss: 0.00001744
Iteration 137/1000 | Loss: 0.00001743
Iteration 138/1000 | Loss: 0.00001743
Iteration 139/1000 | Loss: 0.00001743
Iteration 140/1000 | Loss: 0.00001743
Iteration 141/1000 | Loss: 0.00001743
Iteration 142/1000 | Loss: 0.00001743
Iteration 143/1000 | Loss: 0.00001743
Iteration 144/1000 | Loss: 0.00001743
Iteration 145/1000 | Loss: 0.00001743
Iteration 146/1000 | Loss: 0.00001743
Iteration 147/1000 | Loss: 0.00001743
Iteration 148/1000 | Loss: 0.00001743
Iteration 149/1000 | Loss: 0.00001743
Iteration 150/1000 | Loss: 0.00001743
Iteration 151/1000 | Loss: 0.00001742
Iteration 152/1000 | Loss: 0.00001742
Iteration 153/1000 | Loss: 0.00001742
Iteration 154/1000 | Loss: 0.00001742
Iteration 155/1000 | Loss: 0.00001742
Iteration 156/1000 | Loss: 0.00001742
Iteration 157/1000 | Loss: 0.00001742
Iteration 158/1000 | Loss: 0.00001742
Iteration 159/1000 | Loss: 0.00001741
Iteration 160/1000 | Loss: 0.00001741
Iteration 161/1000 | Loss: 0.00001741
Iteration 162/1000 | Loss: 0.00001741
Iteration 163/1000 | Loss: 0.00001741
Iteration 164/1000 | Loss: 0.00001741
Iteration 165/1000 | Loss: 0.00001741
Iteration 166/1000 | Loss: 0.00001741
Iteration 167/1000 | Loss: 0.00001741
Iteration 168/1000 | Loss: 0.00001740
Iteration 169/1000 | Loss: 0.00001740
Iteration 170/1000 | Loss: 0.00001740
Iteration 171/1000 | Loss: 0.00001740
Iteration 172/1000 | Loss: 0.00001740
Iteration 173/1000 | Loss: 0.00001740
Iteration 174/1000 | Loss: 0.00001740
Iteration 175/1000 | Loss: 0.00001740
Iteration 176/1000 | Loss: 0.00001740
Iteration 177/1000 | Loss: 0.00001740
Iteration 178/1000 | Loss: 0.00001740
Iteration 179/1000 | Loss: 0.00001740
Iteration 180/1000 | Loss: 0.00001740
Iteration 181/1000 | Loss: 0.00001740
Iteration 182/1000 | Loss: 0.00001740
Iteration 183/1000 | Loss: 0.00001740
Iteration 184/1000 | Loss: 0.00001740
Iteration 185/1000 | Loss: 0.00001740
Iteration 186/1000 | Loss: 0.00001740
Iteration 187/1000 | Loss: 0.00001740
Iteration 188/1000 | Loss: 0.00001740
Iteration 189/1000 | Loss: 0.00001739
Iteration 190/1000 | Loss: 0.00001739
Iteration 191/1000 | Loss: 0.00001739
Iteration 192/1000 | Loss: 0.00001739
Iteration 193/1000 | Loss: 0.00001739
Iteration 194/1000 | Loss: 0.00001739
Iteration 195/1000 | Loss: 0.00001739
Iteration 196/1000 | Loss: 0.00001739
Iteration 197/1000 | Loss: 0.00001739
Iteration 198/1000 | Loss: 0.00001739
Iteration 199/1000 | Loss: 0.00001739
Iteration 200/1000 | Loss: 0.00001739
Iteration 201/1000 | Loss: 0.00001739
Iteration 202/1000 | Loss: 0.00001739
Iteration 203/1000 | Loss: 0.00001739
Iteration 204/1000 | Loss: 0.00001739
Iteration 205/1000 | Loss: 0.00001739
Iteration 206/1000 | Loss: 0.00001739
Iteration 207/1000 | Loss: 0.00001739
Iteration 208/1000 | Loss: 0.00001739
Iteration 209/1000 | Loss: 0.00001739
Iteration 210/1000 | Loss: 0.00001739
Iteration 211/1000 | Loss: 0.00001739
Iteration 212/1000 | Loss: 0.00001739
Iteration 213/1000 | Loss: 0.00001739
Iteration 214/1000 | Loss: 0.00001739
Iteration 215/1000 | Loss: 0.00001739
Iteration 216/1000 | Loss: 0.00001739
Iteration 217/1000 | Loss: 0.00001739
Iteration 218/1000 | Loss: 0.00001739
Iteration 219/1000 | Loss: 0.00001739
Iteration 220/1000 | Loss: 0.00001739
Iteration 221/1000 | Loss: 0.00001739
Iteration 222/1000 | Loss: 0.00001739
Iteration 223/1000 | Loss: 0.00001739
Iteration 224/1000 | Loss: 0.00001739
Iteration 225/1000 | Loss: 0.00001739
Iteration 226/1000 | Loss: 0.00001739
Iteration 227/1000 | Loss: 0.00001739
Iteration 228/1000 | Loss: 0.00001739
Iteration 229/1000 | Loss: 0.00001739
Iteration 230/1000 | Loss: 0.00001739
Iteration 231/1000 | Loss: 0.00001739
Iteration 232/1000 | Loss: 0.00001739
Iteration 233/1000 | Loss: 0.00001739
Iteration 234/1000 | Loss: 0.00001739
Iteration 235/1000 | Loss: 0.00001739
Iteration 236/1000 | Loss: 0.00001739
Iteration 237/1000 | Loss: 0.00001739
Iteration 238/1000 | Loss: 0.00001739
Iteration 239/1000 | Loss: 0.00001739
Iteration 240/1000 | Loss: 0.00001739
Iteration 241/1000 | Loss: 0.00001739
Iteration 242/1000 | Loss: 0.00001739
Iteration 243/1000 | Loss: 0.00001739
Iteration 244/1000 | Loss: 0.00001739
Iteration 245/1000 | Loss: 0.00001739
Iteration 246/1000 | Loss: 0.00001739
Iteration 247/1000 | Loss: 0.00001739
Iteration 248/1000 | Loss: 0.00001739
Iteration 249/1000 | Loss: 0.00001739
Iteration 250/1000 | Loss: 0.00001739
Iteration 251/1000 | Loss: 0.00001739
Iteration 252/1000 | Loss: 0.00001739
Iteration 253/1000 | Loss: 0.00001739
Iteration 254/1000 | Loss: 0.00001739
Iteration 255/1000 | Loss: 0.00001739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [1.739097569952719e-05, 1.739097569952719e-05, 1.739097569952719e-05, 1.739097569952719e-05, 1.739097569952719e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.739097569952719e-05

Optimization complete. Final v2v error: 3.474137783050537 mm

Highest mean error: 5.11630392074585 mm for frame 67

Lowest mean error: 2.8796002864837646 mm for frame 98

Saving results

Total time: 44.2556209564209
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801459
Iteration 2/25 | Loss: 0.00115563
Iteration 3/25 | Loss: 0.00106519
Iteration 4/25 | Loss: 0.00105651
Iteration 5/25 | Loss: 0.00105405
Iteration 6/25 | Loss: 0.00105385
Iteration 7/25 | Loss: 0.00105384
Iteration 8/25 | Loss: 0.00105384
Iteration 9/25 | Loss: 0.00105384
Iteration 10/25 | Loss: 0.00105384
Iteration 11/25 | Loss: 0.00105384
Iteration 12/25 | Loss: 0.00105384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010538403876125813, 0.0010538403876125813, 0.0010538403876125813, 0.0010538403876125813, 0.0010538403876125813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010538403876125813

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35810220
Iteration 2/25 | Loss: 0.00081690
Iteration 3/25 | Loss: 0.00081689
Iteration 4/25 | Loss: 0.00081689
Iteration 5/25 | Loss: 0.00081689
Iteration 6/25 | Loss: 0.00081689
Iteration 7/25 | Loss: 0.00081689
Iteration 8/25 | Loss: 0.00081689
Iteration 9/25 | Loss: 0.00081689
Iteration 10/25 | Loss: 0.00081689
Iteration 11/25 | Loss: 0.00081689
Iteration 12/25 | Loss: 0.00081689
Iteration 13/25 | Loss: 0.00081689
Iteration 14/25 | Loss: 0.00081689
Iteration 15/25 | Loss: 0.00081689
Iteration 16/25 | Loss: 0.00081689
Iteration 17/25 | Loss: 0.00081689
Iteration 18/25 | Loss: 0.00081689
Iteration 19/25 | Loss: 0.00081689
Iteration 20/25 | Loss: 0.00081689
Iteration 21/25 | Loss: 0.00081689
Iteration 22/25 | Loss: 0.00081689
Iteration 23/25 | Loss: 0.00081689
Iteration 24/25 | Loss: 0.00081689
Iteration 25/25 | Loss: 0.00081689

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081689
Iteration 2/1000 | Loss: 0.00002399
Iteration 3/1000 | Loss: 0.00001527
Iteration 4/1000 | Loss: 0.00001244
Iteration 5/1000 | Loss: 0.00001130
Iteration 6/1000 | Loss: 0.00001060
Iteration 7/1000 | Loss: 0.00001013
Iteration 8/1000 | Loss: 0.00000976
Iteration 9/1000 | Loss: 0.00000949
Iteration 10/1000 | Loss: 0.00000944
Iteration 11/1000 | Loss: 0.00000936
Iteration 12/1000 | Loss: 0.00000935
Iteration 13/1000 | Loss: 0.00000935
Iteration 14/1000 | Loss: 0.00000913
Iteration 15/1000 | Loss: 0.00000906
Iteration 16/1000 | Loss: 0.00000903
Iteration 17/1000 | Loss: 0.00000903
Iteration 18/1000 | Loss: 0.00000902
Iteration 19/1000 | Loss: 0.00000902
Iteration 20/1000 | Loss: 0.00000898
Iteration 21/1000 | Loss: 0.00000897
Iteration 22/1000 | Loss: 0.00000896
Iteration 23/1000 | Loss: 0.00000896
Iteration 24/1000 | Loss: 0.00000895
Iteration 25/1000 | Loss: 0.00000895
Iteration 26/1000 | Loss: 0.00000894
Iteration 27/1000 | Loss: 0.00000892
Iteration 28/1000 | Loss: 0.00000891
Iteration 29/1000 | Loss: 0.00000891
Iteration 30/1000 | Loss: 0.00000891
Iteration 31/1000 | Loss: 0.00000891
Iteration 32/1000 | Loss: 0.00000889
Iteration 33/1000 | Loss: 0.00000888
Iteration 34/1000 | Loss: 0.00000887
Iteration 35/1000 | Loss: 0.00000886
Iteration 36/1000 | Loss: 0.00000885
Iteration 37/1000 | Loss: 0.00000885
Iteration 38/1000 | Loss: 0.00000882
Iteration 39/1000 | Loss: 0.00000880
Iteration 40/1000 | Loss: 0.00000879
Iteration 41/1000 | Loss: 0.00000879
Iteration 42/1000 | Loss: 0.00000879
Iteration 43/1000 | Loss: 0.00000879
Iteration 44/1000 | Loss: 0.00000879
Iteration 45/1000 | Loss: 0.00000877
Iteration 46/1000 | Loss: 0.00000877
Iteration 47/1000 | Loss: 0.00000876
Iteration 48/1000 | Loss: 0.00000876
Iteration 49/1000 | Loss: 0.00000876
Iteration 50/1000 | Loss: 0.00000876
Iteration 51/1000 | Loss: 0.00000876
Iteration 52/1000 | Loss: 0.00000876
Iteration 53/1000 | Loss: 0.00000876
Iteration 54/1000 | Loss: 0.00000876
Iteration 55/1000 | Loss: 0.00000876
Iteration 56/1000 | Loss: 0.00000876
Iteration 57/1000 | Loss: 0.00000875
Iteration 58/1000 | Loss: 0.00000875
Iteration 59/1000 | Loss: 0.00000874
Iteration 60/1000 | Loss: 0.00000874
Iteration 61/1000 | Loss: 0.00000874
Iteration 62/1000 | Loss: 0.00000873
Iteration 63/1000 | Loss: 0.00000873
Iteration 64/1000 | Loss: 0.00000872
Iteration 65/1000 | Loss: 0.00000872
Iteration 66/1000 | Loss: 0.00000871
Iteration 67/1000 | Loss: 0.00000871
Iteration 68/1000 | Loss: 0.00000871
Iteration 69/1000 | Loss: 0.00000871
Iteration 70/1000 | Loss: 0.00000871
Iteration 71/1000 | Loss: 0.00000871
Iteration 72/1000 | Loss: 0.00000870
Iteration 73/1000 | Loss: 0.00000870
Iteration 74/1000 | Loss: 0.00000870
Iteration 75/1000 | Loss: 0.00000870
Iteration 76/1000 | Loss: 0.00000869
Iteration 77/1000 | Loss: 0.00000869
Iteration 78/1000 | Loss: 0.00000869
Iteration 79/1000 | Loss: 0.00000869
Iteration 80/1000 | Loss: 0.00000869
Iteration 81/1000 | Loss: 0.00000869
Iteration 82/1000 | Loss: 0.00000868
Iteration 83/1000 | Loss: 0.00000868
Iteration 84/1000 | Loss: 0.00000867
Iteration 85/1000 | Loss: 0.00000867
Iteration 86/1000 | Loss: 0.00000867
Iteration 87/1000 | Loss: 0.00000867
Iteration 88/1000 | Loss: 0.00000866
Iteration 89/1000 | Loss: 0.00000866
Iteration 90/1000 | Loss: 0.00000866
Iteration 91/1000 | Loss: 0.00000865
Iteration 92/1000 | Loss: 0.00000865
Iteration 93/1000 | Loss: 0.00000865
Iteration 94/1000 | Loss: 0.00000865
Iteration 95/1000 | Loss: 0.00000865
Iteration 96/1000 | Loss: 0.00000865
Iteration 97/1000 | Loss: 0.00000865
Iteration 98/1000 | Loss: 0.00000865
Iteration 99/1000 | Loss: 0.00000865
Iteration 100/1000 | Loss: 0.00000865
Iteration 101/1000 | Loss: 0.00000864
Iteration 102/1000 | Loss: 0.00000864
Iteration 103/1000 | Loss: 0.00000864
Iteration 104/1000 | Loss: 0.00000864
Iteration 105/1000 | Loss: 0.00000864
Iteration 106/1000 | Loss: 0.00000863
Iteration 107/1000 | Loss: 0.00000863
Iteration 108/1000 | Loss: 0.00000863
Iteration 109/1000 | Loss: 0.00000863
Iteration 110/1000 | Loss: 0.00000863
Iteration 111/1000 | Loss: 0.00000863
Iteration 112/1000 | Loss: 0.00000862
Iteration 113/1000 | Loss: 0.00000862
Iteration 114/1000 | Loss: 0.00000862
Iteration 115/1000 | Loss: 0.00000862
Iteration 116/1000 | Loss: 0.00000861
Iteration 117/1000 | Loss: 0.00000861
Iteration 118/1000 | Loss: 0.00000861
Iteration 119/1000 | Loss: 0.00000861
Iteration 120/1000 | Loss: 0.00000860
Iteration 121/1000 | Loss: 0.00000860
Iteration 122/1000 | Loss: 0.00000860
Iteration 123/1000 | Loss: 0.00000860
Iteration 124/1000 | Loss: 0.00000860
Iteration 125/1000 | Loss: 0.00000860
Iteration 126/1000 | Loss: 0.00000860
Iteration 127/1000 | Loss: 0.00000860
Iteration 128/1000 | Loss: 0.00000859
Iteration 129/1000 | Loss: 0.00000859
Iteration 130/1000 | Loss: 0.00000859
Iteration 131/1000 | Loss: 0.00000859
Iteration 132/1000 | Loss: 0.00000859
Iteration 133/1000 | Loss: 0.00000858
Iteration 134/1000 | Loss: 0.00000858
Iteration 135/1000 | Loss: 0.00000858
Iteration 136/1000 | Loss: 0.00000858
Iteration 137/1000 | Loss: 0.00000858
Iteration 138/1000 | Loss: 0.00000858
Iteration 139/1000 | Loss: 0.00000858
Iteration 140/1000 | Loss: 0.00000858
Iteration 141/1000 | Loss: 0.00000857
Iteration 142/1000 | Loss: 0.00000857
Iteration 143/1000 | Loss: 0.00000857
Iteration 144/1000 | Loss: 0.00000857
Iteration 145/1000 | Loss: 0.00000857
Iteration 146/1000 | Loss: 0.00000857
Iteration 147/1000 | Loss: 0.00000857
Iteration 148/1000 | Loss: 0.00000856
Iteration 149/1000 | Loss: 0.00000855
Iteration 150/1000 | Loss: 0.00000855
Iteration 151/1000 | Loss: 0.00000855
Iteration 152/1000 | Loss: 0.00000855
Iteration 153/1000 | Loss: 0.00000854
Iteration 154/1000 | Loss: 0.00000854
Iteration 155/1000 | Loss: 0.00000854
Iteration 156/1000 | Loss: 0.00000854
Iteration 157/1000 | Loss: 0.00000854
Iteration 158/1000 | Loss: 0.00000854
Iteration 159/1000 | Loss: 0.00000854
Iteration 160/1000 | Loss: 0.00000853
Iteration 161/1000 | Loss: 0.00000853
Iteration 162/1000 | Loss: 0.00000852
Iteration 163/1000 | Loss: 0.00000852
Iteration 164/1000 | Loss: 0.00000852
Iteration 165/1000 | Loss: 0.00000851
Iteration 166/1000 | Loss: 0.00000851
Iteration 167/1000 | Loss: 0.00000851
Iteration 168/1000 | Loss: 0.00000851
Iteration 169/1000 | Loss: 0.00000851
Iteration 170/1000 | Loss: 0.00000850
Iteration 171/1000 | Loss: 0.00000850
Iteration 172/1000 | Loss: 0.00000850
Iteration 173/1000 | Loss: 0.00000850
Iteration 174/1000 | Loss: 0.00000850
Iteration 175/1000 | Loss: 0.00000849
Iteration 176/1000 | Loss: 0.00000849
Iteration 177/1000 | Loss: 0.00000848
Iteration 178/1000 | Loss: 0.00000848
Iteration 179/1000 | Loss: 0.00000848
Iteration 180/1000 | Loss: 0.00000848
Iteration 181/1000 | Loss: 0.00000848
Iteration 182/1000 | Loss: 0.00000848
Iteration 183/1000 | Loss: 0.00000847
Iteration 184/1000 | Loss: 0.00000847
Iteration 185/1000 | Loss: 0.00000847
Iteration 186/1000 | Loss: 0.00000847
Iteration 187/1000 | Loss: 0.00000847
Iteration 188/1000 | Loss: 0.00000847
Iteration 189/1000 | Loss: 0.00000847
Iteration 190/1000 | Loss: 0.00000847
Iteration 191/1000 | Loss: 0.00000847
Iteration 192/1000 | Loss: 0.00000847
Iteration 193/1000 | Loss: 0.00000847
Iteration 194/1000 | Loss: 0.00000847
Iteration 195/1000 | Loss: 0.00000847
Iteration 196/1000 | Loss: 0.00000846
Iteration 197/1000 | Loss: 0.00000846
Iteration 198/1000 | Loss: 0.00000846
Iteration 199/1000 | Loss: 0.00000846
Iteration 200/1000 | Loss: 0.00000846
Iteration 201/1000 | Loss: 0.00000846
Iteration 202/1000 | Loss: 0.00000846
Iteration 203/1000 | Loss: 0.00000846
Iteration 204/1000 | Loss: 0.00000846
Iteration 205/1000 | Loss: 0.00000846
Iteration 206/1000 | Loss: 0.00000846
Iteration 207/1000 | Loss: 0.00000846
Iteration 208/1000 | Loss: 0.00000846
Iteration 209/1000 | Loss: 0.00000846
Iteration 210/1000 | Loss: 0.00000845
Iteration 211/1000 | Loss: 0.00000845
Iteration 212/1000 | Loss: 0.00000845
Iteration 213/1000 | Loss: 0.00000845
Iteration 214/1000 | Loss: 0.00000845
Iteration 215/1000 | Loss: 0.00000845
Iteration 216/1000 | Loss: 0.00000845
Iteration 217/1000 | Loss: 0.00000845
Iteration 218/1000 | Loss: 0.00000845
Iteration 219/1000 | Loss: 0.00000845
Iteration 220/1000 | Loss: 0.00000845
Iteration 221/1000 | Loss: 0.00000845
Iteration 222/1000 | Loss: 0.00000845
Iteration 223/1000 | Loss: 0.00000845
Iteration 224/1000 | Loss: 0.00000845
Iteration 225/1000 | Loss: 0.00000845
Iteration 226/1000 | Loss: 0.00000845
Iteration 227/1000 | Loss: 0.00000845
Iteration 228/1000 | Loss: 0.00000845
Iteration 229/1000 | Loss: 0.00000845
Iteration 230/1000 | Loss: 0.00000845
Iteration 231/1000 | Loss: 0.00000845
Iteration 232/1000 | Loss: 0.00000845
Iteration 233/1000 | Loss: 0.00000845
Iteration 234/1000 | Loss: 0.00000845
Iteration 235/1000 | Loss: 0.00000845
Iteration 236/1000 | Loss: 0.00000845
Iteration 237/1000 | Loss: 0.00000845
Iteration 238/1000 | Loss: 0.00000845
Iteration 239/1000 | Loss: 0.00000845
Iteration 240/1000 | Loss: 0.00000845
Iteration 241/1000 | Loss: 0.00000845
Iteration 242/1000 | Loss: 0.00000845
Iteration 243/1000 | Loss: 0.00000845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [8.446938409178983e-06, 8.446938409178983e-06, 8.446938409178983e-06, 8.446938409178983e-06, 8.446938409178983e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.446938409178983e-06

Optimization complete. Final v2v error: 2.487001895904541 mm

Highest mean error: 2.6635849475860596 mm for frame 38

Lowest mean error: 2.34999418258667 mm for frame 123

Saving results

Total time: 41.666112184524536
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778814
Iteration 2/25 | Loss: 0.00144330
Iteration 3/25 | Loss: 0.00121097
Iteration 4/25 | Loss: 0.00118769
Iteration 5/25 | Loss: 0.00118356
Iteration 6/25 | Loss: 0.00118318
Iteration 7/25 | Loss: 0.00118318
Iteration 8/25 | Loss: 0.00118318
Iteration 9/25 | Loss: 0.00118318
Iteration 10/25 | Loss: 0.00118318
Iteration 11/25 | Loss: 0.00118318
Iteration 12/25 | Loss: 0.00118318
Iteration 13/25 | Loss: 0.00118318
Iteration 14/25 | Loss: 0.00118318
Iteration 15/25 | Loss: 0.00118318
Iteration 16/25 | Loss: 0.00118318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011831766460090876, 0.0011831766460090876, 0.0011831766460090876, 0.0011831766460090876, 0.0011831766460090876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011831766460090876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33039474
Iteration 2/25 | Loss: 0.00070312
Iteration 3/25 | Loss: 0.00070312
Iteration 4/25 | Loss: 0.00070312
Iteration 5/25 | Loss: 0.00070312
Iteration 6/25 | Loss: 0.00070312
Iteration 7/25 | Loss: 0.00070312
Iteration 8/25 | Loss: 0.00070312
Iteration 9/25 | Loss: 0.00070312
Iteration 10/25 | Loss: 0.00070312
Iteration 11/25 | Loss: 0.00070312
Iteration 12/25 | Loss: 0.00070312
Iteration 13/25 | Loss: 0.00070312
Iteration 14/25 | Loss: 0.00070312
Iteration 15/25 | Loss: 0.00070312
Iteration 16/25 | Loss: 0.00070312
Iteration 17/25 | Loss: 0.00070312
Iteration 18/25 | Loss: 0.00070312
Iteration 19/25 | Loss: 0.00070312
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007031180430203676, 0.0007031180430203676, 0.0007031180430203676, 0.0007031180430203676, 0.0007031180430203676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007031180430203676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070312
Iteration 2/1000 | Loss: 0.00003120
Iteration 3/1000 | Loss: 0.00002247
Iteration 4/1000 | Loss: 0.00002106
Iteration 5/1000 | Loss: 0.00002020
Iteration 6/1000 | Loss: 0.00001962
Iteration 7/1000 | Loss: 0.00001937
Iteration 8/1000 | Loss: 0.00001902
Iteration 9/1000 | Loss: 0.00001884
Iteration 10/1000 | Loss: 0.00001880
Iteration 11/1000 | Loss: 0.00001878
Iteration 12/1000 | Loss: 0.00001861
Iteration 13/1000 | Loss: 0.00001860
Iteration 14/1000 | Loss: 0.00001858
Iteration 15/1000 | Loss: 0.00001839
Iteration 16/1000 | Loss: 0.00001839
Iteration 17/1000 | Loss: 0.00001837
Iteration 18/1000 | Loss: 0.00001832
Iteration 19/1000 | Loss: 0.00001825
Iteration 20/1000 | Loss: 0.00001825
Iteration 21/1000 | Loss: 0.00001825
Iteration 22/1000 | Loss: 0.00001825
Iteration 23/1000 | Loss: 0.00001825
Iteration 24/1000 | Loss: 0.00001825
Iteration 25/1000 | Loss: 0.00001824
Iteration 26/1000 | Loss: 0.00001824
Iteration 27/1000 | Loss: 0.00001824
Iteration 28/1000 | Loss: 0.00001817
Iteration 29/1000 | Loss: 0.00001815
Iteration 30/1000 | Loss: 0.00001814
Iteration 31/1000 | Loss: 0.00001814
Iteration 32/1000 | Loss: 0.00001813
Iteration 33/1000 | Loss: 0.00001812
Iteration 34/1000 | Loss: 0.00001812
Iteration 35/1000 | Loss: 0.00001811
Iteration 36/1000 | Loss: 0.00001811
Iteration 37/1000 | Loss: 0.00001811
Iteration 38/1000 | Loss: 0.00001811
Iteration 39/1000 | Loss: 0.00001810
Iteration 40/1000 | Loss: 0.00001810
Iteration 41/1000 | Loss: 0.00001810
Iteration 42/1000 | Loss: 0.00001810
Iteration 43/1000 | Loss: 0.00001810
Iteration 44/1000 | Loss: 0.00001810
Iteration 45/1000 | Loss: 0.00001810
Iteration 46/1000 | Loss: 0.00001809
Iteration 47/1000 | Loss: 0.00001809
Iteration 48/1000 | Loss: 0.00001809
Iteration 49/1000 | Loss: 0.00001808
Iteration 50/1000 | Loss: 0.00001808
Iteration 51/1000 | Loss: 0.00001808
Iteration 52/1000 | Loss: 0.00001808
Iteration 53/1000 | Loss: 0.00001808
Iteration 54/1000 | Loss: 0.00001807
Iteration 55/1000 | Loss: 0.00001807
Iteration 56/1000 | Loss: 0.00001806
Iteration 57/1000 | Loss: 0.00001806
Iteration 58/1000 | Loss: 0.00001806
Iteration 59/1000 | Loss: 0.00001806
Iteration 60/1000 | Loss: 0.00001806
Iteration 61/1000 | Loss: 0.00001806
Iteration 62/1000 | Loss: 0.00001806
Iteration 63/1000 | Loss: 0.00001806
Iteration 64/1000 | Loss: 0.00001806
Iteration 65/1000 | Loss: 0.00001806
Iteration 66/1000 | Loss: 0.00001806
Iteration 67/1000 | Loss: 0.00001805
Iteration 68/1000 | Loss: 0.00001805
Iteration 69/1000 | Loss: 0.00001805
Iteration 70/1000 | Loss: 0.00001805
Iteration 71/1000 | Loss: 0.00001805
Iteration 72/1000 | Loss: 0.00001805
Iteration 73/1000 | Loss: 0.00001805
Iteration 74/1000 | Loss: 0.00001805
Iteration 75/1000 | Loss: 0.00001805
Iteration 76/1000 | Loss: 0.00001805
Iteration 77/1000 | Loss: 0.00001805
Iteration 78/1000 | Loss: 0.00001805
Iteration 79/1000 | Loss: 0.00001804
Iteration 80/1000 | Loss: 0.00001804
Iteration 81/1000 | Loss: 0.00001804
Iteration 82/1000 | Loss: 0.00001804
Iteration 83/1000 | Loss: 0.00001803
Iteration 84/1000 | Loss: 0.00001803
Iteration 85/1000 | Loss: 0.00001803
Iteration 86/1000 | Loss: 0.00001803
Iteration 87/1000 | Loss: 0.00001803
Iteration 88/1000 | Loss: 0.00001803
Iteration 89/1000 | Loss: 0.00001803
Iteration 90/1000 | Loss: 0.00001802
Iteration 91/1000 | Loss: 0.00001802
Iteration 92/1000 | Loss: 0.00001802
Iteration 93/1000 | Loss: 0.00001802
Iteration 94/1000 | Loss: 0.00001802
Iteration 95/1000 | Loss: 0.00001802
Iteration 96/1000 | Loss: 0.00001802
Iteration 97/1000 | Loss: 0.00001802
Iteration 98/1000 | Loss: 0.00001802
Iteration 99/1000 | Loss: 0.00001802
Iteration 100/1000 | Loss: 0.00001801
Iteration 101/1000 | Loss: 0.00001801
Iteration 102/1000 | Loss: 0.00001801
Iteration 103/1000 | Loss: 0.00001801
Iteration 104/1000 | Loss: 0.00001801
Iteration 105/1000 | Loss: 0.00001801
Iteration 106/1000 | Loss: 0.00001801
Iteration 107/1000 | Loss: 0.00001801
Iteration 108/1000 | Loss: 0.00001801
Iteration 109/1000 | Loss: 0.00001801
Iteration 110/1000 | Loss: 0.00001800
Iteration 111/1000 | Loss: 0.00001800
Iteration 112/1000 | Loss: 0.00001800
Iteration 113/1000 | Loss: 0.00001800
Iteration 114/1000 | Loss: 0.00001800
Iteration 115/1000 | Loss: 0.00001800
Iteration 116/1000 | Loss: 0.00001800
Iteration 117/1000 | Loss: 0.00001799
Iteration 118/1000 | Loss: 0.00001799
Iteration 119/1000 | Loss: 0.00001799
Iteration 120/1000 | Loss: 0.00001799
Iteration 121/1000 | Loss: 0.00001799
Iteration 122/1000 | Loss: 0.00001798
Iteration 123/1000 | Loss: 0.00001798
Iteration 124/1000 | Loss: 0.00001798
Iteration 125/1000 | Loss: 0.00001798
Iteration 126/1000 | Loss: 0.00001798
Iteration 127/1000 | Loss: 0.00001798
Iteration 128/1000 | Loss: 0.00001798
Iteration 129/1000 | Loss: 0.00001797
Iteration 130/1000 | Loss: 0.00001797
Iteration 131/1000 | Loss: 0.00001797
Iteration 132/1000 | Loss: 0.00001797
Iteration 133/1000 | Loss: 0.00001797
Iteration 134/1000 | Loss: 0.00001797
Iteration 135/1000 | Loss: 0.00001797
Iteration 136/1000 | Loss: 0.00001797
Iteration 137/1000 | Loss: 0.00001796
Iteration 138/1000 | Loss: 0.00001796
Iteration 139/1000 | Loss: 0.00001796
Iteration 140/1000 | Loss: 0.00001796
Iteration 141/1000 | Loss: 0.00001796
Iteration 142/1000 | Loss: 0.00001796
Iteration 143/1000 | Loss: 0.00001795
Iteration 144/1000 | Loss: 0.00001795
Iteration 145/1000 | Loss: 0.00001795
Iteration 146/1000 | Loss: 0.00001795
Iteration 147/1000 | Loss: 0.00001795
Iteration 148/1000 | Loss: 0.00001795
Iteration 149/1000 | Loss: 0.00001794
Iteration 150/1000 | Loss: 0.00001794
Iteration 151/1000 | Loss: 0.00001794
Iteration 152/1000 | Loss: 0.00001794
Iteration 153/1000 | Loss: 0.00001794
Iteration 154/1000 | Loss: 0.00001794
Iteration 155/1000 | Loss: 0.00001794
Iteration 156/1000 | Loss: 0.00001794
Iteration 157/1000 | Loss: 0.00001794
Iteration 158/1000 | Loss: 0.00001794
Iteration 159/1000 | Loss: 0.00001794
Iteration 160/1000 | Loss: 0.00001794
Iteration 161/1000 | Loss: 0.00001794
Iteration 162/1000 | Loss: 0.00001794
Iteration 163/1000 | Loss: 0.00001794
Iteration 164/1000 | Loss: 0.00001794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.794118361431174e-05, 1.794118361431174e-05, 1.794118361431174e-05, 1.794118361431174e-05, 1.794118361431174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.794118361431174e-05

Optimization complete. Final v2v error: 3.543653726577759 mm

Highest mean error: 3.710348606109619 mm for frame 174

Lowest mean error: 3.3441007137298584 mm for frame 0

Saving results

Total time: 40.44274687767029
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436083
Iteration 2/25 | Loss: 0.00117381
Iteration 3/25 | Loss: 0.00110562
Iteration 4/25 | Loss: 0.00109040
Iteration 5/25 | Loss: 0.00108550
Iteration 6/25 | Loss: 0.00108467
Iteration 7/25 | Loss: 0.00108467
Iteration 8/25 | Loss: 0.00108467
Iteration 9/25 | Loss: 0.00108467
Iteration 10/25 | Loss: 0.00108467
Iteration 11/25 | Loss: 0.00108467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010846656514331698, 0.0010846656514331698, 0.0010846656514331698, 0.0010846656514331698, 0.0010846656514331698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010846656514331698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51250923
Iteration 2/25 | Loss: 0.00080990
Iteration 3/25 | Loss: 0.00080989
Iteration 4/25 | Loss: 0.00080989
Iteration 5/25 | Loss: 0.00080989
Iteration 6/25 | Loss: 0.00080989
Iteration 7/25 | Loss: 0.00080989
Iteration 8/25 | Loss: 0.00080989
Iteration 9/25 | Loss: 0.00080989
Iteration 10/25 | Loss: 0.00080989
Iteration 11/25 | Loss: 0.00080989
Iteration 12/25 | Loss: 0.00080989
Iteration 13/25 | Loss: 0.00080989
Iteration 14/25 | Loss: 0.00080989
Iteration 15/25 | Loss: 0.00080989
Iteration 16/25 | Loss: 0.00080989
Iteration 17/25 | Loss: 0.00080989
Iteration 18/25 | Loss: 0.00080989
Iteration 19/25 | Loss: 0.00080989
Iteration 20/25 | Loss: 0.00080989
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008098908583633602, 0.0008098908583633602, 0.0008098908583633602, 0.0008098908583633602, 0.0008098908583633602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008098908583633602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080989
Iteration 2/1000 | Loss: 0.00002467
Iteration 3/1000 | Loss: 0.00001721
Iteration 4/1000 | Loss: 0.00001525
Iteration 5/1000 | Loss: 0.00001458
Iteration 6/1000 | Loss: 0.00001392
Iteration 7/1000 | Loss: 0.00001353
Iteration 8/1000 | Loss: 0.00001319
Iteration 9/1000 | Loss: 0.00001293
Iteration 10/1000 | Loss: 0.00001271
Iteration 11/1000 | Loss: 0.00001259
Iteration 12/1000 | Loss: 0.00001250
Iteration 13/1000 | Loss: 0.00001249
Iteration 14/1000 | Loss: 0.00001248
Iteration 15/1000 | Loss: 0.00001244
Iteration 16/1000 | Loss: 0.00001240
Iteration 17/1000 | Loss: 0.00001239
Iteration 18/1000 | Loss: 0.00001228
Iteration 19/1000 | Loss: 0.00001226
Iteration 20/1000 | Loss: 0.00001222
Iteration 21/1000 | Loss: 0.00001220
Iteration 22/1000 | Loss: 0.00001219
Iteration 23/1000 | Loss: 0.00001219
Iteration 24/1000 | Loss: 0.00001219
Iteration 25/1000 | Loss: 0.00001218
Iteration 26/1000 | Loss: 0.00001218
Iteration 27/1000 | Loss: 0.00001217
Iteration 28/1000 | Loss: 0.00001216
Iteration 29/1000 | Loss: 0.00001214
Iteration 30/1000 | Loss: 0.00001212
Iteration 31/1000 | Loss: 0.00001210
Iteration 32/1000 | Loss: 0.00001209
Iteration 33/1000 | Loss: 0.00001209
Iteration 34/1000 | Loss: 0.00001209
Iteration 35/1000 | Loss: 0.00001209
Iteration 36/1000 | Loss: 0.00001209
Iteration 37/1000 | Loss: 0.00001209
Iteration 38/1000 | Loss: 0.00001208
Iteration 39/1000 | Loss: 0.00001207
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001202
Iteration 42/1000 | Loss: 0.00001201
Iteration 43/1000 | Loss: 0.00001200
Iteration 44/1000 | Loss: 0.00001199
Iteration 45/1000 | Loss: 0.00001199
Iteration 46/1000 | Loss: 0.00001199
Iteration 47/1000 | Loss: 0.00001199
Iteration 48/1000 | Loss: 0.00001198
Iteration 49/1000 | Loss: 0.00001198
Iteration 50/1000 | Loss: 0.00001198
Iteration 51/1000 | Loss: 0.00001196
Iteration 52/1000 | Loss: 0.00001196
Iteration 53/1000 | Loss: 0.00001195
Iteration 54/1000 | Loss: 0.00001195
Iteration 55/1000 | Loss: 0.00001194
Iteration 56/1000 | Loss: 0.00001194
Iteration 57/1000 | Loss: 0.00001194
Iteration 58/1000 | Loss: 0.00001194
Iteration 59/1000 | Loss: 0.00001194
Iteration 60/1000 | Loss: 0.00001194
Iteration 61/1000 | Loss: 0.00001193
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001192
Iteration 64/1000 | Loss: 0.00001192
Iteration 65/1000 | Loss: 0.00001190
Iteration 66/1000 | Loss: 0.00001190
Iteration 67/1000 | Loss: 0.00001189
Iteration 68/1000 | Loss: 0.00001189
Iteration 69/1000 | Loss: 0.00001189
Iteration 70/1000 | Loss: 0.00001188
Iteration 71/1000 | Loss: 0.00001188
Iteration 72/1000 | Loss: 0.00001186
Iteration 73/1000 | Loss: 0.00001186
Iteration 74/1000 | Loss: 0.00001186
Iteration 75/1000 | Loss: 0.00001185
Iteration 76/1000 | Loss: 0.00001185
Iteration 77/1000 | Loss: 0.00001184
Iteration 78/1000 | Loss: 0.00001184
Iteration 79/1000 | Loss: 0.00001184
Iteration 80/1000 | Loss: 0.00001183
Iteration 81/1000 | Loss: 0.00001183
Iteration 82/1000 | Loss: 0.00001183
Iteration 83/1000 | Loss: 0.00001182
Iteration 84/1000 | Loss: 0.00001182
Iteration 85/1000 | Loss: 0.00001182
Iteration 86/1000 | Loss: 0.00001182
Iteration 87/1000 | Loss: 0.00001182
Iteration 88/1000 | Loss: 0.00001182
Iteration 89/1000 | Loss: 0.00001182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.1823804015875794e-05, 1.1823804015875794e-05, 1.1823804015875794e-05, 1.1823804015875794e-05, 1.1823804015875794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1823804015875794e-05

Optimization complete. Final v2v error: 2.951664924621582 mm

Highest mean error: 3.3598053455352783 mm for frame 113

Lowest mean error: 2.8568522930145264 mm for frame 83

Saving results

Total time: 33.95304489135742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998851
Iteration 2/25 | Loss: 0.00998851
Iteration 3/25 | Loss: 0.00998851
Iteration 4/25 | Loss: 0.00291771
Iteration 5/25 | Loss: 0.00188931
Iteration 6/25 | Loss: 0.00178236
Iteration 7/25 | Loss: 0.00173826
Iteration 8/25 | Loss: 0.00160432
Iteration 9/25 | Loss: 0.00139958
Iteration 10/25 | Loss: 0.00134788
Iteration 11/25 | Loss: 0.00131348
Iteration 12/25 | Loss: 0.00129861
Iteration 13/25 | Loss: 0.00128327
Iteration 14/25 | Loss: 0.00127329
Iteration 15/25 | Loss: 0.00126412
Iteration 16/25 | Loss: 0.00125331
Iteration 17/25 | Loss: 0.00125226
Iteration 18/25 | Loss: 0.00125892
Iteration 19/25 | Loss: 0.00125456
Iteration 20/25 | Loss: 0.00125448
Iteration 21/25 | Loss: 0.00125236
Iteration 22/25 | Loss: 0.00124945
Iteration 23/25 | Loss: 0.00124802
Iteration 24/25 | Loss: 0.00125397
Iteration 25/25 | Loss: 0.00125471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34519458
Iteration 2/25 | Loss: 0.00261473
Iteration 3/25 | Loss: 0.00251011
Iteration 4/25 | Loss: 0.00251011
Iteration 5/25 | Loss: 0.00251011
Iteration 6/25 | Loss: 0.00251011
Iteration 7/25 | Loss: 0.00251011
Iteration 8/25 | Loss: 0.00251011
Iteration 9/25 | Loss: 0.00251011
Iteration 10/25 | Loss: 0.00251011
Iteration 11/25 | Loss: 0.00251011
Iteration 12/25 | Loss: 0.00251011
Iteration 13/25 | Loss: 0.00251011
Iteration 14/25 | Loss: 0.00251011
Iteration 15/25 | Loss: 0.00251011
Iteration 16/25 | Loss: 0.00251011
Iteration 17/25 | Loss: 0.00251011
Iteration 18/25 | Loss: 0.00251011
Iteration 19/25 | Loss: 0.00251011
Iteration 20/25 | Loss: 0.00251011
Iteration 21/25 | Loss: 0.00251011
Iteration 22/25 | Loss: 0.00251011
Iteration 23/25 | Loss: 0.00251011
Iteration 24/25 | Loss: 0.00251011
Iteration 25/25 | Loss: 0.00251011
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0025101066567003727, 0.0025101066567003727, 0.0025101066567003727, 0.0025101066567003727, 0.0025101066567003727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025101066567003727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00251011
Iteration 2/1000 | Loss: 0.00053283
Iteration 3/1000 | Loss: 0.00042849
Iteration 4/1000 | Loss: 0.00043409
Iteration 5/1000 | Loss: 0.00051490
Iteration 6/1000 | Loss: 0.00016165
Iteration 7/1000 | Loss: 0.00026850
Iteration 8/1000 | Loss: 0.00029079
Iteration 9/1000 | Loss: 0.00019391
Iteration 10/1000 | Loss: 0.00014304
Iteration 11/1000 | Loss: 0.00020504
Iteration 12/1000 | Loss: 0.00024359
Iteration 13/1000 | Loss: 0.00054034
Iteration 14/1000 | Loss: 0.00031075
Iteration 15/1000 | Loss: 0.00030732
Iteration 16/1000 | Loss: 0.00031152
Iteration 17/1000 | Loss: 0.00027678
Iteration 18/1000 | Loss: 0.00024910
Iteration 19/1000 | Loss: 0.00015609
Iteration 20/1000 | Loss: 0.00016449
Iteration 21/1000 | Loss: 0.00028438
Iteration 22/1000 | Loss: 0.00025925
Iteration 23/1000 | Loss: 0.00009283
Iteration 24/1000 | Loss: 0.00023344
Iteration 25/1000 | Loss: 0.00410855
Iteration 26/1000 | Loss: 0.00223812
Iteration 27/1000 | Loss: 0.00240376
Iteration 28/1000 | Loss: 0.00082145
Iteration 29/1000 | Loss: 0.00054621
Iteration 30/1000 | Loss: 0.00051775
Iteration 31/1000 | Loss: 0.00061347
Iteration 32/1000 | Loss: 0.00021325
Iteration 33/1000 | Loss: 0.00010862
Iteration 34/1000 | Loss: 0.00041886
Iteration 35/1000 | Loss: 0.00028945
Iteration 36/1000 | Loss: 0.00009889
Iteration 37/1000 | Loss: 0.00022321
Iteration 38/1000 | Loss: 0.00027087
Iteration 39/1000 | Loss: 0.00043640
Iteration 40/1000 | Loss: 0.00033742
Iteration 41/1000 | Loss: 0.00020053
Iteration 42/1000 | Loss: 0.00036989
Iteration 43/1000 | Loss: 0.00035676
Iteration 44/1000 | Loss: 0.00026322
Iteration 45/1000 | Loss: 0.00016558
Iteration 46/1000 | Loss: 0.00063466
Iteration 47/1000 | Loss: 0.00022194
Iteration 48/1000 | Loss: 0.00009339
Iteration 49/1000 | Loss: 0.00005743
Iteration 50/1000 | Loss: 0.00017194
Iteration 51/1000 | Loss: 0.00005413
Iteration 52/1000 | Loss: 0.00019117
Iteration 53/1000 | Loss: 0.00020963
Iteration 54/1000 | Loss: 0.00004356
Iteration 55/1000 | Loss: 0.00046132
Iteration 56/1000 | Loss: 0.00027001
Iteration 57/1000 | Loss: 0.00010067
Iteration 58/1000 | Loss: 0.00016086
Iteration 59/1000 | Loss: 0.00025977
Iteration 60/1000 | Loss: 0.00033849
Iteration 61/1000 | Loss: 0.00011912
Iteration 62/1000 | Loss: 0.00032912
Iteration 63/1000 | Loss: 0.00019027
Iteration 64/1000 | Loss: 0.00040255
Iteration 65/1000 | Loss: 0.00021458
Iteration 66/1000 | Loss: 0.00027134
Iteration 67/1000 | Loss: 0.00031432
Iteration 68/1000 | Loss: 0.00029734
Iteration 69/1000 | Loss: 0.00022754
Iteration 70/1000 | Loss: 0.00008248
Iteration 71/1000 | Loss: 0.00008565
Iteration 72/1000 | Loss: 0.00014882
Iteration 73/1000 | Loss: 0.00013574
Iteration 74/1000 | Loss: 0.00017490
Iteration 75/1000 | Loss: 0.00011358
Iteration 76/1000 | Loss: 0.00004647
Iteration 77/1000 | Loss: 0.00007019
Iteration 78/1000 | Loss: 0.00004980
Iteration 79/1000 | Loss: 0.00007226
Iteration 80/1000 | Loss: 0.00004296
Iteration 81/1000 | Loss: 0.00005585
Iteration 82/1000 | Loss: 0.00032481
Iteration 83/1000 | Loss: 0.00008555
Iteration 84/1000 | Loss: 0.00003384
Iteration 85/1000 | Loss: 0.00014985
Iteration 86/1000 | Loss: 0.00003123
Iteration 87/1000 | Loss: 0.00002724
Iteration 88/1000 | Loss: 0.00016367
Iteration 89/1000 | Loss: 0.00007234
Iteration 90/1000 | Loss: 0.00002411
Iteration 91/1000 | Loss: 0.00002144
Iteration 92/1000 | Loss: 0.00031150
Iteration 93/1000 | Loss: 0.00024539
Iteration 94/1000 | Loss: 0.00020009
Iteration 95/1000 | Loss: 0.00042661
Iteration 96/1000 | Loss: 0.00019720
Iteration 97/1000 | Loss: 0.00016803
Iteration 98/1000 | Loss: 0.00016936
Iteration 99/1000 | Loss: 0.00007201
Iteration 100/1000 | Loss: 0.00006302
Iteration 101/1000 | Loss: 0.00004495
Iteration 102/1000 | Loss: 0.00011525
Iteration 103/1000 | Loss: 0.00009989
Iteration 104/1000 | Loss: 0.00002768
Iteration 105/1000 | Loss: 0.00006313
Iteration 106/1000 | Loss: 0.00006993
Iteration 107/1000 | Loss: 0.00021812
Iteration 108/1000 | Loss: 0.00010198
Iteration 109/1000 | Loss: 0.00007131
Iteration 110/1000 | Loss: 0.00002051
Iteration 111/1000 | Loss: 0.00016095
Iteration 112/1000 | Loss: 0.00006762
Iteration 113/1000 | Loss: 0.00002276
Iteration 114/1000 | Loss: 0.00019808
Iteration 115/1000 | Loss: 0.00023080
Iteration 116/1000 | Loss: 0.00010990
Iteration 117/1000 | Loss: 0.00002621
Iteration 118/1000 | Loss: 0.00021324
Iteration 119/1000 | Loss: 0.00004419
Iteration 120/1000 | Loss: 0.00002682
Iteration 121/1000 | Loss: 0.00006969
Iteration 122/1000 | Loss: 0.00030251
Iteration 123/1000 | Loss: 0.00030904
Iteration 124/1000 | Loss: 0.00039506
Iteration 125/1000 | Loss: 0.00059951
Iteration 126/1000 | Loss: 0.00017737
Iteration 127/1000 | Loss: 0.00002233
Iteration 128/1000 | Loss: 0.00004639
Iteration 129/1000 | Loss: 0.00002156
Iteration 130/1000 | Loss: 0.00021096
Iteration 131/1000 | Loss: 0.00002965
Iteration 132/1000 | Loss: 0.00006380
Iteration 133/1000 | Loss: 0.00020419
Iteration 134/1000 | Loss: 0.00006425
Iteration 135/1000 | Loss: 0.00020459
Iteration 136/1000 | Loss: 0.00010547
Iteration 137/1000 | Loss: 0.00006724
Iteration 138/1000 | Loss: 0.00013575
Iteration 139/1000 | Loss: 0.00002988
Iteration 140/1000 | Loss: 0.00002155
Iteration 141/1000 | Loss: 0.00015343
Iteration 142/1000 | Loss: 0.00018930
Iteration 143/1000 | Loss: 0.00011722
Iteration 144/1000 | Loss: 0.00006758
Iteration 145/1000 | Loss: 0.00011539
Iteration 146/1000 | Loss: 0.00007583
Iteration 147/1000 | Loss: 0.00015533
Iteration 148/1000 | Loss: 0.00002723
Iteration 149/1000 | Loss: 0.00002438
Iteration 150/1000 | Loss: 0.00001929
Iteration 151/1000 | Loss: 0.00001866
Iteration 152/1000 | Loss: 0.00007335
Iteration 153/1000 | Loss: 0.00009489
Iteration 154/1000 | Loss: 0.00009578
Iteration 155/1000 | Loss: 0.00002799
Iteration 156/1000 | Loss: 0.00002065
Iteration 157/1000 | Loss: 0.00001996
Iteration 158/1000 | Loss: 0.00002634
Iteration 159/1000 | Loss: 0.00001822
Iteration 160/1000 | Loss: 0.00003778
Iteration 161/1000 | Loss: 0.00002993
Iteration 162/1000 | Loss: 0.00001737
Iteration 163/1000 | Loss: 0.00002369
Iteration 164/1000 | Loss: 0.00001892
Iteration 165/1000 | Loss: 0.00001695
Iteration 166/1000 | Loss: 0.00001663
Iteration 167/1000 | Loss: 0.00001654
Iteration 168/1000 | Loss: 0.00001653
Iteration 169/1000 | Loss: 0.00001652
Iteration 170/1000 | Loss: 0.00001652
Iteration 171/1000 | Loss: 0.00001651
Iteration 172/1000 | Loss: 0.00001651
Iteration 173/1000 | Loss: 0.00001650
Iteration 174/1000 | Loss: 0.00001646
Iteration 175/1000 | Loss: 0.00001725
Iteration 176/1000 | Loss: 0.00001633
Iteration 177/1000 | Loss: 0.00001632
Iteration 178/1000 | Loss: 0.00001632
Iteration 179/1000 | Loss: 0.00001623
Iteration 180/1000 | Loss: 0.00001721
Iteration 181/1000 | Loss: 0.00046650
Iteration 182/1000 | Loss: 0.00020827
Iteration 183/1000 | Loss: 0.00007451
Iteration 184/1000 | Loss: 0.00004224
Iteration 185/1000 | Loss: 0.00002130
Iteration 186/1000 | Loss: 0.00002603
Iteration 187/1000 | Loss: 0.00032521
Iteration 188/1000 | Loss: 0.00006238
Iteration 189/1000 | Loss: 0.00002799
Iteration 190/1000 | Loss: 0.00007610
Iteration 191/1000 | Loss: 0.00026828
Iteration 192/1000 | Loss: 0.00020639
Iteration 193/1000 | Loss: 0.00150551
Iteration 194/1000 | Loss: 0.00004556
Iteration 195/1000 | Loss: 0.00023806
Iteration 196/1000 | Loss: 0.00054422
Iteration 197/1000 | Loss: 0.00043227
Iteration 198/1000 | Loss: 0.00005261
Iteration 199/1000 | Loss: 0.00002313
Iteration 200/1000 | Loss: 0.00001745
Iteration 201/1000 | Loss: 0.00015591
Iteration 202/1000 | Loss: 0.00003426
Iteration 203/1000 | Loss: 0.00001876
Iteration 204/1000 | Loss: 0.00001700
Iteration 205/1000 | Loss: 0.00001696
Iteration 206/1000 | Loss: 0.00001696
Iteration 207/1000 | Loss: 0.00001695
Iteration 208/1000 | Loss: 0.00001695
Iteration 209/1000 | Loss: 0.00001695
Iteration 210/1000 | Loss: 0.00001694
Iteration 211/1000 | Loss: 0.00001693
Iteration 212/1000 | Loss: 0.00001692
Iteration 213/1000 | Loss: 0.00001692
Iteration 214/1000 | Loss: 0.00009778
Iteration 215/1000 | Loss: 0.00003242
Iteration 216/1000 | Loss: 0.00001969
Iteration 217/1000 | Loss: 0.00005712
Iteration 218/1000 | Loss: 0.00015313
Iteration 219/1000 | Loss: 0.00002775
Iteration 220/1000 | Loss: 0.00002197
Iteration 221/1000 | Loss: 0.00002285
Iteration 222/1000 | Loss: 0.00002520
Iteration 223/1000 | Loss: 0.00001840
Iteration 224/1000 | Loss: 0.00001770
Iteration 225/1000 | Loss: 0.00006684
Iteration 226/1000 | Loss: 0.00001925
Iteration 227/1000 | Loss: 0.00001690
Iteration 228/1000 | Loss: 0.00001662
Iteration 229/1000 | Loss: 0.00002241
Iteration 230/1000 | Loss: 0.00002945
Iteration 231/1000 | Loss: 0.00001845
Iteration 232/1000 | Loss: 0.00001887
Iteration 233/1000 | Loss: 0.00001628
Iteration 234/1000 | Loss: 0.00001627
Iteration 235/1000 | Loss: 0.00056639
Iteration 236/1000 | Loss: 0.00006314
Iteration 237/1000 | Loss: 0.00003163
Iteration 238/1000 | Loss: 0.00002078
Iteration 239/1000 | Loss: 0.00055373
Iteration 240/1000 | Loss: 0.00001694
Iteration 241/1000 | Loss: 0.00001500
Iteration 242/1000 | Loss: 0.00001428
Iteration 243/1000 | Loss: 0.00001404
Iteration 244/1000 | Loss: 0.00001387
Iteration 245/1000 | Loss: 0.00001374
Iteration 246/1000 | Loss: 0.00001374
Iteration 247/1000 | Loss: 0.00001367
Iteration 248/1000 | Loss: 0.00002611
Iteration 249/1000 | Loss: 0.00001654
Iteration 250/1000 | Loss: 0.00001624
Iteration 251/1000 | Loss: 0.00001361
Iteration 252/1000 | Loss: 0.00001361
Iteration 253/1000 | Loss: 0.00001609
Iteration 254/1000 | Loss: 0.00001475
Iteration 255/1000 | Loss: 0.00001349
Iteration 256/1000 | Loss: 0.00001349
Iteration 257/1000 | Loss: 0.00001349
Iteration 258/1000 | Loss: 0.00001349
Iteration 259/1000 | Loss: 0.00001349
Iteration 260/1000 | Loss: 0.00001348
Iteration 261/1000 | Loss: 0.00001348
Iteration 262/1000 | Loss: 0.00001348
Iteration 263/1000 | Loss: 0.00001348
Iteration 264/1000 | Loss: 0.00001348
Iteration 265/1000 | Loss: 0.00001348
Iteration 266/1000 | Loss: 0.00001348
Iteration 267/1000 | Loss: 0.00001348
Iteration 268/1000 | Loss: 0.00001348
Iteration 269/1000 | Loss: 0.00001348
Iteration 270/1000 | Loss: 0.00001347
Iteration 271/1000 | Loss: 0.00001346
Iteration 272/1000 | Loss: 0.00001345
Iteration 273/1000 | Loss: 0.00001345
Iteration 274/1000 | Loss: 0.00001344
Iteration 275/1000 | Loss: 0.00001343
Iteration 276/1000 | Loss: 0.00001343
Iteration 277/1000 | Loss: 0.00001342
Iteration 278/1000 | Loss: 0.00001342
Iteration 279/1000 | Loss: 0.00014135
Iteration 280/1000 | Loss: 0.00006145
Iteration 281/1000 | Loss: 0.00007107
Iteration 282/1000 | Loss: 0.00001755
Iteration 283/1000 | Loss: 0.00001470
Iteration 284/1000 | Loss: 0.00001360
Iteration 285/1000 | Loss: 0.00001585
Iteration 286/1000 | Loss: 0.00001337
Iteration 287/1000 | Loss: 0.00001245
Iteration 288/1000 | Loss: 0.00001208
Iteration 289/1000 | Loss: 0.00001207
Iteration 290/1000 | Loss: 0.00001204
Iteration 291/1000 | Loss: 0.00001188
Iteration 292/1000 | Loss: 0.00001187
Iteration 293/1000 | Loss: 0.00001187
Iteration 294/1000 | Loss: 0.00001185
Iteration 295/1000 | Loss: 0.00001183
Iteration 296/1000 | Loss: 0.00001183
Iteration 297/1000 | Loss: 0.00001182
Iteration 298/1000 | Loss: 0.00001181
Iteration 299/1000 | Loss: 0.00001181
Iteration 300/1000 | Loss: 0.00001180
Iteration 301/1000 | Loss: 0.00001179
Iteration 302/1000 | Loss: 0.00001179
Iteration 303/1000 | Loss: 0.00001177
Iteration 304/1000 | Loss: 0.00001177
Iteration 305/1000 | Loss: 0.00001177
Iteration 306/1000 | Loss: 0.00001177
Iteration 307/1000 | Loss: 0.00001177
Iteration 308/1000 | Loss: 0.00001177
Iteration 309/1000 | Loss: 0.00001176
Iteration 310/1000 | Loss: 0.00001176
Iteration 311/1000 | Loss: 0.00001176
Iteration 312/1000 | Loss: 0.00001176
Iteration 313/1000 | Loss: 0.00001176
Iteration 314/1000 | Loss: 0.00001176
Iteration 315/1000 | Loss: 0.00001176
Iteration 316/1000 | Loss: 0.00001176
Iteration 317/1000 | Loss: 0.00001176
Iteration 318/1000 | Loss: 0.00001176
Iteration 319/1000 | Loss: 0.00001176
Iteration 320/1000 | Loss: 0.00001175
Iteration 321/1000 | Loss: 0.00001175
Iteration 322/1000 | Loss: 0.00001175
Iteration 323/1000 | Loss: 0.00001175
Iteration 324/1000 | Loss: 0.00001175
Iteration 325/1000 | Loss: 0.00001174
Iteration 326/1000 | Loss: 0.00001174
Iteration 327/1000 | Loss: 0.00001174
Iteration 328/1000 | Loss: 0.00001174
Iteration 329/1000 | Loss: 0.00001174
Iteration 330/1000 | Loss: 0.00001174
Iteration 331/1000 | Loss: 0.00001174
Iteration 332/1000 | Loss: 0.00001174
Iteration 333/1000 | Loss: 0.00001174
Iteration 334/1000 | Loss: 0.00001173
Iteration 335/1000 | Loss: 0.00001173
Iteration 336/1000 | Loss: 0.00001173
Iteration 337/1000 | Loss: 0.00001173
Iteration 338/1000 | Loss: 0.00001173
Iteration 339/1000 | Loss: 0.00001173
Iteration 340/1000 | Loss: 0.00001173
Iteration 341/1000 | Loss: 0.00001173
Iteration 342/1000 | Loss: 0.00001173
Iteration 343/1000 | Loss: 0.00001173
Iteration 344/1000 | Loss: 0.00001173
Iteration 345/1000 | Loss: 0.00001173
Iteration 346/1000 | Loss: 0.00001173
Iteration 347/1000 | Loss: 0.00001173
Iteration 348/1000 | Loss: 0.00001173
Iteration 349/1000 | Loss: 0.00001173
Iteration 350/1000 | Loss: 0.00001173
Iteration 351/1000 | Loss: 0.00001173
Iteration 352/1000 | Loss: 0.00001173
Iteration 353/1000 | Loss: 0.00001173
Iteration 354/1000 | Loss: 0.00001173
Iteration 355/1000 | Loss: 0.00001173
Iteration 356/1000 | Loss: 0.00001173
Iteration 357/1000 | Loss: 0.00001173
Iteration 358/1000 | Loss: 0.00001173
Iteration 359/1000 | Loss: 0.00001173
Iteration 360/1000 | Loss: 0.00001173
Iteration 361/1000 | Loss: 0.00001173
Iteration 362/1000 | Loss: 0.00001173
Iteration 363/1000 | Loss: 0.00001173
Iteration 364/1000 | Loss: 0.00001173
Iteration 365/1000 | Loss: 0.00001173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 365. Stopping optimization.
Last 5 losses: [1.1726098819053732e-05, 1.1726098819053732e-05, 1.1726098819053732e-05, 1.1726098819053732e-05, 1.1726098819053732e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1726098819053732e-05

Optimization complete. Final v2v error: 2.5821006298065186 mm

Highest mean error: 10.370417594909668 mm for frame 73

Lowest mean error: 2.2087767124176025 mm for frame 220

Saving results

Total time: 437.7975857257843
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002756
Iteration 2/25 | Loss: 0.01002754
Iteration 3/25 | Loss: 0.00347181
Iteration 4/25 | Loss: 0.00252318
Iteration 5/25 | Loss: 0.00232586
Iteration 6/25 | Loss: 0.00207509
Iteration 7/25 | Loss: 0.00152661
Iteration 8/25 | Loss: 0.00140158
Iteration 9/25 | Loss: 0.00135722
Iteration 10/25 | Loss: 0.00128163
Iteration 11/25 | Loss: 0.00126367
Iteration 12/25 | Loss: 0.00125960
Iteration 13/25 | Loss: 0.00125867
Iteration 14/25 | Loss: 0.00125842
Iteration 15/25 | Loss: 0.00125830
Iteration 16/25 | Loss: 0.00125830
Iteration 17/25 | Loss: 0.00125830
Iteration 18/25 | Loss: 0.00125830
Iteration 19/25 | Loss: 0.00125830
Iteration 20/25 | Loss: 0.00125830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012582953786477447, 0.0012582953786477447, 0.0012582953786477447, 0.0012582953786477447, 0.0012582953786477447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012582953786477447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32891834
Iteration 2/25 | Loss: 0.00095739
Iteration 3/25 | Loss: 0.00095739
Iteration 4/25 | Loss: 0.00095739
Iteration 5/25 | Loss: 0.00095739
Iteration 6/25 | Loss: 0.00095739
Iteration 7/25 | Loss: 0.00095738
Iteration 8/25 | Loss: 0.00095738
Iteration 9/25 | Loss: 0.00095738
Iteration 10/25 | Loss: 0.00095738
Iteration 11/25 | Loss: 0.00095738
Iteration 12/25 | Loss: 0.00095738
Iteration 13/25 | Loss: 0.00095738
Iteration 14/25 | Loss: 0.00095738
Iteration 15/25 | Loss: 0.00095738
Iteration 16/25 | Loss: 0.00095738
Iteration 17/25 | Loss: 0.00095738
Iteration 18/25 | Loss: 0.00095738
Iteration 19/25 | Loss: 0.00095738
Iteration 20/25 | Loss: 0.00095738
Iteration 21/25 | Loss: 0.00095738
Iteration 22/25 | Loss: 0.00095738
Iteration 23/25 | Loss: 0.00095738
Iteration 24/25 | Loss: 0.00095738
Iteration 25/25 | Loss: 0.00095738

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095738
Iteration 2/1000 | Loss: 0.00006990
Iteration 3/1000 | Loss: 0.00005000
Iteration 4/1000 | Loss: 0.00004585
Iteration 5/1000 | Loss: 0.00004328
Iteration 6/1000 | Loss: 0.00004171
Iteration 7/1000 | Loss: 0.00004037
Iteration 8/1000 | Loss: 0.00003952
Iteration 9/1000 | Loss: 0.00003881
Iteration 10/1000 | Loss: 0.00003808
Iteration 11/1000 | Loss: 0.00009040
Iteration 12/1000 | Loss: 0.00027149
Iteration 13/1000 | Loss: 0.00022494
Iteration 14/1000 | Loss: 0.00008483
Iteration 15/1000 | Loss: 0.00006671
Iteration 16/1000 | Loss: 0.00004372
Iteration 17/1000 | Loss: 0.00003814
Iteration 18/1000 | Loss: 0.00003431
Iteration 19/1000 | Loss: 0.00003130
Iteration 20/1000 | Loss: 0.00002923
Iteration 21/1000 | Loss: 0.00002775
Iteration 22/1000 | Loss: 0.00002644
Iteration 23/1000 | Loss: 0.00012487
Iteration 24/1000 | Loss: 0.00002952
Iteration 25/1000 | Loss: 0.00002641
Iteration 26/1000 | Loss: 0.00002511
Iteration 27/1000 | Loss: 0.00002471
Iteration 28/1000 | Loss: 0.00002428
Iteration 29/1000 | Loss: 0.00002403
Iteration 30/1000 | Loss: 0.00002395
Iteration 31/1000 | Loss: 0.00002381
Iteration 32/1000 | Loss: 0.00002380
Iteration 33/1000 | Loss: 0.00002380
Iteration 34/1000 | Loss: 0.00002372
Iteration 35/1000 | Loss: 0.00002369
Iteration 36/1000 | Loss: 0.00002369
Iteration 37/1000 | Loss: 0.00002368
Iteration 38/1000 | Loss: 0.00002367
Iteration 39/1000 | Loss: 0.00002366
Iteration 40/1000 | Loss: 0.00002364
Iteration 41/1000 | Loss: 0.00002360
Iteration 42/1000 | Loss: 0.00002360
Iteration 43/1000 | Loss: 0.00002360
Iteration 44/1000 | Loss: 0.00002360
Iteration 45/1000 | Loss: 0.00002360
Iteration 46/1000 | Loss: 0.00002360
Iteration 47/1000 | Loss: 0.00002359
Iteration 48/1000 | Loss: 0.00002359
Iteration 49/1000 | Loss: 0.00002359
Iteration 50/1000 | Loss: 0.00002358
Iteration 51/1000 | Loss: 0.00002358
Iteration 52/1000 | Loss: 0.00002357
Iteration 53/1000 | Loss: 0.00002356
Iteration 54/1000 | Loss: 0.00002354
Iteration 55/1000 | Loss: 0.00002354
Iteration 56/1000 | Loss: 0.00002353
Iteration 57/1000 | Loss: 0.00002351
Iteration 58/1000 | Loss: 0.00002351
Iteration 59/1000 | Loss: 0.00002350
Iteration 60/1000 | Loss: 0.00002350
Iteration 61/1000 | Loss: 0.00002349
Iteration 62/1000 | Loss: 0.00002349
Iteration 63/1000 | Loss: 0.00002349
Iteration 64/1000 | Loss: 0.00002349
Iteration 65/1000 | Loss: 0.00002348
Iteration 66/1000 | Loss: 0.00002348
Iteration 67/1000 | Loss: 0.00002348
Iteration 68/1000 | Loss: 0.00002348
Iteration 69/1000 | Loss: 0.00002348
Iteration 70/1000 | Loss: 0.00002348
Iteration 71/1000 | Loss: 0.00002348
Iteration 72/1000 | Loss: 0.00002348
Iteration 73/1000 | Loss: 0.00002347
Iteration 74/1000 | Loss: 0.00002347
Iteration 75/1000 | Loss: 0.00002347
Iteration 76/1000 | Loss: 0.00002347
Iteration 77/1000 | Loss: 0.00002347
Iteration 78/1000 | Loss: 0.00002347
Iteration 79/1000 | Loss: 0.00002347
Iteration 80/1000 | Loss: 0.00002347
Iteration 81/1000 | Loss: 0.00002347
Iteration 82/1000 | Loss: 0.00002347
Iteration 83/1000 | Loss: 0.00002347
Iteration 84/1000 | Loss: 0.00002346
Iteration 85/1000 | Loss: 0.00002346
Iteration 86/1000 | Loss: 0.00002346
Iteration 87/1000 | Loss: 0.00002346
Iteration 88/1000 | Loss: 0.00002346
Iteration 89/1000 | Loss: 0.00002346
Iteration 90/1000 | Loss: 0.00002346
Iteration 91/1000 | Loss: 0.00002346
Iteration 92/1000 | Loss: 0.00002345
Iteration 93/1000 | Loss: 0.00002345
Iteration 94/1000 | Loss: 0.00002344
Iteration 95/1000 | Loss: 0.00002344
Iteration 96/1000 | Loss: 0.00002344
Iteration 97/1000 | Loss: 0.00002344
Iteration 98/1000 | Loss: 0.00002344
Iteration 99/1000 | Loss: 0.00002344
Iteration 100/1000 | Loss: 0.00002344
Iteration 101/1000 | Loss: 0.00002344
Iteration 102/1000 | Loss: 0.00002344
Iteration 103/1000 | Loss: 0.00002344
Iteration 104/1000 | Loss: 0.00002344
Iteration 105/1000 | Loss: 0.00002343
Iteration 106/1000 | Loss: 0.00002343
Iteration 107/1000 | Loss: 0.00002343
Iteration 108/1000 | Loss: 0.00002343
Iteration 109/1000 | Loss: 0.00002343
Iteration 110/1000 | Loss: 0.00002343
Iteration 111/1000 | Loss: 0.00002343
Iteration 112/1000 | Loss: 0.00002343
Iteration 113/1000 | Loss: 0.00002343
Iteration 114/1000 | Loss: 0.00002343
Iteration 115/1000 | Loss: 0.00002343
Iteration 116/1000 | Loss: 0.00002343
Iteration 117/1000 | Loss: 0.00002343
Iteration 118/1000 | Loss: 0.00002343
Iteration 119/1000 | Loss: 0.00002343
Iteration 120/1000 | Loss: 0.00002343
Iteration 121/1000 | Loss: 0.00002343
Iteration 122/1000 | Loss: 0.00002342
Iteration 123/1000 | Loss: 0.00002342
Iteration 124/1000 | Loss: 0.00002342
Iteration 125/1000 | Loss: 0.00002342
Iteration 126/1000 | Loss: 0.00002342
Iteration 127/1000 | Loss: 0.00002342
Iteration 128/1000 | Loss: 0.00002342
Iteration 129/1000 | Loss: 0.00002341
Iteration 130/1000 | Loss: 0.00002341
Iteration 131/1000 | Loss: 0.00002341
Iteration 132/1000 | Loss: 0.00002341
Iteration 133/1000 | Loss: 0.00002341
Iteration 134/1000 | Loss: 0.00002341
Iteration 135/1000 | Loss: 0.00002341
Iteration 136/1000 | Loss: 0.00002340
Iteration 137/1000 | Loss: 0.00002340
Iteration 138/1000 | Loss: 0.00002340
Iteration 139/1000 | Loss: 0.00002340
Iteration 140/1000 | Loss: 0.00002340
Iteration 141/1000 | Loss: 0.00002340
Iteration 142/1000 | Loss: 0.00002340
Iteration 143/1000 | Loss: 0.00002339
Iteration 144/1000 | Loss: 0.00002339
Iteration 145/1000 | Loss: 0.00002339
Iteration 146/1000 | Loss: 0.00002339
Iteration 147/1000 | Loss: 0.00002339
Iteration 148/1000 | Loss: 0.00002339
Iteration 149/1000 | Loss: 0.00002339
Iteration 150/1000 | Loss: 0.00002339
Iteration 151/1000 | Loss: 0.00002338
Iteration 152/1000 | Loss: 0.00002338
Iteration 153/1000 | Loss: 0.00002338
Iteration 154/1000 | Loss: 0.00002338
Iteration 155/1000 | Loss: 0.00002338
Iteration 156/1000 | Loss: 0.00002338
Iteration 157/1000 | Loss: 0.00002338
Iteration 158/1000 | Loss: 0.00002338
Iteration 159/1000 | Loss: 0.00002338
Iteration 160/1000 | Loss: 0.00002337
Iteration 161/1000 | Loss: 0.00002337
Iteration 162/1000 | Loss: 0.00002337
Iteration 163/1000 | Loss: 0.00002337
Iteration 164/1000 | Loss: 0.00002337
Iteration 165/1000 | Loss: 0.00002337
Iteration 166/1000 | Loss: 0.00002337
Iteration 167/1000 | Loss: 0.00002337
Iteration 168/1000 | Loss: 0.00002337
Iteration 169/1000 | Loss: 0.00002337
Iteration 170/1000 | Loss: 0.00002337
Iteration 171/1000 | Loss: 0.00002337
Iteration 172/1000 | Loss: 0.00002337
Iteration 173/1000 | Loss: 0.00002337
Iteration 174/1000 | Loss: 0.00002337
Iteration 175/1000 | Loss: 0.00002337
Iteration 176/1000 | Loss: 0.00002337
Iteration 177/1000 | Loss: 0.00002337
Iteration 178/1000 | Loss: 0.00002337
Iteration 179/1000 | Loss: 0.00002337
Iteration 180/1000 | Loss: 0.00002337
Iteration 181/1000 | Loss: 0.00002337
Iteration 182/1000 | Loss: 0.00002337
Iteration 183/1000 | Loss: 0.00002337
Iteration 184/1000 | Loss: 0.00002337
Iteration 185/1000 | Loss: 0.00002337
Iteration 186/1000 | Loss: 0.00002337
Iteration 187/1000 | Loss: 0.00002337
Iteration 188/1000 | Loss: 0.00002337
Iteration 189/1000 | Loss: 0.00002337
Iteration 190/1000 | Loss: 0.00002337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.336675788683351e-05, 2.336675788683351e-05, 2.336675788683351e-05, 2.336675788683351e-05, 2.336675788683351e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.336675788683351e-05

Optimization complete. Final v2v error: 4.107601165771484 mm

Highest mean error: 4.459324359893799 mm for frame 103

Lowest mean error: 3.9949753284454346 mm for frame 97

Saving results

Total time: 92.89233756065369
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00688709
Iteration 2/25 | Loss: 0.00123064
Iteration 3/25 | Loss: 0.00111083
Iteration 4/25 | Loss: 0.00108855
Iteration 5/25 | Loss: 0.00108199
Iteration 6/25 | Loss: 0.00108052
Iteration 7/25 | Loss: 0.00108052
Iteration 8/25 | Loss: 0.00108052
Iteration 9/25 | Loss: 0.00108052
Iteration 10/25 | Loss: 0.00108052
Iteration 11/25 | Loss: 0.00108052
Iteration 12/25 | Loss: 0.00108052
Iteration 13/25 | Loss: 0.00108052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010805233614519238, 0.0010805233614519238, 0.0010805233614519238, 0.0010805233614519238, 0.0010805233614519238]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010805233614519238

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.97275972
Iteration 2/25 | Loss: 0.00096879
Iteration 3/25 | Loss: 0.00096873
Iteration 4/25 | Loss: 0.00096873
Iteration 5/25 | Loss: 0.00096873
Iteration 6/25 | Loss: 0.00096873
Iteration 7/25 | Loss: 0.00096873
Iteration 8/25 | Loss: 0.00096873
Iteration 9/25 | Loss: 0.00096873
Iteration 10/25 | Loss: 0.00096873
Iteration 11/25 | Loss: 0.00096873
Iteration 12/25 | Loss: 0.00096873
Iteration 13/25 | Loss: 0.00096873
Iteration 14/25 | Loss: 0.00096873
Iteration 15/25 | Loss: 0.00096873
Iteration 16/25 | Loss: 0.00096873
Iteration 17/25 | Loss: 0.00096873
Iteration 18/25 | Loss: 0.00096873
Iteration 19/25 | Loss: 0.00096873
Iteration 20/25 | Loss: 0.00096873
Iteration 21/25 | Loss: 0.00096873
Iteration 22/25 | Loss: 0.00096873
Iteration 23/25 | Loss: 0.00096873
Iteration 24/25 | Loss: 0.00096873
Iteration 25/25 | Loss: 0.00096873

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096873
Iteration 2/1000 | Loss: 0.00003090
Iteration 3/1000 | Loss: 0.00001851
Iteration 4/1000 | Loss: 0.00001623
Iteration 5/1000 | Loss: 0.00001537
Iteration 6/1000 | Loss: 0.00001454
Iteration 7/1000 | Loss: 0.00001401
Iteration 8/1000 | Loss: 0.00001361
Iteration 9/1000 | Loss: 0.00001334
Iteration 10/1000 | Loss: 0.00001333
Iteration 11/1000 | Loss: 0.00001307
Iteration 12/1000 | Loss: 0.00001282
Iteration 13/1000 | Loss: 0.00001276
Iteration 14/1000 | Loss: 0.00001275
Iteration 15/1000 | Loss: 0.00001275
Iteration 16/1000 | Loss: 0.00001274
Iteration 17/1000 | Loss: 0.00001271
Iteration 18/1000 | Loss: 0.00001270
Iteration 19/1000 | Loss: 0.00001268
Iteration 20/1000 | Loss: 0.00001259
Iteration 21/1000 | Loss: 0.00001250
Iteration 22/1000 | Loss: 0.00001240
Iteration 23/1000 | Loss: 0.00001237
Iteration 24/1000 | Loss: 0.00001231
Iteration 25/1000 | Loss: 0.00001222
Iteration 26/1000 | Loss: 0.00001217
Iteration 27/1000 | Loss: 0.00001216
Iteration 28/1000 | Loss: 0.00001214
Iteration 29/1000 | Loss: 0.00001214
Iteration 30/1000 | Loss: 0.00001213
Iteration 31/1000 | Loss: 0.00001212
Iteration 32/1000 | Loss: 0.00001211
Iteration 33/1000 | Loss: 0.00001207
Iteration 34/1000 | Loss: 0.00001207
Iteration 35/1000 | Loss: 0.00001205
Iteration 36/1000 | Loss: 0.00001204
Iteration 37/1000 | Loss: 0.00001203
Iteration 38/1000 | Loss: 0.00001203
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001201
Iteration 41/1000 | Loss: 0.00001200
Iteration 42/1000 | Loss: 0.00001198
Iteration 43/1000 | Loss: 0.00001198
Iteration 44/1000 | Loss: 0.00001198
Iteration 45/1000 | Loss: 0.00001198
Iteration 46/1000 | Loss: 0.00001197
Iteration 47/1000 | Loss: 0.00001197
Iteration 48/1000 | Loss: 0.00001197
Iteration 49/1000 | Loss: 0.00001197
Iteration 50/1000 | Loss: 0.00001196
Iteration 51/1000 | Loss: 0.00001196
Iteration 52/1000 | Loss: 0.00001195
Iteration 53/1000 | Loss: 0.00001195
Iteration 54/1000 | Loss: 0.00001194
Iteration 55/1000 | Loss: 0.00001194
Iteration 56/1000 | Loss: 0.00001194
Iteration 57/1000 | Loss: 0.00001193
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001193
Iteration 61/1000 | Loss: 0.00001192
Iteration 62/1000 | Loss: 0.00001192
Iteration 63/1000 | Loss: 0.00001192
Iteration 64/1000 | Loss: 0.00001191
Iteration 65/1000 | Loss: 0.00001191
Iteration 66/1000 | Loss: 0.00001191
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001191
Iteration 69/1000 | Loss: 0.00001191
Iteration 70/1000 | Loss: 0.00001190
Iteration 71/1000 | Loss: 0.00001190
Iteration 72/1000 | Loss: 0.00001190
Iteration 73/1000 | Loss: 0.00001189
Iteration 74/1000 | Loss: 0.00001189
Iteration 75/1000 | Loss: 0.00001189
Iteration 76/1000 | Loss: 0.00001189
Iteration 77/1000 | Loss: 0.00001188
Iteration 78/1000 | Loss: 0.00001188
Iteration 79/1000 | Loss: 0.00001188
Iteration 80/1000 | Loss: 0.00001187
Iteration 81/1000 | Loss: 0.00001187
Iteration 82/1000 | Loss: 0.00001186
Iteration 83/1000 | Loss: 0.00001186
Iteration 84/1000 | Loss: 0.00001186
Iteration 85/1000 | Loss: 0.00001186
Iteration 86/1000 | Loss: 0.00001186
Iteration 87/1000 | Loss: 0.00001185
Iteration 88/1000 | Loss: 0.00001185
Iteration 89/1000 | Loss: 0.00001185
Iteration 90/1000 | Loss: 0.00001185
Iteration 91/1000 | Loss: 0.00001185
Iteration 92/1000 | Loss: 0.00001185
Iteration 93/1000 | Loss: 0.00001185
Iteration 94/1000 | Loss: 0.00001185
Iteration 95/1000 | Loss: 0.00001184
Iteration 96/1000 | Loss: 0.00001184
Iteration 97/1000 | Loss: 0.00001184
Iteration 98/1000 | Loss: 0.00001184
Iteration 99/1000 | Loss: 0.00001183
Iteration 100/1000 | Loss: 0.00001183
Iteration 101/1000 | Loss: 0.00001183
Iteration 102/1000 | Loss: 0.00001183
Iteration 103/1000 | Loss: 0.00001183
Iteration 104/1000 | Loss: 0.00001183
Iteration 105/1000 | Loss: 0.00001183
Iteration 106/1000 | Loss: 0.00001183
Iteration 107/1000 | Loss: 0.00001183
Iteration 108/1000 | Loss: 0.00001183
Iteration 109/1000 | Loss: 0.00001182
Iteration 110/1000 | Loss: 0.00001182
Iteration 111/1000 | Loss: 0.00001182
Iteration 112/1000 | Loss: 0.00001182
Iteration 113/1000 | Loss: 0.00001181
Iteration 114/1000 | Loss: 0.00001181
Iteration 115/1000 | Loss: 0.00001181
Iteration 116/1000 | Loss: 0.00001181
Iteration 117/1000 | Loss: 0.00001181
Iteration 118/1000 | Loss: 0.00001181
Iteration 119/1000 | Loss: 0.00001181
Iteration 120/1000 | Loss: 0.00001181
Iteration 121/1000 | Loss: 0.00001181
Iteration 122/1000 | Loss: 0.00001181
Iteration 123/1000 | Loss: 0.00001181
Iteration 124/1000 | Loss: 0.00001181
Iteration 125/1000 | Loss: 0.00001181
Iteration 126/1000 | Loss: 0.00001181
Iteration 127/1000 | Loss: 0.00001181
Iteration 128/1000 | Loss: 0.00001181
Iteration 129/1000 | Loss: 0.00001181
Iteration 130/1000 | Loss: 0.00001181
Iteration 131/1000 | Loss: 0.00001181
Iteration 132/1000 | Loss: 0.00001181
Iteration 133/1000 | Loss: 0.00001181
Iteration 134/1000 | Loss: 0.00001181
Iteration 135/1000 | Loss: 0.00001181
Iteration 136/1000 | Loss: 0.00001181
Iteration 137/1000 | Loss: 0.00001181
Iteration 138/1000 | Loss: 0.00001181
Iteration 139/1000 | Loss: 0.00001181
Iteration 140/1000 | Loss: 0.00001181
Iteration 141/1000 | Loss: 0.00001181
Iteration 142/1000 | Loss: 0.00001181
Iteration 143/1000 | Loss: 0.00001181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.1808911949628964e-05, 1.1808911949628964e-05, 1.1808911949628964e-05, 1.1808911949628964e-05, 1.1808911949628964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1808911949628964e-05

Optimization complete. Final v2v error: 2.9566404819488525 mm

Highest mean error: 3.2727348804473877 mm for frame 21

Lowest mean error: 2.718256711959839 mm for frame 45

Saving results

Total time: 45.29210329055786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898655
Iteration 2/25 | Loss: 0.00179333
Iteration 3/25 | Loss: 0.00144552
Iteration 4/25 | Loss: 0.00139255
Iteration 5/25 | Loss: 0.00140145
Iteration 6/25 | Loss: 0.00140782
Iteration 7/25 | Loss: 0.00137428
Iteration 8/25 | Loss: 0.00136485
Iteration 9/25 | Loss: 0.00136188
Iteration 10/25 | Loss: 0.00136145
Iteration 11/25 | Loss: 0.00136051
Iteration 12/25 | Loss: 0.00136099
Iteration 13/25 | Loss: 0.00136235
Iteration 14/25 | Loss: 0.00135894
Iteration 15/25 | Loss: 0.00135867
Iteration 16/25 | Loss: 0.00135469
Iteration 17/25 | Loss: 0.00135212
Iteration 18/25 | Loss: 0.00134907
Iteration 19/25 | Loss: 0.00134813
Iteration 20/25 | Loss: 0.00135077
Iteration 21/25 | Loss: 0.00134632
Iteration 22/25 | Loss: 0.00134515
Iteration 23/25 | Loss: 0.00134441
Iteration 24/25 | Loss: 0.00134400
Iteration 25/25 | Loss: 0.00134381

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54978311
Iteration 2/25 | Loss: 0.00104475
Iteration 3/25 | Loss: 0.00104475
Iteration 4/25 | Loss: 0.00104475
Iteration 5/25 | Loss: 0.00104475
Iteration 6/25 | Loss: 0.00104475
Iteration 7/25 | Loss: 0.00104475
Iteration 8/25 | Loss: 0.00104475
Iteration 9/25 | Loss: 0.00104475
Iteration 10/25 | Loss: 0.00104475
Iteration 11/25 | Loss: 0.00104475
Iteration 12/25 | Loss: 0.00104475
Iteration 13/25 | Loss: 0.00104475
Iteration 14/25 | Loss: 0.00104475
Iteration 15/25 | Loss: 0.00104475
Iteration 16/25 | Loss: 0.00104475
Iteration 17/25 | Loss: 0.00104475
Iteration 18/25 | Loss: 0.00104475
Iteration 19/25 | Loss: 0.00104475
Iteration 20/25 | Loss: 0.00104475
Iteration 21/25 | Loss: 0.00104475
Iteration 22/25 | Loss: 0.00104475
Iteration 23/25 | Loss: 0.00104475
Iteration 24/25 | Loss: 0.00104475
Iteration 25/25 | Loss: 0.00104475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104475
Iteration 2/1000 | Loss: 0.00009306
Iteration 3/1000 | Loss: 0.00006417
Iteration 4/1000 | Loss: 0.00004784
Iteration 5/1000 | Loss: 0.00004139
Iteration 6/1000 | Loss: 0.00003787
Iteration 7/1000 | Loss: 0.00003594
Iteration 8/1000 | Loss: 0.00003438
Iteration 9/1000 | Loss: 0.00003328
Iteration 10/1000 | Loss: 0.00003260
Iteration 11/1000 | Loss: 0.00003187
Iteration 12/1000 | Loss: 0.00003131
Iteration 13/1000 | Loss: 0.00003099
Iteration 14/1000 | Loss: 0.00003079
Iteration 15/1000 | Loss: 0.00003061
Iteration 16/1000 | Loss: 0.00003046
Iteration 17/1000 | Loss: 0.00003036
Iteration 18/1000 | Loss: 0.00003029
Iteration 19/1000 | Loss: 0.00003025
Iteration 20/1000 | Loss: 0.00003019
Iteration 21/1000 | Loss: 0.00003019
Iteration 22/1000 | Loss: 0.00003017
Iteration 23/1000 | Loss: 0.00003017
Iteration 24/1000 | Loss: 0.00003016
Iteration 25/1000 | Loss: 0.00003016
Iteration 26/1000 | Loss: 0.00003014
Iteration 27/1000 | Loss: 0.00003012
Iteration 28/1000 | Loss: 0.00003011
Iteration 29/1000 | Loss: 0.00003010
Iteration 30/1000 | Loss: 0.00003010
Iteration 31/1000 | Loss: 0.00003008
Iteration 32/1000 | Loss: 0.00003007
Iteration 33/1000 | Loss: 0.00003007
Iteration 34/1000 | Loss: 0.00003007
Iteration 35/1000 | Loss: 0.00003007
Iteration 36/1000 | Loss: 0.00003007
Iteration 37/1000 | Loss: 0.00003007
Iteration 38/1000 | Loss: 0.00003006
Iteration 39/1000 | Loss: 0.00003006
Iteration 40/1000 | Loss: 0.00003006
Iteration 41/1000 | Loss: 0.00003006
Iteration 42/1000 | Loss: 0.00003006
Iteration 43/1000 | Loss: 0.00003006
Iteration 44/1000 | Loss: 0.00003006
Iteration 45/1000 | Loss: 0.00003006
Iteration 46/1000 | Loss: 0.00003006
Iteration 47/1000 | Loss: 0.00003005
Iteration 48/1000 | Loss: 0.00003005
Iteration 49/1000 | Loss: 0.00003005
Iteration 50/1000 | Loss: 0.00003004
Iteration 51/1000 | Loss: 0.00003004
Iteration 52/1000 | Loss: 0.00003004
Iteration 53/1000 | Loss: 0.00003003
Iteration 54/1000 | Loss: 0.00003003
Iteration 55/1000 | Loss: 0.00003003
Iteration 56/1000 | Loss: 0.00003003
Iteration 57/1000 | Loss: 0.00003002
Iteration 58/1000 | Loss: 0.00003002
Iteration 59/1000 | Loss: 0.00003001
Iteration 60/1000 | Loss: 0.00003001
Iteration 61/1000 | Loss: 0.00003001
Iteration 62/1000 | Loss: 0.00003001
Iteration 63/1000 | Loss: 0.00003000
Iteration 64/1000 | Loss: 0.00003000
Iteration 65/1000 | Loss: 0.00002999
Iteration 66/1000 | Loss: 0.00002999
Iteration 67/1000 | Loss: 0.00002998
Iteration 68/1000 | Loss: 0.00002998
Iteration 69/1000 | Loss: 0.00002998
Iteration 70/1000 | Loss: 0.00002997
Iteration 71/1000 | Loss: 0.00002997
Iteration 72/1000 | Loss: 0.00002997
Iteration 73/1000 | Loss: 0.00002997
Iteration 74/1000 | Loss: 0.00002996
Iteration 75/1000 | Loss: 0.00002996
Iteration 76/1000 | Loss: 0.00002996
Iteration 77/1000 | Loss: 0.00002996
Iteration 78/1000 | Loss: 0.00002996
Iteration 79/1000 | Loss: 0.00002996
Iteration 80/1000 | Loss: 0.00002996
Iteration 81/1000 | Loss: 0.00002996
Iteration 82/1000 | Loss: 0.00002996
Iteration 83/1000 | Loss: 0.00002996
Iteration 84/1000 | Loss: 0.00002996
Iteration 85/1000 | Loss: 0.00002995
Iteration 86/1000 | Loss: 0.00002995
Iteration 87/1000 | Loss: 0.00002995
Iteration 88/1000 | Loss: 0.00002995
Iteration 89/1000 | Loss: 0.00002995
Iteration 90/1000 | Loss: 0.00002995
Iteration 91/1000 | Loss: 0.00002995
Iteration 92/1000 | Loss: 0.00002995
Iteration 93/1000 | Loss: 0.00002994
Iteration 94/1000 | Loss: 0.00002994
Iteration 95/1000 | Loss: 0.00002994
Iteration 96/1000 | Loss: 0.00002994
Iteration 97/1000 | Loss: 0.00002993
Iteration 98/1000 | Loss: 0.00002993
Iteration 99/1000 | Loss: 0.00002993
Iteration 100/1000 | Loss: 0.00002993
Iteration 101/1000 | Loss: 0.00002993
Iteration 102/1000 | Loss: 0.00002992
Iteration 103/1000 | Loss: 0.00002992
Iteration 104/1000 | Loss: 0.00002992
Iteration 105/1000 | Loss: 0.00002992
Iteration 106/1000 | Loss: 0.00002992
Iteration 107/1000 | Loss: 0.00002992
Iteration 108/1000 | Loss: 0.00002992
Iteration 109/1000 | Loss: 0.00002991
Iteration 110/1000 | Loss: 0.00002991
Iteration 111/1000 | Loss: 0.00002991
Iteration 112/1000 | Loss: 0.00002991
Iteration 113/1000 | Loss: 0.00002991
Iteration 114/1000 | Loss: 0.00002991
Iteration 115/1000 | Loss: 0.00002991
Iteration 116/1000 | Loss: 0.00002991
Iteration 117/1000 | Loss: 0.00002991
Iteration 118/1000 | Loss: 0.00002991
Iteration 119/1000 | Loss: 0.00002991
Iteration 120/1000 | Loss: 0.00002991
Iteration 121/1000 | Loss: 0.00002991
Iteration 122/1000 | Loss: 0.00002991
Iteration 123/1000 | Loss: 0.00002991
Iteration 124/1000 | Loss: 0.00002991
Iteration 125/1000 | Loss: 0.00002991
Iteration 126/1000 | Loss: 0.00002991
Iteration 127/1000 | Loss: 0.00002991
Iteration 128/1000 | Loss: 0.00002991
Iteration 129/1000 | Loss: 0.00002991
Iteration 130/1000 | Loss: 0.00002991
Iteration 131/1000 | Loss: 0.00002991
Iteration 132/1000 | Loss: 0.00002991
Iteration 133/1000 | Loss: 0.00002991
Iteration 134/1000 | Loss: 0.00002991
Iteration 135/1000 | Loss: 0.00002991
Iteration 136/1000 | Loss: 0.00002991
Iteration 137/1000 | Loss: 0.00002991
Iteration 138/1000 | Loss: 0.00002991
Iteration 139/1000 | Loss: 0.00002991
Iteration 140/1000 | Loss: 0.00002991
Iteration 141/1000 | Loss: 0.00002991
Iteration 142/1000 | Loss: 0.00002991
Iteration 143/1000 | Loss: 0.00002991
Iteration 144/1000 | Loss: 0.00002991
Iteration 145/1000 | Loss: 0.00002991
Iteration 146/1000 | Loss: 0.00002991
Iteration 147/1000 | Loss: 0.00002991
Iteration 148/1000 | Loss: 0.00002991
Iteration 149/1000 | Loss: 0.00002991
Iteration 150/1000 | Loss: 0.00002991
Iteration 151/1000 | Loss: 0.00002991
Iteration 152/1000 | Loss: 0.00002991
Iteration 153/1000 | Loss: 0.00002991
Iteration 154/1000 | Loss: 0.00002991
Iteration 155/1000 | Loss: 0.00002991
Iteration 156/1000 | Loss: 0.00002991
Iteration 157/1000 | Loss: 0.00002991
Iteration 158/1000 | Loss: 0.00002991
Iteration 159/1000 | Loss: 0.00002991
Iteration 160/1000 | Loss: 0.00002991
Iteration 161/1000 | Loss: 0.00002991
Iteration 162/1000 | Loss: 0.00002991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.9906115742051043e-05, 2.9906115742051043e-05, 2.9906115742051043e-05, 2.9906115742051043e-05, 2.9906115742051043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9906115742051043e-05

Optimization complete. Final v2v error: 4.266064643859863 mm

Highest mean error: 12.789530754089355 mm for frame 11

Lowest mean error: 3.938462018966675 mm for frame 32

Saving results

Total time: 85.31852602958679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960161
Iteration 2/25 | Loss: 0.00274180
Iteration 3/25 | Loss: 0.00205367
Iteration 4/25 | Loss: 0.00189378
Iteration 5/25 | Loss: 0.00195684
Iteration 6/25 | Loss: 0.00173742
Iteration 7/25 | Loss: 0.00164910
Iteration 8/25 | Loss: 0.00166824
Iteration 9/25 | Loss: 0.00161651
Iteration 10/25 | Loss: 0.00153166
Iteration 11/25 | Loss: 0.00149699
Iteration 12/25 | Loss: 0.00150241
Iteration 13/25 | Loss: 0.00147967
Iteration 14/25 | Loss: 0.00145723
Iteration 15/25 | Loss: 0.00146013
Iteration 16/25 | Loss: 0.00145400
Iteration 17/25 | Loss: 0.00144890
Iteration 18/25 | Loss: 0.00144322
Iteration 19/25 | Loss: 0.00144096
Iteration 20/25 | Loss: 0.00144022
Iteration 21/25 | Loss: 0.00143956
Iteration 22/25 | Loss: 0.00144251
Iteration 23/25 | Loss: 0.00143728
Iteration 24/25 | Loss: 0.00143658
Iteration 25/25 | Loss: 0.00143640

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32396138
Iteration 2/25 | Loss: 0.00204268
Iteration 3/25 | Loss: 0.00204268
Iteration 4/25 | Loss: 0.00203308
Iteration 5/25 | Loss: 0.00203308
Iteration 6/25 | Loss: 0.00203308
Iteration 7/25 | Loss: 0.00203308
Iteration 8/25 | Loss: 0.00203308
Iteration 9/25 | Loss: 0.00203308
Iteration 10/25 | Loss: 0.00203308
Iteration 11/25 | Loss: 0.00203308
Iteration 12/25 | Loss: 0.00203308
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0020330774132162333, 0.0020330774132162333, 0.0020330774132162333, 0.0020330774132162333, 0.0020330774132162333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020330774132162333

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00203308
Iteration 2/1000 | Loss: 0.00042421
Iteration 3/1000 | Loss: 0.00040910
Iteration 4/1000 | Loss: 0.00037575
Iteration 5/1000 | Loss: 0.00088152
Iteration 6/1000 | Loss: 0.00074057
Iteration 7/1000 | Loss: 0.00021025
Iteration 8/1000 | Loss: 0.00060970
Iteration 9/1000 | Loss: 0.00048342
Iteration 10/1000 | Loss: 0.00116850
Iteration 11/1000 | Loss: 0.00184237
Iteration 12/1000 | Loss: 0.00088860
Iteration 13/1000 | Loss: 0.00067922
Iteration 14/1000 | Loss: 0.00020597
Iteration 15/1000 | Loss: 0.00019981
Iteration 16/1000 | Loss: 0.00067270
Iteration 17/1000 | Loss: 0.00126742
Iteration 18/1000 | Loss: 0.00048128
Iteration 19/1000 | Loss: 0.00097316
Iteration 20/1000 | Loss: 0.00057266
Iteration 21/1000 | Loss: 0.00021323
Iteration 22/1000 | Loss: 0.00043163
Iteration 23/1000 | Loss: 0.00028240
Iteration 24/1000 | Loss: 0.00008714
Iteration 25/1000 | Loss: 0.00010330
Iteration 26/1000 | Loss: 0.00009252
Iteration 27/1000 | Loss: 0.00009488
Iteration 28/1000 | Loss: 0.00005610
Iteration 29/1000 | Loss: 0.00007331
Iteration 30/1000 | Loss: 0.00037990
Iteration 31/1000 | Loss: 0.00003482
Iteration 32/1000 | Loss: 0.00004556
Iteration 33/1000 | Loss: 0.00035623
Iteration 34/1000 | Loss: 0.00041220
Iteration 35/1000 | Loss: 0.00011233
Iteration 36/1000 | Loss: 0.00004677
Iteration 37/1000 | Loss: 0.00003134
Iteration 38/1000 | Loss: 0.00027623
Iteration 39/1000 | Loss: 0.00022812
Iteration 40/1000 | Loss: 0.00025855
Iteration 41/1000 | Loss: 0.00015573
Iteration 42/1000 | Loss: 0.00016782
Iteration 43/1000 | Loss: 0.00009186
Iteration 44/1000 | Loss: 0.00006878
Iteration 45/1000 | Loss: 0.00002110
Iteration 46/1000 | Loss: 0.00001985
Iteration 47/1000 | Loss: 0.00001902
Iteration 48/1000 | Loss: 0.00002347
Iteration 49/1000 | Loss: 0.00001997
Iteration 50/1000 | Loss: 0.00001841
Iteration 51/1000 | Loss: 0.00001781
Iteration 52/1000 | Loss: 0.00001747
Iteration 53/1000 | Loss: 0.00003881
Iteration 54/1000 | Loss: 0.00001836
Iteration 55/1000 | Loss: 0.00002883
Iteration 56/1000 | Loss: 0.00001840
Iteration 57/1000 | Loss: 0.00001682
Iteration 58/1000 | Loss: 0.00001644
Iteration 59/1000 | Loss: 0.00001620
Iteration 60/1000 | Loss: 0.00001611
Iteration 61/1000 | Loss: 0.00001586
Iteration 62/1000 | Loss: 0.00001579
Iteration 63/1000 | Loss: 0.00001576
Iteration 64/1000 | Loss: 0.00001576
Iteration 65/1000 | Loss: 0.00001576
Iteration 66/1000 | Loss: 0.00001575
Iteration 67/1000 | Loss: 0.00001573
Iteration 68/1000 | Loss: 0.00001572
Iteration 69/1000 | Loss: 0.00001572
Iteration 70/1000 | Loss: 0.00001572
Iteration 71/1000 | Loss: 0.00001571
Iteration 72/1000 | Loss: 0.00001571
Iteration 73/1000 | Loss: 0.00001571
Iteration 74/1000 | Loss: 0.00001571
Iteration 75/1000 | Loss: 0.00001571
Iteration 76/1000 | Loss: 0.00001571
Iteration 77/1000 | Loss: 0.00001571
Iteration 78/1000 | Loss: 0.00001571
Iteration 79/1000 | Loss: 0.00001571
Iteration 80/1000 | Loss: 0.00001571
Iteration 81/1000 | Loss: 0.00001571
Iteration 82/1000 | Loss: 0.00001570
Iteration 83/1000 | Loss: 0.00001570
Iteration 84/1000 | Loss: 0.00001570
Iteration 85/1000 | Loss: 0.00001570
Iteration 86/1000 | Loss: 0.00001569
Iteration 87/1000 | Loss: 0.00001569
Iteration 88/1000 | Loss: 0.00001568
Iteration 89/1000 | Loss: 0.00001568
Iteration 90/1000 | Loss: 0.00001568
Iteration 91/1000 | Loss: 0.00001568
Iteration 92/1000 | Loss: 0.00001568
Iteration 93/1000 | Loss: 0.00001567
Iteration 94/1000 | Loss: 0.00001567
Iteration 95/1000 | Loss: 0.00001567
Iteration 96/1000 | Loss: 0.00001567
Iteration 97/1000 | Loss: 0.00001567
Iteration 98/1000 | Loss: 0.00001567
Iteration 99/1000 | Loss: 0.00001567
Iteration 100/1000 | Loss: 0.00001567
Iteration 101/1000 | Loss: 0.00001566
Iteration 102/1000 | Loss: 0.00001566
Iteration 103/1000 | Loss: 0.00001566
Iteration 104/1000 | Loss: 0.00001566
Iteration 105/1000 | Loss: 0.00001566
Iteration 106/1000 | Loss: 0.00001566
Iteration 107/1000 | Loss: 0.00001566
Iteration 108/1000 | Loss: 0.00001566
Iteration 109/1000 | Loss: 0.00001566
Iteration 110/1000 | Loss: 0.00001566
Iteration 111/1000 | Loss: 0.00001566
Iteration 112/1000 | Loss: 0.00001566
Iteration 113/1000 | Loss: 0.00001566
Iteration 114/1000 | Loss: 0.00001566
Iteration 115/1000 | Loss: 0.00001566
Iteration 116/1000 | Loss: 0.00001566
Iteration 117/1000 | Loss: 0.00001566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.5663434169255197e-05, 1.5663434169255197e-05, 1.5663434169255197e-05, 1.5663434169255197e-05, 1.5663434169255197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5663434169255197e-05

Optimization complete. Final v2v error: 3.373978853225708 mm

Highest mean error: 4.731093406677246 mm for frame 228

Lowest mean error: 2.9728052616119385 mm for frame 168

Saving results

Total time: 153.0909399986267
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_026/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_026/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935816
Iteration 2/25 | Loss: 0.00261317
Iteration 3/25 | Loss: 0.00212809
Iteration 4/25 | Loss: 0.00177504
Iteration 5/25 | Loss: 0.00173138
Iteration 6/25 | Loss: 0.00225212
Iteration 7/25 | Loss: 0.00191922
Iteration 8/25 | Loss: 0.00145882
Iteration 9/25 | Loss: 0.00136933
Iteration 10/25 | Loss: 0.00132318
Iteration 11/25 | Loss: 0.00130569
Iteration 12/25 | Loss: 0.00131216
Iteration 13/25 | Loss: 0.00131073
Iteration 14/25 | Loss: 0.00125474
Iteration 15/25 | Loss: 0.00123672
Iteration 16/25 | Loss: 0.00123703
Iteration 17/25 | Loss: 0.00121810
Iteration 18/25 | Loss: 0.00120762
Iteration 19/25 | Loss: 0.00120510
Iteration 20/25 | Loss: 0.00120424
Iteration 21/25 | Loss: 0.00121123
Iteration 22/25 | Loss: 0.00120721
Iteration 23/25 | Loss: 0.00120213
Iteration 24/25 | Loss: 0.00120158
Iteration 25/25 | Loss: 0.00120139

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34023213
Iteration 2/25 | Loss: 0.00082735
Iteration 3/25 | Loss: 0.00082735
Iteration 4/25 | Loss: 0.00082735
Iteration 5/25 | Loss: 0.00082735
Iteration 6/25 | Loss: 0.00082735
Iteration 7/25 | Loss: 0.00082735
Iteration 8/25 | Loss: 0.00082735
Iteration 9/25 | Loss: 0.00082735
Iteration 10/25 | Loss: 0.00082735
Iteration 11/25 | Loss: 0.00082734
Iteration 12/25 | Loss: 0.00082734
Iteration 13/25 | Loss: 0.00082734
Iteration 14/25 | Loss: 0.00082734
Iteration 15/25 | Loss: 0.00082734
Iteration 16/25 | Loss: 0.00082734
Iteration 17/25 | Loss: 0.00082734
Iteration 18/25 | Loss: 0.00082734
Iteration 19/25 | Loss: 0.00082734
Iteration 20/25 | Loss: 0.00082734
Iteration 21/25 | Loss: 0.00082734
Iteration 22/25 | Loss: 0.00082734
Iteration 23/25 | Loss: 0.00082734
Iteration 24/25 | Loss: 0.00082734
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008273445419035852, 0.0008273445419035852, 0.0008273445419035852, 0.0008273445419035852, 0.0008273445419035852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008273445419035852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082734
Iteration 2/1000 | Loss: 0.00007407
Iteration 3/1000 | Loss: 0.00005287
Iteration 4/1000 | Loss: 0.00004520
Iteration 5/1000 | Loss: 0.00004142
Iteration 6/1000 | Loss: 0.00003902
Iteration 7/1000 | Loss: 0.00003743
Iteration 8/1000 | Loss: 0.00003674
Iteration 9/1000 | Loss: 0.00003604
Iteration 10/1000 | Loss: 0.00003541
Iteration 11/1000 | Loss: 0.00003488
Iteration 12/1000 | Loss: 0.00003448
Iteration 13/1000 | Loss: 0.00141903
Iteration 14/1000 | Loss: 0.00109174
Iteration 15/1000 | Loss: 0.00009569
Iteration 16/1000 | Loss: 0.00005083
Iteration 17/1000 | Loss: 0.00003913
Iteration 18/1000 | Loss: 0.00003212
Iteration 19/1000 | Loss: 0.00002787
Iteration 20/1000 | Loss: 0.00002299
Iteration 21/1000 | Loss: 0.00001999
Iteration 22/1000 | Loss: 0.00001881
Iteration 23/1000 | Loss: 0.00001787
Iteration 24/1000 | Loss: 0.00001715
Iteration 25/1000 | Loss: 0.00001659
Iteration 26/1000 | Loss: 0.00001632
Iteration 27/1000 | Loss: 0.00001592
Iteration 28/1000 | Loss: 0.00001560
Iteration 29/1000 | Loss: 0.00001535
Iteration 30/1000 | Loss: 0.00001520
Iteration 31/1000 | Loss: 0.00001514
Iteration 32/1000 | Loss: 0.00001513
Iteration 33/1000 | Loss: 0.00001513
Iteration 34/1000 | Loss: 0.00001512
Iteration 35/1000 | Loss: 0.00001509
Iteration 36/1000 | Loss: 0.00001509
Iteration 37/1000 | Loss: 0.00001508
Iteration 38/1000 | Loss: 0.00001508
Iteration 39/1000 | Loss: 0.00001507
Iteration 40/1000 | Loss: 0.00001507
Iteration 41/1000 | Loss: 0.00001506
Iteration 42/1000 | Loss: 0.00001505
Iteration 43/1000 | Loss: 0.00001504
Iteration 44/1000 | Loss: 0.00001504
Iteration 45/1000 | Loss: 0.00001504
Iteration 46/1000 | Loss: 0.00001504
Iteration 47/1000 | Loss: 0.00001503
Iteration 48/1000 | Loss: 0.00001503
Iteration 49/1000 | Loss: 0.00001502
Iteration 50/1000 | Loss: 0.00001502
Iteration 51/1000 | Loss: 0.00001502
Iteration 52/1000 | Loss: 0.00001502
Iteration 53/1000 | Loss: 0.00001502
Iteration 54/1000 | Loss: 0.00001502
Iteration 55/1000 | Loss: 0.00001501
Iteration 56/1000 | Loss: 0.00001501
Iteration 57/1000 | Loss: 0.00001501
Iteration 58/1000 | Loss: 0.00001501
Iteration 59/1000 | Loss: 0.00001501
Iteration 60/1000 | Loss: 0.00001501
Iteration 61/1000 | Loss: 0.00001501
Iteration 62/1000 | Loss: 0.00001501
Iteration 63/1000 | Loss: 0.00001501
Iteration 64/1000 | Loss: 0.00001501
Iteration 65/1000 | Loss: 0.00001501
Iteration 66/1000 | Loss: 0.00001501
Iteration 67/1000 | Loss: 0.00001501
Iteration 68/1000 | Loss: 0.00001501
Iteration 69/1000 | Loss: 0.00001500
Iteration 70/1000 | Loss: 0.00001500
Iteration 71/1000 | Loss: 0.00001500
Iteration 72/1000 | Loss: 0.00001500
Iteration 73/1000 | Loss: 0.00001500
Iteration 74/1000 | Loss: 0.00001500
Iteration 75/1000 | Loss: 0.00001500
Iteration 76/1000 | Loss: 0.00001500
Iteration 77/1000 | Loss: 0.00001500
Iteration 78/1000 | Loss: 0.00001500
Iteration 79/1000 | Loss: 0.00001500
Iteration 80/1000 | Loss: 0.00001500
Iteration 81/1000 | Loss: 0.00001499
Iteration 82/1000 | Loss: 0.00001499
Iteration 83/1000 | Loss: 0.00001499
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001499
Iteration 86/1000 | Loss: 0.00001499
Iteration 87/1000 | Loss: 0.00001499
Iteration 88/1000 | Loss: 0.00001499
Iteration 89/1000 | Loss: 0.00001499
Iteration 90/1000 | Loss: 0.00001498
Iteration 91/1000 | Loss: 0.00001498
Iteration 92/1000 | Loss: 0.00001498
Iteration 93/1000 | Loss: 0.00001498
Iteration 94/1000 | Loss: 0.00001498
Iteration 95/1000 | Loss: 0.00001498
Iteration 96/1000 | Loss: 0.00001498
Iteration 97/1000 | Loss: 0.00001498
Iteration 98/1000 | Loss: 0.00001498
Iteration 99/1000 | Loss: 0.00001498
Iteration 100/1000 | Loss: 0.00001498
Iteration 101/1000 | Loss: 0.00001498
Iteration 102/1000 | Loss: 0.00001497
Iteration 103/1000 | Loss: 0.00001497
Iteration 104/1000 | Loss: 0.00001497
Iteration 105/1000 | Loss: 0.00001497
Iteration 106/1000 | Loss: 0.00001497
Iteration 107/1000 | Loss: 0.00001497
Iteration 108/1000 | Loss: 0.00001497
Iteration 109/1000 | Loss: 0.00001497
Iteration 110/1000 | Loss: 0.00001497
Iteration 111/1000 | Loss: 0.00001497
Iteration 112/1000 | Loss: 0.00001497
Iteration 113/1000 | Loss: 0.00001497
Iteration 114/1000 | Loss: 0.00001497
Iteration 115/1000 | Loss: 0.00001497
Iteration 116/1000 | Loss: 0.00001497
Iteration 117/1000 | Loss: 0.00001497
Iteration 118/1000 | Loss: 0.00001497
Iteration 119/1000 | Loss: 0.00001497
Iteration 120/1000 | Loss: 0.00001497
Iteration 121/1000 | Loss: 0.00001497
Iteration 122/1000 | Loss: 0.00001497
Iteration 123/1000 | Loss: 0.00001497
Iteration 124/1000 | Loss: 0.00001497
Iteration 125/1000 | Loss: 0.00001497
Iteration 126/1000 | Loss: 0.00001497
Iteration 127/1000 | Loss: 0.00001497
Iteration 128/1000 | Loss: 0.00001497
Iteration 129/1000 | Loss: 0.00001497
Iteration 130/1000 | Loss: 0.00001497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.4970353731769137e-05, 1.4970353731769137e-05, 1.4970353731769137e-05, 1.4970353731769137e-05, 1.4970353731769137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4970353731769137e-05

Optimization complete. Final v2v error: 3.3133625984191895 mm

Highest mean error: 3.6295535564422607 mm for frame 7

Lowest mean error: 3.194338798522949 mm for frame 14

Saving results

Total time: 91.34911441802979
