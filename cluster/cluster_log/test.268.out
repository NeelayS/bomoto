Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=268, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 15008-15063
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00713443
Iteration 2/25 | Loss: 0.00105010
Iteration 3/25 | Loss: 0.00092831
Iteration 4/25 | Loss: 0.00091544
Iteration 5/25 | Loss: 0.00091202
Iteration 6/25 | Loss: 0.00091120
Iteration 7/25 | Loss: 0.00091120
Iteration 8/25 | Loss: 0.00091120
Iteration 9/25 | Loss: 0.00091120
Iteration 10/25 | Loss: 0.00091120
Iteration 11/25 | Loss: 0.00091120
Iteration 12/25 | Loss: 0.00091120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009112037951126695, 0.0009112037951126695, 0.0009112037951126695, 0.0009112037951126695, 0.0009112037951126695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009112037951126695

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.17106342
Iteration 2/25 | Loss: 0.00070180
Iteration 3/25 | Loss: 0.00070180
Iteration 4/25 | Loss: 0.00070180
Iteration 5/25 | Loss: 0.00070180
Iteration 6/25 | Loss: 0.00070180
Iteration 7/25 | Loss: 0.00070180
Iteration 8/25 | Loss: 0.00070180
Iteration 9/25 | Loss: 0.00070180
Iteration 10/25 | Loss: 0.00070180
Iteration 11/25 | Loss: 0.00070179
Iteration 12/25 | Loss: 0.00070179
Iteration 13/25 | Loss: 0.00070179
Iteration 14/25 | Loss: 0.00070179
Iteration 15/25 | Loss: 0.00070179
Iteration 16/25 | Loss: 0.00070179
Iteration 17/25 | Loss: 0.00070179
Iteration 18/25 | Loss: 0.00070179
Iteration 19/25 | Loss: 0.00070179
Iteration 20/25 | Loss: 0.00070179
Iteration 21/25 | Loss: 0.00070180
Iteration 22/25 | Loss: 0.00070180
Iteration 23/25 | Loss: 0.00070179
Iteration 24/25 | Loss: 0.00070180
Iteration 25/25 | Loss: 0.00070180

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070179
Iteration 2/1000 | Loss: 0.00002822
Iteration 3/1000 | Loss: 0.00002050
Iteration 4/1000 | Loss: 0.00001787
Iteration 5/1000 | Loss: 0.00001678
Iteration 6/1000 | Loss: 0.00001622
Iteration 7/1000 | Loss: 0.00001577
Iteration 8/1000 | Loss: 0.00001541
Iteration 9/1000 | Loss: 0.00001532
Iteration 10/1000 | Loss: 0.00001531
Iteration 11/1000 | Loss: 0.00001530
Iteration 12/1000 | Loss: 0.00001530
Iteration 13/1000 | Loss: 0.00001530
Iteration 14/1000 | Loss: 0.00001523
Iteration 15/1000 | Loss: 0.00001523
Iteration 16/1000 | Loss: 0.00001523
Iteration 17/1000 | Loss: 0.00001522
Iteration 18/1000 | Loss: 0.00001522
Iteration 19/1000 | Loss: 0.00001518
Iteration 20/1000 | Loss: 0.00001518
Iteration 21/1000 | Loss: 0.00001518
Iteration 22/1000 | Loss: 0.00001517
Iteration 23/1000 | Loss: 0.00001517
Iteration 24/1000 | Loss: 0.00001517
Iteration 25/1000 | Loss: 0.00001513
Iteration 26/1000 | Loss: 0.00001513
Iteration 27/1000 | Loss: 0.00001513
Iteration 28/1000 | Loss: 0.00001512
Iteration 29/1000 | Loss: 0.00001512
Iteration 30/1000 | Loss: 0.00001512
Iteration 31/1000 | Loss: 0.00001512
Iteration 32/1000 | Loss: 0.00001512
Iteration 33/1000 | Loss: 0.00001512
Iteration 34/1000 | Loss: 0.00001512
Iteration 35/1000 | Loss: 0.00001512
Iteration 36/1000 | Loss: 0.00001512
Iteration 37/1000 | Loss: 0.00001512
Iteration 38/1000 | Loss: 0.00001511
Iteration 39/1000 | Loss: 0.00001511
Iteration 40/1000 | Loss: 0.00001510
Iteration 41/1000 | Loss: 0.00001510
Iteration 42/1000 | Loss: 0.00001509
Iteration 43/1000 | Loss: 0.00001509
Iteration 44/1000 | Loss: 0.00001509
Iteration 45/1000 | Loss: 0.00001509
Iteration 46/1000 | Loss: 0.00001509
Iteration 47/1000 | Loss: 0.00001509
Iteration 48/1000 | Loss: 0.00001508
Iteration 49/1000 | Loss: 0.00001508
Iteration 50/1000 | Loss: 0.00001508
Iteration 51/1000 | Loss: 0.00001508
Iteration 52/1000 | Loss: 0.00001507
Iteration 53/1000 | Loss: 0.00001507
Iteration 54/1000 | Loss: 0.00001507
Iteration 55/1000 | Loss: 0.00001507
Iteration 56/1000 | Loss: 0.00001507
Iteration 57/1000 | Loss: 0.00001507
Iteration 58/1000 | Loss: 0.00001507
Iteration 59/1000 | Loss: 0.00001507
Iteration 60/1000 | Loss: 0.00001507
Iteration 61/1000 | Loss: 0.00001507
Iteration 62/1000 | Loss: 0.00001507
Iteration 63/1000 | Loss: 0.00001507
Iteration 64/1000 | Loss: 0.00001507
Iteration 65/1000 | Loss: 0.00001507
Iteration 66/1000 | Loss: 0.00001507
Iteration 67/1000 | Loss: 0.00001507
Iteration 68/1000 | Loss: 0.00001507
Iteration 69/1000 | Loss: 0.00001506
Iteration 70/1000 | Loss: 0.00001506
Iteration 71/1000 | Loss: 0.00001506
Iteration 72/1000 | Loss: 0.00001506
Iteration 73/1000 | Loss: 0.00001506
Iteration 74/1000 | Loss: 0.00001505
Iteration 75/1000 | Loss: 0.00001505
Iteration 76/1000 | Loss: 0.00001505
Iteration 77/1000 | Loss: 0.00001505
Iteration 78/1000 | Loss: 0.00001505
Iteration 79/1000 | Loss: 0.00001505
Iteration 80/1000 | Loss: 0.00001505
Iteration 81/1000 | Loss: 0.00001505
Iteration 82/1000 | Loss: 0.00001505
Iteration 83/1000 | Loss: 0.00001505
Iteration 84/1000 | Loss: 0.00001505
Iteration 85/1000 | Loss: 0.00001504
Iteration 86/1000 | Loss: 0.00001504
Iteration 87/1000 | Loss: 0.00001504
Iteration 88/1000 | Loss: 0.00001504
Iteration 89/1000 | Loss: 0.00001504
Iteration 90/1000 | Loss: 0.00001504
Iteration 91/1000 | Loss: 0.00001504
Iteration 92/1000 | Loss: 0.00001504
Iteration 93/1000 | Loss: 0.00001503
Iteration 94/1000 | Loss: 0.00001503
Iteration 95/1000 | Loss: 0.00001503
Iteration 96/1000 | Loss: 0.00001503
Iteration 97/1000 | Loss: 0.00001503
Iteration 98/1000 | Loss: 0.00001503
Iteration 99/1000 | Loss: 0.00001503
Iteration 100/1000 | Loss: 0.00001503
Iteration 101/1000 | Loss: 0.00001503
Iteration 102/1000 | Loss: 0.00001503
Iteration 103/1000 | Loss: 0.00001503
Iteration 104/1000 | Loss: 0.00001503
Iteration 105/1000 | Loss: 0.00001503
Iteration 106/1000 | Loss: 0.00001503
Iteration 107/1000 | Loss: 0.00001503
Iteration 108/1000 | Loss: 0.00001503
Iteration 109/1000 | Loss: 0.00001503
Iteration 110/1000 | Loss: 0.00001502
Iteration 111/1000 | Loss: 0.00001502
Iteration 112/1000 | Loss: 0.00001502
Iteration 113/1000 | Loss: 0.00001502
Iteration 114/1000 | Loss: 0.00001502
Iteration 115/1000 | Loss: 0.00001502
Iteration 116/1000 | Loss: 0.00001502
Iteration 117/1000 | Loss: 0.00001502
Iteration 118/1000 | Loss: 0.00001502
Iteration 119/1000 | Loss: 0.00001502
Iteration 120/1000 | Loss: 0.00001502
Iteration 121/1000 | Loss: 0.00001502
Iteration 122/1000 | Loss: 0.00001502
Iteration 123/1000 | Loss: 0.00001502
Iteration 124/1000 | Loss: 0.00001502
Iteration 125/1000 | Loss: 0.00001502
Iteration 126/1000 | Loss: 0.00001502
Iteration 127/1000 | Loss: 0.00001502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.5024872482172213e-05, 1.5024872482172213e-05, 1.5024872482172213e-05, 1.5024872482172213e-05, 1.5024872482172213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5024872482172213e-05

Optimization complete. Final v2v error: 3.3471038341522217 mm

Highest mean error: 3.5893144607543945 mm for frame 38

Lowest mean error: 3.033524513244629 mm for frame 111

Saving results

Total time: 30.982932806015015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00896619
Iteration 2/25 | Loss: 0.00117629
Iteration 3/25 | Loss: 0.00101870
Iteration 4/25 | Loss: 0.00099561
Iteration 5/25 | Loss: 0.00098871
Iteration 6/25 | Loss: 0.00098763
Iteration 7/25 | Loss: 0.00098763
Iteration 8/25 | Loss: 0.00098763
Iteration 9/25 | Loss: 0.00098763
Iteration 10/25 | Loss: 0.00098763
Iteration 11/25 | Loss: 0.00098763
Iteration 12/25 | Loss: 0.00098763
Iteration 13/25 | Loss: 0.00098763
Iteration 14/25 | Loss: 0.00098763
Iteration 15/25 | Loss: 0.00098763
Iteration 16/25 | Loss: 0.00098763
Iteration 17/25 | Loss: 0.00098763
Iteration 18/25 | Loss: 0.00098763
Iteration 19/25 | Loss: 0.00098763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000987631967291236, 0.000987631967291236, 0.000987631967291236, 0.000987631967291236, 0.000987631967291236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000987631967291236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25087452
Iteration 2/25 | Loss: 0.00076902
Iteration 3/25 | Loss: 0.00076901
Iteration 4/25 | Loss: 0.00076901
Iteration 5/25 | Loss: 0.00076901
Iteration 6/25 | Loss: 0.00076901
Iteration 7/25 | Loss: 0.00076901
Iteration 8/25 | Loss: 0.00076901
Iteration 9/25 | Loss: 0.00076901
Iteration 10/25 | Loss: 0.00076901
Iteration 11/25 | Loss: 0.00076901
Iteration 12/25 | Loss: 0.00076901
Iteration 13/25 | Loss: 0.00076901
Iteration 14/25 | Loss: 0.00076901
Iteration 15/25 | Loss: 0.00076901
Iteration 16/25 | Loss: 0.00076901
Iteration 17/25 | Loss: 0.00076901
Iteration 18/25 | Loss: 0.00076901
Iteration 19/25 | Loss: 0.00076901
Iteration 20/25 | Loss: 0.00076901
Iteration 21/25 | Loss: 0.00076901
Iteration 22/25 | Loss: 0.00076901
Iteration 23/25 | Loss: 0.00076901
Iteration 24/25 | Loss: 0.00076901
Iteration 25/25 | Loss: 0.00076901

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076901
Iteration 2/1000 | Loss: 0.00004620
Iteration 3/1000 | Loss: 0.00003158
Iteration 4/1000 | Loss: 0.00002714
Iteration 5/1000 | Loss: 0.00002538
Iteration 6/1000 | Loss: 0.00002413
Iteration 7/1000 | Loss: 0.00002344
Iteration 8/1000 | Loss: 0.00002298
Iteration 9/1000 | Loss: 0.00002256
Iteration 10/1000 | Loss: 0.00002225
Iteration 11/1000 | Loss: 0.00002211
Iteration 12/1000 | Loss: 0.00002198
Iteration 13/1000 | Loss: 0.00002184
Iteration 14/1000 | Loss: 0.00002176
Iteration 15/1000 | Loss: 0.00002176
Iteration 16/1000 | Loss: 0.00002175
Iteration 17/1000 | Loss: 0.00002175
Iteration 18/1000 | Loss: 0.00002171
Iteration 19/1000 | Loss: 0.00002170
Iteration 20/1000 | Loss: 0.00002169
Iteration 21/1000 | Loss: 0.00002168
Iteration 22/1000 | Loss: 0.00002168
Iteration 23/1000 | Loss: 0.00002168
Iteration 24/1000 | Loss: 0.00002167
Iteration 25/1000 | Loss: 0.00002167
Iteration 26/1000 | Loss: 0.00002166
Iteration 27/1000 | Loss: 0.00002166
Iteration 28/1000 | Loss: 0.00002165
Iteration 29/1000 | Loss: 0.00002165
Iteration 30/1000 | Loss: 0.00002164
Iteration 31/1000 | Loss: 0.00002164
Iteration 32/1000 | Loss: 0.00002164
Iteration 33/1000 | Loss: 0.00002163
Iteration 34/1000 | Loss: 0.00002163
Iteration 35/1000 | Loss: 0.00002163
Iteration 36/1000 | Loss: 0.00002162
Iteration 37/1000 | Loss: 0.00002162
Iteration 38/1000 | Loss: 0.00002162
Iteration 39/1000 | Loss: 0.00002161
Iteration 40/1000 | Loss: 0.00002161
Iteration 41/1000 | Loss: 0.00002161
Iteration 42/1000 | Loss: 0.00002160
Iteration 43/1000 | Loss: 0.00002160
Iteration 44/1000 | Loss: 0.00002160
Iteration 45/1000 | Loss: 0.00002159
Iteration 46/1000 | Loss: 0.00002159
Iteration 47/1000 | Loss: 0.00002159
Iteration 48/1000 | Loss: 0.00002159
Iteration 49/1000 | Loss: 0.00002158
Iteration 50/1000 | Loss: 0.00002158
Iteration 51/1000 | Loss: 0.00002158
Iteration 52/1000 | Loss: 0.00002158
Iteration 53/1000 | Loss: 0.00002158
Iteration 54/1000 | Loss: 0.00002158
Iteration 55/1000 | Loss: 0.00002158
Iteration 56/1000 | Loss: 0.00002158
Iteration 57/1000 | Loss: 0.00002158
Iteration 58/1000 | Loss: 0.00002158
Iteration 59/1000 | Loss: 0.00002158
Iteration 60/1000 | Loss: 0.00002157
Iteration 61/1000 | Loss: 0.00002157
Iteration 62/1000 | Loss: 0.00002157
Iteration 63/1000 | Loss: 0.00002157
Iteration 64/1000 | Loss: 0.00002157
Iteration 65/1000 | Loss: 0.00002156
Iteration 66/1000 | Loss: 0.00002156
Iteration 67/1000 | Loss: 0.00002156
Iteration 68/1000 | Loss: 0.00002156
Iteration 69/1000 | Loss: 0.00002156
Iteration 70/1000 | Loss: 0.00002156
Iteration 71/1000 | Loss: 0.00002156
Iteration 72/1000 | Loss: 0.00002155
Iteration 73/1000 | Loss: 0.00002155
Iteration 74/1000 | Loss: 0.00002155
Iteration 75/1000 | Loss: 0.00002155
Iteration 76/1000 | Loss: 0.00002155
Iteration 77/1000 | Loss: 0.00002155
Iteration 78/1000 | Loss: 0.00002155
Iteration 79/1000 | Loss: 0.00002155
Iteration 80/1000 | Loss: 0.00002155
Iteration 81/1000 | Loss: 0.00002154
Iteration 82/1000 | Loss: 0.00002154
Iteration 83/1000 | Loss: 0.00002154
Iteration 84/1000 | Loss: 0.00002154
Iteration 85/1000 | Loss: 0.00002154
Iteration 86/1000 | Loss: 0.00002154
Iteration 87/1000 | Loss: 0.00002154
Iteration 88/1000 | Loss: 0.00002153
Iteration 89/1000 | Loss: 0.00002153
Iteration 90/1000 | Loss: 0.00002153
Iteration 91/1000 | Loss: 0.00002153
Iteration 92/1000 | Loss: 0.00002153
Iteration 93/1000 | Loss: 0.00002153
Iteration 94/1000 | Loss: 0.00002153
Iteration 95/1000 | Loss: 0.00002153
Iteration 96/1000 | Loss: 0.00002153
Iteration 97/1000 | Loss: 0.00002153
Iteration 98/1000 | Loss: 0.00002153
Iteration 99/1000 | Loss: 0.00002153
Iteration 100/1000 | Loss: 0.00002153
Iteration 101/1000 | Loss: 0.00002153
Iteration 102/1000 | Loss: 0.00002153
Iteration 103/1000 | Loss: 0.00002152
Iteration 104/1000 | Loss: 0.00002152
Iteration 105/1000 | Loss: 0.00002152
Iteration 106/1000 | Loss: 0.00002152
Iteration 107/1000 | Loss: 0.00002152
Iteration 108/1000 | Loss: 0.00002152
Iteration 109/1000 | Loss: 0.00002152
Iteration 110/1000 | Loss: 0.00002152
Iteration 111/1000 | Loss: 0.00002152
Iteration 112/1000 | Loss: 0.00002152
Iteration 113/1000 | Loss: 0.00002151
Iteration 114/1000 | Loss: 0.00002151
Iteration 115/1000 | Loss: 0.00002151
Iteration 116/1000 | Loss: 0.00002151
Iteration 117/1000 | Loss: 0.00002151
Iteration 118/1000 | Loss: 0.00002151
Iteration 119/1000 | Loss: 0.00002151
Iteration 120/1000 | Loss: 0.00002150
Iteration 121/1000 | Loss: 0.00002150
Iteration 122/1000 | Loss: 0.00002150
Iteration 123/1000 | Loss: 0.00002150
Iteration 124/1000 | Loss: 0.00002150
Iteration 125/1000 | Loss: 0.00002150
Iteration 126/1000 | Loss: 0.00002150
Iteration 127/1000 | Loss: 0.00002149
Iteration 128/1000 | Loss: 0.00002149
Iteration 129/1000 | Loss: 0.00002149
Iteration 130/1000 | Loss: 0.00002149
Iteration 131/1000 | Loss: 0.00002149
Iteration 132/1000 | Loss: 0.00002149
Iteration 133/1000 | Loss: 0.00002149
Iteration 134/1000 | Loss: 0.00002149
Iteration 135/1000 | Loss: 0.00002149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.149447209376376e-05, 2.149447209376376e-05, 2.149447209376376e-05, 2.149447209376376e-05, 2.149447209376376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.149447209376376e-05

Optimization complete. Final v2v error: 3.9353458881378174 mm

Highest mean error: 4.796059608459473 mm for frame 209

Lowest mean error: 2.9724185466766357 mm for frame 11

Saving results

Total time: 40.89739751815796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057027
Iteration 2/25 | Loss: 0.00405107
Iteration 3/25 | Loss: 0.00242653
Iteration 4/25 | Loss: 0.00191603
Iteration 5/25 | Loss: 0.00178551
Iteration 6/25 | Loss: 0.00170992
Iteration 7/25 | Loss: 0.00159557
Iteration 8/25 | Loss: 0.00137410
Iteration 9/25 | Loss: 0.00130060
Iteration 10/25 | Loss: 0.00125086
Iteration 11/25 | Loss: 0.00124115
Iteration 12/25 | Loss: 0.00122717
Iteration 13/25 | Loss: 0.00122576
Iteration 14/25 | Loss: 0.00122074
Iteration 15/25 | Loss: 0.00120792
Iteration 16/25 | Loss: 0.00120259
Iteration 17/25 | Loss: 0.00119525
Iteration 18/25 | Loss: 0.00117755
Iteration 19/25 | Loss: 0.00116790
Iteration 20/25 | Loss: 0.00117100
Iteration 21/25 | Loss: 0.00117506
Iteration 22/25 | Loss: 0.00117203
Iteration 23/25 | Loss: 0.00116366
Iteration 24/25 | Loss: 0.00116214
Iteration 25/25 | Loss: 0.00115701

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30950654
Iteration 2/25 | Loss: 0.00383346
Iteration 3/25 | Loss: 0.00351345
Iteration 4/25 | Loss: 0.00351344
Iteration 5/25 | Loss: 0.00351344
Iteration 6/25 | Loss: 0.00351344
Iteration 7/25 | Loss: 0.00351344
Iteration 8/25 | Loss: 0.00351344
Iteration 9/25 | Loss: 0.00351344
Iteration 10/25 | Loss: 0.00351344
Iteration 11/25 | Loss: 0.00351344
Iteration 12/25 | Loss: 0.00351344
Iteration 13/25 | Loss: 0.00351344
Iteration 14/25 | Loss: 0.00351344
Iteration 15/25 | Loss: 0.00351344
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0035134416539222, 0.0035134416539222, 0.0035134416539222, 0.0035134416539222, 0.0035134416539222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035134416539222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00351344
Iteration 2/1000 | Loss: 0.00107911
Iteration 3/1000 | Loss: 0.00170703
Iteration 4/1000 | Loss: 0.00087610
Iteration 5/1000 | Loss: 0.00110253
Iteration 6/1000 | Loss: 0.00088061
Iteration 7/1000 | Loss: 0.00068532
Iteration 8/1000 | Loss: 0.00103613
Iteration 9/1000 | Loss: 0.00073095
Iteration 10/1000 | Loss: 0.00069111
Iteration 11/1000 | Loss: 0.00053936
Iteration 12/1000 | Loss: 0.00042198
Iteration 13/1000 | Loss: 0.00090982
Iteration 14/1000 | Loss: 0.00027992
Iteration 15/1000 | Loss: 0.00088115
Iteration 16/1000 | Loss: 0.00290459
Iteration 17/1000 | Loss: 0.00125086
Iteration 18/1000 | Loss: 0.00041628
Iteration 19/1000 | Loss: 0.00057634
Iteration 20/1000 | Loss: 0.00091576
Iteration 21/1000 | Loss: 0.00027622
Iteration 22/1000 | Loss: 0.00020875
Iteration 23/1000 | Loss: 0.00265867
Iteration 24/1000 | Loss: 0.00053756
Iteration 25/1000 | Loss: 0.00297879
Iteration 26/1000 | Loss: 0.00359913
Iteration 27/1000 | Loss: 0.00508208
Iteration 28/1000 | Loss: 0.00281731
Iteration 29/1000 | Loss: 0.00242408
Iteration 30/1000 | Loss: 0.00197784
Iteration 31/1000 | Loss: 0.00111507
Iteration 32/1000 | Loss: 0.00219645
Iteration 33/1000 | Loss: 0.00262268
Iteration 34/1000 | Loss: 0.00276240
Iteration 35/1000 | Loss: 0.00364315
Iteration 36/1000 | Loss: 0.00435741
Iteration 37/1000 | Loss: 0.00091798
Iteration 38/1000 | Loss: 0.00034426
Iteration 39/1000 | Loss: 0.00033728
Iteration 40/1000 | Loss: 0.00058332
Iteration 41/1000 | Loss: 0.00046936
Iteration 42/1000 | Loss: 0.00038351
Iteration 43/1000 | Loss: 0.00037277
Iteration 44/1000 | Loss: 0.00022901
Iteration 45/1000 | Loss: 0.00014411
Iteration 46/1000 | Loss: 0.00110314
Iteration 47/1000 | Loss: 0.00020860
Iteration 48/1000 | Loss: 0.00029213
Iteration 49/1000 | Loss: 0.00071490
Iteration 50/1000 | Loss: 0.00016959
Iteration 51/1000 | Loss: 0.00012427
Iteration 52/1000 | Loss: 0.00065477
Iteration 53/1000 | Loss: 0.00117117
Iteration 54/1000 | Loss: 0.00048661
Iteration 55/1000 | Loss: 0.00029585
Iteration 56/1000 | Loss: 0.00030111
Iteration 57/1000 | Loss: 0.00033797
Iteration 58/1000 | Loss: 0.00025940
Iteration 59/1000 | Loss: 0.00094773
Iteration 60/1000 | Loss: 0.00072578
Iteration 61/1000 | Loss: 0.00069833
Iteration 62/1000 | Loss: 0.00071109
Iteration 63/1000 | Loss: 0.00027825
Iteration 64/1000 | Loss: 0.00063486
Iteration 65/1000 | Loss: 0.00018681
Iteration 66/1000 | Loss: 0.00019917
Iteration 67/1000 | Loss: 0.00017672
Iteration 68/1000 | Loss: 0.00074723
Iteration 69/1000 | Loss: 0.00046739
Iteration 70/1000 | Loss: 0.00022553
Iteration 71/1000 | Loss: 0.00008742
Iteration 72/1000 | Loss: 0.00013300
Iteration 73/1000 | Loss: 0.00037462
Iteration 74/1000 | Loss: 0.00024170
Iteration 75/1000 | Loss: 0.00009919
Iteration 76/1000 | Loss: 0.00028359
Iteration 77/1000 | Loss: 0.00043389
Iteration 78/1000 | Loss: 0.00024408
Iteration 79/1000 | Loss: 0.00008153
Iteration 80/1000 | Loss: 0.00007193
Iteration 81/1000 | Loss: 0.00007165
Iteration 82/1000 | Loss: 0.00006501
Iteration 83/1000 | Loss: 0.00007256
Iteration 84/1000 | Loss: 0.00006259
Iteration 85/1000 | Loss: 0.00006077
Iteration 86/1000 | Loss: 0.00005991
Iteration 87/1000 | Loss: 0.00006051
Iteration 88/1000 | Loss: 0.00010024
Iteration 89/1000 | Loss: 0.00019937
Iteration 90/1000 | Loss: 0.00027627
Iteration 91/1000 | Loss: 0.00026760
Iteration 92/1000 | Loss: 0.00006629
Iteration 93/1000 | Loss: 0.00008480
Iteration 94/1000 | Loss: 0.00006181
Iteration 95/1000 | Loss: 0.00006255
Iteration 96/1000 | Loss: 0.00006145
Iteration 97/1000 | Loss: 0.00008027
Iteration 98/1000 | Loss: 0.00008029
Iteration 99/1000 | Loss: 0.00006040
Iteration 100/1000 | Loss: 0.00055178
Iteration 101/1000 | Loss: 0.00031691
Iteration 102/1000 | Loss: 0.00035800
Iteration 103/1000 | Loss: 0.00007551
Iteration 104/1000 | Loss: 0.00014426
Iteration 105/1000 | Loss: 0.00014015
Iteration 106/1000 | Loss: 0.00043461
Iteration 107/1000 | Loss: 0.00007613
Iteration 108/1000 | Loss: 0.00032883
Iteration 109/1000 | Loss: 0.00010367
Iteration 110/1000 | Loss: 0.00010247
Iteration 111/1000 | Loss: 0.00018423
Iteration 112/1000 | Loss: 0.00010104
Iteration 113/1000 | Loss: 0.00011867
Iteration 114/1000 | Loss: 0.00009822
Iteration 115/1000 | Loss: 0.00006215
Iteration 116/1000 | Loss: 0.00006118
Iteration 117/1000 | Loss: 0.00009552
Iteration 118/1000 | Loss: 0.00006301
Iteration 119/1000 | Loss: 0.00006252
Iteration 120/1000 | Loss: 0.00011566
Iteration 121/1000 | Loss: 0.00010374
Iteration 122/1000 | Loss: 0.00010063
Iteration 123/1000 | Loss: 0.00007642
Iteration 124/1000 | Loss: 0.00010452
Iteration 125/1000 | Loss: 0.00008626
Iteration 126/1000 | Loss: 0.00006476
Iteration 127/1000 | Loss: 0.00006309
Iteration 128/1000 | Loss: 0.00009308
Iteration 129/1000 | Loss: 0.00005838
Iteration 130/1000 | Loss: 0.00005776
Iteration 131/1000 | Loss: 0.00007143
Iteration 132/1000 | Loss: 0.00005742
Iteration 133/1000 | Loss: 0.00020691
Iteration 134/1000 | Loss: 0.00044734
Iteration 135/1000 | Loss: 0.00026359
Iteration 136/1000 | Loss: 0.00006053
Iteration 137/1000 | Loss: 0.00005747
Iteration 138/1000 | Loss: 0.00005721
Iteration 139/1000 | Loss: 0.00005720
Iteration 140/1000 | Loss: 0.00005719
Iteration 141/1000 | Loss: 0.00005718
Iteration 142/1000 | Loss: 0.00005718
Iteration 143/1000 | Loss: 0.00005718
Iteration 144/1000 | Loss: 0.00005718
Iteration 145/1000 | Loss: 0.00005718
Iteration 146/1000 | Loss: 0.00005718
Iteration 147/1000 | Loss: 0.00005718
Iteration 148/1000 | Loss: 0.00005718
Iteration 149/1000 | Loss: 0.00005718
Iteration 150/1000 | Loss: 0.00005718
Iteration 151/1000 | Loss: 0.00005718
Iteration 152/1000 | Loss: 0.00005718
Iteration 153/1000 | Loss: 0.00005717
Iteration 154/1000 | Loss: 0.00005717
Iteration 155/1000 | Loss: 0.00005717
Iteration 156/1000 | Loss: 0.00005717
Iteration 157/1000 | Loss: 0.00005717
Iteration 158/1000 | Loss: 0.00005716
Iteration 159/1000 | Loss: 0.00005716
Iteration 160/1000 | Loss: 0.00005783
Iteration 161/1000 | Loss: 0.00005783
Iteration 162/1000 | Loss: 0.00005782
Iteration 163/1000 | Loss: 0.00005782
Iteration 164/1000 | Loss: 0.00005833
Iteration 165/1000 | Loss: 0.00005757
Iteration 166/1000 | Loss: 0.00005711
Iteration 167/1000 | Loss: 0.00005711
Iteration 168/1000 | Loss: 0.00005711
Iteration 169/1000 | Loss: 0.00005711
Iteration 170/1000 | Loss: 0.00005711
Iteration 171/1000 | Loss: 0.00005711
Iteration 172/1000 | Loss: 0.00005711
Iteration 173/1000 | Loss: 0.00005711
Iteration 174/1000 | Loss: 0.00005711
Iteration 175/1000 | Loss: 0.00005711
Iteration 176/1000 | Loss: 0.00005711
Iteration 177/1000 | Loss: 0.00005711
Iteration 178/1000 | Loss: 0.00005711
Iteration 179/1000 | Loss: 0.00005711
Iteration 180/1000 | Loss: 0.00005711
Iteration 181/1000 | Loss: 0.00005710
Iteration 182/1000 | Loss: 0.00005710
Iteration 183/1000 | Loss: 0.00005710
Iteration 184/1000 | Loss: 0.00005710
Iteration 185/1000 | Loss: 0.00005710
Iteration 186/1000 | Loss: 0.00005710
Iteration 187/1000 | Loss: 0.00005710
Iteration 188/1000 | Loss: 0.00005710
Iteration 189/1000 | Loss: 0.00005710
Iteration 190/1000 | Loss: 0.00005710
Iteration 191/1000 | Loss: 0.00005710
Iteration 192/1000 | Loss: 0.00005710
Iteration 193/1000 | Loss: 0.00005710
Iteration 194/1000 | Loss: 0.00005710
Iteration 195/1000 | Loss: 0.00005710
Iteration 196/1000 | Loss: 0.00005710
Iteration 197/1000 | Loss: 0.00005710
Iteration 198/1000 | Loss: 0.00005710
Iteration 199/1000 | Loss: 0.00005710
Iteration 200/1000 | Loss: 0.00005710
Iteration 201/1000 | Loss: 0.00005710
Iteration 202/1000 | Loss: 0.00005710
Iteration 203/1000 | Loss: 0.00005710
Iteration 204/1000 | Loss: 0.00005710
Iteration 205/1000 | Loss: 0.00005710
Iteration 206/1000 | Loss: 0.00005710
Iteration 207/1000 | Loss: 0.00005710
Iteration 208/1000 | Loss: 0.00005710
Iteration 209/1000 | Loss: 0.00005710
Iteration 210/1000 | Loss: 0.00005710
Iteration 211/1000 | Loss: 0.00005710
Iteration 212/1000 | Loss: 0.00005710
Iteration 213/1000 | Loss: 0.00005710
Iteration 214/1000 | Loss: 0.00005710
Iteration 215/1000 | Loss: 0.00005710
Iteration 216/1000 | Loss: 0.00005710
Iteration 217/1000 | Loss: 0.00005710
Iteration 218/1000 | Loss: 0.00005710
Iteration 219/1000 | Loss: 0.00005710
Iteration 220/1000 | Loss: 0.00005710
Iteration 221/1000 | Loss: 0.00005710
Iteration 222/1000 | Loss: 0.00005710
Iteration 223/1000 | Loss: 0.00005710
Iteration 224/1000 | Loss: 0.00005710
Iteration 225/1000 | Loss: 0.00005710
Iteration 226/1000 | Loss: 0.00005710
Iteration 227/1000 | Loss: 0.00005710
Iteration 228/1000 | Loss: 0.00005710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [5.710057666874491e-05, 5.710057666874491e-05, 5.710057666874491e-05, 5.710057666874491e-05, 5.710057666874491e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.710057666874491e-05

Optimization complete. Final v2v error: 4.167325019836426 mm

Highest mean error: 13.191360473632812 mm for frame 168

Lowest mean error: 2.87919545173645 mm for frame 16

Saving results

Total time: 285.2790744304657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00932218
Iteration 2/25 | Loss: 0.00106921
Iteration 3/25 | Loss: 0.00094407
Iteration 4/25 | Loss: 0.00093015
Iteration 5/25 | Loss: 0.00092464
Iteration 6/25 | Loss: 0.00092312
Iteration 7/25 | Loss: 0.00092297
Iteration 8/25 | Loss: 0.00092297
Iteration 9/25 | Loss: 0.00092297
Iteration 10/25 | Loss: 0.00092297
Iteration 11/25 | Loss: 0.00092297
Iteration 12/25 | Loss: 0.00092297
Iteration 13/25 | Loss: 0.00092297
Iteration 14/25 | Loss: 0.00092297
Iteration 15/25 | Loss: 0.00092297
Iteration 16/25 | Loss: 0.00092297
Iteration 17/25 | Loss: 0.00092297
Iteration 18/25 | Loss: 0.00092297
Iteration 19/25 | Loss: 0.00092297
Iteration 20/25 | Loss: 0.00092297
Iteration 21/25 | Loss: 0.00092297
Iteration 22/25 | Loss: 0.00092297
Iteration 23/25 | Loss: 0.00092297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009229728020727634, 0.0009229728020727634, 0.0009229728020727634, 0.0009229728020727634, 0.0009229728020727634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009229728020727634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66568422
Iteration 2/25 | Loss: 0.00074521
Iteration 3/25 | Loss: 0.00074521
Iteration 4/25 | Loss: 0.00074520
Iteration 5/25 | Loss: 0.00074520
Iteration 6/25 | Loss: 0.00074520
Iteration 7/25 | Loss: 0.00074520
Iteration 8/25 | Loss: 0.00074520
Iteration 9/25 | Loss: 0.00074520
Iteration 10/25 | Loss: 0.00074520
Iteration 11/25 | Loss: 0.00074520
Iteration 12/25 | Loss: 0.00074520
Iteration 13/25 | Loss: 0.00074520
Iteration 14/25 | Loss: 0.00074520
Iteration 15/25 | Loss: 0.00074520
Iteration 16/25 | Loss: 0.00074520
Iteration 17/25 | Loss: 0.00074520
Iteration 18/25 | Loss: 0.00074520
Iteration 19/25 | Loss: 0.00074520
Iteration 20/25 | Loss: 0.00074520
Iteration 21/25 | Loss: 0.00074520
Iteration 22/25 | Loss: 0.00074520
Iteration 23/25 | Loss: 0.00074520
Iteration 24/25 | Loss: 0.00074520
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000745203229598701, 0.000745203229598701, 0.000745203229598701, 0.000745203229598701, 0.000745203229598701]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000745203229598701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074520
Iteration 2/1000 | Loss: 0.00003637
Iteration 3/1000 | Loss: 0.00002688
Iteration 4/1000 | Loss: 0.00002300
Iteration 5/1000 | Loss: 0.00002094
Iteration 6/1000 | Loss: 0.00001999
Iteration 7/1000 | Loss: 0.00001959
Iteration 8/1000 | Loss: 0.00001913
Iteration 9/1000 | Loss: 0.00001895
Iteration 10/1000 | Loss: 0.00001881
Iteration 11/1000 | Loss: 0.00001880
Iteration 12/1000 | Loss: 0.00001877
Iteration 13/1000 | Loss: 0.00001876
Iteration 14/1000 | Loss: 0.00001876
Iteration 15/1000 | Loss: 0.00001875
Iteration 16/1000 | Loss: 0.00001875
Iteration 17/1000 | Loss: 0.00001875
Iteration 18/1000 | Loss: 0.00001874
Iteration 19/1000 | Loss: 0.00001874
Iteration 20/1000 | Loss: 0.00001874
Iteration 21/1000 | Loss: 0.00001871
Iteration 22/1000 | Loss: 0.00001871
Iteration 23/1000 | Loss: 0.00001870
Iteration 24/1000 | Loss: 0.00001870
Iteration 25/1000 | Loss: 0.00001869
Iteration 26/1000 | Loss: 0.00001869
Iteration 27/1000 | Loss: 0.00001868
Iteration 28/1000 | Loss: 0.00001864
Iteration 29/1000 | Loss: 0.00001864
Iteration 30/1000 | Loss: 0.00001864
Iteration 31/1000 | Loss: 0.00001864
Iteration 32/1000 | Loss: 0.00001864
Iteration 33/1000 | Loss: 0.00001864
Iteration 34/1000 | Loss: 0.00001863
Iteration 35/1000 | Loss: 0.00001863
Iteration 36/1000 | Loss: 0.00001862
Iteration 37/1000 | Loss: 0.00001862
Iteration 38/1000 | Loss: 0.00001861
Iteration 39/1000 | Loss: 0.00001860
Iteration 40/1000 | Loss: 0.00001859
Iteration 41/1000 | Loss: 0.00001859
Iteration 42/1000 | Loss: 0.00001859
Iteration 43/1000 | Loss: 0.00001859
Iteration 44/1000 | Loss: 0.00001859
Iteration 45/1000 | Loss: 0.00001859
Iteration 46/1000 | Loss: 0.00001859
Iteration 47/1000 | Loss: 0.00001859
Iteration 48/1000 | Loss: 0.00001859
Iteration 49/1000 | Loss: 0.00001858
Iteration 50/1000 | Loss: 0.00001858
Iteration 51/1000 | Loss: 0.00001858
Iteration 52/1000 | Loss: 0.00001857
Iteration 53/1000 | Loss: 0.00001857
Iteration 54/1000 | Loss: 0.00001857
Iteration 55/1000 | Loss: 0.00001857
Iteration 56/1000 | Loss: 0.00001857
Iteration 57/1000 | Loss: 0.00001856
Iteration 58/1000 | Loss: 0.00001856
Iteration 59/1000 | Loss: 0.00001856
Iteration 60/1000 | Loss: 0.00001856
Iteration 61/1000 | Loss: 0.00001856
Iteration 62/1000 | Loss: 0.00001855
Iteration 63/1000 | Loss: 0.00001855
Iteration 64/1000 | Loss: 0.00001855
Iteration 65/1000 | Loss: 0.00001855
Iteration 66/1000 | Loss: 0.00001855
Iteration 67/1000 | Loss: 0.00001855
Iteration 68/1000 | Loss: 0.00001855
Iteration 69/1000 | Loss: 0.00001855
Iteration 70/1000 | Loss: 0.00001855
Iteration 71/1000 | Loss: 0.00001855
Iteration 72/1000 | Loss: 0.00001855
Iteration 73/1000 | Loss: 0.00001854
Iteration 74/1000 | Loss: 0.00001854
Iteration 75/1000 | Loss: 0.00001854
Iteration 76/1000 | Loss: 0.00001854
Iteration 77/1000 | Loss: 0.00001854
Iteration 78/1000 | Loss: 0.00001854
Iteration 79/1000 | Loss: 0.00001854
Iteration 80/1000 | Loss: 0.00001854
Iteration 81/1000 | Loss: 0.00001854
Iteration 82/1000 | Loss: 0.00001854
Iteration 83/1000 | Loss: 0.00001854
Iteration 84/1000 | Loss: 0.00001854
Iteration 85/1000 | Loss: 0.00001854
Iteration 86/1000 | Loss: 0.00001853
Iteration 87/1000 | Loss: 0.00001853
Iteration 88/1000 | Loss: 0.00001853
Iteration 89/1000 | Loss: 0.00001853
Iteration 90/1000 | Loss: 0.00001853
Iteration 91/1000 | Loss: 0.00001853
Iteration 92/1000 | Loss: 0.00001853
Iteration 93/1000 | Loss: 0.00001853
Iteration 94/1000 | Loss: 0.00001853
Iteration 95/1000 | Loss: 0.00001853
Iteration 96/1000 | Loss: 0.00001853
Iteration 97/1000 | Loss: 0.00001853
Iteration 98/1000 | Loss: 0.00001853
Iteration 99/1000 | Loss: 0.00001853
Iteration 100/1000 | Loss: 0.00001853
Iteration 101/1000 | Loss: 0.00001853
Iteration 102/1000 | Loss: 0.00001853
Iteration 103/1000 | Loss: 0.00001853
Iteration 104/1000 | Loss: 0.00001853
Iteration 105/1000 | Loss: 0.00001853
Iteration 106/1000 | Loss: 0.00001853
Iteration 107/1000 | Loss: 0.00001853
Iteration 108/1000 | Loss: 0.00001853
Iteration 109/1000 | Loss: 0.00001853
Iteration 110/1000 | Loss: 0.00001853
Iteration 111/1000 | Loss: 0.00001853
Iteration 112/1000 | Loss: 0.00001853
Iteration 113/1000 | Loss: 0.00001853
Iteration 114/1000 | Loss: 0.00001853
Iteration 115/1000 | Loss: 0.00001853
Iteration 116/1000 | Loss: 0.00001853
Iteration 117/1000 | Loss: 0.00001853
Iteration 118/1000 | Loss: 0.00001853
Iteration 119/1000 | Loss: 0.00001853
Iteration 120/1000 | Loss: 0.00001853
Iteration 121/1000 | Loss: 0.00001853
Iteration 122/1000 | Loss: 0.00001853
Iteration 123/1000 | Loss: 0.00001853
Iteration 124/1000 | Loss: 0.00001853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.8534865375841036e-05, 1.8534865375841036e-05, 1.8534865375841036e-05, 1.8534865375841036e-05, 1.8534865375841036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8534865375841036e-05

Optimization complete. Final v2v error: 3.675490379333496 mm

Highest mean error: 4.37429666519165 mm for frame 90

Lowest mean error: 3.128983497619629 mm for frame 1

Saving results

Total time: 30.66718888282776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416919
Iteration 2/25 | Loss: 0.00114844
Iteration 3/25 | Loss: 0.00103322
Iteration 4/25 | Loss: 0.00101291
Iteration 5/25 | Loss: 0.00100477
Iteration 6/25 | Loss: 0.00100276
Iteration 7/25 | Loss: 0.00100255
Iteration 8/25 | Loss: 0.00100255
Iteration 9/25 | Loss: 0.00100255
Iteration 10/25 | Loss: 0.00100255
Iteration 11/25 | Loss: 0.00100255
Iteration 12/25 | Loss: 0.00100255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001002547680400312, 0.001002547680400312, 0.001002547680400312, 0.001002547680400312, 0.001002547680400312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001002547680400312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24689376
Iteration 2/25 | Loss: 0.00079726
Iteration 3/25 | Loss: 0.00079725
Iteration 4/25 | Loss: 0.00079725
Iteration 5/25 | Loss: 0.00079725
Iteration 6/25 | Loss: 0.00079725
Iteration 7/25 | Loss: 0.00079725
Iteration 8/25 | Loss: 0.00079725
Iteration 9/25 | Loss: 0.00079725
Iteration 10/25 | Loss: 0.00079725
Iteration 11/25 | Loss: 0.00079725
Iteration 12/25 | Loss: 0.00079725
Iteration 13/25 | Loss: 0.00079725
Iteration 14/25 | Loss: 0.00079725
Iteration 15/25 | Loss: 0.00079725
Iteration 16/25 | Loss: 0.00079725
Iteration 17/25 | Loss: 0.00079725
Iteration 18/25 | Loss: 0.00079725
Iteration 19/25 | Loss: 0.00079725
Iteration 20/25 | Loss: 0.00079725
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000797252869233489, 0.000797252869233489, 0.000797252869233489, 0.000797252869233489, 0.000797252869233489]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000797252869233489

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079725
Iteration 2/1000 | Loss: 0.00006705
Iteration 3/1000 | Loss: 0.00004494
Iteration 4/1000 | Loss: 0.00003899
Iteration 5/1000 | Loss: 0.00003566
Iteration 6/1000 | Loss: 0.00003383
Iteration 7/1000 | Loss: 0.00003273
Iteration 8/1000 | Loss: 0.00003171
Iteration 9/1000 | Loss: 0.00003097
Iteration 10/1000 | Loss: 0.00003060
Iteration 11/1000 | Loss: 0.00003033
Iteration 12/1000 | Loss: 0.00003032
Iteration 13/1000 | Loss: 0.00003014
Iteration 14/1000 | Loss: 0.00003010
Iteration 15/1000 | Loss: 0.00003003
Iteration 16/1000 | Loss: 0.00002999
Iteration 17/1000 | Loss: 0.00002999
Iteration 18/1000 | Loss: 0.00002998
Iteration 19/1000 | Loss: 0.00002997
Iteration 20/1000 | Loss: 0.00002997
Iteration 21/1000 | Loss: 0.00002996
Iteration 22/1000 | Loss: 0.00002996
Iteration 23/1000 | Loss: 0.00002996
Iteration 24/1000 | Loss: 0.00002996
Iteration 25/1000 | Loss: 0.00002995
Iteration 26/1000 | Loss: 0.00002995
Iteration 27/1000 | Loss: 0.00002995
Iteration 28/1000 | Loss: 0.00002995
Iteration 29/1000 | Loss: 0.00002995
Iteration 30/1000 | Loss: 0.00002995
Iteration 31/1000 | Loss: 0.00002994
Iteration 32/1000 | Loss: 0.00002994
Iteration 33/1000 | Loss: 0.00002994
Iteration 34/1000 | Loss: 0.00002993
Iteration 35/1000 | Loss: 0.00002993
Iteration 36/1000 | Loss: 0.00002993
Iteration 37/1000 | Loss: 0.00002992
Iteration 38/1000 | Loss: 0.00002992
Iteration 39/1000 | Loss: 0.00002992
Iteration 40/1000 | Loss: 0.00002992
Iteration 41/1000 | Loss: 0.00002992
Iteration 42/1000 | Loss: 0.00002992
Iteration 43/1000 | Loss: 0.00002992
Iteration 44/1000 | Loss: 0.00002991
Iteration 45/1000 | Loss: 0.00002991
Iteration 46/1000 | Loss: 0.00002991
Iteration 47/1000 | Loss: 0.00002991
Iteration 48/1000 | Loss: 0.00002991
Iteration 49/1000 | Loss: 0.00002990
Iteration 50/1000 | Loss: 0.00002990
Iteration 51/1000 | Loss: 0.00002989
Iteration 52/1000 | Loss: 0.00002989
Iteration 53/1000 | Loss: 0.00002989
Iteration 54/1000 | Loss: 0.00002989
Iteration 55/1000 | Loss: 0.00002989
Iteration 56/1000 | Loss: 0.00002989
Iteration 57/1000 | Loss: 0.00002989
Iteration 58/1000 | Loss: 0.00002989
Iteration 59/1000 | Loss: 0.00002989
Iteration 60/1000 | Loss: 0.00002988
Iteration 61/1000 | Loss: 0.00002988
Iteration 62/1000 | Loss: 0.00002988
Iteration 63/1000 | Loss: 0.00002988
Iteration 64/1000 | Loss: 0.00002988
Iteration 65/1000 | Loss: 0.00002988
Iteration 66/1000 | Loss: 0.00002987
Iteration 67/1000 | Loss: 0.00002987
Iteration 68/1000 | Loss: 0.00002987
Iteration 69/1000 | Loss: 0.00002987
Iteration 70/1000 | Loss: 0.00002987
Iteration 71/1000 | Loss: 0.00002987
Iteration 72/1000 | Loss: 0.00002987
Iteration 73/1000 | Loss: 0.00002987
Iteration 74/1000 | Loss: 0.00002986
Iteration 75/1000 | Loss: 0.00002986
Iteration 76/1000 | Loss: 0.00002986
Iteration 77/1000 | Loss: 0.00002986
Iteration 78/1000 | Loss: 0.00002986
Iteration 79/1000 | Loss: 0.00002986
Iteration 80/1000 | Loss: 0.00002986
Iteration 81/1000 | Loss: 0.00002986
Iteration 82/1000 | Loss: 0.00002985
Iteration 83/1000 | Loss: 0.00002985
Iteration 84/1000 | Loss: 0.00002985
Iteration 85/1000 | Loss: 0.00002985
Iteration 86/1000 | Loss: 0.00002985
Iteration 87/1000 | Loss: 0.00002984
Iteration 88/1000 | Loss: 0.00002984
Iteration 89/1000 | Loss: 0.00002984
Iteration 90/1000 | Loss: 0.00002984
Iteration 91/1000 | Loss: 0.00002984
Iteration 92/1000 | Loss: 0.00002984
Iteration 93/1000 | Loss: 0.00002984
Iteration 94/1000 | Loss: 0.00002983
Iteration 95/1000 | Loss: 0.00002983
Iteration 96/1000 | Loss: 0.00002983
Iteration 97/1000 | Loss: 0.00002983
Iteration 98/1000 | Loss: 0.00002983
Iteration 99/1000 | Loss: 0.00002983
Iteration 100/1000 | Loss: 0.00002983
Iteration 101/1000 | Loss: 0.00002982
Iteration 102/1000 | Loss: 0.00002982
Iteration 103/1000 | Loss: 0.00002982
Iteration 104/1000 | Loss: 0.00002982
Iteration 105/1000 | Loss: 0.00002982
Iteration 106/1000 | Loss: 0.00002982
Iteration 107/1000 | Loss: 0.00002982
Iteration 108/1000 | Loss: 0.00002982
Iteration 109/1000 | Loss: 0.00002982
Iteration 110/1000 | Loss: 0.00002981
Iteration 111/1000 | Loss: 0.00002981
Iteration 112/1000 | Loss: 0.00002981
Iteration 113/1000 | Loss: 0.00002981
Iteration 114/1000 | Loss: 0.00002981
Iteration 115/1000 | Loss: 0.00002980
Iteration 116/1000 | Loss: 0.00002980
Iteration 117/1000 | Loss: 0.00002980
Iteration 118/1000 | Loss: 0.00002980
Iteration 119/1000 | Loss: 0.00002980
Iteration 120/1000 | Loss: 0.00002979
Iteration 121/1000 | Loss: 0.00002979
Iteration 122/1000 | Loss: 0.00002979
Iteration 123/1000 | Loss: 0.00002979
Iteration 124/1000 | Loss: 0.00002979
Iteration 125/1000 | Loss: 0.00002979
Iteration 126/1000 | Loss: 0.00002979
Iteration 127/1000 | Loss: 0.00002978
Iteration 128/1000 | Loss: 0.00002978
Iteration 129/1000 | Loss: 0.00002978
Iteration 130/1000 | Loss: 0.00002978
Iteration 131/1000 | Loss: 0.00002978
Iteration 132/1000 | Loss: 0.00002977
Iteration 133/1000 | Loss: 0.00002977
Iteration 134/1000 | Loss: 0.00002977
Iteration 135/1000 | Loss: 0.00002977
Iteration 136/1000 | Loss: 0.00002976
Iteration 137/1000 | Loss: 0.00002976
Iteration 138/1000 | Loss: 0.00002976
Iteration 139/1000 | Loss: 0.00002976
Iteration 140/1000 | Loss: 0.00002976
Iteration 141/1000 | Loss: 0.00002976
Iteration 142/1000 | Loss: 0.00002975
Iteration 143/1000 | Loss: 0.00002975
Iteration 144/1000 | Loss: 0.00002975
Iteration 145/1000 | Loss: 0.00002975
Iteration 146/1000 | Loss: 0.00002975
Iteration 147/1000 | Loss: 0.00002975
Iteration 148/1000 | Loss: 0.00002975
Iteration 149/1000 | Loss: 0.00002975
Iteration 150/1000 | Loss: 0.00002975
Iteration 151/1000 | Loss: 0.00002975
Iteration 152/1000 | Loss: 0.00002974
Iteration 153/1000 | Loss: 0.00002974
Iteration 154/1000 | Loss: 0.00002974
Iteration 155/1000 | Loss: 0.00002974
Iteration 156/1000 | Loss: 0.00002974
Iteration 157/1000 | Loss: 0.00002974
Iteration 158/1000 | Loss: 0.00002974
Iteration 159/1000 | Loss: 0.00002974
Iteration 160/1000 | Loss: 0.00002974
Iteration 161/1000 | Loss: 0.00002974
Iteration 162/1000 | Loss: 0.00002974
Iteration 163/1000 | Loss: 0.00002974
Iteration 164/1000 | Loss: 0.00002974
Iteration 165/1000 | Loss: 0.00002974
Iteration 166/1000 | Loss: 0.00002974
Iteration 167/1000 | Loss: 0.00002974
Iteration 168/1000 | Loss: 0.00002974
Iteration 169/1000 | Loss: 0.00002974
Iteration 170/1000 | Loss: 0.00002974
Iteration 171/1000 | Loss: 0.00002974
Iteration 172/1000 | Loss: 0.00002974
Iteration 173/1000 | Loss: 0.00002974
Iteration 174/1000 | Loss: 0.00002974
Iteration 175/1000 | Loss: 0.00002974
Iteration 176/1000 | Loss: 0.00002974
Iteration 177/1000 | Loss: 0.00002974
Iteration 178/1000 | Loss: 0.00002974
Iteration 179/1000 | Loss: 0.00002974
Iteration 180/1000 | Loss: 0.00002974
Iteration 181/1000 | Loss: 0.00002974
Iteration 182/1000 | Loss: 0.00002974
Iteration 183/1000 | Loss: 0.00002974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.973727896460332e-05, 2.973727896460332e-05, 2.973727896460332e-05, 2.973727896460332e-05, 2.973727896460332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.973727896460332e-05

Optimization complete. Final v2v error: 4.4652485847473145 mm

Highest mean error: 4.698149681091309 mm for frame 106

Lowest mean error: 4.131958484649658 mm for frame 131

Saving results

Total time: 38.876357555389404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861339
Iteration 2/25 | Loss: 0.00109080
Iteration 3/25 | Loss: 0.00094560
Iteration 4/25 | Loss: 0.00091587
Iteration 5/25 | Loss: 0.00091007
Iteration 6/25 | Loss: 0.00090869
Iteration 7/25 | Loss: 0.00090869
Iteration 8/25 | Loss: 0.00090869
Iteration 9/25 | Loss: 0.00090869
Iteration 10/25 | Loss: 0.00090869
Iteration 11/25 | Loss: 0.00090869
Iteration 12/25 | Loss: 0.00090869
Iteration 13/25 | Loss: 0.00090869
Iteration 14/25 | Loss: 0.00090869
Iteration 15/25 | Loss: 0.00090869
Iteration 16/25 | Loss: 0.00090869
Iteration 17/25 | Loss: 0.00090869
Iteration 18/25 | Loss: 0.00090869
Iteration 19/25 | Loss: 0.00090869
Iteration 20/25 | Loss: 0.00090869
Iteration 21/25 | Loss: 0.00090869
Iteration 22/25 | Loss: 0.00090869
Iteration 23/25 | Loss: 0.00090869
Iteration 24/25 | Loss: 0.00090869
Iteration 25/25 | Loss: 0.00090869

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30440366
Iteration 2/25 | Loss: 0.00073902
Iteration 3/25 | Loss: 0.00073902
Iteration 4/25 | Loss: 0.00073902
Iteration 5/25 | Loss: 0.00073902
Iteration 6/25 | Loss: 0.00073902
Iteration 7/25 | Loss: 0.00073902
Iteration 8/25 | Loss: 0.00073902
Iteration 9/25 | Loss: 0.00073902
Iteration 10/25 | Loss: 0.00073902
Iteration 11/25 | Loss: 0.00073902
Iteration 12/25 | Loss: 0.00073902
Iteration 13/25 | Loss: 0.00073902
Iteration 14/25 | Loss: 0.00073902
Iteration 15/25 | Loss: 0.00073902
Iteration 16/25 | Loss: 0.00073902
Iteration 17/25 | Loss: 0.00073902
Iteration 18/25 | Loss: 0.00073902
Iteration 19/25 | Loss: 0.00073902
Iteration 20/25 | Loss: 0.00073902
Iteration 21/25 | Loss: 0.00073902
Iteration 22/25 | Loss: 0.00073902
Iteration 23/25 | Loss: 0.00073902
Iteration 24/25 | Loss: 0.00073902
Iteration 25/25 | Loss: 0.00073902

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073902
Iteration 2/1000 | Loss: 0.00003044
Iteration 3/1000 | Loss: 0.00002323
Iteration 4/1000 | Loss: 0.00002079
Iteration 5/1000 | Loss: 0.00001959
Iteration 6/1000 | Loss: 0.00001901
Iteration 7/1000 | Loss: 0.00001840
Iteration 8/1000 | Loss: 0.00001803
Iteration 9/1000 | Loss: 0.00001787
Iteration 10/1000 | Loss: 0.00001783
Iteration 11/1000 | Loss: 0.00001782
Iteration 12/1000 | Loss: 0.00001782
Iteration 13/1000 | Loss: 0.00001779
Iteration 14/1000 | Loss: 0.00001767
Iteration 15/1000 | Loss: 0.00001767
Iteration 16/1000 | Loss: 0.00001760
Iteration 17/1000 | Loss: 0.00001760
Iteration 18/1000 | Loss: 0.00001758
Iteration 19/1000 | Loss: 0.00001758
Iteration 20/1000 | Loss: 0.00001757
Iteration 21/1000 | Loss: 0.00001757
Iteration 22/1000 | Loss: 0.00001756
Iteration 23/1000 | Loss: 0.00001756
Iteration 24/1000 | Loss: 0.00001752
Iteration 25/1000 | Loss: 0.00001751
Iteration 26/1000 | Loss: 0.00001750
Iteration 27/1000 | Loss: 0.00001749
Iteration 28/1000 | Loss: 0.00001749
Iteration 29/1000 | Loss: 0.00001749
Iteration 30/1000 | Loss: 0.00001747
Iteration 31/1000 | Loss: 0.00001747
Iteration 32/1000 | Loss: 0.00001747
Iteration 33/1000 | Loss: 0.00001747
Iteration 34/1000 | Loss: 0.00001747
Iteration 35/1000 | Loss: 0.00001746
Iteration 36/1000 | Loss: 0.00001746
Iteration 37/1000 | Loss: 0.00001746
Iteration 38/1000 | Loss: 0.00001746
Iteration 39/1000 | Loss: 0.00001746
Iteration 40/1000 | Loss: 0.00001743
Iteration 41/1000 | Loss: 0.00001743
Iteration 42/1000 | Loss: 0.00001742
Iteration 43/1000 | Loss: 0.00001742
Iteration 44/1000 | Loss: 0.00001741
Iteration 45/1000 | Loss: 0.00001741
Iteration 46/1000 | Loss: 0.00001740
Iteration 47/1000 | Loss: 0.00001739
Iteration 48/1000 | Loss: 0.00001737
Iteration 49/1000 | Loss: 0.00001737
Iteration 50/1000 | Loss: 0.00001737
Iteration 51/1000 | Loss: 0.00001737
Iteration 52/1000 | Loss: 0.00001737
Iteration 53/1000 | Loss: 0.00001737
Iteration 54/1000 | Loss: 0.00001737
Iteration 55/1000 | Loss: 0.00001737
Iteration 56/1000 | Loss: 0.00001736
Iteration 57/1000 | Loss: 0.00001736
Iteration 58/1000 | Loss: 0.00001736
Iteration 59/1000 | Loss: 0.00001733
Iteration 60/1000 | Loss: 0.00001732
Iteration 61/1000 | Loss: 0.00001732
Iteration 62/1000 | Loss: 0.00001732
Iteration 63/1000 | Loss: 0.00001732
Iteration 64/1000 | Loss: 0.00001731
Iteration 65/1000 | Loss: 0.00001731
Iteration 66/1000 | Loss: 0.00001731
Iteration 67/1000 | Loss: 0.00001730
Iteration 68/1000 | Loss: 0.00001730
Iteration 69/1000 | Loss: 0.00001730
Iteration 70/1000 | Loss: 0.00001730
Iteration 71/1000 | Loss: 0.00001729
Iteration 72/1000 | Loss: 0.00001729
Iteration 73/1000 | Loss: 0.00001729
Iteration 74/1000 | Loss: 0.00001728
Iteration 75/1000 | Loss: 0.00001728
Iteration 76/1000 | Loss: 0.00001728
Iteration 77/1000 | Loss: 0.00001727
Iteration 78/1000 | Loss: 0.00001727
Iteration 79/1000 | Loss: 0.00001727
Iteration 80/1000 | Loss: 0.00001727
Iteration 81/1000 | Loss: 0.00001727
Iteration 82/1000 | Loss: 0.00001726
Iteration 83/1000 | Loss: 0.00001726
Iteration 84/1000 | Loss: 0.00001726
Iteration 85/1000 | Loss: 0.00001726
Iteration 86/1000 | Loss: 0.00001726
Iteration 87/1000 | Loss: 0.00001726
Iteration 88/1000 | Loss: 0.00001726
Iteration 89/1000 | Loss: 0.00001726
Iteration 90/1000 | Loss: 0.00001726
Iteration 91/1000 | Loss: 0.00001726
Iteration 92/1000 | Loss: 0.00001726
Iteration 93/1000 | Loss: 0.00001726
Iteration 94/1000 | Loss: 0.00001726
Iteration 95/1000 | Loss: 0.00001726
Iteration 96/1000 | Loss: 0.00001726
Iteration 97/1000 | Loss: 0.00001726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.7255350030609407e-05, 1.7255350030609407e-05, 1.7255350030609407e-05, 1.7255350030609407e-05, 1.7255350030609407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7255350030609407e-05

Optimization complete. Final v2v error: 3.577474594116211 mm

Highest mean error: 3.9203572273254395 mm for frame 102

Lowest mean error: 3.2492029666900635 mm for frame 48

Saving results

Total time: 36.163472414016724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082063
Iteration 2/25 | Loss: 0.00187134
Iteration 3/25 | Loss: 0.00123054
Iteration 4/25 | Loss: 0.00101385
Iteration 5/25 | Loss: 0.00091319
Iteration 6/25 | Loss: 0.00089879
Iteration 7/25 | Loss: 0.00087568
Iteration 8/25 | Loss: 0.00086225
Iteration 9/25 | Loss: 0.00084659
Iteration 10/25 | Loss: 0.00083879
Iteration 11/25 | Loss: 0.00083106
Iteration 12/25 | Loss: 0.00083122
Iteration 13/25 | Loss: 0.00083303
Iteration 14/25 | Loss: 0.00082629
Iteration 15/25 | Loss: 0.00082176
Iteration 16/25 | Loss: 0.00082163
Iteration 17/25 | Loss: 0.00082879
Iteration 18/25 | Loss: 0.00082105
Iteration 19/25 | Loss: 0.00081539
Iteration 20/25 | Loss: 0.00081514
Iteration 21/25 | Loss: 0.00081575
Iteration 22/25 | Loss: 0.00081352
Iteration 23/25 | Loss: 0.00081349
Iteration 24/25 | Loss: 0.00081349
Iteration 25/25 | Loss: 0.00081349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33421457
Iteration 2/25 | Loss: 0.00076359
Iteration 3/25 | Loss: 0.00074884
Iteration 4/25 | Loss: 0.00074884
Iteration 5/25 | Loss: 0.00074884
Iteration 6/25 | Loss: 0.00074884
Iteration 7/25 | Loss: 0.00074884
Iteration 8/25 | Loss: 0.00074884
Iteration 9/25 | Loss: 0.00074884
Iteration 10/25 | Loss: 0.00074883
Iteration 11/25 | Loss: 0.00074883
Iteration 12/25 | Loss: 0.00074883
Iteration 13/25 | Loss: 0.00074883
Iteration 14/25 | Loss: 0.00074883
Iteration 15/25 | Loss: 0.00074883
Iteration 16/25 | Loss: 0.00074883
Iteration 17/25 | Loss: 0.00074883
Iteration 18/25 | Loss: 0.00074883
Iteration 19/25 | Loss: 0.00074883
Iteration 20/25 | Loss: 0.00074883
Iteration 21/25 | Loss: 0.00074883
Iteration 22/25 | Loss: 0.00074883
Iteration 23/25 | Loss: 0.00074883
Iteration 24/25 | Loss: 0.00074883
Iteration 25/25 | Loss: 0.00074883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074883
Iteration 2/1000 | Loss: 0.00003793
Iteration 3/1000 | Loss: 0.00002547
Iteration 4/1000 | Loss: 0.00003216
Iteration 5/1000 | Loss: 0.00002004
Iteration 6/1000 | Loss: 0.00001912
Iteration 7/1000 | Loss: 0.00001848
Iteration 8/1000 | Loss: 0.00003190
Iteration 9/1000 | Loss: 0.00002912
Iteration 10/1000 | Loss: 0.00001782
Iteration 11/1000 | Loss: 0.00001766
Iteration 12/1000 | Loss: 0.00001762
Iteration 13/1000 | Loss: 0.00001761
Iteration 14/1000 | Loss: 0.00001751
Iteration 15/1000 | Loss: 0.00001751
Iteration 16/1000 | Loss: 0.00001748
Iteration 17/1000 | Loss: 0.00001746
Iteration 18/1000 | Loss: 0.00001745
Iteration 19/1000 | Loss: 0.00001744
Iteration 20/1000 | Loss: 0.00002196
Iteration 21/1000 | Loss: 0.00001789
Iteration 22/1000 | Loss: 0.00001760
Iteration 23/1000 | Loss: 0.00001835
Iteration 24/1000 | Loss: 0.00001740
Iteration 25/1000 | Loss: 0.00001732
Iteration 26/1000 | Loss: 0.00001732
Iteration 27/1000 | Loss: 0.00001732
Iteration 28/1000 | Loss: 0.00001732
Iteration 29/1000 | Loss: 0.00001732
Iteration 30/1000 | Loss: 0.00001732
Iteration 31/1000 | Loss: 0.00001732
Iteration 32/1000 | Loss: 0.00001731
Iteration 33/1000 | Loss: 0.00001731
Iteration 34/1000 | Loss: 0.00001812
Iteration 35/1000 | Loss: 0.00001812
Iteration 36/1000 | Loss: 0.00001783
Iteration 37/1000 | Loss: 0.00001740
Iteration 38/1000 | Loss: 0.00001723
Iteration 39/1000 | Loss: 0.00001723
Iteration 40/1000 | Loss: 0.00001723
Iteration 41/1000 | Loss: 0.00001723
Iteration 42/1000 | Loss: 0.00001723
Iteration 43/1000 | Loss: 0.00001722
Iteration 44/1000 | Loss: 0.00001722
Iteration 45/1000 | Loss: 0.00001722
Iteration 46/1000 | Loss: 0.00001722
Iteration 47/1000 | Loss: 0.00001722
Iteration 48/1000 | Loss: 0.00001722
Iteration 49/1000 | Loss: 0.00001722
Iteration 50/1000 | Loss: 0.00001722
Iteration 51/1000 | Loss: 0.00001722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 51. Stopping optimization.
Last 5 losses: [1.722389606584329e-05, 1.722389606584329e-05, 1.722389606584329e-05, 1.722389606584329e-05, 1.722389606584329e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.722389606584329e-05

Optimization complete. Final v2v error: 3.363879919052124 mm

Highest mean error: 10.211601257324219 mm for frame 90

Lowest mean error: 2.908663272857666 mm for frame 138

Saving results

Total time: 67.37823629379272
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026417
Iteration 2/25 | Loss: 0.00334393
Iteration 3/25 | Loss: 0.00239340
Iteration 4/25 | Loss: 0.00198657
Iteration 5/25 | Loss: 0.00180042
Iteration 6/25 | Loss: 0.00170060
Iteration 7/25 | Loss: 0.00163600
Iteration 8/25 | Loss: 0.00161458
Iteration 9/25 | Loss: 0.00157467
Iteration 10/25 | Loss: 0.00150523
Iteration 11/25 | Loss: 0.00144025
Iteration 12/25 | Loss: 0.00142716
Iteration 13/25 | Loss: 0.00140519
Iteration 14/25 | Loss: 0.00139681
Iteration 15/25 | Loss: 0.00138277
Iteration 16/25 | Loss: 0.00136149
Iteration 17/25 | Loss: 0.00133846
Iteration 18/25 | Loss: 0.00133540
Iteration 19/25 | Loss: 0.00132808
Iteration 20/25 | Loss: 0.00132174
Iteration 21/25 | Loss: 0.00132239
Iteration 22/25 | Loss: 0.00131495
Iteration 23/25 | Loss: 0.00131322
Iteration 24/25 | Loss: 0.00131281
Iteration 25/25 | Loss: 0.00131280

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28687453
Iteration 2/25 | Loss: 0.00528376
Iteration 3/25 | Loss: 0.00466215
Iteration 4/25 | Loss: 0.00466210
Iteration 5/25 | Loss: 0.00466210
Iteration 6/25 | Loss: 0.00466210
Iteration 7/25 | Loss: 0.00466210
Iteration 8/25 | Loss: 0.00466210
Iteration 9/25 | Loss: 0.00466210
Iteration 10/25 | Loss: 0.00466210
Iteration 11/25 | Loss: 0.00466210
Iteration 12/25 | Loss: 0.00466210
Iteration 13/25 | Loss: 0.00466210
Iteration 14/25 | Loss: 0.00466210
Iteration 15/25 | Loss: 0.00466210
Iteration 16/25 | Loss: 0.00466210
Iteration 17/25 | Loss: 0.00466210
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0046620965003967285, 0.0046620965003967285, 0.0046620965003967285, 0.0046620965003967285, 0.0046620965003967285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0046620965003967285

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00466210
Iteration 2/1000 | Loss: 0.00206715
Iteration 3/1000 | Loss: 0.00358353
Iteration 4/1000 | Loss: 0.00133279
Iteration 5/1000 | Loss: 0.00088765
Iteration 6/1000 | Loss: 0.00055154
Iteration 7/1000 | Loss: 0.00070374
Iteration 8/1000 | Loss: 0.00084073
Iteration 9/1000 | Loss: 0.00053388
Iteration 10/1000 | Loss: 0.00445978
Iteration 11/1000 | Loss: 0.00094180
Iteration 12/1000 | Loss: 0.00048704
Iteration 13/1000 | Loss: 0.00066599
Iteration 14/1000 | Loss: 0.00032949
Iteration 15/1000 | Loss: 0.00026947
Iteration 16/1000 | Loss: 0.00076740
Iteration 17/1000 | Loss: 0.00039809
Iteration 18/1000 | Loss: 0.00070659
Iteration 19/1000 | Loss: 0.00046262
Iteration 20/1000 | Loss: 0.00109982
Iteration 21/1000 | Loss: 0.00041948
Iteration 22/1000 | Loss: 0.00027072
Iteration 23/1000 | Loss: 0.00021862
Iteration 24/1000 | Loss: 0.00056078
Iteration 25/1000 | Loss: 0.00031851
Iteration 26/1000 | Loss: 0.00019298
Iteration 27/1000 | Loss: 0.00149591
Iteration 28/1000 | Loss: 0.00279139
Iteration 29/1000 | Loss: 0.00411236
Iteration 30/1000 | Loss: 0.00197405
Iteration 31/1000 | Loss: 0.00085206
Iteration 32/1000 | Loss: 0.00272370
Iteration 33/1000 | Loss: 0.00262204
Iteration 34/1000 | Loss: 0.00121708
Iteration 35/1000 | Loss: 0.00061244
Iteration 36/1000 | Loss: 0.00112749
Iteration 37/1000 | Loss: 0.00260783
Iteration 38/1000 | Loss: 0.00272702
Iteration 39/1000 | Loss: 0.00078647
Iteration 40/1000 | Loss: 0.00067073
Iteration 41/1000 | Loss: 0.00024916
Iteration 42/1000 | Loss: 0.00026575
Iteration 43/1000 | Loss: 0.00095213
Iteration 44/1000 | Loss: 0.00090040
Iteration 45/1000 | Loss: 0.00083230
Iteration 46/1000 | Loss: 0.00018953
Iteration 47/1000 | Loss: 0.00069875
Iteration 48/1000 | Loss: 0.00043232
Iteration 49/1000 | Loss: 0.00042277
Iteration 50/1000 | Loss: 0.00015475
Iteration 51/1000 | Loss: 0.00034058
Iteration 52/1000 | Loss: 0.00037742
Iteration 53/1000 | Loss: 0.00014691
Iteration 54/1000 | Loss: 0.00012968
Iteration 55/1000 | Loss: 0.00012142
Iteration 56/1000 | Loss: 0.00011579
Iteration 57/1000 | Loss: 0.00011183
Iteration 58/1000 | Loss: 0.00043541
Iteration 59/1000 | Loss: 0.00018622
Iteration 60/1000 | Loss: 0.00011688
Iteration 61/1000 | Loss: 0.00011042
Iteration 62/1000 | Loss: 0.00010859
Iteration 63/1000 | Loss: 0.00010543
Iteration 64/1000 | Loss: 0.00010198
Iteration 65/1000 | Loss: 0.00010052
Iteration 66/1000 | Loss: 0.00009976
Iteration 67/1000 | Loss: 0.00009939
Iteration 68/1000 | Loss: 0.00009900
Iteration 69/1000 | Loss: 0.00009873
Iteration 70/1000 | Loss: 0.00009867
Iteration 71/1000 | Loss: 0.00009867
Iteration 72/1000 | Loss: 0.00009866
Iteration 73/1000 | Loss: 0.00009861
Iteration 74/1000 | Loss: 0.00009861
Iteration 75/1000 | Loss: 0.00009860
Iteration 76/1000 | Loss: 0.00009860
Iteration 77/1000 | Loss: 0.00009857
Iteration 78/1000 | Loss: 0.00009857
Iteration 79/1000 | Loss: 0.00009857
Iteration 80/1000 | Loss: 0.00009857
Iteration 81/1000 | Loss: 0.00009857
Iteration 82/1000 | Loss: 0.00009857
Iteration 83/1000 | Loss: 0.00009857
Iteration 84/1000 | Loss: 0.00009857
Iteration 85/1000 | Loss: 0.00009857
Iteration 86/1000 | Loss: 0.00009856
Iteration 87/1000 | Loss: 0.00009856
Iteration 88/1000 | Loss: 0.00009856
Iteration 89/1000 | Loss: 0.00009856
Iteration 90/1000 | Loss: 0.00009856
Iteration 91/1000 | Loss: 0.00009855
Iteration 92/1000 | Loss: 0.00009855
Iteration 93/1000 | Loss: 0.00009853
Iteration 94/1000 | Loss: 0.00009851
Iteration 95/1000 | Loss: 0.00009849
Iteration 96/1000 | Loss: 0.00009847
Iteration 97/1000 | Loss: 0.00009846
Iteration 98/1000 | Loss: 0.00009842
Iteration 99/1000 | Loss: 0.00009842
Iteration 100/1000 | Loss: 0.00009841
Iteration 101/1000 | Loss: 0.00009841
Iteration 102/1000 | Loss: 0.00009841
Iteration 103/1000 | Loss: 0.00009840
Iteration 104/1000 | Loss: 0.00009840
Iteration 105/1000 | Loss: 0.00009839
Iteration 106/1000 | Loss: 0.00009839
Iteration 107/1000 | Loss: 0.00009839
Iteration 108/1000 | Loss: 0.00009838
Iteration 109/1000 | Loss: 0.00009838
Iteration 110/1000 | Loss: 0.00009838
Iteration 111/1000 | Loss: 0.00009838
Iteration 112/1000 | Loss: 0.00009838
Iteration 113/1000 | Loss: 0.00009837
Iteration 114/1000 | Loss: 0.00009837
Iteration 115/1000 | Loss: 0.00009836
Iteration 116/1000 | Loss: 0.00009836
Iteration 117/1000 | Loss: 0.00009836
Iteration 118/1000 | Loss: 0.00009836
Iteration 119/1000 | Loss: 0.00009836
Iteration 120/1000 | Loss: 0.00009836
Iteration 121/1000 | Loss: 0.00009836
Iteration 122/1000 | Loss: 0.00009836
Iteration 123/1000 | Loss: 0.00009835
Iteration 124/1000 | Loss: 0.00009835
Iteration 125/1000 | Loss: 0.00009835
Iteration 126/1000 | Loss: 0.00009835
Iteration 127/1000 | Loss: 0.00009835
Iteration 128/1000 | Loss: 0.00009835
Iteration 129/1000 | Loss: 0.00009835
Iteration 130/1000 | Loss: 0.00009834
Iteration 131/1000 | Loss: 0.00009834
Iteration 132/1000 | Loss: 0.00009834
Iteration 133/1000 | Loss: 0.00009834
Iteration 134/1000 | Loss: 0.00009833
Iteration 135/1000 | Loss: 0.00009833
Iteration 136/1000 | Loss: 0.00009832
Iteration 137/1000 | Loss: 0.00009832
Iteration 138/1000 | Loss: 0.00009832
Iteration 139/1000 | Loss: 0.00009832
Iteration 140/1000 | Loss: 0.00009832
Iteration 141/1000 | Loss: 0.00009832
Iteration 142/1000 | Loss: 0.00009832
Iteration 143/1000 | Loss: 0.00009832
Iteration 144/1000 | Loss: 0.00009832
Iteration 145/1000 | Loss: 0.00009832
Iteration 146/1000 | Loss: 0.00009832
Iteration 147/1000 | Loss: 0.00009832
Iteration 148/1000 | Loss: 0.00009832
Iteration 149/1000 | Loss: 0.00009831
Iteration 150/1000 | Loss: 0.00009831
Iteration 151/1000 | Loss: 0.00009831
Iteration 152/1000 | Loss: 0.00009831
Iteration 153/1000 | Loss: 0.00009831
Iteration 154/1000 | Loss: 0.00009831
Iteration 155/1000 | Loss: 0.00009831
Iteration 156/1000 | Loss: 0.00009831
Iteration 157/1000 | Loss: 0.00009831
Iteration 158/1000 | Loss: 0.00009831
Iteration 159/1000 | Loss: 0.00009831
Iteration 160/1000 | Loss: 0.00009831
Iteration 161/1000 | Loss: 0.00009831
Iteration 162/1000 | Loss: 0.00009831
Iteration 163/1000 | Loss: 0.00009831
Iteration 164/1000 | Loss: 0.00009831
Iteration 165/1000 | Loss: 0.00009831
Iteration 166/1000 | Loss: 0.00009831
Iteration 167/1000 | Loss: 0.00009831
Iteration 168/1000 | Loss: 0.00009831
Iteration 169/1000 | Loss: 0.00009831
Iteration 170/1000 | Loss: 0.00009831
Iteration 171/1000 | Loss: 0.00009831
Iteration 172/1000 | Loss: 0.00009831
Iteration 173/1000 | Loss: 0.00009831
Iteration 174/1000 | Loss: 0.00009831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [9.830775525188074e-05, 9.830775525188074e-05, 9.830775525188074e-05, 9.830775525188074e-05, 9.830775525188074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.830775525188074e-05

Optimization complete. Final v2v error: 5.82833194732666 mm

Highest mean error: 11.947131156921387 mm for frame 96

Lowest mean error: 4.012425422668457 mm for frame 10

Saving results

Total time: 148.8894805908203
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093049
Iteration 2/25 | Loss: 0.00318061
Iteration 3/25 | Loss: 0.00166955
Iteration 4/25 | Loss: 0.00117038
Iteration 5/25 | Loss: 0.00108340
Iteration 6/25 | Loss: 0.00106999
Iteration 7/25 | Loss: 0.00106304
Iteration 8/25 | Loss: 0.00106098
Iteration 9/25 | Loss: 0.00106715
Iteration 10/25 | Loss: 0.00106314
Iteration 11/25 | Loss: 0.00105093
Iteration 12/25 | Loss: 0.00104914
Iteration 13/25 | Loss: 0.00104842
Iteration 14/25 | Loss: 0.00104322
Iteration 15/25 | Loss: 0.00104038
Iteration 16/25 | Loss: 0.00103947
Iteration 17/25 | Loss: 0.00103329
Iteration 18/25 | Loss: 0.00102180
Iteration 19/25 | Loss: 0.00102228
Iteration 20/25 | Loss: 0.00101995
Iteration 21/25 | Loss: 0.00101897
Iteration 22/25 | Loss: 0.00101554
Iteration 23/25 | Loss: 0.00101359
Iteration 24/25 | Loss: 0.00101272
Iteration 25/25 | Loss: 0.00101244

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41430581
Iteration 2/25 | Loss: 0.00082150
Iteration 3/25 | Loss: 0.00082149
Iteration 4/25 | Loss: 0.00082149
Iteration 5/25 | Loss: 0.00082149
Iteration 6/25 | Loss: 0.00082149
Iteration 7/25 | Loss: 0.00082149
Iteration 8/25 | Loss: 0.00082149
Iteration 9/25 | Loss: 0.00082149
Iteration 10/25 | Loss: 0.00082149
Iteration 11/25 | Loss: 0.00082149
Iteration 12/25 | Loss: 0.00082149
Iteration 13/25 | Loss: 0.00082149
Iteration 14/25 | Loss: 0.00082149
Iteration 15/25 | Loss: 0.00082149
Iteration 16/25 | Loss: 0.00082149
Iteration 17/25 | Loss: 0.00082149
Iteration 18/25 | Loss: 0.00082149
Iteration 19/25 | Loss: 0.00082149
Iteration 20/25 | Loss: 0.00082149
Iteration 21/25 | Loss: 0.00082149
Iteration 22/25 | Loss: 0.00082149
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008214922272600234, 0.0008214922272600234, 0.0008214922272600234, 0.0008214922272600234, 0.0008214922272600234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008214922272600234

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082149
Iteration 2/1000 | Loss: 0.00006512
Iteration 3/1000 | Loss: 0.00004543
Iteration 4/1000 | Loss: 0.00003745
Iteration 5/1000 | Loss: 0.00003328
Iteration 6/1000 | Loss: 0.00020503
Iteration 7/1000 | Loss: 0.00017936
Iteration 8/1000 | Loss: 0.00020306
Iteration 9/1000 | Loss: 0.00014720
Iteration 10/1000 | Loss: 0.00016601
Iteration 11/1000 | Loss: 0.00014610
Iteration 12/1000 | Loss: 0.00013145
Iteration 13/1000 | Loss: 0.00003729
Iteration 14/1000 | Loss: 0.00003160
Iteration 15/1000 | Loss: 0.00002940
Iteration 16/1000 | Loss: 0.00004614
Iteration 17/1000 | Loss: 0.00003847
Iteration 18/1000 | Loss: 0.00004787
Iteration 19/1000 | Loss: 0.00004746
Iteration 20/1000 | Loss: 0.00003078
Iteration 21/1000 | Loss: 0.00005237
Iteration 22/1000 | Loss: 0.00005102
Iteration 23/1000 | Loss: 0.00005269
Iteration 24/1000 | Loss: 0.00005164
Iteration 25/1000 | Loss: 0.00005009
Iteration 26/1000 | Loss: 0.00002957
Iteration 27/1000 | Loss: 0.00002761
Iteration 28/1000 | Loss: 0.00002661
Iteration 29/1000 | Loss: 0.00002613
Iteration 30/1000 | Loss: 0.00002580
Iteration 31/1000 | Loss: 0.00002561
Iteration 32/1000 | Loss: 0.00002543
Iteration 33/1000 | Loss: 0.00002542
Iteration 34/1000 | Loss: 0.00002529
Iteration 35/1000 | Loss: 0.00002528
Iteration 36/1000 | Loss: 0.00002523
Iteration 37/1000 | Loss: 0.00002505
Iteration 38/1000 | Loss: 0.00018909
Iteration 39/1000 | Loss: 0.00007170
Iteration 40/1000 | Loss: 0.00002525
Iteration 41/1000 | Loss: 0.00018464
Iteration 42/1000 | Loss: 0.00005470
Iteration 43/1000 | Loss: 0.00016846
Iteration 44/1000 | Loss: 0.00006327
Iteration 45/1000 | Loss: 0.00022615
Iteration 46/1000 | Loss: 0.00010514
Iteration 47/1000 | Loss: 0.00005131
Iteration 48/1000 | Loss: 0.00003801
Iteration 49/1000 | Loss: 0.00003429
Iteration 50/1000 | Loss: 0.00003195
Iteration 51/1000 | Loss: 0.00003053
Iteration 52/1000 | Loss: 0.00002910
Iteration 53/1000 | Loss: 0.00002803
Iteration 54/1000 | Loss: 0.00011192
Iteration 55/1000 | Loss: 0.00005703
Iteration 56/1000 | Loss: 0.00020821
Iteration 57/1000 | Loss: 0.00005952
Iteration 58/1000 | Loss: 0.00003123
Iteration 59/1000 | Loss: 0.00002901
Iteration 60/1000 | Loss: 0.00012103
Iteration 61/1000 | Loss: 0.00013350
Iteration 62/1000 | Loss: 0.00011883
Iteration 63/1000 | Loss: 0.00003082
Iteration 64/1000 | Loss: 0.00011603
Iteration 65/1000 | Loss: 0.00012145
Iteration 66/1000 | Loss: 0.00012489
Iteration 67/1000 | Loss: 0.00013986
Iteration 68/1000 | Loss: 0.00014185
Iteration 69/1000 | Loss: 0.00010355
Iteration 70/1000 | Loss: 0.00003356
Iteration 71/1000 | Loss: 0.00002771
Iteration 72/1000 | Loss: 0.00002606
Iteration 73/1000 | Loss: 0.00002545
Iteration 74/1000 | Loss: 0.00002513
Iteration 75/1000 | Loss: 0.00002479
Iteration 76/1000 | Loss: 0.00002468
Iteration 77/1000 | Loss: 0.00002465
Iteration 78/1000 | Loss: 0.00002464
Iteration 79/1000 | Loss: 0.00002464
Iteration 80/1000 | Loss: 0.00002463
Iteration 81/1000 | Loss: 0.00002463
Iteration 82/1000 | Loss: 0.00002462
Iteration 83/1000 | Loss: 0.00002462
Iteration 84/1000 | Loss: 0.00002462
Iteration 85/1000 | Loss: 0.00002461
Iteration 86/1000 | Loss: 0.00002461
Iteration 87/1000 | Loss: 0.00002461
Iteration 88/1000 | Loss: 0.00002461
Iteration 89/1000 | Loss: 0.00002461
Iteration 90/1000 | Loss: 0.00002460
Iteration 91/1000 | Loss: 0.00002460
Iteration 92/1000 | Loss: 0.00002460
Iteration 93/1000 | Loss: 0.00002460
Iteration 94/1000 | Loss: 0.00002460
Iteration 95/1000 | Loss: 0.00002460
Iteration 96/1000 | Loss: 0.00002459
Iteration 97/1000 | Loss: 0.00002459
Iteration 98/1000 | Loss: 0.00002459
Iteration 99/1000 | Loss: 0.00002459
Iteration 100/1000 | Loss: 0.00002458
Iteration 101/1000 | Loss: 0.00002458
Iteration 102/1000 | Loss: 0.00002458
Iteration 103/1000 | Loss: 0.00002458
Iteration 104/1000 | Loss: 0.00002458
Iteration 105/1000 | Loss: 0.00002458
Iteration 106/1000 | Loss: 0.00002458
Iteration 107/1000 | Loss: 0.00002458
Iteration 108/1000 | Loss: 0.00002458
Iteration 109/1000 | Loss: 0.00002458
Iteration 110/1000 | Loss: 0.00002458
Iteration 111/1000 | Loss: 0.00002458
Iteration 112/1000 | Loss: 0.00002458
Iteration 113/1000 | Loss: 0.00002458
Iteration 114/1000 | Loss: 0.00002458
Iteration 115/1000 | Loss: 0.00002457
Iteration 116/1000 | Loss: 0.00002457
Iteration 117/1000 | Loss: 0.00002457
Iteration 118/1000 | Loss: 0.00002457
Iteration 119/1000 | Loss: 0.00002457
Iteration 120/1000 | Loss: 0.00002457
Iteration 121/1000 | Loss: 0.00002457
Iteration 122/1000 | Loss: 0.00002457
Iteration 123/1000 | Loss: 0.00002457
Iteration 124/1000 | Loss: 0.00002457
Iteration 125/1000 | Loss: 0.00002457
Iteration 126/1000 | Loss: 0.00002457
Iteration 127/1000 | Loss: 0.00002457
Iteration 128/1000 | Loss: 0.00002457
Iteration 129/1000 | Loss: 0.00002456
Iteration 130/1000 | Loss: 0.00002456
Iteration 131/1000 | Loss: 0.00002456
Iteration 132/1000 | Loss: 0.00002456
Iteration 133/1000 | Loss: 0.00002456
Iteration 134/1000 | Loss: 0.00002456
Iteration 135/1000 | Loss: 0.00002456
Iteration 136/1000 | Loss: 0.00002456
Iteration 137/1000 | Loss: 0.00002456
Iteration 138/1000 | Loss: 0.00002456
Iteration 139/1000 | Loss: 0.00002456
Iteration 140/1000 | Loss: 0.00002456
Iteration 141/1000 | Loss: 0.00002455
Iteration 142/1000 | Loss: 0.00002455
Iteration 143/1000 | Loss: 0.00002455
Iteration 144/1000 | Loss: 0.00002455
Iteration 145/1000 | Loss: 0.00002455
Iteration 146/1000 | Loss: 0.00002455
Iteration 147/1000 | Loss: 0.00002455
Iteration 148/1000 | Loss: 0.00002455
Iteration 149/1000 | Loss: 0.00002455
Iteration 150/1000 | Loss: 0.00002455
Iteration 151/1000 | Loss: 0.00002455
Iteration 152/1000 | Loss: 0.00002455
Iteration 153/1000 | Loss: 0.00002455
Iteration 154/1000 | Loss: 0.00002455
Iteration 155/1000 | Loss: 0.00002454
Iteration 156/1000 | Loss: 0.00002454
Iteration 157/1000 | Loss: 0.00002454
Iteration 158/1000 | Loss: 0.00002454
Iteration 159/1000 | Loss: 0.00002454
Iteration 160/1000 | Loss: 0.00002454
Iteration 161/1000 | Loss: 0.00002454
Iteration 162/1000 | Loss: 0.00002454
Iteration 163/1000 | Loss: 0.00002454
Iteration 164/1000 | Loss: 0.00002454
Iteration 165/1000 | Loss: 0.00002454
Iteration 166/1000 | Loss: 0.00002454
Iteration 167/1000 | Loss: 0.00002454
Iteration 168/1000 | Loss: 0.00002454
Iteration 169/1000 | Loss: 0.00002454
Iteration 170/1000 | Loss: 0.00002454
Iteration 171/1000 | Loss: 0.00002454
Iteration 172/1000 | Loss: 0.00002454
Iteration 173/1000 | Loss: 0.00002454
Iteration 174/1000 | Loss: 0.00002454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.4544735424569808e-05, 2.4544735424569808e-05, 2.4544735424569808e-05, 2.4544735424569808e-05, 2.4544735424569808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4544735424569808e-05

Optimization complete. Final v2v error: 4.178955078125 mm

Highest mean error: 8.32577896118164 mm for frame 88

Lowest mean error: 3.780015468597412 mm for frame 83

Saving results

Total time: 165.49211311340332
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01128622
Iteration 2/25 | Loss: 0.00233767
Iteration 3/25 | Loss: 0.00118551
Iteration 4/25 | Loss: 0.00108763
Iteration 5/25 | Loss: 0.00105029
Iteration 6/25 | Loss: 0.00099612
Iteration 7/25 | Loss: 0.00095148
Iteration 8/25 | Loss: 0.00092506
Iteration 9/25 | Loss: 0.00090652
Iteration 10/25 | Loss: 0.00088546
Iteration 11/25 | Loss: 0.00088797
Iteration 12/25 | Loss: 0.00087406
Iteration 13/25 | Loss: 0.00087317
Iteration 14/25 | Loss: 0.00087293
Iteration 15/25 | Loss: 0.00087459
Iteration 16/25 | Loss: 0.00087282
Iteration 17/25 | Loss: 0.00087282
Iteration 18/25 | Loss: 0.00087282
Iteration 19/25 | Loss: 0.00087282
Iteration 20/25 | Loss: 0.00087282
Iteration 21/25 | Loss: 0.00087282
Iteration 22/25 | Loss: 0.00087282
Iteration 23/25 | Loss: 0.00087282
Iteration 24/25 | Loss: 0.00087282
Iteration 25/25 | Loss: 0.00087282

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36965108
Iteration 2/25 | Loss: 0.00083062
Iteration 3/25 | Loss: 0.00083062
Iteration 4/25 | Loss: 0.00083062
Iteration 5/25 | Loss: 0.00083062
Iteration 6/25 | Loss: 0.00083062
Iteration 7/25 | Loss: 0.00083062
Iteration 8/25 | Loss: 0.00083062
Iteration 9/25 | Loss: 0.00083062
Iteration 10/25 | Loss: 0.00083062
Iteration 11/25 | Loss: 0.00083062
Iteration 12/25 | Loss: 0.00083062
Iteration 13/25 | Loss: 0.00083062
Iteration 14/25 | Loss: 0.00083062
Iteration 15/25 | Loss: 0.00083062
Iteration 16/25 | Loss: 0.00083062
Iteration 17/25 | Loss: 0.00083062
Iteration 18/25 | Loss: 0.00083062
Iteration 19/25 | Loss: 0.00083062
Iteration 20/25 | Loss: 0.00083062
Iteration 21/25 | Loss: 0.00083062
Iteration 22/25 | Loss: 0.00083062
Iteration 23/25 | Loss: 0.00083062
Iteration 24/25 | Loss: 0.00083062
Iteration 25/25 | Loss: 0.00083062

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083062
Iteration 2/1000 | Loss: 0.00002416
Iteration 3/1000 | Loss: 0.00001557
Iteration 4/1000 | Loss: 0.00001296
Iteration 5/1000 | Loss: 0.00001208
Iteration 6/1000 | Loss: 0.00001680
Iteration 7/1000 | Loss: 0.00001148
Iteration 8/1000 | Loss: 0.00001135
Iteration 9/1000 | Loss: 0.00001133
Iteration 10/1000 | Loss: 0.00001132
Iteration 11/1000 | Loss: 0.00001380
Iteration 12/1000 | Loss: 0.00001461
Iteration 13/1000 | Loss: 0.00001112
Iteration 14/1000 | Loss: 0.00001111
Iteration 15/1000 | Loss: 0.00001109
Iteration 16/1000 | Loss: 0.00001102
Iteration 17/1000 | Loss: 0.00001102
Iteration 18/1000 | Loss: 0.00001101
Iteration 19/1000 | Loss: 0.00001101
Iteration 20/1000 | Loss: 0.00001097
Iteration 21/1000 | Loss: 0.00001095
Iteration 22/1000 | Loss: 0.00001094
Iteration 23/1000 | Loss: 0.00001341
Iteration 24/1000 | Loss: 0.00001282
Iteration 25/1000 | Loss: 0.00001089
Iteration 26/1000 | Loss: 0.00001088
Iteration 27/1000 | Loss: 0.00001088
Iteration 28/1000 | Loss: 0.00001088
Iteration 29/1000 | Loss: 0.00001088
Iteration 30/1000 | Loss: 0.00001088
Iteration 31/1000 | Loss: 0.00001088
Iteration 32/1000 | Loss: 0.00001088
Iteration 33/1000 | Loss: 0.00001088
Iteration 34/1000 | Loss: 0.00001088
Iteration 35/1000 | Loss: 0.00001088
Iteration 36/1000 | Loss: 0.00001093
Iteration 37/1000 | Loss: 0.00001087
Iteration 38/1000 | Loss: 0.00001087
Iteration 39/1000 | Loss: 0.00001087
Iteration 40/1000 | Loss: 0.00001087
Iteration 41/1000 | Loss: 0.00001087
Iteration 42/1000 | Loss: 0.00001087
Iteration 43/1000 | Loss: 0.00001087
Iteration 44/1000 | Loss: 0.00001087
Iteration 45/1000 | Loss: 0.00001087
Iteration 46/1000 | Loss: 0.00001087
Iteration 47/1000 | Loss: 0.00001087
Iteration 48/1000 | Loss: 0.00001087
Iteration 49/1000 | Loss: 0.00001087
Iteration 50/1000 | Loss: 0.00001087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 50. Stopping optimization.
Last 5 losses: [1.0865107469726354e-05, 1.0865107469726354e-05, 1.0865107469726354e-05, 1.0865107469726354e-05, 1.0865107469726354e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0865107469726354e-05

Optimization complete. Final v2v error: 2.9390382766723633 mm

Highest mean error: 3.26053524017334 mm for frame 0

Lowest mean error: 2.802563428878784 mm for frame 87

Saving results

Total time: 46.08060145378113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00368196
Iteration 2/25 | Loss: 0.00113159
Iteration 3/25 | Loss: 0.00100810
Iteration 4/25 | Loss: 0.00098175
Iteration 5/25 | Loss: 0.00097053
Iteration 6/25 | Loss: 0.00096738
Iteration 7/25 | Loss: 0.00096652
Iteration 8/25 | Loss: 0.00096626
Iteration 9/25 | Loss: 0.00096626
Iteration 10/25 | Loss: 0.00096626
Iteration 11/25 | Loss: 0.00096626
Iteration 12/25 | Loss: 0.00096626
Iteration 13/25 | Loss: 0.00096626
Iteration 14/25 | Loss: 0.00096626
Iteration 15/25 | Loss: 0.00096626
Iteration 16/25 | Loss: 0.00096626
Iteration 17/25 | Loss: 0.00096626
Iteration 18/25 | Loss: 0.00096626
Iteration 19/25 | Loss: 0.00096626
Iteration 20/25 | Loss: 0.00096626
Iteration 21/25 | Loss: 0.00096626
Iteration 22/25 | Loss: 0.00096626
Iteration 23/25 | Loss: 0.00096626
Iteration 24/25 | Loss: 0.00096626
Iteration 25/25 | Loss: 0.00096626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28671980
Iteration 2/25 | Loss: 0.00093493
Iteration 3/25 | Loss: 0.00093493
Iteration 4/25 | Loss: 0.00093493
Iteration 5/25 | Loss: 0.00093493
Iteration 6/25 | Loss: 0.00093493
Iteration 7/25 | Loss: 0.00093493
Iteration 8/25 | Loss: 0.00093493
Iteration 9/25 | Loss: 0.00093493
Iteration 10/25 | Loss: 0.00093493
Iteration 11/25 | Loss: 0.00093493
Iteration 12/25 | Loss: 0.00093493
Iteration 13/25 | Loss: 0.00093493
Iteration 14/25 | Loss: 0.00093493
Iteration 15/25 | Loss: 0.00093493
Iteration 16/25 | Loss: 0.00093493
Iteration 17/25 | Loss: 0.00093493
Iteration 18/25 | Loss: 0.00093493
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009349308675155044, 0.0009349308675155044, 0.0009349308675155044, 0.0009349308675155044, 0.0009349308675155044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009349308675155044

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093493
Iteration 2/1000 | Loss: 0.00005657
Iteration 3/1000 | Loss: 0.00003343
Iteration 4/1000 | Loss: 0.00002456
Iteration 5/1000 | Loss: 0.00002144
Iteration 6/1000 | Loss: 0.00002023
Iteration 7/1000 | Loss: 0.00001930
Iteration 8/1000 | Loss: 0.00001879
Iteration 9/1000 | Loss: 0.00001840
Iteration 10/1000 | Loss: 0.00001796
Iteration 11/1000 | Loss: 0.00001767
Iteration 12/1000 | Loss: 0.00001767
Iteration 13/1000 | Loss: 0.00001749
Iteration 14/1000 | Loss: 0.00001738
Iteration 15/1000 | Loss: 0.00001733
Iteration 16/1000 | Loss: 0.00001732
Iteration 17/1000 | Loss: 0.00001730
Iteration 18/1000 | Loss: 0.00001729
Iteration 19/1000 | Loss: 0.00001724
Iteration 20/1000 | Loss: 0.00001720
Iteration 21/1000 | Loss: 0.00001719
Iteration 22/1000 | Loss: 0.00001718
Iteration 23/1000 | Loss: 0.00001718
Iteration 24/1000 | Loss: 0.00001717
Iteration 25/1000 | Loss: 0.00001717
Iteration 26/1000 | Loss: 0.00001715
Iteration 27/1000 | Loss: 0.00001715
Iteration 28/1000 | Loss: 0.00001711
Iteration 29/1000 | Loss: 0.00001709
Iteration 30/1000 | Loss: 0.00001708
Iteration 31/1000 | Loss: 0.00001708
Iteration 32/1000 | Loss: 0.00001708
Iteration 33/1000 | Loss: 0.00001707
Iteration 34/1000 | Loss: 0.00001707
Iteration 35/1000 | Loss: 0.00001706
Iteration 36/1000 | Loss: 0.00001705
Iteration 37/1000 | Loss: 0.00001704
Iteration 38/1000 | Loss: 0.00001703
Iteration 39/1000 | Loss: 0.00001703
Iteration 40/1000 | Loss: 0.00001702
Iteration 41/1000 | Loss: 0.00001701
Iteration 42/1000 | Loss: 0.00001701
Iteration 43/1000 | Loss: 0.00001700
Iteration 44/1000 | Loss: 0.00001700
Iteration 45/1000 | Loss: 0.00001699
Iteration 46/1000 | Loss: 0.00001699
Iteration 47/1000 | Loss: 0.00001698
Iteration 48/1000 | Loss: 0.00001697
Iteration 49/1000 | Loss: 0.00001696
Iteration 50/1000 | Loss: 0.00001696
Iteration 51/1000 | Loss: 0.00001693
Iteration 52/1000 | Loss: 0.00001693
Iteration 53/1000 | Loss: 0.00001692
Iteration 54/1000 | Loss: 0.00001692
Iteration 55/1000 | Loss: 0.00001691
Iteration 56/1000 | Loss: 0.00001691
Iteration 57/1000 | Loss: 0.00001691
Iteration 58/1000 | Loss: 0.00001690
Iteration 59/1000 | Loss: 0.00001690
Iteration 60/1000 | Loss: 0.00001690
Iteration 61/1000 | Loss: 0.00001690
Iteration 62/1000 | Loss: 0.00001689
Iteration 63/1000 | Loss: 0.00001689
Iteration 64/1000 | Loss: 0.00001689
Iteration 65/1000 | Loss: 0.00001688
Iteration 66/1000 | Loss: 0.00001688
Iteration 67/1000 | Loss: 0.00001688
Iteration 68/1000 | Loss: 0.00001688
Iteration 69/1000 | Loss: 0.00001687
Iteration 70/1000 | Loss: 0.00001687
Iteration 71/1000 | Loss: 0.00001687
Iteration 72/1000 | Loss: 0.00001686
Iteration 73/1000 | Loss: 0.00001686
Iteration 74/1000 | Loss: 0.00001686
Iteration 75/1000 | Loss: 0.00001686
Iteration 76/1000 | Loss: 0.00001686
Iteration 77/1000 | Loss: 0.00001686
Iteration 78/1000 | Loss: 0.00001685
Iteration 79/1000 | Loss: 0.00001685
Iteration 80/1000 | Loss: 0.00001685
Iteration 81/1000 | Loss: 0.00001685
Iteration 82/1000 | Loss: 0.00001685
Iteration 83/1000 | Loss: 0.00001685
Iteration 84/1000 | Loss: 0.00001685
Iteration 85/1000 | Loss: 0.00001685
Iteration 86/1000 | Loss: 0.00001685
Iteration 87/1000 | Loss: 0.00001684
Iteration 88/1000 | Loss: 0.00001684
Iteration 89/1000 | Loss: 0.00001684
Iteration 90/1000 | Loss: 0.00001684
Iteration 91/1000 | Loss: 0.00001684
Iteration 92/1000 | Loss: 0.00001684
Iteration 93/1000 | Loss: 0.00001684
Iteration 94/1000 | Loss: 0.00001684
Iteration 95/1000 | Loss: 0.00001684
Iteration 96/1000 | Loss: 0.00001684
Iteration 97/1000 | Loss: 0.00001684
Iteration 98/1000 | Loss: 0.00001684
Iteration 99/1000 | Loss: 0.00001683
Iteration 100/1000 | Loss: 0.00001683
Iteration 101/1000 | Loss: 0.00001683
Iteration 102/1000 | Loss: 0.00001683
Iteration 103/1000 | Loss: 0.00001683
Iteration 104/1000 | Loss: 0.00001683
Iteration 105/1000 | Loss: 0.00001683
Iteration 106/1000 | Loss: 0.00001683
Iteration 107/1000 | Loss: 0.00001683
Iteration 108/1000 | Loss: 0.00001683
Iteration 109/1000 | Loss: 0.00001683
Iteration 110/1000 | Loss: 0.00001683
Iteration 111/1000 | Loss: 0.00001683
Iteration 112/1000 | Loss: 0.00001683
Iteration 113/1000 | Loss: 0.00001683
Iteration 114/1000 | Loss: 0.00001683
Iteration 115/1000 | Loss: 0.00001683
Iteration 116/1000 | Loss: 0.00001683
Iteration 117/1000 | Loss: 0.00001683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.6825208149384707e-05, 1.6825208149384707e-05, 1.6825208149384707e-05, 1.6825208149384707e-05, 1.6825208149384707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6825208149384707e-05

Optimization complete. Final v2v error: 3.5066170692443848 mm

Highest mean error: 4.005821228027344 mm for frame 31

Lowest mean error: 2.698529005050659 mm for frame 115

Saving results

Total time: 44.66395330429077
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061141
Iteration 2/25 | Loss: 0.00266207
Iteration 3/25 | Loss: 0.00207224
Iteration 4/25 | Loss: 0.00168661
Iteration 5/25 | Loss: 0.00151643
Iteration 6/25 | Loss: 0.00140047
Iteration 7/25 | Loss: 0.00128572
Iteration 8/25 | Loss: 0.00122724
Iteration 9/25 | Loss: 0.00114834
Iteration 10/25 | Loss: 0.00111041
Iteration 11/25 | Loss: 0.00110047
Iteration 12/25 | Loss: 0.00109745
Iteration 13/25 | Loss: 0.00108716
Iteration 14/25 | Loss: 0.00107873
Iteration 15/25 | Loss: 0.00106318
Iteration 16/25 | Loss: 0.00105685
Iteration 17/25 | Loss: 0.00105552
Iteration 18/25 | Loss: 0.00105485
Iteration 19/25 | Loss: 0.00105440
Iteration 20/25 | Loss: 0.00105357
Iteration 21/25 | Loss: 0.00105440
Iteration 22/25 | Loss: 0.00106041
Iteration 23/25 | Loss: 0.00104918
Iteration 24/25 | Loss: 0.00104450
Iteration 25/25 | Loss: 0.00104331

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49626482
Iteration 2/25 | Loss: 0.00349423
Iteration 3/25 | Loss: 0.00216659
Iteration 4/25 | Loss: 0.00216659
Iteration 5/25 | Loss: 0.00216659
Iteration 6/25 | Loss: 0.00216659
Iteration 7/25 | Loss: 0.00216659
Iteration 8/25 | Loss: 0.00216659
Iteration 9/25 | Loss: 0.00216658
Iteration 10/25 | Loss: 0.00216658
Iteration 11/25 | Loss: 0.00216658
Iteration 12/25 | Loss: 0.00216658
Iteration 13/25 | Loss: 0.00216658
Iteration 14/25 | Loss: 0.00216658
Iteration 15/25 | Loss: 0.00216658
Iteration 16/25 | Loss: 0.00216658
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021665843669325113, 0.0021665843669325113, 0.0021665843669325113, 0.0021665843669325113, 0.0021665843669325113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021665843669325113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216658
Iteration 2/1000 | Loss: 0.00126697
Iteration 3/1000 | Loss: 0.00039023
Iteration 4/1000 | Loss: 0.00078809
Iteration 5/1000 | Loss: 0.00140940
Iteration 6/1000 | Loss: 0.00397155
Iteration 7/1000 | Loss: 0.00133923
Iteration 8/1000 | Loss: 0.00092272
Iteration 9/1000 | Loss: 0.00161499
Iteration 10/1000 | Loss: 0.00057740
Iteration 11/1000 | Loss: 0.00102217
Iteration 12/1000 | Loss: 0.00035423
Iteration 13/1000 | Loss: 0.00029646
Iteration 14/1000 | Loss: 0.00036648
Iteration 15/1000 | Loss: 0.00044740
Iteration 16/1000 | Loss: 0.00019604
Iteration 17/1000 | Loss: 0.00033531
Iteration 18/1000 | Loss: 0.00034214
Iteration 19/1000 | Loss: 0.00021119
Iteration 20/1000 | Loss: 0.00019359
Iteration 21/1000 | Loss: 0.00022295
Iteration 22/1000 | Loss: 0.00018365
Iteration 23/1000 | Loss: 0.00012011
Iteration 24/1000 | Loss: 0.00035081
Iteration 25/1000 | Loss: 0.00014007
Iteration 26/1000 | Loss: 0.00041191
Iteration 27/1000 | Loss: 0.00054476
Iteration 28/1000 | Loss: 0.00012127
Iteration 29/1000 | Loss: 0.00011062
Iteration 30/1000 | Loss: 0.00026310
Iteration 31/1000 | Loss: 0.00020341
Iteration 32/1000 | Loss: 0.00031043
Iteration 33/1000 | Loss: 0.00031698
Iteration 34/1000 | Loss: 0.00011432
Iteration 35/1000 | Loss: 0.00011195
Iteration 36/1000 | Loss: 0.00028546
Iteration 37/1000 | Loss: 0.00028359
Iteration 38/1000 | Loss: 0.00035050
Iteration 39/1000 | Loss: 0.00012491
Iteration 40/1000 | Loss: 0.00010654
Iteration 41/1000 | Loss: 0.00009902
Iteration 42/1000 | Loss: 0.00009363
Iteration 43/1000 | Loss: 0.00016103
Iteration 44/1000 | Loss: 0.00009302
Iteration 45/1000 | Loss: 0.00008947
Iteration 46/1000 | Loss: 0.00018817
Iteration 47/1000 | Loss: 0.00016365
Iteration 48/1000 | Loss: 0.00009430
Iteration 49/1000 | Loss: 0.00016095
Iteration 50/1000 | Loss: 0.00043113
Iteration 51/1000 | Loss: 0.00010752
Iteration 52/1000 | Loss: 0.00011514
Iteration 53/1000 | Loss: 0.00008828
Iteration 54/1000 | Loss: 0.00028952
Iteration 55/1000 | Loss: 0.00018165
Iteration 56/1000 | Loss: 0.00007910
Iteration 57/1000 | Loss: 0.00007799
Iteration 58/1000 | Loss: 0.00007692
Iteration 59/1000 | Loss: 0.00027071
Iteration 60/1000 | Loss: 0.00015939
Iteration 61/1000 | Loss: 0.00007621
Iteration 62/1000 | Loss: 0.00007713
Iteration 63/1000 | Loss: 0.00007587
Iteration 64/1000 | Loss: 0.00007606
Iteration 65/1000 | Loss: 0.00026083
Iteration 66/1000 | Loss: 0.00011863
Iteration 67/1000 | Loss: 0.00007691
Iteration 68/1000 | Loss: 0.00007609
Iteration 69/1000 | Loss: 0.00007589
Iteration 70/1000 | Loss: 0.00007585
Iteration 71/1000 | Loss: 0.00007574
Iteration 72/1000 | Loss: 0.00007545
Iteration 73/1000 | Loss: 0.00007544
Iteration 74/1000 | Loss: 0.00007576
Iteration 75/1000 | Loss: 0.00007583
Iteration 76/1000 | Loss: 0.00007583
Iteration 77/1000 | Loss: 0.00007595
Iteration 78/1000 | Loss: 0.00007602
Iteration 79/1000 | Loss: 0.00007620
Iteration 80/1000 | Loss: 0.00007605
Iteration 81/1000 | Loss: 0.00007621
Iteration 82/1000 | Loss: 0.00007627
Iteration 83/1000 | Loss: 0.00007628
Iteration 84/1000 | Loss: 0.00007628
Iteration 85/1000 | Loss: 0.00007613
Iteration 86/1000 | Loss: 0.00007614
Iteration 87/1000 | Loss: 0.00007552
Iteration 88/1000 | Loss: 0.00007582
Iteration 89/1000 | Loss: 0.00007581
Iteration 90/1000 | Loss: 0.00007604
Iteration 91/1000 | Loss: 0.00007595
Iteration 92/1000 | Loss: 0.00007614
Iteration 93/1000 | Loss: 0.00007612
Iteration 94/1000 | Loss: 0.00007602
Iteration 95/1000 | Loss: 0.00007625
Iteration 96/1000 | Loss: 0.00007624
Iteration 97/1000 | Loss: 0.00007539
Iteration 98/1000 | Loss: 0.00007589
Iteration 99/1000 | Loss: 0.00007571
Iteration 100/1000 | Loss: 0.00007529
Iteration 101/1000 | Loss: 0.00007510
Iteration 102/1000 | Loss: 0.00007493
Iteration 103/1000 | Loss: 0.00007492
Iteration 104/1000 | Loss: 0.00007491
Iteration 105/1000 | Loss: 0.00007509
Iteration 106/1000 | Loss: 0.00007496
Iteration 107/1000 | Loss: 0.00007474
Iteration 108/1000 | Loss: 0.00007464
Iteration 109/1000 | Loss: 0.00019217
Iteration 110/1000 | Loss: 0.00008051
Iteration 111/1000 | Loss: 0.00007634
Iteration 112/1000 | Loss: 0.00007478
Iteration 113/1000 | Loss: 0.00007432
Iteration 114/1000 | Loss: 0.00007351
Iteration 115/1000 | Loss: 0.00007318
Iteration 116/1000 | Loss: 0.00007329
Iteration 117/1000 | Loss: 0.00007329
Iteration 118/1000 | Loss: 0.00007486
Iteration 119/1000 | Loss: 0.00007328
Iteration 120/1000 | Loss: 0.00007393
Iteration 121/1000 | Loss: 0.00007414
Iteration 122/1000 | Loss: 0.00007356
Iteration 123/1000 | Loss: 0.00007293
Iteration 124/1000 | Loss: 0.00007292
Iteration 125/1000 | Loss: 0.00007292
Iteration 126/1000 | Loss: 0.00007292
Iteration 127/1000 | Loss: 0.00007292
Iteration 128/1000 | Loss: 0.00007292
Iteration 129/1000 | Loss: 0.00007292
Iteration 130/1000 | Loss: 0.00007292
Iteration 131/1000 | Loss: 0.00007292
Iteration 132/1000 | Loss: 0.00007292
Iteration 133/1000 | Loss: 0.00007292
Iteration 134/1000 | Loss: 0.00007292
Iteration 135/1000 | Loss: 0.00007292
Iteration 136/1000 | Loss: 0.00007292
Iteration 137/1000 | Loss: 0.00007292
Iteration 138/1000 | Loss: 0.00007292
Iteration 139/1000 | Loss: 0.00007292
Iteration 140/1000 | Loss: 0.00007291
Iteration 141/1000 | Loss: 0.00007291
Iteration 142/1000 | Loss: 0.00007291
Iteration 143/1000 | Loss: 0.00007291
Iteration 144/1000 | Loss: 0.00007291
Iteration 145/1000 | Loss: 0.00007291
Iteration 146/1000 | Loss: 0.00007291
Iteration 147/1000 | Loss: 0.00007291
Iteration 148/1000 | Loss: 0.00007291
Iteration 149/1000 | Loss: 0.00007291
Iteration 150/1000 | Loss: 0.00007290
Iteration 151/1000 | Loss: 0.00007290
Iteration 152/1000 | Loss: 0.00007290
Iteration 153/1000 | Loss: 0.00007290
Iteration 154/1000 | Loss: 0.00007290
Iteration 155/1000 | Loss: 0.00007290
Iteration 156/1000 | Loss: 0.00007290
Iteration 157/1000 | Loss: 0.00007290
Iteration 158/1000 | Loss: 0.00007290
Iteration 159/1000 | Loss: 0.00007290
Iteration 160/1000 | Loss: 0.00007290
Iteration 161/1000 | Loss: 0.00007290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [7.290361827472225e-05, 7.290361827472225e-05, 7.290361827472225e-05, 7.290361827472225e-05, 7.290361827472225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.290361827472225e-05

Optimization complete. Final v2v error: 4.868722438812256 mm

Highest mean error: 22.50905990600586 mm for frame 83

Lowest mean error: 3.3080432415008545 mm for frame 24

Saving results

Total time: 235.26418542861938
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034476
Iteration 2/25 | Loss: 0.00226099
Iteration 3/25 | Loss: 0.00158493
Iteration 4/25 | Loss: 0.00141085
Iteration 5/25 | Loss: 0.00126831
Iteration 6/25 | Loss: 0.00125926
Iteration 7/25 | Loss: 0.00136705
Iteration 8/25 | Loss: 0.00129646
Iteration 9/25 | Loss: 0.00110487
Iteration 10/25 | Loss: 0.00097612
Iteration 11/25 | Loss: 0.00093687
Iteration 12/25 | Loss: 0.00091623
Iteration 13/25 | Loss: 0.00090113
Iteration 14/25 | Loss: 0.00090890
Iteration 15/25 | Loss: 0.00089873
Iteration 16/25 | Loss: 0.00089801
Iteration 17/25 | Loss: 0.00089905
Iteration 18/25 | Loss: 0.00089562
Iteration 19/25 | Loss: 0.00088463
Iteration 20/25 | Loss: 0.00087968
Iteration 21/25 | Loss: 0.00087710
Iteration 22/25 | Loss: 0.00087635
Iteration 23/25 | Loss: 0.00087604
Iteration 24/25 | Loss: 0.00087588
Iteration 25/25 | Loss: 0.00087572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31711328
Iteration 2/25 | Loss: 0.00070380
Iteration 3/25 | Loss: 0.00070380
Iteration 4/25 | Loss: 0.00070380
Iteration 5/25 | Loss: 0.00070380
Iteration 6/25 | Loss: 0.00070380
Iteration 7/25 | Loss: 0.00070380
Iteration 8/25 | Loss: 0.00070380
Iteration 9/25 | Loss: 0.00070380
Iteration 10/25 | Loss: 0.00070380
Iteration 11/25 | Loss: 0.00070380
Iteration 12/25 | Loss: 0.00070380
Iteration 13/25 | Loss: 0.00070380
Iteration 14/25 | Loss: 0.00070380
Iteration 15/25 | Loss: 0.00070380
Iteration 16/25 | Loss: 0.00070380
Iteration 17/25 | Loss: 0.00070380
Iteration 18/25 | Loss: 0.00070380
Iteration 19/25 | Loss: 0.00070380
Iteration 20/25 | Loss: 0.00070380
Iteration 21/25 | Loss: 0.00070380
Iteration 22/25 | Loss: 0.00070380
Iteration 23/25 | Loss: 0.00070380
Iteration 24/25 | Loss: 0.00070380
Iteration 25/25 | Loss: 0.00070380

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070380
Iteration 2/1000 | Loss: 0.00005079
Iteration 3/1000 | Loss: 0.00003887
Iteration 4/1000 | Loss: 0.00003376
Iteration 5/1000 | Loss: 0.00003191
Iteration 6/1000 | Loss: 0.00003076
Iteration 7/1000 | Loss: 0.00007237
Iteration 8/1000 | Loss: 0.00003031
Iteration 9/1000 | Loss: 0.00002940
Iteration 10/1000 | Loss: 0.00002921
Iteration 11/1000 | Loss: 0.00005184
Iteration 12/1000 | Loss: 0.00002909
Iteration 13/1000 | Loss: 0.00002897
Iteration 14/1000 | Loss: 0.00002897
Iteration 15/1000 | Loss: 0.00002897
Iteration 16/1000 | Loss: 0.00002896
Iteration 17/1000 | Loss: 0.00002892
Iteration 18/1000 | Loss: 0.00002892
Iteration 19/1000 | Loss: 0.00004496
Iteration 20/1000 | Loss: 0.00002890
Iteration 21/1000 | Loss: 0.00002886
Iteration 22/1000 | Loss: 0.00002886
Iteration 23/1000 | Loss: 0.00002886
Iteration 24/1000 | Loss: 0.00002886
Iteration 25/1000 | Loss: 0.00002885
Iteration 26/1000 | Loss: 0.00002884
Iteration 27/1000 | Loss: 0.00002884
Iteration 28/1000 | Loss: 0.00002883
Iteration 29/1000 | Loss: 0.00002882
Iteration 30/1000 | Loss: 0.00002879
Iteration 31/1000 | Loss: 0.00004242
Iteration 32/1000 | Loss: 0.00002879
Iteration 33/1000 | Loss: 0.00002871
Iteration 34/1000 | Loss: 0.00002871
Iteration 35/1000 | Loss: 0.00002871
Iteration 36/1000 | Loss: 0.00002870
Iteration 37/1000 | Loss: 0.00002870
Iteration 38/1000 | Loss: 0.00002870
Iteration 39/1000 | Loss: 0.00002869
Iteration 40/1000 | Loss: 0.00002869
Iteration 41/1000 | Loss: 0.00002869
Iteration 42/1000 | Loss: 0.00002869
Iteration 43/1000 | Loss: 0.00002869
Iteration 44/1000 | Loss: 0.00002869
Iteration 45/1000 | Loss: 0.00002869
Iteration 46/1000 | Loss: 0.00002868
Iteration 47/1000 | Loss: 0.00002868
Iteration 48/1000 | Loss: 0.00002868
Iteration 49/1000 | Loss: 0.00002868
Iteration 50/1000 | Loss: 0.00002868
Iteration 51/1000 | Loss: 0.00002868
Iteration 52/1000 | Loss: 0.00002868
Iteration 53/1000 | Loss: 0.00002868
Iteration 54/1000 | Loss: 0.00002868
Iteration 55/1000 | Loss: 0.00002868
Iteration 56/1000 | Loss: 0.00002867
Iteration 57/1000 | Loss: 0.00002867
Iteration 58/1000 | Loss: 0.00002866
Iteration 59/1000 | Loss: 0.00002866
Iteration 60/1000 | Loss: 0.00002866
Iteration 61/1000 | Loss: 0.00002865
Iteration 62/1000 | Loss: 0.00002865
Iteration 63/1000 | Loss: 0.00002865
Iteration 64/1000 | Loss: 0.00002865
Iteration 65/1000 | Loss: 0.00002865
Iteration 66/1000 | Loss: 0.00002865
Iteration 67/1000 | Loss: 0.00002865
Iteration 68/1000 | Loss: 0.00002864
Iteration 69/1000 | Loss: 0.00002864
Iteration 70/1000 | Loss: 0.00002864
Iteration 71/1000 | Loss: 0.00002864
Iteration 72/1000 | Loss: 0.00002864
Iteration 73/1000 | Loss: 0.00002863
Iteration 74/1000 | Loss: 0.00002863
Iteration 75/1000 | Loss: 0.00002862
Iteration 76/1000 | Loss: 0.00002862
Iteration 77/1000 | Loss: 0.00002862
Iteration 78/1000 | Loss: 0.00002861
Iteration 79/1000 | Loss: 0.00002861
Iteration 80/1000 | Loss: 0.00002861
Iteration 81/1000 | Loss: 0.00002860
Iteration 82/1000 | Loss: 0.00002860
Iteration 83/1000 | Loss: 0.00002860
Iteration 84/1000 | Loss: 0.00002860
Iteration 85/1000 | Loss: 0.00002860
Iteration 86/1000 | Loss: 0.00002860
Iteration 87/1000 | Loss: 0.00002859
Iteration 88/1000 | Loss: 0.00002859
Iteration 89/1000 | Loss: 0.00002859
Iteration 90/1000 | Loss: 0.00002859
Iteration 91/1000 | Loss: 0.00002858
Iteration 92/1000 | Loss: 0.00002858
Iteration 93/1000 | Loss: 0.00002858
Iteration 94/1000 | Loss: 0.00002858
Iteration 95/1000 | Loss: 0.00002858
Iteration 96/1000 | Loss: 0.00002857
Iteration 97/1000 | Loss: 0.00002855
Iteration 98/1000 | Loss: 0.00002855
Iteration 99/1000 | Loss: 0.00002855
Iteration 100/1000 | Loss: 0.00002854
Iteration 101/1000 | Loss: 0.00002853
Iteration 102/1000 | Loss: 0.00002852
Iteration 103/1000 | Loss: 0.00002851
Iteration 104/1000 | Loss: 0.00002847
Iteration 105/1000 | Loss: 0.00005757
Iteration 106/1000 | Loss: 0.00005562
Iteration 107/1000 | Loss: 0.00002855
Iteration 108/1000 | Loss: 0.00003315
Iteration 109/1000 | Loss: 0.00002894
Iteration 110/1000 | Loss: 0.00002844
Iteration 111/1000 | Loss: 0.00002819
Iteration 112/1000 | Loss: 0.00002816
Iteration 113/1000 | Loss: 0.00002810
Iteration 114/1000 | Loss: 0.00002809
Iteration 115/1000 | Loss: 0.00002808
Iteration 116/1000 | Loss: 0.00002808
Iteration 117/1000 | Loss: 0.00002807
Iteration 118/1000 | Loss: 0.00002807
Iteration 119/1000 | Loss: 0.00002806
Iteration 120/1000 | Loss: 0.00002806
Iteration 121/1000 | Loss: 0.00002805
Iteration 122/1000 | Loss: 0.00002801
Iteration 123/1000 | Loss: 0.00002801
Iteration 124/1000 | Loss: 0.00002800
Iteration 125/1000 | Loss: 0.00002800
Iteration 126/1000 | Loss: 0.00002800
Iteration 127/1000 | Loss: 0.00002799
Iteration 128/1000 | Loss: 0.00002799
Iteration 129/1000 | Loss: 0.00004217
Iteration 130/1000 | Loss: 0.00002803
Iteration 131/1000 | Loss: 0.00002795
Iteration 132/1000 | Loss: 0.00002794
Iteration 133/1000 | Loss: 0.00002794
Iteration 134/1000 | Loss: 0.00002792
Iteration 135/1000 | Loss: 0.00002792
Iteration 136/1000 | Loss: 0.00002792
Iteration 137/1000 | Loss: 0.00002792
Iteration 138/1000 | Loss: 0.00002792
Iteration 139/1000 | Loss: 0.00002792
Iteration 140/1000 | Loss: 0.00002792
Iteration 141/1000 | Loss: 0.00002792
Iteration 142/1000 | Loss: 0.00002792
Iteration 143/1000 | Loss: 0.00002792
Iteration 144/1000 | Loss: 0.00002792
Iteration 145/1000 | Loss: 0.00002791
Iteration 146/1000 | Loss: 0.00002791
Iteration 147/1000 | Loss: 0.00002791
Iteration 148/1000 | Loss: 0.00002791
Iteration 149/1000 | Loss: 0.00002791
Iteration 150/1000 | Loss: 0.00002791
Iteration 151/1000 | Loss: 0.00002791
Iteration 152/1000 | Loss: 0.00002791
Iteration 153/1000 | Loss: 0.00002791
Iteration 154/1000 | Loss: 0.00002790
Iteration 155/1000 | Loss: 0.00002790
Iteration 156/1000 | Loss: 0.00002790
Iteration 157/1000 | Loss: 0.00002790
Iteration 158/1000 | Loss: 0.00002790
Iteration 159/1000 | Loss: 0.00002790
Iteration 160/1000 | Loss: 0.00002790
Iteration 161/1000 | Loss: 0.00002790
Iteration 162/1000 | Loss: 0.00002790
Iteration 163/1000 | Loss: 0.00002789
Iteration 164/1000 | Loss: 0.00002789
Iteration 165/1000 | Loss: 0.00002789
Iteration 166/1000 | Loss: 0.00002789
Iteration 167/1000 | Loss: 0.00002789
Iteration 168/1000 | Loss: 0.00002789
Iteration 169/1000 | Loss: 0.00002789
Iteration 170/1000 | Loss: 0.00002789
Iteration 171/1000 | Loss: 0.00002789
Iteration 172/1000 | Loss: 0.00002789
Iteration 173/1000 | Loss: 0.00002789
Iteration 174/1000 | Loss: 0.00002788
Iteration 175/1000 | Loss: 0.00002788
Iteration 176/1000 | Loss: 0.00002788
Iteration 177/1000 | Loss: 0.00002788
Iteration 178/1000 | Loss: 0.00002788
Iteration 179/1000 | Loss: 0.00002788
Iteration 180/1000 | Loss: 0.00002788
Iteration 181/1000 | Loss: 0.00002788
Iteration 182/1000 | Loss: 0.00002787
Iteration 183/1000 | Loss: 0.00002787
Iteration 184/1000 | Loss: 0.00002787
Iteration 185/1000 | Loss: 0.00002787
Iteration 186/1000 | Loss: 0.00002787
Iteration 187/1000 | Loss: 0.00002787
Iteration 188/1000 | Loss: 0.00002787
Iteration 189/1000 | Loss: 0.00002787
Iteration 190/1000 | Loss: 0.00002787
Iteration 191/1000 | Loss: 0.00002787
Iteration 192/1000 | Loss: 0.00002787
Iteration 193/1000 | Loss: 0.00002787
Iteration 194/1000 | Loss: 0.00002787
Iteration 195/1000 | Loss: 0.00002787
Iteration 196/1000 | Loss: 0.00002786
Iteration 197/1000 | Loss: 0.00002786
Iteration 198/1000 | Loss: 0.00002786
Iteration 199/1000 | Loss: 0.00002786
Iteration 200/1000 | Loss: 0.00002786
Iteration 201/1000 | Loss: 0.00002786
Iteration 202/1000 | Loss: 0.00002786
Iteration 203/1000 | Loss: 0.00002786
Iteration 204/1000 | Loss: 0.00002786
Iteration 205/1000 | Loss: 0.00002786
Iteration 206/1000 | Loss: 0.00002786
Iteration 207/1000 | Loss: 0.00002786
Iteration 208/1000 | Loss: 0.00002786
Iteration 209/1000 | Loss: 0.00002786
Iteration 210/1000 | Loss: 0.00002785
Iteration 211/1000 | Loss: 0.00002785
Iteration 212/1000 | Loss: 0.00002785
Iteration 213/1000 | Loss: 0.00002785
Iteration 214/1000 | Loss: 0.00002785
Iteration 215/1000 | Loss: 0.00002785
Iteration 216/1000 | Loss: 0.00002785
Iteration 217/1000 | Loss: 0.00002785
Iteration 218/1000 | Loss: 0.00002785
Iteration 219/1000 | Loss: 0.00002785
Iteration 220/1000 | Loss: 0.00002785
Iteration 221/1000 | Loss: 0.00002785
Iteration 222/1000 | Loss: 0.00002785
Iteration 223/1000 | Loss: 0.00002785
Iteration 224/1000 | Loss: 0.00002785
Iteration 225/1000 | Loss: 0.00002785
Iteration 226/1000 | Loss: 0.00002784
Iteration 227/1000 | Loss: 0.00002784
Iteration 228/1000 | Loss: 0.00002784
Iteration 229/1000 | Loss: 0.00002784
Iteration 230/1000 | Loss: 0.00002784
Iteration 231/1000 | Loss: 0.00002784
Iteration 232/1000 | Loss: 0.00002784
Iteration 233/1000 | Loss: 0.00002784
Iteration 234/1000 | Loss: 0.00002784
Iteration 235/1000 | Loss: 0.00002784
Iteration 236/1000 | Loss: 0.00002784
Iteration 237/1000 | Loss: 0.00002784
Iteration 238/1000 | Loss: 0.00002784
Iteration 239/1000 | Loss: 0.00002784
Iteration 240/1000 | Loss: 0.00002784
Iteration 241/1000 | Loss: 0.00002784
Iteration 242/1000 | Loss: 0.00002784
Iteration 243/1000 | Loss: 0.00002784
Iteration 244/1000 | Loss: 0.00002784
Iteration 245/1000 | Loss: 0.00002784
Iteration 246/1000 | Loss: 0.00002784
Iteration 247/1000 | Loss: 0.00002784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [2.7843769203172997e-05, 2.7843769203172997e-05, 2.7843769203172997e-05, 2.7843769203172997e-05, 2.7843769203172997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7843769203172997e-05

Optimization complete. Final v2v error: 4.0163164138793945 mm

Highest mean error: 20.002899169921875 mm for frame 120

Lowest mean error: 3.43325138092041 mm for frame 4

Saving results

Total time: 109.61683917045593
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01108753
Iteration 2/25 | Loss: 0.00316392
Iteration 3/25 | Loss: 0.00179574
Iteration 4/25 | Loss: 0.00168784
Iteration 5/25 | Loss: 0.00132065
Iteration 6/25 | Loss: 0.00141816
Iteration 7/25 | Loss: 0.00150980
Iteration 8/25 | Loss: 0.00130756
Iteration 9/25 | Loss: 0.00114767
Iteration 10/25 | Loss: 0.00110471
Iteration 11/25 | Loss: 0.00107077
Iteration 12/25 | Loss: 0.00104402
Iteration 13/25 | Loss: 0.00099151
Iteration 14/25 | Loss: 0.00097927
Iteration 15/25 | Loss: 0.00096598
Iteration 16/25 | Loss: 0.00098006
Iteration 17/25 | Loss: 0.00097175
Iteration 18/25 | Loss: 0.00095760
Iteration 19/25 | Loss: 0.00095039
Iteration 20/25 | Loss: 0.00094454
Iteration 21/25 | Loss: 0.00094480
Iteration 22/25 | Loss: 0.00094308
Iteration 23/25 | Loss: 0.00094358
Iteration 24/25 | Loss: 0.00094397
Iteration 25/25 | Loss: 0.00094478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37244904
Iteration 2/25 | Loss: 0.00121952
Iteration 3/25 | Loss: 0.00103394
Iteration 4/25 | Loss: 0.00103394
Iteration 5/25 | Loss: 0.00103393
Iteration 6/25 | Loss: 0.00103393
Iteration 7/25 | Loss: 0.00103393
Iteration 8/25 | Loss: 0.00103393
Iteration 9/25 | Loss: 0.00103393
Iteration 10/25 | Loss: 0.00103393
Iteration 11/25 | Loss: 0.00103393
Iteration 12/25 | Loss: 0.00103393
Iteration 13/25 | Loss: 0.00103393
Iteration 14/25 | Loss: 0.00103393
Iteration 15/25 | Loss: 0.00103393
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010339327855035663, 0.0010339327855035663, 0.0010339327855035663, 0.0010339327855035663, 0.0010339327855035663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010339327855035663

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103393
Iteration 2/1000 | Loss: 0.00043361
Iteration 3/1000 | Loss: 0.00015673
Iteration 4/1000 | Loss: 0.00029056
Iteration 5/1000 | Loss: 0.00018335
Iteration 6/1000 | Loss: 0.00033313
Iteration 7/1000 | Loss: 0.00027027
Iteration 8/1000 | Loss: 0.00026022
Iteration 9/1000 | Loss: 0.00033519
Iteration 10/1000 | Loss: 0.00007710
Iteration 11/1000 | Loss: 0.00006172
Iteration 12/1000 | Loss: 0.00046510
Iteration 13/1000 | Loss: 0.00016073
Iteration 14/1000 | Loss: 0.00008768
Iteration 15/1000 | Loss: 0.00034395
Iteration 16/1000 | Loss: 0.00021209
Iteration 17/1000 | Loss: 0.00025925
Iteration 18/1000 | Loss: 0.00015760
Iteration 19/1000 | Loss: 0.00017266
Iteration 20/1000 | Loss: 0.00008890
Iteration 21/1000 | Loss: 0.00018647
Iteration 22/1000 | Loss: 0.00027364
Iteration 23/1000 | Loss: 0.00009846
Iteration 24/1000 | Loss: 0.00081953
Iteration 25/1000 | Loss: 0.00182864
Iteration 26/1000 | Loss: 0.00041839
Iteration 27/1000 | Loss: 0.00084230
Iteration 28/1000 | Loss: 0.00009906
Iteration 29/1000 | Loss: 0.00009197
Iteration 30/1000 | Loss: 0.00006267
Iteration 31/1000 | Loss: 0.00008309
Iteration 32/1000 | Loss: 0.00021635
Iteration 33/1000 | Loss: 0.00021536
Iteration 34/1000 | Loss: 0.00016182
Iteration 35/1000 | Loss: 0.00011441
Iteration 36/1000 | Loss: 0.00005114
Iteration 37/1000 | Loss: 0.00010864
Iteration 38/1000 | Loss: 0.00019107
Iteration 39/1000 | Loss: 0.00016272
Iteration 40/1000 | Loss: 0.00004246
Iteration 41/1000 | Loss: 0.00003670
Iteration 42/1000 | Loss: 0.00005285
Iteration 43/1000 | Loss: 0.00004733
Iteration 44/1000 | Loss: 0.00004765
Iteration 45/1000 | Loss: 0.00004349
Iteration 46/1000 | Loss: 0.00003169
Iteration 47/1000 | Loss: 0.00005341
Iteration 48/1000 | Loss: 0.00005020
Iteration 49/1000 | Loss: 0.00003403
Iteration 50/1000 | Loss: 0.00003031
Iteration 51/1000 | Loss: 0.00026353
Iteration 52/1000 | Loss: 0.00014097
Iteration 53/1000 | Loss: 0.00033547
Iteration 54/1000 | Loss: 0.00007202
Iteration 55/1000 | Loss: 0.00017878
Iteration 56/1000 | Loss: 0.00003659
Iteration 57/1000 | Loss: 0.00005995
Iteration 58/1000 | Loss: 0.00019876
Iteration 59/1000 | Loss: 0.00003423
Iteration 60/1000 | Loss: 0.00002945
Iteration 61/1000 | Loss: 0.00002981
Iteration 62/1000 | Loss: 0.00003835
Iteration 63/1000 | Loss: 0.00003563
Iteration 64/1000 | Loss: 0.00003545
Iteration 65/1000 | Loss: 0.00002571
Iteration 66/1000 | Loss: 0.00002460
Iteration 67/1000 | Loss: 0.00002360
Iteration 68/1000 | Loss: 0.00011174
Iteration 69/1000 | Loss: 0.00026363
Iteration 70/1000 | Loss: 0.00012728
Iteration 71/1000 | Loss: 0.00033870
Iteration 72/1000 | Loss: 0.00005771
Iteration 73/1000 | Loss: 0.00002382
Iteration 74/1000 | Loss: 0.00002205
Iteration 75/1000 | Loss: 0.00002087
Iteration 76/1000 | Loss: 0.00002045
Iteration 77/1000 | Loss: 0.00002008
Iteration 78/1000 | Loss: 0.00001973
Iteration 79/1000 | Loss: 0.00001954
Iteration 80/1000 | Loss: 0.00001942
Iteration 81/1000 | Loss: 0.00001933
Iteration 82/1000 | Loss: 0.00001928
Iteration 83/1000 | Loss: 0.00001925
Iteration 84/1000 | Loss: 0.00001923
Iteration 85/1000 | Loss: 0.00001923
Iteration 86/1000 | Loss: 0.00001923
Iteration 87/1000 | Loss: 0.00008641
Iteration 88/1000 | Loss: 0.00001939
Iteration 89/1000 | Loss: 0.00001923
Iteration 90/1000 | Loss: 0.00001920
Iteration 91/1000 | Loss: 0.00001920
Iteration 92/1000 | Loss: 0.00001919
Iteration 93/1000 | Loss: 0.00001919
Iteration 94/1000 | Loss: 0.00001919
Iteration 95/1000 | Loss: 0.00001919
Iteration 96/1000 | Loss: 0.00001919
Iteration 97/1000 | Loss: 0.00001919
Iteration 98/1000 | Loss: 0.00001919
Iteration 99/1000 | Loss: 0.00001919
Iteration 100/1000 | Loss: 0.00001919
Iteration 101/1000 | Loss: 0.00001919
Iteration 102/1000 | Loss: 0.00001919
Iteration 103/1000 | Loss: 0.00001919
Iteration 104/1000 | Loss: 0.00001919
Iteration 105/1000 | Loss: 0.00001919
Iteration 106/1000 | Loss: 0.00001918
Iteration 107/1000 | Loss: 0.00001918
Iteration 108/1000 | Loss: 0.00001918
Iteration 109/1000 | Loss: 0.00001918
Iteration 110/1000 | Loss: 0.00001918
Iteration 111/1000 | Loss: 0.00001917
Iteration 112/1000 | Loss: 0.00001917
Iteration 113/1000 | Loss: 0.00001917
Iteration 114/1000 | Loss: 0.00001917
Iteration 115/1000 | Loss: 0.00001917
Iteration 116/1000 | Loss: 0.00001917
Iteration 117/1000 | Loss: 0.00001917
Iteration 118/1000 | Loss: 0.00001917
Iteration 119/1000 | Loss: 0.00001917
Iteration 120/1000 | Loss: 0.00001917
Iteration 121/1000 | Loss: 0.00001917
Iteration 122/1000 | Loss: 0.00001916
Iteration 123/1000 | Loss: 0.00001916
Iteration 124/1000 | Loss: 0.00001916
Iteration 125/1000 | Loss: 0.00001916
Iteration 126/1000 | Loss: 0.00001916
Iteration 127/1000 | Loss: 0.00001916
Iteration 128/1000 | Loss: 0.00001916
Iteration 129/1000 | Loss: 0.00001915
Iteration 130/1000 | Loss: 0.00001915
Iteration 131/1000 | Loss: 0.00001914
Iteration 132/1000 | Loss: 0.00001914
Iteration 133/1000 | Loss: 0.00001914
Iteration 134/1000 | Loss: 0.00001914
Iteration 135/1000 | Loss: 0.00001914
Iteration 136/1000 | Loss: 0.00001914
Iteration 137/1000 | Loss: 0.00001914
Iteration 138/1000 | Loss: 0.00001914
Iteration 139/1000 | Loss: 0.00001914
Iteration 140/1000 | Loss: 0.00001914
Iteration 141/1000 | Loss: 0.00001914
Iteration 142/1000 | Loss: 0.00001913
Iteration 143/1000 | Loss: 0.00001913
Iteration 144/1000 | Loss: 0.00001913
Iteration 145/1000 | Loss: 0.00001913
Iteration 146/1000 | Loss: 0.00001913
Iteration 147/1000 | Loss: 0.00001913
Iteration 148/1000 | Loss: 0.00001913
Iteration 149/1000 | Loss: 0.00001913
Iteration 150/1000 | Loss: 0.00001913
Iteration 151/1000 | Loss: 0.00001913
Iteration 152/1000 | Loss: 0.00001913
Iteration 153/1000 | Loss: 0.00001913
Iteration 154/1000 | Loss: 0.00001913
Iteration 155/1000 | Loss: 0.00001913
Iteration 156/1000 | Loss: 0.00001913
Iteration 157/1000 | Loss: 0.00001913
Iteration 158/1000 | Loss: 0.00001913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.9134518879582174e-05, 1.9134518879582174e-05, 1.9134518879582174e-05, 1.9134518879582174e-05, 1.9134518879582174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9134518879582174e-05

Optimization complete. Final v2v error: 3.699920892715454 mm

Highest mean error: 9.605934143066406 mm for frame 160

Lowest mean error: 3.2569005489349365 mm for frame 9

Saving results

Total time: 167.6203761100769
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00563689
Iteration 2/25 | Loss: 0.00127337
Iteration 3/25 | Loss: 0.00098208
Iteration 4/25 | Loss: 0.00091161
Iteration 5/25 | Loss: 0.00087987
Iteration 6/25 | Loss: 0.00088871
Iteration 7/25 | Loss: 0.00086653
Iteration 8/25 | Loss: 0.00086841
Iteration 9/25 | Loss: 0.00086433
Iteration 10/25 | Loss: 0.00086404
Iteration 11/25 | Loss: 0.00086379
Iteration 12/25 | Loss: 0.00086364
Iteration 13/25 | Loss: 0.00086344
Iteration 14/25 | Loss: 0.00086312
Iteration 15/25 | Loss: 0.00086349
Iteration 16/25 | Loss: 0.00086306
Iteration 17/25 | Loss: 0.00086334
Iteration 18/25 | Loss: 0.00086307
Iteration 19/25 | Loss: 0.00086323
Iteration 20/25 | Loss: 0.00086302
Iteration 21/25 | Loss: 0.00086293
Iteration 22/25 | Loss: 0.00086292
Iteration 23/25 | Loss: 0.00086292
Iteration 24/25 | Loss: 0.00086292
Iteration 25/25 | Loss: 0.00086292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.18363619
Iteration 2/25 | Loss: 0.00072833
Iteration 3/25 | Loss: 0.00072830
Iteration 4/25 | Loss: 0.00072830
Iteration 5/25 | Loss: 0.00072830
Iteration 6/25 | Loss: 0.00072830
Iteration 7/25 | Loss: 0.00072830
Iteration 8/25 | Loss: 0.00072830
Iteration 9/25 | Loss: 0.00072830
Iteration 10/25 | Loss: 0.00072830
Iteration 11/25 | Loss: 0.00072830
Iteration 12/25 | Loss: 0.00072830
Iteration 13/25 | Loss: 0.00072830
Iteration 14/25 | Loss: 0.00072830
Iteration 15/25 | Loss: 0.00072830
Iteration 16/25 | Loss: 0.00072830
Iteration 17/25 | Loss: 0.00072830
Iteration 18/25 | Loss: 0.00072830
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007282967562787235, 0.0007282967562787235, 0.0007282967562787235, 0.0007282967562787235, 0.0007282967562787235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007282967562787235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072830
Iteration 2/1000 | Loss: 0.00002589
Iteration 3/1000 | Loss: 0.00001990
Iteration 4/1000 | Loss: 0.00001812
Iteration 5/1000 | Loss: 0.00001689
Iteration 6/1000 | Loss: 0.00001625
Iteration 7/1000 | Loss: 0.00001595
Iteration 8/1000 | Loss: 0.00001562
Iteration 9/1000 | Loss: 0.00001554
Iteration 10/1000 | Loss: 0.00001542
Iteration 11/1000 | Loss: 0.00001532
Iteration 12/1000 | Loss: 0.00001519
Iteration 13/1000 | Loss: 0.00001517
Iteration 14/1000 | Loss: 0.00001515
Iteration 15/1000 | Loss: 0.00001514
Iteration 16/1000 | Loss: 0.00001514
Iteration 17/1000 | Loss: 0.00001514
Iteration 18/1000 | Loss: 0.00001513
Iteration 19/1000 | Loss: 0.00001511
Iteration 20/1000 | Loss: 0.00001510
Iteration 21/1000 | Loss: 0.00001507
Iteration 22/1000 | Loss: 0.00001507
Iteration 23/1000 | Loss: 0.00001506
Iteration 24/1000 | Loss: 0.00001505
Iteration 25/1000 | Loss: 0.00001505
Iteration 26/1000 | Loss: 0.00001505
Iteration 27/1000 | Loss: 0.00001505
Iteration 28/1000 | Loss: 0.00001504
Iteration 29/1000 | Loss: 0.00001504
Iteration 30/1000 | Loss: 0.00001504
Iteration 31/1000 | Loss: 0.00001504
Iteration 32/1000 | Loss: 0.00001504
Iteration 33/1000 | Loss: 0.00001504
Iteration 34/1000 | Loss: 0.00001504
Iteration 35/1000 | Loss: 0.00001503
Iteration 36/1000 | Loss: 0.00001499
Iteration 37/1000 | Loss: 0.00001499
Iteration 38/1000 | Loss: 0.00001499
Iteration 39/1000 | Loss: 0.00001499
Iteration 40/1000 | Loss: 0.00001498
Iteration 41/1000 | Loss: 0.00001498
Iteration 42/1000 | Loss: 0.00001498
Iteration 43/1000 | Loss: 0.00001498
Iteration 44/1000 | Loss: 0.00001498
Iteration 45/1000 | Loss: 0.00001497
Iteration 46/1000 | Loss: 0.00001496
Iteration 47/1000 | Loss: 0.00001495
Iteration 48/1000 | Loss: 0.00001495
Iteration 49/1000 | Loss: 0.00001495
Iteration 50/1000 | Loss: 0.00001495
Iteration 51/1000 | Loss: 0.00001495
Iteration 52/1000 | Loss: 0.00001494
Iteration 53/1000 | Loss: 0.00001494
Iteration 54/1000 | Loss: 0.00001493
Iteration 55/1000 | Loss: 0.00001493
Iteration 56/1000 | Loss: 0.00001492
Iteration 57/1000 | Loss: 0.00001492
Iteration 58/1000 | Loss: 0.00001492
Iteration 59/1000 | Loss: 0.00001491
Iteration 60/1000 | Loss: 0.00001491
Iteration 61/1000 | Loss: 0.00001491
Iteration 62/1000 | Loss: 0.00001491
Iteration 63/1000 | Loss: 0.00001490
Iteration 64/1000 | Loss: 0.00001490
Iteration 65/1000 | Loss: 0.00001490
Iteration 66/1000 | Loss: 0.00001490
Iteration 67/1000 | Loss: 0.00001490
Iteration 68/1000 | Loss: 0.00001489
Iteration 69/1000 | Loss: 0.00001489
Iteration 70/1000 | Loss: 0.00001489
Iteration 71/1000 | Loss: 0.00001489
Iteration 72/1000 | Loss: 0.00001489
Iteration 73/1000 | Loss: 0.00001488
Iteration 74/1000 | Loss: 0.00001488
Iteration 75/1000 | Loss: 0.00001488
Iteration 76/1000 | Loss: 0.00001488
Iteration 77/1000 | Loss: 0.00001488
Iteration 78/1000 | Loss: 0.00001488
Iteration 79/1000 | Loss: 0.00001488
Iteration 80/1000 | Loss: 0.00001488
Iteration 81/1000 | Loss: 0.00001488
Iteration 82/1000 | Loss: 0.00001488
Iteration 83/1000 | Loss: 0.00001488
Iteration 84/1000 | Loss: 0.00001487
Iteration 85/1000 | Loss: 0.00001487
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00001487
Iteration 88/1000 | Loss: 0.00001487
Iteration 89/1000 | Loss: 0.00001487
Iteration 90/1000 | Loss: 0.00001487
Iteration 91/1000 | Loss: 0.00001487
Iteration 92/1000 | Loss: 0.00001487
Iteration 93/1000 | Loss: 0.00001487
Iteration 94/1000 | Loss: 0.00001487
Iteration 95/1000 | Loss: 0.00001487
Iteration 96/1000 | Loss: 0.00001487
Iteration 97/1000 | Loss: 0.00001487
Iteration 98/1000 | Loss: 0.00001487
Iteration 99/1000 | Loss: 0.00001487
Iteration 100/1000 | Loss: 0.00001487
Iteration 101/1000 | Loss: 0.00001487
Iteration 102/1000 | Loss: 0.00001487
Iteration 103/1000 | Loss: 0.00001487
Iteration 104/1000 | Loss: 0.00001487
Iteration 105/1000 | Loss: 0.00001487
Iteration 106/1000 | Loss: 0.00001487
Iteration 107/1000 | Loss: 0.00001487
Iteration 108/1000 | Loss: 0.00001487
Iteration 109/1000 | Loss: 0.00001487
Iteration 110/1000 | Loss: 0.00001487
Iteration 111/1000 | Loss: 0.00001487
Iteration 112/1000 | Loss: 0.00001487
Iteration 113/1000 | Loss: 0.00001487
Iteration 114/1000 | Loss: 0.00001487
Iteration 115/1000 | Loss: 0.00001487
Iteration 116/1000 | Loss: 0.00001487
Iteration 117/1000 | Loss: 0.00001487
Iteration 118/1000 | Loss: 0.00001487
Iteration 119/1000 | Loss: 0.00001487
Iteration 120/1000 | Loss: 0.00001487
Iteration 121/1000 | Loss: 0.00001487
Iteration 122/1000 | Loss: 0.00001487
Iteration 123/1000 | Loss: 0.00001487
Iteration 124/1000 | Loss: 0.00001487
Iteration 125/1000 | Loss: 0.00001487
Iteration 126/1000 | Loss: 0.00001487
Iteration 127/1000 | Loss: 0.00001487
Iteration 128/1000 | Loss: 0.00001487
Iteration 129/1000 | Loss: 0.00001487
Iteration 130/1000 | Loss: 0.00001487
Iteration 131/1000 | Loss: 0.00001487
Iteration 132/1000 | Loss: 0.00001487
Iteration 133/1000 | Loss: 0.00001487
Iteration 134/1000 | Loss: 0.00001487
Iteration 135/1000 | Loss: 0.00001487
Iteration 136/1000 | Loss: 0.00001487
Iteration 137/1000 | Loss: 0.00001487
Iteration 138/1000 | Loss: 0.00001487
Iteration 139/1000 | Loss: 0.00001487
Iteration 140/1000 | Loss: 0.00001487
Iteration 141/1000 | Loss: 0.00001487
Iteration 142/1000 | Loss: 0.00001487
Iteration 143/1000 | Loss: 0.00001487
Iteration 144/1000 | Loss: 0.00001487
Iteration 145/1000 | Loss: 0.00001487
Iteration 146/1000 | Loss: 0.00001487
Iteration 147/1000 | Loss: 0.00001487
Iteration 148/1000 | Loss: 0.00001487
Iteration 149/1000 | Loss: 0.00001487
Iteration 150/1000 | Loss: 0.00001487
Iteration 151/1000 | Loss: 0.00001487
Iteration 152/1000 | Loss: 0.00001487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.4868706784909591e-05, 1.4868706784909591e-05, 1.4868706784909591e-05, 1.4868706784909591e-05, 1.4868706784909591e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4868706784909591e-05

Optimization complete. Final v2v error: 3.2792134284973145 mm

Highest mean error: 9.689289093017578 mm for frame 190

Lowest mean error: 2.885335683822632 mm for frame 127

Saving results

Total time: 65.87608671188354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440140
Iteration 2/25 | Loss: 0.00111744
Iteration 3/25 | Loss: 0.00100850
Iteration 4/25 | Loss: 0.00099012
Iteration 5/25 | Loss: 0.00098620
Iteration 6/25 | Loss: 0.00098541
Iteration 7/25 | Loss: 0.00098539
Iteration 8/25 | Loss: 0.00098539
Iteration 9/25 | Loss: 0.00098539
Iteration 10/25 | Loss: 0.00098539
Iteration 11/25 | Loss: 0.00098539
Iteration 12/25 | Loss: 0.00098539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009853867813944817, 0.0009853867813944817, 0.0009853867813944817, 0.0009853867813944817, 0.0009853867813944817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009853867813944817

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07039201
Iteration 2/25 | Loss: 0.00069628
Iteration 3/25 | Loss: 0.00069627
Iteration 4/25 | Loss: 0.00069627
Iteration 5/25 | Loss: 0.00069627
Iteration 6/25 | Loss: 0.00069627
Iteration 7/25 | Loss: 0.00069627
Iteration 8/25 | Loss: 0.00069627
Iteration 9/25 | Loss: 0.00069627
Iteration 10/25 | Loss: 0.00069627
Iteration 11/25 | Loss: 0.00069627
Iteration 12/25 | Loss: 0.00069627
Iteration 13/25 | Loss: 0.00069627
Iteration 14/25 | Loss: 0.00069627
Iteration 15/25 | Loss: 0.00069627
Iteration 16/25 | Loss: 0.00069627
Iteration 17/25 | Loss: 0.00069627
Iteration 18/25 | Loss: 0.00069627
Iteration 19/25 | Loss: 0.00069627
Iteration 20/25 | Loss: 0.00069627
Iteration 21/25 | Loss: 0.00069627
Iteration 22/25 | Loss: 0.00069627
Iteration 23/25 | Loss: 0.00069627
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006962688057683408, 0.0006962688057683408, 0.0006962688057683408, 0.0006962688057683408, 0.0006962688057683408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006962688057683408

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069627
Iteration 2/1000 | Loss: 0.00003960
Iteration 3/1000 | Loss: 0.00002287
Iteration 4/1000 | Loss: 0.00001887
Iteration 5/1000 | Loss: 0.00001809
Iteration 6/1000 | Loss: 0.00001735
Iteration 7/1000 | Loss: 0.00001724
Iteration 8/1000 | Loss: 0.00001688
Iteration 9/1000 | Loss: 0.00001658
Iteration 10/1000 | Loss: 0.00001658
Iteration 11/1000 | Loss: 0.00001657
Iteration 12/1000 | Loss: 0.00001656
Iteration 13/1000 | Loss: 0.00001642
Iteration 14/1000 | Loss: 0.00001642
Iteration 15/1000 | Loss: 0.00001642
Iteration 16/1000 | Loss: 0.00001641
Iteration 17/1000 | Loss: 0.00001640
Iteration 18/1000 | Loss: 0.00001627
Iteration 19/1000 | Loss: 0.00001626
Iteration 20/1000 | Loss: 0.00001623
Iteration 21/1000 | Loss: 0.00001623
Iteration 22/1000 | Loss: 0.00001623
Iteration 23/1000 | Loss: 0.00001623
Iteration 24/1000 | Loss: 0.00001623
Iteration 25/1000 | Loss: 0.00001623
Iteration 26/1000 | Loss: 0.00001623
Iteration 27/1000 | Loss: 0.00001622
Iteration 28/1000 | Loss: 0.00001622
Iteration 29/1000 | Loss: 0.00001618
Iteration 30/1000 | Loss: 0.00001616
Iteration 31/1000 | Loss: 0.00001613
Iteration 32/1000 | Loss: 0.00001613
Iteration 33/1000 | Loss: 0.00001613
Iteration 34/1000 | Loss: 0.00001612
Iteration 35/1000 | Loss: 0.00001612
Iteration 36/1000 | Loss: 0.00001612
Iteration 37/1000 | Loss: 0.00001612
Iteration 38/1000 | Loss: 0.00001612
Iteration 39/1000 | Loss: 0.00001612
Iteration 40/1000 | Loss: 0.00001612
Iteration 41/1000 | Loss: 0.00001612
Iteration 42/1000 | Loss: 0.00001612
Iteration 43/1000 | Loss: 0.00001612
Iteration 44/1000 | Loss: 0.00001612
Iteration 45/1000 | Loss: 0.00001611
Iteration 46/1000 | Loss: 0.00001611
Iteration 47/1000 | Loss: 0.00001611
Iteration 48/1000 | Loss: 0.00001611
Iteration 49/1000 | Loss: 0.00001611
Iteration 50/1000 | Loss: 0.00001611
Iteration 51/1000 | Loss: 0.00001611
Iteration 52/1000 | Loss: 0.00001611
Iteration 53/1000 | Loss: 0.00001611
Iteration 54/1000 | Loss: 0.00001611
Iteration 55/1000 | Loss: 0.00001611
Iteration 56/1000 | Loss: 0.00001611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 56. Stopping optimization.
Last 5 losses: [1.611275911272969e-05, 1.611275911272969e-05, 1.611275911272969e-05, 1.611275911272969e-05, 1.611275911272969e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.611275911272969e-05

Optimization complete. Final v2v error: 3.4652655124664307 mm

Highest mean error: 3.5863711833953857 mm for frame 7

Lowest mean error: 3.3451015949249268 mm for frame 79

Saving results

Total time: 25.909174919128418
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886198
Iteration 2/25 | Loss: 0.00108955
Iteration 3/25 | Loss: 0.00097734
Iteration 4/25 | Loss: 0.00094804
Iteration 5/25 | Loss: 0.00094173
Iteration 6/25 | Loss: 0.00094077
Iteration 7/25 | Loss: 0.00094077
Iteration 8/25 | Loss: 0.00094077
Iteration 9/25 | Loss: 0.00094077
Iteration 10/25 | Loss: 0.00094077
Iteration 11/25 | Loss: 0.00094077
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009407735778950155, 0.0009407735778950155, 0.0009407735778950155, 0.0009407735778950155, 0.0009407735778950155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009407735778950155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31814182
Iteration 2/25 | Loss: 0.00068396
Iteration 3/25 | Loss: 0.00068396
Iteration 4/25 | Loss: 0.00068396
Iteration 5/25 | Loss: 0.00068395
Iteration 6/25 | Loss: 0.00068395
Iteration 7/25 | Loss: 0.00068395
Iteration 8/25 | Loss: 0.00068395
Iteration 9/25 | Loss: 0.00068395
Iteration 10/25 | Loss: 0.00068395
Iteration 11/25 | Loss: 0.00068395
Iteration 12/25 | Loss: 0.00068395
Iteration 13/25 | Loss: 0.00068395
Iteration 14/25 | Loss: 0.00068395
Iteration 15/25 | Loss: 0.00068395
Iteration 16/25 | Loss: 0.00068395
Iteration 17/25 | Loss: 0.00068395
Iteration 18/25 | Loss: 0.00068395
Iteration 19/25 | Loss: 0.00068395
Iteration 20/25 | Loss: 0.00068395
Iteration 21/25 | Loss: 0.00068395
Iteration 22/25 | Loss: 0.00068395
Iteration 23/25 | Loss: 0.00068395
Iteration 24/25 | Loss: 0.00068395
Iteration 25/25 | Loss: 0.00068395

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068395
Iteration 2/1000 | Loss: 0.00003342
Iteration 3/1000 | Loss: 0.00002457
Iteration 4/1000 | Loss: 0.00002188
Iteration 5/1000 | Loss: 0.00002064
Iteration 6/1000 | Loss: 0.00001984
Iteration 7/1000 | Loss: 0.00001924
Iteration 8/1000 | Loss: 0.00001895
Iteration 9/1000 | Loss: 0.00001885
Iteration 10/1000 | Loss: 0.00001884
Iteration 11/1000 | Loss: 0.00001884
Iteration 12/1000 | Loss: 0.00001881
Iteration 13/1000 | Loss: 0.00001880
Iteration 14/1000 | Loss: 0.00001880
Iteration 15/1000 | Loss: 0.00001878
Iteration 16/1000 | Loss: 0.00001878
Iteration 17/1000 | Loss: 0.00001877
Iteration 18/1000 | Loss: 0.00001877
Iteration 19/1000 | Loss: 0.00001876
Iteration 20/1000 | Loss: 0.00001876
Iteration 21/1000 | Loss: 0.00001875
Iteration 22/1000 | Loss: 0.00001875
Iteration 23/1000 | Loss: 0.00001874
Iteration 24/1000 | Loss: 0.00001874
Iteration 25/1000 | Loss: 0.00001873
Iteration 26/1000 | Loss: 0.00001868
Iteration 27/1000 | Loss: 0.00001868
Iteration 28/1000 | Loss: 0.00001868
Iteration 29/1000 | Loss: 0.00001868
Iteration 30/1000 | Loss: 0.00001866
Iteration 31/1000 | Loss: 0.00001865
Iteration 32/1000 | Loss: 0.00001865
Iteration 33/1000 | Loss: 0.00001865
Iteration 34/1000 | Loss: 0.00001864
Iteration 35/1000 | Loss: 0.00001864
Iteration 36/1000 | Loss: 0.00001864
Iteration 37/1000 | Loss: 0.00001864
Iteration 38/1000 | Loss: 0.00001864
Iteration 39/1000 | Loss: 0.00001863
Iteration 40/1000 | Loss: 0.00001863
Iteration 41/1000 | Loss: 0.00001863
Iteration 42/1000 | Loss: 0.00001863
Iteration 43/1000 | Loss: 0.00001863
Iteration 44/1000 | Loss: 0.00001863
Iteration 45/1000 | Loss: 0.00001863
Iteration 46/1000 | Loss: 0.00001863
Iteration 47/1000 | Loss: 0.00001863
Iteration 48/1000 | Loss: 0.00001862
Iteration 49/1000 | Loss: 0.00001862
Iteration 50/1000 | Loss: 0.00001862
Iteration 51/1000 | Loss: 0.00001862
Iteration 52/1000 | Loss: 0.00001862
Iteration 53/1000 | Loss: 0.00001861
Iteration 54/1000 | Loss: 0.00001861
Iteration 55/1000 | Loss: 0.00001861
Iteration 56/1000 | Loss: 0.00001861
Iteration 57/1000 | Loss: 0.00001861
Iteration 58/1000 | Loss: 0.00001861
Iteration 59/1000 | Loss: 0.00001861
Iteration 60/1000 | Loss: 0.00001861
Iteration 61/1000 | Loss: 0.00001861
Iteration 62/1000 | Loss: 0.00001861
Iteration 63/1000 | Loss: 0.00001860
Iteration 64/1000 | Loss: 0.00001860
Iteration 65/1000 | Loss: 0.00001860
Iteration 66/1000 | Loss: 0.00001860
Iteration 67/1000 | Loss: 0.00001860
Iteration 68/1000 | Loss: 0.00001860
Iteration 69/1000 | Loss: 0.00001860
Iteration 70/1000 | Loss: 0.00001860
Iteration 71/1000 | Loss: 0.00001860
Iteration 72/1000 | Loss: 0.00001860
Iteration 73/1000 | Loss: 0.00001860
Iteration 74/1000 | Loss: 0.00001860
Iteration 75/1000 | Loss: 0.00001860
Iteration 76/1000 | Loss: 0.00001860
Iteration 77/1000 | Loss: 0.00001860
Iteration 78/1000 | Loss: 0.00001860
Iteration 79/1000 | Loss: 0.00001860
Iteration 80/1000 | Loss: 0.00001860
Iteration 81/1000 | Loss: 0.00001859
Iteration 82/1000 | Loss: 0.00001859
Iteration 83/1000 | Loss: 0.00001859
Iteration 84/1000 | Loss: 0.00001859
Iteration 85/1000 | Loss: 0.00001859
Iteration 86/1000 | Loss: 0.00001859
Iteration 87/1000 | Loss: 0.00001859
Iteration 88/1000 | Loss: 0.00001859
Iteration 89/1000 | Loss: 0.00001858
Iteration 90/1000 | Loss: 0.00001858
Iteration 91/1000 | Loss: 0.00001858
Iteration 92/1000 | Loss: 0.00001858
Iteration 93/1000 | Loss: 0.00001858
Iteration 94/1000 | Loss: 0.00001858
Iteration 95/1000 | Loss: 0.00001858
Iteration 96/1000 | Loss: 0.00001858
Iteration 97/1000 | Loss: 0.00001858
Iteration 98/1000 | Loss: 0.00001858
Iteration 99/1000 | Loss: 0.00001858
Iteration 100/1000 | Loss: 0.00001858
Iteration 101/1000 | Loss: 0.00001858
Iteration 102/1000 | Loss: 0.00001858
Iteration 103/1000 | Loss: 0.00001858
Iteration 104/1000 | Loss: 0.00001858
Iteration 105/1000 | Loss: 0.00001858
Iteration 106/1000 | Loss: 0.00001858
Iteration 107/1000 | Loss: 0.00001858
Iteration 108/1000 | Loss: 0.00001858
Iteration 109/1000 | Loss: 0.00001858
Iteration 110/1000 | Loss: 0.00001858
Iteration 111/1000 | Loss: 0.00001858
Iteration 112/1000 | Loss: 0.00001858
Iteration 113/1000 | Loss: 0.00001858
Iteration 114/1000 | Loss: 0.00001858
Iteration 115/1000 | Loss: 0.00001858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.8576938600745052e-05, 1.8576938600745052e-05, 1.8576938600745052e-05, 1.8576938600745052e-05, 1.8576938600745052e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8576938600745052e-05

Optimization complete. Final v2v error: 3.5864176750183105 mm

Highest mean error: 3.745372772216797 mm for frame 98

Lowest mean error: 3.460533857345581 mm for frame 58

Saving results

Total time: 27.387329816818237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018191
Iteration 2/25 | Loss: 0.00437356
Iteration 3/25 | Loss: 0.00291725
Iteration 4/25 | Loss: 0.00232351
Iteration 5/25 | Loss: 0.00219198
Iteration 6/25 | Loss: 0.00201333
Iteration 7/25 | Loss: 0.00199580
Iteration 8/25 | Loss: 0.00191339
Iteration 9/25 | Loss: 0.00160497
Iteration 10/25 | Loss: 0.00146459
Iteration 11/25 | Loss: 0.00142051
Iteration 12/25 | Loss: 0.00138139
Iteration 13/25 | Loss: 0.00134634
Iteration 14/25 | Loss: 0.00134712
Iteration 15/25 | Loss: 0.00130402
Iteration 16/25 | Loss: 0.00131235
Iteration 17/25 | Loss: 0.00130897
Iteration 18/25 | Loss: 0.00131297
Iteration 19/25 | Loss: 0.00129960
Iteration 20/25 | Loss: 0.00126789
Iteration 21/25 | Loss: 0.00123678
Iteration 22/25 | Loss: 0.00124387
Iteration 23/25 | Loss: 0.00123492
Iteration 24/25 | Loss: 0.00123965
Iteration 25/25 | Loss: 0.00124163

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.51535225
Iteration 2/25 | Loss: 0.00636928
Iteration 3/25 | Loss: 0.00513867
Iteration 4/25 | Loss: 0.00513866
Iteration 5/25 | Loss: 0.00513866
Iteration 6/25 | Loss: 0.00513866
Iteration 7/25 | Loss: 0.00513866
Iteration 8/25 | Loss: 0.00513866
Iteration 9/25 | Loss: 0.00513866
Iteration 10/25 | Loss: 0.00513866
Iteration 11/25 | Loss: 0.00513866
Iteration 12/25 | Loss: 0.00513866
Iteration 13/25 | Loss: 0.00513866
Iteration 14/25 | Loss: 0.00513866
Iteration 15/25 | Loss: 0.00513866
Iteration 16/25 | Loss: 0.00513866
Iteration 17/25 | Loss: 0.00513866
Iteration 18/25 | Loss: 0.00513866
Iteration 19/25 | Loss: 0.00513866
Iteration 20/25 | Loss: 0.00513866
Iteration 21/25 | Loss: 0.00513866
Iteration 22/25 | Loss: 0.00513866
Iteration 23/25 | Loss: 0.00513866
Iteration 24/25 | Loss: 0.00513866
Iteration 25/25 | Loss: 0.00513866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00513866
Iteration 2/1000 | Loss: 0.00159822
Iteration 3/1000 | Loss: 0.00435501
Iteration 4/1000 | Loss: 0.00333512
Iteration 5/1000 | Loss: 0.00493735
Iteration 6/1000 | Loss: 0.00231366
Iteration 7/1000 | Loss: 0.00446690
Iteration 8/1000 | Loss: 0.00248890
Iteration 9/1000 | Loss: 0.00410325
Iteration 10/1000 | Loss: 0.00263431
Iteration 11/1000 | Loss: 0.00083109
Iteration 12/1000 | Loss: 0.00514724
Iteration 13/1000 | Loss: 0.00113702
Iteration 14/1000 | Loss: 0.00064257
Iteration 15/1000 | Loss: 0.00103636
Iteration 16/1000 | Loss: 0.00126445
Iteration 17/1000 | Loss: 0.00257641
Iteration 18/1000 | Loss: 0.00184389
Iteration 19/1000 | Loss: 0.00188874
Iteration 20/1000 | Loss: 0.00093232
Iteration 21/1000 | Loss: 0.00283884
Iteration 22/1000 | Loss: 0.00108418
Iteration 23/1000 | Loss: 0.00121553
Iteration 24/1000 | Loss: 0.00089567
Iteration 25/1000 | Loss: 0.00231283
Iteration 26/1000 | Loss: 0.00246701
Iteration 27/1000 | Loss: 0.00293643
Iteration 28/1000 | Loss: 0.00335502
Iteration 29/1000 | Loss: 0.00395585
Iteration 30/1000 | Loss: 0.00125603
Iteration 31/1000 | Loss: 0.00188811
Iteration 32/1000 | Loss: 0.00259765
Iteration 33/1000 | Loss: 0.00218476
Iteration 34/1000 | Loss: 0.00178666
Iteration 35/1000 | Loss: 0.00341470
Iteration 36/1000 | Loss: 0.00217989
Iteration 37/1000 | Loss: 0.00403155
Iteration 38/1000 | Loss: 0.00361807
Iteration 39/1000 | Loss: 0.00476110
Iteration 40/1000 | Loss: 0.00707011
Iteration 41/1000 | Loss: 0.00703207
Iteration 42/1000 | Loss: 0.00571411
Iteration 43/1000 | Loss: 0.00913900
Iteration 44/1000 | Loss: 0.00553720
Iteration 45/1000 | Loss: 0.00553406
Iteration 46/1000 | Loss: 0.00617048
Iteration 47/1000 | Loss: 0.00613944
Iteration 48/1000 | Loss: 0.00805816
Iteration 49/1000 | Loss: 0.00748600
Iteration 50/1000 | Loss: 0.00455531
Iteration 51/1000 | Loss: 0.00365284
Iteration 52/1000 | Loss: 0.00504867
Iteration 53/1000 | Loss: 0.00332523
Iteration 54/1000 | Loss: 0.00389349
Iteration 55/1000 | Loss: 0.00364773
Iteration 56/1000 | Loss: 0.00376466
Iteration 57/1000 | Loss: 0.00565177
Iteration 58/1000 | Loss: 0.00353345
Iteration 59/1000 | Loss: 0.00669961
Iteration 60/1000 | Loss: 0.00403104
Iteration 61/1000 | Loss: 0.00450193
Iteration 62/1000 | Loss: 0.00407533
Iteration 63/1000 | Loss: 0.00253977
Iteration 64/1000 | Loss: 0.00270564
Iteration 65/1000 | Loss: 0.00250892
Iteration 66/1000 | Loss: 0.00250709
Iteration 67/1000 | Loss: 0.00318718
Iteration 68/1000 | Loss: 0.00227451
Iteration 69/1000 | Loss: 0.00206726
Iteration 70/1000 | Loss: 0.00277300
Iteration 71/1000 | Loss: 0.00266341
Iteration 72/1000 | Loss: 0.00469107
Iteration 73/1000 | Loss: 0.00342253
Iteration 74/1000 | Loss: 0.00552075
Iteration 75/1000 | Loss: 0.00518732
Iteration 76/1000 | Loss: 0.00395577
Iteration 77/1000 | Loss: 0.00619870
Iteration 78/1000 | Loss: 0.00172162
Iteration 79/1000 | Loss: 0.00165085
Iteration 80/1000 | Loss: 0.00421889
Iteration 81/1000 | Loss: 0.00227814
Iteration 82/1000 | Loss: 0.00234658
Iteration 83/1000 | Loss: 0.00156000
Iteration 84/1000 | Loss: 0.00158067
Iteration 85/1000 | Loss: 0.00152367
Iteration 86/1000 | Loss: 0.00136492
Iteration 87/1000 | Loss: 0.00145765
Iteration 88/1000 | Loss: 0.00164836
Iteration 89/1000 | Loss: 0.00076688
Iteration 90/1000 | Loss: 0.00080055
Iteration 91/1000 | Loss: 0.00070197
Iteration 92/1000 | Loss: 0.00033836
Iteration 93/1000 | Loss: 0.00096220
Iteration 94/1000 | Loss: 0.00092963
Iteration 95/1000 | Loss: 0.00077255
Iteration 96/1000 | Loss: 0.00053986
Iteration 97/1000 | Loss: 0.00072432
Iteration 98/1000 | Loss: 0.00098633
Iteration 99/1000 | Loss: 0.00066947
Iteration 100/1000 | Loss: 0.00052779
Iteration 101/1000 | Loss: 0.00180079
Iteration 102/1000 | Loss: 0.00062711
Iteration 103/1000 | Loss: 0.00093565
Iteration 104/1000 | Loss: 0.00080646
Iteration 105/1000 | Loss: 0.00061887
Iteration 106/1000 | Loss: 0.00045826
Iteration 107/1000 | Loss: 0.00068659
Iteration 108/1000 | Loss: 0.00075592
Iteration 109/1000 | Loss: 0.00076586
Iteration 110/1000 | Loss: 0.00062997
Iteration 111/1000 | Loss: 0.00061274
Iteration 112/1000 | Loss: 0.00085475
Iteration 113/1000 | Loss: 0.00067151
Iteration 114/1000 | Loss: 0.00073633
Iteration 115/1000 | Loss: 0.00086322
Iteration 116/1000 | Loss: 0.00093975
Iteration 117/1000 | Loss: 0.00137630
Iteration 118/1000 | Loss: 0.00105576
Iteration 119/1000 | Loss: 0.00081509
Iteration 120/1000 | Loss: 0.00103073
Iteration 121/1000 | Loss: 0.00156227
Iteration 122/1000 | Loss: 0.00081229
Iteration 123/1000 | Loss: 0.00079793
Iteration 124/1000 | Loss: 0.00055107
Iteration 125/1000 | Loss: 0.00140911
Iteration 126/1000 | Loss: 0.00079396
Iteration 127/1000 | Loss: 0.00080860
Iteration 128/1000 | Loss: 0.00125140
Iteration 129/1000 | Loss: 0.00127354
Iteration 130/1000 | Loss: 0.00060611
Iteration 131/1000 | Loss: 0.00097311
Iteration 132/1000 | Loss: 0.00033444
Iteration 133/1000 | Loss: 0.00025233
Iteration 134/1000 | Loss: 0.00017542
Iteration 135/1000 | Loss: 0.00036585
Iteration 136/1000 | Loss: 0.00027808
Iteration 137/1000 | Loss: 0.00028032
Iteration 138/1000 | Loss: 0.00020024
Iteration 139/1000 | Loss: 0.00030077
Iteration 140/1000 | Loss: 0.00066878
Iteration 141/1000 | Loss: 0.00095631
Iteration 142/1000 | Loss: 0.00028464
Iteration 143/1000 | Loss: 0.00026038
Iteration 144/1000 | Loss: 0.00065147
Iteration 145/1000 | Loss: 0.00049637
Iteration 146/1000 | Loss: 0.00027379
Iteration 147/1000 | Loss: 0.00032483
Iteration 148/1000 | Loss: 0.00058985
Iteration 149/1000 | Loss: 0.00028085
Iteration 150/1000 | Loss: 0.00021625
Iteration 151/1000 | Loss: 0.00028036
Iteration 152/1000 | Loss: 0.00045564
Iteration 153/1000 | Loss: 0.00047508
Iteration 154/1000 | Loss: 0.00061985
Iteration 155/1000 | Loss: 0.00079447
Iteration 156/1000 | Loss: 0.00009888
Iteration 157/1000 | Loss: 0.00011380
Iteration 158/1000 | Loss: 0.00005614
Iteration 159/1000 | Loss: 0.00017884
Iteration 160/1000 | Loss: 0.00059336
Iteration 161/1000 | Loss: 0.00030817
Iteration 162/1000 | Loss: 0.00021927
Iteration 163/1000 | Loss: 0.00006469
Iteration 164/1000 | Loss: 0.00027611
Iteration 165/1000 | Loss: 0.00025274
Iteration 166/1000 | Loss: 0.00059233
Iteration 167/1000 | Loss: 0.00116184
Iteration 168/1000 | Loss: 0.00094409
Iteration 169/1000 | Loss: 0.00054964
Iteration 170/1000 | Loss: 0.00025071
Iteration 171/1000 | Loss: 0.00065457
Iteration 172/1000 | Loss: 0.00030186
Iteration 173/1000 | Loss: 0.00050074
Iteration 174/1000 | Loss: 0.00042909
Iteration 175/1000 | Loss: 0.00037577
Iteration 176/1000 | Loss: 0.00006824
Iteration 177/1000 | Loss: 0.00045221
Iteration 178/1000 | Loss: 0.00029080
Iteration 179/1000 | Loss: 0.00033657
Iteration 180/1000 | Loss: 0.00026638
Iteration 181/1000 | Loss: 0.00014416
Iteration 182/1000 | Loss: 0.00055285
Iteration 183/1000 | Loss: 0.00022300
Iteration 184/1000 | Loss: 0.00047542
Iteration 185/1000 | Loss: 0.00023752
Iteration 186/1000 | Loss: 0.00032879
Iteration 187/1000 | Loss: 0.00011352
Iteration 188/1000 | Loss: 0.00015584
Iteration 189/1000 | Loss: 0.00006700
Iteration 190/1000 | Loss: 0.00014961
Iteration 191/1000 | Loss: 0.00015789
Iteration 192/1000 | Loss: 0.00008713
Iteration 193/1000 | Loss: 0.00011434
Iteration 194/1000 | Loss: 0.00006400
Iteration 195/1000 | Loss: 0.00014516
Iteration 196/1000 | Loss: 0.00005835
Iteration 197/1000 | Loss: 0.00016230
Iteration 198/1000 | Loss: 0.00013462
Iteration 199/1000 | Loss: 0.00018412
Iteration 200/1000 | Loss: 0.00005972
Iteration 201/1000 | Loss: 0.00075144
Iteration 202/1000 | Loss: 0.00029893
Iteration 203/1000 | Loss: 0.00031616
Iteration 204/1000 | Loss: 0.00016966
Iteration 205/1000 | Loss: 0.00005722
Iteration 206/1000 | Loss: 0.00069218
Iteration 207/1000 | Loss: 0.00038658
Iteration 208/1000 | Loss: 0.00016438
Iteration 209/1000 | Loss: 0.00013038
Iteration 210/1000 | Loss: 0.00016105
Iteration 211/1000 | Loss: 0.00012870
Iteration 212/1000 | Loss: 0.00010978
Iteration 213/1000 | Loss: 0.00010419
Iteration 214/1000 | Loss: 0.00012442
Iteration 215/1000 | Loss: 0.00009866
Iteration 216/1000 | Loss: 0.00016694
Iteration 217/1000 | Loss: 0.00009815
Iteration 218/1000 | Loss: 0.00018867
Iteration 219/1000 | Loss: 0.00016034
Iteration 220/1000 | Loss: 0.00019081
Iteration 221/1000 | Loss: 0.00019450
Iteration 222/1000 | Loss: 0.00013045
Iteration 223/1000 | Loss: 0.00004765
Iteration 224/1000 | Loss: 0.00004531
Iteration 225/1000 | Loss: 0.00021872
Iteration 226/1000 | Loss: 0.00008524
Iteration 227/1000 | Loss: 0.00016998
Iteration 228/1000 | Loss: 0.00009758
Iteration 229/1000 | Loss: 0.00006991
Iteration 230/1000 | Loss: 0.00025495
Iteration 231/1000 | Loss: 0.00014319
Iteration 232/1000 | Loss: 0.00016119
Iteration 233/1000 | Loss: 0.00008889
Iteration 234/1000 | Loss: 0.00011890
Iteration 235/1000 | Loss: 0.00010881
Iteration 236/1000 | Loss: 0.00013371
Iteration 237/1000 | Loss: 0.00013829
Iteration 238/1000 | Loss: 0.00015557
Iteration 239/1000 | Loss: 0.00014056
Iteration 240/1000 | Loss: 0.00005361
Iteration 241/1000 | Loss: 0.00005135
Iteration 242/1000 | Loss: 0.00004902
Iteration 243/1000 | Loss: 0.00005082
Iteration 244/1000 | Loss: 0.00004082
Iteration 245/1000 | Loss: 0.00004277
Iteration 246/1000 | Loss: 0.00059170
Iteration 247/1000 | Loss: 0.00083584
Iteration 248/1000 | Loss: 0.00041840
Iteration 249/1000 | Loss: 0.00005696
Iteration 250/1000 | Loss: 0.00041157
Iteration 251/1000 | Loss: 0.00017747
Iteration 252/1000 | Loss: 0.00013709
Iteration 253/1000 | Loss: 0.00045851
Iteration 254/1000 | Loss: 0.00025036
Iteration 255/1000 | Loss: 0.00050424
Iteration 256/1000 | Loss: 0.00024436
Iteration 257/1000 | Loss: 0.00087520
Iteration 258/1000 | Loss: 0.00065023
Iteration 259/1000 | Loss: 0.00058911
Iteration 260/1000 | Loss: 0.00028521
Iteration 261/1000 | Loss: 0.00026678
Iteration 262/1000 | Loss: 0.00073684
Iteration 263/1000 | Loss: 0.00078308
Iteration 264/1000 | Loss: 0.00036094
Iteration 265/1000 | Loss: 0.00046005
Iteration 266/1000 | Loss: 0.00046255
Iteration 267/1000 | Loss: 0.00038427
Iteration 268/1000 | Loss: 0.00048372
Iteration 269/1000 | Loss: 0.00067425
Iteration 270/1000 | Loss: 0.00033242
Iteration 271/1000 | Loss: 0.00026142
Iteration 272/1000 | Loss: 0.00007437
Iteration 273/1000 | Loss: 0.00005867
Iteration 274/1000 | Loss: 0.00004677
Iteration 275/1000 | Loss: 0.00028416
Iteration 276/1000 | Loss: 0.00059635
Iteration 277/1000 | Loss: 0.00014561
Iteration 278/1000 | Loss: 0.00004667
Iteration 279/1000 | Loss: 0.00019095
Iteration 280/1000 | Loss: 0.00028595
Iteration 281/1000 | Loss: 0.00036329
Iteration 282/1000 | Loss: 0.00033652
Iteration 283/1000 | Loss: 0.00017524
Iteration 284/1000 | Loss: 0.00018998
Iteration 285/1000 | Loss: 0.00062610
Iteration 286/1000 | Loss: 0.00044690
Iteration 287/1000 | Loss: 0.00046550
Iteration 288/1000 | Loss: 0.00033492
Iteration 289/1000 | Loss: 0.00046629
Iteration 290/1000 | Loss: 0.00035549
Iteration 291/1000 | Loss: 0.00049027
Iteration 292/1000 | Loss: 0.00017815
Iteration 293/1000 | Loss: 0.00014719
Iteration 294/1000 | Loss: 0.00021662
Iteration 295/1000 | Loss: 0.00008962
Iteration 296/1000 | Loss: 0.00040626
Iteration 297/1000 | Loss: 0.00014564
Iteration 298/1000 | Loss: 0.00028056
Iteration 299/1000 | Loss: 0.00046748
Iteration 300/1000 | Loss: 0.00038636
Iteration 301/1000 | Loss: 0.00009975
Iteration 302/1000 | Loss: 0.00033116
Iteration 303/1000 | Loss: 0.00021928
Iteration 304/1000 | Loss: 0.00005699
Iteration 305/1000 | Loss: 0.00006614
Iteration 306/1000 | Loss: 0.00003935
Iteration 307/1000 | Loss: 0.00039325
Iteration 308/1000 | Loss: 0.00007123
Iteration 309/1000 | Loss: 0.00017968
Iteration 310/1000 | Loss: 0.00005551
Iteration 311/1000 | Loss: 0.00039414
Iteration 312/1000 | Loss: 0.00014825
Iteration 313/1000 | Loss: 0.00016715
Iteration 314/1000 | Loss: 0.00008609
Iteration 315/1000 | Loss: 0.00032879
Iteration 316/1000 | Loss: 0.00029086
Iteration 317/1000 | Loss: 0.00067776
Iteration 318/1000 | Loss: 0.00035895
Iteration 319/1000 | Loss: 0.00037208
Iteration 320/1000 | Loss: 0.00028802
Iteration 321/1000 | Loss: 0.00004337
Iteration 322/1000 | Loss: 0.00005319
Iteration 323/1000 | Loss: 0.00003578
Iteration 324/1000 | Loss: 0.00011209
Iteration 325/1000 | Loss: 0.00003533
Iteration 326/1000 | Loss: 0.00004338
Iteration 327/1000 | Loss: 0.00003455
Iteration 328/1000 | Loss: 0.00010609
Iteration 329/1000 | Loss: 0.00015125
Iteration 330/1000 | Loss: 0.00024741
Iteration 331/1000 | Loss: 0.00012336
Iteration 332/1000 | Loss: 0.00064168
Iteration 333/1000 | Loss: 0.00003592
Iteration 334/1000 | Loss: 0.00003454
Iteration 335/1000 | Loss: 0.00006123
Iteration 336/1000 | Loss: 0.00004166
Iteration 337/1000 | Loss: 0.00003373
Iteration 338/1000 | Loss: 0.00003365
Iteration 339/1000 | Loss: 0.00003363
Iteration 340/1000 | Loss: 0.00003363
Iteration 341/1000 | Loss: 0.00003363
Iteration 342/1000 | Loss: 0.00003363
Iteration 343/1000 | Loss: 0.00003362
Iteration 344/1000 | Loss: 0.00003361
Iteration 345/1000 | Loss: 0.00003333
Iteration 346/1000 | Loss: 0.00003300
Iteration 347/1000 | Loss: 0.00006116
Iteration 348/1000 | Loss: 0.00003904
Iteration 349/1000 | Loss: 0.00003174
Iteration 350/1000 | Loss: 0.00003713
Iteration 351/1000 | Loss: 0.00003105
Iteration 352/1000 | Loss: 0.00003891
Iteration 353/1000 | Loss: 0.00003060
Iteration 354/1000 | Loss: 0.00003037
Iteration 355/1000 | Loss: 0.00003032
Iteration 356/1000 | Loss: 0.00003014
Iteration 357/1000 | Loss: 0.00003012
Iteration 358/1000 | Loss: 0.00003012
Iteration 359/1000 | Loss: 0.00003011
Iteration 360/1000 | Loss: 0.00003011
Iteration 361/1000 | Loss: 0.00003010
Iteration 362/1000 | Loss: 0.00003010
Iteration 363/1000 | Loss: 0.00003010
Iteration 364/1000 | Loss: 0.00003009
Iteration 365/1000 | Loss: 0.00003009
Iteration 366/1000 | Loss: 0.00003009
Iteration 367/1000 | Loss: 0.00003008
Iteration 368/1000 | Loss: 0.00003008
Iteration 369/1000 | Loss: 0.00003008
Iteration 370/1000 | Loss: 0.00003862
Iteration 371/1000 | Loss: 0.00004956
Iteration 372/1000 | Loss: 0.00003211
Iteration 373/1000 | Loss: 0.00003163
Iteration 374/1000 | Loss: 0.00003000
Iteration 375/1000 | Loss: 0.00003000
Iteration 376/1000 | Loss: 0.00003000
Iteration 377/1000 | Loss: 0.00003000
Iteration 378/1000 | Loss: 0.00003000
Iteration 379/1000 | Loss: 0.00003000
Iteration 380/1000 | Loss: 0.00003000
Iteration 381/1000 | Loss: 0.00003000
Iteration 382/1000 | Loss: 0.00002998
Iteration 383/1000 | Loss: 0.00002997
Iteration 384/1000 | Loss: 0.00003458
Iteration 385/1000 | Loss: 0.00003000
Iteration 386/1000 | Loss: 0.00003000
Iteration 387/1000 | Loss: 0.00002999
Iteration 388/1000 | Loss: 0.00002999
Iteration 389/1000 | Loss: 0.00002999
Iteration 390/1000 | Loss: 0.00002998
Iteration 391/1000 | Loss: 0.00002998
Iteration 392/1000 | Loss: 0.00002998
Iteration 393/1000 | Loss: 0.00002998
Iteration 394/1000 | Loss: 0.00002998
Iteration 395/1000 | Loss: 0.00002998
Iteration 396/1000 | Loss: 0.00002998
Iteration 397/1000 | Loss: 0.00002998
Iteration 398/1000 | Loss: 0.00002998
Iteration 399/1000 | Loss: 0.00002998
Iteration 400/1000 | Loss: 0.00002998
Iteration 401/1000 | Loss: 0.00002998
Iteration 402/1000 | Loss: 0.00003062
Iteration 403/1000 | Loss: 0.00002996
Iteration 404/1000 | Loss: 0.00002996
Iteration 405/1000 | Loss: 0.00002996
Iteration 406/1000 | Loss: 0.00002995
Iteration 407/1000 | Loss: 0.00002995
Iteration 408/1000 | Loss: 0.00002995
Iteration 409/1000 | Loss: 0.00002995
Iteration 410/1000 | Loss: 0.00002995
Iteration 411/1000 | Loss: 0.00002995
Iteration 412/1000 | Loss: 0.00002995
Iteration 413/1000 | Loss: 0.00002995
Iteration 414/1000 | Loss: 0.00002995
Iteration 415/1000 | Loss: 0.00002995
Iteration 416/1000 | Loss: 0.00002995
Iteration 417/1000 | Loss: 0.00002995
Iteration 418/1000 | Loss: 0.00002995
Iteration 419/1000 | Loss: 0.00002995
Iteration 420/1000 | Loss: 0.00002995
Iteration 421/1000 | Loss: 0.00002995
Iteration 422/1000 | Loss: 0.00002995
Iteration 423/1000 | Loss: 0.00002995
Iteration 424/1000 | Loss: 0.00002995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 424. Stopping optimization.
Last 5 losses: [2.9952669137855992e-05, 2.9952669137855992e-05, 2.9952669137855992e-05, 2.9952669137855992e-05, 2.9952669137855992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9952669137855992e-05

Optimization complete. Final v2v error: 4.510994911193848 mm

Highest mean error: 8.940003395080566 mm for frame 11

Lowest mean error: 3.60512638092041 mm for frame 151

Saving results

Total time: 573.3701298236847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865308
Iteration 2/25 | Loss: 0.00089036
Iteration 3/25 | Loss: 0.00073266
Iteration 4/25 | Loss: 0.00070613
Iteration 5/25 | Loss: 0.00069983
Iteration 6/25 | Loss: 0.00069834
Iteration 7/25 | Loss: 0.00069834
Iteration 8/25 | Loss: 0.00069834
Iteration 9/25 | Loss: 0.00069834
Iteration 10/25 | Loss: 0.00069834
Iteration 11/25 | Loss: 0.00069834
Iteration 12/25 | Loss: 0.00069834
Iteration 13/25 | Loss: 0.00069834
Iteration 14/25 | Loss: 0.00069834
Iteration 15/25 | Loss: 0.00069834
Iteration 16/25 | Loss: 0.00069834
Iteration 17/25 | Loss: 0.00069834
Iteration 18/25 | Loss: 0.00069834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006983370403759181, 0.0006983370403759181, 0.0006983370403759181, 0.0006983370403759181, 0.0006983370403759181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006983370403759181

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35914743
Iteration 2/25 | Loss: 0.00028911
Iteration 3/25 | Loss: 0.00028911
Iteration 4/25 | Loss: 0.00028910
Iteration 5/25 | Loss: 0.00028910
Iteration 6/25 | Loss: 0.00028910
Iteration 7/25 | Loss: 0.00028910
Iteration 8/25 | Loss: 0.00028910
Iteration 9/25 | Loss: 0.00028910
Iteration 10/25 | Loss: 0.00028910
Iteration 11/25 | Loss: 0.00028910
Iteration 12/25 | Loss: 0.00028910
Iteration 13/25 | Loss: 0.00028910
Iteration 14/25 | Loss: 0.00028910
Iteration 15/25 | Loss: 0.00028910
Iteration 16/25 | Loss: 0.00028910
Iteration 17/25 | Loss: 0.00028910
Iteration 18/25 | Loss: 0.00028910
Iteration 19/25 | Loss: 0.00028910
Iteration 20/25 | Loss: 0.00028910
Iteration 21/25 | Loss: 0.00028910
Iteration 22/25 | Loss: 0.00028910
Iteration 23/25 | Loss: 0.00028910
Iteration 24/25 | Loss: 0.00028910
Iteration 25/25 | Loss: 0.00028910

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028910
Iteration 2/1000 | Loss: 0.00005612
Iteration 3/1000 | Loss: 0.00003022
Iteration 4/1000 | Loss: 0.00002656
Iteration 5/1000 | Loss: 0.00002400
Iteration 6/1000 | Loss: 0.00002236
Iteration 7/1000 | Loss: 0.00002144
Iteration 8/1000 | Loss: 0.00002091
Iteration 9/1000 | Loss: 0.00002064
Iteration 10/1000 | Loss: 0.00002061
Iteration 11/1000 | Loss: 0.00002060
Iteration 12/1000 | Loss: 0.00002042
Iteration 13/1000 | Loss: 0.00002040
Iteration 14/1000 | Loss: 0.00002029
Iteration 15/1000 | Loss: 0.00002021
Iteration 16/1000 | Loss: 0.00002016
Iteration 17/1000 | Loss: 0.00002015
Iteration 18/1000 | Loss: 0.00002015
Iteration 19/1000 | Loss: 0.00002014
Iteration 20/1000 | Loss: 0.00002014
Iteration 21/1000 | Loss: 0.00002013
Iteration 22/1000 | Loss: 0.00002013
Iteration 23/1000 | Loss: 0.00002010
Iteration 24/1000 | Loss: 0.00002009
Iteration 25/1000 | Loss: 0.00002009
Iteration 26/1000 | Loss: 0.00002007
Iteration 27/1000 | Loss: 0.00002003
Iteration 28/1000 | Loss: 0.00002002
Iteration 29/1000 | Loss: 0.00002002
Iteration 30/1000 | Loss: 0.00002001
Iteration 31/1000 | Loss: 0.00002001
Iteration 32/1000 | Loss: 0.00002000
Iteration 33/1000 | Loss: 0.00001996
Iteration 34/1000 | Loss: 0.00001996
Iteration 35/1000 | Loss: 0.00001992
Iteration 36/1000 | Loss: 0.00001988
Iteration 37/1000 | Loss: 0.00001987
Iteration 38/1000 | Loss: 0.00001987
Iteration 39/1000 | Loss: 0.00001987
Iteration 40/1000 | Loss: 0.00001983
Iteration 41/1000 | Loss: 0.00001983
Iteration 42/1000 | Loss: 0.00001983
Iteration 43/1000 | Loss: 0.00001982
Iteration 44/1000 | Loss: 0.00001982
Iteration 45/1000 | Loss: 0.00001982
Iteration 46/1000 | Loss: 0.00001981
Iteration 47/1000 | Loss: 0.00001981
Iteration 48/1000 | Loss: 0.00001981
Iteration 49/1000 | Loss: 0.00001981
Iteration 50/1000 | Loss: 0.00001980
Iteration 51/1000 | Loss: 0.00001980
Iteration 52/1000 | Loss: 0.00001979
Iteration 53/1000 | Loss: 0.00001978
Iteration 54/1000 | Loss: 0.00001978
Iteration 55/1000 | Loss: 0.00001978
Iteration 56/1000 | Loss: 0.00001978
Iteration 57/1000 | Loss: 0.00001978
Iteration 58/1000 | Loss: 0.00001977
Iteration 59/1000 | Loss: 0.00001977
Iteration 60/1000 | Loss: 0.00001977
Iteration 61/1000 | Loss: 0.00001977
Iteration 62/1000 | Loss: 0.00001977
Iteration 63/1000 | Loss: 0.00001977
Iteration 64/1000 | Loss: 0.00001977
Iteration 65/1000 | Loss: 0.00001977
Iteration 66/1000 | Loss: 0.00001977
Iteration 67/1000 | Loss: 0.00001976
Iteration 68/1000 | Loss: 0.00001976
Iteration 69/1000 | Loss: 0.00001976
Iteration 70/1000 | Loss: 0.00001976
Iteration 71/1000 | Loss: 0.00001975
Iteration 72/1000 | Loss: 0.00001975
Iteration 73/1000 | Loss: 0.00001975
Iteration 74/1000 | Loss: 0.00001975
Iteration 75/1000 | Loss: 0.00001975
Iteration 76/1000 | Loss: 0.00001975
Iteration 77/1000 | Loss: 0.00001974
Iteration 78/1000 | Loss: 0.00001974
Iteration 79/1000 | Loss: 0.00001974
Iteration 80/1000 | Loss: 0.00001974
Iteration 81/1000 | Loss: 0.00001974
Iteration 82/1000 | Loss: 0.00001974
Iteration 83/1000 | Loss: 0.00001974
Iteration 84/1000 | Loss: 0.00001974
Iteration 85/1000 | Loss: 0.00001974
Iteration 86/1000 | Loss: 0.00001974
Iteration 87/1000 | Loss: 0.00001974
Iteration 88/1000 | Loss: 0.00001974
Iteration 89/1000 | Loss: 0.00001973
Iteration 90/1000 | Loss: 0.00001973
Iteration 91/1000 | Loss: 0.00001973
Iteration 92/1000 | Loss: 0.00001973
Iteration 93/1000 | Loss: 0.00001973
Iteration 94/1000 | Loss: 0.00001972
Iteration 95/1000 | Loss: 0.00001972
Iteration 96/1000 | Loss: 0.00001971
Iteration 97/1000 | Loss: 0.00001971
Iteration 98/1000 | Loss: 0.00001971
Iteration 99/1000 | Loss: 0.00001971
Iteration 100/1000 | Loss: 0.00001971
Iteration 101/1000 | Loss: 0.00001971
Iteration 102/1000 | Loss: 0.00001970
Iteration 103/1000 | Loss: 0.00001970
Iteration 104/1000 | Loss: 0.00001970
Iteration 105/1000 | Loss: 0.00001970
Iteration 106/1000 | Loss: 0.00001969
Iteration 107/1000 | Loss: 0.00001969
Iteration 108/1000 | Loss: 0.00001969
Iteration 109/1000 | Loss: 0.00001969
Iteration 110/1000 | Loss: 0.00001969
Iteration 111/1000 | Loss: 0.00001969
Iteration 112/1000 | Loss: 0.00001969
Iteration 113/1000 | Loss: 0.00001969
Iteration 114/1000 | Loss: 0.00001969
Iteration 115/1000 | Loss: 0.00001969
Iteration 116/1000 | Loss: 0.00001968
Iteration 117/1000 | Loss: 0.00001968
Iteration 118/1000 | Loss: 0.00001968
Iteration 119/1000 | Loss: 0.00001968
Iteration 120/1000 | Loss: 0.00001968
Iteration 121/1000 | Loss: 0.00001968
Iteration 122/1000 | Loss: 0.00001968
Iteration 123/1000 | Loss: 0.00001968
Iteration 124/1000 | Loss: 0.00001968
Iteration 125/1000 | Loss: 0.00001968
Iteration 126/1000 | Loss: 0.00001968
Iteration 127/1000 | Loss: 0.00001968
Iteration 128/1000 | Loss: 0.00001968
Iteration 129/1000 | Loss: 0.00001968
Iteration 130/1000 | Loss: 0.00001967
Iteration 131/1000 | Loss: 0.00001967
Iteration 132/1000 | Loss: 0.00001967
Iteration 133/1000 | Loss: 0.00001967
Iteration 134/1000 | Loss: 0.00001967
Iteration 135/1000 | Loss: 0.00001967
Iteration 136/1000 | Loss: 0.00001967
Iteration 137/1000 | Loss: 0.00001967
Iteration 138/1000 | Loss: 0.00001967
Iteration 139/1000 | Loss: 0.00001967
Iteration 140/1000 | Loss: 0.00001967
Iteration 141/1000 | Loss: 0.00001967
Iteration 142/1000 | Loss: 0.00001967
Iteration 143/1000 | Loss: 0.00001967
Iteration 144/1000 | Loss: 0.00001967
Iteration 145/1000 | Loss: 0.00001967
Iteration 146/1000 | Loss: 0.00001967
Iteration 147/1000 | Loss: 0.00001967
Iteration 148/1000 | Loss: 0.00001967
Iteration 149/1000 | Loss: 0.00001967
Iteration 150/1000 | Loss: 0.00001967
Iteration 151/1000 | Loss: 0.00001967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.9673178030643612e-05, 1.9673178030643612e-05, 1.9673178030643612e-05, 1.9673178030643612e-05, 1.9673178030643612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9673178030643612e-05

Optimization complete. Final v2v error: 3.748230457305908 mm

Highest mean error: 4.352784633636475 mm for frame 66

Lowest mean error: 3.482156991958618 mm for frame 186

Saving results

Total time: 42.86241102218628
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00569159
Iteration 2/25 | Loss: 0.00118244
Iteration 3/25 | Loss: 0.00085603
Iteration 4/25 | Loss: 0.00080683
Iteration 5/25 | Loss: 0.00079224
Iteration 6/25 | Loss: 0.00078899
Iteration 7/25 | Loss: 0.00078862
Iteration 8/25 | Loss: 0.00078862
Iteration 9/25 | Loss: 0.00078862
Iteration 10/25 | Loss: 0.00078862
Iteration 11/25 | Loss: 0.00078862
Iteration 12/25 | Loss: 0.00078862
Iteration 13/25 | Loss: 0.00078862
Iteration 14/25 | Loss: 0.00078862
Iteration 15/25 | Loss: 0.00078862
Iteration 16/25 | Loss: 0.00078862
Iteration 17/25 | Loss: 0.00078862
Iteration 18/25 | Loss: 0.00078862
Iteration 19/25 | Loss: 0.00078862
Iteration 20/25 | Loss: 0.00078862
Iteration 21/25 | Loss: 0.00078862
Iteration 22/25 | Loss: 0.00078862
Iteration 23/25 | Loss: 0.00078862
Iteration 24/25 | Loss: 0.00078862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007886248640716076, 0.0007886248640716076, 0.0007886248640716076, 0.0007886248640716076, 0.0007886248640716076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007886248640716076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11285245
Iteration 2/25 | Loss: 0.00025950
Iteration 3/25 | Loss: 0.00025948
Iteration 4/25 | Loss: 0.00025947
Iteration 5/25 | Loss: 0.00025947
Iteration 6/25 | Loss: 0.00025947
Iteration 7/25 | Loss: 0.00025947
Iteration 8/25 | Loss: 0.00025947
Iteration 9/25 | Loss: 0.00025947
Iteration 10/25 | Loss: 0.00025947
Iteration 11/25 | Loss: 0.00025947
Iteration 12/25 | Loss: 0.00025947
Iteration 13/25 | Loss: 0.00025947
Iteration 14/25 | Loss: 0.00025947
Iteration 15/25 | Loss: 0.00025947
Iteration 16/25 | Loss: 0.00025947
Iteration 17/25 | Loss: 0.00025947
Iteration 18/25 | Loss: 0.00025947
Iteration 19/25 | Loss: 0.00025947
Iteration 20/25 | Loss: 0.00025947
Iteration 21/25 | Loss: 0.00025947
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0002594722609501332, 0.0002594722609501332, 0.0002594722609501332, 0.0002594722609501332, 0.0002594722609501332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002594722609501332

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025947
Iteration 2/1000 | Loss: 0.00005526
Iteration 3/1000 | Loss: 0.00003592
Iteration 4/1000 | Loss: 0.00003272
Iteration 5/1000 | Loss: 0.00003043
Iteration 6/1000 | Loss: 0.00002909
Iteration 7/1000 | Loss: 0.00002796
Iteration 8/1000 | Loss: 0.00002716
Iteration 9/1000 | Loss: 0.00002662
Iteration 10/1000 | Loss: 0.00002627
Iteration 11/1000 | Loss: 0.00002598
Iteration 12/1000 | Loss: 0.00002585
Iteration 13/1000 | Loss: 0.00002584
Iteration 14/1000 | Loss: 0.00002576
Iteration 15/1000 | Loss: 0.00002573
Iteration 16/1000 | Loss: 0.00002573
Iteration 17/1000 | Loss: 0.00002572
Iteration 18/1000 | Loss: 0.00002572
Iteration 19/1000 | Loss: 0.00002572
Iteration 20/1000 | Loss: 0.00002570
Iteration 21/1000 | Loss: 0.00002570
Iteration 22/1000 | Loss: 0.00002569
Iteration 23/1000 | Loss: 0.00002568
Iteration 24/1000 | Loss: 0.00002568
Iteration 25/1000 | Loss: 0.00002567
Iteration 26/1000 | Loss: 0.00002567
Iteration 27/1000 | Loss: 0.00002566
Iteration 28/1000 | Loss: 0.00002565
Iteration 29/1000 | Loss: 0.00002564
Iteration 30/1000 | Loss: 0.00002564
Iteration 31/1000 | Loss: 0.00002564
Iteration 32/1000 | Loss: 0.00002563
Iteration 33/1000 | Loss: 0.00002562
Iteration 34/1000 | Loss: 0.00002561
Iteration 35/1000 | Loss: 0.00002560
Iteration 36/1000 | Loss: 0.00002560
Iteration 37/1000 | Loss: 0.00002559
Iteration 38/1000 | Loss: 0.00002559
Iteration 39/1000 | Loss: 0.00002559
Iteration 40/1000 | Loss: 0.00002558
Iteration 41/1000 | Loss: 0.00002558
Iteration 42/1000 | Loss: 0.00002558
Iteration 43/1000 | Loss: 0.00002558
Iteration 44/1000 | Loss: 0.00002558
Iteration 45/1000 | Loss: 0.00002558
Iteration 46/1000 | Loss: 0.00002558
Iteration 47/1000 | Loss: 0.00002558
Iteration 48/1000 | Loss: 0.00002558
Iteration 49/1000 | Loss: 0.00002558
Iteration 50/1000 | Loss: 0.00002557
Iteration 51/1000 | Loss: 0.00002557
Iteration 52/1000 | Loss: 0.00002557
Iteration 53/1000 | Loss: 0.00002556
Iteration 54/1000 | Loss: 0.00002556
Iteration 55/1000 | Loss: 0.00002555
Iteration 56/1000 | Loss: 0.00002555
Iteration 57/1000 | Loss: 0.00002555
Iteration 58/1000 | Loss: 0.00002554
Iteration 59/1000 | Loss: 0.00002554
Iteration 60/1000 | Loss: 0.00002554
Iteration 61/1000 | Loss: 0.00002554
Iteration 62/1000 | Loss: 0.00002553
Iteration 63/1000 | Loss: 0.00002553
Iteration 64/1000 | Loss: 0.00002551
Iteration 65/1000 | Loss: 0.00002551
Iteration 66/1000 | Loss: 0.00002551
Iteration 67/1000 | Loss: 0.00002550
Iteration 68/1000 | Loss: 0.00002550
Iteration 69/1000 | Loss: 0.00002550
Iteration 70/1000 | Loss: 0.00002550
Iteration 71/1000 | Loss: 0.00002550
Iteration 72/1000 | Loss: 0.00002550
Iteration 73/1000 | Loss: 0.00002550
Iteration 74/1000 | Loss: 0.00002550
Iteration 75/1000 | Loss: 0.00002550
Iteration 76/1000 | Loss: 0.00002550
Iteration 77/1000 | Loss: 0.00002550
Iteration 78/1000 | Loss: 0.00002550
Iteration 79/1000 | Loss: 0.00002550
Iteration 80/1000 | Loss: 0.00002550
Iteration 81/1000 | Loss: 0.00002550
Iteration 82/1000 | Loss: 0.00002550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [2.5499619368929416e-05, 2.5499619368929416e-05, 2.5499619368929416e-05, 2.5499619368929416e-05, 2.5499619368929416e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5499619368929416e-05

Optimization complete. Final v2v error: 4.298873424530029 mm

Highest mean error: 4.943944454193115 mm for frame 231

Lowest mean error: 3.9602224826812744 mm for frame 75

Saving results

Total time: 38.31185603141785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421280
Iteration 2/25 | Loss: 0.00089034
Iteration 3/25 | Loss: 0.00079630
Iteration 4/25 | Loss: 0.00077386
Iteration 5/25 | Loss: 0.00077023
Iteration 6/25 | Loss: 0.00076973
Iteration 7/25 | Loss: 0.00076973
Iteration 8/25 | Loss: 0.00076973
Iteration 9/25 | Loss: 0.00076973
Iteration 10/25 | Loss: 0.00076973
Iteration 11/25 | Loss: 0.00076973
Iteration 12/25 | Loss: 0.00076973
Iteration 13/25 | Loss: 0.00076973
Iteration 14/25 | Loss: 0.00076973
Iteration 15/25 | Loss: 0.00076973
Iteration 16/25 | Loss: 0.00076973
Iteration 17/25 | Loss: 0.00076973
Iteration 18/25 | Loss: 0.00076973
Iteration 19/25 | Loss: 0.00076973
Iteration 20/25 | Loss: 0.00076973
Iteration 21/25 | Loss: 0.00076973
Iteration 22/25 | Loss: 0.00076973
Iteration 23/25 | Loss: 0.00076973
Iteration 24/25 | Loss: 0.00076973
Iteration 25/25 | Loss: 0.00076973

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.08025026
Iteration 2/25 | Loss: 0.00024006
Iteration 3/25 | Loss: 0.00024004
Iteration 4/25 | Loss: 0.00024004
Iteration 5/25 | Loss: 0.00024004
Iteration 6/25 | Loss: 0.00024004
Iteration 7/25 | Loss: 0.00024004
Iteration 8/25 | Loss: 0.00024004
Iteration 9/25 | Loss: 0.00024004
Iteration 10/25 | Loss: 0.00024004
Iteration 11/25 | Loss: 0.00024004
Iteration 12/25 | Loss: 0.00024004
Iteration 13/25 | Loss: 0.00024004
Iteration 14/25 | Loss: 0.00024004
Iteration 15/25 | Loss: 0.00024004
Iteration 16/25 | Loss: 0.00024004
Iteration 17/25 | Loss: 0.00024004
Iteration 18/25 | Loss: 0.00024004
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00024003854196052998, 0.00024003854196052998, 0.00024003854196052998, 0.00024003854196052998, 0.00024003854196052998]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024003854196052998

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024004
Iteration 2/1000 | Loss: 0.00006189
Iteration 3/1000 | Loss: 0.00004127
Iteration 4/1000 | Loss: 0.00003782
Iteration 5/1000 | Loss: 0.00003491
Iteration 6/1000 | Loss: 0.00003288
Iteration 7/1000 | Loss: 0.00003183
Iteration 8/1000 | Loss: 0.00003098
Iteration 9/1000 | Loss: 0.00003068
Iteration 10/1000 | Loss: 0.00003055
Iteration 11/1000 | Loss: 0.00003033
Iteration 12/1000 | Loss: 0.00003018
Iteration 13/1000 | Loss: 0.00003008
Iteration 14/1000 | Loss: 0.00003008
Iteration 15/1000 | Loss: 0.00003002
Iteration 16/1000 | Loss: 0.00003002
Iteration 17/1000 | Loss: 0.00003001
Iteration 18/1000 | Loss: 0.00003001
Iteration 19/1000 | Loss: 0.00003000
Iteration 20/1000 | Loss: 0.00002998
Iteration 21/1000 | Loss: 0.00002996
Iteration 22/1000 | Loss: 0.00002996
Iteration 23/1000 | Loss: 0.00002995
Iteration 24/1000 | Loss: 0.00002993
Iteration 25/1000 | Loss: 0.00002992
Iteration 26/1000 | Loss: 0.00002991
Iteration 27/1000 | Loss: 0.00002991
Iteration 28/1000 | Loss: 0.00002991
Iteration 29/1000 | Loss: 0.00002990
Iteration 30/1000 | Loss: 0.00002989
Iteration 31/1000 | Loss: 0.00002989
Iteration 32/1000 | Loss: 0.00002988
Iteration 33/1000 | Loss: 0.00002987
Iteration 34/1000 | Loss: 0.00002987
Iteration 35/1000 | Loss: 0.00002987
Iteration 36/1000 | Loss: 0.00002986
Iteration 37/1000 | Loss: 0.00002985
Iteration 38/1000 | Loss: 0.00002984
Iteration 39/1000 | Loss: 0.00002982
Iteration 40/1000 | Loss: 0.00002982
Iteration 41/1000 | Loss: 0.00002982
Iteration 42/1000 | Loss: 0.00002982
Iteration 43/1000 | Loss: 0.00002980
Iteration 44/1000 | Loss: 0.00002980
Iteration 45/1000 | Loss: 0.00002979
Iteration 46/1000 | Loss: 0.00002979
Iteration 47/1000 | Loss: 0.00002979
Iteration 48/1000 | Loss: 0.00002979
Iteration 49/1000 | Loss: 0.00002979
Iteration 50/1000 | Loss: 0.00002979
Iteration 51/1000 | Loss: 0.00002979
Iteration 52/1000 | Loss: 0.00002979
Iteration 53/1000 | Loss: 0.00002979
Iteration 54/1000 | Loss: 0.00002979
Iteration 55/1000 | Loss: 0.00002979
Iteration 56/1000 | Loss: 0.00002978
Iteration 57/1000 | Loss: 0.00002978
Iteration 58/1000 | Loss: 0.00002978
Iteration 59/1000 | Loss: 0.00002978
Iteration 60/1000 | Loss: 0.00002978
Iteration 61/1000 | Loss: 0.00002978
Iteration 62/1000 | Loss: 0.00002977
Iteration 63/1000 | Loss: 0.00002977
Iteration 64/1000 | Loss: 0.00002977
Iteration 65/1000 | Loss: 0.00002977
Iteration 66/1000 | Loss: 0.00002976
Iteration 67/1000 | Loss: 0.00002976
Iteration 68/1000 | Loss: 0.00002976
Iteration 69/1000 | Loss: 0.00002976
Iteration 70/1000 | Loss: 0.00002976
Iteration 71/1000 | Loss: 0.00002976
Iteration 72/1000 | Loss: 0.00002975
Iteration 73/1000 | Loss: 0.00002975
Iteration 74/1000 | Loss: 0.00002975
Iteration 75/1000 | Loss: 0.00002975
Iteration 76/1000 | Loss: 0.00002975
Iteration 77/1000 | Loss: 0.00002975
Iteration 78/1000 | Loss: 0.00002974
Iteration 79/1000 | Loss: 0.00002974
Iteration 80/1000 | Loss: 0.00002974
Iteration 81/1000 | Loss: 0.00002974
Iteration 82/1000 | Loss: 0.00002974
Iteration 83/1000 | Loss: 0.00002974
Iteration 84/1000 | Loss: 0.00002974
Iteration 85/1000 | Loss: 0.00002974
Iteration 86/1000 | Loss: 0.00002974
Iteration 87/1000 | Loss: 0.00002974
Iteration 88/1000 | Loss: 0.00002974
Iteration 89/1000 | Loss: 0.00002974
Iteration 90/1000 | Loss: 0.00002974
Iteration 91/1000 | Loss: 0.00002973
Iteration 92/1000 | Loss: 0.00002973
Iteration 93/1000 | Loss: 0.00002973
Iteration 94/1000 | Loss: 0.00002973
Iteration 95/1000 | Loss: 0.00002973
Iteration 96/1000 | Loss: 0.00002973
Iteration 97/1000 | Loss: 0.00002973
Iteration 98/1000 | Loss: 0.00002973
Iteration 99/1000 | Loss: 0.00002973
Iteration 100/1000 | Loss: 0.00002972
Iteration 101/1000 | Loss: 0.00002972
Iteration 102/1000 | Loss: 0.00002972
Iteration 103/1000 | Loss: 0.00002972
Iteration 104/1000 | Loss: 0.00002972
Iteration 105/1000 | Loss: 0.00002972
Iteration 106/1000 | Loss: 0.00002972
Iteration 107/1000 | Loss: 0.00002972
Iteration 108/1000 | Loss: 0.00002972
Iteration 109/1000 | Loss: 0.00002972
Iteration 110/1000 | Loss: 0.00002971
Iteration 111/1000 | Loss: 0.00002971
Iteration 112/1000 | Loss: 0.00002971
Iteration 113/1000 | Loss: 0.00002971
Iteration 114/1000 | Loss: 0.00002971
Iteration 115/1000 | Loss: 0.00002971
Iteration 116/1000 | Loss: 0.00002971
Iteration 117/1000 | Loss: 0.00002971
Iteration 118/1000 | Loss: 0.00002971
Iteration 119/1000 | Loss: 0.00002971
Iteration 120/1000 | Loss: 0.00002971
Iteration 121/1000 | Loss: 0.00002971
Iteration 122/1000 | Loss: 0.00002970
Iteration 123/1000 | Loss: 0.00002970
Iteration 124/1000 | Loss: 0.00002970
Iteration 125/1000 | Loss: 0.00002970
Iteration 126/1000 | Loss: 0.00002970
Iteration 127/1000 | Loss: 0.00002969
Iteration 128/1000 | Loss: 0.00002969
Iteration 129/1000 | Loss: 0.00002969
Iteration 130/1000 | Loss: 0.00002969
Iteration 131/1000 | Loss: 0.00002969
Iteration 132/1000 | Loss: 0.00002969
Iteration 133/1000 | Loss: 0.00002969
Iteration 134/1000 | Loss: 0.00002969
Iteration 135/1000 | Loss: 0.00002969
Iteration 136/1000 | Loss: 0.00002969
Iteration 137/1000 | Loss: 0.00002969
Iteration 138/1000 | Loss: 0.00002969
Iteration 139/1000 | Loss: 0.00002969
Iteration 140/1000 | Loss: 0.00002969
Iteration 141/1000 | Loss: 0.00002969
Iteration 142/1000 | Loss: 0.00002969
Iteration 143/1000 | Loss: 0.00002969
Iteration 144/1000 | Loss: 0.00002969
Iteration 145/1000 | Loss: 0.00002969
Iteration 146/1000 | Loss: 0.00002969
Iteration 147/1000 | Loss: 0.00002969
Iteration 148/1000 | Loss: 0.00002969
Iteration 149/1000 | Loss: 0.00002969
Iteration 150/1000 | Loss: 0.00002969
Iteration 151/1000 | Loss: 0.00002969
Iteration 152/1000 | Loss: 0.00002969
Iteration 153/1000 | Loss: 0.00002969
Iteration 154/1000 | Loss: 0.00002969
Iteration 155/1000 | Loss: 0.00002969
Iteration 156/1000 | Loss: 0.00002969
Iteration 157/1000 | Loss: 0.00002969
Iteration 158/1000 | Loss: 0.00002969
Iteration 159/1000 | Loss: 0.00002969
Iteration 160/1000 | Loss: 0.00002969
Iteration 161/1000 | Loss: 0.00002969
Iteration 162/1000 | Loss: 0.00002969
Iteration 163/1000 | Loss: 0.00002969
Iteration 164/1000 | Loss: 0.00002969
Iteration 165/1000 | Loss: 0.00002969
Iteration 166/1000 | Loss: 0.00002969
Iteration 167/1000 | Loss: 0.00002969
Iteration 168/1000 | Loss: 0.00002969
Iteration 169/1000 | Loss: 0.00002969
Iteration 170/1000 | Loss: 0.00002969
Iteration 171/1000 | Loss: 0.00002969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.969293564092368e-05, 2.969293564092368e-05, 2.969293564092368e-05, 2.969293564092368e-05, 2.969293564092368e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.969293564092368e-05

Optimization complete. Final v2v error: 4.6211113929748535 mm

Highest mean error: 5.502213954925537 mm for frame 239

Lowest mean error: 4.284619331359863 mm for frame 42

Saving results

Total time: 43.27043104171753
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878135
Iteration 2/25 | Loss: 0.00151819
Iteration 3/25 | Loss: 0.00095162
Iteration 4/25 | Loss: 0.00085555
Iteration 5/25 | Loss: 0.00084360
Iteration 6/25 | Loss: 0.00084472
Iteration 7/25 | Loss: 0.00082123
Iteration 8/25 | Loss: 0.00079968
Iteration 9/25 | Loss: 0.00078972
Iteration 10/25 | Loss: 0.00078266
Iteration 11/25 | Loss: 0.00077896
Iteration 12/25 | Loss: 0.00077775
Iteration 13/25 | Loss: 0.00077862
Iteration 14/25 | Loss: 0.00077435
Iteration 15/25 | Loss: 0.00077296
Iteration 16/25 | Loss: 0.00077254
Iteration 17/25 | Loss: 0.00077254
Iteration 18/25 | Loss: 0.00077254
Iteration 19/25 | Loss: 0.00077254
Iteration 20/25 | Loss: 0.00077254
Iteration 21/25 | Loss: 0.00077254
Iteration 22/25 | Loss: 0.00077254
Iteration 23/25 | Loss: 0.00077254
Iteration 24/25 | Loss: 0.00077254
Iteration 25/25 | Loss: 0.00077254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.64025640
Iteration 2/25 | Loss: 0.00024855
Iteration 3/25 | Loss: 0.00024853
Iteration 4/25 | Loss: 0.00024853
Iteration 5/25 | Loss: 0.00024853
Iteration 6/25 | Loss: 0.00024853
Iteration 7/25 | Loss: 0.00024853
Iteration 8/25 | Loss: 0.00024853
Iteration 9/25 | Loss: 0.00024853
Iteration 10/25 | Loss: 0.00024853
Iteration 11/25 | Loss: 0.00024853
Iteration 12/25 | Loss: 0.00024853
Iteration 13/25 | Loss: 0.00024853
Iteration 14/25 | Loss: 0.00024853
Iteration 15/25 | Loss: 0.00024853
Iteration 16/25 | Loss: 0.00024853
Iteration 17/25 | Loss: 0.00024853
Iteration 18/25 | Loss: 0.00024853
Iteration 19/25 | Loss: 0.00024853
Iteration 20/25 | Loss: 0.00024853
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00024852834758348763, 0.00024852834758348763, 0.00024852834758348763, 0.00024852834758348763, 0.00024852834758348763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024852834758348763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024853
Iteration 2/1000 | Loss: 0.00004797
Iteration 3/1000 | Loss: 0.00003529
Iteration 4/1000 | Loss: 0.00003287
Iteration 5/1000 | Loss: 0.00003091
Iteration 6/1000 | Loss: 0.00002977
Iteration 7/1000 | Loss: 0.00002896
Iteration 8/1000 | Loss: 0.00002846
Iteration 9/1000 | Loss: 0.00002813
Iteration 10/1000 | Loss: 0.00002796
Iteration 11/1000 | Loss: 0.00002781
Iteration 12/1000 | Loss: 0.00002775
Iteration 13/1000 | Loss: 0.00002774
Iteration 14/1000 | Loss: 0.00002772
Iteration 15/1000 | Loss: 0.00002772
Iteration 16/1000 | Loss: 0.00002770
Iteration 17/1000 | Loss: 0.00002770
Iteration 18/1000 | Loss: 0.00002770
Iteration 19/1000 | Loss: 0.00002770
Iteration 20/1000 | Loss: 0.00002769
Iteration 21/1000 | Loss: 0.00002769
Iteration 22/1000 | Loss: 0.00002769
Iteration 23/1000 | Loss: 0.00002769
Iteration 24/1000 | Loss: 0.00002769
Iteration 25/1000 | Loss: 0.00002769
Iteration 26/1000 | Loss: 0.00002769
Iteration 27/1000 | Loss: 0.00002768
Iteration 28/1000 | Loss: 0.00002768
Iteration 29/1000 | Loss: 0.00002767
Iteration 30/1000 | Loss: 0.00002767
Iteration 31/1000 | Loss: 0.00002767
Iteration 32/1000 | Loss: 0.00002766
Iteration 33/1000 | Loss: 0.00002766
Iteration 34/1000 | Loss: 0.00002766
Iteration 35/1000 | Loss: 0.00002765
Iteration 36/1000 | Loss: 0.00002765
Iteration 37/1000 | Loss: 0.00002765
Iteration 38/1000 | Loss: 0.00002762
Iteration 39/1000 | Loss: 0.00002760
Iteration 40/1000 | Loss: 0.00002759
Iteration 41/1000 | Loss: 0.00002758
Iteration 42/1000 | Loss: 0.00002756
Iteration 43/1000 | Loss: 0.00002756
Iteration 44/1000 | Loss: 0.00002756
Iteration 45/1000 | Loss: 0.00002756
Iteration 46/1000 | Loss: 0.00002756
Iteration 47/1000 | Loss: 0.00002755
Iteration 48/1000 | Loss: 0.00002755
Iteration 49/1000 | Loss: 0.00002754
Iteration 50/1000 | Loss: 0.00002754
Iteration 51/1000 | Loss: 0.00002752
Iteration 52/1000 | Loss: 0.00002752
Iteration 53/1000 | Loss: 0.00002752
Iteration 54/1000 | Loss: 0.00002752
Iteration 55/1000 | Loss: 0.00002752
Iteration 56/1000 | Loss: 0.00002752
Iteration 57/1000 | Loss: 0.00002752
Iteration 58/1000 | Loss: 0.00002752
Iteration 59/1000 | Loss: 0.00002752
Iteration 60/1000 | Loss: 0.00002752
Iteration 61/1000 | Loss: 0.00002752
Iteration 62/1000 | Loss: 0.00002752
Iteration 63/1000 | Loss: 0.00002752
Iteration 64/1000 | Loss: 0.00002752
Iteration 65/1000 | Loss: 0.00002752
Iteration 66/1000 | Loss: 0.00002752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [2.7518997740116902e-05, 2.7518997740116902e-05, 2.7518997740116902e-05, 2.7518997740116902e-05, 2.7518997740116902e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7518997740116902e-05

Optimization complete. Final v2v error: 4.480255603790283 mm

Highest mean error: 4.996384143829346 mm for frame 19

Lowest mean error: 4.058077335357666 mm for frame 0

Saving results

Total time: 56.05540633201599
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103998
Iteration 2/25 | Loss: 0.00182382
Iteration 3/25 | Loss: 0.00129989
Iteration 4/25 | Loss: 0.00118482
Iteration 5/25 | Loss: 0.00114416
Iteration 6/25 | Loss: 0.00113611
Iteration 7/25 | Loss: 0.00115937
Iteration 8/25 | Loss: 0.00110264
Iteration 9/25 | Loss: 0.00106234
Iteration 10/25 | Loss: 0.00104066
Iteration 11/25 | Loss: 0.00103868
Iteration 12/25 | Loss: 0.00103330
Iteration 13/25 | Loss: 0.00103178
Iteration 14/25 | Loss: 0.00103355
Iteration 15/25 | Loss: 0.00103773
Iteration 16/25 | Loss: 0.00103781
Iteration 17/25 | Loss: 0.00102480
Iteration 18/25 | Loss: 0.00101792
Iteration 19/25 | Loss: 0.00102164
Iteration 20/25 | Loss: 0.00101528
Iteration 21/25 | Loss: 0.00100321
Iteration 22/25 | Loss: 0.00098349
Iteration 23/25 | Loss: 0.00096986
Iteration 24/25 | Loss: 0.00095884
Iteration 25/25 | Loss: 0.00095148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15884566
Iteration 2/25 | Loss: 0.00066522
Iteration 3/25 | Loss: 0.00066516
Iteration 4/25 | Loss: 0.00066516
Iteration 5/25 | Loss: 0.00066516
Iteration 6/25 | Loss: 0.00066516
Iteration 7/25 | Loss: 0.00066516
Iteration 8/25 | Loss: 0.00066516
Iteration 9/25 | Loss: 0.00066516
Iteration 10/25 | Loss: 0.00066516
Iteration 11/25 | Loss: 0.00066516
Iteration 12/25 | Loss: 0.00066516
Iteration 13/25 | Loss: 0.00066516
Iteration 14/25 | Loss: 0.00066516
Iteration 15/25 | Loss: 0.00066516
Iteration 16/25 | Loss: 0.00066516
Iteration 17/25 | Loss: 0.00066516
Iteration 18/25 | Loss: 0.00066516
Iteration 19/25 | Loss: 0.00066516
Iteration 20/25 | Loss: 0.00066516
Iteration 21/25 | Loss: 0.00066516
Iteration 22/25 | Loss: 0.00066516
Iteration 23/25 | Loss: 0.00066516
Iteration 24/25 | Loss: 0.00066516
Iteration 25/25 | Loss: 0.00066516

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066516
Iteration 2/1000 | Loss: 0.00106921
Iteration 3/1000 | Loss: 0.00010689
Iteration 4/1000 | Loss: 0.00010721
Iteration 5/1000 | Loss: 0.00025159
Iteration 6/1000 | Loss: 0.00007277
Iteration 7/1000 | Loss: 0.00006593
Iteration 8/1000 | Loss: 0.00012632
Iteration 9/1000 | Loss: 0.00006225
Iteration 10/1000 | Loss: 0.00012702
Iteration 11/1000 | Loss: 0.00011019
Iteration 12/1000 | Loss: 0.00009904
Iteration 13/1000 | Loss: 0.00005254
Iteration 14/1000 | Loss: 0.00009784
Iteration 15/1000 | Loss: 0.00012449
Iteration 16/1000 | Loss: 0.00010391
Iteration 17/1000 | Loss: 0.00011655
Iteration 18/1000 | Loss: 0.00015004
Iteration 19/1000 | Loss: 0.00016298
Iteration 20/1000 | Loss: 0.00011165
Iteration 21/1000 | Loss: 0.00008535
Iteration 22/1000 | Loss: 0.00009419
Iteration 23/1000 | Loss: 0.00015545
Iteration 24/1000 | Loss: 0.00011719
Iteration 25/1000 | Loss: 0.00015120
Iteration 26/1000 | Loss: 0.00005604
Iteration 27/1000 | Loss: 0.00017694
Iteration 28/1000 | Loss: 0.00007166
Iteration 29/1000 | Loss: 0.00015764
Iteration 30/1000 | Loss: 0.00015501
Iteration 31/1000 | Loss: 0.00014975
Iteration 32/1000 | Loss: 0.00013488
Iteration 33/1000 | Loss: 0.00015304
Iteration 34/1000 | Loss: 0.00005780
Iteration 35/1000 | Loss: 0.00005864
Iteration 36/1000 | Loss: 0.00004449
Iteration 37/1000 | Loss: 0.00004414
Iteration 38/1000 | Loss: 0.00004239
Iteration 39/1000 | Loss: 0.00004258
Iteration 40/1000 | Loss: 0.00004274
Iteration 41/1000 | Loss: 0.00004897
Iteration 42/1000 | Loss: 0.00004765
Iteration 43/1000 | Loss: 0.00004201
Iteration 44/1000 | Loss: 0.00006986
Iteration 45/1000 | Loss: 0.00006166
Iteration 46/1000 | Loss: 0.00004349
Iteration 47/1000 | Loss: 0.00005311
Iteration 48/1000 | Loss: 0.00004305
Iteration 49/1000 | Loss: 0.00005060
Iteration 50/1000 | Loss: 0.00004206
Iteration 51/1000 | Loss: 0.00004872
Iteration 52/1000 | Loss: 0.00004599
Iteration 53/1000 | Loss: 0.00004272
Iteration 54/1000 | Loss: 0.00004148
Iteration 55/1000 | Loss: 0.00004234
Iteration 56/1000 | Loss: 0.00004527
Iteration 57/1000 | Loss: 0.00003857
Iteration 58/1000 | Loss: 0.00004849
Iteration 59/1000 | Loss: 0.00004301
Iteration 60/1000 | Loss: 0.00003930
Iteration 61/1000 | Loss: 0.00004426
Iteration 62/1000 | Loss: 0.00003763
Iteration 63/1000 | Loss: 0.00003873
Iteration 64/1000 | Loss: 0.00003647
Iteration 65/1000 | Loss: 0.00004248
Iteration 66/1000 | Loss: 0.00004103
Iteration 67/1000 | Loss: 0.00004435
Iteration 68/1000 | Loss: 0.00005126
Iteration 69/1000 | Loss: 0.00003595
Iteration 70/1000 | Loss: 0.00003367
Iteration 71/1000 | Loss: 0.00003277
Iteration 72/1000 | Loss: 0.00003194
Iteration 73/1000 | Loss: 0.00003170
Iteration 74/1000 | Loss: 0.00014189
Iteration 75/1000 | Loss: 0.00003646
Iteration 76/1000 | Loss: 0.00003394
Iteration 77/1000 | Loss: 0.00014064
Iteration 78/1000 | Loss: 0.00005383
Iteration 79/1000 | Loss: 0.00005247
Iteration 80/1000 | Loss: 0.00005001
Iteration 81/1000 | Loss: 0.00003823
Iteration 82/1000 | Loss: 0.00003502
Iteration 83/1000 | Loss: 0.00003359
Iteration 84/1000 | Loss: 0.00003249
Iteration 85/1000 | Loss: 0.00003164
Iteration 86/1000 | Loss: 0.00003122
Iteration 87/1000 | Loss: 0.00003107
Iteration 88/1000 | Loss: 0.00003102
Iteration 89/1000 | Loss: 0.00003101
Iteration 90/1000 | Loss: 0.00003099
Iteration 91/1000 | Loss: 0.00003097
Iteration 92/1000 | Loss: 0.00003095
Iteration 93/1000 | Loss: 0.00003091
Iteration 94/1000 | Loss: 0.00003086
Iteration 95/1000 | Loss: 0.00003083
Iteration 96/1000 | Loss: 0.00003082
Iteration 97/1000 | Loss: 0.00003082
Iteration 98/1000 | Loss: 0.00003082
Iteration 99/1000 | Loss: 0.00003081
Iteration 100/1000 | Loss: 0.00003081
Iteration 101/1000 | Loss: 0.00003075
Iteration 102/1000 | Loss: 0.00003074
Iteration 103/1000 | Loss: 0.00003074
Iteration 104/1000 | Loss: 0.00003073
Iteration 105/1000 | Loss: 0.00003071
Iteration 106/1000 | Loss: 0.00003071
Iteration 107/1000 | Loss: 0.00003071
Iteration 108/1000 | Loss: 0.00003071
Iteration 109/1000 | Loss: 0.00003071
Iteration 110/1000 | Loss: 0.00003071
Iteration 111/1000 | Loss: 0.00003071
Iteration 112/1000 | Loss: 0.00003070
Iteration 113/1000 | Loss: 0.00003069
Iteration 114/1000 | Loss: 0.00003069
Iteration 115/1000 | Loss: 0.00003068
Iteration 116/1000 | Loss: 0.00003068
Iteration 117/1000 | Loss: 0.00003068
Iteration 118/1000 | Loss: 0.00003067
Iteration 119/1000 | Loss: 0.00003067
Iteration 120/1000 | Loss: 0.00003067
Iteration 121/1000 | Loss: 0.00003067
Iteration 122/1000 | Loss: 0.00003067
Iteration 123/1000 | Loss: 0.00003067
Iteration 124/1000 | Loss: 0.00003067
Iteration 125/1000 | Loss: 0.00003067
Iteration 126/1000 | Loss: 0.00003067
Iteration 127/1000 | Loss: 0.00003067
Iteration 128/1000 | Loss: 0.00003066
Iteration 129/1000 | Loss: 0.00003066
Iteration 130/1000 | Loss: 0.00003065
Iteration 131/1000 | Loss: 0.00003065
Iteration 132/1000 | Loss: 0.00003065
Iteration 133/1000 | Loss: 0.00003065
Iteration 134/1000 | Loss: 0.00003065
Iteration 135/1000 | Loss: 0.00003065
Iteration 136/1000 | Loss: 0.00003065
Iteration 137/1000 | Loss: 0.00003064
Iteration 138/1000 | Loss: 0.00003064
Iteration 139/1000 | Loss: 0.00003064
Iteration 140/1000 | Loss: 0.00003064
Iteration 141/1000 | Loss: 0.00003064
Iteration 142/1000 | Loss: 0.00003064
Iteration 143/1000 | Loss: 0.00003064
Iteration 144/1000 | Loss: 0.00003064
Iteration 145/1000 | Loss: 0.00003064
Iteration 146/1000 | Loss: 0.00003064
Iteration 147/1000 | Loss: 0.00003064
Iteration 148/1000 | Loss: 0.00003064
Iteration 149/1000 | Loss: 0.00003064
Iteration 150/1000 | Loss: 0.00003064
Iteration 151/1000 | Loss: 0.00003064
Iteration 152/1000 | Loss: 0.00003064
Iteration 153/1000 | Loss: 0.00003064
Iteration 154/1000 | Loss: 0.00003064
Iteration 155/1000 | Loss: 0.00003064
Iteration 156/1000 | Loss: 0.00003064
Iteration 157/1000 | Loss: 0.00003064
Iteration 158/1000 | Loss: 0.00003064
Iteration 159/1000 | Loss: 0.00003064
Iteration 160/1000 | Loss: 0.00003064
Iteration 161/1000 | Loss: 0.00003064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [3.063640542677604e-05, 3.063640542677604e-05, 3.063640542677604e-05, 3.063640542677604e-05, 3.063640542677604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.063640542677604e-05

Optimization complete. Final v2v error: 4.7886552810668945 mm

Highest mean error: 6.074417591094971 mm for frame 136

Lowest mean error: 4.026890277862549 mm for frame 173

Saving results

Total time: 206.73965406417847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01160535
Iteration 2/25 | Loss: 0.01160535
Iteration 3/25 | Loss: 0.00328941
Iteration 4/25 | Loss: 0.00193398
Iteration 5/25 | Loss: 0.00172732
Iteration 6/25 | Loss: 0.00165119
Iteration 7/25 | Loss: 0.00136544
Iteration 8/25 | Loss: 0.00128126
Iteration 9/25 | Loss: 0.00126220
Iteration 10/25 | Loss: 0.00126089
Iteration 11/25 | Loss: 0.00125431
Iteration 12/25 | Loss: 0.00125372
Iteration 13/25 | Loss: 0.00125048
Iteration 14/25 | Loss: 0.00124986
Iteration 15/25 | Loss: 0.00124963
Iteration 16/25 | Loss: 0.00124946
Iteration 17/25 | Loss: 0.00124936
Iteration 18/25 | Loss: 0.00124928
Iteration 19/25 | Loss: 0.00124915
Iteration 20/25 | Loss: 0.00124900
Iteration 21/25 | Loss: 0.00124889
Iteration 22/25 | Loss: 0.00124887
Iteration 23/25 | Loss: 0.00124887
Iteration 24/25 | Loss: 0.00124887
Iteration 25/25 | Loss: 0.00124887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18899024
Iteration 2/25 | Loss: 0.00267949
Iteration 3/25 | Loss: 0.00267948
Iteration 4/25 | Loss: 0.00267948
Iteration 5/25 | Loss: 0.00267948
Iteration 6/25 | Loss: 0.00267948
Iteration 7/25 | Loss: 0.00267948
Iteration 8/25 | Loss: 0.00267948
Iteration 9/25 | Loss: 0.00267948
Iteration 10/25 | Loss: 0.00267948
Iteration 11/25 | Loss: 0.00267948
Iteration 12/25 | Loss: 0.00267948
Iteration 13/25 | Loss: 0.00267948
Iteration 14/25 | Loss: 0.00267948
Iteration 15/25 | Loss: 0.00267948
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0026794762816280127, 0.0026794762816280127, 0.0026794762816280127, 0.0026794762816280127, 0.0026794762816280127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026794762816280127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00267948
Iteration 2/1000 | Loss: 0.00048226
Iteration 3/1000 | Loss: 0.00054769
Iteration 4/1000 | Loss: 0.00153689
Iteration 5/1000 | Loss: 0.00171756
Iteration 6/1000 | Loss: 0.00330950
Iteration 7/1000 | Loss: 0.00269156
Iteration 8/1000 | Loss: 0.00054810
Iteration 9/1000 | Loss: 0.00029550
Iteration 10/1000 | Loss: 0.00023822
Iteration 11/1000 | Loss: 0.00021555
Iteration 12/1000 | Loss: 0.00020563
Iteration 13/1000 | Loss: 0.00044270
Iteration 14/1000 | Loss: 0.00055292
Iteration 15/1000 | Loss: 0.00033715
Iteration 16/1000 | Loss: 0.00041971
Iteration 17/1000 | Loss: 0.00033754
Iteration 18/1000 | Loss: 0.00138096
Iteration 19/1000 | Loss: 0.00023228
Iteration 20/1000 | Loss: 0.00027384
Iteration 21/1000 | Loss: 0.00021255
Iteration 22/1000 | Loss: 0.00018585
Iteration 23/1000 | Loss: 0.00018268
Iteration 24/1000 | Loss: 0.00057893
Iteration 25/1000 | Loss: 0.00043664
Iteration 26/1000 | Loss: 0.00038610
Iteration 27/1000 | Loss: 0.00035571
Iteration 28/1000 | Loss: 0.00050212
Iteration 29/1000 | Loss: 0.00035570
Iteration 30/1000 | Loss: 0.00103664
Iteration 31/1000 | Loss: 0.00032219
Iteration 32/1000 | Loss: 0.00025823
Iteration 33/1000 | Loss: 0.00021845
Iteration 34/1000 | Loss: 0.00082912
Iteration 35/1000 | Loss: 0.00078700
Iteration 36/1000 | Loss: 0.00199781
Iteration 37/1000 | Loss: 0.00101533
Iteration 38/1000 | Loss: 0.00085890
Iteration 39/1000 | Loss: 0.00030892
Iteration 40/1000 | Loss: 0.00065018
Iteration 41/1000 | Loss: 0.00200410
Iteration 42/1000 | Loss: 0.00095877
Iteration 43/1000 | Loss: 0.00054491
Iteration 44/1000 | Loss: 0.00026594
Iteration 45/1000 | Loss: 0.00018416
Iteration 46/1000 | Loss: 0.00080293
Iteration 47/1000 | Loss: 0.00116708
Iteration 48/1000 | Loss: 0.00054240
Iteration 49/1000 | Loss: 0.00026563
Iteration 50/1000 | Loss: 0.00073050
Iteration 51/1000 | Loss: 0.00056003
Iteration 52/1000 | Loss: 0.00045268
Iteration 53/1000 | Loss: 0.00041376
Iteration 54/1000 | Loss: 0.00045423
Iteration 55/1000 | Loss: 0.00085057
Iteration 56/1000 | Loss: 0.00045121
Iteration 57/1000 | Loss: 0.00166088
Iteration 58/1000 | Loss: 0.00084737
Iteration 59/1000 | Loss: 0.00222990
Iteration 60/1000 | Loss: 0.00422921
Iteration 61/1000 | Loss: 0.00114601
Iteration 62/1000 | Loss: 0.00174682
Iteration 63/1000 | Loss: 0.00047083
Iteration 64/1000 | Loss: 0.00149396
Iteration 65/1000 | Loss: 0.00133868
Iteration 66/1000 | Loss: 0.00454297
Iteration 67/1000 | Loss: 0.00073548
Iteration 68/1000 | Loss: 0.00092032
Iteration 69/1000 | Loss: 0.00065603
Iteration 70/1000 | Loss: 0.00146087
Iteration 71/1000 | Loss: 0.00099880
Iteration 72/1000 | Loss: 0.00114327
Iteration 73/1000 | Loss: 0.00096976
Iteration 74/1000 | Loss: 0.00116353
Iteration 75/1000 | Loss: 0.00090035
Iteration 76/1000 | Loss: 0.00215070
Iteration 77/1000 | Loss: 0.00131758
Iteration 78/1000 | Loss: 0.00397249
Iteration 79/1000 | Loss: 0.00194463
Iteration 80/1000 | Loss: 0.00203055
Iteration 81/1000 | Loss: 0.00443071
Iteration 82/1000 | Loss: 0.00390698
Iteration 83/1000 | Loss: 0.00429393
Iteration 84/1000 | Loss: 0.00382142
Iteration 85/1000 | Loss: 0.00335002
Iteration 86/1000 | Loss: 0.00259128
Iteration 87/1000 | Loss: 0.00282549
Iteration 88/1000 | Loss: 0.00278565
Iteration 89/1000 | Loss: 0.00246854
Iteration 90/1000 | Loss: 0.00068557
Iteration 91/1000 | Loss: 0.00065136
Iteration 92/1000 | Loss: 0.00051188
Iteration 93/1000 | Loss: 0.00111241
Iteration 94/1000 | Loss: 0.00041417
Iteration 95/1000 | Loss: 0.00042458
Iteration 96/1000 | Loss: 0.00054270
Iteration 97/1000 | Loss: 0.00035169
Iteration 98/1000 | Loss: 0.00159012
Iteration 99/1000 | Loss: 0.00264291
Iteration 100/1000 | Loss: 0.00089179
Iteration 101/1000 | Loss: 0.00042890
Iteration 102/1000 | Loss: 0.00031980
Iteration 103/1000 | Loss: 0.00029085
Iteration 104/1000 | Loss: 0.00011630
Iteration 105/1000 | Loss: 0.00029821
Iteration 106/1000 | Loss: 0.00030975
Iteration 107/1000 | Loss: 0.00033501
Iteration 108/1000 | Loss: 0.00032292
Iteration 109/1000 | Loss: 0.00040988
Iteration 110/1000 | Loss: 0.00029500
Iteration 111/1000 | Loss: 0.00027352
Iteration 112/1000 | Loss: 0.00068766
Iteration 113/1000 | Loss: 0.00068172
Iteration 114/1000 | Loss: 0.00074083
Iteration 115/1000 | Loss: 0.00091005
Iteration 116/1000 | Loss: 0.00082404
Iteration 117/1000 | Loss: 0.00064031
Iteration 118/1000 | Loss: 0.00015084
Iteration 119/1000 | Loss: 0.00164732
Iteration 120/1000 | Loss: 0.00018947
Iteration 121/1000 | Loss: 0.00018479
Iteration 122/1000 | Loss: 0.00017061
Iteration 123/1000 | Loss: 0.00017289
Iteration 124/1000 | Loss: 0.00007070
Iteration 125/1000 | Loss: 0.00024527
Iteration 126/1000 | Loss: 0.00006843
Iteration 127/1000 | Loss: 0.00006183
Iteration 128/1000 | Loss: 0.00020270
Iteration 129/1000 | Loss: 0.00006542
Iteration 130/1000 | Loss: 0.00006102
Iteration 131/1000 | Loss: 0.00023446
Iteration 132/1000 | Loss: 0.00046007
Iteration 133/1000 | Loss: 0.00017622
Iteration 134/1000 | Loss: 0.00006630
Iteration 135/1000 | Loss: 0.00006154
Iteration 136/1000 | Loss: 0.00005836
Iteration 137/1000 | Loss: 0.00029384
Iteration 138/1000 | Loss: 0.00046848
Iteration 139/1000 | Loss: 0.00057878
Iteration 140/1000 | Loss: 0.00040893
Iteration 141/1000 | Loss: 0.00022857
Iteration 142/1000 | Loss: 0.00011813
Iteration 143/1000 | Loss: 0.00013680
Iteration 144/1000 | Loss: 0.00012435
Iteration 145/1000 | Loss: 0.00012670
Iteration 146/1000 | Loss: 0.00014547
Iteration 147/1000 | Loss: 0.00041432
Iteration 148/1000 | Loss: 0.00050186
Iteration 149/1000 | Loss: 0.00059073
Iteration 150/1000 | Loss: 0.00067194
Iteration 151/1000 | Loss: 0.00054358
Iteration 152/1000 | Loss: 0.00026313
Iteration 153/1000 | Loss: 0.00014030
Iteration 154/1000 | Loss: 0.00013327
Iteration 155/1000 | Loss: 0.00013441
Iteration 156/1000 | Loss: 0.00013191
Iteration 157/1000 | Loss: 0.00032222
Iteration 158/1000 | Loss: 0.00016105
Iteration 159/1000 | Loss: 0.00012978
Iteration 160/1000 | Loss: 0.00006277
Iteration 161/1000 | Loss: 0.00029179
Iteration 162/1000 | Loss: 0.00014766
Iteration 163/1000 | Loss: 0.00029522
Iteration 164/1000 | Loss: 0.00006466
Iteration 165/1000 | Loss: 0.00005859
Iteration 166/1000 | Loss: 0.00005350
Iteration 167/1000 | Loss: 0.00004997
Iteration 168/1000 | Loss: 0.00073817
Iteration 169/1000 | Loss: 0.00069776
Iteration 170/1000 | Loss: 0.00147019
Iteration 171/1000 | Loss: 0.00045326
Iteration 172/1000 | Loss: 0.00005324
Iteration 173/1000 | Loss: 0.00073628
Iteration 174/1000 | Loss: 0.00027345
Iteration 175/1000 | Loss: 0.00036054
Iteration 176/1000 | Loss: 0.00049280
Iteration 177/1000 | Loss: 0.00113849
Iteration 178/1000 | Loss: 0.00040279
Iteration 179/1000 | Loss: 0.00057849
Iteration 180/1000 | Loss: 0.00026175
Iteration 181/1000 | Loss: 0.00005415
Iteration 182/1000 | Loss: 0.00004925
Iteration 183/1000 | Loss: 0.00004715
Iteration 184/1000 | Loss: 0.00004613
Iteration 185/1000 | Loss: 0.00019649
Iteration 186/1000 | Loss: 0.00009301
Iteration 187/1000 | Loss: 0.00004577
Iteration 188/1000 | Loss: 0.00004491
Iteration 189/1000 | Loss: 0.00004363
Iteration 190/1000 | Loss: 0.00004310
Iteration 191/1000 | Loss: 0.00016501
Iteration 192/1000 | Loss: 0.00005086
Iteration 193/1000 | Loss: 0.00004672
Iteration 194/1000 | Loss: 0.00004403
Iteration 195/1000 | Loss: 0.00004219
Iteration 196/1000 | Loss: 0.00004141
Iteration 197/1000 | Loss: 0.00004093
Iteration 198/1000 | Loss: 0.00004068
Iteration 199/1000 | Loss: 0.00004054
Iteration 200/1000 | Loss: 0.00004048
Iteration 201/1000 | Loss: 0.00004047
Iteration 202/1000 | Loss: 0.00004043
Iteration 203/1000 | Loss: 0.00004038
Iteration 204/1000 | Loss: 0.00004037
Iteration 205/1000 | Loss: 0.00004037
Iteration 206/1000 | Loss: 0.00004036
Iteration 207/1000 | Loss: 0.00004036
Iteration 208/1000 | Loss: 0.00004036
Iteration 209/1000 | Loss: 0.00004036
Iteration 210/1000 | Loss: 0.00004035
Iteration 211/1000 | Loss: 0.00004035
Iteration 212/1000 | Loss: 0.00004035
Iteration 213/1000 | Loss: 0.00004035
Iteration 214/1000 | Loss: 0.00004035
Iteration 215/1000 | Loss: 0.00004035
Iteration 216/1000 | Loss: 0.00004034
Iteration 217/1000 | Loss: 0.00004034
Iteration 218/1000 | Loss: 0.00004034
Iteration 219/1000 | Loss: 0.00004034
Iteration 220/1000 | Loss: 0.00004033
Iteration 221/1000 | Loss: 0.00004033
Iteration 222/1000 | Loss: 0.00004033
Iteration 223/1000 | Loss: 0.00004033
Iteration 224/1000 | Loss: 0.00004033
Iteration 225/1000 | Loss: 0.00004033
Iteration 226/1000 | Loss: 0.00004033
Iteration 227/1000 | Loss: 0.00004033
Iteration 228/1000 | Loss: 0.00004033
Iteration 229/1000 | Loss: 0.00004033
Iteration 230/1000 | Loss: 0.00004032
Iteration 231/1000 | Loss: 0.00004032
Iteration 232/1000 | Loss: 0.00004032
Iteration 233/1000 | Loss: 0.00004031
Iteration 234/1000 | Loss: 0.00004031
Iteration 235/1000 | Loss: 0.00004031
Iteration 236/1000 | Loss: 0.00004031
Iteration 237/1000 | Loss: 0.00004031
Iteration 238/1000 | Loss: 0.00004031
Iteration 239/1000 | Loss: 0.00004031
Iteration 240/1000 | Loss: 0.00004031
Iteration 241/1000 | Loss: 0.00004031
Iteration 242/1000 | Loss: 0.00004030
Iteration 243/1000 | Loss: 0.00004030
Iteration 244/1000 | Loss: 0.00004030
Iteration 245/1000 | Loss: 0.00004030
Iteration 246/1000 | Loss: 0.00004030
Iteration 247/1000 | Loss: 0.00004030
Iteration 248/1000 | Loss: 0.00004029
Iteration 249/1000 | Loss: 0.00004029
Iteration 250/1000 | Loss: 0.00004029
Iteration 251/1000 | Loss: 0.00004029
Iteration 252/1000 | Loss: 0.00004028
Iteration 253/1000 | Loss: 0.00004028
Iteration 254/1000 | Loss: 0.00004028
Iteration 255/1000 | Loss: 0.00004028
Iteration 256/1000 | Loss: 0.00004028
Iteration 257/1000 | Loss: 0.00004028
Iteration 258/1000 | Loss: 0.00004028
Iteration 259/1000 | Loss: 0.00004028
Iteration 260/1000 | Loss: 0.00004027
Iteration 261/1000 | Loss: 0.00004027
Iteration 262/1000 | Loss: 0.00004027
Iteration 263/1000 | Loss: 0.00004027
Iteration 264/1000 | Loss: 0.00004027
Iteration 265/1000 | Loss: 0.00004027
Iteration 266/1000 | Loss: 0.00004027
Iteration 267/1000 | Loss: 0.00004027
Iteration 268/1000 | Loss: 0.00004026
Iteration 269/1000 | Loss: 0.00004026
Iteration 270/1000 | Loss: 0.00004026
Iteration 271/1000 | Loss: 0.00004026
Iteration 272/1000 | Loss: 0.00004026
Iteration 273/1000 | Loss: 0.00004026
Iteration 274/1000 | Loss: 0.00004026
Iteration 275/1000 | Loss: 0.00004026
Iteration 276/1000 | Loss: 0.00004026
Iteration 277/1000 | Loss: 0.00004026
Iteration 278/1000 | Loss: 0.00004026
Iteration 279/1000 | Loss: 0.00004025
Iteration 280/1000 | Loss: 0.00004025
Iteration 281/1000 | Loss: 0.00004025
Iteration 282/1000 | Loss: 0.00004025
Iteration 283/1000 | Loss: 0.00004025
Iteration 284/1000 | Loss: 0.00004025
Iteration 285/1000 | Loss: 0.00004025
Iteration 286/1000 | Loss: 0.00004025
Iteration 287/1000 | Loss: 0.00004025
Iteration 288/1000 | Loss: 0.00004025
Iteration 289/1000 | Loss: 0.00004025
Iteration 290/1000 | Loss: 0.00004025
Iteration 291/1000 | Loss: 0.00004025
Iteration 292/1000 | Loss: 0.00004025
Iteration 293/1000 | Loss: 0.00004025
Iteration 294/1000 | Loss: 0.00004025
Iteration 295/1000 | Loss: 0.00004025
Iteration 296/1000 | Loss: 0.00004024
Iteration 297/1000 | Loss: 0.00004024
Iteration 298/1000 | Loss: 0.00004024
Iteration 299/1000 | Loss: 0.00004024
Iteration 300/1000 | Loss: 0.00004024
Iteration 301/1000 | Loss: 0.00004024
Iteration 302/1000 | Loss: 0.00004024
Iteration 303/1000 | Loss: 0.00004024
Iteration 304/1000 | Loss: 0.00004024
Iteration 305/1000 | Loss: 0.00004024
Iteration 306/1000 | Loss: 0.00004024
Iteration 307/1000 | Loss: 0.00004024
Iteration 308/1000 | Loss: 0.00004024
Iteration 309/1000 | Loss: 0.00004023
Iteration 310/1000 | Loss: 0.00004023
Iteration 311/1000 | Loss: 0.00004023
Iteration 312/1000 | Loss: 0.00004023
Iteration 313/1000 | Loss: 0.00004023
Iteration 314/1000 | Loss: 0.00004023
Iteration 315/1000 | Loss: 0.00004023
Iteration 316/1000 | Loss: 0.00004023
Iteration 317/1000 | Loss: 0.00004023
Iteration 318/1000 | Loss: 0.00004023
Iteration 319/1000 | Loss: 0.00004023
Iteration 320/1000 | Loss: 0.00004023
Iteration 321/1000 | Loss: 0.00004023
Iteration 322/1000 | Loss: 0.00004023
Iteration 323/1000 | Loss: 0.00004022
Iteration 324/1000 | Loss: 0.00004022
Iteration 325/1000 | Loss: 0.00004022
Iteration 326/1000 | Loss: 0.00004022
Iteration 327/1000 | Loss: 0.00004021
Iteration 328/1000 | Loss: 0.00004021
Iteration 329/1000 | Loss: 0.00004021
Iteration 330/1000 | Loss: 0.00004021
Iteration 331/1000 | Loss: 0.00004021
Iteration 332/1000 | Loss: 0.00004021
Iteration 333/1000 | Loss: 0.00004021
Iteration 334/1000 | Loss: 0.00004021
Iteration 335/1000 | Loss: 0.00004020
Iteration 336/1000 | Loss: 0.00004020
Iteration 337/1000 | Loss: 0.00004020
Iteration 338/1000 | Loss: 0.00004020
Iteration 339/1000 | Loss: 0.00004020
Iteration 340/1000 | Loss: 0.00004020
Iteration 341/1000 | Loss: 0.00004020
Iteration 342/1000 | Loss: 0.00004020
Iteration 343/1000 | Loss: 0.00004020
Iteration 344/1000 | Loss: 0.00004020
Iteration 345/1000 | Loss: 0.00004019
Iteration 346/1000 | Loss: 0.00004019
Iteration 347/1000 | Loss: 0.00004019
Iteration 348/1000 | Loss: 0.00004019
Iteration 349/1000 | Loss: 0.00004019
Iteration 350/1000 | Loss: 0.00004019
Iteration 351/1000 | Loss: 0.00004019
Iteration 352/1000 | Loss: 0.00004019
Iteration 353/1000 | Loss: 0.00004019
Iteration 354/1000 | Loss: 0.00004019
Iteration 355/1000 | Loss: 0.00004019
Iteration 356/1000 | Loss: 0.00004019
Iteration 357/1000 | Loss: 0.00004018
Iteration 358/1000 | Loss: 0.00004018
Iteration 359/1000 | Loss: 0.00004018
Iteration 360/1000 | Loss: 0.00004018
Iteration 361/1000 | Loss: 0.00004018
Iteration 362/1000 | Loss: 0.00004018
Iteration 363/1000 | Loss: 0.00004018
Iteration 364/1000 | Loss: 0.00004018
Iteration 365/1000 | Loss: 0.00004018
Iteration 366/1000 | Loss: 0.00004018
Iteration 367/1000 | Loss: 0.00004018
Iteration 368/1000 | Loss: 0.00004018
Iteration 369/1000 | Loss: 0.00004018
Iteration 370/1000 | Loss: 0.00004018
Iteration 371/1000 | Loss: 0.00004018
Iteration 372/1000 | Loss: 0.00004017
Iteration 373/1000 | Loss: 0.00004017
Iteration 374/1000 | Loss: 0.00004017
Iteration 375/1000 | Loss: 0.00004017
Iteration 376/1000 | Loss: 0.00004017
Iteration 377/1000 | Loss: 0.00004017
Iteration 378/1000 | Loss: 0.00004017
Iteration 379/1000 | Loss: 0.00004017
Iteration 380/1000 | Loss: 0.00004017
Iteration 381/1000 | Loss: 0.00004017
Iteration 382/1000 | Loss: 0.00004017
Iteration 383/1000 | Loss: 0.00004016
Iteration 384/1000 | Loss: 0.00004016
Iteration 385/1000 | Loss: 0.00004016
Iteration 386/1000 | Loss: 0.00004016
Iteration 387/1000 | Loss: 0.00004016
Iteration 388/1000 | Loss: 0.00004016
Iteration 389/1000 | Loss: 0.00004016
Iteration 390/1000 | Loss: 0.00004016
Iteration 391/1000 | Loss: 0.00004016
Iteration 392/1000 | Loss: 0.00004016
Iteration 393/1000 | Loss: 0.00004016
Iteration 394/1000 | Loss: 0.00004016
Iteration 395/1000 | Loss: 0.00004016
Iteration 396/1000 | Loss: 0.00004016
Iteration 397/1000 | Loss: 0.00004016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 397. Stopping optimization.
Last 5 losses: [4.016037200926803e-05, 4.016037200926803e-05, 4.016037200926803e-05, 4.016037200926803e-05, 4.016037200926803e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.016037200926803e-05

Optimization complete. Final v2v error: 5.038874626159668 mm

Highest mean error: 13.310553550720215 mm for frame 125

Lowest mean error: 4.643077850341797 mm for frame 138

Saving results

Total time: 375.7753314971924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402678
Iteration 2/25 | Loss: 0.00083675
Iteration 3/25 | Loss: 0.00071431
Iteration 4/25 | Loss: 0.00068785
Iteration 5/25 | Loss: 0.00068188
Iteration 6/25 | Loss: 0.00068042
Iteration 7/25 | Loss: 0.00068027
Iteration 8/25 | Loss: 0.00068027
Iteration 9/25 | Loss: 0.00068027
Iteration 10/25 | Loss: 0.00068027
Iteration 11/25 | Loss: 0.00068027
Iteration 12/25 | Loss: 0.00068027
Iteration 13/25 | Loss: 0.00068027
Iteration 14/25 | Loss: 0.00068027
Iteration 15/25 | Loss: 0.00068027
Iteration 16/25 | Loss: 0.00068027
Iteration 17/25 | Loss: 0.00068027
Iteration 18/25 | Loss: 0.00068027
Iteration 19/25 | Loss: 0.00068027
Iteration 20/25 | Loss: 0.00068027
Iteration 21/25 | Loss: 0.00068027
Iteration 22/25 | Loss: 0.00068027
Iteration 23/25 | Loss: 0.00068027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006802662392146885, 0.0006802662392146885, 0.0006802662392146885, 0.0006802662392146885, 0.0006802662392146885]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006802662392146885

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43848908
Iteration 2/25 | Loss: 0.00029070
Iteration 3/25 | Loss: 0.00029070
Iteration 4/25 | Loss: 0.00029070
Iteration 5/25 | Loss: 0.00029070
Iteration 6/25 | Loss: 0.00029070
Iteration 7/25 | Loss: 0.00029070
Iteration 8/25 | Loss: 0.00029070
Iteration 9/25 | Loss: 0.00029070
Iteration 10/25 | Loss: 0.00029070
Iteration 11/25 | Loss: 0.00029070
Iteration 12/25 | Loss: 0.00029070
Iteration 13/25 | Loss: 0.00029070
Iteration 14/25 | Loss: 0.00029070
Iteration 15/25 | Loss: 0.00029070
Iteration 16/25 | Loss: 0.00029070
Iteration 17/25 | Loss: 0.00029070
Iteration 18/25 | Loss: 0.00029070
Iteration 19/25 | Loss: 0.00029070
Iteration 20/25 | Loss: 0.00029070
Iteration 21/25 | Loss: 0.00029070
Iteration 22/25 | Loss: 0.00029070
Iteration 23/25 | Loss: 0.00029070
Iteration 24/25 | Loss: 0.00029070
Iteration 25/25 | Loss: 0.00029070

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029070
Iteration 2/1000 | Loss: 0.00004429
Iteration 3/1000 | Loss: 0.00002510
Iteration 4/1000 | Loss: 0.00001925
Iteration 5/1000 | Loss: 0.00001733
Iteration 6/1000 | Loss: 0.00001637
Iteration 7/1000 | Loss: 0.00001555
Iteration 8/1000 | Loss: 0.00001525
Iteration 9/1000 | Loss: 0.00001500
Iteration 10/1000 | Loss: 0.00001499
Iteration 11/1000 | Loss: 0.00001494
Iteration 12/1000 | Loss: 0.00001494
Iteration 13/1000 | Loss: 0.00001491
Iteration 14/1000 | Loss: 0.00001490
Iteration 15/1000 | Loss: 0.00001488
Iteration 16/1000 | Loss: 0.00001488
Iteration 17/1000 | Loss: 0.00001485
Iteration 18/1000 | Loss: 0.00001485
Iteration 19/1000 | Loss: 0.00001480
Iteration 20/1000 | Loss: 0.00001480
Iteration 21/1000 | Loss: 0.00001476
Iteration 22/1000 | Loss: 0.00001475
Iteration 23/1000 | Loss: 0.00001475
Iteration 24/1000 | Loss: 0.00001475
Iteration 25/1000 | Loss: 0.00001475
Iteration 26/1000 | Loss: 0.00001474
Iteration 27/1000 | Loss: 0.00001474
Iteration 28/1000 | Loss: 0.00001474
Iteration 29/1000 | Loss: 0.00001473
Iteration 30/1000 | Loss: 0.00001471
Iteration 31/1000 | Loss: 0.00001471
Iteration 32/1000 | Loss: 0.00001471
Iteration 33/1000 | Loss: 0.00001471
Iteration 34/1000 | Loss: 0.00001471
Iteration 35/1000 | Loss: 0.00001470
Iteration 36/1000 | Loss: 0.00001470
Iteration 37/1000 | Loss: 0.00001470
Iteration 38/1000 | Loss: 0.00001470
Iteration 39/1000 | Loss: 0.00001470
Iteration 40/1000 | Loss: 0.00001468
Iteration 41/1000 | Loss: 0.00001468
Iteration 42/1000 | Loss: 0.00001468
Iteration 43/1000 | Loss: 0.00001468
Iteration 44/1000 | Loss: 0.00001468
Iteration 45/1000 | Loss: 0.00001468
Iteration 46/1000 | Loss: 0.00001468
Iteration 47/1000 | Loss: 0.00001468
Iteration 48/1000 | Loss: 0.00001468
Iteration 49/1000 | Loss: 0.00001468
Iteration 50/1000 | Loss: 0.00001468
Iteration 51/1000 | Loss: 0.00001468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 51. Stopping optimization.
Last 5 losses: [1.4677039871457964e-05, 1.4677039871457964e-05, 1.4677039871457964e-05, 1.4677039871457964e-05, 1.4677039871457964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4677039871457964e-05

Optimization complete. Final v2v error: 3.297269344329834 mm

Highest mean error: 3.9396915435791016 mm for frame 11

Lowest mean error: 2.9925873279571533 mm for frame 89

Saving results

Total time: 28.099865674972534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790832
Iteration 2/25 | Loss: 0.00105485
Iteration 3/25 | Loss: 0.00092542
Iteration 4/25 | Loss: 0.00089420
Iteration 5/25 | Loss: 0.00088603
Iteration 6/25 | Loss: 0.00088463
Iteration 7/25 | Loss: 0.00088454
Iteration 8/25 | Loss: 0.00088454
Iteration 9/25 | Loss: 0.00088454
Iteration 10/25 | Loss: 0.00088454
Iteration 11/25 | Loss: 0.00088454
Iteration 12/25 | Loss: 0.00088454
Iteration 13/25 | Loss: 0.00088454
Iteration 14/25 | Loss: 0.00088454
Iteration 15/25 | Loss: 0.00088454
Iteration 16/25 | Loss: 0.00088454
Iteration 17/25 | Loss: 0.00088438
Iteration 18/25 | Loss: 0.00088438
Iteration 19/25 | Loss: 0.00088438
Iteration 20/25 | Loss: 0.00088438
Iteration 21/25 | Loss: 0.00088438
Iteration 22/25 | Loss: 0.00088438
Iteration 23/25 | Loss: 0.00088438
Iteration 24/25 | Loss: 0.00088438
Iteration 25/25 | Loss: 0.00088438

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36575973
Iteration 2/25 | Loss: 0.00039056
Iteration 3/25 | Loss: 0.00039048
Iteration 4/25 | Loss: 0.00039048
Iteration 5/25 | Loss: 0.00039048
Iteration 6/25 | Loss: 0.00039048
Iteration 7/25 | Loss: 0.00039048
Iteration 8/25 | Loss: 0.00039047
Iteration 9/25 | Loss: 0.00039047
Iteration 10/25 | Loss: 0.00039047
Iteration 11/25 | Loss: 0.00039047
Iteration 12/25 | Loss: 0.00039047
Iteration 13/25 | Loss: 0.00039047
Iteration 14/25 | Loss: 0.00039047
Iteration 15/25 | Loss: 0.00039047
Iteration 16/25 | Loss: 0.00039047
Iteration 17/25 | Loss: 0.00039047
Iteration 18/25 | Loss: 0.00039047
Iteration 19/25 | Loss: 0.00039047
Iteration 20/25 | Loss: 0.00039047
Iteration 21/25 | Loss: 0.00039047
Iteration 22/25 | Loss: 0.00039047
Iteration 23/25 | Loss: 0.00039047
Iteration 24/25 | Loss: 0.00039047
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00039047427708283067, 0.00039047427708283067, 0.00039047427708283067, 0.00039047427708283067, 0.00039047427708283067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039047427708283067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039047
Iteration 2/1000 | Loss: 0.00008526
Iteration 3/1000 | Loss: 0.00005433
Iteration 4/1000 | Loss: 0.00004712
Iteration 5/1000 | Loss: 0.00004338
Iteration 6/1000 | Loss: 0.00004107
Iteration 7/1000 | Loss: 0.00003964
Iteration 8/1000 | Loss: 0.00003856
Iteration 9/1000 | Loss: 0.00003800
Iteration 10/1000 | Loss: 0.00003762
Iteration 11/1000 | Loss: 0.00003737
Iteration 12/1000 | Loss: 0.00003705
Iteration 13/1000 | Loss: 0.00003704
Iteration 14/1000 | Loss: 0.00003703
Iteration 15/1000 | Loss: 0.00003703
Iteration 16/1000 | Loss: 0.00003694
Iteration 17/1000 | Loss: 0.00003691
Iteration 18/1000 | Loss: 0.00003691
Iteration 19/1000 | Loss: 0.00003691
Iteration 20/1000 | Loss: 0.00003691
Iteration 21/1000 | Loss: 0.00003691
Iteration 22/1000 | Loss: 0.00003691
Iteration 23/1000 | Loss: 0.00003691
Iteration 24/1000 | Loss: 0.00003691
Iteration 25/1000 | Loss: 0.00003691
Iteration 26/1000 | Loss: 0.00003691
Iteration 27/1000 | Loss: 0.00003691
Iteration 28/1000 | Loss: 0.00003691
Iteration 29/1000 | Loss: 0.00003690
Iteration 30/1000 | Loss: 0.00003689
Iteration 31/1000 | Loss: 0.00003688
Iteration 32/1000 | Loss: 0.00003686
Iteration 33/1000 | Loss: 0.00003685
Iteration 34/1000 | Loss: 0.00003684
Iteration 35/1000 | Loss: 0.00003684
Iteration 36/1000 | Loss: 0.00003683
Iteration 37/1000 | Loss: 0.00003683
Iteration 38/1000 | Loss: 0.00003683
Iteration 39/1000 | Loss: 0.00003682
Iteration 40/1000 | Loss: 0.00003682
Iteration 41/1000 | Loss: 0.00003682
Iteration 42/1000 | Loss: 0.00003682
Iteration 43/1000 | Loss: 0.00003681
Iteration 44/1000 | Loss: 0.00003681
Iteration 45/1000 | Loss: 0.00003680
Iteration 46/1000 | Loss: 0.00003680
Iteration 47/1000 | Loss: 0.00003680
Iteration 48/1000 | Loss: 0.00003679
Iteration 49/1000 | Loss: 0.00003679
Iteration 50/1000 | Loss: 0.00003679
Iteration 51/1000 | Loss: 0.00003679
Iteration 52/1000 | Loss: 0.00003679
Iteration 53/1000 | Loss: 0.00003679
Iteration 54/1000 | Loss: 0.00003679
Iteration 55/1000 | Loss: 0.00003679
Iteration 56/1000 | Loss: 0.00003679
Iteration 57/1000 | Loss: 0.00003679
Iteration 58/1000 | Loss: 0.00003678
Iteration 59/1000 | Loss: 0.00003678
Iteration 60/1000 | Loss: 0.00003678
Iteration 61/1000 | Loss: 0.00003678
Iteration 62/1000 | Loss: 0.00003678
Iteration 63/1000 | Loss: 0.00003677
Iteration 64/1000 | Loss: 0.00003677
Iteration 65/1000 | Loss: 0.00003677
Iteration 66/1000 | Loss: 0.00003677
Iteration 67/1000 | Loss: 0.00003677
Iteration 68/1000 | Loss: 0.00003677
Iteration 69/1000 | Loss: 0.00003677
Iteration 70/1000 | Loss: 0.00003677
Iteration 71/1000 | Loss: 0.00003676
Iteration 72/1000 | Loss: 0.00003676
Iteration 73/1000 | Loss: 0.00003675
Iteration 74/1000 | Loss: 0.00003675
Iteration 75/1000 | Loss: 0.00003675
Iteration 76/1000 | Loss: 0.00003675
Iteration 77/1000 | Loss: 0.00003675
Iteration 78/1000 | Loss: 0.00003675
Iteration 79/1000 | Loss: 0.00003675
Iteration 80/1000 | Loss: 0.00003675
Iteration 81/1000 | Loss: 0.00003675
Iteration 82/1000 | Loss: 0.00003675
Iteration 83/1000 | Loss: 0.00003675
Iteration 84/1000 | Loss: 0.00003675
Iteration 85/1000 | Loss: 0.00003675
Iteration 86/1000 | Loss: 0.00003675
Iteration 87/1000 | Loss: 0.00003675
Iteration 88/1000 | Loss: 0.00003675
Iteration 89/1000 | Loss: 0.00003675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [3.674567778944038e-05, 3.674567778944038e-05, 3.674567778944038e-05, 3.674567778944038e-05, 3.674567778944038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.674567778944038e-05

Optimization complete. Final v2v error: 5.181600570678711 mm

Highest mean error: 5.724785804748535 mm for frame 42

Lowest mean error: 4.583606719970703 mm for frame 10

Saving results

Total time: 33.35013222694397
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00607711
Iteration 2/25 | Loss: 0.00108290
Iteration 3/25 | Loss: 0.00090450
Iteration 4/25 | Loss: 0.00086767
Iteration 5/25 | Loss: 0.00085022
Iteration 6/25 | Loss: 0.00084482
Iteration 7/25 | Loss: 0.00084277
Iteration 8/25 | Loss: 0.00084255
Iteration 9/25 | Loss: 0.00084255
Iteration 10/25 | Loss: 0.00084255
Iteration 11/25 | Loss: 0.00084255
Iteration 12/25 | Loss: 0.00084255
Iteration 13/25 | Loss: 0.00084255
Iteration 14/25 | Loss: 0.00084255
Iteration 15/25 | Loss: 0.00084255
Iteration 16/25 | Loss: 0.00084255
Iteration 17/25 | Loss: 0.00084255
Iteration 18/25 | Loss: 0.00084255
Iteration 19/25 | Loss: 0.00084255
Iteration 20/25 | Loss: 0.00084255
Iteration 21/25 | Loss: 0.00084255
Iteration 22/25 | Loss: 0.00084255
Iteration 23/25 | Loss: 0.00084255
Iteration 24/25 | Loss: 0.00084255
Iteration 25/25 | Loss: 0.00084255

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03314352
Iteration 2/25 | Loss: 0.00027111
Iteration 3/25 | Loss: 0.00027103
Iteration 4/25 | Loss: 0.00027103
Iteration 5/25 | Loss: 0.00027103
Iteration 6/25 | Loss: 0.00027103
Iteration 7/25 | Loss: 0.00027103
Iteration 8/25 | Loss: 0.00027103
Iteration 9/25 | Loss: 0.00027103
Iteration 10/25 | Loss: 0.00027103
Iteration 11/25 | Loss: 0.00027103
Iteration 12/25 | Loss: 0.00027103
Iteration 13/25 | Loss: 0.00027103
Iteration 14/25 | Loss: 0.00027103
Iteration 15/25 | Loss: 0.00027103
Iteration 16/25 | Loss: 0.00027103
Iteration 17/25 | Loss: 0.00027103
Iteration 18/25 | Loss: 0.00027103
Iteration 19/25 | Loss: 0.00027103
Iteration 20/25 | Loss: 0.00027103
Iteration 21/25 | Loss: 0.00027103
Iteration 22/25 | Loss: 0.00027103
Iteration 23/25 | Loss: 0.00027103
Iteration 24/25 | Loss: 0.00027103
Iteration 25/25 | Loss: 0.00027103

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027103
Iteration 2/1000 | Loss: 0.00009872
Iteration 3/1000 | Loss: 0.00005695
Iteration 4/1000 | Loss: 0.00004238
Iteration 5/1000 | Loss: 0.00003831
Iteration 6/1000 | Loss: 0.00003704
Iteration 7/1000 | Loss: 0.00003598
Iteration 8/1000 | Loss: 0.00003490
Iteration 9/1000 | Loss: 0.00003431
Iteration 10/1000 | Loss: 0.00003396
Iteration 11/1000 | Loss: 0.00003361
Iteration 12/1000 | Loss: 0.00003340
Iteration 13/1000 | Loss: 0.00003322
Iteration 14/1000 | Loss: 0.00003302
Iteration 15/1000 | Loss: 0.00003300
Iteration 16/1000 | Loss: 0.00003285
Iteration 17/1000 | Loss: 0.00003281
Iteration 18/1000 | Loss: 0.00003281
Iteration 19/1000 | Loss: 0.00003281
Iteration 20/1000 | Loss: 0.00003280
Iteration 21/1000 | Loss: 0.00003280
Iteration 22/1000 | Loss: 0.00003278
Iteration 23/1000 | Loss: 0.00003273
Iteration 24/1000 | Loss: 0.00003265
Iteration 25/1000 | Loss: 0.00003264
Iteration 26/1000 | Loss: 0.00003262
Iteration 27/1000 | Loss: 0.00003261
Iteration 28/1000 | Loss: 0.00003261
Iteration 29/1000 | Loss: 0.00003261
Iteration 30/1000 | Loss: 0.00003261
Iteration 31/1000 | Loss: 0.00003261
Iteration 32/1000 | Loss: 0.00003261
Iteration 33/1000 | Loss: 0.00003261
Iteration 34/1000 | Loss: 0.00003261
Iteration 35/1000 | Loss: 0.00003261
Iteration 36/1000 | Loss: 0.00003261
Iteration 37/1000 | Loss: 0.00003261
Iteration 38/1000 | Loss: 0.00003261
Iteration 39/1000 | Loss: 0.00003259
Iteration 40/1000 | Loss: 0.00003256
Iteration 41/1000 | Loss: 0.00003256
Iteration 42/1000 | Loss: 0.00003256
Iteration 43/1000 | Loss: 0.00003256
Iteration 44/1000 | Loss: 0.00003256
Iteration 45/1000 | Loss: 0.00003256
Iteration 46/1000 | Loss: 0.00003256
Iteration 47/1000 | Loss: 0.00003256
Iteration 48/1000 | Loss: 0.00003255
Iteration 49/1000 | Loss: 0.00003255
Iteration 50/1000 | Loss: 0.00003255
Iteration 51/1000 | Loss: 0.00003255
Iteration 52/1000 | Loss: 0.00003255
Iteration 53/1000 | Loss: 0.00003255
Iteration 54/1000 | Loss: 0.00003255
Iteration 55/1000 | Loss: 0.00003255
Iteration 56/1000 | Loss: 0.00003254
Iteration 57/1000 | Loss: 0.00003251
Iteration 58/1000 | Loss: 0.00003251
Iteration 59/1000 | Loss: 0.00003251
Iteration 60/1000 | Loss: 0.00003250
Iteration 61/1000 | Loss: 0.00003250
Iteration 62/1000 | Loss: 0.00003250
Iteration 63/1000 | Loss: 0.00003250
Iteration 64/1000 | Loss: 0.00003250
Iteration 65/1000 | Loss: 0.00003250
Iteration 66/1000 | Loss: 0.00003250
Iteration 67/1000 | Loss: 0.00003250
Iteration 68/1000 | Loss: 0.00003250
Iteration 69/1000 | Loss: 0.00003250
Iteration 70/1000 | Loss: 0.00003247
Iteration 71/1000 | Loss: 0.00003246
Iteration 72/1000 | Loss: 0.00003246
Iteration 73/1000 | Loss: 0.00003246
Iteration 74/1000 | Loss: 0.00003245
Iteration 75/1000 | Loss: 0.00003245
Iteration 76/1000 | Loss: 0.00003245
Iteration 77/1000 | Loss: 0.00003244
Iteration 78/1000 | Loss: 0.00003243
Iteration 79/1000 | Loss: 0.00003243
Iteration 80/1000 | Loss: 0.00003242
Iteration 81/1000 | Loss: 0.00003242
Iteration 82/1000 | Loss: 0.00003242
Iteration 83/1000 | Loss: 0.00003241
Iteration 84/1000 | Loss: 0.00003241
Iteration 85/1000 | Loss: 0.00003240
Iteration 86/1000 | Loss: 0.00003240
Iteration 87/1000 | Loss: 0.00003239
Iteration 88/1000 | Loss: 0.00003239
Iteration 89/1000 | Loss: 0.00003239
Iteration 90/1000 | Loss: 0.00003239
Iteration 91/1000 | Loss: 0.00003238
Iteration 92/1000 | Loss: 0.00003238
Iteration 93/1000 | Loss: 0.00003238
Iteration 94/1000 | Loss: 0.00003238
Iteration 95/1000 | Loss: 0.00003238
Iteration 96/1000 | Loss: 0.00003238
Iteration 97/1000 | Loss: 0.00003237
Iteration 98/1000 | Loss: 0.00003237
Iteration 99/1000 | Loss: 0.00003237
Iteration 100/1000 | Loss: 0.00003237
Iteration 101/1000 | Loss: 0.00003237
Iteration 102/1000 | Loss: 0.00003236
Iteration 103/1000 | Loss: 0.00003236
Iteration 104/1000 | Loss: 0.00003236
Iteration 105/1000 | Loss: 0.00003236
Iteration 106/1000 | Loss: 0.00003236
Iteration 107/1000 | Loss: 0.00003236
Iteration 108/1000 | Loss: 0.00003236
Iteration 109/1000 | Loss: 0.00003235
Iteration 110/1000 | Loss: 0.00003235
Iteration 111/1000 | Loss: 0.00003235
Iteration 112/1000 | Loss: 0.00003234
Iteration 113/1000 | Loss: 0.00003233
Iteration 114/1000 | Loss: 0.00003233
Iteration 115/1000 | Loss: 0.00003233
Iteration 116/1000 | Loss: 0.00003232
Iteration 117/1000 | Loss: 0.00003232
Iteration 118/1000 | Loss: 0.00003232
Iteration 119/1000 | Loss: 0.00003232
Iteration 120/1000 | Loss: 0.00003232
Iteration 121/1000 | Loss: 0.00003232
Iteration 122/1000 | Loss: 0.00003232
Iteration 123/1000 | Loss: 0.00003231
Iteration 124/1000 | Loss: 0.00003230
Iteration 125/1000 | Loss: 0.00003230
Iteration 126/1000 | Loss: 0.00003230
Iteration 127/1000 | Loss: 0.00003230
Iteration 128/1000 | Loss: 0.00003230
Iteration 129/1000 | Loss: 0.00003229
Iteration 130/1000 | Loss: 0.00003229
Iteration 131/1000 | Loss: 0.00003229
Iteration 132/1000 | Loss: 0.00003229
Iteration 133/1000 | Loss: 0.00003229
Iteration 134/1000 | Loss: 0.00003229
Iteration 135/1000 | Loss: 0.00003229
Iteration 136/1000 | Loss: 0.00003229
Iteration 137/1000 | Loss: 0.00003229
Iteration 138/1000 | Loss: 0.00003229
Iteration 139/1000 | Loss: 0.00003229
Iteration 140/1000 | Loss: 0.00003229
Iteration 141/1000 | Loss: 0.00003229
Iteration 142/1000 | Loss: 0.00003229
Iteration 143/1000 | Loss: 0.00003229
Iteration 144/1000 | Loss: 0.00003229
Iteration 145/1000 | Loss: 0.00003228
Iteration 146/1000 | Loss: 0.00003228
Iteration 147/1000 | Loss: 0.00003228
Iteration 148/1000 | Loss: 0.00003227
Iteration 149/1000 | Loss: 0.00003227
Iteration 150/1000 | Loss: 0.00003227
Iteration 151/1000 | Loss: 0.00003227
Iteration 152/1000 | Loss: 0.00003227
Iteration 153/1000 | Loss: 0.00003227
Iteration 154/1000 | Loss: 0.00003227
Iteration 155/1000 | Loss: 0.00003227
Iteration 156/1000 | Loss: 0.00003226
Iteration 157/1000 | Loss: 0.00003226
Iteration 158/1000 | Loss: 0.00003226
Iteration 159/1000 | Loss: 0.00003226
Iteration 160/1000 | Loss: 0.00003225
Iteration 161/1000 | Loss: 0.00003225
Iteration 162/1000 | Loss: 0.00003225
Iteration 163/1000 | Loss: 0.00003225
Iteration 164/1000 | Loss: 0.00003225
Iteration 165/1000 | Loss: 0.00003225
Iteration 166/1000 | Loss: 0.00003225
Iteration 167/1000 | Loss: 0.00003225
Iteration 168/1000 | Loss: 0.00003225
Iteration 169/1000 | Loss: 0.00003225
Iteration 170/1000 | Loss: 0.00003225
Iteration 171/1000 | Loss: 0.00003225
Iteration 172/1000 | Loss: 0.00003225
Iteration 173/1000 | Loss: 0.00003225
Iteration 174/1000 | Loss: 0.00003225
Iteration 175/1000 | Loss: 0.00003224
Iteration 176/1000 | Loss: 0.00003224
Iteration 177/1000 | Loss: 0.00003224
Iteration 178/1000 | Loss: 0.00003224
Iteration 179/1000 | Loss: 0.00003224
Iteration 180/1000 | Loss: 0.00003224
Iteration 181/1000 | Loss: 0.00003224
Iteration 182/1000 | Loss: 0.00003224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [3.224421743652783e-05, 3.224421743652783e-05, 3.224421743652783e-05, 3.224421743652783e-05, 3.224421743652783e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.224421743652783e-05

Optimization complete. Final v2v error: 4.781143665313721 mm

Highest mean error: 5.149569988250732 mm for frame 74

Lowest mean error: 4.425936222076416 mm for frame 113

Saving results

Total time: 46.1957004070282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386099
Iteration 2/25 | Loss: 0.00080777
Iteration 3/25 | Loss: 0.00068868
Iteration 4/25 | Loss: 0.00067479
Iteration 5/25 | Loss: 0.00067052
Iteration 6/25 | Loss: 0.00066929
Iteration 7/25 | Loss: 0.00066912
Iteration 8/25 | Loss: 0.00066912
Iteration 9/25 | Loss: 0.00066912
Iteration 10/25 | Loss: 0.00066912
Iteration 11/25 | Loss: 0.00066912
Iteration 12/25 | Loss: 0.00066912
Iteration 13/25 | Loss: 0.00066912
Iteration 14/25 | Loss: 0.00066912
Iteration 15/25 | Loss: 0.00066912
Iteration 16/25 | Loss: 0.00066912
Iteration 17/25 | Loss: 0.00066912
Iteration 18/25 | Loss: 0.00066912
Iteration 19/25 | Loss: 0.00066912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006691219168715179, 0.0006691219168715179, 0.0006691219168715179, 0.0006691219168715179, 0.0006691219168715179]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006691219168715179

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37121069
Iteration 2/25 | Loss: 0.00026299
Iteration 3/25 | Loss: 0.00026299
Iteration 4/25 | Loss: 0.00026299
Iteration 5/25 | Loss: 0.00026299
Iteration 6/25 | Loss: 0.00026299
Iteration 7/25 | Loss: 0.00026298
Iteration 8/25 | Loss: 0.00026298
Iteration 9/25 | Loss: 0.00026298
Iteration 10/25 | Loss: 0.00026298
Iteration 11/25 | Loss: 0.00026298
Iteration 12/25 | Loss: 0.00026298
Iteration 13/25 | Loss: 0.00026298
Iteration 14/25 | Loss: 0.00026298
Iteration 15/25 | Loss: 0.00026298
Iteration 16/25 | Loss: 0.00026298
Iteration 17/25 | Loss: 0.00026298
Iteration 18/25 | Loss: 0.00026298
Iteration 19/25 | Loss: 0.00026298
Iteration 20/25 | Loss: 0.00026298
Iteration 21/25 | Loss: 0.00026298
Iteration 22/25 | Loss: 0.00026298
Iteration 23/25 | Loss: 0.00026298
Iteration 24/25 | Loss: 0.00026298
Iteration 25/25 | Loss: 0.00026298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026298
Iteration 2/1000 | Loss: 0.00003855
Iteration 3/1000 | Loss: 0.00002047
Iteration 4/1000 | Loss: 0.00001793
Iteration 5/1000 | Loss: 0.00001669
Iteration 6/1000 | Loss: 0.00001569
Iteration 7/1000 | Loss: 0.00001517
Iteration 8/1000 | Loss: 0.00001492
Iteration 9/1000 | Loss: 0.00001483
Iteration 10/1000 | Loss: 0.00001482
Iteration 11/1000 | Loss: 0.00001474
Iteration 12/1000 | Loss: 0.00001473
Iteration 13/1000 | Loss: 0.00001473
Iteration 14/1000 | Loss: 0.00001472
Iteration 15/1000 | Loss: 0.00001471
Iteration 16/1000 | Loss: 0.00001471
Iteration 17/1000 | Loss: 0.00001471
Iteration 18/1000 | Loss: 0.00001471
Iteration 19/1000 | Loss: 0.00001471
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00001471
Iteration 22/1000 | Loss: 0.00001471
Iteration 23/1000 | Loss: 0.00001470
Iteration 24/1000 | Loss: 0.00001470
Iteration 25/1000 | Loss: 0.00001469
Iteration 26/1000 | Loss: 0.00001468
Iteration 27/1000 | Loss: 0.00001467
Iteration 28/1000 | Loss: 0.00001466
Iteration 29/1000 | Loss: 0.00001466
Iteration 30/1000 | Loss: 0.00001466
Iteration 31/1000 | Loss: 0.00001465
Iteration 32/1000 | Loss: 0.00001464
Iteration 33/1000 | Loss: 0.00001460
Iteration 34/1000 | Loss: 0.00001455
Iteration 35/1000 | Loss: 0.00001455
Iteration 36/1000 | Loss: 0.00001454
Iteration 37/1000 | Loss: 0.00001454
Iteration 38/1000 | Loss: 0.00001453
Iteration 39/1000 | Loss: 0.00001453
Iteration 40/1000 | Loss: 0.00001452
Iteration 41/1000 | Loss: 0.00001452
Iteration 42/1000 | Loss: 0.00001451
Iteration 43/1000 | Loss: 0.00001448
Iteration 44/1000 | Loss: 0.00001448
Iteration 45/1000 | Loss: 0.00001446
Iteration 46/1000 | Loss: 0.00001446
Iteration 47/1000 | Loss: 0.00001445
Iteration 48/1000 | Loss: 0.00001445
Iteration 49/1000 | Loss: 0.00001445
Iteration 50/1000 | Loss: 0.00001445
Iteration 51/1000 | Loss: 0.00001445
Iteration 52/1000 | Loss: 0.00001445
Iteration 53/1000 | Loss: 0.00001445
Iteration 54/1000 | Loss: 0.00001445
Iteration 55/1000 | Loss: 0.00001444
Iteration 56/1000 | Loss: 0.00001444
Iteration 57/1000 | Loss: 0.00001444
Iteration 58/1000 | Loss: 0.00001443
Iteration 59/1000 | Loss: 0.00001443
Iteration 60/1000 | Loss: 0.00001443
Iteration 61/1000 | Loss: 0.00001443
Iteration 62/1000 | Loss: 0.00001443
Iteration 63/1000 | Loss: 0.00001442
Iteration 64/1000 | Loss: 0.00001442
Iteration 65/1000 | Loss: 0.00001442
Iteration 66/1000 | Loss: 0.00001442
Iteration 67/1000 | Loss: 0.00001442
Iteration 68/1000 | Loss: 0.00001442
Iteration 69/1000 | Loss: 0.00001442
Iteration 70/1000 | Loss: 0.00001442
Iteration 71/1000 | Loss: 0.00001442
Iteration 72/1000 | Loss: 0.00001442
Iteration 73/1000 | Loss: 0.00001442
Iteration 74/1000 | Loss: 0.00001442
Iteration 75/1000 | Loss: 0.00001442
Iteration 76/1000 | Loss: 0.00001442
Iteration 77/1000 | Loss: 0.00001442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.4420237675949465e-05, 1.4420237675949465e-05, 1.4420237675949465e-05, 1.4420237675949465e-05, 1.4420237675949465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4420237675949465e-05

Optimization complete. Final v2v error: 3.277517318725586 mm

Highest mean error: 3.5135977268218994 mm for frame 133

Lowest mean error: 3.0320894718170166 mm for frame 10

Saving results

Total time: 28.904416799545288
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810254
Iteration 2/25 | Loss: 0.00182121
Iteration 3/25 | Loss: 0.00101280
Iteration 4/25 | Loss: 0.00082311
Iteration 5/25 | Loss: 0.00075106
Iteration 6/25 | Loss: 0.00074121
Iteration 7/25 | Loss: 0.00073978
Iteration 8/25 | Loss: 0.00074260
Iteration 9/25 | Loss: 0.00074068
Iteration 10/25 | Loss: 0.00074135
Iteration 11/25 | Loss: 0.00073825
Iteration 12/25 | Loss: 0.00073462
Iteration 13/25 | Loss: 0.00073329
Iteration 14/25 | Loss: 0.00073245
Iteration 15/25 | Loss: 0.00073213
Iteration 16/25 | Loss: 0.00073195
Iteration 17/25 | Loss: 0.00073316
Iteration 18/25 | Loss: 0.00073152
Iteration 19/25 | Loss: 0.00073092
Iteration 20/25 | Loss: 0.00073061
Iteration 21/25 | Loss: 0.00073046
Iteration 22/25 | Loss: 0.00073046
Iteration 23/25 | Loss: 0.00073046
Iteration 24/25 | Loss: 0.00073041
Iteration 25/25 | Loss: 0.00073030

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.06630945
Iteration 2/25 | Loss: 0.00021278
Iteration 3/25 | Loss: 0.00021277
Iteration 4/25 | Loss: 0.00021277
Iteration 5/25 | Loss: 0.00021277
Iteration 6/25 | Loss: 0.00021276
Iteration 7/25 | Loss: 0.00021276
Iteration 8/25 | Loss: 0.00021276
Iteration 9/25 | Loss: 0.00021276
Iteration 10/25 | Loss: 0.00021276
Iteration 11/25 | Loss: 0.00021276
Iteration 12/25 | Loss: 0.00021276
Iteration 13/25 | Loss: 0.00021276
Iteration 14/25 | Loss: 0.00021276
Iteration 15/25 | Loss: 0.00021276
Iteration 16/25 | Loss: 0.00021276
Iteration 17/25 | Loss: 0.00021276
Iteration 18/25 | Loss: 0.00021276
Iteration 19/25 | Loss: 0.00021276
Iteration 20/25 | Loss: 0.00021276
Iteration 21/25 | Loss: 0.00021276
Iteration 22/25 | Loss: 0.00021276
Iteration 23/25 | Loss: 0.00021276
Iteration 24/25 | Loss: 0.00021276
Iteration 25/25 | Loss: 0.00021276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021276
Iteration 2/1000 | Loss: 0.00003696
Iteration 3/1000 | Loss: 0.00002395
Iteration 4/1000 | Loss: 0.00002197
Iteration 5/1000 | Loss: 0.00002083
Iteration 6/1000 | Loss: 0.00002001
Iteration 7/1000 | Loss: 0.00001965
Iteration 8/1000 | Loss: 0.00001931
Iteration 9/1000 | Loss: 0.00001908
Iteration 10/1000 | Loss: 0.00001904
Iteration 11/1000 | Loss: 0.00001895
Iteration 12/1000 | Loss: 0.00001892
Iteration 13/1000 | Loss: 0.00001889
Iteration 14/1000 | Loss: 0.00001885
Iteration 15/1000 | Loss: 0.00001881
Iteration 16/1000 | Loss: 0.00001880
Iteration 17/1000 | Loss: 0.00001879
Iteration 18/1000 | Loss: 0.00001878
Iteration 19/1000 | Loss: 0.00001878
Iteration 20/1000 | Loss: 0.00001877
Iteration 21/1000 | Loss: 0.00001876
Iteration 22/1000 | Loss: 0.00001876
Iteration 23/1000 | Loss: 0.00001876
Iteration 24/1000 | Loss: 0.00001875
Iteration 25/1000 | Loss: 0.00001874
Iteration 26/1000 | Loss: 0.00001874
Iteration 27/1000 | Loss: 0.00001873
Iteration 28/1000 | Loss: 0.00001873
Iteration 29/1000 | Loss: 0.00001872
Iteration 30/1000 | Loss: 0.00001870
Iteration 31/1000 | Loss: 0.00001870
Iteration 32/1000 | Loss: 0.00001869
Iteration 33/1000 | Loss: 0.00001869
Iteration 34/1000 | Loss: 0.00001869
Iteration 35/1000 | Loss: 0.00001868
Iteration 36/1000 | Loss: 0.00001868
Iteration 37/1000 | Loss: 0.00001868
Iteration 38/1000 | Loss: 0.00001868
Iteration 39/1000 | Loss: 0.00001868
Iteration 40/1000 | Loss: 0.00001868
Iteration 41/1000 | Loss: 0.00001868
Iteration 42/1000 | Loss: 0.00001868
Iteration 43/1000 | Loss: 0.00001868
Iteration 44/1000 | Loss: 0.00001868
Iteration 45/1000 | Loss: 0.00001868
Iteration 46/1000 | Loss: 0.00001868
Iteration 47/1000 | Loss: 0.00001868
Iteration 48/1000 | Loss: 0.00001867
Iteration 49/1000 | Loss: 0.00001867
Iteration 50/1000 | Loss: 0.00001867
Iteration 51/1000 | Loss: 0.00001866
Iteration 52/1000 | Loss: 0.00001866
Iteration 53/1000 | Loss: 0.00001866
Iteration 54/1000 | Loss: 0.00001866
Iteration 55/1000 | Loss: 0.00001866
Iteration 56/1000 | Loss: 0.00001866
Iteration 57/1000 | Loss: 0.00001865
Iteration 58/1000 | Loss: 0.00001865
Iteration 59/1000 | Loss: 0.00001865
Iteration 60/1000 | Loss: 0.00001865
Iteration 61/1000 | Loss: 0.00001864
Iteration 62/1000 | Loss: 0.00001864
Iteration 63/1000 | Loss: 0.00001864
Iteration 64/1000 | Loss: 0.00001864
Iteration 65/1000 | Loss: 0.00001863
Iteration 66/1000 | Loss: 0.00001863
Iteration 67/1000 | Loss: 0.00001863
Iteration 68/1000 | Loss: 0.00001862
Iteration 69/1000 | Loss: 0.00001862
Iteration 70/1000 | Loss: 0.00001862
Iteration 71/1000 | Loss: 0.00001861
Iteration 72/1000 | Loss: 0.00001861
Iteration 73/1000 | Loss: 0.00001861
Iteration 74/1000 | Loss: 0.00001861
Iteration 75/1000 | Loss: 0.00001861
Iteration 76/1000 | Loss: 0.00001861
Iteration 77/1000 | Loss: 0.00001860
Iteration 78/1000 | Loss: 0.00001860
Iteration 79/1000 | Loss: 0.00001859
Iteration 80/1000 | Loss: 0.00001859
Iteration 81/1000 | Loss: 0.00001858
Iteration 82/1000 | Loss: 0.00001858
Iteration 83/1000 | Loss: 0.00001858
Iteration 84/1000 | Loss: 0.00001857
Iteration 85/1000 | Loss: 0.00001857
Iteration 86/1000 | Loss: 0.00001857
Iteration 87/1000 | Loss: 0.00001857
Iteration 88/1000 | Loss: 0.00001856
Iteration 89/1000 | Loss: 0.00001856
Iteration 90/1000 | Loss: 0.00001856
Iteration 91/1000 | Loss: 0.00001856
Iteration 92/1000 | Loss: 0.00001856
Iteration 93/1000 | Loss: 0.00001856
Iteration 94/1000 | Loss: 0.00001856
Iteration 95/1000 | Loss: 0.00001855
Iteration 96/1000 | Loss: 0.00001855
Iteration 97/1000 | Loss: 0.00001855
Iteration 98/1000 | Loss: 0.00001855
Iteration 99/1000 | Loss: 0.00001854
Iteration 100/1000 | Loss: 0.00001854
Iteration 101/1000 | Loss: 0.00001854
Iteration 102/1000 | Loss: 0.00001854
Iteration 103/1000 | Loss: 0.00001853
Iteration 104/1000 | Loss: 0.00001853
Iteration 105/1000 | Loss: 0.00001853
Iteration 106/1000 | Loss: 0.00001853
Iteration 107/1000 | Loss: 0.00001853
Iteration 108/1000 | Loss: 0.00001853
Iteration 109/1000 | Loss: 0.00001852
Iteration 110/1000 | Loss: 0.00001852
Iteration 111/1000 | Loss: 0.00001852
Iteration 112/1000 | Loss: 0.00001852
Iteration 113/1000 | Loss: 0.00001852
Iteration 114/1000 | Loss: 0.00001852
Iteration 115/1000 | Loss: 0.00001852
Iteration 116/1000 | Loss: 0.00001851
Iteration 117/1000 | Loss: 0.00001851
Iteration 118/1000 | Loss: 0.00001851
Iteration 119/1000 | Loss: 0.00001851
Iteration 120/1000 | Loss: 0.00001851
Iteration 121/1000 | Loss: 0.00001851
Iteration 122/1000 | Loss: 0.00001851
Iteration 123/1000 | Loss: 0.00001851
Iteration 124/1000 | Loss: 0.00001851
Iteration 125/1000 | Loss: 0.00001851
Iteration 126/1000 | Loss: 0.00001851
Iteration 127/1000 | Loss: 0.00001851
Iteration 128/1000 | Loss: 0.00001851
Iteration 129/1000 | Loss: 0.00001851
Iteration 130/1000 | Loss: 0.00001851
Iteration 131/1000 | Loss: 0.00001851
Iteration 132/1000 | Loss: 0.00001851
Iteration 133/1000 | Loss: 0.00001851
Iteration 134/1000 | Loss: 0.00001850
Iteration 135/1000 | Loss: 0.00001850
Iteration 136/1000 | Loss: 0.00001850
Iteration 137/1000 | Loss: 0.00001850
Iteration 138/1000 | Loss: 0.00001850
Iteration 139/1000 | Loss: 0.00001850
Iteration 140/1000 | Loss: 0.00001850
Iteration 141/1000 | Loss: 0.00001850
Iteration 142/1000 | Loss: 0.00001850
Iteration 143/1000 | Loss: 0.00001850
Iteration 144/1000 | Loss: 0.00001850
Iteration 145/1000 | Loss: 0.00001850
Iteration 146/1000 | Loss: 0.00001850
Iteration 147/1000 | Loss: 0.00001850
Iteration 148/1000 | Loss: 0.00001850
Iteration 149/1000 | Loss: 0.00001850
Iteration 150/1000 | Loss: 0.00001850
Iteration 151/1000 | Loss: 0.00001850
Iteration 152/1000 | Loss: 0.00001850
Iteration 153/1000 | Loss: 0.00001850
Iteration 154/1000 | Loss: 0.00001850
Iteration 155/1000 | Loss: 0.00001850
Iteration 156/1000 | Loss: 0.00001850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.8504382751416415e-05, 1.8504382751416415e-05, 1.8504382751416415e-05, 1.8504382751416415e-05, 1.8504382751416415e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8504382751416415e-05

Optimization complete. Final v2v error: 3.7705023288726807 mm

Highest mean error: 4.3778510093688965 mm for frame 35

Lowest mean error: 3.3934576511383057 mm for frame 66

Saving results

Total time: 76.4541425704956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00547472
Iteration 2/25 | Loss: 0.00098193
Iteration 3/25 | Loss: 0.00082251
Iteration 4/25 | Loss: 0.00079420
Iteration 5/25 | Loss: 0.00078453
Iteration 6/25 | Loss: 0.00078299
Iteration 7/25 | Loss: 0.00078299
Iteration 8/25 | Loss: 0.00078299
Iteration 9/25 | Loss: 0.00078299
Iteration 10/25 | Loss: 0.00078299
Iteration 11/25 | Loss: 0.00078299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007829937385395169, 0.0007829937385395169, 0.0007829937385395169, 0.0007829937385395169, 0.0007829937385395169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007829937385395169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.18009663
Iteration 2/25 | Loss: 0.00026921
Iteration 3/25 | Loss: 0.00026919
Iteration 4/25 | Loss: 0.00026919
Iteration 5/25 | Loss: 0.00026919
Iteration 6/25 | Loss: 0.00026919
Iteration 7/25 | Loss: 0.00026919
Iteration 8/25 | Loss: 0.00026919
Iteration 9/25 | Loss: 0.00026919
Iteration 10/25 | Loss: 0.00026919
Iteration 11/25 | Loss: 0.00026919
Iteration 12/25 | Loss: 0.00026919
Iteration 13/25 | Loss: 0.00026919
Iteration 14/25 | Loss: 0.00026919
Iteration 15/25 | Loss: 0.00026919
Iteration 16/25 | Loss: 0.00026919
Iteration 17/25 | Loss: 0.00026919
Iteration 18/25 | Loss: 0.00026919
Iteration 19/25 | Loss: 0.00026919
Iteration 20/25 | Loss: 0.00026919
Iteration 21/25 | Loss: 0.00026919
Iteration 22/25 | Loss: 0.00026919
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0002691892150323838, 0.0002691892150323838, 0.0002691892150323838, 0.0002691892150323838, 0.0002691892150323838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002691892150323838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026919
Iteration 2/1000 | Loss: 0.00004956
Iteration 3/1000 | Loss: 0.00003121
Iteration 4/1000 | Loss: 0.00002855
Iteration 5/1000 | Loss: 0.00002695
Iteration 6/1000 | Loss: 0.00002583
Iteration 7/1000 | Loss: 0.00002515
Iteration 8/1000 | Loss: 0.00002465
Iteration 9/1000 | Loss: 0.00002413
Iteration 10/1000 | Loss: 0.00002385
Iteration 11/1000 | Loss: 0.00002382
Iteration 12/1000 | Loss: 0.00002360
Iteration 13/1000 | Loss: 0.00002359
Iteration 14/1000 | Loss: 0.00002347
Iteration 15/1000 | Loss: 0.00002344
Iteration 16/1000 | Loss: 0.00002343
Iteration 17/1000 | Loss: 0.00002342
Iteration 18/1000 | Loss: 0.00002342
Iteration 19/1000 | Loss: 0.00002342
Iteration 20/1000 | Loss: 0.00002342
Iteration 21/1000 | Loss: 0.00002342
Iteration 22/1000 | Loss: 0.00002342
Iteration 23/1000 | Loss: 0.00002342
Iteration 24/1000 | Loss: 0.00002341
Iteration 25/1000 | Loss: 0.00002341
Iteration 26/1000 | Loss: 0.00002341
Iteration 27/1000 | Loss: 0.00002341
Iteration 28/1000 | Loss: 0.00002341
Iteration 29/1000 | Loss: 0.00002340
Iteration 30/1000 | Loss: 0.00002340
Iteration 31/1000 | Loss: 0.00002340
Iteration 32/1000 | Loss: 0.00002340
Iteration 33/1000 | Loss: 0.00002339
Iteration 34/1000 | Loss: 0.00002339
Iteration 35/1000 | Loss: 0.00002339
Iteration 36/1000 | Loss: 0.00002339
Iteration 37/1000 | Loss: 0.00002339
Iteration 38/1000 | Loss: 0.00002338
Iteration 39/1000 | Loss: 0.00002338
Iteration 40/1000 | Loss: 0.00002338
Iteration 41/1000 | Loss: 0.00002338
Iteration 42/1000 | Loss: 0.00002338
Iteration 43/1000 | Loss: 0.00002337
Iteration 44/1000 | Loss: 0.00002337
Iteration 45/1000 | Loss: 0.00002337
Iteration 46/1000 | Loss: 0.00002337
Iteration 47/1000 | Loss: 0.00002337
Iteration 48/1000 | Loss: 0.00002336
Iteration 49/1000 | Loss: 0.00002336
Iteration 50/1000 | Loss: 0.00002336
Iteration 51/1000 | Loss: 0.00002336
Iteration 52/1000 | Loss: 0.00002336
Iteration 53/1000 | Loss: 0.00002336
Iteration 54/1000 | Loss: 0.00002336
Iteration 55/1000 | Loss: 0.00002336
Iteration 56/1000 | Loss: 0.00002336
Iteration 57/1000 | Loss: 0.00002335
Iteration 58/1000 | Loss: 0.00002335
Iteration 59/1000 | Loss: 0.00002335
Iteration 60/1000 | Loss: 0.00002334
Iteration 61/1000 | Loss: 0.00002334
Iteration 62/1000 | Loss: 0.00002334
Iteration 63/1000 | Loss: 0.00002333
Iteration 64/1000 | Loss: 0.00002333
Iteration 65/1000 | Loss: 0.00002333
Iteration 66/1000 | Loss: 0.00002332
Iteration 67/1000 | Loss: 0.00002332
Iteration 68/1000 | Loss: 0.00002331
Iteration 69/1000 | Loss: 0.00002331
Iteration 70/1000 | Loss: 0.00002331
Iteration 71/1000 | Loss: 0.00002331
Iteration 72/1000 | Loss: 0.00002330
Iteration 73/1000 | Loss: 0.00002330
Iteration 74/1000 | Loss: 0.00002330
Iteration 75/1000 | Loss: 0.00002330
Iteration 76/1000 | Loss: 0.00002330
Iteration 77/1000 | Loss: 0.00002329
Iteration 78/1000 | Loss: 0.00002329
Iteration 79/1000 | Loss: 0.00002329
Iteration 80/1000 | Loss: 0.00002329
Iteration 81/1000 | Loss: 0.00002328
Iteration 82/1000 | Loss: 0.00002328
Iteration 83/1000 | Loss: 0.00002328
Iteration 84/1000 | Loss: 0.00002328
Iteration 85/1000 | Loss: 0.00002328
Iteration 86/1000 | Loss: 0.00002328
Iteration 87/1000 | Loss: 0.00002328
Iteration 88/1000 | Loss: 0.00002328
Iteration 89/1000 | Loss: 0.00002328
Iteration 90/1000 | Loss: 0.00002328
Iteration 91/1000 | Loss: 0.00002328
Iteration 92/1000 | Loss: 0.00002328
Iteration 93/1000 | Loss: 0.00002328
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [2.328024129383266e-05, 2.328024129383266e-05, 2.328024129383266e-05, 2.328024129383266e-05, 2.328024129383266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.328024129383266e-05

Optimization complete. Final v2v error: 4.179978370666504 mm

Highest mean error: 4.687775135040283 mm for frame 197

Lowest mean error: 3.8645761013031006 mm for frame 148

Saving results

Total time: 37.14872407913208
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086608
Iteration 2/25 | Loss: 0.00240338
Iteration 3/25 | Loss: 0.00300357
Iteration 4/25 | Loss: 0.00164415
Iteration 5/25 | Loss: 0.00133676
Iteration 6/25 | Loss: 0.00097801
Iteration 7/25 | Loss: 0.00096774
Iteration 8/25 | Loss: 0.00092496
Iteration 9/25 | Loss: 0.00087963
Iteration 10/25 | Loss: 0.00087353
Iteration 11/25 | Loss: 0.00085914
Iteration 12/25 | Loss: 0.00087804
Iteration 13/25 | Loss: 0.00086947
Iteration 14/25 | Loss: 0.00087399
Iteration 15/25 | Loss: 0.00085430
Iteration 16/25 | Loss: 0.00085111
Iteration 17/25 | Loss: 0.00086305
Iteration 18/25 | Loss: 0.00084848
Iteration 19/25 | Loss: 0.00084163
Iteration 20/25 | Loss: 0.00084418
Iteration 21/25 | Loss: 0.00084124
Iteration 22/25 | Loss: 0.00084142
Iteration 23/25 | Loss: 0.00083936
Iteration 24/25 | Loss: 0.00084689
Iteration 25/25 | Loss: 0.00083881

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41965175
Iteration 2/25 | Loss: 0.00114681
Iteration 3/25 | Loss: 0.00114681
Iteration 4/25 | Loss: 0.00068603
Iteration 5/25 | Loss: 0.00025465
Iteration 6/25 | Loss: 0.00025465
Iteration 7/25 | Loss: 0.00025464
Iteration 8/25 | Loss: 0.00025464
Iteration 9/25 | Loss: 0.00025464
Iteration 10/25 | Loss: 0.00025464
Iteration 11/25 | Loss: 0.00025464
Iteration 12/25 | Loss: 0.00025464
Iteration 13/25 | Loss: 0.00025464
Iteration 14/25 | Loss: 0.00025464
Iteration 15/25 | Loss: 0.00025464
Iteration 16/25 | Loss: 0.00025464
Iteration 17/25 | Loss: 0.00025464
Iteration 18/25 | Loss: 0.00025464
Iteration 19/25 | Loss: 0.00025464
Iteration 20/25 | Loss: 0.00025464
Iteration 21/25 | Loss: 0.00025464
Iteration 22/25 | Loss: 0.00025464
Iteration 23/25 | Loss: 0.00025464
Iteration 24/25 | Loss: 0.00025464
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00025464320788159966, 0.00025464320788159966, 0.00025464320788159966, 0.00025464320788159966, 0.00025464320788159966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025464320788159966

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025464
Iteration 2/1000 | Loss: 0.00006899
Iteration 3/1000 | Loss: 0.00004423
Iteration 4/1000 | Loss: 0.00003995
Iteration 5/1000 | Loss: 0.00020502
Iteration 6/1000 | Loss: 0.00003933
Iteration 7/1000 | Loss: 0.00003546
Iteration 8/1000 | Loss: 0.00003366
Iteration 9/1000 | Loss: 0.00003201
Iteration 10/1000 | Loss: 0.00022094
Iteration 11/1000 | Loss: 0.00024847
Iteration 12/1000 | Loss: 0.00003443
Iteration 13/1000 | Loss: 0.00003202
Iteration 14/1000 | Loss: 0.00020942
Iteration 15/1000 | Loss: 0.00066134
Iteration 16/1000 | Loss: 0.00024206
Iteration 17/1000 | Loss: 0.00008936
Iteration 18/1000 | Loss: 0.00003168
Iteration 19/1000 | Loss: 0.00003022
Iteration 20/1000 | Loss: 0.00002949
Iteration 21/1000 | Loss: 0.00002885
Iteration 22/1000 | Loss: 0.00002855
Iteration 23/1000 | Loss: 0.00002849
Iteration 24/1000 | Loss: 0.00002841
Iteration 25/1000 | Loss: 0.00002826
Iteration 26/1000 | Loss: 0.00002814
Iteration 27/1000 | Loss: 0.00002801
Iteration 28/1000 | Loss: 0.00002797
Iteration 29/1000 | Loss: 0.00002797
Iteration 30/1000 | Loss: 0.00002796
Iteration 31/1000 | Loss: 0.00002796
Iteration 32/1000 | Loss: 0.00002795
Iteration 33/1000 | Loss: 0.00002795
Iteration 34/1000 | Loss: 0.00002794
Iteration 35/1000 | Loss: 0.00002794
Iteration 36/1000 | Loss: 0.00002793
Iteration 37/1000 | Loss: 0.00002793
Iteration 38/1000 | Loss: 0.00002793
Iteration 39/1000 | Loss: 0.00002793
Iteration 40/1000 | Loss: 0.00002792
Iteration 41/1000 | Loss: 0.00002792
Iteration 42/1000 | Loss: 0.00002792
Iteration 43/1000 | Loss: 0.00002792
Iteration 44/1000 | Loss: 0.00002792
Iteration 45/1000 | Loss: 0.00002792
Iteration 46/1000 | Loss: 0.00002791
Iteration 47/1000 | Loss: 0.00002791
Iteration 48/1000 | Loss: 0.00002791
Iteration 49/1000 | Loss: 0.00002791
Iteration 50/1000 | Loss: 0.00002790
Iteration 51/1000 | Loss: 0.00002790
Iteration 52/1000 | Loss: 0.00002790
Iteration 53/1000 | Loss: 0.00002790
Iteration 54/1000 | Loss: 0.00002790
Iteration 55/1000 | Loss: 0.00002790
Iteration 56/1000 | Loss: 0.00002789
Iteration 57/1000 | Loss: 0.00002789
Iteration 58/1000 | Loss: 0.00002789
Iteration 59/1000 | Loss: 0.00002789
Iteration 60/1000 | Loss: 0.00002789
Iteration 61/1000 | Loss: 0.00002789
Iteration 62/1000 | Loss: 0.00002789
Iteration 63/1000 | Loss: 0.00002789
Iteration 64/1000 | Loss: 0.00002788
Iteration 65/1000 | Loss: 0.00002788
Iteration 66/1000 | Loss: 0.00002788
Iteration 67/1000 | Loss: 0.00002788
Iteration 68/1000 | Loss: 0.00002788
Iteration 69/1000 | Loss: 0.00002787
Iteration 70/1000 | Loss: 0.00002787
Iteration 71/1000 | Loss: 0.00002787
Iteration 72/1000 | Loss: 0.00002787
Iteration 73/1000 | Loss: 0.00002787
Iteration 74/1000 | Loss: 0.00002787
Iteration 75/1000 | Loss: 0.00002786
Iteration 76/1000 | Loss: 0.00002786
Iteration 77/1000 | Loss: 0.00002786
Iteration 78/1000 | Loss: 0.00002786
Iteration 79/1000 | Loss: 0.00002786
Iteration 80/1000 | Loss: 0.00002786
Iteration 81/1000 | Loss: 0.00002786
Iteration 82/1000 | Loss: 0.00002786
Iteration 83/1000 | Loss: 0.00002786
Iteration 84/1000 | Loss: 0.00002786
Iteration 85/1000 | Loss: 0.00002786
Iteration 86/1000 | Loss: 0.00002786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [2.7861804483109154e-05, 2.7861804483109154e-05, 2.7861804483109154e-05, 2.7861804483109154e-05, 2.7861804483109154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7861804483109154e-05

Optimization complete. Final v2v error: 4.436006546020508 mm

Highest mean error: 5.497984409332275 mm for frame 107

Lowest mean error: 3.941776752471924 mm for frame 2

Saving results

Total time: 102.10073637962341
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845172
Iteration 2/25 | Loss: 0.00098553
Iteration 3/25 | Loss: 0.00074852
Iteration 4/25 | Loss: 0.00071641
Iteration 5/25 | Loss: 0.00071039
Iteration 6/25 | Loss: 0.00070925
Iteration 7/25 | Loss: 0.00070925
Iteration 8/25 | Loss: 0.00070925
Iteration 9/25 | Loss: 0.00070925
Iteration 10/25 | Loss: 0.00070925
Iteration 11/25 | Loss: 0.00070925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007092492887750268, 0.0007092492887750268, 0.0007092492887750268, 0.0007092492887750268, 0.0007092492887750268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007092492887750268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35671806
Iteration 2/25 | Loss: 0.00027900
Iteration 3/25 | Loss: 0.00027900
Iteration 4/25 | Loss: 0.00027899
Iteration 5/25 | Loss: 0.00027899
Iteration 6/25 | Loss: 0.00027899
Iteration 7/25 | Loss: 0.00027899
Iteration 8/25 | Loss: 0.00027899
Iteration 9/25 | Loss: 0.00027899
Iteration 10/25 | Loss: 0.00027899
Iteration 11/25 | Loss: 0.00027899
Iteration 12/25 | Loss: 0.00027899
Iteration 13/25 | Loss: 0.00027899
Iteration 14/25 | Loss: 0.00027899
Iteration 15/25 | Loss: 0.00027899
Iteration 16/25 | Loss: 0.00027899
Iteration 17/25 | Loss: 0.00027899
Iteration 18/25 | Loss: 0.00027899
Iteration 19/25 | Loss: 0.00027899
Iteration 20/25 | Loss: 0.00027899
Iteration 21/25 | Loss: 0.00027899
Iteration 22/25 | Loss: 0.00027899
Iteration 23/25 | Loss: 0.00027899
Iteration 24/25 | Loss: 0.00027899
Iteration 25/25 | Loss: 0.00027899

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027899
Iteration 2/1000 | Loss: 0.00004704
Iteration 3/1000 | Loss: 0.00002807
Iteration 4/1000 | Loss: 0.00002512
Iteration 5/1000 | Loss: 0.00002326
Iteration 6/1000 | Loss: 0.00002186
Iteration 7/1000 | Loss: 0.00002111
Iteration 8/1000 | Loss: 0.00002045
Iteration 9/1000 | Loss: 0.00001998
Iteration 10/1000 | Loss: 0.00001984
Iteration 11/1000 | Loss: 0.00001969
Iteration 12/1000 | Loss: 0.00001967
Iteration 13/1000 | Loss: 0.00001948
Iteration 14/1000 | Loss: 0.00001941
Iteration 15/1000 | Loss: 0.00001938
Iteration 16/1000 | Loss: 0.00001937
Iteration 17/1000 | Loss: 0.00001934
Iteration 18/1000 | Loss: 0.00001929
Iteration 19/1000 | Loss: 0.00001927
Iteration 20/1000 | Loss: 0.00001927
Iteration 21/1000 | Loss: 0.00001926
Iteration 22/1000 | Loss: 0.00001926
Iteration 23/1000 | Loss: 0.00001925
Iteration 24/1000 | Loss: 0.00001925
Iteration 25/1000 | Loss: 0.00001924
Iteration 26/1000 | Loss: 0.00001924
Iteration 27/1000 | Loss: 0.00001924
Iteration 28/1000 | Loss: 0.00001923
Iteration 29/1000 | Loss: 0.00001922
Iteration 30/1000 | Loss: 0.00001922
Iteration 31/1000 | Loss: 0.00001922
Iteration 32/1000 | Loss: 0.00001921
Iteration 33/1000 | Loss: 0.00001920
Iteration 34/1000 | Loss: 0.00001920
Iteration 35/1000 | Loss: 0.00001920
Iteration 36/1000 | Loss: 0.00001920
Iteration 37/1000 | Loss: 0.00001920
Iteration 38/1000 | Loss: 0.00001919
Iteration 39/1000 | Loss: 0.00001919
Iteration 40/1000 | Loss: 0.00001918
Iteration 41/1000 | Loss: 0.00001918
Iteration 42/1000 | Loss: 0.00001918
Iteration 43/1000 | Loss: 0.00001918
Iteration 44/1000 | Loss: 0.00001917
Iteration 45/1000 | Loss: 0.00001917
Iteration 46/1000 | Loss: 0.00001917
Iteration 47/1000 | Loss: 0.00001916
Iteration 48/1000 | Loss: 0.00001916
Iteration 49/1000 | Loss: 0.00001915
Iteration 50/1000 | Loss: 0.00001915
Iteration 51/1000 | Loss: 0.00001914
Iteration 52/1000 | Loss: 0.00001913
Iteration 53/1000 | Loss: 0.00001912
Iteration 54/1000 | Loss: 0.00001912
Iteration 55/1000 | Loss: 0.00001912
Iteration 56/1000 | Loss: 0.00001912
Iteration 57/1000 | Loss: 0.00001912
Iteration 58/1000 | Loss: 0.00001912
Iteration 59/1000 | Loss: 0.00001912
Iteration 60/1000 | Loss: 0.00001912
Iteration 61/1000 | Loss: 0.00001912
Iteration 62/1000 | Loss: 0.00001912
Iteration 63/1000 | Loss: 0.00001912
Iteration 64/1000 | Loss: 0.00001911
Iteration 65/1000 | Loss: 0.00001911
Iteration 66/1000 | Loss: 0.00001911
Iteration 67/1000 | Loss: 0.00001911
Iteration 68/1000 | Loss: 0.00001911
Iteration 69/1000 | Loss: 0.00001911
Iteration 70/1000 | Loss: 0.00001911
Iteration 71/1000 | Loss: 0.00001911
Iteration 72/1000 | Loss: 0.00001911
Iteration 73/1000 | Loss: 0.00001910
Iteration 74/1000 | Loss: 0.00001910
Iteration 75/1000 | Loss: 0.00001910
Iteration 76/1000 | Loss: 0.00001910
Iteration 77/1000 | Loss: 0.00001910
Iteration 78/1000 | Loss: 0.00001908
Iteration 79/1000 | Loss: 0.00001908
Iteration 80/1000 | Loss: 0.00001908
Iteration 81/1000 | Loss: 0.00001908
Iteration 82/1000 | Loss: 0.00001908
Iteration 83/1000 | Loss: 0.00001908
Iteration 84/1000 | Loss: 0.00001908
Iteration 85/1000 | Loss: 0.00001908
Iteration 86/1000 | Loss: 0.00001907
Iteration 87/1000 | Loss: 0.00001907
Iteration 88/1000 | Loss: 0.00001907
Iteration 89/1000 | Loss: 0.00001906
Iteration 90/1000 | Loss: 0.00001906
Iteration 91/1000 | Loss: 0.00001906
Iteration 92/1000 | Loss: 0.00001905
Iteration 93/1000 | Loss: 0.00001905
Iteration 94/1000 | Loss: 0.00001904
Iteration 95/1000 | Loss: 0.00001904
Iteration 96/1000 | Loss: 0.00001904
Iteration 97/1000 | Loss: 0.00001904
Iteration 98/1000 | Loss: 0.00001903
Iteration 99/1000 | Loss: 0.00001903
Iteration 100/1000 | Loss: 0.00001903
Iteration 101/1000 | Loss: 0.00001903
Iteration 102/1000 | Loss: 0.00001903
Iteration 103/1000 | Loss: 0.00001902
Iteration 104/1000 | Loss: 0.00001902
Iteration 105/1000 | Loss: 0.00001902
Iteration 106/1000 | Loss: 0.00001902
Iteration 107/1000 | Loss: 0.00001902
Iteration 108/1000 | Loss: 0.00001902
Iteration 109/1000 | Loss: 0.00001902
Iteration 110/1000 | Loss: 0.00001902
Iteration 111/1000 | Loss: 0.00001902
Iteration 112/1000 | Loss: 0.00001902
Iteration 113/1000 | Loss: 0.00001902
Iteration 114/1000 | Loss: 0.00001902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.901968425954692e-05, 1.901968425954692e-05, 1.901968425954692e-05, 1.901968425954692e-05, 1.901968425954692e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.901968425954692e-05

Optimization complete. Final v2v error: 3.751819372177124 mm

Highest mean error: 4.006222724914551 mm for frame 154

Lowest mean error: 3.5125732421875 mm for frame 209

Saving results

Total time: 39.48541831970215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540329
Iteration 2/25 | Loss: 0.00103397
Iteration 3/25 | Loss: 0.00072619
Iteration 4/25 | Loss: 0.00069574
Iteration 5/25 | Loss: 0.00068909
Iteration 6/25 | Loss: 0.00068691
Iteration 7/25 | Loss: 0.00068672
Iteration 8/25 | Loss: 0.00068672
Iteration 9/25 | Loss: 0.00068672
Iteration 10/25 | Loss: 0.00068672
Iteration 11/25 | Loss: 0.00068672
Iteration 12/25 | Loss: 0.00068672
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006867246120236814, 0.0006867246120236814, 0.0006867246120236814, 0.0006867246120236814, 0.0006867246120236814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006867246120236814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81112880
Iteration 2/25 | Loss: 0.00017204
Iteration 3/25 | Loss: 0.00017203
Iteration 4/25 | Loss: 0.00017203
Iteration 5/25 | Loss: 0.00017203
Iteration 6/25 | Loss: 0.00017203
Iteration 7/25 | Loss: 0.00017203
Iteration 8/25 | Loss: 0.00017203
Iteration 9/25 | Loss: 0.00017203
Iteration 10/25 | Loss: 0.00017203
Iteration 11/25 | Loss: 0.00017203
Iteration 12/25 | Loss: 0.00017203
Iteration 13/25 | Loss: 0.00017203
Iteration 14/25 | Loss: 0.00017203
Iteration 15/25 | Loss: 0.00017203
Iteration 16/25 | Loss: 0.00017203
Iteration 17/25 | Loss: 0.00017203
Iteration 18/25 | Loss: 0.00017203
Iteration 19/25 | Loss: 0.00017203
Iteration 20/25 | Loss: 0.00017203
Iteration 21/25 | Loss: 0.00017203
Iteration 22/25 | Loss: 0.00017203
Iteration 23/25 | Loss: 0.00017203
Iteration 24/25 | Loss: 0.00017203
Iteration 25/25 | Loss: 0.00017203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017203
Iteration 2/1000 | Loss: 0.00003893
Iteration 3/1000 | Loss: 0.00002212
Iteration 4/1000 | Loss: 0.00002029
Iteration 5/1000 | Loss: 0.00001931
Iteration 6/1000 | Loss: 0.00001862
Iteration 7/1000 | Loss: 0.00001805
Iteration 8/1000 | Loss: 0.00001766
Iteration 9/1000 | Loss: 0.00001744
Iteration 10/1000 | Loss: 0.00001730
Iteration 11/1000 | Loss: 0.00001727
Iteration 12/1000 | Loss: 0.00001718
Iteration 13/1000 | Loss: 0.00001712
Iteration 14/1000 | Loss: 0.00001711
Iteration 15/1000 | Loss: 0.00001711
Iteration 16/1000 | Loss: 0.00001710
Iteration 17/1000 | Loss: 0.00001703
Iteration 18/1000 | Loss: 0.00001698
Iteration 19/1000 | Loss: 0.00001698
Iteration 20/1000 | Loss: 0.00001697
Iteration 21/1000 | Loss: 0.00001693
Iteration 22/1000 | Loss: 0.00001693
Iteration 23/1000 | Loss: 0.00001688
Iteration 24/1000 | Loss: 0.00001687
Iteration 25/1000 | Loss: 0.00001686
Iteration 26/1000 | Loss: 0.00001684
Iteration 27/1000 | Loss: 0.00001684
Iteration 28/1000 | Loss: 0.00001683
Iteration 29/1000 | Loss: 0.00001683
Iteration 30/1000 | Loss: 0.00001683
Iteration 31/1000 | Loss: 0.00001682
Iteration 32/1000 | Loss: 0.00001682
Iteration 33/1000 | Loss: 0.00001679
Iteration 34/1000 | Loss: 0.00001678
Iteration 35/1000 | Loss: 0.00001678
Iteration 36/1000 | Loss: 0.00001677
Iteration 37/1000 | Loss: 0.00001677
Iteration 38/1000 | Loss: 0.00001677
Iteration 39/1000 | Loss: 0.00001677
Iteration 40/1000 | Loss: 0.00001677
Iteration 41/1000 | Loss: 0.00001677
Iteration 42/1000 | Loss: 0.00001677
Iteration 43/1000 | Loss: 0.00001677
Iteration 44/1000 | Loss: 0.00001677
Iteration 45/1000 | Loss: 0.00001677
Iteration 46/1000 | Loss: 0.00001677
Iteration 47/1000 | Loss: 0.00001676
Iteration 48/1000 | Loss: 0.00001676
Iteration 49/1000 | Loss: 0.00001676
Iteration 50/1000 | Loss: 0.00001675
Iteration 51/1000 | Loss: 0.00001675
Iteration 52/1000 | Loss: 0.00001675
Iteration 53/1000 | Loss: 0.00001675
Iteration 54/1000 | Loss: 0.00001675
Iteration 55/1000 | Loss: 0.00001675
Iteration 56/1000 | Loss: 0.00001675
Iteration 57/1000 | Loss: 0.00001675
Iteration 58/1000 | Loss: 0.00001674
Iteration 59/1000 | Loss: 0.00001674
Iteration 60/1000 | Loss: 0.00001674
Iteration 61/1000 | Loss: 0.00001674
Iteration 62/1000 | Loss: 0.00001674
Iteration 63/1000 | Loss: 0.00001674
Iteration 64/1000 | Loss: 0.00001673
Iteration 65/1000 | Loss: 0.00001673
Iteration 66/1000 | Loss: 0.00001673
Iteration 67/1000 | Loss: 0.00001673
Iteration 68/1000 | Loss: 0.00001672
Iteration 69/1000 | Loss: 0.00001672
Iteration 70/1000 | Loss: 0.00001672
Iteration 71/1000 | Loss: 0.00001671
Iteration 72/1000 | Loss: 0.00001671
Iteration 73/1000 | Loss: 0.00001671
Iteration 74/1000 | Loss: 0.00001671
Iteration 75/1000 | Loss: 0.00001670
Iteration 76/1000 | Loss: 0.00001670
Iteration 77/1000 | Loss: 0.00001670
Iteration 78/1000 | Loss: 0.00001669
Iteration 79/1000 | Loss: 0.00001669
Iteration 80/1000 | Loss: 0.00001669
Iteration 81/1000 | Loss: 0.00001669
Iteration 82/1000 | Loss: 0.00001669
Iteration 83/1000 | Loss: 0.00001668
Iteration 84/1000 | Loss: 0.00001668
Iteration 85/1000 | Loss: 0.00001668
Iteration 86/1000 | Loss: 0.00001667
Iteration 87/1000 | Loss: 0.00001667
Iteration 88/1000 | Loss: 0.00001667
Iteration 89/1000 | Loss: 0.00001667
Iteration 90/1000 | Loss: 0.00001666
Iteration 91/1000 | Loss: 0.00001666
Iteration 92/1000 | Loss: 0.00001666
Iteration 93/1000 | Loss: 0.00001666
Iteration 94/1000 | Loss: 0.00001666
Iteration 95/1000 | Loss: 0.00001666
Iteration 96/1000 | Loss: 0.00001666
Iteration 97/1000 | Loss: 0.00001666
Iteration 98/1000 | Loss: 0.00001666
Iteration 99/1000 | Loss: 0.00001666
Iteration 100/1000 | Loss: 0.00001666
Iteration 101/1000 | Loss: 0.00001665
Iteration 102/1000 | Loss: 0.00001665
Iteration 103/1000 | Loss: 0.00001665
Iteration 104/1000 | Loss: 0.00001665
Iteration 105/1000 | Loss: 0.00001665
Iteration 106/1000 | Loss: 0.00001665
Iteration 107/1000 | Loss: 0.00001665
Iteration 108/1000 | Loss: 0.00001665
Iteration 109/1000 | Loss: 0.00001664
Iteration 110/1000 | Loss: 0.00001664
Iteration 111/1000 | Loss: 0.00001664
Iteration 112/1000 | Loss: 0.00001664
Iteration 113/1000 | Loss: 0.00001664
Iteration 114/1000 | Loss: 0.00001664
Iteration 115/1000 | Loss: 0.00001664
Iteration 116/1000 | Loss: 0.00001664
Iteration 117/1000 | Loss: 0.00001664
Iteration 118/1000 | Loss: 0.00001663
Iteration 119/1000 | Loss: 0.00001663
Iteration 120/1000 | Loss: 0.00001663
Iteration 121/1000 | Loss: 0.00001663
Iteration 122/1000 | Loss: 0.00001663
Iteration 123/1000 | Loss: 0.00001663
Iteration 124/1000 | Loss: 0.00001663
Iteration 125/1000 | Loss: 0.00001663
Iteration 126/1000 | Loss: 0.00001663
Iteration 127/1000 | Loss: 0.00001663
Iteration 128/1000 | Loss: 0.00001662
Iteration 129/1000 | Loss: 0.00001662
Iteration 130/1000 | Loss: 0.00001662
Iteration 131/1000 | Loss: 0.00001662
Iteration 132/1000 | Loss: 0.00001662
Iteration 133/1000 | Loss: 0.00001662
Iteration 134/1000 | Loss: 0.00001662
Iteration 135/1000 | Loss: 0.00001662
Iteration 136/1000 | Loss: 0.00001662
Iteration 137/1000 | Loss: 0.00001662
Iteration 138/1000 | Loss: 0.00001662
Iteration 139/1000 | Loss: 0.00001662
Iteration 140/1000 | Loss: 0.00001662
Iteration 141/1000 | Loss: 0.00001662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.6622536350041628e-05, 1.6622536350041628e-05, 1.6622536350041628e-05, 1.6622536350041628e-05, 1.6622536350041628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6622536350041628e-05

Optimization complete. Final v2v error: 3.516510009765625 mm

Highest mean error: 3.800137996673584 mm for frame 15

Lowest mean error: 3.2353789806365967 mm for frame 76

Saving results

Total time: 43.53213357925415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457927
Iteration 2/25 | Loss: 0.00092364
Iteration 3/25 | Loss: 0.00082119
Iteration 4/25 | Loss: 0.00079483
Iteration 5/25 | Loss: 0.00078880
Iteration 6/25 | Loss: 0.00078825
Iteration 7/25 | Loss: 0.00078825
Iteration 8/25 | Loss: 0.00078825
Iteration 9/25 | Loss: 0.00078825
Iteration 10/25 | Loss: 0.00078825
Iteration 11/25 | Loss: 0.00078825
Iteration 12/25 | Loss: 0.00078825
Iteration 13/25 | Loss: 0.00078825
Iteration 14/25 | Loss: 0.00078825
Iteration 15/25 | Loss: 0.00078825
Iteration 16/25 | Loss: 0.00078825
Iteration 17/25 | Loss: 0.00078825
Iteration 18/25 | Loss: 0.00078825
Iteration 19/25 | Loss: 0.00078825
Iteration 20/25 | Loss: 0.00078825
Iteration 21/25 | Loss: 0.00078825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007882456411607563, 0.0007882456411607563, 0.0007882456411607563, 0.0007882456411607563, 0.0007882456411607563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007882456411607563

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96788490
Iteration 2/25 | Loss: 0.00040295
Iteration 3/25 | Loss: 0.00040291
Iteration 4/25 | Loss: 0.00040291
Iteration 5/25 | Loss: 0.00040291
Iteration 6/25 | Loss: 0.00040291
Iteration 7/25 | Loss: 0.00040291
Iteration 8/25 | Loss: 0.00040291
Iteration 9/25 | Loss: 0.00040291
Iteration 10/25 | Loss: 0.00040291
Iteration 11/25 | Loss: 0.00040291
Iteration 12/25 | Loss: 0.00040291
Iteration 13/25 | Loss: 0.00040291
Iteration 14/25 | Loss: 0.00040291
Iteration 15/25 | Loss: 0.00040291
Iteration 16/25 | Loss: 0.00040291
Iteration 17/25 | Loss: 0.00040291
Iteration 18/25 | Loss: 0.00040291
Iteration 19/25 | Loss: 0.00040291
Iteration 20/25 | Loss: 0.00040291
Iteration 21/25 | Loss: 0.00040291
Iteration 22/25 | Loss: 0.00040291
Iteration 23/25 | Loss: 0.00040291
Iteration 24/25 | Loss: 0.00040291
Iteration 25/25 | Loss: 0.00040291

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040291
Iteration 2/1000 | Loss: 0.00005338
Iteration 3/1000 | Loss: 0.00003589
Iteration 4/1000 | Loss: 0.00003171
Iteration 5/1000 | Loss: 0.00002899
Iteration 6/1000 | Loss: 0.00002767
Iteration 7/1000 | Loss: 0.00002695
Iteration 8/1000 | Loss: 0.00002650
Iteration 9/1000 | Loss: 0.00002630
Iteration 10/1000 | Loss: 0.00002609
Iteration 11/1000 | Loss: 0.00002597
Iteration 12/1000 | Loss: 0.00002596
Iteration 13/1000 | Loss: 0.00002595
Iteration 14/1000 | Loss: 0.00002588
Iteration 15/1000 | Loss: 0.00002585
Iteration 16/1000 | Loss: 0.00002584
Iteration 17/1000 | Loss: 0.00002583
Iteration 18/1000 | Loss: 0.00002582
Iteration 19/1000 | Loss: 0.00002581
Iteration 20/1000 | Loss: 0.00002580
Iteration 21/1000 | Loss: 0.00002579
Iteration 22/1000 | Loss: 0.00002578
Iteration 23/1000 | Loss: 0.00002578
Iteration 24/1000 | Loss: 0.00002573
Iteration 25/1000 | Loss: 0.00002571
Iteration 26/1000 | Loss: 0.00002571
Iteration 27/1000 | Loss: 0.00002570
Iteration 28/1000 | Loss: 0.00002569
Iteration 29/1000 | Loss: 0.00002569
Iteration 30/1000 | Loss: 0.00002568
Iteration 31/1000 | Loss: 0.00002568
Iteration 32/1000 | Loss: 0.00002566
Iteration 33/1000 | Loss: 0.00002564
Iteration 34/1000 | Loss: 0.00002564
Iteration 35/1000 | Loss: 0.00002563
Iteration 36/1000 | Loss: 0.00002563
Iteration 37/1000 | Loss: 0.00002562
Iteration 38/1000 | Loss: 0.00002560
Iteration 39/1000 | Loss: 0.00002560
Iteration 40/1000 | Loss: 0.00002559
Iteration 41/1000 | Loss: 0.00002559
Iteration 42/1000 | Loss: 0.00002559
Iteration 43/1000 | Loss: 0.00002558
Iteration 44/1000 | Loss: 0.00002558
Iteration 45/1000 | Loss: 0.00002557
Iteration 46/1000 | Loss: 0.00002557
Iteration 47/1000 | Loss: 0.00002557
Iteration 48/1000 | Loss: 0.00002556
Iteration 49/1000 | Loss: 0.00002556
Iteration 50/1000 | Loss: 0.00002555
Iteration 51/1000 | Loss: 0.00002555
Iteration 52/1000 | Loss: 0.00002555
Iteration 53/1000 | Loss: 0.00002554
Iteration 54/1000 | Loss: 0.00002554
Iteration 55/1000 | Loss: 0.00002554
Iteration 56/1000 | Loss: 0.00002553
Iteration 57/1000 | Loss: 0.00002553
Iteration 58/1000 | Loss: 0.00002553
Iteration 59/1000 | Loss: 0.00002553
Iteration 60/1000 | Loss: 0.00002552
Iteration 61/1000 | Loss: 0.00002552
Iteration 62/1000 | Loss: 0.00002552
Iteration 63/1000 | Loss: 0.00002551
Iteration 64/1000 | Loss: 0.00002551
Iteration 65/1000 | Loss: 0.00002551
Iteration 66/1000 | Loss: 0.00002551
Iteration 67/1000 | Loss: 0.00002551
Iteration 68/1000 | Loss: 0.00002551
Iteration 69/1000 | Loss: 0.00002550
Iteration 70/1000 | Loss: 0.00002550
Iteration 71/1000 | Loss: 0.00002550
Iteration 72/1000 | Loss: 0.00002550
Iteration 73/1000 | Loss: 0.00002550
Iteration 74/1000 | Loss: 0.00002550
Iteration 75/1000 | Loss: 0.00002550
Iteration 76/1000 | Loss: 0.00002550
Iteration 77/1000 | Loss: 0.00002550
Iteration 78/1000 | Loss: 0.00002550
Iteration 79/1000 | Loss: 0.00002550
Iteration 80/1000 | Loss: 0.00002549
Iteration 81/1000 | Loss: 0.00002549
Iteration 82/1000 | Loss: 0.00002549
Iteration 83/1000 | Loss: 0.00002549
Iteration 84/1000 | Loss: 0.00002549
Iteration 85/1000 | Loss: 0.00002549
Iteration 86/1000 | Loss: 0.00002549
Iteration 87/1000 | Loss: 0.00002549
Iteration 88/1000 | Loss: 0.00002549
Iteration 89/1000 | Loss: 0.00002549
Iteration 90/1000 | Loss: 0.00002548
Iteration 91/1000 | Loss: 0.00002548
Iteration 92/1000 | Loss: 0.00002548
Iteration 93/1000 | Loss: 0.00002548
Iteration 94/1000 | Loss: 0.00002548
Iteration 95/1000 | Loss: 0.00002548
Iteration 96/1000 | Loss: 0.00002547
Iteration 97/1000 | Loss: 0.00002547
Iteration 98/1000 | Loss: 0.00002547
Iteration 99/1000 | Loss: 0.00002547
Iteration 100/1000 | Loss: 0.00002547
Iteration 101/1000 | Loss: 0.00002547
Iteration 102/1000 | Loss: 0.00002547
Iteration 103/1000 | Loss: 0.00002547
Iteration 104/1000 | Loss: 0.00002547
Iteration 105/1000 | Loss: 0.00002547
Iteration 106/1000 | Loss: 0.00002547
Iteration 107/1000 | Loss: 0.00002547
Iteration 108/1000 | Loss: 0.00002547
Iteration 109/1000 | Loss: 0.00002546
Iteration 110/1000 | Loss: 0.00002546
Iteration 111/1000 | Loss: 0.00002546
Iteration 112/1000 | Loss: 0.00002546
Iteration 113/1000 | Loss: 0.00002546
Iteration 114/1000 | Loss: 0.00002546
Iteration 115/1000 | Loss: 0.00002546
Iteration 116/1000 | Loss: 0.00002546
Iteration 117/1000 | Loss: 0.00002546
Iteration 118/1000 | Loss: 0.00002546
Iteration 119/1000 | Loss: 0.00002546
Iteration 120/1000 | Loss: 0.00002546
Iteration 121/1000 | Loss: 0.00002546
Iteration 122/1000 | Loss: 0.00002546
Iteration 123/1000 | Loss: 0.00002546
Iteration 124/1000 | Loss: 0.00002546
Iteration 125/1000 | Loss: 0.00002546
Iteration 126/1000 | Loss: 0.00002546
Iteration 127/1000 | Loss: 0.00002546
Iteration 128/1000 | Loss: 0.00002546
Iteration 129/1000 | Loss: 0.00002546
Iteration 130/1000 | Loss: 0.00002546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.545745337556582e-05, 2.545745337556582e-05, 2.545745337556582e-05, 2.545745337556582e-05, 2.545745337556582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.545745337556582e-05

Optimization complete. Final v2v error: 4.3546857833862305 mm

Highest mean error: 4.658100128173828 mm for frame 152

Lowest mean error: 3.721686840057373 mm for frame 8

Saving results

Total time: 39.1001923084259
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398523
Iteration 2/25 | Loss: 0.00083017
Iteration 3/25 | Loss: 0.00074097
Iteration 4/25 | Loss: 0.00072489
Iteration 5/25 | Loss: 0.00072071
Iteration 6/25 | Loss: 0.00072013
Iteration 7/25 | Loss: 0.00072013
Iteration 8/25 | Loss: 0.00072013
Iteration 9/25 | Loss: 0.00072013
Iteration 10/25 | Loss: 0.00072013
Iteration 11/25 | Loss: 0.00072013
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007201279513537884, 0.0007201279513537884, 0.0007201279513537884, 0.0007201279513537884, 0.0007201279513537884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007201279513537884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.94582129
Iteration 2/25 | Loss: 0.00022383
Iteration 3/25 | Loss: 0.00022383
Iteration 4/25 | Loss: 0.00022383
Iteration 5/25 | Loss: 0.00022382
Iteration 6/25 | Loss: 0.00022382
Iteration 7/25 | Loss: 0.00022382
Iteration 8/25 | Loss: 0.00022382
Iteration 9/25 | Loss: 0.00022382
Iteration 10/25 | Loss: 0.00022382
Iteration 11/25 | Loss: 0.00022382
Iteration 12/25 | Loss: 0.00022382
Iteration 13/25 | Loss: 0.00022382
Iteration 14/25 | Loss: 0.00022382
Iteration 15/25 | Loss: 0.00022382
Iteration 16/25 | Loss: 0.00022382
Iteration 17/25 | Loss: 0.00022382
Iteration 18/25 | Loss: 0.00022382
Iteration 19/25 | Loss: 0.00022382
Iteration 20/25 | Loss: 0.00022382
Iteration 21/25 | Loss: 0.00022382
Iteration 22/25 | Loss: 0.00022382
Iteration 23/25 | Loss: 0.00022382
Iteration 24/25 | Loss: 0.00022382
Iteration 25/25 | Loss: 0.00022382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022382
Iteration 2/1000 | Loss: 0.00004415
Iteration 3/1000 | Loss: 0.00003028
Iteration 4/1000 | Loss: 0.00002693
Iteration 5/1000 | Loss: 0.00002474
Iteration 6/1000 | Loss: 0.00002342
Iteration 7/1000 | Loss: 0.00002256
Iteration 8/1000 | Loss: 0.00002227
Iteration 9/1000 | Loss: 0.00002219
Iteration 10/1000 | Loss: 0.00002217
Iteration 11/1000 | Loss: 0.00002217
Iteration 12/1000 | Loss: 0.00002208
Iteration 13/1000 | Loss: 0.00002205
Iteration 14/1000 | Loss: 0.00002198
Iteration 15/1000 | Loss: 0.00002197
Iteration 16/1000 | Loss: 0.00002191
Iteration 17/1000 | Loss: 0.00002185
Iteration 18/1000 | Loss: 0.00002184
Iteration 19/1000 | Loss: 0.00002183
Iteration 20/1000 | Loss: 0.00002183
Iteration 21/1000 | Loss: 0.00002180
Iteration 22/1000 | Loss: 0.00002177
Iteration 23/1000 | Loss: 0.00002175
Iteration 24/1000 | Loss: 0.00002174
Iteration 25/1000 | Loss: 0.00002174
Iteration 26/1000 | Loss: 0.00002174
Iteration 27/1000 | Loss: 0.00002173
Iteration 28/1000 | Loss: 0.00002173
Iteration 29/1000 | Loss: 0.00002172
Iteration 30/1000 | Loss: 0.00002172
Iteration 31/1000 | Loss: 0.00002170
Iteration 32/1000 | Loss: 0.00002170
Iteration 33/1000 | Loss: 0.00002170
Iteration 34/1000 | Loss: 0.00002170
Iteration 35/1000 | Loss: 0.00002167
Iteration 36/1000 | Loss: 0.00002167
Iteration 37/1000 | Loss: 0.00002166
Iteration 38/1000 | Loss: 0.00002166
Iteration 39/1000 | Loss: 0.00002166
Iteration 40/1000 | Loss: 0.00002166
Iteration 41/1000 | Loss: 0.00002166
Iteration 42/1000 | Loss: 0.00002166
Iteration 43/1000 | Loss: 0.00002166
Iteration 44/1000 | Loss: 0.00002166
Iteration 45/1000 | Loss: 0.00002166
Iteration 46/1000 | Loss: 0.00002166
Iteration 47/1000 | Loss: 0.00002165
Iteration 48/1000 | Loss: 0.00002164
Iteration 49/1000 | Loss: 0.00002164
Iteration 50/1000 | Loss: 0.00002164
Iteration 51/1000 | Loss: 0.00002163
Iteration 52/1000 | Loss: 0.00002163
Iteration 53/1000 | Loss: 0.00002162
Iteration 54/1000 | Loss: 0.00002162
Iteration 55/1000 | Loss: 0.00002162
Iteration 56/1000 | Loss: 0.00002162
Iteration 57/1000 | Loss: 0.00002159
Iteration 58/1000 | Loss: 0.00002158
Iteration 59/1000 | Loss: 0.00002158
Iteration 60/1000 | Loss: 0.00002158
Iteration 61/1000 | Loss: 0.00002158
Iteration 62/1000 | Loss: 0.00002158
Iteration 63/1000 | Loss: 0.00002158
Iteration 64/1000 | Loss: 0.00002158
Iteration 65/1000 | Loss: 0.00002158
Iteration 66/1000 | Loss: 0.00002158
Iteration 67/1000 | Loss: 0.00002158
Iteration 68/1000 | Loss: 0.00002158
Iteration 69/1000 | Loss: 0.00002158
Iteration 70/1000 | Loss: 0.00002158
Iteration 71/1000 | Loss: 0.00002158
Iteration 72/1000 | Loss: 0.00002158
Iteration 73/1000 | Loss: 0.00002158
Iteration 74/1000 | Loss: 0.00002158
Iteration 75/1000 | Loss: 0.00002158
Iteration 76/1000 | Loss: 0.00002158
Iteration 77/1000 | Loss: 0.00002158
Iteration 78/1000 | Loss: 0.00002158
Iteration 79/1000 | Loss: 0.00002158
Iteration 80/1000 | Loss: 0.00002158
Iteration 81/1000 | Loss: 0.00002158
Iteration 82/1000 | Loss: 0.00002158
Iteration 83/1000 | Loss: 0.00002158
Iteration 84/1000 | Loss: 0.00002158
Iteration 85/1000 | Loss: 0.00002158
Iteration 86/1000 | Loss: 0.00002158
Iteration 87/1000 | Loss: 0.00002158
Iteration 88/1000 | Loss: 0.00002158
Iteration 89/1000 | Loss: 0.00002158
Iteration 90/1000 | Loss: 0.00002158
Iteration 91/1000 | Loss: 0.00002158
Iteration 92/1000 | Loss: 0.00002158
Iteration 93/1000 | Loss: 0.00002158
Iteration 94/1000 | Loss: 0.00002158
Iteration 95/1000 | Loss: 0.00002158
Iteration 96/1000 | Loss: 0.00002158
Iteration 97/1000 | Loss: 0.00002158
Iteration 98/1000 | Loss: 0.00002158
Iteration 99/1000 | Loss: 0.00002158
Iteration 100/1000 | Loss: 0.00002158
Iteration 101/1000 | Loss: 0.00002158
Iteration 102/1000 | Loss: 0.00002158
Iteration 103/1000 | Loss: 0.00002158
Iteration 104/1000 | Loss: 0.00002158
Iteration 105/1000 | Loss: 0.00002158
Iteration 106/1000 | Loss: 0.00002158
Iteration 107/1000 | Loss: 0.00002158
Iteration 108/1000 | Loss: 0.00002158
Iteration 109/1000 | Loss: 0.00002158
Iteration 110/1000 | Loss: 0.00002158
Iteration 111/1000 | Loss: 0.00002158
Iteration 112/1000 | Loss: 0.00002158
Iteration 113/1000 | Loss: 0.00002158
Iteration 114/1000 | Loss: 0.00002158
Iteration 115/1000 | Loss: 0.00002158
Iteration 116/1000 | Loss: 0.00002158
Iteration 117/1000 | Loss: 0.00002158
Iteration 118/1000 | Loss: 0.00002158
Iteration 119/1000 | Loss: 0.00002158
Iteration 120/1000 | Loss: 0.00002158
Iteration 121/1000 | Loss: 0.00002158
Iteration 122/1000 | Loss: 0.00002158
Iteration 123/1000 | Loss: 0.00002158
Iteration 124/1000 | Loss: 0.00002158
Iteration 125/1000 | Loss: 0.00002158
Iteration 126/1000 | Loss: 0.00002158
Iteration 127/1000 | Loss: 0.00002158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.1579480744549073e-05, 2.1579480744549073e-05, 2.1579480744549073e-05, 2.1579480744549073e-05, 2.1579480744549073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1579480744549073e-05

Optimization complete. Final v2v error: 3.9203529357910156 mm

Highest mean error: 4.21903133392334 mm for frame 46

Lowest mean error: 3.7009477615356445 mm for frame 124

Saving results

Total time: 34.66439390182495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00758020
Iteration 2/25 | Loss: 0.00123753
Iteration 3/25 | Loss: 0.00094802
Iteration 4/25 | Loss: 0.00089041
Iteration 5/25 | Loss: 0.00087883
Iteration 6/25 | Loss: 0.00087650
Iteration 7/25 | Loss: 0.00087903
Iteration 8/25 | Loss: 0.00088313
Iteration 9/25 | Loss: 0.00088203
Iteration 10/25 | Loss: 0.00087724
Iteration 11/25 | Loss: 0.00087515
Iteration 12/25 | Loss: 0.00087326
Iteration 13/25 | Loss: 0.00087333
Iteration 14/25 | Loss: 0.00087408
Iteration 15/25 | Loss: 0.00087261
Iteration 16/25 | Loss: 0.00087302
Iteration 17/25 | Loss: 0.00087259
Iteration 18/25 | Loss: 0.00087370
Iteration 19/25 | Loss: 0.00087233
Iteration 20/25 | Loss: 0.00086993
Iteration 21/25 | Loss: 0.00086883
Iteration 22/25 | Loss: 0.00087050
Iteration 23/25 | Loss: 0.00086817
Iteration 24/25 | Loss: 0.00086466
Iteration 25/25 | Loss: 0.00086455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25138175
Iteration 2/25 | Loss: 0.00040817
Iteration 3/25 | Loss: 0.00040813
Iteration 4/25 | Loss: 0.00040813
Iteration 5/25 | Loss: 0.00040813
Iteration 6/25 | Loss: 0.00040813
Iteration 7/25 | Loss: 0.00040813
Iteration 8/25 | Loss: 0.00040813
Iteration 9/25 | Loss: 0.00040813
Iteration 10/25 | Loss: 0.00040813
Iteration 11/25 | Loss: 0.00040813
Iteration 12/25 | Loss: 0.00040813
Iteration 13/25 | Loss: 0.00040813
Iteration 14/25 | Loss: 0.00040813
Iteration 15/25 | Loss: 0.00040813
Iteration 16/25 | Loss: 0.00040813
Iteration 17/25 | Loss: 0.00040813
Iteration 18/25 | Loss: 0.00040813
Iteration 19/25 | Loss: 0.00040813
Iteration 20/25 | Loss: 0.00040813
Iteration 21/25 | Loss: 0.00040813
Iteration 22/25 | Loss: 0.00040813
Iteration 23/25 | Loss: 0.00040813
Iteration 24/25 | Loss: 0.00040813
Iteration 25/25 | Loss: 0.00040813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040813
Iteration 2/1000 | Loss: 0.00006998
Iteration 3/1000 | Loss: 0.00004178
Iteration 4/1000 | Loss: 0.00003620
Iteration 5/1000 | Loss: 0.00010354
Iteration 6/1000 | Loss: 0.00010921
Iteration 7/1000 | Loss: 0.00015453
Iteration 8/1000 | Loss: 0.00015916
Iteration 9/1000 | Loss: 0.00014964
Iteration 10/1000 | Loss: 0.00011860
Iteration 11/1000 | Loss: 0.00018428
Iteration 12/1000 | Loss: 0.00017411
Iteration 13/1000 | Loss: 0.00018968
Iteration 14/1000 | Loss: 0.00015891
Iteration 15/1000 | Loss: 0.00016950
Iteration 16/1000 | Loss: 0.00012215
Iteration 17/1000 | Loss: 0.00017414
Iteration 18/1000 | Loss: 0.00004862
Iteration 19/1000 | Loss: 0.00015041
Iteration 20/1000 | Loss: 0.00005222
Iteration 21/1000 | Loss: 0.00010815
Iteration 22/1000 | Loss: 0.00010278
Iteration 23/1000 | Loss: 0.00018471
Iteration 24/1000 | Loss: 0.00009888
Iteration 25/1000 | Loss: 0.00006985
Iteration 26/1000 | Loss: 0.00014074
Iteration 27/1000 | Loss: 0.00008988
Iteration 28/1000 | Loss: 0.00009954
Iteration 29/1000 | Loss: 0.00011246
Iteration 30/1000 | Loss: 0.00010704
Iteration 31/1000 | Loss: 0.00012045
Iteration 32/1000 | Loss: 0.00019821
Iteration 33/1000 | Loss: 0.00018716
Iteration 34/1000 | Loss: 0.00008419
Iteration 35/1000 | Loss: 0.00021446
Iteration 36/1000 | Loss: 0.00024228
Iteration 37/1000 | Loss: 0.00010232
Iteration 38/1000 | Loss: 0.00005616
Iteration 39/1000 | Loss: 0.00012506
Iteration 40/1000 | Loss: 0.00025642
Iteration 41/1000 | Loss: 0.00019024
Iteration 42/1000 | Loss: 0.00010635
Iteration 43/1000 | Loss: 0.00006431
Iteration 44/1000 | Loss: 0.00016299
Iteration 45/1000 | Loss: 0.00010026
Iteration 46/1000 | Loss: 0.00020809
Iteration 47/1000 | Loss: 0.00026518
Iteration 48/1000 | Loss: 0.00014490
Iteration 49/1000 | Loss: 0.00008149
Iteration 50/1000 | Loss: 0.00013778
Iteration 51/1000 | Loss: 0.00011045
Iteration 52/1000 | Loss: 0.00005360
Iteration 53/1000 | Loss: 0.00016000
Iteration 54/1000 | Loss: 0.00011568
Iteration 55/1000 | Loss: 0.00008606
Iteration 56/1000 | Loss: 0.00009218
Iteration 57/1000 | Loss: 0.00014862
Iteration 58/1000 | Loss: 0.00010285
Iteration 59/1000 | Loss: 0.00011600
Iteration 60/1000 | Loss: 0.00011570
Iteration 61/1000 | Loss: 0.00011267
Iteration 62/1000 | Loss: 0.00019556
Iteration 63/1000 | Loss: 0.00004431
Iteration 64/1000 | Loss: 0.00004859
Iteration 65/1000 | Loss: 0.00003115
Iteration 66/1000 | Loss: 0.00003030
Iteration 67/1000 | Loss: 0.00002971
Iteration 68/1000 | Loss: 0.00002937
Iteration 69/1000 | Loss: 0.00002915
Iteration 70/1000 | Loss: 0.00002879
Iteration 71/1000 | Loss: 0.00002859
Iteration 72/1000 | Loss: 0.00002846
Iteration 73/1000 | Loss: 0.00002830
Iteration 74/1000 | Loss: 0.00002825
Iteration 75/1000 | Loss: 0.00002825
Iteration 76/1000 | Loss: 0.00002823
Iteration 77/1000 | Loss: 0.00002823
Iteration 78/1000 | Loss: 0.00002822
Iteration 79/1000 | Loss: 0.00002822
Iteration 80/1000 | Loss: 0.00002820
Iteration 81/1000 | Loss: 0.00002815
Iteration 82/1000 | Loss: 0.00002815
Iteration 83/1000 | Loss: 0.00002815
Iteration 84/1000 | Loss: 0.00002815
Iteration 85/1000 | Loss: 0.00002814
Iteration 86/1000 | Loss: 0.00002813
Iteration 87/1000 | Loss: 0.00002812
Iteration 88/1000 | Loss: 0.00002812
Iteration 89/1000 | Loss: 0.00002812
Iteration 90/1000 | Loss: 0.00002811
Iteration 91/1000 | Loss: 0.00002811
Iteration 92/1000 | Loss: 0.00002811
Iteration 93/1000 | Loss: 0.00002810
Iteration 94/1000 | Loss: 0.00002810
Iteration 95/1000 | Loss: 0.00002809
Iteration 96/1000 | Loss: 0.00002809
Iteration 97/1000 | Loss: 0.00002809
Iteration 98/1000 | Loss: 0.00002808
Iteration 99/1000 | Loss: 0.00002808
Iteration 100/1000 | Loss: 0.00002808
Iteration 101/1000 | Loss: 0.00002808
Iteration 102/1000 | Loss: 0.00002807
Iteration 103/1000 | Loss: 0.00002807
Iteration 104/1000 | Loss: 0.00002807
Iteration 105/1000 | Loss: 0.00002807
Iteration 106/1000 | Loss: 0.00002806
Iteration 107/1000 | Loss: 0.00002806
Iteration 108/1000 | Loss: 0.00002806
Iteration 109/1000 | Loss: 0.00002806
Iteration 110/1000 | Loss: 0.00002806
Iteration 111/1000 | Loss: 0.00002805
Iteration 112/1000 | Loss: 0.00002805
Iteration 113/1000 | Loss: 0.00002804
Iteration 114/1000 | Loss: 0.00002804
Iteration 115/1000 | Loss: 0.00002804
Iteration 116/1000 | Loss: 0.00002803
Iteration 117/1000 | Loss: 0.00002803
Iteration 118/1000 | Loss: 0.00002803
Iteration 119/1000 | Loss: 0.00002802
Iteration 120/1000 | Loss: 0.00002802
Iteration 121/1000 | Loss: 0.00002802
Iteration 122/1000 | Loss: 0.00002802
Iteration 123/1000 | Loss: 0.00002802
Iteration 124/1000 | Loss: 0.00002802
Iteration 125/1000 | Loss: 0.00002802
Iteration 126/1000 | Loss: 0.00002802
Iteration 127/1000 | Loss: 0.00002802
Iteration 128/1000 | Loss: 0.00002801
Iteration 129/1000 | Loss: 0.00002801
Iteration 130/1000 | Loss: 0.00002801
Iteration 131/1000 | Loss: 0.00002801
Iteration 132/1000 | Loss: 0.00002800
Iteration 133/1000 | Loss: 0.00002800
Iteration 134/1000 | Loss: 0.00002799
Iteration 135/1000 | Loss: 0.00002799
Iteration 136/1000 | Loss: 0.00002799
Iteration 137/1000 | Loss: 0.00002798
Iteration 138/1000 | Loss: 0.00002798
Iteration 139/1000 | Loss: 0.00002798
Iteration 140/1000 | Loss: 0.00002798
Iteration 141/1000 | Loss: 0.00002798
Iteration 142/1000 | Loss: 0.00002798
Iteration 143/1000 | Loss: 0.00002797
Iteration 144/1000 | Loss: 0.00002797
Iteration 145/1000 | Loss: 0.00002797
Iteration 146/1000 | Loss: 0.00002797
Iteration 147/1000 | Loss: 0.00002797
Iteration 148/1000 | Loss: 0.00002797
Iteration 149/1000 | Loss: 0.00002797
Iteration 150/1000 | Loss: 0.00002797
Iteration 151/1000 | Loss: 0.00002797
Iteration 152/1000 | Loss: 0.00002796
Iteration 153/1000 | Loss: 0.00002796
Iteration 154/1000 | Loss: 0.00002795
Iteration 155/1000 | Loss: 0.00002795
Iteration 156/1000 | Loss: 0.00002795
Iteration 157/1000 | Loss: 0.00002795
Iteration 158/1000 | Loss: 0.00002795
Iteration 159/1000 | Loss: 0.00002794
Iteration 160/1000 | Loss: 0.00002794
Iteration 161/1000 | Loss: 0.00002794
Iteration 162/1000 | Loss: 0.00002794
Iteration 163/1000 | Loss: 0.00002793
Iteration 164/1000 | Loss: 0.00002793
Iteration 165/1000 | Loss: 0.00002793
Iteration 166/1000 | Loss: 0.00002793
Iteration 167/1000 | Loss: 0.00002793
Iteration 168/1000 | Loss: 0.00002793
Iteration 169/1000 | Loss: 0.00002792
Iteration 170/1000 | Loss: 0.00002792
Iteration 171/1000 | Loss: 0.00002792
Iteration 172/1000 | Loss: 0.00002792
Iteration 173/1000 | Loss: 0.00002792
Iteration 174/1000 | Loss: 0.00002792
Iteration 175/1000 | Loss: 0.00002792
Iteration 176/1000 | Loss: 0.00002792
Iteration 177/1000 | Loss: 0.00002792
Iteration 178/1000 | Loss: 0.00002792
Iteration 179/1000 | Loss: 0.00002792
Iteration 180/1000 | Loss: 0.00002792
Iteration 181/1000 | Loss: 0.00002792
Iteration 182/1000 | Loss: 0.00002792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.7920248612645082e-05, 2.7920248612645082e-05, 2.7920248612645082e-05, 2.7920248612645082e-05, 2.7920248612645082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7920248612645082e-05

Optimization complete. Final v2v error: 4.48978328704834 mm

Highest mean error: 5.233205318450928 mm for frame 141

Lowest mean error: 3.857527017593384 mm for frame 34

Saving results

Total time: 179.2848162651062
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00334709
Iteration 2/25 | Loss: 0.00104346
Iteration 3/25 | Loss: 0.00080122
Iteration 4/25 | Loss: 0.00072398
Iteration 5/25 | Loss: 0.00070349
Iteration 6/25 | Loss: 0.00070002
Iteration 7/25 | Loss: 0.00069885
Iteration 8/25 | Loss: 0.00069856
Iteration 9/25 | Loss: 0.00069844
Iteration 10/25 | Loss: 0.00069844
Iteration 11/25 | Loss: 0.00069844
Iteration 12/25 | Loss: 0.00069844
Iteration 13/25 | Loss: 0.00069844
Iteration 14/25 | Loss: 0.00069844
Iteration 15/25 | Loss: 0.00069844
Iteration 16/25 | Loss: 0.00069844
Iteration 17/25 | Loss: 0.00069844
Iteration 18/25 | Loss: 0.00069844
Iteration 19/25 | Loss: 0.00069844
Iteration 20/25 | Loss: 0.00069844
Iteration 21/25 | Loss: 0.00069844
Iteration 22/25 | Loss: 0.00069844
Iteration 23/25 | Loss: 0.00069844
Iteration 24/25 | Loss: 0.00069844
Iteration 25/25 | Loss: 0.00069844

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35668445
Iteration 2/25 | Loss: 0.00024383
Iteration 3/25 | Loss: 0.00024383
Iteration 4/25 | Loss: 0.00024382
Iteration 5/25 | Loss: 0.00024382
Iteration 6/25 | Loss: 0.00024382
Iteration 7/25 | Loss: 0.00024382
Iteration 8/25 | Loss: 0.00024382
Iteration 9/25 | Loss: 0.00024382
Iteration 10/25 | Loss: 0.00024382
Iteration 11/25 | Loss: 0.00024382
Iteration 12/25 | Loss: 0.00024382
Iteration 13/25 | Loss: 0.00024382
Iteration 14/25 | Loss: 0.00024382
Iteration 15/25 | Loss: 0.00024382
Iteration 16/25 | Loss: 0.00024382
Iteration 17/25 | Loss: 0.00024382
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00024382295669056475, 0.00024382295669056475, 0.00024382295669056475, 0.00024382295669056475, 0.00024382295669056475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024382295669056475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024382
Iteration 2/1000 | Loss: 0.00005077
Iteration 3/1000 | Loss: 0.00003099
Iteration 4/1000 | Loss: 0.00002614
Iteration 5/1000 | Loss: 0.00002427
Iteration 6/1000 | Loss: 0.00002299
Iteration 7/1000 | Loss: 0.00002233
Iteration 8/1000 | Loss: 0.00002175
Iteration 9/1000 | Loss: 0.00002127
Iteration 10/1000 | Loss: 0.00002088
Iteration 11/1000 | Loss: 0.00002072
Iteration 12/1000 | Loss: 0.00002057
Iteration 13/1000 | Loss: 0.00002052
Iteration 14/1000 | Loss: 0.00002045
Iteration 15/1000 | Loss: 0.00002037
Iteration 16/1000 | Loss: 0.00002037
Iteration 17/1000 | Loss: 0.00002035
Iteration 18/1000 | Loss: 0.00002033
Iteration 19/1000 | Loss: 0.00002032
Iteration 20/1000 | Loss: 0.00002032
Iteration 21/1000 | Loss: 0.00002031
Iteration 22/1000 | Loss: 0.00002030
Iteration 23/1000 | Loss: 0.00002030
Iteration 24/1000 | Loss: 0.00002027
Iteration 25/1000 | Loss: 0.00002025
Iteration 26/1000 | Loss: 0.00002024
Iteration 27/1000 | Loss: 0.00002023
Iteration 28/1000 | Loss: 0.00002023
Iteration 29/1000 | Loss: 0.00002023
Iteration 30/1000 | Loss: 0.00002022
Iteration 31/1000 | Loss: 0.00002022
Iteration 32/1000 | Loss: 0.00002021
Iteration 33/1000 | Loss: 0.00002021
Iteration 34/1000 | Loss: 0.00002021
Iteration 35/1000 | Loss: 0.00002020
Iteration 36/1000 | Loss: 0.00002020
Iteration 37/1000 | Loss: 0.00002019
Iteration 38/1000 | Loss: 0.00002019
Iteration 39/1000 | Loss: 0.00002018
Iteration 40/1000 | Loss: 0.00002018
Iteration 41/1000 | Loss: 0.00002018
Iteration 42/1000 | Loss: 0.00002018
Iteration 43/1000 | Loss: 0.00002017
Iteration 44/1000 | Loss: 0.00002017
Iteration 45/1000 | Loss: 0.00002017
Iteration 46/1000 | Loss: 0.00002016
Iteration 47/1000 | Loss: 0.00002016
Iteration 48/1000 | Loss: 0.00002016
Iteration 49/1000 | Loss: 0.00002016
Iteration 50/1000 | Loss: 0.00002015
Iteration 51/1000 | Loss: 0.00002015
Iteration 52/1000 | Loss: 0.00002015
Iteration 53/1000 | Loss: 0.00002015
Iteration 54/1000 | Loss: 0.00002015
Iteration 55/1000 | Loss: 0.00002015
Iteration 56/1000 | Loss: 0.00002015
Iteration 57/1000 | Loss: 0.00002015
Iteration 58/1000 | Loss: 0.00002015
Iteration 59/1000 | Loss: 0.00002014
Iteration 60/1000 | Loss: 0.00002014
Iteration 61/1000 | Loss: 0.00002014
Iteration 62/1000 | Loss: 0.00002014
Iteration 63/1000 | Loss: 0.00002014
Iteration 64/1000 | Loss: 0.00002014
Iteration 65/1000 | Loss: 0.00002014
Iteration 66/1000 | Loss: 0.00002014
Iteration 67/1000 | Loss: 0.00002014
Iteration 68/1000 | Loss: 0.00002014
Iteration 69/1000 | Loss: 0.00002013
Iteration 70/1000 | Loss: 0.00002013
Iteration 71/1000 | Loss: 0.00002013
Iteration 72/1000 | Loss: 0.00002013
Iteration 73/1000 | Loss: 0.00002013
Iteration 74/1000 | Loss: 0.00002013
Iteration 75/1000 | Loss: 0.00002012
Iteration 76/1000 | Loss: 0.00002012
Iteration 77/1000 | Loss: 0.00002012
Iteration 78/1000 | Loss: 0.00002012
Iteration 79/1000 | Loss: 0.00002012
Iteration 80/1000 | Loss: 0.00002012
Iteration 81/1000 | Loss: 0.00002012
Iteration 82/1000 | Loss: 0.00002012
Iteration 83/1000 | Loss: 0.00002011
Iteration 84/1000 | Loss: 0.00002011
Iteration 85/1000 | Loss: 0.00002011
Iteration 86/1000 | Loss: 0.00002011
Iteration 87/1000 | Loss: 0.00002010
Iteration 88/1000 | Loss: 0.00002010
Iteration 89/1000 | Loss: 0.00002010
Iteration 90/1000 | Loss: 0.00002010
Iteration 91/1000 | Loss: 0.00002010
Iteration 92/1000 | Loss: 0.00002009
Iteration 93/1000 | Loss: 0.00002009
Iteration 94/1000 | Loss: 0.00002009
Iteration 95/1000 | Loss: 0.00002009
Iteration 96/1000 | Loss: 0.00002009
Iteration 97/1000 | Loss: 0.00002008
Iteration 98/1000 | Loss: 0.00002008
Iteration 99/1000 | Loss: 0.00002008
Iteration 100/1000 | Loss: 0.00002008
Iteration 101/1000 | Loss: 0.00002008
Iteration 102/1000 | Loss: 0.00002008
Iteration 103/1000 | Loss: 0.00002008
Iteration 104/1000 | Loss: 0.00002007
Iteration 105/1000 | Loss: 0.00002007
Iteration 106/1000 | Loss: 0.00002007
Iteration 107/1000 | Loss: 0.00002007
Iteration 108/1000 | Loss: 0.00002007
Iteration 109/1000 | Loss: 0.00002007
Iteration 110/1000 | Loss: 0.00002007
Iteration 111/1000 | Loss: 0.00002007
Iteration 112/1000 | Loss: 0.00002007
Iteration 113/1000 | Loss: 0.00002007
Iteration 114/1000 | Loss: 0.00002007
Iteration 115/1000 | Loss: 0.00002007
Iteration 116/1000 | Loss: 0.00002007
Iteration 117/1000 | Loss: 0.00002007
Iteration 118/1000 | Loss: 0.00002007
Iteration 119/1000 | Loss: 0.00002007
Iteration 120/1000 | Loss: 0.00002007
Iteration 121/1000 | Loss: 0.00002007
Iteration 122/1000 | Loss: 0.00002007
Iteration 123/1000 | Loss: 0.00002007
Iteration 124/1000 | Loss: 0.00002007
Iteration 125/1000 | Loss: 0.00002007
Iteration 126/1000 | Loss: 0.00002007
Iteration 127/1000 | Loss: 0.00002007
Iteration 128/1000 | Loss: 0.00002007
Iteration 129/1000 | Loss: 0.00002007
Iteration 130/1000 | Loss: 0.00002007
Iteration 131/1000 | Loss: 0.00002007
Iteration 132/1000 | Loss: 0.00002007
Iteration 133/1000 | Loss: 0.00002007
Iteration 134/1000 | Loss: 0.00002007
Iteration 135/1000 | Loss: 0.00002007
Iteration 136/1000 | Loss: 0.00002007
Iteration 137/1000 | Loss: 0.00002007
Iteration 138/1000 | Loss: 0.00002007
Iteration 139/1000 | Loss: 0.00002007
Iteration 140/1000 | Loss: 0.00002007
Iteration 141/1000 | Loss: 0.00002007
Iteration 142/1000 | Loss: 0.00002007
Iteration 143/1000 | Loss: 0.00002007
Iteration 144/1000 | Loss: 0.00002007
Iteration 145/1000 | Loss: 0.00002007
Iteration 146/1000 | Loss: 0.00002007
Iteration 147/1000 | Loss: 0.00002007
Iteration 148/1000 | Loss: 0.00002007
Iteration 149/1000 | Loss: 0.00002007
Iteration 150/1000 | Loss: 0.00002007
Iteration 151/1000 | Loss: 0.00002007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.0067283912794665e-05, 2.0067283912794665e-05, 2.0067283912794665e-05, 2.0067283912794665e-05, 2.0067283912794665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0067283912794665e-05

Optimization complete. Final v2v error: 3.8418290615081787 mm

Highest mean error: 4.312987804412842 mm for frame 29

Lowest mean error: 3.3382680416107178 mm for frame 58

Saving results

Total time: 41.562565088272095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885442
Iteration 2/25 | Loss: 0.00138257
Iteration 3/25 | Loss: 0.00085143
Iteration 4/25 | Loss: 0.00078640
Iteration 5/25 | Loss: 0.00078132
Iteration 6/25 | Loss: 0.00076913
Iteration 7/25 | Loss: 0.00075606
Iteration 8/25 | Loss: 0.00075390
Iteration 9/25 | Loss: 0.00076045
Iteration 10/25 | Loss: 0.00075571
Iteration 11/25 | Loss: 0.00075270
Iteration 12/25 | Loss: 0.00075135
Iteration 13/25 | Loss: 0.00075127
Iteration 14/25 | Loss: 0.00075126
Iteration 15/25 | Loss: 0.00075126
Iteration 16/25 | Loss: 0.00075126
Iteration 17/25 | Loss: 0.00075126
Iteration 18/25 | Loss: 0.00075126
Iteration 19/25 | Loss: 0.00075126
Iteration 20/25 | Loss: 0.00075126
Iteration 21/25 | Loss: 0.00075126
Iteration 22/25 | Loss: 0.00075126
Iteration 23/25 | Loss: 0.00075126
Iteration 24/25 | Loss: 0.00075126
Iteration 25/25 | Loss: 0.00075126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.43098998
Iteration 2/25 | Loss: 0.00022279
Iteration 3/25 | Loss: 0.00022275
Iteration 4/25 | Loss: 0.00022275
Iteration 5/25 | Loss: 0.00022275
Iteration 6/25 | Loss: 0.00022275
Iteration 7/25 | Loss: 0.00022275
Iteration 8/25 | Loss: 0.00022275
Iteration 9/25 | Loss: 0.00022275
Iteration 10/25 | Loss: 0.00022275
Iteration 11/25 | Loss: 0.00022275
Iteration 12/25 | Loss: 0.00022275
Iteration 13/25 | Loss: 0.00022275
Iteration 14/25 | Loss: 0.00022275
Iteration 15/25 | Loss: 0.00022275
Iteration 16/25 | Loss: 0.00022275
Iteration 17/25 | Loss: 0.00022275
Iteration 18/25 | Loss: 0.00022275
Iteration 19/25 | Loss: 0.00022275
Iteration 20/25 | Loss: 0.00022275
Iteration 21/25 | Loss: 0.00022275
Iteration 22/25 | Loss: 0.00022275
Iteration 23/25 | Loss: 0.00022275
Iteration 24/25 | Loss: 0.00022275
Iteration 25/25 | Loss: 0.00022275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022275
Iteration 2/1000 | Loss: 0.00003962
Iteration 3/1000 | Loss: 0.00006736
Iteration 4/1000 | Loss: 0.00002995
Iteration 5/1000 | Loss: 0.00002847
Iteration 6/1000 | Loss: 0.00002759
Iteration 7/1000 | Loss: 0.00002704
Iteration 8/1000 | Loss: 0.00002668
Iteration 9/1000 | Loss: 0.00002645
Iteration 10/1000 | Loss: 0.00002627
Iteration 11/1000 | Loss: 0.00002613
Iteration 12/1000 | Loss: 0.00002612
Iteration 13/1000 | Loss: 0.00002605
Iteration 14/1000 | Loss: 0.00002604
Iteration 15/1000 | Loss: 0.00002603
Iteration 16/1000 | Loss: 0.00002602
Iteration 17/1000 | Loss: 0.00002602
Iteration 18/1000 | Loss: 0.00002602
Iteration 19/1000 | Loss: 0.00002601
Iteration 20/1000 | Loss: 0.00002601
Iteration 21/1000 | Loss: 0.00002601
Iteration 22/1000 | Loss: 0.00002600
Iteration 23/1000 | Loss: 0.00002600
Iteration 24/1000 | Loss: 0.00002600
Iteration 25/1000 | Loss: 0.00002600
Iteration 26/1000 | Loss: 0.00002599
Iteration 27/1000 | Loss: 0.00002599
Iteration 28/1000 | Loss: 0.00002599
Iteration 29/1000 | Loss: 0.00002599
Iteration 30/1000 | Loss: 0.00002597
Iteration 31/1000 | Loss: 0.00002597
Iteration 32/1000 | Loss: 0.00002597
Iteration 33/1000 | Loss: 0.00002597
Iteration 34/1000 | Loss: 0.00002597
Iteration 35/1000 | Loss: 0.00002597
Iteration 36/1000 | Loss: 0.00002597
Iteration 37/1000 | Loss: 0.00002597
Iteration 38/1000 | Loss: 0.00002597
Iteration 39/1000 | Loss: 0.00002597
Iteration 40/1000 | Loss: 0.00002597
Iteration 41/1000 | Loss: 0.00002597
Iteration 42/1000 | Loss: 0.00002596
Iteration 43/1000 | Loss: 0.00002596
Iteration 44/1000 | Loss: 0.00002596
Iteration 45/1000 | Loss: 0.00002596
Iteration 46/1000 | Loss: 0.00002595
Iteration 47/1000 | Loss: 0.00002594
Iteration 48/1000 | Loss: 0.00002594
Iteration 49/1000 | Loss: 0.00002591
Iteration 50/1000 | Loss: 0.00002590
Iteration 51/1000 | Loss: 0.00002587
Iteration 52/1000 | Loss: 0.00002587
Iteration 53/1000 | Loss: 0.00002587
Iteration 54/1000 | Loss: 0.00002587
Iteration 55/1000 | Loss: 0.00002587
Iteration 56/1000 | Loss: 0.00002587
Iteration 57/1000 | Loss: 0.00002586
Iteration 58/1000 | Loss: 0.00002586
Iteration 59/1000 | Loss: 0.00002586
Iteration 60/1000 | Loss: 0.00002586
Iteration 61/1000 | Loss: 0.00002586
Iteration 62/1000 | Loss: 0.00002586
Iteration 63/1000 | Loss: 0.00002586
Iteration 64/1000 | Loss: 0.00002585
Iteration 65/1000 | Loss: 0.00002585
Iteration 66/1000 | Loss: 0.00002584
Iteration 67/1000 | Loss: 0.00002584
Iteration 68/1000 | Loss: 0.00002583
Iteration 69/1000 | Loss: 0.00002583
Iteration 70/1000 | Loss: 0.00002583
Iteration 71/1000 | Loss: 0.00002583
Iteration 72/1000 | Loss: 0.00002583
Iteration 73/1000 | Loss: 0.00002583
Iteration 74/1000 | Loss: 0.00002583
Iteration 75/1000 | Loss: 0.00002583
Iteration 76/1000 | Loss: 0.00002582
Iteration 77/1000 | Loss: 0.00002582
Iteration 78/1000 | Loss: 0.00002582
Iteration 79/1000 | Loss: 0.00002582
Iteration 80/1000 | Loss: 0.00002581
Iteration 81/1000 | Loss: 0.00002581
Iteration 82/1000 | Loss: 0.00002581
Iteration 83/1000 | Loss: 0.00002581
Iteration 84/1000 | Loss: 0.00002580
Iteration 85/1000 | Loss: 0.00002580
Iteration 86/1000 | Loss: 0.00002580
Iteration 87/1000 | Loss: 0.00002580
Iteration 88/1000 | Loss: 0.00002580
Iteration 89/1000 | Loss: 0.00002580
Iteration 90/1000 | Loss: 0.00002580
Iteration 91/1000 | Loss: 0.00002579
Iteration 92/1000 | Loss: 0.00002579
Iteration 93/1000 | Loss: 0.00002579
Iteration 94/1000 | Loss: 0.00002578
Iteration 95/1000 | Loss: 0.00002578
Iteration 96/1000 | Loss: 0.00002578
Iteration 97/1000 | Loss: 0.00002578
Iteration 98/1000 | Loss: 0.00002578
Iteration 99/1000 | Loss: 0.00002578
Iteration 100/1000 | Loss: 0.00002578
Iteration 101/1000 | Loss: 0.00002578
Iteration 102/1000 | Loss: 0.00002578
Iteration 103/1000 | Loss: 0.00002578
Iteration 104/1000 | Loss: 0.00002578
Iteration 105/1000 | Loss: 0.00002578
Iteration 106/1000 | Loss: 0.00002578
Iteration 107/1000 | Loss: 0.00002578
Iteration 108/1000 | Loss: 0.00002578
Iteration 109/1000 | Loss: 0.00002578
Iteration 110/1000 | Loss: 0.00002578
Iteration 111/1000 | Loss: 0.00002578
Iteration 112/1000 | Loss: 0.00002578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [2.578125349828042e-05, 2.578125349828042e-05, 2.578125349828042e-05, 2.578125349828042e-05, 2.578125349828042e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.578125349828042e-05

Optimization complete. Final v2v error: 4.383369445800781 mm

Highest mean error: 4.866242408752441 mm for frame 127

Lowest mean error: 4.0549139976501465 mm for frame 210

Saving results

Total time: 52.54562544822693
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424607
Iteration 2/25 | Loss: 0.00127721
Iteration 3/25 | Loss: 0.00085413
Iteration 4/25 | Loss: 0.00074496
Iteration 5/25 | Loss: 0.00072996
Iteration 6/25 | Loss: 0.00072649
Iteration 7/25 | Loss: 0.00072570
Iteration 8/25 | Loss: 0.00072549
Iteration 9/25 | Loss: 0.00072549
Iteration 10/25 | Loss: 0.00072549
Iteration 11/25 | Loss: 0.00072549
Iteration 12/25 | Loss: 0.00072549
Iteration 13/25 | Loss: 0.00072549
Iteration 14/25 | Loss: 0.00072549
Iteration 15/25 | Loss: 0.00072549
Iteration 16/25 | Loss: 0.00072549
Iteration 17/25 | Loss: 0.00072549
Iteration 18/25 | Loss: 0.00072549
Iteration 19/25 | Loss: 0.00072549
Iteration 20/25 | Loss: 0.00072549
Iteration 21/25 | Loss: 0.00072549
Iteration 22/25 | Loss: 0.00072548
Iteration 23/25 | Loss: 0.00072548
Iteration 24/25 | Loss: 0.00072548
Iteration 25/25 | Loss: 0.00072548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34000254
Iteration 2/25 | Loss: 0.00026092
Iteration 3/25 | Loss: 0.00026092
Iteration 4/25 | Loss: 0.00026092
Iteration 5/25 | Loss: 0.00026092
Iteration 6/25 | Loss: 0.00026091
Iteration 7/25 | Loss: 0.00026091
Iteration 8/25 | Loss: 0.00026091
Iteration 9/25 | Loss: 0.00026091
Iteration 10/25 | Loss: 0.00026091
Iteration 11/25 | Loss: 0.00026091
Iteration 12/25 | Loss: 0.00026091
Iteration 13/25 | Loss: 0.00026091
Iteration 14/25 | Loss: 0.00026091
Iteration 15/25 | Loss: 0.00026091
Iteration 16/25 | Loss: 0.00026091
Iteration 17/25 | Loss: 0.00026091
Iteration 18/25 | Loss: 0.00026091
Iteration 19/25 | Loss: 0.00026091
Iteration 20/25 | Loss: 0.00026091
Iteration 21/25 | Loss: 0.00026091
Iteration 22/25 | Loss: 0.00026091
Iteration 23/25 | Loss: 0.00026091
Iteration 24/25 | Loss: 0.00026091
Iteration 25/25 | Loss: 0.00026091

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026091
Iteration 2/1000 | Loss: 0.00006002
Iteration 3/1000 | Loss: 0.00003242
Iteration 4/1000 | Loss: 0.00002806
Iteration 5/1000 | Loss: 0.00002594
Iteration 6/1000 | Loss: 0.00002336
Iteration 7/1000 | Loss: 0.00002257
Iteration 8/1000 | Loss: 0.00002185
Iteration 9/1000 | Loss: 0.00002121
Iteration 10/1000 | Loss: 0.00002087
Iteration 11/1000 | Loss: 0.00002058
Iteration 12/1000 | Loss: 0.00002030
Iteration 13/1000 | Loss: 0.00002014
Iteration 14/1000 | Loss: 0.00002007
Iteration 15/1000 | Loss: 0.00001999
Iteration 16/1000 | Loss: 0.00001992
Iteration 17/1000 | Loss: 0.00001992
Iteration 18/1000 | Loss: 0.00001990
Iteration 19/1000 | Loss: 0.00001990
Iteration 20/1000 | Loss: 0.00001989
Iteration 21/1000 | Loss: 0.00001989
Iteration 22/1000 | Loss: 0.00001989
Iteration 23/1000 | Loss: 0.00001989
Iteration 24/1000 | Loss: 0.00001988
Iteration 25/1000 | Loss: 0.00001988
Iteration 26/1000 | Loss: 0.00001987
Iteration 27/1000 | Loss: 0.00001987
Iteration 28/1000 | Loss: 0.00001987
Iteration 29/1000 | Loss: 0.00001986
Iteration 30/1000 | Loss: 0.00001985
Iteration 31/1000 | Loss: 0.00001985
Iteration 32/1000 | Loss: 0.00001985
Iteration 33/1000 | Loss: 0.00001985
Iteration 34/1000 | Loss: 0.00001985
Iteration 35/1000 | Loss: 0.00001985
Iteration 36/1000 | Loss: 0.00001985
Iteration 37/1000 | Loss: 0.00001985
Iteration 38/1000 | Loss: 0.00001984
Iteration 39/1000 | Loss: 0.00001984
Iteration 40/1000 | Loss: 0.00001984
Iteration 41/1000 | Loss: 0.00001983
Iteration 42/1000 | Loss: 0.00001982
Iteration 43/1000 | Loss: 0.00001982
Iteration 44/1000 | Loss: 0.00001981
Iteration 45/1000 | Loss: 0.00001981
Iteration 46/1000 | Loss: 0.00001980
Iteration 47/1000 | Loss: 0.00001980
Iteration 48/1000 | Loss: 0.00001979
Iteration 49/1000 | Loss: 0.00001976
Iteration 50/1000 | Loss: 0.00001976
Iteration 51/1000 | Loss: 0.00001976
Iteration 52/1000 | Loss: 0.00001975
Iteration 53/1000 | Loss: 0.00001973
Iteration 54/1000 | Loss: 0.00001973
Iteration 55/1000 | Loss: 0.00001973
Iteration 56/1000 | Loss: 0.00001973
Iteration 57/1000 | Loss: 0.00001973
Iteration 58/1000 | Loss: 0.00001973
Iteration 59/1000 | Loss: 0.00001973
Iteration 60/1000 | Loss: 0.00001972
Iteration 61/1000 | Loss: 0.00001972
Iteration 62/1000 | Loss: 0.00001972
Iteration 63/1000 | Loss: 0.00001972
Iteration 64/1000 | Loss: 0.00001972
Iteration 65/1000 | Loss: 0.00001972
Iteration 66/1000 | Loss: 0.00001972
Iteration 67/1000 | Loss: 0.00001972
Iteration 68/1000 | Loss: 0.00001972
Iteration 69/1000 | Loss: 0.00001972
Iteration 70/1000 | Loss: 0.00001972
Iteration 71/1000 | Loss: 0.00001971
Iteration 72/1000 | Loss: 0.00001970
Iteration 73/1000 | Loss: 0.00001970
Iteration 74/1000 | Loss: 0.00001970
Iteration 75/1000 | Loss: 0.00001969
Iteration 76/1000 | Loss: 0.00001969
Iteration 77/1000 | Loss: 0.00001969
Iteration 78/1000 | Loss: 0.00001968
Iteration 79/1000 | Loss: 0.00001968
Iteration 80/1000 | Loss: 0.00001968
Iteration 81/1000 | Loss: 0.00001968
Iteration 82/1000 | Loss: 0.00001967
Iteration 83/1000 | Loss: 0.00001967
Iteration 84/1000 | Loss: 0.00001966
Iteration 85/1000 | Loss: 0.00001966
Iteration 86/1000 | Loss: 0.00001966
Iteration 87/1000 | Loss: 0.00001966
Iteration 88/1000 | Loss: 0.00001966
Iteration 89/1000 | Loss: 0.00001966
Iteration 90/1000 | Loss: 0.00001966
Iteration 91/1000 | Loss: 0.00001966
Iteration 92/1000 | Loss: 0.00001966
Iteration 93/1000 | Loss: 0.00001965
Iteration 94/1000 | Loss: 0.00001965
Iteration 95/1000 | Loss: 0.00001965
Iteration 96/1000 | Loss: 0.00001965
Iteration 97/1000 | Loss: 0.00001965
Iteration 98/1000 | Loss: 0.00001964
Iteration 99/1000 | Loss: 0.00001964
Iteration 100/1000 | Loss: 0.00001964
Iteration 101/1000 | Loss: 0.00001964
Iteration 102/1000 | Loss: 0.00001964
Iteration 103/1000 | Loss: 0.00001964
Iteration 104/1000 | Loss: 0.00001964
Iteration 105/1000 | Loss: 0.00001964
Iteration 106/1000 | Loss: 0.00001963
Iteration 107/1000 | Loss: 0.00001963
Iteration 108/1000 | Loss: 0.00001963
Iteration 109/1000 | Loss: 0.00001963
Iteration 110/1000 | Loss: 0.00001963
Iteration 111/1000 | Loss: 0.00001963
Iteration 112/1000 | Loss: 0.00001963
Iteration 113/1000 | Loss: 0.00001963
Iteration 114/1000 | Loss: 0.00001963
Iteration 115/1000 | Loss: 0.00001963
Iteration 116/1000 | Loss: 0.00001963
Iteration 117/1000 | Loss: 0.00001963
Iteration 118/1000 | Loss: 0.00001963
Iteration 119/1000 | Loss: 0.00001963
Iteration 120/1000 | Loss: 0.00001963
Iteration 121/1000 | Loss: 0.00001962
Iteration 122/1000 | Loss: 0.00001962
Iteration 123/1000 | Loss: 0.00001962
Iteration 124/1000 | Loss: 0.00001962
Iteration 125/1000 | Loss: 0.00001962
Iteration 126/1000 | Loss: 0.00001962
Iteration 127/1000 | Loss: 0.00001962
Iteration 128/1000 | Loss: 0.00001962
Iteration 129/1000 | Loss: 0.00001962
Iteration 130/1000 | Loss: 0.00001962
Iteration 131/1000 | Loss: 0.00001962
Iteration 132/1000 | Loss: 0.00001962
Iteration 133/1000 | Loss: 0.00001961
Iteration 134/1000 | Loss: 0.00001961
Iteration 135/1000 | Loss: 0.00001961
Iteration 136/1000 | Loss: 0.00001961
Iteration 137/1000 | Loss: 0.00001961
Iteration 138/1000 | Loss: 0.00001961
Iteration 139/1000 | Loss: 0.00001961
Iteration 140/1000 | Loss: 0.00001961
Iteration 141/1000 | Loss: 0.00001961
Iteration 142/1000 | Loss: 0.00001961
Iteration 143/1000 | Loss: 0.00001961
Iteration 144/1000 | Loss: 0.00001961
Iteration 145/1000 | Loss: 0.00001960
Iteration 146/1000 | Loss: 0.00001960
Iteration 147/1000 | Loss: 0.00001960
Iteration 148/1000 | Loss: 0.00001960
Iteration 149/1000 | Loss: 0.00001960
Iteration 150/1000 | Loss: 0.00001960
Iteration 151/1000 | Loss: 0.00001960
Iteration 152/1000 | Loss: 0.00001959
Iteration 153/1000 | Loss: 0.00001959
Iteration 154/1000 | Loss: 0.00001959
Iteration 155/1000 | Loss: 0.00001959
Iteration 156/1000 | Loss: 0.00001959
Iteration 157/1000 | Loss: 0.00001959
Iteration 158/1000 | Loss: 0.00001959
Iteration 159/1000 | Loss: 0.00001959
Iteration 160/1000 | Loss: 0.00001959
Iteration 161/1000 | Loss: 0.00001959
Iteration 162/1000 | Loss: 0.00001958
Iteration 163/1000 | Loss: 0.00001958
Iteration 164/1000 | Loss: 0.00001958
Iteration 165/1000 | Loss: 0.00001958
Iteration 166/1000 | Loss: 0.00001958
Iteration 167/1000 | Loss: 0.00001958
Iteration 168/1000 | Loss: 0.00001958
Iteration 169/1000 | Loss: 0.00001958
Iteration 170/1000 | Loss: 0.00001958
Iteration 171/1000 | Loss: 0.00001958
Iteration 172/1000 | Loss: 0.00001958
Iteration 173/1000 | Loss: 0.00001958
Iteration 174/1000 | Loss: 0.00001958
Iteration 175/1000 | Loss: 0.00001958
Iteration 176/1000 | Loss: 0.00001958
Iteration 177/1000 | Loss: 0.00001958
Iteration 178/1000 | Loss: 0.00001958
Iteration 179/1000 | Loss: 0.00001958
Iteration 180/1000 | Loss: 0.00001957
Iteration 181/1000 | Loss: 0.00001957
Iteration 182/1000 | Loss: 0.00001957
Iteration 183/1000 | Loss: 0.00001957
Iteration 184/1000 | Loss: 0.00001957
Iteration 185/1000 | Loss: 0.00001957
Iteration 186/1000 | Loss: 0.00001957
Iteration 187/1000 | Loss: 0.00001957
Iteration 188/1000 | Loss: 0.00001957
Iteration 189/1000 | Loss: 0.00001957
Iteration 190/1000 | Loss: 0.00001957
Iteration 191/1000 | Loss: 0.00001957
Iteration 192/1000 | Loss: 0.00001957
Iteration 193/1000 | Loss: 0.00001957
Iteration 194/1000 | Loss: 0.00001957
Iteration 195/1000 | Loss: 0.00001957
Iteration 196/1000 | Loss: 0.00001957
Iteration 197/1000 | Loss: 0.00001957
Iteration 198/1000 | Loss: 0.00001956
Iteration 199/1000 | Loss: 0.00001956
Iteration 200/1000 | Loss: 0.00001956
Iteration 201/1000 | Loss: 0.00001956
Iteration 202/1000 | Loss: 0.00001956
Iteration 203/1000 | Loss: 0.00001956
Iteration 204/1000 | Loss: 0.00001956
Iteration 205/1000 | Loss: 0.00001955
Iteration 206/1000 | Loss: 0.00001955
Iteration 207/1000 | Loss: 0.00001955
Iteration 208/1000 | Loss: 0.00001955
Iteration 209/1000 | Loss: 0.00001955
Iteration 210/1000 | Loss: 0.00001955
Iteration 211/1000 | Loss: 0.00001955
Iteration 212/1000 | Loss: 0.00001955
Iteration 213/1000 | Loss: 0.00001955
Iteration 214/1000 | Loss: 0.00001955
Iteration 215/1000 | Loss: 0.00001955
Iteration 216/1000 | Loss: 0.00001955
Iteration 217/1000 | Loss: 0.00001955
Iteration 218/1000 | Loss: 0.00001955
Iteration 219/1000 | Loss: 0.00001955
Iteration 220/1000 | Loss: 0.00001955
Iteration 221/1000 | Loss: 0.00001955
Iteration 222/1000 | Loss: 0.00001955
Iteration 223/1000 | Loss: 0.00001955
Iteration 224/1000 | Loss: 0.00001955
Iteration 225/1000 | Loss: 0.00001954
Iteration 226/1000 | Loss: 0.00001954
Iteration 227/1000 | Loss: 0.00001954
Iteration 228/1000 | Loss: 0.00001954
Iteration 229/1000 | Loss: 0.00001954
Iteration 230/1000 | Loss: 0.00001954
Iteration 231/1000 | Loss: 0.00001954
Iteration 232/1000 | Loss: 0.00001954
Iteration 233/1000 | Loss: 0.00001954
Iteration 234/1000 | Loss: 0.00001954
Iteration 235/1000 | Loss: 0.00001954
Iteration 236/1000 | Loss: 0.00001954
Iteration 237/1000 | Loss: 0.00001954
Iteration 238/1000 | Loss: 0.00001954
Iteration 239/1000 | Loss: 0.00001954
Iteration 240/1000 | Loss: 0.00001954
Iteration 241/1000 | Loss: 0.00001954
Iteration 242/1000 | Loss: 0.00001954
Iteration 243/1000 | Loss: 0.00001954
Iteration 244/1000 | Loss: 0.00001954
Iteration 245/1000 | Loss: 0.00001954
Iteration 246/1000 | Loss: 0.00001954
Iteration 247/1000 | Loss: 0.00001954
Iteration 248/1000 | Loss: 0.00001954
Iteration 249/1000 | Loss: 0.00001954
Iteration 250/1000 | Loss: 0.00001954
Iteration 251/1000 | Loss: 0.00001954
Iteration 252/1000 | Loss: 0.00001954
Iteration 253/1000 | Loss: 0.00001954
Iteration 254/1000 | Loss: 0.00001954
Iteration 255/1000 | Loss: 0.00001954
Iteration 256/1000 | Loss: 0.00001954
Iteration 257/1000 | Loss: 0.00001954
Iteration 258/1000 | Loss: 0.00001954
Iteration 259/1000 | Loss: 0.00001954
Iteration 260/1000 | Loss: 0.00001954
Iteration 261/1000 | Loss: 0.00001954
Iteration 262/1000 | Loss: 0.00001954
Iteration 263/1000 | Loss: 0.00001954
Iteration 264/1000 | Loss: 0.00001954
Iteration 265/1000 | Loss: 0.00001954
Iteration 266/1000 | Loss: 0.00001954
Iteration 267/1000 | Loss: 0.00001954
Iteration 268/1000 | Loss: 0.00001954
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [1.9544695533113554e-05, 1.9544695533113554e-05, 1.9544695533113554e-05, 1.9544695533113554e-05, 1.9544695533113554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9544695533113554e-05

Optimization complete. Final v2v error: 3.845996856689453 mm

Highest mean error: 4.316485404968262 mm for frame 31

Lowest mean error: 3.4815356731414795 mm for frame 81

Saving results

Total time: 47.01994013786316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412549
Iteration 2/25 | Loss: 0.00097903
Iteration 3/25 | Loss: 0.00082999
Iteration 4/25 | Loss: 0.00079622
Iteration 5/25 | Loss: 0.00078593
Iteration 6/25 | Loss: 0.00078325
Iteration 7/25 | Loss: 0.00078270
Iteration 8/25 | Loss: 0.00078253
Iteration 9/25 | Loss: 0.00078253
Iteration 10/25 | Loss: 0.00078253
Iteration 11/25 | Loss: 0.00078253
Iteration 12/25 | Loss: 0.00078253
Iteration 13/25 | Loss: 0.00078253
Iteration 14/25 | Loss: 0.00078253
Iteration 15/25 | Loss: 0.00078253
Iteration 16/25 | Loss: 0.00078253
Iteration 17/25 | Loss: 0.00078253
Iteration 18/25 | Loss: 0.00078253
Iteration 19/25 | Loss: 0.00078253
Iteration 20/25 | Loss: 0.00078253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007825311040505767, 0.0007825311040505767, 0.0007825311040505767, 0.0007825311040505767, 0.0007825311040505767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007825311040505767

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40035486
Iteration 2/25 | Loss: 0.00030001
Iteration 3/25 | Loss: 0.00030000
Iteration 4/25 | Loss: 0.00030000
Iteration 5/25 | Loss: 0.00030000
Iteration 6/25 | Loss: 0.00030000
Iteration 7/25 | Loss: 0.00030000
Iteration 8/25 | Loss: 0.00030000
Iteration 9/25 | Loss: 0.00030000
Iteration 10/25 | Loss: 0.00030000
Iteration 11/25 | Loss: 0.00030000
Iteration 12/25 | Loss: 0.00030000
Iteration 13/25 | Loss: 0.00030000
Iteration 14/25 | Loss: 0.00030000
Iteration 15/25 | Loss: 0.00030000
Iteration 16/25 | Loss: 0.00030000
Iteration 17/25 | Loss: 0.00030000
Iteration 18/25 | Loss: 0.00030000
Iteration 19/25 | Loss: 0.00030000
Iteration 20/25 | Loss: 0.00030000
Iteration 21/25 | Loss: 0.00030000
Iteration 22/25 | Loss: 0.00030000
Iteration 23/25 | Loss: 0.00030000
Iteration 24/25 | Loss: 0.00030000
Iteration 25/25 | Loss: 0.00030000

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030000
Iteration 2/1000 | Loss: 0.00007288
Iteration 3/1000 | Loss: 0.00004629
Iteration 4/1000 | Loss: 0.00003887
Iteration 5/1000 | Loss: 0.00003686
Iteration 6/1000 | Loss: 0.00003446
Iteration 7/1000 | Loss: 0.00003305
Iteration 8/1000 | Loss: 0.00003210
Iteration 9/1000 | Loss: 0.00003123
Iteration 10/1000 | Loss: 0.00003066
Iteration 11/1000 | Loss: 0.00003045
Iteration 12/1000 | Loss: 0.00003035
Iteration 13/1000 | Loss: 0.00003022
Iteration 14/1000 | Loss: 0.00003021
Iteration 15/1000 | Loss: 0.00003013
Iteration 16/1000 | Loss: 0.00003010
Iteration 17/1000 | Loss: 0.00003010
Iteration 18/1000 | Loss: 0.00003009
Iteration 19/1000 | Loss: 0.00003009
Iteration 20/1000 | Loss: 0.00003009
Iteration 21/1000 | Loss: 0.00003008
Iteration 22/1000 | Loss: 0.00003008
Iteration 23/1000 | Loss: 0.00003007
Iteration 24/1000 | Loss: 0.00003006
Iteration 25/1000 | Loss: 0.00003006
Iteration 26/1000 | Loss: 0.00003006
Iteration 27/1000 | Loss: 0.00003005
Iteration 28/1000 | Loss: 0.00003005
Iteration 29/1000 | Loss: 0.00003005
Iteration 30/1000 | Loss: 0.00003004
Iteration 31/1000 | Loss: 0.00003004
Iteration 32/1000 | Loss: 0.00003004
Iteration 33/1000 | Loss: 0.00003003
Iteration 34/1000 | Loss: 0.00003003
Iteration 35/1000 | Loss: 0.00003003
Iteration 36/1000 | Loss: 0.00003002
Iteration 37/1000 | Loss: 0.00003002
Iteration 38/1000 | Loss: 0.00003002
Iteration 39/1000 | Loss: 0.00003002
Iteration 40/1000 | Loss: 0.00003002
Iteration 41/1000 | Loss: 0.00003002
Iteration 42/1000 | Loss: 0.00003002
Iteration 43/1000 | Loss: 0.00003002
Iteration 44/1000 | Loss: 0.00003002
Iteration 45/1000 | Loss: 0.00003001
Iteration 46/1000 | Loss: 0.00003001
Iteration 47/1000 | Loss: 0.00003000
Iteration 48/1000 | Loss: 0.00003000
Iteration 49/1000 | Loss: 0.00002999
Iteration 50/1000 | Loss: 0.00002999
Iteration 51/1000 | Loss: 0.00002999
Iteration 52/1000 | Loss: 0.00002999
Iteration 53/1000 | Loss: 0.00002998
Iteration 54/1000 | Loss: 0.00002998
Iteration 55/1000 | Loss: 0.00002998
Iteration 56/1000 | Loss: 0.00002998
Iteration 57/1000 | Loss: 0.00002998
Iteration 58/1000 | Loss: 0.00002997
Iteration 59/1000 | Loss: 0.00002997
Iteration 60/1000 | Loss: 0.00002997
Iteration 61/1000 | Loss: 0.00002996
Iteration 62/1000 | Loss: 0.00002996
Iteration 63/1000 | Loss: 0.00002995
Iteration 64/1000 | Loss: 0.00002995
Iteration 65/1000 | Loss: 0.00002995
Iteration 66/1000 | Loss: 0.00002995
Iteration 67/1000 | Loss: 0.00002994
Iteration 68/1000 | Loss: 0.00002994
Iteration 69/1000 | Loss: 0.00002994
Iteration 70/1000 | Loss: 0.00002993
Iteration 71/1000 | Loss: 0.00002993
Iteration 72/1000 | Loss: 0.00002993
Iteration 73/1000 | Loss: 0.00002993
Iteration 74/1000 | Loss: 0.00002993
Iteration 75/1000 | Loss: 0.00002993
Iteration 76/1000 | Loss: 0.00002993
Iteration 77/1000 | Loss: 0.00002993
Iteration 78/1000 | Loss: 0.00002993
Iteration 79/1000 | Loss: 0.00002993
Iteration 80/1000 | Loss: 0.00002993
Iteration 81/1000 | Loss: 0.00002993
Iteration 82/1000 | Loss: 0.00002993
Iteration 83/1000 | Loss: 0.00002992
Iteration 84/1000 | Loss: 0.00002992
Iteration 85/1000 | Loss: 0.00002992
Iteration 86/1000 | Loss: 0.00002992
Iteration 87/1000 | Loss: 0.00002992
Iteration 88/1000 | Loss: 0.00002992
Iteration 89/1000 | Loss: 0.00002992
Iteration 90/1000 | Loss: 0.00002992
Iteration 91/1000 | Loss: 0.00002992
Iteration 92/1000 | Loss: 0.00002992
Iteration 93/1000 | Loss: 0.00002992
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [2.9916660423623398e-05, 2.9916660423623398e-05, 2.9916660423623398e-05, 2.9916660423623398e-05, 2.9916660423623398e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9916660423623398e-05

Optimization complete. Final v2v error: 4.680389881134033 mm

Highest mean error: 5.505346298217773 mm for frame 42

Lowest mean error: 4.205591201782227 mm for frame 95

Saving results

Total time: 35.7538583278656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430236
Iteration 2/25 | Loss: 0.00094305
Iteration 3/25 | Loss: 0.00082351
Iteration 4/25 | Loss: 0.00080675
Iteration 5/25 | Loss: 0.00079997
Iteration 6/25 | Loss: 0.00079972
Iteration 7/25 | Loss: 0.00079972
Iteration 8/25 | Loss: 0.00079972
Iteration 9/25 | Loss: 0.00079972
Iteration 10/25 | Loss: 0.00079972
Iteration 11/25 | Loss: 0.00079972
Iteration 12/25 | Loss: 0.00079972
Iteration 13/25 | Loss: 0.00079972
Iteration 14/25 | Loss: 0.00079972
Iteration 15/25 | Loss: 0.00079972
Iteration 16/25 | Loss: 0.00079972
Iteration 17/25 | Loss: 0.00079972
Iteration 18/25 | Loss: 0.00079972
Iteration 19/25 | Loss: 0.00079972
Iteration 20/25 | Loss: 0.00079972
Iteration 21/25 | Loss: 0.00079972
Iteration 22/25 | Loss: 0.00079972
Iteration 23/25 | Loss: 0.00079972
Iteration 24/25 | Loss: 0.00079972
Iteration 25/25 | Loss: 0.00079972

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13395798
Iteration 2/25 | Loss: 0.00039741
Iteration 3/25 | Loss: 0.00039739
Iteration 4/25 | Loss: 0.00039739
Iteration 5/25 | Loss: 0.00039739
Iteration 6/25 | Loss: 0.00039739
Iteration 7/25 | Loss: 0.00039739
Iteration 8/25 | Loss: 0.00039739
Iteration 9/25 | Loss: 0.00039739
Iteration 10/25 | Loss: 0.00039739
Iteration 11/25 | Loss: 0.00039739
Iteration 12/25 | Loss: 0.00039739
Iteration 13/25 | Loss: 0.00039739
Iteration 14/25 | Loss: 0.00039739
Iteration 15/25 | Loss: 0.00039739
Iteration 16/25 | Loss: 0.00039739
Iteration 17/25 | Loss: 0.00039739
Iteration 18/25 | Loss: 0.00039739
Iteration 19/25 | Loss: 0.00039739
Iteration 20/25 | Loss: 0.00039739
Iteration 21/25 | Loss: 0.00039739
Iteration 22/25 | Loss: 0.00039739
Iteration 23/25 | Loss: 0.00039739
Iteration 24/25 | Loss: 0.00039739
Iteration 25/25 | Loss: 0.00039739
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0003973900165874511, 0.0003973900165874511, 0.0003973900165874511, 0.0003973900165874511, 0.0003973900165874511]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003973900165874511

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039739
Iteration 2/1000 | Loss: 0.00007463
Iteration 3/1000 | Loss: 0.00003797
Iteration 4/1000 | Loss: 0.00003279
Iteration 5/1000 | Loss: 0.00002994
Iteration 6/1000 | Loss: 0.00002802
Iteration 7/1000 | Loss: 0.00002730
Iteration 8/1000 | Loss: 0.00002670
Iteration 9/1000 | Loss: 0.00002626
Iteration 10/1000 | Loss: 0.00002594
Iteration 11/1000 | Loss: 0.00002561
Iteration 12/1000 | Loss: 0.00002546
Iteration 13/1000 | Loss: 0.00002542
Iteration 14/1000 | Loss: 0.00002541
Iteration 15/1000 | Loss: 0.00002541
Iteration 16/1000 | Loss: 0.00002541
Iteration 17/1000 | Loss: 0.00002540
Iteration 18/1000 | Loss: 0.00002540
Iteration 19/1000 | Loss: 0.00002540
Iteration 20/1000 | Loss: 0.00002540
Iteration 21/1000 | Loss: 0.00002537
Iteration 22/1000 | Loss: 0.00002537
Iteration 23/1000 | Loss: 0.00002537
Iteration 24/1000 | Loss: 0.00002537
Iteration 25/1000 | Loss: 0.00002537
Iteration 26/1000 | Loss: 0.00002537
Iteration 27/1000 | Loss: 0.00002537
Iteration 28/1000 | Loss: 0.00002536
Iteration 29/1000 | Loss: 0.00002536
Iteration 30/1000 | Loss: 0.00002531
Iteration 31/1000 | Loss: 0.00002531
Iteration 32/1000 | Loss: 0.00002531
Iteration 33/1000 | Loss: 0.00002531
Iteration 34/1000 | Loss: 0.00002530
Iteration 35/1000 | Loss: 0.00002530
Iteration 36/1000 | Loss: 0.00002529
Iteration 37/1000 | Loss: 0.00002529
Iteration 38/1000 | Loss: 0.00002529
Iteration 39/1000 | Loss: 0.00002529
Iteration 40/1000 | Loss: 0.00002529
Iteration 41/1000 | Loss: 0.00002529
Iteration 42/1000 | Loss: 0.00002529
Iteration 43/1000 | Loss: 0.00002528
Iteration 44/1000 | Loss: 0.00002527
Iteration 45/1000 | Loss: 0.00002526
Iteration 46/1000 | Loss: 0.00002526
Iteration 47/1000 | Loss: 0.00002526
Iteration 48/1000 | Loss: 0.00002526
Iteration 49/1000 | Loss: 0.00002526
Iteration 50/1000 | Loss: 0.00002526
Iteration 51/1000 | Loss: 0.00002526
Iteration 52/1000 | Loss: 0.00002526
Iteration 53/1000 | Loss: 0.00002526
Iteration 54/1000 | Loss: 0.00002526
Iteration 55/1000 | Loss: 0.00002526
Iteration 56/1000 | Loss: 0.00002526
Iteration 57/1000 | Loss: 0.00002525
Iteration 58/1000 | Loss: 0.00002525
Iteration 59/1000 | Loss: 0.00002525
Iteration 60/1000 | Loss: 0.00002524
Iteration 61/1000 | Loss: 0.00002524
Iteration 62/1000 | Loss: 0.00002524
Iteration 63/1000 | Loss: 0.00002524
Iteration 64/1000 | Loss: 0.00002524
Iteration 65/1000 | Loss: 0.00002523
Iteration 66/1000 | Loss: 0.00002523
Iteration 67/1000 | Loss: 0.00002523
Iteration 68/1000 | Loss: 0.00002523
Iteration 69/1000 | Loss: 0.00002523
Iteration 70/1000 | Loss: 0.00002523
Iteration 71/1000 | Loss: 0.00002523
Iteration 72/1000 | Loss: 0.00002522
Iteration 73/1000 | Loss: 0.00002522
Iteration 74/1000 | Loss: 0.00002522
Iteration 75/1000 | Loss: 0.00002522
Iteration 76/1000 | Loss: 0.00002522
Iteration 77/1000 | Loss: 0.00002522
Iteration 78/1000 | Loss: 0.00002522
Iteration 79/1000 | Loss: 0.00002522
Iteration 80/1000 | Loss: 0.00002521
Iteration 81/1000 | Loss: 0.00002521
Iteration 82/1000 | Loss: 0.00002521
Iteration 83/1000 | Loss: 0.00002521
Iteration 84/1000 | Loss: 0.00002521
Iteration 85/1000 | Loss: 0.00002521
Iteration 86/1000 | Loss: 0.00002521
Iteration 87/1000 | Loss: 0.00002521
Iteration 88/1000 | Loss: 0.00002521
Iteration 89/1000 | Loss: 0.00002521
Iteration 90/1000 | Loss: 0.00002521
Iteration 91/1000 | Loss: 0.00002521
Iteration 92/1000 | Loss: 0.00002521
Iteration 93/1000 | Loss: 0.00002521
Iteration 94/1000 | Loss: 0.00002521
Iteration 95/1000 | Loss: 0.00002520
Iteration 96/1000 | Loss: 0.00002520
Iteration 97/1000 | Loss: 0.00002520
Iteration 98/1000 | Loss: 0.00002520
Iteration 99/1000 | Loss: 0.00002519
Iteration 100/1000 | Loss: 0.00002519
Iteration 101/1000 | Loss: 0.00002519
Iteration 102/1000 | Loss: 0.00002519
Iteration 103/1000 | Loss: 0.00002519
Iteration 104/1000 | Loss: 0.00002519
Iteration 105/1000 | Loss: 0.00002519
Iteration 106/1000 | Loss: 0.00002519
Iteration 107/1000 | Loss: 0.00002519
Iteration 108/1000 | Loss: 0.00002519
Iteration 109/1000 | Loss: 0.00002519
Iteration 110/1000 | Loss: 0.00002519
Iteration 111/1000 | Loss: 0.00002519
Iteration 112/1000 | Loss: 0.00002519
Iteration 113/1000 | Loss: 0.00002519
Iteration 114/1000 | Loss: 0.00002519
Iteration 115/1000 | Loss: 0.00002519
Iteration 116/1000 | Loss: 0.00002519
Iteration 117/1000 | Loss: 0.00002519
Iteration 118/1000 | Loss: 0.00002519
Iteration 119/1000 | Loss: 0.00002519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.519169356673956e-05, 2.519169356673956e-05, 2.519169356673956e-05, 2.519169356673956e-05, 2.519169356673956e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.519169356673956e-05

Optimization complete. Final v2v error: 4.316826820373535 mm

Highest mean error: 4.411945819854736 mm for frame 114

Lowest mean error: 4.104624271392822 mm for frame 69

Saving results

Total time: 32.19040846824646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_0007/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_0007/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962792
Iteration 2/25 | Loss: 0.00157103
Iteration 3/25 | Loss: 0.00105065
Iteration 4/25 | Loss: 0.00097899
Iteration 5/25 | Loss: 0.00096281
Iteration 6/25 | Loss: 0.00096164
Iteration 7/25 | Loss: 0.00096164
Iteration 8/25 | Loss: 0.00096164
Iteration 9/25 | Loss: 0.00096164
Iteration 10/25 | Loss: 0.00096164
Iteration 11/25 | Loss: 0.00096164
Iteration 12/25 | Loss: 0.00096164
Iteration 13/25 | Loss: 0.00096164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009616407915018499, 0.0009616407915018499, 0.0009616407915018499, 0.0009616407915018499, 0.0009616407915018499]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009616407915018499

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17103004
Iteration 2/25 | Loss: 0.00027751
Iteration 3/25 | Loss: 0.00027750
Iteration 4/25 | Loss: 0.00027750
Iteration 5/25 | Loss: 0.00027750
Iteration 6/25 | Loss: 0.00027750
Iteration 7/25 | Loss: 0.00027750
Iteration 8/25 | Loss: 0.00027750
Iteration 9/25 | Loss: 0.00027750
Iteration 10/25 | Loss: 0.00027750
Iteration 11/25 | Loss: 0.00027750
Iteration 12/25 | Loss: 0.00027750
Iteration 13/25 | Loss: 0.00027750
Iteration 14/25 | Loss: 0.00027750
Iteration 15/25 | Loss: 0.00027750
Iteration 16/25 | Loss: 0.00027750
Iteration 17/25 | Loss: 0.00027750
Iteration 18/25 | Loss: 0.00027750
Iteration 19/25 | Loss: 0.00027750
Iteration 20/25 | Loss: 0.00027750
Iteration 21/25 | Loss: 0.00027750
Iteration 22/25 | Loss: 0.00027750
Iteration 23/25 | Loss: 0.00027750
Iteration 24/25 | Loss: 0.00027750
Iteration 25/25 | Loss: 0.00027750

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027750
Iteration 2/1000 | Loss: 0.00010028
Iteration 3/1000 | Loss: 0.00007026
Iteration 4/1000 | Loss: 0.00006523
Iteration 5/1000 | Loss: 0.00006161
Iteration 6/1000 | Loss: 0.00005923
Iteration 7/1000 | Loss: 0.00005740
Iteration 8/1000 | Loss: 0.00005605
Iteration 9/1000 | Loss: 0.00005495
Iteration 10/1000 | Loss: 0.00005429
Iteration 11/1000 | Loss: 0.00005390
Iteration 12/1000 | Loss: 0.00005359
Iteration 13/1000 | Loss: 0.00005333
Iteration 14/1000 | Loss: 0.00005316
Iteration 15/1000 | Loss: 0.00005304
Iteration 16/1000 | Loss: 0.00005291
Iteration 17/1000 | Loss: 0.00005291
Iteration 18/1000 | Loss: 0.00005283
Iteration 19/1000 | Loss: 0.00005283
Iteration 20/1000 | Loss: 0.00005278
Iteration 21/1000 | Loss: 0.00005278
Iteration 22/1000 | Loss: 0.00005277
Iteration 23/1000 | Loss: 0.00005277
Iteration 24/1000 | Loss: 0.00005275
Iteration 25/1000 | Loss: 0.00005275
Iteration 26/1000 | Loss: 0.00005275
Iteration 27/1000 | Loss: 0.00005275
Iteration 28/1000 | Loss: 0.00005275
Iteration 29/1000 | Loss: 0.00005275
Iteration 30/1000 | Loss: 0.00005274
Iteration 31/1000 | Loss: 0.00005274
Iteration 32/1000 | Loss: 0.00005274
Iteration 33/1000 | Loss: 0.00005274
Iteration 34/1000 | Loss: 0.00005274
Iteration 35/1000 | Loss: 0.00005274
Iteration 36/1000 | Loss: 0.00005272
Iteration 37/1000 | Loss: 0.00005272
Iteration 38/1000 | Loss: 0.00005272
Iteration 39/1000 | Loss: 0.00005272
Iteration 40/1000 | Loss: 0.00005272
Iteration 41/1000 | Loss: 0.00005272
Iteration 42/1000 | Loss: 0.00005272
Iteration 43/1000 | Loss: 0.00005272
Iteration 44/1000 | Loss: 0.00005271
Iteration 45/1000 | Loss: 0.00005270
Iteration 46/1000 | Loss: 0.00005270
Iteration 47/1000 | Loss: 0.00005270
Iteration 48/1000 | Loss: 0.00005270
Iteration 49/1000 | Loss: 0.00005270
Iteration 50/1000 | Loss: 0.00005270
Iteration 51/1000 | Loss: 0.00005270
Iteration 52/1000 | Loss: 0.00005270
Iteration 53/1000 | Loss: 0.00005269
Iteration 54/1000 | Loss: 0.00005268
Iteration 55/1000 | Loss: 0.00005268
Iteration 56/1000 | Loss: 0.00005267
Iteration 57/1000 | Loss: 0.00005267
Iteration 58/1000 | Loss: 0.00005267
Iteration 59/1000 | Loss: 0.00005266
Iteration 60/1000 | Loss: 0.00005266
Iteration 61/1000 | Loss: 0.00005266
Iteration 62/1000 | Loss: 0.00005266
Iteration 63/1000 | Loss: 0.00005265
Iteration 64/1000 | Loss: 0.00005265
Iteration 65/1000 | Loss: 0.00005265
Iteration 66/1000 | Loss: 0.00005265
Iteration 67/1000 | Loss: 0.00005264
Iteration 68/1000 | Loss: 0.00005264
Iteration 69/1000 | Loss: 0.00005264
Iteration 70/1000 | Loss: 0.00005264
Iteration 71/1000 | Loss: 0.00005264
Iteration 72/1000 | Loss: 0.00005264
Iteration 73/1000 | Loss: 0.00005263
Iteration 74/1000 | Loss: 0.00005263
Iteration 75/1000 | Loss: 0.00005263
Iteration 76/1000 | Loss: 0.00005263
Iteration 77/1000 | Loss: 0.00005263
Iteration 78/1000 | Loss: 0.00005262
Iteration 79/1000 | Loss: 0.00005262
Iteration 80/1000 | Loss: 0.00005262
Iteration 81/1000 | Loss: 0.00005262
Iteration 82/1000 | Loss: 0.00005261
Iteration 83/1000 | Loss: 0.00005261
Iteration 84/1000 | Loss: 0.00005261
Iteration 85/1000 | Loss: 0.00005261
Iteration 86/1000 | Loss: 0.00005260
Iteration 87/1000 | Loss: 0.00005260
Iteration 88/1000 | Loss: 0.00005260
Iteration 89/1000 | Loss: 0.00005260
Iteration 90/1000 | Loss: 0.00005259
Iteration 91/1000 | Loss: 0.00005259
Iteration 92/1000 | Loss: 0.00005259
Iteration 93/1000 | Loss: 0.00005259
Iteration 94/1000 | Loss: 0.00005259
Iteration 95/1000 | Loss: 0.00005259
Iteration 96/1000 | Loss: 0.00005259
Iteration 97/1000 | Loss: 0.00005259
Iteration 98/1000 | Loss: 0.00005258
Iteration 99/1000 | Loss: 0.00005258
Iteration 100/1000 | Loss: 0.00005258
Iteration 101/1000 | Loss: 0.00005258
Iteration 102/1000 | Loss: 0.00005257
Iteration 103/1000 | Loss: 0.00005257
Iteration 104/1000 | Loss: 0.00005257
Iteration 105/1000 | Loss: 0.00005257
Iteration 106/1000 | Loss: 0.00005257
Iteration 107/1000 | Loss: 0.00005257
Iteration 108/1000 | Loss: 0.00005257
Iteration 109/1000 | Loss: 0.00005257
Iteration 110/1000 | Loss: 0.00005257
Iteration 111/1000 | Loss: 0.00005257
Iteration 112/1000 | Loss: 0.00005256
Iteration 113/1000 | Loss: 0.00005256
Iteration 114/1000 | Loss: 0.00005256
Iteration 115/1000 | Loss: 0.00005256
Iteration 116/1000 | Loss: 0.00005256
Iteration 117/1000 | Loss: 0.00005256
Iteration 118/1000 | Loss: 0.00005256
Iteration 119/1000 | Loss: 0.00005256
Iteration 120/1000 | Loss: 0.00005256
Iteration 121/1000 | Loss: 0.00005256
Iteration 122/1000 | Loss: 0.00005256
Iteration 123/1000 | Loss: 0.00005256
Iteration 124/1000 | Loss: 0.00005256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [5.256030635791831e-05, 5.256030635791831e-05, 5.256030635791831e-05, 5.256030635791831e-05, 5.256030635791831e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.256030635791831e-05

Optimization complete. Final v2v error: 5.977327823638916 mm

Highest mean error: 6.890514850616455 mm for frame 148

Lowest mean error: 5.388188362121582 mm for frame 191

Saving results

Total time: 47.502742528915405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01106040
Iteration 2/25 | Loss: 0.00197992
Iteration 3/25 | Loss: 0.00138915
Iteration 4/25 | Loss: 0.00129407
Iteration 5/25 | Loss: 0.00125695
Iteration 6/25 | Loss: 0.00115294
Iteration 7/25 | Loss: 0.00109519
Iteration 8/25 | Loss: 0.00106177
Iteration 9/25 | Loss: 0.00103272
Iteration 10/25 | Loss: 0.00101102
Iteration 11/25 | Loss: 0.00101111
Iteration 12/25 | Loss: 0.00100759
Iteration 13/25 | Loss: 0.00099887
Iteration 14/25 | Loss: 0.00099513
Iteration 15/25 | Loss: 0.00099400
Iteration 16/25 | Loss: 0.00099367
Iteration 17/25 | Loss: 0.00099351
Iteration 18/25 | Loss: 0.00099344
Iteration 19/25 | Loss: 0.00099343
Iteration 20/25 | Loss: 0.00099343
Iteration 21/25 | Loss: 0.00099343
Iteration 22/25 | Loss: 0.00099343
Iteration 23/25 | Loss: 0.00099343
Iteration 24/25 | Loss: 0.00099343
Iteration 25/25 | Loss: 0.00099343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20564020
Iteration 2/25 | Loss: 0.00150455
Iteration 3/25 | Loss: 0.00141823
Iteration 4/25 | Loss: 0.00141823
Iteration 5/25 | Loss: 0.00141823
Iteration 6/25 | Loss: 0.00141823
Iteration 7/25 | Loss: 0.00141823
Iteration 8/25 | Loss: 0.00141823
Iteration 9/25 | Loss: 0.00141823
Iteration 10/25 | Loss: 0.00141823
Iteration 11/25 | Loss: 0.00141823
Iteration 12/25 | Loss: 0.00141823
Iteration 13/25 | Loss: 0.00141823
Iteration 14/25 | Loss: 0.00141823
Iteration 15/25 | Loss: 0.00141823
Iteration 16/25 | Loss: 0.00141823
Iteration 17/25 | Loss: 0.00141823
Iteration 18/25 | Loss: 0.00141823
Iteration 19/25 | Loss: 0.00141823
Iteration 20/25 | Loss: 0.00141823
Iteration 21/25 | Loss: 0.00141823
Iteration 22/25 | Loss: 0.00141823
Iteration 23/25 | Loss: 0.00141823
Iteration 24/25 | Loss: 0.00141823
Iteration 25/25 | Loss: 0.00141823

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141823
Iteration 2/1000 | Loss: 0.00009284
Iteration 3/1000 | Loss: 0.00006416
Iteration 4/1000 | Loss: 0.00005411
Iteration 5/1000 | Loss: 0.00004922
Iteration 6/1000 | Loss: 0.00004684
Iteration 7/1000 | Loss: 0.00011151
Iteration 8/1000 | Loss: 0.00062062
Iteration 9/1000 | Loss: 0.00044050
Iteration 10/1000 | Loss: 0.00013482
Iteration 11/1000 | Loss: 0.00074418
Iteration 12/1000 | Loss: 0.00028264
Iteration 13/1000 | Loss: 0.00006408
Iteration 14/1000 | Loss: 0.00004440
Iteration 15/1000 | Loss: 0.00065209
Iteration 16/1000 | Loss: 0.00148336
Iteration 17/1000 | Loss: 0.00043819
Iteration 18/1000 | Loss: 0.00005833
Iteration 19/1000 | Loss: 0.00004199
Iteration 20/1000 | Loss: 0.00013427
Iteration 21/1000 | Loss: 0.00019821
Iteration 22/1000 | Loss: 0.00003014
Iteration 23/1000 | Loss: 0.00002881
Iteration 24/1000 | Loss: 0.00002775
Iteration 25/1000 | Loss: 0.00002716
Iteration 26/1000 | Loss: 0.00002678
Iteration 27/1000 | Loss: 0.00010385
Iteration 28/1000 | Loss: 0.00002657
Iteration 29/1000 | Loss: 0.00002626
Iteration 30/1000 | Loss: 0.00007918
Iteration 31/1000 | Loss: 0.00004470
Iteration 32/1000 | Loss: 0.00004807
Iteration 33/1000 | Loss: 0.00002620
Iteration 34/1000 | Loss: 0.00002612
Iteration 35/1000 | Loss: 0.00002603
Iteration 36/1000 | Loss: 0.00002601
Iteration 37/1000 | Loss: 0.00002584
Iteration 38/1000 | Loss: 0.00002580
Iteration 39/1000 | Loss: 0.00002580
Iteration 40/1000 | Loss: 0.00002578
Iteration 41/1000 | Loss: 0.00002577
Iteration 42/1000 | Loss: 0.00002577
Iteration 43/1000 | Loss: 0.00002577
Iteration 44/1000 | Loss: 0.00002577
Iteration 45/1000 | Loss: 0.00002577
Iteration 46/1000 | Loss: 0.00002575
Iteration 47/1000 | Loss: 0.00002574
Iteration 48/1000 | Loss: 0.00002574
Iteration 49/1000 | Loss: 0.00002573
Iteration 50/1000 | Loss: 0.00002573
Iteration 51/1000 | Loss: 0.00002572
Iteration 52/1000 | Loss: 0.00002572
Iteration 53/1000 | Loss: 0.00002572
Iteration 54/1000 | Loss: 0.00002571
Iteration 55/1000 | Loss: 0.00002571
Iteration 56/1000 | Loss: 0.00002571
Iteration 57/1000 | Loss: 0.00002571
Iteration 58/1000 | Loss: 0.00002571
Iteration 59/1000 | Loss: 0.00002571
Iteration 60/1000 | Loss: 0.00002571
Iteration 61/1000 | Loss: 0.00002571
Iteration 62/1000 | Loss: 0.00002571
Iteration 63/1000 | Loss: 0.00002571
Iteration 64/1000 | Loss: 0.00002570
Iteration 65/1000 | Loss: 0.00002570
Iteration 66/1000 | Loss: 0.00002570
Iteration 67/1000 | Loss: 0.00002570
Iteration 68/1000 | Loss: 0.00002570
Iteration 69/1000 | Loss: 0.00002570
Iteration 70/1000 | Loss: 0.00002570
Iteration 71/1000 | Loss: 0.00002570
Iteration 72/1000 | Loss: 0.00002570
Iteration 73/1000 | Loss: 0.00002570
Iteration 74/1000 | Loss: 0.00002569
Iteration 75/1000 | Loss: 0.00002569
Iteration 76/1000 | Loss: 0.00002569
Iteration 77/1000 | Loss: 0.00002569
Iteration 78/1000 | Loss: 0.00002569
Iteration 79/1000 | Loss: 0.00002569
Iteration 80/1000 | Loss: 0.00002569
Iteration 81/1000 | Loss: 0.00002568
Iteration 82/1000 | Loss: 0.00002568
Iteration 83/1000 | Loss: 0.00002568
Iteration 84/1000 | Loss: 0.00002568
Iteration 85/1000 | Loss: 0.00002568
Iteration 86/1000 | Loss: 0.00002567
Iteration 87/1000 | Loss: 0.00002567
Iteration 88/1000 | Loss: 0.00002567
Iteration 89/1000 | Loss: 0.00002567
Iteration 90/1000 | Loss: 0.00002566
Iteration 91/1000 | Loss: 0.00002566
Iteration 92/1000 | Loss: 0.00002566
Iteration 93/1000 | Loss: 0.00002566
Iteration 94/1000 | Loss: 0.00002566
Iteration 95/1000 | Loss: 0.00002566
Iteration 96/1000 | Loss: 0.00002566
Iteration 97/1000 | Loss: 0.00002566
Iteration 98/1000 | Loss: 0.00002566
Iteration 99/1000 | Loss: 0.00002566
Iteration 100/1000 | Loss: 0.00002566
Iteration 101/1000 | Loss: 0.00002566
Iteration 102/1000 | Loss: 0.00002566
Iteration 103/1000 | Loss: 0.00002566
Iteration 104/1000 | Loss: 0.00002566
Iteration 105/1000 | Loss: 0.00002566
Iteration 106/1000 | Loss: 0.00002566
Iteration 107/1000 | Loss: 0.00002566
Iteration 108/1000 | Loss: 0.00002566
Iteration 109/1000 | Loss: 0.00002566
Iteration 110/1000 | Loss: 0.00002566
Iteration 111/1000 | Loss: 0.00002566
Iteration 112/1000 | Loss: 0.00002566
Iteration 113/1000 | Loss: 0.00002566
Iteration 114/1000 | Loss: 0.00002566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.566154762462247e-05, 2.566154762462247e-05, 2.566154762462247e-05, 2.566154762462247e-05, 2.566154762462247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.566154762462247e-05

Optimization complete. Final v2v error: 4.257246017456055 mm

Highest mean error: 10.570330619812012 mm for frame 109

Lowest mean error: 3.887282609939575 mm for frame 224

Saving results

Total time: 96.64548540115356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411537
Iteration 2/25 | Loss: 0.00115477
Iteration 3/25 | Loss: 0.00097846
Iteration 4/25 | Loss: 0.00095572
Iteration 5/25 | Loss: 0.00095060
Iteration 6/25 | Loss: 0.00095001
Iteration 7/25 | Loss: 0.00095001
Iteration 8/25 | Loss: 0.00095001
Iteration 9/25 | Loss: 0.00095001
Iteration 10/25 | Loss: 0.00095001
Iteration 11/25 | Loss: 0.00095001
Iteration 12/25 | Loss: 0.00095001
Iteration 13/25 | Loss: 0.00095001
Iteration 14/25 | Loss: 0.00095001
Iteration 15/25 | Loss: 0.00095001
Iteration 16/25 | Loss: 0.00095001
Iteration 17/25 | Loss: 0.00095001
Iteration 18/25 | Loss: 0.00095001
Iteration 19/25 | Loss: 0.00095001
Iteration 20/25 | Loss: 0.00095001
Iteration 21/25 | Loss: 0.00095001
Iteration 22/25 | Loss: 0.00095001
Iteration 23/25 | Loss: 0.00095001
Iteration 24/25 | Loss: 0.00095001
Iteration 25/25 | Loss: 0.00095001

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23853195
Iteration 2/25 | Loss: 0.00118551
Iteration 3/25 | Loss: 0.00118550
Iteration 4/25 | Loss: 0.00118550
Iteration 5/25 | Loss: 0.00118550
Iteration 6/25 | Loss: 0.00118550
Iteration 7/25 | Loss: 0.00118550
Iteration 8/25 | Loss: 0.00118550
Iteration 9/25 | Loss: 0.00118550
Iteration 10/25 | Loss: 0.00118550
Iteration 11/25 | Loss: 0.00118550
Iteration 12/25 | Loss: 0.00118550
Iteration 13/25 | Loss: 0.00118550
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011854983167722821, 0.0011854983167722821, 0.0011854983167722821, 0.0011854983167722821, 0.0011854983167722821]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011854983167722821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118550
Iteration 2/1000 | Loss: 0.00005464
Iteration 3/1000 | Loss: 0.00003372
Iteration 4/1000 | Loss: 0.00002873
Iteration 5/1000 | Loss: 0.00002674
Iteration 6/1000 | Loss: 0.00002556
Iteration 7/1000 | Loss: 0.00002483
Iteration 8/1000 | Loss: 0.00002427
Iteration 9/1000 | Loss: 0.00002378
Iteration 10/1000 | Loss: 0.00002336
Iteration 11/1000 | Loss: 0.00002307
Iteration 12/1000 | Loss: 0.00002291
Iteration 13/1000 | Loss: 0.00002288
Iteration 14/1000 | Loss: 0.00002272
Iteration 15/1000 | Loss: 0.00002271
Iteration 16/1000 | Loss: 0.00002262
Iteration 17/1000 | Loss: 0.00002259
Iteration 18/1000 | Loss: 0.00002258
Iteration 19/1000 | Loss: 0.00002258
Iteration 20/1000 | Loss: 0.00002256
Iteration 21/1000 | Loss: 0.00002256
Iteration 22/1000 | Loss: 0.00002256
Iteration 23/1000 | Loss: 0.00002256
Iteration 24/1000 | Loss: 0.00002255
Iteration 25/1000 | Loss: 0.00002255
Iteration 26/1000 | Loss: 0.00002255
Iteration 27/1000 | Loss: 0.00002254
Iteration 28/1000 | Loss: 0.00002254
Iteration 29/1000 | Loss: 0.00002254
Iteration 30/1000 | Loss: 0.00002254
Iteration 31/1000 | Loss: 0.00002254
Iteration 32/1000 | Loss: 0.00002253
Iteration 33/1000 | Loss: 0.00002253
Iteration 34/1000 | Loss: 0.00002253
Iteration 35/1000 | Loss: 0.00002252
Iteration 36/1000 | Loss: 0.00002252
Iteration 37/1000 | Loss: 0.00002252
Iteration 38/1000 | Loss: 0.00002252
Iteration 39/1000 | Loss: 0.00002252
Iteration 40/1000 | Loss: 0.00002252
Iteration 41/1000 | Loss: 0.00002252
Iteration 42/1000 | Loss: 0.00002252
Iteration 43/1000 | Loss: 0.00002251
Iteration 44/1000 | Loss: 0.00002251
Iteration 45/1000 | Loss: 0.00002251
Iteration 46/1000 | Loss: 0.00002251
Iteration 47/1000 | Loss: 0.00002251
Iteration 48/1000 | Loss: 0.00002250
Iteration 49/1000 | Loss: 0.00002249
Iteration 50/1000 | Loss: 0.00002249
Iteration 51/1000 | Loss: 0.00002249
Iteration 52/1000 | Loss: 0.00002248
Iteration 53/1000 | Loss: 0.00002248
Iteration 54/1000 | Loss: 0.00002248
Iteration 55/1000 | Loss: 0.00002248
Iteration 56/1000 | Loss: 0.00002247
Iteration 57/1000 | Loss: 0.00002247
Iteration 58/1000 | Loss: 0.00002247
Iteration 59/1000 | Loss: 0.00002247
Iteration 60/1000 | Loss: 0.00002246
Iteration 61/1000 | Loss: 0.00002246
Iteration 62/1000 | Loss: 0.00002246
Iteration 63/1000 | Loss: 0.00002246
Iteration 64/1000 | Loss: 0.00002246
Iteration 65/1000 | Loss: 0.00002246
Iteration 66/1000 | Loss: 0.00002246
Iteration 67/1000 | Loss: 0.00002246
Iteration 68/1000 | Loss: 0.00002246
Iteration 69/1000 | Loss: 0.00002246
Iteration 70/1000 | Loss: 0.00002245
Iteration 71/1000 | Loss: 0.00002245
Iteration 72/1000 | Loss: 0.00002245
Iteration 73/1000 | Loss: 0.00002245
Iteration 74/1000 | Loss: 0.00002244
Iteration 75/1000 | Loss: 0.00002244
Iteration 76/1000 | Loss: 0.00002244
Iteration 77/1000 | Loss: 0.00002243
Iteration 78/1000 | Loss: 0.00002243
Iteration 79/1000 | Loss: 0.00002243
Iteration 80/1000 | Loss: 0.00002243
Iteration 81/1000 | Loss: 0.00002243
Iteration 82/1000 | Loss: 0.00002242
Iteration 83/1000 | Loss: 0.00002242
Iteration 84/1000 | Loss: 0.00002242
Iteration 85/1000 | Loss: 0.00002241
Iteration 86/1000 | Loss: 0.00002241
Iteration 87/1000 | Loss: 0.00002241
Iteration 88/1000 | Loss: 0.00002241
Iteration 89/1000 | Loss: 0.00002241
Iteration 90/1000 | Loss: 0.00002240
Iteration 91/1000 | Loss: 0.00002240
Iteration 92/1000 | Loss: 0.00002240
Iteration 93/1000 | Loss: 0.00002240
Iteration 94/1000 | Loss: 0.00002240
Iteration 95/1000 | Loss: 0.00002240
Iteration 96/1000 | Loss: 0.00002239
Iteration 97/1000 | Loss: 0.00002239
Iteration 98/1000 | Loss: 0.00002239
Iteration 99/1000 | Loss: 0.00002239
Iteration 100/1000 | Loss: 0.00002238
Iteration 101/1000 | Loss: 0.00002238
Iteration 102/1000 | Loss: 0.00002238
Iteration 103/1000 | Loss: 0.00002238
Iteration 104/1000 | Loss: 0.00002238
Iteration 105/1000 | Loss: 0.00002238
Iteration 106/1000 | Loss: 0.00002238
Iteration 107/1000 | Loss: 0.00002238
Iteration 108/1000 | Loss: 0.00002238
Iteration 109/1000 | Loss: 0.00002237
Iteration 110/1000 | Loss: 0.00002237
Iteration 111/1000 | Loss: 0.00002237
Iteration 112/1000 | Loss: 0.00002237
Iteration 113/1000 | Loss: 0.00002237
Iteration 114/1000 | Loss: 0.00002237
Iteration 115/1000 | Loss: 0.00002237
Iteration 116/1000 | Loss: 0.00002237
Iteration 117/1000 | Loss: 0.00002237
Iteration 118/1000 | Loss: 0.00002237
Iteration 119/1000 | Loss: 0.00002237
Iteration 120/1000 | Loss: 0.00002237
Iteration 121/1000 | Loss: 0.00002237
Iteration 122/1000 | Loss: 0.00002237
Iteration 123/1000 | Loss: 0.00002237
Iteration 124/1000 | Loss: 0.00002237
Iteration 125/1000 | Loss: 0.00002236
Iteration 126/1000 | Loss: 0.00002236
Iteration 127/1000 | Loss: 0.00002236
Iteration 128/1000 | Loss: 0.00002236
Iteration 129/1000 | Loss: 0.00002236
Iteration 130/1000 | Loss: 0.00002236
Iteration 131/1000 | Loss: 0.00002235
Iteration 132/1000 | Loss: 0.00002235
Iteration 133/1000 | Loss: 0.00002235
Iteration 134/1000 | Loss: 0.00002235
Iteration 135/1000 | Loss: 0.00002235
Iteration 136/1000 | Loss: 0.00002235
Iteration 137/1000 | Loss: 0.00002235
Iteration 138/1000 | Loss: 0.00002235
Iteration 139/1000 | Loss: 0.00002235
Iteration 140/1000 | Loss: 0.00002234
Iteration 141/1000 | Loss: 0.00002234
Iteration 142/1000 | Loss: 0.00002234
Iteration 143/1000 | Loss: 0.00002234
Iteration 144/1000 | Loss: 0.00002234
Iteration 145/1000 | Loss: 0.00002234
Iteration 146/1000 | Loss: 0.00002234
Iteration 147/1000 | Loss: 0.00002233
Iteration 148/1000 | Loss: 0.00002233
Iteration 149/1000 | Loss: 0.00002233
Iteration 150/1000 | Loss: 0.00002233
Iteration 151/1000 | Loss: 0.00002233
Iteration 152/1000 | Loss: 0.00002233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.2333631932269782e-05, 2.2333631932269782e-05, 2.2333631932269782e-05, 2.2333631932269782e-05, 2.2333631932269782e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2333631932269782e-05

Optimization complete. Final v2v error: 4.0171661376953125 mm

Highest mean error: 4.874255657196045 mm for frame 6

Lowest mean error: 3.3971035480499268 mm for frame 102

Saving results

Total time: 43.25346231460571
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486280
Iteration 2/25 | Loss: 0.00109981
Iteration 3/25 | Loss: 0.00099464
Iteration 4/25 | Loss: 0.00097200
Iteration 5/25 | Loss: 0.00096332
Iteration 6/25 | Loss: 0.00096202
Iteration 7/25 | Loss: 0.00096171
Iteration 8/25 | Loss: 0.00096171
Iteration 9/25 | Loss: 0.00096171
Iteration 10/25 | Loss: 0.00096171
Iteration 11/25 | Loss: 0.00096171
Iteration 12/25 | Loss: 0.00096171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009617080213502049, 0.0009617080213502049, 0.0009617080213502049, 0.0009617080213502049, 0.0009617080213502049]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009617080213502049

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.35324907
Iteration 2/25 | Loss: 0.00104034
Iteration 3/25 | Loss: 0.00104034
Iteration 4/25 | Loss: 0.00104034
Iteration 5/25 | Loss: 0.00104034
Iteration 6/25 | Loss: 0.00104034
Iteration 7/25 | Loss: 0.00104034
Iteration 8/25 | Loss: 0.00104034
Iteration 9/25 | Loss: 0.00104034
Iteration 10/25 | Loss: 0.00104034
Iteration 11/25 | Loss: 0.00104034
Iteration 12/25 | Loss: 0.00104034
Iteration 13/25 | Loss: 0.00104034
Iteration 14/25 | Loss: 0.00104034
Iteration 15/25 | Loss: 0.00104034
Iteration 16/25 | Loss: 0.00104034
Iteration 17/25 | Loss: 0.00104034
Iteration 18/25 | Loss: 0.00104034
Iteration 19/25 | Loss: 0.00104034
Iteration 20/25 | Loss: 0.00104034
Iteration 21/25 | Loss: 0.00104034
Iteration 22/25 | Loss: 0.00104034
Iteration 23/25 | Loss: 0.00104034
Iteration 24/25 | Loss: 0.00104034
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010403378400951624, 0.0010403378400951624, 0.0010403378400951624, 0.0010403378400951624, 0.0010403378400951624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010403378400951624

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104034
Iteration 2/1000 | Loss: 0.00004947
Iteration 3/1000 | Loss: 0.00003791
Iteration 4/1000 | Loss: 0.00003382
Iteration 5/1000 | Loss: 0.00003133
Iteration 6/1000 | Loss: 0.00002968
Iteration 7/1000 | Loss: 0.00002882
Iteration 8/1000 | Loss: 0.00002802
Iteration 9/1000 | Loss: 0.00002758
Iteration 10/1000 | Loss: 0.00002730
Iteration 11/1000 | Loss: 0.00002710
Iteration 12/1000 | Loss: 0.00002697
Iteration 13/1000 | Loss: 0.00002683
Iteration 14/1000 | Loss: 0.00002674
Iteration 15/1000 | Loss: 0.00002674
Iteration 16/1000 | Loss: 0.00002667
Iteration 17/1000 | Loss: 0.00002667
Iteration 18/1000 | Loss: 0.00002666
Iteration 19/1000 | Loss: 0.00002665
Iteration 20/1000 | Loss: 0.00002665
Iteration 21/1000 | Loss: 0.00002665
Iteration 22/1000 | Loss: 0.00002664
Iteration 23/1000 | Loss: 0.00002664
Iteration 24/1000 | Loss: 0.00002663
Iteration 25/1000 | Loss: 0.00002663
Iteration 26/1000 | Loss: 0.00002663
Iteration 27/1000 | Loss: 0.00002663
Iteration 28/1000 | Loss: 0.00002662
Iteration 29/1000 | Loss: 0.00002662
Iteration 30/1000 | Loss: 0.00002661
Iteration 31/1000 | Loss: 0.00002661
Iteration 32/1000 | Loss: 0.00002661
Iteration 33/1000 | Loss: 0.00002661
Iteration 34/1000 | Loss: 0.00002661
Iteration 35/1000 | Loss: 0.00002661
Iteration 36/1000 | Loss: 0.00002661
Iteration 37/1000 | Loss: 0.00002660
Iteration 38/1000 | Loss: 0.00002660
Iteration 39/1000 | Loss: 0.00002660
Iteration 40/1000 | Loss: 0.00002660
Iteration 41/1000 | Loss: 0.00002660
Iteration 42/1000 | Loss: 0.00002660
Iteration 43/1000 | Loss: 0.00002660
Iteration 44/1000 | Loss: 0.00002660
Iteration 45/1000 | Loss: 0.00002660
Iteration 46/1000 | Loss: 0.00002659
Iteration 47/1000 | Loss: 0.00002659
Iteration 48/1000 | Loss: 0.00002659
Iteration 49/1000 | Loss: 0.00002658
Iteration 50/1000 | Loss: 0.00002658
Iteration 51/1000 | Loss: 0.00002658
Iteration 52/1000 | Loss: 0.00002658
Iteration 53/1000 | Loss: 0.00002657
Iteration 54/1000 | Loss: 0.00002657
Iteration 55/1000 | Loss: 0.00002657
Iteration 56/1000 | Loss: 0.00002657
Iteration 57/1000 | Loss: 0.00002657
Iteration 58/1000 | Loss: 0.00002657
Iteration 59/1000 | Loss: 0.00002656
Iteration 60/1000 | Loss: 0.00002656
Iteration 61/1000 | Loss: 0.00002656
Iteration 62/1000 | Loss: 0.00002656
Iteration 63/1000 | Loss: 0.00002656
Iteration 64/1000 | Loss: 0.00002656
Iteration 65/1000 | Loss: 0.00002656
Iteration 66/1000 | Loss: 0.00002656
Iteration 67/1000 | Loss: 0.00002655
Iteration 68/1000 | Loss: 0.00002655
Iteration 69/1000 | Loss: 0.00002655
Iteration 70/1000 | Loss: 0.00002655
Iteration 71/1000 | Loss: 0.00002655
Iteration 72/1000 | Loss: 0.00002655
Iteration 73/1000 | Loss: 0.00002655
Iteration 74/1000 | Loss: 0.00002655
Iteration 75/1000 | Loss: 0.00002655
Iteration 76/1000 | Loss: 0.00002655
Iteration 77/1000 | Loss: 0.00002655
Iteration 78/1000 | Loss: 0.00002655
Iteration 79/1000 | Loss: 0.00002655
Iteration 80/1000 | Loss: 0.00002655
Iteration 81/1000 | Loss: 0.00002655
Iteration 82/1000 | Loss: 0.00002655
Iteration 83/1000 | Loss: 0.00002655
Iteration 84/1000 | Loss: 0.00002655
Iteration 85/1000 | Loss: 0.00002655
Iteration 86/1000 | Loss: 0.00002655
Iteration 87/1000 | Loss: 0.00002655
Iteration 88/1000 | Loss: 0.00002655
Iteration 89/1000 | Loss: 0.00002655
Iteration 90/1000 | Loss: 0.00002655
Iteration 91/1000 | Loss: 0.00002655
Iteration 92/1000 | Loss: 0.00002655
Iteration 93/1000 | Loss: 0.00002655
Iteration 94/1000 | Loss: 0.00002655
Iteration 95/1000 | Loss: 0.00002655
Iteration 96/1000 | Loss: 0.00002655
Iteration 97/1000 | Loss: 0.00002655
Iteration 98/1000 | Loss: 0.00002655
Iteration 99/1000 | Loss: 0.00002655
Iteration 100/1000 | Loss: 0.00002655
Iteration 101/1000 | Loss: 0.00002655
Iteration 102/1000 | Loss: 0.00002655
Iteration 103/1000 | Loss: 0.00002655
Iteration 104/1000 | Loss: 0.00002655
Iteration 105/1000 | Loss: 0.00002655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.6551022529019974e-05, 2.6551022529019974e-05, 2.6551022529019974e-05, 2.6551022529019974e-05, 2.6551022529019974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6551022529019974e-05

Optimization complete. Final v2v error: 4.388301849365234 mm

Highest mean error: 4.818182945251465 mm for frame 147

Lowest mean error: 3.8923120498657227 mm for frame 45

Saving results

Total time: 33.65858769416809
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00497897
Iteration 2/25 | Loss: 0.00112021
Iteration 3/25 | Loss: 0.00100225
Iteration 4/25 | Loss: 0.00097743
Iteration 5/25 | Loss: 0.00096451
Iteration 6/25 | Loss: 0.00096171
Iteration 7/25 | Loss: 0.00096094
Iteration 8/25 | Loss: 0.00096094
Iteration 9/25 | Loss: 0.00096094
Iteration 10/25 | Loss: 0.00096094
Iteration 11/25 | Loss: 0.00096094
Iteration 12/25 | Loss: 0.00096094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009609425324015319, 0.0009609425324015319, 0.0009609425324015319, 0.0009609425324015319, 0.0009609425324015319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009609425324015319

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.25079012
Iteration 2/25 | Loss: 0.00099821
Iteration 3/25 | Loss: 0.00099821
Iteration 4/25 | Loss: 0.00099821
Iteration 5/25 | Loss: 0.00099821
Iteration 6/25 | Loss: 0.00099821
Iteration 7/25 | Loss: 0.00099821
Iteration 8/25 | Loss: 0.00099821
Iteration 9/25 | Loss: 0.00099821
Iteration 10/25 | Loss: 0.00099821
Iteration 11/25 | Loss: 0.00099821
Iteration 12/25 | Loss: 0.00099821
Iteration 13/25 | Loss: 0.00099821
Iteration 14/25 | Loss: 0.00099821
Iteration 15/25 | Loss: 0.00099821
Iteration 16/25 | Loss: 0.00099821
Iteration 17/25 | Loss: 0.00099821
Iteration 18/25 | Loss: 0.00099821
Iteration 19/25 | Loss: 0.00099821
Iteration 20/25 | Loss: 0.00099821
Iteration 21/25 | Loss: 0.00099821
Iteration 22/25 | Loss: 0.00099821
Iteration 23/25 | Loss: 0.00099821
Iteration 24/25 | Loss: 0.00099821
Iteration 25/25 | Loss: 0.00099821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099821
Iteration 2/1000 | Loss: 0.00005066
Iteration 3/1000 | Loss: 0.00003536
Iteration 4/1000 | Loss: 0.00003079
Iteration 5/1000 | Loss: 0.00002874
Iteration 6/1000 | Loss: 0.00002758
Iteration 7/1000 | Loss: 0.00002674
Iteration 8/1000 | Loss: 0.00002613
Iteration 9/1000 | Loss: 0.00002558
Iteration 10/1000 | Loss: 0.00002527
Iteration 11/1000 | Loss: 0.00002517
Iteration 12/1000 | Loss: 0.00002507
Iteration 13/1000 | Loss: 0.00002506
Iteration 14/1000 | Loss: 0.00002505
Iteration 15/1000 | Loss: 0.00002496
Iteration 16/1000 | Loss: 0.00002478
Iteration 17/1000 | Loss: 0.00002471
Iteration 18/1000 | Loss: 0.00002467
Iteration 19/1000 | Loss: 0.00002467
Iteration 20/1000 | Loss: 0.00002464
Iteration 21/1000 | Loss: 0.00002462
Iteration 22/1000 | Loss: 0.00002462
Iteration 23/1000 | Loss: 0.00002461
Iteration 24/1000 | Loss: 0.00002460
Iteration 25/1000 | Loss: 0.00002460
Iteration 26/1000 | Loss: 0.00002458
Iteration 27/1000 | Loss: 0.00002458
Iteration 28/1000 | Loss: 0.00002458
Iteration 29/1000 | Loss: 0.00002457
Iteration 30/1000 | Loss: 0.00002457
Iteration 31/1000 | Loss: 0.00002457
Iteration 32/1000 | Loss: 0.00002456
Iteration 33/1000 | Loss: 0.00002456
Iteration 34/1000 | Loss: 0.00002455
Iteration 35/1000 | Loss: 0.00002455
Iteration 36/1000 | Loss: 0.00002454
Iteration 37/1000 | Loss: 0.00002454
Iteration 38/1000 | Loss: 0.00002454
Iteration 39/1000 | Loss: 0.00002453
Iteration 40/1000 | Loss: 0.00002453
Iteration 41/1000 | Loss: 0.00002453
Iteration 42/1000 | Loss: 0.00002453
Iteration 43/1000 | Loss: 0.00002452
Iteration 44/1000 | Loss: 0.00002452
Iteration 45/1000 | Loss: 0.00002452
Iteration 46/1000 | Loss: 0.00002451
Iteration 47/1000 | Loss: 0.00002451
Iteration 48/1000 | Loss: 0.00002451
Iteration 49/1000 | Loss: 0.00002450
Iteration 50/1000 | Loss: 0.00002450
Iteration 51/1000 | Loss: 0.00002450
Iteration 52/1000 | Loss: 0.00002450
Iteration 53/1000 | Loss: 0.00002450
Iteration 54/1000 | Loss: 0.00002449
Iteration 55/1000 | Loss: 0.00002449
Iteration 56/1000 | Loss: 0.00002449
Iteration 57/1000 | Loss: 0.00002449
Iteration 58/1000 | Loss: 0.00002449
Iteration 59/1000 | Loss: 0.00002448
Iteration 60/1000 | Loss: 0.00002448
Iteration 61/1000 | Loss: 0.00002448
Iteration 62/1000 | Loss: 0.00002448
Iteration 63/1000 | Loss: 0.00002448
Iteration 64/1000 | Loss: 0.00002448
Iteration 65/1000 | Loss: 0.00002448
Iteration 66/1000 | Loss: 0.00002448
Iteration 67/1000 | Loss: 0.00002448
Iteration 68/1000 | Loss: 0.00002448
Iteration 69/1000 | Loss: 0.00002447
Iteration 70/1000 | Loss: 0.00002447
Iteration 71/1000 | Loss: 0.00002447
Iteration 72/1000 | Loss: 0.00002447
Iteration 73/1000 | Loss: 0.00002447
Iteration 74/1000 | Loss: 0.00002447
Iteration 75/1000 | Loss: 0.00002446
Iteration 76/1000 | Loss: 0.00002446
Iteration 77/1000 | Loss: 0.00002446
Iteration 78/1000 | Loss: 0.00002446
Iteration 79/1000 | Loss: 0.00002446
Iteration 80/1000 | Loss: 0.00002445
Iteration 81/1000 | Loss: 0.00002445
Iteration 82/1000 | Loss: 0.00002445
Iteration 83/1000 | Loss: 0.00002445
Iteration 84/1000 | Loss: 0.00002445
Iteration 85/1000 | Loss: 0.00002445
Iteration 86/1000 | Loss: 0.00002444
Iteration 87/1000 | Loss: 0.00002444
Iteration 88/1000 | Loss: 0.00002444
Iteration 89/1000 | Loss: 0.00002444
Iteration 90/1000 | Loss: 0.00002443
Iteration 91/1000 | Loss: 0.00002443
Iteration 92/1000 | Loss: 0.00002443
Iteration 93/1000 | Loss: 0.00002443
Iteration 94/1000 | Loss: 0.00002443
Iteration 95/1000 | Loss: 0.00002443
Iteration 96/1000 | Loss: 0.00002443
Iteration 97/1000 | Loss: 0.00002443
Iteration 98/1000 | Loss: 0.00002443
Iteration 99/1000 | Loss: 0.00002442
Iteration 100/1000 | Loss: 0.00002442
Iteration 101/1000 | Loss: 0.00002442
Iteration 102/1000 | Loss: 0.00002442
Iteration 103/1000 | Loss: 0.00002442
Iteration 104/1000 | Loss: 0.00002442
Iteration 105/1000 | Loss: 0.00002442
Iteration 106/1000 | Loss: 0.00002441
Iteration 107/1000 | Loss: 0.00002441
Iteration 108/1000 | Loss: 0.00002441
Iteration 109/1000 | Loss: 0.00002441
Iteration 110/1000 | Loss: 0.00002441
Iteration 111/1000 | Loss: 0.00002441
Iteration 112/1000 | Loss: 0.00002441
Iteration 113/1000 | Loss: 0.00002441
Iteration 114/1000 | Loss: 0.00002440
Iteration 115/1000 | Loss: 0.00002440
Iteration 116/1000 | Loss: 0.00002440
Iteration 117/1000 | Loss: 0.00002440
Iteration 118/1000 | Loss: 0.00002440
Iteration 119/1000 | Loss: 0.00002439
Iteration 120/1000 | Loss: 0.00002439
Iteration 121/1000 | Loss: 0.00002439
Iteration 122/1000 | Loss: 0.00002439
Iteration 123/1000 | Loss: 0.00002439
Iteration 124/1000 | Loss: 0.00002439
Iteration 125/1000 | Loss: 0.00002439
Iteration 126/1000 | Loss: 0.00002439
Iteration 127/1000 | Loss: 0.00002439
Iteration 128/1000 | Loss: 0.00002438
Iteration 129/1000 | Loss: 0.00002438
Iteration 130/1000 | Loss: 0.00002438
Iteration 131/1000 | Loss: 0.00002438
Iteration 132/1000 | Loss: 0.00002438
Iteration 133/1000 | Loss: 0.00002438
Iteration 134/1000 | Loss: 0.00002438
Iteration 135/1000 | Loss: 0.00002438
Iteration 136/1000 | Loss: 0.00002438
Iteration 137/1000 | Loss: 0.00002438
Iteration 138/1000 | Loss: 0.00002438
Iteration 139/1000 | Loss: 0.00002438
Iteration 140/1000 | Loss: 0.00002438
Iteration 141/1000 | Loss: 0.00002438
Iteration 142/1000 | Loss: 0.00002438
Iteration 143/1000 | Loss: 0.00002438
Iteration 144/1000 | Loss: 0.00002438
Iteration 145/1000 | Loss: 0.00002438
Iteration 146/1000 | Loss: 0.00002438
Iteration 147/1000 | Loss: 0.00002438
Iteration 148/1000 | Loss: 0.00002438
Iteration 149/1000 | Loss: 0.00002438
Iteration 150/1000 | Loss: 0.00002438
Iteration 151/1000 | Loss: 0.00002438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.43785743805347e-05, 2.43785743805347e-05, 2.43785743805347e-05, 2.43785743805347e-05, 2.43785743805347e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.43785743805347e-05

Optimization complete. Final v2v error: 4.212409973144531 mm

Highest mean error: 4.8964314460754395 mm for frame 1

Lowest mean error: 3.8435213565826416 mm for frame 130

Saving results

Total time: 38.67470407485962
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818226
Iteration 2/25 | Loss: 0.00145074
Iteration 3/25 | Loss: 0.00112932
Iteration 4/25 | Loss: 0.00107024
Iteration 5/25 | Loss: 0.00106159
Iteration 6/25 | Loss: 0.00105918
Iteration 7/25 | Loss: 0.00105864
Iteration 8/25 | Loss: 0.00105847
Iteration 9/25 | Loss: 0.00105847
Iteration 10/25 | Loss: 0.00105847
Iteration 11/25 | Loss: 0.00105847
Iteration 12/25 | Loss: 0.00105847
Iteration 13/25 | Loss: 0.00105847
Iteration 14/25 | Loss: 0.00105847
Iteration 15/25 | Loss: 0.00105847
Iteration 16/25 | Loss: 0.00105847
Iteration 17/25 | Loss: 0.00105847
Iteration 18/25 | Loss: 0.00105847
Iteration 19/25 | Loss: 0.00105847
Iteration 20/25 | Loss: 0.00105847
Iteration 21/25 | Loss: 0.00105847
Iteration 22/25 | Loss: 0.00105847
Iteration 23/25 | Loss: 0.00105847
Iteration 24/25 | Loss: 0.00105847
Iteration 25/25 | Loss: 0.00105847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25665390
Iteration 2/25 | Loss: 0.00108386
Iteration 3/25 | Loss: 0.00108386
Iteration 4/25 | Loss: 0.00108386
Iteration 5/25 | Loss: 0.00108386
Iteration 6/25 | Loss: 0.00108386
Iteration 7/25 | Loss: 0.00108386
Iteration 8/25 | Loss: 0.00108386
Iteration 9/25 | Loss: 0.00108386
Iteration 10/25 | Loss: 0.00108386
Iteration 11/25 | Loss: 0.00108386
Iteration 12/25 | Loss: 0.00108385
Iteration 13/25 | Loss: 0.00108386
Iteration 14/25 | Loss: 0.00108386
Iteration 15/25 | Loss: 0.00108386
Iteration 16/25 | Loss: 0.00108386
Iteration 17/25 | Loss: 0.00108386
Iteration 18/25 | Loss: 0.00108386
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001083855051547289, 0.001083855051547289, 0.001083855051547289, 0.001083855051547289, 0.001083855051547289]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001083855051547289

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108386
Iteration 2/1000 | Loss: 0.00006614
Iteration 3/1000 | Loss: 0.00004462
Iteration 4/1000 | Loss: 0.00003531
Iteration 5/1000 | Loss: 0.00003291
Iteration 6/1000 | Loss: 0.00003091
Iteration 7/1000 | Loss: 0.00002993
Iteration 8/1000 | Loss: 0.00002921
Iteration 9/1000 | Loss: 0.00002851
Iteration 10/1000 | Loss: 0.00002808
Iteration 11/1000 | Loss: 0.00002779
Iteration 12/1000 | Loss: 0.00002753
Iteration 13/1000 | Loss: 0.00002733
Iteration 14/1000 | Loss: 0.00002719
Iteration 15/1000 | Loss: 0.00002717
Iteration 16/1000 | Loss: 0.00002712
Iteration 17/1000 | Loss: 0.00002712
Iteration 18/1000 | Loss: 0.00002711
Iteration 19/1000 | Loss: 0.00002710
Iteration 20/1000 | Loss: 0.00002706
Iteration 21/1000 | Loss: 0.00002706
Iteration 22/1000 | Loss: 0.00002706
Iteration 23/1000 | Loss: 0.00002706
Iteration 24/1000 | Loss: 0.00002706
Iteration 25/1000 | Loss: 0.00002705
Iteration 26/1000 | Loss: 0.00002705
Iteration 27/1000 | Loss: 0.00002704
Iteration 28/1000 | Loss: 0.00002704
Iteration 29/1000 | Loss: 0.00002703
Iteration 30/1000 | Loss: 0.00002702
Iteration 31/1000 | Loss: 0.00002702
Iteration 32/1000 | Loss: 0.00002702
Iteration 33/1000 | Loss: 0.00002702
Iteration 34/1000 | Loss: 0.00002701
Iteration 35/1000 | Loss: 0.00002701
Iteration 36/1000 | Loss: 0.00002701
Iteration 37/1000 | Loss: 0.00002701
Iteration 38/1000 | Loss: 0.00002701
Iteration 39/1000 | Loss: 0.00002701
Iteration 40/1000 | Loss: 0.00002701
Iteration 41/1000 | Loss: 0.00002701
Iteration 42/1000 | Loss: 0.00002701
Iteration 43/1000 | Loss: 0.00002701
Iteration 44/1000 | Loss: 0.00002700
Iteration 45/1000 | Loss: 0.00002700
Iteration 46/1000 | Loss: 0.00002699
Iteration 47/1000 | Loss: 0.00002699
Iteration 48/1000 | Loss: 0.00002699
Iteration 49/1000 | Loss: 0.00002698
Iteration 50/1000 | Loss: 0.00002698
Iteration 51/1000 | Loss: 0.00002698
Iteration 52/1000 | Loss: 0.00002698
Iteration 53/1000 | Loss: 0.00002698
Iteration 54/1000 | Loss: 0.00002698
Iteration 55/1000 | Loss: 0.00002697
Iteration 56/1000 | Loss: 0.00002697
Iteration 57/1000 | Loss: 0.00002697
Iteration 58/1000 | Loss: 0.00002697
Iteration 59/1000 | Loss: 0.00002697
Iteration 60/1000 | Loss: 0.00002696
Iteration 61/1000 | Loss: 0.00002696
Iteration 62/1000 | Loss: 0.00002696
Iteration 63/1000 | Loss: 0.00002696
Iteration 64/1000 | Loss: 0.00002696
Iteration 65/1000 | Loss: 0.00002696
Iteration 66/1000 | Loss: 0.00002695
Iteration 67/1000 | Loss: 0.00002695
Iteration 68/1000 | Loss: 0.00002695
Iteration 69/1000 | Loss: 0.00002695
Iteration 70/1000 | Loss: 0.00002695
Iteration 71/1000 | Loss: 0.00002695
Iteration 72/1000 | Loss: 0.00002695
Iteration 73/1000 | Loss: 0.00002695
Iteration 74/1000 | Loss: 0.00002695
Iteration 75/1000 | Loss: 0.00002695
Iteration 76/1000 | Loss: 0.00002695
Iteration 77/1000 | Loss: 0.00002695
Iteration 78/1000 | Loss: 0.00002694
Iteration 79/1000 | Loss: 0.00002694
Iteration 80/1000 | Loss: 0.00002694
Iteration 81/1000 | Loss: 0.00002694
Iteration 82/1000 | Loss: 0.00002694
Iteration 83/1000 | Loss: 0.00002693
Iteration 84/1000 | Loss: 0.00002693
Iteration 85/1000 | Loss: 0.00002693
Iteration 86/1000 | Loss: 0.00002693
Iteration 87/1000 | Loss: 0.00002693
Iteration 88/1000 | Loss: 0.00002693
Iteration 89/1000 | Loss: 0.00002693
Iteration 90/1000 | Loss: 0.00002693
Iteration 91/1000 | Loss: 0.00002693
Iteration 92/1000 | Loss: 0.00002693
Iteration 93/1000 | Loss: 0.00002693
Iteration 94/1000 | Loss: 0.00002693
Iteration 95/1000 | Loss: 0.00002693
Iteration 96/1000 | Loss: 0.00002693
Iteration 97/1000 | Loss: 0.00002693
Iteration 98/1000 | Loss: 0.00002693
Iteration 99/1000 | Loss: 0.00002693
Iteration 100/1000 | Loss: 0.00002693
Iteration 101/1000 | Loss: 0.00002693
Iteration 102/1000 | Loss: 0.00002693
Iteration 103/1000 | Loss: 0.00002693
Iteration 104/1000 | Loss: 0.00002693
Iteration 105/1000 | Loss: 0.00002693
Iteration 106/1000 | Loss: 0.00002693
Iteration 107/1000 | Loss: 0.00002693
Iteration 108/1000 | Loss: 0.00002693
Iteration 109/1000 | Loss: 0.00002693
Iteration 110/1000 | Loss: 0.00002693
Iteration 111/1000 | Loss: 0.00002693
Iteration 112/1000 | Loss: 0.00002693
Iteration 113/1000 | Loss: 0.00002693
Iteration 114/1000 | Loss: 0.00002693
Iteration 115/1000 | Loss: 0.00002693
Iteration 116/1000 | Loss: 0.00002693
Iteration 117/1000 | Loss: 0.00002693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.6928157240035944e-05, 2.6928157240035944e-05, 2.6928157240035944e-05, 2.6928157240035944e-05, 2.6928157240035944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6928157240035944e-05

Optimization complete. Final v2v error: 4.392238140106201 mm

Highest mean error: 5.767199516296387 mm for frame 74

Lowest mean error: 3.8352556228637695 mm for frame 128

Saving results

Total time: 37.61711406707764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882107
Iteration 2/25 | Loss: 0.00147390
Iteration 3/25 | Loss: 0.00098087
Iteration 4/25 | Loss: 0.00092769
Iteration 5/25 | Loss: 0.00091862
Iteration 6/25 | Loss: 0.00091602
Iteration 7/25 | Loss: 0.00091578
Iteration 8/25 | Loss: 0.00091578
Iteration 9/25 | Loss: 0.00091578
Iteration 10/25 | Loss: 0.00091578
Iteration 11/25 | Loss: 0.00091578
Iteration 12/25 | Loss: 0.00091578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009157803724519908, 0.0009157803724519908, 0.0009157803724519908, 0.0009157803724519908, 0.0009157803724519908]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009157803724519908

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73948896
Iteration 2/25 | Loss: 0.00081274
Iteration 3/25 | Loss: 0.00081271
Iteration 4/25 | Loss: 0.00081271
Iteration 5/25 | Loss: 0.00081271
Iteration 6/25 | Loss: 0.00081271
Iteration 7/25 | Loss: 0.00081271
Iteration 8/25 | Loss: 0.00081271
Iteration 9/25 | Loss: 0.00081271
Iteration 10/25 | Loss: 0.00081271
Iteration 11/25 | Loss: 0.00081271
Iteration 12/25 | Loss: 0.00081271
Iteration 13/25 | Loss: 0.00081271
Iteration 14/25 | Loss: 0.00081271
Iteration 15/25 | Loss: 0.00081271
Iteration 16/25 | Loss: 0.00081271
Iteration 17/25 | Loss: 0.00081271
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008127053733915091, 0.0008127053733915091, 0.0008127053733915091, 0.0008127053733915091, 0.0008127053733915091]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008127053733915091

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081271
Iteration 2/1000 | Loss: 0.00003333
Iteration 3/1000 | Loss: 0.00002663
Iteration 4/1000 | Loss: 0.00002321
Iteration 5/1000 | Loss: 0.00002201
Iteration 6/1000 | Loss: 0.00002151
Iteration 7/1000 | Loss: 0.00002115
Iteration 8/1000 | Loss: 0.00002079
Iteration 9/1000 | Loss: 0.00002044
Iteration 10/1000 | Loss: 0.00002025
Iteration 11/1000 | Loss: 0.00002004
Iteration 12/1000 | Loss: 0.00001986
Iteration 13/1000 | Loss: 0.00001978
Iteration 14/1000 | Loss: 0.00001975
Iteration 15/1000 | Loss: 0.00001970
Iteration 16/1000 | Loss: 0.00001967
Iteration 17/1000 | Loss: 0.00001967
Iteration 18/1000 | Loss: 0.00001966
Iteration 19/1000 | Loss: 0.00001966
Iteration 20/1000 | Loss: 0.00001965
Iteration 21/1000 | Loss: 0.00001965
Iteration 22/1000 | Loss: 0.00001964
Iteration 23/1000 | Loss: 0.00001964
Iteration 24/1000 | Loss: 0.00001964
Iteration 25/1000 | Loss: 0.00001962
Iteration 26/1000 | Loss: 0.00001962
Iteration 27/1000 | Loss: 0.00001962
Iteration 28/1000 | Loss: 0.00001961
Iteration 29/1000 | Loss: 0.00001961
Iteration 30/1000 | Loss: 0.00001961
Iteration 31/1000 | Loss: 0.00001960
Iteration 32/1000 | Loss: 0.00001960
Iteration 33/1000 | Loss: 0.00001960
Iteration 34/1000 | Loss: 0.00001959
Iteration 35/1000 | Loss: 0.00001958
Iteration 36/1000 | Loss: 0.00001958
Iteration 37/1000 | Loss: 0.00001957
Iteration 38/1000 | Loss: 0.00001957
Iteration 39/1000 | Loss: 0.00001956
Iteration 40/1000 | Loss: 0.00001956
Iteration 41/1000 | Loss: 0.00001955
Iteration 42/1000 | Loss: 0.00001955
Iteration 43/1000 | Loss: 0.00001955
Iteration 44/1000 | Loss: 0.00001955
Iteration 45/1000 | Loss: 0.00001955
Iteration 46/1000 | Loss: 0.00001954
Iteration 47/1000 | Loss: 0.00001954
Iteration 48/1000 | Loss: 0.00001954
Iteration 49/1000 | Loss: 0.00001954
Iteration 50/1000 | Loss: 0.00001953
Iteration 51/1000 | Loss: 0.00001953
Iteration 52/1000 | Loss: 0.00001953
Iteration 53/1000 | Loss: 0.00001953
Iteration 54/1000 | Loss: 0.00001953
Iteration 55/1000 | Loss: 0.00001952
Iteration 56/1000 | Loss: 0.00001952
Iteration 57/1000 | Loss: 0.00001952
Iteration 58/1000 | Loss: 0.00001952
Iteration 59/1000 | Loss: 0.00001952
Iteration 60/1000 | Loss: 0.00001952
Iteration 61/1000 | Loss: 0.00001951
Iteration 62/1000 | Loss: 0.00001951
Iteration 63/1000 | Loss: 0.00001951
Iteration 64/1000 | Loss: 0.00001951
Iteration 65/1000 | Loss: 0.00001951
Iteration 66/1000 | Loss: 0.00001951
Iteration 67/1000 | Loss: 0.00001951
Iteration 68/1000 | Loss: 0.00001951
Iteration 69/1000 | Loss: 0.00001951
Iteration 70/1000 | Loss: 0.00001951
Iteration 71/1000 | Loss: 0.00001951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.951162630575709e-05, 1.951162630575709e-05, 1.951162630575709e-05, 1.951162630575709e-05, 1.951162630575709e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.951162630575709e-05

Optimization complete. Final v2v error: 3.9128572940826416 mm

Highest mean error: 4.246675491333008 mm for frame 125

Lowest mean error: 3.3781073093414307 mm for frame 2

Saving results

Total time: 37.150060415267944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899037
Iteration 2/25 | Loss: 0.00147698
Iteration 3/25 | Loss: 0.00116928
Iteration 4/25 | Loss: 0.00112219
Iteration 5/25 | Loss: 0.00106366
Iteration 6/25 | Loss: 0.00101835
Iteration 7/25 | Loss: 0.00100556
Iteration 8/25 | Loss: 0.00100455
Iteration 9/25 | Loss: 0.00100438
Iteration 10/25 | Loss: 0.00100432
Iteration 11/25 | Loss: 0.00100432
Iteration 12/25 | Loss: 0.00100432
Iteration 13/25 | Loss: 0.00100432
Iteration 14/25 | Loss: 0.00100432
Iteration 15/25 | Loss: 0.00100432
Iteration 16/25 | Loss: 0.00100431
Iteration 17/25 | Loss: 0.00100431
Iteration 18/25 | Loss: 0.00100431
Iteration 19/25 | Loss: 0.00100431
Iteration 20/25 | Loss: 0.00100431
Iteration 21/25 | Loss: 0.00100431
Iteration 22/25 | Loss: 0.00100431
Iteration 23/25 | Loss: 0.00100431
Iteration 24/25 | Loss: 0.00100431
Iteration 25/25 | Loss: 0.00100431

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82881188
Iteration 2/25 | Loss: 0.00086322
Iteration 3/25 | Loss: 0.00086321
Iteration 4/25 | Loss: 0.00086321
Iteration 5/25 | Loss: 0.00086321
Iteration 6/25 | Loss: 0.00086321
Iteration 7/25 | Loss: 0.00086321
Iteration 8/25 | Loss: 0.00086321
Iteration 9/25 | Loss: 0.00086321
Iteration 10/25 | Loss: 0.00086321
Iteration 11/25 | Loss: 0.00086321
Iteration 12/25 | Loss: 0.00086321
Iteration 13/25 | Loss: 0.00086321
Iteration 14/25 | Loss: 0.00086321
Iteration 15/25 | Loss: 0.00086321
Iteration 16/25 | Loss: 0.00086321
Iteration 17/25 | Loss: 0.00086321
Iteration 18/25 | Loss: 0.00086321
Iteration 19/25 | Loss: 0.00086321
Iteration 20/25 | Loss: 0.00086321
Iteration 21/25 | Loss: 0.00086321
Iteration 22/25 | Loss: 0.00086321
Iteration 23/25 | Loss: 0.00086321
Iteration 24/25 | Loss: 0.00086321
Iteration 25/25 | Loss: 0.00086321

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086321
Iteration 2/1000 | Loss: 0.00005738
Iteration 3/1000 | Loss: 0.00004413
Iteration 4/1000 | Loss: 0.00003917
Iteration 5/1000 | Loss: 0.00003627
Iteration 6/1000 | Loss: 0.00003443
Iteration 7/1000 | Loss: 0.00003347
Iteration 8/1000 | Loss: 0.00003244
Iteration 9/1000 | Loss: 0.00003185
Iteration 10/1000 | Loss: 0.00003149
Iteration 11/1000 | Loss: 0.00003125
Iteration 12/1000 | Loss: 0.00003111
Iteration 13/1000 | Loss: 0.00003111
Iteration 14/1000 | Loss: 0.00003106
Iteration 15/1000 | Loss: 0.00003105
Iteration 16/1000 | Loss: 0.00003105
Iteration 17/1000 | Loss: 0.00003104
Iteration 18/1000 | Loss: 0.00003104
Iteration 19/1000 | Loss: 0.00003104
Iteration 20/1000 | Loss: 0.00003104
Iteration 21/1000 | Loss: 0.00003103
Iteration 22/1000 | Loss: 0.00003103
Iteration 23/1000 | Loss: 0.00003103
Iteration 24/1000 | Loss: 0.00003103
Iteration 25/1000 | Loss: 0.00003103
Iteration 26/1000 | Loss: 0.00003102
Iteration 27/1000 | Loss: 0.00003102
Iteration 28/1000 | Loss: 0.00003102
Iteration 29/1000 | Loss: 0.00003102
Iteration 30/1000 | Loss: 0.00003102
Iteration 31/1000 | Loss: 0.00003102
Iteration 32/1000 | Loss: 0.00003102
Iteration 33/1000 | Loss: 0.00003102
Iteration 34/1000 | Loss: 0.00003102
Iteration 35/1000 | Loss: 0.00003102
Iteration 36/1000 | Loss: 0.00003102
Iteration 37/1000 | Loss: 0.00003101
Iteration 38/1000 | Loss: 0.00003101
Iteration 39/1000 | Loss: 0.00003100
Iteration 40/1000 | Loss: 0.00003099
Iteration 41/1000 | Loss: 0.00003099
Iteration 42/1000 | Loss: 0.00003099
Iteration 43/1000 | Loss: 0.00003098
Iteration 44/1000 | Loss: 0.00003095
Iteration 45/1000 | Loss: 0.00003095
Iteration 46/1000 | Loss: 0.00003095
Iteration 47/1000 | Loss: 0.00003095
Iteration 48/1000 | Loss: 0.00003095
Iteration 49/1000 | Loss: 0.00003094
Iteration 50/1000 | Loss: 0.00003094
Iteration 51/1000 | Loss: 0.00003094
Iteration 52/1000 | Loss: 0.00003094
Iteration 53/1000 | Loss: 0.00003094
Iteration 54/1000 | Loss: 0.00003094
Iteration 55/1000 | Loss: 0.00003094
Iteration 56/1000 | Loss: 0.00003094
Iteration 57/1000 | Loss: 0.00003094
Iteration 58/1000 | Loss: 0.00003093
Iteration 59/1000 | Loss: 0.00003093
Iteration 60/1000 | Loss: 0.00003093
Iteration 61/1000 | Loss: 0.00003093
Iteration 62/1000 | Loss: 0.00003093
Iteration 63/1000 | Loss: 0.00003093
Iteration 64/1000 | Loss: 0.00003093
Iteration 65/1000 | Loss: 0.00003093
Iteration 66/1000 | Loss: 0.00003093
Iteration 67/1000 | Loss: 0.00003092
Iteration 68/1000 | Loss: 0.00003092
Iteration 69/1000 | Loss: 0.00003092
Iteration 70/1000 | Loss: 0.00003092
Iteration 71/1000 | Loss: 0.00003092
Iteration 72/1000 | Loss: 0.00003092
Iteration 73/1000 | Loss: 0.00003092
Iteration 74/1000 | Loss: 0.00003092
Iteration 75/1000 | Loss: 0.00003092
Iteration 76/1000 | Loss: 0.00003091
Iteration 77/1000 | Loss: 0.00003091
Iteration 78/1000 | Loss: 0.00003091
Iteration 79/1000 | Loss: 0.00003091
Iteration 80/1000 | Loss: 0.00003091
Iteration 81/1000 | Loss: 0.00003091
Iteration 82/1000 | Loss: 0.00003091
Iteration 83/1000 | Loss: 0.00003090
Iteration 84/1000 | Loss: 0.00003090
Iteration 85/1000 | Loss: 0.00003090
Iteration 86/1000 | Loss: 0.00003090
Iteration 87/1000 | Loss: 0.00003090
Iteration 88/1000 | Loss: 0.00003090
Iteration 89/1000 | Loss: 0.00003090
Iteration 90/1000 | Loss: 0.00003090
Iteration 91/1000 | Loss: 0.00003090
Iteration 92/1000 | Loss: 0.00003090
Iteration 93/1000 | Loss: 0.00003090
Iteration 94/1000 | Loss: 0.00003089
Iteration 95/1000 | Loss: 0.00003089
Iteration 96/1000 | Loss: 0.00003089
Iteration 97/1000 | Loss: 0.00003089
Iteration 98/1000 | Loss: 0.00003089
Iteration 99/1000 | Loss: 0.00003089
Iteration 100/1000 | Loss: 0.00003088
Iteration 101/1000 | Loss: 0.00003088
Iteration 102/1000 | Loss: 0.00003088
Iteration 103/1000 | Loss: 0.00003088
Iteration 104/1000 | Loss: 0.00003088
Iteration 105/1000 | Loss: 0.00003088
Iteration 106/1000 | Loss: 0.00003088
Iteration 107/1000 | Loss: 0.00003088
Iteration 108/1000 | Loss: 0.00003088
Iteration 109/1000 | Loss: 0.00003088
Iteration 110/1000 | Loss: 0.00003088
Iteration 111/1000 | Loss: 0.00003088
Iteration 112/1000 | Loss: 0.00003088
Iteration 113/1000 | Loss: 0.00003088
Iteration 114/1000 | Loss: 0.00003087
Iteration 115/1000 | Loss: 0.00003087
Iteration 116/1000 | Loss: 0.00003087
Iteration 117/1000 | Loss: 0.00003087
Iteration 118/1000 | Loss: 0.00003087
Iteration 119/1000 | Loss: 0.00003087
Iteration 120/1000 | Loss: 0.00003086
Iteration 121/1000 | Loss: 0.00003086
Iteration 122/1000 | Loss: 0.00003086
Iteration 123/1000 | Loss: 0.00003086
Iteration 124/1000 | Loss: 0.00003085
Iteration 125/1000 | Loss: 0.00003085
Iteration 126/1000 | Loss: 0.00003085
Iteration 127/1000 | Loss: 0.00003085
Iteration 128/1000 | Loss: 0.00003085
Iteration 129/1000 | Loss: 0.00003085
Iteration 130/1000 | Loss: 0.00003085
Iteration 131/1000 | Loss: 0.00003085
Iteration 132/1000 | Loss: 0.00003085
Iteration 133/1000 | Loss: 0.00003084
Iteration 134/1000 | Loss: 0.00003084
Iteration 135/1000 | Loss: 0.00003084
Iteration 136/1000 | Loss: 0.00003084
Iteration 137/1000 | Loss: 0.00003084
Iteration 138/1000 | Loss: 0.00003083
Iteration 139/1000 | Loss: 0.00003083
Iteration 140/1000 | Loss: 0.00003083
Iteration 141/1000 | Loss: 0.00003083
Iteration 142/1000 | Loss: 0.00003083
Iteration 143/1000 | Loss: 0.00003083
Iteration 144/1000 | Loss: 0.00003083
Iteration 145/1000 | Loss: 0.00003083
Iteration 146/1000 | Loss: 0.00003083
Iteration 147/1000 | Loss: 0.00003083
Iteration 148/1000 | Loss: 0.00003082
Iteration 149/1000 | Loss: 0.00003082
Iteration 150/1000 | Loss: 0.00003082
Iteration 151/1000 | Loss: 0.00003082
Iteration 152/1000 | Loss: 0.00003082
Iteration 153/1000 | Loss: 0.00003082
Iteration 154/1000 | Loss: 0.00003082
Iteration 155/1000 | Loss: 0.00003082
Iteration 156/1000 | Loss: 0.00003081
Iteration 157/1000 | Loss: 0.00003081
Iteration 158/1000 | Loss: 0.00003081
Iteration 159/1000 | Loss: 0.00003081
Iteration 160/1000 | Loss: 0.00003081
Iteration 161/1000 | Loss: 0.00003081
Iteration 162/1000 | Loss: 0.00003081
Iteration 163/1000 | Loss: 0.00003081
Iteration 164/1000 | Loss: 0.00003081
Iteration 165/1000 | Loss: 0.00003081
Iteration 166/1000 | Loss: 0.00003081
Iteration 167/1000 | Loss: 0.00003081
Iteration 168/1000 | Loss: 0.00003080
Iteration 169/1000 | Loss: 0.00003080
Iteration 170/1000 | Loss: 0.00003080
Iteration 171/1000 | Loss: 0.00003080
Iteration 172/1000 | Loss: 0.00003080
Iteration 173/1000 | Loss: 0.00003080
Iteration 174/1000 | Loss: 0.00003080
Iteration 175/1000 | Loss: 0.00003080
Iteration 176/1000 | Loss: 0.00003079
Iteration 177/1000 | Loss: 0.00003079
Iteration 178/1000 | Loss: 0.00003079
Iteration 179/1000 | Loss: 0.00003079
Iteration 180/1000 | Loss: 0.00003079
Iteration 181/1000 | Loss: 0.00003079
Iteration 182/1000 | Loss: 0.00003079
Iteration 183/1000 | Loss: 0.00003079
Iteration 184/1000 | Loss: 0.00003079
Iteration 185/1000 | Loss: 0.00003079
Iteration 186/1000 | Loss: 0.00003079
Iteration 187/1000 | Loss: 0.00003079
Iteration 188/1000 | Loss: 0.00003079
Iteration 189/1000 | Loss: 0.00003079
Iteration 190/1000 | Loss: 0.00003079
Iteration 191/1000 | Loss: 0.00003079
Iteration 192/1000 | Loss: 0.00003079
Iteration 193/1000 | Loss: 0.00003079
Iteration 194/1000 | Loss: 0.00003078
Iteration 195/1000 | Loss: 0.00003078
Iteration 196/1000 | Loss: 0.00003078
Iteration 197/1000 | Loss: 0.00003078
Iteration 198/1000 | Loss: 0.00003078
Iteration 199/1000 | Loss: 0.00003078
Iteration 200/1000 | Loss: 0.00003078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [3.078354347962886e-05, 3.078354347962886e-05, 3.078354347962886e-05, 3.078354347962886e-05, 3.078354347962886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.078354347962886e-05

Optimization complete. Final v2v error: 4.7118988037109375 mm

Highest mean error: 4.996368408203125 mm for frame 75

Lowest mean error: 4.414177417755127 mm for frame 37

Saving results

Total time: 46.20897650718689
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386194
Iteration 2/25 | Loss: 0.00107850
Iteration 3/25 | Loss: 0.00092016
Iteration 4/25 | Loss: 0.00090469
Iteration 5/25 | Loss: 0.00089861
Iteration 6/25 | Loss: 0.00089549
Iteration 7/25 | Loss: 0.00089497
Iteration 8/25 | Loss: 0.00089497
Iteration 9/25 | Loss: 0.00089497
Iteration 10/25 | Loss: 0.00089497
Iteration 11/25 | Loss: 0.00089497
Iteration 12/25 | Loss: 0.00089497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000894965894985944, 0.000894965894985944, 0.000894965894985944, 0.000894965894985944, 0.000894965894985944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000894965894985944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38218236
Iteration 2/25 | Loss: 0.00108608
Iteration 3/25 | Loss: 0.00108608
Iteration 4/25 | Loss: 0.00108608
Iteration 5/25 | Loss: 0.00108608
Iteration 6/25 | Loss: 0.00108608
Iteration 7/25 | Loss: 0.00108607
Iteration 8/25 | Loss: 0.00108607
Iteration 9/25 | Loss: 0.00108607
Iteration 10/25 | Loss: 0.00108607
Iteration 11/25 | Loss: 0.00108607
Iteration 12/25 | Loss: 0.00108607
Iteration 13/25 | Loss: 0.00108607
Iteration 14/25 | Loss: 0.00108607
Iteration 15/25 | Loss: 0.00108607
Iteration 16/25 | Loss: 0.00108607
Iteration 17/25 | Loss: 0.00108607
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001086074160411954, 0.001086074160411954, 0.001086074160411954, 0.001086074160411954, 0.001086074160411954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001086074160411954

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108607
Iteration 2/1000 | Loss: 0.00003114
Iteration 3/1000 | Loss: 0.00002250
Iteration 4/1000 | Loss: 0.00002058
Iteration 5/1000 | Loss: 0.00001958
Iteration 6/1000 | Loss: 0.00001895
Iteration 7/1000 | Loss: 0.00001846
Iteration 8/1000 | Loss: 0.00001821
Iteration 9/1000 | Loss: 0.00001796
Iteration 10/1000 | Loss: 0.00001774
Iteration 11/1000 | Loss: 0.00001755
Iteration 12/1000 | Loss: 0.00001751
Iteration 13/1000 | Loss: 0.00001741
Iteration 14/1000 | Loss: 0.00001736
Iteration 15/1000 | Loss: 0.00001734
Iteration 16/1000 | Loss: 0.00001734
Iteration 17/1000 | Loss: 0.00001734
Iteration 18/1000 | Loss: 0.00001733
Iteration 19/1000 | Loss: 0.00001729
Iteration 20/1000 | Loss: 0.00001728
Iteration 21/1000 | Loss: 0.00001728
Iteration 22/1000 | Loss: 0.00001727
Iteration 23/1000 | Loss: 0.00001726
Iteration 24/1000 | Loss: 0.00001724
Iteration 25/1000 | Loss: 0.00001724
Iteration 26/1000 | Loss: 0.00001723
Iteration 27/1000 | Loss: 0.00001723
Iteration 28/1000 | Loss: 0.00001723
Iteration 29/1000 | Loss: 0.00001723
Iteration 30/1000 | Loss: 0.00001723
Iteration 31/1000 | Loss: 0.00001723
Iteration 32/1000 | Loss: 0.00001723
Iteration 33/1000 | Loss: 0.00001723
Iteration 34/1000 | Loss: 0.00001723
Iteration 35/1000 | Loss: 0.00001722
Iteration 36/1000 | Loss: 0.00001722
Iteration 37/1000 | Loss: 0.00001721
Iteration 38/1000 | Loss: 0.00001720
Iteration 39/1000 | Loss: 0.00001720
Iteration 40/1000 | Loss: 0.00001720
Iteration 41/1000 | Loss: 0.00001720
Iteration 42/1000 | Loss: 0.00001720
Iteration 43/1000 | Loss: 0.00001720
Iteration 44/1000 | Loss: 0.00001720
Iteration 45/1000 | Loss: 0.00001719
Iteration 46/1000 | Loss: 0.00001719
Iteration 47/1000 | Loss: 0.00001719
Iteration 48/1000 | Loss: 0.00001719
Iteration 49/1000 | Loss: 0.00001718
Iteration 50/1000 | Loss: 0.00001718
Iteration 51/1000 | Loss: 0.00001717
Iteration 52/1000 | Loss: 0.00001717
Iteration 53/1000 | Loss: 0.00001716
Iteration 54/1000 | Loss: 0.00001716
Iteration 55/1000 | Loss: 0.00001716
Iteration 56/1000 | Loss: 0.00001716
Iteration 57/1000 | Loss: 0.00001716
Iteration 58/1000 | Loss: 0.00001716
Iteration 59/1000 | Loss: 0.00001716
Iteration 60/1000 | Loss: 0.00001715
Iteration 61/1000 | Loss: 0.00001715
Iteration 62/1000 | Loss: 0.00001715
Iteration 63/1000 | Loss: 0.00001714
Iteration 64/1000 | Loss: 0.00001714
Iteration 65/1000 | Loss: 0.00001714
Iteration 66/1000 | Loss: 0.00001713
Iteration 67/1000 | Loss: 0.00001713
Iteration 68/1000 | Loss: 0.00001713
Iteration 69/1000 | Loss: 0.00001712
Iteration 70/1000 | Loss: 0.00001712
Iteration 71/1000 | Loss: 0.00001712
Iteration 72/1000 | Loss: 0.00001712
Iteration 73/1000 | Loss: 0.00001711
Iteration 74/1000 | Loss: 0.00001711
Iteration 75/1000 | Loss: 0.00001710
Iteration 76/1000 | Loss: 0.00001710
Iteration 77/1000 | Loss: 0.00001710
Iteration 78/1000 | Loss: 0.00001709
Iteration 79/1000 | Loss: 0.00001709
Iteration 80/1000 | Loss: 0.00001709
Iteration 81/1000 | Loss: 0.00001709
Iteration 82/1000 | Loss: 0.00001709
Iteration 83/1000 | Loss: 0.00001709
Iteration 84/1000 | Loss: 0.00001709
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001709
Iteration 87/1000 | Loss: 0.00001709
Iteration 88/1000 | Loss: 0.00001708
Iteration 89/1000 | Loss: 0.00001708
Iteration 90/1000 | Loss: 0.00001708
Iteration 91/1000 | Loss: 0.00001708
Iteration 92/1000 | Loss: 0.00001708
Iteration 93/1000 | Loss: 0.00001708
Iteration 94/1000 | Loss: 0.00001708
Iteration 95/1000 | Loss: 0.00001708
Iteration 96/1000 | Loss: 0.00001708
Iteration 97/1000 | Loss: 0.00001708
Iteration 98/1000 | Loss: 0.00001708
Iteration 99/1000 | Loss: 0.00001707
Iteration 100/1000 | Loss: 0.00001707
Iteration 101/1000 | Loss: 0.00001707
Iteration 102/1000 | Loss: 0.00001707
Iteration 103/1000 | Loss: 0.00001707
Iteration 104/1000 | Loss: 0.00001707
Iteration 105/1000 | Loss: 0.00001707
Iteration 106/1000 | Loss: 0.00001707
Iteration 107/1000 | Loss: 0.00001706
Iteration 108/1000 | Loss: 0.00001706
Iteration 109/1000 | Loss: 0.00001706
Iteration 110/1000 | Loss: 0.00001706
Iteration 111/1000 | Loss: 0.00001706
Iteration 112/1000 | Loss: 0.00001706
Iteration 113/1000 | Loss: 0.00001706
Iteration 114/1000 | Loss: 0.00001706
Iteration 115/1000 | Loss: 0.00001705
Iteration 116/1000 | Loss: 0.00001705
Iteration 117/1000 | Loss: 0.00001705
Iteration 118/1000 | Loss: 0.00001705
Iteration 119/1000 | Loss: 0.00001705
Iteration 120/1000 | Loss: 0.00001705
Iteration 121/1000 | Loss: 0.00001705
Iteration 122/1000 | Loss: 0.00001705
Iteration 123/1000 | Loss: 0.00001705
Iteration 124/1000 | Loss: 0.00001705
Iteration 125/1000 | Loss: 0.00001705
Iteration 126/1000 | Loss: 0.00001705
Iteration 127/1000 | Loss: 0.00001705
Iteration 128/1000 | Loss: 0.00001705
Iteration 129/1000 | Loss: 0.00001704
Iteration 130/1000 | Loss: 0.00001704
Iteration 131/1000 | Loss: 0.00001704
Iteration 132/1000 | Loss: 0.00001704
Iteration 133/1000 | Loss: 0.00001704
Iteration 134/1000 | Loss: 0.00001704
Iteration 135/1000 | Loss: 0.00001704
Iteration 136/1000 | Loss: 0.00001704
Iteration 137/1000 | Loss: 0.00001704
Iteration 138/1000 | Loss: 0.00001704
Iteration 139/1000 | Loss: 0.00001704
Iteration 140/1000 | Loss: 0.00001704
Iteration 141/1000 | Loss: 0.00001704
Iteration 142/1000 | Loss: 0.00001704
Iteration 143/1000 | Loss: 0.00001704
Iteration 144/1000 | Loss: 0.00001704
Iteration 145/1000 | Loss: 0.00001704
Iteration 146/1000 | Loss: 0.00001704
Iteration 147/1000 | Loss: 0.00001703
Iteration 148/1000 | Loss: 0.00001703
Iteration 149/1000 | Loss: 0.00001703
Iteration 150/1000 | Loss: 0.00001703
Iteration 151/1000 | Loss: 0.00001703
Iteration 152/1000 | Loss: 0.00001703
Iteration 153/1000 | Loss: 0.00001703
Iteration 154/1000 | Loss: 0.00001703
Iteration 155/1000 | Loss: 0.00001703
Iteration 156/1000 | Loss: 0.00001703
Iteration 157/1000 | Loss: 0.00001703
Iteration 158/1000 | Loss: 0.00001703
Iteration 159/1000 | Loss: 0.00001703
Iteration 160/1000 | Loss: 0.00001703
Iteration 161/1000 | Loss: 0.00001703
Iteration 162/1000 | Loss: 0.00001703
Iteration 163/1000 | Loss: 0.00001703
Iteration 164/1000 | Loss: 0.00001703
Iteration 165/1000 | Loss: 0.00001703
Iteration 166/1000 | Loss: 0.00001703
Iteration 167/1000 | Loss: 0.00001703
Iteration 168/1000 | Loss: 0.00001703
Iteration 169/1000 | Loss: 0.00001703
Iteration 170/1000 | Loss: 0.00001703
Iteration 171/1000 | Loss: 0.00001703
Iteration 172/1000 | Loss: 0.00001703
Iteration 173/1000 | Loss: 0.00001703
Iteration 174/1000 | Loss: 0.00001703
Iteration 175/1000 | Loss: 0.00001703
Iteration 176/1000 | Loss: 0.00001703
Iteration 177/1000 | Loss: 0.00001703
Iteration 178/1000 | Loss: 0.00001703
Iteration 179/1000 | Loss: 0.00001703
Iteration 180/1000 | Loss: 0.00001703
Iteration 181/1000 | Loss: 0.00001703
Iteration 182/1000 | Loss: 0.00001703
Iteration 183/1000 | Loss: 0.00001703
Iteration 184/1000 | Loss: 0.00001703
Iteration 185/1000 | Loss: 0.00001703
Iteration 186/1000 | Loss: 0.00001703
Iteration 187/1000 | Loss: 0.00001703
Iteration 188/1000 | Loss: 0.00001703
Iteration 189/1000 | Loss: 0.00001703
Iteration 190/1000 | Loss: 0.00001703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.702763620414771e-05, 1.702763620414771e-05, 1.702763620414771e-05, 1.702763620414771e-05, 1.702763620414771e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.702763620414771e-05

Optimization complete. Final v2v error: 3.596395969390869 mm

Highest mean error: 4.140026092529297 mm for frame 81

Lowest mean error: 3.177528142929077 mm for frame 9

Saving results

Total time: 43.597694396972656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903998
Iteration 2/25 | Loss: 0.00117182
Iteration 3/25 | Loss: 0.00097032
Iteration 4/25 | Loss: 0.00095355
Iteration 5/25 | Loss: 0.00094671
Iteration 6/25 | Loss: 0.00094475
Iteration 7/25 | Loss: 0.00094475
Iteration 8/25 | Loss: 0.00094475
Iteration 9/25 | Loss: 0.00094475
Iteration 10/25 | Loss: 0.00094475
Iteration 11/25 | Loss: 0.00094475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009447474149055779, 0.0009447474149055779, 0.0009447474149055779, 0.0009447474149055779, 0.0009447474149055779]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009447474149055779

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25207293
Iteration 2/25 | Loss: 0.00101126
Iteration 3/25 | Loss: 0.00101126
Iteration 4/25 | Loss: 0.00101126
Iteration 5/25 | Loss: 0.00101126
Iteration 6/25 | Loss: 0.00101126
Iteration 7/25 | Loss: 0.00101126
Iteration 8/25 | Loss: 0.00101126
Iteration 9/25 | Loss: 0.00101126
Iteration 10/25 | Loss: 0.00101126
Iteration 11/25 | Loss: 0.00101126
Iteration 12/25 | Loss: 0.00101126
Iteration 13/25 | Loss: 0.00101126
Iteration 14/25 | Loss: 0.00101126
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010112583404406905, 0.0010112583404406905, 0.0010112583404406905, 0.0010112583404406905, 0.0010112583404406905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010112583404406905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101126
Iteration 2/1000 | Loss: 0.00004292
Iteration 3/1000 | Loss: 0.00003114
Iteration 4/1000 | Loss: 0.00002869
Iteration 5/1000 | Loss: 0.00002708
Iteration 6/1000 | Loss: 0.00002601
Iteration 7/1000 | Loss: 0.00002502
Iteration 8/1000 | Loss: 0.00002442
Iteration 9/1000 | Loss: 0.00002408
Iteration 10/1000 | Loss: 0.00002390
Iteration 11/1000 | Loss: 0.00002371
Iteration 12/1000 | Loss: 0.00002366
Iteration 13/1000 | Loss: 0.00002360
Iteration 14/1000 | Loss: 0.00002359
Iteration 15/1000 | Loss: 0.00002358
Iteration 16/1000 | Loss: 0.00002357
Iteration 17/1000 | Loss: 0.00002356
Iteration 18/1000 | Loss: 0.00002356
Iteration 19/1000 | Loss: 0.00002356
Iteration 20/1000 | Loss: 0.00002355
Iteration 21/1000 | Loss: 0.00002355
Iteration 22/1000 | Loss: 0.00002355
Iteration 23/1000 | Loss: 0.00002354
Iteration 24/1000 | Loss: 0.00002354
Iteration 25/1000 | Loss: 0.00002354
Iteration 26/1000 | Loss: 0.00002353
Iteration 27/1000 | Loss: 0.00002352
Iteration 28/1000 | Loss: 0.00002352
Iteration 29/1000 | Loss: 0.00002352
Iteration 30/1000 | Loss: 0.00002352
Iteration 31/1000 | Loss: 0.00002352
Iteration 32/1000 | Loss: 0.00002352
Iteration 33/1000 | Loss: 0.00002352
Iteration 34/1000 | Loss: 0.00002351
Iteration 35/1000 | Loss: 0.00002351
Iteration 36/1000 | Loss: 0.00002351
Iteration 37/1000 | Loss: 0.00002351
Iteration 38/1000 | Loss: 0.00002350
Iteration 39/1000 | Loss: 0.00002350
Iteration 40/1000 | Loss: 0.00002349
Iteration 41/1000 | Loss: 0.00002349
Iteration 42/1000 | Loss: 0.00002348
Iteration 43/1000 | Loss: 0.00002348
Iteration 44/1000 | Loss: 0.00002348
Iteration 45/1000 | Loss: 0.00002348
Iteration 46/1000 | Loss: 0.00002347
Iteration 47/1000 | Loss: 0.00002347
Iteration 48/1000 | Loss: 0.00002347
Iteration 49/1000 | Loss: 0.00002347
Iteration 50/1000 | Loss: 0.00002347
Iteration 51/1000 | Loss: 0.00002347
Iteration 52/1000 | Loss: 0.00002347
Iteration 53/1000 | Loss: 0.00002347
Iteration 54/1000 | Loss: 0.00002347
Iteration 55/1000 | Loss: 0.00002347
Iteration 56/1000 | Loss: 0.00002346
Iteration 57/1000 | Loss: 0.00002346
Iteration 58/1000 | Loss: 0.00002346
Iteration 59/1000 | Loss: 0.00002346
Iteration 60/1000 | Loss: 0.00002346
Iteration 61/1000 | Loss: 0.00002345
Iteration 62/1000 | Loss: 0.00002345
Iteration 63/1000 | Loss: 0.00002345
Iteration 64/1000 | Loss: 0.00002345
Iteration 65/1000 | Loss: 0.00002345
Iteration 66/1000 | Loss: 0.00002345
Iteration 67/1000 | Loss: 0.00002345
Iteration 68/1000 | Loss: 0.00002344
Iteration 69/1000 | Loss: 0.00002344
Iteration 70/1000 | Loss: 0.00002344
Iteration 71/1000 | Loss: 0.00002344
Iteration 72/1000 | Loss: 0.00002344
Iteration 73/1000 | Loss: 0.00002344
Iteration 74/1000 | Loss: 0.00002343
Iteration 75/1000 | Loss: 0.00002343
Iteration 76/1000 | Loss: 0.00002343
Iteration 77/1000 | Loss: 0.00002343
Iteration 78/1000 | Loss: 0.00002343
Iteration 79/1000 | Loss: 0.00002343
Iteration 80/1000 | Loss: 0.00002342
Iteration 81/1000 | Loss: 0.00002342
Iteration 82/1000 | Loss: 0.00002342
Iteration 83/1000 | Loss: 0.00002342
Iteration 84/1000 | Loss: 0.00002342
Iteration 85/1000 | Loss: 0.00002342
Iteration 86/1000 | Loss: 0.00002341
Iteration 87/1000 | Loss: 0.00002341
Iteration 88/1000 | Loss: 0.00002341
Iteration 89/1000 | Loss: 0.00002341
Iteration 90/1000 | Loss: 0.00002341
Iteration 91/1000 | Loss: 0.00002341
Iteration 92/1000 | Loss: 0.00002341
Iteration 93/1000 | Loss: 0.00002341
Iteration 94/1000 | Loss: 0.00002341
Iteration 95/1000 | Loss: 0.00002341
Iteration 96/1000 | Loss: 0.00002341
Iteration 97/1000 | Loss: 0.00002341
Iteration 98/1000 | Loss: 0.00002341
Iteration 99/1000 | Loss: 0.00002340
Iteration 100/1000 | Loss: 0.00002340
Iteration 101/1000 | Loss: 0.00002340
Iteration 102/1000 | Loss: 0.00002340
Iteration 103/1000 | Loss: 0.00002340
Iteration 104/1000 | Loss: 0.00002340
Iteration 105/1000 | Loss: 0.00002340
Iteration 106/1000 | Loss: 0.00002339
Iteration 107/1000 | Loss: 0.00002339
Iteration 108/1000 | Loss: 0.00002339
Iteration 109/1000 | Loss: 0.00002339
Iteration 110/1000 | Loss: 0.00002339
Iteration 111/1000 | Loss: 0.00002339
Iteration 112/1000 | Loss: 0.00002339
Iteration 113/1000 | Loss: 0.00002339
Iteration 114/1000 | Loss: 0.00002338
Iteration 115/1000 | Loss: 0.00002338
Iteration 116/1000 | Loss: 0.00002338
Iteration 117/1000 | Loss: 0.00002338
Iteration 118/1000 | Loss: 0.00002338
Iteration 119/1000 | Loss: 0.00002338
Iteration 120/1000 | Loss: 0.00002338
Iteration 121/1000 | Loss: 0.00002338
Iteration 122/1000 | Loss: 0.00002338
Iteration 123/1000 | Loss: 0.00002338
Iteration 124/1000 | Loss: 0.00002338
Iteration 125/1000 | Loss: 0.00002338
Iteration 126/1000 | Loss: 0.00002338
Iteration 127/1000 | Loss: 0.00002337
Iteration 128/1000 | Loss: 0.00002337
Iteration 129/1000 | Loss: 0.00002337
Iteration 130/1000 | Loss: 0.00002337
Iteration 131/1000 | Loss: 0.00002337
Iteration 132/1000 | Loss: 0.00002337
Iteration 133/1000 | Loss: 0.00002337
Iteration 134/1000 | Loss: 0.00002337
Iteration 135/1000 | Loss: 0.00002337
Iteration 136/1000 | Loss: 0.00002337
Iteration 137/1000 | Loss: 0.00002337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.3369002519757487e-05, 2.3369002519757487e-05, 2.3369002519757487e-05, 2.3369002519757487e-05, 2.3369002519757487e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3369002519757487e-05

Optimization complete. Final v2v error: 4.074094295501709 mm

Highest mean error: 4.835063934326172 mm for frame 9

Lowest mean error: 3.661860466003418 mm for frame 208

Saving results

Total time: 38.600175619125366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01134118
Iteration 2/25 | Loss: 0.01134118
Iteration 3/25 | Loss: 0.01134118
Iteration 4/25 | Loss: 0.01134117
Iteration 5/25 | Loss: 0.01134117
Iteration 6/25 | Loss: 0.01134117
Iteration 7/25 | Loss: 0.01134117
Iteration 8/25 | Loss: 0.01134117
Iteration 9/25 | Loss: 0.01134117
Iteration 10/25 | Loss: 0.01134117
Iteration 11/25 | Loss: 0.01134117
Iteration 12/25 | Loss: 0.01134116
Iteration 13/25 | Loss: 0.01134116
Iteration 14/25 | Loss: 0.01134116
Iteration 15/25 | Loss: 0.01134116
Iteration 16/25 | Loss: 0.01134116
Iteration 17/25 | Loss: 0.01134116
Iteration 18/25 | Loss: 0.01134116
Iteration 19/25 | Loss: 0.01134115
Iteration 20/25 | Loss: 0.01134115
Iteration 21/25 | Loss: 0.01134115
Iteration 22/25 | Loss: 0.01134115
Iteration 23/25 | Loss: 0.01134115
Iteration 24/25 | Loss: 0.01134115
Iteration 25/25 | Loss: 0.01134115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58954120
Iteration 2/25 | Loss: 0.09370091
Iteration 3/25 | Loss: 0.07526042
Iteration 4/25 | Loss: 0.07427885
Iteration 5/25 | Loss: 0.07426299
Iteration 6/25 | Loss: 0.07426299
Iteration 7/25 | Loss: 0.07426298
Iteration 8/25 | Loss: 0.07426298
Iteration 9/25 | Loss: 0.07426298
Iteration 10/25 | Loss: 0.07426297
Iteration 11/25 | Loss: 0.07426297
Iteration 12/25 | Loss: 0.07426297
Iteration 13/25 | Loss: 0.07426297
Iteration 14/25 | Loss: 0.07426297
Iteration 15/25 | Loss: 0.07426297
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.07426296919584274, 0.07426296919584274, 0.07426296919584274, 0.07426296919584274, 0.07426296919584274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07426296919584274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07426297
Iteration 2/1000 | Loss: 0.00236428
Iteration 3/1000 | Loss: 0.00047309
Iteration 4/1000 | Loss: 0.00080058
Iteration 5/1000 | Loss: 0.00025480
Iteration 6/1000 | Loss: 0.00052953
Iteration 7/1000 | Loss: 0.00009599
Iteration 8/1000 | Loss: 0.00008449
Iteration 9/1000 | Loss: 0.00047999
Iteration 10/1000 | Loss: 0.00011044
Iteration 11/1000 | Loss: 0.00011779
Iteration 12/1000 | Loss: 0.00012674
Iteration 13/1000 | Loss: 0.00005866
Iteration 14/1000 | Loss: 0.00003767
Iteration 15/1000 | Loss: 0.00025634
Iteration 16/1000 | Loss: 0.00005358
Iteration 17/1000 | Loss: 0.00004195
Iteration 18/1000 | Loss: 0.00003066
Iteration 19/1000 | Loss: 0.00009497
Iteration 20/1000 | Loss: 0.00002867
Iteration 21/1000 | Loss: 0.00005441
Iteration 22/1000 | Loss: 0.00029199
Iteration 23/1000 | Loss: 0.00004806
Iteration 24/1000 | Loss: 0.00003523
Iteration 25/1000 | Loss: 0.00007241
Iteration 26/1000 | Loss: 0.00002594
Iteration 27/1000 | Loss: 0.00002527
Iteration 28/1000 | Loss: 0.00002456
Iteration 29/1000 | Loss: 0.00002396
Iteration 30/1000 | Loss: 0.00009703
Iteration 31/1000 | Loss: 0.00002336
Iteration 32/1000 | Loss: 0.00002294
Iteration 33/1000 | Loss: 0.00002263
Iteration 34/1000 | Loss: 0.00002233
Iteration 35/1000 | Loss: 0.00002205
Iteration 36/1000 | Loss: 0.00002188
Iteration 37/1000 | Loss: 0.00002177
Iteration 38/1000 | Loss: 0.00002169
Iteration 39/1000 | Loss: 0.00002161
Iteration 40/1000 | Loss: 0.00002161
Iteration 41/1000 | Loss: 0.00002158
Iteration 42/1000 | Loss: 0.00002158
Iteration 43/1000 | Loss: 0.00002157
Iteration 44/1000 | Loss: 0.00002156
Iteration 45/1000 | Loss: 0.00002156
Iteration 46/1000 | Loss: 0.00002156
Iteration 47/1000 | Loss: 0.00002155
Iteration 48/1000 | Loss: 0.00002155
Iteration 49/1000 | Loss: 0.00002155
Iteration 50/1000 | Loss: 0.00002154
Iteration 51/1000 | Loss: 0.00002154
Iteration 52/1000 | Loss: 0.00002154
Iteration 53/1000 | Loss: 0.00002153
Iteration 54/1000 | Loss: 0.00002153
Iteration 55/1000 | Loss: 0.00002153
Iteration 56/1000 | Loss: 0.00002153
Iteration 57/1000 | Loss: 0.00002153
Iteration 58/1000 | Loss: 0.00002153
Iteration 59/1000 | Loss: 0.00002153
Iteration 60/1000 | Loss: 0.00002153
Iteration 61/1000 | Loss: 0.00002153
Iteration 62/1000 | Loss: 0.00002153
Iteration 63/1000 | Loss: 0.00002153
Iteration 64/1000 | Loss: 0.00002153
Iteration 65/1000 | Loss: 0.00002153
Iteration 66/1000 | Loss: 0.00002153
Iteration 67/1000 | Loss: 0.00002152
Iteration 68/1000 | Loss: 0.00007289
Iteration 69/1000 | Loss: 0.00002155
Iteration 70/1000 | Loss: 0.00002222
Iteration 71/1000 | Loss: 0.00002153
Iteration 72/1000 | Loss: 0.00002158
Iteration 73/1000 | Loss: 0.00002157
Iteration 74/1000 | Loss: 0.00002145
Iteration 75/1000 | Loss: 0.00002145
Iteration 76/1000 | Loss: 0.00002145
Iteration 77/1000 | Loss: 0.00002145
Iteration 78/1000 | Loss: 0.00002145
Iteration 79/1000 | Loss: 0.00002145
Iteration 80/1000 | Loss: 0.00002145
Iteration 81/1000 | Loss: 0.00002145
Iteration 82/1000 | Loss: 0.00002145
Iteration 83/1000 | Loss: 0.00002145
Iteration 84/1000 | Loss: 0.00002145
Iteration 85/1000 | Loss: 0.00002144
Iteration 86/1000 | Loss: 0.00002144
Iteration 87/1000 | Loss: 0.00002144
Iteration 88/1000 | Loss: 0.00002144
Iteration 89/1000 | Loss: 0.00002144
Iteration 90/1000 | Loss: 0.00002144
Iteration 91/1000 | Loss: 0.00002144
Iteration 92/1000 | Loss: 0.00002144
Iteration 93/1000 | Loss: 0.00002143
Iteration 94/1000 | Loss: 0.00002143
Iteration 95/1000 | Loss: 0.00002143
Iteration 96/1000 | Loss: 0.00002143
Iteration 97/1000 | Loss: 0.00002143
Iteration 98/1000 | Loss: 0.00002143
Iteration 99/1000 | Loss: 0.00002143
Iteration 100/1000 | Loss: 0.00002143
Iteration 101/1000 | Loss: 0.00002143
Iteration 102/1000 | Loss: 0.00002143
Iteration 103/1000 | Loss: 0.00002143
Iteration 104/1000 | Loss: 0.00002143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [2.1432682842714712e-05, 2.1432682842714712e-05, 2.1432682842714712e-05, 2.1432682842714712e-05, 2.1432682842714712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1432682842714712e-05

Optimization complete. Final v2v error: 3.929969310760498 mm

Highest mean error: 4.303086757659912 mm for frame 142

Lowest mean error: 3.701967716217041 mm for frame 104

Saving results

Total time: 70.49006962776184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00629606
Iteration 2/25 | Loss: 0.00116816
Iteration 3/25 | Loss: 0.00105974
Iteration 4/25 | Loss: 0.00101658
Iteration 5/25 | Loss: 0.00100323
Iteration 6/25 | Loss: 0.00100005
Iteration 7/25 | Loss: 0.00099963
Iteration 8/25 | Loss: 0.00099963
Iteration 9/25 | Loss: 0.00099963
Iteration 10/25 | Loss: 0.00099963
Iteration 11/25 | Loss: 0.00099963
Iteration 12/25 | Loss: 0.00099963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009996291482821107, 0.0009996291482821107, 0.0009996291482821107, 0.0009996291482821107, 0.0009996291482821107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009996291482821107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39557052
Iteration 2/25 | Loss: 0.00142708
Iteration 3/25 | Loss: 0.00142708
Iteration 4/25 | Loss: 0.00142708
Iteration 5/25 | Loss: 0.00142708
Iteration 6/25 | Loss: 0.00142708
Iteration 7/25 | Loss: 0.00142708
Iteration 8/25 | Loss: 0.00142708
Iteration 9/25 | Loss: 0.00142708
Iteration 10/25 | Loss: 0.00142708
Iteration 11/25 | Loss: 0.00142708
Iteration 12/25 | Loss: 0.00142708
Iteration 13/25 | Loss: 0.00142708
Iteration 14/25 | Loss: 0.00142708
Iteration 15/25 | Loss: 0.00142708
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014270782703533769, 0.0014270782703533769, 0.0014270782703533769, 0.0014270782703533769, 0.0014270782703533769]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014270782703533769

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142708
Iteration 2/1000 | Loss: 0.00007013
Iteration 3/1000 | Loss: 0.00003947
Iteration 4/1000 | Loss: 0.00002975
Iteration 5/1000 | Loss: 0.00002642
Iteration 6/1000 | Loss: 0.00002452
Iteration 7/1000 | Loss: 0.00002377
Iteration 8/1000 | Loss: 0.00002275
Iteration 9/1000 | Loss: 0.00002197
Iteration 10/1000 | Loss: 0.00002160
Iteration 11/1000 | Loss: 0.00002123
Iteration 12/1000 | Loss: 0.00002104
Iteration 13/1000 | Loss: 0.00002103
Iteration 14/1000 | Loss: 0.00002093
Iteration 15/1000 | Loss: 0.00002075
Iteration 16/1000 | Loss: 0.00002074
Iteration 17/1000 | Loss: 0.00002073
Iteration 18/1000 | Loss: 0.00002069
Iteration 19/1000 | Loss: 0.00002066
Iteration 20/1000 | Loss: 0.00002064
Iteration 21/1000 | Loss: 0.00002060
Iteration 22/1000 | Loss: 0.00002059
Iteration 23/1000 | Loss: 0.00002059
Iteration 24/1000 | Loss: 0.00002058
Iteration 25/1000 | Loss: 0.00002058
Iteration 26/1000 | Loss: 0.00002058
Iteration 27/1000 | Loss: 0.00002058
Iteration 28/1000 | Loss: 0.00002058
Iteration 29/1000 | Loss: 0.00002058
Iteration 30/1000 | Loss: 0.00002057
Iteration 31/1000 | Loss: 0.00002057
Iteration 32/1000 | Loss: 0.00002057
Iteration 33/1000 | Loss: 0.00002056
Iteration 34/1000 | Loss: 0.00002055
Iteration 35/1000 | Loss: 0.00002055
Iteration 36/1000 | Loss: 0.00002055
Iteration 37/1000 | Loss: 0.00002055
Iteration 38/1000 | Loss: 0.00002054
Iteration 39/1000 | Loss: 0.00002054
Iteration 40/1000 | Loss: 0.00002053
Iteration 41/1000 | Loss: 0.00002052
Iteration 42/1000 | Loss: 0.00002052
Iteration 43/1000 | Loss: 0.00002052
Iteration 44/1000 | Loss: 0.00002051
Iteration 45/1000 | Loss: 0.00002051
Iteration 46/1000 | Loss: 0.00002051
Iteration 47/1000 | Loss: 0.00002050
Iteration 48/1000 | Loss: 0.00002050
Iteration 49/1000 | Loss: 0.00002050
Iteration 50/1000 | Loss: 0.00002050
Iteration 51/1000 | Loss: 0.00002049
Iteration 52/1000 | Loss: 0.00002049
Iteration 53/1000 | Loss: 0.00002049
Iteration 54/1000 | Loss: 0.00002048
Iteration 55/1000 | Loss: 0.00002048
Iteration 56/1000 | Loss: 0.00002048
Iteration 57/1000 | Loss: 0.00002048
Iteration 58/1000 | Loss: 0.00002048
Iteration 59/1000 | Loss: 0.00002047
Iteration 60/1000 | Loss: 0.00002047
Iteration 61/1000 | Loss: 0.00002047
Iteration 62/1000 | Loss: 0.00002047
Iteration 63/1000 | Loss: 0.00002047
Iteration 64/1000 | Loss: 0.00002047
Iteration 65/1000 | Loss: 0.00002047
Iteration 66/1000 | Loss: 0.00002047
Iteration 67/1000 | Loss: 0.00002047
Iteration 68/1000 | Loss: 0.00002046
Iteration 69/1000 | Loss: 0.00002046
Iteration 70/1000 | Loss: 0.00002046
Iteration 71/1000 | Loss: 0.00002045
Iteration 72/1000 | Loss: 0.00002045
Iteration 73/1000 | Loss: 0.00002044
Iteration 74/1000 | Loss: 0.00002044
Iteration 75/1000 | Loss: 0.00002044
Iteration 76/1000 | Loss: 0.00002044
Iteration 77/1000 | Loss: 0.00002043
Iteration 78/1000 | Loss: 0.00002043
Iteration 79/1000 | Loss: 0.00002043
Iteration 80/1000 | Loss: 0.00002042
Iteration 81/1000 | Loss: 0.00002042
Iteration 82/1000 | Loss: 0.00002041
Iteration 83/1000 | Loss: 0.00002041
Iteration 84/1000 | Loss: 0.00002041
Iteration 85/1000 | Loss: 0.00002039
Iteration 86/1000 | Loss: 0.00002039
Iteration 87/1000 | Loss: 0.00002039
Iteration 88/1000 | Loss: 0.00002039
Iteration 89/1000 | Loss: 0.00002039
Iteration 90/1000 | Loss: 0.00002039
Iteration 91/1000 | Loss: 0.00002038
Iteration 92/1000 | Loss: 0.00002038
Iteration 93/1000 | Loss: 0.00002038
Iteration 94/1000 | Loss: 0.00002037
Iteration 95/1000 | Loss: 0.00002037
Iteration 96/1000 | Loss: 0.00002037
Iteration 97/1000 | Loss: 0.00002036
Iteration 98/1000 | Loss: 0.00002036
Iteration 99/1000 | Loss: 0.00002036
Iteration 100/1000 | Loss: 0.00002035
Iteration 101/1000 | Loss: 0.00002035
Iteration 102/1000 | Loss: 0.00002035
Iteration 103/1000 | Loss: 0.00002035
Iteration 104/1000 | Loss: 0.00002035
Iteration 105/1000 | Loss: 0.00002035
Iteration 106/1000 | Loss: 0.00002034
Iteration 107/1000 | Loss: 0.00002034
Iteration 108/1000 | Loss: 0.00002034
Iteration 109/1000 | Loss: 0.00002034
Iteration 110/1000 | Loss: 0.00002034
Iteration 111/1000 | Loss: 0.00002034
Iteration 112/1000 | Loss: 0.00002034
Iteration 113/1000 | Loss: 0.00002034
Iteration 114/1000 | Loss: 0.00002034
Iteration 115/1000 | Loss: 0.00002034
Iteration 116/1000 | Loss: 0.00002034
Iteration 117/1000 | Loss: 0.00002033
Iteration 118/1000 | Loss: 0.00002033
Iteration 119/1000 | Loss: 0.00002033
Iteration 120/1000 | Loss: 0.00002033
Iteration 121/1000 | Loss: 0.00002033
Iteration 122/1000 | Loss: 0.00002032
Iteration 123/1000 | Loss: 0.00002032
Iteration 124/1000 | Loss: 0.00002032
Iteration 125/1000 | Loss: 0.00002032
Iteration 126/1000 | Loss: 0.00002032
Iteration 127/1000 | Loss: 0.00002032
Iteration 128/1000 | Loss: 0.00002031
Iteration 129/1000 | Loss: 0.00002031
Iteration 130/1000 | Loss: 0.00002031
Iteration 131/1000 | Loss: 0.00002031
Iteration 132/1000 | Loss: 0.00002031
Iteration 133/1000 | Loss: 0.00002031
Iteration 134/1000 | Loss: 0.00002031
Iteration 135/1000 | Loss: 0.00002030
Iteration 136/1000 | Loss: 0.00002030
Iteration 137/1000 | Loss: 0.00002030
Iteration 138/1000 | Loss: 0.00002030
Iteration 139/1000 | Loss: 0.00002030
Iteration 140/1000 | Loss: 0.00002030
Iteration 141/1000 | Loss: 0.00002030
Iteration 142/1000 | Loss: 0.00002030
Iteration 143/1000 | Loss: 0.00002030
Iteration 144/1000 | Loss: 0.00002030
Iteration 145/1000 | Loss: 0.00002030
Iteration 146/1000 | Loss: 0.00002030
Iteration 147/1000 | Loss: 0.00002030
Iteration 148/1000 | Loss: 0.00002030
Iteration 149/1000 | Loss: 0.00002030
Iteration 150/1000 | Loss: 0.00002030
Iteration 151/1000 | Loss: 0.00002029
Iteration 152/1000 | Loss: 0.00002029
Iteration 153/1000 | Loss: 0.00002029
Iteration 154/1000 | Loss: 0.00002029
Iteration 155/1000 | Loss: 0.00002029
Iteration 156/1000 | Loss: 0.00002029
Iteration 157/1000 | Loss: 0.00002029
Iteration 158/1000 | Loss: 0.00002029
Iteration 159/1000 | Loss: 0.00002029
Iteration 160/1000 | Loss: 0.00002028
Iteration 161/1000 | Loss: 0.00002028
Iteration 162/1000 | Loss: 0.00002028
Iteration 163/1000 | Loss: 0.00002028
Iteration 164/1000 | Loss: 0.00002028
Iteration 165/1000 | Loss: 0.00002028
Iteration 166/1000 | Loss: 0.00002028
Iteration 167/1000 | Loss: 0.00002028
Iteration 168/1000 | Loss: 0.00002027
Iteration 169/1000 | Loss: 0.00002027
Iteration 170/1000 | Loss: 0.00002027
Iteration 171/1000 | Loss: 0.00002027
Iteration 172/1000 | Loss: 0.00002027
Iteration 173/1000 | Loss: 0.00002027
Iteration 174/1000 | Loss: 0.00002027
Iteration 175/1000 | Loss: 0.00002026
Iteration 176/1000 | Loss: 0.00002026
Iteration 177/1000 | Loss: 0.00002026
Iteration 178/1000 | Loss: 0.00002026
Iteration 179/1000 | Loss: 0.00002026
Iteration 180/1000 | Loss: 0.00002026
Iteration 181/1000 | Loss: 0.00002026
Iteration 182/1000 | Loss: 0.00002026
Iteration 183/1000 | Loss: 0.00002026
Iteration 184/1000 | Loss: 0.00002026
Iteration 185/1000 | Loss: 0.00002026
Iteration 186/1000 | Loss: 0.00002026
Iteration 187/1000 | Loss: 0.00002026
Iteration 188/1000 | Loss: 0.00002025
Iteration 189/1000 | Loss: 0.00002025
Iteration 190/1000 | Loss: 0.00002025
Iteration 191/1000 | Loss: 0.00002025
Iteration 192/1000 | Loss: 0.00002025
Iteration 193/1000 | Loss: 0.00002025
Iteration 194/1000 | Loss: 0.00002025
Iteration 195/1000 | Loss: 0.00002025
Iteration 196/1000 | Loss: 0.00002025
Iteration 197/1000 | Loss: 0.00002025
Iteration 198/1000 | Loss: 0.00002025
Iteration 199/1000 | Loss: 0.00002025
Iteration 200/1000 | Loss: 0.00002024
Iteration 201/1000 | Loss: 0.00002024
Iteration 202/1000 | Loss: 0.00002024
Iteration 203/1000 | Loss: 0.00002024
Iteration 204/1000 | Loss: 0.00002024
Iteration 205/1000 | Loss: 0.00002024
Iteration 206/1000 | Loss: 0.00002024
Iteration 207/1000 | Loss: 0.00002024
Iteration 208/1000 | Loss: 0.00002024
Iteration 209/1000 | Loss: 0.00002024
Iteration 210/1000 | Loss: 0.00002023
Iteration 211/1000 | Loss: 0.00002023
Iteration 212/1000 | Loss: 0.00002023
Iteration 213/1000 | Loss: 0.00002023
Iteration 214/1000 | Loss: 0.00002023
Iteration 215/1000 | Loss: 0.00002023
Iteration 216/1000 | Loss: 0.00002022
Iteration 217/1000 | Loss: 0.00002022
Iteration 218/1000 | Loss: 0.00002022
Iteration 219/1000 | Loss: 0.00002022
Iteration 220/1000 | Loss: 0.00002022
Iteration 221/1000 | Loss: 0.00002022
Iteration 222/1000 | Loss: 0.00002022
Iteration 223/1000 | Loss: 0.00002022
Iteration 224/1000 | Loss: 0.00002022
Iteration 225/1000 | Loss: 0.00002022
Iteration 226/1000 | Loss: 0.00002022
Iteration 227/1000 | Loss: 0.00002022
Iteration 228/1000 | Loss: 0.00002022
Iteration 229/1000 | Loss: 0.00002022
Iteration 230/1000 | Loss: 0.00002022
Iteration 231/1000 | Loss: 0.00002022
Iteration 232/1000 | Loss: 0.00002022
Iteration 233/1000 | Loss: 0.00002022
Iteration 234/1000 | Loss: 0.00002022
Iteration 235/1000 | Loss: 0.00002022
Iteration 236/1000 | Loss: 0.00002021
Iteration 237/1000 | Loss: 0.00002021
Iteration 238/1000 | Loss: 0.00002021
Iteration 239/1000 | Loss: 0.00002021
Iteration 240/1000 | Loss: 0.00002021
Iteration 241/1000 | Loss: 0.00002021
Iteration 242/1000 | Loss: 0.00002021
Iteration 243/1000 | Loss: 0.00002021
Iteration 244/1000 | Loss: 0.00002021
Iteration 245/1000 | Loss: 0.00002021
Iteration 246/1000 | Loss: 0.00002021
Iteration 247/1000 | Loss: 0.00002021
Iteration 248/1000 | Loss: 0.00002021
Iteration 249/1000 | Loss: 0.00002021
Iteration 250/1000 | Loss: 0.00002021
Iteration 251/1000 | Loss: 0.00002020
Iteration 252/1000 | Loss: 0.00002020
Iteration 253/1000 | Loss: 0.00002020
Iteration 254/1000 | Loss: 0.00002020
Iteration 255/1000 | Loss: 0.00002020
Iteration 256/1000 | Loss: 0.00002020
Iteration 257/1000 | Loss: 0.00002020
Iteration 258/1000 | Loss: 0.00002020
Iteration 259/1000 | Loss: 0.00002020
Iteration 260/1000 | Loss: 0.00002020
Iteration 261/1000 | Loss: 0.00002020
Iteration 262/1000 | Loss: 0.00002020
Iteration 263/1000 | Loss: 0.00002020
Iteration 264/1000 | Loss: 0.00002020
Iteration 265/1000 | Loss: 0.00002020
Iteration 266/1000 | Loss: 0.00002020
Iteration 267/1000 | Loss: 0.00002020
Iteration 268/1000 | Loss: 0.00002019
Iteration 269/1000 | Loss: 0.00002019
Iteration 270/1000 | Loss: 0.00002019
Iteration 271/1000 | Loss: 0.00002019
Iteration 272/1000 | Loss: 0.00002019
Iteration 273/1000 | Loss: 0.00002019
Iteration 274/1000 | Loss: 0.00002019
Iteration 275/1000 | Loss: 0.00002019
Iteration 276/1000 | Loss: 0.00002019
Iteration 277/1000 | Loss: 0.00002019
Iteration 278/1000 | Loss: 0.00002019
Iteration 279/1000 | Loss: 0.00002019
Iteration 280/1000 | Loss: 0.00002019
Iteration 281/1000 | Loss: 0.00002019
Iteration 282/1000 | Loss: 0.00002019
Iteration 283/1000 | Loss: 0.00002019
Iteration 284/1000 | Loss: 0.00002019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [2.0185621906421147e-05, 2.0185621906421147e-05, 2.0185621906421147e-05, 2.0185621906421147e-05, 2.0185621906421147e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0185621906421147e-05

Optimization complete. Final v2v error: 3.8995487689971924 mm

Highest mean error: 4.4016828536987305 mm for frame 0

Lowest mean error: 3.524200439453125 mm for frame 233

Saving results

Total time: 55.228413105010986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868364
Iteration 2/25 | Loss: 0.00139058
Iteration 3/25 | Loss: 0.00109225
Iteration 4/25 | Loss: 0.00103678
Iteration 5/25 | Loss: 0.00102058
Iteration 6/25 | Loss: 0.00101603
Iteration 7/25 | Loss: 0.00101461
Iteration 8/25 | Loss: 0.00101426
Iteration 9/25 | Loss: 0.00101426
Iteration 10/25 | Loss: 0.00101426
Iteration 11/25 | Loss: 0.00101426
Iteration 12/25 | Loss: 0.00101426
Iteration 13/25 | Loss: 0.00101426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010142576647922397, 0.0010142576647922397, 0.0010142576647922397, 0.0010142576647922397, 0.0010142576647922397]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010142576647922397

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.43348265
Iteration 2/25 | Loss: 0.00122323
Iteration 3/25 | Loss: 0.00122323
Iteration 4/25 | Loss: 0.00122322
Iteration 5/25 | Loss: 0.00122322
Iteration 6/25 | Loss: 0.00122322
Iteration 7/25 | Loss: 0.00122322
Iteration 8/25 | Loss: 0.00122322
Iteration 9/25 | Loss: 0.00122322
Iteration 10/25 | Loss: 0.00122322
Iteration 11/25 | Loss: 0.00122322
Iteration 12/25 | Loss: 0.00122322
Iteration 13/25 | Loss: 0.00122322
Iteration 14/25 | Loss: 0.00122322
Iteration 15/25 | Loss: 0.00122322
Iteration 16/25 | Loss: 0.00122322
Iteration 17/25 | Loss: 0.00122322
Iteration 18/25 | Loss: 0.00122322
Iteration 19/25 | Loss: 0.00122322
Iteration 20/25 | Loss: 0.00122322
Iteration 21/25 | Loss: 0.00122322
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012232234003022313, 0.0012232234003022313, 0.0012232234003022313, 0.0012232234003022313, 0.0012232234003022313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012232234003022313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122322
Iteration 2/1000 | Loss: 0.00006989
Iteration 3/1000 | Loss: 0.00004511
Iteration 4/1000 | Loss: 0.00003688
Iteration 5/1000 | Loss: 0.00003310
Iteration 6/1000 | Loss: 0.00003176
Iteration 7/1000 | Loss: 0.00003040
Iteration 8/1000 | Loss: 0.00002951
Iteration 9/1000 | Loss: 0.00002879
Iteration 10/1000 | Loss: 0.00002828
Iteration 11/1000 | Loss: 0.00002797
Iteration 12/1000 | Loss: 0.00002787
Iteration 13/1000 | Loss: 0.00002771
Iteration 14/1000 | Loss: 0.00002754
Iteration 15/1000 | Loss: 0.00002740
Iteration 16/1000 | Loss: 0.00002735
Iteration 17/1000 | Loss: 0.00002732
Iteration 18/1000 | Loss: 0.00002731
Iteration 19/1000 | Loss: 0.00002731
Iteration 20/1000 | Loss: 0.00002731
Iteration 21/1000 | Loss: 0.00002730
Iteration 22/1000 | Loss: 0.00002729
Iteration 23/1000 | Loss: 0.00002729
Iteration 24/1000 | Loss: 0.00002728
Iteration 25/1000 | Loss: 0.00002728
Iteration 26/1000 | Loss: 0.00002725
Iteration 27/1000 | Loss: 0.00002725
Iteration 28/1000 | Loss: 0.00002724
Iteration 29/1000 | Loss: 0.00002724
Iteration 30/1000 | Loss: 0.00002723
Iteration 31/1000 | Loss: 0.00002723
Iteration 32/1000 | Loss: 0.00002723
Iteration 33/1000 | Loss: 0.00002723
Iteration 34/1000 | Loss: 0.00002723
Iteration 35/1000 | Loss: 0.00002723
Iteration 36/1000 | Loss: 0.00002723
Iteration 37/1000 | Loss: 0.00002722
Iteration 38/1000 | Loss: 0.00002722
Iteration 39/1000 | Loss: 0.00002722
Iteration 40/1000 | Loss: 0.00002721
Iteration 41/1000 | Loss: 0.00002721
Iteration 42/1000 | Loss: 0.00002721
Iteration 43/1000 | Loss: 0.00002721
Iteration 44/1000 | Loss: 0.00002721
Iteration 45/1000 | Loss: 0.00002721
Iteration 46/1000 | Loss: 0.00002720
Iteration 47/1000 | Loss: 0.00002720
Iteration 48/1000 | Loss: 0.00002720
Iteration 49/1000 | Loss: 0.00002720
Iteration 50/1000 | Loss: 0.00002720
Iteration 51/1000 | Loss: 0.00002720
Iteration 52/1000 | Loss: 0.00002720
Iteration 53/1000 | Loss: 0.00002720
Iteration 54/1000 | Loss: 0.00002720
Iteration 55/1000 | Loss: 0.00002720
Iteration 56/1000 | Loss: 0.00002720
Iteration 57/1000 | Loss: 0.00002720
Iteration 58/1000 | Loss: 0.00002719
Iteration 59/1000 | Loss: 0.00002719
Iteration 60/1000 | Loss: 0.00002719
Iteration 61/1000 | Loss: 0.00002719
Iteration 62/1000 | Loss: 0.00002718
Iteration 63/1000 | Loss: 0.00002718
Iteration 64/1000 | Loss: 0.00002718
Iteration 65/1000 | Loss: 0.00002718
Iteration 66/1000 | Loss: 0.00002718
Iteration 67/1000 | Loss: 0.00002717
Iteration 68/1000 | Loss: 0.00002717
Iteration 69/1000 | Loss: 0.00002717
Iteration 70/1000 | Loss: 0.00002716
Iteration 71/1000 | Loss: 0.00002716
Iteration 72/1000 | Loss: 0.00002716
Iteration 73/1000 | Loss: 0.00002716
Iteration 74/1000 | Loss: 0.00002716
Iteration 75/1000 | Loss: 0.00002716
Iteration 76/1000 | Loss: 0.00002716
Iteration 77/1000 | Loss: 0.00002715
Iteration 78/1000 | Loss: 0.00002715
Iteration 79/1000 | Loss: 0.00002715
Iteration 80/1000 | Loss: 0.00002715
Iteration 81/1000 | Loss: 0.00002715
Iteration 82/1000 | Loss: 0.00002715
Iteration 83/1000 | Loss: 0.00002715
Iteration 84/1000 | Loss: 0.00002714
Iteration 85/1000 | Loss: 0.00002714
Iteration 86/1000 | Loss: 0.00002714
Iteration 87/1000 | Loss: 0.00002713
Iteration 88/1000 | Loss: 0.00002713
Iteration 89/1000 | Loss: 0.00002713
Iteration 90/1000 | Loss: 0.00002713
Iteration 91/1000 | Loss: 0.00002713
Iteration 92/1000 | Loss: 0.00002713
Iteration 93/1000 | Loss: 0.00002713
Iteration 94/1000 | Loss: 0.00002713
Iteration 95/1000 | Loss: 0.00002713
Iteration 96/1000 | Loss: 0.00002712
Iteration 97/1000 | Loss: 0.00002712
Iteration 98/1000 | Loss: 0.00002712
Iteration 99/1000 | Loss: 0.00002712
Iteration 100/1000 | Loss: 0.00002712
Iteration 101/1000 | Loss: 0.00002712
Iteration 102/1000 | Loss: 0.00002712
Iteration 103/1000 | Loss: 0.00002712
Iteration 104/1000 | Loss: 0.00002711
Iteration 105/1000 | Loss: 0.00002711
Iteration 106/1000 | Loss: 0.00002711
Iteration 107/1000 | Loss: 0.00002711
Iteration 108/1000 | Loss: 0.00002711
Iteration 109/1000 | Loss: 0.00002711
Iteration 110/1000 | Loss: 0.00002711
Iteration 111/1000 | Loss: 0.00002711
Iteration 112/1000 | Loss: 0.00002710
Iteration 113/1000 | Loss: 0.00002710
Iteration 114/1000 | Loss: 0.00002710
Iteration 115/1000 | Loss: 0.00002710
Iteration 116/1000 | Loss: 0.00002710
Iteration 117/1000 | Loss: 0.00002710
Iteration 118/1000 | Loss: 0.00002710
Iteration 119/1000 | Loss: 0.00002710
Iteration 120/1000 | Loss: 0.00002710
Iteration 121/1000 | Loss: 0.00002709
Iteration 122/1000 | Loss: 0.00002709
Iteration 123/1000 | Loss: 0.00002709
Iteration 124/1000 | Loss: 0.00002709
Iteration 125/1000 | Loss: 0.00002709
Iteration 126/1000 | Loss: 0.00002709
Iteration 127/1000 | Loss: 0.00002709
Iteration 128/1000 | Loss: 0.00002708
Iteration 129/1000 | Loss: 0.00002708
Iteration 130/1000 | Loss: 0.00002708
Iteration 131/1000 | Loss: 0.00002708
Iteration 132/1000 | Loss: 0.00002708
Iteration 133/1000 | Loss: 0.00002708
Iteration 134/1000 | Loss: 0.00002708
Iteration 135/1000 | Loss: 0.00002708
Iteration 136/1000 | Loss: 0.00002708
Iteration 137/1000 | Loss: 0.00002707
Iteration 138/1000 | Loss: 0.00002707
Iteration 139/1000 | Loss: 0.00002707
Iteration 140/1000 | Loss: 0.00002707
Iteration 141/1000 | Loss: 0.00002707
Iteration 142/1000 | Loss: 0.00002707
Iteration 143/1000 | Loss: 0.00002707
Iteration 144/1000 | Loss: 0.00002706
Iteration 145/1000 | Loss: 0.00002706
Iteration 146/1000 | Loss: 0.00002706
Iteration 147/1000 | Loss: 0.00002706
Iteration 148/1000 | Loss: 0.00002706
Iteration 149/1000 | Loss: 0.00002706
Iteration 150/1000 | Loss: 0.00002706
Iteration 151/1000 | Loss: 0.00002706
Iteration 152/1000 | Loss: 0.00002705
Iteration 153/1000 | Loss: 0.00002705
Iteration 154/1000 | Loss: 0.00002705
Iteration 155/1000 | Loss: 0.00002705
Iteration 156/1000 | Loss: 0.00002705
Iteration 157/1000 | Loss: 0.00002705
Iteration 158/1000 | Loss: 0.00002705
Iteration 159/1000 | Loss: 0.00002705
Iteration 160/1000 | Loss: 0.00002705
Iteration 161/1000 | Loss: 0.00002705
Iteration 162/1000 | Loss: 0.00002705
Iteration 163/1000 | Loss: 0.00002704
Iteration 164/1000 | Loss: 0.00002704
Iteration 165/1000 | Loss: 0.00002704
Iteration 166/1000 | Loss: 0.00002704
Iteration 167/1000 | Loss: 0.00002704
Iteration 168/1000 | Loss: 0.00002704
Iteration 169/1000 | Loss: 0.00002704
Iteration 170/1000 | Loss: 0.00002704
Iteration 171/1000 | Loss: 0.00002704
Iteration 172/1000 | Loss: 0.00002704
Iteration 173/1000 | Loss: 0.00002704
Iteration 174/1000 | Loss: 0.00002704
Iteration 175/1000 | Loss: 0.00002704
Iteration 176/1000 | Loss: 0.00002704
Iteration 177/1000 | Loss: 0.00002704
Iteration 178/1000 | Loss: 0.00002704
Iteration 179/1000 | Loss: 0.00002704
Iteration 180/1000 | Loss: 0.00002703
Iteration 181/1000 | Loss: 0.00002703
Iteration 182/1000 | Loss: 0.00002703
Iteration 183/1000 | Loss: 0.00002703
Iteration 184/1000 | Loss: 0.00002703
Iteration 185/1000 | Loss: 0.00002703
Iteration 186/1000 | Loss: 0.00002703
Iteration 187/1000 | Loss: 0.00002703
Iteration 188/1000 | Loss: 0.00002703
Iteration 189/1000 | Loss: 0.00002703
Iteration 190/1000 | Loss: 0.00002703
Iteration 191/1000 | Loss: 0.00002703
Iteration 192/1000 | Loss: 0.00002703
Iteration 193/1000 | Loss: 0.00002703
Iteration 194/1000 | Loss: 0.00002703
Iteration 195/1000 | Loss: 0.00002703
Iteration 196/1000 | Loss: 0.00002703
Iteration 197/1000 | Loss: 0.00002703
Iteration 198/1000 | Loss: 0.00002702
Iteration 199/1000 | Loss: 0.00002702
Iteration 200/1000 | Loss: 0.00002702
Iteration 201/1000 | Loss: 0.00002702
Iteration 202/1000 | Loss: 0.00002702
Iteration 203/1000 | Loss: 0.00002702
Iteration 204/1000 | Loss: 0.00002702
Iteration 205/1000 | Loss: 0.00002702
Iteration 206/1000 | Loss: 0.00002702
Iteration 207/1000 | Loss: 0.00002702
Iteration 208/1000 | Loss: 0.00002702
Iteration 209/1000 | Loss: 0.00002702
Iteration 210/1000 | Loss: 0.00002702
Iteration 211/1000 | Loss: 0.00002702
Iteration 212/1000 | Loss: 0.00002702
Iteration 213/1000 | Loss: 0.00002702
Iteration 214/1000 | Loss: 0.00002702
Iteration 215/1000 | Loss: 0.00002702
Iteration 216/1000 | Loss: 0.00002702
Iteration 217/1000 | Loss: 0.00002702
Iteration 218/1000 | Loss: 0.00002702
Iteration 219/1000 | Loss: 0.00002701
Iteration 220/1000 | Loss: 0.00002701
Iteration 221/1000 | Loss: 0.00002701
Iteration 222/1000 | Loss: 0.00002701
Iteration 223/1000 | Loss: 0.00002701
Iteration 224/1000 | Loss: 0.00002701
Iteration 225/1000 | Loss: 0.00002701
Iteration 226/1000 | Loss: 0.00002701
Iteration 227/1000 | Loss: 0.00002701
Iteration 228/1000 | Loss: 0.00002701
Iteration 229/1000 | Loss: 0.00002701
Iteration 230/1000 | Loss: 0.00002701
Iteration 231/1000 | Loss: 0.00002701
Iteration 232/1000 | Loss: 0.00002701
Iteration 233/1000 | Loss: 0.00002701
Iteration 234/1000 | Loss: 0.00002701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [2.70125219685724e-05, 2.70125219685724e-05, 2.70125219685724e-05, 2.70125219685724e-05, 2.70125219685724e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.70125219685724e-05

Optimization complete. Final v2v error: 4.4465413093566895 mm

Highest mean error: 6.127013683319092 mm for frame 95

Lowest mean error: 3.5163357257843018 mm for frame 111

Saving results

Total time: 46.70137858390808
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462653
Iteration 2/25 | Loss: 0.00101745
Iteration 3/25 | Loss: 0.00092212
Iteration 4/25 | Loss: 0.00090738
Iteration 5/25 | Loss: 0.00090103
Iteration 6/25 | Loss: 0.00089941
Iteration 7/25 | Loss: 0.00089906
Iteration 8/25 | Loss: 0.00089906
Iteration 9/25 | Loss: 0.00089906
Iteration 10/25 | Loss: 0.00089906
Iteration 11/25 | Loss: 0.00089906
Iteration 12/25 | Loss: 0.00089906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008990623173303902, 0.0008990623173303902, 0.0008990623173303902, 0.0008990623173303902, 0.0008990623173303902]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008990623173303902

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36062849
Iteration 2/25 | Loss: 0.00115975
Iteration 3/25 | Loss: 0.00115975
Iteration 4/25 | Loss: 0.00115974
Iteration 5/25 | Loss: 0.00115974
Iteration 6/25 | Loss: 0.00115974
Iteration 7/25 | Loss: 0.00115974
Iteration 8/25 | Loss: 0.00115974
Iteration 9/25 | Loss: 0.00115974
Iteration 10/25 | Loss: 0.00115974
Iteration 11/25 | Loss: 0.00115974
Iteration 12/25 | Loss: 0.00115974
Iteration 13/25 | Loss: 0.00115974
Iteration 14/25 | Loss: 0.00115974
Iteration 15/25 | Loss: 0.00115974
Iteration 16/25 | Loss: 0.00115974
Iteration 17/25 | Loss: 0.00115974
Iteration 18/25 | Loss: 0.00115974
Iteration 19/25 | Loss: 0.00115974
Iteration 20/25 | Loss: 0.00115974
Iteration 21/25 | Loss: 0.00115974
Iteration 22/25 | Loss: 0.00115974
Iteration 23/25 | Loss: 0.00115974
Iteration 24/25 | Loss: 0.00115974
Iteration 25/25 | Loss: 0.00115974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115974
Iteration 2/1000 | Loss: 0.00002818
Iteration 3/1000 | Loss: 0.00002241
Iteration 4/1000 | Loss: 0.00002044
Iteration 5/1000 | Loss: 0.00001931
Iteration 6/1000 | Loss: 0.00001856
Iteration 7/1000 | Loss: 0.00001808
Iteration 8/1000 | Loss: 0.00001777
Iteration 9/1000 | Loss: 0.00001777
Iteration 10/1000 | Loss: 0.00001768
Iteration 11/1000 | Loss: 0.00001768
Iteration 12/1000 | Loss: 0.00001767
Iteration 13/1000 | Loss: 0.00001766
Iteration 14/1000 | Loss: 0.00001762
Iteration 15/1000 | Loss: 0.00001762
Iteration 16/1000 | Loss: 0.00001762
Iteration 17/1000 | Loss: 0.00001762
Iteration 18/1000 | Loss: 0.00001762
Iteration 19/1000 | Loss: 0.00001762
Iteration 20/1000 | Loss: 0.00001762
Iteration 21/1000 | Loss: 0.00001762
Iteration 22/1000 | Loss: 0.00001762
Iteration 23/1000 | Loss: 0.00001762
Iteration 24/1000 | Loss: 0.00001760
Iteration 25/1000 | Loss: 0.00001760
Iteration 26/1000 | Loss: 0.00001758
Iteration 27/1000 | Loss: 0.00001758
Iteration 28/1000 | Loss: 0.00001758
Iteration 29/1000 | Loss: 0.00001758
Iteration 30/1000 | Loss: 0.00001758
Iteration 31/1000 | Loss: 0.00001758
Iteration 32/1000 | Loss: 0.00001757
Iteration 33/1000 | Loss: 0.00001757
Iteration 34/1000 | Loss: 0.00001757
Iteration 35/1000 | Loss: 0.00001757
Iteration 36/1000 | Loss: 0.00001752
Iteration 37/1000 | Loss: 0.00001751
Iteration 38/1000 | Loss: 0.00001746
Iteration 39/1000 | Loss: 0.00001746
Iteration 40/1000 | Loss: 0.00001745
Iteration 41/1000 | Loss: 0.00001745
Iteration 42/1000 | Loss: 0.00001745
Iteration 43/1000 | Loss: 0.00001745
Iteration 44/1000 | Loss: 0.00001744
Iteration 45/1000 | Loss: 0.00001744
Iteration 46/1000 | Loss: 0.00001744
Iteration 47/1000 | Loss: 0.00001744
Iteration 48/1000 | Loss: 0.00001743
Iteration 49/1000 | Loss: 0.00001743
Iteration 50/1000 | Loss: 0.00001742
Iteration 51/1000 | Loss: 0.00001742
Iteration 52/1000 | Loss: 0.00001741
Iteration 53/1000 | Loss: 0.00001741
Iteration 54/1000 | Loss: 0.00001741
Iteration 55/1000 | Loss: 0.00001740
Iteration 56/1000 | Loss: 0.00001740
Iteration 57/1000 | Loss: 0.00001740
Iteration 58/1000 | Loss: 0.00001740
Iteration 59/1000 | Loss: 0.00001740
Iteration 60/1000 | Loss: 0.00001739
Iteration 61/1000 | Loss: 0.00001738
Iteration 62/1000 | Loss: 0.00001738
Iteration 63/1000 | Loss: 0.00001738
Iteration 64/1000 | Loss: 0.00001738
Iteration 65/1000 | Loss: 0.00001737
Iteration 66/1000 | Loss: 0.00001737
Iteration 67/1000 | Loss: 0.00001737
Iteration 68/1000 | Loss: 0.00001737
Iteration 69/1000 | Loss: 0.00001737
Iteration 70/1000 | Loss: 0.00001737
Iteration 71/1000 | Loss: 0.00001737
Iteration 72/1000 | Loss: 0.00001737
Iteration 73/1000 | Loss: 0.00001737
Iteration 74/1000 | Loss: 0.00001736
Iteration 75/1000 | Loss: 0.00001736
Iteration 76/1000 | Loss: 0.00001736
Iteration 77/1000 | Loss: 0.00001736
Iteration 78/1000 | Loss: 0.00001736
Iteration 79/1000 | Loss: 0.00001736
Iteration 80/1000 | Loss: 0.00001736
Iteration 81/1000 | Loss: 0.00001736
Iteration 82/1000 | Loss: 0.00001735
Iteration 83/1000 | Loss: 0.00001735
Iteration 84/1000 | Loss: 0.00001735
Iteration 85/1000 | Loss: 0.00001735
Iteration 86/1000 | Loss: 0.00001735
Iteration 87/1000 | Loss: 0.00001735
Iteration 88/1000 | Loss: 0.00001734
Iteration 89/1000 | Loss: 0.00001734
Iteration 90/1000 | Loss: 0.00001734
Iteration 91/1000 | Loss: 0.00001734
Iteration 92/1000 | Loss: 0.00001734
Iteration 93/1000 | Loss: 0.00001734
Iteration 94/1000 | Loss: 0.00001734
Iteration 95/1000 | Loss: 0.00001734
Iteration 96/1000 | Loss: 0.00001734
Iteration 97/1000 | Loss: 0.00001734
Iteration 98/1000 | Loss: 0.00001734
Iteration 99/1000 | Loss: 0.00001733
Iteration 100/1000 | Loss: 0.00001733
Iteration 101/1000 | Loss: 0.00001733
Iteration 102/1000 | Loss: 0.00001733
Iteration 103/1000 | Loss: 0.00001733
Iteration 104/1000 | Loss: 0.00001733
Iteration 105/1000 | Loss: 0.00001732
Iteration 106/1000 | Loss: 0.00001732
Iteration 107/1000 | Loss: 0.00001732
Iteration 108/1000 | Loss: 0.00001731
Iteration 109/1000 | Loss: 0.00001731
Iteration 110/1000 | Loss: 0.00001731
Iteration 111/1000 | Loss: 0.00001731
Iteration 112/1000 | Loss: 0.00001731
Iteration 113/1000 | Loss: 0.00001731
Iteration 114/1000 | Loss: 0.00001731
Iteration 115/1000 | Loss: 0.00001731
Iteration 116/1000 | Loss: 0.00001731
Iteration 117/1000 | Loss: 0.00001731
Iteration 118/1000 | Loss: 0.00001731
Iteration 119/1000 | Loss: 0.00001731
Iteration 120/1000 | Loss: 0.00001731
Iteration 121/1000 | Loss: 0.00001730
Iteration 122/1000 | Loss: 0.00001730
Iteration 123/1000 | Loss: 0.00001730
Iteration 124/1000 | Loss: 0.00001730
Iteration 125/1000 | Loss: 0.00001730
Iteration 126/1000 | Loss: 0.00001730
Iteration 127/1000 | Loss: 0.00001730
Iteration 128/1000 | Loss: 0.00001730
Iteration 129/1000 | Loss: 0.00001730
Iteration 130/1000 | Loss: 0.00001730
Iteration 131/1000 | Loss: 0.00001730
Iteration 132/1000 | Loss: 0.00001730
Iteration 133/1000 | Loss: 0.00001730
Iteration 134/1000 | Loss: 0.00001730
Iteration 135/1000 | Loss: 0.00001730
Iteration 136/1000 | Loss: 0.00001730
Iteration 137/1000 | Loss: 0.00001730
Iteration 138/1000 | Loss: 0.00001730
Iteration 139/1000 | Loss: 0.00001730
Iteration 140/1000 | Loss: 0.00001730
Iteration 141/1000 | Loss: 0.00001730
Iteration 142/1000 | Loss: 0.00001730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.7298178136115894e-05, 1.7298178136115894e-05, 1.7298178136115894e-05, 1.7298178136115894e-05, 1.7298178136115894e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7298178136115894e-05

Optimization complete. Final v2v error: 3.5137693881988525 mm

Highest mean error: 3.773961305618286 mm for frame 29

Lowest mean error: 3.3212642669677734 mm for frame 134

Saving results

Total time: 33.33028984069824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_1159/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_1159/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426947
Iteration 2/25 | Loss: 0.00111371
Iteration 3/25 | Loss: 0.00099002
Iteration 4/25 | Loss: 0.00096708
Iteration 5/25 | Loss: 0.00095649
Iteration 6/25 | Loss: 0.00095455
Iteration 7/25 | Loss: 0.00095426
Iteration 8/25 | Loss: 0.00095426
Iteration 9/25 | Loss: 0.00095426
Iteration 10/25 | Loss: 0.00095426
Iteration 11/25 | Loss: 0.00095426
Iteration 12/25 | Loss: 0.00095426
Iteration 13/25 | Loss: 0.00095419
Iteration 14/25 | Loss: 0.00095419
Iteration 15/25 | Loss: 0.00095419
Iteration 16/25 | Loss: 0.00095419
Iteration 17/25 | Loss: 0.00095419
Iteration 18/25 | Loss: 0.00095419
Iteration 19/25 | Loss: 0.00095419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009541928884573281, 0.0009541928884573281, 0.0009541928884573281, 0.0009541928884573281, 0.0009541928884573281]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009541928884573281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21517837
Iteration 2/25 | Loss: 0.00116788
Iteration 3/25 | Loss: 0.00116788
Iteration 4/25 | Loss: 0.00116788
Iteration 5/25 | Loss: 0.00116788
Iteration 6/25 | Loss: 0.00116788
Iteration 7/25 | Loss: 0.00116788
Iteration 8/25 | Loss: 0.00116788
Iteration 9/25 | Loss: 0.00116788
Iteration 10/25 | Loss: 0.00116788
Iteration 11/25 | Loss: 0.00116788
Iteration 12/25 | Loss: 0.00116788
Iteration 13/25 | Loss: 0.00116788
Iteration 14/25 | Loss: 0.00116788
Iteration 15/25 | Loss: 0.00116788
Iteration 16/25 | Loss: 0.00116788
Iteration 17/25 | Loss: 0.00116788
Iteration 18/25 | Loss: 0.00116788
Iteration 19/25 | Loss: 0.00116788
Iteration 20/25 | Loss: 0.00116788
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011678808368742466, 0.0011678808368742466, 0.0011678808368742466, 0.0011678808368742466, 0.0011678808368742466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011678808368742466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116788
Iteration 2/1000 | Loss: 0.00005116
Iteration 3/1000 | Loss: 0.00004150
Iteration 4/1000 | Loss: 0.00003748
Iteration 5/1000 | Loss: 0.00003538
Iteration 6/1000 | Loss: 0.00003374
Iteration 7/1000 | Loss: 0.00003260
Iteration 8/1000 | Loss: 0.00003197
Iteration 9/1000 | Loss: 0.00003171
Iteration 10/1000 | Loss: 0.00003154
Iteration 11/1000 | Loss: 0.00003143
Iteration 12/1000 | Loss: 0.00003130
Iteration 13/1000 | Loss: 0.00003128
Iteration 14/1000 | Loss: 0.00003124
Iteration 15/1000 | Loss: 0.00003121
Iteration 16/1000 | Loss: 0.00003120
Iteration 17/1000 | Loss: 0.00003120
Iteration 18/1000 | Loss: 0.00003120
Iteration 19/1000 | Loss: 0.00003120
Iteration 20/1000 | Loss: 0.00003120
Iteration 21/1000 | Loss: 0.00003120
Iteration 22/1000 | Loss: 0.00003120
Iteration 23/1000 | Loss: 0.00003120
Iteration 24/1000 | Loss: 0.00003120
Iteration 25/1000 | Loss: 0.00003120
Iteration 26/1000 | Loss: 0.00003120
Iteration 27/1000 | Loss: 0.00003120
Iteration 28/1000 | Loss: 0.00003119
Iteration 29/1000 | Loss: 0.00003119
Iteration 30/1000 | Loss: 0.00003118
Iteration 31/1000 | Loss: 0.00003118
Iteration 32/1000 | Loss: 0.00003117
Iteration 33/1000 | Loss: 0.00003117
Iteration 34/1000 | Loss: 0.00003117
Iteration 35/1000 | Loss: 0.00003116
Iteration 36/1000 | Loss: 0.00003116
Iteration 37/1000 | Loss: 0.00003116
Iteration 38/1000 | Loss: 0.00003115
Iteration 39/1000 | Loss: 0.00003115
Iteration 40/1000 | Loss: 0.00003115
Iteration 41/1000 | Loss: 0.00003114
Iteration 42/1000 | Loss: 0.00003113
Iteration 43/1000 | Loss: 0.00003113
Iteration 44/1000 | Loss: 0.00003113
Iteration 45/1000 | Loss: 0.00003113
Iteration 46/1000 | Loss: 0.00003113
Iteration 47/1000 | Loss: 0.00003113
Iteration 48/1000 | Loss: 0.00003113
Iteration 49/1000 | Loss: 0.00003113
Iteration 50/1000 | Loss: 0.00003113
Iteration 51/1000 | Loss: 0.00003113
Iteration 52/1000 | Loss: 0.00003113
Iteration 53/1000 | Loss: 0.00003112
Iteration 54/1000 | Loss: 0.00003112
Iteration 55/1000 | Loss: 0.00003112
Iteration 56/1000 | Loss: 0.00003112
Iteration 57/1000 | Loss: 0.00003112
Iteration 58/1000 | Loss: 0.00003112
Iteration 59/1000 | Loss: 0.00003112
Iteration 60/1000 | Loss: 0.00003112
Iteration 61/1000 | Loss: 0.00003112
Iteration 62/1000 | Loss: 0.00003112
Iteration 63/1000 | Loss: 0.00003111
Iteration 64/1000 | Loss: 0.00003111
Iteration 65/1000 | Loss: 0.00003111
Iteration 66/1000 | Loss: 0.00003111
Iteration 67/1000 | Loss: 0.00003111
Iteration 68/1000 | Loss: 0.00003110
Iteration 69/1000 | Loss: 0.00003110
Iteration 70/1000 | Loss: 0.00003110
Iteration 71/1000 | Loss: 0.00003110
Iteration 72/1000 | Loss: 0.00003110
Iteration 73/1000 | Loss: 0.00003109
Iteration 74/1000 | Loss: 0.00003109
Iteration 75/1000 | Loss: 0.00003109
Iteration 76/1000 | Loss: 0.00003109
Iteration 77/1000 | Loss: 0.00003109
Iteration 78/1000 | Loss: 0.00003109
Iteration 79/1000 | Loss: 0.00003109
Iteration 80/1000 | Loss: 0.00003109
Iteration 81/1000 | Loss: 0.00003109
Iteration 82/1000 | Loss: 0.00003109
Iteration 83/1000 | Loss: 0.00003109
Iteration 84/1000 | Loss: 0.00003109
Iteration 85/1000 | Loss: 0.00003109
Iteration 86/1000 | Loss: 0.00003108
Iteration 87/1000 | Loss: 0.00003108
Iteration 88/1000 | Loss: 0.00003108
Iteration 89/1000 | Loss: 0.00003108
Iteration 90/1000 | Loss: 0.00003108
Iteration 91/1000 | Loss: 0.00003108
Iteration 92/1000 | Loss: 0.00003108
Iteration 93/1000 | Loss: 0.00003108
Iteration 94/1000 | Loss: 0.00003108
Iteration 95/1000 | Loss: 0.00003107
Iteration 96/1000 | Loss: 0.00003107
Iteration 97/1000 | Loss: 0.00003107
Iteration 98/1000 | Loss: 0.00003107
Iteration 99/1000 | Loss: 0.00003107
Iteration 100/1000 | Loss: 0.00003107
Iteration 101/1000 | Loss: 0.00003107
Iteration 102/1000 | Loss: 0.00003107
Iteration 103/1000 | Loss: 0.00003107
Iteration 104/1000 | Loss: 0.00003107
Iteration 105/1000 | Loss: 0.00003107
Iteration 106/1000 | Loss: 0.00003107
Iteration 107/1000 | Loss: 0.00003107
Iteration 108/1000 | Loss: 0.00003106
Iteration 109/1000 | Loss: 0.00003106
Iteration 110/1000 | Loss: 0.00003106
Iteration 111/1000 | Loss: 0.00003106
Iteration 112/1000 | Loss: 0.00003106
Iteration 113/1000 | Loss: 0.00003106
Iteration 114/1000 | Loss: 0.00003106
Iteration 115/1000 | Loss: 0.00003106
Iteration 116/1000 | Loss: 0.00003106
Iteration 117/1000 | Loss: 0.00003106
Iteration 118/1000 | Loss: 0.00003106
Iteration 119/1000 | Loss: 0.00003106
Iteration 120/1000 | Loss: 0.00003106
Iteration 121/1000 | Loss: 0.00003106
Iteration 122/1000 | Loss: 0.00003106
Iteration 123/1000 | Loss: 0.00003106
Iteration 124/1000 | Loss: 0.00003106
Iteration 125/1000 | Loss: 0.00003106
Iteration 126/1000 | Loss: 0.00003106
Iteration 127/1000 | Loss: 0.00003105
Iteration 128/1000 | Loss: 0.00003105
Iteration 129/1000 | Loss: 0.00003105
Iteration 130/1000 | Loss: 0.00003105
Iteration 131/1000 | Loss: 0.00003105
Iteration 132/1000 | Loss: 0.00003105
Iteration 133/1000 | Loss: 0.00003105
Iteration 134/1000 | Loss: 0.00003105
Iteration 135/1000 | Loss: 0.00003105
Iteration 136/1000 | Loss: 0.00003105
Iteration 137/1000 | Loss: 0.00003105
Iteration 138/1000 | Loss: 0.00003105
Iteration 139/1000 | Loss: 0.00003105
Iteration 140/1000 | Loss: 0.00003105
Iteration 141/1000 | Loss: 0.00003105
Iteration 142/1000 | Loss: 0.00003105
Iteration 143/1000 | Loss: 0.00003104
Iteration 144/1000 | Loss: 0.00003104
Iteration 145/1000 | Loss: 0.00003104
Iteration 146/1000 | Loss: 0.00003104
Iteration 147/1000 | Loss: 0.00003104
Iteration 148/1000 | Loss: 0.00003104
Iteration 149/1000 | Loss: 0.00003104
Iteration 150/1000 | Loss: 0.00003104
Iteration 151/1000 | Loss: 0.00003104
Iteration 152/1000 | Loss: 0.00003104
Iteration 153/1000 | Loss: 0.00003104
Iteration 154/1000 | Loss: 0.00003104
Iteration 155/1000 | Loss: 0.00003104
Iteration 156/1000 | Loss: 0.00003104
Iteration 157/1000 | Loss: 0.00003104
Iteration 158/1000 | Loss: 0.00003104
Iteration 159/1000 | Loss: 0.00003104
Iteration 160/1000 | Loss: 0.00003104
Iteration 161/1000 | Loss: 0.00003104
Iteration 162/1000 | Loss: 0.00003104
Iteration 163/1000 | Loss: 0.00003104
Iteration 164/1000 | Loss: 0.00003104
Iteration 165/1000 | Loss: 0.00003104
Iteration 166/1000 | Loss: 0.00003104
Iteration 167/1000 | Loss: 0.00003104
Iteration 168/1000 | Loss: 0.00003104
Iteration 169/1000 | Loss: 0.00003104
Iteration 170/1000 | Loss: 0.00003104
Iteration 171/1000 | Loss: 0.00003104
Iteration 172/1000 | Loss: 0.00003104
Iteration 173/1000 | Loss: 0.00003104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [3.1038820452522486e-05, 3.1038820452522486e-05, 3.1038820452522486e-05, 3.1038820452522486e-05, 3.1038820452522486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1038820452522486e-05

Optimization complete. Final v2v error: 4.610724925994873 mm

Highest mean error: 5.024826526641846 mm for frame 13

Lowest mean error: 4.256070613861084 mm for frame 32

Saving results

Total time: 35.82310891151428
