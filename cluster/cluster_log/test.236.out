Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=236, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13216-13271
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493440
Iteration 2/25 | Loss: 0.00128567
Iteration 3/25 | Loss: 0.00120535
Iteration 4/25 | Loss: 0.00119431
Iteration 5/25 | Loss: 0.00119148
Iteration 6/25 | Loss: 0.00119148
Iteration 7/25 | Loss: 0.00119148
Iteration 8/25 | Loss: 0.00119148
Iteration 9/25 | Loss: 0.00119148
Iteration 10/25 | Loss: 0.00119148
Iteration 11/25 | Loss: 0.00119148
Iteration 12/25 | Loss: 0.00119148
Iteration 13/25 | Loss: 0.00119148
Iteration 14/25 | Loss: 0.00119148
Iteration 15/25 | Loss: 0.00119148
Iteration 16/25 | Loss: 0.00119148
Iteration 17/25 | Loss: 0.00119148
Iteration 18/25 | Loss: 0.00119148
Iteration 19/25 | Loss: 0.00119148
Iteration 20/25 | Loss: 0.00119148
Iteration 21/25 | Loss: 0.00119148
Iteration 22/25 | Loss: 0.00119148
Iteration 23/25 | Loss: 0.00119148
Iteration 24/25 | Loss: 0.00119148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011914836941286922, 0.0011914836941286922, 0.0011914836941286922, 0.0011914836941286922, 0.0011914836941286922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011914836941286922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.24597740
Iteration 2/25 | Loss: 0.00093063
Iteration 3/25 | Loss: 0.00093061
Iteration 4/25 | Loss: 0.00093061
Iteration 5/25 | Loss: 0.00093061
Iteration 6/25 | Loss: 0.00093061
Iteration 7/25 | Loss: 0.00093061
Iteration 8/25 | Loss: 0.00093061
Iteration 9/25 | Loss: 0.00093061
Iteration 10/25 | Loss: 0.00093061
Iteration 11/25 | Loss: 0.00093061
Iteration 12/25 | Loss: 0.00093061
Iteration 13/25 | Loss: 0.00093061
Iteration 14/25 | Loss: 0.00093061
Iteration 15/25 | Loss: 0.00093061
Iteration 16/25 | Loss: 0.00093061
Iteration 17/25 | Loss: 0.00093061
Iteration 18/25 | Loss: 0.00093061
Iteration 19/25 | Loss: 0.00093061
Iteration 20/25 | Loss: 0.00093061
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000930611218791455, 0.000930611218791455, 0.000930611218791455, 0.000930611218791455, 0.000930611218791455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000930611218791455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093061
Iteration 2/1000 | Loss: 0.00002472
Iteration 3/1000 | Loss: 0.00001774
Iteration 4/1000 | Loss: 0.00001556
Iteration 5/1000 | Loss: 0.00001468
Iteration 6/1000 | Loss: 0.00001399
Iteration 7/1000 | Loss: 0.00001364
Iteration 8/1000 | Loss: 0.00001329
Iteration 9/1000 | Loss: 0.00001293
Iteration 10/1000 | Loss: 0.00001286
Iteration 11/1000 | Loss: 0.00001273
Iteration 12/1000 | Loss: 0.00001257
Iteration 13/1000 | Loss: 0.00001245
Iteration 14/1000 | Loss: 0.00001243
Iteration 15/1000 | Loss: 0.00001241
Iteration 16/1000 | Loss: 0.00001234
Iteration 17/1000 | Loss: 0.00001225
Iteration 18/1000 | Loss: 0.00001224
Iteration 19/1000 | Loss: 0.00001219
Iteration 20/1000 | Loss: 0.00001210
Iteration 21/1000 | Loss: 0.00001199
Iteration 22/1000 | Loss: 0.00001197
Iteration 23/1000 | Loss: 0.00001195
Iteration 24/1000 | Loss: 0.00001187
Iteration 25/1000 | Loss: 0.00001181
Iteration 26/1000 | Loss: 0.00001176
Iteration 27/1000 | Loss: 0.00001171
Iteration 28/1000 | Loss: 0.00001171
Iteration 29/1000 | Loss: 0.00001171
Iteration 30/1000 | Loss: 0.00001170
Iteration 31/1000 | Loss: 0.00001170
Iteration 32/1000 | Loss: 0.00001170
Iteration 33/1000 | Loss: 0.00001166
Iteration 34/1000 | Loss: 0.00001165
Iteration 35/1000 | Loss: 0.00001165
Iteration 36/1000 | Loss: 0.00001155
Iteration 37/1000 | Loss: 0.00001155
Iteration 38/1000 | Loss: 0.00001152
Iteration 39/1000 | Loss: 0.00001152
Iteration 40/1000 | Loss: 0.00001151
Iteration 41/1000 | Loss: 0.00001150
Iteration 42/1000 | Loss: 0.00001150
Iteration 43/1000 | Loss: 0.00001149
Iteration 44/1000 | Loss: 0.00001148
Iteration 45/1000 | Loss: 0.00001147
Iteration 46/1000 | Loss: 0.00001146
Iteration 47/1000 | Loss: 0.00001145
Iteration 48/1000 | Loss: 0.00001143
Iteration 49/1000 | Loss: 0.00001141
Iteration 50/1000 | Loss: 0.00001140
Iteration 51/1000 | Loss: 0.00001140
Iteration 52/1000 | Loss: 0.00001139
Iteration 53/1000 | Loss: 0.00001139
Iteration 54/1000 | Loss: 0.00001139
Iteration 55/1000 | Loss: 0.00001138
Iteration 56/1000 | Loss: 0.00001138
Iteration 57/1000 | Loss: 0.00001137
Iteration 58/1000 | Loss: 0.00001137
Iteration 59/1000 | Loss: 0.00001136
Iteration 60/1000 | Loss: 0.00001136
Iteration 61/1000 | Loss: 0.00001136
Iteration 62/1000 | Loss: 0.00001136
Iteration 63/1000 | Loss: 0.00001135
Iteration 64/1000 | Loss: 0.00001135
Iteration 65/1000 | Loss: 0.00001135
Iteration 66/1000 | Loss: 0.00001134
Iteration 67/1000 | Loss: 0.00001133
Iteration 68/1000 | Loss: 0.00001133
Iteration 69/1000 | Loss: 0.00001132
Iteration 70/1000 | Loss: 0.00001132
Iteration 71/1000 | Loss: 0.00001132
Iteration 72/1000 | Loss: 0.00001131
Iteration 73/1000 | Loss: 0.00001131
Iteration 74/1000 | Loss: 0.00001131
Iteration 75/1000 | Loss: 0.00001131
Iteration 76/1000 | Loss: 0.00001131
Iteration 77/1000 | Loss: 0.00001131
Iteration 78/1000 | Loss: 0.00001130
Iteration 79/1000 | Loss: 0.00001130
Iteration 80/1000 | Loss: 0.00001130
Iteration 81/1000 | Loss: 0.00001130
Iteration 82/1000 | Loss: 0.00001130
Iteration 83/1000 | Loss: 0.00001129
Iteration 84/1000 | Loss: 0.00001129
Iteration 85/1000 | Loss: 0.00001129
Iteration 86/1000 | Loss: 0.00001129
Iteration 87/1000 | Loss: 0.00001128
Iteration 88/1000 | Loss: 0.00001128
Iteration 89/1000 | Loss: 0.00001128
Iteration 90/1000 | Loss: 0.00001128
Iteration 91/1000 | Loss: 0.00001128
Iteration 92/1000 | Loss: 0.00001128
Iteration 93/1000 | Loss: 0.00001127
Iteration 94/1000 | Loss: 0.00001127
Iteration 95/1000 | Loss: 0.00001127
Iteration 96/1000 | Loss: 0.00001127
Iteration 97/1000 | Loss: 0.00001127
Iteration 98/1000 | Loss: 0.00001127
Iteration 99/1000 | Loss: 0.00001127
Iteration 100/1000 | Loss: 0.00001127
Iteration 101/1000 | Loss: 0.00001127
Iteration 102/1000 | Loss: 0.00001126
Iteration 103/1000 | Loss: 0.00001126
Iteration 104/1000 | Loss: 0.00001126
Iteration 105/1000 | Loss: 0.00001126
Iteration 106/1000 | Loss: 0.00001126
Iteration 107/1000 | Loss: 0.00001126
Iteration 108/1000 | Loss: 0.00001126
Iteration 109/1000 | Loss: 0.00001126
Iteration 110/1000 | Loss: 0.00001125
Iteration 111/1000 | Loss: 0.00001125
Iteration 112/1000 | Loss: 0.00001125
Iteration 113/1000 | Loss: 0.00001125
Iteration 114/1000 | Loss: 0.00001125
Iteration 115/1000 | Loss: 0.00001124
Iteration 116/1000 | Loss: 0.00001124
Iteration 117/1000 | Loss: 0.00001124
Iteration 118/1000 | Loss: 0.00001124
Iteration 119/1000 | Loss: 0.00001124
Iteration 120/1000 | Loss: 0.00001124
Iteration 121/1000 | Loss: 0.00001124
Iteration 122/1000 | Loss: 0.00001124
Iteration 123/1000 | Loss: 0.00001124
Iteration 124/1000 | Loss: 0.00001124
Iteration 125/1000 | Loss: 0.00001124
Iteration 126/1000 | Loss: 0.00001124
Iteration 127/1000 | Loss: 0.00001124
Iteration 128/1000 | Loss: 0.00001124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.12430507215322e-05, 1.12430507215322e-05, 1.12430507215322e-05, 1.12430507215322e-05, 1.12430507215322e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.12430507215322e-05

Optimization complete. Final v2v error: 2.909162998199463 mm

Highest mean error: 3.2417428493499756 mm for frame 215

Lowest mean error: 2.712665557861328 mm for frame 236

Saving results

Total time: 47.85214900970459
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970969
Iteration 2/25 | Loss: 0.00162033
Iteration 3/25 | Loss: 0.00142633
Iteration 4/25 | Loss: 0.00148028
Iteration 5/25 | Loss: 0.00138842
Iteration 6/25 | Loss: 0.00137271
Iteration 7/25 | Loss: 0.00135322
Iteration 8/25 | Loss: 0.00134919
Iteration 9/25 | Loss: 0.00133906
Iteration 10/25 | Loss: 0.00133013
Iteration 11/25 | Loss: 0.00132726
Iteration 12/25 | Loss: 0.00132834
Iteration 13/25 | Loss: 0.00132658
Iteration 14/25 | Loss: 0.00132340
Iteration 15/25 | Loss: 0.00132228
Iteration 16/25 | Loss: 0.00132129
Iteration 17/25 | Loss: 0.00132160
Iteration 18/25 | Loss: 0.00132036
Iteration 19/25 | Loss: 0.00131986
Iteration 20/25 | Loss: 0.00131979
Iteration 21/25 | Loss: 0.00131979
Iteration 22/25 | Loss: 0.00131978
Iteration 23/25 | Loss: 0.00131978
Iteration 24/25 | Loss: 0.00131977
Iteration 25/25 | Loss: 0.00131977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27231693
Iteration 2/25 | Loss: 0.00129993
Iteration 3/25 | Loss: 0.00129990
Iteration 4/25 | Loss: 0.00129990
Iteration 5/25 | Loss: 0.00129990
Iteration 6/25 | Loss: 0.00129990
Iteration 7/25 | Loss: 0.00129990
Iteration 8/25 | Loss: 0.00129990
Iteration 9/25 | Loss: 0.00129990
Iteration 10/25 | Loss: 0.00129990
Iteration 11/25 | Loss: 0.00129990
Iteration 12/25 | Loss: 0.00129990
Iteration 13/25 | Loss: 0.00129990
Iteration 14/25 | Loss: 0.00129990
Iteration 15/25 | Loss: 0.00129990
Iteration 16/25 | Loss: 0.00129990
Iteration 17/25 | Loss: 0.00129990
Iteration 18/25 | Loss: 0.00129990
Iteration 19/25 | Loss: 0.00129990
Iteration 20/25 | Loss: 0.00129990
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012998987222090364, 0.0012998987222090364, 0.0012998987222090364, 0.0012998987222090364, 0.0012998987222090364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012998987222090364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129990
Iteration 2/1000 | Loss: 0.00005017
Iteration 3/1000 | Loss: 0.00003259
Iteration 4/1000 | Loss: 0.00002832
Iteration 5/1000 | Loss: 0.00002673
Iteration 6/1000 | Loss: 0.00002509
Iteration 7/1000 | Loss: 0.00002412
Iteration 8/1000 | Loss: 0.00002355
Iteration 9/1000 | Loss: 0.00002311
Iteration 10/1000 | Loss: 0.00002286
Iteration 11/1000 | Loss: 0.00002251
Iteration 12/1000 | Loss: 0.00002228
Iteration 13/1000 | Loss: 0.00002208
Iteration 14/1000 | Loss: 0.00002198
Iteration 15/1000 | Loss: 0.00002191
Iteration 16/1000 | Loss: 0.00002189
Iteration 17/1000 | Loss: 0.00002185
Iteration 18/1000 | Loss: 0.00002185
Iteration 19/1000 | Loss: 0.00002183
Iteration 20/1000 | Loss: 0.00002183
Iteration 21/1000 | Loss: 0.00002182
Iteration 22/1000 | Loss: 0.00002177
Iteration 23/1000 | Loss: 0.00002169
Iteration 24/1000 | Loss: 0.00002169
Iteration 25/1000 | Loss: 0.00002165
Iteration 26/1000 | Loss: 0.00002165
Iteration 27/1000 | Loss: 0.00002163
Iteration 28/1000 | Loss: 0.00002163
Iteration 29/1000 | Loss: 0.00002162
Iteration 30/1000 | Loss: 0.00002162
Iteration 31/1000 | Loss: 0.00002161
Iteration 32/1000 | Loss: 0.00002159
Iteration 33/1000 | Loss: 0.00002159
Iteration 34/1000 | Loss: 0.00002158
Iteration 35/1000 | Loss: 0.00002158
Iteration 36/1000 | Loss: 0.00002157
Iteration 37/1000 | Loss: 0.00002156
Iteration 38/1000 | Loss: 0.00002155
Iteration 39/1000 | Loss: 0.00002152
Iteration 40/1000 | Loss: 0.00002152
Iteration 41/1000 | Loss: 0.00002151
Iteration 42/1000 | Loss: 0.00002149
Iteration 43/1000 | Loss: 0.00002149
Iteration 44/1000 | Loss: 0.00002149
Iteration 45/1000 | Loss: 0.00002149
Iteration 46/1000 | Loss: 0.00002149
Iteration 47/1000 | Loss: 0.00002149
Iteration 48/1000 | Loss: 0.00002149
Iteration 49/1000 | Loss: 0.00002149
Iteration 50/1000 | Loss: 0.00002149
Iteration 51/1000 | Loss: 0.00002149
Iteration 52/1000 | Loss: 0.00002147
Iteration 53/1000 | Loss: 0.00002147
Iteration 54/1000 | Loss: 0.00002146
Iteration 55/1000 | Loss: 0.00002145
Iteration 56/1000 | Loss: 0.00002145
Iteration 57/1000 | Loss: 0.00002145
Iteration 58/1000 | Loss: 0.00002144
Iteration 59/1000 | Loss: 0.00002144
Iteration 60/1000 | Loss: 0.00002143
Iteration 61/1000 | Loss: 0.00002143
Iteration 62/1000 | Loss: 0.00002142
Iteration 63/1000 | Loss: 0.00002142
Iteration 64/1000 | Loss: 0.00002142
Iteration 65/1000 | Loss: 0.00002142
Iteration 66/1000 | Loss: 0.00002142
Iteration 67/1000 | Loss: 0.00002142
Iteration 68/1000 | Loss: 0.00002142
Iteration 69/1000 | Loss: 0.00002141
Iteration 70/1000 | Loss: 0.00002141
Iteration 71/1000 | Loss: 0.00002140
Iteration 72/1000 | Loss: 0.00002140
Iteration 73/1000 | Loss: 0.00002140
Iteration 74/1000 | Loss: 0.00002140
Iteration 75/1000 | Loss: 0.00002139
Iteration 76/1000 | Loss: 0.00002139
Iteration 77/1000 | Loss: 0.00002139
Iteration 78/1000 | Loss: 0.00002139
Iteration 79/1000 | Loss: 0.00002138
Iteration 80/1000 | Loss: 0.00002138
Iteration 81/1000 | Loss: 0.00002138
Iteration 82/1000 | Loss: 0.00002137
Iteration 83/1000 | Loss: 0.00002137
Iteration 84/1000 | Loss: 0.00002137
Iteration 85/1000 | Loss: 0.00002137
Iteration 86/1000 | Loss: 0.00002137
Iteration 87/1000 | Loss: 0.00002136
Iteration 88/1000 | Loss: 0.00002136
Iteration 89/1000 | Loss: 0.00002136
Iteration 90/1000 | Loss: 0.00002135
Iteration 91/1000 | Loss: 0.00002135
Iteration 92/1000 | Loss: 0.00002135
Iteration 93/1000 | Loss: 0.00002135
Iteration 94/1000 | Loss: 0.00002135
Iteration 95/1000 | Loss: 0.00002135
Iteration 96/1000 | Loss: 0.00002134
Iteration 97/1000 | Loss: 0.00002134
Iteration 98/1000 | Loss: 0.00002134
Iteration 99/1000 | Loss: 0.00002134
Iteration 100/1000 | Loss: 0.00002134
Iteration 101/1000 | Loss: 0.00002134
Iteration 102/1000 | Loss: 0.00002134
Iteration 103/1000 | Loss: 0.00002134
Iteration 104/1000 | Loss: 0.00002134
Iteration 105/1000 | Loss: 0.00002133
Iteration 106/1000 | Loss: 0.00002133
Iteration 107/1000 | Loss: 0.00002133
Iteration 108/1000 | Loss: 0.00002133
Iteration 109/1000 | Loss: 0.00002133
Iteration 110/1000 | Loss: 0.00002132
Iteration 111/1000 | Loss: 0.00002132
Iteration 112/1000 | Loss: 0.00002132
Iteration 113/1000 | Loss: 0.00002132
Iteration 114/1000 | Loss: 0.00002131
Iteration 115/1000 | Loss: 0.00002131
Iteration 116/1000 | Loss: 0.00002131
Iteration 117/1000 | Loss: 0.00002131
Iteration 118/1000 | Loss: 0.00002131
Iteration 119/1000 | Loss: 0.00002131
Iteration 120/1000 | Loss: 0.00002130
Iteration 121/1000 | Loss: 0.00002130
Iteration 122/1000 | Loss: 0.00002130
Iteration 123/1000 | Loss: 0.00002130
Iteration 124/1000 | Loss: 0.00002129
Iteration 125/1000 | Loss: 0.00002129
Iteration 126/1000 | Loss: 0.00002129
Iteration 127/1000 | Loss: 0.00002129
Iteration 128/1000 | Loss: 0.00002129
Iteration 129/1000 | Loss: 0.00002129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.1294381440384313e-05, 2.1294381440384313e-05, 2.1294381440384313e-05, 2.1294381440384313e-05, 2.1294381440384313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1294381440384313e-05

Optimization complete. Final v2v error: 3.7799651622772217 mm

Highest mean error: 4.768555164337158 mm for frame 78

Lowest mean error: 2.8738269805908203 mm for frame 168

Saving results

Total time: 73.4413514137268
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060608
Iteration 2/25 | Loss: 0.00258642
Iteration 3/25 | Loss: 0.00228615
Iteration 4/25 | Loss: 0.00188307
Iteration 5/25 | Loss: 0.00161811
Iteration 6/25 | Loss: 0.00141459
Iteration 7/25 | Loss: 0.00136169
Iteration 8/25 | Loss: 0.00135003
Iteration 9/25 | Loss: 0.00135317
Iteration 10/25 | Loss: 0.00132932
Iteration 11/25 | Loss: 0.00130652
Iteration 12/25 | Loss: 0.00129628
Iteration 13/25 | Loss: 0.00129509
Iteration 14/25 | Loss: 0.00129381
Iteration 15/25 | Loss: 0.00129353
Iteration 16/25 | Loss: 0.00129015
Iteration 17/25 | Loss: 0.00128806
Iteration 18/25 | Loss: 0.00128388
Iteration 19/25 | Loss: 0.00128711
Iteration 20/25 | Loss: 0.00128552
Iteration 21/25 | Loss: 0.00128401
Iteration 22/25 | Loss: 0.00128410
Iteration 23/25 | Loss: 0.00128349
Iteration 24/25 | Loss: 0.00127928
Iteration 25/25 | Loss: 0.00127601

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31156862
Iteration 2/25 | Loss: 0.00094778
Iteration 3/25 | Loss: 0.00094777
Iteration 4/25 | Loss: 0.00094777
Iteration 5/25 | Loss: 0.00094777
Iteration 6/25 | Loss: 0.00094777
Iteration 7/25 | Loss: 0.00094776
Iteration 8/25 | Loss: 0.00094776
Iteration 9/25 | Loss: 0.00094776
Iteration 10/25 | Loss: 0.00094776
Iteration 11/25 | Loss: 0.00094776
Iteration 12/25 | Loss: 0.00094776
Iteration 13/25 | Loss: 0.00094776
Iteration 14/25 | Loss: 0.00094776
Iteration 15/25 | Loss: 0.00094776
Iteration 16/25 | Loss: 0.00094776
Iteration 17/25 | Loss: 0.00094776
Iteration 18/25 | Loss: 0.00094776
Iteration 19/25 | Loss: 0.00094776
Iteration 20/25 | Loss: 0.00094776
Iteration 21/25 | Loss: 0.00094776
Iteration 22/25 | Loss: 0.00094776
Iteration 23/25 | Loss: 0.00094776
Iteration 24/25 | Loss: 0.00094776
Iteration 25/25 | Loss: 0.00094776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094776
Iteration 2/1000 | Loss: 0.00004043
Iteration 3/1000 | Loss: 0.00002296
Iteration 4/1000 | Loss: 0.00002044
Iteration 5/1000 | Loss: 0.00001887
Iteration 6/1000 | Loss: 0.00001812
Iteration 7/1000 | Loss: 0.00001737
Iteration 8/1000 | Loss: 0.00001699
Iteration 9/1000 | Loss: 0.00001650
Iteration 10/1000 | Loss: 0.00001617
Iteration 11/1000 | Loss: 0.00001594
Iteration 12/1000 | Loss: 0.00001574
Iteration 13/1000 | Loss: 0.00001564
Iteration 14/1000 | Loss: 0.00001552
Iteration 15/1000 | Loss: 0.00001551
Iteration 16/1000 | Loss: 0.00001550
Iteration 17/1000 | Loss: 0.00001550
Iteration 18/1000 | Loss: 0.00001542
Iteration 19/1000 | Loss: 0.00001539
Iteration 20/1000 | Loss: 0.00001538
Iteration 21/1000 | Loss: 0.00001536
Iteration 22/1000 | Loss: 0.00001535
Iteration 23/1000 | Loss: 0.00001535
Iteration 24/1000 | Loss: 0.00001525
Iteration 25/1000 | Loss: 0.00001525
Iteration 26/1000 | Loss: 0.00001525
Iteration 27/1000 | Loss: 0.00001525
Iteration 28/1000 | Loss: 0.00001525
Iteration 29/1000 | Loss: 0.00001525
Iteration 30/1000 | Loss: 0.00001525
Iteration 31/1000 | Loss: 0.00001525
Iteration 32/1000 | Loss: 0.00001525
Iteration 33/1000 | Loss: 0.00001525
Iteration 34/1000 | Loss: 0.00001525
Iteration 35/1000 | Loss: 0.00001525
Iteration 36/1000 | Loss: 0.00001525
Iteration 37/1000 | Loss: 0.00001523
Iteration 38/1000 | Loss: 0.00001522
Iteration 39/1000 | Loss: 0.00001522
Iteration 40/1000 | Loss: 0.00001522
Iteration 41/1000 | Loss: 0.00001522
Iteration 42/1000 | Loss: 0.00001521
Iteration 43/1000 | Loss: 0.00001521
Iteration 44/1000 | Loss: 0.00001521
Iteration 45/1000 | Loss: 0.00001520
Iteration 46/1000 | Loss: 0.00001519
Iteration 47/1000 | Loss: 0.00001519
Iteration 48/1000 | Loss: 0.00001518
Iteration 49/1000 | Loss: 0.00001518
Iteration 50/1000 | Loss: 0.00001518
Iteration 51/1000 | Loss: 0.00001517
Iteration 52/1000 | Loss: 0.00001517
Iteration 53/1000 | Loss: 0.00001517
Iteration 54/1000 | Loss: 0.00001516
Iteration 55/1000 | Loss: 0.00001516
Iteration 56/1000 | Loss: 0.00001516
Iteration 57/1000 | Loss: 0.00001515
Iteration 58/1000 | Loss: 0.00001515
Iteration 59/1000 | Loss: 0.00001514
Iteration 60/1000 | Loss: 0.00001514
Iteration 61/1000 | Loss: 0.00001514
Iteration 62/1000 | Loss: 0.00001514
Iteration 63/1000 | Loss: 0.00001513
Iteration 64/1000 | Loss: 0.00001513
Iteration 65/1000 | Loss: 0.00001513
Iteration 66/1000 | Loss: 0.00001513
Iteration 67/1000 | Loss: 0.00001512
Iteration 68/1000 | Loss: 0.00001512
Iteration 69/1000 | Loss: 0.00001511
Iteration 70/1000 | Loss: 0.00001511
Iteration 71/1000 | Loss: 0.00001510
Iteration 72/1000 | Loss: 0.00001510
Iteration 73/1000 | Loss: 0.00002277
Iteration 74/1000 | Loss: 0.00001856
Iteration 75/1000 | Loss: 0.00001558
Iteration 76/1000 | Loss: 0.00001520
Iteration 77/1000 | Loss: 0.00001519
Iteration 78/1000 | Loss: 0.00001518
Iteration 79/1000 | Loss: 0.00001993
Iteration 80/1000 | Loss: 0.00001615
Iteration 81/1000 | Loss: 0.00001538
Iteration 82/1000 | Loss: 0.00001505
Iteration 83/1000 | Loss: 0.00001500
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001499
Iteration 86/1000 | Loss: 0.00001498
Iteration 87/1000 | Loss: 0.00001498
Iteration 88/1000 | Loss: 0.00001498
Iteration 89/1000 | Loss: 0.00001498
Iteration 90/1000 | Loss: 0.00001497
Iteration 91/1000 | Loss: 0.00001497
Iteration 92/1000 | Loss: 0.00001497
Iteration 93/1000 | Loss: 0.00001496
Iteration 94/1000 | Loss: 0.00001496
Iteration 95/1000 | Loss: 0.00001495
Iteration 96/1000 | Loss: 0.00001495
Iteration 97/1000 | Loss: 0.00001494
Iteration 98/1000 | Loss: 0.00001494
Iteration 99/1000 | Loss: 0.00001494
Iteration 100/1000 | Loss: 0.00001494
Iteration 101/1000 | Loss: 0.00001494
Iteration 102/1000 | Loss: 0.00001494
Iteration 103/1000 | Loss: 0.00001493
Iteration 104/1000 | Loss: 0.00001493
Iteration 105/1000 | Loss: 0.00001493
Iteration 106/1000 | Loss: 0.00001493
Iteration 107/1000 | Loss: 0.00001493
Iteration 108/1000 | Loss: 0.00001493
Iteration 109/1000 | Loss: 0.00001493
Iteration 110/1000 | Loss: 0.00001492
Iteration 111/1000 | Loss: 0.00001492
Iteration 112/1000 | Loss: 0.00001492
Iteration 113/1000 | Loss: 0.00001492
Iteration 114/1000 | Loss: 0.00001492
Iteration 115/1000 | Loss: 0.00001492
Iteration 116/1000 | Loss: 0.00001492
Iteration 117/1000 | Loss: 0.00001492
Iteration 118/1000 | Loss: 0.00001492
Iteration 119/1000 | Loss: 0.00001492
Iteration 120/1000 | Loss: 0.00001492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.4924031347618438e-05, 1.4924031347618438e-05, 1.4924031347618438e-05, 1.4924031347618438e-05, 1.4924031347618438e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4924031347618438e-05

Optimization complete. Final v2v error: 3.27207088470459 mm

Highest mean error: 5.341012954711914 mm for frame 65

Lowest mean error: 3.079761505126953 mm for frame 194

Saving results

Total time: 96.06618118286133
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00916768
Iteration 2/25 | Loss: 0.00175265
Iteration 3/25 | Loss: 0.00139904
Iteration 4/25 | Loss: 0.00137056
Iteration 5/25 | Loss: 0.00136132
Iteration 6/25 | Loss: 0.00136027
Iteration 7/25 | Loss: 0.00136027
Iteration 8/25 | Loss: 0.00136027
Iteration 9/25 | Loss: 0.00136027
Iteration 10/25 | Loss: 0.00136027
Iteration 11/25 | Loss: 0.00136027
Iteration 12/25 | Loss: 0.00136027
Iteration 13/25 | Loss: 0.00136027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013602706603705883, 0.0013602706603705883, 0.0013602706603705883, 0.0013602706603705883, 0.0013602706603705883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013602706603705883

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96853477
Iteration 2/25 | Loss: 0.00117011
Iteration 3/25 | Loss: 0.00117011
Iteration 4/25 | Loss: 0.00117011
Iteration 5/25 | Loss: 0.00117011
Iteration 6/25 | Loss: 0.00117010
Iteration 7/25 | Loss: 0.00117010
Iteration 8/25 | Loss: 0.00117010
Iteration 9/25 | Loss: 0.00117010
Iteration 10/25 | Loss: 0.00117010
Iteration 11/25 | Loss: 0.00117010
Iteration 12/25 | Loss: 0.00117010
Iteration 13/25 | Loss: 0.00117010
Iteration 14/25 | Loss: 0.00117010
Iteration 15/25 | Loss: 0.00117010
Iteration 16/25 | Loss: 0.00117010
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011701033217832446, 0.0011701033217832446, 0.0011701033217832446, 0.0011701033217832446, 0.0011701033217832446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011701033217832446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117010
Iteration 2/1000 | Loss: 0.00007274
Iteration 3/1000 | Loss: 0.00004644
Iteration 4/1000 | Loss: 0.00004053
Iteration 5/1000 | Loss: 0.00003844
Iteration 6/1000 | Loss: 0.00003706
Iteration 7/1000 | Loss: 0.00003607
Iteration 8/1000 | Loss: 0.00003537
Iteration 9/1000 | Loss: 0.00003485
Iteration 10/1000 | Loss: 0.00003439
Iteration 11/1000 | Loss: 0.00003409
Iteration 12/1000 | Loss: 0.00003374
Iteration 13/1000 | Loss: 0.00003343
Iteration 14/1000 | Loss: 0.00003317
Iteration 15/1000 | Loss: 0.00003291
Iteration 16/1000 | Loss: 0.00003266
Iteration 17/1000 | Loss: 0.00003245
Iteration 18/1000 | Loss: 0.00003229
Iteration 19/1000 | Loss: 0.00003226
Iteration 20/1000 | Loss: 0.00003213
Iteration 21/1000 | Loss: 0.00003213
Iteration 22/1000 | Loss: 0.00003202
Iteration 23/1000 | Loss: 0.00003194
Iteration 24/1000 | Loss: 0.00003193
Iteration 25/1000 | Loss: 0.00003189
Iteration 26/1000 | Loss: 0.00003187
Iteration 27/1000 | Loss: 0.00003186
Iteration 28/1000 | Loss: 0.00003186
Iteration 29/1000 | Loss: 0.00003186
Iteration 30/1000 | Loss: 0.00003185
Iteration 31/1000 | Loss: 0.00003185
Iteration 32/1000 | Loss: 0.00003185
Iteration 33/1000 | Loss: 0.00003185
Iteration 34/1000 | Loss: 0.00003183
Iteration 35/1000 | Loss: 0.00003181
Iteration 36/1000 | Loss: 0.00003180
Iteration 37/1000 | Loss: 0.00003180
Iteration 38/1000 | Loss: 0.00003180
Iteration 39/1000 | Loss: 0.00003178
Iteration 40/1000 | Loss: 0.00003178
Iteration 41/1000 | Loss: 0.00003178
Iteration 42/1000 | Loss: 0.00003178
Iteration 43/1000 | Loss: 0.00003178
Iteration 44/1000 | Loss: 0.00003177
Iteration 45/1000 | Loss: 0.00003177
Iteration 46/1000 | Loss: 0.00003177
Iteration 47/1000 | Loss: 0.00003177
Iteration 48/1000 | Loss: 0.00003176
Iteration 49/1000 | Loss: 0.00003176
Iteration 50/1000 | Loss: 0.00003176
Iteration 51/1000 | Loss: 0.00003176
Iteration 52/1000 | Loss: 0.00003176
Iteration 53/1000 | Loss: 0.00003176
Iteration 54/1000 | Loss: 0.00003176
Iteration 55/1000 | Loss: 0.00003176
Iteration 56/1000 | Loss: 0.00003176
Iteration 57/1000 | Loss: 0.00003176
Iteration 58/1000 | Loss: 0.00003176
Iteration 59/1000 | Loss: 0.00003175
Iteration 60/1000 | Loss: 0.00003175
Iteration 61/1000 | Loss: 0.00003175
Iteration 62/1000 | Loss: 0.00003175
Iteration 63/1000 | Loss: 0.00003175
Iteration 64/1000 | Loss: 0.00003174
Iteration 65/1000 | Loss: 0.00003174
Iteration 66/1000 | Loss: 0.00003174
Iteration 67/1000 | Loss: 0.00003174
Iteration 68/1000 | Loss: 0.00003174
Iteration 69/1000 | Loss: 0.00003174
Iteration 70/1000 | Loss: 0.00003173
Iteration 71/1000 | Loss: 0.00003173
Iteration 72/1000 | Loss: 0.00003173
Iteration 73/1000 | Loss: 0.00003173
Iteration 74/1000 | Loss: 0.00003173
Iteration 75/1000 | Loss: 0.00003173
Iteration 76/1000 | Loss: 0.00003173
Iteration 77/1000 | Loss: 0.00003173
Iteration 78/1000 | Loss: 0.00003173
Iteration 79/1000 | Loss: 0.00003173
Iteration 80/1000 | Loss: 0.00003172
Iteration 81/1000 | Loss: 0.00003172
Iteration 82/1000 | Loss: 0.00003172
Iteration 83/1000 | Loss: 0.00003172
Iteration 84/1000 | Loss: 0.00003172
Iteration 85/1000 | Loss: 0.00003171
Iteration 86/1000 | Loss: 0.00003171
Iteration 87/1000 | Loss: 0.00003171
Iteration 88/1000 | Loss: 0.00003171
Iteration 89/1000 | Loss: 0.00003171
Iteration 90/1000 | Loss: 0.00003171
Iteration 91/1000 | Loss: 0.00003171
Iteration 92/1000 | Loss: 0.00003170
Iteration 93/1000 | Loss: 0.00003170
Iteration 94/1000 | Loss: 0.00003170
Iteration 95/1000 | Loss: 0.00003170
Iteration 96/1000 | Loss: 0.00003169
Iteration 97/1000 | Loss: 0.00003169
Iteration 98/1000 | Loss: 0.00003169
Iteration 99/1000 | Loss: 0.00003169
Iteration 100/1000 | Loss: 0.00003168
Iteration 101/1000 | Loss: 0.00003168
Iteration 102/1000 | Loss: 0.00003168
Iteration 103/1000 | Loss: 0.00003168
Iteration 104/1000 | Loss: 0.00003168
Iteration 105/1000 | Loss: 0.00003168
Iteration 106/1000 | Loss: 0.00003168
Iteration 107/1000 | Loss: 0.00003167
Iteration 108/1000 | Loss: 0.00003167
Iteration 109/1000 | Loss: 0.00003167
Iteration 110/1000 | Loss: 0.00003167
Iteration 111/1000 | Loss: 0.00003167
Iteration 112/1000 | Loss: 0.00003166
Iteration 113/1000 | Loss: 0.00003166
Iteration 114/1000 | Loss: 0.00003166
Iteration 115/1000 | Loss: 0.00003166
Iteration 116/1000 | Loss: 0.00003166
Iteration 117/1000 | Loss: 0.00003166
Iteration 118/1000 | Loss: 0.00003165
Iteration 119/1000 | Loss: 0.00003165
Iteration 120/1000 | Loss: 0.00003165
Iteration 121/1000 | Loss: 0.00003165
Iteration 122/1000 | Loss: 0.00003165
Iteration 123/1000 | Loss: 0.00003165
Iteration 124/1000 | Loss: 0.00003165
Iteration 125/1000 | Loss: 0.00003165
Iteration 126/1000 | Loss: 0.00003164
Iteration 127/1000 | Loss: 0.00003164
Iteration 128/1000 | Loss: 0.00003164
Iteration 129/1000 | Loss: 0.00003164
Iteration 130/1000 | Loss: 0.00003164
Iteration 131/1000 | Loss: 0.00003164
Iteration 132/1000 | Loss: 0.00003163
Iteration 133/1000 | Loss: 0.00003163
Iteration 134/1000 | Loss: 0.00003163
Iteration 135/1000 | Loss: 0.00003163
Iteration 136/1000 | Loss: 0.00003163
Iteration 137/1000 | Loss: 0.00003163
Iteration 138/1000 | Loss: 0.00003163
Iteration 139/1000 | Loss: 0.00003163
Iteration 140/1000 | Loss: 0.00003163
Iteration 141/1000 | Loss: 0.00003163
Iteration 142/1000 | Loss: 0.00003163
Iteration 143/1000 | Loss: 0.00003163
Iteration 144/1000 | Loss: 0.00003163
Iteration 145/1000 | Loss: 0.00003163
Iteration 146/1000 | Loss: 0.00003162
Iteration 147/1000 | Loss: 0.00003162
Iteration 148/1000 | Loss: 0.00003162
Iteration 149/1000 | Loss: 0.00003162
Iteration 150/1000 | Loss: 0.00003162
Iteration 151/1000 | Loss: 0.00003162
Iteration 152/1000 | Loss: 0.00003162
Iteration 153/1000 | Loss: 0.00003162
Iteration 154/1000 | Loss: 0.00003162
Iteration 155/1000 | Loss: 0.00003162
Iteration 156/1000 | Loss: 0.00003162
Iteration 157/1000 | Loss: 0.00003162
Iteration 158/1000 | Loss: 0.00003162
Iteration 159/1000 | Loss: 0.00003161
Iteration 160/1000 | Loss: 0.00003161
Iteration 161/1000 | Loss: 0.00003161
Iteration 162/1000 | Loss: 0.00003161
Iteration 163/1000 | Loss: 0.00003161
Iteration 164/1000 | Loss: 0.00003161
Iteration 165/1000 | Loss: 0.00003161
Iteration 166/1000 | Loss: 0.00003161
Iteration 167/1000 | Loss: 0.00003161
Iteration 168/1000 | Loss: 0.00003161
Iteration 169/1000 | Loss: 0.00003161
Iteration 170/1000 | Loss: 0.00003161
Iteration 171/1000 | Loss: 0.00003161
Iteration 172/1000 | Loss: 0.00003161
Iteration 173/1000 | Loss: 0.00003160
Iteration 174/1000 | Loss: 0.00003160
Iteration 175/1000 | Loss: 0.00003160
Iteration 176/1000 | Loss: 0.00003160
Iteration 177/1000 | Loss: 0.00003160
Iteration 178/1000 | Loss: 0.00003160
Iteration 179/1000 | Loss: 0.00003160
Iteration 180/1000 | Loss: 0.00003160
Iteration 181/1000 | Loss: 0.00003160
Iteration 182/1000 | Loss: 0.00003160
Iteration 183/1000 | Loss: 0.00003159
Iteration 184/1000 | Loss: 0.00003159
Iteration 185/1000 | Loss: 0.00003159
Iteration 186/1000 | Loss: 0.00003159
Iteration 187/1000 | Loss: 0.00003159
Iteration 188/1000 | Loss: 0.00003159
Iteration 189/1000 | Loss: 0.00003159
Iteration 190/1000 | Loss: 0.00003159
Iteration 191/1000 | Loss: 0.00003159
Iteration 192/1000 | Loss: 0.00003159
Iteration 193/1000 | Loss: 0.00003159
Iteration 194/1000 | Loss: 0.00003159
Iteration 195/1000 | Loss: 0.00003159
Iteration 196/1000 | Loss: 0.00003159
Iteration 197/1000 | Loss: 0.00003158
Iteration 198/1000 | Loss: 0.00003158
Iteration 199/1000 | Loss: 0.00003158
Iteration 200/1000 | Loss: 0.00003158
Iteration 201/1000 | Loss: 0.00003158
Iteration 202/1000 | Loss: 0.00003158
Iteration 203/1000 | Loss: 0.00003158
Iteration 204/1000 | Loss: 0.00003158
Iteration 205/1000 | Loss: 0.00003158
Iteration 206/1000 | Loss: 0.00003158
Iteration 207/1000 | Loss: 0.00003158
Iteration 208/1000 | Loss: 0.00003158
Iteration 209/1000 | Loss: 0.00003158
Iteration 210/1000 | Loss: 0.00003158
Iteration 211/1000 | Loss: 0.00003158
Iteration 212/1000 | Loss: 0.00003157
Iteration 213/1000 | Loss: 0.00003157
Iteration 214/1000 | Loss: 0.00003157
Iteration 215/1000 | Loss: 0.00003157
Iteration 216/1000 | Loss: 0.00003157
Iteration 217/1000 | Loss: 0.00003157
Iteration 218/1000 | Loss: 0.00003157
Iteration 219/1000 | Loss: 0.00003157
Iteration 220/1000 | Loss: 0.00003157
Iteration 221/1000 | Loss: 0.00003157
Iteration 222/1000 | Loss: 0.00003157
Iteration 223/1000 | Loss: 0.00003157
Iteration 224/1000 | Loss: 0.00003157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [3.157163519063033e-05, 3.157163519063033e-05, 3.157163519063033e-05, 3.157163519063033e-05, 3.157163519063033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.157163519063033e-05

Optimization complete. Final v2v error: 4.612441539764404 mm

Highest mean error: 5.766085624694824 mm for frame 118

Lowest mean error: 3.792616605758667 mm for frame 19

Saving results

Total time: 61.18077325820923
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00763081
Iteration 2/25 | Loss: 0.00152790
Iteration 3/25 | Loss: 0.00125914
Iteration 4/25 | Loss: 0.00123157
Iteration 5/25 | Loss: 0.00122701
Iteration 6/25 | Loss: 0.00122644
Iteration 7/25 | Loss: 0.00122644
Iteration 8/25 | Loss: 0.00122644
Iteration 9/25 | Loss: 0.00122644
Iteration 10/25 | Loss: 0.00122644
Iteration 11/25 | Loss: 0.00122644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012264440301805735, 0.0012264440301805735, 0.0012264440301805735, 0.0012264440301805735, 0.0012264440301805735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012264440301805735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31954277
Iteration 2/25 | Loss: 0.00083619
Iteration 3/25 | Loss: 0.00083616
Iteration 4/25 | Loss: 0.00083616
Iteration 5/25 | Loss: 0.00083616
Iteration 6/25 | Loss: 0.00083616
Iteration 7/25 | Loss: 0.00083616
Iteration 8/25 | Loss: 0.00083616
Iteration 9/25 | Loss: 0.00083616
Iteration 10/25 | Loss: 0.00083616
Iteration 11/25 | Loss: 0.00083616
Iteration 12/25 | Loss: 0.00083616
Iteration 13/25 | Loss: 0.00083616
Iteration 14/25 | Loss: 0.00083616
Iteration 15/25 | Loss: 0.00083616
Iteration 16/25 | Loss: 0.00083616
Iteration 17/25 | Loss: 0.00083616
Iteration 18/25 | Loss: 0.00083616
Iteration 19/25 | Loss: 0.00083616
Iteration 20/25 | Loss: 0.00083616
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008361568325199187, 0.0008361568325199187, 0.0008361568325199187, 0.0008361568325199187, 0.0008361568325199187]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008361568325199187

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083616
Iteration 2/1000 | Loss: 0.00003253
Iteration 3/1000 | Loss: 0.00002283
Iteration 4/1000 | Loss: 0.00001962
Iteration 5/1000 | Loss: 0.00001820
Iteration 6/1000 | Loss: 0.00001743
Iteration 7/1000 | Loss: 0.00001671
Iteration 8/1000 | Loss: 0.00001623
Iteration 9/1000 | Loss: 0.00001580
Iteration 10/1000 | Loss: 0.00001529
Iteration 11/1000 | Loss: 0.00001500
Iteration 12/1000 | Loss: 0.00001487
Iteration 13/1000 | Loss: 0.00001486
Iteration 14/1000 | Loss: 0.00001465
Iteration 15/1000 | Loss: 0.00001461
Iteration 16/1000 | Loss: 0.00001460
Iteration 17/1000 | Loss: 0.00001452
Iteration 18/1000 | Loss: 0.00001445
Iteration 19/1000 | Loss: 0.00001439
Iteration 20/1000 | Loss: 0.00001435
Iteration 21/1000 | Loss: 0.00001434
Iteration 22/1000 | Loss: 0.00001433
Iteration 23/1000 | Loss: 0.00001431
Iteration 24/1000 | Loss: 0.00001430
Iteration 25/1000 | Loss: 0.00001429
Iteration 26/1000 | Loss: 0.00001429
Iteration 27/1000 | Loss: 0.00001428
Iteration 28/1000 | Loss: 0.00001427
Iteration 29/1000 | Loss: 0.00001426
Iteration 30/1000 | Loss: 0.00001426
Iteration 31/1000 | Loss: 0.00001425
Iteration 32/1000 | Loss: 0.00001425
Iteration 33/1000 | Loss: 0.00001424
Iteration 34/1000 | Loss: 0.00001424
Iteration 35/1000 | Loss: 0.00001424
Iteration 36/1000 | Loss: 0.00001424
Iteration 37/1000 | Loss: 0.00001424
Iteration 38/1000 | Loss: 0.00001424
Iteration 39/1000 | Loss: 0.00001424
Iteration 40/1000 | Loss: 0.00001423
Iteration 41/1000 | Loss: 0.00001423
Iteration 42/1000 | Loss: 0.00001423
Iteration 43/1000 | Loss: 0.00001422
Iteration 44/1000 | Loss: 0.00001420
Iteration 45/1000 | Loss: 0.00001420
Iteration 46/1000 | Loss: 0.00001419
Iteration 47/1000 | Loss: 0.00001418
Iteration 48/1000 | Loss: 0.00001418
Iteration 49/1000 | Loss: 0.00001417
Iteration 50/1000 | Loss: 0.00001417
Iteration 51/1000 | Loss: 0.00001416
Iteration 52/1000 | Loss: 0.00001416
Iteration 53/1000 | Loss: 0.00001416
Iteration 54/1000 | Loss: 0.00001416
Iteration 55/1000 | Loss: 0.00001416
Iteration 56/1000 | Loss: 0.00001415
Iteration 57/1000 | Loss: 0.00001415
Iteration 58/1000 | Loss: 0.00001415
Iteration 59/1000 | Loss: 0.00001415
Iteration 60/1000 | Loss: 0.00001415
Iteration 61/1000 | Loss: 0.00001415
Iteration 62/1000 | Loss: 0.00001415
Iteration 63/1000 | Loss: 0.00001414
Iteration 64/1000 | Loss: 0.00001414
Iteration 65/1000 | Loss: 0.00001414
Iteration 66/1000 | Loss: 0.00001414
Iteration 67/1000 | Loss: 0.00001413
Iteration 68/1000 | Loss: 0.00001413
Iteration 69/1000 | Loss: 0.00001413
Iteration 70/1000 | Loss: 0.00001413
Iteration 71/1000 | Loss: 0.00001413
Iteration 72/1000 | Loss: 0.00001413
Iteration 73/1000 | Loss: 0.00001412
Iteration 74/1000 | Loss: 0.00001412
Iteration 75/1000 | Loss: 0.00001411
Iteration 76/1000 | Loss: 0.00001411
Iteration 77/1000 | Loss: 0.00001410
Iteration 78/1000 | Loss: 0.00001410
Iteration 79/1000 | Loss: 0.00001410
Iteration 80/1000 | Loss: 0.00001409
Iteration 81/1000 | Loss: 0.00001409
Iteration 82/1000 | Loss: 0.00001409
Iteration 83/1000 | Loss: 0.00001409
Iteration 84/1000 | Loss: 0.00001409
Iteration 85/1000 | Loss: 0.00001409
Iteration 86/1000 | Loss: 0.00001409
Iteration 87/1000 | Loss: 0.00001409
Iteration 88/1000 | Loss: 0.00001409
Iteration 89/1000 | Loss: 0.00001409
Iteration 90/1000 | Loss: 0.00001409
Iteration 91/1000 | Loss: 0.00001409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.409019205311779e-05, 1.409019205311779e-05, 1.409019205311779e-05, 1.409019205311779e-05, 1.409019205311779e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.409019205311779e-05

Optimization complete. Final v2v error: 3.1943609714508057 mm

Highest mean error: 3.8025734424591064 mm for frame 22

Lowest mean error: 2.8130390644073486 mm for frame 77

Saving results

Total time: 39.74631714820862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502315
Iteration 2/25 | Loss: 0.00136423
Iteration 3/25 | Loss: 0.00125715
Iteration 4/25 | Loss: 0.00120838
Iteration 5/25 | Loss: 0.00120330
Iteration 6/25 | Loss: 0.00120205
Iteration 7/25 | Loss: 0.00120193
Iteration 8/25 | Loss: 0.00120193
Iteration 9/25 | Loss: 0.00120193
Iteration 10/25 | Loss: 0.00120193
Iteration 11/25 | Loss: 0.00120193
Iteration 12/25 | Loss: 0.00120193
Iteration 13/25 | Loss: 0.00120193
Iteration 14/25 | Loss: 0.00120193
Iteration 15/25 | Loss: 0.00120193
Iteration 16/25 | Loss: 0.00120193
Iteration 17/25 | Loss: 0.00120193
Iteration 18/25 | Loss: 0.00120193
Iteration 19/25 | Loss: 0.00120193
Iteration 20/25 | Loss: 0.00120193
Iteration 21/25 | Loss: 0.00120193
Iteration 22/25 | Loss: 0.00120193
Iteration 23/25 | Loss: 0.00120193
Iteration 24/25 | Loss: 0.00120193
Iteration 25/25 | Loss: 0.00120193

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.91198206
Iteration 2/25 | Loss: 0.00096595
Iteration 3/25 | Loss: 0.00096591
Iteration 4/25 | Loss: 0.00096591
Iteration 5/25 | Loss: 0.00096591
Iteration 6/25 | Loss: 0.00096591
Iteration 7/25 | Loss: 0.00096591
Iteration 8/25 | Loss: 0.00096591
Iteration 9/25 | Loss: 0.00096591
Iteration 10/25 | Loss: 0.00096591
Iteration 11/25 | Loss: 0.00096591
Iteration 12/25 | Loss: 0.00096591
Iteration 13/25 | Loss: 0.00096591
Iteration 14/25 | Loss: 0.00096591
Iteration 15/25 | Loss: 0.00096591
Iteration 16/25 | Loss: 0.00096591
Iteration 17/25 | Loss: 0.00096591
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009659095085225999, 0.0009659095085225999, 0.0009659095085225999, 0.0009659095085225999, 0.0009659095085225999]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009659095085225999

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096591
Iteration 2/1000 | Loss: 0.00002770
Iteration 3/1000 | Loss: 0.00001903
Iteration 4/1000 | Loss: 0.00001639
Iteration 5/1000 | Loss: 0.00001516
Iteration 6/1000 | Loss: 0.00001439
Iteration 7/1000 | Loss: 0.00001399
Iteration 8/1000 | Loss: 0.00001374
Iteration 9/1000 | Loss: 0.00001345
Iteration 10/1000 | Loss: 0.00001318
Iteration 11/1000 | Loss: 0.00001300
Iteration 12/1000 | Loss: 0.00001292
Iteration 13/1000 | Loss: 0.00001289
Iteration 14/1000 | Loss: 0.00001283
Iteration 15/1000 | Loss: 0.00001282
Iteration 16/1000 | Loss: 0.00001282
Iteration 17/1000 | Loss: 0.00001280
Iteration 18/1000 | Loss: 0.00001273
Iteration 19/1000 | Loss: 0.00001272
Iteration 20/1000 | Loss: 0.00001271
Iteration 21/1000 | Loss: 0.00001268
Iteration 22/1000 | Loss: 0.00001268
Iteration 23/1000 | Loss: 0.00001267
Iteration 24/1000 | Loss: 0.00001267
Iteration 25/1000 | Loss: 0.00001266
Iteration 26/1000 | Loss: 0.00001263
Iteration 27/1000 | Loss: 0.00001263
Iteration 28/1000 | Loss: 0.00001262
Iteration 29/1000 | Loss: 0.00001258
Iteration 30/1000 | Loss: 0.00001258
Iteration 31/1000 | Loss: 0.00001256
Iteration 32/1000 | Loss: 0.00001256
Iteration 33/1000 | Loss: 0.00001255
Iteration 34/1000 | Loss: 0.00001253
Iteration 35/1000 | Loss: 0.00001253
Iteration 36/1000 | Loss: 0.00001253
Iteration 37/1000 | Loss: 0.00001253
Iteration 38/1000 | Loss: 0.00001253
Iteration 39/1000 | Loss: 0.00001253
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001252
Iteration 42/1000 | Loss: 0.00001252
Iteration 43/1000 | Loss: 0.00001251
Iteration 44/1000 | Loss: 0.00001251
Iteration 45/1000 | Loss: 0.00001251
Iteration 46/1000 | Loss: 0.00001251
Iteration 47/1000 | Loss: 0.00001250
Iteration 48/1000 | Loss: 0.00001250
Iteration 49/1000 | Loss: 0.00001249
Iteration 50/1000 | Loss: 0.00001249
Iteration 51/1000 | Loss: 0.00001248
Iteration 52/1000 | Loss: 0.00001248
Iteration 53/1000 | Loss: 0.00001248
Iteration 54/1000 | Loss: 0.00001247
Iteration 55/1000 | Loss: 0.00001247
Iteration 56/1000 | Loss: 0.00001247
Iteration 57/1000 | Loss: 0.00001247
Iteration 58/1000 | Loss: 0.00001246
Iteration 59/1000 | Loss: 0.00001246
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001246
Iteration 62/1000 | Loss: 0.00001246
Iteration 63/1000 | Loss: 0.00001245
Iteration 64/1000 | Loss: 0.00001245
Iteration 65/1000 | Loss: 0.00001245
Iteration 66/1000 | Loss: 0.00001244
Iteration 67/1000 | Loss: 0.00001244
Iteration 68/1000 | Loss: 0.00001244
Iteration 69/1000 | Loss: 0.00001243
Iteration 70/1000 | Loss: 0.00001243
Iteration 71/1000 | Loss: 0.00001243
Iteration 72/1000 | Loss: 0.00001243
Iteration 73/1000 | Loss: 0.00001243
Iteration 74/1000 | Loss: 0.00001242
Iteration 75/1000 | Loss: 0.00001242
Iteration 76/1000 | Loss: 0.00001242
Iteration 77/1000 | Loss: 0.00001241
Iteration 78/1000 | Loss: 0.00001241
Iteration 79/1000 | Loss: 0.00001241
Iteration 80/1000 | Loss: 0.00001241
Iteration 81/1000 | Loss: 0.00001241
Iteration 82/1000 | Loss: 0.00001241
Iteration 83/1000 | Loss: 0.00001241
Iteration 84/1000 | Loss: 0.00001241
Iteration 85/1000 | Loss: 0.00001240
Iteration 86/1000 | Loss: 0.00001240
Iteration 87/1000 | Loss: 0.00001240
Iteration 88/1000 | Loss: 0.00001240
Iteration 89/1000 | Loss: 0.00001240
Iteration 90/1000 | Loss: 0.00001239
Iteration 91/1000 | Loss: 0.00001239
Iteration 92/1000 | Loss: 0.00001239
Iteration 93/1000 | Loss: 0.00001239
Iteration 94/1000 | Loss: 0.00001239
Iteration 95/1000 | Loss: 0.00001239
Iteration 96/1000 | Loss: 0.00001239
Iteration 97/1000 | Loss: 0.00001239
Iteration 98/1000 | Loss: 0.00001239
Iteration 99/1000 | Loss: 0.00001239
Iteration 100/1000 | Loss: 0.00001239
Iteration 101/1000 | Loss: 0.00001239
Iteration 102/1000 | Loss: 0.00001238
Iteration 103/1000 | Loss: 0.00001238
Iteration 104/1000 | Loss: 0.00001238
Iteration 105/1000 | Loss: 0.00001238
Iteration 106/1000 | Loss: 0.00001238
Iteration 107/1000 | Loss: 0.00001238
Iteration 108/1000 | Loss: 0.00001238
Iteration 109/1000 | Loss: 0.00001238
Iteration 110/1000 | Loss: 0.00001238
Iteration 111/1000 | Loss: 0.00001237
Iteration 112/1000 | Loss: 0.00001237
Iteration 113/1000 | Loss: 0.00001237
Iteration 114/1000 | Loss: 0.00001237
Iteration 115/1000 | Loss: 0.00001237
Iteration 116/1000 | Loss: 0.00001237
Iteration 117/1000 | Loss: 0.00001237
Iteration 118/1000 | Loss: 0.00001237
Iteration 119/1000 | Loss: 0.00001237
Iteration 120/1000 | Loss: 0.00001237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.237472224602243e-05, 1.237472224602243e-05, 1.237472224602243e-05, 1.237472224602243e-05, 1.237472224602243e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.237472224602243e-05

Optimization complete. Final v2v error: 2.972332239151001 mm

Highest mean error: 3.387216329574585 mm for frame 238

Lowest mean error: 2.645190715789795 mm for frame 131

Saving results

Total time: 43.350929498672485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992942
Iteration 2/25 | Loss: 0.00193365
Iteration 3/25 | Loss: 0.00183170
Iteration 4/25 | Loss: 0.00154158
Iteration 5/25 | Loss: 0.00149346
Iteration 6/25 | Loss: 0.00146643
Iteration 7/25 | Loss: 0.00140331
Iteration 8/25 | Loss: 0.00137388
Iteration 9/25 | Loss: 0.00143751
Iteration 10/25 | Loss: 0.00136607
Iteration 11/25 | Loss: 0.00136045
Iteration 12/25 | Loss: 0.00135889
Iteration 13/25 | Loss: 0.00135848
Iteration 14/25 | Loss: 0.00135815
Iteration 15/25 | Loss: 0.00136300
Iteration 16/25 | Loss: 0.00136185
Iteration 17/25 | Loss: 0.00136265
Iteration 18/25 | Loss: 0.00135950
Iteration 19/25 | Loss: 0.00136446
Iteration 20/25 | Loss: 0.00136094
Iteration 21/25 | Loss: 0.00136018
Iteration 22/25 | Loss: 0.00135929
Iteration 23/25 | Loss: 0.00136195
Iteration 24/25 | Loss: 0.00135818
Iteration 25/25 | Loss: 0.00135545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84234285
Iteration 2/25 | Loss: 0.00160928
Iteration 3/25 | Loss: 0.00160925
Iteration 4/25 | Loss: 0.00160925
Iteration 5/25 | Loss: 0.00160925
Iteration 6/25 | Loss: 0.00160925
Iteration 7/25 | Loss: 0.00160925
Iteration 8/25 | Loss: 0.00160925
Iteration 9/25 | Loss: 0.00160925
Iteration 10/25 | Loss: 0.00160925
Iteration 11/25 | Loss: 0.00160925
Iteration 12/25 | Loss: 0.00160925
Iteration 13/25 | Loss: 0.00160925
Iteration 14/25 | Loss: 0.00160925
Iteration 15/25 | Loss: 0.00160925
Iteration 16/25 | Loss: 0.00160925
Iteration 17/25 | Loss: 0.00160925
Iteration 18/25 | Loss: 0.00160925
Iteration 19/25 | Loss: 0.00160925
Iteration 20/25 | Loss: 0.00160925
Iteration 21/25 | Loss: 0.00160925
Iteration 22/25 | Loss: 0.00160925
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0016092490404844284, 0.0016092490404844284, 0.0016092490404844284, 0.0016092490404844284, 0.0016092490404844284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016092490404844284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160925
Iteration 2/1000 | Loss: 0.00018210
Iteration 3/1000 | Loss: 0.00012793
Iteration 4/1000 | Loss: 0.00010554
Iteration 5/1000 | Loss: 0.00009696
Iteration 6/1000 | Loss: 0.00009054
Iteration 7/1000 | Loss: 0.00094281
Iteration 8/1000 | Loss: 0.00143903
Iteration 9/1000 | Loss: 0.00100509
Iteration 10/1000 | Loss: 0.00011218
Iteration 11/1000 | Loss: 0.00008447
Iteration 12/1000 | Loss: 0.00007208
Iteration 13/1000 | Loss: 0.00006316
Iteration 14/1000 | Loss: 0.00005844
Iteration 15/1000 | Loss: 0.00005608
Iteration 16/1000 | Loss: 0.00058199
Iteration 17/1000 | Loss: 0.00007147
Iteration 18/1000 | Loss: 0.00005957
Iteration 19/1000 | Loss: 0.00005482
Iteration 20/1000 | Loss: 0.00029070
Iteration 21/1000 | Loss: 0.00092147
Iteration 22/1000 | Loss: 0.00063644
Iteration 23/1000 | Loss: 0.00029406
Iteration 24/1000 | Loss: 0.00118106
Iteration 25/1000 | Loss: 0.00079909
Iteration 26/1000 | Loss: 0.00073799
Iteration 27/1000 | Loss: 0.00085595
Iteration 28/1000 | Loss: 0.00060816
Iteration 29/1000 | Loss: 0.00053541
Iteration 30/1000 | Loss: 0.00034261
Iteration 31/1000 | Loss: 0.00007337
Iteration 32/1000 | Loss: 0.00005409
Iteration 33/1000 | Loss: 0.00004437
Iteration 34/1000 | Loss: 0.00029844
Iteration 35/1000 | Loss: 0.00035669
Iteration 36/1000 | Loss: 0.00028744
Iteration 37/1000 | Loss: 0.00039947
Iteration 38/1000 | Loss: 0.00003682
Iteration 39/1000 | Loss: 0.00003363
Iteration 40/1000 | Loss: 0.00003130
Iteration 41/1000 | Loss: 0.00056637
Iteration 42/1000 | Loss: 0.00017390
Iteration 43/1000 | Loss: 0.00048413
Iteration 44/1000 | Loss: 0.00003655
Iteration 45/1000 | Loss: 0.00003259
Iteration 46/1000 | Loss: 0.00003064
Iteration 47/1000 | Loss: 0.00002801
Iteration 48/1000 | Loss: 0.00002578
Iteration 49/1000 | Loss: 0.00027626
Iteration 50/1000 | Loss: 0.00053168
Iteration 51/1000 | Loss: 0.00041886
Iteration 52/1000 | Loss: 0.00038711
Iteration 53/1000 | Loss: 0.00025568
Iteration 54/1000 | Loss: 0.00094855
Iteration 55/1000 | Loss: 0.00050308
Iteration 56/1000 | Loss: 0.00007883
Iteration 57/1000 | Loss: 0.00004980
Iteration 58/1000 | Loss: 0.00011796
Iteration 59/1000 | Loss: 0.00003877
Iteration 60/1000 | Loss: 0.00003116
Iteration 61/1000 | Loss: 0.00002719
Iteration 62/1000 | Loss: 0.00002444
Iteration 63/1000 | Loss: 0.00002224
Iteration 64/1000 | Loss: 0.00002020
Iteration 65/1000 | Loss: 0.00001930
Iteration 66/1000 | Loss: 0.00001874
Iteration 67/1000 | Loss: 0.00001843
Iteration 68/1000 | Loss: 0.00001816
Iteration 69/1000 | Loss: 0.00001794
Iteration 70/1000 | Loss: 0.00001779
Iteration 71/1000 | Loss: 0.00001776
Iteration 72/1000 | Loss: 0.00001765
Iteration 73/1000 | Loss: 0.00001748
Iteration 74/1000 | Loss: 0.00001745
Iteration 75/1000 | Loss: 0.00001733
Iteration 76/1000 | Loss: 0.00001733
Iteration 77/1000 | Loss: 0.00001727
Iteration 78/1000 | Loss: 0.00001725
Iteration 79/1000 | Loss: 0.00001724
Iteration 80/1000 | Loss: 0.00001723
Iteration 81/1000 | Loss: 0.00001723
Iteration 82/1000 | Loss: 0.00001723
Iteration 83/1000 | Loss: 0.00001722
Iteration 84/1000 | Loss: 0.00001722
Iteration 85/1000 | Loss: 0.00001722
Iteration 86/1000 | Loss: 0.00001722
Iteration 87/1000 | Loss: 0.00001721
Iteration 88/1000 | Loss: 0.00001721
Iteration 89/1000 | Loss: 0.00001721
Iteration 90/1000 | Loss: 0.00001721
Iteration 91/1000 | Loss: 0.00001720
Iteration 92/1000 | Loss: 0.00001720
Iteration 93/1000 | Loss: 0.00001720
Iteration 94/1000 | Loss: 0.00001719
Iteration 95/1000 | Loss: 0.00001719
Iteration 96/1000 | Loss: 0.00001719
Iteration 97/1000 | Loss: 0.00001719
Iteration 98/1000 | Loss: 0.00001718
Iteration 99/1000 | Loss: 0.00001718
Iteration 100/1000 | Loss: 0.00001718
Iteration 101/1000 | Loss: 0.00001718
Iteration 102/1000 | Loss: 0.00001718
Iteration 103/1000 | Loss: 0.00001717
Iteration 104/1000 | Loss: 0.00001717
Iteration 105/1000 | Loss: 0.00001716
Iteration 106/1000 | Loss: 0.00001716
Iteration 107/1000 | Loss: 0.00001716
Iteration 108/1000 | Loss: 0.00001716
Iteration 109/1000 | Loss: 0.00001716
Iteration 110/1000 | Loss: 0.00001716
Iteration 111/1000 | Loss: 0.00001716
Iteration 112/1000 | Loss: 0.00001716
Iteration 113/1000 | Loss: 0.00001716
Iteration 114/1000 | Loss: 0.00001716
Iteration 115/1000 | Loss: 0.00001716
Iteration 116/1000 | Loss: 0.00001715
Iteration 117/1000 | Loss: 0.00001715
Iteration 118/1000 | Loss: 0.00001715
Iteration 119/1000 | Loss: 0.00001714
Iteration 120/1000 | Loss: 0.00001714
Iteration 121/1000 | Loss: 0.00001714
Iteration 122/1000 | Loss: 0.00001714
Iteration 123/1000 | Loss: 0.00001713
Iteration 124/1000 | Loss: 0.00001713
Iteration 125/1000 | Loss: 0.00001713
Iteration 126/1000 | Loss: 0.00001713
Iteration 127/1000 | Loss: 0.00001713
Iteration 128/1000 | Loss: 0.00001713
Iteration 129/1000 | Loss: 0.00001713
Iteration 130/1000 | Loss: 0.00001713
Iteration 131/1000 | Loss: 0.00001713
Iteration 132/1000 | Loss: 0.00001712
Iteration 133/1000 | Loss: 0.00001712
Iteration 134/1000 | Loss: 0.00001712
Iteration 135/1000 | Loss: 0.00001712
Iteration 136/1000 | Loss: 0.00001712
Iteration 137/1000 | Loss: 0.00001712
Iteration 138/1000 | Loss: 0.00001711
Iteration 139/1000 | Loss: 0.00001711
Iteration 140/1000 | Loss: 0.00001711
Iteration 141/1000 | Loss: 0.00001711
Iteration 142/1000 | Loss: 0.00001711
Iteration 143/1000 | Loss: 0.00001711
Iteration 144/1000 | Loss: 0.00001711
Iteration 145/1000 | Loss: 0.00001711
Iteration 146/1000 | Loss: 0.00001711
Iteration 147/1000 | Loss: 0.00001711
Iteration 148/1000 | Loss: 0.00001711
Iteration 149/1000 | Loss: 0.00001711
Iteration 150/1000 | Loss: 0.00001710
Iteration 151/1000 | Loss: 0.00001710
Iteration 152/1000 | Loss: 0.00001710
Iteration 153/1000 | Loss: 0.00001710
Iteration 154/1000 | Loss: 0.00001710
Iteration 155/1000 | Loss: 0.00001710
Iteration 156/1000 | Loss: 0.00001710
Iteration 157/1000 | Loss: 0.00001710
Iteration 158/1000 | Loss: 0.00001710
Iteration 159/1000 | Loss: 0.00001710
Iteration 160/1000 | Loss: 0.00001710
Iteration 161/1000 | Loss: 0.00001710
Iteration 162/1000 | Loss: 0.00001710
Iteration 163/1000 | Loss: 0.00001710
Iteration 164/1000 | Loss: 0.00001710
Iteration 165/1000 | Loss: 0.00001710
Iteration 166/1000 | Loss: 0.00001710
Iteration 167/1000 | Loss: 0.00001710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.7102678611990996e-05, 1.7102678611990996e-05, 1.7102678611990996e-05, 1.7102678611990996e-05, 1.7102678611990996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7102678611990996e-05

Optimization complete. Final v2v error: 3.4248156547546387 mm

Highest mean error: 4.241522312164307 mm for frame 88

Lowest mean error: 2.7338030338287354 mm for frame 33

Saving results

Total time: 153.21187448501587
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977493
Iteration 2/25 | Loss: 0.00344239
Iteration 3/25 | Loss: 0.00236884
Iteration 4/25 | Loss: 0.00213311
Iteration 5/25 | Loss: 0.00197023
Iteration 6/25 | Loss: 0.00186227
Iteration 7/25 | Loss: 0.00173970
Iteration 8/25 | Loss: 0.00171378
Iteration 9/25 | Loss: 0.00165255
Iteration 10/25 | Loss: 0.00162081
Iteration 11/25 | Loss: 0.00159395
Iteration 12/25 | Loss: 0.00157396
Iteration 13/25 | Loss: 0.00156944
Iteration 14/25 | Loss: 0.00150976
Iteration 15/25 | Loss: 0.00150454
Iteration 16/25 | Loss: 0.00149852
Iteration 17/25 | Loss: 0.00149247
Iteration 18/25 | Loss: 0.00147991
Iteration 19/25 | Loss: 0.00147574
Iteration 20/25 | Loss: 0.00147076
Iteration 21/25 | Loss: 0.00146907
Iteration 22/25 | Loss: 0.00146900
Iteration 23/25 | Loss: 0.00146864
Iteration 24/25 | Loss: 0.00146867
Iteration 25/25 | Loss: 0.00146981

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38439882
Iteration 2/25 | Loss: 0.00323583
Iteration 3/25 | Loss: 0.00252657
Iteration 4/25 | Loss: 0.00252657
Iteration 5/25 | Loss: 0.00252657
Iteration 6/25 | Loss: 0.00252657
Iteration 7/25 | Loss: 0.00252657
Iteration 8/25 | Loss: 0.00252657
Iteration 9/25 | Loss: 0.00252657
Iteration 10/25 | Loss: 0.00252657
Iteration 11/25 | Loss: 0.00252657
Iteration 12/25 | Loss: 0.00252657
Iteration 13/25 | Loss: 0.00252657
Iteration 14/25 | Loss: 0.00252657
Iteration 15/25 | Loss: 0.00252657
Iteration 16/25 | Loss: 0.00252657
Iteration 17/25 | Loss: 0.00252657
Iteration 18/25 | Loss: 0.00252657
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0025265654549002647, 0.0025265654549002647, 0.0025265654549002647, 0.0025265654549002647, 0.0025265654549002647]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025265654549002647

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00252657
Iteration 2/1000 | Loss: 0.00118673
Iteration 3/1000 | Loss: 0.00073196
Iteration 4/1000 | Loss: 0.00038292
Iteration 5/1000 | Loss: 0.00030479
Iteration 6/1000 | Loss: 0.00198337
Iteration 7/1000 | Loss: 0.00024476
Iteration 8/1000 | Loss: 0.00038939
Iteration 9/1000 | Loss: 0.00071715
Iteration 10/1000 | Loss: 0.00017974
Iteration 11/1000 | Loss: 0.00034387
Iteration 12/1000 | Loss: 0.00024382
Iteration 13/1000 | Loss: 0.00048867
Iteration 14/1000 | Loss: 0.00016104
Iteration 15/1000 | Loss: 0.00016725
Iteration 16/1000 | Loss: 0.00034710
Iteration 17/1000 | Loss: 0.00059660
Iteration 18/1000 | Loss: 0.00024926
Iteration 19/1000 | Loss: 0.00018826
Iteration 20/1000 | Loss: 0.00027156
Iteration 21/1000 | Loss: 0.00020140
Iteration 22/1000 | Loss: 0.00073068
Iteration 23/1000 | Loss: 0.00140820
Iteration 24/1000 | Loss: 0.00135791
Iteration 25/1000 | Loss: 0.00199731
Iteration 26/1000 | Loss: 0.00278252
Iteration 27/1000 | Loss: 0.00135577
Iteration 28/1000 | Loss: 0.00061632
Iteration 29/1000 | Loss: 0.00030691
Iteration 30/1000 | Loss: 0.00108875
Iteration 31/1000 | Loss: 0.00240760
Iteration 32/1000 | Loss: 0.00043343
Iteration 33/1000 | Loss: 0.00073650
Iteration 34/1000 | Loss: 0.00029451
Iteration 35/1000 | Loss: 0.00060805
Iteration 36/1000 | Loss: 0.00035300
Iteration 37/1000 | Loss: 0.00017118
Iteration 38/1000 | Loss: 0.00010206
Iteration 39/1000 | Loss: 0.00040062
Iteration 40/1000 | Loss: 0.00008941
Iteration 41/1000 | Loss: 0.00014991
Iteration 42/1000 | Loss: 0.00015553
Iteration 43/1000 | Loss: 0.00008492
Iteration 44/1000 | Loss: 0.00009334
Iteration 45/1000 | Loss: 0.00015313
Iteration 46/1000 | Loss: 0.00022736
Iteration 47/1000 | Loss: 0.00013329
Iteration 48/1000 | Loss: 0.00013515
Iteration 49/1000 | Loss: 0.00013714
Iteration 50/1000 | Loss: 0.00025694
Iteration 51/1000 | Loss: 0.00005170
Iteration 52/1000 | Loss: 0.00017522
Iteration 53/1000 | Loss: 0.00004084
Iteration 54/1000 | Loss: 0.00015556
Iteration 55/1000 | Loss: 0.00016328
Iteration 56/1000 | Loss: 0.00017728
Iteration 57/1000 | Loss: 0.00050783
Iteration 58/1000 | Loss: 0.00050226
Iteration 59/1000 | Loss: 0.00013963
Iteration 60/1000 | Loss: 0.00009293
Iteration 61/1000 | Loss: 0.00006097
Iteration 62/1000 | Loss: 0.00012749
Iteration 63/1000 | Loss: 0.00010063
Iteration 64/1000 | Loss: 0.00007694
Iteration 65/1000 | Loss: 0.00033337
Iteration 66/1000 | Loss: 0.00003664
Iteration 67/1000 | Loss: 0.00005902
Iteration 68/1000 | Loss: 0.00002684
Iteration 69/1000 | Loss: 0.00004248
Iteration 70/1000 | Loss: 0.00002548
Iteration 71/1000 | Loss: 0.00013789
Iteration 72/1000 | Loss: 0.00002567
Iteration 73/1000 | Loss: 0.00003182
Iteration 74/1000 | Loss: 0.00003681
Iteration 75/1000 | Loss: 0.00002579
Iteration 76/1000 | Loss: 0.00004169
Iteration 77/1000 | Loss: 0.00002525
Iteration 78/1000 | Loss: 0.00002569
Iteration 79/1000 | Loss: 0.00004555
Iteration 80/1000 | Loss: 0.00002129
Iteration 81/1000 | Loss: 0.00009637
Iteration 82/1000 | Loss: 0.00019155
Iteration 83/1000 | Loss: 0.00075374
Iteration 84/1000 | Loss: 0.00017674
Iteration 85/1000 | Loss: 0.00009946
Iteration 86/1000 | Loss: 0.00010352
Iteration 87/1000 | Loss: 0.00002763
Iteration 88/1000 | Loss: 0.00003167
Iteration 89/1000 | Loss: 0.00002796
Iteration 90/1000 | Loss: 0.00005613
Iteration 91/1000 | Loss: 0.00005343
Iteration 92/1000 | Loss: 0.00005830
Iteration 93/1000 | Loss: 0.00003252
Iteration 94/1000 | Loss: 0.00001870
Iteration 95/1000 | Loss: 0.00003160
Iteration 96/1000 | Loss: 0.00006684
Iteration 97/1000 | Loss: 0.00002569
Iteration 98/1000 | Loss: 0.00001794
Iteration 99/1000 | Loss: 0.00003925
Iteration 100/1000 | Loss: 0.00001779
Iteration 101/1000 | Loss: 0.00001765
Iteration 102/1000 | Loss: 0.00003674
Iteration 103/1000 | Loss: 0.00001902
Iteration 104/1000 | Loss: 0.00001949
Iteration 105/1000 | Loss: 0.00001729
Iteration 106/1000 | Loss: 0.00001729
Iteration 107/1000 | Loss: 0.00001729
Iteration 108/1000 | Loss: 0.00001729
Iteration 109/1000 | Loss: 0.00001729
Iteration 110/1000 | Loss: 0.00001729
Iteration 111/1000 | Loss: 0.00001729
Iteration 112/1000 | Loss: 0.00001729
Iteration 113/1000 | Loss: 0.00001729
Iteration 114/1000 | Loss: 0.00001728
Iteration 115/1000 | Loss: 0.00001728
Iteration 116/1000 | Loss: 0.00001728
Iteration 117/1000 | Loss: 0.00001728
Iteration 118/1000 | Loss: 0.00001728
Iteration 119/1000 | Loss: 0.00001899
Iteration 120/1000 | Loss: 0.00001720
Iteration 121/1000 | Loss: 0.00001720
Iteration 122/1000 | Loss: 0.00001720
Iteration 123/1000 | Loss: 0.00001720
Iteration 124/1000 | Loss: 0.00001720
Iteration 125/1000 | Loss: 0.00001720
Iteration 126/1000 | Loss: 0.00001719
Iteration 127/1000 | Loss: 0.00001719
Iteration 128/1000 | Loss: 0.00001719
Iteration 129/1000 | Loss: 0.00001719
Iteration 130/1000 | Loss: 0.00001715
Iteration 131/1000 | Loss: 0.00001715
Iteration 132/1000 | Loss: 0.00013717
Iteration 133/1000 | Loss: 0.00021786
Iteration 134/1000 | Loss: 0.00004467
Iteration 135/1000 | Loss: 0.00002800
Iteration 136/1000 | Loss: 0.00001968
Iteration 137/1000 | Loss: 0.00003410
Iteration 138/1000 | Loss: 0.00005675
Iteration 139/1000 | Loss: 0.00002198
Iteration 140/1000 | Loss: 0.00002514
Iteration 141/1000 | Loss: 0.00022246
Iteration 142/1000 | Loss: 0.00014673
Iteration 143/1000 | Loss: 0.00019737
Iteration 144/1000 | Loss: 0.00010538
Iteration 145/1000 | Loss: 0.00002715
Iteration 146/1000 | Loss: 0.00003668
Iteration 147/1000 | Loss: 0.00002282
Iteration 148/1000 | Loss: 0.00002177
Iteration 149/1000 | Loss: 0.00002006
Iteration 150/1000 | Loss: 0.00001985
Iteration 151/1000 | Loss: 0.00004223
Iteration 152/1000 | Loss: 0.00001981
Iteration 153/1000 | Loss: 0.00001979
Iteration 154/1000 | Loss: 0.00001978
Iteration 155/1000 | Loss: 0.00002657
Iteration 156/1000 | Loss: 0.00001976
Iteration 157/1000 | Loss: 0.00001975
Iteration 158/1000 | Loss: 0.00001974
Iteration 159/1000 | Loss: 0.00001973
Iteration 160/1000 | Loss: 0.00001973
Iteration 161/1000 | Loss: 0.00003453
Iteration 162/1000 | Loss: 0.00020401
Iteration 163/1000 | Loss: 0.00003104
Iteration 164/1000 | Loss: 0.00001843
Iteration 165/1000 | Loss: 0.00003301
Iteration 166/1000 | Loss: 0.00001776
Iteration 167/1000 | Loss: 0.00001740
Iteration 168/1000 | Loss: 0.00001997
Iteration 169/1000 | Loss: 0.00001708
Iteration 170/1000 | Loss: 0.00001700
Iteration 171/1000 | Loss: 0.00001681
Iteration 172/1000 | Loss: 0.00001673
Iteration 173/1000 | Loss: 0.00001669
Iteration 174/1000 | Loss: 0.00001668
Iteration 175/1000 | Loss: 0.00001668
Iteration 176/1000 | Loss: 0.00001668
Iteration 177/1000 | Loss: 0.00001668
Iteration 178/1000 | Loss: 0.00001668
Iteration 179/1000 | Loss: 0.00001668
Iteration 180/1000 | Loss: 0.00001668
Iteration 181/1000 | Loss: 0.00001668
Iteration 182/1000 | Loss: 0.00001668
Iteration 183/1000 | Loss: 0.00001668
Iteration 184/1000 | Loss: 0.00001668
Iteration 185/1000 | Loss: 0.00001667
Iteration 186/1000 | Loss: 0.00001667
Iteration 187/1000 | Loss: 0.00001667
Iteration 188/1000 | Loss: 0.00001667
Iteration 189/1000 | Loss: 0.00001667
Iteration 190/1000 | Loss: 0.00001666
Iteration 191/1000 | Loss: 0.00001666
Iteration 192/1000 | Loss: 0.00001666
Iteration 193/1000 | Loss: 0.00001666
Iteration 194/1000 | Loss: 0.00001666
Iteration 195/1000 | Loss: 0.00001666
Iteration 196/1000 | Loss: 0.00001666
Iteration 197/1000 | Loss: 0.00001665
Iteration 198/1000 | Loss: 0.00001665
Iteration 199/1000 | Loss: 0.00001665
Iteration 200/1000 | Loss: 0.00001665
Iteration 201/1000 | Loss: 0.00001665
Iteration 202/1000 | Loss: 0.00001665
Iteration 203/1000 | Loss: 0.00001665
Iteration 204/1000 | Loss: 0.00001665
Iteration 205/1000 | Loss: 0.00001664
Iteration 206/1000 | Loss: 0.00001664
Iteration 207/1000 | Loss: 0.00001664
Iteration 208/1000 | Loss: 0.00001664
Iteration 209/1000 | Loss: 0.00001664
Iteration 210/1000 | Loss: 0.00001664
Iteration 211/1000 | Loss: 0.00001664
Iteration 212/1000 | Loss: 0.00001664
Iteration 213/1000 | Loss: 0.00001664
Iteration 214/1000 | Loss: 0.00001664
Iteration 215/1000 | Loss: 0.00001664
Iteration 216/1000 | Loss: 0.00001664
Iteration 217/1000 | Loss: 0.00001664
Iteration 218/1000 | Loss: 0.00001663
Iteration 219/1000 | Loss: 0.00001663
Iteration 220/1000 | Loss: 0.00001663
Iteration 221/1000 | Loss: 0.00001663
Iteration 222/1000 | Loss: 0.00001663
Iteration 223/1000 | Loss: 0.00001663
Iteration 224/1000 | Loss: 0.00001663
Iteration 225/1000 | Loss: 0.00001663
Iteration 226/1000 | Loss: 0.00001663
Iteration 227/1000 | Loss: 0.00001663
Iteration 228/1000 | Loss: 0.00001663
Iteration 229/1000 | Loss: 0.00001663
Iteration 230/1000 | Loss: 0.00001663
Iteration 231/1000 | Loss: 0.00001663
Iteration 232/1000 | Loss: 0.00001663
Iteration 233/1000 | Loss: 0.00001663
Iteration 234/1000 | Loss: 0.00001663
Iteration 235/1000 | Loss: 0.00001663
Iteration 236/1000 | Loss: 0.00001663
Iteration 237/1000 | Loss: 0.00001663
Iteration 238/1000 | Loss: 0.00001662
Iteration 239/1000 | Loss: 0.00001662
Iteration 240/1000 | Loss: 0.00001662
Iteration 241/1000 | Loss: 0.00001662
Iteration 242/1000 | Loss: 0.00001662
Iteration 243/1000 | Loss: 0.00001662
Iteration 244/1000 | Loss: 0.00001662
Iteration 245/1000 | Loss: 0.00001662
Iteration 246/1000 | Loss: 0.00001662
Iteration 247/1000 | Loss: 0.00001662
Iteration 248/1000 | Loss: 0.00001662
Iteration 249/1000 | Loss: 0.00001662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.6622032489976846e-05, 1.6622032489976846e-05, 1.6622032489976846e-05, 1.6622032489976846e-05, 1.6622032489976846e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6622032489976846e-05

Optimization complete. Final v2v error: 3.1764867305755615 mm

Highest mean error: 10.802079200744629 mm for frame 170

Lowest mean error: 2.7571916580200195 mm for frame 151

Saving results

Total time: 281.611661195755
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994219
Iteration 2/25 | Loss: 0.00186443
Iteration 3/25 | Loss: 0.00182167
Iteration 4/25 | Loss: 0.00145034
Iteration 5/25 | Loss: 0.00141971
Iteration 6/25 | Loss: 0.00139397
Iteration 7/25 | Loss: 0.00135593
Iteration 8/25 | Loss: 0.00134526
Iteration 9/25 | Loss: 0.00135151
Iteration 10/25 | Loss: 0.00132656
Iteration 11/25 | Loss: 0.00130291
Iteration 12/25 | Loss: 0.00129532
Iteration 13/25 | Loss: 0.00131285
Iteration 14/25 | Loss: 0.00128062
Iteration 15/25 | Loss: 0.00127651
Iteration 16/25 | Loss: 0.00127714
Iteration 17/25 | Loss: 0.00127598
Iteration 18/25 | Loss: 0.00127411
Iteration 19/25 | Loss: 0.00127335
Iteration 20/25 | Loss: 0.00126938
Iteration 21/25 | Loss: 0.00127751
Iteration 22/25 | Loss: 0.00129670
Iteration 23/25 | Loss: 0.00126277
Iteration 24/25 | Loss: 0.00126937
Iteration 25/25 | Loss: 0.00126685

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37321270
Iteration 2/25 | Loss: 0.00222209
Iteration 3/25 | Loss: 0.00126595
Iteration 4/25 | Loss: 0.00126594
Iteration 5/25 | Loss: 0.00126594
Iteration 6/25 | Loss: 0.00126594
Iteration 7/25 | Loss: 0.00126594
Iteration 8/25 | Loss: 0.00126594
Iteration 9/25 | Loss: 0.00126594
Iteration 10/25 | Loss: 0.00126594
Iteration 11/25 | Loss: 0.00126594
Iteration 12/25 | Loss: 0.00126594
Iteration 13/25 | Loss: 0.00126594
Iteration 14/25 | Loss: 0.00126594
Iteration 15/25 | Loss: 0.00126594
Iteration 16/25 | Loss: 0.00126594
Iteration 17/25 | Loss: 0.00126594
Iteration 18/25 | Loss: 0.00126594
Iteration 19/25 | Loss: 0.00126594
Iteration 20/25 | Loss: 0.00126594
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012659396743401885, 0.0012659396743401885, 0.0012659396743401885, 0.0012659396743401885, 0.0012659396743401885]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012659396743401885

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126594
Iteration 2/1000 | Loss: 0.00271773
Iteration 3/1000 | Loss: 0.00511112
Iteration 4/1000 | Loss: 0.00321795
Iteration 5/1000 | Loss: 0.00207736
Iteration 6/1000 | Loss: 0.00232589
Iteration 7/1000 | Loss: 0.00160307
Iteration 8/1000 | Loss: 0.00208690
Iteration 9/1000 | Loss: 0.00086211
Iteration 10/1000 | Loss: 0.00232418
Iteration 11/1000 | Loss: 0.00286644
Iteration 12/1000 | Loss: 0.00151829
Iteration 13/1000 | Loss: 0.00010508
Iteration 14/1000 | Loss: 0.00183819
Iteration 15/1000 | Loss: 0.00203081
Iteration 16/1000 | Loss: 0.00036302
Iteration 17/1000 | Loss: 0.00013426
Iteration 18/1000 | Loss: 0.00013065
Iteration 19/1000 | Loss: 0.00006689
Iteration 20/1000 | Loss: 0.00253050
Iteration 21/1000 | Loss: 0.00303333
Iteration 22/1000 | Loss: 0.00168014
Iteration 23/1000 | Loss: 0.00227102
Iteration 24/1000 | Loss: 0.00157117
Iteration 25/1000 | Loss: 0.00022005
Iteration 26/1000 | Loss: 0.00031612
Iteration 27/1000 | Loss: 0.00063410
Iteration 28/1000 | Loss: 0.00027380
Iteration 29/1000 | Loss: 0.00009933
Iteration 30/1000 | Loss: 0.00019914
Iteration 31/1000 | Loss: 0.00012999
Iteration 32/1000 | Loss: 0.00020752
Iteration 33/1000 | Loss: 0.00004019
Iteration 34/1000 | Loss: 0.00035657
Iteration 35/1000 | Loss: 0.00029770
Iteration 36/1000 | Loss: 0.00031148
Iteration 37/1000 | Loss: 0.00211138
Iteration 38/1000 | Loss: 0.00057458
Iteration 39/1000 | Loss: 0.00189188
Iteration 40/1000 | Loss: 0.00129779
Iteration 41/1000 | Loss: 0.00121431
Iteration 42/1000 | Loss: 0.00161596
Iteration 43/1000 | Loss: 0.00016155
Iteration 44/1000 | Loss: 0.00192916
Iteration 45/1000 | Loss: 0.00031978
Iteration 46/1000 | Loss: 0.00015558
Iteration 47/1000 | Loss: 0.00013991
Iteration 48/1000 | Loss: 0.00023978
Iteration 49/1000 | Loss: 0.00007804
Iteration 50/1000 | Loss: 0.00008628
Iteration 51/1000 | Loss: 0.00003206
Iteration 52/1000 | Loss: 0.00009154
Iteration 53/1000 | Loss: 0.00010199
Iteration 54/1000 | Loss: 0.00022106
Iteration 55/1000 | Loss: 0.00041112
Iteration 56/1000 | Loss: 0.00017261
Iteration 57/1000 | Loss: 0.00009394
Iteration 58/1000 | Loss: 0.00009098
Iteration 59/1000 | Loss: 0.00023379
Iteration 60/1000 | Loss: 0.00011941
Iteration 61/1000 | Loss: 0.00020338
Iteration 62/1000 | Loss: 0.00012421
Iteration 63/1000 | Loss: 0.00009263
Iteration 64/1000 | Loss: 0.00012085
Iteration 65/1000 | Loss: 0.00010848
Iteration 66/1000 | Loss: 0.00009483
Iteration 67/1000 | Loss: 0.00009813
Iteration 68/1000 | Loss: 0.00009635
Iteration 69/1000 | Loss: 0.00007501
Iteration 70/1000 | Loss: 0.00024536
Iteration 71/1000 | Loss: 0.00007679
Iteration 72/1000 | Loss: 0.00014267
Iteration 73/1000 | Loss: 0.00007532
Iteration 74/1000 | Loss: 0.00013849
Iteration 75/1000 | Loss: 0.00007534
Iteration 76/1000 | Loss: 0.00012901
Iteration 77/1000 | Loss: 0.00010249
Iteration 78/1000 | Loss: 0.00009686
Iteration 79/1000 | Loss: 0.00009405
Iteration 80/1000 | Loss: 0.00006944
Iteration 81/1000 | Loss: 0.00013998
Iteration 82/1000 | Loss: 0.00010489
Iteration 83/1000 | Loss: 0.00016322
Iteration 84/1000 | Loss: 0.00021856
Iteration 85/1000 | Loss: 0.00015223
Iteration 86/1000 | Loss: 0.00024337
Iteration 87/1000 | Loss: 0.00006804
Iteration 88/1000 | Loss: 0.00006589
Iteration 89/1000 | Loss: 0.00004424
Iteration 90/1000 | Loss: 0.00007216
Iteration 91/1000 | Loss: 0.00001854
Iteration 92/1000 | Loss: 0.00001724
Iteration 93/1000 | Loss: 0.00001651
Iteration 94/1000 | Loss: 0.00002844
Iteration 95/1000 | Loss: 0.00001669
Iteration 96/1000 | Loss: 0.00001576
Iteration 97/1000 | Loss: 0.00001524
Iteration 98/1000 | Loss: 0.00001504
Iteration 99/1000 | Loss: 0.00001502
Iteration 100/1000 | Loss: 0.00001500
Iteration 101/1000 | Loss: 0.00001480
Iteration 102/1000 | Loss: 0.00001480
Iteration 103/1000 | Loss: 0.00001479
Iteration 104/1000 | Loss: 0.00001479
Iteration 105/1000 | Loss: 0.00001479
Iteration 106/1000 | Loss: 0.00001478
Iteration 107/1000 | Loss: 0.00001478
Iteration 108/1000 | Loss: 0.00001478
Iteration 109/1000 | Loss: 0.00001478
Iteration 110/1000 | Loss: 0.00001477
Iteration 111/1000 | Loss: 0.00001476
Iteration 112/1000 | Loss: 0.00001467
Iteration 113/1000 | Loss: 0.00001459
Iteration 114/1000 | Loss: 0.00001456
Iteration 115/1000 | Loss: 0.00001442
Iteration 116/1000 | Loss: 0.00001440
Iteration 117/1000 | Loss: 0.00001439
Iteration 118/1000 | Loss: 0.00001438
Iteration 119/1000 | Loss: 0.00001438
Iteration 120/1000 | Loss: 0.00001438
Iteration 121/1000 | Loss: 0.00001437
Iteration 122/1000 | Loss: 0.00001437
Iteration 123/1000 | Loss: 0.00001437
Iteration 124/1000 | Loss: 0.00001436
Iteration 125/1000 | Loss: 0.00001435
Iteration 126/1000 | Loss: 0.00001435
Iteration 127/1000 | Loss: 0.00001435
Iteration 128/1000 | Loss: 0.00001435
Iteration 129/1000 | Loss: 0.00001435
Iteration 130/1000 | Loss: 0.00001435
Iteration 131/1000 | Loss: 0.00001435
Iteration 132/1000 | Loss: 0.00001434
Iteration 133/1000 | Loss: 0.00001434
Iteration 134/1000 | Loss: 0.00001434
Iteration 135/1000 | Loss: 0.00001434
Iteration 136/1000 | Loss: 0.00001433
Iteration 137/1000 | Loss: 0.00001433
Iteration 138/1000 | Loss: 0.00001433
Iteration 139/1000 | Loss: 0.00001433
Iteration 140/1000 | Loss: 0.00001433
Iteration 141/1000 | Loss: 0.00001432
Iteration 142/1000 | Loss: 0.00001432
Iteration 143/1000 | Loss: 0.00001432
Iteration 144/1000 | Loss: 0.00001432
Iteration 145/1000 | Loss: 0.00001432
Iteration 146/1000 | Loss: 0.00001432
Iteration 147/1000 | Loss: 0.00001432
Iteration 148/1000 | Loss: 0.00001431
Iteration 149/1000 | Loss: 0.00001431
Iteration 150/1000 | Loss: 0.00001431
Iteration 151/1000 | Loss: 0.00001431
Iteration 152/1000 | Loss: 0.00001431
Iteration 153/1000 | Loss: 0.00001431
Iteration 154/1000 | Loss: 0.00001431
Iteration 155/1000 | Loss: 0.00001431
Iteration 156/1000 | Loss: 0.00001431
Iteration 157/1000 | Loss: 0.00001430
Iteration 158/1000 | Loss: 0.00001430
Iteration 159/1000 | Loss: 0.00001430
Iteration 160/1000 | Loss: 0.00001430
Iteration 161/1000 | Loss: 0.00001429
Iteration 162/1000 | Loss: 0.00001429
Iteration 163/1000 | Loss: 0.00001429
Iteration 164/1000 | Loss: 0.00001429
Iteration 165/1000 | Loss: 0.00001429
Iteration 166/1000 | Loss: 0.00001428
Iteration 167/1000 | Loss: 0.00001428
Iteration 168/1000 | Loss: 0.00001428
Iteration 169/1000 | Loss: 0.00001428
Iteration 170/1000 | Loss: 0.00001428
Iteration 171/1000 | Loss: 0.00001428
Iteration 172/1000 | Loss: 0.00001428
Iteration 173/1000 | Loss: 0.00001428
Iteration 174/1000 | Loss: 0.00001427
Iteration 175/1000 | Loss: 0.00001427
Iteration 176/1000 | Loss: 0.00001427
Iteration 177/1000 | Loss: 0.00001427
Iteration 178/1000 | Loss: 0.00001427
Iteration 179/1000 | Loss: 0.00001426
Iteration 180/1000 | Loss: 0.00001426
Iteration 181/1000 | Loss: 0.00001426
Iteration 182/1000 | Loss: 0.00001426
Iteration 183/1000 | Loss: 0.00001425
Iteration 184/1000 | Loss: 0.00001425
Iteration 185/1000 | Loss: 0.00001425
Iteration 186/1000 | Loss: 0.00001425
Iteration 187/1000 | Loss: 0.00001424
Iteration 188/1000 | Loss: 0.00001424
Iteration 189/1000 | Loss: 0.00001424
Iteration 190/1000 | Loss: 0.00001424
Iteration 191/1000 | Loss: 0.00001424
Iteration 192/1000 | Loss: 0.00001424
Iteration 193/1000 | Loss: 0.00001423
Iteration 194/1000 | Loss: 0.00001423
Iteration 195/1000 | Loss: 0.00001423
Iteration 196/1000 | Loss: 0.00001423
Iteration 197/1000 | Loss: 0.00001423
Iteration 198/1000 | Loss: 0.00001422
Iteration 199/1000 | Loss: 0.00001422
Iteration 200/1000 | Loss: 0.00001422
Iteration 201/1000 | Loss: 0.00001421
Iteration 202/1000 | Loss: 0.00001420
Iteration 203/1000 | Loss: 0.00001419
Iteration 204/1000 | Loss: 0.00001419
Iteration 205/1000 | Loss: 0.00001419
Iteration 206/1000 | Loss: 0.00001418
Iteration 207/1000 | Loss: 0.00001418
Iteration 208/1000 | Loss: 0.00001417
Iteration 209/1000 | Loss: 0.00001416
Iteration 210/1000 | Loss: 0.00001416
Iteration 211/1000 | Loss: 0.00001415
Iteration 212/1000 | Loss: 0.00001415
Iteration 213/1000 | Loss: 0.00001415
Iteration 214/1000 | Loss: 0.00001414
Iteration 215/1000 | Loss: 0.00001412
Iteration 216/1000 | Loss: 0.00001411
Iteration 217/1000 | Loss: 0.00001410
Iteration 218/1000 | Loss: 0.00001409
Iteration 219/1000 | Loss: 0.00001409
Iteration 220/1000 | Loss: 0.00001408
Iteration 221/1000 | Loss: 0.00003729
Iteration 222/1000 | Loss: 0.00003613
Iteration 223/1000 | Loss: 0.00004414
Iteration 224/1000 | Loss: 0.00002516
Iteration 225/1000 | Loss: 0.00003241
Iteration 226/1000 | Loss: 0.00003990
Iteration 227/1000 | Loss: 0.00002639
Iteration 228/1000 | Loss: 0.00003326
Iteration 229/1000 | Loss: 0.00004772
Iteration 230/1000 | Loss: 0.00002625
Iteration 231/1000 | Loss: 0.00001980
Iteration 232/1000 | Loss: 0.00001518
Iteration 233/1000 | Loss: 0.00003175
Iteration 234/1000 | Loss: 0.00004460
Iteration 235/1000 | Loss: 0.00002624
Iteration 236/1000 | Loss: 0.00002350
Iteration 237/1000 | Loss: 0.00003748
Iteration 238/1000 | Loss: 0.00002634
Iteration 239/1000 | Loss: 0.00001613
Iteration 240/1000 | Loss: 0.00001494
Iteration 241/1000 | Loss: 0.00003119
Iteration 242/1000 | Loss: 0.00003213
Iteration 243/1000 | Loss: 0.00025776
Iteration 244/1000 | Loss: 0.00002929
Iteration 245/1000 | Loss: 0.00002546
Iteration 246/1000 | Loss: 0.00003515
Iteration 247/1000 | Loss: 0.00003017
Iteration 248/1000 | Loss: 0.00002078
Iteration 249/1000 | Loss: 0.00003301
Iteration 250/1000 | Loss: 0.00002937
Iteration 251/1000 | Loss: 0.00002124
Iteration 252/1000 | Loss: 0.00003041
Iteration 253/1000 | Loss: 0.00002945
Iteration 254/1000 | Loss: 0.00002144
Iteration 255/1000 | Loss: 0.00001963
Iteration 256/1000 | Loss: 0.00001951
Iteration 257/1000 | Loss: 0.00003162
Iteration 258/1000 | Loss: 0.00003290
Iteration 259/1000 | Loss: 0.00004286
Iteration 260/1000 | Loss: 0.00003068
Iteration 261/1000 | Loss: 0.00004759
Iteration 262/1000 | Loss: 0.00002979
Iteration 263/1000 | Loss: 0.00004465
Iteration 264/1000 | Loss: 0.00002994
Iteration 265/1000 | Loss: 0.00004782
Iteration 266/1000 | Loss: 0.00002982
Iteration 267/1000 | Loss: 0.00004842
Iteration 268/1000 | Loss: 0.00002973
Iteration 269/1000 | Loss: 0.00002973
Iteration 270/1000 | Loss: 0.00004530
Iteration 271/1000 | Loss: 0.00002968
Iteration 272/1000 | Loss: 0.00004164
Iteration 273/1000 | Loss: 0.00002272
Iteration 274/1000 | Loss: 0.00003751
Iteration 275/1000 | Loss: 0.00001602
Iteration 276/1000 | Loss: 0.00001458
Iteration 277/1000 | Loss: 0.00001414
Iteration 278/1000 | Loss: 0.00001413
Iteration 279/1000 | Loss: 0.00001412
Iteration 280/1000 | Loss: 0.00001412
Iteration 281/1000 | Loss: 0.00001410
Iteration 282/1000 | Loss: 0.00001410
Iteration 283/1000 | Loss: 0.00001410
Iteration 284/1000 | Loss: 0.00001409
Iteration 285/1000 | Loss: 0.00001409
Iteration 286/1000 | Loss: 0.00001409
Iteration 287/1000 | Loss: 0.00001408
Iteration 288/1000 | Loss: 0.00001407
Iteration 289/1000 | Loss: 0.00001407
Iteration 290/1000 | Loss: 0.00001407
Iteration 291/1000 | Loss: 0.00001407
Iteration 292/1000 | Loss: 0.00001406
Iteration 293/1000 | Loss: 0.00001406
Iteration 294/1000 | Loss: 0.00001406
Iteration 295/1000 | Loss: 0.00001406
Iteration 296/1000 | Loss: 0.00001406
Iteration 297/1000 | Loss: 0.00001406
Iteration 298/1000 | Loss: 0.00001405
Iteration 299/1000 | Loss: 0.00001405
Iteration 300/1000 | Loss: 0.00001405
Iteration 301/1000 | Loss: 0.00001404
Iteration 302/1000 | Loss: 0.00001404
Iteration 303/1000 | Loss: 0.00001403
Iteration 304/1000 | Loss: 0.00001403
Iteration 305/1000 | Loss: 0.00001403
Iteration 306/1000 | Loss: 0.00001403
Iteration 307/1000 | Loss: 0.00001403
Iteration 308/1000 | Loss: 0.00001403
Iteration 309/1000 | Loss: 0.00001403
Iteration 310/1000 | Loss: 0.00001403
Iteration 311/1000 | Loss: 0.00001403
Iteration 312/1000 | Loss: 0.00001403
Iteration 313/1000 | Loss: 0.00001402
Iteration 314/1000 | Loss: 0.00001402
Iteration 315/1000 | Loss: 0.00001402
Iteration 316/1000 | Loss: 0.00001401
Iteration 317/1000 | Loss: 0.00001401
Iteration 318/1000 | Loss: 0.00001401
Iteration 319/1000 | Loss: 0.00001401
Iteration 320/1000 | Loss: 0.00001401
Iteration 321/1000 | Loss: 0.00001401
Iteration 322/1000 | Loss: 0.00001401
Iteration 323/1000 | Loss: 0.00001401
Iteration 324/1000 | Loss: 0.00001401
Iteration 325/1000 | Loss: 0.00001401
Iteration 326/1000 | Loss: 0.00001401
Iteration 327/1000 | Loss: 0.00001401
Iteration 328/1000 | Loss: 0.00001401
Iteration 329/1000 | Loss: 0.00001401
Iteration 330/1000 | Loss: 0.00001401
Iteration 331/1000 | Loss: 0.00001401
Iteration 332/1000 | Loss: 0.00001401
Iteration 333/1000 | Loss: 0.00001401
Iteration 334/1000 | Loss: 0.00001401
Iteration 335/1000 | Loss: 0.00001401
Iteration 336/1000 | Loss: 0.00001401
Iteration 337/1000 | Loss: 0.00001401
Iteration 338/1000 | Loss: 0.00001401
Iteration 339/1000 | Loss: 0.00001401
Iteration 340/1000 | Loss: 0.00001401
Iteration 341/1000 | Loss: 0.00001401
Iteration 342/1000 | Loss: 0.00001401
Iteration 343/1000 | Loss: 0.00001401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 343. Stopping optimization.
Last 5 losses: [1.401313602400478e-05, 1.401313602400478e-05, 1.401313602400478e-05, 1.401313602400478e-05, 1.401313602400478e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.401313602400478e-05

Optimization complete. Final v2v error: 3.1184263229370117 mm

Highest mean error: 6.529306888580322 mm for frame 70

Lowest mean error: 2.804917097091675 mm for frame 13

Saving results

Total time: 285.95075011253357
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416637
Iteration 2/25 | Loss: 0.00146232
Iteration 3/25 | Loss: 0.00125083
Iteration 4/25 | Loss: 0.00122938
Iteration 5/25 | Loss: 0.00122460
Iteration 6/25 | Loss: 0.00122350
Iteration 7/25 | Loss: 0.00122338
Iteration 8/25 | Loss: 0.00122338
Iteration 9/25 | Loss: 0.00122338
Iteration 10/25 | Loss: 0.00122338
Iteration 11/25 | Loss: 0.00122338
Iteration 12/25 | Loss: 0.00122338
Iteration 13/25 | Loss: 0.00122338
Iteration 14/25 | Loss: 0.00122338
Iteration 15/25 | Loss: 0.00122338
Iteration 16/25 | Loss: 0.00122338
Iteration 17/25 | Loss: 0.00122338
Iteration 18/25 | Loss: 0.00122338
Iteration 19/25 | Loss: 0.00122338
Iteration 20/25 | Loss: 0.00122338
Iteration 21/25 | Loss: 0.00122338
Iteration 22/25 | Loss: 0.00122338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012233818415552378, 0.0012233818415552378, 0.0012233818415552378, 0.0012233818415552378, 0.0012233818415552378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012233818415552378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32498193
Iteration 2/25 | Loss: 0.00085984
Iteration 3/25 | Loss: 0.00085984
Iteration 4/25 | Loss: 0.00085984
Iteration 5/25 | Loss: 0.00085984
Iteration 6/25 | Loss: 0.00085984
Iteration 7/25 | Loss: 0.00085984
Iteration 8/25 | Loss: 0.00085984
Iteration 9/25 | Loss: 0.00085984
Iteration 10/25 | Loss: 0.00085984
Iteration 11/25 | Loss: 0.00085984
Iteration 12/25 | Loss: 0.00085984
Iteration 13/25 | Loss: 0.00085984
Iteration 14/25 | Loss: 0.00085984
Iteration 15/25 | Loss: 0.00085984
Iteration 16/25 | Loss: 0.00085984
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008598362910561264, 0.0008598362910561264, 0.0008598362910561264, 0.0008598362910561264, 0.0008598362910561264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008598362910561264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085984
Iteration 2/1000 | Loss: 0.00003706
Iteration 3/1000 | Loss: 0.00002381
Iteration 4/1000 | Loss: 0.00002002
Iteration 5/1000 | Loss: 0.00001845
Iteration 6/1000 | Loss: 0.00001742
Iteration 7/1000 | Loss: 0.00001680
Iteration 8/1000 | Loss: 0.00001624
Iteration 9/1000 | Loss: 0.00001592
Iteration 10/1000 | Loss: 0.00001568
Iteration 11/1000 | Loss: 0.00001546
Iteration 12/1000 | Loss: 0.00001531
Iteration 13/1000 | Loss: 0.00001528
Iteration 14/1000 | Loss: 0.00001521
Iteration 15/1000 | Loss: 0.00001519
Iteration 16/1000 | Loss: 0.00001518
Iteration 17/1000 | Loss: 0.00001515
Iteration 18/1000 | Loss: 0.00001514
Iteration 19/1000 | Loss: 0.00001513
Iteration 20/1000 | Loss: 0.00001513
Iteration 21/1000 | Loss: 0.00001510
Iteration 22/1000 | Loss: 0.00001509
Iteration 23/1000 | Loss: 0.00001507
Iteration 24/1000 | Loss: 0.00001507
Iteration 25/1000 | Loss: 0.00001506
Iteration 26/1000 | Loss: 0.00001506
Iteration 27/1000 | Loss: 0.00001504
Iteration 28/1000 | Loss: 0.00001503
Iteration 29/1000 | Loss: 0.00001503
Iteration 30/1000 | Loss: 0.00001502
Iteration 31/1000 | Loss: 0.00001501
Iteration 32/1000 | Loss: 0.00001501
Iteration 33/1000 | Loss: 0.00001501
Iteration 34/1000 | Loss: 0.00001500
Iteration 35/1000 | Loss: 0.00001500
Iteration 36/1000 | Loss: 0.00001497
Iteration 37/1000 | Loss: 0.00001497
Iteration 38/1000 | Loss: 0.00001497
Iteration 39/1000 | Loss: 0.00001497
Iteration 40/1000 | Loss: 0.00001497
Iteration 41/1000 | Loss: 0.00001496
Iteration 42/1000 | Loss: 0.00001496
Iteration 43/1000 | Loss: 0.00001496
Iteration 44/1000 | Loss: 0.00001496
Iteration 45/1000 | Loss: 0.00001495
Iteration 46/1000 | Loss: 0.00001494
Iteration 47/1000 | Loss: 0.00001494
Iteration 48/1000 | Loss: 0.00001494
Iteration 49/1000 | Loss: 0.00001493
Iteration 50/1000 | Loss: 0.00001493
Iteration 51/1000 | Loss: 0.00001493
Iteration 52/1000 | Loss: 0.00001493
Iteration 53/1000 | Loss: 0.00001493
Iteration 54/1000 | Loss: 0.00001493
Iteration 55/1000 | Loss: 0.00001493
Iteration 56/1000 | Loss: 0.00001493
Iteration 57/1000 | Loss: 0.00001493
Iteration 58/1000 | Loss: 0.00001493
Iteration 59/1000 | Loss: 0.00001493
Iteration 60/1000 | Loss: 0.00001492
Iteration 61/1000 | Loss: 0.00001492
Iteration 62/1000 | Loss: 0.00001492
Iteration 63/1000 | Loss: 0.00001492
Iteration 64/1000 | Loss: 0.00001491
Iteration 65/1000 | Loss: 0.00001491
Iteration 66/1000 | Loss: 0.00001491
Iteration 67/1000 | Loss: 0.00001491
Iteration 68/1000 | Loss: 0.00001490
Iteration 69/1000 | Loss: 0.00001490
Iteration 70/1000 | Loss: 0.00001490
Iteration 71/1000 | Loss: 0.00001489
Iteration 72/1000 | Loss: 0.00001489
Iteration 73/1000 | Loss: 0.00001489
Iteration 74/1000 | Loss: 0.00001489
Iteration 75/1000 | Loss: 0.00001489
Iteration 76/1000 | Loss: 0.00001488
Iteration 77/1000 | Loss: 0.00001488
Iteration 78/1000 | Loss: 0.00001487
Iteration 79/1000 | Loss: 0.00001487
Iteration 80/1000 | Loss: 0.00001486
Iteration 81/1000 | Loss: 0.00001486
Iteration 82/1000 | Loss: 0.00001486
Iteration 83/1000 | Loss: 0.00001486
Iteration 84/1000 | Loss: 0.00001486
Iteration 85/1000 | Loss: 0.00001486
Iteration 86/1000 | Loss: 0.00001485
Iteration 87/1000 | Loss: 0.00001485
Iteration 88/1000 | Loss: 0.00001485
Iteration 89/1000 | Loss: 0.00001484
Iteration 90/1000 | Loss: 0.00001483
Iteration 91/1000 | Loss: 0.00001483
Iteration 92/1000 | Loss: 0.00001483
Iteration 93/1000 | Loss: 0.00001482
Iteration 94/1000 | Loss: 0.00001482
Iteration 95/1000 | Loss: 0.00001482
Iteration 96/1000 | Loss: 0.00001482
Iteration 97/1000 | Loss: 0.00001482
Iteration 98/1000 | Loss: 0.00001482
Iteration 99/1000 | Loss: 0.00001482
Iteration 100/1000 | Loss: 0.00001482
Iteration 101/1000 | Loss: 0.00001482
Iteration 102/1000 | Loss: 0.00001481
Iteration 103/1000 | Loss: 0.00001481
Iteration 104/1000 | Loss: 0.00001481
Iteration 105/1000 | Loss: 0.00001481
Iteration 106/1000 | Loss: 0.00001481
Iteration 107/1000 | Loss: 0.00001481
Iteration 108/1000 | Loss: 0.00001480
Iteration 109/1000 | Loss: 0.00001479
Iteration 110/1000 | Loss: 0.00001478
Iteration 111/1000 | Loss: 0.00001478
Iteration 112/1000 | Loss: 0.00001477
Iteration 113/1000 | Loss: 0.00001476
Iteration 114/1000 | Loss: 0.00001476
Iteration 115/1000 | Loss: 0.00001476
Iteration 116/1000 | Loss: 0.00001475
Iteration 117/1000 | Loss: 0.00001475
Iteration 118/1000 | Loss: 0.00001474
Iteration 119/1000 | Loss: 0.00001474
Iteration 120/1000 | Loss: 0.00001474
Iteration 121/1000 | Loss: 0.00001474
Iteration 122/1000 | Loss: 0.00001474
Iteration 123/1000 | Loss: 0.00001474
Iteration 124/1000 | Loss: 0.00001473
Iteration 125/1000 | Loss: 0.00001473
Iteration 126/1000 | Loss: 0.00001473
Iteration 127/1000 | Loss: 0.00001473
Iteration 128/1000 | Loss: 0.00001473
Iteration 129/1000 | Loss: 0.00001472
Iteration 130/1000 | Loss: 0.00001472
Iteration 131/1000 | Loss: 0.00001472
Iteration 132/1000 | Loss: 0.00001472
Iteration 133/1000 | Loss: 0.00001471
Iteration 134/1000 | Loss: 0.00001471
Iteration 135/1000 | Loss: 0.00001471
Iteration 136/1000 | Loss: 0.00001471
Iteration 137/1000 | Loss: 0.00001471
Iteration 138/1000 | Loss: 0.00001471
Iteration 139/1000 | Loss: 0.00001471
Iteration 140/1000 | Loss: 0.00001470
Iteration 141/1000 | Loss: 0.00001470
Iteration 142/1000 | Loss: 0.00001470
Iteration 143/1000 | Loss: 0.00001470
Iteration 144/1000 | Loss: 0.00001470
Iteration 145/1000 | Loss: 0.00001470
Iteration 146/1000 | Loss: 0.00001470
Iteration 147/1000 | Loss: 0.00001470
Iteration 148/1000 | Loss: 0.00001470
Iteration 149/1000 | Loss: 0.00001469
Iteration 150/1000 | Loss: 0.00001469
Iteration 151/1000 | Loss: 0.00001469
Iteration 152/1000 | Loss: 0.00001469
Iteration 153/1000 | Loss: 0.00001469
Iteration 154/1000 | Loss: 0.00001469
Iteration 155/1000 | Loss: 0.00001469
Iteration 156/1000 | Loss: 0.00001468
Iteration 157/1000 | Loss: 0.00001468
Iteration 158/1000 | Loss: 0.00001468
Iteration 159/1000 | Loss: 0.00001468
Iteration 160/1000 | Loss: 0.00001468
Iteration 161/1000 | Loss: 0.00001467
Iteration 162/1000 | Loss: 0.00001467
Iteration 163/1000 | Loss: 0.00001467
Iteration 164/1000 | Loss: 0.00001467
Iteration 165/1000 | Loss: 0.00001467
Iteration 166/1000 | Loss: 0.00001467
Iteration 167/1000 | Loss: 0.00001466
Iteration 168/1000 | Loss: 0.00001466
Iteration 169/1000 | Loss: 0.00001466
Iteration 170/1000 | Loss: 0.00001465
Iteration 171/1000 | Loss: 0.00001465
Iteration 172/1000 | Loss: 0.00001465
Iteration 173/1000 | Loss: 0.00001465
Iteration 174/1000 | Loss: 0.00001465
Iteration 175/1000 | Loss: 0.00001464
Iteration 176/1000 | Loss: 0.00001464
Iteration 177/1000 | Loss: 0.00001464
Iteration 178/1000 | Loss: 0.00001464
Iteration 179/1000 | Loss: 0.00001464
Iteration 180/1000 | Loss: 0.00001464
Iteration 181/1000 | Loss: 0.00001464
Iteration 182/1000 | Loss: 0.00001464
Iteration 183/1000 | Loss: 0.00001463
Iteration 184/1000 | Loss: 0.00001463
Iteration 185/1000 | Loss: 0.00001463
Iteration 186/1000 | Loss: 0.00001463
Iteration 187/1000 | Loss: 0.00001463
Iteration 188/1000 | Loss: 0.00001462
Iteration 189/1000 | Loss: 0.00001462
Iteration 190/1000 | Loss: 0.00001462
Iteration 191/1000 | Loss: 0.00001462
Iteration 192/1000 | Loss: 0.00001462
Iteration 193/1000 | Loss: 0.00001461
Iteration 194/1000 | Loss: 0.00001461
Iteration 195/1000 | Loss: 0.00001461
Iteration 196/1000 | Loss: 0.00001461
Iteration 197/1000 | Loss: 0.00001461
Iteration 198/1000 | Loss: 0.00001461
Iteration 199/1000 | Loss: 0.00001461
Iteration 200/1000 | Loss: 0.00001460
Iteration 201/1000 | Loss: 0.00001460
Iteration 202/1000 | Loss: 0.00001460
Iteration 203/1000 | Loss: 0.00001460
Iteration 204/1000 | Loss: 0.00001460
Iteration 205/1000 | Loss: 0.00001460
Iteration 206/1000 | Loss: 0.00001460
Iteration 207/1000 | Loss: 0.00001460
Iteration 208/1000 | Loss: 0.00001460
Iteration 209/1000 | Loss: 0.00001460
Iteration 210/1000 | Loss: 0.00001460
Iteration 211/1000 | Loss: 0.00001460
Iteration 212/1000 | Loss: 0.00001460
Iteration 213/1000 | Loss: 0.00001460
Iteration 214/1000 | Loss: 0.00001460
Iteration 215/1000 | Loss: 0.00001460
Iteration 216/1000 | Loss: 0.00001460
Iteration 217/1000 | Loss: 0.00001459
Iteration 218/1000 | Loss: 0.00001459
Iteration 219/1000 | Loss: 0.00001459
Iteration 220/1000 | Loss: 0.00001459
Iteration 221/1000 | Loss: 0.00001459
Iteration 222/1000 | Loss: 0.00001459
Iteration 223/1000 | Loss: 0.00001459
Iteration 224/1000 | Loss: 0.00001459
Iteration 225/1000 | Loss: 0.00001459
Iteration 226/1000 | Loss: 0.00001459
Iteration 227/1000 | Loss: 0.00001458
Iteration 228/1000 | Loss: 0.00001458
Iteration 229/1000 | Loss: 0.00001458
Iteration 230/1000 | Loss: 0.00001458
Iteration 231/1000 | Loss: 0.00001458
Iteration 232/1000 | Loss: 0.00001458
Iteration 233/1000 | Loss: 0.00001458
Iteration 234/1000 | Loss: 0.00001458
Iteration 235/1000 | Loss: 0.00001458
Iteration 236/1000 | Loss: 0.00001458
Iteration 237/1000 | Loss: 0.00001458
Iteration 238/1000 | Loss: 0.00001458
Iteration 239/1000 | Loss: 0.00001458
Iteration 240/1000 | Loss: 0.00001458
Iteration 241/1000 | Loss: 0.00001458
Iteration 242/1000 | Loss: 0.00001458
Iteration 243/1000 | Loss: 0.00001458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.4582695257558953e-05, 1.4582695257558953e-05, 1.4582695257558953e-05, 1.4582695257558953e-05, 1.4582695257558953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4582695257558953e-05

Optimization complete. Final v2v error: 3.2401299476623535 mm

Highest mean error: 4.012204170227051 mm for frame 72

Lowest mean error: 2.7629783153533936 mm for frame 13

Saving results

Total time: 43.95588803291321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432700
Iteration 2/25 | Loss: 0.00129721
Iteration 3/25 | Loss: 0.00121320
Iteration 4/25 | Loss: 0.00120241
Iteration 5/25 | Loss: 0.00119949
Iteration 6/25 | Loss: 0.00119885
Iteration 7/25 | Loss: 0.00119885
Iteration 8/25 | Loss: 0.00119885
Iteration 9/25 | Loss: 0.00119885
Iteration 10/25 | Loss: 0.00119885
Iteration 11/25 | Loss: 0.00119885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011988453334197402, 0.0011988453334197402, 0.0011988453334197402, 0.0011988453334197402, 0.0011988453334197402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011988453334197402

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.50318623
Iteration 2/25 | Loss: 0.00094180
Iteration 3/25 | Loss: 0.00094179
Iteration 4/25 | Loss: 0.00094179
Iteration 5/25 | Loss: 0.00094179
Iteration 6/25 | Loss: 0.00094179
Iteration 7/25 | Loss: 0.00094179
Iteration 8/25 | Loss: 0.00094179
Iteration 9/25 | Loss: 0.00094179
Iteration 10/25 | Loss: 0.00094179
Iteration 11/25 | Loss: 0.00094179
Iteration 12/25 | Loss: 0.00094179
Iteration 13/25 | Loss: 0.00094179
Iteration 14/25 | Loss: 0.00094179
Iteration 15/25 | Loss: 0.00094179
Iteration 16/25 | Loss: 0.00094179
Iteration 17/25 | Loss: 0.00094179
Iteration 18/25 | Loss: 0.00094178
Iteration 19/25 | Loss: 0.00094178
Iteration 20/25 | Loss: 0.00094178
Iteration 21/25 | Loss: 0.00094178
Iteration 22/25 | Loss: 0.00094178
Iteration 23/25 | Loss: 0.00094178
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009417849942110479, 0.0009417849942110479, 0.0009417849942110479, 0.0009417849942110479, 0.0009417849942110479]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009417849942110479

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094178
Iteration 2/1000 | Loss: 0.00002792
Iteration 3/1000 | Loss: 0.00001985
Iteration 4/1000 | Loss: 0.00001679
Iteration 5/1000 | Loss: 0.00001560
Iteration 6/1000 | Loss: 0.00001485
Iteration 7/1000 | Loss: 0.00001433
Iteration 8/1000 | Loss: 0.00001384
Iteration 9/1000 | Loss: 0.00001360
Iteration 10/1000 | Loss: 0.00001335
Iteration 11/1000 | Loss: 0.00001311
Iteration 12/1000 | Loss: 0.00001300
Iteration 13/1000 | Loss: 0.00001295
Iteration 14/1000 | Loss: 0.00001295
Iteration 15/1000 | Loss: 0.00001289
Iteration 16/1000 | Loss: 0.00001288
Iteration 17/1000 | Loss: 0.00001287
Iteration 18/1000 | Loss: 0.00001286
Iteration 19/1000 | Loss: 0.00001285
Iteration 20/1000 | Loss: 0.00001283
Iteration 21/1000 | Loss: 0.00001280
Iteration 22/1000 | Loss: 0.00001280
Iteration 23/1000 | Loss: 0.00001279
Iteration 24/1000 | Loss: 0.00001279
Iteration 25/1000 | Loss: 0.00001278
Iteration 26/1000 | Loss: 0.00001275
Iteration 27/1000 | Loss: 0.00001275
Iteration 28/1000 | Loss: 0.00001273
Iteration 29/1000 | Loss: 0.00001273
Iteration 30/1000 | Loss: 0.00001273
Iteration 31/1000 | Loss: 0.00001273
Iteration 32/1000 | Loss: 0.00001273
Iteration 33/1000 | Loss: 0.00001273
Iteration 34/1000 | Loss: 0.00001272
Iteration 35/1000 | Loss: 0.00001272
Iteration 36/1000 | Loss: 0.00001271
Iteration 37/1000 | Loss: 0.00001269
Iteration 38/1000 | Loss: 0.00001268
Iteration 39/1000 | Loss: 0.00001268
Iteration 40/1000 | Loss: 0.00001268
Iteration 41/1000 | Loss: 0.00001268
Iteration 42/1000 | Loss: 0.00001268
Iteration 43/1000 | Loss: 0.00001267
Iteration 44/1000 | Loss: 0.00001266
Iteration 45/1000 | Loss: 0.00001266
Iteration 46/1000 | Loss: 0.00001265
Iteration 47/1000 | Loss: 0.00001264
Iteration 48/1000 | Loss: 0.00001264
Iteration 49/1000 | Loss: 0.00001263
Iteration 50/1000 | Loss: 0.00001263
Iteration 51/1000 | Loss: 0.00001262
Iteration 52/1000 | Loss: 0.00001262
Iteration 53/1000 | Loss: 0.00001261
Iteration 54/1000 | Loss: 0.00001260
Iteration 55/1000 | Loss: 0.00001260
Iteration 56/1000 | Loss: 0.00001260
Iteration 57/1000 | Loss: 0.00001259
Iteration 58/1000 | Loss: 0.00001258
Iteration 59/1000 | Loss: 0.00001258
Iteration 60/1000 | Loss: 0.00001254
Iteration 61/1000 | Loss: 0.00001254
Iteration 62/1000 | Loss: 0.00001253
Iteration 63/1000 | Loss: 0.00001253
Iteration 64/1000 | Loss: 0.00001252
Iteration 65/1000 | Loss: 0.00001251
Iteration 66/1000 | Loss: 0.00001250
Iteration 67/1000 | Loss: 0.00001250
Iteration 68/1000 | Loss: 0.00001250
Iteration 69/1000 | Loss: 0.00001249
Iteration 70/1000 | Loss: 0.00001249
Iteration 71/1000 | Loss: 0.00001249
Iteration 72/1000 | Loss: 0.00001248
Iteration 73/1000 | Loss: 0.00001248
Iteration 74/1000 | Loss: 0.00001248
Iteration 75/1000 | Loss: 0.00001247
Iteration 76/1000 | Loss: 0.00001247
Iteration 77/1000 | Loss: 0.00001247
Iteration 78/1000 | Loss: 0.00001247
Iteration 79/1000 | Loss: 0.00001247
Iteration 80/1000 | Loss: 0.00001247
Iteration 81/1000 | Loss: 0.00001247
Iteration 82/1000 | Loss: 0.00001246
Iteration 83/1000 | Loss: 0.00001246
Iteration 84/1000 | Loss: 0.00001245
Iteration 85/1000 | Loss: 0.00001245
Iteration 86/1000 | Loss: 0.00001244
Iteration 87/1000 | Loss: 0.00001243
Iteration 88/1000 | Loss: 0.00001243
Iteration 89/1000 | Loss: 0.00001243
Iteration 90/1000 | Loss: 0.00001242
Iteration 91/1000 | Loss: 0.00001242
Iteration 92/1000 | Loss: 0.00001242
Iteration 93/1000 | Loss: 0.00001242
Iteration 94/1000 | Loss: 0.00001242
Iteration 95/1000 | Loss: 0.00001242
Iteration 96/1000 | Loss: 0.00001242
Iteration 97/1000 | Loss: 0.00001242
Iteration 98/1000 | Loss: 0.00001242
Iteration 99/1000 | Loss: 0.00001241
Iteration 100/1000 | Loss: 0.00001240
Iteration 101/1000 | Loss: 0.00001240
Iteration 102/1000 | Loss: 0.00001240
Iteration 103/1000 | Loss: 0.00001239
Iteration 104/1000 | Loss: 0.00001239
Iteration 105/1000 | Loss: 0.00001238
Iteration 106/1000 | Loss: 0.00001238
Iteration 107/1000 | Loss: 0.00001238
Iteration 108/1000 | Loss: 0.00001238
Iteration 109/1000 | Loss: 0.00001237
Iteration 110/1000 | Loss: 0.00001237
Iteration 111/1000 | Loss: 0.00001237
Iteration 112/1000 | Loss: 0.00001237
Iteration 113/1000 | Loss: 0.00001236
Iteration 114/1000 | Loss: 0.00001236
Iteration 115/1000 | Loss: 0.00001236
Iteration 116/1000 | Loss: 0.00001235
Iteration 117/1000 | Loss: 0.00001235
Iteration 118/1000 | Loss: 0.00001235
Iteration 119/1000 | Loss: 0.00001235
Iteration 120/1000 | Loss: 0.00001235
Iteration 121/1000 | Loss: 0.00001234
Iteration 122/1000 | Loss: 0.00001234
Iteration 123/1000 | Loss: 0.00001234
Iteration 124/1000 | Loss: 0.00001233
Iteration 125/1000 | Loss: 0.00001233
Iteration 126/1000 | Loss: 0.00001233
Iteration 127/1000 | Loss: 0.00001233
Iteration 128/1000 | Loss: 0.00001233
Iteration 129/1000 | Loss: 0.00001233
Iteration 130/1000 | Loss: 0.00001232
Iteration 131/1000 | Loss: 0.00001232
Iteration 132/1000 | Loss: 0.00001232
Iteration 133/1000 | Loss: 0.00001231
Iteration 134/1000 | Loss: 0.00001231
Iteration 135/1000 | Loss: 0.00001231
Iteration 136/1000 | Loss: 0.00001231
Iteration 137/1000 | Loss: 0.00001231
Iteration 138/1000 | Loss: 0.00001231
Iteration 139/1000 | Loss: 0.00001231
Iteration 140/1000 | Loss: 0.00001231
Iteration 141/1000 | Loss: 0.00001230
Iteration 142/1000 | Loss: 0.00001230
Iteration 143/1000 | Loss: 0.00001230
Iteration 144/1000 | Loss: 0.00001230
Iteration 145/1000 | Loss: 0.00001230
Iteration 146/1000 | Loss: 0.00001230
Iteration 147/1000 | Loss: 0.00001230
Iteration 148/1000 | Loss: 0.00001229
Iteration 149/1000 | Loss: 0.00001229
Iteration 150/1000 | Loss: 0.00001229
Iteration 151/1000 | Loss: 0.00001229
Iteration 152/1000 | Loss: 0.00001229
Iteration 153/1000 | Loss: 0.00001229
Iteration 154/1000 | Loss: 0.00001228
Iteration 155/1000 | Loss: 0.00001228
Iteration 156/1000 | Loss: 0.00001228
Iteration 157/1000 | Loss: 0.00001228
Iteration 158/1000 | Loss: 0.00001228
Iteration 159/1000 | Loss: 0.00001228
Iteration 160/1000 | Loss: 0.00001228
Iteration 161/1000 | Loss: 0.00001228
Iteration 162/1000 | Loss: 0.00001228
Iteration 163/1000 | Loss: 0.00001228
Iteration 164/1000 | Loss: 0.00001227
Iteration 165/1000 | Loss: 0.00001227
Iteration 166/1000 | Loss: 0.00001227
Iteration 167/1000 | Loss: 0.00001227
Iteration 168/1000 | Loss: 0.00001227
Iteration 169/1000 | Loss: 0.00001227
Iteration 170/1000 | Loss: 0.00001227
Iteration 171/1000 | Loss: 0.00001226
Iteration 172/1000 | Loss: 0.00001226
Iteration 173/1000 | Loss: 0.00001226
Iteration 174/1000 | Loss: 0.00001226
Iteration 175/1000 | Loss: 0.00001226
Iteration 176/1000 | Loss: 0.00001226
Iteration 177/1000 | Loss: 0.00001226
Iteration 178/1000 | Loss: 0.00001226
Iteration 179/1000 | Loss: 0.00001226
Iteration 180/1000 | Loss: 0.00001226
Iteration 181/1000 | Loss: 0.00001226
Iteration 182/1000 | Loss: 0.00001226
Iteration 183/1000 | Loss: 0.00001226
Iteration 184/1000 | Loss: 0.00001226
Iteration 185/1000 | Loss: 0.00001226
Iteration 186/1000 | Loss: 0.00001226
Iteration 187/1000 | Loss: 0.00001226
Iteration 188/1000 | Loss: 0.00001225
Iteration 189/1000 | Loss: 0.00001225
Iteration 190/1000 | Loss: 0.00001225
Iteration 191/1000 | Loss: 0.00001225
Iteration 192/1000 | Loss: 0.00001225
Iteration 193/1000 | Loss: 0.00001225
Iteration 194/1000 | Loss: 0.00001225
Iteration 195/1000 | Loss: 0.00001225
Iteration 196/1000 | Loss: 0.00001225
Iteration 197/1000 | Loss: 0.00001225
Iteration 198/1000 | Loss: 0.00001225
Iteration 199/1000 | Loss: 0.00001225
Iteration 200/1000 | Loss: 0.00001225
Iteration 201/1000 | Loss: 0.00001225
Iteration 202/1000 | Loss: 0.00001224
Iteration 203/1000 | Loss: 0.00001224
Iteration 204/1000 | Loss: 0.00001224
Iteration 205/1000 | Loss: 0.00001224
Iteration 206/1000 | Loss: 0.00001224
Iteration 207/1000 | Loss: 0.00001224
Iteration 208/1000 | Loss: 0.00001224
Iteration 209/1000 | Loss: 0.00001224
Iteration 210/1000 | Loss: 0.00001224
Iteration 211/1000 | Loss: 0.00001224
Iteration 212/1000 | Loss: 0.00001224
Iteration 213/1000 | Loss: 0.00001223
Iteration 214/1000 | Loss: 0.00001223
Iteration 215/1000 | Loss: 0.00001223
Iteration 216/1000 | Loss: 0.00001223
Iteration 217/1000 | Loss: 0.00001223
Iteration 218/1000 | Loss: 0.00001223
Iteration 219/1000 | Loss: 0.00001223
Iteration 220/1000 | Loss: 0.00001223
Iteration 221/1000 | Loss: 0.00001223
Iteration 222/1000 | Loss: 0.00001223
Iteration 223/1000 | Loss: 0.00001223
Iteration 224/1000 | Loss: 0.00001223
Iteration 225/1000 | Loss: 0.00001223
Iteration 226/1000 | Loss: 0.00001223
Iteration 227/1000 | Loss: 0.00001222
Iteration 228/1000 | Loss: 0.00001222
Iteration 229/1000 | Loss: 0.00001222
Iteration 230/1000 | Loss: 0.00001222
Iteration 231/1000 | Loss: 0.00001222
Iteration 232/1000 | Loss: 0.00001222
Iteration 233/1000 | Loss: 0.00001222
Iteration 234/1000 | Loss: 0.00001222
Iteration 235/1000 | Loss: 0.00001222
Iteration 236/1000 | Loss: 0.00001222
Iteration 237/1000 | Loss: 0.00001222
Iteration 238/1000 | Loss: 0.00001221
Iteration 239/1000 | Loss: 0.00001221
Iteration 240/1000 | Loss: 0.00001221
Iteration 241/1000 | Loss: 0.00001221
Iteration 242/1000 | Loss: 0.00001221
Iteration 243/1000 | Loss: 0.00001221
Iteration 244/1000 | Loss: 0.00001221
Iteration 245/1000 | Loss: 0.00001221
Iteration 246/1000 | Loss: 0.00001221
Iteration 247/1000 | Loss: 0.00001221
Iteration 248/1000 | Loss: 0.00001221
Iteration 249/1000 | Loss: 0.00001221
Iteration 250/1000 | Loss: 0.00001221
Iteration 251/1000 | Loss: 0.00001221
Iteration 252/1000 | Loss: 0.00001221
Iteration 253/1000 | Loss: 0.00001221
Iteration 254/1000 | Loss: 0.00001221
Iteration 255/1000 | Loss: 0.00001221
Iteration 256/1000 | Loss: 0.00001221
Iteration 257/1000 | Loss: 0.00001221
Iteration 258/1000 | Loss: 0.00001221
Iteration 259/1000 | Loss: 0.00001221
Iteration 260/1000 | Loss: 0.00001221
Iteration 261/1000 | Loss: 0.00001221
Iteration 262/1000 | Loss: 0.00001221
Iteration 263/1000 | Loss: 0.00001221
Iteration 264/1000 | Loss: 0.00001221
Iteration 265/1000 | Loss: 0.00001221
Iteration 266/1000 | Loss: 0.00001221
Iteration 267/1000 | Loss: 0.00001221
Iteration 268/1000 | Loss: 0.00001221
Iteration 269/1000 | Loss: 0.00001221
Iteration 270/1000 | Loss: 0.00001221
Iteration 271/1000 | Loss: 0.00001221
Iteration 272/1000 | Loss: 0.00001221
Iteration 273/1000 | Loss: 0.00001221
Iteration 274/1000 | Loss: 0.00001221
Iteration 275/1000 | Loss: 0.00001221
Iteration 276/1000 | Loss: 0.00001221
Iteration 277/1000 | Loss: 0.00001221
Iteration 278/1000 | Loss: 0.00001221
Iteration 279/1000 | Loss: 0.00001221
Iteration 280/1000 | Loss: 0.00001221
Iteration 281/1000 | Loss: 0.00001221
Iteration 282/1000 | Loss: 0.00001221
Iteration 283/1000 | Loss: 0.00001221
Iteration 284/1000 | Loss: 0.00001221
Iteration 285/1000 | Loss: 0.00001221
Iteration 286/1000 | Loss: 0.00001221
Iteration 287/1000 | Loss: 0.00001221
Iteration 288/1000 | Loss: 0.00001221
Iteration 289/1000 | Loss: 0.00001221
Iteration 290/1000 | Loss: 0.00001221
Iteration 291/1000 | Loss: 0.00001221
Iteration 292/1000 | Loss: 0.00001221
Iteration 293/1000 | Loss: 0.00001221
Iteration 294/1000 | Loss: 0.00001221
Iteration 295/1000 | Loss: 0.00001221
Iteration 296/1000 | Loss: 0.00001221
Iteration 297/1000 | Loss: 0.00001221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 297. Stopping optimization.
Last 5 losses: [1.2209809028718155e-05, 1.2209809028718155e-05, 1.2209809028718155e-05, 1.2209809028718155e-05, 1.2209809028718155e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2209809028718155e-05

Optimization complete. Final v2v error: 2.9802310466766357 mm

Highest mean error: 3.705026626586914 mm for frame 89

Lowest mean error: 2.6459720134735107 mm for frame 40

Saving results

Total time: 45.71095371246338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00929770
Iteration 2/25 | Loss: 0.00247145
Iteration 3/25 | Loss: 0.00189818
Iteration 4/25 | Loss: 0.00190947
Iteration 5/25 | Loss: 0.00175689
Iteration 6/25 | Loss: 0.00163498
Iteration 7/25 | Loss: 0.00150007
Iteration 8/25 | Loss: 0.00147024
Iteration 9/25 | Loss: 0.00144683
Iteration 10/25 | Loss: 0.00144308
Iteration 11/25 | Loss: 0.00143082
Iteration 12/25 | Loss: 0.00141446
Iteration 13/25 | Loss: 0.00141495
Iteration 14/25 | Loss: 0.00141100
Iteration 15/25 | Loss: 0.00141391
Iteration 16/25 | Loss: 0.00141328
Iteration 17/25 | Loss: 0.00141402
Iteration 18/25 | Loss: 0.00141144
Iteration 19/25 | Loss: 0.00141135
Iteration 20/25 | Loss: 0.00141501
Iteration 21/25 | Loss: 0.00140575
Iteration 22/25 | Loss: 0.00140297
Iteration 23/25 | Loss: 0.00140248
Iteration 24/25 | Loss: 0.00140247
Iteration 25/25 | Loss: 0.00140247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33871341
Iteration 2/25 | Loss: 0.00097177
Iteration 3/25 | Loss: 0.00097177
Iteration 4/25 | Loss: 0.00097177
Iteration 5/25 | Loss: 0.00097177
Iteration 6/25 | Loss: 0.00097177
Iteration 7/25 | Loss: 0.00097177
Iteration 8/25 | Loss: 0.00097177
Iteration 9/25 | Loss: 0.00097177
Iteration 10/25 | Loss: 0.00097177
Iteration 11/25 | Loss: 0.00097177
Iteration 12/25 | Loss: 0.00097177
Iteration 13/25 | Loss: 0.00097177
Iteration 14/25 | Loss: 0.00097177
Iteration 15/25 | Loss: 0.00097177
Iteration 16/25 | Loss: 0.00097177
Iteration 17/25 | Loss: 0.00097177
Iteration 18/25 | Loss: 0.00097177
Iteration 19/25 | Loss: 0.00097177
Iteration 20/25 | Loss: 0.00097177
Iteration 21/25 | Loss: 0.00097177
Iteration 22/25 | Loss: 0.00097177
Iteration 23/25 | Loss: 0.00097177
Iteration 24/25 | Loss: 0.00097177
Iteration 25/25 | Loss: 0.00097177

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097177
Iteration 2/1000 | Loss: 0.00547457
Iteration 3/1000 | Loss: 0.00340794
Iteration 4/1000 | Loss: 0.00171020
Iteration 5/1000 | Loss: 0.00060616
Iteration 6/1000 | Loss: 0.00051190
Iteration 7/1000 | Loss: 0.00049366
Iteration 8/1000 | Loss: 0.00031615
Iteration 9/1000 | Loss: 0.00035466
Iteration 10/1000 | Loss: 0.00024416
Iteration 11/1000 | Loss: 0.00034552
Iteration 12/1000 | Loss: 0.00037687
Iteration 13/1000 | Loss: 0.00081096
Iteration 14/1000 | Loss: 0.00014975
Iteration 15/1000 | Loss: 0.00013153
Iteration 16/1000 | Loss: 0.00040274
Iteration 17/1000 | Loss: 0.00318936
Iteration 18/1000 | Loss: 0.00332737
Iteration 19/1000 | Loss: 0.00337128
Iteration 20/1000 | Loss: 0.00065031
Iteration 21/1000 | Loss: 0.00093516
Iteration 22/1000 | Loss: 0.00170112
Iteration 23/1000 | Loss: 0.00024252
Iteration 24/1000 | Loss: 0.00018549
Iteration 25/1000 | Loss: 0.00030096
Iteration 26/1000 | Loss: 0.00122594
Iteration 27/1000 | Loss: 0.00051329
Iteration 28/1000 | Loss: 0.00044526
Iteration 29/1000 | Loss: 0.00048320
Iteration 30/1000 | Loss: 0.00099831
Iteration 31/1000 | Loss: 0.00089407
Iteration 32/1000 | Loss: 0.00018945
Iteration 33/1000 | Loss: 0.00016206
Iteration 34/1000 | Loss: 0.00015902
Iteration 35/1000 | Loss: 0.00038123
Iteration 36/1000 | Loss: 0.00112938
Iteration 37/1000 | Loss: 0.00079674
Iteration 38/1000 | Loss: 0.00075275
Iteration 39/1000 | Loss: 0.00014942
Iteration 40/1000 | Loss: 0.00055903
Iteration 41/1000 | Loss: 0.00123408
Iteration 42/1000 | Loss: 0.00045486
Iteration 43/1000 | Loss: 0.00036629
Iteration 44/1000 | Loss: 0.00012487
Iteration 45/1000 | Loss: 0.00010954
Iteration 46/1000 | Loss: 0.00028716
Iteration 47/1000 | Loss: 0.00038553
Iteration 48/1000 | Loss: 0.00070204
Iteration 49/1000 | Loss: 0.00016409
Iteration 50/1000 | Loss: 0.00023798
Iteration 51/1000 | Loss: 0.00010202
Iteration 52/1000 | Loss: 0.00011228
Iteration 53/1000 | Loss: 0.00011085
Iteration 54/1000 | Loss: 0.00039265
Iteration 55/1000 | Loss: 0.00014396
Iteration 56/1000 | Loss: 0.00021958
Iteration 57/1000 | Loss: 0.00012241
Iteration 58/1000 | Loss: 0.00006989
Iteration 59/1000 | Loss: 0.00021060
Iteration 60/1000 | Loss: 0.00009158
Iteration 61/1000 | Loss: 0.00010686
Iteration 62/1000 | Loss: 0.00012740
Iteration 63/1000 | Loss: 0.00006397
Iteration 64/1000 | Loss: 0.00005885
Iteration 65/1000 | Loss: 0.00019380
Iteration 66/1000 | Loss: 0.00005816
Iteration 67/1000 | Loss: 0.00041499
Iteration 68/1000 | Loss: 0.00027597
Iteration 69/1000 | Loss: 0.00021300
Iteration 70/1000 | Loss: 0.00005198
Iteration 71/1000 | Loss: 0.00027743
Iteration 72/1000 | Loss: 0.00010921
Iteration 73/1000 | Loss: 0.00006179
Iteration 74/1000 | Loss: 0.00004851
Iteration 75/1000 | Loss: 0.00004399
Iteration 76/1000 | Loss: 0.00004243
Iteration 77/1000 | Loss: 0.00003945
Iteration 78/1000 | Loss: 0.00005725
Iteration 79/1000 | Loss: 0.00004003
Iteration 80/1000 | Loss: 0.00003632
Iteration 81/1000 | Loss: 0.00003495
Iteration 82/1000 | Loss: 0.00003402
Iteration 83/1000 | Loss: 0.00003279
Iteration 84/1000 | Loss: 0.00003190
Iteration 85/1000 | Loss: 0.00025048
Iteration 86/1000 | Loss: 0.00004238
Iteration 87/1000 | Loss: 0.00003644
Iteration 88/1000 | Loss: 0.00003217
Iteration 89/1000 | Loss: 0.00003041
Iteration 90/1000 | Loss: 0.00002969
Iteration 91/1000 | Loss: 0.00002917
Iteration 92/1000 | Loss: 0.00002893
Iteration 93/1000 | Loss: 0.00002869
Iteration 94/1000 | Loss: 0.00002849
Iteration 95/1000 | Loss: 0.00002847
Iteration 96/1000 | Loss: 0.00002846
Iteration 97/1000 | Loss: 0.00002845
Iteration 98/1000 | Loss: 0.00002841
Iteration 99/1000 | Loss: 0.00002838
Iteration 100/1000 | Loss: 0.00002837
Iteration 101/1000 | Loss: 0.00002837
Iteration 102/1000 | Loss: 0.00002836
Iteration 103/1000 | Loss: 0.00002836
Iteration 104/1000 | Loss: 0.00002836
Iteration 105/1000 | Loss: 0.00002836
Iteration 106/1000 | Loss: 0.00002836
Iteration 107/1000 | Loss: 0.00002836
Iteration 108/1000 | Loss: 0.00002835
Iteration 109/1000 | Loss: 0.00002835
Iteration 110/1000 | Loss: 0.00002830
Iteration 111/1000 | Loss: 0.00002823
Iteration 112/1000 | Loss: 0.00002819
Iteration 113/1000 | Loss: 0.00002818
Iteration 114/1000 | Loss: 0.00002807
Iteration 115/1000 | Loss: 0.00002807
Iteration 116/1000 | Loss: 0.00002807
Iteration 117/1000 | Loss: 0.00002807
Iteration 118/1000 | Loss: 0.00002807
Iteration 119/1000 | Loss: 0.00002806
Iteration 120/1000 | Loss: 0.00002806
Iteration 121/1000 | Loss: 0.00002806
Iteration 122/1000 | Loss: 0.00002806
Iteration 123/1000 | Loss: 0.00002806
Iteration 124/1000 | Loss: 0.00002803
Iteration 125/1000 | Loss: 0.00002803
Iteration 126/1000 | Loss: 0.00002802
Iteration 127/1000 | Loss: 0.00002802
Iteration 128/1000 | Loss: 0.00002802
Iteration 129/1000 | Loss: 0.00002802
Iteration 130/1000 | Loss: 0.00002802
Iteration 131/1000 | Loss: 0.00002802
Iteration 132/1000 | Loss: 0.00002802
Iteration 133/1000 | Loss: 0.00002802
Iteration 134/1000 | Loss: 0.00002802
Iteration 135/1000 | Loss: 0.00002802
Iteration 136/1000 | Loss: 0.00002801
Iteration 137/1000 | Loss: 0.00002801
Iteration 138/1000 | Loss: 0.00002801
Iteration 139/1000 | Loss: 0.00002801
Iteration 140/1000 | Loss: 0.00002801
Iteration 141/1000 | Loss: 0.00002801
Iteration 142/1000 | Loss: 0.00002801
Iteration 143/1000 | Loss: 0.00002801
Iteration 144/1000 | Loss: 0.00002801
Iteration 145/1000 | Loss: 0.00002801
Iteration 146/1000 | Loss: 0.00002801
Iteration 147/1000 | Loss: 0.00002801
Iteration 148/1000 | Loss: 0.00002801
Iteration 149/1000 | Loss: 0.00002801
Iteration 150/1000 | Loss: 0.00002801
Iteration 151/1000 | Loss: 0.00002801
Iteration 152/1000 | Loss: 0.00002801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.8005120839225128e-05, 2.8005120839225128e-05, 2.8005120839225128e-05, 2.8005120839225128e-05, 2.8005120839225128e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8005120839225128e-05

Optimization complete. Final v2v error: 4.1782121658325195 mm

Highest mean error: 6.834071159362793 mm for frame 69

Lowest mean error: 3.4408884048461914 mm for frame 124

Saving results

Total time: 179.64243984222412
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963523
Iteration 2/25 | Loss: 0.00178918
Iteration 3/25 | Loss: 0.00137198
Iteration 4/25 | Loss: 0.00131526
Iteration 5/25 | Loss: 0.00130310
Iteration 6/25 | Loss: 0.00130089
Iteration 7/25 | Loss: 0.00130089
Iteration 8/25 | Loss: 0.00130089
Iteration 9/25 | Loss: 0.00130089
Iteration 10/25 | Loss: 0.00130089
Iteration 11/25 | Loss: 0.00130089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013008913956582546, 0.0013008913956582546, 0.0013008913956582546, 0.0013008913956582546, 0.0013008913956582546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013008913956582546

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.95394826
Iteration 2/25 | Loss: 0.00076965
Iteration 3/25 | Loss: 0.00076965
Iteration 4/25 | Loss: 0.00076965
Iteration 5/25 | Loss: 0.00076965
Iteration 6/25 | Loss: 0.00076965
Iteration 7/25 | Loss: 0.00076965
Iteration 8/25 | Loss: 0.00076964
Iteration 9/25 | Loss: 0.00076964
Iteration 10/25 | Loss: 0.00076964
Iteration 11/25 | Loss: 0.00076964
Iteration 12/25 | Loss: 0.00076964
Iteration 13/25 | Loss: 0.00076964
Iteration 14/25 | Loss: 0.00076964
Iteration 15/25 | Loss: 0.00076964
Iteration 16/25 | Loss: 0.00076964
Iteration 17/25 | Loss: 0.00076964
Iteration 18/25 | Loss: 0.00076964
Iteration 19/25 | Loss: 0.00076964
Iteration 20/25 | Loss: 0.00076964
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007696442189626396, 0.0007696442189626396, 0.0007696442189626396, 0.0007696442189626396, 0.0007696442189626396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007696442189626396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076964
Iteration 2/1000 | Loss: 0.00005069
Iteration 3/1000 | Loss: 0.00003025
Iteration 4/1000 | Loss: 0.00002753
Iteration 5/1000 | Loss: 0.00002569
Iteration 6/1000 | Loss: 0.00002455
Iteration 7/1000 | Loss: 0.00002403
Iteration 8/1000 | Loss: 0.00002357
Iteration 9/1000 | Loss: 0.00002329
Iteration 10/1000 | Loss: 0.00002305
Iteration 11/1000 | Loss: 0.00002300
Iteration 12/1000 | Loss: 0.00002298
Iteration 13/1000 | Loss: 0.00002293
Iteration 14/1000 | Loss: 0.00002280
Iteration 15/1000 | Loss: 0.00002279
Iteration 16/1000 | Loss: 0.00002278
Iteration 17/1000 | Loss: 0.00002278
Iteration 18/1000 | Loss: 0.00002278
Iteration 19/1000 | Loss: 0.00002277
Iteration 20/1000 | Loss: 0.00002272
Iteration 21/1000 | Loss: 0.00002271
Iteration 22/1000 | Loss: 0.00002270
Iteration 23/1000 | Loss: 0.00002266
Iteration 24/1000 | Loss: 0.00002265
Iteration 25/1000 | Loss: 0.00002265
Iteration 26/1000 | Loss: 0.00002264
Iteration 27/1000 | Loss: 0.00002263
Iteration 28/1000 | Loss: 0.00002263
Iteration 29/1000 | Loss: 0.00002263
Iteration 30/1000 | Loss: 0.00002263
Iteration 31/1000 | Loss: 0.00002263
Iteration 32/1000 | Loss: 0.00002263
Iteration 33/1000 | Loss: 0.00002262
Iteration 34/1000 | Loss: 0.00002262
Iteration 35/1000 | Loss: 0.00002262
Iteration 36/1000 | Loss: 0.00002262
Iteration 37/1000 | Loss: 0.00002261
Iteration 38/1000 | Loss: 0.00002261
Iteration 39/1000 | Loss: 0.00002261
Iteration 40/1000 | Loss: 0.00002261
Iteration 41/1000 | Loss: 0.00002261
Iteration 42/1000 | Loss: 0.00002261
Iteration 43/1000 | Loss: 0.00002261
Iteration 44/1000 | Loss: 0.00002261
Iteration 45/1000 | Loss: 0.00002261
Iteration 46/1000 | Loss: 0.00002261
Iteration 47/1000 | Loss: 0.00002260
Iteration 48/1000 | Loss: 0.00002260
Iteration 49/1000 | Loss: 0.00002260
Iteration 50/1000 | Loss: 0.00002260
Iteration 51/1000 | Loss: 0.00002260
Iteration 52/1000 | Loss: 0.00002260
Iteration 53/1000 | Loss: 0.00002259
Iteration 54/1000 | Loss: 0.00002259
Iteration 55/1000 | Loss: 0.00002259
Iteration 56/1000 | Loss: 0.00002259
Iteration 57/1000 | Loss: 0.00002259
Iteration 58/1000 | Loss: 0.00002259
Iteration 59/1000 | Loss: 0.00002259
Iteration 60/1000 | Loss: 0.00002259
Iteration 61/1000 | Loss: 0.00002259
Iteration 62/1000 | Loss: 0.00002259
Iteration 63/1000 | Loss: 0.00002259
Iteration 64/1000 | Loss: 0.00002258
Iteration 65/1000 | Loss: 0.00002258
Iteration 66/1000 | Loss: 0.00002258
Iteration 67/1000 | Loss: 0.00002258
Iteration 68/1000 | Loss: 0.00002258
Iteration 69/1000 | Loss: 0.00002258
Iteration 70/1000 | Loss: 0.00002258
Iteration 71/1000 | Loss: 0.00002258
Iteration 72/1000 | Loss: 0.00002258
Iteration 73/1000 | Loss: 0.00002258
Iteration 74/1000 | Loss: 0.00002258
Iteration 75/1000 | Loss: 0.00002258
Iteration 76/1000 | Loss: 0.00002258
Iteration 77/1000 | Loss: 0.00002258
Iteration 78/1000 | Loss: 0.00002258
Iteration 79/1000 | Loss: 0.00002258
Iteration 80/1000 | Loss: 0.00002257
Iteration 81/1000 | Loss: 0.00002257
Iteration 82/1000 | Loss: 0.00002257
Iteration 83/1000 | Loss: 0.00002257
Iteration 84/1000 | Loss: 0.00002257
Iteration 85/1000 | Loss: 0.00002257
Iteration 86/1000 | Loss: 0.00002257
Iteration 87/1000 | Loss: 0.00002257
Iteration 88/1000 | Loss: 0.00002257
Iteration 89/1000 | Loss: 0.00002257
Iteration 90/1000 | Loss: 0.00002257
Iteration 91/1000 | Loss: 0.00002257
Iteration 92/1000 | Loss: 0.00002257
Iteration 93/1000 | Loss: 0.00002257
Iteration 94/1000 | Loss: 0.00002257
Iteration 95/1000 | Loss: 0.00002257
Iteration 96/1000 | Loss: 0.00002256
Iteration 97/1000 | Loss: 0.00002256
Iteration 98/1000 | Loss: 0.00002256
Iteration 99/1000 | Loss: 0.00002256
Iteration 100/1000 | Loss: 0.00002256
Iteration 101/1000 | Loss: 0.00002256
Iteration 102/1000 | Loss: 0.00002256
Iteration 103/1000 | Loss: 0.00002256
Iteration 104/1000 | Loss: 0.00002256
Iteration 105/1000 | Loss: 0.00002256
Iteration 106/1000 | Loss: 0.00002256
Iteration 107/1000 | Loss: 0.00002256
Iteration 108/1000 | Loss: 0.00002256
Iteration 109/1000 | Loss: 0.00002256
Iteration 110/1000 | Loss: 0.00002256
Iteration 111/1000 | Loss: 0.00002256
Iteration 112/1000 | Loss: 0.00002256
Iteration 113/1000 | Loss: 0.00002256
Iteration 114/1000 | Loss: 0.00002256
Iteration 115/1000 | Loss: 0.00002256
Iteration 116/1000 | Loss: 0.00002256
Iteration 117/1000 | Loss: 0.00002256
Iteration 118/1000 | Loss: 0.00002256
Iteration 119/1000 | Loss: 0.00002256
Iteration 120/1000 | Loss: 0.00002256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.2556954718311317e-05, 2.2556954718311317e-05, 2.2556954718311317e-05, 2.2556954718311317e-05, 2.2556954718311317e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2556954718311317e-05

Optimization complete. Final v2v error: 4.023862838745117 mm

Highest mean error: 4.371922969818115 mm for frame 31

Lowest mean error: 3.81862473487854 mm for frame 10

Saving results

Total time: 33.46341514587402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00921296
Iteration 2/25 | Loss: 0.00169745
Iteration 3/25 | Loss: 0.00150844
Iteration 4/25 | Loss: 0.00147481
Iteration 5/25 | Loss: 0.00146181
Iteration 6/25 | Loss: 0.00145523
Iteration 7/25 | Loss: 0.00144516
Iteration 8/25 | Loss: 0.00144290
Iteration 9/25 | Loss: 0.00144291
Iteration 10/25 | Loss: 0.00143833
Iteration 11/25 | Loss: 0.00143153
Iteration 12/25 | Loss: 0.00142903
Iteration 13/25 | Loss: 0.00142861
Iteration 14/25 | Loss: 0.00142827
Iteration 15/25 | Loss: 0.00142806
Iteration 16/25 | Loss: 0.00142794
Iteration 17/25 | Loss: 0.00142785
Iteration 18/25 | Loss: 0.00142783
Iteration 19/25 | Loss: 0.00142783
Iteration 20/25 | Loss: 0.00142783
Iteration 21/25 | Loss: 0.00142783
Iteration 22/25 | Loss: 0.00142783
Iteration 23/25 | Loss: 0.00142783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014278272865340114, 0.0014278272865340114, 0.0014278272865340114, 0.0014278272865340114, 0.0014278272865340114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014278272865340114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34819424
Iteration 2/25 | Loss: 0.00206893
Iteration 3/25 | Loss: 0.00206886
Iteration 4/25 | Loss: 0.00206886
Iteration 5/25 | Loss: 0.00206886
Iteration 6/25 | Loss: 0.00206886
Iteration 7/25 | Loss: 0.00206886
Iteration 8/25 | Loss: 0.00206886
Iteration 9/25 | Loss: 0.00206886
Iteration 10/25 | Loss: 0.00206886
Iteration 11/25 | Loss: 0.00206886
Iteration 12/25 | Loss: 0.00206886
Iteration 13/25 | Loss: 0.00206886
Iteration 14/25 | Loss: 0.00206886
Iteration 15/25 | Loss: 0.00206886
Iteration 16/25 | Loss: 0.00206886
Iteration 17/25 | Loss: 0.00206886
Iteration 18/25 | Loss: 0.00206886
Iteration 19/25 | Loss: 0.00206886
Iteration 20/25 | Loss: 0.00206886
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0020688604563474655, 0.0020688604563474655, 0.0020688604563474655, 0.0020688604563474655, 0.0020688604563474655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020688604563474655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206886
Iteration 2/1000 | Loss: 0.00018204
Iteration 3/1000 | Loss: 0.00011765
Iteration 4/1000 | Loss: 0.00072835
Iteration 5/1000 | Loss: 0.00009861
Iteration 6/1000 | Loss: 0.00008987
Iteration 7/1000 | Loss: 0.00029037
Iteration 8/1000 | Loss: 0.00085324
Iteration 9/1000 | Loss: 0.00009812
Iteration 10/1000 | Loss: 0.00040437
Iteration 11/1000 | Loss: 0.00008060
Iteration 12/1000 | Loss: 0.00051127
Iteration 13/1000 | Loss: 0.00058604
Iteration 14/1000 | Loss: 0.00041161
Iteration 15/1000 | Loss: 0.00026529
Iteration 16/1000 | Loss: 0.00032110
Iteration 17/1000 | Loss: 0.00010826
Iteration 18/1000 | Loss: 0.00013506
Iteration 19/1000 | Loss: 0.00061676
Iteration 20/1000 | Loss: 0.00050572
Iteration 21/1000 | Loss: 0.00042008
Iteration 22/1000 | Loss: 0.00069400
Iteration 23/1000 | Loss: 0.00051110
Iteration 24/1000 | Loss: 0.00028499
Iteration 25/1000 | Loss: 0.00009085
Iteration 26/1000 | Loss: 0.00006922
Iteration 27/1000 | Loss: 0.00006696
Iteration 28/1000 | Loss: 0.00006474
Iteration 29/1000 | Loss: 0.00006306
Iteration 30/1000 | Loss: 0.00006163
Iteration 31/1000 | Loss: 0.00047932
Iteration 32/1000 | Loss: 0.00007739
Iteration 33/1000 | Loss: 0.00021865
Iteration 34/1000 | Loss: 0.00006252
Iteration 35/1000 | Loss: 0.00035920
Iteration 36/1000 | Loss: 0.00015895
Iteration 37/1000 | Loss: 0.00033180
Iteration 38/1000 | Loss: 0.00006681
Iteration 39/1000 | Loss: 0.00006323
Iteration 40/1000 | Loss: 0.00006162
Iteration 41/1000 | Loss: 0.00005962
Iteration 42/1000 | Loss: 0.00005852
Iteration 43/1000 | Loss: 0.00064185
Iteration 44/1000 | Loss: 0.00098579
Iteration 45/1000 | Loss: 0.00380124
Iteration 46/1000 | Loss: 0.00056102
Iteration 47/1000 | Loss: 0.00043916
Iteration 48/1000 | Loss: 0.00045380
Iteration 49/1000 | Loss: 0.00106541
Iteration 50/1000 | Loss: 0.00028404
Iteration 51/1000 | Loss: 0.00006580
Iteration 52/1000 | Loss: 0.00005503
Iteration 53/1000 | Loss: 0.00004645
Iteration 54/1000 | Loss: 0.00004131
Iteration 55/1000 | Loss: 0.00003737
Iteration 56/1000 | Loss: 0.00003473
Iteration 57/1000 | Loss: 0.00003320
Iteration 58/1000 | Loss: 0.00003214
Iteration 59/1000 | Loss: 0.00003096
Iteration 60/1000 | Loss: 0.00023545
Iteration 61/1000 | Loss: 0.00003188
Iteration 62/1000 | Loss: 0.00002982
Iteration 63/1000 | Loss: 0.00002879
Iteration 64/1000 | Loss: 0.00002800
Iteration 65/1000 | Loss: 0.00002756
Iteration 66/1000 | Loss: 0.00002715
Iteration 67/1000 | Loss: 0.00002679
Iteration 68/1000 | Loss: 0.00002658
Iteration 69/1000 | Loss: 0.00002639
Iteration 70/1000 | Loss: 0.00002631
Iteration 71/1000 | Loss: 0.00002631
Iteration 72/1000 | Loss: 0.00002630
Iteration 73/1000 | Loss: 0.00002626
Iteration 74/1000 | Loss: 0.00002625
Iteration 75/1000 | Loss: 0.00002625
Iteration 76/1000 | Loss: 0.00002624
Iteration 77/1000 | Loss: 0.00002623
Iteration 78/1000 | Loss: 0.00002623
Iteration 79/1000 | Loss: 0.00002622
Iteration 80/1000 | Loss: 0.00002622
Iteration 81/1000 | Loss: 0.00002622
Iteration 82/1000 | Loss: 0.00002622
Iteration 83/1000 | Loss: 0.00002621
Iteration 84/1000 | Loss: 0.00002621
Iteration 85/1000 | Loss: 0.00002621
Iteration 86/1000 | Loss: 0.00002620
Iteration 87/1000 | Loss: 0.00002620
Iteration 88/1000 | Loss: 0.00002619
Iteration 89/1000 | Loss: 0.00002619
Iteration 90/1000 | Loss: 0.00002618
Iteration 91/1000 | Loss: 0.00002618
Iteration 92/1000 | Loss: 0.00002617
Iteration 93/1000 | Loss: 0.00002617
Iteration 94/1000 | Loss: 0.00002617
Iteration 95/1000 | Loss: 0.00002616
Iteration 96/1000 | Loss: 0.00002616
Iteration 97/1000 | Loss: 0.00002616
Iteration 98/1000 | Loss: 0.00002616
Iteration 99/1000 | Loss: 0.00002615
Iteration 100/1000 | Loss: 0.00002615
Iteration 101/1000 | Loss: 0.00002615
Iteration 102/1000 | Loss: 0.00002615
Iteration 103/1000 | Loss: 0.00002615
Iteration 104/1000 | Loss: 0.00002615
Iteration 105/1000 | Loss: 0.00002615
Iteration 106/1000 | Loss: 0.00002615
Iteration 107/1000 | Loss: 0.00002615
Iteration 108/1000 | Loss: 0.00002614
Iteration 109/1000 | Loss: 0.00002614
Iteration 110/1000 | Loss: 0.00002614
Iteration 111/1000 | Loss: 0.00002614
Iteration 112/1000 | Loss: 0.00002614
Iteration 113/1000 | Loss: 0.00002613
Iteration 114/1000 | Loss: 0.00002613
Iteration 115/1000 | Loss: 0.00002613
Iteration 116/1000 | Loss: 0.00002612
Iteration 117/1000 | Loss: 0.00002612
Iteration 118/1000 | Loss: 0.00002612
Iteration 119/1000 | Loss: 0.00002611
Iteration 120/1000 | Loss: 0.00002610
Iteration 121/1000 | Loss: 0.00002610
Iteration 122/1000 | Loss: 0.00002610
Iteration 123/1000 | Loss: 0.00002610
Iteration 124/1000 | Loss: 0.00002610
Iteration 125/1000 | Loss: 0.00002609
Iteration 126/1000 | Loss: 0.00002609
Iteration 127/1000 | Loss: 0.00002609
Iteration 128/1000 | Loss: 0.00002609
Iteration 129/1000 | Loss: 0.00002609
Iteration 130/1000 | Loss: 0.00002609
Iteration 131/1000 | Loss: 0.00002609
Iteration 132/1000 | Loss: 0.00002608
Iteration 133/1000 | Loss: 0.00002608
Iteration 134/1000 | Loss: 0.00002608
Iteration 135/1000 | Loss: 0.00002608
Iteration 136/1000 | Loss: 0.00002608
Iteration 137/1000 | Loss: 0.00002608
Iteration 138/1000 | Loss: 0.00002607
Iteration 139/1000 | Loss: 0.00002607
Iteration 140/1000 | Loss: 0.00002606
Iteration 141/1000 | Loss: 0.00002606
Iteration 142/1000 | Loss: 0.00002606
Iteration 143/1000 | Loss: 0.00002606
Iteration 144/1000 | Loss: 0.00002606
Iteration 145/1000 | Loss: 0.00002606
Iteration 146/1000 | Loss: 0.00002605
Iteration 147/1000 | Loss: 0.00002605
Iteration 148/1000 | Loss: 0.00002605
Iteration 149/1000 | Loss: 0.00002605
Iteration 150/1000 | Loss: 0.00002605
Iteration 151/1000 | Loss: 0.00002605
Iteration 152/1000 | Loss: 0.00002605
Iteration 153/1000 | Loss: 0.00002605
Iteration 154/1000 | Loss: 0.00002604
Iteration 155/1000 | Loss: 0.00002604
Iteration 156/1000 | Loss: 0.00002604
Iteration 157/1000 | Loss: 0.00002603
Iteration 158/1000 | Loss: 0.00002603
Iteration 159/1000 | Loss: 0.00002603
Iteration 160/1000 | Loss: 0.00002603
Iteration 161/1000 | Loss: 0.00002602
Iteration 162/1000 | Loss: 0.00002602
Iteration 163/1000 | Loss: 0.00002602
Iteration 164/1000 | Loss: 0.00002602
Iteration 165/1000 | Loss: 0.00002601
Iteration 166/1000 | Loss: 0.00002601
Iteration 167/1000 | Loss: 0.00002601
Iteration 168/1000 | Loss: 0.00002601
Iteration 169/1000 | Loss: 0.00002601
Iteration 170/1000 | Loss: 0.00002601
Iteration 171/1000 | Loss: 0.00002601
Iteration 172/1000 | Loss: 0.00002601
Iteration 173/1000 | Loss: 0.00002601
Iteration 174/1000 | Loss: 0.00002601
Iteration 175/1000 | Loss: 0.00002601
Iteration 176/1000 | Loss: 0.00002601
Iteration 177/1000 | Loss: 0.00002600
Iteration 178/1000 | Loss: 0.00002600
Iteration 179/1000 | Loss: 0.00002600
Iteration 180/1000 | Loss: 0.00002600
Iteration 181/1000 | Loss: 0.00002600
Iteration 182/1000 | Loss: 0.00002600
Iteration 183/1000 | Loss: 0.00002600
Iteration 184/1000 | Loss: 0.00002600
Iteration 185/1000 | Loss: 0.00002600
Iteration 186/1000 | Loss: 0.00002600
Iteration 187/1000 | Loss: 0.00002600
Iteration 188/1000 | Loss: 0.00002600
Iteration 189/1000 | Loss: 0.00002600
Iteration 190/1000 | Loss: 0.00002600
Iteration 191/1000 | Loss: 0.00002600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.5997162083513103e-05, 2.5997162083513103e-05, 2.5997162083513103e-05, 2.5997162083513103e-05, 2.5997162083513103e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5997162083513103e-05

Optimization complete. Final v2v error: 4.233433246612549 mm

Highest mean error: 5.033036231994629 mm for frame 116

Lowest mean error: 3.4234580993652344 mm for frame 203

Saving results

Total time: 157.94708633422852
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405825
Iteration 2/25 | Loss: 0.00135895
Iteration 3/25 | Loss: 0.00122840
Iteration 4/25 | Loss: 0.00121109
Iteration 5/25 | Loss: 0.00120648
Iteration 6/25 | Loss: 0.00120503
Iteration 7/25 | Loss: 0.00120501
Iteration 8/25 | Loss: 0.00120501
Iteration 9/25 | Loss: 0.00120501
Iteration 10/25 | Loss: 0.00120501
Iteration 11/25 | Loss: 0.00120501
Iteration 12/25 | Loss: 0.00120501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012050129007548094, 0.0012050129007548094, 0.0012050129007548094, 0.0012050129007548094, 0.0012050129007548094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012050129007548094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40768504
Iteration 2/25 | Loss: 0.00110808
Iteration 3/25 | Loss: 0.00110808
Iteration 4/25 | Loss: 0.00110807
Iteration 5/25 | Loss: 0.00110807
Iteration 6/25 | Loss: 0.00110807
Iteration 7/25 | Loss: 0.00110807
Iteration 8/25 | Loss: 0.00110807
Iteration 9/25 | Loss: 0.00110807
Iteration 10/25 | Loss: 0.00110807
Iteration 11/25 | Loss: 0.00110807
Iteration 12/25 | Loss: 0.00110807
Iteration 13/25 | Loss: 0.00110807
Iteration 14/25 | Loss: 0.00110807
Iteration 15/25 | Loss: 0.00110807
Iteration 16/25 | Loss: 0.00110807
Iteration 17/25 | Loss: 0.00110807
Iteration 18/25 | Loss: 0.00110807
Iteration 19/25 | Loss: 0.00110807
Iteration 20/25 | Loss: 0.00110807
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011080714175477624, 0.0011080714175477624, 0.0011080714175477624, 0.0011080714175477624, 0.0011080714175477624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011080714175477624

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110807
Iteration 2/1000 | Loss: 0.00003978
Iteration 3/1000 | Loss: 0.00002484
Iteration 4/1000 | Loss: 0.00001916
Iteration 5/1000 | Loss: 0.00001726
Iteration 6/1000 | Loss: 0.00001635
Iteration 7/1000 | Loss: 0.00001535
Iteration 8/1000 | Loss: 0.00001498
Iteration 9/1000 | Loss: 0.00001458
Iteration 10/1000 | Loss: 0.00001434
Iteration 11/1000 | Loss: 0.00001433
Iteration 12/1000 | Loss: 0.00001427
Iteration 13/1000 | Loss: 0.00001408
Iteration 14/1000 | Loss: 0.00001391
Iteration 15/1000 | Loss: 0.00001388
Iteration 16/1000 | Loss: 0.00001381
Iteration 17/1000 | Loss: 0.00001378
Iteration 18/1000 | Loss: 0.00001377
Iteration 19/1000 | Loss: 0.00001372
Iteration 20/1000 | Loss: 0.00001365
Iteration 21/1000 | Loss: 0.00001362
Iteration 22/1000 | Loss: 0.00001360
Iteration 23/1000 | Loss: 0.00001360
Iteration 24/1000 | Loss: 0.00001360
Iteration 25/1000 | Loss: 0.00001359
Iteration 26/1000 | Loss: 0.00001358
Iteration 27/1000 | Loss: 0.00001358
Iteration 28/1000 | Loss: 0.00001357
Iteration 29/1000 | Loss: 0.00001353
Iteration 30/1000 | Loss: 0.00001352
Iteration 31/1000 | Loss: 0.00001351
Iteration 32/1000 | Loss: 0.00001351
Iteration 33/1000 | Loss: 0.00001350
Iteration 34/1000 | Loss: 0.00001350
Iteration 35/1000 | Loss: 0.00001350
Iteration 36/1000 | Loss: 0.00001349
Iteration 37/1000 | Loss: 0.00001349
Iteration 38/1000 | Loss: 0.00001348
Iteration 39/1000 | Loss: 0.00001348
Iteration 40/1000 | Loss: 0.00001348
Iteration 41/1000 | Loss: 0.00001347
Iteration 42/1000 | Loss: 0.00001347
Iteration 43/1000 | Loss: 0.00001346
Iteration 44/1000 | Loss: 0.00001346
Iteration 45/1000 | Loss: 0.00001346
Iteration 46/1000 | Loss: 0.00001346
Iteration 47/1000 | Loss: 0.00001346
Iteration 48/1000 | Loss: 0.00001345
Iteration 49/1000 | Loss: 0.00001345
Iteration 50/1000 | Loss: 0.00001345
Iteration 51/1000 | Loss: 0.00001345
Iteration 52/1000 | Loss: 0.00001345
Iteration 53/1000 | Loss: 0.00001345
Iteration 54/1000 | Loss: 0.00001343
Iteration 55/1000 | Loss: 0.00001343
Iteration 56/1000 | Loss: 0.00001343
Iteration 57/1000 | Loss: 0.00001343
Iteration 58/1000 | Loss: 0.00001343
Iteration 59/1000 | Loss: 0.00001342
Iteration 60/1000 | Loss: 0.00001342
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001341
Iteration 64/1000 | Loss: 0.00001341
Iteration 65/1000 | Loss: 0.00001341
Iteration 66/1000 | Loss: 0.00001341
Iteration 67/1000 | Loss: 0.00001340
Iteration 68/1000 | Loss: 0.00001340
Iteration 69/1000 | Loss: 0.00001340
Iteration 70/1000 | Loss: 0.00001340
Iteration 71/1000 | Loss: 0.00001339
Iteration 72/1000 | Loss: 0.00001339
Iteration 73/1000 | Loss: 0.00001339
Iteration 74/1000 | Loss: 0.00001339
Iteration 75/1000 | Loss: 0.00001338
Iteration 76/1000 | Loss: 0.00001338
Iteration 77/1000 | Loss: 0.00001338
Iteration 78/1000 | Loss: 0.00001338
Iteration 79/1000 | Loss: 0.00001337
Iteration 80/1000 | Loss: 0.00001337
Iteration 81/1000 | Loss: 0.00001337
Iteration 82/1000 | Loss: 0.00001337
Iteration 83/1000 | Loss: 0.00001337
Iteration 84/1000 | Loss: 0.00001337
Iteration 85/1000 | Loss: 0.00001336
Iteration 86/1000 | Loss: 0.00001336
Iteration 87/1000 | Loss: 0.00001336
Iteration 88/1000 | Loss: 0.00001336
Iteration 89/1000 | Loss: 0.00001336
Iteration 90/1000 | Loss: 0.00001336
Iteration 91/1000 | Loss: 0.00001336
Iteration 92/1000 | Loss: 0.00001335
Iteration 93/1000 | Loss: 0.00001335
Iteration 94/1000 | Loss: 0.00001335
Iteration 95/1000 | Loss: 0.00001335
Iteration 96/1000 | Loss: 0.00001335
Iteration 97/1000 | Loss: 0.00001335
Iteration 98/1000 | Loss: 0.00001334
Iteration 99/1000 | Loss: 0.00001334
Iteration 100/1000 | Loss: 0.00001334
Iteration 101/1000 | Loss: 0.00001334
Iteration 102/1000 | Loss: 0.00001334
Iteration 103/1000 | Loss: 0.00001334
Iteration 104/1000 | Loss: 0.00001334
Iteration 105/1000 | Loss: 0.00001333
Iteration 106/1000 | Loss: 0.00001333
Iteration 107/1000 | Loss: 0.00001333
Iteration 108/1000 | Loss: 0.00001333
Iteration 109/1000 | Loss: 0.00001333
Iteration 110/1000 | Loss: 0.00001333
Iteration 111/1000 | Loss: 0.00001333
Iteration 112/1000 | Loss: 0.00001333
Iteration 113/1000 | Loss: 0.00001333
Iteration 114/1000 | Loss: 0.00001333
Iteration 115/1000 | Loss: 0.00001333
Iteration 116/1000 | Loss: 0.00001332
Iteration 117/1000 | Loss: 0.00001332
Iteration 118/1000 | Loss: 0.00001332
Iteration 119/1000 | Loss: 0.00001332
Iteration 120/1000 | Loss: 0.00001332
Iteration 121/1000 | Loss: 0.00001332
Iteration 122/1000 | Loss: 0.00001332
Iteration 123/1000 | Loss: 0.00001332
Iteration 124/1000 | Loss: 0.00001332
Iteration 125/1000 | Loss: 0.00001332
Iteration 126/1000 | Loss: 0.00001332
Iteration 127/1000 | Loss: 0.00001332
Iteration 128/1000 | Loss: 0.00001331
Iteration 129/1000 | Loss: 0.00001331
Iteration 130/1000 | Loss: 0.00001331
Iteration 131/1000 | Loss: 0.00001331
Iteration 132/1000 | Loss: 0.00001331
Iteration 133/1000 | Loss: 0.00001331
Iteration 134/1000 | Loss: 0.00001331
Iteration 135/1000 | Loss: 0.00001331
Iteration 136/1000 | Loss: 0.00001331
Iteration 137/1000 | Loss: 0.00001331
Iteration 138/1000 | Loss: 0.00001331
Iteration 139/1000 | Loss: 0.00001330
Iteration 140/1000 | Loss: 0.00001330
Iteration 141/1000 | Loss: 0.00001330
Iteration 142/1000 | Loss: 0.00001330
Iteration 143/1000 | Loss: 0.00001330
Iteration 144/1000 | Loss: 0.00001330
Iteration 145/1000 | Loss: 0.00001329
Iteration 146/1000 | Loss: 0.00001329
Iteration 147/1000 | Loss: 0.00001329
Iteration 148/1000 | Loss: 0.00001329
Iteration 149/1000 | Loss: 0.00001329
Iteration 150/1000 | Loss: 0.00001329
Iteration 151/1000 | Loss: 0.00001329
Iteration 152/1000 | Loss: 0.00001329
Iteration 153/1000 | Loss: 0.00001329
Iteration 154/1000 | Loss: 0.00001329
Iteration 155/1000 | Loss: 0.00001329
Iteration 156/1000 | Loss: 0.00001329
Iteration 157/1000 | Loss: 0.00001329
Iteration 158/1000 | Loss: 0.00001329
Iteration 159/1000 | Loss: 0.00001329
Iteration 160/1000 | Loss: 0.00001328
Iteration 161/1000 | Loss: 0.00001328
Iteration 162/1000 | Loss: 0.00001328
Iteration 163/1000 | Loss: 0.00001328
Iteration 164/1000 | Loss: 0.00001328
Iteration 165/1000 | Loss: 0.00001328
Iteration 166/1000 | Loss: 0.00001328
Iteration 167/1000 | Loss: 0.00001328
Iteration 168/1000 | Loss: 0.00001327
Iteration 169/1000 | Loss: 0.00001327
Iteration 170/1000 | Loss: 0.00001327
Iteration 171/1000 | Loss: 0.00001327
Iteration 172/1000 | Loss: 0.00001327
Iteration 173/1000 | Loss: 0.00001327
Iteration 174/1000 | Loss: 0.00001327
Iteration 175/1000 | Loss: 0.00001327
Iteration 176/1000 | Loss: 0.00001327
Iteration 177/1000 | Loss: 0.00001327
Iteration 178/1000 | Loss: 0.00001326
Iteration 179/1000 | Loss: 0.00001326
Iteration 180/1000 | Loss: 0.00001326
Iteration 181/1000 | Loss: 0.00001326
Iteration 182/1000 | Loss: 0.00001326
Iteration 183/1000 | Loss: 0.00001326
Iteration 184/1000 | Loss: 0.00001326
Iteration 185/1000 | Loss: 0.00001326
Iteration 186/1000 | Loss: 0.00001325
Iteration 187/1000 | Loss: 0.00001325
Iteration 188/1000 | Loss: 0.00001325
Iteration 189/1000 | Loss: 0.00001325
Iteration 190/1000 | Loss: 0.00001324
Iteration 191/1000 | Loss: 0.00001324
Iteration 192/1000 | Loss: 0.00001324
Iteration 193/1000 | Loss: 0.00001324
Iteration 194/1000 | Loss: 0.00001324
Iteration 195/1000 | Loss: 0.00001324
Iteration 196/1000 | Loss: 0.00001324
Iteration 197/1000 | Loss: 0.00001324
Iteration 198/1000 | Loss: 0.00001323
Iteration 199/1000 | Loss: 0.00001323
Iteration 200/1000 | Loss: 0.00001323
Iteration 201/1000 | Loss: 0.00001323
Iteration 202/1000 | Loss: 0.00001323
Iteration 203/1000 | Loss: 0.00001323
Iteration 204/1000 | Loss: 0.00001322
Iteration 205/1000 | Loss: 0.00001322
Iteration 206/1000 | Loss: 0.00001322
Iteration 207/1000 | Loss: 0.00001322
Iteration 208/1000 | Loss: 0.00001322
Iteration 209/1000 | Loss: 0.00001321
Iteration 210/1000 | Loss: 0.00001321
Iteration 211/1000 | Loss: 0.00001321
Iteration 212/1000 | Loss: 0.00001321
Iteration 213/1000 | Loss: 0.00001321
Iteration 214/1000 | Loss: 0.00001320
Iteration 215/1000 | Loss: 0.00001320
Iteration 216/1000 | Loss: 0.00001320
Iteration 217/1000 | Loss: 0.00001320
Iteration 218/1000 | Loss: 0.00001320
Iteration 219/1000 | Loss: 0.00001319
Iteration 220/1000 | Loss: 0.00001319
Iteration 221/1000 | Loss: 0.00001319
Iteration 222/1000 | Loss: 0.00001319
Iteration 223/1000 | Loss: 0.00001319
Iteration 224/1000 | Loss: 0.00001319
Iteration 225/1000 | Loss: 0.00001318
Iteration 226/1000 | Loss: 0.00001318
Iteration 227/1000 | Loss: 0.00001318
Iteration 228/1000 | Loss: 0.00001318
Iteration 229/1000 | Loss: 0.00001318
Iteration 230/1000 | Loss: 0.00001318
Iteration 231/1000 | Loss: 0.00001318
Iteration 232/1000 | Loss: 0.00001318
Iteration 233/1000 | Loss: 0.00001317
Iteration 234/1000 | Loss: 0.00001317
Iteration 235/1000 | Loss: 0.00001317
Iteration 236/1000 | Loss: 0.00001317
Iteration 237/1000 | Loss: 0.00001317
Iteration 238/1000 | Loss: 0.00001317
Iteration 239/1000 | Loss: 0.00001317
Iteration 240/1000 | Loss: 0.00001317
Iteration 241/1000 | Loss: 0.00001317
Iteration 242/1000 | Loss: 0.00001316
Iteration 243/1000 | Loss: 0.00001316
Iteration 244/1000 | Loss: 0.00001316
Iteration 245/1000 | Loss: 0.00001316
Iteration 246/1000 | Loss: 0.00001316
Iteration 247/1000 | Loss: 0.00001316
Iteration 248/1000 | Loss: 0.00001316
Iteration 249/1000 | Loss: 0.00001316
Iteration 250/1000 | Loss: 0.00001316
Iteration 251/1000 | Loss: 0.00001316
Iteration 252/1000 | Loss: 0.00001316
Iteration 253/1000 | Loss: 0.00001315
Iteration 254/1000 | Loss: 0.00001315
Iteration 255/1000 | Loss: 0.00001315
Iteration 256/1000 | Loss: 0.00001315
Iteration 257/1000 | Loss: 0.00001315
Iteration 258/1000 | Loss: 0.00001315
Iteration 259/1000 | Loss: 0.00001315
Iteration 260/1000 | Loss: 0.00001315
Iteration 261/1000 | Loss: 0.00001315
Iteration 262/1000 | Loss: 0.00001315
Iteration 263/1000 | Loss: 0.00001315
Iteration 264/1000 | Loss: 0.00001315
Iteration 265/1000 | Loss: 0.00001315
Iteration 266/1000 | Loss: 0.00001315
Iteration 267/1000 | Loss: 0.00001315
Iteration 268/1000 | Loss: 0.00001315
Iteration 269/1000 | Loss: 0.00001315
Iteration 270/1000 | Loss: 0.00001315
Iteration 271/1000 | Loss: 0.00001315
Iteration 272/1000 | Loss: 0.00001315
Iteration 273/1000 | Loss: 0.00001315
Iteration 274/1000 | Loss: 0.00001315
Iteration 275/1000 | Loss: 0.00001315
Iteration 276/1000 | Loss: 0.00001315
Iteration 277/1000 | Loss: 0.00001315
Iteration 278/1000 | Loss: 0.00001314
Iteration 279/1000 | Loss: 0.00001314
Iteration 280/1000 | Loss: 0.00001314
Iteration 281/1000 | Loss: 0.00001314
Iteration 282/1000 | Loss: 0.00001314
Iteration 283/1000 | Loss: 0.00001314
Iteration 284/1000 | Loss: 0.00001314
Iteration 285/1000 | Loss: 0.00001314
Iteration 286/1000 | Loss: 0.00001314
Iteration 287/1000 | Loss: 0.00001314
Iteration 288/1000 | Loss: 0.00001314
Iteration 289/1000 | Loss: 0.00001314
Iteration 290/1000 | Loss: 0.00001314
Iteration 291/1000 | Loss: 0.00001314
Iteration 292/1000 | Loss: 0.00001314
Iteration 293/1000 | Loss: 0.00001314
Iteration 294/1000 | Loss: 0.00001314
Iteration 295/1000 | Loss: 0.00001314
Iteration 296/1000 | Loss: 0.00001314
Iteration 297/1000 | Loss: 0.00001314
Iteration 298/1000 | Loss: 0.00001314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 298. Stopping optimization.
Last 5 losses: [1.3144883268978447e-05, 1.3144883268978447e-05, 1.3144883268978447e-05, 1.3144883268978447e-05, 1.3144883268978447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3144883268978447e-05

Optimization complete. Final v2v error: 3.0958900451660156 mm

Highest mean error: 3.831965923309326 mm for frame 107

Lowest mean error: 2.568730354309082 mm for frame 160

Saving results

Total time: 49.44569754600525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00985126
Iteration 2/25 | Loss: 0.00143245
Iteration 3/25 | Loss: 0.00129737
Iteration 4/25 | Loss: 0.00127460
Iteration 5/25 | Loss: 0.00127236
Iteration 6/25 | Loss: 0.00125815
Iteration 7/25 | Loss: 0.00125613
Iteration 8/25 | Loss: 0.00125507
Iteration 9/25 | Loss: 0.00125431
Iteration 10/25 | Loss: 0.00125406
Iteration 11/25 | Loss: 0.00125399
Iteration 12/25 | Loss: 0.00125399
Iteration 13/25 | Loss: 0.00125399
Iteration 14/25 | Loss: 0.00125399
Iteration 15/25 | Loss: 0.00125399
Iteration 16/25 | Loss: 0.00125398
Iteration 17/25 | Loss: 0.00125398
Iteration 18/25 | Loss: 0.00125398
Iteration 19/25 | Loss: 0.00125398
Iteration 20/25 | Loss: 0.00125398
Iteration 21/25 | Loss: 0.00125398
Iteration 22/25 | Loss: 0.00125398
Iteration 23/25 | Loss: 0.00125398
Iteration 24/25 | Loss: 0.00125398
Iteration 25/25 | Loss: 0.00125398

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.08858109
Iteration 2/25 | Loss: 0.00117831
Iteration 3/25 | Loss: 0.00115946
Iteration 4/25 | Loss: 0.00115946
Iteration 5/25 | Loss: 0.00115945
Iteration 6/25 | Loss: 0.00115945
Iteration 7/25 | Loss: 0.00115945
Iteration 8/25 | Loss: 0.00115945
Iteration 9/25 | Loss: 0.00115945
Iteration 10/25 | Loss: 0.00115945
Iteration 11/25 | Loss: 0.00115945
Iteration 12/25 | Loss: 0.00115945
Iteration 13/25 | Loss: 0.00115945
Iteration 14/25 | Loss: 0.00115945
Iteration 15/25 | Loss: 0.00115945
Iteration 16/25 | Loss: 0.00115945
Iteration 17/25 | Loss: 0.00115945
Iteration 18/25 | Loss: 0.00115945
Iteration 19/25 | Loss: 0.00115945
Iteration 20/25 | Loss: 0.00115945
Iteration 21/25 | Loss: 0.00115945
Iteration 22/25 | Loss: 0.00115945
Iteration 23/25 | Loss: 0.00115945
Iteration 24/25 | Loss: 0.00115945
Iteration 25/25 | Loss: 0.00115945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115945
Iteration 2/1000 | Loss: 0.00005106
Iteration 3/1000 | Loss: 0.00004592
Iteration 4/1000 | Loss: 0.00006433
Iteration 5/1000 | Loss: 0.00001772
Iteration 6/1000 | Loss: 0.00001587
Iteration 7/1000 | Loss: 0.00001531
Iteration 8/1000 | Loss: 0.00001490
Iteration 9/1000 | Loss: 0.00004158
Iteration 10/1000 | Loss: 0.00001463
Iteration 11/1000 | Loss: 0.00001450
Iteration 12/1000 | Loss: 0.00003176
Iteration 13/1000 | Loss: 0.00001444
Iteration 14/1000 | Loss: 0.00001430
Iteration 15/1000 | Loss: 0.00001412
Iteration 16/1000 | Loss: 0.00004293
Iteration 17/1000 | Loss: 0.00001429
Iteration 18/1000 | Loss: 0.00002229
Iteration 19/1000 | Loss: 0.00005257
Iteration 20/1000 | Loss: 0.00001994
Iteration 21/1000 | Loss: 0.00001383
Iteration 22/1000 | Loss: 0.00001382
Iteration 23/1000 | Loss: 0.00001382
Iteration 24/1000 | Loss: 0.00001382
Iteration 25/1000 | Loss: 0.00001381
Iteration 26/1000 | Loss: 0.00001381
Iteration 27/1000 | Loss: 0.00001381
Iteration 28/1000 | Loss: 0.00001381
Iteration 29/1000 | Loss: 0.00001381
Iteration 30/1000 | Loss: 0.00001381
Iteration 31/1000 | Loss: 0.00001379
Iteration 32/1000 | Loss: 0.00001379
Iteration 33/1000 | Loss: 0.00001378
Iteration 34/1000 | Loss: 0.00001378
Iteration 35/1000 | Loss: 0.00001377
Iteration 36/1000 | Loss: 0.00001377
Iteration 37/1000 | Loss: 0.00001376
Iteration 38/1000 | Loss: 0.00001375
Iteration 39/1000 | Loss: 0.00001375
Iteration 40/1000 | Loss: 0.00001375
Iteration 41/1000 | Loss: 0.00001374
Iteration 42/1000 | Loss: 0.00001373
Iteration 43/1000 | Loss: 0.00001371
Iteration 44/1000 | Loss: 0.00001370
Iteration 45/1000 | Loss: 0.00001370
Iteration 46/1000 | Loss: 0.00001370
Iteration 47/1000 | Loss: 0.00001370
Iteration 48/1000 | Loss: 0.00001369
Iteration 49/1000 | Loss: 0.00001369
Iteration 50/1000 | Loss: 0.00001369
Iteration 51/1000 | Loss: 0.00001368
Iteration 52/1000 | Loss: 0.00001368
Iteration 53/1000 | Loss: 0.00001366
Iteration 54/1000 | Loss: 0.00001365
Iteration 55/1000 | Loss: 0.00001365
Iteration 56/1000 | Loss: 0.00001364
Iteration 57/1000 | Loss: 0.00001364
Iteration 58/1000 | Loss: 0.00001364
Iteration 59/1000 | Loss: 0.00001364
Iteration 60/1000 | Loss: 0.00001364
Iteration 61/1000 | Loss: 0.00001364
Iteration 62/1000 | Loss: 0.00001364
Iteration 63/1000 | Loss: 0.00001363
Iteration 64/1000 | Loss: 0.00001363
Iteration 65/1000 | Loss: 0.00001363
Iteration 66/1000 | Loss: 0.00001363
Iteration 67/1000 | Loss: 0.00001363
Iteration 68/1000 | Loss: 0.00001363
Iteration 69/1000 | Loss: 0.00001363
Iteration 70/1000 | Loss: 0.00001363
Iteration 71/1000 | Loss: 0.00001362
Iteration 72/1000 | Loss: 0.00001362
Iteration 73/1000 | Loss: 0.00001361
Iteration 74/1000 | Loss: 0.00001361
Iteration 75/1000 | Loss: 0.00001358
Iteration 76/1000 | Loss: 0.00001357
Iteration 77/1000 | Loss: 0.00001357
Iteration 78/1000 | Loss: 0.00001357
Iteration 79/1000 | Loss: 0.00001357
Iteration 80/1000 | Loss: 0.00001357
Iteration 81/1000 | Loss: 0.00001357
Iteration 82/1000 | Loss: 0.00001357
Iteration 83/1000 | Loss: 0.00001357
Iteration 84/1000 | Loss: 0.00001356
Iteration 85/1000 | Loss: 0.00001356
Iteration 86/1000 | Loss: 0.00001356
Iteration 87/1000 | Loss: 0.00001355
Iteration 88/1000 | Loss: 0.00001355
Iteration 89/1000 | Loss: 0.00001355
Iteration 90/1000 | Loss: 0.00001354
Iteration 91/1000 | Loss: 0.00001353
Iteration 92/1000 | Loss: 0.00001353
Iteration 93/1000 | Loss: 0.00001353
Iteration 94/1000 | Loss: 0.00001353
Iteration 95/1000 | Loss: 0.00001352
Iteration 96/1000 | Loss: 0.00001352
Iteration 97/1000 | Loss: 0.00001352
Iteration 98/1000 | Loss: 0.00001352
Iteration 99/1000 | Loss: 0.00001352
Iteration 100/1000 | Loss: 0.00001352
Iteration 101/1000 | Loss: 0.00001352
Iteration 102/1000 | Loss: 0.00001351
Iteration 103/1000 | Loss: 0.00001351
Iteration 104/1000 | Loss: 0.00001351
Iteration 105/1000 | Loss: 0.00001351
Iteration 106/1000 | Loss: 0.00001351
Iteration 107/1000 | Loss: 0.00001350
Iteration 108/1000 | Loss: 0.00001350
Iteration 109/1000 | Loss: 0.00001350
Iteration 110/1000 | Loss: 0.00001350
Iteration 111/1000 | Loss: 0.00001350
Iteration 112/1000 | Loss: 0.00001350
Iteration 113/1000 | Loss: 0.00001349
Iteration 114/1000 | Loss: 0.00001349
Iteration 115/1000 | Loss: 0.00001349
Iteration 116/1000 | Loss: 0.00001349
Iteration 117/1000 | Loss: 0.00001349
Iteration 118/1000 | Loss: 0.00001349
Iteration 119/1000 | Loss: 0.00001349
Iteration 120/1000 | Loss: 0.00001349
Iteration 121/1000 | Loss: 0.00001349
Iteration 122/1000 | Loss: 0.00001349
Iteration 123/1000 | Loss: 0.00001349
Iteration 124/1000 | Loss: 0.00001349
Iteration 125/1000 | Loss: 0.00001349
Iteration 126/1000 | Loss: 0.00001348
Iteration 127/1000 | Loss: 0.00001348
Iteration 128/1000 | Loss: 0.00001348
Iteration 129/1000 | Loss: 0.00001348
Iteration 130/1000 | Loss: 0.00001348
Iteration 131/1000 | Loss: 0.00001348
Iteration 132/1000 | Loss: 0.00001347
Iteration 133/1000 | Loss: 0.00001347
Iteration 134/1000 | Loss: 0.00001347
Iteration 135/1000 | Loss: 0.00001346
Iteration 136/1000 | Loss: 0.00001346
Iteration 137/1000 | Loss: 0.00001346
Iteration 138/1000 | Loss: 0.00001346
Iteration 139/1000 | Loss: 0.00001346
Iteration 140/1000 | Loss: 0.00001346
Iteration 141/1000 | Loss: 0.00001345
Iteration 142/1000 | Loss: 0.00001345
Iteration 143/1000 | Loss: 0.00001345
Iteration 144/1000 | Loss: 0.00001344
Iteration 145/1000 | Loss: 0.00001344
Iteration 146/1000 | Loss: 0.00001344
Iteration 147/1000 | Loss: 0.00001344
Iteration 148/1000 | Loss: 0.00001344
Iteration 149/1000 | Loss: 0.00001344
Iteration 150/1000 | Loss: 0.00001343
Iteration 151/1000 | Loss: 0.00001343
Iteration 152/1000 | Loss: 0.00001342
Iteration 153/1000 | Loss: 0.00001342
Iteration 154/1000 | Loss: 0.00001341
Iteration 155/1000 | Loss: 0.00001341
Iteration 156/1000 | Loss: 0.00001340
Iteration 157/1000 | Loss: 0.00001340
Iteration 158/1000 | Loss: 0.00001340
Iteration 159/1000 | Loss: 0.00001340
Iteration 160/1000 | Loss: 0.00001340
Iteration 161/1000 | Loss: 0.00001340
Iteration 162/1000 | Loss: 0.00001340
Iteration 163/1000 | Loss: 0.00001339
Iteration 164/1000 | Loss: 0.00001339
Iteration 165/1000 | Loss: 0.00001339
Iteration 166/1000 | Loss: 0.00001339
Iteration 167/1000 | Loss: 0.00001339
Iteration 168/1000 | Loss: 0.00001339
Iteration 169/1000 | Loss: 0.00001339
Iteration 170/1000 | Loss: 0.00001339
Iteration 171/1000 | Loss: 0.00001339
Iteration 172/1000 | Loss: 0.00001339
Iteration 173/1000 | Loss: 0.00001339
Iteration 174/1000 | Loss: 0.00001339
Iteration 175/1000 | Loss: 0.00001339
Iteration 176/1000 | Loss: 0.00001338
Iteration 177/1000 | Loss: 0.00001338
Iteration 178/1000 | Loss: 0.00001338
Iteration 179/1000 | Loss: 0.00001338
Iteration 180/1000 | Loss: 0.00001338
Iteration 181/1000 | Loss: 0.00001338
Iteration 182/1000 | Loss: 0.00001338
Iteration 183/1000 | Loss: 0.00001338
Iteration 184/1000 | Loss: 0.00001338
Iteration 185/1000 | Loss: 0.00001338
Iteration 186/1000 | Loss: 0.00001338
Iteration 187/1000 | Loss: 0.00001337
Iteration 188/1000 | Loss: 0.00001337
Iteration 189/1000 | Loss: 0.00001337
Iteration 190/1000 | Loss: 0.00001337
Iteration 191/1000 | Loss: 0.00001337
Iteration 192/1000 | Loss: 0.00001337
Iteration 193/1000 | Loss: 0.00001337
Iteration 194/1000 | Loss: 0.00001337
Iteration 195/1000 | Loss: 0.00001337
Iteration 196/1000 | Loss: 0.00001336
Iteration 197/1000 | Loss: 0.00001336
Iteration 198/1000 | Loss: 0.00001336
Iteration 199/1000 | Loss: 0.00001335
Iteration 200/1000 | Loss: 0.00001335
Iteration 201/1000 | Loss: 0.00001335
Iteration 202/1000 | Loss: 0.00001335
Iteration 203/1000 | Loss: 0.00001335
Iteration 204/1000 | Loss: 0.00001335
Iteration 205/1000 | Loss: 0.00001335
Iteration 206/1000 | Loss: 0.00001335
Iteration 207/1000 | Loss: 0.00001335
Iteration 208/1000 | Loss: 0.00001335
Iteration 209/1000 | Loss: 0.00001335
Iteration 210/1000 | Loss: 0.00001335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.3349535038287286e-05, 1.3349535038287286e-05, 1.3349535038287286e-05, 1.3349535038287286e-05, 1.3349535038287286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3349535038287286e-05

Optimization complete. Final v2v error: 3.078277587890625 mm

Highest mean error: 3.6035916805267334 mm for frame 172

Lowest mean error: 2.7792017459869385 mm for frame 137

Saving results

Total time: 64.52548670768738
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998961
Iteration 2/25 | Loss: 0.00998961
Iteration 3/25 | Loss: 0.00998961
Iteration 4/25 | Loss: 0.00998961
Iteration 5/25 | Loss: 0.00998961
Iteration 6/25 | Loss: 0.00998961
Iteration 7/25 | Loss: 0.00998961
Iteration 8/25 | Loss: 0.00998960
Iteration 9/25 | Loss: 0.00998960
Iteration 10/25 | Loss: 0.00998960
Iteration 11/25 | Loss: 0.00998960
Iteration 12/25 | Loss: 0.00998960
Iteration 13/25 | Loss: 0.00998960
Iteration 14/25 | Loss: 0.00998960
Iteration 15/25 | Loss: 0.00998960
Iteration 16/25 | Loss: 0.00998960
Iteration 17/25 | Loss: 0.00998960
Iteration 18/25 | Loss: 0.00998960
Iteration 19/25 | Loss: 0.00998960
Iteration 20/25 | Loss: 0.00998959
Iteration 21/25 | Loss: 0.00998959
Iteration 22/25 | Loss: 0.00998959
Iteration 23/25 | Loss: 0.00998959
Iteration 24/25 | Loss: 0.00998959
Iteration 25/25 | Loss: 0.00998959

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59092116
Iteration 2/25 | Loss: 0.11971222
Iteration 3/25 | Loss: 0.11947068
Iteration 4/25 | Loss: 0.11828317
Iteration 5/25 | Loss: 0.11828316
Iteration 6/25 | Loss: 0.11828316
Iteration 7/25 | Loss: 0.11828315
Iteration 8/25 | Loss: 0.11828315
Iteration 9/25 | Loss: 0.11828315
Iteration 10/25 | Loss: 0.11838561
Iteration 11/25 | Loss: 0.11828317
Iteration 12/25 | Loss: 0.11828317
Iteration 13/25 | Loss: 0.11828316
Iteration 14/25 | Loss: 0.11828316
Iteration 15/25 | Loss: 0.11828314
Iteration 16/25 | Loss: 0.11828314
Iteration 17/25 | Loss: 0.11828314
Iteration 18/25 | Loss: 0.11828314
Iteration 19/25 | Loss: 0.11828314
Iteration 20/25 | Loss: 0.11828314
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.11828313767910004, 0.11828313767910004, 0.11828313767910004, 0.11828313767910004, 0.11828313767910004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11828313767910004

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11828314
Iteration 2/1000 | Loss: 0.00306989
Iteration 3/1000 | Loss: 0.00817565
Iteration 4/1000 | Loss: 0.01615704
Iteration 5/1000 | Loss: 0.00294530
Iteration 6/1000 | Loss: 0.00976502
Iteration 7/1000 | Loss: 0.00106468
Iteration 8/1000 | Loss: 0.00106361
Iteration 9/1000 | Loss: 0.00021771
Iteration 10/1000 | Loss: 0.00027519
Iteration 11/1000 | Loss: 0.00024975
Iteration 12/1000 | Loss: 0.00016433
Iteration 13/1000 | Loss: 0.00005742
Iteration 14/1000 | Loss: 0.00016618
Iteration 15/1000 | Loss: 0.00087098
Iteration 16/1000 | Loss: 0.00008860
Iteration 17/1000 | Loss: 0.00009818
Iteration 18/1000 | Loss: 0.00005593
Iteration 19/1000 | Loss: 0.00053949
Iteration 20/1000 | Loss: 0.00004754
Iteration 21/1000 | Loss: 0.00007186
Iteration 22/1000 | Loss: 0.00006517
Iteration 23/1000 | Loss: 0.00002699
Iteration 24/1000 | Loss: 0.00005477
Iteration 25/1000 | Loss: 0.00002393
Iteration 26/1000 | Loss: 0.00013491
Iteration 27/1000 | Loss: 0.00023976
Iteration 28/1000 | Loss: 0.00195476
Iteration 29/1000 | Loss: 0.00500656
Iteration 30/1000 | Loss: 0.00538633
Iteration 31/1000 | Loss: 0.00070267
Iteration 32/1000 | Loss: 0.00015342
Iteration 33/1000 | Loss: 0.00074832
Iteration 34/1000 | Loss: 0.00010565
Iteration 35/1000 | Loss: 0.00023820
Iteration 36/1000 | Loss: 0.00006806
Iteration 37/1000 | Loss: 0.00023963
Iteration 38/1000 | Loss: 0.00005978
Iteration 39/1000 | Loss: 0.00039756
Iteration 40/1000 | Loss: 0.00007661
Iteration 41/1000 | Loss: 0.00040068
Iteration 42/1000 | Loss: 0.00002898
Iteration 43/1000 | Loss: 0.00003926
Iteration 44/1000 | Loss: 0.00009784
Iteration 45/1000 | Loss: 0.00002722
Iteration 46/1000 | Loss: 0.00003837
Iteration 47/1000 | Loss: 0.00005447
Iteration 48/1000 | Loss: 0.00012075
Iteration 49/1000 | Loss: 0.00036325
Iteration 50/1000 | Loss: 0.00002088
Iteration 51/1000 | Loss: 0.00009876
Iteration 52/1000 | Loss: 0.00031993
Iteration 53/1000 | Loss: 0.00002729
Iteration 54/1000 | Loss: 0.00004925
Iteration 55/1000 | Loss: 0.00001938
Iteration 56/1000 | Loss: 0.00005334
Iteration 57/1000 | Loss: 0.00003258
Iteration 58/1000 | Loss: 0.00006636
Iteration 59/1000 | Loss: 0.00030962
Iteration 60/1000 | Loss: 0.00004348
Iteration 61/1000 | Loss: 0.00002841
Iteration 62/1000 | Loss: 0.00020392
Iteration 63/1000 | Loss: 0.00007044
Iteration 64/1000 | Loss: 0.00052678
Iteration 65/1000 | Loss: 0.00009143
Iteration 66/1000 | Loss: 0.00005547
Iteration 67/1000 | Loss: 0.00003295
Iteration 68/1000 | Loss: 0.00002112
Iteration 69/1000 | Loss: 0.00004061
Iteration 70/1000 | Loss: 0.00003991
Iteration 71/1000 | Loss: 0.00002971
Iteration 72/1000 | Loss: 0.00001823
Iteration 73/1000 | Loss: 0.00001904
Iteration 74/1000 | Loss: 0.00001821
Iteration 75/1000 | Loss: 0.00001904
Iteration 76/1000 | Loss: 0.00001819
Iteration 77/1000 | Loss: 0.00001818
Iteration 78/1000 | Loss: 0.00001818
Iteration 79/1000 | Loss: 0.00001817
Iteration 80/1000 | Loss: 0.00001816
Iteration 81/1000 | Loss: 0.00001816
Iteration 82/1000 | Loss: 0.00001816
Iteration 83/1000 | Loss: 0.00002308
Iteration 84/1000 | Loss: 0.00010229
Iteration 85/1000 | Loss: 0.00004755
Iteration 86/1000 | Loss: 0.00005175
Iteration 87/1000 | Loss: 0.00001907
Iteration 88/1000 | Loss: 0.00002068
Iteration 89/1000 | Loss: 0.00005536
Iteration 90/1000 | Loss: 0.00002468
Iteration 91/1000 | Loss: 0.00003932
Iteration 92/1000 | Loss: 0.00002656
Iteration 93/1000 | Loss: 0.00001788
Iteration 94/1000 | Loss: 0.00002717
Iteration 95/1000 | Loss: 0.00002350
Iteration 96/1000 | Loss: 0.00001785
Iteration 97/1000 | Loss: 0.00001784
Iteration 98/1000 | Loss: 0.00001784
Iteration 99/1000 | Loss: 0.00001783
Iteration 100/1000 | Loss: 0.00001783
Iteration 101/1000 | Loss: 0.00001783
Iteration 102/1000 | Loss: 0.00002133
Iteration 103/1000 | Loss: 0.00001854
Iteration 104/1000 | Loss: 0.00001775
Iteration 105/1000 | Loss: 0.00001775
Iteration 106/1000 | Loss: 0.00001775
Iteration 107/1000 | Loss: 0.00001775
Iteration 108/1000 | Loss: 0.00001775
Iteration 109/1000 | Loss: 0.00001775
Iteration 110/1000 | Loss: 0.00001775
Iteration 111/1000 | Loss: 0.00001775
Iteration 112/1000 | Loss: 0.00008509
Iteration 113/1000 | Loss: 0.00002481
Iteration 114/1000 | Loss: 0.00001860
Iteration 115/1000 | Loss: 0.00001812
Iteration 116/1000 | Loss: 0.00003425
Iteration 117/1000 | Loss: 0.00003293
Iteration 118/1000 | Loss: 0.00001875
Iteration 119/1000 | Loss: 0.00001757
Iteration 120/1000 | Loss: 0.00001756
Iteration 121/1000 | Loss: 0.00001756
Iteration 122/1000 | Loss: 0.00001756
Iteration 123/1000 | Loss: 0.00001756
Iteration 124/1000 | Loss: 0.00001756
Iteration 125/1000 | Loss: 0.00001756
Iteration 126/1000 | Loss: 0.00001756
Iteration 127/1000 | Loss: 0.00001756
Iteration 128/1000 | Loss: 0.00003498
Iteration 129/1000 | Loss: 0.00003498
Iteration 130/1000 | Loss: 0.00037625
Iteration 131/1000 | Loss: 0.00002587
Iteration 132/1000 | Loss: 0.00004488
Iteration 133/1000 | Loss: 0.00002231
Iteration 134/1000 | Loss: 0.00001752
Iteration 135/1000 | Loss: 0.00001750
Iteration 136/1000 | Loss: 0.00001750
Iteration 137/1000 | Loss: 0.00001750
Iteration 138/1000 | Loss: 0.00001749
Iteration 139/1000 | Loss: 0.00001749
Iteration 140/1000 | Loss: 0.00001749
Iteration 141/1000 | Loss: 0.00001749
Iteration 142/1000 | Loss: 0.00001748
Iteration 143/1000 | Loss: 0.00002190
Iteration 144/1000 | Loss: 0.00001747
Iteration 145/1000 | Loss: 0.00001747
Iteration 146/1000 | Loss: 0.00001747
Iteration 147/1000 | Loss: 0.00001746
Iteration 148/1000 | Loss: 0.00001746
Iteration 149/1000 | Loss: 0.00001746
Iteration 150/1000 | Loss: 0.00001746
Iteration 151/1000 | Loss: 0.00002447
Iteration 152/1000 | Loss: 0.00003253
Iteration 153/1000 | Loss: 0.00002381
Iteration 154/1000 | Loss: 0.00001767
Iteration 155/1000 | Loss: 0.00001745
Iteration 156/1000 | Loss: 0.00001745
Iteration 157/1000 | Loss: 0.00001745
Iteration 158/1000 | Loss: 0.00001744
Iteration 159/1000 | Loss: 0.00001744
Iteration 160/1000 | Loss: 0.00001744
Iteration 161/1000 | Loss: 0.00001744
Iteration 162/1000 | Loss: 0.00001744
Iteration 163/1000 | Loss: 0.00001744
Iteration 164/1000 | Loss: 0.00001744
Iteration 165/1000 | Loss: 0.00001744
Iteration 166/1000 | Loss: 0.00001744
Iteration 167/1000 | Loss: 0.00001744
Iteration 168/1000 | Loss: 0.00001743
Iteration 169/1000 | Loss: 0.00002201
Iteration 170/1000 | Loss: 0.00002497
Iteration 171/1000 | Loss: 0.00002112
Iteration 172/1000 | Loss: 0.00003176
Iteration 173/1000 | Loss: 0.00001743
Iteration 174/1000 | Loss: 0.00001741
Iteration 175/1000 | Loss: 0.00001741
Iteration 176/1000 | Loss: 0.00002031
Iteration 177/1000 | Loss: 0.00001740
Iteration 178/1000 | Loss: 0.00001740
Iteration 179/1000 | Loss: 0.00001740
Iteration 180/1000 | Loss: 0.00001740
Iteration 181/1000 | Loss: 0.00001740
Iteration 182/1000 | Loss: 0.00001740
Iteration 183/1000 | Loss: 0.00001740
Iteration 184/1000 | Loss: 0.00002227
Iteration 185/1000 | Loss: 0.00001895
Iteration 186/1000 | Loss: 0.00001740
Iteration 187/1000 | Loss: 0.00001740
Iteration 188/1000 | Loss: 0.00001740
Iteration 189/1000 | Loss: 0.00001740
Iteration 190/1000 | Loss: 0.00001740
Iteration 191/1000 | Loss: 0.00001740
Iteration 192/1000 | Loss: 0.00001740
Iteration 193/1000 | Loss: 0.00001740
Iteration 194/1000 | Loss: 0.00001740
Iteration 195/1000 | Loss: 0.00001740
Iteration 196/1000 | Loss: 0.00001740
Iteration 197/1000 | Loss: 0.00001740
Iteration 198/1000 | Loss: 0.00001740
Iteration 199/1000 | Loss: 0.00001740
Iteration 200/1000 | Loss: 0.00001740
Iteration 201/1000 | Loss: 0.00001740
Iteration 202/1000 | Loss: 0.00001740
Iteration 203/1000 | Loss: 0.00001740
Iteration 204/1000 | Loss: 0.00001740
Iteration 205/1000 | Loss: 0.00001740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.7395725080859847e-05, 1.7395725080859847e-05, 1.7395725080859847e-05, 1.7395725080859847e-05, 1.7395725080859847e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7395725080859847e-05

Optimization complete. Final v2v error: 3.3708579540252686 mm

Highest mean error: 11.947237014770508 mm for frame 225

Lowest mean error: 3.2531375885009766 mm for frame 234

Saving results

Total time: 188.0121214389801
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829348
Iteration 2/25 | Loss: 0.00137394
Iteration 3/25 | Loss: 0.00124678
Iteration 4/25 | Loss: 0.00123691
Iteration 5/25 | Loss: 0.00123444
Iteration 6/25 | Loss: 0.00123407
Iteration 7/25 | Loss: 0.00123407
Iteration 8/25 | Loss: 0.00123407
Iteration 9/25 | Loss: 0.00123407
Iteration 10/25 | Loss: 0.00123407
Iteration 11/25 | Loss: 0.00123407
Iteration 12/25 | Loss: 0.00123407
Iteration 13/25 | Loss: 0.00123407
Iteration 14/25 | Loss: 0.00123407
Iteration 15/25 | Loss: 0.00123407
Iteration 16/25 | Loss: 0.00123407
Iteration 17/25 | Loss: 0.00123407
Iteration 18/25 | Loss: 0.00123407
Iteration 19/25 | Loss: 0.00123407
Iteration 20/25 | Loss: 0.00123407
Iteration 21/25 | Loss: 0.00123407
Iteration 22/25 | Loss: 0.00123407
Iteration 23/25 | Loss: 0.00123407
Iteration 24/25 | Loss: 0.00123407
Iteration 25/25 | Loss: 0.00123407

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45208085
Iteration 2/25 | Loss: 0.00094669
Iteration 3/25 | Loss: 0.00094668
Iteration 4/25 | Loss: 0.00094668
Iteration 5/25 | Loss: 0.00094668
Iteration 6/25 | Loss: 0.00094668
Iteration 7/25 | Loss: 0.00094668
Iteration 8/25 | Loss: 0.00094668
Iteration 9/25 | Loss: 0.00094668
Iteration 10/25 | Loss: 0.00094668
Iteration 11/25 | Loss: 0.00094668
Iteration 12/25 | Loss: 0.00094668
Iteration 13/25 | Loss: 0.00094668
Iteration 14/25 | Loss: 0.00094668
Iteration 15/25 | Loss: 0.00094668
Iteration 16/25 | Loss: 0.00094668
Iteration 17/25 | Loss: 0.00094668
Iteration 18/25 | Loss: 0.00094668
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009466814808547497, 0.0009466814808547497, 0.0009466814808547497, 0.0009466814808547497, 0.0009466814808547497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009466814808547497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094668
Iteration 2/1000 | Loss: 0.00003427
Iteration 3/1000 | Loss: 0.00002065
Iteration 4/1000 | Loss: 0.00001703
Iteration 5/1000 | Loss: 0.00001564
Iteration 6/1000 | Loss: 0.00001480
Iteration 7/1000 | Loss: 0.00001426
Iteration 8/1000 | Loss: 0.00001388
Iteration 9/1000 | Loss: 0.00001366
Iteration 10/1000 | Loss: 0.00001346
Iteration 11/1000 | Loss: 0.00001329
Iteration 12/1000 | Loss: 0.00001327
Iteration 13/1000 | Loss: 0.00001316
Iteration 14/1000 | Loss: 0.00001306
Iteration 15/1000 | Loss: 0.00001297
Iteration 16/1000 | Loss: 0.00001297
Iteration 17/1000 | Loss: 0.00001296
Iteration 18/1000 | Loss: 0.00001294
Iteration 19/1000 | Loss: 0.00001292
Iteration 20/1000 | Loss: 0.00001290
Iteration 21/1000 | Loss: 0.00001288
Iteration 22/1000 | Loss: 0.00001285
Iteration 23/1000 | Loss: 0.00001285
Iteration 24/1000 | Loss: 0.00001284
Iteration 25/1000 | Loss: 0.00001284
Iteration 26/1000 | Loss: 0.00001283
Iteration 27/1000 | Loss: 0.00001281
Iteration 28/1000 | Loss: 0.00001280
Iteration 29/1000 | Loss: 0.00001279
Iteration 30/1000 | Loss: 0.00001279
Iteration 31/1000 | Loss: 0.00001279
Iteration 32/1000 | Loss: 0.00001278
Iteration 33/1000 | Loss: 0.00001277
Iteration 34/1000 | Loss: 0.00001277
Iteration 35/1000 | Loss: 0.00001276
Iteration 36/1000 | Loss: 0.00001276
Iteration 37/1000 | Loss: 0.00001276
Iteration 38/1000 | Loss: 0.00001275
Iteration 39/1000 | Loss: 0.00001275
Iteration 40/1000 | Loss: 0.00001275
Iteration 41/1000 | Loss: 0.00001274
Iteration 42/1000 | Loss: 0.00001274
Iteration 43/1000 | Loss: 0.00001274
Iteration 44/1000 | Loss: 0.00001274
Iteration 45/1000 | Loss: 0.00001274
Iteration 46/1000 | Loss: 0.00001273
Iteration 47/1000 | Loss: 0.00001273
Iteration 48/1000 | Loss: 0.00001273
Iteration 49/1000 | Loss: 0.00001273
Iteration 50/1000 | Loss: 0.00001273
Iteration 51/1000 | Loss: 0.00001273
Iteration 52/1000 | Loss: 0.00001273
Iteration 53/1000 | Loss: 0.00001273
Iteration 54/1000 | Loss: 0.00001273
Iteration 55/1000 | Loss: 0.00001273
Iteration 56/1000 | Loss: 0.00001273
Iteration 57/1000 | Loss: 0.00001273
Iteration 58/1000 | Loss: 0.00001273
Iteration 59/1000 | Loss: 0.00001273
Iteration 60/1000 | Loss: 0.00001273
Iteration 61/1000 | Loss: 0.00001273
Iteration 62/1000 | Loss: 0.00001273
Iteration 63/1000 | Loss: 0.00001273
Iteration 64/1000 | Loss: 0.00001273
Iteration 65/1000 | Loss: 0.00001273
Iteration 66/1000 | Loss: 0.00001273
Iteration 67/1000 | Loss: 0.00001273
Iteration 68/1000 | Loss: 0.00001273
Iteration 69/1000 | Loss: 0.00001273
Iteration 70/1000 | Loss: 0.00001273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [1.2728865840472281e-05, 1.2728865840472281e-05, 1.2728865840472281e-05, 1.2728865840472281e-05, 1.2728865840472281e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2728865840472281e-05

Optimization complete. Final v2v error: 3.023320436477661 mm

Highest mean error: 3.8955612182617188 mm for frame 119

Lowest mean error: 2.718703269958496 mm for frame 30

Saving results

Total time: 33.01314306259155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531951
Iteration 2/25 | Loss: 0.00144566
Iteration 3/25 | Loss: 0.00132509
Iteration 4/25 | Loss: 0.00131155
Iteration 5/25 | Loss: 0.00130884
Iteration 6/25 | Loss: 0.00130839
Iteration 7/25 | Loss: 0.00130839
Iteration 8/25 | Loss: 0.00130839
Iteration 9/25 | Loss: 0.00130839
Iteration 10/25 | Loss: 0.00130839
Iteration 11/25 | Loss: 0.00130839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013083862140774727, 0.0013083862140774727, 0.0013083862140774727, 0.0013083862140774727, 0.0013083862140774727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013083862140774727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47207546
Iteration 2/25 | Loss: 0.00073630
Iteration 3/25 | Loss: 0.00073629
Iteration 4/25 | Loss: 0.00073629
Iteration 5/25 | Loss: 0.00073629
Iteration 6/25 | Loss: 0.00073629
Iteration 7/25 | Loss: 0.00073629
Iteration 8/25 | Loss: 0.00073629
Iteration 9/25 | Loss: 0.00073629
Iteration 10/25 | Loss: 0.00073629
Iteration 11/25 | Loss: 0.00073629
Iteration 12/25 | Loss: 0.00073629
Iteration 13/25 | Loss: 0.00073629
Iteration 14/25 | Loss: 0.00073629
Iteration 15/25 | Loss: 0.00073629
Iteration 16/25 | Loss: 0.00073629
Iteration 17/25 | Loss: 0.00073629
Iteration 18/25 | Loss: 0.00073629
Iteration 19/25 | Loss: 0.00073629
Iteration 20/25 | Loss: 0.00073629
Iteration 21/25 | Loss: 0.00073629
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007362900068983436, 0.0007362900068983436, 0.0007362900068983436, 0.0007362900068983436, 0.0007362900068983436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007362900068983436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073629
Iteration 2/1000 | Loss: 0.00004356
Iteration 3/1000 | Loss: 0.00003466
Iteration 4/1000 | Loss: 0.00002925
Iteration 5/1000 | Loss: 0.00002769
Iteration 6/1000 | Loss: 0.00002653
Iteration 7/1000 | Loss: 0.00002583
Iteration 8/1000 | Loss: 0.00002535
Iteration 9/1000 | Loss: 0.00002486
Iteration 10/1000 | Loss: 0.00002445
Iteration 11/1000 | Loss: 0.00002410
Iteration 12/1000 | Loss: 0.00002399
Iteration 13/1000 | Loss: 0.00002382
Iteration 14/1000 | Loss: 0.00002362
Iteration 15/1000 | Loss: 0.00002361
Iteration 16/1000 | Loss: 0.00002344
Iteration 17/1000 | Loss: 0.00002340
Iteration 18/1000 | Loss: 0.00002337
Iteration 19/1000 | Loss: 0.00002336
Iteration 20/1000 | Loss: 0.00002334
Iteration 21/1000 | Loss: 0.00002330
Iteration 22/1000 | Loss: 0.00002330
Iteration 23/1000 | Loss: 0.00002329
Iteration 24/1000 | Loss: 0.00002329
Iteration 25/1000 | Loss: 0.00002329
Iteration 26/1000 | Loss: 0.00002329
Iteration 27/1000 | Loss: 0.00002328
Iteration 28/1000 | Loss: 0.00002327
Iteration 29/1000 | Loss: 0.00002327
Iteration 30/1000 | Loss: 0.00002327
Iteration 31/1000 | Loss: 0.00002327
Iteration 32/1000 | Loss: 0.00002327
Iteration 33/1000 | Loss: 0.00002327
Iteration 34/1000 | Loss: 0.00002327
Iteration 35/1000 | Loss: 0.00002327
Iteration 36/1000 | Loss: 0.00002326
Iteration 37/1000 | Loss: 0.00002326
Iteration 38/1000 | Loss: 0.00002326
Iteration 39/1000 | Loss: 0.00002326
Iteration 40/1000 | Loss: 0.00002326
Iteration 41/1000 | Loss: 0.00002326
Iteration 42/1000 | Loss: 0.00002325
Iteration 43/1000 | Loss: 0.00002325
Iteration 44/1000 | Loss: 0.00002325
Iteration 45/1000 | Loss: 0.00002324
Iteration 46/1000 | Loss: 0.00002324
Iteration 47/1000 | Loss: 0.00002324
Iteration 48/1000 | Loss: 0.00002324
Iteration 49/1000 | Loss: 0.00002324
Iteration 50/1000 | Loss: 0.00002324
Iteration 51/1000 | Loss: 0.00002324
Iteration 52/1000 | Loss: 0.00002323
Iteration 53/1000 | Loss: 0.00002323
Iteration 54/1000 | Loss: 0.00002323
Iteration 55/1000 | Loss: 0.00002322
Iteration 56/1000 | Loss: 0.00002322
Iteration 57/1000 | Loss: 0.00002322
Iteration 58/1000 | Loss: 0.00002322
Iteration 59/1000 | Loss: 0.00002322
Iteration 60/1000 | Loss: 0.00002322
Iteration 61/1000 | Loss: 0.00002322
Iteration 62/1000 | Loss: 0.00002322
Iteration 63/1000 | Loss: 0.00002322
Iteration 64/1000 | Loss: 0.00002321
Iteration 65/1000 | Loss: 0.00002321
Iteration 66/1000 | Loss: 0.00002321
Iteration 67/1000 | Loss: 0.00002321
Iteration 68/1000 | Loss: 0.00002321
Iteration 69/1000 | Loss: 0.00002321
Iteration 70/1000 | Loss: 0.00002320
Iteration 71/1000 | Loss: 0.00002320
Iteration 72/1000 | Loss: 0.00002320
Iteration 73/1000 | Loss: 0.00002320
Iteration 74/1000 | Loss: 0.00002320
Iteration 75/1000 | Loss: 0.00002320
Iteration 76/1000 | Loss: 0.00002320
Iteration 77/1000 | Loss: 0.00002320
Iteration 78/1000 | Loss: 0.00002320
Iteration 79/1000 | Loss: 0.00002319
Iteration 80/1000 | Loss: 0.00002319
Iteration 81/1000 | Loss: 0.00002319
Iteration 82/1000 | Loss: 0.00002319
Iteration 83/1000 | Loss: 0.00002319
Iteration 84/1000 | Loss: 0.00002319
Iteration 85/1000 | Loss: 0.00002319
Iteration 86/1000 | Loss: 0.00002319
Iteration 87/1000 | Loss: 0.00002319
Iteration 88/1000 | Loss: 0.00002319
Iteration 89/1000 | Loss: 0.00002318
Iteration 90/1000 | Loss: 0.00002318
Iteration 91/1000 | Loss: 0.00002318
Iteration 92/1000 | Loss: 0.00002318
Iteration 93/1000 | Loss: 0.00002318
Iteration 94/1000 | Loss: 0.00002318
Iteration 95/1000 | Loss: 0.00002318
Iteration 96/1000 | Loss: 0.00002318
Iteration 97/1000 | Loss: 0.00002318
Iteration 98/1000 | Loss: 0.00002318
Iteration 99/1000 | Loss: 0.00002318
Iteration 100/1000 | Loss: 0.00002318
Iteration 101/1000 | Loss: 0.00002318
Iteration 102/1000 | Loss: 0.00002318
Iteration 103/1000 | Loss: 0.00002318
Iteration 104/1000 | Loss: 0.00002318
Iteration 105/1000 | Loss: 0.00002318
Iteration 106/1000 | Loss: 0.00002318
Iteration 107/1000 | Loss: 0.00002318
Iteration 108/1000 | Loss: 0.00002318
Iteration 109/1000 | Loss: 0.00002318
Iteration 110/1000 | Loss: 0.00002318
Iteration 111/1000 | Loss: 0.00002318
Iteration 112/1000 | Loss: 0.00002318
Iteration 113/1000 | Loss: 0.00002318
Iteration 114/1000 | Loss: 0.00002318
Iteration 115/1000 | Loss: 0.00002318
Iteration 116/1000 | Loss: 0.00002318
Iteration 117/1000 | Loss: 0.00002318
Iteration 118/1000 | Loss: 0.00002318
Iteration 119/1000 | Loss: 0.00002318
Iteration 120/1000 | Loss: 0.00002318
Iteration 121/1000 | Loss: 0.00002318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.318250699318014e-05, 2.318250699318014e-05, 2.318250699318014e-05, 2.318250699318014e-05, 2.318250699318014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.318250699318014e-05

Optimization complete. Final v2v error: 4.023866176605225 mm

Highest mean error: 4.4397101402282715 mm for frame 78

Lowest mean error: 3.730848550796509 mm for frame 13

Saving results

Total time: 36.42819285392761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883447
Iteration 2/25 | Loss: 0.00165275
Iteration 3/25 | Loss: 0.00134558
Iteration 4/25 | Loss: 0.00129503
Iteration 5/25 | Loss: 0.00128073
Iteration 6/25 | Loss: 0.00127761
Iteration 7/25 | Loss: 0.00127707
Iteration 8/25 | Loss: 0.00127707
Iteration 9/25 | Loss: 0.00127707
Iteration 10/25 | Loss: 0.00127707
Iteration 11/25 | Loss: 0.00127707
Iteration 12/25 | Loss: 0.00127707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00127707002684474, 0.00127707002684474, 0.00127707002684474, 0.00127707002684474, 0.00127707002684474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00127707002684474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33900285
Iteration 2/25 | Loss: 0.00127010
Iteration 3/25 | Loss: 0.00127009
Iteration 4/25 | Loss: 0.00127009
Iteration 5/25 | Loss: 0.00127009
Iteration 6/25 | Loss: 0.00127009
Iteration 7/25 | Loss: 0.00127009
Iteration 8/25 | Loss: 0.00127009
Iteration 9/25 | Loss: 0.00127009
Iteration 10/25 | Loss: 0.00127009
Iteration 11/25 | Loss: 0.00127009
Iteration 12/25 | Loss: 0.00127009
Iteration 13/25 | Loss: 0.00127009
Iteration 14/25 | Loss: 0.00127009
Iteration 15/25 | Loss: 0.00127009
Iteration 16/25 | Loss: 0.00127009
Iteration 17/25 | Loss: 0.00127009
Iteration 18/25 | Loss: 0.00127009
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012700918596237898, 0.0012700918596237898, 0.0012700918596237898, 0.0012700918596237898, 0.0012700918596237898]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012700918596237898

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127009
Iteration 2/1000 | Loss: 0.00008378
Iteration 3/1000 | Loss: 0.00005383
Iteration 4/1000 | Loss: 0.00003843
Iteration 5/1000 | Loss: 0.00003500
Iteration 6/1000 | Loss: 0.00003316
Iteration 7/1000 | Loss: 0.00003199
Iteration 8/1000 | Loss: 0.00003081
Iteration 9/1000 | Loss: 0.00003012
Iteration 10/1000 | Loss: 0.00002960
Iteration 11/1000 | Loss: 0.00002917
Iteration 12/1000 | Loss: 0.00002881
Iteration 13/1000 | Loss: 0.00002853
Iteration 14/1000 | Loss: 0.00002830
Iteration 15/1000 | Loss: 0.00002814
Iteration 16/1000 | Loss: 0.00002797
Iteration 17/1000 | Loss: 0.00002782
Iteration 18/1000 | Loss: 0.00002782
Iteration 19/1000 | Loss: 0.00002778
Iteration 20/1000 | Loss: 0.00002775
Iteration 21/1000 | Loss: 0.00002774
Iteration 22/1000 | Loss: 0.00002773
Iteration 23/1000 | Loss: 0.00002769
Iteration 24/1000 | Loss: 0.00002769
Iteration 25/1000 | Loss: 0.00002766
Iteration 26/1000 | Loss: 0.00002766
Iteration 27/1000 | Loss: 0.00002763
Iteration 28/1000 | Loss: 0.00002761
Iteration 29/1000 | Loss: 0.00002760
Iteration 30/1000 | Loss: 0.00002760
Iteration 31/1000 | Loss: 0.00002760
Iteration 32/1000 | Loss: 0.00002760
Iteration 33/1000 | Loss: 0.00002760
Iteration 34/1000 | Loss: 0.00002759
Iteration 35/1000 | Loss: 0.00002759
Iteration 36/1000 | Loss: 0.00002759
Iteration 37/1000 | Loss: 0.00002758
Iteration 38/1000 | Loss: 0.00002758
Iteration 39/1000 | Loss: 0.00002757
Iteration 40/1000 | Loss: 0.00002757
Iteration 41/1000 | Loss: 0.00002756
Iteration 42/1000 | Loss: 0.00002756
Iteration 43/1000 | Loss: 0.00002756
Iteration 44/1000 | Loss: 0.00002756
Iteration 45/1000 | Loss: 0.00002755
Iteration 46/1000 | Loss: 0.00002754
Iteration 47/1000 | Loss: 0.00002754
Iteration 48/1000 | Loss: 0.00002754
Iteration 49/1000 | Loss: 0.00002752
Iteration 50/1000 | Loss: 0.00002752
Iteration 51/1000 | Loss: 0.00002751
Iteration 52/1000 | Loss: 0.00002750
Iteration 53/1000 | Loss: 0.00002750
Iteration 54/1000 | Loss: 0.00002750
Iteration 55/1000 | Loss: 0.00002750
Iteration 56/1000 | Loss: 0.00002750
Iteration 57/1000 | Loss: 0.00002750
Iteration 58/1000 | Loss: 0.00002750
Iteration 59/1000 | Loss: 0.00002750
Iteration 60/1000 | Loss: 0.00002749
Iteration 61/1000 | Loss: 0.00002749
Iteration 62/1000 | Loss: 0.00002749
Iteration 63/1000 | Loss: 0.00002749
Iteration 64/1000 | Loss: 0.00002749
Iteration 65/1000 | Loss: 0.00002749
Iteration 66/1000 | Loss: 0.00002749
Iteration 67/1000 | Loss: 0.00002749
Iteration 68/1000 | Loss: 0.00002749
Iteration 69/1000 | Loss: 0.00002749
Iteration 70/1000 | Loss: 0.00002748
Iteration 71/1000 | Loss: 0.00002748
Iteration 72/1000 | Loss: 0.00002748
Iteration 73/1000 | Loss: 0.00002748
Iteration 74/1000 | Loss: 0.00002748
Iteration 75/1000 | Loss: 0.00002748
Iteration 76/1000 | Loss: 0.00002748
Iteration 77/1000 | Loss: 0.00002748
Iteration 78/1000 | Loss: 0.00002748
Iteration 79/1000 | Loss: 0.00002748
Iteration 80/1000 | Loss: 0.00002747
Iteration 81/1000 | Loss: 0.00002747
Iteration 82/1000 | Loss: 0.00002747
Iteration 83/1000 | Loss: 0.00002747
Iteration 84/1000 | Loss: 0.00002747
Iteration 85/1000 | Loss: 0.00002747
Iteration 86/1000 | Loss: 0.00002747
Iteration 87/1000 | Loss: 0.00002747
Iteration 88/1000 | Loss: 0.00002746
Iteration 89/1000 | Loss: 0.00002746
Iteration 90/1000 | Loss: 0.00002746
Iteration 91/1000 | Loss: 0.00002745
Iteration 92/1000 | Loss: 0.00002745
Iteration 93/1000 | Loss: 0.00002745
Iteration 94/1000 | Loss: 0.00002745
Iteration 95/1000 | Loss: 0.00002745
Iteration 96/1000 | Loss: 0.00002745
Iteration 97/1000 | Loss: 0.00002745
Iteration 98/1000 | Loss: 0.00002745
Iteration 99/1000 | Loss: 0.00002745
Iteration 100/1000 | Loss: 0.00002745
Iteration 101/1000 | Loss: 0.00002745
Iteration 102/1000 | Loss: 0.00002744
Iteration 103/1000 | Loss: 0.00002744
Iteration 104/1000 | Loss: 0.00002744
Iteration 105/1000 | Loss: 0.00002744
Iteration 106/1000 | Loss: 0.00002743
Iteration 107/1000 | Loss: 0.00002743
Iteration 108/1000 | Loss: 0.00002743
Iteration 109/1000 | Loss: 0.00002743
Iteration 110/1000 | Loss: 0.00002743
Iteration 111/1000 | Loss: 0.00002743
Iteration 112/1000 | Loss: 0.00002743
Iteration 113/1000 | Loss: 0.00002743
Iteration 114/1000 | Loss: 0.00002742
Iteration 115/1000 | Loss: 0.00002742
Iteration 116/1000 | Loss: 0.00002742
Iteration 117/1000 | Loss: 0.00002742
Iteration 118/1000 | Loss: 0.00002742
Iteration 119/1000 | Loss: 0.00002742
Iteration 120/1000 | Loss: 0.00002742
Iteration 121/1000 | Loss: 0.00002741
Iteration 122/1000 | Loss: 0.00002741
Iteration 123/1000 | Loss: 0.00002741
Iteration 124/1000 | Loss: 0.00002741
Iteration 125/1000 | Loss: 0.00002741
Iteration 126/1000 | Loss: 0.00002741
Iteration 127/1000 | Loss: 0.00002741
Iteration 128/1000 | Loss: 0.00002741
Iteration 129/1000 | Loss: 0.00002741
Iteration 130/1000 | Loss: 0.00002741
Iteration 131/1000 | Loss: 0.00002741
Iteration 132/1000 | Loss: 0.00002741
Iteration 133/1000 | Loss: 0.00002741
Iteration 134/1000 | Loss: 0.00002741
Iteration 135/1000 | Loss: 0.00002740
Iteration 136/1000 | Loss: 0.00002740
Iteration 137/1000 | Loss: 0.00002740
Iteration 138/1000 | Loss: 0.00002740
Iteration 139/1000 | Loss: 0.00002740
Iteration 140/1000 | Loss: 0.00002740
Iteration 141/1000 | Loss: 0.00002740
Iteration 142/1000 | Loss: 0.00002740
Iteration 143/1000 | Loss: 0.00002740
Iteration 144/1000 | Loss: 0.00002740
Iteration 145/1000 | Loss: 0.00002740
Iteration 146/1000 | Loss: 0.00002740
Iteration 147/1000 | Loss: 0.00002740
Iteration 148/1000 | Loss: 0.00002740
Iteration 149/1000 | Loss: 0.00002740
Iteration 150/1000 | Loss: 0.00002740
Iteration 151/1000 | Loss: 0.00002740
Iteration 152/1000 | Loss: 0.00002740
Iteration 153/1000 | Loss: 0.00002740
Iteration 154/1000 | Loss: 0.00002740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.7397729354561307e-05, 2.7397729354561307e-05, 2.7397729354561307e-05, 2.7397729354561307e-05, 2.7397729354561307e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7397729354561307e-05

Optimization complete. Final v2v error: 4.258802890777588 mm

Highest mean error: 7.259466171264648 mm for frame 116

Lowest mean error: 2.9157841205596924 mm for frame 76

Saving results

Total time: 45.968634605407715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951662
Iteration 2/25 | Loss: 0.00243722
Iteration 3/25 | Loss: 0.00194131
Iteration 4/25 | Loss: 0.00160066
Iteration 5/25 | Loss: 0.00159600
Iteration 6/25 | Loss: 0.00159197
Iteration 7/25 | Loss: 0.00152497
Iteration 8/25 | Loss: 0.00145997
Iteration 9/25 | Loss: 0.00144141
Iteration 10/25 | Loss: 0.00144422
Iteration 11/25 | Loss: 0.00143643
Iteration 12/25 | Loss: 0.00142561
Iteration 13/25 | Loss: 0.00141828
Iteration 14/25 | Loss: 0.00141666
Iteration 15/25 | Loss: 0.00141832
Iteration 16/25 | Loss: 0.00141619
Iteration 17/25 | Loss: 0.00141879
Iteration 18/25 | Loss: 0.00141661
Iteration 19/25 | Loss: 0.00141622
Iteration 20/25 | Loss: 0.00141230
Iteration 21/25 | Loss: 0.00141380
Iteration 22/25 | Loss: 0.00141142
Iteration 23/25 | Loss: 0.00141100
Iteration 24/25 | Loss: 0.00141062
Iteration 25/25 | Loss: 0.00140959

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.38117266
Iteration 2/25 | Loss: 0.00220534
Iteration 3/25 | Loss: 0.00220534
Iteration 4/25 | Loss: 0.00220534
Iteration 5/25 | Loss: 0.00220534
Iteration 6/25 | Loss: 0.00220534
Iteration 7/25 | Loss: 0.00220533
Iteration 8/25 | Loss: 0.00220533
Iteration 9/25 | Loss: 0.00220533
Iteration 10/25 | Loss: 0.00220533
Iteration 11/25 | Loss: 0.00220533
Iteration 12/25 | Loss: 0.00220533
Iteration 13/25 | Loss: 0.00220533
Iteration 14/25 | Loss: 0.00220533
Iteration 15/25 | Loss: 0.00220533
Iteration 16/25 | Loss: 0.00220533
Iteration 17/25 | Loss: 0.00220533
Iteration 18/25 | Loss: 0.00220533
Iteration 19/25 | Loss: 0.00220533
Iteration 20/25 | Loss: 0.00220533
Iteration 21/25 | Loss: 0.00220533
Iteration 22/25 | Loss: 0.00220533
Iteration 23/25 | Loss: 0.00220533
Iteration 24/25 | Loss: 0.00220533
Iteration 25/25 | Loss: 0.00220533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220533
Iteration 2/1000 | Loss: 0.00086520
Iteration 3/1000 | Loss: 0.00051004
Iteration 4/1000 | Loss: 0.00282870
Iteration 5/1000 | Loss: 0.00181898
Iteration 6/1000 | Loss: 0.00091222
Iteration 7/1000 | Loss: 0.00018858
Iteration 8/1000 | Loss: 0.00094652
Iteration 9/1000 | Loss: 0.00012214
Iteration 10/1000 | Loss: 0.00009013
Iteration 11/1000 | Loss: 0.00063689
Iteration 12/1000 | Loss: 0.00075221
Iteration 13/1000 | Loss: 0.00016396
Iteration 14/1000 | Loss: 0.00006421
Iteration 15/1000 | Loss: 0.00005529
Iteration 16/1000 | Loss: 0.00004965
Iteration 17/1000 | Loss: 0.00004511
Iteration 18/1000 | Loss: 0.00004199
Iteration 19/1000 | Loss: 0.00003932
Iteration 20/1000 | Loss: 0.00030733
Iteration 21/1000 | Loss: 0.00023539
Iteration 22/1000 | Loss: 0.00050976
Iteration 23/1000 | Loss: 0.00046945
Iteration 24/1000 | Loss: 0.00006560
Iteration 25/1000 | Loss: 0.00121447
Iteration 26/1000 | Loss: 0.00005554
Iteration 27/1000 | Loss: 0.00004377
Iteration 28/1000 | Loss: 0.00003784
Iteration 29/1000 | Loss: 0.00003525
Iteration 30/1000 | Loss: 0.00003417
Iteration 31/1000 | Loss: 0.00003333
Iteration 32/1000 | Loss: 0.00049506
Iteration 33/1000 | Loss: 0.00007722
Iteration 34/1000 | Loss: 0.00039843
Iteration 35/1000 | Loss: 0.00019483
Iteration 36/1000 | Loss: 0.00005159
Iteration 37/1000 | Loss: 0.00004206
Iteration 38/1000 | Loss: 0.00003835
Iteration 39/1000 | Loss: 0.00003560
Iteration 40/1000 | Loss: 0.00015817
Iteration 41/1000 | Loss: 0.00015076
Iteration 42/1000 | Loss: 0.00016144
Iteration 43/1000 | Loss: 0.00015265
Iteration 44/1000 | Loss: 0.00015533
Iteration 45/1000 | Loss: 0.00015983
Iteration 46/1000 | Loss: 0.00108238
Iteration 47/1000 | Loss: 0.00050490
Iteration 48/1000 | Loss: 0.00040246
Iteration 49/1000 | Loss: 0.00020521
Iteration 50/1000 | Loss: 0.00004993
Iteration 51/1000 | Loss: 0.00004711
Iteration 52/1000 | Loss: 0.00003199
Iteration 53/1000 | Loss: 0.00015209
Iteration 54/1000 | Loss: 0.00042636
Iteration 55/1000 | Loss: 0.00025562
Iteration 56/1000 | Loss: 0.00004462
Iteration 57/1000 | Loss: 0.00003397
Iteration 58/1000 | Loss: 0.00003122
Iteration 59/1000 | Loss: 0.00002954
Iteration 60/1000 | Loss: 0.00018415
Iteration 61/1000 | Loss: 0.00002938
Iteration 62/1000 | Loss: 0.00002687
Iteration 63/1000 | Loss: 0.00002524
Iteration 64/1000 | Loss: 0.00002423
Iteration 65/1000 | Loss: 0.00002366
Iteration 66/1000 | Loss: 0.00002320
Iteration 67/1000 | Loss: 0.00002283
Iteration 68/1000 | Loss: 0.00060479
Iteration 69/1000 | Loss: 0.00002520
Iteration 70/1000 | Loss: 0.00002234
Iteration 71/1000 | Loss: 0.00002156
Iteration 72/1000 | Loss: 0.00002091
Iteration 73/1000 | Loss: 0.00002031
Iteration 74/1000 | Loss: 0.00001984
Iteration 75/1000 | Loss: 0.00001954
Iteration 76/1000 | Loss: 0.00001938
Iteration 77/1000 | Loss: 0.00001927
Iteration 78/1000 | Loss: 0.00001927
Iteration 79/1000 | Loss: 0.00001926
Iteration 80/1000 | Loss: 0.00001926
Iteration 81/1000 | Loss: 0.00001925
Iteration 82/1000 | Loss: 0.00001923
Iteration 83/1000 | Loss: 0.00001921
Iteration 84/1000 | Loss: 0.00001920
Iteration 85/1000 | Loss: 0.00001920
Iteration 86/1000 | Loss: 0.00001920
Iteration 87/1000 | Loss: 0.00001919
Iteration 88/1000 | Loss: 0.00001919
Iteration 89/1000 | Loss: 0.00001918
Iteration 90/1000 | Loss: 0.00001918
Iteration 91/1000 | Loss: 0.00001917
Iteration 92/1000 | Loss: 0.00001916
Iteration 93/1000 | Loss: 0.00001916
Iteration 94/1000 | Loss: 0.00001915
Iteration 95/1000 | Loss: 0.00001912
Iteration 96/1000 | Loss: 0.00001911
Iteration 97/1000 | Loss: 0.00001911
Iteration 98/1000 | Loss: 0.00001910
Iteration 99/1000 | Loss: 0.00001910
Iteration 100/1000 | Loss: 0.00001910
Iteration 101/1000 | Loss: 0.00001909
Iteration 102/1000 | Loss: 0.00001909
Iteration 103/1000 | Loss: 0.00001909
Iteration 104/1000 | Loss: 0.00001909
Iteration 105/1000 | Loss: 0.00001909
Iteration 106/1000 | Loss: 0.00001909
Iteration 107/1000 | Loss: 0.00001909
Iteration 108/1000 | Loss: 0.00001909
Iteration 109/1000 | Loss: 0.00001908
Iteration 110/1000 | Loss: 0.00001908
Iteration 111/1000 | Loss: 0.00001908
Iteration 112/1000 | Loss: 0.00001908
Iteration 113/1000 | Loss: 0.00001908
Iteration 114/1000 | Loss: 0.00001907
Iteration 115/1000 | Loss: 0.00001907
Iteration 116/1000 | Loss: 0.00001907
Iteration 117/1000 | Loss: 0.00001906
Iteration 118/1000 | Loss: 0.00001906
Iteration 119/1000 | Loss: 0.00001906
Iteration 120/1000 | Loss: 0.00001906
Iteration 121/1000 | Loss: 0.00001905
Iteration 122/1000 | Loss: 0.00001905
Iteration 123/1000 | Loss: 0.00001905
Iteration 124/1000 | Loss: 0.00001905
Iteration 125/1000 | Loss: 0.00001905
Iteration 126/1000 | Loss: 0.00001905
Iteration 127/1000 | Loss: 0.00001904
Iteration 128/1000 | Loss: 0.00001904
Iteration 129/1000 | Loss: 0.00001904
Iteration 130/1000 | Loss: 0.00001904
Iteration 131/1000 | Loss: 0.00001904
Iteration 132/1000 | Loss: 0.00001904
Iteration 133/1000 | Loss: 0.00001904
Iteration 134/1000 | Loss: 0.00001903
Iteration 135/1000 | Loss: 0.00001903
Iteration 136/1000 | Loss: 0.00001903
Iteration 137/1000 | Loss: 0.00001903
Iteration 138/1000 | Loss: 0.00001902
Iteration 139/1000 | Loss: 0.00001902
Iteration 140/1000 | Loss: 0.00001902
Iteration 141/1000 | Loss: 0.00001902
Iteration 142/1000 | Loss: 0.00001902
Iteration 143/1000 | Loss: 0.00001902
Iteration 144/1000 | Loss: 0.00001902
Iteration 145/1000 | Loss: 0.00001902
Iteration 146/1000 | Loss: 0.00001902
Iteration 147/1000 | Loss: 0.00001902
Iteration 148/1000 | Loss: 0.00001901
Iteration 149/1000 | Loss: 0.00001901
Iteration 150/1000 | Loss: 0.00001901
Iteration 151/1000 | Loss: 0.00001901
Iteration 152/1000 | Loss: 0.00001901
Iteration 153/1000 | Loss: 0.00001901
Iteration 154/1000 | Loss: 0.00001901
Iteration 155/1000 | Loss: 0.00001901
Iteration 156/1000 | Loss: 0.00001901
Iteration 157/1000 | Loss: 0.00001901
Iteration 158/1000 | Loss: 0.00001901
Iteration 159/1000 | Loss: 0.00001900
Iteration 160/1000 | Loss: 0.00001900
Iteration 161/1000 | Loss: 0.00001900
Iteration 162/1000 | Loss: 0.00001900
Iteration 163/1000 | Loss: 0.00001899
Iteration 164/1000 | Loss: 0.00001899
Iteration 165/1000 | Loss: 0.00001899
Iteration 166/1000 | Loss: 0.00001899
Iteration 167/1000 | Loss: 0.00001899
Iteration 168/1000 | Loss: 0.00001899
Iteration 169/1000 | Loss: 0.00001899
Iteration 170/1000 | Loss: 0.00001899
Iteration 171/1000 | Loss: 0.00001899
Iteration 172/1000 | Loss: 0.00001899
Iteration 173/1000 | Loss: 0.00001899
Iteration 174/1000 | Loss: 0.00001899
Iteration 175/1000 | Loss: 0.00001899
Iteration 176/1000 | Loss: 0.00001899
Iteration 177/1000 | Loss: 0.00001899
Iteration 178/1000 | Loss: 0.00001899
Iteration 179/1000 | Loss: 0.00001899
Iteration 180/1000 | Loss: 0.00001899
Iteration 181/1000 | Loss: 0.00001899
Iteration 182/1000 | Loss: 0.00001899
Iteration 183/1000 | Loss: 0.00001899
Iteration 184/1000 | Loss: 0.00001899
Iteration 185/1000 | Loss: 0.00001899
Iteration 186/1000 | Loss: 0.00001899
Iteration 187/1000 | Loss: 0.00001899
Iteration 188/1000 | Loss: 0.00001899
Iteration 189/1000 | Loss: 0.00001899
Iteration 190/1000 | Loss: 0.00001899
Iteration 191/1000 | Loss: 0.00001899
Iteration 192/1000 | Loss: 0.00001899
Iteration 193/1000 | Loss: 0.00001899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.8989199816132896e-05, 1.8989199816132896e-05, 1.8989199816132896e-05, 1.8989199816132896e-05, 1.8989199816132896e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8989199816132896e-05

Optimization complete. Final v2v error: 3.4027981758117676 mm

Highest mean error: 10.281219482421875 mm for frame 94

Lowest mean error: 2.7369344234466553 mm for frame 126

Saving results

Total time: 168.92806911468506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499080
Iteration 2/25 | Loss: 0.00149084
Iteration 3/25 | Loss: 0.00126073
Iteration 4/25 | Loss: 0.00123980
Iteration 5/25 | Loss: 0.00123697
Iteration 6/25 | Loss: 0.00123619
Iteration 7/25 | Loss: 0.00123619
Iteration 8/25 | Loss: 0.00123619
Iteration 9/25 | Loss: 0.00123619
Iteration 10/25 | Loss: 0.00123619
Iteration 11/25 | Loss: 0.00123619
Iteration 12/25 | Loss: 0.00123619
Iteration 13/25 | Loss: 0.00123619
Iteration 14/25 | Loss: 0.00123619
Iteration 15/25 | Loss: 0.00123619
Iteration 16/25 | Loss: 0.00123619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012361917179077864, 0.0012361917179077864, 0.0012361917179077864, 0.0012361917179077864, 0.0012361917179077864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012361917179077864

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37740290
Iteration 2/25 | Loss: 0.00090310
Iteration 3/25 | Loss: 0.00090310
Iteration 4/25 | Loss: 0.00090310
Iteration 5/25 | Loss: 0.00090310
Iteration 6/25 | Loss: 0.00090310
Iteration 7/25 | Loss: 0.00090310
Iteration 8/25 | Loss: 0.00090310
Iteration 9/25 | Loss: 0.00090310
Iteration 10/25 | Loss: 0.00090310
Iteration 11/25 | Loss: 0.00090310
Iteration 12/25 | Loss: 0.00090309
Iteration 13/25 | Loss: 0.00090309
Iteration 14/25 | Loss: 0.00090309
Iteration 15/25 | Loss: 0.00090309
Iteration 16/25 | Loss: 0.00090309
Iteration 17/25 | Loss: 0.00090309
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009030949440784752, 0.0009030949440784752, 0.0009030949440784752, 0.0009030949440784752, 0.0009030949440784752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009030949440784752

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090309
Iteration 2/1000 | Loss: 0.00003439
Iteration 3/1000 | Loss: 0.00002386
Iteration 4/1000 | Loss: 0.00002067
Iteration 5/1000 | Loss: 0.00001911
Iteration 6/1000 | Loss: 0.00001826
Iteration 7/1000 | Loss: 0.00001777
Iteration 8/1000 | Loss: 0.00001719
Iteration 9/1000 | Loss: 0.00001683
Iteration 10/1000 | Loss: 0.00001659
Iteration 11/1000 | Loss: 0.00001639
Iteration 12/1000 | Loss: 0.00001628
Iteration 13/1000 | Loss: 0.00001624
Iteration 14/1000 | Loss: 0.00001624
Iteration 15/1000 | Loss: 0.00001618
Iteration 16/1000 | Loss: 0.00001618
Iteration 17/1000 | Loss: 0.00001616
Iteration 18/1000 | Loss: 0.00001612
Iteration 19/1000 | Loss: 0.00001612
Iteration 20/1000 | Loss: 0.00001606
Iteration 21/1000 | Loss: 0.00001606
Iteration 22/1000 | Loss: 0.00001606
Iteration 23/1000 | Loss: 0.00001605
Iteration 24/1000 | Loss: 0.00001604
Iteration 25/1000 | Loss: 0.00001603
Iteration 26/1000 | Loss: 0.00001602
Iteration 27/1000 | Loss: 0.00001602
Iteration 28/1000 | Loss: 0.00001601
Iteration 29/1000 | Loss: 0.00001601
Iteration 30/1000 | Loss: 0.00001596
Iteration 31/1000 | Loss: 0.00001596
Iteration 32/1000 | Loss: 0.00001595
Iteration 33/1000 | Loss: 0.00001594
Iteration 34/1000 | Loss: 0.00001593
Iteration 35/1000 | Loss: 0.00001593
Iteration 36/1000 | Loss: 0.00001593
Iteration 37/1000 | Loss: 0.00001593
Iteration 38/1000 | Loss: 0.00001593
Iteration 39/1000 | Loss: 0.00001593
Iteration 40/1000 | Loss: 0.00001593
Iteration 41/1000 | Loss: 0.00001592
Iteration 42/1000 | Loss: 0.00001592
Iteration 43/1000 | Loss: 0.00001591
Iteration 44/1000 | Loss: 0.00001591
Iteration 45/1000 | Loss: 0.00001590
Iteration 46/1000 | Loss: 0.00001590
Iteration 47/1000 | Loss: 0.00001589
Iteration 48/1000 | Loss: 0.00001589
Iteration 49/1000 | Loss: 0.00001588
Iteration 50/1000 | Loss: 0.00001586
Iteration 51/1000 | Loss: 0.00001585
Iteration 52/1000 | Loss: 0.00001585
Iteration 53/1000 | Loss: 0.00001583
Iteration 54/1000 | Loss: 0.00001583
Iteration 55/1000 | Loss: 0.00001582
Iteration 56/1000 | Loss: 0.00001581
Iteration 57/1000 | Loss: 0.00001581
Iteration 58/1000 | Loss: 0.00001580
Iteration 59/1000 | Loss: 0.00001580
Iteration 60/1000 | Loss: 0.00001577
Iteration 61/1000 | Loss: 0.00001577
Iteration 62/1000 | Loss: 0.00001577
Iteration 63/1000 | Loss: 0.00001574
Iteration 64/1000 | Loss: 0.00001573
Iteration 65/1000 | Loss: 0.00001573
Iteration 66/1000 | Loss: 0.00001573
Iteration 67/1000 | Loss: 0.00001573
Iteration 68/1000 | Loss: 0.00001573
Iteration 69/1000 | Loss: 0.00001572
Iteration 70/1000 | Loss: 0.00001572
Iteration 71/1000 | Loss: 0.00001571
Iteration 72/1000 | Loss: 0.00001571
Iteration 73/1000 | Loss: 0.00001570
Iteration 74/1000 | Loss: 0.00001570
Iteration 75/1000 | Loss: 0.00001570
Iteration 76/1000 | Loss: 0.00001570
Iteration 77/1000 | Loss: 0.00001569
Iteration 78/1000 | Loss: 0.00001569
Iteration 79/1000 | Loss: 0.00001569
Iteration 80/1000 | Loss: 0.00001568
Iteration 81/1000 | Loss: 0.00001568
Iteration 82/1000 | Loss: 0.00001568
Iteration 83/1000 | Loss: 0.00001567
Iteration 84/1000 | Loss: 0.00001567
Iteration 85/1000 | Loss: 0.00001567
Iteration 86/1000 | Loss: 0.00001566
Iteration 87/1000 | Loss: 0.00001566
Iteration 88/1000 | Loss: 0.00001566
Iteration 89/1000 | Loss: 0.00001563
Iteration 90/1000 | Loss: 0.00001563
Iteration 91/1000 | Loss: 0.00001562
Iteration 92/1000 | Loss: 0.00001562
Iteration 93/1000 | Loss: 0.00001561
Iteration 94/1000 | Loss: 0.00001561
Iteration 95/1000 | Loss: 0.00001561
Iteration 96/1000 | Loss: 0.00001560
Iteration 97/1000 | Loss: 0.00001560
Iteration 98/1000 | Loss: 0.00001560
Iteration 99/1000 | Loss: 0.00001559
Iteration 100/1000 | Loss: 0.00001559
Iteration 101/1000 | Loss: 0.00001559
Iteration 102/1000 | Loss: 0.00001559
Iteration 103/1000 | Loss: 0.00001559
Iteration 104/1000 | Loss: 0.00001558
Iteration 105/1000 | Loss: 0.00001558
Iteration 106/1000 | Loss: 0.00001557
Iteration 107/1000 | Loss: 0.00001557
Iteration 108/1000 | Loss: 0.00001557
Iteration 109/1000 | Loss: 0.00001557
Iteration 110/1000 | Loss: 0.00001556
Iteration 111/1000 | Loss: 0.00001556
Iteration 112/1000 | Loss: 0.00001556
Iteration 113/1000 | Loss: 0.00001556
Iteration 114/1000 | Loss: 0.00001556
Iteration 115/1000 | Loss: 0.00001555
Iteration 116/1000 | Loss: 0.00001555
Iteration 117/1000 | Loss: 0.00001555
Iteration 118/1000 | Loss: 0.00001555
Iteration 119/1000 | Loss: 0.00001555
Iteration 120/1000 | Loss: 0.00001555
Iteration 121/1000 | Loss: 0.00001554
Iteration 122/1000 | Loss: 0.00001554
Iteration 123/1000 | Loss: 0.00001554
Iteration 124/1000 | Loss: 0.00001554
Iteration 125/1000 | Loss: 0.00001554
Iteration 126/1000 | Loss: 0.00001554
Iteration 127/1000 | Loss: 0.00001554
Iteration 128/1000 | Loss: 0.00001554
Iteration 129/1000 | Loss: 0.00001554
Iteration 130/1000 | Loss: 0.00001554
Iteration 131/1000 | Loss: 0.00001554
Iteration 132/1000 | Loss: 0.00001554
Iteration 133/1000 | Loss: 0.00001554
Iteration 134/1000 | Loss: 0.00001554
Iteration 135/1000 | Loss: 0.00001554
Iteration 136/1000 | Loss: 0.00001554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.553750735183712e-05, 1.553750735183712e-05, 1.553750735183712e-05, 1.553750735183712e-05, 1.553750735183712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.553750735183712e-05

Optimization complete. Final v2v error: 3.1738502979278564 mm

Highest mean error: 4.419787883758545 mm for frame 61

Lowest mean error: 2.7251288890838623 mm for frame 3

Saving results

Total time: 39.47847533226013
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780561
Iteration 2/25 | Loss: 0.00150248
Iteration 3/25 | Loss: 0.00133801
Iteration 4/25 | Loss: 0.00126467
Iteration 5/25 | Loss: 0.00126150
Iteration 6/25 | Loss: 0.00123862
Iteration 7/25 | Loss: 0.00123744
Iteration 8/25 | Loss: 0.00123717
Iteration 9/25 | Loss: 0.00123705
Iteration 10/25 | Loss: 0.00123698
Iteration 11/25 | Loss: 0.00123698
Iteration 12/25 | Loss: 0.00123698
Iteration 13/25 | Loss: 0.00123698
Iteration 14/25 | Loss: 0.00123697
Iteration 15/25 | Loss: 0.00123697
Iteration 16/25 | Loss: 0.00123697
Iteration 17/25 | Loss: 0.00123697
Iteration 18/25 | Loss: 0.00123697
Iteration 19/25 | Loss: 0.00123696
Iteration 20/25 | Loss: 0.00123696
Iteration 21/25 | Loss: 0.00123696
Iteration 22/25 | Loss: 0.00123695
Iteration 23/25 | Loss: 0.00123695
Iteration 24/25 | Loss: 0.00123695
Iteration 25/25 | Loss: 0.00123695

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.14889169
Iteration 2/25 | Loss: 0.00095055
Iteration 3/25 | Loss: 0.00091187
Iteration 4/25 | Loss: 0.00091186
Iteration 5/25 | Loss: 0.00091186
Iteration 6/25 | Loss: 0.00091186
Iteration 7/25 | Loss: 0.00091186
Iteration 8/25 | Loss: 0.00091186
Iteration 9/25 | Loss: 0.00091186
Iteration 10/25 | Loss: 0.00091186
Iteration 11/25 | Loss: 0.00091186
Iteration 12/25 | Loss: 0.00091186
Iteration 13/25 | Loss: 0.00091186
Iteration 14/25 | Loss: 0.00091186
Iteration 15/25 | Loss: 0.00091186
Iteration 16/25 | Loss: 0.00091186
Iteration 17/25 | Loss: 0.00091186
Iteration 18/25 | Loss: 0.00091186
Iteration 19/25 | Loss: 0.00091186
Iteration 20/25 | Loss: 0.00091186
Iteration 21/25 | Loss: 0.00091186
Iteration 22/25 | Loss: 0.00091186
Iteration 23/25 | Loss: 0.00091186
Iteration 24/25 | Loss: 0.00091186
Iteration 25/25 | Loss: 0.00091186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091186
Iteration 2/1000 | Loss: 0.00006694
Iteration 3/1000 | Loss: 0.00002169
Iteration 4/1000 | Loss: 0.00001956
Iteration 5/1000 | Loss: 0.00007305
Iteration 6/1000 | Loss: 0.00001832
Iteration 7/1000 | Loss: 0.00001771
Iteration 8/1000 | Loss: 0.00001737
Iteration 9/1000 | Loss: 0.00001710
Iteration 10/1000 | Loss: 0.00001687
Iteration 11/1000 | Loss: 0.00001677
Iteration 12/1000 | Loss: 0.00001671
Iteration 13/1000 | Loss: 0.00012493
Iteration 14/1000 | Loss: 0.00010804
Iteration 15/1000 | Loss: 0.00003156
Iteration 16/1000 | Loss: 0.00002252
Iteration 17/1000 | Loss: 0.00001672
Iteration 18/1000 | Loss: 0.00011838
Iteration 19/1000 | Loss: 0.00002273
Iteration 20/1000 | Loss: 0.00005658
Iteration 21/1000 | Loss: 0.00002601
Iteration 22/1000 | Loss: 0.00002821
Iteration 23/1000 | Loss: 0.00002840
Iteration 24/1000 | Loss: 0.00002724
Iteration 25/1000 | Loss: 0.00012725
Iteration 26/1000 | Loss: 0.00011329
Iteration 27/1000 | Loss: 0.00012751
Iteration 28/1000 | Loss: 0.00005318
Iteration 29/1000 | Loss: 0.00002173
Iteration 30/1000 | Loss: 0.00026933
Iteration 31/1000 | Loss: 0.00001844
Iteration 32/1000 | Loss: 0.00001744
Iteration 33/1000 | Loss: 0.00001689
Iteration 34/1000 | Loss: 0.00001663
Iteration 35/1000 | Loss: 0.00001645
Iteration 36/1000 | Loss: 0.00001622
Iteration 37/1000 | Loss: 0.00001622
Iteration 38/1000 | Loss: 0.00001613
Iteration 39/1000 | Loss: 0.00001608
Iteration 40/1000 | Loss: 0.00001602
Iteration 41/1000 | Loss: 0.00004001
Iteration 42/1000 | Loss: 0.00010202
Iteration 43/1000 | Loss: 0.00002592
Iteration 44/1000 | Loss: 0.00003558
Iteration 45/1000 | Loss: 0.00001641
Iteration 46/1000 | Loss: 0.00001599
Iteration 47/1000 | Loss: 0.00001576
Iteration 48/1000 | Loss: 0.00001570
Iteration 49/1000 | Loss: 0.00001570
Iteration 50/1000 | Loss: 0.00001570
Iteration 51/1000 | Loss: 0.00001569
Iteration 52/1000 | Loss: 0.00001567
Iteration 53/1000 | Loss: 0.00001562
Iteration 54/1000 | Loss: 0.00001562
Iteration 55/1000 | Loss: 0.00001559
Iteration 56/1000 | Loss: 0.00001558
Iteration 57/1000 | Loss: 0.00001557
Iteration 58/1000 | Loss: 0.00001555
Iteration 59/1000 | Loss: 0.00001538
Iteration 60/1000 | Loss: 0.00001536
Iteration 61/1000 | Loss: 0.00001534
Iteration 62/1000 | Loss: 0.00001533
Iteration 63/1000 | Loss: 0.00001533
Iteration 64/1000 | Loss: 0.00001532
Iteration 65/1000 | Loss: 0.00001532
Iteration 66/1000 | Loss: 0.00001531
Iteration 67/1000 | Loss: 0.00001527
Iteration 68/1000 | Loss: 0.00001526
Iteration 69/1000 | Loss: 0.00001524
Iteration 70/1000 | Loss: 0.00001523
Iteration 71/1000 | Loss: 0.00001523
Iteration 72/1000 | Loss: 0.00001522
Iteration 73/1000 | Loss: 0.00001521
Iteration 74/1000 | Loss: 0.00001520
Iteration 75/1000 | Loss: 0.00001520
Iteration 76/1000 | Loss: 0.00001519
Iteration 77/1000 | Loss: 0.00001519
Iteration 78/1000 | Loss: 0.00001519
Iteration 79/1000 | Loss: 0.00001519
Iteration 80/1000 | Loss: 0.00001519
Iteration 81/1000 | Loss: 0.00001518
Iteration 82/1000 | Loss: 0.00001518
Iteration 83/1000 | Loss: 0.00001518
Iteration 84/1000 | Loss: 0.00001518
Iteration 85/1000 | Loss: 0.00001517
Iteration 86/1000 | Loss: 0.00001517
Iteration 87/1000 | Loss: 0.00001516
Iteration 88/1000 | Loss: 0.00001516
Iteration 89/1000 | Loss: 0.00001516
Iteration 90/1000 | Loss: 0.00001515
Iteration 91/1000 | Loss: 0.00001515
Iteration 92/1000 | Loss: 0.00001515
Iteration 93/1000 | Loss: 0.00001515
Iteration 94/1000 | Loss: 0.00001514
Iteration 95/1000 | Loss: 0.00001514
Iteration 96/1000 | Loss: 0.00001514
Iteration 97/1000 | Loss: 0.00001514
Iteration 98/1000 | Loss: 0.00001513
Iteration 99/1000 | Loss: 0.00001513
Iteration 100/1000 | Loss: 0.00001513
Iteration 101/1000 | Loss: 0.00001513
Iteration 102/1000 | Loss: 0.00001513
Iteration 103/1000 | Loss: 0.00001513
Iteration 104/1000 | Loss: 0.00001512
Iteration 105/1000 | Loss: 0.00001512
Iteration 106/1000 | Loss: 0.00001512
Iteration 107/1000 | Loss: 0.00001512
Iteration 108/1000 | Loss: 0.00001512
Iteration 109/1000 | Loss: 0.00001512
Iteration 110/1000 | Loss: 0.00001512
Iteration 111/1000 | Loss: 0.00001512
Iteration 112/1000 | Loss: 0.00001512
Iteration 113/1000 | Loss: 0.00001512
Iteration 114/1000 | Loss: 0.00001512
Iteration 115/1000 | Loss: 0.00001512
Iteration 116/1000 | Loss: 0.00001512
Iteration 117/1000 | Loss: 0.00001512
Iteration 118/1000 | Loss: 0.00001512
Iteration 119/1000 | Loss: 0.00001512
Iteration 120/1000 | Loss: 0.00001512
Iteration 121/1000 | Loss: 0.00001512
Iteration 122/1000 | Loss: 0.00001512
Iteration 123/1000 | Loss: 0.00001512
Iteration 124/1000 | Loss: 0.00001512
Iteration 125/1000 | Loss: 0.00001512
Iteration 126/1000 | Loss: 0.00001512
Iteration 127/1000 | Loss: 0.00001511
Iteration 128/1000 | Loss: 0.00001511
Iteration 129/1000 | Loss: 0.00001511
Iteration 130/1000 | Loss: 0.00001511
Iteration 131/1000 | Loss: 0.00001511
Iteration 132/1000 | Loss: 0.00001511
Iteration 133/1000 | Loss: 0.00001511
Iteration 134/1000 | Loss: 0.00001511
Iteration 135/1000 | Loss: 0.00001511
Iteration 136/1000 | Loss: 0.00001511
Iteration 137/1000 | Loss: 0.00001511
Iteration 138/1000 | Loss: 0.00001511
Iteration 139/1000 | Loss: 0.00001511
Iteration 140/1000 | Loss: 0.00001511
Iteration 141/1000 | Loss: 0.00001511
Iteration 142/1000 | Loss: 0.00001511
Iteration 143/1000 | Loss: 0.00001511
Iteration 144/1000 | Loss: 0.00001511
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.5114801499294117e-05, 1.5114801499294117e-05, 1.5114801499294117e-05, 1.5114801499294117e-05, 1.5114801499294117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5114801499294117e-05

Optimization complete. Final v2v error: 3.284601926803589 mm

Highest mean error: 4.741082668304443 mm for frame 157

Lowest mean error: 2.918830156326294 mm for frame 75

Saving results

Total time: 102.9359130859375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797056
Iteration 2/25 | Loss: 0.00156030
Iteration 3/25 | Loss: 0.00138241
Iteration 4/25 | Loss: 0.00136956
Iteration 5/25 | Loss: 0.00136756
Iteration 6/25 | Loss: 0.00136756
Iteration 7/25 | Loss: 0.00136756
Iteration 8/25 | Loss: 0.00136756
Iteration 9/25 | Loss: 0.00136756
Iteration 10/25 | Loss: 0.00136756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013675626832991838, 0.0013675626832991838, 0.0013675626832991838, 0.0013675626832991838, 0.0013675626832991838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013675626832991838

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23170471
Iteration 2/25 | Loss: 0.00080407
Iteration 3/25 | Loss: 0.00080407
Iteration 4/25 | Loss: 0.00080407
Iteration 5/25 | Loss: 0.00080407
Iteration 6/25 | Loss: 0.00080407
Iteration 7/25 | Loss: 0.00080407
Iteration 8/25 | Loss: 0.00080407
Iteration 9/25 | Loss: 0.00080407
Iteration 10/25 | Loss: 0.00080407
Iteration 11/25 | Loss: 0.00080407
Iteration 12/25 | Loss: 0.00080407
Iteration 13/25 | Loss: 0.00080407
Iteration 14/25 | Loss: 0.00080407
Iteration 15/25 | Loss: 0.00080407
Iteration 16/25 | Loss: 0.00080407
Iteration 17/25 | Loss: 0.00080407
Iteration 18/25 | Loss: 0.00080407
Iteration 19/25 | Loss: 0.00080407
Iteration 20/25 | Loss: 0.00080407
Iteration 21/25 | Loss: 0.00080407
Iteration 22/25 | Loss: 0.00080407
Iteration 23/25 | Loss: 0.00080407
Iteration 24/25 | Loss: 0.00080407
Iteration 25/25 | Loss: 0.00080407
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000804065668489784, 0.000804065668489784, 0.000804065668489784, 0.000804065668489784, 0.000804065668489784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000804065668489784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080407
Iteration 2/1000 | Loss: 0.00003588
Iteration 3/1000 | Loss: 0.00002760
Iteration 4/1000 | Loss: 0.00002526
Iteration 5/1000 | Loss: 0.00002410
Iteration 6/1000 | Loss: 0.00002341
Iteration 7/1000 | Loss: 0.00002271
Iteration 8/1000 | Loss: 0.00002218
Iteration 9/1000 | Loss: 0.00002185
Iteration 10/1000 | Loss: 0.00002169
Iteration 11/1000 | Loss: 0.00002153
Iteration 12/1000 | Loss: 0.00002146
Iteration 13/1000 | Loss: 0.00002136
Iteration 14/1000 | Loss: 0.00002134
Iteration 15/1000 | Loss: 0.00002133
Iteration 16/1000 | Loss: 0.00002133
Iteration 17/1000 | Loss: 0.00002133
Iteration 18/1000 | Loss: 0.00002132
Iteration 19/1000 | Loss: 0.00002131
Iteration 20/1000 | Loss: 0.00002130
Iteration 21/1000 | Loss: 0.00002130
Iteration 22/1000 | Loss: 0.00002129
Iteration 23/1000 | Loss: 0.00002129
Iteration 24/1000 | Loss: 0.00002129
Iteration 25/1000 | Loss: 0.00002128
Iteration 26/1000 | Loss: 0.00002128
Iteration 27/1000 | Loss: 0.00002128
Iteration 28/1000 | Loss: 0.00002127
Iteration 29/1000 | Loss: 0.00002127
Iteration 30/1000 | Loss: 0.00002127
Iteration 31/1000 | Loss: 0.00002126
Iteration 32/1000 | Loss: 0.00002126
Iteration 33/1000 | Loss: 0.00002126
Iteration 34/1000 | Loss: 0.00002126
Iteration 35/1000 | Loss: 0.00002125
Iteration 36/1000 | Loss: 0.00002125
Iteration 37/1000 | Loss: 0.00002125
Iteration 38/1000 | Loss: 0.00002124
Iteration 39/1000 | Loss: 0.00002124
Iteration 40/1000 | Loss: 0.00002124
Iteration 41/1000 | Loss: 0.00002124
Iteration 42/1000 | Loss: 0.00002123
Iteration 43/1000 | Loss: 0.00002123
Iteration 44/1000 | Loss: 0.00002123
Iteration 45/1000 | Loss: 0.00002123
Iteration 46/1000 | Loss: 0.00002123
Iteration 47/1000 | Loss: 0.00002123
Iteration 48/1000 | Loss: 0.00002123
Iteration 49/1000 | Loss: 0.00002123
Iteration 50/1000 | Loss: 0.00002123
Iteration 51/1000 | Loss: 0.00002122
Iteration 52/1000 | Loss: 0.00002122
Iteration 53/1000 | Loss: 0.00002122
Iteration 54/1000 | Loss: 0.00002122
Iteration 55/1000 | Loss: 0.00002122
Iteration 56/1000 | Loss: 0.00002122
Iteration 57/1000 | Loss: 0.00002121
Iteration 58/1000 | Loss: 0.00002121
Iteration 59/1000 | Loss: 0.00002121
Iteration 60/1000 | Loss: 0.00002121
Iteration 61/1000 | Loss: 0.00002121
Iteration 62/1000 | Loss: 0.00002120
Iteration 63/1000 | Loss: 0.00002120
Iteration 64/1000 | Loss: 0.00002120
Iteration 65/1000 | Loss: 0.00002120
Iteration 66/1000 | Loss: 0.00002120
Iteration 67/1000 | Loss: 0.00002120
Iteration 68/1000 | Loss: 0.00002120
Iteration 69/1000 | Loss: 0.00002120
Iteration 70/1000 | Loss: 0.00002119
Iteration 71/1000 | Loss: 0.00002119
Iteration 72/1000 | Loss: 0.00002119
Iteration 73/1000 | Loss: 0.00002119
Iteration 74/1000 | Loss: 0.00002118
Iteration 75/1000 | Loss: 0.00002118
Iteration 76/1000 | Loss: 0.00002118
Iteration 77/1000 | Loss: 0.00002118
Iteration 78/1000 | Loss: 0.00002118
Iteration 79/1000 | Loss: 0.00002118
Iteration 80/1000 | Loss: 0.00002118
Iteration 81/1000 | Loss: 0.00002118
Iteration 82/1000 | Loss: 0.00002118
Iteration 83/1000 | Loss: 0.00002118
Iteration 84/1000 | Loss: 0.00002118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [2.1179468603804708e-05, 2.1179468603804708e-05, 2.1179468603804708e-05, 2.1179468603804708e-05, 2.1179468603804708e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1179468603804708e-05

Optimization complete. Final v2v error: 3.8382511138916016 mm

Highest mean error: 4.154779434204102 mm for frame 10

Lowest mean error: 3.61151385307312 mm for frame 214

Saving results

Total time: 32.127578020095825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820322
Iteration 2/25 | Loss: 0.00218672
Iteration 3/25 | Loss: 0.00155691
Iteration 4/25 | Loss: 0.00145756
Iteration 5/25 | Loss: 0.00143908
Iteration 6/25 | Loss: 0.00143889
Iteration 7/25 | Loss: 0.00143302
Iteration 8/25 | Loss: 0.00143061
Iteration 9/25 | Loss: 0.00142814
Iteration 10/25 | Loss: 0.00142647
Iteration 11/25 | Loss: 0.00142581
Iteration 12/25 | Loss: 0.00142525
Iteration 13/25 | Loss: 0.00142440
Iteration 14/25 | Loss: 0.00142472
Iteration 15/25 | Loss: 0.00142508
Iteration 16/25 | Loss: 0.00142445
Iteration 17/25 | Loss: 0.00142469
Iteration 18/25 | Loss: 0.00142419
Iteration 19/25 | Loss: 0.00142426
Iteration 20/25 | Loss: 0.00142451
Iteration 21/25 | Loss: 0.00142460
Iteration 22/25 | Loss: 0.00142435
Iteration 23/25 | Loss: 0.00142440
Iteration 24/25 | Loss: 0.00142420
Iteration 25/25 | Loss: 0.00142395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20182014
Iteration 2/25 | Loss: 0.00096640
Iteration 3/25 | Loss: 0.00096637
Iteration 4/25 | Loss: 0.00096637
Iteration 5/25 | Loss: 0.00096637
Iteration 6/25 | Loss: 0.00096636
Iteration 7/25 | Loss: 0.00096636
Iteration 8/25 | Loss: 0.00096636
Iteration 9/25 | Loss: 0.00096636
Iteration 10/25 | Loss: 0.00096636
Iteration 11/25 | Loss: 0.00096636
Iteration 12/25 | Loss: 0.00096636
Iteration 13/25 | Loss: 0.00096636
Iteration 14/25 | Loss: 0.00096636
Iteration 15/25 | Loss: 0.00096636
Iteration 16/25 | Loss: 0.00096636
Iteration 17/25 | Loss: 0.00096636
Iteration 18/25 | Loss: 0.00096636
Iteration 19/25 | Loss: 0.00096636
Iteration 20/25 | Loss: 0.00096636
Iteration 21/25 | Loss: 0.00096636
Iteration 22/25 | Loss: 0.00096636
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009663641103543341, 0.0009663641103543341, 0.0009663641103543341, 0.0009663641103543341, 0.0009663641103543341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009663641103543341

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096636
Iteration 2/1000 | Loss: 0.00005839
Iteration 3/1000 | Loss: 0.00004133
Iteration 4/1000 | Loss: 0.00003722
Iteration 5/1000 | Loss: 0.00003209
Iteration 6/1000 | Loss: 0.00003116
Iteration 7/1000 | Loss: 0.00004637
Iteration 8/1000 | Loss: 0.00003245
Iteration 9/1000 | Loss: 0.00004412
Iteration 10/1000 | Loss: 0.00004598
Iteration 11/1000 | Loss: 0.00004165
Iteration 12/1000 | Loss: 0.00004340
Iteration 13/1000 | Loss: 0.00004044
Iteration 14/1000 | Loss: 0.00004558
Iteration 15/1000 | Loss: 0.00003347
Iteration 16/1000 | Loss: 0.00003902
Iteration 17/1000 | Loss: 0.00003673
Iteration 18/1000 | Loss: 0.00003248
Iteration 19/1000 | Loss: 0.00003134
Iteration 20/1000 | Loss: 0.00004120
Iteration 21/1000 | Loss: 0.00003762
Iteration 22/1000 | Loss: 0.00003745
Iteration 23/1000 | Loss: 0.00004797
Iteration 24/1000 | Loss: 0.00003419
Iteration 25/1000 | Loss: 0.00004330
Iteration 26/1000 | Loss: 0.00004541
Iteration 27/1000 | Loss: 0.00003436
Iteration 28/1000 | Loss: 0.00004492
Iteration 29/1000 | Loss: 0.00003855
Iteration 30/1000 | Loss: 0.00004396
Iteration 31/1000 | Loss: 0.00003750
Iteration 32/1000 | Loss: 0.00003206
Iteration 33/1000 | Loss: 0.00004516
Iteration 34/1000 | Loss: 0.00003923
Iteration 35/1000 | Loss: 0.00004228
Iteration 36/1000 | Loss: 0.00003860
Iteration 37/1000 | Loss: 0.00003238
Iteration 38/1000 | Loss: 0.00002941
Iteration 39/1000 | Loss: 0.00002838
Iteration 40/1000 | Loss: 0.00002801
Iteration 41/1000 | Loss: 0.00002779
Iteration 42/1000 | Loss: 0.00002778
Iteration 43/1000 | Loss: 0.00002763
Iteration 44/1000 | Loss: 0.00002761
Iteration 45/1000 | Loss: 0.00002755
Iteration 46/1000 | Loss: 0.00002749
Iteration 47/1000 | Loss: 0.00002745
Iteration 48/1000 | Loss: 0.00002741
Iteration 49/1000 | Loss: 0.00002741
Iteration 50/1000 | Loss: 0.00002740
Iteration 51/1000 | Loss: 0.00002739
Iteration 52/1000 | Loss: 0.00002738
Iteration 53/1000 | Loss: 0.00002738
Iteration 54/1000 | Loss: 0.00002737
Iteration 55/1000 | Loss: 0.00002737
Iteration 56/1000 | Loss: 0.00002737
Iteration 57/1000 | Loss: 0.00002735
Iteration 58/1000 | Loss: 0.00002735
Iteration 59/1000 | Loss: 0.00002734
Iteration 60/1000 | Loss: 0.00002734
Iteration 61/1000 | Loss: 0.00002734
Iteration 62/1000 | Loss: 0.00002734
Iteration 63/1000 | Loss: 0.00002734
Iteration 64/1000 | Loss: 0.00002734
Iteration 65/1000 | Loss: 0.00002734
Iteration 66/1000 | Loss: 0.00002734
Iteration 67/1000 | Loss: 0.00002733
Iteration 68/1000 | Loss: 0.00002733
Iteration 69/1000 | Loss: 0.00002733
Iteration 70/1000 | Loss: 0.00002733
Iteration 71/1000 | Loss: 0.00002733
Iteration 72/1000 | Loss: 0.00002733
Iteration 73/1000 | Loss: 0.00002733
Iteration 74/1000 | Loss: 0.00002733
Iteration 75/1000 | Loss: 0.00002733
Iteration 76/1000 | Loss: 0.00002733
Iteration 77/1000 | Loss: 0.00002732
Iteration 78/1000 | Loss: 0.00002732
Iteration 79/1000 | Loss: 0.00002732
Iteration 80/1000 | Loss: 0.00002732
Iteration 81/1000 | Loss: 0.00002732
Iteration 82/1000 | Loss: 0.00002732
Iteration 83/1000 | Loss: 0.00002732
Iteration 84/1000 | Loss: 0.00002732
Iteration 85/1000 | Loss: 0.00002731
Iteration 86/1000 | Loss: 0.00002731
Iteration 87/1000 | Loss: 0.00002731
Iteration 88/1000 | Loss: 0.00002731
Iteration 89/1000 | Loss: 0.00002730
Iteration 90/1000 | Loss: 0.00002730
Iteration 91/1000 | Loss: 0.00002730
Iteration 92/1000 | Loss: 0.00002730
Iteration 93/1000 | Loss: 0.00002729
Iteration 94/1000 | Loss: 0.00002729
Iteration 95/1000 | Loss: 0.00002729
Iteration 96/1000 | Loss: 0.00002729
Iteration 97/1000 | Loss: 0.00002729
Iteration 98/1000 | Loss: 0.00002729
Iteration 99/1000 | Loss: 0.00002728
Iteration 100/1000 | Loss: 0.00002728
Iteration 101/1000 | Loss: 0.00002728
Iteration 102/1000 | Loss: 0.00002728
Iteration 103/1000 | Loss: 0.00002728
Iteration 104/1000 | Loss: 0.00002727
Iteration 105/1000 | Loss: 0.00002727
Iteration 106/1000 | Loss: 0.00002727
Iteration 107/1000 | Loss: 0.00002727
Iteration 108/1000 | Loss: 0.00002727
Iteration 109/1000 | Loss: 0.00002727
Iteration 110/1000 | Loss: 0.00002727
Iteration 111/1000 | Loss: 0.00002727
Iteration 112/1000 | Loss: 0.00002726
Iteration 113/1000 | Loss: 0.00002726
Iteration 114/1000 | Loss: 0.00002726
Iteration 115/1000 | Loss: 0.00002726
Iteration 116/1000 | Loss: 0.00002726
Iteration 117/1000 | Loss: 0.00002726
Iteration 118/1000 | Loss: 0.00002726
Iteration 119/1000 | Loss: 0.00002726
Iteration 120/1000 | Loss: 0.00002726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.7264200980425812e-05, 2.7264200980425812e-05, 2.7264200980425812e-05, 2.7264200980425812e-05, 2.7264200980425812e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7264200980425812e-05

Optimization complete. Final v2v error: 4.256314277648926 mm

Highest mean error: 5.38374662399292 mm for frame 155

Lowest mean error: 4.11061429977417 mm for frame 30

Saving results

Total time: 126.31760382652283
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804551
Iteration 2/25 | Loss: 0.00144218
Iteration 3/25 | Loss: 0.00128213
Iteration 4/25 | Loss: 0.00127496
Iteration 5/25 | Loss: 0.00127458
Iteration 6/25 | Loss: 0.00127458
Iteration 7/25 | Loss: 0.00127458
Iteration 8/25 | Loss: 0.00127458
Iteration 9/25 | Loss: 0.00127458
Iteration 10/25 | Loss: 0.00127458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001274580485187471, 0.001274580485187471, 0.001274580485187471, 0.001274580485187471, 0.001274580485187471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001274580485187471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26225805
Iteration 2/25 | Loss: 0.00080614
Iteration 3/25 | Loss: 0.00080613
Iteration 4/25 | Loss: 0.00080613
Iteration 5/25 | Loss: 0.00080613
Iteration 6/25 | Loss: 0.00080613
Iteration 7/25 | Loss: 0.00080613
Iteration 8/25 | Loss: 0.00080612
Iteration 9/25 | Loss: 0.00080612
Iteration 10/25 | Loss: 0.00080612
Iteration 11/25 | Loss: 0.00080612
Iteration 12/25 | Loss: 0.00080612
Iteration 13/25 | Loss: 0.00080612
Iteration 14/25 | Loss: 0.00080612
Iteration 15/25 | Loss: 0.00080612
Iteration 16/25 | Loss: 0.00080612
Iteration 17/25 | Loss: 0.00080612
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008061243570409715, 0.0008061243570409715, 0.0008061243570409715, 0.0008061243570409715, 0.0008061243570409715]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008061243570409715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080612
Iteration 2/1000 | Loss: 0.00002938
Iteration 3/1000 | Loss: 0.00002308
Iteration 4/1000 | Loss: 0.00002156
Iteration 5/1000 | Loss: 0.00002066
Iteration 6/1000 | Loss: 0.00001998
Iteration 7/1000 | Loss: 0.00001947
Iteration 8/1000 | Loss: 0.00001894
Iteration 9/1000 | Loss: 0.00001868
Iteration 10/1000 | Loss: 0.00001858
Iteration 11/1000 | Loss: 0.00001857
Iteration 12/1000 | Loss: 0.00001839
Iteration 13/1000 | Loss: 0.00001818
Iteration 14/1000 | Loss: 0.00001802
Iteration 15/1000 | Loss: 0.00001795
Iteration 16/1000 | Loss: 0.00001794
Iteration 17/1000 | Loss: 0.00001788
Iteration 18/1000 | Loss: 0.00001787
Iteration 19/1000 | Loss: 0.00001786
Iteration 20/1000 | Loss: 0.00001786
Iteration 21/1000 | Loss: 0.00001785
Iteration 22/1000 | Loss: 0.00001783
Iteration 23/1000 | Loss: 0.00001783
Iteration 24/1000 | Loss: 0.00001782
Iteration 25/1000 | Loss: 0.00001782
Iteration 26/1000 | Loss: 0.00001782
Iteration 27/1000 | Loss: 0.00001782
Iteration 28/1000 | Loss: 0.00001782
Iteration 29/1000 | Loss: 0.00001782
Iteration 30/1000 | Loss: 0.00001768
Iteration 31/1000 | Loss: 0.00001768
Iteration 32/1000 | Loss: 0.00001768
Iteration 33/1000 | Loss: 0.00001768
Iteration 34/1000 | Loss: 0.00001768
Iteration 35/1000 | Loss: 0.00001768
Iteration 36/1000 | Loss: 0.00001767
Iteration 37/1000 | Loss: 0.00001767
Iteration 38/1000 | Loss: 0.00001767
Iteration 39/1000 | Loss: 0.00001762
Iteration 40/1000 | Loss: 0.00001762
Iteration 41/1000 | Loss: 0.00001761
Iteration 42/1000 | Loss: 0.00001761
Iteration 43/1000 | Loss: 0.00001761
Iteration 44/1000 | Loss: 0.00001760
Iteration 45/1000 | Loss: 0.00001759
Iteration 46/1000 | Loss: 0.00001759
Iteration 47/1000 | Loss: 0.00001759
Iteration 48/1000 | Loss: 0.00001753
Iteration 49/1000 | Loss: 0.00001753
Iteration 50/1000 | Loss: 0.00001752
Iteration 51/1000 | Loss: 0.00001752
Iteration 52/1000 | Loss: 0.00001751
Iteration 53/1000 | Loss: 0.00001751
Iteration 54/1000 | Loss: 0.00001750
Iteration 55/1000 | Loss: 0.00001750
Iteration 56/1000 | Loss: 0.00001750
Iteration 57/1000 | Loss: 0.00001750
Iteration 58/1000 | Loss: 0.00001750
Iteration 59/1000 | Loss: 0.00001750
Iteration 60/1000 | Loss: 0.00001750
Iteration 61/1000 | Loss: 0.00001749
Iteration 62/1000 | Loss: 0.00001749
Iteration 63/1000 | Loss: 0.00001749
Iteration 64/1000 | Loss: 0.00001749
Iteration 65/1000 | Loss: 0.00001747
Iteration 66/1000 | Loss: 0.00001746
Iteration 67/1000 | Loss: 0.00001746
Iteration 68/1000 | Loss: 0.00001746
Iteration 69/1000 | Loss: 0.00001746
Iteration 70/1000 | Loss: 0.00001745
Iteration 71/1000 | Loss: 0.00001745
Iteration 72/1000 | Loss: 0.00001745
Iteration 73/1000 | Loss: 0.00001745
Iteration 74/1000 | Loss: 0.00001744
Iteration 75/1000 | Loss: 0.00001744
Iteration 76/1000 | Loss: 0.00001744
Iteration 77/1000 | Loss: 0.00001744
Iteration 78/1000 | Loss: 0.00001744
Iteration 79/1000 | Loss: 0.00001744
Iteration 80/1000 | Loss: 0.00001744
Iteration 81/1000 | Loss: 0.00001744
Iteration 82/1000 | Loss: 0.00001744
Iteration 83/1000 | Loss: 0.00001744
Iteration 84/1000 | Loss: 0.00001744
Iteration 85/1000 | Loss: 0.00001743
Iteration 86/1000 | Loss: 0.00001743
Iteration 87/1000 | Loss: 0.00001743
Iteration 88/1000 | Loss: 0.00001743
Iteration 89/1000 | Loss: 0.00001743
Iteration 90/1000 | Loss: 0.00001743
Iteration 91/1000 | Loss: 0.00001743
Iteration 92/1000 | Loss: 0.00001743
Iteration 93/1000 | Loss: 0.00001743
Iteration 94/1000 | Loss: 0.00001743
Iteration 95/1000 | Loss: 0.00001743
Iteration 96/1000 | Loss: 0.00001743
Iteration 97/1000 | Loss: 0.00001743
Iteration 98/1000 | Loss: 0.00001743
Iteration 99/1000 | Loss: 0.00001743
Iteration 100/1000 | Loss: 0.00001743
Iteration 101/1000 | Loss: 0.00001743
Iteration 102/1000 | Loss: 0.00001743
Iteration 103/1000 | Loss: 0.00001743
Iteration 104/1000 | Loss: 0.00001742
Iteration 105/1000 | Loss: 0.00001742
Iteration 106/1000 | Loss: 0.00001742
Iteration 107/1000 | Loss: 0.00001742
Iteration 108/1000 | Loss: 0.00001741
Iteration 109/1000 | Loss: 0.00001741
Iteration 110/1000 | Loss: 0.00001741
Iteration 111/1000 | Loss: 0.00001741
Iteration 112/1000 | Loss: 0.00001741
Iteration 113/1000 | Loss: 0.00001741
Iteration 114/1000 | Loss: 0.00001741
Iteration 115/1000 | Loss: 0.00001741
Iteration 116/1000 | Loss: 0.00001741
Iteration 117/1000 | Loss: 0.00001741
Iteration 118/1000 | Loss: 0.00001741
Iteration 119/1000 | Loss: 0.00001741
Iteration 120/1000 | Loss: 0.00001741
Iteration 121/1000 | Loss: 0.00001740
Iteration 122/1000 | Loss: 0.00001740
Iteration 123/1000 | Loss: 0.00001740
Iteration 124/1000 | Loss: 0.00001740
Iteration 125/1000 | Loss: 0.00001740
Iteration 126/1000 | Loss: 0.00001740
Iteration 127/1000 | Loss: 0.00001740
Iteration 128/1000 | Loss: 0.00001740
Iteration 129/1000 | Loss: 0.00001740
Iteration 130/1000 | Loss: 0.00001740
Iteration 131/1000 | Loss: 0.00001739
Iteration 132/1000 | Loss: 0.00001739
Iteration 133/1000 | Loss: 0.00001739
Iteration 134/1000 | Loss: 0.00001739
Iteration 135/1000 | Loss: 0.00001739
Iteration 136/1000 | Loss: 0.00001739
Iteration 137/1000 | Loss: 0.00001739
Iteration 138/1000 | Loss: 0.00001739
Iteration 139/1000 | Loss: 0.00001739
Iteration 140/1000 | Loss: 0.00001739
Iteration 141/1000 | Loss: 0.00001739
Iteration 142/1000 | Loss: 0.00001739
Iteration 143/1000 | Loss: 0.00001739
Iteration 144/1000 | Loss: 0.00001739
Iteration 145/1000 | Loss: 0.00001739
Iteration 146/1000 | Loss: 0.00001739
Iteration 147/1000 | Loss: 0.00001739
Iteration 148/1000 | Loss: 0.00001739
Iteration 149/1000 | Loss: 0.00001739
Iteration 150/1000 | Loss: 0.00001739
Iteration 151/1000 | Loss: 0.00001739
Iteration 152/1000 | Loss: 0.00001739
Iteration 153/1000 | Loss: 0.00001739
Iteration 154/1000 | Loss: 0.00001739
Iteration 155/1000 | Loss: 0.00001739
Iteration 156/1000 | Loss: 0.00001739
Iteration 157/1000 | Loss: 0.00001739
Iteration 158/1000 | Loss: 0.00001739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.7387919797329232e-05, 1.7387919797329232e-05, 1.7387919797329232e-05, 1.7387919797329232e-05, 1.7387919797329232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7387919797329232e-05

Optimization complete. Final v2v error: 3.5308616161346436 mm

Highest mean error: 3.6741559505462646 mm for frame 212

Lowest mean error: 3.4021966457366943 mm for frame 62

Saving results

Total time: 41.44801115989685
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402086
Iteration 2/25 | Loss: 0.00138937
Iteration 3/25 | Loss: 0.00125152
Iteration 4/25 | Loss: 0.00122562
Iteration 5/25 | Loss: 0.00121570
Iteration 6/25 | Loss: 0.00121234
Iteration 7/25 | Loss: 0.00122793
Iteration 8/25 | Loss: 0.00120293
Iteration 9/25 | Loss: 0.00119543
Iteration 10/25 | Loss: 0.00119292
Iteration 11/25 | Loss: 0.00119270
Iteration 12/25 | Loss: 0.00119270
Iteration 13/25 | Loss: 0.00119270
Iteration 14/25 | Loss: 0.00119270
Iteration 15/25 | Loss: 0.00119270
Iteration 16/25 | Loss: 0.00119270
Iteration 17/25 | Loss: 0.00119270
Iteration 18/25 | Loss: 0.00119270
Iteration 19/25 | Loss: 0.00119270
Iteration 20/25 | Loss: 0.00119270
Iteration 21/25 | Loss: 0.00119270
Iteration 22/25 | Loss: 0.00119270
Iteration 23/25 | Loss: 0.00119270
Iteration 24/25 | Loss: 0.00119270
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001192704075947404, 0.001192704075947404, 0.001192704075947404, 0.001192704075947404, 0.001192704075947404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001192704075947404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34788990
Iteration 2/25 | Loss: 0.00084101
Iteration 3/25 | Loss: 0.00084101
Iteration 4/25 | Loss: 0.00084101
Iteration 5/25 | Loss: 0.00084101
Iteration 6/25 | Loss: 0.00084101
Iteration 7/25 | Loss: 0.00084101
Iteration 8/25 | Loss: 0.00084101
Iteration 9/25 | Loss: 0.00084101
Iteration 10/25 | Loss: 0.00084101
Iteration 11/25 | Loss: 0.00084101
Iteration 12/25 | Loss: 0.00084101
Iteration 13/25 | Loss: 0.00084101
Iteration 14/25 | Loss: 0.00084101
Iteration 15/25 | Loss: 0.00084101
Iteration 16/25 | Loss: 0.00084101
Iteration 17/25 | Loss: 0.00084101
Iteration 18/25 | Loss: 0.00084101
Iteration 19/25 | Loss: 0.00084101
Iteration 20/25 | Loss: 0.00084101
Iteration 21/25 | Loss: 0.00084101
Iteration 22/25 | Loss: 0.00084101
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008410058217123151, 0.0008410058217123151, 0.0008410058217123151, 0.0008410058217123151, 0.0008410058217123151]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008410058217123151

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084101
Iteration 2/1000 | Loss: 0.00002110
Iteration 3/1000 | Loss: 0.00001642
Iteration 4/1000 | Loss: 0.00001499
Iteration 5/1000 | Loss: 0.00001402
Iteration 6/1000 | Loss: 0.00001359
Iteration 7/1000 | Loss: 0.00001330
Iteration 8/1000 | Loss: 0.00001289
Iteration 9/1000 | Loss: 0.00001272
Iteration 10/1000 | Loss: 0.00001254
Iteration 11/1000 | Loss: 0.00001253
Iteration 12/1000 | Loss: 0.00001244
Iteration 13/1000 | Loss: 0.00001236
Iteration 14/1000 | Loss: 0.00001232
Iteration 15/1000 | Loss: 0.00001232
Iteration 16/1000 | Loss: 0.00001232
Iteration 17/1000 | Loss: 0.00001231
Iteration 18/1000 | Loss: 0.00001231
Iteration 19/1000 | Loss: 0.00001230
Iteration 20/1000 | Loss: 0.00001229
Iteration 21/1000 | Loss: 0.00001228
Iteration 22/1000 | Loss: 0.00001228
Iteration 23/1000 | Loss: 0.00001227
Iteration 24/1000 | Loss: 0.00001227
Iteration 25/1000 | Loss: 0.00001226
Iteration 26/1000 | Loss: 0.00001226
Iteration 27/1000 | Loss: 0.00001225
Iteration 28/1000 | Loss: 0.00001223
Iteration 29/1000 | Loss: 0.00001223
Iteration 30/1000 | Loss: 0.00001223
Iteration 31/1000 | Loss: 0.00001223
Iteration 32/1000 | Loss: 0.00001220
Iteration 33/1000 | Loss: 0.00001218
Iteration 34/1000 | Loss: 0.00001218
Iteration 35/1000 | Loss: 0.00001212
Iteration 36/1000 | Loss: 0.00001211
Iteration 37/1000 | Loss: 0.00001208
Iteration 38/1000 | Loss: 0.00001207
Iteration 39/1000 | Loss: 0.00001207
Iteration 40/1000 | Loss: 0.00001207
Iteration 41/1000 | Loss: 0.00001207
Iteration 42/1000 | Loss: 0.00001206
Iteration 43/1000 | Loss: 0.00001206
Iteration 44/1000 | Loss: 0.00001205
Iteration 45/1000 | Loss: 0.00001205
Iteration 46/1000 | Loss: 0.00001204
Iteration 47/1000 | Loss: 0.00001204
Iteration 48/1000 | Loss: 0.00001204
Iteration 49/1000 | Loss: 0.00001204
Iteration 50/1000 | Loss: 0.00001204
Iteration 51/1000 | Loss: 0.00001203
Iteration 52/1000 | Loss: 0.00001202
Iteration 53/1000 | Loss: 0.00001202
Iteration 54/1000 | Loss: 0.00001200
Iteration 55/1000 | Loss: 0.00001200
Iteration 56/1000 | Loss: 0.00001200
Iteration 57/1000 | Loss: 0.00001200
Iteration 58/1000 | Loss: 0.00001200
Iteration 59/1000 | Loss: 0.00001200
Iteration 60/1000 | Loss: 0.00001200
Iteration 61/1000 | Loss: 0.00001200
Iteration 62/1000 | Loss: 0.00001200
Iteration 63/1000 | Loss: 0.00001200
Iteration 64/1000 | Loss: 0.00001200
Iteration 65/1000 | Loss: 0.00001200
Iteration 66/1000 | Loss: 0.00001199
Iteration 67/1000 | Loss: 0.00001199
Iteration 68/1000 | Loss: 0.00001198
Iteration 69/1000 | Loss: 0.00001198
Iteration 70/1000 | Loss: 0.00001198
Iteration 71/1000 | Loss: 0.00001198
Iteration 72/1000 | Loss: 0.00001197
Iteration 73/1000 | Loss: 0.00001197
Iteration 74/1000 | Loss: 0.00001197
Iteration 75/1000 | Loss: 0.00001197
Iteration 76/1000 | Loss: 0.00001197
Iteration 77/1000 | Loss: 0.00001196
Iteration 78/1000 | Loss: 0.00001196
Iteration 79/1000 | Loss: 0.00001196
Iteration 80/1000 | Loss: 0.00001196
Iteration 81/1000 | Loss: 0.00001195
Iteration 82/1000 | Loss: 0.00001195
Iteration 83/1000 | Loss: 0.00001194
Iteration 84/1000 | Loss: 0.00001194
Iteration 85/1000 | Loss: 0.00001194
Iteration 86/1000 | Loss: 0.00001194
Iteration 87/1000 | Loss: 0.00001194
Iteration 88/1000 | Loss: 0.00001194
Iteration 89/1000 | Loss: 0.00001194
Iteration 90/1000 | Loss: 0.00001193
Iteration 91/1000 | Loss: 0.00001193
Iteration 92/1000 | Loss: 0.00001193
Iteration 93/1000 | Loss: 0.00001193
Iteration 94/1000 | Loss: 0.00001193
Iteration 95/1000 | Loss: 0.00001193
Iteration 96/1000 | Loss: 0.00001193
Iteration 97/1000 | Loss: 0.00001192
Iteration 98/1000 | Loss: 0.00001192
Iteration 99/1000 | Loss: 0.00001192
Iteration 100/1000 | Loss: 0.00001192
Iteration 101/1000 | Loss: 0.00001192
Iteration 102/1000 | Loss: 0.00001192
Iteration 103/1000 | Loss: 0.00001191
Iteration 104/1000 | Loss: 0.00001191
Iteration 105/1000 | Loss: 0.00001190
Iteration 106/1000 | Loss: 0.00001190
Iteration 107/1000 | Loss: 0.00001190
Iteration 108/1000 | Loss: 0.00001189
Iteration 109/1000 | Loss: 0.00001189
Iteration 110/1000 | Loss: 0.00001189
Iteration 111/1000 | Loss: 0.00001189
Iteration 112/1000 | Loss: 0.00001188
Iteration 113/1000 | Loss: 0.00001188
Iteration 114/1000 | Loss: 0.00001188
Iteration 115/1000 | Loss: 0.00001187
Iteration 116/1000 | Loss: 0.00001187
Iteration 117/1000 | Loss: 0.00001186
Iteration 118/1000 | Loss: 0.00001186
Iteration 119/1000 | Loss: 0.00001186
Iteration 120/1000 | Loss: 0.00001186
Iteration 121/1000 | Loss: 0.00001186
Iteration 122/1000 | Loss: 0.00001185
Iteration 123/1000 | Loss: 0.00001185
Iteration 124/1000 | Loss: 0.00001185
Iteration 125/1000 | Loss: 0.00001185
Iteration 126/1000 | Loss: 0.00001185
Iteration 127/1000 | Loss: 0.00001185
Iteration 128/1000 | Loss: 0.00001185
Iteration 129/1000 | Loss: 0.00001185
Iteration 130/1000 | Loss: 0.00001185
Iteration 131/1000 | Loss: 0.00001185
Iteration 132/1000 | Loss: 0.00001185
Iteration 133/1000 | Loss: 0.00001185
Iteration 134/1000 | Loss: 0.00001185
Iteration 135/1000 | Loss: 0.00001185
Iteration 136/1000 | Loss: 0.00001185
Iteration 137/1000 | Loss: 0.00001185
Iteration 138/1000 | Loss: 0.00001185
Iteration 139/1000 | Loss: 0.00001185
Iteration 140/1000 | Loss: 0.00001185
Iteration 141/1000 | Loss: 0.00001185
Iteration 142/1000 | Loss: 0.00001185
Iteration 143/1000 | Loss: 0.00001185
Iteration 144/1000 | Loss: 0.00001185
Iteration 145/1000 | Loss: 0.00001185
Iteration 146/1000 | Loss: 0.00001185
Iteration 147/1000 | Loss: 0.00001185
Iteration 148/1000 | Loss: 0.00001185
Iteration 149/1000 | Loss: 0.00001185
Iteration 150/1000 | Loss: 0.00001185
Iteration 151/1000 | Loss: 0.00001185
Iteration 152/1000 | Loss: 0.00001185
Iteration 153/1000 | Loss: 0.00001185
Iteration 154/1000 | Loss: 0.00001185
Iteration 155/1000 | Loss: 0.00001185
Iteration 156/1000 | Loss: 0.00001185
Iteration 157/1000 | Loss: 0.00001185
Iteration 158/1000 | Loss: 0.00001185
Iteration 159/1000 | Loss: 0.00001185
Iteration 160/1000 | Loss: 0.00001185
Iteration 161/1000 | Loss: 0.00001185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.1846150300698355e-05, 1.1846150300698355e-05, 1.1846150300698355e-05, 1.1846150300698355e-05, 1.1846150300698355e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1846150300698355e-05

Optimization complete. Final v2v error: 2.965752601623535 mm

Highest mean error: 3.337052822113037 mm for frame 134

Lowest mean error: 2.775763511657715 mm for frame 232

Saving results

Total time: 54.084028482437134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00592982
Iteration 2/25 | Loss: 0.00157402
Iteration 3/25 | Loss: 0.00131492
Iteration 4/25 | Loss: 0.00129367
Iteration 5/25 | Loss: 0.00128997
Iteration 6/25 | Loss: 0.00128895
Iteration 7/25 | Loss: 0.00128884
Iteration 8/25 | Loss: 0.00128884
Iteration 9/25 | Loss: 0.00128884
Iteration 10/25 | Loss: 0.00128884
Iteration 11/25 | Loss: 0.00128884
Iteration 12/25 | Loss: 0.00128884
Iteration 13/25 | Loss: 0.00128884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012888428755104542, 0.0012888428755104542, 0.0012888428755104542, 0.0012888428755104542, 0.0012888428755104542]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012888428755104542

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17591929
Iteration 2/25 | Loss: 0.00083102
Iteration 3/25 | Loss: 0.00083101
Iteration 4/25 | Loss: 0.00083101
Iteration 5/25 | Loss: 0.00083101
Iteration 6/25 | Loss: 0.00083101
Iteration 7/25 | Loss: 0.00083101
Iteration 8/25 | Loss: 0.00083101
Iteration 9/25 | Loss: 0.00083101
Iteration 10/25 | Loss: 0.00083101
Iteration 11/25 | Loss: 0.00083101
Iteration 12/25 | Loss: 0.00083101
Iteration 13/25 | Loss: 0.00083101
Iteration 14/25 | Loss: 0.00083101
Iteration 15/25 | Loss: 0.00083101
Iteration 16/25 | Loss: 0.00083101
Iteration 17/25 | Loss: 0.00083101
Iteration 18/25 | Loss: 0.00083101
Iteration 19/25 | Loss: 0.00083101
Iteration 20/25 | Loss: 0.00083101
Iteration 21/25 | Loss: 0.00083101
Iteration 22/25 | Loss: 0.00083101
Iteration 23/25 | Loss: 0.00083101
Iteration 24/25 | Loss: 0.00083101
Iteration 25/25 | Loss: 0.00083101

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083101
Iteration 2/1000 | Loss: 0.00004445
Iteration 3/1000 | Loss: 0.00003189
Iteration 4/1000 | Loss: 0.00002411
Iteration 5/1000 | Loss: 0.00002247
Iteration 6/1000 | Loss: 0.00002144
Iteration 7/1000 | Loss: 0.00002083
Iteration 8/1000 | Loss: 0.00002043
Iteration 9/1000 | Loss: 0.00002004
Iteration 10/1000 | Loss: 0.00001976
Iteration 11/1000 | Loss: 0.00001951
Iteration 12/1000 | Loss: 0.00001938
Iteration 13/1000 | Loss: 0.00001929
Iteration 14/1000 | Loss: 0.00001920
Iteration 15/1000 | Loss: 0.00001920
Iteration 16/1000 | Loss: 0.00001919
Iteration 17/1000 | Loss: 0.00001919
Iteration 18/1000 | Loss: 0.00001919
Iteration 19/1000 | Loss: 0.00001918
Iteration 20/1000 | Loss: 0.00001917
Iteration 21/1000 | Loss: 0.00001916
Iteration 22/1000 | Loss: 0.00001915
Iteration 23/1000 | Loss: 0.00001915
Iteration 24/1000 | Loss: 0.00001915
Iteration 25/1000 | Loss: 0.00001914
Iteration 26/1000 | Loss: 0.00001914
Iteration 27/1000 | Loss: 0.00001913
Iteration 28/1000 | Loss: 0.00001911
Iteration 29/1000 | Loss: 0.00001910
Iteration 30/1000 | Loss: 0.00001910
Iteration 31/1000 | Loss: 0.00001909
Iteration 32/1000 | Loss: 0.00001908
Iteration 33/1000 | Loss: 0.00001908
Iteration 34/1000 | Loss: 0.00001905
Iteration 35/1000 | Loss: 0.00001902
Iteration 36/1000 | Loss: 0.00001901
Iteration 37/1000 | Loss: 0.00001899
Iteration 38/1000 | Loss: 0.00001898
Iteration 39/1000 | Loss: 0.00001896
Iteration 40/1000 | Loss: 0.00001894
Iteration 41/1000 | Loss: 0.00001894
Iteration 42/1000 | Loss: 0.00001892
Iteration 43/1000 | Loss: 0.00001892
Iteration 44/1000 | Loss: 0.00001892
Iteration 45/1000 | Loss: 0.00001892
Iteration 46/1000 | Loss: 0.00001891
Iteration 47/1000 | Loss: 0.00001889
Iteration 48/1000 | Loss: 0.00001889
Iteration 49/1000 | Loss: 0.00001889
Iteration 50/1000 | Loss: 0.00001888
Iteration 51/1000 | Loss: 0.00001888
Iteration 52/1000 | Loss: 0.00001887
Iteration 53/1000 | Loss: 0.00001887
Iteration 54/1000 | Loss: 0.00001887
Iteration 55/1000 | Loss: 0.00001886
Iteration 56/1000 | Loss: 0.00001886
Iteration 57/1000 | Loss: 0.00001885
Iteration 58/1000 | Loss: 0.00001885
Iteration 59/1000 | Loss: 0.00001880
Iteration 60/1000 | Loss: 0.00001878
Iteration 61/1000 | Loss: 0.00001878
Iteration 62/1000 | Loss: 0.00001877
Iteration 63/1000 | Loss: 0.00001877
Iteration 64/1000 | Loss: 0.00001877
Iteration 65/1000 | Loss: 0.00001876
Iteration 66/1000 | Loss: 0.00001876
Iteration 67/1000 | Loss: 0.00001876
Iteration 68/1000 | Loss: 0.00001876
Iteration 69/1000 | Loss: 0.00001876
Iteration 70/1000 | Loss: 0.00001876
Iteration 71/1000 | Loss: 0.00001876
Iteration 72/1000 | Loss: 0.00001876
Iteration 73/1000 | Loss: 0.00001876
Iteration 74/1000 | Loss: 0.00001875
Iteration 75/1000 | Loss: 0.00001875
Iteration 76/1000 | Loss: 0.00001874
Iteration 77/1000 | Loss: 0.00001874
Iteration 78/1000 | Loss: 0.00001874
Iteration 79/1000 | Loss: 0.00001874
Iteration 80/1000 | Loss: 0.00001873
Iteration 81/1000 | Loss: 0.00001873
Iteration 82/1000 | Loss: 0.00001872
Iteration 83/1000 | Loss: 0.00001872
Iteration 84/1000 | Loss: 0.00001871
Iteration 85/1000 | Loss: 0.00001871
Iteration 86/1000 | Loss: 0.00001870
Iteration 87/1000 | Loss: 0.00001870
Iteration 88/1000 | Loss: 0.00001870
Iteration 89/1000 | Loss: 0.00001870
Iteration 90/1000 | Loss: 0.00001869
Iteration 91/1000 | Loss: 0.00001869
Iteration 92/1000 | Loss: 0.00001869
Iteration 93/1000 | Loss: 0.00001869
Iteration 94/1000 | Loss: 0.00001868
Iteration 95/1000 | Loss: 0.00001868
Iteration 96/1000 | Loss: 0.00001868
Iteration 97/1000 | Loss: 0.00001868
Iteration 98/1000 | Loss: 0.00001868
Iteration 99/1000 | Loss: 0.00001868
Iteration 100/1000 | Loss: 0.00001867
Iteration 101/1000 | Loss: 0.00001867
Iteration 102/1000 | Loss: 0.00001867
Iteration 103/1000 | Loss: 0.00001867
Iteration 104/1000 | Loss: 0.00001867
Iteration 105/1000 | Loss: 0.00001867
Iteration 106/1000 | Loss: 0.00001867
Iteration 107/1000 | Loss: 0.00001867
Iteration 108/1000 | Loss: 0.00001867
Iteration 109/1000 | Loss: 0.00001867
Iteration 110/1000 | Loss: 0.00001866
Iteration 111/1000 | Loss: 0.00001866
Iteration 112/1000 | Loss: 0.00001866
Iteration 113/1000 | Loss: 0.00001866
Iteration 114/1000 | Loss: 0.00001866
Iteration 115/1000 | Loss: 0.00001865
Iteration 116/1000 | Loss: 0.00001865
Iteration 117/1000 | Loss: 0.00001865
Iteration 118/1000 | Loss: 0.00001865
Iteration 119/1000 | Loss: 0.00001865
Iteration 120/1000 | Loss: 0.00001865
Iteration 121/1000 | Loss: 0.00001865
Iteration 122/1000 | Loss: 0.00001864
Iteration 123/1000 | Loss: 0.00001864
Iteration 124/1000 | Loss: 0.00001864
Iteration 125/1000 | Loss: 0.00001864
Iteration 126/1000 | Loss: 0.00001864
Iteration 127/1000 | Loss: 0.00001864
Iteration 128/1000 | Loss: 0.00001863
Iteration 129/1000 | Loss: 0.00001863
Iteration 130/1000 | Loss: 0.00001863
Iteration 131/1000 | Loss: 0.00001862
Iteration 132/1000 | Loss: 0.00001862
Iteration 133/1000 | Loss: 0.00001862
Iteration 134/1000 | Loss: 0.00001862
Iteration 135/1000 | Loss: 0.00001862
Iteration 136/1000 | Loss: 0.00001862
Iteration 137/1000 | Loss: 0.00001861
Iteration 138/1000 | Loss: 0.00001861
Iteration 139/1000 | Loss: 0.00001861
Iteration 140/1000 | Loss: 0.00001861
Iteration 141/1000 | Loss: 0.00001861
Iteration 142/1000 | Loss: 0.00001860
Iteration 143/1000 | Loss: 0.00001860
Iteration 144/1000 | Loss: 0.00001860
Iteration 145/1000 | Loss: 0.00001860
Iteration 146/1000 | Loss: 0.00001860
Iteration 147/1000 | Loss: 0.00001860
Iteration 148/1000 | Loss: 0.00001859
Iteration 149/1000 | Loss: 0.00001859
Iteration 150/1000 | Loss: 0.00001859
Iteration 151/1000 | Loss: 0.00001859
Iteration 152/1000 | Loss: 0.00001859
Iteration 153/1000 | Loss: 0.00001859
Iteration 154/1000 | Loss: 0.00001858
Iteration 155/1000 | Loss: 0.00001858
Iteration 156/1000 | Loss: 0.00001858
Iteration 157/1000 | Loss: 0.00001857
Iteration 158/1000 | Loss: 0.00001857
Iteration 159/1000 | Loss: 0.00001857
Iteration 160/1000 | Loss: 0.00001857
Iteration 161/1000 | Loss: 0.00001857
Iteration 162/1000 | Loss: 0.00001857
Iteration 163/1000 | Loss: 0.00001857
Iteration 164/1000 | Loss: 0.00001856
Iteration 165/1000 | Loss: 0.00001856
Iteration 166/1000 | Loss: 0.00001856
Iteration 167/1000 | Loss: 0.00001856
Iteration 168/1000 | Loss: 0.00001856
Iteration 169/1000 | Loss: 0.00001856
Iteration 170/1000 | Loss: 0.00001856
Iteration 171/1000 | Loss: 0.00001856
Iteration 172/1000 | Loss: 0.00001856
Iteration 173/1000 | Loss: 0.00001856
Iteration 174/1000 | Loss: 0.00001856
Iteration 175/1000 | Loss: 0.00001856
Iteration 176/1000 | Loss: 0.00001856
Iteration 177/1000 | Loss: 0.00001856
Iteration 178/1000 | Loss: 0.00001856
Iteration 179/1000 | Loss: 0.00001856
Iteration 180/1000 | Loss: 0.00001856
Iteration 181/1000 | Loss: 0.00001856
Iteration 182/1000 | Loss: 0.00001856
Iteration 183/1000 | Loss: 0.00001856
Iteration 184/1000 | Loss: 0.00001856
Iteration 185/1000 | Loss: 0.00001856
Iteration 186/1000 | Loss: 0.00001856
Iteration 187/1000 | Loss: 0.00001856
Iteration 188/1000 | Loss: 0.00001856
Iteration 189/1000 | Loss: 0.00001856
Iteration 190/1000 | Loss: 0.00001856
Iteration 191/1000 | Loss: 0.00001856
Iteration 192/1000 | Loss: 0.00001856
Iteration 193/1000 | Loss: 0.00001856
Iteration 194/1000 | Loss: 0.00001856
Iteration 195/1000 | Loss: 0.00001856
Iteration 196/1000 | Loss: 0.00001856
Iteration 197/1000 | Loss: 0.00001856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.8557930161477998e-05, 1.8557930161477998e-05, 1.8557930161477998e-05, 1.8557930161477998e-05, 1.8557930161477998e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8557930161477998e-05

Optimization complete. Final v2v error: 3.541348457336426 mm

Highest mean error: 5.314370155334473 mm for frame 58

Lowest mean error: 3.093850612640381 mm for frame 131

Saving results

Total time: 43.134589195251465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410869
Iteration 2/25 | Loss: 0.00126275
Iteration 3/25 | Loss: 0.00119184
Iteration 4/25 | Loss: 0.00117966
Iteration 5/25 | Loss: 0.00117566
Iteration 6/25 | Loss: 0.00117487
Iteration 7/25 | Loss: 0.00117487
Iteration 8/25 | Loss: 0.00117487
Iteration 9/25 | Loss: 0.00117487
Iteration 10/25 | Loss: 0.00117487
Iteration 11/25 | Loss: 0.00117487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011748713441193104, 0.0011748713441193104, 0.0011748713441193104, 0.0011748713441193104, 0.0011748713441193104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011748713441193104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.11129618
Iteration 2/25 | Loss: 0.00089335
Iteration 3/25 | Loss: 0.00089335
Iteration 4/25 | Loss: 0.00089335
Iteration 5/25 | Loss: 0.00089335
Iteration 6/25 | Loss: 0.00089335
Iteration 7/25 | Loss: 0.00089334
Iteration 8/25 | Loss: 0.00089334
Iteration 9/25 | Loss: 0.00089334
Iteration 10/25 | Loss: 0.00089334
Iteration 11/25 | Loss: 0.00089334
Iteration 12/25 | Loss: 0.00089334
Iteration 13/25 | Loss: 0.00089334
Iteration 14/25 | Loss: 0.00089334
Iteration 15/25 | Loss: 0.00089334
Iteration 16/25 | Loss: 0.00089334
Iteration 17/25 | Loss: 0.00089334
Iteration 18/25 | Loss: 0.00089334
Iteration 19/25 | Loss: 0.00089334
Iteration 20/25 | Loss: 0.00089334
Iteration 21/25 | Loss: 0.00089334
Iteration 22/25 | Loss: 0.00089334
Iteration 23/25 | Loss: 0.00089334
Iteration 24/25 | Loss: 0.00089334
Iteration 25/25 | Loss: 0.00089334

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089334
Iteration 2/1000 | Loss: 0.00002561
Iteration 3/1000 | Loss: 0.00001750
Iteration 4/1000 | Loss: 0.00001501
Iteration 5/1000 | Loss: 0.00001385
Iteration 6/1000 | Loss: 0.00001313
Iteration 7/1000 | Loss: 0.00001269
Iteration 8/1000 | Loss: 0.00001234
Iteration 9/1000 | Loss: 0.00001217
Iteration 10/1000 | Loss: 0.00001193
Iteration 11/1000 | Loss: 0.00001175
Iteration 12/1000 | Loss: 0.00001172
Iteration 13/1000 | Loss: 0.00001172
Iteration 14/1000 | Loss: 0.00001171
Iteration 15/1000 | Loss: 0.00001170
Iteration 16/1000 | Loss: 0.00001163
Iteration 17/1000 | Loss: 0.00001160
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001148
Iteration 20/1000 | Loss: 0.00001147
Iteration 21/1000 | Loss: 0.00001144
Iteration 22/1000 | Loss: 0.00001144
Iteration 23/1000 | Loss: 0.00001144
Iteration 24/1000 | Loss: 0.00001144
Iteration 25/1000 | Loss: 0.00001144
Iteration 26/1000 | Loss: 0.00001144
Iteration 27/1000 | Loss: 0.00001144
Iteration 28/1000 | Loss: 0.00001144
Iteration 29/1000 | Loss: 0.00001144
Iteration 30/1000 | Loss: 0.00001143
Iteration 31/1000 | Loss: 0.00001143
Iteration 32/1000 | Loss: 0.00001136
Iteration 33/1000 | Loss: 0.00001136
Iteration 34/1000 | Loss: 0.00001134
Iteration 35/1000 | Loss: 0.00001134
Iteration 36/1000 | Loss: 0.00001134
Iteration 37/1000 | Loss: 0.00001134
Iteration 38/1000 | Loss: 0.00001134
Iteration 39/1000 | Loss: 0.00001133
Iteration 40/1000 | Loss: 0.00001133
Iteration 41/1000 | Loss: 0.00001133
Iteration 42/1000 | Loss: 0.00001132
Iteration 43/1000 | Loss: 0.00001131
Iteration 44/1000 | Loss: 0.00001130
Iteration 45/1000 | Loss: 0.00001130
Iteration 46/1000 | Loss: 0.00001129
Iteration 47/1000 | Loss: 0.00001129
Iteration 48/1000 | Loss: 0.00001129
Iteration 49/1000 | Loss: 0.00001128
Iteration 50/1000 | Loss: 0.00001128
Iteration 51/1000 | Loss: 0.00001126
Iteration 52/1000 | Loss: 0.00001126
Iteration 53/1000 | Loss: 0.00001125
Iteration 54/1000 | Loss: 0.00001125
Iteration 55/1000 | Loss: 0.00001124
Iteration 56/1000 | Loss: 0.00001124
Iteration 57/1000 | Loss: 0.00001124
Iteration 58/1000 | Loss: 0.00001124
Iteration 59/1000 | Loss: 0.00001123
Iteration 60/1000 | Loss: 0.00001123
Iteration 61/1000 | Loss: 0.00001122
Iteration 62/1000 | Loss: 0.00001122
Iteration 63/1000 | Loss: 0.00001121
Iteration 64/1000 | Loss: 0.00001121
Iteration 65/1000 | Loss: 0.00001121
Iteration 66/1000 | Loss: 0.00001120
Iteration 67/1000 | Loss: 0.00001120
Iteration 68/1000 | Loss: 0.00001119
Iteration 69/1000 | Loss: 0.00001119
Iteration 70/1000 | Loss: 0.00001119
Iteration 71/1000 | Loss: 0.00001118
Iteration 72/1000 | Loss: 0.00001118
Iteration 73/1000 | Loss: 0.00001117
Iteration 74/1000 | Loss: 0.00001117
Iteration 75/1000 | Loss: 0.00001117
Iteration 76/1000 | Loss: 0.00001117
Iteration 77/1000 | Loss: 0.00001116
Iteration 78/1000 | Loss: 0.00001115
Iteration 79/1000 | Loss: 0.00001114
Iteration 80/1000 | Loss: 0.00001113
Iteration 81/1000 | Loss: 0.00001113
Iteration 82/1000 | Loss: 0.00001113
Iteration 83/1000 | Loss: 0.00001112
Iteration 84/1000 | Loss: 0.00001112
Iteration 85/1000 | Loss: 0.00001111
Iteration 86/1000 | Loss: 0.00001111
Iteration 87/1000 | Loss: 0.00001111
Iteration 88/1000 | Loss: 0.00001110
Iteration 89/1000 | Loss: 0.00001110
Iteration 90/1000 | Loss: 0.00001110
Iteration 91/1000 | Loss: 0.00001110
Iteration 92/1000 | Loss: 0.00001110
Iteration 93/1000 | Loss: 0.00001110
Iteration 94/1000 | Loss: 0.00001110
Iteration 95/1000 | Loss: 0.00001110
Iteration 96/1000 | Loss: 0.00001110
Iteration 97/1000 | Loss: 0.00001109
Iteration 98/1000 | Loss: 0.00001109
Iteration 99/1000 | Loss: 0.00001109
Iteration 100/1000 | Loss: 0.00001109
Iteration 101/1000 | Loss: 0.00001108
Iteration 102/1000 | Loss: 0.00001108
Iteration 103/1000 | Loss: 0.00001108
Iteration 104/1000 | Loss: 0.00001108
Iteration 105/1000 | Loss: 0.00001108
Iteration 106/1000 | Loss: 0.00001108
Iteration 107/1000 | Loss: 0.00001108
Iteration 108/1000 | Loss: 0.00001108
Iteration 109/1000 | Loss: 0.00001107
Iteration 110/1000 | Loss: 0.00001107
Iteration 111/1000 | Loss: 0.00001107
Iteration 112/1000 | Loss: 0.00001107
Iteration 113/1000 | Loss: 0.00001107
Iteration 114/1000 | Loss: 0.00001107
Iteration 115/1000 | Loss: 0.00001107
Iteration 116/1000 | Loss: 0.00001107
Iteration 117/1000 | Loss: 0.00001107
Iteration 118/1000 | Loss: 0.00001107
Iteration 119/1000 | Loss: 0.00001107
Iteration 120/1000 | Loss: 0.00001106
Iteration 121/1000 | Loss: 0.00001106
Iteration 122/1000 | Loss: 0.00001106
Iteration 123/1000 | Loss: 0.00001105
Iteration 124/1000 | Loss: 0.00001105
Iteration 125/1000 | Loss: 0.00001105
Iteration 126/1000 | Loss: 0.00001105
Iteration 127/1000 | Loss: 0.00001105
Iteration 128/1000 | Loss: 0.00001104
Iteration 129/1000 | Loss: 0.00001104
Iteration 130/1000 | Loss: 0.00001104
Iteration 131/1000 | Loss: 0.00001104
Iteration 132/1000 | Loss: 0.00001103
Iteration 133/1000 | Loss: 0.00001103
Iteration 134/1000 | Loss: 0.00001103
Iteration 135/1000 | Loss: 0.00001103
Iteration 136/1000 | Loss: 0.00001103
Iteration 137/1000 | Loss: 0.00001103
Iteration 138/1000 | Loss: 0.00001103
Iteration 139/1000 | Loss: 0.00001103
Iteration 140/1000 | Loss: 0.00001103
Iteration 141/1000 | Loss: 0.00001103
Iteration 142/1000 | Loss: 0.00001103
Iteration 143/1000 | Loss: 0.00001103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.1031467693101149e-05, 1.1031467693101149e-05, 1.1031467693101149e-05, 1.1031467693101149e-05, 1.1031467693101149e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1031467693101149e-05

Optimization complete. Final v2v error: 2.8569729328155518 mm

Highest mean error: 3.1530356407165527 mm for frame 102

Lowest mean error: 2.6551523208618164 mm for frame 34

Saving results

Total time: 38.04748320579529
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011015
Iteration 2/25 | Loss: 0.00348199
Iteration 3/25 | Loss: 0.00245112
Iteration 4/25 | Loss: 0.00216356
Iteration 5/25 | Loss: 0.00186415
Iteration 6/25 | Loss: 0.00177249
Iteration 7/25 | Loss: 0.00166617
Iteration 8/25 | Loss: 0.00157204
Iteration 9/25 | Loss: 0.00150792
Iteration 10/25 | Loss: 0.00145714
Iteration 11/25 | Loss: 0.00142891
Iteration 12/25 | Loss: 0.00140653
Iteration 13/25 | Loss: 0.00139564
Iteration 14/25 | Loss: 0.00138157
Iteration 15/25 | Loss: 0.00137953
Iteration 16/25 | Loss: 0.00137236
Iteration 17/25 | Loss: 0.00137000
Iteration 18/25 | Loss: 0.00136966
Iteration 19/25 | Loss: 0.00136575
Iteration 20/25 | Loss: 0.00136448
Iteration 21/25 | Loss: 0.00136426
Iteration 22/25 | Loss: 0.00136410
Iteration 23/25 | Loss: 0.00136410
Iteration 24/25 | Loss: 0.00136409
Iteration 25/25 | Loss: 0.00136409

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36197257
Iteration 2/25 | Loss: 0.00191925
Iteration 3/25 | Loss: 0.00191924
Iteration 4/25 | Loss: 0.00168868
Iteration 5/25 | Loss: 0.00168723
Iteration 6/25 | Loss: 0.00168723
Iteration 7/25 | Loss: 0.00168723
Iteration 8/25 | Loss: 0.00168723
Iteration 9/25 | Loss: 0.00168723
Iteration 10/25 | Loss: 0.00168723
Iteration 11/25 | Loss: 0.00168723
Iteration 12/25 | Loss: 0.00168723
Iteration 13/25 | Loss: 0.00168723
Iteration 14/25 | Loss: 0.00168723
Iteration 15/25 | Loss: 0.00168723
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0016872291453182697, 0.0016872291453182697, 0.0016872291453182697, 0.0016872291453182697, 0.0016872291453182697]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016872291453182697

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168723
Iteration 2/1000 | Loss: 0.00035267
Iteration 3/1000 | Loss: 0.00071253
Iteration 4/1000 | Loss: 0.00112137
Iteration 5/1000 | Loss: 0.00015268
Iteration 6/1000 | Loss: 0.00017110
Iteration 7/1000 | Loss: 0.00017869
Iteration 8/1000 | Loss: 0.00031104
Iteration 9/1000 | Loss: 0.00035344
Iteration 10/1000 | Loss: 0.00010737
Iteration 11/1000 | Loss: 0.00012344
Iteration 12/1000 | Loss: 0.00056236
Iteration 13/1000 | Loss: 0.00148612
Iteration 14/1000 | Loss: 0.00341722
Iteration 15/1000 | Loss: 0.00038650
Iteration 16/1000 | Loss: 0.00022418
Iteration 17/1000 | Loss: 0.00012980
Iteration 18/1000 | Loss: 0.00029719
Iteration 19/1000 | Loss: 0.00009937
Iteration 20/1000 | Loss: 0.00009814
Iteration 21/1000 | Loss: 0.00020358
Iteration 22/1000 | Loss: 0.00003673
Iteration 23/1000 | Loss: 0.00010494
Iteration 24/1000 | Loss: 0.00011759
Iteration 25/1000 | Loss: 0.00008512
Iteration 26/1000 | Loss: 0.00019955
Iteration 27/1000 | Loss: 0.00002122
Iteration 28/1000 | Loss: 0.00006139
Iteration 29/1000 | Loss: 0.00006591
Iteration 30/1000 | Loss: 0.00003987
Iteration 31/1000 | Loss: 0.00002859
Iteration 32/1000 | Loss: 0.00009770
Iteration 33/1000 | Loss: 0.00003554
Iteration 34/1000 | Loss: 0.00001659
Iteration 35/1000 | Loss: 0.00007044
Iteration 36/1000 | Loss: 0.00002101
Iteration 37/1000 | Loss: 0.00001871
Iteration 38/1000 | Loss: 0.00001680
Iteration 39/1000 | Loss: 0.00002856
Iteration 40/1000 | Loss: 0.00002582
Iteration 41/1000 | Loss: 0.00001699
Iteration 42/1000 | Loss: 0.00001543
Iteration 43/1000 | Loss: 0.00001515
Iteration 44/1000 | Loss: 0.00001515
Iteration 45/1000 | Loss: 0.00001515
Iteration 46/1000 | Loss: 0.00001515
Iteration 47/1000 | Loss: 0.00001515
Iteration 48/1000 | Loss: 0.00001515
Iteration 49/1000 | Loss: 0.00001515
Iteration 50/1000 | Loss: 0.00001515
Iteration 51/1000 | Loss: 0.00001515
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001515
Iteration 54/1000 | Loss: 0.00001514
Iteration 55/1000 | Loss: 0.00001514
Iteration 56/1000 | Loss: 0.00001514
Iteration 57/1000 | Loss: 0.00001514
Iteration 58/1000 | Loss: 0.00001514
Iteration 59/1000 | Loss: 0.00001514
Iteration 60/1000 | Loss: 0.00001514
Iteration 61/1000 | Loss: 0.00001514
Iteration 62/1000 | Loss: 0.00001514
Iteration 63/1000 | Loss: 0.00001514
Iteration 64/1000 | Loss: 0.00001514
Iteration 65/1000 | Loss: 0.00001514
Iteration 66/1000 | Loss: 0.00001513
Iteration 67/1000 | Loss: 0.00001696
Iteration 68/1000 | Loss: 0.00001696
Iteration 69/1000 | Loss: 0.00001695
Iteration 70/1000 | Loss: 0.00001695
Iteration 71/1000 | Loss: 0.00001695
Iteration 72/1000 | Loss: 0.00004124
Iteration 73/1000 | Loss: 0.00001539
Iteration 74/1000 | Loss: 0.00001508
Iteration 75/1000 | Loss: 0.00001508
Iteration 76/1000 | Loss: 0.00001507
Iteration 77/1000 | Loss: 0.00001491
Iteration 78/1000 | Loss: 0.00001491
Iteration 79/1000 | Loss: 0.00001491
Iteration 80/1000 | Loss: 0.00001490
Iteration 81/1000 | Loss: 0.00001490
Iteration 82/1000 | Loss: 0.00001490
Iteration 83/1000 | Loss: 0.00001761
Iteration 84/1000 | Loss: 0.00007207
Iteration 85/1000 | Loss: 0.00002020
Iteration 86/1000 | Loss: 0.00001899
Iteration 87/1000 | Loss: 0.00001561
Iteration 88/1000 | Loss: 0.00001489
Iteration 89/1000 | Loss: 0.00001489
Iteration 90/1000 | Loss: 0.00001489
Iteration 91/1000 | Loss: 0.00001489
Iteration 92/1000 | Loss: 0.00001489
Iteration 93/1000 | Loss: 0.00001489
Iteration 94/1000 | Loss: 0.00001489
Iteration 95/1000 | Loss: 0.00001489
Iteration 96/1000 | Loss: 0.00001489
Iteration 97/1000 | Loss: 0.00001489
Iteration 98/1000 | Loss: 0.00001488
Iteration 99/1000 | Loss: 0.00001488
Iteration 100/1000 | Loss: 0.00001488
Iteration 101/1000 | Loss: 0.00001488
Iteration 102/1000 | Loss: 0.00001488
Iteration 103/1000 | Loss: 0.00001488
Iteration 104/1000 | Loss: 0.00001488
Iteration 105/1000 | Loss: 0.00001488
Iteration 106/1000 | Loss: 0.00001488
Iteration 107/1000 | Loss: 0.00001488
Iteration 108/1000 | Loss: 0.00001488
Iteration 109/1000 | Loss: 0.00001487
Iteration 110/1000 | Loss: 0.00001487
Iteration 111/1000 | Loss: 0.00001487
Iteration 112/1000 | Loss: 0.00001487
Iteration 113/1000 | Loss: 0.00001841
Iteration 114/1000 | Loss: 0.00001658
Iteration 115/1000 | Loss: 0.00001488
Iteration 116/1000 | Loss: 0.00001486
Iteration 117/1000 | Loss: 0.00001486
Iteration 118/1000 | Loss: 0.00001485
Iteration 119/1000 | Loss: 0.00001485
Iteration 120/1000 | Loss: 0.00001485
Iteration 121/1000 | Loss: 0.00001485
Iteration 122/1000 | Loss: 0.00001485
Iteration 123/1000 | Loss: 0.00001485
Iteration 124/1000 | Loss: 0.00001485
Iteration 125/1000 | Loss: 0.00001485
Iteration 126/1000 | Loss: 0.00001485
Iteration 127/1000 | Loss: 0.00001485
Iteration 128/1000 | Loss: 0.00001485
Iteration 129/1000 | Loss: 0.00001485
Iteration 130/1000 | Loss: 0.00001485
Iteration 131/1000 | Loss: 0.00001485
Iteration 132/1000 | Loss: 0.00001485
Iteration 133/1000 | Loss: 0.00001485
Iteration 134/1000 | Loss: 0.00001485
Iteration 135/1000 | Loss: 0.00001485
Iteration 136/1000 | Loss: 0.00001485
Iteration 137/1000 | Loss: 0.00001485
Iteration 138/1000 | Loss: 0.00001485
Iteration 139/1000 | Loss: 0.00001485
Iteration 140/1000 | Loss: 0.00001485
Iteration 141/1000 | Loss: 0.00001485
Iteration 142/1000 | Loss: 0.00001485
Iteration 143/1000 | Loss: 0.00001485
Iteration 144/1000 | Loss: 0.00001485
Iteration 145/1000 | Loss: 0.00001485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.4852394997433294e-05, 1.4852394997433294e-05, 1.4852394997433294e-05, 1.4852394997433294e-05, 1.4852394997433294e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4852394997433294e-05

Optimization complete. Final v2v error: 3.2773327827453613 mm

Highest mean error: 4.012353420257568 mm for frame 93

Lowest mean error: 3.003042697906494 mm for frame 75

Saving results

Total time: 128.48515009880066
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854652
Iteration 2/25 | Loss: 0.00129621
Iteration 3/25 | Loss: 0.00121201
Iteration 4/25 | Loss: 0.00119889
Iteration 5/25 | Loss: 0.00119748
Iteration 6/25 | Loss: 0.00119748
Iteration 7/25 | Loss: 0.00119748
Iteration 8/25 | Loss: 0.00119748
Iteration 9/25 | Loss: 0.00119748
Iteration 10/25 | Loss: 0.00119748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001197482692077756, 0.001197482692077756, 0.001197482692077756, 0.001197482692077756, 0.001197482692077756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001197482692077756

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39183879
Iteration 2/25 | Loss: 0.00092544
Iteration 3/25 | Loss: 0.00092543
Iteration 4/25 | Loss: 0.00092543
Iteration 5/25 | Loss: 0.00092543
Iteration 6/25 | Loss: 0.00092543
Iteration 7/25 | Loss: 0.00092543
Iteration 8/25 | Loss: 0.00092543
Iteration 9/25 | Loss: 0.00092543
Iteration 10/25 | Loss: 0.00092543
Iteration 11/25 | Loss: 0.00092543
Iteration 12/25 | Loss: 0.00092543
Iteration 13/25 | Loss: 0.00092543
Iteration 14/25 | Loss: 0.00092543
Iteration 15/25 | Loss: 0.00092543
Iteration 16/25 | Loss: 0.00092543
Iteration 17/25 | Loss: 0.00092543
Iteration 18/25 | Loss: 0.00092543
Iteration 19/25 | Loss: 0.00092543
Iteration 20/25 | Loss: 0.00092543
Iteration 21/25 | Loss: 0.00092543
Iteration 22/25 | Loss: 0.00092543
Iteration 23/25 | Loss: 0.00092543
Iteration 24/25 | Loss: 0.00092543
Iteration 25/25 | Loss: 0.00092543

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092543
Iteration 2/1000 | Loss: 0.00002639
Iteration 3/1000 | Loss: 0.00001875
Iteration 4/1000 | Loss: 0.00001708
Iteration 5/1000 | Loss: 0.00001615
Iteration 6/1000 | Loss: 0.00001547
Iteration 7/1000 | Loss: 0.00001499
Iteration 8/1000 | Loss: 0.00001465
Iteration 9/1000 | Loss: 0.00001439
Iteration 10/1000 | Loss: 0.00001408
Iteration 11/1000 | Loss: 0.00001401
Iteration 12/1000 | Loss: 0.00001389
Iteration 13/1000 | Loss: 0.00001385
Iteration 14/1000 | Loss: 0.00001383
Iteration 15/1000 | Loss: 0.00001371
Iteration 16/1000 | Loss: 0.00001365
Iteration 17/1000 | Loss: 0.00001365
Iteration 18/1000 | Loss: 0.00001358
Iteration 19/1000 | Loss: 0.00001356
Iteration 20/1000 | Loss: 0.00001355
Iteration 21/1000 | Loss: 0.00001354
Iteration 22/1000 | Loss: 0.00001354
Iteration 23/1000 | Loss: 0.00001353
Iteration 24/1000 | Loss: 0.00001347
Iteration 25/1000 | Loss: 0.00001344
Iteration 26/1000 | Loss: 0.00001342
Iteration 27/1000 | Loss: 0.00001340
Iteration 28/1000 | Loss: 0.00001339
Iteration 29/1000 | Loss: 0.00001338
Iteration 30/1000 | Loss: 0.00001338
Iteration 31/1000 | Loss: 0.00001338
Iteration 32/1000 | Loss: 0.00001337
Iteration 33/1000 | Loss: 0.00001336
Iteration 34/1000 | Loss: 0.00001335
Iteration 35/1000 | Loss: 0.00001334
Iteration 36/1000 | Loss: 0.00001334
Iteration 37/1000 | Loss: 0.00001334
Iteration 38/1000 | Loss: 0.00001331
Iteration 39/1000 | Loss: 0.00001330
Iteration 40/1000 | Loss: 0.00001329
Iteration 41/1000 | Loss: 0.00001329
Iteration 42/1000 | Loss: 0.00001328
Iteration 43/1000 | Loss: 0.00001328
Iteration 44/1000 | Loss: 0.00001328
Iteration 45/1000 | Loss: 0.00001327
Iteration 46/1000 | Loss: 0.00001327
Iteration 47/1000 | Loss: 0.00001324
Iteration 48/1000 | Loss: 0.00001324
Iteration 49/1000 | Loss: 0.00001323
Iteration 50/1000 | Loss: 0.00001323
Iteration 51/1000 | Loss: 0.00001322
Iteration 52/1000 | Loss: 0.00001322
Iteration 53/1000 | Loss: 0.00001322
Iteration 54/1000 | Loss: 0.00001321
Iteration 55/1000 | Loss: 0.00001321
Iteration 56/1000 | Loss: 0.00001320
Iteration 57/1000 | Loss: 0.00001320
Iteration 58/1000 | Loss: 0.00001320
Iteration 59/1000 | Loss: 0.00001320
Iteration 60/1000 | Loss: 0.00001319
Iteration 61/1000 | Loss: 0.00001318
Iteration 62/1000 | Loss: 0.00001317
Iteration 63/1000 | Loss: 0.00001316
Iteration 64/1000 | Loss: 0.00001316
Iteration 65/1000 | Loss: 0.00001316
Iteration 66/1000 | Loss: 0.00001316
Iteration 67/1000 | Loss: 0.00001316
Iteration 68/1000 | Loss: 0.00001315
Iteration 69/1000 | Loss: 0.00001315
Iteration 70/1000 | Loss: 0.00001315
Iteration 71/1000 | Loss: 0.00001315
Iteration 72/1000 | Loss: 0.00001315
Iteration 73/1000 | Loss: 0.00001315
Iteration 74/1000 | Loss: 0.00001314
Iteration 75/1000 | Loss: 0.00001314
Iteration 76/1000 | Loss: 0.00001313
Iteration 77/1000 | Loss: 0.00001313
Iteration 78/1000 | Loss: 0.00001312
Iteration 79/1000 | Loss: 0.00001312
Iteration 80/1000 | Loss: 0.00001312
Iteration 81/1000 | Loss: 0.00001311
Iteration 82/1000 | Loss: 0.00001311
Iteration 83/1000 | Loss: 0.00001311
Iteration 84/1000 | Loss: 0.00001310
Iteration 85/1000 | Loss: 0.00001310
Iteration 86/1000 | Loss: 0.00001310
Iteration 87/1000 | Loss: 0.00001309
Iteration 88/1000 | Loss: 0.00001309
Iteration 89/1000 | Loss: 0.00001309
Iteration 90/1000 | Loss: 0.00001308
Iteration 91/1000 | Loss: 0.00001308
Iteration 92/1000 | Loss: 0.00001308
Iteration 93/1000 | Loss: 0.00001308
Iteration 94/1000 | Loss: 0.00001308
Iteration 95/1000 | Loss: 0.00001308
Iteration 96/1000 | Loss: 0.00001308
Iteration 97/1000 | Loss: 0.00001307
Iteration 98/1000 | Loss: 0.00001307
Iteration 99/1000 | Loss: 0.00001307
Iteration 100/1000 | Loss: 0.00001306
Iteration 101/1000 | Loss: 0.00001306
Iteration 102/1000 | Loss: 0.00001305
Iteration 103/1000 | Loss: 0.00001304
Iteration 104/1000 | Loss: 0.00001304
Iteration 105/1000 | Loss: 0.00001304
Iteration 106/1000 | Loss: 0.00001304
Iteration 107/1000 | Loss: 0.00001304
Iteration 108/1000 | Loss: 0.00001304
Iteration 109/1000 | Loss: 0.00001304
Iteration 110/1000 | Loss: 0.00001304
Iteration 111/1000 | Loss: 0.00001304
Iteration 112/1000 | Loss: 0.00001304
Iteration 113/1000 | Loss: 0.00001303
Iteration 114/1000 | Loss: 0.00001303
Iteration 115/1000 | Loss: 0.00001302
Iteration 116/1000 | Loss: 0.00001302
Iteration 117/1000 | Loss: 0.00001302
Iteration 118/1000 | Loss: 0.00001302
Iteration 119/1000 | Loss: 0.00001301
Iteration 120/1000 | Loss: 0.00001301
Iteration 121/1000 | Loss: 0.00001301
Iteration 122/1000 | Loss: 0.00001301
Iteration 123/1000 | Loss: 0.00001301
Iteration 124/1000 | Loss: 0.00001301
Iteration 125/1000 | Loss: 0.00001301
Iteration 126/1000 | Loss: 0.00001301
Iteration 127/1000 | Loss: 0.00001301
Iteration 128/1000 | Loss: 0.00001301
Iteration 129/1000 | Loss: 0.00001300
Iteration 130/1000 | Loss: 0.00001300
Iteration 131/1000 | Loss: 0.00001300
Iteration 132/1000 | Loss: 0.00001300
Iteration 133/1000 | Loss: 0.00001300
Iteration 134/1000 | Loss: 0.00001300
Iteration 135/1000 | Loss: 0.00001300
Iteration 136/1000 | Loss: 0.00001300
Iteration 137/1000 | Loss: 0.00001300
Iteration 138/1000 | Loss: 0.00001300
Iteration 139/1000 | Loss: 0.00001300
Iteration 140/1000 | Loss: 0.00001300
Iteration 141/1000 | Loss: 0.00001300
Iteration 142/1000 | Loss: 0.00001300
Iteration 143/1000 | Loss: 0.00001300
Iteration 144/1000 | Loss: 0.00001300
Iteration 145/1000 | Loss: 0.00001300
Iteration 146/1000 | Loss: 0.00001300
Iteration 147/1000 | Loss: 0.00001300
Iteration 148/1000 | Loss: 0.00001300
Iteration 149/1000 | Loss: 0.00001300
Iteration 150/1000 | Loss: 0.00001300
Iteration 151/1000 | Loss: 0.00001300
Iteration 152/1000 | Loss: 0.00001300
Iteration 153/1000 | Loss: 0.00001300
Iteration 154/1000 | Loss: 0.00001300
Iteration 155/1000 | Loss: 0.00001300
Iteration 156/1000 | Loss: 0.00001300
Iteration 157/1000 | Loss: 0.00001300
Iteration 158/1000 | Loss: 0.00001300
Iteration 159/1000 | Loss: 0.00001300
Iteration 160/1000 | Loss: 0.00001300
Iteration 161/1000 | Loss: 0.00001300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.3003459571336862e-05, 1.3003459571336862e-05, 1.3003459571336862e-05, 1.3003459571336862e-05, 1.3003459571336862e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3003459571336862e-05

Optimization complete. Final v2v error: 3.0543212890625 mm

Highest mean error: 3.9824442863464355 mm for frame 92

Lowest mean error: 2.840522050857544 mm for frame 13

Saving results

Total time: 42.05936622619629
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842946
Iteration 2/25 | Loss: 0.00143966
Iteration 3/25 | Loss: 0.00128991
Iteration 4/25 | Loss: 0.00126884
Iteration 5/25 | Loss: 0.00126113
Iteration 6/25 | Loss: 0.00125905
Iteration 7/25 | Loss: 0.00125830
Iteration 8/25 | Loss: 0.00125830
Iteration 9/25 | Loss: 0.00125830
Iteration 10/25 | Loss: 0.00125830
Iteration 11/25 | Loss: 0.00125830
Iteration 12/25 | Loss: 0.00125830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012582991039380431, 0.0012582991039380431, 0.0012582991039380431, 0.0012582991039380431, 0.0012582991039380431]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012582991039380431

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.37260079
Iteration 2/25 | Loss: 0.00107377
Iteration 3/25 | Loss: 0.00107377
Iteration 4/25 | Loss: 0.00107377
Iteration 5/25 | Loss: 0.00107377
Iteration 6/25 | Loss: 0.00107377
Iteration 7/25 | Loss: 0.00107377
Iteration 8/25 | Loss: 0.00107377
Iteration 9/25 | Loss: 0.00107377
Iteration 10/25 | Loss: 0.00107377
Iteration 11/25 | Loss: 0.00107377
Iteration 12/25 | Loss: 0.00107377
Iteration 13/25 | Loss: 0.00107377
Iteration 14/25 | Loss: 0.00107377
Iteration 15/25 | Loss: 0.00107377
Iteration 16/25 | Loss: 0.00107377
Iteration 17/25 | Loss: 0.00107377
Iteration 18/25 | Loss: 0.00107377
Iteration 19/25 | Loss: 0.00107377
Iteration 20/25 | Loss: 0.00107377
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010737701086327434, 0.0010737701086327434, 0.0010737701086327434, 0.0010737701086327434, 0.0010737701086327434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010737701086327434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107377
Iteration 2/1000 | Loss: 0.00005707
Iteration 3/1000 | Loss: 0.00003561
Iteration 4/1000 | Loss: 0.00002632
Iteration 5/1000 | Loss: 0.00002322
Iteration 6/1000 | Loss: 0.00002200
Iteration 7/1000 | Loss: 0.00002091
Iteration 8/1000 | Loss: 0.00002023
Iteration 9/1000 | Loss: 0.00001981
Iteration 10/1000 | Loss: 0.00001939
Iteration 11/1000 | Loss: 0.00001905
Iteration 12/1000 | Loss: 0.00001888
Iteration 13/1000 | Loss: 0.00001867
Iteration 14/1000 | Loss: 0.00001844
Iteration 15/1000 | Loss: 0.00001825
Iteration 16/1000 | Loss: 0.00001813
Iteration 17/1000 | Loss: 0.00001812
Iteration 18/1000 | Loss: 0.00001811
Iteration 19/1000 | Loss: 0.00001811
Iteration 20/1000 | Loss: 0.00001807
Iteration 21/1000 | Loss: 0.00001805
Iteration 22/1000 | Loss: 0.00001804
Iteration 23/1000 | Loss: 0.00001800
Iteration 24/1000 | Loss: 0.00001799
Iteration 25/1000 | Loss: 0.00001799
Iteration 26/1000 | Loss: 0.00001799
Iteration 27/1000 | Loss: 0.00001799
Iteration 28/1000 | Loss: 0.00001798
Iteration 29/1000 | Loss: 0.00001798
Iteration 30/1000 | Loss: 0.00001798
Iteration 31/1000 | Loss: 0.00001798
Iteration 32/1000 | Loss: 0.00001797
Iteration 33/1000 | Loss: 0.00001796
Iteration 34/1000 | Loss: 0.00001796
Iteration 35/1000 | Loss: 0.00001796
Iteration 36/1000 | Loss: 0.00001794
Iteration 37/1000 | Loss: 0.00001790
Iteration 38/1000 | Loss: 0.00001787
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001786
Iteration 41/1000 | Loss: 0.00001786
Iteration 42/1000 | Loss: 0.00001783
Iteration 43/1000 | Loss: 0.00001782
Iteration 44/1000 | Loss: 0.00001781
Iteration 45/1000 | Loss: 0.00001781
Iteration 46/1000 | Loss: 0.00001780
Iteration 47/1000 | Loss: 0.00001780
Iteration 48/1000 | Loss: 0.00001780
Iteration 49/1000 | Loss: 0.00001779
Iteration 50/1000 | Loss: 0.00001779
Iteration 51/1000 | Loss: 0.00001779
Iteration 52/1000 | Loss: 0.00001779
Iteration 53/1000 | Loss: 0.00001779
Iteration 54/1000 | Loss: 0.00001779
Iteration 55/1000 | Loss: 0.00001779
Iteration 56/1000 | Loss: 0.00001779
Iteration 57/1000 | Loss: 0.00001779
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001779
Iteration 60/1000 | Loss: 0.00001779
Iteration 61/1000 | Loss: 0.00001779
Iteration 62/1000 | Loss: 0.00001778
Iteration 63/1000 | Loss: 0.00001777
Iteration 64/1000 | Loss: 0.00001777
Iteration 65/1000 | Loss: 0.00001777
Iteration 66/1000 | Loss: 0.00001777
Iteration 67/1000 | Loss: 0.00001777
Iteration 68/1000 | Loss: 0.00001777
Iteration 69/1000 | Loss: 0.00001777
Iteration 70/1000 | Loss: 0.00001777
Iteration 71/1000 | Loss: 0.00001777
Iteration 72/1000 | Loss: 0.00001777
Iteration 73/1000 | Loss: 0.00001777
Iteration 74/1000 | Loss: 0.00001776
Iteration 75/1000 | Loss: 0.00001776
Iteration 76/1000 | Loss: 0.00001775
Iteration 77/1000 | Loss: 0.00001775
Iteration 78/1000 | Loss: 0.00001775
Iteration 79/1000 | Loss: 0.00001775
Iteration 80/1000 | Loss: 0.00001774
Iteration 81/1000 | Loss: 0.00001774
Iteration 82/1000 | Loss: 0.00001774
Iteration 83/1000 | Loss: 0.00001773
Iteration 84/1000 | Loss: 0.00001773
Iteration 85/1000 | Loss: 0.00001773
Iteration 86/1000 | Loss: 0.00001772
Iteration 87/1000 | Loss: 0.00001772
Iteration 88/1000 | Loss: 0.00001772
Iteration 89/1000 | Loss: 0.00001772
Iteration 90/1000 | Loss: 0.00001771
Iteration 91/1000 | Loss: 0.00001771
Iteration 92/1000 | Loss: 0.00001771
Iteration 93/1000 | Loss: 0.00001771
Iteration 94/1000 | Loss: 0.00001771
Iteration 95/1000 | Loss: 0.00001770
Iteration 96/1000 | Loss: 0.00001770
Iteration 97/1000 | Loss: 0.00001770
Iteration 98/1000 | Loss: 0.00001770
Iteration 99/1000 | Loss: 0.00001770
Iteration 100/1000 | Loss: 0.00001770
Iteration 101/1000 | Loss: 0.00001769
Iteration 102/1000 | Loss: 0.00001769
Iteration 103/1000 | Loss: 0.00001769
Iteration 104/1000 | Loss: 0.00001769
Iteration 105/1000 | Loss: 0.00001769
Iteration 106/1000 | Loss: 0.00001769
Iteration 107/1000 | Loss: 0.00001769
Iteration 108/1000 | Loss: 0.00001769
Iteration 109/1000 | Loss: 0.00001769
Iteration 110/1000 | Loss: 0.00001769
Iteration 111/1000 | Loss: 0.00001769
Iteration 112/1000 | Loss: 0.00001768
Iteration 113/1000 | Loss: 0.00001768
Iteration 114/1000 | Loss: 0.00001768
Iteration 115/1000 | Loss: 0.00001768
Iteration 116/1000 | Loss: 0.00001768
Iteration 117/1000 | Loss: 0.00001768
Iteration 118/1000 | Loss: 0.00001767
Iteration 119/1000 | Loss: 0.00001767
Iteration 120/1000 | Loss: 0.00001767
Iteration 121/1000 | Loss: 0.00001767
Iteration 122/1000 | Loss: 0.00001766
Iteration 123/1000 | Loss: 0.00001766
Iteration 124/1000 | Loss: 0.00001766
Iteration 125/1000 | Loss: 0.00001766
Iteration 126/1000 | Loss: 0.00001766
Iteration 127/1000 | Loss: 0.00001766
Iteration 128/1000 | Loss: 0.00001766
Iteration 129/1000 | Loss: 0.00001765
Iteration 130/1000 | Loss: 0.00001765
Iteration 131/1000 | Loss: 0.00001765
Iteration 132/1000 | Loss: 0.00001765
Iteration 133/1000 | Loss: 0.00001765
Iteration 134/1000 | Loss: 0.00001765
Iteration 135/1000 | Loss: 0.00001765
Iteration 136/1000 | Loss: 0.00001764
Iteration 137/1000 | Loss: 0.00001764
Iteration 138/1000 | Loss: 0.00001764
Iteration 139/1000 | Loss: 0.00001764
Iteration 140/1000 | Loss: 0.00001764
Iteration 141/1000 | Loss: 0.00001764
Iteration 142/1000 | Loss: 0.00001764
Iteration 143/1000 | Loss: 0.00001764
Iteration 144/1000 | Loss: 0.00001764
Iteration 145/1000 | Loss: 0.00001764
Iteration 146/1000 | Loss: 0.00001764
Iteration 147/1000 | Loss: 0.00001764
Iteration 148/1000 | Loss: 0.00001763
Iteration 149/1000 | Loss: 0.00001763
Iteration 150/1000 | Loss: 0.00001763
Iteration 151/1000 | Loss: 0.00001763
Iteration 152/1000 | Loss: 0.00001763
Iteration 153/1000 | Loss: 0.00001763
Iteration 154/1000 | Loss: 0.00001763
Iteration 155/1000 | Loss: 0.00001763
Iteration 156/1000 | Loss: 0.00001763
Iteration 157/1000 | Loss: 0.00001763
Iteration 158/1000 | Loss: 0.00001763
Iteration 159/1000 | Loss: 0.00001763
Iteration 160/1000 | Loss: 0.00001763
Iteration 161/1000 | Loss: 0.00001763
Iteration 162/1000 | Loss: 0.00001763
Iteration 163/1000 | Loss: 0.00001763
Iteration 164/1000 | Loss: 0.00001763
Iteration 165/1000 | Loss: 0.00001763
Iteration 166/1000 | Loss: 0.00001763
Iteration 167/1000 | Loss: 0.00001763
Iteration 168/1000 | Loss: 0.00001763
Iteration 169/1000 | Loss: 0.00001763
Iteration 170/1000 | Loss: 0.00001763
Iteration 171/1000 | Loss: 0.00001763
Iteration 172/1000 | Loss: 0.00001763
Iteration 173/1000 | Loss: 0.00001763
Iteration 174/1000 | Loss: 0.00001763
Iteration 175/1000 | Loss: 0.00001763
Iteration 176/1000 | Loss: 0.00001763
Iteration 177/1000 | Loss: 0.00001763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.7631109585636295e-05, 1.7631109585636295e-05, 1.7631109585636295e-05, 1.7631109585636295e-05, 1.7631109585636295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7631109585636295e-05

Optimization complete. Final v2v error: 3.4783358573913574 mm

Highest mean error: 5.639986038208008 mm for frame 70

Lowest mean error: 2.6786630153656006 mm for frame 127

Saving results

Total time: 44.52541446685791
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998232
Iteration 2/25 | Loss: 0.00225662
Iteration 3/25 | Loss: 0.00196093
Iteration 4/25 | Loss: 0.00190795
Iteration 5/25 | Loss: 0.00189384
Iteration 6/25 | Loss: 0.00188822
Iteration 7/25 | Loss: 0.00186563
Iteration 8/25 | Loss: 0.00186389
Iteration 9/25 | Loss: 0.00186273
Iteration 10/25 | Loss: 0.00183948
Iteration 11/25 | Loss: 0.00183759
Iteration 12/25 | Loss: 0.00183528
Iteration 13/25 | Loss: 0.00183215
Iteration 14/25 | Loss: 0.00182918
Iteration 15/25 | Loss: 0.00182640
Iteration 16/25 | Loss: 0.00182315
Iteration 17/25 | Loss: 0.00182045
Iteration 18/25 | Loss: 0.00181868
Iteration 19/25 | Loss: 0.00181766
Iteration 20/25 | Loss: 0.00181709
Iteration 21/25 | Loss: 0.00181661
Iteration 22/25 | Loss: 0.00181621
Iteration 23/25 | Loss: 0.00181591
Iteration 24/25 | Loss: 0.00181566
Iteration 25/25 | Loss: 0.00181547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38532603
Iteration 2/25 | Loss: 0.00414853
Iteration 3/25 | Loss: 0.00414852
Iteration 4/25 | Loss: 0.00414852
Iteration 5/25 | Loss: 0.00414852
Iteration 6/25 | Loss: 0.00414852
Iteration 7/25 | Loss: 0.00414852
Iteration 8/25 | Loss: 0.00414852
Iteration 9/25 | Loss: 0.00414852
Iteration 10/25 | Loss: 0.00414852
Iteration 11/25 | Loss: 0.00414852
Iteration 12/25 | Loss: 0.00414852
Iteration 13/25 | Loss: 0.00414852
Iteration 14/25 | Loss: 0.00414852
Iteration 15/25 | Loss: 0.00414852
Iteration 16/25 | Loss: 0.00414852
Iteration 17/25 | Loss: 0.00414852
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004148516803979874, 0.004148516803979874, 0.004148516803979874, 0.004148516803979874, 0.004148516803979874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004148516803979874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00414852
Iteration 2/1000 | Loss: 0.00116063
Iteration 3/1000 | Loss: 0.00203713
Iteration 4/1000 | Loss: 0.00231211
Iteration 5/1000 | Loss: 0.00259200
Iteration 6/1000 | Loss: 0.00159559
Iteration 7/1000 | Loss: 0.00144524
Iteration 8/1000 | Loss: 0.00558945
Iteration 9/1000 | Loss: 0.00819987
Iteration 10/1000 | Loss: 0.00739242
Iteration 11/1000 | Loss: 0.00748155
Iteration 12/1000 | Loss: 0.00489043
Iteration 13/1000 | Loss: 0.00552183
Iteration 14/1000 | Loss: 0.00432395
Iteration 15/1000 | Loss: 0.00280472
Iteration 16/1000 | Loss: 0.00150095
Iteration 17/1000 | Loss: 0.00212544
Iteration 18/1000 | Loss: 0.00246098
Iteration 19/1000 | Loss: 0.00098806
Iteration 20/1000 | Loss: 0.00104864
Iteration 21/1000 | Loss: 0.00328679
Iteration 22/1000 | Loss: 0.00474822
Iteration 23/1000 | Loss: 0.00258282
Iteration 24/1000 | Loss: 0.00363453
Iteration 25/1000 | Loss: 0.00258167
Iteration 26/1000 | Loss: 0.00292291
Iteration 27/1000 | Loss: 0.00826342
Iteration 28/1000 | Loss: 0.00460876
Iteration 29/1000 | Loss: 0.00638958
Iteration 30/1000 | Loss: 0.00383053
Iteration 31/1000 | Loss: 0.00349402
Iteration 32/1000 | Loss: 0.00388007
Iteration 33/1000 | Loss: 0.00477993
Iteration 34/1000 | Loss: 0.00284702
Iteration 35/1000 | Loss: 0.00272518
Iteration 36/1000 | Loss: 0.00343843
Iteration 37/1000 | Loss: 0.00312175
Iteration 38/1000 | Loss: 0.00549535
Iteration 39/1000 | Loss: 0.00346941
Iteration 40/1000 | Loss: 0.00194958
Iteration 41/1000 | Loss: 0.00427599
Iteration 42/1000 | Loss: 0.00225061
Iteration 43/1000 | Loss: 0.00145191
Iteration 44/1000 | Loss: 0.00356519
Iteration 45/1000 | Loss: 0.00107622
Iteration 46/1000 | Loss: 0.00086957
Iteration 47/1000 | Loss: 0.00202278
Iteration 48/1000 | Loss: 0.00162301
Iteration 49/1000 | Loss: 0.00243808
Iteration 50/1000 | Loss: 0.00173331
Iteration 51/1000 | Loss: 0.00113288
Iteration 52/1000 | Loss: 0.00246212
Iteration 53/1000 | Loss: 0.00170974
Iteration 54/1000 | Loss: 0.00177495
Iteration 55/1000 | Loss: 0.00213422
Iteration 56/1000 | Loss: 0.00110721
Iteration 57/1000 | Loss: 0.00043673
Iteration 58/1000 | Loss: 0.00019311
Iteration 59/1000 | Loss: 0.00032204
Iteration 60/1000 | Loss: 0.00026275
Iteration 61/1000 | Loss: 0.00074290
Iteration 62/1000 | Loss: 0.00047250
Iteration 63/1000 | Loss: 0.00031081
Iteration 64/1000 | Loss: 0.00014005
Iteration 65/1000 | Loss: 0.00007401
Iteration 66/1000 | Loss: 0.00024953
Iteration 67/1000 | Loss: 0.00024513
Iteration 68/1000 | Loss: 0.00024340
Iteration 69/1000 | Loss: 0.00023098
Iteration 70/1000 | Loss: 0.00006499
Iteration 71/1000 | Loss: 0.00029413
Iteration 72/1000 | Loss: 0.00030624
Iteration 73/1000 | Loss: 0.00016509
Iteration 74/1000 | Loss: 0.00005047
Iteration 75/1000 | Loss: 0.00004477
Iteration 76/1000 | Loss: 0.00004087
Iteration 77/1000 | Loss: 0.00025764
Iteration 78/1000 | Loss: 0.00026366
Iteration 79/1000 | Loss: 0.00024842
Iteration 80/1000 | Loss: 0.00038410
Iteration 81/1000 | Loss: 0.00024244
Iteration 82/1000 | Loss: 0.00028494
Iteration 83/1000 | Loss: 0.00023360
Iteration 84/1000 | Loss: 0.00027451
Iteration 85/1000 | Loss: 0.00031552
Iteration 86/1000 | Loss: 0.00004660
Iteration 87/1000 | Loss: 0.00009030
Iteration 88/1000 | Loss: 0.00016426
Iteration 89/1000 | Loss: 0.00052205
Iteration 90/1000 | Loss: 0.00024814
Iteration 91/1000 | Loss: 0.00035684
Iteration 92/1000 | Loss: 0.00020855
Iteration 93/1000 | Loss: 0.00003591
Iteration 94/1000 | Loss: 0.00003295
Iteration 95/1000 | Loss: 0.00040671
Iteration 96/1000 | Loss: 0.00038880
Iteration 97/1000 | Loss: 0.00017433
Iteration 98/1000 | Loss: 0.00013707
Iteration 99/1000 | Loss: 0.00015321
Iteration 100/1000 | Loss: 0.00028929
Iteration 101/1000 | Loss: 0.00015590
Iteration 102/1000 | Loss: 0.00012102
Iteration 103/1000 | Loss: 0.00031515
Iteration 104/1000 | Loss: 0.00011471
Iteration 105/1000 | Loss: 0.00028441
Iteration 106/1000 | Loss: 0.00010287
Iteration 107/1000 | Loss: 0.00008266
Iteration 108/1000 | Loss: 0.00005032
Iteration 109/1000 | Loss: 0.00004437
Iteration 110/1000 | Loss: 0.00003386
Iteration 111/1000 | Loss: 0.00003105
Iteration 112/1000 | Loss: 0.00002924
Iteration 113/1000 | Loss: 0.00002787
Iteration 114/1000 | Loss: 0.00015222
Iteration 115/1000 | Loss: 0.00012367
Iteration 116/1000 | Loss: 0.00002885
Iteration 117/1000 | Loss: 0.00012311
Iteration 118/1000 | Loss: 0.00003194
Iteration 119/1000 | Loss: 0.00003469
Iteration 120/1000 | Loss: 0.00002681
Iteration 121/1000 | Loss: 0.00014153
Iteration 122/1000 | Loss: 0.00008475
Iteration 123/1000 | Loss: 0.00002956
Iteration 124/1000 | Loss: 0.00002649
Iteration 125/1000 | Loss: 0.00002544
Iteration 126/1000 | Loss: 0.00002475
Iteration 127/1000 | Loss: 0.00002444
Iteration 128/1000 | Loss: 0.00002424
Iteration 129/1000 | Loss: 0.00002405
Iteration 130/1000 | Loss: 0.00002380
Iteration 131/1000 | Loss: 0.00002379
Iteration 132/1000 | Loss: 0.00002370
Iteration 133/1000 | Loss: 0.00002370
Iteration 134/1000 | Loss: 0.00002368
Iteration 135/1000 | Loss: 0.00060166
Iteration 136/1000 | Loss: 0.00003462
Iteration 137/1000 | Loss: 0.00055470
Iteration 138/1000 | Loss: 0.00002889
Iteration 139/1000 | Loss: 0.00002508
Iteration 140/1000 | Loss: 0.00002367
Iteration 141/1000 | Loss: 0.00002207
Iteration 142/1000 | Loss: 0.00002101
Iteration 143/1000 | Loss: 0.00002047
Iteration 144/1000 | Loss: 0.00002019
Iteration 145/1000 | Loss: 0.00001996
Iteration 146/1000 | Loss: 0.00001996
Iteration 147/1000 | Loss: 0.00001981
Iteration 148/1000 | Loss: 0.00001974
Iteration 149/1000 | Loss: 0.00001970
Iteration 150/1000 | Loss: 0.00001963
Iteration 151/1000 | Loss: 0.00001962
Iteration 152/1000 | Loss: 0.00001962
Iteration 153/1000 | Loss: 0.00001956
Iteration 154/1000 | Loss: 0.00001955
Iteration 155/1000 | Loss: 0.00001954
Iteration 156/1000 | Loss: 0.00001953
Iteration 157/1000 | Loss: 0.00001953
Iteration 158/1000 | Loss: 0.00001952
Iteration 159/1000 | Loss: 0.00001952
Iteration 160/1000 | Loss: 0.00001951
Iteration 161/1000 | Loss: 0.00001950
Iteration 162/1000 | Loss: 0.00001950
Iteration 163/1000 | Loss: 0.00001950
Iteration 164/1000 | Loss: 0.00001949
Iteration 165/1000 | Loss: 0.00001949
Iteration 166/1000 | Loss: 0.00001948
Iteration 167/1000 | Loss: 0.00001947
Iteration 168/1000 | Loss: 0.00001947
Iteration 169/1000 | Loss: 0.00001945
Iteration 170/1000 | Loss: 0.00001944
Iteration 171/1000 | Loss: 0.00001944
Iteration 172/1000 | Loss: 0.00001944
Iteration 173/1000 | Loss: 0.00001943
Iteration 174/1000 | Loss: 0.00001943
Iteration 175/1000 | Loss: 0.00001942
Iteration 176/1000 | Loss: 0.00001942
Iteration 177/1000 | Loss: 0.00001942
Iteration 178/1000 | Loss: 0.00001942
Iteration 179/1000 | Loss: 0.00001942
Iteration 180/1000 | Loss: 0.00001942
Iteration 181/1000 | Loss: 0.00001942
Iteration 182/1000 | Loss: 0.00001941
Iteration 183/1000 | Loss: 0.00001941
Iteration 184/1000 | Loss: 0.00001941
Iteration 185/1000 | Loss: 0.00001941
Iteration 186/1000 | Loss: 0.00001941
Iteration 187/1000 | Loss: 0.00001941
Iteration 188/1000 | Loss: 0.00001940
Iteration 189/1000 | Loss: 0.00001940
Iteration 190/1000 | Loss: 0.00001939
Iteration 191/1000 | Loss: 0.00001939
Iteration 192/1000 | Loss: 0.00001939
Iteration 193/1000 | Loss: 0.00001939
Iteration 194/1000 | Loss: 0.00001939
Iteration 195/1000 | Loss: 0.00001938
Iteration 196/1000 | Loss: 0.00001938
Iteration 197/1000 | Loss: 0.00001938
Iteration 198/1000 | Loss: 0.00001938
Iteration 199/1000 | Loss: 0.00001937
Iteration 200/1000 | Loss: 0.00001937
Iteration 201/1000 | Loss: 0.00001937
Iteration 202/1000 | Loss: 0.00001937
Iteration 203/1000 | Loss: 0.00001936
Iteration 204/1000 | Loss: 0.00001936
Iteration 205/1000 | Loss: 0.00001936
Iteration 206/1000 | Loss: 0.00001936
Iteration 207/1000 | Loss: 0.00001936
Iteration 208/1000 | Loss: 0.00001936
Iteration 209/1000 | Loss: 0.00001935
Iteration 210/1000 | Loss: 0.00001935
Iteration 211/1000 | Loss: 0.00001935
Iteration 212/1000 | Loss: 0.00001935
Iteration 213/1000 | Loss: 0.00001935
Iteration 214/1000 | Loss: 0.00001935
Iteration 215/1000 | Loss: 0.00001935
Iteration 216/1000 | Loss: 0.00001935
Iteration 217/1000 | Loss: 0.00001935
Iteration 218/1000 | Loss: 0.00001935
Iteration 219/1000 | Loss: 0.00001935
Iteration 220/1000 | Loss: 0.00001935
Iteration 221/1000 | Loss: 0.00001934
Iteration 222/1000 | Loss: 0.00001934
Iteration 223/1000 | Loss: 0.00001934
Iteration 224/1000 | Loss: 0.00001934
Iteration 225/1000 | Loss: 0.00001934
Iteration 226/1000 | Loss: 0.00001934
Iteration 227/1000 | Loss: 0.00001934
Iteration 228/1000 | Loss: 0.00001934
Iteration 229/1000 | Loss: 0.00001934
Iteration 230/1000 | Loss: 0.00001934
Iteration 231/1000 | Loss: 0.00001934
Iteration 232/1000 | Loss: 0.00001934
Iteration 233/1000 | Loss: 0.00001934
Iteration 234/1000 | Loss: 0.00001934
Iteration 235/1000 | Loss: 0.00001934
Iteration 236/1000 | Loss: 0.00001934
Iteration 237/1000 | Loss: 0.00001933
Iteration 238/1000 | Loss: 0.00001933
Iteration 239/1000 | Loss: 0.00001933
Iteration 240/1000 | Loss: 0.00001933
Iteration 241/1000 | Loss: 0.00001933
Iteration 242/1000 | Loss: 0.00001933
Iteration 243/1000 | Loss: 0.00001933
Iteration 244/1000 | Loss: 0.00001933
Iteration 245/1000 | Loss: 0.00001933
Iteration 246/1000 | Loss: 0.00001933
Iteration 247/1000 | Loss: 0.00001933
Iteration 248/1000 | Loss: 0.00001933
Iteration 249/1000 | Loss: 0.00001933
Iteration 250/1000 | Loss: 0.00001933
Iteration 251/1000 | Loss: 0.00001933
Iteration 252/1000 | Loss: 0.00001933
Iteration 253/1000 | Loss: 0.00001933
Iteration 254/1000 | Loss: 0.00001933
Iteration 255/1000 | Loss: 0.00001933
Iteration 256/1000 | Loss: 0.00001933
Iteration 257/1000 | Loss: 0.00001933
Iteration 258/1000 | Loss: 0.00001933
Iteration 259/1000 | Loss: 0.00001933
Iteration 260/1000 | Loss: 0.00001932
Iteration 261/1000 | Loss: 0.00001932
Iteration 262/1000 | Loss: 0.00001932
Iteration 263/1000 | Loss: 0.00001932
Iteration 264/1000 | Loss: 0.00001932
Iteration 265/1000 | Loss: 0.00001932
Iteration 266/1000 | Loss: 0.00001932
Iteration 267/1000 | Loss: 0.00001932
Iteration 268/1000 | Loss: 0.00001932
Iteration 269/1000 | Loss: 0.00001932
Iteration 270/1000 | Loss: 0.00001932
Iteration 271/1000 | Loss: 0.00001932
Iteration 272/1000 | Loss: 0.00001932
Iteration 273/1000 | Loss: 0.00001932
Iteration 274/1000 | Loss: 0.00001932
Iteration 275/1000 | Loss: 0.00001932
Iteration 276/1000 | Loss: 0.00001932
Iteration 277/1000 | Loss: 0.00001932
Iteration 278/1000 | Loss: 0.00001932
Iteration 279/1000 | Loss: 0.00001931
Iteration 280/1000 | Loss: 0.00001931
Iteration 281/1000 | Loss: 0.00001931
Iteration 282/1000 | Loss: 0.00001931
Iteration 283/1000 | Loss: 0.00001931
Iteration 284/1000 | Loss: 0.00001931
Iteration 285/1000 | Loss: 0.00001931
Iteration 286/1000 | Loss: 0.00001931
Iteration 287/1000 | Loss: 0.00001931
Iteration 288/1000 | Loss: 0.00001931
Iteration 289/1000 | Loss: 0.00001931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 289. Stopping optimization.
Last 5 losses: [1.931323276949115e-05, 1.931323276949115e-05, 1.931323276949115e-05, 1.931323276949115e-05, 1.931323276949115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.931323276949115e-05

Optimization complete. Final v2v error: 3.6993672847747803 mm

Highest mean error: 4.3900275230407715 mm for frame 11

Lowest mean error: 3.429779291152954 mm for frame 124

Saving results

Total time: 269.07386922836304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384845
Iteration 2/25 | Loss: 0.00123212
Iteration 3/25 | Loss: 0.00117269
Iteration 4/25 | Loss: 0.00116610
Iteration 5/25 | Loss: 0.00116456
Iteration 6/25 | Loss: 0.00116456
Iteration 7/25 | Loss: 0.00116456
Iteration 8/25 | Loss: 0.00116456
Iteration 9/25 | Loss: 0.00116456
Iteration 10/25 | Loss: 0.00116456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011645633494481444, 0.0011645633494481444, 0.0011645633494481444, 0.0011645633494481444, 0.0011645633494481444]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011645633494481444

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.79562974
Iteration 2/25 | Loss: 0.00092536
Iteration 3/25 | Loss: 0.00092536
Iteration 4/25 | Loss: 0.00092536
Iteration 5/25 | Loss: 0.00092536
Iteration 6/25 | Loss: 0.00092536
Iteration 7/25 | Loss: 0.00092536
Iteration 8/25 | Loss: 0.00092536
Iteration 9/25 | Loss: 0.00092536
Iteration 10/25 | Loss: 0.00092535
Iteration 11/25 | Loss: 0.00092535
Iteration 12/25 | Loss: 0.00092535
Iteration 13/25 | Loss: 0.00092535
Iteration 14/25 | Loss: 0.00092535
Iteration 15/25 | Loss: 0.00092535
Iteration 16/25 | Loss: 0.00092535
Iteration 17/25 | Loss: 0.00092535
Iteration 18/25 | Loss: 0.00092535
Iteration 19/25 | Loss: 0.00092535
Iteration 20/25 | Loss: 0.00092535
Iteration 21/25 | Loss: 0.00092535
Iteration 22/25 | Loss: 0.00092535
Iteration 23/25 | Loss: 0.00092535
Iteration 24/25 | Loss: 0.00092535
Iteration 25/25 | Loss: 0.00092535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092535
Iteration 2/1000 | Loss: 0.00001864
Iteration 3/1000 | Loss: 0.00001380
Iteration 4/1000 | Loss: 0.00001233
Iteration 5/1000 | Loss: 0.00001164
Iteration 6/1000 | Loss: 0.00001112
Iteration 7/1000 | Loss: 0.00001077
Iteration 8/1000 | Loss: 0.00001051
Iteration 9/1000 | Loss: 0.00001026
Iteration 10/1000 | Loss: 0.00001020
Iteration 11/1000 | Loss: 0.00001017
Iteration 12/1000 | Loss: 0.00001008
Iteration 13/1000 | Loss: 0.00000999
Iteration 14/1000 | Loss: 0.00000996
Iteration 15/1000 | Loss: 0.00000995
Iteration 16/1000 | Loss: 0.00000994
Iteration 17/1000 | Loss: 0.00000993
Iteration 18/1000 | Loss: 0.00000992
Iteration 19/1000 | Loss: 0.00000992
Iteration 20/1000 | Loss: 0.00000985
Iteration 21/1000 | Loss: 0.00000984
Iteration 22/1000 | Loss: 0.00000984
Iteration 23/1000 | Loss: 0.00000983
Iteration 24/1000 | Loss: 0.00000982
Iteration 25/1000 | Loss: 0.00000980
Iteration 26/1000 | Loss: 0.00000979
Iteration 27/1000 | Loss: 0.00000979
Iteration 28/1000 | Loss: 0.00000978
Iteration 29/1000 | Loss: 0.00000977
Iteration 30/1000 | Loss: 0.00000977
Iteration 31/1000 | Loss: 0.00000977
Iteration 32/1000 | Loss: 0.00000976
Iteration 33/1000 | Loss: 0.00000975
Iteration 34/1000 | Loss: 0.00000972
Iteration 35/1000 | Loss: 0.00000972
Iteration 36/1000 | Loss: 0.00000972
Iteration 37/1000 | Loss: 0.00000972
Iteration 38/1000 | Loss: 0.00000972
Iteration 39/1000 | Loss: 0.00000972
Iteration 40/1000 | Loss: 0.00000972
Iteration 41/1000 | Loss: 0.00000971
Iteration 42/1000 | Loss: 0.00000971
Iteration 43/1000 | Loss: 0.00000971
Iteration 44/1000 | Loss: 0.00000971
Iteration 45/1000 | Loss: 0.00000971
Iteration 46/1000 | Loss: 0.00000971
Iteration 47/1000 | Loss: 0.00000968
Iteration 48/1000 | Loss: 0.00000967
Iteration 49/1000 | Loss: 0.00000966
Iteration 50/1000 | Loss: 0.00000965
Iteration 51/1000 | Loss: 0.00000962
Iteration 52/1000 | Loss: 0.00000961
Iteration 53/1000 | Loss: 0.00000960
Iteration 54/1000 | Loss: 0.00000960
Iteration 55/1000 | Loss: 0.00000959
Iteration 56/1000 | Loss: 0.00000958
Iteration 57/1000 | Loss: 0.00000958
Iteration 58/1000 | Loss: 0.00000958
Iteration 59/1000 | Loss: 0.00000957
Iteration 60/1000 | Loss: 0.00000957
Iteration 61/1000 | Loss: 0.00000956
Iteration 62/1000 | Loss: 0.00000956
Iteration 63/1000 | Loss: 0.00000955
Iteration 64/1000 | Loss: 0.00000955
Iteration 65/1000 | Loss: 0.00000955
Iteration 66/1000 | Loss: 0.00000954
Iteration 67/1000 | Loss: 0.00000953
Iteration 68/1000 | Loss: 0.00000953
Iteration 69/1000 | Loss: 0.00000952
Iteration 70/1000 | Loss: 0.00000952
Iteration 71/1000 | Loss: 0.00000951
Iteration 72/1000 | Loss: 0.00000951
Iteration 73/1000 | Loss: 0.00000951
Iteration 74/1000 | Loss: 0.00000951
Iteration 75/1000 | Loss: 0.00000951
Iteration 76/1000 | Loss: 0.00000950
Iteration 77/1000 | Loss: 0.00000950
Iteration 78/1000 | Loss: 0.00000949
Iteration 79/1000 | Loss: 0.00000949
Iteration 80/1000 | Loss: 0.00000949
Iteration 81/1000 | Loss: 0.00000949
Iteration 82/1000 | Loss: 0.00000949
Iteration 83/1000 | Loss: 0.00000949
Iteration 84/1000 | Loss: 0.00000949
Iteration 85/1000 | Loss: 0.00000949
Iteration 86/1000 | Loss: 0.00000948
Iteration 87/1000 | Loss: 0.00000948
Iteration 88/1000 | Loss: 0.00000948
Iteration 89/1000 | Loss: 0.00000948
Iteration 90/1000 | Loss: 0.00000948
Iteration 91/1000 | Loss: 0.00000947
Iteration 92/1000 | Loss: 0.00000947
Iteration 93/1000 | Loss: 0.00000947
Iteration 94/1000 | Loss: 0.00000947
Iteration 95/1000 | Loss: 0.00000947
Iteration 96/1000 | Loss: 0.00000946
Iteration 97/1000 | Loss: 0.00000946
Iteration 98/1000 | Loss: 0.00000946
Iteration 99/1000 | Loss: 0.00000946
Iteration 100/1000 | Loss: 0.00000946
Iteration 101/1000 | Loss: 0.00000946
Iteration 102/1000 | Loss: 0.00000946
Iteration 103/1000 | Loss: 0.00000946
Iteration 104/1000 | Loss: 0.00000945
Iteration 105/1000 | Loss: 0.00000945
Iteration 106/1000 | Loss: 0.00000945
Iteration 107/1000 | Loss: 0.00000945
Iteration 108/1000 | Loss: 0.00000944
Iteration 109/1000 | Loss: 0.00000944
Iteration 110/1000 | Loss: 0.00000944
Iteration 111/1000 | Loss: 0.00000944
Iteration 112/1000 | Loss: 0.00000944
Iteration 113/1000 | Loss: 0.00000944
Iteration 114/1000 | Loss: 0.00000944
Iteration 115/1000 | Loss: 0.00000943
Iteration 116/1000 | Loss: 0.00000943
Iteration 117/1000 | Loss: 0.00000943
Iteration 118/1000 | Loss: 0.00000942
Iteration 119/1000 | Loss: 0.00000942
Iteration 120/1000 | Loss: 0.00000942
Iteration 121/1000 | Loss: 0.00000942
Iteration 122/1000 | Loss: 0.00000942
Iteration 123/1000 | Loss: 0.00000942
Iteration 124/1000 | Loss: 0.00000941
Iteration 125/1000 | Loss: 0.00000941
Iteration 126/1000 | Loss: 0.00000941
Iteration 127/1000 | Loss: 0.00000940
Iteration 128/1000 | Loss: 0.00000940
Iteration 129/1000 | Loss: 0.00000940
Iteration 130/1000 | Loss: 0.00000940
Iteration 131/1000 | Loss: 0.00000940
Iteration 132/1000 | Loss: 0.00000940
Iteration 133/1000 | Loss: 0.00000940
Iteration 134/1000 | Loss: 0.00000940
Iteration 135/1000 | Loss: 0.00000940
Iteration 136/1000 | Loss: 0.00000940
Iteration 137/1000 | Loss: 0.00000940
Iteration 138/1000 | Loss: 0.00000940
Iteration 139/1000 | Loss: 0.00000940
Iteration 140/1000 | Loss: 0.00000940
Iteration 141/1000 | Loss: 0.00000939
Iteration 142/1000 | Loss: 0.00000939
Iteration 143/1000 | Loss: 0.00000939
Iteration 144/1000 | Loss: 0.00000939
Iteration 145/1000 | Loss: 0.00000939
Iteration 146/1000 | Loss: 0.00000939
Iteration 147/1000 | Loss: 0.00000939
Iteration 148/1000 | Loss: 0.00000939
Iteration 149/1000 | Loss: 0.00000938
Iteration 150/1000 | Loss: 0.00000938
Iteration 151/1000 | Loss: 0.00000938
Iteration 152/1000 | Loss: 0.00000938
Iteration 153/1000 | Loss: 0.00000938
Iteration 154/1000 | Loss: 0.00000938
Iteration 155/1000 | Loss: 0.00000938
Iteration 156/1000 | Loss: 0.00000938
Iteration 157/1000 | Loss: 0.00000938
Iteration 158/1000 | Loss: 0.00000938
Iteration 159/1000 | Loss: 0.00000938
Iteration 160/1000 | Loss: 0.00000938
Iteration 161/1000 | Loss: 0.00000938
Iteration 162/1000 | Loss: 0.00000938
Iteration 163/1000 | Loss: 0.00000938
Iteration 164/1000 | Loss: 0.00000937
Iteration 165/1000 | Loss: 0.00000937
Iteration 166/1000 | Loss: 0.00000937
Iteration 167/1000 | Loss: 0.00000937
Iteration 168/1000 | Loss: 0.00000937
Iteration 169/1000 | Loss: 0.00000937
Iteration 170/1000 | Loss: 0.00000937
Iteration 171/1000 | Loss: 0.00000937
Iteration 172/1000 | Loss: 0.00000937
Iteration 173/1000 | Loss: 0.00000937
Iteration 174/1000 | Loss: 0.00000937
Iteration 175/1000 | Loss: 0.00000937
Iteration 176/1000 | Loss: 0.00000937
Iteration 177/1000 | Loss: 0.00000937
Iteration 178/1000 | Loss: 0.00000937
Iteration 179/1000 | Loss: 0.00000937
Iteration 180/1000 | Loss: 0.00000937
Iteration 181/1000 | Loss: 0.00000937
Iteration 182/1000 | Loss: 0.00000937
Iteration 183/1000 | Loss: 0.00000937
Iteration 184/1000 | Loss: 0.00000937
Iteration 185/1000 | Loss: 0.00000937
Iteration 186/1000 | Loss: 0.00000937
Iteration 187/1000 | Loss: 0.00000937
Iteration 188/1000 | Loss: 0.00000937
Iteration 189/1000 | Loss: 0.00000937
Iteration 190/1000 | Loss: 0.00000937
Iteration 191/1000 | Loss: 0.00000937
Iteration 192/1000 | Loss: 0.00000937
Iteration 193/1000 | Loss: 0.00000937
Iteration 194/1000 | Loss: 0.00000937
Iteration 195/1000 | Loss: 0.00000937
Iteration 196/1000 | Loss: 0.00000937
Iteration 197/1000 | Loss: 0.00000937
Iteration 198/1000 | Loss: 0.00000937
Iteration 199/1000 | Loss: 0.00000937
Iteration 200/1000 | Loss: 0.00000937
Iteration 201/1000 | Loss: 0.00000937
Iteration 202/1000 | Loss: 0.00000937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [9.366600352223031e-06, 9.366600352223031e-06, 9.366600352223031e-06, 9.366600352223031e-06, 9.366600352223031e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.366600352223031e-06

Optimization complete. Final v2v error: 2.6614787578582764 mm

Highest mean error: 2.82989239692688 mm for frame 131

Lowest mean error: 2.5859031677246094 mm for frame 25

Saving results

Total time: 38.541205167770386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996864
Iteration 2/25 | Loss: 0.00329413
Iteration 3/25 | Loss: 0.00237584
Iteration 4/25 | Loss: 0.00190430
Iteration 5/25 | Loss: 0.00160700
Iteration 6/25 | Loss: 0.00150155
Iteration 7/25 | Loss: 0.00144160
Iteration 8/25 | Loss: 0.00138542
Iteration 9/25 | Loss: 0.00135126
Iteration 10/25 | Loss: 0.00133687
Iteration 11/25 | Loss: 0.00131938
Iteration 12/25 | Loss: 0.00131464
Iteration 13/25 | Loss: 0.00130958
Iteration 14/25 | Loss: 0.00130907
Iteration 15/25 | Loss: 0.00130885
Iteration 16/25 | Loss: 0.00130866
Iteration 17/25 | Loss: 0.00130835
Iteration 18/25 | Loss: 0.00130785
Iteration 19/25 | Loss: 0.00130761
Iteration 20/25 | Loss: 0.00130758
Iteration 21/25 | Loss: 0.00130758
Iteration 22/25 | Loss: 0.00130758
Iteration 23/25 | Loss: 0.00130757
Iteration 24/25 | Loss: 0.00130757
Iteration 25/25 | Loss: 0.00130757

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32182741
Iteration 2/25 | Loss: 0.00115079
Iteration 3/25 | Loss: 0.00115078
Iteration 4/25 | Loss: 0.00115078
Iteration 5/25 | Loss: 0.00115078
Iteration 6/25 | Loss: 0.00115078
Iteration 7/25 | Loss: 0.00115078
Iteration 8/25 | Loss: 0.00115078
Iteration 9/25 | Loss: 0.00115078
Iteration 10/25 | Loss: 0.00115078
Iteration 11/25 | Loss: 0.00115078
Iteration 12/25 | Loss: 0.00115078
Iteration 13/25 | Loss: 0.00115078
Iteration 14/25 | Loss: 0.00115078
Iteration 15/25 | Loss: 0.00115078
Iteration 16/25 | Loss: 0.00115078
Iteration 17/25 | Loss: 0.00115078
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001150782103650272, 0.001150782103650272, 0.001150782103650272, 0.001150782103650272, 0.001150782103650272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001150782103650272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115078
Iteration 2/1000 | Loss: 0.00006358
Iteration 3/1000 | Loss: 0.00003689
Iteration 4/1000 | Loss: 0.00003315
Iteration 5/1000 | Loss: 0.00003025
Iteration 6/1000 | Loss: 0.00002901
Iteration 7/1000 | Loss: 0.00002786
Iteration 8/1000 | Loss: 0.00002724
Iteration 9/1000 | Loss: 0.00007209
Iteration 10/1000 | Loss: 0.00002960
Iteration 11/1000 | Loss: 0.00002797
Iteration 12/1000 | Loss: 0.00002722
Iteration 13/1000 | Loss: 0.00002671
Iteration 14/1000 | Loss: 0.00002635
Iteration 15/1000 | Loss: 0.00002604
Iteration 16/1000 | Loss: 0.00002570
Iteration 17/1000 | Loss: 0.00002541
Iteration 18/1000 | Loss: 0.00002517
Iteration 19/1000 | Loss: 0.00002506
Iteration 20/1000 | Loss: 0.00002505
Iteration 21/1000 | Loss: 0.00002504
Iteration 22/1000 | Loss: 0.00002495
Iteration 23/1000 | Loss: 0.00002490
Iteration 24/1000 | Loss: 0.00002484
Iteration 25/1000 | Loss: 0.00002483
Iteration 26/1000 | Loss: 0.00002482
Iteration 27/1000 | Loss: 0.00002481
Iteration 28/1000 | Loss: 0.00002480
Iteration 29/1000 | Loss: 0.00002480
Iteration 30/1000 | Loss: 0.00002478
Iteration 31/1000 | Loss: 0.00002477
Iteration 32/1000 | Loss: 0.00002476
Iteration 33/1000 | Loss: 0.00002476
Iteration 34/1000 | Loss: 0.00002475
Iteration 35/1000 | Loss: 0.00002468
Iteration 36/1000 | Loss: 0.00002465
Iteration 37/1000 | Loss: 0.00002464
Iteration 38/1000 | Loss: 0.00002464
Iteration 39/1000 | Loss: 0.00002463
Iteration 40/1000 | Loss: 0.00002461
Iteration 41/1000 | Loss: 0.00002461
Iteration 42/1000 | Loss: 0.00002461
Iteration 43/1000 | Loss: 0.00002461
Iteration 44/1000 | Loss: 0.00002461
Iteration 45/1000 | Loss: 0.00002461
Iteration 46/1000 | Loss: 0.00002460
Iteration 47/1000 | Loss: 0.00002460
Iteration 48/1000 | Loss: 0.00002460
Iteration 49/1000 | Loss: 0.00002460
Iteration 50/1000 | Loss: 0.00002460
Iteration 51/1000 | Loss: 0.00002460
Iteration 52/1000 | Loss: 0.00002460
Iteration 53/1000 | Loss: 0.00002460
Iteration 54/1000 | Loss: 0.00002460
Iteration 55/1000 | Loss: 0.00002460
Iteration 56/1000 | Loss: 0.00002460
Iteration 57/1000 | Loss: 0.00002459
Iteration 58/1000 | Loss: 0.00002458
Iteration 59/1000 | Loss: 0.00002458
Iteration 60/1000 | Loss: 0.00002458
Iteration 61/1000 | Loss: 0.00002457
Iteration 62/1000 | Loss: 0.00002457
Iteration 63/1000 | Loss: 0.00002457
Iteration 64/1000 | Loss: 0.00002457
Iteration 65/1000 | Loss: 0.00002456
Iteration 66/1000 | Loss: 0.00002456
Iteration 67/1000 | Loss: 0.00002456
Iteration 68/1000 | Loss: 0.00002456
Iteration 69/1000 | Loss: 0.00002456
Iteration 70/1000 | Loss: 0.00002456
Iteration 71/1000 | Loss: 0.00002456
Iteration 72/1000 | Loss: 0.00002456
Iteration 73/1000 | Loss: 0.00002455
Iteration 74/1000 | Loss: 0.00002455
Iteration 75/1000 | Loss: 0.00002455
Iteration 76/1000 | Loss: 0.00002455
Iteration 77/1000 | Loss: 0.00002454
Iteration 78/1000 | Loss: 0.00002454
Iteration 79/1000 | Loss: 0.00002454
Iteration 80/1000 | Loss: 0.00002453
Iteration 81/1000 | Loss: 0.00002453
Iteration 82/1000 | Loss: 0.00002453
Iteration 83/1000 | Loss: 0.00002453
Iteration 84/1000 | Loss: 0.00002452
Iteration 85/1000 | Loss: 0.00002452
Iteration 86/1000 | Loss: 0.00002452
Iteration 87/1000 | Loss: 0.00002451
Iteration 88/1000 | Loss: 0.00002451
Iteration 89/1000 | Loss: 0.00002450
Iteration 90/1000 | Loss: 0.00002450
Iteration 91/1000 | Loss: 0.00002450
Iteration 92/1000 | Loss: 0.00002450
Iteration 93/1000 | Loss: 0.00002449
Iteration 94/1000 | Loss: 0.00002449
Iteration 95/1000 | Loss: 0.00002449
Iteration 96/1000 | Loss: 0.00002448
Iteration 97/1000 | Loss: 0.00002448
Iteration 98/1000 | Loss: 0.00002447
Iteration 99/1000 | Loss: 0.00002447
Iteration 100/1000 | Loss: 0.00002446
Iteration 101/1000 | Loss: 0.00002446
Iteration 102/1000 | Loss: 0.00002445
Iteration 103/1000 | Loss: 0.00002444
Iteration 104/1000 | Loss: 0.00002444
Iteration 105/1000 | Loss: 0.00002443
Iteration 106/1000 | Loss: 0.00002443
Iteration 107/1000 | Loss: 0.00002442
Iteration 108/1000 | Loss: 0.00002442
Iteration 109/1000 | Loss: 0.00002442
Iteration 110/1000 | Loss: 0.00002441
Iteration 111/1000 | Loss: 0.00002441
Iteration 112/1000 | Loss: 0.00002441
Iteration 113/1000 | Loss: 0.00002440
Iteration 114/1000 | Loss: 0.00002440
Iteration 115/1000 | Loss: 0.00002440
Iteration 116/1000 | Loss: 0.00002439
Iteration 117/1000 | Loss: 0.00002439
Iteration 118/1000 | Loss: 0.00002439
Iteration 119/1000 | Loss: 0.00002438
Iteration 120/1000 | Loss: 0.00002438
Iteration 121/1000 | Loss: 0.00002438
Iteration 122/1000 | Loss: 0.00002438
Iteration 123/1000 | Loss: 0.00002438
Iteration 124/1000 | Loss: 0.00002438
Iteration 125/1000 | Loss: 0.00002438
Iteration 126/1000 | Loss: 0.00002437
Iteration 127/1000 | Loss: 0.00002437
Iteration 128/1000 | Loss: 0.00002437
Iteration 129/1000 | Loss: 0.00002436
Iteration 130/1000 | Loss: 0.00002436
Iteration 131/1000 | Loss: 0.00002436
Iteration 132/1000 | Loss: 0.00002436
Iteration 133/1000 | Loss: 0.00002435
Iteration 134/1000 | Loss: 0.00002435
Iteration 135/1000 | Loss: 0.00002435
Iteration 136/1000 | Loss: 0.00002434
Iteration 137/1000 | Loss: 0.00002434
Iteration 138/1000 | Loss: 0.00002434
Iteration 139/1000 | Loss: 0.00002434
Iteration 140/1000 | Loss: 0.00002433
Iteration 141/1000 | Loss: 0.00002433
Iteration 142/1000 | Loss: 0.00002433
Iteration 143/1000 | Loss: 0.00002433
Iteration 144/1000 | Loss: 0.00002433
Iteration 145/1000 | Loss: 0.00002432
Iteration 146/1000 | Loss: 0.00002432
Iteration 147/1000 | Loss: 0.00002432
Iteration 148/1000 | Loss: 0.00002432
Iteration 149/1000 | Loss: 0.00002432
Iteration 150/1000 | Loss: 0.00002432
Iteration 151/1000 | Loss: 0.00002432
Iteration 152/1000 | Loss: 0.00002432
Iteration 153/1000 | Loss: 0.00002432
Iteration 154/1000 | Loss: 0.00002432
Iteration 155/1000 | Loss: 0.00002431
Iteration 156/1000 | Loss: 0.00002431
Iteration 157/1000 | Loss: 0.00002431
Iteration 158/1000 | Loss: 0.00002431
Iteration 159/1000 | Loss: 0.00002431
Iteration 160/1000 | Loss: 0.00002431
Iteration 161/1000 | Loss: 0.00002431
Iteration 162/1000 | Loss: 0.00002431
Iteration 163/1000 | Loss: 0.00002431
Iteration 164/1000 | Loss: 0.00002430
Iteration 165/1000 | Loss: 0.00002430
Iteration 166/1000 | Loss: 0.00002430
Iteration 167/1000 | Loss: 0.00002430
Iteration 168/1000 | Loss: 0.00002430
Iteration 169/1000 | Loss: 0.00002429
Iteration 170/1000 | Loss: 0.00002429
Iteration 171/1000 | Loss: 0.00002429
Iteration 172/1000 | Loss: 0.00002429
Iteration 173/1000 | Loss: 0.00002429
Iteration 174/1000 | Loss: 0.00002429
Iteration 175/1000 | Loss: 0.00002429
Iteration 176/1000 | Loss: 0.00002429
Iteration 177/1000 | Loss: 0.00002429
Iteration 178/1000 | Loss: 0.00002429
Iteration 179/1000 | Loss: 0.00002429
Iteration 180/1000 | Loss: 0.00002429
Iteration 181/1000 | Loss: 0.00002428
Iteration 182/1000 | Loss: 0.00002428
Iteration 183/1000 | Loss: 0.00002428
Iteration 184/1000 | Loss: 0.00002428
Iteration 185/1000 | Loss: 0.00002428
Iteration 186/1000 | Loss: 0.00002428
Iteration 187/1000 | Loss: 0.00002427
Iteration 188/1000 | Loss: 0.00002427
Iteration 189/1000 | Loss: 0.00002427
Iteration 190/1000 | Loss: 0.00002427
Iteration 191/1000 | Loss: 0.00002427
Iteration 192/1000 | Loss: 0.00002427
Iteration 193/1000 | Loss: 0.00002427
Iteration 194/1000 | Loss: 0.00002427
Iteration 195/1000 | Loss: 0.00002427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [2.4273373128380626e-05, 2.4273373128380626e-05, 2.4273373128380626e-05, 2.4273373128380626e-05, 2.4273373128380626e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4273373128380626e-05

Optimization complete. Final v2v error: 4.230171203613281 mm

Highest mean error: 5.006450653076172 mm for frame 106

Lowest mean error: 3.788724660873413 mm for frame 142

Saving results

Total time: 89.41387486457825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405748
Iteration 2/25 | Loss: 0.00129986
Iteration 3/25 | Loss: 0.00123846
Iteration 4/25 | Loss: 0.00122979
Iteration 5/25 | Loss: 0.00122708
Iteration 6/25 | Loss: 0.00122635
Iteration 7/25 | Loss: 0.00122625
Iteration 8/25 | Loss: 0.00122625
Iteration 9/25 | Loss: 0.00122625
Iteration 10/25 | Loss: 0.00122625
Iteration 11/25 | Loss: 0.00122625
Iteration 12/25 | Loss: 0.00122625
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012262495001778007, 0.0012262495001778007, 0.0012262495001778007, 0.0012262495001778007, 0.0012262495001778007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012262495001778007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43534434
Iteration 2/25 | Loss: 0.00102359
Iteration 3/25 | Loss: 0.00102359
Iteration 4/25 | Loss: 0.00102359
Iteration 5/25 | Loss: 0.00102359
Iteration 6/25 | Loss: 0.00102359
Iteration 7/25 | Loss: 0.00102359
Iteration 8/25 | Loss: 0.00102359
Iteration 9/25 | Loss: 0.00102359
Iteration 10/25 | Loss: 0.00102359
Iteration 11/25 | Loss: 0.00102359
Iteration 12/25 | Loss: 0.00102359
Iteration 13/25 | Loss: 0.00102359
Iteration 14/25 | Loss: 0.00102359
Iteration 15/25 | Loss: 0.00102359
Iteration 16/25 | Loss: 0.00102359
Iteration 17/25 | Loss: 0.00102359
Iteration 18/25 | Loss: 0.00102359
Iteration 19/25 | Loss: 0.00102359
Iteration 20/25 | Loss: 0.00102359
Iteration 21/25 | Loss: 0.00102359
Iteration 22/25 | Loss: 0.00102359
Iteration 23/25 | Loss: 0.00102359
Iteration 24/25 | Loss: 0.00102359
Iteration 25/25 | Loss: 0.00102359

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102359
Iteration 2/1000 | Loss: 0.00003089
Iteration 3/1000 | Loss: 0.00002106
Iteration 4/1000 | Loss: 0.00001662
Iteration 5/1000 | Loss: 0.00001527
Iteration 6/1000 | Loss: 0.00001467
Iteration 7/1000 | Loss: 0.00001427
Iteration 8/1000 | Loss: 0.00001393
Iteration 9/1000 | Loss: 0.00001365
Iteration 10/1000 | Loss: 0.00001346
Iteration 11/1000 | Loss: 0.00001326
Iteration 12/1000 | Loss: 0.00001321
Iteration 13/1000 | Loss: 0.00001318
Iteration 14/1000 | Loss: 0.00001317
Iteration 15/1000 | Loss: 0.00001314
Iteration 16/1000 | Loss: 0.00001296
Iteration 17/1000 | Loss: 0.00001294
Iteration 18/1000 | Loss: 0.00001293
Iteration 19/1000 | Loss: 0.00001292
Iteration 20/1000 | Loss: 0.00001292
Iteration 21/1000 | Loss: 0.00001292
Iteration 22/1000 | Loss: 0.00001286
Iteration 23/1000 | Loss: 0.00001285
Iteration 24/1000 | Loss: 0.00001282
Iteration 25/1000 | Loss: 0.00001280
Iteration 26/1000 | Loss: 0.00001279
Iteration 27/1000 | Loss: 0.00001276
Iteration 28/1000 | Loss: 0.00001276
Iteration 29/1000 | Loss: 0.00001275
Iteration 30/1000 | Loss: 0.00001275
Iteration 31/1000 | Loss: 0.00001275
Iteration 32/1000 | Loss: 0.00001273
Iteration 33/1000 | Loss: 0.00001272
Iteration 34/1000 | Loss: 0.00001272
Iteration 35/1000 | Loss: 0.00001272
Iteration 36/1000 | Loss: 0.00001272
Iteration 37/1000 | Loss: 0.00001272
Iteration 38/1000 | Loss: 0.00001272
Iteration 39/1000 | Loss: 0.00001272
Iteration 40/1000 | Loss: 0.00001271
Iteration 41/1000 | Loss: 0.00001269
Iteration 42/1000 | Loss: 0.00001269
Iteration 43/1000 | Loss: 0.00001268
Iteration 44/1000 | Loss: 0.00001267
Iteration 45/1000 | Loss: 0.00001267
Iteration 46/1000 | Loss: 0.00001266
Iteration 47/1000 | Loss: 0.00001266
Iteration 48/1000 | Loss: 0.00001265
Iteration 49/1000 | Loss: 0.00001265
Iteration 50/1000 | Loss: 0.00001265
Iteration 51/1000 | Loss: 0.00001265
Iteration 52/1000 | Loss: 0.00001264
Iteration 53/1000 | Loss: 0.00001264
Iteration 54/1000 | Loss: 0.00001263
Iteration 55/1000 | Loss: 0.00001263
Iteration 56/1000 | Loss: 0.00001263
Iteration 57/1000 | Loss: 0.00001263
Iteration 58/1000 | Loss: 0.00001262
Iteration 59/1000 | Loss: 0.00001262
Iteration 60/1000 | Loss: 0.00001262
Iteration 61/1000 | Loss: 0.00001262
Iteration 62/1000 | Loss: 0.00001262
Iteration 63/1000 | Loss: 0.00001262
Iteration 64/1000 | Loss: 0.00001262
Iteration 65/1000 | Loss: 0.00001262
Iteration 66/1000 | Loss: 0.00001262
Iteration 67/1000 | Loss: 0.00001262
Iteration 68/1000 | Loss: 0.00001261
Iteration 69/1000 | Loss: 0.00001261
Iteration 70/1000 | Loss: 0.00001259
Iteration 71/1000 | Loss: 0.00001259
Iteration 72/1000 | Loss: 0.00001259
Iteration 73/1000 | Loss: 0.00001259
Iteration 74/1000 | Loss: 0.00001259
Iteration 75/1000 | Loss: 0.00001259
Iteration 76/1000 | Loss: 0.00001259
Iteration 77/1000 | Loss: 0.00001259
Iteration 78/1000 | Loss: 0.00001259
Iteration 79/1000 | Loss: 0.00001259
Iteration 80/1000 | Loss: 0.00001259
Iteration 81/1000 | Loss: 0.00001259
Iteration 82/1000 | Loss: 0.00001259
Iteration 83/1000 | Loss: 0.00001259
Iteration 84/1000 | Loss: 0.00001259
Iteration 85/1000 | Loss: 0.00001259
Iteration 86/1000 | Loss: 0.00001259
Iteration 87/1000 | Loss: 0.00001259
Iteration 88/1000 | Loss: 0.00001259
Iteration 89/1000 | Loss: 0.00001259
Iteration 90/1000 | Loss: 0.00001259
Iteration 91/1000 | Loss: 0.00001259
Iteration 92/1000 | Loss: 0.00001259
Iteration 93/1000 | Loss: 0.00001259
Iteration 94/1000 | Loss: 0.00001259
Iteration 95/1000 | Loss: 0.00001259
Iteration 96/1000 | Loss: 0.00001259
Iteration 97/1000 | Loss: 0.00001259
Iteration 98/1000 | Loss: 0.00001259
Iteration 99/1000 | Loss: 0.00001259
Iteration 100/1000 | Loss: 0.00001259
Iteration 101/1000 | Loss: 0.00001259
Iteration 102/1000 | Loss: 0.00001259
Iteration 103/1000 | Loss: 0.00001259
Iteration 104/1000 | Loss: 0.00001259
Iteration 105/1000 | Loss: 0.00001259
Iteration 106/1000 | Loss: 0.00001259
Iteration 107/1000 | Loss: 0.00001259
Iteration 108/1000 | Loss: 0.00001259
Iteration 109/1000 | Loss: 0.00001259
Iteration 110/1000 | Loss: 0.00001259
Iteration 111/1000 | Loss: 0.00001259
Iteration 112/1000 | Loss: 0.00001259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.258829070138745e-05, 1.258829070138745e-05, 1.258829070138745e-05, 1.258829070138745e-05, 1.258829070138745e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.258829070138745e-05

Optimization complete. Final v2v error: 3.0162501335144043 mm

Highest mean error: 3.8906137943267822 mm for frame 44

Lowest mean error: 2.7390520572662354 mm for frame 1

Saving results

Total time: 33.63045334815979
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843958
Iteration 2/25 | Loss: 0.00148220
Iteration 3/25 | Loss: 0.00127826
Iteration 4/25 | Loss: 0.00125692
Iteration 5/25 | Loss: 0.00125282
Iteration 6/25 | Loss: 0.00125243
Iteration 7/25 | Loss: 0.00125243
Iteration 8/25 | Loss: 0.00125243
Iteration 9/25 | Loss: 0.00125243
Iteration 10/25 | Loss: 0.00125243
Iteration 11/25 | Loss: 0.00125243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012524313060566783, 0.0012524313060566783, 0.0012524313060566783, 0.0012524313060566783, 0.0012524313060566783]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012524313060566783

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92491293
Iteration 2/25 | Loss: 0.00063471
Iteration 3/25 | Loss: 0.00063470
Iteration 4/25 | Loss: 0.00063470
Iteration 5/25 | Loss: 0.00063470
Iteration 6/25 | Loss: 0.00063470
Iteration 7/25 | Loss: 0.00063470
Iteration 8/25 | Loss: 0.00063470
Iteration 9/25 | Loss: 0.00063470
Iteration 10/25 | Loss: 0.00063470
Iteration 11/25 | Loss: 0.00063470
Iteration 12/25 | Loss: 0.00063470
Iteration 13/25 | Loss: 0.00063470
Iteration 14/25 | Loss: 0.00063470
Iteration 15/25 | Loss: 0.00063470
Iteration 16/25 | Loss: 0.00063470
Iteration 17/25 | Loss: 0.00063470
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006347022717818618, 0.0006347022717818618, 0.0006347022717818618, 0.0006347022717818618, 0.0006347022717818618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006347022717818618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063470
Iteration 2/1000 | Loss: 0.00003222
Iteration 3/1000 | Loss: 0.00002572
Iteration 4/1000 | Loss: 0.00002405
Iteration 5/1000 | Loss: 0.00002292
Iteration 6/1000 | Loss: 0.00002230
Iteration 7/1000 | Loss: 0.00002196
Iteration 8/1000 | Loss: 0.00002156
Iteration 9/1000 | Loss: 0.00002120
Iteration 10/1000 | Loss: 0.00002095
Iteration 11/1000 | Loss: 0.00002075
Iteration 12/1000 | Loss: 0.00002060
Iteration 13/1000 | Loss: 0.00002052
Iteration 14/1000 | Loss: 0.00002050
Iteration 15/1000 | Loss: 0.00002041
Iteration 16/1000 | Loss: 0.00002040
Iteration 17/1000 | Loss: 0.00002039
Iteration 18/1000 | Loss: 0.00002039
Iteration 19/1000 | Loss: 0.00002039
Iteration 20/1000 | Loss: 0.00002038
Iteration 21/1000 | Loss: 0.00002037
Iteration 22/1000 | Loss: 0.00002037
Iteration 23/1000 | Loss: 0.00002036
Iteration 24/1000 | Loss: 0.00002034
Iteration 25/1000 | Loss: 0.00002033
Iteration 26/1000 | Loss: 0.00002033
Iteration 27/1000 | Loss: 0.00002032
Iteration 28/1000 | Loss: 0.00002031
Iteration 29/1000 | Loss: 0.00002031
Iteration 30/1000 | Loss: 0.00002031
Iteration 31/1000 | Loss: 0.00002031
Iteration 32/1000 | Loss: 0.00002031
Iteration 33/1000 | Loss: 0.00002030
Iteration 34/1000 | Loss: 0.00002030
Iteration 35/1000 | Loss: 0.00002030
Iteration 36/1000 | Loss: 0.00002030
Iteration 37/1000 | Loss: 0.00002030
Iteration 38/1000 | Loss: 0.00002030
Iteration 39/1000 | Loss: 0.00002030
Iteration 40/1000 | Loss: 0.00002030
Iteration 41/1000 | Loss: 0.00002030
Iteration 42/1000 | Loss: 0.00002030
Iteration 43/1000 | Loss: 0.00002029
Iteration 44/1000 | Loss: 0.00002029
Iteration 45/1000 | Loss: 0.00002029
Iteration 46/1000 | Loss: 0.00002029
Iteration 47/1000 | Loss: 0.00002029
Iteration 48/1000 | Loss: 0.00002029
Iteration 49/1000 | Loss: 0.00002029
Iteration 50/1000 | Loss: 0.00002028
Iteration 51/1000 | Loss: 0.00002028
Iteration 52/1000 | Loss: 0.00002028
Iteration 53/1000 | Loss: 0.00002028
Iteration 54/1000 | Loss: 0.00002028
Iteration 55/1000 | Loss: 0.00002028
Iteration 56/1000 | Loss: 0.00002027
Iteration 57/1000 | Loss: 0.00002027
Iteration 58/1000 | Loss: 0.00002026
Iteration 59/1000 | Loss: 0.00002026
Iteration 60/1000 | Loss: 0.00002026
Iteration 61/1000 | Loss: 0.00002026
Iteration 62/1000 | Loss: 0.00002026
Iteration 63/1000 | Loss: 0.00002026
Iteration 64/1000 | Loss: 0.00002026
Iteration 65/1000 | Loss: 0.00002026
Iteration 66/1000 | Loss: 0.00002026
Iteration 67/1000 | Loss: 0.00002026
Iteration 68/1000 | Loss: 0.00002026
Iteration 69/1000 | Loss: 0.00002026
Iteration 70/1000 | Loss: 0.00002026
Iteration 71/1000 | Loss: 0.00002026
Iteration 72/1000 | Loss: 0.00002026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [2.0258879885659553e-05, 2.0258879885659553e-05, 2.0258879885659553e-05, 2.0258879885659553e-05, 2.0258879885659553e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0258879885659553e-05

Optimization complete. Final v2v error: 3.7771949768066406 mm

Highest mean error: 4.009224891662598 mm for frame 0

Lowest mean error: 3.672463893890381 mm for frame 157

Saving results

Total time: 35.24283266067505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507423
Iteration 2/25 | Loss: 0.00133844
Iteration 3/25 | Loss: 0.00124301
Iteration 4/25 | Loss: 0.00122538
Iteration 5/25 | Loss: 0.00122100
Iteration 6/25 | Loss: 0.00122039
Iteration 7/25 | Loss: 0.00122039
Iteration 8/25 | Loss: 0.00122039
Iteration 9/25 | Loss: 0.00122039
Iteration 10/25 | Loss: 0.00122039
Iteration 11/25 | Loss: 0.00122039
Iteration 12/25 | Loss: 0.00122039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012203933438286185, 0.0012203933438286185, 0.0012203933438286185, 0.0012203933438286185, 0.0012203933438286185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012203933438286185

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.17963028
Iteration 2/25 | Loss: 0.00098764
Iteration 3/25 | Loss: 0.00098762
Iteration 4/25 | Loss: 0.00098761
Iteration 5/25 | Loss: 0.00098761
Iteration 6/25 | Loss: 0.00098761
Iteration 7/25 | Loss: 0.00098761
Iteration 8/25 | Loss: 0.00098761
Iteration 9/25 | Loss: 0.00098761
Iteration 10/25 | Loss: 0.00098761
Iteration 11/25 | Loss: 0.00098761
Iteration 12/25 | Loss: 0.00098761
Iteration 13/25 | Loss: 0.00098761
Iteration 14/25 | Loss: 0.00098761
Iteration 15/25 | Loss: 0.00098761
Iteration 16/25 | Loss: 0.00098761
Iteration 17/25 | Loss: 0.00098761
Iteration 18/25 | Loss: 0.00098761
Iteration 19/25 | Loss: 0.00098761
Iteration 20/25 | Loss: 0.00098761
Iteration 21/25 | Loss: 0.00098761
Iteration 22/25 | Loss: 0.00098761
Iteration 23/25 | Loss: 0.00098761
Iteration 24/25 | Loss: 0.00098761
Iteration 25/25 | Loss: 0.00098761

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098761
Iteration 2/1000 | Loss: 0.00002936
Iteration 3/1000 | Loss: 0.00001985
Iteration 4/1000 | Loss: 0.00001784
Iteration 5/1000 | Loss: 0.00001681
Iteration 6/1000 | Loss: 0.00001605
Iteration 7/1000 | Loss: 0.00001557
Iteration 8/1000 | Loss: 0.00001522
Iteration 9/1000 | Loss: 0.00001481
Iteration 10/1000 | Loss: 0.00001451
Iteration 11/1000 | Loss: 0.00001426
Iteration 12/1000 | Loss: 0.00001421
Iteration 13/1000 | Loss: 0.00001405
Iteration 14/1000 | Loss: 0.00001404
Iteration 15/1000 | Loss: 0.00001404
Iteration 16/1000 | Loss: 0.00001404
Iteration 17/1000 | Loss: 0.00001398
Iteration 18/1000 | Loss: 0.00001397
Iteration 19/1000 | Loss: 0.00001392
Iteration 20/1000 | Loss: 0.00001392
Iteration 21/1000 | Loss: 0.00001390
Iteration 22/1000 | Loss: 0.00001389
Iteration 23/1000 | Loss: 0.00001386
Iteration 24/1000 | Loss: 0.00001385
Iteration 25/1000 | Loss: 0.00001384
Iteration 26/1000 | Loss: 0.00001383
Iteration 27/1000 | Loss: 0.00001383
Iteration 28/1000 | Loss: 0.00001381
Iteration 29/1000 | Loss: 0.00001380
Iteration 30/1000 | Loss: 0.00001379
Iteration 31/1000 | Loss: 0.00001378
Iteration 32/1000 | Loss: 0.00001377
Iteration 33/1000 | Loss: 0.00001377
Iteration 34/1000 | Loss: 0.00001376
Iteration 35/1000 | Loss: 0.00001375
Iteration 36/1000 | Loss: 0.00001375
Iteration 37/1000 | Loss: 0.00001374
Iteration 38/1000 | Loss: 0.00001374
Iteration 39/1000 | Loss: 0.00001373
Iteration 40/1000 | Loss: 0.00001373
Iteration 41/1000 | Loss: 0.00001372
Iteration 42/1000 | Loss: 0.00001372
Iteration 43/1000 | Loss: 0.00001371
Iteration 44/1000 | Loss: 0.00001371
Iteration 45/1000 | Loss: 0.00001370
Iteration 46/1000 | Loss: 0.00001370
Iteration 47/1000 | Loss: 0.00001370
Iteration 48/1000 | Loss: 0.00001369
Iteration 49/1000 | Loss: 0.00001369
Iteration 50/1000 | Loss: 0.00001368
Iteration 51/1000 | Loss: 0.00001368
Iteration 52/1000 | Loss: 0.00001367
Iteration 53/1000 | Loss: 0.00001367
Iteration 54/1000 | Loss: 0.00001367
Iteration 55/1000 | Loss: 0.00001366
Iteration 56/1000 | Loss: 0.00001365
Iteration 57/1000 | Loss: 0.00001365
Iteration 58/1000 | Loss: 0.00001365
Iteration 59/1000 | Loss: 0.00001364
Iteration 60/1000 | Loss: 0.00001364
Iteration 61/1000 | Loss: 0.00001364
Iteration 62/1000 | Loss: 0.00001364
Iteration 63/1000 | Loss: 0.00001364
Iteration 64/1000 | Loss: 0.00001364
Iteration 65/1000 | Loss: 0.00001364
Iteration 66/1000 | Loss: 0.00001364
Iteration 67/1000 | Loss: 0.00001364
Iteration 68/1000 | Loss: 0.00001364
Iteration 69/1000 | Loss: 0.00001364
Iteration 70/1000 | Loss: 0.00001363
Iteration 71/1000 | Loss: 0.00001363
Iteration 72/1000 | Loss: 0.00001363
Iteration 73/1000 | Loss: 0.00001363
Iteration 74/1000 | Loss: 0.00001362
Iteration 75/1000 | Loss: 0.00001362
Iteration 76/1000 | Loss: 0.00001362
Iteration 77/1000 | Loss: 0.00001362
Iteration 78/1000 | Loss: 0.00001362
Iteration 79/1000 | Loss: 0.00001362
Iteration 80/1000 | Loss: 0.00001362
Iteration 81/1000 | Loss: 0.00001362
Iteration 82/1000 | Loss: 0.00001362
Iteration 83/1000 | Loss: 0.00001361
Iteration 84/1000 | Loss: 0.00001361
Iteration 85/1000 | Loss: 0.00001360
Iteration 86/1000 | Loss: 0.00001360
Iteration 87/1000 | Loss: 0.00001360
Iteration 88/1000 | Loss: 0.00001360
Iteration 89/1000 | Loss: 0.00001359
Iteration 90/1000 | Loss: 0.00001359
Iteration 91/1000 | Loss: 0.00001358
Iteration 92/1000 | Loss: 0.00001358
Iteration 93/1000 | Loss: 0.00001358
Iteration 94/1000 | Loss: 0.00001357
Iteration 95/1000 | Loss: 0.00001357
Iteration 96/1000 | Loss: 0.00001357
Iteration 97/1000 | Loss: 0.00001357
Iteration 98/1000 | Loss: 0.00001357
Iteration 99/1000 | Loss: 0.00001357
Iteration 100/1000 | Loss: 0.00001357
Iteration 101/1000 | Loss: 0.00001356
Iteration 102/1000 | Loss: 0.00001356
Iteration 103/1000 | Loss: 0.00001356
Iteration 104/1000 | Loss: 0.00001356
Iteration 105/1000 | Loss: 0.00001356
Iteration 106/1000 | Loss: 0.00001356
Iteration 107/1000 | Loss: 0.00001356
Iteration 108/1000 | Loss: 0.00001356
Iteration 109/1000 | Loss: 0.00001356
Iteration 110/1000 | Loss: 0.00001356
Iteration 111/1000 | Loss: 0.00001355
Iteration 112/1000 | Loss: 0.00001355
Iteration 113/1000 | Loss: 0.00001354
Iteration 114/1000 | Loss: 0.00001354
Iteration 115/1000 | Loss: 0.00001354
Iteration 116/1000 | Loss: 0.00001354
Iteration 117/1000 | Loss: 0.00001354
Iteration 118/1000 | Loss: 0.00001353
Iteration 119/1000 | Loss: 0.00001353
Iteration 120/1000 | Loss: 0.00001353
Iteration 121/1000 | Loss: 0.00001353
Iteration 122/1000 | Loss: 0.00001353
Iteration 123/1000 | Loss: 0.00001353
Iteration 124/1000 | Loss: 0.00001353
Iteration 125/1000 | Loss: 0.00001353
Iteration 126/1000 | Loss: 0.00001352
Iteration 127/1000 | Loss: 0.00001352
Iteration 128/1000 | Loss: 0.00001352
Iteration 129/1000 | Loss: 0.00001352
Iteration 130/1000 | Loss: 0.00001352
Iteration 131/1000 | Loss: 0.00001352
Iteration 132/1000 | Loss: 0.00001352
Iteration 133/1000 | Loss: 0.00001352
Iteration 134/1000 | Loss: 0.00001352
Iteration 135/1000 | Loss: 0.00001352
Iteration 136/1000 | Loss: 0.00001351
Iteration 137/1000 | Loss: 0.00001351
Iteration 138/1000 | Loss: 0.00001351
Iteration 139/1000 | Loss: 0.00001351
Iteration 140/1000 | Loss: 0.00001351
Iteration 141/1000 | Loss: 0.00001351
Iteration 142/1000 | Loss: 0.00001351
Iteration 143/1000 | Loss: 0.00001351
Iteration 144/1000 | Loss: 0.00001351
Iteration 145/1000 | Loss: 0.00001351
Iteration 146/1000 | Loss: 0.00001350
Iteration 147/1000 | Loss: 0.00001350
Iteration 148/1000 | Loss: 0.00001350
Iteration 149/1000 | Loss: 0.00001350
Iteration 150/1000 | Loss: 0.00001350
Iteration 151/1000 | Loss: 0.00001350
Iteration 152/1000 | Loss: 0.00001349
Iteration 153/1000 | Loss: 0.00001349
Iteration 154/1000 | Loss: 0.00001349
Iteration 155/1000 | Loss: 0.00001349
Iteration 156/1000 | Loss: 0.00001349
Iteration 157/1000 | Loss: 0.00001349
Iteration 158/1000 | Loss: 0.00001349
Iteration 159/1000 | Loss: 0.00001349
Iteration 160/1000 | Loss: 0.00001349
Iteration 161/1000 | Loss: 0.00001349
Iteration 162/1000 | Loss: 0.00001349
Iteration 163/1000 | Loss: 0.00001349
Iteration 164/1000 | Loss: 0.00001349
Iteration 165/1000 | Loss: 0.00001349
Iteration 166/1000 | Loss: 0.00001349
Iteration 167/1000 | Loss: 0.00001349
Iteration 168/1000 | Loss: 0.00001349
Iteration 169/1000 | Loss: 0.00001349
Iteration 170/1000 | Loss: 0.00001349
Iteration 171/1000 | Loss: 0.00001349
Iteration 172/1000 | Loss: 0.00001349
Iteration 173/1000 | Loss: 0.00001349
Iteration 174/1000 | Loss: 0.00001349
Iteration 175/1000 | Loss: 0.00001349
Iteration 176/1000 | Loss: 0.00001349
Iteration 177/1000 | Loss: 0.00001349
Iteration 178/1000 | Loss: 0.00001349
Iteration 179/1000 | Loss: 0.00001349
Iteration 180/1000 | Loss: 0.00001349
Iteration 181/1000 | Loss: 0.00001349
Iteration 182/1000 | Loss: 0.00001349
Iteration 183/1000 | Loss: 0.00001349
Iteration 184/1000 | Loss: 0.00001349
Iteration 185/1000 | Loss: 0.00001349
Iteration 186/1000 | Loss: 0.00001349
Iteration 187/1000 | Loss: 0.00001349
Iteration 188/1000 | Loss: 0.00001349
Iteration 189/1000 | Loss: 0.00001349
Iteration 190/1000 | Loss: 0.00001349
Iteration 191/1000 | Loss: 0.00001349
Iteration 192/1000 | Loss: 0.00001349
Iteration 193/1000 | Loss: 0.00001349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.3487801879819017e-05, 1.3487801879819017e-05, 1.3487801879819017e-05, 1.3487801879819017e-05, 1.3487801879819017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3487801879819017e-05

Optimization complete. Final v2v error: 3.131485939025879 mm

Highest mean error: 3.500518798828125 mm for frame 157

Lowest mean error: 2.631394147872925 mm for frame 1

Saving results

Total time: 41.62434935569763
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991576
Iteration 2/25 | Loss: 0.00159303
Iteration 3/25 | Loss: 0.00137537
Iteration 4/25 | Loss: 0.00135366
Iteration 5/25 | Loss: 0.00134644
Iteration 6/25 | Loss: 0.00134488
Iteration 7/25 | Loss: 0.00134488
Iteration 8/25 | Loss: 0.00134488
Iteration 9/25 | Loss: 0.00134488
Iteration 10/25 | Loss: 0.00134488
Iteration 11/25 | Loss: 0.00134488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013448817189782858, 0.0013448817189782858, 0.0013448817189782858, 0.0013448817189782858, 0.0013448817189782858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013448817189782858

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01531768
Iteration 2/25 | Loss: 0.00127787
Iteration 3/25 | Loss: 0.00127786
Iteration 4/25 | Loss: 0.00127786
Iteration 5/25 | Loss: 0.00127786
Iteration 6/25 | Loss: 0.00127786
Iteration 7/25 | Loss: 0.00127785
Iteration 8/25 | Loss: 0.00127785
Iteration 9/25 | Loss: 0.00127785
Iteration 10/25 | Loss: 0.00127785
Iteration 11/25 | Loss: 0.00127785
Iteration 12/25 | Loss: 0.00127785
Iteration 13/25 | Loss: 0.00127785
Iteration 14/25 | Loss: 0.00127785
Iteration 15/25 | Loss: 0.00127785
Iteration 16/25 | Loss: 0.00127785
Iteration 17/25 | Loss: 0.00127785
Iteration 18/25 | Loss: 0.00127785
Iteration 19/25 | Loss: 0.00127785
Iteration 20/25 | Loss: 0.00127785
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012778538512066007, 0.0012778538512066007, 0.0012778538512066007, 0.0012778538512066007, 0.0012778538512066007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012778538512066007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127785
Iteration 2/1000 | Loss: 0.00005584
Iteration 3/1000 | Loss: 0.00003615
Iteration 4/1000 | Loss: 0.00003108
Iteration 5/1000 | Loss: 0.00002938
Iteration 6/1000 | Loss: 0.00002804
Iteration 7/1000 | Loss: 0.00002730
Iteration 8/1000 | Loss: 0.00002663
Iteration 9/1000 | Loss: 0.00002619
Iteration 10/1000 | Loss: 0.00002582
Iteration 11/1000 | Loss: 0.00002550
Iteration 12/1000 | Loss: 0.00002527
Iteration 13/1000 | Loss: 0.00002509
Iteration 14/1000 | Loss: 0.00002490
Iteration 15/1000 | Loss: 0.00002482
Iteration 16/1000 | Loss: 0.00002481
Iteration 17/1000 | Loss: 0.00002475
Iteration 18/1000 | Loss: 0.00002472
Iteration 19/1000 | Loss: 0.00002469
Iteration 20/1000 | Loss: 0.00002458
Iteration 21/1000 | Loss: 0.00002455
Iteration 22/1000 | Loss: 0.00002453
Iteration 23/1000 | Loss: 0.00002453
Iteration 24/1000 | Loss: 0.00002447
Iteration 25/1000 | Loss: 0.00002447
Iteration 26/1000 | Loss: 0.00002446
Iteration 27/1000 | Loss: 0.00002445
Iteration 28/1000 | Loss: 0.00002442
Iteration 29/1000 | Loss: 0.00002442
Iteration 30/1000 | Loss: 0.00002440
Iteration 31/1000 | Loss: 0.00002439
Iteration 32/1000 | Loss: 0.00002439
Iteration 33/1000 | Loss: 0.00002438
Iteration 34/1000 | Loss: 0.00002437
Iteration 35/1000 | Loss: 0.00002437
Iteration 36/1000 | Loss: 0.00002437
Iteration 37/1000 | Loss: 0.00002436
Iteration 38/1000 | Loss: 0.00002432
Iteration 39/1000 | Loss: 0.00002432
Iteration 40/1000 | Loss: 0.00002430
Iteration 41/1000 | Loss: 0.00002430
Iteration 42/1000 | Loss: 0.00002430
Iteration 43/1000 | Loss: 0.00002430
Iteration 44/1000 | Loss: 0.00002429
Iteration 45/1000 | Loss: 0.00002429
Iteration 46/1000 | Loss: 0.00002428
Iteration 47/1000 | Loss: 0.00002428
Iteration 48/1000 | Loss: 0.00002425
Iteration 49/1000 | Loss: 0.00002425
Iteration 50/1000 | Loss: 0.00002425
Iteration 51/1000 | Loss: 0.00002425
Iteration 52/1000 | Loss: 0.00002425
Iteration 53/1000 | Loss: 0.00002425
Iteration 54/1000 | Loss: 0.00002425
Iteration 55/1000 | Loss: 0.00002425
Iteration 56/1000 | Loss: 0.00002424
Iteration 57/1000 | Loss: 0.00002424
Iteration 58/1000 | Loss: 0.00002424
Iteration 59/1000 | Loss: 0.00002423
Iteration 60/1000 | Loss: 0.00002423
Iteration 61/1000 | Loss: 0.00002423
Iteration 62/1000 | Loss: 0.00002423
Iteration 63/1000 | Loss: 0.00002422
Iteration 64/1000 | Loss: 0.00002422
Iteration 65/1000 | Loss: 0.00002422
Iteration 66/1000 | Loss: 0.00002422
Iteration 67/1000 | Loss: 0.00002422
Iteration 68/1000 | Loss: 0.00002422
Iteration 69/1000 | Loss: 0.00002421
Iteration 70/1000 | Loss: 0.00002421
Iteration 71/1000 | Loss: 0.00002420
Iteration 72/1000 | Loss: 0.00002420
Iteration 73/1000 | Loss: 0.00002420
Iteration 74/1000 | Loss: 0.00002420
Iteration 75/1000 | Loss: 0.00002420
Iteration 76/1000 | Loss: 0.00002419
Iteration 77/1000 | Loss: 0.00002419
Iteration 78/1000 | Loss: 0.00002419
Iteration 79/1000 | Loss: 0.00002419
Iteration 80/1000 | Loss: 0.00002419
Iteration 81/1000 | Loss: 0.00002419
Iteration 82/1000 | Loss: 0.00002419
Iteration 83/1000 | Loss: 0.00002419
Iteration 84/1000 | Loss: 0.00002419
Iteration 85/1000 | Loss: 0.00002419
Iteration 86/1000 | Loss: 0.00002419
Iteration 87/1000 | Loss: 0.00002418
Iteration 88/1000 | Loss: 0.00002418
Iteration 89/1000 | Loss: 0.00002418
Iteration 90/1000 | Loss: 0.00002417
Iteration 91/1000 | Loss: 0.00002417
Iteration 92/1000 | Loss: 0.00002417
Iteration 93/1000 | Loss: 0.00002416
Iteration 94/1000 | Loss: 0.00002416
Iteration 95/1000 | Loss: 0.00002416
Iteration 96/1000 | Loss: 0.00002416
Iteration 97/1000 | Loss: 0.00002416
Iteration 98/1000 | Loss: 0.00002416
Iteration 99/1000 | Loss: 0.00002415
Iteration 100/1000 | Loss: 0.00002415
Iteration 101/1000 | Loss: 0.00002415
Iteration 102/1000 | Loss: 0.00002415
Iteration 103/1000 | Loss: 0.00002414
Iteration 104/1000 | Loss: 0.00002414
Iteration 105/1000 | Loss: 0.00002414
Iteration 106/1000 | Loss: 0.00002414
Iteration 107/1000 | Loss: 0.00002413
Iteration 108/1000 | Loss: 0.00002413
Iteration 109/1000 | Loss: 0.00002413
Iteration 110/1000 | Loss: 0.00002413
Iteration 111/1000 | Loss: 0.00002413
Iteration 112/1000 | Loss: 0.00002413
Iteration 113/1000 | Loss: 0.00002412
Iteration 114/1000 | Loss: 0.00002412
Iteration 115/1000 | Loss: 0.00002412
Iteration 116/1000 | Loss: 0.00002412
Iteration 117/1000 | Loss: 0.00002412
Iteration 118/1000 | Loss: 0.00002412
Iteration 119/1000 | Loss: 0.00002412
Iteration 120/1000 | Loss: 0.00002411
Iteration 121/1000 | Loss: 0.00002411
Iteration 122/1000 | Loss: 0.00002411
Iteration 123/1000 | Loss: 0.00002411
Iteration 124/1000 | Loss: 0.00002411
Iteration 125/1000 | Loss: 0.00002411
Iteration 126/1000 | Loss: 0.00002411
Iteration 127/1000 | Loss: 0.00002411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.411404602753464e-05, 2.411404602753464e-05, 2.411404602753464e-05, 2.411404602753464e-05, 2.411404602753464e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.411404602753464e-05

Optimization complete. Final v2v error: 4.067133903503418 mm

Highest mean error: 4.950599670410156 mm for frame 76

Lowest mean error: 3.4412379264831543 mm for frame 37

Saving results

Total time: 44.43843173980713
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00516634
Iteration 2/25 | Loss: 0.00133473
Iteration 3/25 | Loss: 0.00124240
Iteration 4/25 | Loss: 0.00123101
Iteration 5/25 | Loss: 0.00122942
Iteration 6/25 | Loss: 0.00122942
Iteration 7/25 | Loss: 0.00122942
Iteration 8/25 | Loss: 0.00122942
Iteration 9/25 | Loss: 0.00122942
Iteration 10/25 | Loss: 0.00122942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012294197222217917, 0.0012294197222217917, 0.0012294197222217917, 0.0012294197222217917, 0.0012294197222217917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012294197222217917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.94104242
Iteration 2/25 | Loss: 0.00097846
Iteration 3/25 | Loss: 0.00097845
Iteration 4/25 | Loss: 0.00097845
Iteration 5/25 | Loss: 0.00097845
Iteration 6/25 | Loss: 0.00097845
Iteration 7/25 | Loss: 0.00097845
Iteration 8/25 | Loss: 0.00097845
Iteration 9/25 | Loss: 0.00097845
Iteration 10/25 | Loss: 0.00097845
Iteration 11/25 | Loss: 0.00097845
Iteration 12/25 | Loss: 0.00097845
Iteration 13/25 | Loss: 0.00097845
Iteration 14/25 | Loss: 0.00097845
Iteration 15/25 | Loss: 0.00097845
Iteration 16/25 | Loss: 0.00097845
Iteration 17/25 | Loss: 0.00097845
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000978451338596642, 0.000978451338596642, 0.000978451338596642, 0.000978451338596642, 0.000978451338596642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000978451338596642

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097845
Iteration 2/1000 | Loss: 0.00003060
Iteration 3/1000 | Loss: 0.00002224
Iteration 4/1000 | Loss: 0.00002007
Iteration 5/1000 | Loss: 0.00001863
Iteration 6/1000 | Loss: 0.00001772
Iteration 7/1000 | Loss: 0.00001717
Iteration 8/1000 | Loss: 0.00001675
Iteration 9/1000 | Loss: 0.00001625
Iteration 10/1000 | Loss: 0.00001597
Iteration 11/1000 | Loss: 0.00001568
Iteration 12/1000 | Loss: 0.00001552
Iteration 13/1000 | Loss: 0.00001542
Iteration 14/1000 | Loss: 0.00001528
Iteration 15/1000 | Loss: 0.00001525
Iteration 16/1000 | Loss: 0.00001520
Iteration 17/1000 | Loss: 0.00001517
Iteration 18/1000 | Loss: 0.00001517
Iteration 19/1000 | Loss: 0.00001517
Iteration 20/1000 | Loss: 0.00001517
Iteration 21/1000 | Loss: 0.00001517
Iteration 22/1000 | Loss: 0.00001517
Iteration 23/1000 | Loss: 0.00001517
Iteration 24/1000 | Loss: 0.00001517
Iteration 25/1000 | Loss: 0.00001517
Iteration 26/1000 | Loss: 0.00001517
Iteration 27/1000 | Loss: 0.00001517
Iteration 28/1000 | Loss: 0.00001517
Iteration 29/1000 | Loss: 0.00001517
Iteration 30/1000 | Loss: 0.00001517
Iteration 31/1000 | Loss: 0.00001517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 31. Stopping optimization.
Last 5 losses: [1.5166257981036324e-05, 1.5166257981036324e-05, 1.5166257981036324e-05, 1.5166257981036324e-05, 1.5166257981036324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5166257981036324e-05

Optimization complete. Final v2v error: 3.3219454288482666 mm

Highest mean error: 3.854015588760376 mm for frame 74

Lowest mean error: 3.023040533065796 mm for frame 218

Saving results

Total time: 30.748129844665527
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782388
Iteration 2/25 | Loss: 0.00150299
Iteration 3/25 | Loss: 0.00126330
Iteration 4/25 | Loss: 0.00125152
Iteration 5/25 | Loss: 0.00124918
Iteration 6/25 | Loss: 0.00124918
Iteration 7/25 | Loss: 0.00124918
Iteration 8/25 | Loss: 0.00124918
Iteration 9/25 | Loss: 0.00124918
Iteration 10/25 | Loss: 0.00124918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012491804081946611, 0.0012491804081946611, 0.0012491804081946611, 0.0012491804081946611, 0.0012491804081946611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012491804081946611

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38002694
Iteration 2/25 | Loss: 0.00090488
Iteration 3/25 | Loss: 0.00090488
Iteration 4/25 | Loss: 0.00090488
Iteration 5/25 | Loss: 0.00090488
Iteration 6/25 | Loss: 0.00090488
Iteration 7/25 | Loss: 0.00090488
Iteration 8/25 | Loss: 0.00090488
Iteration 9/25 | Loss: 0.00090488
Iteration 10/25 | Loss: 0.00090488
Iteration 11/25 | Loss: 0.00090488
Iteration 12/25 | Loss: 0.00090488
Iteration 13/25 | Loss: 0.00090488
Iteration 14/25 | Loss: 0.00090488
Iteration 15/25 | Loss: 0.00090488
Iteration 16/25 | Loss: 0.00090488
Iteration 17/25 | Loss: 0.00090488
Iteration 18/25 | Loss: 0.00090488
Iteration 19/25 | Loss: 0.00090488
Iteration 20/25 | Loss: 0.00090488
Iteration 21/25 | Loss: 0.00090488
Iteration 22/25 | Loss: 0.00090488
Iteration 23/25 | Loss: 0.00090488
Iteration 24/25 | Loss: 0.00090488
Iteration 25/25 | Loss: 0.00090488

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090488
Iteration 2/1000 | Loss: 0.00003501
Iteration 3/1000 | Loss: 0.00002421
Iteration 4/1000 | Loss: 0.00002136
Iteration 5/1000 | Loss: 0.00001944
Iteration 6/1000 | Loss: 0.00001845
Iteration 7/1000 | Loss: 0.00001780
Iteration 8/1000 | Loss: 0.00001730
Iteration 9/1000 | Loss: 0.00001684
Iteration 10/1000 | Loss: 0.00001649
Iteration 11/1000 | Loss: 0.00001622
Iteration 12/1000 | Loss: 0.00001610
Iteration 13/1000 | Loss: 0.00001608
Iteration 14/1000 | Loss: 0.00001595
Iteration 15/1000 | Loss: 0.00001594
Iteration 16/1000 | Loss: 0.00001592
Iteration 17/1000 | Loss: 0.00001591
Iteration 18/1000 | Loss: 0.00001591
Iteration 19/1000 | Loss: 0.00001591
Iteration 20/1000 | Loss: 0.00001591
Iteration 21/1000 | Loss: 0.00001589
Iteration 22/1000 | Loss: 0.00001585
Iteration 23/1000 | Loss: 0.00001585
Iteration 24/1000 | Loss: 0.00001584
Iteration 25/1000 | Loss: 0.00001580
Iteration 26/1000 | Loss: 0.00001579
Iteration 27/1000 | Loss: 0.00001576
Iteration 28/1000 | Loss: 0.00001576
Iteration 29/1000 | Loss: 0.00001576
Iteration 30/1000 | Loss: 0.00001575
Iteration 31/1000 | Loss: 0.00001575
Iteration 32/1000 | Loss: 0.00001575
Iteration 33/1000 | Loss: 0.00001575
Iteration 34/1000 | Loss: 0.00001574
Iteration 35/1000 | Loss: 0.00001574
Iteration 36/1000 | Loss: 0.00001574
Iteration 37/1000 | Loss: 0.00001574
Iteration 38/1000 | Loss: 0.00001573
Iteration 39/1000 | Loss: 0.00001573
Iteration 40/1000 | Loss: 0.00001572
Iteration 41/1000 | Loss: 0.00001568
Iteration 42/1000 | Loss: 0.00001568
Iteration 43/1000 | Loss: 0.00001568
Iteration 44/1000 | Loss: 0.00001568
Iteration 45/1000 | Loss: 0.00001568
Iteration 46/1000 | Loss: 0.00001568
Iteration 47/1000 | Loss: 0.00001568
Iteration 48/1000 | Loss: 0.00001568
Iteration 49/1000 | Loss: 0.00001568
Iteration 50/1000 | Loss: 0.00001568
Iteration 51/1000 | Loss: 0.00001568
Iteration 52/1000 | Loss: 0.00001567
Iteration 53/1000 | Loss: 0.00001567
Iteration 54/1000 | Loss: 0.00001566
Iteration 55/1000 | Loss: 0.00001566
Iteration 56/1000 | Loss: 0.00001566
Iteration 57/1000 | Loss: 0.00001566
Iteration 58/1000 | Loss: 0.00001566
Iteration 59/1000 | Loss: 0.00001566
Iteration 60/1000 | Loss: 0.00001566
Iteration 61/1000 | Loss: 0.00001564
Iteration 62/1000 | Loss: 0.00001564
Iteration 63/1000 | Loss: 0.00001564
Iteration 64/1000 | Loss: 0.00001563
Iteration 65/1000 | Loss: 0.00001563
Iteration 66/1000 | Loss: 0.00001561
Iteration 67/1000 | Loss: 0.00001561
Iteration 68/1000 | Loss: 0.00001561
Iteration 69/1000 | Loss: 0.00001561
Iteration 70/1000 | Loss: 0.00001560
Iteration 71/1000 | Loss: 0.00001560
Iteration 72/1000 | Loss: 0.00001559
Iteration 73/1000 | Loss: 0.00001559
Iteration 74/1000 | Loss: 0.00001558
Iteration 75/1000 | Loss: 0.00001558
Iteration 76/1000 | Loss: 0.00001558
Iteration 77/1000 | Loss: 0.00001557
Iteration 78/1000 | Loss: 0.00001557
Iteration 79/1000 | Loss: 0.00001557
Iteration 80/1000 | Loss: 0.00001557
Iteration 81/1000 | Loss: 0.00001557
Iteration 82/1000 | Loss: 0.00001556
Iteration 83/1000 | Loss: 0.00001556
Iteration 84/1000 | Loss: 0.00001556
Iteration 85/1000 | Loss: 0.00001556
Iteration 86/1000 | Loss: 0.00001556
Iteration 87/1000 | Loss: 0.00001554
Iteration 88/1000 | Loss: 0.00001554
Iteration 89/1000 | Loss: 0.00001554
Iteration 90/1000 | Loss: 0.00001554
Iteration 91/1000 | Loss: 0.00001554
Iteration 92/1000 | Loss: 0.00001554
Iteration 93/1000 | Loss: 0.00001554
Iteration 94/1000 | Loss: 0.00001554
Iteration 95/1000 | Loss: 0.00001554
Iteration 96/1000 | Loss: 0.00001553
Iteration 97/1000 | Loss: 0.00001553
Iteration 98/1000 | Loss: 0.00001553
Iteration 99/1000 | Loss: 0.00001553
Iteration 100/1000 | Loss: 0.00001553
Iteration 101/1000 | Loss: 0.00001553
Iteration 102/1000 | Loss: 0.00001553
Iteration 103/1000 | Loss: 0.00001553
Iteration 104/1000 | Loss: 0.00001553
Iteration 105/1000 | Loss: 0.00001553
Iteration 106/1000 | Loss: 0.00001553
Iteration 107/1000 | Loss: 0.00001553
Iteration 108/1000 | Loss: 0.00001553
Iteration 109/1000 | Loss: 0.00001553
Iteration 110/1000 | Loss: 0.00001553
Iteration 111/1000 | Loss: 0.00001553
Iteration 112/1000 | Loss: 0.00001552
Iteration 113/1000 | Loss: 0.00001552
Iteration 114/1000 | Loss: 0.00001552
Iteration 115/1000 | Loss: 0.00001552
Iteration 116/1000 | Loss: 0.00001551
Iteration 117/1000 | Loss: 0.00001551
Iteration 118/1000 | Loss: 0.00001551
Iteration 119/1000 | Loss: 0.00001551
Iteration 120/1000 | Loss: 0.00001551
Iteration 121/1000 | Loss: 0.00001551
Iteration 122/1000 | Loss: 0.00001551
Iteration 123/1000 | Loss: 0.00001551
Iteration 124/1000 | Loss: 0.00001550
Iteration 125/1000 | Loss: 0.00001549
Iteration 126/1000 | Loss: 0.00001549
Iteration 127/1000 | Loss: 0.00001549
Iteration 128/1000 | Loss: 0.00001549
Iteration 129/1000 | Loss: 0.00001549
Iteration 130/1000 | Loss: 0.00001549
Iteration 131/1000 | Loss: 0.00001548
Iteration 132/1000 | Loss: 0.00001548
Iteration 133/1000 | Loss: 0.00001548
Iteration 134/1000 | Loss: 0.00001548
Iteration 135/1000 | Loss: 0.00001548
Iteration 136/1000 | Loss: 0.00001548
Iteration 137/1000 | Loss: 0.00001548
Iteration 138/1000 | Loss: 0.00001548
Iteration 139/1000 | Loss: 0.00001548
Iteration 140/1000 | Loss: 0.00001548
Iteration 141/1000 | Loss: 0.00001547
Iteration 142/1000 | Loss: 0.00001547
Iteration 143/1000 | Loss: 0.00001547
Iteration 144/1000 | Loss: 0.00001547
Iteration 145/1000 | Loss: 0.00001547
Iteration 146/1000 | Loss: 0.00001547
Iteration 147/1000 | Loss: 0.00001547
Iteration 148/1000 | Loss: 0.00001547
Iteration 149/1000 | Loss: 0.00001547
Iteration 150/1000 | Loss: 0.00001547
Iteration 151/1000 | Loss: 0.00001546
Iteration 152/1000 | Loss: 0.00001546
Iteration 153/1000 | Loss: 0.00001546
Iteration 154/1000 | Loss: 0.00001546
Iteration 155/1000 | Loss: 0.00001546
Iteration 156/1000 | Loss: 0.00001546
Iteration 157/1000 | Loss: 0.00001546
Iteration 158/1000 | Loss: 0.00001546
Iteration 159/1000 | Loss: 0.00001546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.5459334463230334e-05, 1.5459334463230334e-05, 1.5459334463230334e-05, 1.5459334463230334e-05, 1.5459334463230334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5459334463230334e-05

Optimization complete. Final v2v error: 3.3295390605926514 mm

Highest mean error: 3.6597468852996826 mm for frame 178

Lowest mean error: 3.0165185928344727 mm for frame 220

Saving results

Total time: 43.090994358062744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959144
Iteration 2/25 | Loss: 0.00422190
Iteration 3/25 | Loss: 0.00287422
Iteration 4/25 | Loss: 0.00247871
Iteration 5/25 | Loss: 0.00234782
Iteration 6/25 | Loss: 0.00229005
Iteration 7/25 | Loss: 0.00219480
Iteration 8/25 | Loss: 0.00201288
Iteration 9/25 | Loss: 0.00197993
Iteration 10/25 | Loss: 0.00188928
Iteration 11/25 | Loss: 0.00185541
Iteration 12/25 | Loss: 0.00181933
Iteration 13/25 | Loss: 0.00182220
Iteration 14/25 | Loss: 0.00177504
Iteration 15/25 | Loss: 0.00178322
Iteration 16/25 | Loss: 0.00175226
Iteration 17/25 | Loss: 0.00173801
Iteration 18/25 | Loss: 0.00173012
Iteration 19/25 | Loss: 0.00171651
Iteration 20/25 | Loss: 0.00170346
Iteration 21/25 | Loss: 0.00169020
Iteration 22/25 | Loss: 0.00168594
Iteration 23/25 | Loss: 0.00168451
Iteration 24/25 | Loss: 0.00167565
Iteration 25/25 | Loss: 0.00167912

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42851138
Iteration 2/25 | Loss: 0.00766684
Iteration 3/25 | Loss: 0.00497120
Iteration 4/25 | Loss: 0.00497119
Iteration 5/25 | Loss: 0.00497119
Iteration 6/25 | Loss: 0.00497119
Iteration 7/25 | Loss: 0.00497119
Iteration 8/25 | Loss: 0.00497119
Iteration 9/25 | Loss: 0.00497119
Iteration 10/25 | Loss: 0.00497119
Iteration 11/25 | Loss: 0.00497119
Iteration 12/25 | Loss: 0.00497119
Iteration 13/25 | Loss: 0.00497119
Iteration 14/25 | Loss: 0.00497119
Iteration 15/25 | Loss: 0.00497119
Iteration 16/25 | Loss: 0.00497119
Iteration 17/25 | Loss: 0.00497119
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004971190821379423, 0.004971190821379423, 0.004971190821379423, 0.004971190821379423, 0.004971190821379423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004971190821379423

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00497119
Iteration 2/1000 | Loss: 0.00342795
Iteration 3/1000 | Loss: 0.00289792
Iteration 4/1000 | Loss: 0.00098160
Iteration 5/1000 | Loss: 0.00124371
Iteration 6/1000 | Loss: 0.00061189
Iteration 7/1000 | Loss: 0.00065124
Iteration 8/1000 | Loss: 0.00047357
Iteration 9/1000 | Loss: 0.00066018
Iteration 10/1000 | Loss: 0.00172728
Iteration 11/1000 | Loss: 0.00183205
Iteration 12/1000 | Loss: 0.00032531
Iteration 13/1000 | Loss: 0.00103915
Iteration 14/1000 | Loss: 0.00031095
Iteration 15/1000 | Loss: 0.00067690
Iteration 16/1000 | Loss: 0.00033286
Iteration 17/1000 | Loss: 0.00054051
Iteration 18/1000 | Loss: 0.00028309
Iteration 19/1000 | Loss: 0.00035054
Iteration 20/1000 | Loss: 0.00026220
Iteration 21/1000 | Loss: 0.00011452
Iteration 22/1000 | Loss: 0.00056559
Iteration 23/1000 | Loss: 0.00168716
Iteration 24/1000 | Loss: 0.00043946
Iteration 25/1000 | Loss: 0.00013456
Iteration 26/1000 | Loss: 0.00030829
Iteration 27/1000 | Loss: 0.00040724
Iteration 28/1000 | Loss: 0.00017498
Iteration 29/1000 | Loss: 0.00019723
Iteration 30/1000 | Loss: 0.00009771
Iteration 31/1000 | Loss: 0.00083455
Iteration 32/1000 | Loss: 0.00069295
Iteration 33/1000 | Loss: 0.00215258
Iteration 34/1000 | Loss: 0.00167788
Iteration 35/1000 | Loss: 0.00030942
Iteration 36/1000 | Loss: 0.00014821
Iteration 37/1000 | Loss: 0.00015564
Iteration 38/1000 | Loss: 0.00011315
Iteration 39/1000 | Loss: 0.00018516
Iteration 40/1000 | Loss: 0.00139629
Iteration 41/1000 | Loss: 0.00183499
Iteration 42/1000 | Loss: 0.00091670
Iteration 43/1000 | Loss: 0.00011844
Iteration 44/1000 | Loss: 0.00009097
Iteration 45/1000 | Loss: 0.00027090
Iteration 46/1000 | Loss: 0.00018835
Iteration 47/1000 | Loss: 0.00008783
Iteration 48/1000 | Loss: 0.00008157
Iteration 49/1000 | Loss: 0.00018890
Iteration 50/1000 | Loss: 0.00008528
Iteration 51/1000 | Loss: 0.00007845
Iteration 52/1000 | Loss: 0.00048739
Iteration 53/1000 | Loss: 0.00053807
Iteration 54/1000 | Loss: 0.00030954
Iteration 55/1000 | Loss: 0.00007450
Iteration 56/1000 | Loss: 0.00007231
Iteration 57/1000 | Loss: 0.00007124
Iteration 58/1000 | Loss: 0.00028347
Iteration 59/1000 | Loss: 0.00102677
Iteration 60/1000 | Loss: 0.00007068
Iteration 61/1000 | Loss: 0.00006973
Iteration 62/1000 | Loss: 0.00006905
Iteration 63/1000 | Loss: 0.00051637
Iteration 64/1000 | Loss: 0.00018379
Iteration 65/1000 | Loss: 0.00012302
Iteration 66/1000 | Loss: 0.00025873
Iteration 67/1000 | Loss: 0.00011784
Iteration 68/1000 | Loss: 0.00006985
Iteration 69/1000 | Loss: 0.00030604
Iteration 70/1000 | Loss: 0.00059549
Iteration 71/1000 | Loss: 0.00063516
Iteration 72/1000 | Loss: 0.00017347
Iteration 73/1000 | Loss: 0.00038843
Iteration 74/1000 | Loss: 0.00007572
Iteration 75/1000 | Loss: 0.00014548
Iteration 76/1000 | Loss: 0.00006746
Iteration 77/1000 | Loss: 0.00019435
Iteration 78/1000 | Loss: 0.00012654
Iteration 79/1000 | Loss: 0.00006299
Iteration 80/1000 | Loss: 0.00010694
Iteration 81/1000 | Loss: 0.00007235
Iteration 82/1000 | Loss: 0.00006143
Iteration 83/1000 | Loss: 0.00012222
Iteration 84/1000 | Loss: 0.00006350
Iteration 85/1000 | Loss: 0.00006184
Iteration 86/1000 | Loss: 0.00006042
Iteration 87/1000 | Loss: 0.00006005
Iteration 88/1000 | Loss: 0.00032746
Iteration 89/1000 | Loss: 0.00203066
Iteration 90/1000 | Loss: 0.00322326
Iteration 91/1000 | Loss: 0.00044705
Iteration 92/1000 | Loss: 0.00020433
Iteration 93/1000 | Loss: 0.00040859
Iteration 94/1000 | Loss: 0.00032359
Iteration 95/1000 | Loss: 0.00066945
Iteration 96/1000 | Loss: 0.00006966
Iteration 97/1000 | Loss: 0.00036061
Iteration 98/1000 | Loss: 0.00036572
Iteration 99/1000 | Loss: 0.00015089
Iteration 100/1000 | Loss: 0.00020659
Iteration 101/1000 | Loss: 0.00011837
Iteration 102/1000 | Loss: 0.00005900
Iteration 103/1000 | Loss: 0.00027520
Iteration 104/1000 | Loss: 0.00014946
Iteration 105/1000 | Loss: 0.00025357
Iteration 106/1000 | Loss: 0.00015532
Iteration 107/1000 | Loss: 0.00030923
Iteration 108/1000 | Loss: 0.00015515
Iteration 109/1000 | Loss: 0.00033409
Iteration 110/1000 | Loss: 0.00098983
Iteration 111/1000 | Loss: 0.00050459
Iteration 112/1000 | Loss: 0.00008365
Iteration 113/1000 | Loss: 0.00006564
Iteration 114/1000 | Loss: 0.00006377
Iteration 115/1000 | Loss: 0.00007039
Iteration 116/1000 | Loss: 0.00013272
Iteration 117/1000 | Loss: 0.00005707
Iteration 118/1000 | Loss: 0.00005590
Iteration 119/1000 | Loss: 0.00005516
Iteration 120/1000 | Loss: 0.00005466
Iteration 121/1000 | Loss: 0.00005433
Iteration 122/1000 | Loss: 0.00005408
Iteration 123/1000 | Loss: 0.00032041
Iteration 124/1000 | Loss: 0.00006609
Iteration 125/1000 | Loss: 0.00005942
Iteration 126/1000 | Loss: 0.00005466
Iteration 127/1000 | Loss: 0.00005382
Iteration 128/1000 | Loss: 0.00005363
Iteration 129/1000 | Loss: 0.00005352
Iteration 130/1000 | Loss: 0.00005338
Iteration 131/1000 | Loss: 0.00005334
Iteration 132/1000 | Loss: 0.00005333
Iteration 133/1000 | Loss: 0.00010621
Iteration 134/1000 | Loss: 0.00102182
Iteration 135/1000 | Loss: 0.00010626
Iteration 136/1000 | Loss: 0.00022565
Iteration 137/1000 | Loss: 0.00011349
Iteration 138/1000 | Loss: 0.00012874
Iteration 139/1000 | Loss: 0.00008750
Iteration 140/1000 | Loss: 0.00015501
Iteration 141/1000 | Loss: 0.00017960
Iteration 142/1000 | Loss: 0.00019068
Iteration 143/1000 | Loss: 0.00010232
Iteration 144/1000 | Loss: 0.00005895
Iteration 145/1000 | Loss: 0.00012513
Iteration 146/1000 | Loss: 0.00005380
Iteration 147/1000 | Loss: 0.00005150
Iteration 148/1000 | Loss: 0.00005056
Iteration 149/1000 | Loss: 0.00004989
Iteration 150/1000 | Loss: 0.00040443
Iteration 151/1000 | Loss: 0.00152756
Iteration 152/1000 | Loss: 0.00243284
Iteration 153/1000 | Loss: 0.00200297
Iteration 154/1000 | Loss: 0.00262188
Iteration 155/1000 | Loss: 0.00240476
Iteration 156/1000 | Loss: 0.00065280
Iteration 157/1000 | Loss: 0.00194240
Iteration 158/1000 | Loss: 0.00018169
Iteration 159/1000 | Loss: 0.00064283
Iteration 160/1000 | Loss: 0.00111760
Iteration 161/1000 | Loss: 0.00026173
Iteration 162/1000 | Loss: 0.00010564
Iteration 163/1000 | Loss: 0.00020624
Iteration 164/1000 | Loss: 0.00015043
Iteration 165/1000 | Loss: 0.00007068
Iteration 166/1000 | Loss: 0.00006166
Iteration 167/1000 | Loss: 0.00008136
Iteration 168/1000 | Loss: 0.00006114
Iteration 169/1000 | Loss: 0.00005333
Iteration 170/1000 | Loss: 0.00006090
Iteration 171/1000 | Loss: 0.00005177
Iteration 172/1000 | Loss: 0.00005109
Iteration 173/1000 | Loss: 0.00005071
Iteration 174/1000 | Loss: 0.00024373
Iteration 175/1000 | Loss: 0.00005096
Iteration 176/1000 | Loss: 0.00004980
Iteration 177/1000 | Loss: 0.00010334
Iteration 178/1000 | Loss: 0.00004989
Iteration 179/1000 | Loss: 0.00004932
Iteration 180/1000 | Loss: 0.00004923
Iteration 181/1000 | Loss: 0.00004917
Iteration 182/1000 | Loss: 0.00005643
Iteration 183/1000 | Loss: 0.00004960
Iteration 184/1000 | Loss: 0.00004921
Iteration 185/1000 | Loss: 0.00004886
Iteration 186/1000 | Loss: 0.00004871
Iteration 187/1000 | Loss: 0.00004848
Iteration 188/1000 | Loss: 0.00004843
Iteration 189/1000 | Loss: 0.00004838
Iteration 190/1000 | Loss: 0.00004838
Iteration 191/1000 | Loss: 0.00004837
Iteration 192/1000 | Loss: 0.00004837
Iteration 193/1000 | Loss: 0.00004836
Iteration 194/1000 | Loss: 0.00004832
Iteration 195/1000 | Loss: 0.00013256
Iteration 196/1000 | Loss: 0.00005424
Iteration 197/1000 | Loss: 0.00005080
Iteration 198/1000 | Loss: 0.00004951
Iteration 199/1000 | Loss: 0.00004836
Iteration 200/1000 | Loss: 0.00004767
Iteration 201/1000 | Loss: 0.00004738
Iteration 202/1000 | Loss: 0.00004718
Iteration 203/1000 | Loss: 0.00004699
Iteration 204/1000 | Loss: 0.00027100
Iteration 205/1000 | Loss: 0.00004792
Iteration 206/1000 | Loss: 0.00004674
Iteration 207/1000 | Loss: 0.00004619
Iteration 208/1000 | Loss: 0.00004558
Iteration 209/1000 | Loss: 0.00004531
Iteration 210/1000 | Loss: 0.00004510
Iteration 211/1000 | Loss: 0.00004507
Iteration 212/1000 | Loss: 0.00004506
Iteration 213/1000 | Loss: 0.00004504
Iteration 214/1000 | Loss: 0.00004504
Iteration 215/1000 | Loss: 0.00004504
Iteration 216/1000 | Loss: 0.00004502
Iteration 217/1000 | Loss: 0.00004501
Iteration 218/1000 | Loss: 0.00004501
Iteration 219/1000 | Loss: 0.00004501
Iteration 220/1000 | Loss: 0.00004501
Iteration 221/1000 | Loss: 0.00004500
Iteration 222/1000 | Loss: 0.00004500
Iteration 223/1000 | Loss: 0.00004499
Iteration 224/1000 | Loss: 0.00004499
Iteration 225/1000 | Loss: 0.00004499
Iteration 226/1000 | Loss: 0.00004497
Iteration 227/1000 | Loss: 0.00004494
Iteration 228/1000 | Loss: 0.00004494
Iteration 229/1000 | Loss: 0.00004494
Iteration 230/1000 | Loss: 0.00004494
Iteration 231/1000 | Loss: 0.00004494
Iteration 232/1000 | Loss: 0.00004493
Iteration 233/1000 | Loss: 0.00004493
Iteration 234/1000 | Loss: 0.00004493
Iteration 235/1000 | Loss: 0.00004493
Iteration 236/1000 | Loss: 0.00004493
Iteration 237/1000 | Loss: 0.00004492
Iteration 238/1000 | Loss: 0.00004492
Iteration 239/1000 | Loss: 0.00004492
Iteration 240/1000 | Loss: 0.00004492
Iteration 241/1000 | Loss: 0.00004491
Iteration 242/1000 | Loss: 0.00004491
Iteration 243/1000 | Loss: 0.00004491
Iteration 244/1000 | Loss: 0.00004491
Iteration 245/1000 | Loss: 0.00004491
Iteration 246/1000 | Loss: 0.00004491
Iteration 247/1000 | Loss: 0.00004491
Iteration 248/1000 | Loss: 0.00004490
Iteration 249/1000 | Loss: 0.00004490
Iteration 250/1000 | Loss: 0.00004490
Iteration 251/1000 | Loss: 0.00004490
Iteration 252/1000 | Loss: 0.00004490
Iteration 253/1000 | Loss: 0.00004490
Iteration 254/1000 | Loss: 0.00004490
Iteration 255/1000 | Loss: 0.00004489
Iteration 256/1000 | Loss: 0.00004489
Iteration 257/1000 | Loss: 0.00004489
Iteration 258/1000 | Loss: 0.00004489
Iteration 259/1000 | Loss: 0.00004489
Iteration 260/1000 | Loss: 0.00004489
Iteration 261/1000 | Loss: 0.00004489
Iteration 262/1000 | Loss: 0.00004488
Iteration 263/1000 | Loss: 0.00004488
Iteration 264/1000 | Loss: 0.00004488
Iteration 265/1000 | Loss: 0.00004488
Iteration 266/1000 | Loss: 0.00004488
Iteration 267/1000 | Loss: 0.00004488
Iteration 268/1000 | Loss: 0.00004487
Iteration 269/1000 | Loss: 0.00004487
Iteration 270/1000 | Loss: 0.00004487
Iteration 271/1000 | Loss: 0.00004486
Iteration 272/1000 | Loss: 0.00004486
Iteration 273/1000 | Loss: 0.00004486
Iteration 274/1000 | Loss: 0.00004486
Iteration 275/1000 | Loss: 0.00004486
Iteration 276/1000 | Loss: 0.00004486
Iteration 277/1000 | Loss: 0.00004486
Iteration 278/1000 | Loss: 0.00004486
Iteration 279/1000 | Loss: 0.00004486
Iteration 280/1000 | Loss: 0.00004486
Iteration 281/1000 | Loss: 0.00004486
Iteration 282/1000 | Loss: 0.00004486
Iteration 283/1000 | Loss: 0.00004486
Iteration 284/1000 | Loss: 0.00004486
Iteration 285/1000 | Loss: 0.00004486
Iteration 286/1000 | Loss: 0.00004486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 286. Stopping optimization.
Last 5 losses: [4.485556928557344e-05, 4.485556928557344e-05, 4.485556928557344e-05, 4.485556928557344e-05, 4.485556928557344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.485556928557344e-05

Optimization complete. Final v2v error: 4.066049098968506 mm

Highest mean error: 11.78743839263916 mm for frame 107

Lowest mean error: 3.1056764125823975 mm for frame 83

Saving results

Total time: 383.40716004371643
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865175
Iteration 2/25 | Loss: 0.00162711
Iteration 3/25 | Loss: 0.00137061
Iteration 4/25 | Loss: 0.00135234
Iteration 5/25 | Loss: 0.00134757
Iteration 6/25 | Loss: 0.00134717
Iteration 7/25 | Loss: 0.00133162
Iteration 8/25 | Loss: 0.00131033
Iteration 9/25 | Loss: 0.00129467
Iteration 10/25 | Loss: 0.00129060
Iteration 11/25 | Loss: 0.00128952
Iteration 12/25 | Loss: 0.00128926
Iteration 13/25 | Loss: 0.00128922
Iteration 14/25 | Loss: 0.00128922
Iteration 15/25 | Loss: 0.00128921
Iteration 16/25 | Loss: 0.00128921
Iteration 17/25 | Loss: 0.00128921
Iteration 18/25 | Loss: 0.00128921
Iteration 19/25 | Loss: 0.00128921
Iteration 20/25 | Loss: 0.00128921
Iteration 21/25 | Loss: 0.00128921
Iteration 22/25 | Loss: 0.00128921
Iteration 23/25 | Loss: 0.00128921
Iteration 24/25 | Loss: 0.00128921
Iteration 25/25 | Loss: 0.00128921

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90968591
Iteration 2/25 | Loss: 0.00058053
Iteration 3/25 | Loss: 0.00058052
Iteration 4/25 | Loss: 0.00058052
Iteration 5/25 | Loss: 0.00058052
Iteration 6/25 | Loss: 0.00058052
Iteration 7/25 | Loss: 0.00058052
Iteration 8/25 | Loss: 0.00058052
Iteration 9/25 | Loss: 0.00058052
Iteration 10/25 | Loss: 0.00058052
Iteration 11/25 | Loss: 0.00058052
Iteration 12/25 | Loss: 0.00058052
Iteration 13/25 | Loss: 0.00058052
Iteration 14/25 | Loss: 0.00058052
Iteration 15/25 | Loss: 0.00058052
Iteration 16/25 | Loss: 0.00058052
Iteration 17/25 | Loss: 0.00058052
Iteration 18/25 | Loss: 0.00058052
Iteration 19/25 | Loss: 0.00058052
Iteration 20/25 | Loss: 0.00058052
Iteration 21/25 | Loss: 0.00058052
Iteration 22/25 | Loss: 0.00058052
Iteration 23/25 | Loss: 0.00058052
Iteration 24/25 | Loss: 0.00058052
Iteration 25/25 | Loss: 0.00058052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058052
Iteration 2/1000 | Loss: 0.00004398
Iteration 3/1000 | Loss: 0.00003609
Iteration 4/1000 | Loss: 0.00003341
Iteration 5/1000 | Loss: 0.00003198
Iteration 6/1000 | Loss: 0.00003107
Iteration 7/1000 | Loss: 0.00003026
Iteration 8/1000 | Loss: 0.00002971
Iteration 9/1000 | Loss: 0.00002932
Iteration 10/1000 | Loss: 0.00002905
Iteration 11/1000 | Loss: 0.00002902
Iteration 12/1000 | Loss: 0.00002899
Iteration 13/1000 | Loss: 0.00002899
Iteration 14/1000 | Loss: 0.00002885
Iteration 15/1000 | Loss: 0.00002882
Iteration 16/1000 | Loss: 0.00002874
Iteration 17/1000 | Loss: 0.00002861
Iteration 18/1000 | Loss: 0.00002858
Iteration 19/1000 | Loss: 0.00002858
Iteration 20/1000 | Loss: 0.00002858
Iteration 21/1000 | Loss: 0.00002858
Iteration 22/1000 | Loss: 0.00002858
Iteration 23/1000 | Loss: 0.00002858
Iteration 24/1000 | Loss: 0.00002858
Iteration 25/1000 | Loss: 0.00002858
Iteration 26/1000 | Loss: 0.00002858
Iteration 27/1000 | Loss: 0.00002857
Iteration 28/1000 | Loss: 0.00002857
Iteration 29/1000 | Loss: 0.00002857
Iteration 30/1000 | Loss: 0.00002857
Iteration 31/1000 | Loss: 0.00002857
Iteration 32/1000 | Loss: 0.00002856
Iteration 33/1000 | Loss: 0.00002856
Iteration 34/1000 | Loss: 0.00002856
Iteration 35/1000 | Loss: 0.00002856
Iteration 36/1000 | Loss: 0.00002856
Iteration 37/1000 | Loss: 0.00002856
Iteration 38/1000 | Loss: 0.00002856
Iteration 39/1000 | Loss: 0.00002856
Iteration 40/1000 | Loss: 0.00002856
Iteration 41/1000 | Loss: 0.00002856
Iteration 42/1000 | Loss: 0.00002856
Iteration 43/1000 | Loss: 0.00002855
Iteration 44/1000 | Loss: 0.00002855
Iteration 45/1000 | Loss: 0.00002855
Iteration 46/1000 | Loss: 0.00002855
Iteration 47/1000 | Loss: 0.00002855
Iteration 48/1000 | Loss: 0.00002854
Iteration 49/1000 | Loss: 0.00002854
Iteration 50/1000 | Loss: 0.00002854
Iteration 51/1000 | Loss: 0.00002853
Iteration 52/1000 | Loss: 0.00002853
Iteration 53/1000 | Loss: 0.00002853
Iteration 54/1000 | Loss: 0.00002853
Iteration 55/1000 | Loss: 0.00002853
Iteration 56/1000 | Loss: 0.00002853
Iteration 57/1000 | Loss: 0.00002853
Iteration 58/1000 | Loss: 0.00002853
Iteration 59/1000 | Loss: 0.00002852
Iteration 60/1000 | Loss: 0.00002852
Iteration 61/1000 | Loss: 0.00002852
Iteration 62/1000 | Loss: 0.00002852
Iteration 63/1000 | Loss: 0.00002852
Iteration 64/1000 | Loss: 0.00002852
Iteration 65/1000 | Loss: 0.00002851
Iteration 66/1000 | Loss: 0.00002851
Iteration 67/1000 | Loss: 0.00002851
Iteration 68/1000 | Loss: 0.00002851
Iteration 69/1000 | Loss: 0.00002851
Iteration 70/1000 | Loss: 0.00002851
Iteration 71/1000 | Loss: 0.00002851
Iteration 72/1000 | Loss: 0.00002851
Iteration 73/1000 | Loss: 0.00002851
Iteration 74/1000 | Loss: 0.00002850
Iteration 75/1000 | Loss: 0.00002850
Iteration 76/1000 | Loss: 0.00002850
Iteration 77/1000 | Loss: 0.00002850
Iteration 78/1000 | Loss: 0.00002850
Iteration 79/1000 | Loss: 0.00002850
Iteration 80/1000 | Loss: 0.00002850
Iteration 81/1000 | Loss: 0.00002850
Iteration 82/1000 | Loss: 0.00002850
Iteration 83/1000 | Loss: 0.00002850
Iteration 84/1000 | Loss: 0.00002850
Iteration 85/1000 | Loss: 0.00002850
Iteration 86/1000 | Loss: 0.00002850
Iteration 87/1000 | Loss: 0.00002850
Iteration 88/1000 | Loss: 0.00002850
Iteration 89/1000 | Loss: 0.00002850
Iteration 90/1000 | Loss: 0.00002849
Iteration 91/1000 | Loss: 0.00002849
Iteration 92/1000 | Loss: 0.00002849
Iteration 93/1000 | Loss: 0.00002849
Iteration 94/1000 | Loss: 0.00002849
Iteration 95/1000 | Loss: 0.00002849
Iteration 96/1000 | Loss: 0.00002849
Iteration 97/1000 | Loss: 0.00002848
Iteration 98/1000 | Loss: 0.00002848
Iteration 99/1000 | Loss: 0.00002848
Iteration 100/1000 | Loss: 0.00002848
Iteration 101/1000 | Loss: 0.00002848
Iteration 102/1000 | Loss: 0.00002848
Iteration 103/1000 | Loss: 0.00002848
Iteration 104/1000 | Loss: 0.00002848
Iteration 105/1000 | Loss: 0.00002848
Iteration 106/1000 | Loss: 0.00002848
Iteration 107/1000 | Loss: 0.00002848
Iteration 108/1000 | Loss: 0.00002848
Iteration 109/1000 | Loss: 0.00002847
Iteration 110/1000 | Loss: 0.00002847
Iteration 111/1000 | Loss: 0.00002847
Iteration 112/1000 | Loss: 0.00002847
Iteration 113/1000 | Loss: 0.00002847
Iteration 114/1000 | Loss: 0.00002846
Iteration 115/1000 | Loss: 0.00002846
Iteration 116/1000 | Loss: 0.00002846
Iteration 117/1000 | Loss: 0.00002846
Iteration 118/1000 | Loss: 0.00002846
Iteration 119/1000 | Loss: 0.00002846
Iteration 120/1000 | Loss: 0.00002846
Iteration 121/1000 | Loss: 0.00002846
Iteration 122/1000 | Loss: 0.00002846
Iteration 123/1000 | Loss: 0.00002846
Iteration 124/1000 | Loss: 0.00002846
Iteration 125/1000 | Loss: 0.00002846
Iteration 126/1000 | Loss: 0.00002846
Iteration 127/1000 | Loss: 0.00002846
Iteration 128/1000 | Loss: 0.00002846
Iteration 129/1000 | Loss: 0.00002846
Iteration 130/1000 | Loss: 0.00002846
Iteration 131/1000 | Loss: 0.00002846
Iteration 132/1000 | Loss: 0.00002846
Iteration 133/1000 | Loss: 0.00002846
Iteration 134/1000 | Loss: 0.00002846
Iteration 135/1000 | Loss: 0.00002846
Iteration 136/1000 | Loss: 0.00002846
Iteration 137/1000 | Loss: 0.00002846
Iteration 138/1000 | Loss: 0.00002846
Iteration 139/1000 | Loss: 0.00002846
Iteration 140/1000 | Loss: 0.00002846
Iteration 141/1000 | Loss: 0.00002846
Iteration 142/1000 | Loss: 0.00002846
Iteration 143/1000 | Loss: 0.00002846
Iteration 144/1000 | Loss: 0.00002846
Iteration 145/1000 | Loss: 0.00002846
Iteration 146/1000 | Loss: 0.00002846
Iteration 147/1000 | Loss: 0.00002846
Iteration 148/1000 | Loss: 0.00002846
Iteration 149/1000 | Loss: 0.00002846
Iteration 150/1000 | Loss: 0.00002846
Iteration 151/1000 | Loss: 0.00002846
Iteration 152/1000 | Loss: 0.00002846
Iteration 153/1000 | Loss: 0.00002846
Iteration 154/1000 | Loss: 0.00002846
Iteration 155/1000 | Loss: 0.00002846
Iteration 156/1000 | Loss: 0.00002846
Iteration 157/1000 | Loss: 0.00002846
Iteration 158/1000 | Loss: 0.00002846
Iteration 159/1000 | Loss: 0.00002846
Iteration 160/1000 | Loss: 0.00002846
Iteration 161/1000 | Loss: 0.00002846
Iteration 162/1000 | Loss: 0.00002846
Iteration 163/1000 | Loss: 0.00002846
Iteration 164/1000 | Loss: 0.00002846
Iteration 165/1000 | Loss: 0.00002846
Iteration 166/1000 | Loss: 0.00002846
Iteration 167/1000 | Loss: 0.00002846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [2.8455877327360213e-05, 2.8455877327360213e-05, 2.8455877327360213e-05, 2.8455877327360213e-05, 2.8455877327360213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8455877327360213e-05

Optimization complete. Final v2v error: 4.48613166809082 mm

Highest mean error: 4.742244243621826 mm for frame 148

Lowest mean error: 4.371153831481934 mm for frame 36

Saving results

Total time: 49.427600145339966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00725128
Iteration 2/25 | Loss: 0.00137701
Iteration 3/25 | Loss: 0.00125617
Iteration 4/25 | Loss: 0.00122023
Iteration 5/25 | Loss: 0.00120167
Iteration 6/25 | Loss: 0.00120524
Iteration 7/25 | Loss: 0.00118944
Iteration 8/25 | Loss: 0.00118775
Iteration 9/25 | Loss: 0.00118805
Iteration 10/25 | Loss: 0.00118615
Iteration 11/25 | Loss: 0.00118483
Iteration 12/25 | Loss: 0.00118825
Iteration 13/25 | Loss: 0.00118667
Iteration 14/25 | Loss: 0.00118530
Iteration 15/25 | Loss: 0.00118491
Iteration 16/25 | Loss: 0.00118475
Iteration 17/25 | Loss: 0.00118429
Iteration 18/25 | Loss: 0.00118372
Iteration 19/25 | Loss: 0.00118357
Iteration 20/25 | Loss: 0.00118348
Iteration 21/25 | Loss: 0.00118345
Iteration 22/25 | Loss: 0.00118345
Iteration 23/25 | Loss: 0.00118344
Iteration 24/25 | Loss: 0.00118344
Iteration 25/25 | Loss: 0.00118343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.70605040
Iteration 2/25 | Loss: 0.00134468
Iteration 3/25 | Loss: 0.00134464
Iteration 4/25 | Loss: 0.00134464
Iteration 5/25 | Loss: 0.00134464
Iteration 6/25 | Loss: 0.00134464
Iteration 7/25 | Loss: 0.00134464
Iteration 8/25 | Loss: 0.00134464
Iteration 9/25 | Loss: 0.00134464
Iteration 10/25 | Loss: 0.00134464
Iteration 11/25 | Loss: 0.00134464
Iteration 12/25 | Loss: 0.00134464
Iteration 13/25 | Loss: 0.00134464
Iteration 14/25 | Loss: 0.00134464
Iteration 15/25 | Loss: 0.00134464
Iteration 16/25 | Loss: 0.00134464
Iteration 17/25 | Loss: 0.00134464
Iteration 18/25 | Loss: 0.00134464
Iteration 19/25 | Loss: 0.00134464
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013446396915242076, 0.0013446396915242076, 0.0013446396915242076, 0.0013446396915242076, 0.0013446396915242076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013446396915242076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134464
Iteration 2/1000 | Loss: 0.00004114
Iteration 3/1000 | Loss: 0.00002736
Iteration 4/1000 | Loss: 0.00001986
Iteration 5/1000 | Loss: 0.00001846
Iteration 6/1000 | Loss: 0.00001730
Iteration 7/1000 | Loss: 0.00001656
Iteration 8/1000 | Loss: 0.00001600
Iteration 9/1000 | Loss: 0.00001566
Iteration 10/1000 | Loss: 0.00001542
Iteration 11/1000 | Loss: 0.00001517
Iteration 12/1000 | Loss: 0.00001515
Iteration 13/1000 | Loss: 0.00001495
Iteration 14/1000 | Loss: 0.00001476
Iteration 15/1000 | Loss: 0.00001462
Iteration 16/1000 | Loss: 0.00001460
Iteration 17/1000 | Loss: 0.00001446
Iteration 18/1000 | Loss: 0.00001446
Iteration 19/1000 | Loss: 0.00001445
Iteration 20/1000 | Loss: 0.00001442
Iteration 21/1000 | Loss: 0.00001442
Iteration 22/1000 | Loss: 0.00001441
Iteration 23/1000 | Loss: 0.00001439
Iteration 24/1000 | Loss: 0.00001437
Iteration 25/1000 | Loss: 0.00001435
Iteration 26/1000 | Loss: 0.00001435
Iteration 27/1000 | Loss: 0.00001435
Iteration 28/1000 | Loss: 0.00001434
Iteration 29/1000 | Loss: 0.00001431
Iteration 30/1000 | Loss: 0.00001430
Iteration 31/1000 | Loss: 0.00001430
Iteration 32/1000 | Loss: 0.00001430
Iteration 33/1000 | Loss: 0.00001423
Iteration 34/1000 | Loss: 0.00001422
Iteration 35/1000 | Loss: 0.00003905
Iteration 36/1000 | Loss: 0.00002032
Iteration 37/1000 | Loss: 0.00001842
Iteration 38/1000 | Loss: 0.00002780
Iteration 39/1000 | Loss: 0.00003774
Iteration 40/1000 | Loss: 0.00002770
Iteration 41/1000 | Loss: 0.00003669
Iteration 42/1000 | Loss: 0.00002816
Iteration 43/1000 | Loss: 0.00001930
Iteration 44/1000 | Loss: 0.00001612
Iteration 45/1000 | Loss: 0.00003597
Iteration 46/1000 | Loss: 0.00025560
Iteration 47/1000 | Loss: 0.00001851
Iteration 48/1000 | Loss: 0.00001654
Iteration 49/1000 | Loss: 0.00001569
Iteration 50/1000 | Loss: 0.00001500
Iteration 51/1000 | Loss: 0.00001460
Iteration 52/1000 | Loss: 0.00001432
Iteration 53/1000 | Loss: 0.00001412
Iteration 54/1000 | Loss: 0.00001396
Iteration 55/1000 | Loss: 0.00001395
Iteration 56/1000 | Loss: 0.00001391
Iteration 57/1000 | Loss: 0.00001390
Iteration 58/1000 | Loss: 0.00001388
Iteration 59/1000 | Loss: 0.00001387
Iteration 60/1000 | Loss: 0.00001386
Iteration 61/1000 | Loss: 0.00001386
Iteration 62/1000 | Loss: 0.00001385
Iteration 63/1000 | Loss: 0.00001383
Iteration 64/1000 | Loss: 0.00001383
Iteration 65/1000 | Loss: 0.00001382
Iteration 66/1000 | Loss: 0.00001382
Iteration 67/1000 | Loss: 0.00001382
Iteration 68/1000 | Loss: 0.00001382
Iteration 69/1000 | Loss: 0.00001382
Iteration 70/1000 | Loss: 0.00001382
Iteration 71/1000 | Loss: 0.00001382
Iteration 72/1000 | Loss: 0.00001381
Iteration 73/1000 | Loss: 0.00001381
Iteration 74/1000 | Loss: 0.00001381
Iteration 75/1000 | Loss: 0.00001380
Iteration 76/1000 | Loss: 0.00001380
Iteration 77/1000 | Loss: 0.00001380
Iteration 78/1000 | Loss: 0.00001379
Iteration 79/1000 | Loss: 0.00001379
Iteration 80/1000 | Loss: 0.00001379
Iteration 81/1000 | Loss: 0.00001379
Iteration 82/1000 | Loss: 0.00001379
Iteration 83/1000 | Loss: 0.00001378
Iteration 84/1000 | Loss: 0.00001378
Iteration 85/1000 | Loss: 0.00001378
Iteration 86/1000 | Loss: 0.00001378
Iteration 87/1000 | Loss: 0.00001378
Iteration 88/1000 | Loss: 0.00001378
Iteration 89/1000 | Loss: 0.00001378
Iteration 90/1000 | Loss: 0.00001378
Iteration 91/1000 | Loss: 0.00001378
Iteration 92/1000 | Loss: 0.00001378
Iteration 93/1000 | Loss: 0.00001378
Iteration 94/1000 | Loss: 0.00001377
Iteration 95/1000 | Loss: 0.00001377
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00001377
Iteration 98/1000 | Loss: 0.00001377
Iteration 99/1000 | Loss: 0.00001376
Iteration 100/1000 | Loss: 0.00001376
Iteration 101/1000 | Loss: 0.00001376
Iteration 102/1000 | Loss: 0.00001376
Iteration 103/1000 | Loss: 0.00001376
Iteration 104/1000 | Loss: 0.00001375
Iteration 105/1000 | Loss: 0.00001375
Iteration 106/1000 | Loss: 0.00001375
Iteration 107/1000 | Loss: 0.00001375
Iteration 108/1000 | Loss: 0.00001375
Iteration 109/1000 | Loss: 0.00001374
Iteration 110/1000 | Loss: 0.00001374
Iteration 111/1000 | Loss: 0.00001374
Iteration 112/1000 | Loss: 0.00001374
Iteration 113/1000 | Loss: 0.00001374
Iteration 114/1000 | Loss: 0.00001374
Iteration 115/1000 | Loss: 0.00001373
Iteration 116/1000 | Loss: 0.00001373
Iteration 117/1000 | Loss: 0.00001373
Iteration 118/1000 | Loss: 0.00001373
Iteration 119/1000 | Loss: 0.00001372
Iteration 120/1000 | Loss: 0.00001372
Iteration 121/1000 | Loss: 0.00001372
Iteration 122/1000 | Loss: 0.00001372
Iteration 123/1000 | Loss: 0.00001371
Iteration 124/1000 | Loss: 0.00001371
Iteration 125/1000 | Loss: 0.00001371
Iteration 126/1000 | Loss: 0.00001371
Iteration 127/1000 | Loss: 0.00001371
Iteration 128/1000 | Loss: 0.00001371
Iteration 129/1000 | Loss: 0.00001371
Iteration 130/1000 | Loss: 0.00001371
Iteration 131/1000 | Loss: 0.00001370
Iteration 132/1000 | Loss: 0.00001370
Iteration 133/1000 | Loss: 0.00001370
Iteration 134/1000 | Loss: 0.00001370
Iteration 135/1000 | Loss: 0.00001370
Iteration 136/1000 | Loss: 0.00001370
Iteration 137/1000 | Loss: 0.00001370
Iteration 138/1000 | Loss: 0.00001370
Iteration 139/1000 | Loss: 0.00001370
Iteration 140/1000 | Loss: 0.00001370
Iteration 141/1000 | Loss: 0.00001369
Iteration 142/1000 | Loss: 0.00001369
Iteration 143/1000 | Loss: 0.00001369
Iteration 144/1000 | Loss: 0.00001369
Iteration 145/1000 | Loss: 0.00001369
Iteration 146/1000 | Loss: 0.00001369
Iteration 147/1000 | Loss: 0.00001369
Iteration 148/1000 | Loss: 0.00001369
Iteration 149/1000 | Loss: 0.00001369
Iteration 150/1000 | Loss: 0.00001369
Iteration 151/1000 | Loss: 0.00001369
Iteration 152/1000 | Loss: 0.00001369
Iteration 153/1000 | Loss: 0.00001369
Iteration 154/1000 | Loss: 0.00001369
Iteration 155/1000 | Loss: 0.00001369
Iteration 156/1000 | Loss: 0.00001369
Iteration 157/1000 | Loss: 0.00001369
Iteration 158/1000 | Loss: 0.00001369
Iteration 159/1000 | Loss: 0.00001369
Iteration 160/1000 | Loss: 0.00001369
Iteration 161/1000 | Loss: 0.00001369
Iteration 162/1000 | Loss: 0.00001369
Iteration 163/1000 | Loss: 0.00001369
Iteration 164/1000 | Loss: 0.00001369
Iteration 165/1000 | Loss: 0.00001369
Iteration 166/1000 | Loss: 0.00001369
Iteration 167/1000 | Loss: 0.00001369
Iteration 168/1000 | Loss: 0.00001369
Iteration 169/1000 | Loss: 0.00001369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.3692891116079409e-05, 1.3692891116079409e-05, 1.3692891116079409e-05, 1.3692891116079409e-05, 1.3692891116079409e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3692891116079409e-05

Optimization complete. Final v2v error: 3.1656594276428223 mm

Highest mean error: 3.7440128326416016 mm for frame 20

Lowest mean error: 2.8659377098083496 mm for frame 128

Saving results

Total time: 98.74067521095276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804471
Iteration 2/25 | Loss: 0.00132380
Iteration 3/25 | Loss: 0.00123870
Iteration 4/25 | Loss: 0.00121865
Iteration 5/25 | Loss: 0.00121261
Iteration 6/25 | Loss: 0.00121075
Iteration 7/25 | Loss: 0.00121043
Iteration 8/25 | Loss: 0.00121043
Iteration 9/25 | Loss: 0.00121043
Iteration 10/25 | Loss: 0.00121043
Iteration 11/25 | Loss: 0.00121043
Iteration 12/25 | Loss: 0.00121043
Iteration 13/25 | Loss: 0.00121043
Iteration 14/25 | Loss: 0.00121043
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012104292400181293, 0.0012104292400181293, 0.0012104292400181293, 0.0012104292400181293, 0.0012104292400181293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012104292400181293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58591223
Iteration 2/25 | Loss: 0.00113353
Iteration 3/25 | Loss: 0.00113353
Iteration 4/25 | Loss: 0.00113353
Iteration 5/25 | Loss: 0.00113353
Iteration 6/25 | Loss: 0.00113353
Iteration 7/25 | Loss: 0.00113353
Iteration 8/25 | Loss: 0.00113353
Iteration 9/25 | Loss: 0.00113353
Iteration 10/25 | Loss: 0.00113353
Iteration 11/25 | Loss: 0.00113353
Iteration 12/25 | Loss: 0.00113353
Iteration 13/25 | Loss: 0.00113353
Iteration 14/25 | Loss: 0.00113353
Iteration 15/25 | Loss: 0.00113353
Iteration 16/25 | Loss: 0.00113353
Iteration 17/25 | Loss: 0.00113353
Iteration 18/25 | Loss: 0.00113353
Iteration 19/25 | Loss: 0.00113353
Iteration 20/25 | Loss: 0.00113353
Iteration 21/25 | Loss: 0.00113353
Iteration 22/25 | Loss: 0.00113353
Iteration 23/25 | Loss: 0.00113353
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011335270246490836, 0.0011335270246490836, 0.0011335270246490836, 0.0011335270246490836, 0.0011335270246490836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011335270246490836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113353
Iteration 2/1000 | Loss: 0.00005497
Iteration 3/1000 | Loss: 0.00003979
Iteration 4/1000 | Loss: 0.00003126
Iteration 5/1000 | Loss: 0.00002866
Iteration 6/1000 | Loss: 0.00002663
Iteration 7/1000 | Loss: 0.00002541
Iteration 8/1000 | Loss: 0.00002460
Iteration 9/1000 | Loss: 0.00002417
Iteration 10/1000 | Loss: 0.00002373
Iteration 11/1000 | Loss: 0.00002340
Iteration 12/1000 | Loss: 0.00002327
Iteration 13/1000 | Loss: 0.00002307
Iteration 14/1000 | Loss: 0.00002296
Iteration 15/1000 | Loss: 0.00002295
Iteration 16/1000 | Loss: 0.00002279
Iteration 17/1000 | Loss: 0.00002272
Iteration 18/1000 | Loss: 0.00002271
Iteration 19/1000 | Loss: 0.00002271
Iteration 20/1000 | Loss: 0.00002263
Iteration 21/1000 | Loss: 0.00002259
Iteration 22/1000 | Loss: 0.00002258
Iteration 23/1000 | Loss: 0.00002257
Iteration 24/1000 | Loss: 0.00002256
Iteration 25/1000 | Loss: 0.00002254
Iteration 26/1000 | Loss: 0.00002252
Iteration 27/1000 | Loss: 0.00002251
Iteration 28/1000 | Loss: 0.00002251
Iteration 29/1000 | Loss: 0.00002248
Iteration 30/1000 | Loss: 0.00002247
Iteration 31/1000 | Loss: 0.00002246
Iteration 32/1000 | Loss: 0.00002246
Iteration 33/1000 | Loss: 0.00002245
Iteration 34/1000 | Loss: 0.00002244
Iteration 35/1000 | Loss: 0.00002242
Iteration 36/1000 | Loss: 0.00002239
Iteration 37/1000 | Loss: 0.00002237
Iteration 38/1000 | Loss: 0.00002236
Iteration 39/1000 | Loss: 0.00002233
Iteration 40/1000 | Loss: 0.00002232
Iteration 41/1000 | Loss: 0.00002232
Iteration 42/1000 | Loss: 0.00002231
Iteration 43/1000 | Loss: 0.00002231
Iteration 44/1000 | Loss: 0.00002230
Iteration 45/1000 | Loss: 0.00002230
Iteration 46/1000 | Loss: 0.00002228
Iteration 47/1000 | Loss: 0.00002228
Iteration 48/1000 | Loss: 0.00002228
Iteration 49/1000 | Loss: 0.00002228
Iteration 50/1000 | Loss: 0.00002228
Iteration 51/1000 | Loss: 0.00002228
Iteration 52/1000 | Loss: 0.00002228
Iteration 53/1000 | Loss: 0.00002227
Iteration 54/1000 | Loss: 0.00002227
Iteration 55/1000 | Loss: 0.00002225
Iteration 56/1000 | Loss: 0.00002225
Iteration 57/1000 | Loss: 0.00002225
Iteration 58/1000 | Loss: 0.00002225
Iteration 59/1000 | Loss: 0.00002224
Iteration 60/1000 | Loss: 0.00002224
Iteration 61/1000 | Loss: 0.00002223
Iteration 62/1000 | Loss: 0.00002223
Iteration 63/1000 | Loss: 0.00002222
Iteration 64/1000 | Loss: 0.00002222
Iteration 65/1000 | Loss: 0.00002221
Iteration 66/1000 | Loss: 0.00002221
Iteration 67/1000 | Loss: 0.00002221
Iteration 68/1000 | Loss: 0.00002220
Iteration 69/1000 | Loss: 0.00002220
Iteration 70/1000 | Loss: 0.00002220
Iteration 71/1000 | Loss: 0.00002219
Iteration 72/1000 | Loss: 0.00002219
Iteration 73/1000 | Loss: 0.00002218
Iteration 74/1000 | Loss: 0.00002218
Iteration 75/1000 | Loss: 0.00002217
Iteration 76/1000 | Loss: 0.00002217
Iteration 77/1000 | Loss: 0.00002217
Iteration 78/1000 | Loss: 0.00002216
Iteration 79/1000 | Loss: 0.00002216
Iteration 80/1000 | Loss: 0.00002216
Iteration 81/1000 | Loss: 0.00002216
Iteration 82/1000 | Loss: 0.00002216
Iteration 83/1000 | Loss: 0.00002215
Iteration 84/1000 | Loss: 0.00002215
Iteration 85/1000 | Loss: 0.00002215
Iteration 86/1000 | Loss: 0.00002215
Iteration 87/1000 | Loss: 0.00002215
Iteration 88/1000 | Loss: 0.00002215
Iteration 89/1000 | Loss: 0.00002215
Iteration 90/1000 | Loss: 0.00002215
Iteration 91/1000 | Loss: 0.00002214
Iteration 92/1000 | Loss: 0.00002214
Iteration 93/1000 | Loss: 0.00002214
Iteration 94/1000 | Loss: 0.00002214
Iteration 95/1000 | Loss: 0.00002214
Iteration 96/1000 | Loss: 0.00002213
Iteration 97/1000 | Loss: 0.00002213
Iteration 98/1000 | Loss: 0.00002213
Iteration 99/1000 | Loss: 0.00002213
Iteration 100/1000 | Loss: 0.00002212
Iteration 101/1000 | Loss: 0.00002212
Iteration 102/1000 | Loss: 0.00002211
Iteration 103/1000 | Loss: 0.00002211
Iteration 104/1000 | Loss: 0.00002211
Iteration 105/1000 | Loss: 0.00002211
Iteration 106/1000 | Loss: 0.00002210
Iteration 107/1000 | Loss: 0.00002210
Iteration 108/1000 | Loss: 0.00002210
Iteration 109/1000 | Loss: 0.00002210
Iteration 110/1000 | Loss: 0.00002209
Iteration 111/1000 | Loss: 0.00002209
Iteration 112/1000 | Loss: 0.00002209
Iteration 113/1000 | Loss: 0.00002209
Iteration 114/1000 | Loss: 0.00002208
Iteration 115/1000 | Loss: 0.00002208
Iteration 116/1000 | Loss: 0.00002208
Iteration 117/1000 | Loss: 0.00002208
Iteration 118/1000 | Loss: 0.00002208
Iteration 119/1000 | Loss: 0.00002208
Iteration 120/1000 | Loss: 0.00002207
Iteration 121/1000 | Loss: 0.00002207
Iteration 122/1000 | Loss: 0.00002207
Iteration 123/1000 | Loss: 0.00002207
Iteration 124/1000 | Loss: 0.00002206
Iteration 125/1000 | Loss: 0.00002206
Iteration 126/1000 | Loss: 0.00002206
Iteration 127/1000 | Loss: 0.00002206
Iteration 128/1000 | Loss: 0.00002206
Iteration 129/1000 | Loss: 0.00002206
Iteration 130/1000 | Loss: 0.00002206
Iteration 131/1000 | Loss: 0.00002206
Iteration 132/1000 | Loss: 0.00002206
Iteration 133/1000 | Loss: 0.00002206
Iteration 134/1000 | Loss: 0.00002205
Iteration 135/1000 | Loss: 0.00002205
Iteration 136/1000 | Loss: 0.00002205
Iteration 137/1000 | Loss: 0.00002205
Iteration 138/1000 | Loss: 0.00002205
Iteration 139/1000 | Loss: 0.00002205
Iteration 140/1000 | Loss: 0.00002204
Iteration 141/1000 | Loss: 0.00002204
Iteration 142/1000 | Loss: 0.00002204
Iteration 143/1000 | Loss: 0.00002204
Iteration 144/1000 | Loss: 0.00002204
Iteration 145/1000 | Loss: 0.00002204
Iteration 146/1000 | Loss: 0.00002204
Iteration 147/1000 | Loss: 0.00002204
Iteration 148/1000 | Loss: 0.00002204
Iteration 149/1000 | Loss: 0.00002204
Iteration 150/1000 | Loss: 0.00002204
Iteration 151/1000 | Loss: 0.00002204
Iteration 152/1000 | Loss: 0.00002204
Iteration 153/1000 | Loss: 0.00002204
Iteration 154/1000 | Loss: 0.00002204
Iteration 155/1000 | Loss: 0.00002204
Iteration 156/1000 | Loss: 0.00002204
Iteration 157/1000 | Loss: 0.00002204
Iteration 158/1000 | Loss: 0.00002204
Iteration 159/1000 | Loss: 0.00002204
Iteration 160/1000 | Loss: 0.00002204
Iteration 161/1000 | Loss: 0.00002204
Iteration 162/1000 | Loss: 0.00002204
Iteration 163/1000 | Loss: 0.00002204
Iteration 164/1000 | Loss: 0.00002204
Iteration 165/1000 | Loss: 0.00002204
Iteration 166/1000 | Loss: 0.00002204
Iteration 167/1000 | Loss: 0.00002204
Iteration 168/1000 | Loss: 0.00002204
Iteration 169/1000 | Loss: 0.00002204
Iteration 170/1000 | Loss: 0.00002204
Iteration 171/1000 | Loss: 0.00002204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.2044385332264937e-05, 2.2044385332264937e-05, 2.2044385332264937e-05, 2.2044385332264937e-05, 2.2044385332264937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2044385332264937e-05

Optimization complete. Final v2v error: 3.8726449012756348 mm

Highest mean error: 5.9338884353637695 mm for frame 136

Lowest mean error: 2.8040578365325928 mm for frame 83

Saving results

Total time: 44.30256938934326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843862
Iteration 2/25 | Loss: 0.00153637
Iteration 3/25 | Loss: 0.00128521
Iteration 4/25 | Loss: 0.00125729
Iteration 5/25 | Loss: 0.00125260
Iteration 6/25 | Loss: 0.00125220
Iteration 7/25 | Loss: 0.00125220
Iteration 8/25 | Loss: 0.00125220
Iteration 9/25 | Loss: 0.00125220
Iteration 10/25 | Loss: 0.00125220
Iteration 11/25 | Loss: 0.00125220
Iteration 12/25 | Loss: 0.00125220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012521966127678752, 0.0012521966127678752, 0.0012521966127678752, 0.0012521966127678752, 0.0012521966127678752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012521966127678752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91866380
Iteration 2/25 | Loss: 0.00061699
Iteration 3/25 | Loss: 0.00061698
Iteration 4/25 | Loss: 0.00061698
Iteration 5/25 | Loss: 0.00061698
Iteration 6/25 | Loss: 0.00061698
Iteration 7/25 | Loss: 0.00061698
Iteration 8/25 | Loss: 0.00061698
Iteration 9/25 | Loss: 0.00061698
Iteration 10/25 | Loss: 0.00061698
Iteration 11/25 | Loss: 0.00061698
Iteration 12/25 | Loss: 0.00061698
Iteration 13/25 | Loss: 0.00061698
Iteration 14/25 | Loss: 0.00061698
Iteration 15/25 | Loss: 0.00061698
Iteration 16/25 | Loss: 0.00061698
Iteration 17/25 | Loss: 0.00061698
Iteration 18/25 | Loss: 0.00061698
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006169769330881536, 0.0006169769330881536, 0.0006169769330881536, 0.0006169769330881536, 0.0006169769330881536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006169769330881536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061698
Iteration 2/1000 | Loss: 0.00003422
Iteration 3/1000 | Loss: 0.00002701
Iteration 4/1000 | Loss: 0.00002491
Iteration 5/1000 | Loss: 0.00002347
Iteration 6/1000 | Loss: 0.00002273
Iteration 7/1000 | Loss: 0.00002219
Iteration 8/1000 | Loss: 0.00002165
Iteration 9/1000 | Loss: 0.00002121
Iteration 10/1000 | Loss: 0.00002099
Iteration 11/1000 | Loss: 0.00002071
Iteration 12/1000 | Loss: 0.00002065
Iteration 13/1000 | Loss: 0.00002051
Iteration 14/1000 | Loss: 0.00002048
Iteration 15/1000 | Loss: 0.00002047
Iteration 16/1000 | Loss: 0.00002046
Iteration 17/1000 | Loss: 0.00002042
Iteration 18/1000 | Loss: 0.00002036
Iteration 19/1000 | Loss: 0.00002030
Iteration 20/1000 | Loss: 0.00002028
Iteration 21/1000 | Loss: 0.00002027
Iteration 22/1000 | Loss: 0.00002027
Iteration 23/1000 | Loss: 0.00002021
Iteration 24/1000 | Loss: 0.00002021
Iteration 25/1000 | Loss: 0.00002021
Iteration 26/1000 | Loss: 0.00002021
Iteration 27/1000 | Loss: 0.00002018
Iteration 28/1000 | Loss: 0.00002018
Iteration 29/1000 | Loss: 0.00002018
Iteration 30/1000 | Loss: 0.00002018
Iteration 31/1000 | Loss: 0.00002018
Iteration 32/1000 | Loss: 0.00002018
Iteration 33/1000 | Loss: 0.00002018
Iteration 34/1000 | Loss: 0.00002018
Iteration 35/1000 | Loss: 0.00002017
Iteration 36/1000 | Loss: 0.00002017
Iteration 37/1000 | Loss: 0.00002017
Iteration 38/1000 | Loss: 0.00002017
Iteration 39/1000 | Loss: 0.00002017
Iteration 40/1000 | Loss: 0.00002017
Iteration 41/1000 | Loss: 0.00002017
Iteration 42/1000 | Loss: 0.00002016
Iteration 43/1000 | Loss: 0.00002016
Iteration 44/1000 | Loss: 0.00002016
Iteration 45/1000 | Loss: 0.00002015
Iteration 46/1000 | Loss: 0.00002015
Iteration 47/1000 | Loss: 0.00002015
Iteration 48/1000 | Loss: 0.00002015
Iteration 49/1000 | Loss: 0.00002015
Iteration 50/1000 | Loss: 0.00002015
Iteration 51/1000 | Loss: 0.00002015
Iteration 52/1000 | Loss: 0.00002015
Iteration 53/1000 | Loss: 0.00002014
Iteration 54/1000 | Loss: 0.00002014
Iteration 55/1000 | Loss: 0.00002013
Iteration 56/1000 | Loss: 0.00002013
Iteration 57/1000 | Loss: 0.00002012
Iteration 58/1000 | Loss: 0.00002012
Iteration 59/1000 | Loss: 0.00002012
Iteration 60/1000 | Loss: 0.00002012
Iteration 61/1000 | Loss: 0.00002012
Iteration 62/1000 | Loss: 0.00002012
Iteration 63/1000 | Loss: 0.00002012
Iteration 64/1000 | Loss: 0.00002012
Iteration 65/1000 | Loss: 0.00002012
Iteration 66/1000 | Loss: 0.00002012
Iteration 67/1000 | Loss: 0.00002012
Iteration 68/1000 | Loss: 0.00002011
Iteration 69/1000 | Loss: 0.00002011
Iteration 70/1000 | Loss: 0.00002010
Iteration 71/1000 | Loss: 0.00002010
Iteration 72/1000 | Loss: 0.00002009
Iteration 73/1000 | Loss: 0.00002009
Iteration 74/1000 | Loss: 0.00002009
Iteration 75/1000 | Loss: 0.00002009
Iteration 76/1000 | Loss: 0.00002009
Iteration 77/1000 | Loss: 0.00002009
Iteration 78/1000 | Loss: 0.00002009
Iteration 79/1000 | Loss: 0.00002009
Iteration 80/1000 | Loss: 0.00002009
Iteration 81/1000 | Loss: 0.00002009
Iteration 82/1000 | Loss: 0.00002009
Iteration 83/1000 | Loss: 0.00002009
Iteration 84/1000 | Loss: 0.00002009
Iteration 85/1000 | Loss: 0.00002009
Iteration 86/1000 | Loss: 0.00002008
Iteration 87/1000 | Loss: 0.00002007
Iteration 88/1000 | Loss: 0.00002007
Iteration 89/1000 | Loss: 0.00002007
Iteration 90/1000 | Loss: 0.00002007
Iteration 91/1000 | Loss: 0.00002006
Iteration 92/1000 | Loss: 0.00002006
Iteration 93/1000 | Loss: 0.00002006
Iteration 94/1000 | Loss: 0.00002006
Iteration 95/1000 | Loss: 0.00002005
Iteration 96/1000 | Loss: 0.00002005
Iteration 97/1000 | Loss: 0.00002005
Iteration 98/1000 | Loss: 0.00002005
Iteration 99/1000 | Loss: 0.00002005
Iteration 100/1000 | Loss: 0.00002005
Iteration 101/1000 | Loss: 0.00002005
Iteration 102/1000 | Loss: 0.00002005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [2.0050760213052854e-05, 2.0050760213052854e-05, 2.0050760213052854e-05, 2.0050760213052854e-05, 2.0050760213052854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0050760213052854e-05

Optimization complete. Final v2v error: 3.7603867053985596 mm

Highest mean error: 3.898446798324585 mm for frame 126

Lowest mean error: 3.679213285446167 mm for frame 50

Saving results

Total time: 39.062989473342896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409638
Iteration 2/25 | Loss: 0.00131249
Iteration 3/25 | Loss: 0.00121653
Iteration 4/25 | Loss: 0.00120210
Iteration 5/25 | Loss: 0.00119741
Iteration 6/25 | Loss: 0.00119630
Iteration 7/25 | Loss: 0.00119630
Iteration 8/25 | Loss: 0.00119630
Iteration 9/25 | Loss: 0.00119630
Iteration 10/25 | Loss: 0.00119630
Iteration 11/25 | Loss: 0.00119630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011963043361902237, 0.0011963043361902237, 0.0011963043361902237, 0.0011963043361902237, 0.0011963043361902237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011963043361902237

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.05317211
Iteration 2/25 | Loss: 0.00100412
Iteration 3/25 | Loss: 0.00100410
Iteration 4/25 | Loss: 0.00100410
Iteration 5/25 | Loss: 0.00100410
Iteration 6/25 | Loss: 0.00100410
Iteration 7/25 | Loss: 0.00100410
Iteration 8/25 | Loss: 0.00100410
Iteration 9/25 | Loss: 0.00100410
Iteration 10/25 | Loss: 0.00100410
Iteration 11/25 | Loss: 0.00100410
Iteration 12/25 | Loss: 0.00100410
Iteration 13/25 | Loss: 0.00100410
Iteration 14/25 | Loss: 0.00100410
Iteration 15/25 | Loss: 0.00100410
Iteration 16/25 | Loss: 0.00100410
Iteration 17/25 | Loss: 0.00100410
Iteration 18/25 | Loss: 0.00100410
Iteration 19/25 | Loss: 0.00100410
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010041005443781614, 0.0010041005443781614, 0.0010041005443781614, 0.0010041005443781614, 0.0010041005443781614]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010041005443781614

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100410
Iteration 2/1000 | Loss: 0.00003096
Iteration 3/1000 | Loss: 0.00002118
Iteration 4/1000 | Loss: 0.00001724
Iteration 5/1000 | Loss: 0.00001591
Iteration 6/1000 | Loss: 0.00001512
Iteration 7/1000 | Loss: 0.00001444
Iteration 8/1000 | Loss: 0.00001403
Iteration 9/1000 | Loss: 0.00001372
Iteration 10/1000 | Loss: 0.00001353
Iteration 11/1000 | Loss: 0.00001326
Iteration 12/1000 | Loss: 0.00001304
Iteration 13/1000 | Loss: 0.00001295
Iteration 14/1000 | Loss: 0.00001289
Iteration 15/1000 | Loss: 0.00001288
Iteration 16/1000 | Loss: 0.00001283
Iteration 17/1000 | Loss: 0.00001274
Iteration 18/1000 | Loss: 0.00001267
Iteration 19/1000 | Loss: 0.00001265
Iteration 20/1000 | Loss: 0.00001264
Iteration 21/1000 | Loss: 0.00001263
Iteration 22/1000 | Loss: 0.00001263
Iteration 23/1000 | Loss: 0.00001259
Iteration 24/1000 | Loss: 0.00001258
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001257
Iteration 27/1000 | Loss: 0.00001257
Iteration 28/1000 | Loss: 0.00001257
Iteration 29/1000 | Loss: 0.00001256
Iteration 30/1000 | Loss: 0.00001256
Iteration 31/1000 | Loss: 0.00001255
Iteration 32/1000 | Loss: 0.00001254
Iteration 33/1000 | Loss: 0.00001254
Iteration 34/1000 | Loss: 0.00001253
Iteration 35/1000 | Loss: 0.00001253
Iteration 36/1000 | Loss: 0.00001252
Iteration 37/1000 | Loss: 0.00001252
Iteration 38/1000 | Loss: 0.00001252
Iteration 39/1000 | Loss: 0.00001251
Iteration 40/1000 | Loss: 0.00001251
Iteration 41/1000 | Loss: 0.00001251
Iteration 42/1000 | Loss: 0.00001251
Iteration 43/1000 | Loss: 0.00001250
Iteration 44/1000 | Loss: 0.00001250
Iteration 45/1000 | Loss: 0.00001248
Iteration 46/1000 | Loss: 0.00001248
Iteration 47/1000 | Loss: 0.00001247
Iteration 48/1000 | Loss: 0.00001247
Iteration 49/1000 | Loss: 0.00001247
Iteration 50/1000 | Loss: 0.00001246
Iteration 51/1000 | Loss: 0.00001246
Iteration 52/1000 | Loss: 0.00001246
Iteration 53/1000 | Loss: 0.00001246
Iteration 54/1000 | Loss: 0.00001245
Iteration 55/1000 | Loss: 0.00001245
Iteration 56/1000 | Loss: 0.00001245
Iteration 57/1000 | Loss: 0.00001244
Iteration 58/1000 | Loss: 0.00001242
Iteration 59/1000 | Loss: 0.00001241
Iteration 60/1000 | Loss: 0.00001241
Iteration 61/1000 | Loss: 0.00001241
Iteration 62/1000 | Loss: 0.00001240
Iteration 63/1000 | Loss: 0.00001240
Iteration 64/1000 | Loss: 0.00001240
Iteration 65/1000 | Loss: 0.00001240
Iteration 66/1000 | Loss: 0.00001240
Iteration 67/1000 | Loss: 0.00001239
Iteration 68/1000 | Loss: 0.00001239
Iteration 69/1000 | Loss: 0.00001239
Iteration 70/1000 | Loss: 0.00001239
Iteration 71/1000 | Loss: 0.00001239
Iteration 72/1000 | Loss: 0.00001239
Iteration 73/1000 | Loss: 0.00001239
Iteration 74/1000 | Loss: 0.00001238
Iteration 75/1000 | Loss: 0.00001238
Iteration 76/1000 | Loss: 0.00001238
Iteration 77/1000 | Loss: 0.00001237
Iteration 78/1000 | Loss: 0.00001237
Iteration 79/1000 | Loss: 0.00001237
Iteration 80/1000 | Loss: 0.00001236
Iteration 81/1000 | Loss: 0.00001236
Iteration 82/1000 | Loss: 0.00001236
Iteration 83/1000 | Loss: 0.00001236
Iteration 84/1000 | Loss: 0.00001236
Iteration 85/1000 | Loss: 0.00001236
Iteration 86/1000 | Loss: 0.00001236
Iteration 87/1000 | Loss: 0.00001236
Iteration 88/1000 | Loss: 0.00001236
Iteration 89/1000 | Loss: 0.00001236
Iteration 90/1000 | Loss: 0.00001236
Iteration 91/1000 | Loss: 0.00001236
Iteration 92/1000 | Loss: 0.00001236
Iteration 93/1000 | Loss: 0.00001235
Iteration 94/1000 | Loss: 0.00001235
Iteration 95/1000 | Loss: 0.00001234
Iteration 96/1000 | Loss: 0.00001234
Iteration 97/1000 | Loss: 0.00001234
Iteration 98/1000 | Loss: 0.00001233
Iteration 99/1000 | Loss: 0.00001233
Iteration 100/1000 | Loss: 0.00001233
Iteration 101/1000 | Loss: 0.00001233
Iteration 102/1000 | Loss: 0.00001232
Iteration 103/1000 | Loss: 0.00001232
Iteration 104/1000 | Loss: 0.00001232
Iteration 105/1000 | Loss: 0.00001232
Iteration 106/1000 | Loss: 0.00001231
Iteration 107/1000 | Loss: 0.00001231
Iteration 108/1000 | Loss: 0.00001231
Iteration 109/1000 | Loss: 0.00001231
Iteration 110/1000 | Loss: 0.00001231
Iteration 111/1000 | Loss: 0.00001231
Iteration 112/1000 | Loss: 0.00001231
Iteration 113/1000 | Loss: 0.00001230
Iteration 114/1000 | Loss: 0.00001230
Iteration 115/1000 | Loss: 0.00001230
Iteration 116/1000 | Loss: 0.00001229
Iteration 117/1000 | Loss: 0.00001229
Iteration 118/1000 | Loss: 0.00001229
Iteration 119/1000 | Loss: 0.00001229
Iteration 120/1000 | Loss: 0.00001229
Iteration 121/1000 | Loss: 0.00001229
Iteration 122/1000 | Loss: 0.00001229
Iteration 123/1000 | Loss: 0.00001228
Iteration 124/1000 | Loss: 0.00001228
Iteration 125/1000 | Loss: 0.00001228
Iteration 126/1000 | Loss: 0.00001227
Iteration 127/1000 | Loss: 0.00001227
Iteration 128/1000 | Loss: 0.00001227
Iteration 129/1000 | Loss: 0.00001227
Iteration 130/1000 | Loss: 0.00001227
Iteration 131/1000 | Loss: 0.00001226
Iteration 132/1000 | Loss: 0.00001226
Iteration 133/1000 | Loss: 0.00001226
Iteration 134/1000 | Loss: 0.00001226
Iteration 135/1000 | Loss: 0.00001226
Iteration 136/1000 | Loss: 0.00001226
Iteration 137/1000 | Loss: 0.00001225
Iteration 138/1000 | Loss: 0.00001225
Iteration 139/1000 | Loss: 0.00001225
Iteration 140/1000 | Loss: 0.00001225
Iteration 141/1000 | Loss: 0.00001224
Iteration 142/1000 | Loss: 0.00001224
Iteration 143/1000 | Loss: 0.00001224
Iteration 144/1000 | Loss: 0.00001224
Iteration 145/1000 | Loss: 0.00001223
Iteration 146/1000 | Loss: 0.00001223
Iteration 147/1000 | Loss: 0.00001223
Iteration 148/1000 | Loss: 0.00001223
Iteration 149/1000 | Loss: 0.00001223
Iteration 150/1000 | Loss: 0.00001222
Iteration 151/1000 | Loss: 0.00001222
Iteration 152/1000 | Loss: 0.00001222
Iteration 153/1000 | Loss: 0.00001222
Iteration 154/1000 | Loss: 0.00001222
Iteration 155/1000 | Loss: 0.00001222
Iteration 156/1000 | Loss: 0.00001222
Iteration 157/1000 | Loss: 0.00001221
Iteration 158/1000 | Loss: 0.00001221
Iteration 159/1000 | Loss: 0.00001221
Iteration 160/1000 | Loss: 0.00001221
Iteration 161/1000 | Loss: 0.00001221
Iteration 162/1000 | Loss: 0.00001221
Iteration 163/1000 | Loss: 0.00001221
Iteration 164/1000 | Loss: 0.00001221
Iteration 165/1000 | Loss: 0.00001221
Iteration 166/1000 | Loss: 0.00001221
Iteration 167/1000 | Loss: 0.00001220
Iteration 168/1000 | Loss: 0.00001220
Iteration 169/1000 | Loss: 0.00001220
Iteration 170/1000 | Loss: 0.00001220
Iteration 171/1000 | Loss: 0.00001220
Iteration 172/1000 | Loss: 0.00001220
Iteration 173/1000 | Loss: 0.00001220
Iteration 174/1000 | Loss: 0.00001219
Iteration 175/1000 | Loss: 0.00001219
Iteration 176/1000 | Loss: 0.00001219
Iteration 177/1000 | Loss: 0.00001219
Iteration 178/1000 | Loss: 0.00001219
Iteration 179/1000 | Loss: 0.00001219
Iteration 180/1000 | Loss: 0.00001219
Iteration 181/1000 | Loss: 0.00001219
Iteration 182/1000 | Loss: 0.00001219
Iteration 183/1000 | Loss: 0.00001218
Iteration 184/1000 | Loss: 0.00001218
Iteration 185/1000 | Loss: 0.00001218
Iteration 186/1000 | Loss: 0.00001218
Iteration 187/1000 | Loss: 0.00001218
Iteration 188/1000 | Loss: 0.00001218
Iteration 189/1000 | Loss: 0.00001218
Iteration 190/1000 | Loss: 0.00001218
Iteration 191/1000 | Loss: 0.00001218
Iteration 192/1000 | Loss: 0.00001218
Iteration 193/1000 | Loss: 0.00001218
Iteration 194/1000 | Loss: 0.00001218
Iteration 195/1000 | Loss: 0.00001217
Iteration 196/1000 | Loss: 0.00001217
Iteration 197/1000 | Loss: 0.00001217
Iteration 198/1000 | Loss: 0.00001217
Iteration 199/1000 | Loss: 0.00001217
Iteration 200/1000 | Loss: 0.00001217
Iteration 201/1000 | Loss: 0.00001217
Iteration 202/1000 | Loss: 0.00001217
Iteration 203/1000 | Loss: 0.00001217
Iteration 204/1000 | Loss: 0.00001217
Iteration 205/1000 | Loss: 0.00001217
Iteration 206/1000 | Loss: 0.00001217
Iteration 207/1000 | Loss: 0.00001217
Iteration 208/1000 | Loss: 0.00001216
Iteration 209/1000 | Loss: 0.00001216
Iteration 210/1000 | Loss: 0.00001216
Iteration 211/1000 | Loss: 0.00001216
Iteration 212/1000 | Loss: 0.00001216
Iteration 213/1000 | Loss: 0.00001216
Iteration 214/1000 | Loss: 0.00001216
Iteration 215/1000 | Loss: 0.00001216
Iteration 216/1000 | Loss: 0.00001216
Iteration 217/1000 | Loss: 0.00001216
Iteration 218/1000 | Loss: 0.00001216
Iteration 219/1000 | Loss: 0.00001215
Iteration 220/1000 | Loss: 0.00001215
Iteration 221/1000 | Loss: 0.00001215
Iteration 222/1000 | Loss: 0.00001215
Iteration 223/1000 | Loss: 0.00001215
Iteration 224/1000 | Loss: 0.00001215
Iteration 225/1000 | Loss: 0.00001215
Iteration 226/1000 | Loss: 0.00001215
Iteration 227/1000 | Loss: 0.00001215
Iteration 228/1000 | Loss: 0.00001215
Iteration 229/1000 | Loss: 0.00001215
Iteration 230/1000 | Loss: 0.00001215
Iteration 231/1000 | Loss: 0.00001215
Iteration 232/1000 | Loss: 0.00001215
Iteration 233/1000 | Loss: 0.00001214
Iteration 234/1000 | Loss: 0.00001214
Iteration 235/1000 | Loss: 0.00001214
Iteration 236/1000 | Loss: 0.00001214
Iteration 237/1000 | Loss: 0.00001214
Iteration 238/1000 | Loss: 0.00001214
Iteration 239/1000 | Loss: 0.00001214
Iteration 240/1000 | Loss: 0.00001214
Iteration 241/1000 | Loss: 0.00001214
Iteration 242/1000 | Loss: 0.00001214
Iteration 243/1000 | Loss: 0.00001214
Iteration 244/1000 | Loss: 0.00001214
Iteration 245/1000 | Loss: 0.00001214
Iteration 246/1000 | Loss: 0.00001214
Iteration 247/1000 | Loss: 0.00001214
Iteration 248/1000 | Loss: 0.00001214
Iteration 249/1000 | Loss: 0.00001214
Iteration 250/1000 | Loss: 0.00001214
Iteration 251/1000 | Loss: 0.00001214
Iteration 252/1000 | Loss: 0.00001214
Iteration 253/1000 | Loss: 0.00001214
Iteration 254/1000 | Loss: 0.00001214
Iteration 255/1000 | Loss: 0.00001214
Iteration 256/1000 | Loss: 0.00001214
Iteration 257/1000 | Loss: 0.00001214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.2136520126659889e-05, 1.2136520126659889e-05, 1.2136520126659889e-05, 1.2136520126659889e-05, 1.2136520126659889e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2136520126659889e-05

Optimization complete. Final v2v error: 2.999945878982544 mm

Highest mean error: 3.471470594406128 mm for frame 69

Lowest mean error: 2.744994640350342 mm for frame 28

Saving results

Total time: 46.884872913360596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079805
Iteration 2/25 | Loss: 0.00236573
Iteration 3/25 | Loss: 0.00165931
Iteration 4/25 | Loss: 0.00153782
Iteration 5/25 | Loss: 0.00150800
Iteration 6/25 | Loss: 0.00149652
Iteration 7/25 | Loss: 0.00147895
Iteration 8/25 | Loss: 0.00146544
Iteration 9/25 | Loss: 0.00146208
Iteration 10/25 | Loss: 0.00146207
Iteration 11/25 | Loss: 0.00145905
Iteration 12/25 | Loss: 0.00145779
Iteration 13/25 | Loss: 0.00145762
Iteration 14/25 | Loss: 0.00145650
Iteration 15/25 | Loss: 0.00145632
Iteration 16/25 | Loss: 0.00145631
Iteration 17/25 | Loss: 0.00145631
Iteration 18/25 | Loss: 0.00145631
Iteration 19/25 | Loss: 0.00145630
Iteration 20/25 | Loss: 0.00145630
Iteration 21/25 | Loss: 0.00145630
Iteration 22/25 | Loss: 0.00145630
Iteration 23/25 | Loss: 0.00145630
Iteration 24/25 | Loss: 0.00145630
Iteration 25/25 | Loss: 0.00145629

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.43993759
Iteration 2/25 | Loss: 0.00198290
Iteration 3/25 | Loss: 0.00198290
Iteration 4/25 | Loss: 0.00198290
Iteration 5/25 | Loss: 0.00198290
Iteration 6/25 | Loss: 0.00198290
Iteration 7/25 | Loss: 0.00198290
Iteration 8/25 | Loss: 0.00198290
Iteration 9/25 | Loss: 0.00198290
Iteration 10/25 | Loss: 0.00198290
Iteration 11/25 | Loss: 0.00198290
Iteration 12/25 | Loss: 0.00198290
Iteration 13/25 | Loss: 0.00198290
Iteration 14/25 | Loss: 0.00198290
Iteration 15/25 | Loss: 0.00198290
Iteration 16/25 | Loss: 0.00198290
Iteration 17/25 | Loss: 0.00198290
Iteration 18/25 | Loss: 0.00198290
Iteration 19/25 | Loss: 0.00198290
Iteration 20/25 | Loss: 0.00198290
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019828982185572386, 0.0019828982185572386, 0.0019828982185572386, 0.0019828982185572386, 0.0019828982185572386]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019828982185572386

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198290
Iteration 2/1000 | Loss: 0.00023293
Iteration 3/1000 | Loss: 0.00034889
Iteration 4/1000 | Loss: 0.00016790
Iteration 5/1000 | Loss: 0.00034052
Iteration 6/1000 | Loss: 0.00031435
Iteration 7/1000 | Loss: 0.00013146
Iteration 8/1000 | Loss: 0.00009259
Iteration 9/1000 | Loss: 0.00007421
Iteration 10/1000 | Loss: 0.00006067
Iteration 11/1000 | Loss: 0.00005524
Iteration 12/1000 | Loss: 0.00005261
Iteration 13/1000 | Loss: 0.00005042
Iteration 14/1000 | Loss: 0.00010440
Iteration 15/1000 | Loss: 0.00009116
Iteration 16/1000 | Loss: 0.00010378
Iteration 17/1000 | Loss: 0.00006826
Iteration 18/1000 | Loss: 0.00005078
Iteration 19/1000 | Loss: 0.00004676
Iteration 20/1000 | Loss: 0.00006220
Iteration 21/1000 | Loss: 0.00006887
Iteration 22/1000 | Loss: 0.00004758
Iteration 23/1000 | Loss: 0.00004397
Iteration 24/1000 | Loss: 0.00004992
Iteration 25/1000 | Loss: 0.00008475
Iteration 26/1000 | Loss: 0.00006544
Iteration 27/1000 | Loss: 0.00005638
Iteration 28/1000 | Loss: 0.00008435
Iteration 29/1000 | Loss: 0.00005995
Iteration 30/1000 | Loss: 0.00007993
Iteration 31/1000 | Loss: 0.00007064
Iteration 32/1000 | Loss: 0.00005878
Iteration 33/1000 | Loss: 0.00006519
Iteration 34/1000 | Loss: 0.00006262
Iteration 35/1000 | Loss: 0.00004795
Iteration 36/1000 | Loss: 0.00005585
Iteration 37/1000 | Loss: 0.00005324
Iteration 38/1000 | Loss: 0.00005048
Iteration 39/1000 | Loss: 0.00005225
Iteration 40/1000 | Loss: 0.00008030
Iteration 41/1000 | Loss: 0.00009319
Iteration 42/1000 | Loss: 0.00004356
Iteration 43/1000 | Loss: 0.00004227
Iteration 44/1000 | Loss: 0.00004407
Iteration 45/1000 | Loss: 0.00005070
Iteration 46/1000 | Loss: 0.00005654
Iteration 47/1000 | Loss: 0.00004707
Iteration 48/1000 | Loss: 0.00004364
Iteration 49/1000 | Loss: 0.00005613
Iteration 50/1000 | Loss: 0.00005777
Iteration 51/1000 | Loss: 0.00005322
Iteration 52/1000 | Loss: 0.00006736
Iteration 53/1000 | Loss: 0.00004349
Iteration 54/1000 | Loss: 0.00004111
Iteration 55/1000 | Loss: 0.00004289
Iteration 56/1000 | Loss: 0.00005027
Iteration 57/1000 | Loss: 0.00004868
Iteration 58/1000 | Loss: 0.00004921
Iteration 59/1000 | Loss: 0.00004776
Iteration 60/1000 | Loss: 0.00004312
Iteration 61/1000 | Loss: 0.00005788
Iteration 62/1000 | Loss: 0.00005407
Iteration 63/1000 | Loss: 0.00004780
Iteration 64/1000 | Loss: 0.00004846
Iteration 65/1000 | Loss: 0.00005760
Iteration 66/1000 | Loss: 0.00004900
Iteration 67/1000 | Loss: 0.00005715
Iteration 68/1000 | Loss: 0.00005401
Iteration 69/1000 | Loss: 0.00005516
Iteration 70/1000 | Loss: 0.00005565
Iteration 71/1000 | Loss: 0.00005481
Iteration 72/1000 | Loss: 0.00005050
Iteration 73/1000 | Loss: 0.00005600
Iteration 74/1000 | Loss: 0.00005684
Iteration 75/1000 | Loss: 0.00005707
Iteration 76/1000 | Loss: 0.00006008
Iteration 77/1000 | Loss: 0.00005655
Iteration 78/1000 | Loss: 0.00005877
Iteration 79/1000 | Loss: 0.00004618
Iteration 80/1000 | Loss: 0.00004359
Iteration 81/1000 | Loss: 0.00004199
Iteration 82/1000 | Loss: 0.00004100
Iteration 83/1000 | Loss: 0.00004043
Iteration 84/1000 | Loss: 0.00004004
Iteration 85/1000 | Loss: 0.00003969
Iteration 86/1000 | Loss: 0.00003943
Iteration 87/1000 | Loss: 0.00003933
Iteration 88/1000 | Loss: 0.00003930
Iteration 89/1000 | Loss: 0.00003918
Iteration 90/1000 | Loss: 0.00003918
Iteration 91/1000 | Loss: 0.00003918
Iteration 92/1000 | Loss: 0.00003917
Iteration 93/1000 | Loss: 0.00003915
Iteration 94/1000 | Loss: 0.00003914
Iteration 95/1000 | Loss: 0.00003912
Iteration 96/1000 | Loss: 0.00003911
Iteration 97/1000 | Loss: 0.00003910
Iteration 98/1000 | Loss: 0.00003910
Iteration 99/1000 | Loss: 0.00003910
Iteration 100/1000 | Loss: 0.00003910
Iteration 101/1000 | Loss: 0.00003910
Iteration 102/1000 | Loss: 0.00003910
Iteration 103/1000 | Loss: 0.00003910
Iteration 104/1000 | Loss: 0.00003910
Iteration 105/1000 | Loss: 0.00003909
Iteration 106/1000 | Loss: 0.00003909
Iteration 107/1000 | Loss: 0.00003909
Iteration 108/1000 | Loss: 0.00003909
Iteration 109/1000 | Loss: 0.00003909
Iteration 110/1000 | Loss: 0.00003909
Iteration 111/1000 | Loss: 0.00003908
Iteration 112/1000 | Loss: 0.00003908
Iteration 113/1000 | Loss: 0.00003908
Iteration 114/1000 | Loss: 0.00003908
Iteration 115/1000 | Loss: 0.00003908
Iteration 116/1000 | Loss: 0.00003908
Iteration 117/1000 | Loss: 0.00003908
Iteration 118/1000 | Loss: 0.00003907
Iteration 119/1000 | Loss: 0.00003907
Iteration 120/1000 | Loss: 0.00003907
Iteration 121/1000 | Loss: 0.00003907
Iteration 122/1000 | Loss: 0.00003907
Iteration 123/1000 | Loss: 0.00003907
Iteration 124/1000 | Loss: 0.00003907
Iteration 125/1000 | Loss: 0.00003907
Iteration 126/1000 | Loss: 0.00003907
Iteration 127/1000 | Loss: 0.00003907
Iteration 128/1000 | Loss: 0.00003907
Iteration 129/1000 | Loss: 0.00003907
Iteration 130/1000 | Loss: 0.00003906
Iteration 131/1000 | Loss: 0.00003906
Iteration 132/1000 | Loss: 0.00003906
Iteration 133/1000 | Loss: 0.00003906
Iteration 134/1000 | Loss: 0.00003906
Iteration 135/1000 | Loss: 0.00003906
Iteration 136/1000 | Loss: 0.00003906
Iteration 137/1000 | Loss: 0.00003906
Iteration 138/1000 | Loss: 0.00003906
Iteration 139/1000 | Loss: 0.00003905
Iteration 140/1000 | Loss: 0.00003905
Iteration 141/1000 | Loss: 0.00003905
Iteration 142/1000 | Loss: 0.00003905
Iteration 143/1000 | Loss: 0.00003905
Iteration 144/1000 | Loss: 0.00003905
Iteration 145/1000 | Loss: 0.00003905
Iteration 146/1000 | Loss: 0.00003904
Iteration 147/1000 | Loss: 0.00003904
Iteration 148/1000 | Loss: 0.00003904
Iteration 149/1000 | Loss: 0.00003904
Iteration 150/1000 | Loss: 0.00003904
Iteration 151/1000 | Loss: 0.00003904
Iteration 152/1000 | Loss: 0.00003904
Iteration 153/1000 | Loss: 0.00003904
Iteration 154/1000 | Loss: 0.00003904
Iteration 155/1000 | Loss: 0.00003904
Iteration 156/1000 | Loss: 0.00003904
Iteration 157/1000 | Loss: 0.00003903
Iteration 158/1000 | Loss: 0.00003903
Iteration 159/1000 | Loss: 0.00003900
Iteration 160/1000 | Loss: 0.00003898
Iteration 161/1000 | Loss: 0.00003896
Iteration 162/1000 | Loss: 0.00003896
Iteration 163/1000 | Loss: 0.00003896
Iteration 164/1000 | Loss: 0.00003894
Iteration 165/1000 | Loss: 0.00003894
Iteration 166/1000 | Loss: 0.00003886
Iteration 167/1000 | Loss: 0.00003878
Iteration 168/1000 | Loss: 0.00003874
Iteration 169/1000 | Loss: 0.00003874
Iteration 170/1000 | Loss: 0.00003872
Iteration 171/1000 | Loss: 0.00003872
Iteration 172/1000 | Loss: 0.00003872
Iteration 173/1000 | Loss: 0.00003872
Iteration 174/1000 | Loss: 0.00003872
Iteration 175/1000 | Loss: 0.00003872
Iteration 176/1000 | Loss: 0.00003872
Iteration 177/1000 | Loss: 0.00003871
Iteration 178/1000 | Loss: 0.00003871
Iteration 179/1000 | Loss: 0.00003871
Iteration 180/1000 | Loss: 0.00003871
Iteration 181/1000 | Loss: 0.00003870
Iteration 182/1000 | Loss: 0.00003870
Iteration 183/1000 | Loss: 0.00003870
Iteration 184/1000 | Loss: 0.00003870
Iteration 185/1000 | Loss: 0.00003870
Iteration 186/1000 | Loss: 0.00003870
Iteration 187/1000 | Loss: 0.00003870
Iteration 188/1000 | Loss: 0.00003870
Iteration 189/1000 | Loss: 0.00003870
Iteration 190/1000 | Loss: 0.00003870
Iteration 191/1000 | Loss: 0.00003870
Iteration 192/1000 | Loss: 0.00003870
Iteration 193/1000 | Loss: 0.00003869
Iteration 194/1000 | Loss: 0.00003869
Iteration 195/1000 | Loss: 0.00003869
Iteration 196/1000 | Loss: 0.00003868
Iteration 197/1000 | Loss: 0.00003867
Iteration 198/1000 | Loss: 0.00003866
Iteration 199/1000 | Loss: 0.00003866
Iteration 200/1000 | Loss: 0.00003866
Iteration 201/1000 | Loss: 0.00003866
Iteration 202/1000 | Loss: 0.00003866
Iteration 203/1000 | Loss: 0.00003866
Iteration 204/1000 | Loss: 0.00003866
Iteration 205/1000 | Loss: 0.00003866
Iteration 206/1000 | Loss: 0.00003866
Iteration 207/1000 | Loss: 0.00003866
Iteration 208/1000 | Loss: 0.00003866
Iteration 209/1000 | Loss: 0.00003866
Iteration 210/1000 | Loss: 0.00003866
Iteration 211/1000 | Loss: 0.00003865
Iteration 212/1000 | Loss: 0.00003865
Iteration 213/1000 | Loss: 0.00003865
Iteration 214/1000 | Loss: 0.00003864
Iteration 215/1000 | Loss: 0.00003864
Iteration 216/1000 | Loss: 0.00003864
Iteration 217/1000 | Loss: 0.00003863
Iteration 218/1000 | Loss: 0.00003863
Iteration 219/1000 | Loss: 0.00003863
Iteration 220/1000 | Loss: 0.00003863
Iteration 221/1000 | Loss: 0.00003863
Iteration 222/1000 | Loss: 0.00003863
Iteration 223/1000 | Loss: 0.00003863
Iteration 224/1000 | Loss: 0.00003863
Iteration 225/1000 | Loss: 0.00003863
Iteration 226/1000 | Loss: 0.00003863
Iteration 227/1000 | Loss: 0.00003863
Iteration 228/1000 | Loss: 0.00003862
Iteration 229/1000 | Loss: 0.00003861
Iteration 230/1000 | Loss: 0.00003860
Iteration 231/1000 | Loss: 0.00003860
Iteration 232/1000 | Loss: 0.00003860
Iteration 233/1000 | Loss: 0.00003859
Iteration 234/1000 | Loss: 0.00003858
Iteration 235/1000 | Loss: 0.00003856
Iteration 236/1000 | Loss: 0.00003856
Iteration 237/1000 | Loss: 0.00003856
Iteration 238/1000 | Loss: 0.00003856
Iteration 239/1000 | Loss: 0.00003856
Iteration 240/1000 | Loss: 0.00003855
Iteration 241/1000 | Loss: 0.00003855
Iteration 242/1000 | Loss: 0.00003854
Iteration 243/1000 | Loss: 0.00003854
Iteration 244/1000 | Loss: 0.00003854
Iteration 245/1000 | Loss: 0.00003853
Iteration 246/1000 | Loss: 0.00003853
Iteration 247/1000 | Loss: 0.00003853
Iteration 248/1000 | Loss: 0.00003853
Iteration 249/1000 | Loss: 0.00003853
Iteration 250/1000 | Loss: 0.00003852
Iteration 251/1000 | Loss: 0.00003852
Iteration 252/1000 | Loss: 0.00003852
Iteration 253/1000 | Loss: 0.00003852
Iteration 254/1000 | Loss: 0.00003851
Iteration 255/1000 | Loss: 0.00003851
Iteration 256/1000 | Loss: 0.00003851
Iteration 257/1000 | Loss: 0.00003851
Iteration 258/1000 | Loss: 0.00003851
Iteration 259/1000 | Loss: 0.00003851
Iteration 260/1000 | Loss: 0.00003850
Iteration 261/1000 | Loss: 0.00003850
Iteration 262/1000 | Loss: 0.00003850
Iteration 263/1000 | Loss: 0.00003850
Iteration 264/1000 | Loss: 0.00003849
Iteration 265/1000 | Loss: 0.00003849
Iteration 266/1000 | Loss: 0.00003849
Iteration 267/1000 | Loss: 0.00003849
Iteration 268/1000 | Loss: 0.00003848
Iteration 269/1000 | Loss: 0.00003848
Iteration 270/1000 | Loss: 0.00003848
Iteration 271/1000 | Loss: 0.00003847
Iteration 272/1000 | Loss: 0.00003847
Iteration 273/1000 | Loss: 0.00003847
Iteration 274/1000 | Loss: 0.00003847
Iteration 275/1000 | Loss: 0.00003846
Iteration 276/1000 | Loss: 0.00003846
Iteration 277/1000 | Loss: 0.00003846
Iteration 278/1000 | Loss: 0.00003846
Iteration 279/1000 | Loss: 0.00003846
Iteration 280/1000 | Loss: 0.00003846
Iteration 281/1000 | Loss: 0.00003846
Iteration 282/1000 | Loss: 0.00003846
Iteration 283/1000 | Loss: 0.00003846
Iteration 284/1000 | Loss: 0.00003846
Iteration 285/1000 | Loss: 0.00003845
Iteration 286/1000 | Loss: 0.00003845
Iteration 287/1000 | Loss: 0.00003845
Iteration 288/1000 | Loss: 0.00003845
Iteration 289/1000 | Loss: 0.00003844
Iteration 290/1000 | Loss: 0.00003844
Iteration 291/1000 | Loss: 0.00003844
Iteration 292/1000 | Loss: 0.00003844
Iteration 293/1000 | Loss: 0.00003844
Iteration 294/1000 | Loss: 0.00003844
Iteration 295/1000 | Loss: 0.00003844
Iteration 296/1000 | Loss: 0.00003844
Iteration 297/1000 | Loss: 0.00003844
Iteration 298/1000 | Loss: 0.00003844
Iteration 299/1000 | Loss: 0.00003844
Iteration 300/1000 | Loss: 0.00003843
Iteration 301/1000 | Loss: 0.00003843
Iteration 302/1000 | Loss: 0.00003843
Iteration 303/1000 | Loss: 0.00003843
Iteration 304/1000 | Loss: 0.00003843
Iteration 305/1000 | Loss: 0.00003843
Iteration 306/1000 | Loss: 0.00003843
Iteration 307/1000 | Loss: 0.00003843
Iteration 308/1000 | Loss: 0.00003843
Iteration 309/1000 | Loss: 0.00003843
Iteration 310/1000 | Loss: 0.00003843
Iteration 311/1000 | Loss: 0.00003843
Iteration 312/1000 | Loss: 0.00003843
Iteration 313/1000 | Loss: 0.00003843
Iteration 314/1000 | Loss: 0.00003842
Iteration 315/1000 | Loss: 0.00003842
Iteration 316/1000 | Loss: 0.00003842
Iteration 317/1000 | Loss: 0.00003842
Iteration 318/1000 | Loss: 0.00003842
Iteration 319/1000 | Loss: 0.00003842
Iteration 320/1000 | Loss: 0.00003842
Iteration 321/1000 | Loss: 0.00003842
Iteration 322/1000 | Loss: 0.00003842
Iteration 323/1000 | Loss: 0.00003842
Iteration 324/1000 | Loss: 0.00003842
Iteration 325/1000 | Loss: 0.00003842
Iteration 326/1000 | Loss: 0.00003842
Iteration 327/1000 | Loss: 0.00003842
Iteration 328/1000 | Loss: 0.00003842
Iteration 329/1000 | Loss: 0.00003842
Iteration 330/1000 | Loss: 0.00003842
Iteration 331/1000 | Loss: 0.00003842
Iteration 332/1000 | Loss: 0.00003842
Iteration 333/1000 | Loss: 0.00003842
Iteration 334/1000 | Loss: 0.00003842
Iteration 335/1000 | Loss: 0.00003842
Iteration 336/1000 | Loss: 0.00003842
Iteration 337/1000 | Loss: 0.00003842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 337. Stopping optimization.
Last 5 losses: [3.8423091609729454e-05, 3.8423091609729454e-05, 3.8423091609729454e-05, 3.8423091609729454e-05, 3.8423091609729454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8423091609729454e-05

Optimization complete. Final v2v error: 5.041079044342041 mm

Highest mean error: 6.147314548492432 mm for frame 123

Lowest mean error: 3.84932017326355 mm for frame 2

Saving results

Total time: 193.73735451698303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958799
Iteration 2/25 | Loss: 0.00958799
Iteration 3/25 | Loss: 0.00958799
Iteration 4/25 | Loss: 0.00958799
Iteration 5/25 | Loss: 0.00958798
Iteration 6/25 | Loss: 0.00958798
Iteration 7/25 | Loss: 0.00958798
Iteration 8/25 | Loss: 0.00958798
Iteration 9/25 | Loss: 0.00958798
Iteration 10/25 | Loss: 0.00958797
Iteration 11/25 | Loss: 0.00958797
Iteration 12/25 | Loss: 0.00958797
Iteration 13/25 | Loss: 0.00958797
Iteration 14/25 | Loss: 0.00958797
Iteration 15/25 | Loss: 0.00958796
Iteration 16/25 | Loss: 0.00958796
Iteration 17/25 | Loss: 0.00958796
Iteration 18/25 | Loss: 0.00958796
Iteration 19/25 | Loss: 0.00958796
Iteration 20/25 | Loss: 0.00958796
Iteration 21/25 | Loss: 0.00958796
Iteration 22/25 | Loss: 0.00958795
Iteration 23/25 | Loss: 0.00958795
Iteration 24/25 | Loss: 0.00958795
Iteration 25/25 | Loss: 0.00958795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38040233
Iteration 2/25 | Loss: 0.17921679
Iteration 3/25 | Loss: 0.17804644
Iteration 4/25 | Loss: 0.17822163
Iteration 5/25 | Loss: 0.17772704
Iteration 6/25 | Loss: 0.17772700
Iteration 7/25 | Loss: 0.17772697
Iteration 8/25 | Loss: 0.17772697
Iteration 9/25 | Loss: 0.17772697
Iteration 10/25 | Loss: 0.17772694
Iteration 11/25 | Loss: 0.17772692
Iteration 12/25 | Loss: 0.17772692
Iteration 13/25 | Loss: 0.17772692
Iteration 14/25 | Loss: 0.17772692
Iteration 15/25 | Loss: 0.17772694
Iteration 16/25 | Loss: 0.17772694
Iteration 17/25 | Loss: 0.17772694
Iteration 18/25 | Loss: 0.17772692
Iteration 19/25 | Loss: 0.17772692
Iteration 20/25 | Loss: 0.17772692
Iteration 21/25 | Loss: 0.17772692
Iteration 22/25 | Loss: 0.17772692
Iteration 23/25 | Loss: 0.17772692
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.17772692441940308, 0.17772692441940308, 0.17772692441940308, 0.17772692441940308, 0.17772692441940308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17772692441940308

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17772692
Iteration 2/1000 | Loss: 0.00205924
Iteration 3/1000 | Loss: 0.00041406
Iteration 4/1000 | Loss: 0.00017033
Iteration 5/1000 | Loss: 0.00040994
Iteration 6/1000 | Loss: 0.00013100
Iteration 7/1000 | Loss: 0.00008847
Iteration 8/1000 | Loss: 0.00006387
Iteration 9/1000 | Loss: 0.00002801
Iteration 10/1000 | Loss: 0.00103524
Iteration 11/1000 | Loss: 0.00068613
Iteration 12/1000 | Loss: 0.00033757
Iteration 13/1000 | Loss: 0.00065954
Iteration 14/1000 | Loss: 0.00002906
Iteration 15/1000 | Loss: 0.00010662
Iteration 16/1000 | Loss: 0.00003180
Iteration 17/1000 | Loss: 0.00022884
Iteration 18/1000 | Loss: 0.00068333
Iteration 19/1000 | Loss: 0.00001910
Iteration 20/1000 | Loss: 0.00009180
Iteration 21/1000 | Loss: 0.00018154
Iteration 22/1000 | Loss: 0.00020592
Iteration 23/1000 | Loss: 0.00123118
Iteration 24/1000 | Loss: 0.00004456
Iteration 25/1000 | Loss: 0.00021262
Iteration 26/1000 | Loss: 0.00002234
Iteration 27/1000 | Loss: 0.00042359
Iteration 28/1000 | Loss: 0.00001681
Iteration 29/1000 | Loss: 0.00017287
Iteration 30/1000 | Loss: 0.00002067
Iteration 31/1000 | Loss: 0.00006946
Iteration 32/1000 | Loss: 0.00011274
Iteration 33/1000 | Loss: 0.00004129
Iteration 34/1000 | Loss: 0.00001463
Iteration 35/1000 | Loss: 0.00004754
Iteration 36/1000 | Loss: 0.00005875
Iteration 37/1000 | Loss: 0.00004659
Iteration 38/1000 | Loss: 0.00002960
Iteration 39/1000 | Loss: 0.00004222
Iteration 40/1000 | Loss: 0.00006306
Iteration 41/1000 | Loss: 0.00003911
Iteration 42/1000 | Loss: 0.00021072
Iteration 43/1000 | Loss: 0.00010115
Iteration 44/1000 | Loss: 0.00001633
Iteration 45/1000 | Loss: 0.00001384
Iteration 46/1000 | Loss: 0.00001498
Iteration 47/1000 | Loss: 0.00007624
Iteration 48/1000 | Loss: 0.00010989
Iteration 49/1000 | Loss: 0.00002595
Iteration 50/1000 | Loss: 0.00005322
Iteration 51/1000 | Loss: 0.00058768
Iteration 52/1000 | Loss: 0.00004011
Iteration 53/1000 | Loss: 0.00007893
Iteration 54/1000 | Loss: 0.00001491
Iteration 55/1000 | Loss: 0.00001339
Iteration 56/1000 | Loss: 0.00001338
Iteration 57/1000 | Loss: 0.00001328
Iteration 58/1000 | Loss: 0.00001939
Iteration 59/1000 | Loss: 0.00001323
Iteration 60/1000 | Loss: 0.00001323
Iteration 61/1000 | Loss: 0.00001323
Iteration 62/1000 | Loss: 0.00001323
Iteration 63/1000 | Loss: 0.00001323
Iteration 64/1000 | Loss: 0.00001323
Iteration 65/1000 | Loss: 0.00001323
Iteration 66/1000 | Loss: 0.00001322
Iteration 67/1000 | Loss: 0.00001322
Iteration 68/1000 | Loss: 0.00001322
Iteration 69/1000 | Loss: 0.00001322
Iteration 70/1000 | Loss: 0.00001322
Iteration 71/1000 | Loss: 0.00001322
Iteration 72/1000 | Loss: 0.00001322
Iteration 73/1000 | Loss: 0.00001322
Iteration 74/1000 | Loss: 0.00001322
Iteration 75/1000 | Loss: 0.00001322
Iteration 76/1000 | Loss: 0.00001321
Iteration 77/1000 | Loss: 0.00001320
Iteration 78/1000 | Loss: 0.00001320
Iteration 79/1000 | Loss: 0.00001320
Iteration 80/1000 | Loss: 0.00001319
Iteration 81/1000 | Loss: 0.00001319
Iteration 82/1000 | Loss: 0.00001319
Iteration 83/1000 | Loss: 0.00001319
Iteration 84/1000 | Loss: 0.00002372
Iteration 85/1000 | Loss: 0.00001320
Iteration 86/1000 | Loss: 0.00001318
Iteration 87/1000 | Loss: 0.00001317
Iteration 88/1000 | Loss: 0.00001317
Iteration 89/1000 | Loss: 0.00001317
Iteration 90/1000 | Loss: 0.00001317
Iteration 91/1000 | Loss: 0.00001317
Iteration 92/1000 | Loss: 0.00001317
Iteration 93/1000 | Loss: 0.00001317
Iteration 94/1000 | Loss: 0.00001316
Iteration 95/1000 | Loss: 0.00001316
Iteration 96/1000 | Loss: 0.00001316
Iteration 97/1000 | Loss: 0.00001316
Iteration 98/1000 | Loss: 0.00001316
Iteration 99/1000 | Loss: 0.00001316
Iteration 100/1000 | Loss: 0.00001316
Iteration 101/1000 | Loss: 0.00001316
Iteration 102/1000 | Loss: 0.00001315
Iteration 103/1000 | Loss: 0.00001315
Iteration 104/1000 | Loss: 0.00001315
Iteration 105/1000 | Loss: 0.00001315
Iteration 106/1000 | Loss: 0.00001315
Iteration 107/1000 | Loss: 0.00001315
Iteration 108/1000 | Loss: 0.00001315
Iteration 109/1000 | Loss: 0.00001315
Iteration 110/1000 | Loss: 0.00001315
Iteration 111/1000 | Loss: 0.00001315
Iteration 112/1000 | Loss: 0.00001314
Iteration 113/1000 | Loss: 0.00001314
Iteration 114/1000 | Loss: 0.00001314
Iteration 115/1000 | Loss: 0.00001314
Iteration 116/1000 | Loss: 0.00001314
Iteration 117/1000 | Loss: 0.00003536
Iteration 118/1000 | Loss: 0.00001315
Iteration 119/1000 | Loss: 0.00001315
Iteration 120/1000 | Loss: 0.00001315
Iteration 121/1000 | Loss: 0.00001315
Iteration 122/1000 | Loss: 0.00001314
Iteration 123/1000 | Loss: 0.00001314
Iteration 124/1000 | Loss: 0.00001314
Iteration 125/1000 | Loss: 0.00001314
Iteration 126/1000 | Loss: 0.00001314
Iteration 127/1000 | Loss: 0.00001314
Iteration 128/1000 | Loss: 0.00001314
Iteration 129/1000 | Loss: 0.00001314
Iteration 130/1000 | Loss: 0.00001314
Iteration 131/1000 | Loss: 0.00001313
Iteration 132/1000 | Loss: 0.00001313
Iteration 133/1000 | Loss: 0.00001313
Iteration 134/1000 | Loss: 0.00001313
Iteration 135/1000 | Loss: 0.00001313
Iteration 136/1000 | Loss: 0.00001313
Iteration 137/1000 | Loss: 0.00001313
Iteration 138/1000 | Loss: 0.00001313
Iteration 139/1000 | Loss: 0.00001313
Iteration 140/1000 | Loss: 0.00001312
Iteration 141/1000 | Loss: 0.00001312
Iteration 142/1000 | Loss: 0.00001312
Iteration 143/1000 | Loss: 0.00001312
Iteration 144/1000 | Loss: 0.00001312
Iteration 145/1000 | Loss: 0.00001312
Iteration 146/1000 | Loss: 0.00001312
Iteration 147/1000 | Loss: 0.00001312
Iteration 148/1000 | Loss: 0.00001312
Iteration 149/1000 | Loss: 0.00001312
Iteration 150/1000 | Loss: 0.00001312
Iteration 151/1000 | Loss: 0.00001312
Iteration 152/1000 | Loss: 0.00001312
Iteration 153/1000 | Loss: 0.00001312
Iteration 154/1000 | Loss: 0.00001312
Iteration 155/1000 | Loss: 0.00001312
Iteration 156/1000 | Loss: 0.00001312
Iteration 157/1000 | Loss: 0.00001312
Iteration 158/1000 | Loss: 0.00001312
Iteration 159/1000 | Loss: 0.00001312
Iteration 160/1000 | Loss: 0.00001312
Iteration 161/1000 | Loss: 0.00001312
Iteration 162/1000 | Loss: 0.00001312
Iteration 163/1000 | Loss: 0.00001312
Iteration 164/1000 | Loss: 0.00001312
Iteration 165/1000 | Loss: 0.00001312
Iteration 166/1000 | Loss: 0.00001312
Iteration 167/1000 | Loss: 0.00001312
Iteration 168/1000 | Loss: 0.00001312
Iteration 169/1000 | Loss: 0.00001312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.312385211349465e-05, 1.312385211349465e-05, 1.312385211349465e-05, 1.312385211349465e-05, 1.312385211349465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.312385211349465e-05

Optimization complete. Final v2v error: 3.097874641418457 mm

Highest mean error: 3.3084158897399902 mm for frame 209

Lowest mean error: 2.898041248321533 mm for frame 3

Saving results

Total time: 109.38825750350952
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845060
Iteration 2/25 | Loss: 0.00158136
Iteration 3/25 | Loss: 0.00133226
Iteration 4/25 | Loss: 0.00131120
Iteration 5/25 | Loss: 0.00130642
Iteration 6/25 | Loss: 0.00130642
Iteration 7/25 | Loss: 0.00130642
Iteration 8/25 | Loss: 0.00130642
Iteration 9/25 | Loss: 0.00130642
Iteration 10/25 | Loss: 0.00130642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013064246159046888, 0.0013064246159046888, 0.0013064246159046888, 0.0013064246159046888, 0.0013064246159046888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013064246159046888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.15442786
Iteration 2/25 | Loss: 0.00076846
Iteration 3/25 | Loss: 0.00076846
Iteration 4/25 | Loss: 0.00076846
Iteration 5/25 | Loss: 0.00076846
Iteration 6/25 | Loss: 0.00076846
Iteration 7/25 | Loss: 0.00076846
Iteration 8/25 | Loss: 0.00076846
Iteration 9/25 | Loss: 0.00076846
Iteration 10/25 | Loss: 0.00076846
Iteration 11/25 | Loss: 0.00076846
Iteration 12/25 | Loss: 0.00076846
Iteration 13/25 | Loss: 0.00076846
Iteration 14/25 | Loss: 0.00076846
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007684601587243378, 0.0007684601587243378, 0.0007684601587243378, 0.0007684601587243378, 0.0007684601587243378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007684601587243378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076846
Iteration 2/1000 | Loss: 0.00006045
Iteration 3/1000 | Loss: 0.00003554
Iteration 4/1000 | Loss: 0.00002855
Iteration 5/1000 | Loss: 0.00002689
Iteration 6/1000 | Loss: 0.00002576
Iteration 7/1000 | Loss: 0.00002508
Iteration 8/1000 | Loss: 0.00002468
Iteration 9/1000 | Loss: 0.00002432
Iteration 10/1000 | Loss: 0.00002405
Iteration 11/1000 | Loss: 0.00002378
Iteration 12/1000 | Loss: 0.00002360
Iteration 13/1000 | Loss: 0.00002344
Iteration 14/1000 | Loss: 0.00002337
Iteration 15/1000 | Loss: 0.00002335
Iteration 16/1000 | Loss: 0.00002335
Iteration 17/1000 | Loss: 0.00002334
Iteration 18/1000 | Loss: 0.00002333
Iteration 19/1000 | Loss: 0.00002333
Iteration 20/1000 | Loss: 0.00002333
Iteration 21/1000 | Loss: 0.00002333
Iteration 22/1000 | Loss: 0.00002333
Iteration 23/1000 | Loss: 0.00002333
Iteration 24/1000 | Loss: 0.00002333
Iteration 25/1000 | Loss: 0.00002333
Iteration 26/1000 | Loss: 0.00002333
Iteration 27/1000 | Loss: 0.00002333
Iteration 28/1000 | Loss: 0.00002333
Iteration 29/1000 | Loss: 0.00002333
Iteration 30/1000 | Loss: 0.00002333
Iteration 31/1000 | Loss: 0.00002333
Iteration 32/1000 | Loss: 0.00002333
Iteration 33/1000 | Loss: 0.00002333
Iteration 34/1000 | Loss: 0.00002332
Iteration 35/1000 | Loss: 0.00002332
Iteration 36/1000 | Loss: 0.00002332
Iteration 37/1000 | Loss: 0.00002332
Iteration 38/1000 | Loss: 0.00002332
Iteration 39/1000 | Loss: 0.00002332
Iteration 40/1000 | Loss: 0.00002332
Iteration 41/1000 | Loss: 0.00002332
Iteration 42/1000 | Loss: 0.00002331
Iteration 43/1000 | Loss: 0.00002331
Iteration 44/1000 | Loss: 0.00002331
Iteration 45/1000 | Loss: 0.00002331
Iteration 46/1000 | Loss: 0.00002331
Iteration 47/1000 | Loss: 0.00002331
Iteration 48/1000 | Loss: 0.00002330
Iteration 49/1000 | Loss: 0.00002330
Iteration 50/1000 | Loss: 0.00002329
Iteration 51/1000 | Loss: 0.00002329
Iteration 52/1000 | Loss: 0.00002326
Iteration 53/1000 | Loss: 0.00002325
Iteration 54/1000 | Loss: 0.00002325
Iteration 55/1000 | Loss: 0.00002325
Iteration 56/1000 | Loss: 0.00002325
Iteration 57/1000 | Loss: 0.00002325
Iteration 58/1000 | Loss: 0.00002325
Iteration 59/1000 | Loss: 0.00002324
Iteration 60/1000 | Loss: 0.00002324
Iteration 61/1000 | Loss: 0.00002324
Iteration 62/1000 | Loss: 0.00002324
Iteration 63/1000 | Loss: 0.00002324
Iteration 64/1000 | Loss: 0.00002324
Iteration 65/1000 | Loss: 0.00002324
Iteration 66/1000 | Loss: 0.00002324
Iteration 67/1000 | Loss: 0.00002323
Iteration 68/1000 | Loss: 0.00002323
Iteration 69/1000 | Loss: 0.00002322
Iteration 70/1000 | Loss: 0.00002322
Iteration 71/1000 | Loss: 0.00002321
Iteration 72/1000 | Loss: 0.00002321
Iteration 73/1000 | Loss: 0.00002320
Iteration 74/1000 | Loss: 0.00002320
Iteration 75/1000 | Loss: 0.00002320
Iteration 76/1000 | Loss: 0.00002319
Iteration 77/1000 | Loss: 0.00002319
Iteration 78/1000 | Loss: 0.00002319
Iteration 79/1000 | Loss: 0.00002318
Iteration 80/1000 | Loss: 0.00002318
Iteration 81/1000 | Loss: 0.00002318
Iteration 82/1000 | Loss: 0.00002318
Iteration 83/1000 | Loss: 0.00002318
Iteration 84/1000 | Loss: 0.00002318
Iteration 85/1000 | Loss: 0.00002318
Iteration 86/1000 | Loss: 0.00002318
Iteration 87/1000 | Loss: 0.00002317
Iteration 88/1000 | Loss: 0.00002317
Iteration 89/1000 | Loss: 0.00002317
Iteration 90/1000 | Loss: 0.00002317
Iteration 91/1000 | Loss: 0.00002317
Iteration 92/1000 | Loss: 0.00002317
Iteration 93/1000 | Loss: 0.00002317
Iteration 94/1000 | Loss: 0.00002317
Iteration 95/1000 | Loss: 0.00002317
Iteration 96/1000 | Loss: 0.00002317
Iteration 97/1000 | Loss: 0.00002317
Iteration 98/1000 | Loss: 0.00002317
Iteration 99/1000 | Loss: 0.00002317
Iteration 100/1000 | Loss: 0.00002317
Iteration 101/1000 | Loss: 0.00002316
Iteration 102/1000 | Loss: 0.00002316
Iteration 103/1000 | Loss: 0.00002316
Iteration 104/1000 | Loss: 0.00002316
Iteration 105/1000 | Loss: 0.00002316
Iteration 106/1000 | Loss: 0.00002316
Iteration 107/1000 | Loss: 0.00002315
Iteration 108/1000 | Loss: 0.00002315
Iteration 109/1000 | Loss: 0.00002315
Iteration 110/1000 | Loss: 0.00002315
Iteration 111/1000 | Loss: 0.00002315
Iteration 112/1000 | Loss: 0.00002315
Iteration 113/1000 | Loss: 0.00002315
Iteration 114/1000 | Loss: 0.00002314
Iteration 115/1000 | Loss: 0.00002314
Iteration 116/1000 | Loss: 0.00002313
Iteration 117/1000 | Loss: 0.00002313
Iteration 118/1000 | Loss: 0.00002313
Iteration 119/1000 | Loss: 0.00002313
Iteration 120/1000 | Loss: 0.00002313
Iteration 121/1000 | Loss: 0.00002313
Iteration 122/1000 | Loss: 0.00002313
Iteration 123/1000 | Loss: 0.00002312
Iteration 124/1000 | Loss: 0.00002312
Iteration 125/1000 | Loss: 0.00002312
Iteration 126/1000 | Loss: 0.00002312
Iteration 127/1000 | Loss: 0.00002312
Iteration 128/1000 | Loss: 0.00002312
Iteration 129/1000 | Loss: 0.00002312
Iteration 130/1000 | Loss: 0.00002312
Iteration 131/1000 | Loss: 0.00002311
Iteration 132/1000 | Loss: 0.00002311
Iteration 133/1000 | Loss: 0.00002311
Iteration 134/1000 | Loss: 0.00002311
Iteration 135/1000 | Loss: 0.00002311
Iteration 136/1000 | Loss: 0.00002311
Iteration 137/1000 | Loss: 0.00002311
Iteration 138/1000 | Loss: 0.00002311
Iteration 139/1000 | Loss: 0.00002311
Iteration 140/1000 | Loss: 0.00002311
Iteration 141/1000 | Loss: 0.00002311
Iteration 142/1000 | Loss: 0.00002310
Iteration 143/1000 | Loss: 0.00002310
Iteration 144/1000 | Loss: 0.00002310
Iteration 145/1000 | Loss: 0.00002310
Iteration 146/1000 | Loss: 0.00002310
Iteration 147/1000 | Loss: 0.00002310
Iteration 148/1000 | Loss: 0.00002310
Iteration 149/1000 | Loss: 0.00002310
Iteration 150/1000 | Loss: 0.00002310
Iteration 151/1000 | Loss: 0.00002310
Iteration 152/1000 | Loss: 0.00002310
Iteration 153/1000 | Loss: 0.00002310
Iteration 154/1000 | Loss: 0.00002310
Iteration 155/1000 | Loss: 0.00002310
Iteration 156/1000 | Loss: 0.00002309
Iteration 157/1000 | Loss: 0.00002309
Iteration 158/1000 | Loss: 0.00002309
Iteration 159/1000 | Loss: 0.00002309
Iteration 160/1000 | Loss: 0.00002309
Iteration 161/1000 | Loss: 0.00002309
Iteration 162/1000 | Loss: 0.00002309
Iteration 163/1000 | Loss: 0.00002308
Iteration 164/1000 | Loss: 0.00002308
Iteration 165/1000 | Loss: 0.00002308
Iteration 166/1000 | Loss: 0.00002308
Iteration 167/1000 | Loss: 0.00002308
Iteration 168/1000 | Loss: 0.00002308
Iteration 169/1000 | Loss: 0.00002308
Iteration 170/1000 | Loss: 0.00002308
Iteration 171/1000 | Loss: 0.00002308
Iteration 172/1000 | Loss: 0.00002308
Iteration 173/1000 | Loss: 0.00002308
Iteration 174/1000 | Loss: 0.00002308
Iteration 175/1000 | Loss: 0.00002308
Iteration 176/1000 | Loss: 0.00002307
Iteration 177/1000 | Loss: 0.00002307
Iteration 178/1000 | Loss: 0.00002307
Iteration 179/1000 | Loss: 0.00002307
Iteration 180/1000 | Loss: 0.00002307
Iteration 181/1000 | Loss: 0.00002307
Iteration 182/1000 | Loss: 0.00002307
Iteration 183/1000 | Loss: 0.00002306
Iteration 184/1000 | Loss: 0.00002306
Iteration 185/1000 | Loss: 0.00002306
Iteration 186/1000 | Loss: 0.00002306
Iteration 187/1000 | Loss: 0.00002306
Iteration 188/1000 | Loss: 0.00002306
Iteration 189/1000 | Loss: 0.00002306
Iteration 190/1000 | Loss: 0.00002306
Iteration 191/1000 | Loss: 0.00002306
Iteration 192/1000 | Loss: 0.00002306
Iteration 193/1000 | Loss: 0.00002306
Iteration 194/1000 | Loss: 0.00002306
Iteration 195/1000 | Loss: 0.00002306
Iteration 196/1000 | Loss: 0.00002306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.3057566068018787e-05, 2.3057566068018787e-05, 2.3057566068018787e-05, 2.3057566068018787e-05, 2.3057566068018787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3057566068018787e-05

Optimization complete. Final v2v error: 4.075685977935791 mm

Highest mean error: 4.258835315704346 mm for frame 52

Lowest mean error: 3.688899517059326 mm for frame 102

Saving results

Total time: 43.00705337524414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928776
Iteration 2/25 | Loss: 0.00275486
Iteration 3/25 | Loss: 0.00188058
Iteration 4/25 | Loss: 0.00175675
Iteration 5/25 | Loss: 0.00177765
Iteration 6/25 | Loss: 0.00168522
Iteration 7/25 | Loss: 0.00156218
Iteration 8/25 | Loss: 0.00151081
Iteration 9/25 | Loss: 0.00146990
Iteration 10/25 | Loss: 0.00143765
Iteration 11/25 | Loss: 0.00142448
Iteration 12/25 | Loss: 0.00144191
Iteration 13/25 | Loss: 0.00143061
Iteration 14/25 | Loss: 0.00141691
Iteration 15/25 | Loss: 0.00138687
Iteration 16/25 | Loss: 0.00136659
Iteration 17/25 | Loss: 0.00137174
Iteration 18/25 | Loss: 0.00136129
Iteration 19/25 | Loss: 0.00136314
Iteration 20/25 | Loss: 0.00134854
Iteration 21/25 | Loss: 0.00133760
Iteration 22/25 | Loss: 0.00133052
Iteration 23/25 | Loss: 0.00133156
Iteration 24/25 | Loss: 0.00133294
Iteration 25/25 | Loss: 0.00133155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15367758
Iteration 2/25 | Loss: 0.00168057
Iteration 3/25 | Loss: 0.00168056
Iteration 4/25 | Loss: 0.00168056
Iteration 5/25 | Loss: 0.00168056
Iteration 6/25 | Loss: 0.00168056
Iteration 7/25 | Loss: 0.00168056
Iteration 8/25 | Loss: 0.00168056
Iteration 9/25 | Loss: 0.00168056
Iteration 10/25 | Loss: 0.00168056
Iteration 11/25 | Loss: 0.00168056
Iteration 12/25 | Loss: 0.00168056
Iteration 13/25 | Loss: 0.00168056
Iteration 14/25 | Loss: 0.00168056
Iteration 15/25 | Loss: 0.00168056
Iteration 16/25 | Loss: 0.00168056
Iteration 17/25 | Loss: 0.00168056
Iteration 18/25 | Loss: 0.00168056
Iteration 19/25 | Loss: 0.00168056
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016805588966235518, 0.0016805588966235518, 0.0016805588966235518, 0.0016805588966235518, 0.0016805588966235518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016805588966235518

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168056
Iteration 2/1000 | Loss: 0.00020843
Iteration 3/1000 | Loss: 0.00065693
Iteration 4/1000 | Loss: 0.00031949
Iteration 5/1000 | Loss: 0.00007973
Iteration 6/1000 | Loss: 0.00024220
Iteration 7/1000 | Loss: 0.00053670
Iteration 8/1000 | Loss: 0.00007222
Iteration 9/1000 | Loss: 0.00006173
Iteration 10/1000 | Loss: 0.00059747
Iteration 11/1000 | Loss: 0.00013303
Iteration 12/1000 | Loss: 0.00007051
Iteration 13/1000 | Loss: 0.00010724
Iteration 14/1000 | Loss: 0.00018718
Iteration 15/1000 | Loss: 0.00005968
Iteration 16/1000 | Loss: 0.00006329
Iteration 17/1000 | Loss: 0.00005300
Iteration 18/1000 | Loss: 0.00006310
Iteration 19/1000 | Loss: 0.00005617
Iteration 20/1000 | Loss: 0.00067933
Iteration 21/1000 | Loss: 0.00021628
Iteration 22/1000 | Loss: 0.00015219
Iteration 23/1000 | Loss: 0.00008632
Iteration 24/1000 | Loss: 0.00019730
Iteration 25/1000 | Loss: 0.00005737
Iteration 26/1000 | Loss: 0.00005277
Iteration 27/1000 | Loss: 0.00005927
Iteration 28/1000 | Loss: 0.00005918
Iteration 29/1000 | Loss: 0.00006523
Iteration 30/1000 | Loss: 0.00005813
Iteration 31/1000 | Loss: 0.00005798
Iteration 32/1000 | Loss: 0.00006126
Iteration 33/1000 | Loss: 0.00005564
Iteration 34/1000 | Loss: 0.00006041
Iteration 35/1000 | Loss: 0.00005738
Iteration 36/1000 | Loss: 0.00005865
Iteration 37/1000 | Loss: 0.00005684
Iteration 38/1000 | Loss: 0.00006037
Iteration 39/1000 | Loss: 0.00005594
Iteration 40/1000 | Loss: 0.00005987
Iteration 41/1000 | Loss: 0.00005575
Iteration 42/1000 | Loss: 0.00005320
Iteration 43/1000 | Loss: 0.00005507
Iteration 44/1000 | Loss: 0.00005602
Iteration 45/1000 | Loss: 0.00005617
Iteration 46/1000 | Loss: 0.00005631
Iteration 47/1000 | Loss: 0.00005950
Iteration 48/1000 | Loss: 0.00005793
Iteration 49/1000 | Loss: 0.00005605
Iteration 50/1000 | Loss: 0.00005330
Iteration 51/1000 | Loss: 0.00005143
Iteration 52/1000 | Loss: 0.00005006
Iteration 53/1000 | Loss: 0.00005084
Iteration 54/1000 | Loss: 0.00005821
Iteration 55/1000 | Loss: 0.00005075
Iteration 56/1000 | Loss: 0.00005523
Iteration 57/1000 | Loss: 0.00005096
Iteration 58/1000 | Loss: 0.00004736
Iteration 59/1000 | Loss: 0.00004816
Iteration 60/1000 | Loss: 0.00004846
Iteration 61/1000 | Loss: 0.00005499
Iteration 62/1000 | Loss: 0.00004851
Iteration 63/1000 | Loss: 0.00004937
Iteration 64/1000 | Loss: 0.00005654
Iteration 65/1000 | Loss: 0.00005001
Iteration 66/1000 | Loss: 0.00004981
Iteration 67/1000 | Loss: 0.00005055
Iteration 68/1000 | Loss: 0.00005483
Iteration 69/1000 | Loss: 0.00006534
Iteration 70/1000 | Loss: 0.00010982
Iteration 71/1000 | Loss: 0.00096116
Iteration 72/1000 | Loss: 0.00017534
Iteration 73/1000 | Loss: 0.00003968
Iteration 74/1000 | Loss: 0.00003688
Iteration 75/1000 | Loss: 0.00003502
Iteration 76/1000 | Loss: 0.00007493
Iteration 77/1000 | Loss: 0.00003322
Iteration 78/1000 | Loss: 0.00003265
Iteration 79/1000 | Loss: 0.00003230
Iteration 80/1000 | Loss: 0.00071386
Iteration 81/1000 | Loss: 0.00046688
Iteration 82/1000 | Loss: 0.00015395
Iteration 83/1000 | Loss: 0.00004490
Iteration 84/1000 | Loss: 0.00003893
Iteration 85/1000 | Loss: 0.00007081
Iteration 86/1000 | Loss: 0.00003234
Iteration 87/1000 | Loss: 0.00007461
Iteration 88/1000 | Loss: 0.00003041
Iteration 89/1000 | Loss: 0.00002987
Iteration 90/1000 | Loss: 0.00002955
Iteration 91/1000 | Loss: 0.00008489
Iteration 92/1000 | Loss: 0.00002931
Iteration 93/1000 | Loss: 0.00002912
Iteration 94/1000 | Loss: 0.00002893
Iteration 95/1000 | Loss: 0.00002876
Iteration 96/1000 | Loss: 0.00002870
Iteration 97/1000 | Loss: 0.00002863
Iteration 98/1000 | Loss: 0.00002859
Iteration 99/1000 | Loss: 0.00002857
Iteration 100/1000 | Loss: 0.00002856
Iteration 101/1000 | Loss: 0.00002855
Iteration 102/1000 | Loss: 0.00002854
Iteration 103/1000 | Loss: 0.00002854
Iteration 104/1000 | Loss: 0.00002853
Iteration 105/1000 | Loss: 0.00002851
Iteration 106/1000 | Loss: 0.00002850
Iteration 107/1000 | Loss: 0.00002845
Iteration 108/1000 | Loss: 0.00002845
Iteration 109/1000 | Loss: 0.00002844
Iteration 110/1000 | Loss: 0.00002843
Iteration 111/1000 | Loss: 0.00002843
Iteration 112/1000 | Loss: 0.00002841
Iteration 113/1000 | Loss: 0.00002841
Iteration 114/1000 | Loss: 0.00002841
Iteration 115/1000 | Loss: 0.00002841
Iteration 116/1000 | Loss: 0.00002839
Iteration 117/1000 | Loss: 0.00002836
Iteration 118/1000 | Loss: 0.00002836
Iteration 119/1000 | Loss: 0.00002835
Iteration 120/1000 | Loss: 0.00002835
Iteration 121/1000 | Loss: 0.00002834
Iteration 122/1000 | Loss: 0.00002834
Iteration 123/1000 | Loss: 0.00002834
Iteration 124/1000 | Loss: 0.00002834
Iteration 125/1000 | Loss: 0.00002833
Iteration 126/1000 | Loss: 0.00002833
Iteration 127/1000 | Loss: 0.00002833
Iteration 128/1000 | Loss: 0.00002833
Iteration 129/1000 | Loss: 0.00002832
Iteration 130/1000 | Loss: 0.00002832
Iteration 131/1000 | Loss: 0.00002831
Iteration 132/1000 | Loss: 0.00002831
Iteration 133/1000 | Loss: 0.00002830
Iteration 134/1000 | Loss: 0.00002830
Iteration 135/1000 | Loss: 0.00002828
Iteration 136/1000 | Loss: 0.00002827
Iteration 137/1000 | Loss: 0.00002827
Iteration 138/1000 | Loss: 0.00067460
Iteration 139/1000 | Loss: 0.00102224
Iteration 140/1000 | Loss: 0.00111783
Iteration 141/1000 | Loss: 0.00006928
Iteration 142/1000 | Loss: 0.00004286
Iteration 143/1000 | Loss: 0.00003429
Iteration 144/1000 | Loss: 0.00003123
Iteration 145/1000 | Loss: 0.00008174
Iteration 146/1000 | Loss: 0.00002778
Iteration 147/1000 | Loss: 0.00007098
Iteration 148/1000 | Loss: 0.00002589
Iteration 149/1000 | Loss: 0.00005455
Iteration 150/1000 | Loss: 0.00002494
Iteration 151/1000 | Loss: 0.00003762
Iteration 152/1000 | Loss: 0.00002447
Iteration 153/1000 | Loss: 0.00006059
Iteration 154/1000 | Loss: 0.00002414
Iteration 155/1000 | Loss: 0.00002401
Iteration 156/1000 | Loss: 0.00002398
Iteration 157/1000 | Loss: 0.00002396
Iteration 158/1000 | Loss: 0.00002391
Iteration 159/1000 | Loss: 0.00008510
Iteration 160/1000 | Loss: 0.00002393
Iteration 161/1000 | Loss: 0.00002381
Iteration 162/1000 | Loss: 0.00002377
Iteration 163/1000 | Loss: 0.00002376
Iteration 164/1000 | Loss: 0.00002376
Iteration 165/1000 | Loss: 0.00002376
Iteration 166/1000 | Loss: 0.00002376
Iteration 167/1000 | Loss: 0.00002375
Iteration 168/1000 | Loss: 0.00002375
Iteration 169/1000 | Loss: 0.00002375
Iteration 170/1000 | Loss: 0.00002375
Iteration 171/1000 | Loss: 0.00002375
Iteration 172/1000 | Loss: 0.00002375
Iteration 173/1000 | Loss: 0.00002375
Iteration 174/1000 | Loss: 0.00002374
Iteration 175/1000 | Loss: 0.00002373
Iteration 176/1000 | Loss: 0.00002373
Iteration 177/1000 | Loss: 0.00002372
Iteration 178/1000 | Loss: 0.00002372
Iteration 179/1000 | Loss: 0.00002372
Iteration 180/1000 | Loss: 0.00002372
Iteration 181/1000 | Loss: 0.00002372
Iteration 182/1000 | Loss: 0.00002372
Iteration 183/1000 | Loss: 0.00002372
Iteration 184/1000 | Loss: 0.00002371
Iteration 185/1000 | Loss: 0.00002371
Iteration 186/1000 | Loss: 0.00002371
Iteration 187/1000 | Loss: 0.00002371
Iteration 188/1000 | Loss: 0.00002371
Iteration 189/1000 | Loss: 0.00002371
Iteration 190/1000 | Loss: 0.00002371
Iteration 191/1000 | Loss: 0.00002371
Iteration 192/1000 | Loss: 0.00002371
Iteration 193/1000 | Loss: 0.00002371
Iteration 194/1000 | Loss: 0.00002371
Iteration 195/1000 | Loss: 0.00002370
Iteration 196/1000 | Loss: 0.00002370
Iteration 197/1000 | Loss: 0.00002370
Iteration 198/1000 | Loss: 0.00002370
Iteration 199/1000 | Loss: 0.00002370
Iteration 200/1000 | Loss: 0.00002370
Iteration 201/1000 | Loss: 0.00002370
Iteration 202/1000 | Loss: 0.00002369
Iteration 203/1000 | Loss: 0.00002369
Iteration 204/1000 | Loss: 0.00002369
Iteration 205/1000 | Loss: 0.00002369
Iteration 206/1000 | Loss: 0.00002369
Iteration 207/1000 | Loss: 0.00002368
Iteration 208/1000 | Loss: 0.00002368
Iteration 209/1000 | Loss: 0.00002368
Iteration 210/1000 | Loss: 0.00002368
Iteration 211/1000 | Loss: 0.00002368
Iteration 212/1000 | Loss: 0.00002368
Iteration 213/1000 | Loss: 0.00002368
Iteration 214/1000 | Loss: 0.00002368
Iteration 215/1000 | Loss: 0.00002368
Iteration 216/1000 | Loss: 0.00002368
Iteration 217/1000 | Loss: 0.00002368
Iteration 218/1000 | Loss: 0.00002368
Iteration 219/1000 | Loss: 0.00002367
Iteration 220/1000 | Loss: 0.00002367
Iteration 221/1000 | Loss: 0.00002367
Iteration 222/1000 | Loss: 0.00002367
Iteration 223/1000 | Loss: 0.00002367
Iteration 224/1000 | Loss: 0.00002367
Iteration 225/1000 | Loss: 0.00002367
Iteration 226/1000 | Loss: 0.00002367
Iteration 227/1000 | Loss: 0.00002367
Iteration 228/1000 | Loss: 0.00002367
Iteration 229/1000 | Loss: 0.00002367
Iteration 230/1000 | Loss: 0.00002367
Iteration 231/1000 | Loss: 0.00002367
Iteration 232/1000 | Loss: 0.00002367
Iteration 233/1000 | Loss: 0.00002367
Iteration 234/1000 | Loss: 0.00002366
Iteration 235/1000 | Loss: 0.00002366
Iteration 236/1000 | Loss: 0.00002366
Iteration 237/1000 | Loss: 0.00002366
Iteration 238/1000 | Loss: 0.00002366
Iteration 239/1000 | Loss: 0.00002365
Iteration 240/1000 | Loss: 0.00002365
Iteration 241/1000 | Loss: 0.00002365
Iteration 242/1000 | Loss: 0.00002365
Iteration 243/1000 | Loss: 0.00002365
Iteration 244/1000 | Loss: 0.00002365
Iteration 245/1000 | Loss: 0.00002365
Iteration 246/1000 | Loss: 0.00002365
Iteration 247/1000 | Loss: 0.00002364
Iteration 248/1000 | Loss: 0.00002364
Iteration 249/1000 | Loss: 0.00002364
Iteration 250/1000 | Loss: 0.00002364
Iteration 251/1000 | Loss: 0.00002363
Iteration 252/1000 | Loss: 0.00002363
Iteration 253/1000 | Loss: 0.00002363
Iteration 254/1000 | Loss: 0.00002363
Iteration 255/1000 | Loss: 0.00002363
Iteration 256/1000 | Loss: 0.00002363
Iteration 257/1000 | Loss: 0.00002362
Iteration 258/1000 | Loss: 0.00002362
Iteration 259/1000 | Loss: 0.00002362
Iteration 260/1000 | Loss: 0.00002362
Iteration 261/1000 | Loss: 0.00002362
Iteration 262/1000 | Loss: 0.00002361
Iteration 263/1000 | Loss: 0.00002361
Iteration 264/1000 | Loss: 0.00002361
Iteration 265/1000 | Loss: 0.00002361
Iteration 266/1000 | Loss: 0.00002361
Iteration 267/1000 | Loss: 0.00002360
Iteration 268/1000 | Loss: 0.00002360
Iteration 269/1000 | Loss: 0.00002360
Iteration 270/1000 | Loss: 0.00002360
Iteration 271/1000 | Loss: 0.00002360
Iteration 272/1000 | Loss: 0.00002360
Iteration 273/1000 | Loss: 0.00002360
Iteration 274/1000 | Loss: 0.00002359
Iteration 275/1000 | Loss: 0.00002359
Iteration 276/1000 | Loss: 0.00002359
Iteration 277/1000 | Loss: 0.00002359
Iteration 278/1000 | Loss: 0.00002359
Iteration 279/1000 | Loss: 0.00002359
Iteration 280/1000 | Loss: 0.00002359
Iteration 281/1000 | Loss: 0.00002359
Iteration 282/1000 | Loss: 0.00002359
Iteration 283/1000 | Loss: 0.00002359
Iteration 284/1000 | Loss: 0.00002359
Iteration 285/1000 | Loss: 0.00002359
Iteration 286/1000 | Loss: 0.00002359
Iteration 287/1000 | Loss: 0.00002359
Iteration 288/1000 | Loss: 0.00002359
Iteration 289/1000 | Loss: 0.00002359
Iteration 290/1000 | Loss: 0.00002359
Iteration 291/1000 | Loss: 0.00002359
Iteration 292/1000 | Loss: 0.00002359
Iteration 293/1000 | Loss: 0.00002359
Iteration 294/1000 | Loss: 0.00002359
Iteration 295/1000 | Loss: 0.00002359
Iteration 296/1000 | Loss: 0.00002359
Iteration 297/1000 | Loss: 0.00002359
Iteration 298/1000 | Loss: 0.00002358
Iteration 299/1000 | Loss: 0.00002358
Iteration 300/1000 | Loss: 0.00002358
Iteration 301/1000 | Loss: 0.00002358
Iteration 302/1000 | Loss: 0.00002358
Iteration 303/1000 | Loss: 0.00002358
Iteration 304/1000 | Loss: 0.00002358
Iteration 305/1000 | Loss: 0.00002358
Iteration 306/1000 | Loss: 0.00002358
Iteration 307/1000 | Loss: 0.00002358
Iteration 308/1000 | Loss: 0.00002358
Iteration 309/1000 | Loss: 0.00002358
Iteration 310/1000 | Loss: 0.00002358
Iteration 311/1000 | Loss: 0.00002358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 311. Stopping optimization.
Last 5 losses: [2.3584487280459143e-05, 2.3584487280459143e-05, 2.3584487280459143e-05, 2.3584487280459143e-05, 2.3584487280459143e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3584487280459143e-05

Optimization complete. Final v2v error: 3.892284631729126 mm

Highest mean error: 6.779368877410889 mm for frame 18

Lowest mean error: 2.7962117195129395 mm for frame 92

Saving results

Total time: 227.077050447464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00703295
Iteration 2/25 | Loss: 0.00162706
Iteration 3/25 | Loss: 0.00134423
Iteration 4/25 | Loss: 0.00131426
Iteration 5/25 | Loss: 0.00129374
Iteration 6/25 | Loss: 0.00128901
Iteration 7/25 | Loss: 0.00128704
Iteration 8/25 | Loss: 0.00128647
Iteration 9/25 | Loss: 0.00128634
Iteration 10/25 | Loss: 0.00128632
Iteration 11/25 | Loss: 0.00128632
Iteration 12/25 | Loss: 0.00128632
Iteration 13/25 | Loss: 0.00128631
Iteration 14/25 | Loss: 0.00128631
Iteration 15/25 | Loss: 0.00128631
Iteration 16/25 | Loss: 0.00128631
Iteration 17/25 | Loss: 0.00128631
Iteration 18/25 | Loss: 0.00128631
Iteration 19/25 | Loss: 0.00128631
Iteration 20/25 | Loss: 0.00128631
Iteration 21/25 | Loss: 0.00128631
Iteration 22/25 | Loss: 0.00128631
Iteration 23/25 | Loss: 0.00128631
Iteration 24/25 | Loss: 0.00128631
Iteration 25/25 | Loss: 0.00128631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.50737715
Iteration 2/25 | Loss: 0.00140254
Iteration 3/25 | Loss: 0.00140253
Iteration 4/25 | Loss: 0.00140253
Iteration 5/25 | Loss: 0.00140253
Iteration 6/25 | Loss: 0.00140252
Iteration 7/25 | Loss: 0.00140252
Iteration 8/25 | Loss: 0.00140252
Iteration 9/25 | Loss: 0.00140252
Iteration 10/25 | Loss: 0.00140252
Iteration 11/25 | Loss: 0.00140252
Iteration 12/25 | Loss: 0.00140252
Iteration 13/25 | Loss: 0.00140252
Iteration 14/25 | Loss: 0.00140252
Iteration 15/25 | Loss: 0.00140252
Iteration 16/25 | Loss: 0.00140252
Iteration 17/25 | Loss: 0.00140252
Iteration 18/25 | Loss: 0.00140252
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014025229029357433, 0.0014025229029357433, 0.0014025229029357433, 0.0014025229029357433, 0.0014025229029357433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014025229029357433

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140252
Iteration 2/1000 | Loss: 0.00009430
Iteration 3/1000 | Loss: 0.00005429
Iteration 4/1000 | Loss: 0.00003773
Iteration 5/1000 | Loss: 0.00003353
Iteration 6/1000 | Loss: 0.00003033
Iteration 7/1000 | Loss: 0.00004690
Iteration 8/1000 | Loss: 0.00002994
Iteration 9/1000 | Loss: 0.00002806
Iteration 10/1000 | Loss: 0.00002722
Iteration 11/1000 | Loss: 0.00003381
Iteration 12/1000 | Loss: 0.00002776
Iteration 13/1000 | Loss: 0.00002662
Iteration 14/1000 | Loss: 0.00002594
Iteration 15/1000 | Loss: 0.00002549
Iteration 16/1000 | Loss: 0.00002504
Iteration 17/1000 | Loss: 0.00002460
Iteration 18/1000 | Loss: 0.00002407
Iteration 19/1000 | Loss: 0.00002373
Iteration 20/1000 | Loss: 0.00002359
Iteration 21/1000 | Loss: 0.00002347
Iteration 22/1000 | Loss: 0.00002323
Iteration 23/1000 | Loss: 0.00002301
Iteration 24/1000 | Loss: 0.00002283
Iteration 25/1000 | Loss: 0.00002275
Iteration 26/1000 | Loss: 0.00002271
Iteration 27/1000 | Loss: 0.00002265
Iteration 28/1000 | Loss: 0.00002263
Iteration 29/1000 | Loss: 0.00002262
Iteration 30/1000 | Loss: 0.00002262
Iteration 31/1000 | Loss: 0.00002261
Iteration 32/1000 | Loss: 0.00002260
Iteration 33/1000 | Loss: 0.00002260
Iteration 34/1000 | Loss: 0.00002260
Iteration 35/1000 | Loss: 0.00002259
Iteration 36/1000 | Loss: 0.00002258
Iteration 37/1000 | Loss: 0.00002258
Iteration 38/1000 | Loss: 0.00002250
Iteration 39/1000 | Loss: 0.00002250
Iteration 40/1000 | Loss: 0.00002249
Iteration 41/1000 | Loss: 0.00002248
Iteration 42/1000 | Loss: 0.00002248
Iteration 43/1000 | Loss: 0.00002247
Iteration 44/1000 | Loss: 0.00002245
Iteration 45/1000 | Loss: 0.00002245
Iteration 46/1000 | Loss: 0.00002245
Iteration 47/1000 | Loss: 0.00002244
Iteration 48/1000 | Loss: 0.00002242
Iteration 49/1000 | Loss: 0.00002241
Iteration 50/1000 | Loss: 0.00002240
Iteration 51/1000 | Loss: 0.00002240
Iteration 52/1000 | Loss: 0.00002240
Iteration 53/1000 | Loss: 0.00002239
Iteration 54/1000 | Loss: 0.00002239
Iteration 55/1000 | Loss: 0.00002239
Iteration 56/1000 | Loss: 0.00002238
Iteration 57/1000 | Loss: 0.00002238
Iteration 58/1000 | Loss: 0.00002237
Iteration 59/1000 | Loss: 0.00002237
Iteration 60/1000 | Loss: 0.00002237
Iteration 61/1000 | Loss: 0.00002237
Iteration 62/1000 | Loss: 0.00002237
Iteration 63/1000 | Loss: 0.00002237
Iteration 64/1000 | Loss: 0.00002237
Iteration 65/1000 | Loss: 0.00002237
Iteration 66/1000 | Loss: 0.00002237
Iteration 67/1000 | Loss: 0.00002236
Iteration 68/1000 | Loss: 0.00002236
Iteration 69/1000 | Loss: 0.00002236
Iteration 70/1000 | Loss: 0.00002236
Iteration 71/1000 | Loss: 0.00002236
Iteration 72/1000 | Loss: 0.00002236
Iteration 73/1000 | Loss: 0.00002236
Iteration 74/1000 | Loss: 0.00002234
Iteration 75/1000 | Loss: 0.00002234
Iteration 76/1000 | Loss: 0.00002233
Iteration 77/1000 | Loss: 0.00002233
Iteration 78/1000 | Loss: 0.00002233
Iteration 79/1000 | Loss: 0.00002230
Iteration 80/1000 | Loss: 0.00002230
Iteration 81/1000 | Loss: 0.00002230
Iteration 82/1000 | Loss: 0.00002229
Iteration 83/1000 | Loss: 0.00002227
Iteration 84/1000 | Loss: 0.00002227
Iteration 85/1000 | Loss: 0.00002226
Iteration 86/1000 | Loss: 0.00002226
Iteration 87/1000 | Loss: 0.00002226
Iteration 88/1000 | Loss: 0.00002225
Iteration 89/1000 | Loss: 0.00002225
Iteration 90/1000 | Loss: 0.00002225
Iteration 91/1000 | Loss: 0.00002224
Iteration 92/1000 | Loss: 0.00002224
Iteration 93/1000 | Loss: 0.00002224
Iteration 94/1000 | Loss: 0.00002224
Iteration 95/1000 | Loss: 0.00002224
Iteration 96/1000 | Loss: 0.00002224
Iteration 97/1000 | Loss: 0.00002224
Iteration 98/1000 | Loss: 0.00002224
Iteration 99/1000 | Loss: 0.00002224
Iteration 100/1000 | Loss: 0.00002223
Iteration 101/1000 | Loss: 0.00002223
Iteration 102/1000 | Loss: 0.00002223
Iteration 103/1000 | Loss: 0.00002223
Iteration 104/1000 | Loss: 0.00002223
Iteration 105/1000 | Loss: 0.00002222
Iteration 106/1000 | Loss: 0.00002222
Iteration 107/1000 | Loss: 0.00002222
Iteration 108/1000 | Loss: 0.00002222
Iteration 109/1000 | Loss: 0.00002221
Iteration 110/1000 | Loss: 0.00002221
Iteration 111/1000 | Loss: 0.00002221
Iteration 112/1000 | Loss: 0.00002221
Iteration 113/1000 | Loss: 0.00002221
Iteration 114/1000 | Loss: 0.00002220
Iteration 115/1000 | Loss: 0.00002220
Iteration 116/1000 | Loss: 0.00002220
Iteration 117/1000 | Loss: 0.00002220
Iteration 118/1000 | Loss: 0.00002220
Iteration 119/1000 | Loss: 0.00002220
Iteration 120/1000 | Loss: 0.00002220
Iteration 121/1000 | Loss: 0.00002220
Iteration 122/1000 | Loss: 0.00002220
Iteration 123/1000 | Loss: 0.00002220
Iteration 124/1000 | Loss: 0.00002219
Iteration 125/1000 | Loss: 0.00002219
Iteration 126/1000 | Loss: 0.00002219
Iteration 127/1000 | Loss: 0.00002219
Iteration 128/1000 | Loss: 0.00002219
Iteration 129/1000 | Loss: 0.00002219
Iteration 130/1000 | Loss: 0.00002219
Iteration 131/1000 | Loss: 0.00002218
Iteration 132/1000 | Loss: 0.00002218
Iteration 133/1000 | Loss: 0.00002218
Iteration 134/1000 | Loss: 0.00002218
Iteration 135/1000 | Loss: 0.00002218
Iteration 136/1000 | Loss: 0.00002218
Iteration 137/1000 | Loss: 0.00002218
Iteration 138/1000 | Loss: 0.00002217
Iteration 139/1000 | Loss: 0.00002217
Iteration 140/1000 | Loss: 0.00002217
Iteration 141/1000 | Loss: 0.00002217
Iteration 142/1000 | Loss: 0.00002217
Iteration 143/1000 | Loss: 0.00002217
Iteration 144/1000 | Loss: 0.00002216
Iteration 145/1000 | Loss: 0.00002216
Iteration 146/1000 | Loss: 0.00002216
Iteration 147/1000 | Loss: 0.00002216
Iteration 148/1000 | Loss: 0.00002216
Iteration 149/1000 | Loss: 0.00002216
Iteration 150/1000 | Loss: 0.00002216
Iteration 151/1000 | Loss: 0.00002216
Iteration 152/1000 | Loss: 0.00002216
Iteration 153/1000 | Loss: 0.00002216
Iteration 154/1000 | Loss: 0.00002215
Iteration 155/1000 | Loss: 0.00002215
Iteration 156/1000 | Loss: 0.00002215
Iteration 157/1000 | Loss: 0.00002215
Iteration 158/1000 | Loss: 0.00002215
Iteration 159/1000 | Loss: 0.00002215
Iteration 160/1000 | Loss: 0.00002215
Iteration 161/1000 | Loss: 0.00002214
Iteration 162/1000 | Loss: 0.00002214
Iteration 163/1000 | Loss: 0.00002214
Iteration 164/1000 | Loss: 0.00002214
Iteration 165/1000 | Loss: 0.00002214
Iteration 166/1000 | Loss: 0.00002214
Iteration 167/1000 | Loss: 0.00002214
Iteration 168/1000 | Loss: 0.00002213
Iteration 169/1000 | Loss: 0.00002213
Iteration 170/1000 | Loss: 0.00002213
Iteration 171/1000 | Loss: 0.00002213
Iteration 172/1000 | Loss: 0.00002213
Iteration 173/1000 | Loss: 0.00002213
Iteration 174/1000 | Loss: 0.00002213
Iteration 175/1000 | Loss: 0.00002213
Iteration 176/1000 | Loss: 0.00002212
Iteration 177/1000 | Loss: 0.00002212
Iteration 178/1000 | Loss: 0.00002212
Iteration 179/1000 | Loss: 0.00002212
Iteration 180/1000 | Loss: 0.00002212
Iteration 181/1000 | Loss: 0.00002212
Iteration 182/1000 | Loss: 0.00002212
Iteration 183/1000 | Loss: 0.00002212
Iteration 184/1000 | Loss: 0.00002212
Iteration 185/1000 | Loss: 0.00002211
Iteration 186/1000 | Loss: 0.00002211
Iteration 187/1000 | Loss: 0.00002211
Iteration 188/1000 | Loss: 0.00002211
Iteration 189/1000 | Loss: 0.00002211
Iteration 190/1000 | Loss: 0.00002211
Iteration 191/1000 | Loss: 0.00002211
Iteration 192/1000 | Loss: 0.00002211
Iteration 193/1000 | Loss: 0.00002211
Iteration 194/1000 | Loss: 0.00002210
Iteration 195/1000 | Loss: 0.00002210
Iteration 196/1000 | Loss: 0.00002210
Iteration 197/1000 | Loss: 0.00002210
Iteration 198/1000 | Loss: 0.00002210
Iteration 199/1000 | Loss: 0.00002210
Iteration 200/1000 | Loss: 0.00002210
Iteration 201/1000 | Loss: 0.00002210
Iteration 202/1000 | Loss: 0.00002210
Iteration 203/1000 | Loss: 0.00002210
Iteration 204/1000 | Loss: 0.00002210
Iteration 205/1000 | Loss: 0.00002210
Iteration 206/1000 | Loss: 0.00002210
Iteration 207/1000 | Loss: 0.00002210
Iteration 208/1000 | Loss: 0.00002210
Iteration 209/1000 | Loss: 0.00002210
Iteration 210/1000 | Loss: 0.00002210
Iteration 211/1000 | Loss: 0.00002210
Iteration 212/1000 | Loss: 0.00002210
Iteration 213/1000 | Loss: 0.00002210
Iteration 214/1000 | Loss: 0.00002210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.2099780835560523e-05, 2.2099780835560523e-05, 2.2099780835560523e-05, 2.2099780835560523e-05, 2.2099780835560523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2099780835560523e-05

Optimization complete. Final v2v error: 3.838216781616211 mm

Highest mean error: 5.133228302001953 mm for frame 39

Lowest mean error: 2.969346523284912 mm for frame 211

Saving results

Total time: 75.84386110305786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00351482
Iteration 2/25 | Loss: 0.00122238
Iteration 3/25 | Loss: 0.00116907
Iteration 4/25 | Loss: 0.00115916
Iteration 5/25 | Loss: 0.00115575
Iteration 6/25 | Loss: 0.00115448
Iteration 7/25 | Loss: 0.00115448
Iteration 8/25 | Loss: 0.00115448
Iteration 9/25 | Loss: 0.00115448
Iteration 10/25 | Loss: 0.00115448
Iteration 11/25 | Loss: 0.00115448
Iteration 12/25 | Loss: 0.00115448
Iteration 13/25 | Loss: 0.00115448
Iteration 14/25 | Loss: 0.00115448
Iteration 15/25 | Loss: 0.00115448
Iteration 16/25 | Loss: 0.00115448
Iteration 17/25 | Loss: 0.00115448
Iteration 18/25 | Loss: 0.00115448
Iteration 19/25 | Loss: 0.00115448
Iteration 20/25 | Loss: 0.00115448
Iteration 21/25 | Loss: 0.00115448
Iteration 22/25 | Loss: 0.00115448
Iteration 23/25 | Loss: 0.00115448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011544814333319664, 0.0011544814333319664, 0.0011544814333319664, 0.0011544814333319664, 0.0011544814333319664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011544814333319664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36273158
Iteration 2/25 | Loss: 0.00113791
Iteration 3/25 | Loss: 0.00113791
Iteration 4/25 | Loss: 0.00113790
Iteration 5/25 | Loss: 0.00113790
Iteration 6/25 | Loss: 0.00113790
Iteration 7/25 | Loss: 0.00113790
Iteration 8/25 | Loss: 0.00113790
Iteration 9/25 | Loss: 0.00113790
Iteration 10/25 | Loss: 0.00113790
Iteration 11/25 | Loss: 0.00113790
Iteration 12/25 | Loss: 0.00113790
Iteration 13/25 | Loss: 0.00113790
Iteration 14/25 | Loss: 0.00113790
Iteration 15/25 | Loss: 0.00113790
Iteration 16/25 | Loss: 0.00113790
Iteration 17/25 | Loss: 0.00113790
Iteration 18/25 | Loss: 0.00113790
Iteration 19/25 | Loss: 0.00113790
Iteration 20/25 | Loss: 0.00113790
Iteration 21/25 | Loss: 0.00113790
Iteration 22/25 | Loss: 0.00113790
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001137902494519949, 0.001137902494519949, 0.001137902494519949, 0.001137902494519949, 0.001137902494519949]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001137902494519949

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113790
Iteration 2/1000 | Loss: 0.00002940
Iteration 3/1000 | Loss: 0.00001751
Iteration 4/1000 | Loss: 0.00001516
Iteration 5/1000 | Loss: 0.00001399
Iteration 6/1000 | Loss: 0.00001336
Iteration 7/1000 | Loss: 0.00001280
Iteration 8/1000 | Loss: 0.00001252
Iteration 9/1000 | Loss: 0.00001244
Iteration 10/1000 | Loss: 0.00001243
Iteration 11/1000 | Loss: 0.00001233
Iteration 12/1000 | Loss: 0.00001222
Iteration 13/1000 | Loss: 0.00001221
Iteration 14/1000 | Loss: 0.00001219
Iteration 15/1000 | Loss: 0.00001219
Iteration 16/1000 | Loss: 0.00001218
Iteration 17/1000 | Loss: 0.00001218
Iteration 18/1000 | Loss: 0.00001209
Iteration 19/1000 | Loss: 0.00001206
Iteration 20/1000 | Loss: 0.00001206
Iteration 21/1000 | Loss: 0.00001204
Iteration 22/1000 | Loss: 0.00001203
Iteration 23/1000 | Loss: 0.00001198
Iteration 24/1000 | Loss: 0.00001195
Iteration 25/1000 | Loss: 0.00001194
Iteration 26/1000 | Loss: 0.00001191
Iteration 27/1000 | Loss: 0.00001187
Iteration 28/1000 | Loss: 0.00001180
Iteration 29/1000 | Loss: 0.00001179
Iteration 30/1000 | Loss: 0.00001179
Iteration 31/1000 | Loss: 0.00001178
Iteration 32/1000 | Loss: 0.00001178
Iteration 33/1000 | Loss: 0.00001177
Iteration 34/1000 | Loss: 0.00001176
Iteration 35/1000 | Loss: 0.00001176
Iteration 36/1000 | Loss: 0.00001176
Iteration 37/1000 | Loss: 0.00001176
Iteration 38/1000 | Loss: 0.00001175
Iteration 39/1000 | Loss: 0.00001175
Iteration 40/1000 | Loss: 0.00001173
Iteration 41/1000 | Loss: 0.00001173
Iteration 42/1000 | Loss: 0.00001172
Iteration 43/1000 | Loss: 0.00001172
Iteration 44/1000 | Loss: 0.00001172
Iteration 45/1000 | Loss: 0.00001171
Iteration 46/1000 | Loss: 0.00001171
Iteration 47/1000 | Loss: 0.00001170
Iteration 48/1000 | Loss: 0.00001170
Iteration 49/1000 | Loss: 0.00001170
Iteration 50/1000 | Loss: 0.00001168
Iteration 51/1000 | Loss: 0.00001167
Iteration 52/1000 | Loss: 0.00001167
Iteration 53/1000 | Loss: 0.00001167
Iteration 54/1000 | Loss: 0.00001167
Iteration 55/1000 | Loss: 0.00001167
Iteration 56/1000 | Loss: 0.00001166
Iteration 57/1000 | Loss: 0.00001166
Iteration 58/1000 | Loss: 0.00001166
Iteration 59/1000 | Loss: 0.00001165
Iteration 60/1000 | Loss: 0.00001164
Iteration 61/1000 | Loss: 0.00001164
Iteration 62/1000 | Loss: 0.00001164
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001163
Iteration 65/1000 | Loss: 0.00001162
Iteration 66/1000 | Loss: 0.00001161
Iteration 67/1000 | Loss: 0.00001161
Iteration 68/1000 | Loss: 0.00001161
Iteration 69/1000 | Loss: 0.00001161
Iteration 70/1000 | Loss: 0.00001161
Iteration 71/1000 | Loss: 0.00001161
Iteration 72/1000 | Loss: 0.00001161
Iteration 73/1000 | Loss: 0.00001161
Iteration 74/1000 | Loss: 0.00001161
Iteration 75/1000 | Loss: 0.00001161
Iteration 76/1000 | Loss: 0.00001160
Iteration 77/1000 | Loss: 0.00001160
Iteration 78/1000 | Loss: 0.00001160
Iteration 79/1000 | Loss: 0.00001160
Iteration 80/1000 | Loss: 0.00001159
Iteration 81/1000 | Loss: 0.00001159
Iteration 82/1000 | Loss: 0.00001158
Iteration 83/1000 | Loss: 0.00001158
Iteration 84/1000 | Loss: 0.00001158
Iteration 85/1000 | Loss: 0.00001158
Iteration 86/1000 | Loss: 0.00001158
Iteration 87/1000 | Loss: 0.00001158
Iteration 88/1000 | Loss: 0.00001157
Iteration 89/1000 | Loss: 0.00001157
Iteration 90/1000 | Loss: 0.00001156
Iteration 91/1000 | Loss: 0.00001156
Iteration 92/1000 | Loss: 0.00001156
Iteration 93/1000 | Loss: 0.00001156
Iteration 94/1000 | Loss: 0.00001155
Iteration 95/1000 | Loss: 0.00001155
Iteration 96/1000 | Loss: 0.00001155
Iteration 97/1000 | Loss: 0.00001154
Iteration 98/1000 | Loss: 0.00001154
Iteration 99/1000 | Loss: 0.00001154
Iteration 100/1000 | Loss: 0.00001153
Iteration 101/1000 | Loss: 0.00001153
Iteration 102/1000 | Loss: 0.00001153
Iteration 103/1000 | Loss: 0.00001153
Iteration 104/1000 | Loss: 0.00001152
Iteration 105/1000 | Loss: 0.00001152
Iteration 106/1000 | Loss: 0.00001152
Iteration 107/1000 | Loss: 0.00001152
Iteration 108/1000 | Loss: 0.00001151
Iteration 109/1000 | Loss: 0.00001151
Iteration 110/1000 | Loss: 0.00001150
Iteration 111/1000 | Loss: 0.00001149
Iteration 112/1000 | Loss: 0.00001149
Iteration 113/1000 | Loss: 0.00001149
Iteration 114/1000 | Loss: 0.00001148
Iteration 115/1000 | Loss: 0.00001148
Iteration 116/1000 | Loss: 0.00001148
Iteration 117/1000 | Loss: 0.00001148
Iteration 118/1000 | Loss: 0.00001147
Iteration 119/1000 | Loss: 0.00001146
Iteration 120/1000 | Loss: 0.00001146
Iteration 121/1000 | Loss: 0.00001145
Iteration 122/1000 | Loss: 0.00001145
Iteration 123/1000 | Loss: 0.00001144
Iteration 124/1000 | Loss: 0.00001144
Iteration 125/1000 | Loss: 0.00001143
Iteration 126/1000 | Loss: 0.00001143
Iteration 127/1000 | Loss: 0.00001142
Iteration 128/1000 | Loss: 0.00001142
Iteration 129/1000 | Loss: 0.00001141
Iteration 130/1000 | Loss: 0.00001141
Iteration 131/1000 | Loss: 0.00001141
Iteration 132/1000 | Loss: 0.00001141
Iteration 133/1000 | Loss: 0.00001141
Iteration 134/1000 | Loss: 0.00001140
Iteration 135/1000 | Loss: 0.00001140
Iteration 136/1000 | Loss: 0.00001140
Iteration 137/1000 | Loss: 0.00001140
Iteration 138/1000 | Loss: 0.00001140
Iteration 139/1000 | Loss: 0.00001140
Iteration 140/1000 | Loss: 0.00001140
Iteration 141/1000 | Loss: 0.00001139
Iteration 142/1000 | Loss: 0.00001139
Iteration 143/1000 | Loss: 0.00001139
Iteration 144/1000 | Loss: 0.00001139
Iteration 145/1000 | Loss: 0.00001139
Iteration 146/1000 | Loss: 0.00001138
Iteration 147/1000 | Loss: 0.00001138
Iteration 148/1000 | Loss: 0.00001138
Iteration 149/1000 | Loss: 0.00001138
Iteration 150/1000 | Loss: 0.00001138
Iteration 151/1000 | Loss: 0.00001138
Iteration 152/1000 | Loss: 0.00001138
Iteration 153/1000 | Loss: 0.00001137
Iteration 154/1000 | Loss: 0.00001137
Iteration 155/1000 | Loss: 0.00001137
Iteration 156/1000 | Loss: 0.00001137
Iteration 157/1000 | Loss: 0.00001137
Iteration 158/1000 | Loss: 0.00001137
Iteration 159/1000 | Loss: 0.00001137
Iteration 160/1000 | Loss: 0.00001137
Iteration 161/1000 | Loss: 0.00001137
Iteration 162/1000 | Loss: 0.00001137
Iteration 163/1000 | Loss: 0.00001137
Iteration 164/1000 | Loss: 0.00001137
Iteration 165/1000 | Loss: 0.00001137
Iteration 166/1000 | Loss: 0.00001137
Iteration 167/1000 | Loss: 0.00001137
Iteration 168/1000 | Loss: 0.00001137
Iteration 169/1000 | Loss: 0.00001136
Iteration 170/1000 | Loss: 0.00001136
Iteration 171/1000 | Loss: 0.00001136
Iteration 172/1000 | Loss: 0.00001136
Iteration 173/1000 | Loss: 0.00001136
Iteration 174/1000 | Loss: 0.00001136
Iteration 175/1000 | Loss: 0.00001136
Iteration 176/1000 | Loss: 0.00001135
Iteration 177/1000 | Loss: 0.00001135
Iteration 178/1000 | Loss: 0.00001135
Iteration 179/1000 | Loss: 0.00001135
Iteration 180/1000 | Loss: 0.00001135
Iteration 181/1000 | Loss: 0.00001135
Iteration 182/1000 | Loss: 0.00001135
Iteration 183/1000 | Loss: 0.00001135
Iteration 184/1000 | Loss: 0.00001135
Iteration 185/1000 | Loss: 0.00001135
Iteration 186/1000 | Loss: 0.00001135
Iteration 187/1000 | Loss: 0.00001134
Iteration 188/1000 | Loss: 0.00001134
Iteration 189/1000 | Loss: 0.00001134
Iteration 190/1000 | Loss: 0.00001134
Iteration 191/1000 | Loss: 0.00001134
Iteration 192/1000 | Loss: 0.00001134
Iteration 193/1000 | Loss: 0.00001134
Iteration 194/1000 | Loss: 0.00001134
Iteration 195/1000 | Loss: 0.00001134
Iteration 196/1000 | Loss: 0.00001134
Iteration 197/1000 | Loss: 0.00001134
Iteration 198/1000 | Loss: 0.00001134
Iteration 199/1000 | Loss: 0.00001134
Iteration 200/1000 | Loss: 0.00001134
Iteration 201/1000 | Loss: 0.00001134
Iteration 202/1000 | Loss: 0.00001134
Iteration 203/1000 | Loss: 0.00001134
Iteration 204/1000 | Loss: 0.00001134
Iteration 205/1000 | Loss: 0.00001134
Iteration 206/1000 | Loss: 0.00001134
Iteration 207/1000 | Loss: 0.00001134
Iteration 208/1000 | Loss: 0.00001134
Iteration 209/1000 | Loss: 0.00001134
Iteration 210/1000 | Loss: 0.00001134
Iteration 211/1000 | Loss: 0.00001134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.134103695221711e-05, 1.134103695221711e-05, 1.134103695221711e-05, 1.134103695221711e-05, 1.134103695221711e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.134103695221711e-05

Optimization complete. Final v2v error: 2.867158889770508 mm

Highest mean error: 3.6703426837921143 mm for frame 25

Lowest mean error: 2.427835702896118 mm for frame 134

Saving results

Total time: 40.0629346370697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811988
Iteration 2/25 | Loss: 0.00125032
Iteration 3/25 | Loss: 0.00118562
Iteration 4/25 | Loss: 0.00117962
Iteration 5/25 | Loss: 0.00117816
Iteration 6/25 | Loss: 0.00117816
Iteration 7/25 | Loss: 0.00117816
Iteration 8/25 | Loss: 0.00117816
Iteration 9/25 | Loss: 0.00117816
Iteration 10/25 | Loss: 0.00117816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011781614739447832, 0.0011781614739447832, 0.0011781614739447832, 0.0011781614739447832, 0.0011781614739447832]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011781614739447832

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35553014
Iteration 2/25 | Loss: 0.00094057
Iteration 3/25 | Loss: 0.00094057
Iteration 4/25 | Loss: 0.00094057
Iteration 5/25 | Loss: 0.00094057
Iteration 6/25 | Loss: 0.00094057
Iteration 7/25 | Loss: 0.00094057
Iteration 8/25 | Loss: 0.00094057
Iteration 9/25 | Loss: 0.00094057
Iteration 10/25 | Loss: 0.00094056
Iteration 11/25 | Loss: 0.00094056
Iteration 12/25 | Loss: 0.00094056
Iteration 13/25 | Loss: 0.00094056
Iteration 14/25 | Loss: 0.00094056
Iteration 15/25 | Loss: 0.00094056
Iteration 16/25 | Loss: 0.00094056
Iteration 17/25 | Loss: 0.00094056
Iteration 18/25 | Loss: 0.00094056
Iteration 19/25 | Loss: 0.00094056
Iteration 20/25 | Loss: 0.00094056
Iteration 21/25 | Loss: 0.00094056
Iteration 22/25 | Loss: 0.00094056
Iteration 23/25 | Loss: 0.00094056
Iteration 24/25 | Loss: 0.00094056
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009405639721080661, 0.0009405639721080661, 0.0009405639721080661, 0.0009405639721080661, 0.0009405639721080661]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009405639721080661

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094056
Iteration 2/1000 | Loss: 0.00002126
Iteration 3/1000 | Loss: 0.00001453
Iteration 4/1000 | Loss: 0.00001307
Iteration 5/1000 | Loss: 0.00001232
Iteration 6/1000 | Loss: 0.00001158
Iteration 7/1000 | Loss: 0.00001130
Iteration 8/1000 | Loss: 0.00001107
Iteration 9/1000 | Loss: 0.00001105
Iteration 10/1000 | Loss: 0.00001103
Iteration 11/1000 | Loss: 0.00001073
Iteration 12/1000 | Loss: 0.00001057
Iteration 13/1000 | Loss: 0.00001054
Iteration 14/1000 | Loss: 0.00001049
Iteration 15/1000 | Loss: 0.00001049
Iteration 16/1000 | Loss: 0.00001049
Iteration 17/1000 | Loss: 0.00001047
Iteration 18/1000 | Loss: 0.00001045
Iteration 19/1000 | Loss: 0.00001044
Iteration 20/1000 | Loss: 0.00001043
Iteration 21/1000 | Loss: 0.00001042
Iteration 22/1000 | Loss: 0.00001035
Iteration 23/1000 | Loss: 0.00001035
Iteration 24/1000 | Loss: 0.00001031
Iteration 25/1000 | Loss: 0.00001030
Iteration 26/1000 | Loss: 0.00001030
Iteration 27/1000 | Loss: 0.00001030
Iteration 28/1000 | Loss: 0.00001030
Iteration 29/1000 | Loss: 0.00001030
Iteration 30/1000 | Loss: 0.00001030
Iteration 31/1000 | Loss: 0.00001030
Iteration 32/1000 | Loss: 0.00001030
Iteration 33/1000 | Loss: 0.00001030
Iteration 34/1000 | Loss: 0.00001029
Iteration 35/1000 | Loss: 0.00001028
Iteration 36/1000 | Loss: 0.00001027
Iteration 37/1000 | Loss: 0.00001026
Iteration 38/1000 | Loss: 0.00001026
Iteration 39/1000 | Loss: 0.00001025
Iteration 40/1000 | Loss: 0.00001025
Iteration 41/1000 | Loss: 0.00001025
Iteration 42/1000 | Loss: 0.00001024
Iteration 43/1000 | Loss: 0.00001024
Iteration 44/1000 | Loss: 0.00001023
Iteration 45/1000 | Loss: 0.00001023
Iteration 46/1000 | Loss: 0.00001022
Iteration 47/1000 | Loss: 0.00001021
Iteration 48/1000 | Loss: 0.00001015
Iteration 49/1000 | Loss: 0.00001014
Iteration 50/1000 | Loss: 0.00001010
Iteration 51/1000 | Loss: 0.00001003
Iteration 52/1000 | Loss: 0.00001003
Iteration 53/1000 | Loss: 0.00001003
Iteration 54/1000 | Loss: 0.00001003
Iteration 55/1000 | Loss: 0.00001001
Iteration 56/1000 | Loss: 0.00001000
Iteration 57/1000 | Loss: 0.00001000
Iteration 58/1000 | Loss: 0.00000999
Iteration 59/1000 | Loss: 0.00000998
Iteration 60/1000 | Loss: 0.00000998
Iteration 61/1000 | Loss: 0.00000998
Iteration 62/1000 | Loss: 0.00000998
Iteration 63/1000 | Loss: 0.00000997
Iteration 64/1000 | Loss: 0.00000997
Iteration 65/1000 | Loss: 0.00000996
Iteration 66/1000 | Loss: 0.00000996
Iteration 67/1000 | Loss: 0.00000995
Iteration 68/1000 | Loss: 0.00000995
Iteration 69/1000 | Loss: 0.00000995
Iteration 70/1000 | Loss: 0.00000994
Iteration 71/1000 | Loss: 0.00000994
Iteration 72/1000 | Loss: 0.00000994
Iteration 73/1000 | Loss: 0.00000994
Iteration 74/1000 | Loss: 0.00000994
Iteration 75/1000 | Loss: 0.00000994
Iteration 76/1000 | Loss: 0.00000994
Iteration 77/1000 | Loss: 0.00000993
Iteration 78/1000 | Loss: 0.00000993
Iteration 79/1000 | Loss: 0.00000992
Iteration 80/1000 | Loss: 0.00000992
Iteration 81/1000 | Loss: 0.00000992
Iteration 82/1000 | Loss: 0.00000991
Iteration 83/1000 | Loss: 0.00000991
Iteration 84/1000 | Loss: 0.00000991
Iteration 85/1000 | Loss: 0.00000991
Iteration 86/1000 | Loss: 0.00000991
Iteration 87/1000 | Loss: 0.00000991
Iteration 88/1000 | Loss: 0.00000990
Iteration 89/1000 | Loss: 0.00000990
Iteration 90/1000 | Loss: 0.00000990
Iteration 91/1000 | Loss: 0.00000990
Iteration 92/1000 | Loss: 0.00000990
Iteration 93/1000 | Loss: 0.00000990
Iteration 94/1000 | Loss: 0.00000989
Iteration 95/1000 | Loss: 0.00000989
Iteration 96/1000 | Loss: 0.00000989
Iteration 97/1000 | Loss: 0.00000989
Iteration 98/1000 | Loss: 0.00000989
Iteration 99/1000 | Loss: 0.00000988
Iteration 100/1000 | Loss: 0.00000988
Iteration 101/1000 | Loss: 0.00000988
Iteration 102/1000 | Loss: 0.00000988
Iteration 103/1000 | Loss: 0.00000988
Iteration 104/1000 | Loss: 0.00000988
Iteration 105/1000 | Loss: 0.00000987
Iteration 106/1000 | Loss: 0.00000987
Iteration 107/1000 | Loss: 0.00000987
Iteration 108/1000 | Loss: 0.00000987
Iteration 109/1000 | Loss: 0.00000986
Iteration 110/1000 | Loss: 0.00000986
Iteration 111/1000 | Loss: 0.00000986
Iteration 112/1000 | Loss: 0.00000986
Iteration 113/1000 | Loss: 0.00000986
Iteration 114/1000 | Loss: 0.00000985
Iteration 115/1000 | Loss: 0.00000985
Iteration 116/1000 | Loss: 0.00000985
Iteration 117/1000 | Loss: 0.00000985
Iteration 118/1000 | Loss: 0.00000985
Iteration 119/1000 | Loss: 0.00000985
Iteration 120/1000 | Loss: 0.00000985
Iteration 121/1000 | Loss: 0.00000985
Iteration 122/1000 | Loss: 0.00000984
Iteration 123/1000 | Loss: 0.00000984
Iteration 124/1000 | Loss: 0.00000984
Iteration 125/1000 | Loss: 0.00000984
Iteration 126/1000 | Loss: 0.00000984
Iteration 127/1000 | Loss: 0.00000984
Iteration 128/1000 | Loss: 0.00000984
Iteration 129/1000 | Loss: 0.00000984
Iteration 130/1000 | Loss: 0.00000984
Iteration 131/1000 | Loss: 0.00000984
Iteration 132/1000 | Loss: 0.00000983
Iteration 133/1000 | Loss: 0.00000983
Iteration 134/1000 | Loss: 0.00000982
Iteration 135/1000 | Loss: 0.00000982
Iteration 136/1000 | Loss: 0.00000982
Iteration 137/1000 | Loss: 0.00000981
Iteration 138/1000 | Loss: 0.00000981
Iteration 139/1000 | Loss: 0.00000981
Iteration 140/1000 | Loss: 0.00000981
Iteration 141/1000 | Loss: 0.00000981
Iteration 142/1000 | Loss: 0.00000980
Iteration 143/1000 | Loss: 0.00000980
Iteration 144/1000 | Loss: 0.00000980
Iteration 145/1000 | Loss: 0.00000979
Iteration 146/1000 | Loss: 0.00000979
Iteration 147/1000 | Loss: 0.00000978
Iteration 148/1000 | Loss: 0.00000978
Iteration 149/1000 | Loss: 0.00000978
Iteration 150/1000 | Loss: 0.00000978
Iteration 151/1000 | Loss: 0.00000977
Iteration 152/1000 | Loss: 0.00000977
Iteration 153/1000 | Loss: 0.00000977
Iteration 154/1000 | Loss: 0.00000977
Iteration 155/1000 | Loss: 0.00000977
Iteration 156/1000 | Loss: 0.00000977
Iteration 157/1000 | Loss: 0.00000977
Iteration 158/1000 | Loss: 0.00000977
Iteration 159/1000 | Loss: 0.00000976
Iteration 160/1000 | Loss: 0.00000976
Iteration 161/1000 | Loss: 0.00000976
Iteration 162/1000 | Loss: 0.00000976
Iteration 163/1000 | Loss: 0.00000976
Iteration 164/1000 | Loss: 0.00000976
Iteration 165/1000 | Loss: 0.00000976
Iteration 166/1000 | Loss: 0.00000976
Iteration 167/1000 | Loss: 0.00000976
Iteration 168/1000 | Loss: 0.00000976
Iteration 169/1000 | Loss: 0.00000976
Iteration 170/1000 | Loss: 0.00000975
Iteration 171/1000 | Loss: 0.00000975
Iteration 172/1000 | Loss: 0.00000975
Iteration 173/1000 | Loss: 0.00000975
Iteration 174/1000 | Loss: 0.00000975
Iteration 175/1000 | Loss: 0.00000975
Iteration 176/1000 | Loss: 0.00000975
Iteration 177/1000 | Loss: 0.00000975
Iteration 178/1000 | Loss: 0.00000975
Iteration 179/1000 | Loss: 0.00000975
Iteration 180/1000 | Loss: 0.00000975
Iteration 181/1000 | Loss: 0.00000975
Iteration 182/1000 | Loss: 0.00000974
Iteration 183/1000 | Loss: 0.00000974
Iteration 184/1000 | Loss: 0.00000974
Iteration 185/1000 | Loss: 0.00000974
Iteration 186/1000 | Loss: 0.00000974
Iteration 187/1000 | Loss: 0.00000974
Iteration 188/1000 | Loss: 0.00000974
Iteration 189/1000 | Loss: 0.00000974
Iteration 190/1000 | Loss: 0.00000974
Iteration 191/1000 | Loss: 0.00000974
Iteration 192/1000 | Loss: 0.00000974
Iteration 193/1000 | Loss: 0.00000974
Iteration 194/1000 | Loss: 0.00000973
Iteration 195/1000 | Loss: 0.00000973
Iteration 196/1000 | Loss: 0.00000973
Iteration 197/1000 | Loss: 0.00000973
Iteration 198/1000 | Loss: 0.00000973
Iteration 199/1000 | Loss: 0.00000973
Iteration 200/1000 | Loss: 0.00000973
Iteration 201/1000 | Loss: 0.00000973
Iteration 202/1000 | Loss: 0.00000973
Iteration 203/1000 | Loss: 0.00000973
Iteration 204/1000 | Loss: 0.00000973
Iteration 205/1000 | Loss: 0.00000973
Iteration 206/1000 | Loss: 0.00000973
Iteration 207/1000 | Loss: 0.00000973
Iteration 208/1000 | Loss: 0.00000973
Iteration 209/1000 | Loss: 0.00000972
Iteration 210/1000 | Loss: 0.00000972
Iteration 211/1000 | Loss: 0.00000972
Iteration 212/1000 | Loss: 0.00000972
Iteration 213/1000 | Loss: 0.00000972
Iteration 214/1000 | Loss: 0.00000972
Iteration 215/1000 | Loss: 0.00000972
Iteration 216/1000 | Loss: 0.00000972
Iteration 217/1000 | Loss: 0.00000972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [9.718919500301126e-06, 9.718919500301126e-06, 9.718919500301126e-06, 9.718919500301126e-06, 9.718919500301126e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.718919500301126e-06

Optimization complete. Final v2v error: 2.661914110183716 mm

Highest mean error: 2.8284456729888916 mm for frame 70

Lowest mean error: 2.5197224617004395 mm for frame 215

Saving results

Total time: 46.822816371917725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450506
Iteration 2/25 | Loss: 0.00129050
Iteration 3/25 | Loss: 0.00121507
Iteration 4/25 | Loss: 0.00120424
Iteration 5/25 | Loss: 0.00120079
Iteration 6/25 | Loss: 0.00120027
Iteration 7/25 | Loss: 0.00120027
Iteration 8/25 | Loss: 0.00120027
Iteration 9/25 | Loss: 0.00120027
Iteration 10/25 | Loss: 0.00120027
Iteration 11/25 | Loss: 0.00120027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012002669973298907, 0.0012002669973298907, 0.0012002669973298907, 0.0012002669973298907, 0.0012002669973298907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012002669973298907

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.42069483
Iteration 2/25 | Loss: 0.00094600
Iteration 3/25 | Loss: 0.00094599
Iteration 4/25 | Loss: 0.00094599
Iteration 5/25 | Loss: 0.00094599
Iteration 6/25 | Loss: 0.00094599
Iteration 7/25 | Loss: 0.00094599
Iteration 8/25 | Loss: 0.00094599
Iteration 9/25 | Loss: 0.00094599
Iteration 10/25 | Loss: 0.00094599
Iteration 11/25 | Loss: 0.00094599
Iteration 12/25 | Loss: 0.00094599
Iteration 13/25 | Loss: 0.00094599
Iteration 14/25 | Loss: 0.00094599
Iteration 15/25 | Loss: 0.00094599
Iteration 16/25 | Loss: 0.00094599
Iteration 17/25 | Loss: 0.00094599
Iteration 18/25 | Loss: 0.00094599
Iteration 19/25 | Loss: 0.00094599
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009459914872422814, 0.0009459914872422814, 0.0009459914872422814, 0.0009459914872422814, 0.0009459914872422814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009459914872422814

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094599
Iteration 2/1000 | Loss: 0.00003117
Iteration 3/1000 | Loss: 0.00002244
Iteration 4/1000 | Loss: 0.00001933
Iteration 5/1000 | Loss: 0.00001803
Iteration 6/1000 | Loss: 0.00001724
Iteration 7/1000 | Loss: 0.00001663
Iteration 8/1000 | Loss: 0.00001626
Iteration 9/1000 | Loss: 0.00001594
Iteration 10/1000 | Loss: 0.00001564
Iteration 11/1000 | Loss: 0.00001550
Iteration 12/1000 | Loss: 0.00001549
Iteration 13/1000 | Loss: 0.00001548
Iteration 14/1000 | Loss: 0.00001548
Iteration 15/1000 | Loss: 0.00001542
Iteration 16/1000 | Loss: 0.00001539
Iteration 17/1000 | Loss: 0.00001524
Iteration 18/1000 | Loss: 0.00001511
Iteration 19/1000 | Loss: 0.00001508
Iteration 20/1000 | Loss: 0.00001501
Iteration 21/1000 | Loss: 0.00001497
Iteration 22/1000 | Loss: 0.00001497
Iteration 23/1000 | Loss: 0.00001492
Iteration 24/1000 | Loss: 0.00001489
Iteration 25/1000 | Loss: 0.00001488
Iteration 26/1000 | Loss: 0.00001487
Iteration 27/1000 | Loss: 0.00001487
Iteration 28/1000 | Loss: 0.00001486
Iteration 29/1000 | Loss: 0.00001484
Iteration 30/1000 | Loss: 0.00001484
Iteration 31/1000 | Loss: 0.00001483
Iteration 32/1000 | Loss: 0.00001482
Iteration 33/1000 | Loss: 0.00001478
Iteration 34/1000 | Loss: 0.00001474
Iteration 35/1000 | Loss: 0.00001473
Iteration 36/1000 | Loss: 0.00001472
Iteration 37/1000 | Loss: 0.00001472
Iteration 38/1000 | Loss: 0.00001467
Iteration 39/1000 | Loss: 0.00001467
Iteration 40/1000 | Loss: 0.00001467
Iteration 41/1000 | Loss: 0.00001467
Iteration 42/1000 | Loss: 0.00001467
Iteration 43/1000 | Loss: 0.00001465
Iteration 44/1000 | Loss: 0.00001465
Iteration 45/1000 | Loss: 0.00001464
Iteration 46/1000 | Loss: 0.00001460
Iteration 47/1000 | Loss: 0.00001459
Iteration 48/1000 | Loss: 0.00001459
Iteration 49/1000 | Loss: 0.00001457
Iteration 50/1000 | Loss: 0.00001456
Iteration 51/1000 | Loss: 0.00001456
Iteration 52/1000 | Loss: 0.00001455
Iteration 53/1000 | Loss: 0.00001455
Iteration 54/1000 | Loss: 0.00001455
Iteration 55/1000 | Loss: 0.00001454
Iteration 56/1000 | Loss: 0.00001454
Iteration 57/1000 | Loss: 0.00001453
Iteration 58/1000 | Loss: 0.00001453
Iteration 59/1000 | Loss: 0.00001452
Iteration 60/1000 | Loss: 0.00001452
Iteration 61/1000 | Loss: 0.00001452
Iteration 62/1000 | Loss: 0.00001452
Iteration 63/1000 | Loss: 0.00001452
Iteration 64/1000 | Loss: 0.00001452
Iteration 65/1000 | Loss: 0.00001452
Iteration 66/1000 | Loss: 0.00001452
Iteration 67/1000 | Loss: 0.00001451
Iteration 68/1000 | Loss: 0.00001451
Iteration 69/1000 | Loss: 0.00001451
Iteration 70/1000 | Loss: 0.00001451
Iteration 71/1000 | Loss: 0.00001451
Iteration 72/1000 | Loss: 0.00001451
Iteration 73/1000 | Loss: 0.00001451
Iteration 74/1000 | Loss: 0.00001451
Iteration 75/1000 | Loss: 0.00001451
Iteration 76/1000 | Loss: 0.00001450
Iteration 77/1000 | Loss: 0.00001450
Iteration 78/1000 | Loss: 0.00001450
Iteration 79/1000 | Loss: 0.00001450
Iteration 80/1000 | Loss: 0.00001449
Iteration 81/1000 | Loss: 0.00001449
Iteration 82/1000 | Loss: 0.00001449
Iteration 83/1000 | Loss: 0.00001449
Iteration 84/1000 | Loss: 0.00001449
Iteration 85/1000 | Loss: 0.00001449
Iteration 86/1000 | Loss: 0.00001448
Iteration 87/1000 | Loss: 0.00001448
Iteration 88/1000 | Loss: 0.00001448
Iteration 89/1000 | Loss: 0.00001448
Iteration 90/1000 | Loss: 0.00001447
Iteration 91/1000 | Loss: 0.00001447
Iteration 92/1000 | Loss: 0.00001447
Iteration 93/1000 | Loss: 0.00001447
Iteration 94/1000 | Loss: 0.00001447
Iteration 95/1000 | Loss: 0.00001447
Iteration 96/1000 | Loss: 0.00001447
Iteration 97/1000 | Loss: 0.00001447
Iteration 98/1000 | Loss: 0.00001447
Iteration 99/1000 | Loss: 0.00001446
Iteration 100/1000 | Loss: 0.00001446
Iteration 101/1000 | Loss: 0.00001446
Iteration 102/1000 | Loss: 0.00001446
Iteration 103/1000 | Loss: 0.00001446
Iteration 104/1000 | Loss: 0.00001446
Iteration 105/1000 | Loss: 0.00001445
Iteration 106/1000 | Loss: 0.00001445
Iteration 107/1000 | Loss: 0.00001445
Iteration 108/1000 | Loss: 0.00001445
Iteration 109/1000 | Loss: 0.00001445
Iteration 110/1000 | Loss: 0.00001445
Iteration 111/1000 | Loss: 0.00001445
Iteration 112/1000 | Loss: 0.00001445
Iteration 113/1000 | Loss: 0.00001444
Iteration 114/1000 | Loss: 0.00001444
Iteration 115/1000 | Loss: 0.00001444
Iteration 116/1000 | Loss: 0.00001444
Iteration 117/1000 | Loss: 0.00001444
Iteration 118/1000 | Loss: 0.00001444
Iteration 119/1000 | Loss: 0.00001444
Iteration 120/1000 | Loss: 0.00001444
Iteration 121/1000 | Loss: 0.00001444
Iteration 122/1000 | Loss: 0.00001444
Iteration 123/1000 | Loss: 0.00001443
Iteration 124/1000 | Loss: 0.00001443
Iteration 125/1000 | Loss: 0.00001443
Iteration 126/1000 | Loss: 0.00001443
Iteration 127/1000 | Loss: 0.00001443
Iteration 128/1000 | Loss: 0.00001443
Iteration 129/1000 | Loss: 0.00001443
Iteration 130/1000 | Loss: 0.00001443
Iteration 131/1000 | Loss: 0.00001443
Iteration 132/1000 | Loss: 0.00001443
Iteration 133/1000 | Loss: 0.00001443
Iteration 134/1000 | Loss: 0.00001442
Iteration 135/1000 | Loss: 0.00001442
Iteration 136/1000 | Loss: 0.00001442
Iteration 137/1000 | Loss: 0.00001442
Iteration 138/1000 | Loss: 0.00001442
Iteration 139/1000 | Loss: 0.00001442
Iteration 140/1000 | Loss: 0.00001442
Iteration 141/1000 | Loss: 0.00001442
Iteration 142/1000 | Loss: 0.00001442
Iteration 143/1000 | Loss: 0.00001441
Iteration 144/1000 | Loss: 0.00001441
Iteration 145/1000 | Loss: 0.00001441
Iteration 146/1000 | Loss: 0.00001441
Iteration 147/1000 | Loss: 0.00001441
Iteration 148/1000 | Loss: 0.00001441
Iteration 149/1000 | Loss: 0.00001441
Iteration 150/1000 | Loss: 0.00001441
Iteration 151/1000 | Loss: 0.00001441
Iteration 152/1000 | Loss: 0.00001441
Iteration 153/1000 | Loss: 0.00001441
Iteration 154/1000 | Loss: 0.00001440
Iteration 155/1000 | Loss: 0.00001440
Iteration 156/1000 | Loss: 0.00001440
Iteration 157/1000 | Loss: 0.00001440
Iteration 158/1000 | Loss: 0.00001439
Iteration 159/1000 | Loss: 0.00001439
Iteration 160/1000 | Loss: 0.00001439
Iteration 161/1000 | Loss: 0.00001439
Iteration 162/1000 | Loss: 0.00001439
Iteration 163/1000 | Loss: 0.00001439
Iteration 164/1000 | Loss: 0.00001439
Iteration 165/1000 | Loss: 0.00001439
Iteration 166/1000 | Loss: 0.00001439
Iteration 167/1000 | Loss: 0.00001439
Iteration 168/1000 | Loss: 0.00001439
Iteration 169/1000 | Loss: 0.00001439
Iteration 170/1000 | Loss: 0.00001439
Iteration 171/1000 | Loss: 0.00001439
Iteration 172/1000 | Loss: 0.00001439
Iteration 173/1000 | Loss: 0.00001439
Iteration 174/1000 | Loss: 0.00001438
Iteration 175/1000 | Loss: 0.00001438
Iteration 176/1000 | Loss: 0.00001438
Iteration 177/1000 | Loss: 0.00001438
Iteration 178/1000 | Loss: 0.00001438
Iteration 179/1000 | Loss: 0.00001438
Iteration 180/1000 | Loss: 0.00001438
Iteration 181/1000 | Loss: 0.00001438
Iteration 182/1000 | Loss: 0.00001438
Iteration 183/1000 | Loss: 0.00001438
Iteration 184/1000 | Loss: 0.00001438
Iteration 185/1000 | Loss: 0.00001438
Iteration 186/1000 | Loss: 0.00001438
Iteration 187/1000 | Loss: 0.00001438
Iteration 188/1000 | Loss: 0.00001438
Iteration 189/1000 | Loss: 0.00001438
Iteration 190/1000 | Loss: 0.00001438
Iteration 191/1000 | Loss: 0.00001438
Iteration 192/1000 | Loss: 0.00001437
Iteration 193/1000 | Loss: 0.00001437
Iteration 194/1000 | Loss: 0.00001437
Iteration 195/1000 | Loss: 0.00001437
Iteration 196/1000 | Loss: 0.00001437
Iteration 197/1000 | Loss: 0.00001437
Iteration 198/1000 | Loss: 0.00001437
Iteration 199/1000 | Loss: 0.00001437
Iteration 200/1000 | Loss: 0.00001437
Iteration 201/1000 | Loss: 0.00001437
Iteration 202/1000 | Loss: 0.00001437
Iteration 203/1000 | Loss: 0.00001437
Iteration 204/1000 | Loss: 0.00001437
Iteration 205/1000 | Loss: 0.00001437
Iteration 206/1000 | Loss: 0.00001436
Iteration 207/1000 | Loss: 0.00001436
Iteration 208/1000 | Loss: 0.00001436
Iteration 209/1000 | Loss: 0.00001436
Iteration 210/1000 | Loss: 0.00001436
Iteration 211/1000 | Loss: 0.00001436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.4364855815074407e-05, 1.4364855815074407e-05, 1.4364855815074407e-05, 1.4364855815074407e-05, 1.4364855815074407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4364855815074407e-05

Optimization complete. Final v2v error: 3.1958377361297607 mm

Highest mean error: 3.7971434593200684 mm for frame 94

Lowest mean error: 2.831848621368408 mm for frame 32

Saving results

Total time: 45.0084228515625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_016/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_016/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409479
Iteration 2/25 | Loss: 0.00131709
Iteration 3/25 | Loss: 0.00122210
Iteration 4/25 | Loss: 0.00120728
Iteration 5/25 | Loss: 0.00120279
Iteration 6/25 | Loss: 0.00120180
Iteration 7/25 | Loss: 0.00120180
Iteration 8/25 | Loss: 0.00120180
Iteration 9/25 | Loss: 0.00120180
Iteration 10/25 | Loss: 0.00120180
Iteration 11/25 | Loss: 0.00120180
Iteration 12/25 | Loss: 0.00120180
Iteration 13/25 | Loss: 0.00120180
Iteration 14/25 | Loss: 0.00120180
Iteration 15/25 | Loss: 0.00120180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012017980916425586, 0.0012017980916425586, 0.0012017980916425586, 0.0012017980916425586, 0.0012017980916425586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012017980916425586

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34276736
Iteration 2/25 | Loss: 0.00107223
Iteration 3/25 | Loss: 0.00107221
Iteration 4/25 | Loss: 0.00107220
Iteration 5/25 | Loss: 0.00107220
Iteration 6/25 | Loss: 0.00107220
Iteration 7/25 | Loss: 0.00107220
Iteration 8/25 | Loss: 0.00107220
Iteration 9/25 | Loss: 0.00107220
Iteration 10/25 | Loss: 0.00107220
Iteration 11/25 | Loss: 0.00107220
Iteration 12/25 | Loss: 0.00107220
Iteration 13/25 | Loss: 0.00107220
Iteration 14/25 | Loss: 0.00107220
Iteration 15/25 | Loss: 0.00107220
Iteration 16/25 | Loss: 0.00107220
Iteration 17/25 | Loss: 0.00107220
Iteration 18/25 | Loss: 0.00107220
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010722019942477345, 0.0010722019942477345, 0.0010722019942477345, 0.0010722019942477345, 0.0010722019942477345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010722019942477345

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107220
Iteration 2/1000 | Loss: 0.00003976
Iteration 3/1000 | Loss: 0.00002694
Iteration 4/1000 | Loss: 0.00002198
Iteration 5/1000 | Loss: 0.00001972
Iteration 6/1000 | Loss: 0.00001803
Iteration 7/1000 | Loss: 0.00001696
Iteration 8/1000 | Loss: 0.00001637
Iteration 9/1000 | Loss: 0.00001588
Iteration 10/1000 | Loss: 0.00001558
Iteration 11/1000 | Loss: 0.00001544
Iteration 12/1000 | Loss: 0.00001523
Iteration 13/1000 | Loss: 0.00001500
Iteration 14/1000 | Loss: 0.00001488
Iteration 15/1000 | Loss: 0.00001487
Iteration 16/1000 | Loss: 0.00001486
Iteration 17/1000 | Loss: 0.00001483
Iteration 18/1000 | Loss: 0.00001483
Iteration 19/1000 | Loss: 0.00001482
Iteration 20/1000 | Loss: 0.00001481
Iteration 21/1000 | Loss: 0.00001480
Iteration 22/1000 | Loss: 0.00001480
Iteration 23/1000 | Loss: 0.00001479
Iteration 24/1000 | Loss: 0.00001479
Iteration 25/1000 | Loss: 0.00001478
Iteration 26/1000 | Loss: 0.00001477
Iteration 27/1000 | Loss: 0.00001477
Iteration 28/1000 | Loss: 0.00001476
Iteration 29/1000 | Loss: 0.00001476
Iteration 30/1000 | Loss: 0.00001475
Iteration 31/1000 | Loss: 0.00001475
Iteration 32/1000 | Loss: 0.00001475
Iteration 33/1000 | Loss: 0.00001474
Iteration 34/1000 | Loss: 0.00001473
Iteration 35/1000 | Loss: 0.00001472
Iteration 36/1000 | Loss: 0.00001468
Iteration 37/1000 | Loss: 0.00001468
Iteration 38/1000 | Loss: 0.00001466
Iteration 39/1000 | Loss: 0.00001466
Iteration 40/1000 | Loss: 0.00001465
Iteration 41/1000 | Loss: 0.00001464
Iteration 42/1000 | Loss: 0.00001461
Iteration 43/1000 | Loss: 0.00001460
Iteration 44/1000 | Loss: 0.00001460
Iteration 45/1000 | Loss: 0.00001460
Iteration 46/1000 | Loss: 0.00001459
Iteration 47/1000 | Loss: 0.00001459
Iteration 48/1000 | Loss: 0.00001459
Iteration 49/1000 | Loss: 0.00001459
Iteration 50/1000 | Loss: 0.00001459
Iteration 51/1000 | Loss: 0.00001458
Iteration 52/1000 | Loss: 0.00001458
Iteration 53/1000 | Loss: 0.00001458
Iteration 54/1000 | Loss: 0.00001458
Iteration 55/1000 | Loss: 0.00001457
Iteration 56/1000 | Loss: 0.00001457
Iteration 57/1000 | Loss: 0.00001456
Iteration 58/1000 | Loss: 0.00001456
Iteration 59/1000 | Loss: 0.00001455
Iteration 60/1000 | Loss: 0.00001454
Iteration 61/1000 | Loss: 0.00001453
Iteration 62/1000 | Loss: 0.00001452
Iteration 63/1000 | Loss: 0.00001452
Iteration 64/1000 | Loss: 0.00001452
Iteration 65/1000 | Loss: 0.00001451
Iteration 66/1000 | Loss: 0.00001451
Iteration 67/1000 | Loss: 0.00001451
Iteration 68/1000 | Loss: 0.00001449
Iteration 69/1000 | Loss: 0.00001449
Iteration 70/1000 | Loss: 0.00001448
Iteration 71/1000 | Loss: 0.00001448
Iteration 72/1000 | Loss: 0.00001448
Iteration 73/1000 | Loss: 0.00001447
Iteration 74/1000 | Loss: 0.00001447
Iteration 75/1000 | Loss: 0.00001447
Iteration 76/1000 | Loss: 0.00001446
Iteration 77/1000 | Loss: 0.00001446
Iteration 78/1000 | Loss: 0.00001446
Iteration 79/1000 | Loss: 0.00001446
Iteration 80/1000 | Loss: 0.00001446
Iteration 81/1000 | Loss: 0.00001446
Iteration 82/1000 | Loss: 0.00001446
Iteration 83/1000 | Loss: 0.00001446
Iteration 84/1000 | Loss: 0.00001445
Iteration 85/1000 | Loss: 0.00001445
Iteration 86/1000 | Loss: 0.00001445
Iteration 87/1000 | Loss: 0.00001444
Iteration 88/1000 | Loss: 0.00001444
Iteration 89/1000 | Loss: 0.00001444
Iteration 90/1000 | Loss: 0.00001444
Iteration 91/1000 | Loss: 0.00001444
Iteration 92/1000 | Loss: 0.00001443
Iteration 93/1000 | Loss: 0.00001443
Iteration 94/1000 | Loss: 0.00001443
Iteration 95/1000 | Loss: 0.00001443
Iteration 96/1000 | Loss: 0.00001443
Iteration 97/1000 | Loss: 0.00001442
Iteration 98/1000 | Loss: 0.00001442
Iteration 99/1000 | Loss: 0.00001442
Iteration 100/1000 | Loss: 0.00001441
Iteration 101/1000 | Loss: 0.00001441
Iteration 102/1000 | Loss: 0.00001440
Iteration 103/1000 | Loss: 0.00001440
Iteration 104/1000 | Loss: 0.00001440
Iteration 105/1000 | Loss: 0.00001439
Iteration 106/1000 | Loss: 0.00001439
Iteration 107/1000 | Loss: 0.00001439
Iteration 108/1000 | Loss: 0.00001438
Iteration 109/1000 | Loss: 0.00001438
Iteration 110/1000 | Loss: 0.00001438
Iteration 111/1000 | Loss: 0.00001438
Iteration 112/1000 | Loss: 0.00001437
Iteration 113/1000 | Loss: 0.00001437
Iteration 114/1000 | Loss: 0.00001437
Iteration 115/1000 | Loss: 0.00001437
Iteration 116/1000 | Loss: 0.00001436
Iteration 117/1000 | Loss: 0.00001436
Iteration 118/1000 | Loss: 0.00001436
Iteration 119/1000 | Loss: 0.00001436
Iteration 120/1000 | Loss: 0.00001436
Iteration 121/1000 | Loss: 0.00001436
Iteration 122/1000 | Loss: 0.00001435
Iteration 123/1000 | Loss: 0.00001435
Iteration 124/1000 | Loss: 0.00001435
Iteration 125/1000 | Loss: 0.00001435
Iteration 126/1000 | Loss: 0.00001435
Iteration 127/1000 | Loss: 0.00001435
Iteration 128/1000 | Loss: 0.00001434
Iteration 129/1000 | Loss: 0.00001434
Iteration 130/1000 | Loss: 0.00001434
Iteration 131/1000 | Loss: 0.00001434
Iteration 132/1000 | Loss: 0.00001434
Iteration 133/1000 | Loss: 0.00001434
Iteration 134/1000 | Loss: 0.00001434
Iteration 135/1000 | Loss: 0.00001434
Iteration 136/1000 | Loss: 0.00001434
Iteration 137/1000 | Loss: 0.00001433
Iteration 138/1000 | Loss: 0.00001433
Iteration 139/1000 | Loss: 0.00001433
Iteration 140/1000 | Loss: 0.00001433
Iteration 141/1000 | Loss: 0.00001433
Iteration 142/1000 | Loss: 0.00001433
Iteration 143/1000 | Loss: 0.00001433
Iteration 144/1000 | Loss: 0.00001433
Iteration 145/1000 | Loss: 0.00001433
Iteration 146/1000 | Loss: 0.00001433
Iteration 147/1000 | Loss: 0.00001432
Iteration 148/1000 | Loss: 0.00001432
Iteration 149/1000 | Loss: 0.00001432
Iteration 150/1000 | Loss: 0.00001432
Iteration 151/1000 | Loss: 0.00001432
Iteration 152/1000 | Loss: 0.00001432
Iteration 153/1000 | Loss: 0.00001432
Iteration 154/1000 | Loss: 0.00001432
Iteration 155/1000 | Loss: 0.00001431
Iteration 156/1000 | Loss: 0.00001431
Iteration 157/1000 | Loss: 0.00001431
Iteration 158/1000 | Loss: 0.00001431
Iteration 159/1000 | Loss: 0.00001431
Iteration 160/1000 | Loss: 0.00001431
Iteration 161/1000 | Loss: 0.00001431
Iteration 162/1000 | Loss: 0.00001431
Iteration 163/1000 | Loss: 0.00001431
Iteration 164/1000 | Loss: 0.00001431
Iteration 165/1000 | Loss: 0.00001431
Iteration 166/1000 | Loss: 0.00001430
Iteration 167/1000 | Loss: 0.00001430
Iteration 168/1000 | Loss: 0.00001430
Iteration 169/1000 | Loss: 0.00001430
Iteration 170/1000 | Loss: 0.00001430
Iteration 171/1000 | Loss: 0.00001430
Iteration 172/1000 | Loss: 0.00001430
Iteration 173/1000 | Loss: 0.00001430
Iteration 174/1000 | Loss: 0.00001430
Iteration 175/1000 | Loss: 0.00001430
Iteration 176/1000 | Loss: 0.00001430
Iteration 177/1000 | Loss: 0.00001430
Iteration 178/1000 | Loss: 0.00001429
Iteration 179/1000 | Loss: 0.00001429
Iteration 180/1000 | Loss: 0.00001429
Iteration 181/1000 | Loss: 0.00001429
Iteration 182/1000 | Loss: 0.00001429
Iteration 183/1000 | Loss: 0.00001428
Iteration 184/1000 | Loss: 0.00001428
Iteration 185/1000 | Loss: 0.00001428
Iteration 186/1000 | Loss: 0.00001427
Iteration 187/1000 | Loss: 0.00001427
Iteration 188/1000 | Loss: 0.00001427
Iteration 189/1000 | Loss: 0.00001427
Iteration 190/1000 | Loss: 0.00001427
Iteration 191/1000 | Loss: 0.00001427
Iteration 192/1000 | Loss: 0.00001427
Iteration 193/1000 | Loss: 0.00001427
Iteration 194/1000 | Loss: 0.00001426
Iteration 195/1000 | Loss: 0.00001426
Iteration 196/1000 | Loss: 0.00001426
Iteration 197/1000 | Loss: 0.00001426
Iteration 198/1000 | Loss: 0.00001426
Iteration 199/1000 | Loss: 0.00001425
Iteration 200/1000 | Loss: 0.00001425
Iteration 201/1000 | Loss: 0.00001425
Iteration 202/1000 | Loss: 0.00001424
Iteration 203/1000 | Loss: 0.00001424
Iteration 204/1000 | Loss: 0.00001424
Iteration 205/1000 | Loss: 0.00001424
Iteration 206/1000 | Loss: 0.00001423
Iteration 207/1000 | Loss: 0.00001423
Iteration 208/1000 | Loss: 0.00001423
Iteration 209/1000 | Loss: 0.00001423
Iteration 210/1000 | Loss: 0.00001423
Iteration 211/1000 | Loss: 0.00001423
Iteration 212/1000 | Loss: 0.00001423
Iteration 213/1000 | Loss: 0.00001423
Iteration 214/1000 | Loss: 0.00001423
Iteration 215/1000 | Loss: 0.00001423
Iteration 216/1000 | Loss: 0.00001423
Iteration 217/1000 | Loss: 0.00001423
Iteration 218/1000 | Loss: 0.00001422
Iteration 219/1000 | Loss: 0.00001422
Iteration 220/1000 | Loss: 0.00001422
Iteration 221/1000 | Loss: 0.00001422
Iteration 222/1000 | Loss: 0.00001422
Iteration 223/1000 | Loss: 0.00001422
Iteration 224/1000 | Loss: 0.00001422
Iteration 225/1000 | Loss: 0.00001422
Iteration 226/1000 | Loss: 0.00001422
Iteration 227/1000 | Loss: 0.00001422
Iteration 228/1000 | Loss: 0.00001422
Iteration 229/1000 | Loss: 0.00001422
Iteration 230/1000 | Loss: 0.00001421
Iteration 231/1000 | Loss: 0.00001421
Iteration 232/1000 | Loss: 0.00001421
Iteration 233/1000 | Loss: 0.00001421
Iteration 234/1000 | Loss: 0.00001421
Iteration 235/1000 | Loss: 0.00001421
Iteration 236/1000 | Loss: 0.00001421
Iteration 237/1000 | Loss: 0.00001421
Iteration 238/1000 | Loss: 0.00001421
Iteration 239/1000 | Loss: 0.00001421
Iteration 240/1000 | Loss: 0.00001421
Iteration 241/1000 | Loss: 0.00001421
Iteration 242/1000 | Loss: 0.00001421
Iteration 243/1000 | Loss: 0.00001421
Iteration 244/1000 | Loss: 0.00001421
Iteration 245/1000 | Loss: 0.00001421
Iteration 246/1000 | Loss: 0.00001420
Iteration 247/1000 | Loss: 0.00001420
Iteration 248/1000 | Loss: 0.00001420
Iteration 249/1000 | Loss: 0.00001420
Iteration 250/1000 | Loss: 0.00001420
Iteration 251/1000 | Loss: 0.00001420
Iteration 252/1000 | Loss: 0.00001420
Iteration 253/1000 | Loss: 0.00001420
Iteration 254/1000 | Loss: 0.00001420
Iteration 255/1000 | Loss: 0.00001420
Iteration 256/1000 | Loss: 0.00001419
Iteration 257/1000 | Loss: 0.00001419
Iteration 258/1000 | Loss: 0.00001419
Iteration 259/1000 | Loss: 0.00001419
Iteration 260/1000 | Loss: 0.00001419
Iteration 261/1000 | Loss: 0.00001419
Iteration 262/1000 | Loss: 0.00001419
Iteration 263/1000 | Loss: 0.00001419
Iteration 264/1000 | Loss: 0.00001419
Iteration 265/1000 | Loss: 0.00001419
Iteration 266/1000 | Loss: 0.00001419
Iteration 267/1000 | Loss: 0.00001419
Iteration 268/1000 | Loss: 0.00001419
Iteration 269/1000 | Loss: 0.00001419
Iteration 270/1000 | Loss: 0.00001418
Iteration 271/1000 | Loss: 0.00001418
Iteration 272/1000 | Loss: 0.00001418
Iteration 273/1000 | Loss: 0.00001418
Iteration 274/1000 | Loss: 0.00001418
Iteration 275/1000 | Loss: 0.00001418
Iteration 276/1000 | Loss: 0.00001418
Iteration 277/1000 | Loss: 0.00001418
Iteration 278/1000 | Loss: 0.00001418
Iteration 279/1000 | Loss: 0.00001418
Iteration 280/1000 | Loss: 0.00001418
Iteration 281/1000 | Loss: 0.00001418
Iteration 282/1000 | Loss: 0.00001418
Iteration 283/1000 | Loss: 0.00001418
Iteration 284/1000 | Loss: 0.00001418
Iteration 285/1000 | Loss: 0.00001418
Iteration 286/1000 | Loss: 0.00001418
Iteration 287/1000 | Loss: 0.00001418
Iteration 288/1000 | Loss: 0.00001418
Iteration 289/1000 | Loss: 0.00001418
Iteration 290/1000 | Loss: 0.00001417
Iteration 291/1000 | Loss: 0.00001417
Iteration 292/1000 | Loss: 0.00001417
Iteration 293/1000 | Loss: 0.00001417
Iteration 294/1000 | Loss: 0.00001417
Iteration 295/1000 | Loss: 0.00001417
Iteration 296/1000 | Loss: 0.00001417
Iteration 297/1000 | Loss: 0.00001417
Iteration 298/1000 | Loss: 0.00001417
Iteration 299/1000 | Loss: 0.00001417
Iteration 300/1000 | Loss: 0.00001417
Iteration 301/1000 | Loss: 0.00001417
Iteration 302/1000 | Loss: 0.00001417
Iteration 303/1000 | Loss: 0.00001417
Iteration 304/1000 | Loss: 0.00001417
Iteration 305/1000 | Loss: 0.00001417
Iteration 306/1000 | Loss: 0.00001417
Iteration 307/1000 | Loss: 0.00001417
Iteration 308/1000 | Loss: 0.00001417
Iteration 309/1000 | Loss: 0.00001417
Iteration 310/1000 | Loss: 0.00001417
Iteration 311/1000 | Loss: 0.00001417
Iteration 312/1000 | Loss: 0.00001417
Iteration 313/1000 | Loss: 0.00001417
Iteration 314/1000 | Loss: 0.00001417
Iteration 315/1000 | Loss: 0.00001417
Iteration 316/1000 | Loss: 0.00001417
Iteration 317/1000 | Loss: 0.00001417
Iteration 318/1000 | Loss: 0.00001416
Iteration 319/1000 | Loss: 0.00001416
Iteration 320/1000 | Loss: 0.00001416
Iteration 321/1000 | Loss: 0.00001416
Iteration 322/1000 | Loss: 0.00001416
Iteration 323/1000 | Loss: 0.00001416
Iteration 324/1000 | Loss: 0.00001416
Iteration 325/1000 | Loss: 0.00001416
Iteration 326/1000 | Loss: 0.00001416
Iteration 327/1000 | Loss: 0.00001416
Iteration 328/1000 | Loss: 0.00001416
Iteration 329/1000 | Loss: 0.00001416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 329. Stopping optimization.
Last 5 losses: [1.4162614206725266e-05, 1.4162614206725266e-05, 1.4162614206725266e-05, 1.4162614206725266e-05, 1.4162614206725266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4162614206725266e-05

Optimization complete. Final v2v error: 3.0926353931427 mm

Highest mean error: 5.039017677307129 mm for frame 83

Lowest mean error: 2.5965774059295654 mm for frame 164

Saving results

Total time: 52.16403079032898
