Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=50, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 2800-2855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992022
Iteration 2/25 | Loss: 0.00331928
Iteration 3/25 | Loss: 0.00209976
Iteration 4/25 | Loss: 0.00193778
Iteration 5/25 | Loss: 0.00179614
Iteration 6/25 | Loss: 0.00174837
Iteration 7/25 | Loss: 0.00172482
Iteration 8/25 | Loss: 0.00168494
Iteration 9/25 | Loss: 0.00159828
Iteration 10/25 | Loss: 0.00157856
Iteration 11/25 | Loss: 0.00154693
Iteration 12/25 | Loss: 0.00152734
Iteration 13/25 | Loss: 0.00150019
Iteration 14/25 | Loss: 0.00149583
Iteration 15/25 | Loss: 0.00148892
Iteration 16/25 | Loss: 0.00147544
Iteration 17/25 | Loss: 0.00147348
Iteration 18/25 | Loss: 0.00146815
Iteration 19/25 | Loss: 0.00146434
Iteration 20/25 | Loss: 0.00146024
Iteration 21/25 | Loss: 0.00146522
Iteration 22/25 | Loss: 0.00145804
Iteration 23/25 | Loss: 0.00145010
Iteration 24/25 | Loss: 0.00145096
Iteration 25/25 | Loss: 0.00145080

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27778542
Iteration 2/25 | Loss: 0.00463115
Iteration 3/25 | Loss: 0.00371555
Iteration 4/25 | Loss: 0.00371554
Iteration 5/25 | Loss: 0.00371554
Iteration 6/25 | Loss: 0.00371554
Iteration 7/25 | Loss: 0.00371554
Iteration 8/25 | Loss: 0.00371554
Iteration 9/25 | Loss: 0.00371554
Iteration 10/25 | Loss: 0.00371554
Iteration 11/25 | Loss: 0.00371554
Iteration 12/25 | Loss: 0.00371554
Iteration 13/25 | Loss: 0.00371554
Iteration 14/25 | Loss: 0.00371554
Iteration 15/25 | Loss: 0.00371554
Iteration 16/25 | Loss: 0.00371554
Iteration 17/25 | Loss: 0.00371554
Iteration 18/25 | Loss: 0.00371554
Iteration 19/25 | Loss: 0.00371553
Iteration 20/25 | Loss: 0.00371554
Iteration 21/25 | Loss: 0.00371554
Iteration 22/25 | Loss: 0.00371553
Iteration 23/25 | Loss: 0.00371553
Iteration 24/25 | Loss: 0.00371554
Iteration 25/25 | Loss: 0.00371554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00371553
Iteration 2/1000 | Loss: 0.00039761
Iteration 3/1000 | Loss: 0.00148638
Iteration 4/1000 | Loss: 0.00058193
Iteration 5/1000 | Loss: 0.00023820
Iteration 6/1000 | Loss: 0.00020563
Iteration 7/1000 | Loss: 0.00105606
Iteration 8/1000 | Loss: 0.00039908
Iteration 9/1000 | Loss: 0.00047001
Iteration 10/1000 | Loss: 0.00019127
Iteration 11/1000 | Loss: 0.00034811
Iteration 12/1000 | Loss: 0.00084733
Iteration 13/1000 | Loss: 0.00071517
Iteration 14/1000 | Loss: 0.00013604
Iteration 15/1000 | Loss: 0.00012756
Iteration 16/1000 | Loss: 0.00054308
Iteration 17/1000 | Loss: 0.00050480
Iteration 18/1000 | Loss: 0.00036854
Iteration 19/1000 | Loss: 0.00035623
Iteration 20/1000 | Loss: 0.00043424
Iteration 21/1000 | Loss: 0.00015536
Iteration 22/1000 | Loss: 0.00020398
Iteration 23/1000 | Loss: 0.00012182
Iteration 24/1000 | Loss: 0.00040572
Iteration 25/1000 | Loss: 0.00015603
Iteration 26/1000 | Loss: 0.00011804
Iteration 27/1000 | Loss: 0.00045070
Iteration 28/1000 | Loss: 0.00047790
Iteration 29/1000 | Loss: 0.00030509
Iteration 30/1000 | Loss: 0.00012663
Iteration 31/1000 | Loss: 0.00018919
Iteration 32/1000 | Loss: 0.00014934
Iteration 33/1000 | Loss: 0.00011040
Iteration 34/1000 | Loss: 0.00011803
Iteration 35/1000 | Loss: 0.00023922
Iteration 36/1000 | Loss: 0.00011948
Iteration 37/1000 | Loss: 0.00012723
Iteration 38/1000 | Loss: 0.00010843
Iteration 39/1000 | Loss: 0.00026027
Iteration 40/1000 | Loss: 0.00065692
Iteration 41/1000 | Loss: 0.00052932
Iteration 42/1000 | Loss: 0.00032460
Iteration 43/1000 | Loss: 0.00027607
Iteration 44/1000 | Loss: 0.00024611
Iteration 45/1000 | Loss: 0.00012724
Iteration 46/1000 | Loss: 0.00011864
Iteration 47/1000 | Loss: 0.00017960
Iteration 48/1000 | Loss: 0.00014131
Iteration 49/1000 | Loss: 0.00011288
Iteration 50/1000 | Loss: 0.00010786
Iteration 51/1000 | Loss: 0.00011326
Iteration 52/1000 | Loss: 0.00009850
Iteration 53/1000 | Loss: 0.00033340
Iteration 54/1000 | Loss: 0.00027793
Iteration 55/1000 | Loss: 0.00020419
Iteration 56/1000 | Loss: 0.00011135
Iteration 57/1000 | Loss: 0.00010246
Iteration 58/1000 | Loss: 0.00027480
Iteration 59/1000 | Loss: 0.00035272
Iteration 60/1000 | Loss: 0.00024245
Iteration 61/1000 | Loss: 0.00042311
Iteration 62/1000 | Loss: 0.00035378
Iteration 63/1000 | Loss: 0.00039151
Iteration 64/1000 | Loss: 0.00032835
Iteration 65/1000 | Loss: 0.00035441
Iteration 66/1000 | Loss: 0.00019946
Iteration 67/1000 | Loss: 0.00021032
Iteration 68/1000 | Loss: 0.00011863
Iteration 69/1000 | Loss: 0.00011146
Iteration 70/1000 | Loss: 0.00014503
Iteration 71/1000 | Loss: 0.00015735
Iteration 72/1000 | Loss: 0.00014385
Iteration 73/1000 | Loss: 0.00016659
Iteration 74/1000 | Loss: 0.00014115
Iteration 75/1000 | Loss: 0.00019335
Iteration 76/1000 | Loss: 0.00018616
Iteration 77/1000 | Loss: 0.00014094
Iteration 78/1000 | Loss: 0.00017618
Iteration 79/1000 | Loss: 0.00018520
Iteration 80/1000 | Loss: 0.00019491
Iteration 81/1000 | Loss: 0.00018022
Iteration 82/1000 | Loss: 0.00019190
Iteration 83/1000 | Loss: 0.00014591
Iteration 84/1000 | Loss: 0.00018590
Iteration 85/1000 | Loss: 0.00014062
Iteration 86/1000 | Loss: 0.00016429
Iteration 87/1000 | Loss: 0.00018494
Iteration 88/1000 | Loss: 0.00027325
Iteration 89/1000 | Loss: 0.00024127
Iteration 90/1000 | Loss: 0.00010264
Iteration 91/1000 | Loss: 0.00011303
Iteration 92/1000 | Loss: 0.00010238
Iteration 93/1000 | Loss: 0.00011029
Iteration 94/1000 | Loss: 0.00053860
Iteration 95/1000 | Loss: 0.00052511
Iteration 96/1000 | Loss: 0.00019246
Iteration 97/1000 | Loss: 0.00010874
Iteration 98/1000 | Loss: 0.00049646
Iteration 99/1000 | Loss: 0.00036622
Iteration 100/1000 | Loss: 0.00022404
Iteration 101/1000 | Loss: 0.00025041
Iteration 102/1000 | Loss: 0.00016990
Iteration 103/1000 | Loss: 0.00009855
Iteration 104/1000 | Loss: 0.00009466
Iteration 105/1000 | Loss: 0.00010935
Iteration 106/1000 | Loss: 0.00009359
Iteration 107/1000 | Loss: 0.00009509
Iteration 108/1000 | Loss: 0.00010464
Iteration 109/1000 | Loss: 0.00009896
Iteration 110/1000 | Loss: 0.00009060
Iteration 111/1000 | Loss: 0.00020243
Iteration 112/1000 | Loss: 0.00011413
Iteration 113/1000 | Loss: 0.00009255
Iteration 114/1000 | Loss: 0.00011846
Iteration 115/1000 | Loss: 0.00009745
Iteration 116/1000 | Loss: 0.00012187
Iteration 117/1000 | Loss: 0.00009222
Iteration 118/1000 | Loss: 0.00008826
Iteration 119/1000 | Loss: 0.00009684
Iteration 120/1000 | Loss: 0.00009277
Iteration 121/1000 | Loss: 0.00020424
Iteration 122/1000 | Loss: 0.00021055
Iteration 123/1000 | Loss: 0.00014152
Iteration 124/1000 | Loss: 0.00016479
Iteration 125/1000 | Loss: 0.00011288
Iteration 126/1000 | Loss: 0.00010069
Iteration 127/1000 | Loss: 0.00009082
Iteration 128/1000 | Loss: 0.00014928
Iteration 129/1000 | Loss: 0.00009494
Iteration 130/1000 | Loss: 0.00010984
Iteration 131/1000 | Loss: 0.00012066
Iteration 132/1000 | Loss: 0.00010070
Iteration 133/1000 | Loss: 0.00010053
Iteration 134/1000 | Loss: 0.00012624
Iteration 135/1000 | Loss: 0.00010016
Iteration 136/1000 | Loss: 0.00011841
Iteration 137/1000 | Loss: 0.00010249
Iteration 138/1000 | Loss: 0.00010526
Iteration 139/1000 | Loss: 0.00009874
Iteration 140/1000 | Loss: 0.00010107
Iteration 141/1000 | Loss: 0.00011513
Iteration 142/1000 | Loss: 0.00010341
Iteration 143/1000 | Loss: 0.00009868
Iteration 144/1000 | Loss: 0.00009026
Iteration 145/1000 | Loss: 0.00009349
Iteration 146/1000 | Loss: 0.00011541
Iteration 147/1000 | Loss: 0.00010064
Iteration 148/1000 | Loss: 0.00010730
Iteration 149/1000 | Loss: 0.00009486
Iteration 150/1000 | Loss: 0.00011159
Iteration 151/1000 | Loss: 0.00010312
Iteration 152/1000 | Loss: 0.00010274
Iteration 153/1000 | Loss: 0.00010196
Iteration 154/1000 | Loss: 0.00010104
Iteration 155/1000 | Loss: 0.00010286
Iteration 156/1000 | Loss: 0.00010572
Iteration 157/1000 | Loss: 0.00010009
Iteration 158/1000 | Loss: 0.00009965
Iteration 159/1000 | Loss: 0.00012251
Iteration 160/1000 | Loss: 0.00009098
Iteration 161/1000 | Loss: 0.00008733
Iteration 162/1000 | Loss: 0.00009045
Iteration 163/1000 | Loss: 0.00008897
Iteration 164/1000 | Loss: 0.00009011
Iteration 165/1000 | Loss: 0.00019645
Iteration 166/1000 | Loss: 0.00009234
Iteration 167/1000 | Loss: 0.00012212
Iteration 168/1000 | Loss: 0.00009056
Iteration 169/1000 | Loss: 0.00020801
Iteration 170/1000 | Loss: 0.00016974
Iteration 171/1000 | Loss: 0.00008519
Iteration 172/1000 | Loss: 0.00008498
Iteration 173/1000 | Loss: 0.00008483
Iteration 174/1000 | Loss: 0.00008901
Iteration 175/1000 | Loss: 0.00008467
Iteration 176/1000 | Loss: 0.00008459
Iteration 177/1000 | Loss: 0.00008459
Iteration 178/1000 | Loss: 0.00008666
Iteration 179/1000 | Loss: 0.00008655
Iteration 180/1000 | Loss: 0.00076562
Iteration 181/1000 | Loss: 0.00019248
Iteration 182/1000 | Loss: 0.00018871
Iteration 183/1000 | Loss: 0.00027219
Iteration 184/1000 | Loss: 0.00018291
Iteration 185/1000 | Loss: 0.00009056
Iteration 186/1000 | Loss: 0.00044969
Iteration 187/1000 | Loss: 0.00026169
Iteration 188/1000 | Loss: 0.00029131
Iteration 189/1000 | Loss: 0.00019191
Iteration 190/1000 | Loss: 0.00015497
Iteration 191/1000 | Loss: 0.00013255
Iteration 192/1000 | Loss: 0.00009471
Iteration 193/1000 | Loss: 0.00009031
Iteration 194/1000 | Loss: 0.00009735
Iteration 195/1000 | Loss: 0.00023540
Iteration 196/1000 | Loss: 0.00027934
Iteration 197/1000 | Loss: 0.00023690
Iteration 198/1000 | Loss: 0.00013099
Iteration 199/1000 | Loss: 0.00010333
Iteration 200/1000 | Loss: 0.00008803
Iteration 201/1000 | Loss: 0.00008534
Iteration 202/1000 | Loss: 0.00008244
Iteration 203/1000 | Loss: 0.00024231
Iteration 204/1000 | Loss: 0.00008112
Iteration 205/1000 | Loss: 0.00018290
Iteration 206/1000 | Loss: 0.00007897
Iteration 207/1000 | Loss: 0.00009014
Iteration 208/1000 | Loss: 0.00007794
Iteration 209/1000 | Loss: 0.00007916
Iteration 210/1000 | Loss: 0.00013586
Iteration 211/1000 | Loss: 0.00007733
Iteration 212/1000 | Loss: 0.00028901
Iteration 213/1000 | Loss: 0.00017055
Iteration 214/1000 | Loss: 0.00020286
Iteration 215/1000 | Loss: 0.00040687
Iteration 216/1000 | Loss: 0.00035429
Iteration 217/1000 | Loss: 0.00057198
Iteration 218/1000 | Loss: 0.00076235
Iteration 219/1000 | Loss: 0.00030096
Iteration 220/1000 | Loss: 0.00031526
Iteration 221/1000 | Loss: 0.00024359
Iteration 222/1000 | Loss: 0.00029147
Iteration 223/1000 | Loss: 0.00009491
Iteration 224/1000 | Loss: 0.00009082
Iteration 225/1000 | Loss: 0.00008974
Iteration 226/1000 | Loss: 0.00008074
Iteration 227/1000 | Loss: 0.00008577
Iteration 228/1000 | Loss: 0.00008397
Iteration 229/1000 | Loss: 0.00014188
Iteration 230/1000 | Loss: 0.00007794
Iteration 231/1000 | Loss: 0.00007698
Iteration 232/1000 | Loss: 0.00007894
Iteration 233/1000 | Loss: 0.00007643
Iteration 234/1000 | Loss: 0.00008443
Iteration 235/1000 | Loss: 0.00007579
Iteration 236/1000 | Loss: 0.00007558
Iteration 237/1000 | Loss: 0.00007557
Iteration 238/1000 | Loss: 0.00007553
Iteration 239/1000 | Loss: 0.00008844
Iteration 240/1000 | Loss: 0.00007633
Iteration 241/1000 | Loss: 0.00007538
Iteration 242/1000 | Loss: 0.00007538
Iteration 243/1000 | Loss: 0.00007606
Iteration 244/1000 | Loss: 0.00007528
Iteration 245/1000 | Loss: 0.00007528
Iteration 246/1000 | Loss: 0.00007528
Iteration 247/1000 | Loss: 0.00007528
Iteration 248/1000 | Loss: 0.00007528
Iteration 249/1000 | Loss: 0.00007527
Iteration 250/1000 | Loss: 0.00007527
Iteration 251/1000 | Loss: 0.00007527
Iteration 252/1000 | Loss: 0.00007524
Iteration 253/1000 | Loss: 0.00007522
Iteration 254/1000 | Loss: 0.00007522
Iteration 255/1000 | Loss: 0.00007521
Iteration 256/1000 | Loss: 0.00007521
Iteration 257/1000 | Loss: 0.00007520
Iteration 258/1000 | Loss: 0.00007519
Iteration 259/1000 | Loss: 0.00007519
Iteration 260/1000 | Loss: 0.00007518
Iteration 261/1000 | Loss: 0.00007518
Iteration 262/1000 | Loss: 0.00007516
Iteration 263/1000 | Loss: 0.00007515
Iteration 264/1000 | Loss: 0.00007515
Iteration 265/1000 | Loss: 0.00007514
Iteration 266/1000 | Loss: 0.00007514
Iteration 267/1000 | Loss: 0.00007514
Iteration 268/1000 | Loss: 0.00007653
Iteration 269/1000 | Loss: 0.00007505
Iteration 270/1000 | Loss: 0.00007504
Iteration 271/1000 | Loss: 0.00007504
Iteration 272/1000 | Loss: 0.00007503
Iteration 273/1000 | Loss: 0.00007503
Iteration 274/1000 | Loss: 0.00007503
Iteration 275/1000 | Loss: 0.00007503
Iteration 276/1000 | Loss: 0.00007503
Iteration 277/1000 | Loss: 0.00007502
Iteration 278/1000 | Loss: 0.00007501
Iteration 279/1000 | Loss: 0.00007500
Iteration 280/1000 | Loss: 0.00008583
Iteration 281/1000 | Loss: 0.00007598
Iteration 282/1000 | Loss: 0.00007484
Iteration 283/1000 | Loss: 0.00007483
Iteration 284/1000 | Loss: 0.00007483
Iteration 285/1000 | Loss: 0.00007483
Iteration 286/1000 | Loss: 0.00007483
Iteration 287/1000 | Loss: 0.00007625
Iteration 288/1000 | Loss: 0.00007481
Iteration 289/1000 | Loss: 0.00007481
Iteration 290/1000 | Loss: 0.00007480
Iteration 291/1000 | Loss: 0.00007480
Iteration 292/1000 | Loss: 0.00007479
Iteration 293/1000 | Loss: 0.00007479
Iteration 294/1000 | Loss: 0.00007478
Iteration 295/1000 | Loss: 0.00007478
Iteration 296/1000 | Loss: 0.00007974
Iteration 297/1000 | Loss: 0.00007505
Iteration 298/1000 | Loss: 0.00007529
Iteration 299/1000 | Loss: 0.00007470
Iteration 300/1000 | Loss: 0.00007470
Iteration 301/1000 | Loss: 0.00007470
Iteration 302/1000 | Loss: 0.00007469
Iteration 303/1000 | Loss: 0.00007469
Iteration 304/1000 | Loss: 0.00007469
Iteration 305/1000 | Loss: 0.00007469
Iteration 306/1000 | Loss: 0.00007469
Iteration 307/1000 | Loss: 0.00007466
Iteration 308/1000 | Loss: 0.00008753
Iteration 309/1000 | Loss: 0.00007908
Iteration 310/1000 | Loss: 0.00007456
Iteration 311/1000 | Loss: 0.00007448
Iteration 312/1000 | Loss: 0.00007448
Iteration 313/1000 | Loss: 0.00007447
Iteration 314/1000 | Loss: 0.00007447
Iteration 315/1000 | Loss: 0.00007447
Iteration 316/1000 | Loss: 0.00007446
Iteration 317/1000 | Loss: 0.00007446
Iteration 318/1000 | Loss: 0.00007446
Iteration 319/1000 | Loss: 0.00007446
Iteration 320/1000 | Loss: 0.00007446
Iteration 321/1000 | Loss: 0.00007445
Iteration 322/1000 | Loss: 0.00007445
Iteration 323/1000 | Loss: 0.00007445
Iteration 324/1000 | Loss: 0.00007445
Iteration 325/1000 | Loss: 0.00007879
Iteration 326/1000 | Loss: 0.00024430
Iteration 327/1000 | Loss: 0.00020018
Iteration 328/1000 | Loss: 0.00023705
Iteration 329/1000 | Loss: 0.00017148
Iteration 330/1000 | Loss: 0.00011627
Iteration 331/1000 | Loss: 0.00011015
Iteration 332/1000 | Loss: 0.00046427
Iteration 333/1000 | Loss: 0.00027077
Iteration 334/1000 | Loss: 0.00016123
Iteration 335/1000 | Loss: 0.00015782
Iteration 336/1000 | Loss: 0.00017814
Iteration 337/1000 | Loss: 0.00007751
Iteration 338/1000 | Loss: 0.00008321
Iteration 339/1000 | Loss: 0.00007723
Iteration 340/1000 | Loss: 0.00007467
Iteration 341/1000 | Loss: 0.00007401
Iteration 342/1000 | Loss: 0.00007376
Iteration 343/1000 | Loss: 0.00007733
Iteration 344/1000 | Loss: 0.00007345
Iteration 345/1000 | Loss: 0.00007343
Iteration 346/1000 | Loss: 0.00007605
Iteration 347/1000 | Loss: 0.00007328
Iteration 348/1000 | Loss: 0.00007326
Iteration 349/1000 | Loss: 0.00026414
Iteration 350/1000 | Loss: 0.00014385
Iteration 351/1000 | Loss: 0.00012155
Iteration 352/1000 | Loss: 0.00007676
Iteration 353/1000 | Loss: 0.00007482
Iteration 354/1000 | Loss: 0.00008573
Iteration 355/1000 | Loss: 0.00008450
Iteration 356/1000 | Loss: 0.00007886
Iteration 357/1000 | Loss: 0.00007782
Iteration 358/1000 | Loss: 0.00007375
Iteration 359/1000 | Loss: 0.00007283
Iteration 360/1000 | Loss: 0.00007256
Iteration 361/1000 | Loss: 0.00007985
Iteration 362/1000 | Loss: 0.00010249
Iteration 363/1000 | Loss: 0.00007249
Iteration 364/1000 | Loss: 0.00007434
Iteration 365/1000 | Loss: 0.00007236
Iteration 366/1000 | Loss: 0.00007235
Iteration 367/1000 | Loss: 0.00007235
Iteration 368/1000 | Loss: 0.00007235
Iteration 369/1000 | Loss: 0.00007234
Iteration 370/1000 | Loss: 0.00007234
Iteration 371/1000 | Loss: 0.00007234
Iteration 372/1000 | Loss: 0.00007234
Iteration 373/1000 | Loss: 0.00007234
Iteration 374/1000 | Loss: 0.00007234
Iteration 375/1000 | Loss: 0.00007234
Iteration 376/1000 | Loss: 0.00007234
Iteration 377/1000 | Loss: 0.00007234
Iteration 378/1000 | Loss: 0.00007234
Iteration 379/1000 | Loss: 0.00007234
Iteration 380/1000 | Loss: 0.00007234
Iteration 381/1000 | Loss: 0.00007234
Iteration 382/1000 | Loss: 0.00007233
Iteration 383/1000 | Loss: 0.00007233
Iteration 384/1000 | Loss: 0.00007233
Iteration 385/1000 | Loss: 0.00007233
Iteration 386/1000 | Loss: 0.00007233
Iteration 387/1000 | Loss: 0.00007233
Iteration 388/1000 | Loss: 0.00007233
Iteration 389/1000 | Loss: 0.00007233
Iteration 390/1000 | Loss: 0.00007233
Iteration 391/1000 | Loss: 0.00007232
Iteration 392/1000 | Loss: 0.00007232
Iteration 393/1000 | Loss: 0.00007232
Iteration 394/1000 | Loss: 0.00007232
Iteration 395/1000 | Loss: 0.00007232
Iteration 396/1000 | Loss: 0.00007232
Iteration 397/1000 | Loss: 0.00007319
Iteration 398/1000 | Loss: 0.00007268
Iteration 399/1000 | Loss: 0.00008262
Iteration 400/1000 | Loss: 0.00010313
Iteration 401/1000 | Loss: 0.00007230
Iteration 402/1000 | Loss: 0.00007230
Iteration 403/1000 | Loss: 0.00007229
Iteration 404/1000 | Loss: 0.00007229
Iteration 405/1000 | Loss: 0.00007229
Iteration 406/1000 | Loss: 0.00007229
Iteration 407/1000 | Loss: 0.00007229
Iteration 408/1000 | Loss: 0.00007229
Iteration 409/1000 | Loss: 0.00007229
Iteration 410/1000 | Loss: 0.00007229
Iteration 411/1000 | Loss: 0.00007229
Iteration 412/1000 | Loss: 0.00007229
Iteration 413/1000 | Loss: 0.00007229
Iteration 414/1000 | Loss: 0.00007229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 414. Stopping optimization.
Last 5 losses: [7.229296170407906e-05, 7.229296170407906e-05, 7.229296170407906e-05, 7.229296170407906e-05, 7.229296170407906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.229296170407906e-05

Optimization complete. Final v2v error: 4.2685651779174805 mm

Highest mean error: 11.958616256713867 mm for frame 149

Lowest mean error: 2.478292465209961 mm for frame 125

Saving results

Total time: 522.6480045318604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038068
Iteration 2/25 | Loss: 0.00228630
Iteration 3/25 | Loss: 0.00159290
Iteration 4/25 | Loss: 0.00155140
Iteration 5/25 | Loss: 0.00154223
Iteration 6/25 | Loss: 0.00152310
Iteration 7/25 | Loss: 0.00150857
Iteration 8/25 | Loss: 0.00147184
Iteration 9/25 | Loss: 0.00145501
Iteration 10/25 | Loss: 0.00142833
Iteration 11/25 | Loss: 0.00141613
Iteration 12/25 | Loss: 0.00141424
Iteration 13/25 | Loss: 0.00141048
Iteration 14/25 | Loss: 0.00143329
Iteration 15/25 | Loss: 0.00140460
Iteration 16/25 | Loss: 0.00139814
Iteration 17/25 | Loss: 0.00139269
Iteration 18/25 | Loss: 0.00139105
Iteration 19/25 | Loss: 0.00138960
Iteration 20/25 | Loss: 0.00139230
Iteration 21/25 | Loss: 0.00140037
Iteration 22/25 | Loss: 0.00138733
Iteration 23/25 | Loss: 0.00138458
Iteration 24/25 | Loss: 0.00138154
Iteration 25/25 | Loss: 0.00137942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.35479927
Iteration 2/25 | Loss: 0.00141192
Iteration 3/25 | Loss: 0.00141192
Iteration 4/25 | Loss: 0.00141191
Iteration 5/25 | Loss: 0.00141191
Iteration 6/25 | Loss: 0.00141191
Iteration 7/25 | Loss: 0.00141191
Iteration 8/25 | Loss: 0.00141191
Iteration 9/25 | Loss: 0.00141191
Iteration 10/25 | Loss: 0.00141191
Iteration 11/25 | Loss: 0.00141191
Iteration 12/25 | Loss: 0.00141191
Iteration 13/25 | Loss: 0.00141191
Iteration 14/25 | Loss: 0.00141191
Iteration 15/25 | Loss: 0.00141191
Iteration 16/25 | Loss: 0.00141191
Iteration 17/25 | Loss: 0.00141191
Iteration 18/25 | Loss: 0.00141191
Iteration 19/25 | Loss: 0.00141191
Iteration 20/25 | Loss: 0.00141191
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014119120314717293, 0.0014119120314717293, 0.0014119120314717293, 0.0014119120314717293, 0.0014119120314717293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014119120314717293

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141191
Iteration 2/1000 | Loss: 0.00006183
Iteration 3/1000 | Loss: 0.00003936
Iteration 4/1000 | Loss: 0.00003121
Iteration 5/1000 | Loss: 0.00002833
Iteration 6/1000 | Loss: 0.00002695
Iteration 7/1000 | Loss: 0.00002616
Iteration 8/1000 | Loss: 0.00002552
Iteration 9/1000 | Loss: 0.00002503
Iteration 10/1000 | Loss: 0.00002466
Iteration 11/1000 | Loss: 0.00002429
Iteration 12/1000 | Loss: 0.00002400
Iteration 13/1000 | Loss: 0.00010280
Iteration 14/1000 | Loss: 0.00002539
Iteration 15/1000 | Loss: 0.00002387
Iteration 16/1000 | Loss: 0.00002322
Iteration 17/1000 | Loss: 0.00002256
Iteration 18/1000 | Loss: 0.00002217
Iteration 19/1000 | Loss: 0.00002201
Iteration 20/1000 | Loss: 0.00002193
Iteration 21/1000 | Loss: 0.00002189
Iteration 22/1000 | Loss: 0.00002179
Iteration 23/1000 | Loss: 0.00002171
Iteration 24/1000 | Loss: 0.00002170
Iteration 25/1000 | Loss: 0.00002168
Iteration 26/1000 | Loss: 0.00002163
Iteration 27/1000 | Loss: 0.00002163
Iteration 28/1000 | Loss: 0.00002162
Iteration 29/1000 | Loss: 0.00002162
Iteration 30/1000 | Loss: 0.00002162
Iteration 31/1000 | Loss: 0.00002161
Iteration 32/1000 | Loss: 0.00002160
Iteration 33/1000 | Loss: 0.00002160
Iteration 34/1000 | Loss: 0.00002160
Iteration 35/1000 | Loss: 0.00002160
Iteration 36/1000 | Loss: 0.00002160
Iteration 37/1000 | Loss: 0.00002159
Iteration 38/1000 | Loss: 0.00002159
Iteration 39/1000 | Loss: 0.00002158
Iteration 40/1000 | Loss: 0.00002157
Iteration 41/1000 | Loss: 0.00002157
Iteration 42/1000 | Loss: 0.00002156
Iteration 43/1000 | Loss: 0.00002154
Iteration 44/1000 | Loss: 0.00002151
Iteration 45/1000 | Loss: 0.00002151
Iteration 46/1000 | Loss: 0.00002150
Iteration 47/1000 | Loss: 0.00002150
Iteration 48/1000 | Loss: 0.00002147
Iteration 49/1000 | Loss: 0.00002147
Iteration 50/1000 | Loss: 0.00002141
Iteration 51/1000 | Loss: 0.00002141
Iteration 52/1000 | Loss: 0.00002141
Iteration 53/1000 | Loss: 0.00002141
Iteration 54/1000 | Loss: 0.00002141
Iteration 55/1000 | Loss: 0.00002141
Iteration 56/1000 | Loss: 0.00002141
Iteration 57/1000 | Loss: 0.00002141
Iteration 58/1000 | Loss: 0.00002141
Iteration 59/1000 | Loss: 0.00002141
Iteration 60/1000 | Loss: 0.00002140
Iteration 61/1000 | Loss: 0.00002140
Iteration 62/1000 | Loss: 0.00002140
Iteration 63/1000 | Loss: 0.00002140
Iteration 64/1000 | Loss: 0.00002140
Iteration 65/1000 | Loss: 0.00002140
Iteration 66/1000 | Loss: 0.00002140
Iteration 67/1000 | Loss: 0.00002140
Iteration 68/1000 | Loss: 0.00002140
Iteration 69/1000 | Loss: 0.00002139
Iteration 70/1000 | Loss: 0.00002139
Iteration 71/1000 | Loss: 0.00002139
Iteration 72/1000 | Loss: 0.00002139
Iteration 73/1000 | Loss: 0.00002139
Iteration 74/1000 | Loss: 0.00002138
Iteration 75/1000 | Loss: 0.00002138
Iteration 76/1000 | Loss: 0.00002138
Iteration 77/1000 | Loss: 0.00002138
Iteration 78/1000 | Loss: 0.00002138
Iteration 79/1000 | Loss: 0.00002138
Iteration 80/1000 | Loss: 0.00002138
Iteration 81/1000 | Loss: 0.00002138
Iteration 82/1000 | Loss: 0.00002138
Iteration 83/1000 | Loss: 0.00002138
Iteration 84/1000 | Loss: 0.00002138
Iteration 85/1000 | Loss: 0.00002138
Iteration 86/1000 | Loss: 0.00002137
Iteration 87/1000 | Loss: 0.00002137
Iteration 88/1000 | Loss: 0.00002137
Iteration 89/1000 | Loss: 0.00002137
Iteration 90/1000 | Loss: 0.00002137
Iteration 91/1000 | Loss: 0.00002136
Iteration 92/1000 | Loss: 0.00002136
Iteration 93/1000 | Loss: 0.00002136
Iteration 94/1000 | Loss: 0.00002136
Iteration 95/1000 | Loss: 0.00002136
Iteration 96/1000 | Loss: 0.00002136
Iteration 97/1000 | Loss: 0.00002136
Iteration 98/1000 | Loss: 0.00002136
Iteration 99/1000 | Loss: 0.00002136
Iteration 100/1000 | Loss: 0.00002135
Iteration 101/1000 | Loss: 0.00002135
Iteration 102/1000 | Loss: 0.00002135
Iteration 103/1000 | Loss: 0.00002135
Iteration 104/1000 | Loss: 0.00002135
Iteration 105/1000 | Loss: 0.00002135
Iteration 106/1000 | Loss: 0.00002135
Iteration 107/1000 | Loss: 0.00002135
Iteration 108/1000 | Loss: 0.00002135
Iteration 109/1000 | Loss: 0.00002134
Iteration 110/1000 | Loss: 0.00002134
Iteration 111/1000 | Loss: 0.00002132
Iteration 112/1000 | Loss: 0.00002132
Iteration 113/1000 | Loss: 0.00002132
Iteration 114/1000 | Loss: 0.00002132
Iteration 115/1000 | Loss: 0.00002132
Iteration 116/1000 | Loss: 0.00002132
Iteration 117/1000 | Loss: 0.00002132
Iteration 118/1000 | Loss: 0.00002132
Iteration 119/1000 | Loss: 0.00002132
Iteration 120/1000 | Loss: 0.00002131
Iteration 121/1000 | Loss: 0.00002131
Iteration 122/1000 | Loss: 0.00002131
Iteration 123/1000 | Loss: 0.00002131
Iteration 124/1000 | Loss: 0.00002130
Iteration 125/1000 | Loss: 0.00002130
Iteration 126/1000 | Loss: 0.00002130
Iteration 127/1000 | Loss: 0.00002130
Iteration 128/1000 | Loss: 0.00002129
Iteration 129/1000 | Loss: 0.00002129
Iteration 130/1000 | Loss: 0.00002129
Iteration 131/1000 | Loss: 0.00002129
Iteration 132/1000 | Loss: 0.00002129
Iteration 133/1000 | Loss: 0.00002129
Iteration 134/1000 | Loss: 0.00002129
Iteration 135/1000 | Loss: 0.00002129
Iteration 136/1000 | Loss: 0.00002129
Iteration 137/1000 | Loss: 0.00002128
Iteration 138/1000 | Loss: 0.00002128
Iteration 139/1000 | Loss: 0.00002128
Iteration 140/1000 | Loss: 0.00002128
Iteration 141/1000 | Loss: 0.00002128
Iteration 142/1000 | Loss: 0.00002128
Iteration 143/1000 | Loss: 0.00002128
Iteration 144/1000 | Loss: 0.00002127
Iteration 145/1000 | Loss: 0.00002127
Iteration 146/1000 | Loss: 0.00002126
Iteration 147/1000 | Loss: 0.00002126
Iteration 148/1000 | Loss: 0.00002126
Iteration 149/1000 | Loss: 0.00002125
Iteration 150/1000 | Loss: 0.00002125
Iteration 151/1000 | Loss: 0.00002125
Iteration 152/1000 | Loss: 0.00002125
Iteration 153/1000 | Loss: 0.00002125
Iteration 154/1000 | Loss: 0.00002125
Iteration 155/1000 | Loss: 0.00002125
Iteration 156/1000 | Loss: 0.00002124
Iteration 157/1000 | Loss: 0.00002124
Iteration 158/1000 | Loss: 0.00002124
Iteration 159/1000 | Loss: 0.00002124
Iteration 160/1000 | Loss: 0.00002124
Iteration 161/1000 | Loss: 0.00002124
Iteration 162/1000 | Loss: 0.00002124
Iteration 163/1000 | Loss: 0.00002124
Iteration 164/1000 | Loss: 0.00002124
Iteration 165/1000 | Loss: 0.00002124
Iteration 166/1000 | Loss: 0.00002124
Iteration 167/1000 | Loss: 0.00002123
Iteration 168/1000 | Loss: 0.00002123
Iteration 169/1000 | Loss: 0.00002122
Iteration 170/1000 | Loss: 0.00002122
Iteration 171/1000 | Loss: 0.00002122
Iteration 172/1000 | Loss: 0.00002122
Iteration 173/1000 | Loss: 0.00002122
Iteration 174/1000 | Loss: 0.00002122
Iteration 175/1000 | Loss: 0.00002122
Iteration 176/1000 | Loss: 0.00002121
Iteration 177/1000 | Loss: 0.00002121
Iteration 178/1000 | Loss: 0.00002120
Iteration 179/1000 | Loss: 0.00002120
Iteration 180/1000 | Loss: 0.00002120
Iteration 181/1000 | Loss: 0.00002120
Iteration 182/1000 | Loss: 0.00002119
Iteration 183/1000 | Loss: 0.00002119
Iteration 184/1000 | Loss: 0.00002119
Iteration 185/1000 | Loss: 0.00002119
Iteration 186/1000 | Loss: 0.00002119
Iteration 187/1000 | Loss: 0.00002118
Iteration 188/1000 | Loss: 0.00002118
Iteration 189/1000 | Loss: 0.00002118
Iteration 190/1000 | Loss: 0.00002118
Iteration 191/1000 | Loss: 0.00002118
Iteration 192/1000 | Loss: 0.00002118
Iteration 193/1000 | Loss: 0.00002118
Iteration 194/1000 | Loss: 0.00002118
Iteration 195/1000 | Loss: 0.00002117
Iteration 196/1000 | Loss: 0.00002117
Iteration 197/1000 | Loss: 0.00002117
Iteration 198/1000 | Loss: 0.00002117
Iteration 199/1000 | Loss: 0.00002117
Iteration 200/1000 | Loss: 0.00002117
Iteration 201/1000 | Loss: 0.00002117
Iteration 202/1000 | Loss: 0.00002116
Iteration 203/1000 | Loss: 0.00002116
Iteration 204/1000 | Loss: 0.00002116
Iteration 205/1000 | Loss: 0.00002116
Iteration 206/1000 | Loss: 0.00002116
Iteration 207/1000 | Loss: 0.00002115
Iteration 208/1000 | Loss: 0.00002115
Iteration 209/1000 | Loss: 0.00002115
Iteration 210/1000 | Loss: 0.00002115
Iteration 211/1000 | Loss: 0.00002115
Iteration 212/1000 | Loss: 0.00002114
Iteration 213/1000 | Loss: 0.00002114
Iteration 214/1000 | Loss: 0.00002114
Iteration 215/1000 | Loss: 0.00002622
Iteration 216/1000 | Loss: 0.00002302
Iteration 217/1000 | Loss: 0.00002131
Iteration 218/1000 | Loss: 0.00002119
Iteration 219/1000 | Loss: 0.00002118
Iteration 220/1000 | Loss: 0.00002118
Iteration 221/1000 | Loss: 0.00002118
Iteration 222/1000 | Loss: 0.00002117
Iteration 223/1000 | Loss: 0.00002117
Iteration 224/1000 | Loss: 0.00002536
Iteration 225/1000 | Loss: 0.00002212
Iteration 226/1000 | Loss: 0.00002128
Iteration 227/1000 | Loss: 0.00002100
Iteration 228/1000 | Loss: 0.00002092
Iteration 229/1000 | Loss: 0.00002088
Iteration 230/1000 | Loss: 0.00002086
Iteration 231/1000 | Loss: 0.00002086
Iteration 232/1000 | Loss: 0.00002085
Iteration 233/1000 | Loss: 0.00002085
Iteration 234/1000 | Loss: 0.00002085
Iteration 235/1000 | Loss: 0.00002084
Iteration 236/1000 | Loss: 0.00002084
Iteration 237/1000 | Loss: 0.00002083
Iteration 238/1000 | Loss: 0.00002083
Iteration 239/1000 | Loss: 0.00002082
Iteration 240/1000 | Loss: 0.00002082
Iteration 241/1000 | Loss: 0.00002081
Iteration 242/1000 | Loss: 0.00002081
Iteration 243/1000 | Loss: 0.00002080
Iteration 244/1000 | Loss: 0.00002080
Iteration 245/1000 | Loss: 0.00002079
Iteration 246/1000 | Loss: 0.00002079
Iteration 247/1000 | Loss: 0.00002078
Iteration 248/1000 | Loss: 0.00002078
Iteration 249/1000 | Loss: 0.00002078
Iteration 250/1000 | Loss: 0.00002077
Iteration 251/1000 | Loss: 0.00002077
Iteration 252/1000 | Loss: 0.00002077
Iteration 253/1000 | Loss: 0.00002077
Iteration 254/1000 | Loss: 0.00002076
Iteration 255/1000 | Loss: 0.00002076
Iteration 256/1000 | Loss: 0.00002075
Iteration 257/1000 | Loss: 0.00002075
Iteration 258/1000 | Loss: 0.00002074
Iteration 259/1000 | Loss: 0.00002074
Iteration 260/1000 | Loss: 0.00002074
Iteration 261/1000 | Loss: 0.00002074
Iteration 262/1000 | Loss: 0.00002074
Iteration 263/1000 | Loss: 0.00002073
Iteration 264/1000 | Loss: 0.00002073
Iteration 265/1000 | Loss: 0.00002073
Iteration 266/1000 | Loss: 0.00002073
Iteration 267/1000 | Loss: 0.00002072
Iteration 268/1000 | Loss: 0.00002072
Iteration 269/1000 | Loss: 0.00002072
Iteration 270/1000 | Loss: 0.00002072
Iteration 271/1000 | Loss: 0.00002072
Iteration 272/1000 | Loss: 0.00002072
Iteration 273/1000 | Loss: 0.00002071
Iteration 274/1000 | Loss: 0.00002071
Iteration 275/1000 | Loss: 0.00002071
Iteration 276/1000 | Loss: 0.00002071
Iteration 277/1000 | Loss: 0.00002071
Iteration 278/1000 | Loss: 0.00002071
Iteration 279/1000 | Loss: 0.00002071
Iteration 280/1000 | Loss: 0.00002070
Iteration 281/1000 | Loss: 0.00002070
Iteration 282/1000 | Loss: 0.00002070
Iteration 283/1000 | Loss: 0.00002070
Iteration 284/1000 | Loss: 0.00002070
Iteration 285/1000 | Loss: 0.00002070
Iteration 286/1000 | Loss: 0.00002070
Iteration 287/1000 | Loss: 0.00002070
Iteration 288/1000 | Loss: 0.00002069
Iteration 289/1000 | Loss: 0.00002069
Iteration 290/1000 | Loss: 0.00002069
Iteration 291/1000 | Loss: 0.00002068
Iteration 292/1000 | Loss: 0.00002068
Iteration 293/1000 | Loss: 0.00002068
Iteration 294/1000 | Loss: 0.00002068
Iteration 295/1000 | Loss: 0.00002068
Iteration 296/1000 | Loss: 0.00002068
Iteration 297/1000 | Loss: 0.00002068
Iteration 298/1000 | Loss: 0.00002068
Iteration 299/1000 | Loss: 0.00002068
Iteration 300/1000 | Loss: 0.00002068
Iteration 301/1000 | Loss: 0.00002067
Iteration 302/1000 | Loss: 0.00002067
Iteration 303/1000 | Loss: 0.00002067
Iteration 304/1000 | Loss: 0.00002067
Iteration 305/1000 | Loss: 0.00002067
Iteration 306/1000 | Loss: 0.00002067
Iteration 307/1000 | Loss: 0.00002067
Iteration 308/1000 | Loss: 0.00002067
Iteration 309/1000 | Loss: 0.00002067
Iteration 310/1000 | Loss: 0.00002067
Iteration 311/1000 | Loss: 0.00002067
Iteration 312/1000 | Loss: 0.00002067
Iteration 313/1000 | Loss: 0.00002067
Iteration 314/1000 | Loss: 0.00002067
Iteration 315/1000 | Loss: 0.00002067
Iteration 316/1000 | Loss: 0.00002067
Iteration 317/1000 | Loss: 0.00002067
Iteration 318/1000 | Loss: 0.00002067
Iteration 319/1000 | Loss: 0.00002067
Iteration 320/1000 | Loss: 0.00002067
Iteration 321/1000 | Loss: 0.00002067
Iteration 322/1000 | Loss: 0.00002067
Iteration 323/1000 | Loss: 0.00002067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 323. Stopping optimization.
Last 5 losses: [2.0673418475780636e-05, 2.0673418475780636e-05, 2.0673418475780636e-05, 2.0673418475780636e-05, 2.0673418475780636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0673418475780636e-05

Optimization complete. Final v2v error: 3.714059591293335 mm

Highest mean error: 4.766198635101318 mm for frame 184

Lowest mean error: 3.232785224914551 mm for frame 142

Saving results

Total time: 111.4299669265747
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397782
Iteration 2/25 | Loss: 0.00127691
Iteration 3/25 | Loss: 0.00120315
Iteration 4/25 | Loss: 0.00119450
Iteration 5/25 | Loss: 0.00119375
Iteration 6/25 | Loss: 0.00119375
Iteration 7/25 | Loss: 0.00119375
Iteration 8/25 | Loss: 0.00119375
Iteration 9/25 | Loss: 0.00119375
Iteration 10/25 | Loss: 0.00119375
Iteration 11/25 | Loss: 0.00119375
Iteration 12/25 | Loss: 0.00119375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001193753443658352, 0.001193753443658352, 0.001193753443658352, 0.001193753443658352, 0.001193753443658352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001193753443658352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07358599
Iteration 2/25 | Loss: 0.00132931
Iteration 3/25 | Loss: 0.00132931
Iteration 4/25 | Loss: 0.00132930
Iteration 5/25 | Loss: 0.00132930
Iteration 6/25 | Loss: 0.00132930
Iteration 7/25 | Loss: 0.00132930
Iteration 8/25 | Loss: 0.00132930
Iteration 9/25 | Loss: 0.00132930
Iteration 10/25 | Loss: 0.00132930
Iteration 11/25 | Loss: 0.00132930
Iteration 12/25 | Loss: 0.00132930
Iteration 13/25 | Loss: 0.00132930
Iteration 14/25 | Loss: 0.00132930
Iteration 15/25 | Loss: 0.00132930
Iteration 16/25 | Loss: 0.00132930
Iteration 17/25 | Loss: 0.00132930
Iteration 18/25 | Loss: 0.00132930
Iteration 19/25 | Loss: 0.00132930
Iteration 20/25 | Loss: 0.00132930
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013293029041960835, 0.0013293029041960835, 0.0013293029041960835, 0.0013293029041960835, 0.0013293029041960835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013293029041960835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132930
Iteration 2/1000 | Loss: 0.00002271
Iteration 3/1000 | Loss: 0.00001607
Iteration 4/1000 | Loss: 0.00001437
Iteration 5/1000 | Loss: 0.00001332
Iteration 6/1000 | Loss: 0.00001249
Iteration 7/1000 | Loss: 0.00001211
Iteration 8/1000 | Loss: 0.00001173
Iteration 9/1000 | Loss: 0.00001134
Iteration 10/1000 | Loss: 0.00001111
Iteration 11/1000 | Loss: 0.00001094
Iteration 12/1000 | Loss: 0.00001090
Iteration 13/1000 | Loss: 0.00001082
Iteration 14/1000 | Loss: 0.00001061
Iteration 15/1000 | Loss: 0.00001046
Iteration 16/1000 | Loss: 0.00001037
Iteration 17/1000 | Loss: 0.00001036
Iteration 18/1000 | Loss: 0.00001023
Iteration 19/1000 | Loss: 0.00001022
Iteration 20/1000 | Loss: 0.00001021
Iteration 21/1000 | Loss: 0.00001017
Iteration 22/1000 | Loss: 0.00001009
Iteration 23/1000 | Loss: 0.00001006
Iteration 24/1000 | Loss: 0.00001005
Iteration 25/1000 | Loss: 0.00001005
Iteration 26/1000 | Loss: 0.00001005
Iteration 27/1000 | Loss: 0.00001003
Iteration 28/1000 | Loss: 0.00001003
Iteration 29/1000 | Loss: 0.00001003
Iteration 30/1000 | Loss: 0.00001002
Iteration 31/1000 | Loss: 0.00001000
Iteration 32/1000 | Loss: 0.00001000
Iteration 33/1000 | Loss: 0.00001000
Iteration 34/1000 | Loss: 0.00001000
Iteration 35/1000 | Loss: 0.00001000
Iteration 36/1000 | Loss: 0.00001000
Iteration 37/1000 | Loss: 0.00000999
Iteration 38/1000 | Loss: 0.00000999
Iteration 39/1000 | Loss: 0.00000999
Iteration 40/1000 | Loss: 0.00000999
Iteration 41/1000 | Loss: 0.00000999
Iteration 42/1000 | Loss: 0.00000999
Iteration 43/1000 | Loss: 0.00000999
Iteration 44/1000 | Loss: 0.00000998
Iteration 45/1000 | Loss: 0.00000996
Iteration 46/1000 | Loss: 0.00000992
Iteration 47/1000 | Loss: 0.00000992
Iteration 48/1000 | Loss: 0.00000992
Iteration 49/1000 | Loss: 0.00000991
Iteration 50/1000 | Loss: 0.00000990
Iteration 51/1000 | Loss: 0.00000989
Iteration 52/1000 | Loss: 0.00000989
Iteration 53/1000 | Loss: 0.00000988
Iteration 54/1000 | Loss: 0.00000987
Iteration 55/1000 | Loss: 0.00000987
Iteration 56/1000 | Loss: 0.00000987
Iteration 57/1000 | Loss: 0.00000986
Iteration 58/1000 | Loss: 0.00000986
Iteration 59/1000 | Loss: 0.00000985
Iteration 60/1000 | Loss: 0.00000985
Iteration 61/1000 | Loss: 0.00000984
Iteration 62/1000 | Loss: 0.00000984
Iteration 63/1000 | Loss: 0.00000984
Iteration 64/1000 | Loss: 0.00000983
Iteration 65/1000 | Loss: 0.00000983
Iteration 66/1000 | Loss: 0.00000983
Iteration 67/1000 | Loss: 0.00000983
Iteration 68/1000 | Loss: 0.00000983
Iteration 69/1000 | Loss: 0.00000982
Iteration 70/1000 | Loss: 0.00000982
Iteration 71/1000 | Loss: 0.00000981
Iteration 72/1000 | Loss: 0.00000981
Iteration 73/1000 | Loss: 0.00000980
Iteration 74/1000 | Loss: 0.00000980
Iteration 75/1000 | Loss: 0.00000979
Iteration 76/1000 | Loss: 0.00000979
Iteration 77/1000 | Loss: 0.00000979
Iteration 78/1000 | Loss: 0.00000978
Iteration 79/1000 | Loss: 0.00000977
Iteration 80/1000 | Loss: 0.00000977
Iteration 81/1000 | Loss: 0.00000976
Iteration 82/1000 | Loss: 0.00000976
Iteration 83/1000 | Loss: 0.00000975
Iteration 84/1000 | Loss: 0.00000975
Iteration 85/1000 | Loss: 0.00000975
Iteration 86/1000 | Loss: 0.00000975
Iteration 87/1000 | Loss: 0.00000974
Iteration 88/1000 | Loss: 0.00000974
Iteration 89/1000 | Loss: 0.00000974
Iteration 90/1000 | Loss: 0.00000973
Iteration 91/1000 | Loss: 0.00000971
Iteration 92/1000 | Loss: 0.00000971
Iteration 93/1000 | Loss: 0.00000971
Iteration 94/1000 | Loss: 0.00000971
Iteration 95/1000 | Loss: 0.00000971
Iteration 96/1000 | Loss: 0.00000971
Iteration 97/1000 | Loss: 0.00000971
Iteration 98/1000 | Loss: 0.00000970
Iteration 99/1000 | Loss: 0.00000970
Iteration 100/1000 | Loss: 0.00000970
Iteration 101/1000 | Loss: 0.00000970
Iteration 102/1000 | Loss: 0.00000969
Iteration 103/1000 | Loss: 0.00000969
Iteration 104/1000 | Loss: 0.00000969
Iteration 105/1000 | Loss: 0.00000969
Iteration 106/1000 | Loss: 0.00000969
Iteration 107/1000 | Loss: 0.00000969
Iteration 108/1000 | Loss: 0.00000969
Iteration 109/1000 | Loss: 0.00000969
Iteration 110/1000 | Loss: 0.00000969
Iteration 111/1000 | Loss: 0.00000969
Iteration 112/1000 | Loss: 0.00000969
Iteration 113/1000 | Loss: 0.00000969
Iteration 114/1000 | Loss: 0.00000969
Iteration 115/1000 | Loss: 0.00000969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [9.687641977507155e-06, 9.687641977507155e-06, 9.687641977507155e-06, 9.687641977507155e-06, 9.687641977507155e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.687641977507155e-06

Optimization complete. Final v2v error: 2.710397243499756 mm

Highest mean error: 3.1549415588378906 mm for frame 211

Lowest mean error: 2.602788209915161 mm for frame 77

Saving results

Total time: 42.41942620277405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00734655
Iteration 2/25 | Loss: 0.00134287
Iteration 3/25 | Loss: 0.00126814
Iteration 4/25 | Loss: 0.00126210
Iteration 5/25 | Loss: 0.00126035
Iteration 6/25 | Loss: 0.00126035
Iteration 7/25 | Loss: 0.00126035
Iteration 8/25 | Loss: 0.00126035
Iteration 9/25 | Loss: 0.00126035
Iteration 10/25 | Loss: 0.00126035
Iteration 11/25 | Loss: 0.00126035
Iteration 12/25 | Loss: 0.00126035
Iteration 13/25 | Loss: 0.00126035
Iteration 14/25 | Loss: 0.00126035
Iteration 15/25 | Loss: 0.00126035
Iteration 16/25 | Loss: 0.00126035
Iteration 17/25 | Loss: 0.00126035
Iteration 18/25 | Loss: 0.00126035
Iteration 19/25 | Loss: 0.00126035
Iteration 20/25 | Loss: 0.00126035
Iteration 21/25 | Loss: 0.00126035
Iteration 22/25 | Loss: 0.00126035
Iteration 23/25 | Loss: 0.00126035
Iteration 24/25 | Loss: 0.00126035
Iteration 25/25 | Loss: 0.00126035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27507603
Iteration 2/25 | Loss: 0.00129376
Iteration 3/25 | Loss: 0.00129369
Iteration 4/25 | Loss: 0.00129368
Iteration 5/25 | Loss: 0.00129368
Iteration 6/25 | Loss: 0.00129368
Iteration 7/25 | Loss: 0.00129368
Iteration 8/25 | Loss: 0.00129368
Iteration 9/25 | Loss: 0.00129368
Iteration 10/25 | Loss: 0.00129368
Iteration 11/25 | Loss: 0.00129368
Iteration 12/25 | Loss: 0.00129368
Iteration 13/25 | Loss: 0.00129368
Iteration 14/25 | Loss: 0.00129368
Iteration 15/25 | Loss: 0.00129368
Iteration 16/25 | Loss: 0.00129368
Iteration 17/25 | Loss: 0.00129368
Iteration 18/25 | Loss: 0.00129368
Iteration 19/25 | Loss: 0.00129368
Iteration 20/25 | Loss: 0.00129368
Iteration 21/25 | Loss: 0.00129368
Iteration 22/25 | Loss: 0.00129368
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012936824932694435, 0.0012936824932694435, 0.0012936824932694435, 0.0012936824932694435, 0.0012936824932694435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012936824932694435

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129368
Iteration 2/1000 | Loss: 0.00003745
Iteration 3/1000 | Loss: 0.00002727
Iteration 4/1000 | Loss: 0.00002405
Iteration 5/1000 | Loss: 0.00002282
Iteration 6/1000 | Loss: 0.00002184
Iteration 7/1000 | Loss: 0.00002107
Iteration 8/1000 | Loss: 0.00002063
Iteration 9/1000 | Loss: 0.00002009
Iteration 10/1000 | Loss: 0.00001974
Iteration 11/1000 | Loss: 0.00001971
Iteration 12/1000 | Loss: 0.00001941
Iteration 13/1000 | Loss: 0.00001915
Iteration 14/1000 | Loss: 0.00001883
Iteration 15/1000 | Loss: 0.00001855
Iteration 16/1000 | Loss: 0.00001829
Iteration 17/1000 | Loss: 0.00001810
Iteration 18/1000 | Loss: 0.00001808
Iteration 19/1000 | Loss: 0.00001808
Iteration 20/1000 | Loss: 0.00001808
Iteration 21/1000 | Loss: 0.00001807
Iteration 22/1000 | Loss: 0.00001805
Iteration 23/1000 | Loss: 0.00001805
Iteration 24/1000 | Loss: 0.00001805
Iteration 25/1000 | Loss: 0.00001805
Iteration 26/1000 | Loss: 0.00001804
Iteration 27/1000 | Loss: 0.00001804
Iteration 28/1000 | Loss: 0.00001804
Iteration 29/1000 | Loss: 0.00001797
Iteration 30/1000 | Loss: 0.00001793
Iteration 31/1000 | Loss: 0.00001792
Iteration 32/1000 | Loss: 0.00001792
Iteration 33/1000 | Loss: 0.00001792
Iteration 34/1000 | Loss: 0.00001792
Iteration 35/1000 | Loss: 0.00001791
Iteration 36/1000 | Loss: 0.00001791
Iteration 37/1000 | Loss: 0.00001790
Iteration 38/1000 | Loss: 0.00001790
Iteration 39/1000 | Loss: 0.00001789
Iteration 40/1000 | Loss: 0.00001788
Iteration 41/1000 | Loss: 0.00001788
Iteration 42/1000 | Loss: 0.00001788
Iteration 43/1000 | Loss: 0.00001787
Iteration 44/1000 | Loss: 0.00001786
Iteration 45/1000 | Loss: 0.00001786
Iteration 46/1000 | Loss: 0.00001785
Iteration 47/1000 | Loss: 0.00001785
Iteration 48/1000 | Loss: 0.00001784
Iteration 49/1000 | Loss: 0.00001781
Iteration 50/1000 | Loss: 0.00001779
Iteration 51/1000 | Loss: 0.00001778
Iteration 52/1000 | Loss: 0.00001778
Iteration 53/1000 | Loss: 0.00001778
Iteration 54/1000 | Loss: 0.00001778
Iteration 55/1000 | Loss: 0.00001777
Iteration 56/1000 | Loss: 0.00001777
Iteration 57/1000 | Loss: 0.00001777
Iteration 58/1000 | Loss: 0.00001776
Iteration 59/1000 | Loss: 0.00001775
Iteration 60/1000 | Loss: 0.00001775
Iteration 61/1000 | Loss: 0.00001774
Iteration 62/1000 | Loss: 0.00001774
Iteration 63/1000 | Loss: 0.00001773
Iteration 64/1000 | Loss: 0.00001772
Iteration 65/1000 | Loss: 0.00001772
Iteration 66/1000 | Loss: 0.00001771
Iteration 67/1000 | Loss: 0.00001771
Iteration 68/1000 | Loss: 0.00001770
Iteration 69/1000 | Loss: 0.00001770
Iteration 70/1000 | Loss: 0.00001770
Iteration 71/1000 | Loss: 0.00001770
Iteration 72/1000 | Loss: 0.00001769
Iteration 73/1000 | Loss: 0.00001769
Iteration 74/1000 | Loss: 0.00001769
Iteration 75/1000 | Loss: 0.00001768
Iteration 76/1000 | Loss: 0.00001767
Iteration 77/1000 | Loss: 0.00001763
Iteration 78/1000 | Loss: 0.00001762
Iteration 79/1000 | Loss: 0.00001761
Iteration 80/1000 | Loss: 0.00001760
Iteration 81/1000 | Loss: 0.00001759
Iteration 82/1000 | Loss: 0.00001757
Iteration 83/1000 | Loss: 0.00001757
Iteration 84/1000 | Loss: 0.00001757
Iteration 85/1000 | Loss: 0.00001756
Iteration 86/1000 | Loss: 0.00001756
Iteration 87/1000 | Loss: 0.00001756
Iteration 88/1000 | Loss: 0.00001755
Iteration 89/1000 | Loss: 0.00001755
Iteration 90/1000 | Loss: 0.00001755
Iteration 91/1000 | Loss: 0.00001755
Iteration 92/1000 | Loss: 0.00001755
Iteration 93/1000 | Loss: 0.00001754
Iteration 94/1000 | Loss: 0.00001754
Iteration 95/1000 | Loss: 0.00001754
Iteration 96/1000 | Loss: 0.00001754
Iteration 97/1000 | Loss: 0.00001754
Iteration 98/1000 | Loss: 0.00001754
Iteration 99/1000 | Loss: 0.00001753
Iteration 100/1000 | Loss: 0.00001753
Iteration 101/1000 | Loss: 0.00001752
Iteration 102/1000 | Loss: 0.00001752
Iteration 103/1000 | Loss: 0.00001752
Iteration 104/1000 | Loss: 0.00001752
Iteration 105/1000 | Loss: 0.00001752
Iteration 106/1000 | Loss: 0.00001751
Iteration 107/1000 | Loss: 0.00001751
Iteration 108/1000 | Loss: 0.00001750
Iteration 109/1000 | Loss: 0.00001750
Iteration 110/1000 | Loss: 0.00001750
Iteration 111/1000 | Loss: 0.00001750
Iteration 112/1000 | Loss: 0.00001749
Iteration 113/1000 | Loss: 0.00001749
Iteration 114/1000 | Loss: 0.00001748
Iteration 115/1000 | Loss: 0.00001748
Iteration 116/1000 | Loss: 0.00001748
Iteration 117/1000 | Loss: 0.00001748
Iteration 118/1000 | Loss: 0.00001748
Iteration 119/1000 | Loss: 0.00001748
Iteration 120/1000 | Loss: 0.00001748
Iteration 121/1000 | Loss: 0.00001748
Iteration 122/1000 | Loss: 0.00001748
Iteration 123/1000 | Loss: 0.00001747
Iteration 124/1000 | Loss: 0.00001747
Iteration 125/1000 | Loss: 0.00001747
Iteration 126/1000 | Loss: 0.00001747
Iteration 127/1000 | Loss: 0.00001747
Iteration 128/1000 | Loss: 0.00001747
Iteration 129/1000 | Loss: 0.00001747
Iteration 130/1000 | Loss: 0.00001747
Iteration 131/1000 | Loss: 0.00001747
Iteration 132/1000 | Loss: 0.00001747
Iteration 133/1000 | Loss: 0.00001747
Iteration 134/1000 | Loss: 0.00001746
Iteration 135/1000 | Loss: 0.00001746
Iteration 136/1000 | Loss: 0.00001746
Iteration 137/1000 | Loss: 0.00001746
Iteration 138/1000 | Loss: 0.00001746
Iteration 139/1000 | Loss: 0.00001746
Iteration 140/1000 | Loss: 0.00001746
Iteration 141/1000 | Loss: 0.00001746
Iteration 142/1000 | Loss: 0.00001746
Iteration 143/1000 | Loss: 0.00001746
Iteration 144/1000 | Loss: 0.00001746
Iteration 145/1000 | Loss: 0.00001746
Iteration 146/1000 | Loss: 0.00001746
Iteration 147/1000 | Loss: 0.00001746
Iteration 148/1000 | Loss: 0.00001746
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.745882946124766e-05, 1.745882946124766e-05, 1.745882946124766e-05, 1.745882946124766e-05, 1.745882946124766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.745882946124766e-05

Optimization complete. Final v2v error: 3.527576446533203 mm

Highest mean error: 3.7492313385009766 mm for frame 45

Lowest mean error: 3.3962769508361816 mm for frame 98

Saving results

Total time: 41.26167130470276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977969
Iteration 2/25 | Loss: 0.00977969
Iteration 3/25 | Loss: 0.00275211
Iteration 4/25 | Loss: 0.00182685
Iteration 5/25 | Loss: 0.00165930
Iteration 6/25 | Loss: 0.00147575
Iteration 7/25 | Loss: 0.00135385
Iteration 8/25 | Loss: 0.00133344
Iteration 9/25 | Loss: 0.00132843
Iteration 10/25 | Loss: 0.00132711
Iteration 11/25 | Loss: 0.00132388
Iteration 12/25 | Loss: 0.00132045
Iteration 13/25 | Loss: 0.00131992
Iteration 14/25 | Loss: 0.00131980
Iteration 15/25 | Loss: 0.00131965
Iteration 16/25 | Loss: 0.00131960
Iteration 17/25 | Loss: 0.00131960
Iteration 18/25 | Loss: 0.00131960
Iteration 19/25 | Loss: 0.00131959
Iteration 20/25 | Loss: 0.00131959
Iteration 21/25 | Loss: 0.00131958
Iteration 22/25 | Loss: 0.00131958
Iteration 23/25 | Loss: 0.00131958
Iteration 24/25 | Loss: 0.00131958
Iteration 25/25 | Loss: 0.00131958

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26633668
Iteration 2/25 | Loss: 0.00143637
Iteration 3/25 | Loss: 0.00143636
Iteration 4/25 | Loss: 0.00143636
Iteration 5/25 | Loss: 0.00143636
Iteration 6/25 | Loss: 0.00143636
Iteration 7/25 | Loss: 0.00143636
Iteration 8/25 | Loss: 0.00143636
Iteration 9/25 | Loss: 0.00143636
Iteration 10/25 | Loss: 0.00143636
Iteration 11/25 | Loss: 0.00143636
Iteration 12/25 | Loss: 0.00143636
Iteration 13/25 | Loss: 0.00143636
Iteration 14/25 | Loss: 0.00143636
Iteration 15/25 | Loss: 0.00143636
Iteration 16/25 | Loss: 0.00143636
Iteration 17/25 | Loss: 0.00143636
Iteration 18/25 | Loss: 0.00143636
Iteration 19/25 | Loss: 0.00143636
Iteration 20/25 | Loss: 0.00143636
Iteration 21/25 | Loss: 0.00143636
Iteration 22/25 | Loss: 0.00143636
Iteration 23/25 | Loss: 0.00143636
Iteration 24/25 | Loss: 0.00143636
Iteration 25/25 | Loss: 0.00143636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143636
Iteration 2/1000 | Loss: 0.00003751
Iteration 3/1000 | Loss: 0.00002631
Iteration 4/1000 | Loss: 0.00002475
Iteration 5/1000 | Loss: 0.00002369
Iteration 6/1000 | Loss: 0.00002311
Iteration 7/1000 | Loss: 0.00002267
Iteration 8/1000 | Loss: 0.00002235
Iteration 9/1000 | Loss: 0.00002210
Iteration 10/1000 | Loss: 0.00002182
Iteration 11/1000 | Loss: 0.00002160
Iteration 12/1000 | Loss: 0.00002159
Iteration 13/1000 | Loss: 0.00002150
Iteration 14/1000 | Loss: 0.00002140
Iteration 15/1000 | Loss: 0.00002139
Iteration 16/1000 | Loss: 0.00002133
Iteration 17/1000 | Loss: 0.00002133
Iteration 18/1000 | Loss: 0.00002133
Iteration 19/1000 | Loss: 0.00002133
Iteration 20/1000 | Loss: 0.00002132
Iteration 21/1000 | Loss: 0.00002132
Iteration 22/1000 | Loss: 0.00002132
Iteration 23/1000 | Loss: 0.00002132
Iteration 24/1000 | Loss: 0.00002131
Iteration 25/1000 | Loss: 0.00002114
Iteration 26/1000 | Loss: 0.00002109
Iteration 27/1000 | Loss: 0.00002102
Iteration 28/1000 | Loss: 0.00002101
Iteration 29/1000 | Loss: 0.00002101
Iteration 30/1000 | Loss: 0.00002101
Iteration 31/1000 | Loss: 0.00002101
Iteration 32/1000 | Loss: 0.00002101
Iteration 33/1000 | Loss: 0.00002101
Iteration 34/1000 | Loss: 0.00002101
Iteration 35/1000 | Loss: 0.00002101
Iteration 36/1000 | Loss: 0.00002101
Iteration 37/1000 | Loss: 0.00002101
Iteration 38/1000 | Loss: 0.00002100
Iteration 39/1000 | Loss: 0.00002100
Iteration 40/1000 | Loss: 0.00002100
Iteration 41/1000 | Loss: 0.00002099
Iteration 42/1000 | Loss: 0.00002099
Iteration 43/1000 | Loss: 0.00002098
Iteration 44/1000 | Loss: 0.00002097
Iteration 45/1000 | Loss: 0.00002095
Iteration 46/1000 | Loss: 0.00002095
Iteration 47/1000 | Loss: 0.00002095
Iteration 48/1000 | Loss: 0.00002095
Iteration 49/1000 | Loss: 0.00002094
Iteration 50/1000 | Loss: 0.00002094
Iteration 51/1000 | Loss: 0.00002094
Iteration 52/1000 | Loss: 0.00002093
Iteration 53/1000 | Loss: 0.00002092
Iteration 54/1000 | Loss: 0.00002092
Iteration 55/1000 | Loss: 0.00002092
Iteration 56/1000 | Loss: 0.00002092
Iteration 57/1000 | Loss: 0.00002091
Iteration 58/1000 | Loss: 0.00002091
Iteration 59/1000 | Loss: 0.00002088
Iteration 60/1000 | Loss: 0.00002088
Iteration 61/1000 | Loss: 0.00002088
Iteration 62/1000 | Loss: 0.00002088
Iteration 63/1000 | Loss: 0.00002088
Iteration 64/1000 | Loss: 0.00002088
Iteration 65/1000 | Loss: 0.00002088
Iteration 66/1000 | Loss: 0.00002088
Iteration 67/1000 | Loss: 0.00002088
Iteration 68/1000 | Loss: 0.00002088
Iteration 69/1000 | Loss: 0.00002088
Iteration 70/1000 | Loss: 0.00002087
Iteration 71/1000 | Loss: 0.00002087
Iteration 72/1000 | Loss: 0.00002087
Iteration 73/1000 | Loss: 0.00002087
Iteration 74/1000 | Loss: 0.00002086
Iteration 75/1000 | Loss: 0.00002086
Iteration 76/1000 | Loss: 0.00002085
Iteration 77/1000 | Loss: 0.00002085
Iteration 78/1000 | Loss: 0.00002085
Iteration 79/1000 | Loss: 0.00002085
Iteration 80/1000 | Loss: 0.00002085
Iteration 81/1000 | Loss: 0.00002085
Iteration 82/1000 | Loss: 0.00002085
Iteration 83/1000 | Loss: 0.00002085
Iteration 84/1000 | Loss: 0.00002084
Iteration 85/1000 | Loss: 0.00002084
Iteration 86/1000 | Loss: 0.00002083
Iteration 87/1000 | Loss: 0.00002083
Iteration 88/1000 | Loss: 0.00002082
Iteration 89/1000 | Loss: 0.00002082
Iteration 90/1000 | Loss: 0.00002081
Iteration 91/1000 | Loss: 0.00002081
Iteration 92/1000 | Loss: 0.00002081
Iteration 93/1000 | Loss: 0.00002081
Iteration 94/1000 | Loss: 0.00002081
Iteration 95/1000 | Loss: 0.00002080
Iteration 96/1000 | Loss: 0.00002080
Iteration 97/1000 | Loss: 0.00002079
Iteration 98/1000 | Loss: 0.00002079
Iteration 99/1000 | Loss: 0.00002079
Iteration 100/1000 | Loss: 0.00002078
Iteration 101/1000 | Loss: 0.00002078
Iteration 102/1000 | Loss: 0.00002076
Iteration 103/1000 | Loss: 0.00002076
Iteration 104/1000 | Loss: 0.00002075
Iteration 105/1000 | Loss: 0.00002075
Iteration 106/1000 | Loss: 0.00002074
Iteration 107/1000 | Loss: 0.00002074
Iteration 108/1000 | Loss: 0.00002074
Iteration 109/1000 | Loss: 0.00002074
Iteration 110/1000 | Loss: 0.00002074
Iteration 111/1000 | Loss: 0.00002074
Iteration 112/1000 | Loss: 0.00002074
Iteration 113/1000 | Loss: 0.00002074
Iteration 114/1000 | Loss: 0.00002074
Iteration 115/1000 | Loss: 0.00002074
Iteration 116/1000 | Loss: 0.00002074
Iteration 117/1000 | Loss: 0.00002073
Iteration 118/1000 | Loss: 0.00002073
Iteration 119/1000 | Loss: 0.00002073
Iteration 120/1000 | Loss: 0.00002073
Iteration 121/1000 | Loss: 0.00002073
Iteration 122/1000 | Loss: 0.00002073
Iteration 123/1000 | Loss: 0.00002073
Iteration 124/1000 | Loss: 0.00002073
Iteration 125/1000 | Loss: 0.00002073
Iteration 126/1000 | Loss: 0.00002073
Iteration 127/1000 | Loss: 0.00002073
Iteration 128/1000 | Loss: 0.00002072
Iteration 129/1000 | Loss: 0.00002072
Iteration 130/1000 | Loss: 0.00002072
Iteration 131/1000 | Loss: 0.00002072
Iteration 132/1000 | Loss: 0.00002072
Iteration 133/1000 | Loss: 0.00002072
Iteration 134/1000 | Loss: 0.00002072
Iteration 135/1000 | Loss: 0.00002072
Iteration 136/1000 | Loss: 0.00002072
Iteration 137/1000 | Loss: 0.00002072
Iteration 138/1000 | Loss: 0.00002072
Iteration 139/1000 | Loss: 0.00002072
Iteration 140/1000 | Loss: 0.00002072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.0720513930427842e-05, 2.0720513930427842e-05, 2.0720513930427842e-05, 2.0720513930427842e-05, 2.0720513930427842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0720513930427842e-05

Optimization complete. Final v2v error: 3.927730083465576 mm

Highest mean error: 3.966827869415283 mm for frame 169

Lowest mean error: 3.874295949935913 mm for frame 94

Saving results

Total time: 63.3747456073761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739990
Iteration 2/25 | Loss: 0.00156756
Iteration 3/25 | Loss: 0.00133305
Iteration 4/25 | Loss: 0.00129330
Iteration 5/25 | Loss: 0.00128399
Iteration 6/25 | Loss: 0.00128124
Iteration 7/25 | Loss: 0.00127890
Iteration 8/25 | Loss: 0.00127736
Iteration 9/25 | Loss: 0.00127578
Iteration 10/25 | Loss: 0.00127888
Iteration 11/25 | Loss: 0.00127837
Iteration 12/25 | Loss: 0.00127552
Iteration 13/25 | Loss: 0.00127403
Iteration 14/25 | Loss: 0.00127375
Iteration 15/25 | Loss: 0.00127372
Iteration 16/25 | Loss: 0.00127372
Iteration 17/25 | Loss: 0.00127372
Iteration 18/25 | Loss: 0.00127372
Iteration 19/25 | Loss: 0.00127371
Iteration 20/25 | Loss: 0.00127371
Iteration 21/25 | Loss: 0.00127371
Iteration 22/25 | Loss: 0.00127371
Iteration 23/25 | Loss: 0.00127371
Iteration 24/25 | Loss: 0.00127371
Iteration 25/25 | Loss: 0.00127370

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.58717251
Iteration 2/25 | Loss: 0.00167145
Iteration 3/25 | Loss: 0.00166358
Iteration 4/25 | Loss: 0.00166358
Iteration 5/25 | Loss: 0.00166358
Iteration 6/25 | Loss: 0.00166358
Iteration 7/25 | Loss: 0.00166357
Iteration 8/25 | Loss: 0.00166357
Iteration 9/25 | Loss: 0.00166357
Iteration 10/25 | Loss: 0.00166357
Iteration 11/25 | Loss: 0.00166357
Iteration 12/25 | Loss: 0.00166357
Iteration 13/25 | Loss: 0.00166357
Iteration 14/25 | Loss: 0.00166357
Iteration 15/25 | Loss: 0.00166357
Iteration 16/25 | Loss: 0.00166357
Iteration 17/25 | Loss: 0.00166357
Iteration 18/25 | Loss: 0.00166357
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016635737847536802, 0.0016635737847536802, 0.0016635737847536802, 0.0016635737847536802, 0.0016635737847536802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016635737847536802

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166357
Iteration 2/1000 | Loss: 0.00012486
Iteration 3/1000 | Loss: 0.00004157
Iteration 4/1000 | Loss: 0.00003454
Iteration 5/1000 | Loss: 0.00002730
Iteration 6/1000 | Loss: 0.00011692
Iteration 7/1000 | Loss: 0.00003164
Iteration 8/1000 | Loss: 0.00002865
Iteration 9/1000 | Loss: 0.00002722
Iteration 10/1000 | Loss: 0.00002646
Iteration 11/1000 | Loss: 0.00002571
Iteration 12/1000 | Loss: 0.00015662
Iteration 13/1000 | Loss: 0.00003534
Iteration 14/1000 | Loss: 0.00002790
Iteration 15/1000 | Loss: 0.00003897
Iteration 16/1000 | Loss: 0.00002601
Iteration 17/1000 | Loss: 0.00002550
Iteration 18/1000 | Loss: 0.00002504
Iteration 19/1000 | Loss: 0.00014367
Iteration 20/1000 | Loss: 0.00016040
Iteration 21/1000 | Loss: 0.00012779
Iteration 22/1000 | Loss: 0.00003291
Iteration 23/1000 | Loss: 0.00009444
Iteration 24/1000 | Loss: 0.00002826
Iteration 25/1000 | Loss: 0.00002694
Iteration 26/1000 | Loss: 0.00007324
Iteration 27/1000 | Loss: 0.00002395
Iteration 28/1000 | Loss: 0.00002351
Iteration 29/1000 | Loss: 0.00002281
Iteration 30/1000 | Loss: 0.00002989
Iteration 31/1000 | Loss: 0.00002316
Iteration 32/1000 | Loss: 0.00002255
Iteration 33/1000 | Loss: 0.00002201
Iteration 34/1000 | Loss: 0.00002175
Iteration 35/1000 | Loss: 0.00002165
Iteration 36/1000 | Loss: 0.00002134
Iteration 37/1000 | Loss: 0.00002088
Iteration 38/1000 | Loss: 0.00007411
Iteration 39/1000 | Loss: 0.00007930
Iteration 40/1000 | Loss: 0.00002286
Iteration 41/1000 | Loss: 0.00006959
Iteration 42/1000 | Loss: 0.00006047
Iteration 43/1000 | Loss: 0.00003573
Iteration 44/1000 | Loss: 0.00002128
Iteration 45/1000 | Loss: 0.00002008
Iteration 46/1000 | Loss: 0.00001967
Iteration 47/1000 | Loss: 0.00001944
Iteration 48/1000 | Loss: 0.00001915
Iteration 49/1000 | Loss: 0.00001882
Iteration 50/1000 | Loss: 0.00001849
Iteration 51/1000 | Loss: 0.00011675
Iteration 52/1000 | Loss: 0.00001825
Iteration 53/1000 | Loss: 0.00001774
Iteration 54/1000 | Loss: 0.00001755
Iteration 55/1000 | Loss: 0.00001749
Iteration 56/1000 | Loss: 0.00001748
Iteration 57/1000 | Loss: 0.00006153
Iteration 58/1000 | Loss: 0.00030757
Iteration 59/1000 | Loss: 0.00001789
Iteration 60/1000 | Loss: 0.00001737
Iteration 61/1000 | Loss: 0.00001737
Iteration 62/1000 | Loss: 0.00004974
Iteration 63/1000 | Loss: 0.00001750
Iteration 64/1000 | Loss: 0.00002106
Iteration 65/1000 | Loss: 0.00001744
Iteration 66/1000 | Loss: 0.00001736
Iteration 67/1000 | Loss: 0.00001736
Iteration 68/1000 | Loss: 0.00001735
Iteration 69/1000 | Loss: 0.00001735
Iteration 70/1000 | Loss: 0.00001735
Iteration 71/1000 | Loss: 0.00001735
Iteration 72/1000 | Loss: 0.00001735
Iteration 73/1000 | Loss: 0.00001735
Iteration 74/1000 | Loss: 0.00001735
Iteration 75/1000 | Loss: 0.00001734
Iteration 76/1000 | Loss: 0.00001734
Iteration 77/1000 | Loss: 0.00001734
Iteration 78/1000 | Loss: 0.00001733
Iteration 79/1000 | Loss: 0.00001732
Iteration 80/1000 | Loss: 0.00001732
Iteration 81/1000 | Loss: 0.00001732
Iteration 82/1000 | Loss: 0.00001732
Iteration 83/1000 | Loss: 0.00001732
Iteration 84/1000 | Loss: 0.00001731
Iteration 85/1000 | Loss: 0.00001731
Iteration 86/1000 | Loss: 0.00001731
Iteration 87/1000 | Loss: 0.00001731
Iteration 88/1000 | Loss: 0.00001731
Iteration 89/1000 | Loss: 0.00001730
Iteration 90/1000 | Loss: 0.00002546
Iteration 91/1000 | Loss: 0.00001773
Iteration 92/1000 | Loss: 0.00001736
Iteration 93/1000 | Loss: 0.00001736
Iteration 94/1000 | Loss: 0.00001735
Iteration 95/1000 | Loss: 0.00001734
Iteration 96/1000 | Loss: 0.00001733
Iteration 97/1000 | Loss: 0.00001807
Iteration 98/1000 | Loss: 0.00001730
Iteration 99/1000 | Loss: 0.00001730
Iteration 100/1000 | Loss: 0.00001730
Iteration 101/1000 | Loss: 0.00001730
Iteration 102/1000 | Loss: 0.00001730
Iteration 103/1000 | Loss: 0.00001730
Iteration 104/1000 | Loss: 0.00001730
Iteration 105/1000 | Loss: 0.00001729
Iteration 106/1000 | Loss: 0.00001729
Iteration 107/1000 | Loss: 0.00001729
Iteration 108/1000 | Loss: 0.00001729
Iteration 109/1000 | Loss: 0.00001728
Iteration 110/1000 | Loss: 0.00001728
Iteration 111/1000 | Loss: 0.00001728
Iteration 112/1000 | Loss: 0.00001728
Iteration 113/1000 | Loss: 0.00001728
Iteration 114/1000 | Loss: 0.00001728
Iteration 115/1000 | Loss: 0.00001728
Iteration 116/1000 | Loss: 0.00001728
Iteration 117/1000 | Loss: 0.00001728
Iteration 118/1000 | Loss: 0.00001728
Iteration 119/1000 | Loss: 0.00001727
Iteration 120/1000 | Loss: 0.00001727
Iteration 121/1000 | Loss: 0.00001727
Iteration 122/1000 | Loss: 0.00001727
Iteration 123/1000 | Loss: 0.00001727
Iteration 124/1000 | Loss: 0.00001727
Iteration 125/1000 | Loss: 0.00001727
Iteration 126/1000 | Loss: 0.00001727
Iteration 127/1000 | Loss: 0.00001727
Iteration 128/1000 | Loss: 0.00001727
Iteration 129/1000 | Loss: 0.00001727
Iteration 130/1000 | Loss: 0.00001727
Iteration 131/1000 | Loss: 0.00001727
Iteration 132/1000 | Loss: 0.00001727
Iteration 133/1000 | Loss: 0.00001726
Iteration 134/1000 | Loss: 0.00001726
Iteration 135/1000 | Loss: 0.00001726
Iteration 136/1000 | Loss: 0.00001726
Iteration 137/1000 | Loss: 0.00001726
Iteration 138/1000 | Loss: 0.00001726
Iteration 139/1000 | Loss: 0.00001726
Iteration 140/1000 | Loss: 0.00001726
Iteration 141/1000 | Loss: 0.00001726
Iteration 142/1000 | Loss: 0.00001726
Iteration 143/1000 | Loss: 0.00001726
Iteration 144/1000 | Loss: 0.00001726
Iteration 145/1000 | Loss: 0.00001726
Iteration 146/1000 | Loss: 0.00001726
Iteration 147/1000 | Loss: 0.00001726
Iteration 148/1000 | Loss: 0.00001726
Iteration 149/1000 | Loss: 0.00001726
Iteration 150/1000 | Loss: 0.00001726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.7260303138755262e-05, 1.7260303138755262e-05, 1.7260303138755262e-05, 1.7260303138755262e-05, 1.7260303138755262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7260303138755262e-05

Optimization complete. Final v2v error: 3.4632699489593506 mm

Highest mean error: 5.394380569458008 mm for frame 149

Lowest mean error: 2.6447017192840576 mm for frame 94

Saving results

Total time: 134.6074149608612
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855882
Iteration 2/25 | Loss: 0.00129659
Iteration 3/25 | Loss: 0.00122179
Iteration 4/25 | Loss: 0.00120279
Iteration 5/25 | Loss: 0.00119569
Iteration 6/25 | Loss: 0.00119341
Iteration 7/25 | Loss: 0.00119277
Iteration 8/25 | Loss: 0.00119277
Iteration 9/25 | Loss: 0.00119276
Iteration 10/25 | Loss: 0.00119276
Iteration 11/25 | Loss: 0.00119276
Iteration 12/25 | Loss: 0.00119276
Iteration 13/25 | Loss: 0.00119276
Iteration 14/25 | Loss: 0.00119276
Iteration 15/25 | Loss: 0.00119276
Iteration 16/25 | Loss: 0.00119276
Iteration 17/25 | Loss: 0.00119276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011927555315196514, 0.0011927555315196514, 0.0011927555315196514, 0.0011927555315196514, 0.0011927555315196514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011927555315196514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23952508
Iteration 2/25 | Loss: 0.00219787
Iteration 3/25 | Loss: 0.00219787
Iteration 4/25 | Loss: 0.00219787
Iteration 5/25 | Loss: 0.00219786
Iteration 6/25 | Loss: 0.00219786
Iteration 7/25 | Loss: 0.00219786
Iteration 8/25 | Loss: 0.00219786
Iteration 9/25 | Loss: 0.00219786
Iteration 10/25 | Loss: 0.00219786
Iteration 11/25 | Loss: 0.00219786
Iteration 12/25 | Loss: 0.00219786
Iteration 13/25 | Loss: 0.00219786
Iteration 14/25 | Loss: 0.00219786
Iteration 15/25 | Loss: 0.00219786
Iteration 16/25 | Loss: 0.00219786
Iteration 17/25 | Loss: 0.00219786
Iteration 18/25 | Loss: 0.00219786
Iteration 19/25 | Loss: 0.00219786
Iteration 20/25 | Loss: 0.00219786
Iteration 21/25 | Loss: 0.00219786
Iteration 22/25 | Loss: 0.00219786
Iteration 23/25 | Loss: 0.00219786
Iteration 24/25 | Loss: 0.00219786
Iteration 25/25 | Loss: 0.00219786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219786
Iteration 2/1000 | Loss: 0.00003993
Iteration 3/1000 | Loss: 0.00002893
Iteration 4/1000 | Loss: 0.00002319
Iteration 5/1000 | Loss: 0.00002145
Iteration 6/1000 | Loss: 0.00002016
Iteration 7/1000 | Loss: 0.00001928
Iteration 8/1000 | Loss: 0.00001856
Iteration 9/1000 | Loss: 0.00001802
Iteration 10/1000 | Loss: 0.00001764
Iteration 11/1000 | Loss: 0.00001736
Iteration 12/1000 | Loss: 0.00001715
Iteration 13/1000 | Loss: 0.00001706
Iteration 14/1000 | Loss: 0.00001691
Iteration 15/1000 | Loss: 0.00001681
Iteration 16/1000 | Loss: 0.00001679
Iteration 17/1000 | Loss: 0.00001665
Iteration 18/1000 | Loss: 0.00001663
Iteration 19/1000 | Loss: 0.00001659
Iteration 20/1000 | Loss: 0.00001658
Iteration 21/1000 | Loss: 0.00001657
Iteration 22/1000 | Loss: 0.00001657
Iteration 23/1000 | Loss: 0.00001657
Iteration 24/1000 | Loss: 0.00001657
Iteration 25/1000 | Loss: 0.00001656
Iteration 26/1000 | Loss: 0.00001656
Iteration 27/1000 | Loss: 0.00001648
Iteration 28/1000 | Loss: 0.00001647
Iteration 29/1000 | Loss: 0.00001646
Iteration 30/1000 | Loss: 0.00001645
Iteration 31/1000 | Loss: 0.00001644
Iteration 32/1000 | Loss: 0.00001644
Iteration 33/1000 | Loss: 0.00001644
Iteration 34/1000 | Loss: 0.00001643
Iteration 35/1000 | Loss: 0.00001643
Iteration 36/1000 | Loss: 0.00001643
Iteration 37/1000 | Loss: 0.00001642
Iteration 38/1000 | Loss: 0.00001641
Iteration 39/1000 | Loss: 0.00001641
Iteration 40/1000 | Loss: 0.00001640
Iteration 41/1000 | Loss: 0.00001637
Iteration 42/1000 | Loss: 0.00001637
Iteration 43/1000 | Loss: 0.00001636
Iteration 44/1000 | Loss: 0.00001636
Iteration 45/1000 | Loss: 0.00001633
Iteration 46/1000 | Loss: 0.00001633
Iteration 47/1000 | Loss: 0.00001632
Iteration 48/1000 | Loss: 0.00001631
Iteration 49/1000 | Loss: 0.00001631
Iteration 50/1000 | Loss: 0.00001631
Iteration 51/1000 | Loss: 0.00001631
Iteration 52/1000 | Loss: 0.00001630
Iteration 53/1000 | Loss: 0.00001630
Iteration 54/1000 | Loss: 0.00001630
Iteration 55/1000 | Loss: 0.00001630
Iteration 56/1000 | Loss: 0.00001630
Iteration 57/1000 | Loss: 0.00001630
Iteration 58/1000 | Loss: 0.00001630
Iteration 59/1000 | Loss: 0.00001630
Iteration 60/1000 | Loss: 0.00001630
Iteration 61/1000 | Loss: 0.00001629
Iteration 62/1000 | Loss: 0.00001628
Iteration 63/1000 | Loss: 0.00001627
Iteration 64/1000 | Loss: 0.00001627
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00001627
Iteration 67/1000 | Loss: 0.00001626
Iteration 68/1000 | Loss: 0.00001626
Iteration 69/1000 | Loss: 0.00001626
Iteration 70/1000 | Loss: 0.00001626
Iteration 71/1000 | Loss: 0.00001626
Iteration 72/1000 | Loss: 0.00001625
Iteration 73/1000 | Loss: 0.00001625
Iteration 74/1000 | Loss: 0.00001625
Iteration 75/1000 | Loss: 0.00001625
Iteration 76/1000 | Loss: 0.00001625
Iteration 77/1000 | Loss: 0.00001625
Iteration 78/1000 | Loss: 0.00001624
Iteration 79/1000 | Loss: 0.00001624
Iteration 80/1000 | Loss: 0.00001624
Iteration 81/1000 | Loss: 0.00001623
Iteration 82/1000 | Loss: 0.00001623
Iteration 83/1000 | Loss: 0.00001623
Iteration 84/1000 | Loss: 0.00001623
Iteration 85/1000 | Loss: 0.00001623
Iteration 86/1000 | Loss: 0.00001623
Iteration 87/1000 | Loss: 0.00001622
Iteration 88/1000 | Loss: 0.00001622
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001622
Iteration 91/1000 | Loss: 0.00001622
Iteration 92/1000 | Loss: 0.00001622
Iteration 93/1000 | Loss: 0.00001622
Iteration 94/1000 | Loss: 0.00001622
Iteration 95/1000 | Loss: 0.00001622
Iteration 96/1000 | Loss: 0.00001621
Iteration 97/1000 | Loss: 0.00001621
Iteration 98/1000 | Loss: 0.00001621
Iteration 99/1000 | Loss: 0.00001621
Iteration 100/1000 | Loss: 0.00001620
Iteration 101/1000 | Loss: 0.00001620
Iteration 102/1000 | Loss: 0.00001620
Iteration 103/1000 | Loss: 0.00001620
Iteration 104/1000 | Loss: 0.00001620
Iteration 105/1000 | Loss: 0.00001620
Iteration 106/1000 | Loss: 0.00001620
Iteration 107/1000 | Loss: 0.00001620
Iteration 108/1000 | Loss: 0.00001619
Iteration 109/1000 | Loss: 0.00001619
Iteration 110/1000 | Loss: 0.00001619
Iteration 111/1000 | Loss: 0.00001619
Iteration 112/1000 | Loss: 0.00001619
Iteration 113/1000 | Loss: 0.00001619
Iteration 114/1000 | Loss: 0.00001619
Iteration 115/1000 | Loss: 0.00001619
Iteration 116/1000 | Loss: 0.00001619
Iteration 117/1000 | Loss: 0.00001619
Iteration 118/1000 | Loss: 0.00001619
Iteration 119/1000 | Loss: 0.00001618
Iteration 120/1000 | Loss: 0.00001618
Iteration 121/1000 | Loss: 0.00001618
Iteration 122/1000 | Loss: 0.00001618
Iteration 123/1000 | Loss: 0.00001618
Iteration 124/1000 | Loss: 0.00001617
Iteration 125/1000 | Loss: 0.00001617
Iteration 126/1000 | Loss: 0.00001617
Iteration 127/1000 | Loss: 0.00001617
Iteration 128/1000 | Loss: 0.00001617
Iteration 129/1000 | Loss: 0.00001617
Iteration 130/1000 | Loss: 0.00001617
Iteration 131/1000 | Loss: 0.00001617
Iteration 132/1000 | Loss: 0.00001617
Iteration 133/1000 | Loss: 0.00001616
Iteration 134/1000 | Loss: 0.00001616
Iteration 135/1000 | Loss: 0.00001616
Iteration 136/1000 | Loss: 0.00001616
Iteration 137/1000 | Loss: 0.00001616
Iteration 138/1000 | Loss: 0.00001615
Iteration 139/1000 | Loss: 0.00001615
Iteration 140/1000 | Loss: 0.00001615
Iteration 141/1000 | Loss: 0.00001615
Iteration 142/1000 | Loss: 0.00001615
Iteration 143/1000 | Loss: 0.00001615
Iteration 144/1000 | Loss: 0.00001615
Iteration 145/1000 | Loss: 0.00001615
Iteration 146/1000 | Loss: 0.00001615
Iteration 147/1000 | Loss: 0.00001615
Iteration 148/1000 | Loss: 0.00001614
Iteration 149/1000 | Loss: 0.00001614
Iteration 150/1000 | Loss: 0.00001614
Iteration 151/1000 | Loss: 0.00001614
Iteration 152/1000 | Loss: 0.00001614
Iteration 153/1000 | Loss: 0.00001614
Iteration 154/1000 | Loss: 0.00001614
Iteration 155/1000 | Loss: 0.00001614
Iteration 156/1000 | Loss: 0.00001614
Iteration 157/1000 | Loss: 0.00001614
Iteration 158/1000 | Loss: 0.00001614
Iteration 159/1000 | Loss: 0.00001614
Iteration 160/1000 | Loss: 0.00001614
Iteration 161/1000 | Loss: 0.00001614
Iteration 162/1000 | Loss: 0.00001614
Iteration 163/1000 | Loss: 0.00001614
Iteration 164/1000 | Loss: 0.00001614
Iteration 165/1000 | Loss: 0.00001613
Iteration 166/1000 | Loss: 0.00001613
Iteration 167/1000 | Loss: 0.00001613
Iteration 168/1000 | Loss: 0.00001613
Iteration 169/1000 | Loss: 0.00001613
Iteration 170/1000 | Loss: 0.00001613
Iteration 171/1000 | Loss: 0.00001613
Iteration 172/1000 | Loss: 0.00001613
Iteration 173/1000 | Loss: 0.00001612
Iteration 174/1000 | Loss: 0.00001612
Iteration 175/1000 | Loss: 0.00001612
Iteration 176/1000 | Loss: 0.00001612
Iteration 177/1000 | Loss: 0.00001612
Iteration 178/1000 | Loss: 0.00001612
Iteration 179/1000 | Loss: 0.00001612
Iteration 180/1000 | Loss: 0.00001612
Iteration 181/1000 | Loss: 0.00001611
Iteration 182/1000 | Loss: 0.00001611
Iteration 183/1000 | Loss: 0.00001611
Iteration 184/1000 | Loss: 0.00001611
Iteration 185/1000 | Loss: 0.00001611
Iteration 186/1000 | Loss: 0.00001611
Iteration 187/1000 | Loss: 0.00001611
Iteration 188/1000 | Loss: 0.00001611
Iteration 189/1000 | Loss: 0.00001610
Iteration 190/1000 | Loss: 0.00001610
Iteration 191/1000 | Loss: 0.00001610
Iteration 192/1000 | Loss: 0.00001610
Iteration 193/1000 | Loss: 0.00001610
Iteration 194/1000 | Loss: 0.00001610
Iteration 195/1000 | Loss: 0.00001610
Iteration 196/1000 | Loss: 0.00001610
Iteration 197/1000 | Loss: 0.00001610
Iteration 198/1000 | Loss: 0.00001610
Iteration 199/1000 | Loss: 0.00001609
Iteration 200/1000 | Loss: 0.00001609
Iteration 201/1000 | Loss: 0.00001609
Iteration 202/1000 | Loss: 0.00001609
Iteration 203/1000 | Loss: 0.00001609
Iteration 204/1000 | Loss: 0.00001608
Iteration 205/1000 | Loss: 0.00001608
Iteration 206/1000 | Loss: 0.00001608
Iteration 207/1000 | Loss: 0.00001608
Iteration 208/1000 | Loss: 0.00001608
Iteration 209/1000 | Loss: 0.00001608
Iteration 210/1000 | Loss: 0.00001608
Iteration 211/1000 | Loss: 0.00001608
Iteration 212/1000 | Loss: 0.00001608
Iteration 213/1000 | Loss: 0.00001608
Iteration 214/1000 | Loss: 0.00001608
Iteration 215/1000 | Loss: 0.00001607
Iteration 216/1000 | Loss: 0.00001607
Iteration 217/1000 | Loss: 0.00001607
Iteration 218/1000 | Loss: 0.00001607
Iteration 219/1000 | Loss: 0.00001607
Iteration 220/1000 | Loss: 0.00001607
Iteration 221/1000 | Loss: 0.00001607
Iteration 222/1000 | Loss: 0.00001607
Iteration 223/1000 | Loss: 0.00001607
Iteration 224/1000 | Loss: 0.00001607
Iteration 225/1000 | Loss: 0.00001606
Iteration 226/1000 | Loss: 0.00001606
Iteration 227/1000 | Loss: 0.00001606
Iteration 228/1000 | Loss: 0.00001606
Iteration 229/1000 | Loss: 0.00001606
Iteration 230/1000 | Loss: 0.00001606
Iteration 231/1000 | Loss: 0.00001606
Iteration 232/1000 | Loss: 0.00001606
Iteration 233/1000 | Loss: 0.00001606
Iteration 234/1000 | Loss: 0.00001606
Iteration 235/1000 | Loss: 0.00001606
Iteration 236/1000 | Loss: 0.00001606
Iteration 237/1000 | Loss: 0.00001606
Iteration 238/1000 | Loss: 0.00001606
Iteration 239/1000 | Loss: 0.00001606
Iteration 240/1000 | Loss: 0.00001606
Iteration 241/1000 | Loss: 0.00001606
Iteration 242/1000 | Loss: 0.00001605
Iteration 243/1000 | Loss: 0.00001605
Iteration 244/1000 | Loss: 0.00001605
Iteration 245/1000 | Loss: 0.00001605
Iteration 246/1000 | Loss: 0.00001605
Iteration 247/1000 | Loss: 0.00001605
Iteration 248/1000 | Loss: 0.00001605
Iteration 249/1000 | Loss: 0.00001605
Iteration 250/1000 | Loss: 0.00001605
Iteration 251/1000 | Loss: 0.00001605
Iteration 252/1000 | Loss: 0.00001605
Iteration 253/1000 | Loss: 0.00001605
Iteration 254/1000 | Loss: 0.00001605
Iteration 255/1000 | Loss: 0.00001605
Iteration 256/1000 | Loss: 0.00001605
Iteration 257/1000 | Loss: 0.00001605
Iteration 258/1000 | Loss: 0.00001605
Iteration 259/1000 | Loss: 0.00001605
Iteration 260/1000 | Loss: 0.00001604
Iteration 261/1000 | Loss: 0.00001604
Iteration 262/1000 | Loss: 0.00001604
Iteration 263/1000 | Loss: 0.00001604
Iteration 264/1000 | Loss: 0.00001604
Iteration 265/1000 | Loss: 0.00001604
Iteration 266/1000 | Loss: 0.00001604
Iteration 267/1000 | Loss: 0.00001604
Iteration 268/1000 | Loss: 0.00001604
Iteration 269/1000 | Loss: 0.00001604
Iteration 270/1000 | Loss: 0.00001604
Iteration 271/1000 | Loss: 0.00001604
Iteration 272/1000 | Loss: 0.00001604
Iteration 273/1000 | Loss: 0.00001604
Iteration 274/1000 | Loss: 0.00001604
Iteration 275/1000 | Loss: 0.00001604
Iteration 276/1000 | Loss: 0.00001604
Iteration 277/1000 | Loss: 0.00001604
Iteration 278/1000 | Loss: 0.00001604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 278. Stopping optimization.
Last 5 losses: [1.6044514268287458e-05, 1.6044514268287458e-05, 1.6044514268287458e-05, 1.6044514268287458e-05, 1.6044514268287458e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6044514268287458e-05

Optimization complete. Final v2v error: 3.384364604949951 mm

Highest mean error: 3.9671742916107178 mm for frame 121

Lowest mean error: 2.711569309234619 mm for frame 69

Saving results

Total time: 50.08828020095825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414407
Iteration 2/25 | Loss: 0.00153595
Iteration 3/25 | Loss: 0.00128792
Iteration 4/25 | Loss: 0.00125491
Iteration 5/25 | Loss: 0.00125010
Iteration 6/25 | Loss: 0.00124886
Iteration 7/25 | Loss: 0.00124872
Iteration 8/25 | Loss: 0.00124872
Iteration 9/25 | Loss: 0.00124872
Iteration 10/25 | Loss: 0.00124872
Iteration 11/25 | Loss: 0.00124872
Iteration 12/25 | Loss: 0.00124872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012487247586250305, 0.0012487247586250305, 0.0012487247586250305, 0.0012487247586250305, 0.0012487247586250305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012487247586250305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25004101
Iteration 2/25 | Loss: 0.00142614
Iteration 3/25 | Loss: 0.00142614
Iteration 4/25 | Loss: 0.00142614
Iteration 5/25 | Loss: 0.00142614
Iteration 6/25 | Loss: 0.00142614
Iteration 7/25 | Loss: 0.00142614
Iteration 8/25 | Loss: 0.00142614
Iteration 9/25 | Loss: 0.00142614
Iteration 10/25 | Loss: 0.00142614
Iteration 11/25 | Loss: 0.00142614
Iteration 12/25 | Loss: 0.00142614
Iteration 13/25 | Loss: 0.00142614
Iteration 14/25 | Loss: 0.00142614
Iteration 15/25 | Loss: 0.00142614
Iteration 16/25 | Loss: 0.00142614
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014261379837989807, 0.0014261379837989807, 0.0014261379837989807, 0.0014261379837989807, 0.0014261379837989807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014261379837989807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142614
Iteration 2/1000 | Loss: 0.00004862
Iteration 3/1000 | Loss: 0.00003043
Iteration 4/1000 | Loss: 0.00002060
Iteration 5/1000 | Loss: 0.00001851
Iteration 6/1000 | Loss: 0.00001736
Iteration 7/1000 | Loss: 0.00001658
Iteration 8/1000 | Loss: 0.00001592
Iteration 9/1000 | Loss: 0.00001550
Iteration 10/1000 | Loss: 0.00001512
Iteration 11/1000 | Loss: 0.00001489
Iteration 12/1000 | Loss: 0.00001463
Iteration 13/1000 | Loss: 0.00001449
Iteration 14/1000 | Loss: 0.00001446
Iteration 15/1000 | Loss: 0.00001438
Iteration 16/1000 | Loss: 0.00001437
Iteration 17/1000 | Loss: 0.00001437
Iteration 18/1000 | Loss: 0.00001436
Iteration 19/1000 | Loss: 0.00001432
Iteration 20/1000 | Loss: 0.00001432
Iteration 21/1000 | Loss: 0.00001429
Iteration 22/1000 | Loss: 0.00001429
Iteration 23/1000 | Loss: 0.00001428
Iteration 24/1000 | Loss: 0.00001428
Iteration 25/1000 | Loss: 0.00001418
Iteration 26/1000 | Loss: 0.00001414
Iteration 27/1000 | Loss: 0.00001413
Iteration 28/1000 | Loss: 0.00001412
Iteration 29/1000 | Loss: 0.00001412
Iteration 30/1000 | Loss: 0.00001409
Iteration 31/1000 | Loss: 0.00001408
Iteration 32/1000 | Loss: 0.00001408
Iteration 33/1000 | Loss: 0.00001407
Iteration 34/1000 | Loss: 0.00001407
Iteration 35/1000 | Loss: 0.00001407
Iteration 36/1000 | Loss: 0.00001406
Iteration 37/1000 | Loss: 0.00001406
Iteration 38/1000 | Loss: 0.00001406
Iteration 39/1000 | Loss: 0.00001405
Iteration 40/1000 | Loss: 0.00001405
Iteration 41/1000 | Loss: 0.00001404
Iteration 42/1000 | Loss: 0.00001403
Iteration 43/1000 | Loss: 0.00001403
Iteration 44/1000 | Loss: 0.00001403
Iteration 45/1000 | Loss: 0.00001403
Iteration 46/1000 | Loss: 0.00001403
Iteration 47/1000 | Loss: 0.00001403
Iteration 48/1000 | Loss: 0.00001403
Iteration 49/1000 | Loss: 0.00001403
Iteration 50/1000 | Loss: 0.00001403
Iteration 51/1000 | Loss: 0.00001403
Iteration 52/1000 | Loss: 0.00001403
Iteration 53/1000 | Loss: 0.00001402
Iteration 54/1000 | Loss: 0.00001402
Iteration 55/1000 | Loss: 0.00001400
Iteration 56/1000 | Loss: 0.00001400
Iteration 57/1000 | Loss: 0.00001399
Iteration 58/1000 | Loss: 0.00001399
Iteration 59/1000 | Loss: 0.00001399
Iteration 60/1000 | Loss: 0.00001398
Iteration 61/1000 | Loss: 0.00001398
Iteration 62/1000 | Loss: 0.00001397
Iteration 63/1000 | Loss: 0.00001396
Iteration 64/1000 | Loss: 0.00001396
Iteration 65/1000 | Loss: 0.00001395
Iteration 66/1000 | Loss: 0.00001395
Iteration 67/1000 | Loss: 0.00001394
Iteration 68/1000 | Loss: 0.00001394
Iteration 69/1000 | Loss: 0.00001393
Iteration 70/1000 | Loss: 0.00001392
Iteration 71/1000 | Loss: 0.00001389
Iteration 72/1000 | Loss: 0.00001389
Iteration 73/1000 | Loss: 0.00001389
Iteration 74/1000 | Loss: 0.00001388
Iteration 75/1000 | Loss: 0.00001388
Iteration 76/1000 | Loss: 0.00001387
Iteration 77/1000 | Loss: 0.00001385
Iteration 78/1000 | Loss: 0.00001384
Iteration 79/1000 | Loss: 0.00001384
Iteration 80/1000 | Loss: 0.00001383
Iteration 81/1000 | Loss: 0.00001383
Iteration 82/1000 | Loss: 0.00001382
Iteration 83/1000 | Loss: 0.00001378
Iteration 84/1000 | Loss: 0.00001377
Iteration 85/1000 | Loss: 0.00001377
Iteration 86/1000 | Loss: 0.00001377
Iteration 87/1000 | Loss: 0.00001376
Iteration 88/1000 | Loss: 0.00001376
Iteration 89/1000 | Loss: 0.00001375
Iteration 90/1000 | Loss: 0.00001375
Iteration 91/1000 | Loss: 0.00001375
Iteration 92/1000 | Loss: 0.00001375
Iteration 93/1000 | Loss: 0.00001374
Iteration 94/1000 | Loss: 0.00001374
Iteration 95/1000 | Loss: 0.00001374
Iteration 96/1000 | Loss: 0.00001374
Iteration 97/1000 | Loss: 0.00001374
Iteration 98/1000 | Loss: 0.00001374
Iteration 99/1000 | Loss: 0.00001374
Iteration 100/1000 | Loss: 0.00001373
Iteration 101/1000 | Loss: 0.00001372
Iteration 102/1000 | Loss: 0.00001372
Iteration 103/1000 | Loss: 0.00001371
Iteration 104/1000 | Loss: 0.00001371
Iteration 105/1000 | Loss: 0.00001370
Iteration 106/1000 | Loss: 0.00001370
Iteration 107/1000 | Loss: 0.00001370
Iteration 108/1000 | Loss: 0.00001370
Iteration 109/1000 | Loss: 0.00001370
Iteration 110/1000 | Loss: 0.00001370
Iteration 111/1000 | Loss: 0.00001369
Iteration 112/1000 | Loss: 0.00001369
Iteration 113/1000 | Loss: 0.00001368
Iteration 114/1000 | Loss: 0.00001368
Iteration 115/1000 | Loss: 0.00001367
Iteration 116/1000 | Loss: 0.00001367
Iteration 117/1000 | Loss: 0.00001367
Iteration 118/1000 | Loss: 0.00001367
Iteration 119/1000 | Loss: 0.00001366
Iteration 120/1000 | Loss: 0.00001366
Iteration 121/1000 | Loss: 0.00001366
Iteration 122/1000 | Loss: 0.00001366
Iteration 123/1000 | Loss: 0.00001366
Iteration 124/1000 | Loss: 0.00001366
Iteration 125/1000 | Loss: 0.00001366
Iteration 126/1000 | Loss: 0.00001366
Iteration 127/1000 | Loss: 0.00001366
Iteration 128/1000 | Loss: 0.00001366
Iteration 129/1000 | Loss: 0.00001366
Iteration 130/1000 | Loss: 0.00001366
Iteration 131/1000 | Loss: 0.00001365
Iteration 132/1000 | Loss: 0.00001365
Iteration 133/1000 | Loss: 0.00001365
Iteration 134/1000 | Loss: 0.00001365
Iteration 135/1000 | Loss: 0.00001365
Iteration 136/1000 | Loss: 0.00001365
Iteration 137/1000 | Loss: 0.00001365
Iteration 138/1000 | Loss: 0.00001365
Iteration 139/1000 | Loss: 0.00001365
Iteration 140/1000 | Loss: 0.00001365
Iteration 141/1000 | Loss: 0.00001364
Iteration 142/1000 | Loss: 0.00001364
Iteration 143/1000 | Loss: 0.00001364
Iteration 144/1000 | Loss: 0.00001364
Iteration 145/1000 | Loss: 0.00001363
Iteration 146/1000 | Loss: 0.00001363
Iteration 147/1000 | Loss: 0.00001363
Iteration 148/1000 | Loss: 0.00001363
Iteration 149/1000 | Loss: 0.00001363
Iteration 150/1000 | Loss: 0.00001363
Iteration 151/1000 | Loss: 0.00001362
Iteration 152/1000 | Loss: 0.00001362
Iteration 153/1000 | Loss: 0.00001362
Iteration 154/1000 | Loss: 0.00001362
Iteration 155/1000 | Loss: 0.00001362
Iteration 156/1000 | Loss: 0.00001362
Iteration 157/1000 | Loss: 0.00001362
Iteration 158/1000 | Loss: 0.00001362
Iteration 159/1000 | Loss: 0.00001361
Iteration 160/1000 | Loss: 0.00001361
Iteration 161/1000 | Loss: 0.00001361
Iteration 162/1000 | Loss: 0.00001361
Iteration 163/1000 | Loss: 0.00001361
Iteration 164/1000 | Loss: 0.00001361
Iteration 165/1000 | Loss: 0.00001361
Iteration 166/1000 | Loss: 0.00001361
Iteration 167/1000 | Loss: 0.00001361
Iteration 168/1000 | Loss: 0.00001361
Iteration 169/1000 | Loss: 0.00001361
Iteration 170/1000 | Loss: 0.00001361
Iteration 171/1000 | Loss: 0.00001361
Iteration 172/1000 | Loss: 0.00001360
Iteration 173/1000 | Loss: 0.00001360
Iteration 174/1000 | Loss: 0.00001360
Iteration 175/1000 | Loss: 0.00001360
Iteration 176/1000 | Loss: 0.00001360
Iteration 177/1000 | Loss: 0.00001360
Iteration 178/1000 | Loss: 0.00001360
Iteration 179/1000 | Loss: 0.00001360
Iteration 180/1000 | Loss: 0.00001360
Iteration 181/1000 | Loss: 0.00001360
Iteration 182/1000 | Loss: 0.00001360
Iteration 183/1000 | Loss: 0.00001360
Iteration 184/1000 | Loss: 0.00001359
Iteration 185/1000 | Loss: 0.00001359
Iteration 186/1000 | Loss: 0.00001359
Iteration 187/1000 | Loss: 0.00001359
Iteration 188/1000 | Loss: 0.00001359
Iteration 189/1000 | Loss: 0.00001359
Iteration 190/1000 | Loss: 0.00001359
Iteration 191/1000 | Loss: 0.00001359
Iteration 192/1000 | Loss: 0.00001359
Iteration 193/1000 | Loss: 0.00001359
Iteration 194/1000 | Loss: 0.00001359
Iteration 195/1000 | Loss: 0.00001359
Iteration 196/1000 | Loss: 0.00001359
Iteration 197/1000 | Loss: 0.00001359
Iteration 198/1000 | Loss: 0.00001359
Iteration 199/1000 | Loss: 0.00001359
Iteration 200/1000 | Loss: 0.00001359
Iteration 201/1000 | Loss: 0.00001359
Iteration 202/1000 | Loss: 0.00001359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.3589328773377929e-05, 1.3589328773377929e-05, 1.3589328773377929e-05, 1.3589328773377929e-05, 1.3589328773377929e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3589328773377929e-05

Optimization complete. Final v2v error: 3.1396396160125732 mm

Highest mean error: 3.885124683380127 mm for frame 72

Lowest mean error: 2.65690279006958 mm for frame 13

Saving results

Total time: 43.67535328865051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966057
Iteration 2/25 | Loss: 0.00966057
Iteration 3/25 | Loss: 0.00966057
Iteration 4/25 | Loss: 0.00966057
Iteration 5/25 | Loss: 0.00966057
Iteration 6/25 | Loss: 0.00966057
Iteration 7/25 | Loss: 0.00966057
Iteration 8/25 | Loss: 0.00966057
Iteration 9/25 | Loss: 0.00966057
Iteration 10/25 | Loss: 0.00966057
Iteration 11/25 | Loss: 0.00966057
Iteration 12/25 | Loss: 0.00966057
Iteration 13/25 | Loss: 0.00966056
Iteration 14/25 | Loss: 0.00966056
Iteration 15/25 | Loss: 0.00966056
Iteration 16/25 | Loss: 0.00966056
Iteration 17/25 | Loss: 0.00966056
Iteration 18/25 | Loss: 0.00966056
Iteration 19/25 | Loss: 0.00966056
Iteration 20/25 | Loss: 0.00966056
Iteration 21/25 | Loss: 0.00966056
Iteration 22/25 | Loss: 0.00966056
Iteration 23/25 | Loss: 0.00966056
Iteration 24/25 | Loss: 0.00966056
Iteration 25/25 | Loss: 0.00966055

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43920720
Iteration 2/25 | Loss: 0.18474291
Iteration 3/25 | Loss: 0.18473724
Iteration 4/25 | Loss: 0.18473721
Iteration 5/25 | Loss: 0.18473721
Iteration 6/25 | Loss: 0.18473718
Iteration 7/25 | Loss: 0.18473718
Iteration 8/25 | Loss: 0.18473716
Iteration 9/25 | Loss: 0.18473716
Iteration 10/25 | Loss: 0.18473716
Iteration 11/25 | Loss: 0.18473718
Iteration 12/25 | Loss: 0.18473716
Iteration 13/25 | Loss: 0.18473716
Iteration 14/25 | Loss: 0.18473716
Iteration 15/25 | Loss: 0.18473716
Iteration 16/25 | Loss: 0.18473715
Iteration 17/25 | Loss: 0.18473715
Iteration 18/25 | Loss: 0.18473715
Iteration 19/25 | Loss: 0.18473715
Iteration 20/25 | Loss: 0.18473715
Iteration 21/25 | Loss: 0.18473715
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.18473714590072632, 0.18473714590072632, 0.18473714590072632, 0.18473714590072632, 0.18473714590072632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18473714590072632

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18473715
Iteration 2/1000 | Loss: 0.01192737
Iteration 3/1000 | Loss: 0.00349687
Iteration 4/1000 | Loss: 0.00314259
Iteration 5/1000 | Loss: 0.00240862
Iteration 6/1000 | Loss: 0.00131478
Iteration 7/1000 | Loss: 0.00182677
Iteration 8/1000 | Loss: 0.00200597
Iteration 9/1000 | Loss: 0.00073390
Iteration 10/1000 | Loss: 0.00050497
Iteration 11/1000 | Loss: 0.00018154
Iteration 12/1000 | Loss: 0.00032718
Iteration 13/1000 | Loss: 0.00056170
Iteration 14/1000 | Loss: 0.00074127
Iteration 15/1000 | Loss: 0.00008870
Iteration 16/1000 | Loss: 0.00099401
Iteration 17/1000 | Loss: 0.00065132
Iteration 18/1000 | Loss: 0.00006609
Iteration 19/1000 | Loss: 0.00008356
Iteration 20/1000 | Loss: 0.00008634
Iteration 21/1000 | Loss: 0.00005149
Iteration 22/1000 | Loss: 0.00038563
Iteration 23/1000 | Loss: 0.00055268
Iteration 24/1000 | Loss: 0.00012814
Iteration 25/1000 | Loss: 0.00056493
Iteration 26/1000 | Loss: 0.00068145
Iteration 27/1000 | Loss: 0.00008508
Iteration 28/1000 | Loss: 0.00008418
Iteration 29/1000 | Loss: 0.00100801
Iteration 30/1000 | Loss: 0.00241890
Iteration 31/1000 | Loss: 0.00007614
Iteration 32/1000 | Loss: 0.00008066
Iteration 33/1000 | Loss: 0.00002628
Iteration 34/1000 | Loss: 0.00002539
Iteration 35/1000 | Loss: 0.00012391
Iteration 36/1000 | Loss: 0.00002465
Iteration 37/1000 | Loss: 0.00002391
Iteration 38/1000 | Loss: 0.00006468
Iteration 39/1000 | Loss: 0.00015527
Iteration 40/1000 | Loss: 0.00009234
Iteration 41/1000 | Loss: 0.00002266
Iteration 42/1000 | Loss: 0.00002989
Iteration 43/1000 | Loss: 0.00024906
Iteration 44/1000 | Loss: 0.00005807
Iteration 45/1000 | Loss: 0.00002291
Iteration 46/1000 | Loss: 0.00009054
Iteration 47/1000 | Loss: 0.00020947
Iteration 48/1000 | Loss: 0.00011476
Iteration 49/1000 | Loss: 0.00002623
Iteration 50/1000 | Loss: 0.00010164
Iteration 51/1000 | Loss: 0.00002207
Iteration 52/1000 | Loss: 0.00006266
Iteration 53/1000 | Loss: 0.00056102
Iteration 54/1000 | Loss: 0.00005051
Iteration 55/1000 | Loss: 0.00021548
Iteration 56/1000 | Loss: 0.00019744
Iteration 57/1000 | Loss: 0.00014136
Iteration 58/1000 | Loss: 0.00002230
Iteration 59/1000 | Loss: 0.00017821
Iteration 60/1000 | Loss: 0.00018290
Iteration 61/1000 | Loss: 0.00005355
Iteration 62/1000 | Loss: 0.00003033
Iteration 63/1000 | Loss: 0.00017202
Iteration 64/1000 | Loss: 0.00002608
Iteration 65/1000 | Loss: 0.00002375
Iteration 66/1000 | Loss: 0.00002152
Iteration 67/1000 | Loss: 0.00002150
Iteration 68/1000 | Loss: 0.00002148
Iteration 69/1000 | Loss: 0.00002219
Iteration 70/1000 | Loss: 0.00002219
Iteration 71/1000 | Loss: 0.00005931
Iteration 72/1000 | Loss: 0.00002142
Iteration 73/1000 | Loss: 0.00002131
Iteration 74/1000 | Loss: 0.00002127
Iteration 75/1000 | Loss: 0.00002123
Iteration 76/1000 | Loss: 0.00002123
Iteration 77/1000 | Loss: 0.00002123
Iteration 78/1000 | Loss: 0.00002122
Iteration 79/1000 | Loss: 0.00002121
Iteration 80/1000 | Loss: 0.00002121
Iteration 81/1000 | Loss: 0.00002121
Iteration 82/1000 | Loss: 0.00002121
Iteration 83/1000 | Loss: 0.00002121
Iteration 84/1000 | Loss: 0.00002121
Iteration 85/1000 | Loss: 0.00002121
Iteration 86/1000 | Loss: 0.00002120
Iteration 87/1000 | Loss: 0.00002120
Iteration 88/1000 | Loss: 0.00002120
Iteration 89/1000 | Loss: 0.00002120
Iteration 90/1000 | Loss: 0.00002120
Iteration 91/1000 | Loss: 0.00002120
Iteration 92/1000 | Loss: 0.00002120
Iteration 93/1000 | Loss: 0.00002120
Iteration 94/1000 | Loss: 0.00002120
Iteration 95/1000 | Loss: 0.00002120
Iteration 96/1000 | Loss: 0.00002120
Iteration 97/1000 | Loss: 0.00002117
Iteration 98/1000 | Loss: 0.00002115
Iteration 99/1000 | Loss: 0.00002112
Iteration 100/1000 | Loss: 0.00002112
Iteration 101/1000 | Loss: 0.00002112
Iteration 102/1000 | Loss: 0.00002112
Iteration 103/1000 | Loss: 0.00002112
Iteration 104/1000 | Loss: 0.00002112
Iteration 105/1000 | Loss: 0.00002112
Iteration 106/1000 | Loss: 0.00002112
Iteration 107/1000 | Loss: 0.00002111
Iteration 108/1000 | Loss: 0.00002111
Iteration 109/1000 | Loss: 0.00002110
Iteration 110/1000 | Loss: 0.00002110
Iteration 111/1000 | Loss: 0.00002110
Iteration 112/1000 | Loss: 0.00002109
Iteration 113/1000 | Loss: 0.00002109
Iteration 114/1000 | Loss: 0.00002108
Iteration 115/1000 | Loss: 0.00002108
Iteration 116/1000 | Loss: 0.00002108
Iteration 117/1000 | Loss: 0.00002108
Iteration 118/1000 | Loss: 0.00002108
Iteration 119/1000 | Loss: 0.00002108
Iteration 120/1000 | Loss: 0.00002108
Iteration 121/1000 | Loss: 0.00002107
Iteration 122/1000 | Loss: 0.00002107
Iteration 123/1000 | Loss: 0.00002107
Iteration 124/1000 | Loss: 0.00002107
Iteration 125/1000 | Loss: 0.00002106
Iteration 126/1000 | Loss: 0.00002106
Iteration 127/1000 | Loss: 0.00002106
Iteration 128/1000 | Loss: 0.00002106
Iteration 129/1000 | Loss: 0.00002105
Iteration 130/1000 | Loss: 0.00002105
Iteration 131/1000 | Loss: 0.00002105
Iteration 132/1000 | Loss: 0.00002105
Iteration 133/1000 | Loss: 0.00002105
Iteration 134/1000 | Loss: 0.00002104
Iteration 135/1000 | Loss: 0.00002104
Iteration 136/1000 | Loss: 0.00002100
Iteration 137/1000 | Loss: 0.00013805
Iteration 138/1000 | Loss: 0.00011261
Iteration 139/1000 | Loss: 0.00002658
Iteration 140/1000 | Loss: 0.00002461
Iteration 141/1000 | Loss: 0.00003570
Iteration 142/1000 | Loss: 0.00002101
Iteration 143/1000 | Loss: 0.00002094
Iteration 144/1000 | Loss: 0.00002094
Iteration 145/1000 | Loss: 0.00002094
Iteration 146/1000 | Loss: 0.00002094
Iteration 147/1000 | Loss: 0.00002093
Iteration 148/1000 | Loss: 0.00002093
Iteration 149/1000 | Loss: 0.00002093
Iteration 150/1000 | Loss: 0.00002088
Iteration 151/1000 | Loss: 0.00002087
Iteration 152/1000 | Loss: 0.00002087
Iteration 153/1000 | Loss: 0.00002087
Iteration 154/1000 | Loss: 0.00002087
Iteration 155/1000 | Loss: 0.00002087
Iteration 156/1000 | Loss: 0.00002087
Iteration 157/1000 | Loss: 0.00002087
Iteration 158/1000 | Loss: 0.00002087
Iteration 159/1000 | Loss: 0.00002087
Iteration 160/1000 | Loss: 0.00002086
Iteration 161/1000 | Loss: 0.00002086
Iteration 162/1000 | Loss: 0.00002086
Iteration 163/1000 | Loss: 0.00002086
Iteration 164/1000 | Loss: 0.00002086
Iteration 165/1000 | Loss: 0.00002086
Iteration 166/1000 | Loss: 0.00002085
Iteration 167/1000 | Loss: 0.00002085
Iteration 168/1000 | Loss: 0.00002085
Iteration 169/1000 | Loss: 0.00002085
Iteration 170/1000 | Loss: 0.00002085
Iteration 171/1000 | Loss: 0.00002085
Iteration 172/1000 | Loss: 0.00002085
Iteration 173/1000 | Loss: 0.00002085
Iteration 174/1000 | Loss: 0.00002084
Iteration 175/1000 | Loss: 0.00002084
Iteration 176/1000 | Loss: 0.00002084
Iteration 177/1000 | Loss: 0.00002083
Iteration 178/1000 | Loss: 0.00002083
Iteration 179/1000 | Loss: 0.00002083
Iteration 180/1000 | Loss: 0.00002083
Iteration 181/1000 | Loss: 0.00002082
Iteration 182/1000 | Loss: 0.00002082
Iteration 183/1000 | Loss: 0.00002082
Iteration 184/1000 | Loss: 0.00002082
Iteration 185/1000 | Loss: 0.00002082
Iteration 186/1000 | Loss: 0.00002082
Iteration 187/1000 | Loss: 0.00002082
Iteration 188/1000 | Loss: 0.00002082
Iteration 189/1000 | Loss: 0.00002082
Iteration 190/1000 | Loss: 0.00002082
Iteration 191/1000 | Loss: 0.00002082
Iteration 192/1000 | Loss: 0.00002082
Iteration 193/1000 | Loss: 0.00002082
Iteration 194/1000 | Loss: 0.00002082
Iteration 195/1000 | Loss: 0.00002082
Iteration 196/1000 | Loss: 0.00002082
Iteration 197/1000 | Loss: 0.00002082
Iteration 198/1000 | Loss: 0.00002082
Iteration 199/1000 | Loss: 0.00002082
Iteration 200/1000 | Loss: 0.00002082
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [2.0818633856833912e-05, 2.0818633856833912e-05, 2.0818633856833912e-05, 2.0818633856833912e-05, 2.0818633856833912e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0818633856833912e-05

Optimization complete. Final v2v error: 3.9651975631713867 mm

Highest mean error: 4.341216564178467 mm for frame 198

Lowest mean error: 3.775621175765991 mm for frame 215

Saving results

Total time: 136.8934576511383
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795125
Iteration 2/25 | Loss: 0.00135585
Iteration 3/25 | Loss: 0.00126354
Iteration 4/25 | Loss: 0.00125812
Iteration 5/25 | Loss: 0.00125759
Iteration 6/25 | Loss: 0.00125759
Iteration 7/25 | Loss: 0.00125759
Iteration 8/25 | Loss: 0.00125759
Iteration 9/25 | Loss: 0.00125759
Iteration 10/25 | Loss: 0.00125759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012575880391523242, 0.0012575880391523242, 0.0012575880391523242, 0.0012575880391523242, 0.0012575880391523242]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012575880391523242

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25774467
Iteration 2/25 | Loss: 0.00132918
Iteration 3/25 | Loss: 0.00132918
Iteration 4/25 | Loss: 0.00132918
Iteration 5/25 | Loss: 0.00132917
Iteration 6/25 | Loss: 0.00132917
Iteration 7/25 | Loss: 0.00132917
Iteration 8/25 | Loss: 0.00132917
Iteration 9/25 | Loss: 0.00132917
Iteration 10/25 | Loss: 0.00132917
Iteration 11/25 | Loss: 0.00132917
Iteration 12/25 | Loss: 0.00132917
Iteration 13/25 | Loss: 0.00132917
Iteration 14/25 | Loss: 0.00132917
Iteration 15/25 | Loss: 0.00132917
Iteration 16/25 | Loss: 0.00132917
Iteration 17/25 | Loss: 0.00132917
Iteration 18/25 | Loss: 0.00132917
Iteration 19/25 | Loss: 0.00132917
Iteration 20/25 | Loss: 0.00132917
Iteration 21/25 | Loss: 0.00132917
Iteration 22/25 | Loss: 0.00132917
Iteration 23/25 | Loss: 0.00132917
Iteration 24/25 | Loss: 0.00132917
Iteration 25/25 | Loss: 0.00132917

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132917
Iteration 2/1000 | Loss: 0.00002457
Iteration 3/1000 | Loss: 0.00001852
Iteration 4/1000 | Loss: 0.00001714
Iteration 5/1000 | Loss: 0.00001634
Iteration 6/1000 | Loss: 0.00001583
Iteration 7/1000 | Loss: 0.00001549
Iteration 8/1000 | Loss: 0.00001509
Iteration 9/1000 | Loss: 0.00001471
Iteration 10/1000 | Loss: 0.00001469
Iteration 11/1000 | Loss: 0.00001453
Iteration 12/1000 | Loss: 0.00001441
Iteration 13/1000 | Loss: 0.00001422
Iteration 14/1000 | Loss: 0.00001421
Iteration 15/1000 | Loss: 0.00001413
Iteration 16/1000 | Loss: 0.00001406
Iteration 17/1000 | Loss: 0.00001398
Iteration 18/1000 | Loss: 0.00001398
Iteration 19/1000 | Loss: 0.00001397
Iteration 20/1000 | Loss: 0.00001385
Iteration 21/1000 | Loss: 0.00001383
Iteration 22/1000 | Loss: 0.00001382
Iteration 23/1000 | Loss: 0.00001381
Iteration 24/1000 | Loss: 0.00001380
Iteration 25/1000 | Loss: 0.00001380
Iteration 26/1000 | Loss: 0.00001379
Iteration 27/1000 | Loss: 0.00001379
Iteration 28/1000 | Loss: 0.00001378
Iteration 29/1000 | Loss: 0.00001378
Iteration 30/1000 | Loss: 0.00001377
Iteration 31/1000 | Loss: 0.00001377
Iteration 32/1000 | Loss: 0.00001376
Iteration 33/1000 | Loss: 0.00001375
Iteration 34/1000 | Loss: 0.00001373
Iteration 35/1000 | Loss: 0.00001372
Iteration 36/1000 | Loss: 0.00001372
Iteration 37/1000 | Loss: 0.00001372
Iteration 38/1000 | Loss: 0.00001372
Iteration 39/1000 | Loss: 0.00001372
Iteration 40/1000 | Loss: 0.00001372
Iteration 41/1000 | Loss: 0.00001372
Iteration 42/1000 | Loss: 0.00001372
Iteration 43/1000 | Loss: 0.00001372
Iteration 44/1000 | Loss: 0.00001372
Iteration 45/1000 | Loss: 0.00001371
Iteration 46/1000 | Loss: 0.00001371
Iteration 47/1000 | Loss: 0.00001371
Iteration 48/1000 | Loss: 0.00001371
Iteration 49/1000 | Loss: 0.00001370
Iteration 50/1000 | Loss: 0.00001370
Iteration 51/1000 | Loss: 0.00001362
Iteration 52/1000 | Loss: 0.00001355
Iteration 53/1000 | Loss: 0.00001354
Iteration 54/1000 | Loss: 0.00001354
Iteration 55/1000 | Loss: 0.00001352
Iteration 56/1000 | Loss: 0.00001352
Iteration 57/1000 | Loss: 0.00001351
Iteration 58/1000 | Loss: 0.00001351
Iteration 59/1000 | Loss: 0.00001351
Iteration 60/1000 | Loss: 0.00001350
Iteration 61/1000 | Loss: 0.00001349
Iteration 62/1000 | Loss: 0.00001349
Iteration 63/1000 | Loss: 0.00001348
Iteration 64/1000 | Loss: 0.00001348
Iteration 65/1000 | Loss: 0.00001348
Iteration 66/1000 | Loss: 0.00001348
Iteration 67/1000 | Loss: 0.00001347
Iteration 68/1000 | Loss: 0.00001347
Iteration 69/1000 | Loss: 0.00001346
Iteration 70/1000 | Loss: 0.00001346
Iteration 71/1000 | Loss: 0.00001345
Iteration 72/1000 | Loss: 0.00001345
Iteration 73/1000 | Loss: 0.00001345
Iteration 74/1000 | Loss: 0.00001345
Iteration 75/1000 | Loss: 0.00001344
Iteration 76/1000 | Loss: 0.00001344
Iteration 77/1000 | Loss: 0.00001344
Iteration 78/1000 | Loss: 0.00001344
Iteration 79/1000 | Loss: 0.00001344
Iteration 80/1000 | Loss: 0.00001344
Iteration 81/1000 | Loss: 0.00001343
Iteration 82/1000 | Loss: 0.00001343
Iteration 83/1000 | Loss: 0.00001343
Iteration 84/1000 | Loss: 0.00001343
Iteration 85/1000 | Loss: 0.00001343
Iteration 86/1000 | Loss: 0.00001343
Iteration 87/1000 | Loss: 0.00001343
Iteration 88/1000 | Loss: 0.00001343
Iteration 89/1000 | Loss: 0.00001343
Iteration 90/1000 | Loss: 0.00001343
Iteration 91/1000 | Loss: 0.00001343
Iteration 92/1000 | Loss: 0.00001343
Iteration 93/1000 | Loss: 0.00001342
Iteration 94/1000 | Loss: 0.00001342
Iteration 95/1000 | Loss: 0.00001342
Iteration 96/1000 | Loss: 0.00001342
Iteration 97/1000 | Loss: 0.00001342
Iteration 98/1000 | Loss: 0.00001342
Iteration 99/1000 | Loss: 0.00001342
Iteration 100/1000 | Loss: 0.00001342
Iteration 101/1000 | Loss: 0.00001342
Iteration 102/1000 | Loss: 0.00001342
Iteration 103/1000 | Loss: 0.00001342
Iteration 104/1000 | Loss: 0.00001342
Iteration 105/1000 | Loss: 0.00001342
Iteration 106/1000 | Loss: 0.00001342
Iteration 107/1000 | Loss: 0.00001342
Iteration 108/1000 | Loss: 0.00001342
Iteration 109/1000 | Loss: 0.00001342
Iteration 110/1000 | Loss: 0.00001342
Iteration 111/1000 | Loss: 0.00001342
Iteration 112/1000 | Loss: 0.00001342
Iteration 113/1000 | Loss: 0.00001341
Iteration 114/1000 | Loss: 0.00001341
Iteration 115/1000 | Loss: 0.00001341
Iteration 116/1000 | Loss: 0.00001341
Iteration 117/1000 | Loss: 0.00001341
Iteration 118/1000 | Loss: 0.00001341
Iteration 119/1000 | Loss: 0.00001341
Iteration 120/1000 | Loss: 0.00001341
Iteration 121/1000 | Loss: 0.00001341
Iteration 122/1000 | Loss: 0.00001341
Iteration 123/1000 | Loss: 0.00001341
Iteration 124/1000 | Loss: 0.00001341
Iteration 125/1000 | Loss: 0.00001341
Iteration 126/1000 | Loss: 0.00001341
Iteration 127/1000 | Loss: 0.00001341
Iteration 128/1000 | Loss: 0.00001341
Iteration 129/1000 | Loss: 0.00001341
Iteration 130/1000 | Loss: 0.00001341
Iteration 131/1000 | Loss: 0.00001341
Iteration 132/1000 | Loss: 0.00001341
Iteration 133/1000 | Loss: 0.00001341
Iteration 134/1000 | Loss: 0.00001341
Iteration 135/1000 | Loss: 0.00001341
Iteration 136/1000 | Loss: 0.00001341
Iteration 137/1000 | Loss: 0.00001341
Iteration 138/1000 | Loss: 0.00001341
Iteration 139/1000 | Loss: 0.00001341
Iteration 140/1000 | Loss: 0.00001341
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.3414770364761353e-05, 1.3414770364761353e-05, 1.3414770364761353e-05, 1.3414770364761353e-05, 1.3414770364761353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3414770364761353e-05

Optimization complete. Final v2v error: 3.091330051422119 mm

Highest mean error: 3.2469213008880615 mm for frame 141

Lowest mean error: 2.9905011653900146 mm for frame 43

Saving results

Total time: 41.766698598861694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772894
Iteration 2/25 | Loss: 0.00170455
Iteration 3/25 | Loss: 0.00143367
Iteration 4/25 | Loss: 0.00140103
Iteration 5/25 | Loss: 0.00137628
Iteration 6/25 | Loss: 0.00138822
Iteration 7/25 | Loss: 0.00138210
Iteration 8/25 | Loss: 0.00136978
Iteration 9/25 | Loss: 0.00136067
Iteration 10/25 | Loss: 0.00135840
Iteration 11/25 | Loss: 0.00135725
Iteration 12/25 | Loss: 0.00136265
Iteration 13/25 | Loss: 0.00135093
Iteration 14/25 | Loss: 0.00134173
Iteration 15/25 | Loss: 0.00134363
Iteration 16/25 | Loss: 0.00133971
Iteration 17/25 | Loss: 0.00133746
Iteration 18/25 | Loss: 0.00133678
Iteration 19/25 | Loss: 0.00133670
Iteration 20/25 | Loss: 0.00133626
Iteration 21/25 | Loss: 0.00133598
Iteration 22/25 | Loss: 0.00133591
Iteration 23/25 | Loss: 0.00133558
Iteration 24/25 | Loss: 0.00133580
Iteration 25/25 | Loss: 0.00133903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24642920
Iteration 2/25 | Loss: 0.00159456
Iteration 3/25 | Loss: 0.00157608
Iteration 4/25 | Loss: 0.00157607
Iteration 5/25 | Loss: 0.00157607
Iteration 6/25 | Loss: 0.00157607
Iteration 7/25 | Loss: 0.00157607
Iteration 8/25 | Loss: 0.00157607
Iteration 9/25 | Loss: 0.00157607
Iteration 10/25 | Loss: 0.00157607
Iteration 11/25 | Loss: 0.00157607
Iteration 12/25 | Loss: 0.00157607
Iteration 13/25 | Loss: 0.00157607
Iteration 14/25 | Loss: 0.00157607
Iteration 15/25 | Loss: 0.00157607
Iteration 16/25 | Loss: 0.00157607
Iteration 17/25 | Loss: 0.00157607
Iteration 18/25 | Loss: 0.00157607
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015760703245177865, 0.0015760703245177865, 0.0015760703245177865, 0.0015760703245177865, 0.0015760703245177865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015760703245177865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157607
Iteration 2/1000 | Loss: 0.00011261
Iteration 3/1000 | Loss: 0.00003968
Iteration 4/1000 | Loss: 0.00003668
Iteration 5/1000 | Loss: 0.00004285
Iteration 6/1000 | Loss: 0.00002957
Iteration 7/1000 | Loss: 0.00004017
Iteration 8/1000 | Loss: 0.00004047
Iteration 9/1000 | Loss: 0.00004271
Iteration 10/1000 | Loss: 0.00003849
Iteration 11/1000 | Loss: 0.00003874
Iteration 12/1000 | Loss: 0.00003950
Iteration 13/1000 | Loss: 0.00004828
Iteration 14/1000 | Loss: 0.00004525
Iteration 15/1000 | Loss: 0.00003892
Iteration 16/1000 | Loss: 0.00005008
Iteration 17/1000 | Loss: 0.00005250
Iteration 18/1000 | Loss: 0.00004913
Iteration 19/1000 | Loss: 0.00004891
Iteration 20/1000 | Loss: 0.00005036
Iteration 21/1000 | Loss: 0.00005633
Iteration 22/1000 | Loss: 0.00004601
Iteration 23/1000 | Loss: 0.00003733
Iteration 24/1000 | Loss: 0.00004464
Iteration 25/1000 | Loss: 0.00004426
Iteration 26/1000 | Loss: 0.00004283
Iteration 27/1000 | Loss: 0.00005205
Iteration 28/1000 | Loss: 0.00004555
Iteration 29/1000 | Loss: 0.00005062
Iteration 30/1000 | Loss: 0.00004797
Iteration 31/1000 | Loss: 0.00005018
Iteration 32/1000 | Loss: 0.00005075
Iteration 33/1000 | Loss: 0.00005038
Iteration 34/1000 | Loss: 0.00005066
Iteration 35/1000 | Loss: 0.00004611
Iteration 36/1000 | Loss: 0.00005068
Iteration 37/1000 | Loss: 0.00005478
Iteration 38/1000 | Loss: 0.00004769
Iteration 39/1000 | Loss: 0.00004530
Iteration 40/1000 | Loss: 0.00005015
Iteration 41/1000 | Loss: 0.00004427
Iteration 42/1000 | Loss: 0.00004903
Iteration 43/1000 | Loss: 0.00004707
Iteration 44/1000 | Loss: 0.00004994
Iteration 45/1000 | Loss: 0.00005098
Iteration 46/1000 | Loss: 0.00004518
Iteration 47/1000 | Loss: 0.00005540
Iteration 48/1000 | Loss: 0.00004888
Iteration 49/1000 | Loss: 0.00005447
Iteration 50/1000 | Loss: 0.00004995
Iteration 51/1000 | Loss: 0.00005071
Iteration 52/1000 | Loss: 0.00004549
Iteration 53/1000 | Loss: 0.00005481
Iteration 54/1000 | Loss: 0.00004397
Iteration 55/1000 | Loss: 0.00005923
Iteration 56/1000 | Loss: 0.00004364
Iteration 57/1000 | Loss: 0.00006428
Iteration 58/1000 | Loss: 0.00005083
Iteration 59/1000 | Loss: 0.00005787
Iteration 60/1000 | Loss: 0.00005240
Iteration 61/1000 | Loss: 0.00004479
Iteration 62/1000 | Loss: 0.00005550
Iteration 63/1000 | Loss: 0.00004715
Iteration 64/1000 | Loss: 0.00003719
Iteration 65/1000 | Loss: 0.00003582
Iteration 66/1000 | Loss: 0.00004264
Iteration 67/1000 | Loss: 0.00004259
Iteration 68/1000 | Loss: 0.00005154
Iteration 69/1000 | Loss: 0.00004596
Iteration 70/1000 | Loss: 0.00005137
Iteration 71/1000 | Loss: 0.00003392
Iteration 72/1000 | Loss: 0.00004087
Iteration 73/1000 | Loss: 0.00004049
Iteration 74/1000 | Loss: 0.00002722
Iteration 75/1000 | Loss: 0.00002749
Iteration 76/1000 | Loss: 0.00003170
Iteration 77/1000 | Loss: 0.00003718
Iteration 78/1000 | Loss: 0.00003906
Iteration 79/1000 | Loss: 0.00003481
Iteration 80/1000 | Loss: 0.00003579
Iteration 81/1000 | Loss: 0.00002849
Iteration 82/1000 | Loss: 0.00003470
Iteration 83/1000 | Loss: 0.00003786
Iteration 84/1000 | Loss: 0.00003394
Iteration 85/1000 | Loss: 0.00003850
Iteration 86/1000 | Loss: 0.00003339
Iteration 87/1000 | Loss: 0.00003608
Iteration 88/1000 | Loss: 0.00003496
Iteration 89/1000 | Loss: 0.00003506
Iteration 90/1000 | Loss: 0.00003743
Iteration 91/1000 | Loss: 0.00003592
Iteration 92/1000 | Loss: 0.00003431
Iteration 93/1000 | Loss: 0.00003445
Iteration 94/1000 | Loss: 0.00003806
Iteration 95/1000 | Loss: 0.00003487
Iteration 96/1000 | Loss: 0.00003517
Iteration 97/1000 | Loss: 0.00003837
Iteration 98/1000 | Loss: 0.00003377
Iteration 99/1000 | Loss: 0.00003711
Iteration 100/1000 | Loss: 0.00003759
Iteration 101/1000 | Loss: 0.00003798
Iteration 102/1000 | Loss: 0.00002825
Iteration 103/1000 | Loss: 0.00004159
Iteration 104/1000 | Loss: 0.00004434
Iteration 105/1000 | Loss: 0.00004030
Iteration 106/1000 | Loss: 0.00004315
Iteration 107/1000 | Loss: 0.00003706
Iteration 108/1000 | Loss: 0.00003616
Iteration 109/1000 | Loss: 0.00003418
Iteration 110/1000 | Loss: 0.00003503
Iteration 111/1000 | Loss: 0.00003365
Iteration 112/1000 | Loss: 0.00003430
Iteration 113/1000 | Loss: 0.00003546
Iteration 114/1000 | Loss: 0.00003575
Iteration 115/1000 | Loss: 0.00003383
Iteration 116/1000 | Loss: 0.00003679
Iteration 117/1000 | Loss: 0.00003660
Iteration 118/1000 | Loss: 0.00003784
Iteration 119/1000 | Loss: 0.00003733
Iteration 120/1000 | Loss: 0.00003569
Iteration 121/1000 | Loss: 0.00003430
Iteration 122/1000 | Loss: 0.00003486
Iteration 123/1000 | Loss: 0.00003626
Iteration 124/1000 | Loss: 0.00004580
Iteration 125/1000 | Loss: 0.00003750
Iteration 126/1000 | Loss: 0.00004383
Iteration 127/1000 | Loss: 0.00003743
Iteration 128/1000 | Loss: 0.00002814
Iteration 129/1000 | Loss: 0.00002641
Iteration 130/1000 | Loss: 0.00002524
Iteration 131/1000 | Loss: 0.00002464
Iteration 132/1000 | Loss: 0.00002431
Iteration 133/1000 | Loss: 0.00002402
Iteration 134/1000 | Loss: 0.00002399
Iteration 135/1000 | Loss: 0.00002397
Iteration 136/1000 | Loss: 0.00002394
Iteration 137/1000 | Loss: 0.00002382
Iteration 138/1000 | Loss: 0.00002382
Iteration 139/1000 | Loss: 0.00002381
Iteration 140/1000 | Loss: 0.00002381
Iteration 141/1000 | Loss: 0.00002380
Iteration 142/1000 | Loss: 0.00002380
Iteration 143/1000 | Loss: 0.00002376
Iteration 144/1000 | Loss: 0.00002374
Iteration 145/1000 | Loss: 0.00002374
Iteration 146/1000 | Loss: 0.00002373
Iteration 147/1000 | Loss: 0.00002371
Iteration 148/1000 | Loss: 0.00002371
Iteration 149/1000 | Loss: 0.00002370
Iteration 150/1000 | Loss: 0.00002369
Iteration 151/1000 | Loss: 0.00002368
Iteration 152/1000 | Loss: 0.00002367
Iteration 153/1000 | Loss: 0.00002367
Iteration 154/1000 | Loss: 0.00002366
Iteration 155/1000 | Loss: 0.00002364
Iteration 156/1000 | Loss: 0.00002364
Iteration 157/1000 | Loss: 0.00002357
Iteration 158/1000 | Loss: 0.00002343
Iteration 159/1000 | Loss: 0.00002342
Iteration 160/1000 | Loss: 0.00002342
Iteration 161/1000 | Loss: 0.00002341
Iteration 162/1000 | Loss: 0.00002339
Iteration 163/1000 | Loss: 0.00002338
Iteration 164/1000 | Loss: 0.00002334
Iteration 165/1000 | Loss: 0.00002330
Iteration 166/1000 | Loss: 0.00002330
Iteration 167/1000 | Loss: 0.00002329
Iteration 168/1000 | Loss: 0.00002329
Iteration 169/1000 | Loss: 0.00002329
Iteration 170/1000 | Loss: 0.00002328
Iteration 171/1000 | Loss: 0.00002328
Iteration 172/1000 | Loss: 0.00002327
Iteration 173/1000 | Loss: 0.00002327
Iteration 174/1000 | Loss: 0.00002327
Iteration 175/1000 | Loss: 0.00002327
Iteration 176/1000 | Loss: 0.00002327
Iteration 177/1000 | Loss: 0.00002326
Iteration 178/1000 | Loss: 0.00002326
Iteration 179/1000 | Loss: 0.00002326
Iteration 180/1000 | Loss: 0.00002324
Iteration 181/1000 | Loss: 0.00002320
Iteration 182/1000 | Loss: 0.00002320
Iteration 183/1000 | Loss: 0.00002319
Iteration 184/1000 | Loss: 0.00002319
Iteration 185/1000 | Loss: 0.00002314
Iteration 186/1000 | Loss: 0.00002314
Iteration 187/1000 | Loss: 0.00002313
Iteration 188/1000 | Loss: 0.00002313
Iteration 189/1000 | Loss: 0.00002312
Iteration 190/1000 | Loss: 0.00002312
Iteration 191/1000 | Loss: 0.00002311
Iteration 192/1000 | Loss: 0.00002311
Iteration 193/1000 | Loss: 0.00002310
Iteration 194/1000 | Loss: 0.00002309
Iteration 195/1000 | Loss: 0.00002309
Iteration 196/1000 | Loss: 0.00002309
Iteration 197/1000 | Loss: 0.00002308
Iteration 198/1000 | Loss: 0.00002308
Iteration 199/1000 | Loss: 0.00002307
Iteration 200/1000 | Loss: 0.00002306
Iteration 201/1000 | Loss: 0.00002306
Iteration 202/1000 | Loss: 0.00002295
Iteration 203/1000 | Loss: 0.00002294
Iteration 204/1000 | Loss: 0.00002294
Iteration 205/1000 | Loss: 0.00002294
Iteration 206/1000 | Loss: 0.00002293
Iteration 207/1000 | Loss: 0.00002293
Iteration 208/1000 | Loss: 0.00002292
Iteration 209/1000 | Loss: 0.00002291
Iteration 210/1000 | Loss: 0.00002291
Iteration 211/1000 | Loss: 0.00002291
Iteration 212/1000 | Loss: 0.00002290
Iteration 213/1000 | Loss: 0.00002290
Iteration 214/1000 | Loss: 0.00002290
Iteration 215/1000 | Loss: 0.00002290
Iteration 216/1000 | Loss: 0.00002290
Iteration 217/1000 | Loss: 0.00002289
Iteration 218/1000 | Loss: 0.00002289
Iteration 219/1000 | Loss: 0.00002285
Iteration 220/1000 | Loss: 0.00002285
Iteration 221/1000 | Loss: 0.00002285
Iteration 222/1000 | Loss: 0.00002285
Iteration 223/1000 | Loss: 0.00002285
Iteration 224/1000 | Loss: 0.00002284
Iteration 225/1000 | Loss: 0.00002284
Iteration 226/1000 | Loss: 0.00002280
Iteration 227/1000 | Loss: 0.00002280
Iteration 228/1000 | Loss: 0.00002279
Iteration 229/1000 | Loss: 0.00002278
Iteration 230/1000 | Loss: 0.00002278
Iteration 231/1000 | Loss: 0.00002277
Iteration 232/1000 | Loss: 0.00002277
Iteration 233/1000 | Loss: 0.00002276
Iteration 234/1000 | Loss: 0.00002272
Iteration 235/1000 | Loss: 0.00002269
Iteration 236/1000 | Loss: 0.00002269
Iteration 237/1000 | Loss: 0.00002268
Iteration 238/1000 | Loss: 0.00002268
Iteration 239/1000 | Loss: 0.00002268
Iteration 240/1000 | Loss: 0.00002268
Iteration 241/1000 | Loss: 0.00002268
Iteration 242/1000 | Loss: 0.00002268
Iteration 243/1000 | Loss: 0.00002268
Iteration 244/1000 | Loss: 0.00002267
Iteration 245/1000 | Loss: 0.00002267
Iteration 246/1000 | Loss: 0.00002264
Iteration 247/1000 | Loss: 0.00002264
Iteration 248/1000 | Loss: 0.00002264
Iteration 249/1000 | Loss: 0.00002264
Iteration 250/1000 | Loss: 0.00002264
Iteration 251/1000 | Loss: 0.00002264
Iteration 252/1000 | Loss: 0.00002263
Iteration 253/1000 | Loss: 0.00002263
Iteration 254/1000 | Loss: 0.00002262
Iteration 255/1000 | Loss: 0.00002262
Iteration 256/1000 | Loss: 0.00002262
Iteration 257/1000 | Loss: 0.00002262
Iteration 258/1000 | Loss: 0.00002261
Iteration 259/1000 | Loss: 0.00002260
Iteration 260/1000 | Loss: 0.00002260
Iteration 261/1000 | Loss: 0.00002260
Iteration 262/1000 | Loss: 0.00002260
Iteration 263/1000 | Loss: 0.00002259
Iteration 264/1000 | Loss: 0.00002259
Iteration 265/1000 | Loss: 0.00002258
Iteration 266/1000 | Loss: 0.00002258
Iteration 267/1000 | Loss: 0.00002257
Iteration 268/1000 | Loss: 0.00002257
Iteration 269/1000 | Loss: 0.00002257
Iteration 270/1000 | Loss: 0.00002256
Iteration 271/1000 | Loss: 0.00002256
Iteration 272/1000 | Loss: 0.00002256
Iteration 273/1000 | Loss: 0.00002256
Iteration 274/1000 | Loss: 0.00002255
Iteration 275/1000 | Loss: 0.00002255
Iteration 276/1000 | Loss: 0.00002255
Iteration 277/1000 | Loss: 0.00002255
Iteration 278/1000 | Loss: 0.00002254
Iteration 279/1000 | Loss: 0.00002254
Iteration 280/1000 | Loss: 0.00002254
Iteration 281/1000 | Loss: 0.00002254
Iteration 282/1000 | Loss: 0.00002253
Iteration 283/1000 | Loss: 0.00002253
Iteration 284/1000 | Loss: 0.00002253
Iteration 285/1000 | Loss: 0.00002253
Iteration 286/1000 | Loss: 0.00002253
Iteration 287/1000 | Loss: 0.00002253
Iteration 288/1000 | Loss: 0.00002253
Iteration 289/1000 | Loss: 0.00002253
Iteration 290/1000 | Loss: 0.00002252
Iteration 291/1000 | Loss: 0.00002252
Iteration 292/1000 | Loss: 0.00002251
Iteration 293/1000 | Loss: 0.00002251
Iteration 294/1000 | Loss: 0.00002250
Iteration 295/1000 | Loss: 0.00002250
Iteration 296/1000 | Loss: 0.00002249
Iteration 297/1000 | Loss: 0.00016701
Iteration 298/1000 | Loss: 0.00011620
Iteration 299/1000 | Loss: 0.00002467
Iteration 300/1000 | Loss: 0.00002309
Iteration 301/1000 | Loss: 0.00002271
Iteration 302/1000 | Loss: 0.00002262
Iteration 303/1000 | Loss: 0.00002255
Iteration 304/1000 | Loss: 0.00002253
Iteration 305/1000 | Loss: 0.00002252
Iteration 306/1000 | Loss: 0.00002252
Iteration 307/1000 | Loss: 0.00002252
Iteration 308/1000 | Loss: 0.00002252
Iteration 309/1000 | Loss: 0.00002251
Iteration 310/1000 | Loss: 0.00002251
Iteration 311/1000 | Loss: 0.00002251
Iteration 312/1000 | Loss: 0.00002251
Iteration 313/1000 | Loss: 0.00002250
Iteration 314/1000 | Loss: 0.00002250
Iteration 315/1000 | Loss: 0.00002249
Iteration 316/1000 | Loss: 0.00002248
Iteration 317/1000 | Loss: 0.00002248
Iteration 318/1000 | Loss: 0.00002247
Iteration 319/1000 | Loss: 0.00002246
Iteration 320/1000 | Loss: 0.00002246
Iteration 321/1000 | Loss: 0.00002245
Iteration 322/1000 | Loss: 0.00002245
Iteration 323/1000 | Loss: 0.00002245
Iteration 324/1000 | Loss: 0.00002244
Iteration 325/1000 | Loss: 0.00002244
Iteration 326/1000 | Loss: 0.00002244
Iteration 327/1000 | Loss: 0.00002244
Iteration 328/1000 | Loss: 0.00002244
Iteration 329/1000 | Loss: 0.00002243
Iteration 330/1000 | Loss: 0.00002243
Iteration 331/1000 | Loss: 0.00002242
Iteration 332/1000 | Loss: 0.00018557
Iteration 333/1000 | Loss: 0.00005973
Iteration 334/1000 | Loss: 0.00002289
Iteration 335/1000 | Loss: 0.00002257
Iteration 336/1000 | Loss: 0.00002252
Iteration 337/1000 | Loss: 0.00002250
Iteration 338/1000 | Loss: 0.00002249
Iteration 339/1000 | Loss: 0.00002248
Iteration 340/1000 | Loss: 0.00002247
Iteration 341/1000 | Loss: 0.00002247
Iteration 342/1000 | Loss: 0.00002247
Iteration 343/1000 | Loss: 0.00002245
Iteration 344/1000 | Loss: 0.00002244
Iteration 345/1000 | Loss: 0.00002241
Iteration 346/1000 | Loss: 0.00002241
Iteration 347/1000 | Loss: 0.00002240
Iteration 348/1000 | Loss: 0.00002240
Iteration 349/1000 | Loss: 0.00002240
Iteration 350/1000 | Loss: 0.00002239
Iteration 351/1000 | Loss: 0.00002239
Iteration 352/1000 | Loss: 0.00002239
Iteration 353/1000 | Loss: 0.00018566
Iteration 354/1000 | Loss: 0.00006296
Iteration 355/1000 | Loss: 0.00002339
Iteration 356/1000 | Loss: 0.00002255
Iteration 357/1000 | Loss: 0.00002239
Iteration 358/1000 | Loss: 0.00002238
Iteration 359/1000 | Loss: 0.00002238
Iteration 360/1000 | Loss: 0.00002237
Iteration 361/1000 | Loss: 0.00002237
Iteration 362/1000 | Loss: 0.00002237
Iteration 363/1000 | Loss: 0.00002237
Iteration 364/1000 | Loss: 0.00002237
Iteration 365/1000 | Loss: 0.00002237
Iteration 366/1000 | Loss: 0.00002237
Iteration 367/1000 | Loss: 0.00002237
Iteration 368/1000 | Loss: 0.00002236
Iteration 369/1000 | Loss: 0.00002236
Iteration 370/1000 | Loss: 0.00002236
Iteration 371/1000 | Loss: 0.00002236
Iteration 372/1000 | Loss: 0.00002236
Iteration 373/1000 | Loss: 0.00002236
Iteration 374/1000 | Loss: 0.00002236
Iteration 375/1000 | Loss: 0.00002235
Iteration 376/1000 | Loss: 0.00002235
Iteration 377/1000 | Loss: 0.00002235
Iteration 378/1000 | Loss: 0.00002235
Iteration 379/1000 | Loss: 0.00002235
Iteration 380/1000 | Loss: 0.00002235
Iteration 381/1000 | Loss: 0.00002235
Iteration 382/1000 | Loss: 0.00002235
Iteration 383/1000 | Loss: 0.00002235
Iteration 384/1000 | Loss: 0.00002235
Iteration 385/1000 | Loss: 0.00002235
Iteration 386/1000 | Loss: 0.00002235
Iteration 387/1000 | Loss: 0.00002235
Iteration 388/1000 | Loss: 0.00002234
Iteration 389/1000 | Loss: 0.00002234
Iteration 390/1000 | Loss: 0.00002234
Iteration 391/1000 | Loss: 0.00002234
Iteration 392/1000 | Loss: 0.00002234
Iteration 393/1000 | Loss: 0.00002234
Iteration 394/1000 | Loss: 0.00002234
Iteration 395/1000 | Loss: 0.00002233
Iteration 396/1000 | Loss: 0.00002233
Iteration 397/1000 | Loss: 0.00002233
Iteration 398/1000 | Loss: 0.00002233
Iteration 399/1000 | Loss: 0.00002233
Iteration 400/1000 | Loss: 0.00002233
Iteration 401/1000 | Loss: 0.00002233
Iteration 402/1000 | Loss: 0.00002233
Iteration 403/1000 | Loss: 0.00002233
Iteration 404/1000 | Loss: 0.00002233
Iteration 405/1000 | Loss: 0.00002233
Iteration 406/1000 | Loss: 0.00002233
Iteration 407/1000 | Loss: 0.00002233
Iteration 408/1000 | Loss: 0.00002233
Iteration 409/1000 | Loss: 0.00002233
Iteration 410/1000 | Loss: 0.00002233
Iteration 411/1000 | Loss: 0.00002233
Iteration 412/1000 | Loss: 0.00002233
Iteration 413/1000 | Loss: 0.00002233
Iteration 414/1000 | Loss: 0.00002233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 414. Stopping optimization.
Last 5 losses: [2.232560837001074e-05, 2.232560837001074e-05, 2.232560837001074e-05, 2.232560837001074e-05, 2.232560837001074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.232560837001074e-05

Optimization complete. Final v2v error: 3.838362455368042 mm

Highest mean error: 7.94436502456665 mm for frame 115

Lowest mean error: 3.193849563598633 mm for frame 209

Saving results

Total time: 320.22695779800415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00953718
Iteration 2/25 | Loss: 0.00142200
Iteration 3/25 | Loss: 0.00134775
Iteration 4/25 | Loss: 0.00123276
Iteration 5/25 | Loss: 0.00122585
Iteration 6/25 | Loss: 0.00122383
Iteration 7/25 | Loss: 0.00122417
Iteration 8/25 | Loss: 0.00122362
Iteration 9/25 | Loss: 0.00122273
Iteration 10/25 | Loss: 0.00122187
Iteration 11/25 | Loss: 0.00122149
Iteration 12/25 | Loss: 0.00122130
Iteration 13/25 | Loss: 0.00122127
Iteration 14/25 | Loss: 0.00122127
Iteration 15/25 | Loss: 0.00122127
Iteration 16/25 | Loss: 0.00122127
Iteration 17/25 | Loss: 0.00122127
Iteration 18/25 | Loss: 0.00122127
Iteration 19/25 | Loss: 0.00122126
Iteration 20/25 | Loss: 0.00122126
Iteration 21/25 | Loss: 0.00122126
Iteration 22/25 | Loss: 0.00122126
Iteration 23/25 | Loss: 0.00122126
Iteration 24/25 | Loss: 0.00122126
Iteration 25/25 | Loss: 0.00122126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28267646
Iteration 2/25 | Loss: 0.00157928
Iteration 3/25 | Loss: 0.00157928
Iteration 4/25 | Loss: 0.00157928
Iteration 5/25 | Loss: 0.00157928
Iteration 6/25 | Loss: 0.00157928
Iteration 7/25 | Loss: 0.00157928
Iteration 8/25 | Loss: 0.00157927
Iteration 9/25 | Loss: 0.00157927
Iteration 10/25 | Loss: 0.00157927
Iteration 11/25 | Loss: 0.00157927
Iteration 12/25 | Loss: 0.00157927
Iteration 13/25 | Loss: 0.00157927
Iteration 14/25 | Loss: 0.00157927
Iteration 15/25 | Loss: 0.00157927
Iteration 16/25 | Loss: 0.00157927
Iteration 17/25 | Loss: 0.00157927
Iteration 18/25 | Loss: 0.00157927
Iteration 19/25 | Loss: 0.00157927
Iteration 20/25 | Loss: 0.00157927
Iteration 21/25 | Loss: 0.00157927
Iteration 22/25 | Loss: 0.00157927
Iteration 23/25 | Loss: 0.00157927
Iteration 24/25 | Loss: 0.00157927
Iteration 25/25 | Loss: 0.00157927

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157927
Iteration 2/1000 | Loss: 0.00002250
Iteration 3/1000 | Loss: 0.00001692
Iteration 4/1000 | Loss: 0.00001492
Iteration 5/1000 | Loss: 0.00001401
Iteration 6/1000 | Loss: 0.00001326
Iteration 7/1000 | Loss: 0.00001280
Iteration 8/1000 | Loss: 0.00001266
Iteration 9/1000 | Loss: 0.00001245
Iteration 10/1000 | Loss: 0.00001215
Iteration 11/1000 | Loss: 0.00001198
Iteration 12/1000 | Loss: 0.00001193
Iteration 13/1000 | Loss: 0.00001190
Iteration 14/1000 | Loss: 0.00001188
Iteration 15/1000 | Loss: 0.00001177
Iteration 16/1000 | Loss: 0.00001173
Iteration 17/1000 | Loss: 0.00001165
Iteration 18/1000 | Loss: 0.00001148
Iteration 19/1000 | Loss: 0.00001147
Iteration 20/1000 | Loss: 0.00001146
Iteration 21/1000 | Loss: 0.00001146
Iteration 22/1000 | Loss: 0.00001145
Iteration 23/1000 | Loss: 0.00001140
Iteration 24/1000 | Loss: 0.00001138
Iteration 25/1000 | Loss: 0.00001136
Iteration 26/1000 | Loss: 0.00001133
Iteration 27/1000 | Loss: 0.00001132
Iteration 28/1000 | Loss: 0.00001125
Iteration 29/1000 | Loss: 0.00001125
Iteration 30/1000 | Loss: 0.00001121
Iteration 31/1000 | Loss: 0.00001120
Iteration 32/1000 | Loss: 0.00001118
Iteration 33/1000 | Loss: 0.00001117
Iteration 34/1000 | Loss: 0.00001116
Iteration 35/1000 | Loss: 0.00001116
Iteration 36/1000 | Loss: 0.00001115
Iteration 37/1000 | Loss: 0.00001115
Iteration 38/1000 | Loss: 0.00001114
Iteration 39/1000 | Loss: 0.00001114
Iteration 40/1000 | Loss: 0.00001114
Iteration 41/1000 | Loss: 0.00001114
Iteration 42/1000 | Loss: 0.00001113
Iteration 43/1000 | Loss: 0.00001113
Iteration 44/1000 | Loss: 0.00001113
Iteration 45/1000 | Loss: 0.00001112
Iteration 46/1000 | Loss: 0.00001111
Iteration 47/1000 | Loss: 0.00001111
Iteration 48/1000 | Loss: 0.00001111
Iteration 49/1000 | Loss: 0.00001111
Iteration 50/1000 | Loss: 0.00001111
Iteration 51/1000 | Loss: 0.00001111
Iteration 52/1000 | Loss: 0.00001111
Iteration 53/1000 | Loss: 0.00001111
Iteration 54/1000 | Loss: 0.00001110
Iteration 55/1000 | Loss: 0.00001109
Iteration 56/1000 | Loss: 0.00001108
Iteration 57/1000 | Loss: 0.00001107
Iteration 58/1000 | Loss: 0.00001107
Iteration 59/1000 | Loss: 0.00001105
Iteration 60/1000 | Loss: 0.00001105
Iteration 61/1000 | Loss: 0.00001105
Iteration 62/1000 | Loss: 0.00001105
Iteration 63/1000 | Loss: 0.00001104
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001104
Iteration 66/1000 | Loss: 0.00001104
Iteration 67/1000 | Loss: 0.00001103
Iteration 68/1000 | Loss: 0.00001103
Iteration 69/1000 | Loss: 0.00001102
Iteration 70/1000 | Loss: 0.00001102
Iteration 71/1000 | Loss: 0.00001102
Iteration 72/1000 | Loss: 0.00001102
Iteration 73/1000 | Loss: 0.00001100
Iteration 74/1000 | Loss: 0.00001100
Iteration 75/1000 | Loss: 0.00001100
Iteration 76/1000 | Loss: 0.00001099
Iteration 77/1000 | Loss: 0.00001099
Iteration 78/1000 | Loss: 0.00001098
Iteration 79/1000 | Loss: 0.00001097
Iteration 80/1000 | Loss: 0.00001096
Iteration 81/1000 | Loss: 0.00001096
Iteration 82/1000 | Loss: 0.00001095
Iteration 83/1000 | Loss: 0.00001095
Iteration 84/1000 | Loss: 0.00001095
Iteration 85/1000 | Loss: 0.00001095
Iteration 86/1000 | Loss: 0.00001094
Iteration 87/1000 | Loss: 0.00001094
Iteration 88/1000 | Loss: 0.00001094
Iteration 89/1000 | Loss: 0.00001094
Iteration 90/1000 | Loss: 0.00001094
Iteration 91/1000 | Loss: 0.00001094
Iteration 92/1000 | Loss: 0.00001093
Iteration 93/1000 | Loss: 0.00001093
Iteration 94/1000 | Loss: 0.00001093
Iteration 95/1000 | Loss: 0.00001093
Iteration 96/1000 | Loss: 0.00001092
Iteration 97/1000 | Loss: 0.00001092
Iteration 98/1000 | Loss: 0.00001092
Iteration 99/1000 | Loss: 0.00001092
Iteration 100/1000 | Loss: 0.00001092
Iteration 101/1000 | Loss: 0.00001091
Iteration 102/1000 | Loss: 0.00001091
Iteration 103/1000 | Loss: 0.00001091
Iteration 104/1000 | Loss: 0.00001090
Iteration 105/1000 | Loss: 0.00001090
Iteration 106/1000 | Loss: 0.00001090
Iteration 107/1000 | Loss: 0.00001090
Iteration 108/1000 | Loss: 0.00001090
Iteration 109/1000 | Loss: 0.00001090
Iteration 110/1000 | Loss: 0.00001089
Iteration 111/1000 | Loss: 0.00001089
Iteration 112/1000 | Loss: 0.00001089
Iteration 113/1000 | Loss: 0.00001089
Iteration 114/1000 | Loss: 0.00001089
Iteration 115/1000 | Loss: 0.00001089
Iteration 116/1000 | Loss: 0.00001088
Iteration 117/1000 | Loss: 0.00001088
Iteration 118/1000 | Loss: 0.00001088
Iteration 119/1000 | Loss: 0.00001088
Iteration 120/1000 | Loss: 0.00001088
Iteration 121/1000 | Loss: 0.00001088
Iteration 122/1000 | Loss: 0.00001088
Iteration 123/1000 | Loss: 0.00001088
Iteration 124/1000 | Loss: 0.00001088
Iteration 125/1000 | Loss: 0.00001088
Iteration 126/1000 | Loss: 0.00001088
Iteration 127/1000 | Loss: 0.00001088
Iteration 128/1000 | Loss: 0.00001088
Iteration 129/1000 | Loss: 0.00001088
Iteration 130/1000 | Loss: 0.00001088
Iteration 131/1000 | Loss: 0.00001088
Iteration 132/1000 | Loss: 0.00001088
Iteration 133/1000 | Loss: 0.00001088
Iteration 134/1000 | Loss: 0.00001088
Iteration 135/1000 | Loss: 0.00001088
Iteration 136/1000 | Loss: 0.00001088
Iteration 137/1000 | Loss: 0.00001088
Iteration 138/1000 | Loss: 0.00001088
Iteration 139/1000 | Loss: 0.00001088
Iteration 140/1000 | Loss: 0.00001088
Iteration 141/1000 | Loss: 0.00001088
Iteration 142/1000 | Loss: 0.00001088
Iteration 143/1000 | Loss: 0.00001088
Iteration 144/1000 | Loss: 0.00001088
Iteration 145/1000 | Loss: 0.00001088
Iteration 146/1000 | Loss: 0.00001088
Iteration 147/1000 | Loss: 0.00001088
Iteration 148/1000 | Loss: 0.00001088
Iteration 149/1000 | Loss: 0.00001088
Iteration 150/1000 | Loss: 0.00001088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.0875305633817334e-05, 1.0875305633817334e-05, 1.0875305633817334e-05, 1.0875305633817334e-05, 1.0875305633817334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0875305633817334e-05

Optimization complete. Final v2v error: 2.8578829765319824 mm

Highest mean error: 3.136676549911499 mm for frame 112

Lowest mean error: 2.6822309494018555 mm for frame 165

Saving results

Total time: 60.188109397888184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00699127
Iteration 2/25 | Loss: 0.00183885
Iteration 3/25 | Loss: 0.00154241
Iteration 4/25 | Loss: 0.00147729
Iteration 5/25 | Loss: 0.00138613
Iteration 6/25 | Loss: 0.00136320
Iteration 7/25 | Loss: 0.00136711
Iteration 8/25 | Loss: 0.00134474
Iteration 9/25 | Loss: 0.00133800
Iteration 10/25 | Loss: 0.00132061
Iteration 11/25 | Loss: 0.00132451
Iteration 12/25 | Loss: 0.00131623
Iteration 13/25 | Loss: 0.00131488
Iteration 14/25 | Loss: 0.00131371
Iteration 15/25 | Loss: 0.00131265
Iteration 16/25 | Loss: 0.00131190
Iteration 17/25 | Loss: 0.00131149
Iteration 18/25 | Loss: 0.00131126
Iteration 19/25 | Loss: 0.00131105
Iteration 20/25 | Loss: 0.00131088
Iteration 21/25 | Loss: 0.00131077
Iteration 22/25 | Loss: 0.00131070
Iteration 23/25 | Loss: 0.00131070
Iteration 24/25 | Loss: 0.00131069
Iteration 25/25 | Loss: 0.00131069

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.61540937
Iteration 2/25 | Loss: 0.00401124
Iteration 3/25 | Loss: 0.00257768
Iteration 4/25 | Loss: 0.00257768
Iteration 5/25 | Loss: 0.00257768
Iteration 6/25 | Loss: 0.00257768
Iteration 7/25 | Loss: 0.00257768
Iteration 8/25 | Loss: 0.00257768
Iteration 9/25 | Loss: 0.00257768
Iteration 10/25 | Loss: 0.00257768
Iteration 11/25 | Loss: 0.00257768
Iteration 12/25 | Loss: 0.00257768
Iteration 13/25 | Loss: 0.00257768
Iteration 14/25 | Loss: 0.00257768
Iteration 15/25 | Loss: 0.00257768
Iteration 16/25 | Loss: 0.00257768
Iteration 17/25 | Loss: 0.00257768
Iteration 18/25 | Loss: 0.00257768
Iteration 19/25 | Loss: 0.00257768
Iteration 20/25 | Loss: 0.00257768
Iteration 21/25 | Loss: 0.00257768
Iteration 22/25 | Loss: 0.00257768
Iteration 23/25 | Loss: 0.00257768
Iteration 24/25 | Loss: 0.00257768
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0025776750408113003, 0.0025776750408113003, 0.0025776750408113003, 0.0025776750408113003, 0.0025776750408113003]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025776750408113003

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00257768
Iteration 2/1000 | Loss: 0.00031621
Iteration 3/1000 | Loss: 0.00104393
Iteration 4/1000 | Loss: 0.00101158
Iteration 5/1000 | Loss: 0.00248282
Iteration 6/1000 | Loss: 0.00158838
Iteration 7/1000 | Loss: 0.00234038
Iteration 8/1000 | Loss: 0.00274186
Iteration 9/1000 | Loss: 0.00054947
Iteration 10/1000 | Loss: 0.00422002
Iteration 11/1000 | Loss: 0.00383991
Iteration 12/1000 | Loss: 0.00580557
Iteration 13/1000 | Loss: 0.00857034
Iteration 14/1000 | Loss: 0.00453579
Iteration 15/1000 | Loss: 0.00306516
Iteration 16/1000 | Loss: 0.00429190
Iteration 17/1000 | Loss: 0.00225324
Iteration 18/1000 | Loss: 0.00190501
Iteration 19/1000 | Loss: 0.00117486
Iteration 20/1000 | Loss: 0.00074574
Iteration 21/1000 | Loss: 0.00182991
Iteration 22/1000 | Loss: 0.00070816
Iteration 23/1000 | Loss: 0.00025142
Iteration 24/1000 | Loss: 0.00007090
Iteration 25/1000 | Loss: 0.00060075
Iteration 26/1000 | Loss: 0.00004448
Iteration 27/1000 | Loss: 0.00060447
Iteration 28/1000 | Loss: 0.00005770
Iteration 29/1000 | Loss: 0.00041484
Iteration 30/1000 | Loss: 0.00003379
Iteration 31/1000 | Loss: 0.00017186
Iteration 32/1000 | Loss: 0.00002796
Iteration 33/1000 | Loss: 0.00035968
Iteration 34/1000 | Loss: 0.00002547
Iteration 35/1000 | Loss: 0.00002385
Iteration 36/1000 | Loss: 0.00018662
Iteration 37/1000 | Loss: 0.00018542
Iteration 38/1000 | Loss: 0.00025208
Iteration 39/1000 | Loss: 0.00004045
Iteration 40/1000 | Loss: 0.00002950
Iteration 41/1000 | Loss: 0.00013853
Iteration 42/1000 | Loss: 0.00002175
Iteration 43/1000 | Loss: 0.00002091
Iteration 44/1000 | Loss: 0.00002022
Iteration 45/1000 | Loss: 0.00030475
Iteration 46/1000 | Loss: 0.00003548
Iteration 47/1000 | Loss: 0.00006254
Iteration 48/1000 | Loss: 0.00001920
Iteration 49/1000 | Loss: 0.00001856
Iteration 50/1000 | Loss: 0.00001807
Iteration 51/1000 | Loss: 0.00001771
Iteration 52/1000 | Loss: 0.00001748
Iteration 53/1000 | Loss: 0.00001733
Iteration 54/1000 | Loss: 0.00001733
Iteration 55/1000 | Loss: 0.00001728
Iteration 56/1000 | Loss: 0.00001726
Iteration 57/1000 | Loss: 0.00001726
Iteration 58/1000 | Loss: 0.00001725
Iteration 59/1000 | Loss: 0.00001716
Iteration 60/1000 | Loss: 0.00001712
Iteration 61/1000 | Loss: 0.00001711
Iteration 62/1000 | Loss: 0.00001711
Iteration 63/1000 | Loss: 0.00001710
Iteration 64/1000 | Loss: 0.00001710
Iteration 65/1000 | Loss: 0.00001709
Iteration 66/1000 | Loss: 0.00001707
Iteration 67/1000 | Loss: 0.00001707
Iteration 68/1000 | Loss: 0.00001706
Iteration 69/1000 | Loss: 0.00001706
Iteration 70/1000 | Loss: 0.00001706
Iteration 71/1000 | Loss: 0.00001706
Iteration 72/1000 | Loss: 0.00001706
Iteration 73/1000 | Loss: 0.00001705
Iteration 74/1000 | Loss: 0.00001704
Iteration 75/1000 | Loss: 0.00001704
Iteration 76/1000 | Loss: 0.00001703
Iteration 77/1000 | Loss: 0.00001703
Iteration 78/1000 | Loss: 0.00001703
Iteration 79/1000 | Loss: 0.00001702
Iteration 80/1000 | Loss: 0.00001701
Iteration 81/1000 | Loss: 0.00001700
Iteration 82/1000 | Loss: 0.00001700
Iteration 83/1000 | Loss: 0.00001700
Iteration 84/1000 | Loss: 0.00001699
Iteration 85/1000 | Loss: 0.00001699
Iteration 86/1000 | Loss: 0.00001699
Iteration 87/1000 | Loss: 0.00001699
Iteration 88/1000 | Loss: 0.00001698
Iteration 89/1000 | Loss: 0.00001698
Iteration 90/1000 | Loss: 0.00001697
Iteration 91/1000 | Loss: 0.00001696
Iteration 92/1000 | Loss: 0.00001696
Iteration 93/1000 | Loss: 0.00001696
Iteration 94/1000 | Loss: 0.00001696
Iteration 95/1000 | Loss: 0.00001696
Iteration 96/1000 | Loss: 0.00001696
Iteration 97/1000 | Loss: 0.00001696
Iteration 98/1000 | Loss: 0.00001695
Iteration 99/1000 | Loss: 0.00001695
Iteration 100/1000 | Loss: 0.00001695
Iteration 101/1000 | Loss: 0.00001695
Iteration 102/1000 | Loss: 0.00001695
Iteration 103/1000 | Loss: 0.00001694
Iteration 104/1000 | Loss: 0.00001692
Iteration 105/1000 | Loss: 0.00001691
Iteration 106/1000 | Loss: 0.00001690
Iteration 107/1000 | Loss: 0.00001689
Iteration 108/1000 | Loss: 0.00001689
Iteration 109/1000 | Loss: 0.00001688
Iteration 110/1000 | Loss: 0.00001688
Iteration 111/1000 | Loss: 0.00001688
Iteration 112/1000 | Loss: 0.00001688
Iteration 113/1000 | Loss: 0.00001688
Iteration 114/1000 | Loss: 0.00001687
Iteration 115/1000 | Loss: 0.00001687
Iteration 116/1000 | Loss: 0.00001687
Iteration 117/1000 | Loss: 0.00001687
Iteration 118/1000 | Loss: 0.00001686
Iteration 119/1000 | Loss: 0.00001686
Iteration 120/1000 | Loss: 0.00001686
Iteration 121/1000 | Loss: 0.00001685
Iteration 122/1000 | Loss: 0.00001685
Iteration 123/1000 | Loss: 0.00001685
Iteration 124/1000 | Loss: 0.00001685
Iteration 125/1000 | Loss: 0.00001685
Iteration 126/1000 | Loss: 0.00001685
Iteration 127/1000 | Loss: 0.00001684
Iteration 128/1000 | Loss: 0.00001684
Iteration 129/1000 | Loss: 0.00001684
Iteration 130/1000 | Loss: 0.00001684
Iteration 131/1000 | Loss: 0.00001684
Iteration 132/1000 | Loss: 0.00001684
Iteration 133/1000 | Loss: 0.00001684
Iteration 134/1000 | Loss: 0.00001684
Iteration 135/1000 | Loss: 0.00001684
Iteration 136/1000 | Loss: 0.00001684
Iteration 137/1000 | Loss: 0.00001684
Iteration 138/1000 | Loss: 0.00001683
Iteration 139/1000 | Loss: 0.00001683
Iteration 140/1000 | Loss: 0.00001682
Iteration 141/1000 | Loss: 0.00001682
Iteration 142/1000 | Loss: 0.00001682
Iteration 143/1000 | Loss: 0.00001682
Iteration 144/1000 | Loss: 0.00001682
Iteration 145/1000 | Loss: 0.00001682
Iteration 146/1000 | Loss: 0.00001682
Iteration 147/1000 | Loss: 0.00001682
Iteration 148/1000 | Loss: 0.00001682
Iteration 149/1000 | Loss: 0.00001682
Iteration 150/1000 | Loss: 0.00001681
Iteration 151/1000 | Loss: 0.00001681
Iteration 152/1000 | Loss: 0.00001681
Iteration 153/1000 | Loss: 0.00001681
Iteration 154/1000 | Loss: 0.00001681
Iteration 155/1000 | Loss: 0.00001681
Iteration 156/1000 | Loss: 0.00001680
Iteration 157/1000 | Loss: 0.00001680
Iteration 158/1000 | Loss: 0.00001680
Iteration 159/1000 | Loss: 0.00001680
Iteration 160/1000 | Loss: 0.00001679
Iteration 161/1000 | Loss: 0.00001679
Iteration 162/1000 | Loss: 0.00001679
Iteration 163/1000 | Loss: 0.00001678
Iteration 164/1000 | Loss: 0.00001678
Iteration 165/1000 | Loss: 0.00001678
Iteration 166/1000 | Loss: 0.00001678
Iteration 167/1000 | Loss: 0.00001677
Iteration 168/1000 | Loss: 0.00001676
Iteration 169/1000 | Loss: 0.00001676
Iteration 170/1000 | Loss: 0.00001676
Iteration 171/1000 | Loss: 0.00001676
Iteration 172/1000 | Loss: 0.00001676
Iteration 173/1000 | Loss: 0.00001676
Iteration 174/1000 | Loss: 0.00001675
Iteration 175/1000 | Loss: 0.00001675
Iteration 176/1000 | Loss: 0.00001675
Iteration 177/1000 | Loss: 0.00001675
Iteration 178/1000 | Loss: 0.00001675
Iteration 179/1000 | Loss: 0.00001675
Iteration 180/1000 | Loss: 0.00001675
Iteration 181/1000 | Loss: 0.00001675
Iteration 182/1000 | Loss: 0.00001675
Iteration 183/1000 | Loss: 0.00001675
Iteration 184/1000 | Loss: 0.00001674
Iteration 185/1000 | Loss: 0.00001674
Iteration 186/1000 | Loss: 0.00001674
Iteration 187/1000 | Loss: 0.00001674
Iteration 188/1000 | Loss: 0.00001674
Iteration 189/1000 | Loss: 0.00001674
Iteration 190/1000 | Loss: 0.00001674
Iteration 191/1000 | Loss: 0.00001674
Iteration 192/1000 | Loss: 0.00001674
Iteration 193/1000 | Loss: 0.00001674
Iteration 194/1000 | Loss: 0.00001674
Iteration 195/1000 | Loss: 0.00001674
Iteration 196/1000 | Loss: 0.00001674
Iteration 197/1000 | Loss: 0.00001674
Iteration 198/1000 | Loss: 0.00001674
Iteration 199/1000 | Loss: 0.00001674
Iteration 200/1000 | Loss: 0.00001674
Iteration 201/1000 | Loss: 0.00001674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.6740195860620588e-05, 1.6740195860620588e-05, 1.6740195860620588e-05, 1.6740195860620588e-05, 1.6740195860620588e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6740195860620588e-05

Optimization complete. Final v2v error: 3.125718593597412 mm

Highest mean error: 11.781967163085938 mm for frame 2

Lowest mean error: 2.790691375732422 mm for frame 78

Saving results

Total time: 141.30390667915344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057913
Iteration 2/25 | Loss: 0.01057913
Iteration 3/25 | Loss: 0.01057913
Iteration 4/25 | Loss: 0.01057912
Iteration 5/25 | Loss: 0.01057912
Iteration 6/25 | Loss: 0.01057912
Iteration 7/25 | Loss: 0.01057912
Iteration 8/25 | Loss: 0.01057911
Iteration 9/25 | Loss: 0.01057911
Iteration 10/25 | Loss: 0.01057911
Iteration 11/25 | Loss: 0.01057911
Iteration 12/25 | Loss: 0.01057910
Iteration 13/25 | Loss: 0.01057910
Iteration 14/25 | Loss: 0.01057910
Iteration 15/25 | Loss: 0.01057910
Iteration 16/25 | Loss: 0.01057910
Iteration 17/25 | Loss: 0.01057909
Iteration 18/25 | Loss: 0.01057909
Iteration 19/25 | Loss: 0.01057909
Iteration 20/25 | Loss: 0.01057909
Iteration 21/25 | Loss: 0.01057909
Iteration 22/25 | Loss: 0.01057909
Iteration 23/25 | Loss: 0.01057908
Iteration 24/25 | Loss: 0.01057908
Iteration 25/25 | Loss: 0.01057908

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.04451561
Iteration 2/25 | Loss: 0.14574990
Iteration 3/25 | Loss: 0.14574973
Iteration 4/25 | Loss: 0.14574969
Iteration 5/25 | Loss: 0.14574969
Iteration 6/25 | Loss: 0.14574969
Iteration 7/25 | Loss: 0.14574967
Iteration 8/25 | Loss: 0.14574967
Iteration 9/25 | Loss: 0.14574966
Iteration 10/25 | Loss: 0.14574966
Iteration 11/25 | Loss: 0.14574966
Iteration 12/25 | Loss: 0.14574966
Iteration 13/25 | Loss: 0.14574966
Iteration 14/25 | Loss: 0.14574966
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.14574965834617615, 0.14574965834617615, 0.14574965834617615, 0.14574965834617615, 0.14574965834617615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.14574965834617615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.14574966
Iteration 2/1000 | Loss: 0.00462074
Iteration 3/1000 | Loss: 0.00224240
Iteration 4/1000 | Loss: 0.00197695
Iteration 5/1000 | Loss: 0.00120845
Iteration 6/1000 | Loss: 0.00018328
Iteration 7/1000 | Loss: 0.00018259
Iteration 8/1000 | Loss: 0.00004795
Iteration 9/1000 | Loss: 0.00021085
Iteration 10/1000 | Loss: 0.00003135
Iteration 11/1000 | Loss: 0.00028573
Iteration 12/1000 | Loss: 0.00009839
Iteration 13/1000 | Loss: 0.00002325
Iteration 14/1000 | Loss: 0.00008839
Iteration 15/1000 | Loss: 0.00005931
Iteration 16/1000 | Loss: 0.00003012
Iteration 17/1000 | Loss: 0.00009655
Iteration 18/1000 | Loss: 0.00029964
Iteration 19/1000 | Loss: 0.00024510
Iteration 20/1000 | Loss: 0.00007810
Iteration 21/1000 | Loss: 0.00001939
Iteration 22/1000 | Loss: 0.00001794
Iteration 23/1000 | Loss: 0.00006094
Iteration 24/1000 | Loss: 0.00003717
Iteration 25/1000 | Loss: 0.00001657
Iteration 26/1000 | Loss: 0.00001728
Iteration 27/1000 | Loss: 0.00009087
Iteration 28/1000 | Loss: 0.00001597
Iteration 29/1000 | Loss: 0.00006021
Iteration 30/1000 | Loss: 0.00001527
Iteration 31/1000 | Loss: 0.00001494
Iteration 32/1000 | Loss: 0.00012293
Iteration 33/1000 | Loss: 0.00006834
Iteration 34/1000 | Loss: 0.00004422
Iteration 35/1000 | Loss: 0.00001974
Iteration 36/1000 | Loss: 0.00002385
Iteration 37/1000 | Loss: 0.00001559
Iteration 38/1000 | Loss: 0.00001428
Iteration 39/1000 | Loss: 0.00012638
Iteration 40/1000 | Loss: 0.00002697
Iteration 41/1000 | Loss: 0.00001508
Iteration 42/1000 | Loss: 0.00002680
Iteration 43/1000 | Loss: 0.00001558
Iteration 44/1000 | Loss: 0.00001369
Iteration 45/1000 | Loss: 0.00001369
Iteration 46/1000 | Loss: 0.00001369
Iteration 47/1000 | Loss: 0.00001369
Iteration 48/1000 | Loss: 0.00001369
Iteration 49/1000 | Loss: 0.00001369
Iteration 50/1000 | Loss: 0.00001369
Iteration 51/1000 | Loss: 0.00001369
Iteration 52/1000 | Loss: 0.00001368
Iteration 53/1000 | Loss: 0.00001368
Iteration 54/1000 | Loss: 0.00001368
Iteration 55/1000 | Loss: 0.00001368
Iteration 56/1000 | Loss: 0.00001367
Iteration 57/1000 | Loss: 0.00001367
Iteration 58/1000 | Loss: 0.00001366
Iteration 59/1000 | Loss: 0.00001359
Iteration 60/1000 | Loss: 0.00001359
Iteration 61/1000 | Loss: 0.00001358
Iteration 62/1000 | Loss: 0.00006093
Iteration 63/1000 | Loss: 0.00001362
Iteration 64/1000 | Loss: 0.00001356
Iteration 65/1000 | Loss: 0.00001356
Iteration 66/1000 | Loss: 0.00001355
Iteration 67/1000 | Loss: 0.00001355
Iteration 68/1000 | Loss: 0.00001355
Iteration 69/1000 | Loss: 0.00001355
Iteration 70/1000 | Loss: 0.00001355
Iteration 71/1000 | Loss: 0.00001355
Iteration 72/1000 | Loss: 0.00001355
Iteration 73/1000 | Loss: 0.00001682
Iteration 74/1000 | Loss: 0.00006227
Iteration 75/1000 | Loss: 0.00001962
Iteration 76/1000 | Loss: 0.00002421
Iteration 77/1000 | Loss: 0.00001352
Iteration 78/1000 | Loss: 0.00001352
Iteration 79/1000 | Loss: 0.00001351
Iteration 80/1000 | Loss: 0.00001348
Iteration 81/1000 | Loss: 0.00001345
Iteration 82/1000 | Loss: 0.00001345
Iteration 83/1000 | Loss: 0.00002266
Iteration 84/1000 | Loss: 0.00001337
Iteration 85/1000 | Loss: 0.00001337
Iteration 86/1000 | Loss: 0.00001336
Iteration 87/1000 | Loss: 0.00001336
Iteration 88/1000 | Loss: 0.00001336
Iteration 89/1000 | Loss: 0.00001336
Iteration 90/1000 | Loss: 0.00001333
Iteration 91/1000 | Loss: 0.00001339
Iteration 92/1000 | Loss: 0.00001331
Iteration 93/1000 | Loss: 0.00001331
Iteration 94/1000 | Loss: 0.00001331
Iteration 95/1000 | Loss: 0.00001331
Iteration 96/1000 | Loss: 0.00001331
Iteration 97/1000 | Loss: 0.00001331
Iteration 98/1000 | Loss: 0.00001330
Iteration 99/1000 | Loss: 0.00001330
Iteration 100/1000 | Loss: 0.00001330
Iteration 101/1000 | Loss: 0.00001330
Iteration 102/1000 | Loss: 0.00001330
Iteration 103/1000 | Loss: 0.00001329
Iteration 104/1000 | Loss: 0.00001329
Iteration 105/1000 | Loss: 0.00001329
Iteration 106/1000 | Loss: 0.00001328
Iteration 107/1000 | Loss: 0.00001328
Iteration 108/1000 | Loss: 0.00001328
Iteration 109/1000 | Loss: 0.00001328
Iteration 110/1000 | Loss: 0.00001328
Iteration 111/1000 | Loss: 0.00001327
Iteration 112/1000 | Loss: 0.00001327
Iteration 113/1000 | Loss: 0.00001327
Iteration 114/1000 | Loss: 0.00001326
Iteration 115/1000 | Loss: 0.00007455
Iteration 116/1000 | Loss: 0.00005357
Iteration 117/1000 | Loss: 0.00001375
Iteration 118/1000 | Loss: 0.00001680
Iteration 119/1000 | Loss: 0.00001322
Iteration 120/1000 | Loss: 0.00001322
Iteration 121/1000 | Loss: 0.00001322
Iteration 122/1000 | Loss: 0.00001321
Iteration 123/1000 | Loss: 0.00001321
Iteration 124/1000 | Loss: 0.00001321
Iteration 125/1000 | Loss: 0.00001321
Iteration 126/1000 | Loss: 0.00001321
Iteration 127/1000 | Loss: 0.00001321
Iteration 128/1000 | Loss: 0.00001321
Iteration 129/1000 | Loss: 0.00001321
Iteration 130/1000 | Loss: 0.00001321
Iteration 131/1000 | Loss: 0.00001320
Iteration 132/1000 | Loss: 0.00001320
Iteration 133/1000 | Loss: 0.00001320
Iteration 134/1000 | Loss: 0.00001320
Iteration 135/1000 | Loss: 0.00001320
Iteration 136/1000 | Loss: 0.00001320
Iteration 137/1000 | Loss: 0.00001320
Iteration 138/1000 | Loss: 0.00001320
Iteration 139/1000 | Loss: 0.00001319
Iteration 140/1000 | Loss: 0.00001319
Iteration 141/1000 | Loss: 0.00001319
Iteration 142/1000 | Loss: 0.00001319
Iteration 143/1000 | Loss: 0.00001319
Iteration 144/1000 | Loss: 0.00001319
Iteration 145/1000 | Loss: 0.00001319
Iteration 146/1000 | Loss: 0.00001319
Iteration 147/1000 | Loss: 0.00001319
Iteration 148/1000 | Loss: 0.00001318
Iteration 149/1000 | Loss: 0.00001318
Iteration 150/1000 | Loss: 0.00001318
Iteration 151/1000 | Loss: 0.00001318
Iteration 152/1000 | Loss: 0.00001318
Iteration 153/1000 | Loss: 0.00001317
Iteration 154/1000 | Loss: 0.00001317
Iteration 155/1000 | Loss: 0.00001317
Iteration 156/1000 | Loss: 0.00001317
Iteration 157/1000 | Loss: 0.00001317
Iteration 158/1000 | Loss: 0.00001317
Iteration 159/1000 | Loss: 0.00001317
Iteration 160/1000 | Loss: 0.00001317
Iteration 161/1000 | Loss: 0.00001317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.3172818398743402e-05, 1.3172818398743402e-05, 1.3172818398743402e-05, 1.3172818398743402e-05, 1.3172818398743402e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3172818398743402e-05

Optimization complete. Final v2v error: 3.118587017059326 mm

Highest mean error: 3.738945722579956 mm for frame 176

Lowest mean error: 2.786766767501831 mm for frame 37

Saving results

Total time: 105.81127119064331
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800933
Iteration 2/25 | Loss: 0.00149361
Iteration 3/25 | Loss: 0.00126595
Iteration 4/25 | Loss: 0.00123818
Iteration 5/25 | Loss: 0.00123278
Iteration 6/25 | Loss: 0.00124095
Iteration 7/25 | Loss: 0.00124050
Iteration 8/25 | Loss: 0.00123223
Iteration 9/25 | Loss: 0.00122414
Iteration 10/25 | Loss: 0.00122251
Iteration 11/25 | Loss: 0.00122187
Iteration 12/25 | Loss: 0.00122203
Iteration 13/25 | Loss: 0.00122086
Iteration 14/25 | Loss: 0.00122019
Iteration 15/25 | Loss: 0.00122000
Iteration 16/25 | Loss: 0.00121990
Iteration 17/25 | Loss: 0.00121989
Iteration 18/25 | Loss: 0.00121989
Iteration 19/25 | Loss: 0.00121989
Iteration 20/25 | Loss: 0.00121989
Iteration 21/25 | Loss: 0.00121989
Iteration 22/25 | Loss: 0.00121989
Iteration 23/25 | Loss: 0.00121989
Iteration 24/25 | Loss: 0.00121989
Iteration 25/25 | Loss: 0.00121989

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42872369
Iteration 2/25 | Loss: 0.00138678
Iteration 3/25 | Loss: 0.00137309
Iteration 4/25 | Loss: 0.00137309
Iteration 5/25 | Loss: 0.00137309
Iteration 6/25 | Loss: 0.00137309
Iteration 7/25 | Loss: 0.00137308
Iteration 8/25 | Loss: 0.00137308
Iteration 9/25 | Loss: 0.00137308
Iteration 10/25 | Loss: 0.00137308
Iteration 11/25 | Loss: 0.00137308
Iteration 12/25 | Loss: 0.00137308
Iteration 13/25 | Loss: 0.00137308
Iteration 14/25 | Loss: 0.00137308
Iteration 15/25 | Loss: 0.00137308
Iteration 16/25 | Loss: 0.00137308
Iteration 17/25 | Loss: 0.00137308
Iteration 18/25 | Loss: 0.00137308
Iteration 19/25 | Loss: 0.00137308
Iteration 20/25 | Loss: 0.00137308
Iteration 21/25 | Loss: 0.00137308
Iteration 22/25 | Loss: 0.00137308
Iteration 23/25 | Loss: 0.00137308
Iteration 24/25 | Loss: 0.00137308
Iteration 25/25 | Loss: 0.00137308

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137308
Iteration 2/1000 | Loss: 0.00003055
Iteration 3/1000 | Loss: 0.00002028
Iteration 4/1000 | Loss: 0.00001884
Iteration 5/1000 | Loss: 0.00001774
Iteration 6/1000 | Loss: 0.00001708
Iteration 7/1000 | Loss: 0.00011729
Iteration 8/1000 | Loss: 0.00001797
Iteration 9/1000 | Loss: 0.00001594
Iteration 10/1000 | Loss: 0.00001507
Iteration 11/1000 | Loss: 0.00001446
Iteration 12/1000 | Loss: 0.00001414
Iteration 13/1000 | Loss: 0.00001384
Iteration 14/1000 | Loss: 0.00001379
Iteration 15/1000 | Loss: 0.00001361
Iteration 16/1000 | Loss: 0.00001332
Iteration 17/1000 | Loss: 0.00001327
Iteration 18/1000 | Loss: 0.00001326
Iteration 19/1000 | Loss: 0.00001325
Iteration 20/1000 | Loss: 0.00001316
Iteration 21/1000 | Loss: 0.00001311
Iteration 22/1000 | Loss: 0.00001311
Iteration 23/1000 | Loss: 0.00001310
Iteration 24/1000 | Loss: 0.00001309
Iteration 25/1000 | Loss: 0.00001309
Iteration 26/1000 | Loss: 0.00001304
Iteration 27/1000 | Loss: 0.00001304
Iteration 28/1000 | Loss: 0.00001302
Iteration 29/1000 | Loss: 0.00001302
Iteration 30/1000 | Loss: 0.00001302
Iteration 31/1000 | Loss: 0.00001302
Iteration 32/1000 | Loss: 0.00001301
Iteration 33/1000 | Loss: 0.00001301
Iteration 34/1000 | Loss: 0.00001300
Iteration 35/1000 | Loss: 0.00001300
Iteration 36/1000 | Loss: 0.00001299
Iteration 37/1000 | Loss: 0.00001298
Iteration 38/1000 | Loss: 0.00001297
Iteration 39/1000 | Loss: 0.00001297
Iteration 40/1000 | Loss: 0.00001296
Iteration 41/1000 | Loss: 0.00001296
Iteration 42/1000 | Loss: 0.00001296
Iteration 43/1000 | Loss: 0.00001295
Iteration 44/1000 | Loss: 0.00001295
Iteration 45/1000 | Loss: 0.00001295
Iteration 46/1000 | Loss: 0.00001295
Iteration 47/1000 | Loss: 0.00001295
Iteration 48/1000 | Loss: 0.00001294
Iteration 49/1000 | Loss: 0.00001294
Iteration 50/1000 | Loss: 0.00001294
Iteration 51/1000 | Loss: 0.00001294
Iteration 52/1000 | Loss: 0.00001294
Iteration 53/1000 | Loss: 0.00001293
Iteration 54/1000 | Loss: 0.00001293
Iteration 55/1000 | Loss: 0.00001293
Iteration 56/1000 | Loss: 0.00001292
Iteration 57/1000 | Loss: 0.00001292
Iteration 58/1000 | Loss: 0.00001292
Iteration 59/1000 | Loss: 0.00001291
Iteration 60/1000 | Loss: 0.00001291
Iteration 61/1000 | Loss: 0.00001291
Iteration 62/1000 | Loss: 0.00001290
Iteration 63/1000 | Loss: 0.00001290
Iteration 64/1000 | Loss: 0.00001290
Iteration 65/1000 | Loss: 0.00001290
Iteration 66/1000 | Loss: 0.00001289
Iteration 67/1000 | Loss: 0.00001289
Iteration 68/1000 | Loss: 0.00001289
Iteration 69/1000 | Loss: 0.00001289
Iteration 70/1000 | Loss: 0.00001288
Iteration 71/1000 | Loss: 0.00001288
Iteration 72/1000 | Loss: 0.00001288
Iteration 73/1000 | Loss: 0.00001288
Iteration 74/1000 | Loss: 0.00001287
Iteration 75/1000 | Loss: 0.00001287
Iteration 76/1000 | Loss: 0.00001287
Iteration 77/1000 | Loss: 0.00001287
Iteration 78/1000 | Loss: 0.00001287
Iteration 79/1000 | Loss: 0.00001287
Iteration 80/1000 | Loss: 0.00001287
Iteration 81/1000 | Loss: 0.00001287
Iteration 82/1000 | Loss: 0.00001287
Iteration 83/1000 | Loss: 0.00001287
Iteration 84/1000 | Loss: 0.00001286
Iteration 85/1000 | Loss: 0.00001286
Iteration 86/1000 | Loss: 0.00001286
Iteration 87/1000 | Loss: 0.00001286
Iteration 88/1000 | Loss: 0.00001286
Iteration 89/1000 | Loss: 0.00001286
Iteration 90/1000 | Loss: 0.00001286
Iteration 91/1000 | Loss: 0.00001285
Iteration 92/1000 | Loss: 0.00001285
Iteration 93/1000 | Loss: 0.00001285
Iteration 94/1000 | Loss: 0.00001285
Iteration 95/1000 | Loss: 0.00001285
Iteration 96/1000 | Loss: 0.00001285
Iteration 97/1000 | Loss: 0.00001285
Iteration 98/1000 | Loss: 0.00001285
Iteration 99/1000 | Loss: 0.00001285
Iteration 100/1000 | Loss: 0.00001285
Iteration 101/1000 | Loss: 0.00001284
Iteration 102/1000 | Loss: 0.00001284
Iteration 103/1000 | Loss: 0.00001284
Iteration 104/1000 | Loss: 0.00001284
Iteration 105/1000 | Loss: 0.00001284
Iteration 106/1000 | Loss: 0.00001284
Iteration 107/1000 | Loss: 0.00001284
Iteration 108/1000 | Loss: 0.00001284
Iteration 109/1000 | Loss: 0.00001284
Iteration 110/1000 | Loss: 0.00001283
Iteration 111/1000 | Loss: 0.00001283
Iteration 112/1000 | Loss: 0.00001283
Iteration 113/1000 | Loss: 0.00001282
Iteration 114/1000 | Loss: 0.00001282
Iteration 115/1000 | Loss: 0.00001282
Iteration 116/1000 | Loss: 0.00001282
Iteration 117/1000 | Loss: 0.00001282
Iteration 118/1000 | Loss: 0.00001281
Iteration 119/1000 | Loss: 0.00001281
Iteration 120/1000 | Loss: 0.00001280
Iteration 121/1000 | Loss: 0.00001280
Iteration 122/1000 | Loss: 0.00001280
Iteration 123/1000 | Loss: 0.00001279
Iteration 124/1000 | Loss: 0.00001279
Iteration 125/1000 | Loss: 0.00001279
Iteration 126/1000 | Loss: 0.00001279
Iteration 127/1000 | Loss: 0.00001279
Iteration 128/1000 | Loss: 0.00001278
Iteration 129/1000 | Loss: 0.00001278
Iteration 130/1000 | Loss: 0.00001278
Iteration 131/1000 | Loss: 0.00001277
Iteration 132/1000 | Loss: 0.00001276
Iteration 133/1000 | Loss: 0.00001276
Iteration 134/1000 | Loss: 0.00001276
Iteration 135/1000 | Loss: 0.00001276
Iteration 136/1000 | Loss: 0.00001276
Iteration 137/1000 | Loss: 0.00001276
Iteration 138/1000 | Loss: 0.00001276
Iteration 139/1000 | Loss: 0.00001276
Iteration 140/1000 | Loss: 0.00001276
Iteration 141/1000 | Loss: 0.00001276
Iteration 142/1000 | Loss: 0.00001276
Iteration 143/1000 | Loss: 0.00001276
Iteration 144/1000 | Loss: 0.00001276
Iteration 145/1000 | Loss: 0.00001275
Iteration 146/1000 | Loss: 0.00001275
Iteration 147/1000 | Loss: 0.00001275
Iteration 148/1000 | Loss: 0.00001275
Iteration 149/1000 | Loss: 0.00001275
Iteration 150/1000 | Loss: 0.00001275
Iteration 151/1000 | Loss: 0.00001275
Iteration 152/1000 | Loss: 0.00001275
Iteration 153/1000 | Loss: 0.00001275
Iteration 154/1000 | Loss: 0.00001275
Iteration 155/1000 | Loss: 0.00001275
Iteration 156/1000 | Loss: 0.00001275
Iteration 157/1000 | Loss: 0.00001274
Iteration 158/1000 | Loss: 0.00001274
Iteration 159/1000 | Loss: 0.00001274
Iteration 160/1000 | Loss: 0.00001274
Iteration 161/1000 | Loss: 0.00001274
Iteration 162/1000 | Loss: 0.00001274
Iteration 163/1000 | Loss: 0.00001274
Iteration 164/1000 | Loss: 0.00001274
Iteration 165/1000 | Loss: 0.00001274
Iteration 166/1000 | Loss: 0.00001274
Iteration 167/1000 | Loss: 0.00001274
Iteration 168/1000 | Loss: 0.00001274
Iteration 169/1000 | Loss: 0.00001274
Iteration 170/1000 | Loss: 0.00001273
Iteration 171/1000 | Loss: 0.00001273
Iteration 172/1000 | Loss: 0.00001273
Iteration 173/1000 | Loss: 0.00001273
Iteration 174/1000 | Loss: 0.00001273
Iteration 175/1000 | Loss: 0.00001273
Iteration 176/1000 | Loss: 0.00001273
Iteration 177/1000 | Loss: 0.00001273
Iteration 178/1000 | Loss: 0.00001273
Iteration 179/1000 | Loss: 0.00001273
Iteration 180/1000 | Loss: 0.00001273
Iteration 181/1000 | Loss: 0.00001273
Iteration 182/1000 | Loss: 0.00001273
Iteration 183/1000 | Loss: 0.00001273
Iteration 184/1000 | Loss: 0.00001273
Iteration 185/1000 | Loss: 0.00001272
Iteration 186/1000 | Loss: 0.00001272
Iteration 187/1000 | Loss: 0.00001272
Iteration 188/1000 | Loss: 0.00001272
Iteration 189/1000 | Loss: 0.00001272
Iteration 190/1000 | Loss: 0.00001272
Iteration 191/1000 | Loss: 0.00001272
Iteration 192/1000 | Loss: 0.00001272
Iteration 193/1000 | Loss: 0.00001272
Iteration 194/1000 | Loss: 0.00001272
Iteration 195/1000 | Loss: 0.00001272
Iteration 196/1000 | Loss: 0.00001272
Iteration 197/1000 | Loss: 0.00001272
Iteration 198/1000 | Loss: 0.00001272
Iteration 199/1000 | Loss: 0.00001272
Iteration 200/1000 | Loss: 0.00001272
Iteration 201/1000 | Loss: 0.00001272
Iteration 202/1000 | Loss: 0.00001272
Iteration 203/1000 | Loss: 0.00001272
Iteration 204/1000 | Loss: 0.00001272
Iteration 205/1000 | Loss: 0.00001272
Iteration 206/1000 | Loss: 0.00001272
Iteration 207/1000 | Loss: 0.00001272
Iteration 208/1000 | Loss: 0.00001272
Iteration 209/1000 | Loss: 0.00001272
Iteration 210/1000 | Loss: 0.00001272
Iteration 211/1000 | Loss: 0.00001272
Iteration 212/1000 | Loss: 0.00001272
Iteration 213/1000 | Loss: 0.00001272
Iteration 214/1000 | Loss: 0.00001272
Iteration 215/1000 | Loss: 0.00001272
Iteration 216/1000 | Loss: 0.00001272
Iteration 217/1000 | Loss: 0.00001272
Iteration 218/1000 | Loss: 0.00001272
Iteration 219/1000 | Loss: 0.00001272
Iteration 220/1000 | Loss: 0.00001272
Iteration 221/1000 | Loss: 0.00001272
Iteration 222/1000 | Loss: 0.00001272
Iteration 223/1000 | Loss: 0.00001272
Iteration 224/1000 | Loss: 0.00001272
Iteration 225/1000 | Loss: 0.00001272
Iteration 226/1000 | Loss: 0.00001272
Iteration 227/1000 | Loss: 0.00001272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.2722745850624051e-05, 1.2722745850624051e-05, 1.2722745850624051e-05, 1.2722745850624051e-05, 1.2722745850624051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2722745850624051e-05

Optimization complete. Final v2v error: 3.08235502243042 mm

Highest mean error: 3.9294846057891846 mm for frame 108

Lowest mean error: 2.583794593811035 mm for frame 75

Saving results

Total time: 73.19253873825073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405658
Iteration 2/25 | Loss: 0.00133906
Iteration 3/25 | Loss: 0.00126125
Iteration 4/25 | Loss: 0.00124766
Iteration 5/25 | Loss: 0.00124239
Iteration 6/25 | Loss: 0.00124146
Iteration 7/25 | Loss: 0.00124146
Iteration 8/25 | Loss: 0.00124146
Iteration 9/25 | Loss: 0.00124146
Iteration 10/25 | Loss: 0.00124146
Iteration 11/25 | Loss: 0.00124146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012414645170792937, 0.0012414645170792937, 0.0012414645170792937, 0.0012414645170792937, 0.0012414645170792937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012414645170792937

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25345814
Iteration 2/25 | Loss: 0.00232839
Iteration 3/25 | Loss: 0.00232839
Iteration 4/25 | Loss: 0.00232839
Iteration 5/25 | Loss: 0.00232839
Iteration 6/25 | Loss: 0.00232838
Iteration 7/25 | Loss: 0.00232838
Iteration 8/25 | Loss: 0.00232838
Iteration 9/25 | Loss: 0.00232838
Iteration 10/25 | Loss: 0.00232838
Iteration 11/25 | Loss: 0.00232838
Iteration 12/25 | Loss: 0.00232838
Iteration 13/25 | Loss: 0.00232838
Iteration 14/25 | Loss: 0.00232838
Iteration 15/25 | Loss: 0.00232838
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0023283837363123894, 0.0023283837363123894, 0.0023283837363123894, 0.0023283837363123894, 0.0023283837363123894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023283837363123894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232838
Iteration 2/1000 | Loss: 0.00005147
Iteration 3/1000 | Loss: 0.00003192
Iteration 4/1000 | Loss: 0.00002521
Iteration 5/1000 | Loss: 0.00002243
Iteration 6/1000 | Loss: 0.00002081
Iteration 7/1000 | Loss: 0.00001956
Iteration 8/1000 | Loss: 0.00001892
Iteration 9/1000 | Loss: 0.00001852
Iteration 10/1000 | Loss: 0.00001819
Iteration 11/1000 | Loss: 0.00001803
Iteration 12/1000 | Loss: 0.00001797
Iteration 13/1000 | Loss: 0.00001785
Iteration 14/1000 | Loss: 0.00001778
Iteration 15/1000 | Loss: 0.00001776
Iteration 16/1000 | Loss: 0.00001763
Iteration 17/1000 | Loss: 0.00001753
Iteration 18/1000 | Loss: 0.00001749
Iteration 19/1000 | Loss: 0.00001748
Iteration 20/1000 | Loss: 0.00001746
Iteration 21/1000 | Loss: 0.00001745
Iteration 22/1000 | Loss: 0.00001745
Iteration 23/1000 | Loss: 0.00001743
Iteration 24/1000 | Loss: 0.00001742
Iteration 25/1000 | Loss: 0.00001741
Iteration 26/1000 | Loss: 0.00001735
Iteration 27/1000 | Loss: 0.00001735
Iteration 28/1000 | Loss: 0.00001732
Iteration 29/1000 | Loss: 0.00001732
Iteration 30/1000 | Loss: 0.00001731
Iteration 31/1000 | Loss: 0.00001731
Iteration 32/1000 | Loss: 0.00001731
Iteration 33/1000 | Loss: 0.00001730
Iteration 34/1000 | Loss: 0.00001729
Iteration 35/1000 | Loss: 0.00001729
Iteration 36/1000 | Loss: 0.00001729
Iteration 37/1000 | Loss: 0.00001729
Iteration 38/1000 | Loss: 0.00001728
Iteration 39/1000 | Loss: 0.00001728
Iteration 40/1000 | Loss: 0.00001727
Iteration 41/1000 | Loss: 0.00001726
Iteration 42/1000 | Loss: 0.00001725
Iteration 43/1000 | Loss: 0.00001724
Iteration 44/1000 | Loss: 0.00001723
Iteration 45/1000 | Loss: 0.00001723
Iteration 46/1000 | Loss: 0.00001723
Iteration 47/1000 | Loss: 0.00001723
Iteration 48/1000 | Loss: 0.00001722
Iteration 49/1000 | Loss: 0.00001722
Iteration 50/1000 | Loss: 0.00001721
Iteration 51/1000 | Loss: 0.00001721
Iteration 52/1000 | Loss: 0.00001720
Iteration 53/1000 | Loss: 0.00001720
Iteration 54/1000 | Loss: 0.00001719
Iteration 55/1000 | Loss: 0.00001718
Iteration 56/1000 | Loss: 0.00001718
Iteration 57/1000 | Loss: 0.00001718
Iteration 58/1000 | Loss: 0.00001717
Iteration 59/1000 | Loss: 0.00001717
Iteration 60/1000 | Loss: 0.00001716
Iteration 61/1000 | Loss: 0.00001716
Iteration 62/1000 | Loss: 0.00001716
Iteration 63/1000 | Loss: 0.00001716
Iteration 64/1000 | Loss: 0.00001716
Iteration 65/1000 | Loss: 0.00001716
Iteration 66/1000 | Loss: 0.00001715
Iteration 67/1000 | Loss: 0.00001715
Iteration 68/1000 | Loss: 0.00001715
Iteration 69/1000 | Loss: 0.00001715
Iteration 70/1000 | Loss: 0.00001714
Iteration 71/1000 | Loss: 0.00001714
Iteration 72/1000 | Loss: 0.00001714
Iteration 73/1000 | Loss: 0.00001713
Iteration 74/1000 | Loss: 0.00001713
Iteration 75/1000 | Loss: 0.00001713
Iteration 76/1000 | Loss: 0.00001713
Iteration 77/1000 | Loss: 0.00001712
Iteration 78/1000 | Loss: 0.00001712
Iteration 79/1000 | Loss: 0.00001712
Iteration 80/1000 | Loss: 0.00001712
Iteration 81/1000 | Loss: 0.00001712
Iteration 82/1000 | Loss: 0.00001712
Iteration 83/1000 | Loss: 0.00001711
Iteration 84/1000 | Loss: 0.00001711
Iteration 85/1000 | Loss: 0.00001711
Iteration 86/1000 | Loss: 0.00001710
Iteration 87/1000 | Loss: 0.00001710
Iteration 88/1000 | Loss: 0.00001710
Iteration 89/1000 | Loss: 0.00001710
Iteration 90/1000 | Loss: 0.00001710
Iteration 91/1000 | Loss: 0.00001709
Iteration 92/1000 | Loss: 0.00001709
Iteration 93/1000 | Loss: 0.00001709
Iteration 94/1000 | Loss: 0.00001708
Iteration 95/1000 | Loss: 0.00001708
Iteration 96/1000 | Loss: 0.00001708
Iteration 97/1000 | Loss: 0.00001707
Iteration 98/1000 | Loss: 0.00001707
Iteration 99/1000 | Loss: 0.00001707
Iteration 100/1000 | Loss: 0.00001707
Iteration 101/1000 | Loss: 0.00001707
Iteration 102/1000 | Loss: 0.00001707
Iteration 103/1000 | Loss: 0.00001706
Iteration 104/1000 | Loss: 0.00001706
Iteration 105/1000 | Loss: 0.00001706
Iteration 106/1000 | Loss: 0.00001706
Iteration 107/1000 | Loss: 0.00001706
Iteration 108/1000 | Loss: 0.00001706
Iteration 109/1000 | Loss: 0.00001706
Iteration 110/1000 | Loss: 0.00001706
Iteration 111/1000 | Loss: 0.00001706
Iteration 112/1000 | Loss: 0.00001706
Iteration 113/1000 | Loss: 0.00001706
Iteration 114/1000 | Loss: 0.00001706
Iteration 115/1000 | Loss: 0.00001705
Iteration 116/1000 | Loss: 0.00001705
Iteration 117/1000 | Loss: 0.00001705
Iteration 118/1000 | Loss: 0.00001704
Iteration 119/1000 | Loss: 0.00001704
Iteration 120/1000 | Loss: 0.00001704
Iteration 121/1000 | Loss: 0.00001703
Iteration 122/1000 | Loss: 0.00001703
Iteration 123/1000 | Loss: 0.00001703
Iteration 124/1000 | Loss: 0.00001702
Iteration 125/1000 | Loss: 0.00001702
Iteration 126/1000 | Loss: 0.00001702
Iteration 127/1000 | Loss: 0.00001702
Iteration 128/1000 | Loss: 0.00001702
Iteration 129/1000 | Loss: 0.00001702
Iteration 130/1000 | Loss: 0.00001702
Iteration 131/1000 | Loss: 0.00001702
Iteration 132/1000 | Loss: 0.00001701
Iteration 133/1000 | Loss: 0.00001701
Iteration 134/1000 | Loss: 0.00001701
Iteration 135/1000 | Loss: 0.00001701
Iteration 136/1000 | Loss: 0.00001701
Iteration 137/1000 | Loss: 0.00001701
Iteration 138/1000 | Loss: 0.00001701
Iteration 139/1000 | Loss: 0.00001700
Iteration 140/1000 | Loss: 0.00001700
Iteration 141/1000 | Loss: 0.00001700
Iteration 142/1000 | Loss: 0.00001700
Iteration 143/1000 | Loss: 0.00001699
Iteration 144/1000 | Loss: 0.00001699
Iteration 145/1000 | Loss: 0.00001699
Iteration 146/1000 | Loss: 0.00001699
Iteration 147/1000 | Loss: 0.00001699
Iteration 148/1000 | Loss: 0.00001699
Iteration 149/1000 | Loss: 0.00001698
Iteration 150/1000 | Loss: 0.00001698
Iteration 151/1000 | Loss: 0.00001697
Iteration 152/1000 | Loss: 0.00001697
Iteration 153/1000 | Loss: 0.00001697
Iteration 154/1000 | Loss: 0.00001697
Iteration 155/1000 | Loss: 0.00001696
Iteration 156/1000 | Loss: 0.00001696
Iteration 157/1000 | Loss: 0.00001696
Iteration 158/1000 | Loss: 0.00001696
Iteration 159/1000 | Loss: 0.00001695
Iteration 160/1000 | Loss: 0.00001695
Iteration 161/1000 | Loss: 0.00001695
Iteration 162/1000 | Loss: 0.00001694
Iteration 163/1000 | Loss: 0.00001694
Iteration 164/1000 | Loss: 0.00001694
Iteration 165/1000 | Loss: 0.00001694
Iteration 166/1000 | Loss: 0.00001693
Iteration 167/1000 | Loss: 0.00001693
Iteration 168/1000 | Loss: 0.00001692
Iteration 169/1000 | Loss: 0.00001692
Iteration 170/1000 | Loss: 0.00001692
Iteration 171/1000 | Loss: 0.00001691
Iteration 172/1000 | Loss: 0.00001691
Iteration 173/1000 | Loss: 0.00001691
Iteration 174/1000 | Loss: 0.00001690
Iteration 175/1000 | Loss: 0.00001690
Iteration 176/1000 | Loss: 0.00001690
Iteration 177/1000 | Loss: 0.00001690
Iteration 178/1000 | Loss: 0.00001690
Iteration 179/1000 | Loss: 0.00001689
Iteration 180/1000 | Loss: 0.00001689
Iteration 181/1000 | Loss: 0.00001689
Iteration 182/1000 | Loss: 0.00001689
Iteration 183/1000 | Loss: 0.00001689
Iteration 184/1000 | Loss: 0.00001688
Iteration 185/1000 | Loss: 0.00001688
Iteration 186/1000 | Loss: 0.00001688
Iteration 187/1000 | Loss: 0.00001688
Iteration 188/1000 | Loss: 0.00001688
Iteration 189/1000 | Loss: 0.00001688
Iteration 190/1000 | Loss: 0.00001688
Iteration 191/1000 | Loss: 0.00001687
Iteration 192/1000 | Loss: 0.00001687
Iteration 193/1000 | Loss: 0.00001687
Iteration 194/1000 | Loss: 0.00001687
Iteration 195/1000 | Loss: 0.00001686
Iteration 196/1000 | Loss: 0.00001686
Iteration 197/1000 | Loss: 0.00001686
Iteration 198/1000 | Loss: 0.00001686
Iteration 199/1000 | Loss: 0.00001686
Iteration 200/1000 | Loss: 0.00001685
Iteration 201/1000 | Loss: 0.00001685
Iteration 202/1000 | Loss: 0.00001685
Iteration 203/1000 | Loss: 0.00001685
Iteration 204/1000 | Loss: 0.00001685
Iteration 205/1000 | Loss: 0.00001685
Iteration 206/1000 | Loss: 0.00001685
Iteration 207/1000 | Loss: 0.00001685
Iteration 208/1000 | Loss: 0.00001685
Iteration 209/1000 | Loss: 0.00001685
Iteration 210/1000 | Loss: 0.00001685
Iteration 211/1000 | Loss: 0.00001685
Iteration 212/1000 | Loss: 0.00001685
Iteration 213/1000 | Loss: 0.00001685
Iteration 214/1000 | Loss: 0.00001685
Iteration 215/1000 | Loss: 0.00001685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.6854497516760603e-05, 1.6854497516760603e-05, 1.6854497516760603e-05, 1.6854497516760603e-05, 1.6854497516760603e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6854497516760603e-05

Optimization complete. Final v2v error: 3.4127447605133057 mm

Highest mean error: 3.8415775299072266 mm for frame 236

Lowest mean error: 2.992769241333008 mm for frame 17

Saving results

Total time: 48.771405935287476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807533
Iteration 2/25 | Loss: 0.00167257
Iteration 3/25 | Loss: 0.00137481
Iteration 4/25 | Loss: 0.00135057
Iteration 5/25 | Loss: 0.00134665
Iteration 6/25 | Loss: 0.00134570
Iteration 7/25 | Loss: 0.00134570
Iteration 8/25 | Loss: 0.00134570
Iteration 9/25 | Loss: 0.00134570
Iteration 10/25 | Loss: 0.00134570
Iteration 11/25 | Loss: 0.00134570
Iteration 12/25 | Loss: 0.00134570
Iteration 13/25 | Loss: 0.00134570
Iteration 14/25 | Loss: 0.00134570
Iteration 15/25 | Loss: 0.00134570
Iteration 16/25 | Loss: 0.00134570
Iteration 17/25 | Loss: 0.00134570
Iteration 18/25 | Loss: 0.00134570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001345704193226993, 0.001345704193226993, 0.001345704193226993, 0.001345704193226993, 0.001345704193226993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001345704193226993

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30725312
Iteration 2/25 | Loss: 0.00135203
Iteration 3/25 | Loss: 0.00135203
Iteration 4/25 | Loss: 0.00135203
Iteration 5/25 | Loss: 0.00135203
Iteration 6/25 | Loss: 0.00135203
Iteration 7/25 | Loss: 0.00135203
Iteration 8/25 | Loss: 0.00135203
Iteration 9/25 | Loss: 0.00135203
Iteration 10/25 | Loss: 0.00135203
Iteration 11/25 | Loss: 0.00135203
Iteration 12/25 | Loss: 0.00135203
Iteration 13/25 | Loss: 0.00135203
Iteration 14/25 | Loss: 0.00135203
Iteration 15/25 | Loss: 0.00135203
Iteration 16/25 | Loss: 0.00135203
Iteration 17/25 | Loss: 0.00135203
Iteration 18/25 | Loss: 0.00135203
Iteration 19/25 | Loss: 0.00135203
Iteration 20/25 | Loss: 0.00135203
Iteration 21/25 | Loss: 0.00135203
Iteration 22/25 | Loss: 0.00135203
Iteration 23/25 | Loss: 0.00135203
Iteration 24/25 | Loss: 0.00135203
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013520291540771723, 0.0013520291540771723, 0.0013520291540771723, 0.0013520291540771723, 0.0013520291540771723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013520291540771723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135203
Iteration 2/1000 | Loss: 0.00005059
Iteration 3/1000 | Loss: 0.00003503
Iteration 4/1000 | Loss: 0.00002738
Iteration 5/1000 | Loss: 0.00002521
Iteration 6/1000 | Loss: 0.00002418
Iteration 7/1000 | Loss: 0.00002342
Iteration 8/1000 | Loss: 0.00002286
Iteration 9/1000 | Loss: 0.00002254
Iteration 10/1000 | Loss: 0.00002214
Iteration 11/1000 | Loss: 0.00002190
Iteration 12/1000 | Loss: 0.00002162
Iteration 13/1000 | Loss: 0.00002144
Iteration 14/1000 | Loss: 0.00002125
Iteration 15/1000 | Loss: 0.00002110
Iteration 16/1000 | Loss: 0.00002105
Iteration 17/1000 | Loss: 0.00002104
Iteration 18/1000 | Loss: 0.00002100
Iteration 19/1000 | Loss: 0.00002092
Iteration 20/1000 | Loss: 0.00002087
Iteration 21/1000 | Loss: 0.00002087
Iteration 22/1000 | Loss: 0.00002086
Iteration 23/1000 | Loss: 0.00002086
Iteration 24/1000 | Loss: 0.00002085
Iteration 25/1000 | Loss: 0.00002084
Iteration 26/1000 | Loss: 0.00002084
Iteration 27/1000 | Loss: 0.00002084
Iteration 28/1000 | Loss: 0.00002083
Iteration 29/1000 | Loss: 0.00002083
Iteration 30/1000 | Loss: 0.00002083
Iteration 31/1000 | Loss: 0.00002083
Iteration 32/1000 | Loss: 0.00002083
Iteration 33/1000 | Loss: 0.00002083
Iteration 34/1000 | Loss: 0.00002082
Iteration 35/1000 | Loss: 0.00002081
Iteration 36/1000 | Loss: 0.00002080
Iteration 37/1000 | Loss: 0.00002079
Iteration 38/1000 | Loss: 0.00002078
Iteration 39/1000 | Loss: 0.00002077
Iteration 40/1000 | Loss: 0.00002077
Iteration 41/1000 | Loss: 0.00002076
Iteration 42/1000 | Loss: 0.00002075
Iteration 43/1000 | Loss: 0.00002075
Iteration 44/1000 | Loss: 0.00002075
Iteration 45/1000 | Loss: 0.00002074
Iteration 46/1000 | Loss: 0.00002073
Iteration 47/1000 | Loss: 0.00002072
Iteration 48/1000 | Loss: 0.00002071
Iteration 49/1000 | Loss: 0.00002071
Iteration 50/1000 | Loss: 0.00002071
Iteration 51/1000 | Loss: 0.00002071
Iteration 52/1000 | Loss: 0.00002071
Iteration 53/1000 | Loss: 0.00002071
Iteration 54/1000 | Loss: 0.00002071
Iteration 55/1000 | Loss: 0.00002071
Iteration 56/1000 | Loss: 0.00002071
Iteration 57/1000 | Loss: 0.00002071
Iteration 58/1000 | Loss: 0.00002071
Iteration 59/1000 | Loss: 0.00002070
Iteration 60/1000 | Loss: 0.00002070
Iteration 61/1000 | Loss: 0.00002070
Iteration 62/1000 | Loss: 0.00002070
Iteration 63/1000 | Loss: 0.00002068
Iteration 64/1000 | Loss: 0.00002068
Iteration 65/1000 | Loss: 0.00002067
Iteration 66/1000 | Loss: 0.00002067
Iteration 67/1000 | Loss: 0.00002067
Iteration 68/1000 | Loss: 0.00002067
Iteration 69/1000 | Loss: 0.00002067
Iteration 70/1000 | Loss: 0.00002066
Iteration 71/1000 | Loss: 0.00002066
Iteration 72/1000 | Loss: 0.00002066
Iteration 73/1000 | Loss: 0.00002066
Iteration 74/1000 | Loss: 0.00002065
Iteration 75/1000 | Loss: 0.00002065
Iteration 76/1000 | Loss: 0.00002065
Iteration 77/1000 | Loss: 0.00002065
Iteration 78/1000 | Loss: 0.00002065
Iteration 79/1000 | Loss: 0.00002065
Iteration 80/1000 | Loss: 0.00002064
Iteration 81/1000 | Loss: 0.00002064
Iteration 82/1000 | Loss: 0.00002064
Iteration 83/1000 | Loss: 0.00002064
Iteration 84/1000 | Loss: 0.00002064
Iteration 85/1000 | Loss: 0.00002064
Iteration 86/1000 | Loss: 0.00002064
Iteration 87/1000 | Loss: 0.00002064
Iteration 88/1000 | Loss: 0.00002064
Iteration 89/1000 | Loss: 0.00002064
Iteration 90/1000 | Loss: 0.00002064
Iteration 91/1000 | Loss: 0.00002064
Iteration 92/1000 | Loss: 0.00002063
Iteration 93/1000 | Loss: 0.00002063
Iteration 94/1000 | Loss: 0.00002063
Iteration 95/1000 | Loss: 0.00002063
Iteration 96/1000 | Loss: 0.00002063
Iteration 97/1000 | Loss: 0.00002062
Iteration 98/1000 | Loss: 0.00002062
Iteration 99/1000 | Loss: 0.00002062
Iteration 100/1000 | Loss: 0.00002062
Iteration 101/1000 | Loss: 0.00002061
Iteration 102/1000 | Loss: 0.00002061
Iteration 103/1000 | Loss: 0.00002061
Iteration 104/1000 | Loss: 0.00002061
Iteration 105/1000 | Loss: 0.00002060
Iteration 106/1000 | Loss: 0.00002060
Iteration 107/1000 | Loss: 0.00002060
Iteration 108/1000 | Loss: 0.00002060
Iteration 109/1000 | Loss: 0.00002060
Iteration 110/1000 | Loss: 0.00002060
Iteration 111/1000 | Loss: 0.00002060
Iteration 112/1000 | Loss: 0.00002060
Iteration 113/1000 | Loss: 0.00002060
Iteration 114/1000 | Loss: 0.00002060
Iteration 115/1000 | Loss: 0.00002060
Iteration 116/1000 | Loss: 0.00002060
Iteration 117/1000 | Loss: 0.00002060
Iteration 118/1000 | Loss: 0.00002060
Iteration 119/1000 | Loss: 0.00002059
Iteration 120/1000 | Loss: 0.00002059
Iteration 121/1000 | Loss: 0.00002059
Iteration 122/1000 | Loss: 0.00002059
Iteration 123/1000 | Loss: 0.00002058
Iteration 124/1000 | Loss: 0.00002058
Iteration 125/1000 | Loss: 0.00002058
Iteration 126/1000 | Loss: 0.00002058
Iteration 127/1000 | Loss: 0.00002058
Iteration 128/1000 | Loss: 0.00002058
Iteration 129/1000 | Loss: 0.00002058
Iteration 130/1000 | Loss: 0.00002058
Iteration 131/1000 | Loss: 0.00002058
Iteration 132/1000 | Loss: 0.00002057
Iteration 133/1000 | Loss: 0.00002057
Iteration 134/1000 | Loss: 0.00002057
Iteration 135/1000 | Loss: 0.00002057
Iteration 136/1000 | Loss: 0.00002057
Iteration 137/1000 | Loss: 0.00002057
Iteration 138/1000 | Loss: 0.00002057
Iteration 139/1000 | Loss: 0.00002056
Iteration 140/1000 | Loss: 0.00002056
Iteration 141/1000 | Loss: 0.00002056
Iteration 142/1000 | Loss: 0.00002056
Iteration 143/1000 | Loss: 0.00002056
Iteration 144/1000 | Loss: 0.00002056
Iteration 145/1000 | Loss: 0.00002056
Iteration 146/1000 | Loss: 0.00002055
Iteration 147/1000 | Loss: 0.00002055
Iteration 148/1000 | Loss: 0.00002055
Iteration 149/1000 | Loss: 0.00002055
Iteration 150/1000 | Loss: 0.00002055
Iteration 151/1000 | Loss: 0.00002055
Iteration 152/1000 | Loss: 0.00002055
Iteration 153/1000 | Loss: 0.00002055
Iteration 154/1000 | Loss: 0.00002055
Iteration 155/1000 | Loss: 0.00002055
Iteration 156/1000 | Loss: 0.00002055
Iteration 157/1000 | Loss: 0.00002055
Iteration 158/1000 | Loss: 0.00002055
Iteration 159/1000 | Loss: 0.00002055
Iteration 160/1000 | Loss: 0.00002054
Iteration 161/1000 | Loss: 0.00002054
Iteration 162/1000 | Loss: 0.00002054
Iteration 163/1000 | Loss: 0.00002054
Iteration 164/1000 | Loss: 0.00002054
Iteration 165/1000 | Loss: 0.00002054
Iteration 166/1000 | Loss: 0.00002054
Iteration 167/1000 | Loss: 0.00002054
Iteration 168/1000 | Loss: 0.00002054
Iteration 169/1000 | Loss: 0.00002054
Iteration 170/1000 | Loss: 0.00002054
Iteration 171/1000 | Loss: 0.00002054
Iteration 172/1000 | Loss: 0.00002054
Iteration 173/1000 | Loss: 0.00002054
Iteration 174/1000 | Loss: 0.00002054
Iteration 175/1000 | Loss: 0.00002054
Iteration 176/1000 | Loss: 0.00002054
Iteration 177/1000 | Loss: 0.00002053
Iteration 178/1000 | Loss: 0.00002053
Iteration 179/1000 | Loss: 0.00002053
Iteration 180/1000 | Loss: 0.00002053
Iteration 181/1000 | Loss: 0.00002053
Iteration 182/1000 | Loss: 0.00002053
Iteration 183/1000 | Loss: 0.00002053
Iteration 184/1000 | Loss: 0.00002053
Iteration 185/1000 | Loss: 0.00002053
Iteration 186/1000 | Loss: 0.00002053
Iteration 187/1000 | Loss: 0.00002053
Iteration 188/1000 | Loss: 0.00002053
Iteration 189/1000 | Loss: 0.00002053
Iteration 190/1000 | Loss: 0.00002053
Iteration 191/1000 | Loss: 0.00002053
Iteration 192/1000 | Loss: 0.00002053
Iteration 193/1000 | Loss: 0.00002053
Iteration 194/1000 | Loss: 0.00002053
Iteration 195/1000 | Loss: 0.00002053
Iteration 196/1000 | Loss: 0.00002053
Iteration 197/1000 | Loss: 0.00002053
Iteration 198/1000 | Loss: 0.00002053
Iteration 199/1000 | Loss: 0.00002053
Iteration 200/1000 | Loss: 0.00002053
Iteration 201/1000 | Loss: 0.00002053
Iteration 202/1000 | Loss: 0.00002053
Iteration 203/1000 | Loss: 0.00002053
Iteration 204/1000 | Loss: 0.00002053
Iteration 205/1000 | Loss: 0.00002053
Iteration 206/1000 | Loss: 0.00002053
Iteration 207/1000 | Loss: 0.00002053
Iteration 208/1000 | Loss: 0.00002053
Iteration 209/1000 | Loss: 0.00002053
Iteration 210/1000 | Loss: 0.00002053
Iteration 211/1000 | Loss: 0.00002053
Iteration 212/1000 | Loss: 0.00002053
Iteration 213/1000 | Loss: 0.00002053
Iteration 214/1000 | Loss: 0.00002053
Iteration 215/1000 | Loss: 0.00002053
Iteration 216/1000 | Loss: 0.00002053
Iteration 217/1000 | Loss: 0.00002053
Iteration 218/1000 | Loss: 0.00002053
Iteration 219/1000 | Loss: 0.00002053
Iteration 220/1000 | Loss: 0.00002053
Iteration 221/1000 | Loss: 0.00002053
Iteration 222/1000 | Loss: 0.00002053
Iteration 223/1000 | Loss: 0.00002053
Iteration 224/1000 | Loss: 0.00002053
Iteration 225/1000 | Loss: 0.00002053
Iteration 226/1000 | Loss: 0.00002053
Iteration 227/1000 | Loss: 0.00002053
Iteration 228/1000 | Loss: 0.00002053
Iteration 229/1000 | Loss: 0.00002053
Iteration 230/1000 | Loss: 0.00002053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [2.05283777177101e-05, 2.05283777177101e-05, 2.05283777177101e-05, 2.05283777177101e-05, 2.05283777177101e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.05283777177101e-05

Optimization complete. Final v2v error: 3.7168614864349365 mm

Highest mean error: 5.027409076690674 mm for frame 149

Lowest mean error: 2.84029483795166 mm for frame 7

Saving results

Total time: 44.562360763549805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780191
Iteration 2/25 | Loss: 0.00141879
Iteration 3/25 | Loss: 0.00131612
Iteration 4/25 | Loss: 0.00129698
Iteration 5/25 | Loss: 0.00129103
Iteration 6/25 | Loss: 0.00128912
Iteration 7/25 | Loss: 0.00128911
Iteration 8/25 | Loss: 0.00128911
Iteration 9/25 | Loss: 0.00128911
Iteration 10/25 | Loss: 0.00128911
Iteration 11/25 | Loss: 0.00128911
Iteration 12/25 | Loss: 0.00128911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012891057413071394, 0.0012891057413071394, 0.0012891057413071394, 0.0012891057413071394, 0.0012891057413071394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012891057413071394

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26779389
Iteration 2/25 | Loss: 0.00194351
Iteration 3/25 | Loss: 0.00194350
Iteration 4/25 | Loss: 0.00194350
Iteration 5/25 | Loss: 0.00194350
Iteration 6/25 | Loss: 0.00194350
Iteration 7/25 | Loss: 0.00194350
Iteration 8/25 | Loss: 0.00194350
Iteration 9/25 | Loss: 0.00194350
Iteration 10/25 | Loss: 0.00194350
Iteration 11/25 | Loss: 0.00194350
Iteration 12/25 | Loss: 0.00194350
Iteration 13/25 | Loss: 0.00194350
Iteration 14/25 | Loss: 0.00194350
Iteration 15/25 | Loss: 0.00194350
Iteration 16/25 | Loss: 0.00194350
Iteration 17/25 | Loss: 0.00194350
Iteration 18/25 | Loss: 0.00194350
Iteration 19/25 | Loss: 0.00194350
Iteration 20/25 | Loss: 0.00194350
Iteration 21/25 | Loss: 0.00194350
Iteration 22/25 | Loss: 0.00194350
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0019434994319453835, 0.0019434994319453835, 0.0019434994319453835, 0.0019434994319453835, 0.0019434994319453835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019434994319453835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194350
Iteration 2/1000 | Loss: 0.00005161
Iteration 3/1000 | Loss: 0.00003599
Iteration 4/1000 | Loss: 0.00002760
Iteration 5/1000 | Loss: 0.00002504
Iteration 6/1000 | Loss: 0.00002394
Iteration 7/1000 | Loss: 0.00002297
Iteration 8/1000 | Loss: 0.00002230
Iteration 9/1000 | Loss: 0.00002174
Iteration 10/1000 | Loss: 0.00002139
Iteration 11/1000 | Loss: 0.00002113
Iteration 12/1000 | Loss: 0.00002105
Iteration 13/1000 | Loss: 0.00002103
Iteration 14/1000 | Loss: 0.00002090
Iteration 15/1000 | Loss: 0.00002083
Iteration 16/1000 | Loss: 0.00002083
Iteration 17/1000 | Loss: 0.00002082
Iteration 18/1000 | Loss: 0.00002079
Iteration 19/1000 | Loss: 0.00002074
Iteration 20/1000 | Loss: 0.00002069
Iteration 21/1000 | Loss: 0.00002065
Iteration 22/1000 | Loss: 0.00002060
Iteration 23/1000 | Loss: 0.00002056
Iteration 24/1000 | Loss: 0.00002054
Iteration 25/1000 | Loss: 0.00002054
Iteration 26/1000 | Loss: 0.00002052
Iteration 27/1000 | Loss: 0.00002051
Iteration 28/1000 | Loss: 0.00002050
Iteration 29/1000 | Loss: 0.00002048
Iteration 30/1000 | Loss: 0.00002042
Iteration 31/1000 | Loss: 0.00002034
Iteration 32/1000 | Loss: 0.00002033
Iteration 33/1000 | Loss: 0.00002031
Iteration 34/1000 | Loss: 0.00002030
Iteration 35/1000 | Loss: 0.00002030
Iteration 36/1000 | Loss: 0.00002029
Iteration 37/1000 | Loss: 0.00002029
Iteration 38/1000 | Loss: 0.00002029
Iteration 39/1000 | Loss: 0.00002028
Iteration 40/1000 | Loss: 0.00002028
Iteration 41/1000 | Loss: 0.00002027
Iteration 42/1000 | Loss: 0.00002027
Iteration 43/1000 | Loss: 0.00002027
Iteration 44/1000 | Loss: 0.00002026
Iteration 45/1000 | Loss: 0.00002026
Iteration 46/1000 | Loss: 0.00002025
Iteration 47/1000 | Loss: 0.00002025
Iteration 48/1000 | Loss: 0.00002024
Iteration 49/1000 | Loss: 0.00002024
Iteration 50/1000 | Loss: 0.00002024
Iteration 51/1000 | Loss: 0.00002023
Iteration 52/1000 | Loss: 0.00002023
Iteration 53/1000 | Loss: 0.00002023
Iteration 54/1000 | Loss: 0.00002022
Iteration 55/1000 | Loss: 0.00002022
Iteration 56/1000 | Loss: 0.00002022
Iteration 57/1000 | Loss: 0.00002021
Iteration 58/1000 | Loss: 0.00002020
Iteration 59/1000 | Loss: 0.00002020
Iteration 60/1000 | Loss: 0.00002020
Iteration 61/1000 | Loss: 0.00002020
Iteration 62/1000 | Loss: 0.00002020
Iteration 63/1000 | Loss: 0.00002019
Iteration 64/1000 | Loss: 0.00002019
Iteration 65/1000 | Loss: 0.00002019
Iteration 66/1000 | Loss: 0.00002019
Iteration 67/1000 | Loss: 0.00002019
Iteration 68/1000 | Loss: 0.00002019
Iteration 69/1000 | Loss: 0.00002019
Iteration 70/1000 | Loss: 0.00002019
Iteration 71/1000 | Loss: 0.00002018
Iteration 72/1000 | Loss: 0.00002018
Iteration 73/1000 | Loss: 0.00002017
Iteration 74/1000 | Loss: 0.00002017
Iteration 75/1000 | Loss: 0.00002017
Iteration 76/1000 | Loss: 0.00002017
Iteration 77/1000 | Loss: 0.00002017
Iteration 78/1000 | Loss: 0.00002016
Iteration 79/1000 | Loss: 0.00002016
Iteration 80/1000 | Loss: 0.00002016
Iteration 81/1000 | Loss: 0.00002015
Iteration 82/1000 | Loss: 0.00002015
Iteration 83/1000 | Loss: 0.00002015
Iteration 84/1000 | Loss: 0.00002015
Iteration 85/1000 | Loss: 0.00002015
Iteration 86/1000 | Loss: 0.00002015
Iteration 87/1000 | Loss: 0.00002015
Iteration 88/1000 | Loss: 0.00002015
Iteration 89/1000 | Loss: 0.00002014
Iteration 90/1000 | Loss: 0.00002014
Iteration 91/1000 | Loss: 0.00002014
Iteration 92/1000 | Loss: 0.00002014
Iteration 93/1000 | Loss: 0.00002014
Iteration 94/1000 | Loss: 0.00002014
Iteration 95/1000 | Loss: 0.00002013
Iteration 96/1000 | Loss: 0.00002013
Iteration 97/1000 | Loss: 0.00002012
Iteration 98/1000 | Loss: 0.00002012
Iteration 99/1000 | Loss: 0.00002012
Iteration 100/1000 | Loss: 0.00002012
Iteration 101/1000 | Loss: 0.00002012
Iteration 102/1000 | Loss: 0.00002012
Iteration 103/1000 | Loss: 0.00002011
Iteration 104/1000 | Loss: 0.00002011
Iteration 105/1000 | Loss: 0.00002011
Iteration 106/1000 | Loss: 0.00002010
Iteration 107/1000 | Loss: 0.00002010
Iteration 108/1000 | Loss: 0.00002010
Iteration 109/1000 | Loss: 0.00002009
Iteration 110/1000 | Loss: 0.00002009
Iteration 111/1000 | Loss: 0.00002008
Iteration 112/1000 | Loss: 0.00002008
Iteration 113/1000 | Loss: 0.00002008
Iteration 114/1000 | Loss: 0.00002007
Iteration 115/1000 | Loss: 0.00002007
Iteration 116/1000 | Loss: 0.00002007
Iteration 117/1000 | Loss: 0.00002007
Iteration 118/1000 | Loss: 0.00002007
Iteration 119/1000 | Loss: 0.00002007
Iteration 120/1000 | Loss: 0.00002006
Iteration 121/1000 | Loss: 0.00002006
Iteration 122/1000 | Loss: 0.00002006
Iteration 123/1000 | Loss: 0.00002006
Iteration 124/1000 | Loss: 0.00002006
Iteration 125/1000 | Loss: 0.00002006
Iteration 126/1000 | Loss: 0.00002006
Iteration 127/1000 | Loss: 0.00002005
Iteration 128/1000 | Loss: 0.00002005
Iteration 129/1000 | Loss: 0.00002005
Iteration 130/1000 | Loss: 0.00002005
Iteration 131/1000 | Loss: 0.00002005
Iteration 132/1000 | Loss: 0.00002005
Iteration 133/1000 | Loss: 0.00002005
Iteration 134/1000 | Loss: 0.00002005
Iteration 135/1000 | Loss: 0.00002004
Iteration 136/1000 | Loss: 0.00002004
Iteration 137/1000 | Loss: 0.00002004
Iteration 138/1000 | Loss: 0.00002004
Iteration 139/1000 | Loss: 0.00002004
Iteration 140/1000 | Loss: 0.00002004
Iteration 141/1000 | Loss: 0.00002004
Iteration 142/1000 | Loss: 0.00002004
Iteration 143/1000 | Loss: 0.00002004
Iteration 144/1000 | Loss: 0.00002003
Iteration 145/1000 | Loss: 0.00002003
Iteration 146/1000 | Loss: 0.00002003
Iteration 147/1000 | Loss: 0.00002003
Iteration 148/1000 | Loss: 0.00002003
Iteration 149/1000 | Loss: 0.00002003
Iteration 150/1000 | Loss: 0.00002002
Iteration 151/1000 | Loss: 0.00002002
Iteration 152/1000 | Loss: 0.00002002
Iteration 153/1000 | Loss: 0.00002002
Iteration 154/1000 | Loss: 0.00002002
Iteration 155/1000 | Loss: 0.00002002
Iteration 156/1000 | Loss: 0.00002002
Iteration 157/1000 | Loss: 0.00002002
Iteration 158/1000 | Loss: 0.00002002
Iteration 159/1000 | Loss: 0.00002002
Iteration 160/1000 | Loss: 0.00002001
Iteration 161/1000 | Loss: 0.00002001
Iteration 162/1000 | Loss: 0.00002001
Iteration 163/1000 | Loss: 0.00002001
Iteration 164/1000 | Loss: 0.00002001
Iteration 165/1000 | Loss: 0.00002001
Iteration 166/1000 | Loss: 0.00002001
Iteration 167/1000 | Loss: 0.00002001
Iteration 168/1000 | Loss: 0.00002001
Iteration 169/1000 | Loss: 0.00002001
Iteration 170/1000 | Loss: 0.00002001
Iteration 171/1000 | Loss: 0.00002001
Iteration 172/1000 | Loss: 0.00002001
Iteration 173/1000 | Loss: 0.00002001
Iteration 174/1000 | Loss: 0.00002001
Iteration 175/1000 | Loss: 0.00002001
Iteration 176/1000 | Loss: 0.00002001
Iteration 177/1000 | Loss: 0.00002000
Iteration 178/1000 | Loss: 0.00002000
Iteration 179/1000 | Loss: 0.00002000
Iteration 180/1000 | Loss: 0.00002000
Iteration 181/1000 | Loss: 0.00002000
Iteration 182/1000 | Loss: 0.00002000
Iteration 183/1000 | Loss: 0.00002000
Iteration 184/1000 | Loss: 0.00002000
Iteration 185/1000 | Loss: 0.00002000
Iteration 186/1000 | Loss: 0.00002000
Iteration 187/1000 | Loss: 0.00002000
Iteration 188/1000 | Loss: 0.00002000
Iteration 189/1000 | Loss: 0.00002000
Iteration 190/1000 | Loss: 0.00001999
Iteration 191/1000 | Loss: 0.00001999
Iteration 192/1000 | Loss: 0.00001999
Iteration 193/1000 | Loss: 0.00001999
Iteration 194/1000 | Loss: 0.00001999
Iteration 195/1000 | Loss: 0.00001999
Iteration 196/1000 | Loss: 0.00001999
Iteration 197/1000 | Loss: 0.00001999
Iteration 198/1000 | Loss: 0.00001999
Iteration 199/1000 | Loss: 0.00001999
Iteration 200/1000 | Loss: 0.00001999
Iteration 201/1000 | Loss: 0.00001999
Iteration 202/1000 | Loss: 0.00001999
Iteration 203/1000 | Loss: 0.00001999
Iteration 204/1000 | Loss: 0.00001999
Iteration 205/1000 | Loss: 0.00001999
Iteration 206/1000 | Loss: 0.00001999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.9989658539998345e-05, 1.9989658539998345e-05, 1.9989658539998345e-05, 1.9989658539998345e-05, 1.9989658539998345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9989658539998345e-05

Optimization complete. Final v2v error: 3.687251329421997 mm

Highest mean error: 4.603151798248291 mm for frame 96

Lowest mean error: 2.8803718090057373 mm for frame 3

Saving results

Total time: 44.81257176399231
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006078
Iteration 2/25 | Loss: 0.00316721
Iteration 3/25 | Loss: 0.00261276
Iteration 4/25 | Loss: 0.00219654
Iteration 5/25 | Loss: 0.00205711
Iteration 6/25 | Loss: 0.00199465
Iteration 7/25 | Loss: 0.00193964
Iteration 8/25 | Loss: 0.00183879
Iteration 9/25 | Loss: 0.00173453
Iteration 10/25 | Loss: 0.00175449
Iteration 11/25 | Loss: 0.00169090
Iteration 12/25 | Loss: 0.00168617
Iteration 13/25 | Loss: 0.00166691
Iteration 14/25 | Loss: 0.00165157
Iteration 15/25 | Loss: 0.00164886
Iteration 16/25 | Loss: 0.00162904
Iteration 17/25 | Loss: 0.00162933
Iteration 18/25 | Loss: 0.00162059
Iteration 19/25 | Loss: 0.00161901
Iteration 20/25 | Loss: 0.00161651
Iteration 21/25 | Loss: 0.00161502
Iteration 22/25 | Loss: 0.00161448
Iteration 23/25 | Loss: 0.00161419
Iteration 24/25 | Loss: 0.00161340
Iteration 25/25 | Loss: 0.00161279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26524937
Iteration 2/25 | Loss: 0.00731053
Iteration 3/25 | Loss: 0.00582772
Iteration 4/25 | Loss: 0.00582772
Iteration 5/25 | Loss: 0.00582772
Iteration 6/25 | Loss: 0.00582772
Iteration 7/25 | Loss: 0.00582772
Iteration 8/25 | Loss: 0.00582772
Iteration 9/25 | Loss: 0.00582772
Iteration 10/25 | Loss: 0.00582772
Iteration 11/25 | Loss: 0.00582772
Iteration 12/25 | Loss: 0.00582772
Iteration 13/25 | Loss: 0.00582772
Iteration 14/25 | Loss: 0.00582772
Iteration 15/25 | Loss: 0.00582772
Iteration 16/25 | Loss: 0.00582772
Iteration 17/25 | Loss: 0.00582772
Iteration 18/25 | Loss: 0.00582772
Iteration 19/25 | Loss: 0.00582772
Iteration 20/25 | Loss: 0.00582772
Iteration 21/25 | Loss: 0.00582772
Iteration 22/25 | Loss: 0.00582772
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.005827718414366245, 0.005827718414366245, 0.005827718414366245, 0.005827718414366245, 0.005827718414366245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005827718414366245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00582772
Iteration 2/1000 | Loss: 0.00176568
Iteration 3/1000 | Loss: 0.00145469
Iteration 4/1000 | Loss: 0.00045372
Iteration 5/1000 | Loss: 0.00036925
Iteration 6/1000 | Loss: 0.00030396
Iteration 7/1000 | Loss: 0.00186744
Iteration 8/1000 | Loss: 0.00032504
Iteration 9/1000 | Loss: 0.00049229
Iteration 10/1000 | Loss: 0.00027975
Iteration 11/1000 | Loss: 0.00047852
Iteration 12/1000 | Loss: 0.00026647
Iteration 13/1000 | Loss: 0.00035896
Iteration 14/1000 | Loss: 0.00045831
Iteration 15/1000 | Loss: 0.00026232
Iteration 16/1000 | Loss: 0.00038107
Iteration 17/1000 | Loss: 0.00025135
Iteration 18/1000 | Loss: 0.00024862
Iteration 19/1000 | Loss: 0.00024554
Iteration 20/1000 | Loss: 0.00155962
Iteration 21/1000 | Loss: 0.00024174
Iteration 22/1000 | Loss: 0.00035245
Iteration 23/1000 | Loss: 0.00059761
Iteration 24/1000 | Loss: 0.00041784
Iteration 25/1000 | Loss: 0.00074697
Iteration 26/1000 | Loss: 0.00025228
Iteration 27/1000 | Loss: 0.00052498
Iteration 28/1000 | Loss: 0.00023460
Iteration 29/1000 | Loss: 0.00040876
Iteration 30/1000 | Loss: 0.00023624
Iteration 31/1000 | Loss: 0.00045126
Iteration 32/1000 | Loss: 0.00036982
Iteration 33/1000 | Loss: 0.00055709
Iteration 34/1000 | Loss: 0.00065404
Iteration 35/1000 | Loss: 0.00070475
Iteration 36/1000 | Loss: 0.00025296
Iteration 37/1000 | Loss: 0.00023258
Iteration 38/1000 | Loss: 0.00022859
Iteration 39/1000 | Loss: 0.00048698
Iteration 40/1000 | Loss: 0.00039414
Iteration 41/1000 | Loss: 0.00022227
Iteration 42/1000 | Loss: 0.00043499
Iteration 43/1000 | Loss: 0.00036738
Iteration 44/1000 | Loss: 0.00021246
Iteration 45/1000 | Loss: 0.00020709
Iteration 46/1000 | Loss: 0.00020116
Iteration 47/1000 | Loss: 0.00034464
Iteration 48/1000 | Loss: 0.00130552
Iteration 49/1000 | Loss: 0.00031424
Iteration 50/1000 | Loss: 0.00019881
Iteration 51/1000 | Loss: 0.00019295
Iteration 52/1000 | Loss: 0.00020130
Iteration 53/1000 | Loss: 0.00046625
Iteration 54/1000 | Loss: 0.00018679
Iteration 55/1000 | Loss: 0.00033320
Iteration 56/1000 | Loss: 0.00018349
Iteration 57/1000 | Loss: 0.00018082
Iteration 58/1000 | Loss: 0.00046859
Iteration 59/1000 | Loss: 0.00088751
Iteration 60/1000 | Loss: 0.00017980
Iteration 61/1000 | Loss: 0.00017727
Iteration 62/1000 | Loss: 0.00017616
Iteration 63/1000 | Loss: 0.00017507
Iteration 64/1000 | Loss: 0.00017411
Iteration 65/1000 | Loss: 0.00030114
Iteration 66/1000 | Loss: 0.00056469
Iteration 67/1000 | Loss: 0.00176158
Iteration 68/1000 | Loss: 0.00026264
Iteration 69/1000 | Loss: 0.00018279
Iteration 70/1000 | Loss: 0.00030307
Iteration 71/1000 | Loss: 0.00017461
Iteration 72/1000 | Loss: 0.00028184
Iteration 73/1000 | Loss: 0.00016867
Iteration 74/1000 | Loss: 0.00027281
Iteration 75/1000 | Loss: 0.00023489
Iteration 76/1000 | Loss: 0.00016713
Iteration 77/1000 | Loss: 0.00016643
Iteration 78/1000 | Loss: 0.00016599
Iteration 79/1000 | Loss: 0.00034237
Iteration 80/1000 | Loss: 0.00019248
Iteration 81/1000 | Loss: 0.00016565
Iteration 82/1000 | Loss: 0.00026703
Iteration 83/1000 | Loss: 0.00016550
Iteration 84/1000 | Loss: 0.00016518
Iteration 85/1000 | Loss: 0.00016506
Iteration 86/1000 | Loss: 0.00016500
Iteration 87/1000 | Loss: 0.00016492
Iteration 88/1000 | Loss: 0.00016492
Iteration 89/1000 | Loss: 0.00016491
Iteration 90/1000 | Loss: 0.00016489
Iteration 91/1000 | Loss: 0.00035046
Iteration 92/1000 | Loss: 0.00017306
Iteration 93/1000 | Loss: 0.00016690
Iteration 94/1000 | Loss: 0.00016506
Iteration 95/1000 | Loss: 0.00016473
Iteration 96/1000 | Loss: 0.00016468
Iteration 97/1000 | Loss: 0.00016468
Iteration 98/1000 | Loss: 0.00016465
Iteration 99/1000 | Loss: 0.00016465
Iteration 100/1000 | Loss: 0.00016465
Iteration 101/1000 | Loss: 0.00016464
Iteration 102/1000 | Loss: 0.00016464
Iteration 103/1000 | Loss: 0.00016464
Iteration 104/1000 | Loss: 0.00016464
Iteration 105/1000 | Loss: 0.00016464
Iteration 106/1000 | Loss: 0.00016464
Iteration 107/1000 | Loss: 0.00016463
Iteration 108/1000 | Loss: 0.00016463
Iteration 109/1000 | Loss: 0.00016463
Iteration 110/1000 | Loss: 0.00016463
Iteration 111/1000 | Loss: 0.00016463
Iteration 112/1000 | Loss: 0.00016463
Iteration 113/1000 | Loss: 0.00016463
Iteration 114/1000 | Loss: 0.00016463
Iteration 115/1000 | Loss: 0.00016463
Iteration 116/1000 | Loss: 0.00016462
Iteration 117/1000 | Loss: 0.00016461
Iteration 118/1000 | Loss: 0.00016461
Iteration 119/1000 | Loss: 0.00016461
Iteration 120/1000 | Loss: 0.00016460
Iteration 121/1000 | Loss: 0.00016460
Iteration 122/1000 | Loss: 0.00016460
Iteration 123/1000 | Loss: 0.00016460
Iteration 124/1000 | Loss: 0.00016459
Iteration 125/1000 | Loss: 0.00016459
Iteration 126/1000 | Loss: 0.00016459
Iteration 127/1000 | Loss: 0.00016459
Iteration 128/1000 | Loss: 0.00016459
Iteration 129/1000 | Loss: 0.00016459
Iteration 130/1000 | Loss: 0.00016459
Iteration 131/1000 | Loss: 0.00016459
Iteration 132/1000 | Loss: 0.00016458
Iteration 133/1000 | Loss: 0.00016458
Iteration 134/1000 | Loss: 0.00016458
Iteration 135/1000 | Loss: 0.00016458
Iteration 136/1000 | Loss: 0.00016458
Iteration 137/1000 | Loss: 0.00016458
Iteration 138/1000 | Loss: 0.00016458
Iteration 139/1000 | Loss: 0.00016458
Iteration 140/1000 | Loss: 0.00016458
Iteration 141/1000 | Loss: 0.00016458
Iteration 142/1000 | Loss: 0.00016458
Iteration 143/1000 | Loss: 0.00016458
Iteration 144/1000 | Loss: 0.00016458
Iteration 145/1000 | Loss: 0.00016457
Iteration 146/1000 | Loss: 0.00016457
Iteration 147/1000 | Loss: 0.00016457
Iteration 148/1000 | Loss: 0.00016457
Iteration 149/1000 | Loss: 0.00016457
Iteration 150/1000 | Loss: 0.00016457
Iteration 151/1000 | Loss: 0.00016457
Iteration 152/1000 | Loss: 0.00016457
Iteration 153/1000 | Loss: 0.00016457
Iteration 154/1000 | Loss: 0.00016457
Iteration 155/1000 | Loss: 0.00016457
Iteration 156/1000 | Loss: 0.00016457
Iteration 157/1000 | Loss: 0.00016457
Iteration 158/1000 | Loss: 0.00016456
Iteration 159/1000 | Loss: 0.00016456
Iteration 160/1000 | Loss: 0.00016456
Iteration 161/1000 | Loss: 0.00016456
Iteration 162/1000 | Loss: 0.00016455
Iteration 163/1000 | Loss: 0.00016455
Iteration 164/1000 | Loss: 0.00016455
Iteration 165/1000 | Loss: 0.00016455
Iteration 166/1000 | Loss: 0.00016455
Iteration 167/1000 | Loss: 0.00016454
Iteration 168/1000 | Loss: 0.00016454
Iteration 169/1000 | Loss: 0.00016454
Iteration 170/1000 | Loss: 0.00016454
Iteration 171/1000 | Loss: 0.00016454
Iteration 172/1000 | Loss: 0.00016453
Iteration 173/1000 | Loss: 0.00016453
Iteration 174/1000 | Loss: 0.00016453
Iteration 175/1000 | Loss: 0.00016453
Iteration 176/1000 | Loss: 0.00016453
Iteration 177/1000 | Loss: 0.00016453
Iteration 178/1000 | Loss: 0.00016452
Iteration 179/1000 | Loss: 0.00016452
Iteration 180/1000 | Loss: 0.00031967
Iteration 181/1000 | Loss: 0.00016472
Iteration 182/1000 | Loss: 0.00016454
Iteration 183/1000 | Loss: 0.00016452
Iteration 184/1000 | Loss: 0.00016452
Iteration 185/1000 | Loss: 0.00016452
Iteration 186/1000 | Loss: 0.00016452
Iteration 187/1000 | Loss: 0.00016451
Iteration 188/1000 | Loss: 0.00016451
Iteration 189/1000 | Loss: 0.00016451
Iteration 190/1000 | Loss: 0.00016451
Iteration 191/1000 | Loss: 0.00016451
Iteration 192/1000 | Loss: 0.00016450
Iteration 193/1000 | Loss: 0.00016450
Iteration 194/1000 | Loss: 0.00016450
Iteration 195/1000 | Loss: 0.00016450
Iteration 196/1000 | Loss: 0.00016450
Iteration 197/1000 | Loss: 0.00016450
Iteration 198/1000 | Loss: 0.00016450
Iteration 199/1000 | Loss: 0.00016450
Iteration 200/1000 | Loss: 0.00016449
Iteration 201/1000 | Loss: 0.00016449
Iteration 202/1000 | Loss: 0.00016449
Iteration 203/1000 | Loss: 0.00016449
Iteration 204/1000 | Loss: 0.00016449
Iteration 205/1000 | Loss: 0.00016449
Iteration 206/1000 | Loss: 0.00016449
Iteration 207/1000 | Loss: 0.00016449
Iteration 208/1000 | Loss: 0.00016449
Iteration 209/1000 | Loss: 0.00016448
Iteration 210/1000 | Loss: 0.00016448
Iteration 211/1000 | Loss: 0.00016448
Iteration 212/1000 | Loss: 0.00016448
Iteration 213/1000 | Loss: 0.00016448
Iteration 214/1000 | Loss: 0.00016448
Iteration 215/1000 | Loss: 0.00016448
Iteration 216/1000 | Loss: 0.00016448
Iteration 217/1000 | Loss: 0.00016448
Iteration 218/1000 | Loss: 0.00016448
Iteration 219/1000 | Loss: 0.00016448
Iteration 220/1000 | Loss: 0.00016448
Iteration 221/1000 | Loss: 0.00016448
Iteration 222/1000 | Loss: 0.00016448
Iteration 223/1000 | Loss: 0.00016448
Iteration 224/1000 | Loss: 0.00016448
Iteration 225/1000 | Loss: 0.00016448
Iteration 226/1000 | Loss: 0.00016448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [0.00016448277165181935, 0.00016448277165181935, 0.00016448277165181935, 0.00016448277165181935, 0.00016448277165181935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00016448277165181935

Optimization complete. Final v2v error: 6.721164226531982 mm

Highest mean error: 11.128212928771973 mm for frame 40

Lowest mean error: 3.8333523273468018 mm for frame 150

Saving results

Total time: 183.3791389465332
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051021
Iteration 2/25 | Loss: 0.00277356
Iteration 3/25 | Loss: 0.00227719
Iteration 4/25 | Loss: 0.00178327
Iteration 5/25 | Loss: 0.00171522
Iteration 6/25 | Loss: 0.00147781
Iteration 7/25 | Loss: 0.00138837
Iteration 8/25 | Loss: 0.00131348
Iteration 9/25 | Loss: 0.00126153
Iteration 10/25 | Loss: 0.00125872
Iteration 11/25 | Loss: 0.00124592
Iteration 12/25 | Loss: 0.00124222
Iteration 13/25 | Loss: 0.00124146
Iteration 14/25 | Loss: 0.00124130
Iteration 15/25 | Loss: 0.00124129
Iteration 16/25 | Loss: 0.00124129
Iteration 17/25 | Loss: 0.00124129
Iteration 18/25 | Loss: 0.00124129
Iteration 19/25 | Loss: 0.00124129
Iteration 20/25 | Loss: 0.00124129
Iteration 21/25 | Loss: 0.00124129
Iteration 22/25 | Loss: 0.00124129
Iteration 23/25 | Loss: 0.00124129
Iteration 24/25 | Loss: 0.00124129
Iteration 25/25 | Loss: 0.00124129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25733519
Iteration 2/25 | Loss: 0.00164326
Iteration 3/25 | Loss: 0.00164326
Iteration 4/25 | Loss: 0.00164326
Iteration 5/25 | Loss: 0.00164326
Iteration 6/25 | Loss: 0.00164326
Iteration 7/25 | Loss: 0.00164326
Iteration 8/25 | Loss: 0.00164326
Iteration 9/25 | Loss: 0.00164326
Iteration 10/25 | Loss: 0.00164326
Iteration 11/25 | Loss: 0.00164326
Iteration 12/25 | Loss: 0.00164326
Iteration 13/25 | Loss: 0.00164326
Iteration 14/25 | Loss: 0.00164326
Iteration 15/25 | Loss: 0.00164326
Iteration 16/25 | Loss: 0.00164326
Iteration 17/25 | Loss: 0.00164326
Iteration 18/25 | Loss: 0.00164326
Iteration 19/25 | Loss: 0.00164326
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001643260126002133, 0.001643260126002133, 0.001643260126002133, 0.001643260126002133, 0.001643260126002133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001643260126002133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164326
Iteration 2/1000 | Loss: 0.00004936
Iteration 3/1000 | Loss: 0.00003384
Iteration 4/1000 | Loss: 0.00002920
Iteration 5/1000 | Loss: 0.00002709
Iteration 6/1000 | Loss: 0.00002570
Iteration 7/1000 | Loss: 0.00002456
Iteration 8/1000 | Loss: 0.00002374
Iteration 9/1000 | Loss: 0.00002314
Iteration 10/1000 | Loss: 0.00002267
Iteration 11/1000 | Loss: 0.00002216
Iteration 12/1000 | Loss: 0.00091281
Iteration 13/1000 | Loss: 0.00042747
Iteration 14/1000 | Loss: 0.00022571
Iteration 15/1000 | Loss: 0.00042165
Iteration 16/1000 | Loss: 0.00026474
Iteration 17/1000 | Loss: 0.00028746
Iteration 18/1000 | Loss: 0.00024387
Iteration 19/1000 | Loss: 0.00028982
Iteration 20/1000 | Loss: 0.00033612
Iteration 21/1000 | Loss: 0.00019551
Iteration 22/1000 | Loss: 0.00002122
Iteration 23/1000 | Loss: 0.00001879
Iteration 24/1000 | Loss: 0.00001742
Iteration 25/1000 | Loss: 0.00001661
Iteration 26/1000 | Loss: 0.00001606
Iteration 27/1000 | Loss: 0.00001571
Iteration 28/1000 | Loss: 0.00001539
Iteration 29/1000 | Loss: 0.00001520
Iteration 30/1000 | Loss: 0.00001495
Iteration 31/1000 | Loss: 0.00001481
Iteration 32/1000 | Loss: 0.00001480
Iteration 33/1000 | Loss: 0.00001459
Iteration 34/1000 | Loss: 0.00001458
Iteration 35/1000 | Loss: 0.00001457
Iteration 36/1000 | Loss: 0.00001457
Iteration 37/1000 | Loss: 0.00001457
Iteration 38/1000 | Loss: 0.00001456
Iteration 39/1000 | Loss: 0.00001456
Iteration 40/1000 | Loss: 0.00001456
Iteration 41/1000 | Loss: 0.00001455
Iteration 42/1000 | Loss: 0.00001454
Iteration 43/1000 | Loss: 0.00001453
Iteration 44/1000 | Loss: 0.00001446
Iteration 45/1000 | Loss: 0.00001446
Iteration 46/1000 | Loss: 0.00001446
Iteration 47/1000 | Loss: 0.00001446
Iteration 48/1000 | Loss: 0.00001446
Iteration 49/1000 | Loss: 0.00001446
Iteration 50/1000 | Loss: 0.00001446
Iteration 51/1000 | Loss: 0.00001446
Iteration 52/1000 | Loss: 0.00001445
Iteration 53/1000 | Loss: 0.00001443
Iteration 54/1000 | Loss: 0.00001443
Iteration 55/1000 | Loss: 0.00001442
Iteration 56/1000 | Loss: 0.00001442
Iteration 57/1000 | Loss: 0.00001442
Iteration 58/1000 | Loss: 0.00001441
Iteration 59/1000 | Loss: 0.00001441
Iteration 60/1000 | Loss: 0.00001441
Iteration 61/1000 | Loss: 0.00001440
Iteration 62/1000 | Loss: 0.00001440
Iteration 63/1000 | Loss: 0.00001439
Iteration 64/1000 | Loss: 0.00001438
Iteration 65/1000 | Loss: 0.00001438
Iteration 66/1000 | Loss: 0.00001437
Iteration 67/1000 | Loss: 0.00001437
Iteration 68/1000 | Loss: 0.00001437
Iteration 69/1000 | Loss: 0.00001436
Iteration 70/1000 | Loss: 0.00001436
Iteration 71/1000 | Loss: 0.00001435
Iteration 72/1000 | Loss: 0.00001435
Iteration 73/1000 | Loss: 0.00001435
Iteration 74/1000 | Loss: 0.00001435
Iteration 75/1000 | Loss: 0.00001435
Iteration 76/1000 | Loss: 0.00001435
Iteration 77/1000 | Loss: 0.00001435
Iteration 78/1000 | Loss: 0.00001435
Iteration 79/1000 | Loss: 0.00001435
Iteration 80/1000 | Loss: 0.00001435
Iteration 81/1000 | Loss: 0.00001434
Iteration 82/1000 | Loss: 0.00001434
Iteration 83/1000 | Loss: 0.00001434
Iteration 84/1000 | Loss: 0.00001434
Iteration 85/1000 | Loss: 0.00001434
Iteration 86/1000 | Loss: 0.00001433
Iteration 87/1000 | Loss: 0.00001433
Iteration 88/1000 | Loss: 0.00001433
Iteration 89/1000 | Loss: 0.00001433
Iteration 90/1000 | Loss: 0.00001433
Iteration 91/1000 | Loss: 0.00001433
Iteration 92/1000 | Loss: 0.00001433
Iteration 93/1000 | Loss: 0.00001432
Iteration 94/1000 | Loss: 0.00001432
Iteration 95/1000 | Loss: 0.00001432
Iteration 96/1000 | Loss: 0.00001432
Iteration 97/1000 | Loss: 0.00001431
Iteration 98/1000 | Loss: 0.00001431
Iteration 99/1000 | Loss: 0.00001431
Iteration 100/1000 | Loss: 0.00001430
Iteration 101/1000 | Loss: 0.00001430
Iteration 102/1000 | Loss: 0.00001430
Iteration 103/1000 | Loss: 0.00001430
Iteration 104/1000 | Loss: 0.00001430
Iteration 105/1000 | Loss: 0.00001430
Iteration 106/1000 | Loss: 0.00001430
Iteration 107/1000 | Loss: 0.00001430
Iteration 108/1000 | Loss: 0.00001429
Iteration 109/1000 | Loss: 0.00001429
Iteration 110/1000 | Loss: 0.00001429
Iteration 111/1000 | Loss: 0.00001428
Iteration 112/1000 | Loss: 0.00001428
Iteration 113/1000 | Loss: 0.00001428
Iteration 114/1000 | Loss: 0.00001428
Iteration 115/1000 | Loss: 0.00001427
Iteration 116/1000 | Loss: 0.00001427
Iteration 117/1000 | Loss: 0.00001427
Iteration 118/1000 | Loss: 0.00001427
Iteration 119/1000 | Loss: 0.00001427
Iteration 120/1000 | Loss: 0.00001427
Iteration 121/1000 | Loss: 0.00001427
Iteration 122/1000 | Loss: 0.00001426
Iteration 123/1000 | Loss: 0.00001426
Iteration 124/1000 | Loss: 0.00001426
Iteration 125/1000 | Loss: 0.00001426
Iteration 126/1000 | Loss: 0.00001426
Iteration 127/1000 | Loss: 0.00001426
Iteration 128/1000 | Loss: 0.00001426
Iteration 129/1000 | Loss: 0.00001426
Iteration 130/1000 | Loss: 0.00001426
Iteration 131/1000 | Loss: 0.00001426
Iteration 132/1000 | Loss: 0.00001425
Iteration 133/1000 | Loss: 0.00001425
Iteration 134/1000 | Loss: 0.00001425
Iteration 135/1000 | Loss: 0.00001425
Iteration 136/1000 | Loss: 0.00001425
Iteration 137/1000 | Loss: 0.00001424
Iteration 138/1000 | Loss: 0.00001424
Iteration 139/1000 | Loss: 0.00001424
Iteration 140/1000 | Loss: 0.00001424
Iteration 141/1000 | Loss: 0.00001423
Iteration 142/1000 | Loss: 0.00001423
Iteration 143/1000 | Loss: 0.00001423
Iteration 144/1000 | Loss: 0.00001423
Iteration 145/1000 | Loss: 0.00001423
Iteration 146/1000 | Loss: 0.00001423
Iteration 147/1000 | Loss: 0.00001423
Iteration 148/1000 | Loss: 0.00001423
Iteration 149/1000 | Loss: 0.00001423
Iteration 150/1000 | Loss: 0.00001423
Iteration 151/1000 | Loss: 0.00001423
Iteration 152/1000 | Loss: 0.00001423
Iteration 153/1000 | Loss: 0.00001423
Iteration 154/1000 | Loss: 0.00001423
Iteration 155/1000 | Loss: 0.00001423
Iteration 156/1000 | Loss: 0.00001423
Iteration 157/1000 | Loss: 0.00001423
Iteration 158/1000 | Loss: 0.00001423
Iteration 159/1000 | Loss: 0.00001423
Iteration 160/1000 | Loss: 0.00001423
Iteration 161/1000 | Loss: 0.00001423
Iteration 162/1000 | Loss: 0.00001423
Iteration 163/1000 | Loss: 0.00001423
Iteration 164/1000 | Loss: 0.00001423
Iteration 165/1000 | Loss: 0.00001423
Iteration 166/1000 | Loss: 0.00001423
Iteration 167/1000 | Loss: 0.00001423
Iteration 168/1000 | Loss: 0.00001423
Iteration 169/1000 | Loss: 0.00001423
Iteration 170/1000 | Loss: 0.00001423
Iteration 171/1000 | Loss: 0.00001423
Iteration 172/1000 | Loss: 0.00001423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.4226931853045244e-05, 1.4226931853045244e-05, 1.4226931853045244e-05, 1.4226931853045244e-05, 1.4226931853045244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4226931853045244e-05

Optimization complete. Final v2v error: 3.20458722114563 mm

Highest mean error: 3.746778964996338 mm for frame 76

Lowest mean error: 2.9311013221740723 mm for frame 25

Saving results

Total time: 80.85368633270264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991260
Iteration 2/25 | Loss: 0.00209861
Iteration 3/25 | Loss: 0.00166238
Iteration 4/25 | Loss: 0.00152056
Iteration 5/25 | Loss: 0.00150949
Iteration 6/25 | Loss: 0.00147290
Iteration 7/25 | Loss: 0.00141175
Iteration 8/25 | Loss: 0.00134936
Iteration 9/25 | Loss: 0.00131709
Iteration 10/25 | Loss: 0.00130587
Iteration 11/25 | Loss: 0.00129960
Iteration 12/25 | Loss: 0.00129098
Iteration 13/25 | Loss: 0.00128557
Iteration 14/25 | Loss: 0.00128502
Iteration 15/25 | Loss: 0.00129292
Iteration 16/25 | Loss: 0.00128732
Iteration 17/25 | Loss: 0.00128060
Iteration 18/25 | Loss: 0.00127279
Iteration 19/25 | Loss: 0.00127108
Iteration 20/25 | Loss: 0.00127052
Iteration 21/25 | Loss: 0.00126693
Iteration 22/25 | Loss: 0.00126859
Iteration 23/25 | Loss: 0.00127073
Iteration 24/25 | Loss: 0.00127121
Iteration 25/25 | Loss: 0.00127032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31306446
Iteration 2/25 | Loss: 0.00267265
Iteration 3/25 | Loss: 0.00160579
Iteration 4/25 | Loss: 0.00160578
Iteration 5/25 | Loss: 0.00160578
Iteration 6/25 | Loss: 0.00160578
Iteration 7/25 | Loss: 0.00160578
Iteration 8/25 | Loss: 0.00160578
Iteration 9/25 | Loss: 0.00160578
Iteration 10/25 | Loss: 0.00160578
Iteration 11/25 | Loss: 0.00160578
Iteration 12/25 | Loss: 0.00160578
Iteration 13/25 | Loss: 0.00160578
Iteration 14/25 | Loss: 0.00160578
Iteration 15/25 | Loss: 0.00160578
Iteration 16/25 | Loss: 0.00160578
Iteration 17/25 | Loss: 0.00160578
Iteration 18/25 | Loss: 0.00160578
Iteration 19/25 | Loss: 0.00160578
Iteration 20/25 | Loss: 0.00160578
Iteration 21/25 | Loss: 0.00160578
Iteration 22/25 | Loss: 0.00160578
Iteration 23/25 | Loss: 0.00160578
Iteration 24/25 | Loss: 0.00160578
Iteration 25/25 | Loss: 0.00160578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160578
Iteration 2/1000 | Loss: 0.00216420
Iteration 3/1000 | Loss: 0.00082003
Iteration 4/1000 | Loss: 0.00030173
Iteration 5/1000 | Loss: 0.00035474
Iteration 6/1000 | Loss: 0.00017700
Iteration 7/1000 | Loss: 0.00022560
Iteration 8/1000 | Loss: 0.00031437
Iteration 9/1000 | Loss: 0.00022637
Iteration 10/1000 | Loss: 0.00024788
Iteration 11/1000 | Loss: 0.00015240
Iteration 12/1000 | Loss: 0.00011786
Iteration 13/1000 | Loss: 0.00016077
Iteration 14/1000 | Loss: 0.00013613
Iteration 15/1000 | Loss: 0.00014714
Iteration 16/1000 | Loss: 0.00012337
Iteration 17/1000 | Loss: 0.00014216
Iteration 18/1000 | Loss: 0.00016839
Iteration 19/1000 | Loss: 0.00017083
Iteration 20/1000 | Loss: 0.00013210
Iteration 21/1000 | Loss: 0.00012907
Iteration 22/1000 | Loss: 0.00012781
Iteration 23/1000 | Loss: 0.00015106
Iteration 24/1000 | Loss: 0.00014687
Iteration 25/1000 | Loss: 0.00014455
Iteration 26/1000 | Loss: 0.00015236
Iteration 27/1000 | Loss: 0.00017664
Iteration 28/1000 | Loss: 0.00019773
Iteration 29/1000 | Loss: 0.00025343
Iteration 30/1000 | Loss: 0.00048802
Iteration 31/1000 | Loss: 0.00002247
Iteration 32/1000 | Loss: 0.00001837
Iteration 33/1000 | Loss: 0.00001645
Iteration 34/1000 | Loss: 0.00001566
Iteration 35/1000 | Loss: 0.00001484
Iteration 36/1000 | Loss: 0.00001448
Iteration 37/1000 | Loss: 0.00001430
Iteration 38/1000 | Loss: 0.00001409
Iteration 39/1000 | Loss: 0.00001397
Iteration 40/1000 | Loss: 0.00001382
Iteration 41/1000 | Loss: 0.00001382
Iteration 42/1000 | Loss: 0.00001375
Iteration 43/1000 | Loss: 0.00001375
Iteration 44/1000 | Loss: 0.00001370
Iteration 45/1000 | Loss: 0.00001358
Iteration 46/1000 | Loss: 0.00001351
Iteration 47/1000 | Loss: 0.00001346
Iteration 48/1000 | Loss: 0.00001345
Iteration 49/1000 | Loss: 0.00001344
Iteration 50/1000 | Loss: 0.00001344
Iteration 51/1000 | Loss: 0.00001344
Iteration 52/1000 | Loss: 0.00001344
Iteration 53/1000 | Loss: 0.00001343
Iteration 54/1000 | Loss: 0.00001343
Iteration 55/1000 | Loss: 0.00001343
Iteration 56/1000 | Loss: 0.00001343
Iteration 57/1000 | Loss: 0.00001343
Iteration 58/1000 | Loss: 0.00001343
Iteration 59/1000 | Loss: 0.00001342
Iteration 60/1000 | Loss: 0.00001342
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001341
Iteration 64/1000 | Loss: 0.00001341
Iteration 65/1000 | Loss: 0.00001341
Iteration 66/1000 | Loss: 0.00001341
Iteration 67/1000 | Loss: 0.00001340
Iteration 68/1000 | Loss: 0.00001340
Iteration 69/1000 | Loss: 0.00001340
Iteration 70/1000 | Loss: 0.00001340
Iteration 71/1000 | Loss: 0.00001340
Iteration 72/1000 | Loss: 0.00001339
Iteration 73/1000 | Loss: 0.00001339
Iteration 74/1000 | Loss: 0.00001339
Iteration 75/1000 | Loss: 0.00001339
Iteration 76/1000 | Loss: 0.00001339
Iteration 77/1000 | Loss: 0.00001339
Iteration 78/1000 | Loss: 0.00001338
Iteration 79/1000 | Loss: 0.00001338
Iteration 80/1000 | Loss: 0.00001338
Iteration 81/1000 | Loss: 0.00001338
Iteration 82/1000 | Loss: 0.00001337
Iteration 83/1000 | Loss: 0.00001336
Iteration 84/1000 | Loss: 0.00001336
Iteration 85/1000 | Loss: 0.00001336
Iteration 86/1000 | Loss: 0.00001336
Iteration 87/1000 | Loss: 0.00001336
Iteration 88/1000 | Loss: 0.00001336
Iteration 89/1000 | Loss: 0.00001336
Iteration 90/1000 | Loss: 0.00001335
Iteration 91/1000 | Loss: 0.00001335
Iteration 92/1000 | Loss: 0.00001335
Iteration 93/1000 | Loss: 0.00001334
Iteration 94/1000 | Loss: 0.00001334
Iteration 95/1000 | Loss: 0.00001334
Iteration 96/1000 | Loss: 0.00001333
Iteration 97/1000 | Loss: 0.00001333
Iteration 98/1000 | Loss: 0.00001332
Iteration 99/1000 | Loss: 0.00001332
Iteration 100/1000 | Loss: 0.00001332
Iteration 101/1000 | Loss: 0.00001332
Iteration 102/1000 | Loss: 0.00001332
Iteration 103/1000 | Loss: 0.00001332
Iteration 104/1000 | Loss: 0.00001332
Iteration 105/1000 | Loss: 0.00001332
Iteration 106/1000 | Loss: 0.00001332
Iteration 107/1000 | Loss: 0.00001331
Iteration 108/1000 | Loss: 0.00001331
Iteration 109/1000 | Loss: 0.00001331
Iteration 110/1000 | Loss: 0.00001331
Iteration 111/1000 | Loss: 0.00001331
Iteration 112/1000 | Loss: 0.00001330
Iteration 113/1000 | Loss: 0.00001330
Iteration 114/1000 | Loss: 0.00001330
Iteration 115/1000 | Loss: 0.00001330
Iteration 116/1000 | Loss: 0.00001330
Iteration 117/1000 | Loss: 0.00001330
Iteration 118/1000 | Loss: 0.00001330
Iteration 119/1000 | Loss: 0.00001330
Iteration 120/1000 | Loss: 0.00001329
Iteration 121/1000 | Loss: 0.00001329
Iteration 122/1000 | Loss: 0.00001328
Iteration 123/1000 | Loss: 0.00001328
Iteration 124/1000 | Loss: 0.00001328
Iteration 125/1000 | Loss: 0.00001328
Iteration 126/1000 | Loss: 0.00001328
Iteration 127/1000 | Loss: 0.00001328
Iteration 128/1000 | Loss: 0.00001327
Iteration 129/1000 | Loss: 0.00001327
Iteration 130/1000 | Loss: 0.00001327
Iteration 131/1000 | Loss: 0.00001327
Iteration 132/1000 | Loss: 0.00001327
Iteration 133/1000 | Loss: 0.00001327
Iteration 134/1000 | Loss: 0.00001327
Iteration 135/1000 | Loss: 0.00001327
Iteration 136/1000 | Loss: 0.00001327
Iteration 137/1000 | Loss: 0.00001327
Iteration 138/1000 | Loss: 0.00001327
Iteration 139/1000 | Loss: 0.00001327
Iteration 140/1000 | Loss: 0.00001327
Iteration 141/1000 | Loss: 0.00001327
Iteration 142/1000 | Loss: 0.00001327
Iteration 143/1000 | Loss: 0.00001326
Iteration 144/1000 | Loss: 0.00001326
Iteration 145/1000 | Loss: 0.00001326
Iteration 146/1000 | Loss: 0.00001326
Iteration 147/1000 | Loss: 0.00001326
Iteration 148/1000 | Loss: 0.00001326
Iteration 149/1000 | Loss: 0.00001326
Iteration 150/1000 | Loss: 0.00001326
Iteration 151/1000 | Loss: 0.00001326
Iteration 152/1000 | Loss: 0.00001325
Iteration 153/1000 | Loss: 0.00001325
Iteration 154/1000 | Loss: 0.00001325
Iteration 155/1000 | Loss: 0.00001325
Iteration 156/1000 | Loss: 0.00001325
Iteration 157/1000 | Loss: 0.00001325
Iteration 158/1000 | Loss: 0.00001325
Iteration 159/1000 | Loss: 0.00001324
Iteration 160/1000 | Loss: 0.00001324
Iteration 161/1000 | Loss: 0.00001324
Iteration 162/1000 | Loss: 0.00001324
Iteration 163/1000 | Loss: 0.00001324
Iteration 164/1000 | Loss: 0.00001324
Iteration 165/1000 | Loss: 0.00001323
Iteration 166/1000 | Loss: 0.00001323
Iteration 167/1000 | Loss: 0.00001323
Iteration 168/1000 | Loss: 0.00001323
Iteration 169/1000 | Loss: 0.00001322
Iteration 170/1000 | Loss: 0.00001322
Iteration 171/1000 | Loss: 0.00001322
Iteration 172/1000 | Loss: 0.00001322
Iteration 173/1000 | Loss: 0.00001322
Iteration 174/1000 | Loss: 0.00001322
Iteration 175/1000 | Loss: 0.00001322
Iteration 176/1000 | Loss: 0.00001322
Iteration 177/1000 | Loss: 0.00001322
Iteration 178/1000 | Loss: 0.00001322
Iteration 179/1000 | Loss: 0.00001322
Iteration 180/1000 | Loss: 0.00001322
Iteration 181/1000 | Loss: 0.00001322
Iteration 182/1000 | Loss: 0.00001322
Iteration 183/1000 | Loss: 0.00001322
Iteration 184/1000 | Loss: 0.00001322
Iteration 185/1000 | Loss: 0.00001322
Iteration 186/1000 | Loss: 0.00001322
Iteration 187/1000 | Loss: 0.00001322
Iteration 188/1000 | Loss: 0.00001322
Iteration 189/1000 | Loss: 0.00001322
Iteration 190/1000 | Loss: 0.00001322
Iteration 191/1000 | Loss: 0.00001322
Iteration 192/1000 | Loss: 0.00001322
Iteration 193/1000 | Loss: 0.00001322
Iteration 194/1000 | Loss: 0.00001322
Iteration 195/1000 | Loss: 0.00001322
Iteration 196/1000 | Loss: 0.00001322
Iteration 197/1000 | Loss: 0.00001322
Iteration 198/1000 | Loss: 0.00001322
Iteration 199/1000 | Loss: 0.00001322
Iteration 200/1000 | Loss: 0.00001322
Iteration 201/1000 | Loss: 0.00001322
Iteration 202/1000 | Loss: 0.00001322
Iteration 203/1000 | Loss: 0.00001322
Iteration 204/1000 | Loss: 0.00001322
Iteration 205/1000 | Loss: 0.00001322
Iteration 206/1000 | Loss: 0.00001322
Iteration 207/1000 | Loss: 0.00001322
Iteration 208/1000 | Loss: 0.00001322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.321881154581206e-05, 1.321881154581206e-05, 1.321881154581206e-05, 1.321881154581206e-05, 1.321881154581206e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.321881154581206e-05

Optimization complete. Final v2v error: 3.0571229457855225 mm

Highest mean error: 4.313385486602783 mm for frame 63

Lowest mean error: 2.6154754161834717 mm for frame 113

Saving results

Total time: 113.3611192703247
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450728
Iteration 2/25 | Loss: 0.00143595
Iteration 3/25 | Loss: 0.00129073
Iteration 4/25 | Loss: 0.00126515
Iteration 5/25 | Loss: 0.00125937
Iteration 6/25 | Loss: 0.00125839
Iteration 7/25 | Loss: 0.00125839
Iteration 8/25 | Loss: 0.00125839
Iteration 9/25 | Loss: 0.00125839
Iteration 10/25 | Loss: 0.00125839
Iteration 11/25 | Loss: 0.00125839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012583922361955047, 0.0012583922361955047, 0.0012583922361955047, 0.0012583922361955047, 0.0012583922361955047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012583922361955047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19005144
Iteration 2/25 | Loss: 0.00216908
Iteration 3/25 | Loss: 0.00216907
Iteration 4/25 | Loss: 0.00216907
Iteration 5/25 | Loss: 0.00216907
Iteration 6/25 | Loss: 0.00216907
Iteration 7/25 | Loss: 0.00216907
Iteration 8/25 | Loss: 0.00216907
Iteration 9/25 | Loss: 0.00216907
Iteration 10/25 | Loss: 0.00216907
Iteration 11/25 | Loss: 0.00216907
Iteration 12/25 | Loss: 0.00216907
Iteration 13/25 | Loss: 0.00216907
Iteration 14/25 | Loss: 0.00216907
Iteration 15/25 | Loss: 0.00216907
Iteration 16/25 | Loss: 0.00216907
Iteration 17/25 | Loss: 0.00216907
Iteration 18/25 | Loss: 0.00216907
Iteration 19/25 | Loss: 0.00216907
Iteration 20/25 | Loss: 0.00216907
Iteration 21/25 | Loss: 0.00216907
Iteration 22/25 | Loss: 0.00216907
Iteration 23/25 | Loss: 0.00216907
Iteration 24/25 | Loss: 0.00216907
Iteration 25/25 | Loss: 0.00216907
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.002169068204239011, 0.002169068204239011, 0.002169068204239011, 0.002169068204239011, 0.002169068204239011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002169068204239011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216907
Iteration 2/1000 | Loss: 0.00004822
Iteration 3/1000 | Loss: 0.00003001
Iteration 4/1000 | Loss: 0.00002303
Iteration 5/1000 | Loss: 0.00002105
Iteration 6/1000 | Loss: 0.00001943
Iteration 7/1000 | Loss: 0.00001851
Iteration 8/1000 | Loss: 0.00001789
Iteration 9/1000 | Loss: 0.00001740
Iteration 10/1000 | Loss: 0.00001684
Iteration 11/1000 | Loss: 0.00001650
Iteration 12/1000 | Loss: 0.00001637
Iteration 13/1000 | Loss: 0.00001629
Iteration 14/1000 | Loss: 0.00001607
Iteration 15/1000 | Loss: 0.00001588
Iteration 16/1000 | Loss: 0.00001583
Iteration 17/1000 | Loss: 0.00001581
Iteration 18/1000 | Loss: 0.00001579
Iteration 19/1000 | Loss: 0.00001576
Iteration 20/1000 | Loss: 0.00001574
Iteration 21/1000 | Loss: 0.00001563
Iteration 22/1000 | Loss: 0.00001562
Iteration 23/1000 | Loss: 0.00001562
Iteration 24/1000 | Loss: 0.00001558
Iteration 25/1000 | Loss: 0.00001553
Iteration 26/1000 | Loss: 0.00001553
Iteration 27/1000 | Loss: 0.00001552
Iteration 28/1000 | Loss: 0.00001551
Iteration 29/1000 | Loss: 0.00001551
Iteration 30/1000 | Loss: 0.00001549
Iteration 31/1000 | Loss: 0.00001549
Iteration 32/1000 | Loss: 0.00001549
Iteration 33/1000 | Loss: 0.00001549
Iteration 34/1000 | Loss: 0.00001549
Iteration 35/1000 | Loss: 0.00001547
Iteration 36/1000 | Loss: 0.00001547
Iteration 37/1000 | Loss: 0.00001542
Iteration 38/1000 | Loss: 0.00001540
Iteration 39/1000 | Loss: 0.00001540
Iteration 40/1000 | Loss: 0.00001539
Iteration 41/1000 | Loss: 0.00001539
Iteration 42/1000 | Loss: 0.00001539
Iteration 43/1000 | Loss: 0.00001538
Iteration 44/1000 | Loss: 0.00001537
Iteration 45/1000 | Loss: 0.00001537
Iteration 46/1000 | Loss: 0.00001537
Iteration 47/1000 | Loss: 0.00001536
Iteration 48/1000 | Loss: 0.00001536
Iteration 49/1000 | Loss: 0.00001536
Iteration 50/1000 | Loss: 0.00001536
Iteration 51/1000 | Loss: 0.00001536
Iteration 52/1000 | Loss: 0.00001536
Iteration 53/1000 | Loss: 0.00001536
Iteration 54/1000 | Loss: 0.00001536
Iteration 55/1000 | Loss: 0.00001536
Iteration 56/1000 | Loss: 0.00001535
Iteration 57/1000 | Loss: 0.00001534
Iteration 58/1000 | Loss: 0.00001534
Iteration 59/1000 | Loss: 0.00001533
Iteration 60/1000 | Loss: 0.00001532
Iteration 61/1000 | Loss: 0.00001532
Iteration 62/1000 | Loss: 0.00001532
Iteration 63/1000 | Loss: 0.00001531
Iteration 64/1000 | Loss: 0.00001531
Iteration 65/1000 | Loss: 0.00001531
Iteration 66/1000 | Loss: 0.00001530
Iteration 67/1000 | Loss: 0.00001529
Iteration 68/1000 | Loss: 0.00001529
Iteration 69/1000 | Loss: 0.00001528
Iteration 70/1000 | Loss: 0.00001528
Iteration 71/1000 | Loss: 0.00001527
Iteration 72/1000 | Loss: 0.00001526
Iteration 73/1000 | Loss: 0.00001525
Iteration 74/1000 | Loss: 0.00001524
Iteration 75/1000 | Loss: 0.00001524
Iteration 76/1000 | Loss: 0.00001524
Iteration 77/1000 | Loss: 0.00001523
Iteration 78/1000 | Loss: 0.00001523
Iteration 79/1000 | Loss: 0.00001522
Iteration 80/1000 | Loss: 0.00001521
Iteration 81/1000 | Loss: 0.00001521
Iteration 82/1000 | Loss: 0.00001521
Iteration 83/1000 | Loss: 0.00001521
Iteration 84/1000 | Loss: 0.00001520
Iteration 85/1000 | Loss: 0.00001520
Iteration 86/1000 | Loss: 0.00001520
Iteration 87/1000 | Loss: 0.00001519
Iteration 88/1000 | Loss: 0.00001518
Iteration 89/1000 | Loss: 0.00001518
Iteration 90/1000 | Loss: 0.00001517
Iteration 91/1000 | Loss: 0.00001517
Iteration 92/1000 | Loss: 0.00001517
Iteration 93/1000 | Loss: 0.00001516
Iteration 94/1000 | Loss: 0.00001516
Iteration 95/1000 | Loss: 0.00001516
Iteration 96/1000 | Loss: 0.00001516
Iteration 97/1000 | Loss: 0.00001516
Iteration 98/1000 | Loss: 0.00001515
Iteration 99/1000 | Loss: 0.00001515
Iteration 100/1000 | Loss: 0.00001515
Iteration 101/1000 | Loss: 0.00001515
Iteration 102/1000 | Loss: 0.00001515
Iteration 103/1000 | Loss: 0.00001515
Iteration 104/1000 | Loss: 0.00001514
Iteration 105/1000 | Loss: 0.00001514
Iteration 106/1000 | Loss: 0.00001513
Iteration 107/1000 | Loss: 0.00001513
Iteration 108/1000 | Loss: 0.00001513
Iteration 109/1000 | Loss: 0.00001513
Iteration 110/1000 | Loss: 0.00001512
Iteration 111/1000 | Loss: 0.00001512
Iteration 112/1000 | Loss: 0.00001512
Iteration 113/1000 | Loss: 0.00001512
Iteration 114/1000 | Loss: 0.00001511
Iteration 115/1000 | Loss: 0.00001511
Iteration 116/1000 | Loss: 0.00001511
Iteration 117/1000 | Loss: 0.00001511
Iteration 118/1000 | Loss: 0.00001511
Iteration 119/1000 | Loss: 0.00001511
Iteration 120/1000 | Loss: 0.00001511
Iteration 121/1000 | Loss: 0.00001511
Iteration 122/1000 | Loss: 0.00001511
Iteration 123/1000 | Loss: 0.00001510
Iteration 124/1000 | Loss: 0.00001510
Iteration 125/1000 | Loss: 0.00001510
Iteration 126/1000 | Loss: 0.00001510
Iteration 127/1000 | Loss: 0.00001509
Iteration 128/1000 | Loss: 0.00001509
Iteration 129/1000 | Loss: 0.00001509
Iteration 130/1000 | Loss: 0.00001509
Iteration 131/1000 | Loss: 0.00001508
Iteration 132/1000 | Loss: 0.00001508
Iteration 133/1000 | Loss: 0.00001507
Iteration 134/1000 | Loss: 0.00001507
Iteration 135/1000 | Loss: 0.00001507
Iteration 136/1000 | Loss: 0.00001507
Iteration 137/1000 | Loss: 0.00001507
Iteration 138/1000 | Loss: 0.00001507
Iteration 139/1000 | Loss: 0.00001507
Iteration 140/1000 | Loss: 0.00001507
Iteration 141/1000 | Loss: 0.00001507
Iteration 142/1000 | Loss: 0.00001506
Iteration 143/1000 | Loss: 0.00001506
Iteration 144/1000 | Loss: 0.00001506
Iteration 145/1000 | Loss: 0.00001506
Iteration 146/1000 | Loss: 0.00001506
Iteration 147/1000 | Loss: 0.00001506
Iteration 148/1000 | Loss: 0.00001505
Iteration 149/1000 | Loss: 0.00001505
Iteration 150/1000 | Loss: 0.00001505
Iteration 151/1000 | Loss: 0.00001505
Iteration 152/1000 | Loss: 0.00001505
Iteration 153/1000 | Loss: 0.00001505
Iteration 154/1000 | Loss: 0.00001505
Iteration 155/1000 | Loss: 0.00001504
Iteration 156/1000 | Loss: 0.00001504
Iteration 157/1000 | Loss: 0.00001504
Iteration 158/1000 | Loss: 0.00001504
Iteration 159/1000 | Loss: 0.00001504
Iteration 160/1000 | Loss: 0.00001504
Iteration 161/1000 | Loss: 0.00001504
Iteration 162/1000 | Loss: 0.00001504
Iteration 163/1000 | Loss: 0.00001504
Iteration 164/1000 | Loss: 0.00001504
Iteration 165/1000 | Loss: 0.00001503
Iteration 166/1000 | Loss: 0.00001503
Iteration 167/1000 | Loss: 0.00001503
Iteration 168/1000 | Loss: 0.00001503
Iteration 169/1000 | Loss: 0.00001503
Iteration 170/1000 | Loss: 0.00001503
Iteration 171/1000 | Loss: 0.00001503
Iteration 172/1000 | Loss: 0.00001503
Iteration 173/1000 | Loss: 0.00001503
Iteration 174/1000 | Loss: 0.00001502
Iteration 175/1000 | Loss: 0.00001502
Iteration 176/1000 | Loss: 0.00001502
Iteration 177/1000 | Loss: 0.00001502
Iteration 178/1000 | Loss: 0.00001502
Iteration 179/1000 | Loss: 0.00001502
Iteration 180/1000 | Loss: 0.00001502
Iteration 181/1000 | Loss: 0.00001502
Iteration 182/1000 | Loss: 0.00001501
Iteration 183/1000 | Loss: 0.00001501
Iteration 184/1000 | Loss: 0.00001501
Iteration 185/1000 | Loss: 0.00001501
Iteration 186/1000 | Loss: 0.00001501
Iteration 187/1000 | Loss: 0.00001501
Iteration 188/1000 | Loss: 0.00001501
Iteration 189/1000 | Loss: 0.00001501
Iteration 190/1000 | Loss: 0.00001501
Iteration 191/1000 | Loss: 0.00001501
Iteration 192/1000 | Loss: 0.00001501
Iteration 193/1000 | Loss: 0.00001501
Iteration 194/1000 | Loss: 0.00001501
Iteration 195/1000 | Loss: 0.00001501
Iteration 196/1000 | Loss: 0.00001501
Iteration 197/1000 | Loss: 0.00001500
Iteration 198/1000 | Loss: 0.00001500
Iteration 199/1000 | Loss: 0.00001500
Iteration 200/1000 | Loss: 0.00001500
Iteration 201/1000 | Loss: 0.00001500
Iteration 202/1000 | Loss: 0.00001500
Iteration 203/1000 | Loss: 0.00001500
Iteration 204/1000 | Loss: 0.00001500
Iteration 205/1000 | Loss: 0.00001500
Iteration 206/1000 | Loss: 0.00001500
Iteration 207/1000 | Loss: 0.00001500
Iteration 208/1000 | Loss: 0.00001500
Iteration 209/1000 | Loss: 0.00001500
Iteration 210/1000 | Loss: 0.00001500
Iteration 211/1000 | Loss: 0.00001500
Iteration 212/1000 | Loss: 0.00001500
Iteration 213/1000 | Loss: 0.00001500
Iteration 214/1000 | Loss: 0.00001500
Iteration 215/1000 | Loss: 0.00001500
Iteration 216/1000 | Loss: 0.00001499
Iteration 217/1000 | Loss: 0.00001499
Iteration 218/1000 | Loss: 0.00001499
Iteration 219/1000 | Loss: 0.00001499
Iteration 220/1000 | Loss: 0.00001499
Iteration 221/1000 | Loss: 0.00001499
Iteration 222/1000 | Loss: 0.00001499
Iteration 223/1000 | Loss: 0.00001499
Iteration 224/1000 | Loss: 0.00001499
Iteration 225/1000 | Loss: 0.00001499
Iteration 226/1000 | Loss: 0.00001499
Iteration 227/1000 | Loss: 0.00001499
Iteration 228/1000 | Loss: 0.00001499
Iteration 229/1000 | Loss: 0.00001499
Iteration 230/1000 | Loss: 0.00001499
Iteration 231/1000 | Loss: 0.00001499
Iteration 232/1000 | Loss: 0.00001499
Iteration 233/1000 | Loss: 0.00001499
Iteration 234/1000 | Loss: 0.00001499
Iteration 235/1000 | Loss: 0.00001499
Iteration 236/1000 | Loss: 0.00001499
Iteration 237/1000 | Loss: 0.00001499
Iteration 238/1000 | Loss: 0.00001499
Iteration 239/1000 | Loss: 0.00001499
Iteration 240/1000 | Loss: 0.00001499
Iteration 241/1000 | Loss: 0.00001499
Iteration 242/1000 | Loss: 0.00001499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.4989114788477309e-05, 1.4989114788477309e-05, 1.4989114788477309e-05, 1.4989114788477309e-05, 1.4989114788477309e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4989114788477309e-05

Optimization complete. Final v2v error: 3.2702553272247314 mm

Highest mean error: 3.918428897857666 mm for frame 250

Lowest mean error: 2.86594820022583 mm for frame 23

Saving results

Total time: 55.95223784446716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811723
Iteration 2/25 | Loss: 0.00139471
Iteration 3/25 | Loss: 0.00126555
Iteration 4/25 | Loss: 0.00124780
Iteration 5/25 | Loss: 0.00123877
Iteration 6/25 | Loss: 0.00123724
Iteration 7/25 | Loss: 0.00123599
Iteration 8/25 | Loss: 0.00123652
Iteration 9/25 | Loss: 0.00123519
Iteration 10/25 | Loss: 0.00123467
Iteration 11/25 | Loss: 0.00123450
Iteration 12/25 | Loss: 0.00123445
Iteration 13/25 | Loss: 0.00123445
Iteration 14/25 | Loss: 0.00123445
Iteration 15/25 | Loss: 0.00123444
Iteration 16/25 | Loss: 0.00123444
Iteration 17/25 | Loss: 0.00123444
Iteration 18/25 | Loss: 0.00123444
Iteration 19/25 | Loss: 0.00123444
Iteration 20/25 | Loss: 0.00123444
Iteration 21/25 | Loss: 0.00123443
Iteration 22/25 | Loss: 0.00123443
Iteration 23/25 | Loss: 0.00123443
Iteration 24/25 | Loss: 0.00123443
Iteration 25/25 | Loss: 0.00123443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.55663490
Iteration 2/25 | Loss: 0.00130520
Iteration 3/25 | Loss: 0.00130518
Iteration 4/25 | Loss: 0.00130518
Iteration 5/25 | Loss: 0.00130518
Iteration 6/25 | Loss: 0.00130518
Iteration 7/25 | Loss: 0.00130518
Iteration 8/25 | Loss: 0.00130518
Iteration 9/25 | Loss: 0.00130518
Iteration 10/25 | Loss: 0.00130518
Iteration 11/25 | Loss: 0.00130518
Iteration 12/25 | Loss: 0.00130518
Iteration 13/25 | Loss: 0.00130518
Iteration 14/25 | Loss: 0.00130518
Iteration 15/25 | Loss: 0.00130518
Iteration 16/25 | Loss: 0.00130518
Iteration 17/25 | Loss: 0.00130518
Iteration 18/25 | Loss: 0.00130518
Iteration 19/25 | Loss: 0.00130518
Iteration 20/25 | Loss: 0.00130518
Iteration 21/25 | Loss: 0.00130518
Iteration 22/25 | Loss: 0.00130518
Iteration 23/25 | Loss: 0.00130518
Iteration 24/25 | Loss: 0.00130518
Iteration 25/25 | Loss: 0.00130518

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130518
Iteration 2/1000 | Loss: 0.00003034
Iteration 3/1000 | Loss: 0.00002155
Iteration 4/1000 | Loss: 0.00001835
Iteration 5/1000 | Loss: 0.00001734
Iteration 6/1000 | Loss: 0.00001654
Iteration 7/1000 | Loss: 0.00001592
Iteration 8/1000 | Loss: 0.00001552
Iteration 9/1000 | Loss: 0.00001513
Iteration 10/1000 | Loss: 0.00001481
Iteration 11/1000 | Loss: 0.00001460
Iteration 12/1000 | Loss: 0.00001457
Iteration 13/1000 | Loss: 0.00001440
Iteration 14/1000 | Loss: 0.00001429
Iteration 15/1000 | Loss: 0.00001428
Iteration 16/1000 | Loss: 0.00001426
Iteration 17/1000 | Loss: 0.00001426
Iteration 18/1000 | Loss: 0.00001424
Iteration 19/1000 | Loss: 0.00001422
Iteration 20/1000 | Loss: 0.00001421
Iteration 21/1000 | Loss: 0.00001419
Iteration 22/1000 | Loss: 0.00001415
Iteration 23/1000 | Loss: 0.00001411
Iteration 24/1000 | Loss: 0.00001407
Iteration 25/1000 | Loss: 0.00001407
Iteration 26/1000 | Loss: 0.00001406
Iteration 27/1000 | Loss: 0.00001406
Iteration 28/1000 | Loss: 0.00001406
Iteration 29/1000 | Loss: 0.00001406
Iteration 30/1000 | Loss: 0.00001406
Iteration 31/1000 | Loss: 0.00001406
Iteration 32/1000 | Loss: 0.00001406
Iteration 33/1000 | Loss: 0.00001406
Iteration 34/1000 | Loss: 0.00001406
Iteration 35/1000 | Loss: 0.00001406
Iteration 36/1000 | Loss: 0.00001405
Iteration 37/1000 | Loss: 0.00001405
Iteration 38/1000 | Loss: 0.00001405
Iteration 39/1000 | Loss: 0.00001405
Iteration 40/1000 | Loss: 0.00001404
Iteration 41/1000 | Loss: 0.00001404
Iteration 42/1000 | Loss: 0.00001403
Iteration 43/1000 | Loss: 0.00001402
Iteration 44/1000 | Loss: 0.00001402
Iteration 45/1000 | Loss: 0.00001401
Iteration 46/1000 | Loss: 0.00001401
Iteration 47/1000 | Loss: 0.00001400
Iteration 48/1000 | Loss: 0.00001400
Iteration 49/1000 | Loss: 0.00001400
Iteration 50/1000 | Loss: 0.00001399
Iteration 51/1000 | Loss: 0.00001399
Iteration 52/1000 | Loss: 0.00001399
Iteration 53/1000 | Loss: 0.00001399
Iteration 54/1000 | Loss: 0.00001398
Iteration 55/1000 | Loss: 0.00001397
Iteration 56/1000 | Loss: 0.00001397
Iteration 57/1000 | Loss: 0.00001397
Iteration 58/1000 | Loss: 0.00001396
Iteration 59/1000 | Loss: 0.00001396
Iteration 60/1000 | Loss: 0.00001396
Iteration 61/1000 | Loss: 0.00001396
Iteration 62/1000 | Loss: 0.00001396
Iteration 63/1000 | Loss: 0.00001396
Iteration 64/1000 | Loss: 0.00001396
Iteration 65/1000 | Loss: 0.00001395
Iteration 66/1000 | Loss: 0.00001395
Iteration 67/1000 | Loss: 0.00001394
Iteration 68/1000 | Loss: 0.00001394
Iteration 69/1000 | Loss: 0.00001394
Iteration 70/1000 | Loss: 0.00001393
Iteration 71/1000 | Loss: 0.00001393
Iteration 72/1000 | Loss: 0.00001393
Iteration 73/1000 | Loss: 0.00001393
Iteration 74/1000 | Loss: 0.00001393
Iteration 75/1000 | Loss: 0.00001393
Iteration 76/1000 | Loss: 0.00001393
Iteration 77/1000 | Loss: 0.00001393
Iteration 78/1000 | Loss: 0.00001392
Iteration 79/1000 | Loss: 0.00001392
Iteration 80/1000 | Loss: 0.00001392
Iteration 81/1000 | Loss: 0.00001392
Iteration 82/1000 | Loss: 0.00001392
Iteration 83/1000 | Loss: 0.00001392
Iteration 84/1000 | Loss: 0.00001392
Iteration 85/1000 | Loss: 0.00001392
Iteration 86/1000 | Loss: 0.00001392
Iteration 87/1000 | Loss: 0.00001392
Iteration 88/1000 | Loss: 0.00001391
Iteration 89/1000 | Loss: 0.00001391
Iteration 90/1000 | Loss: 0.00001391
Iteration 91/1000 | Loss: 0.00001390
Iteration 92/1000 | Loss: 0.00001390
Iteration 93/1000 | Loss: 0.00001390
Iteration 94/1000 | Loss: 0.00001389
Iteration 95/1000 | Loss: 0.00001389
Iteration 96/1000 | Loss: 0.00001389
Iteration 97/1000 | Loss: 0.00001388
Iteration 98/1000 | Loss: 0.00001388
Iteration 99/1000 | Loss: 0.00001388
Iteration 100/1000 | Loss: 0.00001388
Iteration 101/1000 | Loss: 0.00001388
Iteration 102/1000 | Loss: 0.00001388
Iteration 103/1000 | Loss: 0.00001388
Iteration 104/1000 | Loss: 0.00001388
Iteration 105/1000 | Loss: 0.00001387
Iteration 106/1000 | Loss: 0.00001387
Iteration 107/1000 | Loss: 0.00001387
Iteration 108/1000 | Loss: 0.00001387
Iteration 109/1000 | Loss: 0.00001387
Iteration 110/1000 | Loss: 0.00001387
Iteration 111/1000 | Loss: 0.00001387
Iteration 112/1000 | Loss: 0.00001387
Iteration 113/1000 | Loss: 0.00001387
Iteration 114/1000 | Loss: 0.00001387
Iteration 115/1000 | Loss: 0.00001387
Iteration 116/1000 | Loss: 0.00001387
Iteration 117/1000 | Loss: 0.00001387
Iteration 118/1000 | Loss: 0.00001387
Iteration 119/1000 | Loss: 0.00001387
Iteration 120/1000 | Loss: 0.00001387
Iteration 121/1000 | Loss: 0.00001387
Iteration 122/1000 | Loss: 0.00001387
Iteration 123/1000 | Loss: 0.00001387
Iteration 124/1000 | Loss: 0.00001387
Iteration 125/1000 | Loss: 0.00001387
Iteration 126/1000 | Loss: 0.00001387
Iteration 127/1000 | Loss: 0.00001387
Iteration 128/1000 | Loss: 0.00001387
Iteration 129/1000 | Loss: 0.00001387
Iteration 130/1000 | Loss: 0.00001387
Iteration 131/1000 | Loss: 0.00001387
Iteration 132/1000 | Loss: 0.00001387
Iteration 133/1000 | Loss: 0.00001387
Iteration 134/1000 | Loss: 0.00001387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.3867578672943637e-05, 1.3867578672943637e-05, 1.3867578672943637e-05, 1.3867578672943637e-05, 1.3867578672943637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3867578672943637e-05

Optimization complete. Final v2v error: 3.1837568283081055 mm

Highest mean error: 3.5994811058044434 mm for frame 204

Lowest mean error: 2.8984456062316895 mm for frame 107

Saving results

Total time: 54.30707097053528
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00901202
Iteration 2/25 | Loss: 0.00141223
Iteration 3/25 | Loss: 0.00129133
Iteration 4/25 | Loss: 0.00127105
Iteration 5/25 | Loss: 0.00126447
Iteration 6/25 | Loss: 0.00126278
Iteration 7/25 | Loss: 0.00126278
Iteration 8/25 | Loss: 0.00126278
Iteration 9/25 | Loss: 0.00126278
Iteration 10/25 | Loss: 0.00126278
Iteration 11/25 | Loss: 0.00126278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012627755058929324, 0.0012627755058929324, 0.0012627755058929324, 0.0012627755058929324, 0.0012627755058929324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012627755058929324

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29350460
Iteration 2/25 | Loss: 0.00149592
Iteration 3/25 | Loss: 0.00149591
Iteration 4/25 | Loss: 0.00149591
Iteration 5/25 | Loss: 0.00149591
Iteration 6/25 | Loss: 0.00149591
Iteration 7/25 | Loss: 0.00149591
Iteration 8/25 | Loss: 0.00149591
Iteration 9/25 | Loss: 0.00149591
Iteration 10/25 | Loss: 0.00149591
Iteration 11/25 | Loss: 0.00149591
Iteration 12/25 | Loss: 0.00149591
Iteration 13/25 | Loss: 0.00149591
Iteration 14/25 | Loss: 0.00149591
Iteration 15/25 | Loss: 0.00149591
Iteration 16/25 | Loss: 0.00149591
Iteration 17/25 | Loss: 0.00149591
Iteration 18/25 | Loss: 0.00149591
Iteration 19/25 | Loss: 0.00149591
Iteration 20/25 | Loss: 0.00149591
Iteration 21/25 | Loss: 0.00149591
Iteration 22/25 | Loss: 0.00149591
Iteration 23/25 | Loss: 0.00149591
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014959092950448394, 0.0014959092950448394, 0.0014959092950448394, 0.0014959092950448394, 0.0014959092950448394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014959092950448394

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149591
Iteration 2/1000 | Loss: 0.00004677
Iteration 3/1000 | Loss: 0.00003238
Iteration 4/1000 | Loss: 0.00002618
Iteration 5/1000 | Loss: 0.00002423
Iteration 6/1000 | Loss: 0.00002301
Iteration 7/1000 | Loss: 0.00002236
Iteration 8/1000 | Loss: 0.00002173
Iteration 9/1000 | Loss: 0.00002112
Iteration 10/1000 | Loss: 0.00002084
Iteration 11/1000 | Loss: 0.00002058
Iteration 12/1000 | Loss: 0.00002051
Iteration 13/1000 | Loss: 0.00002036
Iteration 14/1000 | Loss: 0.00002032
Iteration 15/1000 | Loss: 0.00002028
Iteration 16/1000 | Loss: 0.00002026
Iteration 17/1000 | Loss: 0.00002022
Iteration 18/1000 | Loss: 0.00002019
Iteration 19/1000 | Loss: 0.00002017
Iteration 20/1000 | Loss: 0.00002001
Iteration 21/1000 | Loss: 0.00001998
Iteration 22/1000 | Loss: 0.00001990
Iteration 23/1000 | Loss: 0.00001987
Iteration 24/1000 | Loss: 0.00001986
Iteration 25/1000 | Loss: 0.00001982
Iteration 26/1000 | Loss: 0.00001982
Iteration 27/1000 | Loss: 0.00001980
Iteration 28/1000 | Loss: 0.00001978
Iteration 29/1000 | Loss: 0.00001974
Iteration 30/1000 | Loss: 0.00001971
Iteration 31/1000 | Loss: 0.00001971
Iteration 32/1000 | Loss: 0.00001970
Iteration 33/1000 | Loss: 0.00001970
Iteration 34/1000 | Loss: 0.00001968
Iteration 35/1000 | Loss: 0.00001968
Iteration 36/1000 | Loss: 0.00001967
Iteration 37/1000 | Loss: 0.00001967
Iteration 38/1000 | Loss: 0.00001966
Iteration 39/1000 | Loss: 0.00001962
Iteration 40/1000 | Loss: 0.00001960
Iteration 41/1000 | Loss: 0.00001960
Iteration 42/1000 | Loss: 0.00001959
Iteration 43/1000 | Loss: 0.00001959
Iteration 44/1000 | Loss: 0.00001959
Iteration 45/1000 | Loss: 0.00001958
Iteration 46/1000 | Loss: 0.00001958
Iteration 47/1000 | Loss: 0.00001957
Iteration 48/1000 | Loss: 0.00001957
Iteration 49/1000 | Loss: 0.00001957
Iteration 50/1000 | Loss: 0.00001956
Iteration 51/1000 | Loss: 0.00001956
Iteration 52/1000 | Loss: 0.00001955
Iteration 53/1000 | Loss: 0.00001955
Iteration 54/1000 | Loss: 0.00001954
Iteration 55/1000 | Loss: 0.00001954
Iteration 56/1000 | Loss: 0.00001954
Iteration 57/1000 | Loss: 0.00001954
Iteration 58/1000 | Loss: 0.00001954
Iteration 59/1000 | Loss: 0.00001953
Iteration 60/1000 | Loss: 0.00001953
Iteration 61/1000 | Loss: 0.00001953
Iteration 62/1000 | Loss: 0.00001952
Iteration 63/1000 | Loss: 0.00001952
Iteration 64/1000 | Loss: 0.00001952
Iteration 65/1000 | Loss: 0.00001951
Iteration 66/1000 | Loss: 0.00001951
Iteration 67/1000 | Loss: 0.00001951
Iteration 68/1000 | Loss: 0.00001951
Iteration 69/1000 | Loss: 0.00001950
Iteration 70/1000 | Loss: 0.00001950
Iteration 71/1000 | Loss: 0.00001950
Iteration 72/1000 | Loss: 0.00001950
Iteration 73/1000 | Loss: 0.00001950
Iteration 74/1000 | Loss: 0.00001950
Iteration 75/1000 | Loss: 0.00001950
Iteration 76/1000 | Loss: 0.00001950
Iteration 77/1000 | Loss: 0.00001949
Iteration 78/1000 | Loss: 0.00001949
Iteration 79/1000 | Loss: 0.00001949
Iteration 80/1000 | Loss: 0.00001949
Iteration 81/1000 | Loss: 0.00001949
Iteration 82/1000 | Loss: 0.00001949
Iteration 83/1000 | Loss: 0.00001948
Iteration 84/1000 | Loss: 0.00001948
Iteration 85/1000 | Loss: 0.00001948
Iteration 86/1000 | Loss: 0.00001948
Iteration 87/1000 | Loss: 0.00001948
Iteration 88/1000 | Loss: 0.00001948
Iteration 89/1000 | Loss: 0.00001947
Iteration 90/1000 | Loss: 0.00001947
Iteration 91/1000 | Loss: 0.00001947
Iteration 92/1000 | Loss: 0.00001947
Iteration 93/1000 | Loss: 0.00001947
Iteration 94/1000 | Loss: 0.00001947
Iteration 95/1000 | Loss: 0.00001947
Iteration 96/1000 | Loss: 0.00001947
Iteration 97/1000 | Loss: 0.00001947
Iteration 98/1000 | Loss: 0.00001947
Iteration 99/1000 | Loss: 0.00001946
Iteration 100/1000 | Loss: 0.00001946
Iteration 101/1000 | Loss: 0.00001946
Iteration 102/1000 | Loss: 0.00001945
Iteration 103/1000 | Loss: 0.00001945
Iteration 104/1000 | Loss: 0.00001945
Iteration 105/1000 | Loss: 0.00001944
Iteration 106/1000 | Loss: 0.00001944
Iteration 107/1000 | Loss: 0.00001944
Iteration 108/1000 | Loss: 0.00001944
Iteration 109/1000 | Loss: 0.00001943
Iteration 110/1000 | Loss: 0.00001943
Iteration 111/1000 | Loss: 0.00001943
Iteration 112/1000 | Loss: 0.00001943
Iteration 113/1000 | Loss: 0.00001943
Iteration 114/1000 | Loss: 0.00001943
Iteration 115/1000 | Loss: 0.00001942
Iteration 116/1000 | Loss: 0.00001942
Iteration 117/1000 | Loss: 0.00001942
Iteration 118/1000 | Loss: 0.00001942
Iteration 119/1000 | Loss: 0.00001942
Iteration 120/1000 | Loss: 0.00001942
Iteration 121/1000 | Loss: 0.00001942
Iteration 122/1000 | Loss: 0.00001941
Iteration 123/1000 | Loss: 0.00001941
Iteration 124/1000 | Loss: 0.00001941
Iteration 125/1000 | Loss: 0.00001941
Iteration 126/1000 | Loss: 0.00001941
Iteration 127/1000 | Loss: 0.00001941
Iteration 128/1000 | Loss: 0.00001940
Iteration 129/1000 | Loss: 0.00001940
Iteration 130/1000 | Loss: 0.00001940
Iteration 131/1000 | Loss: 0.00001940
Iteration 132/1000 | Loss: 0.00001940
Iteration 133/1000 | Loss: 0.00001940
Iteration 134/1000 | Loss: 0.00001939
Iteration 135/1000 | Loss: 0.00001939
Iteration 136/1000 | Loss: 0.00001939
Iteration 137/1000 | Loss: 0.00001939
Iteration 138/1000 | Loss: 0.00001939
Iteration 139/1000 | Loss: 0.00001938
Iteration 140/1000 | Loss: 0.00001938
Iteration 141/1000 | Loss: 0.00001938
Iteration 142/1000 | Loss: 0.00001938
Iteration 143/1000 | Loss: 0.00001937
Iteration 144/1000 | Loss: 0.00001937
Iteration 145/1000 | Loss: 0.00001937
Iteration 146/1000 | Loss: 0.00001937
Iteration 147/1000 | Loss: 0.00001937
Iteration 148/1000 | Loss: 0.00001937
Iteration 149/1000 | Loss: 0.00001937
Iteration 150/1000 | Loss: 0.00001937
Iteration 151/1000 | Loss: 0.00001937
Iteration 152/1000 | Loss: 0.00001937
Iteration 153/1000 | Loss: 0.00001937
Iteration 154/1000 | Loss: 0.00001937
Iteration 155/1000 | Loss: 0.00001936
Iteration 156/1000 | Loss: 0.00001936
Iteration 157/1000 | Loss: 0.00001936
Iteration 158/1000 | Loss: 0.00001936
Iteration 159/1000 | Loss: 0.00001936
Iteration 160/1000 | Loss: 0.00001936
Iteration 161/1000 | Loss: 0.00001936
Iteration 162/1000 | Loss: 0.00001936
Iteration 163/1000 | Loss: 0.00001936
Iteration 164/1000 | Loss: 0.00001936
Iteration 165/1000 | Loss: 0.00001936
Iteration 166/1000 | Loss: 0.00001936
Iteration 167/1000 | Loss: 0.00001936
Iteration 168/1000 | Loss: 0.00001936
Iteration 169/1000 | Loss: 0.00001935
Iteration 170/1000 | Loss: 0.00001935
Iteration 171/1000 | Loss: 0.00001935
Iteration 172/1000 | Loss: 0.00001935
Iteration 173/1000 | Loss: 0.00001935
Iteration 174/1000 | Loss: 0.00001935
Iteration 175/1000 | Loss: 0.00001935
Iteration 176/1000 | Loss: 0.00001935
Iteration 177/1000 | Loss: 0.00001935
Iteration 178/1000 | Loss: 0.00001935
Iteration 179/1000 | Loss: 0.00001935
Iteration 180/1000 | Loss: 0.00001935
Iteration 181/1000 | Loss: 0.00001935
Iteration 182/1000 | Loss: 0.00001935
Iteration 183/1000 | Loss: 0.00001935
Iteration 184/1000 | Loss: 0.00001935
Iteration 185/1000 | Loss: 0.00001935
Iteration 186/1000 | Loss: 0.00001935
Iteration 187/1000 | Loss: 0.00001935
Iteration 188/1000 | Loss: 0.00001935
Iteration 189/1000 | Loss: 0.00001935
Iteration 190/1000 | Loss: 0.00001935
Iteration 191/1000 | Loss: 0.00001935
Iteration 192/1000 | Loss: 0.00001935
Iteration 193/1000 | Loss: 0.00001935
Iteration 194/1000 | Loss: 0.00001935
Iteration 195/1000 | Loss: 0.00001935
Iteration 196/1000 | Loss: 0.00001935
Iteration 197/1000 | Loss: 0.00001935
Iteration 198/1000 | Loss: 0.00001935
Iteration 199/1000 | Loss: 0.00001935
Iteration 200/1000 | Loss: 0.00001935
Iteration 201/1000 | Loss: 0.00001935
Iteration 202/1000 | Loss: 0.00001935
Iteration 203/1000 | Loss: 0.00001935
Iteration 204/1000 | Loss: 0.00001935
Iteration 205/1000 | Loss: 0.00001935
Iteration 206/1000 | Loss: 0.00001935
Iteration 207/1000 | Loss: 0.00001935
Iteration 208/1000 | Loss: 0.00001935
Iteration 209/1000 | Loss: 0.00001935
Iteration 210/1000 | Loss: 0.00001935
Iteration 211/1000 | Loss: 0.00001935
Iteration 212/1000 | Loss: 0.00001935
Iteration 213/1000 | Loss: 0.00001935
Iteration 214/1000 | Loss: 0.00001935
Iteration 215/1000 | Loss: 0.00001935
Iteration 216/1000 | Loss: 0.00001935
Iteration 217/1000 | Loss: 0.00001935
Iteration 218/1000 | Loss: 0.00001935
Iteration 219/1000 | Loss: 0.00001935
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.935171167133376e-05, 1.935171167133376e-05, 1.935171167133376e-05, 1.935171167133376e-05, 1.935171167133376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.935171167133376e-05

Optimization complete. Final v2v error: 3.67985463142395 mm

Highest mean error: 5.716983318328857 mm for frame 70

Lowest mean error: 3.210505723953247 mm for frame 1

Saving results

Total time: 44.80795931816101
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_025/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_025/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389864
Iteration 2/25 | Loss: 0.00125874
Iteration 3/25 | Loss: 0.00120169
Iteration 4/25 | Loss: 0.00119400
Iteration 5/25 | Loss: 0.00119216
Iteration 6/25 | Loss: 0.00119216
Iteration 7/25 | Loss: 0.00119216
Iteration 8/25 | Loss: 0.00119216
Iteration 9/25 | Loss: 0.00119216
Iteration 10/25 | Loss: 0.00119216
Iteration 11/25 | Loss: 0.00119216
Iteration 12/25 | Loss: 0.00119216
Iteration 13/25 | Loss: 0.00119216
Iteration 14/25 | Loss: 0.00119216
Iteration 15/25 | Loss: 0.00119216
Iteration 16/25 | Loss: 0.00119216
Iteration 17/25 | Loss: 0.00119216
Iteration 18/25 | Loss: 0.00119216
Iteration 19/25 | Loss: 0.00119216
Iteration 20/25 | Loss: 0.00119216
Iteration 21/25 | Loss: 0.00119216
Iteration 22/25 | Loss: 0.00119216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011921597179025412, 0.0011921597179025412, 0.0011921597179025412, 0.0011921597179025412, 0.0011921597179025412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011921597179025412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27787542
Iteration 2/25 | Loss: 0.00142561
Iteration 3/25 | Loss: 0.00142560
Iteration 4/25 | Loss: 0.00142560
Iteration 5/25 | Loss: 0.00142560
Iteration 6/25 | Loss: 0.00142560
Iteration 7/25 | Loss: 0.00142560
Iteration 8/25 | Loss: 0.00142560
Iteration 9/25 | Loss: 0.00142560
Iteration 10/25 | Loss: 0.00142560
Iteration 11/25 | Loss: 0.00142560
Iteration 12/25 | Loss: 0.00142560
Iteration 13/25 | Loss: 0.00142560
Iteration 14/25 | Loss: 0.00142560
Iteration 15/25 | Loss: 0.00142560
Iteration 16/25 | Loss: 0.00142560
Iteration 17/25 | Loss: 0.00142560
Iteration 18/25 | Loss: 0.00142560
Iteration 19/25 | Loss: 0.00142560
Iteration 20/25 | Loss: 0.00142560
Iteration 21/25 | Loss: 0.00142560
Iteration 22/25 | Loss: 0.00142560
Iteration 23/25 | Loss: 0.00142560
Iteration 24/25 | Loss: 0.00142560
Iteration 25/25 | Loss: 0.00142560

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142560
Iteration 2/1000 | Loss: 0.00002636
Iteration 3/1000 | Loss: 0.00001930
Iteration 4/1000 | Loss: 0.00001644
Iteration 5/1000 | Loss: 0.00001500
Iteration 6/1000 | Loss: 0.00001394
Iteration 7/1000 | Loss: 0.00001325
Iteration 8/1000 | Loss: 0.00001252
Iteration 9/1000 | Loss: 0.00001225
Iteration 10/1000 | Loss: 0.00001187
Iteration 11/1000 | Loss: 0.00001162
Iteration 12/1000 | Loss: 0.00001139
Iteration 13/1000 | Loss: 0.00001134
Iteration 14/1000 | Loss: 0.00001132
Iteration 15/1000 | Loss: 0.00001131
Iteration 16/1000 | Loss: 0.00001123
Iteration 17/1000 | Loss: 0.00001110
Iteration 18/1000 | Loss: 0.00001107
Iteration 19/1000 | Loss: 0.00001101
Iteration 20/1000 | Loss: 0.00001098
Iteration 21/1000 | Loss: 0.00001097
Iteration 22/1000 | Loss: 0.00001093
Iteration 23/1000 | Loss: 0.00001093
Iteration 24/1000 | Loss: 0.00001093
Iteration 25/1000 | Loss: 0.00001092
Iteration 26/1000 | Loss: 0.00001092
Iteration 27/1000 | Loss: 0.00001090
Iteration 28/1000 | Loss: 0.00001082
Iteration 29/1000 | Loss: 0.00001073
Iteration 30/1000 | Loss: 0.00001068
Iteration 31/1000 | Loss: 0.00001067
Iteration 32/1000 | Loss: 0.00001067
Iteration 33/1000 | Loss: 0.00001066
Iteration 34/1000 | Loss: 0.00001063
Iteration 35/1000 | Loss: 0.00001062
Iteration 36/1000 | Loss: 0.00001061
Iteration 37/1000 | Loss: 0.00001057
Iteration 38/1000 | Loss: 0.00001056
Iteration 39/1000 | Loss: 0.00001056
Iteration 40/1000 | Loss: 0.00001056
Iteration 41/1000 | Loss: 0.00001055
Iteration 42/1000 | Loss: 0.00001055
Iteration 43/1000 | Loss: 0.00001054
Iteration 44/1000 | Loss: 0.00001054
Iteration 45/1000 | Loss: 0.00001053
Iteration 46/1000 | Loss: 0.00001051
Iteration 47/1000 | Loss: 0.00001047
Iteration 48/1000 | Loss: 0.00001046
Iteration 49/1000 | Loss: 0.00001046
Iteration 50/1000 | Loss: 0.00001045
Iteration 51/1000 | Loss: 0.00001045
Iteration 52/1000 | Loss: 0.00001043
Iteration 53/1000 | Loss: 0.00001043
Iteration 54/1000 | Loss: 0.00001043
Iteration 55/1000 | Loss: 0.00001042
Iteration 56/1000 | Loss: 0.00001042
Iteration 57/1000 | Loss: 0.00001042
Iteration 58/1000 | Loss: 0.00001042
Iteration 59/1000 | Loss: 0.00001042
Iteration 60/1000 | Loss: 0.00001042
Iteration 61/1000 | Loss: 0.00001042
Iteration 62/1000 | Loss: 0.00001042
Iteration 63/1000 | Loss: 0.00001042
Iteration 64/1000 | Loss: 0.00001042
Iteration 65/1000 | Loss: 0.00001042
Iteration 66/1000 | Loss: 0.00001042
Iteration 67/1000 | Loss: 0.00001041
Iteration 68/1000 | Loss: 0.00001041
Iteration 69/1000 | Loss: 0.00001041
Iteration 70/1000 | Loss: 0.00001041
Iteration 71/1000 | Loss: 0.00001040
Iteration 72/1000 | Loss: 0.00001040
Iteration 73/1000 | Loss: 0.00001040
Iteration 74/1000 | Loss: 0.00001040
Iteration 75/1000 | Loss: 0.00001040
Iteration 76/1000 | Loss: 0.00001040
Iteration 77/1000 | Loss: 0.00001039
Iteration 78/1000 | Loss: 0.00001039
Iteration 79/1000 | Loss: 0.00001039
Iteration 80/1000 | Loss: 0.00001039
Iteration 81/1000 | Loss: 0.00001039
Iteration 82/1000 | Loss: 0.00001039
Iteration 83/1000 | Loss: 0.00001039
Iteration 84/1000 | Loss: 0.00001039
Iteration 85/1000 | Loss: 0.00001039
Iteration 86/1000 | Loss: 0.00001039
Iteration 87/1000 | Loss: 0.00001038
Iteration 88/1000 | Loss: 0.00001038
Iteration 89/1000 | Loss: 0.00001038
Iteration 90/1000 | Loss: 0.00001038
Iteration 91/1000 | Loss: 0.00001038
Iteration 92/1000 | Loss: 0.00001038
Iteration 93/1000 | Loss: 0.00001038
Iteration 94/1000 | Loss: 0.00001038
Iteration 95/1000 | Loss: 0.00001037
Iteration 96/1000 | Loss: 0.00001037
Iteration 97/1000 | Loss: 0.00001037
Iteration 98/1000 | Loss: 0.00001037
Iteration 99/1000 | Loss: 0.00001037
Iteration 100/1000 | Loss: 0.00001037
Iteration 101/1000 | Loss: 0.00001037
Iteration 102/1000 | Loss: 0.00001037
Iteration 103/1000 | Loss: 0.00001037
Iteration 104/1000 | Loss: 0.00001037
Iteration 105/1000 | Loss: 0.00001037
Iteration 106/1000 | Loss: 0.00001037
Iteration 107/1000 | Loss: 0.00001037
Iteration 108/1000 | Loss: 0.00001037
Iteration 109/1000 | Loss: 0.00001036
Iteration 110/1000 | Loss: 0.00001036
Iteration 111/1000 | Loss: 0.00001036
Iteration 112/1000 | Loss: 0.00001036
Iteration 113/1000 | Loss: 0.00001036
Iteration 114/1000 | Loss: 0.00001036
Iteration 115/1000 | Loss: 0.00001036
Iteration 116/1000 | Loss: 0.00001036
Iteration 117/1000 | Loss: 0.00001036
Iteration 118/1000 | Loss: 0.00001036
Iteration 119/1000 | Loss: 0.00001036
Iteration 120/1000 | Loss: 0.00001036
Iteration 121/1000 | Loss: 0.00001036
Iteration 122/1000 | Loss: 0.00001036
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.0362807188357692e-05, 1.0362807188357692e-05, 1.0362807188357692e-05, 1.0362807188357692e-05, 1.0362807188357692e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0362807188357692e-05

Optimization complete. Final v2v error: 2.811887741088867 mm

Highest mean error: 3.0015270709991455 mm for frame 140

Lowest mean error: 2.7104368209838867 mm for frame 38

Saving results

Total time: 38.79102611541748
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00907445
Iteration 2/25 | Loss: 0.00165770
Iteration 3/25 | Loss: 0.00131583
Iteration 4/25 | Loss: 0.00124441
Iteration 5/25 | Loss: 0.00122435
Iteration 6/25 | Loss: 0.00119873
Iteration 7/25 | Loss: 0.00119092
Iteration 8/25 | Loss: 0.00118053
Iteration 9/25 | Loss: 0.00117807
Iteration 10/25 | Loss: 0.00117213
Iteration 11/25 | Loss: 0.00116943
Iteration 12/25 | Loss: 0.00116683
Iteration 13/25 | Loss: 0.00116249
Iteration 14/25 | Loss: 0.00116287
Iteration 15/25 | Loss: 0.00116054
Iteration 16/25 | Loss: 0.00115774
Iteration 17/25 | Loss: 0.00115954
Iteration 18/25 | Loss: 0.00115908
Iteration 19/25 | Loss: 0.00115572
Iteration 20/25 | Loss: 0.00115282
Iteration 21/25 | Loss: 0.00115045
Iteration 22/25 | Loss: 0.00115171
Iteration 23/25 | Loss: 0.00115237
Iteration 24/25 | Loss: 0.00115101
Iteration 25/25 | Loss: 0.00115135

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83383310
Iteration 2/25 | Loss: 0.00078330
Iteration 3/25 | Loss: 0.00078330
Iteration 4/25 | Loss: 0.00078330
Iteration 5/25 | Loss: 0.00078330
Iteration 6/25 | Loss: 0.00078330
Iteration 7/25 | Loss: 0.00078330
Iteration 8/25 | Loss: 0.00078330
Iteration 9/25 | Loss: 0.00078330
Iteration 10/25 | Loss: 0.00078330
Iteration 11/25 | Loss: 0.00078330
Iteration 12/25 | Loss: 0.00078330
Iteration 13/25 | Loss: 0.00078330
Iteration 14/25 | Loss: 0.00078330
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007832958362996578, 0.0007832958362996578, 0.0007832958362996578, 0.0007832958362996578, 0.0007832958362996578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007832958362996578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078330
Iteration 2/1000 | Loss: 0.00012378
Iteration 3/1000 | Loss: 0.00024688
Iteration 4/1000 | Loss: 0.00009515
Iteration 5/1000 | Loss: 0.00048215
Iteration 6/1000 | Loss: 0.00010538
Iteration 7/1000 | Loss: 0.00009676
Iteration 8/1000 | Loss: 0.00010070
Iteration 9/1000 | Loss: 0.00008257
Iteration 10/1000 | Loss: 0.00007225
Iteration 11/1000 | Loss: 0.00005223
Iteration 12/1000 | Loss: 0.00053902
Iteration 13/1000 | Loss: 0.00012196
Iteration 14/1000 | Loss: 0.00008924
Iteration 15/1000 | Loss: 0.00007621
Iteration 16/1000 | Loss: 0.00006490
Iteration 17/1000 | Loss: 0.00005940
Iteration 18/1000 | Loss: 0.00005731
Iteration 19/1000 | Loss: 0.00006137
Iteration 20/1000 | Loss: 0.00006009
Iteration 21/1000 | Loss: 0.00006646
Iteration 22/1000 | Loss: 0.00005846
Iteration 23/1000 | Loss: 0.00007736
Iteration 24/1000 | Loss: 0.00012633
Iteration 25/1000 | Loss: 0.00008493
Iteration 26/1000 | Loss: 0.00008321
Iteration 27/1000 | Loss: 0.00009671
Iteration 28/1000 | Loss: 0.00006070
Iteration 29/1000 | Loss: 0.00008669
Iteration 30/1000 | Loss: 0.00007296
Iteration 31/1000 | Loss: 0.00008102
Iteration 32/1000 | Loss: 0.00024772
Iteration 33/1000 | Loss: 0.00030745
Iteration 34/1000 | Loss: 0.00019156
Iteration 35/1000 | Loss: 0.00020278
Iteration 36/1000 | Loss: 0.00022659
Iteration 37/1000 | Loss: 0.00016757
Iteration 38/1000 | Loss: 0.00003330
Iteration 39/1000 | Loss: 0.00003086
Iteration 40/1000 | Loss: 0.00014758
Iteration 41/1000 | Loss: 0.00011771
Iteration 42/1000 | Loss: 0.00012405
Iteration 43/1000 | Loss: 0.00007534
Iteration 44/1000 | Loss: 0.00012883
Iteration 45/1000 | Loss: 0.00003312
Iteration 46/1000 | Loss: 0.00003093
Iteration 47/1000 | Loss: 0.00002963
Iteration 48/1000 | Loss: 0.00002838
Iteration 49/1000 | Loss: 0.00002788
Iteration 50/1000 | Loss: 0.00002736
Iteration 51/1000 | Loss: 0.00002693
Iteration 52/1000 | Loss: 0.00002664
Iteration 53/1000 | Loss: 0.00002635
Iteration 54/1000 | Loss: 0.00002604
Iteration 55/1000 | Loss: 0.00002599
Iteration 56/1000 | Loss: 0.00002578
Iteration 57/1000 | Loss: 0.00002566
Iteration 58/1000 | Loss: 0.00002565
Iteration 59/1000 | Loss: 0.00002565
Iteration 60/1000 | Loss: 0.00002564
Iteration 61/1000 | Loss: 0.00002551
Iteration 62/1000 | Loss: 0.00002550
Iteration 63/1000 | Loss: 0.00002548
Iteration 64/1000 | Loss: 0.00002547
Iteration 65/1000 | Loss: 0.00002547
Iteration 66/1000 | Loss: 0.00002546
Iteration 67/1000 | Loss: 0.00002545
Iteration 68/1000 | Loss: 0.00002545
Iteration 69/1000 | Loss: 0.00002545
Iteration 70/1000 | Loss: 0.00002545
Iteration 71/1000 | Loss: 0.00002544
Iteration 72/1000 | Loss: 0.00002544
Iteration 73/1000 | Loss: 0.00002544
Iteration 74/1000 | Loss: 0.00002543
Iteration 75/1000 | Loss: 0.00002543
Iteration 76/1000 | Loss: 0.00002543
Iteration 77/1000 | Loss: 0.00002542
Iteration 78/1000 | Loss: 0.00002541
Iteration 79/1000 | Loss: 0.00002541
Iteration 80/1000 | Loss: 0.00002541
Iteration 81/1000 | Loss: 0.00002540
Iteration 82/1000 | Loss: 0.00002540
Iteration 83/1000 | Loss: 0.00002540
Iteration 84/1000 | Loss: 0.00002539
Iteration 85/1000 | Loss: 0.00002539
Iteration 86/1000 | Loss: 0.00002539
Iteration 87/1000 | Loss: 0.00002538
Iteration 88/1000 | Loss: 0.00002538
Iteration 89/1000 | Loss: 0.00002537
Iteration 90/1000 | Loss: 0.00002537
Iteration 91/1000 | Loss: 0.00002537
Iteration 92/1000 | Loss: 0.00002536
Iteration 93/1000 | Loss: 0.00002536
Iteration 94/1000 | Loss: 0.00002535
Iteration 95/1000 | Loss: 0.00002535
Iteration 96/1000 | Loss: 0.00002535
Iteration 97/1000 | Loss: 0.00002535
Iteration 98/1000 | Loss: 0.00002535
Iteration 99/1000 | Loss: 0.00002534
Iteration 100/1000 | Loss: 0.00002534
Iteration 101/1000 | Loss: 0.00002534
Iteration 102/1000 | Loss: 0.00002533
Iteration 103/1000 | Loss: 0.00002533
Iteration 104/1000 | Loss: 0.00002533
Iteration 105/1000 | Loss: 0.00002532
Iteration 106/1000 | Loss: 0.00002532
Iteration 107/1000 | Loss: 0.00002532
Iteration 108/1000 | Loss: 0.00002531
Iteration 109/1000 | Loss: 0.00002531
Iteration 110/1000 | Loss: 0.00002531
Iteration 111/1000 | Loss: 0.00002531
Iteration 112/1000 | Loss: 0.00002530
Iteration 113/1000 | Loss: 0.00002530
Iteration 114/1000 | Loss: 0.00002530
Iteration 115/1000 | Loss: 0.00002529
Iteration 116/1000 | Loss: 0.00002529
Iteration 117/1000 | Loss: 0.00002528
Iteration 118/1000 | Loss: 0.00002528
Iteration 119/1000 | Loss: 0.00002528
Iteration 120/1000 | Loss: 0.00002527
Iteration 121/1000 | Loss: 0.00002527
Iteration 122/1000 | Loss: 0.00002526
Iteration 123/1000 | Loss: 0.00002526
Iteration 124/1000 | Loss: 0.00002526
Iteration 125/1000 | Loss: 0.00002525
Iteration 126/1000 | Loss: 0.00002525
Iteration 127/1000 | Loss: 0.00002525
Iteration 128/1000 | Loss: 0.00002525
Iteration 129/1000 | Loss: 0.00002525
Iteration 130/1000 | Loss: 0.00002524
Iteration 131/1000 | Loss: 0.00002524
Iteration 132/1000 | Loss: 0.00002524
Iteration 133/1000 | Loss: 0.00002524
Iteration 134/1000 | Loss: 0.00002523
Iteration 135/1000 | Loss: 0.00002523
Iteration 136/1000 | Loss: 0.00002523
Iteration 137/1000 | Loss: 0.00002523
Iteration 138/1000 | Loss: 0.00002523
Iteration 139/1000 | Loss: 0.00002522
Iteration 140/1000 | Loss: 0.00002522
Iteration 141/1000 | Loss: 0.00002522
Iteration 142/1000 | Loss: 0.00002522
Iteration 143/1000 | Loss: 0.00002522
Iteration 144/1000 | Loss: 0.00002522
Iteration 145/1000 | Loss: 0.00002522
Iteration 146/1000 | Loss: 0.00002522
Iteration 147/1000 | Loss: 0.00002522
Iteration 148/1000 | Loss: 0.00002522
Iteration 149/1000 | Loss: 0.00002522
Iteration 150/1000 | Loss: 0.00002522
Iteration 151/1000 | Loss: 0.00002522
Iteration 152/1000 | Loss: 0.00002521
Iteration 153/1000 | Loss: 0.00002521
Iteration 154/1000 | Loss: 0.00002521
Iteration 155/1000 | Loss: 0.00002521
Iteration 156/1000 | Loss: 0.00002521
Iteration 157/1000 | Loss: 0.00002521
Iteration 158/1000 | Loss: 0.00002521
Iteration 159/1000 | Loss: 0.00002521
Iteration 160/1000 | Loss: 0.00002521
Iteration 161/1000 | Loss: 0.00002521
Iteration 162/1000 | Loss: 0.00002521
Iteration 163/1000 | Loss: 0.00002520
Iteration 164/1000 | Loss: 0.00002520
Iteration 165/1000 | Loss: 0.00002520
Iteration 166/1000 | Loss: 0.00002520
Iteration 167/1000 | Loss: 0.00002520
Iteration 168/1000 | Loss: 0.00002520
Iteration 169/1000 | Loss: 0.00002519
Iteration 170/1000 | Loss: 0.00002519
Iteration 171/1000 | Loss: 0.00002519
Iteration 172/1000 | Loss: 0.00002519
Iteration 173/1000 | Loss: 0.00002519
Iteration 174/1000 | Loss: 0.00002519
Iteration 175/1000 | Loss: 0.00002519
Iteration 176/1000 | Loss: 0.00002519
Iteration 177/1000 | Loss: 0.00002519
Iteration 178/1000 | Loss: 0.00002519
Iteration 179/1000 | Loss: 0.00002518
Iteration 180/1000 | Loss: 0.00002518
Iteration 181/1000 | Loss: 0.00002518
Iteration 182/1000 | Loss: 0.00002518
Iteration 183/1000 | Loss: 0.00002518
Iteration 184/1000 | Loss: 0.00002518
Iteration 185/1000 | Loss: 0.00002518
Iteration 186/1000 | Loss: 0.00002518
Iteration 187/1000 | Loss: 0.00002517
Iteration 188/1000 | Loss: 0.00002517
Iteration 189/1000 | Loss: 0.00002517
Iteration 190/1000 | Loss: 0.00002517
Iteration 191/1000 | Loss: 0.00002517
Iteration 192/1000 | Loss: 0.00002517
Iteration 193/1000 | Loss: 0.00002517
Iteration 194/1000 | Loss: 0.00002517
Iteration 195/1000 | Loss: 0.00002516
Iteration 196/1000 | Loss: 0.00002516
Iteration 197/1000 | Loss: 0.00002516
Iteration 198/1000 | Loss: 0.00002516
Iteration 199/1000 | Loss: 0.00002516
Iteration 200/1000 | Loss: 0.00002516
Iteration 201/1000 | Loss: 0.00002516
Iteration 202/1000 | Loss: 0.00002516
Iteration 203/1000 | Loss: 0.00002516
Iteration 204/1000 | Loss: 0.00002516
Iteration 205/1000 | Loss: 0.00002516
Iteration 206/1000 | Loss: 0.00002516
Iteration 207/1000 | Loss: 0.00002516
Iteration 208/1000 | Loss: 0.00002516
Iteration 209/1000 | Loss: 0.00002515
Iteration 210/1000 | Loss: 0.00002515
Iteration 211/1000 | Loss: 0.00002515
Iteration 212/1000 | Loss: 0.00002515
Iteration 213/1000 | Loss: 0.00002515
Iteration 214/1000 | Loss: 0.00002515
Iteration 215/1000 | Loss: 0.00002515
Iteration 216/1000 | Loss: 0.00002515
Iteration 217/1000 | Loss: 0.00002515
Iteration 218/1000 | Loss: 0.00002515
Iteration 219/1000 | Loss: 0.00002515
Iteration 220/1000 | Loss: 0.00002515
Iteration 221/1000 | Loss: 0.00002515
Iteration 222/1000 | Loss: 0.00002515
Iteration 223/1000 | Loss: 0.00002515
Iteration 224/1000 | Loss: 0.00002515
Iteration 225/1000 | Loss: 0.00002515
Iteration 226/1000 | Loss: 0.00002515
Iteration 227/1000 | Loss: 0.00002515
Iteration 228/1000 | Loss: 0.00002515
Iteration 229/1000 | Loss: 0.00002515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [2.514933657948859e-05, 2.514933657948859e-05, 2.514933657948859e-05, 2.514933657948859e-05, 2.514933657948859e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.514933657948859e-05

Optimization complete. Final v2v error: 4.283804893493652 mm

Highest mean error: 5.411892414093018 mm for frame 143

Lowest mean error: 3.6066622734069824 mm for frame 2

Saving results

Total time: 156.75178956985474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041059
Iteration 2/25 | Loss: 0.00245067
Iteration 3/25 | Loss: 0.00173361
Iteration 4/25 | Loss: 0.00159286
Iteration 5/25 | Loss: 0.00175432
Iteration 6/25 | Loss: 0.00153154
Iteration 7/25 | Loss: 0.00121041
Iteration 8/25 | Loss: 0.00111827
Iteration 9/25 | Loss: 0.00111743
Iteration 10/25 | Loss: 0.00112005
Iteration 11/25 | Loss: 0.00111043
Iteration 12/25 | Loss: 0.00109856
Iteration 13/25 | Loss: 0.00108377
Iteration 14/25 | Loss: 0.00107616
Iteration 15/25 | Loss: 0.00107313
Iteration 16/25 | Loss: 0.00107300
Iteration 17/25 | Loss: 0.00106592
Iteration 18/25 | Loss: 0.00106223
Iteration 19/25 | Loss: 0.00105841
Iteration 20/25 | Loss: 0.00105967
Iteration 21/25 | Loss: 0.00106014
Iteration 22/25 | Loss: 0.00105664
Iteration 23/25 | Loss: 0.00105479
Iteration 24/25 | Loss: 0.00105095
Iteration 25/25 | Loss: 0.00105167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46447611
Iteration 2/25 | Loss: 0.00059307
Iteration 3/25 | Loss: 0.00059307
Iteration 4/25 | Loss: 0.00059307
Iteration 5/25 | Loss: 0.00059307
Iteration 6/25 | Loss: 0.00059306
Iteration 7/25 | Loss: 0.00059306
Iteration 8/25 | Loss: 0.00059306
Iteration 9/25 | Loss: 0.00059306
Iteration 10/25 | Loss: 0.00059306
Iteration 11/25 | Loss: 0.00059306
Iteration 12/25 | Loss: 0.00059306
Iteration 13/25 | Loss: 0.00059306
Iteration 14/25 | Loss: 0.00059306
Iteration 15/25 | Loss: 0.00059306
Iteration 16/25 | Loss: 0.00059306
Iteration 17/25 | Loss: 0.00059306
Iteration 18/25 | Loss: 0.00059306
Iteration 19/25 | Loss: 0.00059306
Iteration 20/25 | Loss: 0.00059306
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005930639454163611, 0.0005930639454163611, 0.0005930639454163611, 0.0005930639454163611, 0.0005930639454163611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005930639454163611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059306
Iteration 2/1000 | Loss: 0.00024541
Iteration 3/1000 | Loss: 0.00018344
Iteration 4/1000 | Loss: 0.00010341
Iteration 5/1000 | Loss: 0.00056071
Iteration 6/1000 | Loss: 0.00016176
Iteration 7/1000 | Loss: 0.00010950
Iteration 8/1000 | Loss: 0.00055135
Iteration 9/1000 | Loss: 0.00036969
Iteration 10/1000 | Loss: 0.00058822
Iteration 11/1000 | Loss: 0.00056334
Iteration 12/1000 | Loss: 0.00004513
Iteration 13/1000 | Loss: 0.00102872
Iteration 14/1000 | Loss: 0.00006456
Iteration 15/1000 | Loss: 0.00004163
Iteration 16/1000 | Loss: 0.00003080
Iteration 17/1000 | Loss: 0.00002828
Iteration 18/1000 | Loss: 0.00002567
Iteration 19/1000 | Loss: 0.00002314
Iteration 20/1000 | Loss: 0.00002120
Iteration 21/1000 | Loss: 0.00002010
Iteration 22/1000 | Loss: 0.00001955
Iteration 23/1000 | Loss: 0.00001906
Iteration 24/1000 | Loss: 0.00001862
Iteration 25/1000 | Loss: 0.00001835
Iteration 26/1000 | Loss: 0.00001817
Iteration 27/1000 | Loss: 0.00001802
Iteration 28/1000 | Loss: 0.00001793
Iteration 29/1000 | Loss: 0.00001784
Iteration 30/1000 | Loss: 0.00001784
Iteration 31/1000 | Loss: 0.00001782
Iteration 32/1000 | Loss: 0.00001782
Iteration 33/1000 | Loss: 0.00001782
Iteration 34/1000 | Loss: 0.00001781
Iteration 35/1000 | Loss: 0.00001781
Iteration 36/1000 | Loss: 0.00001781
Iteration 37/1000 | Loss: 0.00001780
Iteration 38/1000 | Loss: 0.00001779
Iteration 39/1000 | Loss: 0.00001779
Iteration 40/1000 | Loss: 0.00001779
Iteration 41/1000 | Loss: 0.00001779
Iteration 42/1000 | Loss: 0.00001779
Iteration 43/1000 | Loss: 0.00001778
Iteration 44/1000 | Loss: 0.00001778
Iteration 45/1000 | Loss: 0.00001778
Iteration 46/1000 | Loss: 0.00001778
Iteration 47/1000 | Loss: 0.00001777
Iteration 48/1000 | Loss: 0.00001776
Iteration 49/1000 | Loss: 0.00001776
Iteration 50/1000 | Loss: 0.00001775
Iteration 51/1000 | Loss: 0.00001775
Iteration 52/1000 | Loss: 0.00001775
Iteration 53/1000 | Loss: 0.00001774
Iteration 54/1000 | Loss: 0.00001774
Iteration 55/1000 | Loss: 0.00001774
Iteration 56/1000 | Loss: 0.00001773
Iteration 57/1000 | Loss: 0.00001773
Iteration 58/1000 | Loss: 0.00001772
Iteration 59/1000 | Loss: 0.00001772
Iteration 60/1000 | Loss: 0.00001771
Iteration 61/1000 | Loss: 0.00001771
Iteration 62/1000 | Loss: 0.00001770
Iteration 63/1000 | Loss: 0.00078020
Iteration 64/1000 | Loss: 0.00001941
Iteration 65/1000 | Loss: 0.00001714
Iteration 66/1000 | Loss: 0.00001610
Iteration 67/1000 | Loss: 0.00001549
Iteration 68/1000 | Loss: 0.00001509
Iteration 69/1000 | Loss: 0.00001480
Iteration 70/1000 | Loss: 0.00001468
Iteration 71/1000 | Loss: 0.00001460
Iteration 72/1000 | Loss: 0.00001459
Iteration 73/1000 | Loss: 0.00001456
Iteration 74/1000 | Loss: 0.00001455
Iteration 75/1000 | Loss: 0.00001455
Iteration 76/1000 | Loss: 0.00001454
Iteration 77/1000 | Loss: 0.00001454
Iteration 78/1000 | Loss: 0.00001453
Iteration 79/1000 | Loss: 0.00001453
Iteration 80/1000 | Loss: 0.00001453
Iteration 81/1000 | Loss: 0.00001452
Iteration 82/1000 | Loss: 0.00001452
Iteration 83/1000 | Loss: 0.00001452
Iteration 84/1000 | Loss: 0.00001451
Iteration 85/1000 | Loss: 0.00001451
Iteration 86/1000 | Loss: 0.00001450
Iteration 87/1000 | Loss: 0.00001450
Iteration 88/1000 | Loss: 0.00001449
Iteration 89/1000 | Loss: 0.00001449
Iteration 90/1000 | Loss: 0.00001449
Iteration 91/1000 | Loss: 0.00001449
Iteration 92/1000 | Loss: 0.00001449
Iteration 93/1000 | Loss: 0.00001449
Iteration 94/1000 | Loss: 0.00001449
Iteration 95/1000 | Loss: 0.00001449
Iteration 96/1000 | Loss: 0.00001448
Iteration 97/1000 | Loss: 0.00001448
Iteration 98/1000 | Loss: 0.00001447
Iteration 99/1000 | Loss: 0.00001447
Iteration 100/1000 | Loss: 0.00001447
Iteration 101/1000 | Loss: 0.00001447
Iteration 102/1000 | Loss: 0.00001447
Iteration 103/1000 | Loss: 0.00001446
Iteration 104/1000 | Loss: 0.00001446
Iteration 105/1000 | Loss: 0.00001446
Iteration 106/1000 | Loss: 0.00001446
Iteration 107/1000 | Loss: 0.00001446
Iteration 108/1000 | Loss: 0.00001446
Iteration 109/1000 | Loss: 0.00001446
Iteration 110/1000 | Loss: 0.00001446
Iteration 111/1000 | Loss: 0.00001445
Iteration 112/1000 | Loss: 0.00001445
Iteration 113/1000 | Loss: 0.00001445
Iteration 114/1000 | Loss: 0.00001445
Iteration 115/1000 | Loss: 0.00001445
Iteration 116/1000 | Loss: 0.00001445
Iteration 117/1000 | Loss: 0.00001445
Iteration 118/1000 | Loss: 0.00001444
Iteration 119/1000 | Loss: 0.00001444
Iteration 120/1000 | Loss: 0.00001444
Iteration 121/1000 | Loss: 0.00001444
Iteration 122/1000 | Loss: 0.00001444
Iteration 123/1000 | Loss: 0.00001444
Iteration 124/1000 | Loss: 0.00001444
Iteration 125/1000 | Loss: 0.00001444
Iteration 126/1000 | Loss: 0.00001444
Iteration 127/1000 | Loss: 0.00001444
Iteration 128/1000 | Loss: 0.00001444
Iteration 129/1000 | Loss: 0.00001444
Iteration 130/1000 | Loss: 0.00001443
Iteration 131/1000 | Loss: 0.00001443
Iteration 132/1000 | Loss: 0.00001443
Iteration 133/1000 | Loss: 0.00001443
Iteration 134/1000 | Loss: 0.00001443
Iteration 135/1000 | Loss: 0.00001443
Iteration 136/1000 | Loss: 0.00001443
Iteration 137/1000 | Loss: 0.00001443
Iteration 138/1000 | Loss: 0.00001443
Iteration 139/1000 | Loss: 0.00001443
Iteration 140/1000 | Loss: 0.00001443
Iteration 141/1000 | Loss: 0.00001443
Iteration 142/1000 | Loss: 0.00001443
Iteration 143/1000 | Loss: 0.00001442
Iteration 144/1000 | Loss: 0.00001442
Iteration 145/1000 | Loss: 0.00001442
Iteration 146/1000 | Loss: 0.00001442
Iteration 147/1000 | Loss: 0.00001442
Iteration 148/1000 | Loss: 0.00001442
Iteration 149/1000 | Loss: 0.00001442
Iteration 150/1000 | Loss: 0.00001442
Iteration 151/1000 | Loss: 0.00001442
Iteration 152/1000 | Loss: 0.00001441
Iteration 153/1000 | Loss: 0.00001441
Iteration 154/1000 | Loss: 0.00001441
Iteration 155/1000 | Loss: 0.00001441
Iteration 156/1000 | Loss: 0.00001441
Iteration 157/1000 | Loss: 0.00001441
Iteration 158/1000 | Loss: 0.00001441
Iteration 159/1000 | Loss: 0.00001441
Iteration 160/1000 | Loss: 0.00001441
Iteration 161/1000 | Loss: 0.00001441
Iteration 162/1000 | Loss: 0.00001441
Iteration 163/1000 | Loss: 0.00001441
Iteration 164/1000 | Loss: 0.00001441
Iteration 165/1000 | Loss: 0.00001441
Iteration 166/1000 | Loss: 0.00001441
Iteration 167/1000 | Loss: 0.00001441
Iteration 168/1000 | Loss: 0.00001441
Iteration 169/1000 | Loss: 0.00001440
Iteration 170/1000 | Loss: 0.00001440
Iteration 171/1000 | Loss: 0.00001440
Iteration 172/1000 | Loss: 0.00001440
Iteration 173/1000 | Loss: 0.00001440
Iteration 174/1000 | Loss: 0.00001440
Iteration 175/1000 | Loss: 0.00001440
Iteration 176/1000 | Loss: 0.00001440
Iteration 177/1000 | Loss: 0.00001440
Iteration 178/1000 | Loss: 0.00001440
Iteration 179/1000 | Loss: 0.00001440
Iteration 180/1000 | Loss: 0.00001440
Iteration 181/1000 | Loss: 0.00001440
Iteration 182/1000 | Loss: 0.00001440
Iteration 183/1000 | Loss: 0.00001440
Iteration 184/1000 | Loss: 0.00001440
Iteration 185/1000 | Loss: 0.00001440
Iteration 186/1000 | Loss: 0.00001440
Iteration 187/1000 | Loss: 0.00001440
Iteration 188/1000 | Loss: 0.00001440
Iteration 189/1000 | Loss: 0.00001440
Iteration 190/1000 | Loss: 0.00001440
Iteration 191/1000 | Loss: 0.00001440
Iteration 192/1000 | Loss: 0.00001440
Iteration 193/1000 | Loss: 0.00001440
Iteration 194/1000 | Loss: 0.00001440
Iteration 195/1000 | Loss: 0.00001440
Iteration 196/1000 | Loss: 0.00001440
Iteration 197/1000 | Loss: 0.00001440
Iteration 198/1000 | Loss: 0.00001440
Iteration 199/1000 | Loss: 0.00001440
Iteration 200/1000 | Loss: 0.00001440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.4398127859749366e-05, 1.4398127859749366e-05, 1.4398127859749366e-05, 1.4398127859749366e-05, 1.4398127859749366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4398127859749366e-05

Optimization complete. Final v2v error: 3.2282326221466064 mm

Highest mean error: 4.267063617706299 mm for frame 102

Lowest mean error: 2.7841877937316895 mm for frame 73

Saving results

Total time: 110.20345854759216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01144647
Iteration 2/25 | Loss: 0.00226993
Iteration 3/25 | Loss: 0.00150243
Iteration 4/25 | Loss: 0.00134070
Iteration 5/25 | Loss: 0.00128826
Iteration 6/25 | Loss: 0.00127486
Iteration 7/25 | Loss: 0.00127002
Iteration 8/25 | Loss: 0.00126416
Iteration 9/25 | Loss: 0.00125841
Iteration 10/25 | Loss: 0.00125714
Iteration 11/25 | Loss: 0.00125667
Iteration 12/25 | Loss: 0.00125632
Iteration 13/25 | Loss: 0.00125708
Iteration 14/25 | Loss: 0.00125539
Iteration 15/25 | Loss: 0.00125511
Iteration 16/25 | Loss: 0.00125484
Iteration 17/25 | Loss: 0.00125441
Iteration 18/25 | Loss: 0.00125513
Iteration 19/25 | Loss: 0.00125345
Iteration 20/25 | Loss: 0.00125322
Iteration 21/25 | Loss: 0.00125321
Iteration 22/25 | Loss: 0.00125319
Iteration 23/25 | Loss: 0.00125318
Iteration 24/25 | Loss: 0.00125318
Iteration 25/25 | Loss: 0.00125318

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.69725108
Iteration 2/25 | Loss: 0.00078559
Iteration 3/25 | Loss: 0.00078559
Iteration 4/25 | Loss: 0.00078559
Iteration 5/25 | Loss: 0.00078559
Iteration 6/25 | Loss: 0.00078559
Iteration 7/25 | Loss: 0.00078559
Iteration 8/25 | Loss: 0.00078559
Iteration 9/25 | Loss: 0.00078559
Iteration 10/25 | Loss: 0.00078559
Iteration 11/25 | Loss: 0.00078559
Iteration 12/25 | Loss: 0.00078559
Iteration 13/25 | Loss: 0.00078559
Iteration 14/25 | Loss: 0.00078559
Iteration 15/25 | Loss: 0.00078559
Iteration 16/25 | Loss: 0.00078559
Iteration 17/25 | Loss: 0.00078559
Iteration 18/25 | Loss: 0.00078559
Iteration 19/25 | Loss: 0.00078559
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007855862495489419, 0.0007855862495489419, 0.0007855862495489419, 0.0007855862495489419, 0.0007855862495489419]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007855862495489419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078559
Iteration 2/1000 | Loss: 0.00009455
Iteration 3/1000 | Loss: 0.00006354
Iteration 4/1000 | Loss: 0.00005660
Iteration 5/1000 | Loss: 0.00005441
Iteration 6/1000 | Loss: 0.00005283
Iteration 7/1000 | Loss: 0.00005166
Iteration 8/1000 | Loss: 0.00005094
Iteration 9/1000 | Loss: 0.00005022
Iteration 10/1000 | Loss: 0.00004970
Iteration 11/1000 | Loss: 0.00004920
Iteration 12/1000 | Loss: 0.00004886
Iteration 13/1000 | Loss: 0.00004849
Iteration 14/1000 | Loss: 0.00004817
Iteration 15/1000 | Loss: 0.00004788
Iteration 16/1000 | Loss: 0.00004757
Iteration 17/1000 | Loss: 0.00004734
Iteration 18/1000 | Loss: 0.00004719
Iteration 19/1000 | Loss: 0.00004701
Iteration 20/1000 | Loss: 0.00004683
Iteration 21/1000 | Loss: 0.00004668
Iteration 22/1000 | Loss: 0.00004666
Iteration 23/1000 | Loss: 0.00004652
Iteration 24/1000 | Loss: 0.00004651
Iteration 25/1000 | Loss: 0.00004646
Iteration 26/1000 | Loss: 0.00004646
Iteration 27/1000 | Loss: 0.00004640
Iteration 28/1000 | Loss: 0.00004640
Iteration 29/1000 | Loss: 0.00004639
Iteration 30/1000 | Loss: 0.00004635
Iteration 31/1000 | Loss: 0.00004635
Iteration 32/1000 | Loss: 0.00004633
Iteration 33/1000 | Loss: 0.00004632
Iteration 34/1000 | Loss: 0.00004631
Iteration 35/1000 | Loss: 0.00004630
Iteration 36/1000 | Loss: 0.00004628
Iteration 37/1000 | Loss: 0.00004627
Iteration 38/1000 | Loss: 0.00004626
Iteration 39/1000 | Loss: 0.00004624
Iteration 40/1000 | Loss: 0.00004623
Iteration 41/1000 | Loss: 0.00004623
Iteration 42/1000 | Loss: 0.00004622
Iteration 43/1000 | Loss: 0.00004621
Iteration 44/1000 | Loss: 0.00004621
Iteration 45/1000 | Loss: 0.00004620
Iteration 46/1000 | Loss: 0.00004620
Iteration 47/1000 | Loss: 0.00004620
Iteration 48/1000 | Loss: 0.00004620
Iteration 49/1000 | Loss: 0.00004619
Iteration 50/1000 | Loss: 0.00004619
Iteration 51/1000 | Loss: 0.00004619
Iteration 52/1000 | Loss: 0.00004618
Iteration 53/1000 | Loss: 0.00004618
Iteration 54/1000 | Loss: 0.00004618
Iteration 55/1000 | Loss: 0.00004618
Iteration 56/1000 | Loss: 0.00004618
Iteration 57/1000 | Loss: 0.00004618
Iteration 58/1000 | Loss: 0.00004618
Iteration 59/1000 | Loss: 0.00004618
Iteration 60/1000 | Loss: 0.00004618
Iteration 61/1000 | Loss: 0.00004618
Iteration 62/1000 | Loss: 0.00004617
Iteration 63/1000 | Loss: 0.00004617
Iteration 64/1000 | Loss: 0.00004617
Iteration 65/1000 | Loss: 0.00004617
Iteration 66/1000 | Loss: 0.00004617
Iteration 67/1000 | Loss: 0.00004617
Iteration 68/1000 | Loss: 0.00004616
Iteration 69/1000 | Loss: 0.00004616
Iteration 70/1000 | Loss: 0.00004616
Iteration 71/1000 | Loss: 0.00004616
Iteration 72/1000 | Loss: 0.00004616
Iteration 73/1000 | Loss: 0.00004616
Iteration 74/1000 | Loss: 0.00004616
Iteration 75/1000 | Loss: 0.00004616
Iteration 76/1000 | Loss: 0.00004616
Iteration 77/1000 | Loss: 0.00004616
Iteration 78/1000 | Loss: 0.00004616
Iteration 79/1000 | Loss: 0.00004616
Iteration 80/1000 | Loss: 0.00004615
Iteration 81/1000 | Loss: 0.00004615
Iteration 82/1000 | Loss: 0.00004615
Iteration 83/1000 | Loss: 0.00004615
Iteration 84/1000 | Loss: 0.00004615
Iteration 85/1000 | Loss: 0.00004614
Iteration 86/1000 | Loss: 0.00004614
Iteration 87/1000 | Loss: 0.00004614
Iteration 88/1000 | Loss: 0.00004614
Iteration 89/1000 | Loss: 0.00004614
Iteration 90/1000 | Loss: 0.00004614
Iteration 91/1000 | Loss: 0.00004614
Iteration 92/1000 | Loss: 0.00004614
Iteration 93/1000 | Loss: 0.00004613
Iteration 94/1000 | Loss: 0.00004613
Iteration 95/1000 | Loss: 0.00004613
Iteration 96/1000 | Loss: 0.00004613
Iteration 97/1000 | Loss: 0.00004612
Iteration 98/1000 | Loss: 0.00004612
Iteration 99/1000 | Loss: 0.00004612
Iteration 100/1000 | Loss: 0.00004612
Iteration 101/1000 | Loss: 0.00004612
Iteration 102/1000 | Loss: 0.00004612
Iteration 103/1000 | Loss: 0.00004612
Iteration 104/1000 | Loss: 0.00004612
Iteration 105/1000 | Loss: 0.00004611
Iteration 106/1000 | Loss: 0.00004611
Iteration 107/1000 | Loss: 0.00004611
Iteration 108/1000 | Loss: 0.00004611
Iteration 109/1000 | Loss: 0.00004611
Iteration 110/1000 | Loss: 0.00004611
Iteration 111/1000 | Loss: 0.00004611
Iteration 112/1000 | Loss: 0.00004611
Iteration 113/1000 | Loss: 0.00004611
Iteration 114/1000 | Loss: 0.00004610
Iteration 115/1000 | Loss: 0.00004610
Iteration 116/1000 | Loss: 0.00004610
Iteration 117/1000 | Loss: 0.00004610
Iteration 118/1000 | Loss: 0.00004610
Iteration 119/1000 | Loss: 0.00004610
Iteration 120/1000 | Loss: 0.00004609
Iteration 121/1000 | Loss: 0.00004609
Iteration 122/1000 | Loss: 0.00004609
Iteration 123/1000 | Loss: 0.00004609
Iteration 124/1000 | Loss: 0.00004609
Iteration 125/1000 | Loss: 0.00004609
Iteration 126/1000 | Loss: 0.00004609
Iteration 127/1000 | Loss: 0.00004609
Iteration 128/1000 | Loss: 0.00004609
Iteration 129/1000 | Loss: 0.00004608
Iteration 130/1000 | Loss: 0.00004608
Iteration 131/1000 | Loss: 0.00004608
Iteration 132/1000 | Loss: 0.00004608
Iteration 133/1000 | Loss: 0.00004608
Iteration 134/1000 | Loss: 0.00004608
Iteration 135/1000 | Loss: 0.00004608
Iteration 136/1000 | Loss: 0.00004608
Iteration 137/1000 | Loss: 0.00004607
Iteration 138/1000 | Loss: 0.00004607
Iteration 139/1000 | Loss: 0.00004607
Iteration 140/1000 | Loss: 0.00004607
Iteration 141/1000 | Loss: 0.00004607
Iteration 142/1000 | Loss: 0.00004607
Iteration 143/1000 | Loss: 0.00004607
Iteration 144/1000 | Loss: 0.00004607
Iteration 145/1000 | Loss: 0.00004607
Iteration 146/1000 | Loss: 0.00004607
Iteration 147/1000 | Loss: 0.00004607
Iteration 148/1000 | Loss: 0.00004607
Iteration 149/1000 | Loss: 0.00004607
Iteration 150/1000 | Loss: 0.00004607
Iteration 151/1000 | Loss: 0.00004607
Iteration 152/1000 | Loss: 0.00004606
Iteration 153/1000 | Loss: 0.00004606
Iteration 154/1000 | Loss: 0.00004606
Iteration 155/1000 | Loss: 0.00004606
Iteration 156/1000 | Loss: 0.00004606
Iteration 157/1000 | Loss: 0.00004606
Iteration 158/1000 | Loss: 0.00004606
Iteration 159/1000 | Loss: 0.00004606
Iteration 160/1000 | Loss: 0.00004606
Iteration 161/1000 | Loss: 0.00004606
Iteration 162/1000 | Loss: 0.00004606
Iteration 163/1000 | Loss: 0.00004606
Iteration 164/1000 | Loss: 0.00004606
Iteration 165/1000 | Loss: 0.00004605
Iteration 166/1000 | Loss: 0.00004605
Iteration 167/1000 | Loss: 0.00004605
Iteration 168/1000 | Loss: 0.00004605
Iteration 169/1000 | Loss: 0.00004605
Iteration 170/1000 | Loss: 0.00004605
Iteration 171/1000 | Loss: 0.00004605
Iteration 172/1000 | Loss: 0.00004605
Iteration 173/1000 | Loss: 0.00004605
Iteration 174/1000 | Loss: 0.00004605
Iteration 175/1000 | Loss: 0.00004605
Iteration 176/1000 | Loss: 0.00004605
Iteration 177/1000 | Loss: 0.00004605
Iteration 178/1000 | Loss: 0.00004605
Iteration 179/1000 | Loss: 0.00004605
Iteration 180/1000 | Loss: 0.00004604
Iteration 181/1000 | Loss: 0.00004604
Iteration 182/1000 | Loss: 0.00004604
Iteration 183/1000 | Loss: 0.00004604
Iteration 184/1000 | Loss: 0.00004604
Iteration 185/1000 | Loss: 0.00004604
Iteration 186/1000 | Loss: 0.00004604
Iteration 187/1000 | Loss: 0.00004604
Iteration 188/1000 | Loss: 0.00004604
Iteration 189/1000 | Loss: 0.00004604
Iteration 190/1000 | Loss: 0.00004604
Iteration 191/1000 | Loss: 0.00004604
Iteration 192/1000 | Loss: 0.00004604
Iteration 193/1000 | Loss: 0.00004604
Iteration 194/1000 | Loss: 0.00004604
Iteration 195/1000 | Loss: 0.00004604
Iteration 196/1000 | Loss: 0.00004604
Iteration 197/1000 | Loss: 0.00004604
Iteration 198/1000 | Loss: 0.00004604
Iteration 199/1000 | Loss: 0.00004604
Iteration 200/1000 | Loss: 0.00004604
Iteration 201/1000 | Loss: 0.00004604
Iteration 202/1000 | Loss: 0.00004604
Iteration 203/1000 | Loss: 0.00004604
Iteration 204/1000 | Loss: 0.00004604
Iteration 205/1000 | Loss: 0.00004604
Iteration 206/1000 | Loss: 0.00004604
Iteration 207/1000 | Loss: 0.00004604
Iteration 208/1000 | Loss: 0.00004604
Iteration 209/1000 | Loss: 0.00004604
Iteration 210/1000 | Loss: 0.00004604
Iteration 211/1000 | Loss: 0.00004604
Iteration 212/1000 | Loss: 0.00004604
Iteration 213/1000 | Loss: 0.00004604
Iteration 214/1000 | Loss: 0.00004604
Iteration 215/1000 | Loss: 0.00004604
Iteration 216/1000 | Loss: 0.00004604
Iteration 217/1000 | Loss: 0.00004604
Iteration 218/1000 | Loss: 0.00004604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [4.603620618581772e-05, 4.603620618581772e-05, 4.603620618581772e-05, 4.603620618581772e-05, 4.603620618581772e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.603620618581772e-05

Optimization complete. Final v2v error: 5.388909816741943 mm

Highest mean error: 7.236833572387695 mm for frame 77

Lowest mean error: 3.511107921600342 mm for frame 16

Saving results

Total time: 92.90668249130249
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01136397
Iteration 2/25 | Loss: 0.00575872
Iteration 3/25 | Loss: 0.00381415
Iteration 4/25 | Loss: 0.00285604
Iteration 5/25 | Loss: 0.00261070
Iteration 6/25 | Loss: 0.00233270
Iteration 7/25 | Loss: 0.00214221
Iteration 8/25 | Loss: 0.00202158
Iteration 9/25 | Loss: 0.00195684
Iteration 10/25 | Loss: 0.00189193
Iteration 11/25 | Loss: 0.00180238
Iteration 12/25 | Loss: 0.00177617
Iteration 13/25 | Loss: 0.00174720
Iteration 14/25 | Loss: 0.00172888
Iteration 15/25 | Loss: 0.00170806
Iteration 16/25 | Loss: 0.00168490
Iteration 17/25 | Loss: 0.00165336
Iteration 18/25 | Loss: 0.00166078
Iteration 19/25 | Loss: 0.00164805
Iteration 20/25 | Loss: 0.00163914
Iteration 21/25 | Loss: 0.00163398
Iteration 22/25 | Loss: 0.00163775
Iteration 23/25 | Loss: 0.00163177
Iteration 24/25 | Loss: 0.00163027
Iteration 25/25 | Loss: 0.00163052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.70710886
Iteration 2/25 | Loss: 0.00463186
Iteration 3/25 | Loss: 0.00463186
Iteration 4/25 | Loss: 0.00463186
Iteration 5/25 | Loss: 0.00463186
Iteration 6/25 | Loss: 0.00463186
Iteration 7/25 | Loss: 0.00463186
Iteration 8/25 | Loss: 0.00463186
Iteration 9/25 | Loss: 0.00463186
Iteration 10/25 | Loss: 0.00463186
Iteration 11/25 | Loss: 0.00463186
Iteration 12/25 | Loss: 0.00463186
Iteration 13/25 | Loss: 0.00463186
Iteration 14/25 | Loss: 0.00463186
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.004631855525076389, 0.004631855525076389, 0.004631855525076389, 0.004631855525076389, 0.004631855525076389]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004631855525076389

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00463186
Iteration 2/1000 | Loss: 0.00631027
Iteration 3/1000 | Loss: 0.00153738
Iteration 4/1000 | Loss: 0.00154000
Iteration 5/1000 | Loss: 0.00164679
Iteration 6/1000 | Loss: 0.00161803
Iteration 7/1000 | Loss: 0.00107870
Iteration 8/1000 | Loss: 0.00093614
Iteration 9/1000 | Loss: 0.00042632
Iteration 10/1000 | Loss: 0.00024446
Iteration 11/1000 | Loss: 0.00045227
Iteration 12/1000 | Loss: 0.00030169
Iteration 13/1000 | Loss: 0.00032510
Iteration 14/1000 | Loss: 0.00012099
Iteration 15/1000 | Loss: 0.00033627
Iteration 16/1000 | Loss: 0.00042520
Iteration 17/1000 | Loss: 0.00037138
Iteration 18/1000 | Loss: 0.00014259
Iteration 19/1000 | Loss: 0.00017538
Iteration 20/1000 | Loss: 0.00045874
Iteration 21/1000 | Loss: 0.00025036
Iteration 22/1000 | Loss: 0.00035151
Iteration 23/1000 | Loss: 0.00010191
Iteration 24/1000 | Loss: 0.00007538
Iteration 25/1000 | Loss: 0.00021594
Iteration 26/1000 | Loss: 0.00018807
Iteration 27/1000 | Loss: 0.00026278
Iteration 28/1000 | Loss: 0.00024539
Iteration 29/1000 | Loss: 0.00021695
Iteration 30/1000 | Loss: 0.00015529
Iteration 31/1000 | Loss: 0.00014349
Iteration 32/1000 | Loss: 0.00004999
Iteration 33/1000 | Loss: 0.00037322
Iteration 34/1000 | Loss: 0.00007315
Iteration 35/1000 | Loss: 0.00008820
Iteration 36/1000 | Loss: 0.00005939
Iteration 37/1000 | Loss: 0.00004774
Iteration 38/1000 | Loss: 0.00005344
Iteration 39/1000 | Loss: 0.00004348
Iteration 40/1000 | Loss: 0.00004040
Iteration 41/1000 | Loss: 0.00003871
Iteration 42/1000 | Loss: 0.00003742
Iteration 43/1000 | Loss: 0.00003637
Iteration 44/1000 | Loss: 0.00003561
Iteration 45/1000 | Loss: 0.00025627
Iteration 46/1000 | Loss: 0.00005073
Iteration 47/1000 | Loss: 0.00004034
Iteration 48/1000 | Loss: 0.00003690
Iteration 49/1000 | Loss: 0.00003393
Iteration 50/1000 | Loss: 0.00003274
Iteration 51/1000 | Loss: 0.00003182
Iteration 52/1000 | Loss: 0.00003128
Iteration 53/1000 | Loss: 0.00003095
Iteration 54/1000 | Loss: 0.00003071
Iteration 55/1000 | Loss: 0.00003071
Iteration 56/1000 | Loss: 0.00003052
Iteration 57/1000 | Loss: 0.00003039
Iteration 58/1000 | Loss: 0.00003027
Iteration 59/1000 | Loss: 0.00003019
Iteration 60/1000 | Loss: 0.00003015
Iteration 61/1000 | Loss: 0.00003015
Iteration 62/1000 | Loss: 0.00003013
Iteration 63/1000 | Loss: 0.00003012
Iteration 64/1000 | Loss: 0.00003012
Iteration 65/1000 | Loss: 0.00003011
Iteration 66/1000 | Loss: 0.00003007
Iteration 67/1000 | Loss: 0.00003005
Iteration 68/1000 | Loss: 0.00003002
Iteration 69/1000 | Loss: 0.00003002
Iteration 70/1000 | Loss: 0.00003002
Iteration 71/1000 | Loss: 0.00003002
Iteration 72/1000 | Loss: 0.00003002
Iteration 73/1000 | Loss: 0.00003000
Iteration 74/1000 | Loss: 0.00002999
Iteration 75/1000 | Loss: 0.00002990
Iteration 76/1000 | Loss: 0.00002983
Iteration 77/1000 | Loss: 0.00002983
Iteration 78/1000 | Loss: 0.00002982
Iteration 79/1000 | Loss: 0.00002982
Iteration 80/1000 | Loss: 0.00002981
Iteration 81/1000 | Loss: 0.00002977
Iteration 82/1000 | Loss: 0.00002970
Iteration 83/1000 | Loss: 0.00002969
Iteration 84/1000 | Loss: 0.00002967
Iteration 85/1000 | Loss: 0.00002964
Iteration 86/1000 | Loss: 0.00002957
Iteration 87/1000 | Loss: 0.00002950
Iteration 88/1000 | Loss: 0.00002947
Iteration 89/1000 | Loss: 0.00002947
Iteration 90/1000 | Loss: 0.00002947
Iteration 91/1000 | Loss: 0.00002945
Iteration 92/1000 | Loss: 0.00002934
Iteration 93/1000 | Loss: 0.00002933
Iteration 94/1000 | Loss: 0.00002933
Iteration 95/1000 | Loss: 0.00002933
Iteration 96/1000 | Loss: 0.00002933
Iteration 97/1000 | Loss: 0.00002932
Iteration 98/1000 | Loss: 0.00002932
Iteration 99/1000 | Loss: 0.00002932
Iteration 100/1000 | Loss: 0.00002931
Iteration 101/1000 | Loss: 0.00002931
Iteration 102/1000 | Loss: 0.00002930
Iteration 103/1000 | Loss: 0.00002930
Iteration 104/1000 | Loss: 0.00002930
Iteration 105/1000 | Loss: 0.00002930
Iteration 106/1000 | Loss: 0.00002930
Iteration 107/1000 | Loss: 0.00002930
Iteration 108/1000 | Loss: 0.00002930
Iteration 109/1000 | Loss: 0.00002927
Iteration 110/1000 | Loss: 0.00002927
Iteration 111/1000 | Loss: 0.00002927
Iteration 112/1000 | Loss: 0.00002927
Iteration 113/1000 | Loss: 0.00002927
Iteration 114/1000 | Loss: 0.00002926
Iteration 115/1000 | Loss: 0.00002924
Iteration 116/1000 | Loss: 0.00002924
Iteration 117/1000 | Loss: 0.00002923
Iteration 118/1000 | Loss: 0.00002923
Iteration 119/1000 | Loss: 0.00002923
Iteration 120/1000 | Loss: 0.00002922
Iteration 121/1000 | Loss: 0.00002922
Iteration 122/1000 | Loss: 0.00002922
Iteration 123/1000 | Loss: 0.00002921
Iteration 124/1000 | Loss: 0.00002921
Iteration 125/1000 | Loss: 0.00002920
Iteration 126/1000 | Loss: 0.00002920
Iteration 127/1000 | Loss: 0.00002920
Iteration 128/1000 | Loss: 0.00002920
Iteration 129/1000 | Loss: 0.00002920
Iteration 130/1000 | Loss: 0.00002920
Iteration 131/1000 | Loss: 0.00002919
Iteration 132/1000 | Loss: 0.00002919
Iteration 133/1000 | Loss: 0.00002919
Iteration 134/1000 | Loss: 0.00002919
Iteration 135/1000 | Loss: 0.00002919
Iteration 136/1000 | Loss: 0.00002918
Iteration 137/1000 | Loss: 0.00002917
Iteration 138/1000 | Loss: 0.00002917
Iteration 139/1000 | Loss: 0.00002917
Iteration 140/1000 | Loss: 0.00002917
Iteration 141/1000 | Loss: 0.00002916
Iteration 142/1000 | Loss: 0.00002916
Iteration 143/1000 | Loss: 0.00002916
Iteration 144/1000 | Loss: 0.00002916
Iteration 145/1000 | Loss: 0.00002916
Iteration 146/1000 | Loss: 0.00002916
Iteration 147/1000 | Loss: 0.00002916
Iteration 148/1000 | Loss: 0.00002916
Iteration 149/1000 | Loss: 0.00002916
Iteration 150/1000 | Loss: 0.00002916
Iteration 151/1000 | Loss: 0.00002916
Iteration 152/1000 | Loss: 0.00002916
Iteration 153/1000 | Loss: 0.00002916
Iteration 154/1000 | Loss: 0.00002915
Iteration 155/1000 | Loss: 0.00002915
Iteration 156/1000 | Loss: 0.00002915
Iteration 157/1000 | Loss: 0.00002915
Iteration 158/1000 | Loss: 0.00002915
Iteration 159/1000 | Loss: 0.00002915
Iteration 160/1000 | Loss: 0.00002915
Iteration 161/1000 | Loss: 0.00002915
Iteration 162/1000 | Loss: 0.00002915
Iteration 163/1000 | Loss: 0.00002915
Iteration 164/1000 | Loss: 0.00002914
Iteration 165/1000 | Loss: 0.00002914
Iteration 166/1000 | Loss: 0.00002914
Iteration 167/1000 | Loss: 0.00002914
Iteration 168/1000 | Loss: 0.00002914
Iteration 169/1000 | Loss: 0.00002914
Iteration 170/1000 | Loss: 0.00002914
Iteration 171/1000 | Loss: 0.00002914
Iteration 172/1000 | Loss: 0.00002914
Iteration 173/1000 | Loss: 0.00002914
Iteration 174/1000 | Loss: 0.00002914
Iteration 175/1000 | Loss: 0.00002914
Iteration 176/1000 | Loss: 0.00002914
Iteration 177/1000 | Loss: 0.00002914
Iteration 178/1000 | Loss: 0.00002914
Iteration 179/1000 | Loss: 0.00002914
Iteration 180/1000 | Loss: 0.00002914
Iteration 181/1000 | Loss: 0.00002913
Iteration 182/1000 | Loss: 0.00002913
Iteration 183/1000 | Loss: 0.00002913
Iteration 184/1000 | Loss: 0.00002913
Iteration 185/1000 | Loss: 0.00002913
Iteration 186/1000 | Loss: 0.00002913
Iteration 187/1000 | Loss: 0.00002913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [2.9134256692486815e-05, 2.9134256692486815e-05, 2.9134256692486815e-05, 2.9134256692486815e-05, 2.9134256692486815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9134256692486815e-05

Optimization complete. Final v2v error: 4.45613956451416 mm

Highest mean error: 4.937740325927734 mm for frame 134

Lowest mean error: 4.225244998931885 mm for frame 76

Saving results

Total time: 164.82516503334045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00570465
Iteration 2/25 | Loss: 0.00112691
Iteration 3/25 | Loss: 0.00102792
Iteration 4/25 | Loss: 0.00101316
Iteration 5/25 | Loss: 0.00100815
Iteration 6/25 | Loss: 0.00100730
Iteration 7/25 | Loss: 0.00100730
Iteration 8/25 | Loss: 0.00100730
Iteration 9/25 | Loss: 0.00100730
Iteration 10/25 | Loss: 0.00100730
Iteration 11/25 | Loss: 0.00100730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001007303362712264, 0.001007303362712264, 0.001007303362712264, 0.001007303362712264, 0.001007303362712264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001007303362712264

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.49401283
Iteration 2/25 | Loss: 0.00042711
Iteration 3/25 | Loss: 0.00042710
Iteration 4/25 | Loss: 0.00042710
Iteration 5/25 | Loss: 0.00042710
Iteration 6/25 | Loss: 0.00042710
Iteration 7/25 | Loss: 0.00042710
Iteration 8/25 | Loss: 0.00042710
Iteration 9/25 | Loss: 0.00042710
Iteration 10/25 | Loss: 0.00042710
Iteration 11/25 | Loss: 0.00042710
Iteration 12/25 | Loss: 0.00042710
Iteration 13/25 | Loss: 0.00042710
Iteration 14/25 | Loss: 0.00042710
Iteration 15/25 | Loss: 0.00042710
Iteration 16/25 | Loss: 0.00042710
Iteration 17/25 | Loss: 0.00042710
Iteration 18/25 | Loss: 0.00042710
Iteration 19/25 | Loss: 0.00042710
Iteration 20/25 | Loss: 0.00042710
Iteration 21/25 | Loss: 0.00042710
Iteration 22/25 | Loss: 0.00042710
Iteration 23/25 | Loss: 0.00042710
Iteration 24/25 | Loss: 0.00042710
Iteration 25/25 | Loss: 0.00042710

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042710
Iteration 2/1000 | Loss: 0.00003048
Iteration 3/1000 | Loss: 0.00001821
Iteration 4/1000 | Loss: 0.00001716
Iteration 5/1000 | Loss: 0.00001626
Iteration 6/1000 | Loss: 0.00001588
Iteration 7/1000 | Loss: 0.00001560
Iteration 8/1000 | Loss: 0.00001548
Iteration 9/1000 | Loss: 0.00001540
Iteration 10/1000 | Loss: 0.00001540
Iteration 11/1000 | Loss: 0.00001539
Iteration 12/1000 | Loss: 0.00001526
Iteration 13/1000 | Loss: 0.00001522
Iteration 14/1000 | Loss: 0.00001515
Iteration 15/1000 | Loss: 0.00001512
Iteration 16/1000 | Loss: 0.00001511
Iteration 17/1000 | Loss: 0.00001511
Iteration 18/1000 | Loss: 0.00001510
Iteration 19/1000 | Loss: 0.00001510
Iteration 20/1000 | Loss: 0.00001509
Iteration 21/1000 | Loss: 0.00001509
Iteration 22/1000 | Loss: 0.00001509
Iteration 23/1000 | Loss: 0.00001508
Iteration 24/1000 | Loss: 0.00001508
Iteration 25/1000 | Loss: 0.00001508
Iteration 26/1000 | Loss: 0.00001507
Iteration 27/1000 | Loss: 0.00001507
Iteration 28/1000 | Loss: 0.00001507
Iteration 29/1000 | Loss: 0.00001507
Iteration 30/1000 | Loss: 0.00001507
Iteration 31/1000 | Loss: 0.00001506
Iteration 32/1000 | Loss: 0.00001506
Iteration 33/1000 | Loss: 0.00001506
Iteration 34/1000 | Loss: 0.00001506
Iteration 35/1000 | Loss: 0.00001505
Iteration 36/1000 | Loss: 0.00001505
Iteration 37/1000 | Loss: 0.00001505
Iteration 38/1000 | Loss: 0.00001505
Iteration 39/1000 | Loss: 0.00001504
Iteration 40/1000 | Loss: 0.00001504
Iteration 41/1000 | Loss: 0.00001504
Iteration 42/1000 | Loss: 0.00001504
Iteration 43/1000 | Loss: 0.00001504
Iteration 44/1000 | Loss: 0.00001503
Iteration 45/1000 | Loss: 0.00001503
Iteration 46/1000 | Loss: 0.00001503
Iteration 47/1000 | Loss: 0.00001502
Iteration 48/1000 | Loss: 0.00001502
Iteration 49/1000 | Loss: 0.00001501
Iteration 50/1000 | Loss: 0.00001501
Iteration 51/1000 | Loss: 0.00001501
Iteration 52/1000 | Loss: 0.00001500
Iteration 53/1000 | Loss: 0.00001500
Iteration 54/1000 | Loss: 0.00001500
Iteration 55/1000 | Loss: 0.00001500
Iteration 56/1000 | Loss: 0.00001500
Iteration 57/1000 | Loss: 0.00001500
Iteration 58/1000 | Loss: 0.00001500
Iteration 59/1000 | Loss: 0.00001500
Iteration 60/1000 | Loss: 0.00001500
Iteration 61/1000 | Loss: 0.00001499
Iteration 62/1000 | Loss: 0.00001499
Iteration 63/1000 | Loss: 0.00001499
Iteration 64/1000 | Loss: 0.00001498
Iteration 65/1000 | Loss: 0.00001498
Iteration 66/1000 | Loss: 0.00001498
Iteration 67/1000 | Loss: 0.00001498
Iteration 68/1000 | Loss: 0.00001498
Iteration 69/1000 | Loss: 0.00001498
Iteration 70/1000 | Loss: 0.00001498
Iteration 71/1000 | Loss: 0.00001498
Iteration 72/1000 | Loss: 0.00001497
Iteration 73/1000 | Loss: 0.00001497
Iteration 74/1000 | Loss: 0.00001497
Iteration 75/1000 | Loss: 0.00001497
Iteration 76/1000 | Loss: 0.00001497
Iteration 77/1000 | Loss: 0.00001497
Iteration 78/1000 | Loss: 0.00001497
Iteration 79/1000 | Loss: 0.00001497
Iteration 80/1000 | Loss: 0.00001496
Iteration 81/1000 | Loss: 0.00001496
Iteration 82/1000 | Loss: 0.00001496
Iteration 83/1000 | Loss: 0.00001496
Iteration 84/1000 | Loss: 0.00001496
Iteration 85/1000 | Loss: 0.00001496
Iteration 86/1000 | Loss: 0.00001496
Iteration 87/1000 | Loss: 0.00001496
Iteration 88/1000 | Loss: 0.00001496
Iteration 89/1000 | Loss: 0.00001496
Iteration 90/1000 | Loss: 0.00001496
Iteration 91/1000 | Loss: 0.00001496
Iteration 92/1000 | Loss: 0.00001496
Iteration 93/1000 | Loss: 0.00001496
Iteration 94/1000 | Loss: 0.00001496
Iteration 95/1000 | Loss: 0.00001496
Iteration 96/1000 | Loss: 0.00001496
Iteration 97/1000 | Loss: 0.00001496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.496272216172656e-05, 1.496272216172656e-05, 1.496272216172656e-05, 1.496272216172656e-05, 1.496272216172656e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.496272216172656e-05

Optimization complete. Final v2v error: 3.3201942443847656 mm

Highest mean error: 4.359379291534424 mm for frame 62

Lowest mean error: 2.6047351360321045 mm for frame 158

Saving results

Total time: 27.610114574432373
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984223
Iteration 2/25 | Loss: 0.00138494
Iteration 3/25 | Loss: 0.00114067
Iteration 4/25 | Loss: 0.00111157
Iteration 5/25 | Loss: 0.00110629
Iteration 6/25 | Loss: 0.00110568
Iteration 7/25 | Loss: 0.00110568
Iteration 8/25 | Loss: 0.00110568
Iteration 9/25 | Loss: 0.00110568
Iteration 10/25 | Loss: 0.00110568
Iteration 11/25 | Loss: 0.00110568
Iteration 12/25 | Loss: 0.00110568
Iteration 13/25 | Loss: 0.00110568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001105675706639886, 0.001105675706639886, 0.001105675706639886, 0.001105675706639886, 0.001105675706639886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001105675706639886

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06620932
Iteration 2/25 | Loss: 0.00072779
Iteration 3/25 | Loss: 0.00072779
Iteration 4/25 | Loss: 0.00072779
Iteration 5/25 | Loss: 0.00072779
Iteration 6/25 | Loss: 0.00072779
Iteration 7/25 | Loss: 0.00072779
Iteration 8/25 | Loss: 0.00072779
Iteration 9/25 | Loss: 0.00072779
Iteration 10/25 | Loss: 0.00072779
Iteration 11/25 | Loss: 0.00072779
Iteration 12/25 | Loss: 0.00072779
Iteration 13/25 | Loss: 0.00072779
Iteration 14/25 | Loss: 0.00072779
Iteration 15/25 | Loss: 0.00072779
Iteration 16/25 | Loss: 0.00072779
Iteration 17/25 | Loss: 0.00072779
Iteration 18/25 | Loss: 0.00072779
Iteration 19/25 | Loss: 0.00072779
Iteration 20/25 | Loss: 0.00072779
Iteration 21/25 | Loss: 0.00072779
Iteration 22/25 | Loss: 0.00072779
Iteration 23/25 | Loss: 0.00072779
Iteration 24/25 | Loss: 0.00072779
Iteration 25/25 | Loss: 0.00072779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072779
Iteration 2/1000 | Loss: 0.00005758
Iteration 3/1000 | Loss: 0.00003628
Iteration 4/1000 | Loss: 0.00003235
Iteration 5/1000 | Loss: 0.00003122
Iteration 6/1000 | Loss: 0.00003053
Iteration 7/1000 | Loss: 0.00003014
Iteration 8/1000 | Loss: 0.00002986
Iteration 9/1000 | Loss: 0.00002967
Iteration 10/1000 | Loss: 0.00002950
Iteration 11/1000 | Loss: 0.00002928
Iteration 12/1000 | Loss: 0.00002916
Iteration 13/1000 | Loss: 0.00002902
Iteration 14/1000 | Loss: 0.00002899
Iteration 15/1000 | Loss: 0.00002893
Iteration 16/1000 | Loss: 0.00002891
Iteration 17/1000 | Loss: 0.00002890
Iteration 18/1000 | Loss: 0.00002884
Iteration 19/1000 | Loss: 0.00002880
Iteration 20/1000 | Loss: 0.00002880
Iteration 21/1000 | Loss: 0.00002870
Iteration 22/1000 | Loss: 0.00002857
Iteration 23/1000 | Loss: 0.00002854
Iteration 24/1000 | Loss: 0.00002850
Iteration 25/1000 | Loss: 0.00002850
Iteration 26/1000 | Loss: 0.00002850
Iteration 27/1000 | Loss: 0.00002850
Iteration 28/1000 | Loss: 0.00002850
Iteration 29/1000 | Loss: 0.00002850
Iteration 30/1000 | Loss: 0.00002850
Iteration 31/1000 | Loss: 0.00002849
Iteration 32/1000 | Loss: 0.00002849
Iteration 33/1000 | Loss: 0.00002849
Iteration 34/1000 | Loss: 0.00002849
Iteration 35/1000 | Loss: 0.00002849
Iteration 36/1000 | Loss: 0.00002848
Iteration 37/1000 | Loss: 0.00002847
Iteration 38/1000 | Loss: 0.00002846
Iteration 39/1000 | Loss: 0.00002846
Iteration 40/1000 | Loss: 0.00002846
Iteration 41/1000 | Loss: 0.00002845
Iteration 42/1000 | Loss: 0.00002843
Iteration 43/1000 | Loss: 0.00002842
Iteration 44/1000 | Loss: 0.00002842
Iteration 45/1000 | Loss: 0.00002841
Iteration 46/1000 | Loss: 0.00002840
Iteration 47/1000 | Loss: 0.00002839
Iteration 48/1000 | Loss: 0.00002839
Iteration 49/1000 | Loss: 0.00002830
Iteration 50/1000 | Loss: 0.00002825
Iteration 51/1000 | Loss: 0.00002823
Iteration 52/1000 | Loss: 0.00002821
Iteration 53/1000 | Loss: 0.00002817
Iteration 54/1000 | Loss: 0.00002817
Iteration 55/1000 | Loss: 0.00002817
Iteration 56/1000 | Loss: 0.00002816
Iteration 57/1000 | Loss: 0.00002815
Iteration 58/1000 | Loss: 0.00002815
Iteration 59/1000 | Loss: 0.00002814
Iteration 60/1000 | Loss: 0.00002814
Iteration 61/1000 | Loss: 0.00002814
Iteration 62/1000 | Loss: 0.00002813
Iteration 63/1000 | Loss: 0.00002813
Iteration 64/1000 | Loss: 0.00002813
Iteration 65/1000 | Loss: 0.00002813
Iteration 66/1000 | Loss: 0.00002813
Iteration 67/1000 | Loss: 0.00002813
Iteration 68/1000 | Loss: 0.00002812
Iteration 69/1000 | Loss: 0.00002812
Iteration 70/1000 | Loss: 0.00002812
Iteration 71/1000 | Loss: 0.00002812
Iteration 72/1000 | Loss: 0.00002812
Iteration 73/1000 | Loss: 0.00002811
Iteration 74/1000 | Loss: 0.00002811
Iteration 75/1000 | Loss: 0.00002810
Iteration 76/1000 | Loss: 0.00002810
Iteration 77/1000 | Loss: 0.00002810
Iteration 78/1000 | Loss: 0.00002810
Iteration 79/1000 | Loss: 0.00002809
Iteration 80/1000 | Loss: 0.00002809
Iteration 81/1000 | Loss: 0.00002809
Iteration 82/1000 | Loss: 0.00002809
Iteration 83/1000 | Loss: 0.00002808
Iteration 84/1000 | Loss: 0.00002808
Iteration 85/1000 | Loss: 0.00002807
Iteration 86/1000 | Loss: 0.00002807
Iteration 87/1000 | Loss: 0.00002807
Iteration 88/1000 | Loss: 0.00002807
Iteration 89/1000 | Loss: 0.00002807
Iteration 90/1000 | Loss: 0.00002807
Iteration 91/1000 | Loss: 0.00002807
Iteration 92/1000 | Loss: 0.00002807
Iteration 93/1000 | Loss: 0.00002807
Iteration 94/1000 | Loss: 0.00002806
Iteration 95/1000 | Loss: 0.00002806
Iteration 96/1000 | Loss: 0.00002806
Iteration 97/1000 | Loss: 0.00002806
Iteration 98/1000 | Loss: 0.00002806
Iteration 99/1000 | Loss: 0.00002806
Iteration 100/1000 | Loss: 0.00002806
Iteration 101/1000 | Loss: 0.00002806
Iteration 102/1000 | Loss: 0.00002806
Iteration 103/1000 | Loss: 0.00002806
Iteration 104/1000 | Loss: 0.00002806
Iteration 105/1000 | Loss: 0.00002806
Iteration 106/1000 | Loss: 0.00002806
Iteration 107/1000 | Loss: 0.00002806
Iteration 108/1000 | Loss: 0.00002806
Iteration 109/1000 | Loss: 0.00002806
Iteration 110/1000 | Loss: 0.00002806
Iteration 111/1000 | Loss: 0.00002806
Iteration 112/1000 | Loss: 0.00002806
Iteration 113/1000 | Loss: 0.00002806
Iteration 114/1000 | Loss: 0.00002806
Iteration 115/1000 | Loss: 0.00002806
Iteration 116/1000 | Loss: 0.00002806
Iteration 117/1000 | Loss: 0.00002806
Iteration 118/1000 | Loss: 0.00002806
Iteration 119/1000 | Loss: 0.00002806
Iteration 120/1000 | Loss: 0.00002806
Iteration 121/1000 | Loss: 0.00002806
Iteration 122/1000 | Loss: 0.00002806
Iteration 123/1000 | Loss: 0.00002806
Iteration 124/1000 | Loss: 0.00002806
Iteration 125/1000 | Loss: 0.00002806
Iteration 126/1000 | Loss: 0.00002806
Iteration 127/1000 | Loss: 0.00002806
Iteration 128/1000 | Loss: 0.00002806
Iteration 129/1000 | Loss: 0.00002806
Iteration 130/1000 | Loss: 0.00002806
Iteration 131/1000 | Loss: 0.00002806
Iteration 132/1000 | Loss: 0.00002806
Iteration 133/1000 | Loss: 0.00002806
Iteration 134/1000 | Loss: 0.00002806
Iteration 135/1000 | Loss: 0.00002806
Iteration 136/1000 | Loss: 0.00002806
Iteration 137/1000 | Loss: 0.00002806
Iteration 138/1000 | Loss: 0.00002806
Iteration 139/1000 | Loss: 0.00002806
Iteration 140/1000 | Loss: 0.00002806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.8059768737875856e-05, 2.8059768737875856e-05, 2.8059768737875856e-05, 2.8059768737875856e-05, 2.8059768737875856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8059768737875856e-05

Optimization complete. Final v2v error: 4.366803169250488 mm

Highest mean error: 5.2139787673950195 mm for frame 79

Lowest mean error: 3.7571282386779785 mm for frame 175

Saving results

Total time: 44.51196765899658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977267
Iteration 2/25 | Loss: 0.00113333
Iteration 3/25 | Loss: 0.00100564
Iteration 4/25 | Loss: 0.00098539
Iteration 5/25 | Loss: 0.00097862
Iteration 6/25 | Loss: 0.00097754
Iteration 7/25 | Loss: 0.00097754
Iteration 8/25 | Loss: 0.00097754
Iteration 9/25 | Loss: 0.00097754
Iteration 10/25 | Loss: 0.00097754
Iteration 11/25 | Loss: 0.00097754
Iteration 12/25 | Loss: 0.00097754
Iteration 13/25 | Loss: 0.00097754
Iteration 14/25 | Loss: 0.00097754
Iteration 15/25 | Loss: 0.00097754
Iteration 16/25 | Loss: 0.00097754
Iteration 17/25 | Loss: 0.00097754
Iteration 18/25 | Loss: 0.00097754
Iteration 19/25 | Loss: 0.00097754
Iteration 20/25 | Loss: 0.00097754
Iteration 21/25 | Loss: 0.00097754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009775375947356224, 0.0009775375947356224, 0.0009775375947356224, 0.0009775375947356224, 0.0009775375947356224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009775375947356224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50807190
Iteration 2/25 | Loss: 0.00035412
Iteration 3/25 | Loss: 0.00035412
Iteration 4/25 | Loss: 0.00035412
Iteration 5/25 | Loss: 0.00035412
Iteration 6/25 | Loss: 0.00035412
Iteration 7/25 | Loss: 0.00035412
Iteration 8/25 | Loss: 0.00035412
Iteration 9/25 | Loss: 0.00035412
Iteration 10/25 | Loss: 0.00035412
Iteration 11/25 | Loss: 0.00035412
Iteration 12/25 | Loss: 0.00035412
Iteration 13/25 | Loss: 0.00035412
Iteration 14/25 | Loss: 0.00035412
Iteration 15/25 | Loss: 0.00035412
Iteration 16/25 | Loss: 0.00035412
Iteration 17/25 | Loss: 0.00035412
Iteration 18/25 | Loss: 0.00035412
Iteration 19/25 | Loss: 0.00035412
Iteration 20/25 | Loss: 0.00035412
Iteration 21/25 | Loss: 0.00035412
Iteration 22/25 | Loss: 0.00035412
Iteration 23/25 | Loss: 0.00035412
Iteration 24/25 | Loss: 0.00035412
Iteration 25/25 | Loss: 0.00035412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035412
Iteration 2/1000 | Loss: 0.00001963
Iteration 3/1000 | Loss: 0.00001388
Iteration 4/1000 | Loss: 0.00001285
Iteration 5/1000 | Loss: 0.00001241
Iteration 6/1000 | Loss: 0.00001224
Iteration 7/1000 | Loss: 0.00001223
Iteration 8/1000 | Loss: 0.00001222
Iteration 9/1000 | Loss: 0.00001218
Iteration 10/1000 | Loss: 0.00001217
Iteration 11/1000 | Loss: 0.00001216
Iteration 12/1000 | Loss: 0.00001216
Iteration 13/1000 | Loss: 0.00001215
Iteration 14/1000 | Loss: 0.00001215
Iteration 15/1000 | Loss: 0.00001215
Iteration 16/1000 | Loss: 0.00001205
Iteration 17/1000 | Loss: 0.00001202
Iteration 18/1000 | Loss: 0.00001199
Iteration 19/1000 | Loss: 0.00001198
Iteration 20/1000 | Loss: 0.00001197
Iteration 21/1000 | Loss: 0.00001197
Iteration 22/1000 | Loss: 0.00001196
Iteration 23/1000 | Loss: 0.00001196
Iteration 24/1000 | Loss: 0.00001196
Iteration 25/1000 | Loss: 0.00001196
Iteration 26/1000 | Loss: 0.00001194
Iteration 27/1000 | Loss: 0.00001194
Iteration 28/1000 | Loss: 0.00001194
Iteration 29/1000 | Loss: 0.00001193
Iteration 30/1000 | Loss: 0.00001193
Iteration 31/1000 | Loss: 0.00001192
Iteration 32/1000 | Loss: 0.00001191
Iteration 33/1000 | Loss: 0.00001191
Iteration 34/1000 | Loss: 0.00001191
Iteration 35/1000 | Loss: 0.00001191
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001190
Iteration 38/1000 | Loss: 0.00001190
Iteration 39/1000 | Loss: 0.00001190
Iteration 40/1000 | Loss: 0.00001190
Iteration 41/1000 | Loss: 0.00001190
Iteration 42/1000 | Loss: 0.00001190
Iteration 43/1000 | Loss: 0.00001190
Iteration 44/1000 | Loss: 0.00001190
Iteration 45/1000 | Loss: 0.00001190
Iteration 46/1000 | Loss: 0.00001190
Iteration 47/1000 | Loss: 0.00001189
Iteration 48/1000 | Loss: 0.00001189
Iteration 49/1000 | Loss: 0.00001189
Iteration 50/1000 | Loss: 0.00001189
Iteration 51/1000 | Loss: 0.00001189
Iteration 52/1000 | Loss: 0.00001189
Iteration 53/1000 | Loss: 0.00001189
Iteration 54/1000 | Loss: 0.00001188
Iteration 55/1000 | Loss: 0.00001188
Iteration 56/1000 | Loss: 0.00001188
Iteration 57/1000 | Loss: 0.00001188
Iteration 58/1000 | Loss: 0.00001187
Iteration 59/1000 | Loss: 0.00001187
Iteration 60/1000 | Loss: 0.00001187
Iteration 61/1000 | Loss: 0.00001187
Iteration 62/1000 | Loss: 0.00001187
Iteration 63/1000 | Loss: 0.00001187
Iteration 64/1000 | Loss: 0.00001186
Iteration 65/1000 | Loss: 0.00001186
Iteration 66/1000 | Loss: 0.00001186
Iteration 67/1000 | Loss: 0.00001185
Iteration 68/1000 | Loss: 0.00001185
Iteration 69/1000 | Loss: 0.00001185
Iteration 70/1000 | Loss: 0.00001185
Iteration 71/1000 | Loss: 0.00001184
Iteration 72/1000 | Loss: 0.00001184
Iteration 73/1000 | Loss: 0.00001184
Iteration 74/1000 | Loss: 0.00001184
Iteration 75/1000 | Loss: 0.00001184
Iteration 76/1000 | Loss: 0.00001184
Iteration 77/1000 | Loss: 0.00001184
Iteration 78/1000 | Loss: 0.00001183
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00001183
Iteration 81/1000 | Loss: 0.00001183
Iteration 82/1000 | Loss: 0.00001183
Iteration 83/1000 | Loss: 0.00001182
Iteration 84/1000 | Loss: 0.00001182
Iteration 85/1000 | Loss: 0.00001182
Iteration 86/1000 | Loss: 0.00001182
Iteration 87/1000 | Loss: 0.00001182
Iteration 88/1000 | Loss: 0.00001182
Iteration 89/1000 | Loss: 0.00001182
Iteration 90/1000 | Loss: 0.00001182
Iteration 91/1000 | Loss: 0.00001182
Iteration 92/1000 | Loss: 0.00001182
Iteration 93/1000 | Loss: 0.00001182
Iteration 94/1000 | Loss: 0.00001181
Iteration 95/1000 | Loss: 0.00001181
Iteration 96/1000 | Loss: 0.00001181
Iteration 97/1000 | Loss: 0.00001181
Iteration 98/1000 | Loss: 0.00001181
Iteration 99/1000 | Loss: 0.00001181
Iteration 100/1000 | Loss: 0.00001181
Iteration 101/1000 | Loss: 0.00001181
Iteration 102/1000 | Loss: 0.00001181
Iteration 103/1000 | Loss: 0.00001181
Iteration 104/1000 | Loss: 0.00001180
Iteration 105/1000 | Loss: 0.00001180
Iteration 106/1000 | Loss: 0.00001180
Iteration 107/1000 | Loss: 0.00001180
Iteration 108/1000 | Loss: 0.00001180
Iteration 109/1000 | Loss: 0.00001180
Iteration 110/1000 | Loss: 0.00001180
Iteration 111/1000 | Loss: 0.00001180
Iteration 112/1000 | Loss: 0.00001180
Iteration 113/1000 | Loss: 0.00001180
Iteration 114/1000 | Loss: 0.00001180
Iteration 115/1000 | Loss: 0.00001180
Iteration 116/1000 | Loss: 0.00001180
Iteration 117/1000 | Loss: 0.00001179
Iteration 118/1000 | Loss: 0.00001179
Iteration 119/1000 | Loss: 0.00001179
Iteration 120/1000 | Loss: 0.00001179
Iteration 121/1000 | Loss: 0.00001179
Iteration 122/1000 | Loss: 0.00001179
Iteration 123/1000 | Loss: 0.00001179
Iteration 124/1000 | Loss: 0.00001179
Iteration 125/1000 | Loss: 0.00001179
Iteration 126/1000 | Loss: 0.00001179
Iteration 127/1000 | Loss: 0.00001179
Iteration 128/1000 | Loss: 0.00001178
Iteration 129/1000 | Loss: 0.00001178
Iteration 130/1000 | Loss: 0.00001178
Iteration 131/1000 | Loss: 0.00001178
Iteration 132/1000 | Loss: 0.00001178
Iteration 133/1000 | Loss: 0.00001178
Iteration 134/1000 | Loss: 0.00001178
Iteration 135/1000 | Loss: 0.00001178
Iteration 136/1000 | Loss: 0.00001178
Iteration 137/1000 | Loss: 0.00001178
Iteration 138/1000 | Loss: 0.00001178
Iteration 139/1000 | Loss: 0.00001178
Iteration 140/1000 | Loss: 0.00001178
Iteration 141/1000 | Loss: 0.00001178
Iteration 142/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.1782555702666286e-05, 1.1782555702666286e-05, 1.1782555702666286e-05, 1.1782555702666286e-05, 1.1782555702666286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1782555702666286e-05

Optimization complete. Final v2v error: 2.925231695175171 mm

Highest mean error: 3.466242790222168 mm for frame 151

Lowest mean error: 2.537452459335327 mm for frame 239

Saving results

Total time: 32.082744121551514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00528708
Iteration 2/25 | Loss: 0.00115319
Iteration 3/25 | Loss: 0.00105511
Iteration 4/25 | Loss: 0.00102758
Iteration 5/25 | Loss: 0.00101750
Iteration 6/25 | Loss: 0.00101481
Iteration 7/25 | Loss: 0.00101350
Iteration 8/25 | Loss: 0.00101324
Iteration 9/25 | Loss: 0.00101324
Iteration 10/25 | Loss: 0.00101324
Iteration 11/25 | Loss: 0.00101324
Iteration 12/25 | Loss: 0.00101324
Iteration 13/25 | Loss: 0.00101324
Iteration 14/25 | Loss: 0.00101323
Iteration 15/25 | Loss: 0.00101323
Iteration 16/25 | Loss: 0.00101323
Iteration 17/25 | Loss: 0.00101323
Iteration 18/25 | Loss: 0.00101323
Iteration 19/25 | Loss: 0.00101323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010132280876860023, 0.0010132280876860023, 0.0010132280876860023, 0.0010132280876860023, 0.0010132280876860023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010132280876860023

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.13692093
Iteration 2/25 | Loss: 0.00041989
Iteration 3/25 | Loss: 0.00041989
Iteration 4/25 | Loss: 0.00041989
Iteration 5/25 | Loss: 0.00041989
Iteration 6/25 | Loss: 0.00041988
Iteration 7/25 | Loss: 0.00041988
Iteration 8/25 | Loss: 0.00041988
Iteration 9/25 | Loss: 0.00041988
Iteration 10/25 | Loss: 0.00041988
Iteration 11/25 | Loss: 0.00041988
Iteration 12/25 | Loss: 0.00041988
Iteration 13/25 | Loss: 0.00041988
Iteration 14/25 | Loss: 0.00041988
Iteration 15/25 | Loss: 0.00041988
Iteration 16/25 | Loss: 0.00041988
Iteration 17/25 | Loss: 0.00041988
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000419883755967021, 0.000419883755967021, 0.000419883755967021, 0.000419883755967021, 0.000419883755967021]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000419883755967021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041988
Iteration 2/1000 | Loss: 0.00004228
Iteration 3/1000 | Loss: 0.00002538
Iteration 4/1000 | Loss: 0.00002191
Iteration 5/1000 | Loss: 0.00002115
Iteration 6/1000 | Loss: 0.00002049
Iteration 7/1000 | Loss: 0.00001995
Iteration 8/1000 | Loss: 0.00001964
Iteration 9/1000 | Loss: 0.00001941
Iteration 10/1000 | Loss: 0.00001925
Iteration 11/1000 | Loss: 0.00001923
Iteration 12/1000 | Loss: 0.00001915
Iteration 13/1000 | Loss: 0.00001901
Iteration 14/1000 | Loss: 0.00001900
Iteration 15/1000 | Loss: 0.00001899
Iteration 16/1000 | Loss: 0.00001898
Iteration 17/1000 | Loss: 0.00001897
Iteration 18/1000 | Loss: 0.00001893
Iteration 19/1000 | Loss: 0.00001893
Iteration 20/1000 | Loss: 0.00001890
Iteration 21/1000 | Loss: 0.00001890
Iteration 22/1000 | Loss: 0.00001890
Iteration 23/1000 | Loss: 0.00001890
Iteration 24/1000 | Loss: 0.00001890
Iteration 25/1000 | Loss: 0.00001890
Iteration 26/1000 | Loss: 0.00001889
Iteration 27/1000 | Loss: 0.00001889
Iteration 28/1000 | Loss: 0.00001888
Iteration 29/1000 | Loss: 0.00001887
Iteration 30/1000 | Loss: 0.00001884
Iteration 31/1000 | Loss: 0.00001881
Iteration 32/1000 | Loss: 0.00001881
Iteration 33/1000 | Loss: 0.00001880
Iteration 34/1000 | Loss: 0.00001880
Iteration 35/1000 | Loss: 0.00001880
Iteration 36/1000 | Loss: 0.00001879
Iteration 37/1000 | Loss: 0.00001879
Iteration 38/1000 | Loss: 0.00001878
Iteration 39/1000 | Loss: 0.00001878
Iteration 40/1000 | Loss: 0.00001878
Iteration 41/1000 | Loss: 0.00001877
Iteration 42/1000 | Loss: 0.00001877
Iteration 43/1000 | Loss: 0.00001877
Iteration 44/1000 | Loss: 0.00001877
Iteration 45/1000 | Loss: 0.00001877
Iteration 46/1000 | Loss: 0.00001877
Iteration 47/1000 | Loss: 0.00001876
Iteration 48/1000 | Loss: 0.00001876
Iteration 49/1000 | Loss: 0.00001876
Iteration 50/1000 | Loss: 0.00001876
Iteration 51/1000 | Loss: 0.00001876
Iteration 52/1000 | Loss: 0.00001876
Iteration 53/1000 | Loss: 0.00001875
Iteration 54/1000 | Loss: 0.00001875
Iteration 55/1000 | Loss: 0.00001875
Iteration 56/1000 | Loss: 0.00001875
Iteration 57/1000 | Loss: 0.00001874
Iteration 58/1000 | Loss: 0.00001874
Iteration 59/1000 | Loss: 0.00001874
Iteration 60/1000 | Loss: 0.00001874
Iteration 61/1000 | Loss: 0.00001873
Iteration 62/1000 | Loss: 0.00001873
Iteration 63/1000 | Loss: 0.00001873
Iteration 64/1000 | Loss: 0.00001873
Iteration 65/1000 | Loss: 0.00001873
Iteration 66/1000 | Loss: 0.00001873
Iteration 67/1000 | Loss: 0.00001873
Iteration 68/1000 | Loss: 0.00001873
Iteration 69/1000 | Loss: 0.00001872
Iteration 70/1000 | Loss: 0.00001872
Iteration 71/1000 | Loss: 0.00001872
Iteration 72/1000 | Loss: 0.00001872
Iteration 73/1000 | Loss: 0.00001872
Iteration 74/1000 | Loss: 0.00001872
Iteration 75/1000 | Loss: 0.00001872
Iteration 76/1000 | Loss: 0.00001872
Iteration 77/1000 | Loss: 0.00001872
Iteration 78/1000 | Loss: 0.00001871
Iteration 79/1000 | Loss: 0.00001871
Iteration 80/1000 | Loss: 0.00001871
Iteration 81/1000 | Loss: 0.00001871
Iteration 82/1000 | Loss: 0.00001871
Iteration 83/1000 | Loss: 0.00001871
Iteration 84/1000 | Loss: 0.00001871
Iteration 85/1000 | Loss: 0.00001871
Iteration 86/1000 | Loss: 0.00001870
Iteration 87/1000 | Loss: 0.00001870
Iteration 88/1000 | Loss: 0.00001870
Iteration 89/1000 | Loss: 0.00001870
Iteration 90/1000 | Loss: 0.00001870
Iteration 91/1000 | Loss: 0.00001870
Iteration 92/1000 | Loss: 0.00001870
Iteration 93/1000 | Loss: 0.00001869
Iteration 94/1000 | Loss: 0.00001869
Iteration 95/1000 | Loss: 0.00001869
Iteration 96/1000 | Loss: 0.00001869
Iteration 97/1000 | Loss: 0.00001869
Iteration 98/1000 | Loss: 0.00001869
Iteration 99/1000 | Loss: 0.00001869
Iteration 100/1000 | Loss: 0.00001869
Iteration 101/1000 | Loss: 0.00001868
Iteration 102/1000 | Loss: 0.00001868
Iteration 103/1000 | Loss: 0.00001868
Iteration 104/1000 | Loss: 0.00001868
Iteration 105/1000 | Loss: 0.00001867
Iteration 106/1000 | Loss: 0.00001867
Iteration 107/1000 | Loss: 0.00001867
Iteration 108/1000 | Loss: 0.00001867
Iteration 109/1000 | Loss: 0.00001866
Iteration 110/1000 | Loss: 0.00001866
Iteration 111/1000 | Loss: 0.00001866
Iteration 112/1000 | Loss: 0.00001865
Iteration 113/1000 | Loss: 0.00001865
Iteration 114/1000 | Loss: 0.00001865
Iteration 115/1000 | Loss: 0.00001865
Iteration 116/1000 | Loss: 0.00001865
Iteration 117/1000 | Loss: 0.00001865
Iteration 118/1000 | Loss: 0.00001864
Iteration 119/1000 | Loss: 0.00001864
Iteration 120/1000 | Loss: 0.00001864
Iteration 121/1000 | Loss: 0.00001863
Iteration 122/1000 | Loss: 0.00001863
Iteration 123/1000 | Loss: 0.00001863
Iteration 124/1000 | Loss: 0.00001863
Iteration 125/1000 | Loss: 0.00001862
Iteration 126/1000 | Loss: 0.00001862
Iteration 127/1000 | Loss: 0.00001862
Iteration 128/1000 | Loss: 0.00001862
Iteration 129/1000 | Loss: 0.00001862
Iteration 130/1000 | Loss: 0.00001862
Iteration 131/1000 | Loss: 0.00001861
Iteration 132/1000 | Loss: 0.00001861
Iteration 133/1000 | Loss: 0.00001861
Iteration 134/1000 | Loss: 0.00001860
Iteration 135/1000 | Loss: 0.00001860
Iteration 136/1000 | Loss: 0.00001860
Iteration 137/1000 | Loss: 0.00001860
Iteration 138/1000 | Loss: 0.00001859
Iteration 139/1000 | Loss: 0.00001859
Iteration 140/1000 | Loss: 0.00001859
Iteration 141/1000 | Loss: 0.00001859
Iteration 142/1000 | Loss: 0.00001859
Iteration 143/1000 | Loss: 0.00001859
Iteration 144/1000 | Loss: 0.00001859
Iteration 145/1000 | Loss: 0.00001859
Iteration 146/1000 | Loss: 0.00001859
Iteration 147/1000 | Loss: 0.00001858
Iteration 148/1000 | Loss: 0.00001858
Iteration 149/1000 | Loss: 0.00001858
Iteration 150/1000 | Loss: 0.00001858
Iteration 151/1000 | Loss: 0.00001858
Iteration 152/1000 | Loss: 0.00001857
Iteration 153/1000 | Loss: 0.00001857
Iteration 154/1000 | Loss: 0.00001857
Iteration 155/1000 | Loss: 0.00001857
Iteration 156/1000 | Loss: 0.00001857
Iteration 157/1000 | Loss: 0.00001856
Iteration 158/1000 | Loss: 0.00001856
Iteration 159/1000 | Loss: 0.00001856
Iteration 160/1000 | Loss: 0.00001856
Iteration 161/1000 | Loss: 0.00001856
Iteration 162/1000 | Loss: 0.00001856
Iteration 163/1000 | Loss: 0.00001856
Iteration 164/1000 | Loss: 0.00001855
Iteration 165/1000 | Loss: 0.00001855
Iteration 166/1000 | Loss: 0.00001855
Iteration 167/1000 | Loss: 0.00001854
Iteration 168/1000 | Loss: 0.00001854
Iteration 169/1000 | Loss: 0.00001854
Iteration 170/1000 | Loss: 0.00001854
Iteration 171/1000 | Loss: 0.00001854
Iteration 172/1000 | Loss: 0.00001854
Iteration 173/1000 | Loss: 0.00001854
Iteration 174/1000 | Loss: 0.00001854
Iteration 175/1000 | Loss: 0.00001854
Iteration 176/1000 | Loss: 0.00001853
Iteration 177/1000 | Loss: 0.00001853
Iteration 178/1000 | Loss: 0.00001853
Iteration 179/1000 | Loss: 0.00001853
Iteration 180/1000 | Loss: 0.00001853
Iteration 181/1000 | Loss: 0.00001853
Iteration 182/1000 | Loss: 0.00001853
Iteration 183/1000 | Loss: 0.00001853
Iteration 184/1000 | Loss: 0.00001853
Iteration 185/1000 | Loss: 0.00001852
Iteration 186/1000 | Loss: 0.00001852
Iteration 187/1000 | Loss: 0.00001852
Iteration 188/1000 | Loss: 0.00001852
Iteration 189/1000 | Loss: 0.00001852
Iteration 190/1000 | Loss: 0.00001852
Iteration 191/1000 | Loss: 0.00001851
Iteration 192/1000 | Loss: 0.00001851
Iteration 193/1000 | Loss: 0.00001851
Iteration 194/1000 | Loss: 0.00001851
Iteration 195/1000 | Loss: 0.00001851
Iteration 196/1000 | Loss: 0.00001851
Iteration 197/1000 | Loss: 0.00001851
Iteration 198/1000 | Loss: 0.00001851
Iteration 199/1000 | Loss: 0.00001851
Iteration 200/1000 | Loss: 0.00001851
Iteration 201/1000 | Loss: 0.00001851
Iteration 202/1000 | Loss: 0.00001851
Iteration 203/1000 | Loss: 0.00001851
Iteration 204/1000 | Loss: 0.00001851
Iteration 205/1000 | Loss: 0.00001851
Iteration 206/1000 | Loss: 0.00001851
Iteration 207/1000 | Loss: 0.00001851
Iteration 208/1000 | Loss: 0.00001851
Iteration 209/1000 | Loss: 0.00001851
Iteration 210/1000 | Loss: 0.00001851
Iteration 211/1000 | Loss: 0.00001851
Iteration 212/1000 | Loss: 0.00001851
Iteration 213/1000 | Loss: 0.00001851
Iteration 214/1000 | Loss: 0.00001851
Iteration 215/1000 | Loss: 0.00001851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.8513283066567965e-05, 1.8513283066567965e-05, 1.8513283066567965e-05, 1.8513283066567965e-05, 1.8513283066567965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8513283066567965e-05

Optimization complete. Final v2v error: 3.6183955669403076 mm

Highest mean error: 4.824690818786621 mm for frame 68

Lowest mean error: 2.8822362422943115 mm for frame 8

Saving results

Total time: 42.66624855995178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00663594
Iteration 2/25 | Loss: 0.00139707
Iteration 3/25 | Loss: 0.00106077
Iteration 4/25 | Loss: 0.00101580
Iteration 5/25 | Loss: 0.00101236
Iteration 6/25 | Loss: 0.00101175
Iteration 7/25 | Loss: 0.00101175
Iteration 8/25 | Loss: 0.00101175
Iteration 9/25 | Loss: 0.00101175
Iteration 10/25 | Loss: 0.00101175
Iteration 11/25 | Loss: 0.00101175
Iteration 12/25 | Loss: 0.00101175
Iteration 13/25 | Loss: 0.00101175
Iteration 14/25 | Loss: 0.00101175
Iteration 15/25 | Loss: 0.00101175
Iteration 16/25 | Loss: 0.00101175
Iteration 17/25 | Loss: 0.00101175
Iteration 18/25 | Loss: 0.00101175
Iteration 19/25 | Loss: 0.00101175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010117547353729606, 0.0010117547353729606, 0.0010117547353729606, 0.0010117547353729606, 0.0010117547353729606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010117547353729606

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15845060
Iteration 2/25 | Loss: 0.00034850
Iteration 3/25 | Loss: 0.00034848
Iteration 4/25 | Loss: 0.00034848
Iteration 5/25 | Loss: 0.00034847
Iteration 6/25 | Loss: 0.00034847
Iteration 7/25 | Loss: 0.00034847
Iteration 8/25 | Loss: 0.00034847
Iteration 9/25 | Loss: 0.00034847
Iteration 10/25 | Loss: 0.00034847
Iteration 11/25 | Loss: 0.00034847
Iteration 12/25 | Loss: 0.00034847
Iteration 13/25 | Loss: 0.00034847
Iteration 14/25 | Loss: 0.00034847
Iteration 15/25 | Loss: 0.00034847
Iteration 16/25 | Loss: 0.00034847
Iteration 17/25 | Loss: 0.00034847
Iteration 18/25 | Loss: 0.00034847
Iteration 19/25 | Loss: 0.00034847
Iteration 20/25 | Loss: 0.00034847
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003484730841591954, 0.0003484730841591954, 0.0003484730841591954, 0.0003484730841591954, 0.0003484730841591954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003484730841591954

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034847
Iteration 2/1000 | Loss: 0.00003631
Iteration 3/1000 | Loss: 0.00001665
Iteration 4/1000 | Loss: 0.00001491
Iteration 5/1000 | Loss: 0.00001412
Iteration 6/1000 | Loss: 0.00001358
Iteration 7/1000 | Loss: 0.00001321
Iteration 8/1000 | Loss: 0.00001298
Iteration 9/1000 | Loss: 0.00001278
Iteration 10/1000 | Loss: 0.00001265
Iteration 11/1000 | Loss: 0.00001264
Iteration 12/1000 | Loss: 0.00001260
Iteration 13/1000 | Loss: 0.00001259
Iteration 14/1000 | Loss: 0.00001253
Iteration 15/1000 | Loss: 0.00001250
Iteration 16/1000 | Loss: 0.00001249
Iteration 17/1000 | Loss: 0.00001249
Iteration 18/1000 | Loss: 0.00001248
Iteration 19/1000 | Loss: 0.00001248
Iteration 20/1000 | Loss: 0.00001247
Iteration 21/1000 | Loss: 0.00001246
Iteration 22/1000 | Loss: 0.00001246
Iteration 23/1000 | Loss: 0.00001245
Iteration 24/1000 | Loss: 0.00001245
Iteration 25/1000 | Loss: 0.00001244
Iteration 26/1000 | Loss: 0.00001244
Iteration 27/1000 | Loss: 0.00001243
Iteration 28/1000 | Loss: 0.00001243
Iteration 29/1000 | Loss: 0.00001243
Iteration 30/1000 | Loss: 0.00001242
Iteration 31/1000 | Loss: 0.00001242
Iteration 32/1000 | Loss: 0.00001242
Iteration 33/1000 | Loss: 0.00001240
Iteration 34/1000 | Loss: 0.00001240
Iteration 35/1000 | Loss: 0.00001240
Iteration 36/1000 | Loss: 0.00001240
Iteration 37/1000 | Loss: 0.00001239
Iteration 38/1000 | Loss: 0.00001238
Iteration 39/1000 | Loss: 0.00001238
Iteration 40/1000 | Loss: 0.00001238
Iteration 41/1000 | Loss: 0.00001238
Iteration 42/1000 | Loss: 0.00001238
Iteration 43/1000 | Loss: 0.00001238
Iteration 44/1000 | Loss: 0.00001238
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001238
Iteration 47/1000 | Loss: 0.00001238
Iteration 48/1000 | Loss: 0.00001238
Iteration 49/1000 | Loss: 0.00001237
Iteration 50/1000 | Loss: 0.00001237
Iteration 51/1000 | Loss: 0.00001237
Iteration 52/1000 | Loss: 0.00001237
Iteration 53/1000 | Loss: 0.00001237
Iteration 54/1000 | Loss: 0.00001237
Iteration 55/1000 | Loss: 0.00001237
Iteration 56/1000 | Loss: 0.00001237
Iteration 57/1000 | Loss: 0.00001237
Iteration 58/1000 | Loss: 0.00001236
Iteration 59/1000 | Loss: 0.00001236
Iteration 60/1000 | Loss: 0.00001236
Iteration 61/1000 | Loss: 0.00001236
Iteration 62/1000 | Loss: 0.00001236
Iteration 63/1000 | Loss: 0.00001236
Iteration 64/1000 | Loss: 0.00001235
Iteration 65/1000 | Loss: 0.00001235
Iteration 66/1000 | Loss: 0.00001235
Iteration 67/1000 | Loss: 0.00001235
Iteration 68/1000 | Loss: 0.00001235
Iteration 69/1000 | Loss: 0.00001235
Iteration 70/1000 | Loss: 0.00001235
Iteration 71/1000 | Loss: 0.00001234
Iteration 72/1000 | Loss: 0.00001234
Iteration 73/1000 | Loss: 0.00001234
Iteration 74/1000 | Loss: 0.00001234
Iteration 75/1000 | Loss: 0.00001234
Iteration 76/1000 | Loss: 0.00001234
Iteration 77/1000 | Loss: 0.00001234
Iteration 78/1000 | Loss: 0.00001234
Iteration 79/1000 | Loss: 0.00001234
Iteration 80/1000 | Loss: 0.00001234
Iteration 81/1000 | Loss: 0.00001234
Iteration 82/1000 | Loss: 0.00001233
Iteration 83/1000 | Loss: 0.00001233
Iteration 84/1000 | Loss: 0.00001233
Iteration 85/1000 | Loss: 0.00001233
Iteration 86/1000 | Loss: 0.00001233
Iteration 87/1000 | Loss: 0.00001233
Iteration 88/1000 | Loss: 0.00001233
Iteration 89/1000 | Loss: 0.00001233
Iteration 90/1000 | Loss: 0.00001233
Iteration 91/1000 | Loss: 0.00001233
Iteration 92/1000 | Loss: 0.00001233
Iteration 93/1000 | Loss: 0.00001232
Iteration 94/1000 | Loss: 0.00001232
Iteration 95/1000 | Loss: 0.00001232
Iteration 96/1000 | Loss: 0.00001232
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001232
Iteration 99/1000 | Loss: 0.00001232
Iteration 100/1000 | Loss: 0.00001232
Iteration 101/1000 | Loss: 0.00001232
Iteration 102/1000 | Loss: 0.00001232
Iteration 103/1000 | Loss: 0.00001232
Iteration 104/1000 | Loss: 0.00001232
Iteration 105/1000 | Loss: 0.00001232
Iteration 106/1000 | Loss: 0.00001232
Iteration 107/1000 | Loss: 0.00001232
Iteration 108/1000 | Loss: 0.00001232
Iteration 109/1000 | Loss: 0.00001232
Iteration 110/1000 | Loss: 0.00001232
Iteration 111/1000 | Loss: 0.00001232
Iteration 112/1000 | Loss: 0.00001232
Iteration 113/1000 | Loss: 0.00001232
Iteration 114/1000 | Loss: 0.00001232
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001232
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.2322106158535462e-05, 1.2322106158535462e-05, 1.2322106158535462e-05, 1.2322106158535462e-05, 1.2322106158535462e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2322106158535462e-05

Optimization complete. Final v2v error: 2.9536964893341064 mm

Highest mean error: 3.716582775115967 mm for frame 228

Lowest mean error: 2.650529146194458 mm for frame 2

Saving results

Total time: 36.10016083717346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105799
Iteration 2/25 | Loss: 0.01105799
Iteration 3/25 | Loss: 0.00164292
Iteration 4/25 | Loss: 0.00121766
Iteration 5/25 | Loss: 0.00110219
Iteration 6/25 | Loss: 0.00110166
Iteration 7/25 | Loss: 0.00109115
Iteration 8/25 | Loss: 0.00109165
Iteration 9/25 | Loss: 0.00108684
Iteration 10/25 | Loss: 0.00108662
Iteration 11/25 | Loss: 0.00108658
Iteration 12/25 | Loss: 0.00108658
Iteration 13/25 | Loss: 0.00108658
Iteration 14/25 | Loss: 0.00108658
Iteration 15/25 | Loss: 0.00108658
Iteration 16/25 | Loss: 0.00108658
Iteration 17/25 | Loss: 0.00108658
Iteration 18/25 | Loss: 0.00108658
Iteration 19/25 | Loss: 0.00108658
Iteration 20/25 | Loss: 0.00108658
Iteration 21/25 | Loss: 0.00108658
Iteration 22/25 | Loss: 0.00108658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010865796357393265, 0.0010865796357393265, 0.0010865796357393265, 0.0010865796357393265, 0.0010865796357393265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010865796357393265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54908717
Iteration 2/25 | Loss: 0.00041309
Iteration 3/25 | Loss: 0.00041308
Iteration 4/25 | Loss: 0.00041308
Iteration 5/25 | Loss: 0.00041308
Iteration 6/25 | Loss: 0.00041308
Iteration 7/25 | Loss: 0.00041308
Iteration 8/25 | Loss: 0.00041308
Iteration 9/25 | Loss: 0.00041308
Iteration 10/25 | Loss: 0.00041308
Iteration 11/25 | Loss: 0.00041308
Iteration 12/25 | Loss: 0.00041308
Iteration 13/25 | Loss: 0.00041308
Iteration 14/25 | Loss: 0.00041308
Iteration 15/25 | Loss: 0.00041308
Iteration 16/25 | Loss: 0.00041308
Iteration 17/25 | Loss: 0.00041308
Iteration 18/25 | Loss: 0.00041308
Iteration 19/25 | Loss: 0.00041308
Iteration 20/25 | Loss: 0.00041308
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00041308169602416456, 0.00041308169602416456, 0.00041308169602416456, 0.00041308169602416456, 0.00041308169602416456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00041308169602416456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041308
Iteration 2/1000 | Loss: 0.00003553
Iteration 3/1000 | Loss: 0.00002202
Iteration 4/1000 | Loss: 0.00001979
Iteration 5/1000 | Loss: 0.00001903
Iteration 6/1000 | Loss: 0.00011963
Iteration 7/1000 | Loss: 0.00001853
Iteration 8/1000 | Loss: 0.00007856
Iteration 9/1000 | Loss: 0.00001821
Iteration 10/1000 | Loss: 0.00009163
Iteration 11/1000 | Loss: 0.00002846
Iteration 12/1000 | Loss: 0.00002189
Iteration 13/1000 | Loss: 0.00001798
Iteration 14/1000 | Loss: 0.00001774
Iteration 15/1000 | Loss: 0.00001761
Iteration 16/1000 | Loss: 0.00001760
Iteration 17/1000 | Loss: 0.00001751
Iteration 18/1000 | Loss: 0.00001751
Iteration 19/1000 | Loss: 0.00001750
Iteration 20/1000 | Loss: 0.00001750
Iteration 21/1000 | Loss: 0.00001750
Iteration 22/1000 | Loss: 0.00001750
Iteration 23/1000 | Loss: 0.00001750
Iteration 24/1000 | Loss: 0.00001750
Iteration 25/1000 | Loss: 0.00001750
Iteration 26/1000 | Loss: 0.00001750
Iteration 27/1000 | Loss: 0.00001749
Iteration 28/1000 | Loss: 0.00001749
Iteration 29/1000 | Loss: 0.00001749
Iteration 30/1000 | Loss: 0.00001749
Iteration 31/1000 | Loss: 0.00001749
Iteration 32/1000 | Loss: 0.00001749
Iteration 33/1000 | Loss: 0.00001749
Iteration 34/1000 | Loss: 0.00001749
Iteration 35/1000 | Loss: 0.00001743
Iteration 36/1000 | Loss: 0.00001743
Iteration 37/1000 | Loss: 0.00001742
Iteration 38/1000 | Loss: 0.00001742
Iteration 39/1000 | Loss: 0.00001741
Iteration 40/1000 | Loss: 0.00001741
Iteration 41/1000 | Loss: 0.00001741
Iteration 42/1000 | Loss: 0.00001741
Iteration 43/1000 | Loss: 0.00001741
Iteration 44/1000 | Loss: 0.00001741
Iteration 45/1000 | Loss: 0.00001741
Iteration 46/1000 | Loss: 0.00001741
Iteration 47/1000 | Loss: 0.00001741
Iteration 48/1000 | Loss: 0.00001741
Iteration 49/1000 | Loss: 0.00001740
Iteration 50/1000 | Loss: 0.00001740
Iteration 51/1000 | Loss: 0.00001740
Iteration 52/1000 | Loss: 0.00001740
Iteration 53/1000 | Loss: 0.00001740
Iteration 54/1000 | Loss: 0.00001740
Iteration 55/1000 | Loss: 0.00001740
Iteration 56/1000 | Loss: 0.00001739
Iteration 57/1000 | Loss: 0.00001739
Iteration 58/1000 | Loss: 0.00001739
Iteration 59/1000 | Loss: 0.00001739
Iteration 60/1000 | Loss: 0.00001739
Iteration 61/1000 | Loss: 0.00001739
Iteration 62/1000 | Loss: 0.00001738
Iteration 63/1000 | Loss: 0.00001738
Iteration 64/1000 | Loss: 0.00001738
Iteration 65/1000 | Loss: 0.00001738
Iteration 66/1000 | Loss: 0.00001737
Iteration 67/1000 | Loss: 0.00001737
Iteration 68/1000 | Loss: 0.00001737
Iteration 69/1000 | Loss: 0.00001736
Iteration 70/1000 | Loss: 0.00001736
Iteration 71/1000 | Loss: 0.00001736
Iteration 72/1000 | Loss: 0.00001736
Iteration 73/1000 | Loss: 0.00001736
Iteration 74/1000 | Loss: 0.00001735
Iteration 75/1000 | Loss: 0.00001735
Iteration 76/1000 | Loss: 0.00001735
Iteration 77/1000 | Loss: 0.00001735
Iteration 78/1000 | Loss: 0.00001734
Iteration 79/1000 | Loss: 0.00001734
Iteration 80/1000 | Loss: 0.00001734
Iteration 81/1000 | Loss: 0.00001734
Iteration 82/1000 | Loss: 0.00001734
Iteration 83/1000 | Loss: 0.00001734
Iteration 84/1000 | Loss: 0.00001734
Iteration 85/1000 | Loss: 0.00001733
Iteration 86/1000 | Loss: 0.00001733
Iteration 87/1000 | Loss: 0.00001733
Iteration 88/1000 | Loss: 0.00001733
Iteration 89/1000 | Loss: 0.00001733
Iteration 90/1000 | Loss: 0.00001733
Iteration 91/1000 | Loss: 0.00001733
Iteration 92/1000 | Loss: 0.00001733
Iteration 93/1000 | Loss: 0.00001733
Iteration 94/1000 | Loss: 0.00001733
Iteration 95/1000 | Loss: 0.00001733
Iteration 96/1000 | Loss: 0.00001733
Iteration 97/1000 | Loss: 0.00001733
Iteration 98/1000 | Loss: 0.00001733
Iteration 99/1000 | Loss: 0.00001733
Iteration 100/1000 | Loss: 0.00001732
Iteration 101/1000 | Loss: 0.00001732
Iteration 102/1000 | Loss: 0.00001732
Iteration 103/1000 | Loss: 0.00001732
Iteration 104/1000 | Loss: 0.00001732
Iteration 105/1000 | Loss: 0.00001732
Iteration 106/1000 | Loss: 0.00001732
Iteration 107/1000 | Loss: 0.00001732
Iteration 108/1000 | Loss: 0.00001732
Iteration 109/1000 | Loss: 0.00001732
Iteration 110/1000 | Loss: 0.00001732
Iteration 111/1000 | Loss: 0.00001731
Iteration 112/1000 | Loss: 0.00001731
Iteration 113/1000 | Loss: 0.00001731
Iteration 114/1000 | Loss: 0.00001731
Iteration 115/1000 | Loss: 0.00001731
Iteration 116/1000 | Loss: 0.00001731
Iteration 117/1000 | Loss: 0.00001731
Iteration 118/1000 | Loss: 0.00001730
Iteration 119/1000 | Loss: 0.00001730
Iteration 120/1000 | Loss: 0.00001730
Iteration 121/1000 | Loss: 0.00001730
Iteration 122/1000 | Loss: 0.00001730
Iteration 123/1000 | Loss: 0.00001730
Iteration 124/1000 | Loss: 0.00001730
Iteration 125/1000 | Loss: 0.00001730
Iteration 126/1000 | Loss: 0.00001730
Iteration 127/1000 | Loss: 0.00001729
Iteration 128/1000 | Loss: 0.00001729
Iteration 129/1000 | Loss: 0.00001729
Iteration 130/1000 | Loss: 0.00001729
Iteration 131/1000 | Loss: 0.00001729
Iteration 132/1000 | Loss: 0.00001729
Iteration 133/1000 | Loss: 0.00001729
Iteration 134/1000 | Loss: 0.00001729
Iteration 135/1000 | Loss: 0.00001729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.729456926113926e-05, 1.729456926113926e-05, 1.729456926113926e-05, 1.729456926113926e-05, 1.729456926113926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.729456926113926e-05

Optimization complete. Final v2v error: 3.5064260959625244 mm

Highest mean error: 3.840359687805176 mm for frame 31

Lowest mean error: 3.165957450866699 mm for frame 81

Saving results

Total time: 51.152872800827026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082171
Iteration 2/25 | Loss: 0.00429114
Iteration 3/25 | Loss: 0.00307862
Iteration 4/25 | Loss: 0.00287413
Iteration 5/25 | Loss: 0.00245048
Iteration 6/25 | Loss: 0.00196364
Iteration 7/25 | Loss: 0.00137592
Iteration 8/25 | Loss: 0.00121623
Iteration 9/25 | Loss: 0.00115732
Iteration 10/25 | Loss: 0.00111685
Iteration 11/25 | Loss: 0.00109958
Iteration 12/25 | Loss: 0.00108477
Iteration 13/25 | Loss: 0.00107964
Iteration 14/25 | Loss: 0.00107521
Iteration 15/25 | Loss: 0.00106427
Iteration 16/25 | Loss: 0.00106114
Iteration 17/25 | Loss: 0.00106008
Iteration 18/25 | Loss: 0.00105966
Iteration 19/25 | Loss: 0.00105947
Iteration 20/25 | Loss: 0.00105935
Iteration 21/25 | Loss: 0.00105931
Iteration 22/25 | Loss: 0.00105931
Iteration 23/25 | Loss: 0.00105931
Iteration 24/25 | Loss: 0.00105931
Iteration 25/25 | Loss: 0.00105930

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43664491
Iteration 2/25 | Loss: 0.00077350
Iteration 3/25 | Loss: 0.00077350
Iteration 4/25 | Loss: 0.00077350
Iteration 5/25 | Loss: 0.00077350
Iteration 6/25 | Loss: 0.00077350
Iteration 7/25 | Loss: 0.00077350
Iteration 8/25 | Loss: 0.00077350
Iteration 9/25 | Loss: 0.00077350
Iteration 10/25 | Loss: 0.00077350
Iteration 11/25 | Loss: 0.00077350
Iteration 12/25 | Loss: 0.00077350
Iteration 13/25 | Loss: 0.00077350
Iteration 14/25 | Loss: 0.00077350
Iteration 15/25 | Loss: 0.00077350
Iteration 16/25 | Loss: 0.00077350
Iteration 17/25 | Loss: 0.00077350
Iteration 18/25 | Loss: 0.00077350
Iteration 19/25 | Loss: 0.00077350
Iteration 20/25 | Loss: 0.00077350
Iteration 21/25 | Loss: 0.00077350
Iteration 22/25 | Loss: 0.00077350
Iteration 23/25 | Loss: 0.00077350
Iteration 24/25 | Loss: 0.00077350
Iteration 25/25 | Loss: 0.00077350

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077350
Iteration 2/1000 | Loss: 0.00013899
Iteration 3/1000 | Loss: 0.00010274
Iteration 4/1000 | Loss: 0.00008970
Iteration 5/1000 | Loss: 0.00008128
Iteration 6/1000 | Loss: 0.00007688
Iteration 7/1000 | Loss: 0.00086664
Iteration 8/1000 | Loss: 0.01391918
Iteration 9/1000 | Loss: 0.00028023
Iteration 10/1000 | Loss: 0.00021279
Iteration 11/1000 | Loss: 0.00006490
Iteration 12/1000 | Loss: 0.00004254
Iteration 13/1000 | Loss: 0.00003069
Iteration 14/1000 | Loss: 0.00002403
Iteration 15/1000 | Loss: 0.00001989
Iteration 16/1000 | Loss: 0.00001666
Iteration 17/1000 | Loss: 0.00001453
Iteration 18/1000 | Loss: 0.00001289
Iteration 19/1000 | Loss: 0.00001144
Iteration 20/1000 | Loss: 0.00001060
Iteration 21/1000 | Loss: 0.00001001
Iteration 22/1000 | Loss: 0.00000953
Iteration 23/1000 | Loss: 0.00000910
Iteration 24/1000 | Loss: 0.00000887
Iteration 25/1000 | Loss: 0.00000883
Iteration 26/1000 | Loss: 0.00000876
Iteration 27/1000 | Loss: 0.00000868
Iteration 28/1000 | Loss: 0.00000867
Iteration 29/1000 | Loss: 0.00000867
Iteration 30/1000 | Loss: 0.00000867
Iteration 31/1000 | Loss: 0.00000867
Iteration 32/1000 | Loss: 0.00000867
Iteration 33/1000 | Loss: 0.00000866
Iteration 34/1000 | Loss: 0.00000866
Iteration 35/1000 | Loss: 0.00000866
Iteration 36/1000 | Loss: 0.00000865
Iteration 37/1000 | Loss: 0.00000864
Iteration 38/1000 | Loss: 0.00000861
Iteration 39/1000 | Loss: 0.00000861
Iteration 40/1000 | Loss: 0.00000859
Iteration 41/1000 | Loss: 0.00000854
Iteration 42/1000 | Loss: 0.00000854
Iteration 43/1000 | Loss: 0.00000853
Iteration 44/1000 | Loss: 0.00000852
Iteration 45/1000 | Loss: 0.00000850
Iteration 46/1000 | Loss: 0.00000850
Iteration 47/1000 | Loss: 0.00000850
Iteration 48/1000 | Loss: 0.00000849
Iteration 49/1000 | Loss: 0.00000849
Iteration 50/1000 | Loss: 0.00000848
Iteration 51/1000 | Loss: 0.00000848
Iteration 52/1000 | Loss: 0.00000847
Iteration 53/1000 | Loss: 0.00000847
Iteration 54/1000 | Loss: 0.00000847
Iteration 55/1000 | Loss: 0.00000846
Iteration 56/1000 | Loss: 0.00000846
Iteration 57/1000 | Loss: 0.00000846
Iteration 58/1000 | Loss: 0.00000846
Iteration 59/1000 | Loss: 0.00000846
Iteration 60/1000 | Loss: 0.00000845
Iteration 61/1000 | Loss: 0.00000845
Iteration 62/1000 | Loss: 0.00000845
Iteration 63/1000 | Loss: 0.00000845
Iteration 64/1000 | Loss: 0.00000845
Iteration 65/1000 | Loss: 0.00000845
Iteration 66/1000 | Loss: 0.00000845
Iteration 67/1000 | Loss: 0.00000845
Iteration 68/1000 | Loss: 0.00000845
Iteration 69/1000 | Loss: 0.00000844
Iteration 70/1000 | Loss: 0.00000844
Iteration 71/1000 | Loss: 0.00000844
Iteration 72/1000 | Loss: 0.00000844
Iteration 73/1000 | Loss: 0.00000844
Iteration 74/1000 | Loss: 0.00000844
Iteration 75/1000 | Loss: 0.00000843
Iteration 76/1000 | Loss: 0.00000843
Iteration 77/1000 | Loss: 0.00000843
Iteration 78/1000 | Loss: 0.00000842
Iteration 79/1000 | Loss: 0.00000842
Iteration 80/1000 | Loss: 0.00000842
Iteration 81/1000 | Loss: 0.00000842
Iteration 82/1000 | Loss: 0.00000842
Iteration 83/1000 | Loss: 0.00000842
Iteration 84/1000 | Loss: 0.00000842
Iteration 85/1000 | Loss: 0.00000841
Iteration 86/1000 | Loss: 0.00000841
Iteration 87/1000 | Loss: 0.00000841
Iteration 88/1000 | Loss: 0.00000841
Iteration 89/1000 | Loss: 0.00000841
Iteration 90/1000 | Loss: 0.00000841
Iteration 91/1000 | Loss: 0.00000841
Iteration 92/1000 | Loss: 0.00000840
Iteration 93/1000 | Loss: 0.00000840
Iteration 94/1000 | Loss: 0.00000840
Iteration 95/1000 | Loss: 0.00000839
Iteration 96/1000 | Loss: 0.00000839
Iteration 97/1000 | Loss: 0.00000839
Iteration 98/1000 | Loss: 0.00000839
Iteration 99/1000 | Loss: 0.00000839
Iteration 100/1000 | Loss: 0.00000839
Iteration 101/1000 | Loss: 0.00000839
Iteration 102/1000 | Loss: 0.00000839
Iteration 103/1000 | Loss: 0.00000839
Iteration 104/1000 | Loss: 0.00000838
Iteration 105/1000 | Loss: 0.00000838
Iteration 106/1000 | Loss: 0.00000838
Iteration 107/1000 | Loss: 0.00000838
Iteration 108/1000 | Loss: 0.00000838
Iteration 109/1000 | Loss: 0.00000838
Iteration 110/1000 | Loss: 0.00000838
Iteration 111/1000 | Loss: 0.00000838
Iteration 112/1000 | Loss: 0.00000838
Iteration 113/1000 | Loss: 0.00000837
Iteration 114/1000 | Loss: 0.00000837
Iteration 115/1000 | Loss: 0.00000837
Iteration 116/1000 | Loss: 0.00000837
Iteration 117/1000 | Loss: 0.00000837
Iteration 118/1000 | Loss: 0.00000837
Iteration 119/1000 | Loss: 0.00000837
Iteration 120/1000 | Loss: 0.00000837
Iteration 121/1000 | Loss: 0.00000837
Iteration 122/1000 | Loss: 0.00000837
Iteration 123/1000 | Loss: 0.00000837
Iteration 124/1000 | Loss: 0.00000837
Iteration 125/1000 | Loss: 0.00000837
Iteration 126/1000 | Loss: 0.00000837
Iteration 127/1000 | Loss: 0.00000837
Iteration 128/1000 | Loss: 0.00000836
Iteration 129/1000 | Loss: 0.00000836
Iteration 130/1000 | Loss: 0.00000836
Iteration 131/1000 | Loss: 0.00000836
Iteration 132/1000 | Loss: 0.00000836
Iteration 133/1000 | Loss: 0.00000836
Iteration 134/1000 | Loss: 0.00000836
Iteration 135/1000 | Loss: 0.00000836
Iteration 136/1000 | Loss: 0.00000836
Iteration 137/1000 | Loss: 0.00000836
Iteration 138/1000 | Loss: 0.00000836
Iteration 139/1000 | Loss: 0.00000836
Iteration 140/1000 | Loss: 0.00000836
Iteration 141/1000 | Loss: 0.00000836
Iteration 142/1000 | Loss: 0.00000836
Iteration 143/1000 | Loss: 0.00000836
Iteration 144/1000 | Loss: 0.00000836
Iteration 145/1000 | Loss: 0.00000836
Iteration 146/1000 | Loss: 0.00000836
Iteration 147/1000 | Loss: 0.00000836
Iteration 148/1000 | Loss: 0.00000835
Iteration 149/1000 | Loss: 0.00000835
Iteration 150/1000 | Loss: 0.00000835
Iteration 151/1000 | Loss: 0.00000835
Iteration 152/1000 | Loss: 0.00000835
Iteration 153/1000 | Loss: 0.00000835
Iteration 154/1000 | Loss: 0.00000835
Iteration 155/1000 | Loss: 0.00000835
Iteration 156/1000 | Loss: 0.00000835
Iteration 157/1000 | Loss: 0.00000835
Iteration 158/1000 | Loss: 0.00000835
Iteration 159/1000 | Loss: 0.00000835
Iteration 160/1000 | Loss: 0.00000835
Iteration 161/1000 | Loss: 0.00000835
Iteration 162/1000 | Loss: 0.00000835
Iteration 163/1000 | Loss: 0.00000835
Iteration 164/1000 | Loss: 0.00000835
Iteration 165/1000 | Loss: 0.00000835
Iteration 166/1000 | Loss: 0.00000835
Iteration 167/1000 | Loss: 0.00000835
Iteration 168/1000 | Loss: 0.00000835
Iteration 169/1000 | Loss: 0.00000835
Iteration 170/1000 | Loss: 0.00000835
Iteration 171/1000 | Loss: 0.00000835
Iteration 172/1000 | Loss: 0.00000835
Iteration 173/1000 | Loss: 0.00000835
Iteration 174/1000 | Loss: 0.00000835
Iteration 175/1000 | Loss: 0.00000835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [8.349911695404444e-06, 8.349911695404444e-06, 8.349911695404444e-06, 8.349911695404444e-06, 8.349911695404444e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.349911695404444e-06

Optimization complete. Final v2v error: 2.4955146312713623 mm

Highest mean error: 2.590240001678467 mm for frame 120

Lowest mean error: 2.2901272773742676 mm for frame 134

Saving results

Total time: 80.76123642921448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074514
Iteration 2/25 | Loss: 0.00252346
Iteration 3/25 | Loss: 0.00185369
Iteration 4/25 | Loss: 0.00145593
Iteration 5/25 | Loss: 0.00139867
Iteration 6/25 | Loss: 0.00130357
Iteration 7/25 | Loss: 0.00123017
Iteration 8/25 | Loss: 0.00113335
Iteration 9/25 | Loss: 0.00107833
Iteration 10/25 | Loss: 0.00105952
Iteration 11/25 | Loss: 0.00104704
Iteration 12/25 | Loss: 0.00104769
Iteration 13/25 | Loss: 0.00103226
Iteration 14/25 | Loss: 0.00102793
Iteration 15/25 | Loss: 0.00102377
Iteration 16/25 | Loss: 0.00102195
Iteration 17/25 | Loss: 0.00102099
Iteration 18/25 | Loss: 0.00102065
Iteration 19/25 | Loss: 0.00102060
Iteration 20/25 | Loss: 0.00102059
Iteration 21/25 | Loss: 0.00102059
Iteration 22/25 | Loss: 0.00102059
Iteration 23/25 | Loss: 0.00102059
Iteration 24/25 | Loss: 0.00102059
Iteration 25/25 | Loss: 0.00102058

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46450019
Iteration 2/25 | Loss: 0.00140074
Iteration 3/25 | Loss: 0.00108844
Iteration 4/25 | Loss: 0.00108844
Iteration 5/25 | Loss: 0.00108844
Iteration 6/25 | Loss: 0.00108844
Iteration 7/25 | Loss: 0.00108844
Iteration 8/25 | Loss: 0.00108844
Iteration 9/25 | Loss: 0.00108844
Iteration 10/25 | Loss: 0.00108844
Iteration 11/25 | Loss: 0.00108844
Iteration 12/25 | Loss: 0.00108844
Iteration 13/25 | Loss: 0.00108844
Iteration 14/25 | Loss: 0.00108844
Iteration 15/25 | Loss: 0.00108844
Iteration 16/25 | Loss: 0.00108844
Iteration 17/25 | Loss: 0.00108844
Iteration 18/25 | Loss: 0.00108844
Iteration 19/25 | Loss: 0.00108844
Iteration 20/25 | Loss: 0.00108844
Iteration 21/25 | Loss: 0.00108844
Iteration 22/25 | Loss: 0.00108844
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001088437857106328, 0.001088437857106328, 0.001088437857106328, 0.001088437857106328, 0.001088437857106328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001088437857106328

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108844
Iteration 2/1000 | Loss: 0.00073906
Iteration 3/1000 | Loss: 0.00103738
Iteration 4/1000 | Loss: 0.00152422
Iteration 5/1000 | Loss: 0.00266964
Iteration 6/1000 | Loss: 0.00406555
Iteration 7/1000 | Loss: 0.00560595
Iteration 8/1000 | Loss: 0.00404686
Iteration 9/1000 | Loss: 0.00176907
Iteration 10/1000 | Loss: 0.00158918
Iteration 11/1000 | Loss: 0.00029536
Iteration 12/1000 | Loss: 0.00012393
Iteration 13/1000 | Loss: 0.00027959
Iteration 14/1000 | Loss: 0.00037938
Iteration 15/1000 | Loss: 0.00011128
Iteration 16/1000 | Loss: 0.00074001
Iteration 17/1000 | Loss: 0.00007440
Iteration 18/1000 | Loss: 0.00038311
Iteration 19/1000 | Loss: 0.00006739
Iteration 20/1000 | Loss: 0.00125013
Iteration 21/1000 | Loss: 0.00152578
Iteration 22/1000 | Loss: 0.00062565
Iteration 23/1000 | Loss: 0.00082548
Iteration 24/1000 | Loss: 0.00097940
Iteration 25/1000 | Loss: 0.00077280
Iteration 26/1000 | Loss: 0.00007663
Iteration 27/1000 | Loss: 0.00125631
Iteration 28/1000 | Loss: 0.00080904
Iteration 29/1000 | Loss: 0.00113425
Iteration 30/1000 | Loss: 0.00136051
Iteration 31/1000 | Loss: 0.00138733
Iteration 32/1000 | Loss: 0.00105175
Iteration 33/1000 | Loss: 0.00096983
Iteration 34/1000 | Loss: 0.00090387
Iteration 35/1000 | Loss: 0.00079752
Iteration 36/1000 | Loss: 0.00068192
Iteration 37/1000 | Loss: 0.00009243
Iteration 38/1000 | Loss: 0.00009226
Iteration 39/1000 | Loss: 0.00005641
Iteration 40/1000 | Loss: 0.00005314
Iteration 41/1000 | Loss: 0.00035390
Iteration 42/1000 | Loss: 0.00010109
Iteration 43/1000 | Loss: 0.00028388
Iteration 44/1000 | Loss: 0.00009032
Iteration 45/1000 | Loss: 0.00019631
Iteration 46/1000 | Loss: 0.00005467
Iteration 47/1000 | Loss: 0.00005130
Iteration 48/1000 | Loss: 0.00005011
Iteration 49/1000 | Loss: 0.00110200
Iteration 50/1000 | Loss: 0.00114055
Iteration 51/1000 | Loss: 0.00078118
Iteration 52/1000 | Loss: 0.00224310
Iteration 53/1000 | Loss: 0.00015666
Iteration 54/1000 | Loss: 0.00005211
Iteration 55/1000 | Loss: 0.00004662
Iteration 56/1000 | Loss: 0.00168707
Iteration 57/1000 | Loss: 0.00061404
Iteration 58/1000 | Loss: 0.00070670
Iteration 59/1000 | Loss: 0.00004001
Iteration 60/1000 | Loss: 0.00114813
Iteration 61/1000 | Loss: 0.00174628
Iteration 62/1000 | Loss: 0.00084993
Iteration 63/1000 | Loss: 0.00066406
Iteration 64/1000 | Loss: 0.00056347
Iteration 65/1000 | Loss: 0.00011571
Iteration 66/1000 | Loss: 0.00005273
Iteration 67/1000 | Loss: 0.00002670
Iteration 68/1000 | Loss: 0.00002388
Iteration 69/1000 | Loss: 0.00029885
Iteration 70/1000 | Loss: 0.00021815
Iteration 71/1000 | Loss: 0.00002106
Iteration 72/1000 | Loss: 0.00010646
Iteration 73/1000 | Loss: 0.00052465
Iteration 74/1000 | Loss: 0.00002595
Iteration 75/1000 | Loss: 0.00007170
Iteration 76/1000 | Loss: 0.00010752
Iteration 77/1000 | Loss: 0.00001992
Iteration 78/1000 | Loss: 0.00028274
Iteration 79/1000 | Loss: 0.00002152
Iteration 80/1000 | Loss: 0.00009863
Iteration 81/1000 | Loss: 0.00008052
Iteration 82/1000 | Loss: 0.00004007
Iteration 83/1000 | Loss: 0.00007295
Iteration 84/1000 | Loss: 0.00005481
Iteration 85/1000 | Loss: 0.00021183
Iteration 86/1000 | Loss: 0.00001922
Iteration 87/1000 | Loss: 0.00001880
Iteration 88/1000 | Loss: 0.00002212
Iteration 89/1000 | Loss: 0.00002020
Iteration 90/1000 | Loss: 0.00001841
Iteration 91/1000 | Loss: 0.00001832
Iteration 92/1000 | Loss: 0.00001831
Iteration 93/1000 | Loss: 0.00001831
Iteration 94/1000 | Loss: 0.00001831
Iteration 95/1000 | Loss: 0.00001831
Iteration 96/1000 | Loss: 0.00001831
Iteration 97/1000 | Loss: 0.00001831
Iteration 98/1000 | Loss: 0.00001831
Iteration 99/1000 | Loss: 0.00001831
Iteration 100/1000 | Loss: 0.00001828
Iteration 101/1000 | Loss: 0.00001827
Iteration 102/1000 | Loss: 0.00001826
Iteration 103/1000 | Loss: 0.00001826
Iteration 104/1000 | Loss: 0.00002084
Iteration 105/1000 | Loss: 0.00001938
Iteration 106/1000 | Loss: 0.00001819
Iteration 107/1000 | Loss: 0.00001819
Iteration 108/1000 | Loss: 0.00001819
Iteration 109/1000 | Loss: 0.00001818
Iteration 110/1000 | Loss: 0.00001818
Iteration 111/1000 | Loss: 0.00001818
Iteration 112/1000 | Loss: 0.00001818
Iteration 113/1000 | Loss: 0.00001818
Iteration 114/1000 | Loss: 0.00001818
Iteration 115/1000 | Loss: 0.00001818
Iteration 116/1000 | Loss: 0.00001818
Iteration 117/1000 | Loss: 0.00001818
Iteration 118/1000 | Loss: 0.00001818
Iteration 119/1000 | Loss: 0.00001818
Iteration 120/1000 | Loss: 0.00001818
Iteration 121/1000 | Loss: 0.00001817
Iteration 122/1000 | Loss: 0.00001817
Iteration 123/1000 | Loss: 0.00001817
Iteration 124/1000 | Loss: 0.00001817
Iteration 125/1000 | Loss: 0.00001816
Iteration 126/1000 | Loss: 0.00001816
Iteration 127/1000 | Loss: 0.00001815
Iteration 128/1000 | Loss: 0.00001815
Iteration 129/1000 | Loss: 0.00001815
Iteration 130/1000 | Loss: 0.00001815
Iteration 131/1000 | Loss: 0.00001815
Iteration 132/1000 | Loss: 0.00001814
Iteration 133/1000 | Loss: 0.00001814
Iteration 134/1000 | Loss: 0.00001814
Iteration 135/1000 | Loss: 0.00001814
Iteration 136/1000 | Loss: 0.00001814
Iteration 137/1000 | Loss: 0.00001814
Iteration 138/1000 | Loss: 0.00001814
Iteration 139/1000 | Loss: 0.00001814
Iteration 140/1000 | Loss: 0.00001814
Iteration 141/1000 | Loss: 0.00001814
Iteration 142/1000 | Loss: 0.00001814
Iteration 143/1000 | Loss: 0.00001813
Iteration 144/1000 | Loss: 0.00001813
Iteration 145/1000 | Loss: 0.00001811
Iteration 146/1000 | Loss: 0.00001811
Iteration 147/1000 | Loss: 0.00001810
Iteration 148/1000 | Loss: 0.00001810
Iteration 149/1000 | Loss: 0.00001810
Iteration 150/1000 | Loss: 0.00001958
Iteration 151/1000 | Loss: 0.00001870
Iteration 152/1000 | Loss: 0.00001808
Iteration 153/1000 | Loss: 0.00001808
Iteration 154/1000 | Loss: 0.00001808
Iteration 155/1000 | Loss: 0.00001808
Iteration 156/1000 | Loss: 0.00001808
Iteration 157/1000 | Loss: 0.00001808
Iteration 158/1000 | Loss: 0.00001808
Iteration 159/1000 | Loss: 0.00001945
Iteration 160/1000 | Loss: 0.00001859
Iteration 161/1000 | Loss: 0.00001806
Iteration 162/1000 | Loss: 0.00001805
Iteration 163/1000 | Loss: 0.00001805
Iteration 164/1000 | Loss: 0.00001805
Iteration 165/1000 | Loss: 0.00001805
Iteration 166/1000 | Loss: 0.00001805
Iteration 167/1000 | Loss: 0.00001805
Iteration 168/1000 | Loss: 0.00001805
Iteration 169/1000 | Loss: 0.00001805
Iteration 170/1000 | Loss: 0.00001805
Iteration 171/1000 | Loss: 0.00001805
Iteration 172/1000 | Loss: 0.00001805
Iteration 173/1000 | Loss: 0.00001805
Iteration 174/1000 | Loss: 0.00001804
Iteration 175/1000 | Loss: 0.00001804
Iteration 176/1000 | Loss: 0.00001802
Iteration 177/1000 | Loss: 0.00001904
Iteration 178/1000 | Loss: 0.00028622
Iteration 179/1000 | Loss: 0.00002674
Iteration 180/1000 | Loss: 0.00001962
Iteration 181/1000 | Loss: 0.00001828
Iteration 182/1000 | Loss: 0.00001808
Iteration 183/1000 | Loss: 0.00001805
Iteration 184/1000 | Loss: 0.00001802
Iteration 185/1000 | Loss: 0.00001802
Iteration 186/1000 | Loss: 0.00001802
Iteration 187/1000 | Loss: 0.00001802
Iteration 188/1000 | Loss: 0.00001802
Iteration 189/1000 | Loss: 0.00001802
Iteration 190/1000 | Loss: 0.00001802
Iteration 191/1000 | Loss: 0.00001802
Iteration 192/1000 | Loss: 0.00001802
Iteration 193/1000 | Loss: 0.00001801
Iteration 194/1000 | Loss: 0.00001801
Iteration 195/1000 | Loss: 0.00001801
Iteration 196/1000 | Loss: 0.00001801
Iteration 197/1000 | Loss: 0.00001801
Iteration 198/1000 | Loss: 0.00001800
Iteration 199/1000 | Loss: 0.00001800
Iteration 200/1000 | Loss: 0.00001800
Iteration 201/1000 | Loss: 0.00001799
Iteration 202/1000 | Loss: 0.00001799
Iteration 203/1000 | Loss: 0.00001799
Iteration 204/1000 | Loss: 0.00001798
Iteration 205/1000 | Loss: 0.00001798
Iteration 206/1000 | Loss: 0.00001798
Iteration 207/1000 | Loss: 0.00001798
Iteration 208/1000 | Loss: 0.00001798
Iteration 209/1000 | Loss: 0.00001798
Iteration 210/1000 | Loss: 0.00001797
Iteration 211/1000 | Loss: 0.00001797
Iteration 212/1000 | Loss: 0.00001797
Iteration 213/1000 | Loss: 0.00001797
Iteration 214/1000 | Loss: 0.00001797
Iteration 215/1000 | Loss: 0.00001797
Iteration 216/1000 | Loss: 0.00001797
Iteration 217/1000 | Loss: 0.00001797
Iteration 218/1000 | Loss: 0.00001797
Iteration 219/1000 | Loss: 0.00001797
Iteration 220/1000 | Loss: 0.00001797
Iteration 221/1000 | Loss: 0.00001797
Iteration 222/1000 | Loss: 0.00001797
Iteration 223/1000 | Loss: 0.00001797
Iteration 224/1000 | Loss: 0.00001797
Iteration 225/1000 | Loss: 0.00001797
Iteration 226/1000 | Loss: 0.00001797
Iteration 227/1000 | Loss: 0.00001797
Iteration 228/1000 | Loss: 0.00001797
Iteration 229/1000 | Loss: 0.00001797
Iteration 230/1000 | Loss: 0.00001797
Iteration 231/1000 | Loss: 0.00001797
Iteration 232/1000 | Loss: 0.00001797
Iteration 233/1000 | Loss: 0.00001797
Iteration 234/1000 | Loss: 0.00001797
Iteration 235/1000 | Loss: 0.00001797
Iteration 236/1000 | Loss: 0.00001797
Iteration 237/1000 | Loss: 0.00001797
Iteration 238/1000 | Loss: 0.00001797
Iteration 239/1000 | Loss: 0.00001797
Iteration 240/1000 | Loss: 0.00001797
Iteration 241/1000 | Loss: 0.00001797
Iteration 242/1000 | Loss: 0.00001797
Iteration 243/1000 | Loss: 0.00001797
Iteration 244/1000 | Loss: 0.00001797
Iteration 245/1000 | Loss: 0.00001797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.7971062334254384e-05, 1.7971062334254384e-05, 1.7971062334254384e-05, 1.7971062334254384e-05, 1.7971062334254384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7971062334254384e-05

Optimization complete. Final v2v error: 3.397890567779541 mm

Highest mean error: 8.714683532714844 mm for frame 37

Lowest mean error: 2.955550193786621 mm for frame 9

Saving results

Total time: 176.14092206954956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416578
Iteration 2/25 | Loss: 0.00110221
Iteration 3/25 | Loss: 0.00099076
Iteration 4/25 | Loss: 0.00097866
Iteration 5/25 | Loss: 0.00097628
Iteration 6/25 | Loss: 0.00097602
Iteration 7/25 | Loss: 0.00097602
Iteration 8/25 | Loss: 0.00097602
Iteration 9/25 | Loss: 0.00097602
Iteration 10/25 | Loss: 0.00097602
Iteration 11/25 | Loss: 0.00097602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009760200628079474, 0.0009760200628079474, 0.0009760200628079474, 0.0009760200628079474, 0.0009760200628079474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009760200628079474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47309101
Iteration 2/25 | Loss: 0.00040298
Iteration 3/25 | Loss: 0.00040298
Iteration 4/25 | Loss: 0.00040298
Iteration 5/25 | Loss: 0.00040297
Iteration 6/25 | Loss: 0.00040297
Iteration 7/25 | Loss: 0.00040297
Iteration 8/25 | Loss: 0.00040297
Iteration 9/25 | Loss: 0.00040297
Iteration 10/25 | Loss: 0.00040297
Iteration 11/25 | Loss: 0.00040297
Iteration 12/25 | Loss: 0.00040297
Iteration 13/25 | Loss: 0.00040297
Iteration 14/25 | Loss: 0.00040297
Iteration 15/25 | Loss: 0.00040297
Iteration 16/25 | Loss: 0.00040297
Iteration 17/25 | Loss: 0.00040297
Iteration 18/25 | Loss: 0.00040297
Iteration 19/25 | Loss: 0.00040297
Iteration 20/25 | Loss: 0.00040297
Iteration 21/25 | Loss: 0.00040297
Iteration 22/25 | Loss: 0.00040297
Iteration 23/25 | Loss: 0.00040297
Iteration 24/25 | Loss: 0.00040297
Iteration 25/25 | Loss: 0.00040297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040297
Iteration 2/1000 | Loss: 0.00002368
Iteration 3/1000 | Loss: 0.00001537
Iteration 4/1000 | Loss: 0.00001423
Iteration 5/1000 | Loss: 0.00001388
Iteration 6/1000 | Loss: 0.00001359
Iteration 7/1000 | Loss: 0.00001354
Iteration 8/1000 | Loss: 0.00001353
Iteration 9/1000 | Loss: 0.00001353
Iteration 10/1000 | Loss: 0.00001352
Iteration 11/1000 | Loss: 0.00001331
Iteration 12/1000 | Loss: 0.00001321
Iteration 13/1000 | Loss: 0.00001320
Iteration 14/1000 | Loss: 0.00001317
Iteration 15/1000 | Loss: 0.00001317
Iteration 16/1000 | Loss: 0.00001317
Iteration 17/1000 | Loss: 0.00001316
Iteration 18/1000 | Loss: 0.00001316
Iteration 19/1000 | Loss: 0.00001315
Iteration 20/1000 | Loss: 0.00001315
Iteration 21/1000 | Loss: 0.00001315
Iteration 22/1000 | Loss: 0.00001315
Iteration 23/1000 | Loss: 0.00001315
Iteration 24/1000 | Loss: 0.00001314
Iteration 25/1000 | Loss: 0.00001314
Iteration 26/1000 | Loss: 0.00001314
Iteration 27/1000 | Loss: 0.00001313
Iteration 28/1000 | Loss: 0.00001313
Iteration 29/1000 | Loss: 0.00001312
Iteration 30/1000 | Loss: 0.00001308
Iteration 31/1000 | Loss: 0.00001308
Iteration 32/1000 | Loss: 0.00001308
Iteration 33/1000 | Loss: 0.00001308
Iteration 34/1000 | Loss: 0.00001306
Iteration 35/1000 | Loss: 0.00001305
Iteration 36/1000 | Loss: 0.00001305
Iteration 37/1000 | Loss: 0.00001304
Iteration 38/1000 | Loss: 0.00001304
Iteration 39/1000 | Loss: 0.00001304
Iteration 40/1000 | Loss: 0.00001304
Iteration 41/1000 | Loss: 0.00001304
Iteration 42/1000 | Loss: 0.00001303
Iteration 43/1000 | Loss: 0.00001303
Iteration 44/1000 | Loss: 0.00001303
Iteration 45/1000 | Loss: 0.00001303
Iteration 46/1000 | Loss: 0.00001302
Iteration 47/1000 | Loss: 0.00001301
Iteration 48/1000 | Loss: 0.00001301
Iteration 49/1000 | Loss: 0.00001301
Iteration 50/1000 | Loss: 0.00001301
Iteration 51/1000 | Loss: 0.00001300
Iteration 52/1000 | Loss: 0.00001300
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001299
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00001299
Iteration 57/1000 | Loss: 0.00001298
Iteration 58/1000 | Loss: 0.00001298
Iteration 59/1000 | Loss: 0.00001298
Iteration 60/1000 | Loss: 0.00001298
Iteration 61/1000 | Loss: 0.00001298
Iteration 62/1000 | Loss: 0.00001298
Iteration 63/1000 | Loss: 0.00001298
Iteration 64/1000 | Loss: 0.00001298
Iteration 65/1000 | Loss: 0.00001297
Iteration 66/1000 | Loss: 0.00001297
Iteration 67/1000 | Loss: 0.00001297
Iteration 68/1000 | Loss: 0.00001297
Iteration 69/1000 | Loss: 0.00001297
Iteration 70/1000 | Loss: 0.00001296
Iteration 71/1000 | Loss: 0.00001296
Iteration 72/1000 | Loss: 0.00001295
Iteration 73/1000 | Loss: 0.00001295
Iteration 74/1000 | Loss: 0.00001295
Iteration 75/1000 | Loss: 0.00001294
Iteration 76/1000 | Loss: 0.00001294
Iteration 77/1000 | Loss: 0.00001294
Iteration 78/1000 | Loss: 0.00001294
Iteration 79/1000 | Loss: 0.00001293
Iteration 80/1000 | Loss: 0.00001293
Iteration 81/1000 | Loss: 0.00001292
Iteration 82/1000 | Loss: 0.00001292
Iteration 83/1000 | Loss: 0.00001292
Iteration 84/1000 | Loss: 0.00001292
Iteration 85/1000 | Loss: 0.00001291
Iteration 86/1000 | Loss: 0.00001291
Iteration 87/1000 | Loss: 0.00001291
Iteration 88/1000 | Loss: 0.00001291
Iteration 89/1000 | Loss: 0.00001291
Iteration 90/1000 | Loss: 0.00001291
Iteration 91/1000 | Loss: 0.00001291
Iteration 92/1000 | Loss: 0.00001291
Iteration 93/1000 | Loss: 0.00001291
Iteration 94/1000 | Loss: 0.00001291
Iteration 95/1000 | Loss: 0.00001291
Iteration 96/1000 | Loss: 0.00001290
Iteration 97/1000 | Loss: 0.00001290
Iteration 98/1000 | Loss: 0.00001290
Iteration 99/1000 | Loss: 0.00001290
Iteration 100/1000 | Loss: 0.00001290
Iteration 101/1000 | Loss: 0.00001290
Iteration 102/1000 | Loss: 0.00001290
Iteration 103/1000 | Loss: 0.00001290
Iteration 104/1000 | Loss: 0.00001290
Iteration 105/1000 | Loss: 0.00001290
Iteration 106/1000 | Loss: 0.00001290
Iteration 107/1000 | Loss: 0.00001290
Iteration 108/1000 | Loss: 0.00001290
Iteration 109/1000 | Loss: 0.00001290
Iteration 110/1000 | Loss: 0.00001289
Iteration 111/1000 | Loss: 0.00001289
Iteration 112/1000 | Loss: 0.00001289
Iteration 113/1000 | Loss: 0.00001289
Iteration 114/1000 | Loss: 0.00001289
Iteration 115/1000 | Loss: 0.00001289
Iteration 116/1000 | Loss: 0.00001289
Iteration 117/1000 | Loss: 0.00001289
Iteration 118/1000 | Loss: 0.00001289
Iteration 119/1000 | Loss: 0.00001289
Iteration 120/1000 | Loss: 0.00001289
Iteration 121/1000 | Loss: 0.00001289
Iteration 122/1000 | Loss: 0.00001289
Iteration 123/1000 | Loss: 0.00001289
Iteration 124/1000 | Loss: 0.00001289
Iteration 125/1000 | Loss: 0.00001289
Iteration 126/1000 | Loss: 0.00001289
Iteration 127/1000 | Loss: 0.00001289
Iteration 128/1000 | Loss: 0.00001289
Iteration 129/1000 | Loss: 0.00001289
Iteration 130/1000 | Loss: 0.00001289
Iteration 131/1000 | Loss: 0.00001289
Iteration 132/1000 | Loss: 0.00001289
Iteration 133/1000 | Loss: 0.00001289
Iteration 134/1000 | Loss: 0.00001289
Iteration 135/1000 | Loss: 0.00001289
Iteration 136/1000 | Loss: 0.00001289
Iteration 137/1000 | Loss: 0.00001289
Iteration 138/1000 | Loss: 0.00001289
Iteration 139/1000 | Loss: 0.00001289
Iteration 140/1000 | Loss: 0.00001289
Iteration 141/1000 | Loss: 0.00001289
Iteration 142/1000 | Loss: 0.00001289
Iteration 143/1000 | Loss: 0.00001289
Iteration 144/1000 | Loss: 0.00001289
Iteration 145/1000 | Loss: 0.00001289
Iteration 146/1000 | Loss: 0.00001289
Iteration 147/1000 | Loss: 0.00001289
Iteration 148/1000 | Loss: 0.00001289
Iteration 149/1000 | Loss: 0.00001289
Iteration 150/1000 | Loss: 0.00001289
Iteration 151/1000 | Loss: 0.00001289
Iteration 152/1000 | Loss: 0.00001289
Iteration 153/1000 | Loss: 0.00001289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.2888269338873215e-05, 1.2888269338873215e-05, 1.2888269338873215e-05, 1.2888269338873215e-05, 1.2888269338873215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2888269338873215e-05

Optimization complete. Final v2v error: 3.0703485012054443 mm

Highest mean error: 3.263267993927002 mm for frame 62

Lowest mean error: 2.879119396209717 mm for frame 123

Saving results

Total time: 28.758061408996582
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885104
Iteration 2/25 | Loss: 0.00153587
Iteration 3/25 | Loss: 0.00117053
Iteration 4/25 | Loss: 0.00111436
Iteration 5/25 | Loss: 0.00109566
Iteration 6/25 | Loss: 0.00109834
Iteration 7/25 | Loss: 0.00108877
Iteration 8/25 | Loss: 0.00108483
Iteration 9/25 | Loss: 0.00108639
Iteration 10/25 | Loss: 0.00108335
Iteration 11/25 | Loss: 0.00108211
Iteration 12/25 | Loss: 0.00108172
Iteration 13/25 | Loss: 0.00108154
Iteration 14/25 | Loss: 0.00108149
Iteration 15/25 | Loss: 0.00108149
Iteration 16/25 | Loss: 0.00108149
Iteration 17/25 | Loss: 0.00108149
Iteration 18/25 | Loss: 0.00108149
Iteration 19/25 | Loss: 0.00108149
Iteration 20/25 | Loss: 0.00108148
Iteration 21/25 | Loss: 0.00108148
Iteration 22/25 | Loss: 0.00108148
Iteration 23/25 | Loss: 0.00108148
Iteration 24/25 | Loss: 0.00108148
Iteration 25/25 | Loss: 0.00108147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46544921
Iteration 2/25 | Loss: 0.00044858
Iteration 3/25 | Loss: 0.00044857
Iteration 4/25 | Loss: 0.00044857
Iteration 5/25 | Loss: 0.00044857
Iteration 6/25 | Loss: 0.00044857
Iteration 7/25 | Loss: 0.00044857
Iteration 8/25 | Loss: 0.00044857
Iteration 9/25 | Loss: 0.00044857
Iteration 10/25 | Loss: 0.00044857
Iteration 11/25 | Loss: 0.00044857
Iteration 12/25 | Loss: 0.00044857
Iteration 13/25 | Loss: 0.00044857
Iteration 14/25 | Loss: 0.00044857
Iteration 15/25 | Loss: 0.00044857
Iteration 16/25 | Loss: 0.00044857
Iteration 17/25 | Loss: 0.00044857
Iteration 18/25 | Loss: 0.00044857
Iteration 19/25 | Loss: 0.00044857
Iteration 20/25 | Loss: 0.00044857
Iteration 21/25 | Loss: 0.00044857
Iteration 22/25 | Loss: 0.00044857
Iteration 23/25 | Loss: 0.00044857
Iteration 24/25 | Loss: 0.00044857
Iteration 25/25 | Loss: 0.00044857

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044857
Iteration 2/1000 | Loss: 0.00005281
Iteration 3/1000 | Loss: 0.00003301
Iteration 4/1000 | Loss: 0.00002950
Iteration 5/1000 | Loss: 0.00002824
Iteration 6/1000 | Loss: 0.00002754
Iteration 7/1000 | Loss: 0.00002697
Iteration 8/1000 | Loss: 0.00002647
Iteration 9/1000 | Loss: 0.00002610
Iteration 10/1000 | Loss: 0.00002586
Iteration 11/1000 | Loss: 0.00002567
Iteration 12/1000 | Loss: 0.00002551
Iteration 13/1000 | Loss: 0.00002537
Iteration 14/1000 | Loss: 0.00002527
Iteration 15/1000 | Loss: 0.00002525
Iteration 16/1000 | Loss: 0.00002522
Iteration 17/1000 | Loss: 0.00002521
Iteration 18/1000 | Loss: 0.00002519
Iteration 19/1000 | Loss: 0.00002518
Iteration 20/1000 | Loss: 0.00002517
Iteration 21/1000 | Loss: 0.00002511
Iteration 22/1000 | Loss: 0.00002508
Iteration 23/1000 | Loss: 0.00002508
Iteration 24/1000 | Loss: 0.00002507
Iteration 25/1000 | Loss: 0.00002507
Iteration 26/1000 | Loss: 0.00002504
Iteration 27/1000 | Loss: 0.00002503
Iteration 28/1000 | Loss: 0.00002503
Iteration 29/1000 | Loss: 0.00002503
Iteration 30/1000 | Loss: 0.00002503
Iteration 31/1000 | Loss: 0.00002503
Iteration 32/1000 | Loss: 0.00002503
Iteration 33/1000 | Loss: 0.00002502
Iteration 34/1000 | Loss: 0.00002501
Iteration 35/1000 | Loss: 0.00002501
Iteration 36/1000 | Loss: 0.00002500
Iteration 37/1000 | Loss: 0.00002499
Iteration 38/1000 | Loss: 0.00002499
Iteration 39/1000 | Loss: 0.00002496
Iteration 40/1000 | Loss: 0.00002496
Iteration 41/1000 | Loss: 0.00002496
Iteration 42/1000 | Loss: 0.00002496
Iteration 43/1000 | Loss: 0.00002496
Iteration 44/1000 | Loss: 0.00002495
Iteration 45/1000 | Loss: 0.00002495
Iteration 46/1000 | Loss: 0.00002495
Iteration 47/1000 | Loss: 0.00002495
Iteration 48/1000 | Loss: 0.00002493
Iteration 49/1000 | Loss: 0.00002492
Iteration 50/1000 | Loss: 0.00002492
Iteration 51/1000 | Loss: 0.00002492
Iteration 52/1000 | Loss: 0.00002491
Iteration 53/1000 | Loss: 0.00002491
Iteration 54/1000 | Loss: 0.00002489
Iteration 55/1000 | Loss: 0.00002488
Iteration 56/1000 | Loss: 0.00002488
Iteration 57/1000 | Loss: 0.00002487
Iteration 58/1000 | Loss: 0.00002487
Iteration 59/1000 | Loss: 0.00002487
Iteration 60/1000 | Loss: 0.00002487
Iteration 61/1000 | Loss: 0.00002487
Iteration 62/1000 | Loss: 0.00002487
Iteration 63/1000 | Loss: 0.00002487
Iteration 64/1000 | Loss: 0.00002487
Iteration 65/1000 | Loss: 0.00002487
Iteration 66/1000 | Loss: 0.00002486
Iteration 67/1000 | Loss: 0.00002486
Iteration 68/1000 | Loss: 0.00002485
Iteration 69/1000 | Loss: 0.00002484
Iteration 70/1000 | Loss: 0.00002484
Iteration 71/1000 | Loss: 0.00002484
Iteration 72/1000 | Loss: 0.00002484
Iteration 73/1000 | Loss: 0.00002484
Iteration 74/1000 | Loss: 0.00002484
Iteration 75/1000 | Loss: 0.00002484
Iteration 76/1000 | Loss: 0.00002484
Iteration 77/1000 | Loss: 0.00002484
Iteration 78/1000 | Loss: 0.00002484
Iteration 79/1000 | Loss: 0.00002483
Iteration 80/1000 | Loss: 0.00002483
Iteration 81/1000 | Loss: 0.00002482
Iteration 82/1000 | Loss: 0.00002480
Iteration 83/1000 | Loss: 0.00002480
Iteration 84/1000 | Loss: 0.00002480
Iteration 85/1000 | Loss: 0.00002480
Iteration 86/1000 | Loss: 0.00002480
Iteration 87/1000 | Loss: 0.00002480
Iteration 88/1000 | Loss: 0.00002480
Iteration 89/1000 | Loss: 0.00002480
Iteration 90/1000 | Loss: 0.00002480
Iteration 91/1000 | Loss: 0.00002480
Iteration 92/1000 | Loss: 0.00002480
Iteration 93/1000 | Loss: 0.00002479
Iteration 94/1000 | Loss: 0.00002479
Iteration 95/1000 | Loss: 0.00002478
Iteration 96/1000 | Loss: 0.00002478
Iteration 97/1000 | Loss: 0.00002478
Iteration 98/1000 | Loss: 0.00002478
Iteration 99/1000 | Loss: 0.00002477
Iteration 100/1000 | Loss: 0.00002477
Iteration 101/1000 | Loss: 0.00002477
Iteration 102/1000 | Loss: 0.00002477
Iteration 103/1000 | Loss: 0.00002477
Iteration 104/1000 | Loss: 0.00002476
Iteration 105/1000 | Loss: 0.00002476
Iteration 106/1000 | Loss: 0.00002476
Iteration 107/1000 | Loss: 0.00002476
Iteration 108/1000 | Loss: 0.00002476
Iteration 109/1000 | Loss: 0.00002476
Iteration 110/1000 | Loss: 0.00002475
Iteration 111/1000 | Loss: 0.00002474
Iteration 112/1000 | Loss: 0.00002474
Iteration 113/1000 | Loss: 0.00002474
Iteration 114/1000 | Loss: 0.00002474
Iteration 115/1000 | Loss: 0.00002474
Iteration 116/1000 | Loss: 0.00002474
Iteration 117/1000 | Loss: 0.00002473
Iteration 118/1000 | Loss: 0.00002473
Iteration 119/1000 | Loss: 0.00002473
Iteration 120/1000 | Loss: 0.00002473
Iteration 121/1000 | Loss: 0.00002473
Iteration 122/1000 | Loss: 0.00002473
Iteration 123/1000 | Loss: 0.00002473
Iteration 124/1000 | Loss: 0.00002473
Iteration 125/1000 | Loss: 0.00002473
Iteration 126/1000 | Loss: 0.00002472
Iteration 127/1000 | Loss: 0.00002472
Iteration 128/1000 | Loss: 0.00002471
Iteration 129/1000 | Loss: 0.00002471
Iteration 130/1000 | Loss: 0.00002470
Iteration 131/1000 | Loss: 0.00002470
Iteration 132/1000 | Loss: 0.00002470
Iteration 133/1000 | Loss: 0.00002470
Iteration 134/1000 | Loss: 0.00002470
Iteration 135/1000 | Loss: 0.00002470
Iteration 136/1000 | Loss: 0.00002470
Iteration 137/1000 | Loss: 0.00002470
Iteration 138/1000 | Loss: 0.00002469
Iteration 139/1000 | Loss: 0.00002469
Iteration 140/1000 | Loss: 0.00002469
Iteration 141/1000 | Loss: 0.00002468
Iteration 142/1000 | Loss: 0.00002468
Iteration 143/1000 | Loss: 0.00002468
Iteration 144/1000 | Loss: 0.00002468
Iteration 145/1000 | Loss: 0.00002468
Iteration 146/1000 | Loss: 0.00002468
Iteration 147/1000 | Loss: 0.00002468
Iteration 148/1000 | Loss: 0.00002468
Iteration 149/1000 | Loss: 0.00002468
Iteration 150/1000 | Loss: 0.00002468
Iteration 151/1000 | Loss: 0.00002468
Iteration 152/1000 | Loss: 0.00002468
Iteration 153/1000 | Loss: 0.00002468
Iteration 154/1000 | Loss: 0.00002468
Iteration 155/1000 | Loss: 0.00002468
Iteration 156/1000 | Loss: 0.00002468
Iteration 157/1000 | Loss: 0.00002468
Iteration 158/1000 | Loss: 0.00002468
Iteration 159/1000 | Loss: 0.00002468
Iteration 160/1000 | Loss: 0.00002468
Iteration 161/1000 | Loss: 0.00002468
Iteration 162/1000 | Loss: 0.00002468
Iteration 163/1000 | Loss: 0.00002468
Iteration 164/1000 | Loss: 0.00002468
Iteration 165/1000 | Loss: 0.00002468
Iteration 166/1000 | Loss: 0.00002468
Iteration 167/1000 | Loss: 0.00002468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [2.467896047164686e-05, 2.467896047164686e-05, 2.467896047164686e-05, 2.467896047164686e-05, 2.467896047164686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.467896047164686e-05

Optimization complete. Final v2v error: 3.950855255126953 mm

Highest mean error: 5.595365524291992 mm for frame 57

Lowest mean error: 2.8443639278411865 mm for frame 37

Saving results

Total time: 56.952741861343384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980736
Iteration 2/25 | Loss: 0.00151185
Iteration 3/25 | Loss: 0.00125010
Iteration 4/25 | Loss: 0.00121418
Iteration 5/25 | Loss: 0.00122814
Iteration 6/25 | Loss: 0.00121635
Iteration 7/25 | Loss: 0.00119211
Iteration 8/25 | Loss: 0.00118575
Iteration 9/25 | Loss: 0.00118197
Iteration 10/25 | Loss: 0.00118599
Iteration 11/25 | Loss: 0.00118571
Iteration 12/25 | Loss: 0.00118356
Iteration 13/25 | Loss: 0.00118033
Iteration 14/25 | Loss: 0.00117998
Iteration 15/25 | Loss: 0.00117991
Iteration 16/25 | Loss: 0.00117991
Iteration 17/25 | Loss: 0.00117991
Iteration 18/25 | Loss: 0.00117991
Iteration 19/25 | Loss: 0.00117991
Iteration 20/25 | Loss: 0.00117991
Iteration 21/25 | Loss: 0.00117991
Iteration 22/25 | Loss: 0.00117991
Iteration 23/25 | Loss: 0.00117991
Iteration 24/25 | Loss: 0.00117991
Iteration 25/25 | Loss: 0.00117991

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38682270
Iteration 2/25 | Loss: 0.00068082
Iteration 3/25 | Loss: 0.00068077
Iteration 4/25 | Loss: 0.00068077
Iteration 5/25 | Loss: 0.00068077
Iteration 6/25 | Loss: 0.00068076
Iteration 7/25 | Loss: 0.00068076
Iteration 8/25 | Loss: 0.00068076
Iteration 9/25 | Loss: 0.00068076
Iteration 10/25 | Loss: 0.00068076
Iteration 11/25 | Loss: 0.00068076
Iteration 12/25 | Loss: 0.00068076
Iteration 13/25 | Loss: 0.00068076
Iteration 14/25 | Loss: 0.00068076
Iteration 15/25 | Loss: 0.00068076
Iteration 16/25 | Loss: 0.00068076
Iteration 17/25 | Loss: 0.00068076
Iteration 18/25 | Loss: 0.00068076
Iteration 19/25 | Loss: 0.00068076
Iteration 20/25 | Loss: 0.00068076
Iteration 21/25 | Loss: 0.00068076
Iteration 22/25 | Loss: 0.00068076
Iteration 23/25 | Loss: 0.00068076
Iteration 24/25 | Loss: 0.00068076
Iteration 25/25 | Loss: 0.00068076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068076
Iteration 2/1000 | Loss: 0.00007123
Iteration 3/1000 | Loss: 0.00004711
Iteration 4/1000 | Loss: 0.00003717
Iteration 5/1000 | Loss: 0.00003443
Iteration 6/1000 | Loss: 0.00004674
Iteration 7/1000 | Loss: 0.00003525
Iteration 8/1000 | Loss: 0.00003340
Iteration 9/1000 | Loss: 0.00003115
Iteration 10/1000 | Loss: 0.00003024
Iteration 11/1000 | Loss: 0.00002993
Iteration 12/1000 | Loss: 0.00002965
Iteration 13/1000 | Loss: 0.00002933
Iteration 14/1000 | Loss: 0.00002895
Iteration 15/1000 | Loss: 0.00002888
Iteration 16/1000 | Loss: 0.00002865
Iteration 17/1000 | Loss: 0.00002864
Iteration 18/1000 | Loss: 0.00002854
Iteration 19/1000 | Loss: 0.00002853
Iteration 20/1000 | Loss: 0.00002842
Iteration 21/1000 | Loss: 0.00002838
Iteration 22/1000 | Loss: 0.00002837
Iteration 23/1000 | Loss: 0.00002837
Iteration 24/1000 | Loss: 0.00002837
Iteration 25/1000 | Loss: 0.00002836
Iteration 26/1000 | Loss: 0.00002836
Iteration 27/1000 | Loss: 0.00002836
Iteration 28/1000 | Loss: 0.00002835
Iteration 29/1000 | Loss: 0.00002835
Iteration 30/1000 | Loss: 0.00002835
Iteration 31/1000 | Loss: 0.00002834
Iteration 32/1000 | Loss: 0.00002834
Iteration 33/1000 | Loss: 0.00002833
Iteration 34/1000 | Loss: 0.00002833
Iteration 35/1000 | Loss: 0.00002833
Iteration 36/1000 | Loss: 0.00002831
Iteration 37/1000 | Loss: 0.00002826
Iteration 38/1000 | Loss: 0.00002820
Iteration 39/1000 | Loss: 0.00002820
Iteration 40/1000 | Loss: 0.00002819
Iteration 41/1000 | Loss: 0.00002818
Iteration 42/1000 | Loss: 0.00002818
Iteration 43/1000 | Loss: 0.00002818
Iteration 44/1000 | Loss: 0.00002818
Iteration 45/1000 | Loss: 0.00002817
Iteration 46/1000 | Loss: 0.00002817
Iteration 47/1000 | Loss: 0.00002817
Iteration 48/1000 | Loss: 0.00002817
Iteration 49/1000 | Loss: 0.00002817
Iteration 50/1000 | Loss: 0.00002817
Iteration 51/1000 | Loss: 0.00002817
Iteration 52/1000 | Loss: 0.00002817
Iteration 53/1000 | Loss: 0.00002816
Iteration 54/1000 | Loss: 0.00002816
Iteration 55/1000 | Loss: 0.00002816
Iteration 56/1000 | Loss: 0.00002816
Iteration 57/1000 | Loss: 0.00002815
Iteration 58/1000 | Loss: 0.00002815
Iteration 59/1000 | Loss: 0.00002815
Iteration 60/1000 | Loss: 0.00002814
Iteration 61/1000 | Loss: 0.00002814
Iteration 62/1000 | Loss: 0.00002814
Iteration 63/1000 | Loss: 0.00002813
Iteration 64/1000 | Loss: 0.00002813
Iteration 65/1000 | Loss: 0.00002813
Iteration 66/1000 | Loss: 0.00002813
Iteration 67/1000 | Loss: 0.00002813
Iteration 68/1000 | Loss: 0.00002813
Iteration 69/1000 | Loss: 0.00002812
Iteration 70/1000 | Loss: 0.00002812
Iteration 71/1000 | Loss: 0.00002812
Iteration 72/1000 | Loss: 0.00002812
Iteration 73/1000 | Loss: 0.00002811
Iteration 74/1000 | Loss: 0.00002811
Iteration 75/1000 | Loss: 0.00002811
Iteration 76/1000 | Loss: 0.00002811
Iteration 77/1000 | Loss: 0.00002811
Iteration 78/1000 | Loss: 0.00002811
Iteration 79/1000 | Loss: 0.00002810
Iteration 80/1000 | Loss: 0.00002810
Iteration 81/1000 | Loss: 0.00002810
Iteration 82/1000 | Loss: 0.00002810
Iteration 83/1000 | Loss: 0.00002810
Iteration 84/1000 | Loss: 0.00002810
Iteration 85/1000 | Loss: 0.00002810
Iteration 86/1000 | Loss: 0.00002810
Iteration 87/1000 | Loss: 0.00002810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [2.80984186247224e-05, 2.80984186247224e-05, 2.80984186247224e-05, 2.80984186247224e-05, 2.80984186247224e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.80984186247224e-05

Optimization complete. Final v2v error: 4.483642578125 mm

Highest mean error: 5.32550573348999 mm for frame 105

Lowest mean error: 3.8899590969085693 mm for frame 79

Saving results

Total time: 63.43323636054993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897265
Iteration 2/25 | Loss: 0.00124457
Iteration 3/25 | Loss: 0.00109712
Iteration 4/25 | Loss: 0.00107655
Iteration 5/25 | Loss: 0.00106894
Iteration 6/25 | Loss: 0.00106791
Iteration 7/25 | Loss: 0.00106791
Iteration 8/25 | Loss: 0.00106791
Iteration 9/25 | Loss: 0.00106791
Iteration 10/25 | Loss: 0.00106791
Iteration 11/25 | Loss: 0.00106791
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010679055703803897, 0.0010679055703803897, 0.0010679055703803897, 0.0010679055703803897, 0.0010679055703803897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010679055703803897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39623511
Iteration 2/25 | Loss: 0.00044539
Iteration 3/25 | Loss: 0.00044538
Iteration 4/25 | Loss: 0.00044538
Iteration 5/25 | Loss: 0.00044538
Iteration 6/25 | Loss: 0.00044538
Iteration 7/25 | Loss: 0.00044538
Iteration 8/25 | Loss: 0.00044538
Iteration 9/25 | Loss: 0.00044538
Iteration 10/25 | Loss: 0.00044538
Iteration 11/25 | Loss: 0.00044538
Iteration 12/25 | Loss: 0.00044538
Iteration 13/25 | Loss: 0.00044538
Iteration 14/25 | Loss: 0.00044538
Iteration 15/25 | Loss: 0.00044538
Iteration 16/25 | Loss: 0.00044538
Iteration 17/25 | Loss: 0.00044538
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004453803412616253, 0.0004453803412616253, 0.0004453803412616253, 0.0004453803412616253, 0.0004453803412616253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004453803412616253

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044538
Iteration 2/1000 | Loss: 0.00003832
Iteration 3/1000 | Loss: 0.00002256
Iteration 4/1000 | Loss: 0.00002053
Iteration 5/1000 | Loss: 0.00001952
Iteration 6/1000 | Loss: 0.00001862
Iteration 7/1000 | Loss: 0.00001824
Iteration 8/1000 | Loss: 0.00001794
Iteration 9/1000 | Loss: 0.00001778
Iteration 10/1000 | Loss: 0.00001767
Iteration 11/1000 | Loss: 0.00001753
Iteration 12/1000 | Loss: 0.00001751
Iteration 13/1000 | Loss: 0.00001746
Iteration 14/1000 | Loss: 0.00001746
Iteration 15/1000 | Loss: 0.00001745
Iteration 16/1000 | Loss: 0.00001733
Iteration 17/1000 | Loss: 0.00001732
Iteration 18/1000 | Loss: 0.00001730
Iteration 19/1000 | Loss: 0.00001729
Iteration 20/1000 | Loss: 0.00001727
Iteration 21/1000 | Loss: 0.00001727
Iteration 22/1000 | Loss: 0.00001723
Iteration 23/1000 | Loss: 0.00001722
Iteration 24/1000 | Loss: 0.00001719
Iteration 25/1000 | Loss: 0.00001719
Iteration 26/1000 | Loss: 0.00001717
Iteration 27/1000 | Loss: 0.00001717
Iteration 28/1000 | Loss: 0.00001716
Iteration 29/1000 | Loss: 0.00001716
Iteration 30/1000 | Loss: 0.00001715
Iteration 31/1000 | Loss: 0.00001715
Iteration 32/1000 | Loss: 0.00001714
Iteration 33/1000 | Loss: 0.00001714
Iteration 34/1000 | Loss: 0.00001713
Iteration 35/1000 | Loss: 0.00001712
Iteration 36/1000 | Loss: 0.00001711
Iteration 37/1000 | Loss: 0.00001710
Iteration 38/1000 | Loss: 0.00001709
Iteration 39/1000 | Loss: 0.00001709
Iteration 40/1000 | Loss: 0.00001709
Iteration 41/1000 | Loss: 0.00001708
Iteration 42/1000 | Loss: 0.00001708
Iteration 43/1000 | Loss: 0.00001708
Iteration 44/1000 | Loss: 0.00001707
Iteration 45/1000 | Loss: 0.00001707
Iteration 46/1000 | Loss: 0.00001707
Iteration 47/1000 | Loss: 0.00001707
Iteration 48/1000 | Loss: 0.00001707
Iteration 49/1000 | Loss: 0.00001707
Iteration 50/1000 | Loss: 0.00001707
Iteration 51/1000 | Loss: 0.00001706
Iteration 52/1000 | Loss: 0.00001706
Iteration 53/1000 | Loss: 0.00001706
Iteration 54/1000 | Loss: 0.00001706
Iteration 55/1000 | Loss: 0.00001706
Iteration 56/1000 | Loss: 0.00001705
Iteration 57/1000 | Loss: 0.00001705
Iteration 58/1000 | Loss: 0.00001705
Iteration 59/1000 | Loss: 0.00001705
Iteration 60/1000 | Loss: 0.00001704
Iteration 61/1000 | Loss: 0.00001704
Iteration 62/1000 | Loss: 0.00001704
Iteration 63/1000 | Loss: 0.00001704
Iteration 64/1000 | Loss: 0.00001704
Iteration 65/1000 | Loss: 0.00001704
Iteration 66/1000 | Loss: 0.00001704
Iteration 67/1000 | Loss: 0.00001704
Iteration 68/1000 | Loss: 0.00001704
Iteration 69/1000 | Loss: 0.00001704
Iteration 70/1000 | Loss: 0.00001703
Iteration 71/1000 | Loss: 0.00001703
Iteration 72/1000 | Loss: 0.00001703
Iteration 73/1000 | Loss: 0.00001703
Iteration 74/1000 | Loss: 0.00001703
Iteration 75/1000 | Loss: 0.00001703
Iteration 76/1000 | Loss: 0.00001702
Iteration 77/1000 | Loss: 0.00001702
Iteration 78/1000 | Loss: 0.00001702
Iteration 79/1000 | Loss: 0.00001702
Iteration 80/1000 | Loss: 0.00001702
Iteration 81/1000 | Loss: 0.00001702
Iteration 82/1000 | Loss: 0.00001702
Iteration 83/1000 | Loss: 0.00001701
Iteration 84/1000 | Loss: 0.00001701
Iteration 85/1000 | Loss: 0.00001701
Iteration 86/1000 | Loss: 0.00001701
Iteration 87/1000 | Loss: 0.00001701
Iteration 88/1000 | Loss: 0.00001700
Iteration 89/1000 | Loss: 0.00001700
Iteration 90/1000 | Loss: 0.00001700
Iteration 91/1000 | Loss: 0.00001699
Iteration 92/1000 | Loss: 0.00001699
Iteration 93/1000 | Loss: 0.00001699
Iteration 94/1000 | Loss: 0.00001699
Iteration 95/1000 | Loss: 0.00001699
Iteration 96/1000 | Loss: 0.00001699
Iteration 97/1000 | Loss: 0.00001698
Iteration 98/1000 | Loss: 0.00001698
Iteration 99/1000 | Loss: 0.00001698
Iteration 100/1000 | Loss: 0.00001698
Iteration 101/1000 | Loss: 0.00001698
Iteration 102/1000 | Loss: 0.00001698
Iteration 103/1000 | Loss: 0.00001698
Iteration 104/1000 | Loss: 0.00001698
Iteration 105/1000 | Loss: 0.00001698
Iteration 106/1000 | Loss: 0.00001698
Iteration 107/1000 | Loss: 0.00001698
Iteration 108/1000 | Loss: 0.00001698
Iteration 109/1000 | Loss: 0.00001698
Iteration 110/1000 | Loss: 0.00001698
Iteration 111/1000 | Loss: 0.00001698
Iteration 112/1000 | Loss: 0.00001697
Iteration 113/1000 | Loss: 0.00001697
Iteration 114/1000 | Loss: 0.00001697
Iteration 115/1000 | Loss: 0.00001697
Iteration 116/1000 | Loss: 0.00001697
Iteration 117/1000 | Loss: 0.00001697
Iteration 118/1000 | Loss: 0.00001697
Iteration 119/1000 | Loss: 0.00001697
Iteration 120/1000 | Loss: 0.00001696
Iteration 121/1000 | Loss: 0.00001696
Iteration 122/1000 | Loss: 0.00001696
Iteration 123/1000 | Loss: 0.00001696
Iteration 124/1000 | Loss: 0.00001696
Iteration 125/1000 | Loss: 0.00001696
Iteration 126/1000 | Loss: 0.00001696
Iteration 127/1000 | Loss: 0.00001696
Iteration 128/1000 | Loss: 0.00001696
Iteration 129/1000 | Loss: 0.00001696
Iteration 130/1000 | Loss: 0.00001696
Iteration 131/1000 | Loss: 0.00001696
Iteration 132/1000 | Loss: 0.00001696
Iteration 133/1000 | Loss: 0.00001696
Iteration 134/1000 | Loss: 0.00001696
Iteration 135/1000 | Loss: 0.00001696
Iteration 136/1000 | Loss: 0.00001696
Iteration 137/1000 | Loss: 0.00001696
Iteration 138/1000 | Loss: 0.00001696
Iteration 139/1000 | Loss: 0.00001696
Iteration 140/1000 | Loss: 0.00001696
Iteration 141/1000 | Loss: 0.00001696
Iteration 142/1000 | Loss: 0.00001696
Iteration 143/1000 | Loss: 0.00001696
Iteration 144/1000 | Loss: 0.00001696
Iteration 145/1000 | Loss: 0.00001696
Iteration 146/1000 | Loss: 0.00001696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.6958174455794506e-05, 1.6958174455794506e-05, 1.6958174455794506e-05, 1.6958174455794506e-05, 1.6958174455794506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6958174455794506e-05

Optimization complete. Final v2v error: 3.384324073791504 mm

Highest mean error: 4.07609748840332 mm for frame 174

Lowest mean error: 2.8126068115234375 mm for frame 104

Saving results

Total time: 39.02801847457886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480851
Iteration 2/25 | Loss: 0.00109839
Iteration 3/25 | Loss: 0.00098519
Iteration 4/25 | Loss: 0.00097927
Iteration 5/25 | Loss: 0.00097720
Iteration 6/25 | Loss: 0.00097655
Iteration 7/25 | Loss: 0.00097655
Iteration 8/25 | Loss: 0.00097655
Iteration 9/25 | Loss: 0.00097655
Iteration 10/25 | Loss: 0.00097655
Iteration 11/25 | Loss: 0.00097655
Iteration 12/25 | Loss: 0.00097655
Iteration 13/25 | Loss: 0.00097655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009765536524355412, 0.0009765536524355412, 0.0009765536524355412, 0.0009765536524355412, 0.0009765536524355412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009765536524355412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46980667
Iteration 2/25 | Loss: 0.00036655
Iteration 3/25 | Loss: 0.00036653
Iteration 4/25 | Loss: 0.00036653
Iteration 5/25 | Loss: 0.00036653
Iteration 6/25 | Loss: 0.00036653
Iteration 7/25 | Loss: 0.00036653
Iteration 8/25 | Loss: 0.00036653
Iteration 9/25 | Loss: 0.00036653
Iteration 10/25 | Loss: 0.00036653
Iteration 11/25 | Loss: 0.00036653
Iteration 12/25 | Loss: 0.00036653
Iteration 13/25 | Loss: 0.00036653
Iteration 14/25 | Loss: 0.00036653
Iteration 15/25 | Loss: 0.00036653
Iteration 16/25 | Loss: 0.00036653
Iteration 17/25 | Loss: 0.00036653
Iteration 18/25 | Loss: 0.00036653
Iteration 19/25 | Loss: 0.00036653
Iteration 20/25 | Loss: 0.00036653
Iteration 21/25 | Loss: 0.00036653
Iteration 22/25 | Loss: 0.00036653
Iteration 23/25 | Loss: 0.00036653
Iteration 24/25 | Loss: 0.00036653
Iteration 25/25 | Loss: 0.00036653

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036653
Iteration 2/1000 | Loss: 0.00003277
Iteration 3/1000 | Loss: 0.00001597
Iteration 4/1000 | Loss: 0.00001397
Iteration 5/1000 | Loss: 0.00001340
Iteration 6/1000 | Loss: 0.00001298
Iteration 7/1000 | Loss: 0.00001279
Iteration 8/1000 | Loss: 0.00001276
Iteration 9/1000 | Loss: 0.00001276
Iteration 10/1000 | Loss: 0.00001267
Iteration 11/1000 | Loss: 0.00001267
Iteration 12/1000 | Loss: 0.00001259
Iteration 13/1000 | Loss: 0.00001258
Iteration 14/1000 | Loss: 0.00001257
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001256
Iteration 17/1000 | Loss: 0.00001252
Iteration 18/1000 | Loss: 0.00001249
Iteration 19/1000 | Loss: 0.00001249
Iteration 20/1000 | Loss: 0.00001248
Iteration 21/1000 | Loss: 0.00001247
Iteration 22/1000 | Loss: 0.00001245
Iteration 23/1000 | Loss: 0.00001244
Iteration 24/1000 | Loss: 0.00001244
Iteration 25/1000 | Loss: 0.00001243
Iteration 26/1000 | Loss: 0.00001242
Iteration 27/1000 | Loss: 0.00001240
Iteration 28/1000 | Loss: 0.00001239
Iteration 29/1000 | Loss: 0.00001239
Iteration 30/1000 | Loss: 0.00001239
Iteration 31/1000 | Loss: 0.00001238
Iteration 32/1000 | Loss: 0.00001238
Iteration 33/1000 | Loss: 0.00001236
Iteration 34/1000 | Loss: 0.00001236
Iteration 35/1000 | Loss: 0.00001235
Iteration 36/1000 | Loss: 0.00001235
Iteration 37/1000 | Loss: 0.00001234
Iteration 38/1000 | Loss: 0.00001231
Iteration 39/1000 | Loss: 0.00001230
Iteration 40/1000 | Loss: 0.00001230
Iteration 41/1000 | Loss: 0.00001229
Iteration 42/1000 | Loss: 0.00001229
Iteration 43/1000 | Loss: 0.00001228
Iteration 44/1000 | Loss: 0.00001228
Iteration 45/1000 | Loss: 0.00001228
Iteration 46/1000 | Loss: 0.00001227
Iteration 47/1000 | Loss: 0.00001227
Iteration 48/1000 | Loss: 0.00001227
Iteration 49/1000 | Loss: 0.00001227
Iteration 50/1000 | Loss: 0.00001227
Iteration 51/1000 | Loss: 0.00001226
Iteration 52/1000 | Loss: 0.00001226
Iteration 53/1000 | Loss: 0.00001226
Iteration 54/1000 | Loss: 0.00001226
Iteration 55/1000 | Loss: 0.00001226
Iteration 56/1000 | Loss: 0.00001226
Iteration 57/1000 | Loss: 0.00001226
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001225
Iteration 60/1000 | Loss: 0.00001225
Iteration 61/1000 | Loss: 0.00001225
Iteration 62/1000 | Loss: 0.00001224
Iteration 63/1000 | Loss: 0.00001224
Iteration 64/1000 | Loss: 0.00001224
Iteration 65/1000 | Loss: 0.00001224
Iteration 66/1000 | Loss: 0.00001224
Iteration 67/1000 | Loss: 0.00001224
Iteration 68/1000 | Loss: 0.00001223
Iteration 69/1000 | Loss: 0.00001223
Iteration 70/1000 | Loss: 0.00001223
Iteration 71/1000 | Loss: 0.00001223
Iteration 72/1000 | Loss: 0.00001223
Iteration 73/1000 | Loss: 0.00001223
Iteration 74/1000 | Loss: 0.00001223
Iteration 75/1000 | Loss: 0.00001223
Iteration 76/1000 | Loss: 0.00001223
Iteration 77/1000 | Loss: 0.00001223
Iteration 78/1000 | Loss: 0.00001223
Iteration 79/1000 | Loss: 0.00001223
Iteration 80/1000 | Loss: 0.00001223
Iteration 81/1000 | Loss: 0.00001222
Iteration 82/1000 | Loss: 0.00001222
Iteration 83/1000 | Loss: 0.00001222
Iteration 84/1000 | Loss: 0.00001221
Iteration 85/1000 | Loss: 0.00001221
Iteration 86/1000 | Loss: 0.00001220
Iteration 87/1000 | Loss: 0.00001220
Iteration 88/1000 | Loss: 0.00001220
Iteration 89/1000 | Loss: 0.00001220
Iteration 90/1000 | Loss: 0.00001220
Iteration 91/1000 | Loss: 0.00001220
Iteration 92/1000 | Loss: 0.00001219
Iteration 93/1000 | Loss: 0.00001219
Iteration 94/1000 | Loss: 0.00001219
Iteration 95/1000 | Loss: 0.00001219
Iteration 96/1000 | Loss: 0.00001219
Iteration 97/1000 | Loss: 0.00001219
Iteration 98/1000 | Loss: 0.00001218
Iteration 99/1000 | Loss: 0.00001218
Iteration 100/1000 | Loss: 0.00001218
Iteration 101/1000 | Loss: 0.00001218
Iteration 102/1000 | Loss: 0.00001217
Iteration 103/1000 | Loss: 0.00001217
Iteration 104/1000 | Loss: 0.00001217
Iteration 105/1000 | Loss: 0.00001217
Iteration 106/1000 | Loss: 0.00001217
Iteration 107/1000 | Loss: 0.00001217
Iteration 108/1000 | Loss: 0.00001217
Iteration 109/1000 | Loss: 0.00001216
Iteration 110/1000 | Loss: 0.00001216
Iteration 111/1000 | Loss: 0.00001216
Iteration 112/1000 | Loss: 0.00001216
Iteration 113/1000 | Loss: 0.00001216
Iteration 114/1000 | Loss: 0.00001215
Iteration 115/1000 | Loss: 0.00001215
Iteration 116/1000 | Loss: 0.00001215
Iteration 117/1000 | Loss: 0.00001215
Iteration 118/1000 | Loss: 0.00001214
Iteration 119/1000 | Loss: 0.00001214
Iteration 120/1000 | Loss: 0.00001214
Iteration 121/1000 | Loss: 0.00001214
Iteration 122/1000 | Loss: 0.00001214
Iteration 123/1000 | Loss: 0.00001214
Iteration 124/1000 | Loss: 0.00001214
Iteration 125/1000 | Loss: 0.00001214
Iteration 126/1000 | Loss: 0.00001214
Iteration 127/1000 | Loss: 0.00001213
Iteration 128/1000 | Loss: 0.00001213
Iteration 129/1000 | Loss: 0.00001213
Iteration 130/1000 | Loss: 0.00001213
Iteration 131/1000 | Loss: 0.00001213
Iteration 132/1000 | Loss: 0.00001213
Iteration 133/1000 | Loss: 0.00001213
Iteration 134/1000 | Loss: 0.00001213
Iteration 135/1000 | Loss: 0.00001213
Iteration 136/1000 | Loss: 0.00001212
Iteration 137/1000 | Loss: 0.00001212
Iteration 138/1000 | Loss: 0.00001212
Iteration 139/1000 | Loss: 0.00001212
Iteration 140/1000 | Loss: 0.00001212
Iteration 141/1000 | Loss: 0.00001212
Iteration 142/1000 | Loss: 0.00001211
Iteration 143/1000 | Loss: 0.00001211
Iteration 144/1000 | Loss: 0.00001211
Iteration 145/1000 | Loss: 0.00001211
Iteration 146/1000 | Loss: 0.00001211
Iteration 147/1000 | Loss: 0.00001211
Iteration 148/1000 | Loss: 0.00001211
Iteration 149/1000 | Loss: 0.00001211
Iteration 150/1000 | Loss: 0.00001211
Iteration 151/1000 | Loss: 0.00001211
Iteration 152/1000 | Loss: 0.00001211
Iteration 153/1000 | Loss: 0.00001211
Iteration 154/1000 | Loss: 0.00001211
Iteration 155/1000 | Loss: 0.00001211
Iteration 156/1000 | Loss: 0.00001211
Iteration 157/1000 | Loss: 0.00001211
Iteration 158/1000 | Loss: 0.00001211
Iteration 159/1000 | Loss: 0.00001211
Iteration 160/1000 | Loss: 0.00001211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.2108647752029356e-05, 1.2108647752029356e-05, 1.2108647752029356e-05, 1.2108647752029356e-05, 1.2108647752029356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2108647752029356e-05

Optimization complete. Final v2v error: 2.894434928894043 mm

Highest mean error: 3.418830633163452 mm for frame 60

Lowest mean error: 2.5865893363952637 mm for frame 16

Saving results

Total time: 32.28408598899841
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941654
Iteration 2/25 | Loss: 0.00155018
Iteration 3/25 | Loss: 0.00107326
Iteration 4/25 | Loss: 0.00100208
Iteration 5/25 | Loss: 0.00098222
Iteration 6/25 | Loss: 0.00097697
Iteration 7/25 | Loss: 0.00097348
Iteration 8/25 | Loss: 0.00097224
Iteration 9/25 | Loss: 0.00097019
Iteration 10/25 | Loss: 0.00096816
Iteration 11/25 | Loss: 0.00096736
Iteration 12/25 | Loss: 0.00096691
Iteration 13/25 | Loss: 0.00096673
Iteration 14/25 | Loss: 0.00096662
Iteration 15/25 | Loss: 0.00096661
Iteration 16/25 | Loss: 0.00096661
Iteration 17/25 | Loss: 0.00096661
Iteration 18/25 | Loss: 0.00096661
Iteration 19/25 | Loss: 0.00096661
Iteration 20/25 | Loss: 0.00096661
Iteration 21/25 | Loss: 0.00096660
Iteration 22/25 | Loss: 0.00096660
Iteration 23/25 | Loss: 0.00096660
Iteration 24/25 | Loss: 0.00096660
Iteration 25/25 | Loss: 0.00096660

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14182615
Iteration 2/25 | Loss: 0.00041671
Iteration 3/25 | Loss: 0.00041671
Iteration 4/25 | Loss: 0.00041671
Iteration 5/25 | Loss: 0.00041671
Iteration 6/25 | Loss: 0.00041671
Iteration 7/25 | Loss: 0.00041671
Iteration 8/25 | Loss: 0.00041671
Iteration 9/25 | Loss: 0.00041671
Iteration 10/25 | Loss: 0.00041671
Iteration 11/25 | Loss: 0.00041671
Iteration 12/25 | Loss: 0.00041671
Iteration 13/25 | Loss: 0.00041671
Iteration 14/25 | Loss: 0.00041671
Iteration 15/25 | Loss: 0.00041671
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0004167066654190421, 0.0004167066654190421, 0.0004167066654190421, 0.0004167066654190421, 0.0004167066654190421]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004167066654190421

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041671
Iteration 2/1000 | Loss: 0.00002726
Iteration 3/1000 | Loss: 0.00006688
Iteration 4/1000 | Loss: 0.00002163
Iteration 5/1000 | Loss: 0.00001679
Iteration 6/1000 | Loss: 0.00001569
Iteration 7/1000 | Loss: 0.00008729
Iteration 8/1000 | Loss: 0.00001522
Iteration 9/1000 | Loss: 0.00005408
Iteration 10/1000 | Loss: 0.00001499
Iteration 11/1000 | Loss: 0.00001492
Iteration 12/1000 | Loss: 0.00001477
Iteration 13/1000 | Loss: 0.00001472
Iteration 14/1000 | Loss: 0.00001456
Iteration 15/1000 | Loss: 0.00001454
Iteration 16/1000 | Loss: 0.00001454
Iteration 17/1000 | Loss: 0.00001453
Iteration 18/1000 | Loss: 0.00001452
Iteration 19/1000 | Loss: 0.00001452
Iteration 20/1000 | Loss: 0.00001451
Iteration 21/1000 | Loss: 0.00001449
Iteration 22/1000 | Loss: 0.00001449
Iteration 23/1000 | Loss: 0.00001449
Iteration 24/1000 | Loss: 0.00001449
Iteration 25/1000 | Loss: 0.00001448
Iteration 26/1000 | Loss: 0.00001448
Iteration 27/1000 | Loss: 0.00001448
Iteration 28/1000 | Loss: 0.00001448
Iteration 29/1000 | Loss: 0.00001448
Iteration 30/1000 | Loss: 0.00001446
Iteration 31/1000 | Loss: 0.00001446
Iteration 32/1000 | Loss: 0.00001444
Iteration 33/1000 | Loss: 0.00001443
Iteration 34/1000 | Loss: 0.00001443
Iteration 35/1000 | Loss: 0.00001443
Iteration 36/1000 | Loss: 0.00001442
Iteration 37/1000 | Loss: 0.00001442
Iteration 38/1000 | Loss: 0.00001441
Iteration 39/1000 | Loss: 0.00001441
Iteration 40/1000 | Loss: 0.00001440
Iteration 41/1000 | Loss: 0.00001440
Iteration 42/1000 | Loss: 0.00001439
Iteration 43/1000 | Loss: 0.00001439
Iteration 44/1000 | Loss: 0.00001439
Iteration 45/1000 | Loss: 0.00001439
Iteration 46/1000 | Loss: 0.00001438
Iteration 47/1000 | Loss: 0.00001438
Iteration 48/1000 | Loss: 0.00001438
Iteration 49/1000 | Loss: 0.00001438
Iteration 50/1000 | Loss: 0.00001438
Iteration 51/1000 | Loss: 0.00001437
Iteration 52/1000 | Loss: 0.00001437
Iteration 53/1000 | Loss: 0.00001437
Iteration 54/1000 | Loss: 0.00001437
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001436
Iteration 59/1000 | Loss: 0.00001436
Iteration 60/1000 | Loss: 0.00001436
Iteration 61/1000 | Loss: 0.00001436
Iteration 62/1000 | Loss: 0.00001436
Iteration 63/1000 | Loss: 0.00001436
Iteration 64/1000 | Loss: 0.00005283
Iteration 65/1000 | Loss: 0.00005388
Iteration 66/1000 | Loss: 0.00001702
Iteration 67/1000 | Loss: 0.00001498
Iteration 68/1000 | Loss: 0.00003635
Iteration 69/1000 | Loss: 0.00001437
Iteration 70/1000 | Loss: 0.00001437
Iteration 71/1000 | Loss: 0.00001437
Iteration 72/1000 | Loss: 0.00001436
Iteration 73/1000 | Loss: 0.00001436
Iteration 74/1000 | Loss: 0.00001435
Iteration 75/1000 | Loss: 0.00001435
Iteration 76/1000 | Loss: 0.00001434
Iteration 77/1000 | Loss: 0.00001434
Iteration 78/1000 | Loss: 0.00001434
Iteration 79/1000 | Loss: 0.00001433
Iteration 80/1000 | Loss: 0.00001433
Iteration 81/1000 | Loss: 0.00001433
Iteration 82/1000 | Loss: 0.00001433
Iteration 83/1000 | Loss: 0.00001432
Iteration 84/1000 | Loss: 0.00001432
Iteration 85/1000 | Loss: 0.00001432
Iteration 86/1000 | Loss: 0.00001432
Iteration 87/1000 | Loss: 0.00001432
Iteration 88/1000 | Loss: 0.00001432
Iteration 89/1000 | Loss: 0.00001432
Iteration 90/1000 | Loss: 0.00001432
Iteration 91/1000 | Loss: 0.00001432
Iteration 92/1000 | Loss: 0.00001432
Iteration 93/1000 | Loss: 0.00001432
Iteration 94/1000 | Loss: 0.00001432
Iteration 95/1000 | Loss: 0.00001432
Iteration 96/1000 | Loss: 0.00001432
Iteration 97/1000 | Loss: 0.00001432
Iteration 98/1000 | Loss: 0.00001432
Iteration 99/1000 | Loss: 0.00001432
Iteration 100/1000 | Loss: 0.00001432
Iteration 101/1000 | Loss: 0.00001432
Iteration 102/1000 | Loss: 0.00001432
Iteration 103/1000 | Loss: 0.00001432
Iteration 104/1000 | Loss: 0.00001432
Iteration 105/1000 | Loss: 0.00001432
Iteration 106/1000 | Loss: 0.00001432
Iteration 107/1000 | Loss: 0.00001432
Iteration 108/1000 | Loss: 0.00001432
Iteration 109/1000 | Loss: 0.00001432
Iteration 110/1000 | Loss: 0.00001432
Iteration 111/1000 | Loss: 0.00001432
Iteration 112/1000 | Loss: 0.00001432
Iteration 113/1000 | Loss: 0.00001432
Iteration 114/1000 | Loss: 0.00001432
Iteration 115/1000 | Loss: 0.00001432
Iteration 116/1000 | Loss: 0.00001432
Iteration 117/1000 | Loss: 0.00001432
Iteration 118/1000 | Loss: 0.00001432
Iteration 119/1000 | Loss: 0.00001432
Iteration 120/1000 | Loss: 0.00001432
Iteration 121/1000 | Loss: 0.00001432
Iteration 122/1000 | Loss: 0.00001432
Iteration 123/1000 | Loss: 0.00001432
Iteration 124/1000 | Loss: 0.00001432
Iteration 125/1000 | Loss: 0.00001432
Iteration 126/1000 | Loss: 0.00001432
Iteration 127/1000 | Loss: 0.00001432
Iteration 128/1000 | Loss: 0.00001432
Iteration 129/1000 | Loss: 0.00001432
Iteration 130/1000 | Loss: 0.00001432
Iteration 131/1000 | Loss: 0.00001432
Iteration 132/1000 | Loss: 0.00001432
Iteration 133/1000 | Loss: 0.00001432
Iteration 134/1000 | Loss: 0.00001432
Iteration 135/1000 | Loss: 0.00001432
Iteration 136/1000 | Loss: 0.00001432
Iteration 137/1000 | Loss: 0.00001432
Iteration 138/1000 | Loss: 0.00001432
Iteration 139/1000 | Loss: 0.00001432
Iteration 140/1000 | Loss: 0.00001432
Iteration 141/1000 | Loss: 0.00001432
Iteration 142/1000 | Loss: 0.00001432
Iteration 143/1000 | Loss: 0.00001432
Iteration 144/1000 | Loss: 0.00001432
Iteration 145/1000 | Loss: 0.00001432
Iteration 146/1000 | Loss: 0.00001432
Iteration 147/1000 | Loss: 0.00001432
Iteration 148/1000 | Loss: 0.00001432
Iteration 149/1000 | Loss: 0.00001432
Iteration 150/1000 | Loss: 0.00001432
Iteration 151/1000 | Loss: 0.00001432
Iteration 152/1000 | Loss: 0.00001432
Iteration 153/1000 | Loss: 0.00001432
Iteration 154/1000 | Loss: 0.00001432
Iteration 155/1000 | Loss: 0.00001432
Iteration 156/1000 | Loss: 0.00001432
Iteration 157/1000 | Loss: 0.00001432
Iteration 158/1000 | Loss: 0.00001432
Iteration 159/1000 | Loss: 0.00001432
Iteration 160/1000 | Loss: 0.00001432
Iteration 161/1000 | Loss: 0.00001432
Iteration 162/1000 | Loss: 0.00001432
Iteration 163/1000 | Loss: 0.00001432
Iteration 164/1000 | Loss: 0.00001432
Iteration 165/1000 | Loss: 0.00001432
Iteration 166/1000 | Loss: 0.00001432
Iteration 167/1000 | Loss: 0.00001432
Iteration 168/1000 | Loss: 0.00001432
Iteration 169/1000 | Loss: 0.00001432
Iteration 170/1000 | Loss: 0.00001432
Iteration 171/1000 | Loss: 0.00001432
Iteration 172/1000 | Loss: 0.00001432
Iteration 173/1000 | Loss: 0.00001432
Iteration 174/1000 | Loss: 0.00001432
Iteration 175/1000 | Loss: 0.00001432
Iteration 176/1000 | Loss: 0.00001432
Iteration 177/1000 | Loss: 0.00001432
Iteration 178/1000 | Loss: 0.00001432
Iteration 179/1000 | Loss: 0.00001432
Iteration 180/1000 | Loss: 0.00001432
Iteration 181/1000 | Loss: 0.00001432
Iteration 182/1000 | Loss: 0.00001432
Iteration 183/1000 | Loss: 0.00001432
Iteration 184/1000 | Loss: 0.00001432
Iteration 185/1000 | Loss: 0.00001432
Iteration 186/1000 | Loss: 0.00001432
Iteration 187/1000 | Loss: 0.00001432
Iteration 188/1000 | Loss: 0.00001432
Iteration 189/1000 | Loss: 0.00001432
Iteration 190/1000 | Loss: 0.00001432
Iteration 191/1000 | Loss: 0.00001432
Iteration 192/1000 | Loss: 0.00001432
Iteration 193/1000 | Loss: 0.00001432
Iteration 194/1000 | Loss: 0.00001432
Iteration 195/1000 | Loss: 0.00001432
Iteration 196/1000 | Loss: 0.00001432
Iteration 197/1000 | Loss: 0.00001432
Iteration 198/1000 | Loss: 0.00001432
Iteration 199/1000 | Loss: 0.00001432
Iteration 200/1000 | Loss: 0.00001432
Iteration 201/1000 | Loss: 0.00001432
Iteration 202/1000 | Loss: 0.00001432
Iteration 203/1000 | Loss: 0.00001432
Iteration 204/1000 | Loss: 0.00001432
Iteration 205/1000 | Loss: 0.00001432
Iteration 206/1000 | Loss: 0.00001432
Iteration 207/1000 | Loss: 0.00001432
Iteration 208/1000 | Loss: 0.00001432
Iteration 209/1000 | Loss: 0.00001432
Iteration 210/1000 | Loss: 0.00001432
Iteration 211/1000 | Loss: 0.00001432
Iteration 212/1000 | Loss: 0.00001432
Iteration 213/1000 | Loss: 0.00001432
Iteration 214/1000 | Loss: 0.00001432
Iteration 215/1000 | Loss: 0.00001432
Iteration 216/1000 | Loss: 0.00001432
Iteration 217/1000 | Loss: 0.00001432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.4315685803012457e-05, 1.4315685803012457e-05, 1.4315685803012457e-05, 1.4315685803012457e-05, 1.4315685803012457e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4315685803012457e-05

Optimization complete. Final v2v error: 3.2293283939361572 mm

Highest mean error: 8.11555004119873 mm for frame 35

Lowest mean error: 2.771404504776001 mm for frame 1

Saving results

Total time: 65.58154964447021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810000
Iteration 2/25 | Loss: 0.00180726
Iteration 3/25 | Loss: 0.00116798
Iteration 4/25 | Loss: 0.00106915
Iteration 5/25 | Loss: 0.00100529
Iteration 6/25 | Loss: 0.00095734
Iteration 7/25 | Loss: 0.00094094
Iteration 8/25 | Loss: 0.00093209
Iteration 9/25 | Loss: 0.00093684
Iteration 10/25 | Loss: 0.00092525
Iteration 11/25 | Loss: 0.00092366
Iteration 12/25 | Loss: 0.00092514
Iteration 13/25 | Loss: 0.00091624
Iteration 14/25 | Loss: 0.00091658
Iteration 15/25 | Loss: 0.00091751
Iteration 16/25 | Loss: 0.00091599
Iteration 17/25 | Loss: 0.00091579
Iteration 18/25 | Loss: 0.00091542
Iteration 19/25 | Loss: 0.00091915
Iteration 20/25 | Loss: 0.00091525
Iteration 21/25 | Loss: 0.00091419
Iteration 22/25 | Loss: 0.00091419
Iteration 23/25 | Loss: 0.00091418
Iteration 24/25 | Loss: 0.00091418
Iteration 25/25 | Loss: 0.00091418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13794470
Iteration 2/25 | Loss: 0.00040253
Iteration 3/25 | Loss: 0.00039502
Iteration 4/25 | Loss: 0.00039494
Iteration 5/25 | Loss: 0.00038873
Iteration 6/25 | Loss: 0.00038873
Iteration 7/25 | Loss: 0.00038873
Iteration 8/25 | Loss: 0.00038873
Iteration 9/25 | Loss: 0.00038873
Iteration 10/25 | Loss: 0.00038873
Iteration 11/25 | Loss: 0.00038873
Iteration 12/25 | Loss: 0.00038872
Iteration 13/25 | Loss: 0.00038872
Iteration 14/25 | Loss: 0.00038872
Iteration 15/25 | Loss: 0.00038872
Iteration 16/25 | Loss: 0.00038872
Iteration 17/25 | Loss: 0.00038872
Iteration 18/25 | Loss: 0.00038872
Iteration 19/25 | Loss: 0.00038872
Iteration 20/25 | Loss: 0.00038872
Iteration 21/25 | Loss: 0.00038872
Iteration 22/25 | Loss: 0.00038872
Iteration 23/25 | Loss: 0.00038872
Iteration 24/25 | Loss: 0.00038872
Iteration 25/25 | Loss: 0.00038872

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038872
Iteration 2/1000 | Loss: 0.00006651
Iteration 3/1000 | Loss: 0.00007077
Iteration 4/1000 | Loss: 0.00008834
Iteration 5/1000 | Loss: 0.00014806
Iteration 6/1000 | Loss: 0.00062314
Iteration 7/1000 | Loss: 0.00076761
Iteration 8/1000 | Loss: 0.00009166
Iteration 9/1000 | Loss: 0.00003521
Iteration 10/1000 | Loss: 0.00002543
Iteration 11/1000 | Loss: 0.00004445
Iteration 12/1000 | Loss: 0.00001242
Iteration 13/1000 | Loss: 0.00001224
Iteration 14/1000 | Loss: 0.00014658
Iteration 15/1000 | Loss: 0.00001700
Iteration 16/1000 | Loss: 0.00001209
Iteration 17/1000 | Loss: 0.00001204
Iteration 18/1000 | Loss: 0.00001199
Iteration 19/1000 | Loss: 0.00001199
Iteration 20/1000 | Loss: 0.00001198
Iteration 21/1000 | Loss: 0.00001198
Iteration 22/1000 | Loss: 0.00001198
Iteration 23/1000 | Loss: 0.00001198
Iteration 24/1000 | Loss: 0.00001198
Iteration 25/1000 | Loss: 0.00001197
Iteration 26/1000 | Loss: 0.00001197
Iteration 27/1000 | Loss: 0.00001191
Iteration 28/1000 | Loss: 0.00001186
Iteration 29/1000 | Loss: 0.00001184
Iteration 30/1000 | Loss: 0.00001183
Iteration 31/1000 | Loss: 0.00003240
Iteration 32/1000 | Loss: 0.00001883
Iteration 33/1000 | Loss: 0.00001228
Iteration 34/1000 | Loss: 0.00001812
Iteration 35/1000 | Loss: 0.00001547
Iteration 36/1000 | Loss: 0.00001172
Iteration 37/1000 | Loss: 0.00002183
Iteration 38/1000 | Loss: 0.00001339
Iteration 39/1000 | Loss: 0.00001167
Iteration 40/1000 | Loss: 0.00001306
Iteration 41/1000 | Loss: 0.00001165
Iteration 42/1000 | Loss: 0.00001165
Iteration 43/1000 | Loss: 0.00001165
Iteration 44/1000 | Loss: 0.00001165
Iteration 45/1000 | Loss: 0.00001165
Iteration 46/1000 | Loss: 0.00001165
Iteration 47/1000 | Loss: 0.00001165
Iteration 48/1000 | Loss: 0.00001165
Iteration 49/1000 | Loss: 0.00001165
Iteration 50/1000 | Loss: 0.00001165
Iteration 51/1000 | Loss: 0.00001165
Iteration 52/1000 | Loss: 0.00001165
Iteration 53/1000 | Loss: 0.00001165
Iteration 54/1000 | Loss: 0.00001165
Iteration 55/1000 | Loss: 0.00001165
Iteration 56/1000 | Loss: 0.00001165
Iteration 57/1000 | Loss: 0.00001165
Iteration 58/1000 | Loss: 0.00001165
Iteration 59/1000 | Loss: 0.00001165
Iteration 60/1000 | Loss: 0.00001165
Iteration 61/1000 | Loss: 0.00001165
Iteration 62/1000 | Loss: 0.00001165
Iteration 63/1000 | Loss: 0.00001165
Iteration 64/1000 | Loss: 0.00001165
Iteration 65/1000 | Loss: 0.00001165
Iteration 66/1000 | Loss: 0.00001165
Iteration 67/1000 | Loss: 0.00001165
Iteration 68/1000 | Loss: 0.00001165
Iteration 69/1000 | Loss: 0.00001165
Iteration 70/1000 | Loss: 0.00001165
Iteration 71/1000 | Loss: 0.00001165
Iteration 72/1000 | Loss: 0.00001165
Iteration 73/1000 | Loss: 0.00001165
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001165
Iteration 76/1000 | Loss: 0.00001165
Iteration 77/1000 | Loss: 0.00001165
Iteration 78/1000 | Loss: 0.00001165
Iteration 79/1000 | Loss: 0.00001165
Iteration 80/1000 | Loss: 0.00001165
Iteration 81/1000 | Loss: 0.00001165
Iteration 82/1000 | Loss: 0.00001165
Iteration 83/1000 | Loss: 0.00001165
Iteration 84/1000 | Loss: 0.00001165
Iteration 85/1000 | Loss: 0.00001165
Iteration 86/1000 | Loss: 0.00001165
Iteration 87/1000 | Loss: 0.00001165
Iteration 88/1000 | Loss: 0.00001165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.1648910003714263e-05, 1.1648910003714263e-05, 1.1648910003714263e-05, 1.1648910003714263e-05, 1.1648910003714263e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1648910003714263e-05

Optimization complete. Final v2v error: 2.8235790729522705 mm

Highest mean error: 8.616598129272461 mm for frame 98

Lowest mean error: 2.4358882904052734 mm for frame 33

Saving results

Total time: 76.11811661720276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064400
Iteration 2/25 | Loss: 0.00304784
Iteration 3/25 | Loss: 0.00186308
Iteration 4/25 | Loss: 0.00159816
Iteration 5/25 | Loss: 0.00154476
Iteration 6/25 | Loss: 0.00142148
Iteration 7/25 | Loss: 0.00138249
Iteration 8/25 | Loss: 0.00132953
Iteration 9/25 | Loss: 0.00129863
Iteration 10/25 | Loss: 0.00130851
Iteration 11/25 | Loss: 0.00125782
Iteration 12/25 | Loss: 0.00125191
Iteration 13/25 | Loss: 0.00124038
Iteration 14/25 | Loss: 0.00123641
Iteration 15/25 | Loss: 0.00123669
Iteration 16/25 | Loss: 0.00123091
Iteration 17/25 | Loss: 0.00123308
Iteration 18/25 | Loss: 0.00123183
Iteration 19/25 | Loss: 0.00122961
Iteration 20/25 | Loss: 0.00122792
Iteration 21/25 | Loss: 0.00122777
Iteration 22/25 | Loss: 0.00122762
Iteration 23/25 | Loss: 0.00122758
Iteration 24/25 | Loss: 0.00122758
Iteration 25/25 | Loss: 0.00122758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46824789
Iteration 2/25 | Loss: 0.00219430
Iteration 3/25 | Loss: 0.00203575
Iteration 4/25 | Loss: 0.00203574
Iteration 5/25 | Loss: 0.00203574
Iteration 6/25 | Loss: 0.00203574
Iteration 7/25 | Loss: 0.00203574
Iteration 8/25 | Loss: 0.00203574
Iteration 9/25 | Loss: 0.00203574
Iteration 10/25 | Loss: 0.00203574
Iteration 11/25 | Loss: 0.00203574
Iteration 12/25 | Loss: 0.00203574
Iteration 13/25 | Loss: 0.00203574
Iteration 14/25 | Loss: 0.00203574
Iteration 15/25 | Loss: 0.00203574
Iteration 16/25 | Loss: 0.00203574
Iteration 17/25 | Loss: 0.00203574
Iteration 18/25 | Loss: 0.00203574
Iteration 19/25 | Loss: 0.00203574
Iteration 20/25 | Loss: 0.00203574
Iteration 21/25 | Loss: 0.00203574
Iteration 22/25 | Loss: 0.00203574
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0020357409957796335, 0.0020357409957796335, 0.0020357409957796335, 0.0020357409957796335, 0.0020357409957796335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020357409957796335

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00203574
Iteration 2/1000 | Loss: 0.00060494
Iteration 3/1000 | Loss: 0.00030611
Iteration 4/1000 | Loss: 0.00018505
Iteration 5/1000 | Loss: 0.00025123
Iteration 6/1000 | Loss: 0.00016865
Iteration 7/1000 | Loss: 0.00017013
Iteration 8/1000 | Loss: 0.00028708
Iteration 9/1000 | Loss: 0.00012076
Iteration 10/1000 | Loss: 0.00017151
Iteration 11/1000 | Loss: 0.00063894
Iteration 12/1000 | Loss: 0.00021045
Iteration 13/1000 | Loss: 0.00063092
Iteration 14/1000 | Loss: 0.00015554
Iteration 15/1000 | Loss: 0.00105852
Iteration 16/1000 | Loss: 0.00115202
Iteration 17/1000 | Loss: 0.00169208
Iteration 18/1000 | Loss: 0.00107283
Iteration 19/1000 | Loss: 0.00092376
Iteration 20/1000 | Loss: 0.00200880
Iteration 21/1000 | Loss: 0.00014893
Iteration 22/1000 | Loss: 0.00024540
Iteration 23/1000 | Loss: 0.00026397
Iteration 24/1000 | Loss: 0.00015070
Iteration 25/1000 | Loss: 0.00062543
Iteration 26/1000 | Loss: 0.00008306
Iteration 27/1000 | Loss: 0.00020221
Iteration 28/1000 | Loss: 0.00006960
Iteration 29/1000 | Loss: 0.00007067
Iteration 30/1000 | Loss: 0.00006147
Iteration 31/1000 | Loss: 0.00012828
Iteration 32/1000 | Loss: 0.00005601
Iteration 33/1000 | Loss: 0.00012869
Iteration 34/1000 | Loss: 0.00005280
Iteration 35/1000 | Loss: 0.00009794
Iteration 36/1000 | Loss: 0.00007304
Iteration 37/1000 | Loss: 0.00006036
Iteration 38/1000 | Loss: 0.00004871
Iteration 39/1000 | Loss: 0.00006310
Iteration 40/1000 | Loss: 0.00004776
Iteration 41/1000 | Loss: 0.00014544
Iteration 42/1000 | Loss: 0.00109733
Iteration 43/1000 | Loss: 0.00015657
Iteration 44/1000 | Loss: 0.00007000
Iteration 45/1000 | Loss: 0.00005732
Iteration 46/1000 | Loss: 0.00008177
Iteration 47/1000 | Loss: 0.00004139
Iteration 48/1000 | Loss: 0.00005774
Iteration 49/1000 | Loss: 0.00003197
Iteration 50/1000 | Loss: 0.00005820
Iteration 51/1000 | Loss: 0.00003431
Iteration 52/1000 | Loss: 0.00002783
Iteration 53/1000 | Loss: 0.00002659
Iteration 54/1000 | Loss: 0.00002586
Iteration 55/1000 | Loss: 0.00003287
Iteration 56/1000 | Loss: 0.00003229
Iteration 57/1000 | Loss: 0.00007309
Iteration 58/1000 | Loss: 0.00010173
Iteration 59/1000 | Loss: 0.00006959
Iteration 60/1000 | Loss: 0.00003100
Iteration 61/1000 | Loss: 0.00003057
Iteration 62/1000 | Loss: 0.00002485
Iteration 63/1000 | Loss: 0.00002465
Iteration 64/1000 | Loss: 0.00002464
Iteration 65/1000 | Loss: 0.00002461
Iteration 66/1000 | Loss: 0.00002456
Iteration 67/1000 | Loss: 0.00002456
Iteration 68/1000 | Loss: 0.00002454
Iteration 69/1000 | Loss: 0.00002454
Iteration 70/1000 | Loss: 0.00002454
Iteration 71/1000 | Loss: 0.00002454
Iteration 72/1000 | Loss: 0.00002454
Iteration 73/1000 | Loss: 0.00002454
Iteration 74/1000 | Loss: 0.00002454
Iteration 75/1000 | Loss: 0.00002454
Iteration 76/1000 | Loss: 0.00002454
Iteration 77/1000 | Loss: 0.00002454
Iteration 78/1000 | Loss: 0.00002454
Iteration 79/1000 | Loss: 0.00002454
Iteration 80/1000 | Loss: 0.00002454
Iteration 81/1000 | Loss: 0.00002453
Iteration 82/1000 | Loss: 0.00002453
Iteration 83/1000 | Loss: 0.00002453
Iteration 84/1000 | Loss: 0.00002453
Iteration 85/1000 | Loss: 0.00002453
Iteration 86/1000 | Loss: 0.00002452
Iteration 87/1000 | Loss: 0.00002452
Iteration 88/1000 | Loss: 0.00002452
Iteration 89/1000 | Loss: 0.00002451
Iteration 90/1000 | Loss: 0.00002450
Iteration 91/1000 | Loss: 0.00002450
Iteration 92/1000 | Loss: 0.00002450
Iteration 93/1000 | Loss: 0.00002449
Iteration 94/1000 | Loss: 0.00002446
Iteration 95/1000 | Loss: 0.00002445
Iteration 96/1000 | Loss: 0.00002445
Iteration 97/1000 | Loss: 0.00002445
Iteration 98/1000 | Loss: 0.00004573
Iteration 99/1000 | Loss: 0.00002502
Iteration 100/1000 | Loss: 0.00002765
Iteration 101/1000 | Loss: 0.00002446
Iteration 102/1000 | Loss: 0.00002446
Iteration 103/1000 | Loss: 0.00002446
Iteration 104/1000 | Loss: 0.00002446
Iteration 105/1000 | Loss: 0.00002446
Iteration 106/1000 | Loss: 0.00002446
Iteration 107/1000 | Loss: 0.00002446
Iteration 108/1000 | Loss: 0.00002446
Iteration 109/1000 | Loss: 0.00002445
Iteration 110/1000 | Loss: 0.00002445
Iteration 111/1000 | Loss: 0.00002445
Iteration 112/1000 | Loss: 0.00002445
Iteration 113/1000 | Loss: 0.00002445
Iteration 114/1000 | Loss: 0.00002445
Iteration 115/1000 | Loss: 0.00002445
Iteration 116/1000 | Loss: 0.00002445
Iteration 117/1000 | Loss: 0.00002445
Iteration 118/1000 | Loss: 0.00002443
Iteration 119/1000 | Loss: 0.00002442
Iteration 120/1000 | Loss: 0.00002442
Iteration 121/1000 | Loss: 0.00002442
Iteration 122/1000 | Loss: 0.00002441
Iteration 123/1000 | Loss: 0.00002441
Iteration 124/1000 | Loss: 0.00002441
Iteration 125/1000 | Loss: 0.00002441
Iteration 126/1000 | Loss: 0.00002440
Iteration 127/1000 | Loss: 0.00002440
Iteration 128/1000 | Loss: 0.00002440
Iteration 129/1000 | Loss: 0.00002440
Iteration 130/1000 | Loss: 0.00002440
Iteration 131/1000 | Loss: 0.00002439
Iteration 132/1000 | Loss: 0.00002439
Iteration 133/1000 | Loss: 0.00013539
Iteration 134/1000 | Loss: 0.00005515
Iteration 135/1000 | Loss: 0.00006460
Iteration 136/1000 | Loss: 0.00002550
Iteration 137/1000 | Loss: 0.00002438
Iteration 138/1000 | Loss: 0.00003279
Iteration 139/1000 | Loss: 0.00002323
Iteration 140/1000 | Loss: 0.00002354
Iteration 141/1000 | Loss: 0.00003651
Iteration 142/1000 | Loss: 0.00002682
Iteration 143/1000 | Loss: 0.00002674
Iteration 144/1000 | Loss: 0.00002254
Iteration 145/1000 | Loss: 0.00002251
Iteration 146/1000 | Loss: 0.00002251
Iteration 147/1000 | Loss: 0.00002248
Iteration 148/1000 | Loss: 0.00002247
Iteration 149/1000 | Loss: 0.00002246
Iteration 150/1000 | Loss: 0.00002245
Iteration 151/1000 | Loss: 0.00002245
Iteration 152/1000 | Loss: 0.00002245
Iteration 153/1000 | Loss: 0.00002244
Iteration 154/1000 | Loss: 0.00002244
Iteration 155/1000 | Loss: 0.00002916
Iteration 156/1000 | Loss: 0.00002243
Iteration 157/1000 | Loss: 0.00002242
Iteration 158/1000 | Loss: 0.00002242
Iteration 159/1000 | Loss: 0.00002242
Iteration 160/1000 | Loss: 0.00002241
Iteration 161/1000 | Loss: 0.00002241
Iteration 162/1000 | Loss: 0.00002241
Iteration 163/1000 | Loss: 0.00002240
Iteration 164/1000 | Loss: 0.00002240
Iteration 165/1000 | Loss: 0.00002240
Iteration 166/1000 | Loss: 0.00002240
Iteration 167/1000 | Loss: 0.00002240
Iteration 168/1000 | Loss: 0.00002239
Iteration 169/1000 | Loss: 0.00002239
Iteration 170/1000 | Loss: 0.00002239
Iteration 171/1000 | Loss: 0.00002239
Iteration 172/1000 | Loss: 0.00002239
Iteration 173/1000 | Loss: 0.00002239
Iteration 174/1000 | Loss: 0.00002239
Iteration 175/1000 | Loss: 0.00002239
Iteration 176/1000 | Loss: 0.00002239
Iteration 177/1000 | Loss: 0.00002239
Iteration 178/1000 | Loss: 0.00002239
Iteration 179/1000 | Loss: 0.00002238
Iteration 180/1000 | Loss: 0.00002238
Iteration 181/1000 | Loss: 0.00002238
Iteration 182/1000 | Loss: 0.00002237
Iteration 183/1000 | Loss: 0.00002237
Iteration 184/1000 | Loss: 0.00002237
Iteration 185/1000 | Loss: 0.00002236
Iteration 186/1000 | Loss: 0.00002236
Iteration 187/1000 | Loss: 0.00002236
Iteration 188/1000 | Loss: 0.00002236
Iteration 189/1000 | Loss: 0.00002235
Iteration 190/1000 | Loss: 0.00002235
Iteration 191/1000 | Loss: 0.00002982
Iteration 192/1000 | Loss: 0.00002436
Iteration 193/1000 | Loss: 0.00002234
Iteration 194/1000 | Loss: 0.00002234
Iteration 195/1000 | Loss: 0.00002234
Iteration 196/1000 | Loss: 0.00002234
Iteration 197/1000 | Loss: 0.00002234
Iteration 198/1000 | Loss: 0.00002234
Iteration 199/1000 | Loss: 0.00002234
Iteration 200/1000 | Loss: 0.00002234
Iteration 201/1000 | Loss: 0.00002234
Iteration 202/1000 | Loss: 0.00002534
Iteration 203/1000 | Loss: 0.00002506
Iteration 204/1000 | Loss: 0.00002235
Iteration 205/1000 | Loss: 0.00002235
Iteration 206/1000 | Loss: 0.00002235
Iteration 207/1000 | Loss: 0.00002235
Iteration 208/1000 | Loss: 0.00002235
Iteration 209/1000 | Loss: 0.00002235
Iteration 210/1000 | Loss: 0.00002234
Iteration 211/1000 | Loss: 0.00002234
Iteration 212/1000 | Loss: 0.00002234
Iteration 213/1000 | Loss: 0.00002234
Iteration 214/1000 | Loss: 0.00002234
Iteration 215/1000 | Loss: 0.00002234
Iteration 216/1000 | Loss: 0.00002233
Iteration 217/1000 | Loss: 0.00002233
Iteration 218/1000 | Loss: 0.00002233
Iteration 219/1000 | Loss: 0.00002233
Iteration 220/1000 | Loss: 0.00002233
Iteration 221/1000 | Loss: 0.00002233
Iteration 222/1000 | Loss: 0.00002233
Iteration 223/1000 | Loss: 0.00002233
Iteration 224/1000 | Loss: 0.00002233
Iteration 225/1000 | Loss: 0.00002233
Iteration 226/1000 | Loss: 0.00002233
Iteration 227/1000 | Loss: 0.00002233
Iteration 228/1000 | Loss: 0.00002232
Iteration 229/1000 | Loss: 0.00002232
Iteration 230/1000 | Loss: 0.00002232
Iteration 231/1000 | Loss: 0.00002232
Iteration 232/1000 | Loss: 0.00002232
Iteration 233/1000 | Loss: 0.00002232
Iteration 234/1000 | Loss: 0.00002232
Iteration 235/1000 | Loss: 0.00002231
Iteration 236/1000 | Loss: 0.00002231
Iteration 237/1000 | Loss: 0.00002231
Iteration 238/1000 | Loss: 0.00002231
Iteration 239/1000 | Loss: 0.00002231
Iteration 240/1000 | Loss: 0.00002231
Iteration 241/1000 | Loss: 0.00002231
Iteration 242/1000 | Loss: 0.00002231
Iteration 243/1000 | Loss: 0.00002231
Iteration 244/1000 | Loss: 0.00002231
Iteration 245/1000 | Loss: 0.00002231
Iteration 246/1000 | Loss: 0.00002231
Iteration 247/1000 | Loss: 0.00002230
Iteration 248/1000 | Loss: 0.00002230
Iteration 249/1000 | Loss: 0.00002230
Iteration 250/1000 | Loss: 0.00002230
Iteration 251/1000 | Loss: 0.00002230
Iteration 252/1000 | Loss: 0.00002230
Iteration 253/1000 | Loss: 0.00002230
Iteration 254/1000 | Loss: 0.00002230
Iteration 255/1000 | Loss: 0.00002230
Iteration 256/1000 | Loss: 0.00002230
Iteration 257/1000 | Loss: 0.00002230
Iteration 258/1000 | Loss: 0.00002230
Iteration 259/1000 | Loss: 0.00002229
Iteration 260/1000 | Loss: 0.00002229
Iteration 261/1000 | Loss: 0.00002229
Iteration 262/1000 | Loss: 0.00002229
Iteration 263/1000 | Loss: 0.00002229
Iteration 264/1000 | Loss: 0.00002229
Iteration 265/1000 | Loss: 0.00002229
Iteration 266/1000 | Loss: 0.00002229
Iteration 267/1000 | Loss: 0.00002229
Iteration 268/1000 | Loss: 0.00002229
Iteration 269/1000 | Loss: 0.00002229
Iteration 270/1000 | Loss: 0.00002229
Iteration 271/1000 | Loss: 0.00002229
Iteration 272/1000 | Loss: 0.00002229
Iteration 273/1000 | Loss: 0.00002229
Iteration 274/1000 | Loss: 0.00002229
Iteration 275/1000 | Loss: 0.00002229
Iteration 276/1000 | Loss: 0.00002229
Iteration 277/1000 | Loss: 0.00002229
Iteration 278/1000 | Loss: 0.00002229
Iteration 279/1000 | Loss: 0.00002229
Iteration 280/1000 | Loss: 0.00002229
Iteration 281/1000 | Loss: 0.00002229
Iteration 282/1000 | Loss: 0.00002229
Iteration 283/1000 | Loss: 0.00002229
Iteration 284/1000 | Loss: 0.00002229
Iteration 285/1000 | Loss: 0.00002229
Iteration 286/1000 | Loss: 0.00002229
Iteration 287/1000 | Loss: 0.00002229
Iteration 288/1000 | Loss: 0.00002229
Iteration 289/1000 | Loss: 0.00002229
Iteration 290/1000 | Loss: 0.00002229
Iteration 291/1000 | Loss: 0.00002229
Iteration 292/1000 | Loss: 0.00002229
Iteration 293/1000 | Loss: 0.00002229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 293. Stopping optimization.
Last 5 losses: [2.2290925699053332e-05, 2.2290925699053332e-05, 2.2290925699053332e-05, 2.2290925699053332e-05, 2.2290925699053332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2290925699053332e-05

Optimization complete. Final v2v error: 3.590186357498169 mm

Highest mean error: 13.258514404296875 mm for frame 172

Lowest mean error: 2.5607855319976807 mm for frame 166

Saving results

Total time: 192.3726041316986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499729
Iteration 2/25 | Loss: 0.00129210
Iteration 3/25 | Loss: 0.00102392
Iteration 4/25 | Loss: 0.00100089
Iteration 5/25 | Loss: 0.00099755
Iteration 6/25 | Loss: 0.00099667
Iteration 7/25 | Loss: 0.00099664
Iteration 8/25 | Loss: 0.00099664
Iteration 9/25 | Loss: 0.00099664
Iteration 10/25 | Loss: 0.00099664
Iteration 11/25 | Loss: 0.00099664
Iteration 12/25 | Loss: 0.00099664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009966420475393534, 0.0009966420475393534, 0.0009966420475393534, 0.0009966420475393534, 0.0009966420475393534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009966420475393534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46378899
Iteration 2/25 | Loss: 0.00041843
Iteration 3/25 | Loss: 0.00041843
Iteration 4/25 | Loss: 0.00041843
Iteration 5/25 | Loss: 0.00041843
Iteration 6/25 | Loss: 0.00041843
Iteration 7/25 | Loss: 0.00041843
Iteration 8/25 | Loss: 0.00041843
Iteration 9/25 | Loss: 0.00041843
Iteration 10/25 | Loss: 0.00041843
Iteration 11/25 | Loss: 0.00041843
Iteration 12/25 | Loss: 0.00041843
Iteration 13/25 | Loss: 0.00041843
Iteration 14/25 | Loss: 0.00041843
Iteration 15/25 | Loss: 0.00041843
Iteration 16/25 | Loss: 0.00041843
Iteration 17/25 | Loss: 0.00041843
Iteration 18/25 | Loss: 0.00041843
Iteration 19/25 | Loss: 0.00041843
Iteration 20/25 | Loss: 0.00041843
Iteration 21/25 | Loss: 0.00041843
Iteration 22/25 | Loss: 0.00041843
Iteration 23/25 | Loss: 0.00041843
Iteration 24/25 | Loss: 0.00041843
Iteration 25/25 | Loss: 0.00041843

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041843
Iteration 2/1000 | Loss: 0.00004732
Iteration 3/1000 | Loss: 0.00002271
Iteration 4/1000 | Loss: 0.00002000
Iteration 5/1000 | Loss: 0.00001887
Iteration 6/1000 | Loss: 0.00001822
Iteration 7/1000 | Loss: 0.00001781
Iteration 8/1000 | Loss: 0.00001759
Iteration 9/1000 | Loss: 0.00001738
Iteration 10/1000 | Loss: 0.00001724
Iteration 11/1000 | Loss: 0.00001724
Iteration 12/1000 | Loss: 0.00001718
Iteration 13/1000 | Loss: 0.00001708
Iteration 14/1000 | Loss: 0.00001705
Iteration 15/1000 | Loss: 0.00001705
Iteration 16/1000 | Loss: 0.00001704
Iteration 17/1000 | Loss: 0.00001702
Iteration 18/1000 | Loss: 0.00001702
Iteration 19/1000 | Loss: 0.00001701
Iteration 20/1000 | Loss: 0.00001700
Iteration 21/1000 | Loss: 0.00001700
Iteration 22/1000 | Loss: 0.00001700
Iteration 23/1000 | Loss: 0.00001700
Iteration 24/1000 | Loss: 0.00001700
Iteration 25/1000 | Loss: 0.00001700
Iteration 26/1000 | Loss: 0.00001700
Iteration 27/1000 | Loss: 0.00001700
Iteration 28/1000 | Loss: 0.00001700
Iteration 29/1000 | Loss: 0.00001698
Iteration 30/1000 | Loss: 0.00001697
Iteration 31/1000 | Loss: 0.00001691
Iteration 32/1000 | Loss: 0.00001686
Iteration 33/1000 | Loss: 0.00001686
Iteration 34/1000 | Loss: 0.00001686
Iteration 35/1000 | Loss: 0.00001686
Iteration 36/1000 | Loss: 0.00001685
Iteration 37/1000 | Loss: 0.00001685
Iteration 38/1000 | Loss: 0.00001684
Iteration 39/1000 | Loss: 0.00001683
Iteration 40/1000 | Loss: 0.00001683
Iteration 41/1000 | Loss: 0.00001682
Iteration 42/1000 | Loss: 0.00001682
Iteration 43/1000 | Loss: 0.00001681
Iteration 44/1000 | Loss: 0.00001681
Iteration 45/1000 | Loss: 0.00001681
Iteration 46/1000 | Loss: 0.00001680
Iteration 47/1000 | Loss: 0.00001680
Iteration 48/1000 | Loss: 0.00001680
Iteration 49/1000 | Loss: 0.00001679
Iteration 50/1000 | Loss: 0.00001679
Iteration 51/1000 | Loss: 0.00001679
Iteration 52/1000 | Loss: 0.00001678
Iteration 53/1000 | Loss: 0.00001678
Iteration 54/1000 | Loss: 0.00001678
Iteration 55/1000 | Loss: 0.00001678
Iteration 56/1000 | Loss: 0.00001678
Iteration 57/1000 | Loss: 0.00001678
Iteration 58/1000 | Loss: 0.00001677
Iteration 59/1000 | Loss: 0.00001677
Iteration 60/1000 | Loss: 0.00001677
Iteration 61/1000 | Loss: 0.00001677
Iteration 62/1000 | Loss: 0.00001676
Iteration 63/1000 | Loss: 0.00001676
Iteration 64/1000 | Loss: 0.00001675
Iteration 65/1000 | Loss: 0.00001675
Iteration 66/1000 | Loss: 0.00001675
Iteration 67/1000 | Loss: 0.00001675
Iteration 68/1000 | Loss: 0.00001674
Iteration 69/1000 | Loss: 0.00001674
Iteration 70/1000 | Loss: 0.00001673
Iteration 71/1000 | Loss: 0.00001673
Iteration 72/1000 | Loss: 0.00001673
Iteration 73/1000 | Loss: 0.00001673
Iteration 74/1000 | Loss: 0.00001673
Iteration 75/1000 | Loss: 0.00001673
Iteration 76/1000 | Loss: 0.00001671
Iteration 77/1000 | Loss: 0.00001671
Iteration 78/1000 | Loss: 0.00001671
Iteration 79/1000 | Loss: 0.00001671
Iteration 80/1000 | Loss: 0.00001670
Iteration 81/1000 | Loss: 0.00001670
Iteration 82/1000 | Loss: 0.00001670
Iteration 83/1000 | Loss: 0.00001670
Iteration 84/1000 | Loss: 0.00001670
Iteration 85/1000 | Loss: 0.00001670
Iteration 86/1000 | Loss: 0.00001670
Iteration 87/1000 | Loss: 0.00001670
Iteration 88/1000 | Loss: 0.00001670
Iteration 89/1000 | Loss: 0.00001670
Iteration 90/1000 | Loss: 0.00001670
Iteration 91/1000 | Loss: 0.00001670
Iteration 92/1000 | Loss: 0.00001670
Iteration 93/1000 | Loss: 0.00001670
Iteration 94/1000 | Loss: 0.00001670
Iteration 95/1000 | Loss: 0.00001670
Iteration 96/1000 | Loss: 0.00001670
Iteration 97/1000 | Loss: 0.00001670
Iteration 98/1000 | Loss: 0.00001670
Iteration 99/1000 | Loss: 0.00001670
Iteration 100/1000 | Loss: 0.00001670
Iteration 101/1000 | Loss: 0.00001670
Iteration 102/1000 | Loss: 0.00001670
Iteration 103/1000 | Loss: 0.00001670
Iteration 104/1000 | Loss: 0.00001670
Iteration 105/1000 | Loss: 0.00001670
Iteration 106/1000 | Loss: 0.00001670
Iteration 107/1000 | Loss: 0.00001670
Iteration 108/1000 | Loss: 0.00001670
Iteration 109/1000 | Loss: 0.00001670
Iteration 110/1000 | Loss: 0.00001670
Iteration 111/1000 | Loss: 0.00001670
Iteration 112/1000 | Loss: 0.00001670
Iteration 113/1000 | Loss: 0.00001670
Iteration 114/1000 | Loss: 0.00001670
Iteration 115/1000 | Loss: 0.00001670
Iteration 116/1000 | Loss: 0.00001670
Iteration 117/1000 | Loss: 0.00001670
Iteration 118/1000 | Loss: 0.00001670
Iteration 119/1000 | Loss: 0.00001670
Iteration 120/1000 | Loss: 0.00001670
Iteration 121/1000 | Loss: 0.00001670
Iteration 122/1000 | Loss: 0.00001670
Iteration 123/1000 | Loss: 0.00001670
Iteration 124/1000 | Loss: 0.00001670
Iteration 125/1000 | Loss: 0.00001670
Iteration 126/1000 | Loss: 0.00001670
Iteration 127/1000 | Loss: 0.00001670
Iteration 128/1000 | Loss: 0.00001670
Iteration 129/1000 | Loss: 0.00001670
Iteration 130/1000 | Loss: 0.00001670
Iteration 131/1000 | Loss: 0.00001670
Iteration 132/1000 | Loss: 0.00001670
Iteration 133/1000 | Loss: 0.00001670
Iteration 134/1000 | Loss: 0.00001670
Iteration 135/1000 | Loss: 0.00001670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.6700212654541247e-05, 1.6700212654541247e-05, 1.6700212654541247e-05, 1.6700212654541247e-05, 1.6700212654541247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6700212654541247e-05

Optimization complete. Final v2v error: 3.1129908561706543 mm

Highest mean error: 5.30891752243042 mm for frame 78

Lowest mean error: 2.4196274280548096 mm for frame 117

Saving results

Total time: 33.70520067214966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00888904
Iteration 2/25 | Loss: 0.00154783
Iteration 3/25 | Loss: 0.00119240
Iteration 4/25 | Loss: 0.00116204
Iteration 5/25 | Loss: 0.00115836
Iteration 6/25 | Loss: 0.00115836
Iteration 7/25 | Loss: 0.00115836
Iteration 8/25 | Loss: 0.00115836
Iteration 9/25 | Loss: 0.00115836
Iteration 10/25 | Loss: 0.00115836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001158358296379447, 0.001158358296379447, 0.001158358296379447, 0.001158358296379447, 0.001158358296379447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001158358296379447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.17338295
Iteration 2/25 | Loss: 0.00062259
Iteration 3/25 | Loss: 0.00062257
Iteration 4/25 | Loss: 0.00062257
Iteration 5/25 | Loss: 0.00062257
Iteration 6/25 | Loss: 0.00062257
Iteration 7/25 | Loss: 0.00062257
Iteration 8/25 | Loss: 0.00062257
Iteration 9/25 | Loss: 0.00062257
Iteration 10/25 | Loss: 0.00062257
Iteration 11/25 | Loss: 0.00062257
Iteration 12/25 | Loss: 0.00062257
Iteration 13/25 | Loss: 0.00062257
Iteration 14/25 | Loss: 0.00062257
Iteration 15/25 | Loss: 0.00062257
Iteration 16/25 | Loss: 0.00062257
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006225708639249206, 0.0006225708639249206, 0.0006225708639249206, 0.0006225708639249206, 0.0006225708639249206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006225708639249206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062257
Iteration 2/1000 | Loss: 0.00006109
Iteration 3/1000 | Loss: 0.00003171
Iteration 4/1000 | Loss: 0.00002697
Iteration 5/1000 | Loss: 0.00002533
Iteration 6/1000 | Loss: 0.00002407
Iteration 7/1000 | Loss: 0.00002334
Iteration 8/1000 | Loss: 0.00002279
Iteration 9/1000 | Loss: 0.00002234
Iteration 10/1000 | Loss: 0.00002201
Iteration 11/1000 | Loss: 0.00002174
Iteration 12/1000 | Loss: 0.00002169
Iteration 13/1000 | Loss: 0.00002159
Iteration 14/1000 | Loss: 0.00002158
Iteration 15/1000 | Loss: 0.00002156
Iteration 16/1000 | Loss: 0.00002156
Iteration 17/1000 | Loss: 0.00002154
Iteration 18/1000 | Loss: 0.00002154
Iteration 19/1000 | Loss: 0.00002152
Iteration 20/1000 | Loss: 0.00002152
Iteration 21/1000 | Loss: 0.00002151
Iteration 22/1000 | Loss: 0.00002151
Iteration 23/1000 | Loss: 0.00002151
Iteration 24/1000 | Loss: 0.00002151
Iteration 25/1000 | Loss: 0.00002151
Iteration 26/1000 | Loss: 0.00002150
Iteration 27/1000 | Loss: 0.00002150
Iteration 28/1000 | Loss: 0.00002150
Iteration 29/1000 | Loss: 0.00002149
Iteration 30/1000 | Loss: 0.00002149
Iteration 31/1000 | Loss: 0.00002148
Iteration 32/1000 | Loss: 0.00002148
Iteration 33/1000 | Loss: 0.00002148
Iteration 34/1000 | Loss: 0.00002147
Iteration 35/1000 | Loss: 0.00002147
Iteration 36/1000 | Loss: 0.00002142
Iteration 37/1000 | Loss: 0.00002141
Iteration 38/1000 | Loss: 0.00002141
Iteration 39/1000 | Loss: 0.00002141
Iteration 40/1000 | Loss: 0.00002140
Iteration 41/1000 | Loss: 0.00002140
Iteration 42/1000 | Loss: 0.00002140
Iteration 43/1000 | Loss: 0.00002140
Iteration 44/1000 | Loss: 0.00002140
Iteration 45/1000 | Loss: 0.00002140
Iteration 46/1000 | Loss: 0.00002140
Iteration 47/1000 | Loss: 0.00002139
Iteration 48/1000 | Loss: 0.00002139
Iteration 49/1000 | Loss: 0.00002139
Iteration 50/1000 | Loss: 0.00002139
Iteration 51/1000 | Loss: 0.00002138
Iteration 52/1000 | Loss: 0.00002138
Iteration 53/1000 | Loss: 0.00002137
Iteration 54/1000 | Loss: 0.00002137
Iteration 55/1000 | Loss: 0.00002136
Iteration 56/1000 | Loss: 0.00002136
Iteration 57/1000 | Loss: 0.00002136
Iteration 58/1000 | Loss: 0.00002136
Iteration 59/1000 | Loss: 0.00002136
Iteration 60/1000 | Loss: 0.00002136
Iteration 61/1000 | Loss: 0.00002136
Iteration 62/1000 | Loss: 0.00002136
Iteration 63/1000 | Loss: 0.00002136
Iteration 64/1000 | Loss: 0.00002136
Iteration 65/1000 | Loss: 0.00002136
Iteration 66/1000 | Loss: 0.00002136
Iteration 67/1000 | Loss: 0.00002136
Iteration 68/1000 | Loss: 0.00002136
Iteration 69/1000 | Loss: 0.00002135
Iteration 70/1000 | Loss: 0.00002135
Iteration 71/1000 | Loss: 0.00002135
Iteration 72/1000 | Loss: 0.00002135
Iteration 73/1000 | Loss: 0.00002135
Iteration 74/1000 | Loss: 0.00002135
Iteration 75/1000 | Loss: 0.00002135
Iteration 76/1000 | Loss: 0.00002135
Iteration 77/1000 | Loss: 0.00002135
Iteration 78/1000 | Loss: 0.00002135
Iteration 79/1000 | Loss: 0.00002135
Iteration 80/1000 | Loss: 0.00002135
Iteration 81/1000 | Loss: 0.00002135
Iteration 82/1000 | Loss: 0.00002135
Iteration 83/1000 | Loss: 0.00002134
Iteration 84/1000 | Loss: 0.00002134
Iteration 85/1000 | Loss: 0.00002134
Iteration 86/1000 | Loss: 0.00002134
Iteration 87/1000 | Loss: 0.00002134
Iteration 88/1000 | Loss: 0.00002134
Iteration 89/1000 | Loss: 0.00002134
Iteration 90/1000 | Loss: 0.00002134
Iteration 91/1000 | Loss: 0.00002134
Iteration 92/1000 | Loss: 0.00002134
Iteration 93/1000 | Loss: 0.00002134
Iteration 94/1000 | Loss: 0.00002134
Iteration 95/1000 | Loss: 0.00002134
Iteration 96/1000 | Loss: 0.00002134
Iteration 97/1000 | Loss: 0.00002134
Iteration 98/1000 | Loss: 0.00002134
Iteration 99/1000 | Loss: 0.00002134
Iteration 100/1000 | Loss: 0.00002134
Iteration 101/1000 | Loss: 0.00002134
Iteration 102/1000 | Loss: 0.00002133
Iteration 103/1000 | Loss: 0.00002133
Iteration 104/1000 | Loss: 0.00002133
Iteration 105/1000 | Loss: 0.00002133
Iteration 106/1000 | Loss: 0.00002133
Iteration 107/1000 | Loss: 0.00002133
Iteration 108/1000 | Loss: 0.00002133
Iteration 109/1000 | Loss: 0.00002133
Iteration 110/1000 | Loss: 0.00002133
Iteration 111/1000 | Loss: 0.00002133
Iteration 112/1000 | Loss: 0.00002133
Iteration 113/1000 | Loss: 0.00002133
Iteration 114/1000 | Loss: 0.00002133
Iteration 115/1000 | Loss: 0.00002133
Iteration 116/1000 | Loss: 0.00002133
Iteration 117/1000 | Loss: 0.00002133
Iteration 118/1000 | Loss: 0.00002133
Iteration 119/1000 | Loss: 0.00002133
Iteration 120/1000 | Loss: 0.00002133
Iteration 121/1000 | Loss: 0.00002133
Iteration 122/1000 | Loss: 0.00002133
Iteration 123/1000 | Loss: 0.00002133
Iteration 124/1000 | Loss: 0.00002133
Iteration 125/1000 | Loss: 0.00002133
Iteration 126/1000 | Loss: 0.00002133
Iteration 127/1000 | Loss: 0.00002133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.1328040020307526e-05, 2.1328040020307526e-05, 2.1328040020307526e-05, 2.1328040020307526e-05, 2.1328040020307526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1328040020307526e-05

Optimization complete. Final v2v error: 4.003253936767578 mm

Highest mean error: 4.687062740325928 mm for frame 9

Lowest mean error: 3.4998772144317627 mm for frame 236

Saving results

Total time: 37.034788846969604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053202
Iteration 2/25 | Loss: 0.01053202
Iteration 3/25 | Loss: 0.01053202
Iteration 4/25 | Loss: 0.01053201
Iteration 5/25 | Loss: 0.01053201
Iteration 6/25 | Loss: 0.00260424
Iteration 7/25 | Loss: 0.00204963
Iteration 8/25 | Loss: 0.00160108
Iteration 9/25 | Loss: 0.00126428
Iteration 10/25 | Loss: 0.00119495
Iteration 11/25 | Loss: 0.00117186
Iteration 12/25 | Loss: 0.00111873
Iteration 13/25 | Loss: 0.00108889
Iteration 14/25 | Loss: 0.00106941
Iteration 15/25 | Loss: 0.00106568
Iteration 16/25 | Loss: 0.00107244
Iteration 17/25 | Loss: 0.00106066
Iteration 18/25 | Loss: 0.00105501
Iteration 19/25 | Loss: 0.00105673
Iteration 20/25 | Loss: 0.00105563
Iteration 21/25 | Loss: 0.00105546
Iteration 22/25 | Loss: 0.00105022
Iteration 23/25 | Loss: 0.00104794
Iteration 24/25 | Loss: 0.00104713
Iteration 25/25 | Loss: 0.00104695

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43032134
Iteration 2/25 | Loss: 0.00037317
Iteration 3/25 | Loss: 0.00037317
Iteration 4/25 | Loss: 0.00037317
Iteration 5/25 | Loss: 0.00037317
Iteration 6/25 | Loss: 0.00037317
Iteration 7/25 | Loss: 0.00037317
Iteration 8/25 | Loss: 0.00037317
Iteration 9/25 | Loss: 0.00037317
Iteration 10/25 | Loss: 0.00037317
Iteration 11/25 | Loss: 0.00037317
Iteration 12/25 | Loss: 0.00037317
Iteration 13/25 | Loss: 0.00037317
Iteration 14/25 | Loss: 0.00037317
Iteration 15/25 | Loss: 0.00037317
Iteration 16/25 | Loss: 0.00037317
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0003731675387825817, 0.0003731675387825817, 0.0003731675387825817, 0.0003731675387825817, 0.0003731675387825817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003731675387825817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037317
Iteration 2/1000 | Loss: 0.00004150
Iteration 3/1000 | Loss: 0.00002782
Iteration 4/1000 | Loss: 0.00003695
Iteration 5/1000 | Loss: 0.00003260
Iteration 6/1000 | Loss: 0.00002517
Iteration 7/1000 | Loss: 0.00002182
Iteration 8/1000 | Loss: 0.00021425
Iteration 9/1000 | Loss: 0.00013797
Iteration 10/1000 | Loss: 0.00004209
Iteration 11/1000 | Loss: 0.00002518
Iteration 12/1000 | Loss: 0.00002141
Iteration 13/1000 | Loss: 0.00002012
Iteration 14/1000 | Loss: 0.00001893
Iteration 15/1000 | Loss: 0.00007743
Iteration 16/1000 | Loss: 0.00003657
Iteration 17/1000 | Loss: 0.00002037
Iteration 18/1000 | Loss: 0.00001930
Iteration 19/1000 | Loss: 0.00019269
Iteration 20/1000 | Loss: 0.00002316
Iteration 21/1000 | Loss: 0.00001938
Iteration 22/1000 | Loss: 0.00001778
Iteration 23/1000 | Loss: 0.00001668
Iteration 24/1000 | Loss: 0.00001605
Iteration 25/1000 | Loss: 0.00001586
Iteration 26/1000 | Loss: 0.00001586
Iteration 27/1000 | Loss: 0.00001580
Iteration 28/1000 | Loss: 0.00001580
Iteration 29/1000 | Loss: 0.00001580
Iteration 30/1000 | Loss: 0.00001580
Iteration 31/1000 | Loss: 0.00001579
Iteration 32/1000 | Loss: 0.00001577
Iteration 33/1000 | Loss: 0.00001577
Iteration 34/1000 | Loss: 0.00001575
Iteration 35/1000 | Loss: 0.00001574
Iteration 36/1000 | Loss: 0.00001573
Iteration 37/1000 | Loss: 0.00001572
Iteration 38/1000 | Loss: 0.00001567
Iteration 39/1000 | Loss: 0.00001557
Iteration 40/1000 | Loss: 0.00001552
Iteration 41/1000 | Loss: 0.00001552
Iteration 42/1000 | Loss: 0.00001552
Iteration 43/1000 | Loss: 0.00001552
Iteration 44/1000 | Loss: 0.00001552
Iteration 45/1000 | Loss: 0.00001552
Iteration 46/1000 | Loss: 0.00001552
Iteration 47/1000 | Loss: 0.00001551
Iteration 48/1000 | Loss: 0.00001551
Iteration 49/1000 | Loss: 0.00001551
Iteration 50/1000 | Loss: 0.00001551
Iteration 51/1000 | Loss: 0.00001549
Iteration 52/1000 | Loss: 0.00001549
Iteration 53/1000 | Loss: 0.00001549
Iteration 54/1000 | Loss: 0.00001548
Iteration 55/1000 | Loss: 0.00001548
Iteration 56/1000 | Loss: 0.00001548
Iteration 57/1000 | Loss: 0.00001548
Iteration 58/1000 | Loss: 0.00001548
Iteration 59/1000 | Loss: 0.00001548
Iteration 60/1000 | Loss: 0.00001548
Iteration 61/1000 | Loss: 0.00001548
Iteration 62/1000 | Loss: 0.00001548
Iteration 63/1000 | Loss: 0.00001548
Iteration 64/1000 | Loss: 0.00001548
Iteration 65/1000 | Loss: 0.00001547
Iteration 66/1000 | Loss: 0.00001547
Iteration 67/1000 | Loss: 0.00001547
Iteration 68/1000 | Loss: 0.00001547
Iteration 69/1000 | Loss: 0.00001547
Iteration 70/1000 | Loss: 0.00001547
Iteration 71/1000 | Loss: 0.00001546
Iteration 72/1000 | Loss: 0.00001546
Iteration 73/1000 | Loss: 0.00001546
Iteration 74/1000 | Loss: 0.00001546
Iteration 75/1000 | Loss: 0.00001546
Iteration 76/1000 | Loss: 0.00001546
Iteration 77/1000 | Loss: 0.00001546
Iteration 78/1000 | Loss: 0.00001546
Iteration 79/1000 | Loss: 0.00001546
Iteration 80/1000 | Loss: 0.00001546
Iteration 81/1000 | Loss: 0.00001546
Iteration 82/1000 | Loss: 0.00001546
Iteration 83/1000 | Loss: 0.00001546
Iteration 84/1000 | Loss: 0.00001545
Iteration 85/1000 | Loss: 0.00001545
Iteration 86/1000 | Loss: 0.00001545
Iteration 87/1000 | Loss: 0.00001545
Iteration 88/1000 | Loss: 0.00001545
Iteration 89/1000 | Loss: 0.00001545
Iteration 90/1000 | Loss: 0.00001545
Iteration 91/1000 | Loss: 0.00001545
Iteration 92/1000 | Loss: 0.00001545
Iteration 93/1000 | Loss: 0.00001545
Iteration 94/1000 | Loss: 0.00001545
Iteration 95/1000 | Loss: 0.00001545
Iteration 96/1000 | Loss: 0.00001545
Iteration 97/1000 | Loss: 0.00001545
Iteration 98/1000 | Loss: 0.00001545
Iteration 99/1000 | Loss: 0.00001545
Iteration 100/1000 | Loss: 0.00001544
Iteration 101/1000 | Loss: 0.00001544
Iteration 102/1000 | Loss: 0.00001544
Iteration 103/1000 | Loss: 0.00001544
Iteration 104/1000 | Loss: 0.00001544
Iteration 105/1000 | Loss: 0.00001544
Iteration 106/1000 | Loss: 0.00001544
Iteration 107/1000 | Loss: 0.00001544
Iteration 108/1000 | Loss: 0.00001544
Iteration 109/1000 | Loss: 0.00001544
Iteration 110/1000 | Loss: 0.00001544
Iteration 111/1000 | Loss: 0.00001544
Iteration 112/1000 | Loss: 0.00001544
Iteration 113/1000 | Loss: 0.00001544
Iteration 114/1000 | Loss: 0.00001543
Iteration 115/1000 | Loss: 0.00001543
Iteration 116/1000 | Loss: 0.00001543
Iteration 117/1000 | Loss: 0.00001543
Iteration 118/1000 | Loss: 0.00001543
Iteration 119/1000 | Loss: 0.00001543
Iteration 120/1000 | Loss: 0.00001543
Iteration 121/1000 | Loss: 0.00001543
Iteration 122/1000 | Loss: 0.00001543
Iteration 123/1000 | Loss: 0.00001543
Iteration 124/1000 | Loss: 0.00001543
Iteration 125/1000 | Loss: 0.00001543
Iteration 126/1000 | Loss: 0.00001543
Iteration 127/1000 | Loss: 0.00001543
Iteration 128/1000 | Loss: 0.00001543
Iteration 129/1000 | Loss: 0.00001543
Iteration 130/1000 | Loss: 0.00001543
Iteration 131/1000 | Loss: 0.00001543
Iteration 132/1000 | Loss: 0.00001543
Iteration 133/1000 | Loss: 0.00001543
Iteration 134/1000 | Loss: 0.00001543
Iteration 135/1000 | Loss: 0.00001543
Iteration 136/1000 | Loss: 0.00001543
Iteration 137/1000 | Loss: 0.00001543
Iteration 138/1000 | Loss: 0.00001543
Iteration 139/1000 | Loss: 0.00001543
Iteration 140/1000 | Loss: 0.00001543
Iteration 141/1000 | Loss: 0.00001543
Iteration 142/1000 | Loss: 0.00001543
Iteration 143/1000 | Loss: 0.00001543
Iteration 144/1000 | Loss: 0.00001543
Iteration 145/1000 | Loss: 0.00001543
Iteration 146/1000 | Loss: 0.00001543
Iteration 147/1000 | Loss: 0.00001543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.542921927466523e-05, 1.542921927466523e-05, 1.542921927466523e-05, 1.542921927466523e-05, 1.542921927466523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.542921927466523e-05

Optimization complete. Final v2v error: 3.2928035259246826 mm

Highest mean error: 3.948007345199585 mm for frame 26

Lowest mean error: 2.9320666790008545 mm for frame 62

Saving results

Total time: 95.66911602020264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481228
Iteration 2/25 | Loss: 0.00137191
Iteration 3/25 | Loss: 0.00108305
Iteration 4/25 | Loss: 0.00105017
Iteration 5/25 | Loss: 0.00104708
Iteration 6/25 | Loss: 0.00104663
Iteration 7/25 | Loss: 0.00104663
Iteration 8/25 | Loss: 0.00104663
Iteration 9/25 | Loss: 0.00104663
Iteration 10/25 | Loss: 0.00104663
Iteration 11/25 | Loss: 0.00104663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010466263629496098, 0.0010466263629496098, 0.0010466263629496098, 0.0010466263629496098, 0.0010466263629496098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010466263629496098

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44785285
Iteration 2/25 | Loss: 0.00042122
Iteration 3/25 | Loss: 0.00042121
Iteration 4/25 | Loss: 0.00042120
Iteration 5/25 | Loss: 0.00042120
Iteration 6/25 | Loss: 0.00042120
Iteration 7/25 | Loss: 0.00042120
Iteration 8/25 | Loss: 0.00042120
Iteration 9/25 | Loss: 0.00042120
Iteration 10/25 | Loss: 0.00042120
Iteration 11/25 | Loss: 0.00042120
Iteration 12/25 | Loss: 0.00042120
Iteration 13/25 | Loss: 0.00042120
Iteration 14/25 | Loss: 0.00042120
Iteration 15/25 | Loss: 0.00042120
Iteration 16/25 | Loss: 0.00042120
Iteration 17/25 | Loss: 0.00042120
Iteration 18/25 | Loss: 0.00042120
Iteration 19/25 | Loss: 0.00042120
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004212024505250156, 0.0004212024505250156, 0.0004212024505250156, 0.0004212024505250156, 0.0004212024505250156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004212024505250156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042120
Iteration 2/1000 | Loss: 0.00004679
Iteration 3/1000 | Loss: 0.00002036
Iteration 4/1000 | Loss: 0.00001845
Iteration 5/1000 | Loss: 0.00001768
Iteration 6/1000 | Loss: 0.00001727
Iteration 7/1000 | Loss: 0.00001700
Iteration 8/1000 | Loss: 0.00001678
Iteration 9/1000 | Loss: 0.00001656
Iteration 10/1000 | Loss: 0.00001656
Iteration 11/1000 | Loss: 0.00001642
Iteration 12/1000 | Loss: 0.00001640
Iteration 13/1000 | Loss: 0.00001638
Iteration 14/1000 | Loss: 0.00001636
Iteration 15/1000 | Loss: 0.00001635
Iteration 16/1000 | Loss: 0.00001635
Iteration 17/1000 | Loss: 0.00001634
Iteration 18/1000 | Loss: 0.00001634
Iteration 19/1000 | Loss: 0.00001631
Iteration 20/1000 | Loss: 0.00001631
Iteration 21/1000 | Loss: 0.00001630
Iteration 22/1000 | Loss: 0.00001630
Iteration 23/1000 | Loss: 0.00001629
Iteration 24/1000 | Loss: 0.00001629
Iteration 25/1000 | Loss: 0.00001628
Iteration 26/1000 | Loss: 0.00001628
Iteration 27/1000 | Loss: 0.00001626
Iteration 28/1000 | Loss: 0.00001626
Iteration 29/1000 | Loss: 0.00001626
Iteration 30/1000 | Loss: 0.00001626
Iteration 31/1000 | Loss: 0.00001626
Iteration 32/1000 | Loss: 0.00001626
Iteration 33/1000 | Loss: 0.00001626
Iteration 34/1000 | Loss: 0.00001626
Iteration 35/1000 | Loss: 0.00001626
Iteration 36/1000 | Loss: 0.00001626
Iteration 37/1000 | Loss: 0.00001626
Iteration 38/1000 | Loss: 0.00001626
Iteration 39/1000 | Loss: 0.00001625
Iteration 40/1000 | Loss: 0.00001625
Iteration 41/1000 | Loss: 0.00001624
Iteration 42/1000 | Loss: 0.00001624
Iteration 43/1000 | Loss: 0.00001624
Iteration 44/1000 | Loss: 0.00001623
Iteration 45/1000 | Loss: 0.00001623
Iteration 46/1000 | Loss: 0.00001623
Iteration 47/1000 | Loss: 0.00001623
Iteration 48/1000 | Loss: 0.00001623
Iteration 49/1000 | Loss: 0.00001623
Iteration 50/1000 | Loss: 0.00001623
Iteration 51/1000 | Loss: 0.00001622
Iteration 52/1000 | Loss: 0.00001622
Iteration 53/1000 | Loss: 0.00001622
Iteration 54/1000 | Loss: 0.00001622
Iteration 55/1000 | Loss: 0.00001622
Iteration 56/1000 | Loss: 0.00001622
Iteration 57/1000 | Loss: 0.00001622
Iteration 58/1000 | Loss: 0.00001622
Iteration 59/1000 | Loss: 0.00001622
Iteration 60/1000 | Loss: 0.00001621
Iteration 61/1000 | Loss: 0.00001621
Iteration 62/1000 | Loss: 0.00001621
Iteration 63/1000 | Loss: 0.00001620
Iteration 64/1000 | Loss: 0.00001620
Iteration 65/1000 | Loss: 0.00001620
Iteration 66/1000 | Loss: 0.00001619
Iteration 67/1000 | Loss: 0.00001619
Iteration 68/1000 | Loss: 0.00001619
Iteration 69/1000 | Loss: 0.00001619
Iteration 70/1000 | Loss: 0.00001619
Iteration 71/1000 | Loss: 0.00001619
Iteration 72/1000 | Loss: 0.00001619
Iteration 73/1000 | Loss: 0.00001618
Iteration 74/1000 | Loss: 0.00001618
Iteration 75/1000 | Loss: 0.00001618
Iteration 76/1000 | Loss: 0.00001618
Iteration 77/1000 | Loss: 0.00001618
Iteration 78/1000 | Loss: 0.00001617
Iteration 79/1000 | Loss: 0.00001617
Iteration 80/1000 | Loss: 0.00001617
Iteration 81/1000 | Loss: 0.00001616
Iteration 82/1000 | Loss: 0.00001616
Iteration 83/1000 | Loss: 0.00001616
Iteration 84/1000 | Loss: 0.00001616
Iteration 85/1000 | Loss: 0.00001616
Iteration 86/1000 | Loss: 0.00001616
Iteration 87/1000 | Loss: 0.00001616
Iteration 88/1000 | Loss: 0.00001616
Iteration 89/1000 | Loss: 0.00001615
Iteration 90/1000 | Loss: 0.00001615
Iteration 91/1000 | Loss: 0.00001615
Iteration 92/1000 | Loss: 0.00001615
Iteration 93/1000 | Loss: 0.00001615
Iteration 94/1000 | Loss: 0.00001615
Iteration 95/1000 | Loss: 0.00001615
Iteration 96/1000 | Loss: 0.00001615
Iteration 97/1000 | Loss: 0.00001615
Iteration 98/1000 | Loss: 0.00001615
Iteration 99/1000 | Loss: 0.00001615
Iteration 100/1000 | Loss: 0.00001614
Iteration 101/1000 | Loss: 0.00001614
Iteration 102/1000 | Loss: 0.00001614
Iteration 103/1000 | Loss: 0.00001614
Iteration 104/1000 | Loss: 0.00001614
Iteration 105/1000 | Loss: 0.00001614
Iteration 106/1000 | Loss: 0.00001614
Iteration 107/1000 | Loss: 0.00001614
Iteration 108/1000 | Loss: 0.00001614
Iteration 109/1000 | Loss: 0.00001614
Iteration 110/1000 | Loss: 0.00001614
Iteration 111/1000 | Loss: 0.00001614
Iteration 112/1000 | Loss: 0.00001614
Iteration 113/1000 | Loss: 0.00001614
Iteration 114/1000 | Loss: 0.00001614
Iteration 115/1000 | Loss: 0.00001614
Iteration 116/1000 | Loss: 0.00001614
Iteration 117/1000 | Loss: 0.00001614
Iteration 118/1000 | Loss: 0.00001614
Iteration 119/1000 | Loss: 0.00001614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.6142863387358375e-05, 1.6142863387358375e-05, 1.6142863387358375e-05, 1.6142863387358375e-05, 1.6142863387358375e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6142863387358375e-05

Optimization complete. Final v2v error: 3.368604898452759 mm

Highest mean error: 3.7334110736846924 mm for frame 58

Lowest mean error: 2.7612321376800537 mm for frame 239

Saving results

Total time: 33.683016300201416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2326/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2326/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00698984
Iteration 2/25 | Loss: 0.00132585
Iteration 3/25 | Loss: 0.00107739
Iteration 4/25 | Loss: 0.00105104
Iteration 5/25 | Loss: 0.00104880
Iteration 6/25 | Loss: 0.00104866
Iteration 7/25 | Loss: 0.00104866
Iteration 8/25 | Loss: 0.00104866
Iteration 9/25 | Loss: 0.00104866
Iteration 10/25 | Loss: 0.00104866
Iteration 11/25 | Loss: 0.00104866
Iteration 12/25 | Loss: 0.00104866
Iteration 13/25 | Loss: 0.00104866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010486604878678918, 0.0010486604878678918, 0.0010486604878678918, 0.0010486604878678918, 0.0010486604878678918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010486604878678918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38064981
Iteration 2/25 | Loss: 0.00042065
Iteration 3/25 | Loss: 0.00042065
Iteration 4/25 | Loss: 0.00042065
Iteration 5/25 | Loss: 0.00042065
Iteration 6/25 | Loss: 0.00042065
Iteration 7/25 | Loss: 0.00042065
Iteration 8/25 | Loss: 0.00042065
Iteration 9/25 | Loss: 0.00042065
Iteration 10/25 | Loss: 0.00042065
Iteration 11/25 | Loss: 0.00042065
Iteration 12/25 | Loss: 0.00042065
Iteration 13/25 | Loss: 0.00042065
Iteration 14/25 | Loss: 0.00042065
Iteration 15/25 | Loss: 0.00042065
Iteration 16/25 | Loss: 0.00042065
Iteration 17/25 | Loss: 0.00042065
Iteration 18/25 | Loss: 0.00042065
Iteration 19/25 | Loss: 0.00042065
Iteration 20/25 | Loss: 0.00042065
Iteration 21/25 | Loss: 0.00042065
Iteration 22/25 | Loss: 0.00042065
Iteration 23/25 | Loss: 0.00042065
Iteration 24/25 | Loss: 0.00042065
Iteration 25/25 | Loss: 0.00042065

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042065
Iteration 2/1000 | Loss: 0.00004098
Iteration 3/1000 | Loss: 0.00002209
Iteration 4/1000 | Loss: 0.00001682
Iteration 5/1000 | Loss: 0.00001550
Iteration 6/1000 | Loss: 0.00001468
Iteration 7/1000 | Loss: 0.00001432
Iteration 8/1000 | Loss: 0.00001403
Iteration 9/1000 | Loss: 0.00001387
Iteration 10/1000 | Loss: 0.00001369
Iteration 11/1000 | Loss: 0.00001365
Iteration 12/1000 | Loss: 0.00001363
Iteration 13/1000 | Loss: 0.00001362
Iteration 14/1000 | Loss: 0.00001362
Iteration 15/1000 | Loss: 0.00001361
Iteration 16/1000 | Loss: 0.00001359
Iteration 17/1000 | Loss: 0.00001358
Iteration 18/1000 | Loss: 0.00001358
Iteration 19/1000 | Loss: 0.00001357
Iteration 20/1000 | Loss: 0.00001357
Iteration 21/1000 | Loss: 0.00001357
Iteration 22/1000 | Loss: 0.00001356
Iteration 23/1000 | Loss: 0.00001356
Iteration 24/1000 | Loss: 0.00001356
Iteration 25/1000 | Loss: 0.00001356
Iteration 26/1000 | Loss: 0.00001356
Iteration 27/1000 | Loss: 0.00001355
Iteration 28/1000 | Loss: 0.00001355
Iteration 29/1000 | Loss: 0.00001355
Iteration 30/1000 | Loss: 0.00001355
Iteration 31/1000 | Loss: 0.00001354
Iteration 32/1000 | Loss: 0.00001353
Iteration 33/1000 | Loss: 0.00001352
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001351
Iteration 36/1000 | Loss: 0.00001351
Iteration 37/1000 | Loss: 0.00001350
Iteration 38/1000 | Loss: 0.00001349
Iteration 39/1000 | Loss: 0.00001349
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001342
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001341
Iteration 46/1000 | Loss: 0.00001341
Iteration 47/1000 | Loss: 0.00001341
Iteration 48/1000 | Loss: 0.00001341
Iteration 49/1000 | Loss: 0.00001341
Iteration 50/1000 | Loss: 0.00001341
Iteration 51/1000 | Loss: 0.00001340
Iteration 52/1000 | Loss: 0.00001338
Iteration 53/1000 | Loss: 0.00001338
Iteration 54/1000 | Loss: 0.00001338
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001338
Iteration 57/1000 | Loss: 0.00001338
Iteration 58/1000 | Loss: 0.00001338
Iteration 59/1000 | Loss: 0.00001338
Iteration 60/1000 | Loss: 0.00001338
Iteration 61/1000 | Loss: 0.00001337
Iteration 62/1000 | Loss: 0.00001337
Iteration 63/1000 | Loss: 0.00001337
Iteration 64/1000 | Loss: 0.00001336
Iteration 65/1000 | Loss: 0.00001336
Iteration 66/1000 | Loss: 0.00001335
Iteration 67/1000 | Loss: 0.00001335
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001334
Iteration 70/1000 | Loss: 0.00001334
Iteration 71/1000 | Loss: 0.00001333
Iteration 72/1000 | Loss: 0.00001333
Iteration 73/1000 | Loss: 0.00001333
Iteration 74/1000 | Loss: 0.00001333
Iteration 75/1000 | Loss: 0.00001332
Iteration 76/1000 | Loss: 0.00001332
Iteration 77/1000 | Loss: 0.00001332
Iteration 78/1000 | Loss: 0.00001332
Iteration 79/1000 | Loss: 0.00001331
Iteration 80/1000 | Loss: 0.00001331
Iteration 81/1000 | Loss: 0.00001331
Iteration 82/1000 | Loss: 0.00001331
Iteration 83/1000 | Loss: 0.00001330
Iteration 84/1000 | Loss: 0.00001330
Iteration 85/1000 | Loss: 0.00001330
Iteration 86/1000 | Loss: 0.00001330
Iteration 87/1000 | Loss: 0.00001330
Iteration 88/1000 | Loss: 0.00001330
Iteration 89/1000 | Loss: 0.00001330
Iteration 90/1000 | Loss: 0.00001330
Iteration 91/1000 | Loss: 0.00001329
Iteration 92/1000 | Loss: 0.00001329
Iteration 93/1000 | Loss: 0.00001329
Iteration 94/1000 | Loss: 0.00001329
Iteration 95/1000 | Loss: 0.00001329
Iteration 96/1000 | Loss: 0.00001329
Iteration 97/1000 | Loss: 0.00001329
Iteration 98/1000 | Loss: 0.00001329
Iteration 99/1000 | Loss: 0.00001329
Iteration 100/1000 | Loss: 0.00001329
Iteration 101/1000 | Loss: 0.00001329
Iteration 102/1000 | Loss: 0.00001329
Iteration 103/1000 | Loss: 0.00001329
Iteration 104/1000 | Loss: 0.00001328
Iteration 105/1000 | Loss: 0.00001328
Iteration 106/1000 | Loss: 0.00001328
Iteration 107/1000 | Loss: 0.00001328
Iteration 108/1000 | Loss: 0.00001328
Iteration 109/1000 | Loss: 0.00001328
Iteration 110/1000 | Loss: 0.00001328
Iteration 111/1000 | Loss: 0.00001328
Iteration 112/1000 | Loss: 0.00001328
Iteration 113/1000 | Loss: 0.00001328
Iteration 114/1000 | Loss: 0.00001328
Iteration 115/1000 | Loss: 0.00001328
Iteration 116/1000 | Loss: 0.00001328
Iteration 117/1000 | Loss: 0.00001328
Iteration 118/1000 | Loss: 0.00001328
Iteration 119/1000 | Loss: 0.00001328
Iteration 120/1000 | Loss: 0.00001328
Iteration 121/1000 | Loss: 0.00001328
Iteration 122/1000 | Loss: 0.00001328
Iteration 123/1000 | Loss: 0.00001328
Iteration 124/1000 | Loss: 0.00001328
Iteration 125/1000 | Loss: 0.00001328
Iteration 126/1000 | Loss: 0.00001328
Iteration 127/1000 | Loss: 0.00001328
Iteration 128/1000 | Loss: 0.00001328
Iteration 129/1000 | Loss: 0.00001328
Iteration 130/1000 | Loss: 0.00001328
Iteration 131/1000 | Loss: 0.00001328
Iteration 132/1000 | Loss: 0.00001328
Iteration 133/1000 | Loss: 0.00001328
Iteration 134/1000 | Loss: 0.00001328
Iteration 135/1000 | Loss: 0.00001328
Iteration 136/1000 | Loss: 0.00001328
Iteration 137/1000 | Loss: 0.00001328
Iteration 138/1000 | Loss: 0.00001328
Iteration 139/1000 | Loss: 0.00001328
Iteration 140/1000 | Loss: 0.00001328
Iteration 141/1000 | Loss: 0.00001328
Iteration 142/1000 | Loss: 0.00001328
Iteration 143/1000 | Loss: 0.00001328
Iteration 144/1000 | Loss: 0.00001328
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.3278065125632565e-05, 1.3278065125632565e-05, 1.3278065125632565e-05, 1.3278065125632565e-05, 1.3278065125632565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3278065125632565e-05

Optimization complete. Final v2v error: 3.0686752796173096 mm

Highest mean error: 3.4549572467803955 mm for frame 161

Lowest mean error: 2.6786625385284424 mm for frame 54

Saving results

Total time: 31.62770438194275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015513
Iteration 2/25 | Loss: 0.01015513
Iteration 3/25 | Loss: 0.01015512
Iteration 4/25 | Loss: 0.00318078
Iteration 5/25 | Loss: 0.00185160
Iteration 6/25 | Loss: 0.00156596
Iteration 7/25 | Loss: 0.00146904
Iteration 8/25 | Loss: 0.00138522
Iteration 9/25 | Loss: 0.00135759
Iteration 10/25 | Loss: 0.00136213
Iteration 11/25 | Loss: 0.00128743
Iteration 12/25 | Loss: 0.00120484
Iteration 13/25 | Loss: 0.00119299
Iteration 14/25 | Loss: 0.00118967
Iteration 15/25 | Loss: 0.00118744
Iteration 16/25 | Loss: 0.00118585
Iteration 17/25 | Loss: 0.00118915
Iteration 18/25 | Loss: 0.00119213
Iteration 19/25 | Loss: 0.00119204
Iteration 20/25 | Loss: 0.00118538
Iteration 21/25 | Loss: 0.00117994
Iteration 22/25 | Loss: 0.00117879
Iteration 23/25 | Loss: 0.00117845
Iteration 24/25 | Loss: 0.00118170
Iteration 25/25 | Loss: 0.00118169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36998451
Iteration 2/25 | Loss: 0.00088461
Iteration 3/25 | Loss: 0.00087792
Iteration 4/25 | Loss: 0.00087792
Iteration 5/25 | Loss: 0.00087792
Iteration 6/25 | Loss: 0.00087791
Iteration 7/25 | Loss: 0.00087791
Iteration 8/25 | Loss: 0.00087791
Iteration 9/25 | Loss: 0.00087791
Iteration 10/25 | Loss: 0.00087791
Iteration 11/25 | Loss: 0.00087791
Iteration 12/25 | Loss: 0.00087791
Iteration 13/25 | Loss: 0.00087791
Iteration 14/25 | Loss: 0.00087791
Iteration 15/25 | Loss: 0.00087791
Iteration 16/25 | Loss: 0.00087791
Iteration 17/25 | Loss: 0.00087791
Iteration 18/25 | Loss: 0.00087791
Iteration 19/25 | Loss: 0.00087791
Iteration 20/25 | Loss: 0.00087791
Iteration 21/25 | Loss: 0.00087791
Iteration 22/25 | Loss: 0.00087791
Iteration 23/25 | Loss: 0.00087791
Iteration 24/25 | Loss: 0.00087791
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008779121562838554, 0.0008779121562838554, 0.0008779121562838554, 0.0008779121562838554, 0.0008779121562838554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008779121562838554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087791
Iteration 2/1000 | Loss: 0.00009732
Iteration 3/1000 | Loss: 0.00003742
Iteration 4/1000 | Loss: 0.00010333
Iteration 5/1000 | Loss: 0.00003214
Iteration 6/1000 | Loss: 0.00002905
Iteration 7/1000 | Loss: 0.00002687
Iteration 8/1000 | Loss: 0.00016403
Iteration 9/1000 | Loss: 0.00007479
Iteration 10/1000 | Loss: 0.00006107
Iteration 11/1000 | Loss: 0.00015717
Iteration 12/1000 | Loss: 0.00003004
Iteration 13/1000 | Loss: 0.00002675
Iteration 14/1000 | Loss: 0.00002491
Iteration 15/1000 | Loss: 0.00002310
Iteration 16/1000 | Loss: 0.00002220
Iteration 17/1000 | Loss: 0.00002760
Iteration 18/1000 | Loss: 0.00002428
Iteration 19/1000 | Loss: 0.00002425
Iteration 20/1000 | Loss: 0.00002128
Iteration 21/1000 | Loss: 0.00002060
Iteration 22/1000 | Loss: 0.00002020
Iteration 23/1000 | Loss: 0.00001996
Iteration 24/1000 | Loss: 0.00001994
Iteration 25/1000 | Loss: 0.00001984
Iteration 26/1000 | Loss: 0.00001964
Iteration 27/1000 | Loss: 0.00001955
Iteration 28/1000 | Loss: 0.00001954
Iteration 29/1000 | Loss: 0.00001954
Iteration 30/1000 | Loss: 0.00001954
Iteration 31/1000 | Loss: 0.00001954
Iteration 32/1000 | Loss: 0.00001954
Iteration 33/1000 | Loss: 0.00001954
Iteration 34/1000 | Loss: 0.00001954
Iteration 35/1000 | Loss: 0.00001954
Iteration 36/1000 | Loss: 0.00001954
Iteration 37/1000 | Loss: 0.00001953
Iteration 38/1000 | Loss: 0.00001953
Iteration 39/1000 | Loss: 0.00001952
Iteration 40/1000 | Loss: 0.00001952
Iteration 41/1000 | Loss: 0.00001951
Iteration 42/1000 | Loss: 0.00001951
Iteration 43/1000 | Loss: 0.00001951
Iteration 44/1000 | Loss: 0.00001950
Iteration 45/1000 | Loss: 0.00001950
Iteration 46/1000 | Loss: 0.00001950
Iteration 47/1000 | Loss: 0.00001950
Iteration 48/1000 | Loss: 0.00001949
Iteration 49/1000 | Loss: 0.00001949
Iteration 50/1000 | Loss: 0.00001949
Iteration 51/1000 | Loss: 0.00001949
Iteration 52/1000 | Loss: 0.00001949
Iteration 53/1000 | Loss: 0.00001949
Iteration 54/1000 | Loss: 0.00001949
Iteration 55/1000 | Loss: 0.00001949
Iteration 56/1000 | Loss: 0.00001949
Iteration 57/1000 | Loss: 0.00001948
Iteration 58/1000 | Loss: 0.00001948
Iteration 59/1000 | Loss: 0.00001948
Iteration 60/1000 | Loss: 0.00001948
Iteration 61/1000 | Loss: 0.00001948
Iteration 62/1000 | Loss: 0.00001948
Iteration 63/1000 | Loss: 0.00001947
Iteration 64/1000 | Loss: 0.00001947
Iteration 65/1000 | Loss: 0.00001947
Iteration 66/1000 | Loss: 0.00001947
Iteration 67/1000 | Loss: 0.00001947
Iteration 68/1000 | Loss: 0.00001947
Iteration 69/1000 | Loss: 0.00001947
Iteration 70/1000 | Loss: 0.00001947
Iteration 71/1000 | Loss: 0.00001947
Iteration 72/1000 | Loss: 0.00001946
Iteration 73/1000 | Loss: 0.00001946
Iteration 74/1000 | Loss: 0.00001946
Iteration 75/1000 | Loss: 0.00001946
Iteration 76/1000 | Loss: 0.00001946
Iteration 77/1000 | Loss: 0.00001946
Iteration 78/1000 | Loss: 0.00001946
Iteration 79/1000 | Loss: 0.00001946
Iteration 80/1000 | Loss: 0.00001946
Iteration 81/1000 | Loss: 0.00001945
Iteration 82/1000 | Loss: 0.00001945
Iteration 83/1000 | Loss: 0.00001945
Iteration 84/1000 | Loss: 0.00001945
Iteration 85/1000 | Loss: 0.00001945
Iteration 86/1000 | Loss: 0.00001945
Iteration 87/1000 | Loss: 0.00001945
Iteration 88/1000 | Loss: 0.00001944
Iteration 89/1000 | Loss: 0.00001944
Iteration 90/1000 | Loss: 0.00001943
Iteration 91/1000 | Loss: 0.00001943
Iteration 92/1000 | Loss: 0.00001942
Iteration 93/1000 | Loss: 0.00001942
Iteration 94/1000 | Loss: 0.00001942
Iteration 95/1000 | Loss: 0.00001942
Iteration 96/1000 | Loss: 0.00001942
Iteration 97/1000 | Loss: 0.00001942
Iteration 98/1000 | Loss: 0.00001942
Iteration 99/1000 | Loss: 0.00001942
Iteration 100/1000 | Loss: 0.00001942
Iteration 101/1000 | Loss: 0.00001941
Iteration 102/1000 | Loss: 0.00001941
Iteration 103/1000 | Loss: 0.00001941
Iteration 104/1000 | Loss: 0.00001941
Iteration 105/1000 | Loss: 0.00001941
Iteration 106/1000 | Loss: 0.00001941
Iteration 107/1000 | Loss: 0.00001941
Iteration 108/1000 | Loss: 0.00001941
Iteration 109/1000 | Loss: 0.00001941
Iteration 110/1000 | Loss: 0.00001941
Iteration 111/1000 | Loss: 0.00001941
Iteration 112/1000 | Loss: 0.00001940
Iteration 113/1000 | Loss: 0.00001940
Iteration 114/1000 | Loss: 0.00001940
Iteration 115/1000 | Loss: 0.00001940
Iteration 116/1000 | Loss: 0.00001940
Iteration 117/1000 | Loss: 0.00001940
Iteration 118/1000 | Loss: 0.00001940
Iteration 119/1000 | Loss: 0.00001940
Iteration 120/1000 | Loss: 0.00001939
Iteration 121/1000 | Loss: 0.00001939
Iteration 122/1000 | Loss: 0.00001939
Iteration 123/1000 | Loss: 0.00001939
Iteration 124/1000 | Loss: 0.00001939
Iteration 125/1000 | Loss: 0.00001939
Iteration 126/1000 | Loss: 0.00001938
Iteration 127/1000 | Loss: 0.00001938
Iteration 128/1000 | Loss: 0.00001938
Iteration 129/1000 | Loss: 0.00001937
Iteration 130/1000 | Loss: 0.00001937
Iteration 131/1000 | Loss: 0.00001937
Iteration 132/1000 | Loss: 0.00001937
Iteration 133/1000 | Loss: 0.00001937
Iteration 134/1000 | Loss: 0.00001937
Iteration 135/1000 | Loss: 0.00001937
Iteration 136/1000 | Loss: 0.00001937
Iteration 137/1000 | Loss: 0.00001937
Iteration 138/1000 | Loss: 0.00001937
Iteration 139/1000 | Loss: 0.00001936
Iteration 140/1000 | Loss: 0.00001936
Iteration 141/1000 | Loss: 0.00001936
Iteration 142/1000 | Loss: 0.00001936
Iteration 143/1000 | Loss: 0.00001936
Iteration 144/1000 | Loss: 0.00001936
Iteration 145/1000 | Loss: 0.00001935
Iteration 146/1000 | Loss: 0.00001935
Iteration 147/1000 | Loss: 0.00001935
Iteration 148/1000 | Loss: 0.00001935
Iteration 149/1000 | Loss: 0.00001935
Iteration 150/1000 | Loss: 0.00001934
Iteration 151/1000 | Loss: 0.00001934
Iteration 152/1000 | Loss: 0.00001934
Iteration 153/1000 | Loss: 0.00001934
Iteration 154/1000 | Loss: 0.00001934
Iteration 155/1000 | Loss: 0.00001934
Iteration 156/1000 | Loss: 0.00001934
Iteration 157/1000 | Loss: 0.00001933
Iteration 158/1000 | Loss: 0.00001933
Iteration 159/1000 | Loss: 0.00001933
Iteration 160/1000 | Loss: 0.00001933
Iteration 161/1000 | Loss: 0.00001932
Iteration 162/1000 | Loss: 0.00001932
Iteration 163/1000 | Loss: 0.00001932
Iteration 164/1000 | Loss: 0.00001931
Iteration 165/1000 | Loss: 0.00001931
Iteration 166/1000 | Loss: 0.00001931
Iteration 167/1000 | Loss: 0.00001930
Iteration 168/1000 | Loss: 0.00001930
Iteration 169/1000 | Loss: 0.00001930
Iteration 170/1000 | Loss: 0.00001930
Iteration 171/1000 | Loss: 0.00001930
Iteration 172/1000 | Loss: 0.00001929
Iteration 173/1000 | Loss: 0.00001929
Iteration 174/1000 | Loss: 0.00001929
Iteration 175/1000 | Loss: 0.00001929
Iteration 176/1000 | Loss: 0.00001929
Iteration 177/1000 | Loss: 0.00001928
Iteration 178/1000 | Loss: 0.00001928
Iteration 179/1000 | Loss: 0.00001928
Iteration 180/1000 | Loss: 0.00001928
Iteration 181/1000 | Loss: 0.00001927
Iteration 182/1000 | Loss: 0.00001927
Iteration 183/1000 | Loss: 0.00001927
Iteration 184/1000 | Loss: 0.00001926
Iteration 185/1000 | Loss: 0.00001926
Iteration 186/1000 | Loss: 0.00001926
Iteration 187/1000 | Loss: 0.00001926
Iteration 188/1000 | Loss: 0.00001925
Iteration 189/1000 | Loss: 0.00001925
Iteration 190/1000 | Loss: 0.00001925
Iteration 191/1000 | Loss: 0.00001925
Iteration 192/1000 | Loss: 0.00001925
Iteration 193/1000 | Loss: 0.00016252
Iteration 194/1000 | Loss: 0.00016571
Iteration 195/1000 | Loss: 0.00002835
Iteration 196/1000 | Loss: 0.00002397
Iteration 197/1000 | Loss: 0.00021951
Iteration 198/1000 | Loss: 0.00002761
Iteration 199/1000 | Loss: 0.00014606
Iteration 200/1000 | Loss: 0.00002562
Iteration 201/1000 | Loss: 0.00018917
Iteration 202/1000 | Loss: 0.00028285
Iteration 203/1000 | Loss: 0.00020656
Iteration 204/1000 | Loss: 0.00024414
Iteration 205/1000 | Loss: 0.00023603
Iteration 206/1000 | Loss: 0.00016569
Iteration 207/1000 | Loss: 0.00015879
Iteration 208/1000 | Loss: 0.00003534
Iteration 209/1000 | Loss: 0.00020081
Iteration 210/1000 | Loss: 0.00020443
Iteration 211/1000 | Loss: 0.00010945
Iteration 212/1000 | Loss: 0.00021857
Iteration 213/1000 | Loss: 0.00016116
Iteration 214/1000 | Loss: 0.00003183
Iteration 215/1000 | Loss: 0.00002887
Iteration 216/1000 | Loss: 0.00057452
Iteration 217/1000 | Loss: 0.00033098
Iteration 218/1000 | Loss: 0.00058565
Iteration 219/1000 | Loss: 0.00015300
Iteration 220/1000 | Loss: 0.00014238
Iteration 221/1000 | Loss: 0.00015778
Iteration 222/1000 | Loss: 0.00015453
Iteration 223/1000 | Loss: 0.00015612
Iteration 224/1000 | Loss: 0.00003025
Iteration 225/1000 | Loss: 0.00003248
Iteration 226/1000 | Loss: 0.00002652
Iteration 227/1000 | Loss: 0.00002535
Iteration 228/1000 | Loss: 0.00003323
Iteration 229/1000 | Loss: 0.00003183
Iteration 230/1000 | Loss: 0.00003324
Iteration 231/1000 | Loss: 0.00002407
Iteration 232/1000 | Loss: 0.00002327
Iteration 233/1000 | Loss: 0.00002173
Iteration 234/1000 | Loss: 0.00002074
Iteration 235/1000 | Loss: 0.00002019
Iteration 236/1000 | Loss: 0.00001963
Iteration 237/1000 | Loss: 0.00001921
Iteration 238/1000 | Loss: 0.00001890
Iteration 239/1000 | Loss: 0.00001858
Iteration 240/1000 | Loss: 0.00001838
Iteration 241/1000 | Loss: 0.00001835
Iteration 242/1000 | Loss: 0.00001832
Iteration 243/1000 | Loss: 0.00001832
Iteration 244/1000 | Loss: 0.00001832
Iteration 245/1000 | Loss: 0.00001832
Iteration 246/1000 | Loss: 0.00001832
Iteration 247/1000 | Loss: 0.00001831
Iteration 248/1000 | Loss: 0.00001831
Iteration 249/1000 | Loss: 0.00001831
Iteration 250/1000 | Loss: 0.00001830
Iteration 251/1000 | Loss: 0.00001830
Iteration 252/1000 | Loss: 0.00001830
Iteration 253/1000 | Loss: 0.00001830
Iteration 254/1000 | Loss: 0.00001829
Iteration 255/1000 | Loss: 0.00001829
Iteration 256/1000 | Loss: 0.00001829
Iteration 257/1000 | Loss: 0.00001829
Iteration 258/1000 | Loss: 0.00001829
Iteration 259/1000 | Loss: 0.00001829
Iteration 260/1000 | Loss: 0.00001829
Iteration 261/1000 | Loss: 0.00001829
Iteration 262/1000 | Loss: 0.00001829
Iteration 263/1000 | Loss: 0.00001828
Iteration 264/1000 | Loss: 0.00001828
Iteration 265/1000 | Loss: 0.00001828
Iteration 266/1000 | Loss: 0.00001827
Iteration 267/1000 | Loss: 0.00001827
Iteration 268/1000 | Loss: 0.00001827
Iteration 269/1000 | Loss: 0.00001827
Iteration 270/1000 | Loss: 0.00001827
Iteration 271/1000 | Loss: 0.00001827
Iteration 272/1000 | Loss: 0.00001827
Iteration 273/1000 | Loss: 0.00001826
Iteration 274/1000 | Loss: 0.00001826
Iteration 275/1000 | Loss: 0.00001826
Iteration 276/1000 | Loss: 0.00001826
Iteration 277/1000 | Loss: 0.00001826
Iteration 278/1000 | Loss: 0.00001825
Iteration 279/1000 | Loss: 0.00001825
Iteration 280/1000 | Loss: 0.00001824
Iteration 281/1000 | Loss: 0.00001824
Iteration 282/1000 | Loss: 0.00001824
Iteration 283/1000 | Loss: 0.00001824
Iteration 284/1000 | Loss: 0.00001824
Iteration 285/1000 | Loss: 0.00001823
Iteration 286/1000 | Loss: 0.00001823
Iteration 287/1000 | Loss: 0.00001823
Iteration 288/1000 | Loss: 0.00001823
Iteration 289/1000 | Loss: 0.00001823
Iteration 290/1000 | Loss: 0.00001823
Iteration 291/1000 | Loss: 0.00001822
Iteration 292/1000 | Loss: 0.00001822
Iteration 293/1000 | Loss: 0.00001822
Iteration 294/1000 | Loss: 0.00001822
Iteration 295/1000 | Loss: 0.00001822
Iteration 296/1000 | Loss: 0.00001822
Iteration 297/1000 | Loss: 0.00001822
Iteration 298/1000 | Loss: 0.00001822
Iteration 299/1000 | Loss: 0.00001822
Iteration 300/1000 | Loss: 0.00001822
Iteration 301/1000 | Loss: 0.00001822
Iteration 302/1000 | Loss: 0.00001822
Iteration 303/1000 | Loss: 0.00001822
Iteration 304/1000 | Loss: 0.00001822
Iteration 305/1000 | Loss: 0.00001822
Iteration 306/1000 | Loss: 0.00001821
Iteration 307/1000 | Loss: 0.00001821
Iteration 308/1000 | Loss: 0.00001821
Iteration 309/1000 | Loss: 0.00001821
Iteration 310/1000 | Loss: 0.00001821
Iteration 311/1000 | Loss: 0.00001821
Iteration 312/1000 | Loss: 0.00001821
Iteration 313/1000 | Loss: 0.00001821
Iteration 314/1000 | Loss: 0.00001821
Iteration 315/1000 | Loss: 0.00001821
Iteration 316/1000 | Loss: 0.00001821
Iteration 317/1000 | Loss: 0.00001821
Iteration 318/1000 | Loss: 0.00001821
Iteration 319/1000 | Loss: 0.00001821
Iteration 320/1000 | Loss: 0.00001821
Iteration 321/1000 | Loss: 0.00001821
Iteration 322/1000 | Loss: 0.00001821
Iteration 323/1000 | Loss: 0.00001821
Iteration 324/1000 | Loss: 0.00001821
Iteration 325/1000 | Loss: 0.00001821
Iteration 326/1000 | Loss: 0.00001821
Iteration 327/1000 | Loss: 0.00001821
Iteration 328/1000 | Loss: 0.00001821
Iteration 329/1000 | Loss: 0.00001821
Iteration 330/1000 | Loss: 0.00001821
Iteration 331/1000 | Loss: 0.00001821
Iteration 332/1000 | Loss: 0.00001821
Iteration 333/1000 | Loss: 0.00001821
Iteration 334/1000 | Loss: 0.00001821
Iteration 335/1000 | Loss: 0.00001821
Iteration 336/1000 | Loss: 0.00001821
Iteration 337/1000 | Loss: 0.00001821
Iteration 338/1000 | Loss: 0.00001821
Iteration 339/1000 | Loss: 0.00001821
Iteration 340/1000 | Loss: 0.00001821
Iteration 341/1000 | Loss: 0.00001821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 341. Stopping optimization.
Last 5 losses: [1.8207021639682353e-05, 1.8207021639682353e-05, 1.8207021639682353e-05, 1.8207021639682353e-05, 1.8207021639682353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8207021639682353e-05

Optimization complete. Final v2v error: 3.5815937519073486 mm

Highest mean error: 5.218216896057129 mm for frame 147

Lowest mean error: 3.1338369846343994 mm for frame 113

Saving results

Total time: 184.09338927268982
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865724
Iteration 2/25 | Loss: 0.00135997
Iteration 3/25 | Loss: 0.00118316
Iteration 4/25 | Loss: 0.00116766
Iteration 5/25 | Loss: 0.00116550
Iteration 6/25 | Loss: 0.00116550
Iteration 7/25 | Loss: 0.00116550
Iteration 8/25 | Loss: 0.00116550
Iteration 9/25 | Loss: 0.00116550
Iteration 10/25 | Loss: 0.00116550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011654974659904838, 0.0011654974659904838, 0.0011654974659904838, 0.0011654974659904838, 0.0011654974659904838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011654974659904838

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35549295
Iteration 2/25 | Loss: 0.00073743
Iteration 3/25 | Loss: 0.00073743
Iteration 4/25 | Loss: 0.00073743
Iteration 5/25 | Loss: 0.00073743
Iteration 6/25 | Loss: 0.00073743
Iteration 7/25 | Loss: 0.00073743
Iteration 8/25 | Loss: 0.00073743
Iteration 9/25 | Loss: 0.00073743
Iteration 10/25 | Loss: 0.00073743
Iteration 11/25 | Loss: 0.00073743
Iteration 12/25 | Loss: 0.00073743
Iteration 13/25 | Loss: 0.00073743
Iteration 14/25 | Loss: 0.00073743
Iteration 15/25 | Loss: 0.00073743
Iteration 16/25 | Loss: 0.00073743
Iteration 17/25 | Loss: 0.00073743
Iteration 18/25 | Loss: 0.00073743
Iteration 19/25 | Loss: 0.00073743
Iteration 20/25 | Loss: 0.00073743
Iteration 21/25 | Loss: 0.00073743
Iteration 22/25 | Loss: 0.00073743
Iteration 23/25 | Loss: 0.00073743
Iteration 24/25 | Loss: 0.00073743
Iteration 25/25 | Loss: 0.00073743
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007374261622317135, 0.0007374261622317135, 0.0007374261622317135, 0.0007374261622317135, 0.0007374261622317135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007374261622317135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073743
Iteration 2/1000 | Loss: 0.00002348
Iteration 3/1000 | Loss: 0.00001804
Iteration 4/1000 | Loss: 0.00001684
Iteration 5/1000 | Loss: 0.00001626
Iteration 6/1000 | Loss: 0.00001582
Iteration 7/1000 | Loss: 0.00001552
Iteration 8/1000 | Loss: 0.00001521
Iteration 9/1000 | Loss: 0.00001498
Iteration 10/1000 | Loss: 0.00001483
Iteration 11/1000 | Loss: 0.00001480
Iteration 12/1000 | Loss: 0.00001467
Iteration 13/1000 | Loss: 0.00001456
Iteration 14/1000 | Loss: 0.00001454
Iteration 15/1000 | Loss: 0.00001452
Iteration 16/1000 | Loss: 0.00001451
Iteration 17/1000 | Loss: 0.00001451
Iteration 18/1000 | Loss: 0.00001450
Iteration 19/1000 | Loss: 0.00001448
Iteration 20/1000 | Loss: 0.00001448
Iteration 21/1000 | Loss: 0.00001447
Iteration 22/1000 | Loss: 0.00001443
Iteration 23/1000 | Loss: 0.00001440
Iteration 24/1000 | Loss: 0.00001440
Iteration 25/1000 | Loss: 0.00001440
Iteration 26/1000 | Loss: 0.00001440
Iteration 27/1000 | Loss: 0.00001440
Iteration 28/1000 | Loss: 0.00001439
Iteration 29/1000 | Loss: 0.00001439
Iteration 30/1000 | Loss: 0.00001439
Iteration 31/1000 | Loss: 0.00001439
Iteration 32/1000 | Loss: 0.00001439
Iteration 33/1000 | Loss: 0.00001439
Iteration 34/1000 | Loss: 0.00001439
Iteration 35/1000 | Loss: 0.00001439
Iteration 36/1000 | Loss: 0.00001437
Iteration 37/1000 | Loss: 0.00001436
Iteration 38/1000 | Loss: 0.00001436
Iteration 39/1000 | Loss: 0.00001435
Iteration 40/1000 | Loss: 0.00001435
Iteration 41/1000 | Loss: 0.00001435
Iteration 42/1000 | Loss: 0.00001435
Iteration 43/1000 | Loss: 0.00001434
Iteration 44/1000 | Loss: 0.00001433
Iteration 45/1000 | Loss: 0.00001433
Iteration 46/1000 | Loss: 0.00001433
Iteration 47/1000 | Loss: 0.00001433
Iteration 48/1000 | Loss: 0.00001432
Iteration 49/1000 | Loss: 0.00001432
Iteration 50/1000 | Loss: 0.00001432
Iteration 51/1000 | Loss: 0.00001432
Iteration 52/1000 | Loss: 0.00001431
Iteration 53/1000 | Loss: 0.00001431
Iteration 54/1000 | Loss: 0.00001430
Iteration 55/1000 | Loss: 0.00001430
Iteration 56/1000 | Loss: 0.00001429
Iteration 57/1000 | Loss: 0.00001428
Iteration 58/1000 | Loss: 0.00001428
Iteration 59/1000 | Loss: 0.00001428
Iteration 60/1000 | Loss: 0.00001428
Iteration 61/1000 | Loss: 0.00001428
Iteration 62/1000 | Loss: 0.00001427
Iteration 63/1000 | Loss: 0.00001427
Iteration 64/1000 | Loss: 0.00001427
Iteration 65/1000 | Loss: 0.00001427
Iteration 66/1000 | Loss: 0.00001427
Iteration 67/1000 | Loss: 0.00001426
Iteration 68/1000 | Loss: 0.00001426
Iteration 69/1000 | Loss: 0.00001425
Iteration 70/1000 | Loss: 0.00001425
Iteration 71/1000 | Loss: 0.00001425
Iteration 72/1000 | Loss: 0.00001425
Iteration 73/1000 | Loss: 0.00001425
Iteration 74/1000 | Loss: 0.00001425
Iteration 75/1000 | Loss: 0.00001424
Iteration 76/1000 | Loss: 0.00001424
Iteration 77/1000 | Loss: 0.00001424
Iteration 78/1000 | Loss: 0.00001424
Iteration 79/1000 | Loss: 0.00001424
Iteration 80/1000 | Loss: 0.00001423
Iteration 81/1000 | Loss: 0.00001423
Iteration 82/1000 | Loss: 0.00001423
Iteration 83/1000 | Loss: 0.00001423
Iteration 84/1000 | Loss: 0.00001423
Iteration 85/1000 | Loss: 0.00001422
Iteration 86/1000 | Loss: 0.00001422
Iteration 87/1000 | Loss: 0.00001422
Iteration 88/1000 | Loss: 0.00001422
Iteration 89/1000 | Loss: 0.00001421
Iteration 90/1000 | Loss: 0.00001421
Iteration 91/1000 | Loss: 0.00001420
Iteration 92/1000 | Loss: 0.00001420
Iteration 93/1000 | Loss: 0.00001420
Iteration 94/1000 | Loss: 0.00001420
Iteration 95/1000 | Loss: 0.00001420
Iteration 96/1000 | Loss: 0.00001419
Iteration 97/1000 | Loss: 0.00001419
Iteration 98/1000 | Loss: 0.00001419
Iteration 99/1000 | Loss: 0.00001419
Iteration 100/1000 | Loss: 0.00001419
Iteration 101/1000 | Loss: 0.00001419
Iteration 102/1000 | Loss: 0.00001418
Iteration 103/1000 | Loss: 0.00001418
Iteration 104/1000 | Loss: 0.00001418
Iteration 105/1000 | Loss: 0.00001417
Iteration 106/1000 | Loss: 0.00001417
Iteration 107/1000 | Loss: 0.00001417
Iteration 108/1000 | Loss: 0.00001416
Iteration 109/1000 | Loss: 0.00001416
Iteration 110/1000 | Loss: 0.00001416
Iteration 111/1000 | Loss: 0.00001416
Iteration 112/1000 | Loss: 0.00001416
Iteration 113/1000 | Loss: 0.00001416
Iteration 114/1000 | Loss: 0.00001416
Iteration 115/1000 | Loss: 0.00001415
Iteration 116/1000 | Loss: 0.00001415
Iteration 117/1000 | Loss: 0.00001415
Iteration 118/1000 | Loss: 0.00001415
Iteration 119/1000 | Loss: 0.00001414
Iteration 120/1000 | Loss: 0.00001414
Iteration 121/1000 | Loss: 0.00001414
Iteration 122/1000 | Loss: 0.00001414
Iteration 123/1000 | Loss: 0.00001414
Iteration 124/1000 | Loss: 0.00001414
Iteration 125/1000 | Loss: 0.00001413
Iteration 126/1000 | Loss: 0.00001413
Iteration 127/1000 | Loss: 0.00001413
Iteration 128/1000 | Loss: 0.00001413
Iteration 129/1000 | Loss: 0.00001413
Iteration 130/1000 | Loss: 0.00001413
Iteration 131/1000 | Loss: 0.00001412
Iteration 132/1000 | Loss: 0.00001412
Iteration 133/1000 | Loss: 0.00001412
Iteration 134/1000 | Loss: 0.00001411
Iteration 135/1000 | Loss: 0.00001411
Iteration 136/1000 | Loss: 0.00001411
Iteration 137/1000 | Loss: 0.00001411
Iteration 138/1000 | Loss: 0.00001411
Iteration 139/1000 | Loss: 0.00001411
Iteration 140/1000 | Loss: 0.00001411
Iteration 141/1000 | Loss: 0.00001411
Iteration 142/1000 | Loss: 0.00001411
Iteration 143/1000 | Loss: 0.00001411
Iteration 144/1000 | Loss: 0.00001410
Iteration 145/1000 | Loss: 0.00001410
Iteration 146/1000 | Loss: 0.00001410
Iteration 147/1000 | Loss: 0.00001409
Iteration 148/1000 | Loss: 0.00001409
Iteration 149/1000 | Loss: 0.00001409
Iteration 150/1000 | Loss: 0.00001409
Iteration 151/1000 | Loss: 0.00001409
Iteration 152/1000 | Loss: 0.00001409
Iteration 153/1000 | Loss: 0.00001409
Iteration 154/1000 | Loss: 0.00001409
Iteration 155/1000 | Loss: 0.00001409
Iteration 156/1000 | Loss: 0.00001409
Iteration 157/1000 | Loss: 0.00001409
Iteration 158/1000 | Loss: 0.00001409
Iteration 159/1000 | Loss: 0.00001409
Iteration 160/1000 | Loss: 0.00001409
Iteration 161/1000 | Loss: 0.00001409
Iteration 162/1000 | Loss: 0.00001409
Iteration 163/1000 | Loss: 0.00001409
Iteration 164/1000 | Loss: 0.00001409
Iteration 165/1000 | Loss: 0.00001409
Iteration 166/1000 | Loss: 0.00001409
Iteration 167/1000 | Loss: 0.00001409
Iteration 168/1000 | Loss: 0.00001409
Iteration 169/1000 | Loss: 0.00001409
Iteration 170/1000 | Loss: 0.00001409
Iteration 171/1000 | Loss: 0.00001409
Iteration 172/1000 | Loss: 0.00001409
Iteration 173/1000 | Loss: 0.00001409
Iteration 174/1000 | Loss: 0.00001409
Iteration 175/1000 | Loss: 0.00001409
Iteration 176/1000 | Loss: 0.00001409
Iteration 177/1000 | Loss: 0.00001409
Iteration 178/1000 | Loss: 0.00001409
Iteration 179/1000 | Loss: 0.00001409
Iteration 180/1000 | Loss: 0.00001409
Iteration 181/1000 | Loss: 0.00001409
Iteration 182/1000 | Loss: 0.00001409
Iteration 183/1000 | Loss: 0.00001409
Iteration 184/1000 | Loss: 0.00001409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.4089809155848343e-05, 1.4089809155848343e-05, 1.4089809155848343e-05, 1.4089809155848343e-05, 1.4089809155848343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4089809155848343e-05

Optimization complete. Final v2v error: 3.1739096641540527 mm

Highest mean error: 3.689988136291504 mm for frame 151

Lowest mean error: 2.8877968788146973 mm for frame 94

Saving results

Total time: 41.53679299354553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481532
Iteration 2/25 | Loss: 0.00120616
Iteration 3/25 | Loss: 0.00111576
Iteration 4/25 | Loss: 0.00110428
Iteration 5/25 | Loss: 0.00110138
Iteration 6/25 | Loss: 0.00110138
Iteration 7/25 | Loss: 0.00110138
Iteration 8/25 | Loss: 0.00110138
Iteration 9/25 | Loss: 0.00110138
Iteration 10/25 | Loss: 0.00110138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011013804469257593, 0.0011013804469257593, 0.0011013804469257593, 0.0011013804469257593, 0.0011013804469257593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011013804469257593

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.65965939
Iteration 2/25 | Loss: 0.00080487
Iteration 3/25 | Loss: 0.00080487
Iteration 4/25 | Loss: 0.00080487
Iteration 5/25 | Loss: 0.00080487
Iteration 6/25 | Loss: 0.00080487
Iteration 7/25 | Loss: 0.00080487
Iteration 8/25 | Loss: 0.00080487
Iteration 9/25 | Loss: 0.00080486
Iteration 10/25 | Loss: 0.00080486
Iteration 11/25 | Loss: 0.00080486
Iteration 12/25 | Loss: 0.00080486
Iteration 13/25 | Loss: 0.00080486
Iteration 14/25 | Loss: 0.00080486
Iteration 15/25 | Loss: 0.00080486
Iteration 16/25 | Loss: 0.00080486
Iteration 17/25 | Loss: 0.00080486
Iteration 18/25 | Loss: 0.00080486
Iteration 19/25 | Loss: 0.00080486
Iteration 20/25 | Loss: 0.00080486
Iteration 21/25 | Loss: 0.00080486
Iteration 22/25 | Loss: 0.00080486
Iteration 23/25 | Loss: 0.00080486
Iteration 24/25 | Loss: 0.00080486
Iteration 25/25 | Loss: 0.00080486

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080486
Iteration 2/1000 | Loss: 0.00001934
Iteration 3/1000 | Loss: 0.00001513
Iteration 4/1000 | Loss: 0.00001378
Iteration 5/1000 | Loss: 0.00001307
Iteration 6/1000 | Loss: 0.00001259
Iteration 7/1000 | Loss: 0.00001239
Iteration 8/1000 | Loss: 0.00001210
Iteration 9/1000 | Loss: 0.00001181
Iteration 10/1000 | Loss: 0.00001169
Iteration 11/1000 | Loss: 0.00001154
Iteration 12/1000 | Loss: 0.00001150
Iteration 13/1000 | Loss: 0.00001150
Iteration 14/1000 | Loss: 0.00001149
Iteration 15/1000 | Loss: 0.00001145
Iteration 16/1000 | Loss: 0.00001144
Iteration 17/1000 | Loss: 0.00001141
Iteration 18/1000 | Loss: 0.00001139
Iteration 19/1000 | Loss: 0.00001136
Iteration 20/1000 | Loss: 0.00001135
Iteration 21/1000 | Loss: 0.00001129
Iteration 22/1000 | Loss: 0.00001128
Iteration 23/1000 | Loss: 0.00001122
Iteration 24/1000 | Loss: 0.00001122
Iteration 25/1000 | Loss: 0.00001121
Iteration 26/1000 | Loss: 0.00001120
Iteration 27/1000 | Loss: 0.00001120
Iteration 28/1000 | Loss: 0.00001120
Iteration 29/1000 | Loss: 0.00001119
Iteration 30/1000 | Loss: 0.00001119
Iteration 31/1000 | Loss: 0.00001119
Iteration 32/1000 | Loss: 0.00001119
Iteration 33/1000 | Loss: 0.00001117
Iteration 34/1000 | Loss: 0.00001117
Iteration 35/1000 | Loss: 0.00001116
Iteration 36/1000 | Loss: 0.00001116
Iteration 37/1000 | Loss: 0.00001115
Iteration 38/1000 | Loss: 0.00001114
Iteration 39/1000 | Loss: 0.00001114
Iteration 40/1000 | Loss: 0.00001113
Iteration 41/1000 | Loss: 0.00001111
Iteration 42/1000 | Loss: 0.00001111
Iteration 43/1000 | Loss: 0.00001110
Iteration 44/1000 | Loss: 0.00001110
Iteration 45/1000 | Loss: 0.00001110
Iteration 46/1000 | Loss: 0.00001110
Iteration 47/1000 | Loss: 0.00001110
Iteration 48/1000 | Loss: 0.00001110
Iteration 49/1000 | Loss: 0.00001109
Iteration 50/1000 | Loss: 0.00001108
Iteration 51/1000 | Loss: 0.00001108
Iteration 52/1000 | Loss: 0.00001107
Iteration 53/1000 | Loss: 0.00001107
Iteration 54/1000 | Loss: 0.00001107
Iteration 55/1000 | Loss: 0.00001107
Iteration 56/1000 | Loss: 0.00001107
Iteration 57/1000 | Loss: 0.00001107
Iteration 58/1000 | Loss: 0.00001106
Iteration 59/1000 | Loss: 0.00001106
Iteration 60/1000 | Loss: 0.00001106
Iteration 61/1000 | Loss: 0.00001106
Iteration 62/1000 | Loss: 0.00001105
Iteration 63/1000 | Loss: 0.00001105
Iteration 64/1000 | Loss: 0.00001105
Iteration 65/1000 | Loss: 0.00001104
Iteration 66/1000 | Loss: 0.00001104
Iteration 67/1000 | Loss: 0.00001104
Iteration 68/1000 | Loss: 0.00001103
Iteration 69/1000 | Loss: 0.00001103
Iteration 70/1000 | Loss: 0.00001102
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001101
Iteration 73/1000 | Loss: 0.00001100
Iteration 74/1000 | Loss: 0.00001100
Iteration 75/1000 | Loss: 0.00001100
Iteration 76/1000 | Loss: 0.00001099
Iteration 77/1000 | Loss: 0.00001099
Iteration 78/1000 | Loss: 0.00001099
Iteration 79/1000 | Loss: 0.00001098
Iteration 80/1000 | Loss: 0.00001098
Iteration 81/1000 | Loss: 0.00001098
Iteration 82/1000 | Loss: 0.00001098
Iteration 83/1000 | Loss: 0.00001097
Iteration 84/1000 | Loss: 0.00001097
Iteration 85/1000 | Loss: 0.00001097
Iteration 86/1000 | Loss: 0.00001097
Iteration 87/1000 | Loss: 0.00001097
Iteration 88/1000 | Loss: 0.00001097
Iteration 89/1000 | Loss: 0.00001097
Iteration 90/1000 | Loss: 0.00001097
Iteration 91/1000 | Loss: 0.00001097
Iteration 92/1000 | Loss: 0.00001097
Iteration 93/1000 | Loss: 0.00001097
Iteration 94/1000 | Loss: 0.00001097
Iteration 95/1000 | Loss: 0.00001097
Iteration 96/1000 | Loss: 0.00001097
Iteration 97/1000 | Loss: 0.00001097
Iteration 98/1000 | Loss: 0.00001097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.0965764886350371e-05, 1.0965764886350371e-05, 1.0965764886350371e-05, 1.0965764886350371e-05, 1.0965764886350371e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0965764886350371e-05

Optimization complete. Final v2v error: 2.8377678394317627 mm

Highest mean error: 3.140639305114746 mm for frame 213

Lowest mean error: 2.5729992389678955 mm for frame 126

Saving results

Total time: 35.805840492248535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784088
Iteration 2/25 | Loss: 0.00138165
Iteration 3/25 | Loss: 0.00119244
Iteration 4/25 | Loss: 0.00118095
Iteration 5/25 | Loss: 0.00118025
Iteration 6/25 | Loss: 0.00118025
Iteration 7/25 | Loss: 0.00118025
Iteration 8/25 | Loss: 0.00118025
Iteration 9/25 | Loss: 0.00118025
Iteration 10/25 | Loss: 0.00118025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011802527587860823, 0.0011802527587860823, 0.0011802527587860823, 0.0011802527587860823, 0.0011802527587860823]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011802527587860823

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32803130
Iteration 2/25 | Loss: 0.00056336
Iteration 3/25 | Loss: 0.00056336
Iteration 4/25 | Loss: 0.00056336
Iteration 5/25 | Loss: 0.00056335
Iteration 6/25 | Loss: 0.00056335
Iteration 7/25 | Loss: 0.00056335
Iteration 8/25 | Loss: 0.00056335
Iteration 9/25 | Loss: 0.00056335
Iteration 10/25 | Loss: 0.00056335
Iteration 11/25 | Loss: 0.00056335
Iteration 12/25 | Loss: 0.00056335
Iteration 13/25 | Loss: 0.00056335
Iteration 14/25 | Loss: 0.00056335
Iteration 15/25 | Loss: 0.00056335
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005633535911329091, 0.0005633535911329091, 0.0005633535911329091, 0.0005633535911329091, 0.0005633535911329091]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005633535911329091

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056335
Iteration 2/1000 | Loss: 0.00002737
Iteration 3/1000 | Loss: 0.00002260
Iteration 4/1000 | Loss: 0.00002121
Iteration 5/1000 | Loss: 0.00002025
Iteration 6/1000 | Loss: 0.00001958
Iteration 7/1000 | Loss: 0.00001887
Iteration 8/1000 | Loss: 0.00001855
Iteration 9/1000 | Loss: 0.00001832
Iteration 10/1000 | Loss: 0.00001826
Iteration 11/1000 | Loss: 0.00001812
Iteration 12/1000 | Loss: 0.00001803
Iteration 13/1000 | Loss: 0.00001802
Iteration 14/1000 | Loss: 0.00001801
Iteration 15/1000 | Loss: 0.00001801
Iteration 16/1000 | Loss: 0.00001801
Iteration 17/1000 | Loss: 0.00001801
Iteration 18/1000 | Loss: 0.00001801
Iteration 19/1000 | Loss: 0.00001800
Iteration 20/1000 | Loss: 0.00001800
Iteration 21/1000 | Loss: 0.00001795
Iteration 22/1000 | Loss: 0.00001794
Iteration 23/1000 | Loss: 0.00001794
Iteration 24/1000 | Loss: 0.00001791
Iteration 25/1000 | Loss: 0.00001791
Iteration 26/1000 | Loss: 0.00001791
Iteration 27/1000 | Loss: 0.00001791
Iteration 28/1000 | Loss: 0.00001791
Iteration 29/1000 | Loss: 0.00001791
Iteration 30/1000 | Loss: 0.00001791
Iteration 31/1000 | Loss: 0.00001791
Iteration 32/1000 | Loss: 0.00001790
Iteration 33/1000 | Loss: 0.00001790
Iteration 34/1000 | Loss: 0.00001790
Iteration 35/1000 | Loss: 0.00001790
Iteration 36/1000 | Loss: 0.00001790
Iteration 37/1000 | Loss: 0.00001788
Iteration 38/1000 | Loss: 0.00001788
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001787
Iteration 41/1000 | Loss: 0.00001786
Iteration 42/1000 | Loss: 0.00001786
Iteration 43/1000 | Loss: 0.00001785
Iteration 44/1000 | Loss: 0.00001785
Iteration 45/1000 | Loss: 0.00001785
Iteration 46/1000 | Loss: 0.00001785
Iteration 47/1000 | Loss: 0.00001785
Iteration 48/1000 | Loss: 0.00001785
Iteration 49/1000 | Loss: 0.00001785
Iteration 50/1000 | Loss: 0.00001785
Iteration 51/1000 | Loss: 0.00001785
Iteration 52/1000 | Loss: 0.00001784
Iteration 53/1000 | Loss: 0.00001784
Iteration 54/1000 | Loss: 0.00001784
Iteration 55/1000 | Loss: 0.00001784
Iteration 56/1000 | Loss: 0.00001784
Iteration 57/1000 | Loss: 0.00001784
Iteration 58/1000 | Loss: 0.00001784
Iteration 59/1000 | Loss: 0.00001784
Iteration 60/1000 | Loss: 0.00001783
Iteration 61/1000 | Loss: 0.00001783
Iteration 62/1000 | Loss: 0.00001783
Iteration 63/1000 | Loss: 0.00001783
Iteration 64/1000 | Loss: 0.00001783
Iteration 65/1000 | Loss: 0.00001780
Iteration 66/1000 | Loss: 0.00001780
Iteration 67/1000 | Loss: 0.00001780
Iteration 68/1000 | Loss: 0.00001779
Iteration 69/1000 | Loss: 0.00001778
Iteration 70/1000 | Loss: 0.00001778
Iteration 71/1000 | Loss: 0.00001778
Iteration 72/1000 | Loss: 0.00001778
Iteration 73/1000 | Loss: 0.00001778
Iteration 74/1000 | Loss: 0.00001778
Iteration 75/1000 | Loss: 0.00001778
Iteration 76/1000 | Loss: 0.00001777
Iteration 77/1000 | Loss: 0.00001777
Iteration 78/1000 | Loss: 0.00001777
Iteration 79/1000 | Loss: 0.00001777
Iteration 80/1000 | Loss: 0.00001777
Iteration 81/1000 | Loss: 0.00001777
Iteration 82/1000 | Loss: 0.00001777
Iteration 83/1000 | Loss: 0.00001777
Iteration 84/1000 | Loss: 0.00001777
Iteration 85/1000 | Loss: 0.00001776
Iteration 86/1000 | Loss: 0.00001776
Iteration 87/1000 | Loss: 0.00001776
Iteration 88/1000 | Loss: 0.00001776
Iteration 89/1000 | Loss: 0.00001776
Iteration 90/1000 | Loss: 0.00001776
Iteration 91/1000 | Loss: 0.00001776
Iteration 92/1000 | Loss: 0.00001776
Iteration 93/1000 | Loss: 0.00001776
Iteration 94/1000 | Loss: 0.00001775
Iteration 95/1000 | Loss: 0.00001775
Iteration 96/1000 | Loss: 0.00001775
Iteration 97/1000 | Loss: 0.00001775
Iteration 98/1000 | Loss: 0.00001775
Iteration 99/1000 | Loss: 0.00001775
Iteration 100/1000 | Loss: 0.00001775
Iteration 101/1000 | Loss: 0.00001775
Iteration 102/1000 | Loss: 0.00001775
Iteration 103/1000 | Loss: 0.00001775
Iteration 104/1000 | Loss: 0.00001775
Iteration 105/1000 | Loss: 0.00001775
Iteration 106/1000 | Loss: 0.00001775
Iteration 107/1000 | Loss: 0.00001774
Iteration 108/1000 | Loss: 0.00001774
Iteration 109/1000 | Loss: 0.00001774
Iteration 110/1000 | Loss: 0.00001774
Iteration 111/1000 | Loss: 0.00001774
Iteration 112/1000 | Loss: 0.00001774
Iteration 113/1000 | Loss: 0.00001774
Iteration 114/1000 | Loss: 0.00001773
Iteration 115/1000 | Loss: 0.00001773
Iteration 116/1000 | Loss: 0.00001773
Iteration 117/1000 | Loss: 0.00001773
Iteration 118/1000 | Loss: 0.00001773
Iteration 119/1000 | Loss: 0.00001773
Iteration 120/1000 | Loss: 0.00001773
Iteration 121/1000 | Loss: 0.00001773
Iteration 122/1000 | Loss: 0.00001773
Iteration 123/1000 | Loss: 0.00001773
Iteration 124/1000 | Loss: 0.00001773
Iteration 125/1000 | Loss: 0.00001773
Iteration 126/1000 | Loss: 0.00001773
Iteration 127/1000 | Loss: 0.00001773
Iteration 128/1000 | Loss: 0.00001773
Iteration 129/1000 | Loss: 0.00001773
Iteration 130/1000 | Loss: 0.00001773
Iteration 131/1000 | Loss: 0.00001773
Iteration 132/1000 | Loss: 0.00001773
Iteration 133/1000 | Loss: 0.00001772
Iteration 134/1000 | Loss: 0.00001772
Iteration 135/1000 | Loss: 0.00001772
Iteration 136/1000 | Loss: 0.00001772
Iteration 137/1000 | Loss: 0.00001772
Iteration 138/1000 | Loss: 0.00001772
Iteration 139/1000 | Loss: 0.00001772
Iteration 140/1000 | Loss: 0.00001772
Iteration 141/1000 | Loss: 0.00001772
Iteration 142/1000 | Loss: 0.00001772
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.772327050275635e-05, 1.772327050275635e-05, 1.772327050275635e-05, 1.772327050275635e-05, 1.772327050275635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.772327050275635e-05

Optimization complete. Final v2v error: 3.5091092586517334 mm

Highest mean error: 3.732468366622925 mm for frame 24

Lowest mean error: 3.2986550331115723 mm for frame 88

Saving results

Total time: 34.91625452041626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415511
Iteration 2/25 | Loss: 0.00115571
Iteration 3/25 | Loss: 0.00108172
Iteration 4/25 | Loss: 0.00107241
Iteration 5/25 | Loss: 0.00106951
Iteration 6/25 | Loss: 0.00106896
Iteration 7/25 | Loss: 0.00106896
Iteration 8/25 | Loss: 0.00106896
Iteration 9/25 | Loss: 0.00106896
Iteration 10/25 | Loss: 0.00106896
Iteration 11/25 | Loss: 0.00106896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010689620394259691, 0.0010689620394259691, 0.0010689620394259691, 0.0010689620394259691, 0.0010689620394259691]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010689620394259691

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69964004
Iteration 2/25 | Loss: 0.00084985
Iteration 3/25 | Loss: 0.00084985
Iteration 4/25 | Loss: 0.00084985
Iteration 5/25 | Loss: 0.00084985
Iteration 6/25 | Loss: 0.00084984
Iteration 7/25 | Loss: 0.00084984
Iteration 8/25 | Loss: 0.00084984
Iteration 9/25 | Loss: 0.00084984
Iteration 10/25 | Loss: 0.00084984
Iteration 11/25 | Loss: 0.00084984
Iteration 12/25 | Loss: 0.00084984
Iteration 13/25 | Loss: 0.00084984
Iteration 14/25 | Loss: 0.00084984
Iteration 15/25 | Loss: 0.00084984
Iteration 16/25 | Loss: 0.00084984
Iteration 17/25 | Loss: 0.00084984
Iteration 18/25 | Loss: 0.00084984
Iteration 19/25 | Loss: 0.00084984
Iteration 20/25 | Loss: 0.00084984
Iteration 21/25 | Loss: 0.00084984
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008498425595462322, 0.0008498425595462322, 0.0008498425595462322, 0.0008498425595462322, 0.0008498425595462322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008498425595462322

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084984
Iteration 2/1000 | Loss: 0.00002914
Iteration 3/1000 | Loss: 0.00001904
Iteration 4/1000 | Loss: 0.00001485
Iteration 5/1000 | Loss: 0.00001336
Iteration 6/1000 | Loss: 0.00001263
Iteration 7/1000 | Loss: 0.00001203
Iteration 8/1000 | Loss: 0.00001170
Iteration 9/1000 | Loss: 0.00001136
Iteration 10/1000 | Loss: 0.00001122
Iteration 11/1000 | Loss: 0.00001110
Iteration 12/1000 | Loss: 0.00001090
Iteration 13/1000 | Loss: 0.00001087
Iteration 14/1000 | Loss: 0.00001086
Iteration 15/1000 | Loss: 0.00001082
Iteration 16/1000 | Loss: 0.00001081
Iteration 17/1000 | Loss: 0.00001081
Iteration 18/1000 | Loss: 0.00001080
Iteration 19/1000 | Loss: 0.00001076
Iteration 20/1000 | Loss: 0.00001074
Iteration 21/1000 | Loss: 0.00001072
Iteration 22/1000 | Loss: 0.00001070
Iteration 23/1000 | Loss: 0.00001069
Iteration 24/1000 | Loss: 0.00001063
Iteration 25/1000 | Loss: 0.00001063
Iteration 26/1000 | Loss: 0.00001061
Iteration 27/1000 | Loss: 0.00001059
Iteration 28/1000 | Loss: 0.00001057
Iteration 29/1000 | Loss: 0.00001057
Iteration 30/1000 | Loss: 0.00001056
Iteration 31/1000 | Loss: 0.00001056
Iteration 32/1000 | Loss: 0.00001054
Iteration 33/1000 | Loss: 0.00001053
Iteration 34/1000 | Loss: 0.00001052
Iteration 35/1000 | Loss: 0.00001052
Iteration 36/1000 | Loss: 0.00001051
Iteration 37/1000 | Loss: 0.00001051
Iteration 38/1000 | Loss: 0.00001051
Iteration 39/1000 | Loss: 0.00001051
Iteration 40/1000 | Loss: 0.00001050
Iteration 41/1000 | Loss: 0.00001050
Iteration 42/1000 | Loss: 0.00001049
Iteration 43/1000 | Loss: 0.00001049
Iteration 44/1000 | Loss: 0.00001048
Iteration 45/1000 | Loss: 0.00001048
Iteration 46/1000 | Loss: 0.00001047
Iteration 47/1000 | Loss: 0.00001047
Iteration 48/1000 | Loss: 0.00001046
Iteration 49/1000 | Loss: 0.00001046
Iteration 50/1000 | Loss: 0.00001046
Iteration 51/1000 | Loss: 0.00001046
Iteration 52/1000 | Loss: 0.00001046
Iteration 53/1000 | Loss: 0.00001046
Iteration 54/1000 | Loss: 0.00001045
Iteration 55/1000 | Loss: 0.00001045
Iteration 56/1000 | Loss: 0.00001045
Iteration 57/1000 | Loss: 0.00001045
Iteration 58/1000 | Loss: 0.00001045
Iteration 59/1000 | Loss: 0.00001045
Iteration 60/1000 | Loss: 0.00001045
Iteration 61/1000 | Loss: 0.00001044
Iteration 62/1000 | Loss: 0.00001044
Iteration 63/1000 | Loss: 0.00001044
Iteration 64/1000 | Loss: 0.00001044
Iteration 65/1000 | Loss: 0.00001043
Iteration 66/1000 | Loss: 0.00001043
Iteration 67/1000 | Loss: 0.00001043
Iteration 68/1000 | Loss: 0.00001043
Iteration 69/1000 | Loss: 0.00001042
Iteration 70/1000 | Loss: 0.00001042
Iteration 71/1000 | Loss: 0.00001042
Iteration 72/1000 | Loss: 0.00001042
Iteration 73/1000 | Loss: 0.00001042
Iteration 74/1000 | Loss: 0.00001042
Iteration 75/1000 | Loss: 0.00001042
Iteration 76/1000 | Loss: 0.00001042
Iteration 77/1000 | Loss: 0.00001042
Iteration 78/1000 | Loss: 0.00001042
Iteration 79/1000 | Loss: 0.00001041
Iteration 80/1000 | Loss: 0.00001041
Iteration 81/1000 | Loss: 0.00001041
Iteration 82/1000 | Loss: 0.00001041
Iteration 83/1000 | Loss: 0.00001040
Iteration 84/1000 | Loss: 0.00001040
Iteration 85/1000 | Loss: 0.00001040
Iteration 86/1000 | Loss: 0.00001040
Iteration 87/1000 | Loss: 0.00001040
Iteration 88/1000 | Loss: 0.00001040
Iteration 89/1000 | Loss: 0.00001040
Iteration 90/1000 | Loss: 0.00001039
Iteration 91/1000 | Loss: 0.00001039
Iteration 92/1000 | Loss: 0.00001039
Iteration 93/1000 | Loss: 0.00001039
Iteration 94/1000 | Loss: 0.00001039
Iteration 95/1000 | Loss: 0.00001039
Iteration 96/1000 | Loss: 0.00001039
Iteration 97/1000 | Loss: 0.00001038
Iteration 98/1000 | Loss: 0.00001038
Iteration 99/1000 | Loss: 0.00001038
Iteration 100/1000 | Loss: 0.00001038
Iteration 101/1000 | Loss: 0.00001037
Iteration 102/1000 | Loss: 0.00001037
Iteration 103/1000 | Loss: 0.00001036
Iteration 104/1000 | Loss: 0.00001036
Iteration 105/1000 | Loss: 0.00001036
Iteration 106/1000 | Loss: 0.00001036
Iteration 107/1000 | Loss: 0.00001036
Iteration 108/1000 | Loss: 0.00001036
Iteration 109/1000 | Loss: 0.00001036
Iteration 110/1000 | Loss: 0.00001036
Iteration 111/1000 | Loss: 0.00001036
Iteration 112/1000 | Loss: 0.00001036
Iteration 113/1000 | Loss: 0.00001035
Iteration 114/1000 | Loss: 0.00001035
Iteration 115/1000 | Loss: 0.00001035
Iteration 116/1000 | Loss: 0.00001034
Iteration 117/1000 | Loss: 0.00001034
Iteration 118/1000 | Loss: 0.00001034
Iteration 119/1000 | Loss: 0.00001033
Iteration 120/1000 | Loss: 0.00001033
Iteration 121/1000 | Loss: 0.00001032
Iteration 122/1000 | Loss: 0.00001032
Iteration 123/1000 | Loss: 0.00001031
Iteration 124/1000 | Loss: 0.00001031
Iteration 125/1000 | Loss: 0.00001030
Iteration 126/1000 | Loss: 0.00001030
Iteration 127/1000 | Loss: 0.00001030
Iteration 128/1000 | Loss: 0.00001029
Iteration 129/1000 | Loss: 0.00001029
Iteration 130/1000 | Loss: 0.00001029
Iteration 131/1000 | Loss: 0.00001028
Iteration 132/1000 | Loss: 0.00001028
Iteration 133/1000 | Loss: 0.00001028
Iteration 134/1000 | Loss: 0.00001027
Iteration 135/1000 | Loss: 0.00001027
Iteration 136/1000 | Loss: 0.00001027
Iteration 137/1000 | Loss: 0.00001027
Iteration 138/1000 | Loss: 0.00001026
Iteration 139/1000 | Loss: 0.00001026
Iteration 140/1000 | Loss: 0.00001026
Iteration 141/1000 | Loss: 0.00001026
Iteration 142/1000 | Loss: 0.00001025
Iteration 143/1000 | Loss: 0.00001025
Iteration 144/1000 | Loss: 0.00001025
Iteration 145/1000 | Loss: 0.00001025
Iteration 146/1000 | Loss: 0.00001024
Iteration 147/1000 | Loss: 0.00001024
Iteration 148/1000 | Loss: 0.00001024
Iteration 149/1000 | Loss: 0.00001024
Iteration 150/1000 | Loss: 0.00001024
Iteration 151/1000 | Loss: 0.00001024
Iteration 152/1000 | Loss: 0.00001024
Iteration 153/1000 | Loss: 0.00001023
Iteration 154/1000 | Loss: 0.00001023
Iteration 155/1000 | Loss: 0.00001023
Iteration 156/1000 | Loss: 0.00001023
Iteration 157/1000 | Loss: 0.00001023
Iteration 158/1000 | Loss: 0.00001023
Iteration 159/1000 | Loss: 0.00001023
Iteration 160/1000 | Loss: 0.00001023
Iteration 161/1000 | Loss: 0.00001023
Iteration 162/1000 | Loss: 0.00001022
Iteration 163/1000 | Loss: 0.00001022
Iteration 164/1000 | Loss: 0.00001022
Iteration 165/1000 | Loss: 0.00001022
Iteration 166/1000 | Loss: 0.00001022
Iteration 167/1000 | Loss: 0.00001022
Iteration 168/1000 | Loss: 0.00001022
Iteration 169/1000 | Loss: 0.00001022
Iteration 170/1000 | Loss: 0.00001022
Iteration 171/1000 | Loss: 0.00001022
Iteration 172/1000 | Loss: 0.00001022
Iteration 173/1000 | Loss: 0.00001022
Iteration 174/1000 | Loss: 0.00001021
Iteration 175/1000 | Loss: 0.00001021
Iteration 176/1000 | Loss: 0.00001021
Iteration 177/1000 | Loss: 0.00001021
Iteration 178/1000 | Loss: 0.00001020
Iteration 179/1000 | Loss: 0.00001020
Iteration 180/1000 | Loss: 0.00001020
Iteration 181/1000 | Loss: 0.00001020
Iteration 182/1000 | Loss: 0.00001020
Iteration 183/1000 | Loss: 0.00001019
Iteration 184/1000 | Loss: 0.00001019
Iteration 185/1000 | Loss: 0.00001019
Iteration 186/1000 | Loss: 0.00001019
Iteration 187/1000 | Loss: 0.00001019
Iteration 188/1000 | Loss: 0.00001018
Iteration 189/1000 | Loss: 0.00001018
Iteration 190/1000 | Loss: 0.00001018
Iteration 191/1000 | Loss: 0.00001018
Iteration 192/1000 | Loss: 0.00001018
Iteration 193/1000 | Loss: 0.00001018
Iteration 194/1000 | Loss: 0.00001018
Iteration 195/1000 | Loss: 0.00001017
Iteration 196/1000 | Loss: 0.00001017
Iteration 197/1000 | Loss: 0.00001017
Iteration 198/1000 | Loss: 0.00001017
Iteration 199/1000 | Loss: 0.00001017
Iteration 200/1000 | Loss: 0.00001017
Iteration 201/1000 | Loss: 0.00001017
Iteration 202/1000 | Loss: 0.00001017
Iteration 203/1000 | Loss: 0.00001017
Iteration 204/1000 | Loss: 0.00001017
Iteration 205/1000 | Loss: 0.00001017
Iteration 206/1000 | Loss: 0.00001017
Iteration 207/1000 | Loss: 0.00001017
Iteration 208/1000 | Loss: 0.00001017
Iteration 209/1000 | Loss: 0.00001017
Iteration 210/1000 | Loss: 0.00001017
Iteration 211/1000 | Loss: 0.00001017
Iteration 212/1000 | Loss: 0.00001016
Iteration 213/1000 | Loss: 0.00001016
Iteration 214/1000 | Loss: 0.00001016
Iteration 215/1000 | Loss: 0.00001016
Iteration 216/1000 | Loss: 0.00001016
Iteration 217/1000 | Loss: 0.00001016
Iteration 218/1000 | Loss: 0.00001016
Iteration 219/1000 | Loss: 0.00001016
Iteration 220/1000 | Loss: 0.00001016
Iteration 221/1000 | Loss: 0.00001016
Iteration 222/1000 | Loss: 0.00001016
Iteration 223/1000 | Loss: 0.00001015
Iteration 224/1000 | Loss: 0.00001015
Iteration 225/1000 | Loss: 0.00001015
Iteration 226/1000 | Loss: 0.00001015
Iteration 227/1000 | Loss: 0.00001015
Iteration 228/1000 | Loss: 0.00001015
Iteration 229/1000 | Loss: 0.00001015
Iteration 230/1000 | Loss: 0.00001015
Iteration 231/1000 | Loss: 0.00001015
Iteration 232/1000 | Loss: 0.00001015
Iteration 233/1000 | Loss: 0.00001015
Iteration 234/1000 | Loss: 0.00001015
Iteration 235/1000 | Loss: 0.00001015
Iteration 236/1000 | Loss: 0.00001015
Iteration 237/1000 | Loss: 0.00001015
Iteration 238/1000 | Loss: 0.00001015
Iteration 239/1000 | Loss: 0.00001015
Iteration 240/1000 | Loss: 0.00001015
Iteration 241/1000 | Loss: 0.00001015
Iteration 242/1000 | Loss: 0.00001015
Iteration 243/1000 | Loss: 0.00001015
Iteration 244/1000 | Loss: 0.00001014
Iteration 245/1000 | Loss: 0.00001014
Iteration 246/1000 | Loss: 0.00001014
Iteration 247/1000 | Loss: 0.00001014
Iteration 248/1000 | Loss: 0.00001014
Iteration 249/1000 | Loss: 0.00001014
Iteration 250/1000 | Loss: 0.00001014
Iteration 251/1000 | Loss: 0.00001014
Iteration 252/1000 | Loss: 0.00001014
Iteration 253/1000 | Loss: 0.00001014
Iteration 254/1000 | Loss: 0.00001014
Iteration 255/1000 | Loss: 0.00001014
Iteration 256/1000 | Loss: 0.00001014
Iteration 257/1000 | Loss: 0.00001014
Iteration 258/1000 | Loss: 0.00001014
Iteration 259/1000 | Loss: 0.00001014
Iteration 260/1000 | Loss: 0.00001014
Iteration 261/1000 | Loss: 0.00001014
Iteration 262/1000 | Loss: 0.00001014
Iteration 263/1000 | Loss: 0.00001014
Iteration 264/1000 | Loss: 0.00001014
Iteration 265/1000 | Loss: 0.00001014
Iteration 266/1000 | Loss: 0.00001014
Iteration 267/1000 | Loss: 0.00001014
Iteration 268/1000 | Loss: 0.00001014
Iteration 269/1000 | Loss: 0.00001014
Iteration 270/1000 | Loss: 0.00001014
Iteration 271/1000 | Loss: 0.00001014
Iteration 272/1000 | Loss: 0.00001014
Iteration 273/1000 | Loss: 0.00001014
Iteration 274/1000 | Loss: 0.00001014
Iteration 275/1000 | Loss: 0.00001014
Iteration 276/1000 | Loss: 0.00001014
Iteration 277/1000 | Loss: 0.00001014
Iteration 278/1000 | Loss: 0.00001014
Iteration 279/1000 | Loss: 0.00001014
Iteration 280/1000 | Loss: 0.00001014
Iteration 281/1000 | Loss: 0.00001014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [1.0138445759366732e-05, 1.0138445759366732e-05, 1.0138445759366732e-05, 1.0138445759366732e-05, 1.0138445759366732e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0138445759366732e-05

Optimization complete. Final v2v error: 2.718017101287842 mm

Highest mean error: 3.626734495162964 mm for frame 74

Lowest mean error: 2.417428493499756 mm for frame 96

Saving results

Total time: 45.52795362472534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795609
Iteration 2/25 | Loss: 0.00132361
Iteration 3/25 | Loss: 0.00116822
Iteration 4/25 | Loss: 0.00115307
Iteration 5/25 | Loss: 0.00114828
Iteration 6/25 | Loss: 0.00114714
Iteration 7/25 | Loss: 0.00114714
Iteration 8/25 | Loss: 0.00114714
Iteration 9/25 | Loss: 0.00114714
Iteration 10/25 | Loss: 0.00114714
Iteration 11/25 | Loss: 0.00114714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011471415637061, 0.0011471415637061, 0.0011471415637061, 0.0011471415637061, 0.0011471415637061]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011471415637061

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26331913
Iteration 2/25 | Loss: 0.00066995
Iteration 3/25 | Loss: 0.00066989
Iteration 4/25 | Loss: 0.00066989
Iteration 5/25 | Loss: 0.00066989
Iteration 6/25 | Loss: 0.00066989
Iteration 7/25 | Loss: 0.00066989
Iteration 8/25 | Loss: 0.00066989
Iteration 9/25 | Loss: 0.00066989
Iteration 10/25 | Loss: 0.00066989
Iteration 11/25 | Loss: 0.00066988
Iteration 12/25 | Loss: 0.00066988
Iteration 13/25 | Loss: 0.00066988
Iteration 14/25 | Loss: 0.00066988
Iteration 15/25 | Loss: 0.00066988
Iteration 16/25 | Loss: 0.00066988
Iteration 17/25 | Loss: 0.00066988
Iteration 18/25 | Loss: 0.00066988
Iteration 19/25 | Loss: 0.00066988
Iteration 20/25 | Loss: 0.00066988
Iteration 21/25 | Loss: 0.00066988
Iteration 22/25 | Loss: 0.00066988
Iteration 23/25 | Loss: 0.00066988
Iteration 24/25 | Loss: 0.00066988
Iteration 25/25 | Loss: 0.00066988

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066988
Iteration 2/1000 | Loss: 0.00004842
Iteration 3/1000 | Loss: 0.00002776
Iteration 4/1000 | Loss: 0.00002246
Iteration 5/1000 | Loss: 0.00001991
Iteration 6/1000 | Loss: 0.00001896
Iteration 7/1000 | Loss: 0.00001809
Iteration 8/1000 | Loss: 0.00001747
Iteration 9/1000 | Loss: 0.00001694
Iteration 10/1000 | Loss: 0.00001656
Iteration 11/1000 | Loss: 0.00001639
Iteration 12/1000 | Loss: 0.00001620
Iteration 13/1000 | Loss: 0.00001605
Iteration 14/1000 | Loss: 0.00001597
Iteration 15/1000 | Loss: 0.00001589
Iteration 16/1000 | Loss: 0.00001582
Iteration 17/1000 | Loss: 0.00001582
Iteration 18/1000 | Loss: 0.00001581
Iteration 19/1000 | Loss: 0.00001581
Iteration 20/1000 | Loss: 0.00001581
Iteration 21/1000 | Loss: 0.00001580
Iteration 22/1000 | Loss: 0.00001580
Iteration 23/1000 | Loss: 0.00001578
Iteration 24/1000 | Loss: 0.00001578
Iteration 25/1000 | Loss: 0.00001577
Iteration 26/1000 | Loss: 0.00001577
Iteration 27/1000 | Loss: 0.00001576
Iteration 28/1000 | Loss: 0.00001575
Iteration 29/1000 | Loss: 0.00001575
Iteration 30/1000 | Loss: 0.00001573
Iteration 31/1000 | Loss: 0.00001573
Iteration 32/1000 | Loss: 0.00001573
Iteration 33/1000 | Loss: 0.00001573
Iteration 34/1000 | Loss: 0.00001573
Iteration 35/1000 | Loss: 0.00001573
Iteration 36/1000 | Loss: 0.00001572
Iteration 37/1000 | Loss: 0.00001572
Iteration 38/1000 | Loss: 0.00001570
Iteration 39/1000 | Loss: 0.00001570
Iteration 40/1000 | Loss: 0.00001570
Iteration 41/1000 | Loss: 0.00001570
Iteration 42/1000 | Loss: 0.00001570
Iteration 43/1000 | Loss: 0.00001569
Iteration 44/1000 | Loss: 0.00001569
Iteration 45/1000 | Loss: 0.00001568
Iteration 46/1000 | Loss: 0.00001568
Iteration 47/1000 | Loss: 0.00001567
Iteration 48/1000 | Loss: 0.00001567
Iteration 49/1000 | Loss: 0.00001567
Iteration 50/1000 | Loss: 0.00001567
Iteration 51/1000 | Loss: 0.00001566
Iteration 52/1000 | Loss: 0.00001566
Iteration 53/1000 | Loss: 0.00001566
Iteration 54/1000 | Loss: 0.00001565
Iteration 55/1000 | Loss: 0.00001565
Iteration 56/1000 | Loss: 0.00001565
Iteration 57/1000 | Loss: 0.00001564
Iteration 58/1000 | Loss: 0.00001563
Iteration 59/1000 | Loss: 0.00001563
Iteration 60/1000 | Loss: 0.00001562
Iteration 61/1000 | Loss: 0.00001562
Iteration 62/1000 | Loss: 0.00001561
Iteration 63/1000 | Loss: 0.00001561
Iteration 64/1000 | Loss: 0.00001560
Iteration 65/1000 | Loss: 0.00001560
Iteration 66/1000 | Loss: 0.00001560
Iteration 67/1000 | Loss: 0.00001560
Iteration 68/1000 | Loss: 0.00001560
Iteration 69/1000 | Loss: 0.00001560
Iteration 70/1000 | Loss: 0.00001560
Iteration 71/1000 | Loss: 0.00001560
Iteration 72/1000 | Loss: 0.00001560
Iteration 73/1000 | Loss: 0.00001560
Iteration 74/1000 | Loss: 0.00001559
Iteration 75/1000 | Loss: 0.00001559
Iteration 76/1000 | Loss: 0.00001559
Iteration 77/1000 | Loss: 0.00001559
Iteration 78/1000 | Loss: 0.00001558
Iteration 79/1000 | Loss: 0.00001558
Iteration 80/1000 | Loss: 0.00001558
Iteration 81/1000 | Loss: 0.00001557
Iteration 82/1000 | Loss: 0.00001557
Iteration 83/1000 | Loss: 0.00001557
Iteration 84/1000 | Loss: 0.00001557
Iteration 85/1000 | Loss: 0.00001557
Iteration 86/1000 | Loss: 0.00001557
Iteration 87/1000 | Loss: 0.00001557
Iteration 88/1000 | Loss: 0.00001556
Iteration 89/1000 | Loss: 0.00001556
Iteration 90/1000 | Loss: 0.00001555
Iteration 91/1000 | Loss: 0.00001555
Iteration 92/1000 | Loss: 0.00001555
Iteration 93/1000 | Loss: 0.00001555
Iteration 94/1000 | Loss: 0.00001555
Iteration 95/1000 | Loss: 0.00001555
Iteration 96/1000 | Loss: 0.00001555
Iteration 97/1000 | Loss: 0.00001555
Iteration 98/1000 | Loss: 0.00001554
Iteration 99/1000 | Loss: 0.00001554
Iteration 100/1000 | Loss: 0.00001554
Iteration 101/1000 | Loss: 0.00001554
Iteration 102/1000 | Loss: 0.00001554
Iteration 103/1000 | Loss: 0.00001554
Iteration 104/1000 | Loss: 0.00001554
Iteration 105/1000 | Loss: 0.00001554
Iteration 106/1000 | Loss: 0.00001553
Iteration 107/1000 | Loss: 0.00001553
Iteration 108/1000 | Loss: 0.00001553
Iteration 109/1000 | Loss: 0.00001553
Iteration 110/1000 | Loss: 0.00001553
Iteration 111/1000 | Loss: 0.00001553
Iteration 112/1000 | Loss: 0.00001552
Iteration 113/1000 | Loss: 0.00001552
Iteration 114/1000 | Loss: 0.00001552
Iteration 115/1000 | Loss: 0.00001552
Iteration 116/1000 | Loss: 0.00001552
Iteration 117/1000 | Loss: 0.00001552
Iteration 118/1000 | Loss: 0.00001551
Iteration 119/1000 | Loss: 0.00001551
Iteration 120/1000 | Loss: 0.00001551
Iteration 121/1000 | Loss: 0.00001551
Iteration 122/1000 | Loss: 0.00001551
Iteration 123/1000 | Loss: 0.00001551
Iteration 124/1000 | Loss: 0.00001551
Iteration 125/1000 | Loss: 0.00001551
Iteration 126/1000 | Loss: 0.00001550
Iteration 127/1000 | Loss: 0.00001550
Iteration 128/1000 | Loss: 0.00001550
Iteration 129/1000 | Loss: 0.00001550
Iteration 130/1000 | Loss: 0.00001550
Iteration 131/1000 | Loss: 0.00001550
Iteration 132/1000 | Loss: 0.00001549
Iteration 133/1000 | Loss: 0.00001549
Iteration 134/1000 | Loss: 0.00001549
Iteration 135/1000 | Loss: 0.00001549
Iteration 136/1000 | Loss: 0.00001549
Iteration 137/1000 | Loss: 0.00001549
Iteration 138/1000 | Loss: 0.00001549
Iteration 139/1000 | Loss: 0.00001549
Iteration 140/1000 | Loss: 0.00001548
Iteration 141/1000 | Loss: 0.00001548
Iteration 142/1000 | Loss: 0.00001548
Iteration 143/1000 | Loss: 0.00001548
Iteration 144/1000 | Loss: 0.00001548
Iteration 145/1000 | Loss: 0.00001548
Iteration 146/1000 | Loss: 0.00001548
Iteration 147/1000 | Loss: 0.00001547
Iteration 148/1000 | Loss: 0.00001547
Iteration 149/1000 | Loss: 0.00001547
Iteration 150/1000 | Loss: 0.00001547
Iteration 151/1000 | Loss: 0.00001546
Iteration 152/1000 | Loss: 0.00001546
Iteration 153/1000 | Loss: 0.00001546
Iteration 154/1000 | Loss: 0.00001546
Iteration 155/1000 | Loss: 0.00001546
Iteration 156/1000 | Loss: 0.00001546
Iteration 157/1000 | Loss: 0.00001546
Iteration 158/1000 | Loss: 0.00001546
Iteration 159/1000 | Loss: 0.00001546
Iteration 160/1000 | Loss: 0.00001545
Iteration 161/1000 | Loss: 0.00001545
Iteration 162/1000 | Loss: 0.00001545
Iteration 163/1000 | Loss: 0.00001545
Iteration 164/1000 | Loss: 0.00001545
Iteration 165/1000 | Loss: 0.00001545
Iteration 166/1000 | Loss: 0.00001545
Iteration 167/1000 | Loss: 0.00001545
Iteration 168/1000 | Loss: 0.00001545
Iteration 169/1000 | Loss: 0.00001545
Iteration 170/1000 | Loss: 0.00001545
Iteration 171/1000 | Loss: 0.00001544
Iteration 172/1000 | Loss: 0.00001544
Iteration 173/1000 | Loss: 0.00001544
Iteration 174/1000 | Loss: 0.00001544
Iteration 175/1000 | Loss: 0.00001544
Iteration 176/1000 | Loss: 0.00001544
Iteration 177/1000 | Loss: 0.00001544
Iteration 178/1000 | Loss: 0.00001543
Iteration 179/1000 | Loss: 0.00001543
Iteration 180/1000 | Loss: 0.00001543
Iteration 181/1000 | Loss: 0.00001543
Iteration 182/1000 | Loss: 0.00001543
Iteration 183/1000 | Loss: 0.00001543
Iteration 184/1000 | Loss: 0.00001543
Iteration 185/1000 | Loss: 0.00001543
Iteration 186/1000 | Loss: 0.00001543
Iteration 187/1000 | Loss: 0.00001542
Iteration 188/1000 | Loss: 0.00001542
Iteration 189/1000 | Loss: 0.00001542
Iteration 190/1000 | Loss: 0.00001542
Iteration 191/1000 | Loss: 0.00001541
Iteration 192/1000 | Loss: 0.00001541
Iteration 193/1000 | Loss: 0.00001541
Iteration 194/1000 | Loss: 0.00001541
Iteration 195/1000 | Loss: 0.00001541
Iteration 196/1000 | Loss: 0.00001541
Iteration 197/1000 | Loss: 0.00001541
Iteration 198/1000 | Loss: 0.00001541
Iteration 199/1000 | Loss: 0.00001541
Iteration 200/1000 | Loss: 0.00001541
Iteration 201/1000 | Loss: 0.00001541
Iteration 202/1000 | Loss: 0.00001541
Iteration 203/1000 | Loss: 0.00001541
Iteration 204/1000 | Loss: 0.00001540
Iteration 205/1000 | Loss: 0.00001540
Iteration 206/1000 | Loss: 0.00001540
Iteration 207/1000 | Loss: 0.00001540
Iteration 208/1000 | Loss: 0.00001540
Iteration 209/1000 | Loss: 0.00001539
Iteration 210/1000 | Loss: 0.00001539
Iteration 211/1000 | Loss: 0.00001539
Iteration 212/1000 | Loss: 0.00001539
Iteration 213/1000 | Loss: 0.00001539
Iteration 214/1000 | Loss: 0.00001539
Iteration 215/1000 | Loss: 0.00001539
Iteration 216/1000 | Loss: 0.00001539
Iteration 217/1000 | Loss: 0.00001539
Iteration 218/1000 | Loss: 0.00001539
Iteration 219/1000 | Loss: 0.00001539
Iteration 220/1000 | Loss: 0.00001539
Iteration 221/1000 | Loss: 0.00001539
Iteration 222/1000 | Loss: 0.00001539
Iteration 223/1000 | Loss: 0.00001539
Iteration 224/1000 | Loss: 0.00001538
Iteration 225/1000 | Loss: 0.00001538
Iteration 226/1000 | Loss: 0.00001538
Iteration 227/1000 | Loss: 0.00001538
Iteration 228/1000 | Loss: 0.00001538
Iteration 229/1000 | Loss: 0.00001538
Iteration 230/1000 | Loss: 0.00001538
Iteration 231/1000 | Loss: 0.00001538
Iteration 232/1000 | Loss: 0.00001538
Iteration 233/1000 | Loss: 0.00001538
Iteration 234/1000 | Loss: 0.00001538
Iteration 235/1000 | Loss: 0.00001538
Iteration 236/1000 | Loss: 0.00001538
Iteration 237/1000 | Loss: 0.00001538
Iteration 238/1000 | Loss: 0.00001538
Iteration 239/1000 | Loss: 0.00001538
Iteration 240/1000 | Loss: 0.00001538
Iteration 241/1000 | Loss: 0.00001538
Iteration 242/1000 | Loss: 0.00001538
Iteration 243/1000 | Loss: 0.00001538
Iteration 244/1000 | Loss: 0.00001538
Iteration 245/1000 | Loss: 0.00001538
Iteration 246/1000 | Loss: 0.00001538
Iteration 247/1000 | Loss: 0.00001538
Iteration 248/1000 | Loss: 0.00001538
Iteration 249/1000 | Loss: 0.00001538
Iteration 250/1000 | Loss: 0.00001538
Iteration 251/1000 | Loss: 0.00001538
Iteration 252/1000 | Loss: 0.00001538
Iteration 253/1000 | Loss: 0.00001538
Iteration 254/1000 | Loss: 0.00001538
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [1.5379817341454327e-05, 1.5379817341454327e-05, 1.5379817341454327e-05, 1.5379817341454327e-05, 1.5379817341454327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5379817341454327e-05

Optimization complete. Final v2v error: 3.250532627105713 mm

Highest mean error: 4.478524684906006 mm for frame 52

Lowest mean error: 2.6104466915130615 mm for frame 83

Saving results

Total time: 45.4576313495636
