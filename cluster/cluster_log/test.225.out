Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=225, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12600-12655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836336
Iteration 2/25 | Loss: 0.00163325
Iteration 3/25 | Loss: 0.00138243
Iteration 4/25 | Loss: 0.00134633
Iteration 5/25 | Loss: 0.00133650
Iteration 6/25 | Loss: 0.00135951
Iteration 7/25 | Loss: 0.00128782
Iteration 8/25 | Loss: 0.00127096
Iteration 9/25 | Loss: 0.00123975
Iteration 10/25 | Loss: 0.00123606
Iteration 11/25 | Loss: 0.00123478
Iteration 12/25 | Loss: 0.00123347
Iteration 13/25 | Loss: 0.00123324
Iteration 14/25 | Loss: 0.00123323
Iteration 15/25 | Loss: 0.00123323
Iteration 16/25 | Loss: 0.00123323
Iteration 17/25 | Loss: 0.00123323
Iteration 18/25 | Loss: 0.00123323
Iteration 19/25 | Loss: 0.00123323
Iteration 20/25 | Loss: 0.00123322
Iteration 21/25 | Loss: 0.00123322
Iteration 22/25 | Loss: 0.00123322
Iteration 23/25 | Loss: 0.00123322
Iteration 24/25 | Loss: 0.00123322
Iteration 25/25 | Loss: 0.00123322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87378597
Iteration 2/25 | Loss: 0.00085332
Iteration 3/25 | Loss: 0.00085331
Iteration 4/25 | Loss: 0.00085331
Iteration 5/25 | Loss: 0.00085331
Iteration 6/25 | Loss: 0.00085331
Iteration 7/25 | Loss: 0.00085331
Iteration 8/25 | Loss: 0.00085331
Iteration 9/25 | Loss: 0.00085331
Iteration 10/25 | Loss: 0.00085331
Iteration 11/25 | Loss: 0.00085331
Iteration 12/25 | Loss: 0.00085331
Iteration 13/25 | Loss: 0.00085331
Iteration 14/25 | Loss: 0.00085331
Iteration 15/25 | Loss: 0.00085331
Iteration 16/25 | Loss: 0.00085331
Iteration 17/25 | Loss: 0.00085331
Iteration 18/25 | Loss: 0.00085331
Iteration 19/25 | Loss: 0.00085331
Iteration 20/25 | Loss: 0.00085331
Iteration 21/25 | Loss: 0.00085331
Iteration 22/25 | Loss: 0.00085331
Iteration 23/25 | Loss: 0.00085331
Iteration 24/25 | Loss: 0.00085331
Iteration 25/25 | Loss: 0.00085331

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085331
Iteration 2/1000 | Loss: 0.00003274
Iteration 3/1000 | Loss: 0.00002486
Iteration 4/1000 | Loss: 0.00002247
Iteration 5/1000 | Loss: 0.00002153
Iteration 6/1000 | Loss: 0.00002103
Iteration 7/1000 | Loss: 0.00002037
Iteration 8/1000 | Loss: 0.00002001
Iteration 9/1000 | Loss: 0.00001960
Iteration 10/1000 | Loss: 0.00001925
Iteration 11/1000 | Loss: 0.00001908
Iteration 12/1000 | Loss: 0.00001907
Iteration 13/1000 | Loss: 0.00001906
Iteration 14/1000 | Loss: 0.00001900
Iteration 15/1000 | Loss: 0.00001895
Iteration 16/1000 | Loss: 0.00001888
Iteration 17/1000 | Loss: 0.00001887
Iteration 18/1000 | Loss: 0.00001887
Iteration 19/1000 | Loss: 0.00001886
Iteration 20/1000 | Loss: 0.00001886
Iteration 21/1000 | Loss: 0.00001885
Iteration 22/1000 | Loss: 0.00001884
Iteration 23/1000 | Loss: 0.00001882
Iteration 24/1000 | Loss: 0.00001873
Iteration 25/1000 | Loss: 0.00001871
Iteration 26/1000 | Loss: 0.00001871
Iteration 27/1000 | Loss: 0.00001870
Iteration 28/1000 | Loss: 0.00001870
Iteration 29/1000 | Loss: 0.00001870
Iteration 30/1000 | Loss: 0.00001870
Iteration 31/1000 | Loss: 0.00001870
Iteration 32/1000 | Loss: 0.00001870
Iteration 33/1000 | Loss: 0.00001869
Iteration 34/1000 | Loss: 0.00001869
Iteration 35/1000 | Loss: 0.00001868
Iteration 36/1000 | Loss: 0.00001868
Iteration 37/1000 | Loss: 0.00001867
Iteration 38/1000 | Loss: 0.00001866
Iteration 39/1000 | Loss: 0.00001866
Iteration 40/1000 | Loss: 0.00001866
Iteration 41/1000 | Loss: 0.00001866
Iteration 42/1000 | Loss: 0.00001866
Iteration 43/1000 | Loss: 0.00001866
Iteration 44/1000 | Loss: 0.00001865
Iteration 45/1000 | Loss: 0.00001865
Iteration 46/1000 | Loss: 0.00001865
Iteration 47/1000 | Loss: 0.00001865
Iteration 48/1000 | Loss: 0.00001864
Iteration 49/1000 | Loss: 0.00001863
Iteration 50/1000 | Loss: 0.00001863
Iteration 51/1000 | Loss: 0.00001862
Iteration 52/1000 | Loss: 0.00001862
Iteration 53/1000 | Loss: 0.00001862
Iteration 54/1000 | Loss: 0.00001862
Iteration 55/1000 | Loss: 0.00001861
Iteration 56/1000 | Loss: 0.00001861
Iteration 57/1000 | Loss: 0.00001861
Iteration 58/1000 | Loss: 0.00001860
Iteration 59/1000 | Loss: 0.00001860
Iteration 60/1000 | Loss: 0.00001860
Iteration 61/1000 | Loss: 0.00001860
Iteration 62/1000 | Loss: 0.00001860
Iteration 63/1000 | Loss: 0.00001860
Iteration 64/1000 | Loss: 0.00001860
Iteration 65/1000 | Loss: 0.00001860
Iteration 66/1000 | Loss: 0.00001860
Iteration 67/1000 | Loss: 0.00001860
Iteration 68/1000 | Loss: 0.00001860
Iteration 69/1000 | Loss: 0.00001860
Iteration 70/1000 | Loss: 0.00001859
Iteration 71/1000 | Loss: 0.00001859
Iteration 72/1000 | Loss: 0.00001859
Iteration 73/1000 | Loss: 0.00001859
Iteration 74/1000 | Loss: 0.00001858
Iteration 75/1000 | Loss: 0.00001858
Iteration 76/1000 | Loss: 0.00001858
Iteration 77/1000 | Loss: 0.00001858
Iteration 78/1000 | Loss: 0.00001858
Iteration 79/1000 | Loss: 0.00001857
Iteration 80/1000 | Loss: 0.00001857
Iteration 81/1000 | Loss: 0.00001857
Iteration 82/1000 | Loss: 0.00001857
Iteration 83/1000 | Loss: 0.00001857
Iteration 84/1000 | Loss: 0.00001857
Iteration 85/1000 | Loss: 0.00001857
Iteration 86/1000 | Loss: 0.00001857
Iteration 87/1000 | Loss: 0.00001857
Iteration 88/1000 | Loss: 0.00001857
Iteration 89/1000 | Loss: 0.00001857
Iteration 90/1000 | Loss: 0.00001856
Iteration 91/1000 | Loss: 0.00001856
Iteration 92/1000 | Loss: 0.00001856
Iteration 93/1000 | Loss: 0.00001856
Iteration 94/1000 | Loss: 0.00001856
Iteration 95/1000 | Loss: 0.00001856
Iteration 96/1000 | Loss: 0.00001856
Iteration 97/1000 | Loss: 0.00001856
Iteration 98/1000 | Loss: 0.00001855
Iteration 99/1000 | Loss: 0.00001855
Iteration 100/1000 | Loss: 0.00001855
Iteration 101/1000 | Loss: 0.00001855
Iteration 102/1000 | Loss: 0.00001855
Iteration 103/1000 | Loss: 0.00001855
Iteration 104/1000 | Loss: 0.00001855
Iteration 105/1000 | Loss: 0.00001855
Iteration 106/1000 | Loss: 0.00001855
Iteration 107/1000 | Loss: 0.00001855
Iteration 108/1000 | Loss: 0.00001855
Iteration 109/1000 | Loss: 0.00001854
Iteration 110/1000 | Loss: 0.00001854
Iteration 111/1000 | Loss: 0.00001854
Iteration 112/1000 | Loss: 0.00001854
Iteration 113/1000 | Loss: 0.00001854
Iteration 114/1000 | Loss: 0.00001854
Iteration 115/1000 | Loss: 0.00001854
Iteration 116/1000 | Loss: 0.00001854
Iteration 117/1000 | Loss: 0.00001854
Iteration 118/1000 | Loss: 0.00001854
Iteration 119/1000 | Loss: 0.00001854
Iteration 120/1000 | Loss: 0.00001853
Iteration 121/1000 | Loss: 0.00001853
Iteration 122/1000 | Loss: 0.00001853
Iteration 123/1000 | Loss: 0.00001853
Iteration 124/1000 | Loss: 0.00001853
Iteration 125/1000 | Loss: 0.00001852
Iteration 126/1000 | Loss: 0.00001852
Iteration 127/1000 | Loss: 0.00001852
Iteration 128/1000 | Loss: 0.00001852
Iteration 129/1000 | Loss: 0.00001851
Iteration 130/1000 | Loss: 0.00001851
Iteration 131/1000 | Loss: 0.00001851
Iteration 132/1000 | Loss: 0.00001850
Iteration 133/1000 | Loss: 0.00001850
Iteration 134/1000 | Loss: 0.00001850
Iteration 135/1000 | Loss: 0.00001850
Iteration 136/1000 | Loss: 0.00001850
Iteration 137/1000 | Loss: 0.00001850
Iteration 138/1000 | Loss: 0.00001850
Iteration 139/1000 | Loss: 0.00001850
Iteration 140/1000 | Loss: 0.00001850
Iteration 141/1000 | Loss: 0.00001850
Iteration 142/1000 | Loss: 0.00001850
Iteration 143/1000 | Loss: 0.00001850
Iteration 144/1000 | Loss: 0.00001850
Iteration 145/1000 | Loss: 0.00001850
Iteration 146/1000 | Loss: 0.00001850
Iteration 147/1000 | Loss: 0.00001850
Iteration 148/1000 | Loss: 0.00001850
Iteration 149/1000 | Loss: 0.00001850
Iteration 150/1000 | Loss: 0.00001850
Iteration 151/1000 | Loss: 0.00001850
Iteration 152/1000 | Loss: 0.00001850
Iteration 153/1000 | Loss: 0.00001850
Iteration 154/1000 | Loss: 0.00001850
Iteration 155/1000 | Loss: 0.00001850
Iteration 156/1000 | Loss: 0.00001850
Iteration 157/1000 | Loss: 0.00001850
Iteration 158/1000 | Loss: 0.00001850
Iteration 159/1000 | Loss: 0.00001850
Iteration 160/1000 | Loss: 0.00001850
Iteration 161/1000 | Loss: 0.00001850
Iteration 162/1000 | Loss: 0.00001850
Iteration 163/1000 | Loss: 0.00001850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.8498250938137062e-05, 1.8498250938137062e-05, 1.8498250938137062e-05, 1.8498250938137062e-05, 1.8498250938137062e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8498250938137062e-05

Optimization complete. Final v2v error: 3.6586010456085205 mm

Highest mean error: 3.841625213623047 mm for frame 19

Lowest mean error: 3.5316295623779297 mm for frame 0

Saving results

Total time: 52.86422395706177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470596
Iteration 2/25 | Loss: 0.00135976
Iteration 3/25 | Loss: 0.00127674
Iteration 4/25 | Loss: 0.00126057
Iteration 5/25 | Loss: 0.00125747
Iteration 6/25 | Loss: 0.00125747
Iteration 7/25 | Loss: 0.00125747
Iteration 8/25 | Loss: 0.00125747
Iteration 9/25 | Loss: 0.00125747
Iteration 10/25 | Loss: 0.00125747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012574699940159917, 0.0012574699940159917, 0.0012574699940159917, 0.0012574699940159917, 0.0012574699940159917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012574699940159917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27511036
Iteration 2/25 | Loss: 0.00136654
Iteration 3/25 | Loss: 0.00136653
Iteration 4/25 | Loss: 0.00136653
Iteration 5/25 | Loss: 0.00136653
Iteration 6/25 | Loss: 0.00136653
Iteration 7/25 | Loss: 0.00136653
Iteration 8/25 | Loss: 0.00136653
Iteration 9/25 | Loss: 0.00136653
Iteration 10/25 | Loss: 0.00136653
Iteration 11/25 | Loss: 0.00136653
Iteration 12/25 | Loss: 0.00136653
Iteration 13/25 | Loss: 0.00136653
Iteration 14/25 | Loss: 0.00136653
Iteration 15/25 | Loss: 0.00136653
Iteration 16/25 | Loss: 0.00136653
Iteration 17/25 | Loss: 0.00136653
Iteration 18/25 | Loss: 0.00136653
Iteration 19/25 | Loss: 0.00136653
Iteration 20/25 | Loss: 0.00136653
Iteration 21/25 | Loss: 0.00136653
Iteration 22/25 | Loss: 0.00136653
Iteration 23/25 | Loss: 0.00136653
Iteration 24/25 | Loss: 0.00136653
Iteration 25/25 | Loss: 0.00136653

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136653
Iteration 2/1000 | Loss: 0.00002191
Iteration 3/1000 | Loss: 0.00001744
Iteration 4/1000 | Loss: 0.00001654
Iteration 5/1000 | Loss: 0.00001593
Iteration 6/1000 | Loss: 0.00001550
Iteration 7/1000 | Loss: 0.00001525
Iteration 8/1000 | Loss: 0.00001492
Iteration 9/1000 | Loss: 0.00001469
Iteration 10/1000 | Loss: 0.00001466
Iteration 11/1000 | Loss: 0.00001462
Iteration 12/1000 | Loss: 0.00001461
Iteration 13/1000 | Loss: 0.00001452
Iteration 14/1000 | Loss: 0.00001438
Iteration 15/1000 | Loss: 0.00001422
Iteration 16/1000 | Loss: 0.00001413
Iteration 17/1000 | Loss: 0.00001411
Iteration 18/1000 | Loss: 0.00001408
Iteration 19/1000 | Loss: 0.00001405
Iteration 20/1000 | Loss: 0.00001400
Iteration 21/1000 | Loss: 0.00001395
Iteration 22/1000 | Loss: 0.00001395
Iteration 23/1000 | Loss: 0.00001395
Iteration 24/1000 | Loss: 0.00001395
Iteration 25/1000 | Loss: 0.00001391
Iteration 26/1000 | Loss: 0.00001389
Iteration 27/1000 | Loss: 0.00001388
Iteration 28/1000 | Loss: 0.00001388
Iteration 29/1000 | Loss: 0.00001385
Iteration 30/1000 | Loss: 0.00001385
Iteration 31/1000 | Loss: 0.00001384
Iteration 32/1000 | Loss: 0.00001384
Iteration 33/1000 | Loss: 0.00001384
Iteration 34/1000 | Loss: 0.00001384
Iteration 35/1000 | Loss: 0.00001383
Iteration 36/1000 | Loss: 0.00001383
Iteration 37/1000 | Loss: 0.00001382
Iteration 38/1000 | Loss: 0.00001382
Iteration 39/1000 | Loss: 0.00001381
Iteration 40/1000 | Loss: 0.00001381
Iteration 41/1000 | Loss: 0.00001381
Iteration 42/1000 | Loss: 0.00001380
Iteration 43/1000 | Loss: 0.00001380
Iteration 44/1000 | Loss: 0.00001380
Iteration 45/1000 | Loss: 0.00001380
Iteration 46/1000 | Loss: 0.00001380
Iteration 47/1000 | Loss: 0.00001379
Iteration 48/1000 | Loss: 0.00001379
Iteration 49/1000 | Loss: 0.00001378
Iteration 50/1000 | Loss: 0.00001378
Iteration 51/1000 | Loss: 0.00001378
Iteration 52/1000 | Loss: 0.00001378
Iteration 53/1000 | Loss: 0.00001378
Iteration 54/1000 | Loss: 0.00001377
Iteration 55/1000 | Loss: 0.00001377
Iteration 56/1000 | Loss: 0.00001377
Iteration 57/1000 | Loss: 0.00001377
Iteration 58/1000 | Loss: 0.00001377
Iteration 59/1000 | Loss: 0.00001377
Iteration 60/1000 | Loss: 0.00001377
Iteration 61/1000 | Loss: 0.00001377
Iteration 62/1000 | Loss: 0.00001377
Iteration 63/1000 | Loss: 0.00001376
Iteration 64/1000 | Loss: 0.00001376
Iteration 65/1000 | Loss: 0.00001376
Iteration 66/1000 | Loss: 0.00001375
Iteration 67/1000 | Loss: 0.00001375
Iteration 68/1000 | Loss: 0.00001375
Iteration 69/1000 | Loss: 0.00001375
Iteration 70/1000 | Loss: 0.00001375
Iteration 71/1000 | Loss: 0.00001375
Iteration 72/1000 | Loss: 0.00001375
Iteration 73/1000 | Loss: 0.00001375
Iteration 74/1000 | Loss: 0.00001375
Iteration 75/1000 | Loss: 0.00001374
Iteration 76/1000 | Loss: 0.00001374
Iteration 77/1000 | Loss: 0.00001374
Iteration 78/1000 | Loss: 0.00001374
Iteration 79/1000 | Loss: 0.00001374
Iteration 80/1000 | Loss: 0.00001374
Iteration 81/1000 | Loss: 0.00001374
Iteration 82/1000 | Loss: 0.00001374
Iteration 83/1000 | Loss: 0.00001373
Iteration 84/1000 | Loss: 0.00001373
Iteration 85/1000 | Loss: 0.00001373
Iteration 86/1000 | Loss: 0.00001373
Iteration 87/1000 | Loss: 0.00001373
Iteration 88/1000 | Loss: 0.00001373
Iteration 89/1000 | Loss: 0.00001373
Iteration 90/1000 | Loss: 0.00001373
Iteration 91/1000 | Loss: 0.00001373
Iteration 92/1000 | Loss: 0.00001373
Iteration 93/1000 | Loss: 0.00001373
Iteration 94/1000 | Loss: 0.00001373
Iteration 95/1000 | Loss: 0.00001373
Iteration 96/1000 | Loss: 0.00001373
Iteration 97/1000 | Loss: 0.00001373
Iteration 98/1000 | Loss: 0.00001373
Iteration 99/1000 | Loss: 0.00001373
Iteration 100/1000 | Loss: 0.00001373
Iteration 101/1000 | Loss: 0.00001373
Iteration 102/1000 | Loss: 0.00001373
Iteration 103/1000 | Loss: 0.00001373
Iteration 104/1000 | Loss: 0.00001373
Iteration 105/1000 | Loss: 0.00001373
Iteration 106/1000 | Loss: 0.00001373
Iteration 107/1000 | Loss: 0.00001373
Iteration 108/1000 | Loss: 0.00001373
Iteration 109/1000 | Loss: 0.00001373
Iteration 110/1000 | Loss: 0.00001373
Iteration 111/1000 | Loss: 0.00001373
Iteration 112/1000 | Loss: 0.00001373
Iteration 113/1000 | Loss: 0.00001373
Iteration 114/1000 | Loss: 0.00001373
Iteration 115/1000 | Loss: 0.00001373
Iteration 116/1000 | Loss: 0.00001373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.3732401384913828e-05, 1.3732401384913828e-05, 1.3732401384913828e-05, 1.3732401384913828e-05, 1.3732401384913828e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3732401384913828e-05

Optimization complete. Final v2v error: 3.1189305782318115 mm

Highest mean error: 3.5146422386169434 mm for frame 147

Lowest mean error: 2.9028663635253906 mm for frame 121

Saving results

Total time: 38.50514578819275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970692
Iteration 2/25 | Loss: 0.00269878
Iteration 3/25 | Loss: 0.00173149
Iteration 4/25 | Loss: 0.00162863
Iteration 5/25 | Loss: 0.00167195
Iteration 6/25 | Loss: 0.00149702
Iteration 7/25 | Loss: 0.00137355
Iteration 8/25 | Loss: 0.00130005
Iteration 9/25 | Loss: 0.00127322
Iteration 10/25 | Loss: 0.00129325
Iteration 11/25 | Loss: 0.00124862
Iteration 12/25 | Loss: 0.00122046
Iteration 13/25 | Loss: 0.00121441
Iteration 14/25 | Loss: 0.00121386
Iteration 15/25 | Loss: 0.00121617
Iteration 16/25 | Loss: 0.00121369
Iteration 17/25 | Loss: 0.00121369
Iteration 18/25 | Loss: 0.00121369
Iteration 19/25 | Loss: 0.00121369
Iteration 20/25 | Loss: 0.00121369
Iteration 21/25 | Loss: 0.00121369
Iteration 22/25 | Loss: 0.00121368
Iteration 23/25 | Loss: 0.00121368
Iteration 24/25 | Loss: 0.00121368
Iteration 25/25 | Loss: 0.00121368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28230977
Iteration 2/25 | Loss: 0.00170542
Iteration 3/25 | Loss: 0.00139751
Iteration 4/25 | Loss: 0.00139751
Iteration 5/25 | Loss: 0.00139751
Iteration 6/25 | Loss: 0.00139751
Iteration 7/25 | Loss: 0.00139751
Iteration 8/25 | Loss: 0.00139751
Iteration 9/25 | Loss: 0.00139751
Iteration 10/25 | Loss: 0.00139751
Iteration 11/25 | Loss: 0.00139751
Iteration 12/25 | Loss: 0.00139751
Iteration 13/25 | Loss: 0.00139751
Iteration 14/25 | Loss: 0.00139751
Iteration 15/25 | Loss: 0.00139751
Iteration 16/25 | Loss: 0.00139751
Iteration 17/25 | Loss: 0.00139751
Iteration 18/25 | Loss: 0.00139751
Iteration 19/25 | Loss: 0.00139751
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013975065667182207, 0.0013975065667182207, 0.0013975065667182207, 0.0013975065667182207, 0.0013975065667182207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013975065667182207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139751
Iteration 2/1000 | Loss: 0.00018230
Iteration 3/1000 | Loss: 0.00073111
Iteration 4/1000 | Loss: 0.00023260
Iteration 5/1000 | Loss: 0.00001905
Iteration 6/1000 | Loss: 0.00017151
Iteration 7/1000 | Loss: 0.00004232
Iteration 8/1000 | Loss: 0.00007596
Iteration 9/1000 | Loss: 0.00003369
Iteration 10/1000 | Loss: 0.00002196
Iteration 11/1000 | Loss: 0.00005995
Iteration 12/1000 | Loss: 0.00004393
Iteration 13/1000 | Loss: 0.00026916
Iteration 14/1000 | Loss: 0.00007256
Iteration 15/1000 | Loss: 0.00001768
Iteration 16/1000 | Loss: 0.00010086
Iteration 17/1000 | Loss: 0.00002105
Iteration 18/1000 | Loss: 0.00002878
Iteration 19/1000 | Loss: 0.00001604
Iteration 20/1000 | Loss: 0.00002916
Iteration 21/1000 | Loss: 0.00004404
Iteration 22/1000 | Loss: 0.00005546
Iteration 23/1000 | Loss: 0.00001876
Iteration 24/1000 | Loss: 0.00002058
Iteration 25/1000 | Loss: 0.00001880
Iteration 26/1000 | Loss: 0.00002082
Iteration 27/1000 | Loss: 0.00002090
Iteration 28/1000 | Loss: 0.00001395
Iteration 29/1000 | Loss: 0.00001392
Iteration 30/1000 | Loss: 0.00001392
Iteration 31/1000 | Loss: 0.00001392
Iteration 32/1000 | Loss: 0.00001392
Iteration 33/1000 | Loss: 0.00001392
Iteration 34/1000 | Loss: 0.00001392
Iteration 35/1000 | Loss: 0.00001585
Iteration 36/1000 | Loss: 0.00001388
Iteration 37/1000 | Loss: 0.00001388
Iteration 38/1000 | Loss: 0.00001388
Iteration 39/1000 | Loss: 0.00001388
Iteration 40/1000 | Loss: 0.00001388
Iteration 41/1000 | Loss: 0.00001388
Iteration 42/1000 | Loss: 0.00002275
Iteration 43/1000 | Loss: 0.00007132
Iteration 44/1000 | Loss: 0.00001389
Iteration 45/1000 | Loss: 0.00001378
Iteration 46/1000 | Loss: 0.00001377
Iteration 47/1000 | Loss: 0.00001377
Iteration 48/1000 | Loss: 0.00001377
Iteration 49/1000 | Loss: 0.00001377
Iteration 50/1000 | Loss: 0.00001377
Iteration 51/1000 | Loss: 0.00001376
Iteration 52/1000 | Loss: 0.00001376
Iteration 53/1000 | Loss: 0.00001376
Iteration 54/1000 | Loss: 0.00001376
Iteration 55/1000 | Loss: 0.00001376
Iteration 56/1000 | Loss: 0.00001375
Iteration 57/1000 | Loss: 0.00001375
Iteration 58/1000 | Loss: 0.00001375
Iteration 59/1000 | Loss: 0.00001375
Iteration 60/1000 | Loss: 0.00001375
Iteration 61/1000 | Loss: 0.00001375
Iteration 62/1000 | Loss: 0.00001374
Iteration 63/1000 | Loss: 0.00001374
Iteration 64/1000 | Loss: 0.00001374
Iteration 65/1000 | Loss: 0.00001374
Iteration 66/1000 | Loss: 0.00001373
Iteration 67/1000 | Loss: 0.00001373
Iteration 68/1000 | Loss: 0.00001373
Iteration 69/1000 | Loss: 0.00001372
Iteration 70/1000 | Loss: 0.00001372
Iteration 71/1000 | Loss: 0.00001372
Iteration 72/1000 | Loss: 0.00001372
Iteration 73/1000 | Loss: 0.00001371
Iteration 74/1000 | Loss: 0.00001371
Iteration 75/1000 | Loss: 0.00001371
Iteration 76/1000 | Loss: 0.00001371
Iteration 77/1000 | Loss: 0.00001370
Iteration 78/1000 | Loss: 0.00001370
Iteration 79/1000 | Loss: 0.00001370
Iteration 80/1000 | Loss: 0.00001370
Iteration 81/1000 | Loss: 0.00001370
Iteration 82/1000 | Loss: 0.00001370
Iteration 83/1000 | Loss: 0.00001370
Iteration 84/1000 | Loss: 0.00001370
Iteration 85/1000 | Loss: 0.00001370
Iteration 86/1000 | Loss: 0.00001370
Iteration 87/1000 | Loss: 0.00001370
Iteration 88/1000 | Loss: 0.00001370
Iteration 89/1000 | Loss: 0.00001370
Iteration 90/1000 | Loss: 0.00001370
Iteration 91/1000 | Loss: 0.00001369
Iteration 92/1000 | Loss: 0.00001369
Iteration 93/1000 | Loss: 0.00001369
Iteration 94/1000 | Loss: 0.00001369
Iteration 95/1000 | Loss: 0.00001369
Iteration 96/1000 | Loss: 0.00001369
Iteration 97/1000 | Loss: 0.00001369
Iteration 98/1000 | Loss: 0.00001369
Iteration 99/1000 | Loss: 0.00001369
Iteration 100/1000 | Loss: 0.00001369
Iteration 101/1000 | Loss: 0.00001369
Iteration 102/1000 | Loss: 0.00001369
Iteration 103/1000 | Loss: 0.00001369
Iteration 104/1000 | Loss: 0.00001368
Iteration 105/1000 | Loss: 0.00001368
Iteration 106/1000 | Loss: 0.00001368
Iteration 107/1000 | Loss: 0.00001368
Iteration 108/1000 | Loss: 0.00001368
Iteration 109/1000 | Loss: 0.00001368
Iteration 110/1000 | Loss: 0.00001368
Iteration 111/1000 | Loss: 0.00001367
Iteration 112/1000 | Loss: 0.00001367
Iteration 113/1000 | Loss: 0.00001367
Iteration 114/1000 | Loss: 0.00001367
Iteration 115/1000 | Loss: 0.00001367
Iteration 116/1000 | Loss: 0.00001367
Iteration 117/1000 | Loss: 0.00002667
Iteration 118/1000 | Loss: 0.00003508
Iteration 119/1000 | Loss: 0.00002101
Iteration 120/1000 | Loss: 0.00001535
Iteration 121/1000 | Loss: 0.00005149
Iteration 122/1000 | Loss: 0.00001513
Iteration 123/1000 | Loss: 0.00002685
Iteration 124/1000 | Loss: 0.00001669
Iteration 125/1000 | Loss: 0.00001375
Iteration 126/1000 | Loss: 0.00001588
Iteration 127/1000 | Loss: 0.00001374
Iteration 128/1000 | Loss: 0.00001449
Iteration 129/1000 | Loss: 0.00001362
Iteration 130/1000 | Loss: 0.00001362
Iteration 131/1000 | Loss: 0.00001362
Iteration 132/1000 | Loss: 0.00001362
Iteration 133/1000 | Loss: 0.00001361
Iteration 134/1000 | Loss: 0.00001361
Iteration 135/1000 | Loss: 0.00001361
Iteration 136/1000 | Loss: 0.00001361
Iteration 137/1000 | Loss: 0.00001361
Iteration 138/1000 | Loss: 0.00001361
Iteration 139/1000 | Loss: 0.00001361
Iteration 140/1000 | Loss: 0.00001361
Iteration 141/1000 | Loss: 0.00001361
Iteration 142/1000 | Loss: 0.00001361
Iteration 143/1000 | Loss: 0.00001361
Iteration 144/1000 | Loss: 0.00001361
Iteration 145/1000 | Loss: 0.00001361
Iteration 146/1000 | Loss: 0.00001361
Iteration 147/1000 | Loss: 0.00001361
Iteration 148/1000 | Loss: 0.00001361
Iteration 149/1000 | Loss: 0.00001361
Iteration 150/1000 | Loss: 0.00001361
Iteration 151/1000 | Loss: 0.00001361
Iteration 152/1000 | Loss: 0.00001361
Iteration 153/1000 | Loss: 0.00001361
Iteration 154/1000 | Loss: 0.00001361
Iteration 155/1000 | Loss: 0.00001361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.3606326319859363e-05, 1.3606326319859363e-05, 1.3606326319859363e-05, 1.3606326319859363e-05, 1.3606326319859363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3606326319859363e-05

Optimization complete. Final v2v error: 3.1246347427368164 mm

Highest mean error: 3.8562872409820557 mm for frame 91

Lowest mean error: 2.801701784133911 mm for frame 131

Saving results

Total time: 96.70847082138062
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997816
Iteration 2/25 | Loss: 0.00997816
Iteration 3/25 | Loss: 0.00447076
Iteration 4/25 | Loss: 0.00331177
Iteration 5/25 | Loss: 0.00228457
Iteration 6/25 | Loss: 0.00206696
Iteration 7/25 | Loss: 0.00212228
Iteration 8/25 | Loss: 0.00202680
Iteration 9/25 | Loss: 0.00189627
Iteration 10/25 | Loss: 0.00185572
Iteration 11/25 | Loss: 0.00176633
Iteration 12/25 | Loss: 0.00173701
Iteration 13/25 | Loss: 0.00169341
Iteration 14/25 | Loss: 0.00166781
Iteration 15/25 | Loss: 0.00164481
Iteration 16/25 | Loss: 0.00162226
Iteration 17/25 | Loss: 0.00163019
Iteration 18/25 | Loss: 0.00158126
Iteration 19/25 | Loss: 0.00157360
Iteration 20/25 | Loss: 0.00156853
Iteration 21/25 | Loss: 0.00156433
Iteration 22/25 | Loss: 0.00155456
Iteration 23/25 | Loss: 0.00154920
Iteration 24/25 | Loss: 0.00155515
Iteration 25/25 | Loss: 0.00155136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23489773
Iteration 2/25 | Loss: 0.00419990
Iteration 3/25 | Loss: 0.00302653
Iteration 4/25 | Loss: 0.00302653
Iteration 5/25 | Loss: 0.00302653
Iteration 6/25 | Loss: 0.00302653
Iteration 7/25 | Loss: 0.00302653
Iteration 8/25 | Loss: 0.00302653
Iteration 9/25 | Loss: 0.00302653
Iteration 10/25 | Loss: 0.00302653
Iteration 11/25 | Loss: 0.00302653
Iteration 12/25 | Loss: 0.00302653
Iteration 13/25 | Loss: 0.00302653
Iteration 14/25 | Loss: 0.00302653
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0030265292152762413, 0.0030265292152762413, 0.0030265292152762413, 0.0030265292152762413, 0.0030265292152762413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030265292152762413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00302653
Iteration 2/1000 | Loss: 0.00425112
Iteration 3/1000 | Loss: 0.00111884
Iteration 4/1000 | Loss: 0.00091852
Iteration 5/1000 | Loss: 0.00063271
Iteration 6/1000 | Loss: 0.00136525
Iteration 7/1000 | Loss: 0.00511646
Iteration 8/1000 | Loss: 0.00123570
Iteration 9/1000 | Loss: 0.00087431
Iteration 10/1000 | Loss: 0.00037056
Iteration 11/1000 | Loss: 0.00067876
Iteration 12/1000 | Loss: 0.00043477
Iteration 13/1000 | Loss: 0.00023306
Iteration 14/1000 | Loss: 0.00072817
Iteration 15/1000 | Loss: 0.00094625
Iteration 16/1000 | Loss: 0.00315290
Iteration 17/1000 | Loss: 0.00220575
Iteration 18/1000 | Loss: 0.00151618
Iteration 19/1000 | Loss: 0.00145071
Iteration 20/1000 | Loss: 0.00036522
Iteration 21/1000 | Loss: 0.00044367
Iteration 22/1000 | Loss: 0.00036198
Iteration 23/1000 | Loss: 0.00056905
Iteration 24/1000 | Loss: 0.00068226
Iteration 25/1000 | Loss: 0.00040778
Iteration 26/1000 | Loss: 0.00058144
Iteration 27/1000 | Loss: 0.00116351
Iteration 28/1000 | Loss: 0.00088061
Iteration 29/1000 | Loss: 0.00039063
Iteration 30/1000 | Loss: 0.00050744
Iteration 31/1000 | Loss: 0.00044204
Iteration 32/1000 | Loss: 0.00039156
Iteration 33/1000 | Loss: 0.00095272
Iteration 34/1000 | Loss: 0.00042492
Iteration 35/1000 | Loss: 0.00012411
Iteration 36/1000 | Loss: 0.00011693
Iteration 37/1000 | Loss: 0.00032190
Iteration 38/1000 | Loss: 0.00056858
Iteration 39/1000 | Loss: 0.00028384
Iteration 40/1000 | Loss: 0.00035171
Iteration 41/1000 | Loss: 0.00024964
Iteration 42/1000 | Loss: 0.00028261
Iteration 43/1000 | Loss: 0.00013434
Iteration 44/1000 | Loss: 0.00008534
Iteration 45/1000 | Loss: 0.00034018
Iteration 46/1000 | Loss: 0.00015999
Iteration 47/1000 | Loss: 0.00008244
Iteration 48/1000 | Loss: 0.00040348
Iteration 49/1000 | Loss: 0.00017858
Iteration 50/1000 | Loss: 0.00018834
Iteration 51/1000 | Loss: 0.00046241
Iteration 52/1000 | Loss: 0.00033910
Iteration 53/1000 | Loss: 0.00034913
Iteration 54/1000 | Loss: 0.00034804
Iteration 55/1000 | Loss: 0.00055555
Iteration 56/1000 | Loss: 0.00098382
Iteration 57/1000 | Loss: 0.00044752
Iteration 58/1000 | Loss: 0.00040351
Iteration 59/1000 | Loss: 0.00017889
Iteration 60/1000 | Loss: 0.00029527
Iteration 61/1000 | Loss: 0.00027249
Iteration 62/1000 | Loss: 0.00056948
Iteration 63/1000 | Loss: 0.00031170
Iteration 64/1000 | Loss: 0.00042316
Iteration 65/1000 | Loss: 0.00019483
Iteration 66/1000 | Loss: 0.00030823
Iteration 67/1000 | Loss: 0.00022829
Iteration 68/1000 | Loss: 0.00077871
Iteration 69/1000 | Loss: 0.00013190
Iteration 70/1000 | Loss: 0.00012668
Iteration 71/1000 | Loss: 0.00006829
Iteration 72/1000 | Loss: 0.00030563
Iteration 73/1000 | Loss: 0.00007131
Iteration 74/1000 | Loss: 0.00006218
Iteration 75/1000 | Loss: 0.00007573
Iteration 76/1000 | Loss: 0.00006088
Iteration 77/1000 | Loss: 0.00026440
Iteration 78/1000 | Loss: 0.00026223
Iteration 79/1000 | Loss: 0.00075285
Iteration 80/1000 | Loss: 0.00019777
Iteration 81/1000 | Loss: 0.00004165
Iteration 82/1000 | Loss: 0.00004790
Iteration 83/1000 | Loss: 0.00004108
Iteration 84/1000 | Loss: 0.00006048
Iteration 85/1000 | Loss: 0.00003241
Iteration 86/1000 | Loss: 0.00005462
Iteration 87/1000 | Loss: 0.00047006
Iteration 88/1000 | Loss: 0.00008137
Iteration 89/1000 | Loss: 0.00003602
Iteration 90/1000 | Loss: 0.00003458
Iteration 91/1000 | Loss: 0.00003477
Iteration 92/1000 | Loss: 0.00003370
Iteration 93/1000 | Loss: 0.00006883
Iteration 94/1000 | Loss: 0.00003085
Iteration 95/1000 | Loss: 0.00003587
Iteration 96/1000 | Loss: 0.00003565
Iteration 97/1000 | Loss: 0.00002835
Iteration 98/1000 | Loss: 0.00014268
Iteration 99/1000 | Loss: 0.00003067
Iteration 100/1000 | Loss: 0.00003662
Iteration 101/1000 | Loss: 0.00003286
Iteration 102/1000 | Loss: 0.00018191
Iteration 103/1000 | Loss: 0.00015606
Iteration 104/1000 | Loss: 0.00008924
Iteration 105/1000 | Loss: 0.00012963
Iteration 106/1000 | Loss: 0.00013804
Iteration 107/1000 | Loss: 0.00003283
Iteration 108/1000 | Loss: 0.00005659
Iteration 109/1000 | Loss: 0.00002854
Iteration 110/1000 | Loss: 0.00004959
Iteration 111/1000 | Loss: 0.00002707
Iteration 112/1000 | Loss: 0.00013712
Iteration 113/1000 | Loss: 0.00010230
Iteration 114/1000 | Loss: 0.00003732
Iteration 115/1000 | Loss: 0.00003391
Iteration 116/1000 | Loss: 0.00013753
Iteration 117/1000 | Loss: 0.00008878
Iteration 118/1000 | Loss: 0.00011474
Iteration 119/1000 | Loss: 0.00004239
Iteration 120/1000 | Loss: 0.00005234
Iteration 121/1000 | Loss: 0.00002968
Iteration 122/1000 | Loss: 0.00002871
Iteration 123/1000 | Loss: 0.00013735
Iteration 124/1000 | Loss: 0.00032295
Iteration 125/1000 | Loss: 0.00004563
Iteration 126/1000 | Loss: 0.00004498
Iteration 127/1000 | Loss: 0.00006140
Iteration 128/1000 | Loss: 0.00003853
Iteration 129/1000 | Loss: 0.00002765
Iteration 130/1000 | Loss: 0.00003979
Iteration 131/1000 | Loss: 0.00002686
Iteration 132/1000 | Loss: 0.00003508
Iteration 133/1000 | Loss: 0.00004265
Iteration 134/1000 | Loss: 0.00002666
Iteration 135/1000 | Loss: 0.00002627
Iteration 136/1000 | Loss: 0.00002626
Iteration 137/1000 | Loss: 0.00003100
Iteration 138/1000 | Loss: 0.00003526
Iteration 139/1000 | Loss: 0.00002822
Iteration 140/1000 | Loss: 0.00002631
Iteration 141/1000 | Loss: 0.00002594
Iteration 142/1000 | Loss: 0.00002589
Iteration 143/1000 | Loss: 0.00002588
Iteration 144/1000 | Loss: 0.00002587
Iteration 145/1000 | Loss: 0.00002587
Iteration 146/1000 | Loss: 0.00002587
Iteration 147/1000 | Loss: 0.00002587
Iteration 148/1000 | Loss: 0.00002586
Iteration 149/1000 | Loss: 0.00002586
Iteration 150/1000 | Loss: 0.00002586
Iteration 151/1000 | Loss: 0.00002585
Iteration 152/1000 | Loss: 0.00002584
Iteration 153/1000 | Loss: 0.00002583
Iteration 154/1000 | Loss: 0.00002583
Iteration 155/1000 | Loss: 0.00002582
Iteration 156/1000 | Loss: 0.00004497
Iteration 157/1000 | Loss: 0.00002582
Iteration 158/1000 | Loss: 0.00002577
Iteration 159/1000 | Loss: 0.00002577
Iteration 160/1000 | Loss: 0.00002577
Iteration 161/1000 | Loss: 0.00002576
Iteration 162/1000 | Loss: 0.00002576
Iteration 163/1000 | Loss: 0.00002576
Iteration 164/1000 | Loss: 0.00002576
Iteration 165/1000 | Loss: 0.00002576
Iteration 166/1000 | Loss: 0.00002576
Iteration 167/1000 | Loss: 0.00002576
Iteration 168/1000 | Loss: 0.00002576
Iteration 169/1000 | Loss: 0.00002575
Iteration 170/1000 | Loss: 0.00002575
Iteration 171/1000 | Loss: 0.00002575
Iteration 172/1000 | Loss: 0.00002575
Iteration 173/1000 | Loss: 0.00002574
Iteration 174/1000 | Loss: 0.00002574
Iteration 175/1000 | Loss: 0.00002573
Iteration 176/1000 | Loss: 0.00002573
Iteration 177/1000 | Loss: 0.00003922
Iteration 178/1000 | Loss: 0.00002704
Iteration 179/1000 | Loss: 0.00002574
Iteration 180/1000 | Loss: 0.00002572
Iteration 181/1000 | Loss: 0.00002571
Iteration 182/1000 | Loss: 0.00002580
Iteration 183/1000 | Loss: 0.00002580
Iteration 184/1000 | Loss: 0.00002580
Iteration 185/1000 | Loss: 0.00002561
Iteration 186/1000 | Loss: 0.00002561
Iteration 187/1000 | Loss: 0.00002561
Iteration 188/1000 | Loss: 0.00002561
Iteration 189/1000 | Loss: 0.00002561
Iteration 190/1000 | Loss: 0.00002561
Iteration 191/1000 | Loss: 0.00002561
Iteration 192/1000 | Loss: 0.00002561
Iteration 193/1000 | Loss: 0.00002561
Iteration 194/1000 | Loss: 0.00002561
Iteration 195/1000 | Loss: 0.00002561
Iteration 196/1000 | Loss: 0.00002561
Iteration 197/1000 | Loss: 0.00002561
Iteration 198/1000 | Loss: 0.00002560
Iteration 199/1000 | Loss: 0.00002560
Iteration 200/1000 | Loss: 0.00002560
Iteration 201/1000 | Loss: 0.00002560
Iteration 202/1000 | Loss: 0.00002560
Iteration 203/1000 | Loss: 0.00002560
Iteration 204/1000 | Loss: 0.00002559
Iteration 205/1000 | Loss: 0.00002559
Iteration 206/1000 | Loss: 0.00002559
Iteration 207/1000 | Loss: 0.00002559
Iteration 208/1000 | Loss: 0.00002559
Iteration 209/1000 | Loss: 0.00002559
Iteration 210/1000 | Loss: 0.00002559
Iteration 211/1000 | Loss: 0.00002559
Iteration 212/1000 | Loss: 0.00002559
Iteration 213/1000 | Loss: 0.00002559
Iteration 214/1000 | Loss: 0.00002559
Iteration 215/1000 | Loss: 0.00002559
Iteration 216/1000 | Loss: 0.00002559
Iteration 217/1000 | Loss: 0.00002558
Iteration 218/1000 | Loss: 0.00002558
Iteration 219/1000 | Loss: 0.00002558
Iteration 220/1000 | Loss: 0.00004075
Iteration 221/1000 | Loss: 0.00004725
Iteration 222/1000 | Loss: 0.00002866
Iteration 223/1000 | Loss: 0.00002561
Iteration 224/1000 | Loss: 0.00004859
Iteration 225/1000 | Loss: 0.00004763
Iteration 226/1000 | Loss: 0.00007380
Iteration 227/1000 | Loss: 0.00002676
Iteration 228/1000 | Loss: 0.00003436
Iteration 229/1000 | Loss: 0.00005172
Iteration 230/1000 | Loss: 0.00003476
Iteration 231/1000 | Loss: 0.00002582
Iteration 232/1000 | Loss: 0.00003579
Iteration 233/1000 | Loss: 0.00005554
Iteration 234/1000 | Loss: 0.00003777
Iteration 235/1000 | Loss: 0.00005046
Iteration 236/1000 | Loss: 0.00003756
Iteration 237/1000 | Loss: 0.00004627
Iteration 238/1000 | Loss: 0.00004051
Iteration 239/1000 | Loss: 0.00032143
Iteration 240/1000 | Loss: 0.00015880
Iteration 241/1000 | Loss: 0.00025299
Iteration 242/1000 | Loss: 0.00023370
Iteration 243/1000 | Loss: 0.00030299
Iteration 244/1000 | Loss: 0.00012300
Iteration 245/1000 | Loss: 0.00006093
Iteration 246/1000 | Loss: 0.00011688
Iteration 247/1000 | Loss: 0.00004740
Iteration 248/1000 | Loss: 0.00004239
Iteration 249/1000 | Loss: 0.00002817
Iteration 250/1000 | Loss: 0.00006656
Iteration 251/1000 | Loss: 0.00003927
Iteration 252/1000 | Loss: 0.00004761
Iteration 253/1000 | Loss: 0.00005982
Iteration 254/1000 | Loss: 0.00002657
Iteration 255/1000 | Loss: 0.00002920
Iteration 256/1000 | Loss: 0.00002623
Iteration 257/1000 | Loss: 0.00002621
Iteration 258/1000 | Loss: 0.00007924
Iteration 259/1000 | Loss: 0.00002625
Iteration 260/1000 | Loss: 0.00003102
Iteration 261/1000 | Loss: 0.00002791
Iteration 262/1000 | Loss: 0.00004232
Iteration 263/1000 | Loss: 0.00002638
Iteration 264/1000 | Loss: 0.00006355
Iteration 265/1000 | Loss: 0.00002818
Iteration 266/1000 | Loss: 0.00003682
Iteration 267/1000 | Loss: 0.00002552
Iteration 268/1000 | Loss: 0.00004964
Iteration 269/1000 | Loss: 0.00002944
Iteration 270/1000 | Loss: 0.00003667
Iteration 271/1000 | Loss: 0.00002554
Iteration 272/1000 | Loss: 0.00004018
Iteration 273/1000 | Loss: 0.00004375
Iteration 274/1000 | Loss: 0.00002452
Iteration 275/1000 | Loss: 0.00002448
Iteration 276/1000 | Loss: 0.00004059
Iteration 277/1000 | Loss: 0.00002434
Iteration 278/1000 | Loss: 0.00002431
Iteration 279/1000 | Loss: 0.00002431
Iteration 280/1000 | Loss: 0.00002430
Iteration 281/1000 | Loss: 0.00002429
Iteration 282/1000 | Loss: 0.00002428
Iteration 283/1000 | Loss: 0.00002428
Iteration 284/1000 | Loss: 0.00002428
Iteration 285/1000 | Loss: 0.00002427
Iteration 286/1000 | Loss: 0.00002427
Iteration 287/1000 | Loss: 0.00002427
Iteration 288/1000 | Loss: 0.00002427
Iteration 289/1000 | Loss: 0.00002427
Iteration 290/1000 | Loss: 0.00002426
Iteration 291/1000 | Loss: 0.00002426
Iteration 292/1000 | Loss: 0.00002425
Iteration 293/1000 | Loss: 0.00002695
Iteration 294/1000 | Loss: 0.00002422
Iteration 295/1000 | Loss: 0.00002422
Iteration 296/1000 | Loss: 0.00002422
Iteration 297/1000 | Loss: 0.00002422
Iteration 298/1000 | Loss: 0.00002422
Iteration 299/1000 | Loss: 0.00002422
Iteration 300/1000 | Loss: 0.00002422
Iteration 301/1000 | Loss: 0.00002422
Iteration 302/1000 | Loss: 0.00002422
Iteration 303/1000 | Loss: 0.00002422
Iteration 304/1000 | Loss: 0.00002422
Iteration 305/1000 | Loss: 0.00002422
Iteration 306/1000 | Loss: 0.00002422
Iteration 307/1000 | Loss: 0.00002422
Iteration 308/1000 | Loss: 0.00002422
Iteration 309/1000 | Loss: 0.00002422
Iteration 310/1000 | Loss: 0.00002422
Iteration 311/1000 | Loss: 0.00002422
Iteration 312/1000 | Loss: 0.00002422
Iteration 313/1000 | Loss: 0.00002422
Iteration 314/1000 | Loss: 0.00002422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 314. Stopping optimization.
Last 5 losses: [2.4217450118158013e-05, 2.4217450118158013e-05, 2.4217450118158013e-05, 2.4217450118158013e-05, 2.4217450118158013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4217450118158013e-05

Optimization complete. Final v2v error: 3.507148504257202 mm

Highest mean error: 10.800544738769531 mm for frame 82

Lowest mean error: 3.0061209201812744 mm for frame 32

Saving results

Total time: 379.8673906326294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867436
Iteration 2/25 | Loss: 0.00153674
Iteration 3/25 | Loss: 0.00128086
Iteration 4/25 | Loss: 0.00126046
Iteration 5/25 | Loss: 0.00125834
Iteration 6/25 | Loss: 0.00125834
Iteration 7/25 | Loss: 0.00125834
Iteration 8/25 | Loss: 0.00125834
Iteration 9/25 | Loss: 0.00125834
Iteration 10/25 | Loss: 0.00125834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012583439238369465, 0.0012583439238369465, 0.0012583439238369465, 0.0012583439238369465, 0.0012583439238369465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012583439238369465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86257333
Iteration 2/25 | Loss: 0.00087453
Iteration 3/25 | Loss: 0.00087453
Iteration 4/25 | Loss: 0.00087453
Iteration 5/25 | Loss: 0.00087453
Iteration 6/25 | Loss: 0.00087453
Iteration 7/25 | Loss: 0.00087453
Iteration 8/25 | Loss: 0.00087453
Iteration 9/25 | Loss: 0.00087453
Iteration 10/25 | Loss: 0.00087453
Iteration 11/25 | Loss: 0.00087453
Iteration 12/25 | Loss: 0.00087453
Iteration 13/25 | Loss: 0.00087453
Iteration 14/25 | Loss: 0.00087453
Iteration 15/25 | Loss: 0.00087453
Iteration 16/25 | Loss: 0.00087453
Iteration 17/25 | Loss: 0.00087453
Iteration 18/25 | Loss: 0.00087453
Iteration 19/25 | Loss: 0.00087453
Iteration 20/25 | Loss: 0.00087453
Iteration 21/25 | Loss: 0.00087453
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008745258674025536, 0.0008745258674025536, 0.0008745258674025536, 0.0008745258674025536, 0.0008745258674025536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008745258674025536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087453
Iteration 2/1000 | Loss: 0.00004232
Iteration 3/1000 | Loss: 0.00003287
Iteration 4/1000 | Loss: 0.00003059
Iteration 5/1000 | Loss: 0.00002991
Iteration 6/1000 | Loss: 0.00002872
Iteration 7/1000 | Loss: 0.00002805
Iteration 8/1000 | Loss: 0.00002778
Iteration 9/1000 | Loss: 0.00002748
Iteration 10/1000 | Loss: 0.00002721
Iteration 11/1000 | Loss: 0.00002707
Iteration 12/1000 | Loss: 0.00002687
Iteration 13/1000 | Loss: 0.00002682
Iteration 14/1000 | Loss: 0.00002680
Iteration 15/1000 | Loss: 0.00002663
Iteration 16/1000 | Loss: 0.00002656
Iteration 17/1000 | Loss: 0.00002650
Iteration 18/1000 | Loss: 0.00002650
Iteration 19/1000 | Loss: 0.00002649
Iteration 20/1000 | Loss: 0.00002649
Iteration 21/1000 | Loss: 0.00002648
Iteration 22/1000 | Loss: 0.00002648
Iteration 23/1000 | Loss: 0.00002648
Iteration 24/1000 | Loss: 0.00002648
Iteration 25/1000 | Loss: 0.00002648
Iteration 26/1000 | Loss: 0.00002647
Iteration 27/1000 | Loss: 0.00002646
Iteration 28/1000 | Loss: 0.00002646
Iteration 29/1000 | Loss: 0.00002644
Iteration 30/1000 | Loss: 0.00002644
Iteration 31/1000 | Loss: 0.00002644
Iteration 32/1000 | Loss: 0.00002644
Iteration 33/1000 | Loss: 0.00002644
Iteration 34/1000 | Loss: 0.00002644
Iteration 35/1000 | Loss: 0.00002644
Iteration 36/1000 | Loss: 0.00002644
Iteration 37/1000 | Loss: 0.00002644
Iteration 38/1000 | Loss: 0.00002643
Iteration 39/1000 | Loss: 0.00002643
Iteration 40/1000 | Loss: 0.00002643
Iteration 41/1000 | Loss: 0.00002642
Iteration 42/1000 | Loss: 0.00002642
Iteration 43/1000 | Loss: 0.00002642
Iteration 44/1000 | Loss: 0.00002642
Iteration 45/1000 | Loss: 0.00002642
Iteration 46/1000 | Loss: 0.00002642
Iteration 47/1000 | Loss: 0.00002642
Iteration 48/1000 | Loss: 0.00002642
Iteration 49/1000 | Loss: 0.00002642
Iteration 50/1000 | Loss: 0.00002642
Iteration 51/1000 | Loss: 0.00002642
Iteration 52/1000 | Loss: 0.00002641
Iteration 53/1000 | Loss: 0.00002641
Iteration 54/1000 | Loss: 0.00002641
Iteration 55/1000 | Loss: 0.00002641
Iteration 56/1000 | Loss: 0.00002641
Iteration 57/1000 | Loss: 0.00002641
Iteration 58/1000 | Loss: 0.00002641
Iteration 59/1000 | Loss: 0.00002641
Iteration 60/1000 | Loss: 0.00002640
Iteration 61/1000 | Loss: 0.00002640
Iteration 62/1000 | Loss: 0.00002640
Iteration 63/1000 | Loss: 0.00002640
Iteration 64/1000 | Loss: 0.00002640
Iteration 65/1000 | Loss: 0.00002640
Iteration 66/1000 | Loss: 0.00002640
Iteration 67/1000 | Loss: 0.00002639
Iteration 68/1000 | Loss: 0.00002639
Iteration 69/1000 | Loss: 0.00002639
Iteration 70/1000 | Loss: 0.00002639
Iteration 71/1000 | Loss: 0.00002639
Iteration 72/1000 | Loss: 0.00002638
Iteration 73/1000 | Loss: 0.00002638
Iteration 74/1000 | Loss: 0.00002638
Iteration 75/1000 | Loss: 0.00002637
Iteration 76/1000 | Loss: 0.00002637
Iteration 77/1000 | Loss: 0.00002637
Iteration 78/1000 | Loss: 0.00002637
Iteration 79/1000 | Loss: 0.00002637
Iteration 80/1000 | Loss: 0.00002637
Iteration 81/1000 | Loss: 0.00002637
Iteration 82/1000 | Loss: 0.00002637
Iteration 83/1000 | Loss: 0.00002637
Iteration 84/1000 | Loss: 0.00002637
Iteration 85/1000 | Loss: 0.00002637
Iteration 86/1000 | Loss: 0.00002637
Iteration 87/1000 | Loss: 0.00002636
Iteration 88/1000 | Loss: 0.00002636
Iteration 89/1000 | Loss: 0.00002636
Iteration 90/1000 | Loss: 0.00002636
Iteration 91/1000 | Loss: 0.00002636
Iteration 92/1000 | Loss: 0.00002636
Iteration 93/1000 | Loss: 0.00002636
Iteration 94/1000 | Loss: 0.00002636
Iteration 95/1000 | Loss: 0.00002636
Iteration 96/1000 | Loss: 0.00002636
Iteration 97/1000 | Loss: 0.00002636
Iteration 98/1000 | Loss: 0.00002636
Iteration 99/1000 | Loss: 0.00002636
Iteration 100/1000 | Loss: 0.00002636
Iteration 101/1000 | Loss: 0.00002636
Iteration 102/1000 | Loss: 0.00002636
Iteration 103/1000 | Loss: 0.00002636
Iteration 104/1000 | Loss: 0.00002636
Iteration 105/1000 | Loss: 0.00002636
Iteration 106/1000 | Loss: 0.00002636
Iteration 107/1000 | Loss: 0.00002636
Iteration 108/1000 | Loss: 0.00002636
Iteration 109/1000 | Loss: 0.00002636
Iteration 110/1000 | Loss: 0.00002636
Iteration 111/1000 | Loss: 0.00002636
Iteration 112/1000 | Loss: 0.00002636
Iteration 113/1000 | Loss: 0.00002636
Iteration 114/1000 | Loss: 0.00002636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.6355788577347994e-05, 2.6355788577347994e-05, 2.6355788577347994e-05, 2.6355788577347994e-05, 2.6355788577347994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6355788577347994e-05

Optimization complete. Final v2v error: 4.379438877105713 mm

Highest mean error: 4.656492233276367 mm for frame 1

Lowest mean error: 4.0719475746154785 mm for frame 81

Saving results

Total time: 33.09854006767273
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780408
Iteration 2/25 | Loss: 0.00131964
Iteration 3/25 | Loss: 0.00121424
Iteration 4/25 | Loss: 0.00120173
Iteration 5/25 | Loss: 0.00119795
Iteration 6/25 | Loss: 0.00119700
Iteration 7/25 | Loss: 0.00119700
Iteration 8/25 | Loss: 0.00119700
Iteration 9/25 | Loss: 0.00119700
Iteration 10/25 | Loss: 0.00119700
Iteration 11/25 | Loss: 0.00119700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001197001081891358, 0.001197001081891358, 0.001197001081891358, 0.001197001081891358, 0.001197001081891358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001197001081891358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37206292
Iteration 2/25 | Loss: 0.00137315
Iteration 3/25 | Loss: 0.00137315
Iteration 4/25 | Loss: 0.00137315
Iteration 5/25 | Loss: 0.00137315
Iteration 6/25 | Loss: 0.00137315
Iteration 7/25 | Loss: 0.00137315
Iteration 8/25 | Loss: 0.00137315
Iteration 9/25 | Loss: 0.00137315
Iteration 10/25 | Loss: 0.00137315
Iteration 11/25 | Loss: 0.00137315
Iteration 12/25 | Loss: 0.00137315
Iteration 13/25 | Loss: 0.00137315
Iteration 14/25 | Loss: 0.00137315
Iteration 15/25 | Loss: 0.00137315
Iteration 16/25 | Loss: 0.00137315
Iteration 17/25 | Loss: 0.00137315
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001373146427795291, 0.001373146427795291, 0.001373146427795291, 0.001373146427795291, 0.001373146427795291]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001373146427795291

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137315
Iteration 2/1000 | Loss: 0.00002326
Iteration 3/1000 | Loss: 0.00001505
Iteration 4/1000 | Loss: 0.00001342
Iteration 5/1000 | Loss: 0.00001273
Iteration 6/1000 | Loss: 0.00001224
Iteration 7/1000 | Loss: 0.00001177
Iteration 8/1000 | Loss: 0.00001148
Iteration 9/1000 | Loss: 0.00001122
Iteration 10/1000 | Loss: 0.00001094
Iteration 11/1000 | Loss: 0.00001088
Iteration 12/1000 | Loss: 0.00001075
Iteration 13/1000 | Loss: 0.00001069
Iteration 14/1000 | Loss: 0.00001064
Iteration 15/1000 | Loss: 0.00001062
Iteration 16/1000 | Loss: 0.00001061
Iteration 17/1000 | Loss: 0.00001057
Iteration 18/1000 | Loss: 0.00001053
Iteration 19/1000 | Loss: 0.00001051
Iteration 20/1000 | Loss: 0.00001050
Iteration 21/1000 | Loss: 0.00001049
Iteration 22/1000 | Loss: 0.00001048
Iteration 23/1000 | Loss: 0.00001048
Iteration 24/1000 | Loss: 0.00001047
Iteration 25/1000 | Loss: 0.00001047
Iteration 26/1000 | Loss: 0.00001046
Iteration 27/1000 | Loss: 0.00001046
Iteration 28/1000 | Loss: 0.00001045
Iteration 29/1000 | Loss: 0.00001044
Iteration 30/1000 | Loss: 0.00001044
Iteration 31/1000 | Loss: 0.00001039
Iteration 32/1000 | Loss: 0.00001035
Iteration 33/1000 | Loss: 0.00001033
Iteration 34/1000 | Loss: 0.00001032
Iteration 35/1000 | Loss: 0.00001031
Iteration 36/1000 | Loss: 0.00001030
Iteration 37/1000 | Loss: 0.00001030
Iteration 38/1000 | Loss: 0.00001029
Iteration 39/1000 | Loss: 0.00001029
Iteration 40/1000 | Loss: 0.00001028
Iteration 41/1000 | Loss: 0.00001028
Iteration 42/1000 | Loss: 0.00001027
Iteration 43/1000 | Loss: 0.00001027
Iteration 44/1000 | Loss: 0.00001026
Iteration 45/1000 | Loss: 0.00001026
Iteration 46/1000 | Loss: 0.00001026
Iteration 47/1000 | Loss: 0.00001025
Iteration 48/1000 | Loss: 0.00001025
Iteration 49/1000 | Loss: 0.00001025
Iteration 50/1000 | Loss: 0.00001024
Iteration 51/1000 | Loss: 0.00001022
Iteration 52/1000 | Loss: 0.00001022
Iteration 53/1000 | Loss: 0.00001021
Iteration 54/1000 | Loss: 0.00001021
Iteration 55/1000 | Loss: 0.00001021
Iteration 56/1000 | Loss: 0.00001021
Iteration 57/1000 | Loss: 0.00001019
Iteration 58/1000 | Loss: 0.00001019
Iteration 59/1000 | Loss: 0.00001018
Iteration 60/1000 | Loss: 0.00001018
Iteration 61/1000 | Loss: 0.00001017
Iteration 62/1000 | Loss: 0.00001017
Iteration 63/1000 | Loss: 0.00001017
Iteration 64/1000 | Loss: 0.00001017
Iteration 65/1000 | Loss: 0.00001017
Iteration 66/1000 | Loss: 0.00001016
Iteration 67/1000 | Loss: 0.00001016
Iteration 68/1000 | Loss: 0.00001016
Iteration 69/1000 | Loss: 0.00001016
Iteration 70/1000 | Loss: 0.00001016
Iteration 71/1000 | Loss: 0.00001015
Iteration 72/1000 | Loss: 0.00001015
Iteration 73/1000 | Loss: 0.00001014
Iteration 74/1000 | Loss: 0.00001014
Iteration 75/1000 | Loss: 0.00001014
Iteration 76/1000 | Loss: 0.00001014
Iteration 77/1000 | Loss: 0.00001014
Iteration 78/1000 | Loss: 0.00001014
Iteration 79/1000 | Loss: 0.00001014
Iteration 80/1000 | Loss: 0.00001014
Iteration 81/1000 | Loss: 0.00001013
Iteration 82/1000 | Loss: 0.00001013
Iteration 83/1000 | Loss: 0.00001013
Iteration 84/1000 | Loss: 0.00001013
Iteration 85/1000 | Loss: 0.00001012
Iteration 86/1000 | Loss: 0.00001012
Iteration 87/1000 | Loss: 0.00001012
Iteration 88/1000 | Loss: 0.00001011
Iteration 89/1000 | Loss: 0.00001011
Iteration 90/1000 | Loss: 0.00001011
Iteration 91/1000 | Loss: 0.00001011
Iteration 92/1000 | Loss: 0.00001011
Iteration 93/1000 | Loss: 0.00001011
Iteration 94/1000 | Loss: 0.00001010
Iteration 95/1000 | Loss: 0.00001010
Iteration 96/1000 | Loss: 0.00001010
Iteration 97/1000 | Loss: 0.00001010
Iteration 98/1000 | Loss: 0.00001009
Iteration 99/1000 | Loss: 0.00001009
Iteration 100/1000 | Loss: 0.00001008
Iteration 101/1000 | Loss: 0.00001008
Iteration 102/1000 | Loss: 0.00001008
Iteration 103/1000 | Loss: 0.00001008
Iteration 104/1000 | Loss: 0.00001008
Iteration 105/1000 | Loss: 0.00001008
Iteration 106/1000 | Loss: 0.00001008
Iteration 107/1000 | Loss: 0.00001008
Iteration 108/1000 | Loss: 0.00001008
Iteration 109/1000 | Loss: 0.00001008
Iteration 110/1000 | Loss: 0.00001008
Iteration 111/1000 | Loss: 0.00001007
Iteration 112/1000 | Loss: 0.00001007
Iteration 113/1000 | Loss: 0.00001007
Iteration 114/1000 | Loss: 0.00001007
Iteration 115/1000 | Loss: 0.00001007
Iteration 116/1000 | Loss: 0.00001007
Iteration 117/1000 | Loss: 0.00001007
Iteration 118/1000 | Loss: 0.00001007
Iteration 119/1000 | Loss: 0.00001007
Iteration 120/1000 | Loss: 0.00001007
Iteration 121/1000 | Loss: 0.00001007
Iteration 122/1000 | Loss: 0.00001007
Iteration 123/1000 | Loss: 0.00001007
Iteration 124/1000 | Loss: 0.00001007
Iteration 125/1000 | Loss: 0.00001007
Iteration 126/1000 | Loss: 0.00001007
Iteration 127/1000 | Loss: 0.00001007
Iteration 128/1000 | Loss: 0.00001007
Iteration 129/1000 | Loss: 0.00001007
Iteration 130/1000 | Loss: 0.00001007
Iteration 131/1000 | Loss: 0.00001007
Iteration 132/1000 | Loss: 0.00001006
Iteration 133/1000 | Loss: 0.00001006
Iteration 134/1000 | Loss: 0.00001006
Iteration 135/1000 | Loss: 0.00001006
Iteration 136/1000 | Loss: 0.00001006
Iteration 137/1000 | Loss: 0.00001006
Iteration 138/1000 | Loss: 0.00001006
Iteration 139/1000 | Loss: 0.00001006
Iteration 140/1000 | Loss: 0.00001006
Iteration 141/1000 | Loss: 0.00001006
Iteration 142/1000 | Loss: 0.00001006
Iteration 143/1000 | Loss: 0.00001006
Iteration 144/1000 | Loss: 0.00001006
Iteration 145/1000 | Loss: 0.00001006
Iteration 146/1000 | Loss: 0.00001006
Iteration 147/1000 | Loss: 0.00001006
Iteration 148/1000 | Loss: 0.00001006
Iteration 149/1000 | Loss: 0.00001006
Iteration 150/1000 | Loss: 0.00001006
Iteration 151/1000 | Loss: 0.00001006
Iteration 152/1000 | Loss: 0.00001006
Iteration 153/1000 | Loss: 0.00001006
Iteration 154/1000 | Loss: 0.00001006
Iteration 155/1000 | Loss: 0.00001006
Iteration 156/1000 | Loss: 0.00001006
Iteration 157/1000 | Loss: 0.00001006
Iteration 158/1000 | Loss: 0.00001006
Iteration 159/1000 | Loss: 0.00001006
Iteration 160/1000 | Loss: 0.00001006
Iteration 161/1000 | Loss: 0.00001006
Iteration 162/1000 | Loss: 0.00001006
Iteration 163/1000 | Loss: 0.00001006
Iteration 164/1000 | Loss: 0.00001006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.0055159691546578e-05, 1.0055159691546578e-05, 1.0055159691546578e-05, 1.0055159691546578e-05, 1.0055159691546578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0055159691546578e-05

Optimization complete. Final v2v error: 2.7057950496673584 mm

Highest mean error: 3.5458950996398926 mm for frame 59

Lowest mean error: 2.4609036445617676 mm for frame 2

Saving results

Total time: 37.024855613708496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414409
Iteration 2/25 | Loss: 0.00132333
Iteration 3/25 | Loss: 0.00122288
Iteration 4/25 | Loss: 0.00121015
Iteration 5/25 | Loss: 0.00120719
Iteration 6/25 | Loss: 0.00120664
Iteration 7/25 | Loss: 0.00120664
Iteration 8/25 | Loss: 0.00120664
Iteration 9/25 | Loss: 0.00120664
Iteration 10/25 | Loss: 0.00120664
Iteration 11/25 | Loss: 0.00120664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001206639688462019, 0.001206639688462019, 0.001206639688462019, 0.001206639688462019, 0.001206639688462019]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001206639688462019

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.67705250
Iteration 2/25 | Loss: 0.00137056
Iteration 3/25 | Loss: 0.00137055
Iteration 4/25 | Loss: 0.00137055
Iteration 5/25 | Loss: 0.00137055
Iteration 6/25 | Loss: 0.00137055
Iteration 7/25 | Loss: 0.00137055
Iteration 8/25 | Loss: 0.00137054
Iteration 9/25 | Loss: 0.00137054
Iteration 10/25 | Loss: 0.00137054
Iteration 11/25 | Loss: 0.00137054
Iteration 12/25 | Loss: 0.00137054
Iteration 13/25 | Loss: 0.00137054
Iteration 14/25 | Loss: 0.00137054
Iteration 15/25 | Loss: 0.00137054
Iteration 16/25 | Loss: 0.00137054
Iteration 17/25 | Loss: 0.00137054
Iteration 18/25 | Loss: 0.00137054
Iteration 19/25 | Loss: 0.00137054
Iteration 20/25 | Loss: 0.00137054
Iteration 21/25 | Loss: 0.00137054
Iteration 22/25 | Loss: 0.00137054
Iteration 23/25 | Loss: 0.00137054
Iteration 24/25 | Loss: 0.00137054
Iteration 25/25 | Loss: 0.00137054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137054
Iteration 2/1000 | Loss: 0.00002635
Iteration 3/1000 | Loss: 0.00001892
Iteration 4/1000 | Loss: 0.00001555
Iteration 5/1000 | Loss: 0.00001450
Iteration 6/1000 | Loss: 0.00001383
Iteration 7/1000 | Loss: 0.00001328
Iteration 8/1000 | Loss: 0.00001284
Iteration 9/1000 | Loss: 0.00001255
Iteration 10/1000 | Loss: 0.00001222
Iteration 11/1000 | Loss: 0.00001208
Iteration 12/1000 | Loss: 0.00001193
Iteration 13/1000 | Loss: 0.00001187
Iteration 14/1000 | Loss: 0.00001182
Iteration 15/1000 | Loss: 0.00001169
Iteration 16/1000 | Loss: 0.00001168
Iteration 17/1000 | Loss: 0.00001167
Iteration 18/1000 | Loss: 0.00001165
Iteration 19/1000 | Loss: 0.00001164
Iteration 20/1000 | Loss: 0.00001164
Iteration 21/1000 | Loss: 0.00001163
Iteration 22/1000 | Loss: 0.00001162
Iteration 23/1000 | Loss: 0.00001159
Iteration 24/1000 | Loss: 0.00001157
Iteration 25/1000 | Loss: 0.00001156
Iteration 26/1000 | Loss: 0.00001156
Iteration 27/1000 | Loss: 0.00001156
Iteration 28/1000 | Loss: 0.00001155
Iteration 29/1000 | Loss: 0.00001154
Iteration 30/1000 | Loss: 0.00001153
Iteration 31/1000 | Loss: 0.00001153
Iteration 32/1000 | Loss: 0.00001152
Iteration 33/1000 | Loss: 0.00001152
Iteration 34/1000 | Loss: 0.00001151
Iteration 35/1000 | Loss: 0.00001151
Iteration 36/1000 | Loss: 0.00001150
Iteration 37/1000 | Loss: 0.00001149
Iteration 38/1000 | Loss: 0.00001149
Iteration 39/1000 | Loss: 0.00001148
Iteration 40/1000 | Loss: 0.00001141
Iteration 41/1000 | Loss: 0.00001139
Iteration 42/1000 | Loss: 0.00001136
Iteration 43/1000 | Loss: 0.00001135
Iteration 44/1000 | Loss: 0.00001134
Iteration 45/1000 | Loss: 0.00001127
Iteration 46/1000 | Loss: 0.00001126
Iteration 47/1000 | Loss: 0.00001125
Iteration 48/1000 | Loss: 0.00001125
Iteration 49/1000 | Loss: 0.00001125
Iteration 50/1000 | Loss: 0.00001124
Iteration 51/1000 | Loss: 0.00001123
Iteration 52/1000 | Loss: 0.00001123
Iteration 53/1000 | Loss: 0.00001123
Iteration 54/1000 | Loss: 0.00001118
Iteration 55/1000 | Loss: 0.00001118
Iteration 56/1000 | Loss: 0.00001118
Iteration 57/1000 | Loss: 0.00001118
Iteration 58/1000 | Loss: 0.00001117
Iteration 59/1000 | Loss: 0.00001117
Iteration 60/1000 | Loss: 0.00001117
Iteration 61/1000 | Loss: 0.00001117
Iteration 62/1000 | Loss: 0.00001117
Iteration 63/1000 | Loss: 0.00001115
Iteration 64/1000 | Loss: 0.00001115
Iteration 65/1000 | Loss: 0.00001115
Iteration 66/1000 | Loss: 0.00001115
Iteration 67/1000 | Loss: 0.00001115
Iteration 68/1000 | Loss: 0.00001115
Iteration 69/1000 | Loss: 0.00001115
Iteration 70/1000 | Loss: 0.00001115
Iteration 71/1000 | Loss: 0.00001115
Iteration 72/1000 | Loss: 0.00001115
Iteration 73/1000 | Loss: 0.00001114
Iteration 74/1000 | Loss: 0.00001114
Iteration 75/1000 | Loss: 0.00001114
Iteration 76/1000 | Loss: 0.00001113
Iteration 77/1000 | Loss: 0.00001113
Iteration 78/1000 | Loss: 0.00001113
Iteration 79/1000 | Loss: 0.00001113
Iteration 80/1000 | Loss: 0.00001112
Iteration 81/1000 | Loss: 0.00001111
Iteration 82/1000 | Loss: 0.00001111
Iteration 83/1000 | Loss: 0.00001111
Iteration 84/1000 | Loss: 0.00001110
Iteration 85/1000 | Loss: 0.00001110
Iteration 86/1000 | Loss: 0.00001110
Iteration 87/1000 | Loss: 0.00001110
Iteration 88/1000 | Loss: 0.00001110
Iteration 89/1000 | Loss: 0.00001109
Iteration 90/1000 | Loss: 0.00001109
Iteration 91/1000 | Loss: 0.00001109
Iteration 92/1000 | Loss: 0.00001109
Iteration 93/1000 | Loss: 0.00001108
Iteration 94/1000 | Loss: 0.00001108
Iteration 95/1000 | Loss: 0.00001108
Iteration 96/1000 | Loss: 0.00001108
Iteration 97/1000 | Loss: 0.00001107
Iteration 98/1000 | Loss: 0.00001107
Iteration 99/1000 | Loss: 0.00001107
Iteration 100/1000 | Loss: 0.00001107
Iteration 101/1000 | Loss: 0.00001107
Iteration 102/1000 | Loss: 0.00001107
Iteration 103/1000 | Loss: 0.00001107
Iteration 104/1000 | Loss: 0.00001107
Iteration 105/1000 | Loss: 0.00001107
Iteration 106/1000 | Loss: 0.00001107
Iteration 107/1000 | Loss: 0.00001107
Iteration 108/1000 | Loss: 0.00001107
Iteration 109/1000 | Loss: 0.00001107
Iteration 110/1000 | Loss: 0.00001107
Iteration 111/1000 | Loss: 0.00001107
Iteration 112/1000 | Loss: 0.00001107
Iteration 113/1000 | Loss: 0.00001107
Iteration 114/1000 | Loss: 0.00001107
Iteration 115/1000 | Loss: 0.00001107
Iteration 116/1000 | Loss: 0.00001107
Iteration 117/1000 | Loss: 0.00001107
Iteration 118/1000 | Loss: 0.00001107
Iteration 119/1000 | Loss: 0.00001107
Iteration 120/1000 | Loss: 0.00001107
Iteration 121/1000 | Loss: 0.00001107
Iteration 122/1000 | Loss: 0.00001107
Iteration 123/1000 | Loss: 0.00001107
Iteration 124/1000 | Loss: 0.00001107
Iteration 125/1000 | Loss: 0.00001107
Iteration 126/1000 | Loss: 0.00001107
Iteration 127/1000 | Loss: 0.00001107
Iteration 128/1000 | Loss: 0.00001107
Iteration 129/1000 | Loss: 0.00001107
Iteration 130/1000 | Loss: 0.00001107
Iteration 131/1000 | Loss: 0.00001107
Iteration 132/1000 | Loss: 0.00001107
Iteration 133/1000 | Loss: 0.00001107
Iteration 134/1000 | Loss: 0.00001107
Iteration 135/1000 | Loss: 0.00001107
Iteration 136/1000 | Loss: 0.00001107
Iteration 137/1000 | Loss: 0.00001107
Iteration 138/1000 | Loss: 0.00001107
Iteration 139/1000 | Loss: 0.00001107
Iteration 140/1000 | Loss: 0.00001107
Iteration 141/1000 | Loss: 0.00001107
Iteration 142/1000 | Loss: 0.00001107
Iteration 143/1000 | Loss: 0.00001107
Iteration 144/1000 | Loss: 0.00001107
Iteration 145/1000 | Loss: 0.00001107
Iteration 146/1000 | Loss: 0.00001107
Iteration 147/1000 | Loss: 0.00001107
Iteration 148/1000 | Loss: 0.00001107
Iteration 149/1000 | Loss: 0.00001107
Iteration 150/1000 | Loss: 0.00001107
Iteration 151/1000 | Loss: 0.00001107
Iteration 152/1000 | Loss: 0.00001107
Iteration 153/1000 | Loss: 0.00001107
Iteration 154/1000 | Loss: 0.00001107
Iteration 155/1000 | Loss: 0.00001107
Iteration 156/1000 | Loss: 0.00001107
Iteration 157/1000 | Loss: 0.00001107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.1065223588957451e-05, 1.1065223588957451e-05, 1.1065223588957451e-05, 1.1065223588957451e-05, 1.1065223588957451e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1065223588957451e-05

Optimization complete. Final v2v error: 2.866532325744629 mm

Highest mean error: 3.4980268478393555 mm for frame 85

Lowest mean error: 2.557138681411743 mm for frame 41

Saving results

Total time: 39.867717266082764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493504
Iteration 2/25 | Loss: 0.00134501
Iteration 3/25 | Loss: 0.00124851
Iteration 4/25 | Loss: 0.00123040
Iteration 5/25 | Loss: 0.00122627
Iteration 6/25 | Loss: 0.00122594
Iteration 7/25 | Loss: 0.00122594
Iteration 8/25 | Loss: 0.00122594
Iteration 9/25 | Loss: 0.00122594
Iteration 10/25 | Loss: 0.00122594
Iteration 11/25 | Loss: 0.00122594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012259421637281775, 0.0012259421637281775, 0.0012259421637281775, 0.0012259421637281775, 0.0012259421637281775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012259421637281775

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.50882053
Iteration 2/25 | Loss: 0.00143514
Iteration 3/25 | Loss: 0.00143511
Iteration 4/25 | Loss: 0.00143511
Iteration 5/25 | Loss: 0.00143511
Iteration 6/25 | Loss: 0.00143511
Iteration 7/25 | Loss: 0.00143511
Iteration 8/25 | Loss: 0.00143511
Iteration 9/25 | Loss: 0.00143511
Iteration 10/25 | Loss: 0.00143511
Iteration 11/25 | Loss: 0.00143511
Iteration 12/25 | Loss: 0.00143511
Iteration 13/25 | Loss: 0.00143511
Iteration 14/25 | Loss: 0.00143511
Iteration 15/25 | Loss: 0.00143511
Iteration 16/25 | Loss: 0.00143511
Iteration 17/25 | Loss: 0.00143511
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014351087156683207, 0.0014351087156683207, 0.0014351087156683207, 0.0014351087156683207, 0.0014351087156683207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014351087156683207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143511
Iteration 2/1000 | Loss: 0.00002348
Iteration 3/1000 | Loss: 0.00001808
Iteration 4/1000 | Loss: 0.00001681
Iteration 5/1000 | Loss: 0.00001586
Iteration 6/1000 | Loss: 0.00001519
Iteration 7/1000 | Loss: 0.00001487
Iteration 8/1000 | Loss: 0.00001448
Iteration 9/1000 | Loss: 0.00001404
Iteration 10/1000 | Loss: 0.00001372
Iteration 11/1000 | Loss: 0.00001342
Iteration 12/1000 | Loss: 0.00001329
Iteration 13/1000 | Loss: 0.00001316
Iteration 14/1000 | Loss: 0.00001306
Iteration 15/1000 | Loss: 0.00001299
Iteration 16/1000 | Loss: 0.00001299
Iteration 17/1000 | Loss: 0.00001299
Iteration 18/1000 | Loss: 0.00001299
Iteration 19/1000 | Loss: 0.00001292
Iteration 20/1000 | Loss: 0.00001291
Iteration 21/1000 | Loss: 0.00001291
Iteration 22/1000 | Loss: 0.00001291
Iteration 23/1000 | Loss: 0.00001290
Iteration 24/1000 | Loss: 0.00001288
Iteration 25/1000 | Loss: 0.00001287
Iteration 26/1000 | Loss: 0.00001287
Iteration 27/1000 | Loss: 0.00001282
Iteration 28/1000 | Loss: 0.00001282
Iteration 29/1000 | Loss: 0.00001280
Iteration 30/1000 | Loss: 0.00001279
Iteration 31/1000 | Loss: 0.00001278
Iteration 32/1000 | Loss: 0.00001278
Iteration 33/1000 | Loss: 0.00001277
Iteration 34/1000 | Loss: 0.00001277
Iteration 35/1000 | Loss: 0.00001276
Iteration 36/1000 | Loss: 0.00001275
Iteration 37/1000 | Loss: 0.00001274
Iteration 38/1000 | Loss: 0.00001274
Iteration 39/1000 | Loss: 0.00001273
Iteration 40/1000 | Loss: 0.00001273
Iteration 41/1000 | Loss: 0.00001272
Iteration 42/1000 | Loss: 0.00001271
Iteration 43/1000 | Loss: 0.00001270
Iteration 44/1000 | Loss: 0.00001270
Iteration 45/1000 | Loss: 0.00001268
Iteration 46/1000 | Loss: 0.00001268
Iteration 47/1000 | Loss: 0.00001268
Iteration 48/1000 | Loss: 0.00001268
Iteration 49/1000 | Loss: 0.00001268
Iteration 50/1000 | Loss: 0.00001268
Iteration 51/1000 | Loss: 0.00001268
Iteration 52/1000 | Loss: 0.00001267
Iteration 53/1000 | Loss: 0.00001267
Iteration 54/1000 | Loss: 0.00001266
Iteration 55/1000 | Loss: 0.00001266
Iteration 56/1000 | Loss: 0.00001265
Iteration 57/1000 | Loss: 0.00001265
Iteration 58/1000 | Loss: 0.00001265
Iteration 59/1000 | Loss: 0.00001264
Iteration 60/1000 | Loss: 0.00001264
Iteration 61/1000 | Loss: 0.00001264
Iteration 62/1000 | Loss: 0.00001264
Iteration 63/1000 | Loss: 0.00001263
Iteration 64/1000 | Loss: 0.00001263
Iteration 65/1000 | Loss: 0.00001262
Iteration 66/1000 | Loss: 0.00001262
Iteration 67/1000 | Loss: 0.00001261
Iteration 68/1000 | Loss: 0.00001261
Iteration 69/1000 | Loss: 0.00001261
Iteration 70/1000 | Loss: 0.00001261
Iteration 71/1000 | Loss: 0.00001261
Iteration 72/1000 | Loss: 0.00001261
Iteration 73/1000 | Loss: 0.00001260
Iteration 74/1000 | Loss: 0.00001259
Iteration 75/1000 | Loss: 0.00001259
Iteration 76/1000 | Loss: 0.00001259
Iteration 77/1000 | Loss: 0.00001259
Iteration 78/1000 | Loss: 0.00001259
Iteration 79/1000 | Loss: 0.00001259
Iteration 80/1000 | Loss: 0.00001258
Iteration 81/1000 | Loss: 0.00001258
Iteration 82/1000 | Loss: 0.00001258
Iteration 83/1000 | Loss: 0.00001258
Iteration 84/1000 | Loss: 0.00001257
Iteration 85/1000 | Loss: 0.00001257
Iteration 86/1000 | Loss: 0.00001256
Iteration 87/1000 | Loss: 0.00001256
Iteration 88/1000 | Loss: 0.00001256
Iteration 89/1000 | Loss: 0.00001256
Iteration 90/1000 | Loss: 0.00001256
Iteration 91/1000 | Loss: 0.00001255
Iteration 92/1000 | Loss: 0.00001255
Iteration 93/1000 | Loss: 0.00001255
Iteration 94/1000 | Loss: 0.00001255
Iteration 95/1000 | Loss: 0.00001255
Iteration 96/1000 | Loss: 0.00001255
Iteration 97/1000 | Loss: 0.00001255
Iteration 98/1000 | Loss: 0.00001254
Iteration 99/1000 | Loss: 0.00001254
Iteration 100/1000 | Loss: 0.00001254
Iteration 101/1000 | Loss: 0.00001253
Iteration 102/1000 | Loss: 0.00001253
Iteration 103/1000 | Loss: 0.00001253
Iteration 104/1000 | Loss: 0.00001253
Iteration 105/1000 | Loss: 0.00001253
Iteration 106/1000 | Loss: 0.00001252
Iteration 107/1000 | Loss: 0.00001252
Iteration 108/1000 | Loss: 0.00001252
Iteration 109/1000 | Loss: 0.00001252
Iteration 110/1000 | Loss: 0.00001252
Iteration 111/1000 | Loss: 0.00001251
Iteration 112/1000 | Loss: 0.00001251
Iteration 113/1000 | Loss: 0.00001251
Iteration 114/1000 | Loss: 0.00001251
Iteration 115/1000 | Loss: 0.00001251
Iteration 116/1000 | Loss: 0.00001251
Iteration 117/1000 | Loss: 0.00001251
Iteration 118/1000 | Loss: 0.00001251
Iteration 119/1000 | Loss: 0.00001251
Iteration 120/1000 | Loss: 0.00001251
Iteration 121/1000 | Loss: 0.00001251
Iteration 122/1000 | Loss: 0.00001251
Iteration 123/1000 | Loss: 0.00001251
Iteration 124/1000 | Loss: 0.00001251
Iteration 125/1000 | Loss: 0.00001251
Iteration 126/1000 | Loss: 0.00001251
Iteration 127/1000 | Loss: 0.00001251
Iteration 128/1000 | Loss: 0.00001251
Iteration 129/1000 | Loss: 0.00001251
Iteration 130/1000 | Loss: 0.00001251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.2510980013757944e-05, 1.2510980013757944e-05, 1.2510980013757944e-05, 1.2510980013757944e-05, 1.2510980013757944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2510980013757944e-05

Optimization complete. Final v2v error: 3.031090259552002 mm

Highest mean error: 3.568223476409912 mm for frame 92

Lowest mean error: 2.55208158493042 mm for frame 247

Saving results

Total time: 43.37498331069946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807960
Iteration 2/25 | Loss: 0.00163470
Iteration 3/25 | Loss: 0.00137625
Iteration 4/25 | Loss: 0.00135645
Iteration 5/25 | Loss: 0.00135466
Iteration 6/25 | Loss: 0.00135466
Iteration 7/25 | Loss: 0.00135466
Iteration 8/25 | Loss: 0.00135466
Iteration 9/25 | Loss: 0.00135466
Iteration 10/25 | Loss: 0.00135466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013546644477173686, 0.0013546644477173686, 0.0013546644477173686, 0.0013546644477173686, 0.0013546644477173686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013546644477173686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18409860
Iteration 2/25 | Loss: 0.00103460
Iteration 3/25 | Loss: 0.00103457
Iteration 4/25 | Loss: 0.00103457
Iteration 5/25 | Loss: 0.00103457
Iteration 6/25 | Loss: 0.00103457
Iteration 7/25 | Loss: 0.00103457
Iteration 8/25 | Loss: 0.00103457
Iteration 9/25 | Loss: 0.00103457
Iteration 10/25 | Loss: 0.00103457
Iteration 11/25 | Loss: 0.00103457
Iteration 12/25 | Loss: 0.00103457
Iteration 13/25 | Loss: 0.00103457
Iteration 14/25 | Loss: 0.00103457
Iteration 15/25 | Loss: 0.00103457
Iteration 16/25 | Loss: 0.00103457
Iteration 17/25 | Loss: 0.00103457
Iteration 18/25 | Loss: 0.00103457
Iteration 19/25 | Loss: 0.00103457
Iteration 20/25 | Loss: 0.00103457
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010345684131607413, 0.0010345684131607413, 0.0010345684131607413, 0.0010345684131607413, 0.0010345684131607413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010345684131607413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103457
Iteration 2/1000 | Loss: 0.00003650
Iteration 3/1000 | Loss: 0.00002464
Iteration 4/1000 | Loss: 0.00002255
Iteration 5/1000 | Loss: 0.00002187
Iteration 6/1000 | Loss: 0.00002135
Iteration 7/1000 | Loss: 0.00002109
Iteration 8/1000 | Loss: 0.00002062
Iteration 9/1000 | Loss: 0.00002037
Iteration 10/1000 | Loss: 0.00002019
Iteration 11/1000 | Loss: 0.00002012
Iteration 12/1000 | Loss: 0.00002009
Iteration 13/1000 | Loss: 0.00002003
Iteration 14/1000 | Loss: 0.00002001
Iteration 15/1000 | Loss: 0.00002000
Iteration 16/1000 | Loss: 0.00001999
Iteration 17/1000 | Loss: 0.00001998
Iteration 18/1000 | Loss: 0.00001997
Iteration 19/1000 | Loss: 0.00001995
Iteration 20/1000 | Loss: 0.00001992
Iteration 21/1000 | Loss: 0.00001987
Iteration 22/1000 | Loss: 0.00001983
Iteration 23/1000 | Loss: 0.00001981
Iteration 24/1000 | Loss: 0.00001981
Iteration 25/1000 | Loss: 0.00001979
Iteration 26/1000 | Loss: 0.00001979
Iteration 27/1000 | Loss: 0.00001979
Iteration 28/1000 | Loss: 0.00001979
Iteration 29/1000 | Loss: 0.00001978
Iteration 30/1000 | Loss: 0.00001978
Iteration 31/1000 | Loss: 0.00001978
Iteration 32/1000 | Loss: 0.00001978
Iteration 33/1000 | Loss: 0.00001978
Iteration 34/1000 | Loss: 0.00001978
Iteration 35/1000 | Loss: 0.00001977
Iteration 36/1000 | Loss: 0.00001977
Iteration 37/1000 | Loss: 0.00001976
Iteration 38/1000 | Loss: 0.00001976
Iteration 39/1000 | Loss: 0.00001976
Iteration 40/1000 | Loss: 0.00001975
Iteration 41/1000 | Loss: 0.00001975
Iteration 42/1000 | Loss: 0.00001974
Iteration 43/1000 | Loss: 0.00001972
Iteration 44/1000 | Loss: 0.00001972
Iteration 45/1000 | Loss: 0.00001972
Iteration 46/1000 | Loss: 0.00001972
Iteration 47/1000 | Loss: 0.00001972
Iteration 48/1000 | Loss: 0.00001972
Iteration 49/1000 | Loss: 0.00001971
Iteration 50/1000 | Loss: 0.00001971
Iteration 51/1000 | Loss: 0.00001971
Iteration 52/1000 | Loss: 0.00001971
Iteration 53/1000 | Loss: 0.00001971
Iteration 54/1000 | Loss: 0.00001971
Iteration 55/1000 | Loss: 0.00001970
Iteration 56/1000 | Loss: 0.00001970
Iteration 57/1000 | Loss: 0.00001968
Iteration 58/1000 | Loss: 0.00001968
Iteration 59/1000 | Loss: 0.00001968
Iteration 60/1000 | Loss: 0.00001968
Iteration 61/1000 | Loss: 0.00001968
Iteration 62/1000 | Loss: 0.00001968
Iteration 63/1000 | Loss: 0.00001968
Iteration 64/1000 | Loss: 0.00001967
Iteration 65/1000 | Loss: 0.00001967
Iteration 66/1000 | Loss: 0.00001967
Iteration 67/1000 | Loss: 0.00001967
Iteration 68/1000 | Loss: 0.00001967
Iteration 69/1000 | Loss: 0.00001967
Iteration 70/1000 | Loss: 0.00001967
Iteration 71/1000 | Loss: 0.00001967
Iteration 72/1000 | Loss: 0.00001967
Iteration 73/1000 | Loss: 0.00001965
Iteration 74/1000 | Loss: 0.00001965
Iteration 75/1000 | Loss: 0.00001965
Iteration 76/1000 | Loss: 0.00001965
Iteration 77/1000 | Loss: 0.00001964
Iteration 78/1000 | Loss: 0.00001964
Iteration 79/1000 | Loss: 0.00001964
Iteration 80/1000 | Loss: 0.00001964
Iteration 81/1000 | Loss: 0.00001963
Iteration 82/1000 | Loss: 0.00001963
Iteration 83/1000 | Loss: 0.00001963
Iteration 84/1000 | Loss: 0.00001963
Iteration 85/1000 | Loss: 0.00001963
Iteration 86/1000 | Loss: 0.00001962
Iteration 87/1000 | Loss: 0.00001962
Iteration 88/1000 | Loss: 0.00001962
Iteration 89/1000 | Loss: 0.00001962
Iteration 90/1000 | Loss: 0.00001962
Iteration 91/1000 | Loss: 0.00001962
Iteration 92/1000 | Loss: 0.00001961
Iteration 93/1000 | Loss: 0.00001961
Iteration 94/1000 | Loss: 0.00001961
Iteration 95/1000 | Loss: 0.00001961
Iteration 96/1000 | Loss: 0.00001961
Iteration 97/1000 | Loss: 0.00001961
Iteration 98/1000 | Loss: 0.00001960
Iteration 99/1000 | Loss: 0.00001960
Iteration 100/1000 | Loss: 0.00001960
Iteration 101/1000 | Loss: 0.00001959
Iteration 102/1000 | Loss: 0.00001959
Iteration 103/1000 | Loss: 0.00001959
Iteration 104/1000 | Loss: 0.00001959
Iteration 105/1000 | Loss: 0.00001959
Iteration 106/1000 | Loss: 0.00001959
Iteration 107/1000 | Loss: 0.00001959
Iteration 108/1000 | Loss: 0.00001958
Iteration 109/1000 | Loss: 0.00001958
Iteration 110/1000 | Loss: 0.00001958
Iteration 111/1000 | Loss: 0.00001957
Iteration 112/1000 | Loss: 0.00001957
Iteration 113/1000 | Loss: 0.00001957
Iteration 114/1000 | Loss: 0.00001957
Iteration 115/1000 | Loss: 0.00001957
Iteration 116/1000 | Loss: 0.00001956
Iteration 117/1000 | Loss: 0.00001956
Iteration 118/1000 | Loss: 0.00001956
Iteration 119/1000 | Loss: 0.00001956
Iteration 120/1000 | Loss: 0.00001956
Iteration 121/1000 | Loss: 0.00001955
Iteration 122/1000 | Loss: 0.00001955
Iteration 123/1000 | Loss: 0.00001955
Iteration 124/1000 | Loss: 0.00001955
Iteration 125/1000 | Loss: 0.00001954
Iteration 126/1000 | Loss: 0.00001954
Iteration 127/1000 | Loss: 0.00001954
Iteration 128/1000 | Loss: 0.00001954
Iteration 129/1000 | Loss: 0.00001954
Iteration 130/1000 | Loss: 0.00001954
Iteration 131/1000 | Loss: 0.00001954
Iteration 132/1000 | Loss: 0.00001954
Iteration 133/1000 | Loss: 0.00001954
Iteration 134/1000 | Loss: 0.00001954
Iteration 135/1000 | Loss: 0.00001953
Iteration 136/1000 | Loss: 0.00001953
Iteration 137/1000 | Loss: 0.00001953
Iteration 138/1000 | Loss: 0.00001953
Iteration 139/1000 | Loss: 0.00001953
Iteration 140/1000 | Loss: 0.00001953
Iteration 141/1000 | Loss: 0.00001953
Iteration 142/1000 | Loss: 0.00001953
Iteration 143/1000 | Loss: 0.00001953
Iteration 144/1000 | Loss: 0.00001953
Iteration 145/1000 | Loss: 0.00001953
Iteration 146/1000 | Loss: 0.00001952
Iteration 147/1000 | Loss: 0.00001952
Iteration 148/1000 | Loss: 0.00001952
Iteration 149/1000 | Loss: 0.00001952
Iteration 150/1000 | Loss: 0.00001952
Iteration 151/1000 | Loss: 0.00001952
Iteration 152/1000 | Loss: 0.00001952
Iteration 153/1000 | Loss: 0.00001952
Iteration 154/1000 | Loss: 0.00001952
Iteration 155/1000 | Loss: 0.00001952
Iteration 156/1000 | Loss: 0.00001952
Iteration 157/1000 | Loss: 0.00001952
Iteration 158/1000 | Loss: 0.00001952
Iteration 159/1000 | Loss: 0.00001951
Iteration 160/1000 | Loss: 0.00001951
Iteration 161/1000 | Loss: 0.00001951
Iteration 162/1000 | Loss: 0.00001951
Iteration 163/1000 | Loss: 0.00001951
Iteration 164/1000 | Loss: 0.00001951
Iteration 165/1000 | Loss: 0.00001951
Iteration 166/1000 | Loss: 0.00001951
Iteration 167/1000 | Loss: 0.00001951
Iteration 168/1000 | Loss: 0.00001951
Iteration 169/1000 | Loss: 0.00001951
Iteration 170/1000 | Loss: 0.00001951
Iteration 171/1000 | Loss: 0.00001951
Iteration 172/1000 | Loss: 0.00001951
Iteration 173/1000 | Loss: 0.00001951
Iteration 174/1000 | Loss: 0.00001951
Iteration 175/1000 | Loss: 0.00001951
Iteration 176/1000 | Loss: 0.00001951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.951215926965233e-05, 1.951215926965233e-05, 1.951215926965233e-05, 1.951215926965233e-05, 1.951215926965233e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.951215926965233e-05

Optimization complete. Final v2v error: 3.684797763824463 mm

Highest mean error: 3.9221785068511963 mm for frame 156

Lowest mean error: 3.444986581802368 mm for frame 93

Saving results

Total time: 42.25682544708252
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063475
Iteration 2/25 | Loss: 0.01063475
Iteration 3/25 | Loss: 0.01063475
Iteration 4/25 | Loss: 0.00642568
Iteration 5/25 | Loss: 0.00356833
Iteration 6/25 | Loss: 0.00321570
Iteration 7/25 | Loss: 0.00255171
Iteration 8/25 | Loss: 0.00221316
Iteration 9/25 | Loss: 0.00199341
Iteration 10/25 | Loss: 0.00168518
Iteration 11/25 | Loss: 0.00159521
Iteration 12/25 | Loss: 0.00154739
Iteration 13/25 | Loss: 0.00147552
Iteration 14/25 | Loss: 0.00143519
Iteration 15/25 | Loss: 0.00141482
Iteration 16/25 | Loss: 0.00138362
Iteration 17/25 | Loss: 0.00137223
Iteration 18/25 | Loss: 0.00137701
Iteration 19/25 | Loss: 0.00136251
Iteration 20/25 | Loss: 0.00136169
Iteration 21/25 | Loss: 0.00136138
Iteration 22/25 | Loss: 0.00136493
Iteration 23/25 | Loss: 0.00136136
Iteration 24/25 | Loss: 0.00136222
Iteration 25/25 | Loss: 0.00136161

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.53826457
Iteration 2/25 | Loss: 0.00120349
Iteration 3/25 | Loss: 0.00120349
Iteration 4/25 | Loss: 0.00120349
Iteration 5/25 | Loss: 0.00120349
Iteration 6/25 | Loss: 0.00120349
Iteration 7/25 | Loss: 0.00120349
Iteration 8/25 | Loss: 0.00120348
Iteration 9/25 | Loss: 0.00120212
Iteration 10/25 | Loss: 0.00120206
Iteration 11/25 | Loss: 0.00119794
Iteration 12/25 | Loss: 0.00119794
Iteration 13/25 | Loss: 0.00119794
Iteration 14/25 | Loss: 0.00119794
Iteration 15/25 | Loss: 0.00119794
Iteration 16/25 | Loss: 0.00119794
Iteration 17/25 | Loss: 0.00119794
Iteration 18/25 | Loss: 0.00119794
Iteration 19/25 | Loss: 0.00119794
Iteration 20/25 | Loss: 0.00119794
Iteration 21/25 | Loss: 0.00119794
Iteration 22/25 | Loss: 0.00119794
Iteration 23/25 | Loss: 0.00119794
Iteration 24/25 | Loss: 0.00119794
Iteration 25/25 | Loss: 0.00119794

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119794
Iteration 2/1000 | Loss: 0.00007252
Iteration 3/1000 | Loss: 0.00002598
Iteration 4/1000 | Loss: 0.00002383
Iteration 5/1000 | Loss: 0.00002263
Iteration 6/1000 | Loss: 0.00002213
Iteration 7/1000 | Loss: 0.00002176
Iteration 8/1000 | Loss: 0.00002141
Iteration 9/1000 | Loss: 0.00002116
Iteration 10/1000 | Loss: 0.00002116
Iteration 11/1000 | Loss: 0.00002093
Iteration 12/1000 | Loss: 0.00033804
Iteration 13/1000 | Loss: 0.00028852
Iteration 14/1000 | Loss: 0.00002473
Iteration 15/1000 | Loss: 0.00002153
Iteration 16/1000 | Loss: 0.00002905
Iteration 17/1000 | Loss: 0.00001966
Iteration 18/1000 | Loss: 0.00001910
Iteration 19/1000 | Loss: 0.00003122
Iteration 20/1000 | Loss: 0.00001848
Iteration 21/1000 | Loss: 0.00001845
Iteration 22/1000 | Loss: 0.00002384
Iteration 23/1000 | Loss: 0.00001815
Iteration 24/1000 | Loss: 0.00001799
Iteration 25/1000 | Loss: 0.00001788
Iteration 26/1000 | Loss: 0.00001774
Iteration 27/1000 | Loss: 0.00001760
Iteration 28/1000 | Loss: 0.00001757
Iteration 29/1000 | Loss: 0.00001755
Iteration 30/1000 | Loss: 0.00001755
Iteration 31/1000 | Loss: 0.00001754
Iteration 32/1000 | Loss: 0.00001754
Iteration 33/1000 | Loss: 0.00001752
Iteration 34/1000 | Loss: 0.00001751
Iteration 35/1000 | Loss: 0.00001751
Iteration 36/1000 | Loss: 0.00001751
Iteration 37/1000 | Loss: 0.00001751
Iteration 38/1000 | Loss: 0.00001750
Iteration 39/1000 | Loss: 0.00001750
Iteration 40/1000 | Loss: 0.00001750
Iteration 41/1000 | Loss: 0.00001750
Iteration 42/1000 | Loss: 0.00001750
Iteration 43/1000 | Loss: 0.00001750
Iteration 44/1000 | Loss: 0.00001748
Iteration 45/1000 | Loss: 0.00001748
Iteration 46/1000 | Loss: 0.00001748
Iteration 47/1000 | Loss: 0.00001748
Iteration 48/1000 | Loss: 0.00001748
Iteration 49/1000 | Loss: 0.00001748
Iteration 50/1000 | Loss: 0.00001748
Iteration 51/1000 | Loss: 0.00001748
Iteration 52/1000 | Loss: 0.00001748
Iteration 53/1000 | Loss: 0.00001748
Iteration 54/1000 | Loss: 0.00001748
Iteration 55/1000 | Loss: 0.00001747
Iteration 56/1000 | Loss: 0.00001747
Iteration 57/1000 | Loss: 0.00001747
Iteration 58/1000 | Loss: 0.00001746
Iteration 59/1000 | Loss: 0.00001746
Iteration 60/1000 | Loss: 0.00001746
Iteration 61/1000 | Loss: 0.00001746
Iteration 62/1000 | Loss: 0.00001746
Iteration 63/1000 | Loss: 0.00001746
Iteration 64/1000 | Loss: 0.00001746
Iteration 65/1000 | Loss: 0.00001745
Iteration 66/1000 | Loss: 0.00001745
Iteration 67/1000 | Loss: 0.00001745
Iteration 68/1000 | Loss: 0.00001745
Iteration 69/1000 | Loss: 0.00001745
Iteration 70/1000 | Loss: 0.00001745
Iteration 71/1000 | Loss: 0.00001745
Iteration 72/1000 | Loss: 0.00001745
Iteration 73/1000 | Loss: 0.00001745
Iteration 74/1000 | Loss: 0.00001745
Iteration 75/1000 | Loss: 0.00001745
Iteration 76/1000 | Loss: 0.00001745
Iteration 77/1000 | Loss: 0.00001745
Iteration 78/1000 | Loss: 0.00001745
Iteration 79/1000 | Loss: 0.00001745
Iteration 80/1000 | Loss: 0.00001745
Iteration 81/1000 | Loss: 0.00001745
Iteration 82/1000 | Loss: 0.00001745
Iteration 83/1000 | Loss: 0.00001745
Iteration 84/1000 | Loss: 0.00001745
Iteration 85/1000 | Loss: 0.00001745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.7451384337618947e-05, 1.7451384337618947e-05, 1.7451384337618947e-05, 1.7451384337618947e-05, 1.7451384337618947e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7451384337618947e-05

Optimization complete. Final v2v error: 3.6252148151397705 mm

Highest mean error: 4.5864973068237305 mm for frame 196

Lowest mean error: 3.479673385620117 mm for frame 47

Saving results

Total time: 92.26985049247742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808512
Iteration 2/25 | Loss: 0.00149715
Iteration 3/25 | Loss: 0.00130912
Iteration 4/25 | Loss: 0.00128272
Iteration 5/25 | Loss: 0.00127571
Iteration 6/25 | Loss: 0.00127333
Iteration 7/25 | Loss: 0.00127252
Iteration 8/25 | Loss: 0.00127223
Iteration 9/25 | Loss: 0.00127206
Iteration 10/25 | Loss: 0.00127195
Iteration 11/25 | Loss: 0.00127482
Iteration 12/25 | Loss: 0.00127337
Iteration 13/25 | Loss: 0.00127204
Iteration 14/25 | Loss: 0.00127138
Iteration 15/25 | Loss: 0.00126942
Iteration 16/25 | Loss: 0.00126895
Iteration 17/25 | Loss: 0.00126883
Iteration 18/25 | Loss: 0.00126881
Iteration 19/25 | Loss: 0.00126881
Iteration 20/25 | Loss: 0.00126881
Iteration 21/25 | Loss: 0.00126880
Iteration 22/25 | Loss: 0.00126880
Iteration 23/25 | Loss: 0.00126880
Iteration 24/25 | Loss: 0.00126880
Iteration 25/25 | Loss: 0.00126880

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.54408455
Iteration 2/25 | Loss: 0.00199425
Iteration 3/25 | Loss: 0.00199425
Iteration 4/25 | Loss: 0.00199424
Iteration 5/25 | Loss: 0.00199424
Iteration 6/25 | Loss: 0.00199424
Iteration 7/25 | Loss: 0.00199424
Iteration 8/25 | Loss: 0.00199424
Iteration 9/25 | Loss: 0.00199424
Iteration 10/25 | Loss: 0.00199424
Iteration 11/25 | Loss: 0.00199424
Iteration 12/25 | Loss: 0.00199424
Iteration 13/25 | Loss: 0.00199424
Iteration 14/25 | Loss: 0.00199424
Iteration 15/25 | Loss: 0.00199424
Iteration 16/25 | Loss: 0.00199424
Iteration 17/25 | Loss: 0.00199424
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001994242426007986, 0.001994242426007986, 0.001994242426007986, 0.001994242426007986, 0.001994242426007986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001994242426007986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199424
Iteration 2/1000 | Loss: 0.00006238
Iteration 3/1000 | Loss: 0.00003915
Iteration 4/1000 | Loss: 0.00003157
Iteration 5/1000 | Loss: 0.00002905
Iteration 6/1000 | Loss: 0.00002779
Iteration 7/1000 | Loss: 0.00002638
Iteration 8/1000 | Loss: 0.00002552
Iteration 9/1000 | Loss: 0.00002475
Iteration 10/1000 | Loss: 0.00002422
Iteration 11/1000 | Loss: 0.00002389
Iteration 12/1000 | Loss: 0.00002387
Iteration 13/1000 | Loss: 0.00002363
Iteration 14/1000 | Loss: 0.00002350
Iteration 15/1000 | Loss: 0.00002343
Iteration 16/1000 | Loss: 0.00002328
Iteration 17/1000 | Loss: 0.00002316
Iteration 18/1000 | Loss: 0.00002308
Iteration 19/1000 | Loss: 0.00002292
Iteration 20/1000 | Loss: 0.00002285
Iteration 21/1000 | Loss: 0.00002276
Iteration 22/1000 | Loss: 0.00002273
Iteration 23/1000 | Loss: 0.00002270
Iteration 24/1000 | Loss: 0.00002269
Iteration 25/1000 | Loss: 0.00002269
Iteration 26/1000 | Loss: 0.00002268
Iteration 27/1000 | Loss: 0.00002268
Iteration 28/1000 | Loss: 0.00002267
Iteration 29/1000 | Loss: 0.00002266
Iteration 30/1000 | Loss: 0.00002266
Iteration 31/1000 | Loss: 0.00002266
Iteration 32/1000 | Loss: 0.00002265
Iteration 33/1000 | Loss: 0.00002265
Iteration 34/1000 | Loss: 0.00002264
Iteration 35/1000 | Loss: 0.00002263
Iteration 36/1000 | Loss: 0.00002262
Iteration 37/1000 | Loss: 0.00002262
Iteration 38/1000 | Loss: 0.00002262
Iteration 39/1000 | Loss: 0.00002261
Iteration 40/1000 | Loss: 0.00002261
Iteration 41/1000 | Loss: 0.00002259
Iteration 42/1000 | Loss: 0.00002259
Iteration 43/1000 | Loss: 0.00002258
Iteration 44/1000 | Loss: 0.00002258
Iteration 45/1000 | Loss: 0.00002258
Iteration 46/1000 | Loss: 0.00002258
Iteration 47/1000 | Loss: 0.00002258
Iteration 48/1000 | Loss: 0.00002258
Iteration 49/1000 | Loss: 0.00002258
Iteration 50/1000 | Loss: 0.00002257
Iteration 51/1000 | Loss: 0.00002257
Iteration 52/1000 | Loss: 0.00002257
Iteration 53/1000 | Loss: 0.00002256
Iteration 54/1000 | Loss: 0.00002256
Iteration 55/1000 | Loss: 0.00002256
Iteration 56/1000 | Loss: 0.00002256
Iteration 57/1000 | Loss: 0.00002256
Iteration 58/1000 | Loss: 0.00002255
Iteration 59/1000 | Loss: 0.00002255
Iteration 60/1000 | Loss: 0.00002255
Iteration 61/1000 | Loss: 0.00002254
Iteration 62/1000 | Loss: 0.00002254
Iteration 63/1000 | Loss: 0.00002254
Iteration 64/1000 | Loss: 0.00002254
Iteration 65/1000 | Loss: 0.00002254
Iteration 66/1000 | Loss: 0.00002254
Iteration 67/1000 | Loss: 0.00002254
Iteration 68/1000 | Loss: 0.00002254
Iteration 69/1000 | Loss: 0.00002253
Iteration 70/1000 | Loss: 0.00002253
Iteration 71/1000 | Loss: 0.00002253
Iteration 72/1000 | Loss: 0.00002252
Iteration 73/1000 | Loss: 0.00002252
Iteration 74/1000 | Loss: 0.00002252
Iteration 75/1000 | Loss: 0.00002252
Iteration 76/1000 | Loss: 0.00002251
Iteration 77/1000 | Loss: 0.00002250
Iteration 78/1000 | Loss: 0.00002250
Iteration 79/1000 | Loss: 0.00002250
Iteration 80/1000 | Loss: 0.00002250
Iteration 81/1000 | Loss: 0.00002249
Iteration 82/1000 | Loss: 0.00002249
Iteration 83/1000 | Loss: 0.00002249
Iteration 84/1000 | Loss: 0.00002249
Iteration 85/1000 | Loss: 0.00002249
Iteration 86/1000 | Loss: 0.00002248
Iteration 87/1000 | Loss: 0.00002248
Iteration 88/1000 | Loss: 0.00002248
Iteration 89/1000 | Loss: 0.00002248
Iteration 90/1000 | Loss: 0.00002248
Iteration 91/1000 | Loss: 0.00002247
Iteration 92/1000 | Loss: 0.00002247
Iteration 93/1000 | Loss: 0.00002247
Iteration 94/1000 | Loss: 0.00002246
Iteration 95/1000 | Loss: 0.00002246
Iteration 96/1000 | Loss: 0.00002246
Iteration 97/1000 | Loss: 0.00002245
Iteration 98/1000 | Loss: 0.00002245
Iteration 99/1000 | Loss: 0.00002245
Iteration 100/1000 | Loss: 0.00002244
Iteration 101/1000 | Loss: 0.00002244
Iteration 102/1000 | Loss: 0.00002244
Iteration 103/1000 | Loss: 0.00002244
Iteration 104/1000 | Loss: 0.00002244
Iteration 105/1000 | Loss: 0.00002244
Iteration 106/1000 | Loss: 0.00002244
Iteration 107/1000 | Loss: 0.00002243
Iteration 108/1000 | Loss: 0.00002243
Iteration 109/1000 | Loss: 0.00002243
Iteration 110/1000 | Loss: 0.00002242
Iteration 111/1000 | Loss: 0.00002242
Iteration 112/1000 | Loss: 0.00002242
Iteration 113/1000 | Loss: 0.00002241
Iteration 114/1000 | Loss: 0.00002241
Iteration 115/1000 | Loss: 0.00002241
Iteration 116/1000 | Loss: 0.00002241
Iteration 117/1000 | Loss: 0.00002241
Iteration 118/1000 | Loss: 0.00002240
Iteration 119/1000 | Loss: 0.00002240
Iteration 120/1000 | Loss: 0.00002240
Iteration 121/1000 | Loss: 0.00002240
Iteration 122/1000 | Loss: 0.00002239
Iteration 123/1000 | Loss: 0.00002239
Iteration 124/1000 | Loss: 0.00002239
Iteration 125/1000 | Loss: 0.00002239
Iteration 126/1000 | Loss: 0.00002239
Iteration 127/1000 | Loss: 0.00002239
Iteration 128/1000 | Loss: 0.00002238
Iteration 129/1000 | Loss: 0.00002238
Iteration 130/1000 | Loss: 0.00002238
Iteration 131/1000 | Loss: 0.00002238
Iteration 132/1000 | Loss: 0.00002238
Iteration 133/1000 | Loss: 0.00002237
Iteration 134/1000 | Loss: 0.00002237
Iteration 135/1000 | Loss: 0.00002237
Iteration 136/1000 | Loss: 0.00002236
Iteration 137/1000 | Loss: 0.00002236
Iteration 138/1000 | Loss: 0.00002236
Iteration 139/1000 | Loss: 0.00002235
Iteration 140/1000 | Loss: 0.00002235
Iteration 141/1000 | Loss: 0.00002235
Iteration 142/1000 | Loss: 0.00002235
Iteration 143/1000 | Loss: 0.00002235
Iteration 144/1000 | Loss: 0.00002234
Iteration 145/1000 | Loss: 0.00002234
Iteration 146/1000 | Loss: 0.00002234
Iteration 147/1000 | Loss: 0.00002234
Iteration 148/1000 | Loss: 0.00002233
Iteration 149/1000 | Loss: 0.00002233
Iteration 150/1000 | Loss: 0.00002233
Iteration 151/1000 | Loss: 0.00002233
Iteration 152/1000 | Loss: 0.00002233
Iteration 153/1000 | Loss: 0.00002233
Iteration 154/1000 | Loss: 0.00002233
Iteration 155/1000 | Loss: 0.00002233
Iteration 156/1000 | Loss: 0.00002233
Iteration 157/1000 | Loss: 0.00002232
Iteration 158/1000 | Loss: 0.00002232
Iteration 159/1000 | Loss: 0.00002232
Iteration 160/1000 | Loss: 0.00002232
Iteration 161/1000 | Loss: 0.00002232
Iteration 162/1000 | Loss: 0.00002232
Iteration 163/1000 | Loss: 0.00002232
Iteration 164/1000 | Loss: 0.00002232
Iteration 165/1000 | Loss: 0.00002232
Iteration 166/1000 | Loss: 0.00002232
Iteration 167/1000 | Loss: 0.00002232
Iteration 168/1000 | Loss: 0.00002232
Iteration 169/1000 | Loss: 0.00002232
Iteration 170/1000 | Loss: 0.00002231
Iteration 171/1000 | Loss: 0.00002231
Iteration 172/1000 | Loss: 0.00002231
Iteration 173/1000 | Loss: 0.00002231
Iteration 174/1000 | Loss: 0.00002231
Iteration 175/1000 | Loss: 0.00002231
Iteration 176/1000 | Loss: 0.00002231
Iteration 177/1000 | Loss: 0.00002231
Iteration 178/1000 | Loss: 0.00002230
Iteration 179/1000 | Loss: 0.00002230
Iteration 180/1000 | Loss: 0.00002230
Iteration 181/1000 | Loss: 0.00002230
Iteration 182/1000 | Loss: 0.00002229
Iteration 183/1000 | Loss: 0.00002229
Iteration 184/1000 | Loss: 0.00002229
Iteration 185/1000 | Loss: 0.00002229
Iteration 186/1000 | Loss: 0.00002229
Iteration 187/1000 | Loss: 0.00002229
Iteration 188/1000 | Loss: 0.00002229
Iteration 189/1000 | Loss: 0.00002229
Iteration 190/1000 | Loss: 0.00002229
Iteration 191/1000 | Loss: 0.00002228
Iteration 192/1000 | Loss: 0.00002228
Iteration 193/1000 | Loss: 0.00002228
Iteration 194/1000 | Loss: 0.00002228
Iteration 195/1000 | Loss: 0.00002228
Iteration 196/1000 | Loss: 0.00002228
Iteration 197/1000 | Loss: 0.00002228
Iteration 198/1000 | Loss: 0.00002228
Iteration 199/1000 | Loss: 0.00002228
Iteration 200/1000 | Loss: 0.00002228
Iteration 201/1000 | Loss: 0.00002228
Iteration 202/1000 | Loss: 0.00002227
Iteration 203/1000 | Loss: 0.00002227
Iteration 204/1000 | Loss: 0.00002227
Iteration 205/1000 | Loss: 0.00002227
Iteration 206/1000 | Loss: 0.00002227
Iteration 207/1000 | Loss: 0.00002227
Iteration 208/1000 | Loss: 0.00002227
Iteration 209/1000 | Loss: 0.00002227
Iteration 210/1000 | Loss: 0.00002227
Iteration 211/1000 | Loss: 0.00002227
Iteration 212/1000 | Loss: 0.00002227
Iteration 213/1000 | Loss: 0.00002227
Iteration 214/1000 | Loss: 0.00002227
Iteration 215/1000 | Loss: 0.00002227
Iteration 216/1000 | Loss: 0.00002227
Iteration 217/1000 | Loss: 0.00002227
Iteration 218/1000 | Loss: 0.00002227
Iteration 219/1000 | Loss: 0.00002227
Iteration 220/1000 | Loss: 0.00002227
Iteration 221/1000 | Loss: 0.00002227
Iteration 222/1000 | Loss: 0.00002227
Iteration 223/1000 | Loss: 0.00002227
Iteration 224/1000 | Loss: 0.00002227
Iteration 225/1000 | Loss: 0.00002227
Iteration 226/1000 | Loss: 0.00002227
Iteration 227/1000 | Loss: 0.00002227
Iteration 228/1000 | Loss: 0.00002227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [2.226858305220958e-05, 2.226858305220958e-05, 2.226858305220958e-05, 2.226858305220958e-05, 2.226858305220958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.226858305220958e-05

Optimization complete. Final v2v error: 3.9720609188079834 mm

Highest mean error: 5.6464972496032715 mm for frame 50

Lowest mean error: 2.9863224029541016 mm for frame 27

Saving results

Total time: 68.54859375953674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067867
Iteration 2/25 | Loss: 0.00293563
Iteration 3/25 | Loss: 0.00203675
Iteration 4/25 | Loss: 0.00198975
Iteration 5/25 | Loss: 0.00176015
Iteration 6/25 | Loss: 0.00146714
Iteration 7/25 | Loss: 0.00145121
Iteration 8/25 | Loss: 0.00128528
Iteration 9/25 | Loss: 0.00130594
Iteration 10/25 | Loss: 0.00125561
Iteration 11/25 | Loss: 0.00122928
Iteration 12/25 | Loss: 0.00122810
Iteration 13/25 | Loss: 0.00122770
Iteration 14/25 | Loss: 0.00122756
Iteration 15/25 | Loss: 0.00122754
Iteration 16/25 | Loss: 0.00122754
Iteration 17/25 | Loss: 0.00122754
Iteration 18/25 | Loss: 0.00122754
Iteration 19/25 | Loss: 0.00122754
Iteration 20/25 | Loss: 0.00122754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012275351909920573, 0.0012275351909920573, 0.0012275351909920573, 0.0012275351909920573, 0.0012275351909920573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012275351909920573

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42051363
Iteration 2/25 | Loss: 0.00150018
Iteration 3/25 | Loss: 0.00150018
Iteration 4/25 | Loss: 0.00150018
Iteration 5/25 | Loss: 0.00150018
Iteration 6/25 | Loss: 0.00150018
Iteration 7/25 | Loss: 0.00150018
Iteration 8/25 | Loss: 0.00150018
Iteration 9/25 | Loss: 0.00150018
Iteration 10/25 | Loss: 0.00150018
Iteration 11/25 | Loss: 0.00150018
Iteration 12/25 | Loss: 0.00150018
Iteration 13/25 | Loss: 0.00150018
Iteration 14/25 | Loss: 0.00150018
Iteration 15/25 | Loss: 0.00150018
Iteration 16/25 | Loss: 0.00150018
Iteration 17/25 | Loss: 0.00150018
Iteration 18/25 | Loss: 0.00150018
Iteration 19/25 | Loss: 0.00150018
Iteration 20/25 | Loss: 0.00150018
Iteration 21/25 | Loss: 0.00150018
Iteration 22/25 | Loss: 0.00150018
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0015001781284809113, 0.0015001781284809113, 0.0015001781284809113, 0.0015001781284809113, 0.0015001781284809113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015001781284809113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150018
Iteration 2/1000 | Loss: 0.00003567
Iteration 3/1000 | Loss: 0.00002372
Iteration 4/1000 | Loss: 0.00002099
Iteration 5/1000 | Loss: 0.00001969
Iteration 6/1000 | Loss: 0.00001895
Iteration 7/1000 | Loss: 0.00001840
Iteration 8/1000 | Loss: 0.00001791
Iteration 9/1000 | Loss: 0.00001754
Iteration 10/1000 | Loss: 0.00001724
Iteration 11/1000 | Loss: 0.00001693
Iteration 12/1000 | Loss: 0.00001685
Iteration 13/1000 | Loss: 0.00001671
Iteration 14/1000 | Loss: 0.00001663
Iteration 15/1000 | Loss: 0.00001646
Iteration 16/1000 | Loss: 0.00001636
Iteration 17/1000 | Loss: 0.00001626
Iteration 18/1000 | Loss: 0.00001623
Iteration 19/1000 | Loss: 0.00001622
Iteration 20/1000 | Loss: 0.00001621
Iteration 21/1000 | Loss: 0.00001617
Iteration 22/1000 | Loss: 0.00001616
Iteration 23/1000 | Loss: 0.00001607
Iteration 24/1000 | Loss: 0.00001606
Iteration 25/1000 | Loss: 0.00001606
Iteration 26/1000 | Loss: 0.00001606
Iteration 27/1000 | Loss: 0.00001602
Iteration 28/1000 | Loss: 0.00001600
Iteration 29/1000 | Loss: 0.00001599
Iteration 30/1000 | Loss: 0.00001599
Iteration 31/1000 | Loss: 0.00001598
Iteration 32/1000 | Loss: 0.00001598
Iteration 33/1000 | Loss: 0.00001597
Iteration 34/1000 | Loss: 0.00001597
Iteration 35/1000 | Loss: 0.00001596
Iteration 36/1000 | Loss: 0.00001596
Iteration 37/1000 | Loss: 0.00001595
Iteration 38/1000 | Loss: 0.00001595
Iteration 39/1000 | Loss: 0.00001594
Iteration 40/1000 | Loss: 0.00001594
Iteration 41/1000 | Loss: 0.00001593
Iteration 42/1000 | Loss: 0.00001593
Iteration 43/1000 | Loss: 0.00001592
Iteration 44/1000 | Loss: 0.00001592
Iteration 45/1000 | Loss: 0.00001591
Iteration 46/1000 | Loss: 0.00001590
Iteration 47/1000 | Loss: 0.00001590
Iteration 48/1000 | Loss: 0.00001590
Iteration 49/1000 | Loss: 0.00001590
Iteration 50/1000 | Loss: 0.00001590
Iteration 51/1000 | Loss: 0.00001590
Iteration 52/1000 | Loss: 0.00001590
Iteration 53/1000 | Loss: 0.00001590
Iteration 54/1000 | Loss: 0.00001590
Iteration 55/1000 | Loss: 0.00001590
Iteration 56/1000 | Loss: 0.00001589
Iteration 57/1000 | Loss: 0.00001589
Iteration 58/1000 | Loss: 0.00001589
Iteration 59/1000 | Loss: 0.00001588
Iteration 60/1000 | Loss: 0.00001588
Iteration 61/1000 | Loss: 0.00001588
Iteration 62/1000 | Loss: 0.00001588
Iteration 63/1000 | Loss: 0.00001588
Iteration 64/1000 | Loss: 0.00001588
Iteration 65/1000 | Loss: 0.00001588
Iteration 66/1000 | Loss: 0.00001588
Iteration 67/1000 | Loss: 0.00001587
Iteration 68/1000 | Loss: 0.00001587
Iteration 69/1000 | Loss: 0.00001586
Iteration 70/1000 | Loss: 0.00001586
Iteration 71/1000 | Loss: 0.00001586
Iteration 72/1000 | Loss: 0.00001585
Iteration 73/1000 | Loss: 0.00001585
Iteration 74/1000 | Loss: 0.00001585
Iteration 75/1000 | Loss: 0.00001585
Iteration 76/1000 | Loss: 0.00001585
Iteration 77/1000 | Loss: 0.00001584
Iteration 78/1000 | Loss: 0.00001584
Iteration 79/1000 | Loss: 0.00001584
Iteration 80/1000 | Loss: 0.00001584
Iteration 81/1000 | Loss: 0.00001584
Iteration 82/1000 | Loss: 0.00001584
Iteration 83/1000 | Loss: 0.00001584
Iteration 84/1000 | Loss: 0.00001583
Iteration 85/1000 | Loss: 0.00001583
Iteration 86/1000 | Loss: 0.00001582
Iteration 87/1000 | Loss: 0.00001581
Iteration 88/1000 | Loss: 0.00001581
Iteration 89/1000 | Loss: 0.00001581
Iteration 90/1000 | Loss: 0.00001581
Iteration 91/1000 | Loss: 0.00001580
Iteration 92/1000 | Loss: 0.00001580
Iteration 93/1000 | Loss: 0.00001579
Iteration 94/1000 | Loss: 0.00001579
Iteration 95/1000 | Loss: 0.00001578
Iteration 96/1000 | Loss: 0.00001578
Iteration 97/1000 | Loss: 0.00001578
Iteration 98/1000 | Loss: 0.00001578
Iteration 99/1000 | Loss: 0.00001578
Iteration 100/1000 | Loss: 0.00001577
Iteration 101/1000 | Loss: 0.00001577
Iteration 102/1000 | Loss: 0.00001577
Iteration 103/1000 | Loss: 0.00001576
Iteration 104/1000 | Loss: 0.00001576
Iteration 105/1000 | Loss: 0.00001576
Iteration 106/1000 | Loss: 0.00001576
Iteration 107/1000 | Loss: 0.00001575
Iteration 108/1000 | Loss: 0.00001575
Iteration 109/1000 | Loss: 0.00001574
Iteration 110/1000 | Loss: 0.00001574
Iteration 111/1000 | Loss: 0.00001574
Iteration 112/1000 | Loss: 0.00001574
Iteration 113/1000 | Loss: 0.00001574
Iteration 114/1000 | Loss: 0.00001574
Iteration 115/1000 | Loss: 0.00001574
Iteration 116/1000 | Loss: 0.00001573
Iteration 117/1000 | Loss: 0.00001573
Iteration 118/1000 | Loss: 0.00001573
Iteration 119/1000 | Loss: 0.00001573
Iteration 120/1000 | Loss: 0.00001573
Iteration 121/1000 | Loss: 0.00001573
Iteration 122/1000 | Loss: 0.00001573
Iteration 123/1000 | Loss: 0.00001573
Iteration 124/1000 | Loss: 0.00001573
Iteration 125/1000 | Loss: 0.00001573
Iteration 126/1000 | Loss: 0.00001573
Iteration 127/1000 | Loss: 0.00001572
Iteration 128/1000 | Loss: 0.00001572
Iteration 129/1000 | Loss: 0.00001572
Iteration 130/1000 | Loss: 0.00001572
Iteration 131/1000 | Loss: 0.00001572
Iteration 132/1000 | Loss: 0.00001572
Iteration 133/1000 | Loss: 0.00001572
Iteration 134/1000 | Loss: 0.00001572
Iteration 135/1000 | Loss: 0.00001571
Iteration 136/1000 | Loss: 0.00001571
Iteration 137/1000 | Loss: 0.00001571
Iteration 138/1000 | Loss: 0.00001571
Iteration 139/1000 | Loss: 0.00001571
Iteration 140/1000 | Loss: 0.00001571
Iteration 141/1000 | Loss: 0.00001571
Iteration 142/1000 | Loss: 0.00001571
Iteration 143/1000 | Loss: 0.00001571
Iteration 144/1000 | Loss: 0.00001571
Iteration 145/1000 | Loss: 0.00001571
Iteration 146/1000 | Loss: 0.00001571
Iteration 147/1000 | Loss: 0.00001571
Iteration 148/1000 | Loss: 0.00001571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.570850145071745e-05, 1.570850145071745e-05, 1.570850145071745e-05, 1.570850145071745e-05, 1.570850145071745e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.570850145071745e-05

Optimization complete. Final v2v error: 3.3205113410949707 mm

Highest mean error: 4.424047946929932 mm for frame 95

Lowest mean error: 2.8167922496795654 mm for frame 157

Saving results

Total time: 61.974280834198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828541
Iteration 2/25 | Loss: 0.00129812
Iteration 3/25 | Loss: 0.00120909
Iteration 4/25 | Loss: 0.00120010
Iteration 5/25 | Loss: 0.00119825
Iteration 6/25 | Loss: 0.00119811
Iteration 7/25 | Loss: 0.00119811
Iteration 8/25 | Loss: 0.00119811
Iteration 9/25 | Loss: 0.00119811
Iteration 10/25 | Loss: 0.00119811
Iteration 11/25 | Loss: 0.00119811
Iteration 12/25 | Loss: 0.00119811
Iteration 13/25 | Loss: 0.00119811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001198107609525323, 0.001198107609525323, 0.001198107609525323, 0.001198107609525323, 0.001198107609525323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001198107609525323

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28322697
Iteration 2/25 | Loss: 0.00126045
Iteration 3/25 | Loss: 0.00126042
Iteration 4/25 | Loss: 0.00126042
Iteration 5/25 | Loss: 0.00126042
Iteration 6/25 | Loss: 0.00126042
Iteration 7/25 | Loss: 0.00126042
Iteration 8/25 | Loss: 0.00126042
Iteration 9/25 | Loss: 0.00126042
Iteration 10/25 | Loss: 0.00126042
Iteration 11/25 | Loss: 0.00126042
Iteration 12/25 | Loss: 0.00126042
Iteration 13/25 | Loss: 0.00126042
Iteration 14/25 | Loss: 0.00126042
Iteration 15/25 | Loss: 0.00126042
Iteration 16/25 | Loss: 0.00126042
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012604212388396263, 0.0012604212388396263, 0.0012604212388396263, 0.0012604212388396263, 0.0012604212388396263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012604212388396263

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126042
Iteration 2/1000 | Loss: 0.00002067
Iteration 3/1000 | Loss: 0.00001487
Iteration 4/1000 | Loss: 0.00001250
Iteration 5/1000 | Loss: 0.00001160
Iteration 6/1000 | Loss: 0.00001080
Iteration 7/1000 | Loss: 0.00001027
Iteration 8/1000 | Loss: 0.00000992
Iteration 9/1000 | Loss: 0.00000986
Iteration 10/1000 | Loss: 0.00000986
Iteration 11/1000 | Loss: 0.00000983
Iteration 12/1000 | Loss: 0.00000955
Iteration 13/1000 | Loss: 0.00000939
Iteration 14/1000 | Loss: 0.00000936
Iteration 15/1000 | Loss: 0.00000925
Iteration 16/1000 | Loss: 0.00000922
Iteration 17/1000 | Loss: 0.00000915
Iteration 18/1000 | Loss: 0.00000913
Iteration 19/1000 | Loss: 0.00000912
Iteration 20/1000 | Loss: 0.00000910
Iteration 21/1000 | Loss: 0.00000902
Iteration 22/1000 | Loss: 0.00000894
Iteration 23/1000 | Loss: 0.00000891
Iteration 24/1000 | Loss: 0.00000891
Iteration 25/1000 | Loss: 0.00000890
Iteration 26/1000 | Loss: 0.00000888
Iteration 27/1000 | Loss: 0.00000887
Iteration 28/1000 | Loss: 0.00000887
Iteration 29/1000 | Loss: 0.00000886
Iteration 30/1000 | Loss: 0.00000886
Iteration 31/1000 | Loss: 0.00000886
Iteration 32/1000 | Loss: 0.00000885
Iteration 33/1000 | Loss: 0.00000885
Iteration 34/1000 | Loss: 0.00000884
Iteration 35/1000 | Loss: 0.00000884
Iteration 36/1000 | Loss: 0.00000883
Iteration 37/1000 | Loss: 0.00000882
Iteration 38/1000 | Loss: 0.00000882
Iteration 39/1000 | Loss: 0.00000882
Iteration 40/1000 | Loss: 0.00000882
Iteration 41/1000 | Loss: 0.00000882
Iteration 42/1000 | Loss: 0.00000881
Iteration 43/1000 | Loss: 0.00000881
Iteration 44/1000 | Loss: 0.00000881
Iteration 45/1000 | Loss: 0.00000881
Iteration 46/1000 | Loss: 0.00000881
Iteration 47/1000 | Loss: 0.00000880
Iteration 48/1000 | Loss: 0.00000880
Iteration 49/1000 | Loss: 0.00000879
Iteration 50/1000 | Loss: 0.00000879
Iteration 51/1000 | Loss: 0.00000879
Iteration 52/1000 | Loss: 0.00000879
Iteration 53/1000 | Loss: 0.00000878
Iteration 54/1000 | Loss: 0.00000878
Iteration 55/1000 | Loss: 0.00000878
Iteration 56/1000 | Loss: 0.00000877
Iteration 57/1000 | Loss: 0.00000877
Iteration 58/1000 | Loss: 0.00000877
Iteration 59/1000 | Loss: 0.00000877
Iteration 60/1000 | Loss: 0.00000877
Iteration 61/1000 | Loss: 0.00000877
Iteration 62/1000 | Loss: 0.00000877
Iteration 63/1000 | Loss: 0.00000877
Iteration 64/1000 | Loss: 0.00000877
Iteration 65/1000 | Loss: 0.00000876
Iteration 66/1000 | Loss: 0.00000876
Iteration 67/1000 | Loss: 0.00000876
Iteration 68/1000 | Loss: 0.00000876
Iteration 69/1000 | Loss: 0.00000876
Iteration 70/1000 | Loss: 0.00000876
Iteration 71/1000 | Loss: 0.00000876
Iteration 72/1000 | Loss: 0.00000875
Iteration 73/1000 | Loss: 0.00000875
Iteration 74/1000 | Loss: 0.00000875
Iteration 75/1000 | Loss: 0.00000875
Iteration 76/1000 | Loss: 0.00000874
Iteration 77/1000 | Loss: 0.00000874
Iteration 78/1000 | Loss: 0.00000874
Iteration 79/1000 | Loss: 0.00000874
Iteration 80/1000 | Loss: 0.00000873
Iteration 81/1000 | Loss: 0.00000873
Iteration 82/1000 | Loss: 0.00000873
Iteration 83/1000 | Loss: 0.00000872
Iteration 84/1000 | Loss: 0.00000872
Iteration 85/1000 | Loss: 0.00000872
Iteration 86/1000 | Loss: 0.00000872
Iteration 87/1000 | Loss: 0.00000872
Iteration 88/1000 | Loss: 0.00000871
Iteration 89/1000 | Loss: 0.00000871
Iteration 90/1000 | Loss: 0.00000870
Iteration 91/1000 | Loss: 0.00000870
Iteration 92/1000 | Loss: 0.00000869
Iteration 93/1000 | Loss: 0.00000869
Iteration 94/1000 | Loss: 0.00000868
Iteration 95/1000 | Loss: 0.00000868
Iteration 96/1000 | Loss: 0.00000867
Iteration 97/1000 | Loss: 0.00000867
Iteration 98/1000 | Loss: 0.00000866
Iteration 99/1000 | Loss: 0.00000866
Iteration 100/1000 | Loss: 0.00000866
Iteration 101/1000 | Loss: 0.00000865
Iteration 102/1000 | Loss: 0.00000865
Iteration 103/1000 | Loss: 0.00000865
Iteration 104/1000 | Loss: 0.00000865
Iteration 105/1000 | Loss: 0.00000864
Iteration 106/1000 | Loss: 0.00000864
Iteration 107/1000 | Loss: 0.00000863
Iteration 108/1000 | Loss: 0.00000863
Iteration 109/1000 | Loss: 0.00000863
Iteration 110/1000 | Loss: 0.00000862
Iteration 111/1000 | Loss: 0.00000862
Iteration 112/1000 | Loss: 0.00000862
Iteration 113/1000 | Loss: 0.00000862
Iteration 114/1000 | Loss: 0.00000862
Iteration 115/1000 | Loss: 0.00000862
Iteration 116/1000 | Loss: 0.00000862
Iteration 117/1000 | Loss: 0.00000862
Iteration 118/1000 | Loss: 0.00000862
Iteration 119/1000 | Loss: 0.00000862
Iteration 120/1000 | Loss: 0.00000862
Iteration 121/1000 | Loss: 0.00000862
Iteration 122/1000 | Loss: 0.00000862
Iteration 123/1000 | Loss: 0.00000862
Iteration 124/1000 | Loss: 0.00000862
Iteration 125/1000 | Loss: 0.00000861
Iteration 126/1000 | Loss: 0.00000861
Iteration 127/1000 | Loss: 0.00000860
Iteration 128/1000 | Loss: 0.00000860
Iteration 129/1000 | Loss: 0.00000860
Iteration 130/1000 | Loss: 0.00000860
Iteration 131/1000 | Loss: 0.00000860
Iteration 132/1000 | Loss: 0.00000859
Iteration 133/1000 | Loss: 0.00000859
Iteration 134/1000 | Loss: 0.00000859
Iteration 135/1000 | Loss: 0.00000859
Iteration 136/1000 | Loss: 0.00000859
Iteration 137/1000 | Loss: 0.00000859
Iteration 138/1000 | Loss: 0.00000859
Iteration 139/1000 | Loss: 0.00000858
Iteration 140/1000 | Loss: 0.00000858
Iteration 141/1000 | Loss: 0.00000858
Iteration 142/1000 | Loss: 0.00000857
Iteration 143/1000 | Loss: 0.00000857
Iteration 144/1000 | Loss: 0.00000857
Iteration 145/1000 | Loss: 0.00000857
Iteration 146/1000 | Loss: 0.00000856
Iteration 147/1000 | Loss: 0.00000856
Iteration 148/1000 | Loss: 0.00000856
Iteration 149/1000 | Loss: 0.00000856
Iteration 150/1000 | Loss: 0.00000856
Iteration 151/1000 | Loss: 0.00000856
Iteration 152/1000 | Loss: 0.00000855
Iteration 153/1000 | Loss: 0.00000855
Iteration 154/1000 | Loss: 0.00000855
Iteration 155/1000 | Loss: 0.00000855
Iteration 156/1000 | Loss: 0.00000855
Iteration 157/1000 | Loss: 0.00000854
Iteration 158/1000 | Loss: 0.00000854
Iteration 159/1000 | Loss: 0.00000854
Iteration 160/1000 | Loss: 0.00000853
Iteration 161/1000 | Loss: 0.00000853
Iteration 162/1000 | Loss: 0.00000853
Iteration 163/1000 | Loss: 0.00000853
Iteration 164/1000 | Loss: 0.00000853
Iteration 165/1000 | Loss: 0.00000853
Iteration 166/1000 | Loss: 0.00000853
Iteration 167/1000 | Loss: 0.00000853
Iteration 168/1000 | Loss: 0.00000853
Iteration 169/1000 | Loss: 0.00000852
Iteration 170/1000 | Loss: 0.00000852
Iteration 171/1000 | Loss: 0.00000852
Iteration 172/1000 | Loss: 0.00000852
Iteration 173/1000 | Loss: 0.00000852
Iteration 174/1000 | Loss: 0.00000852
Iteration 175/1000 | Loss: 0.00000852
Iteration 176/1000 | Loss: 0.00000851
Iteration 177/1000 | Loss: 0.00000851
Iteration 178/1000 | Loss: 0.00000851
Iteration 179/1000 | Loss: 0.00000851
Iteration 180/1000 | Loss: 0.00000851
Iteration 181/1000 | Loss: 0.00000851
Iteration 182/1000 | Loss: 0.00000851
Iteration 183/1000 | Loss: 0.00000851
Iteration 184/1000 | Loss: 0.00000851
Iteration 185/1000 | Loss: 0.00000851
Iteration 186/1000 | Loss: 0.00000851
Iteration 187/1000 | Loss: 0.00000851
Iteration 188/1000 | Loss: 0.00000851
Iteration 189/1000 | Loss: 0.00000851
Iteration 190/1000 | Loss: 0.00000850
Iteration 191/1000 | Loss: 0.00000850
Iteration 192/1000 | Loss: 0.00000850
Iteration 193/1000 | Loss: 0.00000850
Iteration 194/1000 | Loss: 0.00000850
Iteration 195/1000 | Loss: 0.00000850
Iteration 196/1000 | Loss: 0.00000850
Iteration 197/1000 | Loss: 0.00000850
Iteration 198/1000 | Loss: 0.00000850
Iteration 199/1000 | Loss: 0.00000850
Iteration 200/1000 | Loss: 0.00000850
Iteration 201/1000 | Loss: 0.00000850
Iteration 202/1000 | Loss: 0.00000850
Iteration 203/1000 | Loss: 0.00000849
Iteration 204/1000 | Loss: 0.00000849
Iteration 205/1000 | Loss: 0.00000849
Iteration 206/1000 | Loss: 0.00000849
Iteration 207/1000 | Loss: 0.00000849
Iteration 208/1000 | Loss: 0.00000849
Iteration 209/1000 | Loss: 0.00000849
Iteration 210/1000 | Loss: 0.00000849
Iteration 211/1000 | Loss: 0.00000849
Iteration 212/1000 | Loss: 0.00000849
Iteration 213/1000 | Loss: 0.00000849
Iteration 214/1000 | Loss: 0.00000849
Iteration 215/1000 | Loss: 0.00000849
Iteration 216/1000 | Loss: 0.00000849
Iteration 217/1000 | Loss: 0.00000849
Iteration 218/1000 | Loss: 0.00000849
Iteration 219/1000 | Loss: 0.00000849
Iteration 220/1000 | Loss: 0.00000849
Iteration 221/1000 | Loss: 0.00000849
Iteration 222/1000 | Loss: 0.00000849
Iteration 223/1000 | Loss: 0.00000849
Iteration 224/1000 | Loss: 0.00000849
Iteration 225/1000 | Loss: 0.00000849
Iteration 226/1000 | Loss: 0.00000849
Iteration 227/1000 | Loss: 0.00000849
Iteration 228/1000 | Loss: 0.00000849
Iteration 229/1000 | Loss: 0.00000849
Iteration 230/1000 | Loss: 0.00000849
Iteration 231/1000 | Loss: 0.00000849
Iteration 232/1000 | Loss: 0.00000849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [8.494954272464383e-06, 8.494954272464383e-06, 8.494954272464383e-06, 8.494954272464383e-06, 8.494954272464383e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.494954272464383e-06

Optimization complete. Final v2v error: 2.5371601581573486 mm

Highest mean error: 2.775242567062378 mm for frame 29

Lowest mean error: 2.4270033836364746 mm for frame 105

Saving results

Total time: 42.15951442718506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404729
Iteration 2/25 | Loss: 0.00137534
Iteration 3/25 | Loss: 0.00122871
Iteration 4/25 | Loss: 0.00121679
Iteration 5/25 | Loss: 0.00121523
Iteration 6/25 | Loss: 0.00121498
Iteration 7/25 | Loss: 0.00121498
Iteration 8/25 | Loss: 0.00121498
Iteration 9/25 | Loss: 0.00121498
Iteration 10/25 | Loss: 0.00121498
Iteration 11/25 | Loss: 0.00121498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012149757239967585, 0.0012149757239967585, 0.0012149757239967585, 0.0012149757239967585, 0.0012149757239967585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012149757239967585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27899790
Iteration 2/25 | Loss: 0.00118907
Iteration 3/25 | Loss: 0.00118906
Iteration 4/25 | Loss: 0.00118906
Iteration 5/25 | Loss: 0.00118906
Iteration 6/25 | Loss: 0.00118906
Iteration 7/25 | Loss: 0.00118906
Iteration 8/25 | Loss: 0.00118906
Iteration 9/25 | Loss: 0.00118906
Iteration 10/25 | Loss: 0.00118906
Iteration 11/25 | Loss: 0.00118906
Iteration 12/25 | Loss: 0.00118906
Iteration 13/25 | Loss: 0.00118906
Iteration 14/25 | Loss: 0.00118906
Iteration 15/25 | Loss: 0.00118906
Iteration 16/25 | Loss: 0.00118906
Iteration 17/25 | Loss: 0.00118906
Iteration 18/25 | Loss: 0.00118906
Iteration 19/25 | Loss: 0.00118906
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011890596942976117, 0.0011890596942976117, 0.0011890596942976117, 0.0011890596942976117, 0.0011890596942976117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011890596942976117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118906
Iteration 2/1000 | Loss: 0.00002657
Iteration 3/1000 | Loss: 0.00001849
Iteration 4/1000 | Loss: 0.00001623
Iteration 5/1000 | Loss: 0.00001532
Iteration 6/1000 | Loss: 0.00001458
Iteration 7/1000 | Loss: 0.00001385
Iteration 8/1000 | Loss: 0.00001354
Iteration 9/1000 | Loss: 0.00001320
Iteration 10/1000 | Loss: 0.00001296
Iteration 11/1000 | Loss: 0.00001276
Iteration 12/1000 | Loss: 0.00001261
Iteration 13/1000 | Loss: 0.00001257
Iteration 14/1000 | Loss: 0.00001254
Iteration 15/1000 | Loss: 0.00001248
Iteration 16/1000 | Loss: 0.00001239
Iteration 17/1000 | Loss: 0.00001236
Iteration 18/1000 | Loss: 0.00001236
Iteration 19/1000 | Loss: 0.00001224
Iteration 20/1000 | Loss: 0.00001224
Iteration 21/1000 | Loss: 0.00001222
Iteration 22/1000 | Loss: 0.00001221
Iteration 23/1000 | Loss: 0.00001221
Iteration 24/1000 | Loss: 0.00001220
Iteration 25/1000 | Loss: 0.00001220
Iteration 26/1000 | Loss: 0.00001219
Iteration 27/1000 | Loss: 0.00001219
Iteration 28/1000 | Loss: 0.00001218
Iteration 29/1000 | Loss: 0.00001218
Iteration 30/1000 | Loss: 0.00001217
Iteration 31/1000 | Loss: 0.00001217
Iteration 32/1000 | Loss: 0.00001216
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001214
Iteration 35/1000 | Loss: 0.00001214
Iteration 36/1000 | Loss: 0.00001212
Iteration 37/1000 | Loss: 0.00001208
Iteration 38/1000 | Loss: 0.00001207
Iteration 39/1000 | Loss: 0.00001206
Iteration 40/1000 | Loss: 0.00001206
Iteration 41/1000 | Loss: 0.00001205
Iteration 42/1000 | Loss: 0.00001201
Iteration 43/1000 | Loss: 0.00001200
Iteration 44/1000 | Loss: 0.00001199
Iteration 45/1000 | Loss: 0.00001195
Iteration 46/1000 | Loss: 0.00001194
Iteration 47/1000 | Loss: 0.00001194
Iteration 48/1000 | Loss: 0.00001193
Iteration 49/1000 | Loss: 0.00001193
Iteration 50/1000 | Loss: 0.00001192
Iteration 51/1000 | Loss: 0.00001190
Iteration 52/1000 | Loss: 0.00001190
Iteration 53/1000 | Loss: 0.00001189
Iteration 54/1000 | Loss: 0.00001189
Iteration 55/1000 | Loss: 0.00001189
Iteration 56/1000 | Loss: 0.00001187
Iteration 57/1000 | Loss: 0.00001185
Iteration 58/1000 | Loss: 0.00001185
Iteration 59/1000 | Loss: 0.00001185
Iteration 60/1000 | Loss: 0.00001185
Iteration 61/1000 | Loss: 0.00001185
Iteration 62/1000 | Loss: 0.00001185
Iteration 63/1000 | Loss: 0.00001184
Iteration 64/1000 | Loss: 0.00001184
Iteration 65/1000 | Loss: 0.00001184
Iteration 66/1000 | Loss: 0.00001184
Iteration 67/1000 | Loss: 0.00001184
Iteration 68/1000 | Loss: 0.00001184
Iteration 69/1000 | Loss: 0.00001184
Iteration 70/1000 | Loss: 0.00001183
Iteration 71/1000 | Loss: 0.00001182
Iteration 72/1000 | Loss: 0.00001182
Iteration 73/1000 | Loss: 0.00001182
Iteration 74/1000 | Loss: 0.00001182
Iteration 75/1000 | Loss: 0.00001182
Iteration 76/1000 | Loss: 0.00001182
Iteration 77/1000 | Loss: 0.00001182
Iteration 78/1000 | Loss: 0.00001182
Iteration 79/1000 | Loss: 0.00001181
Iteration 80/1000 | Loss: 0.00001181
Iteration 81/1000 | Loss: 0.00001181
Iteration 82/1000 | Loss: 0.00001181
Iteration 83/1000 | Loss: 0.00001180
Iteration 84/1000 | Loss: 0.00001180
Iteration 85/1000 | Loss: 0.00001180
Iteration 86/1000 | Loss: 0.00001180
Iteration 87/1000 | Loss: 0.00001179
Iteration 88/1000 | Loss: 0.00001179
Iteration 89/1000 | Loss: 0.00001179
Iteration 90/1000 | Loss: 0.00001179
Iteration 91/1000 | Loss: 0.00001179
Iteration 92/1000 | Loss: 0.00001178
Iteration 93/1000 | Loss: 0.00001178
Iteration 94/1000 | Loss: 0.00001178
Iteration 95/1000 | Loss: 0.00001177
Iteration 96/1000 | Loss: 0.00001177
Iteration 97/1000 | Loss: 0.00001177
Iteration 98/1000 | Loss: 0.00001177
Iteration 99/1000 | Loss: 0.00001177
Iteration 100/1000 | Loss: 0.00001176
Iteration 101/1000 | Loss: 0.00001176
Iteration 102/1000 | Loss: 0.00001176
Iteration 103/1000 | Loss: 0.00001176
Iteration 104/1000 | Loss: 0.00001176
Iteration 105/1000 | Loss: 0.00001176
Iteration 106/1000 | Loss: 0.00001176
Iteration 107/1000 | Loss: 0.00001176
Iteration 108/1000 | Loss: 0.00001175
Iteration 109/1000 | Loss: 0.00001175
Iteration 110/1000 | Loss: 0.00001175
Iteration 111/1000 | Loss: 0.00001175
Iteration 112/1000 | Loss: 0.00001175
Iteration 113/1000 | Loss: 0.00001175
Iteration 114/1000 | Loss: 0.00001175
Iteration 115/1000 | Loss: 0.00001175
Iteration 116/1000 | Loss: 0.00001175
Iteration 117/1000 | Loss: 0.00001174
Iteration 118/1000 | Loss: 0.00001174
Iteration 119/1000 | Loss: 0.00001174
Iteration 120/1000 | Loss: 0.00001174
Iteration 121/1000 | Loss: 0.00001174
Iteration 122/1000 | Loss: 0.00001174
Iteration 123/1000 | Loss: 0.00001174
Iteration 124/1000 | Loss: 0.00001174
Iteration 125/1000 | Loss: 0.00001174
Iteration 126/1000 | Loss: 0.00001174
Iteration 127/1000 | Loss: 0.00001173
Iteration 128/1000 | Loss: 0.00001173
Iteration 129/1000 | Loss: 0.00001173
Iteration 130/1000 | Loss: 0.00001173
Iteration 131/1000 | Loss: 0.00001173
Iteration 132/1000 | Loss: 0.00001172
Iteration 133/1000 | Loss: 0.00001172
Iteration 134/1000 | Loss: 0.00001172
Iteration 135/1000 | Loss: 0.00001172
Iteration 136/1000 | Loss: 0.00001171
Iteration 137/1000 | Loss: 0.00001171
Iteration 138/1000 | Loss: 0.00001171
Iteration 139/1000 | Loss: 0.00001170
Iteration 140/1000 | Loss: 0.00001170
Iteration 141/1000 | Loss: 0.00001170
Iteration 142/1000 | Loss: 0.00001170
Iteration 143/1000 | Loss: 0.00001169
Iteration 144/1000 | Loss: 0.00001169
Iteration 145/1000 | Loss: 0.00001169
Iteration 146/1000 | Loss: 0.00001169
Iteration 147/1000 | Loss: 0.00001169
Iteration 148/1000 | Loss: 0.00001169
Iteration 149/1000 | Loss: 0.00001169
Iteration 150/1000 | Loss: 0.00001169
Iteration 151/1000 | Loss: 0.00001169
Iteration 152/1000 | Loss: 0.00001169
Iteration 153/1000 | Loss: 0.00001168
Iteration 154/1000 | Loss: 0.00001168
Iteration 155/1000 | Loss: 0.00001168
Iteration 156/1000 | Loss: 0.00001168
Iteration 157/1000 | Loss: 0.00001168
Iteration 158/1000 | Loss: 0.00001168
Iteration 159/1000 | Loss: 0.00001168
Iteration 160/1000 | Loss: 0.00001167
Iteration 161/1000 | Loss: 0.00001167
Iteration 162/1000 | Loss: 0.00001166
Iteration 163/1000 | Loss: 0.00001166
Iteration 164/1000 | Loss: 0.00001166
Iteration 165/1000 | Loss: 0.00001166
Iteration 166/1000 | Loss: 0.00001166
Iteration 167/1000 | Loss: 0.00001166
Iteration 168/1000 | Loss: 0.00001165
Iteration 169/1000 | Loss: 0.00001165
Iteration 170/1000 | Loss: 0.00001164
Iteration 171/1000 | Loss: 0.00001164
Iteration 172/1000 | Loss: 0.00001164
Iteration 173/1000 | Loss: 0.00001164
Iteration 174/1000 | Loss: 0.00001163
Iteration 175/1000 | Loss: 0.00001163
Iteration 176/1000 | Loss: 0.00001163
Iteration 177/1000 | Loss: 0.00001163
Iteration 178/1000 | Loss: 0.00001163
Iteration 179/1000 | Loss: 0.00001162
Iteration 180/1000 | Loss: 0.00001162
Iteration 181/1000 | Loss: 0.00001162
Iteration 182/1000 | Loss: 0.00001162
Iteration 183/1000 | Loss: 0.00001162
Iteration 184/1000 | Loss: 0.00001162
Iteration 185/1000 | Loss: 0.00001162
Iteration 186/1000 | Loss: 0.00001162
Iteration 187/1000 | Loss: 0.00001162
Iteration 188/1000 | Loss: 0.00001162
Iteration 189/1000 | Loss: 0.00001162
Iteration 190/1000 | Loss: 0.00001162
Iteration 191/1000 | Loss: 0.00001161
Iteration 192/1000 | Loss: 0.00001161
Iteration 193/1000 | Loss: 0.00001161
Iteration 194/1000 | Loss: 0.00001161
Iteration 195/1000 | Loss: 0.00001160
Iteration 196/1000 | Loss: 0.00001160
Iteration 197/1000 | Loss: 0.00001160
Iteration 198/1000 | Loss: 0.00001160
Iteration 199/1000 | Loss: 0.00001160
Iteration 200/1000 | Loss: 0.00001160
Iteration 201/1000 | Loss: 0.00001160
Iteration 202/1000 | Loss: 0.00001160
Iteration 203/1000 | Loss: 0.00001160
Iteration 204/1000 | Loss: 0.00001160
Iteration 205/1000 | Loss: 0.00001160
Iteration 206/1000 | Loss: 0.00001159
Iteration 207/1000 | Loss: 0.00001159
Iteration 208/1000 | Loss: 0.00001159
Iteration 209/1000 | Loss: 0.00001159
Iteration 210/1000 | Loss: 0.00001159
Iteration 211/1000 | Loss: 0.00001158
Iteration 212/1000 | Loss: 0.00001158
Iteration 213/1000 | Loss: 0.00001158
Iteration 214/1000 | Loss: 0.00001158
Iteration 215/1000 | Loss: 0.00001158
Iteration 216/1000 | Loss: 0.00001158
Iteration 217/1000 | Loss: 0.00001158
Iteration 218/1000 | Loss: 0.00001158
Iteration 219/1000 | Loss: 0.00001158
Iteration 220/1000 | Loss: 0.00001157
Iteration 221/1000 | Loss: 0.00001157
Iteration 222/1000 | Loss: 0.00001157
Iteration 223/1000 | Loss: 0.00001157
Iteration 224/1000 | Loss: 0.00001157
Iteration 225/1000 | Loss: 0.00001157
Iteration 226/1000 | Loss: 0.00001157
Iteration 227/1000 | Loss: 0.00001157
Iteration 228/1000 | Loss: 0.00001157
Iteration 229/1000 | Loss: 0.00001157
Iteration 230/1000 | Loss: 0.00001157
Iteration 231/1000 | Loss: 0.00001157
Iteration 232/1000 | Loss: 0.00001157
Iteration 233/1000 | Loss: 0.00001157
Iteration 234/1000 | Loss: 0.00001157
Iteration 235/1000 | Loss: 0.00001157
Iteration 236/1000 | Loss: 0.00001157
Iteration 237/1000 | Loss: 0.00001157
Iteration 238/1000 | Loss: 0.00001157
Iteration 239/1000 | Loss: 0.00001157
Iteration 240/1000 | Loss: 0.00001157
Iteration 241/1000 | Loss: 0.00001157
Iteration 242/1000 | Loss: 0.00001157
Iteration 243/1000 | Loss: 0.00001157
Iteration 244/1000 | Loss: 0.00001157
Iteration 245/1000 | Loss: 0.00001157
Iteration 246/1000 | Loss: 0.00001157
Iteration 247/1000 | Loss: 0.00001157
Iteration 248/1000 | Loss: 0.00001157
Iteration 249/1000 | Loss: 0.00001157
Iteration 250/1000 | Loss: 0.00001157
Iteration 251/1000 | Loss: 0.00001157
Iteration 252/1000 | Loss: 0.00001157
Iteration 253/1000 | Loss: 0.00001157
Iteration 254/1000 | Loss: 0.00001157
Iteration 255/1000 | Loss: 0.00001157
Iteration 256/1000 | Loss: 0.00001157
Iteration 257/1000 | Loss: 0.00001157
Iteration 258/1000 | Loss: 0.00001157
Iteration 259/1000 | Loss: 0.00001157
Iteration 260/1000 | Loss: 0.00001157
Iteration 261/1000 | Loss: 0.00001157
Iteration 262/1000 | Loss: 0.00001157
Iteration 263/1000 | Loss: 0.00001157
Iteration 264/1000 | Loss: 0.00001157
Iteration 265/1000 | Loss: 0.00001157
Iteration 266/1000 | Loss: 0.00001157
Iteration 267/1000 | Loss: 0.00001157
Iteration 268/1000 | Loss: 0.00001157
Iteration 269/1000 | Loss: 0.00001157
Iteration 270/1000 | Loss: 0.00001157
Iteration 271/1000 | Loss: 0.00001157
Iteration 272/1000 | Loss: 0.00001157
Iteration 273/1000 | Loss: 0.00001157
Iteration 274/1000 | Loss: 0.00001157
Iteration 275/1000 | Loss: 0.00001157
Iteration 276/1000 | Loss: 0.00001157
Iteration 277/1000 | Loss: 0.00001157
Iteration 278/1000 | Loss: 0.00001157
Iteration 279/1000 | Loss: 0.00001157
Iteration 280/1000 | Loss: 0.00001157
Iteration 281/1000 | Loss: 0.00001157
Iteration 282/1000 | Loss: 0.00001157
Iteration 283/1000 | Loss: 0.00001157
Iteration 284/1000 | Loss: 0.00001157
Iteration 285/1000 | Loss: 0.00001157
Iteration 286/1000 | Loss: 0.00001157
Iteration 287/1000 | Loss: 0.00001157
Iteration 288/1000 | Loss: 0.00001157
Iteration 289/1000 | Loss: 0.00001157
Iteration 290/1000 | Loss: 0.00001157
Iteration 291/1000 | Loss: 0.00001157
Iteration 292/1000 | Loss: 0.00001157
Iteration 293/1000 | Loss: 0.00001157
Iteration 294/1000 | Loss: 0.00001157
Iteration 295/1000 | Loss: 0.00001157
Iteration 296/1000 | Loss: 0.00001157
Iteration 297/1000 | Loss: 0.00001157
Iteration 298/1000 | Loss: 0.00001157
Iteration 299/1000 | Loss: 0.00001157
Iteration 300/1000 | Loss: 0.00001157
Iteration 301/1000 | Loss: 0.00001157
Iteration 302/1000 | Loss: 0.00001157
Iteration 303/1000 | Loss: 0.00001157
Iteration 304/1000 | Loss: 0.00001157
Iteration 305/1000 | Loss: 0.00001157
Iteration 306/1000 | Loss: 0.00001157
Iteration 307/1000 | Loss: 0.00001157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 307. Stopping optimization.
Last 5 losses: [1.1571381037356332e-05, 1.1571381037356332e-05, 1.1571381037356332e-05, 1.1571381037356332e-05, 1.1571381037356332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1571381037356332e-05

Optimization complete. Final v2v error: 2.9340035915374756 mm

Highest mean error: 3.04807710647583 mm for frame 87

Lowest mean error: 2.821699380874634 mm for frame 118

Saving results

Total time: 47.035300970077515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00218057
Iteration 2/25 | Loss: 0.00130911
Iteration 3/25 | Loss: 0.00121182
Iteration 4/25 | Loss: 0.00118847
Iteration 5/25 | Loss: 0.00118004
Iteration 6/25 | Loss: 0.00117732
Iteration 7/25 | Loss: 0.00117669
Iteration 8/25 | Loss: 0.00117669
Iteration 9/25 | Loss: 0.00117669
Iteration 10/25 | Loss: 0.00117669
Iteration 11/25 | Loss: 0.00117669
Iteration 12/25 | Loss: 0.00117669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011766869574785233, 0.0011766869574785233, 0.0011766869574785233, 0.0011766869574785233, 0.0011766869574785233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011766869574785233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25945854
Iteration 2/25 | Loss: 0.00268189
Iteration 3/25 | Loss: 0.00268189
Iteration 4/25 | Loss: 0.00268189
Iteration 5/25 | Loss: 0.00268189
Iteration 6/25 | Loss: 0.00268189
Iteration 7/25 | Loss: 0.00268189
Iteration 8/25 | Loss: 0.00268189
Iteration 9/25 | Loss: 0.00268189
Iteration 10/25 | Loss: 0.00268189
Iteration 11/25 | Loss: 0.00268189
Iteration 12/25 | Loss: 0.00268189
Iteration 13/25 | Loss: 0.00268189
Iteration 14/25 | Loss: 0.00268189
Iteration 15/25 | Loss: 0.00268189
Iteration 16/25 | Loss: 0.00268189
Iteration 17/25 | Loss: 0.00268189
Iteration 18/25 | Loss: 0.00268189
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00268188607878983, 0.00268188607878983, 0.00268188607878983, 0.00268188607878983, 0.00268188607878983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00268188607878983

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00268189
Iteration 2/1000 | Loss: 0.00003290
Iteration 3/1000 | Loss: 0.00002258
Iteration 4/1000 | Loss: 0.00001953
Iteration 5/1000 | Loss: 0.00001817
Iteration 6/1000 | Loss: 0.00001739
Iteration 7/1000 | Loss: 0.00001675
Iteration 8/1000 | Loss: 0.00001639
Iteration 9/1000 | Loss: 0.00001622
Iteration 10/1000 | Loss: 0.00001598
Iteration 11/1000 | Loss: 0.00001577
Iteration 12/1000 | Loss: 0.00001565
Iteration 13/1000 | Loss: 0.00001564
Iteration 14/1000 | Loss: 0.00001563
Iteration 15/1000 | Loss: 0.00001557
Iteration 16/1000 | Loss: 0.00001557
Iteration 17/1000 | Loss: 0.00001557
Iteration 18/1000 | Loss: 0.00001557
Iteration 19/1000 | Loss: 0.00001556
Iteration 20/1000 | Loss: 0.00001556
Iteration 21/1000 | Loss: 0.00001549
Iteration 22/1000 | Loss: 0.00001548
Iteration 23/1000 | Loss: 0.00001545
Iteration 24/1000 | Loss: 0.00001544
Iteration 25/1000 | Loss: 0.00001543
Iteration 26/1000 | Loss: 0.00001543
Iteration 27/1000 | Loss: 0.00001541
Iteration 28/1000 | Loss: 0.00001541
Iteration 29/1000 | Loss: 0.00001541
Iteration 30/1000 | Loss: 0.00001541
Iteration 31/1000 | Loss: 0.00001537
Iteration 32/1000 | Loss: 0.00001536
Iteration 33/1000 | Loss: 0.00001532
Iteration 34/1000 | Loss: 0.00001531
Iteration 35/1000 | Loss: 0.00001531
Iteration 36/1000 | Loss: 0.00001531
Iteration 37/1000 | Loss: 0.00001530
Iteration 38/1000 | Loss: 0.00001530
Iteration 39/1000 | Loss: 0.00001529
Iteration 40/1000 | Loss: 0.00001528
Iteration 41/1000 | Loss: 0.00001528
Iteration 42/1000 | Loss: 0.00001527
Iteration 43/1000 | Loss: 0.00001527
Iteration 44/1000 | Loss: 0.00001526
Iteration 45/1000 | Loss: 0.00001524
Iteration 46/1000 | Loss: 0.00001524
Iteration 47/1000 | Loss: 0.00001523
Iteration 48/1000 | Loss: 0.00001522
Iteration 49/1000 | Loss: 0.00001521
Iteration 50/1000 | Loss: 0.00001521
Iteration 51/1000 | Loss: 0.00001521
Iteration 52/1000 | Loss: 0.00001520
Iteration 53/1000 | Loss: 0.00001520
Iteration 54/1000 | Loss: 0.00001519
Iteration 55/1000 | Loss: 0.00001519
Iteration 56/1000 | Loss: 0.00001517
Iteration 57/1000 | Loss: 0.00001516
Iteration 58/1000 | Loss: 0.00001516
Iteration 59/1000 | Loss: 0.00001515
Iteration 60/1000 | Loss: 0.00001515
Iteration 61/1000 | Loss: 0.00001515
Iteration 62/1000 | Loss: 0.00001514
Iteration 63/1000 | Loss: 0.00001513
Iteration 64/1000 | Loss: 0.00001511
Iteration 65/1000 | Loss: 0.00001511
Iteration 66/1000 | Loss: 0.00001510
Iteration 67/1000 | Loss: 0.00001510
Iteration 68/1000 | Loss: 0.00001510
Iteration 69/1000 | Loss: 0.00001510
Iteration 70/1000 | Loss: 0.00001510
Iteration 71/1000 | Loss: 0.00001510
Iteration 72/1000 | Loss: 0.00001510
Iteration 73/1000 | Loss: 0.00001510
Iteration 74/1000 | Loss: 0.00001510
Iteration 75/1000 | Loss: 0.00001509
Iteration 76/1000 | Loss: 0.00001509
Iteration 77/1000 | Loss: 0.00001509
Iteration 78/1000 | Loss: 0.00001509
Iteration 79/1000 | Loss: 0.00001509
Iteration 80/1000 | Loss: 0.00001509
Iteration 81/1000 | Loss: 0.00001509
Iteration 82/1000 | Loss: 0.00001508
Iteration 83/1000 | Loss: 0.00001508
Iteration 84/1000 | Loss: 0.00001508
Iteration 85/1000 | Loss: 0.00001508
Iteration 86/1000 | Loss: 0.00001508
Iteration 87/1000 | Loss: 0.00001507
Iteration 88/1000 | Loss: 0.00001507
Iteration 89/1000 | Loss: 0.00001506
Iteration 90/1000 | Loss: 0.00001506
Iteration 91/1000 | Loss: 0.00001506
Iteration 92/1000 | Loss: 0.00001506
Iteration 93/1000 | Loss: 0.00001506
Iteration 94/1000 | Loss: 0.00001505
Iteration 95/1000 | Loss: 0.00001505
Iteration 96/1000 | Loss: 0.00001504
Iteration 97/1000 | Loss: 0.00001504
Iteration 98/1000 | Loss: 0.00001504
Iteration 99/1000 | Loss: 0.00001503
Iteration 100/1000 | Loss: 0.00001503
Iteration 101/1000 | Loss: 0.00001503
Iteration 102/1000 | Loss: 0.00001503
Iteration 103/1000 | Loss: 0.00001503
Iteration 104/1000 | Loss: 0.00001503
Iteration 105/1000 | Loss: 0.00001503
Iteration 106/1000 | Loss: 0.00001503
Iteration 107/1000 | Loss: 0.00001503
Iteration 108/1000 | Loss: 0.00001503
Iteration 109/1000 | Loss: 0.00001502
Iteration 110/1000 | Loss: 0.00001502
Iteration 111/1000 | Loss: 0.00001502
Iteration 112/1000 | Loss: 0.00001502
Iteration 113/1000 | Loss: 0.00001502
Iteration 114/1000 | Loss: 0.00001502
Iteration 115/1000 | Loss: 0.00001502
Iteration 116/1000 | Loss: 0.00001502
Iteration 117/1000 | Loss: 0.00001502
Iteration 118/1000 | Loss: 0.00001502
Iteration 119/1000 | Loss: 0.00001502
Iteration 120/1000 | Loss: 0.00001501
Iteration 121/1000 | Loss: 0.00001500
Iteration 122/1000 | Loss: 0.00001500
Iteration 123/1000 | Loss: 0.00001500
Iteration 124/1000 | Loss: 0.00001500
Iteration 125/1000 | Loss: 0.00001500
Iteration 126/1000 | Loss: 0.00001500
Iteration 127/1000 | Loss: 0.00001500
Iteration 128/1000 | Loss: 0.00001500
Iteration 129/1000 | Loss: 0.00001499
Iteration 130/1000 | Loss: 0.00001499
Iteration 131/1000 | Loss: 0.00001499
Iteration 132/1000 | Loss: 0.00001499
Iteration 133/1000 | Loss: 0.00001499
Iteration 134/1000 | Loss: 0.00001498
Iteration 135/1000 | Loss: 0.00001498
Iteration 136/1000 | Loss: 0.00001498
Iteration 137/1000 | Loss: 0.00001498
Iteration 138/1000 | Loss: 0.00001498
Iteration 139/1000 | Loss: 0.00001498
Iteration 140/1000 | Loss: 0.00001498
Iteration 141/1000 | Loss: 0.00001498
Iteration 142/1000 | Loss: 0.00001498
Iteration 143/1000 | Loss: 0.00001498
Iteration 144/1000 | Loss: 0.00001498
Iteration 145/1000 | Loss: 0.00001498
Iteration 146/1000 | Loss: 0.00001498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.4978202671045437e-05, 1.4978202671045437e-05, 1.4978202671045437e-05, 1.4978202671045437e-05, 1.4978202671045437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4978202671045437e-05

Optimization complete. Final v2v error: 3.2904977798461914 mm

Highest mean error: 3.768521308898926 mm for frame 61

Lowest mean error: 2.9335579872131348 mm for frame 34

Saving results

Total time: 38.384703159332275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462429
Iteration 2/25 | Loss: 0.00162826
Iteration 3/25 | Loss: 0.00141196
Iteration 4/25 | Loss: 0.00134670
Iteration 5/25 | Loss: 0.00132770
Iteration 6/25 | Loss: 0.00131716
Iteration 7/25 | Loss: 0.00130246
Iteration 8/25 | Loss: 0.00129622
Iteration 9/25 | Loss: 0.00129438
Iteration 10/25 | Loss: 0.00129354
Iteration 11/25 | Loss: 0.00129527
Iteration 12/25 | Loss: 0.00129204
Iteration 13/25 | Loss: 0.00129085
Iteration 14/25 | Loss: 0.00129025
Iteration 15/25 | Loss: 0.00129200
Iteration 16/25 | Loss: 0.00129288
Iteration 17/25 | Loss: 0.00129254
Iteration 18/25 | Loss: 0.00129259
Iteration 19/25 | Loss: 0.00129226
Iteration 20/25 | Loss: 0.00129270
Iteration 21/25 | Loss: 0.00129245
Iteration 22/25 | Loss: 0.00128997
Iteration 23/25 | Loss: 0.00128854
Iteration 24/25 | Loss: 0.00128805
Iteration 25/25 | Loss: 0.00128789

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29316199
Iteration 2/25 | Loss: 0.00176962
Iteration 3/25 | Loss: 0.00176959
Iteration 4/25 | Loss: 0.00176959
Iteration 5/25 | Loss: 0.00176959
Iteration 6/25 | Loss: 0.00176959
Iteration 7/25 | Loss: 0.00176959
Iteration 8/25 | Loss: 0.00176959
Iteration 9/25 | Loss: 0.00176959
Iteration 10/25 | Loss: 0.00176959
Iteration 11/25 | Loss: 0.00176959
Iteration 12/25 | Loss: 0.00176959
Iteration 13/25 | Loss: 0.00176959
Iteration 14/25 | Loss: 0.00176959
Iteration 15/25 | Loss: 0.00176959
Iteration 16/25 | Loss: 0.00176959
Iteration 17/25 | Loss: 0.00176959
Iteration 18/25 | Loss: 0.00176959
Iteration 19/25 | Loss: 0.00176959
Iteration 20/25 | Loss: 0.00176959
Iteration 21/25 | Loss: 0.00176959
Iteration 22/25 | Loss: 0.00176959
Iteration 23/25 | Loss: 0.00176959
Iteration 24/25 | Loss: 0.00176959
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0017695894930511713, 0.0017695894930511713, 0.0017695894930511713, 0.0017695894930511713, 0.0017695894930511713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017695894930511713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176959
Iteration 2/1000 | Loss: 0.00010673
Iteration 3/1000 | Loss: 0.00006073
Iteration 4/1000 | Loss: 0.00004328
Iteration 5/1000 | Loss: 0.00003665
Iteration 6/1000 | Loss: 0.00003620
Iteration 7/1000 | Loss: 0.00003812
Iteration 8/1000 | Loss: 0.00004463
Iteration 9/1000 | Loss: 0.00004319
Iteration 10/1000 | Loss: 0.00004006
Iteration 11/1000 | Loss: 0.00003266
Iteration 12/1000 | Loss: 0.00003353
Iteration 13/1000 | Loss: 0.00002938
Iteration 14/1000 | Loss: 0.00003031
Iteration 15/1000 | Loss: 0.00003199
Iteration 16/1000 | Loss: 0.00002870
Iteration 17/1000 | Loss: 0.00002849
Iteration 18/1000 | Loss: 0.00003816
Iteration 19/1000 | Loss: 0.00003330
Iteration 20/1000 | Loss: 0.00003311
Iteration 21/1000 | Loss: 0.00004045
Iteration 22/1000 | Loss: 0.00002670
Iteration 23/1000 | Loss: 0.00003280
Iteration 24/1000 | Loss: 0.00003071
Iteration 25/1000 | Loss: 0.00002958
Iteration 26/1000 | Loss: 0.00003227
Iteration 27/1000 | Loss: 0.00003333
Iteration 28/1000 | Loss: 0.00004139
Iteration 29/1000 | Loss: 0.00004920
Iteration 30/1000 | Loss: 0.00003673
Iteration 31/1000 | Loss: 0.00003017
Iteration 32/1000 | Loss: 0.00002580
Iteration 33/1000 | Loss: 0.00002698
Iteration 34/1000 | Loss: 0.00004071
Iteration 35/1000 | Loss: 0.00003235
Iteration 36/1000 | Loss: 0.00002660
Iteration 37/1000 | Loss: 0.00002464
Iteration 38/1000 | Loss: 0.00002649
Iteration 39/1000 | Loss: 0.00003848
Iteration 40/1000 | Loss: 0.00003169
Iteration 41/1000 | Loss: 0.00002550
Iteration 42/1000 | Loss: 0.00002419
Iteration 43/1000 | Loss: 0.00002552
Iteration 44/1000 | Loss: 0.00002611
Iteration 45/1000 | Loss: 0.00002374
Iteration 46/1000 | Loss: 0.00002397
Iteration 47/1000 | Loss: 0.00002517
Iteration 48/1000 | Loss: 0.00002388
Iteration 49/1000 | Loss: 0.00002511
Iteration 50/1000 | Loss: 0.00002394
Iteration 51/1000 | Loss: 0.00002374
Iteration 52/1000 | Loss: 0.00003579
Iteration 53/1000 | Loss: 0.00003169
Iteration 54/1000 | Loss: 0.00003233
Iteration 55/1000 | Loss: 0.00003152
Iteration 56/1000 | Loss: 0.00004073
Iteration 57/1000 | Loss: 0.00003511
Iteration 58/1000 | Loss: 0.00002606
Iteration 59/1000 | Loss: 0.00002550
Iteration 60/1000 | Loss: 0.00002384
Iteration 61/1000 | Loss: 0.00002371
Iteration 62/1000 | Loss: 0.00002418
Iteration 63/1000 | Loss: 0.00003089
Iteration 64/1000 | Loss: 0.00002867
Iteration 65/1000 | Loss: 0.00002610
Iteration 66/1000 | Loss: 0.00002451
Iteration 67/1000 | Loss: 0.00002321
Iteration 68/1000 | Loss: 0.00002502
Iteration 69/1000 | Loss: 0.00002529
Iteration 70/1000 | Loss: 0.00002722
Iteration 71/1000 | Loss: 0.00002654
Iteration 72/1000 | Loss: 0.00002782
Iteration 73/1000 | Loss: 0.00002900
Iteration 74/1000 | Loss: 0.00003873
Iteration 75/1000 | Loss: 0.00003368
Iteration 76/1000 | Loss: 0.00002936
Iteration 77/1000 | Loss: 0.00003309
Iteration 78/1000 | Loss: 0.00003470
Iteration 79/1000 | Loss: 0.00003509
Iteration 80/1000 | Loss: 0.00003642
Iteration 81/1000 | Loss: 0.00003756
Iteration 82/1000 | Loss: 0.00003600
Iteration 83/1000 | Loss: 0.00003487
Iteration 84/1000 | Loss: 0.00004450
Iteration 85/1000 | Loss: 0.00002968
Iteration 86/1000 | Loss: 0.00002540
Iteration 87/1000 | Loss: 0.00004019
Iteration 88/1000 | Loss: 0.00004414
Iteration 89/1000 | Loss: 0.00003926
Iteration 90/1000 | Loss: 0.00004678
Iteration 91/1000 | Loss: 0.00003813
Iteration 92/1000 | Loss: 0.00003772
Iteration 93/1000 | Loss: 0.00004879
Iteration 94/1000 | Loss: 0.00004905
Iteration 95/1000 | Loss: 0.00004129
Iteration 96/1000 | Loss: 0.00004120
Iteration 97/1000 | Loss: 0.00003309
Iteration 98/1000 | Loss: 0.00002916
Iteration 99/1000 | Loss: 0.00002846
Iteration 100/1000 | Loss: 0.00002708
Iteration 101/1000 | Loss: 0.00004016
Iteration 102/1000 | Loss: 0.00004060
Iteration 103/1000 | Loss: 0.00004634
Iteration 104/1000 | Loss: 0.00004175
Iteration 105/1000 | Loss: 0.00005148
Iteration 106/1000 | Loss: 0.00004791
Iteration 107/1000 | Loss: 0.00004666
Iteration 108/1000 | Loss: 0.00005216
Iteration 109/1000 | Loss: 0.00005145
Iteration 110/1000 | Loss: 0.00005262
Iteration 111/1000 | Loss: 0.00003344
Iteration 112/1000 | Loss: 0.00002783
Iteration 113/1000 | Loss: 0.00003394
Iteration 114/1000 | Loss: 0.00003145
Iteration 115/1000 | Loss: 0.00003765
Iteration 116/1000 | Loss: 0.00003347
Iteration 117/1000 | Loss: 0.00003412
Iteration 118/1000 | Loss: 0.00002736
Iteration 119/1000 | Loss: 0.00003234
Iteration 120/1000 | Loss: 0.00002897
Iteration 121/1000 | Loss: 0.00003294
Iteration 122/1000 | Loss: 0.00002871
Iteration 123/1000 | Loss: 0.00003625
Iteration 124/1000 | Loss: 0.00003247
Iteration 125/1000 | Loss: 0.00003474
Iteration 126/1000 | Loss: 0.00003212
Iteration 127/1000 | Loss: 0.00002636
Iteration 128/1000 | Loss: 0.00002253
Iteration 129/1000 | Loss: 0.00003104
Iteration 130/1000 | Loss: 0.00003319
Iteration 131/1000 | Loss: 0.00003224
Iteration 132/1000 | Loss: 0.00003487
Iteration 133/1000 | Loss: 0.00003724
Iteration 134/1000 | Loss: 0.00003844
Iteration 135/1000 | Loss: 0.00004405
Iteration 136/1000 | Loss: 0.00003864
Iteration 137/1000 | Loss: 0.00003014
Iteration 138/1000 | Loss: 0.00002366
Iteration 139/1000 | Loss: 0.00003524
Iteration 140/1000 | Loss: 0.00003695
Iteration 141/1000 | Loss: 0.00004243
Iteration 142/1000 | Loss: 0.00004310
Iteration 143/1000 | Loss: 0.00004791
Iteration 144/1000 | Loss: 0.00004742
Iteration 145/1000 | Loss: 0.00006042
Iteration 146/1000 | Loss: 0.00004527
Iteration 147/1000 | Loss: 0.00002877
Iteration 148/1000 | Loss: 0.00002326
Iteration 149/1000 | Loss: 0.00002235
Iteration 150/1000 | Loss: 0.00002350
Iteration 151/1000 | Loss: 0.00002365
Iteration 152/1000 | Loss: 0.00002396
Iteration 153/1000 | Loss: 0.00002561
Iteration 154/1000 | Loss: 0.00003121
Iteration 155/1000 | Loss: 0.00003703
Iteration 156/1000 | Loss: 0.00003825
Iteration 157/1000 | Loss: 0.00003892
Iteration 158/1000 | Loss: 0.00003823
Iteration 159/1000 | Loss: 0.00003772
Iteration 160/1000 | Loss: 0.00004370
Iteration 161/1000 | Loss: 0.00004391
Iteration 162/1000 | Loss: 0.00004609
Iteration 163/1000 | Loss: 0.00005497
Iteration 164/1000 | Loss: 0.00005066
Iteration 165/1000 | Loss: 0.00005371
Iteration 166/1000 | Loss: 0.00005195
Iteration 167/1000 | Loss: 0.00006557
Iteration 168/1000 | Loss: 0.00003925
Iteration 169/1000 | Loss: 0.00002779
Iteration 170/1000 | Loss: 0.00002273
Iteration 171/1000 | Loss: 0.00003194
Iteration 172/1000 | Loss: 0.00002562
Iteration 173/1000 | Loss: 0.00002346
Iteration 174/1000 | Loss: 0.00002330
Iteration 175/1000 | Loss: 0.00002729
Iteration 176/1000 | Loss: 0.00003315
Iteration 177/1000 | Loss: 0.00002937
Iteration 178/1000 | Loss: 0.00002504
Iteration 179/1000 | Loss: 0.00002490
Iteration 180/1000 | Loss: 0.00003177
Iteration 181/1000 | Loss: 0.00002542
Iteration 182/1000 | Loss: 0.00002644
Iteration 183/1000 | Loss: 0.00002450
Iteration 184/1000 | Loss: 0.00002770
Iteration 185/1000 | Loss: 0.00002781
Iteration 186/1000 | Loss: 0.00003295
Iteration 187/1000 | Loss: 0.00003089
Iteration 188/1000 | Loss: 0.00003597
Iteration 189/1000 | Loss: 0.00003176
Iteration 190/1000 | Loss: 0.00003864
Iteration 191/1000 | Loss: 0.00003166
Iteration 192/1000 | Loss: 0.00003377
Iteration 193/1000 | Loss: 0.00002985
Iteration 194/1000 | Loss: 0.00002442
Iteration 195/1000 | Loss: 0.00002391
Iteration 196/1000 | Loss: 0.00002572
Iteration 197/1000 | Loss: 0.00003193
Iteration 198/1000 | Loss: 0.00002715
Iteration 199/1000 | Loss: 0.00002611
Iteration 200/1000 | Loss: 0.00002630
Iteration 201/1000 | Loss: 0.00002825
Iteration 202/1000 | Loss: 0.00003372
Iteration 203/1000 | Loss: 0.00003337
Iteration 204/1000 | Loss: 0.00003876
Iteration 205/1000 | Loss: 0.00003232
Iteration 206/1000 | Loss: 0.00004071
Iteration 207/1000 | Loss: 0.00003700
Iteration 208/1000 | Loss: 0.00003940
Iteration 209/1000 | Loss: 0.00003461
Iteration 210/1000 | Loss: 0.00004995
Iteration 211/1000 | Loss: 0.00004206
Iteration 212/1000 | Loss: 0.00005016
Iteration 213/1000 | Loss: 0.00003190
Iteration 214/1000 | Loss: 0.00002411
Iteration 215/1000 | Loss: 0.00002682
Iteration 216/1000 | Loss: 0.00003737
Iteration 217/1000 | Loss: 0.00005154
Iteration 218/1000 | Loss: 0.00004450
Iteration 219/1000 | Loss: 0.00004646
Iteration 220/1000 | Loss: 0.00003771
Iteration 221/1000 | Loss: 0.00002954
Iteration 222/1000 | Loss: 0.00003998
Iteration 223/1000 | Loss: 0.00003298
Iteration 224/1000 | Loss: 0.00003949
Iteration 225/1000 | Loss: 0.00003467
Iteration 226/1000 | Loss: 0.00004577
Iteration 227/1000 | Loss: 0.00004715
Iteration 228/1000 | Loss: 0.00003776
Iteration 229/1000 | Loss: 0.00004072
Iteration 230/1000 | Loss: 0.00004840
Iteration 231/1000 | Loss: 0.00004772
Iteration 232/1000 | Loss: 0.00005648
Iteration 233/1000 | Loss: 0.00005265
Iteration 234/1000 | Loss: 0.00005711
Iteration 235/1000 | Loss: 0.00005365
Iteration 236/1000 | Loss: 0.00005836
Iteration 237/1000 | Loss: 0.00006414
Iteration 238/1000 | Loss: 0.00005810
Iteration 239/1000 | Loss: 0.00005675
Iteration 240/1000 | Loss: 0.00006491
Iteration 241/1000 | Loss: 0.00005766
Iteration 242/1000 | Loss: 0.00006406
Iteration 243/1000 | Loss: 0.00006307
Iteration 244/1000 | Loss: 0.00007884
Iteration 245/1000 | Loss: 0.00006823
Iteration 246/1000 | Loss: 0.00007191
Iteration 247/1000 | Loss: 0.00006746
Iteration 248/1000 | Loss: 0.00007544
Iteration 249/1000 | Loss: 0.00007305
Iteration 250/1000 | Loss: 0.00003741
Iteration 251/1000 | Loss: 0.00003434
Iteration 252/1000 | Loss: 0.00004783
Iteration 253/1000 | Loss: 0.00004947
Iteration 254/1000 | Loss: 0.00006134
Iteration 255/1000 | Loss: 0.00021329
Iteration 256/1000 | Loss: 0.00012984
Iteration 257/1000 | Loss: 0.00004830
Iteration 258/1000 | Loss: 0.00003691
Iteration 259/1000 | Loss: 0.00003054
Iteration 260/1000 | Loss: 0.00002930
Iteration 261/1000 | Loss: 0.00002825
Iteration 262/1000 | Loss: 0.00002821
Iteration 263/1000 | Loss: 0.00003096
Iteration 264/1000 | Loss: 0.00002974
Iteration 265/1000 | Loss: 0.00002937
Iteration 266/1000 | Loss: 0.00003382
Iteration 267/1000 | Loss: 0.00002589
Iteration 268/1000 | Loss: 0.00003095
Iteration 269/1000 | Loss: 0.00003069
Iteration 270/1000 | Loss: 0.00002577
Iteration 271/1000 | Loss: 0.00003212
Iteration 272/1000 | Loss: 0.00002892
Iteration 273/1000 | Loss: 0.00002850
Iteration 274/1000 | Loss: 0.00002607
Iteration 275/1000 | Loss: 0.00002614
Iteration 276/1000 | Loss: 0.00003372
Iteration 277/1000 | Loss: 0.00002977
Iteration 278/1000 | Loss: 0.00003275
Iteration 279/1000 | Loss: 0.00003166
Iteration 280/1000 | Loss: 0.00002541
Iteration 281/1000 | Loss: 0.00002456
Iteration 282/1000 | Loss: 0.00002621
Iteration 283/1000 | Loss: 0.00002553
Iteration 284/1000 | Loss: 0.00002408
Iteration 285/1000 | Loss: 0.00002987
Iteration 286/1000 | Loss: 0.00002861
Iteration 287/1000 | Loss: 0.00002472
Iteration 288/1000 | Loss: 0.00002600
Iteration 289/1000 | Loss: 0.00002606
Iteration 290/1000 | Loss: 0.00003061
Iteration 291/1000 | Loss: 0.00002907
Iteration 292/1000 | Loss: 0.00003344
Iteration 293/1000 | Loss: 0.00003378
Iteration 294/1000 | Loss: 0.00003961
Iteration 295/1000 | Loss: 0.00003723
Iteration 296/1000 | Loss: 0.00004196
Iteration 297/1000 | Loss: 0.00003868
Iteration 298/1000 | Loss: 0.00004788
Iteration 299/1000 | Loss: 0.00004596
Iteration 300/1000 | Loss: 0.00005666
Iteration 301/1000 | Loss: 0.00004896
Iteration 302/1000 | Loss: 0.00004912
Iteration 303/1000 | Loss: 0.00004623
Iteration 304/1000 | Loss: 0.00006061
Iteration 305/1000 | Loss: 0.00004086
Iteration 306/1000 | Loss: 0.00004052
Iteration 307/1000 | Loss: 0.00003562
Iteration 308/1000 | Loss: 0.00002837
Iteration 309/1000 | Loss: 0.00002505
Iteration 310/1000 | Loss: 0.00003507
Iteration 311/1000 | Loss: 0.00002665
Iteration 312/1000 | Loss: 0.00003538
Iteration 313/1000 | Loss: 0.00003037
Iteration 314/1000 | Loss: 0.00004738
Iteration 315/1000 | Loss: 0.00003418
Iteration 316/1000 | Loss: 0.00004929
Iteration 317/1000 | Loss: 0.00003517
Iteration 318/1000 | Loss: 0.00005401
Iteration 319/1000 | Loss: 0.00003946
Iteration 320/1000 | Loss: 0.00007078
Iteration 321/1000 | Loss: 0.00004621
Iteration 322/1000 | Loss: 0.00007059
Iteration 323/1000 | Loss: 0.00004672
Iteration 324/1000 | Loss: 0.00006722
Iteration 325/1000 | Loss: 0.00004761
Iteration 326/1000 | Loss: 0.00006399
Iteration 327/1000 | Loss: 0.00003382
Iteration 328/1000 | Loss: 0.00002952
Iteration 329/1000 | Loss: 0.00003389
Iteration 330/1000 | Loss: 0.00004370
Iteration 331/1000 | Loss: 0.00004125
Iteration 332/1000 | Loss: 0.00004181
Iteration 333/1000 | Loss: 0.00004389
Iteration 334/1000 | Loss: 0.00006318
Iteration 335/1000 | Loss: 0.00004365
Iteration 336/1000 | Loss: 0.00004387
Iteration 337/1000 | Loss: 0.00004027
Iteration 338/1000 | Loss: 0.00007363
Iteration 339/1000 | Loss: 0.00005072
Iteration 340/1000 | Loss: 0.00007378
Iteration 341/1000 | Loss: 0.00004578
Iteration 342/1000 | Loss: 0.00006306
Iteration 343/1000 | Loss: 0.00005620
Iteration 344/1000 | Loss: 0.00007048
Iteration 345/1000 | Loss: 0.00005715
Iteration 346/1000 | Loss: 0.00006361
Iteration 347/1000 | Loss: 0.00004418
Iteration 348/1000 | Loss: 0.00005271
Iteration 349/1000 | Loss: 0.00004885
Iteration 350/1000 | Loss: 0.00005465
Iteration 351/1000 | Loss: 0.00004549
Iteration 352/1000 | Loss: 0.00005935
Iteration 353/1000 | Loss: 0.00004854
Iteration 354/1000 | Loss: 0.00005368
Iteration 355/1000 | Loss: 0.00003860
Iteration 356/1000 | Loss: 0.00004210
Iteration 357/1000 | Loss: 0.00002902
Iteration 358/1000 | Loss: 0.00002592
Iteration 359/1000 | Loss: 0.00002430
Iteration 360/1000 | Loss: 0.00002730
Iteration 361/1000 | Loss: 0.00003136
Iteration 362/1000 | Loss: 0.00004646
Iteration 363/1000 | Loss: 0.00003648
Iteration 364/1000 | Loss: 0.00003983
Iteration 365/1000 | Loss: 0.00003565
Iteration 366/1000 | Loss: 0.00005247
Iteration 367/1000 | Loss: 0.00004226
Iteration 368/1000 | Loss: 0.00003865
Iteration 369/1000 | Loss: 0.00002965
Iteration 370/1000 | Loss: 0.00004525
Iteration 371/1000 | Loss: 0.00003189
Iteration 372/1000 | Loss: 0.00005085
Iteration 373/1000 | Loss: 0.00003038
Iteration 374/1000 | Loss: 0.00002882
Iteration 375/1000 | Loss: 0.00002691
Iteration 376/1000 | Loss: 0.00002397
Iteration 377/1000 | Loss: 0.00002499
Iteration 378/1000 | Loss: 0.00002533
Iteration 379/1000 | Loss: 0.00002427
Iteration 380/1000 | Loss: 0.00002901
Iteration 381/1000 | Loss: 0.00003769
Iteration 382/1000 | Loss: 0.00002689
Iteration 383/1000 | Loss: 0.00002559
Iteration 384/1000 | Loss: 0.00002737
Iteration 385/1000 | Loss: 0.00003981
Iteration 386/1000 | Loss: 0.00002852
Iteration 387/1000 | Loss: 0.00002429
Iteration 388/1000 | Loss: 0.00002744
Iteration 389/1000 | Loss: 0.00002649
Iteration 390/1000 | Loss: 0.00002384
Iteration 391/1000 | Loss: 0.00002396
Iteration 392/1000 | Loss: 0.00002674
Iteration 393/1000 | Loss: 0.00002733
Iteration 394/1000 | Loss: 0.00002789
Iteration 395/1000 | Loss: 0.00003000
Iteration 396/1000 | Loss: 0.00003286
Iteration 397/1000 | Loss: 0.00003362
Iteration 398/1000 | Loss: 0.00003654
Iteration 399/1000 | Loss: 0.00003107
Iteration 400/1000 | Loss: 0.00003599
Iteration 401/1000 | Loss: 0.00003455
Iteration 402/1000 | Loss: 0.00003736
Iteration 403/1000 | Loss: 0.00003535
Iteration 404/1000 | Loss: 0.00004358
Iteration 405/1000 | Loss: 0.00004059
Iteration 406/1000 | Loss: 0.00004601
Iteration 407/1000 | Loss: 0.00002880
Iteration 408/1000 | Loss: 0.00002759
Iteration 409/1000 | Loss: 0.00002855
Iteration 410/1000 | Loss: 0.00003215
Iteration 411/1000 | Loss: 0.00004282
Iteration 412/1000 | Loss: 0.00003388
Iteration 413/1000 | Loss: 0.00005122
Iteration 414/1000 | Loss: 0.00005009
Iteration 415/1000 | Loss: 0.00003363
Iteration 416/1000 | Loss: 0.00002289
Iteration 417/1000 | Loss: 0.00002916
Iteration 418/1000 | Loss: 0.00004375
Iteration 419/1000 | Loss: 0.00004060
Iteration 420/1000 | Loss: 0.00005222
Iteration 421/1000 | Loss: 0.00004915
Iteration 422/1000 | Loss: 0.00005676
Iteration 423/1000 | Loss: 0.00004673
Iteration 424/1000 | Loss: 0.00004961
Iteration 425/1000 | Loss: 0.00004888
Iteration 426/1000 | Loss: 0.00006217
Iteration 427/1000 | Loss: 0.00005430
Iteration 428/1000 | Loss: 0.00006541
Iteration 429/1000 | Loss: 0.00004720
Iteration 430/1000 | Loss: 0.00005090
Iteration 431/1000 | Loss: 0.00004040
Iteration 432/1000 | Loss: 0.00005407
Iteration 433/1000 | Loss: 0.00004791
Iteration 434/1000 | Loss: 0.00002921
Iteration 435/1000 | Loss: 0.00002371
Iteration 436/1000 | Loss: 0.00002316
Iteration 437/1000 | Loss: 0.00002911
Iteration 438/1000 | Loss: 0.00002618
Iteration 439/1000 | Loss: 0.00002608
Iteration 440/1000 | Loss: 0.00002647
Iteration 441/1000 | Loss: 0.00002561
Iteration 442/1000 | Loss: 0.00002818
Iteration 443/1000 | Loss: 0.00002824
Iteration 444/1000 | Loss: 0.00002781
Iteration 445/1000 | Loss: 0.00002765
Iteration 446/1000 | Loss: 0.00002727
Iteration 447/1000 | Loss: 0.00002870
Iteration 448/1000 | Loss: 0.00003387
Iteration 449/1000 | Loss: 0.00003755
Iteration 450/1000 | Loss: 0.00003919
Iteration 451/1000 | Loss: 0.00004277
Iteration 452/1000 | Loss: 0.00004341
Iteration 453/1000 | Loss: 0.00004702
Iteration 454/1000 | Loss: 0.00004464
Iteration 455/1000 | Loss: 0.00004835
Iteration 456/1000 | Loss: 0.00004709
Iteration 457/1000 | Loss: 0.00005199
Iteration 458/1000 | Loss: 0.00004401
Iteration 459/1000 | Loss: 0.00004848
Iteration 460/1000 | Loss: 0.00005385
Iteration 461/1000 | Loss: 0.00006487
Iteration 462/1000 | Loss: 0.00002683
Iteration 463/1000 | Loss: 0.00002331
Iteration 464/1000 | Loss: 0.00002550
Iteration 465/1000 | Loss: 0.00002464
Iteration 466/1000 | Loss: 0.00002400
Iteration 467/1000 | Loss: 0.00002424
Iteration 468/1000 | Loss: 0.00003627
Iteration 469/1000 | Loss: 0.00002611
Iteration 470/1000 | Loss: 0.00002626
Iteration 471/1000 | Loss: 0.00002785
Iteration 472/1000 | Loss: 0.00002490
Iteration 473/1000 | Loss: 0.00002654
Iteration 474/1000 | Loss: 0.00002418
Iteration 475/1000 | Loss: 0.00002250
Iteration 476/1000 | Loss: 0.00002991
Iteration 477/1000 | Loss: 0.00002663
Iteration 478/1000 | Loss: 0.00003655
Iteration 479/1000 | Loss: 0.00003962
Iteration 480/1000 | Loss: 0.00003296
Iteration 481/1000 | Loss: 0.00002709
Iteration 482/1000 | Loss: 0.00005137
Iteration 483/1000 | Loss: 0.00004492
Iteration 484/1000 | Loss: 0.00003778
Iteration 485/1000 | Loss: 0.00002414
Iteration 486/1000 | Loss: 0.00003306
Iteration 487/1000 | Loss: 0.00003017
Iteration 488/1000 | Loss: 0.00002993
Iteration 489/1000 | Loss: 0.00002599
Iteration 490/1000 | Loss: 0.00003837
Iteration 491/1000 | Loss: 0.00003053
Iteration 492/1000 | Loss: 0.00003753
Iteration 493/1000 | Loss: 0.00002907
Iteration 494/1000 | Loss: 0.00004215
Iteration 495/1000 | Loss: 0.00002905
Iteration 496/1000 | Loss: 0.00004245
Iteration 497/1000 | Loss: 0.00003106
Iteration 498/1000 | Loss: 0.00002992
Iteration 499/1000 | Loss: 0.00003610
Iteration 500/1000 | Loss: 0.00004372
Iteration 501/1000 | Loss: 0.00002912
Iteration 502/1000 | Loss: 0.00004418
Iteration 503/1000 | Loss: 0.00003456
Iteration 504/1000 | Loss: 0.00004764
Iteration 505/1000 | Loss: 0.00004380
Iteration 506/1000 | Loss: 0.00005088
Iteration 507/1000 | Loss: 0.00004424
Iteration 508/1000 | Loss: 0.00005899
Iteration 509/1000 | Loss: 0.00003649
Iteration 510/1000 | Loss: 0.00004771
Iteration 511/1000 | Loss: 0.00004313
Iteration 512/1000 | Loss: 0.00005594
Iteration 513/1000 | Loss: 0.00004886
Iteration 514/1000 | Loss: 0.00005211
Iteration 515/1000 | Loss: 0.00004460
Iteration 516/1000 | Loss: 0.00004346
Iteration 517/1000 | Loss: 0.00003766
Iteration 518/1000 | Loss: 0.00004145
Iteration 519/1000 | Loss: 0.00003302
Iteration 520/1000 | Loss: 0.00004549
Iteration 521/1000 | Loss: 0.00004176
Iteration 522/1000 | Loss: 0.00005299
Iteration 523/1000 | Loss: 0.00004808
Iteration 524/1000 | Loss: 0.00003715
Iteration 525/1000 | Loss: 0.00002980
Iteration 526/1000 | Loss: 0.00003471
Iteration 527/1000 | Loss: 0.00003074
Iteration 528/1000 | Loss: 0.00003074
Iteration 529/1000 | Loss: 0.00002503
Iteration 530/1000 | Loss: 0.00003309
Iteration 531/1000 | Loss: 0.00002973
Iteration 532/1000 | Loss: 0.00004642
Iteration 533/1000 | Loss: 0.00003705
Iteration 534/1000 | Loss: 0.00004869
Iteration 535/1000 | Loss: 0.00003787
Iteration 536/1000 | Loss: 0.00005261
Iteration 537/1000 | Loss: 0.00004093
Iteration 538/1000 | Loss: 0.00005685
Iteration 539/1000 | Loss: 0.00004396
Iteration 540/1000 | Loss: 0.00005023
Iteration 541/1000 | Loss: 0.00004708
Iteration 542/1000 | Loss: 0.00005542
Iteration 543/1000 | Loss: 0.00005040
Iteration 544/1000 | Loss: 0.00003518
Iteration 545/1000 | Loss: 0.00002720
Iteration 546/1000 | Loss: 0.00004705
Iteration 547/1000 | Loss: 0.00004248
Iteration 548/1000 | Loss: 0.00006482
Iteration 549/1000 | Loss: 0.00005534
Iteration 550/1000 | Loss: 0.00007302
Iteration 551/1000 | Loss: 0.00006051
Iteration 552/1000 | Loss: 0.00007118
Iteration 553/1000 | Loss: 0.00006139
Iteration 554/1000 | Loss: 0.00004791
Iteration 555/1000 | Loss: 0.00004039
Iteration 556/1000 | Loss: 0.00004621
Iteration 557/1000 | Loss: 0.00005767
Iteration 558/1000 | Loss: 0.00005941
Iteration 559/1000 | Loss: 0.00006936
Iteration 560/1000 | Loss: 0.00002759
Iteration 561/1000 | Loss: 0.00002318
Iteration 562/1000 | Loss: 0.00003161
Iteration 563/1000 | Loss: 0.00002826
Iteration 564/1000 | Loss: 0.00003355
Iteration 565/1000 | Loss: 0.00002598
Iteration 566/1000 | Loss: 0.00005401
Iteration 567/1000 | Loss: 0.00004606
Iteration 568/1000 | Loss: 0.00005961
Iteration 569/1000 | Loss: 0.00005919
Iteration 570/1000 | Loss: 0.00006384
Iteration 571/1000 | Loss: 0.00006001
Iteration 572/1000 | Loss: 0.00005473
Iteration 573/1000 | Loss: 0.00005126
Iteration 574/1000 | Loss: 0.00006277
Iteration 575/1000 | Loss: 0.00005944
Iteration 576/1000 | Loss: 0.00005740
Iteration 577/1000 | Loss: 0.00005838
Iteration 578/1000 | Loss: 0.00002815
Iteration 579/1000 | Loss: 0.00002515
Iteration 580/1000 | Loss: 0.00004334
Iteration 581/1000 | Loss: 0.00003408
Iteration 582/1000 | Loss: 0.00005232
Iteration 583/1000 | Loss: 0.00004238
Iteration 584/1000 | Loss: 0.00005666
Iteration 585/1000 | Loss: 0.00004420
Iteration 586/1000 | Loss: 0.00007131
Iteration 587/1000 | Loss: 0.00004519
Iteration 588/1000 | Loss: 0.00005036
Iteration 589/1000 | Loss: 0.00004452
Iteration 590/1000 | Loss: 0.00002894
Iteration 591/1000 | Loss: 0.00002426
Iteration 592/1000 | Loss: 0.00003849
Iteration 593/1000 | Loss: 0.00002997
Iteration 594/1000 | Loss: 0.00005604
Iteration 595/1000 | Loss: 0.00003507
Iteration 596/1000 | Loss: 0.00002521
Iteration 597/1000 | Loss: 0.00002316
Iteration 598/1000 | Loss: 0.00002669
Iteration 599/1000 | Loss: 0.00002324
Iteration 600/1000 | Loss: 0.00002865
Iteration 601/1000 | Loss: 0.00002605
Iteration 602/1000 | Loss: 0.00002356
Iteration 603/1000 | Loss: 0.00002523
Iteration 604/1000 | Loss: 0.00003014
Iteration 605/1000 | Loss: 0.00002668
Iteration 606/1000 | Loss: 0.00003246
Iteration 607/1000 | Loss: 0.00003234
Iteration 608/1000 | Loss: 0.00002503
Iteration 609/1000 | Loss: 0.00003073
Iteration 610/1000 | Loss: 0.00002676
Iteration 611/1000 | Loss: 0.00003197
Iteration 612/1000 | Loss: 0.00002884
Iteration 613/1000 | Loss: 0.00003708
Iteration 614/1000 | Loss: 0.00003575
Iteration 615/1000 | Loss: 0.00002465
Iteration 616/1000 | Loss: 0.00002337
Iteration 617/1000 | Loss: 0.00002433
Iteration 618/1000 | Loss: 0.00003200
Iteration 619/1000 | Loss: 0.00003607
Iteration 620/1000 | Loss: 0.00003621
Iteration 621/1000 | Loss: 0.00003969
Iteration 622/1000 | Loss: 0.00004303
Iteration 623/1000 | Loss: 0.00004423
Iteration 624/1000 | Loss: 0.00003980
Iteration 625/1000 | Loss: 0.00004303
Iteration 626/1000 | Loss: 0.00003131
Iteration 627/1000 | Loss: 0.00002729
Iteration 628/1000 | Loss: 0.00002637
Iteration 629/1000 | Loss: 0.00002817
Iteration 630/1000 | Loss: 0.00002565
Iteration 631/1000 | Loss: 0.00002875
Iteration 632/1000 | Loss: 0.00003081
Iteration 633/1000 | Loss: 0.00004136
Iteration 634/1000 | Loss: 0.00005297
Iteration 635/1000 | Loss: 0.00002938
Iteration 636/1000 | Loss: 0.00002140
Iteration 637/1000 | Loss: 0.00002294
Iteration 638/1000 | Loss: 0.00002749
Iteration 639/1000 | Loss: 0.00003680
Iteration 640/1000 | Loss: 0.00002985
Iteration 641/1000 | Loss: 0.00002596
Iteration 642/1000 | Loss: 0.00002991
Iteration 643/1000 | Loss: 0.00002781
Iteration 644/1000 | Loss: 0.00002484
Iteration 645/1000 | Loss: 0.00002503
Iteration 646/1000 | Loss: 0.00003305
Iteration 647/1000 | Loss: 0.00002950
Iteration 648/1000 | Loss: 0.00003169
Iteration 649/1000 | Loss: 0.00002679
Iteration 650/1000 | Loss: 0.00003685
Iteration 651/1000 | Loss: 0.00002938
Iteration 652/1000 | Loss: 0.00002928
Iteration 653/1000 | Loss: 0.00003100
Iteration 654/1000 | Loss: 0.00003412
Iteration 655/1000 | Loss: 0.00003072
Iteration 656/1000 | Loss: 0.00002543
Iteration 657/1000 | Loss: 0.00002333
Iteration 658/1000 | Loss: 0.00002619
Iteration 659/1000 | Loss: 0.00002950
Iteration 660/1000 | Loss: 0.00003528
Iteration 661/1000 | Loss: 0.00003194
Iteration 662/1000 | Loss: 0.00003341
Iteration 663/1000 | Loss: 0.00003192
Iteration 664/1000 | Loss: 0.00003110
Iteration 665/1000 | Loss: 0.00003237
Iteration 666/1000 | Loss: 0.00003928
Iteration 667/1000 | Loss: 0.00003959
Iteration 668/1000 | Loss: 0.00004407
Iteration 669/1000 | Loss: 0.00004499
Iteration 670/1000 | Loss: 0.00003670
Iteration 671/1000 | Loss: 0.00002737
Iteration 672/1000 | Loss: 0.00004165
Iteration 673/1000 | Loss: 0.00003030
Iteration 674/1000 | Loss: 0.00003994
Iteration 675/1000 | Loss: 0.00004236
Iteration 676/1000 | Loss: 0.00005117
Iteration 677/1000 | Loss: 0.00004267
Iteration 678/1000 | Loss: 0.00004439
Iteration 679/1000 | Loss: 0.00003486
Iteration 680/1000 | Loss: 0.00004411
Iteration 681/1000 | Loss: 0.00002993
Iteration 682/1000 | Loss: 0.00002633
Iteration 683/1000 | Loss: 0.00003162
Iteration 684/1000 | Loss: 0.00003321
Iteration 685/1000 | Loss: 0.00003465
Iteration 686/1000 | Loss: 0.00004078
Iteration 687/1000 | Loss: 0.00005404
Iteration 688/1000 | Loss: 0.00004178
Iteration 689/1000 | Loss: 0.00002742
Iteration 690/1000 | Loss: 0.00002406
Iteration 691/1000 | Loss: 0.00002342
Iteration 692/1000 | Loss: 0.00002358
Iteration 693/1000 | Loss: 0.00004205
Iteration 694/1000 | Loss: 0.00003248
Iteration 695/1000 | Loss: 0.00002771
Iteration 696/1000 | Loss: 0.00002923
Iteration 697/1000 | Loss: 0.00003944
Iteration 698/1000 | Loss: 0.00004665
Iteration 699/1000 | Loss: 0.00005445
Iteration 700/1000 | Loss: 0.00005586
Iteration 701/1000 | Loss: 0.00006713
Iteration 702/1000 | Loss: 0.00006449
Iteration 703/1000 | Loss: 0.00006915
Iteration 704/1000 | Loss: 0.00006135
Iteration 705/1000 | Loss: 0.00006078
Iteration 706/1000 | Loss: 0.00005855
Iteration 707/1000 | Loss: 0.00007212
Iteration 708/1000 | Loss: 0.00006454
Iteration 709/1000 | Loss: 0.00006115
Iteration 710/1000 | Loss: 0.00006978
Iteration 711/1000 | Loss: 0.00008029
Iteration 712/1000 | Loss: 0.00005925
Iteration 713/1000 | Loss: 0.00004338
Iteration 714/1000 | Loss: 0.00006041
Iteration 715/1000 | Loss: 0.00006278
Iteration 716/1000 | Loss: 0.00005554
Iteration 717/1000 | Loss: 0.00005765
Iteration 718/1000 | Loss: 0.00005790
Iteration 719/1000 | Loss: 0.00004712
Iteration 720/1000 | Loss: 0.00005945
Iteration 721/1000 | Loss: 0.00005377
Iteration 722/1000 | Loss: 0.00007408
Iteration 723/1000 | Loss: 0.00006364
Iteration 724/1000 | Loss: 0.00007993
Iteration 725/1000 | Loss: 0.00006756
Iteration 726/1000 | Loss: 0.00007774
Iteration 727/1000 | Loss: 0.00004794
Iteration 728/1000 | Loss: 0.00005392
Iteration 729/1000 | Loss: 0.00004495
Iteration 730/1000 | Loss: 0.00005789
Iteration 731/1000 | Loss: 0.00005113
Iteration 732/1000 | Loss: 0.00007454
Iteration 733/1000 | Loss: 0.00006888
Iteration 734/1000 | Loss: 0.00007026
Iteration 735/1000 | Loss: 0.00005060
Iteration 736/1000 | Loss: 0.00006781
Iteration 737/1000 | Loss: 0.00006631
Iteration 738/1000 | Loss: 0.00006590
Iteration 739/1000 | Loss: 0.00006313
Iteration 740/1000 | Loss: 0.00005314
Iteration 741/1000 | Loss: 0.00003035
Iteration 742/1000 | Loss: 0.00002325
Iteration 743/1000 | Loss: 0.00002660
Iteration 744/1000 | Loss: 0.00002767
Iteration 745/1000 | Loss: 0.00002969
Iteration 746/1000 | Loss: 0.00003099
Iteration 747/1000 | Loss: 0.00003860
Iteration 748/1000 | Loss: 0.00003909
Iteration 749/1000 | Loss: 0.00004145
Iteration 750/1000 | Loss: 0.00003505
Iteration 751/1000 | Loss: 0.00004074
Iteration 752/1000 | Loss: 0.00003994
Iteration 753/1000 | Loss: 0.00002999
Iteration 754/1000 | Loss: 0.00002408
Iteration 755/1000 | Loss: 0.00002837
Iteration 756/1000 | Loss: 0.00002402
Iteration 757/1000 | Loss: 0.00003034
Iteration 758/1000 | Loss: 0.00003259
Iteration 759/1000 | Loss: 0.00003002
Iteration 760/1000 | Loss: 0.00002351
Iteration 761/1000 | Loss: 0.00002692
Iteration 762/1000 | Loss: 0.00002976
Iteration 763/1000 | Loss: 0.00003217
Iteration 764/1000 | Loss: 0.00003543
Iteration 765/1000 | Loss: 0.00003656
Iteration 766/1000 | Loss: 0.00003035
Iteration 767/1000 | Loss: 0.00002941
Iteration 768/1000 | Loss: 0.00003223
Iteration 769/1000 | Loss: 0.00002982
Iteration 770/1000 | Loss: 0.00002977
Iteration 771/1000 | Loss: 0.00002468
Iteration 772/1000 | Loss: 0.00002207
Iteration 773/1000 | Loss: 0.00002488
Iteration 774/1000 | Loss: 0.00003187
Iteration 775/1000 | Loss: 0.00003027
Iteration 776/1000 | Loss: 0.00002997
Iteration 777/1000 | Loss: 0.00002984
Iteration 778/1000 | Loss: 0.00002515
Iteration 779/1000 | Loss: 0.00002495
Iteration 780/1000 | Loss: 0.00002629
Iteration 781/1000 | Loss: 0.00002507
Iteration 782/1000 | Loss: 0.00002531
Iteration 783/1000 | Loss: 0.00002956
Iteration 784/1000 | Loss: 0.00002584
Iteration 785/1000 | Loss: 0.00002685
Iteration 786/1000 | Loss: 0.00002545
Iteration 787/1000 | Loss: 0.00002749
Iteration 788/1000 | Loss: 0.00003821
Iteration 789/1000 | Loss: 0.00003657
Iteration 790/1000 | Loss: 0.00003667
Iteration 791/1000 | Loss: 0.00003198
Iteration 792/1000 | Loss: 0.00004064
Iteration 793/1000 | Loss: 0.00003218
Iteration 794/1000 | Loss: 0.00002550
Iteration 795/1000 | Loss: 0.00002434
Iteration 796/1000 | Loss: 0.00002198
Iteration 797/1000 | Loss: 0.00002864
Iteration 798/1000 | Loss: 0.00003125
Iteration 799/1000 | Loss: 0.00003536
Iteration 800/1000 | Loss: 0.00003015
Iteration 801/1000 | Loss: 0.00002525
Iteration 802/1000 | Loss: 0.00002484
Iteration 803/1000 | Loss: 0.00002527
Iteration 804/1000 | Loss: 0.00002926
Iteration 805/1000 | Loss: 0.00002608
Iteration 806/1000 | Loss: 0.00002826
Iteration 807/1000 | Loss: 0.00003212
Iteration 808/1000 | Loss: 0.00003148
Iteration 809/1000 | Loss: 0.00002696
Iteration 810/1000 | Loss: 0.00003042
Iteration 811/1000 | Loss: 0.00002843
Iteration 812/1000 | Loss: 0.00002991
Iteration 813/1000 | Loss: 0.00002866
Iteration 814/1000 | Loss: 0.00002754
Iteration 815/1000 | Loss: 0.00003093
Iteration 816/1000 | Loss: 0.00003414
Iteration 817/1000 | Loss: 0.00003293
Iteration 818/1000 | Loss: 0.00002902
Iteration 819/1000 | Loss: 0.00002867
Iteration 820/1000 | Loss: 0.00003477
Iteration 821/1000 | Loss: 0.00003069
Iteration 822/1000 | Loss: 0.00003112
Iteration 823/1000 | Loss: 0.00003075
Iteration 824/1000 | Loss: 0.00003771
Iteration 825/1000 | Loss: 0.00004461
Iteration 826/1000 | Loss: 0.00004618
Iteration 827/1000 | Loss: 0.00003444
Iteration 828/1000 | Loss: 0.00003905
Iteration 829/1000 | Loss: 0.00004050
Iteration 830/1000 | Loss: 0.00004781
Iteration 831/1000 | Loss: 0.00004948
Iteration 832/1000 | Loss: 0.00005911
Iteration 833/1000 | Loss: 0.00005522
Iteration 834/1000 | Loss: 0.00005131
Iteration 835/1000 | Loss: 0.00005076
Iteration 836/1000 | Loss: 0.00005835
Iteration 837/1000 | Loss: 0.00005038
Iteration 838/1000 | Loss: 0.00006075
Iteration 839/1000 | Loss: 0.00005531
Iteration 840/1000 | Loss: 0.00006468
Iteration 841/1000 | Loss: 0.00004091
Iteration 842/1000 | Loss: 0.00005421
Iteration 843/1000 | Loss: 0.00005359
Iteration 844/1000 | Loss: 0.00004777
Iteration 845/1000 | Loss: 0.00004803
Iteration 846/1000 | Loss: 0.00004354
Iteration 847/1000 | Loss: 0.00003888
Iteration 848/1000 | Loss: 0.00004606
Iteration 849/1000 | Loss: 0.00005159
Iteration 850/1000 | Loss: 0.00004838
Iteration 851/1000 | Loss: 0.00003677
Iteration 852/1000 | Loss: 0.00004575
Iteration 853/1000 | Loss: 0.00003801
Iteration 854/1000 | Loss: 0.00004524
Iteration 855/1000 | Loss: 0.00004146
Iteration 856/1000 | Loss: 0.00004813
Iteration 857/1000 | Loss: 0.00003919
Iteration 858/1000 | Loss: 0.00003791
Iteration 859/1000 | Loss: 0.00003095
Iteration 860/1000 | Loss: 0.00003900
Iteration 861/1000 | Loss: 0.00003460
Iteration 862/1000 | Loss: 0.00002494
Iteration 863/1000 | Loss: 0.00002558
Iteration 864/1000 | Loss: 0.00002683
Iteration 865/1000 | Loss: 0.00002616
Iteration 866/1000 | Loss: 0.00002501
Iteration 867/1000 | Loss: 0.00002914
Iteration 868/1000 | Loss: 0.00003010
Iteration 869/1000 | Loss: 0.00002367
Iteration 870/1000 | Loss: 0.00002350
Iteration 871/1000 | Loss: 0.00002856
Iteration 872/1000 | Loss: 0.00002731
Iteration 873/1000 | Loss: 0.00002447
Iteration 874/1000 | Loss: 0.00002372
Iteration 875/1000 | Loss: 0.00002469
Iteration 876/1000 | Loss: 0.00002664
Iteration 877/1000 | Loss: 0.00002430
Iteration 878/1000 | Loss: 0.00002670
Iteration 879/1000 | Loss: 0.00002841
Iteration 880/1000 | Loss: 0.00002986
Iteration 881/1000 | Loss: 0.00002708
Iteration 882/1000 | Loss: 0.00002449
Iteration 883/1000 | Loss: 0.00002551
Iteration 884/1000 | Loss: 0.00002688
Iteration 885/1000 | Loss: 0.00003252
Iteration 886/1000 | Loss: 0.00003197
Iteration 887/1000 | Loss: 0.00003787
Iteration 888/1000 | Loss: 0.00003127
Iteration 889/1000 | Loss: 0.00002625
Iteration 890/1000 | Loss: 0.00003360
Iteration 891/1000 | Loss: 0.00003269
Iteration 892/1000 | Loss: 0.00004467
Iteration 893/1000 | Loss: 0.00003655
Iteration 894/1000 | Loss: 0.00004312
Iteration 895/1000 | Loss: 0.00003484
Iteration 896/1000 | Loss: 0.00004433
Iteration 897/1000 | Loss: 0.00003776
Iteration 898/1000 | Loss: 0.00004488
Iteration 899/1000 | Loss: 0.00003969
Iteration 900/1000 | Loss: 0.00004866
Iteration 901/1000 | Loss: 0.00003891
Iteration 902/1000 | Loss: 0.00004677
Iteration 903/1000 | Loss: 0.00003844
Iteration 904/1000 | Loss: 0.00005453
Iteration 905/1000 | Loss: 0.00003804
Iteration 906/1000 | Loss: 0.00005454
Iteration 907/1000 | Loss: 0.00003752
Iteration 908/1000 | Loss: 0.00004555
Iteration 909/1000 | Loss: 0.00004115
Iteration 910/1000 | Loss: 0.00005430
Iteration 911/1000 | Loss: 0.00002996
Iteration 912/1000 | Loss: 0.00002489
Iteration 913/1000 | Loss: 0.00002458
Iteration 914/1000 | Loss: 0.00002514
Iteration 915/1000 | Loss: 0.00002829
Iteration 916/1000 | Loss: 0.00002618
Iteration 917/1000 | Loss: 0.00002628
Iteration 918/1000 | Loss: 0.00003741
Iteration 919/1000 | Loss: 0.00002608
Iteration 920/1000 | Loss: 0.00002500
Iteration 921/1000 | Loss: 0.00002988
Iteration 922/1000 | Loss: 0.00002674
Iteration 923/1000 | Loss: 0.00003492
Iteration 924/1000 | Loss: 0.00004122
Iteration 925/1000 | Loss: 0.00002713
Iteration 926/1000 | Loss: 0.00002809
Iteration 927/1000 | Loss: 0.00003590
Iteration 928/1000 | Loss: 0.00003269
Iteration 929/1000 | Loss: 0.00003692
Iteration 930/1000 | Loss: 0.00003020
Iteration 931/1000 | Loss: 0.00002989
Iteration 932/1000 | Loss: 0.00003100
Iteration 933/1000 | Loss: 0.00003606
Iteration 934/1000 | Loss: 0.00003326
Iteration 935/1000 | Loss: 0.00002687
Iteration 936/1000 | Loss: 0.00002557
Iteration 937/1000 | Loss: 0.00003130
Iteration 938/1000 | Loss: 0.00003419
Iteration 939/1000 | Loss: 0.00004166
Iteration 940/1000 | Loss: 0.00003588
Iteration 941/1000 | Loss: 0.00004380
Iteration 942/1000 | Loss: 0.00004111
Iteration 943/1000 | Loss: 0.00005136
Iteration 944/1000 | Loss: 0.00004436
Iteration 945/1000 | Loss: 0.00003710
Iteration 946/1000 | Loss: 0.00003896
Iteration 947/1000 | Loss: 0.00004507
Iteration 948/1000 | Loss: 0.00004439
Iteration 949/1000 | Loss: 0.00005343
Iteration 950/1000 | Loss: 0.00005060
Iteration 951/1000 | Loss: 0.00005399
Iteration 952/1000 | Loss: 0.00002941
Iteration 953/1000 | Loss: 0.00002350
Iteration 954/1000 | Loss: 0.00002659
Iteration 955/1000 | Loss: 0.00002358
Iteration 956/1000 | Loss: 0.00002442
Iteration 957/1000 | Loss: 0.00002405
Iteration 958/1000 | Loss: 0.00003141
Iteration 959/1000 | Loss: 0.00002626
Iteration 960/1000 | Loss: 0.00004039
Iteration 961/1000 | Loss: 0.00003061
Iteration 962/1000 | Loss: 0.00004459
Iteration 963/1000 | Loss: 0.00003176
Iteration 964/1000 | Loss: 0.00004642
Iteration 965/1000 | Loss: 0.00003355
Iteration 966/1000 | Loss: 0.00005228
Iteration 967/1000 | Loss: 0.00003574
Iteration 968/1000 | Loss: 0.00005080
Iteration 969/1000 | Loss: 0.00003790
Iteration 970/1000 | Loss: 0.00004321
Iteration 971/1000 | Loss: 0.00003195
Iteration 972/1000 | Loss: 0.00004992
Iteration 973/1000 | Loss: 0.00004029
Iteration 974/1000 | Loss: 0.00003574
Iteration 975/1000 | Loss: 0.00002588
Iteration 976/1000 | Loss: 0.00002212
Iteration 977/1000 | Loss: 0.00002755
Iteration 978/1000 | Loss: 0.00002347
Iteration 979/1000 | Loss: 0.00002189
Iteration 980/1000 | Loss: 0.00004030
Iteration 981/1000 | Loss: 0.00003417
Iteration 982/1000 | Loss: 0.00004564
Iteration 983/1000 | Loss: 0.00004001
Iteration 984/1000 | Loss: 0.00002892
Iteration 985/1000 | Loss: 0.00002444
Iteration 986/1000 | Loss: 0.00002598
Iteration 987/1000 | Loss: 0.00002304
Iteration 988/1000 | Loss: 0.00004582
Iteration 989/1000 | Loss: 0.00004259
Iteration 990/1000 | Loss: 0.00004849
Iteration 991/1000 | Loss: 0.00004162
Iteration 992/1000 | Loss: 0.00005532
Iteration 993/1000 | Loss: 0.00004787
Iteration 994/1000 | Loss: 0.00006235
Iteration 995/1000 | Loss: 0.00005415
Iteration 996/1000 | Loss: 0.00005482
Iteration 997/1000 | Loss: 0.00004847
Iteration 998/1000 | Loss: 0.00004678
Iteration 999/1000 | Loss: 0.00004730
Iteration 1000/1000 | Loss: 0.00006587

Optimization complete. Final v2v error: 4.37255334854126 mm

Highest mean error: 12.124520301818848 mm for frame 108

Lowest mean error: 2.518576145172119 mm for frame 31

Saving results

Total time: 1446.8307280540466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439702
Iteration 2/25 | Loss: 0.00128676
Iteration 3/25 | Loss: 0.00123011
Iteration 4/25 | Loss: 0.00121641
Iteration 5/25 | Loss: 0.00121284
Iteration 6/25 | Loss: 0.00121272
Iteration 7/25 | Loss: 0.00121272
Iteration 8/25 | Loss: 0.00121272
Iteration 9/25 | Loss: 0.00121272
Iteration 10/25 | Loss: 0.00121272
Iteration 11/25 | Loss: 0.00121272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001212723320350051, 0.001212723320350051, 0.001212723320350051, 0.001212723320350051, 0.001212723320350051]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001212723320350051

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29944146
Iteration 2/25 | Loss: 0.00145949
Iteration 3/25 | Loss: 0.00145949
Iteration 4/25 | Loss: 0.00145949
Iteration 5/25 | Loss: 0.00145949
Iteration 6/25 | Loss: 0.00145949
Iteration 7/25 | Loss: 0.00145949
Iteration 8/25 | Loss: 0.00145949
Iteration 9/25 | Loss: 0.00145949
Iteration 10/25 | Loss: 0.00145949
Iteration 11/25 | Loss: 0.00145949
Iteration 12/25 | Loss: 0.00145949
Iteration 13/25 | Loss: 0.00145949
Iteration 14/25 | Loss: 0.00145949
Iteration 15/25 | Loss: 0.00145949
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014594882959499955, 0.0014594882959499955, 0.0014594882959499955, 0.0014594882959499955, 0.0014594882959499955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014594882959499955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145949
Iteration 2/1000 | Loss: 0.00002983
Iteration 3/1000 | Loss: 0.00002040
Iteration 4/1000 | Loss: 0.00001834
Iteration 5/1000 | Loss: 0.00001738
Iteration 6/1000 | Loss: 0.00001659
Iteration 7/1000 | Loss: 0.00001570
Iteration 8/1000 | Loss: 0.00001529
Iteration 9/1000 | Loss: 0.00001486
Iteration 10/1000 | Loss: 0.00001458
Iteration 11/1000 | Loss: 0.00001433
Iteration 12/1000 | Loss: 0.00001416
Iteration 13/1000 | Loss: 0.00001415
Iteration 14/1000 | Loss: 0.00001404
Iteration 15/1000 | Loss: 0.00001388
Iteration 16/1000 | Loss: 0.00001379
Iteration 17/1000 | Loss: 0.00001374
Iteration 18/1000 | Loss: 0.00001372
Iteration 19/1000 | Loss: 0.00001371
Iteration 20/1000 | Loss: 0.00001370
Iteration 21/1000 | Loss: 0.00001370
Iteration 22/1000 | Loss: 0.00001369
Iteration 23/1000 | Loss: 0.00001366
Iteration 24/1000 | Loss: 0.00001357
Iteration 25/1000 | Loss: 0.00001350
Iteration 26/1000 | Loss: 0.00001347
Iteration 27/1000 | Loss: 0.00001346
Iteration 28/1000 | Loss: 0.00001344
Iteration 29/1000 | Loss: 0.00001340
Iteration 30/1000 | Loss: 0.00001340
Iteration 31/1000 | Loss: 0.00001339
Iteration 32/1000 | Loss: 0.00001338
Iteration 33/1000 | Loss: 0.00001337
Iteration 34/1000 | Loss: 0.00001335
Iteration 35/1000 | Loss: 0.00001334
Iteration 36/1000 | Loss: 0.00001333
Iteration 37/1000 | Loss: 0.00001326
Iteration 38/1000 | Loss: 0.00001325
Iteration 39/1000 | Loss: 0.00001325
Iteration 40/1000 | Loss: 0.00001325
Iteration 41/1000 | Loss: 0.00001324
Iteration 42/1000 | Loss: 0.00001323
Iteration 43/1000 | Loss: 0.00001323
Iteration 44/1000 | Loss: 0.00001321
Iteration 45/1000 | Loss: 0.00001321
Iteration 46/1000 | Loss: 0.00001320
Iteration 47/1000 | Loss: 0.00001320
Iteration 48/1000 | Loss: 0.00001320
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001319
Iteration 51/1000 | Loss: 0.00001317
Iteration 52/1000 | Loss: 0.00001317
Iteration 53/1000 | Loss: 0.00001317
Iteration 54/1000 | Loss: 0.00001317
Iteration 55/1000 | Loss: 0.00001317
Iteration 56/1000 | Loss: 0.00001317
Iteration 57/1000 | Loss: 0.00001316
Iteration 58/1000 | Loss: 0.00001316
Iteration 59/1000 | Loss: 0.00001316
Iteration 60/1000 | Loss: 0.00001315
Iteration 61/1000 | Loss: 0.00001315
Iteration 62/1000 | Loss: 0.00001314
Iteration 63/1000 | Loss: 0.00001313
Iteration 64/1000 | Loss: 0.00001313
Iteration 65/1000 | Loss: 0.00001313
Iteration 66/1000 | Loss: 0.00001313
Iteration 67/1000 | Loss: 0.00001313
Iteration 68/1000 | Loss: 0.00001313
Iteration 69/1000 | Loss: 0.00001313
Iteration 70/1000 | Loss: 0.00001313
Iteration 71/1000 | Loss: 0.00001312
Iteration 72/1000 | Loss: 0.00001311
Iteration 73/1000 | Loss: 0.00001311
Iteration 74/1000 | Loss: 0.00001311
Iteration 75/1000 | Loss: 0.00001311
Iteration 76/1000 | Loss: 0.00001311
Iteration 77/1000 | Loss: 0.00001311
Iteration 78/1000 | Loss: 0.00001310
Iteration 79/1000 | Loss: 0.00001310
Iteration 80/1000 | Loss: 0.00001310
Iteration 81/1000 | Loss: 0.00001310
Iteration 82/1000 | Loss: 0.00001310
Iteration 83/1000 | Loss: 0.00001310
Iteration 84/1000 | Loss: 0.00001310
Iteration 85/1000 | Loss: 0.00001310
Iteration 86/1000 | Loss: 0.00001310
Iteration 87/1000 | Loss: 0.00001309
Iteration 88/1000 | Loss: 0.00001309
Iteration 89/1000 | Loss: 0.00001309
Iteration 90/1000 | Loss: 0.00001308
Iteration 91/1000 | Loss: 0.00001308
Iteration 92/1000 | Loss: 0.00001307
Iteration 93/1000 | Loss: 0.00001307
Iteration 94/1000 | Loss: 0.00001307
Iteration 95/1000 | Loss: 0.00001307
Iteration 96/1000 | Loss: 0.00001307
Iteration 97/1000 | Loss: 0.00001307
Iteration 98/1000 | Loss: 0.00001307
Iteration 99/1000 | Loss: 0.00001307
Iteration 100/1000 | Loss: 0.00001307
Iteration 101/1000 | Loss: 0.00001306
Iteration 102/1000 | Loss: 0.00001306
Iteration 103/1000 | Loss: 0.00001306
Iteration 104/1000 | Loss: 0.00001306
Iteration 105/1000 | Loss: 0.00001305
Iteration 106/1000 | Loss: 0.00001305
Iteration 107/1000 | Loss: 0.00001305
Iteration 108/1000 | Loss: 0.00001305
Iteration 109/1000 | Loss: 0.00001305
Iteration 110/1000 | Loss: 0.00001305
Iteration 111/1000 | Loss: 0.00001305
Iteration 112/1000 | Loss: 0.00001305
Iteration 113/1000 | Loss: 0.00001305
Iteration 114/1000 | Loss: 0.00001305
Iteration 115/1000 | Loss: 0.00001305
Iteration 116/1000 | Loss: 0.00001305
Iteration 117/1000 | Loss: 0.00001305
Iteration 118/1000 | Loss: 0.00001305
Iteration 119/1000 | Loss: 0.00001304
Iteration 120/1000 | Loss: 0.00001304
Iteration 121/1000 | Loss: 0.00001304
Iteration 122/1000 | Loss: 0.00001304
Iteration 123/1000 | Loss: 0.00001304
Iteration 124/1000 | Loss: 0.00001304
Iteration 125/1000 | Loss: 0.00001304
Iteration 126/1000 | Loss: 0.00001304
Iteration 127/1000 | Loss: 0.00001304
Iteration 128/1000 | Loss: 0.00001304
Iteration 129/1000 | Loss: 0.00001304
Iteration 130/1000 | Loss: 0.00001304
Iteration 131/1000 | Loss: 0.00001304
Iteration 132/1000 | Loss: 0.00001304
Iteration 133/1000 | Loss: 0.00001304
Iteration 134/1000 | Loss: 0.00001304
Iteration 135/1000 | Loss: 0.00001304
Iteration 136/1000 | Loss: 0.00001304
Iteration 137/1000 | Loss: 0.00001304
Iteration 138/1000 | Loss: 0.00001304
Iteration 139/1000 | Loss: 0.00001304
Iteration 140/1000 | Loss: 0.00001304
Iteration 141/1000 | Loss: 0.00001304
Iteration 142/1000 | Loss: 0.00001304
Iteration 143/1000 | Loss: 0.00001304
Iteration 144/1000 | Loss: 0.00001304
Iteration 145/1000 | Loss: 0.00001304
Iteration 146/1000 | Loss: 0.00001304
Iteration 147/1000 | Loss: 0.00001304
Iteration 148/1000 | Loss: 0.00001304
Iteration 149/1000 | Loss: 0.00001304
Iteration 150/1000 | Loss: 0.00001304
Iteration 151/1000 | Loss: 0.00001304
Iteration 152/1000 | Loss: 0.00001304
Iteration 153/1000 | Loss: 0.00001304
Iteration 154/1000 | Loss: 0.00001304
Iteration 155/1000 | Loss: 0.00001304
Iteration 156/1000 | Loss: 0.00001304
Iteration 157/1000 | Loss: 0.00001304
Iteration 158/1000 | Loss: 0.00001304
Iteration 159/1000 | Loss: 0.00001304
Iteration 160/1000 | Loss: 0.00001304
Iteration 161/1000 | Loss: 0.00001304
Iteration 162/1000 | Loss: 0.00001304
Iteration 163/1000 | Loss: 0.00001304
Iteration 164/1000 | Loss: 0.00001304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.3040702469879761e-05, 1.3040702469879761e-05, 1.3040702469879761e-05, 1.3040702469879761e-05, 1.3040702469879761e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3040702469879761e-05

Optimization complete. Final v2v error: 3.1015946865081787 mm

Highest mean error: 3.268686056137085 mm for frame 139

Lowest mean error: 2.9539265632629395 mm for frame 15

Saving results

Total time: 43.255845069885254
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419805
Iteration 2/25 | Loss: 0.00129284
Iteration 3/25 | Loss: 0.00120593
Iteration 4/25 | Loss: 0.00119882
Iteration 5/25 | Loss: 0.00119673
Iteration 6/25 | Loss: 0.00119642
Iteration 7/25 | Loss: 0.00119642
Iteration 8/25 | Loss: 0.00119642
Iteration 9/25 | Loss: 0.00119642
Iteration 10/25 | Loss: 0.00119642
Iteration 11/25 | Loss: 0.00119642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011964177247136831, 0.0011964177247136831, 0.0011964177247136831, 0.0011964177247136831, 0.0011964177247136831]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011964177247136831

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07119036
Iteration 2/25 | Loss: 0.00125844
Iteration 3/25 | Loss: 0.00125844
Iteration 4/25 | Loss: 0.00125844
Iteration 5/25 | Loss: 0.00125844
Iteration 6/25 | Loss: 0.00125844
Iteration 7/25 | Loss: 0.00125844
Iteration 8/25 | Loss: 0.00125844
Iteration 9/25 | Loss: 0.00125844
Iteration 10/25 | Loss: 0.00125844
Iteration 11/25 | Loss: 0.00125844
Iteration 12/25 | Loss: 0.00125844
Iteration 13/25 | Loss: 0.00125844
Iteration 14/25 | Loss: 0.00125844
Iteration 15/25 | Loss: 0.00125844
Iteration 16/25 | Loss: 0.00125844
Iteration 17/25 | Loss: 0.00125844
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012584395008161664, 0.0012584395008161664, 0.0012584395008161664, 0.0012584395008161664, 0.0012584395008161664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012584395008161664

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125844
Iteration 2/1000 | Loss: 0.00003760
Iteration 3/1000 | Loss: 0.00002158
Iteration 4/1000 | Loss: 0.00001809
Iteration 5/1000 | Loss: 0.00001649
Iteration 6/1000 | Loss: 0.00001528
Iteration 7/1000 | Loss: 0.00001473
Iteration 8/1000 | Loss: 0.00001403
Iteration 9/1000 | Loss: 0.00001362
Iteration 10/1000 | Loss: 0.00001313
Iteration 11/1000 | Loss: 0.00001286
Iteration 12/1000 | Loss: 0.00001260
Iteration 13/1000 | Loss: 0.00001249
Iteration 14/1000 | Loss: 0.00001248
Iteration 15/1000 | Loss: 0.00001231
Iteration 16/1000 | Loss: 0.00001206
Iteration 17/1000 | Loss: 0.00001204
Iteration 18/1000 | Loss: 0.00001202
Iteration 19/1000 | Loss: 0.00001200
Iteration 20/1000 | Loss: 0.00001200
Iteration 21/1000 | Loss: 0.00001200
Iteration 22/1000 | Loss: 0.00001200
Iteration 23/1000 | Loss: 0.00001200
Iteration 24/1000 | Loss: 0.00001200
Iteration 25/1000 | Loss: 0.00001200
Iteration 26/1000 | Loss: 0.00001200
Iteration 27/1000 | Loss: 0.00001200
Iteration 28/1000 | Loss: 0.00001200
Iteration 29/1000 | Loss: 0.00001200
Iteration 30/1000 | Loss: 0.00001199
Iteration 31/1000 | Loss: 0.00001199
Iteration 32/1000 | Loss: 0.00001196
Iteration 33/1000 | Loss: 0.00001195
Iteration 34/1000 | Loss: 0.00001195
Iteration 35/1000 | Loss: 0.00001194
Iteration 36/1000 | Loss: 0.00001193
Iteration 37/1000 | Loss: 0.00001193
Iteration 38/1000 | Loss: 0.00001192
Iteration 39/1000 | Loss: 0.00001192
Iteration 40/1000 | Loss: 0.00001191
Iteration 41/1000 | Loss: 0.00001190
Iteration 42/1000 | Loss: 0.00001190
Iteration 43/1000 | Loss: 0.00001190
Iteration 44/1000 | Loss: 0.00001190
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001189
Iteration 47/1000 | Loss: 0.00001189
Iteration 48/1000 | Loss: 0.00001189
Iteration 49/1000 | Loss: 0.00001189
Iteration 50/1000 | Loss: 0.00001189
Iteration 51/1000 | Loss: 0.00001189
Iteration 52/1000 | Loss: 0.00001189
Iteration 53/1000 | Loss: 0.00001189
Iteration 54/1000 | Loss: 0.00001189
Iteration 55/1000 | Loss: 0.00001189
Iteration 56/1000 | Loss: 0.00001189
Iteration 57/1000 | Loss: 0.00001189
Iteration 58/1000 | Loss: 0.00001189
Iteration 59/1000 | Loss: 0.00001189
Iteration 60/1000 | Loss: 0.00001188
Iteration 61/1000 | Loss: 0.00001188
Iteration 62/1000 | Loss: 0.00001188
Iteration 63/1000 | Loss: 0.00001187
Iteration 64/1000 | Loss: 0.00001187
Iteration 65/1000 | Loss: 0.00001187
Iteration 66/1000 | Loss: 0.00001186
Iteration 67/1000 | Loss: 0.00001186
Iteration 68/1000 | Loss: 0.00001186
Iteration 69/1000 | Loss: 0.00001186
Iteration 70/1000 | Loss: 0.00001186
Iteration 71/1000 | Loss: 0.00001186
Iteration 72/1000 | Loss: 0.00001185
Iteration 73/1000 | Loss: 0.00001184
Iteration 74/1000 | Loss: 0.00001183
Iteration 75/1000 | Loss: 0.00001182
Iteration 76/1000 | Loss: 0.00001182
Iteration 77/1000 | Loss: 0.00001182
Iteration 78/1000 | Loss: 0.00001181
Iteration 79/1000 | Loss: 0.00001181
Iteration 80/1000 | Loss: 0.00001180
Iteration 81/1000 | Loss: 0.00001180
Iteration 82/1000 | Loss: 0.00001180
Iteration 83/1000 | Loss: 0.00001180
Iteration 84/1000 | Loss: 0.00001179
Iteration 85/1000 | Loss: 0.00001179
Iteration 86/1000 | Loss: 0.00001179
Iteration 87/1000 | Loss: 0.00001179
Iteration 88/1000 | Loss: 0.00001179
Iteration 89/1000 | Loss: 0.00001178
Iteration 90/1000 | Loss: 0.00001178
Iteration 91/1000 | Loss: 0.00001178
Iteration 92/1000 | Loss: 0.00001178
Iteration 93/1000 | Loss: 0.00001178
Iteration 94/1000 | Loss: 0.00001177
Iteration 95/1000 | Loss: 0.00001177
Iteration 96/1000 | Loss: 0.00001176
Iteration 97/1000 | Loss: 0.00001176
Iteration 98/1000 | Loss: 0.00001176
Iteration 99/1000 | Loss: 0.00001176
Iteration 100/1000 | Loss: 0.00001176
Iteration 101/1000 | Loss: 0.00001176
Iteration 102/1000 | Loss: 0.00001176
Iteration 103/1000 | Loss: 0.00001176
Iteration 104/1000 | Loss: 0.00001175
Iteration 105/1000 | Loss: 0.00001175
Iteration 106/1000 | Loss: 0.00001175
Iteration 107/1000 | Loss: 0.00001174
Iteration 108/1000 | Loss: 0.00001174
Iteration 109/1000 | Loss: 0.00001174
Iteration 110/1000 | Loss: 0.00001174
Iteration 111/1000 | Loss: 0.00001174
Iteration 112/1000 | Loss: 0.00001174
Iteration 113/1000 | Loss: 0.00001174
Iteration 114/1000 | Loss: 0.00001174
Iteration 115/1000 | Loss: 0.00001174
Iteration 116/1000 | Loss: 0.00001174
Iteration 117/1000 | Loss: 0.00001174
Iteration 118/1000 | Loss: 0.00001174
Iteration 119/1000 | Loss: 0.00001174
Iteration 120/1000 | Loss: 0.00001174
Iteration 121/1000 | Loss: 0.00001174
Iteration 122/1000 | Loss: 0.00001174
Iteration 123/1000 | Loss: 0.00001174
Iteration 124/1000 | Loss: 0.00001174
Iteration 125/1000 | Loss: 0.00001174
Iteration 126/1000 | Loss: 0.00001173
Iteration 127/1000 | Loss: 0.00001173
Iteration 128/1000 | Loss: 0.00001173
Iteration 129/1000 | Loss: 0.00001172
Iteration 130/1000 | Loss: 0.00001172
Iteration 131/1000 | Loss: 0.00001172
Iteration 132/1000 | Loss: 0.00001172
Iteration 133/1000 | Loss: 0.00001172
Iteration 134/1000 | Loss: 0.00001172
Iteration 135/1000 | Loss: 0.00001172
Iteration 136/1000 | Loss: 0.00001172
Iteration 137/1000 | Loss: 0.00001172
Iteration 138/1000 | Loss: 0.00001172
Iteration 139/1000 | Loss: 0.00001172
Iteration 140/1000 | Loss: 0.00001172
Iteration 141/1000 | Loss: 0.00001172
Iteration 142/1000 | Loss: 0.00001172
Iteration 143/1000 | Loss: 0.00001172
Iteration 144/1000 | Loss: 0.00001172
Iteration 145/1000 | Loss: 0.00001172
Iteration 146/1000 | Loss: 0.00001172
Iteration 147/1000 | Loss: 0.00001172
Iteration 148/1000 | Loss: 0.00001172
Iteration 149/1000 | Loss: 0.00001172
Iteration 150/1000 | Loss: 0.00001172
Iteration 151/1000 | Loss: 0.00001172
Iteration 152/1000 | Loss: 0.00001172
Iteration 153/1000 | Loss: 0.00001172
Iteration 154/1000 | Loss: 0.00001172
Iteration 155/1000 | Loss: 0.00001172
Iteration 156/1000 | Loss: 0.00001172
Iteration 157/1000 | Loss: 0.00001172
Iteration 158/1000 | Loss: 0.00001172
Iteration 159/1000 | Loss: 0.00001172
Iteration 160/1000 | Loss: 0.00001172
Iteration 161/1000 | Loss: 0.00001172
Iteration 162/1000 | Loss: 0.00001172
Iteration 163/1000 | Loss: 0.00001172
Iteration 164/1000 | Loss: 0.00001172
Iteration 165/1000 | Loss: 0.00001172
Iteration 166/1000 | Loss: 0.00001172
Iteration 167/1000 | Loss: 0.00001172
Iteration 168/1000 | Loss: 0.00001172
Iteration 169/1000 | Loss: 0.00001172
Iteration 170/1000 | Loss: 0.00001172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.1722181625373196e-05, 1.1722181625373196e-05, 1.1722181625373196e-05, 1.1722181625373196e-05, 1.1722181625373196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1722181625373196e-05

Optimization complete. Final v2v error: 2.974712610244751 mm

Highest mean error: 2.993889808654785 mm for frame 52

Lowest mean error: 2.962247371673584 mm for frame 7

Saving results

Total time: 36.79906511306763
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00901939
Iteration 2/25 | Loss: 0.00137949
Iteration 3/25 | Loss: 0.00126836
Iteration 4/25 | Loss: 0.00124811
Iteration 5/25 | Loss: 0.00124175
Iteration 6/25 | Loss: 0.00124021
Iteration 7/25 | Loss: 0.00124021
Iteration 8/25 | Loss: 0.00124021
Iteration 9/25 | Loss: 0.00124021
Iteration 10/25 | Loss: 0.00124021
Iteration 11/25 | Loss: 0.00124021
Iteration 12/25 | Loss: 0.00124021
Iteration 13/25 | Loss: 0.00124021
Iteration 14/25 | Loss: 0.00124021
Iteration 15/25 | Loss: 0.00124021
Iteration 16/25 | Loss: 0.00124021
Iteration 17/25 | Loss: 0.00124021
Iteration 18/25 | Loss: 0.00124021
Iteration 19/25 | Loss: 0.00124021
Iteration 20/25 | Loss: 0.00124021
Iteration 21/25 | Loss: 0.00124021
Iteration 22/25 | Loss: 0.00124021
Iteration 23/25 | Loss: 0.00124021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012402096763253212, 0.0012402096763253212, 0.0012402096763253212, 0.0012402096763253212, 0.0012402096763253212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012402096763253212

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29590833
Iteration 2/25 | Loss: 0.00143914
Iteration 3/25 | Loss: 0.00143913
Iteration 4/25 | Loss: 0.00143913
Iteration 5/25 | Loss: 0.00143912
Iteration 6/25 | Loss: 0.00143912
Iteration 7/25 | Loss: 0.00143912
Iteration 8/25 | Loss: 0.00143912
Iteration 9/25 | Loss: 0.00143912
Iteration 10/25 | Loss: 0.00143912
Iteration 11/25 | Loss: 0.00143912
Iteration 12/25 | Loss: 0.00143912
Iteration 13/25 | Loss: 0.00143912
Iteration 14/25 | Loss: 0.00143912
Iteration 15/25 | Loss: 0.00143912
Iteration 16/25 | Loss: 0.00143912
Iteration 17/25 | Loss: 0.00143912
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014391235308721662, 0.0014391235308721662, 0.0014391235308721662, 0.0014391235308721662, 0.0014391235308721662]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014391235308721662

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143912
Iteration 2/1000 | Loss: 0.00003557
Iteration 3/1000 | Loss: 0.00002452
Iteration 4/1000 | Loss: 0.00002207
Iteration 5/1000 | Loss: 0.00002078
Iteration 6/1000 | Loss: 0.00002021
Iteration 7/1000 | Loss: 0.00001972
Iteration 8/1000 | Loss: 0.00001912
Iteration 9/1000 | Loss: 0.00001888
Iteration 10/1000 | Loss: 0.00001866
Iteration 11/1000 | Loss: 0.00001857
Iteration 12/1000 | Loss: 0.00001836
Iteration 13/1000 | Loss: 0.00001821
Iteration 14/1000 | Loss: 0.00001813
Iteration 15/1000 | Loss: 0.00001812
Iteration 16/1000 | Loss: 0.00001812
Iteration 17/1000 | Loss: 0.00001804
Iteration 18/1000 | Loss: 0.00001792
Iteration 19/1000 | Loss: 0.00001788
Iteration 20/1000 | Loss: 0.00001787
Iteration 21/1000 | Loss: 0.00001787
Iteration 22/1000 | Loss: 0.00001786
Iteration 23/1000 | Loss: 0.00001781
Iteration 24/1000 | Loss: 0.00001781
Iteration 25/1000 | Loss: 0.00001780
Iteration 26/1000 | Loss: 0.00001780
Iteration 27/1000 | Loss: 0.00001779
Iteration 28/1000 | Loss: 0.00001779
Iteration 29/1000 | Loss: 0.00001775
Iteration 30/1000 | Loss: 0.00001775
Iteration 31/1000 | Loss: 0.00001775
Iteration 32/1000 | Loss: 0.00001774
Iteration 33/1000 | Loss: 0.00001770
Iteration 34/1000 | Loss: 0.00001770
Iteration 35/1000 | Loss: 0.00001769
Iteration 36/1000 | Loss: 0.00001768
Iteration 37/1000 | Loss: 0.00001767
Iteration 38/1000 | Loss: 0.00001766
Iteration 39/1000 | Loss: 0.00001766
Iteration 40/1000 | Loss: 0.00001765
Iteration 41/1000 | Loss: 0.00001765
Iteration 42/1000 | Loss: 0.00001764
Iteration 43/1000 | Loss: 0.00001764
Iteration 44/1000 | Loss: 0.00001764
Iteration 45/1000 | Loss: 0.00001762
Iteration 46/1000 | Loss: 0.00001761
Iteration 47/1000 | Loss: 0.00001761
Iteration 48/1000 | Loss: 0.00001761
Iteration 49/1000 | Loss: 0.00001760
Iteration 50/1000 | Loss: 0.00001760
Iteration 51/1000 | Loss: 0.00001760
Iteration 52/1000 | Loss: 0.00001760
Iteration 53/1000 | Loss: 0.00001759
Iteration 54/1000 | Loss: 0.00001759
Iteration 55/1000 | Loss: 0.00001758
Iteration 56/1000 | Loss: 0.00001758
Iteration 57/1000 | Loss: 0.00001758
Iteration 58/1000 | Loss: 0.00001758
Iteration 59/1000 | Loss: 0.00001758
Iteration 60/1000 | Loss: 0.00001757
Iteration 61/1000 | Loss: 0.00001756
Iteration 62/1000 | Loss: 0.00001756
Iteration 63/1000 | Loss: 0.00001756
Iteration 64/1000 | Loss: 0.00001755
Iteration 65/1000 | Loss: 0.00001755
Iteration 66/1000 | Loss: 0.00001755
Iteration 67/1000 | Loss: 0.00001755
Iteration 68/1000 | Loss: 0.00001754
Iteration 69/1000 | Loss: 0.00001754
Iteration 70/1000 | Loss: 0.00001754
Iteration 71/1000 | Loss: 0.00001754
Iteration 72/1000 | Loss: 0.00001753
Iteration 73/1000 | Loss: 0.00001753
Iteration 74/1000 | Loss: 0.00001753
Iteration 75/1000 | Loss: 0.00001752
Iteration 76/1000 | Loss: 0.00001752
Iteration 77/1000 | Loss: 0.00001752
Iteration 78/1000 | Loss: 0.00001752
Iteration 79/1000 | Loss: 0.00001752
Iteration 80/1000 | Loss: 0.00001752
Iteration 81/1000 | Loss: 0.00001752
Iteration 82/1000 | Loss: 0.00001752
Iteration 83/1000 | Loss: 0.00001752
Iteration 84/1000 | Loss: 0.00001752
Iteration 85/1000 | Loss: 0.00001751
Iteration 86/1000 | Loss: 0.00001751
Iteration 87/1000 | Loss: 0.00001751
Iteration 88/1000 | Loss: 0.00001751
Iteration 89/1000 | Loss: 0.00001751
Iteration 90/1000 | Loss: 0.00001751
Iteration 91/1000 | Loss: 0.00001750
Iteration 92/1000 | Loss: 0.00001750
Iteration 93/1000 | Loss: 0.00001750
Iteration 94/1000 | Loss: 0.00001750
Iteration 95/1000 | Loss: 0.00001750
Iteration 96/1000 | Loss: 0.00001749
Iteration 97/1000 | Loss: 0.00001749
Iteration 98/1000 | Loss: 0.00001749
Iteration 99/1000 | Loss: 0.00001749
Iteration 100/1000 | Loss: 0.00001749
Iteration 101/1000 | Loss: 0.00001749
Iteration 102/1000 | Loss: 0.00001749
Iteration 103/1000 | Loss: 0.00001749
Iteration 104/1000 | Loss: 0.00001749
Iteration 105/1000 | Loss: 0.00001749
Iteration 106/1000 | Loss: 0.00001748
Iteration 107/1000 | Loss: 0.00001748
Iteration 108/1000 | Loss: 0.00001748
Iteration 109/1000 | Loss: 0.00001748
Iteration 110/1000 | Loss: 0.00001748
Iteration 111/1000 | Loss: 0.00001747
Iteration 112/1000 | Loss: 0.00001747
Iteration 113/1000 | Loss: 0.00001747
Iteration 114/1000 | Loss: 0.00001747
Iteration 115/1000 | Loss: 0.00001747
Iteration 116/1000 | Loss: 0.00001747
Iteration 117/1000 | Loss: 0.00001747
Iteration 118/1000 | Loss: 0.00001746
Iteration 119/1000 | Loss: 0.00001746
Iteration 120/1000 | Loss: 0.00001746
Iteration 121/1000 | Loss: 0.00001746
Iteration 122/1000 | Loss: 0.00001745
Iteration 123/1000 | Loss: 0.00001745
Iteration 124/1000 | Loss: 0.00001745
Iteration 125/1000 | Loss: 0.00001744
Iteration 126/1000 | Loss: 0.00001744
Iteration 127/1000 | Loss: 0.00001744
Iteration 128/1000 | Loss: 0.00001744
Iteration 129/1000 | Loss: 0.00001744
Iteration 130/1000 | Loss: 0.00001744
Iteration 131/1000 | Loss: 0.00001744
Iteration 132/1000 | Loss: 0.00001743
Iteration 133/1000 | Loss: 0.00001743
Iteration 134/1000 | Loss: 0.00001743
Iteration 135/1000 | Loss: 0.00001742
Iteration 136/1000 | Loss: 0.00001742
Iteration 137/1000 | Loss: 0.00001742
Iteration 138/1000 | Loss: 0.00001742
Iteration 139/1000 | Loss: 0.00001742
Iteration 140/1000 | Loss: 0.00001742
Iteration 141/1000 | Loss: 0.00001741
Iteration 142/1000 | Loss: 0.00001741
Iteration 143/1000 | Loss: 0.00001741
Iteration 144/1000 | Loss: 0.00001740
Iteration 145/1000 | Loss: 0.00001740
Iteration 146/1000 | Loss: 0.00001740
Iteration 147/1000 | Loss: 0.00001740
Iteration 148/1000 | Loss: 0.00001740
Iteration 149/1000 | Loss: 0.00001740
Iteration 150/1000 | Loss: 0.00001740
Iteration 151/1000 | Loss: 0.00001740
Iteration 152/1000 | Loss: 0.00001740
Iteration 153/1000 | Loss: 0.00001740
Iteration 154/1000 | Loss: 0.00001739
Iteration 155/1000 | Loss: 0.00001739
Iteration 156/1000 | Loss: 0.00001739
Iteration 157/1000 | Loss: 0.00001739
Iteration 158/1000 | Loss: 0.00001739
Iteration 159/1000 | Loss: 0.00001739
Iteration 160/1000 | Loss: 0.00001738
Iteration 161/1000 | Loss: 0.00001738
Iteration 162/1000 | Loss: 0.00001738
Iteration 163/1000 | Loss: 0.00001738
Iteration 164/1000 | Loss: 0.00001737
Iteration 165/1000 | Loss: 0.00001737
Iteration 166/1000 | Loss: 0.00001737
Iteration 167/1000 | Loss: 0.00001737
Iteration 168/1000 | Loss: 0.00001737
Iteration 169/1000 | Loss: 0.00001737
Iteration 170/1000 | Loss: 0.00001737
Iteration 171/1000 | Loss: 0.00001737
Iteration 172/1000 | Loss: 0.00001737
Iteration 173/1000 | Loss: 0.00001737
Iteration 174/1000 | Loss: 0.00001737
Iteration 175/1000 | Loss: 0.00001737
Iteration 176/1000 | Loss: 0.00001737
Iteration 177/1000 | Loss: 0.00001736
Iteration 178/1000 | Loss: 0.00001736
Iteration 179/1000 | Loss: 0.00001736
Iteration 180/1000 | Loss: 0.00001736
Iteration 181/1000 | Loss: 0.00001736
Iteration 182/1000 | Loss: 0.00001736
Iteration 183/1000 | Loss: 0.00001736
Iteration 184/1000 | Loss: 0.00001736
Iteration 185/1000 | Loss: 0.00001736
Iteration 186/1000 | Loss: 0.00001736
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.735633850330487e-05, 1.735633850330487e-05, 1.735633850330487e-05, 1.735633850330487e-05, 1.735633850330487e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.735633850330487e-05

Optimization complete. Final v2v error: 3.478480815887451 mm

Highest mean error: 5.503724575042725 mm for frame 70

Lowest mean error: 2.9789323806762695 mm for frame 95

Saving results

Total time: 43.06540250778198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403643
Iteration 2/25 | Loss: 0.00133561
Iteration 3/25 | Loss: 0.00121940
Iteration 4/25 | Loss: 0.00120480
Iteration 5/25 | Loss: 0.00120227
Iteration 6/25 | Loss: 0.00120187
Iteration 7/25 | Loss: 0.00120187
Iteration 8/25 | Loss: 0.00120187
Iteration 9/25 | Loss: 0.00120187
Iteration 10/25 | Loss: 0.00120187
Iteration 11/25 | Loss: 0.00120187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012018687557429075, 0.0012018687557429075, 0.0012018687557429075, 0.0012018687557429075, 0.0012018687557429075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012018687557429075

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27203274
Iteration 2/25 | Loss: 0.00144649
Iteration 3/25 | Loss: 0.00144648
Iteration 4/25 | Loss: 0.00144648
Iteration 5/25 | Loss: 0.00144648
Iteration 6/25 | Loss: 0.00144648
Iteration 7/25 | Loss: 0.00144648
Iteration 8/25 | Loss: 0.00144648
Iteration 9/25 | Loss: 0.00144648
Iteration 10/25 | Loss: 0.00144648
Iteration 11/25 | Loss: 0.00144648
Iteration 12/25 | Loss: 0.00144648
Iteration 13/25 | Loss: 0.00144648
Iteration 14/25 | Loss: 0.00144648
Iteration 15/25 | Loss: 0.00144648
Iteration 16/25 | Loss: 0.00144648
Iteration 17/25 | Loss: 0.00144648
Iteration 18/25 | Loss: 0.00144648
Iteration 19/25 | Loss: 0.00144648
Iteration 20/25 | Loss: 0.00144648
Iteration 21/25 | Loss: 0.00144648
Iteration 22/25 | Loss: 0.00144648
Iteration 23/25 | Loss: 0.00144648
Iteration 24/25 | Loss: 0.00144648
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014464838895946741, 0.0014464838895946741, 0.0014464838895946741, 0.0014464838895946741, 0.0014464838895946741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014464838895946741

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144648
Iteration 2/1000 | Loss: 0.00003340
Iteration 3/1000 | Loss: 0.00002346
Iteration 4/1000 | Loss: 0.00001929
Iteration 5/1000 | Loss: 0.00001733
Iteration 6/1000 | Loss: 0.00001630
Iteration 7/1000 | Loss: 0.00001554
Iteration 8/1000 | Loss: 0.00001507
Iteration 9/1000 | Loss: 0.00001465
Iteration 10/1000 | Loss: 0.00001431
Iteration 11/1000 | Loss: 0.00001400
Iteration 12/1000 | Loss: 0.00001373
Iteration 13/1000 | Loss: 0.00001369
Iteration 14/1000 | Loss: 0.00001361
Iteration 15/1000 | Loss: 0.00001356
Iteration 16/1000 | Loss: 0.00001355
Iteration 17/1000 | Loss: 0.00001354
Iteration 18/1000 | Loss: 0.00001349
Iteration 19/1000 | Loss: 0.00001348
Iteration 20/1000 | Loss: 0.00001347
Iteration 21/1000 | Loss: 0.00001346
Iteration 22/1000 | Loss: 0.00001344
Iteration 23/1000 | Loss: 0.00001343
Iteration 24/1000 | Loss: 0.00001343
Iteration 25/1000 | Loss: 0.00001341
Iteration 26/1000 | Loss: 0.00001338
Iteration 27/1000 | Loss: 0.00001336
Iteration 28/1000 | Loss: 0.00001336
Iteration 29/1000 | Loss: 0.00001335
Iteration 30/1000 | Loss: 0.00001335
Iteration 31/1000 | Loss: 0.00001326
Iteration 32/1000 | Loss: 0.00001325
Iteration 33/1000 | Loss: 0.00001324
Iteration 34/1000 | Loss: 0.00001324
Iteration 35/1000 | Loss: 0.00001323
Iteration 36/1000 | Loss: 0.00001323
Iteration 37/1000 | Loss: 0.00001322
Iteration 38/1000 | Loss: 0.00001322
Iteration 39/1000 | Loss: 0.00001322
Iteration 40/1000 | Loss: 0.00001321
Iteration 41/1000 | Loss: 0.00001320
Iteration 42/1000 | Loss: 0.00001320
Iteration 43/1000 | Loss: 0.00001320
Iteration 44/1000 | Loss: 0.00001319
Iteration 45/1000 | Loss: 0.00001319
Iteration 46/1000 | Loss: 0.00001319
Iteration 47/1000 | Loss: 0.00001319
Iteration 48/1000 | Loss: 0.00001318
Iteration 49/1000 | Loss: 0.00001318
Iteration 50/1000 | Loss: 0.00001318
Iteration 51/1000 | Loss: 0.00001318
Iteration 52/1000 | Loss: 0.00001318
Iteration 53/1000 | Loss: 0.00001317
Iteration 54/1000 | Loss: 0.00001317
Iteration 55/1000 | Loss: 0.00001316
Iteration 56/1000 | Loss: 0.00001316
Iteration 57/1000 | Loss: 0.00001316
Iteration 58/1000 | Loss: 0.00001315
Iteration 59/1000 | Loss: 0.00001315
Iteration 60/1000 | Loss: 0.00001314
Iteration 61/1000 | Loss: 0.00001312
Iteration 62/1000 | Loss: 0.00001311
Iteration 63/1000 | Loss: 0.00001311
Iteration 64/1000 | Loss: 0.00001311
Iteration 65/1000 | Loss: 0.00001309
Iteration 66/1000 | Loss: 0.00001306
Iteration 67/1000 | Loss: 0.00001305
Iteration 68/1000 | Loss: 0.00001304
Iteration 69/1000 | Loss: 0.00001302
Iteration 70/1000 | Loss: 0.00001301
Iteration 71/1000 | Loss: 0.00001301
Iteration 72/1000 | Loss: 0.00001299
Iteration 73/1000 | Loss: 0.00001298
Iteration 74/1000 | Loss: 0.00001298
Iteration 75/1000 | Loss: 0.00001297
Iteration 76/1000 | Loss: 0.00001293
Iteration 77/1000 | Loss: 0.00001293
Iteration 78/1000 | Loss: 0.00001292
Iteration 79/1000 | Loss: 0.00001291
Iteration 80/1000 | Loss: 0.00001291
Iteration 81/1000 | Loss: 0.00001291
Iteration 82/1000 | Loss: 0.00001291
Iteration 83/1000 | Loss: 0.00001290
Iteration 84/1000 | Loss: 0.00001290
Iteration 85/1000 | Loss: 0.00001289
Iteration 86/1000 | Loss: 0.00001288
Iteration 87/1000 | Loss: 0.00001287
Iteration 88/1000 | Loss: 0.00001287
Iteration 89/1000 | Loss: 0.00001286
Iteration 90/1000 | Loss: 0.00001286
Iteration 91/1000 | Loss: 0.00001286
Iteration 92/1000 | Loss: 0.00001286
Iteration 93/1000 | Loss: 0.00001285
Iteration 94/1000 | Loss: 0.00001285
Iteration 95/1000 | Loss: 0.00001285
Iteration 96/1000 | Loss: 0.00001284
Iteration 97/1000 | Loss: 0.00001284
Iteration 98/1000 | Loss: 0.00001283
Iteration 99/1000 | Loss: 0.00001283
Iteration 100/1000 | Loss: 0.00001282
Iteration 101/1000 | Loss: 0.00001282
Iteration 102/1000 | Loss: 0.00001281
Iteration 103/1000 | Loss: 0.00001281
Iteration 104/1000 | Loss: 0.00001280
Iteration 105/1000 | Loss: 0.00001279
Iteration 106/1000 | Loss: 0.00001277
Iteration 107/1000 | Loss: 0.00001277
Iteration 108/1000 | Loss: 0.00001276
Iteration 109/1000 | Loss: 0.00001276
Iteration 110/1000 | Loss: 0.00001276
Iteration 111/1000 | Loss: 0.00001276
Iteration 112/1000 | Loss: 0.00001275
Iteration 113/1000 | Loss: 0.00001275
Iteration 114/1000 | Loss: 0.00001275
Iteration 115/1000 | Loss: 0.00001275
Iteration 116/1000 | Loss: 0.00001274
Iteration 117/1000 | Loss: 0.00001274
Iteration 118/1000 | Loss: 0.00001273
Iteration 119/1000 | Loss: 0.00001273
Iteration 120/1000 | Loss: 0.00001272
Iteration 121/1000 | Loss: 0.00001272
Iteration 122/1000 | Loss: 0.00001272
Iteration 123/1000 | Loss: 0.00001272
Iteration 124/1000 | Loss: 0.00001272
Iteration 125/1000 | Loss: 0.00001272
Iteration 126/1000 | Loss: 0.00001271
Iteration 127/1000 | Loss: 0.00001271
Iteration 128/1000 | Loss: 0.00001271
Iteration 129/1000 | Loss: 0.00001271
Iteration 130/1000 | Loss: 0.00001271
Iteration 131/1000 | Loss: 0.00001270
Iteration 132/1000 | Loss: 0.00001270
Iteration 133/1000 | Loss: 0.00001270
Iteration 134/1000 | Loss: 0.00001270
Iteration 135/1000 | Loss: 0.00001270
Iteration 136/1000 | Loss: 0.00001269
Iteration 137/1000 | Loss: 0.00001269
Iteration 138/1000 | Loss: 0.00001269
Iteration 139/1000 | Loss: 0.00001269
Iteration 140/1000 | Loss: 0.00001268
Iteration 141/1000 | Loss: 0.00001268
Iteration 142/1000 | Loss: 0.00001268
Iteration 143/1000 | Loss: 0.00001268
Iteration 144/1000 | Loss: 0.00001267
Iteration 145/1000 | Loss: 0.00001267
Iteration 146/1000 | Loss: 0.00001267
Iteration 147/1000 | Loss: 0.00001267
Iteration 148/1000 | Loss: 0.00001267
Iteration 149/1000 | Loss: 0.00001266
Iteration 150/1000 | Loss: 0.00001266
Iteration 151/1000 | Loss: 0.00001266
Iteration 152/1000 | Loss: 0.00001266
Iteration 153/1000 | Loss: 0.00001266
Iteration 154/1000 | Loss: 0.00001265
Iteration 155/1000 | Loss: 0.00001265
Iteration 156/1000 | Loss: 0.00001265
Iteration 157/1000 | Loss: 0.00001265
Iteration 158/1000 | Loss: 0.00001264
Iteration 159/1000 | Loss: 0.00001264
Iteration 160/1000 | Loss: 0.00001264
Iteration 161/1000 | Loss: 0.00001264
Iteration 162/1000 | Loss: 0.00001264
Iteration 163/1000 | Loss: 0.00001263
Iteration 164/1000 | Loss: 0.00001263
Iteration 165/1000 | Loss: 0.00001263
Iteration 166/1000 | Loss: 0.00001263
Iteration 167/1000 | Loss: 0.00001262
Iteration 168/1000 | Loss: 0.00001262
Iteration 169/1000 | Loss: 0.00001262
Iteration 170/1000 | Loss: 0.00001262
Iteration 171/1000 | Loss: 0.00001262
Iteration 172/1000 | Loss: 0.00001262
Iteration 173/1000 | Loss: 0.00001262
Iteration 174/1000 | Loss: 0.00001262
Iteration 175/1000 | Loss: 0.00001262
Iteration 176/1000 | Loss: 0.00001261
Iteration 177/1000 | Loss: 0.00001261
Iteration 178/1000 | Loss: 0.00001261
Iteration 179/1000 | Loss: 0.00001261
Iteration 180/1000 | Loss: 0.00001261
Iteration 181/1000 | Loss: 0.00001261
Iteration 182/1000 | Loss: 0.00001261
Iteration 183/1000 | Loss: 0.00001261
Iteration 184/1000 | Loss: 0.00001261
Iteration 185/1000 | Loss: 0.00001261
Iteration 186/1000 | Loss: 0.00001261
Iteration 187/1000 | Loss: 0.00001261
Iteration 188/1000 | Loss: 0.00001261
Iteration 189/1000 | Loss: 0.00001261
Iteration 190/1000 | Loss: 0.00001261
Iteration 191/1000 | Loss: 0.00001261
Iteration 192/1000 | Loss: 0.00001261
Iteration 193/1000 | Loss: 0.00001261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.2611371857929043e-05, 1.2611371857929043e-05, 1.2611371857929043e-05, 1.2611371857929043e-05, 1.2611371857929043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2611371857929043e-05

Optimization complete. Final v2v error: 2.9908199310302734 mm

Highest mean error: 3.461214542388916 mm for frame 87

Lowest mean error: 2.663235664367676 mm for frame 11

Saving results

Total time: 45.42796015739441
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00538808
Iteration 2/25 | Loss: 0.00126092
Iteration 3/25 | Loss: 0.00119989
Iteration 4/25 | Loss: 0.00119078
Iteration 5/25 | Loss: 0.00118701
Iteration 6/25 | Loss: 0.00118677
Iteration 7/25 | Loss: 0.00118677
Iteration 8/25 | Loss: 0.00118677
Iteration 9/25 | Loss: 0.00118677
Iteration 10/25 | Loss: 0.00118677
Iteration 11/25 | Loss: 0.00118677
Iteration 12/25 | Loss: 0.00118677
Iteration 13/25 | Loss: 0.00118677
Iteration 14/25 | Loss: 0.00118677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001186771085485816, 0.001186771085485816, 0.001186771085485816, 0.001186771085485816, 0.001186771085485816]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001186771085485816

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.12007999
Iteration 2/25 | Loss: 0.00145217
Iteration 3/25 | Loss: 0.00145217
Iteration 4/25 | Loss: 0.00145217
Iteration 5/25 | Loss: 0.00145217
Iteration 6/25 | Loss: 0.00145217
Iteration 7/25 | Loss: 0.00145217
Iteration 8/25 | Loss: 0.00145216
Iteration 9/25 | Loss: 0.00145216
Iteration 10/25 | Loss: 0.00145216
Iteration 11/25 | Loss: 0.00145216
Iteration 12/25 | Loss: 0.00145216
Iteration 13/25 | Loss: 0.00145216
Iteration 14/25 | Loss: 0.00145216
Iteration 15/25 | Loss: 0.00145216
Iteration 16/25 | Loss: 0.00145216
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014521641423925757, 0.0014521641423925757, 0.0014521641423925757, 0.0014521641423925757, 0.0014521641423925757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014521641423925757

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145216
Iteration 2/1000 | Loss: 0.00001829
Iteration 3/1000 | Loss: 0.00001454
Iteration 4/1000 | Loss: 0.00001341
Iteration 5/1000 | Loss: 0.00001276
Iteration 6/1000 | Loss: 0.00001226
Iteration 7/1000 | Loss: 0.00001185
Iteration 8/1000 | Loss: 0.00001150
Iteration 9/1000 | Loss: 0.00001113
Iteration 10/1000 | Loss: 0.00001086
Iteration 11/1000 | Loss: 0.00001066
Iteration 12/1000 | Loss: 0.00001056
Iteration 13/1000 | Loss: 0.00001045
Iteration 14/1000 | Loss: 0.00001042
Iteration 15/1000 | Loss: 0.00001040
Iteration 16/1000 | Loss: 0.00001038
Iteration 17/1000 | Loss: 0.00001023
Iteration 18/1000 | Loss: 0.00001017
Iteration 19/1000 | Loss: 0.00001016
Iteration 20/1000 | Loss: 0.00001015
Iteration 21/1000 | Loss: 0.00001014
Iteration 22/1000 | Loss: 0.00001014
Iteration 23/1000 | Loss: 0.00001012
Iteration 24/1000 | Loss: 0.00001012
Iteration 25/1000 | Loss: 0.00001011
Iteration 26/1000 | Loss: 0.00001010
Iteration 27/1000 | Loss: 0.00001010
Iteration 28/1000 | Loss: 0.00001010
Iteration 29/1000 | Loss: 0.00001010
Iteration 30/1000 | Loss: 0.00001009
Iteration 31/1000 | Loss: 0.00001009
Iteration 32/1000 | Loss: 0.00001007
Iteration 33/1000 | Loss: 0.00001007
Iteration 34/1000 | Loss: 0.00001006
Iteration 35/1000 | Loss: 0.00001006
Iteration 36/1000 | Loss: 0.00001006
Iteration 37/1000 | Loss: 0.00001005
Iteration 38/1000 | Loss: 0.00001005
Iteration 39/1000 | Loss: 0.00001005
Iteration 40/1000 | Loss: 0.00001004
Iteration 41/1000 | Loss: 0.00001002
Iteration 42/1000 | Loss: 0.00001002
Iteration 43/1000 | Loss: 0.00001001
Iteration 44/1000 | Loss: 0.00001001
Iteration 45/1000 | Loss: 0.00001001
Iteration 46/1000 | Loss: 0.00001000
Iteration 47/1000 | Loss: 0.00001000
Iteration 48/1000 | Loss: 0.00001000
Iteration 49/1000 | Loss: 0.00000999
Iteration 50/1000 | Loss: 0.00000998
Iteration 51/1000 | Loss: 0.00000998
Iteration 52/1000 | Loss: 0.00000997
Iteration 53/1000 | Loss: 0.00000997
Iteration 54/1000 | Loss: 0.00000997
Iteration 55/1000 | Loss: 0.00000996
Iteration 56/1000 | Loss: 0.00000996
Iteration 57/1000 | Loss: 0.00000996
Iteration 58/1000 | Loss: 0.00000995
Iteration 59/1000 | Loss: 0.00000995
Iteration 60/1000 | Loss: 0.00000995
Iteration 61/1000 | Loss: 0.00000995
Iteration 62/1000 | Loss: 0.00000994
Iteration 63/1000 | Loss: 0.00000994
Iteration 64/1000 | Loss: 0.00000994
Iteration 65/1000 | Loss: 0.00000994
Iteration 66/1000 | Loss: 0.00000993
Iteration 67/1000 | Loss: 0.00000993
Iteration 68/1000 | Loss: 0.00000993
Iteration 69/1000 | Loss: 0.00000993
Iteration 70/1000 | Loss: 0.00000992
Iteration 71/1000 | Loss: 0.00000992
Iteration 72/1000 | Loss: 0.00000991
Iteration 73/1000 | Loss: 0.00000991
Iteration 74/1000 | Loss: 0.00000991
Iteration 75/1000 | Loss: 0.00000991
Iteration 76/1000 | Loss: 0.00000991
Iteration 77/1000 | Loss: 0.00000990
Iteration 78/1000 | Loss: 0.00000990
Iteration 79/1000 | Loss: 0.00000990
Iteration 80/1000 | Loss: 0.00000990
Iteration 81/1000 | Loss: 0.00000989
Iteration 82/1000 | Loss: 0.00000989
Iteration 83/1000 | Loss: 0.00000988
Iteration 84/1000 | Loss: 0.00000988
Iteration 85/1000 | Loss: 0.00000988
Iteration 86/1000 | Loss: 0.00000987
Iteration 87/1000 | Loss: 0.00000987
Iteration 88/1000 | Loss: 0.00000987
Iteration 89/1000 | Loss: 0.00000986
Iteration 90/1000 | Loss: 0.00000986
Iteration 91/1000 | Loss: 0.00000986
Iteration 92/1000 | Loss: 0.00000985
Iteration 93/1000 | Loss: 0.00000985
Iteration 94/1000 | Loss: 0.00000985
Iteration 95/1000 | Loss: 0.00000984
Iteration 96/1000 | Loss: 0.00000984
Iteration 97/1000 | Loss: 0.00000983
Iteration 98/1000 | Loss: 0.00000983
Iteration 99/1000 | Loss: 0.00000983
Iteration 100/1000 | Loss: 0.00000983
Iteration 101/1000 | Loss: 0.00000983
Iteration 102/1000 | Loss: 0.00000982
Iteration 103/1000 | Loss: 0.00000981
Iteration 104/1000 | Loss: 0.00000981
Iteration 105/1000 | Loss: 0.00000981
Iteration 106/1000 | Loss: 0.00000981
Iteration 107/1000 | Loss: 0.00000981
Iteration 108/1000 | Loss: 0.00000980
Iteration 109/1000 | Loss: 0.00000980
Iteration 110/1000 | Loss: 0.00000980
Iteration 111/1000 | Loss: 0.00000980
Iteration 112/1000 | Loss: 0.00000980
Iteration 113/1000 | Loss: 0.00000980
Iteration 114/1000 | Loss: 0.00000980
Iteration 115/1000 | Loss: 0.00000980
Iteration 116/1000 | Loss: 0.00000980
Iteration 117/1000 | Loss: 0.00000979
Iteration 118/1000 | Loss: 0.00000979
Iteration 119/1000 | Loss: 0.00000979
Iteration 120/1000 | Loss: 0.00000979
Iteration 121/1000 | Loss: 0.00000979
Iteration 122/1000 | Loss: 0.00000979
Iteration 123/1000 | Loss: 0.00000979
Iteration 124/1000 | Loss: 0.00000979
Iteration 125/1000 | Loss: 0.00000978
Iteration 126/1000 | Loss: 0.00000978
Iteration 127/1000 | Loss: 0.00000978
Iteration 128/1000 | Loss: 0.00000978
Iteration 129/1000 | Loss: 0.00000978
Iteration 130/1000 | Loss: 0.00000978
Iteration 131/1000 | Loss: 0.00000978
Iteration 132/1000 | Loss: 0.00000978
Iteration 133/1000 | Loss: 0.00000977
Iteration 134/1000 | Loss: 0.00000977
Iteration 135/1000 | Loss: 0.00000977
Iteration 136/1000 | Loss: 0.00000977
Iteration 137/1000 | Loss: 0.00000977
Iteration 138/1000 | Loss: 0.00000977
Iteration 139/1000 | Loss: 0.00000977
Iteration 140/1000 | Loss: 0.00000977
Iteration 141/1000 | Loss: 0.00000977
Iteration 142/1000 | Loss: 0.00000977
Iteration 143/1000 | Loss: 0.00000977
Iteration 144/1000 | Loss: 0.00000977
Iteration 145/1000 | Loss: 0.00000977
Iteration 146/1000 | Loss: 0.00000977
Iteration 147/1000 | Loss: 0.00000977
Iteration 148/1000 | Loss: 0.00000977
Iteration 149/1000 | Loss: 0.00000977
Iteration 150/1000 | Loss: 0.00000977
Iteration 151/1000 | Loss: 0.00000977
Iteration 152/1000 | Loss: 0.00000977
Iteration 153/1000 | Loss: 0.00000977
Iteration 154/1000 | Loss: 0.00000977
Iteration 155/1000 | Loss: 0.00000977
Iteration 156/1000 | Loss: 0.00000977
Iteration 157/1000 | Loss: 0.00000977
Iteration 158/1000 | Loss: 0.00000977
Iteration 159/1000 | Loss: 0.00000977
Iteration 160/1000 | Loss: 0.00000977
Iteration 161/1000 | Loss: 0.00000977
Iteration 162/1000 | Loss: 0.00000977
Iteration 163/1000 | Loss: 0.00000977
Iteration 164/1000 | Loss: 0.00000977
Iteration 165/1000 | Loss: 0.00000977
Iteration 166/1000 | Loss: 0.00000977
Iteration 167/1000 | Loss: 0.00000977
Iteration 168/1000 | Loss: 0.00000977
Iteration 169/1000 | Loss: 0.00000977
Iteration 170/1000 | Loss: 0.00000977
Iteration 171/1000 | Loss: 0.00000977
Iteration 172/1000 | Loss: 0.00000977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [9.768751624505967e-06, 9.768751624505967e-06, 9.768751624505967e-06, 9.768751624505967e-06, 9.768751624505967e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.768751624505967e-06

Optimization complete. Final v2v error: 2.7292850017547607 mm

Highest mean error: 2.9867260456085205 mm for frame 95

Lowest mean error: 2.522915840148926 mm for frame 20

Saving results

Total time: 41.02345561981201
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395662
Iteration 2/25 | Loss: 0.00125308
Iteration 3/25 | Loss: 0.00120188
Iteration 4/25 | Loss: 0.00119223
Iteration 5/25 | Loss: 0.00118888
Iteration 6/25 | Loss: 0.00118874
Iteration 7/25 | Loss: 0.00118874
Iteration 8/25 | Loss: 0.00118874
Iteration 9/25 | Loss: 0.00118874
Iteration 10/25 | Loss: 0.00118874
Iteration 11/25 | Loss: 0.00118874
Iteration 12/25 | Loss: 0.00118874
Iteration 13/25 | Loss: 0.00118874
Iteration 14/25 | Loss: 0.00118874
Iteration 15/25 | Loss: 0.00118874
Iteration 16/25 | Loss: 0.00118874
Iteration 17/25 | Loss: 0.00118874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011887371074408293, 0.0011887371074408293, 0.0011887371074408293, 0.0011887371074408293, 0.0011887371074408293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011887371074408293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28518617
Iteration 2/25 | Loss: 0.00220097
Iteration 3/25 | Loss: 0.00220097
Iteration 4/25 | Loss: 0.00220097
Iteration 5/25 | Loss: 0.00220097
Iteration 6/25 | Loss: 0.00220097
Iteration 7/25 | Loss: 0.00220097
Iteration 8/25 | Loss: 0.00220097
Iteration 9/25 | Loss: 0.00220097
Iteration 10/25 | Loss: 0.00220097
Iteration 11/25 | Loss: 0.00220097
Iteration 12/25 | Loss: 0.00220097
Iteration 13/25 | Loss: 0.00220097
Iteration 14/25 | Loss: 0.00220097
Iteration 15/25 | Loss: 0.00220097
Iteration 16/25 | Loss: 0.00220097
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0022009697277098894, 0.0022009697277098894, 0.0022009697277098894, 0.0022009697277098894, 0.0022009697277098894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022009697277098894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220097
Iteration 2/1000 | Loss: 0.00003910
Iteration 3/1000 | Loss: 0.00002652
Iteration 4/1000 | Loss: 0.00002151
Iteration 5/1000 | Loss: 0.00001955
Iteration 6/1000 | Loss: 0.00001815
Iteration 7/1000 | Loss: 0.00001724
Iteration 8/1000 | Loss: 0.00001665
Iteration 9/1000 | Loss: 0.00001625
Iteration 10/1000 | Loss: 0.00001595
Iteration 11/1000 | Loss: 0.00001555
Iteration 12/1000 | Loss: 0.00001530
Iteration 13/1000 | Loss: 0.00001526
Iteration 14/1000 | Loss: 0.00001524
Iteration 15/1000 | Loss: 0.00001507
Iteration 16/1000 | Loss: 0.00001501
Iteration 17/1000 | Loss: 0.00001497
Iteration 18/1000 | Loss: 0.00001496
Iteration 19/1000 | Loss: 0.00001496
Iteration 20/1000 | Loss: 0.00001494
Iteration 21/1000 | Loss: 0.00001488
Iteration 22/1000 | Loss: 0.00001485
Iteration 23/1000 | Loss: 0.00001485
Iteration 24/1000 | Loss: 0.00001484
Iteration 25/1000 | Loss: 0.00001483
Iteration 26/1000 | Loss: 0.00001482
Iteration 27/1000 | Loss: 0.00001482
Iteration 28/1000 | Loss: 0.00001481
Iteration 29/1000 | Loss: 0.00001481
Iteration 30/1000 | Loss: 0.00001476
Iteration 31/1000 | Loss: 0.00001473
Iteration 32/1000 | Loss: 0.00001472
Iteration 33/1000 | Loss: 0.00001471
Iteration 34/1000 | Loss: 0.00001466
Iteration 35/1000 | Loss: 0.00001464
Iteration 36/1000 | Loss: 0.00001464
Iteration 37/1000 | Loss: 0.00001463
Iteration 38/1000 | Loss: 0.00001463
Iteration 39/1000 | Loss: 0.00001462
Iteration 40/1000 | Loss: 0.00001461
Iteration 41/1000 | Loss: 0.00001461
Iteration 42/1000 | Loss: 0.00001461
Iteration 43/1000 | Loss: 0.00001460
Iteration 44/1000 | Loss: 0.00001460
Iteration 45/1000 | Loss: 0.00001460
Iteration 46/1000 | Loss: 0.00001459
Iteration 47/1000 | Loss: 0.00001459
Iteration 48/1000 | Loss: 0.00001459
Iteration 49/1000 | Loss: 0.00001458
Iteration 50/1000 | Loss: 0.00001458
Iteration 51/1000 | Loss: 0.00001457
Iteration 52/1000 | Loss: 0.00001457
Iteration 53/1000 | Loss: 0.00001456
Iteration 54/1000 | Loss: 0.00001456
Iteration 55/1000 | Loss: 0.00001455
Iteration 56/1000 | Loss: 0.00001455
Iteration 57/1000 | Loss: 0.00001454
Iteration 58/1000 | Loss: 0.00001454
Iteration 59/1000 | Loss: 0.00001454
Iteration 60/1000 | Loss: 0.00001454
Iteration 61/1000 | Loss: 0.00001453
Iteration 62/1000 | Loss: 0.00001453
Iteration 63/1000 | Loss: 0.00001453
Iteration 64/1000 | Loss: 0.00001452
Iteration 65/1000 | Loss: 0.00001452
Iteration 66/1000 | Loss: 0.00001452
Iteration 67/1000 | Loss: 0.00001451
Iteration 68/1000 | Loss: 0.00001451
Iteration 69/1000 | Loss: 0.00001451
Iteration 70/1000 | Loss: 0.00001451
Iteration 71/1000 | Loss: 0.00001451
Iteration 72/1000 | Loss: 0.00001450
Iteration 73/1000 | Loss: 0.00001450
Iteration 74/1000 | Loss: 0.00001450
Iteration 75/1000 | Loss: 0.00001450
Iteration 76/1000 | Loss: 0.00001450
Iteration 77/1000 | Loss: 0.00001449
Iteration 78/1000 | Loss: 0.00001449
Iteration 79/1000 | Loss: 0.00001448
Iteration 80/1000 | Loss: 0.00001448
Iteration 81/1000 | Loss: 0.00001448
Iteration 82/1000 | Loss: 0.00001448
Iteration 83/1000 | Loss: 0.00001448
Iteration 84/1000 | Loss: 0.00001448
Iteration 85/1000 | Loss: 0.00001447
Iteration 86/1000 | Loss: 0.00001447
Iteration 87/1000 | Loss: 0.00001446
Iteration 88/1000 | Loss: 0.00001446
Iteration 89/1000 | Loss: 0.00001446
Iteration 90/1000 | Loss: 0.00001445
Iteration 91/1000 | Loss: 0.00001445
Iteration 92/1000 | Loss: 0.00001445
Iteration 93/1000 | Loss: 0.00001444
Iteration 94/1000 | Loss: 0.00001444
Iteration 95/1000 | Loss: 0.00001444
Iteration 96/1000 | Loss: 0.00001444
Iteration 97/1000 | Loss: 0.00001444
Iteration 98/1000 | Loss: 0.00001443
Iteration 99/1000 | Loss: 0.00001443
Iteration 100/1000 | Loss: 0.00001442
Iteration 101/1000 | Loss: 0.00001442
Iteration 102/1000 | Loss: 0.00001441
Iteration 103/1000 | Loss: 0.00001441
Iteration 104/1000 | Loss: 0.00001441
Iteration 105/1000 | Loss: 0.00001441
Iteration 106/1000 | Loss: 0.00001441
Iteration 107/1000 | Loss: 0.00001440
Iteration 108/1000 | Loss: 0.00001440
Iteration 109/1000 | Loss: 0.00001440
Iteration 110/1000 | Loss: 0.00001440
Iteration 111/1000 | Loss: 0.00001440
Iteration 112/1000 | Loss: 0.00001440
Iteration 113/1000 | Loss: 0.00001440
Iteration 114/1000 | Loss: 0.00001440
Iteration 115/1000 | Loss: 0.00001440
Iteration 116/1000 | Loss: 0.00001439
Iteration 117/1000 | Loss: 0.00001438
Iteration 118/1000 | Loss: 0.00001438
Iteration 119/1000 | Loss: 0.00001437
Iteration 120/1000 | Loss: 0.00001437
Iteration 121/1000 | Loss: 0.00001437
Iteration 122/1000 | Loss: 0.00001436
Iteration 123/1000 | Loss: 0.00001435
Iteration 124/1000 | Loss: 0.00001435
Iteration 125/1000 | Loss: 0.00001434
Iteration 126/1000 | Loss: 0.00001434
Iteration 127/1000 | Loss: 0.00001434
Iteration 128/1000 | Loss: 0.00001434
Iteration 129/1000 | Loss: 0.00001434
Iteration 130/1000 | Loss: 0.00001434
Iteration 131/1000 | Loss: 0.00001433
Iteration 132/1000 | Loss: 0.00001433
Iteration 133/1000 | Loss: 0.00001433
Iteration 134/1000 | Loss: 0.00001432
Iteration 135/1000 | Loss: 0.00001432
Iteration 136/1000 | Loss: 0.00001432
Iteration 137/1000 | Loss: 0.00001432
Iteration 138/1000 | Loss: 0.00001431
Iteration 139/1000 | Loss: 0.00001431
Iteration 140/1000 | Loss: 0.00001431
Iteration 141/1000 | Loss: 0.00001431
Iteration 142/1000 | Loss: 0.00001430
Iteration 143/1000 | Loss: 0.00001430
Iteration 144/1000 | Loss: 0.00001430
Iteration 145/1000 | Loss: 0.00001429
Iteration 146/1000 | Loss: 0.00001429
Iteration 147/1000 | Loss: 0.00001428
Iteration 148/1000 | Loss: 0.00001428
Iteration 149/1000 | Loss: 0.00001428
Iteration 150/1000 | Loss: 0.00001428
Iteration 151/1000 | Loss: 0.00001427
Iteration 152/1000 | Loss: 0.00001427
Iteration 153/1000 | Loss: 0.00001427
Iteration 154/1000 | Loss: 0.00001426
Iteration 155/1000 | Loss: 0.00001426
Iteration 156/1000 | Loss: 0.00001426
Iteration 157/1000 | Loss: 0.00001426
Iteration 158/1000 | Loss: 0.00001426
Iteration 159/1000 | Loss: 0.00001426
Iteration 160/1000 | Loss: 0.00001425
Iteration 161/1000 | Loss: 0.00001425
Iteration 162/1000 | Loss: 0.00001425
Iteration 163/1000 | Loss: 0.00001425
Iteration 164/1000 | Loss: 0.00001425
Iteration 165/1000 | Loss: 0.00001425
Iteration 166/1000 | Loss: 0.00001425
Iteration 167/1000 | Loss: 0.00001425
Iteration 168/1000 | Loss: 0.00001424
Iteration 169/1000 | Loss: 0.00001424
Iteration 170/1000 | Loss: 0.00001424
Iteration 171/1000 | Loss: 0.00001424
Iteration 172/1000 | Loss: 0.00001424
Iteration 173/1000 | Loss: 0.00001424
Iteration 174/1000 | Loss: 0.00001424
Iteration 175/1000 | Loss: 0.00001424
Iteration 176/1000 | Loss: 0.00001423
Iteration 177/1000 | Loss: 0.00001423
Iteration 178/1000 | Loss: 0.00001423
Iteration 179/1000 | Loss: 0.00001423
Iteration 180/1000 | Loss: 0.00001423
Iteration 181/1000 | Loss: 0.00001423
Iteration 182/1000 | Loss: 0.00001423
Iteration 183/1000 | Loss: 0.00001422
Iteration 184/1000 | Loss: 0.00001422
Iteration 185/1000 | Loss: 0.00001422
Iteration 186/1000 | Loss: 0.00001422
Iteration 187/1000 | Loss: 0.00001422
Iteration 188/1000 | Loss: 0.00001422
Iteration 189/1000 | Loss: 0.00001422
Iteration 190/1000 | Loss: 0.00001422
Iteration 191/1000 | Loss: 0.00001422
Iteration 192/1000 | Loss: 0.00001421
Iteration 193/1000 | Loss: 0.00001421
Iteration 194/1000 | Loss: 0.00001421
Iteration 195/1000 | Loss: 0.00001421
Iteration 196/1000 | Loss: 0.00001421
Iteration 197/1000 | Loss: 0.00001421
Iteration 198/1000 | Loss: 0.00001421
Iteration 199/1000 | Loss: 0.00001421
Iteration 200/1000 | Loss: 0.00001421
Iteration 201/1000 | Loss: 0.00001421
Iteration 202/1000 | Loss: 0.00001421
Iteration 203/1000 | Loss: 0.00001421
Iteration 204/1000 | Loss: 0.00001421
Iteration 205/1000 | Loss: 0.00001421
Iteration 206/1000 | Loss: 0.00001421
Iteration 207/1000 | Loss: 0.00001421
Iteration 208/1000 | Loss: 0.00001421
Iteration 209/1000 | Loss: 0.00001421
Iteration 210/1000 | Loss: 0.00001421
Iteration 211/1000 | Loss: 0.00001421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.4205795196176041e-05, 1.4205795196176041e-05, 1.4205795196176041e-05, 1.4205795196176041e-05, 1.4205795196176041e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4205795196176041e-05

Optimization complete. Final v2v error: 3.1179981231689453 mm

Highest mean error: 3.9402990341186523 mm for frame 120

Lowest mean error: 2.5297372341156006 mm for frame 27

Saving results

Total time: 50.34587049484253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951567
Iteration 2/25 | Loss: 0.00350762
Iteration 3/25 | Loss: 0.00246343
Iteration 4/25 | Loss: 0.00238572
Iteration 5/25 | Loss: 0.00224319
Iteration 6/25 | Loss: 0.00203590
Iteration 7/25 | Loss: 0.00203036
Iteration 8/25 | Loss: 0.00195024
Iteration 9/25 | Loss: 0.00194502
Iteration 10/25 | Loss: 0.00191463
Iteration 11/25 | Loss: 0.00190424
Iteration 12/25 | Loss: 0.00191047
Iteration 13/25 | Loss: 0.00190652
Iteration 14/25 | Loss: 0.00191084
Iteration 15/25 | Loss: 0.00188291
Iteration 16/25 | Loss: 0.00188823
Iteration 17/25 | Loss: 0.00187586
Iteration 18/25 | Loss: 0.00187473
Iteration 19/25 | Loss: 0.00187458
Iteration 20/25 | Loss: 0.00187425
Iteration 21/25 | Loss: 0.00186997
Iteration 22/25 | Loss: 0.00186691
Iteration 23/25 | Loss: 0.00186669
Iteration 24/25 | Loss: 0.00186667
Iteration 25/25 | Loss: 0.00186666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23318660
Iteration 2/25 | Loss: 0.00613630
Iteration 3/25 | Loss: 0.00434145
Iteration 4/25 | Loss: 0.00434145
Iteration 5/25 | Loss: 0.00434144
Iteration 6/25 | Loss: 0.00434144
Iteration 7/25 | Loss: 0.00434144
Iteration 8/25 | Loss: 0.00434144
Iteration 9/25 | Loss: 0.00434144
Iteration 10/25 | Loss: 0.00434144
Iteration 11/25 | Loss: 0.00434144
Iteration 12/25 | Loss: 0.00434144
Iteration 13/25 | Loss: 0.00434144
Iteration 14/25 | Loss: 0.00434144
Iteration 15/25 | Loss: 0.00434144
Iteration 16/25 | Loss: 0.00434144
Iteration 17/25 | Loss: 0.00434144
Iteration 18/25 | Loss: 0.00434144
Iteration 19/25 | Loss: 0.00434144
Iteration 20/25 | Loss: 0.00434144
Iteration 21/25 | Loss: 0.00434144
Iteration 22/25 | Loss: 0.00434144
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.004341442137956619, 0.004341442137956619, 0.004341442137956619, 0.004341442137956619, 0.004341442137956619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004341442137956619

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00434144
Iteration 2/1000 | Loss: 0.00078064
Iteration 3/1000 | Loss: 0.00188689
Iteration 4/1000 | Loss: 0.00054615
Iteration 5/1000 | Loss: 0.00090119
Iteration 6/1000 | Loss: 0.00096766
Iteration 7/1000 | Loss: 0.00034130
Iteration 8/1000 | Loss: 0.00097806
Iteration 9/1000 | Loss: 0.00056222
Iteration 10/1000 | Loss: 0.00181717
Iteration 11/1000 | Loss: 0.00033792
Iteration 12/1000 | Loss: 0.00032492
Iteration 13/1000 | Loss: 0.00039639
Iteration 14/1000 | Loss: 0.00071401
Iteration 15/1000 | Loss: 0.00679531
Iteration 16/1000 | Loss: 0.00541462
Iteration 17/1000 | Loss: 0.00121758
Iteration 18/1000 | Loss: 0.00083820
Iteration 19/1000 | Loss: 0.00071027
Iteration 20/1000 | Loss: 0.00025698
Iteration 21/1000 | Loss: 0.00035171
Iteration 22/1000 | Loss: 0.00031888
Iteration 23/1000 | Loss: 0.00291135
Iteration 24/1000 | Loss: 0.00347337
Iteration 25/1000 | Loss: 0.00043473
Iteration 26/1000 | Loss: 0.00039956
Iteration 27/1000 | Loss: 0.00057961
Iteration 28/1000 | Loss: 0.00166171
Iteration 29/1000 | Loss: 0.00006083
Iteration 30/1000 | Loss: 0.00019370
Iteration 31/1000 | Loss: 0.00017368
Iteration 32/1000 | Loss: 0.00002652
Iteration 33/1000 | Loss: 0.00179809
Iteration 34/1000 | Loss: 0.00056466
Iteration 35/1000 | Loss: 0.00022510
Iteration 36/1000 | Loss: 0.00039617
Iteration 37/1000 | Loss: 0.00046903
Iteration 38/1000 | Loss: 0.00218866
Iteration 39/1000 | Loss: 0.00005560
Iteration 40/1000 | Loss: 0.00005604
Iteration 41/1000 | Loss: 0.00004243
Iteration 42/1000 | Loss: 0.00005049
Iteration 43/1000 | Loss: 0.00049231
Iteration 44/1000 | Loss: 0.00022263
Iteration 45/1000 | Loss: 0.00002089
Iteration 46/1000 | Loss: 0.00002950
Iteration 47/1000 | Loss: 0.00002722
Iteration 48/1000 | Loss: 0.00001451
Iteration 49/1000 | Loss: 0.00003243
Iteration 50/1000 | Loss: 0.00034669
Iteration 51/1000 | Loss: 0.00041228
Iteration 52/1000 | Loss: 0.00003816
Iteration 53/1000 | Loss: 0.00021451
Iteration 54/1000 | Loss: 0.00001689
Iteration 55/1000 | Loss: 0.00002922
Iteration 56/1000 | Loss: 0.00002843
Iteration 57/1000 | Loss: 0.00001333
Iteration 58/1000 | Loss: 0.00018755
Iteration 59/1000 | Loss: 0.00002947
Iteration 60/1000 | Loss: 0.00008184
Iteration 61/1000 | Loss: 0.00001533
Iteration 62/1000 | Loss: 0.00002336
Iteration 63/1000 | Loss: 0.00001298
Iteration 64/1000 | Loss: 0.00003461
Iteration 65/1000 | Loss: 0.00034823
Iteration 66/1000 | Loss: 0.00020467
Iteration 67/1000 | Loss: 0.00017038
Iteration 68/1000 | Loss: 0.00002527
Iteration 69/1000 | Loss: 0.00006458
Iteration 70/1000 | Loss: 0.00002051
Iteration 71/1000 | Loss: 0.00005078
Iteration 72/1000 | Loss: 0.00002614
Iteration 73/1000 | Loss: 0.00004096
Iteration 74/1000 | Loss: 0.00001989
Iteration 75/1000 | Loss: 0.00001450
Iteration 76/1000 | Loss: 0.00001280
Iteration 77/1000 | Loss: 0.00001271
Iteration 78/1000 | Loss: 0.00005716
Iteration 79/1000 | Loss: 0.00002271
Iteration 80/1000 | Loss: 0.00006292
Iteration 81/1000 | Loss: 0.00001763
Iteration 82/1000 | Loss: 0.00002023
Iteration 83/1000 | Loss: 0.00001273
Iteration 84/1000 | Loss: 0.00001268
Iteration 85/1000 | Loss: 0.00004847
Iteration 86/1000 | Loss: 0.00002583
Iteration 87/1000 | Loss: 0.00001327
Iteration 88/1000 | Loss: 0.00001791
Iteration 89/1000 | Loss: 0.00004832
Iteration 90/1000 | Loss: 0.00001647
Iteration 91/1000 | Loss: 0.00001258
Iteration 92/1000 | Loss: 0.00001258
Iteration 93/1000 | Loss: 0.00001258
Iteration 94/1000 | Loss: 0.00001258
Iteration 95/1000 | Loss: 0.00001258
Iteration 96/1000 | Loss: 0.00001258
Iteration 97/1000 | Loss: 0.00001258
Iteration 98/1000 | Loss: 0.00001257
Iteration 99/1000 | Loss: 0.00001257
Iteration 100/1000 | Loss: 0.00001257
Iteration 101/1000 | Loss: 0.00001257
Iteration 102/1000 | Loss: 0.00001256
Iteration 103/1000 | Loss: 0.00001256
Iteration 104/1000 | Loss: 0.00001256
Iteration 105/1000 | Loss: 0.00001256
Iteration 106/1000 | Loss: 0.00001256
Iteration 107/1000 | Loss: 0.00001256
Iteration 108/1000 | Loss: 0.00001256
Iteration 109/1000 | Loss: 0.00001256
Iteration 110/1000 | Loss: 0.00001256
Iteration 111/1000 | Loss: 0.00001256
Iteration 112/1000 | Loss: 0.00001256
Iteration 113/1000 | Loss: 0.00001256
Iteration 114/1000 | Loss: 0.00001256
Iteration 115/1000 | Loss: 0.00001256
Iteration 116/1000 | Loss: 0.00001256
Iteration 117/1000 | Loss: 0.00001256
Iteration 118/1000 | Loss: 0.00001256
Iteration 119/1000 | Loss: 0.00001256
Iteration 120/1000 | Loss: 0.00001256
Iteration 121/1000 | Loss: 0.00001256
Iteration 122/1000 | Loss: 0.00001256
Iteration 123/1000 | Loss: 0.00001256
Iteration 124/1000 | Loss: 0.00001256
Iteration 125/1000 | Loss: 0.00001256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.2557626178022474e-05, 1.2557626178022474e-05, 1.2557626178022474e-05, 1.2557626178022474e-05, 1.2557626178022474e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2557626178022474e-05

Optimization complete. Final v2v error: 3.0627057552337646 mm

Highest mean error: 3.484597682952881 mm for frame 112

Lowest mean error: 2.8087847232818604 mm for frame 46

Saving results

Total time: 165.88008427619934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465435
Iteration 2/25 | Loss: 0.00138144
Iteration 3/25 | Loss: 0.00123608
Iteration 4/25 | Loss: 0.00122326
Iteration 5/25 | Loss: 0.00122130
Iteration 6/25 | Loss: 0.00122111
Iteration 7/25 | Loss: 0.00122111
Iteration 8/25 | Loss: 0.00122111
Iteration 9/25 | Loss: 0.00122111
Iteration 10/25 | Loss: 0.00122111
Iteration 11/25 | Loss: 0.00122111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00122111476957798, 0.00122111476957798, 0.00122111476957798, 0.00122111476957798, 0.00122111476957798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00122111476957798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28953862
Iteration 2/25 | Loss: 0.00130683
Iteration 3/25 | Loss: 0.00130683
Iteration 4/25 | Loss: 0.00130683
Iteration 5/25 | Loss: 0.00130683
Iteration 6/25 | Loss: 0.00130683
Iteration 7/25 | Loss: 0.00130683
Iteration 8/25 | Loss: 0.00130682
Iteration 9/25 | Loss: 0.00130682
Iteration 10/25 | Loss: 0.00130682
Iteration 11/25 | Loss: 0.00130682
Iteration 12/25 | Loss: 0.00130682
Iteration 13/25 | Loss: 0.00130682
Iteration 14/25 | Loss: 0.00130682
Iteration 15/25 | Loss: 0.00130682
Iteration 16/25 | Loss: 0.00130682
Iteration 17/25 | Loss: 0.00130682
Iteration 18/25 | Loss: 0.00130682
Iteration 19/25 | Loss: 0.00130682
Iteration 20/25 | Loss: 0.00130682
Iteration 21/25 | Loss: 0.00130682
Iteration 22/25 | Loss: 0.00130682
Iteration 23/25 | Loss: 0.00130682
Iteration 24/25 | Loss: 0.00130682
Iteration 25/25 | Loss: 0.00130682

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130682
Iteration 2/1000 | Loss: 0.00002831
Iteration 3/1000 | Loss: 0.00001880
Iteration 4/1000 | Loss: 0.00001531
Iteration 5/1000 | Loss: 0.00001421
Iteration 6/1000 | Loss: 0.00001337
Iteration 7/1000 | Loss: 0.00001296
Iteration 8/1000 | Loss: 0.00001244
Iteration 9/1000 | Loss: 0.00001203
Iteration 10/1000 | Loss: 0.00001172
Iteration 11/1000 | Loss: 0.00001142
Iteration 12/1000 | Loss: 0.00001129
Iteration 13/1000 | Loss: 0.00001119
Iteration 14/1000 | Loss: 0.00001110
Iteration 15/1000 | Loss: 0.00001105
Iteration 16/1000 | Loss: 0.00001104
Iteration 17/1000 | Loss: 0.00001104
Iteration 18/1000 | Loss: 0.00001103
Iteration 19/1000 | Loss: 0.00001101
Iteration 20/1000 | Loss: 0.00001100
Iteration 21/1000 | Loss: 0.00001099
Iteration 22/1000 | Loss: 0.00001098
Iteration 23/1000 | Loss: 0.00001097
Iteration 24/1000 | Loss: 0.00001096
Iteration 25/1000 | Loss: 0.00001096
Iteration 26/1000 | Loss: 0.00001096
Iteration 27/1000 | Loss: 0.00001095
Iteration 28/1000 | Loss: 0.00001095
Iteration 29/1000 | Loss: 0.00001094
Iteration 30/1000 | Loss: 0.00001094
Iteration 31/1000 | Loss: 0.00001093
Iteration 32/1000 | Loss: 0.00001093
Iteration 33/1000 | Loss: 0.00001092
Iteration 34/1000 | Loss: 0.00001091
Iteration 35/1000 | Loss: 0.00001091
Iteration 36/1000 | Loss: 0.00001090
Iteration 37/1000 | Loss: 0.00001089
Iteration 38/1000 | Loss: 0.00001089
Iteration 39/1000 | Loss: 0.00001088
Iteration 40/1000 | Loss: 0.00001087
Iteration 41/1000 | Loss: 0.00001086
Iteration 42/1000 | Loss: 0.00001085
Iteration 43/1000 | Loss: 0.00001083
Iteration 44/1000 | Loss: 0.00001083
Iteration 45/1000 | Loss: 0.00001082
Iteration 46/1000 | Loss: 0.00001079
Iteration 47/1000 | Loss: 0.00001076
Iteration 48/1000 | Loss: 0.00001075
Iteration 49/1000 | Loss: 0.00001075
Iteration 50/1000 | Loss: 0.00001074
Iteration 51/1000 | Loss: 0.00001074
Iteration 52/1000 | Loss: 0.00001072
Iteration 53/1000 | Loss: 0.00001071
Iteration 54/1000 | Loss: 0.00001071
Iteration 55/1000 | Loss: 0.00001071
Iteration 56/1000 | Loss: 0.00001070
Iteration 57/1000 | Loss: 0.00001070
Iteration 58/1000 | Loss: 0.00001070
Iteration 59/1000 | Loss: 0.00001069
Iteration 60/1000 | Loss: 0.00001069
Iteration 61/1000 | Loss: 0.00001069
Iteration 62/1000 | Loss: 0.00001068
Iteration 63/1000 | Loss: 0.00001068
Iteration 64/1000 | Loss: 0.00001068
Iteration 65/1000 | Loss: 0.00001068
Iteration 66/1000 | Loss: 0.00001067
Iteration 67/1000 | Loss: 0.00001067
Iteration 68/1000 | Loss: 0.00001066
Iteration 69/1000 | Loss: 0.00001066
Iteration 70/1000 | Loss: 0.00001065
Iteration 71/1000 | Loss: 0.00001065
Iteration 72/1000 | Loss: 0.00001065
Iteration 73/1000 | Loss: 0.00001065
Iteration 74/1000 | Loss: 0.00001064
Iteration 75/1000 | Loss: 0.00001064
Iteration 76/1000 | Loss: 0.00001063
Iteration 77/1000 | Loss: 0.00001063
Iteration 78/1000 | Loss: 0.00001063
Iteration 79/1000 | Loss: 0.00001062
Iteration 80/1000 | Loss: 0.00001062
Iteration 81/1000 | Loss: 0.00001062
Iteration 82/1000 | Loss: 0.00001061
Iteration 83/1000 | Loss: 0.00001061
Iteration 84/1000 | Loss: 0.00001061
Iteration 85/1000 | Loss: 0.00001060
Iteration 86/1000 | Loss: 0.00001060
Iteration 87/1000 | Loss: 0.00001060
Iteration 88/1000 | Loss: 0.00001059
Iteration 89/1000 | Loss: 0.00001059
Iteration 90/1000 | Loss: 0.00001059
Iteration 91/1000 | Loss: 0.00001058
Iteration 92/1000 | Loss: 0.00001058
Iteration 93/1000 | Loss: 0.00001058
Iteration 94/1000 | Loss: 0.00001058
Iteration 95/1000 | Loss: 0.00001058
Iteration 96/1000 | Loss: 0.00001057
Iteration 97/1000 | Loss: 0.00001057
Iteration 98/1000 | Loss: 0.00001056
Iteration 99/1000 | Loss: 0.00001056
Iteration 100/1000 | Loss: 0.00001056
Iteration 101/1000 | Loss: 0.00001056
Iteration 102/1000 | Loss: 0.00001055
Iteration 103/1000 | Loss: 0.00001055
Iteration 104/1000 | Loss: 0.00001055
Iteration 105/1000 | Loss: 0.00001055
Iteration 106/1000 | Loss: 0.00001055
Iteration 107/1000 | Loss: 0.00001054
Iteration 108/1000 | Loss: 0.00001054
Iteration 109/1000 | Loss: 0.00001054
Iteration 110/1000 | Loss: 0.00001054
Iteration 111/1000 | Loss: 0.00001053
Iteration 112/1000 | Loss: 0.00001053
Iteration 113/1000 | Loss: 0.00001053
Iteration 114/1000 | Loss: 0.00001052
Iteration 115/1000 | Loss: 0.00001052
Iteration 116/1000 | Loss: 0.00001052
Iteration 117/1000 | Loss: 0.00001052
Iteration 118/1000 | Loss: 0.00001052
Iteration 119/1000 | Loss: 0.00001051
Iteration 120/1000 | Loss: 0.00001051
Iteration 121/1000 | Loss: 0.00001051
Iteration 122/1000 | Loss: 0.00001051
Iteration 123/1000 | Loss: 0.00001050
Iteration 124/1000 | Loss: 0.00001050
Iteration 125/1000 | Loss: 0.00001050
Iteration 126/1000 | Loss: 0.00001050
Iteration 127/1000 | Loss: 0.00001050
Iteration 128/1000 | Loss: 0.00001050
Iteration 129/1000 | Loss: 0.00001049
Iteration 130/1000 | Loss: 0.00001049
Iteration 131/1000 | Loss: 0.00001049
Iteration 132/1000 | Loss: 0.00001048
Iteration 133/1000 | Loss: 0.00001048
Iteration 134/1000 | Loss: 0.00001048
Iteration 135/1000 | Loss: 0.00001048
Iteration 136/1000 | Loss: 0.00001048
Iteration 137/1000 | Loss: 0.00001048
Iteration 138/1000 | Loss: 0.00001048
Iteration 139/1000 | Loss: 0.00001048
Iteration 140/1000 | Loss: 0.00001048
Iteration 141/1000 | Loss: 0.00001048
Iteration 142/1000 | Loss: 0.00001047
Iteration 143/1000 | Loss: 0.00001047
Iteration 144/1000 | Loss: 0.00001047
Iteration 145/1000 | Loss: 0.00001047
Iteration 146/1000 | Loss: 0.00001047
Iteration 147/1000 | Loss: 0.00001047
Iteration 148/1000 | Loss: 0.00001047
Iteration 149/1000 | Loss: 0.00001047
Iteration 150/1000 | Loss: 0.00001047
Iteration 151/1000 | Loss: 0.00001047
Iteration 152/1000 | Loss: 0.00001046
Iteration 153/1000 | Loss: 0.00001046
Iteration 154/1000 | Loss: 0.00001046
Iteration 155/1000 | Loss: 0.00001046
Iteration 156/1000 | Loss: 0.00001046
Iteration 157/1000 | Loss: 0.00001046
Iteration 158/1000 | Loss: 0.00001045
Iteration 159/1000 | Loss: 0.00001045
Iteration 160/1000 | Loss: 0.00001045
Iteration 161/1000 | Loss: 0.00001044
Iteration 162/1000 | Loss: 0.00001044
Iteration 163/1000 | Loss: 0.00001044
Iteration 164/1000 | Loss: 0.00001044
Iteration 165/1000 | Loss: 0.00001044
Iteration 166/1000 | Loss: 0.00001044
Iteration 167/1000 | Loss: 0.00001044
Iteration 168/1000 | Loss: 0.00001043
Iteration 169/1000 | Loss: 0.00001043
Iteration 170/1000 | Loss: 0.00001043
Iteration 171/1000 | Loss: 0.00001043
Iteration 172/1000 | Loss: 0.00001043
Iteration 173/1000 | Loss: 0.00001042
Iteration 174/1000 | Loss: 0.00001042
Iteration 175/1000 | Loss: 0.00001042
Iteration 176/1000 | Loss: 0.00001041
Iteration 177/1000 | Loss: 0.00001041
Iteration 178/1000 | Loss: 0.00001041
Iteration 179/1000 | Loss: 0.00001041
Iteration 180/1000 | Loss: 0.00001041
Iteration 181/1000 | Loss: 0.00001041
Iteration 182/1000 | Loss: 0.00001041
Iteration 183/1000 | Loss: 0.00001041
Iteration 184/1000 | Loss: 0.00001040
Iteration 185/1000 | Loss: 0.00001040
Iteration 186/1000 | Loss: 0.00001040
Iteration 187/1000 | Loss: 0.00001040
Iteration 188/1000 | Loss: 0.00001039
Iteration 189/1000 | Loss: 0.00001039
Iteration 190/1000 | Loss: 0.00001039
Iteration 191/1000 | Loss: 0.00001039
Iteration 192/1000 | Loss: 0.00001038
Iteration 193/1000 | Loss: 0.00001038
Iteration 194/1000 | Loss: 0.00001038
Iteration 195/1000 | Loss: 0.00001038
Iteration 196/1000 | Loss: 0.00001038
Iteration 197/1000 | Loss: 0.00001038
Iteration 198/1000 | Loss: 0.00001038
Iteration 199/1000 | Loss: 0.00001038
Iteration 200/1000 | Loss: 0.00001038
Iteration 201/1000 | Loss: 0.00001038
Iteration 202/1000 | Loss: 0.00001038
Iteration 203/1000 | Loss: 0.00001038
Iteration 204/1000 | Loss: 0.00001038
Iteration 205/1000 | Loss: 0.00001038
Iteration 206/1000 | Loss: 0.00001038
Iteration 207/1000 | Loss: 0.00001038
Iteration 208/1000 | Loss: 0.00001038
Iteration 209/1000 | Loss: 0.00001038
Iteration 210/1000 | Loss: 0.00001038
Iteration 211/1000 | Loss: 0.00001038
Iteration 212/1000 | Loss: 0.00001038
Iteration 213/1000 | Loss: 0.00001038
Iteration 214/1000 | Loss: 0.00001038
Iteration 215/1000 | Loss: 0.00001038
Iteration 216/1000 | Loss: 0.00001038
Iteration 217/1000 | Loss: 0.00001038
Iteration 218/1000 | Loss: 0.00001038
Iteration 219/1000 | Loss: 0.00001038
Iteration 220/1000 | Loss: 0.00001038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.0378976185165811e-05, 1.0378976185165811e-05, 1.0378976185165811e-05, 1.0378976185165811e-05, 1.0378976185165811e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0378976185165811e-05

Optimization complete. Final v2v error: 2.720423936843872 mm

Highest mean error: 3.407848358154297 mm for frame 83

Lowest mean error: 2.454324245452881 mm for frame 160

Saving results

Total time: 45.81143093109131
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00542767
Iteration 2/25 | Loss: 0.00125280
Iteration 3/25 | Loss: 0.00119182
Iteration 4/25 | Loss: 0.00118397
Iteration 5/25 | Loss: 0.00118155
Iteration 6/25 | Loss: 0.00118107
Iteration 7/25 | Loss: 0.00118107
Iteration 8/25 | Loss: 0.00118107
Iteration 9/25 | Loss: 0.00118107
Iteration 10/25 | Loss: 0.00118107
Iteration 11/25 | Loss: 0.00118107
Iteration 12/25 | Loss: 0.00118107
Iteration 13/25 | Loss: 0.00118107
Iteration 14/25 | Loss: 0.00118107
Iteration 15/25 | Loss: 0.00118107
Iteration 16/25 | Loss: 0.00118107
Iteration 17/25 | Loss: 0.00118107
Iteration 18/25 | Loss: 0.00118107
Iteration 19/25 | Loss: 0.00118107
Iteration 20/25 | Loss: 0.00118107
Iteration 21/25 | Loss: 0.00118107
Iteration 22/25 | Loss: 0.00118107
Iteration 23/25 | Loss: 0.00118107
Iteration 24/25 | Loss: 0.00118107
Iteration 25/25 | Loss: 0.00118107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.12676549
Iteration 2/25 | Loss: 0.00142933
Iteration 3/25 | Loss: 0.00142932
Iteration 4/25 | Loss: 0.00142932
Iteration 5/25 | Loss: 0.00142932
Iteration 6/25 | Loss: 0.00142932
Iteration 7/25 | Loss: 0.00142932
Iteration 8/25 | Loss: 0.00142932
Iteration 9/25 | Loss: 0.00142932
Iteration 10/25 | Loss: 0.00142932
Iteration 11/25 | Loss: 0.00142932
Iteration 12/25 | Loss: 0.00142932
Iteration 13/25 | Loss: 0.00142932
Iteration 14/25 | Loss: 0.00142932
Iteration 15/25 | Loss: 0.00142932
Iteration 16/25 | Loss: 0.00142932
Iteration 17/25 | Loss: 0.00142932
Iteration 18/25 | Loss: 0.00142932
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014293212443590164, 0.0014293212443590164, 0.0014293212443590164, 0.0014293212443590164, 0.0014293212443590164]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014293212443590164

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142932
Iteration 2/1000 | Loss: 0.00001887
Iteration 3/1000 | Loss: 0.00001385
Iteration 4/1000 | Loss: 0.00001237
Iteration 5/1000 | Loss: 0.00001164
Iteration 6/1000 | Loss: 0.00001107
Iteration 7/1000 | Loss: 0.00001070
Iteration 8/1000 | Loss: 0.00001048
Iteration 9/1000 | Loss: 0.00001019
Iteration 10/1000 | Loss: 0.00001005
Iteration 11/1000 | Loss: 0.00000991
Iteration 12/1000 | Loss: 0.00000980
Iteration 13/1000 | Loss: 0.00000971
Iteration 14/1000 | Loss: 0.00000969
Iteration 15/1000 | Loss: 0.00000966
Iteration 16/1000 | Loss: 0.00000961
Iteration 17/1000 | Loss: 0.00000960
Iteration 18/1000 | Loss: 0.00000958
Iteration 19/1000 | Loss: 0.00000958
Iteration 20/1000 | Loss: 0.00000955
Iteration 21/1000 | Loss: 0.00000954
Iteration 22/1000 | Loss: 0.00000954
Iteration 23/1000 | Loss: 0.00000954
Iteration 24/1000 | Loss: 0.00000952
Iteration 25/1000 | Loss: 0.00000952
Iteration 26/1000 | Loss: 0.00000952
Iteration 27/1000 | Loss: 0.00000947
Iteration 28/1000 | Loss: 0.00000946
Iteration 29/1000 | Loss: 0.00000945
Iteration 30/1000 | Loss: 0.00000945
Iteration 31/1000 | Loss: 0.00000944
Iteration 32/1000 | Loss: 0.00000943
Iteration 33/1000 | Loss: 0.00000942
Iteration 34/1000 | Loss: 0.00000941
Iteration 35/1000 | Loss: 0.00000941
Iteration 36/1000 | Loss: 0.00000940
Iteration 37/1000 | Loss: 0.00000939
Iteration 38/1000 | Loss: 0.00000938
Iteration 39/1000 | Loss: 0.00000938
Iteration 40/1000 | Loss: 0.00000938
Iteration 41/1000 | Loss: 0.00000938
Iteration 42/1000 | Loss: 0.00000938
Iteration 43/1000 | Loss: 0.00000937
Iteration 44/1000 | Loss: 0.00000937
Iteration 45/1000 | Loss: 0.00000936
Iteration 46/1000 | Loss: 0.00000936
Iteration 47/1000 | Loss: 0.00000933
Iteration 48/1000 | Loss: 0.00000933
Iteration 49/1000 | Loss: 0.00000932
Iteration 50/1000 | Loss: 0.00000931
Iteration 51/1000 | Loss: 0.00000931
Iteration 52/1000 | Loss: 0.00000928
Iteration 53/1000 | Loss: 0.00000927
Iteration 54/1000 | Loss: 0.00000927
Iteration 55/1000 | Loss: 0.00000927
Iteration 56/1000 | Loss: 0.00000927
Iteration 57/1000 | Loss: 0.00000927
Iteration 58/1000 | Loss: 0.00000927
Iteration 59/1000 | Loss: 0.00000927
Iteration 60/1000 | Loss: 0.00000927
Iteration 61/1000 | Loss: 0.00000926
Iteration 62/1000 | Loss: 0.00000926
Iteration 63/1000 | Loss: 0.00000926
Iteration 64/1000 | Loss: 0.00000926
Iteration 65/1000 | Loss: 0.00000926
Iteration 66/1000 | Loss: 0.00000926
Iteration 67/1000 | Loss: 0.00000926
Iteration 68/1000 | Loss: 0.00000925
Iteration 69/1000 | Loss: 0.00000925
Iteration 70/1000 | Loss: 0.00000925
Iteration 71/1000 | Loss: 0.00000925
Iteration 72/1000 | Loss: 0.00000925
Iteration 73/1000 | Loss: 0.00000925
Iteration 74/1000 | Loss: 0.00000924
Iteration 75/1000 | Loss: 0.00000924
Iteration 76/1000 | Loss: 0.00000924
Iteration 77/1000 | Loss: 0.00000923
Iteration 78/1000 | Loss: 0.00000923
Iteration 79/1000 | Loss: 0.00000922
Iteration 80/1000 | Loss: 0.00000922
Iteration 81/1000 | Loss: 0.00000922
Iteration 82/1000 | Loss: 0.00000922
Iteration 83/1000 | Loss: 0.00000922
Iteration 84/1000 | Loss: 0.00000922
Iteration 85/1000 | Loss: 0.00000922
Iteration 86/1000 | Loss: 0.00000921
Iteration 87/1000 | Loss: 0.00000921
Iteration 88/1000 | Loss: 0.00000921
Iteration 89/1000 | Loss: 0.00000921
Iteration 90/1000 | Loss: 0.00000921
Iteration 91/1000 | Loss: 0.00000921
Iteration 92/1000 | Loss: 0.00000921
Iteration 93/1000 | Loss: 0.00000921
Iteration 94/1000 | Loss: 0.00000920
Iteration 95/1000 | Loss: 0.00000920
Iteration 96/1000 | Loss: 0.00000920
Iteration 97/1000 | Loss: 0.00000920
Iteration 98/1000 | Loss: 0.00000920
Iteration 99/1000 | Loss: 0.00000919
Iteration 100/1000 | Loss: 0.00000919
Iteration 101/1000 | Loss: 0.00000919
Iteration 102/1000 | Loss: 0.00000919
Iteration 103/1000 | Loss: 0.00000918
Iteration 104/1000 | Loss: 0.00000918
Iteration 105/1000 | Loss: 0.00000918
Iteration 106/1000 | Loss: 0.00000918
Iteration 107/1000 | Loss: 0.00000918
Iteration 108/1000 | Loss: 0.00000918
Iteration 109/1000 | Loss: 0.00000918
Iteration 110/1000 | Loss: 0.00000917
Iteration 111/1000 | Loss: 0.00000917
Iteration 112/1000 | Loss: 0.00000917
Iteration 113/1000 | Loss: 0.00000916
Iteration 114/1000 | Loss: 0.00000916
Iteration 115/1000 | Loss: 0.00000916
Iteration 116/1000 | Loss: 0.00000916
Iteration 117/1000 | Loss: 0.00000916
Iteration 118/1000 | Loss: 0.00000916
Iteration 119/1000 | Loss: 0.00000916
Iteration 120/1000 | Loss: 0.00000916
Iteration 121/1000 | Loss: 0.00000916
Iteration 122/1000 | Loss: 0.00000916
Iteration 123/1000 | Loss: 0.00000915
Iteration 124/1000 | Loss: 0.00000915
Iteration 125/1000 | Loss: 0.00000915
Iteration 126/1000 | Loss: 0.00000915
Iteration 127/1000 | Loss: 0.00000915
Iteration 128/1000 | Loss: 0.00000915
Iteration 129/1000 | Loss: 0.00000914
Iteration 130/1000 | Loss: 0.00000914
Iteration 131/1000 | Loss: 0.00000914
Iteration 132/1000 | Loss: 0.00000914
Iteration 133/1000 | Loss: 0.00000914
Iteration 134/1000 | Loss: 0.00000914
Iteration 135/1000 | Loss: 0.00000914
Iteration 136/1000 | Loss: 0.00000914
Iteration 137/1000 | Loss: 0.00000913
Iteration 138/1000 | Loss: 0.00000913
Iteration 139/1000 | Loss: 0.00000913
Iteration 140/1000 | Loss: 0.00000913
Iteration 141/1000 | Loss: 0.00000913
Iteration 142/1000 | Loss: 0.00000913
Iteration 143/1000 | Loss: 0.00000913
Iteration 144/1000 | Loss: 0.00000913
Iteration 145/1000 | Loss: 0.00000913
Iteration 146/1000 | Loss: 0.00000913
Iteration 147/1000 | Loss: 0.00000913
Iteration 148/1000 | Loss: 0.00000912
Iteration 149/1000 | Loss: 0.00000912
Iteration 150/1000 | Loss: 0.00000912
Iteration 151/1000 | Loss: 0.00000912
Iteration 152/1000 | Loss: 0.00000912
Iteration 153/1000 | Loss: 0.00000912
Iteration 154/1000 | Loss: 0.00000911
Iteration 155/1000 | Loss: 0.00000911
Iteration 156/1000 | Loss: 0.00000911
Iteration 157/1000 | Loss: 0.00000911
Iteration 158/1000 | Loss: 0.00000911
Iteration 159/1000 | Loss: 0.00000911
Iteration 160/1000 | Loss: 0.00000911
Iteration 161/1000 | Loss: 0.00000911
Iteration 162/1000 | Loss: 0.00000911
Iteration 163/1000 | Loss: 0.00000911
Iteration 164/1000 | Loss: 0.00000911
Iteration 165/1000 | Loss: 0.00000910
Iteration 166/1000 | Loss: 0.00000910
Iteration 167/1000 | Loss: 0.00000910
Iteration 168/1000 | Loss: 0.00000910
Iteration 169/1000 | Loss: 0.00000910
Iteration 170/1000 | Loss: 0.00000910
Iteration 171/1000 | Loss: 0.00000910
Iteration 172/1000 | Loss: 0.00000910
Iteration 173/1000 | Loss: 0.00000910
Iteration 174/1000 | Loss: 0.00000910
Iteration 175/1000 | Loss: 0.00000910
Iteration 176/1000 | Loss: 0.00000910
Iteration 177/1000 | Loss: 0.00000910
Iteration 178/1000 | Loss: 0.00000910
Iteration 179/1000 | Loss: 0.00000910
Iteration 180/1000 | Loss: 0.00000910
Iteration 181/1000 | Loss: 0.00000910
Iteration 182/1000 | Loss: 0.00000910
Iteration 183/1000 | Loss: 0.00000910
Iteration 184/1000 | Loss: 0.00000910
Iteration 185/1000 | Loss: 0.00000909
Iteration 186/1000 | Loss: 0.00000909
Iteration 187/1000 | Loss: 0.00000909
Iteration 188/1000 | Loss: 0.00000909
Iteration 189/1000 | Loss: 0.00000909
Iteration 190/1000 | Loss: 0.00000909
Iteration 191/1000 | Loss: 0.00000909
Iteration 192/1000 | Loss: 0.00000909
Iteration 193/1000 | Loss: 0.00000909
Iteration 194/1000 | Loss: 0.00000909
Iteration 195/1000 | Loss: 0.00000909
Iteration 196/1000 | Loss: 0.00000909
Iteration 197/1000 | Loss: 0.00000909
Iteration 198/1000 | Loss: 0.00000909
Iteration 199/1000 | Loss: 0.00000908
Iteration 200/1000 | Loss: 0.00000908
Iteration 201/1000 | Loss: 0.00000908
Iteration 202/1000 | Loss: 0.00000908
Iteration 203/1000 | Loss: 0.00000908
Iteration 204/1000 | Loss: 0.00000908
Iteration 205/1000 | Loss: 0.00000908
Iteration 206/1000 | Loss: 0.00000908
Iteration 207/1000 | Loss: 0.00000907
Iteration 208/1000 | Loss: 0.00000907
Iteration 209/1000 | Loss: 0.00000907
Iteration 210/1000 | Loss: 0.00000907
Iteration 211/1000 | Loss: 0.00000907
Iteration 212/1000 | Loss: 0.00000907
Iteration 213/1000 | Loss: 0.00000907
Iteration 214/1000 | Loss: 0.00000907
Iteration 215/1000 | Loss: 0.00000907
Iteration 216/1000 | Loss: 0.00000906
Iteration 217/1000 | Loss: 0.00000906
Iteration 218/1000 | Loss: 0.00000906
Iteration 219/1000 | Loss: 0.00000906
Iteration 220/1000 | Loss: 0.00000905
Iteration 221/1000 | Loss: 0.00000905
Iteration 222/1000 | Loss: 0.00000905
Iteration 223/1000 | Loss: 0.00000905
Iteration 224/1000 | Loss: 0.00000905
Iteration 225/1000 | Loss: 0.00000905
Iteration 226/1000 | Loss: 0.00000905
Iteration 227/1000 | Loss: 0.00000905
Iteration 228/1000 | Loss: 0.00000905
Iteration 229/1000 | Loss: 0.00000904
Iteration 230/1000 | Loss: 0.00000904
Iteration 231/1000 | Loss: 0.00000904
Iteration 232/1000 | Loss: 0.00000904
Iteration 233/1000 | Loss: 0.00000904
Iteration 234/1000 | Loss: 0.00000904
Iteration 235/1000 | Loss: 0.00000904
Iteration 236/1000 | Loss: 0.00000904
Iteration 237/1000 | Loss: 0.00000904
Iteration 238/1000 | Loss: 0.00000904
Iteration 239/1000 | Loss: 0.00000904
Iteration 240/1000 | Loss: 0.00000904
Iteration 241/1000 | Loss: 0.00000904
Iteration 242/1000 | Loss: 0.00000904
Iteration 243/1000 | Loss: 0.00000903
Iteration 244/1000 | Loss: 0.00000903
Iteration 245/1000 | Loss: 0.00000903
Iteration 246/1000 | Loss: 0.00000903
Iteration 247/1000 | Loss: 0.00000903
Iteration 248/1000 | Loss: 0.00000902
Iteration 249/1000 | Loss: 0.00000902
Iteration 250/1000 | Loss: 0.00000902
Iteration 251/1000 | Loss: 0.00000902
Iteration 252/1000 | Loss: 0.00000902
Iteration 253/1000 | Loss: 0.00000902
Iteration 254/1000 | Loss: 0.00000902
Iteration 255/1000 | Loss: 0.00000902
Iteration 256/1000 | Loss: 0.00000902
Iteration 257/1000 | Loss: 0.00000902
Iteration 258/1000 | Loss: 0.00000902
Iteration 259/1000 | Loss: 0.00000902
Iteration 260/1000 | Loss: 0.00000902
Iteration 261/1000 | Loss: 0.00000902
Iteration 262/1000 | Loss: 0.00000902
Iteration 263/1000 | Loss: 0.00000901
Iteration 264/1000 | Loss: 0.00000901
Iteration 265/1000 | Loss: 0.00000901
Iteration 266/1000 | Loss: 0.00000901
Iteration 267/1000 | Loss: 0.00000901
Iteration 268/1000 | Loss: 0.00000901
Iteration 269/1000 | Loss: 0.00000901
Iteration 270/1000 | Loss: 0.00000901
Iteration 271/1000 | Loss: 0.00000901
Iteration 272/1000 | Loss: 0.00000901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [9.013085218612105e-06, 9.013085218612105e-06, 9.013085218612105e-06, 9.013085218612105e-06, 9.013085218612105e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.013085218612105e-06

Optimization complete. Final v2v error: 2.6018424034118652 mm

Highest mean error: 2.801239490509033 mm for frame 75

Lowest mean error: 2.502225160598755 mm for frame 138

Saving results

Total time: 46.24734377861023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862618
Iteration 2/25 | Loss: 0.00172976
Iteration 3/25 | Loss: 0.00140234
Iteration 4/25 | Loss: 0.00135972
Iteration 5/25 | Loss: 0.00135342
Iteration 6/25 | Loss: 0.00135235
Iteration 7/25 | Loss: 0.00134219
Iteration 8/25 | Loss: 0.00133212
Iteration 9/25 | Loss: 0.00133065
Iteration 10/25 | Loss: 0.00132516
Iteration 11/25 | Loss: 0.00131913
Iteration 12/25 | Loss: 0.00131330
Iteration 13/25 | Loss: 0.00130688
Iteration 14/25 | Loss: 0.00129691
Iteration 15/25 | Loss: 0.00129383
Iteration 16/25 | Loss: 0.00129360
Iteration 17/25 | Loss: 0.00129231
Iteration 18/25 | Loss: 0.00128921
Iteration 19/25 | Loss: 0.00128786
Iteration 20/25 | Loss: 0.00128746
Iteration 21/25 | Loss: 0.00128736
Iteration 22/25 | Loss: 0.00128736
Iteration 23/25 | Loss: 0.00128736
Iteration 24/25 | Loss: 0.00128736
Iteration 25/25 | Loss: 0.00128736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86246872
Iteration 2/25 | Loss: 0.00082830
Iteration 3/25 | Loss: 0.00082829
Iteration 4/25 | Loss: 0.00082829
Iteration 5/25 | Loss: 0.00082829
Iteration 6/25 | Loss: 0.00082829
Iteration 7/25 | Loss: 0.00082829
Iteration 8/25 | Loss: 0.00082829
Iteration 9/25 | Loss: 0.00082829
Iteration 10/25 | Loss: 0.00082829
Iteration 11/25 | Loss: 0.00082829
Iteration 12/25 | Loss: 0.00082829
Iteration 13/25 | Loss: 0.00082829
Iteration 14/25 | Loss: 0.00082829
Iteration 15/25 | Loss: 0.00082829
Iteration 16/25 | Loss: 0.00082829
Iteration 17/25 | Loss: 0.00082829
Iteration 18/25 | Loss: 0.00082829
Iteration 19/25 | Loss: 0.00082829
Iteration 20/25 | Loss: 0.00082829
Iteration 21/25 | Loss: 0.00082829
Iteration 22/25 | Loss: 0.00082829
Iteration 23/25 | Loss: 0.00082829
Iteration 24/25 | Loss: 0.00082829
Iteration 25/25 | Loss: 0.00082829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082829
Iteration 2/1000 | Loss: 0.00004363
Iteration 3/1000 | Loss: 0.00003445
Iteration 4/1000 | Loss: 0.00003194
Iteration 5/1000 | Loss: 0.00003064
Iteration 6/1000 | Loss: 0.00002973
Iteration 7/1000 | Loss: 0.00002889
Iteration 8/1000 | Loss: 0.00002858
Iteration 9/1000 | Loss: 0.00002807
Iteration 10/1000 | Loss: 0.00002778
Iteration 11/1000 | Loss: 0.00002756
Iteration 12/1000 | Loss: 0.00002752
Iteration 13/1000 | Loss: 0.00002737
Iteration 14/1000 | Loss: 0.00002728
Iteration 15/1000 | Loss: 0.00002726
Iteration 16/1000 | Loss: 0.00002725
Iteration 17/1000 | Loss: 0.00002725
Iteration 18/1000 | Loss: 0.00002725
Iteration 19/1000 | Loss: 0.00002725
Iteration 20/1000 | Loss: 0.00002719
Iteration 21/1000 | Loss: 0.00002713
Iteration 22/1000 | Loss: 0.00002712
Iteration 23/1000 | Loss: 0.00002712
Iteration 24/1000 | Loss: 0.00002712
Iteration 25/1000 | Loss: 0.00002712
Iteration 26/1000 | Loss: 0.00002712
Iteration 27/1000 | Loss: 0.00002711
Iteration 28/1000 | Loss: 0.00002711
Iteration 29/1000 | Loss: 0.00002711
Iteration 30/1000 | Loss: 0.00002710
Iteration 31/1000 | Loss: 0.00002710
Iteration 32/1000 | Loss: 0.00002710
Iteration 33/1000 | Loss: 0.00002710
Iteration 34/1000 | Loss: 0.00002710
Iteration 35/1000 | Loss: 0.00002710
Iteration 36/1000 | Loss: 0.00002709
Iteration 37/1000 | Loss: 0.00002708
Iteration 38/1000 | Loss: 0.00002708
Iteration 39/1000 | Loss: 0.00002708
Iteration 40/1000 | Loss: 0.00002707
Iteration 41/1000 | Loss: 0.00002707
Iteration 42/1000 | Loss: 0.00002707
Iteration 43/1000 | Loss: 0.00002707
Iteration 44/1000 | Loss: 0.00002706
Iteration 45/1000 | Loss: 0.00002706
Iteration 46/1000 | Loss: 0.00002705
Iteration 47/1000 | Loss: 0.00002705
Iteration 48/1000 | Loss: 0.00002705
Iteration 49/1000 | Loss: 0.00002705
Iteration 50/1000 | Loss: 0.00002705
Iteration 51/1000 | Loss: 0.00002705
Iteration 52/1000 | Loss: 0.00002705
Iteration 53/1000 | Loss: 0.00002704
Iteration 54/1000 | Loss: 0.00002704
Iteration 55/1000 | Loss: 0.00002704
Iteration 56/1000 | Loss: 0.00002704
Iteration 57/1000 | Loss: 0.00002703
Iteration 58/1000 | Loss: 0.00002703
Iteration 59/1000 | Loss: 0.00002703
Iteration 60/1000 | Loss: 0.00002703
Iteration 61/1000 | Loss: 0.00002703
Iteration 62/1000 | Loss: 0.00002702
Iteration 63/1000 | Loss: 0.00002702
Iteration 64/1000 | Loss: 0.00002702
Iteration 65/1000 | Loss: 0.00002702
Iteration 66/1000 | Loss: 0.00002702
Iteration 67/1000 | Loss: 0.00002702
Iteration 68/1000 | Loss: 0.00002702
Iteration 69/1000 | Loss: 0.00002702
Iteration 70/1000 | Loss: 0.00002702
Iteration 71/1000 | Loss: 0.00002702
Iteration 72/1000 | Loss: 0.00002702
Iteration 73/1000 | Loss: 0.00002702
Iteration 74/1000 | Loss: 0.00002702
Iteration 75/1000 | Loss: 0.00002702
Iteration 76/1000 | Loss: 0.00002702
Iteration 77/1000 | Loss: 0.00002702
Iteration 78/1000 | Loss: 0.00002702
Iteration 79/1000 | Loss: 0.00002702
Iteration 80/1000 | Loss: 0.00002702
Iteration 81/1000 | Loss: 0.00002702
Iteration 82/1000 | Loss: 0.00002702
Iteration 83/1000 | Loss: 0.00002702
Iteration 84/1000 | Loss: 0.00002702
Iteration 85/1000 | Loss: 0.00002702
Iteration 86/1000 | Loss: 0.00002702
Iteration 87/1000 | Loss: 0.00002702
Iteration 88/1000 | Loss: 0.00002702
Iteration 89/1000 | Loss: 0.00002702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [2.7015556042897515e-05, 2.7015556042897515e-05, 2.7015556042897515e-05, 2.7015556042897515e-05, 2.7015556042897515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7015556042897515e-05

Optimization complete. Final v2v error: 4.405418872833252 mm

Highest mean error: 4.672196388244629 mm for frame 133

Lowest mean error: 4.278050899505615 mm for frame 69

Saving results

Total time: 60.966139793395996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00653443
Iteration 2/25 | Loss: 0.00130858
Iteration 3/25 | Loss: 0.00122373
Iteration 4/25 | Loss: 0.00120900
Iteration 5/25 | Loss: 0.00120445
Iteration 6/25 | Loss: 0.00120353
Iteration 7/25 | Loss: 0.00120353
Iteration 8/25 | Loss: 0.00120353
Iteration 9/25 | Loss: 0.00120353
Iteration 10/25 | Loss: 0.00120353
Iteration 11/25 | Loss: 0.00120353
Iteration 12/25 | Loss: 0.00120353
Iteration 13/25 | Loss: 0.00120353
Iteration 14/25 | Loss: 0.00120353
Iteration 15/25 | Loss: 0.00120353
Iteration 16/25 | Loss: 0.00120353
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001203531282953918, 0.001203531282953918, 0.001203531282953918, 0.001203531282953918, 0.001203531282953918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001203531282953918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.24504948
Iteration 2/25 | Loss: 0.00136730
Iteration 3/25 | Loss: 0.00136730
Iteration 4/25 | Loss: 0.00136730
Iteration 5/25 | Loss: 0.00136730
Iteration 6/25 | Loss: 0.00136729
Iteration 7/25 | Loss: 0.00136729
Iteration 8/25 | Loss: 0.00136729
Iteration 9/25 | Loss: 0.00136729
Iteration 10/25 | Loss: 0.00136729
Iteration 11/25 | Loss: 0.00136729
Iteration 12/25 | Loss: 0.00136729
Iteration 13/25 | Loss: 0.00136729
Iteration 14/25 | Loss: 0.00136729
Iteration 15/25 | Loss: 0.00136729
Iteration 16/25 | Loss: 0.00136729
Iteration 17/25 | Loss: 0.00136729
Iteration 18/25 | Loss: 0.00136729
Iteration 19/25 | Loss: 0.00136729
Iteration 20/25 | Loss: 0.00136729
Iteration 21/25 | Loss: 0.00136729
Iteration 22/25 | Loss: 0.00136729
Iteration 23/25 | Loss: 0.00136729
Iteration 24/25 | Loss: 0.00136729
Iteration 25/25 | Loss: 0.00136729

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136729
Iteration 2/1000 | Loss: 0.00002679
Iteration 3/1000 | Loss: 0.00001815
Iteration 4/1000 | Loss: 0.00001546
Iteration 5/1000 | Loss: 0.00001471
Iteration 6/1000 | Loss: 0.00001404
Iteration 7/1000 | Loss: 0.00001328
Iteration 8/1000 | Loss: 0.00001294
Iteration 9/1000 | Loss: 0.00001255
Iteration 10/1000 | Loss: 0.00001226
Iteration 11/1000 | Loss: 0.00001223
Iteration 12/1000 | Loss: 0.00001210
Iteration 13/1000 | Loss: 0.00001191
Iteration 14/1000 | Loss: 0.00001180
Iteration 15/1000 | Loss: 0.00001171
Iteration 16/1000 | Loss: 0.00001170
Iteration 17/1000 | Loss: 0.00001158
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001153
Iteration 20/1000 | Loss: 0.00001152
Iteration 21/1000 | Loss: 0.00001151
Iteration 22/1000 | Loss: 0.00001151
Iteration 23/1000 | Loss: 0.00001150
Iteration 24/1000 | Loss: 0.00001148
Iteration 25/1000 | Loss: 0.00001147
Iteration 26/1000 | Loss: 0.00001146
Iteration 27/1000 | Loss: 0.00001146
Iteration 28/1000 | Loss: 0.00001143
Iteration 29/1000 | Loss: 0.00001142
Iteration 30/1000 | Loss: 0.00001142
Iteration 31/1000 | Loss: 0.00001140
Iteration 32/1000 | Loss: 0.00001140
Iteration 33/1000 | Loss: 0.00001139
Iteration 34/1000 | Loss: 0.00001136
Iteration 35/1000 | Loss: 0.00001134
Iteration 36/1000 | Loss: 0.00001133
Iteration 37/1000 | Loss: 0.00001133
Iteration 38/1000 | Loss: 0.00001133
Iteration 39/1000 | Loss: 0.00001132
Iteration 40/1000 | Loss: 0.00001132
Iteration 41/1000 | Loss: 0.00001132
Iteration 42/1000 | Loss: 0.00001131
Iteration 43/1000 | Loss: 0.00001131
Iteration 44/1000 | Loss: 0.00001131
Iteration 45/1000 | Loss: 0.00001131
Iteration 46/1000 | Loss: 0.00001131
Iteration 47/1000 | Loss: 0.00001131
Iteration 48/1000 | Loss: 0.00001130
Iteration 49/1000 | Loss: 0.00001130
Iteration 50/1000 | Loss: 0.00001130
Iteration 51/1000 | Loss: 0.00001129
Iteration 52/1000 | Loss: 0.00001129
Iteration 53/1000 | Loss: 0.00001129
Iteration 54/1000 | Loss: 0.00001129
Iteration 55/1000 | Loss: 0.00001129
Iteration 56/1000 | Loss: 0.00001128
Iteration 57/1000 | Loss: 0.00001128
Iteration 58/1000 | Loss: 0.00001128
Iteration 59/1000 | Loss: 0.00001128
Iteration 60/1000 | Loss: 0.00001128
Iteration 61/1000 | Loss: 0.00001128
Iteration 62/1000 | Loss: 0.00001128
Iteration 63/1000 | Loss: 0.00001127
Iteration 64/1000 | Loss: 0.00001127
Iteration 65/1000 | Loss: 0.00001126
Iteration 66/1000 | Loss: 0.00001126
Iteration 67/1000 | Loss: 0.00001126
Iteration 68/1000 | Loss: 0.00001125
Iteration 69/1000 | Loss: 0.00001125
Iteration 70/1000 | Loss: 0.00001125
Iteration 71/1000 | Loss: 0.00001125
Iteration 72/1000 | Loss: 0.00001125
Iteration 73/1000 | Loss: 0.00001124
Iteration 74/1000 | Loss: 0.00001124
Iteration 75/1000 | Loss: 0.00001124
Iteration 76/1000 | Loss: 0.00001124
Iteration 77/1000 | Loss: 0.00001124
Iteration 78/1000 | Loss: 0.00001123
Iteration 79/1000 | Loss: 0.00001123
Iteration 80/1000 | Loss: 0.00001122
Iteration 81/1000 | Loss: 0.00001122
Iteration 82/1000 | Loss: 0.00001122
Iteration 83/1000 | Loss: 0.00001121
Iteration 84/1000 | Loss: 0.00001121
Iteration 85/1000 | Loss: 0.00001121
Iteration 86/1000 | Loss: 0.00001120
Iteration 87/1000 | Loss: 0.00001120
Iteration 88/1000 | Loss: 0.00001120
Iteration 89/1000 | Loss: 0.00001120
Iteration 90/1000 | Loss: 0.00001119
Iteration 91/1000 | Loss: 0.00001119
Iteration 92/1000 | Loss: 0.00001118
Iteration 93/1000 | Loss: 0.00001118
Iteration 94/1000 | Loss: 0.00001118
Iteration 95/1000 | Loss: 0.00001118
Iteration 96/1000 | Loss: 0.00001118
Iteration 97/1000 | Loss: 0.00001118
Iteration 98/1000 | Loss: 0.00001118
Iteration 99/1000 | Loss: 0.00001117
Iteration 100/1000 | Loss: 0.00001117
Iteration 101/1000 | Loss: 0.00001117
Iteration 102/1000 | Loss: 0.00001117
Iteration 103/1000 | Loss: 0.00001117
Iteration 104/1000 | Loss: 0.00001117
Iteration 105/1000 | Loss: 0.00001116
Iteration 106/1000 | Loss: 0.00001116
Iteration 107/1000 | Loss: 0.00001116
Iteration 108/1000 | Loss: 0.00001116
Iteration 109/1000 | Loss: 0.00001116
Iteration 110/1000 | Loss: 0.00001116
Iteration 111/1000 | Loss: 0.00001115
Iteration 112/1000 | Loss: 0.00001115
Iteration 113/1000 | Loss: 0.00001115
Iteration 114/1000 | Loss: 0.00001115
Iteration 115/1000 | Loss: 0.00001115
Iteration 116/1000 | Loss: 0.00001115
Iteration 117/1000 | Loss: 0.00001115
Iteration 118/1000 | Loss: 0.00001115
Iteration 119/1000 | Loss: 0.00001115
Iteration 120/1000 | Loss: 0.00001115
Iteration 121/1000 | Loss: 0.00001115
Iteration 122/1000 | Loss: 0.00001115
Iteration 123/1000 | Loss: 0.00001115
Iteration 124/1000 | Loss: 0.00001115
Iteration 125/1000 | Loss: 0.00001115
Iteration 126/1000 | Loss: 0.00001115
Iteration 127/1000 | Loss: 0.00001115
Iteration 128/1000 | Loss: 0.00001115
Iteration 129/1000 | Loss: 0.00001115
Iteration 130/1000 | Loss: 0.00001115
Iteration 131/1000 | Loss: 0.00001115
Iteration 132/1000 | Loss: 0.00001115
Iteration 133/1000 | Loss: 0.00001115
Iteration 134/1000 | Loss: 0.00001115
Iteration 135/1000 | Loss: 0.00001115
Iteration 136/1000 | Loss: 0.00001115
Iteration 137/1000 | Loss: 0.00001115
Iteration 138/1000 | Loss: 0.00001115
Iteration 139/1000 | Loss: 0.00001115
Iteration 140/1000 | Loss: 0.00001115
Iteration 141/1000 | Loss: 0.00001115
Iteration 142/1000 | Loss: 0.00001115
Iteration 143/1000 | Loss: 0.00001115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.115329814638244e-05, 1.115329814638244e-05, 1.115329814638244e-05, 1.115329814638244e-05, 1.115329814638244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.115329814638244e-05

Optimization complete. Final v2v error: 2.8772785663604736 mm

Highest mean error: 3.1105921268463135 mm for frame 72

Lowest mean error: 2.6844465732574463 mm for frame 12

Saving results

Total time: 38.93753743171692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802385
Iteration 2/25 | Loss: 0.00152373
Iteration 3/25 | Loss: 0.00131235
Iteration 4/25 | Loss: 0.00128596
Iteration 5/25 | Loss: 0.00128096
Iteration 6/25 | Loss: 0.00128056
Iteration 7/25 | Loss: 0.00128056
Iteration 8/25 | Loss: 0.00128056
Iteration 9/25 | Loss: 0.00128056
Iteration 10/25 | Loss: 0.00128056
Iteration 11/25 | Loss: 0.00128056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012805594597011805, 0.0012805594597011805, 0.0012805594597011805, 0.0012805594597011805, 0.0012805594597011805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012805594597011805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27061331
Iteration 2/25 | Loss: 0.00138049
Iteration 3/25 | Loss: 0.00138049
Iteration 4/25 | Loss: 0.00138049
Iteration 5/25 | Loss: 0.00138049
Iteration 6/25 | Loss: 0.00138049
Iteration 7/25 | Loss: 0.00138049
Iteration 8/25 | Loss: 0.00138049
Iteration 9/25 | Loss: 0.00138049
Iteration 10/25 | Loss: 0.00138049
Iteration 11/25 | Loss: 0.00138049
Iteration 12/25 | Loss: 0.00138049
Iteration 13/25 | Loss: 0.00138049
Iteration 14/25 | Loss: 0.00138049
Iteration 15/25 | Loss: 0.00138049
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001380491186864674, 0.001380491186864674, 0.001380491186864674, 0.001380491186864674, 0.001380491186864674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001380491186864674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138049
Iteration 2/1000 | Loss: 0.00003889
Iteration 3/1000 | Loss: 0.00002581
Iteration 4/1000 | Loss: 0.00002305
Iteration 5/1000 | Loss: 0.00002200
Iteration 6/1000 | Loss: 0.00002132
Iteration 7/1000 | Loss: 0.00002071
Iteration 8/1000 | Loss: 0.00002026
Iteration 9/1000 | Loss: 0.00001990
Iteration 10/1000 | Loss: 0.00001948
Iteration 11/1000 | Loss: 0.00001931
Iteration 12/1000 | Loss: 0.00001922
Iteration 13/1000 | Loss: 0.00001915
Iteration 14/1000 | Loss: 0.00001897
Iteration 15/1000 | Loss: 0.00001882
Iteration 16/1000 | Loss: 0.00001881
Iteration 17/1000 | Loss: 0.00001881
Iteration 18/1000 | Loss: 0.00001880
Iteration 19/1000 | Loss: 0.00001880
Iteration 20/1000 | Loss: 0.00001879
Iteration 21/1000 | Loss: 0.00001879
Iteration 22/1000 | Loss: 0.00001878
Iteration 23/1000 | Loss: 0.00001872
Iteration 24/1000 | Loss: 0.00001872
Iteration 25/1000 | Loss: 0.00001871
Iteration 26/1000 | Loss: 0.00001867
Iteration 27/1000 | Loss: 0.00001866
Iteration 28/1000 | Loss: 0.00001866
Iteration 29/1000 | Loss: 0.00001864
Iteration 30/1000 | Loss: 0.00001864
Iteration 31/1000 | Loss: 0.00001863
Iteration 32/1000 | Loss: 0.00001863
Iteration 33/1000 | Loss: 0.00001862
Iteration 34/1000 | Loss: 0.00001861
Iteration 35/1000 | Loss: 0.00001861
Iteration 36/1000 | Loss: 0.00001861
Iteration 37/1000 | Loss: 0.00001860
Iteration 38/1000 | Loss: 0.00001860
Iteration 39/1000 | Loss: 0.00001860
Iteration 40/1000 | Loss: 0.00001859
Iteration 41/1000 | Loss: 0.00001859
Iteration 42/1000 | Loss: 0.00001858
Iteration 43/1000 | Loss: 0.00001858
Iteration 44/1000 | Loss: 0.00001858
Iteration 45/1000 | Loss: 0.00001858
Iteration 46/1000 | Loss: 0.00001858
Iteration 47/1000 | Loss: 0.00001857
Iteration 48/1000 | Loss: 0.00001857
Iteration 49/1000 | Loss: 0.00001857
Iteration 50/1000 | Loss: 0.00001857
Iteration 51/1000 | Loss: 0.00001856
Iteration 52/1000 | Loss: 0.00001856
Iteration 53/1000 | Loss: 0.00001856
Iteration 54/1000 | Loss: 0.00001856
Iteration 55/1000 | Loss: 0.00001856
Iteration 56/1000 | Loss: 0.00001856
Iteration 57/1000 | Loss: 0.00001856
Iteration 58/1000 | Loss: 0.00001856
Iteration 59/1000 | Loss: 0.00001856
Iteration 60/1000 | Loss: 0.00001856
Iteration 61/1000 | Loss: 0.00001856
Iteration 62/1000 | Loss: 0.00001856
Iteration 63/1000 | Loss: 0.00001856
Iteration 64/1000 | Loss: 0.00001856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [1.8555478163762018e-05, 1.8555478163762018e-05, 1.8555478163762018e-05, 1.8555478163762018e-05, 1.8555478163762018e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8555478163762018e-05

Optimization complete. Final v2v error: 3.6482136249542236 mm

Highest mean error: 4.258711814880371 mm for frame 219

Lowest mean error: 3.211535930633545 mm for frame 5

Saving results

Total time: 36.118640422821045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073052
Iteration 2/25 | Loss: 0.01073052
Iteration 3/25 | Loss: 0.01073052
Iteration 4/25 | Loss: 0.00265171
Iteration 5/25 | Loss: 0.00214237
Iteration 6/25 | Loss: 0.00217777
Iteration 7/25 | Loss: 0.00152167
Iteration 8/25 | Loss: 0.00146495
Iteration 9/25 | Loss: 0.00142336
Iteration 10/25 | Loss: 0.00140941
Iteration 11/25 | Loss: 0.00139817
Iteration 12/25 | Loss: 0.00139870
Iteration 13/25 | Loss: 0.00139378
Iteration 14/25 | Loss: 0.00139591
Iteration 15/25 | Loss: 0.00139188
Iteration 16/25 | Loss: 0.00139211
Iteration 17/25 | Loss: 0.00139040
Iteration 18/25 | Loss: 0.00138733
Iteration 19/25 | Loss: 0.00138751
Iteration 20/25 | Loss: 0.00138760
Iteration 21/25 | Loss: 0.00138763
Iteration 22/25 | Loss: 0.00139075
Iteration 23/25 | Loss: 0.00139261
Iteration 24/25 | Loss: 0.00139119
Iteration 25/25 | Loss: 0.00138844

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57321954
Iteration 2/25 | Loss: 0.00183750
Iteration 3/25 | Loss: 0.00183025
Iteration 4/25 | Loss: 0.00183025
Iteration 5/25 | Loss: 0.00183025
Iteration 6/25 | Loss: 0.00183025
Iteration 7/25 | Loss: 0.00183025
Iteration 8/25 | Loss: 0.00183025
Iteration 9/25 | Loss: 0.00183025
Iteration 10/25 | Loss: 0.00183025
Iteration 11/25 | Loss: 0.00183025
Iteration 12/25 | Loss: 0.00183025
Iteration 13/25 | Loss: 0.00183025
Iteration 14/25 | Loss: 0.00183025
Iteration 15/25 | Loss: 0.00183025
Iteration 16/25 | Loss: 0.00183025
Iteration 17/25 | Loss: 0.00183025
Iteration 18/25 | Loss: 0.00183025
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018302457174286246, 0.0018302457174286246, 0.0018302457174286246, 0.0018302457174286246, 0.0018302457174286246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018302457174286246

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183025
Iteration 2/1000 | Loss: 0.00071129
Iteration 3/1000 | Loss: 0.00055594
Iteration 4/1000 | Loss: 0.00056250
Iteration 5/1000 | Loss: 0.00048358
Iteration 6/1000 | Loss: 0.00042625
Iteration 7/1000 | Loss: 0.00038949
Iteration 8/1000 | Loss: 0.00045811
Iteration 9/1000 | Loss: 0.00056679
Iteration 10/1000 | Loss: 0.00054366
Iteration 11/1000 | Loss: 0.00047953
Iteration 12/1000 | Loss: 0.00037403
Iteration 13/1000 | Loss: 0.00041775
Iteration 14/1000 | Loss: 0.00046720
Iteration 15/1000 | Loss: 0.00064764
Iteration 16/1000 | Loss: 0.00051033
Iteration 17/1000 | Loss: 0.00054379
Iteration 18/1000 | Loss: 0.00049257
Iteration 19/1000 | Loss: 0.00059249
Iteration 20/1000 | Loss: 0.00069919
Iteration 21/1000 | Loss: 0.00056315
Iteration 22/1000 | Loss: 0.00024993
Iteration 23/1000 | Loss: 0.00030140
Iteration 24/1000 | Loss: 0.00031178
Iteration 25/1000 | Loss: 0.00037903
Iteration 26/1000 | Loss: 0.00030981
Iteration 27/1000 | Loss: 0.00023375
Iteration 28/1000 | Loss: 0.00048130
Iteration 29/1000 | Loss: 0.00049865
Iteration 30/1000 | Loss: 0.00044205
Iteration 31/1000 | Loss: 0.00037676
Iteration 32/1000 | Loss: 0.00055401
Iteration 33/1000 | Loss: 0.00070952
Iteration 34/1000 | Loss: 0.00040855
Iteration 35/1000 | Loss: 0.00071683
Iteration 36/1000 | Loss: 0.00058459
Iteration 37/1000 | Loss: 0.00042951
Iteration 38/1000 | Loss: 0.00067026
Iteration 39/1000 | Loss: 0.00051111
Iteration 40/1000 | Loss: 0.00046749
Iteration 41/1000 | Loss: 0.00063194
Iteration 42/1000 | Loss: 0.00046281
Iteration 43/1000 | Loss: 0.00096081
Iteration 44/1000 | Loss: 0.00087054
Iteration 45/1000 | Loss: 0.00100643
Iteration 46/1000 | Loss: 0.00047870
Iteration 47/1000 | Loss: 0.00056195
Iteration 48/1000 | Loss: 0.00038893
Iteration 49/1000 | Loss: 0.00074853
Iteration 50/1000 | Loss: 0.00055645
Iteration 51/1000 | Loss: 0.00051166
Iteration 52/1000 | Loss: 0.00055617
Iteration 53/1000 | Loss: 0.00036331
Iteration 54/1000 | Loss: 0.00065561
Iteration 55/1000 | Loss: 0.00049204
Iteration 56/1000 | Loss: 0.00037468
Iteration 57/1000 | Loss: 0.00039847
Iteration 58/1000 | Loss: 0.00039961
Iteration 59/1000 | Loss: 0.00044450
Iteration 60/1000 | Loss: 0.00086203
Iteration 61/1000 | Loss: 0.00040924
Iteration 62/1000 | Loss: 0.00070659
Iteration 63/1000 | Loss: 0.00045987
Iteration 64/1000 | Loss: 0.00061717
Iteration 65/1000 | Loss: 0.00038265
Iteration 66/1000 | Loss: 0.00085305
Iteration 67/1000 | Loss: 0.00035096
Iteration 68/1000 | Loss: 0.00042617
Iteration 69/1000 | Loss: 0.00031847
Iteration 70/1000 | Loss: 0.00069281
Iteration 71/1000 | Loss: 0.00062659
Iteration 72/1000 | Loss: 0.00054720
Iteration 73/1000 | Loss: 0.00052776
Iteration 74/1000 | Loss: 0.00090614
Iteration 75/1000 | Loss: 0.00078162
Iteration 76/1000 | Loss: 0.00057338
Iteration 77/1000 | Loss: 0.00043088
Iteration 78/1000 | Loss: 0.00033860
Iteration 79/1000 | Loss: 0.00042844
Iteration 80/1000 | Loss: 0.00052843
Iteration 81/1000 | Loss: 0.00040777
Iteration 82/1000 | Loss: 0.00046882
Iteration 83/1000 | Loss: 0.00066895
Iteration 84/1000 | Loss: 0.00048552
Iteration 85/1000 | Loss: 0.00034967
Iteration 86/1000 | Loss: 0.00037327
Iteration 87/1000 | Loss: 0.00043780
Iteration 88/1000 | Loss: 0.00030981
Iteration 89/1000 | Loss: 0.00062547
Iteration 90/1000 | Loss: 0.00047860
Iteration 91/1000 | Loss: 0.00035873
Iteration 92/1000 | Loss: 0.00058778
Iteration 93/1000 | Loss: 0.00051204
Iteration 94/1000 | Loss: 0.00086374
Iteration 95/1000 | Loss: 0.00062911
Iteration 96/1000 | Loss: 0.00137244
Iteration 97/1000 | Loss: 0.00063280
Iteration 98/1000 | Loss: 0.00026355
Iteration 99/1000 | Loss: 0.00016388
Iteration 100/1000 | Loss: 0.00060210
Iteration 101/1000 | Loss: 0.00053313
Iteration 102/1000 | Loss: 0.00011820
Iteration 103/1000 | Loss: 0.00068943
Iteration 104/1000 | Loss: 0.00044513
Iteration 105/1000 | Loss: 0.00030188
Iteration 106/1000 | Loss: 0.00045501
Iteration 107/1000 | Loss: 0.00036017
Iteration 108/1000 | Loss: 0.00024237
Iteration 109/1000 | Loss: 0.00031075
Iteration 110/1000 | Loss: 0.00032715
Iteration 111/1000 | Loss: 0.00067752
Iteration 112/1000 | Loss: 0.00078716
Iteration 113/1000 | Loss: 0.00031375
Iteration 114/1000 | Loss: 0.00010567
Iteration 115/1000 | Loss: 0.00128126
Iteration 116/1000 | Loss: 0.00091741
Iteration 117/1000 | Loss: 0.00042888
Iteration 118/1000 | Loss: 0.00022742
Iteration 119/1000 | Loss: 0.00023176
Iteration 120/1000 | Loss: 0.00015375
Iteration 121/1000 | Loss: 0.00012960
Iteration 122/1000 | Loss: 0.00031523
Iteration 123/1000 | Loss: 0.00030202
Iteration 124/1000 | Loss: 0.00028524
Iteration 125/1000 | Loss: 0.00044286
Iteration 126/1000 | Loss: 0.00039614
Iteration 127/1000 | Loss: 0.00037124
Iteration 128/1000 | Loss: 0.00011928
Iteration 129/1000 | Loss: 0.00037805
Iteration 130/1000 | Loss: 0.00029273
Iteration 131/1000 | Loss: 0.00043744
Iteration 132/1000 | Loss: 0.00026719
Iteration 133/1000 | Loss: 0.00045853
Iteration 134/1000 | Loss: 0.00036395
Iteration 135/1000 | Loss: 0.00038311
Iteration 136/1000 | Loss: 0.00025308
Iteration 137/1000 | Loss: 0.00034070
Iteration 138/1000 | Loss: 0.00035728
Iteration 139/1000 | Loss: 0.00034341
Iteration 140/1000 | Loss: 0.00039734
Iteration 141/1000 | Loss: 0.00054967
Iteration 142/1000 | Loss: 0.00038795
Iteration 143/1000 | Loss: 0.00039421
Iteration 144/1000 | Loss: 0.00022139
Iteration 145/1000 | Loss: 0.00025372
Iteration 146/1000 | Loss: 0.00027988
Iteration 147/1000 | Loss: 0.00020405
Iteration 148/1000 | Loss: 0.00023327
Iteration 149/1000 | Loss: 0.00020451
Iteration 150/1000 | Loss: 0.00046382
Iteration 151/1000 | Loss: 0.00033826
Iteration 152/1000 | Loss: 0.00020834
Iteration 153/1000 | Loss: 0.00028277
Iteration 154/1000 | Loss: 0.00031477
Iteration 155/1000 | Loss: 0.00022777
Iteration 156/1000 | Loss: 0.00050995
Iteration 157/1000 | Loss: 0.00046360
Iteration 158/1000 | Loss: 0.00026631
Iteration 159/1000 | Loss: 0.00037777
Iteration 160/1000 | Loss: 0.00031881
Iteration 161/1000 | Loss: 0.00030930
Iteration 162/1000 | Loss: 0.00027994
Iteration 163/1000 | Loss: 0.00020503
Iteration 164/1000 | Loss: 0.00063463
Iteration 165/1000 | Loss: 0.00066554
Iteration 166/1000 | Loss: 0.00072995
Iteration 167/1000 | Loss: 0.00046226
Iteration 168/1000 | Loss: 0.00046324
Iteration 169/1000 | Loss: 0.00056135
Iteration 170/1000 | Loss: 0.00023562
Iteration 171/1000 | Loss: 0.00031031
Iteration 172/1000 | Loss: 0.00027569
Iteration 173/1000 | Loss: 0.00037116
Iteration 174/1000 | Loss: 0.00051329
Iteration 175/1000 | Loss: 0.00021774
Iteration 176/1000 | Loss: 0.00019580
Iteration 177/1000 | Loss: 0.00037611
Iteration 178/1000 | Loss: 0.00028222
Iteration 179/1000 | Loss: 0.00037872
Iteration 180/1000 | Loss: 0.00020199
Iteration 181/1000 | Loss: 0.00018867
Iteration 182/1000 | Loss: 0.00022763
Iteration 183/1000 | Loss: 0.00026234
Iteration 184/1000 | Loss: 0.00048797
Iteration 185/1000 | Loss: 0.00029196
Iteration 186/1000 | Loss: 0.00036754
Iteration 187/1000 | Loss: 0.00047571
Iteration 188/1000 | Loss: 0.00011843
Iteration 189/1000 | Loss: 0.00064642
Iteration 190/1000 | Loss: 0.00031023
Iteration 191/1000 | Loss: 0.00045851
Iteration 192/1000 | Loss: 0.00071190
Iteration 193/1000 | Loss: 0.00029520
Iteration 194/1000 | Loss: 0.00024133
Iteration 195/1000 | Loss: 0.00017023
Iteration 196/1000 | Loss: 0.00035774
Iteration 197/1000 | Loss: 0.00021218
Iteration 198/1000 | Loss: 0.00033865
Iteration 199/1000 | Loss: 0.00031594
Iteration 200/1000 | Loss: 0.00023366
Iteration 201/1000 | Loss: 0.00023038
Iteration 202/1000 | Loss: 0.00019279
Iteration 203/1000 | Loss: 0.00024271
Iteration 204/1000 | Loss: 0.00034028
Iteration 205/1000 | Loss: 0.00032845
Iteration 206/1000 | Loss: 0.00025754
Iteration 207/1000 | Loss: 0.00018840
Iteration 208/1000 | Loss: 0.00016413
Iteration 209/1000 | Loss: 0.00016909
Iteration 210/1000 | Loss: 0.00019540
Iteration 211/1000 | Loss: 0.00028921
Iteration 212/1000 | Loss: 0.00017215
Iteration 213/1000 | Loss: 0.00016519
Iteration 214/1000 | Loss: 0.00044664
Iteration 215/1000 | Loss: 0.00040905
Iteration 216/1000 | Loss: 0.00017160
Iteration 217/1000 | Loss: 0.00006237
Iteration 218/1000 | Loss: 0.00020342
Iteration 219/1000 | Loss: 0.00036840
Iteration 220/1000 | Loss: 0.00033945
Iteration 221/1000 | Loss: 0.00010541
Iteration 222/1000 | Loss: 0.00041755
Iteration 223/1000 | Loss: 0.00029846
Iteration 224/1000 | Loss: 0.00048937
Iteration 225/1000 | Loss: 0.00015729
Iteration 226/1000 | Loss: 0.00054608
Iteration 227/1000 | Loss: 0.00023592
Iteration 228/1000 | Loss: 0.00012529
Iteration 229/1000 | Loss: 0.00025254
Iteration 230/1000 | Loss: 0.00020834
Iteration 231/1000 | Loss: 0.00023892
Iteration 232/1000 | Loss: 0.00028749
Iteration 233/1000 | Loss: 0.00028362
Iteration 234/1000 | Loss: 0.00027002
Iteration 235/1000 | Loss: 0.00029509
Iteration 236/1000 | Loss: 0.00025795
Iteration 237/1000 | Loss: 0.00007538
Iteration 238/1000 | Loss: 0.00023781
Iteration 239/1000 | Loss: 0.00038272
Iteration 240/1000 | Loss: 0.00026632
Iteration 241/1000 | Loss: 0.00024697
Iteration 242/1000 | Loss: 0.00027309
Iteration 243/1000 | Loss: 0.00021446
Iteration 244/1000 | Loss: 0.00037642
Iteration 245/1000 | Loss: 0.00033763
Iteration 246/1000 | Loss: 0.00032528
Iteration 247/1000 | Loss: 0.00041422
Iteration 248/1000 | Loss: 0.00015027
Iteration 249/1000 | Loss: 0.00053064
Iteration 250/1000 | Loss: 0.00015363
Iteration 251/1000 | Loss: 0.00027548
Iteration 252/1000 | Loss: 0.00019252
Iteration 253/1000 | Loss: 0.00007202
Iteration 254/1000 | Loss: 0.00018977
Iteration 255/1000 | Loss: 0.00012585
Iteration 256/1000 | Loss: 0.00011951
Iteration 257/1000 | Loss: 0.00011696
Iteration 258/1000 | Loss: 0.00016236
Iteration 259/1000 | Loss: 0.00019759
Iteration 260/1000 | Loss: 0.00016222
Iteration 261/1000 | Loss: 0.00016342
Iteration 262/1000 | Loss: 0.00008089
Iteration 263/1000 | Loss: 0.00003418
Iteration 264/1000 | Loss: 0.00010348
Iteration 265/1000 | Loss: 0.00006338
Iteration 266/1000 | Loss: 0.00009902
Iteration 267/1000 | Loss: 0.00009458
Iteration 268/1000 | Loss: 0.00011554
Iteration 269/1000 | Loss: 0.00009318
Iteration 270/1000 | Loss: 0.00010884
Iteration 271/1000 | Loss: 0.00008888
Iteration 272/1000 | Loss: 0.00009849
Iteration 273/1000 | Loss: 0.00007245
Iteration 274/1000 | Loss: 0.00011254
Iteration 275/1000 | Loss: 0.00021089
Iteration 276/1000 | Loss: 0.00057202
Iteration 277/1000 | Loss: 0.00040530
Iteration 278/1000 | Loss: 0.00037273
Iteration 279/1000 | Loss: 0.00023383
Iteration 280/1000 | Loss: 0.00012412
Iteration 281/1000 | Loss: 0.00012764
Iteration 282/1000 | Loss: 0.00030854
Iteration 283/1000 | Loss: 0.00039040
Iteration 284/1000 | Loss: 0.00047492
Iteration 285/1000 | Loss: 0.00067198
Iteration 286/1000 | Loss: 0.00035403
Iteration 287/1000 | Loss: 0.00041184
Iteration 288/1000 | Loss: 0.00017395
Iteration 289/1000 | Loss: 0.00015805
Iteration 290/1000 | Loss: 0.00049384
Iteration 291/1000 | Loss: 0.00027748
Iteration 292/1000 | Loss: 0.00026997
Iteration 293/1000 | Loss: 0.00021860
Iteration 294/1000 | Loss: 0.00016723
Iteration 295/1000 | Loss: 0.00028447
Iteration 296/1000 | Loss: 0.00022711
Iteration 297/1000 | Loss: 0.00023608
Iteration 298/1000 | Loss: 0.00060177
Iteration 299/1000 | Loss: 0.00023150
Iteration 300/1000 | Loss: 0.00036523
Iteration 301/1000 | Loss: 0.00102418
Iteration 302/1000 | Loss: 0.00035318
Iteration 303/1000 | Loss: 0.00051450
Iteration 304/1000 | Loss: 0.00004057
Iteration 305/1000 | Loss: 0.00003216
Iteration 306/1000 | Loss: 0.00004260
Iteration 307/1000 | Loss: 0.00002668
Iteration 308/1000 | Loss: 0.00002549
Iteration 309/1000 | Loss: 0.00002400
Iteration 310/1000 | Loss: 0.00002266
Iteration 311/1000 | Loss: 0.00002191
Iteration 312/1000 | Loss: 0.00002139
Iteration 313/1000 | Loss: 0.00002103
Iteration 314/1000 | Loss: 0.00002072
Iteration 315/1000 | Loss: 0.00002055
Iteration 316/1000 | Loss: 0.00002052
Iteration 317/1000 | Loss: 0.00002043
Iteration 318/1000 | Loss: 0.00002025
Iteration 319/1000 | Loss: 0.00002012
Iteration 320/1000 | Loss: 0.00002010
Iteration 321/1000 | Loss: 0.00002004
Iteration 322/1000 | Loss: 0.00002003
Iteration 323/1000 | Loss: 0.00002002
Iteration 324/1000 | Loss: 0.00002002
Iteration 325/1000 | Loss: 0.00002001
Iteration 326/1000 | Loss: 0.00002000
Iteration 327/1000 | Loss: 0.00002000
Iteration 328/1000 | Loss: 0.00002000
Iteration 329/1000 | Loss: 0.00001999
Iteration 330/1000 | Loss: 0.00001999
Iteration 331/1000 | Loss: 0.00001999
Iteration 332/1000 | Loss: 0.00001998
Iteration 333/1000 | Loss: 0.00001998
Iteration 334/1000 | Loss: 0.00001998
Iteration 335/1000 | Loss: 0.00001998
Iteration 336/1000 | Loss: 0.00001998
Iteration 337/1000 | Loss: 0.00001997
Iteration 338/1000 | Loss: 0.00001997
Iteration 339/1000 | Loss: 0.00001997
Iteration 340/1000 | Loss: 0.00001997
Iteration 341/1000 | Loss: 0.00001996
Iteration 342/1000 | Loss: 0.00001996
Iteration 343/1000 | Loss: 0.00001995
Iteration 344/1000 | Loss: 0.00001995
Iteration 345/1000 | Loss: 0.00001995
Iteration 346/1000 | Loss: 0.00001995
Iteration 347/1000 | Loss: 0.00001995
Iteration 348/1000 | Loss: 0.00001995
Iteration 349/1000 | Loss: 0.00001995
Iteration 350/1000 | Loss: 0.00001995
Iteration 351/1000 | Loss: 0.00001995
Iteration 352/1000 | Loss: 0.00001994
Iteration 353/1000 | Loss: 0.00001994
Iteration 354/1000 | Loss: 0.00001994
Iteration 355/1000 | Loss: 0.00001994
Iteration 356/1000 | Loss: 0.00001994
Iteration 357/1000 | Loss: 0.00001994
Iteration 358/1000 | Loss: 0.00001994
Iteration 359/1000 | Loss: 0.00001994
Iteration 360/1000 | Loss: 0.00001993
Iteration 361/1000 | Loss: 0.00001993
Iteration 362/1000 | Loss: 0.00001993
Iteration 363/1000 | Loss: 0.00001993
Iteration 364/1000 | Loss: 0.00001993
Iteration 365/1000 | Loss: 0.00001993
Iteration 366/1000 | Loss: 0.00001993
Iteration 367/1000 | Loss: 0.00001993
Iteration 368/1000 | Loss: 0.00001993
Iteration 369/1000 | Loss: 0.00001992
Iteration 370/1000 | Loss: 0.00001992
Iteration 371/1000 | Loss: 0.00001992
Iteration 372/1000 | Loss: 0.00001992
Iteration 373/1000 | Loss: 0.00001992
Iteration 374/1000 | Loss: 0.00001992
Iteration 375/1000 | Loss: 0.00001992
Iteration 376/1000 | Loss: 0.00001992
Iteration 377/1000 | Loss: 0.00001992
Iteration 378/1000 | Loss: 0.00001992
Iteration 379/1000 | Loss: 0.00001991
Iteration 380/1000 | Loss: 0.00001991
Iteration 381/1000 | Loss: 0.00001991
Iteration 382/1000 | Loss: 0.00001991
Iteration 383/1000 | Loss: 0.00001991
Iteration 384/1000 | Loss: 0.00001991
Iteration 385/1000 | Loss: 0.00001991
Iteration 386/1000 | Loss: 0.00001991
Iteration 387/1000 | Loss: 0.00001991
Iteration 388/1000 | Loss: 0.00001991
Iteration 389/1000 | Loss: 0.00001991
Iteration 390/1000 | Loss: 0.00001991
Iteration 391/1000 | Loss: 0.00001990
Iteration 392/1000 | Loss: 0.00001990
Iteration 393/1000 | Loss: 0.00001990
Iteration 394/1000 | Loss: 0.00001990
Iteration 395/1000 | Loss: 0.00001990
Iteration 396/1000 | Loss: 0.00001990
Iteration 397/1000 | Loss: 0.00001990
Iteration 398/1000 | Loss: 0.00001990
Iteration 399/1000 | Loss: 0.00001990
Iteration 400/1000 | Loss: 0.00001990
Iteration 401/1000 | Loss: 0.00001990
Iteration 402/1000 | Loss: 0.00001990
Iteration 403/1000 | Loss: 0.00001990
Iteration 404/1000 | Loss: 0.00001990
Iteration 405/1000 | Loss: 0.00001990
Iteration 406/1000 | Loss: 0.00001990
Iteration 407/1000 | Loss: 0.00001990
Iteration 408/1000 | Loss: 0.00001990
Iteration 409/1000 | Loss: 0.00001990
Iteration 410/1000 | Loss: 0.00001990
Iteration 411/1000 | Loss: 0.00001990
Iteration 412/1000 | Loss: 0.00001990
Iteration 413/1000 | Loss: 0.00001990
Iteration 414/1000 | Loss: 0.00001990
Iteration 415/1000 | Loss: 0.00001990
Iteration 416/1000 | Loss: 0.00001990
Iteration 417/1000 | Loss: 0.00001990
Iteration 418/1000 | Loss: 0.00001990
Iteration 419/1000 | Loss: 0.00001990
Iteration 420/1000 | Loss: 0.00001990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 420. Stopping optimization.
Last 5 losses: [1.989612792385742e-05, 1.989612792385742e-05, 1.989612792385742e-05, 1.989612792385742e-05, 1.989612792385742e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.989612792385742e-05

Optimization complete. Final v2v error: 3.6210615634918213 mm

Highest mean error: 3.9793307781219482 mm for frame 29

Lowest mean error: 3.0756025314331055 mm for frame 145

Saving results

Total time: 495.02871108055115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393856
Iteration 2/25 | Loss: 0.00132986
Iteration 3/25 | Loss: 0.00121656
Iteration 4/25 | Loss: 0.00120231
Iteration 5/25 | Loss: 0.00119990
Iteration 6/25 | Loss: 0.00119984
Iteration 7/25 | Loss: 0.00119984
Iteration 8/25 | Loss: 0.00119984
Iteration 9/25 | Loss: 0.00119984
Iteration 10/25 | Loss: 0.00119984
Iteration 11/25 | Loss: 0.00119984
Iteration 12/25 | Loss: 0.00119984
Iteration 13/25 | Loss: 0.00119984
Iteration 14/25 | Loss: 0.00119984
Iteration 15/25 | Loss: 0.00119984
Iteration 16/25 | Loss: 0.00119984
Iteration 17/25 | Loss: 0.00119984
Iteration 18/25 | Loss: 0.00119984
Iteration 19/25 | Loss: 0.00119984
Iteration 20/25 | Loss: 0.00119984
Iteration 21/25 | Loss: 0.00119984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011998355621472, 0.0011998355621472, 0.0011998355621472, 0.0011998355621472, 0.0011998355621472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011998355621472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28504455
Iteration 2/25 | Loss: 0.00126653
Iteration 3/25 | Loss: 0.00126653
Iteration 4/25 | Loss: 0.00126653
Iteration 5/25 | Loss: 0.00126653
Iteration 6/25 | Loss: 0.00126653
Iteration 7/25 | Loss: 0.00126653
Iteration 8/25 | Loss: 0.00126653
Iteration 9/25 | Loss: 0.00126653
Iteration 10/25 | Loss: 0.00126653
Iteration 11/25 | Loss: 0.00126653
Iteration 12/25 | Loss: 0.00126653
Iteration 13/25 | Loss: 0.00126653
Iteration 14/25 | Loss: 0.00126653
Iteration 15/25 | Loss: 0.00126653
Iteration 16/25 | Loss: 0.00126653
Iteration 17/25 | Loss: 0.00126653
Iteration 18/25 | Loss: 0.00126653
Iteration 19/25 | Loss: 0.00126653
Iteration 20/25 | Loss: 0.00126653
Iteration 21/25 | Loss: 0.00126653
Iteration 22/25 | Loss: 0.00126653
Iteration 23/25 | Loss: 0.00126653
Iteration 24/25 | Loss: 0.00126653
Iteration 25/25 | Loss: 0.00126653
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012665258254855871, 0.0012665258254855871, 0.0012665258254855871, 0.0012665258254855871, 0.0012665258254855871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012665258254855871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126653
Iteration 2/1000 | Loss: 0.00002140
Iteration 3/1000 | Loss: 0.00001662
Iteration 4/1000 | Loss: 0.00001533
Iteration 5/1000 | Loss: 0.00001437
Iteration 6/1000 | Loss: 0.00001367
Iteration 7/1000 | Loss: 0.00001337
Iteration 8/1000 | Loss: 0.00001288
Iteration 9/1000 | Loss: 0.00001256
Iteration 10/1000 | Loss: 0.00001233
Iteration 11/1000 | Loss: 0.00001233
Iteration 12/1000 | Loss: 0.00001222
Iteration 13/1000 | Loss: 0.00001211
Iteration 14/1000 | Loss: 0.00001204
Iteration 15/1000 | Loss: 0.00001198
Iteration 16/1000 | Loss: 0.00001197
Iteration 17/1000 | Loss: 0.00001196
Iteration 18/1000 | Loss: 0.00001192
Iteration 19/1000 | Loss: 0.00001191
Iteration 20/1000 | Loss: 0.00001190
Iteration 21/1000 | Loss: 0.00001186
Iteration 22/1000 | Loss: 0.00001183
Iteration 23/1000 | Loss: 0.00001180
Iteration 24/1000 | Loss: 0.00001179
Iteration 25/1000 | Loss: 0.00001178
Iteration 26/1000 | Loss: 0.00001177
Iteration 27/1000 | Loss: 0.00001176
Iteration 28/1000 | Loss: 0.00001175
Iteration 29/1000 | Loss: 0.00001168
Iteration 30/1000 | Loss: 0.00001165
Iteration 31/1000 | Loss: 0.00001164
Iteration 32/1000 | Loss: 0.00001163
Iteration 33/1000 | Loss: 0.00001162
Iteration 34/1000 | Loss: 0.00001162
Iteration 35/1000 | Loss: 0.00001161
Iteration 36/1000 | Loss: 0.00001161
Iteration 37/1000 | Loss: 0.00001159
Iteration 38/1000 | Loss: 0.00001159
Iteration 39/1000 | Loss: 0.00001155
Iteration 40/1000 | Loss: 0.00001155
Iteration 41/1000 | Loss: 0.00001153
Iteration 42/1000 | Loss: 0.00001152
Iteration 43/1000 | Loss: 0.00001151
Iteration 44/1000 | Loss: 0.00001149
Iteration 45/1000 | Loss: 0.00001149
Iteration 46/1000 | Loss: 0.00001148
Iteration 47/1000 | Loss: 0.00001148
Iteration 48/1000 | Loss: 0.00001147
Iteration 49/1000 | Loss: 0.00001147
Iteration 50/1000 | Loss: 0.00001147
Iteration 51/1000 | Loss: 0.00001146
Iteration 52/1000 | Loss: 0.00001145
Iteration 53/1000 | Loss: 0.00001144
Iteration 54/1000 | Loss: 0.00001144
Iteration 55/1000 | Loss: 0.00001144
Iteration 56/1000 | Loss: 0.00001143
Iteration 57/1000 | Loss: 0.00001142
Iteration 58/1000 | Loss: 0.00001140
Iteration 59/1000 | Loss: 0.00001140
Iteration 60/1000 | Loss: 0.00001140
Iteration 61/1000 | Loss: 0.00001139
Iteration 62/1000 | Loss: 0.00001139
Iteration 63/1000 | Loss: 0.00001139
Iteration 64/1000 | Loss: 0.00001138
Iteration 65/1000 | Loss: 0.00001137
Iteration 66/1000 | Loss: 0.00001136
Iteration 67/1000 | Loss: 0.00001135
Iteration 68/1000 | Loss: 0.00001135
Iteration 69/1000 | Loss: 0.00001135
Iteration 70/1000 | Loss: 0.00001135
Iteration 71/1000 | Loss: 0.00001135
Iteration 72/1000 | Loss: 0.00001135
Iteration 73/1000 | Loss: 0.00001135
Iteration 74/1000 | Loss: 0.00001135
Iteration 75/1000 | Loss: 0.00001135
Iteration 76/1000 | Loss: 0.00001135
Iteration 77/1000 | Loss: 0.00001135
Iteration 78/1000 | Loss: 0.00001134
Iteration 79/1000 | Loss: 0.00001134
Iteration 80/1000 | Loss: 0.00001134
Iteration 81/1000 | Loss: 0.00001134
Iteration 82/1000 | Loss: 0.00001134
Iteration 83/1000 | Loss: 0.00001133
Iteration 84/1000 | Loss: 0.00001133
Iteration 85/1000 | Loss: 0.00001132
Iteration 86/1000 | Loss: 0.00001132
Iteration 87/1000 | Loss: 0.00001132
Iteration 88/1000 | Loss: 0.00001132
Iteration 89/1000 | Loss: 0.00001132
Iteration 90/1000 | Loss: 0.00001132
Iteration 91/1000 | Loss: 0.00001132
Iteration 92/1000 | Loss: 0.00001132
Iteration 93/1000 | Loss: 0.00001132
Iteration 94/1000 | Loss: 0.00001132
Iteration 95/1000 | Loss: 0.00001132
Iteration 96/1000 | Loss: 0.00001132
Iteration 97/1000 | Loss: 0.00001132
Iteration 98/1000 | Loss: 0.00001132
Iteration 99/1000 | Loss: 0.00001132
Iteration 100/1000 | Loss: 0.00001132
Iteration 101/1000 | Loss: 0.00001132
Iteration 102/1000 | Loss: 0.00001132
Iteration 103/1000 | Loss: 0.00001132
Iteration 104/1000 | Loss: 0.00001132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.1318597898934968e-05, 1.1318597898934968e-05, 1.1318597898934968e-05, 1.1318597898934968e-05, 1.1318597898934968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1318597898934968e-05

Optimization complete. Final v2v error: 2.898040771484375 mm

Highest mean error: 3.203110933303833 mm for frame 106

Lowest mean error: 2.6642708778381348 mm for frame 178

Saving results

Total time: 37.93415975570679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00581498
Iteration 2/25 | Loss: 0.00132248
Iteration 3/25 | Loss: 0.00124849
Iteration 4/25 | Loss: 0.00123125
Iteration 5/25 | Loss: 0.00122562
Iteration 6/25 | Loss: 0.00122423
Iteration 7/25 | Loss: 0.00122423
Iteration 8/25 | Loss: 0.00122423
Iteration 9/25 | Loss: 0.00122423
Iteration 10/25 | Loss: 0.00122423
Iteration 11/25 | Loss: 0.00122423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001224226551130414, 0.001224226551130414, 0.001224226551130414, 0.001224226551130414, 0.001224226551130414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001224226551130414

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.09158587
Iteration 2/25 | Loss: 0.00144962
Iteration 3/25 | Loss: 0.00144962
Iteration 4/25 | Loss: 0.00144962
Iteration 5/25 | Loss: 0.00144962
Iteration 6/25 | Loss: 0.00144962
Iteration 7/25 | Loss: 0.00144962
Iteration 8/25 | Loss: 0.00144962
Iteration 9/25 | Loss: 0.00144962
Iteration 10/25 | Loss: 0.00144962
Iteration 11/25 | Loss: 0.00144962
Iteration 12/25 | Loss: 0.00144962
Iteration 13/25 | Loss: 0.00144962
Iteration 14/25 | Loss: 0.00144962
Iteration 15/25 | Loss: 0.00144962
Iteration 16/25 | Loss: 0.00144962
Iteration 17/25 | Loss: 0.00144962
Iteration 18/25 | Loss: 0.00144962
Iteration 19/25 | Loss: 0.00144962
Iteration 20/25 | Loss: 0.00144962
Iteration 21/25 | Loss: 0.00144962
Iteration 22/25 | Loss: 0.00144962
Iteration 23/25 | Loss: 0.00144962
Iteration 24/25 | Loss: 0.00144962
Iteration 25/25 | Loss: 0.00144962
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014496167423203588, 0.0014496167423203588, 0.0014496167423203588, 0.0014496167423203588, 0.0014496167423203588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014496167423203588

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144962
Iteration 2/1000 | Loss: 0.00002578
Iteration 3/1000 | Loss: 0.00002060
Iteration 4/1000 | Loss: 0.00001965
Iteration 5/1000 | Loss: 0.00001891
Iteration 6/1000 | Loss: 0.00001824
Iteration 7/1000 | Loss: 0.00001785
Iteration 8/1000 | Loss: 0.00001750
Iteration 9/1000 | Loss: 0.00001722
Iteration 10/1000 | Loss: 0.00001697
Iteration 11/1000 | Loss: 0.00001676
Iteration 12/1000 | Loss: 0.00001657
Iteration 13/1000 | Loss: 0.00001652
Iteration 14/1000 | Loss: 0.00001649
Iteration 15/1000 | Loss: 0.00001649
Iteration 16/1000 | Loss: 0.00001648
Iteration 17/1000 | Loss: 0.00001646
Iteration 18/1000 | Loss: 0.00001644
Iteration 19/1000 | Loss: 0.00001643
Iteration 20/1000 | Loss: 0.00001641
Iteration 21/1000 | Loss: 0.00001638
Iteration 22/1000 | Loss: 0.00001638
Iteration 23/1000 | Loss: 0.00001634
Iteration 24/1000 | Loss: 0.00001633
Iteration 25/1000 | Loss: 0.00001632
Iteration 26/1000 | Loss: 0.00001631
Iteration 27/1000 | Loss: 0.00001631
Iteration 28/1000 | Loss: 0.00001631
Iteration 29/1000 | Loss: 0.00001631
Iteration 30/1000 | Loss: 0.00001631
Iteration 31/1000 | Loss: 0.00001631
Iteration 32/1000 | Loss: 0.00001629
Iteration 33/1000 | Loss: 0.00001627
Iteration 34/1000 | Loss: 0.00001627
Iteration 35/1000 | Loss: 0.00001627
Iteration 36/1000 | Loss: 0.00001627
Iteration 37/1000 | Loss: 0.00001627
Iteration 38/1000 | Loss: 0.00001627
Iteration 39/1000 | Loss: 0.00001627
Iteration 40/1000 | Loss: 0.00001627
Iteration 41/1000 | Loss: 0.00001626
Iteration 42/1000 | Loss: 0.00001626
Iteration 43/1000 | Loss: 0.00001626
Iteration 44/1000 | Loss: 0.00001626
Iteration 45/1000 | Loss: 0.00001626
Iteration 46/1000 | Loss: 0.00001626
Iteration 47/1000 | Loss: 0.00001626
Iteration 48/1000 | Loss: 0.00001626
Iteration 49/1000 | Loss: 0.00001625
Iteration 50/1000 | Loss: 0.00001625
Iteration 51/1000 | Loss: 0.00001623
Iteration 52/1000 | Loss: 0.00001622
Iteration 53/1000 | Loss: 0.00001622
Iteration 54/1000 | Loss: 0.00001622
Iteration 55/1000 | Loss: 0.00001622
Iteration 56/1000 | Loss: 0.00001622
Iteration 57/1000 | Loss: 0.00001622
Iteration 58/1000 | Loss: 0.00001622
Iteration 59/1000 | Loss: 0.00001622
Iteration 60/1000 | Loss: 0.00001622
Iteration 61/1000 | Loss: 0.00001621
Iteration 62/1000 | Loss: 0.00001621
Iteration 63/1000 | Loss: 0.00001621
Iteration 64/1000 | Loss: 0.00001620
Iteration 65/1000 | Loss: 0.00001620
Iteration 66/1000 | Loss: 0.00001620
Iteration 67/1000 | Loss: 0.00001620
Iteration 68/1000 | Loss: 0.00001619
Iteration 69/1000 | Loss: 0.00001619
Iteration 70/1000 | Loss: 0.00001619
Iteration 71/1000 | Loss: 0.00001619
Iteration 72/1000 | Loss: 0.00001619
Iteration 73/1000 | Loss: 0.00001619
Iteration 74/1000 | Loss: 0.00001618
Iteration 75/1000 | Loss: 0.00001618
Iteration 76/1000 | Loss: 0.00001617
Iteration 77/1000 | Loss: 0.00001616
Iteration 78/1000 | Loss: 0.00001616
Iteration 79/1000 | Loss: 0.00001616
Iteration 80/1000 | Loss: 0.00001616
Iteration 81/1000 | Loss: 0.00001616
Iteration 82/1000 | Loss: 0.00001616
Iteration 83/1000 | Loss: 0.00001615
Iteration 84/1000 | Loss: 0.00001615
Iteration 85/1000 | Loss: 0.00001615
Iteration 86/1000 | Loss: 0.00001615
Iteration 87/1000 | Loss: 0.00001615
Iteration 88/1000 | Loss: 0.00001614
Iteration 89/1000 | Loss: 0.00001614
Iteration 90/1000 | Loss: 0.00001614
Iteration 91/1000 | Loss: 0.00001614
Iteration 92/1000 | Loss: 0.00001614
Iteration 93/1000 | Loss: 0.00001614
Iteration 94/1000 | Loss: 0.00001613
Iteration 95/1000 | Loss: 0.00001613
Iteration 96/1000 | Loss: 0.00001613
Iteration 97/1000 | Loss: 0.00001613
Iteration 98/1000 | Loss: 0.00001613
Iteration 99/1000 | Loss: 0.00001613
Iteration 100/1000 | Loss: 0.00001613
Iteration 101/1000 | Loss: 0.00001613
Iteration 102/1000 | Loss: 0.00001613
Iteration 103/1000 | Loss: 0.00001613
Iteration 104/1000 | Loss: 0.00001613
Iteration 105/1000 | Loss: 0.00001613
Iteration 106/1000 | Loss: 0.00001613
Iteration 107/1000 | Loss: 0.00001613
Iteration 108/1000 | Loss: 0.00001613
Iteration 109/1000 | Loss: 0.00001613
Iteration 110/1000 | Loss: 0.00001613
Iteration 111/1000 | Loss: 0.00001613
Iteration 112/1000 | Loss: 0.00001613
Iteration 113/1000 | Loss: 0.00001613
Iteration 114/1000 | Loss: 0.00001613
Iteration 115/1000 | Loss: 0.00001613
Iteration 116/1000 | Loss: 0.00001613
Iteration 117/1000 | Loss: 0.00001612
Iteration 118/1000 | Loss: 0.00001612
Iteration 119/1000 | Loss: 0.00001612
Iteration 120/1000 | Loss: 0.00001612
Iteration 121/1000 | Loss: 0.00001612
Iteration 122/1000 | Loss: 0.00001612
Iteration 123/1000 | Loss: 0.00001612
Iteration 124/1000 | Loss: 0.00001612
Iteration 125/1000 | Loss: 0.00001612
Iteration 126/1000 | Loss: 0.00001612
Iteration 127/1000 | Loss: 0.00001612
Iteration 128/1000 | Loss: 0.00001612
Iteration 129/1000 | Loss: 0.00001612
Iteration 130/1000 | Loss: 0.00001612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.6124209651025012e-05, 1.6124209651025012e-05, 1.6124209651025012e-05, 1.6124209651025012e-05, 1.6124209651025012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6124209651025012e-05

Optimization complete. Final v2v error: 3.4463255405426025 mm

Highest mean error: 3.719545602798462 mm for frame 94

Lowest mean error: 3.2768499851226807 mm for frame 83

Saving results

Total time: 36.005213499069214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468731
Iteration 2/25 | Loss: 0.00136042
Iteration 3/25 | Loss: 0.00127737
Iteration 4/25 | Loss: 0.00126387
Iteration 5/25 | Loss: 0.00125874
Iteration 6/25 | Loss: 0.00125784
Iteration 7/25 | Loss: 0.00125784
Iteration 8/25 | Loss: 0.00125784
Iteration 9/25 | Loss: 0.00125784
Iteration 10/25 | Loss: 0.00125784
Iteration 11/25 | Loss: 0.00125784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012578405439853668, 0.0012578405439853668, 0.0012578405439853668, 0.0012578405439853668, 0.0012578405439853668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012578405439853668

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.04906273
Iteration 2/25 | Loss: 0.00150359
Iteration 3/25 | Loss: 0.00150358
Iteration 4/25 | Loss: 0.00150358
Iteration 5/25 | Loss: 0.00150358
Iteration 6/25 | Loss: 0.00150358
Iteration 7/25 | Loss: 0.00150358
Iteration 8/25 | Loss: 0.00150358
Iteration 9/25 | Loss: 0.00150358
Iteration 10/25 | Loss: 0.00150358
Iteration 11/25 | Loss: 0.00150358
Iteration 12/25 | Loss: 0.00150358
Iteration 13/25 | Loss: 0.00150358
Iteration 14/25 | Loss: 0.00150358
Iteration 15/25 | Loss: 0.00150358
Iteration 16/25 | Loss: 0.00150358
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015035764081403613, 0.0015035764081403613, 0.0015035764081403613, 0.0015035764081403613, 0.0015035764081403613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015035764081403613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150358
Iteration 2/1000 | Loss: 0.00002805
Iteration 3/1000 | Loss: 0.00002131
Iteration 4/1000 | Loss: 0.00001999
Iteration 5/1000 | Loss: 0.00001937
Iteration 6/1000 | Loss: 0.00001900
Iteration 7/1000 | Loss: 0.00001856
Iteration 8/1000 | Loss: 0.00001825
Iteration 9/1000 | Loss: 0.00001791
Iteration 10/1000 | Loss: 0.00001780
Iteration 11/1000 | Loss: 0.00001773
Iteration 12/1000 | Loss: 0.00001758
Iteration 13/1000 | Loss: 0.00001752
Iteration 14/1000 | Loss: 0.00001744
Iteration 15/1000 | Loss: 0.00001737
Iteration 16/1000 | Loss: 0.00001735
Iteration 17/1000 | Loss: 0.00001734
Iteration 18/1000 | Loss: 0.00001733
Iteration 19/1000 | Loss: 0.00001733
Iteration 20/1000 | Loss: 0.00001732
Iteration 21/1000 | Loss: 0.00001732
Iteration 22/1000 | Loss: 0.00001731
Iteration 23/1000 | Loss: 0.00001729
Iteration 24/1000 | Loss: 0.00001728
Iteration 25/1000 | Loss: 0.00001728
Iteration 26/1000 | Loss: 0.00001727
Iteration 27/1000 | Loss: 0.00001727
Iteration 28/1000 | Loss: 0.00001723
Iteration 29/1000 | Loss: 0.00001723
Iteration 30/1000 | Loss: 0.00001723
Iteration 31/1000 | Loss: 0.00001723
Iteration 32/1000 | Loss: 0.00001721
Iteration 33/1000 | Loss: 0.00001721
Iteration 34/1000 | Loss: 0.00001720
Iteration 35/1000 | Loss: 0.00001717
Iteration 36/1000 | Loss: 0.00001717
Iteration 37/1000 | Loss: 0.00001717
Iteration 38/1000 | Loss: 0.00001717
Iteration 39/1000 | Loss: 0.00001717
Iteration 40/1000 | Loss: 0.00001717
Iteration 41/1000 | Loss: 0.00001717
Iteration 42/1000 | Loss: 0.00001717
Iteration 43/1000 | Loss: 0.00001716
Iteration 44/1000 | Loss: 0.00001716
Iteration 45/1000 | Loss: 0.00001715
Iteration 46/1000 | Loss: 0.00001715
Iteration 47/1000 | Loss: 0.00001714
Iteration 48/1000 | Loss: 0.00001714
Iteration 49/1000 | Loss: 0.00001713
Iteration 50/1000 | Loss: 0.00001713
Iteration 51/1000 | Loss: 0.00001712
Iteration 52/1000 | Loss: 0.00001712
Iteration 53/1000 | Loss: 0.00001712
Iteration 54/1000 | Loss: 0.00001712
Iteration 55/1000 | Loss: 0.00001712
Iteration 56/1000 | Loss: 0.00001712
Iteration 57/1000 | Loss: 0.00001712
Iteration 58/1000 | Loss: 0.00001711
Iteration 59/1000 | Loss: 0.00001711
Iteration 60/1000 | Loss: 0.00001711
Iteration 61/1000 | Loss: 0.00001711
Iteration 62/1000 | Loss: 0.00001710
Iteration 63/1000 | Loss: 0.00001710
Iteration 64/1000 | Loss: 0.00001709
Iteration 65/1000 | Loss: 0.00001709
Iteration 66/1000 | Loss: 0.00001709
Iteration 67/1000 | Loss: 0.00001709
Iteration 68/1000 | Loss: 0.00001709
Iteration 69/1000 | Loss: 0.00001709
Iteration 70/1000 | Loss: 0.00001709
Iteration 71/1000 | Loss: 0.00001709
Iteration 72/1000 | Loss: 0.00001709
Iteration 73/1000 | Loss: 0.00001708
Iteration 74/1000 | Loss: 0.00001708
Iteration 75/1000 | Loss: 0.00001708
Iteration 76/1000 | Loss: 0.00001708
Iteration 77/1000 | Loss: 0.00001708
Iteration 78/1000 | Loss: 0.00001707
Iteration 79/1000 | Loss: 0.00001707
Iteration 80/1000 | Loss: 0.00001706
Iteration 81/1000 | Loss: 0.00001706
Iteration 82/1000 | Loss: 0.00001706
Iteration 83/1000 | Loss: 0.00001706
Iteration 84/1000 | Loss: 0.00001706
Iteration 85/1000 | Loss: 0.00001706
Iteration 86/1000 | Loss: 0.00001705
Iteration 87/1000 | Loss: 0.00001705
Iteration 88/1000 | Loss: 0.00001705
Iteration 89/1000 | Loss: 0.00001704
Iteration 90/1000 | Loss: 0.00001704
Iteration 91/1000 | Loss: 0.00001704
Iteration 92/1000 | Loss: 0.00001704
Iteration 93/1000 | Loss: 0.00001704
Iteration 94/1000 | Loss: 0.00001704
Iteration 95/1000 | Loss: 0.00001704
Iteration 96/1000 | Loss: 0.00001704
Iteration 97/1000 | Loss: 0.00001704
Iteration 98/1000 | Loss: 0.00001703
Iteration 99/1000 | Loss: 0.00001703
Iteration 100/1000 | Loss: 0.00001703
Iteration 101/1000 | Loss: 0.00001703
Iteration 102/1000 | Loss: 0.00001703
Iteration 103/1000 | Loss: 0.00001703
Iteration 104/1000 | Loss: 0.00001702
Iteration 105/1000 | Loss: 0.00001702
Iteration 106/1000 | Loss: 0.00001702
Iteration 107/1000 | Loss: 0.00001702
Iteration 108/1000 | Loss: 0.00001702
Iteration 109/1000 | Loss: 0.00001701
Iteration 110/1000 | Loss: 0.00001701
Iteration 111/1000 | Loss: 0.00001700
Iteration 112/1000 | Loss: 0.00001700
Iteration 113/1000 | Loss: 0.00001699
Iteration 114/1000 | Loss: 0.00001699
Iteration 115/1000 | Loss: 0.00001699
Iteration 116/1000 | Loss: 0.00001699
Iteration 117/1000 | Loss: 0.00001699
Iteration 118/1000 | Loss: 0.00001698
Iteration 119/1000 | Loss: 0.00001698
Iteration 120/1000 | Loss: 0.00001698
Iteration 121/1000 | Loss: 0.00001698
Iteration 122/1000 | Loss: 0.00001698
Iteration 123/1000 | Loss: 0.00001698
Iteration 124/1000 | Loss: 0.00001698
Iteration 125/1000 | Loss: 0.00001698
Iteration 126/1000 | Loss: 0.00001698
Iteration 127/1000 | Loss: 0.00001698
Iteration 128/1000 | Loss: 0.00001698
Iteration 129/1000 | Loss: 0.00001698
Iteration 130/1000 | Loss: 0.00001698
Iteration 131/1000 | Loss: 0.00001698
Iteration 132/1000 | Loss: 0.00001698
Iteration 133/1000 | Loss: 0.00001698
Iteration 134/1000 | Loss: 0.00001698
Iteration 135/1000 | Loss: 0.00001698
Iteration 136/1000 | Loss: 0.00001698
Iteration 137/1000 | Loss: 0.00001698
Iteration 138/1000 | Loss: 0.00001698
Iteration 139/1000 | Loss: 0.00001698
Iteration 140/1000 | Loss: 0.00001698
Iteration 141/1000 | Loss: 0.00001698
Iteration 142/1000 | Loss: 0.00001698
Iteration 143/1000 | Loss: 0.00001698
Iteration 144/1000 | Loss: 0.00001698
Iteration 145/1000 | Loss: 0.00001698
Iteration 146/1000 | Loss: 0.00001698
Iteration 147/1000 | Loss: 0.00001698
Iteration 148/1000 | Loss: 0.00001698
Iteration 149/1000 | Loss: 0.00001698
Iteration 150/1000 | Loss: 0.00001698
Iteration 151/1000 | Loss: 0.00001698
Iteration 152/1000 | Loss: 0.00001698
Iteration 153/1000 | Loss: 0.00001698
Iteration 154/1000 | Loss: 0.00001698
Iteration 155/1000 | Loss: 0.00001698
Iteration 156/1000 | Loss: 0.00001698
Iteration 157/1000 | Loss: 0.00001698
Iteration 158/1000 | Loss: 0.00001698
Iteration 159/1000 | Loss: 0.00001698
Iteration 160/1000 | Loss: 0.00001698
Iteration 161/1000 | Loss: 0.00001698
Iteration 162/1000 | Loss: 0.00001698
Iteration 163/1000 | Loss: 0.00001698
Iteration 164/1000 | Loss: 0.00001698
Iteration 165/1000 | Loss: 0.00001698
Iteration 166/1000 | Loss: 0.00001698
Iteration 167/1000 | Loss: 0.00001698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.6976040569716133e-05, 1.6976040569716133e-05, 1.6976040569716133e-05, 1.6976040569716133e-05, 1.6976040569716133e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6976040569716133e-05

Optimization complete. Final v2v error: 3.4809539318084717 mm

Highest mean error: 4.196833610534668 mm for frame 187

Lowest mean error: 3.3502299785614014 mm for frame 126

Saving results

Total time: 39.09474062919617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00516881
Iteration 2/25 | Loss: 0.00131032
Iteration 3/25 | Loss: 0.00123887
Iteration 4/25 | Loss: 0.00122861
Iteration 5/25 | Loss: 0.00122712
Iteration 6/25 | Loss: 0.00122712
Iteration 7/25 | Loss: 0.00122712
Iteration 8/25 | Loss: 0.00122712
Iteration 9/25 | Loss: 0.00122712
Iteration 10/25 | Loss: 0.00122712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001227120985276997, 0.001227120985276997, 0.001227120985276997, 0.001227120985276997, 0.001227120985276997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001227120985276997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.69877529
Iteration 2/25 | Loss: 0.00149626
Iteration 3/25 | Loss: 0.00149624
Iteration 4/25 | Loss: 0.00149624
Iteration 5/25 | Loss: 0.00149624
Iteration 6/25 | Loss: 0.00149624
Iteration 7/25 | Loss: 0.00149624
Iteration 8/25 | Loss: 0.00149624
Iteration 9/25 | Loss: 0.00149624
Iteration 10/25 | Loss: 0.00149624
Iteration 11/25 | Loss: 0.00149624
Iteration 12/25 | Loss: 0.00149624
Iteration 13/25 | Loss: 0.00149624
Iteration 14/25 | Loss: 0.00149624
Iteration 15/25 | Loss: 0.00149624
Iteration 16/25 | Loss: 0.00149624
Iteration 17/25 | Loss: 0.00149624
Iteration 18/25 | Loss: 0.00149624
Iteration 19/25 | Loss: 0.00149624
Iteration 20/25 | Loss: 0.00149624
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014962388668209314, 0.0014962388668209314, 0.0014962388668209314, 0.0014962388668209314, 0.0014962388668209314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014962388668209314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149624
Iteration 2/1000 | Loss: 0.00002611
Iteration 3/1000 | Loss: 0.00001918
Iteration 4/1000 | Loss: 0.00001788
Iteration 5/1000 | Loss: 0.00001672
Iteration 6/1000 | Loss: 0.00001610
Iteration 7/1000 | Loss: 0.00001560
Iteration 8/1000 | Loss: 0.00001510
Iteration 9/1000 | Loss: 0.00001474
Iteration 10/1000 | Loss: 0.00001448
Iteration 11/1000 | Loss: 0.00001421
Iteration 12/1000 | Loss: 0.00001401
Iteration 13/1000 | Loss: 0.00001387
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001372
Iteration 16/1000 | Loss: 0.00001371
Iteration 17/1000 | Loss: 0.00001370
Iteration 18/1000 | Loss: 0.00001369
Iteration 19/1000 | Loss: 0.00001368
Iteration 20/1000 | Loss: 0.00001367
Iteration 21/1000 | Loss: 0.00001362
Iteration 22/1000 | Loss: 0.00001360
Iteration 23/1000 | Loss: 0.00001359
Iteration 24/1000 | Loss: 0.00001359
Iteration 25/1000 | Loss: 0.00001358
Iteration 26/1000 | Loss: 0.00001358
Iteration 27/1000 | Loss: 0.00001357
Iteration 28/1000 | Loss: 0.00001357
Iteration 29/1000 | Loss: 0.00001357
Iteration 30/1000 | Loss: 0.00001356
Iteration 31/1000 | Loss: 0.00001356
Iteration 32/1000 | Loss: 0.00001355
Iteration 33/1000 | Loss: 0.00001355
Iteration 34/1000 | Loss: 0.00001354
Iteration 35/1000 | Loss: 0.00001352
Iteration 36/1000 | Loss: 0.00001352
Iteration 37/1000 | Loss: 0.00001352
Iteration 38/1000 | Loss: 0.00001351
Iteration 39/1000 | Loss: 0.00001351
Iteration 40/1000 | Loss: 0.00001350
Iteration 41/1000 | Loss: 0.00001350
Iteration 42/1000 | Loss: 0.00001350
Iteration 43/1000 | Loss: 0.00001349
Iteration 44/1000 | Loss: 0.00001349
Iteration 45/1000 | Loss: 0.00001348
Iteration 46/1000 | Loss: 0.00001348
Iteration 47/1000 | Loss: 0.00001348
Iteration 48/1000 | Loss: 0.00001347
Iteration 49/1000 | Loss: 0.00001347
Iteration 50/1000 | Loss: 0.00001347
Iteration 51/1000 | Loss: 0.00001346
Iteration 52/1000 | Loss: 0.00001346
Iteration 53/1000 | Loss: 0.00001346
Iteration 54/1000 | Loss: 0.00001346
Iteration 55/1000 | Loss: 0.00001346
Iteration 56/1000 | Loss: 0.00001346
Iteration 57/1000 | Loss: 0.00001345
Iteration 58/1000 | Loss: 0.00001345
Iteration 59/1000 | Loss: 0.00001345
Iteration 60/1000 | Loss: 0.00001344
Iteration 61/1000 | Loss: 0.00001344
Iteration 62/1000 | Loss: 0.00001344
Iteration 63/1000 | Loss: 0.00001344
Iteration 64/1000 | Loss: 0.00001344
Iteration 65/1000 | Loss: 0.00001343
Iteration 66/1000 | Loss: 0.00001343
Iteration 67/1000 | Loss: 0.00001343
Iteration 68/1000 | Loss: 0.00001343
Iteration 69/1000 | Loss: 0.00001343
Iteration 70/1000 | Loss: 0.00001343
Iteration 71/1000 | Loss: 0.00001342
Iteration 72/1000 | Loss: 0.00001342
Iteration 73/1000 | Loss: 0.00001342
Iteration 74/1000 | Loss: 0.00001341
Iteration 75/1000 | Loss: 0.00001341
Iteration 76/1000 | Loss: 0.00001341
Iteration 77/1000 | Loss: 0.00001340
Iteration 78/1000 | Loss: 0.00001340
Iteration 79/1000 | Loss: 0.00001340
Iteration 80/1000 | Loss: 0.00001340
Iteration 81/1000 | Loss: 0.00001340
Iteration 82/1000 | Loss: 0.00001339
Iteration 83/1000 | Loss: 0.00001339
Iteration 84/1000 | Loss: 0.00001339
Iteration 85/1000 | Loss: 0.00001339
Iteration 86/1000 | Loss: 0.00001339
Iteration 87/1000 | Loss: 0.00001339
Iteration 88/1000 | Loss: 0.00001338
Iteration 89/1000 | Loss: 0.00001338
Iteration 90/1000 | Loss: 0.00001338
Iteration 91/1000 | Loss: 0.00001338
Iteration 92/1000 | Loss: 0.00001338
Iteration 93/1000 | Loss: 0.00001338
Iteration 94/1000 | Loss: 0.00001338
Iteration 95/1000 | Loss: 0.00001338
Iteration 96/1000 | Loss: 0.00001338
Iteration 97/1000 | Loss: 0.00001338
Iteration 98/1000 | Loss: 0.00001338
Iteration 99/1000 | Loss: 0.00001338
Iteration 100/1000 | Loss: 0.00001338
Iteration 101/1000 | Loss: 0.00001338
Iteration 102/1000 | Loss: 0.00001337
Iteration 103/1000 | Loss: 0.00001337
Iteration 104/1000 | Loss: 0.00001337
Iteration 105/1000 | Loss: 0.00001337
Iteration 106/1000 | Loss: 0.00001337
Iteration 107/1000 | Loss: 0.00001337
Iteration 108/1000 | Loss: 0.00001337
Iteration 109/1000 | Loss: 0.00001337
Iteration 110/1000 | Loss: 0.00001337
Iteration 111/1000 | Loss: 0.00001337
Iteration 112/1000 | Loss: 0.00001337
Iteration 113/1000 | Loss: 0.00001337
Iteration 114/1000 | Loss: 0.00001337
Iteration 115/1000 | Loss: 0.00001337
Iteration 116/1000 | Loss: 0.00001337
Iteration 117/1000 | Loss: 0.00001336
Iteration 118/1000 | Loss: 0.00001336
Iteration 119/1000 | Loss: 0.00001336
Iteration 120/1000 | Loss: 0.00001336
Iteration 121/1000 | Loss: 0.00001336
Iteration 122/1000 | Loss: 0.00001336
Iteration 123/1000 | Loss: 0.00001336
Iteration 124/1000 | Loss: 0.00001336
Iteration 125/1000 | Loss: 0.00001335
Iteration 126/1000 | Loss: 0.00001335
Iteration 127/1000 | Loss: 0.00001335
Iteration 128/1000 | Loss: 0.00001335
Iteration 129/1000 | Loss: 0.00001335
Iteration 130/1000 | Loss: 0.00001335
Iteration 131/1000 | Loss: 0.00001335
Iteration 132/1000 | Loss: 0.00001335
Iteration 133/1000 | Loss: 0.00001335
Iteration 134/1000 | Loss: 0.00001334
Iteration 135/1000 | Loss: 0.00001334
Iteration 136/1000 | Loss: 0.00001334
Iteration 137/1000 | Loss: 0.00001334
Iteration 138/1000 | Loss: 0.00001334
Iteration 139/1000 | Loss: 0.00001334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.3343897080630995e-05, 1.3343897080630995e-05, 1.3343897080630995e-05, 1.3343897080630995e-05, 1.3343897080630995e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3343897080630995e-05

Optimization complete. Final v2v error: 3.1134228706359863 mm

Highest mean error: 3.4129505157470703 mm for frame 125

Lowest mean error: 2.8713037967681885 mm for frame 255

Saving results

Total time: 43.459755420684814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473081
Iteration 2/25 | Loss: 0.00132387
Iteration 3/25 | Loss: 0.00123550
Iteration 4/25 | Loss: 0.00122447
Iteration 5/25 | Loss: 0.00122271
Iteration 6/25 | Loss: 0.00122271
Iteration 7/25 | Loss: 0.00122271
Iteration 8/25 | Loss: 0.00122271
Iteration 9/25 | Loss: 0.00122271
Iteration 10/25 | Loss: 0.00122271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012227068655192852, 0.0012227068655192852, 0.0012227068655192852, 0.0012227068655192852, 0.0012227068655192852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012227068655192852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.92915201
Iteration 2/25 | Loss: 0.00124877
Iteration 3/25 | Loss: 0.00124876
Iteration 4/25 | Loss: 0.00124876
Iteration 5/25 | Loss: 0.00124876
Iteration 6/25 | Loss: 0.00124876
Iteration 7/25 | Loss: 0.00124876
Iteration 8/25 | Loss: 0.00124876
Iteration 9/25 | Loss: 0.00124876
Iteration 10/25 | Loss: 0.00124876
Iteration 11/25 | Loss: 0.00124876
Iteration 12/25 | Loss: 0.00124876
Iteration 13/25 | Loss: 0.00124876
Iteration 14/25 | Loss: 0.00124876
Iteration 15/25 | Loss: 0.00124876
Iteration 16/25 | Loss: 0.00124876
Iteration 17/25 | Loss: 0.00124876
Iteration 18/25 | Loss: 0.00124876
Iteration 19/25 | Loss: 0.00124876
Iteration 20/25 | Loss: 0.00124876
Iteration 21/25 | Loss: 0.00124876
Iteration 22/25 | Loss: 0.00124876
Iteration 23/25 | Loss: 0.00124876
Iteration 24/25 | Loss: 0.00124876
Iteration 25/25 | Loss: 0.00124876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124876
Iteration 2/1000 | Loss: 0.00002387
Iteration 3/1000 | Loss: 0.00001833
Iteration 4/1000 | Loss: 0.00001668
Iteration 5/1000 | Loss: 0.00001569
Iteration 6/1000 | Loss: 0.00001509
Iteration 7/1000 | Loss: 0.00001454
Iteration 8/1000 | Loss: 0.00001410
Iteration 9/1000 | Loss: 0.00001374
Iteration 10/1000 | Loss: 0.00001352
Iteration 11/1000 | Loss: 0.00001338
Iteration 12/1000 | Loss: 0.00001316
Iteration 13/1000 | Loss: 0.00001307
Iteration 14/1000 | Loss: 0.00001306
Iteration 15/1000 | Loss: 0.00001293
Iteration 16/1000 | Loss: 0.00001275
Iteration 17/1000 | Loss: 0.00001265
Iteration 18/1000 | Loss: 0.00001264
Iteration 19/1000 | Loss: 0.00001259
Iteration 20/1000 | Loss: 0.00001253
Iteration 21/1000 | Loss: 0.00001251
Iteration 22/1000 | Loss: 0.00001250
Iteration 23/1000 | Loss: 0.00001250
Iteration 24/1000 | Loss: 0.00001250
Iteration 25/1000 | Loss: 0.00001249
Iteration 26/1000 | Loss: 0.00001242
Iteration 27/1000 | Loss: 0.00001241
Iteration 28/1000 | Loss: 0.00001230
Iteration 29/1000 | Loss: 0.00001229
Iteration 30/1000 | Loss: 0.00001227
Iteration 31/1000 | Loss: 0.00001227
Iteration 32/1000 | Loss: 0.00001226
Iteration 33/1000 | Loss: 0.00001226
Iteration 34/1000 | Loss: 0.00001226
Iteration 35/1000 | Loss: 0.00001225
Iteration 36/1000 | Loss: 0.00001225
Iteration 37/1000 | Loss: 0.00001224
Iteration 38/1000 | Loss: 0.00001224
Iteration 39/1000 | Loss: 0.00001223
Iteration 40/1000 | Loss: 0.00001223
Iteration 41/1000 | Loss: 0.00001222
Iteration 42/1000 | Loss: 0.00001222
Iteration 43/1000 | Loss: 0.00001222
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001221
Iteration 46/1000 | Loss: 0.00001221
Iteration 47/1000 | Loss: 0.00001221
Iteration 48/1000 | Loss: 0.00001220
Iteration 49/1000 | Loss: 0.00001219
Iteration 50/1000 | Loss: 0.00001219
Iteration 51/1000 | Loss: 0.00001219
Iteration 52/1000 | Loss: 0.00001219
Iteration 53/1000 | Loss: 0.00001219
Iteration 54/1000 | Loss: 0.00001218
Iteration 55/1000 | Loss: 0.00001218
Iteration 56/1000 | Loss: 0.00001218
Iteration 57/1000 | Loss: 0.00001218
Iteration 58/1000 | Loss: 0.00001218
Iteration 59/1000 | Loss: 0.00001218
Iteration 60/1000 | Loss: 0.00001218
Iteration 61/1000 | Loss: 0.00001218
Iteration 62/1000 | Loss: 0.00001217
Iteration 63/1000 | Loss: 0.00001217
Iteration 64/1000 | Loss: 0.00001215
Iteration 65/1000 | Loss: 0.00001215
Iteration 66/1000 | Loss: 0.00001214
Iteration 67/1000 | Loss: 0.00001214
Iteration 68/1000 | Loss: 0.00001214
Iteration 69/1000 | Loss: 0.00001214
Iteration 70/1000 | Loss: 0.00001214
Iteration 71/1000 | Loss: 0.00001214
Iteration 72/1000 | Loss: 0.00001214
Iteration 73/1000 | Loss: 0.00001213
Iteration 74/1000 | Loss: 0.00001213
Iteration 75/1000 | Loss: 0.00001213
Iteration 76/1000 | Loss: 0.00001213
Iteration 77/1000 | Loss: 0.00001213
Iteration 78/1000 | Loss: 0.00001213
Iteration 79/1000 | Loss: 0.00001213
Iteration 80/1000 | Loss: 0.00001213
Iteration 81/1000 | Loss: 0.00001212
Iteration 82/1000 | Loss: 0.00001212
Iteration 83/1000 | Loss: 0.00001212
Iteration 84/1000 | Loss: 0.00001212
Iteration 85/1000 | Loss: 0.00001212
Iteration 86/1000 | Loss: 0.00001212
Iteration 87/1000 | Loss: 0.00001212
Iteration 88/1000 | Loss: 0.00001211
Iteration 89/1000 | Loss: 0.00001211
Iteration 90/1000 | Loss: 0.00001211
Iteration 91/1000 | Loss: 0.00001211
Iteration 92/1000 | Loss: 0.00001210
Iteration 93/1000 | Loss: 0.00001210
Iteration 94/1000 | Loss: 0.00001210
Iteration 95/1000 | Loss: 0.00001210
Iteration 96/1000 | Loss: 0.00001210
Iteration 97/1000 | Loss: 0.00001210
Iteration 98/1000 | Loss: 0.00001210
Iteration 99/1000 | Loss: 0.00001210
Iteration 100/1000 | Loss: 0.00001210
Iteration 101/1000 | Loss: 0.00001209
Iteration 102/1000 | Loss: 0.00001209
Iteration 103/1000 | Loss: 0.00001209
Iteration 104/1000 | Loss: 0.00001209
Iteration 105/1000 | Loss: 0.00001209
Iteration 106/1000 | Loss: 0.00001209
Iteration 107/1000 | Loss: 0.00001209
Iteration 108/1000 | Loss: 0.00001209
Iteration 109/1000 | Loss: 0.00001208
Iteration 110/1000 | Loss: 0.00001208
Iteration 111/1000 | Loss: 0.00001208
Iteration 112/1000 | Loss: 0.00001208
Iteration 113/1000 | Loss: 0.00001208
Iteration 114/1000 | Loss: 0.00001208
Iteration 115/1000 | Loss: 0.00001208
Iteration 116/1000 | Loss: 0.00001208
Iteration 117/1000 | Loss: 0.00001208
Iteration 118/1000 | Loss: 0.00001208
Iteration 119/1000 | Loss: 0.00001208
Iteration 120/1000 | Loss: 0.00001208
Iteration 121/1000 | Loss: 0.00001208
Iteration 122/1000 | Loss: 0.00001208
Iteration 123/1000 | Loss: 0.00001208
Iteration 124/1000 | Loss: 0.00001208
Iteration 125/1000 | Loss: 0.00001208
Iteration 126/1000 | Loss: 0.00001208
Iteration 127/1000 | Loss: 0.00001208
Iteration 128/1000 | Loss: 0.00001208
Iteration 129/1000 | Loss: 0.00001208
Iteration 130/1000 | Loss: 0.00001208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.2078859072062187e-05, 1.2078859072062187e-05, 1.2078859072062187e-05, 1.2078859072062187e-05, 1.2078859072062187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2078859072062187e-05

Optimization complete. Final v2v error: 2.991053581237793 mm

Highest mean error: 3.2814369201660156 mm for frame 211

Lowest mean error: 2.844607353210449 mm for frame 47

Saving results

Total time: 41.505468130111694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043161
Iteration 2/25 | Loss: 0.00174879
Iteration 3/25 | Loss: 0.00138986
Iteration 4/25 | Loss: 0.00136052
Iteration 5/25 | Loss: 0.00135347
Iteration 6/25 | Loss: 0.00135159
Iteration 7/25 | Loss: 0.00135084
Iteration 8/25 | Loss: 0.00135192
Iteration 9/25 | Loss: 0.00135125
Iteration 10/25 | Loss: 0.00135001
Iteration 11/25 | Loss: 0.00134965
Iteration 12/25 | Loss: 0.00134944
Iteration 13/25 | Loss: 0.00134943
Iteration 14/25 | Loss: 0.00134943
Iteration 15/25 | Loss: 0.00134943
Iteration 16/25 | Loss: 0.00134943
Iteration 17/25 | Loss: 0.00134943
Iteration 18/25 | Loss: 0.00134943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001349428202956915, 0.001349428202956915, 0.001349428202956915, 0.001349428202956915, 0.001349428202956915]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001349428202956915

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30673242
Iteration 2/25 | Loss: 0.00215170
Iteration 3/25 | Loss: 0.00215170
Iteration 4/25 | Loss: 0.00215170
Iteration 5/25 | Loss: 0.00215170
Iteration 6/25 | Loss: 0.00215170
Iteration 7/25 | Loss: 0.00215170
Iteration 8/25 | Loss: 0.00215170
Iteration 9/25 | Loss: 0.00215170
Iteration 10/25 | Loss: 0.00215170
Iteration 11/25 | Loss: 0.00215170
Iteration 12/25 | Loss: 0.00215170
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0021516988053917885, 0.0021516988053917885, 0.0021516988053917885, 0.0021516988053917885, 0.0021516988053917885]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021516988053917885

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215170
Iteration 2/1000 | Loss: 0.00006088
Iteration 3/1000 | Loss: 0.00004020
Iteration 4/1000 | Loss: 0.00003405
Iteration 5/1000 | Loss: 0.00003204
Iteration 6/1000 | Loss: 0.00003063
Iteration 7/1000 | Loss: 0.00002990
Iteration 8/1000 | Loss: 0.00002925
Iteration 9/1000 | Loss: 0.00002894
Iteration 10/1000 | Loss: 0.00002859
Iteration 11/1000 | Loss: 0.00002833
Iteration 12/1000 | Loss: 0.00002820
Iteration 13/1000 | Loss: 0.00002812
Iteration 14/1000 | Loss: 0.00002790
Iteration 15/1000 | Loss: 0.00002790
Iteration 16/1000 | Loss: 0.00002778
Iteration 17/1000 | Loss: 0.00002777
Iteration 18/1000 | Loss: 0.00002776
Iteration 19/1000 | Loss: 0.00002775
Iteration 20/1000 | Loss: 0.00002773
Iteration 21/1000 | Loss: 0.00002773
Iteration 22/1000 | Loss: 0.00002767
Iteration 23/1000 | Loss: 0.00002763
Iteration 24/1000 | Loss: 0.00002762
Iteration 25/1000 | Loss: 0.00002761
Iteration 26/1000 | Loss: 0.00002761
Iteration 27/1000 | Loss: 0.00002760
Iteration 28/1000 | Loss: 0.00002760
Iteration 29/1000 | Loss: 0.00002759
Iteration 30/1000 | Loss: 0.00002759
Iteration 31/1000 | Loss: 0.00002759
Iteration 32/1000 | Loss: 0.00002758
Iteration 33/1000 | Loss: 0.00002756
Iteration 34/1000 | Loss: 0.00002756
Iteration 35/1000 | Loss: 0.00002756
Iteration 36/1000 | Loss: 0.00002756
Iteration 37/1000 | Loss: 0.00002756
Iteration 38/1000 | Loss: 0.00002756
Iteration 39/1000 | Loss: 0.00002756
Iteration 40/1000 | Loss: 0.00002756
Iteration 41/1000 | Loss: 0.00002756
Iteration 42/1000 | Loss: 0.00002755
Iteration 43/1000 | Loss: 0.00002754
Iteration 44/1000 | Loss: 0.00002754
Iteration 45/1000 | Loss: 0.00002754
Iteration 46/1000 | Loss: 0.00002753
Iteration 47/1000 | Loss: 0.00002753
Iteration 48/1000 | Loss: 0.00002753
Iteration 49/1000 | Loss: 0.00002753
Iteration 50/1000 | Loss: 0.00002752
Iteration 51/1000 | Loss: 0.00002752
Iteration 52/1000 | Loss: 0.00002752
Iteration 53/1000 | Loss: 0.00002752
Iteration 54/1000 | Loss: 0.00002752
Iteration 55/1000 | Loss: 0.00002752
Iteration 56/1000 | Loss: 0.00002751
Iteration 57/1000 | Loss: 0.00002751
Iteration 58/1000 | Loss: 0.00002751
Iteration 59/1000 | Loss: 0.00002751
Iteration 60/1000 | Loss: 0.00002751
Iteration 61/1000 | Loss: 0.00002751
Iteration 62/1000 | Loss: 0.00002751
Iteration 63/1000 | Loss: 0.00002750
Iteration 64/1000 | Loss: 0.00002750
Iteration 65/1000 | Loss: 0.00002750
Iteration 66/1000 | Loss: 0.00002750
Iteration 67/1000 | Loss: 0.00002750
Iteration 68/1000 | Loss: 0.00002749
Iteration 69/1000 | Loss: 0.00002749
Iteration 70/1000 | Loss: 0.00002749
Iteration 71/1000 | Loss: 0.00002749
Iteration 72/1000 | Loss: 0.00002749
Iteration 73/1000 | Loss: 0.00002749
Iteration 74/1000 | Loss: 0.00002748
Iteration 75/1000 | Loss: 0.00002748
Iteration 76/1000 | Loss: 0.00002748
Iteration 77/1000 | Loss: 0.00002748
Iteration 78/1000 | Loss: 0.00002748
Iteration 79/1000 | Loss: 0.00002748
Iteration 80/1000 | Loss: 0.00002748
Iteration 81/1000 | Loss: 0.00002748
Iteration 82/1000 | Loss: 0.00002748
Iteration 83/1000 | Loss: 0.00002748
Iteration 84/1000 | Loss: 0.00002748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [2.7479638447402976e-05, 2.7479638447402976e-05, 2.7479638447402976e-05, 2.7479638447402976e-05, 2.7479638447402976e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7479638447402976e-05

Optimization complete. Final v2v error: 4.400700569152832 mm

Highest mean error: 4.760463237762451 mm for frame 106

Lowest mean error: 3.428907632827759 mm for frame 8

Saving results

Total time: 52.57452178001404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022780
Iteration 2/25 | Loss: 0.00179223
Iteration 3/25 | Loss: 0.00144478
Iteration 4/25 | Loss: 0.00144939
Iteration 5/25 | Loss: 0.00138728
Iteration 6/25 | Loss: 0.00133749
Iteration 7/25 | Loss: 0.00130782
Iteration 8/25 | Loss: 0.00125508
Iteration 9/25 | Loss: 0.00124266
Iteration 10/25 | Loss: 0.00128198
Iteration 11/25 | Loss: 0.00124138
Iteration 12/25 | Loss: 0.00122991
Iteration 13/25 | Loss: 0.00122153
Iteration 14/25 | Loss: 0.00121882
Iteration 15/25 | Loss: 0.00121704
Iteration 16/25 | Loss: 0.00121613
Iteration 17/25 | Loss: 0.00121585
Iteration 18/25 | Loss: 0.00121573
Iteration 19/25 | Loss: 0.00121559
Iteration 20/25 | Loss: 0.00121544
Iteration 21/25 | Loss: 0.00121538
Iteration 22/25 | Loss: 0.00121537
Iteration 23/25 | Loss: 0.00121537
Iteration 24/25 | Loss: 0.00121537
Iteration 25/25 | Loss: 0.00121537

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46868026
Iteration 2/25 | Loss: 0.00156139
Iteration 3/25 | Loss: 0.00156139
Iteration 4/25 | Loss: 0.00156139
Iteration 5/25 | Loss: 0.00156139
Iteration 6/25 | Loss: 0.00156139
Iteration 7/25 | Loss: 0.00156139
Iteration 8/25 | Loss: 0.00156139
Iteration 9/25 | Loss: 0.00156139
Iteration 10/25 | Loss: 0.00156139
Iteration 11/25 | Loss: 0.00156139
Iteration 12/25 | Loss: 0.00156138
Iteration 13/25 | Loss: 0.00156138
Iteration 14/25 | Loss: 0.00156138
Iteration 15/25 | Loss: 0.00156138
Iteration 16/25 | Loss: 0.00156138
Iteration 17/25 | Loss: 0.00156138
Iteration 18/25 | Loss: 0.00156138
Iteration 19/25 | Loss: 0.00156138
Iteration 20/25 | Loss: 0.00156138
Iteration 21/25 | Loss: 0.00156138
Iteration 22/25 | Loss: 0.00156138
Iteration 23/25 | Loss: 0.00156138
Iteration 24/25 | Loss: 0.00156138
Iteration 25/25 | Loss: 0.00156138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156138
Iteration 2/1000 | Loss: 0.00002643
Iteration 3/1000 | Loss: 0.00001795
Iteration 4/1000 | Loss: 0.00001517
Iteration 5/1000 | Loss: 0.00029341
Iteration 6/1000 | Loss: 0.00001454
Iteration 7/1000 | Loss: 0.00002754
Iteration 8/1000 | Loss: 0.00054432
Iteration 9/1000 | Loss: 0.00001835
Iteration 10/1000 | Loss: 0.00014992
Iteration 11/1000 | Loss: 0.00001374
Iteration 12/1000 | Loss: 0.00001322
Iteration 13/1000 | Loss: 0.00003506
Iteration 14/1000 | Loss: 0.00001280
Iteration 15/1000 | Loss: 0.00001263
Iteration 16/1000 | Loss: 0.00001259
Iteration 17/1000 | Loss: 0.00001245
Iteration 18/1000 | Loss: 0.00001242
Iteration 19/1000 | Loss: 0.00001231
Iteration 20/1000 | Loss: 0.00001227
Iteration 21/1000 | Loss: 0.00026166
Iteration 22/1000 | Loss: 0.00001988
Iteration 23/1000 | Loss: 0.00001563
Iteration 24/1000 | Loss: 0.00001462
Iteration 25/1000 | Loss: 0.00001364
Iteration 26/1000 | Loss: 0.00001331
Iteration 27/1000 | Loss: 0.00001322
Iteration 28/1000 | Loss: 0.00029974
Iteration 29/1000 | Loss: 0.00024976
Iteration 30/1000 | Loss: 0.00003404
Iteration 31/1000 | Loss: 0.00001335
Iteration 32/1000 | Loss: 0.00001234
Iteration 33/1000 | Loss: 0.00001213
Iteration 34/1000 | Loss: 0.00001209
Iteration 35/1000 | Loss: 0.00001208
Iteration 36/1000 | Loss: 0.00001206
Iteration 37/1000 | Loss: 0.00001204
Iteration 38/1000 | Loss: 0.00001203
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001197
Iteration 41/1000 | Loss: 0.00001196
Iteration 42/1000 | Loss: 0.00001195
Iteration 43/1000 | Loss: 0.00001194
Iteration 44/1000 | Loss: 0.00001189
Iteration 45/1000 | Loss: 0.00001185
Iteration 46/1000 | Loss: 0.00001182
Iteration 47/1000 | Loss: 0.00001181
Iteration 48/1000 | Loss: 0.00001180
Iteration 49/1000 | Loss: 0.00001180
Iteration 50/1000 | Loss: 0.00001180
Iteration 51/1000 | Loss: 0.00001180
Iteration 52/1000 | Loss: 0.00001180
Iteration 53/1000 | Loss: 0.00001180
Iteration 54/1000 | Loss: 0.00001180
Iteration 55/1000 | Loss: 0.00001179
Iteration 56/1000 | Loss: 0.00001179
Iteration 57/1000 | Loss: 0.00001178
Iteration 58/1000 | Loss: 0.00001178
Iteration 59/1000 | Loss: 0.00001178
Iteration 60/1000 | Loss: 0.00001177
Iteration 61/1000 | Loss: 0.00001177
Iteration 62/1000 | Loss: 0.00001177
Iteration 63/1000 | Loss: 0.00001176
Iteration 64/1000 | Loss: 0.00001176
Iteration 65/1000 | Loss: 0.00001176
Iteration 66/1000 | Loss: 0.00001174
Iteration 67/1000 | Loss: 0.00001174
Iteration 68/1000 | Loss: 0.00001174
Iteration 69/1000 | Loss: 0.00001174
Iteration 70/1000 | Loss: 0.00001174
Iteration 71/1000 | Loss: 0.00001173
Iteration 72/1000 | Loss: 0.00001173
Iteration 73/1000 | Loss: 0.00001172
Iteration 74/1000 | Loss: 0.00001172
Iteration 75/1000 | Loss: 0.00001172
Iteration 76/1000 | Loss: 0.00001172
Iteration 77/1000 | Loss: 0.00001172
Iteration 78/1000 | Loss: 0.00001172
Iteration 79/1000 | Loss: 0.00001172
Iteration 80/1000 | Loss: 0.00001171
Iteration 81/1000 | Loss: 0.00001171
Iteration 82/1000 | Loss: 0.00001171
Iteration 83/1000 | Loss: 0.00001171
Iteration 84/1000 | Loss: 0.00001171
Iteration 85/1000 | Loss: 0.00001171
Iteration 86/1000 | Loss: 0.00001171
Iteration 87/1000 | Loss: 0.00001171
Iteration 88/1000 | Loss: 0.00001170
Iteration 89/1000 | Loss: 0.00001170
Iteration 90/1000 | Loss: 0.00001170
Iteration 91/1000 | Loss: 0.00001170
Iteration 92/1000 | Loss: 0.00001170
Iteration 93/1000 | Loss: 0.00001170
Iteration 94/1000 | Loss: 0.00001170
Iteration 95/1000 | Loss: 0.00001170
Iteration 96/1000 | Loss: 0.00001170
Iteration 97/1000 | Loss: 0.00001170
Iteration 98/1000 | Loss: 0.00001170
Iteration 99/1000 | Loss: 0.00001170
Iteration 100/1000 | Loss: 0.00001170
Iteration 101/1000 | Loss: 0.00001170
Iteration 102/1000 | Loss: 0.00001170
Iteration 103/1000 | Loss: 0.00001169
Iteration 104/1000 | Loss: 0.00001169
Iteration 105/1000 | Loss: 0.00001169
Iteration 106/1000 | Loss: 0.00001169
Iteration 107/1000 | Loss: 0.00001169
Iteration 108/1000 | Loss: 0.00001169
Iteration 109/1000 | Loss: 0.00001169
Iteration 110/1000 | Loss: 0.00001169
Iteration 111/1000 | Loss: 0.00001169
Iteration 112/1000 | Loss: 0.00001169
Iteration 113/1000 | Loss: 0.00001169
Iteration 114/1000 | Loss: 0.00001168
Iteration 115/1000 | Loss: 0.00001168
Iteration 116/1000 | Loss: 0.00001168
Iteration 117/1000 | Loss: 0.00001168
Iteration 118/1000 | Loss: 0.00001167
Iteration 119/1000 | Loss: 0.00001167
Iteration 120/1000 | Loss: 0.00001167
Iteration 121/1000 | Loss: 0.00001167
Iteration 122/1000 | Loss: 0.00001167
Iteration 123/1000 | Loss: 0.00001167
Iteration 124/1000 | Loss: 0.00001167
Iteration 125/1000 | Loss: 0.00001167
Iteration 126/1000 | Loss: 0.00001167
Iteration 127/1000 | Loss: 0.00001167
Iteration 128/1000 | Loss: 0.00001167
Iteration 129/1000 | Loss: 0.00001167
Iteration 130/1000 | Loss: 0.00001167
Iteration 131/1000 | Loss: 0.00001166
Iteration 132/1000 | Loss: 0.00001166
Iteration 133/1000 | Loss: 0.00001166
Iteration 134/1000 | Loss: 0.00001166
Iteration 135/1000 | Loss: 0.00001166
Iteration 136/1000 | Loss: 0.00001166
Iteration 137/1000 | Loss: 0.00001166
Iteration 138/1000 | Loss: 0.00001166
Iteration 139/1000 | Loss: 0.00001166
Iteration 140/1000 | Loss: 0.00001166
Iteration 141/1000 | Loss: 0.00001166
Iteration 142/1000 | Loss: 0.00001166
Iteration 143/1000 | Loss: 0.00001165
Iteration 144/1000 | Loss: 0.00001165
Iteration 145/1000 | Loss: 0.00001165
Iteration 146/1000 | Loss: 0.00001165
Iteration 147/1000 | Loss: 0.00001165
Iteration 148/1000 | Loss: 0.00001165
Iteration 149/1000 | Loss: 0.00001165
Iteration 150/1000 | Loss: 0.00001165
Iteration 151/1000 | Loss: 0.00001165
Iteration 152/1000 | Loss: 0.00001165
Iteration 153/1000 | Loss: 0.00001164
Iteration 154/1000 | Loss: 0.00001164
Iteration 155/1000 | Loss: 0.00001164
Iteration 156/1000 | Loss: 0.00001164
Iteration 157/1000 | Loss: 0.00001164
Iteration 158/1000 | Loss: 0.00001163
Iteration 159/1000 | Loss: 0.00001163
Iteration 160/1000 | Loss: 0.00001163
Iteration 161/1000 | Loss: 0.00019896
Iteration 162/1000 | Loss: 0.00053442
Iteration 163/1000 | Loss: 0.00005069
Iteration 164/1000 | Loss: 0.00003431
Iteration 165/1000 | Loss: 0.00003128
Iteration 166/1000 | Loss: 0.00008411
Iteration 167/1000 | Loss: 0.00018768
Iteration 168/1000 | Loss: 0.00007326
Iteration 169/1000 | Loss: 0.00001756
Iteration 170/1000 | Loss: 0.00001537
Iteration 171/1000 | Loss: 0.00001405
Iteration 172/1000 | Loss: 0.00001338
Iteration 173/1000 | Loss: 0.00001289
Iteration 174/1000 | Loss: 0.00001237
Iteration 175/1000 | Loss: 0.00001198
Iteration 176/1000 | Loss: 0.00002802
Iteration 177/1000 | Loss: 0.00001231
Iteration 178/1000 | Loss: 0.00001148
Iteration 179/1000 | Loss: 0.00001144
Iteration 180/1000 | Loss: 0.00001143
Iteration 181/1000 | Loss: 0.00001143
Iteration 182/1000 | Loss: 0.00001139
Iteration 183/1000 | Loss: 0.00001139
Iteration 184/1000 | Loss: 0.00001139
Iteration 185/1000 | Loss: 0.00001139
Iteration 186/1000 | Loss: 0.00001139
Iteration 187/1000 | Loss: 0.00001139
Iteration 188/1000 | Loss: 0.00001139
Iteration 189/1000 | Loss: 0.00001139
Iteration 190/1000 | Loss: 0.00001529
Iteration 191/1000 | Loss: 0.00001179
Iteration 192/1000 | Loss: 0.00001138
Iteration 193/1000 | Loss: 0.00001138
Iteration 194/1000 | Loss: 0.00001138
Iteration 195/1000 | Loss: 0.00001138
Iteration 196/1000 | Loss: 0.00001138
Iteration 197/1000 | Loss: 0.00001138
Iteration 198/1000 | Loss: 0.00001138
Iteration 199/1000 | Loss: 0.00001138
Iteration 200/1000 | Loss: 0.00001138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.1381516742403619e-05, 1.1381516742403619e-05, 1.1381516742403619e-05, 1.1381516742403619e-05, 1.1381516742403619e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1381516742403619e-05

Optimization complete. Final v2v error: 2.775660753250122 mm

Highest mean error: 6.5368475914001465 mm for frame 74

Lowest mean error: 2.490356922149658 mm for frame 134

Saving results

Total time: 115.84096574783325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951809
Iteration 2/25 | Loss: 0.00201773
Iteration 3/25 | Loss: 0.00143737
Iteration 4/25 | Loss: 0.00138677
Iteration 5/25 | Loss: 0.00134811
Iteration 6/25 | Loss: 0.00134975
Iteration 7/25 | Loss: 0.00129508
Iteration 8/25 | Loss: 0.00128125
Iteration 9/25 | Loss: 0.00127933
Iteration 10/25 | Loss: 0.00127553
Iteration 11/25 | Loss: 0.00127788
Iteration 12/25 | Loss: 0.00127351
Iteration 13/25 | Loss: 0.00127147
Iteration 14/25 | Loss: 0.00127123
Iteration 15/25 | Loss: 0.00127112
Iteration 16/25 | Loss: 0.00127111
Iteration 17/25 | Loss: 0.00127111
Iteration 18/25 | Loss: 0.00127103
Iteration 19/25 | Loss: 0.00127184
Iteration 20/25 | Loss: 0.00127096
Iteration 21/25 | Loss: 0.00127034
Iteration 22/25 | Loss: 0.00127009
Iteration 23/25 | Loss: 0.00126998
Iteration 24/25 | Loss: 0.00126997
Iteration 25/25 | Loss: 0.00126997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28482521
Iteration 2/25 | Loss: 0.00111151
Iteration 3/25 | Loss: 0.00111151
Iteration 4/25 | Loss: 0.00111151
Iteration 5/25 | Loss: 0.00111151
Iteration 6/25 | Loss: 0.00111151
Iteration 7/25 | Loss: 0.00111150
Iteration 8/25 | Loss: 0.00111150
Iteration 9/25 | Loss: 0.00111150
Iteration 10/25 | Loss: 0.00111150
Iteration 11/25 | Loss: 0.00111150
Iteration 12/25 | Loss: 0.00111150
Iteration 13/25 | Loss: 0.00111150
Iteration 14/25 | Loss: 0.00111150
Iteration 15/25 | Loss: 0.00111150
Iteration 16/25 | Loss: 0.00111150
Iteration 17/25 | Loss: 0.00111150
Iteration 18/25 | Loss: 0.00111150
Iteration 19/25 | Loss: 0.00111150
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011115038068965077, 0.0011115038068965077, 0.0011115038068965077, 0.0011115038068965077, 0.0011115038068965077]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011115038068965077

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111150
Iteration 2/1000 | Loss: 0.00003452
Iteration 3/1000 | Loss: 0.00002212
Iteration 4/1000 | Loss: 0.00001914
Iteration 5/1000 | Loss: 0.00001812
Iteration 6/1000 | Loss: 0.00006388
Iteration 7/1000 | Loss: 0.00001717
Iteration 8/1000 | Loss: 0.00001678
Iteration 9/1000 | Loss: 0.00001644
Iteration 10/1000 | Loss: 0.00001607
Iteration 11/1000 | Loss: 0.00001575
Iteration 12/1000 | Loss: 0.00001560
Iteration 13/1000 | Loss: 0.00001542
Iteration 14/1000 | Loss: 0.00001536
Iteration 15/1000 | Loss: 0.00001529
Iteration 16/1000 | Loss: 0.00001526
Iteration 17/1000 | Loss: 0.00001525
Iteration 18/1000 | Loss: 0.00001525
Iteration 19/1000 | Loss: 0.00001524
Iteration 20/1000 | Loss: 0.00001521
Iteration 21/1000 | Loss: 0.00001521
Iteration 22/1000 | Loss: 0.00001519
Iteration 23/1000 | Loss: 0.00001519
Iteration 24/1000 | Loss: 0.00001518
Iteration 25/1000 | Loss: 0.00001518
Iteration 26/1000 | Loss: 0.00001517
Iteration 27/1000 | Loss: 0.00001517
Iteration 28/1000 | Loss: 0.00001516
Iteration 29/1000 | Loss: 0.00001514
Iteration 30/1000 | Loss: 0.00001514
Iteration 31/1000 | Loss: 0.00001513
Iteration 32/1000 | Loss: 0.00001512
Iteration 33/1000 | Loss: 0.00001512
Iteration 34/1000 | Loss: 0.00001512
Iteration 35/1000 | Loss: 0.00001510
Iteration 36/1000 | Loss: 0.00001510
Iteration 37/1000 | Loss: 0.00001509
Iteration 38/1000 | Loss: 0.00001508
Iteration 39/1000 | Loss: 0.00001508
Iteration 40/1000 | Loss: 0.00001507
Iteration 41/1000 | Loss: 0.00001504
Iteration 42/1000 | Loss: 0.00001502
Iteration 43/1000 | Loss: 0.00001499
Iteration 44/1000 | Loss: 0.00001499
Iteration 45/1000 | Loss: 0.00001498
Iteration 46/1000 | Loss: 0.00001498
Iteration 47/1000 | Loss: 0.00001497
Iteration 48/1000 | Loss: 0.00001497
Iteration 49/1000 | Loss: 0.00001496
Iteration 50/1000 | Loss: 0.00001495
Iteration 51/1000 | Loss: 0.00001495
Iteration 52/1000 | Loss: 0.00001494
Iteration 53/1000 | Loss: 0.00001494
Iteration 54/1000 | Loss: 0.00001494
Iteration 55/1000 | Loss: 0.00001494
Iteration 56/1000 | Loss: 0.00001494
Iteration 57/1000 | Loss: 0.00001493
Iteration 58/1000 | Loss: 0.00001493
Iteration 59/1000 | Loss: 0.00001493
Iteration 60/1000 | Loss: 0.00001493
Iteration 61/1000 | Loss: 0.00001493
Iteration 62/1000 | Loss: 0.00001493
Iteration 63/1000 | Loss: 0.00001493
Iteration 64/1000 | Loss: 0.00001493
Iteration 65/1000 | Loss: 0.00001492
Iteration 66/1000 | Loss: 0.00001492
Iteration 67/1000 | Loss: 0.00001492
Iteration 68/1000 | Loss: 0.00001492
Iteration 69/1000 | Loss: 0.00001492
Iteration 70/1000 | Loss: 0.00001491
Iteration 71/1000 | Loss: 0.00001491
Iteration 72/1000 | Loss: 0.00001491
Iteration 73/1000 | Loss: 0.00001491
Iteration 74/1000 | Loss: 0.00001491
Iteration 75/1000 | Loss: 0.00001491
Iteration 76/1000 | Loss: 0.00001491
Iteration 77/1000 | Loss: 0.00001491
Iteration 78/1000 | Loss: 0.00001490
Iteration 79/1000 | Loss: 0.00001490
Iteration 80/1000 | Loss: 0.00001489
Iteration 81/1000 | Loss: 0.00001489
Iteration 82/1000 | Loss: 0.00001489
Iteration 83/1000 | Loss: 0.00001488
Iteration 84/1000 | Loss: 0.00001488
Iteration 85/1000 | Loss: 0.00001488
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00001487
Iteration 88/1000 | Loss: 0.00001487
Iteration 89/1000 | Loss: 0.00001486
Iteration 90/1000 | Loss: 0.00001486
Iteration 91/1000 | Loss: 0.00001486
Iteration 92/1000 | Loss: 0.00001485
Iteration 93/1000 | Loss: 0.00001485
Iteration 94/1000 | Loss: 0.00001485
Iteration 95/1000 | Loss: 0.00001485
Iteration 96/1000 | Loss: 0.00001484
Iteration 97/1000 | Loss: 0.00001483
Iteration 98/1000 | Loss: 0.00001483
Iteration 99/1000 | Loss: 0.00001482
Iteration 100/1000 | Loss: 0.00001482
Iteration 101/1000 | Loss: 0.00001481
Iteration 102/1000 | Loss: 0.00001481
Iteration 103/1000 | Loss: 0.00001481
Iteration 104/1000 | Loss: 0.00001480
Iteration 105/1000 | Loss: 0.00001480
Iteration 106/1000 | Loss: 0.00001479
Iteration 107/1000 | Loss: 0.00001479
Iteration 108/1000 | Loss: 0.00001479
Iteration 109/1000 | Loss: 0.00001478
Iteration 110/1000 | Loss: 0.00001478
Iteration 111/1000 | Loss: 0.00001478
Iteration 112/1000 | Loss: 0.00001478
Iteration 113/1000 | Loss: 0.00001478
Iteration 114/1000 | Loss: 0.00001478
Iteration 115/1000 | Loss: 0.00001478
Iteration 116/1000 | Loss: 0.00001478
Iteration 117/1000 | Loss: 0.00001478
Iteration 118/1000 | Loss: 0.00001477
Iteration 119/1000 | Loss: 0.00001477
Iteration 120/1000 | Loss: 0.00001477
Iteration 121/1000 | Loss: 0.00001477
Iteration 122/1000 | Loss: 0.00001477
Iteration 123/1000 | Loss: 0.00001476
Iteration 124/1000 | Loss: 0.00001476
Iteration 125/1000 | Loss: 0.00001475
Iteration 126/1000 | Loss: 0.00001475
Iteration 127/1000 | Loss: 0.00001475
Iteration 128/1000 | Loss: 0.00001475
Iteration 129/1000 | Loss: 0.00001475
Iteration 130/1000 | Loss: 0.00001475
Iteration 131/1000 | Loss: 0.00001475
Iteration 132/1000 | Loss: 0.00001475
Iteration 133/1000 | Loss: 0.00001475
Iteration 134/1000 | Loss: 0.00001475
Iteration 135/1000 | Loss: 0.00001474
Iteration 136/1000 | Loss: 0.00001474
Iteration 137/1000 | Loss: 0.00001474
Iteration 138/1000 | Loss: 0.00001474
Iteration 139/1000 | Loss: 0.00001473
Iteration 140/1000 | Loss: 0.00001473
Iteration 141/1000 | Loss: 0.00001473
Iteration 142/1000 | Loss: 0.00001473
Iteration 143/1000 | Loss: 0.00001473
Iteration 144/1000 | Loss: 0.00001473
Iteration 145/1000 | Loss: 0.00001473
Iteration 146/1000 | Loss: 0.00001472
Iteration 147/1000 | Loss: 0.00001472
Iteration 148/1000 | Loss: 0.00001472
Iteration 149/1000 | Loss: 0.00001472
Iteration 150/1000 | Loss: 0.00001472
Iteration 151/1000 | Loss: 0.00001472
Iteration 152/1000 | Loss: 0.00001472
Iteration 153/1000 | Loss: 0.00001472
Iteration 154/1000 | Loss: 0.00001472
Iteration 155/1000 | Loss: 0.00001471
Iteration 156/1000 | Loss: 0.00001471
Iteration 157/1000 | Loss: 0.00001471
Iteration 158/1000 | Loss: 0.00001471
Iteration 159/1000 | Loss: 0.00001471
Iteration 160/1000 | Loss: 0.00001471
Iteration 161/1000 | Loss: 0.00001471
Iteration 162/1000 | Loss: 0.00001471
Iteration 163/1000 | Loss: 0.00001471
Iteration 164/1000 | Loss: 0.00001471
Iteration 165/1000 | Loss: 0.00001471
Iteration 166/1000 | Loss: 0.00001471
Iteration 167/1000 | Loss: 0.00001471
Iteration 168/1000 | Loss: 0.00001471
Iteration 169/1000 | Loss: 0.00001470
Iteration 170/1000 | Loss: 0.00001470
Iteration 171/1000 | Loss: 0.00001470
Iteration 172/1000 | Loss: 0.00001470
Iteration 173/1000 | Loss: 0.00001470
Iteration 174/1000 | Loss: 0.00001470
Iteration 175/1000 | Loss: 0.00001470
Iteration 176/1000 | Loss: 0.00001470
Iteration 177/1000 | Loss: 0.00001470
Iteration 178/1000 | Loss: 0.00001470
Iteration 179/1000 | Loss: 0.00001470
Iteration 180/1000 | Loss: 0.00001470
Iteration 181/1000 | Loss: 0.00001469
Iteration 182/1000 | Loss: 0.00001469
Iteration 183/1000 | Loss: 0.00001469
Iteration 184/1000 | Loss: 0.00001469
Iteration 185/1000 | Loss: 0.00001469
Iteration 186/1000 | Loss: 0.00001469
Iteration 187/1000 | Loss: 0.00001469
Iteration 188/1000 | Loss: 0.00001469
Iteration 189/1000 | Loss: 0.00001468
Iteration 190/1000 | Loss: 0.00001468
Iteration 191/1000 | Loss: 0.00001468
Iteration 192/1000 | Loss: 0.00001468
Iteration 193/1000 | Loss: 0.00001468
Iteration 194/1000 | Loss: 0.00001468
Iteration 195/1000 | Loss: 0.00001468
Iteration 196/1000 | Loss: 0.00001468
Iteration 197/1000 | Loss: 0.00001468
Iteration 198/1000 | Loss: 0.00001468
Iteration 199/1000 | Loss: 0.00001468
Iteration 200/1000 | Loss: 0.00001468
Iteration 201/1000 | Loss: 0.00001468
Iteration 202/1000 | Loss: 0.00001468
Iteration 203/1000 | Loss: 0.00001468
Iteration 204/1000 | Loss: 0.00001468
Iteration 205/1000 | Loss: 0.00001468
Iteration 206/1000 | Loss: 0.00001468
Iteration 207/1000 | Loss: 0.00001468
Iteration 208/1000 | Loss: 0.00001468
Iteration 209/1000 | Loss: 0.00001468
Iteration 210/1000 | Loss: 0.00001468
Iteration 211/1000 | Loss: 0.00001468
Iteration 212/1000 | Loss: 0.00001468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.4675428246846423e-05, 1.4675428246846423e-05, 1.4675428246846423e-05, 1.4675428246846423e-05, 1.4675428246846423e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4675428246846423e-05

Optimization complete. Final v2v error: 3.2228281497955322 mm

Highest mean error: 4.481154441833496 mm for frame 85

Lowest mean error: 2.7329976558685303 mm for frame 208

Saving results

Total time: 84.69663500785828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998273
Iteration 2/25 | Loss: 0.00221402
Iteration 3/25 | Loss: 0.00174286
Iteration 4/25 | Loss: 0.00156906
Iteration 5/25 | Loss: 0.00153648
Iteration 6/25 | Loss: 0.00151210
Iteration 7/25 | Loss: 0.00143134
Iteration 8/25 | Loss: 0.00141681
Iteration 9/25 | Loss: 0.00140503
Iteration 10/25 | Loss: 0.00139711
Iteration 11/25 | Loss: 0.00139367
Iteration 12/25 | Loss: 0.00139797
Iteration 13/25 | Loss: 0.00138997
Iteration 14/25 | Loss: 0.00139438
Iteration 15/25 | Loss: 0.00137303
Iteration 16/25 | Loss: 0.00136344
Iteration 17/25 | Loss: 0.00135569
Iteration 18/25 | Loss: 0.00135170
Iteration 19/25 | Loss: 0.00134840
Iteration 20/25 | Loss: 0.00135278
Iteration 21/25 | Loss: 0.00135873
Iteration 22/25 | Loss: 0.00135924
Iteration 23/25 | Loss: 0.00136263
Iteration 24/25 | Loss: 0.00135317
Iteration 25/25 | Loss: 0.00134867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28311026
Iteration 2/25 | Loss: 0.00225030
Iteration 3/25 | Loss: 0.00224715
Iteration 4/25 | Loss: 0.00224715
Iteration 5/25 | Loss: 0.00224715
Iteration 6/25 | Loss: 0.00224715
Iteration 7/25 | Loss: 0.00224715
Iteration 8/25 | Loss: 0.00224715
Iteration 9/25 | Loss: 0.00224715
Iteration 10/25 | Loss: 0.00224715
Iteration 11/25 | Loss: 0.00224715
Iteration 12/25 | Loss: 0.00224715
Iteration 13/25 | Loss: 0.00224715
Iteration 14/25 | Loss: 0.00224715
Iteration 15/25 | Loss: 0.00224715
Iteration 16/25 | Loss: 0.00224715
Iteration 17/25 | Loss: 0.00224715
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002247146563604474, 0.002247146563604474, 0.002247146563604474, 0.002247146563604474, 0.002247146563604474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002247146563604474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224715
Iteration 2/1000 | Loss: 0.00031774
Iteration 3/1000 | Loss: 0.00015718
Iteration 4/1000 | Loss: 0.00035507
Iteration 5/1000 | Loss: 0.00011257
Iteration 6/1000 | Loss: 0.00012601
Iteration 7/1000 | Loss: 0.00011037
Iteration 8/1000 | Loss: 0.00012171
Iteration 9/1000 | Loss: 0.00011695
Iteration 10/1000 | Loss: 0.00025789
Iteration 11/1000 | Loss: 0.00031693
Iteration 12/1000 | Loss: 0.00039601
Iteration 13/1000 | Loss: 0.00011996
Iteration 14/1000 | Loss: 0.00012106
Iteration 15/1000 | Loss: 0.00009774
Iteration 16/1000 | Loss: 0.00011795
Iteration 17/1000 | Loss: 0.00034217
Iteration 18/1000 | Loss: 0.00022699
Iteration 19/1000 | Loss: 0.00035940
Iteration 20/1000 | Loss: 0.00024419
Iteration 21/1000 | Loss: 0.00009377
Iteration 22/1000 | Loss: 0.00011172
Iteration 23/1000 | Loss: 0.00010022
Iteration 24/1000 | Loss: 0.00010182
Iteration 25/1000 | Loss: 0.00014394
Iteration 26/1000 | Loss: 0.00011563
Iteration 27/1000 | Loss: 0.00010397
Iteration 28/1000 | Loss: 0.00010780
Iteration 29/1000 | Loss: 0.00010707
Iteration 30/1000 | Loss: 0.00013192
Iteration 31/1000 | Loss: 0.00010964
Iteration 32/1000 | Loss: 0.00013933
Iteration 33/1000 | Loss: 0.00007986
Iteration 34/1000 | Loss: 0.00007184
Iteration 35/1000 | Loss: 0.00006713
Iteration 36/1000 | Loss: 0.00056454
Iteration 37/1000 | Loss: 0.00274464
Iteration 38/1000 | Loss: 0.00363582
Iteration 39/1000 | Loss: 0.00015354
Iteration 40/1000 | Loss: 0.00017078
Iteration 41/1000 | Loss: 0.00009682
Iteration 42/1000 | Loss: 0.00005657
Iteration 43/1000 | Loss: 0.00005275
Iteration 44/1000 | Loss: 0.00004239
Iteration 45/1000 | Loss: 0.00003342
Iteration 46/1000 | Loss: 0.00003054
Iteration 47/1000 | Loss: 0.00033513
Iteration 48/1000 | Loss: 0.00002413
Iteration 49/1000 | Loss: 0.00002130
Iteration 50/1000 | Loss: 0.00032320
Iteration 51/1000 | Loss: 0.00112522
Iteration 52/1000 | Loss: 0.00104974
Iteration 53/1000 | Loss: 0.00002051
Iteration 54/1000 | Loss: 0.00001606
Iteration 55/1000 | Loss: 0.00001503
Iteration 56/1000 | Loss: 0.00001432
Iteration 57/1000 | Loss: 0.00001347
Iteration 58/1000 | Loss: 0.00001278
Iteration 59/1000 | Loss: 0.00001234
Iteration 60/1000 | Loss: 0.00001183
Iteration 61/1000 | Loss: 0.00001159
Iteration 62/1000 | Loss: 0.00001143
Iteration 63/1000 | Loss: 0.00001140
Iteration 64/1000 | Loss: 0.00001138
Iteration 65/1000 | Loss: 0.00001133
Iteration 66/1000 | Loss: 0.00001131
Iteration 67/1000 | Loss: 0.00001130
Iteration 68/1000 | Loss: 0.00001129
Iteration 69/1000 | Loss: 0.00001125
Iteration 70/1000 | Loss: 0.00001123
Iteration 71/1000 | Loss: 0.00001122
Iteration 72/1000 | Loss: 0.00001122
Iteration 73/1000 | Loss: 0.00001122
Iteration 74/1000 | Loss: 0.00001122
Iteration 75/1000 | Loss: 0.00001121
Iteration 76/1000 | Loss: 0.00001121
Iteration 77/1000 | Loss: 0.00001121
Iteration 78/1000 | Loss: 0.00001120
Iteration 79/1000 | Loss: 0.00001120
Iteration 80/1000 | Loss: 0.00001120
Iteration 81/1000 | Loss: 0.00001120
Iteration 82/1000 | Loss: 0.00001119
Iteration 83/1000 | Loss: 0.00001119
Iteration 84/1000 | Loss: 0.00001119
Iteration 85/1000 | Loss: 0.00001119
Iteration 86/1000 | Loss: 0.00001119
Iteration 87/1000 | Loss: 0.00001119
Iteration 88/1000 | Loss: 0.00001118
Iteration 89/1000 | Loss: 0.00001118
Iteration 90/1000 | Loss: 0.00001118
Iteration 91/1000 | Loss: 0.00001118
Iteration 92/1000 | Loss: 0.00001118
Iteration 93/1000 | Loss: 0.00001118
Iteration 94/1000 | Loss: 0.00001118
Iteration 95/1000 | Loss: 0.00001118
Iteration 96/1000 | Loss: 0.00001118
Iteration 97/1000 | Loss: 0.00001118
Iteration 98/1000 | Loss: 0.00001117
Iteration 99/1000 | Loss: 0.00001117
Iteration 100/1000 | Loss: 0.00001117
Iteration 101/1000 | Loss: 0.00001116
Iteration 102/1000 | Loss: 0.00001116
Iteration 103/1000 | Loss: 0.00001115
Iteration 104/1000 | Loss: 0.00001114
Iteration 105/1000 | Loss: 0.00001114
Iteration 106/1000 | Loss: 0.00001114
Iteration 107/1000 | Loss: 0.00001113
Iteration 108/1000 | Loss: 0.00001113
Iteration 109/1000 | Loss: 0.00001113
Iteration 110/1000 | Loss: 0.00001113
Iteration 111/1000 | Loss: 0.00001112
Iteration 112/1000 | Loss: 0.00001112
Iteration 113/1000 | Loss: 0.00001112
Iteration 114/1000 | Loss: 0.00001112
Iteration 115/1000 | Loss: 0.00001111
Iteration 116/1000 | Loss: 0.00001111
Iteration 117/1000 | Loss: 0.00001111
Iteration 118/1000 | Loss: 0.00001111
Iteration 119/1000 | Loss: 0.00001111
Iteration 120/1000 | Loss: 0.00001111
Iteration 121/1000 | Loss: 0.00001111
Iteration 122/1000 | Loss: 0.00001111
Iteration 123/1000 | Loss: 0.00001111
Iteration 124/1000 | Loss: 0.00001111
Iteration 125/1000 | Loss: 0.00001111
Iteration 126/1000 | Loss: 0.00001110
Iteration 127/1000 | Loss: 0.00001110
Iteration 128/1000 | Loss: 0.00001110
Iteration 129/1000 | Loss: 0.00001110
Iteration 130/1000 | Loss: 0.00001110
Iteration 131/1000 | Loss: 0.00001109
Iteration 132/1000 | Loss: 0.00001109
Iteration 133/1000 | Loss: 0.00001109
Iteration 134/1000 | Loss: 0.00001109
Iteration 135/1000 | Loss: 0.00001108
Iteration 136/1000 | Loss: 0.00001108
Iteration 137/1000 | Loss: 0.00001108
Iteration 138/1000 | Loss: 0.00001108
Iteration 139/1000 | Loss: 0.00001107
Iteration 140/1000 | Loss: 0.00001107
Iteration 141/1000 | Loss: 0.00001107
Iteration 142/1000 | Loss: 0.00001107
Iteration 143/1000 | Loss: 0.00001107
Iteration 144/1000 | Loss: 0.00001107
Iteration 145/1000 | Loss: 0.00001107
Iteration 146/1000 | Loss: 0.00001107
Iteration 147/1000 | Loss: 0.00001107
Iteration 148/1000 | Loss: 0.00001107
Iteration 149/1000 | Loss: 0.00001106
Iteration 150/1000 | Loss: 0.00001106
Iteration 151/1000 | Loss: 0.00001105
Iteration 152/1000 | Loss: 0.00001105
Iteration 153/1000 | Loss: 0.00001105
Iteration 154/1000 | Loss: 0.00001105
Iteration 155/1000 | Loss: 0.00001105
Iteration 156/1000 | Loss: 0.00001104
Iteration 157/1000 | Loss: 0.00001104
Iteration 158/1000 | Loss: 0.00001104
Iteration 159/1000 | Loss: 0.00001104
Iteration 160/1000 | Loss: 0.00001104
Iteration 161/1000 | Loss: 0.00001103
Iteration 162/1000 | Loss: 0.00001103
Iteration 163/1000 | Loss: 0.00001103
Iteration 164/1000 | Loss: 0.00001103
Iteration 165/1000 | Loss: 0.00001103
Iteration 166/1000 | Loss: 0.00001103
Iteration 167/1000 | Loss: 0.00001103
Iteration 168/1000 | Loss: 0.00001102
Iteration 169/1000 | Loss: 0.00001102
Iteration 170/1000 | Loss: 0.00001102
Iteration 171/1000 | Loss: 0.00001102
Iteration 172/1000 | Loss: 0.00001102
Iteration 173/1000 | Loss: 0.00001102
Iteration 174/1000 | Loss: 0.00001102
Iteration 175/1000 | Loss: 0.00001102
Iteration 176/1000 | Loss: 0.00001102
Iteration 177/1000 | Loss: 0.00001102
Iteration 178/1000 | Loss: 0.00001102
Iteration 179/1000 | Loss: 0.00001102
Iteration 180/1000 | Loss: 0.00001102
Iteration 181/1000 | Loss: 0.00001102
Iteration 182/1000 | Loss: 0.00001102
Iteration 183/1000 | Loss: 0.00001101
Iteration 184/1000 | Loss: 0.00001101
Iteration 185/1000 | Loss: 0.00001101
Iteration 186/1000 | Loss: 0.00001101
Iteration 187/1000 | Loss: 0.00001101
Iteration 188/1000 | Loss: 0.00001101
Iteration 189/1000 | Loss: 0.00001101
Iteration 190/1000 | Loss: 0.00001101
Iteration 191/1000 | Loss: 0.00001101
Iteration 192/1000 | Loss: 0.00001101
Iteration 193/1000 | Loss: 0.00001101
Iteration 194/1000 | Loss: 0.00001101
Iteration 195/1000 | Loss: 0.00001101
Iteration 196/1000 | Loss: 0.00001101
Iteration 197/1000 | Loss: 0.00001100
Iteration 198/1000 | Loss: 0.00001100
Iteration 199/1000 | Loss: 0.00001100
Iteration 200/1000 | Loss: 0.00001100
Iteration 201/1000 | Loss: 0.00001100
Iteration 202/1000 | Loss: 0.00001100
Iteration 203/1000 | Loss: 0.00001100
Iteration 204/1000 | Loss: 0.00001100
Iteration 205/1000 | Loss: 0.00001100
Iteration 206/1000 | Loss: 0.00001100
Iteration 207/1000 | Loss: 0.00001100
Iteration 208/1000 | Loss: 0.00001100
Iteration 209/1000 | Loss: 0.00001100
Iteration 210/1000 | Loss: 0.00001099
Iteration 211/1000 | Loss: 0.00001099
Iteration 212/1000 | Loss: 0.00001099
Iteration 213/1000 | Loss: 0.00001099
Iteration 214/1000 | Loss: 0.00001099
Iteration 215/1000 | Loss: 0.00001099
Iteration 216/1000 | Loss: 0.00001099
Iteration 217/1000 | Loss: 0.00001099
Iteration 218/1000 | Loss: 0.00001099
Iteration 219/1000 | Loss: 0.00001099
Iteration 220/1000 | Loss: 0.00001099
Iteration 221/1000 | Loss: 0.00001099
Iteration 222/1000 | Loss: 0.00001099
Iteration 223/1000 | Loss: 0.00001099
Iteration 224/1000 | Loss: 0.00001099
Iteration 225/1000 | Loss: 0.00001099
Iteration 226/1000 | Loss: 0.00001099
Iteration 227/1000 | Loss: 0.00001099
Iteration 228/1000 | Loss: 0.00001099
Iteration 229/1000 | Loss: 0.00001099
Iteration 230/1000 | Loss: 0.00001099
Iteration 231/1000 | Loss: 0.00001099
Iteration 232/1000 | Loss: 0.00001099
Iteration 233/1000 | Loss: 0.00001099
Iteration 234/1000 | Loss: 0.00001099
Iteration 235/1000 | Loss: 0.00001099
Iteration 236/1000 | Loss: 0.00001099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.0990742339345161e-05, 1.0990742339345161e-05, 1.0990742339345161e-05, 1.0990742339345161e-05, 1.0990742339345161e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0990742339345161e-05

Optimization complete. Final v2v error: 2.789069890975952 mm

Highest mean error: 4.891513824462891 mm for frame 56

Lowest mean error: 2.509406566619873 mm for frame 37

Saving results

Total time: 144.0350694656372
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793097
Iteration 2/25 | Loss: 0.00137629
Iteration 3/25 | Loss: 0.00121275
Iteration 4/25 | Loss: 0.00119137
Iteration 5/25 | Loss: 0.00118718
Iteration 6/25 | Loss: 0.00118713
Iteration 7/25 | Loss: 0.00118713
Iteration 8/25 | Loss: 0.00118713
Iteration 9/25 | Loss: 0.00118713
Iteration 10/25 | Loss: 0.00118713
Iteration 11/25 | Loss: 0.00118713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011871329043060541, 0.0011871329043060541, 0.0011871329043060541, 0.0011871329043060541, 0.0011871329043060541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011871329043060541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27539539
Iteration 2/25 | Loss: 0.00156588
Iteration 3/25 | Loss: 0.00156588
Iteration 4/25 | Loss: 0.00156588
Iteration 5/25 | Loss: 0.00156588
Iteration 6/25 | Loss: 0.00156588
Iteration 7/25 | Loss: 0.00156588
Iteration 8/25 | Loss: 0.00156587
Iteration 9/25 | Loss: 0.00156587
Iteration 10/25 | Loss: 0.00156587
Iteration 11/25 | Loss: 0.00156587
Iteration 12/25 | Loss: 0.00156587
Iteration 13/25 | Loss: 0.00156587
Iteration 14/25 | Loss: 0.00156587
Iteration 15/25 | Loss: 0.00156587
Iteration 16/25 | Loss: 0.00156587
Iteration 17/25 | Loss: 0.00156587
Iteration 18/25 | Loss: 0.00156587
Iteration 19/25 | Loss: 0.00156587
Iteration 20/25 | Loss: 0.00156587
Iteration 21/25 | Loss: 0.00156587
Iteration 22/25 | Loss: 0.00156587
Iteration 23/25 | Loss: 0.00156587
Iteration 24/25 | Loss: 0.00156587
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0015658738557249308, 0.0015658738557249308, 0.0015658738557249308, 0.0015658738557249308, 0.0015658738557249308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015658738557249308

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156587
Iteration 2/1000 | Loss: 0.00003122
Iteration 3/1000 | Loss: 0.00001954
Iteration 4/1000 | Loss: 0.00001540
Iteration 5/1000 | Loss: 0.00001338
Iteration 6/1000 | Loss: 0.00001233
Iteration 7/1000 | Loss: 0.00001149
Iteration 8/1000 | Loss: 0.00001100
Iteration 9/1000 | Loss: 0.00001069
Iteration 10/1000 | Loss: 0.00001030
Iteration 11/1000 | Loss: 0.00000992
Iteration 12/1000 | Loss: 0.00000984
Iteration 13/1000 | Loss: 0.00000982
Iteration 14/1000 | Loss: 0.00000980
Iteration 15/1000 | Loss: 0.00000978
Iteration 16/1000 | Loss: 0.00000969
Iteration 17/1000 | Loss: 0.00000964
Iteration 18/1000 | Loss: 0.00000963
Iteration 19/1000 | Loss: 0.00000961
Iteration 20/1000 | Loss: 0.00000959
Iteration 21/1000 | Loss: 0.00000953
Iteration 22/1000 | Loss: 0.00000950
Iteration 23/1000 | Loss: 0.00000949
Iteration 24/1000 | Loss: 0.00000948
Iteration 25/1000 | Loss: 0.00000947
Iteration 26/1000 | Loss: 0.00000946
Iteration 27/1000 | Loss: 0.00000945
Iteration 28/1000 | Loss: 0.00000944
Iteration 29/1000 | Loss: 0.00000944
Iteration 30/1000 | Loss: 0.00000943
Iteration 31/1000 | Loss: 0.00000943
Iteration 32/1000 | Loss: 0.00000942
Iteration 33/1000 | Loss: 0.00000939
Iteration 34/1000 | Loss: 0.00000936
Iteration 35/1000 | Loss: 0.00000935
Iteration 36/1000 | Loss: 0.00000935
Iteration 37/1000 | Loss: 0.00000931
Iteration 38/1000 | Loss: 0.00000929
Iteration 39/1000 | Loss: 0.00000929
Iteration 40/1000 | Loss: 0.00000928
Iteration 41/1000 | Loss: 0.00000928
Iteration 42/1000 | Loss: 0.00000927
Iteration 43/1000 | Loss: 0.00000927
Iteration 44/1000 | Loss: 0.00000927
Iteration 45/1000 | Loss: 0.00000927
Iteration 46/1000 | Loss: 0.00000926
Iteration 47/1000 | Loss: 0.00000926
Iteration 48/1000 | Loss: 0.00000926
Iteration 49/1000 | Loss: 0.00000926
Iteration 50/1000 | Loss: 0.00000926
Iteration 51/1000 | Loss: 0.00000925
Iteration 52/1000 | Loss: 0.00000925
Iteration 53/1000 | Loss: 0.00000925
Iteration 54/1000 | Loss: 0.00000924
Iteration 55/1000 | Loss: 0.00000924
Iteration 56/1000 | Loss: 0.00000923
Iteration 57/1000 | Loss: 0.00000923
Iteration 58/1000 | Loss: 0.00000923
Iteration 59/1000 | Loss: 0.00000922
Iteration 60/1000 | Loss: 0.00000922
Iteration 61/1000 | Loss: 0.00000922
Iteration 62/1000 | Loss: 0.00000922
Iteration 63/1000 | Loss: 0.00000920
Iteration 64/1000 | Loss: 0.00000920
Iteration 65/1000 | Loss: 0.00000920
Iteration 66/1000 | Loss: 0.00000920
Iteration 67/1000 | Loss: 0.00000919
Iteration 68/1000 | Loss: 0.00000919
Iteration 69/1000 | Loss: 0.00000919
Iteration 70/1000 | Loss: 0.00000918
Iteration 71/1000 | Loss: 0.00000918
Iteration 72/1000 | Loss: 0.00000918
Iteration 73/1000 | Loss: 0.00000917
Iteration 74/1000 | Loss: 0.00000917
Iteration 75/1000 | Loss: 0.00000917
Iteration 76/1000 | Loss: 0.00000917
Iteration 77/1000 | Loss: 0.00000916
Iteration 78/1000 | Loss: 0.00000916
Iteration 79/1000 | Loss: 0.00000916
Iteration 80/1000 | Loss: 0.00000916
Iteration 81/1000 | Loss: 0.00000916
Iteration 82/1000 | Loss: 0.00000916
Iteration 83/1000 | Loss: 0.00000916
Iteration 84/1000 | Loss: 0.00000915
Iteration 85/1000 | Loss: 0.00000915
Iteration 86/1000 | Loss: 0.00000915
Iteration 87/1000 | Loss: 0.00000915
Iteration 88/1000 | Loss: 0.00000914
Iteration 89/1000 | Loss: 0.00000914
Iteration 90/1000 | Loss: 0.00000914
Iteration 91/1000 | Loss: 0.00000914
Iteration 92/1000 | Loss: 0.00000913
Iteration 93/1000 | Loss: 0.00000913
Iteration 94/1000 | Loss: 0.00000913
Iteration 95/1000 | Loss: 0.00000913
Iteration 96/1000 | Loss: 0.00000913
Iteration 97/1000 | Loss: 0.00000913
Iteration 98/1000 | Loss: 0.00000913
Iteration 99/1000 | Loss: 0.00000912
Iteration 100/1000 | Loss: 0.00000912
Iteration 101/1000 | Loss: 0.00000912
Iteration 102/1000 | Loss: 0.00000912
Iteration 103/1000 | Loss: 0.00000911
Iteration 104/1000 | Loss: 0.00000911
Iteration 105/1000 | Loss: 0.00000911
Iteration 106/1000 | Loss: 0.00000911
Iteration 107/1000 | Loss: 0.00000911
Iteration 108/1000 | Loss: 0.00000910
Iteration 109/1000 | Loss: 0.00000910
Iteration 110/1000 | Loss: 0.00000910
Iteration 111/1000 | Loss: 0.00000910
Iteration 112/1000 | Loss: 0.00000910
Iteration 113/1000 | Loss: 0.00000910
Iteration 114/1000 | Loss: 0.00000910
Iteration 115/1000 | Loss: 0.00000910
Iteration 116/1000 | Loss: 0.00000910
Iteration 117/1000 | Loss: 0.00000910
Iteration 118/1000 | Loss: 0.00000910
Iteration 119/1000 | Loss: 0.00000910
Iteration 120/1000 | Loss: 0.00000910
Iteration 121/1000 | Loss: 0.00000910
Iteration 122/1000 | Loss: 0.00000910
Iteration 123/1000 | Loss: 0.00000910
Iteration 124/1000 | Loss: 0.00000910
Iteration 125/1000 | Loss: 0.00000910
Iteration 126/1000 | Loss: 0.00000910
Iteration 127/1000 | Loss: 0.00000910
Iteration 128/1000 | Loss: 0.00000910
Iteration 129/1000 | Loss: 0.00000910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [9.104132914217189e-06, 9.104132914217189e-06, 9.104132914217189e-06, 9.104132914217189e-06, 9.104132914217189e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.104132914217189e-06

Optimization complete. Final v2v error: 2.6225132942199707 mm

Highest mean error: 2.8314239978790283 mm for frame 137

Lowest mean error: 2.5115208625793457 mm for frame 84

Saving results

Total time: 42.75437355041504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018007
Iteration 2/25 | Loss: 0.00189578
Iteration 3/25 | Loss: 0.00193651
Iteration 4/25 | Loss: 0.00188801
Iteration 5/25 | Loss: 0.00176519
Iteration 6/25 | Loss: 0.00124058
Iteration 7/25 | Loss: 0.00123123
Iteration 8/25 | Loss: 0.00122873
Iteration 9/25 | Loss: 0.00122778
Iteration 10/25 | Loss: 0.00122686
Iteration 11/25 | Loss: 0.00122577
Iteration 12/25 | Loss: 0.00122512
Iteration 13/25 | Loss: 0.00122961
Iteration 14/25 | Loss: 0.00122737
Iteration 15/25 | Loss: 0.00122448
Iteration 16/25 | Loss: 0.00122259
Iteration 17/25 | Loss: 0.00122238
Iteration 18/25 | Loss: 0.00122238
Iteration 19/25 | Loss: 0.00122238
Iteration 20/25 | Loss: 0.00122238
Iteration 21/25 | Loss: 0.00122238
Iteration 22/25 | Loss: 0.00122238
Iteration 23/25 | Loss: 0.00122238
Iteration 24/25 | Loss: 0.00122238
Iteration 25/25 | Loss: 0.00122238

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23231375
Iteration 2/25 | Loss: 0.00195542
Iteration 3/25 | Loss: 0.00183664
Iteration 4/25 | Loss: 0.00183664
Iteration 5/25 | Loss: 0.00183664
Iteration 6/25 | Loss: 0.00183664
Iteration 7/25 | Loss: 0.00183664
Iteration 8/25 | Loss: 0.00183664
Iteration 9/25 | Loss: 0.00183663
Iteration 10/25 | Loss: 0.00183663
Iteration 11/25 | Loss: 0.00183663
Iteration 12/25 | Loss: 0.00183663
Iteration 13/25 | Loss: 0.00183663
Iteration 14/25 | Loss: 0.00183663
Iteration 15/25 | Loss: 0.00183663
Iteration 16/25 | Loss: 0.00183663
Iteration 17/25 | Loss: 0.00183663
Iteration 18/25 | Loss: 0.00183663
Iteration 19/25 | Loss: 0.00183663
Iteration 20/25 | Loss: 0.00183663
Iteration 21/25 | Loss: 0.00183663
Iteration 22/25 | Loss: 0.00183663
Iteration 23/25 | Loss: 0.00183663
Iteration 24/25 | Loss: 0.00183663
Iteration 25/25 | Loss: 0.00183663

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183663
Iteration 2/1000 | Loss: 0.00006872
Iteration 3/1000 | Loss: 0.00019046
Iteration 4/1000 | Loss: 0.00002368
Iteration 5/1000 | Loss: 0.00002244
Iteration 6/1000 | Loss: 0.00008048
Iteration 7/1000 | Loss: 0.00002195
Iteration 8/1000 | Loss: 0.00002518
Iteration 9/1000 | Loss: 0.00005299
Iteration 10/1000 | Loss: 0.00002345
Iteration 11/1000 | Loss: 0.00001725
Iteration 12/1000 | Loss: 0.00009376
Iteration 13/1000 | Loss: 0.00057965
Iteration 14/1000 | Loss: 0.00002465
Iteration 15/1000 | Loss: 0.00002550
Iteration 16/1000 | Loss: 0.00030322
Iteration 17/1000 | Loss: 0.00067485
Iteration 18/1000 | Loss: 0.00078437
Iteration 19/1000 | Loss: 0.00023773
Iteration 20/1000 | Loss: 0.00026491
Iteration 21/1000 | Loss: 0.00006342
Iteration 22/1000 | Loss: 0.00007752
Iteration 23/1000 | Loss: 0.00005109
Iteration 24/1000 | Loss: 0.00013237
Iteration 25/1000 | Loss: 0.00005314
Iteration 26/1000 | Loss: 0.00004305
Iteration 27/1000 | Loss: 0.00001571
Iteration 28/1000 | Loss: 0.00005120
Iteration 29/1000 | Loss: 0.00001932
Iteration 30/1000 | Loss: 0.00002917
Iteration 31/1000 | Loss: 0.00008995
Iteration 32/1000 | Loss: 0.00002268
Iteration 33/1000 | Loss: 0.00003185
Iteration 34/1000 | Loss: 0.00005582
Iteration 35/1000 | Loss: 0.00003064
Iteration 36/1000 | Loss: 0.00001545
Iteration 37/1000 | Loss: 0.00001545
Iteration 38/1000 | Loss: 0.00001545
Iteration 39/1000 | Loss: 0.00001545
Iteration 40/1000 | Loss: 0.00001545
Iteration 41/1000 | Loss: 0.00001545
Iteration 42/1000 | Loss: 0.00001545
Iteration 43/1000 | Loss: 0.00001544
Iteration 44/1000 | Loss: 0.00001544
Iteration 45/1000 | Loss: 0.00004445
Iteration 46/1000 | Loss: 0.00003078
Iteration 47/1000 | Loss: 0.00002323
Iteration 48/1000 | Loss: 0.00002863
Iteration 49/1000 | Loss: 0.00001596
Iteration 50/1000 | Loss: 0.00007182
Iteration 51/1000 | Loss: 0.00004359
Iteration 52/1000 | Loss: 0.00004521
Iteration 53/1000 | Loss: 0.00002463
Iteration 54/1000 | Loss: 0.00003563
Iteration 55/1000 | Loss: 0.00001543
Iteration 56/1000 | Loss: 0.00002149
Iteration 57/1000 | Loss: 0.00001535
Iteration 58/1000 | Loss: 0.00002574
Iteration 59/1000 | Loss: 0.00001562
Iteration 60/1000 | Loss: 0.00002164
Iteration 61/1000 | Loss: 0.00001561
Iteration 62/1000 | Loss: 0.00001531
Iteration 63/1000 | Loss: 0.00001531
Iteration 64/1000 | Loss: 0.00001531
Iteration 65/1000 | Loss: 0.00001531
Iteration 66/1000 | Loss: 0.00001531
Iteration 67/1000 | Loss: 0.00001531
Iteration 68/1000 | Loss: 0.00001531
Iteration 69/1000 | Loss: 0.00001531
Iteration 70/1000 | Loss: 0.00001531
Iteration 71/1000 | Loss: 0.00001531
Iteration 72/1000 | Loss: 0.00001531
Iteration 73/1000 | Loss: 0.00001531
Iteration 74/1000 | Loss: 0.00001531
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.530531153548509e-05, 1.530531153548509e-05, 1.530531153548509e-05, 1.530531153548509e-05, 1.530531153548509e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.530531153548509e-05

Optimization complete. Final v2v error: 3.3529868125915527 mm

Highest mean error: 4.067162990570068 mm for frame 4

Lowest mean error: 3.025850534439087 mm for frame 172

Saving results

Total time: 103.88042879104614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809033
Iteration 2/25 | Loss: 0.00127525
Iteration 3/25 | Loss: 0.00120030
Iteration 4/25 | Loss: 0.00119219
Iteration 5/25 | Loss: 0.00119039
Iteration 6/25 | Loss: 0.00119031
Iteration 7/25 | Loss: 0.00119031
Iteration 8/25 | Loss: 0.00119031
Iteration 9/25 | Loss: 0.00119031
Iteration 10/25 | Loss: 0.00119031
Iteration 11/25 | Loss: 0.00119031
Iteration 12/25 | Loss: 0.00119031
Iteration 13/25 | Loss: 0.00119031
Iteration 14/25 | Loss: 0.00119031
Iteration 15/25 | Loss: 0.00119031
Iteration 16/25 | Loss: 0.00119031
Iteration 17/25 | Loss: 0.00119031
Iteration 18/25 | Loss: 0.00119031
Iteration 19/25 | Loss: 0.00119031
Iteration 20/25 | Loss: 0.00119031
Iteration 21/25 | Loss: 0.00119031
Iteration 22/25 | Loss: 0.00119031
Iteration 23/25 | Loss: 0.00119031
Iteration 24/25 | Loss: 0.00119031
Iteration 25/25 | Loss: 0.00119031

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28567028
Iteration 2/25 | Loss: 0.00126895
Iteration 3/25 | Loss: 0.00126893
Iteration 4/25 | Loss: 0.00126893
Iteration 5/25 | Loss: 0.00126893
Iteration 6/25 | Loss: 0.00126893
Iteration 7/25 | Loss: 0.00126893
Iteration 8/25 | Loss: 0.00126893
Iteration 9/25 | Loss: 0.00126893
Iteration 10/25 | Loss: 0.00126893
Iteration 11/25 | Loss: 0.00126893
Iteration 12/25 | Loss: 0.00126893
Iteration 13/25 | Loss: 0.00126893
Iteration 14/25 | Loss: 0.00126893
Iteration 15/25 | Loss: 0.00126893
Iteration 16/25 | Loss: 0.00126893
Iteration 17/25 | Loss: 0.00126893
Iteration 18/25 | Loss: 0.00126893
Iteration 19/25 | Loss: 0.00126893
Iteration 20/25 | Loss: 0.00126893
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012689250288531184, 0.0012689250288531184, 0.0012689250288531184, 0.0012689250288531184, 0.0012689250288531184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012689250288531184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126893
Iteration 2/1000 | Loss: 0.00001926
Iteration 3/1000 | Loss: 0.00001365
Iteration 4/1000 | Loss: 0.00001223
Iteration 5/1000 | Loss: 0.00001124
Iteration 6/1000 | Loss: 0.00001056
Iteration 7/1000 | Loss: 0.00001002
Iteration 8/1000 | Loss: 0.00000967
Iteration 9/1000 | Loss: 0.00000956
Iteration 10/1000 | Loss: 0.00000937
Iteration 11/1000 | Loss: 0.00000916
Iteration 12/1000 | Loss: 0.00000915
Iteration 13/1000 | Loss: 0.00000909
Iteration 14/1000 | Loss: 0.00000905
Iteration 15/1000 | Loss: 0.00000904
Iteration 16/1000 | Loss: 0.00000901
Iteration 17/1000 | Loss: 0.00000900
Iteration 18/1000 | Loss: 0.00000891
Iteration 19/1000 | Loss: 0.00000890
Iteration 20/1000 | Loss: 0.00000884
Iteration 21/1000 | Loss: 0.00000883
Iteration 22/1000 | Loss: 0.00000880
Iteration 23/1000 | Loss: 0.00000879
Iteration 24/1000 | Loss: 0.00000878
Iteration 25/1000 | Loss: 0.00000878
Iteration 26/1000 | Loss: 0.00000877
Iteration 27/1000 | Loss: 0.00000877
Iteration 28/1000 | Loss: 0.00000877
Iteration 29/1000 | Loss: 0.00000876
Iteration 30/1000 | Loss: 0.00000876
Iteration 31/1000 | Loss: 0.00000876
Iteration 32/1000 | Loss: 0.00000875
Iteration 33/1000 | Loss: 0.00000875
Iteration 34/1000 | Loss: 0.00000874
Iteration 35/1000 | Loss: 0.00000874
Iteration 36/1000 | Loss: 0.00000873
Iteration 37/1000 | Loss: 0.00000873
Iteration 38/1000 | Loss: 0.00000872
Iteration 39/1000 | Loss: 0.00000872
Iteration 40/1000 | Loss: 0.00000871
Iteration 41/1000 | Loss: 0.00000871
Iteration 42/1000 | Loss: 0.00000871
Iteration 43/1000 | Loss: 0.00000871
Iteration 44/1000 | Loss: 0.00000871
Iteration 45/1000 | Loss: 0.00000871
Iteration 46/1000 | Loss: 0.00000870
Iteration 47/1000 | Loss: 0.00000870
Iteration 48/1000 | Loss: 0.00000870
Iteration 49/1000 | Loss: 0.00000870
Iteration 50/1000 | Loss: 0.00000870
Iteration 51/1000 | Loss: 0.00000870
Iteration 52/1000 | Loss: 0.00000870
Iteration 53/1000 | Loss: 0.00000870
Iteration 54/1000 | Loss: 0.00000869
Iteration 55/1000 | Loss: 0.00000868
Iteration 56/1000 | Loss: 0.00000868
Iteration 57/1000 | Loss: 0.00000867
Iteration 58/1000 | Loss: 0.00000867
Iteration 59/1000 | Loss: 0.00000867
Iteration 60/1000 | Loss: 0.00000867
Iteration 61/1000 | Loss: 0.00000867
Iteration 62/1000 | Loss: 0.00000867
Iteration 63/1000 | Loss: 0.00000866
Iteration 64/1000 | Loss: 0.00000866
Iteration 65/1000 | Loss: 0.00000866
Iteration 66/1000 | Loss: 0.00000865
Iteration 67/1000 | Loss: 0.00000865
Iteration 68/1000 | Loss: 0.00000864
Iteration 69/1000 | Loss: 0.00000863
Iteration 70/1000 | Loss: 0.00000863
Iteration 71/1000 | Loss: 0.00000863
Iteration 72/1000 | Loss: 0.00000863
Iteration 73/1000 | Loss: 0.00000863
Iteration 74/1000 | Loss: 0.00000863
Iteration 75/1000 | Loss: 0.00000863
Iteration 76/1000 | Loss: 0.00000862
Iteration 77/1000 | Loss: 0.00000862
Iteration 78/1000 | Loss: 0.00000862
Iteration 79/1000 | Loss: 0.00000862
Iteration 80/1000 | Loss: 0.00000862
Iteration 81/1000 | Loss: 0.00000862
Iteration 82/1000 | Loss: 0.00000861
Iteration 83/1000 | Loss: 0.00000861
Iteration 84/1000 | Loss: 0.00000861
Iteration 85/1000 | Loss: 0.00000861
Iteration 86/1000 | Loss: 0.00000860
Iteration 87/1000 | Loss: 0.00000860
Iteration 88/1000 | Loss: 0.00000860
Iteration 89/1000 | Loss: 0.00000860
Iteration 90/1000 | Loss: 0.00000860
Iteration 91/1000 | Loss: 0.00000859
Iteration 92/1000 | Loss: 0.00000859
Iteration 93/1000 | Loss: 0.00000859
Iteration 94/1000 | Loss: 0.00000858
Iteration 95/1000 | Loss: 0.00000858
Iteration 96/1000 | Loss: 0.00000857
Iteration 97/1000 | Loss: 0.00000856
Iteration 98/1000 | Loss: 0.00000855
Iteration 99/1000 | Loss: 0.00000855
Iteration 100/1000 | Loss: 0.00000854
Iteration 101/1000 | Loss: 0.00000854
Iteration 102/1000 | Loss: 0.00000854
Iteration 103/1000 | Loss: 0.00000854
Iteration 104/1000 | Loss: 0.00000854
Iteration 105/1000 | Loss: 0.00000854
Iteration 106/1000 | Loss: 0.00000854
Iteration 107/1000 | Loss: 0.00000854
Iteration 108/1000 | Loss: 0.00000854
Iteration 109/1000 | Loss: 0.00000854
Iteration 110/1000 | Loss: 0.00000853
Iteration 111/1000 | Loss: 0.00000851
Iteration 112/1000 | Loss: 0.00000851
Iteration 113/1000 | Loss: 0.00000851
Iteration 114/1000 | Loss: 0.00000851
Iteration 115/1000 | Loss: 0.00000850
Iteration 116/1000 | Loss: 0.00000850
Iteration 117/1000 | Loss: 0.00000850
Iteration 118/1000 | Loss: 0.00000849
Iteration 119/1000 | Loss: 0.00000849
Iteration 120/1000 | Loss: 0.00000849
Iteration 121/1000 | Loss: 0.00000849
Iteration 122/1000 | Loss: 0.00000849
Iteration 123/1000 | Loss: 0.00000849
Iteration 124/1000 | Loss: 0.00000848
Iteration 125/1000 | Loss: 0.00000848
Iteration 126/1000 | Loss: 0.00000848
Iteration 127/1000 | Loss: 0.00000848
Iteration 128/1000 | Loss: 0.00000848
Iteration 129/1000 | Loss: 0.00000847
Iteration 130/1000 | Loss: 0.00000847
Iteration 131/1000 | Loss: 0.00000847
Iteration 132/1000 | Loss: 0.00000847
Iteration 133/1000 | Loss: 0.00000847
Iteration 134/1000 | Loss: 0.00000847
Iteration 135/1000 | Loss: 0.00000846
Iteration 136/1000 | Loss: 0.00000846
Iteration 137/1000 | Loss: 0.00000846
Iteration 138/1000 | Loss: 0.00000846
Iteration 139/1000 | Loss: 0.00000846
Iteration 140/1000 | Loss: 0.00000846
Iteration 141/1000 | Loss: 0.00000846
Iteration 142/1000 | Loss: 0.00000845
Iteration 143/1000 | Loss: 0.00000845
Iteration 144/1000 | Loss: 0.00000845
Iteration 145/1000 | Loss: 0.00000844
Iteration 146/1000 | Loss: 0.00000844
Iteration 147/1000 | Loss: 0.00000844
Iteration 148/1000 | Loss: 0.00000844
Iteration 149/1000 | Loss: 0.00000844
Iteration 150/1000 | Loss: 0.00000844
Iteration 151/1000 | Loss: 0.00000844
Iteration 152/1000 | Loss: 0.00000844
Iteration 153/1000 | Loss: 0.00000844
Iteration 154/1000 | Loss: 0.00000844
Iteration 155/1000 | Loss: 0.00000844
Iteration 156/1000 | Loss: 0.00000843
Iteration 157/1000 | Loss: 0.00000843
Iteration 158/1000 | Loss: 0.00000843
Iteration 159/1000 | Loss: 0.00000843
Iteration 160/1000 | Loss: 0.00000843
Iteration 161/1000 | Loss: 0.00000842
Iteration 162/1000 | Loss: 0.00000842
Iteration 163/1000 | Loss: 0.00000842
Iteration 164/1000 | Loss: 0.00000842
Iteration 165/1000 | Loss: 0.00000842
Iteration 166/1000 | Loss: 0.00000842
Iteration 167/1000 | Loss: 0.00000842
Iteration 168/1000 | Loss: 0.00000842
Iteration 169/1000 | Loss: 0.00000842
Iteration 170/1000 | Loss: 0.00000842
Iteration 171/1000 | Loss: 0.00000842
Iteration 172/1000 | Loss: 0.00000842
Iteration 173/1000 | Loss: 0.00000841
Iteration 174/1000 | Loss: 0.00000841
Iteration 175/1000 | Loss: 0.00000841
Iteration 176/1000 | Loss: 0.00000841
Iteration 177/1000 | Loss: 0.00000841
Iteration 178/1000 | Loss: 0.00000841
Iteration 179/1000 | Loss: 0.00000841
Iteration 180/1000 | Loss: 0.00000841
Iteration 181/1000 | Loss: 0.00000841
Iteration 182/1000 | Loss: 0.00000841
Iteration 183/1000 | Loss: 0.00000841
Iteration 184/1000 | Loss: 0.00000841
Iteration 185/1000 | Loss: 0.00000841
Iteration 186/1000 | Loss: 0.00000840
Iteration 187/1000 | Loss: 0.00000840
Iteration 188/1000 | Loss: 0.00000840
Iteration 189/1000 | Loss: 0.00000840
Iteration 190/1000 | Loss: 0.00000840
Iteration 191/1000 | Loss: 0.00000840
Iteration 192/1000 | Loss: 0.00000840
Iteration 193/1000 | Loss: 0.00000840
Iteration 194/1000 | Loss: 0.00000840
Iteration 195/1000 | Loss: 0.00000840
Iteration 196/1000 | Loss: 0.00000840
Iteration 197/1000 | Loss: 0.00000840
Iteration 198/1000 | Loss: 0.00000840
Iteration 199/1000 | Loss: 0.00000840
Iteration 200/1000 | Loss: 0.00000840
Iteration 201/1000 | Loss: 0.00000840
Iteration 202/1000 | Loss: 0.00000840
Iteration 203/1000 | Loss: 0.00000839
Iteration 204/1000 | Loss: 0.00000839
Iteration 205/1000 | Loss: 0.00000839
Iteration 206/1000 | Loss: 0.00000839
Iteration 207/1000 | Loss: 0.00000839
Iteration 208/1000 | Loss: 0.00000839
Iteration 209/1000 | Loss: 0.00000839
Iteration 210/1000 | Loss: 0.00000839
Iteration 211/1000 | Loss: 0.00000839
Iteration 212/1000 | Loss: 0.00000839
Iteration 213/1000 | Loss: 0.00000839
Iteration 214/1000 | Loss: 0.00000839
Iteration 215/1000 | Loss: 0.00000839
Iteration 216/1000 | Loss: 0.00000838
Iteration 217/1000 | Loss: 0.00000838
Iteration 218/1000 | Loss: 0.00000838
Iteration 219/1000 | Loss: 0.00000838
Iteration 220/1000 | Loss: 0.00000837
Iteration 221/1000 | Loss: 0.00000837
Iteration 222/1000 | Loss: 0.00000837
Iteration 223/1000 | Loss: 0.00000837
Iteration 224/1000 | Loss: 0.00000837
Iteration 225/1000 | Loss: 0.00000837
Iteration 226/1000 | Loss: 0.00000837
Iteration 227/1000 | Loss: 0.00000837
Iteration 228/1000 | Loss: 0.00000837
Iteration 229/1000 | Loss: 0.00000837
Iteration 230/1000 | Loss: 0.00000837
Iteration 231/1000 | Loss: 0.00000837
Iteration 232/1000 | Loss: 0.00000837
Iteration 233/1000 | Loss: 0.00000837
Iteration 234/1000 | Loss: 0.00000837
Iteration 235/1000 | Loss: 0.00000836
Iteration 236/1000 | Loss: 0.00000836
Iteration 237/1000 | Loss: 0.00000836
Iteration 238/1000 | Loss: 0.00000836
Iteration 239/1000 | Loss: 0.00000836
Iteration 240/1000 | Loss: 0.00000836
Iteration 241/1000 | Loss: 0.00000836
Iteration 242/1000 | Loss: 0.00000836
Iteration 243/1000 | Loss: 0.00000836
Iteration 244/1000 | Loss: 0.00000836
Iteration 245/1000 | Loss: 0.00000836
Iteration 246/1000 | Loss: 0.00000836
Iteration 247/1000 | Loss: 0.00000836
Iteration 248/1000 | Loss: 0.00000836
Iteration 249/1000 | Loss: 0.00000836
Iteration 250/1000 | Loss: 0.00000836
Iteration 251/1000 | Loss: 0.00000836
Iteration 252/1000 | Loss: 0.00000836
Iteration 253/1000 | Loss: 0.00000836
Iteration 254/1000 | Loss: 0.00000836
Iteration 255/1000 | Loss: 0.00000836
Iteration 256/1000 | Loss: 0.00000835
Iteration 257/1000 | Loss: 0.00000835
Iteration 258/1000 | Loss: 0.00000835
Iteration 259/1000 | Loss: 0.00000835
Iteration 260/1000 | Loss: 0.00000835
Iteration 261/1000 | Loss: 0.00000835
Iteration 262/1000 | Loss: 0.00000835
Iteration 263/1000 | Loss: 0.00000835
Iteration 264/1000 | Loss: 0.00000835
Iteration 265/1000 | Loss: 0.00000835
Iteration 266/1000 | Loss: 0.00000835
Iteration 267/1000 | Loss: 0.00000835
Iteration 268/1000 | Loss: 0.00000835
Iteration 269/1000 | Loss: 0.00000835
Iteration 270/1000 | Loss: 0.00000835
Iteration 271/1000 | Loss: 0.00000835
Iteration 272/1000 | Loss: 0.00000835
Iteration 273/1000 | Loss: 0.00000834
Iteration 274/1000 | Loss: 0.00000834
Iteration 275/1000 | Loss: 0.00000834
Iteration 276/1000 | Loss: 0.00000834
Iteration 277/1000 | Loss: 0.00000834
Iteration 278/1000 | Loss: 0.00000834
Iteration 279/1000 | Loss: 0.00000834
Iteration 280/1000 | Loss: 0.00000834
Iteration 281/1000 | Loss: 0.00000834
Iteration 282/1000 | Loss: 0.00000834
Iteration 283/1000 | Loss: 0.00000834
Iteration 284/1000 | Loss: 0.00000834
Iteration 285/1000 | Loss: 0.00000834
Iteration 286/1000 | Loss: 0.00000834
Iteration 287/1000 | Loss: 0.00000834
Iteration 288/1000 | Loss: 0.00000834
Iteration 289/1000 | Loss: 0.00000834
Iteration 290/1000 | Loss: 0.00000834
Iteration 291/1000 | Loss: 0.00000834
Iteration 292/1000 | Loss: 0.00000834
Iteration 293/1000 | Loss: 0.00000834
Iteration 294/1000 | Loss: 0.00000834
Iteration 295/1000 | Loss: 0.00000834
Iteration 296/1000 | Loss: 0.00000834
Iteration 297/1000 | Loss: 0.00000834
Iteration 298/1000 | Loss: 0.00000834
Iteration 299/1000 | Loss: 0.00000834
Iteration 300/1000 | Loss: 0.00000834
Iteration 301/1000 | Loss: 0.00000834
Iteration 302/1000 | Loss: 0.00000834
Iteration 303/1000 | Loss: 0.00000834
Iteration 304/1000 | Loss: 0.00000834
Iteration 305/1000 | Loss: 0.00000834
Iteration 306/1000 | Loss: 0.00000834
Iteration 307/1000 | Loss: 0.00000834
Iteration 308/1000 | Loss: 0.00000834
Iteration 309/1000 | Loss: 0.00000834
Iteration 310/1000 | Loss: 0.00000834
Iteration 311/1000 | Loss: 0.00000834
Iteration 312/1000 | Loss: 0.00000834
Iteration 313/1000 | Loss: 0.00000834
Iteration 314/1000 | Loss: 0.00000834
Iteration 315/1000 | Loss: 0.00000834
Iteration 316/1000 | Loss: 0.00000834
Iteration 317/1000 | Loss: 0.00000834
Iteration 318/1000 | Loss: 0.00000834
Iteration 319/1000 | Loss: 0.00000834
Iteration 320/1000 | Loss: 0.00000834
Iteration 321/1000 | Loss: 0.00000834
Iteration 322/1000 | Loss: 0.00000834
Iteration 323/1000 | Loss: 0.00000834
Iteration 324/1000 | Loss: 0.00000834
Iteration 325/1000 | Loss: 0.00000834
Iteration 326/1000 | Loss: 0.00000834
Iteration 327/1000 | Loss: 0.00000834
Iteration 328/1000 | Loss: 0.00000834
Iteration 329/1000 | Loss: 0.00000834
Iteration 330/1000 | Loss: 0.00000834
Iteration 331/1000 | Loss: 0.00000834
Iteration 332/1000 | Loss: 0.00000834
Iteration 333/1000 | Loss: 0.00000834
Iteration 334/1000 | Loss: 0.00000834
Iteration 335/1000 | Loss: 0.00000834
Iteration 336/1000 | Loss: 0.00000834
Iteration 337/1000 | Loss: 0.00000834
Iteration 338/1000 | Loss: 0.00000834
Iteration 339/1000 | Loss: 0.00000834
Iteration 340/1000 | Loss: 0.00000834
Iteration 341/1000 | Loss: 0.00000834
Iteration 342/1000 | Loss: 0.00000834
Iteration 343/1000 | Loss: 0.00000834
Iteration 344/1000 | Loss: 0.00000834
Iteration 345/1000 | Loss: 0.00000834
Iteration 346/1000 | Loss: 0.00000834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 346. Stopping optimization.
Last 5 losses: [8.337861800100654e-06, 8.337861800100654e-06, 8.337861800100654e-06, 8.337861800100654e-06, 8.337861800100654e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.337861800100654e-06

Optimization complete. Final v2v error: 2.5066206455230713 mm

Highest mean error: 2.6845412254333496 mm for frame 3

Lowest mean error: 2.384470224380493 mm for frame 46

Saving results

Total time: 45.30448532104492
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060914
Iteration 2/25 | Loss: 0.00296864
Iteration 3/25 | Loss: 0.00226870
Iteration 4/25 | Loss: 0.00198818
Iteration 5/25 | Loss: 0.00180529
Iteration 6/25 | Loss: 0.00175913
Iteration 7/25 | Loss: 0.00170798
Iteration 8/25 | Loss: 0.00166173
Iteration 9/25 | Loss: 0.00157935
Iteration 10/25 | Loss: 0.00155159
Iteration 11/25 | Loss: 0.00153612
Iteration 12/25 | Loss: 0.00152353
Iteration 13/25 | Loss: 0.00151588
Iteration 14/25 | Loss: 0.00150639
Iteration 15/25 | Loss: 0.00151902
Iteration 16/25 | Loss: 0.00152525
Iteration 17/25 | Loss: 0.00151652
Iteration 18/25 | Loss: 0.00150281
Iteration 19/25 | Loss: 0.00149802
Iteration 20/25 | Loss: 0.00148576
Iteration 21/25 | Loss: 0.00148657
Iteration 22/25 | Loss: 0.00148671
Iteration 23/25 | Loss: 0.00148085
Iteration 24/25 | Loss: 0.00147911
Iteration 25/25 | Loss: 0.00147683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01791346
Iteration 2/25 | Loss: 0.00144649
Iteration 3/25 | Loss: 0.00118204
Iteration 4/25 | Loss: 0.00118203
Iteration 5/25 | Loss: 0.00118203
Iteration 6/25 | Loss: 0.00118203
Iteration 7/25 | Loss: 0.00118203
Iteration 8/25 | Loss: 0.00118203
Iteration 9/25 | Loss: 0.00118203
Iteration 10/25 | Loss: 0.00118203
Iteration 11/25 | Loss: 0.00118203
Iteration 12/25 | Loss: 0.00118203
Iteration 13/25 | Loss: 0.00118203
Iteration 14/25 | Loss: 0.00118203
Iteration 15/25 | Loss: 0.00118203
Iteration 16/25 | Loss: 0.00118203
Iteration 17/25 | Loss: 0.00118203
Iteration 18/25 | Loss: 0.00118203
Iteration 19/25 | Loss: 0.00118203
Iteration 20/25 | Loss: 0.00118203
Iteration 21/25 | Loss: 0.00118203
Iteration 22/25 | Loss: 0.00118203
Iteration 23/25 | Loss: 0.00118203
Iteration 24/25 | Loss: 0.00118203
Iteration 25/25 | Loss: 0.00118203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118203
Iteration 2/1000 | Loss: 0.00017101
Iteration 3/1000 | Loss: 0.00009198
Iteration 4/1000 | Loss: 0.00009915
Iteration 5/1000 | Loss: 0.00010925
Iteration 6/1000 | Loss: 0.00006701
Iteration 7/1000 | Loss: 0.00007238
Iteration 8/1000 | Loss: 0.00009850
Iteration 9/1000 | Loss: 0.00190010
Iteration 10/1000 | Loss: 0.00222656
Iteration 11/1000 | Loss: 0.00045147
Iteration 12/1000 | Loss: 0.00014319
Iteration 13/1000 | Loss: 0.00039359
Iteration 14/1000 | Loss: 0.00009829
Iteration 15/1000 | Loss: 0.00009644
Iteration 16/1000 | Loss: 0.00009005
Iteration 17/1000 | Loss: 0.00018137
Iteration 18/1000 | Loss: 0.00013382
Iteration 19/1000 | Loss: 0.00015642
Iteration 20/1000 | Loss: 0.00023482
Iteration 21/1000 | Loss: 0.00025264
Iteration 22/1000 | Loss: 0.00009056
Iteration 23/1000 | Loss: 0.00204599
Iteration 24/1000 | Loss: 0.00106691
Iteration 25/1000 | Loss: 0.00009194
Iteration 26/1000 | Loss: 0.00176839
Iteration 27/1000 | Loss: 0.00070192
Iteration 28/1000 | Loss: 0.00008538
Iteration 29/1000 | Loss: 0.00256405
Iteration 30/1000 | Loss: 0.00076688
Iteration 31/1000 | Loss: 0.00107222
Iteration 32/1000 | Loss: 0.00037141
Iteration 33/1000 | Loss: 0.00174404
Iteration 34/1000 | Loss: 0.00053164
Iteration 35/1000 | Loss: 0.00051908
Iteration 36/1000 | Loss: 0.00023863
Iteration 37/1000 | Loss: 0.00081964
Iteration 38/1000 | Loss: 0.00186870
Iteration 39/1000 | Loss: 0.00061660
Iteration 40/1000 | Loss: 0.00073961
Iteration 41/1000 | Loss: 0.00189052
Iteration 42/1000 | Loss: 0.00052658
Iteration 43/1000 | Loss: 0.00064196
Iteration 44/1000 | Loss: 0.00109327
Iteration 45/1000 | Loss: 0.00066645
Iteration 46/1000 | Loss: 0.00063710
Iteration 47/1000 | Loss: 0.00065918
Iteration 48/1000 | Loss: 0.00098226
Iteration 49/1000 | Loss: 0.00058257
Iteration 50/1000 | Loss: 0.00081632
Iteration 51/1000 | Loss: 0.00096781
Iteration 52/1000 | Loss: 0.00081219
Iteration 53/1000 | Loss: 0.00065084
Iteration 54/1000 | Loss: 0.00049656
Iteration 55/1000 | Loss: 0.00083516
Iteration 56/1000 | Loss: 0.00069813
Iteration 57/1000 | Loss: 0.00062289
Iteration 58/1000 | Loss: 0.00039599
Iteration 59/1000 | Loss: 0.00029462
Iteration 60/1000 | Loss: 0.00017314
Iteration 61/1000 | Loss: 0.00054711
Iteration 62/1000 | Loss: 0.00023203
Iteration 63/1000 | Loss: 0.00039157
Iteration 64/1000 | Loss: 0.00085055
Iteration 65/1000 | Loss: 0.00055369
Iteration 66/1000 | Loss: 0.00083684
Iteration 67/1000 | Loss: 0.00073068
Iteration 68/1000 | Loss: 0.00032355
Iteration 69/1000 | Loss: 0.00045930
Iteration 70/1000 | Loss: 0.00035179
Iteration 71/1000 | Loss: 0.00036388
Iteration 72/1000 | Loss: 0.00039621
Iteration 73/1000 | Loss: 0.00032973
Iteration 74/1000 | Loss: 0.00029060
Iteration 75/1000 | Loss: 0.00030963
Iteration 76/1000 | Loss: 0.00036223
Iteration 77/1000 | Loss: 0.00044075
Iteration 78/1000 | Loss: 0.00057005
Iteration 79/1000 | Loss: 0.00010244
Iteration 80/1000 | Loss: 0.00055453
Iteration 81/1000 | Loss: 0.00046966
Iteration 82/1000 | Loss: 0.00020361
Iteration 83/1000 | Loss: 0.00019072
Iteration 84/1000 | Loss: 0.00031102
Iteration 85/1000 | Loss: 0.00016983
Iteration 86/1000 | Loss: 0.00022552
Iteration 87/1000 | Loss: 0.00018572
Iteration 88/1000 | Loss: 0.00012680
Iteration 89/1000 | Loss: 0.00027341
Iteration 90/1000 | Loss: 0.00044930
Iteration 91/1000 | Loss: 0.00050965
Iteration 92/1000 | Loss: 0.00036512
Iteration 93/1000 | Loss: 0.00052730
Iteration 94/1000 | Loss: 0.00027467
Iteration 95/1000 | Loss: 0.00018159
Iteration 96/1000 | Loss: 0.00017277
Iteration 97/1000 | Loss: 0.00007184
Iteration 98/1000 | Loss: 0.00022894
Iteration 99/1000 | Loss: 0.00006408
Iteration 100/1000 | Loss: 0.00010646
Iteration 101/1000 | Loss: 0.00005863
Iteration 102/1000 | Loss: 0.00024208
Iteration 103/1000 | Loss: 0.00015668
Iteration 104/1000 | Loss: 0.00023332
Iteration 105/1000 | Loss: 0.00006317
Iteration 106/1000 | Loss: 0.00005670
Iteration 107/1000 | Loss: 0.00005191
Iteration 108/1000 | Loss: 0.00010863
Iteration 109/1000 | Loss: 0.00005124
Iteration 110/1000 | Loss: 0.00018934
Iteration 111/1000 | Loss: 0.00005676
Iteration 112/1000 | Loss: 0.00007925
Iteration 113/1000 | Loss: 0.00015663
Iteration 114/1000 | Loss: 0.00005502
Iteration 115/1000 | Loss: 0.00006649
Iteration 116/1000 | Loss: 0.00009393
Iteration 117/1000 | Loss: 0.00048210
Iteration 118/1000 | Loss: 0.00023246
Iteration 119/1000 | Loss: 0.00037867
Iteration 120/1000 | Loss: 0.00006403
Iteration 121/1000 | Loss: 0.00009512
Iteration 122/1000 | Loss: 0.00005214
Iteration 123/1000 | Loss: 0.00005536
Iteration 124/1000 | Loss: 0.00005525
Iteration 125/1000 | Loss: 0.00007626
Iteration 126/1000 | Loss: 0.00006414
Iteration 127/1000 | Loss: 0.00004818
Iteration 128/1000 | Loss: 0.00004511
Iteration 129/1000 | Loss: 0.00008585
Iteration 130/1000 | Loss: 0.00006735
Iteration 131/1000 | Loss: 0.00010137
Iteration 132/1000 | Loss: 0.00007361
Iteration 133/1000 | Loss: 0.00006879
Iteration 134/1000 | Loss: 0.00007210
Iteration 135/1000 | Loss: 0.00004310
Iteration 136/1000 | Loss: 0.00004235
Iteration 137/1000 | Loss: 0.00005462
Iteration 138/1000 | Loss: 0.00008417
Iteration 139/1000 | Loss: 0.00005514
Iteration 140/1000 | Loss: 0.00015776
Iteration 141/1000 | Loss: 0.00008967
Iteration 142/1000 | Loss: 0.00008746
Iteration 143/1000 | Loss: 0.00006062
Iteration 144/1000 | Loss: 0.00006850
Iteration 145/1000 | Loss: 0.00006559
Iteration 146/1000 | Loss: 0.00007663
Iteration 147/1000 | Loss: 0.00006582
Iteration 148/1000 | Loss: 0.00010093
Iteration 149/1000 | Loss: 0.00005772
Iteration 150/1000 | Loss: 0.00006554
Iteration 151/1000 | Loss: 0.00010652
Iteration 152/1000 | Loss: 0.00009655
Iteration 153/1000 | Loss: 0.00005364
Iteration 154/1000 | Loss: 0.00006504
Iteration 155/1000 | Loss: 0.00006878
Iteration 156/1000 | Loss: 0.00015540
Iteration 157/1000 | Loss: 0.00009211
Iteration 158/1000 | Loss: 0.00011545
Iteration 159/1000 | Loss: 0.00010488
Iteration 160/1000 | Loss: 0.00008980
Iteration 161/1000 | Loss: 0.00012912
Iteration 162/1000 | Loss: 0.00007754
Iteration 163/1000 | Loss: 0.00026152
Iteration 164/1000 | Loss: 0.00023766
Iteration 165/1000 | Loss: 0.00008106
Iteration 166/1000 | Loss: 0.00006991
Iteration 167/1000 | Loss: 0.00007896
Iteration 168/1000 | Loss: 0.00008177
Iteration 169/1000 | Loss: 0.00007725
Iteration 170/1000 | Loss: 0.00008725
Iteration 171/1000 | Loss: 0.00007486
Iteration 172/1000 | Loss: 0.00007311
Iteration 173/1000 | Loss: 0.00007095
Iteration 174/1000 | Loss: 0.00004225
Iteration 175/1000 | Loss: 0.00004052
Iteration 176/1000 | Loss: 0.00003905
Iteration 177/1000 | Loss: 0.00010712
Iteration 178/1000 | Loss: 0.00003793
Iteration 179/1000 | Loss: 0.00027417
Iteration 180/1000 | Loss: 0.00005084
Iteration 181/1000 | Loss: 0.00005699
Iteration 182/1000 | Loss: 0.00003827
Iteration 183/1000 | Loss: 0.00003730
Iteration 184/1000 | Loss: 0.00003653
Iteration 185/1000 | Loss: 0.00006748
Iteration 186/1000 | Loss: 0.00003580
Iteration 187/1000 | Loss: 0.00003552
Iteration 188/1000 | Loss: 0.00003537
Iteration 189/1000 | Loss: 0.00003529
Iteration 190/1000 | Loss: 0.00003522
Iteration 191/1000 | Loss: 0.00003521
Iteration 192/1000 | Loss: 0.00003521
Iteration 193/1000 | Loss: 0.00003518
Iteration 194/1000 | Loss: 0.00003518
Iteration 195/1000 | Loss: 0.00003515
Iteration 196/1000 | Loss: 0.00003507
Iteration 197/1000 | Loss: 0.00003507
Iteration 198/1000 | Loss: 0.00003507
Iteration 199/1000 | Loss: 0.00003507
Iteration 200/1000 | Loss: 0.00003507
Iteration 201/1000 | Loss: 0.00003507
Iteration 202/1000 | Loss: 0.00003507
Iteration 203/1000 | Loss: 0.00003507
Iteration 204/1000 | Loss: 0.00003506
Iteration 205/1000 | Loss: 0.00003506
Iteration 206/1000 | Loss: 0.00003506
Iteration 207/1000 | Loss: 0.00003506
Iteration 208/1000 | Loss: 0.00003506
Iteration 209/1000 | Loss: 0.00003506
Iteration 210/1000 | Loss: 0.00003506
Iteration 211/1000 | Loss: 0.00003505
Iteration 212/1000 | Loss: 0.00003505
Iteration 213/1000 | Loss: 0.00003505
Iteration 214/1000 | Loss: 0.00003505
Iteration 215/1000 | Loss: 0.00003505
Iteration 216/1000 | Loss: 0.00003505
Iteration 217/1000 | Loss: 0.00003505
Iteration 218/1000 | Loss: 0.00003505
Iteration 219/1000 | Loss: 0.00003505
Iteration 220/1000 | Loss: 0.00003505
Iteration 221/1000 | Loss: 0.00003505
Iteration 222/1000 | Loss: 0.00003505
Iteration 223/1000 | Loss: 0.00003504
Iteration 224/1000 | Loss: 0.00003504
Iteration 225/1000 | Loss: 0.00003504
Iteration 226/1000 | Loss: 0.00003504
Iteration 227/1000 | Loss: 0.00003504
Iteration 228/1000 | Loss: 0.00003503
Iteration 229/1000 | Loss: 0.00003503
Iteration 230/1000 | Loss: 0.00003503
Iteration 231/1000 | Loss: 0.00003503
Iteration 232/1000 | Loss: 0.00003503
Iteration 233/1000 | Loss: 0.00003503
Iteration 234/1000 | Loss: 0.00003502
Iteration 235/1000 | Loss: 0.00003502
Iteration 236/1000 | Loss: 0.00003502
Iteration 237/1000 | Loss: 0.00003501
Iteration 238/1000 | Loss: 0.00003501
Iteration 239/1000 | Loss: 0.00003501
Iteration 240/1000 | Loss: 0.00003501
Iteration 241/1000 | Loss: 0.00003501
Iteration 242/1000 | Loss: 0.00003501
Iteration 243/1000 | Loss: 0.00003500
Iteration 244/1000 | Loss: 0.00003500
Iteration 245/1000 | Loss: 0.00003500
Iteration 246/1000 | Loss: 0.00003500
Iteration 247/1000 | Loss: 0.00003500
Iteration 248/1000 | Loss: 0.00003500
Iteration 249/1000 | Loss: 0.00003500
Iteration 250/1000 | Loss: 0.00003500
Iteration 251/1000 | Loss: 0.00003499
Iteration 252/1000 | Loss: 0.00003499
Iteration 253/1000 | Loss: 0.00003499
Iteration 254/1000 | Loss: 0.00003499
Iteration 255/1000 | Loss: 0.00003499
Iteration 256/1000 | Loss: 0.00003499
Iteration 257/1000 | Loss: 0.00003499
Iteration 258/1000 | Loss: 0.00003499
Iteration 259/1000 | Loss: 0.00003499
Iteration 260/1000 | Loss: 0.00003499
Iteration 261/1000 | Loss: 0.00003498
Iteration 262/1000 | Loss: 0.00003498
Iteration 263/1000 | Loss: 0.00003498
Iteration 264/1000 | Loss: 0.00003498
Iteration 265/1000 | Loss: 0.00003498
Iteration 266/1000 | Loss: 0.00003498
Iteration 267/1000 | Loss: 0.00003498
Iteration 268/1000 | Loss: 0.00003498
Iteration 269/1000 | Loss: 0.00003498
Iteration 270/1000 | Loss: 0.00003498
Iteration 271/1000 | Loss: 0.00003498
Iteration 272/1000 | Loss: 0.00003498
Iteration 273/1000 | Loss: 0.00003498
Iteration 274/1000 | Loss: 0.00003498
Iteration 275/1000 | Loss: 0.00003497
Iteration 276/1000 | Loss: 0.00003497
Iteration 277/1000 | Loss: 0.00003497
Iteration 278/1000 | Loss: 0.00003497
Iteration 279/1000 | Loss: 0.00003497
Iteration 280/1000 | Loss: 0.00003497
Iteration 281/1000 | Loss: 0.00003497
Iteration 282/1000 | Loss: 0.00003497
Iteration 283/1000 | Loss: 0.00003497
Iteration 284/1000 | Loss: 0.00003497
Iteration 285/1000 | Loss: 0.00003497
Iteration 286/1000 | Loss: 0.00003497
Iteration 287/1000 | Loss: 0.00003496
Iteration 288/1000 | Loss: 0.00003496
Iteration 289/1000 | Loss: 0.00003496
Iteration 290/1000 | Loss: 0.00003496
Iteration 291/1000 | Loss: 0.00003496
Iteration 292/1000 | Loss: 0.00003496
Iteration 293/1000 | Loss: 0.00003495
Iteration 294/1000 | Loss: 0.00003495
Iteration 295/1000 | Loss: 0.00003495
Iteration 296/1000 | Loss: 0.00003495
Iteration 297/1000 | Loss: 0.00003495
Iteration 298/1000 | Loss: 0.00003495
Iteration 299/1000 | Loss: 0.00003495
Iteration 300/1000 | Loss: 0.00003495
Iteration 301/1000 | Loss: 0.00003495
Iteration 302/1000 | Loss: 0.00003495
Iteration 303/1000 | Loss: 0.00003495
Iteration 304/1000 | Loss: 0.00003495
Iteration 305/1000 | Loss: 0.00003494
Iteration 306/1000 | Loss: 0.00003494
Iteration 307/1000 | Loss: 0.00003494
Iteration 308/1000 | Loss: 0.00003494
Iteration 309/1000 | Loss: 0.00003494
Iteration 310/1000 | Loss: 0.00003494
Iteration 311/1000 | Loss: 0.00003494
Iteration 312/1000 | Loss: 0.00003494
Iteration 313/1000 | Loss: 0.00003494
Iteration 314/1000 | Loss: 0.00003494
Iteration 315/1000 | Loss: 0.00003494
Iteration 316/1000 | Loss: 0.00003494
Iteration 317/1000 | Loss: 0.00003494
Iteration 318/1000 | Loss: 0.00003494
Iteration 319/1000 | Loss: 0.00003494
Iteration 320/1000 | Loss: 0.00003494
Iteration 321/1000 | Loss: 0.00003494
Iteration 322/1000 | Loss: 0.00003494
Iteration 323/1000 | Loss: 0.00003494
Iteration 324/1000 | Loss: 0.00003493
Iteration 325/1000 | Loss: 0.00003493
Iteration 326/1000 | Loss: 0.00003493
Iteration 327/1000 | Loss: 0.00003493
Iteration 328/1000 | Loss: 0.00003493
Iteration 329/1000 | Loss: 0.00003493
Iteration 330/1000 | Loss: 0.00003493
Iteration 331/1000 | Loss: 0.00003493
Iteration 332/1000 | Loss: 0.00003493
Iteration 333/1000 | Loss: 0.00003493
Iteration 334/1000 | Loss: 0.00003493
Iteration 335/1000 | Loss: 0.00003493
Iteration 336/1000 | Loss: 0.00003493
Iteration 337/1000 | Loss: 0.00003493
Iteration 338/1000 | Loss: 0.00003493
Iteration 339/1000 | Loss: 0.00003493
Iteration 340/1000 | Loss: 0.00003493
Iteration 341/1000 | Loss: 0.00003493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 341. Stopping optimization.
Last 5 losses: [3.492681571515277e-05, 3.492681571515277e-05, 3.492681571515277e-05, 3.492681571515277e-05, 3.492681571515277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.492681571515277e-05

Optimization complete. Final v2v error: 4.671527862548828 mm

Highest mean error: 8.218758583068848 mm for frame 61

Lowest mean error: 3.7218422889709473 mm for frame 0

Saving results

Total time: 367.337641954422
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00721486
Iteration 2/25 | Loss: 0.00177204
Iteration 3/25 | Loss: 0.00140347
Iteration 4/25 | Loss: 0.00134400
Iteration 5/25 | Loss: 0.00132939
Iteration 6/25 | Loss: 0.00132744
Iteration 7/25 | Loss: 0.00132452
Iteration 8/25 | Loss: 0.00132221
Iteration 9/25 | Loss: 0.00132091
Iteration 10/25 | Loss: 0.00132053
Iteration 11/25 | Loss: 0.00132047
Iteration 12/25 | Loss: 0.00132047
Iteration 13/25 | Loss: 0.00132047
Iteration 14/25 | Loss: 0.00132047
Iteration 15/25 | Loss: 0.00132047
Iteration 16/25 | Loss: 0.00132047
Iteration 17/25 | Loss: 0.00132046
Iteration 18/25 | Loss: 0.00132046
Iteration 19/25 | Loss: 0.00132046
Iteration 20/25 | Loss: 0.00132046
Iteration 21/25 | Loss: 0.00132046
Iteration 22/25 | Loss: 0.00132046
Iteration 23/25 | Loss: 0.00132046
Iteration 24/25 | Loss: 0.00132046
Iteration 25/25 | Loss: 0.00132046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.24248552
Iteration 2/25 | Loss: 0.00199375
Iteration 3/25 | Loss: 0.00199375
Iteration 4/25 | Loss: 0.00199374
Iteration 5/25 | Loss: 0.00199374
Iteration 6/25 | Loss: 0.00199374
Iteration 7/25 | Loss: 0.00199374
Iteration 8/25 | Loss: 0.00199374
Iteration 9/25 | Loss: 0.00199374
Iteration 10/25 | Loss: 0.00199374
Iteration 11/25 | Loss: 0.00199374
Iteration 12/25 | Loss: 0.00199374
Iteration 13/25 | Loss: 0.00199374
Iteration 14/25 | Loss: 0.00199374
Iteration 15/25 | Loss: 0.00199374
Iteration 16/25 | Loss: 0.00199374
Iteration 17/25 | Loss: 0.00199374
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019937404431402683, 0.0019937404431402683, 0.0019937404431402683, 0.0019937404431402683, 0.0019937404431402683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019937404431402683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199374
Iteration 2/1000 | Loss: 0.00318593
Iteration 3/1000 | Loss: 0.00009252
Iteration 4/1000 | Loss: 0.00009079
Iteration 5/1000 | Loss: 0.00004939
Iteration 6/1000 | Loss: 0.00004134
Iteration 7/1000 | Loss: 0.00003782
Iteration 8/1000 | Loss: 0.00006282
Iteration 9/1000 | Loss: 0.00129210
Iteration 10/1000 | Loss: 0.00018954
Iteration 11/1000 | Loss: 0.00013776
Iteration 12/1000 | Loss: 0.00006624
Iteration 13/1000 | Loss: 0.00003923
Iteration 14/1000 | Loss: 0.00006733
Iteration 15/1000 | Loss: 0.00005995
Iteration 16/1000 | Loss: 0.00006459
Iteration 17/1000 | Loss: 0.00005612
Iteration 18/1000 | Loss: 0.00005476
Iteration 19/1000 | Loss: 0.00005008
Iteration 20/1000 | Loss: 0.00004754
Iteration 21/1000 | Loss: 0.00004259
Iteration 22/1000 | Loss: 0.00004782
Iteration 23/1000 | Loss: 0.00004820
Iteration 24/1000 | Loss: 0.00003240
Iteration 25/1000 | Loss: 0.00004101
Iteration 26/1000 | Loss: 0.00003280
Iteration 27/1000 | Loss: 0.00006352
Iteration 28/1000 | Loss: 0.00002983
Iteration 29/1000 | Loss: 0.00004894
Iteration 30/1000 | Loss: 0.00013624
Iteration 31/1000 | Loss: 0.00005731
Iteration 32/1000 | Loss: 0.00002991
Iteration 33/1000 | Loss: 0.00010386
Iteration 34/1000 | Loss: 0.00013353
Iteration 35/1000 | Loss: 0.00013969
Iteration 36/1000 | Loss: 0.00003418
Iteration 37/1000 | Loss: 0.00003404
Iteration 38/1000 | Loss: 0.00011417
Iteration 39/1000 | Loss: 0.00002683
Iteration 40/1000 | Loss: 0.00002588
Iteration 41/1000 | Loss: 0.00002532
Iteration 42/1000 | Loss: 0.00002485
Iteration 43/1000 | Loss: 0.00002464
Iteration 44/1000 | Loss: 0.00006212
Iteration 45/1000 | Loss: 0.00002724
Iteration 46/1000 | Loss: 0.00002552
Iteration 47/1000 | Loss: 0.00002502
Iteration 48/1000 | Loss: 0.00002473
Iteration 49/1000 | Loss: 0.00002465
Iteration 50/1000 | Loss: 0.00002450
Iteration 51/1000 | Loss: 0.00002430
Iteration 52/1000 | Loss: 0.00005531
Iteration 53/1000 | Loss: 0.00002978
Iteration 54/1000 | Loss: 0.00003949
Iteration 55/1000 | Loss: 0.00005342
Iteration 56/1000 | Loss: 0.00006469
Iteration 57/1000 | Loss: 0.00003030
Iteration 58/1000 | Loss: 0.00005403
Iteration 59/1000 | Loss: 0.00003972
Iteration 60/1000 | Loss: 0.00005244
Iteration 61/1000 | Loss: 0.00004073
Iteration 62/1000 | Loss: 0.00005243
Iteration 63/1000 | Loss: 0.00003850
Iteration 64/1000 | Loss: 0.00005207
Iteration 65/1000 | Loss: 0.00003897
Iteration 66/1000 | Loss: 0.00005054
Iteration 67/1000 | Loss: 0.00003708
Iteration 68/1000 | Loss: 0.00004358
Iteration 69/1000 | Loss: 0.00003811
Iteration 70/1000 | Loss: 0.00004352
Iteration 71/1000 | Loss: 0.00003982
Iteration 72/1000 | Loss: 0.00006500
Iteration 73/1000 | Loss: 0.00005817
Iteration 74/1000 | Loss: 0.00003341
Iteration 75/1000 | Loss: 0.00004075
Iteration 76/1000 | Loss: 0.00003376
Iteration 77/1000 | Loss: 0.00004120
Iteration 78/1000 | Loss: 0.00004050
Iteration 79/1000 | Loss: 0.00004120
Iteration 80/1000 | Loss: 0.00002817
Iteration 81/1000 | Loss: 0.00004463
Iteration 82/1000 | Loss: 0.00005167
Iteration 83/1000 | Loss: 0.00004089
Iteration 84/1000 | Loss: 0.00003018
Iteration 85/1000 | Loss: 0.00002879
Iteration 86/1000 | Loss: 0.00002786
Iteration 87/1000 | Loss: 0.00003779
Iteration 88/1000 | Loss: 0.00002924
Iteration 89/1000 | Loss: 0.00002452
Iteration 90/1000 | Loss: 0.00002398
Iteration 91/1000 | Loss: 0.00002392
Iteration 92/1000 | Loss: 0.00002391
Iteration 93/1000 | Loss: 0.00002383
Iteration 94/1000 | Loss: 0.00002382
Iteration 95/1000 | Loss: 0.00002379
Iteration 96/1000 | Loss: 0.00002373
Iteration 97/1000 | Loss: 0.00002373
Iteration 98/1000 | Loss: 0.00002372
Iteration 99/1000 | Loss: 0.00002371
Iteration 100/1000 | Loss: 0.00002371
Iteration 101/1000 | Loss: 0.00002371
Iteration 102/1000 | Loss: 0.00002370
Iteration 103/1000 | Loss: 0.00002370
Iteration 104/1000 | Loss: 0.00002370
Iteration 105/1000 | Loss: 0.00002369
Iteration 106/1000 | Loss: 0.00002368
Iteration 107/1000 | Loss: 0.00002367
Iteration 108/1000 | Loss: 0.00002366
Iteration 109/1000 | Loss: 0.00002365
Iteration 110/1000 | Loss: 0.00002363
Iteration 111/1000 | Loss: 0.00002363
Iteration 112/1000 | Loss: 0.00002362
Iteration 113/1000 | Loss: 0.00002362
Iteration 114/1000 | Loss: 0.00002362
Iteration 115/1000 | Loss: 0.00002361
Iteration 116/1000 | Loss: 0.00002361
Iteration 117/1000 | Loss: 0.00002361
Iteration 118/1000 | Loss: 0.00002360
Iteration 119/1000 | Loss: 0.00002360
Iteration 120/1000 | Loss: 0.00002360
Iteration 121/1000 | Loss: 0.00002360
Iteration 122/1000 | Loss: 0.00002360
Iteration 123/1000 | Loss: 0.00002360
Iteration 124/1000 | Loss: 0.00002360
Iteration 125/1000 | Loss: 0.00002359
Iteration 126/1000 | Loss: 0.00002359
Iteration 127/1000 | Loss: 0.00002359
Iteration 128/1000 | Loss: 0.00004619
Iteration 129/1000 | Loss: 0.00003399
Iteration 130/1000 | Loss: 0.00004929
Iteration 131/1000 | Loss: 0.00003851
Iteration 132/1000 | Loss: 0.00004693
Iteration 133/1000 | Loss: 0.00003923
Iteration 134/1000 | Loss: 0.00005234
Iteration 135/1000 | Loss: 0.00003798
Iteration 136/1000 | Loss: 0.00005468
Iteration 137/1000 | Loss: 0.00005126
Iteration 138/1000 | Loss: 0.00003961
Iteration 139/1000 | Loss: 0.00004522
Iteration 140/1000 | Loss: 0.00004091
Iteration 141/1000 | Loss: 0.00004070
Iteration 142/1000 | Loss: 0.00006089
Iteration 143/1000 | Loss: 0.00005320
Iteration 144/1000 | Loss: 0.00003604
Iteration 145/1000 | Loss: 0.00004459
Iteration 146/1000 | Loss: 0.00006426
Iteration 147/1000 | Loss: 0.00004839
Iteration 148/1000 | Loss: 0.00003431
Iteration 149/1000 | Loss: 0.00004489
Iteration 150/1000 | Loss: 0.00003224
Iteration 151/1000 | Loss: 0.00017538
Iteration 152/1000 | Loss: 0.00016473
Iteration 153/1000 | Loss: 0.00002951
Iteration 154/1000 | Loss: 0.00002783
Iteration 155/1000 | Loss: 0.00002592
Iteration 156/1000 | Loss: 0.00002504
Iteration 157/1000 | Loss: 0.00003435
Iteration 158/1000 | Loss: 0.00003138
Iteration 159/1000 | Loss: 0.00003866
Iteration 160/1000 | Loss: 0.00003636
Iteration 161/1000 | Loss: 0.00002426
Iteration 162/1000 | Loss: 0.00002426
Iteration 163/1000 | Loss: 0.00002412
Iteration 164/1000 | Loss: 0.00002399
Iteration 165/1000 | Loss: 0.00002393
Iteration 166/1000 | Loss: 0.00002388
Iteration 167/1000 | Loss: 0.00002385
Iteration 168/1000 | Loss: 0.00002385
Iteration 169/1000 | Loss: 0.00002384
Iteration 170/1000 | Loss: 0.00002383
Iteration 171/1000 | Loss: 0.00002383
Iteration 172/1000 | Loss: 0.00002382
Iteration 173/1000 | Loss: 0.00002382
Iteration 174/1000 | Loss: 0.00002382
Iteration 175/1000 | Loss: 0.00002381
Iteration 176/1000 | Loss: 0.00002381
Iteration 177/1000 | Loss: 0.00002381
Iteration 178/1000 | Loss: 0.00002381
Iteration 179/1000 | Loss: 0.00002381
Iteration 180/1000 | Loss: 0.00002381
Iteration 181/1000 | Loss: 0.00002381
Iteration 182/1000 | Loss: 0.00002381
Iteration 183/1000 | Loss: 0.00002380
Iteration 184/1000 | Loss: 0.00002380
Iteration 185/1000 | Loss: 0.00002380
Iteration 186/1000 | Loss: 0.00002380
Iteration 187/1000 | Loss: 0.00002380
Iteration 188/1000 | Loss: 0.00002380
Iteration 189/1000 | Loss: 0.00002379
Iteration 190/1000 | Loss: 0.00002379
Iteration 191/1000 | Loss: 0.00002379
Iteration 192/1000 | Loss: 0.00002379
Iteration 193/1000 | Loss: 0.00002379
Iteration 194/1000 | Loss: 0.00002379
Iteration 195/1000 | Loss: 0.00002379
Iteration 196/1000 | Loss: 0.00002379
Iteration 197/1000 | Loss: 0.00002379
Iteration 198/1000 | Loss: 0.00002379
Iteration 199/1000 | Loss: 0.00002378
Iteration 200/1000 | Loss: 0.00002378
Iteration 201/1000 | Loss: 0.00002378
Iteration 202/1000 | Loss: 0.00002378
Iteration 203/1000 | Loss: 0.00002378
Iteration 204/1000 | Loss: 0.00002377
Iteration 205/1000 | Loss: 0.00002377
Iteration 206/1000 | Loss: 0.00002377
Iteration 207/1000 | Loss: 0.00002377
Iteration 208/1000 | Loss: 0.00002377
Iteration 209/1000 | Loss: 0.00002377
Iteration 210/1000 | Loss: 0.00002376
Iteration 211/1000 | Loss: 0.00002376
Iteration 212/1000 | Loss: 0.00002376
Iteration 213/1000 | Loss: 0.00002376
Iteration 214/1000 | Loss: 0.00002376
Iteration 215/1000 | Loss: 0.00002376
Iteration 216/1000 | Loss: 0.00002376
Iteration 217/1000 | Loss: 0.00002376
Iteration 218/1000 | Loss: 0.00002376
Iteration 219/1000 | Loss: 0.00002376
Iteration 220/1000 | Loss: 0.00002376
Iteration 221/1000 | Loss: 0.00002376
Iteration 222/1000 | Loss: 0.00002376
Iteration 223/1000 | Loss: 0.00002376
Iteration 224/1000 | Loss: 0.00002376
Iteration 225/1000 | Loss: 0.00002376
Iteration 226/1000 | Loss: 0.00002376
Iteration 227/1000 | Loss: 0.00002376
Iteration 228/1000 | Loss: 0.00002376
Iteration 229/1000 | Loss: 0.00002376
Iteration 230/1000 | Loss: 0.00002376
Iteration 231/1000 | Loss: 0.00002376
Iteration 232/1000 | Loss: 0.00002376
Iteration 233/1000 | Loss: 0.00002376
Iteration 234/1000 | Loss: 0.00002376
Iteration 235/1000 | Loss: 0.00002376
Iteration 236/1000 | Loss: 0.00002376
Iteration 237/1000 | Loss: 0.00002376
Iteration 238/1000 | Loss: 0.00002376
Iteration 239/1000 | Loss: 0.00002376
Iteration 240/1000 | Loss: 0.00002376
Iteration 241/1000 | Loss: 0.00002376
Iteration 242/1000 | Loss: 0.00002376
Iteration 243/1000 | Loss: 0.00002376
Iteration 244/1000 | Loss: 0.00002376
Iteration 245/1000 | Loss: 0.00002376
Iteration 246/1000 | Loss: 0.00002376
Iteration 247/1000 | Loss: 0.00002376
Iteration 248/1000 | Loss: 0.00002376
Iteration 249/1000 | Loss: 0.00002376
Iteration 250/1000 | Loss: 0.00002376
Iteration 251/1000 | Loss: 0.00002376
Iteration 252/1000 | Loss: 0.00002376
Iteration 253/1000 | Loss: 0.00002376
Iteration 254/1000 | Loss: 0.00002376
Iteration 255/1000 | Loss: 0.00002376
Iteration 256/1000 | Loss: 0.00002376
Iteration 257/1000 | Loss: 0.00002376
Iteration 258/1000 | Loss: 0.00002376
Iteration 259/1000 | Loss: 0.00002376
Iteration 260/1000 | Loss: 0.00002376
Iteration 261/1000 | Loss: 0.00002376
Iteration 262/1000 | Loss: 0.00002376
Iteration 263/1000 | Loss: 0.00002376
Iteration 264/1000 | Loss: 0.00002376
Iteration 265/1000 | Loss: 0.00002376
Iteration 266/1000 | Loss: 0.00002376
Iteration 267/1000 | Loss: 0.00002376
Iteration 268/1000 | Loss: 0.00002376
Iteration 269/1000 | Loss: 0.00002376
Iteration 270/1000 | Loss: 0.00002376
Iteration 271/1000 | Loss: 0.00002376
Iteration 272/1000 | Loss: 0.00002376
Iteration 273/1000 | Loss: 0.00002376
Iteration 274/1000 | Loss: 0.00002376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 274. Stopping optimization.
Last 5 losses: [2.3759395844535902e-05, 2.3759395844535902e-05, 2.3759395844535902e-05, 2.3759395844535902e-05, 2.3759395844535902e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3759395844535902e-05

Optimization complete. Final v2v error: 3.9047200679779053 mm

Highest mean error: 10.373517990112305 mm for frame 66

Lowest mean error: 2.8551559448242188 mm for frame 108

Saving results

Total time: 205.83653593063354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873623
Iteration 2/25 | Loss: 0.00175746
Iteration 3/25 | Loss: 0.00139601
Iteration 4/25 | Loss: 0.00134192
Iteration 5/25 | Loss: 0.00132824
Iteration 6/25 | Loss: 0.00130846
Iteration 7/25 | Loss: 0.00130215
Iteration 8/25 | Loss: 0.00130575
Iteration 9/25 | Loss: 0.00130564
Iteration 10/25 | Loss: 0.00130340
Iteration 11/25 | Loss: 0.00129741
Iteration 12/25 | Loss: 0.00129101
Iteration 13/25 | Loss: 0.00128917
Iteration 14/25 | Loss: 0.00128886
Iteration 15/25 | Loss: 0.00128876
Iteration 16/25 | Loss: 0.00128876
Iteration 17/25 | Loss: 0.00128876
Iteration 18/25 | Loss: 0.00128876
Iteration 19/25 | Loss: 0.00128876
Iteration 20/25 | Loss: 0.00128876
Iteration 21/25 | Loss: 0.00128876
Iteration 22/25 | Loss: 0.00128875
Iteration 23/25 | Loss: 0.00128875
Iteration 24/25 | Loss: 0.00128875
Iteration 25/25 | Loss: 0.00128875

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.64682293
Iteration 2/25 | Loss: 0.00129220
Iteration 3/25 | Loss: 0.00129209
Iteration 4/25 | Loss: 0.00129209
Iteration 5/25 | Loss: 0.00129209
Iteration 6/25 | Loss: 0.00129209
Iteration 7/25 | Loss: 0.00129209
Iteration 8/25 | Loss: 0.00129209
Iteration 9/25 | Loss: 0.00129209
Iteration 10/25 | Loss: 0.00129209
Iteration 11/25 | Loss: 0.00129209
Iteration 12/25 | Loss: 0.00129209
Iteration 13/25 | Loss: 0.00129209
Iteration 14/25 | Loss: 0.00129209
Iteration 15/25 | Loss: 0.00129209
Iteration 16/25 | Loss: 0.00129209
Iteration 17/25 | Loss: 0.00129209
Iteration 18/25 | Loss: 0.00129209
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012920905137434602, 0.0012920905137434602, 0.0012920905137434602, 0.0012920905137434602, 0.0012920905137434602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012920905137434602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129209
Iteration 2/1000 | Loss: 0.00005217
Iteration 3/1000 | Loss: 0.00026161
Iteration 4/1000 | Loss: 0.00017421
Iteration 5/1000 | Loss: 0.00015026
Iteration 6/1000 | Loss: 0.00002862
Iteration 7/1000 | Loss: 0.00002556
Iteration 8/1000 | Loss: 0.00002333
Iteration 9/1000 | Loss: 0.00024378
Iteration 10/1000 | Loss: 0.00031169
Iteration 11/1000 | Loss: 0.00008417
Iteration 12/1000 | Loss: 0.00002900
Iteration 13/1000 | Loss: 0.00002589
Iteration 14/1000 | Loss: 0.00002475
Iteration 15/1000 | Loss: 0.00002395
Iteration 16/1000 | Loss: 0.00009717
Iteration 17/1000 | Loss: 0.00003331
Iteration 18/1000 | Loss: 0.00002527
Iteration 19/1000 | Loss: 0.00002153
Iteration 20/1000 | Loss: 0.00002108
Iteration 21/1000 | Loss: 0.00002070
Iteration 22/1000 | Loss: 0.00013556
Iteration 23/1000 | Loss: 0.00002092
Iteration 24/1000 | Loss: 0.00001974
Iteration 25/1000 | Loss: 0.00001932
Iteration 26/1000 | Loss: 0.00010356
Iteration 27/1000 | Loss: 0.00001873
Iteration 28/1000 | Loss: 0.00001855
Iteration 29/1000 | Loss: 0.00006742
Iteration 30/1000 | Loss: 0.00001868
Iteration 31/1000 | Loss: 0.00001823
Iteration 32/1000 | Loss: 0.00001816
Iteration 33/1000 | Loss: 0.00001813
Iteration 34/1000 | Loss: 0.00001812
Iteration 35/1000 | Loss: 0.00001811
Iteration 36/1000 | Loss: 0.00001811
Iteration 37/1000 | Loss: 0.00001811
Iteration 38/1000 | Loss: 0.00001811
Iteration 39/1000 | Loss: 0.00001810
Iteration 40/1000 | Loss: 0.00001809
Iteration 41/1000 | Loss: 0.00001806
Iteration 42/1000 | Loss: 0.00001806
Iteration 43/1000 | Loss: 0.00001805
Iteration 44/1000 | Loss: 0.00001805
Iteration 45/1000 | Loss: 0.00001804
Iteration 46/1000 | Loss: 0.00001804
Iteration 47/1000 | Loss: 0.00001804
Iteration 48/1000 | Loss: 0.00001803
Iteration 49/1000 | Loss: 0.00001803
Iteration 50/1000 | Loss: 0.00001803
Iteration 51/1000 | Loss: 0.00001802
Iteration 52/1000 | Loss: 0.00001802
Iteration 53/1000 | Loss: 0.00001801
Iteration 54/1000 | Loss: 0.00001801
Iteration 55/1000 | Loss: 0.00001800
Iteration 56/1000 | Loss: 0.00001800
Iteration 57/1000 | Loss: 0.00001800
Iteration 58/1000 | Loss: 0.00001800
Iteration 59/1000 | Loss: 0.00001800
Iteration 60/1000 | Loss: 0.00001800
Iteration 61/1000 | Loss: 0.00001799
Iteration 62/1000 | Loss: 0.00010370
Iteration 63/1000 | Loss: 0.00002124
Iteration 64/1000 | Loss: 0.00001801
Iteration 65/1000 | Loss: 0.00001800
Iteration 66/1000 | Loss: 0.00001799
Iteration 67/1000 | Loss: 0.00003844
Iteration 68/1000 | Loss: 0.00001801
Iteration 69/1000 | Loss: 0.00001797
Iteration 70/1000 | Loss: 0.00001797
Iteration 71/1000 | Loss: 0.00001797
Iteration 72/1000 | Loss: 0.00001797
Iteration 73/1000 | Loss: 0.00001797
Iteration 74/1000 | Loss: 0.00001796
Iteration 75/1000 | Loss: 0.00001796
Iteration 76/1000 | Loss: 0.00001796
Iteration 77/1000 | Loss: 0.00001795
Iteration 78/1000 | Loss: 0.00001795
Iteration 79/1000 | Loss: 0.00001795
Iteration 80/1000 | Loss: 0.00001794
Iteration 81/1000 | Loss: 0.00001794
Iteration 82/1000 | Loss: 0.00001794
Iteration 83/1000 | Loss: 0.00001793
Iteration 84/1000 | Loss: 0.00001793
Iteration 85/1000 | Loss: 0.00001792
Iteration 86/1000 | Loss: 0.00001792
Iteration 87/1000 | Loss: 0.00001791
Iteration 88/1000 | Loss: 0.00001791
Iteration 89/1000 | Loss: 0.00001791
Iteration 90/1000 | Loss: 0.00001790
Iteration 91/1000 | Loss: 0.00001790
Iteration 92/1000 | Loss: 0.00001789
Iteration 93/1000 | Loss: 0.00001789
Iteration 94/1000 | Loss: 0.00001789
Iteration 95/1000 | Loss: 0.00001789
Iteration 96/1000 | Loss: 0.00001789
Iteration 97/1000 | Loss: 0.00001788
Iteration 98/1000 | Loss: 0.00001788
Iteration 99/1000 | Loss: 0.00001788
Iteration 100/1000 | Loss: 0.00001788
Iteration 101/1000 | Loss: 0.00001788
Iteration 102/1000 | Loss: 0.00001788
Iteration 103/1000 | Loss: 0.00001788
Iteration 104/1000 | Loss: 0.00001788
Iteration 105/1000 | Loss: 0.00001788
Iteration 106/1000 | Loss: 0.00001787
Iteration 107/1000 | Loss: 0.00001787
Iteration 108/1000 | Loss: 0.00006359
Iteration 109/1000 | Loss: 0.00001795
Iteration 110/1000 | Loss: 0.00001787
Iteration 111/1000 | Loss: 0.00001784
Iteration 112/1000 | Loss: 0.00001784
Iteration 113/1000 | Loss: 0.00001784
Iteration 114/1000 | Loss: 0.00001784
Iteration 115/1000 | Loss: 0.00001784
Iteration 116/1000 | Loss: 0.00001784
Iteration 117/1000 | Loss: 0.00001784
Iteration 118/1000 | Loss: 0.00001784
Iteration 119/1000 | Loss: 0.00001784
Iteration 120/1000 | Loss: 0.00001784
Iteration 121/1000 | Loss: 0.00001783
Iteration 122/1000 | Loss: 0.00001783
Iteration 123/1000 | Loss: 0.00001783
Iteration 124/1000 | Loss: 0.00001782
Iteration 125/1000 | Loss: 0.00001782
Iteration 126/1000 | Loss: 0.00001782
Iteration 127/1000 | Loss: 0.00001782
Iteration 128/1000 | Loss: 0.00001782
Iteration 129/1000 | Loss: 0.00001781
Iteration 130/1000 | Loss: 0.00001781
Iteration 131/1000 | Loss: 0.00001781
Iteration 132/1000 | Loss: 0.00001781
Iteration 133/1000 | Loss: 0.00001781
Iteration 134/1000 | Loss: 0.00001781
Iteration 135/1000 | Loss: 0.00001781
Iteration 136/1000 | Loss: 0.00001781
Iteration 137/1000 | Loss: 0.00001781
Iteration 138/1000 | Loss: 0.00001780
Iteration 139/1000 | Loss: 0.00001780
Iteration 140/1000 | Loss: 0.00001780
Iteration 141/1000 | Loss: 0.00001780
Iteration 142/1000 | Loss: 0.00001780
Iteration 143/1000 | Loss: 0.00001780
Iteration 144/1000 | Loss: 0.00001780
Iteration 145/1000 | Loss: 0.00001780
Iteration 146/1000 | Loss: 0.00001780
Iteration 147/1000 | Loss: 0.00001780
Iteration 148/1000 | Loss: 0.00001780
Iteration 149/1000 | Loss: 0.00001780
Iteration 150/1000 | Loss: 0.00001779
Iteration 151/1000 | Loss: 0.00001779
Iteration 152/1000 | Loss: 0.00001779
Iteration 153/1000 | Loss: 0.00001779
Iteration 154/1000 | Loss: 0.00001779
Iteration 155/1000 | Loss: 0.00001779
Iteration 156/1000 | Loss: 0.00001779
Iteration 157/1000 | Loss: 0.00001779
Iteration 158/1000 | Loss: 0.00001778
Iteration 159/1000 | Loss: 0.00001778
Iteration 160/1000 | Loss: 0.00001778
Iteration 161/1000 | Loss: 0.00001778
Iteration 162/1000 | Loss: 0.00001778
Iteration 163/1000 | Loss: 0.00001778
Iteration 164/1000 | Loss: 0.00001778
Iteration 165/1000 | Loss: 0.00001778
Iteration 166/1000 | Loss: 0.00001778
Iteration 167/1000 | Loss: 0.00001778
Iteration 168/1000 | Loss: 0.00001778
Iteration 169/1000 | Loss: 0.00001778
Iteration 170/1000 | Loss: 0.00001778
Iteration 171/1000 | Loss: 0.00001778
Iteration 172/1000 | Loss: 0.00001778
Iteration 173/1000 | Loss: 0.00001778
Iteration 174/1000 | Loss: 0.00001778
Iteration 175/1000 | Loss: 0.00001778
Iteration 176/1000 | Loss: 0.00001778
Iteration 177/1000 | Loss: 0.00001777
Iteration 178/1000 | Loss: 0.00001777
Iteration 179/1000 | Loss: 0.00001777
Iteration 180/1000 | Loss: 0.00001777
Iteration 181/1000 | Loss: 0.00001777
Iteration 182/1000 | Loss: 0.00001777
Iteration 183/1000 | Loss: 0.00001777
Iteration 184/1000 | Loss: 0.00001777
Iteration 185/1000 | Loss: 0.00001777
Iteration 186/1000 | Loss: 0.00001777
Iteration 187/1000 | Loss: 0.00001777
Iteration 188/1000 | Loss: 0.00001777
Iteration 189/1000 | Loss: 0.00001777
Iteration 190/1000 | Loss: 0.00001776
Iteration 191/1000 | Loss: 0.00001776
Iteration 192/1000 | Loss: 0.00001776
Iteration 193/1000 | Loss: 0.00001776
Iteration 194/1000 | Loss: 0.00001776
Iteration 195/1000 | Loss: 0.00001776
Iteration 196/1000 | Loss: 0.00001776
Iteration 197/1000 | Loss: 0.00001776
Iteration 198/1000 | Loss: 0.00001776
Iteration 199/1000 | Loss: 0.00001776
Iteration 200/1000 | Loss: 0.00001776
Iteration 201/1000 | Loss: 0.00001776
Iteration 202/1000 | Loss: 0.00001776
Iteration 203/1000 | Loss: 0.00001775
Iteration 204/1000 | Loss: 0.00001775
Iteration 205/1000 | Loss: 0.00001775
Iteration 206/1000 | Loss: 0.00001775
Iteration 207/1000 | Loss: 0.00001775
Iteration 208/1000 | Loss: 0.00001775
Iteration 209/1000 | Loss: 0.00001775
Iteration 210/1000 | Loss: 0.00001775
Iteration 211/1000 | Loss: 0.00001775
Iteration 212/1000 | Loss: 0.00001774
Iteration 213/1000 | Loss: 0.00001774
Iteration 214/1000 | Loss: 0.00001774
Iteration 215/1000 | Loss: 0.00001774
Iteration 216/1000 | Loss: 0.00001774
Iteration 217/1000 | Loss: 0.00001774
Iteration 218/1000 | Loss: 0.00001774
Iteration 219/1000 | Loss: 0.00001774
Iteration 220/1000 | Loss: 0.00001774
Iteration 221/1000 | Loss: 0.00001774
Iteration 222/1000 | Loss: 0.00001774
Iteration 223/1000 | Loss: 0.00001774
Iteration 224/1000 | Loss: 0.00001774
Iteration 225/1000 | Loss: 0.00001773
Iteration 226/1000 | Loss: 0.00001773
Iteration 227/1000 | Loss: 0.00001773
Iteration 228/1000 | Loss: 0.00001773
Iteration 229/1000 | Loss: 0.00001773
Iteration 230/1000 | Loss: 0.00001773
Iteration 231/1000 | Loss: 0.00001773
Iteration 232/1000 | Loss: 0.00001773
Iteration 233/1000 | Loss: 0.00001773
Iteration 234/1000 | Loss: 0.00001773
Iteration 235/1000 | Loss: 0.00001773
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.7734342691255733e-05, 1.7734342691255733e-05, 1.7734342691255733e-05, 1.7734342691255733e-05, 1.7734342691255733e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7734342691255733e-05

Optimization complete. Final v2v error: 3.5233805179595947 mm

Highest mean error: 5.585977077484131 mm for frame 99

Lowest mean error: 2.8654139041900635 mm for frame 19

Saving results

Total time: 94.26591467857361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398207
Iteration 2/25 | Loss: 0.00133762
Iteration 3/25 | Loss: 0.00121196
Iteration 4/25 | Loss: 0.00119597
Iteration 5/25 | Loss: 0.00119386
Iteration 6/25 | Loss: 0.00119374
Iteration 7/25 | Loss: 0.00119374
Iteration 8/25 | Loss: 0.00119374
Iteration 9/25 | Loss: 0.00119374
Iteration 10/25 | Loss: 0.00119374
Iteration 11/25 | Loss: 0.00119374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011937397066503763, 0.0011937397066503763, 0.0011937397066503763, 0.0011937397066503763, 0.0011937397066503763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011937397066503763

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27677429
Iteration 2/25 | Loss: 0.00123243
Iteration 3/25 | Loss: 0.00123243
Iteration 4/25 | Loss: 0.00123243
Iteration 5/25 | Loss: 0.00123242
Iteration 6/25 | Loss: 0.00123242
Iteration 7/25 | Loss: 0.00123242
Iteration 8/25 | Loss: 0.00123242
Iteration 9/25 | Loss: 0.00123242
Iteration 10/25 | Loss: 0.00123242
Iteration 11/25 | Loss: 0.00123242
Iteration 12/25 | Loss: 0.00123242
Iteration 13/25 | Loss: 0.00123242
Iteration 14/25 | Loss: 0.00123242
Iteration 15/25 | Loss: 0.00123242
Iteration 16/25 | Loss: 0.00123242
Iteration 17/25 | Loss: 0.00123242
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001232422306202352, 0.001232422306202352, 0.001232422306202352, 0.001232422306202352, 0.001232422306202352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001232422306202352

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123242
Iteration 2/1000 | Loss: 0.00002257
Iteration 3/1000 | Loss: 0.00001677
Iteration 4/1000 | Loss: 0.00001511
Iteration 5/1000 | Loss: 0.00001413
Iteration 6/1000 | Loss: 0.00001351
Iteration 7/1000 | Loss: 0.00001303
Iteration 8/1000 | Loss: 0.00001274
Iteration 9/1000 | Loss: 0.00001239
Iteration 10/1000 | Loss: 0.00001217
Iteration 11/1000 | Loss: 0.00001210
Iteration 12/1000 | Loss: 0.00001209
Iteration 13/1000 | Loss: 0.00001206
Iteration 14/1000 | Loss: 0.00001185
Iteration 15/1000 | Loss: 0.00001166
Iteration 16/1000 | Loss: 0.00001156
Iteration 17/1000 | Loss: 0.00001156
Iteration 18/1000 | Loss: 0.00001142
Iteration 19/1000 | Loss: 0.00001137
Iteration 20/1000 | Loss: 0.00001128
Iteration 21/1000 | Loss: 0.00001117
Iteration 22/1000 | Loss: 0.00001115
Iteration 23/1000 | Loss: 0.00001108
Iteration 24/1000 | Loss: 0.00001107
Iteration 25/1000 | Loss: 0.00001098
Iteration 26/1000 | Loss: 0.00001097
Iteration 27/1000 | Loss: 0.00001097
Iteration 28/1000 | Loss: 0.00001096
Iteration 29/1000 | Loss: 0.00001096
Iteration 30/1000 | Loss: 0.00001096
Iteration 31/1000 | Loss: 0.00001095
Iteration 32/1000 | Loss: 0.00001095
Iteration 33/1000 | Loss: 0.00001095
Iteration 34/1000 | Loss: 0.00001094
Iteration 35/1000 | Loss: 0.00001094
Iteration 36/1000 | Loss: 0.00001094
Iteration 37/1000 | Loss: 0.00001094
Iteration 38/1000 | Loss: 0.00001094
Iteration 39/1000 | Loss: 0.00001093
Iteration 40/1000 | Loss: 0.00001093
Iteration 41/1000 | Loss: 0.00001093
Iteration 42/1000 | Loss: 0.00001092
Iteration 43/1000 | Loss: 0.00001092
Iteration 44/1000 | Loss: 0.00001092
Iteration 45/1000 | Loss: 0.00001092
Iteration 46/1000 | Loss: 0.00001091
Iteration 47/1000 | Loss: 0.00001091
Iteration 48/1000 | Loss: 0.00001091
Iteration 49/1000 | Loss: 0.00001091
Iteration 50/1000 | Loss: 0.00001090
Iteration 51/1000 | Loss: 0.00001090
Iteration 52/1000 | Loss: 0.00001090
Iteration 53/1000 | Loss: 0.00001090
Iteration 54/1000 | Loss: 0.00001090
Iteration 55/1000 | Loss: 0.00001090
Iteration 56/1000 | Loss: 0.00001089
Iteration 57/1000 | Loss: 0.00001089
Iteration 58/1000 | Loss: 0.00001089
Iteration 59/1000 | Loss: 0.00001089
Iteration 60/1000 | Loss: 0.00001089
Iteration 61/1000 | Loss: 0.00001088
Iteration 62/1000 | Loss: 0.00001088
Iteration 63/1000 | Loss: 0.00001088
Iteration 64/1000 | Loss: 0.00001088
Iteration 65/1000 | Loss: 0.00001088
Iteration 66/1000 | Loss: 0.00001087
Iteration 67/1000 | Loss: 0.00001087
Iteration 68/1000 | Loss: 0.00001087
Iteration 69/1000 | Loss: 0.00001087
Iteration 70/1000 | Loss: 0.00001087
Iteration 71/1000 | Loss: 0.00001087
Iteration 72/1000 | Loss: 0.00001087
Iteration 73/1000 | Loss: 0.00001086
Iteration 74/1000 | Loss: 0.00001086
Iteration 75/1000 | Loss: 0.00001086
Iteration 76/1000 | Loss: 0.00001086
Iteration 77/1000 | Loss: 0.00001085
Iteration 78/1000 | Loss: 0.00001085
Iteration 79/1000 | Loss: 0.00001085
Iteration 80/1000 | Loss: 0.00001085
Iteration 81/1000 | Loss: 0.00001085
Iteration 82/1000 | Loss: 0.00001085
Iteration 83/1000 | Loss: 0.00001084
Iteration 84/1000 | Loss: 0.00001084
Iteration 85/1000 | Loss: 0.00001084
Iteration 86/1000 | Loss: 0.00001084
Iteration 87/1000 | Loss: 0.00001084
Iteration 88/1000 | Loss: 0.00001084
Iteration 89/1000 | Loss: 0.00001084
Iteration 90/1000 | Loss: 0.00001084
Iteration 91/1000 | Loss: 0.00001084
Iteration 92/1000 | Loss: 0.00001083
Iteration 93/1000 | Loss: 0.00001083
Iteration 94/1000 | Loss: 0.00001082
Iteration 95/1000 | Loss: 0.00001082
Iteration 96/1000 | Loss: 0.00001082
Iteration 97/1000 | Loss: 0.00001082
Iteration 98/1000 | Loss: 0.00001081
Iteration 99/1000 | Loss: 0.00001081
Iteration 100/1000 | Loss: 0.00001081
Iteration 101/1000 | Loss: 0.00001080
Iteration 102/1000 | Loss: 0.00001080
Iteration 103/1000 | Loss: 0.00001080
Iteration 104/1000 | Loss: 0.00001079
Iteration 105/1000 | Loss: 0.00001079
Iteration 106/1000 | Loss: 0.00001079
Iteration 107/1000 | Loss: 0.00001078
Iteration 108/1000 | Loss: 0.00001078
Iteration 109/1000 | Loss: 0.00001078
Iteration 110/1000 | Loss: 0.00001078
Iteration 111/1000 | Loss: 0.00001078
Iteration 112/1000 | Loss: 0.00001078
Iteration 113/1000 | Loss: 0.00001078
Iteration 114/1000 | Loss: 0.00001077
Iteration 115/1000 | Loss: 0.00001077
Iteration 116/1000 | Loss: 0.00001077
Iteration 117/1000 | Loss: 0.00001077
Iteration 118/1000 | Loss: 0.00001077
Iteration 119/1000 | Loss: 0.00001077
Iteration 120/1000 | Loss: 0.00001077
Iteration 121/1000 | Loss: 0.00001076
Iteration 122/1000 | Loss: 0.00001076
Iteration 123/1000 | Loss: 0.00001076
Iteration 124/1000 | Loss: 0.00001076
Iteration 125/1000 | Loss: 0.00001076
Iteration 126/1000 | Loss: 0.00001076
Iteration 127/1000 | Loss: 0.00001076
Iteration 128/1000 | Loss: 0.00001076
Iteration 129/1000 | Loss: 0.00001076
Iteration 130/1000 | Loss: 0.00001076
Iteration 131/1000 | Loss: 0.00001076
Iteration 132/1000 | Loss: 0.00001076
Iteration 133/1000 | Loss: 0.00001076
Iteration 134/1000 | Loss: 0.00001075
Iteration 135/1000 | Loss: 0.00001075
Iteration 136/1000 | Loss: 0.00001075
Iteration 137/1000 | Loss: 0.00001075
Iteration 138/1000 | Loss: 0.00001075
Iteration 139/1000 | Loss: 0.00001075
Iteration 140/1000 | Loss: 0.00001075
Iteration 141/1000 | Loss: 0.00001075
Iteration 142/1000 | Loss: 0.00001075
Iteration 143/1000 | Loss: 0.00001075
Iteration 144/1000 | Loss: 0.00001075
Iteration 145/1000 | Loss: 0.00001075
Iteration 146/1000 | Loss: 0.00001075
Iteration 147/1000 | Loss: 0.00001075
Iteration 148/1000 | Loss: 0.00001075
Iteration 149/1000 | Loss: 0.00001075
Iteration 150/1000 | Loss: 0.00001075
Iteration 151/1000 | Loss: 0.00001075
Iteration 152/1000 | Loss: 0.00001074
Iteration 153/1000 | Loss: 0.00001074
Iteration 154/1000 | Loss: 0.00001074
Iteration 155/1000 | Loss: 0.00001074
Iteration 156/1000 | Loss: 0.00001074
Iteration 157/1000 | Loss: 0.00001074
Iteration 158/1000 | Loss: 0.00001074
Iteration 159/1000 | Loss: 0.00001074
Iteration 160/1000 | Loss: 0.00001074
Iteration 161/1000 | Loss: 0.00001074
Iteration 162/1000 | Loss: 0.00001074
Iteration 163/1000 | Loss: 0.00001074
Iteration 164/1000 | Loss: 0.00001074
Iteration 165/1000 | Loss: 0.00001074
Iteration 166/1000 | Loss: 0.00001074
Iteration 167/1000 | Loss: 0.00001074
Iteration 168/1000 | Loss: 0.00001074
Iteration 169/1000 | Loss: 0.00001074
Iteration 170/1000 | Loss: 0.00001074
Iteration 171/1000 | Loss: 0.00001074
Iteration 172/1000 | Loss: 0.00001074
Iteration 173/1000 | Loss: 0.00001074
Iteration 174/1000 | Loss: 0.00001074
Iteration 175/1000 | Loss: 0.00001074
Iteration 176/1000 | Loss: 0.00001074
Iteration 177/1000 | Loss: 0.00001074
Iteration 178/1000 | Loss: 0.00001074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.0735986506915651e-05, 1.0735986506915651e-05, 1.0735986506915651e-05, 1.0735986506915651e-05, 1.0735986506915651e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0735986506915651e-05

Optimization complete. Final v2v error: 2.8156046867370605 mm

Highest mean error: 3.1266427040100098 mm for frame 85

Lowest mean error: 2.590449571609497 mm for frame 15

Saving results

Total time: 43.466341972351074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394314
Iteration 2/25 | Loss: 0.00123036
Iteration 3/25 | Loss: 0.00117865
Iteration 4/25 | Loss: 0.00117253
Iteration 5/25 | Loss: 0.00117058
Iteration 6/25 | Loss: 0.00117036
Iteration 7/25 | Loss: 0.00117036
Iteration 8/25 | Loss: 0.00117036
Iteration 9/25 | Loss: 0.00117036
Iteration 10/25 | Loss: 0.00117036
Iteration 11/25 | Loss: 0.00117036
Iteration 12/25 | Loss: 0.00117036
Iteration 13/25 | Loss: 0.00117036
Iteration 14/25 | Loss: 0.00117036
Iteration 15/25 | Loss: 0.00117036
Iteration 16/25 | Loss: 0.00117036
Iteration 17/25 | Loss: 0.00117036
Iteration 18/25 | Loss: 0.00117036
Iteration 19/25 | Loss: 0.00117036
Iteration 20/25 | Loss: 0.00117036
Iteration 21/25 | Loss: 0.00117036
Iteration 22/25 | Loss: 0.00117036
Iteration 23/25 | Loss: 0.00117036
Iteration 24/25 | Loss: 0.00117036
Iteration 25/25 | Loss: 0.00117036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.95895147
Iteration 2/25 | Loss: 0.00139061
Iteration 3/25 | Loss: 0.00139061
Iteration 4/25 | Loss: 0.00139061
Iteration 5/25 | Loss: 0.00139061
Iteration 6/25 | Loss: 0.00139060
Iteration 7/25 | Loss: 0.00139060
Iteration 8/25 | Loss: 0.00139060
Iteration 9/25 | Loss: 0.00139060
Iteration 10/25 | Loss: 0.00139060
Iteration 11/25 | Loss: 0.00139060
Iteration 12/25 | Loss: 0.00139060
Iteration 13/25 | Loss: 0.00139060
Iteration 14/25 | Loss: 0.00139060
Iteration 15/25 | Loss: 0.00139060
Iteration 16/25 | Loss: 0.00139060
Iteration 17/25 | Loss: 0.00139060
Iteration 18/25 | Loss: 0.00139060
Iteration 19/25 | Loss: 0.00139060
Iteration 20/25 | Loss: 0.00139060
Iteration 21/25 | Loss: 0.00139060
Iteration 22/25 | Loss: 0.00139060
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013906036037951708, 0.0013906036037951708, 0.0013906036037951708, 0.0013906036037951708, 0.0013906036037951708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013906036037951708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139060
Iteration 2/1000 | Loss: 0.00001710
Iteration 3/1000 | Loss: 0.00001252
Iteration 4/1000 | Loss: 0.00001118
Iteration 5/1000 | Loss: 0.00001052
Iteration 6/1000 | Loss: 0.00000996
Iteration 7/1000 | Loss: 0.00000968
Iteration 8/1000 | Loss: 0.00000947
Iteration 9/1000 | Loss: 0.00000914
Iteration 10/1000 | Loss: 0.00000900
Iteration 11/1000 | Loss: 0.00000898
Iteration 12/1000 | Loss: 0.00000888
Iteration 13/1000 | Loss: 0.00000886
Iteration 14/1000 | Loss: 0.00000886
Iteration 15/1000 | Loss: 0.00000885
Iteration 16/1000 | Loss: 0.00000880
Iteration 17/1000 | Loss: 0.00000877
Iteration 18/1000 | Loss: 0.00000875
Iteration 19/1000 | Loss: 0.00000874
Iteration 20/1000 | Loss: 0.00000874
Iteration 21/1000 | Loss: 0.00000873
Iteration 22/1000 | Loss: 0.00000873
Iteration 23/1000 | Loss: 0.00000870
Iteration 24/1000 | Loss: 0.00000866
Iteration 25/1000 | Loss: 0.00000865
Iteration 26/1000 | Loss: 0.00000863
Iteration 27/1000 | Loss: 0.00000862
Iteration 28/1000 | Loss: 0.00000862
Iteration 29/1000 | Loss: 0.00000861
Iteration 30/1000 | Loss: 0.00000861
Iteration 31/1000 | Loss: 0.00000861
Iteration 32/1000 | Loss: 0.00000857
Iteration 33/1000 | Loss: 0.00000856
Iteration 34/1000 | Loss: 0.00000855
Iteration 35/1000 | Loss: 0.00000854
Iteration 36/1000 | Loss: 0.00000853
Iteration 37/1000 | Loss: 0.00000853
Iteration 38/1000 | Loss: 0.00000852
Iteration 39/1000 | Loss: 0.00000852
Iteration 40/1000 | Loss: 0.00000852
Iteration 41/1000 | Loss: 0.00000852
Iteration 42/1000 | Loss: 0.00000852
Iteration 43/1000 | Loss: 0.00000851
Iteration 44/1000 | Loss: 0.00000851
Iteration 45/1000 | Loss: 0.00000851
Iteration 46/1000 | Loss: 0.00000849
Iteration 47/1000 | Loss: 0.00000845
Iteration 48/1000 | Loss: 0.00000845
Iteration 49/1000 | Loss: 0.00000844
Iteration 50/1000 | Loss: 0.00000843
Iteration 51/1000 | Loss: 0.00000842
Iteration 52/1000 | Loss: 0.00000842
Iteration 53/1000 | Loss: 0.00000841
Iteration 54/1000 | Loss: 0.00000841
Iteration 55/1000 | Loss: 0.00000839
Iteration 56/1000 | Loss: 0.00000837
Iteration 57/1000 | Loss: 0.00000836
Iteration 58/1000 | Loss: 0.00000836
Iteration 59/1000 | Loss: 0.00000835
Iteration 60/1000 | Loss: 0.00000835
Iteration 61/1000 | Loss: 0.00000835
Iteration 62/1000 | Loss: 0.00000835
Iteration 63/1000 | Loss: 0.00000835
Iteration 64/1000 | Loss: 0.00000835
Iteration 65/1000 | Loss: 0.00000835
Iteration 66/1000 | Loss: 0.00000834
Iteration 67/1000 | Loss: 0.00000834
Iteration 68/1000 | Loss: 0.00000834
Iteration 69/1000 | Loss: 0.00000834
Iteration 70/1000 | Loss: 0.00000834
Iteration 71/1000 | Loss: 0.00000834
Iteration 72/1000 | Loss: 0.00000834
Iteration 73/1000 | Loss: 0.00000834
Iteration 74/1000 | Loss: 0.00000834
Iteration 75/1000 | Loss: 0.00000834
Iteration 76/1000 | Loss: 0.00000834
Iteration 77/1000 | Loss: 0.00000834
Iteration 78/1000 | Loss: 0.00000834
Iteration 79/1000 | Loss: 0.00000834
Iteration 80/1000 | Loss: 0.00000833
Iteration 81/1000 | Loss: 0.00000833
Iteration 82/1000 | Loss: 0.00000833
Iteration 83/1000 | Loss: 0.00000833
Iteration 84/1000 | Loss: 0.00000832
Iteration 85/1000 | Loss: 0.00000830
Iteration 86/1000 | Loss: 0.00000830
Iteration 87/1000 | Loss: 0.00000830
Iteration 88/1000 | Loss: 0.00000830
Iteration 89/1000 | Loss: 0.00000830
Iteration 90/1000 | Loss: 0.00000830
Iteration 91/1000 | Loss: 0.00000830
Iteration 92/1000 | Loss: 0.00000829
Iteration 93/1000 | Loss: 0.00000829
Iteration 94/1000 | Loss: 0.00000829
Iteration 95/1000 | Loss: 0.00000829
Iteration 96/1000 | Loss: 0.00000828
Iteration 97/1000 | Loss: 0.00000828
Iteration 98/1000 | Loss: 0.00000827
Iteration 99/1000 | Loss: 0.00000827
Iteration 100/1000 | Loss: 0.00000827
Iteration 101/1000 | Loss: 0.00000827
Iteration 102/1000 | Loss: 0.00000827
Iteration 103/1000 | Loss: 0.00000827
Iteration 104/1000 | Loss: 0.00000827
Iteration 105/1000 | Loss: 0.00000827
Iteration 106/1000 | Loss: 0.00000827
Iteration 107/1000 | Loss: 0.00000827
Iteration 108/1000 | Loss: 0.00000826
Iteration 109/1000 | Loss: 0.00000826
Iteration 110/1000 | Loss: 0.00000826
Iteration 111/1000 | Loss: 0.00000826
Iteration 112/1000 | Loss: 0.00000826
Iteration 113/1000 | Loss: 0.00000826
Iteration 114/1000 | Loss: 0.00000826
Iteration 115/1000 | Loss: 0.00000826
Iteration 116/1000 | Loss: 0.00000826
Iteration 117/1000 | Loss: 0.00000826
Iteration 118/1000 | Loss: 0.00000826
Iteration 119/1000 | Loss: 0.00000825
Iteration 120/1000 | Loss: 0.00000825
Iteration 121/1000 | Loss: 0.00000825
Iteration 122/1000 | Loss: 0.00000825
Iteration 123/1000 | Loss: 0.00000825
Iteration 124/1000 | Loss: 0.00000825
Iteration 125/1000 | Loss: 0.00000825
Iteration 126/1000 | Loss: 0.00000825
Iteration 127/1000 | Loss: 0.00000825
Iteration 128/1000 | Loss: 0.00000825
Iteration 129/1000 | Loss: 0.00000824
Iteration 130/1000 | Loss: 0.00000824
Iteration 131/1000 | Loss: 0.00000824
Iteration 132/1000 | Loss: 0.00000824
Iteration 133/1000 | Loss: 0.00000824
Iteration 134/1000 | Loss: 0.00000824
Iteration 135/1000 | Loss: 0.00000824
Iteration 136/1000 | Loss: 0.00000824
Iteration 137/1000 | Loss: 0.00000824
Iteration 138/1000 | Loss: 0.00000823
Iteration 139/1000 | Loss: 0.00000823
Iteration 140/1000 | Loss: 0.00000823
Iteration 141/1000 | Loss: 0.00000822
Iteration 142/1000 | Loss: 0.00000822
Iteration 143/1000 | Loss: 0.00000822
Iteration 144/1000 | Loss: 0.00000822
Iteration 145/1000 | Loss: 0.00000822
Iteration 146/1000 | Loss: 0.00000822
Iteration 147/1000 | Loss: 0.00000822
Iteration 148/1000 | Loss: 0.00000822
Iteration 149/1000 | Loss: 0.00000822
Iteration 150/1000 | Loss: 0.00000822
Iteration 151/1000 | Loss: 0.00000822
Iteration 152/1000 | Loss: 0.00000822
Iteration 153/1000 | Loss: 0.00000822
Iteration 154/1000 | Loss: 0.00000822
Iteration 155/1000 | Loss: 0.00000822
Iteration 156/1000 | Loss: 0.00000821
Iteration 157/1000 | Loss: 0.00000821
Iteration 158/1000 | Loss: 0.00000821
Iteration 159/1000 | Loss: 0.00000821
Iteration 160/1000 | Loss: 0.00000821
Iteration 161/1000 | Loss: 0.00000821
Iteration 162/1000 | Loss: 0.00000821
Iteration 163/1000 | Loss: 0.00000821
Iteration 164/1000 | Loss: 0.00000821
Iteration 165/1000 | Loss: 0.00000821
Iteration 166/1000 | Loss: 0.00000821
Iteration 167/1000 | Loss: 0.00000821
Iteration 168/1000 | Loss: 0.00000821
Iteration 169/1000 | Loss: 0.00000821
Iteration 170/1000 | Loss: 0.00000821
Iteration 171/1000 | Loss: 0.00000820
Iteration 172/1000 | Loss: 0.00000820
Iteration 173/1000 | Loss: 0.00000820
Iteration 174/1000 | Loss: 0.00000819
Iteration 175/1000 | Loss: 0.00000819
Iteration 176/1000 | Loss: 0.00000819
Iteration 177/1000 | Loss: 0.00000819
Iteration 178/1000 | Loss: 0.00000819
Iteration 179/1000 | Loss: 0.00000819
Iteration 180/1000 | Loss: 0.00000819
Iteration 181/1000 | Loss: 0.00000819
Iteration 182/1000 | Loss: 0.00000819
Iteration 183/1000 | Loss: 0.00000819
Iteration 184/1000 | Loss: 0.00000819
Iteration 185/1000 | Loss: 0.00000818
Iteration 186/1000 | Loss: 0.00000818
Iteration 187/1000 | Loss: 0.00000818
Iteration 188/1000 | Loss: 0.00000818
Iteration 189/1000 | Loss: 0.00000818
Iteration 190/1000 | Loss: 0.00000818
Iteration 191/1000 | Loss: 0.00000818
Iteration 192/1000 | Loss: 0.00000818
Iteration 193/1000 | Loss: 0.00000818
Iteration 194/1000 | Loss: 0.00000818
Iteration 195/1000 | Loss: 0.00000818
Iteration 196/1000 | Loss: 0.00000817
Iteration 197/1000 | Loss: 0.00000817
Iteration 198/1000 | Loss: 0.00000817
Iteration 199/1000 | Loss: 0.00000817
Iteration 200/1000 | Loss: 0.00000817
Iteration 201/1000 | Loss: 0.00000817
Iteration 202/1000 | Loss: 0.00000817
Iteration 203/1000 | Loss: 0.00000817
Iteration 204/1000 | Loss: 0.00000817
Iteration 205/1000 | Loss: 0.00000817
Iteration 206/1000 | Loss: 0.00000817
Iteration 207/1000 | Loss: 0.00000816
Iteration 208/1000 | Loss: 0.00000816
Iteration 209/1000 | Loss: 0.00000816
Iteration 210/1000 | Loss: 0.00000816
Iteration 211/1000 | Loss: 0.00000815
Iteration 212/1000 | Loss: 0.00000815
Iteration 213/1000 | Loss: 0.00000815
Iteration 214/1000 | Loss: 0.00000815
Iteration 215/1000 | Loss: 0.00000815
Iteration 216/1000 | Loss: 0.00000814
Iteration 217/1000 | Loss: 0.00000814
Iteration 218/1000 | Loss: 0.00000814
Iteration 219/1000 | Loss: 0.00000814
Iteration 220/1000 | Loss: 0.00000814
Iteration 221/1000 | Loss: 0.00000814
Iteration 222/1000 | Loss: 0.00000814
Iteration 223/1000 | Loss: 0.00000814
Iteration 224/1000 | Loss: 0.00000814
Iteration 225/1000 | Loss: 0.00000814
Iteration 226/1000 | Loss: 0.00000814
Iteration 227/1000 | Loss: 0.00000813
Iteration 228/1000 | Loss: 0.00000813
Iteration 229/1000 | Loss: 0.00000813
Iteration 230/1000 | Loss: 0.00000813
Iteration 231/1000 | Loss: 0.00000813
Iteration 232/1000 | Loss: 0.00000813
Iteration 233/1000 | Loss: 0.00000813
Iteration 234/1000 | Loss: 0.00000813
Iteration 235/1000 | Loss: 0.00000813
Iteration 236/1000 | Loss: 0.00000813
Iteration 237/1000 | Loss: 0.00000813
Iteration 238/1000 | Loss: 0.00000813
Iteration 239/1000 | Loss: 0.00000813
Iteration 240/1000 | Loss: 0.00000813
Iteration 241/1000 | Loss: 0.00000813
Iteration 242/1000 | Loss: 0.00000813
Iteration 243/1000 | Loss: 0.00000813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [8.131238246278372e-06, 8.131238246278372e-06, 8.131238246278372e-06, 8.131238246278372e-06, 8.131238246278372e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.131238246278372e-06

Optimization complete. Final v2v error: 2.46608304977417 mm

Highest mean error: 2.5875163078308105 mm for frame 104

Lowest mean error: 2.3922622203826904 mm for frame 16

Saving results

Total time: 43.2016441822052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00589779
Iteration 2/25 | Loss: 0.00157062
Iteration 3/25 | Loss: 0.00131924
Iteration 4/25 | Loss: 0.00129298
Iteration 5/25 | Loss: 0.00128999
Iteration 6/25 | Loss: 0.00128919
Iteration 7/25 | Loss: 0.00128919
Iteration 8/25 | Loss: 0.00128919
Iteration 9/25 | Loss: 0.00128919
Iteration 10/25 | Loss: 0.00128919
Iteration 11/25 | Loss: 0.00128919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012891897931694984, 0.0012891897931694984, 0.0012891897931694984, 0.0012891897931694984, 0.0012891897931694984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012891897931694984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12720931
Iteration 2/25 | Loss: 0.00120326
Iteration 3/25 | Loss: 0.00120326
Iteration 4/25 | Loss: 0.00120326
Iteration 5/25 | Loss: 0.00120326
Iteration 6/25 | Loss: 0.00120326
Iteration 7/25 | Loss: 0.00120326
Iteration 8/25 | Loss: 0.00120326
Iteration 9/25 | Loss: 0.00120326
Iteration 10/25 | Loss: 0.00120326
Iteration 11/25 | Loss: 0.00120326
Iteration 12/25 | Loss: 0.00120326
Iteration 13/25 | Loss: 0.00120326
Iteration 14/25 | Loss: 0.00120326
Iteration 15/25 | Loss: 0.00120326
Iteration 16/25 | Loss: 0.00120326
Iteration 17/25 | Loss: 0.00120326
Iteration 18/25 | Loss: 0.00120326
Iteration 19/25 | Loss: 0.00120326
Iteration 20/25 | Loss: 0.00120326
Iteration 21/25 | Loss: 0.00120326
Iteration 22/25 | Loss: 0.00120326
Iteration 23/25 | Loss: 0.00120326
Iteration 24/25 | Loss: 0.00120326
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012032550293952227, 0.0012032550293952227, 0.0012032550293952227, 0.0012032550293952227, 0.0012032550293952227]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012032550293952227

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120326
Iteration 2/1000 | Loss: 0.00004947
Iteration 3/1000 | Loss: 0.00002984
Iteration 4/1000 | Loss: 0.00002259
Iteration 5/1000 | Loss: 0.00002115
Iteration 6/1000 | Loss: 0.00002037
Iteration 7/1000 | Loss: 0.00001974
Iteration 8/1000 | Loss: 0.00001938
Iteration 9/1000 | Loss: 0.00001895
Iteration 10/1000 | Loss: 0.00001866
Iteration 11/1000 | Loss: 0.00001840
Iteration 12/1000 | Loss: 0.00001824
Iteration 13/1000 | Loss: 0.00001823
Iteration 14/1000 | Loss: 0.00001808
Iteration 15/1000 | Loss: 0.00001804
Iteration 16/1000 | Loss: 0.00001802
Iteration 17/1000 | Loss: 0.00001798
Iteration 18/1000 | Loss: 0.00001795
Iteration 19/1000 | Loss: 0.00001783
Iteration 20/1000 | Loss: 0.00001778
Iteration 21/1000 | Loss: 0.00001778
Iteration 22/1000 | Loss: 0.00001777
Iteration 23/1000 | Loss: 0.00001776
Iteration 24/1000 | Loss: 0.00001775
Iteration 25/1000 | Loss: 0.00001773
Iteration 26/1000 | Loss: 0.00001768
Iteration 27/1000 | Loss: 0.00001767
Iteration 28/1000 | Loss: 0.00001767
Iteration 29/1000 | Loss: 0.00001767
Iteration 30/1000 | Loss: 0.00001766
Iteration 31/1000 | Loss: 0.00001764
Iteration 32/1000 | Loss: 0.00001763
Iteration 33/1000 | Loss: 0.00001762
Iteration 34/1000 | Loss: 0.00001758
Iteration 35/1000 | Loss: 0.00001758
Iteration 36/1000 | Loss: 0.00001757
Iteration 37/1000 | Loss: 0.00001754
Iteration 38/1000 | Loss: 0.00001752
Iteration 39/1000 | Loss: 0.00001752
Iteration 40/1000 | Loss: 0.00001752
Iteration 41/1000 | Loss: 0.00001751
Iteration 42/1000 | Loss: 0.00001751
Iteration 43/1000 | Loss: 0.00001751
Iteration 44/1000 | Loss: 0.00001750
Iteration 45/1000 | Loss: 0.00001750
Iteration 46/1000 | Loss: 0.00001750
Iteration 47/1000 | Loss: 0.00001750
Iteration 48/1000 | Loss: 0.00001750
Iteration 49/1000 | Loss: 0.00001750
Iteration 50/1000 | Loss: 0.00001749
Iteration 51/1000 | Loss: 0.00001749
Iteration 52/1000 | Loss: 0.00001749
Iteration 53/1000 | Loss: 0.00001749
Iteration 54/1000 | Loss: 0.00001749
Iteration 55/1000 | Loss: 0.00001749
Iteration 56/1000 | Loss: 0.00001749
Iteration 57/1000 | Loss: 0.00001749
Iteration 58/1000 | Loss: 0.00001749
Iteration 59/1000 | Loss: 0.00001749
Iteration 60/1000 | Loss: 0.00001749
Iteration 61/1000 | Loss: 0.00001748
Iteration 62/1000 | Loss: 0.00001748
Iteration 63/1000 | Loss: 0.00001748
Iteration 64/1000 | Loss: 0.00001748
Iteration 65/1000 | Loss: 0.00001748
Iteration 66/1000 | Loss: 0.00001747
Iteration 67/1000 | Loss: 0.00001747
Iteration 68/1000 | Loss: 0.00001747
Iteration 69/1000 | Loss: 0.00001746
Iteration 70/1000 | Loss: 0.00001746
Iteration 71/1000 | Loss: 0.00001746
Iteration 72/1000 | Loss: 0.00001746
Iteration 73/1000 | Loss: 0.00001746
Iteration 74/1000 | Loss: 0.00001746
Iteration 75/1000 | Loss: 0.00001745
Iteration 76/1000 | Loss: 0.00001745
Iteration 77/1000 | Loss: 0.00001745
Iteration 78/1000 | Loss: 0.00001745
Iteration 79/1000 | Loss: 0.00001745
Iteration 80/1000 | Loss: 0.00001744
Iteration 81/1000 | Loss: 0.00001744
Iteration 82/1000 | Loss: 0.00001744
Iteration 83/1000 | Loss: 0.00001743
Iteration 84/1000 | Loss: 0.00001743
Iteration 85/1000 | Loss: 0.00001743
Iteration 86/1000 | Loss: 0.00001742
Iteration 87/1000 | Loss: 0.00001742
Iteration 88/1000 | Loss: 0.00001742
Iteration 89/1000 | Loss: 0.00001742
Iteration 90/1000 | Loss: 0.00001742
Iteration 91/1000 | Loss: 0.00001742
Iteration 92/1000 | Loss: 0.00001741
Iteration 93/1000 | Loss: 0.00001741
Iteration 94/1000 | Loss: 0.00001741
Iteration 95/1000 | Loss: 0.00001740
Iteration 96/1000 | Loss: 0.00001740
Iteration 97/1000 | Loss: 0.00001739
Iteration 98/1000 | Loss: 0.00001739
Iteration 99/1000 | Loss: 0.00001739
Iteration 100/1000 | Loss: 0.00001739
Iteration 101/1000 | Loss: 0.00001739
Iteration 102/1000 | Loss: 0.00001738
Iteration 103/1000 | Loss: 0.00001737
Iteration 104/1000 | Loss: 0.00001737
Iteration 105/1000 | Loss: 0.00001737
Iteration 106/1000 | Loss: 0.00001737
Iteration 107/1000 | Loss: 0.00001737
Iteration 108/1000 | Loss: 0.00001737
Iteration 109/1000 | Loss: 0.00001737
Iteration 110/1000 | Loss: 0.00001736
Iteration 111/1000 | Loss: 0.00001736
Iteration 112/1000 | Loss: 0.00001736
Iteration 113/1000 | Loss: 0.00001735
Iteration 114/1000 | Loss: 0.00001735
Iteration 115/1000 | Loss: 0.00001735
Iteration 116/1000 | Loss: 0.00001735
Iteration 117/1000 | Loss: 0.00001735
Iteration 118/1000 | Loss: 0.00001735
Iteration 119/1000 | Loss: 0.00001734
Iteration 120/1000 | Loss: 0.00001734
Iteration 121/1000 | Loss: 0.00001734
Iteration 122/1000 | Loss: 0.00001734
Iteration 123/1000 | Loss: 0.00001734
Iteration 124/1000 | Loss: 0.00001733
Iteration 125/1000 | Loss: 0.00001733
Iteration 126/1000 | Loss: 0.00001733
Iteration 127/1000 | Loss: 0.00001733
Iteration 128/1000 | Loss: 0.00001733
Iteration 129/1000 | Loss: 0.00001733
Iteration 130/1000 | Loss: 0.00001733
Iteration 131/1000 | Loss: 0.00001732
Iteration 132/1000 | Loss: 0.00001732
Iteration 133/1000 | Loss: 0.00001732
Iteration 134/1000 | Loss: 0.00001732
Iteration 135/1000 | Loss: 0.00001732
Iteration 136/1000 | Loss: 0.00001732
Iteration 137/1000 | Loss: 0.00001732
Iteration 138/1000 | Loss: 0.00001732
Iteration 139/1000 | Loss: 0.00001732
Iteration 140/1000 | Loss: 0.00001732
Iteration 141/1000 | Loss: 0.00001732
Iteration 142/1000 | Loss: 0.00001732
Iteration 143/1000 | Loss: 0.00001732
Iteration 144/1000 | Loss: 0.00001732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.7317051970167086e-05, 1.7317051970167086e-05, 1.7317051970167086e-05, 1.7317051970167086e-05, 1.7317051970167086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7317051970167086e-05

Optimization complete. Final v2v error: 3.4324028491973877 mm

Highest mean error: 5.196383953094482 mm for frame 58

Lowest mean error: 2.9845457077026367 mm for frame 129

Saving results

Total time: 41.79799938201904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450761
Iteration 2/25 | Loss: 0.00131047
Iteration 3/25 | Loss: 0.00122193
Iteration 4/25 | Loss: 0.00120952
Iteration 5/25 | Loss: 0.00120576
Iteration 6/25 | Loss: 0.00120518
Iteration 7/25 | Loss: 0.00120518
Iteration 8/25 | Loss: 0.00120518
Iteration 9/25 | Loss: 0.00120518
Iteration 10/25 | Loss: 0.00120518
Iteration 11/25 | Loss: 0.00120518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012051803059875965, 0.0012051803059875965, 0.0012051803059875965, 0.0012051803059875965, 0.0012051803059875965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012051803059875965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35838199
Iteration 2/25 | Loss: 0.00157086
Iteration 3/25 | Loss: 0.00157085
Iteration 4/25 | Loss: 0.00157085
Iteration 5/25 | Loss: 0.00157085
Iteration 6/25 | Loss: 0.00157085
Iteration 7/25 | Loss: 0.00157085
Iteration 8/25 | Loss: 0.00157085
Iteration 9/25 | Loss: 0.00157085
Iteration 10/25 | Loss: 0.00157085
Iteration 11/25 | Loss: 0.00157085
Iteration 12/25 | Loss: 0.00157085
Iteration 13/25 | Loss: 0.00157085
Iteration 14/25 | Loss: 0.00157085
Iteration 15/25 | Loss: 0.00157085
Iteration 16/25 | Loss: 0.00157085
Iteration 17/25 | Loss: 0.00157085
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015708490973338485, 0.0015708490973338485, 0.0015708490973338485, 0.0015708490973338485, 0.0015708490973338485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015708490973338485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157085
Iteration 2/1000 | Loss: 0.00002329
Iteration 3/1000 | Loss: 0.00001686
Iteration 4/1000 | Loss: 0.00001552
Iteration 5/1000 | Loss: 0.00001478
Iteration 6/1000 | Loss: 0.00001423
Iteration 7/1000 | Loss: 0.00001393
Iteration 8/1000 | Loss: 0.00001352
Iteration 9/1000 | Loss: 0.00001320
Iteration 10/1000 | Loss: 0.00001301
Iteration 11/1000 | Loss: 0.00001283
Iteration 12/1000 | Loss: 0.00001276
Iteration 13/1000 | Loss: 0.00001263
Iteration 14/1000 | Loss: 0.00001263
Iteration 15/1000 | Loss: 0.00001254
Iteration 16/1000 | Loss: 0.00001249
Iteration 17/1000 | Loss: 0.00001248
Iteration 18/1000 | Loss: 0.00001248
Iteration 19/1000 | Loss: 0.00001245
Iteration 20/1000 | Loss: 0.00001244
Iteration 21/1000 | Loss: 0.00001243
Iteration 22/1000 | Loss: 0.00001239
Iteration 23/1000 | Loss: 0.00001238
Iteration 24/1000 | Loss: 0.00001237
Iteration 25/1000 | Loss: 0.00001236
Iteration 26/1000 | Loss: 0.00001235
Iteration 27/1000 | Loss: 0.00001222
Iteration 28/1000 | Loss: 0.00001221
Iteration 29/1000 | Loss: 0.00001220
Iteration 30/1000 | Loss: 0.00001220
Iteration 31/1000 | Loss: 0.00001218
Iteration 32/1000 | Loss: 0.00001216
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001212
Iteration 35/1000 | Loss: 0.00001211
Iteration 36/1000 | Loss: 0.00001210
Iteration 37/1000 | Loss: 0.00001209
Iteration 38/1000 | Loss: 0.00001209
Iteration 39/1000 | Loss: 0.00001209
Iteration 40/1000 | Loss: 0.00001208
Iteration 41/1000 | Loss: 0.00001208
Iteration 42/1000 | Loss: 0.00001207
Iteration 43/1000 | Loss: 0.00001207
Iteration 44/1000 | Loss: 0.00001206
Iteration 45/1000 | Loss: 0.00001206
Iteration 46/1000 | Loss: 0.00001206
Iteration 47/1000 | Loss: 0.00001206
Iteration 48/1000 | Loss: 0.00001205
Iteration 49/1000 | Loss: 0.00001205
Iteration 50/1000 | Loss: 0.00001205
Iteration 51/1000 | Loss: 0.00001205
Iteration 52/1000 | Loss: 0.00001205
Iteration 53/1000 | Loss: 0.00001204
Iteration 54/1000 | Loss: 0.00001204
Iteration 55/1000 | Loss: 0.00001204
Iteration 56/1000 | Loss: 0.00001204
Iteration 57/1000 | Loss: 0.00001204
Iteration 58/1000 | Loss: 0.00001204
Iteration 59/1000 | Loss: 0.00001203
Iteration 60/1000 | Loss: 0.00001203
Iteration 61/1000 | Loss: 0.00001203
Iteration 62/1000 | Loss: 0.00001203
Iteration 63/1000 | Loss: 0.00001203
Iteration 64/1000 | Loss: 0.00001203
Iteration 65/1000 | Loss: 0.00001202
Iteration 66/1000 | Loss: 0.00001202
Iteration 67/1000 | Loss: 0.00001202
Iteration 68/1000 | Loss: 0.00001202
Iteration 69/1000 | Loss: 0.00001202
Iteration 70/1000 | Loss: 0.00001202
Iteration 71/1000 | Loss: 0.00001201
Iteration 72/1000 | Loss: 0.00001201
Iteration 73/1000 | Loss: 0.00001201
Iteration 74/1000 | Loss: 0.00001201
Iteration 75/1000 | Loss: 0.00001201
Iteration 76/1000 | Loss: 0.00001201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.2014953426842112e-05, 1.2014953426842112e-05, 1.2014953426842112e-05, 1.2014953426842112e-05, 1.2014953426842112e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2014953426842112e-05

Optimization complete. Final v2v error: 2.944192409515381 mm

Highest mean error: 3.8791956901550293 mm for frame 158

Lowest mean error: 2.5305659770965576 mm for frame 6

Saving results

Total time: 40.31357026100159
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00360652
Iteration 2/25 | Loss: 0.00129706
Iteration 3/25 | Loss: 0.00121046
Iteration 4/25 | Loss: 0.00119675
Iteration 5/25 | Loss: 0.00119181
Iteration 6/25 | Loss: 0.00119099
Iteration 7/25 | Loss: 0.00119099
Iteration 8/25 | Loss: 0.00119099
Iteration 9/25 | Loss: 0.00119099
Iteration 10/25 | Loss: 0.00119099
Iteration 11/25 | Loss: 0.00119099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011909870663657784, 0.0011909870663657784, 0.0011909870663657784, 0.0011909870663657784, 0.0011909870663657784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011909870663657784

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29360509
Iteration 2/25 | Loss: 0.00199155
Iteration 3/25 | Loss: 0.00199155
Iteration 4/25 | Loss: 0.00199155
Iteration 5/25 | Loss: 0.00199155
Iteration 6/25 | Loss: 0.00199155
Iteration 7/25 | Loss: 0.00199155
Iteration 8/25 | Loss: 0.00199155
Iteration 9/25 | Loss: 0.00199155
Iteration 10/25 | Loss: 0.00199155
Iteration 11/25 | Loss: 0.00199155
Iteration 12/25 | Loss: 0.00199155
Iteration 13/25 | Loss: 0.00199155
Iteration 14/25 | Loss: 0.00199155
Iteration 15/25 | Loss: 0.00199155
Iteration 16/25 | Loss: 0.00199155
Iteration 17/25 | Loss: 0.00199155
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001991549739614129, 0.001991549739614129, 0.001991549739614129, 0.001991549739614129, 0.001991549739614129]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001991549739614129

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199155
Iteration 2/1000 | Loss: 0.00005080
Iteration 3/1000 | Loss: 0.00003430
Iteration 4/1000 | Loss: 0.00002364
Iteration 5/1000 | Loss: 0.00002124
Iteration 6/1000 | Loss: 0.00001964
Iteration 7/1000 | Loss: 0.00001815
Iteration 8/1000 | Loss: 0.00001740
Iteration 9/1000 | Loss: 0.00001685
Iteration 10/1000 | Loss: 0.00001649
Iteration 11/1000 | Loss: 0.00001617
Iteration 12/1000 | Loss: 0.00001591
Iteration 13/1000 | Loss: 0.00001575
Iteration 14/1000 | Loss: 0.00001565
Iteration 15/1000 | Loss: 0.00001561
Iteration 16/1000 | Loss: 0.00001560
Iteration 17/1000 | Loss: 0.00001560
Iteration 18/1000 | Loss: 0.00001554
Iteration 19/1000 | Loss: 0.00001554
Iteration 20/1000 | Loss: 0.00001552
Iteration 21/1000 | Loss: 0.00001551
Iteration 22/1000 | Loss: 0.00001548
Iteration 23/1000 | Loss: 0.00001548
Iteration 24/1000 | Loss: 0.00001543
Iteration 25/1000 | Loss: 0.00001539
Iteration 26/1000 | Loss: 0.00001535
Iteration 27/1000 | Loss: 0.00001533
Iteration 28/1000 | Loss: 0.00001530
Iteration 29/1000 | Loss: 0.00001530
Iteration 30/1000 | Loss: 0.00001530
Iteration 31/1000 | Loss: 0.00001528
Iteration 32/1000 | Loss: 0.00001528
Iteration 33/1000 | Loss: 0.00001528
Iteration 34/1000 | Loss: 0.00001526
Iteration 35/1000 | Loss: 0.00001526
Iteration 36/1000 | Loss: 0.00001525
Iteration 37/1000 | Loss: 0.00001525
Iteration 38/1000 | Loss: 0.00001525
Iteration 39/1000 | Loss: 0.00001525
Iteration 40/1000 | Loss: 0.00001525
Iteration 41/1000 | Loss: 0.00001525
Iteration 42/1000 | Loss: 0.00001525
Iteration 43/1000 | Loss: 0.00001525
Iteration 44/1000 | Loss: 0.00001525
Iteration 45/1000 | Loss: 0.00001524
Iteration 46/1000 | Loss: 0.00001523
Iteration 47/1000 | Loss: 0.00001523
Iteration 48/1000 | Loss: 0.00001523
Iteration 49/1000 | Loss: 0.00001522
Iteration 50/1000 | Loss: 0.00001522
Iteration 51/1000 | Loss: 0.00001522
Iteration 52/1000 | Loss: 0.00001521
Iteration 53/1000 | Loss: 0.00001521
Iteration 54/1000 | Loss: 0.00001521
Iteration 55/1000 | Loss: 0.00001520
Iteration 56/1000 | Loss: 0.00001520
Iteration 57/1000 | Loss: 0.00001520
Iteration 58/1000 | Loss: 0.00001519
Iteration 59/1000 | Loss: 0.00001519
Iteration 60/1000 | Loss: 0.00001518
Iteration 61/1000 | Loss: 0.00001518
Iteration 62/1000 | Loss: 0.00001518
Iteration 63/1000 | Loss: 0.00001518
Iteration 64/1000 | Loss: 0.00001518
Iteration 65/1000 | Loss: 0.00001517
Iteration 66/1000 | Loss: 0.00001517
Iteration 67/1000 | Loss: 0.00001516
Iteration 68/1000 | Loss: 0.00001516
Iteration 69/1000 | Loss: 0.00001516
Iteration 70/1000 | Loss: 0.00001515
Iteration 71/1000 | Loss: 0.00001515
Iteration 72/1000 | Loss: 0.00001515
Iteration 73/1000 | Loss: 0.00001514
Iteration 74/1000 | Loss: 0.00001514
Iteration 75/1000 | Loss: 0.00001513
Iteration 76/1000 | Loss: 0.00001512
Iteration 77/1000 | Loss: 0.00001512
Iteration 78/1000 | Loss: 0.00001512
Iteration 79/1000 | Loss: 0.00001512
Iteration 80/1000 | Loss: 0.00001512
Iteration 81/1000 | Loss: 0.00001511
Iteration 82/1000 | Loss: 0.00001511
Iteration 83/1000 | Loss: 0.00001511
Iteration 84/1000 | Loss: 0.00001511
Iteration 85/1000 | Loss: 0.00001511
Iteration 86/1000 | Loss: 0.00001511
Iteration 87/1000 | Loss: 0.00001511
Iteration 88/1000 | Loss: 0.00001511
Iteration 89/1000 | Loss: 0.00001510
Iteration 90/1000 | Loss: 0.00001509
Iteration 91/1000 | Loss: 0.00001509
Iteration 92/1000 | Loss: 0.00001509
Iteration 93/1000 | Loss: 0.00001508
Iteration 94/1000 | Loss: 0.00001508
Iteration 95/1000 | Loss: 0.00001508
Iteration 96/1000 | Loss: 0.00001508
Iteration 97/1000 | Loss: 0.00001508
Iteration 98/1000 | Loss: 0.00001507
Iteration 99/1000 | Loss: 0.00001507
Iteration 100/1000 | Loss: 0.00001507
Iteration 101/1000 | Loss: 0.00001507
Iteration 102/1000 | Loss: 0.00001506
Iteration 103/1000 | Loss: 0.00001506
Iteration 104/1000 | Loss: 0.00001506
Iteration 105/1000 | Loss: 0.00001506
Iteration 106/1000 | Loss: 0.00001505
Iteration 107/1000 | Loss: 0.00001505
Iteration 108/1000 | Loss: 0.00001505
Iteration 109/1000 | Loss: 0.00001505
Iteration 110/1000 | Loss: 0.00001505
Iteration 111/1000 | Loss: 0.00001505
Iteration 112/1000 | Loss: 0.00001505
Iteration 113/1000 | Loss: 0.00001505
Iteration 114/1000 | Loss: 0.00001504
Iteration 115/1000 | Loss: 0.00001504
Iteration 116/1000 | Loss: 0.00001504
Iteration 117/1000 | Loss: 0.00001503
Iteration 118/1000 | Loss: 0.00001503
Iteration 119/1000 | Loss: 0.00001503
Iteration 120/1000 | Loss: 0.00001503
Iteration 121/1000 | Loss: 0.00001503
Iteration 122/1000 | Loss: 0.00001503
Iteration 123/1000 | Loss: 0.00001503
Iteration 124/1000 | Loss: 0.00001502
Iteration 125/1000 | Loss: 0.00001502
Iteration 126/1000 | Loss: 0.00001502
Iteration 127/1000 | Loss: 0.00001502
Iteration 128/1000 | Loss: 0.00001502
Iteration 129/1000 | Loss: 0.00001502
Iteration 130/1000 | Loss: 0.00001502
Iteration 131/1000 | Loss: 0.00001502
Iteration 132/1000 | Loss: 0.00001502
Iteration 133/1000 | Loss: 0.00001501
Iteration 134/1000 | Loss: 0.00001501
Iteration 135/1000 | Loss: 0.00001501
Iteration 136/1000 | Loss: 0.00001501
Iteration 137/1000 | Loss: 0.00001501
Iteration 138/1000 | Loss: 0.00001501
Iteration 139/1000 | Loss: 0.00001501
Iteration 140/1000 | Loss: 0.00001501
Iteration 141/1000 | Loss: 0.00001501
Iteration 142/1000 | Loss: 0.00001500
Iteration 143/1000 | Loss: 0.00001500
Iteration 144/1000 | Loss: 0.00001500
Iteration 145/1000 | Loss: 0.00001500
Iteration 146/1000 | Loss: 0.00001500
Iteration 147/1000 | Loss: 0.00001500
Iteration 148/1000 | Loss: 0.00001500
Iteration 149/1000 | Loss: 0.00001500
Iteration 150/1000 | Loss: 0.00001500
Iteration 151/1000 | Loss: 0.00001500
Iteration 152/1000 | Loss: 0.00001500
Iteration 153/1000 | Loss: 0.00001499
Iteration 154/1000 | Loss: 0.00001499
Iteration 155/1000 | Loss: 0.00001499
Iteration 156/1000 | Loss: 0.00001499
Iteration 157/1000 | Loss: 0.00001499
Iteration 158/1000 | Loss: 0.00001499
Iteration 159/1000 | Loss: 0.00001499
Iteration 160/1000 | Loss: 0.00001499
Iteration 161/1000 | Loss: 0.00001499
Iteration 162/1000 | Loss: 0.00001499
Iteration 163/1000 | Loss: 0.00001499
Iteration 164/1000 | Loss: 0.00001498
Iteration 165/1000 | Loss: 0.00001498
Iteration 166/1000 | Loss: 0.00001498
Iteration 167/1000 | Loss: 0.00001498
Iteration 168/1000 | Loss: 0.00001498
Iteration 169/1000 | Loss: 0.00001498
Iteration 170/1000 | Loss: 0.00001498
Iteration 171/1000 | Loss: 0.00001498
Iteration 172/1000 | Loss: 0.00001498
Iteration 173/1000 | Loss: 0.00001498
Iteration 174/1000 | Loss: 0.00001497
Iteration 175/1000 | Loss: 0.00001497
Iteration 176/1000 | Loss: 0.00001497
Iteration 177/1000 | Loss: 0.00001497
Iteration 178/1000 | Loss: 0.00001497
Iteration 179/1000 | Loss: 0.00001497
Iteration 180/1000 | Loss: 0.00001497
Iteration 181/1000 | Loss: 0.00001497
Iteration 182/1000 | Loss: 0.00001496
Iteration 183/1000 | Loss: 0.00001496
Iteration 184/1000 | Loss: 0.00001496
Iteration 185/1000 | Loss: 0.00001496
Iteration 186/1000 | Loss: 0.00001496
Iteration 187/1000 | Loss: 0.00001496
Iteration 188/1000 | Loss: 0.00001496
Iteration 189/1000 | Loss: 0.00001496
Iteration 190/1000 | Loss: 0.00001496
Iteration 191/1000 | Loss: 0.00001496
Iteration 192/1000 | Loss: 0.00001496
Iteration 193/1000 | Loss: 0.00001496
Iteration 194/1000 | Loss: 0.00001495
Iteration 195/1000 | Loss: 0.00001495
Iteration 196/1000 | Loss: 0.00001495
Iteration 197/1000 | Loss: 0.00001494
Iteration 198/1000 | Loss: 0.00001494
Iteration 199/1000 | Loss: 0.00001494
Iteration 200/1000 | Loss: 0.00001494
Iteration 201/1000 | Loss: 0.00001494
Iteration 202/1000 | Loss: 0.00001494
Iteration 203/1000 | Loss: 0.00001494
Iteration 204/1000 | Loss: 0.00001494
Iteration 205/1000 | Loss: 0.00001494
Iteration 206/1000 | Loss: 0.00001494
Iteration 207/1000 | Loss: 0.00001494
Iteration 208/1000 | Loss: 0.00001494
Iteration 209/1000 | Loss: 0.00001494
Iteration 210/1000 | Loss: 0.00001494
Iteration 211/1000 | Loss: 0.00001493
Iteration 212/1000 | Loss: 0.00001493
Iteration 213/1000 | Loss: 0.00001493
Iteration 214/1000 | Loss: 0.00001493
Iteration 215/1000 | Loss: 0.00001493
Iteration 216/1000 | Loss: 0.00001493
Iteration 217/1000 | Loss: 0.00001493
Iteration 218/1000 | Loss: 0.00001493
Iteration 219/1000 | Loss: 0.00001493
Iteration 220/1000 | Loss: 0.00001492
Iteration 221/1000 | Loss: 0.00001492
Iteration 222/1000 | Loss: 0.00001492
Iteration 223/1000 | Loss: 0.00001492
Iteration 224/1000 | Loss: 0.00001492
Iteration 225/1000 | Loss: 0.00001492
Iteration 226/1000 | Loss: 0.00001492
Iteration 227/1000 | Loss: 0.00001492
Iteration 228/1000 | Loss: 0.00001491
Iteration 229/1000 | Loss: 0.00001491
Iteration 230/1000 | Loss: 0.00001491
Iteration 231/1000 | Loss: 0.00001491
Iteration 232/1000 | Loss: 0.00001491
Iteration 233/1000 | Loss: 0.00001491
Iteration 234/1000 | Loss: 0.00001490
Iteration 235/1000 | Loss: 0.00001490
Iteration 236/1000 | Loss: 0.00001490
Iteration 237/1000 | Loss: 0.00001490
Iteration 238/1000 | Loss: 0.00001490
Iteration 239/1000 | Loss: 0.00001490
Iteration 240/1000 | Loss: 0.00001490
Iteration 241/1000 | Loss: 0.00001490
Iteration 242/1000 | Loss: 0.00001490
Iteration 243/1000 | Loss: 0.00001490
Iteration 244/1000 | Loss: 0.00001489
Iteration 245/1000 | Loss: 0.00001489
Iteration 246/1000 | Loss: 0.00001489
Iteration 247/1000 | Loss: 0.00001489
Iteration 248/1000 | Loss: 0.00001489
Iteration 249/1000 | Loss: 0.00001489
Iteration 250/1000 | Loss: 0.00001489
Iteration 251/1000 | Loss: 0.00001489
Iteration 252/1000 | Loss: 0.00001489
Iteration 253/1000 | Loss: 0.00001489
Iteration 254/1000 | Loss: 0.00001489
Iteration 255/1000 | Loss: 0.00001489
Iteration 256/1000 | Loss: 0.00001489
Iteration 257/1000 | Loss: 0.00001488
Iteration 258/1000 | Loss: 0.00001488
Iteration 259/1000 | Loss: 0.00001488
Iteration 260/1000 | Loss: 0.00001488
Iteration 261/1000 | Loss: 0.00001488
Iteration 262/1000 | Loss: 0.00001488
Iteration 263/1000 | Loss: 0.00001488
Iteration 264/1000 | Loss: 0.00001488
Iteration 265/1000 | Loss: 0.00001488
Iteration 266/1000 | Loss: 0.00001488
Iteration 267/1000 | Loss: 0.00001488
Iteration 268/1000 | Loss: 0.00001488
Iteration 269/1000 | Loss: 0.00001488
Iteration 270/1000 | Loss: 0.00001487
Iteration 271/1000 | Loss: 0.00001487
Iteration 272/1000 | Loss: 0.00001487
Iteration 273/1000 | Loss: 0.00001487
Iteration 274/1000 | Loss: 0.00001487
Iteration 275/1000 | Loss: 0.00001487
Iteration 276/1000 | Loss: 0.00001487
Iteration 277/1000 | Loss: 0.00001487
Iteration 278/1000 | Loss: 0.00001487
Iteration 279/1000 | Loss: 0.00001487
Iteration 280/1000 | Loss: 0.00001487
Iteration 281/1000 | Loss: 0.00001487
Iteration 282/1000 | Loss: 0.00001487
Iteration 283/1000 | Loss: 0.00001487
Iteration 284/1000 | Loss: 0.00001487
Iteration 285/1000 | Loss: 0.00001487
Iteration 286/1000 | Loss: 0.00001487
Iteration 287/1000 | Loss: 0.00001487
Iteration 288/1000 | Loss: 0.00001487
Iteration 289/1000 | Loss: 0.00001486
Iteration 290/1000 | Loss: 0.00001486
Iteration 291/1000 | Loss: 0.00001486
Iteration 292/1000 | Loss: 0.00001486
Iteration 293/1000 | Loss: 0.00001486
Iteration 294/1000 | Loss: 0.00001486
Iteration 295/1000 | Loss: 0.00001486
Iteration 296/1000 | Loss: 0.00001486
Iteration 297/1000 | Loss: 0.00001486
Iteration 298/1000 | Loss: 0.00001486
Iteration 299/1000 | Loss: 0.00001486
Iteration 300/1000 | Loss: 0.00001486
Iteration 301/1000 | Loss: 0.00001486
Iteration 302/1000 | Loss: 0.00001486
Iteration 303/1000 | Loss: 0.00001486
Iteration 304/1000 | Loss: 0.00001486
Iteration 305/1000 | Loss: 0.00001486
Iteration 306/1000 | Loss: 0.00001486
Iteration 307/1000 | Loss: 0.00001486
Iteration 308/1000 | Loss: 0.00001486
Iteration 309/1000 | Loss: 0.00001486
Iteration 310/1000 | Loss: 0.00001486
Iteration 311/1000 | Loss: 0.00001486
Iteration 312/1000 | Loss: 0.00001486
Iteration 313/1000 | Loss: 0.00001485
Iteration 314/1000 | Loss: 0.00001485
Iteration 315/1000 | Loss: 0.00001485
Iteration 316/1000 | Loss: 0.00001485
Iteration 317/1000 | Loss: 0.00001485
Iteration 318/1000 | Loss: 0.00001485
Iteration 319/1000 | Loss: 0.00001485
Iteration 320/1000 | Loss: 0.00001485
Iteration 321/1000 | Loss: 0.00001485
Iteration 322/1000 | Loss: 0.00001485
Iteration 323/1000 | Loss: 0.00001485
Iteration 324/1000 | Loss: 0.00001485
Iteration 325/1000 | Loss: 0.00001485
Iteration 326/1000 | Loss: 0.00001485
Iteration 327/1000 | Loss: 0.00001485
Iteration 328/1000 | Loss: 0.00001485
Iteration 329/1000 | Loss: 0.00001484
Iteration 330/1000 | Loss: 0.00001484
Iteration 331/1000 | Loss: 0.00001484
Iteration 332/1000 | Loss: 0.00001484
Iteration 333/1000 | Loss: 0.00001484
Iteration 334/1000 | Loss: 0.00001484
Iteration 335/1000 | Loss: 0.00001484
Iteration 336/1000 | Loss: 0.00001484
Iteration 337/1000 | Loss: 0.00001484
Iteration 338/1000 | Loss: 0.00001484
Iteration 339/1000 | Loss: 0.00001484
Iteration 340/1000 | Loss: 0.00001484
Iteration 341/1000 | Loss: 0.00001484
Iteration 342/1000 | Loss: 0.00001484
Iteration 343/1000 | Loss: 0.00001484
Iteration 344/1000 | Loss: 0.00001484
Iteration 345/1000 | Loss: 0.00001484
Iteration 346/1000 | Loss: 0.00001484
Iteration 347/1000 | Loss: 0.00001484
Iteration 348/1000 | Loss: 0.00001484
Iteration 349/1000 | Loss: 0.00001484
Iteration 350/1000 | Loss: 0.00001484
Iteration 351/1000 | Loss: 0.00001484
Iteration 352/1000 | Loss: 0.00001484
Iteration 353/1000 | Loss: 0.00001484
Iteration 354/1000 | Loss: 0.00001484
Iteration 355/1000 | Loss: 0.00001484
Iteration 356/1000 | Loss: 0.00001484
Iteration 357/1000 | Loss: 0.00001484
Iteration 358/1000 | Loss: 0.00001484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 358. Stopping optimization.
Last 5 losses: [1.4835509318800177e-05, 1.4835509318800177e-05, 1.4835509318800177e-05, 1.4835509318800177e-05, 1.4835509318800177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4835509318800177e-05

Optimization complete. Final v2v error: 3.2499806880950928 mm

Highest mean error: 4.745296955108643 mm for frame 151

Lowest mean error: 2.355628252029419 mm for frame 0

Saving results

Total time: 54.0957407951355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450474
Iteration 2/25 | Loss: 0.00141388
Iteration 3/25 | Loss: 0.00127102
Iteration 4/25 | Loss: 0.00124674
Iteration 5/25 | Loss: 0.00124261
Iteration 6/25 | Loss: 0.00124261
Iteration 7/25 | Loss: 0.00124261
Iteration 8/25 | Loss: 0.00124261
Iteration 9/25 | Loss: 0.00124261
Iteration 10/25 | Loss: 0.00124261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001242610509507358, 0.001242610509507358, 0.001242610509507358, 0.001242610509507358, 0.001242610509507358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001242610509507358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19351029
Iteration 2/25 | Loss: 0.00208411
Iteration 3/25 | Loss: 0.00208410
Iteration 4/25 | Loss: 0.00208410
Iteration 5/25 | Loss: 0.00208410
Iteration 6/25 | Loss: 0.00208410
Iteration 7/25 | Loss: 0.00208410
Iteration 8/25 | Loss: 0.00208410
Iteration 9/25 | Loss: 0.00208410
Iteration 10/25 | Loss: 0.00208410
Iteration 11/25 | Loss: 0.00208410
Iteration 12/25 | Loss: 0.00208410
Iteration 13/25 | Loss: 0.00208410
Iteration 14/25 | Loss: 0.00208410
Iteration 15/25 | Loss: 0.00208410
Iteration 16/25 | Loss: 0.00208410
Iteration 17/25 | Loss: 0.00208410
Iteration 18/25 | Loss: 0.00208410
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0020840996876358986, 0.0020840996876358986, 0.0020840996876358986, 0.0020840996876358986, 0.0020840996876358986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020840996876358986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208410
Iteration 2/1000 | Loss: 0.00004732
Iteration 3/1000 | Loss: 0.00002966
Iteration 4/1000 | Loss: 0.00002278
Iteration 5/1000 | Loss: 0.00002074
Iteration 6/1000 | Loss: 0.00001910
Iteration 7/1000 | Loss: 0.00001822
Iteration 8/1000 | Loss: 0.00001761
Iteration 9/1000 | Loss: 0.00001717
Iteration 10/1000 | Loss: 0.00001664
Iteration 11/1000 | Loss: 0.00001627
Iteration 12/1000 | Loss: 0.00001605
Iteration 13/1000 | Loss: 0.00001595
Iteration 14/1000 | Loss: 0.00001592
Iteration 15/1000 | Loss: 0.00001574
Iteration 16/1000 | Loss: 0.00001557
Iteration 17/1000 | Loss: 0.00001548
Iteration 18/1000 | Loss: 0.00001541
Iteration 19/1000 | Loss: 0.00001538
Iteration 20/1000 | Loss: 0.00001537
Iteration 21/1000 | Loss: 0.00001536
Iteration 22/1000 | Loss: 0.00001534
Iteration 23/1000 | Loss: 0.00001534
Iteration 24/1000 | Loss: 0.00001531
Iteration 25/1000 | Loss: 0.00001528
Iteration 26/1000 | Loss: 0.00001527
Iteration 27/1000 | Loss: 0.00001526
Iteration 28/1000 | Loss: 0.00001525
Iteration 29/1000 | Loss: 0.00001525
Iteration 30/1000 | Loss: 0.00001524
Iteration 31/1000 | Loss: 0.00001523
Iteration 32/1000 | Loss: 0.00001523
Iteration 33/1000 | Loss: 0.00001522
Iteration 34/1000 | Loss: 0.00001522
Iteration 35/1000 | Loss: 0.00001522
Iteration 36/1000 | Loss: 0.00001521
Iteration 37/1000 | Loss: 0.00001520
Iteration 38/1000 | Loss: 0.00001520
Iteration 39/1000 | Loss: 0.00001519
Iteration 40/1000 | Loss: 0.00001519
Iteration 41/1000 | Loss: 0.00001519
Iteration 42/1000 | Loss: 0.00001519
Iteration 43/1000 | Loss: 0.00001518
Iteration 44/1000 | Loss: 0.00001518
Iteration 45/1000 | Loss: 0.00001517
Iteration 46/1000 | Loss: 0.00001517
Iteration 47/1000 | Loss: 0.00001517
Iteration 48/1000 | Loss: 0.00001516
Iteration 49/1000 | Loss: 0.00001516
Iteration 50/1000 | Loss: 0.00001516
Iteration 51/1000 | Loss: 0.00001516
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001513
Iteration 54/1000 | Loss: 0.00001513
Iteration 55/1000 | Loss: 0.00001511
Iteration 56/1000 | Loss: 0.00001509
Iteration 57/1000 | Loss: 0.00001508
Iteration 58/1000 | Loss: 0.00001508
Iteration 59/1000 | Loss: 0.00001507
Iteration 60/1000 | Loss: 0.00001507
Iteration 61/1000 | Loss: 0.00001507
Iteration 62/1000 | Loss: 0.00001506
Iteration 63/1000 | Loss: 0.00001505
Iteration 64/1000 | Loss: 0.00001505
Iteration 65/1000 | Loss: 0.00001505
Iteration 66/1000 | Loss: 0.00001504
Iteration 67/1000 | Loss: 0.00001504
Iteration 68/1000 | Loss: 0.00001504
Iteration 69/1000 | Loss: 0.00001503
Iteration 70/1000 | Loss: 0.00001503
Iteration 71/1000 | Loss: 0.00001503
Iteration 72/1000 | Loss: 0.00001503
Iteration 73/1000 | Loss: 0.00001503
Iteration 74/1000 | Loss: 0.00001502
Iteration 75/1000 | Loss: 0.00001502
Iteration 76/1000 | Loss: 0.00001502
Iteration 77/1000 | Loss: 0.00001502
Iteration 78/1000 | Loss: 0.00001501
Iteration 79/1000 | Loss: 0.00001501
Iteration 80/1000 | Loss: 0.00001500
Iteration 81/1000 | Loss: 0.00001500
Iteration 82/1000 | Loss: 0.00001500
Iteration 83/1000 | Loss: 0.00001499
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001498
Iteration 86/1000 | Loss: 0.00001498
Iteration 87/1000 | Loss: 0.00001498
Iteration 88/1000 | Loss: 0.00001497
Iteration 89/1000 | Loss: 0.00001497
Iteration 90/1000 | Loss: 0.00001497
Iteration 91/1000 | Loss: 0.00001497
Iteration 92/1000 | Loss: 0.00001496
Iteration 93/1000 | Loss: 0.00001496
Iteration 94/1000 | Loss: 0.00001495
Iteration 95/1000 | Loss: 0.00001495
Iteration 96/1000 | Loss: 0.00001494
Iteration 97/1000 | Loss: 0.00001494
Iteration 98/1000 | Loss: 0.00001494
Iteration 99/1000 | Loss: 0.00001494
Iteration 100/1000 | Loss: 0.00001494
Iteration 101/1000 | Loss: 0.00001493
Iteration 102/1000 | Loss: 0.00001493
Iteration 103/1000 | Loss: 0.00001493
Iteration 104/1000 | Loss: 0.00001493
Iteration 105/1000 | Loss: 0.00001493
Iteration 106/1000 | Loss: 0.00001493
Iteration 107/1000 | Loss: 0.00001492
Iteration 108/1000 | Loss: 0.00001492
Iteration 109/1000 | Loss: 0.00001491
Iteration 110/1000 | Loss: 0.00001491
Iteration 111/1000 | Loss: 0.00001491
Iteration 112/1000 | Loss: 0.00001491
Iteration 113/1000 | Loss: 0.00001491
Iteration 114/1000 | Loss: 0.00001491
Iteration 115/1000 | Loss: 0.00001491
Iteration 116/1000 | Loss: 0.00001491
Iteration 117/1000 | Loss: 0.00001491
Iteration 118/1000 | Loss: 0.00001490
Iteration 119/1000 | Loss: 0.00001490
Iteration 120/1000 | Loss: 0.00001490
Iteration 121/1000 | Loss: 0.00001489
Iteration 122/1000 | Loss: 0.00001489
Iteration 123/1000 | Loss: 0.00001489
Iteration 124/1000 | Loss: 0.00001488
Iteration 125/1000 | Loss: 0.00001488
Iteration 126/1000 | Loss: 0.00001488
Iteration 127/1000 | Loss: 0.00001488
Iteration 128/1000 | Loss: 0.00001488
Iteration 129/1000 | Loss: 0.00001488
Iteration 130/1000 | Loss: 0.00001488
Iteration 131/1000 | Loss: 0.00001487
Iteration 132/1000 | Loss: 0.00001487
Iteration 133/1000 | Loss: 0.00001487
Iteration 134/1000 | Loss: 0.00001487
Iteration 135/1000 | Loss: 0.00001486
Iteration 136/1000 | Loss: 0.00001486
Iteration 137/1000 | Loss: 0.00001486
Iteration 138/1000 | Loss: 0.00001485
Iteration 139/1000 | Loss: 0.00001485
Iteration 140/1000 | Loss: 0.00001485
Iteration 141/1000 | Loss: 0.00001485
Iteration 142/1000 | Loss: 0.00001485
Iteration 143/1000 | Loss: 0.00001484
Iteration 144/1000 | Loss: 0.00001484
Iteration 145/1000 | Loss: 0.00001484
Iteration 146/1000 | Loss: 0.00001483
Iteration 147/1000 | Loss: 0.00001483
Iteration 148/1000 | Loss: 0.00001483
Iteration 149/1000 | Loss: 0.00001483
Iteration 150/1000 | Loss: 0.00001483
Iteration 151/1000 | Loss: 0.00001482
Iteration 152/1000 | Loss: 0.00001482
Iteration 153/1000 | Loss: 0.00001482
Iteration 154/1000 | Loss: 0.00001482
Iteration 155/1000 | Loss: 0.00001481
Iteration 156/1000 | Loss: 0.00001481
Iteration 157/1000 | Loss: 0.00001481
Iteration 158/1000 | Loss: 0.00001481
Iteration 159/1000 | Loss: 0.00001481
Iteration 160/1000 | Loss: 0.00001481
Iteration 161/1000 | Loss: 0.00001481
Iteration 162/1000 | Loss: 0.00001481
Iteration 163/1000 | Loss: 0.00001481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.4813361303822603e-05, 1.4813361303822603e-05, 1.4813361303822603e-05, 1.4813361303822603e-05, 1.4813361303822603e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4813361303822603e-05

Optimization complete. Final v2v error: 3.248852014541626 mm

Highest mean error: 3.875927209854126 mm for frame 251

Lowest mean error: 2.8470802307128906 mm for frame 25

Saving results

Total time: 51.427072286605835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00727842
Iteration 2/25 | Loss: 0.00198195
Iteration 3/25 | Loss: 0.00161071
Iteration 4/25 | Loss: 0.00151786
Iteration 5/25 | Loss: 0.00141313
Iteration 6/25 | Loss: 0.00127875
Iteration 7/25 | Loss: 0.00123969
Iteration 8/25 | Loss: 0.00123215
Iteration 9/25 | Loss: 0.00122807
Iteration 10/25 | Loss: 0.00121849
Iteration 11/25 | Loss: 0.00121468
Iteration 12/25 | Loss: 0.00121325
Iteration 13/25 | Loss: 0.00121305
Iteration 14/25 | Loss: 0.00121304
Iteration 15/25 | Loss: 0.00121304
Iteration 16/25 | Loss: 0.00121304
Iteration 17/25 | Loss: 0.00121304
Iteration 18/25 | Loss: 0.00121304
Iteration 19/25 | Loss: 0.00121304
Iteration 20/25 | Loss: 0.00121304
Iteration 21/25 | Loss: 0.00121304
Iteration 22/25 | Loss: 0.00121304
Iteration 23/25 | Loss: 0.00121304
Iteration 24/25 | Loss: 0.00121303
Iteration 25/25 | Loss: 0.00121303

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24827123
Iteration 2/25 | Loss: 0.00115178
Iteration 3/25 | Loss: 0.00115177
Iteration 4/25 | Loss: 0.00115177
Iteration 5/25 | Loss: 0.00115177
Iteration 6/25 | Loss: 0.00115177
Iteration 7/25 | Loss: 0.00115177
Iteration 8/25 | Loss: 0.00115177
Iteration 9/25 | Loss: 0.00115177
Iteration 10/25 | Loss: 0.00115177
Iteration 11/25 | Loss: 0.00115177
Iteration 12/25 | Loss: 0.00115177
Iteration 13/25 | Loss: 0.00115177
Iteration 14/25 | Loss: 0.00115177
Iteration 15/25 | Loss: 0.00115177
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011517670936882496, 0.0011517670936882496, 0.0011517670936882496, 0.0011517670936882496, 0.0011517670936882496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011517670936882496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115177
Iteration 2/1000 | Loss: 0.00002636
Iteration 3/1000 | Loss: 0.00001770
Iteration 4/1000 | Loss: 0.00001657
Iteration 5/1000 | Loss: 0.00001574
Iteration 6/1000 | Loss: 0.00001517
Iteration 7/1000 | Loss: 0.00001427
Iteration 8/1000 | Loss: 0.00001383
Iteration 9/1000 | Loss: 0.00001332
Iteration 10/1000 | Loss: 0.00001332
Iteration 11/1000 | Loss: 0.00001315
Iteration 12/1000 | Loss: 0.00001287
Iteration 13/1000 | Loss: 0.00001274
Iteration 14/1000 | Loss: 0.00001265
Iteration 15/1000 | Loss: 0.00001250
Iteration 16/1000 | Loss: 0.00001250
Iteration 17/1000 | Loss: 0.00001250
Iteration 18/1000 | Loss: 0.00001250
Iteration 19/1000 | Loss: 0.00001249
Iteration 20/1000 | Loss: 0.00001248
Iteration 21/1000 | Loss: 0.00001248
Iteration 22/1000 | Loss: 0.00001247
Iteration 23/1000 | Loss: 0.00001246
Iteration 24/1000 | Loss: 0.00001245
Iteration 25/1000 | Loss: 0.00001244
Iteration 26/1000 | Loss: 0.00001243
Iteration 27/1000 | Loss: 0.00001242
Iteration 28/1000 | Loss: 0.00001241
Iteration 29/1000 | Loss: 0.00001241
Iteration 30/1000 | Loss: 0.00001241
Iteration 31/1000 | Loss: 0.00001240
Iteration 32/1000 | Loss: 0.00001240
Iteration 33/1000 | Loss: 0.00001240
Iteration 34/1000 | Loss: 0.00001240
Iteration 35/1000 | Loss: 0.00001240
Iteration 36/1000 | Loss: 0.00001240
Iteration 37/1000 | Loss: 0.00001239
Iteration 38/1000 | Loss: 0.00001239
Iteration 39/1000 | Loss: 0.00001239
Iteration 40/1000 | Loss: 0.00001239
Iteration 41/1000 | Loss: 0.00001239
Iteration 42/1000 | Loss: 0.00001239
Iteration 43/1000 | Loss: 0.00001238
Iteration 44/1000 | Loss: 0.00001238
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001238
Iteration 47/1000 | Loss: 0.00001237
Iteration 48/1000 | Loss: 0.00001237
Iteration 49/1000 | Loss: 0.00001237
Iteration 50/1000 | Loss: 0.00001236
Iteration 51/1000 | Loss: 0.00001236
Iteration 52/1000 | Loss: 0.00001234
Iteration 53/1000 | Loss: 0.00001234
Iteration 54/1000 | Loss: 0.00001234
Iteration 55/1000 | Loss: 0.00001234
Iteration 56/1000 | Loss: 0.00001234
Iteration 57/1000 | Loss: 0.00001233
Iteration 58/1000 | Loss: 0.00001233
Iteration 59/1000 | Loss: 0.00001233
Iteration 60/1000 | Loss: 0.00001233
Iteration 61/1000 | Loss: 0.00001233
Iteration 62/1000 | Loss: 0.00001233
Iteration 63/1000 | Loss: 0.00001232
Iteration 64/1000 | Loss: 0.00001232
Iteration 65/1000 | Loss: 0.00001232
Iteration 66/1000 | Loss: 0.00001232
Iteration 67/1000 | Loss: 0.00001231
Iteration 68/1000 | Loss: 0.00001231
Iteration 69/1000 | Loss: 0.00001231
Iteration 70/1000 | Loss: 0.00001231
Iteration 71/1000 | Loss: 0.00001231
Iteration 72/1000 | Loss: 0.00001231
Iteration 73/1000 | Loss: 0.00001231
Iteration 74/1000 | Loss: 0.00001231
Iteration 75/1000 | Loss: 0.00001231
Iteration 76/1000 | Loss: 0.00001231
Iteration 77/1000 | Loss: 0.00001230
Iteration 78/1000 | Loss: 0.00001230
Iteration 79/1000 | Loss: 0.00001230
Iteration 80/1000 | Loss: 0.00001230
Iteration 81/1000 | Loss: 0.00001230
Iteration 82/1000 | Loss: 0.00001229
Iteration 83/1000 | Loss: 0.00001229
Iteration 84/1000 | Loss: 0.00001229
Iteration 85/1000 | Loss: 0.00001228
Iteration 86/1000 | Loss: 0.00001228
Iteration 87/1000 | Loss: 0.00001228
Iteration 88/1000 | Loss: 0.00001228
Iteration 89/1000 | Loss: 0.00001228
Iteration 90/1000 | Loss: 0.00001228
Iteration 91/1000 | Loss: 0.00001228
Iteration 92/1000 | Loss: 0.00001228
Iteration 93/1000 | Loss: 0.00001228
Iteration 94/1000 | Loss: 0.00001228
Iteration 95/1000 | Loss: 0.00001228
Iteration 96/1000 | Loss: 0.00001228
Iteration 97/1000 | Loss: 0.00001228
Iteration 98/1000 | Loss: 0.00001227
Iteration 99/1000 | Loss: 0.00001227
Iteration 100/1000 | Loss: 0.00001227
Iteration 101/1000 | Loss: 0.00001227
Iteration 102/1000 | Loss: 0.00001227
Iteration 103/1000 | Loss: 0.00001227
Iteration 104/1000 | Loss: 0.00001227
Iteration 105/1000 | Loss: 0.00001227
Iteration 106/1000 | Loss: 0.00001227
Iteration 107/1000 | Loss: 0.00001227
Iteration 108/1000 | Loss: 0.00001227
Iteration 109/1000 | Loss: 0.00001227
Iteration 110/1000 | Loss: 0.00001227
Iteration 111/1000 | Loss: 0.00001227
Iteration 112/1000 | Loss: 0.00001226
Iteration 113/1000 | Loss: 0.00001226
Iteration 114/1000 | Loss: 0.00001226
Iteration 115/1000 | Loss: 0.00001226
Iteration 116/1000 | Loss: 0.00001226
Iteration 117/1000 | Loss: 0.00001226
Iteration 118/1000 | Loss: 0.00001226
Iteration 119/1000 | Loss: 0.00001226
Iteration 120/1000 | Loss: 0.00001226
Iteration 121/1000 | Loss: 0.00001226
Iteration 122/1000 | Loss: 0.00001226
Iteration 123/1000 | Loss: 0.00001226
Iteration 124/1000 | Loss: 0.00001226
Iteration 125/1000 | Loss: 0.00001226
Iteration 126/1000 | Loss: 0.00001226
Iteration 127/1000 | Loss: 0.00001226
Iteration 128/1000 | Loss: 0.00001225
Iteration 129/1000 | Loss: 0.00001225
Iteration 130/1000 | Loss: 0.00001225
Iteration 131/1000 | Loss: 0.00001225
Iteration 132/1000 | Loss: 0.00001224
Iteration 133/1000 | Loss: 0.00001224
Iteration 134/1000 | Loss: 0.00001224
Iteration 135/1000 | Loss: 0.00001224
Iteration 136/1000 | Loss: 0.00001224
Iteration 137/1000 | Loss: 0.00001224
Iteration 138/1000 | Loss: 0.00001224
Iteration 139/1000 | Loss: 0.00001224
Iteration 140/1000 | Loss: 0.00001224
Iteration 141/1000 | Loss: 0.00001223
Iteration 142/1000 | Loss: 0.00001223
Iteration 143/1000 | Loss: 0.00001223
Iteration 144/1000 | Loss: 0.00001223
Iteration 145/1000 | Loss: 0.00001223
Iteration 146/1000 | Loss: 0.00001222
Iteration 147/1000 | Loss: 0.00001222
Iteration 148/1000 | Loss: 0.00001222
Iteration 149/1000 | Loss: 0.00001222
Iteration 150/1000 | Loss: 0.00001222
Iteration 151/1000 | Loss: 0.00001221
Iteration 152/1000 | Loss: 0.00001221
Iteration 153/1000 | Loss: 0.00001221
Iteration 154/1000 | Loss: 0.00001221
Iteration 155/1000 | Loss: 0.00001221
Iteration 156/1000 | Loss: 0.00001221
Iteration 157/1000 | Loss: 0.00001221
Iteration 158/1000 | Loss: 0.00001221
Iteration 159/1000 | Loss: 0.00001221
Iteration 160/1000 | Loss: 0.00001221
Iteration 161/1000 | Loss: 0.00001221
Iteration 162/1000 | Loss: 0.00001221
Iteration 163/1000 | Loss: 0.00001221
Iteration 164/1000 | Loss: 0.00001220
Iteration 165/1000 | Loss: 0.00001220
Iteration 166/1000 | Loss: 0.00001220
Iteration 167/1000 | Loss: 0.00001220
Iteration 168/1000 | Loss: 0.00001220
Iteration 169/1000 | Loss: 0.00001219
Iteration 170/1000 | Loss: 0.00001219
Iteration 171/1000 | Loss: 0.00001219
Iteration 172/1000 | Loss: 0.00001218
Iteration 173/1000 | Loss: 0.00001218
Iteration 174/1000 | Loss: 0.00001218
Iteration 175/1000 | Loss: 0.00001218
Iteration 176/1000 | Loss: 0.00001217
Iteration 177/1000 | Loss: 0.00001217
Iteration 178/1000 | Loss: 0.00001217
Iteration 179/1000 | Loss: 0.00001217
Iteration 180/1000 | Loss: 0.00001217
Iteration 181/1000 | Loss: 0.00001217
Iteration 182/1000 | Loss: 0.00001217
Iteration 183/1000 | Loss: 0.00001217
Iteration 184/1000 | Loss: 0.00001217
Iteration 185/1000 | Loss: 0.00001217
Iteration 186/1000 | Loss: 0.00001217
Iteration 187/1000 | Loss: 0.00001217
Iteration 188/1000 | Loss: 0.00001217
Iteration 189/1000 | Loss: 0.00001217
Iteration 190/1000 | Loss: 0.00001216
Iteration 191/1000 | Loss: 0.00001216
Iteration 192/1000 | Loss: 0.00001216
Iteration 193/1000 | Loss: 0.00001216
Iteration 194/1000 | Loss: 0.00001216
Iteration 195/1000 | Loss: 0.00001216
Iteration 196/1000 | Loss: 0.00001216
Iteration 197/1000 | Loss: 0.00001216
Iteration 198/1000 | Loss: 0.00001216
Iteration 199/1000 | Loss: 0.00001216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.2164538020442706e-05, 1.2164538020442706e-05, 1.2164538020442706e-05, 1.2164538020442706e-05, 1.2164538020442706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2164538020442706e-05

Optimization complete. Final v2v error: 3.011781692504883 mm

Highest mean error: 3.182887554168701 mm for frame 19

Lowest mean error: 2.9062678813934326 mm for frame 75

Saving results

Total time: 62.25797152519226
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507205
Iteration 2/25 | Loss: 0.00132093
Iteration 3/25 | Loss: 0.00124724
Iteration 4/25 | Loss: 0.00123196
Iteration 5/25 | Loss: 0.00122691
Iteration 6/25 | Loss: 0.00122565
Iteration 7/25 | Loss: 0.00122565
Iteration 8/25 | Loss: 0.00122565
Iteration 9/25 | Loss: 0.00122565
Iteration 10/25 | Loss: 0.00122565
Iteration 11/25 | Loss: 0.00122565
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012256513582542539, 0.0012256513582542539, 0.0012256513582542539, 0.0012256513582542539, 0.0012256513582542539]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012256513582542539

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.95608759
Iteration 2/25 | Loss: 0.00153430
Iteration 3/25 | Loss: 0.00153430
Iteration 4/25 | Loss: 0.00153430
Iteration 5/25 | Loss: 0.00153430
Iteration 6/25 | Loss: 0.00153430
Iteration 7/25 | Loss: 0.00153430
Iteration 8/25 | Loss: 0.00153430
Iteration 9/25 | Loss: 0.00153430
Iteration 10/25 | Loss: 0.00153430
Iteration 11/25 | Loss: 0.00153430
Iteration 12/25 | Loss: 0.00153430
Iteration 13/25 | Loss: 0.00153430
Iteration 14/25 | Loss: 0.00153430
Iteration 15/25 | Loss: 0.00153430
Iteration 16/25 | Loss: 0.00153430
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015342952683568, 0.0015342952683568, 0.0015342952683568, 0.0015342952683568, 0.0015342952683568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015342952683568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153430
Iteration 2/1000 | Loss: 0.00003969
Iteration 3/1000 | Loss: 0.00002688
Iteration 4/1000 | Loss: 0.00002213
Iteration 5/1000 | Loss: 0.00002075
Iteration 6/1000 | Loss: 0.00001992
Iteration 7/1000 | Loss: 0.00001945
Iteration 8/1000 | Loss: 0.00001901
Iteration 9/1000 | Loss: 0.00001861
Iteration 10/1000 | Loss: 0.00001835
Iteration 11/1000 | Loss: 0.00001802
Iteration 12/1000 | Loss: 0.00001777
Iteration 13/1000 | Loss: 0.00001773
Iteration 14/1000 | Loss: 0.00001753
Iteration 15/1000 | Loss: 0.00001752
Iteration 16/1000 | Loss: 0.00001732
Iteration 17/1000 | Loss: 0.00001722
Iteration 18/1000 | Loss: 0.00001721
Iteration 19/1000 | Loss: 0.00001721
Iteration 20/1000 | Loss: 0.00001719
Iteration 21/1000 | Loss: 0.00001719
Iteration 22/1000 | Loss: 0.00001712
Iteration 23/1000 | Loss: 0.00001706
Iteration 24/1000 | Loss: 0.00001702
Iteration 25/1000 | Loss: 0.00001702
Iteration 26/1000 | Loss: 0.00001701
Iteration 27/1000 | Loss: 0.00001700
Iteration 28/1000 | Loss: 0.00001700
Iteration 29/1000 | Loss: 0.00001700
Iteration 30/1000 | Loss: 0.00001699
Iteration 31/1000 | Loss: 0.00001699
Iteration 32/1000 | Loss: 0.00001695
Iteration 33/1000 | Loss: 0.00001695
Iteration 34/1000 | Loss: 0.00001694
Iteration 35/1000 | Loss: 0.00001694
Iteration 36/1000 | Loss: 0.00001693
Iteration 37/1000 | Loss: 0.00001693
Iteration 38/1000 | Loss: 0.00001692
Iteration 39/1000 | Loss: 0.00001692
Iteration 40/1000 | Loss: 0.00001692
Iteration 41/1000 | Loss: 0.00001691
Iteration 42/1000 | Loss: 0.00001690
Iteration 43/1000 | Loss: 0.00001690
Iteration 44/1000 | Loss: 0.00001690
Iteration 45/1000 | Loss: 0.00001689
Iteration 46/1000 | Loss: 0.00001689
Iteration 47/1000 | Loss: 0.00001689
Iteration 48/1000 | Loss: 0.00001689
Iteration 49/1000 | Loss: 0.00001688
Iteration 50/1000 | Loss: 0.00001688
Iteration 51/1000 | Loss: 0.00001688
Iteration 52/1000 | Loss: 0.00001688
Iteration 53/1000 | Loss: 0.00001688
Iteration 54/1000 | Loss: 0.00001688
Iteration 55/1000 | Loss: 0.00001687
Iteration 56/1000 | Loss: 0.00001687
Iteration 57/1000 | Loss: 0.00001687
Iteration 58/1000 | Loss: 0.00001686
Iteration 59/1000 | Loss: 0.00001686
Iteration 60/1000 | Loss: 0.00001685
Iteration 61/1000 | Loss: 0.00001685
Iteration 62/1000 | Loss: 0.00001685
Iteration 63/1000 | Loss: 0.00001685
Iteration 64/1000 | Loss: 0.00001685
Iteration 65/1000 | Loss: 0.00001685
Iteration 66/1000 | Loss: 0.00001685
Iteration 67/1000 | Loss: 0.00001684
Iteration 68/1000 | Loss: 0.00001684
Iteration 69/1000 | Loss: 0.00001684
Iteration 70/1000 | Loss: 0.00001684
Iteration 71/1000 | Loss: 0.00001684
Iteration 72/1000 | Loss: 0.00001684
Iteration 73/1000 | Loss: 0.00001684
Iteration 74/1000 | Loss: 0.00001684
Iteration 75/1000 | Loss: 0.00001684
Iteration 76/1000 | Loss: 0.00001684
Iteration 77/1000 | Loss: 0.00001683
Iteration 78/1000 | Loss: 0.00001683
Iteration 79/1000 | Loss: 0.00001682
Iteration 80/1000 | Loss: 0.00001682
Iteration 81/1000 | Loss: 0.00001681
Iteration 82/1000 | Loss: 0.00001681
Iteration 83/1000 | Loss: 0.00001681
Iteration 84/1000 | Loss: 0.00001681
Iteration 85/1000 | Loss: 0.00001681
Iteration 86/1000 | Loss: 0.00001681
Iteration 87/1000 | Loss: 0.00001681
Iteration 88/1000 | Loss: 0.00001681
Iteration 89/1000 | Loss: 0.00001681
Iteration 90/1000 | Loss: 0.00001681
Iteration 91/1000 | Loss: 0.00001681
Iteration 92/1000 | Loss: 0.00001681
Iteration 93/1000 | Loss: 0.00001680
Iteration 94/1000 | Loss: 0.00001680
Iteration 95/1000 | Loss: 0.00001680
Iteration 96/1000 | Loss: 0.00001679
Iteration 97/1000 | Loss: 0.00001679
Iteration 98/1000 | Loss: 0.00001678
Iteration 99/1000 | Loss: 0.00001678
Iteration 100/1000 | Loss: 0.00001677
Iteration 101/1000 | Loss: 0.00001677
Iteration 102/1000 | Loss: 0.00001677
Iteration 103/1000 | Loss: 0.00001677
Iteration 104/1000 | Loss: 0.00001677
Iteration 105/1000 | Loss: 0.00001676
Iteration 106/1000 | Loss: 0.00001676
Iteration 107/1000 | Loss: 0.00001676
Iteration 108/1000 | Loss: 0.00001676
Iteration 109/1000 | Loss: 0.00001675
Iteration 110/1000 | Loss: 0.00001675
Iteration 111/1000 | Loss: 0.00001674
Iteration 112/1000 | Loss: 0.00001674
Iteration 113/1000 | Loss: 0.00001674
Iteration 114/1000 | Loss: 0.00001674
Iteration 115/1000 | Loss: 0.00001673
Iteration 116/1000 | Loss: 0.00001673
Iteration 117/1000 | Loss: 0.00001673
Iteration 118/1000 | Loss: 0.00001673
Iteration 119/1000 | Loss: 0.00001672
Iteration 120/1000 | Loss: 0.00001672
Iteration 121/1000 | Loss: 0.00001672
Iteration 122/1000 | Loss: 0.00001671
Iteration 123/1000 | Loss: 0.00001670
Iteration 124/1000 | Loss: 0.00001670
Iteration 125/1000 | Loss: 0.00001670
Iteration 126/1000 | Loss: 0.00001670
Iteration 127/1000 | Loss: 0.00001670
Iteration 128/1000 | Loss: 0.00001669
Iteration 129/1000 | Loss: 0.00001669
Iteration 130/1000 | Loss: 0.00001669
Iteration 131/1000 | Loss: 0.00001669
Iteration 132/1000 | Loss: 0.00001669
Iteration 133/1000 | Loss: 0.00001668
Iteration 134/1000 | Loss: 0.00001668
Iteration 135/1000 | Loss: 0.00001668
Iteration 136/1000 | Loss: 0.00001668
Iteration 137/1000 | Loss: 0.00001668
Iteration 138/1000 | Loss: 0.00001667
Iteration 139/1000 | Loss: 0.00001667
Iteration 140/1000 | Loss: 0.00001667
Iteration 141/1000 | Loss: 0.00001667
Iteration 142/1000 | Loss: 0.00001667
Iteration 143/1000 | Loss: 0.00001667
Iteration 144/1000 | Loss: 0.00001667
Iteration 145/1000 | Loss: 0.00001667
Iteration 146/1000 | Loss: 0.00001667
Iteration 147/1000 | Loss: 0.00001667
Iteration 148/1000 | Loss: 0.00001667
Iteration 149/1000 | Loss: 0.00001666
Iteration 150/1000 | Loss: 0.00001666
Iteration 151/1000 | Loss: 0.00001666
Iteration 152/1000 | Loss: 0.00001666
Iteration 153/1000 | Loss: 0.00001666
Iteration 154/1000 | Loss: 0.00001665
Iteration 155/1000 | Loss: 0.00001665
Iteration 156/1000 | Loss: 0.00001665
Iteration 157/1000 | Loss: 0.00001665
Iteration 158/1000 | Loss: 0.00001664
Iteration 159/1000 | Loss: 0.00001664
Iteration 160/1000 | Loss: 0.00001664
Iteration 161/1000 | Loss: 0.00001664
Iteration 162/1000 | Loss: 0.00001664
Iteration 163/1000 | Loss: 0.00001664
Iteration 164/1000 | Loss: 0.00001664
Iteration 165/1000 | Loss: 0.00001664
Iteration 166/1000 | Loss: 0.00001664
Iteration 167/1000 | Loss: 0.00001664
Iteration 168/1000 | Loss: 0.00001664
Iteration 169/1000 | Loss: 0.00001664
Iteration 170/1000 | Loss: 0.00001664
Iteration 171/1000 | Loss: 0.00001664
Iteration 172/1000 | Loss: 0.00001664
Iteration 173/1000 | Loss: 0.00001664
Iteration 174/1000 | Loss: 0.00001664
Iteration 175/1000 | Loss: 0.00001664
Iteration 176/1000 | Loss: 0.00001664
Iteration 177/1000 | Loss: 0.00001664
Iteration 178/1000 | Loss: 0.00001664
Iteration 179/1000 | Loss: 0.00001664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.6636253349133767e-05, 1.6636253349133767e-05, 1.6636253349133767e-05, 1.6636253349133767e-05, 1.6636253349133767e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6636253349133767e-05

Optimization complete. Final v2v error: 3.4403762817382812 mm

Highest mean error: 4.563521862030029 mm for frame 67

Lowest mean error: 2.9368109703063965 mm for frame 17

Saving results

Total time: 43.428916931152344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00583462
Iteration 2/25 | Loss: 0.00127573
Iteration 3/25 | Loss: 0.00122487
Iteration 4/25 | Loss: 0.00121922
Iteration 5/25 | Loss: 0.00121763
Iteration 6/25 | Loss: 0.00121763
Iteration 7/25 | Loss: 0.00121763
Iteration 8/25 | Loss: 0.00121763
Iteration 9/25 | Loss: 0.00121763
Iteration 10/25 | Loss: 0.00121763
Iteration 11/25 | Loss: 0.00121763
Iteration 12/25 | Loss: 0.00121763
Iteration 13/25 | Loss: 0.00121763
Iteration 14/25 | Loss: 0.00121763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012176322052255273, 0.0012176322052255273, 0.0012176322052255273, 0.0012176322052255273, 0.0012176322052255273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012176322052255273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 15.30170631
Iteration 2/25 | Loss: 0.00140927
Iteration 3/25 | Loss: 0.00140920
Iteration 4/25 | Loss: 0.00140920
Iteration 5/25 | Loss: 0.00140920
Iteration 6/25 | Loss: 0.00140920
Iteration 7/25 | Loss: 0.00140920
Iteration 8/25 | Loss: 0.00140920
Iteration 9/25 | Loss: 0.00140920
Iteration 10/25 | Loss: 0.00140920
Iteration 11/25 | Loss: 0.00140920
Iteration 12/25 | Loss: 0.00140920
Iteration 13/25 | Loss: 0.00140920
Iteration 14/25 | Loss: 0.00140920
Iteration 15/25 | Loss: 0.00140920
Iteration 16/25 | Loss: 0.00140920
Iteration 17/25 | Loss: 0.00140920
Iteration 18/25 | Loss: 0.00140920
Iteration 19/25 | Loss: 0.00140920
Iteration 20/25 | Loss: 0.00140920
Iteration 21/25 | Loss: 0.00140920
Iteration 22/25 | Loss: 0.00140920
Iteration 23/25 | Loss: 0.00140920
Iteration 24/25 | Loss: 0.00140920
Iteration 25/25 | Loss: 0.00140920

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140920
Iteration 2/1000 | Loss: 0.00002407
Iteration 3/1000 | Loss: 0.00001689
Iteration 4/1000 | Loss: 0.00001430
Iteration 5/1000 | Loss: 0.00001356
Iteration 6/1000 | Loss: 0.00001296
Iteration 7/1000 | Loss: 0.00001235
Iteration 8/1000 | Loss: 0.00001201
Iteration 9/1000 | Loss: 0.00001176
Iteration 10/1000 | Loss: 0.00001144
Iteration 11/1000 | Loss: 0.00001116
Iteration 12/1000 | Loss: 0.00001102
Iteration 13/1000 | Loss: 0.00001093
Iteration 14/1000 | Loss: 0.00001091
Iteration 15/1000 | Loss: 0.00001090
Iteration 16/1000 | Loss: 0.00001081
Iteration 17/1000 | Loss: 0.00001074
Iteration 18/1000 | Loss: 0.00001064
Iteration 19/1000 | Loss: 0.00001063
Iteration 20/1000 | Loss: 0.00001062
Iteration 21/1000 | Loss: 0.00001061
Iteration 22/1000 | Loss: 0.00001060
Iteration 23/1000 | Loss: 0.00001060
Iteration 24/1000 | Loss: 0.00001059
Iteration 25/1000 | Loss: 0.00001058
Iteration 26/1000 | Loss: 0.00001055
Iteration 27/1000 | Loss: 0.00001055
Iteration 28/1000 | Loss: 0.00001055
Iteration 29/1000 | Loss: 0.00001054
Iteration 30/1000 | Loss: 0.00001054
Iteration 31/1000 | Loss: 0.00001054
Iteration 32/1000 | Loss: 0.00001053
Iteration 33/1000 | Loss: 0.00001047
Iteration 34/1000 | Loss: 0.00001046
Iteration 35/1000 | Loss: 0.00001045
Iteration 36/1000 | Loss: 0.00001044
Iteration 37/1000 | Loss: 0.00001043
Iteration 38/1000 | Loss: 0.00001043
Iteration 39/1000 | Loss: 0.00001042
Iteration 40/1000 | Loss: 0.00001042
Iteration 41/1000 | Loss: 0.00001041
Iteration 42/1000 | Loss: 0.00001040
Iteration 43/1000 | Loss: 0.00001039
Iteration 44/1000 | Loss: 0.00001039
Iteration 45/1000 | Loss: 0.00001039
Iteration 46/1000 | Loss: 0.00001039
Iteration 47/1000 | Loss: 0.00001039
Iteration 48/1000 | Loss: 0.00001039
Iteration 49/1000 | Loss: 0.00001039
Iteration 50/1000 | Loss: 0.00001039
Iteration 51/1000 | Loss: 0.00001039
Iteration 52/1000 | Loss: 0.00001039
Iteration 53/1000 | Loss: 0.00001039
Iteration 54/1000 | Loss: 0.00001038
Iteration 55/1000 | Loss: 0.00001038
Iteration 56/1000 | Loss: 0.00001038
Iteration 57/1000 | Loss: 0.00001038
Iteration 58/1000 | Loss: 0.00001038
Iteration 59/1000 | Loss: 0.00001038
Iteration 60/1000 | Loss: 0.00001038
Iteration 61/1000 | Loss: 0.00001038
Iteration 62/1000 | Loss: 0.00001038
Iteration 63/1000 | Loss: 0.00001038
Iteration 64/1000 | Loss: 0.00001038
Iteration 65/1000 | Loss: 0.00001038
Iteration 66/1000 | Loss: 0.00001037
Iteration 67/1000 | Loss: 0.00001037
Iteration 68/1000 | Loss: 0.00001037
Iteration 69/1000 | Loss: 0.00001036
Iteration 70/1000 | Loss: 0.00001036
Iteration 71/1000 | Loss: 0.00001035
Iteration 72/1000 | Loss: 0.00001035
Iteration 73/1000 | Loss: 0.00001035
Iteration 74/1000 | Loss: 0.00001035
Iteration 75/1000 | Loss: 0.00001035
Iteration 76/1000 | Loss: 0.00001035
Iteration 77/1000 | Loss: 0.00001034
Iteration 78/1000 | Loss: 0.00001034
Iteration 79/1000 | Loss: 0.00001034
Iteration 80/1000 | Loss: 0.00001034
Iteration 81/1000 | Loss: 0.00001033
Iteration 82/1000 | Loss: 0.00001033
Iteration 83/1000 | Loss: 0.00001032
Iteration 84/1000 | Loss: 0.00001032
Iteration 85/1000 | Loss: 0.00001030
Iteration 86/1000 | Loss: 0.00001030
Iteration 87/1000 | Loss: 0.00001030
Iteration 88/1000 | Loss: 0.00001030
Iteration 89/1000 | Loss: 0.00001030
Iteration 90/1000 | Loss: 0.00001030
Iteration 91/1000 | Loss: 0.00001030
Iteration 92/1000 | Loss: 0.00001029
Iteration 93/1000 | Loss: 0.00001029
Iteration 94/1000 | Loss: 0.00001029
Iteration 95/1000 | Loss: 0.00001028
Iteration 96/1000 | Loss: 0.00001028
Iteration 97/1000 | Loss: 0.00001028
Iteration 98/1000 | Loss: 0.00001027
Iteration 99/1000 | Loss: 0.00001027
Iteration 100/1000 | Loss: 0.00001027
Iteration 101/1000 | Loss: 0.00001027
Iteration 102/1000 | Loss: 0.00001027
Iteration 103/1000 | Loss: 0.00001027
Iteration 104/1000 | Loss: 0.00001027
Iteration 105/1000 | Loss: 0.00001026
Iteration 106/1000 | Loss: 0.00001026
Iteration 107/1000 | Loss: 0.00001026
Iteration 108/1000 | Loss: 0.00001025
Iteration 109/1000 | Loss: 0.00001025
Iteration 110/1000 | Loss: 0.00001025
Iteration 111/1000 | Loss: 0.00001024
Iteration 112/1000 | Loss: 0.00001024
Iteration 113/1000 | Loss: 0.00001024
Iteration 114/1000 | Loss: 0.00001024
Iteration 115/1000 | Loss: 0.00001024
Iteration 116/1000 | Loss: 0.00001024
Iteration 117/1000 | Loss: 0.00001023
Iteration 118/1000 | Loss: 0.00001023
Iteration 119/1000 | Loss: 0.00001023
Iteration 120/1000 | Loss: 0.00001023
Iteration 121/1000 | Loss: 0.00001023
Iteration 122/1000 | Loss: 0.00001023
Iteration 123/1000 | Loss: 0.00001023
Iteration 124/1000 | Loss: 0.00001023
Iteration 125/1000 | Loss: 0.00001022
Iteration 126/1000 | Loss: 0.00001022
Iteration 127/1000 | Loss: 0.00001022
Iteration 128/1000 | Loss: 0.00001022
Iteration 129/1000 | Loss: 0.00001022
Iteration 130/1000 | Loss: 0.00001021
Iteration 131/1000 | Loss: 0.00001021
Iteration 132/1000 | Loss: 0.00001021
Iteration 133/1000 | Loss: 0.00001021
Iteration 134/1000 | Loss: 0.00001021
Iteration 135/1000 | Loss: 0.00001021
Iteration 136/1000 | Loss: 0.00001021
Iteration 137/1000 | Loss: 0.00001020
Iteration 138/1000 | Loss: 0.00001020
Iteration 139/1000 | Loss: 0.00001020
Iteration 140/1000 | Loss: 0.00001020
Iteration 141/1000 | Loss: 0.00001020
Iteration 142/1000 | Loss: 0.00001020
Iteration 143/1000 | Loss: 0.00001020
Iteration 144/1000 | Loss: 0.00001020
Iteration 145/1000 | Loss: 0.00001020
Iteration 146/1000 | Loss: 0.00001020
Iteration 147/1000 | Loss: 0.00001019
Iteration 148/1000 | Loss: 0.00001019
Iteration 149/1000 | Loss: 0.00001019
Iteration 150/1000 | Loss: 0.00001019
Iteration 151/1000 | Loss: 0.00001018
Iteration 152/1000 | Loss: 0.00001018
Iteration 153/1000 | Loss: 0.00001018
Iteration 154/1000 | Loss: 0.00001018
Iteration 155/1000 | Loss: 0.00001018
Iteration 156/1000 | Loss: 0.00001018
Iteration 157/1000 | Loss: 0.00001018
Iteration 158/1000 | Loss: 0.00001018
Iteration 159/1000 | Loss: 0.00001018
Iteration 160/1000 | Loss: 0.00001018
Iteration 161/1000 | Loss: 0.00001018
Iteration 162/1000 | Loss: 0.00001017
Iteration 163/1000 | Loss: 0.00001017
Iteration 164/1000 | Loss: 0.00001017
Iteration 165/1000 | Loss: 0.00001017
Iteration 166/1000 | Loss: 0.00001017
Iteration 167/1000 | Loss: 0.00001017
Iteration 168/1000 | Loss: 0.00001017
Iteration 169/1000 | Loss: 0.00001017
Iteration 170/1000 | Loss: 0.00001017
Iteration 171/1000 | Loss: 0.00001016
Iteration 172/1000 | Loss: 0.00001016
Iteration 173/1000 | Loss: 0.00001016
Iteration 174/1000 | Loss: 0.00001016
Iteration 175/1000 | Loss: 0.00001016
Iteration 176/1000 | Loss: 0.00001016
Iteration 177/1000 | Loss: 0.00001016
Iteration 178/1000 | Loss: 0.00001016
Iteration 179/1000 | Loss: 0.00001016
Iteration 180/1000 | Loss: 0.00001016
Iteration 181/1000 | Loss: 0.00001016
Iteration 182/1000 | Loss: 0.00001016
Iteration 183/1000 | Loss: 0.00001016
Iteration 184/1000 | Loss: 0.00001016
Iteration 185/1000 | Loss: 0.00001016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.016252463159617e-05, 1.016252463159617e-05, 1.016252463159617e-05, 1.016252463159617e-05, 1.016252463159617e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.016252463159617e-05

Optimization complete. Final v2v error: 2.7324092388153076 mm

Highest mean error: 2.9599804878234863 mm for frame 131

Lowest mean error: 2.4539194107055664 mm for frame 30

Saving results

Total time: 44.040704011917114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00756829
Iteration 2/25 | Loss: 0.00186040
Iteration 3/25 | Loss: 0.00144298
Iteration 4/25 | Loss: 0.00137595
Iteration 5/25 | Loss: 0.00137180
Iteration 6/25 | Loss: 0.00136055
Iteration 7/25 | Loss: 0.00131889
Iteration 8/25 | Loss: 0.00130591
Iteration 9/25 | Loss: 0.00130003
Iteration 10/25 | Loss: 0.00129613
Iteration 11/25 | Loss: 0.00129469
Iteration 12/25 | Loss: 0.00129430
Iteration 13/25 | Loss: 0.00129424
Iteration 14/25 | Loss: 0.00129424
Iteration 15/25 | Loss: 0.00129423
Iteration 16/25 | Loss: 0.00129423
Iteration 17/25 | Loss: 0.00129423
Iteration 18/25 | Loss: 0.00129423
Iteration 19/25 | Loss: 0.00129423
Iteration 20/25 | Loss: 0.00129423
Iteration 21/25 | Loss: 0.00129423
Iteration 22/25 | Loss: 0.00129423
Iteration 23/25 | Loss: 0.00129422
Iteration 24/25 | Loss: 0.00129422
Iteration 25/25 | Loss: 0.00129422

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25529373
Iteration 2/25 | Loss: 0.00130820
Iteration 3/25 | Loss: 0.00130820
Iteration 4/25 | Loss: 0.00130820
Iteration 5/25 | Loss: 0.00130820
Iteration 6/25 | Loss: 0.00130820
Iteration 7/25 | Loss: 0.00130820
Iteration 8/25 | Loss: 0.00130820
Iteration 9/25 | Loss: 0.00130820
Iteration 10/25 | Loss: 0.00130820
Iteration 11/25 | Loss: 0.00130820
Iteration 12/25 | Loss: 0.00130820
Iteration 13/25 | Loss: 0.00130820
Iteration 14/25 | Loss: 0.00130820
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013081962242722511, 0.0013081962242722511, 0.0013081962242722511, 0.0013081962242722511, 0.0013081962242722511]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013081962242722511

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130820
Iteration 2/1000 | Loss: 0.00089400
Iteration 3/1000 | Loss: 0.00045610
Iteration 4/1000 | Loss: 0.00008568
Iteration 5/1000 | Loss: 0.00006854
Iteration 6/1000 | Loss: 0.00005637
Iteration 7/1000 | Loss: 0.00004861
Iteration 8/1000 | Loss: 0.00004558
Iteration 9/1000 | Loss: 0.00004132
Iteration 10/1000 | Loss: 0.00003804
Iteration 11/1000 | Loss: 0.00103706
Iteration 12/1000 | Loss: 0.00013343
Iteration 13/1000 | Loss: 0.00004278
Iteration 14/1000 | Loss: 0.00003453
Iteration 15/1000 | Loss: 0.00004401
Iteration 16/1000 | Loss: 0.00003012
Iteration 17/1000 | Loss: 0.00002625
Iteration 18/1000 | Loss: 0.00003298
Iteration 19/1000 | Loss: 0.00010905
Iteration 20/1000 | Loss: 0.00005002
Iteration 21/1000 | Loss: 0.00003471
Iteration 22/1000 | Loss: 0.00002480
Iteration 23/1000 | Loss: 0.00002347
Iteration 24/1000 | Loss: 0.00002233
Iteration 25/1000 | Loss: 0.00002175
Iteration 26/1000 | Loss: 0.00002117
Iteration 27/1000 | Loss: 0.00002060
Iteration 28/1000 | Loss: 0.00002015
Iteration 29/1000 | Loss: 0.00001975
Iteration 30/1000 | Loss: 0.00001945
Iteration 31/1000 | Loss: 0.00001914
Iteration 32/1000 | Loss: 0.00003627
Iteration 33/1000 | Loss: 0.00002285
Iteration 34/1000 | Loss: 0.00002001
Iteration 35/1000 | Loss: 0.00001856
Iteration 36/1000 | Loss: 0.00001779
Iteration 37/1000 | Loss: 0.00001732
Iteration 38/1000 | Loss: 0.00001699
Iteration 39/1000 | Loss: 0.00001699
Iteration 40/1000 | Loss: 0.00001686
Iteration 41/1000 | Loss: 0.00001676
Iteration 42/1000 | Loss: 0.00001661
Iteration 43/1000 | Loss: 0.00001659
Iteration 44/1000 | Loss: 0.00001658
Iteration 45/1000 | Loss: 0.00001657
Iteration 46/1000 | Loss: 0.00001654
Iteration 47/1000 | Loss: 0.00001653
Iteration 48/1000 | Loss: 0.00001652
Iteration 49/1000 | Loss: 0.00001652
Iteration 50/1000 | Loss: 0.00001651
Iteration 51/1000 | Loss: 0.00001648
Iteration 52/1000 | Loss: 0.00001646
Iteration 53/1000 | Loss: 0.00001646
Iteration 54/1000 | Loss: 0.00001646
Iteration 55/1000 | Loss: 0.00001645
Iteration 56/1000 | Loss: 0.00001645
Iteration 57/1000 | Loss: 0.00001645
Iteration 58/1000 | Loss: 0.00001645
Iteration 59/1000 | Loss: 0.00001645
Iteration 60/1000 | Loss: 0.00001645
Iteration 61/1000 | Loss: 0.00001644
Iteration 62/1000 | Loss: 0.00001644
Iteration 63/1000 | Loss: 0.00001644
Iteration 64/1000 | Loss: 0.00001644
Iteration 65/1000 | Loss: 0.00001643
Iteration 66/1000 | Loss: 0.00001643
Iteration 67/1000 | Loss: 0.00001643
Iteration 68/1000 | Loss: 0.00001643
Iteration 69/1000 | Loss: 0.00001643
Iteration 70/1000 | Loss: 0.00001643
Iteration 71/1000 | Loss: 0.00001643
Iteration 72/1000 | Loss: 0.00001643
Iteration 73/1000 | Loss: 0.00001643
Iteration 74/1000 | Loss: 0.00001643
Iteration 75/1000 | Loss: 0.00001642
Iteration 76/1000 | Loss: 0.00001642
Iteration 77/1000 | Loss: 0.00001642
Iteration 78/1000 | Loss: 0.00001642
Iteration 79/1000 | Loss: 0.00001642
Iteration 80/1000 | Loss: 0.00001641
Iteration 81/1000 | Loss: 0.00001641
Iteration 82/1000 | Loss: 0.00001641
Iteration 83/1000 | Loss: 0.00001641
Iteration 84/1000 | Loss: 0.00001641
Iteration 85/1000 | Loss: 0.00001641
Iteration 86/1000 | Loss: 0.00001641
Iteration 87/1000 | Loss: 0.00001641
Iteration 88/1000 | Loss: 0.00001641
Iteration 89/1000 | Loss: 0.00001641
Iteration 90/1000 | Loss: 0.00001640
Iteration 91/1000 | Loss: 0.00001640
Iteration 92/1000 | Loss: 0.00001640
Iteration 93/1000 | Loss: 0.00001640
Iteration 94/1000 | Loss: 0.00001640
Iteration 95/1000 | Loss: 0.00001640
Iteration 96/1000 | Loss: 0.00001640
Iteration 97/1000 | Loss: 0.00001640
Iteration 98/1000 | Loss: 0.00001640
Iteration 99/1000 | Loss: 0.00001640
Iteration 100/1000 | Loss: 0.00001640
Iteration 101/1000 | Loss: 0.00001640
Iteration 102/1000 | Loss: 0.00001640
Iteration 103/1000 | Loss: 0.00001640
Iteration 104/1000 | Loss: 0.00001640
Iteration 105/1000 | Loss: 0.00001640
Iteration 106/1000 | Loss: 0.00001640
Iteration 107/1000 | Loss: 0.00001639
Iteration 108/1000 | Loss: 0.00001639
Iteration 109/1000 | Loss: 0.00001639
Iteration 110/1000 | Loss: 0.00001639
Iteration 111/1000 | Loss: 0.00001639
Iteration 112/1000 | Loss: 0.00001639
Iteration 113/1000 | Loss: 0.00001639
Iteration 114/1000 | Loss: 0.00001639
Iteration 115/1000 | Loss: 0.00001639
Iteration 116/1000 | Loss: 0.00001639
Iteration 117/1000 | Loss: 0.00001639
Iteration 118/1000 | Loss: 0.00001639
Iteration 119/1000 | Loss: 0.00001639
Iteration 120/1000 | Loss: 0.00001639
Iteration 121/1000 | Loss: 0.00001639
Iteration 122/1000 | Loss: 0.00001639
Iteration 123/1000 | Loss: 0.00001639
Iteration 124/1000 | Loss: 0.00001639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.6393059922847897e-05, 1.6393059922847897e-05, 1.6393059922847897e-05, 1.6393059922847897e-05, 1.6393059922847897e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6393059922847897e-05

Optimization complete. Final v2v error: 3.394320011138916 mm

Highest mean error: 4.457151889801025 mm for frame 219

Lowest mean error: 2.6787753105163574 mm for frame 78

Saving results

Total time: 97.94397401809692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422043
Iteration 2/25 | Loss: 0.00132915
Iteration 3/25 | Loss: 0.00121613
Iteration 4/25 | Loss: 0.00121030
Iteration 5/25 | Loss: 0.00120919
Iteration 6/25 | Loss: 0.00120919
Iteration 7/25 | Loss: 0.00120919
Iteration 8/25 | Loss: 0.00120919
Iteration 9/25 | Loss: 0.00120919
Iteration 10/25 | Loss: 0.00120919
Iteration 11/25 | Loss: 0.00120919
Iteration 12/25 | Loss: 0.00120919
Iteration 13/25 | Loss: 0.00120919
Iteration 14/25 | Loss: 0.00120919
Iteration 15/25 | Loss: 0.00120919
Iteration 16/25 | Loss: 0.00120919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012091916287317872, 0.0012091916287317872, 0.0012091916287317872, 0.0012091916287317872, 0.0012091916287317872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012091916287317872

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10141551
Iteration 2/25 | Loss: 0.00095277
Iteration 3/25 | Loss: 0.00095277
Iteration 4/25 | Loss: 0.00095277
Iteration 5/25 | Loss: 0.00095277
Iteration 6/25 | Loss: 0.00095277
Iteration 7/25 | Loss: 0.00095277
Iteration 8/25 | Loss: 0.00095277
Iteration 9/25 | Loss: 0.00095277
Iteration 10/25 | Loss: 0.00095277
Iteration 11/25 | Loss: 0.00095276
Iteration 12/25 | Loss: 0.00095276
Iteration 13/25 | Loss: 0.00095276
Iteration 14/25 | Loss: 0.00095276
Iteration 15/25 | Loss: 0.00095276
Iteration 16/25 | Loss: 0.00095276
Iteration 17/25 | Loss: 0.00095276
Iteration 18/25 | Loss: 0.00095276
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009527647052891552, 0.0009527647052891552, 0.0009527647052891552, 0.0009527647052891552, 0.0009527647052891552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009527647052891552

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095276
Iteration 2/1000 | Loss: 0.00003176
Iteration 3/1000 | Loss: 0.00001661
Iteration 4/1000 | Loss: 0.00001460
Iteration 5/1000 | Loss: 0.00001368
Iteration 6/1000 | Loss: 0.00001310
Iteration 7/1000 | Loss: 0.00001254
Iteration 8/1000 | Loss: 0.00001215
Iteration 9/1000 | Loss: 0.00001168
Iteration 10/1000 | Loss: 0.00001143
Iteration 11/1000 | Loss: 0.00001116
Iteration 12/1000 | Loss: 0.00001110
Iteration 13/1000 | Loss: 0.00001108
Iteration 14/1000 | Loss: 0.00001107
Iteration 15/1000 | Loss: 0.00001085
Iteration 16/1000 | Loss: 0.00001078
Iteration 17/1000 | Loss: 0.00001077
Iteration 18/1000 | Loss: 0.00001076
Iteration 19/1000 | Loss: 0.00001075
Iteration 20/1000 | Loss: 0.00001075
Iteration 21/1000 | Loss: 0.00001074
Iteration 22/1000 | Loss: 0.00001074
Iteration 23/1000 | Loss: 0.00001073
Iteration 24/1000 | Loss: 0.00001072
Iteration 25/1000 | Loss: 0.00001071
Iteration 26/1000 | Loss: 0.00001071
Iteration 27/1000 | Loss: 0.00001071
Iteration 28/1000 | Loss: 0.00001071
Iteration 29/1000 | Loss: 0.00001071
Iteration 30/1000 | Loss: 0.00001071
Iteration 31/1000 | Loss: 0.00001071
Iteration 32/1000 | Loss: 0.00001071
Iteration 33/1000 | Loss: 0.00001070
Iteration 34/1000 | Loss: 0.00001070
Iteration 35/1000 | Loss: 0.00001069
Iteration 36/1000 | Loss: 0.00001069
Iteration 37/1000 | Loss: 0.00001068
Iteration 38/1000 | Loss: 0.00001068
Iteration 39/1000 | Loss: 0.00001066
Iteration 40/1000 | Loss: 0.00001065
Iteration 41/1000 | Loss: 0.00001065
Iteration 42/1000 | Loss: 0.00001065
Iteration 43/1000 | Loss: 0.00001064
Iteration 44/1000 | Loss: 0.00001064
Iteration 45/1000 | Loss: 0.00001064
Iteration 46/1000 | Loss: 0.00001064
Iteration 47/1000 | Loss: 0.00001064
Iteration 48/1000 | Loss: 0.00001064
Iteration 49/1000 | Loss: 0.00001064
Iteration 50/1000 | Loss: 0.00001064
Iteration 51/1000 | Loss: 0.00001064
Iteration 52/1000 | Loss: 0.00001064
Iteration 53/1000 | Loss: 0.00001064
Iteration 54/1000 | Loss: 0.00001064
Iteration 55/1000 | Loss: 0.00001064
Iteration 56/1000 | Loss: 0.00001064
Iteration 57/1000 | Loss: 0.00001063
Iteration 58/1000 | Loss: 0.00001063
Iteration 59/1000 | Loss: 0.00001063
Iteration 60/1000 | Loss: 0.00001063
Iteration 61/1000 | Loss: 0.00001063
Iteration 62/1000 | Loss: 0.00001063
Iteration 63/1000 | Loss: 0.00001063
Iteration 64/1000 | Loss: 0.00001063
Iteration 65/1000 | Loss: 0.00001062
Iteration 66/1000 | Loss: 0.00001062
Iteration 67/1000 | Loss: 0.00001062
Iteration 68/1000 | Loss: 0.00001062
Iteration 69/1000 | Loss: 0.00001062
Iteration 70/1000 | Loss: 0.00001062
Iteration 71/1000 | Loss: 0.00001062
Iteration 72/1000 | Loss: 0.00001061
Iteration 73/1000 | Loss: 0.00001061
Iteration 74/1000 | Loss: 0.00001061
Iteration 75/1000 | Loss: 0.00001061
Iteration 76/1000 | Loss: 0.00001061
Iteration 77/1000 | Loss: 0.00001061
Iteration 78/1000 | Loss: 0.00001061
Iteration 79/1000 | Loss: 0.00001061
Iteration 80/1000 | Loss: 0.00001061
Iteration 81/1000 | Loss: 0.00001061
Iteration 82/1000 | Loss: 0.00001061
Iteration 83/1000 | Loss: 0.00001060
Iteration 84/1000 | Loss: 0.00001060
Iteration 85/1000 | Loss: 0.00001060
Iteration 86/1000 | Loss: 0.00001060
Iteration 87/1000 | Loss: 0.00001060
Iteration 88/1000 | Loss: 0.00001060
Iteration 89/1000 | Loss: 0.00001059
Iteration 90/1000 | Loss: 0.00001059
Iteration 91/1000 | Loss: 0.00001059
Iteration 92/1000 | Loss: 0.00001059
Iteration 93/1000 | Loss: 0.00001059
Iteration 94/1000 | Loss: 0.00001059
Iteration 95/1000 | Loss: 0.00001059
Iteration 96/1000 | Loss: 0.00001059
Iteration 97/1000 | Loss: 0.00001059
Iteration 98/1000 | Loss: 0.00001058
Iteration 99/1000 | Loss: 0.00001058
Iteration 100/1000 | Loss: 0.00001058
Iteration 101/1000 | Loss: 0.00001058
Iteration 102/1000 | Loss: 0.00001058
Iteration 103/1000 | Loss: 0.00001057
Iteration 104/1000 | Loss: 0.00001057
Iteration 105/1000 | Loss: 0.00001056
Iteration 106/1000 | Loss: 0.00001056
Iteration 107/1000 | Loss: 0.00001055
Iteration 108/1000 | Loss: 0.00001054
Iteration 109/1000 | Loss: 0.00001054
Iteration 110/1000 | Loss: 0.00001052
Iteration 111/1000 | Loss: 0.00001052
Iteration 112/1000 | Loss: 0.00001052
Iteration 113/1000 | Loss: 0.00001051
Iteration 114/1000 | Loss: 0.00001051
Iteration 115/1000 | Loss: 0.00001051
Iteration 116/1000 | Loss: 0.00001050
Iteration 117/1000 | Loss: 0.00001050
Iteration 118/1000 | Loss: 0.00001050
Iteration 119/1000 | Loss: 0.00001050
Iteration 120/1000 | Loss: 0.00001049
Iteration 121/1000 | Loss: 0.00001049
Iteration 122/1000 | Loss: 0.00001049
Iteration 123/1000 | Loss: 0.00001049
Iteration 124/1000 | Loss: 0.00001049
Iteration 125/1000 | Loss: 0.00001049
Iteration 126/1000 | Loss: 0.00001049
Iteration 127/1000 | Loss: 0.00001048
Iteration 128/1000 | Loss: 0.00001048
Iteration 129/1000 | Loss: 0.00001048
Iteration 130/1000 | Loss: 0.00001048
Iteration 131/1000 | Loss: 0.00001048
Iteration 132/1000 | Loss: 0.00001047
Iteration 133/1000 | Loss: 0.00001047
Iteration 134/1000 | Loss: 0.00001047
Iteration 135/1000 | Loss: 0.00001047
Iteration 136/1000 | Loss: 0.00001046
Iteration 137/1000 | Loss: 0.00001046
Iteration 138/1000 | Loss: 0.00001046
Iteration 139/1000 | Loss: 0.00001046
Iteration 140/1000 | Loss: 0.00001045
Iteration 141/1000 | Loss: 0.00001045
Iteration 142/1000 | Loss: 0.00001045
Iteration 143/1000 | Loss: 0.00001045
Iteration 144/1000 | Loss: 0.00001045
Iteration 145/1000 | Loss: 0.00001045
Iteration 146/1000 | Loss: 0.00001044
Iteration 147/1000 | Loss: 0.00001044
Iteration 148/1000 | Loss: 0.00001044
Iteration 149/1000 | Loss: 0.00001044
Iteration 150/1000 | Loss: 0.00001044
Iteration 151/1000 | Loss: 0.00001044
Iteration 152/1000 | Loss: 0.00001043
Iteration 153/1000 | Loss: 0.00001043
Iteration 154/1000 | Loss: 0.00001042
Iteration 155/1000 | Loss: 0.00001042
Iteration 156/1000 | Loss: 0.00001042
Iteration 157/1000 | Loss: 0.00001042
Iteration 158/1000 | Loss: 0.00001042
Iteration 159/1000 | Loss: 0.00001041
Iteration 160/1000 | Loss: 0.00001041
Iteration 161/1000 | Loss: 0.00001040
Iteration 162/1000 | Loss: 0.00001039
Iteration 163/1000 | Loss: 0.00001039
Iteration 164/1000 | Loss: 0.00001039
Iteration 165/1000 | Loss: 0.00001039
Iteration 166/1000 | Loss: 0.00001039
Iteration 167/1000 | Loss: 0.00001039
Iteration 168/1000 | Loss: 0.00001039
Iteration 169/1000 | Loss: 0.00001038
Iteration 170/1000 | Loss: 0.00001038
Iteration 171/1000 | Loss: 0.00001038
Iteration 172/1000 | Loss: 0.00001038
Iteration 173/1000 | Loss: 0.00001038
Iteration 174/1000 | Loss: 0.00001038
Iteration 175/1000 | Loss: 0.00001038
Iteration 176/1000 | Loss: 0.00001037
Iteration 177/1000 | Loss: 0.00001037
Iteration 178/1000 | Loss: 0.00001037
Iteration 179/1000 | Loss: 0.00001037
Iteration 180/1000 | Loss: 0.00001036
Iteration 181/1000 | Loss: 0.00001036
Iteration 182/1000 | Loss: 0.00001036
Iteration 183/1000 | Loss: 0.00001036
Iteration 184/1000 | Loss: 0.00001036
Iteration 185/1000 | Loss: 0.00001036
Iteration 186/1000 | Loss: 0.00001036
Iteration 187/1000 | Loss: 0.00001036
Iteration 188/1000 | Loss: 0.00001036
Iteration 189/1000 | Loss: 0.00001035
Iteration 190/1000 | Loss: 0.00001035
Iteration 191/1000 | Loss: 0.00001035
Iteration 192/1000 | Loss: 0.00001035
Iteration 193/1000 | Loss: 0.00001035
Iteration 194/1000 | Loss: 0.00001035
Iteration 195/1000 | Loss: 0.00001035
Iteration 196/1000 | Loss: 0.00001034
Iteration 197/1000 | Loss: 0.00001034
Iteration 198/1000 | Loss: 0.00001034
Iteration 199/1000 | Loss: 0.00001034
Iteration 200/1000 | Loss: 0.00001034
Iteration 201/1000 | Loss: 0.00001034
Iteration 202/1000 | Loss: 0.00001034
Iteration 203/1000 | Loss: 0.00001034
Iteration 204/1000 | Loss: 0.00001034
Iteration 205/1000 | Loss: 0.00001034
Iteration 206/1000 | Loss: 0.00001034
Iteration 207/1000 | Loss: 0.00001034
Iteration 208/1000 | Loss: 0.00001034
Iteration 209/1000 | Loss: 0.00001033
Iteration 210/1000 | Loss: 0.00001033
Iteration 211/1000 | Loss: 0.00001033
Iteration 212/1000 | Loss: 0.00001033
Iteration 213/1000 | Loss: 0.00001033
Iteration 214/1000 | Loss: 0.00001033
Iteration 215/1000 | Loss: 0.00001033
Iteration 216/1000 | Loss: 0.00001033
Iteration 217/1000 | Loss: 0.00001033
Iteration 218/1000 | Loss: 0.00001033
Iteration 219/1000 | Loss: 0.00001033
Iteration 220/1000 | Loss: 0.00001033
Iteration 221/1000 | Loss: 0.00001033
Iteration 222/1000 | Loss: 0.00001033
Iteration 223/1000 | Loss: 0.00001032
Iteration 224/1000 | Loss: 0.00001032
Iteration 225/1000 | Loss: 0.00001032
Iteration 226/1000 | Loss: 0.00001032
Iteration 227/1000 | Loss: 0.00001032
Iteration 228/1000 | Loss: 0.00001032
Iteration 229/1000 | Loss: 0.00001032
Iteration 230/1000 | Loss: 0.00001032
Iteration 231/1000 | Loss: 0.00001032
Iteration 232/1000 | Loss: 0.00001032
Iteration 233/1000 | Loss: 0.00001032
Iteration 234/1000 | Loss: 0.00001032
Iteration 235/1000 | Loss: 0.00001032
Iteration 236/1000 | Loss: 0.00001032
Iteration 237/1000 | Loss: 0.00001032
Iteration 238/1000 | Loss: 0.00001032
Iteration 239/1000 | Loss: 0.00001032
Iteration 240/1000 | Loss: 0.00001032
Iteration 241/1000 | Loss: 0.00001032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.0321000445401296e-05, 1.0321000445401296e-05, 1.0321000445401296e-05, 1.0321000445401296e-05, 1.0321000445401296e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0321000445401296e-05

Optimization complete. Final v2v error: 2.779956579208374 mm

Highest mean error: 2.8450326919555664 mm for frame 112

Lowest mean error: 2.7569525241851807 mm for frame 1

Saving results

Total time: 39.63430666923523
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462092
Iteration 2/25 | Loss: 0.00136735
Iteration 3/25 | Loss: 0.00123464
Iteration 4/25 | Loss: 0.00120869
Iteration 5/25 | Loss: 0.00120011
Iteration 6/25 | Loss: 0.00119837
Iteration 7/25 | Loss: 0.00119837
Iteration 8/25 | Loss: 0.00119837
Iteration 9/25 | Loss: 0.00119837
Iteration 10/25 | Loss: 0.00119837
Iteration 11/25 | Loss: 0.00119837
Iteration 12/25 | Loss: 0.00119837
Iteration 13/25 | Loss: 0.00119837
Iteration 14/25 | Loss: 0.00119837
Iteration 15/25 | Loss: 0.00119837
Iteration 16/25 | Loss: 0.00119837
Iteration 17/25 | Loss: 0.00119837
Iteration 18/25 | Loss: 0.00119837
Iteration 19/25 | Loss: 0.00119837
Iteration 20/25 | Loss: 0.00119837
Iteration 21/25 | Loss: 0.00119837
Iteration 22/25 | Loss: 0.00119837
Iteration 23/25 | Loss: 0.00119837
Iteration 24/25 | Loss: 0.00119837
Iteration 25/25 | Loss: 0.00119837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21727383
Iteration 2/25 | Loss: 0.00197114
Iteration 3/25 | Loss: 0.00197114
Iteration 4/25 | Loss: 0.00197114
Iteration 5/25 | Loss: 0.00197114
Iteration 6/25 | Loss: 0.00197114
Iteration 7/25 | Loss: 0.00197114
Iteration 8/25 | Loss: 0.00197114
Iteration 9/25 | Loss: 0.00197114
Iteration 10/25 | Loss: 0.00197114
Iteration 11/25 | Loss: 0.00197114
Iteration 12/25 | Loss: 0.00197114
Iteration 13/25 | Loss: 0.00197114
Iteration 14/25 | Loss: 0.00197114
Iteration 15/25 | Loss: 0.00197114
Iteration 16/25 | Loss: 0.00197114
Iteration 17/25 | Loss: 0.00197114
Iteration 18/25 | Loss: 0.00197114
Iteration 19/25 | Loss: 0.00197114
Iteration 20/25 | Loss: 0.00197114
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019711379427462816, 0.0019711379427462816, 0.0019711379427462816, 0.0019711379427462816, 0.0019711379427462816]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019711379427462816

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197114
Iteration 2/1000 | Loss: 0.00006209
Iteration 3/1000 | Loss: 0.00003934
Iteration 4/1000 | Loss: 0.00002879
Iteration 5/1000 | Loss: 0.00002692
Iteration 6/1000 | Loss: 0.00002539
Iteration 7/1000 | Loss: 0.00002450
Iteration 8/1000 | Loss: 0.00002388
Iteration 9/1000 | Loss: 0.00002351
Iteration 10/1000 | Loss: 0.00002329
Iteration 11/1000 | Loss: 0.00002301
Iteration 12/1000 | Loss: 0.00002282
Iteration 13/1000 | Loss: 0.00002262
Iteration 14/1000 | Loss: 0.00002245
Iteration 15/1000 | Loss: 0.00002244
Iteration 16/1000 | Loss: 0.00002243
Iteration 17/1000 | Loss: 0.00002242
Iteration 18/1000 | Loss: 0.00002239
Iteration 19/1000 | Loss: 0.00002220
Iteration 20/1000 | Loss: 0.00002216
Iteration 21/1000 | Loss: 0.00002213
Iteration 22/1000 | Loss: 0.00002212
Iteration 23/1000 | Loss: 0.00002208
Iteration 24/1000 | Loss: 0.00002206
Iteration 25/1000 | Loss: 0.00002205
Iteration 26/1000 | Loss: 0.00002205
Iteration 27/1000 | Loss: 0.00002204
Iteration 28/1000 | Loss: 0.00002203
Iteration 29/1000 | Loss: 0.00002203
Iteration 30/1000 | Loss: 0.00002202
Iteration 31/1000 | Loss: 0.00002201
Iteration 32/1000 | Loss: 0.00002200
Iteration 33/1000 | Loss: 0.00002200
Iteration 34/1000 | Loss: 0.00002200
Iteration 35/1000 | Loss: 0.00002199
Iteration 36/1000 | Loss: 0.00002199
Iteration 37/1000 | Loss: 0.00002198
Iteration 38/1000 | Loss: 0.00002197
Iteration 39/1000 | Loss: 0.00002194
Iteration 40/1000 | Loss: 0.00002194
Iteration 41/1000 | Loss: 0.00002194
Iteration 42/1000 | Loss: 0.00002194
Iteration 43/1000 | Loss: 0.00002194
Iteration 44/1000 | Loss: 0.00002193
Iteration 45/1000 | Loss: 0.00002191
Iteration 46/1000 | Loss: 0.00002191
Iteration 47/1000 | Loss: 0.00002191
Iteration 48/1000 | Loss: 0.00002190
Iteration 49/1000 | Loss: 0.00002190
Iteration 50/1000 | Loss: 0.00002189
Iteration 51/1000 | Loss: 0.00002189
Iteration 52/1000 | Loss: 0.00002189
Iteration 53/1000 | Loss: 0.00002188
Iteration 54/1000 | Loss: 0.00002188
Iteration 55/1000 | Loss: 0.00002188
Iteration 56/1000 | Loss: 0.00002188
Iteration 57/1000 | Loss: 0.00002187
Iteration 58/1000 | Loss: 0.00002187
Iteration 59/1000 | Loss: 0.00002187
Iteration 60/1000 | Loss: 0.00002186
Iteration 61/1000 | Loss: 0.00002186
Iteration 62/1000 | Loss: 0.00002186
Iteration 63/1000 | Loss: 0.00002186
Iteration 64/1000 | Loss: 0.00002186
Iteration 65/1000 | Loss: 0.00002185
Iteration 66/1000 | Loss: 0.00002185
Iteration 67/1000 | Loss: 0.00002185
Iteration 68/1000 | Loss: 0.00002185
Iteration 69/1000 | Loss: 0.00002185
Iteration 70/1000 | Loss: 0.00002185
Iteration 71/1000 | Loss: 0.00002184
Iteration 72/1000 | Loss: 0.00002184
Iteration 73/1000 | Loss: 0.00002184
Iteration 74/1000 | Loss: 0.00002184
Iteration 75/1000 | Loss: 0.00002183
Iteration 76/1000 | Loss: 0.00002183
Iteration 77/1000 | Loss: 0.00002183
Iteration 78/1000 | Loss: 0.00002182
Iteration 79/1000 | Loss: 0.00002182
Iteration 80/1000 | Loss: 0.00002182
Iteration 81/1000 | Loss: 0.00002182
Iteration 82/1000 | Loss: 0.00002182
Iteration 83/1000 | Loss: 0.00002182
Iteration 84/1000 | Loss: 0.00002181
Iteration 85/1000 | Loss: 0.00002181
Iteration 86/1000 | Loss: 0.00002181
Iteration 87/1000 | Loss: 0.00002181
Iteration 88/1000 | Loss: 0.00002181
Iteration 89/1000 | Loss: 0.00002181
Iteration 90/1000 | Loss: 0.00002181
Iteration 91/1000 | Loss: 0.00002180
Iteration 92/1000 | Loss: 0.00002180
Iteration 93/1000 | Loss: 0.00002180
Iteration 94/1000 | Loss: 0.00002180
Iteration 95/1000 | Loss: 0.00002180
Iteration 96/1000 | Loss: 0.00002180
Iteration 97/1000 | Loss: 0.00002180
Iteration 98/1000 | Loss: 0.00002180
Iteration 99/1000 | Loss: 0.00002180
Iteration 100/1000 | Loss: 0.00002180
Iteration 101/1000 | Loss: 0.00002180
Iteration 102/1000 | Loss: 0.00002180
Iteration 103/1000 | Loss: 0.00002179
Iteration 104/1000 | Loss: 0.00002179
Iteration 105/1000 | Loss: 0.00002179
Iteration 106/1000 | Loss: 0.00002179
Iteration 107/1000 | Loss: 0.00002179
Iteration 108/1000 | Loss: 0.00002178
Iteration 109/1000 | Loss: 0.00002178
Iteration 110/1000 | Loss: 0.00002178
Iteration 111/1000 | Loss: 0.00002178
Iteration 112/1000 | Loss: 0.00002178
Iteration 113/1000 | Loss: 0.00002178
Iteration 114/1000 | Loss: 0.00002178
Iteration 115/1000 | Loss: 0.00002178
Iteration 116/1000 | Loss: 0.00002178
Iteration 117/1000 | Loss: 0.00002177
Iteration 118/1000 | Loss: 0.00002177
Iteration 119/1000 | Loss: 0.00002177
Iteration 120/1000 | Loss: 0.00002177
Iteration 121/1000 | Loss: 0.00002177
Iteration 122/1000 | Loss: 0.00002177
Iteration 123/1000 | Loss: 0.00002177
Iteration 124/1000 | Loss: 0.00002177
Iteration 125/1000 | Loss: 0.00002177
Iteration 126/1000 | Loss: 0.00002177
Iteration 127/1000 | Loss: 0.00002177
Iteration 128/1000 | Loss: 0.00002177
Iteration 129/1000 | Loss: 0.00002177
Iteration 130/1000 | Loss: 0.00002177
Iteration 131/1000 | Loss: 0.00002177
Iteration 132/1000 | Loss: 0.00002177
Iteration 133/1000 | Loss: 0.00002177
Iteration 134/1000 | Loss: 0.00002177
Iteration 135/1000 | Loss: 0.00002177
Iteration 136/1000 | Loss: 0.00002177
Iteration 137/1000 | Loss: 0.00002176
Iteration 138/1000 | Loss: 0.00002176
Iteration 139/1000 | Loss: 0.00002176
Iteration 140/1000 | Loss: 0.00002176
Iteration 141/1000 | Loss: 0.00002176
Iteration 142/1000 | Loss: 0.00002176
Iteration 143/1000 | Loss: 0.00002176
Iteration 144/1000 | Loss: 0.00002176
Iteration 145/1000 | Loss: 0.00002176
Iteration 146/1000 | Loss: 0.00002176
Iteration 147/1000 | Loss: 0.00002176
Iteration 148/1000 | Loss: 0.00002176
Iteration 149/1000 | Loss: 0.00002176
Iteration 150/1000 | Loss: 0.00002176
Iteration 151/1000 | Loss: 0.00002176
Iteration 152/1000 | Loss: 0.00002176
Iteration 153/1000 | Loss: 0.00002176
Iteration 154/1000 | Loss: 0.00002176
Iteration 155/1000 | Loss: 0.00002176
Iteration 156/1000 | Loss: 0.00002176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.1763516997452825e-05, 2.1763516997452825e-05, 2.1763516997452825e-05, 2.1763516997452825e-05, 2.1763516997452825e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1763516997452825e-05

Optimization complete. Final v2v error: 3.813076972961426 mm

Highest mean error: 4.423823356628418 mm for frame 175

Lowest mean error: 3.4684269428253174 mm for frame 166

Saving results

Total time: 43.94194293022156
