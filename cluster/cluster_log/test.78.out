Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=78, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 4368-4423
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034094
Iteration 2/25 | Loss: 0.00320233
Iteration 3/25 | Loss: 0.00227900
Iteration 4/25 | Loss: 0.00179312
Iteration 5/25 | Loss: 0.00168806
Iteration 6/25 | Loss: 0.00175420
Iteration 7/25 | Loss: 0.00158057
Iteration 8/25 | Loss: 0.00143707
Iteration 9/25 | Loss: 0.00140458
Iteration 10/25 | Loss: 0.00133256
Iteration 11/25 | Loss: 0.00130565
Iteration 12/25 | Loss: 0.00127319
Iteration 13/25 | Loss: 0.00123390
Iteration 14/25 | Loss: 0.00121942
Iteration 15/25 | Loss: 0.00122045
Iteration 16/25 | Loss: 0.00120599
Iteration 17/25 | Loss: 0.00117974
Iteration 18/25 | Loss: 0.00116946
Iteration 19/25 | Loss: 0.00117173
Iteration 20/25 | Loss: 0.00116899
Iteration 21/25 | Loss: 0.00116613
Iteration 22/25 | Loss: 0.00116110
Iteration 23/25 | Loss: 0.00116036
Iteration 24/25 | Loss: 0.00116420
Iteration 25/25 | Loss: 0.00116113

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.48802257
Iteration 2/25 | Loss: 0.00651583
Iteration 3/25 | Loss: 0.00446546
Iteration 4/25 | Loss: 0.00446546
Iteration 5/25 | Loss: 0.00446546
Iteration 6/25 | Loss: 0.00446546
Iteration 7/25 | Loss: 0.00446546
Iteration 8/25 | Loss: 0.00446546
Iteration 9/25 | Loss: 0.00446546
Iteration 10/25 | Loss: 0.00446546
Iteration 11/25 | Loss: 0.00446546
Iteration 12/25 | Loss: 0.00446546
Iteration 13/25 | Loss: 0.00446546
Iteration 14/25 | Loss: 0.00446546
Iteration 15/25 | Loss: 0.00446546
Iteration 16/25 | Loss: 0.00446546
Iteration 17/25 | Loss: 0.00446546
Iteration 18/25 | Loss: 0.00446546
Iteration 19/25 | Loss: 0.00446546
Iteration 20/25 | Loss: 0.00446546
Iteration 21/25 | Loss: 0.00446546
Iteration 22/25 | Loss: 0.00446546
Iteration 23/25 | Loss: 0.00446546
Iteration 24/25 | Loss: 0.00446546
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.004465456586331129, 0.004465456586331129, 0.004465456586331129, 0.004465456586331129, 0.004465456586331129]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004465456586331129

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00446546
Iteration 2/1000 | Loss: 0.00083224
Iteration 3/1000 | Loss: 0.00567521
Iteration 4/1000 | Loss: 0.00645909
Iteration 5/1000 | Loss: 0.00390698
Iteration 6/1000 | Loss: 0.00195764
Iteration 7/1000 | Loss: 0.01105376
Iteration 8/1000 | Loss: 0.00359455
Iteration 9/1000 | Loss: 0.00266109
Iteration 10/1000 | Loss: 0.00188042
Iteration 11/1000 | Loss: 0.00026973
Iteration 12/1000 | Loss: 0.00102265
Iteration 13/1000 | Loss: 0.00138246
Iteration 14/1000 | Loss: 0.00017498
Iteration 15/1000 | Loss: 0.00017747
Iteration 16/1000 | Loss: 0.00015413
Iteration 17/1000 | Loss: 0.00041269
Iteration 18/1000 | Loss: 0.00026317
Iteration 19/1000 | Loss: 0.00008596
Iteration 20/1000 | Loss: 0.00010317
Iteration 21/1000 | Loss: 0.00006810
Iteration 22/1000 | Loss: 0.00032886
Iteration 23/1000 | Loss: 0.00005285
Iteration 24/1000 | Loss: 0.00004843
Iteration 25/1000 | Loss: 0.00036302
Iteration 26/1000 | Loss: 0.00044470
Iteration 27/1000 | Loss: 0.00006042
Iteration 28/1000 | Loss: 0.00005015
Iteration 29/1000 | Loss: 0.00004691
Iteration 30/1000 | Loss: 0.00004405
Iteration 31/1000 | Loss: 0.00004263
Iteration 32/1000 | Loss: 0.00004148
Iteration 33/1000 | Loss: 0.00003975
Iteration 34/1000 | Loss: 0.00003767
Iteration 35/1000 | Loss: 0.00003583
Iteration 36/1000 | Loss: 0.00003494
Iteration 37/1000 | Loss: 0.00003440
Iteration 38/1000 | Loss: 0.00003389
Iteration 39/1000 | Loss: 0.00003338
Iteration 40/1000 | Loss: 0.00003281
Iteration 41/1000 | Loss: 0.00033713
Iteration 42/1000 | Loss: 0.00003521
Iteration 43/1000 | Loss: 0.00003250
Iteration 44/1000 | Loss: 0.00003146
Iteration 45/1000 | Loss: 0.00003064
Iteration 46/1000 | Loss: 0.00003009
Iteration 47/1000 | Loss: 0.00002971
Iteration 48/1000 | Loss: 0.00002946
Iteration 49/1000 | Loss: 0.00002939
Iteration 50/1000 | Loss: 0.00002936
Iteration 51/1000 | Loss: 0.00002927
Iteration 52/1000 | Loss: 0.00002922
Iteration 53/1000 | Loss: 0.00002920
Iteration 54/1000 | Loss: 0.00002919
Iteration 55/1000 | Loss: 0.00002918
Iteration 56/1000 | Loss: 0.00002918
Iteration 57/1000 | Loss: 0.00002918
Iteration 58/1000 | Loss: 0.00002918
Iteration 59/1000 | Loss: 0.00002918
Iteration 60/1000 | Loss: 0.00002918
Iteration 61/1000 | Loss: 0.00002918
Iteration 62/1000 | Loss: 0.00002918
Iteration 63/1000 | Loss: 0.00002918
Iteration 64/1000 | Loss: 0.00002918
Iteration 65/1000 | Loss: 0.00002918
Iteration 66/1000 | Loss: 0.00002917
Iteration 67/1000 | Loss: 0.00002916
Iteration 68/1000 | Loss: 0.00002916
Iteration 69/1000 | Loss: 0.00002916
Iteration 70/1000 | Loss: 0.00002916
Iteration 71/1000 | Loss: 0.00002916
Iteration 72/1000 | Loss: 0.00002916
Iteration 73/1000 | Loss: 0.00002915
Iteration 74/1000 | Loss: 0.00002915
Iteration 75/1000 | Loss: 0.00002915
Iteration 76/1000 | Loss: 0.00002915
Iteration 77/1000 | Loss: 0.00002915
Iteration 78/1000 | Loss: 0.00002915
Iteration 79/1000 | Loss: 0.00002915
Iteration 80/1000 | Loss: 0.00002914
Iteration 81/1000 | Loss: 0.00002914
Iteration 82/1000 | Loss: 0.00002914
Iteration 83/1000 | Loss: 0.00002913
Iteration 84/1000 | Loss: 0.00002913
Iteration 85/1000 | Loss: 0.00002913
Iteration 86/1000 | Loss: 0.00002913
Iteration 87/1000 | Loss: 0.00002913
Iteration 88/1000 | Loss: 0.00002912
Iteration 89/1000 | Loss: 0.00002912
Iteration 90/1000 | Loss: 0.00002912
Iteration 91/1000 | Loss: 0.00002912
Iteration 92/1000 | Loss: 0.00002912
Iteration 93/1000 | Loss: 0.00002912
Iteration 94/1000 | Loss: 0.00002912
Iteration 95/1000 | Loss: 0.00002912
Iteration 96/1000 | Loss: 0.00002912
Iteration 97/1000 | Loss: 0.00002911
Iteration 98/1000 | Loss: 0.00002911
Iteration 99/1000 | Loss: 0.00002910
Iteration 100/1000 | Loss: 0.00002910
Iteration 101/1000 | Loss: 0.00002910
Iteration 102/1000 | Loss: 0.00002910
Iteration 103/1000 | Loss: 0.00002910
Iteration 104/1000 | Loss: 0.00002910
Iteration 105/1000 | Loss: 0.00002910
Iteration 106/1000 | Loss: 0.00002909
Iteration 107/1000 | Loss: 0.00002909
Iteration 108/1000 | Loss: 0.00002909
Iteration 109/1000 | Loss: 0.00002909
Iteration 110/1000 | Loss: 0.00002909
Iteration 111/1000 | Loss: 0.00002909
Iteration 112/1000 | Loss: 0.00002908
Iteration 113/1000 | Loss: 0.00002908
Iteration 114/1000 | Loss: 0.00002908
Iteration 115/1000 | Loss: 0.00002908
Iteration 116/1000 | Loss: 0.00002907
Iteration 117/1000 | Loss: 0.00002907
Iteration 118/1000 | Loss: 0.00002907
Iteration 119/1000 | Loss: 0.00002907
Iteration 120/1000 | Loss: 0.00002907
Iteration 121/1000 | Loss: 0.00002907
Iteration 122/1000 | Loss: 0.00002906
Iteration 123/1000 | Loss: 0.00002906
Iteration 124/1000 | Loss: 0.00002906
Iteration 125/1000 | Loss: 0.00002906
Iteration 126/1000 | Loss: 0.00002906
Iteration 127/1000 | Loss: 0.00002906
Iteration 128/1000 | Loss: 0.00002906
Iteration 129/1000 | Loss: 0.00002906
Iteration 130/1000 | Loss: 0.00002906
Iteration 131/1000 | Loss: 0.00002906
Iteration 132/1000 | Loss: 0.00002905
Iteration 133/1000 | Loss: 0.00002905
Iteration 134/1000 | Loss: 0.00002905
Iteration 135/1000 | Loss: 0.00002905
Iteration 136/1000 | Loss: 0.00002905
Iteration 137/1000 | Loss: 0.00002905
Iteration 138/1000 | Loss: 0.00002905
Iteration 139/1000 | Loss: 0.00002905
Iteration 140/1000 | Loss: 0.00002905
Iteration 141/1000 | Loss: 0.00002905
Iteration 142/1000 | Loss: 0.00002905
Iteration 143/1000 | Loss: 0.00002905
Iteration 144/1000 | Loss: 0.00002905
Iteration 145/1000 | Loss: 0.00002905
Iteration 146/1000 | Loss: 0.00002905
Iteration 147/1000 | Loss: 0.00002904
Iteration 148/1000 | Loss: 0.00002904
Iteration 149/1000 | Loss: 0.00002904
Iteration 150/1000 | Loss: 0.00002904
Iteration 151/1000 | Loss: 0.00002904
Iteration 152/1000 | Loss: 0.00002904
Iteration 153/1000 | Loss: 0.00002904
Iteration 154/1000 | Loss: 0.00002904
Iteration 155/1000 | Loss: 0.00002903
Iteration 156/1000 | Loss: 0.00002903
Iteration 157/1000 | Loss: 0.00002903
Iteration 158/1000 | Loss: 0.00002903
Iteration 159/1000 | Loss: 0.00002902
Iteration 160/1000 | Loss: 0.00002902
Iteration 161/1000 | Loss: 0.00002902
Iteration 162/1000 | Loss: 0.00002902
Iteration 163/1000 | Loss: 0.00002902
Iteration 164/1000 | Loss: 0.00002901
Iteration 165/1000 | Loss: 0.00002901
Iteration 166/1000 | Loss: 0.00002901
Iteration 167/1000 | Loss: 0.00002901
Iteration 168/1000 | Loss: 0.00002901
Iteration 169/1000 | Loss: 0.00002900
Iteration 170/1000 | Loss: 0.00002900
Iteration 171/1000 | Loss: 0.00002900
Iteration 172/1000 | Loss: 0.00002900
Iteration 173/1000 | Loss: 0.00002899
Iteration 174/1000 | Loss: 0.00002899
Iteration 175/1000 | Loss: 0.00002899
Iteration 176/1000 | Loss: 0.00002899
Iteration 177/1000 | Loss: 0.00002899
Iteration 178/1000 | Loss: 0.00002899
Iteration 179/1000 | Loss: 0.00002899
Iteration 180/1000 | Loss: 0.00002898
Iteration 181/1000 | Loss: 0.00002898
Iteration 182/1000 | Loss: 0.00002898
Iteration 183/1000 | Loss: 0.00002898
Iteration 184/1000 | Loss: 0.00002898
Iteration 185/1000 | Loss: 0.00002898
Iteration 186/1000 | Loss: 0.00002897
Iteration 187/1000 | Loss: 0.00002897
Iteration 188/1000 | Loss: 0.00002897
Iteration 189/1000 | Loss: 0.00002897
Iteration 190/1000 | Loss: 0.00002897
Iteration 191/1000 | Loss: 0.00002897
Iteration 192/1000 | Loss: 0.00002896
Iteration 193/1000 | Loss: 0.00002896
Iteration 194/1000 | Loss: 0.00002896
Iteration 195/1000 | Loss: 0.00002896
Iteration 196/1000 | Loss: 0.00002896
Iteration 197/1000 | Loss: 0.00002896
Iteration 198/1000 | Loss: 0.00002896
Iteration 199/1000 | Loss: 0.00002895
Iteration 200/1000 | Loss: 0.00002895
Iteration 201/1000 | Loss: 0.00002895
Iteration 202/1000 | Loss: 0.00002895
Iteration 203/1000 | Loss: 0.00002895
Iteration 204/1000 | Loss: 0.00002895
Iteration 205/1000 | Loss: 0.00002895
Iteration 206/1000 | Loss: 0.00002895
Iteration 207/1000 | Loss: 0.00002894
Iteration 208/1000 | Loss: 0.00002894
Iteration 209/1000 | Loss: 0.00002893
Iteration 210/1000 | Loss: 0.00002893
Iteration 211/1000 | Loss: 0.00002893
Iteration 212/1000 | Loss: 0.00002893
Iteration 213/1000 | Loss: 0.00002893
Iteration 214/1000 | Loss: 0.00002893
Iteration 215/1000 | Loss: 0.00002893
Iteration 216/1000 | Loss: 0.00002892
Iteration 217/1000 | Loss: 0.00002892
Iteration 218/1000 | Loss: 0.00002892
Iteration 219/1000 | Loss: 0.00002891
Iteration 220/1000 | Loss: 0.00002891
Iteration 221/1000 | Loss: 0.00002891
Iteration 222/1000 | Loss: 0.00002891
Iteration 223/1000 | Loss: 0.00002891
Iteration 224/1000 | Loss: 0.00002891
Iteration 225/1000 | Loss: 0.00002891
Iteration 226/1000 | Loss: 0.00002891
Iteration 227/1000 | Loss: 0.00002891
Iteration 228/1000 | Loss: 0.00002890
Iteration 229/1000 | Loss: 0.00002890
Iteration 230/1000 | Loss: 0.00002890
Iteration 231/1000 | Loss: 0.00002890
Iteration 232/1000 | Loss: 0.00002890
Iteration 233/1000 | Loss: 0.00002890
Iteration 234/1000 | Loss: 0.00002890
Iteration 235/1000 | Loss: 0.00002890
Iteration 236/1000 | Loss: 0.00002889
Iteration 237/1000 | Loss: 0.00002889
Iteration 238/1000 | Loss: 0.00002889
Iteration 239/1000 | Loss: 0.00002889
Iteration 240/1000 | Loss: 0.00002889
Iteration 241/1000 | Loss: 0.00002889
Iteration 242/1000 | Loss: 0.00002889
Iteration 243/1000 | Loss: 0.00002889
Iteration 244/1000 | Loss: 0.00002889
Iteration 245/1000 | Loss: 0.00002889
Iteration 246/1000 | Loss: 0.00002889
Iteration 247/1000 | Loss: 0.00002889
Iteration 248/1000 | Loss: 0.00002888
Iteration 249/1000 | Loss: 0.00002888
Iteration 250/1000 | Loss: 0.00002888
Iteration 251/1000 | Loss: 0.00002888
Iteration 252/1000 | Loss: 0.00002888
Iteration 253/1000 | Loss: 0.00002888
Iteration 254/1000 | Loss: 0.00002888
Iteration 255/1000 | Loss: 0.00002888
Iteration 256/1000 | Loss: 0.00002888
Iteration 257/1000 | Loss: 0.00002888
Iteration 258/1000 | Loss: 0.00002888
Iteration 259/1000 | Loss: 0.00002888
Iteration 260/1000 | Loss: 0.00002888
Iteration 261/1000 | Loss: 0.00002888
Iteration 262/1000 | Loss: 0.00002888
Iteration 263/1000 | Loss: 0.00002888
Iteration 264/1000 | Loss: 0.00002888
Iteration 265/1000 | Loss: 0.00002888
Iteration 266/1000 | Loss: 0.00002887
Iteration 267/1000 | Loss: 0.00002887
Iteration 268/1000 | Loss: 0.00002887
Iteration 269/1000 | Loss: 0.00002887
Iteration 270/1000 | Loss: 0.00002887
Iteration 271/1000 | Loss: 0.00002887
Iteration 272/1000 | Loss: 0.00002887
Iteration 273/1000 | Loss: 0.00002887
Iteration 274/1000 | Loss: 0.00002887
Iteration 275/1000 | Loss: 0.00002887
Iteration 276/1000 | Loss: 0.00002887
Iteration 277/1000 | Loss: 0.00002887
Iteration 278/1000 | Loss: 0.00002887
Iteration 279/1000 | Loss: 0.00002887
Iteration 280/1000 | Loss: 0.00002887
Iteration 281/1000 | Loss: 0.00002886
Iteration 282/1000 | Loss: 0.00002886
Iteration 283/1000 | Loss: 0.00002886
Iteration 284/1000 | Loss: 0.00002886
Iteration 285/1000 | Loss: 0.00002886
Iteration 286/1000 | Loss: 0.00002886
Iteration 287/1000 | Loss: 0.00002886
Iteration 288/1000 | Loss: 0.00002886
Iteration 289/1000 | Loss: 0.00002886
Iteration 290/1000 | Loss: 0.00002886
Iteration 291/1000 | Loss: 0.00002886
Iteration 292/1000 | Loss: 0.00002886
Iteration 293/1000 | Loss: 0.00002886
Iteration 294/1000 | Loss: 0.00002886
Iteration 295/1000 | Loss: 0.00002886
Iteration 296/1000 | Loss: 0.00002886
Iteration 297/1000 | Loss: 0.00002886
Iteration 298/1000 | Loss: 0.00002886
Iteration 299/1000 | Loss: 0.00002886
Iteration 300/1000 | Loss: 0.00002886
Iteration 301/1000 | Loss: 0.00002886
Iteration 302/1000 | Loss: 0.00002886
Iteration 303/1000 | Loss: 0.00002886
Iteration 304/1000 | Loss: 0.00002886
Iteration 305/1000 | Loss: 0.00002886
Iteration 306/1000 | Loss: 0.00002886
Iteration 307/1000 | Loss: 0.00002886
Iteration 308/1000 | Loss: 0.00002886
Iteration 309/1000 | Loss: 0.00002886
Iteration 310/1000 | Loss: 0.00002886
Iteration 311/1000 | Loss: 0.00002886
Iteration 312/1000 | Loss: 0.00002886
Iteration 313/1000 | Loss: 0.00002886
Iteration 314/1000 | Loss: 0.00002886
Iteration 315/1000 | Loss: 0.00002886
Iteration 316/1000 | Loss: 0.00002886
Iteration 317/1000 | Loss: 0.00002886
Iteration 318/1000 | Loss: 0.00002886
Iteration 319/1000 | Loss: 0.00002886
Iteration 320/1000 | Loss: 0.00002886
Iteration 321/1000 | Loss: 0.00002886
Iteration 322/1000 | Loss: 0.00002886
Iteration 323/1000 | Loss: 0.00002886
Iteration 324/1000 | Loss: 0.00002886
Iteration 325/1000 | Loss: 0.00002886
Iteration 326/1000 | Loss: 0.00002886
Iteration 327/1000 | Loss: 0.00002886
Iteration 328/1000 | Loss: 0.00002886
Iteration 329/1000 | Loss: 0.00002886
Iteration 330/1000 | Loss: 0.00002886
Iteration 331/1000 | Loss: 0.00002886
Iteration 332/1000 | Loss: 0.00002886
Iteration 333/1000 | Loss: 0.00002886
Iteration 334/1000 | Loss: 0.00002886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 334. Stopping optimization.
Last 5 losses: [2.8855127311544493e-05, 2.8855127311544493e-05, 2.8855127311544493e-05, 2.8855127311544493e-05, 2.8855127311544493e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8855127311544493e-05

Optimization complete. Final v2v error: 4.081996440887451 mm

Highest mean error: 13.822372436523438 mm for frame 45

Lowest mean error: 3.1351726055145264 mm for frame 141

Saving results

Total time: 143.85137581825256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791474
Iteration 2/25 | Loss: 0.00214998
Iteration 3/25 | Loss: 0.00107827
Iteration 4/25 | Loss: 0.00085000
Iteration 5/25 | Loss: 0.00075234
Iteration 6/25 | Loss: 0.00069747
Iteration 7/25 | Loss: 0.00069469
Iteration 8/25 | Loss: 0.00066635
Iteration 9/25 | Loss: 0.00064447
Iteration 10/25 | Loss: 0.00064043
Iteration 11/25 | Loss: 0.00062729
Iteration 12/25 | Loss: 0.00062091
Iteration 13/25 | Loss: 0.00061684
Iteration 14/25 | Loss: 0.00061611
Iteration 15/25 | Loss: 0.00061317
Iteration 16/25 | Loss: 0.00061664
Iteration 17/25 | Loss: 0.00061211
Iteration 18/25 | Loss: 0.00061111
Iteration 19/25 | Loss: 0.00060859
Iteration 20/25 | Loss: 0.00060974
Iteration 21/25 | Loss: 0.00060800
Iteration 22/25 | Loss: 0.00060797
Iteration 23/25 | Loss: 0.00060797
Iteration 24/25 | Loss: 0.00060796
Iteration 25/25 | Loss: 0.00060796

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.12047172
Iteration 2/25 | Loss: 0.00029786
Iteration 3/25 | Loss: 0.00029786
Iteration 4/25 | Loss: 0.00029786
Iteration 5/25 | Loss: 0.00029785
Iteration 6/25 | Loss: 0.00029785
Iteration 7/25 | Loss: 0.00029785
Iteration 8/25 | Loss: 0.00028174
Iteration 9/25 | Loss: 0.00028174
Iteration 10/25 | Loss: 0.00028174
Iteration 11/25 | Loss: 0.00028174
Iteration 12/25 | Loss: 0.00028174
Iteration 13/25 | Loss: 0.00028174
Iteration 14/25 | Loss: 0.00028174
Iteration 15/25 | Loss: 0.00028174
Iteration 16/25 | Loss: 0.00028174
Iteration 17/25 | Loss: 0.00028174
Iteration 18/25 | Loss: 0.00028174
Iteration 19/25 | Loss: 0.00028174
Iteration 20/25 | Loss: 0.00028174
Iteration 21/25 | Loss: 0.00028174
Iteration 22/25 | Loss: 0.00028174
Iteration 23/25 | Loss: 0.00028174
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002817382337525487, 0.0002817382337525487, 0.0002817382337525487, 0.0002817382337525487, 0.0002817382337525487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002817382337525487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028174
Iteration 2/1000 | Loss: 0.00002300
Iteration 3/1000 | Loss: 0.00002684
Iteration 4/1000 | Loss: 0.00008041
Iteration 5/1000 | Loss: 0.00002154
Iteration 6/1000 | Loss: 0.00001853
Iteration 7/1000 | Loss: 0.00001534
Iteration 8/1000 | Loss: 0.00001357
Iteration 9/1000 | Loss: 0.00001335
Iteration 10/1000 | Loss: 0.00001840
Iteration 11/1000 | Loss: 0.00001318
Iteration 12/1000 | Loss: 0.00002100
Iteration 13/1000 | Loss: 0.00001306
Iteration 14/1000 | Loss: 0.00001293
Iteration 15/1000 | Loss: 0.00001292
Iteration 16/1000 | Loss: 0.00001292
Iteration 17/1000 | Loss: 0.00001291
Iteration 18/1000 | Loss: 0.00001291
Iteration 19/1000 | Loss: 0.00001290
Iteration 20/1000 | Loss: 0.00001289
Iteration 21/1000 | Loss: 0.00001288
Iteration 22/1000 | Loss: 0.00001288
Iteration 23/1000 | Loss: 0.00001288
Iteration 24/1000 | Loss: 0.00001288
Iteration 25/1000 | Loss: 0.00001288
Iteration 26/1000 | Loss: 0.00001288
Iteration 27/1000 | Loss: 0.00001288
Iteration 28/1000 | Loss: 0.00001288
Iteration 29/1000 | Loss: 0.00001288
Iteration 30/1000 | Loss: 0.00001288
Iteration 31/1000 | Loss: 0.00001287
Iteration 32/1000 | Loss: 0.00001287
Iteration 33/1000 | Loss: 0.00001286
Iteration 34/1000 | Loss: 0.00001286
Iteration 35/1000 | Loss: 0.00001286
Iteration 36/1000 | Loss: 0.00001286
Iteration 37/1000 | Loss: 0.00001286
Iteration 38/1000 | Loss: 0.00001286
Iteration 39/1000 | Loss: 0.00001286
Iteration 40/1000 | Loss: 0.00001285
Iteration 41/1000 | Loss: 0.00003894
Iteration 42/1000 | Loss: 0.00001279
Iteration 43/1000 | Loss: 0.00001269
Iteration 44/1000 | Loss: 0.00001268
Iteration 45/1000 | Loss: 0.00001268
Iteration 46/1000 | Loss: 0.00001268
Iteration 47/1000 | Loss: 0.00001268
Iteration 48/1000 | Loss: 0.00001268
Iteration 49/1000 | Loss: 0.00001268
Iteration 50/1000 | Loss: 0.00001268
Iteration 51/1000 | Loss: 0.00001268
Iteration 52/1000 | Loss: 0.00001267
Iteration 53/1000 | Loss: 0.00001267
Iteration 54/1000 | Loss: 0.00001267
Iteration 55/1000 | Loss: 0.00001267
Iteration 56/1000 | Loss: 0.00001266
Iteration 57/1000 | Loss: 0.00001266
Iteration 58/1000 | Loss: 0.00001265
Iteration 59/1000 | Loss: 0.00001265
Iteration 60/1000 | Loss: 0.00002301
Iteration 61/1000 | Loss: 0.00001274
Iteration 62/1000 | Loss: 0.00001260
Iteration 63/1000 | Loss: 0.00001260
Iteration 64/1000 | Loss: 0.00001260
Iteration 65/1000 | Loss: 0.00001260
Iteration 66/1000 | Loss: 0.00001260
Iteration 67/1000 | Loss: 0.00001259
Iteration 68/1000 | Loss: 0.00001259
Iteration 69/1000 | Loss: 0.00001259
Iteration 70/1000 | Loss: 0.00001259
Iteration 71/1000 | Loss: 0.00001259
Iteration 72/1000 | Loss: 0.00001259
Iteration 73/1000 | Loss: 0.00001258
Iteration 74/1000 | Loss: 0.00001258
Iteration 75/1000 | Loss: 0.00001257
Iteration 76/1000 | Loss: 0.00001256
Iteration 77/1000 | Loss: 0.00001256
Iteration 78/1000 | Loss: 0.00001255
Iteration 79/1000 | Loss: 0.00001255
Iteration 80/1000 | Loss: 0.00001255
Iteration 81/1000 | Loss: 0.00001255
Iteration 82/1000 | Loss: 0.00001255
Iteration 83/1000 | Loss: 0.00001254
Iteration 84/1000 | Loss: 0.00001254
Iteration 85/1000 | Loss: 0.00001253
Iteration 86/1000 | Loss: 0.00001253
Iteration 87/1000 | Loss: 0.00001252
Iteration 88/1000 | Loss: 0.00001252
Iteration 89/1000 | Loss: 0.00001251
Iteration 90/1000 | Loss: 0.00001251
Iteration 91/1000 | Loss: 0.00001251
Iteration 92/1000 | Loss: 0.00001251
Iteration 93/1000 | Loss: 0.00001251
Iteration 94/1000 | Loss: 0.00001251
Iteration 95/1000 | Loss: 0.00001251
Iteration 96/1000 | Loss: 0.00001251
Iteration 97/1000 | Loss: 0.00001251
Iteration 98/1000 | Loss: 0.00001251
Iteration 99/1000 | Loss: 0.00001250
Iteration 100/1000 | Loss: 0.00001250
Iteration 101/1000 | Loss: 0.00001250
Iteration 102/1000 | Loss: 0.00001250
Iteration 103/1000 | Loss: 0.00001250
Iteration 104/1000 | Loss: 0.00001250
Iteration 105/1000 | Loss: 0.00001250
Iteration 106/1000 | Loss: 0.00001250
Iteration 107/1000 | Loss: 0.00001250
Iteration 108/1000 | Loss: 0.00001250
Iteration 109/1000 | Loss: 0.00001250
Iteration 110/1000 | Loss: 0.00001250
Iteration 111/1000 | Loss: 0.00001250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.2500695447670296e-05, 1.2500695447670296e-05, 1.2500695447670296e-05, 1.2500695447670296e-05, 1.2500695447670296e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2500695447670296e-05

Optimization complete. Final v2v error: 3.0077812671661377 mm

Highest mean error: 3.2321858406066895 mm for frame 60

Lowest mean error: 2.7962794303894043 mm for frame 148

Saving results

Total time: 71.8619315624237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016543
Iteration 2/25 | Loss: 0.00618425
Iteration 3/25 | Loss: 0.00312164
Iteration 4/25 | Loss: 0.00278823
Iteration 5/25 | Loss: 0.00218739
Iteration 6/25 | Loss: 0.00190926
Iteration 7/25 | Loss: 0.00172428
Iteration 8/25 | Loss: 0.00153405
Iteration 9/25 | Loss: 0.00149392
Iteration 10/25 | Loss: 0.00144830
Iteration 11/25 | Loss: 0.00145264
Iteration 12/25 | Loss: 0.00143539
Iteration 13/25 | Loss: 0.00138883
Iteration 14/25 | Loss: 0.00133208
Iteration 15/25 | Loss: 0.00130141
Iteration 16/25 | Loss: 0.00126108
Iteration 17/25 | Loss: 0.00132832
Iteration 18/25 | Loss: 0.00122525
Iteration 19/25 | Loss: 0.00117931
Iteration 20/25 | Loss: 0.00119016
Iteration 21/25 | Loss: 0.00117715
Iteration 22/25 | Loss: 0.00116190
Iteration 23/25 | Loss: 0.00116126
Iteration 24/25 | Loss: 0.00115804
Iteration 25/25 | Loss: 0.00115503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44357347
Iteration 2/25 | Loss: 0.00689028
Iteration 3/25 | Loss: 0.00248201
Iteration 4/25 | Loss: 0.00248201
Iteration 5/25 | Loss: 0.00248201
Iteration 6/25 | Loss: 0.00248201
Iteration 7/25 | Loss: 0.00254072
Iteration 8/25 | Loss: 0.00254072
Iteration 9/25 | Loss: 0.00254072
Iteration 10/25 | Loss: 0.00248211
Iteration 11/25 | Loss: 0.00248201
Iteration 12/25 | Loss: 0.00248201
Iteration 13/25 | Loss: 0.00248201
Iteration 14/25 | Loss: 0.00248201
Iteration 15/25 | Loss: 0.00248201
Iteration 16/25 | Loss: 0.00248201
Iteration 17/25 | Loss: 0.00248201
Iteration 18/25 | Loss: 0.00248200
Iteration 19/25 | Loss: 0.00248200
Iteration 20/25 | Loss: 0.00248200
Iteration 21/25 | Loss: 0.00248200
Iteration 22/25 | Loss: 0.00248200
Iteration 23/25 | Loss: 0.00248200
Iteration 24/25 | Loss: 0.00248200
Iteration 25/25 | Loss: 0.00248200

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00248200
Iteration 2/1000 | Loss: 0.00765217
Iteration 3/1000 | Loss: 0.00054821
Iteration 4/1000 | Loss: 0.00053783
Iteration 5/1000 | Loss: 0.00071671
Iteration 6/1000 | Loss: 0.00026178
Iteration 7/1000 | Loss: 0.00054442
Iteration 8/1000 | Loss: 0.00059061
Iteration 9/1000 | Loss: 0.00408546
Iteration 10/1000 | Loss: 0.00604534
Iteration 11/1000 | Loss: 0.00488415
Iteration 12/1000 | Loss: 0.00819228
Iteration 13/1000 | Loss: 0.00057848
Iteration 14/1000 | Loss: 0.00100708
Iteration 15/1000 | Loss: 0.00011732
Iteration 16/1000 | Loss: 0.00050898
Iteration 17/1000 | Loss: 0.00048244
Iteration 18/1000 | Loss: 0.00021866
Iteration 19/1000 | Loss: 0.00085185
Iteration 20/1000 | Loss: 0.00517498
Iteration 21/1000 | Loss: 0.00583115
Iteration 22/1000 | Loss: 0.00816146
Iteration 23/1000 | Loss: 0.00607226
Iteration 24/1000 | Loss: 0.00487208
Iteration 25/1000 | Loss: 0.00318083
Iteration 26/1000 | Loss: 0.00501145
Iteration 27/1000 | Loss: 0.00454396
Iteration 28/1000 | Loss: 0.00190309
Iteration 29/1000 | Loss: 0.00317279
Iteration 30/1000 | Loss: 0.00316431
Iteration 31/1000 | Loss: 0.00252263
Iteration 32/1000 | Loss: 0.00321341
Iteration 33/1000 | Loss: 0.00674847
Iteration 34/1000 | Loss: 0.00157580
Iteration 35/1000 | Loss: 0.00163259
Iteration 36/1000 | Loss: 0.00090165
Iteration 37/1000 | Loss: 0.00035229
Iteration 38/1000 | Loss: 0.00021232
Iteration 39/1000 | Loss: 0.00022371
Iteration 40/1000 | Loss: 0.00013235
Iteration 41/1000 | Loss: 0.00058333
Iteration 42/1000 | Loss: 0.00012898
Iteration 43/1000 | Loss: 0.00021757
Iteration 44/1000 | Loss: 0.00012710
Iteration 45/1000 | Loss: 0.00016590
Iteration 46/1000 | Loss: 0.00011056
Iteration 47/1000 | Loss: 0.00051130
Iteration 48/1000 | Loss: 0.00040914
Iteration 49/1000 | Loss: 0.00007111
Iteration 50/1000 | Loss: 0.00007629
Iteration 51/1000 | Loss: 0.00065000
Iteration 52/1000 | Loss: 0.00109826
Iteration 53/1000 | Loss: 0.00021543
Iteration 54/1000 | Loss: 0.00005398
Iteration 55/1000 | Loss: 0.00004841
Iteration 56/1000 | Loss: 0.00007302
Iteration 57/1000 | Loss: 0.00002531
Iteration 58/1000 | Loss: 0.00021029
Iteration 59/1000 | Loss: 0.00004689
Iteration 60/1000 | Loss: 0.00002573
Iteration 61/1000 | Loss: 0.00014330
Iteration 62/1000 | Loss: 0.00015206
Iteration 63/1000 | Loss: 0.00007259
Iteration 64/1000 | Loss: 0.00007242
Iteration 65/1000 | Loss: 0.00012271
Iteration 66/1000 | Loss: 0.00006031
Iteration 67/1000 | Loss: 0.00005348
Iteration 68/1000 | Loss: 0.00011288
Iteration 69/1000 | Loss: 0.00010420
Iteration 70/1000 | Loss: 0.00005429
Iteration 71/1000 | Loss: 0.00007742
Iteration 72/1000 | Loss: 0.00002989
Iteration 73/1000 | Loss: 0.00004842
Iteration 74/1000 | Loss: 0.00004123
Iteration 75/1000 | Loss: 0.00019800
Iteration 76/1000 | Loss: 0.00009619
Iteration 77/1000 | Loss: 0.00005878
Iteration 78/1000 | Loss: 0.00004295
Iteration 79/1000 | Loss: 0.00004806
Iteration 80/1000 | Loss: 0.00009882
Iteration 81/1000 | Loss: 0.00006465
Iteration 82/1000 | Loss: 0.00009186
Iteration 83/1000 | Loss: 0.00005303
Iteration 84/1000 | Loss: 0.00004922
Iteration 85/1000 | Loss: 0.00006117
Iteration 86/1000 | Loss: 0.00012624
Iteration 87/1000 | Loss: 0.00004248
Iteration 88/1000 | Loss: 0.00005942
Iteration 89/1000 | Loss: 0.00012807
Iteration 90/1000 | Loss: 0.00025875
Iteration 91/1000 | Loss: 0.00010929
Iteration 92/1000 | Loss: 0.00007361
Iteration 93/1000 | Loss: 0.00018493
Iteration 94/1000 | Loss: 0.00006926
Iteration 95/1000 | Loss: 0.00016717
Iteration 96/1000 | Loss: 0.00001731
Iteration 97/1000 | Loss: 0.00007725
Iteration 98/1000 | Loss: 0.00001320
Iteration 99/1000 | Loss: 0.00006907
Iteration 100/1000 | Loss: 0.00001347
Iteration 101/1000 | Loss: 0.00001236
Iteration 102/1000 | Loss: 0.00001212
Iteration 103/1000 | Loss: 0.00001209
Iteration 104/1000 | Loss: 0.00001193
Iteration 105/1000 | Loss: 0.00001192
Iteration 106/1000 | Loss: 0.00001183
Iteration 107/1000 | Loss: 0.00001179
Iteration 108/1000 | Loss: 0.00001179
Iteration 109/1000 | Loss: 0.00001178
Iteration 110/1000 | Loss: 0.00001177
Iteration 111/1000 | Loss: 0.00001176
Iteration 112/1000 | Loss: 0.00001176
Iteration 113/1000 | Loss: 0.00001175
Iteration 114/1000 | Loss: 0.00001175
Iteration 115/1000 | Loss: 0.00001174
Iteration 116/1000 | Loss: 0.00001173
Iteration 117/1000 | Loss: 0.00001173
Iteration 118/1000 | Loss: 0.00001172
Iteration 119/1000 | Loss: 0.00001172
Iteration 120/1000 | Loss: 0.00001171
Iteration 121/1000 | Loss: 0.00001170
Iteration 122/1000 | Loss: 0.00001170
Iteration 123/1000 | Loss: 0.00001170
Iteration 124/1000 | Loss: 0.00001170
Iteration 125/1000 | Loss: 0.00001169
Iteration 126/1000 | Loss: 0.00001169
Iteration 127/1000 | Loss: 0.00001168
Iteration 128/1000 | Loss: 0.00001167
Iteration 129/1000 | Loss: 0.00001167
Iteration 130/1000 | Loss: 0.00001166
Iteration 131/1000 | Loss: 0.00001166
Iteration 132/1000 | Loss: 0.00001166
Iteration 133/1000 | Loss: 0.00001165
Iteration 134/1000 | Loss: 0.00001165
Iteration 135/1000 | Loss: 0.00001165
Iteration 136/1000 | Loss: 0.00001165
Iteration 137/1000 | Loss: 0.00001164
Iteration 138/1000 | Loss: 0.00001164
Iteration 139/1000 | Loss: 0.00001164
Iteration 140/1000 | Loss: 0.00001163
Iteration 141/1000 | Loss: 0.00001163
Iteration 142/1000 | Loss: 0.00001163
Iteration 143/1000 | Loss: 0.00001163
Iteration 144/1000 | Loss: 0.00001162
Iteration 145/1000 | Loss: 0.00001162
Iteration 146/1000 | Loss: 0.00001162
Iteration 147/1000 | Loss: 0.00001162
Iteration 148/1000 | Loss: 0.00001162
Iteration 149/1000 | Loss: 0.00001161
Iteration 150/1000 | Loss: 0.00001161
Iteration 151/1000 | Loss: 0.00001161
Iteration 152/1000 | Loss: 0.00001161
Iteration 153/1000 | Loss: 0.00001161
Iteration 154/1000 | Loss: 0.00001160
Iteration 155/1000 | Loss: 0.00001160
Iteration 156/1000 | Loss: 0.00001160
Iteration 157/1000 | Loss: 0.00001160
Iteration 158/1000 | Loss: 0.00001160
Iteration 159/1000 | Loss: 0.00001160
Iteration 160/1000 | Loss: 0.00001160
Iteration 161/1000 | Loss: 0.00001160
Iteration 162/1000 | Loss: 0.00001160
Iteration 163/1000 | Loss: 0.00001160
Iteration 164/1000 | Loss: 0.00001160
Iteration 165/1000 | Loss: 0.00001160
Iteration 166/1000 | Loss: 0.00001160
Iteration 167/1000 | Loss: 0.00001160
Iteration 168/1000 | Loss: 0.00001160
Iteration 169/1000 | Loss: 0.00001160
Iteration 170/1000 | Loss: 0.00001160
Iteration 171/1000 | Loss: 0.00001160
Iteration 172/1000 | Loss: 0.00001160
Iteration 173/1000 | Loss: 0.00001160
Iteration 174/1000 | Loss: 0.00001160
Iteration 175/1000 | Loss: 0.00001160
Iteration 176/1000 | Loss: 0.00001160
Iteration 177/1000 | Loss: 0.00001160
Iteration 178/1000 | Loss: 0.00001160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.1602483937167563e-05, 1.1602483937167563e-05, 1.1602483937167563e-05, 1.1602483937167563e-05, 1.1602483937167563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1602483937167563e-05

Optimization complete. Final v2v error: 2.8732550144195557 mm

Highest mean error: 4.062542915344238 mm for frame 92

Lowest mean error: 2.713196277618408 mm for frame 103

Saving results

Total time: 192.86758995056152
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406352
Iteration 2/25 | Loss: 0.00074652
Iteration 3/25 | Loss: 0.00062769
Iteration 4/25 | Loss: 0.00060733
Iteration 5/25 | Loss: 0.00059978
Iteration 6/25 | Loss: 0.00059823
Iteration 7/25 | Loss: 0.00059799
Iteration 8/25 | Loss: 0.00059799
Iteration 9/25 | Loss: 0.00059799
Iteration 10/25 | Loss: 0.00059799
Iteration 11/25 | Loss: 0.00059799
Iteration 12/25 | Loss: 0.00059799
Iteration 13/25 | Loss: 0.00059799
Iteration 14/25 | Loss: 0.00059799
Iteration 15/25 | Loss: 0.00059799
Iteration 16/25 | Loss: 0.00059799
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005979904090054333, 0.0005979904090054333, 0.0005979904090054333, 0.0005979904090054333, 0.0005979904090054333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005979904090054333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.85900450
Iteration 2/25 | Loss: 0.00028029
Iteration 3/25 | Loss: 0.00028029
Iteration 4/25 | Loss: 0.00028029
Iteration 5/25 | Loss: 0.00028029
Iteration 6/25 | Loss: 0.00028029
Iteration 7/25 | Loss: 0.00028029
Iteration 8/25 | Loss: 0.00028029
Iteration 9/25 | Loss: 0.00028029
Iteration 10/25 | Loss: 0.00028029
Iteration 11/25 | Loss: 0.00028029
Iteration 12/25 | Loss: 0.00028029
Iteration 13/25 | Loss: 0.00028028
Iteration 14/25 | Loss: 0.00028028
Iteration 15/25 | Loss: 0.00028029
Iteration 16/25 | Loss: 0.00028028
Iteration 17/25 | Loss: 0.00028029
Iteration 18/25 | Loss: 0.00028029
Iteration 19/25 | Loss: 0.00028029
Iteration 20/25 | Loss: 0.00028029
Iteration 21/25 | Loss: 0.00028029
Iteration 22/25 | Loss: 0.00028028
Iteration 23/25 | Loss: 0.00028028
Iteration 24/25 | Loss: 0.00028028
Iteration 25/25 | Loss: 0.00028028

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028028
Iteration 2/1000 | Loss: 0.00002197
Iteration 3/1000 | Loss: 0.00001534
Iteration 4/1000 | Loss: 0.00001452
Iteration 5/1000 | Loss: 0.00001369
Iteration 6/1000 | Loss: 0.00001331
Iteration 7/1000 | Loss: 0.00001308
Iteration 8/1000 | Loss: 0.00001291
Iteration 9/1000 | Loss: 0.00001284
Iteration 10/1000 | Loss: 0.00001278
Iteration 11/1000 | Loss: 0.00001277
Iteration 12/1000 | Loss: 0.00001273
Iteration 13/1000 | Loss: 0.00001273
Iteration 14/1000 | Loss: 0.00001272
Iteration 15/1000 | Loss: 0.00001272
Iteration 16/1000 | Loss: 0.00001271
Iteration 17/1000 | Loss: 0.00001265
Iteration 18/1000 | Loss: 0.00001262
Iteration 19/1000 | Loss: 0.00001262
Iteration 20/1000 | Loss: 0.00001262
Iteration 21/1000 | Loss: 0.00001262
Iteration 22/1000 | Loss: 0.00001262
Iteration 23/1000 | Loss: 0.00001261
Iteration 24/1000 | Loss: 0.00001261
Iteration 25/1000 | Loss: 0.00001256
Iteration 26/1000 | Loss: 0.00001254
Iteration 27/1000 | Loss: 0.00001251
Iteration 28/1000 | Loss: 0.00001250
Iteration 29/1000 | Loss: 0.00001250
Iteration 30/1000 | Loss: 0.00001249
Iteration 31/1000 | Loss: 0.00001248
Iteration 32/1000 | Loss: 0.00001248
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001245
Iteration 36/1000 | Loss: 0.00001244
Iteration 37/1000 | Loss: 0.00001244
Iteration 38/1000 | Loss: 0.00001244
Iteration 39/1000 | Loss: 0.00001243
Iteration 40/1000 | Loss: 0.00001243
Iteration 41/1000 | Loss: 0.00001243
Iteration 42/1000 | Loss: 0.00001242
Iteration 43/1000 | Loss: 0.00001242
Iteration 44/1000 | Loss: 0.00001242
Iteration 45/1000 | Loss: 0.00001242
Iteration 46/1000 | Loss: 0.00001242
Iteration 47/1000 | Loss: 0.00001241
Iteration 48/1000 | Loss: 0.00001241
Iteration 49/1000 | Loss: 0.00001241
Iteration 50/1000 | Loss: 0.00001241
Iteration 51/1000 | Loss: 0.00001240
Iteration 52/1000 | Loss: 0.00001240
Iteration 53/1000 | Loss: 0.00001239
Iteration 54/1000 | Loss: 0.00001239
Iteration 55/1000 | Loss: 0.00001239
Iteration 56/1000 | Loss: 0.00001239
Iteration 57/1000 | Loss: 0.00001239
Iteration 58/1000 | Loss: 0.00001239
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001238
Iteration 61/1000 | Loss: 0.00001238
Iteration 62/1000 | Loss: 0.00001238
Iteration 63/1000 | Loss: 0.00001238
Iteration 64/1000 | Loss: 0.00001238
Iteration 65/1000 | Loss: 0.00001238
Iteration 66/1000 | Loss: 0.00001238
Iteration 67/1000 | Loss: 0.00001238
Iteration 68/1000 | Loss: 0.00001238
Iteration 69/1000 | Loss: 0.00001238
Iteration 70/1000 | Loss: 0.00001233
Iteration 71/1000 | Loss: 0.00001232
Iteration 72/1000 | Loss: 0.00001232
Iteration 73/1000 | Loss: 0.00001231
Iteration 74/1000 | Loss: 0.00001231
Iteration 75/1000 | Loss: 0.00001230
Iteration 76/1000 | Loss: 0.00001229
Iteration 77/1000 | Loss: 0.00001229
Iteration 78/1000 | Loss: 0.00001228
Iteration 79/1000 | Loss: 0.00001228
Iteration 80/1000 | Loss: 0.00001228
Iteration 81/1000 | Loss: 0.00001228
Iteration 82/1000 | Loss: 0.00001227
Iteration 83/1000 | Loss: 0.00001227
Iteration 84/1000 | Loss: 0.00001227
Iteration 85/1000 | Loss: 0.00001227
Iteration 86/1000 | Loss: 0.00001227
Iteration 87/1000 | Loss: 0.00001227
Iteration 88/1000 | Loss: 0.00001227
Iteration 89/1000 | Loss: 0.00001227
Iteration 90/1000 | Loss: 0.00001227
Iteration 91/1000 | Loss: 0.00001227
Iteration 92/1000 | Loss: 0.00001227
Iteration 93/1000 | Loss: 0.00001227
Iteration 94/1000 | Loss: 0.00001227
Iteration 95/1000 | Loss: 0.00001227
Iteration 96/1000 | Loss: 0.00001227
Iteration 97/1000 | Loss: 0.00001227
Iteration 98/1000 | Loss: 0.00001227
Iteration 99/1000 | Loss: 0.00001227
Iteration 100/1000 | Loss: 0.00001227
Iteration 101/1000 | Loss: 0.00001227
Iteration 102/1000 | Loss: 0.00001227
Iteration 103/1000 | Loss: 0.00001227
Iteration 104/1000 | Loss: 0.00001227
Iteration 105/1000 | Loss: 0.00001227
Iteration 106/1000 | Loss: 0.00001227
Iteration 107/1000 | Loss: 0.00001227
Iteration 108/1000 | Loss: 0.00001227
Iteration 109/1000 | Loss: 0.00001227
Iteration 110/1000 | Loss: 0.00001227
Iteration 111/1000 | Loss: 0.00001227
Iteration 112/1000 | Loss: 0.00001227
Iteration 113/1000 | Loss: 0.00001227
Iteration 114/1000 | Loss: 0.00001227
Iteration 115/1000 | Loss: 0.00001227
Iteration 116/1000 | Loss: 0.00001227
Iteration 117/1000 | Loss: 0.00001227
Iteration 118/1000 | Loss: 0.00001227
Iteration 119/1000 | Loss: 0.00001227
Iteration 120/1000 | Loss: 0.00001227
Iteration 121/1000 | Loss: 0.00001227
Iteration 122/1000 | Loss: 0.00001227
Iteration 123/1000 | Loss: 0.00001227
Iteration 124/1000 | Loss: 0.00001227
Iteration 125/1000 | Loss: 0.00001227
Iteration 126/1000 | Loss: 0.00001227
Iteration 127/1000 | Loss: 0.00001227
Iteration 128/1000 | Loss: 0.00001227
Iteration 129/1000 | Loss: 0.00001227
Iteration 130/1000 | Loss: 0.00001227
Iteration 131/1000 | Loss: 0.00001227
Iteration 132/1000 | Loss: 0.00001227
Iteration 133/1000 | Loss: 0.00001227
Iteration 134/1000 | Loss: 0.00001227
Iteration 135/1000 | Loss: 0.00001227
Iteration 136/1000 | Loss: 0.00001227
Iteration 137/1000 | Loss: 0.00001227
Iteration 138/1000 | Loss: 0.00001227
Iteration 139/1000 | Loss: 0.00001227
Iteration 140/1000 | Loss: 0.00001227
Iteration 141/1000 | Loss: 0.00001227
Iteration 142/1000 | Loss: 0.00001227
Iteration 143/1000 | Loss: 0.00001227
Iteration 144/1000 | Loss: 0.00001227
Iteration 145/1000 | Loss: 0.00001227
Iteration 146/1000 | Loss: 0.00001227
Iteration 147/1000 | Loss: 0.00001227
Iteration 148/1000 | Loss: 0.00001227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.2268341379240155e-05, 1.2268341379240155e-05, 1.2268341379240155e-05, 1.2268341379240155e-05, 1.2268341379240155e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2268341379240155e-05

Optimization complete. Final v2v error: 3.004141330718994 mm

Highest mean error: 3.2128746509552 mm for frame 122

Lowest mean error: 2.823312282562256 mm for frame 154

Saving results

Total time: 32.96318244934082
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00615022
Iteration 2/25 | Loss: 0.00075256
Iteration 3/25 | Loss: 0.00062509
Iteration 4/25 | Loss: 0.00061153
Iteration 5/25 | Loss: 0.00060763
Iteration 6/25 | Loss: 0.00060643
Iteration 7/25 | Loss: 0.00060640
Iteration 8/25 | Loss: 0.00060640
Iteration 9/25 | Loss: 0.00060640
Iteration 10/25 | Loss: 0.00060640
Iteration 11/25 | Loss: 0.00060640
Iteration 12/25 | Loss: 0.00060640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006063952459953725, 0.0006063952459953725, 0.0006063952459953725, 0.0006063952459953725, 0.0006063952459953725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006063952459953725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.16700554
Iteration 2/25 | Loss: 0.00026313
Iteration 3/25 | Loss: 0.00026311
Iteration 4/25 | Loss: 0.00026311
Iteration 5/25 | Loss: 0.00026311
Iteration 6/25 | Loss: 0.00026311
Iteration 7/25 | Loss: 0.00026311
Iteration 8/25 | Loss: 0.00026311
Iteration 9/25 | Loss: 0.00026311
Iteration 10/25 | Loss: 0.00026311
Iteration 11/25 | Loss: 0.00026311
Iteration 12/25 | Loss: 0.00026311
Iteration 13/25 | Loss: 0.00026311
Iteration 14/25 | Loss: 0.00026311
Iteration 15/25 | Loss: 0.00026311
Iteration 16/25 | Loss: 0.00026311
Iteration 17/25 | Loss: 0.00026311
Iteration 18/25 | Loss: 0.00026311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00026310799876227975, 0.00026310799876227975, 0.00026310799876227975, 0.00026310799876227975, 0.00026310799876227975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026310799876227975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026311
Iteration 2/1000 | Loss: 0.00002418
Iteration 3/1000 | Loss: 0.00001459
Iteration 4/1000 | Loss: 0.00001276
Iteration 5/1000 | Loss: 0.00001193
Iteration 6/1000 | Loss: 0.00001152
Iteration 7/1000 | Loss: 0.00001128
Iteration 8/1000 | Loss: 0.00001116
Iteration 9/1000 | Loss: 0.00001109
Iteration 10/1000 | Loss: 0.00001108
Iteration 11/1000 | Loss: 0.00001107
Iteration 12/1000 | Loss: 0.00001101
Iteration 13/1000 | Loss: 0.00001096
Iteration 14/1000 | Loss: 0.00001095
Iteration 15/1000 | Loss: 0.00001091
Iteration 16/1000 | Loss: 0.00001091
Iteration 17/1000 | Loss: 0.00001091
Iteration 18/1000 | Loss: 0.00001090
Iteration 19/1000 | Loss: 0.00001089
Iteration 20/1000 | Loss: 0.00001089
Iteration 21/1000 | Loss: 0.00001087
Iteration 22/1000 | Loss: 0.00001087
Iteration 23/1000 | Loss: 0.00001086
Iteration 24/1000 | Loss: 0.00001086
Iteration 25/1000 | Loss: 0.00001086
Iteration 26/1000 | Loss: 0.00001086
Iteration 27/1000 | Loss: 0.00001085
Iteration 28/1000 | Loss: 0.00001085
Iteration 29/1000 | Loss: 0.00001082
Iteration 30/1000 | Loss: 0.00001082
Iteration 31/1000 | Loss: 0.00001082
Iteration 32/1000 | Loss: 0.00001082
Iteration 33/1000 | Loss: 0.00001082
Iteration 34/1000 | Loss: 0.00001081
Iteration 35/1000 | Loss: 0.00001081
Iteration 36/1000 | Loss: 0.00001080
Iteration 37/1000 | Loss: 0.00001079
Iteration 38/1000 | Loss: 0.00001079
Iteration 39/1000 | Loss: 0.00001079
Iteration 40/1000 | Loss: 0.00001078
Iteration 41/1000 | Loss: 0.00001078
Iteration 42/1000 | Loss: 0.00001078
Iteration 43/1000 | Loss: 0.00001078
Iteration 44/1000 | Loss: 0.00001078
Iteration 45/1000 | Loss: 0.00001078
Iteration 46/1000 | Loss: 0.00001077
Iteration 47/1000 | Loss: 0.00001077
Iteration 48/1000 | Loss: 0.00001077
Iteration 49/1000 | Loss: 0.00001076
Iteration 50/1000 | Loss: 0.00001076
Iteration 51/1000 | Loss: 0.00001076
Iteration 52/1000 | Loss: 0.00001075
Iteration 53/1000 | Loss: 0.00001074
Iteration 54/1000 | Loss: 0.00001074
Iteration 55/1000 | Loss: 0.00001073
Iteration 56/1000 | Loss: 0.00001073
Iteration 57/1000 | Loss: 0.00001073
Iteration 58/1000 | Loss: 0.00001073
Iteration 59/1000 | Loss: 0.00001073
Iteration 60/1000 | Loss: 0.00001072
Iteration 61/1000 | Loss: 0.00001072
Iteration 62/1000 | Loss: 0.00001072
Iteration 63/1000 | Loss: 0.00001072
Iteration 64/1000 | Loss: 0.00001071
Iteration 65/1000 | Loss: 0.00001071
Iteration 66/1000 | Loss: 0.00001071
Iteration 67/1000 | Loss: 0.00001071
Iteration 68/1000 | Loss: 0.00001070
Iteration 69/1000 | Loss: 0.00001070
Iteration 70/1000 | Loss: 0.00001070
Iteration 71/1000 | Loss: 0.00001070
Iteration 72/1000 | Loss: 0.00001070
Iteration 73/1000 | Loss: 0.00001070
Iteration 74/1000 | Loss: 0.00001070
Iteration 75/1000 | Loss: 0.00001070
Iteration 76/1000 | Loss: 0.00001069
Iteration 77/1000 | Loss: 0.00001069
Iteration 78/1000 | Loss: 0.00001069
Iteration 79/1000 | Loss: 0.00001068
Iteration 80/1000 | Loss: 0.00001068
Iteration 81/1000 | Loss: 0.00001068
Iteration 82/1000 | Loss: 0.00001068
Iteration 83/1000 | Loss: 0.00001068
Iteration 84/1000 | Loss: 0.00001067
Iteration 85/1000 | Loss: 0.00001067
Iteration 86/1000 | Loss: 0.00001067
Iteration 87/1000 | Loss: 0.00001067
Iteration 88/1000 | Loss: 0.00001067
Iteration 89/1000 | Loss: 0.00001067
Iteration 90/1000 | Loss: 0.00001066
Iteration 91/1000 | Loss: 0.00001066
Iteration 92/1000 | Loss: 0.00001066
Iteration 93/1000 | Loss: 0.00001066
Iteration 94/1000 | Loss: 0.00001066
Iteration 95/1000 | Loss: 0.00001066
Iteration 96/1000 | Loss: 0.00001066
Iteration 97/1000 | Loss: 0.00001066
Iteration 98/1000 | Loss: 0.00001066
Iteration 99/1000 | Loss: 0.00001066
Iteration 100/1000 | Loss: 0.00001066
Iteration 101/1000 | Loss: 0.00001066
Iteration 102/1000 | Loss: 0.00001066
Iteration 103/1000 | Loss: 0.00001066
Iteration 104/1000 | Loss: 0.00001066
Iteration 105/1000 | Loss: 0.00001066
Iteration 106/1000 | Loss: 0.00001066
Iteration 107/1000 | Loss: 0.00001066
Iteration 108/1000 | Loss: 0.00001066
Iteration 109/1000 | Loss: 0.00001066
Iteration 110/1000 | Loss: 0.00001066
Iteration 111/1000 | Loss: 0.00001066
Iteration 112/1000 | Loss: 0.00001066
Iteration 113/1000 | Loss: 0.00001066
Iteration 114/1000 | Loss: 0.00001066
Iteration 115/1000 | Loss: 0.00001066
Iteration 116/1000 | Loss: 0.00001066
Iteration 117/1000 | Loss: 0.00001066
Iteration 118/1000 | Loss: 0.00001066
Iteration 119/1000 | Loss: 0.00001066
Iteration 120/1000 | Loss: 0.00001066
Iteration 121/1000 | Loss: 0.00001066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.0656722224666737e-05, 1.0656722224666737e-05, 1.0656722224666737e-05, 1.0656722224666737e-05, 1.0656722224666737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0656722224666737e-05

Optimization complete. Final v2v error: 2.7994139194488525 mm

Highest mean error: 3.1285526752471924 mm for frame 1

Lowest mean error: 2.5874924659729004 mm for frame 43

Saving results

Total time: 31.25340461730957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01090540
Iteration 2/25 | Loss: 0.01090540
Iteration 3/25 | Loss: 0.01090539
Iteration 4/25 | Loss: 0.01090539
Iteration 5/25 | Loss: 0.01090539
Iteration 6/25 | Loss: 0.01090539
Iteration 7/25 | Loss: 0.00321191
Iteration 8/25 | Loss: 0.00185376
Iteration 9/25 | Loss: 0.00163054
Iteration 10/25 | Loss: 0.00147049
Iteration 11/25 | Loss: 0.00147710
Iteration 12/25 | Loss: 0.00132072
Iteration 13/25 | Loss: 0.00133116
Iteration 14/25 | Loss: 0.00130109
Iteration 15/25 | Loss: 0.00124084
Iteration 16/25 | Loss: 0.00118550
Iteration 17/25 | Loss: 0.00115353
Iteration 18/25 | Loss: 0.00113867
Iteration 19/25 | Loss: 0.00114106
Iteration 20/25 | Loss: 0.00111727
Iteration 21/25 | Loss: 0.00110114
Iteration 22/25 | Loss: 0.00111742
Iteration 23/25 | Loss: 0.00108383
Iteration 24/25 | Loss: 0.00110995
Iteration 25/25 | Loss: 0.00108742

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40008545
Iteration 2/25 | Loss: 0.00490176
Iteration 3/25 | Loss: 0.00382881
Iteration 4/25 | Loss: 0.00382881
Iteration 5/25 | Loss: 0.00382881
Iteration 6/25 | Loss: 0.00382881
Iteration 7/25 | Loss: 0.00382881
Iteration 8/25 | Loss: 0.00382881
Iteration 9/25 | Loss: 0.00382881
Iteration 10/25 | Loss: 0.00382881
Iteration 11/25 | Loss: 0.00382881
Iteration 12/25 | Loss: 0.00382881
Iteration 13/25 | Loss: 0.00382881
Iteration 14/25 | Loss: 0.00382881
Iteration 15/25 | Loss: 0.00382881
Iteration 16/25 | Loss: 0.00382881
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0038288075011223555, 0.0038288075011223555, 0.0038288075011223555, 0.0038288075011223555, 0.0038288075011223555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038288075011223555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00382881
Iteration 2/1000 | Loss: 0.00311208
Iteration 3/1000 | Loss: 0.00437695
Iteration 4/1000 | Loss: 0.00160047
Iteration 5/1000 | Loss: 0.00101130
Iteration 6/1000 | Loss: 0.00177361
Iteration 7/1000 | Loss: 0.00594069
Iteration 8/1000 | Loss: 0.00234541
Iteration 9/1000 | Loss: 0.00848608
Iteration 10/1000 | Loss: 0.00170555
Iteration 11/1000 | Loss: 0.00399635
Iteration 12/1000 | Loss: 0.00238995
Iteration 13/1000 | Loss: 0.00095892
Iteration 14/1000 | Loss: 0.00402773
Iteration 15/1000 | Loss: 0.00120121
Iteration 16/1000 | Loss: 0.00130289
Iteration 17/1000 | Loss: 0.00211128
Iteration 18/1000 | Loss: 0.00073379
Iteration 19/1000 | Loss: 0.00047492
Iteration 20/1000 | Loss: 0.00324003
Iteration 21/1000 | Loss: 0.00178928
Iteration 22/1000 | Loss: 0.00301091
Iteration 23/1000 | Loss: 0.00529473
Iteration 24/1000 | Loss: 0.00326517
Iteration 25/1000 | Loss: 0.00528245
Iteration 26/1000 | Loss: 0.00260263
Iteration 27/1000 | Loss: 0.00406055
Iteration 28/1000 | Loss: 0.00181196
Iteration 29/1000 | Loss: 0.00265537
Iteration 30/1000 | Loss: 0.00220283
Iteration 31/1000 | Loss: 0.00330858
Iteration 32/1000 | Loss: 0.00255707
Iteration 33/1000 | Loss: 0.00446783
Iteration 34/1000 | Loss: 0.00098840
Iteration 35/1000 | Loss: 0.00133295
Iteration 36/1000 | Loss: 0.00126709
Iteration 37/1000 | Loss: 0.00075814
Iteration 38/1000 | Loss: 0.00071166
Iteration 39/1000 | Loss: 0.00107538
Iteration 40/1000 | Loss: 0.00056252
Iteration 41/1000 | Loss: 0.00061124
Iteration 42/1000 | Loss: 0.00137844
Iteration 43/1000 | Loss: 0.00051156
Iteration 44/1000 | Loss: 0.00024180
Iteration 45/1000 | Loss: 0.00137507
Iteration 46/1000 | Loss: 0.00051829
Iteration 47/1000 | Loss: 0.00039531
Iteration 48/1000 | Loss: 0.00014570
Iteration 49/1000 | Loss: 0.00140229
Iteration 50/1000 | Loss: 0.00021471
Iteration 51/1000 | Loss: 0.00060013
Iteration 52/1000 | Loss: 0.00025830
Iteration 53/1000 | Loss: 0.00014733
Iteration 54/1000 | Loss: 0.00058262
Iteration 55/1000 | Loss: 0.00008301
Iteration 56/1000 | Loss: 0.00067297
Iteration 57/1000 | Loss: 0.00008724
Iteration 58/1000 | Loss: 0.00009058
Iteration 59/1000 | Loss: 0.00094117
Iteration 60/1000 | Loss: 0.00009736
Iteration 61/1000 | Loss: 0.00008457
Iteration 62/1000 | Loss: 0.00020533
Iteration 63/1000 | Loss: 0.00008014
Iteration 64/1000 | Loss: 0.00005719
Iteration 65/1000 | Loss: 0.00069882
Iteration 66/1000 | Loss: 0.00008994
Iteration 67/1000 | Loss: 0.00010048
Iteration 68/1000 | Loss: 0.00010183
Iteration 69/1000 | Loss: 0.00010899
Iteration 70/1000 | Loss: 0.00017529
Iteration 71/1000 | Loss: 0.00042868
Iteration 72/1000 | Loss: 0.00010389
Iteration 73/1000 | Loss: 0.00048526
Iteration 74/1000 | Loss: 0.00011762
Iteration 75/1000 | Loss: 0.00009785
Iteration 76/1000 | Loss: 0.00039043
Iteration 77/1000 | Loss: 0.00017529
Iteration 78/1000 | Loss: 0.00005667
Iteration 79/1000 | Loss: 0.00009375
Iteration 80/1000 | Loss: 0.00033503
Iteration 81/1000 | Loss: 0.00009808
Iteration 82/1000 | Loss: 0.00012514
Iteration 83/1000 | Loss: 0.00008965
Iteration 84/1000 | Loss: 0.00015985
Iteration 85/1000 | Loss: 0.00009222
Iteration 86/1000 | Loss: 0.00009546
Iteration 87/1000 | Loss: 0.00008712
Iteration 88/1000 | Loss: 0.00009912
Iteration 89/1000 | Loss: 0.00008978
Iteration 90/1000 | Loss: 0.00009552
Iteration 91/1000 | Loss: 0.00008901
Iteration 92/1000 | Loss: 0.00006801
Iteration 93/1000 | Loss: 0.00008972
Iteration 94/1000 | Loss: 0.00013118
Iteration 95/1000 | Loss: 0.00009655
Iteration 96/1000 | Loss: 0.00016284
Iteration 97/1000 | Loss: 0.00009963
Iteration 98/1000 | Loss: 0.00014460
Iteration 99/1000 | Loss: 0.00010742
Iteration 100/1000 | Loss: 0.00008793
Iteration 101/1000 | Loss: 0.00010641
Iteration 102/1000 | Loss: 0.00050067
Iteration 103/1000 | Loss: 0.00013042
Iteration 104/1000 | Loss: 0.00010572
Iteration 105/1000 | Loss: 0.00031464
Iteration 106/1000 | Loss: 0.00040602
Iteration 107/1000 | Loss: 0.00050742
Iteration 108/1000 | Loss: 0.00051697
Iteration 109/1000 | Loss: 0.00040257
Iteration 110/1000 | Loss: 0.00024236
Iteration 111/1000 | Loss: 0.00070567
Iteration 112/1000 | Loss: 0.00026827
Iteration 113/1000 | Loss: 0.00068411
Iteration 114/1000 | Loss: 0.00031704
Iteration 115/1000 | Loss: 0.00042270
Iteration 116/1000 | Loss: 0.00040160
Iteration 117/1000 | Loss: 0.00043114
Iteration 118/1000 | Loss: 0.00076651
Iteration 119/1000 | Loss: 0.00035281
Iteration 120/1000 | Loss: 0.00006127
Iteration 121/1000 | Loss: 0.00030172
Iteration 122/1000 | Loss: 0.00042288
Iteration 123/1000 | Loss: 0.00013659
Iteration 124/1000 | Loss: 0.00032158
Iteration 125/1000 | Loss: 0.00047430
Iteration 126/1000 | Loss: 0.00039004
Iteration 127/1000 | Loss: 0.00031872
Iteration 128/1000 | Loss: 0.00031075
Iteration 129/1000 | Loss: 0.00039090
Iteration 130/1000 | Loss: 0.00048883
Iteration 131/1000 | Loss: 0.00037793
Iteration 132/1000 | Loss: 0.00034274
Iteration 133/1000 | Loss: 0.00085680
Iteration 134/1000 | Loss: 0.00044110
Iteration 135/1000 | Loss: 0.00046628
Iteration 136/1000 | Loss: 0.00006710
Iteration 137/1000 | Loss: 0.00004782
Iteration 138/1000 | Loss: 0.00010383
Iteration 139/1000 | Loss: 0.00015050
Iteration 140/1000 | Loss: 0.00006622
Iteration 141/1000 | Loss: 0.00012157
Iteration 142/1000 | Loss: 0.00010861
Iteration 143/1000 | Loss: 0.00008111
Iteration 144/1000 | Loss: 0.00007342
Iteration 145/1000 | Loss: 0.00011370
Iteration 146/1000 | Loss: 0.00015903
Iteration 147/1000 | Loss: 0.00006753
Iteration 148/1000 | Loss: 0.00005444
Iteration 149/1000 | Loss: 0.00012105
Iteration 150/1000 | Loss: 0.00007672
Iteration 151/1000 | Loss: 0.00020763
Iteration 152/1000 | Loss: 0.00010434
Iteration 153/1000 | Loss: 0.00012527
Iteration 154/1000 | Loss: 0.00006146
Iteration 155/1000 | Loss: 0.00120379
Iteration 156/1000 | Loss: 0.00008236
Iteration 157/1000 | Loss: 0.00006468
Iteration 158/1000 | Loss: 0.00006711
Iteration 159/1000 | Loss: 0.00016457
Iteration 160/1000 | Loss: 0.00005464
Iteration 161/1000 | Loss: 0.00008801
Iteration 162/1000 | Loss: 0.00005323
Iteration 163/1000 | Loss: 0.00007936
Iteration 164/1000 | Loss: 0.00010523
Iteration 165/1000 | Loss: 0.00015047
Iteration 166/1000 | Loss: 0.00006993
Iteration 167/1000 | Loss: 0.00006421
Iteration 168/1000 | Loss: 0.00007152
Iteration 169/1000 | Loss: 0.00008095
Iteration 170/1000 | Loss: 0.00010149
Iteration 171/1000 | Loss: 0.00044422
Iteration 172/1000 | Loss: 0.00059552
Iteration 173/1000 | Loss: 0.00031171
Iteration 174/1000 | Loss: 0.00018527
Iteration 175/1000 | Loss: 0.00006066
Iteration 176/1000 | Loss: 0.00007278
Iteration 177/1000 | Loss: 0.00007729
Iteration 178/1000 | Loss: 0.00006374
Iteration 179/1000 | Loss: 0.00007351
Iteration 180/1000 | Loss: 0.00007670
Iteration 181/1000 | Loss: 0.00005643
Iteration 182/1000 | Loss: 0.00008277
Iteration 183/1000 | Loss: 0.00007382
Iteration 184/1000 | Loss: 0.00005667
Iteration 185/1000 | Loss: 0.00006926
Iteration 186/1000 | Loss: 0.00005775
Iteration 187/1000 | Loss: 0.00006038
Iteration 188/1000 | Loss: 0.00010318
Iteration 189/1000 | Loss: 0.00006788
Iteration 190/1000 | Loss: 0.00004938
Iteration 191/1000 | Loss: 0.00008566
Iteration 192/1000 | Loss: 0.00007778
Iteration 193/1000 | Loss: 0.00011083
Iteration 194/1000 | Loss: 0.00009788
Iteration 195/1000 | Loss: 0.00007937
Iteration 196/1000 | Loss: 0.00009273
Iteration 197/1000 | Loss: 0.00007111
Iteration 198/1000 | Loss: 0.00006596
Iteration 199/1000 | Loss: 0.00007419
Iteration 200/1000 | Loss: 0.00006666
Iteration 201/1000 | Loss: 0.00006895
Iteration 202/1000 | Loss: 0.00010164
Iteration 203/1000 | Loss: 0.00007318
Iteration 204/1000 | Loss: 0.00006060
Iteration 205/1000 | Loss: 0.00004617
Iteration 206/1000 | Loss: 0.00007176
Iteration 207/1000 | Loss: 0.00006956
Iteration 208/1000 | Loss: 0.00011914
Iteration 209/1000 | Loss: 0.00011794
Iteration 210/1000 | Loss: 0.00007942
Iteration 211/1000 | Loss: 0.00006640
Iteration 212/1000 | Loss: 0.00007661
Iteration 213/1000 | Loss: 0.00007464
Iteration 214/1000 | Loss: 0.00007946
Iteration 215/1000 | Loss: 0.00009671
Iteration 216/1000 | Loss: 0.00007335
Iteration 217/1000 | Loss: 0.00012501
Iteration 218/1000 | Loss: 0.00007791
Iteration 219/1000 | Loss: 0.00008968
Iteration 220/1000 | Loss: 0.00017944
Iteration 221/1000 | Loss: 0.00011514
Iteration 222/1000 | Loss: 0.00008556
Iteration 223/1000 | Loss: 0.00026910
Iteration 224/1000 | Loss: 0.00007789
Iteration 225/1000 | Loss: 0.00007370
Iteration 226/1000 | Loss: 0.00007470
Iteration 227/1000 | Loss: 0.00006353
Iteration 228/1000 | Loss: 0.00008058
Iteration 229/1000 | Loss: 0.00006810
Iteration 230/1000 | Loss: 0.00008237
Iteration 231/1000 | Loss: 0.00008437
Iteration 232/1000 | Loss: 0.00008072
Iteration 233/1000 | Loss: 0.00006690
Iteration 234/1000 | Loss: 0.00013748
Iteration 235/1000 | Loss: 0.00007074
Iteration 236/1000 | Loss: 0.00008701
Iteration 237/1000 | Loss: 0.00018881
Iteration 238/1000 | Loss: 0.00006325
Iteration 239/1000 | Loss: 0.00006588
Iteration 240/1000 | Loss: 0.00013040
Iteration 241/1000 | Loss: 0.00009825
Iteration 242/1000 | Loss: 0.00012135
Iteration 243/1000 | Loss: 0.00008674
Iteration 244/1000 | Loss: 0.00014870
Iteration 245/1000 | Loss: 0.00016622
Iteration 246/1000 | Loss: 0.00032799
Iteration 247/1000 | Loss: 0.00026449
Iteration 248/1000 | Loss: 0.00007446
Iteration 249/1000 | Loss: 0.00003860
Iteration 250/1000 | Loss: 0.00004551
Iteration 251/1000 | Loss: 0.00002880
Iteration 252/1000 | Loss: 0.00002873
Iteration 253/1000 | Loss: 0.00002558
Iteration 254/1000 | Loss: 0.00010046
Iteration 255/1000 | Loss: 0.00009925
Iteration 256/1000 | Loss: 0.00005368
Iteration 257/1000 | Loss: 0.00022245
Iteration 258/1000 | Loss: 0.00004426
Iteration 259/1000 | Loss: 0.00002524
Iteration 260/1000 | Loss: 0.00002489
Iteration 261/1000 | Loss: 0.00002486
Iteration 262/1000 | Loss: 0.00002466
Iteration 263/1000 | Loss: 0.00003920
Iteration 264/1000 | Loss: 0.00002452
Iteration 265/1000 | Loss: 0.00002450
Iteration 266/1000 | Loss: 0.00002427
Iteration 267/1000 | Loss: 0.00002421
Iteration 268/1000 | Loss: 0.00002420
Iteration 269/1000 | Loss: 0.00002420
Iteration 270/1000 | Loss: 0.00005009
Iteration 271/1000 | Loss: 0.00007030
Iteration 272/1000 | Loss: 0.00002431
Iteration 273/1000 | Loss: 0.00002394
Iteration 274/1000 | Loss: 0.00002391
Iteration 275/1000 | Loss: 0.00002391
Iteration 276/1000 | Loss: 0.00002391
Iteration 277/1000 | Loss: 0.00002391
Iteration 278/1000 | Loss: 0.00002391
Iteration 279/1000 | Loss: 0.00002391
Iteration 280/1000 | Loss: 0.00002391
Iteration 281/1000 | Loss: 0.00002390
Iteration 282/1000 | Loss: 0.00002390
Iteration 283/1000 | Loss: 0.00002390
Iteration 284/1000 | Loss: 0.00002390
Iteration 285/1000 | Loss: 0.00002390
Iteration 286/1000 | Loss: 0.00002390
Iteration 287/1000 | Loss: 0.00002390
Iteration 288/1000 | Loss: 0.00002389
Iteration 289/1000 | Loss: 0.00002389
Iteration 290/1000 | Loss: 0.00002388
Iteration 291/1000 | Loss: 0.00002388
Iteration 292/1000 | Loss: 0.00002388
Iteration 293/1000 | Loss: 0.00002387
Iteration 294/1000 | Loss: 0.00002387
Iteration 295/1000 | Loss: 0.00006351
Iteration 296/1000 | Loss: 0.00002395
Iteration 297/1000 | Loss: 0.00002385
Iteration 298/1000 | Loss: 0.00002385
Iteration 299/1000 | Loss: 0.00002385
Iteration 300/1000 | Loss: 0.00002384
Iteration 301/1000 | Loss: 0.00002384
Iteration 302/1000 | Loss: 0.00002384
Iteration 303/1000 | Loss: 0.00002383
Iteration 304/1000 | Loss: 0.00002383
Iteration 305/1000 | Loss: 0.00002383
Iteration 306/1000 | Loss: 0.00002383
Iteration 307/1000 | Loss: 0.00002382
Iteration 308/1000 | Loss: 0.00002382
Iteration 309/1000 | Loss: 0.00002382
Iteration 310/1000 | Loss: 0.00002382
Iteration 311/1000 | Loss: 0.00002382
Iteration 312/1000 | Loss: 0.00002382
Iteration 313/1000 | Loss: 0.00002382
Iteration 314/1000 | Loss: 0.00002381
Iteration 315/1000 | Loss: 0.00002381
Iteration 316/1000 | Loss: 0.00002381
Iteration 317/1000 | Loss: 0.00002381
Iteration 318/1000 | Loss: 0.00002381
Iteration 319/1000 | Loss: 0.00002381
Iteration 320/1000 | Loss: 0.00002381
Iteration 321/1000 | Loss: 0.00002381
Iteration 322/1000 | Loss: 0.00002381
Iteration 323/1000 | Loss: 0.00002381
Iteration 324/1000 | Loss: 0.00002381
Iteration 325/1000 | Loss: 0.00002381
Iteration 326/1000 | Loss: 0.00002381
Iteration 327/1000 | Loss: 0.00002380
Iteration 328/1000 | Loss: 0.00002380
Iteration 329/1000 | Loss: 0.00002380
Iteration 330/1000 | Loss: 0.00002380
Iteration 331/1000 | Loss: 0.00002379
Iteration 332/1000 | Loss: 0.00002379
Iteration 333/1000 | Loss: 0.00002379
Iteration 334/1000 | Loss: 0.00002378
Iteration 335/1000 | Loss: 0.00002378
Iteration 336/1000 | Loss: 0.00002378
Iteration 337/1000 | Loss: 0.00002377
Iteration 338/1000 | Loss: 0.00002377
Iteration 339/1000 | Loss: 0.00002377
Iteration 340/1000 | Loss: 0.00002377
Iteration 341/1000 | Loss: 0.00002377
Iteration 342/1000 | Loss: 0.00002377
Iteration 343/1000 | Loss: 0.00002377
Iteration 344/1000 | Loss: 0.00005522
Iteration 345/1000 | Loss: 0.00002396
Iteration 346/1000 | Loss: 0.00002374
Iteration 347/1000 | Loss: 0.00002374
Iteration 348/1000 | Loss: 0.00002373
Iteration 349/1000 | Loss: 0.00002373
Iteration 350/1000 | Loss: 0.00002373
Iteration 351/1000 | Loss: 0.00002373
Iteration 352/1000 | Loss: 0.00002372
Iteration 353/1000 | Loss: 0.00002372
Iteration 354/1000 | Loss: 0.00002372
Iteration 355/1000 | Loss: 0.00002372
Iteration 356/1000 | Loss: 0.00002371
Iteration 357/1000 | Loss: 0.00002371
Iteration 358/1000 | Loss: 0.00002371
Iteration 359/1000 | Loss: 0.00002371
Iteration 360/1000 | Loss: 0.00002371
Iteration 361/1000 | Loss: 0.00002371
Iteration 362/1000 | Loss: 0.00002371
Iteration 363/1000 | Loss: 0.00002371
Iteration 364/1000 | Loss: 0.00002370
Iteration 365/1000 | Loss: 0.00002370
Iteration 366/1000 | Loss: 0.00002370
Iteration 367/1000 | Loss: 0.00002370
Iteration 368/1000 | Loss: 0.00002370
Iteration 369/1000 | Loss: 0.00002370
Iteration 370/1000 | Loss: 0.00002370
Iteration 371/1000 | Loss: 0.00002370
Iteration 372/1000 | Loss: 0.00002370
Iteration 373/1000 | Loss: 0.00002370
Iteration 374/1000 | Loss: 0.00002370
Iteration 375/1000 | Loss: 0.00002370
Iteration 376/1000 | Loss: 0.00002370
Iteration 377/1000 | Loss: 0.00002370
Iteration 378/1000 | Loss: 0.00002370
Iteration 379/1000 | Loss: 0.00002370
Iteration 380/1000 | Loss: 0.00002369
Iteration 381/1000 | Loss: 0.00002369
Iteration 382/1000 | Loss: 0.00002369
Iteration 383/1000 | Loss: 0.00002369
Iteration 384/1000 | Loss: 0.00002369
Iteration 385/1000 | Loss: 0.00002369
Iteration 386/1000 | Loss: 0.00002368
Iteration 387/1000 | Loss: 0.00002368
Iteration 388/1000 | Loss: 0.00002368
Iteration 389/1000 | Loss: 0.00002368
Iteration 390/1000 | Loss: 0.00002368
Iteration 391/1000 | Loss: 0.00002368
Iteration 392/1000 | Loss: 0.00002367
Iteration 393/1000 | Loss: 0.00002367
Iteration 394/1000 | Loss: 0.00002367
Iteration 395/1000 | Loss: 0.00002367
Iteration 396/1000 | Loss: 0.00002367
Iteration 397/1000 | Loss: 0.00002367
Iteration 398/1000 | Loss: 0.00002366
Iteration 399/1000 | Loss: 0.00002366
Iteration 400/1000 | Loss: 0.00002366
Iteration 401/1000 | Loss: 0.00002365
Iteration 402/1000 | Loss: 0.00002365
Iteration 403/1000 | Loss: 0.00002365
Iteration 404/1000 | Loss: 0.00002365
Iteration 405/1000 | Loss: 0.00002365
Iteration 406/1000 | Loss: 0.00002364
Iteration 407/1000 | Loss: 0.00002364
Iteration 408/1000 | Loss: 0.00002364
Iteration 409/1000 | Loss: 0.00002364
Iteration 410/1000 | Loss: 0.00002363
Iteration 411/1000 | Loss: 0.00002363
Iteration 412/1000 | Loss: 0.00002363
Iteration 413/1000 | Loss: 0.00002363
Iteration 414/1000 | Loss: 0.00002363
Iteration 415/1000 | Loss: 0.00002363
Iteration 416/1000 | Loss: 0.00002363
Iteration 417/1000 | Loss: 0.00002363
Iteration 418/1000 | Loss: 0.00002363
Iteration 419/1000 | Loss: 0.00002362
Iteration 420/1000 | Loss: 0.00002362
Iteration 421/1000 | Loss: 0.00002362
Iteration 422/1000 | Loss: 0.00002362
Iteration 423/1000 | Loss: 0.00002362
Iteration 424/1000 | Loss: 0.00002362
Iteration 425/1000 | Loss: 0.00002362
Iteration 426/1000 | Loss: 0.00002362
Iteration 427/1000 | Loss: 0.00002362
Iteration 428/1000 | Loss: 0.00002362
Iteration 429/1000 | Loss: 0.00002362
Iteration 430/1000 | Loss: 0.00002362
Iteration 431/1000 | Loss: 0.00002362
Iteration 432/1000 | Loss: 0.00002362
Iteration 433/1000 | Loss: 0.00002362
Iteration 434/1000 | Loss: 0.00002362
Iteration 435/1000 | Loss: 0.00002362
Iteration 436/1000 | Loss: 0.00002362
Iteration 437/1000 | Loss: 0.00002362
Iteration 438/1000 | Loss: 0.00002362
Iteration 439/1000 | Loss: 0.00002362
Iteration 440/1000 | Loss: 0.00002362
Iteration 441/1000 | Loss: 0.00002362
Iteration 442/1000 | Loss: 0.00002362
Iteration 443/1000 | Loss: 0.00002362
Iteration 444/1000 | Loss: 0.00002362
Iteration 445/1000 | Loss: 0.00002362
Iteration 446/1000 | Loss: 0.00002362
Iteration 447/1000 | Loss: 0.00002362
Iteration 448/1000 | Loss: 0.00002362
Iteration 449/1000 | Loss: 0.00002362
Iteration 450/1000 | Loss: 0.00002362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 450. Stopping optimization.
Last 5 losses: [2.3621762011316605e-05, 2.3621762011316605e-05, 2.3621762011316605e-05, 2.3621762011316605e-05, 2.3621762011316605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3621762011316605e-05

Optimization complete. Final v2v error: 3.8979318141937256 mm

Highest mean error: 6.352348804473877 mm for frame 64

Lowest mean error: 3.322567939758301 mm for frame 100

Saving results

Total time: 474.4384799003601
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043308
Iteration 2/25 | Loss: 0.01043308
Iteration 3/25 | Loss: 0.01043308
Iteration 4/25 | Loss: 0.00261873
Iteration 5/25 | Loss: 0.00181302
Iteration 6/25 | Loss: 0.00148127
Iteration 7/25 | Loss: 0.00177182
Iteration 8/25 | Loss: 0.00128321
Iteration 9/25 | Loss: 0.00105170
Iteration 10/25 | Loss: 0.00091106
Iteration 11/25 | Loss: 0.00078458
Iteration 12/25 | Loss: 0.00075016
Iteration 13/25 | Loss: 0.00073110
Iteration 14/25 | Loss: 0.00072049
Iteration 15/25 | Loss: 0.00070893
Iteration 16/25 | Loss: 0.00070434
Iteration 17/25 | Loss: 0.00069582
Iteration 18/25 | Loss: 0.00069305
Iteration 19/25 | Loss: 0.00069211
Iteration 20/25 | Loss: 0.00069165
Iteration 21/25 | Loss: 0.00069548
Iteration 22/25 | Loss: 0.00069665
Iteration 23/25 | Loss: 0.00069168
Iteration 24/25 | Loss: 0.00068858
Iteration 25/25 | Loss: 0.00068758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40243709
Iteration 2/25 | Loss: 0.00034053
Iteration 3/25 | Loss: 0.00034052
Iteration 4/25 | Loss: 0.00034052
Iteration 5/25 | Loss: 0.00034052
Iteration 6/25 | Loss: 0.00034052
Iteration 7/25 | Loss: 0.00034052
Iteration 8/25 | Loss: 0.00034052
Iteration 9/25 | Loss: 0.00034052
Iteration 10/25 | Loss: 0.00034052
Iteration 11/25 | Loss: 0.00034052
Iteration 12/25 | Loss: 0.00034052
Iteration 13/25 | Loss: 0.00034052
Iteration 14/25 | Loss: 0.00034052
Iteration 15/25 | Loss: 0.00034052
Iteration 16/25 | Loss: 0.00034052
Iteration 17/25 | Loss: 0.00034052
Iteration 18/25 | Loss: 0.00034052
Iteration 19/25 | Loss: 0.00034052
Iteration 20/25 | Loss: 0.00034052
Iteration 21/25 | Loss: 0.00034052
Iteration 22/25 | Loss: 0.00034052
Iteration 23/25 | Loss: 0.00034052
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0003405209572520107, 0.0003405209572520107, 0.0003405209572520107, 0.0003405209572520107, 0.0003405209572520107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003405209572520107

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034052
Iteration 2/1000 | Loss: 0.00005218
Iteration 3/1000 | Loss: 0.00002937
Iteration 4/1000 | Loss: 0.00002489
Iteration 5/1000 | Loss: 0.00017916
Iteration 6/1000 | Loss: 0.00003144
Iteration 7/1000 | Loss: 0.00002652
Iteration 8/1000 | Loss: 0.00059579
Iteration 9/1000 | Loss: 0.00052678
Iteration 10/1000 | Loss: 0.00003214
Iteration 11/1000 | Loss: 0.00002592
Iteration 12/1000 | Loss: 0.00080336
Iteration 13/1000 | Loss: 0.00008404
Iteration 14/1000 | Loss: 0.00002597
Iteration 15/1000 | Loss: 0.00002058
Iteration 16/1000 | Loss: 0.00001945
Iteration 17/1000 | Loss: 0.00010513
Iteration 18/1000 | Loss: 0.00007938
Iteration 19/1000 | Loss: 0.00006281
Iteration 20/1000 | Loss: 0.00002140
Iteration 21/1000 | Loss: 0.00001908
Iteration 22/1000 | Loss: 0.00001847
Iteration 23/1000 | Loss: 0.00001811
Iteration 24/1000 | Loss: 0.00001790
Iteration 25/1000 | Loss: 0.00001783
Iteration 26/1000 | Loss: 0.00001779
Iteration 27/1000 | Loss: 0.00001779
Iteration 28/1000 | Loss: 0.00001779
Iteration 29/1000 | Loss: 0.00001779
Iteration 30/1000 | Loss: 0.00001778
Iteration 31/1000 | Loss: 0.00001778
Iteration 32/1000 | Loss: 0.00001778
Iteration 33/1000 | Loss: 0.00001775
Iteration 34/1000 | Loss: 0.00001768
Iteration 35/1000 | Loss: 0.00001765
Iteration 36/1000 | Loss: 0.00001765
Iteration 37/1000 | Loss: 0.00001765
Iteration 38/1000 | Loss: 0.00001765
Iteration 39/1000 | Loss: 0.00001765
Iteration 40/1000 | Loss: 0.00001765
Iteration 41/1000 | Loss: 0.00001764
Iteration 42/1000 | Loss: 0.00001764
Iteration 43/1000 | Loss: 0.00001761
Iteration 44/1000 | Loss: 0.00001761
Iteration 45/1000 | Loss: 0.00001761
Iteration 46/1000 | Loss: 0.00001760
Iteration 47/1000 | Loss: 0.00001760
Iteration 48/1000 | Loss: 0.00001759
Iteration 49/1000 | Loss: 0.00001759
Iteration 50/1000 | Loss: 0.00001758
Iteration 51/1000 | Loss: 0.00001758
Iteration 52/1000 | Loss: 0.00001758
Iteration 53/1000 | Loss: 0.00001758
Iteration 54/1000 | Loss: 0.00001757
Iteration 55/1000 | Loss: 0.00001757
Iteration 56/1000 | Loss: 0.00001757
Iteration 57/1000 | Loss: 0.00001757
Iteration 58/1000 | Loss: 0.00001757
Iteration 59/1000 | Loss: 0.00001757
Iteration 60/1000 | Loss: 0.00001757
Iteration 61/1000 | Loss: 0.00001757
Iteration 62/1000 | Loss: 0.00001757
Iteration 63/1000 | Loss: 0.00001757
Iteration 64/1000 | Loss: 0.00001757
Iteration 65/1000 | Loss: 0.00001756
Iteration 66/1000 | Loss: 0.00001756
Iteration 67/1000 | Loss: 0.00001756
Iteration 68/1000 | Loss: 0.00001756
Iteration 69/1000 | Loss: 0.00001756
Iteration 70/1000 | Loss: 0.00001756
Iteration 71/1000 | Loss: 0.00001756
Iteration 72/1000 | Loss: 0.00001756
Iteration 73/1000 | Loss: 0.00001756
Iteration 74/1000 | Loss: 0.00001756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.7555401427671313e-05, 1.7555401427671313e-05, 1.7555401427671313e-05, 1.7555401427671313e-05, 1.7555401427671313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7555401427671313e-05

Optimization complete. Final v2v error: 3.398638963699341 mm

Highest mean error: 12.147327423095703 mm for frame 20

Lowest mean error: 3.235647678375244 mm for frame 81

Saving results

Total time: 92.42279052734375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01163266
Iteration 2/25 | Loss: 0.00174338
Iteration 3/25 | Loss: 0.00099405
Iteration 4/25 | Loss: 0.00089938
Iteration 5/25 | Loss: 0.00087211
Iteration 6/25 | Loss: 0.00087145
Iteration 7/25 | Loss: 0.00086952
Iteration 8/25 | Loss: 0.00086726
Iteration 9/25 | Loss: 0.00087483
Iteration 10/25 | Loss: 0.00087781
Iteration 11/25 | Loss: 0.00087198
Iteration 12/25 | Loss: 0.00087063
Iteration 13/25 | Loss: 0.00087249
Iteration 14/25 | Loss: 0.00086877
Iteration 15/25 | Loss: 0.00086296
Iteration 16/25 | Loss: 0.00086462
Iteration 17/25 | Loss: 0.00086632
Iteration 18/25 | Loss: 0.00086630
Iteration 19/25 | Loss: 0.00086296
Iteration 20/25 | Loss: 0.00086210
Iteration 21/25 | Loss: 0.00086408
Iteration 22/25 | Loss: 0.00086302
Iteration 23/25 | Loss: 0.00086284
Iteration 24/25 | Loss: 0.00086253
Iteration 25/25 | Loss: 0.00085767

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06119156
Iteration 2/25 | Loss: 0.00047841
Iteration 3/25 | Loss: 0.00047837
Iteration 4/25 | Loss: 0.00047837
Iteration 5/25 | Loss: 0.00047837
Iteration 6/25 | Loss: 0.00047837
Iteration 7/25 | Loss: 0.00047837
Iteration 8/25 | Loss: 0.00047837
Iteration 9/25 | Loss: 0.00047837
Iteration 10/25 | Loss: 0.00047837
Iteration 11/25 | Loss: 0.00047837
Iteration 12/25 | Loss: 0.00047837
Iteration 13/25 | Loss: 0.00047837
Iteration 14/25 | Loss: 0.00047836
Iteration 15/25 | Loss: 0.00047836
Iteration 16/25 | Loss: 0.00047837
Iteration 17/25 | Loss: 0.00047836
Iteration 18/25 | Loss: 0.00047836
Iteration 19/25 | Loss: 0.00047836
Iteration 20/25 | Loss: 0.00047837
Iteration 21/25 | Loss: 0.00047837
Iteration 22/25 | Loss: 0.00047837
Iteration 23/25 | Loss: 0.00047837
Iteration 24/25 | Loss: 0.00047837
Iteration 25/25 | Loss: 0.00047837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047837
Iteration 2/1000 | Loss: 0.00050008
Iteration 3/1000 | Loss: 0.00043760
Iteration 4/1000 | Loss: 0.00041515
Iteration 5/1000 | Loss: 0.00042012
Iteration 6/1000 | Loss: 0.00043482
Iteration 7/1000 | Loss: 0.00012821
Iteration 8/1000 | Loss: 0.00060275
Iteration 9/1000 | Loss: 0.00042058
Iteration 10/1000 | Loss: 0.00024920
Iteration 11/1000 | Loss: 0.00023382
Iteration 12/1000 | Loss: 0.00044903
Iteration 13/1000 | Loss: 0.00042339
Iteration 14/1000 | Loss: 0.00036232
Iteration 15/1000 | Loss: 0.00053306
Iteration 16/1000 | Loss: 0.00052638
Iteration 17/1000 | Loss: 0.00026199
Iteration 18/1000 | Loss: 0.00034080
Iteration 19/1000 | Loss: 0.00092885
Iteration 20/1000 | Loss: 0.00047619
Iteration 21/1000 | Loss: 0.00028020
Iteration 22/1000 | Loss: 0.00042687
Iteration 23/1000 | Loss: 0.00028113
Iteration 24/1000 | Loss: 0.00044289
Iteration 25/1000 | Loss: 0.00058473
Iteration 26/1000 | Loss: 0.00039326
Iteration 27/1000 | Loss: 0.00070043
Iteration 28/1000 | Loss: 0.00060898
Iteration 29/1000 | Loss: 0.00028114
Iteration 30/1000 | Loss: 0.00029594
Iteration 31/1000 | Loss: 0.00056101
Iteration 32/1000 | Loss: 0.00055895
Iteration 33/1000 | Loss: 0.00044836
Iteration 34/1000 | Loss: 0.00075954
Iteration 35/1000 | Loss: 0.00038729
Iteration 36/1000 | Loss: 0.00033856
Iteration 37/1000 | Loss: 0.00034507
Iteration 38/1000 | Loss: 0.00028263
Iteration 39/1000 | Loss: 0.00031550
Iteration 40/1000 | Loss: 0.00044064
Iteration 41/1000 | Loss: 0.00041452
Iteration 42/1000 | Loss: 0.00041172
Iteration 43/1000 | Loss: 0.00043694
Iteration 44/1000 | Loss: 0.00044989
Iteration 45/1000 | Loss: 0.00039593
Iteration 46/1000 | Loss: 0.00041928
Iteration 47/1000 | Loss: 0.00047276
Iteration 48/1000 | Loss: 0.00047030
Iteration 49/1000 | Loss: 0.00044963
Iteration 50/1000 | Loss: 0.00045950
Iteration 51/1000 | Loss: 0.00044243
Iteration 52/1000 | Loss: 0.00045809
Iteration 53/1000 | Loss: 0.00060627
Iteration 54/1000 | Loss: 0.00046263
Iteration 55/1000 | Loss: 0.00051848
Iteration 56/1000 | Loss: 0.00029183
Iteration 57/1000 | Loss: 0.00037964
Iteration 58/1000 | Loss: 0.00035749
Iteration 59/1000 | Loss: 0.00030883
Iteration 60/1000 | Loss: 0.00043649
Iteration 61/1000 | Loss: 0.00030422
Iteration 62/1000 | Loss: 0.00034698
Iteration 63/1000 | Loss: 0.00038297
Iteration 64/1000 | Loss: 0.00035892
Iteration 65/1000 | Loss: 0.00042612
Iteration 66/1000 | Loss: 0.00032135
Iteration 67/1000 | Loss: 0.00042472
Iteration 68/1000 | Loss: 0.00033306
Iteration 69/1000 | Loss: 0.00058167
Iteration 70/1000 | Loss: 0.00024365
Iteration 71/1000 | Loss: 0.00049909
Iteration 72/1000 | Loss: 0.00035617
Iteration 73/1000 | Loss: 0.00074020
Iteration 74/1000 | Loss: 0.00043871
Iteration 75/1000 | Loss: 0.00059701
Iteration 76/1000 | Loss: 0.00069287
Iteration 77/1000 | Loss: 0.00048919
Iteration 78/1000 | Loss: 0.00026821
Iteration 79/1000 | Loss: 0.00032231
Iteration 80/1000 | Loss: 0.00100753
Iteration 81/1000 | Loss: 0.00079811
Iteration 82/1000 | Loss: 0.00045692
Iteration 83/1000 | Loss: 0.00027226
Iteration 84/1000 | Loss: 0.00025828
Iteration 85/1000 | Loss: 0.00033960
Iteration 86/1000 | Loss: 0.00022200
Iteration 87/1000 | Loss: 0.00034042
Iteration 88/1000 | Loss: 0.00027186
Iteration 89/1000 | Loss: 0.00031007
Iteration 90/1000 | Loss: 0.00032472
Iteration 91/1000 | Loss: 0.00030294
Iteration 92/1000 | Loss: 0.00032369
Iteration 93/1000 | Loss: 0.00030329
Iteration 94/1000 | Loss: 0.00032067
Iteration 95/1000 | Loss: 0.00017767
Iteration 96/1000 | Loss: 0.00033344
Iteration 97/1000 | Loss: 0.00031673
Iteration 98/1000 | Loss: 0.00034380
Iteration 99/1000 | Loss: 0.00030694
Iteration 100/1000 | Loss: 0.00032313
Iteration 101/1000 | Loss: 0.00027798
Iteration 102/1000 | Loss: 0.00031407
Iteration 103/1000 | Loss: 0.00032571
Iteration 104/1000 | Loss: 0.00038054
Iteration 105/1000 | Loss: 0.00031221
Iteration 106/1000 | Loss: 0.00033211
Iteration 107/1000 | Loss: 0.00029912
Iteration 108/1000 | Loss: 0.00024964
Iteration 109/1000 | Loss: 0.00030144
Iteration 110/1000 | Loss: 0.00056282
Iteration 111/1000 | Loss: 0.00038891
Iteration 112/1000 | Loss: 0.00028758
Iteration 113/1000 | Loss: 0.00035148
Iteration 114/1000 | Loss: 0.00031662
Iteration 115/1000 | Loss: 0.00038983
Iteration 116/1000 | Loss: 0.00022268
Iteration 117/1000 | Loss: 0.00025537
Iteration 118/1000 | Loss: 0.00025138
Iteration 119/1000 | Loss: 0.00032850
Iteration 120/1000 | Loss: 0.00025283
Iteration 121/1000 | Loss: 0.00037463
Iteration 122/1000 | Loss: 0.00029177
Iteration 123/1000 | Loss: 0.00031431
Iteration 124/1000 | Loss: 0.00030668
Iteration 125/1000 | Loss: 0.00030907
Iteration 126/1000 | Loss: 0.00033810
Iteration 127/1000 | Loss: 0.00030841
Iteration 128/1000 | Loss: 0.00016365
Iteration 129/1000 | Loss: 0.00016024
Iteration 130/1000 | Loss: 0.00026173
Iteration 131/1000 | Loss: 0.00025127
Iteration 132/1000 | Loss: 0.00028521
Iteration 133/1000 | Loss: 0.00026986
Iteration 134/1000 | Loss: 0.00030388
Iteration 135/1000 | Loss: 0.00024757
Iteration 136/1000 | Loss: 0.00029426
Iteration 137/1000 | Loss: 0.00028753
Iteration 138/1000 | Loss: 0.00028049
Iteration 139/1000 | Loss: 0.00031558
Iteration 140/1000 | Loss: 0.00019341
Iteration 141/1000 | Loss: 0.00027561
Iteration 142/1000 | Loss: 0.00020784
Iteration 143/1000 | Loss: 0.00025757
Iteration 144/1000 | Loss: 0.00032212
Iteration 145/1000 | Loss: 0.00027219
Iteration 146/1000 | Loss: 0.00024814
Iteration 147/1000 | Loss: 0.00028153
Iteration 148/1000 | Loss: 0.00029404
Iteration 149/1000 | Loss: 0.00029715
Iteration 150/1000 | Loss: 0.00046788
Iteration 151/1000 | Loss: 0.00016642
Iteration 152/1000 | Loss: 0.00015605
Iteration 153/1000 | Loss: 0.00019854
Iteration 154/1000 | Loss: 0.00015771
Iteration 155/1000 | Loss: 0.00009345
Iteration 156/1000 | Loss: 0.00017773
Iteration 157/1000 | Loss: 0.00012682
Iteration 158/1000 | Loss: 0.00014062
Iteration 159/1000 | Loss: 0.00016151
Iteration 160/1000 | Loss: 0.00034225
Iteration 161/1000 | Loss: 0.00006597
Iteration 162/1000 | Loss: 0.00006868
Iteration 163/1000 | Loss: 0.00010489
Iteration 164/1000 | Loss: 0.00007415
Iteration 165/1000 | Loss: 0.00009295
Iteration 166/1000 | Loss: 0.00010710
Iteration 167/1000 | Loss: 0.00006407
Iteration 168/1000 | Loss: 0.00010439
Iteration 169/1000 | Loss: 0.00012162
Iteration 170/1000 | Loss: 0.00010456
Iteration 171/1000 | Loss: 0.00005596
Iteration 172/1000 | Loss: 0.00011774
Iteration 173/1000 | Loss: 0.00011052
Iteration 174/1000 | Loss: 0.00022154
Iteration 175/1000 | Loss: 0.00005730
Iteration 176/1000 | Loss: 0.00010842
Iteration 177/1000 | Loss: 0.00007525
Iteration 178/1000 | Loss: 0.00008735
Iteration 179/1000 | Loss: 0.00025066
Iteration 180/1000 | Loss: 0.00008991
Iteration 181/1000 | Loss: 0.00011521
Iteration 182/1000 | Loss: 0.00026589
Iteration 183/1000 | Loss: 0.00004015
Iteration 184/1000 | Loss: 0.00003406
Iteration 185/1000 | Loss: 0.00003203
Iteration 186/1000 | Loss: 0.00003116
Iteration 187/1000 | Loss: 0.00003043
Iteration 188/1000 | Loss: 0.00002985
Iteration 189/1000 | Loss: 0.00002951
Iteration 190/1000 | Loss: 0.00002928
Iteration 191/1000 | Loss: 0.00002901
Iteration 192/1000 | Loss: 0.00002878
Iteration 193/1000 | Loss: 0.00002875
Iteration 194/1000 | Loss: 0.00002864
Iteration 195/1000 | Loss: 0.00002863
Iteration 196/1000 | Loss: 0.00002863
Iteration 197/1000 | Loss: 0.00002861
Iteration 198/1000 | Loss: 0.00002861
Iteration 199/1000 | Loss: 0.00002861
Iteration 200/1000 | Loss: 0.00002860
Iteration 201/1000 | Loss: 0.00002860
Iteration 202/1000 | Loss: 0.00002857
Iteration 203/1000 | Loss: 0.00002854
Iteration 204/1000 | Loss: 0.00002851
Iteration 205/1000 | Loss: 0.00002849
Iteration 206/1000 | Loss: 0.00002845
Iteration 207/1000 | Loss: 0.00002833
Iteration 208/1000 | Loss: 0.00002832
Iteration 209/1000 | Loss: 0.00002832
Iteration 210/1000 | Loss: 0.00002831
Iteration 211/1000 | Loss: 0.00002830
Iteration 212/1000 | Loss: 0.00002829
Iteration 213/1000 | Loss: 0.00002828
Iteration 214/1000 | Loss: 0.00002827
Iteration 215/1000 | Loss: 0.00002827
Iteration 216/1000 | Loss: 0.00002827
Iteration 217/1000 | Loss: 0.00002827
Iteration 218/1000 | Loss: 0.00002827
Iteration 219/1000 | Loss: 0.00002826
Iteration 220/1000 | Loss: 0.00002826
Iteration 221/1000 | Loss: 0.00002826
Iteration 222/1000 | Loss: 0.00002826
Iteration 223/1000 | Loss: 0.00002826
Iteration 224/1000 | Loss: 0.00002826
Iteration 225/1000 | Loss: 0.00002826
Iteration 226/1000 | Loss: 0.00002826
Iteration 227/1000 | Loss: 0.00002825
Iteration 228/1000 | Loss: 0.00002825
Iteration 229/1000 | Loss: 0.00002825
Iteration 230/1000 | Loss: 0.00002825
Iteration 231/1000 | Loss: 0.00002825
Iteration 232/1000 | Loss: 0.00002825
Iteration 233/1000 | Loss: 0.00002825
Iteration 234/1000 | Loss: 0.00002824
Iteration 235/1000 | Loss: 0.00002824
Iteration 236/1000 | Loss: 0.00002824
Iteration 237/1000 | Loss: 0.00002824
Iteration 238/1000 | Loss: 0.00002824
Iteration 239/1000 | Loss: 0.00002823
Iteration 240/1000 | Loss: 0.00002822
Iteration 241/1000 | Loss: 0.00002822
Iteration 242/1000 | Loss: 0.00002821
Iteration 243/1000 | Loss: 0.00002821
Iteration 244/1000 | Loss: 0.00002821
Iteration 245/1000 | Loss: 0.00002820
Iteration 246/1000 | Loss: 0.00002820
Iteration 247/1000 | Loss: 0.00002820
Iteration 248/1000 | Loss: 0.00002819
Iteration 249/1000 | Loss: 0.00002819
Iteration 250/1000 | Loss: 0.00002819
Iteration 251/1000 | Loss: 0.00002819
Iteration 252/1000 | Loss: 0.00002819
Iteration 253/1000 | Loss: 0.00002818
Iteration 254/1000 | Loss: 0.00002818
Iteration 255/1000 | Loss: 0.00002818
Iteration 256/1000 | Loss: 0.00002818
Iteration 257/1000 | Loss: 0.00002818
Iteration 258/1000 | Loss: 0.00002818
Iteration 259/1000 | Loss: 0.00002818
Iteration 260/1000 | Loss: 0.00002818
Iteration 261/1000 | Loss: 0.00002818
Iteration 262/1000 | Loss: 0.00002818
Iteration 263/1000 | Loss: 0.00002818
Iteration 264/1000 | Loss: 0.00002818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [2.817777385644149e-05, 2.817777385644149e-05, 2.817777385644149e-05, 2.817777385644149e-05, 2.817777385644149e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.817777385644149e-05

Optimization complete. Final v2v error: 4.295032024383545 mm

Highest mean error: 5.586627960205078 mm for frame 57

Lowest mean error: 3.7149808406829834 mm for frame 33

Saving results

Total time: 373.280109167099
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035543
Iteration 2/25 | Loss: 0.01035542
Iteration 3/25 | Loss: 0.00270567
Iteration 4/25 | Loss: 0.00181251
Iteration 5/25 | Loss: 0.00153684
Iteration 6/25 | Loss: 0.00146446
Iteration 7/25 | Loss: 0.00150953
Iteration 8/25 | Loss: 0.00150866
Iteration 9/25 | Loss: 0.00139107
Iteration 10/25 | Loss: 0.00128377
Iteration 11/25 | Loss: 0.00121317
Iteration 12/25 | Loss: 0.00117252
Iteration 13/25 | Loss: 0.00115184
Iteration 14/25 | Loss: 0.00112669
Iteration 15/25 | Loss: 0.00110132
Iteration 16/25 | Loss: 0.00108505
Iteration 17/25 | Loss: 0.00108353
Iteration 18/25 | Loss: 0.00108492
Iteration 19/25 | Loss: 0.00107917
Iteration 20/25 | Loss: 0.00106539
Iteration 21/25 | Loss: 0.00106961
Iteration 22/25 | Loss: 0.00107585
Iteration 23/25 | Loss: 0.00106260
Iteration 24/25 | Loss: 0.00105811
Iteration 25/25 | Loss: 0.00104787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60651934
Iteration 2/25 | Loss: 0.00373653
Iteration 3/25 | Loss: 0.00373652
Iteration 4/25 | Loss: 0.00238755
Iteration 5/25 | Loss: 0.00234918
Iteration 6/25 | Loss: 0.00234917
Iteration 7/25 | Loss: 0.00234917
Iteration 8/25 | Loss: 0.00234917
Iteration 9/25 | Loss: 0.00234917
Iteration 10/25 | Loss: 0.00234917
Iteration 11/25 | Loss: 0.00234917
Iteration 12/25 | Loss: 0.00234917
Iteration 13/25 | Loss: 0.00234917
Iteration 14/25 | Loss: 0.00234917
Iteration 15/25 | Loss: 0.00234917
Iteration 16/25 | Loss: 0.00234917
Iteration 17/25 | Loss: 0.00234917
Iteration 18/25 | Loss: 0.00234917
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0023491729516535997, 0.0023491729516535997, 0.0023491729516535997, 0.0023491729516535997, 0.0023491729516535997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023491729516535997

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00234917
Iteration 2/1000 | Loss: 0.00168342
Iteration 3/1000 | Loss: 0.00105619
Iteration 4/1000 | Loss: 0.00057243
Iteration 5/1000 | Loss: 0.00105639
Iteration 6/1000 | Loss: 0.00168886
Iteration 7/1000 | Loss: 0.00032618
Iteration 8/1000 | Loss: 0.00090411
Iteration 9/1000 | Loss: 0.00057481
Iteration 10/1000 | Loss: 0.00146607
Iteration 11/1000 | Loss: 0.00373151
Iteration 12/1000 | Loss: 0.00099492
Iteration 13/1000 | Loss: 0.00047779
Iteration 14/1000 | Loss: 0.00042914
Iteration 15/1000 | Loss: 0.00123152
Iteration 16/1000 | Loss: 0.00159332
Iteration 17/1000 | Loss: 0.00198100
Iteration 18/1000 | Loss: 0.00055594
Iteration 19/1000 | Loss: 0.00323179
Iteration 20/1000 | Loss: 0.00259766
Iteration 21/1000 | Loss: 0.00028667
Iteration 22/1000 | Loss: 0.00175258
Iteration 23/1000 | Loss: 0.00084820
Iteration 24/1000 | Loss: 0.00050049
Iteration 25/1000 | Loss: 0.00097324
Iteration 26/1000 | Loss: 0.00022723
Iteration 27/1000 | Loss: 0.00041272
Iteration 28/1000 | Loss: 0.00051418
Iteration 29/1000 | Loss: 0.00042483
Iteration 30/1000 | Loss: 0.00060706
Iteration 31/1000 | Loss: 0.00032762
Iteration 32/1000 | Loss: 0.00026466
Iteration 33/1000 | Loss: 0.00072096
Iteration 34/1000 | Loss: 0.00040707
Iteration 35/1000 | Loss: 0.00019850
Iteration 36/1000 | Loss: 0.00027691
Iteration 37/1000 | Loss: 0.00026186
Iteration 38/1000 | Loss: 0.00112670
Iteration 39/1000 | Loss: 0.00035011
Iteration 40/1000 | Loss: 0.00038369
Iteration 41/1000 | Loss: 0.00033306
Iteration 42/1000 | Loss: 0.00030405
Iteration 43/1000 | Loss: 0.00031766
Iteration 44/1000 | Loss: 0.00030405
Iteration 45/1000 | Loss: 0.00035557
Iteration 46/1000 | Loss: 0.00071194
Iteration 47/1000 | Loss: 0.00167224
Iteration 48/1000 | Loss: 0.00047237
Iteration 49/1000 | Loss: 0.00029027
Iteration 50/1000 | Loss: 0.00025375
Iteration 51/1000 | Loss: 0.00068141
Iteration 52/1000 | Loss: 0.00045080
Iteration 53/1000 | Loss: 0.00027051
Iteration 54/1000 | Loss: 0.00036546
Iteration 55/1000 | Loss: 0.00032618
Iteration 56/1000 | Loss: 0.00021726
Iteration 57/1000 | Loss: 0.00020626
Iteration 58/1000 | Loss: 0.00038491
Iteration 59/1000 | Loss: 0.00034093
Iteration 60/1000 | Loss: 0.00014927
Iteration 61/1000 | Loss: 0.00064668
Iteration 62/1000 | Loss: 0.00154645
Iteration 63/1000 | Loss: 0.00031741
Iteration 64/1000 | Loss: 0.00032535
Iteration 65/1000 | Loss: 0.00027397
Iteration 66/1000 | Loss: 0.00019765
Iteration 67/1000 | Loss: 0.00027728
Iteration 68/1000 | Loss: 0.00025782
Iteration 69/1000 | Loss: 0.00093309
Iteration 70/1000 | Loss: 0.00105066
Iteration 71/1000 | Loss: 0.00034709
Iteration 72/1000 | Loss: 0.00030877
Iteration 73/1000 | Loss: 0.00039461
Iteration 74/1000 | Loss: 0.00039424
Iteration 75/1000 | Loss: 0.00040463
Iteration 76/1000 | Loss: 0.00033219
Iteration 77/1000 | Loss: 0.00094376
Iteration 78/1000 | Loss: 0.00211795
Iteration 79/1000 | Loss: 0.00140513
Iteration 80/1000 | Loss: 0.00061405
Iteration 81/1000 | Loss: 0.00059548
Iteration 82/1000 | Loss: 0.00033625
Iteration 83/1000 | Loss: 0.00018844
Iteration 84/1000 | Loss: 0.00022745
Iteration 85/1000 | Loss: 0.00024769
Iteration 86/1000 | Loss: 0.00050446
Iteration 87/1000 | Loss: 0.00030918
Iteration 88/1000 | Loss: 0.00032818
Iteration 89/1000 | Loss: 0.00033487
Iteration 90/1000 | Loss: 0.00021295
Iteration 91/1000 | Loss: 0.00028338
Iteration 92/1000 | Loss: 0.00031474
Iteration 93/1000 | Loss: 0.00018702
Iteration 94/1000 | Loss: 0.00030237
Iteration 95/1000 | Loss: 0.00023439
Iteration 96/1000 | Loss: 0.00024917
Iteration 97/1000 | Loss: 0.00037711
Iteration 98/1000 | Loss: 0.00014517
Iteration 99/1000 | Loss: 0.00043422
Iteration 100/1000 | Loss: 0.00074442
Iteration 101/1000 | Loss: 0.00093569
Iteration 102/1000 | Loss: 0.00052217
Iteration 103/1000 | Loss: 0.00044263
Iteration 104/1000 | Loss: 0.00040267
Iteration 105/1000 | Loss: 0.00021060
Iteration 106/1000 | Loss: 0.00028178
Iteration 107/1000 | Loss: 0.00024722
Iteration 108/1000 | Loss: 0.00058565
Iteration 109/1000 | Loss: 0.00031387
Iteration 110/1000 | Loss: 0.00022802
Iteration 111/1000 | Loss: 0.00033508
Iteration 112/1000 | Loss: 0.00025024
Iteration 113/1000 | Loss: 0.00037018
Iteration 114/1000 | Loss: 0.00053229
Iteration 115/1000 | Loss: 0.00138165
Iteration 116/1000 | Loss: 0.00258618
Iteration 117/1000 | Loss: 0.00084058
Iteration 118/1000 | Loss: 0.00045065
Iteration 119/1000 | Loss: 0.00030462
Iteration 120/1000 | Loss: 0.00026759
Iteration 121/1000 | Loss: 0.00026622
Iteration 122/1000 | Loss: 0.00029614
Iteration 123/1000 | Loss: 0.00029913
Iteration 124/1000 | Loss: 0.00023756
Iteration 125/1000 | Loss: 0.00018786
Iteration 126/1000 | Loss: 0.00021374
Iteration 127/1000 | Loss: 0.00018666
Iteration 128/1000 | Loss: 0.00033365
Iteration 129/1000 | Loss: 0.00025437
Iteration 130/1000 | Loss: 0.00025058
Iteration 131/1000 | Loss: 0.00023458
Iteration 132/1000 | Loss: 0.00101025
Iteration 133/1000 | Loss: 0.00125460
Iteration 134/1000 | Loss: 0.00234944
Iteration 135/1000 | Loss: 0.00027752
Iteration 136/1000 | Loss: 0.00054217
Iteration 137/1000 | Loss: 0.00025390
Iteration 138/1000 | Loss: 0.00029296
Iteration 139/1000 | Loss: 0.00026964
Iteration 140/1000 | Loss: 0.00027053
Iteration 141/1000 | Loss: 0.00019783
Iteration 142/1000 | Loss: 0.00074056
Iteration 143/1000 | Loss: 0.00037425
Iteration 144/1000 | Loss: 0.00023622
Iteration 145/1000 | Loss: 0.00027795
Iteration 146/1000 | Loss: 0.00020888
Iteration 147/1000 | Loss: 0.00019883
Iteration 148/1000 | Loss: 0.00016026
Iteration 149/1000 | Loss: 0.00020908
Iteration 150/1000 | Loss: 0.00022696
Iteration 151/1000 | Loss: 0.00028976
Iteration 152/1000 | Loss: 0.00022547
Iteration 153/1000 | Loss: 0.00025453
Iteration 154/1000 | Loss: 0.00024410
Iteration 155/1000 | Loss: 0.00020335
Iteration 156/1000 | Loss: 0.00018527
Iteration 157/1000 | Loss: 0.00023532
Iteration 158/1000 | Loss: 0.00023272
Iteration 159/1000 | Loss: 0.00035301
Iteration 160/1000 | Loss: 0.00024859
Iteration 161/1000 | Loss: 0.00027944
Iteration 162/1000 | Loss: 0.00043522
Iteration 163/1000 | Loss: 0.00020603
Iteration 164/1000 | Loss: 0.00072822
Iteration 165/1000 | Loss: 0.00015196
Iteration 166/1000 | Loss: 0.00023338
Iteration 167/1000 | Loss: 0.00026144
Iteration 168/1000 | Loss: 0.00017668
Iteration 169/1000 | Loss: 0.00013489
Iteration 170/1000 | Loss: 0.00011759
Iteration 171/1000 | Loss: 0.00015547
Iteration 172/1000 | Loss: 0.00030922
Iteration 173/1000 | Loss: 0.00017269
Iteration 174/1000 | Loss: 0.00033070
Iteration 175/1000 | Loss: 0.00016540
Iteration 176/1000 | Loss: 0.00015885
Iteration 177/1000 | Loss: 0.00018078
Iteration 178/1000 | Loss: 0.00015581
Iteration 179/1000 | Loss: 0.00020184
Iteration 180/1000 | Loss: 0.00017627
Iteration 181/1000 | Loss: 0.00020284
Iteration 182/1000 | Loss: 0.00012198
Iteration 183/1000 | Loss: 0.00021815
Iteration 184/1000 | Loss: 0.00019793
Iteration 185/1000 | Loss: 0.00015344
Iteration 186/1000 | Loss: 0.00016701
Iteration 187/1000 | Loss: 0.00014667
Iteration 188/1000 | Loss: 0.00015410
Iteration 189/1000 | Loss: 0.00018198
Iteration 190/1000 | Loss: 0.00014004
Iteration 191/1000 | Loss: 0.00019178
Iteration 192/1000 | Loss: 0.00019152
Iteration 193/1000 | Loss: 0.00019409
Iteration 194/1000 | Loss: 0.00017322
Iteration 195/1000 | Loss: 0.00020325
Iteration 196/1000 | Loss: 0.00019908
Iteration 197/1000 | Loss: 0.00022331
Iteration 198/1000 | Loss: 0.00023473
Iteration 199/1000 | Loss: 0.00021955
Iteration 200/1000 | Loss: 0.00017838
Iteration 201/1000 | Loss: 0.00031062
Iteration 202/1000 | Loss: 0.00011885
Iteration 203/1000 | Loss: 0.00011837
Iteration 204/1000 | Loss: 0.00016420
Iteration 205/1000 | Loss: 0.00021270
Iteration 206/1000 | Loss: 0.00018751
Iteration 207/1000 | Loss: 0.00022716
Iteration 208/1000 | Loss: 0.00021750
Iteration 209/1000 | Loss: 0.00014855
Iteration 210/1000 | Loss: 0.00016227
Iteration 211/1000 | Loss: 0.00020995
Iteration 212/1000 | Loss: 0.00021537
Iteration 213/1000 | Loss: 0.00023245
Iteration 214/1000 | Loss: 0.00014776
Iteration 215/1000 | Loss: 0.00028944
Iteration 216/1000 | Loss: 0.00022208
Iteration 217/1000 | Loss: 0.00023123
Iteration 218/1000 | Loss: 0.00025894
Iteration 219/1000 | Loss: 0.00023716
Iteration 220/1000 | Loss: 0.00023130
Iteration 221/1000 | Loss: 0.00073903
Iteration 222/1000 | Loss: 0.00044261
Iteration 223/1000 | Loss: 0.00033476
Iteration 224/1000 | Loss: 0.00025988
Iteration 225/1000 | Loss: 0.00030584
Iteration 226/1000 | Loss: 0.00024550
Iteration 227/1000 | Loss: 0.00039576
Iteration 228/1000 | Loss: 0.00023648
Iteration 229/1000 | Loss: 0.00041171
Iteration 230/1000 | Loss: 0.00022060
Iteration 231/1000 | Loss: 0.00014969
Iteration 232/1000 | Loss: 0.00014642
Iteration 233/1000 | Loss: 0.00014607
Iteration 234/1000 | Loss: 0.00059425
Iteration 235/1000 | Loss: 0.00033040
Iteration 236/1000 | Loss: 0.00011446
Iteration 237/1000 | Loss: 0.00010878
Iteration 238/1000 | Loss: 0.00070864
Iteration 239/1000 | Loss: 0.00011958
Iteration 240/1000 | Loss: 0.00011235
Iteration 241/1000 | Loss: 0.00010572
Iteration 242/1000 | Loss: 0.00010465
Iteration 243/1000 | Loss: 0.00059513
Iteration 244/1000 | Loss: 0.00046163
Iteration 245/1000 | Loss: 0.00057283
Iteration 246/1000 | Loss: 0.00011879
Iteration 247/1000 | Loss: 0.00010915
Iteration 248/1000 | Loss: 0.00049355
Iteration 249/1000 | Loss: 0.00029912
Iteration 250/1000 | Loss: 0.00011382
Iteration 251/1000 | Loss: 0.00060711
Iteration 252/1000 | Loss: 0.00015717
Iteration 253/1000 | Loss: 0.00012087
Iteration 254/1000 | Loss: 0.00011133
Iteration 255/1000 | Loss: 0.00011224
Iteration 256/1000 | Loss: 0.00023549
Iteration 257/1000 | Loss: 0.00010951
Iteration 258/1000 | Loss: 0.00025166
Iteration 259/1000 | Loss: 0.00013142
Iteration 260/1000 | Loss: 0.00010873
Iteration 261/1000 | Loss: 0.00010550
Iteration 262/1000 | Loss: 0.00010249
Iteration 263/1000 | Loss: 0.00010124
Iteration 264/1000 | Loss: 0.00009998
Iteration 265/1000 | Loss: 0.00033461
Iteration 266/1000 | Loss: 0.00030196
Iteration 267/1000 | Loss: 0.00010752
Iteration 268/1000 | Loss: 0.00028091
Iteration 269/1000 | Loss: 0.00012536
Iteration 270/1000 | Loss: 0.00010325
Iteration 271/1000 | Loss: 0.00010060
Iteration 272/1000 | Loss: 0.00010745
Iteration 273/1000 | Loss: 0.00010318
Iteration 274/1000 | Loss: 0.00010523
Iteration 275/1000 | Loss: 0.00010111
Iteration 276/1000 | Loss: 0.00009962
Iteration 277/1000 | Loss: 0.00009881
Iteration 278/1000 | Loss: 0.00010297
Iteration 279/1000 | Loss: 0.00010179
Iteration 280/1000 | Loss: 0.00010102
Iteration 281/1000 | Loss: 0.00010122
Iteration 282/1000 | Loss: 0.00010139
Iteration 283/1000 | Loss: 0.00010298
Iteration 284/1000 | Loss: 0.00010118
Iteration 285/1000 | Loss: 0.00010051
Iteration 286/1000 | Loss: 0.00009920
Iteration 287/1000 | Loss: 0.00010087
Iteration 288/1000 | Loss: 0.00011274
Iteration 289/1000 | Loss: 0.00010767
Iteration 290/1000 | Loss: 0.00009960
Iteration 291/1000 | Loss: 0.00010089
Iteration 292/1000 | Loss: 0.00009958
Iteration 293/1000 | Loss: 0.00010024
Iteration 294/1000 | Loss: 0.00009913
Iteration 295/1000 | Loss: 0.00010029
Iteration 296/1000 | Loss: 0.00009873
Iteration 297/1000 | Loss: 0.00009555
Iteration 298/1000 | Loss: 0.00009533
Iteration 299/1000 | Loss: 0.00009501
Iteration 300/1000 | Loss: 0.00009427
Iteration 301/1000 | Loss: 0.00009970
Iteration 302/1000 | Loss: 0.00009647
Iteration 303/1000 | Loss: 0.00009520
Iteration 304/1000 | Loss: 0.00009366
Iteration 305/1000 | Loss: 0.00009305
Iteration 306/1000 | Loss: 0.00009285
Iteration 307/1000 | Loss: 0.00009282
Iteration 308/1000 | Loss: 0.00009265
Iteration 309/1000 | Loss: 0.00009262
Iteration 310/1000 | Loss: 0.00009257
Iteration 311/1000 | Loss: 0.00009254
Iteration 312/1000 | Loss: 0.00009254
Iteration 313/1000 | Loss: 0.00009253
Iteration 314/1000 | Loss: 0.00009253
Iteration 315/1000 | Loss: 0.00009253
Iteration 316/1000 | Loss: 0.00009253
Iteration 317/1000 | Loss: 0.00009252
Iteration 318/1000 | Loss: 0.00009252
Iteration 319/1000 | Loss: 0.00009252
Iteration 320/1000 | Loss: 0.00009251
Iteration 321/1000 | Loss: 0.00009251
Iteration 322/1000 | Loss: 0.00009251
Iteration 323/1000 | Loss: 0.00009250
Iteration 324/1000 | Loss: 0.00009250
Iteration 325/1000 | Loss: 0.00009250
Iteration 326/1000 | Loss: 0.00009249
Iteration 327/1000 | Loss: 0.00009249
Iteration 328/1000 | Loss: 0.00009249
Iteration 329/1000 | Loss: 0.00009249
Iteration 330/1000 | Loss: 0.00009249
Iteration 331/1000 | Loss: 0.00009249
Iteration 332/1000 | Loss: 0.00009248
Iteration 333/1000 | Loss: 0.00009248
Iteration 334/1000 | Loss: 0.00009248
Iteration 335/1000 | Loss: 0.00009247
Iteration 336/1000 | Loss: 0.00009247
Iteration 337/1000 | Loss: 0.00009246
Iteration 338/1000 | Loss: 0.00009246
Iteration 339/1000 | Loss: 0.00009246
Iteration 340/1000 | Loss: 0.00009246
Iteration 341/1000 | Loss: 0.00009246
Iteration 342/1000 | Loss: 0.00009245
Iteration 343/1000 | Loss: 0.00009245
Iteration 344/1000 | Loss: 0.00009245
Iteration 345/1000 | Loss: 0.00009245
Iteration 346/1000 | Loss: 0.00009244
Iteration 347/1000 | Loss: 0.00009244
Iteration 348/1000 | Loss: 0.00009244
Iteration 349/1000 | Loss: 0.00009244
Iteration 350/1000 | Loss: 0.00009244
Iteration 351/1000 | Loss: 0.00009244
Iteration 352/1000 | Loss: 0.00009244
Iteration 353/1000 | Loss: 0.00009244
Iteration 354/1000 | Loss: 0.00009244
Iteration 355/1000 | Loss: 0.00009244
Iteration 356/1000 | Loss: 0.00009244
Iteration 357/1000 | Loss: 0.00009244
Iteration 358/1000 | Loss: 0.00009244
Iteration 359/1000 | Loss: 0.00009243
Iteration 360/1000 | Loss: 0.00009243
Iteration 361/1000 | Loss: 0.00009243
Iteration 362/1000 | Loss: 0.00009243
Iteration 363/1000 | Loss: 0.00009243
Iteration 364/1000 | Loss: 0.00009243
Iteration 365/1000 | Loss: 0.00009243
Iteration 366/1000 | Loss: 0.00009243
Iteration 367/1000 | Loss: 0.00009243
Iteration 368/1000 | Loss: 0.00009243
Iteration 369/1000 | Loss: 0.00009242
Iteration 370/1000 | Loss: 0.00009242
Iteration 371/1000 | Loss: 0.00009242
Iteration 372/1000 | Loss: 0.00009242
Iteration 373/1000 | Loss: 0.00009242
Iteration 374/1000 | Loss: 0.00009242
Iteration 375/1000 | Loss: 0.00009242
Iteration 376/1000 | Loss: 0.00009242
Iteration 377/1000 | Loss: 0.00009241
Iteration 378/1000 | Loss: 0.00009241
Iteration 379/1000 | Loss: 0.00009241
Iteration 380/1000 | Loss: 0.00009241
Iteration 381/1000 | Loss: 0.00010523
Iteration 382/1000 | Loss: 0.00009891
Iteration 383/1000 | Loss: 0.00009240
Iteration 384/1000 | Loss: 0.00009240
Iteration 385/1000 | Loss: 0.00009240
Iteration 386/1000 | Loss: 0.00009240
Iteration 387/1000 | Loss: 0.00009240
Iteration 388/1000 | Loss: 0.00009240
Iteration 389/1000 | Loss: 0.00009240
Iteration 390/1000 | Loss: 0.00009240
Iteration 391/1000 | Loss: 0.00009240
Iteration 392/1000 | Loss: 0.00010436
Iteration 393/1000 | Loss: 0.00009860
Iteration 394/1000 | Loss: 0.00009245
Iteration 395/1000 | Loss: 0.00009239
Iteration 396/1000 | Loss: 0.00009239
Iteration 397/1000 | Loss: 0.00009239
Iteration 398/1000 | Loss: 0.00009239
Iteration 399/1000 | Loss: 0.00009239
Iteration 400/1000 | Loss: 0.00009238
Iteration 401/1000 | Loss: 0.00009238
Iteration 402/1000 | Loss: 0.00009238
Iteration 403/1000 | Loss: 0.00009238
Iteration 404/1000 | Loss: 0.00009238
Iteration 405/1000 | Loss: 0.00009238
Iteration 406/1000 | Loss: 0.00009238
Iteration 407/1000 | Loss: 0.00009238
Iteration 408/1000 | Loss: 0.00009238
Iteration 409/1000 | Loss: 0.00009238
Iteration 410/1000 | Loss: 0.00009238
Iteration 411/1000 | Loss: 0.00009237
Iteration 412/1000 | Loss: 0.00009237
Iteration 413/1000 | Loss: 0.00009237
Iteration 414/1000 | Loss: 0.00009237
Iteration 415/1000 | Loss: 0.00009237
Iteration 416/1000 | Loss: 0.00009237
Iteration 417/1000 | Loss: 0.00009237
Iteration 418/1000 | Loss: 0.00009237
Iteration 419/1000 | Loss: 0.00009237
Iteration 420/1000 | Loss: 0.00009237
Iteration 421/1000 | Loss: 0.00009237
Iteration 422/1000 | Loss: 0.00009236
Iteration 423/1000 | Loss: 0.00009236
Iteration 424/1000 | Loss: 0.00009236
Iteration 425/1000 | Loss: 0.00009236
Iteration 426/1000 | Loss: 0.00009236
Iteration 427/1000 | Loss: 0.00009236
Iteration 428/1000 | Loss: 0.00009236
Iteration 429/1000 | Loss: 0.00009236
Iteration 430/1000 | Loss: 0.00009235
Iteration 431/1000 | Loss: 0.00009235
Iteration 432/1000 | Loss: 0.00009235
Iteration 433/1000 | Loss: 0.00009235
Iteration 434/1000 | Loss: 0.00009235
Iteration 435/1000 | Loss: 0.00009235
Iteration 436/1000 | Loss: 0.00009235
Iteration 437/1000 | Loss: 0.00009235
Iteration 438/1000 | Loss: 0.00009235
Iteration 439/1000 | Loss: 0.00009234
Iteration 440/1000 | Loss: 0.00009234
Iteration 441/1000 | Loss: 0.00009234
Iteration 442/1000 | Loss: 0.00009234
Iteration 443/1000 | Loss: 0.00009234
Iteration 444/1000 | Loss: 0.00009234
Iteration 445/1000 | Loss: 0.00009234
Iteration 446/1000 | Loss: 0.00009234
Iteration 447/1000 | Loss: 0.00009234
Iteration 448/1000 | Loss: 0.00009234
Iteration 449/1000 | Loss: 0.00009234
Iteration 450/1000 | Loss: 0.00009233
Iteration 451/1000 | Loss: 0.00009233
Iteration 452/1000 | Loss: 0.00009233
Iteration 453/1000 | Loss: 0.00009233
Iteration 454/1000 | Loss: 0.00009233
Iteration 455/1000 | Loss: 0.00009233
Iteration 456/1000 | Loss: 0.00009233
Iteration 457/1000 | Loss: 0.00009233
Iteration 458/1000 | Loss: 0.00009233
Iteration 459/1000 | Loss: 0.00009233
Iteration 460/1000 | Loss: 0.00009233
Iteration 461/1000 | Loss: 0.00009233
Iteration 462/1000 | Loss: 0.00009233
Iteration 463/1000 | Loss: 0.00009233
Iteration 464/1000 | Loss: 0.00009233
Iteration 465/1000 | Loss: 0.00009233
Iteration 466/1000 | Loss: 0.00009233
Iteration 467/1000 | Loss: 0.00009233
Iteration 468/1000 | Loss: 0.00009233
Iteration 469/1000 | Loss: 0.00009233
Iteration 470/1000 | Loss: 0.00009233
Iteration 471/1000 | Loss: 0.00009233
Iteration 472/1000 | Loss: 0.00009233
Iteration 473/1000 | Loss: 0.00009233
Iteration 474/1000 | Loss: 0.00009233
Iteration 475/1000 | Loss: 0.00009233
Iteration 476/1000 | Loss: 0.00009233
Iteration 477/1000 | Loss: 0.00009233
Iteration 478/1000 | Loss: 0.00009233
Iteration 479/1000 | Loss: 0.00009233
Iteration 480/1000 | Loss: 0.00009233
Iteration 481/1000 | Loss: 0.00009233
Iteration 482/1000 | Loss: 0.00009233
Iteration 483/1000 | Loss: 0.00009233
Iteration 484/1000 | Loss: 0.00009233
Iteration 485/1000 | Loss: 0.00009233
Iteration 486/1000 | Loss: 0.00009233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 486. Stopping optimization.
Last 5 losses: [9.233302989741787e-05, 9.233302989741787e-05, 9.233302989741787e-05, 9.233302989741787e-05, 9.233302989741787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.233302989741787e-05

Optimization complete. Final v2v error: 5.653097629547119 mm

Highest mean error: 13.1144380569458 mm for frame 96

Lowest mean error: 3.545485258102417 mm for frame 36

Saving results

Total time: 555.3341643810272
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392024
Iteration 2/25 | Loss: 0.00097158
Iteration 3/25 | Loss: 0.00065256
Iteration 4/25 | Loss: 0.00061003
Iteration 5/25 | Loss: 0.00060016
Iteration 6/25 | Loss: 0.00059744
Iteration 7/25 | Loss: 0.00059682
Iteration 8/25 | Loss: 0.00059681
Iteration 9/25 | Loss: 0.00059681
Iteration 10/25 | Loss: 0.00059681
Iteration 11/25 | Loss: 0.00059681
Iteration 12/25 | Loss: 0.00059681
Iteration 13/25 | Loss: 0.00059681
Iteration 14/25 | Loss: 0.00059681
Iteration 15/25 | Loss: 0.00059681
Iteration 16/25 | Loss: 0.00059681
Iteration 17/25 | Loss: 0.00059681
Iteration 18/25 | Loss: 0.00059681
Iteration 19/25 | Loss: 0.00059681
Iteration 20/25 | Loss: 0.00059681
Iteration 21/25 | Loss: 0.00059681
Iteration 22/25 | Loss: 0.00059681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005968094919808209, 0.0005968094919808209, 0.0005968094919808209, 0.0005968094919808209, 0.0005968094919808209]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005968094919808209

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45338571
Iteration 2/25 | Loss: 0.00025139
Iteration 3/25 | Loss: 0.00025139
Iteration 4/25 | Loss: 0.00025139
Iteration 5/25 | Loss: 0.00025139
Iteration 6/25 | Loss: 0.00025139
Iteration 7/25 | Loss: 0.00025139
Iteration 8/25 | Loss: 0.00025139
Iteration 9/25 | Loss: 0.00025139
Iteration 10/25 | Loss: 0.00025139
Iteration 11/25 | Loss: 0.00025139
Iteration 12/25 | Loss: 0.00025139
Iteration 13/25 | Loss: 0.00025139
Iteration 14/25 | Loss: 0.00025139
Iteration 15/25 | Loss: 0.00025139
Iteration 16/25 | Loss: 0.00025139
Iteration 17/25 | Loss: 0.00025139
Iteration 18/25 | Loss: 0.00025139
Iteration 19/25 | Loss: 0.00025139
Iteration 20/25 | Loss: 0.00025139
Iteration 21/25 | Loss: 0.00025139
Iteration 22/25 | Loss: 0.00025139
Iteration 23/25 | Loss: 0.00025139
Iteration 24/25 | Loss: 0.00025139
Iteration 25/25 | Loss: 0.00025139
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00025138744967989624, 0.00025138744967989624, 0.00025138744967989624, 0.00025138744967989624, 0.00025138744967989624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025138744967989624

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025139
Iteration 2/1000 | Loss: 0.00001944
Iteration 3/1000 | Loss: 0.00001469
Iteration 4/1000 | Loss: 0.00001368
Iteration 5/1000 | Loss: 0.00001300
Iteration 6/1000 | Loss: 0.00001253
Iteration 7/1000 | Loss: 0.00001221
Iteration 8/1000 | Loss: 0.00001210
Iteration 9/1000 | Loss: 0.00001207
Iteration 10/1000 | Loss: 0.00001203
Iteration 11/1000 | Loss: 0.00001202
Iteration 12/1000 | Loss: 0.00001201
Iteration 13/1000 | Loss: 0.00001201
Iteration 14/1000 | Loss: 0.00001201
Iteration 15/1000 | Loss: 0.00001196
Iteration 16/1000 | Loss: 0.00001192
Iteration 17/1000 | Loss: 0.00001190
Iteration 18/1000 | Loss: 0.00001189
Iteration 19/1000 | Loss: 0.00001189
Iteration 20/1000 | Loss: 0.00001188
Iteration 21/1000 | Loss: 0.00001188
Iteration 22/1000 | Loss: 0.00001186
Iteration 23/1000 | Loss: 0.00001185
Iteration 24/1000 | Loss: 0.00001183
Iteration 25/1000 | Loss: 0.00001183
Iteration 26/1000 | Loss: 0.00001182
Iteration 27/1000 | Loss: 0.00001181
Iteration 28/1000 | Loss: 0.00001181
Iteration 29/1000 | Loss: 0.00001180
Iteration 30/1000 | Loss: 0.00001180
Iteration 31/1000 | Loss: 0.00001179
Iteration 32/1000 | Loss: 0.00001179
Iteration 33/1000 | Loss: 0.00001178
Iteration 34/1000 | Loss: 0.00001177
Iteration 35/1000 | Loss: 0.00001176
Iteration 36/1000 | Loss: 0.00001174
Iteration 37/1000 | Loss: 0.00001174
Iteration 38/1000 | Loss: 0.00001174
Iteration 39/1000 | Loss: 0.00001174
Iteration 40/1000 | Loss: 0.00001174
Iteration 41/1000 | Loss: 0.00001174
Iteration 42/1000 | Loss: 0.00001174
Iteration 43/1000 | Loss: 0.00001174
Iteration 44/1000 | Loss: 0.00001174
Iteration 45/1000 | Loss: 0.00001173
Iteration 46/1000 | Loss: 0.00001173
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001173
Iteration 49/1000 | Loss: 0.00001172
Iteration 50/1000 | Loss: 0.00001172
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001171
Iteration 53/1000 | Loss: 0.00001171
Iteration 54/1000 | Loss: 0.00001171
Iteration 55/1000 | Loss: 0.00001171
Iteration 56/1000 | Loss: 0.00001171
Iteration 57/1000 | Loss: 0.00001171
Iteration 58/1000 | Loss: 0.00001171
Iteration 59/1000 | Loss: 0.00001171
Iteration 60/1000 | Loss: 0.00001171
Iteration 61/1000 | Loss: 0.00001171
Iteration 62/1000 | Loss: 0.00001170
Iteration 63/1000 | Loss: 0.00001170
Iteration 64/1000 | Loss: 0.00001170
Iteration 65/1000 | Loss: 0.00001169
Iteration 66/1000 | Loss: 0.00001169
Iteration 67/1000 | Loss: 0.00001169
Iteration 68/1000 | Loss: 0.00001169
Iteration 69/1000 | Loss: 0.00001168
Iteration 70/1000 | Loss: 0.00001168
Iteration 71/1000 | Loss: 0.00001168
Iteration 72/1000 | Loss: 0.00001167
Iteration 73/1000 | Loss: 0.00001167
Iteration 74/1000 | Loss: 0.00001166
Iteration 75/1000 | Loss: 0.00001166
Iteration 76/1000 | Loss: 0.00001166
Iteration 77/1000 | Loss: 0.00001165
Iteration 78/1000 | Loss: 0.00001165
Iteration 79/1000 | Loss: 0.00001165
Iteration 80/1000 | Loss: 0.00001164
Iteration 81/1000 | Loss: 0.00001164
Iteration 82/1000 | Loss: 0.00001164
Iteration 83/1000 | Loss: 0.00001164
Iteration 84/1000 | Loss: 0.00001164
Iteration 85/1000 | Loss: 0.00001163
Iteration 86/1000 | Loss: 0.00001163
Iteration 87/1000 | Loss: 0.00001163
Iteration 88/1000 | Loss: 0.00001163
Iteration 89/1000 | Loss: 0.00001162
Iteration 90/1000 | Loss: 0.00001162
Iteration 91/1000 | Loss: 0.00001162
Iteration 92/1000 | Loss: 0.00001162
Iteration 93/1000 | Loss: 0.00001162
Iteration 94/1000 | Loss: 0.00001162
Iteration 95/1000 | Loss: 0.00001162
Iteration 96/1000 | Loss: 0.00001161
Iteration 97/1000 | Loss: 0.00001161
Iteration 98/1000 | Loss: 0.00001161
Iteration 99/1000 | Loss: 0.00001161
Iteration 100/1000 | Loss: 0.00001161
Iteration 101/1000 | Loss: 0.00001161
Iteration 102/1000 | Loss: 0.00001161
Iteration 103/1000 | Loss: 0.00001160
Iteration 104/1000 | Loss: 0.00001160
Iteration 105/1000 | Loss: 0.00001160
Iteration 106/1000 | Loss: 0.00001160
Iteration 107/1000 | Loss: 0.00001160
Iteration 108/1000 | Loss: 0.00001160
Iteration 109/1000 | Loss: 0.00001159
Iteration 110/1000 | Loss: 0.00001159
Iteration 111/1000 | Loss: 0.00001159
Iteration 112/1000 | Loss: 0.00001159
Iteration 113/1000 | Loss: 0.00001159
Iteration 114/1000 | Loss: 0.00001158
Iteration 115/1000 | Loss: 0.00001158
Iteration 116/1000 | Loss: 0.00001158
Iteration 117/1000 | Loss: 0.00001158
Iteration 118/1000 | Loss: 0.00001158
Iteration 119/1000 | Loss: 0.00001158
Iteration 120/1000 | Loss: 0.00001158
Iteration 121/1000 | Loss: 0.00001158
Iteration 122/1000 | Loss: 0.00001158
Iteration 123/1000 | Loss: 0.00001158
Iteration 124/1000 | Loss: 0.00001157
Iteration 125/1000 | Loss: 0.00001157
Iteration 126/1000 | Loss: 0.00001157
Iteration 127/1000 | Loss: 0.00001157
Iteration 128/1000 | Loss: 0.00001157
Iteration 129/1000 | Loss: 0.00001157
Iteration 130/1000 | Loss: 0.00001157
Iteration 131/1000 | Loss: 0.00001156
Iteration 132/1000 | Loss: 0.00001156
Iteration 133/1000 | Loss: 0.00001156
Iteration 134/1000 | Loss: 0.00001156
Iteration 135/1000 | Loss: 0.00001155
Iteration 136/1000 | Loss: 0.00001155
Iteration 137/1000 | Loss: 0.00001155
Iteration 138/1000 | Loss: 0.00001155
Iteration 139/1000 | Loss: 0.00001155
Iteration 140/1000 | Loss: 0.00001154
Iteration 141/1000 | Loss: 0.00001154
Iteration 142/1000 | Loss: 0.00001154
Iteration 143/1000 | Loss: 0.00001154
Iteration 144/1000 | Loss: 0.00001154
Iteration 145/1000 | Loss: 0.00001154
Iteration 146/1000 | Loss: 0.00001153
Iteration 147/1000 | Loss: 0.00001153
Iteration 148/1000 | Loss: 0.00001153
Iteration 149/1000 | Loss: 0.00001153
Iteration 150/1000 | Loss: 0.00001153
Iteration 151/1000 | Loss: 0.00001153
Iteration 152/1000 | Loss: 0.00001153
Iteration 153/1000 | Loss: 0.00001153
Iteration 154/1000 | Loss: 0.00001153
Iteration 155/1000 | Loss: 0.00001153
Iteration 156/1000 | Loss: 0.00001153
Iteration 157/1000 | Loss: 0.00001152
Iteration 158/1000 | Loss: 0.00001152
Iteration 159/1000 | Loss: 0.00001152
Iteration 160/1000 | Loss: 0.00001152
Iteration 161/1000 | Loss: 0.00001152
Iteration 162/1000 | Loss: 0.00001151
Iteration 163/1000 | Loss: 0.00001151
Iteration 164/1000 | Loss: 0.00001151
Iteration 165/1000 | Loss: 0.00001151
Iteration 166/1000 | Loss: 0.00001151
Iteration 167/1000 | Loss: 0.00001151
Iteration 168/1000 | Loss: 0.00001151
Iteration 169/1000 | Loss: 0.00001151
Iteration 170/1000 | Loss: 0.00001150
Iteration 171/1000 | Loss: 0.00001150
Iteration 172/1000 | Loss: 0.00001150
Iteration 173/1000 | Loss: 0.00001150
Iteration 174/1000 | Loss: 0.00001150
Iteration 175/1000 | Loss: 0.00001150
Iteration 176/1000 | Loss: 0.00001150
Iteration 177/1000 | Loss: 0.00001150
Iteration 178/1000 | Loss: 0.00001150
Iteration 179/1000 | Loss: 0.00001150
Iteration 180/1000 | Loss: 0.00001150
Iteration 181/1000 | Loss: 0.00001150
Iteration 182/1000 | Loss: 0.00001150
Iteration 183/1000 | Loss: 0.00001150
Iteration 184/1000 | Loss: 0.00001150
Iteration 185/1000 | Loss: 0.00001150
Iteration 186/1000 | Loss: 0.00001150
Iteration 187/1000 | Loss: 0.00001150
Iteration 188/1000 | Loss: 0.00001150
Iteration 189/1000 | Loss: 0.00001150
Iteration 190/1000 | Loss: 0.00001150
Iteration 191/1000 | Loss: 0.00001150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.1503377209010068e-05, 1.1503377209010068e-05, 1.1503377209010068e-05, 1.1503377209010068e-05, 1.1503377209010068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1503377209010068e-05

Optimization complete. Final v2v error: 2.910203218460083 mm

Highest mean error: 2.9691808223724365 mm for frame 85

Lowest mean error: 2.845383644104004 mm for frame 101

Saving results

Total time: 35.93646025657654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055349
Iteration 2/25 | Loss: 0.00119628
Iteration 3/25 | Loss: 0.00081454
Iteration 4/25 | Loss: 0.00074425
Iteration 5/25 | Loss: 0.00073392
Iteration 6/25 | Loss: 0.00073141
Iteration 7/25 | Loss: 0.00073090
Iteration 8/25 | Loss: 0.00073090
Iteration 9/25 | Loss: 0.00073090
Iteration 10/25 | Loss: 0.00073090
Iteration 11/25 | Loss: 0.00073090
Iteration 12/25 | Loss: 0.00073090
Iteration 13/25 | Loss: 0.00073090
Iteration 14/25 | Loss: 0.00073090
Iteration 15/25 | Loss: 0.00073090
Iteration 16/25 | Loss: 0.00073090
Iteration 17/25 | Loss: 0.00073090
Iteration 18/25 | Loss: 0.00073090
Iteration 19/25 | Loss: 0.00073090
Iteration 20/25 | Loss: 0.00073090
Iteration 21/25 | Loss: 0.00073090
Iteration 22/25 | Loss: 0.00073090
Iteration 23/25 | Loss: 0.00073090
Iteration 24/25 | Loss: 0.00073090
Iteration 25/25 | Loss: 0.00073090

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58507884
Iteration 2/25 | Loss: 0.00038531
Iteration 3/25 | Loss: 0.00038530
Iteration 4/25 | Loss: 0.00038530
Iteration 5/25 | Loss: 0.00038530
Iteration 6/25 | Loss: 0.00038530
Iteration 7/25 | Loss: 0.00038530
Iteration 8/25 | Loss: 0.00038530
Iteration 9/25 | Loss: 0.00038530
Iteration 10/25 | Loss: 0.00038530
Iteration 11/25 | Loss: 0.00038530
Iteration 12/25 | Loss: 0.00038530
Iteration 13/25 | Loss: 0.00038530
Iteration 14/25 | Loss: 0.00038530
Iteration 15/25 | Loss: 0.00038530
Iteration 16/25 | Loss: 0.00038530
Iteration 17/25 | Loss: 0.00038530
Iteration 18/25 | Loss: 0.00038530
Iteration 19/25 | Loss: 0.00038530
Iteration 20/25 | Loss: 0.00038530
Iteration 21/25 | Loss: 0.00038530
Iteration 22/25 | Loss: 0.00038530
Iteration 23/25 | Loss: 0.00038530
Iteration 24/25 | Loss: 0.00038530
Iteration 25/25 | Loss: 0.00038530

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038530
Iteration 2/1000 | Loss: 0.00003742
Iteration 3/1000 | Loss: 0.00002637
Iteration 4/1000 | Loss: 0.00002427
Iteration 5/1000 | Loss: 0.00002287
Iteration 6/1000 | Loss: 0.00002213
Iteration 7/1000 | Loss: 0.00002164
Iteration 8/1000 | Loss: 0.00002117
Iteration 9/1000 | Loss: 0.00002089
Iteration 10/1000 | Loss: 0.00002064
Iteration 11/1000 | Loss: 0.00002059
Iteration 12/1000 | Loss: 0.00002043
Iteration 13/1000 | Loss: 0.00002032
Iteration 14/1000 | Loss: 0.00002032
Iteration 15/1000 | Loss: 0.00002026
Iteration 16/1000 | Loss: 0.00002023
Iteration 17/1000 | Loss: 0.00002021
Iteration 18/1000 | Loss: 0.00002021
Iteration 19/1000 | Loss: 0.00002021
Iteration 20/1000 | Loss: 0.00002020
Iteration 21/1000 | Loss: 0.00002020
Iteration 22/1000 | Loss: 0.00002020
Iteration 23/1000 | Loss: 0.00002019
Iteration 24/1000 | Loss: 0.00002018
Iteration 25/1000 | Loss: 0.00002017
Iteration 26/1000 | Loss: 0.00002017
Iteration 27/1000 | Loss: 0.00002017
Iteration 28/1000 | Loss: 0.00002016
Iteration 29/1000 | Loss: 0.00002016
Iteration 30/1000 | Loss: 0.00002016
Iteration 31/1000 | Loss: 0.00002014
Iteration 32/1000 | Loss: 0.00002014
Iteration 33/1000 | Loss: 0.00002014
Iteration 34/1000 | Loss: 0.00002013
Iteration 35/1000 | Loss: 0.00002013
Iteration 36/1000 | Loss: 0.00002013
Iteration 37/1000 | Loss: 0.00002013
Iteration 38/1000 | Loss: 0.00002013
Iteration 39/1000 | Loss: 0.00002012
Iteration 40/1000 | Loss: 0.00002012
Iteration 41/1000 | Loss: 0.00002011
Iteration 42/1000 | Loss: 0.00002011
Iteration 43/1000 | Loss: 0.00002011
Iteration 44/1000 | Loss: 0.00002010
Iteration 45/1000 | Loss: 0.00002010
Iteration 46/1000 | Loss: 0.00002010
Iteration 47/1000 | Loss: 0.00002010
Iteration 48/1000 | Loss: 0.00002010
Iteration 49/1000 | Loss: 0.00002010
Iteration 50/1000 | Loss: 0.00002010
Iteration 51/1000 | Loss: 0.00002010
Iteration 52/1000 | Loss: 0.00002010
Iteration 53/1000 | Loss: 0.00002010
Iteration 54/1000 | Loss: 0.00002010
Iteration 55/1000 | Loss: 0.00002009
Iteration 56/1000 | Loss: 0.00002009
Iteration 57/1000 | Loss: 0.00002009
Iteration 58/1000 | Loss: 0.00002009
Iteration 59/1000 | Loss: 0.00002008
Iteration 60/1000 | Loss: 0.00002008
Iteration 61/1000 | Loss: 0.00002007
Iteration 62/1000 | Loss: 0.00002007
Iteration 63/1000 | Loss: 0.00002007
Iteration 64/1000 | Loss: 0.00002007
Iteration 65/1000 | Loss: 0.00002007
Iteration 66/1000 | Loss: 0.00002007
Iteration 67/1000 | Loss: 0.00002007
Iteration 68/1000 | Loss: 0.00002007
Iteration 69/1000 | Loss: 0.00002006
Iteration 70/1000 | Loss: 0.00002006
Iteration 71/1000 | Loss: 0.00002005
Iteration 72/1000 | Loss: 0.00002005
Iteration 73/1000 | Loss: 0.00002005
Iteration 74/1000 | Loss: 0.00002005
Iteration 75/1000 | Loss: 0.00002005
Iteration 76/1000 | Loss: 0.00002004
Iteration 77/1000 | Loss: 0.00002004
Iteration 78/1000 | Loss: 0.00002004
Iteration 79/1000 | Loss: 0.00002004
Iteration 80/1000 | Loss: 0.00002004
Iteration 81/1000 | Loss: 0.00002003
Iteration 82/1000 | Loss: 0.00002003
Iteration 83/1000 | Loss: 0.00002003
Iteration 84/1000 | Loss: 0.00002003
Iteration 85/1000 | Loss: 0.00002003
Iteration 86/1000 | Loss: 0.00002003
Iteration 87/1000 | Loss: 0.00002003
Iteration 88/1000 | Loss: 0.00002003
Iteration 89/1000 | Loss: 0.00002003
Iteration 90/1000 | Loss: 0.00002003
Iteration 91/1000 | Loss: 0.00002003
Iteration 92/1000 | Loss: 0.00002003
Iteration 93/1000 | Loss: 0.00002003
Iteration 94/1000 | Loss: 0.00002003
Iteration 95/1000 | Loss: 0.00002002
Iteration 96/1000 | Loss: 0.00002002
Iteration 97/1000 | Loss: 0.00002002
Iteration 98/1000 | Loss: 0.00002002
Iteration 99/1000 | Loss: 0.00002002
Iteration 100/1000 | Loss: 0.00002002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.0024977857246995e-05, 2.0024977857246995e-05, 2.0024977857246995e-05, 2.0024977857246995e-05, 2.0024977857246995e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0024977857246995e-05

Optimization complete. Final v2v error: 3.801203966140747 mm

Highest mean error: 4.302515506744385 mm for frame 124

Lowest mean error: 3.3235859870910645 mm for frame 40

Saving results

Total time: 36.340786933898926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00469651
Iteration 2/25 | Loss: 0.00092103
Iteration 3/25 | Loss: 0.00072681
Iteration 4/25 | Loss: 0.00069984
Iteration 5/25 | Loss: 0.00069241
Iteration 6/25 | Loss: 0.00069094
Iteration 7/25 | Loss: 0.00069079
Iteration 8/25 | Loss: 0.00069079
Iteration 9/25 | Loss: 0.00069079
Iteration 10/25 | Loss: 0.00069079
Iteration 11/25 | Loss: 0.00069079
Iteration 12/25 | Loss: 0.00069079
Iteration 13/25 | Loss: 0.00069079
Iteration 14/25 | Loss: 0.00069079
Iteration 15/25 | Loss: 0.00069079
Iteration 16/25 | Loss: 0.00069079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006907873903401196, 0.0006907873903401196, 0.0006907873903401196, 0.0006907873903401196, 0.0006907873903401196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006907873903401196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47657681
Iteration 2/25 | Loss: 0.00033664
Iteration 3/25 | Loss: 0.00033663
Iteration 4/25 | Loss: 0.00033663
Iteration 5/25 | Loss: 0.00033663
Iteration 6/25 | Loss: 0.00033663
Iteration 7/25 | Loss: 0.00033663
Iteration 8/25 | Loss: 0.00033663
Iteration 9/25 | Loss: 0.00033663
Iteration 10/25 | Loss: 0.00033663
Iteration 11/25 | Loss: 0.00033663
Iteration 12/25 | Loss: 0.00033663
Iteration 13/25 | Loss: 0.00033663
Iteration 14/25 | Loss: 0.00033663
Iteration 15/25 | Loss: 0.00033663
Iteration 16/25 | Loss: 0.00033663
Iteration 17/25 | Loss: 0.00033663
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00033663009526208043, 0.00033663009526208043, 0.00033663009526208043, 0.00033663009526208043, 0.00033663009526208043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033663009526208043

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033663
Iteration 2/1000 | Loss: 0.00003904
Iteration 3/1000 | Loss: 0.00002670
Iteration 4/1000 | Loss: 0.00002493
Iteration 5/1000 | Loss: 0.00002373
Iteration 6/1000 | Loss: 0.00002310
Iteration 7/1000 | Loss: 0.00002250
Iteration 8/1000 | Loss: 0.00002209
Iteration 9/1000 | Loss: 0.00002179
Iteration 10/1000 | Loss: 0.00002159
Iteration 11/1000 | Loss: 0.00002147
Iteration 12/1000 | Loss: 0.00002137
Iteration 13/1000 | Loss: 0.00002133
Iteration 14/1000 | Loss: 0.00002132
Iteration 15/1000 | Loss: 0.00002132
Iteration 16/1000 | Loss: 0.00002131
Iteration 17/1000 | Loss: 0.00002131
Iteration 18/1000 | Loss: 0.00002129
Iteration 19/1000 | Loss: 0.00002129
Iteration 20/1000 | Loss: 0.00002129
Iteration 21/1000 | Loss: 0.00002129
Iteration 22/1000 | Loss: 0.00002129
Iteration 23/1000 | Loss: 0.00002128
Iteration 24/1000 | Loss: 0.00002128
Iteration 25/1000 | Loss: 0.00002128
Iteration 26/1000 | Loss: 0.00002127
Iteration 27/1000 | Loss: 0.00002127
Iteration 28/1000 | Loss: 0.00002127
Iteration 29/1000 | Loss: 0.00002126
Iteration 30/1000 | Loss: 0.00002126
Iteration 31/1000 | Loss: 0.00002126
Iteration 32/1000 | Loss: 0.00002125
Iteration 33/1000 | Loss: 0.00002125
Iteration 34/1000 | Loss: 0.00002125
Iteration 35/1000 | Loss: 0.00002125
Iteration 36/1000 | Loss: 0.00002124
Iteration 37/1000 | Loss: 0.00002124
Iteration 38/1000 | Loss: 0.00002124
Iteration 39/1000 | Loss: 0.00002124
Iteration 40/1000 | Loss: 0.00002124
Iteration 41/1000 | Loss: 0.00002124
Iteration 42/1000 | Loss: 0.00002124
Iteration 43/1000 | Loss: 0.00002124
Iteration 44/1000 | Loss: 0.00002124
Iteration 45/1000 | Loss: 0.00002124
Iteration 46/1000 | Loss: 0.00002124
Iteration 47/1000 | Loss: 0.00002124
Iteration 48/1000 | Loss: 0.00002124
Iteration 49/1000 | Loss: 0.00002123
Iteration 50/1000 | Loss: 0.00002123
Iteration 51/1000 | Loss: 0.00002123
Iteration 52/1000 | Loss: 0.00002123
Iteration 53/1000 | Loss: 0.00002123
Iteration 54/1000 | Loss: 0.00002122
Iteration 55/1000 | Loss: 0.00002122
Iteration 56/1000 | Loss: 0.00002122
Iteration 57/1000 | Loss: 0.00002122
Iteration 58/1000 | Loss: 0.00002122
Iteration 59/1000 | Loss: 0.00002121
Iteration 60/1000 | Loss: 0.00002121
Iteration 61/1000 | Loss: 0.00002121
Iteration 62/1000 | Loss: 0.00002120
Iteration 63/1000 | Loss: 0.00002120
Iteration 64/1000 | Loss: 0.00002119
Iteration 65/1000 | Loss: 0.00002119
Iteration 66/1000 | Loss: 0.00002119
Iteration 67/1000 | Loss: 0.00002119
Iteration 68/1000 | Loss: 0.00002119
Iteration 69/1000 | Loss: 0.00002118
Iteration 70/1000 | Loss: 0.00002118
Iteration 71/1000 | Loss: 0.00002118
Iteration 72/1000 | Loss: 0.00002117
Iteration 73/1000 | Loss: 0.00002117
Iteration 74/1000 | Loss: 0.00002117
Iteration 75/1000 | Loss: 0.00002116
Iteration 76/1000 | Loss: 0.00002116
Iteration 77/1000 | Loss: 0.00002115
Iteration 78/1000 | Loss: 0.00002115
Iteration 79/1000 | Loss: 0.00002115
Iteration 80/1000 | Loss: 0.00002114
Iteration 81/1000 | Loss: 0.00002114
Iteration 82/1000 | Loss: 0.00002114
Iteration 83/1000 | Loss: 0.00002113
Iteration 84/1000 | Loss: 0.00002113
Iteration 85/1000 | Loss: 0.00002113
Iteration 86/1000 | Loss: 0.00002113
Iteration 87/1000 | Loss: 0.00002113
Iteration 88/1000 | Loss: 0.00002112
Iteration 89/1000 | Loss: 0.00002112
Iteration 90/1000 | Loss: 0.00002112
Iteration 91/1000 | Loss: 0.00002112
Iteration 92/1000 | Loss: 0.00002112
Iteration 93/1000 | Loss: 0.00002112
Iteration 94/1000 | Loss: 0.00002111
Iteration 95/1000 | Loss: 0.00002111
Iteration 96/1000 | Loss: 0.00002111
Iteration 97/1000 | Loss: 0.00002111
Iteration 98/1000 | Loss: 0.00002111
Iteration 99/1000 | Loss: 0.00002111
Iteration 100/1000 | Loss: 0.00002111
Iteration 101/1000 | Loss: 0.00002111
Iteration 102/1000 | Loss: 0.00002110
Iteration 103/1000 | Loss: 0.00002110
Iteration 104/1000 | Loss: 0.00002110
Iteration 105/1000 | Loss: 0.00002110
Iteration 106/1000 | Loss: 0.00002110
Iteration 107/1000 | Loss: 0.00002110
Iteration 108/1000 | Loss: 0.00002110
Iteration 109/1000 | Loss: 0.00002110
Iteration 110/1000 | Loss: 0.00002110
Iteration 111/1000 | Loss: 0.00002110
Iteration 112/1000 | Loss: 0.00002110
Iteration 113/1000 | Loss: 0.00002110
Iteration 114/1000 | Loss: 0.00002109
Iteration 115/1000 | Loss: 0.00002109
Iteration 116/1000 | Loss: 0.00002109
Iteration 117/1000 | Loss: 0.00002109
Iteration 118/1000 | Loss: 0.00002109
Iteration 119/1000 | Loss: 0.00002109
Iteration 120/1000 | Loss: 0.00002109
Iteration 121/1000 | Loss: 0.00002109
Iteration 122/1000 | Loss: 0.00002109
Iteration 123/1000 | Loss: 0.00002109
Iteration 124/1000 | Loss: 0.00002109
Iteration 125/1000 | Loss: 0.00002109
Iteration 126/1000 | Loss: 0.00002108
Iteration 127/1000 | Loss: 0.00002108
Iteration 128/1000 | Loss: 0.00002108
Iteration 129/1000 | Loss: 0.00002108
Iteration 130/1000 | Loss: 0.00002108
Iteration 131/1000 | Loss: 0.00002108
Iteration 132/1000 | Loss: 0.00002108
Iteration 133/1000 | Loss: 0.00002108
Iteration 134/1000 | Loss: 0.00002108
Iteration 135/1000 | Loss: 0.00002108
Iteration 136/1000 | Loss: 0.00002108
Iteration 137/1000 | Loss: 0.00002108
Iteration 138/1000 | Loss: 0.00002108
Iteration 139/1000 | Loss: 0.00002108
Iteration 140/1000 | Loss: 0.00002108
Iteration 141/1000 | Loss: 0.00002108
Iteration 142/1000 | Loss: 0.00002108
Iteration 143/1000 | Loss: 0.00002108
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [2.1081881641293876e-05, 2.1081881641293876e-05, 2.1081881641293876e-05, 2.1081881641293876e-05, 2.1081881641293876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1081881641293876e-05

Optimization complete. Final v2v error: 3.858532428741455 mm

Highest mean error: 4.598773956298828 mm for frame 57

Lowest mean error: 3.371671199798584 mm for frame 5

Saving results

Total time: 39.584747552871704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01130908
Iteration 2/25 | Loss: 0.00281144
Iteration 3/25 | Loss: 0.00160190
Iteration 4/25 | Loss: 0.00154289
Iteration 5/25 | Loss: 0.00163977
Iteration 6/25 | Loss: 0.00156633
Iteration 7/25 | Loss: 0.00149200
Iteration 8/25 | Loss: 0.00134337
Iteration 9/25 | Loss: 0.00125659
Iteration 10/25 | Loss: 0.00120450
Iteration 11/25 | Loss: 0.00113080
Iteration 12/25 | Loss: 0.00108523
Iteration 13/25 | Loss: 0.00105553
Iteration 14/25 | Loss: 0.00103375
Iteration 15/25 | Loss: 0.00100854
Iteration 16/25 | Loss: 0.00101355
Iteration 17/25 | Loss: 0.00099428
Iteration 18/25 | Loss: 0.00098392
Iteration 19/25 | Loss: 0.00097326
Iteration 20/25 | Loss: 0.00097126
Iteration 21/25 | Loss: 0.00096467
Iteration 22/25 | Loss: 0.00095627
Iteration 23/25 | Loss: 0.00094667
Iteration 24/25 | Loss: 0.00094896
Iteration 25/25 | Loss: 0.00094782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.68263912
Iteration 2/25 | Loss: 0.00137598
Iteration 3/25 | Loss: 0.00137598
Iteration 4/25 | Loss: 0.00137598
Iteration 5/25 | Loss: 0.00137598
Iteration 6/25 | Loss: 0.00137598
Iteration 7/25 | Loss: 0.00137598
Iteration 8/25 | Loss: 0.00137598
Iteration 9/25 | Loss: 0.00137598
Iteration 10/25 | Loss: 0.00137598
Iteration 11/25 | Loss: 0.00137598
Iteration 12/25 | Loss: 0.00137598
Iteration 13/25 | Loss: 0.00137598
Iteration 14/25 | Loss: 0.00137598
Iteration 15/25 | Loss: 0.00137598
Iteration 16/25 | Loss: 0.00137598
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013759755529463291, 0.0013759755529463291, 0.0013759755529463291, 0.0013759755529463291, 0.0013759755529463291]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013759755529463291

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137598
Iteration 2/1000 | Loss: 0.00061660
Iteration 3/1000 | Loss: 0.00095270
Iteration 4/1000 | Loss: 0.00075077
Iteration 5/1000 | Loss: 0.00045370
Iteration 6/1000 | Loss: 0.00134412
Iteration 7/1000 | Loss: 0.00100671
Iteration 8/1000 | Loss: 0.00096402
Iteration 9/1000 | Loss: 0.00101205
Iteration 10/1000 | Loss: 0.00098951
Iteration 11/1000 | Loss: 0.00089576
Iteration 12/1000 | Loss: 0.00083079
Iteration 13/1000 | Loss: 0.00104258
Iteration 14/1000 | Loss: 0.00101866
Iteration 15/1000 | Loss: 0.00091401
Iteration 16/1000 | Loss: 0.00089505
Iteration 17/1000 | Loss: 0.00121802
Iteration 18/1000 | Loss: 0.00122603
Iteration 19/1000 | Loss: 0.00093332
Iteration 20/1000 | Loss: 0.00107882
Iteration 21/1000 | Loss: 0.00110252
Iteration 22/1000 | Loss: 0.00085572
Iteration 23/1000 | Loss: 0.00088444
Iteration 24/1000 | Loss: 0.00060799
Iteration 25/1000 | Loss: 0.00083241
Iteration 26/1000 | Loss: 0.00063834
Iteration 27/1000 | Loss: 0.00088197
Iteration 28/1000 | Loss: 0.00189747
Iteration 29/1000 | Loss: 0.00124011
Iteration 30/1000 | Loss: 0.00140948
Iteration 31/1000 | Loss: 0.00106769
Iteration 32/1000 | Loss: 0.00112484
Iteration 33/1000 | Loss: 0.00132838
Iteration 34/1000 | Loss: 0.00115223
Iteration 35/1000 | Loss: 0.00075657
Iteration 36/1000 | Loss: 0.00089882
Iteration 37/1000 | Loss: 0.00138181
Iteration 38/1000 | Loss: 0.00103253
Iteration 39/1000 | Loss: 0.00092436
Iteration 40/1000 | Loss: 0.00109918
Iteration 41/1000 | Loss: 0.00073163
Iteration 42/1000 | Loss: 0.00021243
Iteration 43/1000 | Loss: 0.00052180
Iteration 44/1000 | Loss: 0.00051126
Iteration 45/1000 | Loss: 0.00081117
Iteration 46/1000 | Loss: 0.00078411
Iteration 47/1000 | Loss: 0.00083217
Iteration 48/1000 | Loss: 0.00099201
Iteration 49/1000 | Loss: 0.00114052
Iteration 50/1000 | Loss: 0.00083300
Iteration 51/1000 | Loss: 0.00040215
Iteration 52/1000 | Loss: 0.00030154
Iteration 53/1000 | Loss: 0.00035918
Iteration 54/1000 | Loss: 0.00043643
Iteration 55/1000 | Loss: 0.00046702
Iteration 56/1000 | Loss: 0.00029527
Iteration 57/1000 | Loss: 0.00043407
Iteration 58/1000 | Loss: 0.00038462
Iteration 59/1000 | Loss: 0.00115823
Iteration 60/1000 | Loss: 0.00083018
Iteration 61/1000 | Loss: 0.00091480
Iteration 62/1000 | Loss: 0.00156258
Iteration 63/1000 | Loss: 0.00094411
Iteration 64/1000 | Loss: 0.00082785
Iteration 65/1000 | Loss: 0.00059679
Iteration 66/1000 | Loss: 0.00020103
Iteration 67/1000 | Loss: 0.00070299
Iteration 68/1000 | Loss: 0.00089658
Iteration 69/1000 | Loss: 0.00032608
Iteration 70/1000 | Loss: 0.00053287
Iteration 71/1000 | Loss: 0.00037848
Iteration 72/1000 | Loss: 0.00112524
Iteration 73/1000 | Loss: 0.00081164
Iteration 74/1000 | Loss: 0.00089043
Iteration 75/1000 | Loss: 0.00070576
Iteration 76/1000 | Loss: 0.00050405
Iteration 77/1000 | Loss: 0.00096373
Iteration 78/1000 | Loss: 0.00098633
Iteration 79/1000 | Loss: 0.00116260
Iteration 80/1000 | Loss: 0.00056850
Iteration 81/1000 | Loss: 0.00015664
Iteration 82/1000 | Loss: 0.00016815
Iteration 83/1000 | Loss: 0.00020742
Iteration 84/1000 | Loss: 0.00018732
Iteration 85/1000 | Loss: 0.00020124
Iteration 86/1000 | Loss: 0.00013039
Iteration 87/1000 | Loss: 0.00046446
Iteration 88/1000 | Loss: 0.00031557
Iteration 89/1000 | Loss: 0.00025109
Iteration 90/1000 | Loss: 0.00020378
Iteration 91/1000 | Loss: 0.00026661
Iteration 92/1000 | Loss: 0.00030763
Iteration 93/1000 | Loss: 0.00025255
Iteration 94/1000 | Loss: 0.00023168
Iteration 95/1000 | Loss: 0.00026356
Iteration 96/1000 | Loss: 0.00021227
Iteration 97/1000 | Loss: 0.00039213
Iteration 98/1000 | Loss: 0.00049945
Iteration 99/1000 | Loss: 0.00023642
Iteration 100/1000 | Loss: 0.00032807
Iteration 101/1000 | Loss: 0.00040116
Iteration 102/1000 | Loss: 0.00046504
Iteration 103/1000 | Loss: 0.00017473
Iteration 104/1000 | Loss: 0.00025267
Iteration 105/1000 | Loss: 0.00028387
Iteration 106/1000 | Loss: 0.00021524
Iteration 107/1000 | Loss: 0.00016092
Iteration 108/1000 | Loss: 0.00040877
Iteration 109/1000 | Loss: 0.00038181
Iteration 110/1000 | Loss: 0.00011246
Iteration 111/1000 | Loss: 0.00061972
Iteration 112/1000 | Loss: 0.00034480
Iteration 113/1000 | Loss: 0.00022270
Iteration 114/1000 | Loss: 0.00012707
Iteration 115/1000 | Loss: 0.00011384
Iteration 116/1000 | Loss: 0.00028807
Iteration 117/1000 | Loss: 0.00047688
Iteration 118/1000 | Loss: 0.00024918
Iteration 119/1000 | Loss: 0.00034450
Iteration 120/1000 | Loss: 0.00030590
Iteration 121/1000 | Loss: 0.00023469
Iteration 122/1000 | Loss: 0.00037973
Iteration 123/1000 | Loss: 0.00033733
Iteration 124/1000 | Loss: 0.00028701
Iteration 125/1000 | Loss: 0.00021738
Iteration 126/1000 | Loss: 0.00020470
Iteration 127/1000 | Loss: 0.00017638
Iteration 128/1000 | Loss: 0.00019926
Iteration 129/1000 | Loss: 0.00006510
Iteration 130/1000 | Loss: 0.00017745
Iteration 131/1000 | Loss: 0.00043108
Iteration 132/1000 | Loss: 0.00029738
Iteration 133/1000 | Loss: 0.00036530
Iteration 134/1000 | Loss: 0.00046566
Iteration 135/1000 | Loss: 0.00010608
Iteration 136/1000 | Loss: 0.00008695
Iteration 137/1000 | Loss: 0.00017092
Iteration 138/1000 | Loss: 0.00030759
Iteration 139/1000 | Loss: 0.00015597
Iteration 140/1000 | Loss: 0.00011540
Iteration 141/1000 | Loss: 0.00011923
Iteration 142/1000 | Loss: 0.00007989
Iteration 143/1000 | Loss: 0.00015910
Iteration 144/1000 | Loss: 0.00017601
Iteration 145/1000 | Loss: 0.00020075
Iteration 146/1000 | Loss: 0.00015804
Iteration 147/1000 | Loss: 0.00046536
Iteration 148/1000 | Loss: 0.00015042
Iteration 149/1000 | Loss: 0.00013073
Iteration 150/1000 | Loss: 0.00011618
Iteration 151/1000 | Loss: 0.00009548
Iteration 152/1000 | Loss: 0.00011468
Iteration 153/1000 | Loss: 0.00006132
Iteration 154/1000 | Loss: 0.00011859
Iteration 155/1000 | Loss: 0.00010121
Iteration 156/1000 | Loss: 0.00006902
Iteration 157/1000 | Loss: 0.00012968
Iteration 158/1000 | Loss: 0.00010953
Iteration 159/1000 | Loss: 0.00012015
Iteration 160/1000 | Loss: 0.00017233
Iteration 161/1000 | Loss: 0.00077631
Iteration 162/1000 | Loss: 0.00031656
Iteration 163/1000 | Loss: 0.00005197
Iteration 164/1000 | Loss: 0.00005679
Iteration 165/1000 | Loss: 0.00005727
Iteration 166/1000 | Loss: 0.00005578
Iteration 167/1000 | Loss: 0.00028442
Iteration 168/1000 | Loss: 0.00016443
Iteration 169/1000 | Loss: 0.00006235
Iteration 170/1000 | Loss: 0.00020919
Iteration 171/1000 | Loss: 0.00020540
Iteration 172/1000 | Loss: 0.00015859
Iteration 173/1000 | Loss: 0.00027600
Iteration 174/1000 | Loss: 0.00046108
Iteration 175/1000 | Loss: 0.00030477
Iteration 176/1000 | Loss: 0.00029737
Iteration 177/1000 | Loss: 0.00031675
Iteration 178/1000 | Loss: 0.00022884
Iteration 179/1000 | Loss: 0.00031649
Iteration 180/1000 | Loss: 0.00031837
Iteration 181/1000 | Loss: 0.00033750
Iteration 182/1000 | Loss: 0.00031826
Iteration 183/1000 | Loss: 0.00037122
Iteration 184/1000 | Loss: 0.00023807
Iteration 185/1000 | Loss: 0.00026931
Iteration 186/1000 | Loss: 0.00016992
Iteration 187/1000 | Loss: 0.00025079
Iteration 188/1000 | Loss: 0.00023484
Iteration 189/1000 | Loss: 0.00025398
Iteration 190/1000 | Loss: 0.00023742
Iteration 191/1000 | Loss: 0.00005671
Iteration 192/1000 | Loss: 0.00005197
Iteration 193/1000 | Loss: 0.00004761
Iteration 194/1000 | Loss: 0.00004855
Iteration 195/1000 | Loss: 0.00005224
Iteration 196/1000 | Loss: 0.00003151
Iteration 197/1000 | Loss: 0.00003454
Iteration 198/1000 | Loss: 0.00006074
Iteration 199/1000 | Loss: 0.00004745
Iteration 200/1000 | Loss: 0.00003766
Iteration 201/1000 | Loss: 0.00003989
Iteration 202/1000 | Loss: 0.00004248
Iteration 203/1000 | Loss: 0.00004386
Iteration 204/1000 | Loss: 0.00004039
Iteration 205/1000 | Loss: 0.00005625
Iteration 206/1000 | Loss: 0.00004736
Iteration 207/1000 | Loss: 0.00003773
Iteration 208/1000 | Loss: 0.00005620
Iteration 209/1000 | Loss: 0.00003310
Iteration 210/1000 | Loss: 0.00003066
Iteration 211/1000 | Loss: 0.00002851
Iteration 212/1000 | Loss: 0.00002712
Iteration 213/1000 | Loss: 0.00002615
Iteration 214/1000 | Loss: 0.00002570
Iteration 215/1000 | Loss: 0.00002548
Iteration 216/1000 | Loss: 0.00002523
Iteration 217/1000 | Loss: 0.00002504
Iteration 218/1000 | Loss: 0.00002487
Iteration 219/1000 | Loss: 0.00002471
Iteration 220/1000 | Loss: 0.00002466
Iteration 221/1000 | Loss: 0.00002465
Iteration 222/1000 | Loss: 0.00002465
Iteration 223/1000 | Loss: 0.00002465
Iteration 224/1000 | Loss: 0.00002465
Iteration 225/1000 | Loss: 0.00002465
Iteration 226/1000 | Loss: 0.00002465
Iteration 227/1000 | Loss: 0.00002465
Iteration 228/1000 | Loss: 0.00002465
Iteration 229/1000 | Loss: 0.00002465
Iteration 230/1000 | Loss: 0.00002464
Iteration 231/1000 | Loss: 0.00002464
Iteration 232/1000 | Loss: 0.00002462
Iteration 233/1000 | Loss: 0.00002462
Iteration 234/1000 | Loss: 0.00002462
Iteration 235/1000 | Loss: 0.00002458
Iteration 236/1000 | Loss: 0.00002458
Iteration 237/1000 | Loss: 0.00002458
Iteration 238/1000 | Loss: 0.00002458
Iteration 239/1000 | Loss: 0.00002458
Iteration 240/1000 | Loss: 0.00002457
Iteration 241/1000 | Loss: 0.00002457
Iteration 242/1000 | Loss: 0.00002457
Iteration 243/1000 | Loss: 0.00002457
Iteration 244/1000 | Loss: 0.00002457
Iteration 245/1000 | Loss: 0.00002457
Iteration 246/1000 | Loss: 0.00002457
Iteration 247/1000 | Loss: 0.00002456
Iteration 248/1000 | Loss: 0.00002456
Iteration 249/1000 | Loss: 0.00002456
Iteration 250/1000 | Loss: 0.00002456
Iteration 251/1000 | Loss: 0.00002456
Iteration 252/1000 | Loss: 0.00002456
Iteration 253/1000 | Loss: 0.00002456
Iteration 254/1000 | Loss: 0.00002456
Iteration 255/1000 | Loss: 0.00002456
Iteration 256/1000 | Loss: 0.00002456
Iteration 257/1000 | Loss: 0.00002455
Iteration 258/1000 | Loss: 0.00002455
Iteration 259/1000 | Loss: 0.00002455
Iteration 260/1000 | Loss: 0.00002455
Iteration 261/1000 | Loss: 0.00002455
Iteration 262/1000 | Loss: 0.00002454
Iteration 263/1000 | Loss: 0.00002454
Iteration 264/1000 | Loss: 0.00002454
Iteration 265/1000 | Loss: 0.00002454
Iteration 266/1000 | Loss: 0.00002453
Iteration 267/1000 | Loss: 0.00002453
Iteration 268/1000 | Loss: 0.00002453
Iteration 269/1000 | Loss: 0.00002453
Iteration 270/1000 | Loss: 0.00002453
Iteration 271/1000 | Loss: 0.00002452
Iteration 272/1000 | Loss: 0.00002452
Iteration 273/1000 | Loss: 0.00002452
Iteration 274/1000 | Loss: 0.00002452
Iteration 275/1000 | Loss: 0.00002451
Iteration 276/1000 | Loss: 0.00002451
Iteration 277/1000 | Loss: 0.00002451
Iteration 278/1000 | Loss: 0.00002451
Iteration 279/1000 | Loss: 0.00002451
Iteration 280/1000 | Loss: 0.00002451
Iteration 281/1000 | Loss: 0.00002451
Iteration 282/1000 | Loss: 0.00002451
Iteration 283/1000 | Loss: 0.00002450
Iteration 284/1000 | Loss: 0.00002450
Iteration 285/1000 | Loss: 0.00002450
Iteration 286/1000 | Loss: 0.00002450
Iteration 287/1000 | Loss: 0.00002450
Iteration 288/1000 | Loss: 0.00002450
Iteration 289/1000 | Loss: 0.00002449
Iteration 290/1000 | Loss: 0.00002449
Iteration 291/1000 | Loss: 0.00002449
Iteration 292/1000 | Loss: 0.00002449
Iteration 293/1000 | Loss: 0.00002449
Iteration 294/1000 | Loss: 0.00002449
Iteration 295/1000 | Loss: 0.00002449
Iteration 296/1000 | Loss: 0.00002449
Iteration 297/1000 | Loss: 0.00002448
Iteration 298/1000 | Loss: 0.00002448
Iteration 299/1000 | Loss: 0.00002448
Iteration 300/1000 | Loss: 0.00002448
Iteration 301/1000 | Loss: 0.00002448
Iteration 302/1000 | Loss: 0.00002448
Iteration 303/1000 | Loss: 0.00002448
Iteration 304/1000 | Loss: 0.00002448
Iteration 305/1000 | Loss: 0.00002448
Iteration 306/1000 | Loss: 0.00002447
Iteration 307/1000 | Loss: 0.00002447
Iteration 308/1000 | Loss: 0.00002447
Iteration 309/1000 | Loss: 0.00002447
Iteration 310/1000 | Loss: 0.00002447
Iteration 311/1000 | Loss: 0.00002447
Iteration 312/1000 | Loss: 0.00002446
Iteration 313/1000 | Loss: 0.00002446
Iteration 314/1000 | Loss: 0.00002446
Iteration 315/1000 | Loss: 0.00002446
Iteration 316/1000 | Loss: 0.00002445
Iteration 317/1000 | Loss: 0.00002445
Iteration 318/1000 | Loss: 0.00002445
Iteration 319/1000 | Loss: 0.00002445
Iteration 320/1000 | Loss: 0.00002445
Iteration 321/1000 | Loss: 0.00002445
Iteration 322/1000 | Loss: 0.00002445
Iteration 323/1000 | Loss: 0.00002445
Iteration 324/1000 | Loss: 0.00002445
Iteration 325/1000 | Loss: 0.00002445
Iteration 326/1000 | Loss: 0.00002445
Iteration 327/1000 | Loss: 0.00002445
Iteration 328/1000 | Loss: 0.00002445
Iteration 329/1000 | Loss: 0.00002445
Iteration 330/1000 | Loss: 0.00002445
Iteration 331/1000 | Loss: 0.00002445
Iteration 332/1000 | Loss: 0.00002445
Iteration 333/1000 | Loss: 0.00002445
Iteration 334/1000 | Loss: 0.00002445
Iteration 335/1000 | Loss: 0.00002445
Iteration 336/1000 | Loss: 0.00002445
Iteration 337/1000 | Loss: 0.00002445
Iteration 338/1000 | Loss: 0.00002445
Iteration 339/1000 | Loss: 0.00002445
Iteration 340/1000 | Loss: 0.00002445
Iteration 341/1000 | Loss: 0.00002445
Iteration 342/1000 | Loss: 0.00002445
Iteration 343/1000 | Loss: 0.00002445
Iteration 344/1000 | Loss: 0.00002445
Iteration 345/1000 | Loss: 0.00002445
Iteration 346/1000 | Loss: 0.00002445
Iteration 347/1000 | Loss: 0.00002445
Iteration 348/1000 | Loss: 0.00002445
Iteration 349/1000 | Loss: 0.00002445
Iteration 350/1000 | Loss: 0.00002445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 350. Stopping optimization.
Last 5 losses: [2.4445153030683286e-05, 2.4445153030683286e-05, 2.4445153030683286e-05, 2.4445153030683286e-05, 2.4445153030683286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4445153030683286e-05

Optimization complete. Final v2v error: 4.06115198135376 mm

Highest mean error: 9.171919822692871 mm for frame 48

Lowest mean error: 3.379920244216919 mm for frame 43

Saving results

Total time: 358.5423963069916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846137
Iteration 2/25 | Loss: 0.00148042
Iteration 3/25 | Loss: 0.00078975
Iteration 4/25 | Loss: 0.00069467
Iteration 5/25 | Loss: 0.00068105
Iteration 6/25 | Loss: 0.00067873
Iteration 7/25 | Loss: 0.00067834
Iteration 8/25 | Loss: 0.00067834
Iteration 9/25 | Loss: 0.00067834
Iteration 10/25 | Loss: 0.00067834
Iteration 11/25 | Loss: 0.00067834
Iteration 12/25 | Loss: 0.00067834
Iteration 13/25 | Loss: 0.00067834
Iteration 14/25 | Loss: 0.00067834
Iteration 15/25 | Loss: 0.00067834
Iteration 16/25 | Loss: 0.00067834
Iteration 17/25 | Loss: 0.00067834
Iteration 18/25 | Loss: 0.00067834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006783382268622518, 0.0006783382268622518, 0.0006783382268622518, 0.0006783382268622518, 0.0006783382268622518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006783382268622518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45953429
Iteration 2/25 | Loss: 0.00031606
Iteration 3/25 | Loss: 0.00031606
Iteration 4/25 | Loss: 0.00031606
Iteration 5/25 | Loss: 0.00031605
Iteration 6/25 | Loss: 0.00031605
Iteration 7/25 | Loss: 0.00031605
Iteration 8/25 | Loss: 0.00031605
Iteration 9/25 | Loss: 0.00031605
Iteration 10/25 | Loss: 0.00031605
Iteration 11/25 | Loss: 0.00031605
Iteration 12/25 | Loss: 0.00031605
Iteration 13/25 | Loss: 0.00031605
Iteration 14/25 | Loss: 0.00031605
Iteration 15/25 | Loss: 0.00031605
Iteration 16/25 | Loss: 0.00031605
Iteration 17/25 | Loss: 0.00031605
Iteration 18/25 | Loss: 0.00031605
Iteration 19/25 | Loss: 0.00031605
Iteration 20/25 | Loss: 0.00031605
Iteration 21/25 | Loss: 0.00031605
Iteration 22/25 | Loss: 0.00031605
Iteration 23/25 | Loss: 0.00031605
Iteration 24/25 | Loss: 0.00031605
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00031605278491042554, 0.00031605278491042554, 0.00031605278491042554, 0.00031605278491042554, 0.00031605278491042554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031605278491042554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031605
Iteration 2/1000 | Loss: 0.00002987
Iteration 3/1000 | Loss: 0.00002374
Iteration 4/1000 | Loss: 0.00002215
Iteration 5/1000 | Loss: 0.00002098
Iteration 6/1000 | Loss: 0.00002020
Iteration 7/1000 | Loss: 0.00001957
Iteration 8/1000 | Loss: 0.00001917
Iteration 9/1000 | Loss: 0.00001891
Iteration 10/1000 | Loss: 0.00001879
Iteration 11/1000 | Loss: 0.00001878
Iteration 12/1000 | Loss: 0.00001870
Iteration 13/1000 | Loss: 0.00001868
Iteration 14/1000 | Loss: 0.00001867
Iteration 15/1000 | Loss: 0.00001867
Iteration 16/1000 | Loss: 0.00001862
Iteration 17/1000 | Loss: 0.00001854
Iteration 18/1000 | Loss: 0.00001854
Iteration 19/1000 | Loss: 0.00001852
Iteration 20/1000 | Loss: 0.00001850
Iteration 21/1000 | Loss: 0.00001850
Iteration 22/1000 | Loss: 0.00001850
Iteration 23/1000 | Loss: 0.00001850
Iteration 24/1000 | Loss: 0.00001850
Iteration 25/1000 | Loss: 0.00001850
Iteration 26/1000 | Loss: 0.00001850
Iteration 27/1000 | Loss: 0.00001849
Iteration 28/1000 | Loss: 0.00001849
Iteration 29/1000 | Loss: 0.00001849
Iteration 30/1000 | Loss: 0.00001848
Iteration 31/1000 | Loss: 0.00001848
Iteration 32/1000 | Loss: 0.00001848
Iteration 33/1000 | Loss: 0.00001847
Iteration 34/1000 | Loss: 0.00001847
Iteration 35/1000 | Loss: 0.00001847
Iteration 36/1000 | Loss: 0.00001847
Iteration 37/1000 | Loss: 0.00001847
Iteration 38/1000 | Loss: 0.00001847
Iteration 39/1000 | Loss: 0.00001846
Iteration 40/1000 | Loss: 0.00001846
Iteration 41/1000 | Loss: 0.00001846
Iteration 42/1000 | Loss: 0.00001846
Iteration 43/1000 | Loss: 0.00001846
Iteration 44/1000 | Loss: 0.00001845
Iteration 45/1000 | Loss: 0.00001845
Iteration 46/1000 | Loss: 0.00001845
Iteration 47/1000 | Loss: 0.00001844
Iteration 48/1000 | Loss: 0.00001844
Iteration 49/1000 | Loss: 0.00001844
Iteration 50/1000 | Loss: 0.00001844
Iteration 51/1000 | Loss: 0.00001844
Iteration 52/1000 | Loss: 0.00001844
Iteration 53/1000 | Loss: 0.00001843
Iteration 54/1000 | Loss: 0.00001843
Iteration 55/1000 | Loss: 0.00001843
Iteration 56/1000 | Loss: 0.00001843
Iteration 57/1000 | Loss: 0.00001842
Iteration 58/1000 | Loss: 0.00001842
Iteration 59/1000 | Loss: 0.00001842
Iteration 60/1000 | Loss: 0.00001842
Iteration 61/1000 | Loss: 0.00001842
Iteration 62/1000 | Loss: 0.00001842
Iteration 63/1000 | Loss: 0.00001842
Iteration 64/1000 | Loss: 0.00001842
Iteration 65/1000 | Loss: 0.00001841
Iteration 66/1000 | Loss: 0.00001841
Iteration 67/1000 | Loss: 0.00001841
Iteration 68/1000 | Loss: 0.00001841
Iteration 69/1000 | Loss: 0.00001841
Iteration 70/1000 | Loss: 0.00001841
Iteration 71/1000 | Loss: 0.00001841
Iteration 72/1000 | Loss: 0.00001840
Iteration 73/1000 | Loss: 0.00001840
Iteration 74/1000 | Loss: 0.00001840
Iteration 75/1000 | Loss: 0.00001840
Iteration 76/1000 | Loss: 0.00001840
Iteration 77/1000 | Loss: 0.00001840
Iteration 78/1000 | Loss: 0.00001840
Iteration 79/1000 | Loss: 0.00001840
Iteration 80/1000 | Loss: 0.00001840
Iteration 81/1000 | Loss: 0.00001840
Iteration 82/1000 | Loss: 0.00001840
Iteration 83/1000 | Loss: 0.00001840
Iteration 84/1000 | Loss: 0.00001840
Iteration 85/1000 | Loss: 0.00001839
Iteration 86/1000 | Loss: 0.00001839
Iteration 87/1000 | Loss: 0.00001839
Iteration 88/1000 | Loss: 0.00001839
Iteration 89/1000 | Loss: 0.00001839
Iteration 90/1000 | Loss: 0.00001839
Iteration 91/1000 | Loss: 0.00001839
Iteration 92/1000 | Loss: 0.00001839
Iteration 93/1000 | Loss: 0.00001839
Iteration 94/1000 | Loss: 0.00001839
Iteration 95/1000 | Loss: 0.00001839
Iteration 96/1000 | Loss: 0.00001839
Iteration 97/1000 | Loss: 0.00001839
Iteration 98/1000 | Loss: 0.00001839
Iteration 99/1000 | Loss: 0.00001839
Iteration 100/1000 | Loss: 0.00001839
Iteration 101/1000 | Loss: 0.00001838
Iteration 102/1000 | Loss: 0.00001838
Iteration 103/1000 | Loss: 0.00001838
Iteration 104/1000 | Loss: 0.00001838
Iteration 105/1000 | Loss: 0.00001838
Iteration 106/1000 | Loss: 0.00001838
Iteration 107/1000 | Loss: 0.00001838
Iteration 108/1000 | Loss: 0.00001838
Iteration 109/1000 | Loss: 0.00001838
Iteration 110/1000 | Loss: 0.00001838
Iteration 111/1000 | Loss: 0.00001838
Iteration 112/1000 | Loss: 0.00001838
Iteration 113/1000 | Loss: 0.00001837
Iteration 114/1000 | Loss: 0.00001837
Iteration 115/1000 | Loss: 0.00001837
Iteration 116/1000 | Loss: 0.00001837
Iteration 117/1000 | Loss: 0.00001837
Iteration 118/1000 | Loss: 0.00001837
Iteration 119/1000 | Loss: 0.00001837
Iteration 120/1000 | Loss: 0.00001837
Iteration 121/1000 | Loss: 0.00001837
Iteration 122/1000 | Loss: 0.00001837
Iteration 123/1000 | Loss: 0.00001837
Iteration 124/1000 | Loss: 0.00001837
Iteration 125/1000 | Loss: 0.00001837
Iteration 126/1000 | Loss: 0.00001837
Iteration 127/1000 | Loss: 0.00001837
Iteration 128/1000 | Loss: 0.00001837
Iteration 129/1000 | Loss: 0.00001837
Iteration 130/1000 | Loss: 0.00001837
Iteration 131/1000 | Loss: 0.00001837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.8368156815995462e-05, 1.8368156815995462e-05, 1.8368156815995462e-05, 1.8368156815995462e-05, 1.8368156815995462e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8368156815995462e-05

Optimization complete. Final v2v error: 3.6016340255737305 mm

Highest mean error: 4.327470302581787 mm for frame 171

Lowest mean error: 2.813340663909912 mm for frame 62

Saving results

Total time: 40.058168172836304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025402
Iteration 2/25 | Loss: 0.00210384
Iteration 3/25 | Loss: 0.00147237
Iteration 4/25 | Loss: 0.00126213
Iteration 5/25 | Loss: 0.00120868
Iteration 6/25 | Loss: 0.00095440
Iteration 7/25 | Loss: 0.00091639
Iteration 8/25 | Loss: 0.00085696
Iteration 9/25 | Loss: 0.00082836
Iteration 10/25 | Loss: 0.00081904
Iteration 11/25 | Loss: 0.00079940
Iteration 12/25 | Loss: 0.00077412
Iteration 13/25 | Loss: 0.00076566
Iteration 14/25 | Loss: 0.00075869
Iteration 15/25 | Loss: 0.00075429
Iteration 16/25 | Loss: 0.00075328
Iteration 17/25 | Loss: 0.00075297
Iteration 18/25 | Loss: 0.00075282
Iteration 19/25 | Loss: 0.00075515
Iteration 20/25 | Loss: 0.00075323
Iteration 21/25 | Loss: 0.00075222
Iteration 22/25 | Loss: 0.00075098
Iteration 23/25 | Loss: 0.00075053
Iteration 24/25 | Loss: 0.00075038
Iteration 25/25 | Loss: 0.00075038

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46196568
Iteration 2/25 | Loss: 0.00031879
Iteration 3/25 | Loss: 0.00031879
Iteration 4/25 | Loss: 0.00031879
Iteration 5/25 | Loss: 0.00031879
Iteration 6/25 | Loss: 0.00031879
Iteration 7/25 | Loss: 0.00031879
Iteration 8/25 | Loss: 0.00031879
Iteration 9/25 | Loss: 0.00031878
Iteration 10/25 | Loss: 0.00031878
Iteration 11/25 | Loss: 0.00031878
Iteration 12/25 | Loss: 0.00031878
Iteration 13/25 | Loss: 0.00031878
Iteration 14/25 | Loss: 0.00031878
Iteration 15/25 | Loss: 0.00031878
Iteration 16/25 | Loss: 0.00031878
Iteration 17/25 | Loss: 0.00031878
Iteration 18/25 | Loss: 0.00031878
Iteration 19/25 | Loss: 0.00031878
Iteration 20/25 | Loss: 0.00031878
Iteration 21/25 | Loss: 0.00031878
Iteration 22/25 | Loss: 0.00031878
Iteration 23/25 | Loss: 0.00031878
Iteration 24/25 | Loss: 0.00031878
Iteration 25/25 | Loss: 0.00031878

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031878
Iteration 2/1000 | Loss: 0.00004171
Iteration 3/1000 | Loss: 0.00010593
Iteration 4/1000 | Loss: 0.00006440
Iteration 5/1000 | Loss: 0.00004406
Iteration 6/1000 | Loss: 0.00003203
Iteration 7/1000 | Loss: 0.00005835
Iteration 8/1000 | Loss: 0.00005577
Iteration 9/1000 | Loss: 0.00004422
Iteration 10/1000 | Loss: 0.00004155
Iteration 11/1000 | Loss: 0.00004427
Iteration 12/1000 | Loss: 0.00004117
Iteration 13/1000 | Loss: 0.00003141
Iteration 14/1000 | Loss: 0.00002856
Iteration 15/1000 | Loss: 0.00002758
Iteration 16/1000 | Loss: 0.00023320
Iteration 17/1000 | Loss: 0.00003665
Iteration 18/1000 | Loss: 0.00003013
Iteration 19/1000 | Loss: 0.00002808
Iteration 20/1000 | Loss: 0.00002601
Iteration 21/1000 | Loss: 0.00002444
Iteration 22/1000 | Loss: 0.00002374
Iteration 23/1000 | Loss: 0.00002351
Iteration 24/1000 | Loss: 0.00002325
Iteration 25/1000 | Loss: 0.00002315
Iteration 26/1000 | Loss: 0.00002315
Iteration 27/1000 | Loss: 0.00002314
Iteration 28/1000 | Loss: 0.00002314
Iteration 29/1000 | Loss: 0.00002314
Iteration 30/1000 | Loss: 0.00002313
Iteration 31/1000 | Loss: 0.00002313
Iteration 32/1000 | Loss: 0.00002313
Iteration 33/1000 | Loss: 0.00002313
Iteration 34/1000 | Loss: 0.00002313
Iteration 35/1000 | Loss: 0.00002312
Iteration 36/1000 | Loss: 0.00002312
Iteration 37/1000 | Loss: 0.00002312
Iteration 38/1000 | Loss: 0.00002312
Iteration 39/1000 | Loss: 0.00002312
Iteration 40/1000 | Loss: 0.00002312
Iteration 41/1000 | Loss: 0.00002312
Iteration 42/1000 | Loss: 0.00002312
Iteration 43/1000 | Loss: 0.00002311
Iteration 44/1000 | Loss: 0.00002311
Iteration 45/1000 | Loss: 0.00002311
Iteration 46/1000 | Loss: 0.00002311
Iteration 47/1000 | Loss: 0.00002311
Iteration 48/1000 | Loss: 0.00002311
Iteration 49/1000 | Loss: 0.00002311
Iteration 50/1000 | Loss: 0.00002311
Iteration 51/1000 | Loss: 0.00002311
Iteration 52/1000 | Loss: 0.00002311
Iteration 53/1000 | Loss: 0.00002311
Iteration 54/1000 | Loss: 0.00002311
Iteration 55/1000 | Loss: 0.00002311
Iteration 56/1000 | Loss: 0.00002311
Iteration 57/1000 | Loss: 0.00002310
Iteration 58/1000 | Loss: 0.00002310
Iteration 59/1000 | Loss: 0.00002310
Iteration 60/1000 | Loss: 0.00002310
Iteration 61/1000 | Loss: 0.00002310
Iteration 62/1000 | Loss: 0.00002310
Iteration 63/1000 | Loss: 0.00002309
Iteration 64/1000 | Loss: 0.00002309
Iteration 65/1000 | Loss: 0.00002309
Iteration 66/1000 | Loss: 0.00002309
Iteration 67/1000 | Loss: 0.00002309
Iteration 68/1000 | Loss: 0.00002308
Iteration 69/1000 | Loss: 0.00002308
Iteration 70/1000 | Loss: 0.00002308
Iteration 71/1000 | Loss: 0.00002308
Iteration 72/1000 | Loss: 0.00002308
Iteration 73/1000 | Loss: 0.00002308
Iteration 74/1000 | Loss: 0.00002308
Iteration 75/1000 | Loss: 0.00002308
Iteration 76/1000 | Loss: 0.00002308
Iteration 77/1000 | Loss: 0.00002308
Iteration 78/1000 | Loss: 0.00002308
Iteration 79/1000 | Loss: 0.00002308
Iteration 80/1000 | Loss: 0.00002308
Iteration 81/1000 | Loss: 0.00002307
Iteration 82/1000 | Loss: 0.00002307
Iteration 83/1000 | Loss: 0.00002307
Iteration 84/1000 | Loss: 0.00002307
Iteration 85/1000 | Loss: 0.00002307
Iteration 86/1000 | Loss: 0.00002307
Iteration 87/1000 | Loss: 0.00002307
Iteration 88/1000 | Loss: 0.00002307
Iteration 89/1000 | Loss: 0.00002307
Iteration 90/1000 | Loss: 0.00002306
Iteration 91/1000 | Loss: 0.00002306
Iteration 92/1000 | Loss: 0.00002306
Iteration 93/1000 | Loss: 0.00002305
Iteration 94/1000 | Loss: 0.00002305
Iteration 95/1000 | Loss: 0.00002305
Iteration 96/1000 | Loss: 0.00002305
Iteration 97/1000 | Loss: 0.00002305
Iteration 98/1000 | Loss: 0.00002305
Iteration 99/1000 | Loss: 0.00002305
Iteration 100/1000 | Loss: 0.00002305
Iteration 101/1000 | Loss: 0.00002305
Iteration 102/1000 | Loss: 0.00002305
Iteration 103/1000 | Loss: 0.00002305
Iteration 104/1000 | Loss: 0.00002305
Iteration 105/1000 | Loss: 0.00002305
Iteration 106/1000 | Loss: 0.00002305
Iteration 107/1000 | Loss: 0.00002305
Iteration 108/1000 | Loss: 0.00002305
Iteration 109/1000 | Loss: 0.00002305
Iteration 110/1000 | Loss: 0.00002305
Iteration 111/1000 | Loss: 0.00002305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.3051416064845398e-05, 2.3051416064845398e-05, 2.3051416064845398e-05, 2.3051416064845398e-05, 2.3051416064845398e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3051416064845398e-05

Optimization complete. Final v2v error: 3.632878303527832 mm

Highest mean error: 21.410486221313477 mm for frame 98

Lowest mean error: 3.3033595085144043 mm for frame 181

Saving results

Total time: 83.90765571594238
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01117296
Iteration 2/25 | Loss: 0.00288801
Iteration 3/25 | Loss: 0.00203361
Iteration 4/25 | Loss: 0.00173152
Iteration 5/25 | Loss: 0.00164693
Iteration 6/25 | Loss: 0.00162251
Iteration 7/25 | Loss: 0.00135709
Iteration 8/25 | Loss: 0.00125935
Iteration 9/25 | Loss: 0.00124377
Iteration 10/25 | Loss: 0.00119767
Iteration 11/25 | Loss: 0.00115474
Iteration 12/25 | Loss: 0.00116290
Iteration 13/25 | Loss: 0.00113003
Iteration 14/25 | Loss: 0.00112293
Iteration 15/25 | Loss: 0.00110981
Iteration 16/25 | Loss: 0.00110940
Iteration 17/25 | Loss: 0.00111170
Iteration 18/25 | Loss: 0.00110243
Iteration 19/25 | Loss: 0.00109629
Iteration 20/25 | Loss: 0.00109228
Iteration 21/25 | Loss: 0.00108840
Iteration 22/25 | Loss: 0.00108855
Iteration 23/25 | Loss: 0.00109294
Iteration 24/25 | Loss: 0.00108686
Iteration 25/25 | Loss: 0.00108576

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.58669806
Iteration 2/25 | Loss: 0.00186425
Iteration 3/25 | Loss: 0.00159338
Iteration 4/25 | Loss: 0.00159338
Iteration 5/25 | Loss: 0.00159337
Iteration 6/25 | Loss: 0.00159337
Iteration 7/25 | Loss: 0.00159337
Iteration 8/25 | Loss: 0.00159337
Iteration 9/25 | Loss: 0.00159337
Iteration 10/25 | Loss: 0.00159337
Iteration 11/25 | Loss: 0.00159337
Iteration 12/25 | Loss: 0.00159337
Iteration 13/25 | Loss: 0.00159337
Iteration 14/25 | Loss: 0.00159337
Iteration 15/25 | Loss: 0.00159337
Iteration 16/25 | Loss: 0.00159337
Iteration 17/25 | Loss: 0.00159337
Iteration 18/25 | Loss: 0.00159337
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015933722024783492, 0.0015933722024783492, 0.0015933722024783492, 0.0015933722024783492, 0.0015933722024783492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015933722024783492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159337
Iteration 2/1000 | Loss: 0.00063570
Iteration 3/1000 | Loss: 0.00026196
Iteration 4/1000 | Loss: 0.00016424
Iteration 5/1000 | Loss: 0.00021008
Iteration 6/1000 | Loss: 0.00141844
Iteration 7/1000 | Loss: 0.00112177
Iteration 8/1000 | Loss: 0.00146591
Iteration 9/1000 | Loss: 0.00331757
Iteration 10/1000 | Loss: 0.00154807
Iteration 11/1000 | Loss: 0.00122400
Iteration 12/1000 | Loss: 0.00124572
Iteration 13/1000 | Loss: 0.00034737
Iteration 14/1000 | Loss: 0.00016317
Iteration 15/1000 | Loss: 0.00093887
Iteration 16/1000 | Loss: 0.00037532
Iteration 17/1000 | Loss: 0.00089365
Iteration 18/1000 | Loss: 0.00548276
Iteration 19/1000 | Loss: 0.00249073
Iteration 20/1000 | Loss: 0.00129404
Iteration 21/1000 | Loss: 0.00057572
Iteration 22/1000 | Loss: 0.00090331
Iteration 23/1000 | Loss: 0.00010788
Iteration 24/1000 | Loss: 0.00018928
Iteration 25/1000 | Loss: 0.00059071
Iteration 26/1000 | Loss: 0.00015710
Iteration 27/1000 | Loss: 0.00006613
Iteration 28/1000 | Loss: 0.00012685
Iteration 29/1000 | Loss: 0.00005731
Iteration 30/1000 | Loss: 0.00015159
Iteration 31/1000 | Loss: 0.00010220
Iteration 32/1000 | Loss: 0.00014903
Iteration 33/1000 | Loss: 0.00009059
Iteration 34/1000 | Loss: 0.00006829
Iteration 35/1000 | Loss: 0.00005471
Iteration 36/1000 | Loss: 0.00005494
Iteration 37/1000 | Loss: 0.00019594
Iteration 38/1000 | Loss: 0.00005336
Iteration 39/1000 | Loss: 0.00005156
Iteration 40/1000 | Loss: 0.00005154
Iteration 41/1000 | Loss: 0.00015012
Iteration 42/1000 | Loss: 0.00005041
Iteration 43/1000 | Loss: 0.00008773
Iteration 44/1000 | Loss: 0.00004999
Iteration 45/1000 | Loss: 0.00004998
Iteration 46/1000 | Loss: 0.00006749
Iteration 47/1000 | Loss: 0.00013023
Iteration 48/1000 | Loss: 0.00018260
Iteration 49/1000 | Loss: 0.00006105
Iteration 50/1000 | Loss: 0.00005307
Iteration 51/1000 | Loss: 0.00004965
Iteration 52/1000 | Loss: 0.00004962
Iteration 53/1000 | Loss: 0.00004962
Iteration 54/1000 | Loss: 0.00004962
Iteration 55/1000 | Loss: 0.00004962
Iteration 56/1000 | Loss: 0.00004962
Iteration 57/1000 | Loss: 0.00004961
Iteration 58/1000 | Loss: 0.00004961
Iteration 59/1000 | Loss: 0.00004953
Iteration 60/1000 | Loss: 0.00004952
Iteration 61/1000 | Loss: 0.00005418
Iteration 62/1000 | Loss: 0.00005418
Iteration 63/1000 | Loss: 0.00005418
Iteration 64/1000 | Loss: 0.00022261
Iteration 65/1000 | Loss: 0.00006236
Iteration 66/1000 | Loss: 0.00004944
Iteration 67/1000 | Loss: 0.00007262
Iteration 68/1000 | Loss: 0.00006078
Iteration 69/1000 | Loss: 0.00004938
Iteration 70/1000 | Loss: 0.00005516
Iteration 71/1000 | Loss: 0.00014584
Iteration 72/1000 | Loss: 0.00006471
Iteration 73/1000 | Loss: 0.00004931
Iteration 74/1000 | Loss: 0.00004931
Iteration 75/1000 | Loss: 0.00004931
Iteration 76/1000 | Loss: 0.00004931
Iteration 77/1000 | Loss: 0.00004931
Iteration 78/1000 | Loss: 0.00004931
Iteration 79/1000 | Loss: 0.00004931
Iteration 80/1000 | Loss: 0.00004931
Iteration 81/1000 | Loss: 0.00004930
Iteration 82/1000 | Loss: 0.00004930
Iteration 83/1000 | Loss: 0.00004930
Iteration 84/1000 | Loss: 0.00004930
Iteration 85/1000 | Loss: 0.00004930
Iteration 86/1000 | Loss: 0.00004929
Iteration 87/1000 | Loss: 0.00004929
Iteration 88/1000 | Loss: 0.00004929
Iteration 89/1000 | Loss: 0.00004929
Iteration 90/1000 | Loss: 0.00004928
Iteration 91/1000 | Loss: 0.00004928
Iteration 92/1000 | Loss: 0.00004928
Iteration 93/1000 | Loss: 0.00004928
Iteration 94/1000 | Loss: 0.00004928
Iteration 95/1000 | Loss: 0.00004928
Iteration 96/1000 | Loss: 0.00007221
Iteration 97/1000 | Loss: 0.00004926
Iteration 98/1000 | Loss: 0.00004925
Iteration 99/1000 | Loss: 0.00004925
Iteration 100/1000 | Loss: 0.00004924
Iteration 101/1000 | Loss: 0.00004924
Iteration 102/1000 | Loss: 0.00004924
Iteration 103/1000 | Loss: 0.00004924
Iteration 104/1000 | Loss: 0.00004924
Iteration 105/1000 | Loss: 0.00004924
Iteration 106/1000 | Loss: 0.00004924
Iteration 107/1000 | Loss: 0.00004924
Iteration 108/1000 | Loss: 0.00004924
Iteration 109/1000 | Loss: 0.00004924
Iteration 110/1000 | Loss: 0.00004924
Iteration 111/1000 | Loss: 0.00004923
Iteration 112/1000 | Loss: 0.00004923
Iteration 113/1000 | Loss: 0.00004923
Iteration 114/1000 | Loss: 0.00004923
Iteration 115/1000 | Loss: 0.00004923
Iteration 116/1000 | Loss: 0.00004923
Iteration 117/1000 | Loss: 0.00004923
Iteration 118/1000 | Loss: 0.00004923
Iteration 119/1000 | Loss: 0.00004923
Iteration 120/1000 | Loss: 0.00004923
Iteration 121/1000 | Loss: 0.00004923
Iteration 122/1000 | Loss: 0.00004923
Iteration 123/1000 | Loss: 0.00004923
Iteration 124/1000 | Loss: 0.00004922
Iteration 125/1000 | Loss: 0.00004922
Iteration 126/1000 | Loss: 0.00004922
Iteration 127/1000 | Loss: 0.00004922
Iteration 128/1000 | Loss: 0.00004922
Iteration 129/1000 | Loss: 0.00004922
Iteration 130/1000 | Loss: 0.00004922
Iteration 131/1000 | Loss: 0.00004922
Iteration 132/1000 | Loss: 0.00004922
Iteration 133/1000 | Loss: 0.00004922
Iteration 134/1000 | Loss: 0.00004922
Iteration 135/1000 | Loss: 0.00004922
Iteration 136/1000 | Loss: 0.00004922
Iteration 137/1000 | Loss: 0.00004922
Iteration 138/1000 | Loss: 0.00004922
Iteration 139/1000 | Loss: 0.00004922
Iteration 140/1000 | Loss: 0.00004922
Iteration 141/1000 | Loss: 0.00004922
Iteration 142/1000 | Loss: 0.00004922
Iteration 143/1000 | Loss: 0.00004922
Iteration 144/1000 | Loss: 0.00004922
Iteration 145/1000 | Loss: 0.00004922
Iteration 146/1000 | Loss: 0.00004922
Iteration 147/1000 | Loss: 0.00004922
Iteration 148/1000 | Loss: 0.00004922
Iteration 149/1000 | Loss: 0.00004922
Iteration 150/1000 | Loss: 0.00004922
Iteration 151/1000 | Loss: 0.00004922
Iteration 152/1000 | Loss: 0.00004922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [4.921905929222703e-05, 4.921905929222703e-05, 4.921905929222703e-05, 4.921905929222703e-05, 4.921905929222703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.921905929222703e-05

Optimization complete. Final v2v error: 5.859759330749512 mm

Highest mean error: 6.986105442047119 mm for frame 206

Lowest mean error: 5.405320167541504 mm for frame 43

Saving results

Total time: 141.42083644866943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827965
Iteration 2/25 | Loss: 0.00118142
Iteration 3/25 | Loss: 0.00076662
Iteration 4/25 | Loss: 0.00069006
Iteration 5/25 | Loss: 0.00067085
Iteration 6/25 | Loss: 0.00066313
Iteration 7/25 | Loss: 0.00066128
Iteration 8/25 | Loss: 0.00066092
Iteration 9/25 | Loss: 0.00066079
Iteration 10/25 | Loss: 0.00066076
Iteration 11/25 | Loss: 0.00066076
Iteration 12/25 | Loss: 0.00066076
Iteration 13/25 | Loss: 0.00066076
Iteration 14/25 | Loss: 0.00066076
Iteration 15/25 | Loss: 0.00066076
Iteration 16/25 | Loss: 0.00066076
Iteration 17/25 | Loss: 0.00066076
Iteration 18/25 | Loss: 0.00066076
Iteration 19/25 | Loss: 0.00066076
Iteration 20/25 | Loss: 0.00066076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006607641698792577, 0.0006607641698792577, 0.0006607641698792577, 0.0006607641698792577, 0.0006607641698792577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006607641698792577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.23433971
Iteration 2/25 | Loss: 0.00030933
Iteration 3/25 | Loss: 0.00030930
Iteration 4/25 | Loss: 0.00030930
Iteration 5/25 | Loss: 0.00030930
Iteration 6/25 | Loss: 0.00030930
Iteration 7/25 | Loss: 0.00030930
Iteration 8/25 | Loss: 0.00030929
Iteration 9/25 | Loss: 0.00030929
Iteration 10/25 | Loss: 0.00030929
Iteration 11/25 | Loss: 0.00030929
Iteration 12/25 | Loss: 0.00030929
Iteration 13/25 | Loss: 0.00030929
Iteration 14/25 | Loss: 0.00030929
Iteration 15/25 | Loss: 0.00030929
Iteration 16/25 | Loss: 0.00030929
Iteration 17/25 | Loss: 0.00030929
Iteration 18/25 | Loss: 0.00030929
Iteration 19/25 | Loss: 0.00030929
Iteration 20/25 | Loss: 0.00030929
Iteration 21/25 | Loss: 0.00030929
Iteration 22/25 | Loss: 0.00030929
Iteration 23/25 | Loss: 0.00030929
Iteration 24/25 | Loss: 0.00030929
Iteration 25/25 | Loss: 0.00030929

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030929
Iteration 2/1000 | Loss: 0.00002560
Iteration 3/1000 | Loss: 0.00001994
Iteration 4/1000 | Loss: 0.00001887
Iteration 5/1000 | Loss: 0.00001806
Iteration 6/1000 | Loss: 0.00001764
Iteration 7/1000 | Loss: 0.00001722
Iteration 8/1000 | Loss: 0.00001692
Iteration 9/1000 | Loss: 0.00001672
Iteration 10/1000 | Loss: 0.00001671
Iteration 11/1000 | Loss: 0.00001666
Iteration 12/1000 | Loss: 0.00001662
Iteration 13/1000 | Loss: 0.00001657
Iteration 14/1000 | Loss: 0.00001653
Iteration 15/1000 | Loss: 0.00001651
Iteration 16/1000 | Loss: 0.00001645
Iteration 17/1000 | Loss: 0.00001643
Iteration 18/1000 | Loss: 0.00001636
Iteration 19/1000 | Loss: 0.00001634
Iteration 20/1000 | Loss: 0.00001633
Iteration 21/1000 | Loss: 0.00001632
Iteration 22/1000 | Loss: 0.00001632
Iteration 23/1000 | Loss: 0.00001632
Iteration 24/1000 | Loss: 0.00001632
Iteration 25/1000 | Loss: 0.00001631
Iteration 26/1000 | Loss: 0.00001629
Iteration 27/1000 | Loss: 0.00001629
Iteration 28/1000 | Loss: 0.00001628
Iteration 29/1000 | Loss: 0.00001628
Iteration 30/1000 | Loss: 0.00001628
Iteration 31/1000 | Loss: 0.00001627
Iteration 32/1000 | Loss: 0.00001627
Iteration 33/1000 | Loss: 0.00001627
Iteration 34/1000 | Loss: 0.00001626
Iteration 35/1000 | Loss: 0.00001626
Iteration 36/1000 | Loss: 0.00001626
Iteration 37/1000 | Loss: 0.00001626
Iteration 38/1000 | Loss: 0.00001625
Iteration 39/1000 | Loss: 0.00001625
Iteration 40/1000 | Loss: 0.00001625
Iteration 41/1000 | Loss: 0.00001624
Iteration 42/1000 | Loss: 0.00001624
Iteration 43/1000 | Loss: 0.00001624
Iteration 44/1000 | Loss: 0.00001623
Iteration 45/1000 | Loss: 0.00001623
Iteration 46/1000 | Loss: 0.00001623
Iteration 47/1000 | Loss: 0.00001623
Iteration 48/1000 | Loss: 0.00001623
Iteration 49/1000 | Loss: 0.00001622
Iteration 50/1000 | Loss: 0.00001622
Iteration 51/1000 | Loss: 0.00001622
Iteration 52/1000 | Loss: 0.00001621
Iteration 53/1000 | Loss: 0.00001621
Iteration 54/1000 | Loss: 0.00001621
Iteration 55/1000 | Loss: 0.00001620
Iteration 56/1000 | Loss: 0.00001620
Iteration 57/1000 | Loss: 0.00001620
Iteration 58/1000 | Loss: 0.00001620
Iteration 59/1000 | Loss: 0.00001620
Iteration 60/1000 | Loss: 0.00001620
Iteration 61/1000 | Loss: 0.00001619
Iteration 62/1000 | Loss: 0.00001619
Iteration 63/1000 | Loss: 0.00001619
Iteration 64/1000 | Loss: 0.00001619
Iteration 65/1000 | Loss: 0.00001619
Iteration 66/1000 | Loss: 0.00001618
Iteration 67/1000 | Loss: 0.00001618
Iteration 68/1000 | Loss: 0.00001618
Iteration 69/1000 | Loss: 0.00001618
Iteration 70/1000 | Loss: 0.00001618
Iteration 71/1000 | Loss: 0.00001618
Iteration 72/1000 | Loss: 0.00001618
Iteration 73/1000 | Loss: 0.00001618
Iteration 74/1000 | Loss: 0.00001618
Iteration 75/1000 | Loss: 0.00001617
Iteration 76/1000 | Loss: 0.00001617
Iteration 77/1000 | Loss: 0.00001617
Iteration 78/1000 | Loss: 0.00001617
Iteration 79/1000 | Loss: 0.00001617
Iteration 80/1000 | Loss: 0.00001617
Iteration 81/1000 | Loss: 0.00001617
Iteration 82/1000 | Loss: 0.00001617
Iteration 83/1000 | Loss: 0.00001617
Iteration 84/1000 | Loss: 0.00001617
Iteration 85/1000 | Loss: 0.00001617
Iteration 86/1000 | Loss: 0.00001617
Iteration 87/1000 | Loss: 0.00001617
Iteration 88/1000 | Loss: 0.00001616
Iteration 89/1000 | Loss: 0.00001616
Iteration 90/1000 | Loss: 0.00001616
Iteration 91/1000 | Loss: 0.00001616
Iteration 92/1000 | Loss: 0.00001616
Iteration 93/1000 | Loss: 0.00001616
Iteration 94/1000 | Loss: 0.00001616
Iteration 95/1000 | Loss: 0.00001616
Iteration 96/1000 | Loss: 0.00001616
Iteration 97/1000 | Loss: 0.00001616
Iteration 98/1000 | Loss: 0.00001616
Iteration 99/1000 | Loss: 0.00001616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.6163750842679292e-05, 1.6163750842679292e-05, 1.6163750842679292e-05, 1.6163750842679292e-05, 1.6163750842679292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6163750842679292e-05

Optimization complete. Final v2v error: 3.389669418334961 mm

Highest mean error: 3.7896015644073486 mm for frame 40

Lowest mean error: 3.0807111263275146 mm for frame 194

Saving results

Total time: 42.61340570449829
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025876
Iteration 2/25 | Loss: 0.01025876
Iteration 3/25 | Loss: 0.00418712
Iteration 4/25 | Loss: 0.00244364
Iteration 5/25 | Loss: 0.00221679
Iteration 6/25 | Loss: 0.00182365
Iteration 7/25 | Loss: 0.00162981
Iteration 8/25 | Loss: 0.00147901
Iteration 9/25 | Loss: 0.00142204
Iteration 10/25 | Loss: 0.00138818
Iteration 11/25 | Loss: 0.00141614
Iteration 12/25 | Loss: 0.00134472
Iteration 13/25 | Loss: 0.00126608
Iteration 14/25 | Loss: 0.00122356
Iteration 15/25 | Loss: 0.00119992
Iteration 16/25 | Loss: 0.00119608
Iteration 17/25 | Loss: 0.00119403
Iteration 18/25 | Loss: 0.00119025
Iteration 19/25 | Loss: 0.00118764
Iteration 20/25 | Loss: 0.00118681
Iteration 21/25 | Loss: 0.00118758
Iteration 22/25 | Loss: 0.00118326
Iteration 23/25 | Loss: 0.00118406
Iteration 24/25 | Loss: 0.00118170
Iteration 25/25 | Loss: 0.00118077

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40768540
Iteration 2/25 | Loss: 0.00472371
Iteration 3/25 | Loss: 0.00472371
Iteration 4/25 | Loss: 0.00472371
Iteration 5/25 | Loss: 0.00469600
Iteration 6/25 | Loss: 0.00469600
Iteration 7/25 | Loss: 0.00469600
Iteration 8/25 | Loss: 0.00469600
Iteration 9/25 | Loss: 0.00469600
Iteration 10/25 | Loss: 0.00469600
Iteration 11/25 | Loss: 0.00469600
Iteration 12/25 | Loss: 0.00469600
Iteration 13/25 | Loss: 0.00469600
Iteration 14/25 | Loss: 0.00469600
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.004695999901741743, 0.004695999901741743, 0.004695999901741743, 0.004695999901741743, 0.004695999901741743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004695999901741743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00469600
Iteration 2/1000 | Loss: 0.00103747
Iteration 3/1000 | Loss: 0.00056763
Iteration 4/1000 | Loss: 0.00064077
Iteration 5/1000 | Loss: 0.00044920
Iteration 6/1000 | Loss: 0.00473531
Iteration 7/1000 | Loss: 0.00096791
Iteration 8/1000 | Loss: 0.00275251
Iteration 9/1000 | Loss: 0.00101028
Iteration 10/1000 | Loss: 0.00521034
Iteration 11/1000 | Loss: 0.00131637
Iteration 12/1000 | Loss: 0.00911274
Iteration 13/1000 | Loss: 0.00202748
Iteration 14/1000 | Loss: 0.00798260
Iteration 15/1000 | Loss: 0.00136328
Iteration 16/1000 | Loss: 0.00791431
Iteration 17/1000 | Loss: 0.00140218
Iteration 18/1000 | Loss: 0.00474799
Iteration 19/1000 | Loss: 0.00295309
Iteration 20/1000 | Loss: 0.00045205
Iteration 21/1000 | Loss: 0.00041459
Iteration 22/1000 | Loss: 0.00199608
Iteration 23/1000 | Loss: 0.00036161
Iteration 24/1000 | Loss: 0.00080741
Iteration 25/1000 | Loss: 0.00044136
Iteration 26/1000 | Loss: 0.00100488
Iteration 27/1000 | Loss: 0.00056442
Iteration 28/1000 | Loss: 0.00065076
Iteration 29/1000 | Loss: 0.00092204
Iteration 30/1000 | Loss: 0.00197514
Iteration 31/1000 | Loss: 0.00093518
Iteration 32/1000 | Loss: 0.00013591
Iteration 33/1000 | Loss: 0.00018170
Iteration 34/1000 | Loss: 0.00011841
Iteration 35/1000 | Loss: 0.00086886
Iteration 36/1000 | Loss: 0.00010951
Iteration 37/1000 | Loss: 0.00019570
Iteration 38/1000 | Loss: 0.00021241
Iteration 39/1000 | Loss: 0.00007523
Iteration 40/1000 | Loss: 0.00017398
Iteration 41/1000 | Loss: 0.00080668
Iteration 42/1000 | Loss: 0.00028569
Iteration 43/1000 | Loss: 0.00036931
Iteration 44/1000 | Loss: 0.00012297
Iteration 45/1000 | Loss: 0.00037251
Iteration 46/1000 | Loss: 0.00021844
Iteration 47/1000 | Loss: 0.00016315
Iteration 48/1000 | Loss: 0.00029364
Iteration 49/1000 | Loss: 0.00035232
Iteration 50/1000 | Loss: 0.00028664
Iteration 51/1000 | Loss: 0.00047042
Iteration 52/1000 | Loss: 0.00005997
Iteration 53/1000 | Loss: 0.00005447
Iteration 54/1000 | Loss: 0.00010027
Iteration 55/1000 | Loss: 0.00005113
Iteration 56/1000 | Loss: 0.00020242
Iteration 57/1000 | Loss: 0.00013576
Iteration 58/1000 | Loss: 0.00022401
Iteration 59/1000 | Loss: 0.00013663
Iteration 60/1000 | Loss: 0.00004913
Iteration 61/1000 | Loss: 0.00022517
Iteration 62/1000 | Loss: 0.00014289
Iteration 63/1000 | Loss: 0.00022354
Iteration 64/1000 | Loss: 0.00012433
Iteration 65/1000 | Loss: 0.00086321
Iteration 66/1000 | Loss: 0.00048371
Iteration 67/1000 | Loss: 0.00105613
Iteration 68/1000 | Loss: 0.00054374
Iteration 69/1000 | Loss: 0.00077155
Iteration 70/1000 | Loss: 0.00069103
Iteration 71/1000 | Loss: 0.00007347
Iteration 72/1000 | Loss: 0.00024026
Iteration 73/1000 | Loss: 0.00009836
Iteration 74/1000 | Loss: 0.00004927
Iteration 75/1000 | Loss: 0.00021098
Iteration 76/1000 | Loss: 0.00024344
Iteration 77/1000 | Loss: 0.00005127
Iteration 78/1000 | Loss: 0.00004253
Iteration 79/1000 | Loss: 0.00003895
Iteration 80/1000 | Loss: 0.00003672
Iteration 81/1000 | Loss: 0.00022024
Iteration 82/1000 | Loss: 0.00004296
Iteration 83/1000 | Loss: 0.00003506
Iteration 84/1000 | Loss: 0.00018164
Iteration 85/1000 | Loss: 0.00004688
Iteration 86/1000 | Loss: 0.00003336
Iteration 87/1000 | Loss: 0.00002982
Iteration 88/1000 | Loss: 0.00002862
Iteration 89/1000 | Loss: 0.00002789
Iteration 90/1000 | Loss: 0.00002728
Iteration 91/1000 | Loss: 0.00002685
Iteration 92/1000 | Loss: 0.00002652
Iteration 93/1000 | Loss: 0.00002626
Iteration 94/1000 | Loss: 0.00002608
Iteration 95/1000 | Loss: 0.00002607
Iteration 96/1000 | Loss: 0.00002593
Iteration 97/1000 | Loss: 0.00002593
Iteration 98/1000 | Loss: 0.00002593
Iteration 99/1000 | Loss: 0.00002593
Iteration 100/1000 | Loss: 0.00002593
Iteration 101/1000 | Loss: 0.00002593
Iteration 102/1000 | Loss: 0.00002592
Iteration 103/1000 | Loss: 0.00002592
Iteration 104/1000 | Loss: 0.00002592
Iteration 105/1000 | Loss: 0.00002592
Iteration 106/1000 | Loss: 0.00002592
Iteration 107/1000 | Loss: 0.00002591
Iteration 108/1000 | Loss: 0.00002591
Iteration 109/1000 | Loss: 0.00002588
Iteration 110/1000 | Loss: 0.00002587
Iteration 111/1000 | Loss: 0.00002587
Iteration 112/1000 | Loss: 0.00002587
Iteration 113/1000 | Loss: 0.00002587
Iteration 114/1000 | Loss: 0.00002587
Iteration 115/1000 | Loss: 0.00002587
Iteration 116/1000 | Loss: 0.00002587
Iteration 117/1000 | Loss: 0.00002587
Iteration 118/1000 | Loss: 0.00002587
Iteration 119/1000 | Loss: 0.00002587
Iteration 120/1000 | Loss: 0.00002587
Iteration 121/1000 | Loss: 0.00002586
Iteration 122/1000 | Loss: 0.00002586
Iteration 123/1000 | Loss: 0.00002586
Iteration 124/1000 | Loss: 0.00002585
Iteration 125/1000 | Loss: 0.00002585
Iteration 126/1000 | Loss: 0.00002584
Iteration 127/1000 | Loss: 0.00002584
Iteration 128/1000 | Loss: 0.00002582
Iteration 129/1000 | Loss: 0.00002582
Iteration 130/1000 | Loss: 0.00002582
Iteration 131/1000 | Loss: 0.00002580
Iteration 132/1000 | Loss: 0.00002580
Iteration 133/1000 | Loss: 0.00002580
Iteration 134/1000 | Loss: 0.00002580
Iteration 135/1000 | Loss: 0.00002580
Iteration 136/1000 | Loss: 0.00002580
Iteration 137/1000 | Loss: 0.00002580
Iteration 138/1000 | Loss: 0.00002580
Iteration 139/1000 | Loss: 0.00002579
Iteration 140/1000 | Loss: 0.00002579
Iteration 141/1000 | Loss: 0.00002578
Iteration 142/1000 | Loss: 0.00002577
Iteration 143/1000 | Loss: 0.00002576
Iteration 144/1000 | Loss: 0.00002576
Iteration 145/1000 | Loss: 0.00002576
Iteration 146/1000 | Loss: 0.00002575
Iteration 147/1000 | Loss: 0.00002574
Iteration 148/1000 | Loss: 0.00002574
Iteration 149/1000 | Loss: 0.00002574
Iteration 150/1000 | Loss: 0.00002574
Iteration 151/1000 | Loss: 0.00002574
Iteration 152/1000 | Loss: 0.00002574
Iteration 153/1000 | Loss: 0.00002574
Iteration 154/1000 | Loss: 0.00002573
Iteration 155/1000 | Loss: 0.00002573
Iteration 156/1000 | Loss: 0.00002573
Iteration 157/1000 | Loss: 0.00002573
Iteration 158/1000 | Loss: 0.00002573
Iteration 159/1000 | Loss: 0.00002573
Iteration 160/1000 | Loss: 0.00002573
Iteration 161/1000 | Loss: 0.00002573
Iteration 162/1000 | Loss: 0.00002573
Iteration 163/1000 | Loss: 0.00002573
Iteration 164/1000 | Loss: 0.00002572
Iteration 165/1000 | Loss: 0.00002572
Iteration 166/1000 | Loss: 0.00002572
Iteration 167/1000 | Loss: 0.00002572
Iteration 168/1000 | Loss: 0.00002572
Iteration 169/1000 | Loss: 0.00002572
Iteration 170/1000 | Loss: 0.00002572
Iteration 171/1000 | Loss: 0.00002572
Iteration 172/1000 | Loss: 0.00002572
Iteration 173/1000 | Loss: 0.00002572
Iteration 174/1000 | Loss: 0.00002572
Iteration 175/1000 | Loss: 0.00002572
Iteration 176/1000 | Loss: 0.00002572
Iteration 177/1000 | Loss: 0.00002572
Iteration 178/1000 | Loss: 0.00002572
Iteration 179/1000 | Loss: 0.00002571
Iteration 180/1000 | Loss: 0.00002571
Iteration 181/1000 | Loss: 0.00002571
Iteration 182/1000 | Loss: 0.00002571
Iteration 183/1000 | Loss: 0.00002571
Iteration 184/1000 | Loss: 0.00002571
Iteration 185/1000 | Loss: 0.00002571
Iteration 186/1000 | Loss: 0.00002571
Iteration 187/1000 | Loss: 0.00002571
Iteration 188/1000 | Loss: 0.00002571
Iteration 189/1000 | Loss: 0.00002571
Iteration 190/1000 | Loss: 0.00002571
Iteration 191/1000 | Loss: 0.00002571
Iteration 192/1000 | Loss: 0.00002571
Iteration 193/1000 | Loss: 0.00002571
Iteration 194/1000 | Loss: 0.00002571
Iteration 195/1000 | Loss: 0.00002571
Iteration 196/1000 | Loss: 0.00002571
Iteration 197/1000 | Loss: 0.00002571
Iteration 198/1000 | Loss: 0.00002571
Iteration 199/1000 | Loss: 0.00002571
Iteration 200/1000 | Loss: 0.00002571
Iteration 201/1000 | Loss: 0.00002571
Iteration 202/1000 | Loss: 0.00002571
Iteration 203/1000 | Loss: 0.00002571
Iteration 204/1000 | Loss: 0.00002571
Iteration 205/1000 | Loss: 0.00002571
Iteration 206/1000 | Loss: 0.00002571
Iteration 207/1000 | Loss: 0.00002571
Iteration 208/1000 | Loss: 0.00002571
Iteration 209/1000 | Loss: 0.00002571
Iteration 210/1000 | Loss: 0.00002571
Iteration 211/1000 | Loss: 0.00002571
Iteration 212/1000 | Loss: 0.00002571
Iteration 213/1000 | Loss: 0.00002571
Iteration 214/1000 | Loss: 0.00002571
Iteration 215/1000 | Loss: 0.00002571
Iteration 216/1000 | Loss: 0.00002571
Iteration 217/1000 | Loss: 0.00002571
Iteration 218/1000 | Loss: 0.00002571
Iteration 219/1000 | Loss: 0.00002571
Iteration 220/1000 | Loss: 0.00002571
Iteration 221/1000 | Loss: 0.00002571
Iteration 222/1000 | Loss: 0.00002571
Iteration 223/1000 | Loss: 0.00002571
Iteration 224/1000 | Loss: 0.00002571
Iteration 225/1000 | Loss: 0.00002571
Iteration 226/1000 | Loss: 0.00002571
Iteration 227/1000 | Loss: 0.00002571
Iteration 228/1000 | Loss: 0.00002571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [2.570856304373592e-05, 2.570856304373592e-05, 2.570856304373592e-05, 2.570856304373592e-05, 2.570856304373592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.570856304373592e-05

Optimization complete. Final v2v error: 3.990311622619629 mm

Highest mean error: 11.68439769744873 mm for frame 53

Lowest mean error: 3.49593448638916 mm for frame 7

Saving results

Total time: 205.01748752593994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971668
Iteration 2/25 | Loss: 0.00182199
Iteration 3/25 | Loss: 0.00127589
Iteration 4/25 | Loss: 0.00112779
Iteration 5/25 | Loss: 0.00118723
Iteration 6/25 | Loss: 0.00108141
Iteration 7/25 | Loss: 0.00105809
Iteration 8/25 | Loss: 0.00104999
Iteration 9/25 | Loss: 0.00101030
Iteration 10/25 | Loss: 0.00099809
Iteration 11/25 | Loss: 0.00099045
Iteration 12/25 | Loss: 0.00098383
Iteration 13/25 | Loss: 0.00096876
Iteration 14/25 | Loss: 0.00095935
Iteration 15/25 | Loss: 0.00094912
Iteration 16/25 | Loss: 0.00094965
Iteration 17/25 | Loss: 0.00094489
Iteration 18/25 | Loss: 0.00094433
Iteration 19/25 | Loss: 0.00094982
Iteration 20/25 | Loss: 0.00094644
Iteration 21/25 | Loss: 0.00093394
Iteration 22/25 | Loss: 0.00094791
Iteration 23/25 | Loss: 0.00093622
Iteration 24/25 | Loss: 0.00092280
Iteration 25/25 | Loss: 0.00091793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.77166295
Iteration 2/25 | Loss: 0.00125956
Iteration 3/25 | Loss: 0.00125942
Iteration 4/25 | Loss: 0.00125942
Iteration 5/25 | Loss: 0.00125942
Iteration 6/25 | Loss: 0.00125941
Iteration 7/25 | Loss: 0.00125941
Iteration 8/25 | Loss: 0.00125941
Iteration 9/25 | Loss: 0.00125941
Iteration 10/25 | Loss: 0.00125941
Iteration 11/25 | Loss: 0.00125941
Iteration 12/25 | Loss: 0.00125941
Iteration 13/25 | Loss: 0.00125941
Iteration 14/25 | Loss: 0.00125941
Iteration 15/25 | Loss: 0.00125941
Iteration 16/25 | Loss: 0.00125941
Iteration 17/25 | Loss: 0.00125941
Iteration 18/25 | Loss: 0.00125941
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001259413082152605, 0.001259413082152605, 0.001259413082152605, 0.001259413082152605, 0.001259413082152605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001259413082152605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125941
Iteration 2/1000 | Loss: 0.00344230
Iteration 3/1000 | Loss: 0.00354987
Iteration 4/1000 | Loss: 0.00064099
Iteration 5/1000 | Loss: 0.00296900
Iteration 6/1000 | Loss: 0.00019718
Iteration 7/1000 | Loss: 0.00021520
Iteration 8/1000 | Loss: 0.00247260
Iteration 9/1000 | Loss: 0.00054510
Iteration 10/1000 | Loss: 0.00067375
Iteration 11/1000 | Loss: 0.00037458
Iteration 12/1000 | Loss: 0.00071120
Iteration 13/1000 | Loss: 0.00102980
Iteration 14/1000 | Loss: 0.00044105
Iteration 15/1000 | Loss: 0.00143328
Iteration 16/1000 | Loss: 0.00075026
Iteration 17/1000 | Loss: 0.00079390
Iteration 18/1000 | Loss: 0.00056945
Iteration 19/1000 | Loss: 0.00045520
Iteration 20/1000 | Loss: 0.00035445
Iteration 21/1000 | Loss: 0.00006812
Iteration 22/1000 | Loss: 0.00009037
Iteration 23/1000 | Loss: 0.00064360
Iteration 24/1000 | Loss: 0.00044003
Iteration 25/1000 | Loss: 0.00005531
Iteration 26/1000 | Loss: 0.00014500
Iteration 27/1000 | Loss: 0.00005182
Iteration 28/1000 | Loss: 0.00007798
Iteration 29/1000 | Loss: 0.00010316
Iteration 30/1000 | Loss: 0.00004888
Iteration 31/1000 | Loss: 0.00007608
Iteration 32/1000 | Loss: 0.00010777
Iteration 33/1000 | Loss: 0.00081897
Iteration 34/1000 | Loss: 0.00279766
Iteration 35/1000 | Loss: 0.00055082
Iteration 36/1000 | Loss: 0.00189026
Iteration 37/1000 | Loss: 0.00214325
Iteration 38/1000 | Loss: 0.00208534
Iteration 39/1000 | Loss: 0.00135893
Iteration 40/1000 | Loss: 0.00124000
Iteration 41/1000 | Loss: 0.00099053
Iteration 42/1000 | Loss: 0.00117713
Iteration 43/1000 | Loss: 0.00129331
Iteration 44/1000 | Loss: 0.00226180
Iteration 45/1000 | Loss: 0.00137260
Iteration 46/1000 | Loss: 0.00217953
Iteration 47/1000 | Loss: 0.00233058
Iteration 48/1000 | Loss: 0.00094855
Iteration 49/1000 | Loss: 0.00175382
Iteration 50/1000 | Loss: 0.00080935
Iteration 51/1000 | Loss: 0.00009260
Iteration 52/1000 | Loss: 0.00230741
Iteration 53/1000 | Loss: 0.00170520
Iteration 54/1000 | Loss: 0.00033926
Iteration 55/1000 | Loss: 0.00040651
Iteration 56/1000 | Loss: 0.00083812
Iteration 57/1000 | Loss: 0.00328710
Iteration 58/1000 | Loss: 0.00258314
Iteration 59/1000 | Loss: 0.00119956
Iteration 60/1000 | Loss: 0.00098951
Iteration 61/1000 | Loss: 0.00173825
Iteration 62/1000 | Loss: 0.00099098
Iteration 63/1000 | Loss: 0.00214503
Iteration 64/1000 | Loss: 0.00104563
Iteration 65/1000 | Loss: 0.00020162
Iteration 66/1000 | Loss: 0.00038284
Iteration 67/1000 | Loss: 0.00049829
Iteration 68/1000 | Loss: 0.00014711
Iteration 69/1000 | Loss: 0.00011938
Iteration 70/1000 | Loss: 0.00006123
Iteration 71/1000 | Loss: 0.00006508
Iteration 72/1000 | Loss: 0.00013169
Iteration 73/1000 | Loss: 0.00013467
Iteration 74/1000 | Loss: 0.00005210
Iteration 75/1000 | Loss: 0.00005881
Iteration 76/1000 | Loss: 0.00005401
Iteration 77/1000 | Loss: 0.00005987
Iteration 78/1000 | Loss: 0.00010169
Iteration 79/1000 | Loss: 0.00008168
Iteration 80/1000 | Loss: 0.00016805
Iteration 81/1000 | Loss: 0.00016161
Iteration 82/1000 | Loss: 0.00022842
Iteration 83/1000 | Loss: 0.00017893
Iteration 84/1000 | Loss: 0.00021467
Iteration 85/1000 | Loss: 0.00016926
Iteration 86/1000 | Loss: 0.00020652
Iteration 87/1000 | Loss: 0.00019212
Iteration 88/1000 | Loss: 0.00019901
Iteration 89/1000 | Loss: 0.00006037
Iteration 90/1000 | Loss: 0.00004522
Iteration 91/1000 | Loss: 0.00004281
Iteration 92/1000 | Loss: 0.00004185
Iteration 93/1000 | Loss: 0.00004117
Iteration 94/1000 | Loss: 0.00004072
Iteration 95/1000 | Loss: 0.00004034
Iteration 96/1000 | Loss: 0.00004870
Iteration 97/1000 | Loss: 0.00004184
Iteration 98/1000 | Loss: 0.00004037
Iteration 99/1000 | Loss: 0.00003990
Iteration 100/1000 | Loss: 0.00003948
Iteration 101/1000 | Loss: 0.00003911
Iteration 102/1000 | Loss: 0.00003859
Iteration 103/1000 | Loss: 0.00003792
Iteration 104/1000 | Loss: 0.00003757
Iteration 105/1000 | Loss: 0.00003709
Iteration 106/1000 | Loss: 0.00006103
Iteration 107/1000 | Loss: 0.00004578
Iteration 108/1000 | Loss: 0.00003745
Iteration 109/1000 | Loss: 0.00003681
Iteration 110/1000 | Loss: 0.00003625
Iteration 111/1000 | Loss: 0.00005920
Iteration 112/1000 | Loss: 0.00005202
Iteration 113/1000 | Loss: 0.00005875
Iteration 114/1000 | Loss: 0.00004910
Iteration 115/1000 | Loss: 0.00005842
Iteration 116/1000 | Loss: 0.00004334
Iteration 117/1000 | Loss: 0.00005424
Iteration 118/1000 | Loss: 0.00004932
Iteration 119/1000 | Loss: 0.00005351
Iteration 120/1000 | Loss: 0.00004803
Iteration 121/1000 | Loss: 0.00003626
Iteration 122/1000 | Loss: 0.00005576
Iteration 123/1000 | Loss: 0.00004552
Iteration 124/1000 | Loss: 0.00005782
Iteration 125/1000 | Loss: 0.00004561
Iteration 126/1000 | Loss: 0.00005152
Iteration 127/1000 | Loss: 0.00004576
Iteration 128/1000 | Loss: 0.00004978
Iteration 129/1000 | Loss: 0.00004441
Iteration 130/1000 | Loss: 0.00005302
Iteration 131/1000 | Loss: 0.00004173
Iteration 132/1000 | Loss: 0.00005222
Iteration 133/1000 | Loss: 0.00004186
Iteration 134/1000 | Loss: 0.00004954
Iteration 135/1000 | Loss: 0.00003766
Iteration 136/1000 | Loss: 0.00003729
Iteration 137/1000 | Loss: 0.00003710
Iteration 138/1000 | Loss: 0.00003694
Iteration 139/1000 | Loss: 0.00003689
Iteration 140/1000 | Loss: 0.00003665
Iteration 141/1000 | Loss: 0.00003639
Iteration 142/1000 | Loss: 0.00003609
Iteration 143/1000 | Loss: 0.00003560
Iteration 144/1000 | Loss: 0.00003529
Iteration 145/1000 | Loss: 0.00003506
Iteration 146/1000 | Loss: 0.00003486
Iteration 147/1000 | Loss: 0.00003486
Iteration 148/1000 | Loss: 0.00003481
Iteration 149/1000 | Loss: 0.00003481
Iteration 150/1000 | Loss: 0.00003476
Iteration 151/1000 | Loss: 0.00003476
Iteration 152/1000 | Loss: 0.00003476
Iteration 153/1000 | Loss: 0.00003475
Iteration 154/1000 | Loss: 0.00003475
Iteration 155/1000 | Loss: 0.00003475
Iteration 156/1000 | Loss: 0.00003475
Iteration 157/1000 | Loss: 0.00003475
Iteration 158/1000 | Loss: 0.00003475
Iteration 159/1000 | Loss: 0.00003475
Iteration 160/1000 | Loss: 0.00003475
Iteration 161/1000 | Loss: 0.00003474
Iteration 162/1000 | Loss: 0.00003474
Iteration 163/1000 | Loss: 0.00003474
Iteration 164/1000 | Loss: 0.00003474
Iteration 165/1000 | Loss: 0.00003474
Iteration 166/1000 | Loss: 0.00003474
Iteration 167/1000 | Loss: 0.00003474
Iteration 168/1000 | Loss: 0.00003474
Iteration 169/1000 | Loss: 0.00003474
Iteration 170/1000 | Loss: 0.00003473
Iteration 171/1000 | Loss: 0.00003473
Iteration 172/1000 | Loss: 0.00003473
Iteration 173/1000 | Loss: 0.00003473
Iteration 174/1000 | Loss: 0.00003473
Iteration 175/1000 | Loss: 0.00003473
Iteration 176/1000 | Loss: 0.00003473
Iteration 177/1000 | Loss: 0.00003473
Iteration 178/1000 | Loss: 0.00003473
Iteration 179/1000 | Loss: 0.00003473
Iteration 180/1000 | Loss: 0.00003473
Iteration 181/1000 | Loss: 0.00003473
Iteration 182/1000 | Loss: 0.00003472
Iteration 183/1000 | Loss: 0.00003472
Iteration 184/1000 | Loss: 0.00003472
Iteration 185/1000 | Loss: 0.00003472
Iteration 186/1000 | Loss: 0.00003472
Iteration 187/1000 | Loss: 0.00003472
Iteration 188/1000 | Loss: 0.00003472
Iteration 189/1000 | Loss: 0.00003472
Iteration 190/1000 | Loss: 0.00003472
Iteration 191/1000 | Loss: 0.00003472
Iteration 192/1000 | Loss: 0.00003472
Iteration 193/1000 | Loss: 0.00003472
Iteration 194/1000 | Loss: 0.00003472
Iteration 195/1000 | Loss: 0.00003471
Iteration 196/1000 | Loss: 0.00003471
Iteration 197/1000 | Loss: 0.00003471
Iteration 198/1000 | Loss: 0.00003471
Iteration 199/1000 | Loss: 0.00003471
Iteration 200/1000 | Loss: 0.00003471
Iteration 201/1000 | Loss: 0.00003471
Iteration 202/1000 | Loss: 0.00003471
Iteration 203/1000 | Loss: 0.00003471
Iteration 204/1000 | Loss: 0.00003471
Iteration 205/1000 | Loss: 0.00003471
Iteration 206/1000 | Loss: 0.00003471
Iteration 207/1000 | Loss: 0.00003471
Iteration 208/1000 | Loss: 0.00003470
Iteration 209/1000 | Loss: 0.00003470
Iteration 210/1000 | Loss: 0.00003470
Iteration 211/1000 | Loss: 0.00003470
Iteration 212/1000 | Loss: 0.00003470
Iteration 213/1000 | Loss: 0.00003470
Iteration 214/1000 | Loss: 0.00003470
Iteration 215/1000 | Loss: 0.00003470
Iteration 216/1000 | Loss: 0.00003469
Iteration 217/1000 | Loss: 0.00003469
Iteration 218/1000 | Loss: 0.00003469
Iteration 219/1000 | Loss: 0.00003469
Iteration 220/1000 | Loss: 0.00003469
Iteration 221/1000 | Loss: 0.00003469
Iteration 222/1000 | Loss: 0.00003469
Iteration 223/1000 | Loss: 0.00003468
Iteration 224/1000 | Loss: 0.00003468
Iteration 225/1000 | Loss: 0.00094838
Iteration 226/1000 | Loss: 0.00010764
Iteration 227/1000 | Loss: 0.00003736
Iteration 228/1000 | Loss: 0.00003524
Iteration 229/1000 | Loss: 0.00003485
Iteration 230/1000 | Loss: 0.00003475
Iteration 231/1000 | Loss: 0.00003475
Iteration 232/1000 | Loss: 0.00003475
Iteration 233/1000 | Loss: 0.00003475
Iteration 234/1000 | Loss: 0.00003473
Iteration 235/1000 | Loss: 0.00003472
Iteration 236/1000 | Loss: 0.00003471
Iteration 237/1000 | Loss: 0.00003471
Iteration 238/1000 | Loss: 0.00003470
Iteration 239/1000 | Loss: 0.00003470
Iteration 240/1000 | Loss: 0.00003470
Iteration 241/1000 | Loss: 0.00003469
Iteration 242/1000 | Loss: 0.00003469
Iteration 243/1000 | Loss: 0.00003468
Iteration 244/1000 | Loss: 0.00003468
Iteration 245/1000 | Loss: 0.00003468
Iteration 246/1000 | Loss: 0.00003467
Iteration 247/1000 | Loss: 0.00003467
Iteration 248/1000 | Loss: 0.00003467
Iteration 249/1000 | Loss: 0.00003467
Iteration 250/1000 | Loss: 0.00003466
Iteration 251/1000 | Loss: 0.00003466
Iteration 252/1000 | Loss: 0.00003466
Iteration 253/1000 | Loss: 0.00003466
Iteration 254/1000 | Loss: 0.00003465
Iteration 255/1000 | Loss: 0.00003465
Iteration 256/1000 | Loss: 0.00003465
Iteration 257/1000 | Loss: 0.00003464
Iteration 258/1000 | Loss: 0.00003464
Iteration 259/1000 | Loss: 0.00003464
Iteration 260/1000 | Loss: 0.00003464
Iteration 261/1000 | Loss: 0.00003464
Iteration 262/1000 | Loss: 0.00003464
Iteration 263/1000 | Loss: 0.00003463
Iteration 264/1000 | Loss: 0.00003463
Iteration 265/1000 | Loss: 0.00003463
Iteration 266/1000 | Loss: 0.00003463
Iteration 267/1000 | Loss: 0.00003463
Iteration 268/1000 | Loss: 0.00003462
Iteration 269/1000 | Loss: 0.00003461
Iteration 270/1000 | Loss: 0.00003461
Iteration 271/1000 | Loss: 0.00003460
Iteration 272/1000 | Loss: 0.00003460
Iteration 273/1000 | Loss: 0.00003460
Iteration 274/1000 | Loss: 0.00003459
Iteration 275/1000 | Loss: 0.00003459
Iteration 276/1000 | Loss: 0.00003459
Iteration 277/1000 | Loss: 0.00003458
Iteration 278/1000 | Loss: 0.00003458
Iteration 279/1000 | Loss: 0.00003457
Iteration 280/1000 | Loss: 0.00003457
Iteration 281/1000 | Loss: 0.00003457
Iteration 282/1000 | Loss: 0.00003456
Iteration 283/1000 | Loss: 0.00003456
Iteration 284/1000 | Loss: 0.00003456
Iteration 285/1000 | Loss: 0.00003455
Iteration 286/1000 | Loss: 0.00003455
Iteration 287/1000 | Loss: 0.00003454
Iteration 288/1000 | Loss: 0.00003454
Iteration 289/1000 | Loss: 0.00003454
Iteration 290/1000 | Loss: 0.00003453
Iteration 291/1000 | Loss: 0.00003453
Iteration 292/1000 | Loss: 0.00003453
Iteration 293/1000 | Loss: 0.00003453
Iteration 294/1000 | Loss: 0.00003453
Iteration 295/1000 | Loss: 0.00003453
Iteration 296/1000 | Loss: 0.00003453
Iteration 297/1000 | Loss: 0.00003453
Iteration 298/1000 | Loss: 0.00003453
Iteration 299/1000 | Loss: 0.00003452
Iteration 300/1000 | Loss: 0.00003452
Iteration 301/1000 | Loss: 0.00003452
Iteration 302/1000 | Loss: 0.00003452
Iteration 303/1000 | Loss: 0.00003452
Iteration 304/1000 | Loss: 0.00003452
Iteration 305/1000 | Loss: 0.00003452
Iteration 306/1000 | Loss: 0.00003452
Iteration 307/1000 | Loss: 0.00003452
Iteration 308/1000 | Loss: 0.00003452
Iteration 309/1000 | Loss: 0.00003452
Iteration 310/1000 | Loss: 0.00003452
Iteration 311/1000 | Loss: 0.00003452
Iteration 312/1000 | Loss: 0.00003452
Iteration 313/1000 | Loss: 0.00003452
Iteration 314/1000 | Loss: 0.00003451
Iteration 315/1000 | Loss: 0.00003451
Iteration 316/1000 | Loss: 0.00003451
Iteration 317/1000 | Loss: 0.00003451
Iteration 318/1000 | Loss: 0.00003450
Iteration 319/1000 | Loss: 0.00094455
Iteration 320/1000 | Loss: 0.00088722
Iteration 321/1000 | Loss: 0.00013103
Iteration 322/1000 | Loss: 0.00004243
Iteration 323/1000 | Loss: 0.00003793
Iteration 324/1000 | Loss: 0.00003581
Iteration 325/1000 | Loss: 0.00003473
Iteration 326/1000 | Loss: 0.00005744
Iteration 327/1000 | Loss: 0.00004685
Iteration 328/1000 | Loss: 0.00003687
Iteration 329/1000 | Loss: 0.00003477
Iteration 330/1000 | Loss: 0.00003399
Iteration 331/1000 | Loss: 0.00003377
Iteration 332/1000 | Loss: 0.00003358
Iteration 333/1000 | Loss: 0.00003354
Iteration 334/1000 | Loss: 0.00003353
Iteration 335/1000 | Loss: 0.00003339
Iteration 336/1000 | Loss: 0.00003338
Iteration 337/1000 | Loss: 0.00003334
Iteration 338/1000 | Loss: 0.00003333
Iteration 339/1000 | Loss: 0.00003333
Iteration 340/1000 | Loss: 0.00003332
Iteration 341/1000 | Loss: 0.00003327
Iteration 342/1000 | Loss: 0.00003326
Iteration 343/1000 | Loss: 0.00003320
Iteration 344/1000 | Loss: 0.00003314
Iteration 345/1000 | Loss: 0.00003313
Iteration 346/1000 | Loss: 0.00003309
Iteration 347/1000 | Loss: 0.00003306
Iteration 348/1000 | Loss: 0.00003306
Iteration 349/1000 | Loss: 0.00003305
Iteration 350/1000 | Loss: 0.00003305
Iteration 351/1000 | Loss: 0.00003304
Iteration 352/1000 | Loss: 0.00003304
Iteration 353/1000 | Loss: 0.00003303
Iteration 354/1000 | Loss: 0.00003303
Iteration 355/1000 | Loss: 0.00003303
Iteration 356/1000 | Loss: 0.00003302
Iteration 357/1000 | Loss: 0.00003302
Iteration 358/1000 | Loss: 0.00003302
Iteration 359/1000 | Loss: 0.00003302
Iteration 360/1000 | Loss: 0.00003302
Iteration 361/1000 | Loss: 0.00003301
Iteration 362/1000 | Loss: 0.00003301
Iteration 363/1000 | Loss: 0.00003300
Iteration 364/1000 | Loss: 0.00003299
Iteration 365/1000 | Loss: 0.00003298
Iteration 366/1000 | Loss: 0.00003298
Iteration 367/1000 | Loss: 0.00003297
Iteration 368/1000 | Loss: 0.00003297
Iteration 369/1000 | Loss: 0.00003297
Iteration 370/1000 | Loss: 0.00003296
Iteration 371/1000 | Loss: 0.00003296
Iteration 372/1000 | Loss: 0.00003296
Iteration 373/1000 | Loss: 0.00003295
Iteration 374/1000 | Loss: 0.00003295
Iteration 375/1000 | Loss: 0.00003295
Iteration 376/1000 | Loss: 0.00003294
Iteration 377/1000 | Loss: 0.00003289
Iteration 378/1000 | Loss: 0.00003289
Iteration 379/1000 | Loss: 0.00003288
Iteration 380/1000 | Loss: 0.00003287
Iteration 381/1000 | Loss: 0.00003285
Iteration 382/1000 | Loss: 0.00003285
Iteration 383/1000 | Loss: 0.00003284
Iteration 384/1000 | Loss: 0.00003284
Iteration 385/1000 | Loss: 0.00003284
Iteration 386/1000 | Loss: 0.00003282
Iteration 387/1000 | Loss: 0.00003281
Iteration 388/1000 | Loss: 0.00003281
Iteration 389/1000 | Loss: 0.00003281
Iteration 390/1000 | Loss: 0.00003280
Iteration 391/1000 | Loss: 0.00003280
Iteration 392/1000 | Loss: 0.00003280
Iteration 393/1000 | Loss: 0.00003280
Iteration 394/1000 | Loss: 0.00003279
Iteration 395/1000 | Loss: 0.00003279
Iteration 396/1000 | Loss: 0.00003279
Iteration 397/1000 | Loss: 0.00003279
Iteration 398/1000 | Loss: 0.00003279
Iteration 399/1000 | Loss: 0.00003279
Iteration 400/1000 | Loss: 0.00003279
Iteration 401/1000 | Loss: 0.00003278
Iteration 402/1000 | Loss: 0.00003278
Iteration 403/1000 | Loss: 0.00003278
Iteration 404/1000 | Loss: 0.00003278
Iteration 405/1000 | Loss: 0.00003278
Iteration 406/1000 | Loss: 0.00003277
Iteration 407/1000 | Loss: 0.00003277
Iteration 408/1000 | Loss: 0.00003277
Iteration 409/1000 | Loss: 0.00003277
Iteration 410/1000 | Loss: 0.00003276
Iteration 411/1000 | Loss: 0.00003276
Iteration 412/1000 | Loss: 0.00003276
Iteration 413/1000 | Loss: 0.00003275
Iteration 414/1000 | Loss: 0.00003275
Iteration 415/1000 | Loss: 0.00003275
Iteration 416/1000 | Loss: 0.00003274
Iteration 417/1000 | Loss: 0.00003274
Iteration 418/1000 | Loss: 0.00003274
Iteration 419/1000 | Loss: 0.00003273
Iteration 420/1000 | Loss: 0.00003273
Iteration 421/1000 | Loss: 0.00003273
Iteration 422/1000 | Loss: 0.00003272
Iteration 423/1000 | Loss: 0.00003272
Iteration 424/1000 | Loss: 0.00003272
Iteration 425/1000 | Loss: 0.00003272
Iteration 426/1000 | Loss: 0.00003272
Iteration 427/1000 | Loss: 0.00003271
Iteration 428/1000 | Loss: 0.00003271
Iteration 429/1000 | Loss: 0.00003271
Iteration 430/1000 | Loss: 0.00003271
Iteration 431/1000 | Loss: 0.00003271
Iteration 432/1000 | Loss: 0.00003271
Iteration 433/1000 | Loss: 0.00003271
Iteration 434/1000 | Loss: 0.00003271
Iteration 435/1000 | Loss: 0.00003271
Iteration 436/1000 | Loss: 0.00003271
Iteration 437/1000 | Loss: 0.00003270
Iteration 438/1000 | Loss: 0.00003270
Iteration 439/1000 | Loss: 0.00003270
Iteration 440/1000 | Loss: 0.00003270
Iteration 441/1000 | Loss: 0.00003269
Iteration 442/1000 | Loss: 0.00003269
Iteration 443/1000 | Loss: 0.00003269
Iteration 444/1000 | Loss: 0.00003269
Iteration 445/1000 | Loss: 0.00003269
Iteration 446/1000 | Loss: 0.00003268
Iteration 447/1000 | Loss: 0.00003268
Iteration 448/1000 | Loss: 0.00003268
Iteration 449/1000 | Loss: 0.00003268
Iteration 450/1000 | Loss: 0.00003268
Iteration 451/1000 | Loss: 0.00003268
Iteration 452/1000 | Loss: 0.00003267
Iteration 453/1000 | Loss: 0.00003267
Iteration 454/1000 | Loss: 0.00003267
Iteration 455/1000 | Loss: 0.00003267
Iteration 456/1000 | Loss: 0.00003267
Iteration 457/1000 | Loss: 0.00003267
Iteration 458/1000 | Loss: 0.00003267
Iteration 459/1000 | Loss: 0.00003266
Iteration 460/1000 | Loss: 0.00003266
Iteration 461/1000 | Loss: 0.00003266
Iteration 462/1000 | Loss: 0.00003265
Iteration 463/1000 | Loss: 0.00003265
Iteration 464/1000 | Loss: 0.00003265
Iteration 465/1000 | Loss: 0.00003265
Iteration 466/1000 | Loss: 0.00003265
Iteration 467/1000 | Loss: 0.00003264
Iteration 468/1000 | Loss: 0.00003264
Iteration 469/1000 | Loss: 0.00003264
Iteration 470/1000 | Loss: 0.00003264
Iteration 471/1000 | Loss: 0.00003263
Iteration 472/1000 | Loss: 0.00003263
Iteration 473/1000 | Loss: 0.00003263
Iteration 474/1000 | Loss: 0.00003263
Iteration 475/1000 | Loss: 0.00003263
Iteration 476/1000 | Loss: 0.00003263
Iteration 477/1000 | Loss: 0.00003263
Iteration 478/1000 | Loss: 0.00003263
Iteration 479/1000 | Loss: 0.00003263
Iteration 480/1000 | Loss: 0.00003263
Iteration 481/1000 | Loss: 0.00003263
Iteration 482/1000 | Loss: 0.00003262
Iteration 483/1000 | Loss: 0.00003262
Iteration 484/1000 | Loss: 0.00003262
Iteration 485/1000 | Loss: 0.00003262
Iteration 486/1000 | Loss: 0.00003262
Iteration 487/1000 | Loss: 0.00003262
Iteration 488/1000 | Loss: 0.00003262
Iteration 489/1000 | Loss: 0.00003262
Iteration 490/1000 | Loss: 0.00003262
Iteration 491/1000 | Loss: 0.00003262
Iteration 492/1000 | Loss: 0.00003261
Iteration 493/1000 | Loss: 0.00003261
Iteration 494/1000 | Loss: 0.00003261
Iteration 495/1000 | Loss: 0.00003261
Iteration 496/1000 | Loss: 0.00003260
Iteration 497/1000 | Loss: 0.00003260
Iteration 498/1000 | Loss: 0.00003260
Iteration 499/1000 | Loss: 0.00003260
Iteration 500/1000 | Loss: 0.00003260
Iteration 501/1000 | Loss: 0.00003259
Iteration 502/1000 | Loss: 0.00003259
Iteration 503/1000 | Loss: 0.00003259
Iteration 504/1000 | Loss: 0.00003258
Iteration 505/1000 | Loss: 0.00003258
Iteration 506/1000 | Loss: 0.00003258
Iteration 507/1000 | Loss: 0.00003258
Iteration 508/1000 | Loss: 0.00003258
Iteration 509/1000 | Loss: 0.00003258
Iteration 510/1000 | Loss: 0.00003258
Iteration 511/1000 | Loss: 0.00003258
Iteration 512/1000 | Loss: 0.00003258
Iteration 513/1000 | Loss: 0.00003257
Iteration 514/1000 | Loss: 0.00003257
Iteration 515/1000 | Loss: 0.00003257
Iteration 516/1000 | Loss: 0.00003257
Iteration 517/1000 | Loss: 0.00003257
Iteration 518/1000 | Loss: 0.00003257
Iteration 519/1000 | Loss: 0.00003257
Iteration 520/1000 | Loss: 0.00003256
Iteration 521/1000 | Loss: 0.00003256
Iteration 522/1000 | Loss: 0.00003256
Iteration 523/1000 | Loss: 0.00003256
Iteration 524/1000 | Loss: 0.00003256
Iteration 525/1000 | Loss: 0.00003256
Iteration 526/1000 | Loss: 0.00003256
Iteration 527/1000 | Loss: 0.00003255
Iteration 528/1000 | Loss: 0.00003255
Iteration 529/1000 | Loss: 0.00003255
Iteration 530/1000 | Loss: 0.00003255
Iteration 531/1000 | Loss: 0.00003255
Iteration 532/1000 | Loss: 0.00003255
Iteration 533/1000 | Loss: 0.00003255
Iteration 534/1000 | Loss: 0.00003255
Iteration 535/1000 | Loss: 0.00003255
Iteration 536/1000 | Loss: 0.00003255
Iteration 537/1000 | Loss: 0.00003255
Iteration 538/1000 | Loss: 0.00003255
Iteration 539/1000 | Loss: 0.00003255
Iteration 540/1000 | Loss: 0.00003255
Iteration 541/1000 | Loss: 0.00003255
Iteration 542/1000 | Loss: 0.00003255
Iteration 543/1000 | Loss: 0.00003255
Iteration 544/1000 | Loss: 0.00003255
Iteration 545/1000 | Loss: 0.00003255
Iteration 546/1000 | Loss: 0.00003255
Iteration 547/1000 | Loss: 0.00003255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 547. Stopping optimization.
Last 5 losses: [3.2546595321036875e-05, 3.2546595321036875e-05, 3.2546595321036875e-05, 3.2546595321036875e-05, 3.2546595321036875e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2546595321036875e-05

Optimization complete. Final v2v error: 4.633786201477051 mm

Highest mean error: 6.599691867828369 mm for frame 115

Lowest mean error: 3.485555410385132 mm for frame 151

Saving results

Total time: 350.94910883903503
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795904
Iteration 2/25 | Loss: 0.00104271
Iteration 3/25 | Loss: 0.00077593
Iteration 4/25 | Loss: 0.00070497
Iteration 5/25 | Loss: 0.00069187
Iteration 6/25 | Loss: 0.00068990
Iteration 7/25 | Loss: 0.00068976
Iteration 8/25 | Loss: 0.00068976
Iteration 9/25 | Loss: 0.00068976
Iteration 10/25 | Loss: 0.00068976
Iteration 11/25 | Loss: 0.00068976
Iteration 12/25 | Loss: 0.00068976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000689757929649204, 0.000689757929649204, 0.000689757929649204, 0.000689757929649204, 0.000689757929649204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000689757929649204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44459069
Iteration 2/25 | Loss: 0.00029349
Iteration 3/25 | Loss: 0.00029346
Iteration 4/25 | Loss: 0.00029346
Iteration 5/25 | Loss: 0.00029346
Iteration 6/25 | Loss: 0.00029345
Iteration 7/25 | Loss: 0.00029345
Iteration 8/25 | Loss: 0.00029345
Iteration 9/25 | Loss: 0.00029345
Iteration 10/25 | Loss: 0.00029345
Iteration 11/25 | Loss: 0.00029345
Iteration 12/25 | Loss: 0.00029345
Iteration 13/25 | Loss: 0.00029345
Iteration 14/25 | Loss: 0.00029345
Iteration 15/25 | Loss: 0.00029345
Iteration 16/25 | Loss: 0.00029345
Iteration 17/25 | Loss: 0.00029345
Iteration 18/25 | Loss: 0.00029345
Iteration 19/25 | Loss: 0.00029345
Iteration 20/25 | Loss: 0.00029345
Iteration 21/25 | Loss: 0.00029345
Iteration 22/25 | Loss: 0.00029345
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0002934534568339586, 0.0002934534568339586, 0.0002934534568339586, 0.0002934534568339586, 0.0002934534568339586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002934534568339586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029345
Iteration 2/1000 | Loss: 0.00004478
Iteration 3/1000 | Loss: 0.00002936
Iteration 4/1000 | Loss: 0.00002668
Iteration 5/1000 | Loss: 0.00002502
Iteration 6/1000 | Loss: 0.00002395
Iteration 7/1000 | Loss: 0.00002319
Iteration 8/1000 | Loss: 0.00002255
Iteration 9/1000 | Loss: 0.00002227
Iteration 10/1000 | Loss: 0.00002205
Iteration 11/1000 | Loss: 0.00002184
Iteration 12/1000 | Loss: 0.00002174
Iteration 13/1000 | Loss: 0.00002163
Iteration 14/1000 | Loss: 0.00002162
Iteration 15/1000 | Loss: 0.00002161
Iteration 16/1000 | Loss: 0.00002158
Iteration 17/1000 | Loss: 0.00002154
Iteration 18/1000 | Loss: 0.00002154
Iteration 19/1000 | Loss: 0.00002153
Iteration 20/1000 | Loss: 0.00002152
Iteration 21/1000 | Loss: 0.00002152
Iteration 22/1000 | Loss: 0.00002151
Iteration 23/1000 | Loss: 0.00002150
Iteration 24/1000 | Loss: 0.00002144
Iteration 25/1000 | Loss: 0.00002137
Iteration 26/1000 | Loss: 0.00002134
Iteration 27/1000 | Loss: 0.00002134
Iteration 28/1000 | Loss: 0.00002134
Iteration 29/1000 | Loss: 0.00002134
Iteration 30/1000 | Loss: 0.00002134
Iteration 31/1000 | Loss: 0.00002134
Iteration 32/1000 | Loss: 0.00002134
Iteration 33/1000 | Loss: 0.00002134
Iteration 34/1000 | Loss: 0.00002134
Iteration 35/1000 | Loss: 0.00002133
Iteration 36/1000 | Loss: 0.00002133
Iteration 37/1000 | Loss: 0.00002133
Iteration 38/1000 | Loss: 0.00002133
Iteration 39/1000 | Loss: 0.00002133
Iteration 40/1000 | Loss: 0.00002132
Iteration 41/1000 | Loss: 0.00002132
Iteration 42/1000 | Loss: 0.00002132
Iteration 43/1000 | Loss: 0.00002131
Iteration 44/1000 | Loss: 0.00002131
Iteration 45/1000 | Loss: 0.00002131
Iteration 46/1000 | Loss: 0.00002131
Iteration 47/1000 | Loss: 0.00002130
Iteration 48/1000 | Loss: 0.00002130
Iteration 49/1000 | Loss: 0.00002130
Iteration 50/1000 | Loss: 0.00002130
Iteration 51/1000 | Loss: 0.00002129
Iteration 52/1000 | Loss: 0.00002129
Iteration 53/1000 | Loss: 0.00002129
Iteration 54/1000 | Loss: 0.00002129
Iteration 55/1000 | Loss: 0.00002129
Iteration 56/1000 | Loss: 0.00002129
Iteration 57/1000 | Loss: 0.00002129
Iteration 58/1000 | Loss: 0.00002128
Iteration 59/1000 | Loss: 0.00002128
Iteration 60/1000 | Loss: 0.00002128
Iteration 61/1000 | Loss: 0.00002128
Iteration 62/1000 | Loss: 0.00002128
Iteration 63/1000 | Loss: 0.00002128
Iteration 64/1000 | Loss: 0.00002127
Iteration 65/1000 | Loss: 0.00002127
Iteration 66/1000 | Loss: 0.00002127
Iteration 67/1000 | Loss: 0.00002127
Iteration 68/1000 | Loss: 0.00002127
Iteration 69/1000 | Loss: 0.00002127
Iteration 70/1000 | Loss: 0.00002127
Iteration 71/1000 | Loss: 0.00002126
Iteration 72/1000 | Loss: 0.00002126
Iteration 73/1000 | Loss: 0.00002126
Iteration 74/1000 | Loss: 0.00002126
Iteration 75/1000 | Loss: 0.00002126
Iteration 76/1000 | Loss: 0.00002126
Iteration 77/1000 | Loss: 0.00002126
Iteration 78/1000 | Loss: 0.00002126
Iteration 79/1000 | Loss: 0.00002126
Iteration 80/1000 | Loss: 0.00002126
Iteration 81/1000 | Loss: 0.00002125
Iteration 82/1000 | Loss: 0.00002125
Iteration 83/1000 | Loss: 0.00002125
Iteration 84/1000 | Loss: 0.00002124
Iteration 85/1000 | Loss: 0.00002124
Iteration 86/1000 | Loss: 0.00002124
Iteration 87/1000 | Loss: 0.00002124
Iteration 88/1000 | Loss: 0.00002124
Iteration 89/1000 | Loss: 0.00002123
Iteration 90/1000 | Loss: 0.00002123
Iteration 91/1000 | Loss: 0.00002123
Iteration 92/1000 | Loss: 0.00002123
Iteration 93/1000 | Loss: 0.00002123
Iteration 94/1000 | Loss: 0.00002123
Iteration 95/1000 | Loss: 0.00002123
Iteration 96/1000 | Loss: 0.00002122
Iteration 97/1000 | Loss: 0.00002122
Iteration 98/1000 | Loss: 0.00002122
Iteration 99/1000 | Loss: 0.00002122
Iteration 100/1000 | Loss: 0.00002122
Iteration 101/1000 | Loss: 0.00002122
Iteration 102/1000 | Loss: 0.00002122
Iteration 103/1000 | Loss: 0.00002122
Iteration 104/1000 | Loss: 0.00002122
Iteration 105/1000 | Loss: 0.00002122
Iteration 106/1000 | Loss: 0.00002122
Iteration 107/1000 | Loss: 0.00002122
Iteration 108/1000 | Loss: 0.00002122
Iteration 109/1000 | Loss: 0.00002122
Iteration 110/1000 | Loss: 0.00002122
Iteration 111/1000 | Loss: 0.00002122
Iteration 112/1000 | Loss: 0.00002122
Iteration 113/1000 | Loss: 0.00002122
Iteration 114/1000 | Loss: 0.00002122
Iteration 115/1000 | Loss: 0.00002122
Iteration 116/1000 | Loss: 0.00002122
Iteration 117/1000 | Loss: 0.00002122
Iteration 118/1000 | Loss: 0.00002122
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.1216801542323083e-05, 2.1216801542323083e-05, 2.1216801542323083e-05, 2.1216801542323083e-05, 2.1216801542323083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1216801542323083e-05

Optimization complete. Final v2v error: 3.956660747528076 mm

Highest mean error: 4.375400543212891 mm for frame 204

Lowest mean error: 3.5779643058776855 mm for frame 42

Saving results

Total time: 39.9819700717926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831600
Iteration 2/25 | Loss: 0.00152059
Iteration 3/25 | Loss: 0.00095681
Iteration 4/25 | Loss: 0.00085464
Iteration 5/25 | Loss: 0.00082506
Iteration 6/25 | Loss: 0.00081179
Iteration 7/25 | Loss: 0.00078013
Iteration 8/25 | Loss: 0.00076472
Iteration 9/25 | Loss: 0.00074801
Iteration 10/25 | Loss: 0.00074442
Iteration 11/25 | Loss: 0.00074352
Iteration 12/25 | Loss: 0.00074324
Iteration 13/25 | Loss: 0.00074310
Iteration 14/25 | Loss: 0.00074305
Iteration 15/25 | Loss: 0.00074305
Iteration 16/25 | Loss: 0.00074305
Iteration 17/25 | Loss: 0.00074305
Iteration 18/25 | Loss: 0.00074305
Iteration 19/25 | Loss: 0.00074304
Iteration 20/25 | Loss: 0.00074304
Iteration 21/25 | Loss: 0.00074304
Iteration 22/25 | Loss: 0.00074304
Iteration 23/25 | Loss: 0.00074304
Iteration 24/25 | Loss: 0.00074304
Iteration 25/25 | Loss: 0.00074304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66291952
Iteration 2/25 | Loss: 0.00031099
Iteration 3/25 | Loss: 0.00031097
Iteration 4/25 | Loss: 0.00031096
Iteration 5/25 | Loss: 0.00031096
Iteration 6/25 | Loss: 0.00031096
Iteration 7/25 | Loss: 0.00031096
Iteration 8/25 | Loss: 0.00031096
Iteration 9/25 | Loss: 0.00031096
Iteration 10/25 | Loss: 0.00031096
Iteration 11/25 | Loss: 0.00031096
Iteration 12/25 | Loss: 0.00031096
Iteration 13/25 | Loss: 0.00031096
Iteration 14/25 | Loss: 0.00031096
Iteration 15/25 | Loss: 0.00031096
Iteration 16/25 | Loss: 0.00031096
Iteration 17/25 | Loss: 0.00031096
Iteration 18/25 | Loss: 0.00031096
Iteration 19/25 | Loss: 0.00031096
Iteration 20/25 | Loss: 0.00031096
Iteration 21/25 | Loss: 0.00031096
Iteration 22/25 | Loss: 0.00031096
Iteration 23/25 | Loss: 0.00031096
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00031096237944439054, 0.00031096237944439054, 0.00031096237944439054, 0.00031096237944439054, 0.00031096237944439054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031096237944439054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031096
Iteration 2/1000 | Loss: 0.00003708
Iteration 3/1000 | Loss: 0.00002821
Iteration 4/1000 | Loss: 0.00002604
Iteration 5/1000 | Loss: 0.00002455
Iteration 6/1000 | Loss: 0.00002362
Iteration 7/1000 | Loss: 0.00002315
Iteration 8/1000 | Loss: 0.00002279
Iteration 9/1000 | Loss: 0.00002249
Iteration 10/1000 | Loss: 0.00002228
Iteration 11/1000 | Loss: 0.00002220
Iteration 12/1000 | Loss: 0.00002205
Iteration 13/1000 | Loss: 0.00002204
Iteration 14/1000 | Loss: 0.00002198
Iteration 15/1000 | Loss: 0.00002197
Iteration 16/1000 | Loss: 0.00002193
Iteration 17/1000 | Loss: 0.00002192
Iteration 18/1000 | Loss: 0.00002191
Iteration 19/1000 | Loss: 0.00002189
Iteration 20/1000 | Loss: 0.00002188
Iteration 21/1000 | Loss: 0.00002185
Iteration 22/1000 | Loss: 0.00002185
Iteration 23/1000 | Loss: 0.00002185
Iteration 24/1000 | Loss: 0.00002184
Iteration 25/1000 | Loss: 0.00002183
Iteration 26/1000 | Loss: 0.00002183
Iteration 27/1000 | Loss: 0.00002183
Iteration 28/1000 | Loss: 0.00002183
Iteration 29/1000 | Loss: 0.00002182
Iteration 30/1000 | Loss: 0.00002182
Iteration 31/1000 | Loss: 0.00002182
Iteration 32/1000 | Loss: 0.00002182
Iteration 33/1000 | Loss: 0.00002182
Iteration 34/1000 | Loss: 0.00002182
Iteration 35/1000 | Loss: 0.00002182
Iteration 36/1000 | Loss: 0.00002182
Iteration 37/1000 | Loss: 0.00002182
Iteration 38/1000 | Loss: 0.00002181
Iteration 39/1000 | Loss: 0.00002180
Iteration 40/1000 | Loss: 0.00002180
Iteration 41/1000 | Loss: 0.00002179
Iteration 42/1000 | Loss: 0.00002179
Iteration 43/1000 | Loss: 0.00002178
Iteration 44/1000 | Loss: 0.00002178
Iteration 45/1000 | Loss: 0.00002177
Iteration 46/1000 | Loss: 0.00002177
Iteration 47/1000 | Loss: 0.00002176
Iteration 48/1000 | Loss: 0.00002176
Iteration 49/1000 | Loss: 0.00002176
Iteration 50/1000 | Loss: 0.00002175
Iteration 51/1000 | Loss: 0.00002175
Iteration 52/1000 | Loss: 0.00002175
Iteration 53/1000 | Loss: 0.00002175
Iteration 54/1000 | Loss: 0.00002175
Iteration 55/1000 | Loss: 0.00002174
Iteration 56/1000 | Loss: 0.00002174
Iteration 57/1000 | Loss: 0.00002174
Iteration 58/1000 | Loss: 0.00002173
Iteration 59/1000 | Loss: 0.00002173
Iteration 60/1000 | Loss: 0.00002173
Iteration 61/1000 | Loss: 0.00002173
Iteration 62/1000 | Loss: 0.00002172
Iteration 63/1000 | Loss: 0.00002172
Iteration 64/1000 | Loss: 0.00002172
Iteration 65/1000 | Loss: 0.00002172
Iteration 66/1000 | Loss: 0.00002172
Iteration 67/1000 | Loss: 0.00002172
Iteration 68/1000 | Loss: 0.00002171
Iteration 69/1000 | Loss: 0.00002171
Iteration 70/1000 | Loss: 0.00002171
Iteration 71/1000 | Loss: 0.00002171
Iteration 72/1000 | Loss: 0.00002171
Iteration 73/1000 | Loss: 0.00002171
Iteration 74/1000 | Loss: 0.00002171
Iteration 75/1000 | Loss: 0.00002170
Iteration 76/1000 | Loss: 0.00002170
Iteration 77/1000 | Loss: 0.00002170
Iteration 78/1000 | Loss: 0.00002170
Iteration 79/1000 | Loss: 0.00002169
Iteration 80/1000 | Loss: 0.00002169
Iteration 81/1000 | Loss: 0.00002169
Iteration 82/1000 | Loss: 0.00002169
Iteration 83/1000 | Loss: 0.00002168
Iteration 84/1000 | Loss: 0.00002168
Iteration 85/1000 | Loss: 0.00002168
Iteration 86/1000 | Loss: 0.00002168
Iteration 87/1000 | Loss: 0.00002167
Iteration 88/1000 | Loss: 0.00002167
Iteration 89/1000 | Loss: 0.00002167
Iteration 90/1000 | Loss: 0.00002167
Iteration 91/1000 | Loss: 0.00002167
Iteration 92/1000 | Loss: 0.00002167
Iteration 93/1000 | Loss: 0.00002167
Iteration 94/1000 | Loss: 0.00002167
Iteration 95/1000 | Loss: 0.00002167
Iteration 96/1000 | Loss: 0.00002166
Iteration 97/1000 | Loss: 0.00002166
Iteration 98/1000 | Loss: 0.00002166
Iteration 99/1000 | Loss: 0.00002165
Iteration 100/1000 | Loss: 0.00002165
Iteration 101/1000 | Loss: 0.00002165
Iteration 102/1000 | Loss: 0.00002165
Iteration 103/1000 | Loss: 0.00002164
Iteration 104/1000 | Loss: 0.00002164
Iteration 105/1000 | Loss: 0.00002164
Iteration 106/1000 | Loss: 0.00002164
Iteration 107/1000 | Loss: 0.00002163
Iteration 108/1000 | Loss: 0.00002163
Iteration 109/1000 | Loss: 0.00002163
Iteration 110/1000 | Loss: 0.00002163
Iteration 111/1000 | Loss: 0.00002162
Iteration 112/1000 | Loss: 0.00002162
Iteration 113/1000 | Loss: 0.00002162
Iteration 114/1000 | Loss: 0.00002162
Iteration 115/1000 | Loss: 0.00002162
Iteration 116/1000 | Loss: 0.00002162
Iteration 117/1000 | Loss: 0.00002162
Iteration 118/1000 | Loss: 0.00002161
Iteration 119/1000 | Loss: 0.00002161
Iteration 120/1000 | Loss: 0.00002161
Iteration 121/1000 | Loss: 0.00002161
Iteration 122/1000 | Loss: 0.00002160
Iteration 123/1000 | Loss: 0.00002160
Iteration 124/1000 | Loss: 0.00002160
Iteration 125/1000 | Loss: 0.00002160
Iteration 126/1000 | Loss: 0.00002160
Iteration 127/1000 | Loss: 0.00002160
Iteration 128/1000 | Loss: 0.00002159
Iteration 129/1000 | Loss: 0.00002159
Iteration 130/1000 | Loss: 0.00002159
Iteration 131/1000 | Loss: 0.00002159
Iteration 132/1000 | Loss: 0.00002159
Iteration 133/1000 | Loss: 0.00002159
Iteration 134/1000 | Loss: 0.00002159
Iteration 135/1000 | Loss: 0.00002159
Iteration 136/1000 | Loss: 0.00002159
Iteration 137/1000 | Loss: 0.00002159
Iteration 138/1000 | Loss: 0.00002159
Iteration 139/1000 | Loss: 0.00002159
Iteration 140/1000 | Loss: 0.00002159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.1592726625385694e-05, 2.1592726625385694e-05, 2.1592726625385694e-05, 2.1592726625385694e-05, 2.1592726625385694e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1592726625385694e-05

Optimization complete. Final v2v error: 3.7589871883392334 mm

Highest mean error: 5.941408157348633 mm for frame 188

Lowest mean error: 2.9181840419769287 mm for frame 109

Saving results

Total time: 56.69220519065857
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065147
Iteration 2/25 | Loss: 0.01065147
Iteration 3/25 | Loss: 0.01065147
Iteration 4/25 | Loss: 0.01065146
Iteration 5/25 | Loss: 0.01065146
Iteration 6/25 | Loss: 0.01065146
Iteration 7/25 | Loss: 0.01065146
Iteration 8/25 | Loss: 0.01065146
Iteration 9/25 | Loss: 0.01065145
Iteration 10/25 | Loss: 0.01065145
Iteration 11/25 | Loss: 0.01065145
Iteration 12/25 | Loss: 0.01065145
Iteration 13/25 | Loss: 0.01065145
Iteration 14/25 | Loss: 0.01065144
Iteration 15/25 | Loss: 0.01065144
Iteration 16/25 | Loss: 0.01065144
Iteration 17/25 | Loss: 0.01065144
Iteration 18/25 | Loss: 0.01065144
Iteration 19/25 | Loss: 0.01065144
Iteration 20/25 | Loss: 0.01065143
Iteration 21/25 | Loss: 0.01065143
Iteration 22/25 | Loss: 0.01065143
Iteration 23/25 | Loss: 0.01065143
Iteration 24/25 | Loss: 0.01065143
Iteration 25/25 | Loss: 0.01065142

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75666499
Iteration 2/25 | Loss: 0.10403644
Iteration 3/25 | Loss: 0.10051622
Iteration 4/25 | Loss: 0.09887312
Iteration 5/25 | Loss: 0.09887310
Iteration 6/25 | Loss: 0.09887310
Iteration 7/25 | Loss: 0.09887310
Iteration 8/25 | Loss: 0.09887310
Iteration 9/25 | Loss: 0.09887310
Iteration 10/25 | Loss: 0.09887309
Iteration 11/25 | Loss: 0.09887309
Iteration 12/25 | Loss: 0.09887309
Iteration 13/25 | Loss: 0.09887309
Iteration 14/25 | Loss: 0.09887309
Iteration 15/25 | Loss: 0.09887309
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0988730937242508, 0.0988730937242508, 0.0988730937242508, 0.0988730937242508, 0.0988730937242508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0988730937242508

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09887309
Iteration 2/1000 | Loss: 0.00860013
Iteration 3/1000 | Loss: 0.00356289
Iteration 4/1000 | Loss: 0.00297715
Iteration 5/1000 | Loss: 0.00070978
Iteration 6/1000 | Loss: 0.00081832
Iteration 7/1000 | Loss: 0.00058036
Iteration 8/1000 | Loss: 0.00039217
Iteration 9/1000 | Loss: 0.00088092
Iteration 10/1000 | Loss: 0.00022149
Iteration 11/1000 | Loss: 0.00019224
Iteration 12/1000 | Loss: 0.00010411
Iteration 13/1000 | Loss: 0.00038618
Iteration 14/1000 | Loss: 0.00165784
Iteration 15/1000 | Loss: 0.00047281
Iteration 16/1000 | Loss: 0.00010171
Iteration 17/1000 | Loss: 0.00043505
Iteration 18/1000 | Loss: 0.00006812
Iteration 19/1000 | Loss: 0.00006534
Iteration 20/1000 | Loss: 0.00005455
Iteration 21/1000 | Loss: 0.00017010
Iteration 22/1000 | Loss: 0.00007159
Iteration 23/1000 | Loss: 0.00030469
Iteration 24/1000 | Loss: 0.00016199
Iteration 25/1000 | Loss: 0.00003942
Iteration 26/1000 | Loss: 0.00020323
Iteration 27/1000 | Loss: 0.00016679
Iteration 28/1000 | Loss: 0.00017593
Iteration 29/1000 | Loss: 0.00019566
Iteration 30/1000 | Loss: 0.00011761
Iteration 31/1000 | Loss: 0.00021257
Iteration 32/1000 | Loss: 0.00011439
Iteration 33/1000 | Loss: 0.00012904
Iteration 34/1000 | Loss: 0.00011314
Iteration 35/1000 | Loss: 0.00010775
Iteration 36/1000 | Loss: 0.00010973
Iteration 37/1000 | Loss: 0.00008769
Iteration 38/1000 | Loss: 0.00009785
Iteration 39/1000 | Loss: 0.00011521
Iteration 40/1000 | Loss: 0.00009178
Iteration 41/1000 | Loss: 0.00007260
Iteration 42/1000 | Loss: 0.00007805
Iteration 43/1000 | Loss: 0.00007715
Iteration 44/1000 | Loss: 0.00006531
Iteration 45/1000 | Loss: 0.00004253
Iteration 46/1000 | Loss: 0.00003578
Iteration 47/1000 | Loss: 0.00003204
Iteration 48/1000 | Loss: 0.00002994
Iteration 49/1000 | Loss: 0.00002923
Iteration 50/1000 | Loss: 0.00002879
Iteration 51/1000 | Loss: 0.00002843
Iteration 52/1000 | Loss: 0.00006075
Iteration 53/1000 | Loss: 0.00013723
Iteration 54/1000 | Loss: 0.00003600
Iteration 55/1000 | Loss: 0.00002783
Iteration 56/1000 | Loss: 0.00002777
Iteration 57/1000 | Loss: 0.00002753
Iteration 58/1000 | Loss: 0.00010048
Iteration 59/1000 | Loss: 0.00002731
Iteration 60/1000 | Loss: 0.00004948
Iteration 61/1000 | Loss: 0.00002711
Iteration 62/1000 | Loss: 0.00002710
Iteration 63/1000 | Loss: 0.00006342
Iteration 64/1000 | Loss: 0.00004567
Iteration 65/1000 | Loss: 0.00002696
Iteration 66/1000 | Loss: 0.00006986
Iteration 67/1000 | Loss: 0.00010905
Iteration 68/1000 | Loss: 0.00027489
Iteration 69/1000 | Loss: 0.00003725
Iteration 70/1000 | Loss: 0.00006576
Iteration 71/1000 | Loss: 0.00003074
Iteration 72/1000 | Loss: 0.00024844
Iteration 73/1000 | Loss: 0.00009414
Iteration 74/1000 | Loss: 0.00003278
Iteration 75/1000 | Loss: 0.00002735
Iteration 76/1000 | Loss: 0.00003513
Iteration 77/1000 | Loss: 0.00002678
Iteration 78/1000 | Loss: 0.00006028
Iteration 79/1000 | Loss: 0.00003288
Iteration 80/1000 | Loss: 0.00003505
Iteration 81/1000 | Loss: 0.00002860
Iteration 82/1000 | Loss: 0.00002674
Iteration 83/1000 | Loss: 0.00002673
Iteration 84/1000 | Loss: 0.00002673
Iteration 85/1000 | Loss: 0.00002673
Iteration 86/1000 | Loss: 0.00002672
Iteration 87/1000 | Loss: 0.00002672
Iteration 88/1000 | Loss: 0.00002671
Iteration 89/1000 | Loss: 0.00002670
Iteration 90/1000 | Loss: 0.00002670
Iteration 91/1000 | Loss: 0.00002669
Iteration 92/1000 | Loss: 0.00002852
Iteration 93/1000 | Loss: 0.00002906
Iteration 94/1000 | Loss: 0.00002674
Iteration 95/1000 | Loss: 0.00002675
Iteration 96/1000 | Loss: 0.00002664
Iteration 97/1000 | Loss: 0.00002664
Iteration 98/1000 | Loss: 0.00002664
Iteration 99/1000 | Loss: 0.00002664
Iteration 100/1000 | Loss: 0.00002664
Iteration 101/1000 | Loss: 0.00002664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.66384486167226e-05, 2.66384486167226e-05, 2.66384486167226e-05, 2.66384486167226e-05, 2.66384486167226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.66384486167226e-05

Optimization complete. Final v2v error: 4.171364784240723 mm

Highest mean error: 17.42446517944336 mm for frame 194

Lowest mean error: 3.7045984268188477 mm for frame 129

Saving results

Total time: 134.41394066810608
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00953881
Iteration 2/25 | Loss: 0.00152561
Iteration 3/25 | Loss: 0.00094061
Iteration 4/25 | Loss: 0.00087634
Iteration 5/25 | Loss: 0.00087856
Iteration 6/25 | Loss: 0.00084429
Iteration 7/25 | Loss: 0.00082328
Iteration 8/25 | Loss: 0.00081508
Iteration 9/25 | Loss: 0.00080624
Iteration 10/25 | Loss: 0.00080723
Iteration 11/25 | Loss: 0.00080611
Iteration 12/25 | Loss: 0.00080507
Iteration 13/25 | Loss: 0.00080761
Iteration 14/25 | Loss: 0.00080144
Iteration 15/25 | Loss: 0.00079706
Iteration 16/25 | Loss: 0.00079938
Iteration 17/25 | Loss: 0.00079759
Iteration 18/25 | Loss: 0.00079320
Iteration 19/25 | Loss: 0.00079160
Iteration 20/25 | Loss: 0.00079152
Iteration 21/25 | Loss: 0.00079187
Iteration 22/25 | Loss: 0.00079086
Iteration 23/25 | Loss: 0.00078841
Iteration 24/25 | Loss: 0.00079147
Iteration 25/25 | Loss: 0.00078842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86701727
Iteration 2/25 | Loss: 0.00075544
Iteration 3/25 | Loss: 0.00075543
Iteration 4/25 | Loss: 0.00075543
Iteration 5/25 | Loss: 0.00075543
Iteration 6/25 | Loss: 0.00075543
Iteration 7/25 | Loss: 0.00075543
Iteration 8/25 | Loss: 0.00075543
Iteration 9/25 | Loss: 0.00075543
Iteration 10/25 | Loss: 0.00075543
Iteration 11/25 | Loss: 0.00075543
Iteration 12/25 | Loss: 0.00075543
Iteration 13/25 | Loss: 0.00075543
Iteration 14/25 | Loss: 0.00075543
Iteration 15/25 | Loss: 0.00075543
Iteration 16/25 | Loss: 0.00075543
Iteration 17/25 | Loss: 0.00075543
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007554286275990307, 0.0007554286275990307, 0.0007554286275990307, 0.0007554286275990307, 0.0007554286275990307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007554286275990307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075543
Iteration 2/1000 | Loss: 0.00023715
Iteration 3/1000 | Loss: 0.00030160
Iteration 4/1000 | Loss: 0.00030591
Iteration 5/1000 | Loss: 0.00025822
Iteration 6/1000 | Loss: 0.00024819
Iteration 7/1000 | Loss: 0.00020171
Iteration 8/1000 | Loss: 0.00019399
Iteration 9/1000 | Loss: 0.00019595
Iteration 10/1000 | Loss: 0.00157937
Iteration 11/1000 | Loss: 0.00025420
Iteration 12/1000 | Loss: 0.00017731
Iteration 13/1000 | Loss: 0.00122103
Iteration 14/1000 | Loss: 0.00118605
Iteration 15/1000 | Loss: 0.00037207
Iteration 16/1000 | Loss: 0.00272991
Iteration 17/1000 | Loss: 0.00057603
Iteration 18/1000 | Loss: 0.00028754
Iteration 19/1000 | Loss: 0.00015494
Iteration 20/1000 | Loss: 0.00016908
Iteration 21/1000 | Loss: 0.00123183
Iteration 22/1000 | Loss: 0.00050737
Iteration 23/1000 | Loss: 0.00281242
Iteration 24/1000 | Loss: 0.00337181
Iteration 25/1000 | Loss: 0.00152332
Iteration 26/1000 | Loss: 0.00172028
Iteration 27/1000 | Loss: 0.00097386
Iteration 28/1000 | Loss: 0.00074958
Iteration 29/1000 | Loss: 0.00026000
Iteration 30/1000 | Loss: 0.00020668
Iteration 31/1000 | Loss: 0.00016537
Iteration 32/1000 | Loss: 0.00009584
Iteration 33/1000 | Loss: 0.00010858
Iteration 34/1000 | Loss: 0.00011200
Iteration 35/1000 | Loss: 0.00127586
Iteration 36/1000 | Loss: 0.00053233
Iteration 37/1000 | Loss: 0.00092165
Iteration 38/1000 | Loss: 0.00059268
Iteration 39/1000 | Loss: 0.00108975
Iteration 40/1000 | Loss: 0.00032132
Iteration 41/1000 | Loss: 0.00014342
Iteration 42/1000 | Loss: 0.00015418
Iteration 43/1000 | Loss: 0.00005795
Iteration 44/1000 | Loss: 0.00027128
Iteration 45/1000 | Loss: 0.00005524
Iteration 46/1000 | Loss: 0.00068939
Iteration 47/1000 | Loss: 0.00112864
Iteration 48/1000 | Loss: 0.00045143
Iteration 49/1000 | Loss: 0.00086412
Iteration 50/1000 | Loss: 0.00030276
Iteration 51/1000 | Loss: 0.00011554
Iteration 52/1000 | Loss: 0.00012994
Iteration 53/1000 | Loss: 0.00004154
Iteration 54/1000 | Loss: 0.00008012
Iteration 55/1000 | Loss: 0.00009320
Iteration 56/1000 | Loss: 0.00003739
Iteration 57/1000 | Loss: 0.00015518
Iteration 58/1000 | Loss: 0.00008949
Iteration 59/1000 | Loss: 0.00004143
Iteration 60/1000 | Loss: 0.00010321
Iteration 61/1000 | Loss: 0.00029439
Iteration 62/1000 | Loss: 0.00014625
Iteration 63/1000 | Loss: 0.00006745
Iteration 64/1000 | Loss: 0.00003520
Iteration 65/1000 | Loss: 0.00003300
Iteration 66/1000 | Loss: 0.00081401
Iteration 67/1000 | Loss: 0.00021133
Iteration 68/1000 | Loss: 0.00018103
Iteration 69/1000 | Loss: 0.00014573
Iteration 70/1000 | Loss: 0.00011078
Iteration 71/1000 | Loss: 0.00012043
Iteration 72/1000 | Loss: 0.00013008
Iteration 73/1000 | Loss: 0.00014589
Iteration 74/1000 | Loss: 0.00005063
Iteration 75/1000 | Loss: 0.00004074
Iteration 76/1000 | Loss: 0.00005687
Iteration 77/1000 | Loss: 0.00017865
Iteration 78/1000 | Loss: 0.00015817
Iteration 79/1000 | Loss: 0.00005029
Iteration 80/1000 | Loss: 0.00003606
Iteration 81/1000 | Loss: 0.00003477
Iteration 82/1000 | Loss: 0.00003380
Iteration 83/1000 | Loss: 0.00007665
Iteration 84/1000 | Loss: 0.00006907
Iteration 85/1000 | Loss: 0.00007349
Iteration 86/1000 | Loss: 0.00006354
Iteration 87/1000 | Loss: 0.00009952
Iteration 88/1000 | Loss: 0.00006154
Iteration 89/1000 | Loss: 0.00018588
Iteration 90/1000 | Loss: 0.00013179
Iteration 91/1000 | Loss: 0.00004574
Iteration 92/1000 | Loss: 0.00012773
Iteration 93/1000 | Loss: 0.00003445
Iteration 94/1000 | Loss: 0.00015999
Iteration 95/1000 | Loss: 0.00015588
Iteration 96/1000 | Loss: 0.00013222
Iteration 97/1000 | Loss: 0.00005023
Iteration 98/1000 | Loss: 0.00004237
Iteration 99/1000 | Loss: 0.00002962
Iteration 100/1000 | Loss: 0.00002911
Iteration 101/1000 | Loss: 0.00003445
Iteration 102/1000 | Loss: 0.00003453
Iteration 103/1000 | Loss: 0.00003408
Iteration 104/1000 | Loss: 0.00003212
Iteration 105/1000 | Loss: 0.00003274
Iteration 106/1000 | Loss: 0.00003031
Iteration 107/1000 | Loss: 0.00003098
Iteration 108/1000 | Loss: 0.00002772
Iteration 109/1000 | Loss: 0.00002704
Iteration 110/1000 | Loss: 0.00002676
Iteration 111/1000 | Loss: 0.00002653
Iteration 112/1000 | Loss: 0.00002647
Iteration 113/1000 | Loss: 0.00002647
Iteration 114/1000 | Loss: 0.00002645
Iteration 115/1000 | Loss: 0.00002644
Iteration 116/1000 | Loss: 0.00003061
Iteration 117/1000 | Loss: 0.00002775
Iteration 118/1000 | Loss: 0.00003056
Iteration 119/1000 | Loss: 0.00002794
Iteration 120/1000 | Loss: 0.00002993
Iteration 121/1000 | Loss: 0.00002742
Iteration 122/1000 | Loss: 0.00002695
Iteration 123/1000 | Loss: 0.00002618
Iteration 124/1000 | Loss: 0.00002553
Iteration 125/1000 | Loss: 0.00002526
Iteration 126/1000 | Loss: 0.00002511
Iteration 127/1000 | Loss: 0.00002508
Iteration 128/1000 | Loss: 0.00002506
Iteration 129/1000 | Loss: 0.00002506
Iteration 130/1000 | Loss: 0.00002503
Iteration 131/1000 | Loss: 0.00002500
Iteration 132/1000 | Loss: 0.00002499
Iteration 133/1000 | Loss: 0.00002498
Iteration 134/1000 | Loss: 0.00002498
Iteration 135/1000 | Loss: 0.00002498
Iteration 136/1000 | Loss: 0.00002498
Iteration 137/1000 | Loss: 0.00002497
Iteration 138/1000 | Loss: 0.00002496
Iteration 139/1000 | Loss: 0.00002496
Iteration 140/1000 | Loss: 0.00002496
Iteration 141/1000 | Loss: 0.00002496
Iteration 142/1000 | Loss: 0.00002495
Iteration 143/1000 | Loss: 0.00002495
Iteration 144/1000 | Loss: 0.00002495
Iteration 145/1000 | Loss: 0.00002495
Iteration 146/1000 | Loss: 0.00002495
Iteration 147/1000 | Loss: 0.00002495
Iteration 148/1000 | Loss: 0.00002494
Iteration 149/1000 | Loss: 0.00002494
Iteration 150/1000 | Loss: 0.00002494
Iteration 151/1000 | Loss: 0.00002493
Iteration 152/1000 | Loss: 0.00002493
Iteration 153/1000 | Loss: 0.00002493
Iteration 154/1000 | Loss: 0.00002493
Iteration 155/1000 | Loss: 0.00002492
Iteration 156/1000 | Loss: 0.00002492
Iteration 157/1000 | Loss: 0.00002492
Iteration 158/1000 | Loss: 0.00002492
Iteration 159/1000 | Loss: 0.00002492
Iteration 160/1000 | Loss: 0.00002492
Iteration 161/1000 | Loss: 0.00002492
Iteration 162/1000 | Loss: 0.00002492
Iteration 163/1000 | Loss: 0.00002492
Iteration 164/1000 | Loss: 0.00002492
Iteration 165/1000 | Loss: 0.00002492
Iteration 166/1000 | Loss: 0.00002492
Iteration 167/1000 | Loss: 0.00002492
Iteration 168/1000 | Loss: 0.00002492
Iteration 169/1000 | Loss: 0.00002492
Iteration 170/1000 | Loss: 0.00002492
Iteration 171/1000 | Loss: 0.00002492
Iteration 172/1000 | Loss: 0.00002492
Iteration 173/1000 | Loss: 0.00002492
Iteration 174/1000 | Loss: 0.00002492
Iteration 175/1000 | Loss: 0.00002492
Iteration 176/1000 | Loss: 0.00002491
Iteration 177/1000 | Loss: 0.00002491
Iteration 178/1000 | Loss: 0.00002491
Iteration 179/1000 | Loss: 0.00002491
Iteration 180/1000 | Loss: 0.00002491
Iteration 181/1000 | Loss: 0.00002491
Iteration 182/1000 | Loss: 0.00002491
Iteration 183/1000 | Loss: 0.00002491
Iteration 184/1000 | Loss: 0.00002491
Iteration 185/1000 | Loss: 0.00002491
Iteration 186/1000 | Loss: 0.00002491
Iteration 187/1000 | Loss: 0.00002491
Iteration 188/1000 | Loss: 0.00002491
Iteration 189/1000 | Loss: 0.00002491
Iteration 190/1000 | Loss: 0.00002491
Iteration 191/1000 | Loss: 0.00002491
Iteration 192/1000 | Loss: 0.00002491
Iteration 193/1000 | Loss: 0.00002491
Iteration 194/1000 | Loss: 0.00002491
Iteration 195/1000 | Loss: 0.00002491
Iteration 196/1000 | Loss: 0.00002491
Iteration 197/1000 | Loss: 0.00002491
Iteration 198/1000 | Loss: 0.00002491
Iteration 199/1000 | Loss: 0.00002491
Iteration 200/1000 | Loss: 0.00002491
Iteration 201/1000 | Loss: 0.00002491
Iteration 202/1000 | Loss: 0.00002491
Iteration 203/1000 | Loss: 0.00002491
Iteration 204/1000 | Loss: 0.00002491
Iteration 205/1000 | Loss: 0.00002491
Iteration 206/1000 | Loss: 0.00002491
Iteration 207/1000 | Loss: 0.00002491
Iteration 208/1000 | Loss: 0.00002491
Iteration 209/1000 | Loss: 0.00002491
Iteration 210/1000 | Loss: 0.00002491
Iteration 211/1000 | Loss: 0.00002491
Iteration 212/1000 | Loss: 0.00002491
Iteration 213/1000 | Loss: 0.00002491
Iteration 214/1000 | Loss: 0.00002491
Iteration 215/1000 | Loss: 0.00002491
Iteration 216/1000 | Loss: 0.00002491
Iteration 217/1000 | Loss: 0.00002491
Iteration 218/1000 | Loss: 0.00002491
Iteration 219/1000 | Loss: 0.00002491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [2.490816950739827e-05, 2.490816950739827e-05, 2.490816950739827e-05, 2.490816950739827e-05, 2.490816950739827e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.490816950739827e-05

Optimization complete. Final v2v error: 4.206791400909424 mm

Highest mean error: 5.773434162139893 mm for frame 139

Lowest mean error: 3.5609705448150635 mm for frame 187

Saving results

Total time: 251.70538878440857
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066203
Iteration 2/25 | Loss: 0.00221256
Iteration 3/25 | Loss: 0.00184028
Iteration 4/25 | Loss: 0.00168709
Iteration 5/25 | Loss: 0.00147667
Iteration 6/25 | Loss: 0.00153079
Iteration 7/25 | Loss: 0.00112290
Iteration 8/25 | Loss: 0.00098443
Iteration 9/25 | Loss: 0.00097468
Iteration 10/25 | Loss: 0.00084694
Iteration 11/25 | Loss: 0.00077771
Iteration 12/25 | Loss: 0.00076774
Iteration 13/25 | Loss: 0.00076164
Iteration 14/25 | Loss: 0.00075690
Iteration 15/25 | Loss: 0.00075603
Iteration 16/25 | Loss: 0.00075589
Iteration 17/25 | Loss: 0.00075584
Iteration 18/25 | Loss: 0.00075584
Iteration 19/25 | Loss: 0.00075584
Iteration 20/25 | Loss: 0.00075584
Iteration 21/25 | Loss: 0.00075584
Iteration 22/25 | Loss: 0.00075584
Iteration 23/25 | Loss: 0.00075583
Iteration 24/25 | Loss: 0.00075583
Iteration 25/25 | Loss: 0.00075583

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50907469
Iteration 2/25 | Loss: 0.00023787
Iteration 3/25 | Loss: 0.00023786
Iteration 4/25 | Loss: 0.00023786
Iteration 5/25 | Loss: 0.00023786
Iteration 6/25 | Loss: 0.00023786
Iteration 7/25 | Loss: 0.00023786
Iteration 8/25 | Loss: 0.00023786
Iteration 9/25 | Loss: 0.00023786
Iteration 10/25 | Loss: 0.00023786
Iteration 11/25 | Loss: 0.00023786
Iteration 12/25 | Loss: 0.00023786
Iteration 13/25 | Loss: 0.00023786
Iteration 14/25 | Loss: 0.00023786
Iteration 15/25 | Loss: 0.00023786
Iteration 16/25 | Loss: 0.00023786
Iteration 17/25 | Loss: 0.00023786
Iteration 18/25 | Loss: 0.00023786
Iteration 19/25 | Loss: 0.00023786
Iteration 20/25 | Loss: 0.00023786
Iteration 21/25 | Loss: 0.00023786
Iteration 22/25 | Loss: 0.00023786
Iteration 23/25 | Loss: 0.00023786
Iteration 24/25 | Loss: 0.00023786
Iteration 25/25 | Loss: 0.00023786
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00023785523080732673, 0.00023785523080732673, 0.00023785523080732673, 0.00023785523080732673, 0.00023785523080732673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023785523080732673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023786
Iteration 2/1000 | Loss: 0.00003177
Iteration 3/1000 | Loss: 0.00002501
Iteration 4/1000 | Loss: 0.00002236
Iteration 5/1000 | Loss: 0.00002121
Iteration 6/1000 | Loss: 0.00002046
Iteration 7/1000 | Loss: 0.00002001
Iteration 8/1000 | Loss: 0.00001967
Iteration 9/1000 | Loss: 0.00001946
Iteration 10/1000 | Loss: 0.00001934
Iteration 11/1000 | Loss: 0.00001933
Iteration 12/1000 | Loss: 0.00001932
Iteration 13/1000 | Loss: 0.00001932
Iteration 14/1000 | Loss: 0.00001931
Iteration 15/1000 | Loss: 0.00001931
Iteration 16/1000 | Loss: 0.00001930
Iteration 17/1000 | Loss: 0.00001930
Iteration 18/1000 | Loss: 0.00001929
Iteration 19/1000 | Loss: 0.00001928
Iteration 20/1000 | Loss: 0.00001928
Iteration 21/1000 | Loss: 0.00001928
Iteration 22/1000 | Loss: 0.00001928
Iteration 23/1000 | Loss: 0.00001927
Iteration 24/1000 | Loss: 0.00001927
Iteration 25/1000 | Loss: 0.00001927
Iteration 26/1000 | Loss: 0.00001926
Iteration 27/1000 | Loss: 0.00001926
Iteration 28/1000 | Loss: 0.00001926
Iteration 29/1000 | Loss: 0.00001925
Iteration 30/1000 | Loss: 0.00001925
Iteration 31/1000 | Loss: 0.00001925
Iteration 32/1000 | Loss: 0.00001925
Iteration 33/1000 | Loss: 0.00001924
Iteration 34/1000 | Loss: 0.00001924
Iteration 35/1000 | Loss: 0.00001924
Iteration 36/1000 | Loss: 0.00001924
Iteration 37/1000 | Loss: 0.00001923
Iteration 38/1000 | Loss: 0.00001923
Iteration 39/1000 | Loss: 0.00001923
Iteration 40/1000 | Loss: 0.00001922
Iteration 41/1000 | Loss: 0.00001922
Iteration 42/1000 | Loss: 0.00001922
Iteration 43/1000 | Loss: 0.00001922
Iteration 44/1000 | Loss: 0.00001921
Iteration 45/1000 | Loss: 0.00001921
Iteration 46/1000 | Loss: 0.00001921
Iteration 47/1000 | Loss: 0.00001920
Iteration 48/1000 | Loss: 0.00001920
Iteration 49/1000 | Loss: 0.00001919
Iteration 50/1000 | Loss: 0.00001919
Iteration 51/1000 | Loss: 0.00001919
Iteration 52/1000 | Loss: 0.00001918
Iteration 53/1000 | Loss: 0.00001918
Iteration 54/1000 | Loss: 0.00001918
Iteration 55/1000 | Loss: 0.00001918
Iteration 56/1000 | Loss: 0.00001918
Iteration 57/1000 | Loss: 0.00001918
Iteration 58/1000 | Loss: 0.00001917
Iteration 59/1000 | Loss: 0.00001917
Iteration 60/1000 | Loss: 0.00001917
Iteration 61/1000 | Loss: 0.00001916
Iteration 62/1000 | Loss: 0.00001916
Iteration 63/1000 | Loss: 0.00001915
Iteration 64/1000 | Loss: 0.00001915
Iteration 65/1000 | Loss: 0.00001915
Iteration 66/1000 | Loss: 0.00001915
Iteration 67/1000 | Loss: 0.00001915
Iteration 68/1000 | Loss: 0.00001915
Iteration 69/1000 | Loss: 0.00001914
Iteration 70/1000 | Loss: 0.00001914
Iteration 71/1000 | Loss: 0.00001914
Iteration 72/1000 | Loss: 0.00001913
Iteration 73/1000 | Loss: 0.00001913
Iteration 74/1000 | Loss: 0.00001913
Iteration 75/1000 | Loss: 0.00001913
Iteration 76/1000 | Loss: 0.00001912
Iteration 77/1000 | Loss: 0.00001912
Iteration 78/1000 | Loss: 0.00001912
Iteration 79/1000 | Loss: 0.00001912
Iteration 80/1000 | Loss: 0.00001912
Iteration 81/1000 | Loss: 0.00001912
Iteration 82/1000 | Loss: 0.00001912
Iteration 83/1000 | Loss: 0.00001912
Iteration 84/1000 | Loss: 0.00001912
Iteration 85/1000 | Loss: 0.00001912
Iteration 86/1000 | Loss: 0.00001912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.9121351215289906e-05, 1.9121351215289906e-05, 1.9121351215289906e-05, 1.9121351215289906e-05, 1.9121351215289906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9121351215289906e-05

Optimization complete. Final v2v error: 3.719512701034546 mm

Highest mean error: 3.9164936542510986 mm for frame 176

Lowest mean error: 3.5402815341949463 mm for frame 167

Saving results

Total time: 48.749292612075806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930050
Iteration 2/25 | Loss: 0.00324971
Iteration 3/25 | Loss: 0.00199636
Iteration 4/25 | Loss: 0.00147553
Iteration 5/25 | Loss: 0.00140833
Iteration 6/25 | Loss: 0.00122994
Iteration 7/25 | Loss: 0.00117645
Iteration 8/25 | Loss: 0.00111210
Iteration 9/25 | Loss: 0.00108047
Iteration 10/25 | Loss: 0.00101148
Iteration 11/25 | Loss: 0.00099488
Iteration 12/25 | Loss: 0.00096146
Iteration 13/25 | Loss: 0.00094850
Iteration 14/25 | Loss: 0.00093924
Iteration 15/25 | Loss: 0.00094394
Iteration 16/25 | Loss: 0.00093967
Iteration 17/25 | Loss: 0.00093658
Iteration 18/25 | Loss: 0.00093200
Iteration 19/25 | Loss: 0.00093025
Iteration 20/25 | Loss: 0.00092906
Iteration 21/25 | Loss: 0.00092845
Iteration 22/25 | Loss: 0.00092813
Iteration 23/25 | Loss: 0.00093045
Iteration 24/25 | Loss: 0.00092933
Iteration 25/25 | Loss: 0.00092873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.66384315
Iteration 2/25 | Loss: 0.00399673
Iteration 3/25 | Loss: 0.00294702
Iteration 4/25 | Loss: 0.00294702
Iteration 5/25 | Loss: 0.00294702
Iteration 6/25 | Loss: 0.00294702
Iteration 7/25 | Loss: 0.00294702
Iteration 8/25 | Loss: 0.00294702
Iteration 9/25 | Loss: 0.00294702
Iteration 10/25 | Loss: 0.00294702
Iteration 11/25 | Loss: 0.00294702
Iteration 12/25 | Loss: 0.00294702
Iteration 13/25 | Loss: 0.00294702
Iteration 14/25 | Loss: 0.00294702
Iteration 15/25 | Loss: 0.00294702
Iteration 16/25 | Loss: 0.00294702
Iteration 17/25 | Loss: 0.00294702
Iteration 18/25 | Loss: 0.00294702
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002947020810097456, 0.002947020810097456, 0.002947020810097456, 0.002947020810097456, 0.002947020810097456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002947020810097456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00294702
Iteration 2/1000 | Loss: 0.00058629
Iteration 3/1000 | Loss: 0.00381160
Iteration 4/1000 | Loss: 0.00454451
Iteration 5/1000 | Loss: 0.00287530
Iteration 6/1000 | Loss: 0.00230546
Iteration 7/1000 | Loss: 0.00362866
Iteration 8/1000 | Loss: 0.00036924
Iteration 9/1000 | Loss: 0.00101220
Iteration 10/1000 | Loss: 0.00224232
Iteration 11/1000 | Loss: 0.00021858
Iteration 12/1000 | Loss: 0.00033765
Iteration 13/1000 | Loss: 0.00491776
Iteration 14/1000 | Loss: 0.00237163
Iteration 15/1000 | Loss: 0.00411144
Iteration 16/1000 | Loss: 0.00116181
Iteration 17/1000 | Loss: 0.00085960
Iteration 18/1000 | Loss: 0.00112830
Iteration 19/1000 | Loss: 0.00054299
Iteration 20/1000 | Loss: 0.00090104
Iteration 21/1000 | Loss: 0.00021078
Iteration 22/1000 | Loss: 0.00400405
Iteration 23/1000 | Loss: 0.00043272
Iteration 24/1000 | Loss: 0.00588734
Iteration 25/1000 | Loss: 0.00217276
Iteration 26/1000 | Loss: 0.00077989
Iteration 27/1000 | Loss: 0.00154516
Iteration 28/1000 | Loss: 0.00058488
Iteration 29/1000 | Loss: 0.00082236
Iteration 30/1000 | Loss: 0.00119219
Iteration 31/1000 | Loss: 0.00061789
Iteration 32/1000 | Loss: 0.00025501
Iteration 33/1000 | Loss: 0.00011776
Iteration 34/1000 | Loss: 0.00011522
Iteration 35/1000 | Loss: 0.00138163
Iteration 36/1000 | Loss: 0.00191924
Iteration 37/1000 | Loss: 0.00042488
Iteration 38/1000 | Loss: 0.00054002
Iteration 39/1000 | Loss: 0.00214174
Iteration 40/1000 | Loss: 0.00075352
Iteration 41/1000 | Loss: 0.00107155
Iteration 42/1000 | Loss: 0.00070481
Iteration 43/1000 | Loss: 0.00056497
Iteration 44/1000 | Loss: 0.00032797
Iteration 45/1000 | Loss: 0.00097357
Iteration 46/1000 | Loss: 0.00289982
Iteration 47/1000 | Loss: 0.00184323
Iteration 48/1000 | Loss: 0.00047060
Iteration 49/1000 | Loss: 0.00208336
Iteration 50/1000 | Loss: 0.00043276
Iteration 51/1000 | Loss: 0.00130908
Iteration 52/1000 | Loss: 0.00116646
Iteration 53/1000 | Loss: 0.00053384
Iteration 54/1000 | Loss: 0.00087788
Iteration 55/1000 | Loss: 0.00055536
Iteration 56/1000 | Loss: 0.00032366
Iteration 57/1000 | Loss: 0.00104515
Iteration 58/1000 | Loss: 0.00179213
Iteration 59/1000 | Loss: 0.00170440
Iteration 60/1000 | Loss: 0.00030237
Iteration 61/1000 | Loss: 0.00021839
Iteration 62/1000 | Loss: 0.00008886
Iteration 63/1000 | Loss: 0.00057249
Iteration 64/1000 | Loss: 0.00030504
Iteration 65/1000 | Loss: 0.00031679
Iteration 66/1000 | Loss: 0.00018476
Iteration 67/1000 | Loss: 0.00042853
Iteration 68/1000 | Loss: 0.00071084
Iteration 69/1000 | Loss: 0.00045735
Iteration 70/1000 | Loss: 0.00045309
Iteration 71/1000 | Loss: 0.00071135
Iteration 72/1000 | Loss: 0.00053625
Iteration 73/1000 | Loss: 0.00031757
Iteration 74/1000 | Loss: 0.00048823
Iteration 75/1000 | Loss: 0.00007329
Iteration 76/1000 | Loss: 0.00015495
Iteration 77/1000 | Loss: 0.00044676
Iteration 78/1000 | Loss: 0.00006052
Iteration 79/1000 | Loss: 0.00030908
Iteration 80/1000 | Loss: 0.00018473
Iteration 81/1000 | Loss: 0.00029796
Iteration 82/1000 | Loss: 0.00021448
Iteration 83/1000 | Loss: 0.00040083
Iteration 84/1000 | Loss: 0.00019401
Iteration 85/1000 | Loss: 0.00036896
Iteration 86/1000 | Loss: 0.00018194
Iteration 87/1000 | Loss: 0.00043251
Iteration 88/1000 | Loss: 0.00049417
Iteration 89/1000 | Loss: 0.00166519
Iteration 90/1000 | Loss: 0.00070245
Iteration 91/1000 | Loss: 0.00074642
Iteration 92/1000 | Loss: 0.00182616
Iteration 93/1000 | Loss: 0.00080378
Iteration 94/1000 | Loss: 0.00073429
Iteration 95/1000 | Loss: 0.00051390
Iteration 96/1000 | Loss: 0.00225615
Iteration 97/1000 | Loss: 0.00071719
Iteration 98/1000 | Loss: 0.00039534
Iteration 99/1000 | Loss: 0.00053518
Iteration 100/1000 | Loss: 0.00102402
Iteration 101/1000 | Loss: 0.00075156
Iteration 102/1000 | Loss: 0.00045297
Iteration 103/1000 | Loss: 0.00006014
Iteration 104/1000 | Loss: 0.00016160
Iteration 105/1000 | Loss: 0.00006790
Iteration 106/1000 | Loss: 0.00017821
Iteration 107/1000 | Loss: 0.00005431
Iteration 108/1000 | Loss: 0.00029073
Iteration 109/1000 | Loss: 0.00005688
Iteration 110/1000 | Loss: 0.00019728
Iteration 111/1000 | Loss: 0.00037544
Iteration 112/1000 | Loss: 0.00064956
Iteration 113/1000 | Loss: 0.00022731
Iteration 114/1000 | Loss: 0.00047660
Iteration 115/1000 | Loss: 0.00027622
Iteration 116/1000 | Loss: 0.00032664
Iteration 117/1000 | Loss: 0.00041564
Iteration 118/1000 | Loss: 0.00034499
Iteration 119/1000 | Loss: 0.00036324
Iteration 120/1000 | Loss: 0.00008279
Iteration 121/1000 | Loss: 0.00011822
Iteration 122/1000 | Loss: 0.00147570
Iteration 123/1000 | Loss: 0.00150402
Iteration 124/1000 | Loss: 0.00241329
Iteration 125/1000 | Loss: 0.00024509
Iteration 126/1000 | Loss: 0.00086254
Iteration 127/1000 | Loss: 0.00213627
Iteration 128/1000 | Loss: 0.00017477
Iteration 129/1000 | Loss: 0.00048221
Iteration 130/1000 | Loss: 0.00055988
Iteration 131/1000 | Loss: 0.00061415
Iteration 132/1000 | Loss: 0.00052362
Iteration 133/1000 | Loss: 0.00041618
Iteration 134/1000 | Loss: 0.00041961
Iteration 135/1000 | Loss: 0.00053722
Iteration 136/1000 | Loss: 0.00038323
Iteration 137/1000 | Loss: 0.00057531
Iteration 138/1000 | Loss: 0.00041893
Iteration 139/1000 | Loss: 0.00180023
Iteration 140/1000 | Loss: 0.00135166
Iteration 141/1000 | Loss: 0.00010857
Iteration 142/1000 | Loss: 0.00017491
Iteration 143/1000 | Loss: 0.00005965
Iteration 144/1000 | Loss: 0.00005675
Iteration 145/1000 | Loss: 0.00005452
Iteration 146/1000 | Loss: 0.00040185
Iteration 147/1000 | Loss: 0.00286517
Iteration 148/1000 | Loss: 0.00116037
Iteration 149/1000 | Loss: 0.00063769
Iteration 150/1000 | Loss: 0.00044804
Iteration 151/1000 | Loss: 0.00052540
Iteration 152/1000 | Loss: 0.00092460
Iteration 153/1000 | Loss: 0.00010244
Iteration 154/1000 | Loss: 0.00014291
Iteration 155/1000 | Loss: 0.00032414
Iteration 156/1000 | Loss: 0.00088610
Iteration 157/1000 | Loss: 0.00096268
Iteration 158/1000 | Loss: 0.00052877
Iteration 159/1000 | Loss: 0.00052829
Iteration 160/1000 | Loss: 0.00040157
Iteration 161/1000 | Loss: 0.00012567
Iteration 162/1000 | Loss: 0.00140655
Iteration 163/1000 | Loss: 0.00085419
Iteration 164/1000 | Loss: 0.00005879
Iteration 165/1000 | Loss: 0.00040717
Iteration 166/1000 | Loss: 0.00019127
Iteration 167/1000 | Loss: 0.00051278
Iteration 168/1000 | Loss: 0.00059550
Iteration 169/1000 | Loss: 0.00175850
Iteration 170/1000 | Loss: 0.00229025
Iteration 171/1000 | Loss: 0.00087862
Iteration 172/1000 | Loss: 0.00049957
Iteration 173/1000 | Loss: 0.00035093
Iteration 174/1000 | Loss: 0.00019067
Iteration 175/1000 | Loss: 0.00024596
Iteration 176/1000 | Loss: 0.00020336
Iteration 177/1000 | Loss: 0.00018704
Iteration 178/1000 | Loss: 0.00013776
Iteration 179/1000 | Loss: 0.00049359
Iteration 180/1000 | Loss: 0.00032348
Iteration 181/1000 | Loss: 0.00014417
Iteration 182/1000 | Loss: 0.00050386
Iteration 183/1000 | Loss: 0.00079787
Iteration 184/1000 | Loss: 0.00222334
Iteration 185/1000 | Loss: 0.00309054
Iteration 186/1000 | Loss: 0.00051715
Iteration 187/1000 | Loss: 0.00102539
Iteration 188/1000 | Loss: 0.00076441
Iteration 189/1000 | Loss: 0.00113297
Iteration 190/1000 | Loss: 0.00011625
Iteration 191/1000 | Loss: 0.00012714
Iteration 192/1000 | Loss: 0.00009827
Iteration 193/1000 | Loss: 0.00115775
Iteration 194/1000 | Loss: 0.00005549
Iteration 195/1000 | Loss: 0.00030624
Iteration 196/1000 | Loss: 0.00097333
Iteration 197/1000 | Loss: 0.00038037
Iteration 198/1000 | Loss: 0.00028913
Iteration 199/1000 | Loss: 0.00027249
Iteration 200/1000 | Loss: 0.00039331
Iteration 201/1000 | Loss: 0.00006153
Iteration 202/1000 | Loss: 0.00041357
Iteration 203/1000 | Loss: 0.00014414
Iteration 204/1000 | Loss: 0.00018029
Iteration 205/1000 | Loss: 0.00033996
Iteration 206/1000 | Loss: 0.00014549
Iteration 207/1000 | Loss: 0.00045840
Iteration 208/1000 | Loss: 0.00047565
Iteration 209/1000 | Loss: 0.00044898
Iteration 210/1000 | Loss: 0.00032327
Iteration 211/1000 | Loss: 0.00042898
Iteration 212/1000 | Loss: 0.00005984
Iteration 213/1000 | Loss: 0.00015687
Iteration 214/1000 | Loss: 0.00196511
Iteration 215/1000 | Loss: 0.00038420
Iteration 216/1000 | Loss: 0.00014377
Iteration 217/1000 | Loss: 0.00018733
Iteration 218/1000 | Loss: 0.00004742
Iteration 219/1000 | Loss: 0.00033212
Iteration 220/1000 | Loss: 0.00048181
Iteration 221/1000 | Loss: 0.00034529
Iteration 222/1000 | Loss: 0.00059221
Iteration 223/1000 | Loss: 0.00038145
Iteration 224/1000 | Loss: 0.00023007
Iteration 225/1000 | Loss: 0.00035929
Iteration 226/1000 | Loss: 0.00048515
Iteration 227/1000 | Loss: 0.00048336
Iteration 228/1000 | Loss: 0.00048196
Iteration 229/1000 | Loss: 0.00023326
Iteration 230/1000 | Loss: 0.00041291
Iteration 231/1000 | Loss: 0.00037324
Iteration 232/1000 | Loss: 0.00022535
Iteration 233/1000 | Loss: 0.00052942
Iteration 234/1000 | Loss: 0.00054886
Iteration 235/1000 | Loss: 0.00047535
Iteration 236/1000 | Loss: 0.00011749
Iteration 237/1000 | Loss: 0.00043936
Iteration 238/1000 | Loss: 0.00012977
Iteration 239/1000 | Loss: 0.00004920
Iteration 240/1000 | Loss: 0.00004755
Iteration 241/1000 | Loss: 0.00024107
Iteration 242/1000 | Loss: 0.00008291
Iteration 243/1000 | Loss: 0.00041128
Iteration 244/1000 | Loss: 0.00029861
Iteration 245/1000 | Loss: 0.00122547
Iteration 246/1000 | Loss: 0.00079068
Iteration 247/1000 | Loss: 0.00013169
Iteration 248/1000 | Loss: 0.00011681
Iteration 249/1000 | Loss: 0.00025349
Iteration 250/1000 | Loss: 0.00038761
Iteration 251/1000 | Loss: 0.00033688
Iteration 252/1000 | Loss: 0.00037196
Iteration 253/1000 | Loss: 0.00039619
Iteration 254/1000 | Loss: 0.00030884
Iteration 255/1000 | Loss: 0.00041827
Iteration 256/1000 | Loss: 0.00104474
Iteration 257/1000 | Loss: 0.00043590
Iteration 258/1000 | Loss: 0.00006571
Iteration 259/1000 | Loss: 0.00005006
Iteration 260/1000 | Loss: 0.00057261
Iteration 261/1000 | Loss: 0.00005343
Iteration 262/1000 | Loss: 0.00029510
Iteration 263/1000 | Loss: 0.00049333
Iteration 264/1000 | Loss: 0.00004362
Iteration 265/1000 | Loss: 0.00004402
Iteration 266/1000 | Loss: 0.00004806
Iteration 267/1000 | Loss: 0.00003096
Iteration 268/1000 | Loss: 0.00003674
Iteration 269/1000 | Loss: 0.00003488
Iteration 270/1000 | Loss: 0.00003616
Iteration 271/1000 | Loss: 0.00003201
Iteration 272/1000 | Loss: 0.00004226
Iteration 273/1000 | Loss: 0.00004038
Iteration 274/1000 | Loss: 0.00089103
Iteration 275/1000 | Loss: 0.00018962
Iteration 276/1000 | Loss: 0.00007850
Iteration 277/1000 | Loss: 0.00005519
Iteration 278/1000 | Loss: 0.00005737
Iteration 279/1000 | Loss: 0.00021479
Iteration 280/1000 | Loss: 0.00012908
Iteration 281/1000 | Loss: 0.00006431
Iteration 282/1000 | Loss: 0.00021557
Iteration 283/1000 | Loss: 0.00005102
Iteration 284/1000 | Loss: 0.00004670
Iteration 285/1000 | Loss: 0.00004198
Iteration 286/1000 | Loss: 0.00004202
Iteration 287/1000 | Loss: 0.00003979
Iteration 288/1000 | Loss: 0.00003631
Iteration 289/1000 | Loss: 0.00004239
Iteration 290/1000 | Loss: 0.00004350
Iteration 291/1000 | Loss: 0.00004426
Iteration 292/1000 | Loss: 0.00004216
Iteration 293/1000 | Loss: 0.00004125
Iteration 294/1000 | Loss: 0.00004100
Iteration 295/1000 | Loss: 0.00004222
Iteration 296/1000 | Loss: 0.00003819
Iteration 297/1000 | Loss: 0.00003797
Iteration 298/1000 | Loss: 0.00004087
Iteration 299/1000 | Loss: 0.00003493
Iteration 300/1000 | Loss: 0.00004023
Iteration 301/1000 | Loss: 0.00003869
Iteration 302/1000 | Loss: 0.00004572
Iteration 303/1000 | Loss: 0.00004426
Iteration 304/1000 | Loss: 0.00004155
Iteration 305/1000 | Loss: 0.00003937
Iteration 306/1000 | Loss: 0.00004178
Iteration 307/1000 | Loss: 0.00003916
Iteration 308/1000 | Loss: 0.00004162
Iteration 309/1000 | Loss: 0.00003914
Iteration 310/1000 | Loss: 0.00004145
Iteration 311/1000 | Loss: 0.00003497
Iteration 312/1000 | Loss: 0.00003495
Iteration 313/1000 | Loss: 0.00003955
Iteration 314/1000 | Loss: 0.00003981
Iteration 315/1000 | Loss: 0.00003864
Iteration 316/1000 | Loss: 0.00004105
Iteration 317/1000 | Loss: 0.00004079
Iteration 318/1000 | Loss: 0.00004045
Iteration 319/1000 | Loss: 0.00004053
Iteration 320/1000 | Loss: 0.00004302
Iteration 321/1000 | Loss: 0.00003982
Iteration 322/1000 | Loss: 0.00004225
Iteration 323/1000 | Loss: 0.00004425
Iteration 324/1000 | Loss: 0.00004556
Iteration 325/1000 | Loss: 0.00003939
Iteration 326/1000 | Loss: 0.00004365
Iteration 327/1000 | Loss: 0.00003927
Iteration 328/1000 | Loss: 0.00004405
Iteration 329/1000 | Loss: 0.00003916
Iteration 330/1000 | Loss: 0.00004197
Iteration 331/1000 | Loss: 0.00004218
Iteration 332/1000 | Loss: 0.00004059
Iteration 333/1000 | Loss: 0.00004055
Iteration 334/1000 | Loss: 0.00004236
Iteration 335/1000 | Loss: 0.00004034
Iteration 336/1000 | Loss: 0.00004138
Iteration 337/1000 | Loss: 0.00003963
Iteration 338/1000 | Loss: 0.00004114
Iteration 339/1000 | Loss: 0.00003964
Iteration 340/1000 | Loss: 0.00004104
Iteration 341/1000 | Loss: 0.00003223
Iteration 342/1000 | Loss: 0.00003948
Iteration 343/1000 | Loss: 0.00003533
Iteration 344/1000 | Loss: 0.00003877
Iteration 345/1000 | Loss: 0.00004131
Iteration 346/1000 | Loss: 0.00004064
Iteration 347/1000 | Loss: 0.00004952
Iteration 348/1000 | Loss: 0.00004011
Iteration 349/1000 | Loss: 0.00004126
Iteration 350/1000 | Loss: 0.00003954
Iteration 351/1000 | Loss: 0.00004094
Iteration 352/1000 | Loss: 0.00003909
Iteration 353/1000 | Loss: 0.00004094
Iteration 354/1000 | Loss: 0.00003881
Iteration 355/1000 | Loss: 0.00004277
Iteration 356/1000 | Loss: 0.00003905
Iteration 357/1000 | Loss: 0.00004052
Iteration 358/1000 | Loss: 0.00003816
Iteration 359/1000 | Loss: 0.00004055
Iteration 360/1000 | Loss: 0.00003806
Iteration 361/1000 | Loss: 0.00003970
Iteration 362/1000 | Loss: 0.00004051
Iteration 363/1000 | Loss: 0.00003984
Iteration 364/1000 | Loss: 0.00004004
Iteration 365/1000 | Loss: 0.00004106
Iteration 366/1000 | Loss: 0.00003828
Iteration 367/1000 | Loss: 0.00004062
Iteration 368/1000 | Loss: 0.00003705
Iteration 369/1000 | Loss: 0.00004074
Iteration 370/1000 | Loss: 0.00003925
Iteration 371/1000 | Loss: 0.00003893
Iteration 372/1000 | Loss: 0.00003889
Iteration 373/1000 | Loss: 0.00004251
Iteration 374/1000 | Loss: 0.00003817
Iteration 375/1000 | Loss: 0.00004136
Iteration 376/1000 | Loss: 0.00003747
Iteration 377/1000 | Loss: 0.00004050
Iteration 378/1000 | Loss: 0.00003800
Iteration 379/1000 | Loss: 0.00003994
Iteration 380/1000 | Loss: 0.00003869
Iteration 381/1000 | Loss: 0.00004405
Iteration 382/1000 | Loss: 0.00003985
Iteration 383/1000 | Loss: 0.00004223
Iteration 384/1000 | Loss: 0.00003852
Iteration 385/1000 | Loss: 0.00004297
Iteration 386/1000 | Loss: 0.00003903
Iteration 387/1000 | Loss: 0.00004303
Iteration 388/1000 | Loss: 0.00004067
Iteration 389/1000 | Loss: 0.00004147
Iteration 390/1000 | Loss: 0.00003800
Iteration 391/1000 | Loss: 0.00004021
Iteration 392/1000 | Loss: 0.00003881
Iteration 393/1000 | Loss: 0.00003989
Iteration 394/1000 | Loss: 0.00003823
Iteration 395/1000 | Loss: 0.00003984
Iteration 396/1000 | Loss: 0.00003856
Iteration 397/1000 | Loss: 0.00004025
Iteration 398/1000 | Loss: 0.00003899
Iteration 399/1000 | Loss: 0.00004388
Iteration 400/1000 | Loss: 0.00003894
Iteration 401/1000 | Loss: 0.00004238
Iteration 402/1000 | Loss: 0.00003889
Iteration 403/1000 | Loss: 0.00004086
Iteration 404/1000 | Loss: 0.00003850
Iteration 405/1000 | Loss: 0.00004034
Iteration 406/1000 | Loss: 0.00003681
Iteration 407/1000 | Loss: 0.00004116
Iteration 408/1000 | Loss: 0.00003781
Iteration 409/1000 | Loss: 0.00004409
Iteration 410/1000 | Loss: 0.00003771
Iteration 411/1000 | Loss: 0.00003968
Iteration 412/1000 | Loss: 0.00003725
Iteration 413/1000 | Loss: 0.00004075
Iteration 414/1000 | Loss: 0.00003899
Iteration 415/1000 | Loss: 0.00004096
Iteration 416/1000 | Loss: 0.00003755
Iteration 417/1000 | Loss: 0.00004589
Iteration 418/1000 | Loss: 0.00004096
Iteration 419/1000 | Loss: 0.00004798
Iteration 420/1000 | Loss: 0.00003817
Iteration 421/1000 | Loss: 0.00004006
Iteration 422/1000 | Loss: 0.00003841
Iteration 423/1000 | Loss: 0.00003974
Iteration 424/1000 | Loss: 0.00003822
Iteration 425/1000 | Loss: 0.00003947
Iteration 426/1000 | Loss: 0.00003827
Iteration 427/1000 | Loss: 0.00003929
Iteration 428/1000 | Loss: 0.00003823
Iteration 429/1000 | Loss: 0.00004240
Iteration 430/1000 | Loss: 0.00003809
Iteration 431/1000 | Loss: 0.00004231
Iteration 432/1000 | Loss: 0.00003794
Iteration 433/1000 | Loss: 0.00004201
Iteration 434/1000 | Loss: 0.00021558
Iteration 435/1000 | Loss: 0.00018824
Iteration 436/1000 | Loss: 0.00020458
Iteration 437/1000 | Loss: 0.00023571
Iteration 438/1000 | Loss: 0.00019400
Iteration 439/1000 | Loss: 0.00015517
Iteration 440/1000 | Loss: 0.00014992
Iteration 441/1000 | Loss: 0.00022350
Iteration 442/1000 | Loss: 0.00014393
Iteration 443/1000 | Loss: 0.00003433
Iteration 444/1000 | Loss: 0.00002746
Iteration 445/1000 | Loss: 0.00024756
Iteration 446/1000 | Loss: 0.00018636
Iteration 447/1000 | Loss: 0.00003270
Iteration 448/1000 | Loss: 0.00002799
Iteration 449/1000 | Loss: 0.00002708
Iteration 450/1000 | Loss: 0.00009804
Iteration 451/1000 | Loss: 0.00022940
Iteration 452/1000 | Loss: 0.00020631
Iteration 453/1000 | Loss: 0.00003259
Iteration 454/1000 | Loss: 0.00003030
Iteration 455/1000 | Loss: 0.00002812
Iteration 456/1000 | Loss: 0.00002752
Iteration 457/1000 | Loss: 0.00002657
Iteration 458/1000 | Loss: 0.00002495
Iteration 459/1000 | Loss: 0.00002363
Iteration 460/1000 | Loss: 0.00002307
Iteration 461/1000 | Loss: 0.00002273
Iteration 462/1000 | Loss: 0.00002252
Iteration 463/1000 | Loss: 0.00002236
Iteration 464/1000 | Loss: 0.00002229
Iteration 465/1000 | Loss: 0.00002228
Iteration 466/1000 | Loss: 0.00002227
Iteration 467/1000 | Loss: 0.00002227
Iteration 468/1000 | Loss: 0.00002226
Iteration 469/1000 | Loss: 0.00002226
Iteration 470/1000 | Loss: 0.00002226
Iteration 471/1000 | Loss: 0.00002225
Iteration 472/1000 | Loss: 0.00002225
Iteration 473/1000 | Loss: 0.00002224
Iteration 474/1000 | Loss: 0.00002224
Iteration 475/1000 | Loss: 0.00002224
Iteration 476/1000 | Loss: 0.00002223
Iteration 477/1000 | Loss: 0.00002220
Iteration 478/1000 | Loss: 0.00002219
Iteration 479/1000 | Loss: 0.00002219
Iteration 480/1000 | Loss: 0.00002218
Iteration 481/1000 | Loss: 0.00002217
Iteration 482/1000 | Loss: 0.00002217
Iteration 483/1000 | Loss: 0.00002216
Iteration 484/1000 | Loss: 0.00002216
Iteration 485/1000 | Loss: 0.00002214
Iteration 486/1000 | Loss: 0.00002214
Iteration 487/1000 | Loss: 0.00002214
Iteration 488/1000 | Loss: 0.00002214
Iteration 489/1000 | Loss: 0.00002214
Iteration 490/1000 | Loss: 0.00002214
Iteration 491/1000 | Loss: 0.00002214
Iteration 492/1000 | Loss: 0.00002213
Iteration 493/1000 | Loss: 0.00002213
Iteration 494/1000 | Loss: 0.00002213
Iteration 495/1000 | Loss: 0.00002212
Iteration 496/1000 | Loss: 0.00002212
Iteration 497/1000 | Loss: 0.00002212
Iteration 498/1000 | Loss: 0.00002211
Iteration 499/1000 | Loss: 0.00002211
Iteration 500/1000 | Loss: 0.00002211
Iteration 501/1000 | Loss: 0.00002211
Iteration 502/1000 | Loss: 0.00002211
Iteration 503/1000 | Loss: 0.00002211
Iteration 504/1000 | Loss: 0.00002211
Iteration 505/1000 | Loss: 0.00002211
Iteration 506/1000 | Loss: 0.00002211
Iteration 507/1000 | Loss: 0.00002211
Iteration 508/1000 | Loss: 0.00002211
Iteration 509/1000 | Loss: 0.00002210
Iteration 510/1000 | Loss: 0.00002210
Iteration 511/1000 | Loss: 0.00002210
Iteration 512/1000 | Loss: 0.00002210
Iteration 513/1000 | Loss: 0.00002210
Iteration 514/1000 | Loss: 0.00002210
Iteration 515/1000 | Loss: 0.00002210
Iteration 516/1000 | Loss: 0.00002210
Iteration 517/1000 | Loss: 0.00002210
Iteration 518/1000 | Loss: 0.00002209
Iteration 519/1000 | Loss: 0.00002209
Iteration 520/1000 | Loss: 0.00002209
Iteration 521/1000 | Loss: 0.00002209
Iteration 522/1000 | Loss: 0.00002209
Iteration 523/1000 | Loss: 0.00002209
Iteration 524/1000 | Loss: 0.00002209
Iteration 525/1000 | Loss: 0.00002209
Iteration 526/1000 | Loss: 0.00002209
Iteration 527/1000 | Loss: 0.00002209
Iteration 528/1000 | Loss: 0.00002209
Iteration 529/1000 | Loss: 0.00002208
Iteration 530/1000 | Loss: 0.00002208
Iteration 531/1000 | Loss: 0.00002208
Iteration 532/1000 | Loss: 0.00002208
Iteration 533/1000 | Loss: 0.00002208
Iteration 534/1000 | Loss: 0.00002208
Iteration 535/1000 | Loss: 0.00002208
Iteration 536/1000 | Loss: 0.00002208
Iteration 537/1000 | Loss: 0.00002208
Iteration 538/1000 | Loss: 0.00002208
Iteration 539/1000 | Loss: 0.00002208
Iteration 540/1000 | Loss: 0.00002208
Iteration 541/1000 | Loss: 0.00002208
Iteration 542/1000 | Loss: 0.00002208
Iteration 543/1000 | Loss: 0.00002208
Iteration 544/1000 | Loss: 0.00002208
Iteration 545/1000 | Loss: 0.00002208
Iteration 546/1000 | Loss: 0.00002208
Iteration 547/1000 | Loss: 0.00002208
Iteration 548/1000 | Loss: 0.00002208
Iteration 549/1000 | Loss: 0.00002208
Iteration 550/1000 | Loss: 0.00002208
Iteration 551/1000 | Loss: 0.00002208
Iteration 552/1000 | Loss: 0.00002208
Iteration 553/1000 | Loss: 0.00002208
Iteration 554/1000 | Loss: 0.00002208
Iteration 555/1000 | Loss: 0.00002208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 555. Stopping optimization.
Last 5 losses: [2.208101432188414e-05, 2.208101432188414e-05, 2.208101432188414e-05, 2.208101432188414e-05, 2.208101432188414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.208101432188414e-05

Optimization complete. Final v2v error: 3.73063325881958 mm

Highest mean error: 14.101465225219727 mm for frame 82

Lowest mean error: 2.947467565536499 mm for frame 47

Saving results

Total time: 722.7188708782196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826829
Iteration 2/25 | Loss: 0.00085782
Iteration 3/25 | Loss: 0.00069448
Iteration 4/25 | Loss: 0.00066242
Iteration 5/25 | Loss: 0.00065444
Iteration 6/25 | Loss: 0.00065306
Iteration 7/25 | Loss: 0.00065258
Iteration 8/25 | Loss: 0.00065258
Iteration 9/25 | Loss: 0.00065258
Iteration 10/25 | Loss: 0.00065258
Iteration 11/25 | Loss: 0.00065258
Iteration 12/25 | Loss: 0.00065258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006525839562527835, 0.0006525839562527835, 0.0006525839562527835, 0.0006525839562527835, 0.0006525839562527835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006525839562527835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50976491
Iteration 2/25 | Loss: 0.00032056
Iteration 3/25 | Loss: 0.00032056
Iteration 4/25 | Loss: 0.00032056
Iteration 5/25 | Loss: 0.00032056
Iteration 6/25 | Loss: 0.00032056
Iteration 7/25 | Loss: 0.00032056
Iteration 8/25 | Loss: 0.00032056
Iteration 9/25 | Loss: 0.00032056
Iteration 10/25 | Loss: 0.00032056
Iteration 11/25 | Loss: 0.00032056
Iteration 12/25 | Loss: 0.00032056
Iteration 13/25 | Loss: 0.00032056
Iteration 14/25 | Loss: 0.00032056
Iteration 15/25 | Loss: 0.00032056
Iteration 16/25 | Loss: 0.00032056
Iteration 17/25 | Loss: 0.00032056
Iteration 18/25 | Loss: 0.00032056
Iteration 19/25 | Loss: 0.00032056
Iteration 20/25 | Loss: 0.00032056
Iteration 21/25 | Loss: 0.00032056
Iteration 22/25 | Loss: 0.00032056
Iteration 23/25 | Loss: 0.00032056
Iteration 24/25 | Loss: 0.00032056
Iteration 25/25 | Loss: 0.00032056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032056
Iteration 2/1000 | Loss: 0.00002941
Iteration 3/1000 | Loss: 0.00002008
Iteration 4/1000 | Loss: 0.00001888
Iteration 5/1000 | Loss: 0.00001807
Iteration 6/1000 | Loss: 0.00001757
Iteration 7/1000 | Loss: 0.00001710
Iteration 8/1000 | Loss: 0.00001680
Iteration 9/1000 | Loss: 0.00001675
Iteration 10/1000 | Loss: 0.00001668
Iteration 11/1000 | Loss: 0.00001654
Iteration 12/1000 | Loss: 0.00001641
Iteration 13/1000 | Loss: 0.00001633
Iteration 14/1000 | Loss: 0.00001633
Iteration 15/1000 | Loss: 0.00001628
Iteration 16/1000 | Loss: 0.00001628
Iteration 17/1000 | Loss: 0.00001628
Iteration 18/1000 | Loss: 0.00001627
Iteration 19/1000 | Loss: 0.00001625
Iteration 20/1000 | Loss: 0.00001625
Iteration 21/1000 | Loss: 0.00001624
Iteration 22/1000 | Loss: 0.00001624
Iteration 23/1000 | Loss: 0.00001624
Iteration 24/1000 | Loss: 0.00001624
Iteration 25/1000 | Loss: 0.00001624
Iteration 26/1000 | Loss: 0.00001624
Iteration 27/1000 | Loss: 0.00001624
Iteration 28/1000 | Loss: 0.00001623
Iteration 29/1000 | Loss: 0.00001623
Iteration 30/1000 | Loss: 0.00001623
Iteration 31/1000 | Loss: 0.00001623
Iteration 32/1000 | Loss: 0.00001623
Iteration 33/1000 | Loss: 0.00001623
Iteration 34/1000 | Loss: 0.00001623
Iteration 35/1000 | Loss: 0.00001621
Iteration 36/1000 | Loss: 0.00001620
Iteration 37/1000 | Loss: 0.00001620
Iteration 38/1000 | Loss: 0.00001619
Iteration 39/1000 | Loss: 0.00001618
Iteration 40/1000 | Loss: 0.00001618
Iteration 41/1000 | Loss: 0.00001617
Iteration 42/1000 | Loss: 0.00001616
Iteration 43/1000 | Loss: 0.00001616
Iteration 44/1000 | Loss: 0.00001616
Iteration 45/1000 | Loss: 0.00001615
Iteration 46/1000 | Loss: 0.00001615
Iteration 47/1000 | Loss: 0.00001615
Iteration 48/1000 | Loss: 0.00001614
Iteration 49/1000 | Loss: 0.00001614
Iteration 50/1000 | Loss: 0.00001614
Iteration 51/1000 | Loss: 0.00001613
Iteration 52/1000 | Loss: 0.00001612
Iteration 53/1000 | Loss: 0.00001612
Iteration 54/1000 | Loss: 0.00001612
Iteration 55/1000 | Loss: 0.00001612
Iteration 56/1000 | Loss: 0.00001612
Iteration 57/1000 | Loss: 0.00001611
Iteration 58/1000 | Loss: 0.00001611
Iteration 59/1000 | Loss: 0.00001611
Iteration 60/1000 | Loss: 0.00001611
Iteration 61/1000 | Loss: 0.00001611
Iteration 62/1000 | Loss: 0.00001611
Iteration 63/1000 | Loss: 0.00001611
Iteration 64/1000 | Loss: 0.00001610
Iteration 65/1000 | Loss: 0.00001610
Iteration 66/1000 | Loss: 0.00001610
Iteration 67/1000 | Loss: 0.00001610
Iteration 68/1000 | Loss: 0.00001610
Iteration 69/1000 | Loss: 0.00001609
Iteration 70/1000 | Loss: 0.00001609
Iteration 71/1000 | Loss: 0.00001609
Iteration 72/1000 | Loss: 0.00001609
Iteration 73/1000 | Loss: 0.00001608
Iteration 74/1000 | Loss: 0.00001608
Iteration 75/1000 | Loss: 0.00001608
Iteration 76/1000 | Loss: 0.00001608
Iteration 77/1000 | Loss: 0.00001608
Iteration 78/1000 | Loss: 0.00001608
Iteration 79/1000 | Loss: 0.00001608
Iteration 80/1000 | Loss: 0.00001608
Iteration 81/1000 | Loss: 0.00001608
Iteration 82/1000 | Loss: 0.00001608
Iteration 83/1000 | Loss: 0.00001607
Iteration 84/1000 | Loss: 0.00001607
Iteration 85/1000 | Loss: 0.00001607
Iteration 86/1000 | Loss: 0.00001606
Iteration 87/1000 | Loss: 0.00001606
Iteration 88/1000 | Loss: 0.00001606
Iteration 89/1000 | Loss: 0.00001604
Iteration 90/1000 | Loss: 0.00001604
Iteration 91/1000 | Loss: 0.00001604
Iteration 92/1000 | Loss: 0.00001603
Iteration 93/1000 | Loss: 0.00001603
Iteration 94/1000 | Loss: 0.00001603
Iteration 95/1000 | Loss: 0.00001603
Iteration 96/1000 | Loss: 0.00001603
Iteration 97/1000 | Loss: 0.00001603
Iteration 98/1000 | Loss: 0.00001603
Iteration 99/1000 | Loss: 0.00001603
Iteration 100/1000 | Loss: 0.00001602
Iteration 101/1000 | Loss: 0.00001601
Iteration 102/1000 | Loss: 0.00001601
Iteration 103/1000 | Loss: 0.00001600
Iteration 104/1000 | Loss: 0.00001600
Iteration 105/1000 | Loss: 0.00001600
Iteration 106/1000 | Loss: 0.00001600
Iteration 107/1000 | Loss: 0.00001600
Iteration 108/1000 | Loss: 0.00001600
Iteration 109/1000 | Loss: 0.00001600
Iteration 110/1000 | Loss: 0.00001600
Iteration 111/1000 | Loss: 0.00001600
Iteration 112/1000 | Loss: 0.00001600
Iteration 113/1000 | Loss: 0.00001599
Iteration 114/1000 | Loss: 0.00001599
Iteration 115/1000 | Loss: 0.00001599
Iteration 116/1000 | Loss: 0.00001599
Iteration 117/1000 | Loss: 0.00001599
Iteration 118/1000 | Loss: 0.00001599
Iteration 119/1000 | Loss: 0.00001598
Iteration 120/1000 | Loss: 0.00001598
Iteration 121/1000 | Loss: 0.00001598
Iteration 122/1000 | Loss: 0.00001598
Iteration 123/1000 | Loss: 0.00001598
Iteration 124/1000 | Loss: 0.00001598
Iteration 125/1000 | Loss: 0.00001598
Iteration 126/1000 | Loss: 0.00001598
Iteration 127/1000 | Loss: 0.00001598
Iteration 128/1000 | Loss: 0.00001598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.598039125383366e-05, 1.598039125383366e-05, 1.598039125383366e-05, 1.598039125383366e-05, 1.598039125383366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.598039125383366e-05

Optimization complete. Final v2v error: 3.4183197021484375 mm

Highest mean error: 3.9822678565979004 mm for frame 99

Lowest mean error: 3.197826623916626 mm for frame 40

Saving results

Total time: 33.636128187179565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882068
Iteration 2/25 | Loss: 0.00119233
Iteration 3/25 | Loss: 0.00081256
Iteration 4/25 | Loss: 0.00073509
Iteration 5/25 | Loss: 0.00072849
Iteration 6/25 | Loss: 0.00072754
Iteration 7/25 | Loss: 0.00072754
Iteration 8/25 | Loss: 0.00072754
Iteration 9/25 | Loss: 0.00072754
Iteration 10/25 | Loss: 0.00072754
Iteration 11/25 | Loss: 0.00072754
Iteration 12/25 | Loss: 0.00072754
Iteration 13/25 | Loss: 0.00072754
Iteration 14/25 | Loss: 0.00072754
Iteration 15/25 | Loss: 0.00072754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007275363313965499, 0.0007275363313965499, 0.0007275363313965499, 0.0007275363313965499, 0.0007275363313965499]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007275363313965499

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98643774
Iteration 2/25 | Loss: 0.00023810
Iteration 3/25 | Loss: 0.00023809
Iteration 4/25 | Loss: 0.00023809
Iteration 5/25 | Loss: 0.00023809
Iteration 6/25 | Loss: 0.00023809
Iteration 7/25 | Loss: 0.00023809
Iteration 8/25 | Loss: 0.00023809
Iteration 9/25 | Loss: 0.00023809
Iteration 10/25 | Loss: 0.00023809
Iteration 11/25 | Loss: 0.00023809
Iteration 12/25 | Loss: 0.00023809
Iteration 13/25 | Loss: 0.00023809
Iteration 14/25 | Loss: 0.00023809
Iteration 15/25 | Loss: 0.00023809
Iteration 16/25 | Loss: 0.00023809
Iteration 17/25 | Loss: 0.00023809
Iteration 18/25 | Loss: 0.00023809
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002380854421062395, 0.0002380854421062395, 0.0002380854421062395, 0.0002380854421062395, 0.0002380854421062395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002380854421062395

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023809
Iteration 2/1000 | Loss: 0.00003584
Iteration 3/1000 | Loss: 0.00002846
Iteration 4/1000 | Loss: 0.00002663
Iteration 5/1000 | Loss: 0.00002519
Iteration 6/1000 | Loss: 0.00002440
Iteration 7/1000 | Loss: 0.00002356
Iteration 8/1000 | Loss: 0.00002315
Iteration 9/1000 | Loss: 0.00002279
Iteration 10/1000 | Loss: 0.00002274
Iteration 11/1000 | Loss: 0.00002271
Iteration 12/1000 | Loss: 0.00002270
Iteration 13/1000 | Loss: 0.00002253
Iteration 14/1000 | Loss: 0.00002243
Iteration 15/1000 | Loss: 0.00002243
Iteration 16/1000 | Loss: 0.00002242
Iteration 17/1000 | Loss: 0.00002242
Iteration 18/1000 | Loss: 0.00002242
Iteration 19/1000 | Loss: 0.00002242
Iteration 20/1000 | Loss: 0.00002242
Iteration 21/1000 | Loss: 0.00002242
Iteration 22/1000 | Loss: 0.00002242
Iteration 23/1000 | Loss: 0.00002242
Iteration 24/1000 | Loss: 0.00002242
Iteration 25/1000 | Loss: 0.00002242
Iteration 26/1000 | Loss: 0.00002241
Iteration 27/1000 | Loss: 0.00002241
Iteration 28/1000 | Loss: 0.00002241
Iteration 29/1000 | Loss: 0.00002240
Iteration 30/1000 | Loss: 0.00002240
Iteration 31/1000 | Loss: 0.00002240
Iteration 32/1000 | Loss: 0.00002240
Iteration 33/1000 | Loss: 0.00002240
Iteration 34/1000 | Loss: 0.00002239
Iteration 35/1000 | Loss: 0.00002239
Iteration 36/1000 | Loss: 0.00002239
Iteration 37/1000 | Loss: 0.00002239
Iteration 38/1000 | Loss: 0.00002239
Iteration 39/1000 | Loss: 0.00002239
Iteration 40/1000 | Loss: 0.00002239
Iteration 41/1000 | Loss: 0.00002238
Iteration 42/1000 | Loss: 0.00002238
Iteration 43/1000 | Loss: 0.00002238
Iteration 44/1000 | Loss: 0.00002238
Iteration 45/1000 | Loss: 0.00002237
Iteration 46/1000 | Loss: 0.00002237
Iteration 47/1000 | Loss: 0.00002237
Iteration 48/1000 | Loss: 0.00002236
Iteration 49/1000 | Loss: 0.00002236
Iteration 50/1000 | Loss: 0.00002235
Iteration 51/1000 | Loss: 0.00002234
Iteration 52/1000 | Loss: 0.00002234
Iteration 53/1000 | Loss: 0.00002234
Iteration 54/1000 | Loss: 0.00002234
Iteration 55/1000 | Loss: 0.00002234
Iteration 56/1000 | Loss: 0.00002233
Iteration 57/1000 | Loss: 0.00002232
Iteration 58/1000 | Loss: 0.00002231
Iteration 59/1000 | Loss: 0.00002231
Iteration 60/1000 | Loss: 0.00002231
Iteration 61/1000 | Loss: 0.00002231
Iteration 62/1000 | Loss: 0.00002231
Iteration 63/1000 | Loss: 0.00002231
Iteration 64/1000 | Loss: 0.00002231
Iteration 65/1000 | Loss: 0.00002231
Iteration 66/1000 | Loss: 0.00002231
Iteration 67/1000 | Loss: 0.00002230
Iteration 68/1000 | Loss: 0.00002230
Iteration 69/1000 | Loss: 0.00002230
Iteration 70/1000 | Loss: 0.00002229
Iteration 71/1000 | Loss: 0.00002229
Iteration 72/1000 | Loss: 0.00002228
Iteration 73/1000 | Loss: 0.00002228
Iteration 74/1000 | Loss: 0.00002228
Iteration 75/1000 | Loss: 0.00002228
Iteration 76/1000 | Loss: 0.00002228
Iteration 77/1000 | Loss: 0.00002228
Iteration 78/1000 | Loss: 0.00002228
Iteration 79/1000 | Loss: 0.00002228
Iteration 80/1000 | Loss: 0.00002227
Iteration 81/1000 | Loss: 0.00002227
Iteration 82/1000 | Loss: 0.00002227
Iteration 83/1000 | Loss: 0.00002227
Iteration 84/1000 | Loss: 0.00002227
Iteration 85/1000 | Loss: 0.00002227
Iteration 86/1000 | Loss: 0.00002227
Iteration 87/1000 | Loss: 0.00002227
Iteration 88/1000 | Loss: 0.00002227
Iteration 89/1000 | Loss: 0.00002227
Iteration 90/1000 | Loss: 0.00002227
Iteration 91/1000 | Loss: 0.00002227
Iteration 92/1000 | Loss: 0.00002227
Iteration 93/1000 | Loss: 0.00002227
Iteration 94/1000 | Loss: 0.00002227
Iteration 95/1000 | Loss: 0.00002227
Iteration 96/1000 | Loss: 0.00002227
Iteration 97/1000 | Loss: 0.00002227
Iteration 98/1000 | Loss: 0.00002227
Iteration 99/1000 | Loss: 0.00002227
Iteration 100/1000 | Loss: 0.00002227
Iteration 101/1000 | Loss: 0.00002227
Iteration 102/1000 | Loss: 0.00002227
Iteration 103/1000 | Loss: 0.00002227
Iteration 104/1000 | Loss: 0.00002227
Iteration 105/1000 | Loss: 0.00002227
Iteration 106/1000 | Loss: 0.00002227
Iteration 107/1000 | Loss: 0.00002227
Iteration 108/1000 | Loss: 0.00002227
Iteration 109/1000 | Loss: 0.00002227
Iteration 110/1000 | Loss: 0.00002227
Iteration 111/1000 | Loss: 0.00002227
Iteration 112/1000 | Loss: 0.00002227
Iteration 113/1000 | Loss: 0.00002227
Iteration 114/1000 | Loss: 0.00002227
Iteration 115/1000 | Loss: 0.00002227
Iteration 116/1000 | Loss: 0.00002227
Iteration 117/1000 | Loss: 0.00002227
Iteration 118/1000 | Loss: 0.00002227
Iteration 119/1000 | Loss: 0.00002227
Iteration 120/1000 | Loss: 0.00002227
Iteration 121/1000 | Loss: 0.00002227
Iteration 122/1000 | Loss: 0.00002227
Iteration 123/1000 | Loss: 0.00002227
Iteration 124/1000 | Loss: 0.00002227
Iteration 125/1000 | Loss: 0.00002227
Iteration 126/1000 | Loss: 0.00002227
Iteration 127/1000 | Loss: 0.00002227
Iteration 128/1000 | Loss: 0.00002227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.2266114683588967e-05, 2.2266114683588967e-05, 2.2266114683588967e-05, 2.2266114683588967e-05, 2.2266114683588967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2266114683588967e-05

Optimization complete. Final v2v error: 4.008847236633301 mm

Highest mean error: 4.6477437019348145 mm for frame 0

Lowest mean error: 3.8770411014556885 mm for frame 94

Saving results

Total time: 31.051761388778687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415828
Iteration 2/25 | Loss: 0.00092775
Iteration 3/25 | Loss: 0.00070124
Iteration 4/25 | Loss: 0.00067278
Iteration 5/25 | Loss: 0.00066640
Iteration 6/25 | Loss: 0.00066399
Iteration 7/25 | Loss: 0.00066351
Iteration 8/25 | Loss: 0.00066351
Iteration 9/25 | Loss: 0.00066351
Iteration 10/25 | Loss: 0.00066351
Iteration 11/25 | Loss: 0.00066351
Iteration 12/25 | Loss: 0.00066351
Iteration 13/25 | Loss: 0.00066351
Iteration 14/25 | Loss: 0.00066351
Iteration 15/25 | Loss: 0.00066351
Iteration 16/25 | Loss: 0.00066351
Iteration 17/25 | Loss: 0.00066351
Iteration 18/25 | Loss: 0.00066351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006635099998675287, 0.0006635099998675287, 0.0006635099998675287, 0.0006635099998675287, 0.0006635099998675287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006635099998675287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.25727034
Iteration 2/25 | Loss: 0.00033307
Iteration 3/25 | Loss: 0.00033305
Iteration 4/25 | Loss: 0.00033305
Iteration 5/25 | Loss: 0.00033305
Iteration 6/25 | Loss: 0.00033305
Iteration 7/25 | Loss: 0.00033305
Iteration 8/25 | Loss: 0.00033305
Iteration 9/25 | Loss: 0.00033305
Iteration 10/25 | Loss: 0.00033305
Iteration 11/25 | Loss: 0.00033305
Iteration 12/25 | Loss: 0.00033305
Iteration 13/25 | Loss: 0.00033305
Iteration 14/25 | Loss: 0.00033305
Iteration 15/25 | Loss: 0.00033305
Iteration 16/25 | Loss: 0.00033305
Iteration 17/25 | Loss: 0.00033305
Iteration 18/25 | Loss: 0.00033305
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003330493054818362, 0.0003330493054818362, 0.0003330493054818362, 0.0003330493054818362, 0.0003330493054818362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003330493054818362

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033305
Iteration 2/1000 | Loss: 0.00002753
Iteration 3/1000 | Loss: 0.00002115
Iteration 4/1000 | Loss: 0.00002005
Iteration 5/1000 | Loss: 0.00001906
Iteration 6/1000 | Loss: 0.00001847
Iteration 7/1000 | Loss: 0.00001800
Iteration 8/1000 | Loss: 0.00001771
Iteration 9/1000 | Loss: 0.00001761
Iteration 10/1000 | Loss: 0.00001759
Iteration 11/1000 | Loss: 0.00001754
Iteration 12/1000 | Loss: 0.00001752
Iteration 13/1000 | Loss: 0.00001752
Iteration 14/1000 | Loss: 0.00001751
Iteration 15/1000 | Loss: 0.00001738
Iteration 16/1000 | Loss: 0.00001737
Iteration 17/1000 | Loss: 0.00001735
Iteration 18/1000 | Loss: 0.00001733
Iteration 19/1000 | Loss: 0.00001732
Iteration 20/1000 | Loss: 0.00001732
Iteration 21/1000 | Loss: 0.00001730
Iteration 22/1000 | Loss: 0.00001730
Iteration 23/1000 | Loss: 0.00001729
Iteration 24/1000 | Loss: 0.00001729
Iteration 25/1000 | Loss: 0.00001728
Iteration 26/1000 | Loss: 0.00001728
Iteration 27/1000 | Loss: 0.00001728
Iteration 28/1000 | Loss: 0.00001727
Iteration 29/1000 | Loss: 0.00001726
Iteration 30/1000 | Loss: 0.00001724
Iteration 31/1000 | Loss: 0.00001724
Iteration 32/1000 | Loss: 0.00001723
Iteration 33/1000 | Loss: 0.00001723
Iteration 34/1000 | Loss: 0.00001723
Iteration 35/1000 | Loss: 0.00001722
Iteration 36/1000 | Loss: 0.00001722
Iteration 37/1000 | Loss: 0.00001722
Iteration 38/1000 | Loss: 0.00001721
Iteration 39/1000 | Loss: 0.00001721
Iteration 40/1000 | Loss: 0.00001721
Iteration 41/1000 | Loss: 0.00001721
Iteration 42/1000 | Loss: 0.00001720
Iteration 43/1000 | Loss: 0.00001720
Iteration 44/1000 | Loss: 0.00001720
Iteration 45/1000 | Loss: 0.00001719
Iteration 46/1000 | Loss: 0.00001719
Iteration 47/1000 | Loss: 0.00001719
Iteration 48/1000 | Loss: 0.00001719
Iteration 49/1000 | Loss: 0.00001718
Iteration 50/1000 | Loss: 0.00001718
Iteration 51/1000 | Loss: 0.00001718
Iteration 52/1000 | Loss: 0.00001718
Iteration 53/1000 | Loss: 0.00001718
Iteration 54/1000 | Loss: 0.00001718
Iteration 55/1000 | Loss: 0.00001717
Iteration 56/1000 | Loss: 0.00001717
Iteration 57/1000 | Loss: 0.00001716
Iteration 58/1000 | Loss: 0.00001716
Iteration 59/1000 | Loss: 0.00001716
Iteration 60/1000 | Loss: 0.00001716
Iteration 61/1000 | Loss: 0.00001716
Iteration 62/1000 | Loss: 0.00001715
Iteration 63/1000 | Loss: 0.00001715
Iteration 64/1000 | Loss: 0.00001715
Iteration 65/1000 | Loss: 0.00001715
Iteration 66/1000 | Loss: 0.00001714
Iteration 67/1000 | Loss: 0.00001714
Iteration 68/1000 | Loss: 0.00001714
Iteration 69/1000 | Loss: 0.00001714
Iteration 70/1000 | Loss: 0.00001714
Iteration 71/1000 | Loss: 0.00001713
Iteration 72/1000 | Loss: 0.00001713
Iteration 73/1000 | Loss: 0.00001713
Iteration 74/1000 | Loss: 0.00001713
Iteration 75/1000 | Loss: 0.00001713
Iteration 76/1000 | Loss: 0.00001713
Iteration 77/1000 | Loss: 0.00001713
Iteration 78/1000 | Loss: 0.00001713
Iteration 79/1000 | Loss: 0.00001713
Iteration 80/1000 | Loss: 0.00001712
Iteration 81/1000 | Loss: 0.00001712
Iteration 82/1000 | Loss: 0.00001712
Iteration 83/1000 | Loss: 0.00001711
Iteration 84/1000 | Loss: 0.00001711
Iteration 85/1000 | Loss: 0.00001711
Iteration 86/1000 | Loss: 0.00001710
Iteration 87/1000 | Loss: 0.00001710
Iteration 88/1000 | Loss: 0.00001710
Iteration 89/1000 | Loss: 0.00001710
Iteration 90/1000 | Loss: 0.00001709
Iteration 91/1000 | Loss: 0.00001709
Iteration 92/1000 | Loss: 0.00001709
Iteration 93/1000 | Loss: 0.00001709
Iteration 94/1000 | Loss: 0.00001709
Iteration 95/1000 | Loss: 0.00001708
Iteration 96/1000 | Loss: 0.00001708
Iteration 97/1000 | Loss: 0.00001708
Iteration 98/1000 | Loss: 0.00001708
Iteration 99/1000 | Loss: 0.00001708
Iteration 100/1000 | Loss: 0.00001708
Iteration 101/1000 | Loss: 0.00001708
Iteration 102/1000 | Loss: 0.00001708
Iteration 103/1000 | Loss: 0.00001708
Iteration 104/1000 | Loss: 0.00001708
Iteration 105/1000 | Loss: 0.00001707
Iteration 106/1000 | Loss: 0.00001707
Iteration 107/1000 | Loss: 0.00001707
Iteration 108/1000 | Loss: 0.00001706
Iteration 109/1000 | Loss: 0.00001706
Iteration 110/1000 | Loss: 0.00001706
Iteration 111/1000 | Loss: 0.00001705
Iteration 112/1000 | Loss: 0.00001705
Iteration 113/1000 | Loss: 0.00001705
Iteration 114/1000 | Loss: 0.00001705
Iteration 115/1000 | Loss: 0.00001705
Iteration 116/1000 | Loss: 0.00001705
Iteration 117/1000 | Loss: 0.00001705
Iteration 118/1000 | Loss: 0.00001705
Iteration 119/1000 | Loss: 0.00001705
Iteration 120/1000 | Loss: 0.00001705
Iteration 121/1000 | Loss: 0.00001704
Iteration 122/1000 | Loss: 0.00001704
Iteration 123/1000 | Loss: 0.00001704
Iteration 124/1000 | Loss: 0.00001704
Iteration 125/1000 | Loss: 0.00001704
Iteration 126/1000 | Loss: 0.00001704
Iteration 127/1000 | Loss: 0.00001704
Iteration 128/1000 | Loss: 0.00001704
Iteration 129/1000 | Loss: 0.00001704
Iteration 130/1000 | Loss: 0.00001704
Iteration 131/1000 | Loss: 0.00001704
Iteration 132/1000 | Loss: 0.00001703
Iteration 133/1000 | Loss: 0.00001703
Iteration 134/1000 | Loss: 0.00001703
Iteration 135/1000 | Loss: 0.00001703
Iteration 136/1000 | Loss: 0.00001703
Iteration 137/1000 | Loss: 0.00001703
Iteration 138/1000 | Loss: 0.00001703
Iteration 139/1000 | Loss: 0.00001702
Iteration 140/1000 | Loss: 0.00001702
Iteration 141/1000 | Loss: 0.00001702
Iteration 142/1000 | Loss: 0.00001702
Iteration 143/1000 | Loss: 0.00001702
Iteration 144/1000 | Loss: 0.00001702
Iteration 145/1000 | Loss: 0.00001702
Iteration 146/1000 | Loss: 0.00001702
Iteration 147/1000 | Loss: 0.00001702
Iteration 148/1000 | Loss: 0.00001702
Iteration 149/1000 | Loss: 0.00001702
Iteration 150/1000 | Loss: 0.00001702
Iteration 151/1000 | Loss: 0.00001702
Iteration 152/1000 | Loss: 0.00001702
Iteration 153/1000 | Loss: 0.00001702
Iteration 154/1000 | Loss: 0.00001702
Iteration 155/1000 | Loss: 0.00001702
Iteration 156/1000 | Loss: 0.00001702
Iteration 157/1000 | Loss: 0.00001701
Iteration 158/1000 | Loss: 0.00001701
Iteration 159/1000 | Loss: 0.00001701
Iteration 160/1000 | Loss: 0.00001701
Iteration 161/1000 | Loss: 0.00001701
Iteration 162/1000 | Loss: 0.00001701
Iteration 163/1000 | Loss: 0.00001701
Iteration 164/1000 | Loss: 0.00001701
Iteration 165/1000 | Loss: 0.00001701
Iteration 166/1000 | Loss: 0.00001701
Iteration 167/1000 | Loss: 0.00001701
Iteration 168/1000 | Loss: 0.00001701
Iteration 169/1000 | Loss: 0.00001701
Iteration 170/1000 | Loss: 0.00001701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.7009078874252737e-05, 1.7009078874252737e-05, 1.7009078874252737e-05, 1.7009078874252737e-05, 1.7009078874252737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7009078874252737e-05

Optimization complete. Final v2v error: 3.510591983795166 mm

Highest mean error: 3.9317626953125 mm for frame 72

Lowest mean error: 3.277806520462036 mm for frame 116

Saving results

Total time: 34.834522008895874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074395
Iteration 2/25 | Loss: 0.00309468
Iteration 3/25 | Loss: 0.00222002
Iteration 4/25 | Loss: 0.00164378
Iteration 5/25 | Loss: 0.00180838
Iteration 6/25 | Loss: 0.00207823
Iteration 7/25 | Loss: 0.00156174
Iteration 8/25 | Loss: 0.00168289
Iteration 9/25 | Loss: 0.00172280
Iteration 10/25 | Loss: 0.00159121
Iteration 11/25 | Loss: 0.00142061
Iteration 12/25 | Loss: 0.00134420
Iteration 13/25 | Loss: 0.00127639
Iteration 14/25 | Loss: 0.00120885
Iteration 15/25 | Loss: 0.00117127
Iteration 16/25 | Loss: 0.00115533
Iteration 17/25 | Loss: 0.00114377
Iteration 18/25 | Loss: 0.00113114
Iteration 19/25 | Loss: 0.00109249
Iteration 20/25 | Loss: 0.00106509
Iteration 21/25 | Loss: 0.00106422
Iteration 22/25 | Loss: 0.00105394
Iteration 23/25 | Loss: 0.00104932
Iteration 24/25 | Loss: 0.00104555
Iteration 25/25 | Loss: 0.00103678

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69021106
Iteration 2/25 | Loss: 0.00305790
Iteration 3/25 | Loss: 0.00301710
Iteration 4/25 | Loss: 0.00301709
Iteration 5/25 | Loss: 0.00301709
Iteration 6/25 | Loss: 0.00301709
Iteration 7/25 | Loss: 0.00301709
Iteration 8/25 | Loss: 0.00301709
Iteration 9/25 | Loss: 0.00301709
Iteration 10/25 | Loss: 0.00301709
Iteration 11/25 | Loss: 0.00301709
Iteration 12/25 | Loss: 0.00301709
Iteration 13/25 | Loss: 0.00301709
Iteration 14/25 | Loss: 0.00301709
Iteration 15/25 | Loss: 0.00301709
Iteration 16/25 | Loss: 0.00301709
Iteration 17/25 | Loss: 0.00301709
Iteration 18/25 | Loss: 0.00301709
Iteration 19/25 | Loss: 0.00301709
Iteration 20/25 | Loss: 0.00301709
Iteration 21/25 | Loss: 0.00301709
Iteration 22/25 | Loss: 0.00301709
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0030170923564583063, 0.0030170923564583063, 0.0030170923564583063, 0.0030170923564583063, 0.0030170923564583063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030170923564583063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00301709
Iteration 2/1000 | Loss: 0.00223333
Iteration 3/1000 | Loss: 0.00215505
Iteration 4/1000 | Loss: 0.00207330
Iteration 5/1000 | Loss: 0.00222967
Iteration 6/1000 | Loss: 0.00182815
Iteration 7/1000 | Loss: 0.00170624
Iteration 8/1000 | Loss: 0.00165680
Iteration 9/1000 | Loss: 0.00154401
Iteration 10/1000 | Loss: 0.00137230
Iteration 11/1000 | Loss: 0.00192446
Iteration 12/1000 | Loss: 0.00146027
Iteration 13/1000 | Loss: 0.00142440
Iteration 14/1000 | Loss: 0.00154310
Iteration 15/1000 | Loss: 0.00064652
Iteration 16/1000 | Loss: 0.00084480
Iteration 17/1000 | Loss: 0.00099268
Iteration 18/1000 | Loss: 0.00074833
Iteration 19/1000 | Loss: 0.00133593
Iteration 20/1000 | Loss: 0.00168731
Iteration 21/1000 | Loss: 0.00143505
Iteration 22/1000 | Loss: 0.00134099
Iteration 23/1000 | Loss: 0.00124190
Iteration 24/1000 | Loss: 0.00144750
Iteration 25/1000 | Loss: 0.00146888
Iteration 26/1000 | Loss: 0.00095070
Iteration 27/1000 | Loss: 0.00154687
Iteration 28/1000 | Loss: 0.00280036
Iteration 29/1000 | Loss: 0.00129647
Iteration 30/1000 | Loss: 0.00119331
Iteration 31/1000 | Loss: 0.00147467
Iteration 32/1000 | Loss: 0.00105776
Iteration 33/1000 | Loss: 0.00158403
Iteration 34/1000 | Loss: 0.00145746
Iteration 35/1000 | Loss: 0.00090748
Iteration 36/1000 | Loss: 0.00115432
Iteration 37/1000 | Loss: 0.00155766
Iteration 38/1000 | Loss: 0.00111938
Iteration 39/1000 | Loss: 0.00091913
Iteration 40/1000 | Loss: 0.00112056
Iteration 41/1000 | Loss: 0.00096871
Iteration 42/1000 | Loss: 0.00086853
Iteration 43/1000 | Loss: 0.00210255
Iteration 44/1000 | Loss: 0.00142860
Iteration 45/1000 | Loss: 0.00056587
Iteration 46/1000 | Loss: 0.00122855
Iteration 47/1000 | Loss: 0.00072516
Iteration 48/1000 | Loss: 0.00063566
Iteration 49/1000 | Loss: 0.00106825
Iteration 50/1000 | Loss: 0.00106665
Iteration 51/1000 | Loss: 0.00139857
Iteration 52/1000 | Loss: 0.00158188
Iteration 53/1000 | Loss: 0.00168108
Iteration 54/1000 | Loss: 0.00124277
Iteration 55/1000 | Loss: 0.00104367
Iteration 56/1000 | Loss: 0.00107883
Iteration 57/1000 | Loss: 0.00114673
Iteration 58/1000 | Loss: 0.00073354
Iteration 59/1000 | Loss: 0.00112134
Iteration 60/1000 | Loss: 0.00133007
Iteration 61/1000 | Loss: 0.00078884
Iteration 62/1000 | Loss: 0.00071648
Iteration 63/1000 | Loss: 0.00077675
Iteration 64/1000 | Loss: 0.00160409
Iteration 65/1000 | Loss: 0.00160653
Iteration 66/1000 | Loss: 0.00246035
Iteration 67/1000 | Loss: 0.00173716
Iteration 68/1000 | Loss: 0.00080524
Iteration 69/1000 | Loss: 0.00137090
Iteration 70/1000 | Loss: 0.00071151
Iteration 71/1000 | Loss: 0.00078068
Iteration 72/1000 | Loss: 0.00114527
Iteration 73/1000 | Loss: 0.00085637
Iteration 74/1000 | Loss: 0.00040348
Iteration 75/1000 | Loss: 0.00062171
Iteration 76/1000 | Loss: 0.00142321
Iteration 77/1000 | Loss: 0.00100756
Iteration 78/1000 | Loss: 0.00070024
Iteration 79/1000 | Loss: 0.00052654
Iteration 80/1000 | Loss: 0.00055086
Iteration 81/1000 | Loss: 0.00138545
Iteration 82/1000 | Loss: 0.00054129
Iteration 83/1000 | Loss: 0.00047611
Iteration 84/1000 | Loss: 0.00044092
Iteration 85/1000 | Loss: 0.00101959
Iteration 86/1000 | Loss: 0.00099818
Iteration 87/1000 | Loss: 0.00117261
Iteration 88/1000 | Loss: 0.00124737
Iteration 89/1000 | Loss: 0.00104706
Iteration 90/1000 | Loss: 0.00060662
Iteration 91/1000 | Loss: 0.00072089
Iteration 92/1000 | Loss: 0.00049322
Iteration 93/1000 | Loss: 0.00051764
Iteration 94/1000 | Loss: 0.00102023
Iteration 95/1000 | Loss: 0.00141377
Iteration 96/1000 | Loss: 0.00106518
Iteration 97/1000 | Loss: 0.00058598
Iteration 98/1000 | Loss: 0.00080318
Iteration 99/1000 | Loss: 0.00063308
Iteration 100/1000 | Loss: 0.00038004
Iteration 101/1000 | Loss: 0.00047617
Iteration 102/1000 | Loss: 0.00043230
Iteration 103/1000 | Loss: 0.00032946
Iteration 104/1000 | Loss: 0.00034138
Iteration 105/1000 | Loss: 0.00021357
Iteration 106/1000 | Loss: 0.00047614
Iteration 107/1000 | Loss: 0.00058727
Iteration 108/1000 | Loss: 0.00050904
Iteration 109/1000 | Loss: 0.00083984
Iteration 110/1000 | Loss: 0.00084651
Iteration 111/1000 | Loss: 0.00035484
Iteration 112/1000 | Loss: 0.00048793
Iteration 113/1000 | Loss: 0.00075090
Iteration 114/1000 | Loss: 0.00050173
Iteration 115/1000 | Loss: 0.00050104
Iteration 116/1000 | Loss: 0.00017571
Iteration 117/1000 | Loss: 0.00051806
Iteration 118/1000 | Loss: 0.00088549
Iteration 119/1000 | Loss: 0.00048428
Iteration 120/1000 | Loss: 0.00072542
Iteration 121/1000 | Loss: 0.00081888
Iteration 122/1000 | Loss: 0.00135174
Iteration 123/1000 | Loss: 0.00074366
Iteration 124/1000 | Loss: 0.00035248
Iteration 125/1000 | Loss: 0.00053442
Iteration 126/1000 | Loss: 0.00045665
Iteration 127/1000 | Loss: 0.00026703
Iteration 128/1000 | Loss: 0.00017912
Iteration 129/1000 | Loss: 0.00076081
Iteration 130/1000 | Loss: 0.00062679
Iteration 131/1000 | Loss: 0.00076474
Iteration 132/1000 | Loss: 0.00044493
Iteration 133/1000 | Loss: 0.00027300
Iteration 134/1000 | Loss: 0.00016717
Iteration 135/1000 | Loss: 0.00059339
Iteration 136/1000 | Loss: 0.00039048
Iteration 137/1000 | Loss: 0.00049451
Iteration 138/1000 | Loss: 0.00128437
Iteration 139/1000 | Loss: 0.00032130
Iteration 140/1000 | Loss: 0.00033181
Iteration 141/1000 | Loss: 0.00111939
Iteration 142/1000 | Loss: 0.00147975
Iteration 143/1000 | Loss: 0.00113508
Iteration 144/1000 | Loss: 0.00187774
Iteration 145/1000 | Loss: 0.00166090
Iteration 146/1000 | Loss: 0.00105945
Iteration 147/1000 | Loss: 0.00129427
Iteration 148/1000 | Loss: 0.00047955
Iteration 149/1000 | Loss: 0.00058169
Iteration 150/1000 | Loss: 0.00037552
Iteration 151/1000 | Loss: 0.00055112
Iteration 152/1000 | Loss: 0.00050524
Iteration 153/1000 | Loss: 0.00097649
Iteration 154/1000 | Loss: 0.00066717
Iteration 155/1000 | Loss: 0.00049963
Iteration 156/1000 | Loss: 0.00042828
Iteration 157/1000 | Loss: 0.00089414
Iteration 158/1000 | Loss: 0.00058064
Iteration 159/1000 | Loss: 0.00028397
Iteration 160/1000 | Loss: 0.00035170
Iteration 161/1000 | Loss: 0.00052668
Iteration 162/1000 | Loss: 0.00093528
Iteration 163/1000 | Loss: 0.00057359
Iteration 164/1000 | Loss: 0.00021915
Iteration 165/1000 | Loss: 0.00019344
Iteration 166/1000 | Loss: 0.00021172
Iteration 167/1000 | Loss: 0.00030918
Iteration 168/1000 | Loss: 0.00047992
Iteration 169/1000 | Loss: 0.00029215
Iteration 170/1000 | Loss: 0.00015369
Iteration 171/1000 | Loss: 0.00038510
Iteration 172/1000 | Loss: 0.00033649
Iteration 173/1000 | Loss: 0.00034777
Iteration 174/1000 | Loss: 0.00085816
Iteration 175/1000 | Loss: 0.00058666
Iteration 176/1000 | Loss: 0.00029754
Iteration 177/1000 | Loss: 0.00031144
Iteration 178/1000 | Loss: 0.00032153
Iteration 179/1000 | Loss: 0.00012763
Iteration 180/1000 | Loss: 0.00032168
Iteration 181/1000 | Loss: 0.00067911
Iteration 182/1000 | Loss: 0.00075375
Iteration 183/1000 | Loss: 0.00057931
Iteration 184/1000 | Loss: 0.00057874
Iteration 185/1000 | Loss: 0.00045651
Iteration 186/1000 | Loss: 0.00053918
Iteration 187/1000 | Loss: 0.00040068
Iteration 188/1000 | Loss: 0.00024482
Iteration 189/1000 | Loss: 0.00014570
Iteration 190/1000 | Loss: 0.00028500
Iteration 191/1000 | Loss: 0.00041139
Iteration 192/1000 | Loss: 0.00047530
Iteration 193/1000 | Loss: 0.00020727
Iteration 194/1000 | Loss: 0.00045987
Iteration 195/1000 | Loss: 0.00027413
Iteration 196/1000 | Loss: 0.00033682
Iteration 197/1000 | Loss: 0.00027454
Iteration 198/1000 | Loss: 0.00044426
Iteration 199/1000 | Loss: 0.00180154
Iteration 200/1000 | Loss: 0.00095993
Iteration 201/1000 | Loss: 0.00039813
Iteration 202/1000 | Loss: 0.00079981
Iteration 203/1000 | Loss: 0.00030771
Iteration 204/1000 | Loss: 0.00028039
Iteration 205/1000 | Loss: 0.00019804
Iteration 206/1000 | Loss: 0.00019913
Iteration 207/1000 | Loss: 0.00012389
Iteration 208/1000 | Loss: 0.00012121
Iteration 209/1000 | Loss: 0.00083861
Iteration 210/1000 | Loss: 0.00036194
Iteration 211/1000 | Loss: 0.00026730
Iteration 212/1000 | Loss: 0.00011585
Iteration 213/1000 | Loss: 0.00031304
Iteration 214/1000 | Loss: 0.00016838
Iteration 215/1000 | Loss: 0.00023960
Iteration 216/1000 | Loss: 0.00013089
Iteration 217/1000 | Loss: 0.00010506
Iteration 218/1000 | Loss: 0.00012705
Iteration 219/1000 | Loss: 0.00041800
Iteration 220/1000 | Loss: 0.00017625
Iteration 221/1000 | Loss: 0.00058857
Iteration 222/1000 | Loss: 0.00056577
Iteration 223/1000 | Loss: 0.00033619
Iteration 224/1000 | Loss: 0.00065364
Iteration 225/1000 | Loss: 0.00050803
Iteration 226/1000 | Loss: 0.00042259
Iteration 227/1000 | Loss: 0.00051904
Iteration 228/1000 | Loss: 0.00052865
Iteration 229/1000 | Loss: 0.00097514
Iteration 230/1000 | Loss: 0.00015827
Iteration 231/1000 | Loss: 0.00012592
Iteration 232/1000 | Loss: 0.00044147
Iteration 233/1000 | Loss: 0.00017326
Iteration 234/1000 | Loss: 0.00049254
Iteration 235/1000 | Loss: 0.00058584
Iteration 236/1000 | Loss: 0.00015775
Iteration 237/1000 | Loss: 0.00014630
Iteration 238/1000 | Loss: 0.00016774
Iteration 239/1000 | Loss: 0.00017537
Iteration 240/1000 | Loss: 0.00021823
Iteration 241/1000 | Loss: 0.00013645
Iteration 242/1000 | Loss: 0.00012704
Iteration 243/1000 | Loss: 0.00013513
Iteration 244/1000 | Loss: 0.00012304
Iteration 245/1000 | Loss: 0.00053214
Iteration 246/1000 | Loss: 0.00026224
Iteration 247/1000 | Loss: 0.00089455
Iteration 248/1000 | Loss: 0.00063498
Iteration 249/1000 | Loss: 0.00038949
Iteration 250/1000 | Loss: 0.00013706
Iteration 251/1000 | Loss: 0.00020766
Iteration 252/1000 | Loss: 0.00010700
Iteration 253/1000 | Loss: 0.00008665
Iteration 254/1000 | Loss: 0.00024819
Iteration 255/1000 | Loss: 0.00014701
Iteration 256/1000 | Loss: 0.00008833
Iteration 257/1000 | Loss: 0.00010345
Iteration 258/1000 | Loss: 0.00008828
Iteration 259/1000 | Loss: 0.00008762
Iteration 260/1000 | Loss: 0.00007647
Iteration 261/1000 | Loss: 0.00007547
Iteration 262/1000 | Loss: 0.00007586
Iteration 263/1000 | Loss: 0.00025621
Iteration 264/1000 | Loss: 0.00008710
Iteration 265/1000 | Loss: 0.00009035
Iteration 266/1000 | Loss: 0.00008740
Iteration 267/1000 | Loss: 0.00009277
Iteration 268/1000 | Loss: 0.00006793
Iteration 269/1000 | Loss: 0.00022363
Iteration 270/1000 | Loss: 0.00010460
Iteration 271/1000 | Loss: 0.00022458
Iteration 272/1000 | Loss: 0.00010357
Iteration 273/1000 | Loss: 0.00006205
Iteration 274/1000 | Loss: 0.00005819
Iteration 275/1000 | Loss: 0.00005645
Iteration 276/1000 | Loss: 0.00005514
Iteration 277/1000 | Loss: 0.00021161
Iteration 278/1000 | Loss: 0.00021555
Iteration 279/1000 | Loss: 0.00053223
Iteration 280/1000 | Loss: 0.00007554
Iteration 281/1000 | Loss: 0.00006585
Iteration 282/1000 | Loss: 0.00068937
Iteration 283/1000 | Loss: 0.00007526
Iteration 284/1000 | Loss: 0.00006563
Iteration 285/1000 | Loss: 0.00051791
Iteration 286/1000 | Loss: 0.00055725
Iteration 287/1000 | Loss: 0.00008961
Iteration 288/1000 | Loss: 0.00007521
Iteration 289/1000 | Loss: 0.00024018
Iteration 290/1000 | Loss: 0.00037076
Iteration 291/1000 | Loss: 0.00022208
Iteration 292/1000 | Loss: 0.00023351
Iteration 293/1000 | Loss: 0.00068839
Iteration 294/1000 | Loss: 0.00076851
Iteration 295/1000 | Loss: 0.00081488
Iteration 296/1000 | Loss: 0.00216970
Iteration 297/1000 | Loss: 0.00062160
Iteration 298/1000 | Loss: 0.00013198
Iteration 299/1000 | Loss: 0.00007342
Iteration 300/1000 | Loss: 0.00021535
Iteration 301/1000 | Loss: 0.00085266
Iteration 302/1000 | Loss: 0.00008609
Iteration 303/1000 | Loss: 0.00007342
Iteration 304/1000 | Loss: 0.00036967
Iteration 305/1000 | Loss: 0.00030651
Iteration 306/1000 | Loss: 0.00007558
Iteration 307/1000 | Loss: 0.00023676
Iteration 308/1000 | Loss: 0.00023025
Iteration 309/1000 | Loss: 0.00007485
Iteration 310/1000 | Loss: 0.00024778
Iteration 311/1000 | Loss: 0.00036017
Iteration 312/1000 | Loss: 0.00106502
Iteration 313/1000 | Loss: 0.00050056
Iteration 314/1000 | Loss: 0.00015972
Iteration 315/1000 | Loss: 0.00007591
Iteration 316/1000 | Loss: 0.00006848
Iteration 317/1000 | Loss: 0.00038640
Iteration 318/1000 | Loss: 0.00024352
Iteration 319/1000 | Loss: 0.00018133
Iteration 320/1000 | Loss: 0.00033813
Iteration 321/1000 | Loss: 0.00028028
Iteration 322/1000 | Loss: 0.00028112
Iteration 323/1000 | Loss: 0.00053551
Iteration 324/1000 | Loss: 0.00067104
Iteration 325/1000 | Loss: 0.00038437
Iteration 326/1000 | Loss: 0.00021582
Iteration 327/1000 | Loss: 0.00016996
Iteration 328/1000 | Loss: 0.00010313
Iteration 329/1000 | Loss: 0.00020961
Iteration 330/1000 | Loss: 0.00008086
Iteration 331/1000 | Loss: 0.00006655
Iteration 332/1000 | Loss: 0.00006051
Iteration 333/1000 | Loss: 0.00044316
Iteration 334/1000 | Loss: 0.00020613
Iteration 335/1000 | Loss: 0.00019432
Iteration 336/1000 | Loss: 0.00007141
Iteration 337/1000 | Loss: 0.00006397
Iteration 338/1000 | Loss: 0.00005859
Iteration 339/1000 | Loss: 0.00005567
Iteration 340/1000 | Loss: 0.00005360
Iteration 341/1000 | Loss: 0.00021210
Iteration 342/1000 | Loss: 0.00027504
Iteration 343/1000 | Loss: 0.00045948
Iteration 344/1000 | Loss: 0.00021521
Iteration 345/1000 | Loss: 0.00021225
Iteration 346/1000 | Loss: 0.00006317
Iteration 347/1000 | Loss: 0.00005848
Iteration 348/1000 | Loss: 0.00030737
Iteration 349/1000 | Loss: 0.00028784
Iteration 350/1000 | Loss: 0.00010783
Iteration 351/1000 | Loss: 0.00021297
Iteration 352/1000 | Loss: 0.00011881
Iteration 353/1000 | Loss: 0.00019819
Iteration 354/1000 | Loss: 0.00005524
Iteration 355/1000 | Loss: 0.00005318
Iteration 356/1000 | Loss: 0.00020566
Iteration 357/1000 | Loss: 0.00083728
Iteration 358/1000 | Loss: 0.00110219
Iteration 359/1000 | Loss: 0.00023138
Iteration 360/1000 | Loss: 0.00025081
Iteration 361/1000 | Loss: 0.00008079
Iteration 362/1000 | Loss: 0.00070576
Iteration 363/1000 | Loss: 0.00198612
Iteration 364/1000 | Loss: 0.00186539
Iteration 365/1000 | Loss: 0.00181885
Iteration 366/1000 | Loss: 0.00129742
Iteration 367/1000 | Loss: 0.00089694
Iteration 368/1000 | Loss: 0.00127063
Iteration 369/1000 | Loss: 0.00020877
Iteration 370/1000 | Loss: 0.00016136
Iteration 371/1000 | Loss: 0.00020516
Iteration 372/1000 | Loss: 0.00008837
Iteration 373/1000 | Loss: 0.00008447
Iteration 374/1000 | Loss: 0.00007551
Iteration 375/1000 | Loss: 0.00006989
Iteration 376/1000 | Loss: 0.00028754
Iteration 377/1000 | Loss: 0.00056768
Iteration 378/1000 | Loss: 0.00022789
Iteration 379/1000 | Loss: 0.00031307
Iteration 380/1000 | Loss: 0.00028825
Iteration 381/1000 | Loss: 0.00027103
Iteration 382/1000 | Loss: 0.00012861
Iteration 383/1000 | Loss: 0.00024265
Iteration 384/1000 | Loss: 0.00011928
Iteration 385/1000 | Loss: 0.00016555
Iteration 386/1000 | Loss: 0.00027588
Iteration 387/1000 | Loss: 0.00019669
Iteration 388/1000 | Loss: 0.00006169
Iteration 389/1000 | Loss: 0.00017607
Iteration 390/1000 | Loss: 0.00005923
Iteration 391/1000 | Loss: 0.00005590
Iteration 392/1000 | Loss: 0.00021523
Iteration 393/1000 | Loss: 0.00007427
Iteration 394/1000 | Loss: 0.00005675
Iteration 395/1000 | Loss: 0.00005311
Iteration 396/1000 | Loss: 0.00005037
Iteration 397/1000 | Loss: 0.00004883
Iteration 398/1000 | Loss: 0.00004754
Iteration 399/1000 | Loss: 0.00004619
Iteration 400/1000 | Loss: 0.00004503
Iteration 401/1000 | Loss: 0.00004418
Iteration 402/1000 | Loss: 0.00019455
Iteration 403/1000 | Loss: 0.00023880
Iteration 404/1000 | Loss: 0.00217893
Iteration 405/1000 | Loss: 0.00125955
Iteration 406/1000 | Loss: 0.00098610
Iteration 407/1000 | Loss: 0.00072258
Iteration 408/1000 | Loss: 0.00017843
Iteration 409/1000 | Loss: 0.00010115
Iteration 410/1000 | Loss: 0.00007587
Iteration 411/1000 | Loss: 0.00006719
Iteration 412/1000 | Loss: 0.00006377
Iteration 413/1000 | Loss: 0.00033450
Iteration 414/1000 | Loss: 0.00019418
Iteration 415/1000 | Loss: 0.00016235
Iteration 416/1000 | Loss: 0.00024136
Iteration 417/1000 | Loss: 0.00007342
Iteration 418/1000 | Loss: 0.00050951
Iteration 419/1000 | Loss: 0.00024873
Iteration 420/1000 | Loss: 0.00018928
Iteration 421/1000 | Loss: 0.00012647
Iteration 422/1000 | Loss: 0.00017435
Iteration 423/1000 | Loss: 0.00014637
Iteration 424/1000 | Loss: 0.00030820
Iteration 425/1000 | Loss: 0.00011043
Iteration 426/1000 | Loss: 0.00008147
Iteration 427/1000 | Loss: 0.00016636
Iteration 428/1000 | Loss: 0.00036345
Iteration 429/1000 | Loss: 0.00007973
Iteration 430/1000 | Loss: 0.00006547
Iteration 431/1000 | Loss: 0.00005836
Iteration 432/1000 | Loss: 0.00006327
Iteration 433/1000 | Loss: 0.00050679
Iteration 434/1000 | Loss: 0.00066340
Iteration 435/1000 | Loss: 0.00074456
Iteration 436/1000 | Loss: 0.00055522
Iteration 437/1000 | Loss: 0.00031834
Iteration 438/1000 | Loss: 0.00029127
Iteration 439/1000 | Loss: 0.00016120
Iteration 440/1000 | Loss: 0.00021288
Iteration 441/1000 | Loss: 0.00023314
Iteration 442/1000 | Loss: 0.00021758
Iteration 443/1000 | Loss: 0.00022157
Iteration 444/1000 | Loss: 0.00011684
Iteration 445/1000 | Loss: 0.00009493
Iteration 446/1000 | Loss: 0.00024645
Iteration 447/1000 | Loss: 0.00108994
Iteration 448/1000 | Loss: 0.00010538
Iteration 449/1000 | Loss: 0.00008940
Iteration 450/1000 | Loss: 0.00009677
Iteration 451/1000 | Loss: 0.00009270
Iteration 452/1000 | Loss: 0.00008826
Iteration 453/1000 | Loss: 0.00008446
Iteration 454/1000 | Loss: 0.00007480
Iteration 455/1000 | Loss: 0.00023002
Iteration 456/1000 | Loss: 0.00023066
Iteration 457/1000 | Loss: 0.00017809
Iteration 458/1000 | Loss: 0.00013452
Iteration 459/1000 | Loss: 0.00075584
Iteration 460/1000 | Loss: 0.00013132
Iteration 461/1000 | Loss: 0.00006308
Iteration 462/1000 | Loss: 0.00005658
Iteration 463/1000 | Loss: 0.00005354
Iteration 464/1000 | Loss: 0.00005151
Iteration 465/1000 | Loss: 0.00004938
Iteration 466/1000 | Loss: 0.00005271
Iteration 467/1000 | Loss: 0.00021160
Iteration 468/1000 | Loss: 0.00005164
Iteration 469/1000 | Loss: 0.00004845
Iteration 470/1000 | Loss: 0.00004675
Iteration 471/1000 | Loss: 0.00019282
Iteration 472/1000 | Loss: 0.00004958
Iteration 473/1000 | Loss: 0.00004660
Iteration 474/1000 | Loss: 0.00004416
Iteration 475/1000 | Loss: 0.00004320
Iteration 476/1000 | Loss: 0.00020851
Iteration 477/1000 | Loss: 0.00004735
Iteration 478/1000 | Loss: 0.00004443
Iteration 479/1000 | Loss: 0.00004228
Iteration 480/1000 | Loss: 0.00004150
Iteration 481/1000 | Loss: 0.00049848
Iteration 482/1000 | Loss: 0.00171189
Iteration 483/1000 | Loss: 0.00267869
Iteration 484/1000 | Loss: 0.00096488
Iteration 485/1000 | Loss: 0.00030719
Iteration 486/1000 | Loss: 0.00091482
Iteration 487/1000 | Loss: 0.00057813
Iteration 488/1000 | Loss: 0.00055420
Iteration 489/1000 | Loss: 0.00017429
Iteration 490/1000 | Loss: 0.00034091
Iteration 491/1000 | Loss: 0.00046315
Iteration 492/1000 | Loss: 0.00031410
Iteration 493/1000 | Loss: 0.00023020
Iteration 494/1000 | Loss: 0.00006840
Iteration 495/1000 | Loss: 0.00005529
Iteration 496/1000 | Loss: 0.00004815
Iteration 497/1000 | Loss: 0.00004165
Iteration 498/1000 | Loss: 0.00016816
Iteration 499/1000 | Loss: 0.00005768
Iteration 500/1000 | Loss: 0.00005228
Iteration 501/1000 | Loss: 0.00004801
Iteration 502/1000 | Loss: 0.00026611
Iteration 503/1000 | Loss: 0.00024463
Iteration 504/1000 | Loss: 0.00028326
Iteration 505/1000 | Loss: 0.00029218
Iteration 506/1000 | Loss: 0.00004487
Iteration 507/1000 | Loss: 0.00003320
Iteration 508/1000 | Loss: 0.00002977
Iteration 509/1000 | Loss: 0.00002683
Iteration 510/1000 | Loss: 0.00002479
Iteration 511/1000 | Loss: 0.00002351
Iteration 512/1000 | Loss: 0.00002171
Iteration 513/1000 | Loss: 0.00002095
Iteration 514/1000 | Loss: 0.00002062
Iteration 515/1000 | Loss: 0.00002031
Iteration 516/1000 | Loss: 0.00001997
Iteration 517/1000 | Loss: 0.00001979
Iteration 518/1000 | Loss: 0.00001974
Iteration 519/1000 | Loss: 0.00001973
Iteration 520/1000 | Loss: 0.00001972
Iteration 521/1000 | Loss: 0.00001972
Iteration 522/1000 | Loss: 0.00001972
Iteration 523/1000 | Loss: 0.00001971
Iteration 524/1000 | Loss: 0.00001971
Iteration 525/1000 | Loss: 0.00001971
Iteration 526/1000 | Loss: 0.00001970
Iteration 527/1000 | Loss: 0.00001970
Iteration 528/1000 | Loss: 0.00001969
Iteration 529/1000 | Loss: 0.00001969
Iteration 530/1000 | Loss: 0.00001968
Iteration 531/1000 | Loss: 0.00001966
Iteration 532/1000 | Loss: 0.00001965
Iteration 533/1000 | Loss: 0.00001964
Iteration 534/1000 | Loss: 0.00001963
Iteration 535/1000 | Loss: 0.00001956
Iteration 536/1000 | Loss: 0.00001956
Iteration 537/1000 | Loss: 0.00001955
Iteration 538/1000 | Loss: 0.00001953
Iteration 539/1000 | Loss: 0.00001950
Iteration 540/1000 | Loss: 0.00001949
Iteration 541/1000 | Loss: 0.00001949
Iteration 542/1000 | Loss: 0.00001949
Iteration 543/1000 | Loss: 0.00001949
Iteration 544/1000 | Loss: 0.00001948
Iteration 545/1000 | Loss: 0.00001948
Iteration 546/1000 | Loss: 0.00001948
Iteration 547/1000 | Loss: 0.00001947
Iteration 548/1000 | Loss: 0.00001946
Iteration 549/1000 | Loss: 0.00001946
Iteration 550/1000 | Loss: 0.00001945
Iteration 551/1000 | Loss: 0.00001945
Iteration 552/1000 | Loss: 0.00001944
Iteration 553/1000 | Loss: 0.00001943
Iteration 554/1000 | Loss: 0.00001943
Iteration 555/1000 | Loss: 0.00001943
Iteration 556/1000 | Loss: 0.00001943
Iteration 557/1000 | Loss: 0.00001942
Iteration 558/1000 | Loss: 0.00001942
Iteration 559/1000 | Loss: 0.00001942
Iteration 560/1000 | Loss: 0.00001941
Iteration 561/1000 | Loss: 0.00001941
Iteration 562/1000 | Loss: 0.00001941
Iteration 563/1000 | Loss: 0.00001941
Iteration 564/1000 | Loss: 0.00001941
Iteration 565/1000 | Loss: 0.00001941
Iteration 566/1000 | Loss: 0.00001940
Iteration 567/1000 | Loss: 0.00001940
Iteration 568/1000 | Loss: 0.00001940
Iteration 569/1000 | Loss: 0.00001940
Iteration 570/1000 | Loss: 0.00001940
Iteration 571/1000 | Loss: 0.00001940
Iteration 572/1000 | Loss: 0.00001940
Iteration 573/1000 | Loss: 0.00001940
Iteration 574/1000 | Loss: 0.00001940
Iteration 575/1000 | Loss: 0.00001940
Iteration 576/1000 | Loss: 0.00001940
Iteration 577/1000 | Loss: 0.00001939
Iteration 578/1000 | Loss: 0.00001939
Iteration 579/1000 | Loss: 0.00001939
Iteration 580/1000 | Loss: 0.00001939
Iteration 581/1000 | Loss: 0.00001939
Iteration 582/1000 | Loss: 0.00001939
Iteration 583/1000 | Loss: 0.00001939
Iteration 584/1000 | Loss: 0.00001939
Iteration 585/1000 | Loss: 0.00001939
Iteration 586/1000 | Loss: 0.00001939
Iteration 587/1000 | Loss: 0.00001939
Iteration 588/1000 | Loss: 0.00001938
Iteration 589/1000 | Loss: 0.00001938
Iteration 590/1000 | Loss: 0.00001938
Iteration 591/1000 | Loss: 0.00001938
Iteration 592/1000 | Loss: 0.00001938
Iteration 593/1000 | Loss: 0.00001938
Iteration 594/1000 | Loss: 0.00001938
Iteration 595/1000 | Loss: 0.00001938
Iteration 596/1000 | Loss: 0.00001938
Iteration 597/1000 | Loss: 0.00001938
Iteration 598/1000 | Loss: 0.00001938
Iteration 599/1000 | Loss: 0.00001938
Iteration 600/1000 | Loss: 0.00001938
Iteration 601/1000 | Loss: 0.00001938
Iteration 602/1000 | Loss: 0.00001938
Iteration 603/1000 | Loss: 0.00001938
Iteration 604/1000 | Loss: 0.00001938
Iteration 605/1000 | Loss: 0.00001938
Iteration 606/1000 | Loss: 0.00001938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 606. Stopping optimization.
Last 5 losses: [1.938025889103301e-05, 1.938025889103301e-05, 1.938025889103301e-05, 1.938025889103301e-05, 1.938025889103301e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.938025889103301e-05

Optimization complete. Final v2v error: 3.6959712505340576 mm

Highest mean error: 5.958055019378662 mm for frame 238

Lowest mean error: 3.4539387226104736 mm for frame 233

Saving results

Total time: 872.3478016853333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408795
Iteration 2/25 | Loss: 0.00092541
Iteration 3/25 | Loss: 0.00074188
Iteration 4/25 | Loss: 0.00070693
Iteration 5/25 | Loss: 0.00069388
Iteration 6/25 | Loss: 0.00069087
Iteration 7/25 | Loss: 0.00068998
Iteration 8/25 | Loss: 0.00068972
Iteration 9/25 | Loss: 0.00068972
Iteration 10/25 | Loss: 0.00068972
Iteration 11/25 | Loss: 0.00068972
Iteration 12/25 | Loss: 0.00068972
Iteration 13/25 | Loss: 0.00068972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006897176499478519, 0.0006897176499478519, 0.0006897176499478519, 0.0006897176499478519, 0.0006897176499478519]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006897176499478519

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48201096
Iteration 2/25 | Loss: 0.00033112
Iteration 3/25 | Loss: 0.00033112
Iteration 4/25 | Loss: 0.00033112
Iteration 5/25 | Loss: 0.00033112
Iteration 6/25 | Loss: 0.00033112
Iteration 7/25 | Loss: 0.00033112
Iteration 8/25 | Loss: 0.00033112
Iteration 9/25 | Loss: 0.00033112
Iteration 10/25 | Loss: 0.00033112
Iteration 11/25 | Loss: 0.00033112
Iteration 12/25 | Loss: 0.00033112
Iteration 13/25 | Loss: 0.00033112
Iteration 14/25 | Loss: 0.00033112
Iteration 15/25 | Loss: 0.00033112
Iteration 16/25 | Loss: 0.00033112
Iteration 17/25 | Loss: 0.00033112
Iteration 18/25 | Loss: 0.00033112
Iteration 19/25 | Loss: 0.00033112
Iteration 20/25 | Loss: 0.00033112
Iteration 21/25 | Loss: 0.00033112
Iteration 22/25 | Loss: 0.00033112
Iteration 23/25 | Loss: 0.00033112
Iteration 24/25 | Loss: 0.00033112
Iteration 25/25 | Loss: 0.00033112
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0003311180043965578, 0.0003311180043965578, 0.0003311180043965578, 0.0003311180043965578, 0.0003311180043965578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003311180043965578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033112
Iteration 2/1000 | Loss: 0.00003779
Iteration 3/1000 | Loss: 0.00002808
Iteration 4/1000 | Loss: 0.00002557
Iteration 5/1000 | Loss: 0.00002422
Iteration 6/1000 | Loss: 0.00002327
Iteration 7/1000 | Loss: 0.00002264
Iteration 8/1000 | Loss: 0.00002217
Iteration 9/1000 | Loss: 0.00002183
Iteration 10/1000 | Loss: 0.00002159
Iteration 11/1000 | Loss: 0.00002142
Iteration 12/1000 | Loss: 0.00002136
Iteration 13/1000 | Loss: 0.00002134
Iteration 14/1000 | Loss: 0.00002131
Iteration 15/1000 | Loss: 0.00002128
Iteration 16/1000 | Loss: 0.00002112
Iteration 17/1000 | Loss: 0.00002101
Iteration 18/1000 | Loss: 0.00002098
Iteration 19/1000 | Loss: 0.00002096
Iteration 20/1000 | Loss: 0.00002095
Iteration 21/1000 | Loss: 0.00002091
Iteration 22/1000 | Loss: 0.00002088
Iteration 23/1000 | Loss: 0.00002088
Iteration 24/1000 | Loss: 0.00002086
Iteration 25/1000 | Loss: 0.00002086
Iteration 26/1000 | Loss: 0.00002086
Iteration 27/1000 | Loss: 0.00002085
Iteration 28/1000 | Loss: 0.00002085
Iteration 29/1000 | Loss: 0.00002084
Iteration 30/1000 | Loss: 0.00002084
Iteration 31/1000 | Loss: 0.00002084
Iteration 32/1000 | Loss: 0.00002084
Iteration 33/1000 | Loss: 0.00002083
Iteration 34/1000 | Loss: 0.00002083
Iteration 35/1000 | Loss: 0.00002083
Iteration 36/1000 | Loss: 0.00002082
Iteration 37/1000 | Loss: 0.00002082
Iteration 38/1000 | Loss: 0.00002082
Iteration 39/1000 | Loss: 0.00002082
Iteration 40/1000 | Loss: 0.00002082
Iteration 41/1000 | Loss: 0.00002082
Iteration 42/1000 | Loss: 0.00002082
Iteration 43/1000 | Loss: 0.00002082
Iteration 44/1000 | Loss: 0.00002082
Iteration 45/1000 | Loss: 0.00002081
Iteration 46/1000 | Loss: 0.00002081
Iteration 47/1000 | Loss: 0.00002081
Iteration 48/1000 | Loss: 0.00002081
Iteration 49/1000 | Loss: 0.00002080
Iteration 50/1000 | Loss: 0.00002080
Iteration 51/1000 | Loss: 0.00002079
Iteration 52/1000 | Loss: 0.00002079
Iteration 53/1000 | Loss: 0.00002079
Iteration 54/1000 | Loss: 0.00002079
Iteration 55/1000 | Loss: 0.00002078
Iteration 56/1000 | Loss: 0.00002078
Iteration 57/1000 | Loss: 0.00002078
Iteration 58/1000 | Loss: 0.00002078
Iteration 59/1000 | Loss: 0.00002078
Iteration 60/1000 | Loss: 0.00002078
Iteration 61/1000 | Loss: 0.00002078
Iteration 62/1000 | Loss: 0.00002078
Iteration 63/1000 | Loss: 0.00002077
Iteration 64/1000 | Loss: 0.00002077
Iteration 65/1000 | Loss: 0.00002077
Iteration 66/1000 | Loss: 0.00002077
Iteration 67/1000 | Loss: 0.00002076
Iteration 68/1000 | Loss: 0.00002076
Iteration 69/1000 | Loss: 0.00002076
Iteration 70/1000 | Loss: 0.00002076
Iteration 71/1000 | Loss: 0.00002076
Iteration 72/1000 | Loss: 0.00002076
Iteration 73/1000 | Loss: 0.00002075
Iteration 74/1000 | Loss: 0.00002075
Iteration 75/1000 | Loss: 0.00002075
Iteration 76/1000 | Loss: 0.00002075
Iteration 77/1000 | Loss: 0.00002074
Iteration 78/1000 | Loss: 0.00002074
Iteration 79/1000 | Loss: 0.00002074
Iteration 80/1000 | Loss: 0.00002074
Iteration 81/1000 | Loss: 0.00002074
Iteration 82/1000 | Loss: 0.00002074
Iteration 83/1000 | Loss: 0.00002073
Iteration 84/1000 | Loss: 0.00002073
Iteration 85/1000 | Loss: 0.00002073
Iteration 86/1000 | Loss: 0.00002073
Iteration 87/1000 | Loss: 0.00002073
Iteration 88/1000 | Loss: 0.00002072
Iteration 89/1000 | Loss: 0.00002072
Iteration 90/1000 | Loss: 0.00002072
Iteration 91/1000 | Loss: 0.00002071
Iteration 92/1000 | Loss: 0.00002071
Iteration 93/1000 | Loss: 0.00002071
Iteration 94/1000 | Loss: 0.00002071
Iteration 95/1000 | Loss: 0.00002071
Iteration 96/1000 | Loss: 0.00002071
Iteration 97/1000 | Loss: 0.00002070
Iteration 98/1000 | Loss: 0.00002070
Iteration 99/1000 | Loss: 0.00002070
Iteration 100/1000 | Loss: 0.00002069
Iteration 101/1000 | Loss: 0.00002069
Iteration 102/1000 | Loss: 0.00002069
Iteration 103/1000 | Loss: 0.00002068
Iteration 104/1000 | Loss: 0.00002068
Iteration 105/1000 | Loss: 0.00002068
Iteration 106/1000 | Loss: 0.00002068
Iteration 107/1000 | Loss: 0.00002068
Iteration 108/1000 | Loss: 0.00002068
Iteration 109/1000 | Loss: 0.00002068
Iteration 110/1000 | Loss: 0.00002067
Iteration 111/1000 | Loss: 0.00002067
Iteration 112/1000 | Loss: 0.00002067
Iteration 113/1000 | Loss: 0.00002066
Iteration 114/1000 | Loss: 0.00002066
Iteration 115/1000 | Loss: 0.00002066
Iteration 116/1000 | Loss: 0.00002066
Iteration 117/1000 | Loss: 0.00002066
Iteration 118/1000 | Loss: 0.00002066
Iteration 119/1000 | Loss: 0.00002065
Iteration 120/1000 | Loss: 0.00002065
Iteration 121/1000 | Loss: 0.00002065
Iteration 122/1000 | Loss: 0.00002065
Iteration 123/1000 | Loss: 0.00002065
Iteration 124/1000 | Loss: 0.00002064
Iteration 125/1000 | Loss: 0.00002064
Iteration 126/1000 | Loss: 0.00002064
Iteration 127/1000 | Loss: 0.00002064
Iteration 128/1000 | Loss: 0.00002064
Iteration 129/1000 | Loss: 0.00002064
Iteration 130/1000 | Loss: 0.00002064
Iteration 131/1000 | Loss: 0.00002568
Iteration 132/1000 | Loss: 0.00002568
Iteration 133/1000 | Loss: 0.00002453
Iteration 134/1000 | Loss: 0.00002155
Iteration 135/1000 | Loss: 0.00002084
Iteration 136/1000 | Loss: 0.00002050
Iteration 137/1000 | Loss: 0.00002033
Iteration 138/1000 | Loss: 0.00002029
Iteration 139/1000 | Loss: 0.00002029
Iteration 140/1000 | Loss: 0.00002027
Iteration 141/1000 | Loss: 0.00002025
Iteration 142/1000 | Loss: 0.00002024
Iteration 143/1000 | Loss: 0.00002023
Iteration 144/1000 | Loss: 0.00002022
Iteration 145/1000 | Loss: 0.00002022
Iteration 146/1000 | Loss: 0.00002021
Iteration 147/1000 | Loss: 0.00002020
Iteration 148/1000 | Loss: 0.00002011
Iteration 149/1000 | Loss: 0.00002005
Iteration 150/1000 | Loss: 0.00001994
Iteration 151/1000 | Loss: 0.00001989
Iteration 152/1000 | Loss: 0.00001985
Iteration 153/1000 | Loss: 0.00001983
Iteration 154/1000 | Loss: 0.00001982
Iteration 155/1000 | Loss: 0.00001977
Iteration 156/1000 | Loss: 0.00001977
Iteration 157/1000 | Loss: 0.00001976
Iteration 158/1000 | Loss: 0.00001976
Iteration 159/1000 | Loss: 0.00001976
Iteration 160/1000 | Loss: 0.00001975
Iteration 161/1000 | Loss: 0.00001975
Iteration 162/1000 | Loss: 0.00001975
Iteration 163/1000 | Loss: 0.00001974
Iteration 164/1000 | Loss: 0.00001974
Iteration 165/1000 | Loss: 0.00001974
Iteration 166/1000 | Loss: 0.00001974
Iteration 167/1000 | Loss: 0.00001974
Iteration 168/1000 | Loss: 0.00001974
Iteration 169/1000 | Loss: 0.00001974
Iteration 170/1000 | Loss: 0.00001974
Iteration 171/1000 | Loss: 0.00001974
Iteration 172/1000 | Loss: 0.00001974
Iteration 173/1000 | Loss: 0.00001973
Iteration 174/1000 | Loss: 0.00001972
Iteration 175/1000 | Loss: 0.00001971
Iteration 176/1000 | Loss: 0.00001971
Iteration 177/1000 | Loss: 0.00001970
Iteration 178/1000 | Loss: 0.00001969
Iteration 179/1000 | Loss: 0.00001969
Iteration 180/1000 | Loss: 0.00001968
Iteration 181/1000 | Loss: 0.00001968
Iteration 182/1000 | Loss: 0.00001968
Iteration 183/1000 | Loss: 0.00001968
Iteration 184/1000 | Loss: 0.00001967
Iteration 185/1000 | Loss: 0.00001967
Iteration 186/1000 | Loss: 0.00001966
Iteration 187/1000 | Loss: 0.00001966
Iteration 188/1000 | Loss: 0.00001966
Iteration 189/1000 | Loss: 0.00001966
Iteration 190/1000 | Loss: 0.00001966
Iteration 191/1000 | Loss: 0.00001965
Iteration 192/1000 | Loss: 0.00001965
Iteration 193/1000 | Loss: 0.00001965
Iteration 194/1000 | Loss: 0.00001964
Iteration 195/1000 | Loss: 0.00001964
Iteration 196/1000 | Loss: 0.00001964
Iteration 197/1000 | Loss: 0.00001964
Iteration 198/1000 | Loss: 0.00001964
Iteration 199/1000 | Loss: 0.00001963
Iteration 200/1000 | Loss: 0.00001963
Iteration 201/1000 | Loss: 0.00001963
Iteration 202/1000 | Loss: 0.00001963
Iteration 203/1000 | Loss: 0.00001962
Iteration 204/1000 | Loss: 0.00001962
Iteration 205/1000 | Loss: 0.00001962
Iteration 206/1000 | Loss: 0.00001962
Iteration 207/1000 | Loss: 0.00001962
Iteration 208/1000 | Loss: 0.00001962
Iteration 209/1000 | Loss: 0.00001962
Iteration 210/1000 | Loss: 0.00001962
Iteration 211/1000 | Loss: 0.00001962
Iteration 212/1000 | Loss: 0.00001962
Iteration 213/1000 | Loss: 0.00001962
Iteration 214/1000 | Loss: 0.00001962
Iteration 215/1000 | Loss: 0.00001962
Iteration 216/1000 | Loss: 0.00001962
Iteration 217/1000 | Loss: 0.00001962
Iteration 218/1000 | Loss: 0.00001962
Iteration 219/1000 | Loss: 0.00001961
Iteration 220/1000 | Loss: 0.00001961
Iteration 221/1000 | Loss: 0.00001961
Iteration 222/1000 | Loss: 0.00001960
Iteration 223/1000 | Loss: 0.00001960
Iteration 224/1000 | Loss: 0.00001960
Iteration 225/1000 | Loss: 0.00001960
Iteration 226/1000 | Loss: 0.00001960
Iteration 227/1000 | Loss: 0.00001959
Iteration 228/1000 | Loss: 0.00001959
Iteration 229/1000 | Loss: 0.00001959
Iteration 230/1000 | Loss: 0.00001959
Iteration 231/1000 | Loss: 0.00001959
Iteration 232/1000 | Loss: 0.00001958
Iteration 233/1000 | Loss: 0.00001958
Iteration 234/1000 | Loss: 0.00001958
Iteration 235/1000 | Loss: 0.00001958
Iteration 236/1000 | Loss: 0.00001958
Iteration 237/1000 | Loss: 0.00001957
Iteration 238/1000 | Loss: 0.00001957
Iteration 239/1000 | Loss: 0.00001957
Iteration 240/1000 | Loss: 0.00001957
Iteration 241/1000 | Loss: 0.00001957
Iteration 242/1000 | Loss: 0.00001957
Iteration 243/1000 | Loss: 0.00001957
Iteration 244/1000 | Loss: 0.00001957
Iteration 245/1000 | Loss: 0.00001957
Iteration 246/1000 | Loss: 0.00001957
Iteration 247/1000 | Loss: 0.00001957
Iteration 248/1000 | Loss: 0.00001957
Iteration 249/1000 | Loss: 0.00001957
Iteration 250/1000 | Loss: 0.00001957
Iteration 251/1000 | Loss: 0.00001957
Iteration 252/1000 | Loss: 0.00001957
Iteration 253/1000 | Loss: 0.00001957
Iteration 254/1000 | Loss: 0.00001957
Iteration 255/1000 | Loss: 0.00001957
Iteration 256/1000 | Loss: 0.00001957
Iteration 257/1000 | Loss: 0.00001957
Iteration 258/1000 | Loss: 0.00001957
Iteration 259/1000 | Loss: 0.00001957
Iteration 260/1000 | Loss: 0.00001957
Iteration 261/1000 | Loss: 0.00001957
Iteration 262/1000 | Loss: 0.00001957
Iteration 263/1000 | Loss: 0.00001957
Iteration 264/1000 | Loss: 0.00001957
Iteration 265/1000 | Loss: 0.00001957
Iteration 266/1000 | Loss: 0.00001957
Iteration 267/1000 | Loss: 0.00001957
Iteration 268/1000 | Loss: 0.00001957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [1.9572406017687172e-05, 1.9572406017687172e-05, 1.9572406017687172e-05, 1.9572406017687172e-05, 1.9572406017687172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9572406017687172e-05

Optimization complete. Final v2v error: 3.6292924880981445 mm

Highest mean error: 4.5795369148254395 mm for frame 169

Lowest mean error: 3.0774731636047363 mm for frame 172

Saving results

Total time: 61.21777820587158
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00618355
Iteration 2/25 | Loss: 0.00131930
Iteration 3/25 | Loss: 0.00103325
Iteration 4/25 | Loss: 0.00093853
Iteration 5/25 | Loss: 0.00092958
Iteration 6/25 | Loss: 0.00094225
Iteration 7/25 | Loss: 0.00090941
Iteration 8/25 | Loss: 0.00085880
Iteration 9/25 | Loss: 0.00082345
Iteration 10/25 | Loss: 0.00080686
Iteration 11/25 | Loss: 0.00080219
Iteration 12/25 | Loss: 0.00079913
Iteration 13/25 | Loss: 0.00079794
Iteration 14/25 | Loss: 0.00079741
Iteration 15/25 | Loss: 0.00079729
Iteration 16/25 | Loss: 0.00079727
Iteration 17/25 | Loss: 0.00079726
Iteration 18/25 | Loss: 0.00079726
Iteration 19/25 | Loss: 0.00079726
Iteration 20/25 | Loss: 0.00079726
Iteration 21/25 | Loss: 0.00079726
Iteration 22/25 | Loss: 0.00079725
Iteration 23/25 | Loss: 0.00079725
Iteration 24/25 | Loss: 0.00079725
Iteration 25/25 | Loss: 0.00079725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33207870
Iteration 2/25 | Loss: 0.00096430
Iteration 3/25 | Loss: 0.00096421
Iteration 4/25 | Loss: 0.00096421
Iteration 5/25 | Loss: 0.00096421
Iteration 6/25 | Loss: 0.00096421
Iteration 7/25 | Loss: 0.00096421
Iteration 8/25 | Loss: 0.00096421
Iteration 9/25 | Loss: 0.00096421
Iteration 10/25 | Loss: 0.00096421
Iteration 11/25 | Loss: 0.00096421
Iteration 12/25 | Loss: 0.00096421
Iteration 13/25 | Loss: 0.00096421
Iteration 14/25 | Loss: 0.00096421
Iteration 15/25 | Loss: 0.00096421
Iteration 16/25 | Loss: 0.00096421
Iteration 17/25 | Loss: 0.00096421
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009642084478400648, 0.0009642084478400648, 0.0009642084478400648, 0.0009642084478400648, 0.0009642084478400648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009642084478400648

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096421
Iteration 2/1000 | Loss: 0.00019333
Iteration 3/1000 | Loss: 0.00007251
Iteration 4/1000 | Loss: 0.00004603
Iteration 5/1000 | Loss: 0.00003459
Iteration 6/1000 | Loss: 0.00003084
Iteration 7/1000 | Loss: 0.00002954
Iteration 8/1000 | Loss: 0.00002835
Iteration 9/1000 | Loss: 0.00002730
Iteration 10/1000 | Loss: 0.00002682
Iteration 11/1000 | Loss: 0.00002638
Iteration 12/1000 | Loss: 0.00002610
Iteration 13/1000 | Loss: 0.00002583
Iteration 14/1000 | Loss: 0.00002560
Iteration 15/1000 | Loss: 0.00002541
Iteration 16/1000 | Loss: 0.00002523
Iteration 17/1000 | Loss: 0.00002518
Iteration 18/1000 | Loss: 0.00002510
Iteration 19/1000 | Loss: 0.00002505
Iteration 20/1000 | Loss: 0.00002503
Iteration 21/1000 | Loss: 0.00002502
Iteration 22/1000 | Loss: 0.00002497
Iteration 23/1000 | Loss: 0.00002496
Iteration 24/1000 | Loss: 0.00002494
Iteration 25/1000 | Loss: 0.00002494
Iteration 26/1000 | Loss: 0.00002493
Iteration 27/1000 | Loss: 0.00002492
Iteration 28/1000 | Loss: 0.00002492
Iteration 29/1000 | Loss: 0.00002492
Iteration 30/1000 | Loss: 0.00002491
Iteration 31/1000 | Loss: 0.00002491
Iteration 32/1000 | Loss: 0.00002491
Iteration 33/1000 | Loss: 0.00002490
Iteration 34/1000 | Loss: 0.00002490
Iteration 35/1000 | Loss: 0.00002489
Iteration 36/1000 | Loss: 0.00002489
Iteration 37/1000 | Loss: 0.00002487
Iteration 38/1000 | Loss: 0.00002487
Iteration 39/1000 | Loss: 0.00002486
Iteration 40/1000 | Loss: 0.00002486
Iteration 41/1000 | Loss: 0.00002485
Iteration 42/1000 | Loss: 0.00002485
Iteration 43/1000 | Loss: 0.00002485
Iteration 44/1000 | Loss: 0.00002484
Iteration 45/1000 | Loss: 0.00002483
Iteration 46/1000 | Loss: 0.00002483
Iteration 47/1000 | Loss: 0.00002483
Iteration 48/1000 | Loss: 0.00002483
Iteration 49/1000 | Loss: 0.00002483
Iteration 50/1000 | Loss: 0.00002483
Iteration 51/1000 | Loss: 0.00002483
Iteration 52/1000 | Loss: 0.00002481
Iteration 53/1000 | Loss: 0.00002480
Iteration 54/1000 | Loss: 0.00002478
Iteration 55/1000 | Loss: 0.00002476
Iteration 56/1000 | Loss: 0.00002476
Iteration 57/1000 | Loss: 0.00002476
Iteration 58/1000 | Loss: 0.00002476
Iteration 59/1000 | Loss: 0.00002476
Iteration 60/1000 | Loss: 0.00002475
Iteration 61/1000 | Loss: 0.00002475
Iteration 62/1000 | Loss: 0.00002474
Iteration 63/1000 | Loss: 0.00002473
Iteration 64/1000 | Loss: 0.00002473
Iteration 65/1000 | Loss: 0.00002472
Iteration 66/1000 | Loss: 0.00002472
Iteration 67/1000 | Loss: 0.00002472
Iteration 68/1000 | Loss: 0.00002471
Iteration 69/1000 | Loss: 0.00002471
Iteration 70/1000 | Loss: 0.00002470
Iteration 71/1000 | Loss: 0.00002470
Iteration 72/1000 | Loss: 0.00002470
Iteration 73/1000 | Loss: 0.00002470
Iteration 74/1000 | Loss: 0.00002469
Iteration 75/1000 | Loss: 0.00002469
Iteration 76/1000 | Loss: 0.00002469
Iteration 77/1000 | Loss: 0.00002469
Iteration 78/1000 | Loss: 0.00002468
Iteration 79/1000 | Loss: 0.00002468
Iteration 80/1000 | Loss: 0.00002468
Iteration 81/1000 | Loss: 0.00002468
Iteration 82/1000 | Loss: 0.00002467
Iteration 83/1000 | Loss: 0.00002467
Iteration 84/1000 | Loss: 0.00002467
Iteration 85/1000 | Loss: 0.00002467
Iteration 86/1000 | Loss: 0.00002467
Iteration 87/1000 | Loss: 0.00002467
Iteration 88/1000 | Loss: 0.00002467
Iteration 89/1000 | Loss: 0.00002466
Iteration 90/1000 | Loss: 0.00002466
Iteration 91/1000 | Loss: 0.00002466
Iteration 92/1000 | Loss: 0.00002466
Iteration 93/1000 | Loss: 0.00002466
Iteration 94/1000 | Loss: 0.00002466
Iteration 95/1000 | Loss: 0.00002466
Iteration 96/1000 | Loss: 0.00002466
Iteration 97/1000 | Loss: 0.00002466
Iteration 98/1000 | Loss: 0.00002466
Iteration 99/1000 | Loss: 0.00002466
Iteration 100/1000 | Loss: 0.00002465
Iteration 101/1000 | Loss: 0.00002465
Iteration 102/1000 | Loss: 0.00002465
Iteration 103/1000 | Loss: 0.00002465
Iteration 104/1000 | Loss: 0.00002465
Iteration 105/1000 | Loss: 0.00002465
Iteration 106/1000 | Loss: 0.00002465
Iteration 107/1000 | Loss: 0.00002464
Iteration 108/1000 | Loss: 0.00002464
Iteration 109/1000 | Loss: 0.00002464
Iteration 110/1000 | Loss: 0.00002464
Iteration 111/1000 | Loss: 0.00002464
Iteration 112/1000 | Loss: 0.00002464
Iteration 113/1000 | Loss: 0.00002464
Iteration 114/1000 | Loss: 0.00002464
Iteration 115/1000 | Loss: 0.00002464
Iteration 116/1000 | Loss: 0.00002464
Iteration 117/1000 | Loss: 0.00002464
Iteration 118/1000 | Loss: 0.00002463
Iteration 119/1000 | Loss: 0.00002463
Iteration 120/1000 | Loss: 0.00002463
Iteration 121/1000 | Loss: 0.00002463
Iteration 122/1000 | Loss: 0.00002463
Iteration 123/1000 | Loss: 0.00002463
Iteration 124/1000 | Loss: 0.00002462
Iteration 125/1000 | Loss: 0.00002462
Iteration 126/1000 | Loss: 0.00002462
Iteration 127/1000 | Loss: 0.00002462
Iteration 128/1000 | Loss: 0.00002462
Iteration 129/1000 | Loss: 0.00002462
Iteration 130/1000 | Loss: 0.00002462
Iteration 131/1000 | Loss: 0.00002462
Iteration 132/1000 | Loss: 0.00002462
Iteration 133/1000 | Loss: 0.00002462
Iteration 134/1000 | Loss: 0.00002462
Iteration 135/1000 | Loss: 0.00002461
Iteration 136/1000 | Loss: 0.00002461
Iteration 137/1000 | Loss: 0.00002461
Iteration 138/1000 | Loss: 0.00002461
Iteration 139/1000 | Loss: 0.00002461
Iteration 140/1000 | Loss: 0.00002461
Iteration 141/1000 | Loss: 0.00002461
Iteration 142/1000 | Loss: 0.00002461
Iteration 143/1000 | Loss: 0.00002461
Iteration 144/1000 | Loss: 0.00002461
Iteration 145/1000 | Loss: 0.00002461
Iteration 146/1000 | Loss: 0.00002461
Iteration 147/1000 | Loss: 0.00002461
Iteration 148/1000 | Loss: 0.00002461
Iteration 149/1000 | Loss: 0.00002461
Iteration 150/1000 | Loss: 0.00002461
Iteration 151/1000 | Loss: 0.00002461
Iteration 152/1000 | Loss: 0.00002461
Iteration 153/1000 | Loss: 0.00002461
Iteration 154/1000 | Loss: 0.00002461
Iteration 155/1000 | Loss: 0.00002461
Iteration 156/1000 | Loss: 0.00002461
Iteration 157/1000 | Loss: 0.00002461
Iteration 158/1000 | Loss: 0.00002461
Iteration 159/1000 | Loss: 0.00002461
Iteration 160/1000 | Loss: 0.00002461
Iteration 161/1000 | Loss: 0.00002461
Iteration 162/1000 | Loss: 0.00002461
Iteration 163/1000 | Loss: 0.00002461
Iteration 164/1000 | Loss: 0.00002461
Iteration 165/1000 | Loss: 0.00002461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [2.4606062652310356e-05, 2.4606062652310356e-05, 2.4606062652310356e-05, 2.4606062652310356e-05, 2.4606062652310356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4606062652310356e-05

Optimization complete. Final v2v error: 3.8732566833496094 mm

Highest mean error: 5.7670793533325195 mm for frame 61

Lowest mean error: 3.0396533012390137 mm for frame 153

Saving results

Total time: 63.32093334197998
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423131
Iteration 2/25 | Loss: 0.00099381
Iteration 3/25 | Loss: 0.00067826
Iteration 4/25 | Loss: 0.00064789
Iteration 5/25 | Loss: 0.00064242
Iteration 6/25 | Loss: 0.00064055
Iteration 7/25 | Loss: 0.00064025
Iteration 8/25 | Loss: 0.00064025
Iteration 9/25 | Loss: 0.00064025
Iteration 10/25 | Loss: 0.00064025
Iteration 11/25 | Loss: 0.00064025
Iteration 12/25 | Loss: 0.00064025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006402474828064442, 0.0006402474828064442, 0.0006402474828064442, 0.0006402474828064442, 0.0006402474828064442]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006402474828064442

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.89730239
Iteration 2/25 | Loss: 0.00030693
Iteration 3/25 | Loss: 0.00030691
Iteration 4/25 | Loss: 0.00030691
Iteration 5/25 | Loss: 0.00030691
Iteration 6/25 | Loss: 0.00030691
Iteration 7/25 | Loss: 0.00030691
Iteration 8/25 | Loss: 0.00030691
Iteration 9/25 | Loss: 0.00030691
Iteration 10/25 | Loss: 0.00030691
Iteration 11/25 | Loss: 0.00030691
Iteration 12/25 | Loss: 0.00030691
Iteration 13/25 | Loss: 0.00030691
Iteration 14/25 | Loss: 0.00030691
Iteration 15/25 | Loss: 0.00030691
Iteration 16/25 | Loss: 0.00030691
Iteration 17/25 | Loss: 0.00030691
Iteration 18/25 | Loss: 0.00030691
Iteration 19/25 | Loss: 0.00030691
Iteration 20/25 | Loss: 0.00030691
Iteration 21/25 | Loss: 0.00030691
Iteration 22/25 | Loss: 0.00030691
Iteration 23/25 | Loss: 0.00030691
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00030691095162183046, 0.00030691095162183046, 0.00030691095162183046, 0.00030691095162183046, 0.00030691095162183046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030691095162183046

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030691
Iteration 2/1000 | Loss: 0.00002184
Iteration 3/1000 | Loss: 0.00001702
Iteration 4/1000 | Loss: 0.00001603
Iteration 5/1000 | Loss: 0.00001502
Iteration 6/1000 | Loss: 0.00001448
Iteration 7/1000 | Loss: 0.00001414
Iteration 8/1000 | Loss: 0.00001393
Iteration 9/1000 | Loss: 0.00001374
Iteration 10/1000 | Loss: 0.00001371
Iteration 11/1000 | Loss: 0.00001362
Iteration 12/1000 | Loss: 0.00001359
Iteration 13/1000 | Loss: 0.00001359
Iteration 14/1000 | Loss: 0.00001358
Iteration 15/1000 | Loss: 0.00001357
Iteration 16/1000 | Loss: 0.00001356
Iteration 17/1000 | Loss: 0.00001356
Iteration 18/1000 | Loss: 0.00001356
Iteration 19/1000 | Loss: 0.00001356
Iteration 20/1000 | Loss: 0.00001356
Iteration 21/1000 | Loss: 0.00001355
Iteration 22/1000 | Loss: 0.00001355
Iteration 23/1000 | Loss: 0.00001355
Iteration 24/1000 | Loss: 0.00001354
Iteration 25/1000 | Loss: 0.00001354
Iteration 26/1000 | Loss: 0.00001354
Iteration 27/1000 | Loss: 0.00001354
Iteration 28/1000 | Loss: 0.00001354
Iteration 29/1000 | Loss: 0.00001353
Iteration 30/1000 | Loss: 0.00001353
Iteration 31/1000 | Loss: 0.00001353
Iteration 32/1000 | Loss: 0.00001353
Iteration 33/1000 | Loss: 0.00001353
Iteration 34/1000 | Loss: 0.00001353
Iteration 35/1000 | Loss: 0.00001353
Iteration 36/1000 | Loss: 0.00001352
Iteration 37/1000 | Loss: 0.00001352
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001351
Iteration 40/1000 | Loss: 0.00001351
Iteration 41/1000 | Loss: 0.00001351
Iteration 42/1000 | Loss: 0.00001350
Iteration 43/1000 | Loss: 0.00001350
Iteration 44/1000 | Loss: 0.00001350
Iteration 45/1000 | Loss: 0.00001349
Iteration 46/1000 | Loss: 0.00001349
Iteration 47/1000 | Loss: 0.00001349
Iteration 48/1000 | Loss: 0.00001348
Iteration 49/1000 | Loss: 0.00001348
Iteration 50/1000 | Loss: 0.00001347
Iteration 51/1000 | Loss: 0.00001347
Iteration 52/1000 | Loss: 0.00001347
Iteration 53/1000 | Loss: 0.00001346
Iteration 54/1000 | Loss: 0.00001346
Iteration 55/1000 | Loss: 0.00001346
Iteration 56/1000 | Loss: 0.00001345
Iteration 57/1000 | Loss: 0.00001345
Iteration 58/1000 | Loss: 0.00001345
Iteration 59/1000 | Loss: 0.00001345
Iteration 60/1000 | Loss: 0.00001344
Iteration 61/1000 | Loss: 0.00001344
Iteration 62/1000 | Loss: 0.00001344
Iteration 63/1000 | Loss: 0.00001344
Iteration 64/1000 | Loss: 0.00001344
Iteration 65/1000 | Loss: 0.00001344
Iteration 66/1000 | Loss: 0.00001344
Iteration 67/1000 | Loss: 0.00001344
Iteration 68/1000 | Loss: 0.00001344
Iteration 69/1000 | Loss: 0.00001344
Iteration 70/1000 | Loss: 0.00001344
Iteration 71/1000 | Loss: 0.00001344
Iteration 72/1000 | Loss: 0.00001343
Iteration 73/1000 | Loss: 0.00001343
Iteration 74/1000 | Loss: 0.00001342
Iteration 75/1000 | Loss: 0.00001342
Iteration 76/1000 | Loss: 0.00001342
Iteration 77/1000 | Loss: 0.00001342
Iteration 78/1000 | Loss: 0.00001341
Iteration 79/1000 | Loss: 0.00001341
Iteration 80/1000 | Loss: 0.00001341
Iteration 81/1000 | Loss: 0.00001341
Iteration 82/1000 | Loss: 0.00001341
Iteration 83/1000 | Loss: 0.00001341
Iteration 84/1000 | Loss: 0.00001341
Iteration 85/1000 | Loss: 0.00001341
Iteration 86/1000 | Loss: 0.00001340
Iteration 87/1000 | Loss: 0.00001340
Iteration 88/1000 | Loss: 0.00001340
Iteration 89/1000 | Loss: 0.00001340
Iteration 90/1000 | Loss: 0.00001340
Iteration 91/1000 | Loss: 0.00001340
Iteration 92/1000 | Loss: 0.00001340
Iteration 93/1000 | Loss: 0.00001340
Iteration 94/1000 | Loss: 0.00001340
Iteration 95/1000 | Loss: 0.00001340
Iteration 96/1000 | Loss: 0.00001340
Iteration 97/1000 | Loss: 0.00001340
Iteration 98/1000 | Loss: 0.00001340
Iteration 99/1000 | Loss: 0.00001340
Iteration 100/1000 | Loss: 0.00001340
Iteration 101/1000 | Loss: 0.00001340
Iteration 102/1000 | Loss: 0.00001340
Iteration 103/1000 | Loss: 0.00001340
Iteration 104/1000 | Loss: 0.00001340
Iteration 105/1000 | Loss: 0.00001340
Iteration 106/1000 | Loss: 0.00001340
Iteration 107/1000 | Loss: 0.00001340
Iteration 108/1000 | Loss: 0.00001340
Iteration 109/1000 | Loss: 0.00001340
Iteration 110/1000 | Loss: 0.00001340
Iteration 111/1000 | Loss: 0.00001340
Iteration 112/1000 | Loss: 0.00001340
Iteration 113/1000 | Loss: 0.00001340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.3396615031524561e-05, 1.3396615031524561e-05, 1.3396615031524561e-05, 1.3396615031524561e-05, 1.3396615031524561e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3396615031524561e-05

Optimization complete. Final v2v error: 3.1319117546081543 mm

Highest mean error: 3.5336475372314453 mm for frame 87

Lowest mean error: 2.926203727722168 mm for frame 53

Saving results

Total time: 30.900702476501465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054123
Iteration 2/25 | Loss: 0.00237816
Iteration 3/25 | Loss: 0.00174804
Iteration 4/25 | Loss: 0.00156746
Iteration 5/25 | Loss: 0.00170658
Iteration 6/25 | Loss: 0.00144366
Iteration 7/25 | Loss: 0.00129659
Iteration 8/25 | Loss: 0.00125870
Iteration 9/25 | Loss: 0.00117830
Iteration 10/25 | Loss: 0.00102499
Iteration 11/25 | Loss: 0.00094733
Iteration 12/25 | Loss: 0.00087981
Iteration 13/25 | Loss: 0.00084561
Iteration 14/25 | Loss: 0.00082452
Iteration 15/25 | Loss: 0.00081408
Iteration 16/25 | Loss: 0.00080458
Iteration 17/25 | Loss: 0.00079755
Iteration 18/25 | Loss: 0.00079460
Iteration 19/25 | Loss: 0.00079346
Iteration 20/25 | Loss: 0.00079306
Iteration 21/25 | Loss: 0.00079270
Iteration 22/25 | Loss: 0.00079237
Iteration 23/25 | Loss: 0.00079288
Iteration 24/25 | Loss: 0.00079040
Iteration 25/25 | Loss: 0.00078981

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40234303
Iteration 2/25 | Loss: 0.00187821
Iteration 3/25 | Loss: 0.00187821
Iteration 4/25 | Loss: 0.00187821
Iteration 5/25 | Loss: 0.00187820
Iteration 6/25 | Loss: 0.00187820
Iteration 7/25 | Loss: 0.00187820
Iteration 8/25 | Loss: 0.00187820
Iteration 9/25 | Loss: 0.00187820
Iteration 10/25 | Loss: 0.00187820
Iteration 11/25 | Loss: 0.00187820
Iteration 12/25 | Loss: 0.00187820
Iteration 13/25 | Loss: 0.00187820
Iteration 14/25 | Loss: 0.00187820
Iteration 15/25 | Loss: 0.00187820
Iteration 16/25 | Loss: 0.00187820
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001878203242085874, 0.001878203242085874, 0.001878203242085874, 0.001878203242085874, 0.001878203242085874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001878203242085874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187820
Iteration 2/1000 | Loss: 0.00021961
Iteration 3/1000 | Loss: 0.00015019
Iteration 4/1000 | Loss: 0.00012160
Iteration 5/1000 | Loss: 0.00010436
Iteration 6/1000 | Loss: 0.00009351
Iteration 7/1000 | Loss: 0.00008440
Iteration 8/1000 | Loss: 0.00007980
Iteration 9/1000 | Loss: 0.00007445
Iteration 10/1000 | Loss: 0.00007141
Iteration 11/1000 | Loss: 0.00006824
Iteration 12/1000 | Loss: 0.00006583
Iteration 13/1000 | Loss: 0.00061135
Iteration 14/1000 | Loss: 0.01080325
Iteration 15/1000 | Loss: 0.00023060
Iteration 16/1000 | Loss: 0.00010163
Iteration 17/1000 | Loss: 0.00005849
Iteration 18/1000 | Loss: 0.00004407
Iteration 19/1000 | Loss: 0.00003314
Iteration 20/1000 | Loss: 0.00002491
Iteration 21/1000 | Loss: 0.00002267
Iteration 22/1000 | Loss: 0.00002142
Iteration 23/1000 | Loss: 0.00002077
Iteration 24/1000 | Loss: 0.00002036
Iteration 25/1000 | Loss: 0.00001998
Iteration 26/1000 | Loss: 0.00001967
Iteration 27/1000 | Loss: 0.00001944
Iteration 28/1000 | Loss: 0.00001927
Iteration 29/1000 | Loss: 0.00001920
Iteration 30/1000 | Loss: 0.00001912
Iteration 31/1000 | Loss: 0.00001910
Iteration 32/1000 | Loss: 0.00001910
Iteration 33/1000 | Loss: 0.00001909
Iteration 34/1000 | Loss: 0.00001908
Iteration 35/1000 | Loss: 0.00001908
Iteration 36/1000 | Loss: 0.00001907
Iteration 37/1000 | Loss: 0.00001905
Iteration 38/1000 | Loss: 0.00001905
Iteration 39/1000 | Loss: 0.00001905
Iteration 40/1000 | Loss: 0.00001905
Iteration 41/1000 | Loss: 0.00001905
Iteration 42/1000 | Loss: 0.00001904
Iteration 43/1000 | Loss: 0.00001903
Iteration 44/1000 | Loss: 0.00001903
Iteration 45/1000 | Loss: 0.00001903
Iteration 46/1000 | Loss: 0.00001902
Iteration 47/1000 | Loss: 0.00001902
Iteration 48/1000 | Loss: 0.00001900
Iteration 49/1000 | Loss: 0.00001900
Iteration 50/1000 | Loss: 0.00001900
Iteration 51/1000 | Loss: 0.00001900
Iteration 52/1000 | Loss: 0.00001900
Iteration 53/1000 | Loss: 0.00001899
Iteration 54/1000 | Loss: 0.00001898
Iteration 55/1000 | Loss: 0.00001898
Iteration 56/1000 | Loss: 0.00001897
Iteration 57/1000 | Loss: 0.00001897
Iteration 58/1000 | Loss: 0.00001896
Iteration 59/1000 | Loss: 0.00001896
Iteration 60/1000 | Loss: 0.00001896
Iteration 61/1000 | Loss: 0.00001895
Iteration 62/1000 | Loss: 0.00001895
Iteration 63/1000 | Loss: 0.00001895
Iteration 64/1000 | Loss: 0.00001895
Iteration 65/1000 | Loss: 0.00001895
Iteration 66/1000 | Loss: 0.00001895
Iteration 67/1000 | Loss: 0.00001895
Iteration 68/1000 | Loss: 0.00001895
Iteration 69/1000 | Loss: 0.00001895
Iteration 70/1000 | Loss: 0.00001894
Iteration 71/1000 | Loss: 0.00001894
Iteration 72/1000 | Loss: 0.00001893
Iteration 73/1000 | Loss: 0.00001893
Iteration 74/1000 | Loss: 0.00001893
Iteration 75/1000 | Loss: 0.00001892
Iteration 76/1000 | Loss: 0.00001892
Iteration 77/1000 | Loss: 0.00001892
Iteration 78/1000 | Loss: 0.00001892
Iteration 79/1000 | Loss: 0.00001892
Iteration 80/1000 | Loss: 0.00001892
Iteration 81/1000 | Loss: 0.00001892
Iteration 82/1000 | Loss: 0.00001891
Iteration 83/1000 | Loss: 0.00001891
Iteration 84/1000 | Loss: 0.00001891
Iteration 85/1000 | Loss: 0.00001891
Iteration 86/1000 | Loss: 0.00001891
Iteration 87/1000 | Loss: 0.00001891
Iteration 88/1000 | Loss: 0.00001891
Iteration 89/1000 | Loss: 0.00001891
Iteration 90/1000 | Loss: 0.00001891
Iteration 91/1000 | Loss: 0.00001891
Iteration 92/1000 | Loss: 0.00001891
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001891
Iteration 95/1000 | Loss: 0.00001891
Iteration 96/1000 | Loss: 0.00001891
Iteration 97/1000 | Loss: 0.00001891
Iteration 98/1000 | Loss: 0.00001891
Iteration 99/1000 | Loss: 0.00001891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.8905968317994848e-05, 1.8905968317994848e-05, 1.8905968317994848e-05, 1.8905968317994848e-05, 1.8905968317994848e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8905968317994848e-05

Optimization complete. Final v2v error: 3.6732306480407715 mm

Highest mean error: 3.919976234436035 mm for frame 89

Lowest mean error: 3.501105308532715 mm for frame 227

Saving results

Total time: 102.93395447731018
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00629532
Iteration 2/25 | Loss: 0.00078076
Iteration 3/25 | Loss: 0.00066125
Iteration 4/25 | Loss: 0.00063639
Iteration 5/25 | Loss: 0.00062488
Iteration 6/25 | Loss: 0.00062314
Iteration 7/25 | Loss: 0.00062278
Iteration 8/25 | Loss: 0.00062278
Iteration 9/25 | Loss: 0.00062278
Iteration 10/25 | Loss: 0.00062278
Iteration 11/25 | Loss: 0.00062278
Iteration 12/25 | Loss: 0.00062278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006227780831977725, 0.0006227780831977725, 0.0006227780831977725, 0.0006227780831977725, 0.0006227780831977725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006227780831977725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75611663
Iteration 2/25 | Loss: 0.00029381
Iteration 3/25 | Loss: 0.00029381
Iteration 4/25 | Loss: 0.00029381
Iteration 5/25 | Loss: 0.00029381
Iteration 6/25 | Loss: 0.00029381
Iteration 7/25 | Loss: 0.00029381
Iteration 8/25 | Loss: 0.00029380
Iteration 9/25 | Loss: 0.00029380
Iteration 10/25 | Loss: 0.00029380
Iteration 11/25 | Loss: 0.00029380
Iteration 12/25 | Loss: 0.00029380
Iteration 13/25 | Loss: 0.00029380
Iteration 14/25 | Loss: 0.00029380
Iteration 15/25 | Loss: 0.00029380
Iteration 16/25 | Loss: 0.00029380
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00029380444902926683, 0.00029380444902926683, 0.00029380444902926683, 0.00029380444902926683, 0.00029380444902926683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029380444902926683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029380
Iteration 2/1000 | Loss: 0.00002863
Iteration 3/1000 | Loss: 0.00001910
Iteration 4/1000 | Loss: 0.00001628
Iteration 5/1000 | Loss: 0.00001512
Iteration 6/1000 | Loss: 0.00001478
Iteration 7/1000 | Loss: 0.00001432
Iteration 8/1000 | Loss: 0.00001411
Iteration 9/1000 | Loss: 0.00001390
Iteration 10/1000 | Loss: 0.00001389
Iteration 11/1000 | Loss: 0.00001389
Iteration 12/1000 | Loss: 0.00001383
Iteration 13/1000 | Loss: 0.00001383
Iteration 14/1000 | Loss: 0.00001382
Iteration 15/1000 | Loss: 0.00001382
Iteration 16/1000 | Loss: 0.00001382
Iteration 17/1000 | Loss: 0.00001382
Iteration 18/1000 | Loss: 0.00001381
Iteration 19/1000 | Loss: 0.00001381
Iteration 20/1000 | Loss: 0.00001376
Iteration 21/1000 | Loss: 0.00001376
Iteration 22/1000 | Loss: 0.00001375
Iteration 23/1000 | Loss: 0.00001375
Iteration 24/1000 | Loss: 0.00001374
Iteration 25/1000 | Loss: 0.00001370
Iteration 26/1000 | Loss: 0.00001370
Iteration 27/1000 | Loss: 0.00001369
Iteration 28/1000 | Loss: 0.00001365
Iteration 29/1000 | Loss: 0.00001361
Iteration 30/1000 | Loss: 0.00001360
Iteration 31/1000 | Loss: 0.00001360
Iteration 32/1000 | Loss: 0.00001357
Iteration 33/1000 | Loss: 0.00001356
Iteration 34/1000 | Loss: 0.00001356
Iteration 35/1000 | Loss: 0.00001355
Iteration 36/1000 | Loss: 0.00001354
Iteration 37/1000 | Loss: 0.00001354
Iteration 38/1000 | Loss: 0.00001354
Iteration 39/1000 | Loss: 0.00001354
Iteration 40/1000 | Loss: 0.00001354
Iteration 41/1000 | Loss: 0.00001353
Iteration 42/1000 | Loss: 0.00001353
Iteration 43/1000 | Loss: 0.00001352
Iteration 44/1000 | Loss: 0.00001351
Iteration 45/1000 | Loss: 0.00001351
Iteration 46/1000 | Loss: 0.00001351
Iteration 47/1000 | Loss: 0.00001351
Iteration 48/1000 | Loss: 0.00001351
Iteration 49/1000 | Loss: 0.00001351
Iteration 50/1000 | Loss: 0.00001350
Iteration 51/1000 | Loss: 0.00001347
Iteration 52/1000 | Loss: 0.00001346
Iteration 53/1000 | Loss: 0.00001346
Iteration 54/1000 | Loss: 0.00001346
Iteration 55/1000 | Loss: 0.00001340
Iteration 56/1000 | Loss: 0.00001340
Iteration 57/1000 | Loss: 0.00001340
Iteration 58/1000 | Loss: 0.00001340
Iteration 59/1000 | Loss: 0.00001339
Iteration 60/1000 | Loss: 0.00001339
Iteration 61/1000 | Loss: 0.00001337
Iteration 62/1000 | Loss: 0.00001336
Iteration 63/1000 | Loss: 0.00001336
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001334
Iteration 70/1000 | Loss: 0.00001334
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001334
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001333
Iteration 75/1000 | Loss: 0.00001333
Iteration 76/1000 | Loss: 0.00001333
Iteration 77/1000 | Loss: 0.00001333
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001332
Iteration 80/1000 | Loss: 0.00001332
Iteration 81/1000 | Loss: 0.00001332
Iteration 82/1000 | Loss: 0.00001331
Iteration 83/1000 | Loss: 0.00001331
Iteration 84/1000 | Loss: 0.00001331
Iteration 85/1000 | Loss: 0.00001331
Iteration 86/1000 | Loss: 0.00001331
Iteration 87/1000 | Loss: 0.00001331
Iteration 88/1000 | Loss: 0.00001331
Iteration 89/1000 | Loss: 0.00001330
Iteration 90/1000 | Loss: 0.00001330
Iteration 91/1000 | Loss: 0.00001329
Iteration 92/1000 | Loss: 0.00001329
Iteration 93/1000 | Loss: 0.00001329
Iteration 94/1000 | Loss: 0.00001328
Iteration 95/1000 | Loss: 0.00001328
Iteration 96/1000 | Loss: 0.00001328
Iteration 97/1000 | Loss: 0.00001328
Iteration 98/1000 | Loss: 0.00001327
Iteration 99/1000 | Loss: 0.00001327
Iteration 100/1000 | Loss: 0.00001327
Iteration 101/1000 | Loss: 0.00001327
Iteration 102/1000 | Loss: 0.00001327
Iteration 103/1000 | Loss: 0.00001327
Iteration 104/1000 | Loss: 0.00001327
Iteration 105/1000 | Loss: 0.00001327
Iteration 106/1000 | Loss: 0.00001327
Iteration 107/1000 | Loss: 0.00001327
Iteration 108/1000 | Loss: 0.00001326
Iteration 109/1000 | Loss: 0.00001326
Iteration 110/1000 | Loss: 0.00001326
Iteration 111/1000 | Loss: 0.00001326
Iteration 112/1000 | Loss: 0.00001326
Iteration 113/1000 | Loss: 0.00001326
Iteration 114/1000 | Loss: 0.00001326
Iteration 115/1000 | Loss: 0.00001326
Iteration 116/1000 | Loss: 0.00001326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.3264881999930367e-05, 1.3264881999930367e-05, 1.3264881999930367e-05, 1.3264881999930367e-05, 1.3264881999930367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3264881999930367e-05

Optimization complete. Final v2v error: 3.1026365756988525 mm

Highest mean error: 3.4219577312469482 mm for frame 78

Lowest mean error: 2.967520236968994 mm for frame 8

Saving results

Total time: 34.05270552635193
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01119666
Iteration 2/25 | Loss: 0.00620575
Iteration 3/25 | Loss: 0.00357588
Iteration 4/25 | Loss: 0.00310359
Iteration 5/25 | Loss: 0.00240016
Iteration 6/25 | Loss: 0.00211784
Iteration 7/25 | Loss: 0.00177656
Iteration 8/25 | Loss: 0.00142871
Iteration 9/25 | Loss: 0.00125761
Iteration 10/25 | Loss: 0.00120246
Iteration 11/25 | Loss: 0.00126659
Iteration 12/25 | Loss: 0.00108517
Iteration 13/25 | Loss: 0.00106418
Iteration 14/25 | Loss: 0.00104290
Iteration 15/25 | Loss: 0.00102944
Iteration 16/25 | Loss: 0.00102953
Iteration 17/25 | Loss: 0.00103078
Iteration 18/25 | Loss: 0.00101940
Iteration 19/25 | Loss: 0.00101404
Iteration 20/25 | Loss: 0.00101096
Iteration 21/25 | Loss: 0.00100909
Iteration 22/25 | Loss: 0.00100891
Iteration 23/25 | Loss: 0.00100890
Iteration 24/25 | Loss: 0.00100890
Iteration 25/25 | Loss: 0.00100890

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.57606590
Iteration 2/25 | Loss: 0.00130823
Iteration 3/25 | Loss: 0.00130823
Iteration 4/25 | Loss: 0.00130823
Iteration 5/25 | Loss: 0.00130823
Iteration 6/25 | Loss: 0.00130823
Iteration 7/25 | Loss: 0.00130823
Iteration 8/25 | Loss: 0.00130823
Iteration 9/25 | Loss: 0.00130823
Iteration 10/25 | Loss: 0.00130823
Iteration 11/25 | Loss: 0.00130823
Iteration 12/25 | Loss: 0.00130823
Iteration 13/25 | Loss: 0.00130823
Iteration 14/25 | Loss: 0.00130823
Iteration 15/25 | Loss: 0.00130823
Iteration 16/25 | Loss: 0.00130823
Iteration 17/25 | Loss: 0.00130823
Iteration 18/25 | Loss: 0.00130823
Iteration 19/25 | Loss: 0.00130823
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013082261430099607, 0.0013082261430099607, 0.0013082261430099607, 0.0013082261430099607, 0.0013082261430099607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013082261430099607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130823
Iteration 2/1000 | Loss: 0.00023368
Iteration 3/1000 | Loss: 0.00016991
Iteration 4/1000 | Loss: 0.00014442
Iteration 5/1000 | Loss: 0.00013470
Iteration 6/1000 | Loss: 0.00012733
Iteration 7/1000 | Loss: 0.00012110
Iteration 8/1000 | Loss: 0.00011771
Iteration 9/1000 | Loss: 0.00090567
Iteration 10/1000 | Loss: 0.02113871
Iteration 11/1000 | Loss: 0.00115790
Iteration 12/1000 | Loss: 0.00015542
Iteration 13/1000 | Loss: 0.00008836
Iteration 14/1000 | Loss: 0.00007256
Iteration 15/1000 | Loss: 0.00006429
Iteration 16/1000 | Loss: 0.00060743
Iteration 17/1000 | Loss: 0.00019599
Iteration 18/1000 | Loss: 0.00003870
Iteration 19/1000 | Loss: 0.00003314
Iteration 20/1000 | Loss: 0.00064141
Iteration 21/1000 | Loss: 0.00003377
Iteration 22/1000 | Loss: 0.00002887
Iteration 23/1000 | Loss: 0.00002825
Iteration 24/1000 | Loss: 0.00002572
Iteration 25/1000 | Loss: 0.00002901
Iteration 26/1000 | Loss: 0.00002320
Iteration 27/1000 | Loss: 0.00002252
Iteration 28/1000 | Loss: 0.00002192
Iteration 29/1000 | Loss: 0.00002156
Iteration 30/1000 | Loss: 0.00002127
Iteration 31/1000 | Loss: 0.00002113
Iteration 32/1000 | Loss: 0.00002096
Iteration 33/1000 | Loss: 0.00002096
Iteration 34/1000 | Loss: 0.00002087
Iteration 35/1000 | Loss: 0.00002084
Iteration 36/1000 | Loss: 0.00002083
Iteration 37/1000 | Loss: 0.00002083
Iteration 38/1000 | Loss: 0.00002083
Iteration 39/1000 | Loss: 0.00002082
Iteration 40/1000 | Loss: 0.00002082
Iteration 41/1000 | Loss: 0.00002082
Iteration 42/1000 | Loss: 0.00002081
Iteration 43/1000 | Loss: 0.00002081
Iteration 44/1000 | Loss: 0.00002080
Iteration 45/1000 | Loss: 0.00002080
Iteration 46/1000 | Loss: 0.00002079
Iteration 47/1000 | Loss: 0.00002079
Iteration 48/1000 | Loss: 0.00002079
Iteration 49/1000 | Loss: 0.00002079
Iteration 50/1000 | Loss: 0.00002079
Iteration 51/1000 | Loss: 0.00002078
Iteration 52/1000 | Loss: 0.00002078
Iteration 53/1000 | Loss: 0.00002078
Iteration 54/1000 | Loss: 0.00002078
Iteration 55/1000 | Loss: 0.00002077
Iteration 56/1000 | Loss: 0.00002077
Iteration 57/1000 | Loss: 0.00002076
Iteration 58/1000 | Loss: 0.00002075
Iteration 59/1000 | Loss: 0.00002075
Iteration 60/1000 | Loss: 0.00002075
Iteration 61/1000 | Loss: 0.00002075
Iteration 62/1000 | Loss: 0.00002075
Iteration 63/1000 | Loss: 0.00002074
Iteration 64/1000 | Loss: 0.00002074
Iteration 65/1000 | Loss: 0.00002074
Iteration 66/1000 | Loss: 0.00002074
Iteration 67/1000 | Loss: 0.00002074
Iteration 68/1000 | Loss: 0.00002074
Iteration 69/1000 | Loss: 0.00002073
Iteration 70/1000 | Loss: 0.00002073
Iteration 71/1000 | Loss: 0.00002073
Iteration 72/1000 | Loss: 0.00002073
Iteration 73/1000 | Loss: 0.00002073
Iteration 74/1000 | Loss: 0.00002073
Iteration 75/1000 | Loss: 0.00002073
Iteration 76/1000 | Loss: 0.00002073
Iteration 77/1000 | Loss: 0.00002072
Iteration 78/1000 | Loss: 0.00002072
Iteration 79/1000 | Loss: 0.00002071
Iteration 80/1000 | Loss: 0.00002071
Iteration 81/1000 | Loss: 0.00002071
Iteration 82/1000 | Loss: 0.00002071
Iteration 83/1000 | Loss: 0.00002070
Iteration 84/1000 | Loss: 0.00002070
Iteration 85/1000 | Loss: 0.00002070
Iteration 86/1000 | Loss: 0.00002070
Iteration 87/1000 | Loss: 0.00002069
Iteration 88/1000 | Loss: 0.00002069
Iteration 89/1000 | Loss: 0.00002069
Iteration 90/1000 | Loss: 0.00002069
Iteration 91/1000 | Loss: 0.00002069
Iteration 92/1000 | Loss: 0.00002069
Iteration 93/1000 | Loss: 0.00002069
Iteration 94/1000 | Loss: 0.00002069
Iteration 95/1000 | Loss: 0.00002069
Iteration 96/1000 | Loss: 0.00002069
Iteration 97/1000 | Loss: 0.00002069
Iteration 98/1000 | Loss: 0.00002069
Iteration 99/1000 | Loss: 0.00002069
Iteration 100/1000 | Loss: 0.00002069
Iteration 101/1000 | Loss: 0.00002069
Iteration 102/1000 | Loss: 0.00002069
Iteration 103/1000 | Loss: 0.00002069
Iteration 104/1000 | Loss: 0.00002069
Iteration 105/1000 | Loss: 0.00002069
Iteration 106/1000 | Loss: 0.00002069
Iteration 107/1000 | Loss: 0.00002069
Iteration 108/1000 | Loss: 0.00002069
Iteration 109/1000 | Loss: 0.00002069
Iteration 110/1000 | Loss: 0.00002069
Iteration 111/1000 | Loss: 0.00002069
Iteration 112/1000 | Loss: 0.00002069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [2.068962021439802e-05, 2.068962021439802e-05, 2.068962021439802e-05, 2.068962021439802e-05, 2.068962021439802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.068962021439802e-05

Optimization complete. Final v2v error: 3.8936331272125244 mm

Highest mean error: 4.001989364624023 mm for frame 139

Lowest mean error: 3.7144033908843994 mm for frame 194

Saving results

Total time: 99.47807216644287
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419161
Iteration 2/25 | Loss: 0.00076802
Iteration 3/25 | Loss: 0.00065291
Iteration 4/25 | Loss: 0.00062917
Iteration 5/25 | Loss: 0.00061884
Iteration 6/25 | Loss: 0.00061693
Iteration 7/25 | Loss: 0.00061637
Iteration 8/25 | Loss: 0.00061632
Iteration 9/25 | Loss: 0.00061632
Iteration 10/25 | Loss: 0.00061632
Iteration 11/25 | Loss: 0.00061632
Iteration 12/25 | Loss: 0.00061632
Iteration 13/25 | Loss: 0.00061632
Iteration 14/25 | Loss: 0.00061632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006163236103020608, 0.0006163236103020608, 0.0006163236103020608, 0.0006163236103020608, 0.0006163236103020608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006163236103020608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.66127467
Iteration 2/25 | Loss: 0.00026887
Iteration 3/25 | Loss: 0.00026887
Iteration 4/25 | Loss: 0.00026887
Iteration 5/25 | Loss: 0.00026887
Iteration 6/25 | Loss: 0.00026887
Iteration 7/25 | Loss: 0.00026887
Iteration 8/25 | Loss: 0.00026887
Iteration 9/25 | Loss: 0.00026887
Iteration 10/25 | Loss: 0.00026887
Iteration 11/25 | Loss: 0.00026887
Iteration 12/25 | Loss: 0.00026887
Iteration 13/25 | Loss: 0.00026887
Iteration 14/25 | Loss: 0.00026887
Iteration 15/25 | Loss: 0.00026887
Iteration 16/25 | Loss: 0.00026887
Iteration 17/25 | Loss: 0.00026887
Iteration 18/25 | Loss: 0.00026887
Iteration 19/25 | Loss: 0.00026887
Iteration 20/25 | Loss: 0.00026887
Iteration 21/25 | Loss: 0.00026887
Iteration 22/25 | Loss: 0.00026887
Iteration 23/25 | Loss: 0.00026887
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002688669483177364, 0.0002688669483177364, 0.0002688669483177364, 0.0002688669483177364, 0.0002688669483177364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002688669483177364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026887
Iteration 2/1000 | Loss: 0.00003438
Iteration 3/1000 | Loss: 0.00002096
Iteration 4/1000 | Loss: 0.00001896
Iteration 5/1000 | Loss: 0.00001810
Iteration 6/1000 | Loss: 0.00001717
Iteration 7/1000 | Loss: 0.00001669
Iteration 8/1000 | Loss: 0.00001623
Iteration 9/1000 | Loss: 0.00001593
Iteration 10/1000 | Loss: 0.00001573
Iteration 11/1000 | Loss: 0.00001563
Iteration 12/1000 | Loss: 0.00001558
Iteration 13/1000 | Loss: 0.00001554
Iteration 14/1000 | Loss: 0.00001554
Iteration 15/1000 | Loss: 0.00001552
Iteration 16/1000 | Loss: 0.00001552
Iteration 17/1000 | Loss: 0.00001551
Iteration 18/1000 | Loss: 0.00001550
Iteration 19/1000 | Loss: 0.00001550
Iteration 20/1000 | Loss: 0.00001547
Iteration 21/1000 | Loss: 0.00001547
Iteration 22/1000 | Loss: 0.00001546
Iteration 23/1000 | Loss: 0.00001546
Iteration 24/1000 | Loss: 0.00001545
Iteration 25/1000 | Loss: 0.00001545
Iteration 26/1000 | Loss: 0.00001545
Iteration 27/1000 | Loss: 0.00001544
Iteration 28/1000 | Loss: 0.00001544
Iteration 29/1000 | Loss: 0.00001540
Iteration 30/1000 | Loss: 0.00001540
Iteration 31/1000 | Loss: 0.00001540
Iteration 32/1000 | Loss: 0.00001539
Iteration 33/1000 | Loss: 0.00001539
Iteration 34/1000 | Loss: 0.00001538
Iteration 35/1000 | Loss: 0.00001536
Iteration 36/1000 | Loss: 0.00001536
Iteration 37/1000 | Loss: 0.00001536
Iteration 38/1000 | Loss: 0.00001536
Iteration 39/1000 | Loss: 0.00001536
Iteration 40/1000 | Loss: 0.00001536
Iteration 41/1000 | Loss: 0.00001536
Iteration 42/1000 | Loss: 0.00001536
Iteration 43/1000 | Loss: 0.00001536
Iteration 44/1000 | Loss: 0.00001536
Iteration 45/1000 | Loss: 0.00001536
Iteration 46/1000 | Loss: 0.00001535
Iteration 47/1000 | Loss: 0.00001535
Iteration 48/1000 | Loss: 0.00001535
Iteration 49/1000 | Loss: 0.00001534
Iteration 50/1000 | Loss: 0.00001534
Iteration 51/1000 | Loss: 0.00001534
Iteration 52/1000 | Loss: 0.00001533
Iteration 53/1000 | Loss: 0.00001533
Iteration 54/1000 | Loss: 0.00001533
Iteration 55/1000 | Loss: 0.00001532
Iteration 56/1000 | Loss: 0.00001532
Iteration 57/1000 | Loss: 0.00001532
Iteration 58/1000 | Loss: 0.00001532
Iteration 59/1000 | Loss: 0.00001532
Iteration 60/1000 | Loss: 0.00001532
Iteration 61/1000 | Loss: 0.00001532
Iteration 62/1000 | Loss: 0.00001532
Iteration 63/1000 | Loss: 0.00001531
Iteration 64/1000 | Loss: 0.00001531
Iteration 65/1000 | Loss: 0.00001531
Iteration 66/1000 | Loss: 0.00001530
Iteration 67/1000 | Loss: 0.00001530
Iteration 68/1000 | Loss: 0.00001529
Iteration 69/1000 | Loss: 0.00001529
Iteration 70/1000 | Loss: 0.00001529
Iteration 71/1000 | Loss: 0.00001529
Iteration 72/1000 | Loss: 0.00001528
Iteration 73/1000 | Loss: 0.00001528
Iteration 74/1000 | Loss: 0.00001528
Iteration 75/1000 | Loss: 0.00001528
Iteration 76/1000 | Loss: 0.00001527
Iteration 77/1000 | Loss: 0.00001527
Iteration 78/1000 | Loss: 0.00001527
Iteration 79/1000 | Loss: 0.00001526
Iteration 80/1000 | Loss: 0.00001526
Iteration 81/1000 | Loss: 0.00001524
Iteration 82/1000 | Loss: 0.00001524
Iteration 83/1000 | Loss: 0.00001523
Iteration 84/1000 | Loss: 0.00001523
Iteration 85/1000 | Loss: 0.00001523
Iteration 86/1000 | Loss: 0.00001522
Iteration 87/1000 | Loss: 0.00001522
Iteration 88/1000 | Loss: 0.00001522
Iteration 89/1000 | Loss: 0.00001521
Iteration 90/1000 | Loss: 0.00001520
Iteration 91/1000 | Loss: 0.00001520
Iteration 92/1000 | Loss: 0.00001520
Iteration 93/1000 | Loss: 0.00001520
Iteration 94/1000 | Loss: 0.00001520
Iteration 95/1000 | Loss: 0.00001520
Iteration 96/1000 | Loss: 0.00001520
Iteration 97/1000 | Loss: 0.00001519
Iteration 98/1000 | Loss: 0.00001519
Iteration 99/1000 | Loss: 0.00001519
Iteration 100/1000 | Loss: 0.00001519
Iteration 101/1000 | Loss: 0.00001518
Iteration 102/1000 | Loss: 0.00001518
Iteration 103/1000 | Loss: 0.00001518
Iteration 104/1000 | Loss: 0.00001518
Iteration 105/1000 | Loss: 0.00001517
Iteration 106/1000 | Loss: 0.00001517
Iteration 107/1000 | Loss: 0.00001517
Iteration 108/1000 | Loss: 0.00001517
Iteration 109/1000 | Loss: 0.00001517
Iteration 110/1000 | Loss: 0.00001517
Iteration 111/1000 | Loss: 0.00001517
Iteration 112/1000 | Loss: 0.00001517
Iteration 113/1000 | Loss: 0.00001517
Iteration 114/1000 | Loss: 0.00001516
Iteration 115/1000 | Loss: 0.00001516
Iteration 116/1000 | Loss: 0.00001516
Iteration 117/1000 | Loss: 0.00001516
Iteration 118/1000 | Loss: 0.00001516
Iteration 119/1000 | Loss: 0.00001516
Iteration 120/1000 | Loss: 0.00001516
Iteration 121/1000 | Loss: 0.00001516
Iteration 122/1000 | Loss: 0.00001516
Iteration 123/1000 | Loss: 0.00001516
Iteration 124/1000 | Loss: 0.00001516
Iteration 125/1000 | Loss: 0.00001516
Iteration 126/1000 | Loss: 0.00001516
Iteration 127/1000 | Loss: 0.00001515
Iteration 128/1000 | Loss: 0.00001515
Iteration 129/1000 | Loss: 0.00001515
Iteration 130/1000 | Loss: 0.00001515
Iteration 131/1000 | Loss: 0.00001515
Iteration 132/1000 | Loss: 0.00001515
Iteration 133/1000 | Loss: 0.00001515
Iteration 134/1000 | Loss: 0.00001515
Iteration 135/1000 | Loss: 0.00001515
Iteration 136/1000 | Loss: 0.00001515
Iteration 137/1000 | Loss: 0.00001515
Iteration 138/1000 | Loss: 0.00001515
Iteration 139/1000 | Loss: 0.00001515
Iteration 140/1000 | Loss: 0.00001515
Iteration 141/1000 | Loss: 0.00001515
Iteration 142/1000 | Loss: 0.00001515
Iteration 143/1000 | Loss: 0.00001514
Iteration 144/1000 | Loss: 0.00001514
Iteration 145/1000 | Loss: 0.00001514
Iteration 146/1000 | Loss: 0.00001514
Iteration 147/1000 | Loss: 0.00001514
Iteration 148/1000 | Loss: 0.00001514
Iteration 149/1000 | Loss: 0.00001514
Iteration 150/1000 | Loss: 0.00001514
Iteration 151/1000 | Loss: 0.00001514
Iteration 152/1000 | Loss: 0.00001514
Iteration 153/1000 | Loss: 0.00001514
Iteration 154/1000 | Loss: 0.00001514
Iteration 155/1000 | Loss: 0.00001514
Iteration 156/1000 | Loss: 0.00001514
Iteration 157/1000 | Loss: 0.00001514
Iteration 158/1000 | Loss: 0.00001514
Iteration 159/1000 | Loss: 0.00001513
Iteration 160/1000 | Loss: 0.00001513
Iteration 161/1000 | Loss: 0.00001513
Iteration 162/1000 | Loss: 0.00001513
Iteration 163/1000 | Loss: 0.00001513
Iteration 164/1000 | Loss: 0.00001513
Iteration 165/1000 | Loss: 0.00001513
Iteration 166/1000 | Loss: 0.00001513
Iteration 167/1000 | Loss: 0.00001513
Iteration 168/1000 | Loss: 0.00001513
Iteration 169/1000 | Loss: 0.00001513
Iteration 170/1000 | Loss: 0.00001513
Iteration 171/1000 | Loss: 0.00001513
Iteration 172/1000 | Loss: 0.00001512
Iteration 173/1000 | Loss: 0.00001512
Iteration 174/1000 | Loss: 0.00001512
Iteration 175/1000 | Loss: 0.00001512
Iteration 176/1000 | Loss: 0.00001512
Iteration 177/1000 | Loss: 0.00001512
Iteration 178/1000 | Loss: 0.00001512
Iteration 179/1000 | Loss: 0.00001512
Iteration 180/1000 | Loss: 0.00001512
Iteration 181/1000 | Loss: 0.00001512
Iteration 182/1000 | Loss: 0.00001512
Iteration 183/1000 | Loss: 0.00001512
Iteration 184/1000 | Loss: 0.00001512
Iteration 185/1000 | Loss: 0.00001512
Iteration 186/1000 | Loss: 0.00001512
Iteration 187/1000 | Loss: 0.00001512
Iteration 188/1000 | Loss: 0.00001512
Iteration 189/1000 | Loss: 0.00001512
Iteration 190/1000 | Loss: 0.00001512
Iteration 191/1000 | Loss: 0.00001512
Iteration 192/1000 | Loss: 0.00001512
Iteration 193/1000 | Loss: 0.00001512
Iteration 194/1000 | Loss: 0.00001512
Iteration 195/1000 | Loss: 0.00001512
Iteration 196/1000 | Loss: 0.00001512
Iteration 197/1000 | Loss: 0.00001512
Iteration 198/1000 | Loss: 0.00001512
Iteration 199/1000 | Loss: 0.00001512
Iteration 200/1000 | Loss: 0.00001512
Iteration 201/1000 | Loss: 0.00001512
Iteration 202/1000 | Loss: 0.00001512
Iteration 203/1000 | Loss: 0.00001512
Iteration 204/1000 | Loss: 0.00001512
Iteration 205/1000 | Loss: 0.00001512
Iteration 206/1000 | Loss: 0.00001512
Iteration 207/1000 | Loss: 0.00001512
Iteration 208/1000 | Loss: 0.00001512
Iteration 209/1000 | Loss: 0.00001512
Iteration 210/1000 | Loss: 0.00001512
Iteration 211/1000 | Loss: 0.00001512
Iteration 212/1000 | Loss: 0.00001512
Iteration 213/1000 | Loss: 0.00001512
Iteration 214/1000 | Loss: 0.00001512
Iteration 215/1000 | Loss: 0.00001512
Iteration 216/1000 | Loss: 0.00001512
Iteration 217/1000 | Loss: 0.00001512
Iteration 218/1000 | Loss: 0.00001512
Iteration 219/1000 | Loss: 0.00001512
Iteration 220/1000 | Loss: 0.00001512
Iteration 221/1000 | Loss: 0.00001512
Iteration 222/1000 | Loss: 0.00001512
Iteration 223/1000 | Loss: 0.00001512
Iteration 224/1000 | Loss: 0.00001512
Iteration 225/1000 | Loss: 0.00001512
Iteration 226/1000 | Loss: 0.00001512
Iteration 227/1000 | Loss: 0.00001512
Iteration 228/1000 | Loss: 0.00001512
Iteration 229/1000 | Loss: 0.00001512
Iteration 230/1000 | Loss: 0.00001512
Iteration 231/1000 | Loss: 0.00001512
Iteration 232/1000 | Loss: 0.00001512
Iteration 233/1000 | Loss: 0.00001512
Iteration 234/1000 | Loss: 0.00001512
Iteration 235/1000 | Loss: 0.00001512
Iteration 236/1000 | Loss: 0.00001512
Iteration 237/1000 | Loss: 0.00001512
Iteration 238/1000 | Loss: 0.00001512
Iteration 239/1000 | Loss: 0.00001512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.5116072972887196e-05, 1.5116072972887196e-05, 1.5116072972887196e-05, 1.5116072972887196e-05, 1.5116072972887196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5116072972887196e-05

Optimization complete. Final v2v error: 3.289865016937256 mm

Highest mean error: 3.8045239448547363 mm for frame 46

Lowest mean error: 2.856145143508911 mm for frame 102

Saving results

Total time: 39.75174856185913
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862945
Iteration 2/25 | Loss: 0.00079593
Iteration 3/25 | Loss: 0.00067231
Iteration 4/25 | Loss: 0.00064210
Iteration 5/25 | Loss: 0.00063009
Iteration 6/25 | Loss: 0.00062900
Iteration 7/25 | Loss: 0.00062900
Iteration 8/25 | Loss: 0.00062900
Iteration 9/25 | Loss: 0.00062900
Iteration 10/25 | Loss: 0.00062900
Iteration 11/25 | Loss: 0.00062900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006289970478974283, 0.0006289970478974283, 0.0006289970478974283, 0.0006289970478974283, 0.0006289970478974283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006289970478974283

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46915746
Iteration 2/25 | Loss: 0.00022313
Iteration 3/25 | Loss: 0.00022312
Iteration 4/25 | Loss: 0.00022312
Iteration 5/25 | Loss: 0.00022312
Iteration 6/25 | Loss: 0.00022312
Iteration 7/25 | Loss: 0.00022312
Iteration 8/25 | Loss: 0.00022312
Iteration 9/25 | Loss: 0.00022312
Iteration 10/25 | Loss: 0.00022312
Iteration 11/25 | Loss: 0.00022312
Iteration 12/25 | Loss: 0.00022312
Iteration 13/25 | Loss: 0.00022312
Iteration 14/25 | Loss: 0.00022312
Iteration 15/25 | Loss: 0.00022312
Iteration 16/25 | Loss: 0.00022312
Iteration 17/25 | Loss: 0.00022312
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00022311553766485304, 0.00022311553766485304, 0.00022311553766485304, 0.00022311553766485304, 0.00022311553766485304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022311553766485304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022312
Iteration 2/1000 | Loss: 0.00003055
Iteration 3/1000 | Loss: 0.00002246
Iteration 4/1000 | Loss: 0.00002047
Iteration 5/1000 | Loss: 0.00001916
Iteration 6/1000 | Loss: 0.00001840
Iteration 7/1000 | Loss: 0.00001778
Iteration 8/1000 | Loss: 0.00001731
Iteration 9/1000 | Loss: 0.00001711
Iteration 10/1000 | Loss: 0.00001690
Iteration 11/1000 | Loss: 0.00001689
Iteration 12/1000 | Loss: 0.00001689
Iteration 13/1000 | Loss: 0.00001688
Iteration 14/1000 | Loss: 0.00001688
Iteration 15/1000 | Loss: 0.00001683
Iteration 16/1000 | Loss: 0.00001682
Iteration 17/1000 | Loss: 0.00001682
Iteration 18/1000 | Loss: 0.00001681
Iteration 19/1000 | Loss: 0.00001677
Iteration 20/1000 | Loss: 0.00001676
Iteration 21/1000 | Loss: 0.00001675
Iteration 22/1000 | Loss: 0.00001675
Iteration 23/1000 | Loss: 0.00001675
Iteration 24/1000 | Loss: 0.00001674
Iteration 25/1000 | Loss: 0.00001674
Iteration 26/1000 | Loss: 0.00001671
Iteration 27/1000 | Loss: 0.00001671
Iteration 28/1000 | Loss: 0.00001671
Iteration 29/1000 | Loss: 0.00001671
Iteration 30/1000 | Loss: 0.00001671
Iteration 31/1000 | Loss: 0.00001671
Iteration 32/1000 | Loss: 0.00001671
Iteration 33/1000 | Loss: 0.00001671
Iteration 34/1000 | Loss: 0.00001671
Iteration 35/1000 | Loss: 0.00001670
Iteration 36/1000 | Loss: 0.00001670
Iteration 37/1000 | Loss: 0.00001670
Iteration 38/1000 | Loss: 0.00001670
Iteration 39/1000 | Loss: 0.00001670
Iteration 40/1000 | Loss: 0.00001670
Iteration 41/1000 | Loss: 0.00001670
Iteration 42/1000 | Loss: 0.00001670
Iteration 43/1000 | Loss: 0.00001670
Iteration 44/1000 | Loss: 0.00001670
Iteration 45/1000 | Loss: 0.00001670
Iteration 46/1000 | Loss: 0.00001670
Iteration 47/1000 | Loss: 0.00001670
Iteration 48/1000 | Loss: 0.00001670
Iteration 49/1000 | Loss: 0.00001670
Iteration 50/1000 | Loss: 0.00001670
Iteration 51/1000 | Loss: 0.00001670
Iteration 52/1000 | Loss: 0.00001670
Iteration 53/1000 | Loss: 0.00001670
Iteration 54/1000 | Loss: 0.00001670
Iteration 55/1000 | Loss: 0.00001670
Iteration 56/1000 | Loss: 0.00001670
Iteration 57/1000 | Loss: 0.00001670
Iteration 58/1000 | Loss: 0.00001670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [1.6702977518434636e-05, 1.6702977518434636e-05, 1.6702977518434636e-05, 1.6702977518434636e-05, 1.6702977518434636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6702977518434636e-05

Optimization complete. Final v2v error: 3.424471616744995 mm

Highest mean error: 3.9858202934265137 mm for frame 97

Lowest mean error: 3.31156063079834 mm for frame 43

Saving results

Total time: 26.98473882675171
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00522064
Iteration 2/25 | Loss: 0.00111433
Iteration 3/25 | Loss: 0.00075983
Iteration 4/25 | Loss: 0.00070509
Iteration 5/25 | Loss: 0.00069215
Iteration 6/25 | Loss: 0.00068577
Iteration 7/25 | Loss: 0.00068429
Iteration 8/25 | Loss: 0.00068370
Iteration 9/25 | Loss: 0.00068351
Iteration 10/25 | Loss: 0.00068349
Iteration 11/25 | Loss: 0.00068349
Iteration 12/25 | Loss: 0.00068349
Iteration 13/25 | Loss: 0.00068349
Iteration 14/25 | Loss: 0.00068349
Iteration 15/25 | Loss: 0.00068349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006834901869297028, 0.0006834901869297028, 0.0006834901869297028, 0.0006834901869297028, 0.0006834901869297028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006834901869297028

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.06499434
Iteration 2/25 | Loss: 0.00030414
Iteration 3/25 | Loss: 0.00030412
Iteration 4/25 | Loss: 0.00030412
Iteration 5/25 | Loss: 0.00030412
Iteration 6/25 | Loss: 0.00030412
Iteration 7/25 | Loss: 0.00030412
Iteration 8/25 | Loss: 0.00030412
Iteration 9/25 | Loss: 0.00030412
Iteration 10/25 | Loss: 0.00030412
Iteration 11/25 | Loss: 0.00030412
Iteration 12/25 | Loss: 0.00030412
Iteration 13/25 | Loss: 0.00030412
Iteration 14/25 | Loss: 0.00030412
Iteration 15/25 | Loss: 0.00030412
Iteration 16/25 | Loss: 0.00030412
Iteration 17/25 | Loss: 0.00030412
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003041154413949698, 0.0003041154413949698, 0.0003041154413949698, 0.0003041154413949698, 0.0003041154413949698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003041154413949698

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030412
Iteration 2/1000 | Loss: 0.00003252
Iteration 3/1000 | Loss: 0.00002398
Iteration 4/1000 | Loss: 0.00002195
Iteration 5/1000 | Loss: 0.00002101
Iteration 6/1000 | Loss: 0.00002024
Iteration 7/1000 | Loss: 0.00001975
Iteration 8/1000 | Loss: 0.00001931
Iteration 9/1000 | Loss: 0.00001898
Iteration 10/1000 | Loss: 0.00001875
Iteration 11/1000 | Loss: 0.00001864
Iteration 12/1000 | Loss: 0.00001863
Iteration 13/1000 | Loss: 0.00001851
Iteration 14/1000 | Loss: 0.00001848
Iteration 15/1000 | Loss: 0.00001847
Iteration 16/1000 | Loss: 0.00001847
Iteration 17/1000 | Loss: 0.00001846
Iteration 18/1000 | Loss: 0.00001841
Iteration 19/1000 | Loss: 0.00001840
Iteration 20/1000 | Loss: 0.00001837
Iteration 21/1000 | Loss: 0.00001832
Iteration 22/1000 | Loss: 0.00001831
Iteration 23/1000 | Loss: 0.00001830
Iteration 24/1000 | Loss: 0.00001826
Iteration 25/1000 | Loss: 0.00001826
Iteration 26/1000 | Loss: 0.00001824
Iteration 27/1000 | Loss: 0.00001823
Iteration 28/1000 | Loss: 0.00001823
Iteration 29/1000 | Loss: 0.00001822
Iteration 30/1000 | Loss: 0.00001822
Iteration 31/1000 | Loss: 0.00001822
Iteration 32/1000 | Loss: 0.00001821
Iteration 33/1000 | Loss: 0.00001821
Iteration 34/1000 | Loss: 0.00001821
Iteration 35/1000 | Loss: 0.00001821
Iteration 36/1000 | Loss: 0.00001820
Iteration 37/1000 | Loss: 0.00001820
Iteration 38/1000 | Loss: 0.00001820
Iteration 39/1000 | Loss: 0.00001819
Iteration 40/1000 | Loss: 0.00001819
Iteration 41/1000 | Loss: 0.00001819
Iteration 42/1000 | Loss: 0.00001819
Iteration 43/1000 | Loss: 0.00001819
Iteration 44/1000 | Loss: 0.00001819
Iteration 45/1000 | Loss: 0.00001819
Iteration 46/1000 | Loss: 0.00001818
Iteration 47/1000 | Loss: 0.00001818
Iteration 48/1000 | Loss: 0.00001818
Iteration 49/1000 | Loss: 0.00001817
Iteration 50/1000 | Loss: 0.00001817
Iteration 51/1000 | Loss: 0.00001817
Iteration 52/1000 | Loss: 0.00001816
Iteration 53/1000 | Loss: 0.00001816
Iteration 54/1000 | Loss: 0.00001816
Iteration 55/1000 | Loss: 0.00001816
Iteration 56/1000 | Loss: 0.00001816
Iteration 57/1000 | Loss: 0.00001816
Iteration 58/1000 | Loss: 0.00001815
Iteration 59/1000 | Loss: 0.00001815
Iteration 60/1000 | Loss: 0.00001815
Iteration 61/1000 | Loss: 0.00001815
Iteration 62/1000 | Loss: 0.00001815
Iteration 63/1000 | Loss: 0.00001814
Iteration 64/1000 | Loss: 0.00001814
Iteration 65/1000 | Loss: 0.00001814
Iteration 66/1000 | Loss: 0.00001814
Iteration 67/1000 | Loss: 0.00001814
Iteration 68/1000 | Loss: 0.00001814
Iteration 69/1000 | Loss: 0.00001814
Iteration 70/1000 | Loss: 0.00001813
Iteration 71/1000 | Loss: 0.00001813
Iteration 72/1000 | Loss: 0.00001813
Iteration 73/1000 | Loss: 0.00001813
Iteration 74/1000 | Loss: 0.00001812
Iteration 75/1000 | Loss: 0.00001812
Iteration 76/1000 | Loss: 0.00001812
Iteration 77/1000 | Loss: 0.00001811
Iteration 78/1000 | Loss: 0.00001811
Iteration 79/1000 | Loss: 0.00001811
Iteration 80/1000 | Loss: 0.00001811
Iteration 81/1000 | Loss: 0.00001811
Iteration 82/1000 | Loss: 0.00001811
Iteration 83/1000 | Loss: 0.00001811
Iteration 84/1000 | Loss: 0.00001810
Iteration 85/1000 | Loss: 0.00001810
Iteration 86/1000 | Loss: 0.00001810
Iteration 87/1000 | Loss: 0.00001810
Iteration 88/1000 | Loss: 0.00001809
Iteration 89/1000 | Loss: 0.00001809
Iteration 90/1000 | Loss: 0.00001809
Iteration 91/1000 | Loss: 0.00001809
Iteration 92/1000 | Loss: 0.00001809
Iteration 93/1000 | Loss: 0.00001808
Iteration 94/1000 | Loss: 0.00001808
Iteration 95/1000 | Loss: 0.00001808
Iteration 96/1000 | Loss: 0.00001808
Iteration 97/1000 | Loss: 0.00001807
Iteration 98/1000 | Loss: 0.00001807
Iteration 99/1000 | Loss: 0.00001807
Iteration 100/1000 | Loss: 0.00001807
Iteration 101/1000 | Loss: 0.00001807
Iteration 102/1000 | Loss: 0.00001807
Iteration 103/1000 | Loss: 0.00001806
Iteration 104/1000 | Loss: 0.00001806
Iteration 105/1000 | Loss: 0.00001806
Iteration 106/1000 | Loss: 0.00001806
Iteration 107/1000 | Loss: 0.00001806
Iteration 108/1000 | Loss: 0.00001805
Iteration 109/1000 | Loss: 0.00001805
Iteration 110/1000 | Loss: 0.00001805
Iteration 111/1000 | Loss: 0.00001805
Iteration 112/1000 | Loss: 0.00001805
Iteration 113/1000 | Loss: 0.00001805
Iteration 114/1000 | Loss: 0.00001805
Iteration 115/1000 | Loss: 0.00001805
Iteration 116/1000 | Loss: 0.00001805
Iteration 117/1000 | Loss: 0.00001804
Iteration 118/1000 | Loss: 0.00001804
Iteration 119/1000 | Loss: 0.00001804
Iteration 120/1000 | Loss: 0.00001804
Iteration 121/1000 | Loss: 0.00001804
Iteration 122/1000 | Loss: 0.00001803
Iteration 123/1000 | Loss: 0.00001803
Iteration 124/1000 | Loss: 0.00001803
Iteration 125/1000 | Loss: 0.00001803
Iteration 126/1000 | Loss: 0.00001803
Iteration 127/1000 | Loss: 0.00001803
Iteration 128/1000 | Loss: 0.00001803
Iteration 129/1000 | Loss: 0.00001803
Iteration 130/1000 | Loss: 0.00001802
Iteration 131/1000 | Loss: 0.00001802
Iteration 132/1000 | Loss: 0.00001802
Iteration 133/1000 | Loss: 0.00001802
Iteration 134/1000 | Loss: 0.00001802
Iteration 135/1000 | Loss: 0.00001802
Iteration 136/1000 | Loss: 0.00001802
Iteration 137/1000 | Loss: 0.00001802
Iteration 138/1000 | Loss: 0.00001802
Iteration 139/1000 | Loss: 0.00001802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.802469341782853e-05, 1.802469341782853e-05, 1.802469341782853e-05, 1.802469341782853e-05, 1.802469341782853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.802469341782853e-05

Optimization complete. Final v2v error: 3.5415284633636475 mm

Highest mean error: 4.757317066192627 mm for frame 60

Lowest mean error: 2.796041250228882 mm for frame 72

Saving results

Total time: 47.399425983428955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884831
Iteration 2/25 | Loss: 0.00108723
Iteration 3/25 | Loss: 0.00076883
Iteration 4/25 | Loss: 0.00070495
Iteration 5/25 | Loss: 0.00069488
Iteration 6/25 | Loss: 0.00069720
Iteration 7/25 | Loss: 0.00070011
Iteration 8/25 | Loss: 0.00068899
Iteration 9/25 | Loss: 0.00068727
Iteration 10/25 | Loss: 0.00067709
Iteration 11/25 | Loss: 0.00067227
Iteration 12/25 | Loss: 0.00067123
Iteration 13/25 | Loss: 0.00067081
Iteration 14/25 | Loss: 0.00067058
Iteration 15/25 | Loss: 0.00067045
Iteration 16/25 | Loss: 0.00067031
Iteration 17/25 | Loss: 0.00067016
Iteration 18/25 | Loss: 0.00067009
Iteration 19/25 | Loss: 0.00067008
Iteration 20/25 | Loss: 0.00067008
Iteration 21/25 | Loss: 0.00067008
Iteration 22/25 | Loss: 0.00067008
Iteration 23/25 | Loss: 0.00067008
Iteration 24/25 | Loss: 0.00067008
Iteration 25/25 | Loss: 0.00067007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27495313
Iteration 2/25 | Loss: 0.00040205
Iteration 3/25 | Loss: 0.00040200
Iteration 4/25 | Loss: 0.00040200
Iteration 5/25 | Loss: 0.00040200
Iteration 6/25 | Loss: 0.00040200
Iteration 7/25 | Loss: 0.00040200
Iteration 8/25 | Loss: 0.00040200
Iteration 9/25 | Loss: 0.00040200
Iteration 10/25 | Loss: 0.00040200
Iteration 11/25 | Loss: 0.00040200
Iteration 12/25 | Loss: 0.00040200
Iteration 13/25 | Loss: 0.00040200
Iteration 14/25 | Loss: 0.00040200
Iteration 15/25 | Loss: 0.00040200
Iteration 16/25 | Loss: 0.00040200
Iteration 17/25 | Loss: 0.00040200
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004019968619104475, 0.0004019968619104475, 0.0004019968619104475, 0.0004019968619104475, 0.0004019968619104475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004019968619104475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040200
Iteration 2/1000 | Loss: 0.00005451
Iteration 3/1000 | Loss: 0.00004331
Iteration 4/1000 | Loss: 0.00003727
Iteration 5/1000 | Loss: 0.00003276
Iteration 6/1000 | Loss: 0.00002997
Iteration 7/1000 | Loss: 0.00002864
Iteration 8/1000 | Loss: 0.00002769
Iteration 9/1000 | Loss: 0.00067439
Iteration 10/1000 | Loss: 0.00004898
Iteration 11/1000 | Loss: 0.00002988
Iteration 12/1000 | Loss: 0.00002587
Iteration 13/1000 | Loss: 0.00002030
Iteration 14/1000 | Loss: 0.00001821
Iteration 15/1000 | Loss: 0.00001619
Iteration 16/1000 | Loss: 0.00001517
Iteration 17/1000 | Loss: 0.00001462
Iteration 18/1000 | Loss: 0.00001424
Iteration 19/1000 | Loss: 0.00001401
Iteration 20/1000 | Loss: 0.00001382
Iteration 21/1000 | Loss: 0.00001371
Iteration 22/1000 | Loss: 0.00001369
Iteration 23/1000 | Loss: 0.00001369
Iteration 24/1000 | Loss: 0.00001368
Iteration 25/1000 | Loss: 0.00001368
Iteration 26/1000 | Loss: 0.00001367
Iteration 27/1000 | Loss: 0.00001367
Iteration 28/1000 | Loss: 0.00001367
Iteration 29/1000 | Loss: 0.00001367
Iteration 30/1000 | Loss: 0.00001367
Iteration 31/1000 | Loss: 0.00001367
Iteration 32/1000 | Loss: 0.00001367
Iteration 33/1000 | Loss: 0.00001367
Iteration 34/1000 | Loss: 0.00001367
Iteration 35/1000 | Loss: 0.00001367
Iteration 36/1000 | Loss: 0.00001367
Iteration 37/1000 | Loss: 0.00001367
Iteration 38/1000 | Loss: 0.00001367
Iteration 39/1000 | Loss: 0.00001367
Iteration 40/1000 | Loss: 0.00001367
Iteration 41/1000 | Loss: 0.00001367
Iteration 42/1000 | Loss: 0.00001367
Iteration 43/1000 | Loss: 0.00001367
Iteration 44/1000 | Loss: 0.00001367
Iteration 45/1000 | Loss: 0.00001367
Iteration 46/1000 | Loss: 0.00001367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 46. Stopping optimization.
Last 5 losses: [1.3667388884641696e-05, 1.3667388884641696e-05, 1.3667388884641696e-05, 1.3667388884641696e-05, 1.3667388884641696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3667388884641696e-05

Optimization complete. Final v2v error: 3.1846837997436523 mm

Highest mean error: 3.8145062923431396 mm for frame 163

Lowest mean error: 2.8828392028808594 mm for frame 110

Saving results

Total time: 61.58673071861267
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00888606
Iteration 2/25 | Loss: 0.00122779
Iteration 3/25 | Loss: 0.00080093
Iteration 4/25 | Loss: 0.00074081
Iteration 5/25 | Loss: 0.00071783
Iteration 6/25 | Loss: 0.00071079
Iteration 7/25 | Loss: 0.00070850
Iteration 8/25 | Loss: 0.00070718
Iteration 9/25 | Loss: 0.00070675
Iteration 10/25 | Loss: 0.00070673
Iteration 11/25 | Loss: 0.00070673
Iteration 12/25 | Loss: 0.00070673
Iteration 13/25 | Loss: 0.00070673
Iteration 14/25 | Loss: 0.00070673
Iteration 15/25 | Loss: 0.00070673
Iteration 16/25 | Loss: 0.00070673
Iteration 17/25 | Loss: 0.00070673
Iteration 18/25 | Loss: 0.00070673
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007067272672429681, 0.0007067272672429681, 0.0007067272672429681, 0.0007067272672429681, 0.0007067272672429681]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007067272672429681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27108955
Iteration 2/25 | Loss: 0.00028032
Iteration 3/25 | Loss: 0.00028031
Iteration 4/25 | Loss: 0.00028031
Iteration 5/25 | Loss: 0.00028031
Iteration 6/25 | Loss: 0.00028031
Iteration 7/25 | Loss: 0.00028031
Iteration 8/25 | Loss: 0.00028031
Iteration 9/25 | Loss: 0.00028031
Iteration 10/25 | Loss: 0.00028031
Iteration 11/25 | Loss: 0.00028031
Iteration 12/25 | Loss: 0.00028031
Iteration 13/25 | Loss: 0.00028031
Iteration 14/25 | Loss: 0.00028031
Iteration 15/25 | Loss: 0.00028031
Iteration 16/25 | Loss: 0.00028031
Iteration 17/25 | Loss: 0.00028031
Iteration 18/25 | Loss: 0.00028031
Iteration 19/25 | Loss: 0.00028031
Iteration 20/25 | Loss: 0.00028031
Iteration 21/25 | Loss: 0.00028031
Iteration 22/25 | Loss: 0.00028031
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00028031200054101646, 0.00028031200054101646, 0.00028031200054101646, 0.00028031200054101646, 0.00028031200054101646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028031200054101646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028031
Iteration 2/1000 | Loss: 0.00004480
Iteration 3/1000 | Loss: 0.00003311
Iteration 4/1000 | Loss: 0.00002675
Iteration 5/1000 | Loss: 0.00002522
Iteration 6/1000 | Loss: 0.00002423
Iteration 7/1000 | Loss: 0.00002350
Iteration 8/1000 | Loss: 0.00002305
Iteration 9/1000 | Loss: 0.00002259
Iteration 10/1000 | Loss: 0.00002213
Iteration 11/1000 | Loss: 0.00002184
Iteration 12/1000 | Loss: 0.00002162
Iteration 13/1000 | Loss: 0.00002145
Iteration 14/1000 | Loss: 0.00002130
Iteration 15/1000 | Loss: 0.00002117
Iteration 16/1000 | Loss: 0.00002116
Iteration 17/1000 | Loss: 0.00002112
Iteration 18/1000 | Loss: 0.00002111
Iteration 19/1000 | Loss: 0.00002111
Iteration 20/1000 | Loss: 0.00002111
Iteration 21/1000 | Loss: 0.00002111
Iteration 22/1000 | Loss: 0.00002110
Iteration 23/1000 | Loss: 0.00002109
Iteration 24/1000 | Loss: 0.00002109
Iteration 25/1000 | Loss: 0.00002108
Iteration 26/1000 | Loss: 0.00002108
Iteration 27/1000 | Loss: 0.00002107
Iteration 28/1000 | Loss: 0.00002107
Iteration 29/1000 | Loss: 0.00002107
Iteration 30/1000 | Loss: 0.00002106
Iteration 31/1000 | Loss: 0.00002106
Iteration 32/1000 | Loss: 0.00002106
Iteration 33/1000 | Loss: 0.00002106
Iteration 34/1000 | Loss: 0.00002106
Iteration 35/1000 | Loss: 0.00002106
Iteration 36/1000 | Loss: 0.00002106
Iteration 37/1000 | Loss: 0.00002106
Iteration 38/1000 | Loss: 0.00002106
Iteration 39/1000 | Loss: 0.00002106
Iteration 40/1000 | Loss: 0.00002105
Iteration 41/1000 | Loss: 0.00002104
Iteration 42/1000 | Loss: 0.00002104
Iteration 43/1000 | Loss: 0.00002103
Iteration 44/1000 | Loss: 0.00002103
Iteration 45/1000 | Loss: 0.00002103
Iteration 46/1000 | Loss: 0.00002103
Iteration 47/1000 | Loss: 0.00002103
Iteration 48/1000 | Loss: 0.00002102
Iteration 49/1000 | Loss: 0.00002102
Iteration 50/1000 | Loss: 0.00002102
Iteration 51/1000 | Loss: 0.00002102
Iteration 52/1000 | Loss: 0.00002102
Iteration 53/1000 | Loss: 0.00002101
Iteration 54/1000 | Loss: 0.00002101
Iteration 55/1000 | Loss: 0.00002101
Iteration 56/1000 | Loss: 0.00002100
Iteration 57/1000 | Loss: 0.00002100
Iteration 58/1000 | Loss: 0.00002100
Iteration 59/1000 | Loss: 0.00002100
Iteration 60/1000 | Loss: 0.00002100
Iteration 61/1000 | Loss: 0.00002100
Iteration 62/1000 | Loss: 0.00002100
Iteration 63/1000 | Loss: 0.00002099
Iteration 64/1000 | Loss: 0.00002099
Iteration 65/1000 | Loss: 0.00002099
Iteration 66/1000 | Loss: 0.00002099
Iteration 67/1000 | Loss: 0.00002099
Iteration 68/1000 | Loss: 0.00002099
Iteration 69/1000 | Loss: 0.00002099
Iteration 70/1000 | Loss: 0.00002099
Iteration 71/1000 | Loss: 0.00002099
Iteration 72/1000 | Loss: 0.00002099
Iteration 73/1000 | Loss: 0.00002099
Iteration 74/1000 | Loss: 0.00002098
Iteration 75/1000 | Loss: 0.00002098
Iteration 76/1000 | Loss: 0.00002098
Iteration 77/1000 | Loss: 0.00002098
Iteration 78/1000 | Loss: 0.00002098
Iteration 79/1000 | Loss: 0.00002098
Iteration 80/1000 | Loss: 0.00002098
Iteration 81/1000 | Loss: 0.00002098
Iteration 82/1000 | Loss: 0.00002098
Iteration 83/1000 | Loss: 0.00002098
Iteration 84/1000 | Loss: 0.00002098
Iteration 85/1000 | Loss: 0.00002098
Iteration 86/1000 | Loss: 0.00002098
Iteration 87/1000 | Loss: 0.00002097
Iteration 88/1000 | Loss: 0.00002097
Iteration 89/1000 | Loss: 0.00002097
Iteration 90/1000 | Loss: 0.00002096
Iteration 91/1000 | Loss: 0.00002096
Iteration 92/1000 | Loss: 0.00002096
Iteration 93/1000 | Loss: 0.00002096
Iteration 94/1000 | Loss: 0.00002096
Iteration 95/1000 | Loss: 0.00002095
Iteration 96/1000 | Loss: 0.00002095
Iteration 97/1000 | Loss: 0.00002095
Iteration 98/1000 | Loss: 0.00002095
Iteration 99/1000 | Loss: 0.00002095
Iteration 100/1000 | Loss: 0.00002094
Iteration 101/1000 | Loss: 0.00002094
Iteration 102/1000 | Loss: 0.00002094
Iteration 103/1000 | Loss: 0.00002094
Iteration 104/1000 | Loss: 0.00002094
Iteration 105/1000 | Loss: 0.00002094
Iteration 106/1000 | Loss: 0.00002093
Iteration 107/1000 | Loss: 0.00002093
Iteration 108/1000 | Loss: 0.00002093
Iteration 109/1000 | Loss: 0.00002093
Iteration 110/1000 | Loss: 0.00002092
Iteration 111/1000 | Loss: 0.00002092
Iteration 112/1000 | Loss: 0.00002092
Iteration 113/1000 | Loss: 0.00002092
Iteration 114/1000 | Loss: 0.00002092
Iteration 115/1000 | Loss: 0.00002091
Iteration 116/1000 | Loss: 0.00002091
Iteration 117/1000 | Loss: 0.00002091
Iteration 118/1000 | Loss: 0.00002091
Iteration 119/1000 | Loss: 0.00002091
Iteration 120/1000 | Loss: 0.00002090
Iteration 121/1000 | Loss: 0.00002090
Iteration 122/1000 | Loss: 0.00002090
Iteration 123/1000 | Loss: 0.00002090
Iteration 124/1000 | Loss: 0.00002089
Iteration 125/1000 | Loss: 0.00002089
Iteration 126/1000 | Loss: 0.00002089
Iteration 127/1000 | Loss: 0.00002089
Iteration 128/1000 | Loss: 0.00002088
Iteration 129/1000 | Loss: 0.00002088
Iteration 130/1000 | Loss: 0.00002088
Iteration 131/1000 | Loss: 0.00002088
Iteration 132/1000 | Loss: 0.00002088
Iteration 133/1000 | Loss: 0.00002087
Iteration 134/1000 | Loss: 0.00002087
Iteration 135/1000 | Loss: 0.00002087
Iteration 136/1000 | Loss: 0.00002087
Iteration 137/1000 | Loss: 0.00002087
Iteration 138/1000 | Loss: 0.00002086
Iteration 139/1000 | Loss: 0.00002086
Iteration 140/1000 | Loss: 0.00002086
Iteration 141/1000 | Loss: 0.00002086
Iteration 142/1000 | Loss: 0.00002086
Iteration 143/1000 | Loss: 0.00002086
Iteration 144/1000 | Loss: 0.00002086
Iteration 145/1000 | Loss: 0.00002086
Iteration 146/1000 | Loss: 0.00002085
Iteration 147/1000 | Loss: 0.00002085
Iteration 148/1000 | Loss: 0.00002085
Iteration 149/1000 | Loss: 0.00002085
Iteration 150/1000 | Loss: 0.00002085
Iteration 151/1000 | Loss: 0.00002085
Iteration 152/1000 | Loss: 0.00002085
Iteration 153/1000 | Loss: 0.00002085
Iteration 154/1000 | Loss: 0.00002085
Iteration 155/1000 | Loss: 0.00002085
Iteration 156/1000 | Loss: 0.00002085
Iteration 157/1000 | Loss: 0.00002085
Iteration 158/1000 | Loss: 0.00002085
Iteration 159/1000 | Loss: 0.00002085
Iteration 160/1000 | Loss: 0.00002085
Iteration 161/1000 | Loss: 0.00002085
Iteration 162/1000 | Loss: 0.00002085
Iteration 163/1000 | Loss: 0.00002085
Iteration 164/1000 | Loss: 0.00002085
Iteration 165/1000 | Loss: 0.00002085
Iteration 166/1000 | Loss: 0.00002085
Iteration 167/1000 | Loss: 0.00002085
Iteration 168/1000 | Loss: 0.00002085
Iteration 169/1000 | Loss: 0.00002085
Iteration 170/1000 | Loss: 0.00002085
Iteration 171/1000 | Loss: 0.00002085
Iteration 172/1000 | Loss: 0.00002085
Iteration 173/1000 | Loss: 0.00002085
Iteration 174/1000 | Loss: 0.00002085
Iteration 175/1000 | Loss: 0.00002085
Iteration 176/1000 | Loss: 0.00002085
Iteration 177/1000 | Loss: 0.00002085
Iteration 178/1000 | Loss: 0.00002085
Iteration 179/1000 | Loss: 0.00002085
Iteration 180/1000 | Loss: 0.00002084
Iteration 181/1000 | Loss: 0.00002084
Iteration 182/1000 | Loss: 0.00002084
Iteration 183/1000 | Loss: 0.00002084
Iteration 184/1000 | Loss: 0.00002084
Iteration 185/1000 | Loss: 0.00002084
Iteration 186/1000 | Loss: 0.00002084
Iteration 187/1000 | Loss: 0.00002084
Iteration 188/1000 | Loss: 0.00002084
Iteration 189/1000 | Loss: 0.00002084
Iteration 190/1000 | Loss: 0.00002084
Iteration 191/1000 | Loss: 0.00002084
Iteration 192/1000 | Loss: 0.00002084
Iteration 193/1000 | Loss: 0.00002084
Iteration 194/1000 | Loss: 0.00002084
Iteration 195/1000 | Loss: 0.00002084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [2.084461084450595e-05, 2.084461084450595e-05, 2.084461084450595e-05, 2.084461084450595e-05, 2.084461084450595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.084461084450595e-05

Optimization complete. Final v2v error: 3.757443428039551 mm

Highest mean error: 6.075801849365234 mm for frame 91

Lowest mean error: 2.7705836296081543 mm for frame 13

Saving results

Total time: 45.424078702926636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045938
Iteration 2/25 | Loss: 0.00342271
Iteration 3/25 | Loss: 0.00177716
Iteration 4/25 | Loss: 0.00150637
Iteration 5/25 | Loss: 0.00139998
Iteration 6/25 | Loss: 0.00159130
Iteration 7/25 | Loss: 0.00167286
Iteration 8/25 | Loss: 0.00134031
Iteration 9/25 | Loss: 0.00101726
Iteration 10/25 | Loss: 0.00093056
Iteration 11/25 | Loss: 0.00082651
Iteration 12/25 | Loss: 0.00077631
Iteration 13/25 | Loss: 0.00077370
Iteration 14/25 | Loss: 0.00076940
Iteration 15/25 | Loss: 0.00074993
Iteration 16/25 | Loss: 0.00074028
Iteration 17/25 | Loss: 0.00072933
Iteration 18/25 | Loss: 0.00071788
Iteration 19/25 | Loss: 0.00071823
Iteration 20/25 | Loss: 0.00070878
Iteration 21/25 | Loss: 0.00072030
Iteration 22/25 | Loss: 0.00070858
Iteration 23/25 | Loss: 0.00070178
Iteration 24/25 | Loss: 0.00070291
Iteration 25/25 | Loss: 0.00071020

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45502174
Iteration 2/25 | Loss: 0.00120695
Iteration 3/25 | Loss: 0.00110367
Iteration 4/25 | Loss: 0.00110366
Iteration 5/25 | Loss: 0.00110366
Iteration 6/25 | Loss: 0.00110366
Iteration 7/25 | Loss: 0.00110366
Iteration 8/25 | Loss: 0.00110366
Iteration 9/25 | Loss: 0.00110366
Iteration 10/25 | Loss: 0.00110366
Iteration 11/25 | Loss: 0.00110366
Iteration 12/25 | Loss: 0.00110366
Iteration 13/25 | Loss: 0.00110366
Iteration 14/25 | Loss: 0.00110366
Iteration 15/25 | Loss: 0.00110366
Iteration 16/25 | Loss: 0.00110366
Iteration 17/25 | Loss: 0.00110366
Iteration 18/25 | Loss: 0.00110366
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011036607902497053, 0.0011036607902497053, 0.0011036607902497053, 0.0011036607902497053, 0.0011036607902497053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011036607902497053

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110366
Iteration 2/1000 | Loss: 0.00036717
Iteration 3/1000 | Loss: 0.00052541
Iteration 4/1000 | Loss: 0.00057473
Iteration 5/1000 | Loss: 0.00028808
Iteration 6/1000 | Loss: 0.00077083
Iteration 7/1000 | Loss: 0.00048046
Iteration 8/1000 | Loss: 0.00042824
Iteration 9/1000 | Loss: 0.00029051
Iteration 10/1000 | Loss: 0.00050768
Iteration 11/1000 | Loss: 0.00077268
Iteration 12/1000 | Loss: 0.00064080
Iteration 13/1000 | Loss: 0.00065605
Iteration 14/1000 | Loss: 0.00062708
Iteration 15/1000 | Loss: 0.00054791
Iteration 16/1000 | Loss: 0.00018638
Iteration 17/1000 | Loss: 0.00027162
Iteration 18/1000 | Loss: 0.00048297
Iteration 19/1000 | Loss: 0.00039027
Iteration 20/1000 | Loss: 0.00028078
Iteration 21/1000 | Loss: 0.00041500
Iteration 22/1000 | Loss: 0.00061403
Iteration 23/1000 | Loss: 0.00038039
Iteration 24/1000 | Loss: 0.00036599
Iteration 25/1000 | Loss: 0.00017710
Iteration 26/1000 | Loss: 0.00030531
Iteration 27/1000 | Loss: 0.00014638
Iteration 28/1000 | Loss: 0.00014311
Iteration 29/1000 | Loss: 0.00017163
Iteration 30/1000 | Loss: 0.00013736
Iteration 31/1000 | Loss: 0.00029242
Iteration 32/1000 | Loss: 0.00025505
Iteration 33/1000 | Loss: 0.00025396
Iteration 34/1000 | Loss: 0.00032282
Iteration 35/1000 | Loss: 0.00118649
Iteration 36/1000 | Loss: 0.00018899
Iteration 37/1000 | Loss: 0.00017743
Iteration 38/1000 | Loss: 0.00020127
Iteration 39/1000 | Loss: 0.00013234
Iteration 40/1000 | Loss: 0.00019971
Iteration 41/1000 | Loss: 0.00013017
Iteration 42/1000 | Loss: 0.00023977
Iteration 43/1000 | Loss: 0.00012171
Iteration 44/1000 | Loss: 0.00012167
Iteration 45/1000 | Loss: 0.00014105
Iteration 46/1000 | Loss: 0.00011592
Iteration 47/1000 | Loss: 0.00022460
Iteration 48/1000 | Loss: 0.00012839
Iteration 49/1000 | Loss: 0.00016916
Iteration 50/1000 | Loss: 0.00015369
Iteration 51/1000 | Loss: 0.00012543
Iteration 52/1000 | Loss: 0.00016056
Iteration 53/1000 | Loss: 0.00012976
Iteration 54/1000 | Loss: 0.00013343
Iteration 55/1000 | Loss: 0.00014484
Iteration 56/1000 | Loss: 0.00013207
Iteration 57/1000 | Loss: 0.00013902
Iteration 58/1000 | Loss: 0.00013179
Iteration 59/1000 | Loss: 0.00020121
Iteration 60/1000 | Loss: 0.00011243
Iteration 61/1000 | Loss: 0.00010762
Iteration 62/1000 | Loss: 0.00012992
Iteration 63/1000 | Loss: 0.00012780
Iteration 64/1000 | Loss: 0.00013151
Iteration 65/1000 | Loss: 0.00042551
Iteration 66/1000 | Loss: 0.00031578
Iteration 67/1000 | Loss: 0.00035270
Iteration 68/1000 | Loss: 0.00015129
Iteration 69/1000 | Loss: 0.00017778
Iteration 70/1000 | Loss: 0.00014753
Iteration 71/1000 | Loss: 0.00017202
Iteration 72/1000 | Loss: 0.00015722
Iteration 73/1000 | Loss: 0.00027656
Iteration 74/1000 | Loss: 0.00014023
Iteration 75/1000 | Loss: 0.00017305
Iteration 76/1000 | Loss: 0.00046245
Iteration 77/1000 | Loss: 0.00011393
Iteration 78/1000 | Loss: 0.00017218
Iteration 79/1000 | Loss: 0.00011847
Iteration 80/1000 | Loss: 0.00011687
Iteration 81/1000 | Loss: 0.00010614
Iteration 82/1000 | Loss: 0.00011170
Iteration 83/1000 | Loss: 0.00013478
Iteration 84/1000 | Loss: 0.00033443
Iteration 85/1000 | Loss: 0.00014044
Iteration 86/1000 | Loss: 0.00015165
Iteration 87/1000 | Loss: 0.00013627
Iteration 88/1000 | Loss: 0.00014717
Iteration 89/1000 | Loss: 0.00013675
Iteration 90/1000 | Loss: 0.00015764
Iteration 91/1000 | Loss: 0.00016504
Iteration 92/1000 | Loss: 0.00016515
Iteration 93/1000 | Loss: 0.00018813
Iteration 94/1000 | Loss: 0.00034994
Iteration 95/1000 | Loss: 0.00013387
Iteration 96/1000 | Loss: 0.00033316
Iteration 97/1000 | Loss: 0.00012809
Iteration 98/1000 | Loss: 0.00026645
Iteration 99/1000 | Loss: 0.00010559
Iteration 100/1000 | Loss: 0.00018094
Iteration 101/1000 | Loss: 0.00019913
Iteration 102/1000 | Loss: 0.00018783
Iteration 103/1000 | Loss: 0.00024936
Iteration 104/1000 | Loss: 0.00015154
Iteration 105/1000 | Loss: 0.00014219
Iteration 106/1000 | Loss: 0.00014915
Iteration 107/1000 | Loss: 0.00012156
Iteration 108/1000 | Loss: 0.00011663
Iteration 109/1000 | Loss: 0.00012066
Iteration 110/1000 | Loss: 0.00012936
Iteration 111/1000 | Loss: 0.00012193
Iteration 112/1000 | Loss: 0.00027409
Iteration 113/1000 | Loss: 0.00024615
Iteration 114/1000 | Loss: 0.00013825
Iteration 115/1000 | Loss: 0.00011984
Iteration 116/1000 | Loss: 0.00012745
Iteration 117/1000 | Loss: 0.00011624
Iteration 118/1000 | Loss: 0.00012847
Iteration 119/1000 | Loss: 0.00011165
Iteration 120/1000 | Loss: 0.00018868
Iteration 121/1000 | Loss: 0.00008121
Iteration 122/1000 | Loss: 0.00010285
Iteration 123/1000 | Loss: 0.00010368
Iteration 124/1000 | Loss: 0.00012959
Iteration 125/1000 | Loss: 0.00012124
Iteration 126/1000 | Loss: 0.00011240
Iteration 127/1000 | Loss: 0.00010301
Iteration 128/1000 | Loss: 0.00092957
Iteration 129/1000 | Loss: 0.00048168
Iteration 130/1000 | Loss: 0.00012083
Iteration 131/1000 | Loss: 0.00010137
Iteration 132/1000 | Loss: 0.00012919
Iteration 133/1000 | Loss: 0.00011008
Iteration 134/1000 | Loss: 0.00010776
Iteration 135/1000 | Loss: 0.00009113
Iteration 136/1000 | Loss: 0.00008839
Iteration 137/1000 | Loss: 0.00169021
Iteration 138/1000 | Loss: 0.00651873
Iteration 139/1000 | Loss: 0.00534793
Iteration 140/1000 | Loss: 0.00028514
Iteration 141/1000 | Loss: 0.00020656
Iteration 142/1000 | Loss: 0.00088216
Iteration 143/1000 | Loss: 0.00555644
Iteration 144/1000 | Loss: 0.00214528
Iteration 145/1000 | Loss: 0.00604408
Iteration 146/1000 | Loss: 0.00288490
Iteration 147/1000 | Loss: 0.00387350
Iteration 148/1000 | Loss: 0.00205297
Iteration 149/1000 | Loss: 0.00200369
Iteration 150/1000 | Loss: 0.00238395
Iteration 151/1000 | Loss: 0.00239177
Iteration 152/1000 | Loss: 0.00054506
Iteration 153/1000 | Loss: 0.00048536
Iteration 154/1000 | Loss: 0.00119977
Iteration 155/1000 | Loss: 0.00070745
Iteration 156/1000 | Loss: 0.00204011
Iteration 157/1000 | Loss: 0.00545587
Iteration 158/1000 | Loss: 0.00378092
Iteration 159/1000 | Loss: 0.00217225
Iteration 160/1000 | Loss: 0.00155951
Iteration 161/1000 | Loss: 0.00092168
Iteration 162/1000 | Loss: 0.00026781
Iteration 163/1000 | Loss: 0.00037094
Iteration 164/1000 | Loss: 0.00020415
Iteration 165/1000 | Loss: 0.00010968
Iteration 166/1000 | Loss: 0.00012759
Iteration 167/1000 | Loss: 0.00020121
Iteration 168/1000 | Loss: 0.00007716
Iteration 169/1000 | Loss: 0.00015678
Iteration 170/1000 | Loss: 0.00034975
Iteration 171/1000 | Loss: 0.00084794
Iteration 172/1000 | Loss: 0.00104614
Iteration 173/1000 | Loss: 0.00112291
Iteration 174/1000 | Loss: 0.00005534
Iteration 175/1000 | Loss: 0.00005171
Iteration 176/1000 | Loss: 0.00006401
Iteration 177/1000 | Loss: 0.00005065
Iteration 178/1000 | Loss: 0.00004151
Iteration 179/1000 | Loss: 0.00007251
Iteration 180/1000 | Loss: 0.00119346
Iteration 181/1000 | Loss: 0.00007599
Iteration 182/1000 | Loss: 0.00009401
Iteration 183/1000 | Loss: 0.00104074
Iteration 184/1000 | Loss: 0.00035881
Iteration 185/1000 | Loss: 0.00004287
Iteration 186/1000 | Loss: 0.00007964
Iteration 187/1000 | Loss: 0.00123233
Iteration 188/1000 | Loss: 0.00068912
Iteration 189/1000 | Loss: 0.00024334
Iteration 190/1000 | Loss: 0.00009320
Iteration 191/1000 | Loss: 0.00010701
Iteration 192/1000 | Loss: 0.00013615
Iteration 193/1000 | Loss: 0.00003169
Iteration 194/1000 | Loss: 0.00003524
Iteration 195/1000 | Loss: 0.00014290
Iteration 196/1000 | Loss: 0.00004915
Iteration 197/1000 | Loss: 0.00002556
Iteration 198/1000 | Loss: 0.00001983
Iteration 199/1000 | Loss: 0.00005143
Iteration 200/1000 | Loss: 0.00001797
Iteration 201/1000 | Loss: 0.00002034
Iteration 202/1000 | Loss: 0.00001444
Iteration 203/1000 | Loss: 0.00003466
Iteration 204/1000 | Loss: 0.00001981
Iteration 205/1000 | Loss: 0.00001394
Iteration 206/1000 | Loss: 0.00001728
Iteration 207/1000 | Loss: 0.00001429
Iteration 208/1000 | Loss: 0.00003667
Iteration 209/1000 | Loss: 0.00002796
Iteration 210/1000 | Loss: 0.00001592
Iteration 211/1000 | Loss: 0.00001821
Iteration 212/1000 | Loss: 0.00001428
Iteration 213/1000 | Loss: 0.00001353
Iteration 214/1000 | Loss: 0.00001340
Iteration 215/1000 | Loss: 0.00001340
Iteration 216/1000 | Loss: 0.00001340
Iteration 217/1000 | Loss: 0.00001340
Iteration 218/1000 | Loss: 0.00001340
Iteration 219/1000 | Loss: 0.00001340
Iteration 220/1000 | Loss: 0.00001340
Iteration 221/1000 | Loss: 0.00001339
Iteration 222/1000 | Loss: 0.00001339
Iteration 223/1000 | Loss: 0.00001339
Iteration 224/1000 | Loss: 0.00001339
Iteration 225/1000 | Loss: 0.00001410
Iteration 226/1000 | Loss: 0.00001405
Iteration 227/1000 | Loss: 0.00001360
Iteration 228/1000 | Loss: 0.00001589
Iteration 229/1000 | Loss: 0.00018780
Iteration 230/1000 | Loss: 0.00001485
Iteration 231/1000 | Loss: 0.00001332
Iteration 232/1000 | Loss: 0.00001331
Iteration 233/1000 | Loss: 0.00001331
Iteration 234/1000 | Loss: 0.00001423
Iteration 235/1000 | Loss: 0.00002710
Iteration 236/1000 | Loss: 0.00001340
Iteration 237/1000 | Loss: 0.00001350
Iteration 238/1000 | Loss: 0.00001329
Iteration 239/1000 | Loss: 0.00001325
Iteration 240/1000 | Loss: 0.00001325
Iteration 241/1000 | Loss: 0.00001325
Iteration 242/1000 | Loss: 0.00001325
Iteration 243/1000 | Loss: 0.00001325
Iteration 244/1000 | Loss: 0.00001325
Iteration 245/1000 | Loss: 0.00001325
Iteration 246/1000 | Loss: 0.00001324
Iteration 247/1000 | Loss: 0.00001324
Iteration 248/1000 | Loss: 0.00001324
Iteration 249/1000 | Loss: 0.00001324
Iteration 250/1000 | Loss: 0.00001324
Iteration 251/1000 | Loss: 0.00001324
Iteration 252/1000 | Loss: 0.00001324
Iteration 253/1000 | Loss: 0.00001324
Iteration 254/1000 | Loss: 0.00001324
Iteration 255/1000 | Loss: 0.00001324
Iteration 256/1000 | Loss: 0.00001324
Iteration 257/1000 | Loss: 0.00001324
Iteration 258/1000 | Loss: 0.00001324
Iteration 259/1000 | Loss: 0.00001324
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.3244374713394791e-05, 1.3244374713394791e-05, 1.3244374713394791e-05, 1.3244374713394791e-05, 1.3244374713394791e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3244374713394791e-05

Optimization complete. Final v2v error: 2.7813942432403564 mm

Highest mean error: 9.830338478088379 mm for frame 74

Lowest mean error: 2.248443603515625 mm for frame 49

Saving results

Total time: 341.17582178115845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391182
Iteration 2/25 | Loss: 0.00085503
Iteration 3/25 | Loss: 0.00065669
Iteration 4/25 | Loss: 0.00062708
Iteration 5/25 | Loss: 0.00061688
Iteration 6/25 | Loss: 0.00061389
Iteration 7/25 | Loss: 0.00061302
Iteration 8/25 | Loss: 0.00061302
Iteration 9/25 | Loss: 0.00061302
Iteration 10/25 | Loss: 0.00061302
Iteration 11/25 | Loss: 0.00061302
Iteration 12/25 | Loss: 0.00061302
Iteration 13/25 | Loss: 0.00061302
Iteration 14/25 | Loss: 0.00061302
Iteration 15/25 | Loss: 0.00061302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006130210240371525, 0.0006130210240371525, 0.0006130210240371525, 0.0006130210240371525, 0.0006130210240371525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006130210240371525

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34735179
Iteration 2/25 | Loss: 0.00028504
Iteration 3/25 | Loss: 0.00028501
Iteration 4/25 | Loss: 0.00028501
Iteration 5/25 | Loss: 0.00028501
Iteration 6/25 | Loss: 0.00028501
Iteration 7/25 | Loss: 0.00028501
Iteration 8/25 | Loss: 0.00028501
Iteration 9/25 | Loss: 0.00028501
Iteration 10/25 | Loss: 0.00028501
Iteration 11/25 | Loss: 0.00028501
Iteration 12/25 | Loss: 0.00028501
Iteration 13/25 | Loss: 0.00028501
Iteration 14/25 | Loss: 0.00028501
Iteration 15/25 | Loss: 0.00028501
Iteration 16/25 | Loss: 0.00028501
Iteration 17/25 | Loss: 0.00028501
Iteration 18/25 | Loss: 0.00028501
Iteration 19/25 | Loss: 0.00028501
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0002850112214218825, 0.0002850112214218825, 0.0002850112214218825, 0.0002850112214218825, 0.0002850112214218825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002850112214218825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028501
Iteration 2/1000 | Loss: 0.00003141
Iteration 3/1000 | Loss: 0.00002099
Iteration 4/1000 | Loss: 0.00001659
Iteration 5/1000 | Loss: 0.00001565
Iteration 6/1000 | Loss: 0.00001509
Iteration 7/1000 | Loss: 0.00001461
Iteration 8/1000 | Loss: 0.00001432
Iteration 9/1000 | Loss: 0.00001404
Iteration 10/1000 | Loss: 0.00001389
Iteration 11/1000 | Loss: 0.00001386
Iteration 12/1000 | Loss: 0.00001384
Iteration 13/1000 | Loss: 0.00001383
Iteration 14/1000 | Loss: 0.00001383
Iteration 15/1000 | Loss: 0.00001377
Iteration 16/1000 | Loss: 0.00001376
Iteration 17/1000 | Loss: 0.00001376
Iteration 18/1000 | Loss: 0.00001375
Iteration 19/1000 | Loss: 0.00001375
Iteration 20/1000 | Loss: 0.00001370
Iteration 21/1000 | Loss: 0.00001368
Iteration 22/1000 | Loss: 0.00001368
Iteration 23/1000 | Loss: 0.00001367
Iteration 24/1000 | Loss: 0.00001367
Iteration 25/1000 | Loss: 0.00001366
Iteration 26/1000 | Loss: 0.00001366
Iteration 27/1000 | Loss: 0.00001365
Iteration 28/1000 | Loss: 0.00001364
Iteration 29/1000 | Loss: 0.00001362
Iteration 30/1000 | Loss: 0.00001362
Iteration 31/1000 | Loss: 0.00001361
Iteration 32/1000 | Loss: 0.00001361
Iteration 33/1000 | Loss: 0.00001360
Iteration 34/1000 | Loss: 0.00001360
Iteration 35/1000 | Loss: 0.00001359
Iteration 36/1000 | Loss: 0.00001358
Iteration 37/1000 | Loss: 0.00001357
Iteration 38/1000 | Loss: 0.00001353
Iteration 39/1000 | Loss: 0.00001353
Iteration 40/1000 | Loss: 0.00001352
Iteration 41/1000 | Loss: 0.00001351
Iteration 42/1000 | Loss: 0.00001351
Iteration 43/1000 | Loss: 0.00001350
Iteration 44/1000 | Loss: 0.00001350
Iteration 45/1000 | Loss: 0.00001350
Iteration 46/1000 | Loss: 0.00001350
Iteration 47/1000 | Loss: 0.00001350
Iteration 48/1000 | Loss: 0.00001350
Iteration 49/1000 | Loss: 0.00001350
Iteration 50/1000 | Loss: 0.00001349
Iteration 51/1000 | Loss: 0.00001349
Iteration 52/1000 | Loss: 0.00001348
Iteration 53/1000 | Loss: 0.00001348
Iteration 54/1000 | Loss: 0.00001348
Iteration 55/1000 | Loss: 0.00001348
Iteration 56/1000 | Loss: 0.00001347
Iteration 57/1000 | Loss: 0.00001347
Iteration 58/1000 | Loss: 0.00001347
Iteration 59/1000 | Loss: 0.00001347
Iteration 60/1000 | Loss: 0.00001347
Iteration 61/1000 | Loss: 0.00001347
Iteration 62/1000 | Loss: 0.00001347
Iteration 63/1000 | Loss: 0.00001347
Iteration 64/1000 | Loss: 0.00001346
Iteration 65/1000 | Loss: 0.00001346
Iteration 66/1000 | Loss: 0.00001346
Iteration 67/1000 | Loss: 0.00001345
Iteration 68/1000 | Loss: 0.00001345
Iteration 69/1000 | Loss: 0.00001345
Iteration 70/1000 | Loss: 0.00001345
Iteration 71/1000 | Loss: 0.00001344
Iteration 72/1000 | Loss: 0.00001344
Iteration 73/1000 | Loss: 0.00001343
Iteration 74/1000 | Loss: 0.00001343
Iteration 75/1000 | Loss: 0.00001342
Iteration 76/1000 | Loss: 0.00001342
Iteration 77/1000 | Loss: 0.00001342
Iteration 78/1000 | Loss: 0.00001341
Iteration 79/1000 | Loss: 0.00001341
Iteration 80/1000 | Loss: 0.00001341
Iteration 81/1000 | Loss: 0.00001340
Iteration 82/1000 | Loss: 0.00001340
Iteration 83/1000 | Loss: 0.00001340
Iteration 84/1000 | Loss: 0.00001339
Iteration 85/1000 | Loss: 0.00001339
Iteration 86/1000 | Loss: 0.00001338
Iteration 87/1000 | Loss: 0.00001338
Iteration 88/1000 | Loss: 0.00001338
Iteration 89/1000 | Loss: 0.00001337
Iteration 90/1000 | Loss: 0.00001337
Iteration 91/1000 | Loss: 0.00001337
Iteration 92/1000 | Loss: 0.00001337
Iteration 93/1000 | Loss: 0.00001337
Iteration 94/1000 | Loss: 0.00001337
Iteration 95/1000 | Loss: 0.00001337
Iteration 96/1000 | Loss: 0.00001337
Iteration 97/1000 | Loss: 0.00001337
Iteration 98/1000 | Loss: 0.00001336
Iteration 99/1000 | Loss: 0.00001336
Iteration 100/1000 | Loss: 0.00001335
Iteration 101/1000 | Loss: 0.00001335
Iteration 102/1000 | Loss: 0.00001335
Iteration 103/1000 | Loss: 0.00001335
Iteration 104/1000 | Loss: 0.00001334
Iteration 105/1000 | Loss: 0.00001334
Iteration 106/1000 | Loss: 0.00001334
Iteration 107/1000 | Loss: 0.00001334
Iteration 108/1000 | Loss: 0.00001334
Iteration 109/1000 | Loss: 0.00001334
Iteration 110/1000 | Loss: 0.00001334
Iteration 111/1000 | Loss: 0.00001333
Iteration 112/1000 | Loss: 0.00001333
Iteration 113/1000 | Loss: 0.00001333
Iteration 114/1000 | Loss: 0.00001333
Iteration 115/1000 | Loss: 0.00001333
Iteration 116/1000 | Loss: 0.00001333
Iteration 117/1000 | Loss: 0.00001333
Iteration 118/1000 | Loss: 0.00001333
Iteration 119/1000 | Loss: 0.00001333
Iteration 120/1000 | Loss: 0.00001333
Iteration 121/1000 | Loss: 0.00001333
Iteration 122/1000 | Loss: 0.00001333
Iteration 123/1000 | Loss: 0.00001333
Iteration 124/1000 | Loss: 0.00001333
Iteration 125/1000 | Loss: 0.00001333
Iteration 126/1000 | Loss: 0.00001332
Iteration 127/1000 | Loss: 0.00001332
Iteration 128/1000 | Loss: 0.00001332
Iteration 129/1000 | Loss: 0.00001332
Iteration 130/1000 | Loss: 0.00001332
Iteration 131/1000 | Loss: 0.00001332
Iteration 132/1000 | Loss: 0.00001332
Iteration 133/1000 | Loss: 0.00001332
Iteration 134/1000 | Loss: 0.00001331
Iteration 135/1000 | Loss: 0.00001331
Iteration 136/1000 | Loss: 0.00001331
Iteration 137/1000 | Loss: 0.00001331
Iteration 138/1000 | Loss: 0.00001331
Iteration 139/1000 | Loss: 0.00001331
Iteration 140/1000 | Loss: 0.00001331
Iteration 141/1000 | Loss: 0.00001331
Iteration 142/1000 | Loss: 0.00001331
Iteration 143/1000 | Loss: 0.00001331
Iteration 144/1000 | Loss: 0.00001331
Iteration 145/1000 | Loss: 0.00001331
Iteration 146/1000 | Loss: 0.00001331
Iteration 147/1000 | Loss: 0.00001331
Iteration 148/1000 | Loss: 0.00001331
Iteration 149/1000 | Loss: 0.00001331
Iteration 150/1000 | Loss: 0.00001331
Iteration 151/1000 | Loss: 0.00001331
Iteration 152/1000 | Loss: 0.00001331
Iteration 153/1000 | Loss: 0.00001331
Iteration 154/1000 | Loss: 0.00001331
Iteration 155/1000 | Loss: 0.00001331
Iteration 156/1000 | Loss: 0.00001331
Iteration 157/1000 | Loss: 0.00001331
Iteration 158/1000 | Loss: 0.00001331
Iteration 159/1000 | Loss: 0.00001331
Iteration 160/1000 | Loss: 0.00001331
Iteration 161/1000 | Loss: 0.00001331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.3307504559634253e-05, 1.3307504559634253e-05, 1.3307504559634253e-05, 1.3307504559634253e-05, 1.3307504559634253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3307504559634253e-05

Optimization complete. Final v2v error: 2.920534610748291 mm

Highest mean error: 5.025729179382324 mm for frame 87

Lowest mean error: 2.3036088943481445 mm for frame 128

Saving results

Total time: 38.43112516403198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015917
Iteration 2/25 | Loss: 0.00277933
Iteration 3/25 | Loss: 0.00215071
Iteration 4/25 | Loss: 0.00200365
Iteration 5/25 | Loss: 0.00190917
Iteration 6/25 | Loss: 0.00164669
Iteration 7/25 | Loss: 0.00161148
Iteration 8/25 | Loss: 0.00155299
Iteration 9/25 | Loss: 0.00154263
Iteration 10/25 | Loss: 0.00152281
Iteration 11/25 | Loss: 0.00151223
Iteration 12/25 | Loss: 0.00151972
Iteration 13/25 | Loss: 0.00149520
Iteration 14/25 | Loss: 0.00149476
Iteration 15/25 | Loss: 0.00149982
Iteration 16/25 | Loss: 0.00148979
Iteration 17/25 | Loss: 0.00148082
Iteration 18/25 | Loss: 0.00147609
Iteration 19/25 | Loss: 0.00147470
Iteration 20/25 | Loss: 0.00147401
Iteration 21/25 | Loss: 0.00147919
Iteration 22/25 | Loss: 0.00147410
Iteration 23/25 | Loss: 0.00147204
Iteration 24/25 | Loss: 0.00147139
Iteration 25/25 | Loss: 0.00147123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46983957
Iteration 2/25 | Loss: 0.00475384
Iteration 3/25 | Loss: 0.00475384
Iteration 4/25 | Loss: 0.00475384
Iteration 5/25 | Loss: 0.00475384
Iteration 6/25 | Loss: 0.00475384
Iteration 7/25 | Loss: 0.00475384
Iteration 8/25 | Loss: 0.00475384
Iteration 9/25 | Loss: 0.00475384
Iteration 10/25 | Loss: 0.00475384
Iteration 11/25 | Loss: 0.00475384
Iteration 12/25 | Loss: 0.00475384
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.004753837827593088, 0.004753837827593088, 0.004753837827593088, 0.004753837827593088, 0.004753837827593088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004753837827593088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00475384
Iteration 2/1000 | Loss: 0.00095496
Iteration 3/1000 | Loss: 0.00537277
Iteration 4/1000 | Loss: 0.00095736
Iteration 5/1000 | Loss: 0.00296691
Iteration 6/1000 | Loss: 0.00335338
Iteration 7/1000 | Loss: 0.00292992
Iteration 8/1000 | Loss: 0.00349636
Iteration 9/1000 | Loss: 0.00212632
Iteration 10/1000 | Loss: 0.00078749
Iteration 11/1000 | Loss: 0.00052827
Iteration 12/1000 | Loss: 0.00041972
Iteration 13/1000 | Loss: 0.00035166
Iteration 14/1000 | Loss: 0.00029880
Iteration 15/1000 | Loss: 0.00032414
Iteration 16/1000 | Loss: 0.00025444
Iteration 17/1000 | Loss: 0.00023845
Iteration 18/1000 | Loss: 0.00022535
Iteration 19/1000 | Loss: 0.00021732
Iteration 20/1000 | Loss: 0.00021105
Iteration 21/1000 | Loss: 0.00020671
Iteration 22/1000 | Loss: 0.00020351
Iteration 23/1000 | Loss: 0.00020083
Iteration 24/1000 | Loss: 0.00019909
Iteration 25/1000 | Loss: 0.00109437
Iteration 26/1000 | Loss: 0.01996718
Iteration 27/1000 | Loss: 0.00197163
Iteration 28/1000 | Loss: 0.00390034
Iteration 29/1000 | Loss: 0.00062981
Iteration 30/1000 | Loss: 0.00045935
Iteration 31/1000 | Loss: 0.00088370
Iteration 32/1000 | Loss: 0.00062436
Iteration 33/1000 | Loss: 0.00081165
Iteration 34/1000 | Loss: 0.00029594
Iteration 35/1000 | Loss: 0.00010304
Iteration 36/1000 | Loss: 0.00007041
Iteration 37/1000 | Loss: 0.00039465
Iteration 38/1000 | Loss: 0.00050657
Iteration 39/1000 | Loss: 0.00015569
Iteration 40/1000 | Loss: 0.00004350
Iteration 41/1000 | Loss: 0.00005892
Iteration 42/1000 | Loss: 0.00003326
Iteration 43/1000 | Loss: 0.00002893
Iteration 44/1000 | Loss: 0.00002519
Iteration 45/1000 | Loss: 0.00002336
Iteration 46/1000 | Loss: 0.00002174
Iteration 47/1000 | Loss: 0.00001991
Iteration 48/1000 | Loss: 0.00001905
Iteration 49/1000 | Loss: 0.00001832
Iteration 50/1000 | Loss: 0.00001784
Iteration 51/1000 | Loss: 0.00001762
Iteration 52/1000 | Loss: 0.00001742
Iteration 53/1000 | Loss: 0.00001738
Iteration 54/1000 | Loss: 0.00001732
Iteration 55/1000 | Loss: 0.00001727
Iteration 56/1000 | Loss: 0.00001725
Iteration 57/1000 | Loss: 0.00001724
Iteration 58/1000 | Loss: 0.00001724
Iteration 59/1000 | Loss: 0.00001724
Iteration 60/1000 | Loss: 0.00001723
Iteration 61/1000 | Loss: 0.00001722
Iteration 62/1000 | Loss: 0.00001719
Iteration 63/1000 | Loss: 0.00001719
Iteration 64/1000 | Loss: 0.00001719
Iteration 65/1000 | Loss: 0.00001719
Iteration 66/1000 | Loss: 0.00001719
Iteration 67/1000 | Loss: 0.00001719
Iteration 68/1000 | Loss: 0.00001719
Iteration 69/1000 | Loss: 0.00001719
Iteration 70/1000 | Loss: 0.00001719
Iteration 71/1000 | Loss: 0.00001719
Iteration 72/1000 | Loss: 0.00001719
Iteration 73/1000 | Loss: 0.00001718
Iteration 74/1000 | Loss: 0.00001718
Iteration 75/1000 | Loss: 0.00001718
Iteration 76/1000 | Loss: 0.00001718
Iteration 77/1000 | Loss: 0.00001717
Iteration 78/1000 | Loss: 0.00001717
Iteration 79/1000 | Loss: 0.00001717
Iteration 80/1000 | Loss: 0.00001715
Iteration 81/1000 | Loss: 0.00001715
Iteration 82/1000 | Loss: 0.00001715
Iteration 83/1000 | Loss: 0.00001714
Iteration 84/1000 | Loss: 0.00001714
Iteration 85/1000 | Loss: 0.00001713
Iteration 86/1000 | Loss: 0.00001713
Iteration 87/1000 | Loss: 0.00001712
Iteration 88/1000 | Loss: 0.00001711
Iteration 89/1000 | Loss: 0.00001711
Iteration 90/1000 | Loss: 0.00001711
Iteration 91/1000 | Loss: 0.00001711
Iteration 92/1000 | Loss: 0.00001711
Iteration 93/1000 | Loss: 0.00001711
Iteration 94/1000 | Loss: 0.00001711
Iteration 95/1000 | Loss: 0.00001711
Iteration 96/1000 | Loss: 0.00001710
Iteration 97/1000 | Loss: 0.00001710
Iteration 98/1000 | Loss: 0.00001709
Iteration 99/1000 | Loss: 0.00001708
Iteration 100/1000 | Loss: 0.00001708
Iteration 101/1000 | Loss: 0.00001708
Iteration 102/1000 | Loss: 0.00001708
Iteration 103/1000 | Loss: 0.00001708
Iteration 104/1000 | Loss: 0.00001708
Iteration 105/1000 | Loss: 0.00001708
Iteration 106/1000 | Loss: 0.00001708
Iteration 107/1000 | Loss: 0.00001708
Iteration 108/1000 | Loss: 0.00001708
Iteration 109/1000 | Loss: 0.00001708
Iteration 110/1000 | Loss: 0.00001708
Iteration 111/1000 | Loss: 0.00001707
Iteration 112/1000 | Loss: 0.00001707
Iteration 113/1000 | Loss: 0.00001707
Iteration 114/1000 | Loss: 0.00001707
Iteration 115/1000 | Loss: 0.00001706
Iteration 116/1000 | Loss: 0.00001706
Iteration 117/1000 | Loss: 0.00001706
Iteration 118/1000 | Loss: 0.00001706
Iteration 119/1000 | Loss: 0.00001706
Iteration 120/1000 | Loss: 0.00001706
Iteration 121/1000 | Loss: 0.00001706
Iteration 122/1000 | Loss: 0.00001705
Iteration 123/1000 | Loss: 0.00001705
Iteration 124/1000 | Loss: 0.00001705
Iteration 125/1000 | Loss: 0.00001705
Iteration 126/1000 | Loss: 0.00001705
Iteration 127/1000 | Loss: 0.00001705
Iteration 128/1000 | Loss: 0.00001705
Iteration 129/1000 | Loss: 0.00001705
Iteration 130/1000 | Loss: 0.00001704
Iteration 131/1000 | Loss: 0.00001704
Iteration 132/1000 | Loss: 0.00001704
Iteration 133/1000 | Loss: 0.00001704
Iteration 134/1000 | Loss: 0.00001704
Iteration 135/1000 | Loss: 0.00001704
Iteration 136/1000 | Loss: 0.00001704
Iteration 137/1000 | Loss: 0.00001704
Iteration 138/1000 | Loss: 0.00001704
Iteration 139/1000 | Loss: 0.00001704
Iteration 140/1000 | Loss: 0.00001704
Iteration 141/1000 | Loss: 0.00001704
Iteration 142/1000 | Loss: 0.00001704
Iteration 143/1000 | Loss: 0.00001704
Iteration 144/1000 | Loss: 0.00001704
Iteration 145/1000 | Loss: 0.00001704
Iteration 146/1000 | Loss: 0.00001704
Iteration 147/1000 | Loss: 0.00001704
Iteration 148/1000 | Loss: 0.00001704
Iteration 149/1000 | Loss: 0.00001704
Iteration 150/1000 | Loss: 0.00001704
Iteration 151/1000 | Loss: 0.00001704
Iteration 152/1000 | Loss: 0.00001704
Iteration 153/1000 | Loss: 0.00001704
Iteration 154/1000 | Loss: 0.00001704
Iteration 155/1000 | Loss: 0.00001704
Iteration 156/1000 | Loss: 0.00001704
Iteration 157/1000 | Loss: 0.00001704
Iteration 158/1000 | Loss: 0.00001704
Iteration 159/1000 | Loss: 0.00001704
Iteration 160/1000 | Loss: 0.00001704
Iteration 161/1000 | Loss: 0.00001704
Iteration 162/1000 | Loss: 0.00001704
Iteration 163/1000 | Loss: 0.00001704
Iteration 164/1000 | Loss: 0.00001704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.703676753095351e-05, 1.703676753095351e-05, 1.703676753095351e-05, 1.703676753095351e-05, 1.703676753095351e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.703676753095351e-05

Optimization complete. Final v2v error: 3.460029363632202 mm

Highest mean error: 3.937225818634033 mm for frame 133

Lowest mean error: 3.3297111988067627 mm for frame 0

Saving results

Total time: 123.84395551681519
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823449
Iteration 2/25 | Loss: 0.00082080
Iteration 3/25 | Loss: 0.00067896
Iteration 4/25 | Loss: 0.00064148
Iteration 5/25 | Loss: 0.00062736
Iteration 6/25 | Loss: 0.00062408
Iteration 7/25 | Loss: 0.00062279
Iteration 8/25 | Loss: 0.00062261
Iteration 9/25 | Loss: 0.00062261
Iteration 10/25 | Loss: 0.00062261
Iteration 11/25 | Loss: 0.00062261
Iteration 12/25 | Loss: 0.00062261
Iteration 13/25 | Loss: 0.00062261
Iteration 14/25 | Loss: 0.00062261
Iteration 15/25 | Loss: 0.00062261
Iteration 16/25 | Loss: 0.00062261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006226141122169793, 0.0006226141122169793, 0.0006226141122169793, 0.0006226141122169793, 0.0006226141122169793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006226141122169793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46869981
Iteration 2/25 | Loss: 0.00038448
Iteration 3/25 | Loss: 0.00038448
Iteration 4/25 | Loss: 0.00038448
Iteration 5/25 | Loss: 0.00038448
Iteration 6/25 | Loss: 0.00038448
Iteration 7/25 | Loss: 0.00038448
Iteration 8/25 | Loss: 0.00038448
Iteration 9/25 | Loss: 0.00038448
Iteration 10/25 | Loss: 0.00038448
Iteration 11/25 | Loss: 0.00038448
Iteration 12/25 | Loss: 0.00038448
Iteration 13/25 | Loss: 0.00038448
Iteration 14/25 | Loss: 0.00038448
Iteration 15/25 | Loss: 0.00038448
Iteration 16/25 | Loss: 0.00038448
Iteration 17/25 | Loss: 0.00038448
Iteration 18/25 | Loss: 0.00038448
Iteration 19/25 | Loss: 0.00038448
Iteration 20/25 | Loss: 0.00038448
Iteration 21/25 | Loss: 0.00038448
Iteration 22/25 | Loss: 0.00038448
Iteration 23/25 | Loss: 0.00038448
Iteration 24/25 | Loss: 0.00038448
Iteration 25/25 | Loss: 0.00038448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038448
Iteration 2/1000 | Loss: 0.00003754
Iteration 3/1000 | Loss: 0.00002662
Iteration 4/1000 | Loss: 0.00002051
Iteration 5/1000 | Loss: 0.00001864
Iteration 6/1000 | Loss: 0.00001780
Iteration 7/1000 | Loss: 0.00001703
Iteration 8/1000 | Loss: 0.00001662
Iteration 9/1000 | Loss: 0.00001623
Iteration 10/1000 | Loss: 0.00001602
Iteration 11/1000 | Loss: 0.00001583
Iteration 12/1000 | Loss: 0.00001568
Iteration 13/1000 | Loss: 0.00001559
Iteration 14/1000 | Loss: 0.00001553
Iteration 15/1000 | Loss: 0.00001552
Iteration 16/1000 | Loss: 0.00001552
Iteration 17/1000 | Loss: 0.00001551
Iteration 18/1000 | Loss: 0.00001550
Iteration 19/1000 | Loss: 0.00001549
Iteration 20/1000 | Loss: 0.00001548
Iteration 21/1000 | Loss: 0.00001547
Iteration 22/1000 | Loss: 0.00001546
Iteration 23/1000 | Loss: 0.00001542
Iteration 24/1000 | Loss: 0.00001540
Iteration 25/1000 | Loss: 0.00001539
Iteration 26/1000 | Loss: 0.00001538
Iteration 27/1000 | Loss: 0.00001535
Iteration 28/1000 | Loss: 0.00001535
Iteration 29/1000 | Loss: 0.00001534
Iteration 30/1000 | Loss: 0.00001534
Iteration 31/1000 | Loss: 0.00001534
Iteration 32/1000 | Loss: 0.00001532
Iteration 33/1000 | Loss: 0.00001532
Iteration 34/1000 | Loss: 0.00001532
Iteration 35/1000 | Loss: 0.00001531
Iteration 36/1000 | Loss: 0.00001531
Iteration 37/1000 | Loss: 0.00001531
Iteration 38/1000 | Loss: 0.00001530
Iteration 39/1000 | Loss: 0.00001530
Iteration 40/1000 | Loss: 0.00001529
Iteration 41/1000 | Loss: 0.00001528
Iteration 42/1000 | Loss: 0.00001527
Iteration 43/1000 | Loss: 0.00001527
Iteration 44/1000 | Loss: 0.00001525
Iteration 45/1000 | Loss: 0.00001525
Iteration 46/1000 | Loss: 0.00001525
Iteration 47/1000 | Loss: 0.00001525
Iteration 48/1000 | Loss: 0.00001525
Iteration 49/1000 | Loss: 0.00001525
Iteration 50/1000 | Loss: 0.00001525
Iteration 51/1000 | Loss: 0.00001525
Iteration 52/1000 | Loss: 0.00001525
Iteration 53/1000 | Loss: 0.00001525
Iteration 54/1000 | Loss: 0.00001524
Iteration 55/1000 | Loss: 0.00001524
Iteration 56/1000 | Loss: 0.00001523
Iteration 57/1000 | Loss: 0.00001523
Iteration 58/1000 | Loss: 0.00001523
Iteration 59/1000 | Loss: 0.00001523
Iteration 60/1000 | Loss: 0.00001522
Iteration 61/1000 | Loss: 0.00001522
Iteration 62/1000 | Loss: 0.00001522
Iteration 63/1000 | Loss: 0.00001521
Iteration 64/1000 | Loss: 0.00001521
Iteration 65/1000 | Loss: 0.00001521
Iteration 66/1000 | Loss: 0.00001521
Iteration 67/1000 | Loss: 0.00001521
Iteration 68/1000 | Loss: 0.00001521
Iteration 69/1000 | Loss: 0.00001520
Iteration 70/1000 | Loss: 0.00001520
Iteration 71/1000 | Loss: 0.00001520
Iteration 72/1000 | Loss: 0.00001520
Iteration 73/1000 | Loss: 0.00001520
Iteration 74/1000 | Loss: 0.00001519
Iteration 75/1000 | Loss: 0.00001519
Iteration 76/1000 | Loss: 0.00001519
Iteration 77/1000 | Loss: 0.00001519
Iteration 78/1000 | Loss: 0.00001518
Iteration 79/1000 | Loss: 0.00001518
Iteration 80/1000 | Loss: 0.00001517
Iteration 81/1000 | Loss: 0.00001517
Iteration 82/1000 | Loss: 0.00001517
Iteration 83/1000 | Loss: 0.00001516
Iteration 84/1000 | Loss: 0.00001516
Iteration 85/1000 | Loss: 0.00001516
Iteration 86/1000 | Loss: 0.00001516
Iteration 87/1000 | Loss: 0.00001516
Iteration 88/1000 | Loss: 0.00001515
Iteration 89/1000 | Loss: 0.00001515
Iteration 90/1000 | Loss: 0.00001515
Iteration 91/1000 | Loss: 0.00001515
Iteration 92/1000 | Loss: 0.00001514
Iteration 93/1000 | Loss: 0.00001514
Iteration 94/1000 | Loss: 0.00001514
Iteration 95/1000 | Loss: 0.00001514
Iteration 96/1000 | Loss: 0.00001513
Iteration 97/1000 | Loss: 0.00001513
Iteration 98/1000 | Loss: 0.00001513
Iteration 99/1000 | Loss: 0.00001513
Iteration 100/1000 | Loss: 0.00001513
Iteration 101/1000 | Loss: 0.00001513
Iteration 102/1000 | Loss: 0.00001513
Iteration 103/1000 | Loss: 0.00001512
Iteration 104/1000 | Loss: 0.00001512
Iteration 105/1000 | Loss: 0.00001512
Iteration 106/1000 | Loss: 0.00001512
Iteration 107/1000 | Loss: 0.00001512
Iteration 108/1000 | Loss: 0.00001511
Iteration 109/1000 | Loss: 0.00001511
Iteration 110/1000 | Loss: 0.00001511
Iteration 111/1000 | Loss: 0.00001511
Iteration 112/1000 | Loss: 0.00001511
Iteration 113/1000 | Loss: 0.00001511
Iteration 114/1000 | Loss: 0.00001511
Iteration 115/1000 | Loss: 0.00001511
Iteration 116/1000 | Loss: 0.00001511
Iteration 117/1000 | Loss: 0.00001511
Iteration 118/1000 | Loss: 0.00001511
Iteration 119/1000 | Loss: 0.00001511
Iteration 120/1000 | Loss: 0.00001511
Iteration 121/1000 | Loss: 0.00001511
Iteration 122/1000 | Loss: 0.00001511
Iteration 123/1000 | Loss: 0.00001511
Iteration 124/1000 | Loss: 0.00001510
Iteration 125/1000 | Loss: 0.00001510
Iteration 126/1000 | Loss: 0.00001510
Iteration 127/1000 | Loss: 0.00001510
Iteration 128/1000 | Loss: 0.00001510
Iteration 129/1000 | Loss: 0.00001510
Iteration 130/1000 | Loss: 0.00001510
Iteration 131/1000 | Loss: 0.00001510
Iteration 132/1000 | Loss: 0.00001510
Iteration 133/1000 | Loss: 0.00001510
Iteration 134/1000 | Loss: 0.00001510
Iteration 135/1000 | Loss: 0.00001510
Iteration 136/1000 | Loss: 0.00001510
Iteration 137/1000 | Loss: 0.00001510
Iteration 138/1000 | Loss: 0.00001510
Iteration 139/1000 | Loss: 0.00001510
Iteration 140/1000 | Loss: 0.00001509
Iteration 141/1000 | Loss: 0.00001509
Iteration 142/1000 | Loss: 0.00001509
Iteration 143/1000 | Loss: 0.00001509
Iteration 144/1000 | Loss: 0.00001509
Iteration 145/1000 | Loss: 0.00001509
Iteration 146/1000 | Loss: 0.00001509
Iteration 147/1000 | Loss: 0.00001509
Iteration 148/1000 | Loss: 0.00001509
Iteration 149/1000 | Loss: 0.00001509
Iteration 150/1000 | Loss: 0.00001508
Iteration 151/1000 | Loss: 0.00001508
Iteration 152/1000 | Loss: 0.00001508
Iteration 153/1000 | Loss: 0.00001508
Iteration 154/1000 | Loss: 0.00001508
Iteration 155/1000 | Loss: 0.00001508
Iteration 156/1000 | Loss: 0.00001508
Iteration 157/1000 | Loss: 0.00001508
Iteration 158/1000 | Loss: 0.00001508
Iteration 159/1000 | Loss: 0.00001508
Iteration 160/1000 | Loss: 0.00001508
Iteration 161/1000 | Loss: 0.00001508
Iteration 162/1000 | Loss: 0.00001508
Iteration 163/1000 | Loss: 0.00001508
Iteration 164/1000 | Loss: 0.00001508
Iteration 165/1000 | Loss: 0.00001508
Iteration 166/1000 | Loss: 0.00001508
Iteration 167/1000 | Loss: 0.00001508
Iteration 168/1000 | Loss: 0.00001508
Iteration 169/1000 | Loss: 0.00001508
Iteration 170/1000 | Loss: 0.00001508
Iteration 171/1000 | Loss: 0.00001508
Iteration 172/1000 | Loss: 0.00001508
Iteration 173/1000 | Loss: 0.00001508
Iteration 174/1000 | Loss: 0.00001507
Iteration 175/1000 | Loss: 0.00001507
Iteration 176/1000 | Loss: 0.00001507
Iteration 177/1000 | Loss: 0.00001507
Iteration 178/1000 | Loss: 0.00001507
Iteration 179/1000 | Loss: 0.00001507
Iteration 180/1000 | Loss: 0.00001507
Iteration 181/1000 | Loss: 0.00001507
Iteration 182/1000 | Loss: 0.00001507
Iteration 183/1000 | Loss: 0.00001507
Iteration 184/1000 | Loss: 0.00001507
Iteration 185/1000 | Loss: 0.00001507
Iteration 186/1000 | Loss: 0.00001507
Iteration 187/1000 | Loss: 0.00001507
Iteration 188/1000 | Loss: 0.00001507
Iteration 189/1000 | Loss: 0.00001507
Iteration 190/1000 | Loss: 0.00001507
Iteration 191/1000 | Loss: 0.00001507
Iteration 192/1000 | Loss: 0.00001507
Iteration 193/1000 | Loss: 0.00001507
Iteration 194/1000 | Loss: 0.00001507
Iteration 195/1000 | Loss: 0.00001507
Iteration 196/1000 | Loss: 0.00001507
Iteration 197/1000 | Loss: 0.00001507
Iteration 198/1000 | Loss: 0.00001507
Iteration 199/1000 | Loss: 0.00001507
Iteration 200/1000 | Loss: 0.00001507
Iteration 201/1000 | Loss: 0.00001507
Iteration 202/1000 | Loss: 0.00001507
Iteration 203/1000 | Loss: 0.00001507
Iteration 204/1000 | Loss: 0.00001507
Iteration 205/1000 | Loss: 0.00001507
Iteration 206/1000 | Loss: 0.00001507
Iteration 207/1000 | Loss: 0.00001507
Iteration 208/1000 | Loss: 0.00001507
Iteration 209/1000 | Loss: 0.00001507
Iteration 210/1000 | Loss: 0.00001507
Iteration 211/1000 | Loss: 0.00001507
Iteration 212/1000 | Loss: 0.00001507
Iteration 213/1000 | Loss: 0.00001507
Iteration 214/1000 | Loss: 0.00001507
Iteration 215/1000 | Loss: 0.00001507
Iteration 216/1000 | Loss: 0.00001507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.5072798305482138e-05, 1.5072798305482138e-05, 1.5072798305482138e-05, 1.5072798305482138e-05, 1.5072798305482138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5072798305482138e-05

Optimization complete. Final v2v error: 3.226414203643799 mm

Highest mean error: 4.65100622177124 mm for frame 33

Lowest mean error: 2.6691417694091797 mm for frame 155

Saving results

Total time: 48.30308818817139
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00956998
Iteration 2/25 | Loss: 0.00138653
Iteration 3/25 | Loss: 0.00084614
Iteration 4/25 | Loss: 0.00074728
Iteration 5/25 | Loss: 0.00072668
Iteration 6/25 | Loss: 0.00074304
Iteration 7/25 | Loss: 0.00069416
Iteration 8/25 | Loss: 0.00067313
Iteration 9/25 | Loss: 0.00065987
Iteration 10/25 | Loss: 0.00065064
Iteration 11/25 | Loss: 0.00064519
Iteration 12/25 | Loss: 0.00063877
Iteration 13/25 | Loss: 0.00063670
Iteration 14/25 | Loss: 0.00063454
Iteration 15/25 | Loss: 0.00063296
Iteration 16/25 | Loss: 0.00063194
Iteration 17/25 | Loss: 0.00063152
Iteration 18/25 | Loss: 0.00063136
Iteration 19/25 | Loss: 0.00063132
Iteration 20/25 | Loss: 0.00063132
Iteration 21/25 | Loss: 0.00063131
Iteration 22/25 | Loss: 0.00063131
Iteration 23/25 | Loss: 0.00063131
Iteration 24/25 | Loss: 0.00063131
Iteration 25/25 | Loss: 0.00063131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47981966
Iteration 2/25 | Loss: 0.00029348
Iteration 3/25 | Loss: 0.00029348
Iteration 4/25 | Loss: 0.00029348
Iteration 5/25 | Loss: 0.00029348
Iteration 6/25 | Loss: 0.00029348
Iteration 7/25 | Loss: 0.00029348
Iteration 8/25 | Loss: 0.00029348
Iteration 9/25 | Loss: 0.00029348
Iteration 10/25 | Loss: 0.00029348
Iteration 11/25 | Loss: 0.00029348
Iteration 12/25 | Loss: 0.00029348
Iteration 13/25 | Loss: 0.00029348
Iteration 14/25 | Loss: 0.00029348
Iteration 15/25 | Loss: 0.00029348
Iteration 16/25 | Loss: 0.00029348
Iteration 17/25 | Loss: 0.00029348
Iteration 18/25 | Loss: 0.00029348
Iteration 19/25 | Loss: 0.00029348
Iteration 20/25 | Loss: 0.00029348
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00029347531381063163, 0.00029347531381063163, 0.00029347531381063163, 0.00029347531381063163, 0.00029347531381063163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029347531381063163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029348
Iteration 2/1000 | Loss: 0.00002254
Iteration 3/1000 | Loss: 0.00001613
Iteration 4/1000 | Loss: 0.00001517
Iteration 5/1000 | Loss: 0.00001428
Iteration 6/1000 | Loss: 0.00001385
Iteration 7/1000 | Loss: 0.00001350
Iteration 8/1000 | Loss: 0.00001334
Iteration 9/1000 | Loss: 0.00001326
Iteration 10/1000 | Loss: 0.00001316
Iteration 11/1000 | Loss: 0.00001316
Iteration 12/1000 | Loss: 0.00001315
Iteration 13/1000 | Loss: 0.00001314
Iteration 14/1000 | Loss: 0.00001313
Iteration 15/1000 | Loss: 0.00001311
Iteration 16/1000 | Loss: 0.00001310
Iteration 17/1000 | Loss: 0.00001293
Iteration 18/1000 | Loss: 0.00001292
Iteration 19/1000 | Loss: 0.00001290
Iteration 20/1000 | Loss: 0.00001284
Iteration 21/1000 | Loss: 0.00001283
Iteration 22/1000 | Loss: 0.00001282
Iteration 23/1000 | Loss: 0.00001281
Iteration 24/1000 | Loss: 0.00001280
Iteration 25/1000 | Loss: 0.00001280
Iteration 26/1000 | Loss: 0.00001271
Iteration 27/1000 | Loss: 0.00001270
Iteration 28/1000 | Loss: 0.00001270
Iteration 29/1000 | Loss: 0.00001268
Iteration 30/1000 | Loss: 0.00001268
Iteration 31/1000 | Loss: 0.00001267
Iteration 32/1000 | Loss: 0.00001266
Iteration 33/1000 | Loss: 0.00001265
Iteration 34/1000 | Loss: 0.00001263
Iteration 35/1000 | Loss: 0.00001263
Iteration 36/1000 | Loss: 0.00001263
Iteration 37/1000 | Loss: 0.00001263
Iteration 38/1000 | Loss: 0.00001263
Iteration 39/1000 | Loss: 0.00001263
Iteration 40/1000 | Loss: 0.00001262
Iteration 41/1000 | Loss: 0.00001262
Iteration 42/1000 | Loss: 0.00001262
Iteration 43/1000 | Loss: 0.00001260
Iteration 44/1000 | Loss: 0.00001260
Iteration 45/1000 | Loss: 0.00001260
Iteration 46/1000 | Loss: 0.00001260
Iteration 47/1000 | Loss: 0.00001260
Iteration 48/1000 | Loss: 0.00001260
Iteration 49/1000 | Loss: 0.00001259
Iteration 50/1000 | Loss: 0.00001259
Iteration 51/1000 | Loss: 0.00001259
Iteration 52/1000 | Loss: 0.00001259
Iteration 53/1000 | Loss: 0.00001259
Iteration 54/1000 | Loss: 0.00001259
Iteration 55/1000 | Loss: 0.00001258
Iteration 56/1000 | Loss: 0.00001258
Iteration 57/1000 | Loss: 0.00001258
Iteration 58/1000 | Loss: 0.00001258
Iteration 59/1000 | Loss: 0.00001258
Iteration 60/1000 | Loss: 0.00001258
Iteration 61/1000 | Loss: 0.00001258
Iteration 62/1000 | Loss: 0.00001257
Iteration 63/1000 | Loss: 0.00001257
Iteration 64/1000 | Loss: 0.00001257
Iteration 65/1000 | Loss: 0.00001257
Iteration 66/1000 | Loss: 0.00001257
Iteration 67/1000 | Loss: 0.00001257
Iteration 68/1000 | Loss: 0.00001257
Iteration 69/1000 | Loss: 0.00001257
Iteration 70/1000 | Loss: 0.00001257
Iteration 71/1000 | Loss: 0.00001257
Iteration 72/1000 | Loss: 0.00001257
Iteration 73/1000 | Loss: 0.00001257
Iteration 74/1000 | Loss: 0.00001257
Iteration 75/1000 | Loss: 0.00001257
Iteration 76/1000 | Loss: 0.00001257
Iteration 77/1000 | Loss: 0.00001256
Iteration 78/1000 | Loss: 0.00001256
Iteration 79/1000 | Loss: 0.00001255
Iteration 80/1000 | Loss: 0.00001255
Iteration 81/1000 | Loss: 0.00001254
Iteration 82/1000 | Loss: 0.00001254
Iteration 83/1000 | Loss: 0.00001254
Iteration 84/1000 | Loss: 0.00001254
Iteration 85/1000 | Loss: 0.00001253
Iteration 86/1000 | Loss: 0.00001253
Iteration 87/1000 | Loss: 0.00001253
Iteration 88/1000 | Loss: 0.00001252
Iteration 89/1000 | Loss: 0.00001252
Iteration 90/1000 | Loss: 0.00001252
Iteration 91/1000 | Loss: 0.00001251
Iteration 92/1000 | Loss: 0.00001251
Iteration 93/1000 | Loss: 0.00001251
Iteration 94/1000 | Loss: 0.00001251
Iteration 95/1000 | Loss: 0.00001251
Iteration 96/1000 | Loss: 0.00001251
Iteration 97/1000 | Loss: 0.00001250
Iteration 98/1000 | Loss: 0.00001250
Iteration 99/1000 | Loss: 0.00001250
Iteration 100/1000 | Loss: 0.00001249
Iteration 101/1000 | Loss: 0.00001249
Iteration 102/1000 | Loss: 0.00001249
Iteration 103/1000 | Loss: 0.00001249
Iteration 104/1000 | Loss: 0.00001249
Iteration 105/1000 | Loss: 0.00001249
Iteration 106/1000 | Loss: 0.00001248
Iteration 107/1000 | Loss: 0.00001248
Iteration 108/1000 | Loss: 0.00001248
Iteration 109/1000 | Loss: 0.00001248
Iteration 110/1000 | Loss: 0.00001248
Iteration 111/1000 | Loss: 0.00001248
Iteration 112/1000 | Loss: 0.00001248
Iteration 113/1000 | Loss: 0.00001248
Iteration 114/1000 | Loss: 0.00001248
Iteration 115/1000 | Loss: 0.00001248
Iteration 116/1000 | Loss: 0.00001248
Iteration 117/1000 | Loss: 0.00001248
Iteration 118/1000 | Loss: 0.00001248
Iteration 119/1000 | Loss: 0.00001248
Iteration 120/1000 | Loss: 0.00001248
Iteration 121/1000 | Loss: 0.00001248
Iteration 122/1000 | Loss: 0.00001248
Iteration 123/1000 | Loss: 0.00001248
Iteration 124/1000 | Loss: 0.00001248
Iteration 125/1000 | Loss: 0.00001248
Iteration 126/1000 | Loss: 0.00001248
Iteration 127/1000 | Loss: 0.00001248
Iteration 128/1000 | Loss: 0.00001248
Iteration 129/1000 | Loss: 0.00001248
Iteration 130/1000 | Loss: 0.00001248
Iteration 131/1000 | Loss: 0.00001248
Iteration 132/1000 | Loss: 0.00001248
Iteration 133/1000 | Loss: 0.00001248
Iteration 134/1000 | Loss: 0.00001248
Iteration 135/1000 | Loss: 0.00001248
Iteration 136/1000 | Loss: 0.00001248
Iteration 137/1000 | Loss: 0.00001248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.24788957691635e-05, 1.24788957691635e-05, 1.24788957691635e-05, 1.24788957691635e-05, 1.24788957691635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.24788957691635e-05

Optimization complete. Final v2v error: 3.013659954071045 mm

Highest mean error: 3.7048022747039795 mm for frame 58

Lowest mean error: 2.6300976276397705 mm for frame 103

Saving results

Total time: 57.10880780220032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830242
Iteration 2/25 | Loss: 0.00096431
Iteration 3/25 | Loss: 0.00072363
Iteration 4/25 | Loss: 0.00068261
Iteration 5/25 | Loss: 0.00067044
Iteration 6/25 | Loss: 0.00066705
Iteration 7/25 | Loss: 0.00066644
Iteration 8/25 | Loss: 0.00066644
Iteration 9/25 | Loss: 0.00066644
Iteration 10/25 | Loss: 0.00066644
Iteration 11/25 | Loss: 0.00066644
Iteration 12/25 | Loss: 0.00066644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006664407555945218, 0.0006664407555945218, 0.0006664407555945218, 0.0006664407555945218, 0.0006664407555945218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006664407555945218

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41277540
Iteration 2/25 | Loss: 0.00029005
Iteration 3/25 | Loss: 0.00029001
Iteration 4/25 | Loss: 0.00029001
Iteration 5/25 | Loss: 0.00029001
Iteration 6/25 | Loss: 0.00029001
Iteration 7/25 | Loss: 0.00029001
Iteration 8/25 | Loss: 0.00029001
Iteration 9/25 | Loss: 0.00029000
Iteration 10/25 | Loss: 0.00029000
Iteration 11/25 | Loss: 0.00029000
Iteration 12/25 | Loss: 0.00029000
Iteration 13/25 | Loss: 0.00029000
Iteration 14/25 | Loss: 0.00029000
Iteration 15/25 | Loss: 0.00029000
Iteration 16/25 | Loss: 0.00029000
Iteration 17/25 | Loss: 0.00029000
Iteration 18/25 | Loss: 0.00029000
Iteration 19/25 | Loss: 0.00029000
Iteration 20/25 | Loss: 0.00029000
Iteration 21/25 | Loss: 0.00029000
Iteration 22/25 | Loss: 0.00029000
Iteration 23/25 | Loss: 0.00029000
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00029000442009419203, 0.00029000442009419203, 0.00029000442009419203, 0.00029000442009419203, 0.00029000442009419203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029000442009419203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029000
Iteration 2/1000 | Loss: 0.00003113
Iteration 3/1000 | Loss: 0.00002178
Iteration 4/1000 | Loss: 0.00001944
Iteration 5/1000 | Loss: 0.00001839
Iteration 6/1000 | Loss: 0.00001750
Iteration 7/1000 | Loss: 0.00001687
Iteration 8/1000 | Loss: 0.00001654
Iteration 9/1000 | Loss: 0.00001622
Iteration 10/1000 | Loss: 0.00001603
Iteration 11/1000 | Loss: 0.00001586
Iteration 12/1000 | Loss: 0.00001580
Iteration 13/1000 | Loss: 0.00001576
Iteration 14/1000 | Loss: 0.00001575
Iteration 15/1000 | Loss: 0.00001575
Iteration 16/1000 | Loss: 0.00001569
Iteration 17/1000 | Loss: 0.00001565
Iteration 18/1000 | Loss: 0.00001564
Iteration 19/1000 | Loss: 0.00001563
Iteration 20/1000 | Loss: 0.00001559
Iteration 21/1000 | Loss: 0.00001552
Iteration 22/1000 | Loss: 0.00001550
Iteration 23/1000 | Loss: 0.00001550
Iteration 24/1000 | Loss: 0.00001549
Iteration 25/1000 | Loss: 0.00001549
Iteration 26/1000 | Loss: 0.00001549
Iteration 27/1000 | Loss: 0.00001549
Iteration 28/1000 | Loss: 0.00001549
Iteration 29/1000 | Loss: 0.00001546
Iteration 30/1000 | Loss: 0.00001546
Iteration 31/1000 | Loss: 0.00001544
Iteration 32/1000 | Loss: 0.00001543
Iteration 33/1000 | Loss: 0.00001543
Iteration 34/1000 | Loss: 0.00001542
Iteration 35/1000 | Loss: 0.00001541
Iteration 36/1000 | Loss: 0.00001541
Iteration 37/1000 | Loss: 0.00001541
Iteration 38/1000 | Loss: 0.00001541
Iteration 39/1000 | Loss: 0.00001541
Iteration 40/1000 | Loss: 0.00001541
Iteration 41/1000 | Loss: 0.00001541
Iteration 42/1000 | Loss: 0.00001540
Iteration 43/1000 | Loss: 0.00001540
Iteration 44/1000 | Loss: 0.00001540
Iteration 45/1000 | Loss: 0.00001539
Iteration 46/1000 | Loss: 0.00001539
Iteration 47/1000 | Loss: 0.00001539
Iteration 48/1000 | Loss: 0.00001539
Iteration 49/1000 | Loss: 0.00001539
Iteration 50/1000 | Loss: 0.00001539
Iteration 51/1000 | Loss: 0.00001538
Iteration 52/1000 | Loss: 0.00001538
Iteration 53/1000 | Loss: 0.00001538
Iteration 54/1000 | Loss: 0.00001538
Iteration 55/1000 | Loss: 0.00001538
Iteration 56/1000 | Loss: 0.00001538
Iteration 57/1000 | Loss: 0.00001538
Iteration 58/1000 | Loss: 0.00001538
Iteration 59/1000 | Loss: 0.00001538
Iteration 60/1000 | Loss: 0.00001538
Iteration 61/1000 | Loss: 0.00001538
Iteration 62/1000 | Loss: 0.00001538
Iteration 63/1000 | Loss: 0.00001538
Iteration 64/1000 | Loss: 0.00001538
Iteration 65/1000 | Loss: 0.00001538
Iteration 66/1000 | Loss: 0.00001538
Iteration 67/1000 | Loss: 0.00001538
Iteration 68/1000 | Loss: 0.00001538
Iteration 69/1000 | Loss: 0.00001538
Iteration 70/1000 | Loss: 0.00001538
Iteration 71/1000 | Loss: 0.00001538
Iteration 72/1000 | Loss: 0.00001538
Iteration 73/1000 | Loss: 0.00001538
Iteration 74/1000 | Loss: 0.00001538
Iteration 75/1000 | Loss: 0.00001538
Iteration 76/1000 | Loss: 0.00001538
Iteration 77/1000 | Loss: 0.00001538
Iteration 78/1000 | Loss: 0.00001538
Iteration 79/1000 | Loss: 0.00001538
Iteration 80/1000 | Loss: 0.00001538
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.5378851458081044e-05, 1.5378851458081044e-05, 1.5378851458081044e-05, 1.5378851458081044e-05, 1.5378851458081044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5378851458081044e-05

Optimization complete. Final v2v error: 3.306950807571411 mm

Highest mean error: 4.643775463104248 mm for frame 134

Lowest mean error: 2.791726589202881 mm for frame 200

Saving results

Total time: 38.732730865478516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849164
Iteration 2/25 | Loss: 0.00078602
Iteration 3/25 | Loss: 0.00062525
Iteration 4/25 | Loss: 0.00060002
Iteration 5/25 | Loss: 0.00059312
Iteration 6/25 | Loss: 0.00059114
Iteration 7/25 | Loss: 0.00059080
Iteration 8/25 | Loss: 0.00059080
Iteration 9/25 | Loss: 0.00059080
Iteration 10/25 | Loss: 0.00059080
Iteration 11/25 | Loss: 0.00059080
Iteration 12/25 | Loss: 0.00059080
Iteration 13/25 | Loss: 0.00059080
Iteration 14/25 | Loss: 0.00059080
Iteration 15/25 | Loss: 0.00059080
Iteration 16/25 | Loss: 0.00059080
Iteration 17/25 | Loss: 0.00059080
Iteration 18/25 | Loss: 0.00059080
Iteration 19/25 | Loss: 0.00059080
Iteration 20/25 | Loss: 0.00059080
Iteration 21/25 | Loss: 0.00059080
Iteration 22/25 | Loss: 0.00059080
Iteration 23/25 | Loss: 0.00059080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005908031016588211, 0.0005908031016588211, 0.0005908031016588211, 0.0005908031016588211, 0.0005908031016588211]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005908031016588211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45779061
Iteration 2/25 | Loss: 0.00025094
Iteration 3/25 | Loss: 0.00025091
Iteration 4/25 | Loss: 0.00025091
Iteration 5/25 | Loss: 0.00025091
Iteration 6/25 | Loss: 0.00025091
Iteration 7/25 | Loss: 0.00025091
Iteration 8/25 | Loss: 0.00025091
Iteration 9/25 | Loss: 0.00025091
Iteration 10/25 | Loss: 0.00025091
Iteration 11/25 | Loss: 0.00025091
Iteration 12/25 | Loss: 0.00025091
Iteration 13/25 | Loss: 0.00025091
Iteration 14/25 | Loss: 0.00025091
Iteration 15/25 | Loss: 0.00025091
Iteration 16/25 | Loss: 0.00025091
Iteration 17/25 | Loss: 0.00025091
Iteration 18/25 | Loss: 0.00025091
Iteration 19/25 | Loss: 0.00025091
Iteration 20/25 | Loss: 0.00025091
Iteration 21/25 | Loss: 0.00025091
Iteration 22/25 | Loss: 0.00025091
Iteration 23/25 | Loss: 0.00025091
Iteration 24/25 | Loss: 0.00025091
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0002509094774723053, 0.0002509094774723053, 0.0002509094774723053, 0.0002509094774723053, 0.0002509094774723053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002509094774723053

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025091
Iteration 2/1000 | Loss: 0.00002130
Iteration 3/1000 | Loss: 0.00001628
Iteration 4/1000 | Loss: 0.00001372
Iteration 5/1000 | Loss: 0.00001293
Iteration 6/1000 | Loss: 0.00001218
Iteration 7/1000 | Loss: 0.00001178
Iteration 8/1000 | Loss: 0.00001150
Iteration 9/1000 | Loss: 0.00001149
Iteration 10/1000 | Loss: 0.00001143
Iteration 11/1000 | Loss: 0.00001142
Iteration 12/1000 | Loss: 0.00001142
Iteration 13/1000 | Loss: 0.00001141
Iteration 14/1000 | Loss: 0.00001141
Iteration 15/1000 | Loss: 0.00001134
Iteration 16/1000 | Loss: 0.00001130
Iteration 17/1000 | Loss: 0.00001125
Iteration 18/1000 | Loss: 0.00001119
Iteration 19/1000 | Loss: 0.00001119
Iteration 20/1000 | Loss: 0.00001118
Iteration 21/1000 | Loss: 0.00001118
Iteration 22/1000 | Loss: 0.00001117
Iteration 23/1000 | Loss: 0.00001116
Iteration 24/1000 | Loss: 0.00001116
Iteration 25/1000 | Loss: 0.00001115
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001114
Iteration 28/1000 | Loss: 0.00001113
Iteration 29/1000 | Loss: 0.00001111
Iteration 30/1000 | Loss: 0.00001111
Iteration 31/1000 | Loss: 0.00001110
Iteration 32/1000 | Loss: 0.00001110
Iteration 33/1000 | Loss: 0.00001109
Iteration 34/1000 | Loss: 0.00001109
Iteration 35/1000 | Loss: 0.00001108
Iteration 36/1000 | Loss: 0.00001108
Iteration 37/1000 | Loss: 0.00001108
Iteration 38/1000 | Loss: 0.00001108
Iteration 39/1000 | Loss: 0.00001107
Iteration 40/1000 | Loss: 0.00001107
Iteration 41/1000 | Loss: 0.00001107
Iteration 42/1000 | Loss: 0.00001107
Iteration 43/1000 | Loss: 0.00001107
Iteration 44/1000 | Loss: 0.00001106
Iteration 45/1000 | Loss: 0.00001106
Iteration 46/1000 | Loss: 0.00001106
Iteration 47/1000 | Loss: 0.00001106
Iteration 48/1000 | Loss: 0.00001106
Iteration 49/1000 | Loss: 0.00001106
Iteration 50/1000 | Loss: 0.00001106
Iteration 51/1000 | Loss: 0.00001106
Iteration 52/1000 | Loss: 0.00001106
Iteration 53/1000 | Loss: 0.00001106
Iteration 54/1000 | Loss: 0.00001105
Iteration 55/1000 | Loss: 0.00001105
Iteration 56/1000 | Loss: 0.00001105
Iteration 57/1000 | Loss: 0.00001105
Iteration 58/1000 | Loss: 0.00001105
Iteration 59/1000 | Loss: 0.00001105
Iteration 60/1000 | Loss: 0.00001105
Iteration 61/1000 | Loss: 0.00001105
Iteration 62/1000 | Loss: 0.00001105
Iteration 63/1000 | Loss: 0.00001104
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001104
Iteration 66/1000 | Loss: 0.00001104
Iteration 67/1000 | Loss: 0.00001104
Iteration 68/1000 | Loss: 0.00001104
Iteration 69/1000 | Loss: 0.00001103
Iteration 70/1000 | Loss: 0.00001103
Iteration 71/1000 | Loss: 0.00001103
Iteration 72/1000 | Loss: 0.00001103
Iteration 73/1000 | Loss: 0.00001103
Iteration 74/1000 | Loss: 0.00001103
Iteration 75/1000 | Loss: 0.00001103
Iteration 76/1000 | Loss: 0.00001103
Iteration 77/1000 | Loss: 0.00001103
Iteration 78/1000 | Loss: 0.00001103
Iteration 79/1000 | Loss: 0.00001103
Iteration 80/1000 | Loss: 0.00001103
Iteration 81/1000 | Loss: 0.00001103
Iteration 82/1000 | Loss: 0.00001103
Iteration 83/1000 | Loss: 0.00001103
Iteration 84/1000 | Loss: 0.00001103
Iteration 85/1000 | Loss: 0.00001103
Iteration 86/1000 | Loss: 0.00001103
Iteration 87/1000 | Loss: 0.00001103
Iteration 88/1000 | Loss: 0.00001103
Iteration 89/1000 | Loss: 0.00001103
Iteration 90/1000 | Loss: 0.00001103
Iteration 91/1000 | Loss: 0.00001103
Iteration 92/1000 | Loss: 0.00001103
Iteration 93/1000 | Loss: 0.00001103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.1029855158994906e-05, 1.1029855158994906e-05, 1.1029855158994906e-05, 1.1029855158994906e-05, 1.1029855158994906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1029855158994906e-05

Optimization complete. Final v2v error: 2.83677077293396 mm

Highest mean error: 3.0226826667785645 mm for frame 6

Lowest mean error: 2.676806926727295 mm for frame 112

Saving results

Total time: 30.233336925506592
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495044
Iteration 2/25 | Loss: 0.00099324
Iteration 3/25 | Loss: 0.00071065
Iteration 4/25 | Loss: 0.00065594
Iteration 5/25 | Loss: 0.00064082
Iteration 6/25 | Loss: 0.00063864
Iteration 7/25 | Loss: 0.00063798
Iteration 8/25 | Loss: 0.00063798
Iteration 9/25 | Loss: 0.00063798
Iteration 10/25 | Loss: 0.00063798
Iteration 11/25 | Loss: 0.00063798
Iteration 12/25 | Loss: 0.00063798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006379819824360311, 0.0006379819824360311, 0.0006379819824360311, 0.0006379819824360311, 0.0006379819824360311]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006379819824360311

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82081658
Iteration 2/25 | Loss: 0.00020237
Iteration 3/25 | Loss: 0.00020237
Iteration 4/25 | Loss: 0.00020237
Iteration 5/25 | Loss: 0.00020237
Iteration 6/25 | Loss: 0.00020237
Iteration 7/25 | Loss: 0.00020237
Iteration 8/25 | Loss: 0.00020237
Iteration 9/25 | Loss: 0.00020237
Iteration 10/25 | Loss: 0.00020237
Iteration 11/25 | Loss: 0.00020237
Iteration 12/25 | Loss: 0.00020237
Iteration 13/25 | Loss: 0.00020237
Iteration 14/25 | Loss: 0.00020237
Iteration 15/25 | Loss: 0.00020237
Iteration 16/25 | Loss: 0.00020237
Iteration 17/25 | Loss: 0.00020237
Iteration 18/25 | Loss: 0.00020237
Iteration 19/25 | Loss: 0.00020237
Iteration 20/25 | Loss: 0.00020237
Iteration 21/25 | Loss: 0.00020237
Iteration 22/25 | Loss: 0.00020237
Iteration 23/25 | Loss: 0.00020237
Iteration 24/25 | Loss: 0.00020237
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00020237026910763234, 0.00020237026910763234, 0.00020237026910763234, 0.00020237026910763234, 0.00020237026910763234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00020237026910763234

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020237
Iteration 2/1000 | Loss: 0.00003547
Iteration 3/1000 | Loss: 0.00002267
Iteration 4/1000 | Loss: 0.00002108
Iteration 5/1000 | Loss: 0.00001998
Iteration 6/1000 | Loss: 0.00001937
Iteration 7/1000 | Loss: 0.00001879
Iteration 8/1000 | Loss: 0.00001833
Iteration 9/1000 | Loss: 0.00001804
Iteration 10/1000 | Loss: 0.00001782
Iteration 11/1000 | Loss: 0.00001773
Iteration 12/1000 | Loss: 0.00001772
Iteration 13/1000 | Loss: 0.00001772
Iteration 14/1000 | Loss: 0.00001758
Iteration 15/1000 | Loss: 0.00001758
Iteration 16/1000 | Loss: 0.00001755
Iteration 17/1000 | Loss: 0.00001741
Iteration 18/1000 | Loss: 0.00001737
Iteration 19/1000 | Loss: 0.00001726
Iteration 20/1000 | Loss: 0.00001726
Iteration 21/1000 | Loss: 0.00001724
Iteration 22/1000 | Loss: 0.00001723
Iteration 23/1000 | Loss: 0.00001723
Iteration 24/1000 | Loss: 0.00001719
Iteration 25/1000 | Loss: 0.00001716
Iteration 26/1000 | Loss: 0.00001715
Iteration 27/1000 | Loss: 0.00001715
Iteration 28/1000 | Loss: 0.00001714
Iteration 29/1000 | Loss: 0.00001711
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001708
Iteration 32/1000 | Loss: 0.00001708
Iteration 33/1000 | Loss: 0.00001708
Iteration 34/1000 | Loss: 0.00001708
Iteration 35/1000 | Loss: 0.00001708
Iteration 36/1000 | Loss: 0.00001708
Iteration 37/1000 | Loss: 0.00001708
Iteration 38/1000 | Loss: 0.00001708
Iteration 39/1000 | Loss: 0.00001708
Iteration 40/1000 | Loss: 0.00001708
Iteration 41/1000 | Loss: 0.00001708
Iteration 42/1000 | Loss: 0.00001708
Iteration 43/1000 | Loss: 0.00001707
Iteration 44/1000 | Loss: 0.00001707
Iteration 45/1000 | Loss: 0.00001707
Iteration 46/1000 | Loss: 0.00001707
Iteration 47/1000 | Loss: 0.00001706
Iteration 48/1000 | Loss: 0.00001706
Iteration 49/1000 | Loss: 0.00001705
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001704
Iteration 52/1000 | Loss: 0.00001704
Iteration 53/1000 | Loss: 0.00001704
Iteration 54/1000 | Loss: 0.00001703
Iteration 55/1000 | Loss: 0.00001703
Iteration 56/1000 | Loss: 0.00001702
Iteration 57/1000 | Loss: 0.00001702
Iteration 58/1000 | Loss: 0.00001701
Iteration 59/1000 | Loss: 0.00001701
Iteration 60/1000 | Loss: 0.00001701
Iteration 61/1000 | Loss: 0.00001699
Iteration 62/1000 | Loss: 0.00001699
Iteration 63/1000 | Loss: 0.00001699
Iteration 64/1000 | Loss: 0.00001698
Iteration 65/1000 | Loss: 0.00001698
Iteration 66/1000 | Loss: 0.00001697
Iteration 67/1000 | Loss: 0.00001697
Iteration 68/1000 | Loss: 0.00001697
Iteration 69/1000 | Loss: 0.00001697
Iteration 70/1000 | Loss: 0.00001697
Iteration 71/1000 | Loss: 0.00001697
Iteration 72/1000 | Loss: 0.00001697
Iteration 73/1000 | Loss: 0.00001696
Iteration 74/1000 | Loss: 0.00001696
Iteration 75/1000 | Loss: 0.00001696
Iteration 76/1000 | Loss: 0.00001696
Iteration 77/1000 | Loss: 0.00001696
Iteration 78/1000 | Loss: 0.00001696
Iteration 79/1000 | Loss: 0.00001695
Iteration 80/1000 | Loss: 0.00001695
Iteration 81/1000 | Loss: 0.00001695
Iteration 82/1000 | Loss: 0.00001695
Iteration 83/1000 | Loss: 0.00001695
Iteration 84/1000 | Loss: 0.00001694
Iteration 85/1000 | Loss: 0.00001694
Iteration 86/1000 | Loss: 0.00001694
Iteration 87/1000 | Loss: 0.00001694
Iteration 88/1000 | Loss: 0.00001694
Iteration 89/1000 | Loss: 0.00001694
Iteration 90/1000 | Loss: 0.00001694
Iteration 91/1000 | Loss: 0.00001694
Iteration 92/1000 | Loss: 0.00001694
Iteration 93/1000 | Loss: 0.00001694
Iteration 94/1000 | Loss: 0.00001694
Iteration 95/1000 | Loss: 0.00001694
Iteration 96/1000 | Loss: 0.00001693
Iteration 97/1000 | Loss: 0.00001693
Iteration 98/1000 | Loss: 0.00001693
Iteration 99/1000 | Loss: 0.00001692
Iteration 100/1000 | Loss: 0.00001692
Iteration 101/1000 | Loss: 0.00001692
Iteration 102/1000 | Loss: 0.00001692
Iteration 103/1000 | Loss: 0.00001692
Iteration 104/1000 | Loss: 0.00001691
Iteration 105/1000 | Loss: 0.00001691
Iteration 106/1000 | Loss: 0.00001691
Iteration 107/1000 | Loss: 0.00001690
Iteration 108/1000 | Loss: 0.00001690
Iteration 109/1000 | Loss: 0.00001690
Iteration 110/1000 | Loss: 0.00001690
Iteration 111/1000 | Loss: 0.00001690
Iteration 112/1000 | Loss: 0.00001689
Iteration 113/1000 | Loss: 0.00001689
Iteration 114/1000 | Loss: 0.00001689
Iteration 115/1000 | Loss: 0.00001688
Iteration 116/1000 | Loss: 0.00001688
Iteration 117/1000 | Loss: 0.00001688
Iteration 118/1000 | Loss: 0.00001688
Iteration 119/1000 | Loss: 0.00001688
Iteration 120/1000 | Loss: 0.00001687
Iteration 121/1000 | Loss: 0.00001687
Iteration 122/1000 | Loss: 0.00001687
Iteration 123/1000 | Loss: 0.00001687
Iteration 124/1000 | Loss: 0.00001687
Iteration 125/1000 | Loss: 0.00001687
Iteration 126/1000 | Loss: 0.00001687
Iteration 127/1000 | Loss: 0.00001686
Iteration 128/1000 | Loss: 0.00001686
Iteration 129/1000 | Loss: 0.00001686
Iteration 130/1000 | Loss: 0.00001686
Iteration 131/1000 | Loss: 0.00001686
Iteration 132/1000 | Loss: 0.00001686
Iteration 133/1000 | Loss: 0.00001686
Iteration 134/1000 | Loss: 0.00001686
Iteration 135/1000 | Loss: 0.00001686
Iteration 136/1000 | Loss: 0.00001686
Iteration 137/1000 | Loss: 0.00001686
Iteration 138/1000 | Loss: 0.00001686
Iteration 139/1000 | Loss: 0.00001686
Iteration 140/1000 | Loss: 0.00001685
Iteration 141/1000 | Loss: 0.00001685
Iteration 142/1000 | Loss: 0.00001685
Iteration 143/1000 | Loss: 0.00001685
Iteration 144/1000 | Loss: 0.00001685
Iteration 145/1000 | Loss: 0.00001685
Iteration 146/1000 | Loss: 0.00001685
Iteration 147/1000 | Loss: 0.00001685
Iteration 148/1000 | Loss: 0.00001685
Iteration 149/1000 | Loss: 0.00001685
Iteration 150/1000 | Loss: 0.00001685
Iteration 151/1000 | Loss: 0.00001685
Iteration 152/1000 | Loss: 0.00001685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.685427560005337e-05, 1.685427560005337e-05, 1.685427560005337e-05, 1.685427560005337e-05, 1.685427560005337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.685427560005337e-05

Optimization complete. Final v2v error: 3.4800503253936768 mm

Highest mean error: 4.404870510101318 mm for frame 1

Lowest mean error: 3.2184433937072754 mm for frame 38

Saving results

Total time: 46.675745725631714
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789583
Iteration 2/25 | Loss: 0.00172390
Iteration 3/25 | Loss: 0.00094757
Iteration 4/25 | Loss: 0.00085095
Iteration 5/25 | Loss: 0.00075252
Iteration 6/25 | Loss: 0.00073679
Iteration 7/25 | Loss: 0.00072889
Iteration 8/25 | Loss: 0.00071487
Iteration 9/25 | Loss: 0.00070420
Iteration 10/25 | Loss: 0.00069909
Iteration 11/25 | Loss: 0.00069409
Iteration 12/25 | Loss: 0.00069335
Iteration 13/25 | Loss: 0.00069042
Iteration 14/25 | Loss: 0.00068947
Iteration 15/25 | Loss: 0.00068907
Iteration 16/25 | Loss: 0.00068895
Iteration 17/25 | Loss: 0.00068894
Iteration 18/25 | Loss: 0.00068894
Iteration 19/25 | Loss: 0.00068894
Iteration 20/25 | Loss: 0.00068894
Iteration 21/25 | Loss: 0.00068894
Iteration 22/25 | Loss: 0.00068893
Iteration 23/25 | Loss: 0.00068893
Iteration 24/25 | Loss: 0.00068893
Iteration 25/25 | Loss: 0.00068893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97980487
Iteration 2/25 | Loss: 0.00036577
Iteration 3/25 | Loss: 0.00036577
Iteration 4/25 | Loss: 0.00033594
Iteration 5/25 | Loss: 0.00033594
Iteration 6/25 | Loss: 0.00033593
Iteration 7/25 | Loss: 0.00033593
Iteration 8/25 | Loss: 0.00033593
Iteration 9/25 | Loss: 0.00033593
Iteration 10/25 | Loss: 0.00033593
Iteration 11/25 | Loss: 0.00033593
Iteration 12/25 | Loss: 0.00033593
Iteration 13/25 | Loss: 0.00033593
Iteration 14/25 | Loss: 0.00033593
Iteration 15/25 | Loss: 0.00033593
Iteration 16/25 | Loss: 0.00033593
Iteration 17/25 | Loss: 0.00033593
Iteration 18/25 | Loss: 0.00033593
Iteration 19/25 | Loss: 0.00033593
Iteration 20/25 | Loss: 0.00033593
Iteration 21/25 | Loss: 0.00033593
Iteration 22/25 | Loss: 0.00033593
Iteration 23/25 | Loss: 0.00033593
Iteration 24/25 | Loss: 0.00033593
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00033592997351661325, 0.00033592997351661325, 0.00033592997351661325, 0.00033592997351661325, 0.00033592997351661325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033592997351661325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033593
Iteration 2/1000 | Loss: 0.00006146
Iteration 3/1000 | Loss: 0.00004907
Iteration 4/1000 | Loss: 0.00004975
Iteration 5/1000 | Loss: 0.00002529
Iteration 6/1000 | Loss: 0.00001708
Iteration 7/1000 | Loss: 0.00008295
Iteration 8/1000 | Loss: 0.00001880
Iteration 9/1000 | Loss: 0.00001618
Iteration 10/1000 | Loss: 0.00001592
Iteration 11/1000 | Loss: 0.00001577
Iteration 12/1000 | Loss: 0.00001561
Iteration 13/1000 | Loss: 0.00001560
Iteration 14/1000 | Loss: 0.00001553
Iteration 15/1000 | Loss: 0.00001550
Iteration 16/1000 | Loss: 0.00004442
Iteration 17/1000 | Loss: 0.00001544
Iteration 18/1000 | Loss: 0.00001541
Iteration 19/1000 | Loss: 0.00001541
Iteration 20/1000 | Loss: 0.00001541
Iteration 21/1000 | Loss: 0.00001541
Iteration 22/1000 | Loss: 0.00001541
Iteration 23/1000 | Loss: 0.00001540
Iteration 24/1000 | Loss: 0.00001540
Iteration 25/1000 | Loss: 0.00001540
Iteration 26/1000 | Loss: 0.00001540
Iteration 27/1000 | Loss: 0.00001539
Iteration 28/1000 | Loss: 0.00001539
Iteration 29/1000 | Loss: 0.00001539
Iteration 30/1000 | Loss: 0.00001537
Iteration 31/1000 | Loss: 0.00001536
Iteration 32/1000 | Loss: 0.00001531
Iteration 33/1000 | Loss: 0.00001530
Iteration 34/1000 | Loss: 0.00004020
Iteration 35/1000 | Loss: 0.00004422
Iteration 36/1000 | Loss: 0.00001536
Iteration 37/1000 | Loss: 0.00002951
Iteration 38/1000 | Loss: 0.00003884
Iteration 39/1000 | Loss: 0.00002382
Iteration 40/1000 | Loss: 0.00001525
Iteration 41/1000 | Loss: 0.00001514
Iteration 42/1000 | Loss: 0.00001512
Iteration 43/1000 | Loss: 0.00001511
Iteration 44/1000 | Loss: 0.00001511
Iteration 45/1000 | Loss: 0.00001511
Iteration 46/1000 | Loss: 0.00001511
Iteration 47/1000 | Loss: 0.00001511
Iteration 48/1000 | Loss: 0.00001511
Iteration 49/1000 | Loss: 0.00001511
Iteration 50/1000 | Loss: 0.00001511
Iteration 51/1000 | Loss: 0.00001511
Iteration 52/1000 | Loss: 0.00001511
Iteration 53/1000 | Loss: 0.00001511
Iteration 54/1000 | Loss: 0.00001511
Iteration 55/1000 | Loss: 0.00001510
Iteration 56/1000 | Loss: 0.00001510
Iteration 57/1000 | Loss: 0.00001510
Iteration 58/1000 | Loss: 0.00001510
Iteration 59/1000 | Loss: 0.00001510
Iteration 60/1000 | Loss: 0.00001510
Iteration 61/1000 | Loss: 0.00001510
Iteration 62/1000 | Loss: 0.00001510
Iteration 63/1000 | Loss: 0.00001510
Iteration 64/1000 | Loss: 0.00001510
Iteration 65/1000 | Loss: 0.00001510
Iteration 66/1000 | Loss: 0.00001510
Iteration 67/1000 | Loss: 0.00001509
Iteration 68/1000 | Loss: 0.00001509
Iteration 69/1000 | Loss: 0.00001509
Iteration 70/1000 | Loss: 0.00001509
Iteration 71/1000 | Loss: 0.00001508
Iteration 72/1000 | Loss: 0.00001508
Iteration 73/1000 | Loss: 0.00001508
Iteration 74/1000 | Loss: 0.00001506
Iteration 75/1000 | Loss: 0.00001506
Iteration 76/1000 | Loss: 0.00001505
Iteration 77/1000 | Loss: 0.00001505
Iteration 78/1000 | Loss: 0.00001505
Iteration 79/1000 | Loss: 0.00001505
Iteration 80/1000 | Loss: 0.00001505
Iteration 81/1000 | Loss: 0.00001505
Iteration 82/1000 | Loss: 0.00001505
Iteration 83/1000 | Loss: 0.00001504
Iteration 84/1000 | Loss: 0.00001504
Iteration 85/1000 | Loss: 0.00001504
Iteration 86/1000 | Loss: 0.00001503
Iteration 87/1000 | Loss: 0.00001503
Iteration 88/1000 | Loss: 0.00001503
Iteration 89/1000 | Loss: 0.00001503
Iteration 90/1000 | Loss: 0.00001503
Iteration 91/1000 | Loss: 0.00001503
Iteration 92/1000 | Loss: 0.00001503
Iteration 93/1000 | Loss: 0.00001502
Iteration 94/1000 | Loss: 0.00001502
Iteration 95/1000 | Loss: 0.00001502
Iteration 96/1000 | Loss: 0.00001502
Iteration 97/1000 | Loss: 0.00001502
Iteration 98/1000 | Loss: 0.00001502
Iteration 99/1000 | Loss: 0.00001502
Iteration 100/1000 | Loss: 0.00001502
Iteration 101/1000 | Loss: 0.00001502
Iteration 102/1000 | Loss: 0.00001502
Iteration 103/1000 | Loss: 0.00001501
Iteration 104/1000 | Loss: 0.00001501
Iteration 105/1000 | Loss: 0.00001501
Iteration 106/1000 | Loss: 0.00001501
Iteration 107/1000 | Loss: 0.00001501
Iteration 108/1000 | Loss: 0.00001501
Iteration 109/1000 | Loss: 0.00001501
Iteration 110/1000 | Loss: 0.00001501
Iteration 111/1000 | Loss: 0.00001500
Iteration 112/1000 | Loss: 0.00001500
Iteration 113/1000 | Loss: 0.00001500
Iteration 114/1000 | Loss: 0.00001500
Iteration 115/1000 | Loss: 0.00001500
Iteration 116/1000 | Loss: 0.00001500
Iteration 117/1000 | Loss: 0.00001500
Iteration 118/1000 | Loss: 0.00001500
Iteration 119/1000 | Loss: 0.00001500
Iteration 120/1000 | Loss: 0.00001500
Iteration 121/1000 | Loss: 0.00001500
Iteration 122/1000 | Loss: 0.00001500
Iteration 123/1000 | Loss: 0.00001500
Iteration 124/1000 | Loss: 0.00001500
Iteration 125/1000 | Loss: 0.00001500
Iteration 126/1000 | Loss: 0.00001500
Iteration 127/1000 | Loss: 0.00001500
Iteration 128/1000 | Loss: 0.00001500
Iteration 129/1000 | Loss: 0.00001500
Iteration 130/1000 | Loss: 0.00001500
Iteration 131/1000 | Loss: 0.00001500
Iteration 132/1000 | Loss: 0.00001500
Iteration 133/1000 | Loss: 0.00001500
Iteration 134/1000 | Loss: 0.00001500
Iteration 135/1000 | Loss: 0.00001500
Iteration 136/1000 | Loss: 0.00001500
Iteration 137/1000 | Loss: 0.00001500
Iteration 138/1000 | Loss: 0.00001500
Iteration 139/1000 | Loss: 0.00001500
Iteration 140/1000 | Loss: 0.00001500
Iteration 141/1000 | Loss: 0.00001500
Iteration 142/1000 | Loss: 0.00001500
Iteration 143/1000 | Loss: 0.00001500
Iteration 144/1000 | Loss: 0.00001500
Iteration 145/1000 | Loss: 0.00001500
Iteration 146/1000 | Loss: 0.00001500
Iteration 147/1000 | Loss: 0.00001500
Iteration 148/1000 | Loss: 0.00001500
Iteration 149/1000 | Loss: 0.00001500
Iteration 150/1000 | Loss: 0.00001500
Iteration 151/1000 | Loss: 0.00001500
Iteration 152/1000 | Loss: 0.00001500
Iteration 153/1000 | Loss: 0.00001500
Iteration 154/1000 | Loss: 0.00001500
Iteration 155/1000 | Loss: 0.00001500
Iteration 156/1000 | Loss: 0.00001500
Iteration 157/1000 | Loss: 0.00001500
Iteration 158/1000 | Loss: 0.00001500
Iteration 159/1000 | Loss: 0.00001500
Iteration 160/1000 | Loss: 0.00001500
Iteration 161/1000 | Loss: 0.00001500
Iteration 162/1000 | Loss: 0.00001500
Iteration 163/1000 | Loss: 0.00001500
Iteration 164/1000 | Loss: 0.00001500
Iteration 165/1000 | Loss: 0.00001500
Iteration 166/1000 | Loss: 0.00001500
Iteration 167/1000 | Loss: 0.00001500
Iteration 168/1000 | Loss: 0.00001500
Iteration 169/1000 | Loss: 0.00001500
Iteration 170/1000 | Loss: 0.00001500
Iteration 171/1000 | Loss: 0.00001500
Iteration 172/1000 | Loss: 0.00001500
Iteration 173/1000 | Loss: 0.00001500
Iteration 174/1000 | Loss: 0.00001500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.4995172023191117e-05, 1.4995172023191117e-05, 1.4995172023191117e-05, 1.4995172023191117e-05, 1.4995172023191117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4995172023191117e-05

Optimization complete. Final v2v error: 3.2165627479553223 mm

Highest mean error: 9.001008033752441 mm for frame 133

Lowest mean error: 2.825437307357788 mm for frame 124

Saving results

Total time: 64.03888201713562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018051
Iteration 2/25 | Loss: 0.00271963
Iteration 3/25 | Loss: 0.00182319
Iteration 4/25 | Loss: 0.00166390
Iteration 5/25 | Loss: 0.00149274
Iteration 6/25 | Loss: 0.00147964
Iteration 7/25 | Loss: 0.00151222
Iteration 8/25 | Loss: 0.00138810
Iteration 9/25 | Loss: 0.00134027
Iteration 10/25 | Loss: 0.00126041
Iteration 11/25 | Loss: 0.00121949
Iteration 12/25 | Loss: 0.00119792
Iteration 13/25 | Loss: 0.00117858
Iteration 14/25 | Loss: 0.00118177
Iteration 15/25 | Loss: 0.00117235
Iteration 16/25 | Loss: 0.00116938
Iteration 17/25 | Loss: 0.00116566
Iteration 18/25 | Loss: 0.00115014
Iteration 19/25 | Loss: 0.00114439
Iteration 20/25 | Loss: 0.00114560
Iteration 21/25 | Loss: 0.00114513
Iteration 22/25 | Loss: 0.00113763
Iteration 23/25 | Loss: 0.00113829
Iteration 24/25 | Loss: 0.00113415
Iteration 25/25 | Loss: 0.00113574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51570272
Iteration 2/25 | Loss: 0.00466711
Iteration 3/25 | Loss: 0.00278788
Iteration 4/25 | Loss: 0.00278788
Iteration 5/25 | Loss: 0.00278788
Iteration 6/25 | Loss: 0.00278788
Iteration 7/25 | Loss: 0.00278788
Iteration 8/25 | Loss: 0.00278788
Iteration 9/25 | Loss: 0.00278788
Iteration 10/25 | Loss: 0.00278788
Iteration 11/25 | Loss: 0.00278788
Iteration 12/25 | Loss: 0.00278788
Iteration 13/25 | Loss: 0.00278788
Iteration 14/25 | Loss: 0.00278788
Iteration 15/25 | Loss: 0.00278788
Iteration 16/25 | Loss: 0.00278788
Iteration 17/25 | Loss: 0.00278788
Iteration 18/25 | Loss: 0.00278788
Iteration 19/25 | Loss: 0.00278788
Iteration 20/25 | Loss: 0.00278788
Iteration 21/25 | Loss: 0.00278788
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002787879668176174, 0.002787879668176174, 0.002787879668176174, 0.002787879668176174, 0.002787879668176174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002787879668176174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00278788
Iteration 2/1000 | Loss: 0.00343663
Iteration 3/1000 | Loss: 0.00350062
Iteration 4/1000 | Loss: 0.00278631
Iteration 5/1000 | Loss: 0.00087925
Iteration 6/1000 | Loss: 0.00377722
Iteration 7/1000 | Loss: 0.00398685
Iteration 8/1000 | Loss: 0.00636393
Iteration 9/1000 | Loss: 0.00867556
Iteration 10/1000 | Loss: 0.00414891
Iteration 11/1000 | Loss: 0.00108876
Iteration 12/1000 | Loss: 0.00215612
Iteration 13/1000 | Loss: 0.00099773
Iteration 14/1000 | Loss: 0.00091486
Iteration 15/1000 | Loss: 0.00070290
Iteration 16/1000 | Loss: 0.00045736
Iteration 17/1000 | Loss: 0.00135162
Iteration 18/1000 | Loss: 0.00059182
Iteration 19/1000 | Loss: 0.00093604
Iteration 20/1000 | Loss: 0.00129078
Iteration 21/1000 | Loss: 0.00208152
Iteration 22/1000 | Loss: 0.00044249
Iteration 23/1000 | Loss: 0.00061529
Iteration 24/1000 | Loss: 0.00061290
Iteration 25/1000 | Loss: 0.00035621
Iteration 26/1000 | Loss: 0.00038434
Iteration 27/1000 | Loss: 0.00065311
Iteration 28/1000 | Loss: 0.00062878
Iteration 29/1000 | Loss: 0.00122279
Iteration 30/1000 | Loss: 0.00063502
Iteration 31/1000 | Loss: 0.00090399
Iteration 32/1000 | Loss: 0.00041463
Iteration 33/1000 | Loss: 0.00053995
Iteration 34/1000 | Loss: 0.00045387
Iteration 35/1000 | Loss: 0.00053525
Iteration 36/1000 | Loss: 0.00044219
Iteration 37/1000 | Loss: 0.00053030
Iteration 38/1000 | Loss: 0.00048013
Iteration 39/1000 | Loss: 0.00106036
Iteration 40/1000 | Loss: 0.00042053
Iteration 41/1000 | Loss: 0.00050056
Iteration 42/1000 | Loss: 0.00056387
Iteration 43/1000 | Loss: 0.00053117
Iteration 44/1000 | Loss: 0.00058124
Iteration 45/1000 | Loss: 0.00080959
Iteration 46/1000 | Loss: 0.00073508
Iteration 47/1000 | Loss: 0.00038974
Iteration 48/1000 | Loss: 0.00045222
Iteration 49/1000 | Loss: 0.00066074
Iteration 50/1000 | Loss: 0.00105811
Iteration 51/1000 | Loss: 0.00087251
Iteration 52/1000 | Loss: 0.00052522
Iteration 53/1000 | Loss: 0.00045949
Iteration 54/1000 | Loss: 0.00072019
Iteration 55/1000 | Loss: 0.00097371
Iteration 56/1000 | Loss: 0.00038444
Iteration 57/1000 | Loss: 0.00080173
Iteration 58/1000 | Loss: 0.00056316
Iteration 59/1000 | Loss: 0.00043269
Iteration 60/1000 | Loss: 0.00076163
Iteration 61/1000 | Loss: 0.00038939
Iteration 62/1000 | Loss: 0.00039626
Iteration 63/1000 | Loss: 0.00082526
Iteration 64/1000 | Loss: 0.00040562
Iteration 65/1000 | Loss: 0.00057916
Iteration 66/1000 | Loss: 0.00062961
Iteration 67/1000 | Loss: 0.00035122
Iteration 68/1000 | Loss: 0.00027474
Iteration 69/1000 | Loss: 0.00098086
Iteration 70/1000 | Loss: 0.00069383
Iteration 71/1000 | Loss: 0.00031553
Iteration 72/1000 | Loss: 0.00034992
Iteration 73/1000 | Loss: 0.00025058
Iteration 74/1000 | Loss: 0.00064429
Iteration 75/1000 | Loss: 0.00072074
Iteration 76/1000 | Loss: 0.00045365
Iteration 77/1000 | Loss: 0.00043548
Iteration 78/1000 | Loss: 0.00039069
Iteration 79/1000 | Loss: 0.00041780
Iteration 80/1000 | Loss: 0.00043306
Iteration 81/1000 | Loss: 0.00033955
Iteration 82/1000 | Loss: 0.00017502
Iteration 83/1000 | Loss: 0.00021261
Iteration 84/1000 | Loss: 0.00142819
Iteration 85/1000 | Loss: 0.00059221
Iteration 86/1000 | Loss: 0.00045182
Iteration 87/1000 | Loss: 0.00046376
Iteration 88/1000 | Loss: 0.00024468
Iteration 89/1000 | Loss: 0.00072204
Iteration 90/1000 | Loss: 0.00042803
Iteration 91/1000 | Loss: 0.00027737
Iteration 92/1000 | Loss: 0.00058844
Iteration 93/1000 | Loss: 0.00039907
Iteration 94/1000 | Loss: 0.00015972
Iteration 95/1000 | Loss: 0.00016947
Iteration 96/1000 | Loss: 0.00034836
Iteration 97/1000 | Loss: 0.00016811
Iteration 98/1000 | Loss: 0.00024690
Iteration 99/1000 | Loss: 0.00065491
Iteration 100/1000 | Loss: 0.00017064
Iteration 101/1000 | Loss: 0.00034111
Iteration 102/1000 | Loss: 0.00016320
Iteration 103/1000 | Loss: 0.00015739
Iteration 104/1000 | Loss: 0.00016020
Iteration 105/1000 | Loss: 0.00017999
Iteration 106/1000 | Loss: 0.00078561
Iteration 107/1000 | Loss: 0.00049976
Iteration 108/1000 | Loss: 0.00032479
Iteration 109/1000 | Loss: 0.00116896
Iteration 110/1000 | Loss: 0.00024895
Iteration 111/1000 | Loss: 0.00069439
Iteration 112/1000 | Loss: 0.00078987
Iteration 113/1000 | Loss: 0.00049392
Iteration 114/1000 | Loss: 0.00056580
Iteration 115/1000 | Loss: 0.00019092
Iteration 116/1000 | Loss: 0.00025015
Iteration 117/1000 | Loss: 0.00016541
Iteration 118/1000 | Loss: 0.00015660
Iteration 119/1000 | Loss: 0.00029486
Iteration 120/1000 | Loss: 0.00102762
Iteration 121/1000 | Loss: 0.00032051
Iteration 122/1000 | Loss: 0.00016584
Iteration 123/1000 | Loss: 0.00029113
Iteration 124/1000 | Loss: 0.00016043
Iteration 125/1000 | Loss: 0.00016218
Iteration 126/1000 | Loss: 0.00015716
Iteration 127/1000 | Loss: 0.00015314
Iteration 128/1000 | Loss: 0.00089393
Iteration 129/1000 | Loss: 0.00047140
Iteration 130/1000 | Loss: 0.00016221
Iteration 131/1000 | Loss: 0.00032318
Iteration 132/1000 | Loss: 0.00024044
Iteration 133/1000 | Loss: 0.00027429
Iteration 134/1000 | Loss: 0.00098075
Iteration 135/1000 | Loss: 0.00142837
Iteration 136/1000 | Loss: 0.00179500
Iteration 137/1000 | Loss: 0.00047230
Iteration 138/1000 | Loss: 0.00021211
Iteration 139/1000 | Loss: 0.00015552
Iteration 140/1000 | Loss: 0.00026943
Iteration 141/1000 | Loss: 0.00015292
Iteration 142/1000 | Loss: 0.00048376
Iteration 143/1000 | Loss: 0.00032460
Iteration 144/1000 | Loss: 0.00052221
Iteration 145/1000 | Loss: 0.00016918
Iteration 146/1000 | Loss: 0.00137956
Iteration 147/1000 | Loss: 0.00208052
Iteration 148/1000 | Loss: 0.00118103
Iteration 149/1000 | Loss: 0.00090058
Iteration 150/1000 | Loss: 0.00110921
Iteration 151/1000 | Loss: 0.00096819
Iteration 152/1000 | Loss: 0.00067643
Iteration 153/1000 | Loss: 0.00037520
Iteration 154/1000 | Loss: 0.00062609
Iteration 155/1000 | Loss: 0.00031780
Iteration 156/1000 | Loss: 0.00016409
Iteration 157/1000 | Loss: 0.00017500
Iteration 158/1000 | Loss: 0.00058108
Iteration 159/1000 | Loss: 0.00034796
Iteration 160/1000 | Loss: 0.00037901
Iteration 161/1000 | Loss: 0.00031407
Iteration 162/1000 | Loss: 0.00032316
Iteration 163/1000 | Loss: 0.00024744
Iteration 164/1000 | Loss: 0.00059952
Iteration 165/1000 | Loss: 0.00037134
Iteration 166/1000 | Loss: 0.00026464
Iteration 167/1000 | Loss: 0.00032057
Iteration 168/1000 | Loss: 0.00089431
Iteration 169/1000 | Loss: 0.00032760
Iteration 170/1000 | Loss: 0.00033332
Iteration 171/1000 | Loss: 0.00016476
Iteration 172/1000 | Loss: 0.00016724
Iteration 173/1000 | Loss: 0.00018317
Iteration 174/1000 | Loss: 0.00018021
Iteration 175/1000 | Loss: 0.00030252
Iteration 176/1000 | Loss: 0.00017448
Iteration 177/1000 | Loss: 0.00015994
Iteration 178/1000 | Loss: 0.00015258
Iteration 179/1000 | Loss: 0.00014863
Iteration 180/1000 | Loss: 0.00014660
Iteration 181/1000 | Loss: 0.00014530
Iteration 182/1000 | Loss: 0.00035938
Iteration 183/1000 | Loss: 0.00045610
Iteration 184/1000 | Loss: 0.00014393
Iteration 185/1000 | Loss: 0.00014340
Iteration 186/1000 | Loss: 0.00014302
Iteration 187/1000 | Loss: 0.00014271
Iteration 188/1000 | Loss: 0.00091030
Iteration 189/1000 | Loss: 0.00015904
Iteration 190/1000 | Loss: 0.00014713
Iteration 191/1000 | Loss: 0.00014479
Iteration 192/1000 | Loss: 0.00014329
Iteration 193/1000 | Loss: 0.00014187
Iteration 194/1000 | Loss: 0.00014098
Iteration 195/1000 | Loss: 0.00014063
Iteration 196/1000 | Loss: 0.00014041
Iteration 197/1000 | Loss: 0.00014036
Iteration 198/1000 | Loss: 0.00014035
Iteration 199/1000 | Loss: 0.00014034
Iteration 200/1000 | Loss: 0.00014034
Iteration 201/1000 | Loss: 0.00014033
Iteration 202/1000 | Loss: 0.00014032
Iteration 203/1000 | Loss: 0.00014031
Iteration 204/1000 | Loss: 0.00014030
Iteration 205/1000 | Loss: 0.00014029
Iteration 206/1000 | Loss: 0.00014029
Iteration 207/1000 | Loss: 0.00014028
Iteration 208/1000 | Loss: 0.00014028
Iteration 209/1000 | Loss: 0.00014027
Iteration 210/1000 | Loss: 0.00014013
Iteration 211/1000 | Loss: 0.00014013
Iteration 212/1000 | Loss: 0.00014013
Iteration 213/1000 | Loss: 0.00014012
Iteration 214/1000 | Loss: 0.00014012
Iteration 215/1000 | Loss: 0.00014012
Iteration 216/1000 | Loss: 0.00014011
Iteration 217/1000 | Loss: 0.00014005
Iteration 218/1000 | Loss: 0.00014003
Iteration 219/1000 | Loss: 0.00014002
Iteration 220/1000 | Loss: 0.00014002
Iteration 221/1000 | Loss: 0.00014001
Iteration 222/1000 | Loss: 0.00014001
Iteration 223/1000 | Loss: 0.00014001
Iteration 224/1000 | Loss: 0.00014000
Iteration 225/1000 | Loss: 0.00014000
Iteration 226/1000 | Loss: 0.00014000
Iteration 227/1000 | Loss: 0.00013999
Iteration 228/1000 | Loss: 0.00013999
Iteration 229/1000 | Loss: 0.00013999
Iteration 230/1000 | Loss: 0.00013998
Iteration 231/1000 | Loss: 0.00013998
Iteration 232/1000 | Loss: 0.00013998
Iteration 233/1000 | Loss: 0.00013998
Iteration 234/1000 | Loss: 0.00013997
Iteration 235/1000 | Loss: 0.00013997
Iteration 236/1000 | Loss: 0.00013997
Iteration 237/1000 | Loss: 0.00013996
Iteration 238/1000 | Loss: 0.00013996
Iteration 239/1000 | Loss: 0.00013996
Iteration 240/1000 | Loss: 0.00013996
Iteration 241/1000 | Loss: 0.00013995
Iteration 242/1000 | Loss: 0.00013995
Iteration 243/1000 | Loss: 0.00013995
Iteration 244/1000 | Loss: 0.00013995
Iteration 245/1000 | Loss: 0.00013995
Iteration 246/1000 | Loss: 0.00013995
Iteration 247/1000 | Loss: 0.00013995
Iteration 248/1000 | Loss: 0.00013995
Iteration 249/1000 | Loss: 0.00013994
Iteration 250/1000 | Loss: 0.00013994
Iteration 251/1000 | Loss: 0.00013994
Iteration 252/1000 | Loss: 0.00013994
Iteration 253/1000 | Loss: 0.00013994
Iteration 254/1000 | Loss: 0.00013994
Iteration 255/1000 | Loss: 0.00013994
Iteration 256/1000 | Loss: 0.00013994
Iteration 257/1000 | Loss: 0.00013993
Iteration 258/1000 | Loss: 0.00013993
Iteration 259/1000 | Loss: 0.00013993
Iteration 260/1000 | Loss: 0.00013993
Iteration 261/1000 | Loss: 0.00013993
Iteration 262/1000 | Loss: 0.00013992
Iteration 263/1000 | Loss: 0.00013992
Iteration 264/1000 | Loss: 0.00013992
Iteration 265/1000 | Loss: 0.00013992
Iteration 266/1000 | Loss: 0.00013992
Iteration 267/1000 | Loss: 0.00013992
Iteration 268/1000 | Loss: 0.00013992
Iteration 269/1000 | Loss: 0.00013992
Iteration 270/1000 | Loss: 0.00013991
Iteration 271/1000 | Loss: 0.00013991
Iteration 272/1000 | Loss: 0.00013991
Iteration 273/1000 | Loss: 0.00013991
Iteration 274/1000 | Loss: 0.00013991
Iteration 275/1000 | Loss: 0.00013991
Iteration 276/1000 | Loss: 0.00013991
Iteration 277/1000 | Loss: 0.00013991
Iteration 278/1000 | Loss: 0.00013990
Iteration 279/1000 | Loss: 0.00013990
Iteration 280/1000 | Loss: 0.00013990
Iteration 281/1000 | Loss: 0.00013990
Iteration 282/1000 | Loss: 0.00013990
Iteration 283/1000 | Loss: 0.00013989
Iteration 284/1000 | Loss: 0.00013989
Iteration 285/1000 | Loss: 0.00013989
Iteration 286/1000 | Loss: 0.00013989
Iteration 287/1000 | Loss: 0.00013989
Iteration 288/1000 | Loss: 0.00013989
Iteration 289/1000 | Loss: 0.00013989
Iteration 290/1000 | Loss: 0.00013989
Iteration 291/1000 | Loss: 0.00013989
Iteration 292/1000 | Loss: 0.00013989
Iteration 293/1000 | Loss: 0.00013989
Iteration 294/1000 | Loss: 0.00013989
Iteration 295/1000 | Loss: 0.00013989
Iteration 296/1000 | Loss: 0.00013989
Iteration 297/1000 | Loss: 0.00013989
Iteration 298/1000 | Loss: 0.00013989
Iteration 299/1000 | Loss: 0.00013989
Iteration 300/1000 | Loss: 0.00013989
Iteration 301/1000 | Loss: 0.00013989
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 301. Stopping optimization.
Last 5 losses: [0.0001398885069647804, 0.0001398885069647804, 0.0001398885069647804, 0.0001398885069647804, 0.0001398885069647804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001398885069647804

Optimization complete. Final v2v error: 6.501052379608154 mm

Highest mean error: 12.487068176269531 mm for frame 134

Lowest mean error: 3.6495988368988037 mm for frame 103

Saving results

Total time: 373.1644768714905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00353400
Iteration 2/25 | Loss: 0.00094278
Iteration 3/25 | Loss: 0.00073134
Iteration 4/25 | Loss: 0.00068250
Iteration 5/25 | Loss: 0.00066164
Iteration 6/25 | Loss: 0.00065545
Iteration 7/25 | Loss: 0.00065348
Iteration 8/25 | Loss: 0.00065261
Iteration 9/25 | Loss: 0.00065251
Iteration 10/25 | Loss: 0.00065251
Iteration 11/25 | Loss: 0.00065251
Iteration 12/25 | Loss: 0.00065251
Iteration 13/25 | Loss: 0.00065251
Iteration 14/25 | Loss: 0.00065251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006525090429931879, 0.0006525090429931879, 0.0006525090429931879, 0.0006525090429931879, 0.0006525090429931879]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006525090429931879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49870682
Iteration 2/25 | Loss: 0.00037417
Iteration 3/25 | Loss: 0.00037417
Iteration 4/25 | Loss: 0.00037417
Iteration 5/25 | Loss: 0.00037417
Iteration 6/25 | Loss: 0.00037417
Iteration 7/25 | Loss: 0.00037417
Iteration 8/25 | Loss: 0.00037417
Iteration 9/25 | Loss: 0.00037417
Iteration 10/25 | Loss: 0.00037417
Iteration 11/25 | Loss: 0.00037417
Iteration 12/25 | Loss: 0.00037417
Iteration 13/25 | Loss: 0.00037417
Iteration 14/25 | Loss: 0.00037417
Iteration 15/25 | Loss: 0.00037417
Iteration 16/25 | Loss: 0.00037417
Iteration 17/25 | Loss: 0.00037417
Iteration 18/25 | Loss: 0.00037417
Iteration 19/25 | Loss: 0.00037417
Iteration 20/25 | Loss: 0.00037417
Iteration 21/25 | Loss: 0.00037417
Iteration 22/25 | Loss: 0.00037417
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0003741697291843593, 0.0003741697291843593, 0.0003741697291843593, 0.0003741697291843593, 0.0003741697291843593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003741697291843593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037417
Iteration 2/1000 | Loss: 0.00003860
Iteration 3/1000 | Loss: 0.00002747
Iteration 4/1000 | Loss: 0.00002139
Iteration 5/1000 | Loss: 0.00001964
Iteration 6/1000 | Loss: 0.00001852
Iteration 7/1000 | Loss: 0.00001779
Iteration 8/1000 | Loss: 0.00001732
Iteration 9/1000 | Loss: 0.00001696
Iteration 10/1000 | Loss: 0.00001671
Iteration 11/1000 | Loss: 0.00001652
Iteration 12/1000 | Loss: 0.00001639
Iteration 13/1000 | Loss: 0.00001638
Iteration 14/1000 | Loss: 0.00001627
Iteration 15/1000 | Loss: 0.00001625
Iteration 16/1000 | Loss: 0.00001618
Iteration 17/1000 | Loss: 0.00001612
Iteration 18/1000 | Loss: 0.00001611
Iteration 19/1000 | Loss: 0.00001608
Iteration 20/1000 | Loss: 0.00001606
Iteration 21/1000 | Loss: 0.00001606
Iteration 22/1000 | Loss: 0.00001603
Iteration 23/1000 | Loss: 0.00001603
Iteration 24/1000 | Loss: 0.00001603
Iteration 25/1000 | Loss: 0.00001602
Iteration 26/1000 | Loss: 0.00001601
Iteration 27/1000 | Loss: 0.00001600
Iteration 28/1000 | Loss: 0.00001599
Iteration 29/1000 | Loss: 0.00001599
Iteration 30/1000 | Loss: 0.00001599
Iteration 31/1000 | Loss: 0.00001599
Iteration 32/1000 | Loss: 0.00001598
Iteration 33/1000 | Loss: 0.00001598
Iteration 34/1000 | Loss: 0.00001598
Iteration 35/1000 | Loss: 0.00001598
Iteration 36/1000 | Loss: 0.00001598
Iteration 37/1000 | Loss: 0.00001598
Iteration 38/1000 | Loss: 0.00001598
Iteration 39/1000 | Loss: 0.00001597
Iteration 40/1000 | Loss: 0.00001597
Iteration 41/1000 | Loss: 0.00001597
Iteration 42/1000 | Loss: 0.00001596
Iteration 43/1000 | Loss: 0.00001595
Iteration 44/1000 | Loss: 0.00001595
Iteration 45/1000 | Loss: 0.00001595
Iteration 46/1000 | Loss: 0.00001595
Iteration 47/1000 | Loss: 0.00001595
Iteration 48/1000 | Loss: 0.00001595
Iteration 49/1000 | Loss: 0.00001595
Iteration 50/1000 | Loss: 0.00001595
Iteration 51/1000 | Loss: 0.00001595
Iteration 52/1000 | Loss: 0.00001595
Iteration 53/1000 | Loss: 0.00001595
Iteration 54/1000 | Loss: 0.00001595
Iteration 55/1000 | Loss: 0.00001594
Iteration 56/1000 | Loss: 0.00001594
Iteration 57/1000 | Loss: 0.00001594
Iteration 58/1000 | Loss: 0.00001593
Iteration 59/1000 | Loss: 0.00001593
Iteration 60/1000 | Loss: 0.00001592
Iteration 61/1000 | Loss: 0.00001592
Iteration 62/1000 | Loss: 0.00001592
Iteration 63/1000 | Loss: 0.00001592
Iteration 64/1000 | Loss: 0.00001591
Iteration 65/1000 | Loss: 0.00001591
Iteration 66/1000 | Loss: 0.00001591
Iteration 67/1000 | Loss: 0.00001591
Iteration 68/1000 | Loss: 0.00001590
Iteration 69/1000 | Loss: 0.00001590
Iteration 70/1000 | Loss: 0.00001590
Iteration 71/1000 | Loss: 0.00001590
Iteration 72/1000 | Loss: 0.00001590
Iteration 73/1000 | Loss: 0.00001589
Iteration 74/1000 | Loss: 0.00001589
Iteration 75/1000 | Loss: 0.00001589
Iteration 76/1000 | Loss: 0.00001589
Iteration 77/1000 | Loss: 0.00001588
Iteration 78/1000 | Loss: 0.00001588
Iteration 79/1000 | Loss: 0.00001588
Iteration 80/1000 | Loss: 0.00001588
Iteration 81/1000 | Loss: 0.00001588
Iteration 82/1000 | Loss: 0.00001588
Iteration 83/1000 | Loss: 0.00001588
Iteration 84/1000 | Loss: 0.00001588
Iteration 85/1000 | Loss: 0.00001588
Iteration 86/1000 | Loss: 0.00001587
Iteration 87/1000 | Loss: 0.00001587
Iteration 88/1000 | Loss: 0.00001587
Iteration 89/1000 | Loss: 0.00001587
Iteration 90/1000 | Loss: 0.00001587
Iteration 91/1000 | Loss: 0.00001586
Iteration 92/1000 | Loss: 0.00001586
Iteration 93/1000 | Loss: 0.00001586
Iteration 94/1000 | Loss: 0.00001586
Iteration 95/1000 | Loss: 0.00001586
Iteration 96/1000 | Loss: 0.00001586
Iteration 97/1000 | Loss: 0.00001585
Iteration 98/1000 | Loss: 0.00001585
Iteration 99/1000 | Loss: 0.00001585
Iteration 100/1000 | Loss: 0.00001585
Iteration 101/1000 | Loss: 0.00001585
Iteration 102/1000 | Loss: 0.00001585
Iteration 103/1000 | Loss: 0.00001585
Iteration 104/1000 | Loss: 0.00001585
Iteration 105/1000 | Loss: 0.00001585
Iteration 106/1000 | Loss: 0.00001585
Iteration 107/1000 | Loss: 0.00001585
Iteration 108/1000 | Loss: 0.00001585
Iteration 109/1000 | Loss: 0.00001585
Iteration 110/1000 | Loss: 0.00001585
Iteration 111/1000 | Loss: 0.00001585
Iteration 112/1000 | Loss: 0.00001585
Iteration 113/1000 | Loss: 0.00001585
Iteration 114/1000 | Loss: 0.00001585
Iteration 115/1000 | Loss: 0.00001585
Iteration 116/1000 | Loss: 0.00001585
Iteration 117/1000 | Loss: 0.00001585
Iteration 118/1000 | Loss: 0.00001585
Iteration 119/1000 | Loss: 0.00001585
Iteration 120/1000 | Loss: 0.00001585
Iteration 121/1000 | Loss: 0.00001585
Iteration 122/1000 | Loss: 0.00001585
Iteration 123/1000 | Loss: 0.00001585
Iteration 124/1000 | Loss: 0.00001585
Iteration 125/1000 | Loss: 0.00001585
Iteration 126/1000 | Loss: 0.00001585
Iteration 127/1000 | Loss: 0.00001585
Iteration 128/1000 | Loss: 0.00001585
Iteration 129/1000 | Loss: 0.00001585
Iteration 130/1000 | Loss: 0.00001585
Iteration 131/1000 | Loss: 0.00001585
Iteration 132/1000 | Loss: 0.00001585
Iteration 133/1000 | Loss: 0.00001585
Iteration 134/1000 | Loss: 0.00001585
Iteration 135/1000 | Loss: 0.00001585
Iteration 136/1000 | Loss: 0.00001585
Iteration 137/1000 | Loss: 0.00001585
Iteration 138/1000 | Loss: 0.00001585
Iteration 139/1000 | Loss: 0.00001585
Iteration 140/1000 | Loss: 0.00001585
Iteration 141/1000 | Loss: 0.00001585
Iteration 142/1000 | Loss: 0.00001585
Iteration 143/1000 | Loss: 0.00001585
Iteration 144/1000 | Loss: 0.00001585
Iteration 145/1000 | Loss: 0.00001585
Iteration 146/1000 | Loss: 0.00001585
Iteration 147/1000 | Loss: 0.00001585
Iteration 148/1000 | Loss: 0.00001585
Iteration 149/1000 | Loss: 0.00001585
Iteration 150/1000 | Loss: 0.00001585
Iteration 151/1000 | Loss: 0.00001585
Iteration 152/1000 | Loss: 0.00001585
Iteration 153/1000 | Loss: 0.00001585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.5850910131121054e-05, 1.5850910131121054e-05, 1.5850910131121054e-05, 1.5850910131121054e-05, 1.5850910131121054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5850910131121054e-05

Optimization complete. Final v2v error: 3.304133653640747 mm

Highest mean error: 4.669853210449219 mm for frame 150

Lowest mean error: 2.4688808917999268 mm for frame 166

Saving results

Total time: 41.24672174453735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827580
Iteration 2/25 | Loss: 0.00089238
Iteration 3/25 | Loss: 0.00070478
Iteration 4/25 | Loss: 0.00066746
Iteration 5/25 | Loss: 0.00065604
Iteration 6/25 | Loss: 0.00065335
Iteration 7/25 | Loss: 0.00065270
Iteration 8/25 | Loss: 0.00065270
Iteration 9/25 | Loss: 0.00065270
Iteration 10/25 | Loss: 0.00065270
Iteration 11/25 | Loss: 0.00065270
Iteration 12/25 | Loss: 0.00065270
Iteration 13/25 | Loss: 0.00065270
Iteration 14/25 | Loss: 0.00065270
Iteration 15/25 | Loss: 0.00065270
Iteration 16/25 | Loss: 0.00065270
Iteration 17/25 | Loss: 0.00065270
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006527025252580643, 0.0006527025252580643, 0.0006527025252580643, 0.0006527025252580643, 0.0006527025252580643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006527025252580643

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53168833
Iteration 2/25 | Loss: 0.00038226
Iteration 3/25 | Loss: 0.00038226
Iteration 4/25 | Loss: 0.00038226
Iteration 5/25 | Loss: 0.00038226
Iteration 6/25 | Loss: 0.00038226
Iteration 7/25 | Loss: 0.00038225
Iteration 8/25 | Loss: 0.00038225
Iteration 9/25 | Loss: 0.00038225
Iteration 10/25 | Loss: 0.00038225
Iteration 11/25 | Loss: 0.00038225
Iteration 12/25 | Loss: 0.00038225
Iteration 13/25 | Loss: 0.00038225
Iteration 14/25 | Loss: 0.00038225
Iteration 15/25 | Loss: 0.00038225
Iteration 16/25 | Loss: 0.00038225
Iteration 17/25 | Loss: 0.00038225
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003822538419626653, 0.0003822538419626653, 0.0003822538419626653, 0.0003822538419626653, 0.0003822538419626653]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003822538419626653

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038225
Iteration 2/1000 | Loss: 0.00003078
Iteration 3/1000 | Loss: 0.00002179
Iteration 4/1000 | Loss: 0.00001956
Iteration 5/1000 | Loss: 0.00001881
Iteration 6/1000 | Loss: 0.00001815
Iteration 7/1000 | Loss: 0.00001777
Iteration 8/1000 | Loss: 0.00001743
Iteration 9/1000 | Loss: 0.00001715
Iteration 10/1000 | Loss: 0.00001692
Iteration 11/1000 | Loss: 0.00001691
Iteration 12/1000 | Loss: 0.00001689
Iteration 13/1000 | Loss: 0.00001683
Iteration 14/1000 | Loss: 0.00001683
Iteration 15/1000 | Loss: 0.00001676
Iteration 16/1000 | Loss: 0.00001669
Iteration 17/1000 | Loss: 0.00001666
Iteration 18/1000 | Loss: 0.00001661
Iteration 19/1000 | Loss: 0.00001659
Iteration 20/1000 | Loss: 0.00001659
Iteration 21/1000 | Loss: 0.00001656
Iteration 22/1000 | Loss: 0.00001655
Iteration 23/1000 | Loss: 0.00001655
Iteration 24/1000 | Loss: 0.00001654
Iteration 25/1000 | Loss: 0.00001654
Iteration 26/1000 | Loss: 0.00001653
Iteration 27/1000 | Loss: 0.00001649
Iteration 28/1000 | Loss: 0.00001649
Iteration 29/1000 | Loss: 0.00001648
Iteration 30/1000 | Loss: 0.00001647
Iteration 31/1000 | Loss: 0.00001646
Iteration 32/1000 | Loss: 0.00001646
Iteration 33/1000 | Loss: 0.00001646
Iteration 34/1000 | Loss: 0.00001645
Iteration 35/1000 | Loss: 0.00001645
Iteration 36/1000 | Loss: 0.00001644
Iteration 37/1000 | Loss: 0.00001644
Iteration 38/1000 | Loss: 0.00001644
Iteration 39/1000 | Loss: 0.00001644
Iteration 40/1000 | Loss: 0.00001643
Iteration 41/1000 | Loss: 0.00001643
Iteration 42/1000 | Loss: 0.00001643
Iteration 43/1000 | Loss: 0.00001642
Iteration 44/1000 | Loss: 0.00001641
Iteration 45/1000 | Loss: 0.00001641
Iteration 46/1000 | Loss: 0.00001641
Iteration 47/1000 | Loss: 0.00001641
Iteration 48/1000 | Loss: 0.00001641
Iteration 49/1000 | Loss: 0.00001640
Iteration 50/1000 | Loss: 0.00001640
Iteration 51/1000 | Loss: 0.00001640
Iteration 52/1000 | Loss: 0.00001640
Iteration 53/1000 | Loss: 0.00001639
Iteration 54/1000 | Loss: 0.00001639
Iteration 55/1000 | Loss: 0.00001638
Iteration 56/1000 | Loss: 0.00001638
Iteration 57/1000 | Loss: 0.00001638
Iteration 58/1000 | Loss: 0.00001638
Iteration 59/1000 | Loss: 0.00001638
Iteration 60/1000 | Loss: 0.00001638
Iteration 61/1000 | Loss: 0.00001638
Iteration 62/1000 | Loss: 0.00001638
Iteration 63/1000 | Loss: 0.00001638
Iteration 64/1000 | Loss: 0.00001637
Iteration 65/1000 | Loss: 0.00001637
Iteration 66/1000 | Loss: 0.00001637
Iteration 67/1000 | Loss: 0.00001637
Iteration 68/1000 | Loss: 0.00001637
Iteration 69/1000 | Loss: 0.00001637
Iteration 70/1000 | Loss: 0.00001636
Iteration 71/1000 | Loss: 0.00001636
Iteration 72/1000 | Loss: 0.00001636
Iteration 73/1000 | Loss: 0.00001636
Iteration 74/1000 | Loss: 0.00001636
Iteration 75/1000 | Loss: 0.00001636
Iteration 76/1000 | Loss: 0.00001636
Iteration 77/1000 | Loss: 0.00001636
Iteration 78/1000 | Loss: 0.00001636
Iteration 79/1000 | Loss: 0.00001636
Iteration 80/1000 | Loss: 0.00001636
Iteration 81/1000 | Loss: 0.00001636
Iteration 82/1000 | Loss: 0.00001636
Iteration 83/1000 | Loss: 0.00001636
Iteration 84/1000 | Loss: 0.00001635
Iteration 85/1000 | Loss: 0.00001635
Iteration 86/1000 | Loss: 0.00001635
Iteration 87/1000 | Loss: 0.00001635
Iteration 88/1000 | Loss: 0.00001635
Iteration 89/1000 | Loss: 0.00001635
Iteration 90/1000 | Loss: 0.00001635
Iteration 91/1000 | Loss: 0.00001634
Iteration 92/1000 | Loss: 0.00001634
Iteration 93/1000 | Loss: 0.00001634
Iteration 94/1000 | Loss: 0.00001634
Iteration 95/1000 | Loss: 0.00001634
Iteration 96/1000 | Loss: 0.00001634
Iteration 97/1000 | Loss: 0.00001633
Iteration 98/1000 | Loss: 0.00001633
Iteration 99/1000 | Loss: 0.00001633
Iteration 100/1000 | Loss: 0.00001633
Iteration 101/1000 | Loss: 0.00001633
Iteration 102/1000 | Loss: 0.00001633
Iteration 103/1000 | Loss: 0.00001632
Iteration 104/1000 | Loss: 0.00001632
Iteration 105/1000 | Loss: 0.00001632
Iteration 106/1000 | Loss: 0.00001631
Iteration 107/1000 | Loss: 0.00001631
Iteration 108/1000 | Loss: 0.00001631
Iteration 109/1000 | Loss: 0.00001631
Iteration 110/1000 | Loss: 0.00001631
Iteration 111/1000 | Loss: 0.00001631
Iteration 112/1000 | Loss: 0.00001631
Iteration 113/1000 | Loss: 0.00001631
Iteration 114/1000 | Loss: 0.00001631
Iteration 115/1000 | Loss: 0.00001631
Iteration 116/1000 | Loss: 0.00001631
Iteration 117/1000 | Loss: 0.00001631
Iteration 118/1000 | Loss: 0.00001631
Iteration 119/1000 | Loss: 0.00001631
Iteration 120/1000 | Loss: 0.00001631
Iteration 121/1000 | Loss: 0.00001631
Iteration 122/1000 | Loss: 0.00001631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.631264058232773e-05, 1.631264058232773e-05, 1.631264058232773e-05, 1.631264058232773e-05, 1.631264058232773e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.631264058232773e-05

Optimization complete. Final v2v error: 3.421799898147583 mm

Highest mean error: 4.330275058746338 mm for frame 70

Lowest mean error: 2.829547166824341 mm for frame 143

Saving results

Total time: 35.2590594291687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392789
Iteration 2/25 | Loss: 0.00083734
Iteration 3/25 | Loss: 0.00066419
Iteration 4/25 | Loss: 0.00063351
Iteration 5/25 | Loss: 0.00062505
Iteration 6/25 | Loss: 0.00062326
Iteration 7/25 | Loss: 0.00062290
Iteration 8/25 | Loss: 0.00062290
Iteration 9/25 | Loss: 0.00062290
Iteration 10/25 | Loss: 0.00062290
Iteration 11/25 | Loss: 0.00062290
Iteration 12/25 | Loss: 0.00062290
Iteration 13/25 | Loss: 0.00062290
Iteration 14/25 | Loss: 0.00062290
Iteration 15/25 | Loss: 0.00062290
Iteration 16/25 | Loss: 0.00062290
Iteration 17/25 | Loss: 0.00062290
Iteration 18/25 | Loss: 0.00062290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000622901483438909, 0.000622901483438909, 0.000622901483438909, 0.000622901483438909, 0.000622901483438909]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000622901483438909

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46109188
Iteration 2/25 | Loss: 0.00028620
Iteration 3/25 | Loss: 0.00028619
Iteration 4/25 | Loss: 0.00028619
Iteration 5/25 | Loss: 0.00028619
Iteration 6/25 | Loss: 0.00028619
Iteration 7/25 | Loss: 0.00028619
Iteration 8/25 | Loss: 0.00028619
Iteration 9/25 | Loss: 0.00028619
Iteration 10/25 | Loss: 0.00028619
Iteration 11/25 | Loss: 0.00028619
Iteration 12/25 | Loss: 0.00028619
Iteration 13/25 | Loss: 0.00028619
Iteration 14/25 | Loss: 0.00028619
Iteration 15/25 | Loss: 0.00028619
Iteration 16/25 | Loss: 0.00028619
Iteration 17/25 | Loss: 0.00028619
Iteration 18/25 | Loss: 0.00028619
Iteration 19/25 | Loss: 0.00028619
Iteration 20/25 | Loss: 0.00028619
Iteration 21/25 | Loss: 0.00028619
Iteration 22/25 | Loss: 0.00028619
Iteration 23/25 | Loss: 0.00028619
Iteration 24/25 | Loss: 0.00028619
Iteration 25/25 | Loss: 0.00028619
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00028619059594348073, 0.00028619059594348073, 0.00028619059594348073, 0.00028619059594348073, 0.00028619059594348073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028619059594348073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028619
Iteration 2/1000 | Loss: 0.00003269
Iteration 3/1000 | Loss: 0.00002262
Iteration 4/1000 | Loss: 0.00001870
Iteration 5/1000 | Loss: 0.00001745
Iteration 6/1000 | Loss: 0.00001674
Iteration 7/1000 | Loss: 0.00001619
Iteration 8/1000 | Loss: 0.00001575
Iteration 9/1000 | Loss: 0.00001551
Iteration 10/1000 | Loss: 0.00001536
Iteration 11/1000 | Loss: 0.00001532
Iteration 12/1000 | Loss: 0.00001532
Iteration 13/1000 | Loss: 0.00001531
Iteration 14/1000 | Loss: 0.00001529
Iteration 15/1000 | Loss: 0.00001525
Iteration 16/1000 | Loss: 0.00001525
Iteration 17/1000 | Loss: 0.00001524
Iteration 18/1000 | Loss: 0.00001524
Iteration 19/1000 | Loss: 0.00001521
Iteration 20/1000 | Loss: 0.00001517
Iteration 21/1000 | Loss: 0.00001517
Iteration 22/1000 | Loss: 0.00001515
Iteration 23/1000 | Loss: 0.00001514
Iteration 24/1000 | Loss: 0.00001514
Iteration 25/1000 | Loss: 0.00001514
Iteration 26/1000 | Loss: 0.00001513
Iteration 27/1000 | Loss: 0.00001513
Iteration 28/1000 | Loss: 0.00001512
Iteration 29/1000 | Loss: 0.00001511
Iteration 30/1000 | Loss: 0.00001511
Iteration 31/1000 | Loss: 0.00001510
Iteration 32/1000 | Loss: 0.00001509
Iteration 33/1000 | Loss: 0.00001509
Iteration 34/1000 | Loss: 0.00001508
Iteration 35/1000 | Loss: 0.00001507
Iteration 36/1000 | Loss: 0.00001506
Iteration 37/1000 | Loss: 0.00001506
Iteration 38/1000 | Loss: 0.00001506
Iteration 39/1000 | Loss: 0.00001506
Iteration 40/1000 | Loss: 0.00001505
Iteration 41/1000 | Loss: 0.00001505
Iteration 42/1000 | Loss: 0.00001505
Iteration 43/1000 | Loss: 0.00001505
Iteration 44/1000 | Loss: 0.00001505
Iteration 45/1000 | Loss: 0.00001505
Iteration 46/1000 | Loss: 0.00001505
Iteration 47/1000 | Loss: 0.00001505
Iteration 48/1000 | Loss: 0.00001504
Iteration 49/1000 | Loss: 0.00001504
Iteration 50/1000 | Loss: 0.00001502
Iteration 51/1000 | Loss: 0.00001502
Iteration 52/1000 | Loss: 0.00001502
Iteration 53/1000 | Loss: 0.00001502
Iteration 54/1000 | Loss: 0.00001502
Iteration 55/1000 | Loss: 0.00001502
Iteration 56/1000 | Loss: 0.00001502
Iteration 57/1000 | Loss: 0.00001502
Iteration 58/1000 | Loss: 0.00001502
Iteration 59/1000 | Loss: 0.00001502
Iteration 60/1000 | Loss: 0.00001501
Iteration 61/1000 | Loss: 0.00001501
Iteration 62/1000 | Loss: 0.00001501
Iteration 63/1000 | Loss: 0.00001500
Iteration 64/1000 | Loss: 0.00001500
Iteration 65/1000 | Loss: 0.00001500
Iteration 66/1000 | Loss: 0.00001499
Iteration 67/1000 | Loss: 0.00001499
Iteration 68/1000 | Loss: 0.00001499
Iteration 69/1000 | Loss: 0.00001499
Iteration 70/1000 | Loss: 0.00001498
Iteration 71/1000 | Loss: 0.00001498
Iteration 72/1000 | Loss: 0.00001498
Iteration 73/1000 | Loss: 0.00001498
Iteration 74/1000 | Loss: 0.00001497
Iteration 75/1000 | Loss: 0.00001497
Iteration 76/1000 | Loss: 0.00001496
Iteration 77/1000 | Loss: 0.00001496
Iteration 78/1000 | Loss: 0.00001496
Iteration 79/1000 | Loss: 0.00001496
Iteration 80/1000 | Loss: 0.00001495
Iteration 81/1000 | Loss: 0.00001495
Iteration 82/1000 | Loss: 0.00001495
Iteration 83/1000 | Loss: 0.00001494
Iteration 84/1000 | Loss: 0.00001494
Iteration 85/1000 | Loss: 0.00001494
Iteration 86/1000 | Loss: 0.00001493
Iteration 87/1000 | Loss: 0.00001493
Iteration 88/1000 | Loss: 0.00001493
Iteration 89/1000 | Loss: 0.00001493
Iteration 90/1000 | Loss: 0.00001493
Iteration 91/1000 | Loss: 0.00001493
Iteration 92/1000 | Loss: 0.00001493
Iteration 93/1000 | Loss: 0.00001492
Iteration 94/1000 | Loss: 0.00001492
Iteration 95/1000 | Loss: 0.00001492
Iteration 96/1000 | Loss: 0.00001492
Iteration 97/1000 | Loss: 0.00001492
Iteration 98/1000 | Loss: 0.00001492
Iteration 99/1000 | Loss: 0.00001492
Iteration 100/1000 | Loss: 0.00001492
Iteration 101/1000 | Loss: 0.00001492
Iteration 102/1000 | Loss: 0.00001491
Iteration 103/1000 | Loss: 0.00001491
Iteration 104/1000 | Loss: 0.00001491
Iteration 105/1000 | Loss: 0.00001491
Iteration 106/1000 | Loss: 0.00001491
Iteration 107/1000 | Loss: 0.00001491
Iteration 108/1000 | Loss: 0.00001491
Iteration 109/1000 | Loss: 0.00001491
Iteration 110/1000 | Loss: 0.00001491
Iteration 111/1000 | Loss: 0.00001491
Iteration 112/1000 | Loss: 0.00001491
Iteration 113/1000 | Loss: 0.00001491
Iteration 114/1000 | Loss: 0.00001491
Iteration 115/1000 | Loss: 0.00001491
Iteration 116/1000 | Loss: 0.00001491
Iteration 117/1000 | Loss: 0.00001490
Iteration 118/1000 | Loss: 0.00001490
Iteration 119/1000 | Loss: 0.00001490
Iteration 120/1000 | Loss: 0.00001490
Iteration 121/1000 | Loss: 0.00001490
Iteration 122/1000 | Loss: 0.00001490
Iteration 123/1000 | Loss: 0.00001490
Iteration 124/1000 | Loss: 0.00001490
Iteration 125/1000 | Loss: 0.00001490
Iteration 126/1000 | Loss: 0.00001490
Iteration 127/1000 | Loss: 0.00001490
Iteration 128/1000 | Loss: 0.00001490
Iteration 129/1000 | Loss: 0.00001490
Iteration 130/1000 | Loss: 0.00001490
Iteration 131/1000 | Loss: 0.00001490
Iteration 132/1000 | Loss: 0.00001490
Iteration 133/1000 | Loss: 0.00001490
Iteration 134/1000 | Loss: 0.00001490
Iteration 135/1000 | Loss: 0.00001489
Iteration 136/1000 | Loss: 0.00001489
Iteration 137/1000 | Loss: 0.00001489
Iteration 138/1000 | Loss: 0.00001489
Iteration 139/1000 | Loss: 0.00001489
Iteration 140/1000 | Loss: 0.00001489
Iteration 141/1000 | Loss: 0.00001489
Iteration 142/1000 | Loss: 0.00001488
Iteration 143/1000 | Loss: 0.00001488
Iteration 144/1000 | Loss: 0.00001488
Iteration 145/1000 | Loss: 0.00001488
Iteration 146/1000 | Loss: 0.00001488
Iteration 147/1000 | Loss: 0.00001488
Iteration 148/1000 | Loss: 0.00001488
Iteration 149/1000 | Loss: 0.00001488
Iteration 150/1000 | Loss: 0.00001488
Iteration 151/1000 | Loss: 0.00001488
Iteration 152/1000 | Loss: 0.00001488
Iteration 153/1000 | Loss: 0.00001488
Iteration 154/1000 | Loss: 0.00001488
Iteration 155/1000 | Loss: 0.00001488
Iteration 156/1000 | Loss: 0.00001488
Iteration 157/1000 | Loss: 0.00001487
Iteration 158/1000 | Loss: 0.00001487
Iteration 159/1000 | Loss: 0.00001487
Iteration 160/1000 | Loss: 0.00001487
Iteration 161/1000 | Loss: 0.00001487
Iteration 162/1000 | Loss: 0.00001487
Iteration 163/1000 | Loss: 0.00001487
Iteration 164/1000 | Loss: 0.00001487
Iteration 165/1000 | Loss: 0.00001487
Iteration 166/1000 | Loss: 0.00001487
Iteration 167/1000 | Loss: 0.00001487
Iteration 168/1000 | Loss: 0.00001487
Iteration 169/1000 | Loss: 0.00001487
Iteration 170/1000 | Loss: 0.00001487
Iteration 171/1000 | Loss: 0.00001487
Iteration 172/1000 | Loss: 0.00001487
Iteration 173/1000 | Loss: 0.00001487
Iteration 174/1000 | Loss: 0.00001487
Iteration 175/1000 | Loss: 0.00001487
Iteration 176/1000 | Loss: 0.00001487
Iteration 177/1000 | Loss: 0.00001486
Iteration 178/1000 | Loss: 0.00001486
Iteration 179/1000 | Loss: 0.00001486
Iteration 180/1000 | Loss: 0.00001486
Iteration 181/1000 | Loss: 0.00001486
Iteration 182/1000 | Loss: 0.00001486
Iteration 183/1000 | Loss: 0.00001486
Iteration 184/1000 | Loss: 0.00001486
Iteration 185/1000 | Loss: 0.00001485
Iteration 186/1000 | Loss: 0.00001485
Iteration 187/1000 | Loss: 0.00001485
Iteration 188/1000 | Loss: 0.00001485
Iteration 189/1000 | Loss: 0.00001485
Iteration 190/1000 | Loss: 0.00001485
Iteration 191/1000 | Loss: 0.00001485
Iteration 192/1000 | Loss: 0.00001485
Iteration 193/1000 | Loss: 0.00001485
Iteration 194/1000 | Loss: 0.00001485
Iteration 195/1000 | Loss: 0.00001485
Iteration 196/1000 | Loss: 0.00001485
Iteration 197/1000 | Loss: 0.00001485
Iteration 198/1000 | Loss: 0.00001485
Iteration 199/1000 | Loss: 0.00001485
Iteration 200/1000 | Loss: 0.00001485
Iteration 201/1000 | Loss: 0.00001485
Iteration 202/1000 | Loss: 0.00001485
Iteration 203/1000 | Loss: 0.00001485
Iteration 204/1000 | Loss: 0.00001485
Iteration 205/1000 | Loss: 0.00001485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.4845518307993189e-05, 1.4845518307993189e-05, 1.4845518307993189e-05, 1.4845518307993189e-05, 1.4845518307993189e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4845518307993189e-05

Optimization complete. Final v2v error: 3.245039701461792 mm

Highest mean error: 3.8818607330322266 mm for frame 73

Lowest mean error: 2.920260429382324 mm for frame 3

Saving results

Total time: 37.90405893325806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020070
Iteration 2/25 | Loss: 0.00330695
Iteration 3/25 | Loss: 0.00224811
Iteration 4/25 | Loss: 0.00193383
Iteration 5/25 | Loss: 0.00192036
Iteration 6/25 | Loss: 0.00188106
Iteration 7/25 | Loss: 0.00170440
Iteration 8/25 | Loss: 0.00144282
Iteration 9/25 | Loss: 0.00126782
Iteration 10/25 | Loss: 0.00115363
Iteration 11/25 | Loss: 0.00110182
Iteration 12/25 | Loss: 0.00107291
Iteration 13/25 | Loss: 0.00105230
Iteration 14/25 | Loss: 0.00101503
Iteration 15/25 | Loss: 0.00097558
Iteration 16/25 | Loss: 0.00095701
Iteration 17/25 | Loss: 0.00095426
Iteration 18/25 | Loss: 0.00093936
Iteration 19/25 | Loss: 0.00093278
Iteration 20/25 | Loss: 0.00091876
Iteration 21/25 | Loss: 0.00092235
Iteration 22/25 | Loss: 0.00091796
Iteration 23/25 | Loss: 0.00091565
Iteration 24/25 | Loss: 0.00091742
Iteration 25/25 | Loss: 0.00092196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43552959
Iteration 2/25 | Loss: 0.00262716
Iteration 3/25 | Loss: 0.00189723
Iteration 4/25 | Loss: 0.00189723
Iteration 5/25 | Loss: 0.00189723
Iteration 6/25 | Loss: 0.00189723
Iteration 7/25 | Loss: 0.00189723
Iteration 8/25 | Loss: 0.00189723
Iteration 9/25 | Loss: 0.00189723
Iteration 10/25 | Loss: 0.00189723
Iteration 11/25 | Loss: 0.00189723
Iteration 12/25 | Loss: 0.00189723
Iteration 13/25 | Loss: 0.00189723
Iteration 14/25 | Loss: 0.00189723
Iteration 15/25 | Loss: 0.00189723
Iteration 16/25 | Loss: 0.00189723
Iteration 17/25 | Loss: 0.00189723
Iteration 18/25 | Loss: 0.00189723
Iteration 19/25 | Loss: 0.00189723
Iteration 20/25 | Loss: 0.00189723
Iteration 21/25 | Loss: 0.00189723
Iteration 22/25 | Loss: 0.00189723
Iteration 23/25 | Loss: 0.00189723
Iteration 24/25 | Loss: 0.00189723
Iteration 25/25 | Loss: 0.00189723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189723
Iteration 2/1000 | Loss: 0.00129473
Iteration 3/1000 | Loss: 0.00213658
Iteration 4/1000 | Loss: 0.00546242
Iteration 5/1000 | Loss: 0.00028404
Iteration 6/1000 | Loss: 0.00032708
Iteration 7/1000 | Loss: 0.00078471
Iteration 8/1000 | Loss: 0.00037347
Iteration 9/1000 | Loss: 0.00034799
Iteration 10/1000 | Loss: 0.00054079
Iteration 11/1000 | Loss: 0.00025004
Iteration 12/1000 | Loss: 0.00030932
Iteration 13/1000 | Loss: 0.00061663
Iteration 14/1000 | Loss: 0.00021126
Iteration 15/1000 | Loss: 0.00030278
Iteration 16/1000 | Loss: 0.00018562
Iteration 17/1000 | Loss: 0.00063934
Iteration 18/1000 | Loss: 0.00042797
Iteration 19/1000 | Loss: 0.00014319
Iteration 20/1000 | Loss: 0.00065276
Iteration 21/1000 | Loss: 0.00112046
Iteration 22/1000 | Loss: 0.00122181
Iteration 23/1000 | Loss: 0.00049766
Iteration 24/1000 | Loss: 0.00192335
Iteration 25/1000 | Loss: 0.00187460
Iteration 26/1000 | Loss: 0.00201519
Iteration 27/1000 | Loss: 0.00194782
Iteration 28/1000 | Loss: 0.00179720
Iteration 29/1000 | Loss: 0.00041750
Iteration 30/1000 | Loss: 0.00061472
Iteration 31/1000 | Loss: 0.00066140
Iteration 32/1000 | Loss: 0.00036631
Iteration 33/1000 | Loss: 0.00024260
Iteration 34/1000 | Loss: 0.00016324
Iteration 35/1000 | Loss: 0.00026379
Iteration 36/1000 | Loss: 0.00065501
Iteration 37/1000 | Loss: 0.00026202
Iteration 38/1000 | Loss: 0.00013127
Iteration 39/1000 | Loss: 0.00027485
Iteration 40/1000 | Loss: 0.00067389
Iteration 41/1000 | Loss: 0.00010742
Iteration 42/1000 | Loss: 0.00018254
Iteration 43/1000 | Loss: 0.00022421
Iteration 44/1000 | Loss: 0.00016227
Iteration 45/1000 | Loss: 0.00093049
Iteration 46/1000 | Loss: 0.00204993
Iteration 47/1000 | Loss: 0.00063914
Iteration 48/1000 | Loss: 0.00028529
Iteration 49/1000 | Loss: 0.00029964
Iteration 50/1000 | Loss: 0.00027343
Iteration 51/1000 | Loss: 0.00013918
Iteration 52/1000 | Loss: 0.00012631
Iteration 53/1000 | Loss: 0.00009617
Iteration 54/1000 | Loss: 0.00020617
Iteration 55/1000 | Loss: 0.00009957
Iteration 56/1000 | Loss: 0.00006736
Iteration 57/1000 | Loss: 0.00008550
Iteration 58/1000 | Loss: 0.00010887
Iteration 59/1000 | Loss: 0.00007420
Iteration 60/1000 | Loss: 0.00008307
Iteration 61/1000 | Loss: 0.00007957
Iteration 62/1000 | Loss: 0.00026851
Iteration 63/1000 | Loss: 0.00008225
Iteration 64/1000 | Loss: 0.00028772
Iteration 65/1000 | Loss: 0.00008146
Iteration 66/1000 | Loss: 0.00007985
Iteration 67/1000 | Loss: 0.00007994
Iteration 68/1000 | Loss: 0.00007665
Iteration 69/1000 | Loss: 0.00008082
Iteration 70/1000 | Loss: 0.00017850
Iteration 71/1000 | Loss: 0.00015719
Iteration 72/1000 | Loss: 0.00008722
Iteration 73/1000 | Loss: 0.00007866
Iteration 74/1000 | Loss: 0.00021360
Iteration 75/1000 | Loss: 0.00019546
Iteration 76/1000 | Loss: 0.00021709
Iteration 77/1000 | Loss: 0.00020854
Iteration 78/1000 | Loss: 0.00035101
Iteration 79/1000 | Loss: 0.00022593
Iteration 80/1000 | Loss: 0.00008912
Iteration 81/1000 | Loss: 0.00008312
Iteration 82/1000 | Loss: 0.00008845
Iteration 83/1000 | Loss: 0.00008582
Iteration 84/1000 | Loss: 0.00007973
Iteration 85/1000 | Loss: 0.00008056
Iteration 86/1000 | Loss: 0.00010472
Iteration 87/1000 | Loss: 0.00009421
Iteration 88/1000 | Loss: 0.00014935
Iteration 89/1000 | Loss: 0.00007318
Iteration 90/1000 | Loss: 0.00008273
Iteration 91/1000 | Loss: 0.00007543
Iteration 92/1000 | Loss: 0.00010320
Iteration 93/1000 | Loss: 0.00061366
Iteration 94/1000 | Loss: 0.00034203
Iteration 95/1000 | Loss: 0.00007061
Iteration 96/1000 | Loss: 0.00009528
Iteration 97/1000 | Loss: 0.00008092
Iteration 98/1000 | Loss: 0.00008136
Iteration 99/1000 | Loss: 0.00008185
Iteration 100/1000 | Loss: 0.00008216
Iteration 101/1000 | Loss: 0.00008048
Iteration 102/1000 | Loss: 0.00007559
Iteration 103/1000 | Loss: 0.00008313
Iteration 104/1000 | Loss: 0.00007862
Iteration 105/1000 | Loss: 0.00052544
Iteration 106/1000 | Loss: 0.00020187
Iteration 107/1000 | Loss: 0.00010500
Iteration 108/1000 | Loss: 0.00007554
Iteration 109/1000 | Loss: 0.00008376
Iteration 110/1000 | Loss: 0.00008793
Iteration 111/1000 | Loss: 0.00007803
Iteration 112/1000 | Loss: 0.00006239
Iteration 113/1000 | Loss: 0.00005690
Iteration 114/1000 | Loss: 0.00005395
Iteration 115/1000 | Loss: 0.00005216
Iteration 116/1000 | Loss: 0.00007190
Iteration 117/1000 | Loss: 0.00007638
Iteration 118/1000 | Loss: 0.00005252
Iteration 119/1000 | Loss: 0.00005500
Iteration 120/1000 | Loss: 0.00005479
Iteration 121/1000 | Loss: 0.00006149
Iteration 122/1000 | Loss: 0.00005111
Iteration 123/1000 | Loss: 0.00004884
Iteration 124/1000 | Loss: 0.00007364
Iteration 125/1000 | Loss: 0.00006395
Iteration 126/1000 | Loss: 0.00007269
Iteration 127/1000 | Loss: 0.00005672
Iteration 128/1000 | Loss: 0.00007201
Iteration 129/1000 | Loss: 0.00005202
Iteration 130/1000 | Loss: 0.00004931
Iteration 131/1000 | Loss: 0.00004803
Iteration 132/1000 | Loss: 0.00004743
Iteration 133/1000 | Loss: 0.00004703
Iteration 134/1000 | Loss: 0.00004647
Iteration 135/1000 | Loss: 0.00006071
Iteration 136/1000 | Loss: 0.00004735
Iteration 137/1000 | Loss: 0.00004648
Iteration 138/1000 | Loss: 0.00004593
Iteration 139/1000 | Loss: 0.00004570
Iteration 140/1000 | Loss: 0.00004556
Iteration 141/1000 | Loss: 0.00004534
Iteration 142/1000 | Loss: 0.00004525
Iteration 143/1000 | Loss: 0.00004519
Iteration 144/1000 | Loss: 0.00004517
Iteration 145/1000 | Loss: 0.00004516
Iteration 146/1000 | Loss: 0.00004515
Iteration 147/1000 | Loss: 0.00004514
Iteration 148/1000 | Loss: 0.00004511
Iteration 149/1000 | Loss: 0.00004510
Iteration 150/1000 | Loss: 0.00004509
Iteration 151/1000 | Loss: 0.00004508
Iteration 152/1000 | Loss: 0.00004508
Iteration 153/1000 | Loss: 0.00004507
Iteration 154/1000 | Loss: 0.00004507
Iteration 155/1000 | Loss: 0.00004506
Iteration 156/1000 | Loss: 0.00004506
Iteration 157/1000 | Loss: 0.00004505
Iteration 158/1000 | Loss: 0.00004505
Iteration 159/1000 | Loss: 0.00004504
Iteration 160/1000 | Loss: 0.00004504
Iteration 161/1000 | Loss: 0.00004503
Iteration 162/1000 | Loss: 0.00004503
Iteration 163/1000 | Loss: 0.00004503
Iteration 164/1000 | Loss: 0.00004503
Iteration 165/1000 | Loss: 0.00004503
Iteration 166/1000 | Loss: 0.00004503
Iteration 167/1000 | Loss: 0.00004503
Iteration 168/1000 | Loss: 0.00004503
Iteration 169/1000 | Loss: 0.00004503
Iteration 170/1000 | Loss: 0.00004502
Iteration 171/1000 | Loss: 0.00004502
Iteration 172/1000 | Loss: 0.00004502
Iteration 173/1000 | Loss: 0.00004501
Iteration 174/1000 | Loss: 0.00004501
Iteration 175/1000 | Loss: 0.00004501
Iteration 176/1000 | Loss: 0.00004501
Iteration 177/1000 | Loss: 0.00004500
Iteration 178/1000 | Loss: 0.00004500
Iteration 179/1000 | Loss: 0.00004500
Iteration 180/1000 | Loss: 0.00004500
Iteration 181/1000 | Loss: 0.00004500
Iteration 182/1000 | Loss: 0.00004499
Iteration 183/1000 | Loss: 0.00004499
Iteration 184/1000 | Loss: 0.00004499
Iteration 185/1000 | Loss: 0.00004499
Iteration 186/1000 | Loss: 0.00004498
Iteration 187/1000 | Loss: 0.00004498
Iteration 188/1000 | Loss: 0.00004498
Iteration 189/1000 | Loss: 0.00004498
Iteration 190/1000 | Loss: 0.00004498
Iteration 191/1000 | Loss: 0.00004497
Iteration 192/1000 | Loss: 0.00004497
Iteration 193/1000 | Loss: 0.00004497
Iteration 194/1000 | Loss: 0.00004497
Iteration 195/1000 | Loss: 0.00004497
Iteration 196/1000 | Loss: 0.00004497
Iteration 197/1000 | Loss: 0.00004497
Iteration 198/1000 | Loss: 0.00004497
Iteration 199/1000 | Loss: 0.00004497
Iteration 200/1000 | Loss: 0.00004497
Iteration 201/1000 | Loss: 0.00004497
Iteration 202/1000 | Loss: 0.00004496
Iteration 203/1000 | Loss: 0.00004496
Iteration 204/1000 | Loss: 0.00004496
Iteration 205/1000 | Loss: 0.00004496
Iteration 206/1000 | Loss: 0.00004496
Iteration 207/1000 | Loss: 0.00004496
Iteration 208/1000 | Loss: 0.00004496
Iteration 209/1000 | Loss: 0.00004496
Iteration 210/1000 | Loss: 0.00004496
Iteration 211/1000 | Loss: 0.00004496
Iteration 212/1000 | Loss: 0.00004496
Iteration 213/1000 | Loss: 0.00004495
Iteration 214/1000 | Loss: 0.00004495
Iteration 215/1000 | Loss: 0.00004495
Iteration 216/1000 | Loss: 0.00004495
Iteration 217/1000 | Loss: 0.00004495
Iteration 218/1000 | Loss: 0.00004495
Iteration 219/1000 | Loss: 0.00004495
Iteration 220/1000 | Loss: 0.00004495
Iteration 221/1000 | Loss: 0.00004495
Iteration 222/1000 | Loss: 0.00004495
Iteration 223/1000 | Loss: 0.00004495
Iteration 224/1000 | Loss: 0.00004495
Iteration 225/1000 | Loss: 0.00004495
Iteration 226/1000 | Loss: 0.00004495
Iteration 227/1000 | Loss: 0.00004495
Iteration 228/1000 | Loss: 0.00004495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [4.494822496781126e-05, 4.494822496781126e-05, 4.494822496781126e-05, 4.494822496781126e-05, 4.494822496781126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.494822496781126e-05

Optimization complete. Final v2v error: 4.335846424102783 mm

Highest mean error: 12.859460830688477 mm for frame 201

Lowest mean error: 3.65812087059021 mm for frame 6

Saving results

Total time: 282.9396288394928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00335053
Iteration 2/25 | Loss: 0.00110147
Iteration 3/25 | Loss: 0.00073326
Iteration 4/25 | Loss: 0.00063960
Iteration 5/25 | Loss: 0.00062609
Iteration 6/25 | Loss: 0.00062306
Iteration 7/25 | Loss: 0.00062222
Iteration 8/25 | Loss: 0.00062199
Iteration 9/25 | Loss: 0.00062199
Iteration 10/25 | Loss: 0.00062199
Iteration 11/25 | Loss: 0.00062199
Iteration 12/25 | Loss: 0.00062199
Iteration 13/25 | Loss: 0.00062199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006219868082553148, 0.0006219868082553148, 0.0006219868082553148, 0.0006219868082553148, 0.0006219868082553148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006219868082553148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50532496
Iteration 2/25 | Loss: 0.00033204
Iteration 3/25 | Loss: 0.00033204
Iteration 4/25 | Loss: 0.00033204
Iteration 5/25 | Loss: 0.00033204
Iteration 6/25 | Loss: 0.00033204
Iteration 7/25 | Loss: 0.00033204
Iteration 8/25 | Loss: 0.00033204
Iteration 9/25 | Loss: 0.00033204
Iteration 10/25 | Loss: 0.00033204
Iteration 11/25 | Loss: 0.00033204
Iteration 12/25 | Loss: 0.00033204
Iteration 13/25 | Loss: 0.00033204
Iteration 14/25 | Loss: 0.00033204
Iteration 15/25 | Loss: 0.00033204
Iteration 16/25 | Loss: 0.00033204
Iteration 17/25 | Loss: 0.00033204
Iteration 18/25 | Loss: 0.00033204
Iteration 19/25 | Loss: 0.00033204
Iteration 20/25 | Loss: 0.00033204
Iteration 21/25 | Loss: 0.00033204
Iteration 22/25 | Loss: 0.00033204
Iteration 23/25 | Loss: 0.00033204
Iteration 24/25 | Loss: 0.00033204
Iteration 25/25 | Loss: 0.00033204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033204
Iteration 2/1000 | Loss: 0.00002849
Iteration 3/1000 | Loss: 0.00001941
Iteration 4/1000 | Loss: 0.00001536
Iteration 5/1000 | Loss: 0.00001420
Iteration 6/1000 | Loss: 0.00001335
Iteration 7/1000 | Loss: 0.00001290
Iteration 8/1000 | Loss: 0.00001257
Iteration 9/1000 | Loss: 0.00001231
Iteration 10/1000 | Loss: 0.00001213
Iteration 11/1000 | Loss: 0.00001199
Iteration 12/1000 | Loss: 0.00001193
Iteration 13/1000 | Loss: 0.00001188
Iteration 14/1000 | Loss: 0.00001185
Iteration 15/1000 | Loss: 0.00001175
Iteration 16/1000 | Loss: 0.00001171
Iteration 17/1000 | Loss: 0.00001167
Iteration 18/1000 | Loss: 0.00001167
Iteration 19/1000 | Loss: 0.00001166
Iteration 20/1000 | Loss: 0.00001165
Iteration 21/1000 | Loss: 0.00001165
Iteration 22/1000 | Loss: 0.00001164
Iteration 23/1000 | Loss: 0.00001164
Iteration 24/1000 | Loss: 0.00001163
Iteration 25/1000 | Loss: 0.00001163
Iteration 26/1000 | Loss: 0.00001162
Iteration 27/1000 | Loss: 0.00001162
Iteration 28/1000 | Loss: 0.00001162
Iteration 29/1000 | Loss: 0.00001162
Iteration 30/1000 | Loss: 0.00001161
Iteration 31/1000 | Loss: 0.00001161
Iteration 32/1000 | Loss: 0.00001161
Iteration 33/1000 | Loss: 0.00001161
Iteration 34/1000 | Loss: 0.00001160
Iteration 35/1000 | Loss: 0.00001160
Iteration 36/1000 | Loss: 0.00001160
Iteration 37/1000 | Loss: 0.00001160
Iteration 38/1000 | Loss: 0.00001160
Iteration 39/1000 | Loss: 0.00001160
Iteration 40/1000 | Loss: 0.00001159
Iteration 41/1000 | Loss: 0.00001159
Iteration 42/1000 | Loss: 0.00001159
Iteration 43/1000 | Loss: 0.00001159
Iteration 44/1000 | Loss: 0.00001159
Iteration 45/1000 | Loss: 0.00001159
Iteration 46/1000 | Loss: 0.00001159
Iteration 47/1000 | Loss: 0.00001159
Iteration 48/1000 | Loss: 0.00001159
Iteration 49/1000 | Loss: 0.00001158
Iteration 50/1000 | Loss: 0.00001158
Iteration 51/1000 | Loss: 0.00001158
Iteration 52/1000 | Loss: 0.00001157
Iteration 53/1000 | Loss: 0.00001157
Iteration 54/1000 | Loss: 0.00001157
Iteration 55/1000 | Loss: 0.00001157
Iteration 56/1000 | Loss: 0.00001156
Iteration 57/1000 | Loss: 0.00001156
Iteration 58/1000 | Loss: 0.00001156
Iteration 59/1000 | Loss: 0.00001156
Iteration 60/1000 | Loss: 0.00001156
Iteration 61/1000 | Loss: 0.00001156
Iteration 62/1000 | Loss: 0.00001156
Iteration 63/1000 | Loss: 0.00001156
Iteration 64/1000 | Loss: 0.00001156
Iteration 65/1000 | Loss: 0.00001156
Iteration 66/1000 | Loss: 0.00001156
Iteration 67/1000 | Loss: 0.00001156
Iteration 68/1000 | Loss: 0.00001155
Iteration 69/1000 | Loss: 0.00001155
Iteration 70/1000 | Loss: 0.00001155
Iteration 71/1000 | Loss: 0.00001154
Iteration 72/1000 | Loss: 0.00001154
Iteration 73/1000 | Loss: 0.00001154
Iteration 74/1000 | Loss: 0.00001154
Iteration 75/1000 | Loss: 0.00001154
Iteration 76/1000 | Loss: 0.00001154
Iteration 77/1000 | Loss: 0.00001154
Iteration 78/1000 | Loss: 0.00001154
Iteration 79/1000 | Loss: 0.00001153
Iteration 80/1000 | Loss: 0.00001153
Iteration 81/1000 | Loss: 0.00001153
Iteration 82/1000 | Loss: 0.00001153
Iteration 83/1000 | Loss: 0.00001153
Iteration 84/1000 | Loss: 0.00001152
Iteration 85/1000 | Loss: 0.00001152
Iteration 86/1000 | Loss: 0.00001152
Iteration 87/1000 | Loss: 0.00001152
Iteration 88/1000 | Loss: 0.00001152
Iteration 89/1000 | Loss: 0.00001151
Iteration 90/1000 | Loss: 0.00001151
Iteration 91/1000 | Loss: 0.00001151
Iteration 92/1000 | Loss: 0.00001151
Iteration 93/1000 | Loss: 0.00001151
Iteration 94/1000 | Loss: 0.00001151
Iteration 95/1000 | Loss: 0.00001151
Iteration 96/1000 | Loss: 0.00001151
Iteration 97/1000 | Loss: 0.00001151
Iteration 98/1000 | Loss: 0.00001151
Iteration 99/1000 | Loss: 0.00001151
Iteration 100/1000 | Loss: 0.00001151
Iteration 101/1000 | Loss: 0.00001151
Iteration 102/1000 | Loss: 0.00001151
Iteration 103/1000 | Loss: 0.00001151
Iteration 104/1000 | Loss: 0.00001151
Iteration 105/1000 | Loss: 0.00001151
Iteration 106/1000 | Loss: 0.00001151
Iteration 107/1000 | Loss: 0.00001151
Iteration 108/1000 | Loss: 0.00001151
Iteration 109/1000 | Loss: 0.00001151
Iteration 110/1000 | Loss: 0.00001151
Iteration 111/1000 | Loss: 0.00001151
Iteration 112/1000 | Loss: 0.00001151
Iteration 113/1000 | Loss: 0.00001151
Iteration 114/1000 | Loss: 0.00001151
Iteration 115/1000 | Loss: 0.00001151
Iteration 116/1000 | Loss: 0.00001151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.1508395800774451e-05, 1.1508395800774451e-05, 1.1508395800774451e-05, 1.1508395800774451e-05, 1.1508395800774451e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1508395800774451e-05

Optimization complete. Final v2v error: 2.8999886512756348 mm

Highest mean error: 3.1827003955841064 mm for frame 28

Lowest mean error: 2.7477645874023438 mm for frame 62

Saving results

Total time: 34.82139468193054
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_022/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_022/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00981172
Iteration 2/25 | Loss: 0.00204694
Iteration 3/25 | Loss: 0.00129225
Iteration 4/25 | Loss: 0.00110072
Iteration 5/25 | Loss: 0.00099843
Iteration 6/25 | Loss: 0.00091495
Iteration 7/25 | Loss: 0.00086652
Iteration 8/25 | Loss: 0.00084150
Iteration 9/25 | Loss: 0.00082420
Iteration 10/25 | Loss: 0.00080351
Iteration 11/25 | Loss: 0.00080251
Iteration 12/25 | Loss: 0.00079149
Iteration 13/25 | Loss: 0.00078940
Iteration 14/25 | Loss: 0.00078852
Iteration 15/25 | Loss: 0.00078978
Iteration 16/25 | Loss: 0.00078565
Iteration 17/25 | Loss: 0.00078477
Iteration 18/25 | Loss: 0.00078440
Iteration 19/25 | Loss: 0.00078409
Iteration 20/25 | Loss: 0.00078329
Iteration 21/25 | Loss: 0.00078267
Iteration 22/25 | Loss: 0.00078601
Iteration 23/25 | Loss: 0.00078237
Iteration 24/25 | Loss: 0.00077616
Iteration 25/25 | Loss: 0.00077852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56070817
Iteration 2/25 | Loss: 0.00122837
Iteration 3/25 | Loss: 0.00045227
Iteration 4/25 | Loss: 0.00045227
Iteration 5/25 | Loss: 0.00045227
Iteration 6/25 | Loss: 0.00045227
Iteration 7/25 | Loss: 0.00045227
Iteration 8/25 | Loss: 0.00045227
Iteration 9/25 | Loss: 0.00045227
Iteration 10/25 | Loss: 0.00045227
Iteration 11/25 | Loss: 0.00045227
Iteration 12/25 | Loss: 0.00045227
Iteration 13/25 | Loss: 0.00045227
Iteration 14/25 | Loss: 0.00045227
Iteration 15/25 | Loss: 0.00045227
Iteration 16/25 | Loss: 0.00045227
Iteration 17/25 | Loss: 0.00045227
Iteration 18/25 | Loss: 0.00045227
Iteration 19/25 | Loss: 0.00045227
Iteration 20/25 | Loss: 0.00045227
Iteration 21/25 | Loss: 0.00045227
Iteration 22/25 | Loss: 0.00045227
Iteration 23/25 | Loss: 0.00045227
Iteration 24/25 | Loss: 0.00045227
Iteration 25/25 | Loss: 0.00045227

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045227
Iteration 2/1000 | Loss: 0.00056443
Iteration 3/1000 | Loss: 0.00043835
Iteration 4/1000 | Loss: 0.00029007
Iteration 5/1000 | Loss: 0.00022182
Iteration 6/1000 | Loss: 0.00003873
Iteration 7/1000 | Loss: 0.00003558
Iteration 8/1000 | Loss: 0.00070155
Iteration 9/1000 | Loss: 0.00033338
Iteration 10/1000 | Loss: 0.00210555
Iteration 11/1000 | Loss: 0.00030203
Iteration 12/1000 | Loss: 0.00011431
Iteration 13/1000 | Loss: 0.00003063
Iteration 14/1000 | Loss: 0.00022500
Iteration 15/1000 | Loss: 0.00003065
Iteration 16/1000 | Loss: 0.00009147
Iteration 17/1000 | Loss: 0.00002916
Iteration 18/1000 | Loss: 0.00002626
Iteration 19/1000 | Loss: 0.00002432
Iteration 20/1000 | Loss: 0.00002351
Iteration 21/1000 | Loss: 0.00002285
Iteration 22/1000 | Loss: 0.00002237
Iteration 23/1000 | Loss: 0.00031337
Iteration 24/1000 | Loss: 0.00007960
Iteration 25/1000 | Loss: 0.00016585
Iteration 26/1000 | Loss: 0.00002292
Iteration 27/1000 | Loss: 0.00002194
Iteration 28/1000 | Loss: 0.00002186
Iteration 29/1000 | Loss: 0.00002163
Iteration 30/1000 | Loss: 0.00002147
Iteration 31/1000 | Loss: 0.00002142
Iteration 32/1000 | Loss: 0.00002138
Iteration 33/1000 | Loss: 0.00002138
Iteration 34/1000 | Loss: 0.00002138
Iteration 35/1000 | Loss: 0.00002138
Iteration 36/1000 | Loss: 0.00002138
Iteration 37/1000 | Loss: 0.00002138
Iteration 38/1000 | Loss: 0.00002138
Iteration 39/1000 | Loss: 0.00002138
Iteration 40/1000 | Loss: 0.00002138
Iteration 41/1000 | Loss: 0.00002136
Iteration 42/1000 | Loss: 0.00002131
Iteration 43/1000 | Loss: 0.00002131
Iteration 44/1000 | Loss: 0.00002131
Iteration 45/1000 | Loss: 0.00002131
Iteration 46/1000 | Loss: 0.00002130
Iteration 47/1000 | Loss: 0.00002128
Iteration 48/1000 | Loss: 0.00002127
Iteration 49/1000 | Loss: 0.00002126
Iteration 50/1000 | Loss: 0.00002126
Iteration 51/1000 | Loss: 0.00002124
Iteration 52/1000 | Loss: 0.00002115
Iteration 53/1000 | Loss: 0.00002115
Iteration 54/1000 | Loss: 0.00002115
Iteration 55/1000 | Loss: 0.00002115
Iteration 56/1000 | Loss: 0.00002115
Iteration 57/1000 | Loss: 0.00002115
Iteration 58/1000 | Loss: 0.00002114
Iteration 59/1000 | Loss: 0.00002114
Iteration 60/1000 | Loss: 0.00002114
Iteration 61/1000 | Loss: 0.00002114
Iteration 62/1000 | Loss: 0.00002114
Iteration 63/1000 | Loss: 0.00002114
Iteration 64/1000 | Loss: 0.00002114
Iteration 65/1000 | Loss: 0.00002114
Iteration 66/1000 | Loss: 0.00002114
Iteration 67/1000 | Loss: 0.00002113
Iteration 68/1000 | Loss: 0.00002113
Iteration 69/1000 | Loss: 0.00002111
Iteration 70/1000 | Loss: 0.00002111
Iteration 71/1000 | Loss: 0.00002111
Iteration 72/1000 | Loss: 0.00002111
Iteration 73/1000 | Loss: 0.00002111
Iteration 74/1000 | Loss: 0.00002111
Iteration 75/1000 | Loss: 0.00002111
Iteration 76/1000 | Loss: 0.00002111
Iteration 77/1000 | Loss: 0.00002111
Iteration 78/1000 | Loss: 0.00002110
Iteration 79/1000 | Loss: 0.00002110
Iteration 80/1000 | Loss: 0.00002110
Iteration 81/1000 | Loss: 0.00002110
Iteration 82/1000 | Loss: 0.00002110
Iteration 83/1000 | Loss: 0.00002109
Iteration 84/1000 | Loss: 0.00002109
Iteration 85/1000 | Loss: 0.00002109
Iteration 86/1000 | Loss: 0.00002108
Iteration 87/1000 | Loss: 0.00002108
Iteration 88/1000 | Loss: 0.00002108
Iteration 89/1000 | Loss: 0.00002108
Iteration 90/1000 | Loss: 0.00002108
Iteration 91/1000 | Loss: 0.00002108
Iteration 92/1000 | Loss: 0.00002108
Iteration 93/1000 | Loss: 0.00002108
Iteration 94/1000 | Loss: 0.00002108
Iteration 95/1000 | Loss: 0.00002108
Iteration 96/1000 | Loss: 0.00002108
Iteration 97/1000 | Loss: 0.00002108
Iteration 98/1000 | Loss: 0.00002108
Iteration 99/1000 | Loss: 0.00002108
Iteration 100/1000 | Loss: 0.00002108
Iteration 101/1000 | Loss: 0.00002108
Iteration 102/1000 | Loss: 0.00002108
Iteration 103/1000 | Loss: 0.00002108
Iteration 104/1000 | Loss: 0.00002108
Iteration 105/1000 | Loss: 0.00002108
Iteration 106/1000 | Loss: 0.00002108
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.1077321434859186e-05, 2.1077321434859186e-05, 2.1077321434859186e-05, 2.1077321434859186e-05, 2.1077321434859186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1077321434859186e-05

Optimization complete. Final v2v error: 3.9263014793395996 mm

Highest mean error: 4.752717018127441 mm for frame 25

Lowest mean error: 3.577772378921509 mm for frame 110

Saving results

Total time: 107.79300904273987
