Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=113, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 6328-6383
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423231
Iteration 2/25 | Loss: 0.00081321
Iteration 3/25 | Loss: 0.00070639
Iteration 4/25 | Loss: 0.00068190
Iteration 5/25 | Loss: 0.00067232
Iteration 6/25 | Loss: 0.00067080
Iteration 7/25 | Loss: 0.00067069
Iteration 8/25 | Loss: 0.00067069
Iteration 9/25 | Loss: 0.00067069
Iteration 10/25 | Loss: 0.00067069
Iteration 11/25 | Loss: 0.00067069
Iteration 12/25 | Loss: 0.00067069
Iteration 13/25 | Loss: 0.00067069
Iteration 14/25 | Loss: 0.00067069
Iteration 15/25 | Loss: 0.00067069
Iteration 16/25 | Loss: 0.00067069
Iteration 17/25 | Loss: 0.00067069
Iteration 18/25 | Loss: 0.00067069
Iteration 19/25 | Loss: 0.00067069
Iteration 20/25 | Loss: 0.00067069
Iteration 21/25 | Loss: 0.00067069
Iteration 22/25 | Loss: 0.00067069
Iteration 23/25 | Loss: 0.00067069
Iteration 24/25 | Loss: 0.00067069
Iteration 25/25 | Loss: 0.00067069

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44181204
Iteration 2/25 | Loss: 0.00013157
Iteration 3/25 | Loss: 0.00013156
Iteration 4/25 | Loss: 0.00013156
Iteration 5/25 | Loss: 0.00013156
Iteration 6/25 | Loss: 0.00013156
Iteration 7/25 | Loss: 0.00013156
Iteration 8/25 | Loss: 0.00013156
Iteration 9/25 | Loss: 0.00013156
Iteration 10/25 | Loss: 0.00013156
Iteration 11/25 | Loss: 0.00013156
Iteration 12/25 | Loss: 0.00013156
Iteration 13/25 | Loss: 0.00013156
Iteration 14/25 | Loss: 0.00013156
Iteration 15/25 | Loss: 0.00013156
Iteration 16/25 | Loss: 0.00013156
Iteration 17/25 | Loss: 0.00013156
Iteration 18/25 | Loss: 0.00013156
Iteration 19/25 | Loss: 0.00013156
Iteration 20/25 | Loss: 0.00013156
Iteration 21/25 | Loss: 0.00013156
Iteration 22/25 | Loss: 0.00013156
Iteration 23/25 | Loss: 0.00013156
Iteration 24/25 | Loss: 0.00013156
Iteration 25/25 | Loss: 0.00013156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00013156
Iteration 2/1000 | Loss: 0.00003793
Iteration 3/1000 | Loss: 0.00003081
Iteration 4/1000 | Loss: 0.00002883
Iteration 5/1000 | Loss: 0.00002719
Iteration 6/1000 | Loss: 0.00002602
Iteration 7/1000 | Loss: 0.00002529
Iteration 8/1000 | Loss: 0.00002485
Iteration 9/1000 | Loss: 0.00002455
Iteration 10/1000 | Loss: 0.00002453
Iteration 11/1000 | Loss: 0.00002431
Iteration 12/1000 | Loss: 0.00002427
Iteration 13/1000 | Loss: 0.00002427
Iteration 14/1000 | Loss: 0.00002425
Iteration 15/1000 | Loss: 0.00002423
Iteration 16/1000 | Loss: 0.00002421
Iteration 17/1000 | Loss: 0.00002419
Iteration 18/1000 | Loss: 0.00002407
Iteration 19/1000 | Loss: 0.00002401
Iteration 20/1000 | Loss: 0.00002400
Iteration 21/1000 | Loss: 0.00002399
Iteration 22/1000 | Loss: 0.00002398
Iteration 23/1000 | Loss: 0.00002397
Iteration 24/1000 | Loss: 0.00002397
Iteration 25/1000 | Loss: 0.00002397
Iteration 26/1000 | Loss: 0.00002396
Iteration 27/1000 | Loss: 0.00002396
Iteration 28/1000 | Loss: 0.00002395
Iteration 29/1000 | Loss: 0.00002395
Iteration 30/1000 | Loss: 0.00002392
Iteration 31/1000 | Loss: 0.00002392
Iteration 32/1000 | Loss: 0.00002391
Iteration 33/1000 | Loss: 0.00002391
Iteration 34/1000 | Loss: 0.00002391
Iteration 35/1000 | Loss: 0.00002390
Iteration 36/1000 | Loss: 0.00002389
Iteration 37/1000 | Loss: 0.00002389
Iteration 38/1000 | Loss: 0.00002389
Iteration 39/1000 | Loss: 0.00002389
Iteration 40/1000 | Loss: 0.00002389
Iteration 41/1000 | Loss: 0.00002389
Iteration 42/1000 | Loss: 0.00002389
Iteration 43/1000 | Loss: 0.00002389
Iteration 44/1000 | Loss: 0.00002389
Iteration 45/1000 | Loss: 0.00002389
Iteration 46/1000 | Loss: 0.00002389
Iteration 47/1000 | Loss: 0.00002388
Iteration 48/1000 | Loss: 0.00002388
Iteration 49/1000 | Loss: 0.00002388
Iteration 50/1000 | Loss: 0.00002388
Iteration 51/1000 | Loss: 0.00002388
Iteration 52/1000 | Loss: 0.00002388
Iteration 53/1000 | Loss: 0.00002387
Iteration 54/1000 | Loss: 0.00002387
Iteration 55/1000 | Loss: 0.00002387
Iteration 56/1000 | Loss: 0.00002387
Iteration 57/1000 | Loss: 0.00002387
Iteration 58/1000 | Loss: 0.00002387
Iteration 59/1000 | Loss: 0.00002387
Iteration 60/1000 | Loss: 0.00002387
Iteration 61/1000 | Loss: 0.00002387
Iteration 62/1000 | Loss: 0.00002387
Iteration 63/1000 | Loss: 0.00002387
Iteration 64/1000 | Loss: 0.00002386
Iteration 65/1000 | Loss: 0.00002386
Iteration 66/1000 | Loss: 0.00002386
Iteration 67/1000 | Loss: 0.00002386
Iteration 68/1000 | Loss: 0.00002386
Iteration 69/1000 | Loss: 0.00002386
Iteration 70/1000 | Loss: 0.00002386
Iteration 71/1000 | Loss: 0.00002386
Iteration 72/1000 | Loss: 0.00002386
Iteration 73/1000 | Loss: 0.00002385
Iteration 74/1000 | Loss: 0.00002385
Iteration 75/1000 | Loss: 0.00002385
Iteration 76/1000 | Loss: 0.00002385
Iteration 77/1000 | Loss: 0.00002385
Iteration 78/1000 | Loss: 0.00002385
Iteration 79/1000 | Loss: 0.00002385
Iteration 80/1000 | Loss: 0.00002385
Iteration 81/1000 | Loss: 0.00002385
Iteration 82/1000 | Loss: 0.00002385
Iteration 83/1000 | Loss: 0.00002385
Iteration 84/1000 | Loss: 0.00002385
Iteration 85/1000 | Loss: 0.00002384
Iteration 86/1000 | Loss: 0.00002384
Iteration 87/1000 | Loss: 0.00002384
Iteration 88/1000 | Loss: 0.00002384
Iteration 89/1000 | Loss: 0.00002384
Iteration 90/1000 | Loss: 0.00002384
Iteration 91/1000 | Loss: 0.00002383
Iteration 92/1000 | Loss: 0.00002383
Iteration 93/1000 | Loss: 0.00002383
Iteration 94/1000 | Loss: 0.00002383
Iteration 95/1000 | Loss: 0.00002383
Iteration 96/1000 | Loss: 0.00002383
Iteration 97/1000 | Loss: 0.00002383
Iteration 98/1000 | Loss: 0.00002382
Iteration 99/1000 | Loss: 0.00002382
Iteration 100/1000 | Loss: 0.00002382
Iteration 101/1000 | Loss: 0.00002382
Iteration 102/1000 | Loss: 0.00002382
Iteration 103/1000 | Loss: 0.00002382
Iteration 104/1000 | Loss: 0.00002382
Iteration 105/1000 | Loss: 0.00002382
Iteration 106/1000 | Loss: 0.00002381
Iteration 107/1000 | Loss: 0.00002381
Iteration 108/1000 | Loss: 0.00002381
Iteration 109/1000 | Loss: 0.00002381
Iteration 110/1000 | Loss: 0.00002381
Iteration 111/1000 | Loss: 0.00002380
Iteration 112/1000 | Loss: 0.00002380
Iteration 113/1000 | Loss: 0.00002380
Iteration 114/1000 | Loss: 0.00002380
Iteration 115/1000 | Loss: 0.00002380
Iteration 116/1000 | Loss: 0.00002380
Iteration 117/1000 | Loss: 0.00002379
Iteration 118/1000 | Loss: 0.00002379
Iteration 119/1000 | Loss: 0.00002379
Iteration 120/1000 | Loss: 0.00002379
Iteration 121/1000 | Loss: 0.00002379
Iteration 122/1000 | Loss: 0.00002379
Iteration 123/1000 | Loss: 0.00002379
Iteration 124/1000 | Loss: 0.00002378
Iteration 125/1000 | Loss: 0.00002378
Iteration 126/1000 | Loss: 0.00002378
Iteration 127/1000 | Loss: 0.00002377
Iteration 128/1000 | Loss: 0.00002377
Iteration 129/1000 | Loss: 0.00002377
Iteration 130/1000 | Loss: 0.00002377
Iteration 131/1000 | Loss: 0.00002377
Iteration 132/1000 | Loss: 0.00002377
Iteration 133/1000 | Loss: 0.00002376
Iteration 134/1000 | Loss: 0.00002376
Iteration 135/1000 | Loss: 0.00002376
Iteration 136/1000 | Loss: 0.00002376
Iteration 137/1000 | Loss: 0.00002376
Iteration 138/1000 | Loss: 0.00002375
Iteration 139/1000 | Loss: 0.00002375
Iteration 140/1000 | Loss: 0.00002375
Iteration 141/1000 | Loss: 0.00002375
Iteration 142/1000 | Loss: 0.00002375
Iteration 143/1000 | Loss: 0.00002375
Iteration 144/1000 | Loss: 0.00002375
Iteration 145/1000 | Loss: 0.00002375
Iteration 146/1000 | Loss: 0.00002375
Iteration 147/1000 | Loss: 0.00002375
Iteration 148/1000 | Loss: 0.00002374
Iteration 149/1000 | Loss: 0.00002374
Iteration 150/1000 | Loss: 0.00002374
Iteration 151/1000 | Loss: 0.00002374
Iteration 152/1000 | Loss: 0.00002374
Iteration 153/1000 | Loss: 0.00002374
Iteration 154/1000 | Loss: 0.00002373
Iteration 155/1000 | Loss: 0.00002373
Iteration 156/1000 | Loss: 0.00002373
Iteration 157/1000 | Loss: 0.00002373
Iteration 158/1000 | Loss: 0.00002373
Iteration 159/1000 | Loss: 0.00002373
Iteration 160/1000 | Loss: 0.00002373
Iteration 161/1000 | Loss: 0.00002373
Iteration 162/1000 | Loss: 0.00002373
Iteration 163/1000 | Loss: 0.00002372
Iteration 164/1000 | Loss: 0.00002372
Iteration 165/1000 | Loss: 0.00002372
Iteration 166/1000 | Loss: 0.00002372
Iteration 167/1000 | Loss: 0.00002372
Iteration 168/1000 | Loss: 0.00002372
Iteration 169/1000 | Loss: 0.00002372
Iteration 170/1000 | Loss: 0.00002372
Iteration 171/1000 | Loss: 0.00002372
Iteration 172/1000 | Loss: 0.00002372
Iteration 173/1000 | Loss: 0.00002371
Iteration 174/1000 | Loss: 0.00002371
Iteration 175/1000 | Loss: 0.00002371
Iteration 176/1000 | Loss: 0.00002371
Iteration 177/1000 | Loss: 0.00002371
Iteration 178/1000 | Loss: 0.00002371
Iteration 179/1000 | Loss: 0.00002371
Iteration 180/1000 | Loss: 0.00002371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.3712642359896563e-05, 2.3712642359896563e-05, 2.3712642359896563e-05, 2.3712642359896563e-05, 2.3712642359896563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3712642359896563e-05

Optimization complete. Final v2v error: 4.14028263092041 mm

Highest mean error: 4.70654821395874 mm for frame 233

Lowest mean error: 3.3582780361175537 mm for frame 0

Saving results

Total time: 44.68336629867554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806253
Iteration 2/25 | Loss: 0.00135370
Iteration 3/25 | Loss: 0.00086159
Iteration 4/25 | Loss: 0.00079051
Iteration 5/25 | Loss: 0.00078211
Iteration 6/25 | Loss: 0.00078054
Iteration 7/25 | Loss: 0.00078053
Iteration 8/25 | Loss: 0.00078053
Iteration 9/25 | Loss: 0.00078053
Iteration 10/25 | Loss: 0.00078053
Iteration 11/25 | Loss: 0.00078053
Iteration 12/25 | Loss: 0.00078053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007805303321219981, 0.0007805303321219981, 0.0007805303321219981, 0.0007805303321219981, 0.0007805303321219981]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007805303321219981

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42046106
Iteration 2/25 | Loss: 0.00018885
Iteration 3/25 | Loss: 0.00018883
Iteration 4/25 | Loss: 0.00018883
Iteration 5/25 | Loss: 0.00018883
Iteration 6/25 | Loss: 0.00018883
Iteration 7/25 | Loss: 0.00018883
Iteration 8/25 | Loss: 0.00018883
Iteration 9/25 | Loss: 0.00018883
Iteration 10/25 | Loss: 0.00018883
Iteration 11/25 | Loss: 0.00018883
Iteration 12/25 | Loss: 0.00018883
Iteration 13/25 | Loss: 0.00018883
Iteration 14/25 | Loss: 0.00018883
Iteration 15/25 | Loss: 0.00018883
Iteration 16/25 | Loss: 0.00018883
Iteration 17/25 | Loss: 0.00018883
Iteration 18/25 | Loss: 0.00018883
Iteration 19/25 | Loss: 0.00018883
Iteration 20/25 | Loss: 0.00018883
Iteration 21/25 | Loss: 0.00018883
Iteration 22/25 | Loss: 0.00018883
Iteration 23/25 | Loss: 0.00018883
Iteration 24/25 | Loss: 0.00018883
Iteration 25/25 | Loss: 0.00018883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018883
Iteration 2/1000 | Loss: 0.00003088
Iteration 3/1000 | Loss: 0.00002698
Iteration 4/1000 | Loss: 0.00002551
Iteration 5/1000 | Loss: 0.00002451
Iteration 6/1000 | Loss: 0.00002398
Iteration 7/1000 | Loss: 0.00002365
Iteration 8/1000 | Loss: 0.00002334
Iteration 9/1000 | Loss: 0.00002303
Iteration 10/1000 | Loss: 0.00002282
Iteration 11/1000 | Loss: 0.00002280
Iteration 12/1000 | Loss: 0.00002279
Iteration 13/1000 | Loss: 0.00002279
Iteration 14/1000 | Loss: 0.00002278
Iteration 15/1000 | Loss: 0.00002277
Iteration 16/1000 | Loss: 0.00002277
Iteration 17/1000 | Loss: 0.00002276
Iteration 18/1000 | Loss: 0.00002275
Iteration 19/1000 | Loss: 0.00002275
Iteration 20/1000 | Loss: 0.00002275
Iteration 21/1000 | Loss: 0.00002275
Iteration 22/1000 | Loss: 0.00002274
Iteration 23/1000 | Loss: 0.00002274
Iteration 24/1000 | Loss: 0.00002274
Iteration 25/1000 | Loss: 0.00002274
Iteration 26/1000 | Loss: 0.00002271
Iteration 27/1000 | Loss: 0.00002270
Iteration 28/1000 | Loss: 0.00002269
Iteration 29/1000 | Loss: 0.00002269
Iteration 30/1000 | Loss: 0.00002267
Iteration 31/1000 | Loss: 0.00002261
Iteration 32/1000 | Loss: 0.00002261
Iteration 33/1000 | Loss: 0.00002261
Iteration 34/1000 | Loss: 0.00002261
Iteration 35/1000 | Loss: 0.00002261
Iteration 36/1000 | Loss: 0.00002261
Iteration 37/1000 | Loss: 0.00002261
Iteration 38/1000 | Loss: 0.00002260
Iteration 39/1000 | Loss: 0.00002259
Iteration 40/1000 | Loss: 0.00002259
Iteration 41/1000 | Loss: 0.00002259
Iteration 42/1000 | Loss: 0.00002259
Iteration 43/1000 | Loss: 0.00002258
Iteration 44/1000 | Loss: 0.00002258
Iteration 45/1000 | Loss: 0.00002258
Iteration 46/1000 | Loss: 0.00002258
Iteration 47/1000 | Loss: 0.00002258
Iteration 48/1000 | Loss: 0.00002257
Iteration 49/1000 | Loss: 0.00002257
Iteration 50/1000 | Loss: 0.00002257
Iteration 51/1000 | Loss: 0.00002256
Iteration 52/1000 | Loss: 0.00002256
Iteration 53/1000 | Loss: 0.00002255
Iteration 54/1000 | Loss: 0.00002255
Iteration 55/1000 | Loss: 0.00002254
Iteration 56/1000 | Loss: 0.00002254
Iteration 57/1000 | Loss: 0.00002254
Iteration 58/1000 | Loss: 0.00002254
Iteration 59/1000 | Loss: 0.00002254
Iteration 60/1000 | Loss: 0.00002254
Iteration 61/1000 | Loss: 0.00002254
Iteration 62/1000 | Loss: 0.00002254
Iteration 63/1000 | Loss: 0.00002254
Iteration 64/1000 | Loss: 0.00002253
Iteration 65/1000 | Loss: 0.00002253
Iteration 66/1000 | Loss: 0.00002253
Iteration 67/1000 | Loss: 0.00002253
Iteration 68/1000 | Loss: 0.00002253
Iteration 69/1000 | Loss: 0.00002252
Iteration 70/1000 | Loss: 0.00002252
Iteration 71/1000 | Loss: 0.00002252
Iteration 72/1000 | Loss: 0.00002252
Iteration 73/1000 | Loss: 0.00002252
Iteration 74/1000 | Loss: 0.00002252
Iteration 75/1000 | Loss: 0.00002252
Iteration 76/1000 | Loss: 0.00002252
Iteration 77/1000 | Loss: 0.00002251
Iteration 78/1000 | Loss: 0.00002251
Iteration 79/1000 | Loss: 0.00002251
Iteration 80/1000 | Loss: 0.00002251
Iteration 81/1000 | Loss: 0.00002251
Iteration 82/1000 | Loss: 0.00002251
Iteration 83/1000 | Loss: 0.00002250
Iteration 84/1000 | Loss: 0.00002250
Iteration 85/1000 | Loss: 0.00002250
Iteration 86/1000 | Loss: 0.00002250
Iteration 87/1000 | Loss: 0.00002250
Iteration 88/1000 | Loss: 0.00002250
Iteration 89/1000 | Loss: 0.00002250
Iteration 90/1000 | Loss: 0.00002250
Iteration 91/1000 | Loss: 0.00002250
Iteration 92/1000 | Loss: 0.00002250
Iteration 93/1000 | Loss: 0.00002249
Iteration 94/1000 | Loss: 0.00002249
Iteration 95/1000 | Loss: 0.00002249
Iteration 96/1000 | Loss: 0.00002249
Iteration 97/1000 | Loss: 0.00002249
Iteration 98/1000 | Loss: 0.00002249
Iteration 99/1000 | Loss: 0.00002249
Iteration 100/1000 | Loss: 0.00002249
Iteration 101/1000 | Loss: 0.00002249
Iteration 102/1000 | Loss: 0.00002249
Iteration 103/1000 | Loss: 0.00002249
Iteration 104/1000 | Loss: 0.00002249
Iteration 105/1000 | Loss: 0.00002249
Iteration 106/1000 | Loss: 0.00002249
Iteration 107/1000 | Loss: 0.00002249
Iteration 108/1000 | Loss: 0.00002249
Iteration 109/1000 | Loss: 0.00002249
Iteration 110/1000 | Loss: 0.00002249
Iteration 111/1000 | Loss: 0.00002249
Iteration 112/1000 | Loss: 0.00002249
Iteration 113/1000 | Loss: 0.00002249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.2491889467346482e-05, 2.2491889467346482e-05, 2.2491889467346482e-05, 2.2491889467346482e-05, 2.2491889467346482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2491889467346482e-05

Optimization complete. Final v2v error: 4.106993675231934 mm

Highest mean error: 4.457324981689453 mm for frame 13

Lowest mean error: 3.7214717864990234 mm for frame 133

Saving results

Total time: 31.724388360977173
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013601
Iteration 2/25 | Loss: 0.00181439
Iteration 3/25 | Loss: 0.00116752
Iteration 4/25 | Loss: 0.00103237
Iteration 5/25 | Loss: 0.00111613
Iteration 6/25 | Loss: 0.00088795
Iteration 7/25 | Loss: 0.00076866
Iteration 8/25 | Loss: 0.00072848
Iteration 9/25 | Loss: 0.00072506
Iteration 10/25 | Loss: 0.00072471
Iteration 11/25 | Loss: 0.00071464
Iteration 12/25 | Loss: 0.00070244
Iteration 13/25 | Loss: 0.00069854
Iteration 14/25 | Loss: 0.00069003
Iteration 15/25 | Loss: 0.00068813
Iteration 16/25 | Loss: 0.00068478
Iteration 17/25 | Loss: 0.00068407
Iteration 18/25 | Loss: 0.00068493
Iteration 19/25 | Loss: 0.00068258
Iteration 20/25 | Loss: 0.00068484
Iteration 21/25 | Loss: 0.00068233
Iteration 22/25 | Loss: 0.00068144
Iteration 23/25 | Loss: 0.00068204
Iteration 24/25 | Loss: 0.00068196
Iteration 25/25 | Loss: 0.00067958

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39934361
Iteration 2/25 | Loss: 0.00024953
Iteration 3/25 | Loss: 0.00024953
Iteration 4/25 | Loss: 0.00024953
Iteration 5/25 | Loss: 0.00024953
Iteration 6/25 | Loss: 0.00024953
Iteration 7/25 | Loss: 0.00024953
Iteration 8/25 | Loss: 0.00024953
Iteration 9/25 | Loss: 0.00024953
Iteration 10/25 | Loss: 0.00024953
Iteration 11/25 | Loss: 0.00024953
Iteration 12/25 | Loss: 0.00024953
Iteration 13/25 | Loss: 0.00024953
Iteration 14/25 | Loss: 0.00024953
Iteration 15/25 | Loss: 0.00024953
Iteration 16/25 | Loss: 0.00024953
Iteration 17/25 | Loss: 0.00024953
Iteration 18/25 | Loss: 0.00024953
Iteration 19/25 | Loss: 0.00024953
Iteration 20/25 | Loss: 0.00024953
Iteration 21/25 | Loss: 0.00024953
Iteration 22/25 | Loss: 0.00024953
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00024953039246611297, 0.00024953039246611297, 0.00024953039246611297, 0.00024953039246611297, 0.00024953039246611297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024953039246611297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024953
Iteration 2/1000 | Loss: 0.00008166
Iteration 3/1000 | Loss: 0.00015457
Iteration 4/1000 | Loss: 0.00016012
Iteration 5/1000 | Loss: 0.00020837
Iteration 6/1000 | Loss: 0.00018237
Iteration 7/1000 | Loss: 0.00012190
Iteration 8/1000 | Loss: 0.00020289
Iteration 9/1000 | Loss: 0.00014700
Iteration 10/1000 | Loss: 0.00010807
Iteration 11/1000 | Loss: 0.00010565
Iteration 12/1000 | Loss: 0.00014176
Iteration 13/1000 | Loss: 0.00013568
Iteration 14/1000 | Loss: 0.00015077
Iteration 15/1000 | Loss: 0.00009918
Iteration 16/1000 | Loss: 0.00008547
Iteration 17/1000 | Loss: 0.00010585
Iteration 18/1000 | Loss: 0.00008410
Iteration 19/1000 | Loss: 0.00008363
Iteration 20/1000 | Loss: 0.00007883
Iteration 21/1000 | Loss: 0.00005960
Iteration 22/1000 | Loss: 0.00008930
Iteration 23/1000 | Loss: 0.00007650
Iteration 24/1000 | Loss: 0.00004631
Iteration 25/1000 | Loss: 0.00006826
Iteration 26/1000 | Loss: 0.00007735
Iteration 27/1000 | Loss: 0.00008153
Iteration 28/1000 | Loss: 0.00006534
Iteration 29/1000 | Loss: 0.00007997
Iteration 30/1000 | Loss: 0.00007326
Iteration 31/1000 | Loss: 0.00008642
Iteration 32/1000 | Loss: 0.00009860
Iteration 33/1000 | Loss: 0.00007125
Iteration 34/1000 | Loss: 0.00007161
Iteration 35/1000 | Loss: 0.00006309
Iteration 36/1000 | Loss: 0.00008449
Iteration 37/1000 | Loss: 0.00007908
Iteration 38/1000 | Loss: 0.00006567
Iteration 39/1000 | Loss: 0.00006071
Iteration 40/1000 | Loss: 0.00005341
Iteration 41/1000 | Loss: 0.00007449
Iteration 42/1000 | Loss: 0.00008660
Iteration 43/1000 | Loss: 0.00007218
Iteration 44/1000 | Loss: 0.00006084
Iteration 45/1000 | Loss: 0.00004548
Iteration 46/1000 | Loss: 0.00004735
Iteration 47/1000 | Loss: 0.00005613
Iteration 48/1000 | Loss: 0.00006777
Iteration 49/1000 | Loss: 0.00007082
Iteration 50/1000 | Loss: 0.00006812
Iteration 51/1000 | Loss: 0.00007004
Iteration 52/1000 | Loss: 0.00007339
Iteration 53/1000 | Loss: 0.00007699
Iteration 54/1000 | Loss: 0.00007327
Iteration 55/1000 | Loss: 0.00008879
Iteration 56/1000 | Loss: 0.00007139
Iteration 57/1000 | Loss: 0.00007482
Iteration 58/1000 | Loss: 0.00006813
Iteration 59/1000 | Loss: 0.00006327
Iteration 60/1000 | Loss: 0.00007299
Iteration 61/1000 | Loss: 0.00007567
Iteration 62/1000 | Loss: 0.00008308
Iteration 63/1000 | Loss: 0.00007250
Iteration 64/1000 | Loss: 0.00004583
Iteration 65/1000 | Loss: 0.00006162
Iteration 66/1000 | Loss: 0.00006029
Iteration 67/1000 | Loss: 0.00007655
Iteration 68/1000 | Loss: 0.00006248
Iteration 69/1000 | Loss: 0.00005093
Iteration 70/1000 | Loss: 0.00003812
Iteration 71/1000 | Loss: 0.00006294
Iteration 72/1000 | Loss: 0.00006617
Iteration 73/1000 | Loss: 0.00003855
Iteration 74/1000 | Loss: 0.00005721
Iteration 75/1000 | Loss: 0.00005311
Iteration 76/1000 | Loss: 0.00004899
Iteration 77/1000 | Loss: 0.00006269
Iteration 78/1000 | Loss: 0.00007689
Iteration 79/1000 | Loss: 0.00008152
Iteration 80/1000 | Loss: 0.00003862
Iteration 81/1000 | Loss: 0.00003792
Iteration 82/1000 | Loss: 0.00003178
Iteration 83/1000 | Loss: 0.00003318
Iteration 84/1000 | Loss: 0.00002628
Iteration 85/1000 | Loss: 0.00002952
Iteration 86/1000 | Loss: 0.00002615
Iteration 87/1000 | Loss: 0.00003446
Iteration 88/1000 | Loss: 0.00003988
Iteration 89/1000 | Loss: 0.00003480
Iteration 90/1000 | Loss: 0.00003087
Iteration 91/1000 | Loss: 0.00004243
Iteration 92/1000 | Loss: 0.00002849
Iteration 93/1000 | Loss: 0.00002635
Iteration 94/1000 | Loss: 0.00003724
Iteration 95/1000 | Loss: 0.00003410
Iteration 96/1000 | Loss: 0.00005760
Iteration 97/1000 | Loss: 0.00002488
Iteration 98/1000 | Loss: 0.00002503
Iteration 99/1000 | Loss: 0.00001869
Iteration 100/1000 | Loss: 0.00003283
Iteration 101/1000 | Loss: 0.00002789
Iteration 102/1000 | Loss: 0.00001905
Iteration 103/1000 | Loss: 0.00003012
Iteration 104/1000 | Loss: 0.00003079
Iteration 105/1000 | Loss: 0.00002486
Iteration 106/1000 | Loss: 0.00002296
Iteration 107/1000 | Loss: 0.00003016
Iteration 108/1000 | Loss: 0.00002241
Iteration 109/1000 | Loss: 0.00002874
Iteration 110/1000 | Loss: 0.00002331
Iteration 111/1000 | Loss: 0.00002775
Iteration 112/1000 | Loss: 0.00002517
Iteration 113/1000 | Loss: 0.00002637
Iteration 114/1000 | Loss: 0.00003004
Iteration 115/1000 | Loss: 0.00002671
Iteration 116/1000 | Loss: 0.00002667
Iteration 117/1000 | Loss: 0.00002484
Iteration 118/1000 | Loss: 0.00002675
Iteration 119/1000 | Loss: 0.00002472
Iteration 120/1000 | Loss: 0.00002472
Iteration 121/1000 | Loss: 0.00003230
Iteration 122/1000 | Loss: 0.00003200
Iteration 123/1000 | Loss: 0.00002988
Iteration 124/1000 | Loss: 0.00002713
Iteration 125/1000 | Loss: 0.00002958
Iteration 126/1000 | Loss: 0.00002571
Iteration 127/1000 | Loss: 0.00002212
Iteration 128/1000 | Loss: 0.00001944
Iteration 129/1000 | Loss: 0.00001813
Iteration 130/1000 | Loss: 0.00001751
Iteration 131/1000 | Loss: 0.00001724
Iteration 132/1000 | Loss: 0.00001690
Iteration 133/1000 | Loss: 0.00001667
Iteration 134/1000 | Loss: 0.00001659
Iteration 135/1000 | Loss: 0.00001658
Iteration 136/1000 | Loss: 0.00001655
Iteration 137/1000 | Loss: 0.00001654
Iteration 138/1000 | Loss: 0.00001654
Iteration 139/1000 | Loss: 0.00001654
Iteration 140/1000 | Loss: 0.00001654
Iteration 141/1000 | Loss: 0.00001654
Iteration 142/1000 | Loss: 0.00001654
Iteration 143/1000 | Loss: 0.00001653
Iteration 144/1000 | Loss: 0.00001653
Iteration 145/1000 | Loss: 0.00001653
Iteration 146/1000 | Loss: 0.00001653
Iteration 147/1000 | Loss: 0.00001652
Iteration 148/1000 | Loss: 0.00001652
Iteration 149/1000 | Loss: 0.00001650
Iteration 150/1000 | Loss: 0.00001650
Iteration 151/1000 | Loss: 0.00001650
Iteration 152/1000 | Loss: 0.00001649
Iteration 153/1000 | Loss: 0.00001649
Iteration 154/1000 | Loss: 0.00001648
Iteration 155/1000 | Loss: 0.00001648
Iteration 156/1000 | Loss: 0.00001648
Iteration 157/1000 | Loss: 0.00001647
Iteration 158/1000 | Loss: 0.00001647
Iteration 159/1000 | Loss: 0.00001647
Iteration 160/1000 | Loss: 0.00001646
Iteration 161/1000 | Loss: 0.00001646
Iteration 162/1000 | Loss: 0.00001646
Iteration 163/1000 | Loss: 0.00001646
Iteration 164/1000 | Loss: 0.00001645
Iteration 165/1000 | Loss: 0.00001645
Iteration 166/1000 | Loss: 0.00001645
Iteration 167/1000 | Loss: 0.00001645
Iteration 168/1000 | Loss: 0.00001645
Iteration 169/1000 | Loss: 0.00001645
Iteration 170/1000 | Loss: 0.00001644
Iteration 171/1000 | Loss: 0.00001644
Iteration 172/1000 | Loss: 0.00001644
Iteration 173/1000 | Loss: 0.00001644
Iteration 174/1000 | Loss: 0.00001644
Iteration 175/1000 | Loss: 0.00001644
Iteration 176/1000 | Loss: 0.00001644
Iteration 177/1000 | Loss: 0.00001644
Iteration 178/1000 | Loss: 0.00001643
Iteration 179/1000 | Loss: 0.00001643
Iteration 180/1000 | Loss: 0.00001643
Iteration 181/1000 | Loss: 0.00001643
Iteration 182/1000 | Loss: 0.00001643
Iteration 183/1000 | Loss: 0.00001643
Iteration 184/1000 | Loss: 0.00001643
Iteration 185/1000 | Loss: 0.00001643
Iteration 186/1000 | Loss: 0.00001643
Iteration 187/1000 | Loss: 0.00001642
Iteration 188/1000 | Loss: 0.00001642
Iteration 189/1000 | Loss: 0.00001642
Iteration 190/1000 | Loss: 0.00001642
Iteration 191/1000 | Loss: 0.00001642
Iteration 192/1000 | Loss: 0.00001642
Iteration 193/1000 | Loss: 0.00001642
Iteration 194/1000 | Loss: 0.00001642
Iteration 195/1000 | Loss: 0.00001642
Iteration 196/1000 | Loss: 0.00001641
Iteration 197/1000 | Loss: 0.00001641
Iteration 198/1000 | Loss: 0.00001641
Iteration 199/1000 | Loss: 0.00001641
Iteration 200/1000 | Loss: 0.00001641
Iteration 201/1000 | Loss: 0.00001641
Iteration 202/1000 | Loss: 0.00001641
Iteration 203/1000 | Loss: 0.00001641
Iteration 204/1000 | Loss: 0.00001641
Iteration 205/1000 | Loss: 0.00001641
Iteration 206/1000 | Loss: 0.00001641
Iteration 207/1000 | Loss: 0.00001641
Iteration 208/1000 | Loss: 0.00001641
Iteration 209/1000 | Loss: 0.00001640
Iteration 210/1000 | Loss: 0.00001640
Iteration 211/1000 | Loss: 0.00001640
Iteration 212/1000 | Loss: 0.00001640
Iteration 213/1000 | Loss: 0.00001640
Iteration 214/1000 | Loss: 0.00001640
Iteration 215/1000 | Loss: 0.00001640
Iteration 216/1000 | Loss: 0.00001640
Iteration 217/1000 | Loss: 0.00001640
Iteration 218/1000 | Loss: 0.00001640
Iteration 219/1000 | Loss: 0.00001640
Iteration 220/1000 | Loss: 0.00001640
Iteration 221/1000 | Loss: 0.00001640
Iteration 222/1000 | Loss: 0.00001639
Iteration 223/1000 | Loss: 0.00001639
Iteration 224/1000 | Loss: 0.00001639
Iteration 225/1000 | Loss: 0.00001639
Iteration 226/1000 | Loss: 0.00001639
Iteration 227/1000 | Loss: 0.00001639
Iteration 228/1000 | Loss: 0.00001639
Iteration 229/1000 | Loss: 0.00001638
Iteration 230/1000 | Loss: 0.00001638
Iteration 231/1000 | Loss: 0.00001638
Iteration 232/1000 | Loss: 0.00001638
Iteration 233/1000 | Loss: 0.00001638
Iteration 234/1000 | Loss: 0.00001638
Iteration 235/1000 | Loss: 0.00001638
Iteration 236/1000 | Loss: 0.00001638
Iteration 237/1000 | Loss: 0.00001638
Iteration 238/1000 | Loss: 0.00001638
Iteration 239/1000 | Loss: 0.00001638
Iteration 240/1000 | Loss: 0.00001638
Iteration 241/1000 | Loss: 0.00001638
Iteration 242/1000 | Loss: 0.00001638
Iteration 243/1000 | Loss: 0.00001638
Iteration 244/1000 | Loss: 0.00001638
Iteration 245/1000 | Loss: 0.00001638
Iteration 246/1000 | Loss: 0.00001638
Iteration 247/1000 | Loss: 0.00001638
Iteration 248/1000 | Loss: 0.00001638
Iteration 249/1000 | Loss: 0.00001638
Iteration 250/1000 | Loss: 0.00001638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [1.6380619854317047e-05, 1.6380619854317047e-05, 1.6380619854317047e-05, 1.6380619854317047e-05, 1.6380619854317047e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6380619854317047e-05

Optimization complete. Final v2v error: 3.3920507431030273 mm

Highest mean error: 3.7820167541503906 mm for frame 226

Lowest mean error: 3.2302420139312744 mm for frame 233

Saving results

Total time: 275.9571862220764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00737426
Iteration 2/25 | Loss: 0.00097319
Iteration 3/25 | Loss: 0.00075466
Iteration 4/25 | Loss: 0.00071406
Iteration 5/25 | Loss: 0.00069937
Iteration 6/25 | Loss: 0.00069542
Iteration 7/25 | Loss: 0.00069411
Iteration 8/25 | Loss: 0.00069362
Iteration 9/25 | Loss: 0.00069362
Iteration 10/25 | Loss: 0.00069362
Iteration 11/25 | Loss: 0.00069362
Iteration 12/25 | Loss: 0.00069362
Iteration 13/25 | Loss: 0.00069362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006936211721040308, 0.0006936211721040308, 0.0006936211721040308, 0.0006936211721040308, 0.0006936211721040308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006936211721040308

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13425517
Iteration 2/25 | Loss: 0.00021714
Iteration 3/25 | Loss: 0.00021714
Iteration 4/25 | Loss: 0.00021714
Iteration 5/25 | Loss: 0.00021713
Iteration 6/25 | Loss: 0.00021713
Iteration 7/25 | Loss: 0.00021713
Iteration 8/25 | Loss: 0.00021713
Iteration 9/25 | Loss: 0.00021713
Iteration 10/25 | Loss: 0.00021713
Iteration 11/25 | Loss: 0.00021713
Iteration 12/25 | Loss: 0.00021713
Iteration 13/25 | Loss: 0.00021713
Iteration 14/25 | Loss: 0.00021713
Iteration 15/25 | Loss: 0.00021713
Iteration 16/25 | Loss: 0.00021713
Iteration 17/25 | Loss: 0.00021713
Iteration 18/25 | Loss: 0.00021713
Iteration 19/25 | Loss: 0.00021713
Iteration 20/25 | Loss: 0.00021713
Iteration 21/25 | Loss: 0.00021713
Iteration 22/25 | Loss: 0.00021713
Iteration 23/25 | Loss: 0.00021713
Iteration 24/25 | Loss: 0.00021713
Iteration 25/25 | Loss: 0.00021713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021713
Iteration 2/1000 | Loss: 0.00002941
Iteration 3/1000 | Loss: 0.00002499
Iteration 4/1000 | Loss: 0.00002313
Iteration 5/1000 | Loss: 0.00002229
Iteration 6/1000 | Loss: 0.00002178
Iteration 7/1000 | Loss: 0.00002139
Iteration 8/1000 | Loss: 0.00002100
Iteration 9/1000 | Loss: 0.00002070
Iteration 10/1000 | Loss: 0.00002061
Iteration 11/1000 | Loss: 0.00002040
Iteration 12/1000 | Loss: 0.00002021
Iteration 13/1000 | Loss: 0.00002011
Iteration 14/1000 | Loss: 0.00002008
Iteration 15/1000 | Loss: 0.00002000
Iteration 16/1000 | Loss: 0.00001995
Iteration 17/1000 | Loss: 0.00001995
Iteration 18/1000 | Loss: 0.00001994
Iteration 19/1000 | Loss: 0.00001993
Iteration 20/1000 | Loss: 0.00001992
Iteration 21/1000 | Loss: 0.00001992
Iteration 22/1000 | Loss: 0.00001991
Iteration 23/1000 | Loss: 0.00001991
Iteration 24/1000 | Loss: 0.00001990
Iteration 25/1000 | Loss: 0.00001990
Iteration 26/1000 | Loss: 0.00001989
Iteration 27/1000 | Loss: 0.00001989
Iteration 28/1000 | Loss: 0.00001988
Iteration 29/1000 | Loss: 0.00001988
Iteration 30/1000 | Loss: 0.00001988
Iteration 31/1000 | Loss: 0.00001988
Iteration 32/1000 | Loss: 0.00001988
Iteration 33/1000 | Loss: 0.00001987
Iteration 34/1000 | Loss: 0.00001987
Iteration 35/1000 | Loss: 0.00001987
Iteration 36/1000 | Loss: 0.00001987
Iteration 37/1000 | Loss: 0.00001986
Iteration 38/1000 | Loss: 0.00001986
Iteration 39/1000 | Loss: 0.00001986
Iteration 40/1000 | Loss: 0.00001985
Iteration 41/1000 | Loss: 0.00001985
Iteration 42/1000 | Loss: 0.00001985
Iteration 43/1000 | Loss: 0.00001984
Iteration 44/1000 | Loss: 0.00001984
Iteration 45/1000 | Loss: 0.00001984
Iteration 46/1000 | Loss: 0.00001983
Iteration 47/1000 | Loss: 0.00001983
Iteration 48/1000 | Loss: 0.00001983
Iteration 49/1000 | Loss: 0.00001981
Iteration 50/1000 | Loss: 0.00001981
Iteration 51/1000 | Loss: 0.00001980
Iteration 52/1000 | Loss: 0.00001980
Iteration 53/1000 | Loss: 0.00001980
Iteration 54/1000 | Loss: 0.00001979
Iteration 55/1000 | Loss: 0.00001979
Iteration 56/1000 | Loss: 0.00001978
Iteration 57/1000 | Loss: 0.00001978
Iteration 58/1000 | Loss: 0.00001977
Iteration 59/1000 | Loss: 0.00001977
Iteration 60/1000 | Loss: 0.00001977
Iteration 61/1000 | Loss: 0.00001976
Iteration 62/1000 | Loss: 0.00001976
Iteration 63/1000 | Loss: 0.00001976
Iteration 64/1000 | Loss: 0.00001976
Iteration 65/1000 | Loss: 0.00001976
Iteration 66/1000 | Loss: 0.00001975
Iteration 67/1000 | Loss: 0.00001975
Iteration 68/1000 | Loss: 0.00001975
Iteration 69/1000 | Loss: 0.00001975
Iteration 70/1000 | Loss: 0.00001975
Iteration 71/1000 | Loss: 0.00001975
Iteration 72/1000 | Loss: 0.00001975
Iteration 73/1000 | Loss: 0.00001975
Iteration 74/1000 | Loss: 0.00001975
Iteration 75/1000 | Loss: 0.00001975
Iteration 76/1000 | Loss: 0.00001975
Iteration 77/1000 | Loss: 0.00001974
Iteration 78/1000 | Loss: 0.00001974
Iteration 79/1000 | Loss: 0.00001974
Iteration 80/1000 | Loss: 0.00001973
Iteration 81/1000 | Loss: 0.00001973
Iteration 82/1000 | Loss: 0.00001973
Iteration 83/1000 | Loss: 0.00001973
Iteration 84/1000 | Loss: 0.00001973
Iteration 85/1000 | Loss: 0.00001973
Iteration 86/1000 | Loss: 0.00001973
Iteration 87/1000 | Loss: 0.00001972
Iteration 88/1000 | Loss: 0.00001972
Iteration 89/1000 | Loss: 0.00001972
Iteration 90/1000 | Loss: 0.00001972
Iteration 91/1000 | Loss: 0.00001972
Iteration 92/1000 | Loss: 0.00001972
Iteration 93/1000 | Loss: 0.00001972
Iteration 94/1000 | Loss: 0.00001972
Iteration 95/1000 | Loss: 0.00001972
Iteration 96/1000 | Loss: 0.00001972
Iteration 97/1000 | Loss: 0.00001971
Iteration 98/1000 | Loss: 0.00001971
Iteration 99/1000 | Loss: 0.00001971
Iteration 100/1000 | Loss: 0.00001971
Iteration 101/1000 | Loss: 0.00001971
Iteration 102/1000 | Loss: 0.00001970
Iteration 103/1000 | Loss: 0.00001970
Iteration 104/1000 | Loss: 0.00001970
Iteration 105/1000 | Loss: 0.00001970
Iteration 106/1000 | Loss: 0.00001970
Iteration 107/1000 | Loss: 0.00001970
Iteration 108/1000 | Loss: 0.00001970
Iteration 109/1000 | Loss: 0.00001970
Iteration 110/1000 | Loss: 0.00001970
Iteration 111/1000 | Loss: 0.00001970
Iteration 112/1000 | Loss: 0.00001970
Iteration 113/1000 | Loss: 0.00001970
Iteration 114/1000 | Loss: 0.00001970
Iteration 115/1000 | Loss: 0.00001970
Iteration 116/1000 | Loss: 0.00001970
Iteration 117/1000 | Loss: 0.00001970
Iteration 118/1000 | Loss: 0.00001970
Iteration 119/1000 | Loss: 0.00001970
Iteration 120/1000 | Loss: 0.00001970
Iteration 121/1000 | Loss: 0.00001970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.9698911273735575e-05, 1.9698911273735575e-05, 1.9698911273735575e-05, 1.9698911273735575e-05, 1.9698911273735575e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9698911273735575e-05

Optimization complete. Final v2v error: 3.729966163635254 mm

Highest mean error: 5.21423864364624 mm for frame 99

Lowest mean error: 2.7794923782348633 mm for frame 144

Saving results

Total time: 37.35618042945862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390471
Iteration 2/25 | Loss: 0.00080967
Iteration 3/25 | Loss: 0.00063591
Iteration 4/25 | Loss: 0.00061371
Iteration 5/25 | Loss: 0.00060084
Iteration 6/25 | Loss: 0.00059686
Iteration 7/25 | Loss: 0.00059573
Iteration 8/25 | Loss: 0.00059562
Iteration 9/25 | Loss: 0.00059562
Iteration 10/25 | Loss: 0.00059562
Iteration 11/25 | Loss: 0.00059562
Iteration 12/25 | Loss: 0.00059562
Iteration 13/25 | Loss: 0.00059562
Iteration 14/25 | Loss: 0.00059562
Iteration 15/25 | Loss: 0.00059562
Iteration 16/25 | Loss: 0.00059562
Iteration 17/25 | Loss: 0.00059562
Iteration 18/25 | Loss: 0.00059562
Iteration 19/25 | Loss: 0.00059562
Iteration 20/25 | Loss: 0.00059562
Iteration 21/25 | Loss: 0.00059562
Iteration 22/25 | Loss: 0.00059562
Iteration 23/25 | Loss: 0.00059562
Iteration 24/25 | Loss: 0.00059562
Iteration 25/25 | Loss: 0.00059562

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66957867
Iteration 2/25 | Loss: 0.00012766
Iteration 3/25 | Loss: 0.00012766
Iteration 4/25 | Loss: 0.00012766
Iteration 5/25 | Loss: 0.00012766
Iteration 6/25 | Loss: 0.00012766
Iteration 7/25 | Loss: 0.00012766
Iteration 8/25 | Loss: 0.00012766
Iteration 9/25 | Loss: 0.00012766
Iteration 10/25 | Loss: 0.00012766
Iteration 11/25 | Loss: 0.00012766
Iteration 12/25 | Loss: 0.00012766
Iteration 13/25 | Loss: 0.00012766
Iteration 14/25 | Loss: 0.00012766
Iteration 15/25 | Loss: 0.00012766
Iteration 16/25 | Loss: 0.00012766
Iteration 17/25 | Loss: 0.00012766
Iteration 18/25 | Loss: 0.00012766
Iteration 19/25 | Loss: 0.00012766
Iteration 20/25 | Loss: 0.00012766
Iteration 21/25 | Loss: 0.00012766
Iteration 22/25 | Loss: 0.00012766
Iteration 23/25 | Loss: 0.00012766
Iteration 24/25 | Loss: 0.00012766
Iteration 25/25 | Loss: 0.00012766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00012766
Iteration 2/1000 | Loss: 0.00001856
Iteration 3/1000 | Loss: 0.00001361
Iteration 4/1000 | Loss: 0.00001162
Iteration 5/1000 | Loss: 0.00001104
Iteration 6/1000 | Loss: 0.00001070
Iteration 7/1000 | Loss: 0.00001047
Iteration 8/1000 | Loss: 0.00001031
Iteration 9/1000 | Loss: 0.00001026
Iteration 10/1000 | Loss: 0.00001024
Iteration 11/1000 | Loss: 0.00001018
Iteration 12/1000 | Loss: 0.00001016
Iteration 13/1000 | Loss: 0.00001016
Iteration 14/1000 | Loss: 0.00001016
Iteration 15/1000 | Loss: 0.00001016
Iteration 16/1000 | Loss: 0.00001016
Iteration 17/1000 | Loss: 0.00001016
Iteration 18/1000 | Loss: 0.00001016
Iteration 19/1000 | Loss: 0.00001016
Iteration 20/1000 | Loss: 0.00001015
Iteration 21/1000 | Loss: 0.00001011
Iteration 22/1000 | Loss: 0.00001010
Iteration 23/1000 | Loss: 0.00001007
Iteration 24/1000 | Loss: 0.00001003
Iteration 25/1000 | Loss: 0.00001003
Iteration 26/1000 | Loss: 0.00001003
Iteration 27/1000 | Loss: 0.00001001
Iteration 28/1000 | Loss: 0.00000999
Iteration 29/1000 | Loss: 0.00000999
Iteration 30/1000 | Loss: 0.00000998
Iteration 31/1000 | Loss: 0.00000998
Iteration 32/1000 | Loss: 0.00000998
Iteration 33/1000 | Loss: 0.00000998
Iteration 34/1000 | Loss: 0.00000997
Iteration 35/1000 | Loss: 0.00000996
Iteration 36/1000 | Loss: 0.00000996
Iteration 37/1000 | Loss: 0.00000996
Iteration 38/1000 | Loss: 0.00000996
Iteration 39/1000 | Loss: 0.00000996
Iteration 40/1000 | Loss: 0.00000995
Iteration 41/1000 | Loss: 0.00000995
Iteration 42/1000 | Loss: 0.00000995
Iteration 43/1000 | Loss: 0.00000995
Iteration 44/1000 | Loss: 0.00000995
Iteration 45/1000 | Loss: 0.00000994
Iteration 46/1000 | Loss: 0.00000994
Iteration 47/1000 | Loss: 0.00000994
Iteration 48/1000 | Loss: 0.00000994
Iteration 49/1000 | Loss: 0.00000993
Iteration 50/1000 | Loss: 0.00000993
Iteration 51/1000 | Loss: 0.00000993
Iteration 52/1000 | Loss: 0.00000992
Iteration 53/1000 | Loss: 0.00000992
Iteration 54/1000 | Loss: 0.00000992
Iteration 55/1000 | Loss: 0.00000992
Iteration 56/1000 | Loss: 0.00000992
Iteration 57/1000 | Loss: 0.00000992
Iteration 58/1000 | Loss: 0.00000991
Iteration 59/1000 | Loss: 0.00000991
Iteration 60/1000 | Loss: 0.00000990
Iteration 61/1000 | Loss: 0.00000990
Iteration 62/1000 | Loss: 0.00000990
Iteration 63/1000 | Loss: 0.00000989
Iteration 64/1000 | Loss: 0.00000989
Iteration 65/1000 | Loss: 0.00000989
Iteration 66/1000 | Loss: 0.00000988
Iteration 67/1000 | Loss: 0.00000988
Iteration 68/1000 | Loss: 0.00000988
Iteration 69/1000 | Loss: 0.00000988
Iteration 70/1000 | Loss: 0.00000987
Iteration 71/1000 | Loss: 0.00000987
Iteration 72/1000 | Loss: 0.00000987
Iteration 73/1000 | Loss: 0.00000987
Iteration 74/1000 | Loss: 0.00000987
Iteration 75/1000 | Loss: 0.00000986
Iteration 76/1000 | Loss: 0.00000986
Iteration 77/1000 | Loss: 0.00000985
Iteration 78/1000 | Loss: 0.00000985
Iteration 79/1000 | Loss: 0.00000985
Iteration 80/1000 | Loss: 0.00000984
Iteration 81/1000 | Loss: 0.00000984
Iteration 82/1000 | Loss: 0.00000983
Iteration 83/1000 | Loss: 0.00000983
Iteration 84/1000 | Loss: 0.00000983
Iteration 85/1000 | Loss: 0.00000983
Iteration 86/1000 | Loss: 0.00000983
Iteration 87/1000 | Loss: 0.00000983
Iteration 88/1000 | Loss: 0.00000982
Iteration 89/1000 | Loss: 0.00000982
Iteration 90/1000 | Loss: 0.00000981
Iteration 91/1000 | Loss: 0.00000981
Iteration 92/1000 | Loss: 0.00000981
Iteration 93/1000 | Loss: 0.00000981
Iteration 94/1000 | Loss: 0.00000981
Iteration 95/1000 | Loss: 0.00000981
Iteration 96/1000 | Loss: 0.00000981
Iteration 97/1000 | Loss: 0.00000980
Iteration 98/1000 | Loss: 0.00000980
Iteration 99/1000 | Loss: 0.00000980
Iteration 100/1000 | Loss: 0.00000980
Iteration 101/1000 | Loss: 0.00000979
Iteration 102/1000 | Loss: 0.00000979
Iteration 103/1000 | Loss: 0.00000979
Iteration 104/1000 | Loss: 0.00000979
Iteration 105/1000 | Loss: 0.00000979
Iteration 106/1000 | Loss: 0.00000979
Iteration 107/1000 | Loss: 0.00000979
Iteration 108/1000 | Loss: 0.00000979
Iteration 109/1000 | Loss: 0.00000979
Iteration 110/1000 | Loss: 0.00000979
Iteration 111/1000 | Loss: 0.00000978
Iteration 112/1000 | Loss: 0.00000978
Iteration 113/1000 | Loss: 0.00000978
Iteration 114/1000 | Loss: 0.00000978
Iteration 115/1000 | Loss: 0.00000978
Iteration 116/1000 | Loss: 0.00000978
Iteration 117/1000 | Loss: 0.00000978
Iteration 118/1000 | Loss: 0.00000978
Iteration 119/1000 | Loss: 0.00000978
Iteration 120/1000 | Loss: 0.00000977
Iteration 121/1000 | Loss: 0.00000977
Iteration 122/1000 | Loss: 0.00000977
Iteration 123/1000 | Loss: 0.00000977
Iteration 124/1000 | Loss: 0.00000977
Iteration 125/1000 | Loss: 0.00000977
Iteration 126/1000 | Loss: 0.00000977
Iteration 127/1000 | Loss: 0.00000977
Iteration 128/1000 | Loss: 0.00000977
Iteration 129/1000 | Loss: 0.00000977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [9.773368219612166e-06, 9.773368219612166e-06, 9.773368219612166e-06, 9.773368219612166e-06, 9.773368219612166e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.773368219612166e-06

Optimization complete. Final v2v error: 2.626066207885742 mm

Highest mean error: 2.955650568008423 mm for frame 42

Lowest mean error: 2.3576855659484863 mm for frame 81

Saving results

Total time: 31.80457329750061
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976651
Iteration 2/25 | Loss: 0.00123208
Iteration 3/25 | Loss: 0.00088862
Iteration 4/25 | Loss: 0.00082604
Iteration 5/25 | Loss: 0.00080589
Iteration 6/25 | Loss: 0.00079002
Iteration 7/25 | Loss: 0.00079306
Iteration 8/25 | Loss: 0.00078400
Iteration 9/25 | Loss: 0.00078529
Iteration 10/25 | Loss: 0.00078171
Iteration 11/25 | Loss: 0.00077988
Iteration 12/25 | Loss: 0.00077917
Iteration 13/25 | Loss: 0.00077877
Iteration 14/25 | Loss: 0.00077863
Iteration 15/25 | Loss: 0.00077851
Iteration 16/25 | Loss: 0.00077841
Iteration 17/25 | Loss: 0.00077840
Iteration 18/25 | Loss: 0.00077840
Iteration 19/25 | Loss: 0.00077839
Iteration 20/25 | Loss: 0.00077839
Iteration 21/25 | Loss: 0.00077839
Iteration 22/25 | Loss: 0.00077839
Iteration 23/25 | Loss: 0.00077839
Iteration 24/25 | Loss: 0.00077839
Iteration 25/25 | Loss: 0.00077839

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10287237
Iteration 2/25 | Loss: 0.00012735
Iteration 3/25 | Loss: 0.00012731
Iteration 4/25 | Loss: 0.00012731
Iteration 5/25 | Loss: 0.00012731
Iteration 6/25 | Loss: 0.00012730
Iteration 7/25 | Loss: 0.00012730
Iteration 8/25 | Loss: 0.00012730
Iteration 9/25 | Loss: 0.00012730
Iteration 10/25 | Loss: 0.00012730
Iteration 11/25 | Loss: 0.00012730
Iteration 12/25 | Loss: 0.00012730
Iteration 13/25 | Loss: 0.00012730
Iteration 14/25 | Loss: 0.00012730
Iteration 15/25 | Loss: 0.00012730
Iteration 16/25 | Loss: 0.00012730
Iteration 17/25 | Loss: 0.00012730
Iteration 18/25 | Loss: 0.00012730
Iteration 19/25 | Loss: 0.00012730
Iteration 20/25 | Loss: 0.00012730
Iteration 21/25 | Loss: 0.00012730
Iteration 22/25 | Loss: 0.00012730
Iteration 23/25 | Loss: 0.00012730
Iteration 24/25 | Loss: 0.00012730
Iteration 25/25 | Loss: 0.00012730

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00012730
Iteration 2/1000 | Loss: 0.00004053
Iteration 3/1000 | Loss: 0.00003517
Iteration 4/1000 | Loss: 0.00003372
Iteration 5/1000 | Loss: 0.00003248
Iteration 6/1000 | Loss: 0.00003177
Iteration 7/1000 | Loss: 0.00003126
Iteration 8/1000 | Loss: 0.00003089
Iteration 9/1000 | Loss: 0.00003069
Iteration 10/1000 | Loss: 0.00003068
Iteration 11/1000 | Loss: 0.00003068
Iteration 12/1000 | Loss: 0.00003052
Iteration 13/1000 | Loss: 0.00003051
Iteration 14/1000 | Loss: 0.00003047
Iteration 15/1000 | Loss: 0.00003047
Iteration 16/1000 | Loss: 0.00003043
Iteration 17/1000 | Loss: 0.00003034
Iteration 18/1000 | Loss: 0.00003031
Iteration 19/1000 | Loss: 0.00003029
Iteration 20/1000 | Loss: 0.00003029
Iteration 21/1000 | Loss: 0.00003029
Iteration 22/1000 | Loss: 0.00003029
Iteration 23/1000 | Loss: 0.00003029
Iteration 24/1000 | Loss: 0.00003029
Iteration 25/1000 | Loss: 0.00003029
Iteration 26/1000 | Loss: 0.00003028
Iteration 27/1000 | Loss: 0.00003028
Iteration 28/1000 | Loss: 0.00003028
Iteration 29/1000 | Loss: 0.00003028
Iteration 30/1000 | Loss: 0.00003028
Iteration 31/1000 | Loss: 0.00003028
Iteration 32/1000 | Loss: 0.00003028
Iteration 33/1000 | Loss: 0.00003028
Iteration 34/1000 | Loss: 0.00003028
Iteration 35/1000 | Loss: 0.00003027
Iteration 36/1000 | Loss: 0.00003027
Iteration 37/1000 | Loss: 0.00003027
Iteration 38/1000 | Loss: 0.00003027
Iteration 39/1000 | Loss: 0.00003027
Iteration 40/1000 | Loss: 0.00003027
Iteration 41/1000 | Loss: 0.00003027
Iteration 42/1000 | Loss: 0.00003026
Iteration 43/1000 | Loss: 0.00003026
Iteration 44/1000 | Loss: 0.00003026
Iteration 45/1000 | Loss: 0.00003026
Iteration 46/1000 | Loss: 0.00003025
Iteration 47/1000 | Loss: 0.00003025
Iteration 48/1000 | Loss: 0.00003025
Iteration 49/1000 | Loss: 0.00003025
Iteration 50/1000 | Loss: 0.00003025
Iteration 51/1000 | Loss: 0.00003025
Iteration 52/1000 | Loss: 0.00003024
Iteration 53/1000 | Loss: 0.00003024
Iteration 54/1000 | Loss: 0.00003024
Iteration 55/1000 | Loss: 0.00003024
Iteration 56/1000 | Loss: 0.00003024
Iteration 57/1000 | Loss: 0.00003024
Iteration 58/1000 | Loss: 0.00003024
Iteration 59/1000 | Loss: 0.00003024
Iteration 60/1000 | Loss: 0.00003024
Iteration 61/1000 | Loss: 0.00003024
Iteration 62/1000 | Loss: 0.00003024
Iteration 63/1000 | Loss: 0.00003024
Iteration 64/1000 | Loss: 0.00003024
Iteration 65/1000 | Loss: 0.00003023
Iteration 66/1000 | Loss: 0.00003023
Iteration 67/1000 | Loss: 0.00003023
Iteration 68/1000 | Loss: 0.00003023
Iteration 69/1000 | Loss: 0.00003023
Iteration 70/1000 | Loss: 0.00003023
Iteration 71/1000 | Loss: 0.00003023
Iteration 72/1000 | Loss: 0.00003023
Iteration 73/1000 | Loss: 0.00003023
Iteration 74/1000 | Loss: 0.00003023
Iteration 75/1000 | Loss: 0.00003023
Iteration 76/1000 | Loss: 0.00003022
Iteration 77/1000 | Loss: 0.00003022
Iteration 78/1000 | Loss: 0.00003022
Iteration 79/1000 | Loss: 0.00003022
Iteration 80/1000 | Loss: 0.00003022
Iteration 81/1000 | Loss: 0.00003022
Iteration 82/1000 | Loss: 0.00003022
Iteration 83/1000 | Loss: 0.00003022
Iteration 84/1000 | Loss: 0.00003022
Iteration 85/1000 | Loss: 0.00003022
Iteration 86/1000 | Loss: 0.00003022
Iteration 87/1000 | Loss: 0.00003022
Iteration 88/1000 | Loss: 0.00003022
Iteration 89/1000 | Loss: 0.00003022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [3.022195778612513e-05, 3.022195778612513e-05, 3.022195778612513e-05, 3.022195778612513e-05, 3.022195778612513e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.022195778612513e-05

Optimization complete. Final v2v error: 4.410739421844482 mm

Highest mean error: 5.280696392059326 mm for frame 105

Lowest mean error: 3.7202565670013428 mm for frame 59

Saving results

Total time: 55.57339406013489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032510
Iteration 2/25 | Loss: 0.01032510
Iteration 3/25 | Loss: 0.01032510
Iteration 4/25 | Loss: 0.01032510
Iteration 5/25 | Loss: 0.01032510
Iteration 6/25 | Loss: 0.01032509
Iteration 7/25 | Loss: 0.00472607
Iteration 8/25 | Loss: 0.00315660
Iteration 9/25 | Loss: 0.00242046
Iteration 10/25 | Loss: 0.00210231
Iteration 11/25 | Loss: 0.00191278
Iteration 12/25 | Loss: 0.00182375
Iteration 13/25 | Loss: 0.00173297
Iteration 14/25 | Loss: 0.00165216
Iteration 15/25 | Loss: 0.00167312
Iteration 16/25 | Loss: 0.00163241
Iteration 17/25 | Loss: 0.00154780
Iteration 18/25 | Loss: 0.00154198
Iteration 19/25 | Loss: 0.00157220
Iteration 20/25 | Loss: 0.00150887
Iteration 21/25 | Loss: 0.00143372
Iteration 22/25 | Loss: 0.00140139
Iteration 23/25 | Loss: 0.00137915
Iteration 24/25 | Loss: 0.00137390
Iteration 25/25 | Loss: 0.00136153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37820053
Iteration 2/25 | Loss: 0.00846269
Iteration 3/25 | Loss: 0.00754467
Iteration 4/25 | Loss: 0.00754463
Iteration 5/25 | Loss: 0.00754463
Iteration 6/25 | Loss: 0.00754463
Iteration 7/25 | Loss: 0.00754463
Iteration 8/25 | Loss: 0.00754463
Iteration 9/25 | Loss: 0.00754463
Iteration 10/25 | Loss: 0.00754463
Iteration 11/25 | Loss: 0.00754463
Iteration 12/25 | Loss: 0.00754463
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.007544630207121372, 0.007544630207121372, 0.007544630207121372, 0.007544630207121372, 0.007544630207121372]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007544630207121372

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00754463
Iteration 2/1000 | Loss: 0.00463269
Iteration 3/1000 | Loss: 0.00548246
Iteration 4/1000 | Loss: 0.00506771
Iteration 5/1000 | Loss: 0.00277299
Iteration 6/1000 | Loss: 0.00255042
Iteration 7/1000 | Loss: 0.00563377
Iteration 8/1000 | Loss: 0.00310227
Iteration 9/1000 | Loss: 0.00067675
Iteration 10/1000 | Loss: 0.00126025
Iteration 11/1000 | Loss: 0.00111697
Iteration 12/1000 | Loss: 0.00125845
Iteration 13/1000 | Loss: 0.00097048
Iteration 14/1000 | Loss: 0.00064812
Iteration 15/1000 | Loss: 0.00038439
Iteration 16/1000 | Loss: 0.00065126
Iteration 17/1000 | Loss: 0.00039432
Iteration 18/1000 | Loss: 0.00041432
Iteration 19/1000 | Loss: 0.00041442
Iteration 20/1000 | Loss: 0.00187603
Iteration 21/1000 | Loss: 0.00045754
Iteration 22/1000 | Loss: 0.00034904
Iteration 23/1000 | Loss: 0.00030940
Iteration 24/1000 | Loss: 0.00068225
Iteration 25/1000 | Loss: 0.00047922
Iteration 26/1000 | Loss: 0.00054174
Iteration 27/1000 | Loss: 0.00176088
Iteration 28/1000 | Loss: 0.00194536
Iteration 29/1000 | Loss: 0.00140697
Iteration 30/1000 | Loss: 0.00090617
Iteration 31/1000 | Loss: 0.00087285
Iteration 32/1000 | Loss: 0.00127235
Iteration 33/1000 | Loss: 0.00129711
Iteration 34/1000 | Loss: 0.00091619
Iteration 35/1000 | Loss: 0.00089676
Iteration 36/1000 | Loss: 0.00050388
Iteration 37/1000 | Loss: 0.00085147
Iteration 38/1000 | Loss: 0.00066803
Iteration 39/1000 | Loss: 0.00049935
Iteration 40/1000 | Loss: 0.00177512
Iteration 41/1000 | Loss: 0.00039564
Iteration 42/1000 | Loss: 0.00030826
Iteration 43/1000 | Loss: 0.00027581
Iteration 44/1000 | Loss: 0.00027312
Iteration 45/1000 | Loss: 0.00031726
Iteration 46/1000 | Loss: 0.00050671
Iteration 47/1000 | Loss: 0.00045873
Iteration 48/1000 | Loss: 0.00025299
Iteration 49/1000 | Loss: 0.00027745
Iteration 50/1000 | Loss: 0.00051570
Iteration 51/1000 | Loss: 0.00074196
Iteration 52/1000 | Loss: 0.00086312
Iteration 53/1000 | Loss: 0.00068232
Iteration 54/1000 | Loss: 0.00028559
Iteration 55/1000 | Loss: 0.00028977
Iteration 56/1000 | Loss: 0.00023770
Iteration 57/1000 | Loss: 0.00033112
Iteration 58/1000 | Loss: 0.00023187
Iteration 59/1000 | Loss: 0.00075842
Iteration 60/1000 | Loss: 0.00028393
Iteration 61/1000 | Loss: 0.00023329
Iteration 62/1000 | Loss: 0.00052399
Iteration 63/1000 | Loss: 0.00035584
Iteration 64/1000 | Loss: 0.00048842
Iteration 65/1000 | Loss: 0.00032308
Iteration 66/1000 | Loss: 0.00111001
Iteration 67/1000 | Loss: 0.00039613
Iteration 68/1000 | Loss: 0.00025769
Iteration 69/1000 | Loss: 0.00022611
Iteration 70/1000 | Loss: 0.00030825
Iteration 71/1000 | Loss: 0.00027457
Iteration 72/1000 | Loss: 0.00054875
Iteration 73/1000 | Loss: 0.00023528
Iteration 74/1000 | Loss: 0.00028456
Iteration 75/1000 | Loss: 0.00039552
Iteration 76/1000 | Loss: 0.00024840
Iteration 77/1000 | Loss: 0.00024119
Iteration 78/1000 | Loss: 0.00027183
Iteration 79/1000 | Loss: 0.00028647
Iteration 80/1000 | Loss: 0.00033800
Iteration 81/1000 | Loss: 0.00021022
Iteration 82/1000 | Loss: 0.00026580
Iteration 83/1000 | Loss: 0.00025522
Iteration 84/1000 | Loss: 0.00020897
Iteration 85/1000 | Loss: 0.00044794
Iteration 86/1000 | Loss: 0.00076963
Iteration 87/1000 | Loss: 0.00170734
Iteration 88/1000 | Loss: 0.00060795
Iteration 89/1000 | Loss: 0.00028253
Iteration 90/1000 | Loss: 0.00022895
Iteration 91/1000 | Loss: 0.00066515
Iteration 92/1000 | Loss: 0.00021055
Iteration 93/1000 | Loss: 0.00020427
Iteration 94/1000 | Loss: 0.00047426
Iteration 95/1000 | Loss: 0.00020094
Iteration 96/1000 | Loss: 0.00019979
Iteration 97/1000 | Loss: 0.00028023
Iteration 98/1000 | Loss: 0.00019567
Iteration 99/1000 | Loss: 0.00025253
Iteration 100/1000 | Loss: 0.00019438
Iteration 101/1000 | Loss: 0.00032892
Iteration 102/1000 | Loss: 0.00020531
Iteration 103/1000 | Loss: 0.00020817
Iteration 104/1000 | Loss: 0.00020844
Iteration 105/1000 | Loss: 0.00029408
Iteration 106/1000 | Loss: 0.00027528
Iteration 107/1000 | Loss: 0.00032783
Iteration 108/1000 | Loss: 0.00031961
Iteration 109/1000 | Loss: 0.00019593
Iteration 110/1000 | Loss: 0.00020425
Iteration 111/1000 | Loss: 0.00019226
Iteration 112/1000 | Loss: 0.00019196
Iteration 113/1000 | Loss: 0.00030447
Iteration 114/1000 | Loss: 0.00041480
Iteration 115/1000 | Loss: 0.00022161
Iteration 116/1000 | Loss: 0.00019927
Iteration 117/1000 | Loss: 0.00019024
Iteration 118/1000 | Loss: 0.00039654
Iteration 119/1000 | Loss: 0.00069118
Iteration 120/1000 | Loss: 0.00075699
Iteration 121/1000 | Loss: 0.00048455
Iteration 122/1000 | Loss: 0.00028578
Iteration 123/1000 | Loss: 0.00019061
Iteration 124/1000 | Loss: 0.00020843
Iteration 125/1000 | Loss: 0.00019343
Iteration 126/1000 | Loss: 0.00024045
Iteration 127/1000 | Loss: 0.00026336
Iteration 128/1000 | Loss: 0.00017634
Iteration 129/1000 | Loss: 0.00017445
Iteration 130/1000 | Loss: 0.00017304
Iteration 131/1000 | Loss: 0.00017140
Iteration 132/1000 | Loss: 0.00031719
Iteration 133/1000 | Loss: 0.00016945
Iteration 134/1000 | Loss: 0.00025535
Iteration 135/1000 | Loss: 0.00035067
Iteration 136/1000 | Loss: 0.00193908
Iteration 137/1000 | Loss: 0.00327470
Iteration 138/1000 | Loss: 0.00253104
Iteration 139/1000 | Loss: 0.00213580
Iteration 140/1000 | Loss: 0.00060011
Iteration 141/1000 | Loss: 0.00055282
Iteration 142/1000 | Loss: 0.00018992
Iteration 143/1000 | Loss: 0.00023750
Iteration 144/1000 | Loss: 0.00037391
Iteration 145/1000 | Loss: 0.00024740
Iteration 146/1000 | Loss: 0.00023422
Iteration 147/1000 | Loss: 0.00014389
Iteration 148/1000 | Loss: 0.00030339
Iteration 149/1000 | Loss: 0.00017990
Iteration 150/1000 | Loss: 0.00013512
Iteration 151/1000 | Loss: 0.00012241
Iteration 152/1000 | Loss: 0.00016507
Iteration 153/1000 | Loss: 0.00012058
Iteration 154/1000 | Loss: 0.00021863
Iteration 155/1000 | Loss: 0.00010227
Iteration 156/1000 | Loss: 0.00009976
Iteration 157/1000 | Loss: 0.00015057
Iteration 158/1000 | Loss: 0.00047280
Iteration 159/1000 | Loss: 0.00107107
Iteration 160/1000 | Loss: 0.00114425
Iteration 161/1000 | Loss: 0.00117334
Iteration 162/1000 | Loss: 0.00166781
Iteration 163/1000 | Loss: 0.00285787
Iteration 164/1000 | Loss: 0.00075543
Iteration 165/1000 | Loss: 0.00073609
Iteration 166/1000 | Loss: 0.00035196
Iteration 167/1000 | Loss: 0.00028503
Iteration 168/1000 | Loss: 0.00013797
Iteration 169/1000 | Loss: 0.00012423
Iteration 170/1000 | Loss: 0.00017263
Iteration 171/1000 | Loss: 0.00023542
Iteration 172/1000 | Loss: 0.00009061
Iteration 173/1000 | Loss: 0.00006852
Iteration 174/1000 | Loss: 0.00006501
Iteration 175/1000 | Loss: 0.00006234
Iteration 176/1000 | Loss: 0.00017754
Iteration 177/1000 | Loss: 0.00042102
Iteration 178/1000 | Loss: 0.00020228
Iteration 179/1000 | Loss: 0.00024205
Iteration 180/1000 | Loss: 0.00006632
Iteration 181/1000 | Loss: 0.00013895
Iteration 182/1000 | Loss: 0.00005820
Iteration 183/1000 | Loss: 0.00005758
Iteration 184/1000 | Loss: 0.00011755
Iteration 185/1000 | Loss: 0.00052997
Iteration 186/1000 | Loss: 0.00013199
Iteration 187/1000 | Loss: 0.00019550
Iteration 188/1000 | Loss: 0.00007407
Iteration 189/1000 | Loss: 0.00014907
Iteration 190/1000 | Loss: 0.00013071
Iteration 191/1000 | Loss: 0.00032253
Iteration 192/1000 | Loss: 0.00006207
Iteration 193/1000 | Loss: 0.00007603
Iteration 194/1000 | Loss: 0.00024760
Iteration 195/1000 | Loss: 0.00009856
Iteration 196/1000 | Loss: 0.00006732
Iteration 197/1000 | Loss: 0.00005358
Iteration 198/1000 | Loss: 0.00007614
Iteration 199/1000 | Loss: 0.00019179
Iteration 200/1000 | Loss: 0.00030944
Iteration 201/1000 | Loss: 0.00061063
Iteration 202/1000 | Loss: 0.00131105
Iteration 203/1000 | Loss: 0.00055102
Iteration 204/1000 | Loss: 0.00172136
Iteration 205/1000 | Loss: 0.00052670
Iteration 206/1000 | Loss: 0.00006719
Iteration 207/1000 | Loss: 0.00009002
Iteration 208/1000 | Loss: 0.00005534
Iteration 209/1000 | Loss: 0.00005346
Iteration 210/1000 | Loss: 0.00009236
Iteration 211/1000 | Loss: 0.00005567
Iteration 212/1000 | Loss: 0.00006844
Iteration 213/1000 | Loss: 0.00005006
Iteration 214/1000 | Loss: 0.00006559
Iteration 215/1000 | Loss: 0.00005125
Iteration 216/1000 | Loss: 0.00005018
Iteration 217/1000 | Loss: 0.00004916
Iteration 218/1000 | Loss: 0.00004915
Iteration 219/1000 | Loss: 0.00004914
Iteration 220/1000 | Loss: 0.00004901
Iteration 221/1000 | Loss: 0.00004888
Iteration 222/1000 | Loss: 0.00004888
Iteration 223/1000 | Loss: 0.00004888
Iteration 224/1000 | Loss: 0.00004887
Iteration 225/1000 | Loss: 0.00004886
Iteration 226/1000 | Loss: 0.00004886
Iteration 227/1000 | Loss: 0.00008386
Iteration 228/1000 | Loss: 0.00004885
Iteration 229/1000 | Loss: 0.00004885
Iteration 230/1000 | Loss: 0.00004884
Iteration 231/1000 | Loss: 0.00004884
Iteration 232/1000 | Loss: 0.00004883
Iteration 233/1000 | Loss: 0.00004883
Iteration 234/1000 | Loss: 0.00004882
Iteration 235/1000 | Loss: 0.00004882
Iteration 236/1000 | Loss: 0.00004882
Iteration 237/1000 | Loss: 0.00004882
Iteration 238/1000 | Loss: 0.00004882
Iteration 239/1000 | Loss: 0.00004882
Iteration 240/1000 | Loss: 0.00004882
Iteration 241/1000 | Loss: 0.00004881
Iteration 242/1000 | Loss: 0.00004881
Iteration 243/1000 | Loss: 0.00004881
Iteration 244/1000 | Loss: 0.00004881
Iteration 245/1000 | Loss: 0.00004881
Iteration 246/1000 | Loss: 0.00004880
Iteration 247/1000 | Loss: 0.00004880
Iteration 248/1000 | Loss: 0.00004880
Iteration 249/1000 | Loss: 0.00004880
Iteration 250/1000 | Loss: 0.00004880
Iteration 251/1000 | Loss: 0.00004879
Iteration 252/1000 | Loss: 0.00004879
Iteration 253/1000 | Loss: 0.00004879
Iteration 254/1000 | Loss: 0.00004879
Iteration 255/1000 | Loss: 0.00004879
Iteration 256/1000 | Loss: 0.00004879
Iteration 257/1000 | Loss: 0.00004879
Iteration 258/1000 | Loss: 0.00004879
Iteration 259/1000 | Loss: 0.00004879
Iteration 260/1000 | Loss: 0.00004879
Iteration 261/1000 | Loss: 0.00004878
Iteration 262/1000 | Loss: 0.00004878
Iteration 263/1000 | Loss: 0.00004878
Iteration 264/1000 | Loss: 0.00004878
Iteration 265/1000 | Loss: 0.00004878
Iteration 266/1000 | Loss: 0.00004878
Iteration 267/1000 | Loss: 0.00004878
Iteration 268/1000 | Loss: 0.00004878
Iteration 269/1000 | Loss: 0.00004878
Iteration 270/1000 | Loss: 0.00004878
Iteration 271/1000 | Loss: 0.00004878
Iteration 272/1000 | Loss: 0.00004878
Iteration 273/1000 | Loss: 0.00004878
Iteration 274/1000 | Loss: 0.00004878
Iteration 275/1000 | Loss: 0.00004878
Iteration 276/1000 | Loss: 0.00004878
Iteration 277/1000 | Loss: 0.00004878
Iteration 278/1000 | Loss: 0.00004878
Iteration 279/1000 | Loss: 0.00004878
Iteration 280/1000 | Loss: 0.00004878
Iteration 281/1000 | Loss: 0.00004878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [4.878358231508173e-05, 4.878358231508173e-05, 4.878358231508173e-05, 4.878358231508173e-05, 4.878358231508173e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.878358231508173e-05

Optimization complete. Final v2v error: 4.174688816070557 mm

Highest mean error: 12.03705883026123 mm for frame 164

Lowest mean error: 2.9748048782348633 mm for frame 14

Saving results

Total time: 398.7427546977997
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840610
Iteration 2/25 | Loss: 0.00083131
Iteration 3/25 | Loss: 0.00070827
Iteration 4/25 | Loss: 0.00066129
Iteration 5/25 | Loss: 0.00064189
Iteration 6/25 | Loss: 0.00063846
Iteration 7/25 | Loss: 0.00063721
Iteration 8/25 | Loss: 0.00063690
Iteration 9/25 | Loss: 0.00063690
Iteration 10/25 | Loss: 0.00063690
Iteration 11/25 | Loss: 0.00063690
Iteration 12/25 | Loss: 0.00063690
Iteration 13/25 | Loss: 0.00063690
Iteration 14/25 | Loss: 0.00063690
Iteration 15/25 | Loss: 0.00063690
Iteration 16/25 | Loss: 0.00063690
Iteration 17/25 | Loss: 0.00063690
Iteration 18/25 | Loss: 0.00063690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006369009497575462, 0.0006369009497575462, 0.0006369009497575462, 0.0006369009497575462, 0.0006369009497575462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006369009497575462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.43698549
Iteration 2/25 | Loss: 0.00013420
Iteration 3/25 | Loss: 0.00013413
Iteration 4/25 | Loss: 0.00013413
Iteration 5/25 | Loss: 0.00013413
Iteration 6/25 | Loss: 0.00013413
Iteration 7/25 | Loss: 0.00013413
Iteration 8/25 | Loss: 0.00013413
Iteration 9/25 | Loss: 0.00013413
Iteration 10/25 | Loss: 0.00013413
Iteration 11/25 | Loss: 0.00013413
Iteration 12/25 | Loss: 0.00013413
Iteration 13/25 | Loss: 0.00013413
Iteration 14/25 | Loss: 0.00013413
Iteration 15/25 | Loss: 0.00013413
Iteration 16/25 | Loss: 0.00013413
Iteration 17/25 | Loss: 0.00013413
Iteration 18/25 | Loss: 0.00013413
Iteration 19/25 | Loss: 0.00013413
Iteration 20/25 | Loss: 0.00013413
Iteration 21/25 | Loss: 0.00013413
Iteration 22/25 | Loss: 0.00013413
Iteration 23/25 | Loss: 0.00013413
Iteration 24/25 | Loss: 0.00013413
Iteration 25/25 | Loss: 0.00013413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00013413
Iteration 2/1000 | Loss: 0.00002479
Iteration 3/1000 | Loss: 0.00001990
Iteration 4/1000 | Loss: 0.00001841
Iteration 5/1000 | Loss: 0.00001747
Iteration 6/1000 | Loss: 0.00001700
Iteration 7/1000 | Loss: 0.00001655
Iteration 8/1000 | Loss: 0.00001627
Iteration 9/1000 | Loss: 0.00001604
Iteration 10/1000 | Loss: 0.00001599
Iteration 11/1000 | Loss: 0.00001594
Iteration 12/1000 | Loss: 0.00001593
Iteration 13/1000 | Loss: 0.00001593
Iteration 14/1000 | Loss: 0.00001592
Iteration 15/1000 | Loss: 0.00001592
Iteration 16/1000 | Loss: 0.00001591
Iteration 17/1000 | Loss: 0.00001584
Iteration 18/1000 | Loss: 0.00001581
Iteration 19/1000 | Loss: 0.00001581
Iteration 20/1000 | Loss: 0.00001581
Iteration 21/1000 | Loss: 0.00001580
Iteration 22/1000 | Loss: 0.00001579
Iteration 23/1000 | Loss: 0.00001578
Iteration 24/1000 | Loss: 0.00001576
Iteration 25/1000 | Loss: 0.00001575
Iteration 26/1000 | Loss: 0.00001575
Iteration 27/1000 | Loss: 0.00001574
Iteration 28/1000 | Loss: 0.00001574
Iteration 29/1000 | Loss: 0.00001574
Iteration 30/1000 | Loss: 0.00001574
Iteration 31/1000 | Loss: 0.00001574
Iteration 32/1000 | Loss: 0.00001574
Iteration 33/1000 | Loss: 0.00001574
Iteration 34/1000 | Loss: 0.00001573
Iteration 35/1000 | Loss: 0.00001572
Iteration 36/1000 | Loss: 0.00001572
Iteration 37/1000 | Loss: 0.00001572
Iteration 38/1000 | Loss: 0.00001572
Iteration 39/1000 | Loss: 0.00001572
Iteration 40/1000 | Loss: 0.00001571
Iteration 41/1000 | Loss: 0.00001571
Iteration 42/1000 | Loss: 0.00001571
Iteration 43/1000 | Loss: 0.00001571
Iteration 44/1000 | Loss: 0.00001570
Iteration 45/1000 | Loss: 0.00001570
Iteration 46/1000 | Loss: 0.00001570
Iteration 47/1000 | Loss: 0.00001569
Iteration 48/1000 | Loss: 0.00001569
Iteration 49/1000 | Loss: 0.00001569
Iteration 50/1000 | Loss: 0.00001568
Iteration 51/1000 | Loss: 0.00001567
Iteration 52/1000 | Loss: 0.00001567
Iteration 53/1000 | Loss: 0.00001567
Iteration 54/1000 | Loss: 0.00001566
Iteration 55/1000 | Loss: 0.00001566
Iteration 56/1000 | Loss: 0.00001566
Iteration 57/1000 | Loss: 0.00001566
Iteration 58/1000 | Loss: 0.00001566
Iteration 59/1000 | Loss: 0.00001566
Iteration 60/1000 | Loss: 0.00001566
Iteration 61/1000 | Loss: 0.00001566
Iteration 62/1000 | Loss: 0.00001566
Iteration 63/1000 | Loss: 0.00001566
Iteration 64/1000 | Loss: 0.00001566
Iteration 65/1000 | Loss: 0.00001565
Iteration 66/1000 | Loss: 0.00001565
Iteration 67/1000 | Loss: 0.00001564
Iteration 68/1000 | Loss: 0.00001564
Iteration 69/1000 | Loss: 0.00001563
Iteration 70/1000 | Loss: 0.00001563
Iteration 71/1000 | Loss: 0.00001563
Iteration 72/1000 | Loss: 0.00001563
Iteration 73/1000 | Loss: 0.00001563
Iteration 74/1000 | Loss: 0.00001562
Iteration 75/1000 | Loss: 0.00001562
Iteration 76/1000 | Loss: 0.00001561
Iteration 77/1000 | Loss: 0.00001561
Iteration 78/1000 | Loss: 0.00001561
Iteration 79/1000 | Loss: 0.00001560
Iteration 80/1000 | Loss: 0.00001560
Iteration 81/1000 | Loss: 0.00001560
Iteration 82/1000 | Loss: 0.00001559
Iteration 83/1000 | Loss: 0.00001559
Iteration 84/1000 | Loss: 0.00001559
Iteration 85/1000 | Loss: 0.00001559
Iteration 86/1000 | Loss: 0.00001559
Iteration 87/1000 | Loss: 0.00001559
Iteration 88/1000 | Loss: 0.00001559
Iteration 89/1000 | Loss: 0.00001559
Iteration 90/1000 | Loss: 0.00001559
Iteration 91/1000 | Loss: 0.00001559
Iteration 92/1000 | Loss: 0.00001559
Iteration 93/1000 | Loss: 0.00001559
Iteration 94/1000 | Loss: 0.00001559
Iteration 95/1000 | Loss: 0.00001559
Iteration 96/1000 | Loss: 0.00001559
Iteration 97/1000 | Loss: 0.00001559
Iteration 98/1000 | Loss: 0.00001559
Iteration 99/1000 | Loss: 0.00001559
Iteration 100/1000 | Loss: 0.00001559
Iteration 101/1000 | Loss: 0.00001559
Iteration 102/1000 | Loss: 0.00001559
Iteration 103/1000 | Loss: 0.00001559
Iteration 104/1000 | Loss: 0.00001559
Iteration 105/1000 | Loss: 0.00001559
Iteration 106/1000 | Loss: 0.00001559
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001559
Iteration 110/1000 | Loss: 0.00001559
Iteration 111/1000 | Loss: 0.00001559
Iteration 112/1000 | Loss: 0.00001559
Iteration 113/1000 | Loss: 0.00001559
Iteration 114/1000 | Loss: 0.00001559
Iteration 115/1000 | Loss: 0.00001559
Iteration 116/1000 | Loss: 0.00001559
Iteration 117/1000 | Loss: 0.00001559
Iteration 118/1000 | Loss: 0.00001559
Iteration 119/1000 | Loss: 0.00001559
Iteration 120/1000 | Loss: 0.00001559
Iteration 121/1000 | Loss: 0.00001559
Iteration 122/1000 | Loss: 0.00001559
Iteration 123/1000 | Loss: 0.00001559
Iteration 124/1000 | Loss: 0.00001559
Iteration 125/1000 | Loss: 0.00001559
Iteration 126/1000 | Loss: 0.00001559
Iteration 127/1000 | Loss: 0.00001559
Iteration 128/1000 | Loss: 0.00001559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.5594096112181433e-05, 1.5594096112181433e-05, 1.5594096112181433e-05, 1.5594096112181433e-05, 1.5594096112181433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5594096112181433e-05

Optimization complete. Final v2v error: 3.3416643142700195 mm

Highest mean error: 3.911501169204712 mm for frame 56

Lowest mean error: 2.900853157043457 mm for frame 229

Saving results

Total time: 38.23850178718567
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040757
Iteration 2/25 | Loss: 0.00191994
Iteration 3/25 | Loss: 0.00120554
Iteration 4/25 | Loss: 0.00103145
Iteration 5/25 | Loss: 0.00085167
Iteration 6/25 | Loss: 0.00079830
Iteration 7/25 | Loss: 0.00075813
Iteration 8/25 | Loss: 0.00074483
Iteration 9/25 | Loss: 0.00073447
Iteration 10/25 | Loss: 0.00074610
Iteration 11/25 | Loss: 0.00073783
Iteration 12/25 | Loss: 0.00074223
Iteration 13/25 | Loss: 0.00072812
Iteration 14/25 | Loss: 0.00072437
Iteration 15/25 | Loss: 0.00072110
Iteration 16/25 | Loss: 0.00071659
Iteration 17/25 | Loss: 0.00071706
Iteration 18/25 | Loss: 0.00071165
Iteration 19/25 | Loss: 0.00070892
Iteration 20/25 | Loss: 0.00069940
Iteration 21/25 | Loss: 0.00068882
Iteration 22/25 | Loss: 0.00068928
Iteration 23/25 | Loss: 0.00068218
Iteration 24/25 | Loss: 0.00068413
Iteration 25/25 | Loss: 0.00068111

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41300952
Iteration 2/25 | Loss: 0.00021392
Iteration 3/25 | Loss: 0.00019486
Iteration 4/25 | Loss: 0.00019486
Iteration 5/25 | Loss: 0.00019486
Iteration 6/25 | Loss: 0.00019486
Iteration 7/25 | Loss: 0.00019486
Iteration 8/25 | Loss: 0.00019486
Iteration 9/25 | Loss: 0.00019486
Iteration 10/25 | Loss: 0.00019486
Iteration 11/25 | Loss: 0.00019486
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00019486174278426915, 0.00019486174278426915, 0.00019486174278426915, 0.00019486174278426915, 0.00019486174278426915]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00019486174278426915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019486
Iteration 2/1000 | Loss: 0.00006396
Iteration 3/1000 | Loss: 0.00003625
Iteration 4/1000 | Loss: 0.00008862
Iteration 5/1000 | Loss: 0.00005299
Iteration 6/1000 | Loss: 0.00025541
Iteration 7/1000 | Loss: 0.00008386
Iteration 8/1000 | Loss: 0.00022280
Iteration 9/1000 | Loss: 0.00010510
Iteration 10/1000 | Loss: 0.00017719
Iteration 11/1000 | Loss: 0.00010051
Iteration 12/1000 | Loss: 0.00013241
Iteration 13/1000 | Loss: 0.00009535
Iteration 14/1000 | Loss: 0.00010686
Iteration 15/1000 | Loss: 0.00006027
Iteration 16/1000 | Loss: 0.00016315
Iteration 17/1000 | Loss: 0.00003123
Iteration 18/1000 | Loss: 0.00002286
Iteration 19/1000 | Loss: 0.00002742
Iteration 20/1000 | Loss: 0.00002099
Iteration 21/1000 | Loss: 0.00002622
Iteration 22/1000 | Loss: 0.00003354
Iteration 23/1000 | Loss: 0.00002004
Iteration 24/1000 | Loss: 0.00001992
Iteration 25/1000 | Loss: 0.00001989
Iteration 26/1000 | Loss: 0.00001988
Iteration 27/1000 | Loss: 0.00002545
Iteration 28/1000 | Loss: 0.00001969
Iteration 29/1000 | Loss: 0.00001956
Iteration 30/1000 | Loss: 0.00001946
Iteration 31/1000 | Loss: 0.00001945
Iteration 32/1000 | Loss: 0.00001943
Iteration 33/1000 | Loss: 0.00001942
Iteration 34/1000 | Loss: 0.00001941
Iteration 35/1000 | Loss: 0.00001938
Iteration 36/1000 | Loss: 0.00001938
Iteration 37/1000 | Loss: 0.00002085
Iteration 38/1000 | Loss: 0.00001964
Iteration 39/1000 | Loss: 0.00001948
Iteration 40/1000 | Loss: 0.00001946
Iteration 41/1000 | Loss: 0.00001945
Iteration 42/1000 | Loss: 0.00001944
Iteration 43/1000 | Loss: 0.00001943
Iteration 44/1000 | Loss: 0.00001942
Iteration 45/1000 | Loss: 0.00001941
Iteration 46/1000 | Loss: 0.00001934
Iteration 47/1000 | Loss: 0.00001932
Iteration 48/1000 | Loss: 0.00001931
Iteration 49/1000 | Loss: 0.00001926
Iteration 50/1000 | Loss: 0.00001925
Iteration 51/1000 | Loss: 0.00001924
Iteration 52/1000 | Loss: 0.00005101
Iteration 53/1000 | Loss: 0.00001936
Iteration 54/1000 | Loss: 0.00001909
Iteration 55/1000 | Loss: 0.00001907
Iteration 56/1000 | Loss: 0.00001906
Iteration 57/1000 | Loss: 0.00001906
Iteration 58/1000 | Loss: 0.00001905
Iteration 59/1000 | Loss: 0.00001905
Iteration 60/1000 | Loss: 0.00001904
Iteration 61/1000 | Loss: 0.00001904
Iteration 62/1000 | Loss: 0.00001901
Iteration 63/1000 | Loss: 0.00001901
Iteration 64/1000 | Loss: 0.00001901
Iteration 65/1000 | Loss: 0.00001901
Iteration 66/1000 | Loss: 0.00001900
Iteration 67/1000 | Loss: 0.00001900
Iteration 68/1000 | Loss: 0.00001900
Iteration 69/1000 | Loss: 0.00001900
Iteration 70/1000 | Loss: 0.00001900
Iteration 71/1000 | Loss: 0.00001899
Iteration 72/1000 | Loss: 0.00001899
Iteration 73/1000 | Loss: 0.00001899
Iteration 74/1000 | Loss: 0.00001899
Iteration 75/1000 | Loss: 0.00001898
Iteration 76/1000 | Loss: 0.00001898
Iteration 77/1000 | Loss: 0.00001898
Iteration 78/1000 | Loss: 0.00001898
Iteration 79/1000 | Loss: 0.00001898
Iteration 80/1000 | Loss: 0.00001898
Iteration 81/1000 | Loss: 0.00001897
Iteration 82/1000 | Loss: 0.00001897
Iteration 83/1000 | Loss: 0.00001897
Iteration 84/1000 | Loss: 0.00001897
Iteration 85/1000 | Loss: 0.00001897
Iteration 86/1000 | Loss: 0.00001897
Iteration 87/1000 | Loss: 0.00001897
Iteration 88/1000 | Loss: 0.00001897
Iteration 89/1000 | Loss: 0.00001896
Iteration 90/1000 | Loss: 0.00001896
Iteration 91/1000 | Loss: 0.00001895
Iteration 92/1000 | Loss: 0.00001895
Iteration 93/1000 | Loss: 0.00001895
Iteration 94/1000 | Loss: 0.00001895
Iteration 95/1000 | Loss: 0.00001895
Iteration 96/1000 | Loss: 0.00001895
Iteration 97/1000 | Loss: 0.00001895
Iteration 98/1000 | Loss: 0.00001895
Iteration 99/1000 | Loss: 0.00001894
Iteration 100/1000 | Loss: 0.00001894
Iteration 101/1000 | Loss: 0.00001894
Iteration 102/1000 | Loss: 0.00001894
Iteration 103/1000 | Loss: 0.00001893
Iteration 104/1000 | Loss: 0.00001893
Iteration 105/1000 | Loss: 0.00001893
Iteration 106/1000 | Loss: 0.00001893
Iteration 107/1000 | Loss: 0.00001893
Iteration 108/1000 | Loss: 0.00001893
Iteration 109/1000 | Loss: 0.00001893
Iteration 110/1000 | Loss: 0.00001893
Iteration 111/1000 | Loss: 0.00001893
Iteration 112/1000 | Loss: 0.00001893
Iteration 113/1000 | Loss: 0.00001893
Iteration 114/1000 | Loss: 0.00001893
Iteration 115/1000 | Loss: 0.00001892
Iteration 116/1000 | Loss: 0.00001892
Iteration 117/1000 | Loss: 0.00001892
Iteration 118/1000 | Loss: 0.00001892
Iteration 119/1000 | Loss: 0.00001892
Iteration 120/1000 | Loss: 0.00001892
Iteration 121/1000 | Loss: 0.00001892
Iteration 122/1000 | Loss: 0.00001892
Iteration 123/1000 | Loss: 0.00001892
Iteration 124/1000 | Loss: 0.00001892
Iteration 125/1000 | Loss: 0.00001892
Iteration 126/1000 | Loss: 0.00001892
Iteration 127/1000 | Loss: 0.00001892
Iteration 128/1000 | Loss: 0.00001892
Iteration 129/1000 | Loss: 0.00001892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.8918653950095177e-05, 1.8918653950095177e-05, 1.8918653950095177e-05, 1.8918653950095177e-05, 1.8918653950095177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8918653950095177e-05

Optimization complete. Final v2v error: 3.4434239864349365 mm

Highest mean error: 17.25471305847168 mm for frame 122

Lowest mean error: 2.8921046257019043 mm for frame 183

Saving results

Total time: 111.56449341773987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424433
Iteration 2/25 | Loss: 0.00092949
Iteration 3/25 | Loss: 0.00066947
Iteration 4/25 | Loss: 0.00063779
Iteration 5/25 | Loss: 0.00063216
Iteration 6/25 | Loss: 0.00063061
Iteration 7/25 | Loss: 0.00063025
Iteration 8/25 | Loss: 0.00063025
Iteration 9/25 | Loss: 0.00063025
Iteration 10/25 | Loss: 0.00063025
Iteration 11/25 | Loss: 0.00063025
Iteration 12/25 | Loss: 0.00063025
Iteration 13/25 | Loss: 0.00063025
Iteration 14/25 | Loss: 0.00063025
Iteration 15/25 | Loss: 0.00063025
Iteration 16/25 | Loss: 0.00063025
Iteration 17/25 | Loss: 0.00063025
Iteration 18/25 | Loss: 0.00063025
Iteration 19/25 | Loss: 0.00063025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006302489200606942, 0.0006302489200606942, 0.0006302489200606942, 0.0006302489200606942, 0.0006302489200606942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006302489200606942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59846711
Iteration 2/25 | Loss: 0.00018274
Iteration 3/25 | Loss: 0.00018274
Iteration 4/25 | Loss: 0.00018274
Iteration 5/25 | Loss: 0.00018274
Iteration 6/25 | Loss: 0.00018274
Iteration 7/25 | Loss: 0.00018274
Iteration 8/25 | Loss: 0.00018273
Iteration 9/25 | Loss: 0.00018273
Iteration 10/25 | Loss: 0.00018273
Iteration 11/25 | Loss: 0.00018273
Iteration 12/25 | Loss: 0.00018273
Iteration 13/25 | Loss: 0.00018273
Iteration 14/25 | Loss: 0.00018273
Iteration 15/25 | Loss: 0.00018273
Iteration 16/25 | Loss: 0.00018273
Iteration 17/25 | Loss: 0.00018273
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00018273424939252436, 0.00018273424939252436, 0.00018273424939252436, 0.00018273424939252436, 0.00018273424939252436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00018273424939252436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018273
Iteration 2/1000 | Loss: 0.00001767
Iteration 3/1000 | Loss: 0.00001481
Iteration 4/1000 | Loss: 0.00001362
Iteration 5/1000 | Loss: 0.00001313
Iteration 6/1000 | Loss: 0.00001302
Iteration 7/1000 | Loss: 0.00001288
Iteration 8/1000 | Loss: 0.00001264
Iteration 9/1000 | Loss: 0.00001250
Iteration 10/1000 | Loss: 0.00001246
Iteration 11/1000 | Loss: 0.00001246
Iteration 12/1000 | Loss: 0.00001241
Iteration 13/1000 | Loss: 0.00001237
Iteration 14/1000 | Loss: 0.00001236
Iteration 15/1000 | Loss: 0.00001236
Iteration 16/1000 | Loss: 0.00001235
Iteration 17/1000 | Loss: 0.00001235
Iteration 18/1000 | Loss: 0.00001235
Iteration 19/1000 | Loss: 0.00001234
Iteration 20/1000 | Loss: 0.00001234
Iteration 21/1000 | Loss: 0.00001234
Iteration 22/1000 | Loss: 0.00001232
Iteration 23/1000 | Loss: 0.00001231
Iteration 24/1000 | Loss: 0.00001230
Iteration 25/1000 | Loss: 0.00001230
Iteration 26/1000 | Loss: 0.00001229
Iteration 27/1000 | Loss: 0.00001226
Iteration 28/1000 | Loss: 0.00001225
Iteration 29/1000 | Loss: 0.00001224
Iteration 30/1000 | Loss: 0.00001224
Iteration 31/1000 | Loss: 0.00001223
Iteration 32/1000 | Loss: 0.00001222
Iteration 33/1000 | Loss: 0.00001221
Iteration 34/1000 | Loss: 0.00001219
Iteration 35/1000 | Loss: 0.00001219
Iteration 36/1000 | Loss: 0.00001219
Iteration 37/1000 | Loss: 0.00001219
Iteration 38/1000 | Loss: 0.00001219
Iteration 39/1000 | Loss: 0.00001219
Iteration 40/1000 | Loss: 0.00001219
Iteration 41/1000 | Loss: 0.00001219
Iteration 42/1000 | Loss: 0.00001219
Iteration 43/1000 | Loss: 0.00001219
Iteration 44/1000 | Loss: 0.00001219
Iteration 45/1000 | Loss: 0.00001219
Iteration 46/1000 | Loss: 0.00001218
Iteration 47/1000 | Loss: 0.00001218
Iteration 48/1000 | Loss: 0.00001218
Iteration 49/1000 | Loss: 0.00001218
Iteration 50/1000 | Loss: 0.00001217
Iteration 51/1000 | Loss: 0.00001217
Iteration 52/1000 | Loss: 0.00001217
Iteration 53/1000 | Loss: 0.00001217
Iteration 54/1000 | Loss: 0.00001217
Iteration 55/1000 | Loss: 0.00001216
Iteration 56/1000 | Loss: 0.00001216
Iteration 57/1000 | Loss: 0.00001216
Iteration 58/1000 | Loss: 0.00001216
Iteration 59/1000 | Loss: 0.00001216
Iteration 60/1000 | Loss: 0.00001216
Iteration 61/1000 | Loss: 0.00001215
Iteration 62/1000 | Loss: 0.00001215
Iteration 63/1000 | Loss: 0.00001215
Iteration 64/1000 | Loss: 0.00001215
Iteration 65/1000 | Loss: 0.00001215
Iteration 66/1000 | Loss: 0.00001215
Iteration 67/1000 | Loss: 0.00001215
Iteration 68/1000 | Loss: 0.00001215
Iteration 69/1000 | Loss: 0.00001215
Iteration 70/1000 | Loss: 0.00001215
Iteration 71/1000 | Loss: 0.00001214
Iteration 72/1000 | Loss: 0.00001214
Iteration 73/1000 | Loss: 0.00001214
Iteration 74/1000 | Loss: 0.00001214
Iteration 75/1000 | Loss: 0.00001214
Iteration 76/1000 | Loss: 0.00001214
Iteration 77/1000 | Loss: 0.00001214
Iteration 78/1000 | Loss: 0.00001214
Iteration 79/1000 | Loss: 0.00001214
Iteration 80/1000 | Loss: 0.00001214
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001214
Iteration 85/1000 | Loss: 0.00001214
Iteration 86/1000 | Loss: 0.00001214
Iteration 87/1000 | Loss: 0.00001214
Iteration 88/1000 | Loss: 0.00001214
Iteration 89/1000 | Loss: 0.00001214
Iteration 90/1000 | Loss: 0.00001214
Iteration 91/1000 | Loss: 0.00001214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.2139261343691032e-05, 1.2139261343691032e-05, 1.2139261343691032e-05, 1.2139261343691032e-05, 1.2139261343691032e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2139261343691032e-05

Optimization complete. Final v2v error: 2.994555711746216 mm

Highest mean error: 3.189023971557617 mm for frame 106

Lowest mean error: 2.87188458442688 mm for frame 72

Saving results

Total time: 27.94675636291504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781300
Iteration 2/25 | Loss: 0.00136967
Iteration 3/25 | Loss: 0.00081655
Iteration 4/25 | Loss: 0.00091649
Iteration 5/25 | Loss: 0.00075447
Iteration 6/25 | Loss: 0.00073436
Iteration 7/25 | Loss: 0.00069226
Iteration 8/25 | Loss: 0.00067724
Iteration 9/25 | Loss: 0.00068535
Iteration 10/25 | Loss: 0.00067130
Iteration 11/25 | Loss: 0.00066842
Iteration 12/25 | Loss: 0.00066792
Iteration 13/25 | Loss: 0.00066791
Iteration 14/25 | Loss: 0.00066791
Iteration 15/25 | Loss: 0.00066791
Iteration 16/25 | Loss: 0.00066791
Iteration 17/25 | Loss: 0.00066791
Iteration 18/25 | Loss: 0.00066791
Iteration 19/25 | Loss: 0.00066791
Iteration 20/25 | Loss: 0.00066791
Iteration 21/25 | Loss: 0.00066791
Iteration 22/25 | Loss: 0.00066790
Iteration 23/25 | Loss: 0.00066790
Iteration 24/25 | Loss: 0.00066790
Iteration 25/25 | Loss: 0.00066790

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.84201670
Iteration 2/25 | Loss: 0.00016917
Iteration 3/25 | Loss: 0.00016917
Iteration 4/25 | Loss: 0.00016917
Iteration 5/25 | Loss: 0.00016917
Iteration 6/25 | Loss: 0.00016917
Iteration 7/25 | Loss: 0.00016917
Iteration 8/25 | Loss: 0.00016917
Iteration 9/25 | Loss: 0.00016917
Iteration 10/25 | Loss: 0.00016917
Iteration 11/25 | Loss: 0.00016917
Iteration 12/25 | Loss: 0.00016916
Iteration 13/25 | Loss: 0.00016916
Iteration 14/25 | Loss: 0.00016916
Iteration 15/25 | Loss: 0.00016916
Iteration 16/25 | Loss: 0.00016916
Iteration 17/25 | Loss: 0.00016916
Iteration 18/25 | Loss: 0.00016916
Iteration 19/25 | Loss: 0.00016916
Iteration 20/25 | Loss: 0.00016916
Iteration 21/25 | Loss: 0.00016916
Iteration 22/25 | Loss: 0.00016916
Iteration 23/25 | Loss: 0.00016916
Iteration 24/25 | Loss: 0.00016916
Iteration 25/25 | Loss: 0.00016916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00016916
Iteration 2/1000 | Loss: 0.00003077
Iteration 3/1000 | Loss: 0.00002580
Iteration 4/1000 | Loss: 0.00002286
Iteration 5/1000 | Loss: 0.00002170
Iteration 6/1000 | Loss: 0.00002079
Iteration 7/1000 | Loss: 0.00002014
Iteration 8/1000 | Loss: 0.00001970
Iteration 9/1000 | Loss: 0.00001934
Iteration 10/1000 | Loss: 0.00001913
Iteration 11/1000 | Loss: 0.00001893
Iteration 12/1000 | Loss: 0.00001885
Iteration 13/1000 | Loss: 0.00001876
Iteration 14/1000 | Loss: 0.00001874
Iteration 15/1000 | Loss: 0.00001863
Iteration 16/1000 | Loss: 0.00001860
Iteration 17/1000 | Loss: 0.00001859
Iteration 18/1000 | Loss: 0.00001855
Iteration 19/1000 | Loss: 0.00001855
Iteration 20/1000 | Loss: 0.00001853
Iteration 21/1000 | Loss: 0.00001852
Iteration 22/1000 | Loss: 0.00001852
Iteration 23/1000 | Loss: 0.00001852
Iteration 24/1000 | Loss: 0.00001851
Iteration 25/1000 | Loss: 0.00001851
Iteration 26/1000 | Loss: 0.00001851
Iteration 27/1000 | Loss: 0.00001851
Iteration 28/1000 | Loss: 0.00001850
Iteration 29/1000 | Loss: 0.00001850
Iteration 30/1000 | Loss: 0.00001850
Iteration 31/1000 | Loss: 0.00001850
Iteration 32/1000 | Loss: 0.00001849
Iteration 33/1000 | Loss: 0.00001849
Iteration 34/1000 | Loss: 0.00001849
Iteration 35/1000 | Loss: 0.00001849
Iteration 36/1000 | Loss: 0.00001849
Iteration 37/1000 | Loss: 0.00001848
Iteration 38/1000 | Loss: 0.00001848
Iteration 39/1000 | Loss: 0.00001848
Iteration 40/1000 | Loss: 0.00001848
Iteration 41/1000 | Loss: 0.00001847
Iteration 42/1000 | Loss: 0.00001847
Iteration 43/1000 | Loss: 0.00001847
Iteration 44/1000 | Loss: 0.00001847
Iteration 45/1000 | Loss: 0.00001847
Iteration 46/1000 | Loss: 0.00001846
Iteration 47/1000 | Loss: 0.00001846
Iteration 48/1000 | Loss: 0.00001846
Iteration 49/1000 | Loss: 0.00001846
Iteration 50/1000 | Loss: 0.00001846
Iteration 51/1000 | Loss: 0.00001846
Iteration 52/1000 | Loss: 0.00001846
Iteration 53/1000 | Loss: 0.00001846
Iteration 54/1000 | Loss: 0.00001845
Iteration 55/1000 | Loss: 0.00001845
Iteration 56/1000 | Loss: 0.00001845
Iteration 57/1000 | Loss: 0.00001845
Iteration 58/1000 | Loss: 0.00001845
Iteration 59/1000 | Loss: 0.00001845
Iteration 60/1000 | Loss: 0.00001845
Iteration 61/1000 | Loss: 0.00001844
Iteration 62/1000 | Loss: 0.00001844
Iteration 63/1000 | Loss: 0.00001844
Iteration 64/1000 | Loss: 0.00001844
Iteration 65/1000 | Loss: 0.00001844
Iteration 66/1000 | Loss: 0.00001844
Iteration 67/1000 | Loss: 0.00001844
Iteration 68/1000 | Loss: 0.00001843
Iteration 69/1000 | Loss: 0.00001843
Iteration 70/1000 | Loss: 0.00001843
Iteration 71/1000 | Loss: 0.00001843
Iteration 72/1000 | Loss: 0.00001843
Iteration 73/1000 | Loss: 0.00001842
Iteration 74/1000 | Loss: 0.00001842
Iteration 75/1000 | Loss: 0.00001842
Iteration 76/1000 | Loss: 0.00001842
Iteration 77/1000 | Loss: 0.00001842
Iteration 78/1000 | Loss: 0.00001842
Iteration 79/1000 | Loss: 0.00001842
Iteration 80/1000 | Loss: 0.00001841
Iteration 81/1000 | Loss: 0.00001841
Iteration 82/1000 | Loss: 0.00001841
Iteration 83/1000 | Loss: 0.00001841
Iteration 84/1000 | Loss: 0.00001841
Iteration 85/1000 | Loss: 0.00001841
Iteration 86/1000 | Loss: 0.00001840
Iteration 87/1000 | Loss: 0.00001840
Iteration 88/1000 | Loss: 0.00001840
Iteration 89/1000 | Loss: 0.00001840
Iteration 90/1000 | Loss: 0.00001840
Iteration 91/1000 | Loss: 0.00001840
Iteration 92/1000 | Loss: 0.00001840
Iteration 93/1000 | Loss: 0.00001839
Iteration 94/1000 | Loss: 0.00001839
Iteration 95/1000 | Loss: 0.00001839
Iteration 96/1000 | Loss: 0.00001839
Iteration 97/1000 | Loss: 0.00001839
Iteration 98/1000 | Loss: 0.00001839
Iteration 99/1000 | Loss: 0.00001839
Iteration 100/1000 | Loss: 0.00001839
Iteration 101/1000 | Loss: 0.00001839
Iteration 102/1000 | Loss: 0.00001838
Iteration 103/1000 | Loss: 0.00001838
Iteration 104/1000 | Loss: 0.00001838
Iteration 105/1000 | Loss: 0.00001838
Iteration 106/1000 | Loss: 0.00001838
Iteration 107/1000 | Loss: 0.00001838
Iteration 108/1000 | Loss: 0.00001838
Iteration 109/1000 | Loss: 0.00001838
Iteration 110/1000 | Loss: 0.00001838
Iteration 111/1000 | Loss: 0.00001838
Iteration 112/1000 | Loss: 0.00001838
Iteration 113/1000 | Loss: 0.00001838
Iteration 114/1000 | Loss: 0.00001838
Iteration 115/1000 | Loss: 0.00001838
Iteration 116/1000 | Loss: 0.00001838
Iteration 117/1000 | Loss: 0.00001837
Iteration 118/1000 | Loss: 0.00001837
Iteration 119/1000 | Loss: 0.00001837
Iteration 120/1000 | Loss: 0.00001837
Iteration 121/1000 | Loss: 0.00001837
Iteration 122/1000 | Loss: 0.00001837
Iteration 123/1000 | Loss: 0.00001837
Iteration 124/1000 | Loss: 0.00001837
Iteration 125/1000 | Loss: 0.00001837
Iteration 126/1000 | Loss: 0.00001837
Iteration 127/1000 | Loss: 0.00001837
Iteration 128/1000 | Loss: 0.00001837
Iteration 129/1000 | Loss: 0.00001837
Iteration 130/1000 | Loss: 0.00001837
Iteration 131/1000 | Loss: 0.00001837
Iteration 132/1000 | Loss: 0.00001837
Iteration 133/1000 | Loss: 0.00001836
Iteration 134/1000 | Loss: 0.00001836
Iteration 135/1000 | Loss: 0.00001836
Iteration 136/1000 | Loss: 0.00001836
Iteration 137/1000 | Loss: 0.00001836
Iteration 138/1000 | Loss: 0.00001836
Iteration 139/1000 | Loss: 0.00001836
Iteration 140/1000 | Loss: 0.00001835
Iteration 141/1000 | Loss: 0.00001835
Iteration 142/1000 | Loss: 0.00001835
Iteration 143/1000 | Loss: 0.00001835
Iteration 144/1000 | Loss: 0.00001835
Iteration 145/1000 | Loss: 0.00001835
Iteration 146/1000 | Loss: 0.00001835
Iteration 147/1000 | Loss: 0.00001835
Iteration 148/1000 | Loss: 0.00001835
Iteration 149/1000 | Loss: 0.00001835
Iteration 150/1000 | Loss: 0.00001835
Iteration 151/1000 | Loss: 0.00001835
Iteration 152/1000 | Loss: 0.00001835
Iteration 153/1000 | Loss: 0.00001835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.8354177882429212e-05, 1.8354177882429212e-05, 1.8354177882429212e-05, 1.8354177882429212e-05, 1.8354177882429212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8354177882429212e-05

Optimization complete. Final v2v error: 3.5779852867126465 mm

Highest mean error: 4.555929183959961 mm for frame 26

Lowest mean error: 2.821916341781616 mm for frame 46

Saving results

Total time: 57.83718967437744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00748579
Iteration 2/25 | Loss: 0.00142406
Iteration 3/25 | Loss: 0.00094778
Iteration 4/25 | Loss: 0.00079535
Iteration 5/25 | Loss: 0.00071022
Iteration 6/25 | Loss: 0.00069045
Iteration 7/25 | Loss: 0.00068605
Iteration 8/25 | Loss: 0.00068349
Iteration 9/25 | Loss: 0.00068231
Iteration 10/25 | Loss: 0.00068187
Iteration 11/25 | Loss: 0.00068169
Iteration 12/25 | Loss: 0.00068162
Iteration 13/25 | Loss: 0.00068161
Iteration 14/25 | Loss: 0.00068161
Iteration 15/25 | Loss: 0.00068161
Iteration 16/25 | Loss: 0.00068160
Iteration 17/25 | Loss: 0.00068160
Iteration 18/25 | Loss: 0.00068160
Iteration 19/25 | Loss: 0.00068160
Iteration 20/25 | Loss: 0.00068160
Iteration 21/25 | Loss: 0.00068160
Iteration 22/25 | Loss: 0.00068160
Iteration 23/25 | Loss: 0.00068160
Iteration 24/25 | Loss: 0.00068159
Iteration 25/25 | Loss: 0.00068159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86026657
Iteration 2/25 | Loss: 0.00015559
Iteration 3/25 | Loss: 0.00015559
Iteration 4/25 | Loss: 0.00015559
Iteration 5/25 | Loss: 0.00015559
Iteration 6/25 | Loss: 0.00015559
Iteration 7/25 | Loss: 0.00015559
Iteration 8/25 | Loss: 0.00015559
Iteration 9/25 | Loss: 0.00015559
Iteration 10/25 | Loss: 0.00015559
Iteration 11/25 | Loss: 0.00015559
Iteration 12/25 | Loss: 0.00015559
Iteration 13/25 | Loss: 0.00015559
Iteration 14/25 | Loss: 0.00015559
Iteration 15/25 | Loss: 0.00015559
Iteration 16/25 | Loss: 0.00015559
Iteration 17/25 | Loss: 0.00015559
Iteration 18/25 | Loss: 0.00015559
Iteration 19/25 | Loss: 0.00015559
Iteration 20/25 | Loss: 0.00015559
Iteration 21/25 | Loss: 0.00015559
Iteration 22/25 | Loss: 0.00015559
Iteration 23/25 | Loss: 0.00015559
Iteration 24/25 | Loss: 0.00015559
Iteration 25/25 | Loss: 0.00015559

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00015559
Iteration 2/1000 | Loss: 0.00003365
Iteration 3/1000 | Loss: 0.00002746
Iteration 4/1000 | Loss: 0.00002561
Iteration 5/1000 | Loss: 0.00002435
Iteration 6/1000 | Loss: 0.00002347
Iteration 7/1000 | Loss: 0.00002292
Iteration 8/1000 | Loss: 0.00002248
Iteration 9/1000 | Loss: 0.00002218
Iteration 10/1000 | Loss: 0.00002200
Iteration 11/1000 | Loss: 0.00002194
Iteration 12/1000 | Loss: 0.00002191
Iteration 13/1000 | Loss: 0.00002188
Iteration 14/1000 | Loss: 0.00002187
Iteration 15/1000 | Loss: 0.00002187
Iteration 16/1000 | Loss: 0.00002186
Iteration 17/1000 | Loss: 0.00002185
Iteration 18/1000 | Loss: 0.00002185
Iteration 19/1000 | Loss: 0.00002184
Iteration 20/1000 | Loss: 0.00002179
Iteration 21/1000 | Loss: 0.00002179
Iteration 22/1000 | Loss: 0.00002179
Iteration 23/1000 | Loss: 0.00002178
Iteration 24/1000 | Loss: 0.00002177
Iteration 25/1000 | Loss: 0.00002177
Iteration 26/1000 | Loss: 0.00002176
Iteration 27/1000 | Loss: 0.00002176
Iteration 28/1000 | Loss: 0.00002175
Iteration 29/1000 | Loss: 0.00002174
Iteration 30/1000 | Loss: 0.00002174
Iteration 31/1000 | Loss: 0.00002174
Iteration 32/1000 | Loss: 0.00002173
Iteration 33/1000 | Loss: 0.00002173
Iteration 34/1000 | Loss: 0.00002173
Iteration 35/1000 | Loss: 0.00002172
Iteration 36/1000 | Loss: 0.00002171
Iteration 37/1000 | Loss: 0.00002171
Iteration 38/1000 | Loss: 0.00002171
Iteration 39/1000 | Loss: 0.00002171
Iteration 40/1000 | Loss: 0.00002170
Iteration 41/1000 | Loss: 0.00002169
Iteration 42/1000 | Loss: 0.00002169
Iteration 43/1000 | Loss: 0.00002168
Iteration 44/1000 | Loss: 0.00002168
Iteration 45/1000 | Loss: 0.00002168
Iteration 46/1000 | Loss: 0.00002168
Iteration 47/1000 | Loss: 0.00002167
Iteration 48/1000 | Loss: 0.00002167
Iteration 49/1000 | Loss: 0.00002166
Iteration 50/1000 | Loss: 0.00002166
Iteration 51/1000 | Loss: 0.00002165
Iteration 52/1000 | Loss: 0.00002165
Iteration 53/1000 | Loss: 0.00002164
Iteration 54/1000 | Loss: 0.00002164
Iteration 55/1000 | Loss: 0.00002163
Iteration 56/1000 | Loss: 0.00002163
Iteration 57/1000 | Loss: 0.00002163
Iteration 58/1000 | Loss: 0.00002163
Iteration 59/1000 | Loss: 0.00002163
Iteration 60/1000 | Loss: 0.00002163
Iteration 61/1000 | Loss: 0.00002163
Iteration 62/1000 | Loss: 0.00002163
Iteration 63/1000 | Loss: 0.00002162
Iteration 64/1000 | Loss: 0.00002160
Iteration 65/1000 | Loss: 0.00002160
Iteration 66/1000 | Loss: 0.00002160
Iteration 67/1000 | Loss: 0.00002160
Iteration 68/1000 | Loss: 0.00002159
Iteration 69/1000 | Loss: 0.00002159
Iteration 70/1000 | Loss: 0.00002159
Iteration 71/1000 | Loss: 0.00002158
Iteration 72/1000 | Loss: 0.00002158
Iteration 73/1000 | Loss: 0.00002158
Iteration 74/1000 | Loss: 0.00002156
Iteration 75/1000 | Loss: 0.00002156
Iteration 76/1000 | Loss: 0.00002155
Iteration 77/1000 | Loss: 0.00002155
Iteration 78/1000 | Loss: 0.00002155
Iteration 79/1000 | Loss: 0.00002155
Iteration 80/1000 | Loss: 0.00002154
Iteration 81/1000 | Loss: 0.00002154
Iteration 82/1000 | Loss: 0.00002154
Iteration 83/1000 | Loss: 0.00002154
Iteration 84/1000 | Loss: 0.00002154
Iteration 85/1000 | Loss: 0.00002154
Iteration 86/1000 | Loss: 0.00002154
Iteration 87/1000 | Loss: 0.00002154
Iteration 88/1000 | Loss: 0.00002154
Iteration 89/1000 | Loss: 0.00002154
Iteration 90/1000 | Loss: 0.00002154
Iteration 91/1000 | Loss: 0.00002153
Iteration 92/1000 | Loss: 0.00002153
Iteration 93/1000 | Loss: 0.00002152
Iteration 94/1000 | Loss: 0.00002152
Iteration 95/1000 | Loss: 0.00002152
Iteration 96/1000 | Loss: 0.00002151
Iteration 97/1000 | Loss: 0.00002151
Iteration 98/1000 | Loss: 0.00002151
Iteration 99/1000 | Loss: 0.00002151
Iteration 100/1000 | Loss: 0.00002151
Iteration 101/1000 | Loss: 0.00002151
Iteration 102/1000 | Loss: 0.00002150
Iteration 103/1000 | Loss: 0.00002150
Iteration 104/1000 | Loss: 0.00002150
Iteration 105/1000 | Loss: 0.00002150
Iteration 106/1000 | Loss: 0.00002150
Iteration 107/1000 | Loss: 0.00002149
Iteration 108/1000 | Loss: 0.00002149
Iteration 109/1000 | Loss: 0.00002149
Iteration 110/1000 | Loss: 0.00002149
Iteration 111/1000 | Loss: 0.00002149
Iteration 112/1000 | Loss: 0.00002148
Iteration 113/1000 | Loss: 0.00002148
Iteration 114/1000 | Loss: 0.00002148
Iteration 115/1000 | Loss: 0.00002148
Iteration 116/1000 | Loss: 0.00002148
Iteration 117/1000 | Loss: 0.00002148
Iteration 118/1000 | Loss: 0.00002148
Iteration 119/1000 | Loss: 0.00002148
Iteration 120/1000 | Loss: 0.00002148
Iteration 121/1000 | Loss: 0.00002148
Iteration 122/1000 | Loss: 0.00002147
Iteration 123/1000 | Loss: 0.00002147
Iteration 124/1000 | Loss: 0.00002147
Iteration 125/1000 | Loss: 0.00002147
Iteration 126/1000 | Loss: 0.00002146
Iteration 127/1000 | Loss: 0.00002146
Iteration 128/1000 | Loss: 0.00002146
Iteration 129/1000 | Loss: 0.00002146
Iteration 130/1000 | Loss: 0.00002146
Iteration 131/1000 | Loss: 0.00002146
Iteration 132/1000 | Loss: 0.00002146
Iteration 133/1000 | Loss: 0.00002146
Iteration 134/1000 | Loss: 0.00002146
Iteration 135/1000 | Loss: 0.00002146
Iteration 136/1000 | Loss: 0.00002146
Iteration 137/1000 | Loss: 0.00002146
Iteration 138/1000 | Loss: 0.00002146
Iteration 139/1000 | Loss: 0.00002146
Iteration 140/1000 | Loss: 0.00002146
Iteration 141/1000 | Loss: 0.00002146
Iteration 142/1000 | Loss: 0.00002146
Iteration 143/1000 | Loss: 0.00002146
Iteration 144/1000 | Loss: 0.00002146
Iteration 145/1000 | Loss: 0.00002146
Iteration 146/1000 | Loss: 0.00002146
Iteration 147/1000 | Loss: 0.00002146
Iteration 148/1000 | Loss: 0.00002146
Iteration 149/1000 | Loss: 0.00002146
Iteration 150/1000 | Loss: 0.00002146
Iteration 151/1000 | Loss: 0.00002146
Iteration 152/1000 | Loss: 0.00002146
Iteration 153/1000 | Loss: 0.00002146
Iteration 154/1000 | Loss: 0.00002146
Iteration 155/1000 | Loss: 0.00002146
Iteration 156/1000 | Loss: 0.00002146
Iteration 157/1000 | Loss: 0.00002146
Iteration 158/1000 | Loss: 0.00002146
Iteration 159/1000 | Loss: 0.00002146
Iteration 160/1000 | Loss: 0.00002146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.145604412362445e-05, 2.145604412362445e-05, 2.145604412362445e-05, 2.145604412362445e-05, 2.145604412362445e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.145604412362445e-05

Optimization complete. Final v2v error: 3.8628125190734863 mm

Highest mean error: 4.218276500701904 mm for frame 103

Lowest mean error: 3.4486021995544434 mm for frame 52

Saving results

Total time: 45.99312400817871
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415260
Iteration 2/25 | Loss: 0.00073236
Iteration 3/25 | Loss: 0.00063564
Iteration 4/25 | Loss: 0.00061351
Iteration 5/25 | Loss: 0.00060410
Iteration 6/25 | Loss: 0.00060223
Iteration 7/25 | Loss: 0.00060156
Iteration 8/25 | Loss: 0.00060148
Iteration 9/25 | Loss: 0.00060148
Iteration 10/25 | Loss: 0.00060148
Iteration 11/25 | Loss: 0.00060148
Iteration 12/25 | Loss: 0.00060148
Iteration 13/25 | Loss: 0.00060148
Iteration 14/25 | Loss: 0.00060148
Iteration 15/25 | Loss: 0.00060148
Iteration 16/25 | Loss: 0.00060148
Iteration 17/25 | Loss: 0.00060148
Iteration 18/25 | Loss: 0.00060148
Iteration 19/25 | Loss: 0.00060148
Iteration 20/25 | Loss: 0.00060148
Iteration 21/25 | Loss: 0.00060148
Iteration 22/25 | Loss: 0.00060148
Iteration 23/25 | Loss: 0.00060148
Iteration 24/25 | Loss: 0.00060148
Iteration 25/25 | Loss: 0.00060148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64748204
Iteration 2/25 | Loss: 0.00013414
Iteration 3/25 | Loss: 0.00013414
Iteration 4/25 | Loss: 0.00013414
Iteration 5/25 | Loss: 0.00013414
Iteration 6/25 | Loss: 0.00013414
Iteration 7/25 | Loss: 0.00013414
Iteration 8/25 | Loss: 0.00013414
Iteration 9/25 | Loss: 0.00013414
Iteration 10/25 | Loss: 0.00013414
Iteration 11/25 | Loss: 0.00013414
Iteration 12/25 | Loss: 0.00013414
Iteration 13/25 | Loss: 0.00013414
Iteration 14/25 | Loss: 0.00013414
Iteration 15/25 | Loss: 0.00013414
Iteration 16/25 | Loss: 0.00013414
Iteration 17/25 | Loss: 0.00013414
Iteration 18/25 | Loss: 0.00013414
Iteration 19/25 | Loss: 0.00013414
Iteration 20/25 | Loss: 0.00013414
Iteration 21/25 | Loss: 0.00013414
Iteration 22/25 | Loss: 0.00013414
Iteration 23/25 | Loss: 0.00013414
Iteration 24/25 | Loss: 0.00013414
Iteration 25/25 | Loss: 0.00013414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00013414
Iteration 2/1000 | Loss: 0.00002063
Iteration 3/1000 | Loss: 0.00001751
Iteration 4/1000 | Loss: 0.00001648
Iteration 5/1000 | Loss: 0.00001564
Iteration 6/1000 | Loss: 0.00001527
Iteration 7/1000 | Loss: 0.00001499
Iteration 8/1000 | Loss: 0.00001484
Iteration 9/1000 | Loss: 0.00001479
Iteration 10/1000 | Loss: 0.00001476
Iteration 11/1000 | Loss: 0.00001465
Iteration 12/1000 | Loss: 0.00001464
Iteration 13/1000 | Loss: 0.00001464
Iteration 14/1000 | Loss: 0.00001463
Iteration 15/1000 | Loss: 0.00001462
Iteration 16/1000 | Loss: 0.00001460
Iteration 17/1000 | Loss: 0.00001459
Iteration 18/1000 | Loss: 0.00001458
Iteration 19/1000 | Loss: 0.00001458
Iteration 20/1000 | Loss: 0.00001458
Iteration 21/1000 | Loss: 0.00001457
Iteration 22/1000 | Loss: 0.00001457
Iteration 23/1000 | Loss: 0.00001456
Iteration 24/1000 | Loss: 0.00001455
Iteration 25/1000 | Loss: 0.00001455
Iteration 26/1000 | Loss: 0.00001455
Iteration 27/1000 | Loss: 0.00001454
Iteration 28/1000 | Loss: 0.00001454
Iteration 29/1000 | Loss: 0.00001453
Iteration 30/1000 | Loss: 0.00001453
Iteration 31/1000 | Loss: 0.00001453
Iteration 32/1000 | Loss: 0.00001452
Iteration 33/1000 | Loss: 0.00001451
Iteration 34/1000 | Loss: 0.00001451
Iteration 35/1000 | Loss: 0.00001450
Iteration 36/1000 | Loss: 0.00001450
Iteration 37/1000 | Loss: 0.00001450
Iteration 38/1000 | Loss: 0.00001449
Iteration 39/1000 | Loss: 0.00001449
Iteration 40/1000 | Loss: 0.00001449
Iteration 41/1000 | Loss: 0.00001448
Iteration 42/1000 | Loss: 0.00001448
Iteration 43/1000 | Loss: 0.00001447
Iteration 44/1000 | Loss: 0.00001447
Iteration 45/1000 | Loss: 0.00001447
Iteration 46/1000 | Loss: 0.00001447
Iteration 47/1000 | Loss: 0.00001447
Iteration 48/1000 | Loss: 0.00001446
Iteration 49/1000 | Loss: 0.00001446
Iteration 50/1000 | Loss: 0.00001446
Iteration 51/1000 | Loss: 0.00001446
Iteration 52/1000 | Loss: 0.00001446
Iteration 53/1000 | Loss: 0.00001445
Iteration 54/1000 | Loss: 0.00001445
Iteration 55/1000 | Loss: 0.00001445
Iteration 56/1000 | Loss: 0.00001445
Iteration 57/1000 | Loss: 0.00001445
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001443
Iteration 62/1000 | Loss: 0.00001443
Iteration 63/1000 | Loss: 0.00001443
Iteration 64/1000 | Loss: 0.00001443
Iteration 65/1000 | Loss: 0.00001441
Iteration 66/1000 | Loss: 0.00001441
Iteration 67/1000 | Loss: 0.00001441
Iteration 68/1000 | Loss: 0.00001441
Iteration 69/1000 | Loss: 0.00001440
Iteration 70/1000 | Loss: 0.00001440
Iteration 71/1000 | Loss: 0.00001439
Iteration 72/1000 | Loss: 0.00001438
Iteration 73/1000 | Loss: 0.00001438
Iteration 74/1000 | Loss: 0.00001437
Iteration 75/1000 | Loss: 0.00001437
Iteration 76/1000 | Loss: 0.00001437
Iteration 77/1000 | Loss: 0.00001437
Iteration 78/1000 | Loss: 0.00001436
Iteration 79/1000 | Loss: 0.00001436
Iteration 80/1000 | Loss: 0.00001435
Iteration 81/1000 | Loss: 0.00001435
Iteration 82/1000 | Loss: 0.00001435
Iteration 83/1000 | Loss: 0.00001435
Iteration 84/1000 | Loss: 0.00001435
Iteration 85/1000 | Loss: 0.00001435
Iteration 86/1000 | Loss: 0.00001435
Iteration 87/1000 | Loss: 0.00001435
Iteration 88/1000 | Loss: 0.00001434
Iteration 89/1000 | Loss: 0.00001434
Iteration 90/1000 | Loss: 0.00001434
Iteration 91/1000 | Loss: 0.00001434
Iteration 92/1000 | Loss: 0.00001434
Iteration 93/1000 | Loss: 0.00001434
Iteration 94/1000 | Loss: 0.00001433
Iteration 95/1000 | Loss: 0.00001433
Iteration 96/1000 | Loss: 0.00001433
Iteration 97/1000 | Loss: 0.00001433
Iteration 98/1000 | Loss: 0.00001433
Iteration 99/1000 | Loss: 0.00001433
Iteration 100/1000 | Loss: 0.00001433
Iteration 101/1000 | Loss: 0.00001432
Iteration 102/1000 | Loss: 0.00001432
Iteration 103/1000 | Loss: 0.00001432
Iteration 104/1000 | Loss: 0.00001432
Iteration 105/1000 | Loss: 0.00001432
Iteration 106/1000 | Loss: 0.00001432
Iteration 107/1000 | Loss: 0.00001432
Iteration 108/1000 | Loss: 0.00001432
Iteration 109/1000 | Loss: 0.00001432
Iteration 110/1000 | Loss: 0.00001432
Iteration 111/1000 | Loss: 0.00001432
Iteration 112/1000 | Loss: 0.00001432
Iteration 113/1000 | Loss: 0.00001432
Iteration 114/1000 | Loss: 0.00001431
Iteration 115/1000 | Loss: 0.00001431
Iteration 116/1000 | Loss: 0.00001431
Iteration 117/1000 | Loss: 0.00001431
Iteration 118/1000 | Loss: 0.00001430
Iteration 119/1000 | Loss: 0.00001430
Iteration 120/1000 | Loss: 0.00001430
Iteration 121/1000 | Loss: 0.00001430
Iteration 122/1000 | Loss: 0.00001430
Iteration 123/1000 | Loss: 0.00001430
Iteration 124/1000 | Loss: 0.00001430
Iteration 125/1000 | Loss: 0.00001430
Iteration 126/1000 | Loss: 0.00001430
Iteration 127/1000 | Loss: 0.00001430
Iteration 128/1000 | Loss: 0.00001429
Iteration 129/1000 | Loss: 0.00001429
Iteration 130/1000 | Loss: 0.00001429
Iteration 131/1000 | Loss: 0.00001429
Iteration 132/1000 | Loss: 0.00001429
Iteration 133/1000 | Loss: 0.00001429
Iteration 134/1000 | Loss: 0.00001429
Iteration 135/1000 | Loss: 0.00001429
Iteration 136/1000 | Loss: 0.00001429
Iteration 137/1000 | Loss: 0.00001429
Iteration 138/1000 | Loss: 0.00001428
Iteration 139/1000 | Loss: 0.00001428
Iteration 140/1000 | Loss: 0.00001428
Iteration 141/1000 | Loss: 0.00001428
Iteration 142/1000 | Loss: 0.00001428
Iteration 143/1000 | Loss: 0.00001428
Iteration 144/1000 | Loss: 0.00001428
Iteration 145/1000 | Loss: 0.00001427
Iteration 146/1000 | Loss: 0.00001427
Iteration 147/1000 | Loss: 0.00001427
Iteration 148/1000 | Loss: 0.00001427
Iteration 149/1000 | Loss: 0.00001427
Iteration 150/1000 | Loss: 0.00001427
Iteration 151/1000 | Loss: 0.00001427
Iteration 152/1000 | Loss: 0.00001427
Iteration 153/1000 | Loss: 0.00001427
Iteration 154/1000 | Loss: 0.00001427
Iteration 155/1000 | Loss: 0.00001427
Iteration 156/1000 | Loss: 0.00001427
Iteration 157/1000 | Loss: 0.00001427
Iteration 158/1000 | Loss: 0.00001427
Iteration 159/1000 | Loss: 0.00001427
Iteration 160/1000 | Loss: 0.00001427
Iteration 161/1000 | Loss: 0.00001427
Iteration 162/1000 | Loss: 0.00001427
Iteration 163/1000 | Loss: 0.00001427
Iteration 164/1000 | Loss: 0.00001427
Iteration 165/1000 | Loss: 0.00001427
Iteration 166/1000 | Loss: 0.00001427
Iteration 167/1000 | Loss: 0.00001427
Iteration 168/1000 | Loss: 0.00001427
Iteration 169/1000 | Loss: 0.00001427
Iteration 170/1000 | Loss: 0.00001427
Iteration 171/1000 | Loss: 0.00001427
Iteration 172/1000 | Loss: 0.00001427
Iteration 173/1000 | Loss: 0.00001427
Iteration 174/1000 | Loss: 0.00001427
Iteration 175/1000 | Loss: 0.00001427
Iteration 176/1000 | Loss: 0.00001427
Iteration 177/1000 | Loss: 0.00001427
Iteration 178/1000 | Loss: 0.00001427
Iteration 179/1000 | Loss: 0.00001427
Iteration 180/1000 | Loss: 0.00001427
Iteration 181/1000 | Loss: 0.00001427
Iteration 182/1000 | Loss: 0.00001427
Iteration 183/1000 | Loss: 0.00001427
Iteration 184/1000 | Loss: 0.00001427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.4270994142862037e-05, 1.4270994142862037e-05, 1.4270994142862037e-05, 1.4270994142862037e-05, 1.4270994142862037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4270994142862037e-05

Optimization complete. Final v2v error: 3.2295854091644287 mm

Highest mean error: 3.9030940532684326 mm for frame 61

Lowest mean error: 2.6837074756622314 mm for frame 119

Saving results

Total time: 34.96506214141846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898536
Iteration 2/25 | Loss: 0.00155185
Iteration 3/25 | Loss: 0.00077557
Iteration 4/25 | Loss: 0.00070297
Iteration 5/25 | Loss: 0.00065506
Iteration 6/25 | Loss: 0.00064343
Iteration 7/25 | Loss: 0.00064216
Iteration 8/25 | Loss: 0.00063891
Iteration 9/25 | Loss: 0.00063628
Iteration 10/25 | Loss: 0.00063633
Iteration 11/25 | Loss: 0.00063563
Iteration 12/25 | Loss: 0.00063492
Iteration 13/25 | Loss: 0.00063445
Iteration 14/25 | Loss: 0.00063425
Iteration 15/25 | Loss: 0.00063402
Iteration 16/25 | Loss: 0.00063360
Iteration 17/25 | Loss: 0.00063307
Iteration 18/25 | Loss: 0.00063278
Iteration 19/25 | Loss: 0.00063268
Iteration 20/25 | Loss: 0.00063266
Iteration 21/25 | Loss: 0.00063266
Iteration 22/25 | Loss: 0.00063266
Iteration 23/25 | Loss: 0.00063266
Iteration 24/25 | Loss: 0.00063265
Iteration 25/25 | Loss: 0.00063265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.23329735
Iteration 2/25 | Loss: 0.00016433
Iteration 3/25 | Loss: 0.00016433
Iteration 4/25 | Loss: 0.00016433
Iteration 5/25 | Loss: 0.00016433
Iteration 6/25 | Loss: 0.00016433
Iteration 7/25 | Loss: 0.00016433
Iteration 8/25 | Loss: 0.00016433
Iteration 9/25 | Loss: 0.00016433
Iteration 10/25 | Loss: 0.00016433
Iteration 11/25 | Loss: 0.00016433
Iteration 12/25 | Loss: 0.00016433
Iteration 13/25 | Loss: 0.00016433
Iteration 14/25 | Loss: 0.00016433
Iteration 15/25 | Loss: 0.00016433
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0001643252035137266, 0.0001643252035137266, 0.0001643252035137266, 0.0001643252035137266, 0.0001643252035137266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001643252035137266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00016433
Iteration 2/1000 | Loss: 0.00005290
Iteration 3/1000 | Loss: 0.00002438
Iteration 4/1000 | Loss: 0.00001702
Iteration 5/1000 | Loss: 0.00001607
Iteration 6/1000 | Loss: 0.00001533
Iteration 7/1000 | Loss: 0.00006613
Iteration 8/1000 | Loss: 0.00045742
Iteration 9/1000 | Loss: 0.00001503
Iteration 10/1000 | Loss: 0.00001464
Iteration 11/1000 | Loss: 0.00001454
Iteration 12/1000 | Loss: 0.00005907
Iteration 13/1000 | Loss: 0.00001429
Iteration 14/1000 | Loss: 0.00001425
Iteration 15/1000 | Loss: 0.00001425
Iteration 16/1000 | Loss: 0.00001424
Iteration 17/1000 | Loss: 0.00001423
Iteration 18/1000 | Loss: 0.00001419
Iteration 19/1000 | Loss: 0.00001418
Iteration 20/1000 | Loss: 0.00001415
Iteration 21/1000 | Loss: 0.00001414
Iteration 22/1000 | Loss: 0.00001408
Iteration 23/1000 | Loss: 0.00001408
Iteration 24/1000 | Loss: 0.00001407
Iteration 25/1000 | Loss: 0.00001406
Iteration 26/1000 | Loss: 0.00001405
Iteration 27/1000 | Loss: 0.00001405
Iteration 28/1000 | Loss: 0.00001404
Iteration 29/1000 | Loss: 0.00001404
Iteration 30/1000 | Loss: 0.00001403
Iteration 31/1000 | Loss: 0.00001399
Iteration 32/1000 | Loss: 0.00001399
Iteration 33/1000 | Loss: 0.00001398
Iteration 34/1000 | Loss: 0.00001398
Iteration 35/1000 | Loss: 0.00001398
Iteration 36/1000 | Loss: 0.00001398
Iteration 37/1000 | Loss: 0.00001397
Iteration 38/1000 | Loss: 0.00001397
Iteration 39/1000 | Loss: 0.00001397
Iteration 40/1000 | Loss: 0.00001397
Iteration 41/1000 | Loss: 0.00001397
Iteration 42/1000 | Loss: 0.00001396
Iteration 43/1000 | Loss: 0.00001396
Iteration 44/1000 | Loss: 0.00001396
Iteration 45/1000 | Loss: 0.00001395
Iteration 46/1000 | Loss: 0.00001395
Iteration 47/1000 | Loss: 0.00001394
Iteration 48/1000 | Loss: 0.00001394
Iteration 49/1000 | Loss: 0.00001394
Iteration 50/1000 | Loss: 0.00001394
Iteration 51/1000 | Loss: 0.00001393
Iteration 52/1000 | Loss: 0.00001393
Iteration 53/1000 | Loss: 0.00001393
Iteration 54/1000 | Loss: 0.00001392
Iteration 55/1000 | Loss: 0.00001389
Iteration 56/1000 | Loss: 0.00001389
Iteration 57/1000 | Loss: 0.00001388
Iteration 58/1000 | Loss: 0.00001387
Iteration 59/1000 | Loss: 0.00001387
Iteration 60/1000 | Loss: 0.00001386
Iteration 61/1000 | Loss: 0.00001386
Iteration 62/1000 | Loss: 0.00001385
Iteration 63/1000 | Loss: 0.00001383
Iteration 64/1000 | Loss: 0.00001383
Iteration 65/1000 | Loss: 0.00001382
Iteration 66/1000 | Loss: 0.00001382
Iteration 67/1000 | Loss: 0.00001382
Iteration 68/1000 | Loss: 0.00001382
Iteration 69/1000 | Loss: 0.00001381
Iteration 70/1000 | Loss: 0.00001381
Iteration 71/1000 | Loss: 0.00001380
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001379
Iteration 74/1000 | Loss: 0.00001379
Iteration 75/1000 | Loss: 0.00001379
Iteration 76/1000 | Loss: 0.00001378
Iteration 77/1000 | Loss: 0.00001378
Iteration 78/1000 | Loss: 0.00001377
Iteration 79/1000 | Loss: 0.00001377
Iteration 80/1000 | Loss: 0.00001377
Iteration 81/1000 | Loss: 0.00001377
Iteration 82/1000 | Loss: 0.00001377
Iteration 83/1000 | Loss: 0.00001377
Iteration 84/1000 | Loss: 0.00001377
Iteration 85/1000 | Loss: 0.00001377
Iteration 86/1000 | Loss: 0.00001377
Iteration 87/1000 | Loss: 0.00001377
Iteration 88/1000 | Loss: 0.00001377
Iteration 89/1000 | Loss: 0.00001376
Iteration 90/1000 | Loss: 0.00001376
Iteration 91/1000 | Loss: 0.00001376
Iteration 92/1000 | Loss: 0.00001376
Iteration 93/1000 | Loss: 0.00001375
Iteration 94/1000 | Loss: 0.00001375
Iteration 95/1000 | Loss: 0.00001375
Iteration 96/1000 | Loss: 0.00006411
Iteration 97/1000 | Loss: 0.00001377
Iteration 98/1000 | Loss: 0.00001371
Iteration 99/1000 | Loss: 0.00001371
Iteration 100/1000 | Loss: 0.00001371
Iteration 101/1000 | Loss: 0.00001371
Iteration 102/1000 | Loss: 0.00001370
Iteration 103/1000 | Loss: 0.00001370
Iteration 104/1000 | Loss: 0.00001370
Iteration 105/1000 | Loss: 0.00001370
Iteration 106/1000 | Loss: 0.00001370
Iteration 107/1000 | Loss: 0.00001370
Iteration 108/1000 | Loss: 0.00001370
Iteration 109/1000 | Loss: 0.00001370
Iteration 110/1000 | Loss: 0.00001370
Iteration 111/1000 | Loss: 0.00001370
Iteration 112/1000 | Loss: 0.00001370
Iteration 113/1000 | Loss: 0.00001369
Iteration 114/1000 | Loss: 0.00001369
Iteration 115/1000 | Loss: 0.00001369
Iteration 116/1000 | Loss: 0.00001369
Iteration 117/1000 | Loss: 0.00001369
Iteration 118/1000 | Loss: 0.00001369
Iteration 119/1000 | Loss: 0.00001369
Iteration 120/1000 | Loss: 0.00001369
Iteration 121/1000 | Loss: 0.00001369
Iteration 122/1000 | Loss: 0.00001369
Iteration 123/1000 | Loss: 0.00001369
Iteration 124/1000 | Loss: 0.00001369
Iteration 125/1000 | Loss: 0.00001369
Iteration 126/1000 | Loss: 0.00001369
Iteration 127/1000 | Loss: 0.00001369
Iteration 128/1000 | Loss: 0.00001369
Iteration 129/1000 | Loss: 0.00001369
Iteration 130/1000 | Loss: 0.00001369
Iteration 131/1000 | Loss: 0.00001369
Iteration 132/1000 | Loss: 0.00001369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.3687908904103097e-05, 1.3687908904103097e-05, 1.3687908904103097e-05, 1.3687908904103097e-05, 1.3687908904103097e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3687908904103097e-05

Optimization complete. Final v2v error: 3.122502565383911 mm

Highest mean error: 9.372882843017578 mm for frame 44

Lowest mean error: 2.6217563152313232 mm for frame 104

Saving results

Total time: 71.86124753952026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01126742
Iteration 2/25 | Loss: 0.00172747
Iteration 3/25 | Loss: 0.00108797
Iteration 4/25 | Loss: 0.00098176
Iteration 5/25 | Loss: 0.00098293
Iteration 6/25 | Loss: 0.00118563
Iteration 7/25 | Loss: 0.00105342
Iteration 8/25 | Loss: 0.00094633
Iteration 9/25 | Loss: 0.00098517
Iteration 10/25 | Loss: 0.00097628
Iteration 11/25 | Loss: 0.00088966
Iteration 12/25 | Loss: 0.00089627
Iteration 13/25 | Loss: 0.00088188
Iteration 14/25 | Loss: 0.00085668
Iteration 15/25 | Loss: 0.00085405
Iteration 16/25 | Loss: 0.00084002
Iteration 17/25 | Loss: 0.00090363
Iteration 18/25 | Loss: 0.00089913
Iteration 19/25 | Loss: 0.00088706
Iteration 20/25 | Loss: 0.00084566
Iteration 21/25 | Loss: 0.00083152
Iteration 22/25 | Loss: 0.00078893
Iteration 23/25 | Loss: 0.00078521
Iteration 24/25 | Loss: 0.00078458
Iteration 25/25 | Loss: 0.00078452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12734163
Iteration 2/25 | Loss: 0.00020829
Iteration 3/25 | Loss: 0.00020828
Iteration 4/25 | Loss: 0.00020828
Iteration 5/25 | Loss: 0.00020828
Iteration 6/25 | Loss: 0.00020828
Iteration 7/25 | Loss: 0.00020828
Iteration 8/25 | Loss: 0.00020828
Iteration 9/25 | Loss: 0.00020828
Iteration 10/25 | Loss: 0.00020828
Iteration 11/25 | Loss: 0.00020828
Iteration 12/25 | Loss: 0.00020828
Iteration 13/25 | Loss: 0.00020828
Iteration 14/25 | Loss: 0.00020828
Iteration 15/25 | Loss: 0.00020828
Iteration 16/25 | Loss: 0.00020828
Iteration 17/25 | Loss: 0.00020828
Iteration 18/25 | Loss: 0.00020828
Iteration 19/25 | Loss: 0.00020828
Iteration 20/25 | Loss: 0.00020828
Iteration 21/25 | Loss: 0.00020828
Iteration 22/25 | Loss: 0.00020828
Iteration 23/25 | Loss: 0.00020828
Iteration 24/25 | Loss: 0.00020828
Iteration 25/25 | Loss: 0.00020828

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020828
Iteration 2/1000 | Loss: 0.00209230
Iteration 3/1000 | Loss: 0.00005314
Iteration 4/1000 | Loss: 0.00004237
Iteration 5/1000 | Loss: 0.00003762
Iteration 6/1000 | Loss: 0.00266392
Iteration 7/1000 | Loss: 0.00021551
Iteration 8/1000 | Loss: 0.00151007
Iteration 9/1000 | Loss: 0.00017741
Iteration 10/1000 | Loss: 0.00225003
Iteration 11/1000 | Loss: 0.00281814
Iteration 12/1000 | Loss: 0.00199898
Iteration 13/1000 | Loss: 0.00105503
Iteration 14/1000 | Loss: 0.00011645
Iteration 15/1000 | Loss: 0.00093414
Iteration 16/1000 | Loss: 0.00006042
Iteration 17/1000 | Loss: 0.00004654
Iteration 18/1000 | Loss: 0.00004252
Iteration 19/1000 | Loss: 0.00003882
Iteration 20/1000 | Loss: 0.00015731
Iteration 21/1000 | Loss: 0.00003915
Iteration 22/1000 | Loss: 0.00009351
Iteration 23/1000 | Loss: 0.00003563
Iteration 24/1000 | Loss: 0.00003486
Iteration 25/1000 | Loss: 0.00013882
Iteration 26/1000 | Loss: 0.00058520
Iteration 27/1000 | Loss: 0.00057398
Iteration 28/1000 | Loss: 0.00073092
Iteration 29/1000 | Loss: 0.00052981
Iteration 30/1000 | Loss: 0.00017007
Iteration 31/1000 | Loss: 0.00043232
Iteration 32/1000 | Loss: 0.00013397
Iteration 33/1000 | Loss: 0.00039599
Iteration 34/1000 | Loss: 0.00049498
Iteration 35/1000 | Loss: 0.00019235
Iteration 36/1000 | Loss: 0.00046370
Iteration 37/1000 | Loss: 0.00029683
Iteration 38/1000 | Loss: 0.00028969
Iteration 39/1000 | Loss: 0.00032615
Iteration 40/1000 | Loss: 0.00023618
Iteration 41/1000 | Loss: 0.00004655
Iteration 42/1000 | Loss: 0.00003630
Iteration 43/1000 | Loss: 0.00091589
Iteration 44/1000 | Loss: 0.00004671
Iteration 45/1000 | Loss: 0.00003599
Iteration 46/1000 | Loss: 0.00003364
Iteration 47/1000 | Loss: 0.00003566
Iteration 48/1000 | Loss: 0.00003241
Iteration 49/1000 | Loss: 0.00003071
Iteration 50/1000 | Loss: 0.00002964
Iteration 51/1000 | Loss: 0.00002901
Iteration 52/1000 | Loss: 0.00002862
Iteration 53/1000 | Loss: 0.00002852
Iteration 54/1000 | Loss: 0.00002812
Iteration 55/1000 | Loss: 0.00002768
Iteration 56/1000 | Loss: 0.00002730
Iteration 57/1000 | Loss: 0.00002722
Iteration 58/1000 | Loss: 0.00002697
Iteration 59/1000 | Loss: 0.00002681
Iteration 60/1000 | Loss: 0.00002676
Iteration 61/1000 | Loss: 0.00002675
Iteration 62/1000 | Loss: 0.00002674
Iteration 63/1000 | Loss: 0.00002673
Iteration 64/1000 | Loss: 0.00002672
Iteration 65/1000 | Loss: 0.00002671
Iteration 66/1000 | Loss: 0.00002670
Iteration 67/1000 | Loss: 0.00002670
Iteration 68/1000 | Loss: 0.00002669
Iteration 69/1000 | Loss: 0.00002669
Iteration 70/1000 | Loss: 0.00002669
Iteration 71/1000 | Loss: 0.00002668
Iteration 72/1000 | Loss: 0.00002668
Iteration 73/1000 | Loss: 0.00002668
Iteration 74/1000 | Loss: 0.00002667
Iteration 75/1000 | Loss: 0.00002667
Iteration 76/1000 | Loss: 0.00002667
Iteration 77/1000 | Loss: 0.00002666
Iteration 78/1000 | Loss: 0.00002666
Iteration 79/1000 | Loss: 0.00002666
Iteration 80/1000 | Loss: 0.00002665
Iteration 81/1000 | Loss: 0.00002665
Iteration 82/1000 | Loss: 0.00002664
Iteration 83/1000 | Loss: 0.00002664
Iteration 84/1000 | Loss: 0.00002664
Iteration 85/1000 | Loss: 0.00002664
Iteration 86/1000 | Loss: 0.00002663
Iteration 87/1000 | Loss: 0.00002663
Iteration 88/1000 | Loss: 0.00002663
Iteration 89/1000 | Loss: 0.00002663
Iteration 90/1000 | Loss: 0.00002663
Iteration 91/1000 | Loss: 0.00002663
Iteration 92/1000 | Loss: 0.00002663
Iteration 93/1000 | Loss: 0.00002663
Iteration 94/1000 | Loss: 0.00002663
Iteration 95/1000 | Loss: 0.00002663
Iteration 96/1000 | Loss: 0.00002663
Iteration 97/1000 | Loss: 0.00002663
Iteration 98/1000 | Loss: 0.00002663
Iteration 99/1000 | Loss: 0.00002663
Iteration 100/1000 | Loss: 0.00002663
Iteration 101/1000 | Loss: 0.00002663
Iteration 102/1000 | Loss: 0.00002663
Iteration 103/1000 | Loss: 0.00002663
Iteration 104/1000 | Loss: 0.00002663
Iteration 105/1000 | Loss: 0.00002663
Iteration 106/1000 | Loss: 0.00002663
Iteration 107/1000 | Loss: 0.00002663
Iteration 108/1000 | Loss: 0.00002663
Iteration 109/1000 | Loss: 0.00002663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.6626941689755768e-05, 2.6626941689755768e-05, 2.6626941689755768e-05, 2.6626941689755768e-05, 2.6626941689755768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6626941689755768e-05

Optimization complete. Final v2v error: 4.129548072814941 mm

Highest mean error: 5.937280178070068 mm for frame 72

Lowest mean error: 3.5856258869171143 mm for frame 100

Saving results

Total time: 128.5317187309265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035156
Iteration 2/25 | Loss: 0.00196352
Iteration 3/25 | Loss: 0.00132573
Iteration 4/25 | Loss: 0.00115045
Iteration 5/25 | Loss: 0.00115483
Iteration 6/25 | Loss: 0.00125800
Iteration 7/25 | Loss: 0.00111710
Iteration 8/25 | Loss: 0.00099803
Iteration 9/25 | Loss: 0.00088162
Iteration 10/25 | Loss: 0.00080940
Iteration 11/25 | Loss: 0.00075693
Iteration 12/25 | Loss: 0.00072880
Iteration 13/25 | Loss: 0.00071069
Iteration 14/25 | Loss: 0.00070712
Iteration 15/25 | Loss: 0.00070757
Iteration 16/25 | Loss: 0.00070521
Iteration 17/25 | Loss: 0.00070221
Iteration 18/25 | Loss: 0.00070330
Iteration 19/25 | Loss: 0.00069999
Iteration 20/25 | Loss: 0.00069825
Iteration 21/25 | Loss: 0.00069682
Iteration 22/25 | Loss: 0.00069524
Iteration 23/25 | Loss: 0.00069583
Iteration 24/25 | Loss: 0.00068828
Iteration 25/25 | Loss: 0.00068600

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45991659
Iteration 2/25 | Loss: 0.00012704
Iteration 3/25 | Loss: 0.00012704
Iteration 4/25 | Loss: 0.00012704
Iteration 5/25 | Loss: 0.00012703
Iteration 6/25 | Loss: 0.00012703
Iteration 7/25 | Loss: 0.00012703
Iteration 8/25 | Loss: 0.00012703
Iteration 9/25 | Loss: 0.00012703
Iteration 10/25 | Loss: 0.00012703
Iteration 11/25 | Loss: 0.00012703
Iteration 12/25 | Loss: 0.00012703
Iteration 13/25 | Loss: 0.00012703
Iteration 14/25 | Loss: 0.00012703
Iteration 15/25 | Loss: 0.00012703
Iteration 16/25 | Loss: 0.00012703
Iteration 17/25 | Loss: 0.00012703
Iteration 18/25 | Loss: 0.00012703
Iteration 19/25 | Loss: 0.00012703
Iteration 20/25 | Loss: 0.00012703
Iteration 21/25 | Loss: 0.00012703
Iteration 22/25 | Loss: 0.00012703
Iteration 23/25 | Loss: 0.00012703
Iteration 24/25 | Loss: 0.00012703
Iteration 25/25 | Loss: 0.00012703

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00012703
Iteration 2/1000 | Loss: 0.00014295
Iteration 3/1000 | Loss: 0.00009506
Iteration 4/1000 | Loss: 0.00002833
Iteration 5/1000 | Loss: 0.00002664
Iteration 6/1000 | Loss: 0.00002459
Iteration 7/1000 | Loss: 0.00010599
Iteration 8/1000 | Loss: 0.00004246
Iteration 9/1000 | Loss: 0.00009503
Iteration 10/1000 | Loss: 0.00005807
Iteration 11/1000 | Loss: 0.00008670
Iteration 12/1000 | Loss: 0.00002385
Iteration 13/1000 | Loss: 0.00002268
Iteration 14/1000 | Loss: 0.00002196
Iteration 15/1000 | Loss: 0.00002165
Iteration 16/1000 | Loss: 0.00002125
Iteration 17/1000 | Loss: 0.00002077
Iteration 18/1000 | Loss: 0.00002038
Iteration 19/1000 | Loss: 0.00002012
Iteration 20/1000 | Loss: 0.00002675
Iteration 21/1000 | Loss: 0.00002458
Iteration 22/1000 | Loss: 0.00002458
Iteration 23/1000 | Loss: 0.00002019
Iteration 24/1000 | Loss: 0.00001964
Iteration 25/1000 | Loss: 0.00001940
Iteration 26/1000 | Loss: 0.00001931
Iteration 27/1000 | Loss: 0.00001929
Iteration 28/1000 | Loss: 0.00001928
Iteration 29/1000 | Loss: 0.00001928
Iteration 30/1000 | Loss: 0.00001927
Iteration 31/1000 | Loss: 0.00001925
Iteration 32/1000 | Loss: 0.00001925
Iteration 33/1000 | Loss: 0.00001924
Iteration 34/1000 | Loss: 0.00001924
Iteration 35/1000 | Loss: 0.00001924
Iteration 36/1000 | Loss: 0.00001924
Iteration 37/1000 | Loss: 0.00001924
Iteration 38/1000 | Loss: 0.00001924
Iteration 39/1000 | Loss: 0.00001924
Iteration 40/1000 | Loss: 0.00001923
Iteration 41/1000 | Loss: 0.00001921
Iteration 42/1000 | Loss: 0.00001921
Iteration 43/1000 | Loss: 0.00001920
Iteration 44/1000 | Loss: 0.00001920
Iteration 45/1000 | Loss: 0.00001919
Iteration 46/1000 | Loss: 0.00001917
Iteration 47/1000 | Loss: 0.00001916
Iteration 48/1000 | Loss: 0.00001916
Iteration 49/1000 | Loss: 0.00001915
Iteration 50/1000 | Loss: 0.00001912
Iteration 51/1000 | Loss: 0.00001912
Iteration 52/1000 | Loss: 0.00001912
Iteration 53/1000 | Loss: 0.00001912
Iteration 54/1000 | Loss: 0.00001912
Iteration 55/1000 | Loss: 0.00001912
Iteration 56/1000 | Loss: 0.00001912
Iteration 57/1000 | Loss: 0.00001912
Iteration 58/1000 | Loss: 0.00001912
Iteration 59/1000 | Loss: 0.00001911
Iteration 60/1000 | Loss: 0.00001911
Iteration 61/1000 | Loss: 0.00001911
Iteration 62/1000 | Loss: 0.00001910
Iteration 63/1000 | Loss: 0.00001910
Iteration 64/1000 | Loss: 0.00001909
Iteration 65/1000 | Loss: 0.00001909
Iteration 66/1000 | Loss: 0.00001909
Iteration 67/1000 | Loss: 0.00001909
Iteration 68/1000 | Loss: 0.00001909
Iteration 69/1000 | Loss: 0.00001908
Iteration 70/1000 | Loss: 0.00001908
Iteration 71/1000 | Loss: 0.00001908
Iteration 72/1000 | Loss: 0.00001908
Iteration 73/1000 | Loss: 0.00001908
Iteration 74/1000 | Loss: 0.00001908
Iteration 75/1000 | Loss: 0.00001908
Iteration 76/1000 | Loss: 0.00001908
Iteration 77/1000 | Loss: 0.00001908
Iteration 78/1000 | Loss: 0.00001908
Iteration 79/1000 | Loss: 0.00001908
Iteration 80/1000 | Loss: 0.00001908
Iteration 81/1000 | Loss: 0.00001908
Iteration 82/1000 | Loss: 0.00001908
Iteration 83/1000 | Loss: 0.00001907
Iteration 84/1000 | Loss: 0.00001907
Iteration 85/1000 | Loss: 0.00001907
Iteration 86/1000 | Loss: 0.00001907
Iteration 87/1000 | Loss: 0.00001907
Iteration 88/1000 | Loss: 0.00001907
Iteration 89/1000 | Loss: 0.00001907
Iteration 90/1000 | Loss: 0.00001906
Iteration 91/1000 | Loss: 0.00001906
Iteration 92/1000 | Loss: 0.00001906
Iteration 93/1000 | Loss: 0.00001906
Iteration 94/1000 | Loss: 0.00001906
Iteration 95/1000 | Loss: 0.00001906
Iteration 96/1000 | Loss: 0.00001906
Iteration 97/1000 | Loss: 0.00001906
Iteration 98/1000 | Loss: 0.00001906
Iteration 99/1000 | Loss: 0.00001906
Iteration 100/1000 | Loss: 0.00001906
Iteration 101/1000 | Loss: 0.00001906
Iteration 102/1000 | Loss: 0.00001905
Iteration 103/1000 | Loss: 0.00001905
Iteration 104/1000 | Loss: 0.00001905
Iteration 105/1000 | Loss: 0.00001905
Iteration 106/1000 | Loss: 0.00001905
Iteration 107/1000 | Loss: 0.00001905
Iteration 108/1000 | Loss: 0.00001905
Iteration 109/1000 | Loss: 0.00001905
Iteration 110/1000 | Loss: 0.00001905
Iteration 111/1000 | Loss: 0.00001905
Iteration 112/1000 | Loss: 0.00001905
Iteration 113/1000 | Loss: 0.00001905
Iteration 114/1000 | Loss: 0.00001905
Iteration 115/1000 | Loss: 0.00001905
Iteration 116/1000 | Loss: 0.00001905
Iteration 117/1000 | Loss: 0.00001905
Iteration 118/1000 | Loss: 0.00001905
Iteration 119/1000 | Loss: 0.00001904
Iteration 120/1000 | Loss: 0.00001904
Iteration 121/1000 | Loss: 0.00001904
Iteration 122/1000 | Loss: 0.00001904
Iteration 123/1000 | Loss: 0.00001904
Iteration 124/1000 | Loss: 0.00001904
Iteration 125/1000 | Loss: 0.00001904
Iteration 126/1000 | Loss: 0.00001904
Iteration 127/1000 | Loss: 0.00001904
Iteration 128/1000 | Loss: 0.00001904
Iteration 129/1000 | Loss: 0.00001904
Iteration 130/1000 | Loss: 0.00001904
Iteration 131/1000 | Loss: 0.00001903
Iteration 132/1000 | Loss: 0.00001903
Iteration 133/1000 | Loss: 0.00001903
Iteration 134/1000 | Loss: 0.00001903
Iteration 135/1000 | Loss: 0.00001903
Iteration 136/1000 | Loss: 0.00001903
Iteration 137/1000 | Loss: 0.00001903
Iteration 138/1000 | Loss: 0.00001903
Iteration 139/1000 | Loss: 0.00001903
Iteration 140/1000 | Loss: 0.00001903
Iteration 141/1000 | Loss: 0.00001903
Iteration 142/1000 | Loss: 0.00001903
Iteration 143/1000 | Loss: 0.00001903
Iteration 144/1000 | Loss: 0.00001903
Iteration 145/1000 | Loss: 0.00001903
Iteration 146/1000 | Loss: 0.00001903
Iteration 147/1000 | Loss: 0.00001903
Iteration 148/1000 | Loss: 0.00001903
Iteration 149/1000 | Loss: 0.00001903
Iteration 150/1000 | Loss: 0.00001902
Iteration 151/1000 | Loss: 0.00001902
Iteration 152/1000 | Loss: 0.00001902
Iteration 153/1000 | Loss: 0.00001902
Iteration 154/1000 | Loss: 0.00001902
Iteration 155/1000 | Loss: 0.00001902
Iteration 156/1000 | Loss: 0.00001902
Iteration 157/1000 | Loss: 0.00001902
Iteration 158/1000 | Loss: 0.00001902
Iteration 159/1000 | Loss: 0.00001902
Iteration 160/1000 | Loss: 0.00001902
Iteration 161/1000 | Loss: 0.00001902
Iteration 162/1000 | Loss: 0.00001902
Iteration 163/1000 | Loss: 0.00001902
Iteration 164/1000 | Loss: 0.00001902
Iteration 165/1000 | Loss: 0.00001902
Iteration 166/1000 | Loss: 0.00001901
Iteration 167/1000 | Loss: 0.00001901
Iteration 168/1000 | Loss: 0.00001901
Iteration 169/1000 | Loss: 0.00001901
Iteration 170/1000 | Loss: 0.00001901
Iteration 171/1000 | Loss: 0.00001901
Iteration 172/1000 | Loss: 0.00001901
Iteration 173/1000 | Loss: 0.00001901
Iteration 174/1000 | Loss: 0.00001901
Iteration 175/1000 | Loss: 0.00001901
Iteration 176/1000 | Loss: 0.00001901
Iteration 177/1000 | Loss: 0.00001901
Iteration 178/1000 | Loss: 0.00001901
Iteration 179/1000 | Loss: 0.00001901
Iteration 180/1000 | Loss: 0.00001901
Iteration 181/1000 | Loss: 0.00001901
Iteration 182/1000 | Loss: 0.00001901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.9013063138118014e-05, 1.9013063138118014e-05, 1.9013063138118014e-05, 1.9013063138118014e-05, 1.9013063138118014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9013063138118014e-05

Optimization complete. Final v2v error: 3.660341262817383 mm

Highest mean error: 8.577364921569824 mm for frame 42

Lowest mean error: 3.1572682857513428 mm for frame 74

Saving results

Total time: 92.23444771766663
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386961
Iteration 2/25 | Loss: 0.00069161
Iteration 3/25 | Loss: 0.00059673
Iteration 4/25 | Loss: 0.00058012
Iteration 5/25 | Loss: 0.00057377
Iteration 6/25 | Loss: 0.00057238
Iteration 7/25 | Loss: 0.00057232
Iteration 8/25 | Loss: 0.00057232
Iteration 9/25 | Loss: 0.00057232
Iteration 10/25 | Loss: 0.00057232
Iteration 11/25 | Loss: 0.00057232
Iteration 12/25 | Loss: 0.00057232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005723241483792663, 0.0005723241483792663, 0.0005723241483792663, 0.0005723241483792663, 0.0005723241483792663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005723241483792663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83874261
Iteration 2/25 | Loss: 0.00011137
Iteration 3/25 | Loss: 0.00011137
Iteration 4/25 | Loss: 0.00011137
Iteration 5/25 | Loss: 0.00011137
Iteration 6/25 | Loss: 0.00011137
Iteration 7/25 | Loss: 0.00011137
Iteration 8/25 | Loss: 0.00011137
Iteration 9/25 | Loss: 0.00011137
Iteration 10/25 | Loss: 0.00011137
Iteration 11/25 | Loss: 0.00011137
Iteration 12/25 | Loss: 0.00011137
Iteration 13/25 | Loss: 0.00011137
Iteration 14/25 | Loss: 0.00011137
Iteration 15/25 | Loss: 0.00011137
Iteration 16/25 | Loss: 0.00011137
Iteration 17/25 | Loss: 0.00011137
Iteration 18/25 | Loss: 0.00011137
Iteration 19/25 | Loss: 0.00011137
Iteration 20/25 | Loss: 0.00011137
Iteration 21/25 | Loss: 0.00011137
Iteration 22/25 | Loss: 0.00011137
Iteration 23/25 | Loss: 0.00011137
Iteration 24/25 | Loss: 0.00011137
Iteration 25/25 | Loss: 0.00011137

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00011137
Iteration 2/1000 | Loss: 0.00001706
Iteration 3/1000 | Loss: 0.00001358
Iteration 4/1000 | Loss: 0.00001261
Iteration 5/1000 | Loss: 0.00001196
Iteration 6/1000 | Loss: 0.00001171
Iteration 7/1000 | Loss: 0.00001146
Iteration 8/1000 | Loss: 0.00001129
Iteration 9/1000 | Loss: 0.00001128
Iteration 10/1000 | Loss: 0.00001128
Iteration 11/1000 | Loss: 0.00001127
Iteration 12/1000 | Loss: 0.00001122
Iteration 13/1000 | Loss: 0.00001121
Iteration 14/1000 | Loss: 0.00001120
Iteration 15/1000 | Loss: 0.00001118
Iteration 16/1000 | Loss: 0.00001114
Iteration 17/1000 | Loss: 0.00001112
Iteration 18/1000 | Loss: 0.00001112
Iteration 19/1000 | Loss: 0.00001112
Iteration 20/1000 | Loss: 0.00001111
Iteration 21/1000 | Loss: 0.00001111
Iteration 22/1000 | Loss: 0.00001110
Iteration 23/1000 | Loss: 0.00001110
Iteration 24/1000 | Loss: 0.00001110
Iteration 25/1000 | Loss: 0.00001108
Iteration 26/1000 | Loss: 0.00001108
Iteration 27/1000 | Loss: 0.00001108
Iteration 28/1000 | Loss: 0.00001106
Iteration 29/1000 | Loss: 0.00001106
Iteration 30/1000 | Loss: 0.00001106
Iteration 31/1000 | Loss: 0.00001104
Iteration 32/1000 | Loss: 0.00001104
Iteration 33/1000 | Loss: 0.00001104
Iteration 34/1000 | Loss: 0.00001103
Iteration 35/1000 | Loss: 0.00001103
Iteration 36/1000 | Loss: 0.00001103
Iteration 37/1000 | Loss: 0.00001102
Iteration 38/1000 | Loss: 0.00001100
Iteration 39/1000 | Loss: 0.00001100
Iteration 40/1000 | Loss: 0.00001099
Iteration 41/1000 | Loss: 0.00001099
Iteration 42/1000 | Loss: 0.00001099
Iteration 43/1000 | Loss: 0.00001098
Iteration 44/1000 | Loss: 0.00001096
Iteration 45/1000 | Loss: 0.00001096
Iteration 46/1000 | Loss: 0.00001096
Iteration 47/1000 | Loss: 0.00001096
Iteration 48/1000 | Loss: 0.00001096
Iteration 49/1000 | Loss: 0.00001096
Iteration 50/1000 | Loss: 0.00001096
Iteration 51/1000 | Loss: 0.00001096
Iteration 52/1000 | Loss: 0.00001096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 52. Stopping optimization.
Last 5 losses: [1.0956707228615414e-05, 1.0956707228615414e-05, 1.0956707228615414e-05, 1.0956707228615414e-05, 1.0956707228615414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0956707228615414e-05

Optimization complete. Final v2v error: 2.8271539211273193 mm

Highest mean error: 3.264549732208252 mm for frame 88

Lowest mean error: 2.614032506942749 mm for frame 160

Saving results

Total time: 28.212417125701904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480552
Iteration 2/25 | Loss: 0.00102313
Iteration 3/25 | Loss: 0.00069321
Iteration 4/25 | Loss: 0.00065121
Iteration 5/25 | Loss: 0.00064045
Iteration 6/25 | Loss: 0.00063876
Iteration 7/25 | Loss: 0.00063876
Iteration 8/25 | Loss: 0.00063876
Iteration 9/25 | Loss: 0.00063876
Iteration 10/25 | Loss: 0.00063876
Iteration 11/25 | Loss: 0.00063876
Iteration 12/25 | Loss: 0.00063876
Iteration 13/25 | Loss: 0.00063876
Iteration 14/25 | Loss: 0.00063876
Iteration 15/25 | Loss: 0.00063876
Iteration 16/25 | Loss: 0.00063876
Iteration 17/25 | Loss: 0.00063876
Iteration 18/25 | Loss: 0.00063876
Iteration 19/25 | Loss: 0.00063876
Iteration 20/25 | Loss: 0.00063876
Iteration 21/25 | Loss: 0.00063876
Iteration 22/25 | Loss: 0.00063876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000638763012830168, 0.000638763012830168, 0.000638763012830168, 0.000638763012830168, 0.000638763012830168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000638763012830168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83050776
Iteration 2/25 | Loss: 0.00013813
Iteration 3/25 | Loss: 0.00013813
Iteration 4/25 | Loss: 0.00013813
Iteration 5/25 | Loss: 0.00013813
Iteration 6/25 | Loss: 0.00013813
Iteration 7/25 | Loss: 0.00013813
Iteration 8/25 | Loss: 0.00013813
Iteration 9/25 | Loss: 0.00013813
Iteration 10/25 | Loss: 0.00013813
Iteration 11/25 | Loss: 0.00013813
Iteration 12/25 | Loss: 0.00013813
Iteration 13/25 | Loss: 0.00013813
Iteration 14/25 | Loss: 0.00013813
Iteration 15/25 | Loss: 0.00013813
Iteration 16/25 | Loss: 0.00013813
Iteration 17/25 | Loss: 0.00013813
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0001381307956762612, 0.0001381307956762612, 0.0001381307956762612, 0.0001381307956762612, 0.0001381307956762612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001381307956762612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00013813
Iteration 2/1000 | Loss: 0.00002273
Iteration 3/1000 | Loss: 0.00001897
Iteration 4/1000 | Loss: 0.00001823
Iteration 5/1000 | Loss: 0.00001756
Iteration 6/1000 | Loss: 0.00001691
Iteration 7/1000 | Loss: 0.00001661
Iteration 8/1000 | Loss: 0.00001620
Iteration 9/1000 | Loss: 0.00001601
Iteration 10/1000 | Loss: 0.00001592
Iteration 11/1000 | Loss: 0.00001592
Iteration 12/1000 | Loss: 0.00001579
Iteration 13/1000 | Loss: 0.00001579
Iteration 14/1000 | Loss: 0.00001578
Iteration 15/1000 | Loss: 0.00001572
Iteration 16/1000 | Loss: 0.00001566
Iteration 17/1000 | Loss: 0.00001564
Iteration 18/1000 | Loss: 0.00001564
Iteration 19/1000 | Loss: 0.00001563
Iteration 20/1000 | Loss: 0.00001561
Iteration 21/1000 | Loss: 0.00001560
Iteration 22/1000 | Loss: 0.00001558
Iteration 23/1000 | Loss: 0.00001557
Iteration 24/1000 | Loss: 0.00001557
Iteration 25/1000 | Loss: 0.00001556
Iteration 26/1000 | Loss: 0.00001549
Iteration 27/1000 | Loss: 0.00001549
Iteration 28/1000 | Loss: 0.00001547
Iteration 29/1000 | Loss: 0.00001547
Iteration 30/1000 | Loss: 0.00001547
Iteration 31/1000 | Loss: 0.00001546
Iteration 32/1000 | Loss: 0.00001546
Iteration 33/1000 | Loss: 0.00001546
Iteration 34/1000 | Loss: 0.00001545
Iteration 35/1000 | Loss: 0.00001544
Iteration 36/1000 | Loss: 0.00001544
Iteration 37/1000 | Loss: 0.00001544
Iteration 38/1000 | Loss: 0.00001543
Iteration 39/1000 | Loss: 0.00001543
Iteration 40/1000 | Loss: 0.00001542
Iteration 41/1000 | Loss: 0.00001542
Iteration 42/1000 | Loss: 0.00001541
Iteration 43/1000 | Loss: 0.00001540
Iteration 44/1000 | Loss: 0.00001540
Iteration 45/1000 | Loss: 0.00001540
Iteration 46/1000 | Loss: 0.00001539
Iteration 47/1000 | Loss: 0.00001539
Iteration 48/1000 | Loss: 0.00001539
Iteration 49/1000 | Loss: 0.00001538
Iteration 50/1000 | Loss: 0.00001538
Iteration 51/1000 | Loss: 0.00001538
Iteration 52/1000 | Loss: 0.00001538
Iteration 53/1000 | Loss: 0.00001537
Iteration 54/1000 | Loss: 0.00001537
Iteration 55/1000 | Loss: 0.00001537
Iteration 56/1000 | Loss: 0.00001537
Iteration 57/1000 | Loss: 0.00001537
Iteration 58/1000 | Loss: 0.00001536
Iteration 59/1000 | Loss: 0.00001536
Iteration 60/1000 | Loss: 0.00001536
Iteration 61/1000 | Loss: 0.00001535
Iteration 62/1000 | Loss: 0.00001534
Iteration 63/1000 | Loss: 0.00001534
Iteration 64/1000 | Loss: 0.00001533
Iteration 65/1000 | Loss: 0.00001533
Iteration 66/1000 | Loss: 0.00001532
Iteration 67/1000 | Loss: 0.00001531
Iteration 68/1000 | Loss: 0.00001531
Iteration 69/1000 | Loss: 0.00001531
Iteration 70/1000 | Loss: 0.00001531
Iteration 71/1000 | Loss: 0.00001531
Iteration 72/1000 | Loss: 0.00001530
Iteration 73/1000 | Loss: 0.00001530
Iteration 74/1000 | Loss: 0.00001530
Iteration 75/1000 | Loss: 0.00001529
Iteration 76/1000 | Loss: 0.00001529
Iteration 77/1000 | Loss: 0.00001529
Iteration 78/1000 | Loss: 0.00001529
Iteration 79/1000 | Loss: 0.00001529
Iteration 80/1000 | Loss: 0.00001529
Iteration 81/1000 | Loss: 0.00001528
Iteration 82/1000 | Loss: 0.00001528
Iteration 83/1000 | Loss: 0.00001528
Iteration 84/1000 | Loss: 0.00001528
Iteration 85/1000 | Loss: 0.00001528
Iteration 86/1000 | Loss: 0.00001528
Iteration 87/1000 | Loss: 0.00001528
Iteration 88/1000 | Loss: 0.00001528
Iteration 89/1000 | Loss: 0.00001528
Iteration 90/1000 | Loss: 0.00001527
Iteration 91/1000 | Loss: 0.00001527
Iteration 92/1000 | Loss: 0.00001527
Iteration 93/1000 | Loss: 0.00001527
Iteration 94/1000 | Loss: 0.00001527
Iteration 95/1000 | Loss: 0.00001526
Iteration 96/1000 | Loss: 0.00001526
Iteration 97/1000 | Loss: 0.00001525
Iteration 98/1000 | Loss: 0.00001525
Iteration 99/1000 | Loss: 0.00001525
Iteration 100/1000 | Loss: 0.00001525
Iteration 101/1000 | Loss: 0.00001525
Iteration 102/1000 | Loss: 0.00001525
Iteration 103/1000 | Loss: 0.00001525
Iteration 104/1000 | Loss: 0.00001525
Iteration 105/1000 | Loss: 0.00001525
Iteration 106/1000 | Loss: 0.00001525
Iteration 107/1000 | Loss: 0.00001525
Iteration 108/1000 | Loss: 0.00001525
Iteration 109/1000 | Loss: 0.00001525
Iteration 110/1000 | Loss: 0.00001524
Iteration 111/1000 | Loss: 0.00001524
Iteration 112/1000 | Loss: 0.00001524
Iteration 113/1000 | Loss: 0.00001524
Iteration 114/1000 | Loss: 0.00001524
Iteration 115/1000 | Loss: 0.00001524
Iteration 116/1000 | Loss: 0.00001524
Iteration 117/1000 | Loss: 0.00001524
Iteration 118/1000 | Loss: 0.00001524
Iteration 119/1000 | Loss: 0.00001524
Iteration 120/1000 | Loss: 0.00001524
Iteration 121/1000 | Loss: 0.00001524
Iteration 122/1000 | Loss: 0.00001524
Iteration 123/1000 | Loss: 0.00001524
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.5242391782521736e-05, 1.5242391782521736e-05, 1.5242391782521736e-05, 1.5242391782521736e-05, 1.5242391782521736e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5242391782521736e-05

Optimization complete. Final v2v error: 3.3678548336029053 mm

Highest mean error: 3.67248272895813 mm for frame 68

Lowest mean error: 3.06467866897583 mm for frame 24

Saving results

Total time: 41.51859426498413
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917488
Iteration 2/25 | Loss: 0.00127686
Iteration 3/25 | Loss: 0.00091792
Iteration 4/25 | Loss: 0.00081498
Iteration 5/25 | Loss: 0.00079362
Iteration 6/25 | Loss: 0.00079502
Iteration 7/25 | Loss: 0.00079244
Iteration 8/25 | Loss: 0.00077381
Iteration 9/25 | Loss: 0.00076515
Iteration 10/25 | Loss: 0.00075647
Iteration 11/25 | Loss: 0.00074591
Iteration 12/25 | Loss: 0.00074271
Iteration 13/25 | Loss: 0.00073774
Iteration 14/25 | Loss: 0.00073456
Iteration 15/25 | Loss: 0.00072710
Iteration 16/25 | Loss: 0.00072707
Iteration 17/25 | Loss: 0.00072522
Iteration 18/25 | Loss: 0.00072449
Iteration 19/25 | Loss: 0.00072228
Iteration 20/25 | Loss: 0.00072058
Iteration 21/25 | Loss: 0.00071987
Iteration 22/25 | Loss: 0.00071949
Iteration 23/25 | Loss: 0.00071929
Iteration 24/25 | Loss: 0.00071903
Iteration 25/25 | Loss: 0.00072290

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.31402445
Iteration 2/25 | Loss: 0.00046412
Iteration 3/25 | Loss: 0.00046412
Iteration 4/25 | Loss: 0.00046412
Iteration 5/25 | Loss: 0.00046412
Iteration 6/25 | Loss: 0.00046412
Iteration 7/25 | Loss: 0.00046412
Iteration 8/25 | Loss: 0.00046412
Iteration 9/25 | Loss: 0.00046412
Iteration 10/25 | Loss: 0.00046412
Iteration 11/25 | Loss: 0.00046412
Iteration 12/25 | Loss: 0.00046412
Iteration 13/25 | Loss: 0.00046412
Iteration 14/25 | Loss: 0.00046412
Iteration 15/25 | Loss: 0.00046412
Iteration 16/25 | Loss: 0.00046412
Iteration 17/25 | Loss: 0.00046412
Iteration 18/25 | Loss: 0.00046412
Iteration 19/25 | Loss: 0.00046412
Iteration 20/25 | Loss: 0.00046412
Iteration 21/25 | Loss: 0.00046412
Iteration 22/25 | Loss: 0.00046412
Iteration 23/25 | Loss: 0.00046412
Iteration 24/25 | Loss: 0.00046412
Iteration 25/25 | Loss: 0.00046412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046412
Iteration 2/1000 | Loss: 0.00048313
Iteration 3/1000 | Loss: 0.00043114
Iteration 4/1000 | Loss: 0.00009688
Iteration 5/1000 | Loss: 0.00009866
Iteration 6/1000 | Loss: 0.00005937
Iteration 7/1000 | Loss: 0.00007498
Iteration 8/1000 | Loss: 0.00014132
Iteration 9/1000 | Loss: 0.00008694
Iteration 10/1000 | Loss: 0.00017690
Iteration 11/1000 | Loss: 0.00005908
Iteration 12/1000 | Loss: 0.00012088
Iteration 13/1000 | Loss: 0.00011127
Iteration 14/1000 | Loss: 0.00005811
Iteration 15/1000 | Loss: 0.00012239
Iteration 16/1000 | Loss: 0.00017670
Iteration 17/1000 | Loss: 0.00005705
Iteration 18/1000 | Loss: 0.00005028
Iteration 19/1000 | Loss: 0.00004744
Iteration 20/1000 | Loss: 0.00025220
Iteration 21/1000 | Loss: 0.00017206
Iteration 22/1000 | Loss: 0.00013693
Iteration 23/1000 | Loss: 0.00005458
Iteration 24/1000 | Loss: 0.00004496
Iteration 25/1000 | Loss: 0.00004077
Iteration 26/1000 | Loss: 0.00003882
Iteration 27/1000 | Loss: 0.00003732
Iteration 28/1000 | Loss: 0.00008133
Iteration 29/1000 | Loss: 0.00004016
Iteration 30/1000 | Loss: 0.00003731
Iteration 31/1000 | Loss: 0.00003537
Iteration 32/1000 | Loss: 0.00003418
Iteration 33/1000 | Loss: 0.00003263
Iteration 34/1000 | Loss: 0.00003099
Iteration 35/1000 | Loss: 0.00002955
Iteration 36/1000 | Loss: 0.00002867
Iteration 37/1000 | Loss: 0.00003913
Iteration 38/1000 | Loss: 0.00003068
Iteration 39/1000 | Loss: 0.00002845
Iteration 40/1000 | Loss: 0.00002784
Iteration 41/1000 | Loss: 0.00002698
Iteration 42/1000 | Loss: 0.00002626
Iteration 43/1000 | Loss: 0.00002605
Iteration 44/1000 | Loss: 0.00002582
Iteration 45/1000 | Loss: 0.00002566
Iteration 46/1000 | Loss: 0.00002560
Iteration 47/1000 | Loss: 0.00002556
Iteration 48/1000 | Loss: 0.00002555
Iteration 49/1000 | Loss: 0.00002549
Iteration 50/1000 | Loss: 0.00002541
Iteration 51/1000 | Loss: 0.00002541
Iteration 52/1000 | Loss: 0.00002538
Iteration 53/1000 | Loss: 0.00002537
Iteration 54/1000 | Loss: 0.00002536
Iteration 55/1000 | Loss: 0.00002536
Iteration 56/1000 | Loss: 0.00002536
Iteration 57/1000 | Loss: 0.00002536
Iteration 58/1000 | Loss: 0.00002536
Iteration 59/1000 | Loss: 0.00002536
Iteration 60/1000 | Loss: 0.00002536
Iteration 61/1000 | Loss: 0.00002536
Iteration 62/1000 | Loss: 0.00002536
Iteration 63/1000 | Loss: 0.00002536
Iteration 64/1000 | Loss: 0.00002535
Iteration 65/1000 | Loss: 0.00002535
Iteration 66/1000 | Loss: 0.00002535
Iteration 67/1000 | Loss: 0.00002534
Iteration 68/1000 | Loss: 0.00002534
Iteration 69/1000 | Loss: 0.00002533
Iteration 70/1000 | Loss: 0.00002532
Iteration 71/1000 | Loss: 0.00002532
Iteration 72/1000 | Loss: 0.00002531
Iteration 73/1000 | Loss: 0.00002531
Iteration 74/1000 | Loss: 0.00002531
Iteration 75/1000 | Loss: 0.00002531
Iteration 76/1000 | Loss: 0.00002530
Iteration 77/1000 | Loss: 0.00002530
Iteration 78/1000 | Loss: 0.00002530
Iteration 79/1000 | Loss: 0.00002530
Iteration 80/1000 | Loss: 0.00002529
Iteration 81/1000 | Loss: 0.00002529
Iteration 82/1000 | Loss: 0.00002528
Iteration 83/1000 | Loss: 0.00002528
Iteration 84/1000 | Loss: 0.00002528
Iteration 85/1000 | Loss: 0.00002527
Iteration 86/1000 | Loss: 0.00002527
Iteration 87/1000 | Loss: 0.00002527
Iteration 88/1000 | Loss: 0.00002526
Iteration 89/1000 | Loss: 0.00002526
Iteration 90/1000 | Loss: 0.00002526
Iteration 91/1000 | Loss: 0.00002526
Iteration 92/1000 | Loss: 0.00002525
Iteration 93/1000 | Loss: 0.00002525
Iteration 94/1000 | Loss: 0.00002525
Iteration 95/1000 | Loss: 0.00002525
Iteration 96/1000 | Loss: 0.00002524
Iteration 97/1000 | Loss: 0.00002524
Iteration 98/1000 | Loss: 0.00002524
Iteration 99/1000 | Loss: 0.00002523
Iteration 100/1000 | Loss: 0.00002523
Iteration 101/1000 | Loss: 0.00002523
Iteration 102/1000 | Loss: 0.00002523
Iteration 103/1000 | Loss: 0.00002523
Iteration 104/1000 | Loss: 0.00002523
Iteration 105/1000 | Loss: 0.00002523
Iteration 106/1000 | Loss: 0.00002522
Iteration 107/1000 | Loss: 0.00002522
Iteration 108/1000 | Loss: 0.00002522
Iteration 109/1000 | Loss: 0.00002522
Iteration 110/1000 | Loss: 0.00002522
Iteration 111/1000 | Loss: 0.00002522
Iteration 112/1000 | Loss: 0.00002522
Iteration 113/1000 | Loss: 0.00002522
Iteration 114/1000 | Loss: 0.00002521
Iteration 115/1000 | Loss: 0.00002521
Iteration 116/1000 | Loss: 0.00002521
Iteration 117/1000 | Loss: 0.00002521
Iteration 118/1000 | Loss: 0.00002521
Iteration 119/1000 | Loss: 0.00002521
Iteration 120/1000 | Loss: 0.00002521
Iteration 121/1000 | Loss: 0.00002521
Iteration 122/1000 | Loss: 0.00002521
Iteration 123/1000 | Loss: 0.00002521
Iteration 124/1000 | Loss: 0.00002520
Iteration 125/1000 | Loss: 0.00002520
Iteration 126/1000 | Loss: 0.00002520
Iteration 127/1000 | Loss: 0.00002520
Iteration 128/1000 | Loss: 0.00002520
Iteration 129/1000 | Loss: 0.00002520
Iteration 130/1000 | Loss: 0.00002520
Iteration 131/1000 | Loss: 0.00002520
Iteration 132/1000 | Loss: 0.00002520
Iteration 133/1000 | Loss: 0.00002520
Iteration 134/1000 | Loss: 0.00002520
Iteration 135/1000 | Loss: 0.00002520
Iteration 136/1000 | Loss: 0.00002519
Iteration 137/1000 | Loss: 0.00002519
Iteration 138/1000 | Loss: 0.00002519
Iteration 139/1000 | Loss: 0.00002519
Iteration 140/1000 | Loss: 0.00002519
Iteration 141/1000 | Loss: 0.00002519
Iteration 142/1000 | Loss: 0.00002519
Iteration 143/1000 | Loss: 0.00002519
Iteration 144/1000 | Loss: 0.00002519
Iteration 145/1000 | Loss: 0.00002519
Iteration 146/1000 | Loss: 0.00002519
Iteration 147/1000 | Loss: 0.00002519
Iteration 148/1000 | Loss: 0.00002519
Iteration 149/1000 | Loss: 0.00002519
Iteration 150/1000 | Loss: 0.00002519
Iteration 151/1000 | Loss: 0.00002519
Iteration 152/1000 | Loss: 0.00002519
Iteration 153/1000 | Loss: 0.00002519
Iteration 154/1000 | Loss: 0.00002518
Iteration 155/1000 | Loss: 0.00002518
Iteration 156/1000 | Loss: 0.00002518
Iteration 157/1000 | Loss: 0.00002518
Iteration 158/1000 | Loss: 0.00002518
Iteration 159/1000 | Loss: 0.00002518
Iteration 160/1000 | Loss: 0.00002518
Iteration 161/1000 | Loss: 0.00002518
Iteration 162/1000 | Loss: 0.00002518
Iteration 163/1000 | Loss: 0.00002518
Iteration 164/1000 | Loss: 0.00002518
Iteration 165/1000 | Loss: 0.00002518
Iteration 166/1000 | Loss: 0.00002517
Iteration 167/1000 | Loss: 0.00002517
Iteration 168/1000 | Loss: 0.00002517
Iteration 169/1000 | Loss: 0.00002517
Iteration 170/1000 | Loss: 0.00002517
Iteration 171/1000 | Loss: 0.00002517
Iteration 172/1000 | Loss: 0.00002517
Iteration 173/1000 | Loss: 0.00002517
Iteration 174/1000 | Loss: 0.00002517
Iteration 175/1000 | Loss: 0.00002516
Iteration 176/1000 | Loss: 0.00002516
Iteration 177/1000 | Loss: 0.00002516
Iteration 178/1000 | Loss: 0.00002516
Iteration 179/1000 | Loss: 0.00002516
Iteration 180/1000 | Loss: 0.00002516
Iteration 181/1000 | Loss: 0.00002516
Iteration 182/1000 | Loss: 0.00002516
Iteration 183/1000 | Loss: 0.00002516
Iteration 184/1000 | Loss: 0.00002516
Iteration 185/1000 | Loss: 0.00002516
Iteration 186/1000 | Loss: 0.00002516
Iteration 187/1000 | Loss: 0.00002516
Iteration 188/1000 | Loss: 0.00002516
Iteration 189/1000 | Loss: 0.00002516
Iteration 190/1000 | Loss: 0.00002516
Iteration 191/1000 | Loss: 0.00002516
Iteration 192/1000 | Loss: 0.00002516
Iteration 193/1000 | Loss: 0.00002516
Iteration 194/1000 | Loss: 0.00002516
Iteration 195/1000 | Loss: 0.00002516
Iteration 196/1000 | Loss: 0.00002516
Iteration 197/1000 | Loss: 0.00002516
Iteration 198/1000 | Loss: 0.00002516
Iteration 199/1000 | Loss: 0.00002516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.5157731215585954e-05, 2.5157731215585954e-05, 2.5157731215585954e-05, 2.5157731215585954e-05, 2.5157731215585954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5157731215585954e-05

Optimization complete. Final v2v error: 4.174229621887207 mm

Highest mean error: 6.282323360443115 mm for frame 93

Lowest mean error: 3.54136061668396 mm for frame 203

Saving results

Total time: 127.36495018005371
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010148
Iteration 2/25 | Loss: 0.01010148
Iteration 3/25 | Loss: 0.01010148
Iteration 4/25 | Loss: 0.01010148
Iteration 5/25 | Loss: 0.01010148
Iteration 6/25 | Loss: 0.01010148
Iteration 7/25 | Loss: 0.01010148
Iteration 8/25 | Loss: 0.01010148
Iteration 9/25 | Loss: 0.01010147
Iteration 10/25 | Loss: 0.01010147
Iteration 11/25 | Loss: 0.01010147
Iteration 12/25 | Loss: 0.01010147
Iteration 13/25 | Loss: 0.01010147
Iteration 14/25 | Loss: 0.01010147
Iteration 15/25 | Loss: 0.01010147
Iteration 16/25 | Loss: 0.01010147
Iteration 17/25 | Loss: 0.01010146
Iteration 18/25 | Loss: 0.01010146
Iteration 19/25 | Loss: 0.01010146
Iteration 20/25 | Loss: 0.01010146
Iteration 21/25 | Loss: 0.01010146
Iteration 22/25 | Loss: 0.01010146
Iteration 23/25 | Loss: 0.01010146
Iteration 24/25 | Loss: 0.01010146
Iteration 25/25 | Loss: 0.01010146

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81485498
Iteration 2/25 | Loss: 0.14420843
Iteration 3/25 | Loss: 0.13633953
Iteration 4/25 | Loss: 0.13170914
Iteration 5/25 | Loss: 0.13170899
Iteration 6/25 | Loss: 0.13170898
Iteration 7/25 | Loss: 0.13170898
Iteration 8/25 | Loss: 0.13170898
Iteration 9/25 | Loss: 0.13170896
Iteration 10/25 | Loss: 0.13170896
Iteration 11/25 | Loss: 0.13170896
Iteration 12/25 | Loss: 0.13170896
Iteration 13/25 | Loss: 0.13170896
Iteration 14/25 | Loss: 0.13170896
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.13170896470546722, 0.13170896470546722, 0.13170896470546722, 0.13170896470546722, 0.13170896470546722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.13170896470546722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.13170896
Iteration 2/1000 | Loss: 0.00826440
Iteration 3/1000 | Loss: 0.00665168
Iteration 4/1000 | Loss: 0.00345805
Iteration 5/1000 | Loss: 0.00286418
Iteration 6/1000 | Loss: 0.00336965
Iteration 7/1000 | Loss: 0.00065099
Iteration 8/1000 | Loss: 0.00087123
Iteration 9/1000 | Loss: 0.00018459
Iteration 10/1000 | Loss: 0.00014537
Iteration 11/1000 | Loss: 0.00024627
Iteration 12/1000 | Loss: 0.00119150
Iteration 13/1000 | Loss: 0.00222194
Iteration 14/1000 | Loss: 0.00227850
Iteration 15/1000 | Loss: 0.00167378
Iteration 16/1000 | Loss: 0.00123092
Iteration 17/1000 | Loss: 0.00038893
Iteration 18/1000 | Loss: 0.00040680
Iteration 19/1000 | Loss: 0.00005862
Iteration 20/1000 | Loss: 0.00005401
Iteration 21/1000 | Loss: 0.00025089
Iteration 22/1000 | Loss: 0.00039651
Iteration 23/1000 | Loss: 0.00032556
Iteration 24/1000 | Loss: 0.00003848
Iteration 25/1000 | Loss: 0.00043488
Iteration 26/1000 | Loss: 0.00029188
Iteration 27/1000 | Loss: 0.00013093
Iteration 28/1000 | Loss: 0.00046150
Iteration 29/1000 | Loss: 0.00054188
Iteration 30/1000 | Loss: 0.00032394
Iteration 31/1000 | Loss: 0.00199264
Iteration 32/1000 | Loss: 0.00196707
Iteration 33/1000 | Loss: 0.00101621
Iteration 34/1000 | Loss: 0.00023770
Iteration 35/1000 | Loss: 0.00042824
Iteration 36/1000 | Loss: 0.00006988
Iteration 37/1000 | Loss: 0.00003846
Iteration 38/1000 | Loss: 0.00009754
Iteration 39/1000 | Loss: 0.00003736
Iteration 40/1000 | Loss: 0.00011197
Iteration 41/1000 | Loss: 0.00013013
Iteration 42/1000 | Loss: 0.00002584
Iteration 43/1000 | Loss: 0.00015459
Iteration 44/1000 | Loss: 0.00016554
Iteration 45/1000 | Loss: 0.00003260
Iteration 46/1000 | Loss: 0.00004188
Iteration 47/1000 | Loss: 0.00008315
Iteration 48/1000 | Loss: 0.00003539
Iteration 49/1000 | Loss: 0.00009482
Iteration 50/1000 | Loss: 0.00022377
Iteration 51/1000 | Loss: 0.00003272
Iteration 52/1000 | Loss: 0.00050165
Iteration 53/1000 | Loss: 0.00004324
Iteration 54/1000 | Loss: 0.00004016
Iteration 55/1000 | Loss: 0.00003509
Iteration 56/1000 | Loss: 0.00002236
Iteration 57/1000 | Loss: 0.00005873
Iteration 58/1000 | Loss: 0.00002212
Iteration 59/1000 | Loss: 0.00036877
Iteration 60/1000 | Loss: 0.00060336
Iteration 61/1000 | Loss: 0.00018450
Iteration 62/1000 | Loss: 0.00002282
Iteration 63/1000 | Loss: 0.00002177
Iteration 64/1000 | Loss: 0.00025456
Iteration 65/1000 | Loss: 0.00019154
Iteration 66/1000 | Loss: 0.00004805
Iteration 67/1000 | Loss: 0.00002153
Iteration 68/1000 | Loss: 0.00002134
Iteration 69/1000 | Loss: 0.00002130
Iteration 70/1000 | Loss: 0.00002129
Iteration 71/1000 | Loss: 0.00002128
Iteration 72/1000 | Loss: 0.00002127
Iteration 73/1000 | Loss: 0.00002127
Iteration 74/1000 | Loss: 0.00002124
Iteration 75/1000 | Loss: 0.00002121
Iteration 76/1000 | Loss: 0.00002120
Iteration 77/1000 | Loss: 0.00002120
Iteration 78/1000 | Loss: 0.00002116
Iteration 79/1000 | Loss: 0.00002116
Iteration 80/1000 | Loss: 0.00002116
Iteration 81/1000 | Loss: 0.00002116
Iteration 82/1000 | Loss: 0.00002116
Iteration 83/1000 | Loss: 0.00002116
Iteration 84/1000 | Loss: 0.00002116
Iteration 85/1000 | Loss: 0.00002116
Iteration 86/1000 | Loss: 0.00002116
Iteration 87/1000 | Loss: 0.00002116
Iteration 88/1000 | Loss: 0.00002115
Iteration 89/1000 | Loss: 0.00002115
Iteration 90/1000 | Loss: 0.00002114
Iteration 91/1000 | Loss: 0.00002110
Iteration 92/1000 | Loss: 0.00002106
Iteration 93/1000 | Loss: 0.00002105
Iteration 94/1000 | Loss: 0.00002105
Iteration 95/1000 | Loss: 0.00002104
Iteration 96/1000 | Loss: 0.00002102
Iteration 97/1000 | Loss: 0.00002101
Iteration 98/1000 | Loss: 0.00011370
Iteration 99/1000 | Loss: 0.00002382
Iteration 100/1000 | Loss: 0.00002137
Iteration 101/1000 | Loss: 0.00002092
Iteration 102/1000 | Loss: 0.00002091
Iteration 103/1000 | Loss: 0.00002091
Iteration 104/1000 | Loss: 0.00002091
Iteration 105/1000 | Loss: 0.00002090
Iteration 106/1000 | Loss: 0.00002089
Iteration 107/1000 | Loss: 0.00002089
Iteration 108/1000 | Loss: 0.00002089
Iteration 109/1000 | Loss: 0.00002088
Iteration 110/1000 | Loss: 0.00002085
Iteration 111/1000 | Loss: 0.00002085
Iteration 112/1000 | Loss: 0.00002085
Iteration 113/1000 | Loss: 0.00002085
Iteration 114/1000 | Loss: 0.00002084
Iteration 115/1000 | Loss: 0.00002084
Iteration 116/1000 | Loss: 0.00002083
Iteration 117/1000 | Loss: 0.00002083
Iteration 118/1000 | Loss: 0.00002083
Iteration 119/1000 | Loss: 0.00002083
Iteration 120/1000 | Loss: 0.00002082
Iteration 121/1000 | Loss: 0.00002082
Iteration 122/1000 | Loss: 0.00002082
Iteration 123/1000 | Loss: 0.00002082
Iteration 124/1000 | Loss: 0.00002081
Iteration 125/1000 | Loss: 0.00002081
Iteration 126/1000 | Loss: 0.00002081
Iteration 127/1000 | Loss: 0.00002081
Iteration 128/1000 | Loss: 0.00002081
Iteration 129/1000 | Loss: 0.00002080
Iteration 130/1000 | Loss: 0.00002078
Iteration 131/1000 | Loss: 0.00002077
Iteration 132/1000 | Loss: 0.00002076
Iteration 133/1000 | Loss: 0.00002076
Iteration 134/1000 | Loss: 0.00002076
Iteration 135/1000 | Loss: 0.00002075
Iteration 136/1000 | Loss: 0.00002075
Iteration 137/1000 | Loss: 0.00002074
Iteration 138/1000 | Loss: 0.00002074
Iteration 139/1000 | Loss: 0.00002074
Iteration 140/1000 | Loss: 0.00002073
Iteration 141/1000 | Loss: 0.00002073
Iteration 142/1000 | Loss: 0.00002073
Iteration 143/1000 | Loss: 0.00002073
Iteration 144/1000 | Loss: 0.00002072
Iteration 145/1000 | Loss: 0.00002072
Iteration 146/1000 | Loss: 0.00002072
Iteration 147/1000 | Loss: 0.00002072
Iteration 148/1000 | Loss: 0.00002071
Iteration 149/1000 | Loss: 0.00002071
Iteration 150/1000 | Loss: 0.00002071
Iteration 151/1000 | Loss: 0.00002071
Iteration 152/1000 | Loss: 0.00002071
Iteration 153/1000 | Loss: 0.00002070
Iteration 154/1000 | Loss: 0.00002070
Iteration 155/1000 | Loss: 0.00002070
Iteration 156/1000 | Loss: 0.00002070
Iteration 157/1000 | Loss: 0.00002070
Iteration 158/1000 | Loss: 0.00002070
Iteration 159/1000 | Loss: 0.00002070
Iteration 160/1000 | Loss: 0.00002070
Iteration 161/1000 | Loss: 0.00002070
Iteration 162/1000 | Loss: 0.00002070
Iteration 163/1000 | Loss: 0.00002070
Iteration 164/1000 | Loss: 0.00002070
Iteration 165/1000 | Loss: 0.00002070
Iteration 166/1000 | Loss: 0.00002070
Iteration 167/1000 | Loss: 0.00002070
Iteration 168/1000 | Loss: 0.00002070
Iteration 169/1000 | Loss: 0.00002069
Iteration 170/1000 | Loss: 0.00002069
Iteration 171/1000 | Loss: 0.00002069
Iteration 172/1000 | Loss: 0.00002069
Iteration 173/1000 | Loss: 0.00002069
Iteration 174/1000 | Loss: 0.00002069
Iteration 175/1000 | Loss: 0.00002069
Iteration 176/1000 | Loss: 0.00002069
Iteration 177/1000 | Loss: 0.00002069
Iteration 178/1000 | Loss: 0.00002069
Iteration 179/1000 | Loss: 0.00002069
Iteration 180/1000 | Loss: 0.00002069
Iteration 181/1000 | Loss: 0.00002069
Iteration 182/1000 | Loss: 0.00002069
Iteration 183/1000 | Loss: 0.00002069
Iteration 184/1000 | Loss: 0.00002069
Iteration 185/1000 | Loss: 0.00002068
Iteration 186/1000 | Loss: 0.00002068
Iteration 187/1000 | Loss: 0.00014903
Iteration 188/1000 | Loss: 0.00002096
Iteration 189/1000 | Loss: 0.00007217
Iteration 190/1000 | Loss: 0.00004706
Iteration 191/1000 | Loss: 0.00002322
Iteration 192/1000 | Loss: 0.00002430
Iteration 193/1000 | Loss: 0.00002180
Iteration 194/1000 | Loss: 0.00002073
Iteration 195/1000 | Loss: 0.00005130
Iteration 196/1000 | Loss: 0.00011001
Iteration 197/1000 | Loss: 0.00002857
Iteration 198/1000 | Loss: 0.00002074
Iteration 199/1000 | Loss: 0.00002072
Iteration 200/1000 | Loss: 0.00002071
Iteration 201/1000 | Loss: 0.00002069
Iteration 202/1000 | Loss: 0.00002797
Iteration 203/1000 | Loss: 0.00002068
Iteration 204/1000 | Loss: 0.00002066
Iteration 205/1000 | Loss: 0.00002066
Iteration 206/1000 | Loss: 0.00002066
Iteration 207/1000 | Loss: 0.00002066
Iteration 208/1000 | Loss: 0.00002066
Iteration 209/1000 | Loss: 0.00002066
Iteration 210/1000 | Loss: 0.00002066
Iteration 211/1000 | Loss: 0.00002066
Iteration 212/1000 | Loss: 0.00006229
Iteration 213/1000 | Loss: 0.00002256
Iteration 214/1000 | Loss: 0.00002068
Iteration 215/1000 | Loss: 0.00002068
Iteration 216/1000 | Loss: 0.00002068
Iteration 217/1000 | Loss: 0.00002067
Iteration 218/1000 | Loss: 0.00002067
Iteration 219/1000 | Loss: 0.00002067
Iteration 220/1000 | Loss: 0.00002067
Iteration 221/1000 | Loss: 0.00002067
Iteration 222/1000 | Loss: 0.00002067
Iteration 223/1000 | Loss: 0.00002066
Iteration 224/1000 | Loss: 0.00002066
Iteration 225/1000 | Loss: 0.00002066
Iteration 226/1000 | Loss: 0.00002065
Iteration 227/1000 | Loss: 0.00002065
Iteration 228/1000 | Loss: 0.00002065
Iteration 229/1000 | Loss: 0.00002920
Iteration 230/1000 | Loss: 0.00002066
Iteration 231/1000 | Loss: 0.00002066
Iteration 232/1000 | Loss: 0.00002066
Iteration 233/1000 | Loss: 0.00002066
Iteration 234/1000 | Loss: 0.00002066
Iteration 235/1000 | Loss: 0.00002066
Iteration 236/1000 | Loss: 0.00002066
Iteration 237/1000 | Loss: 0.00002066
Iteration 238/1000 | Loss: 0.00002199
Iteration 239/1000 | Loss: 0.00002066
Iteration 240/1000 | Loss: 0.00002065
Iteration 241/1000 | Loss: 0.00002065
Iteration 242/1000 | Loss: 0.00002065
Iteration 243/1000 | Loss: 0.00002065
Iteration 244/1000 | Loss: 0.00002065
Iteration 245/1000 | Loss: 0.00002065
Iteration 246/1000 | Loss: 0.00002065
Iteration 247/1000 | Loss: 0.00002065
Iteration 248/1000 | Loss: 0.00002065
Iteration 249/1000 | Loss: 0.00002065
Iteration 250/1000 | Loss: 0.00002065
Iteration 251/1000 | Loss: 0.00002065
Iteration 252/1000 | Loss: 0.00002064
Iteration 253/1000 | Loss: 0.00002064
Iteration 254/1000 | Loss: 0.00002064
Iteration 255/1000 | Loss: 0.00002064
Iteration 256/1000 | Loss: 0.00002064
Iteration 257/1000 | Loss: 0.00002064
Iteration 258/1000 | Loss: 0.00002064
Iteration 259/1000 | Loss: 0.00002064
Iteration 260/1000 | Loss: 0.00002064
Iteration 261/1000 | Loss: 0.00002064
Iteration 262/1000 | Loss: 0.00002064
Iteration 263/1000 | Loss: 0.00002063
Iteration 264/1000 | Loss: 0.00002063
Iteration 265/1000 | Loss: 0.00002063
Iteration 266/1000 | Loss: 0.00002063
Iteration 267/1000 | Loss: 0.00002063
Iteration 268/1000 | Loss: 0.00002063
Iteration 269/1000 | Loss: 0.00002063
Iteration 270/1000 | Loss: 0.00002063
Iteration 271/1000 | Loss: 0.00002063
Iteration 272/1000 | Loss: 0.00002063
Iteration 273/1000 | Loss: 0.00002063
Iteration 274/1000 | Loss: 0.00002063
Iteration 275/1000 | Loss: 0.00002063
Iteration 276/1000 | Loss: 0.00002063
Iteration 277/1000 | Loss: 0.00002063
Iteration 278/1000 | Loss: 0.00002063
Iteration 279/1000 | Loss: 0.00002063
Iteration 280/1000 | Loss: 0.00002063
Iteration 281/1000 | Loss: 0.00002063
Iteration 282/1000 | Loss: 0.00002063
Iteration 283/1000 | Loss: 0.00002063
Iteration 284/1000 | Loss: 0.00002063
Iteration 285/1000 | Loss: 0.00002063
Iteration 286/1000 | Loss: 0.00002063
Iteration 287/1000 | Loss: 0.00002063
Iteration 288/1000 | Loss: 0.00002063
Iteration 289/1000 | Loss: 0.00002063
Iteration 290/1000 | Loss: 0.00002063
Iteration 291/1000 | Loss: 0.00002063
Iteration 292/1000 | Loss: 0.00002063
Iteration 293/1000 | Loss: 0.00002063
Iteration 294/1000 | Loss: 0.00002063
Iteration 295/1000 | Loss: 0.00002063
Iteration 296/1000 | Loss: 0.00002063
Iteration 297/1000 | Loss: 0.00002063
Iteration 298/1000 | Loss: 0.00002063
Iteration 299/1000 | Loss: 0.00002063
Iteration 300/1000 | Loss: 0.00002063
Iteration 301/1000 | Loss: 0.00002063
Iteration 302/1000 | Loss: 0.00002063
Iteration 303/1000 | Loss: 0.00002063
Iteration 304/1000 | Loss: 0.00002063
Iteration 305/1000 | Loss: 0.00002063
Iteration 306/1000 | Loss: 0.00002063
Iteration 307/1000 | Loss: 0.00002063
Iteration 308/1000 | Loss: 0.00002063
Iteration 309/1000 | Loss: 0.00002063
Iteration 310/1000 | Loss: 0.00002063
Iteration 311/1000 | Loss: 0.00002063
Iteration 312/1000 | Loss: 0.00002063
Iteration 313/1000 | Loss: 0.00002063
Iteration 314/1000 | Loss: 0.00002063
Iteration 315/1000 | Loss: 0.00002063
Iteration 316/1000 | Loss: 0.00002063
Iteration 317/1000 | Loss: 0.00002063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 317. Stopping optimization.
Last 5 losses: [2.0626494006137364e-05, 2.0626494006137364e-05, 2.0626494006137364e-05, 2.0626494006137364e-05, 2.0626494006137364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0626494006137364e-05

Optimization complete. Final v2v error: 3.887228488922119 mm

Highest mean error: 4.533612251281738 mm for frame 69

Lowest mean error: 3.5769755840301514 mm for frame 218

Saving results

Total time: 164.76865792274475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438134
Iteration 2/25 | Loss: 0.00074982
Iteration 3/25 | Loss: 0.00064580
Iteration 4/25 | Loss: 0.00061693
Iteration 5/25 | Loss: 0.00060474
Iteration 6/25 | Loss: 0.00060267
Iteration 7/25 | Loss: 0.00060215
Iteration 8/25 | Loss: 0.00060215
Iteration 9/25 | Loss: 0.00060215
Iteration 10/25 | Loss: 0.00060215
Iteration 11/25 | Loss: 0.00060215
Iteration 12/25 | Loss: 0.00060215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006021513836458325, 0.0006021513836458325, 0.0006021513836458325, 0.0006021513836458325, 0.0006021513836458325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006021513836458325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41552901
Iteration 2/25 | Loss: 0.00012988
Iteration 3/25 | Loss: 0.00012988
Iteration 4/25 | Loss: 0.00012988
Iteration 5/25 | Loss: 0.00012988
Iteration 6/25 | Loss: 0.00012988
Iteration 7/25 | Loss: 0.00012988
Iteration 8/25 | Loss: 0.00012988
Iteration 9/25 | Loss: 0.00012988
Iteration 10/25 | Loss: 0.00012988
Iteration 11/25 | Loss: 0.00012988
Iteration 12/25 | Loss: 0.00012988
Iteration 13/25 | Loss: 0.00012988
Iteration 14/25 | Loss: 0.00012988
Iteration 15/25 | Loss: 0.00012988
Iteration 16/25 | Loss: 0.00012988
Iteration 17/25 | Loss: 0.00012988
Iteration 18/25 | Loss: 0.00012988
Iteration 19/25 | Loss: 0.00012988
Iteration 20/25 | Loss: 0.00012988
Iteration 21/25 | Loss: 0.00012988
Iteration 22/25 | Loss: 0.00012988
Iteration 23/25 | Loss: 0.00012988
Iteration 24/25 | Loss: 0.00012988
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00012987923400942236, 0.00012987923400942236, 0.00012987923400942236, 0.00012987923400942236, 0.00012987923400942236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00012987923400942236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00012988
Iteration 2/1000 | Loss: 0.00001741
Iteration 3/1000 | Loss: 0.00001524
Iteration 4/1000 | Loss: 0.00001427
Iteration 5/1000 | Loss: 0.00001369
Iteration 6/1000 | Loss: 0.00001321
Iteration 7/1000 | Loss: 0.00001289
Iteration 8/1000 | Loss: 0.00001287
Iteration 9/1000 | Loss: 0.00001282
Iteration 10/1000 | Loss: 0.00001281
Iteration 11/1000 | Loss: 0.00001281
Iteration 12/1000 | Loss: 0.00001280
Iteration 13/1000 | Loss: 0.00001277
Iteration 14/1000 | Loss: 0.00001273
Iteration 15/1000 | Loss: 0.00001270
Iteration 16/1000 | Loss: 0.00001269
Iteration 17/1000 | Loss: 0.00001268
Iteration 18/1000 | Loss: 0.00001267
Iteration 19/1000 | Loss: 0.00001267
Iteration 20/1000 | Loss: 0.00001267
Iteration 21/1000 | Loss: 0.00001266
Iteration 22/1000 | Loss: 0.00001266
Iteration 23/1000 | Loss: 0.00001266
Iteration 24/1000 | Loss: 0.00001266
Iteration 25/1000 | Loss: 0.00001265
Iteration 26/1000 | Loss: 0.00001265
Iteration 27/1000 | Loss: 0.00001264
Iteration 28/1000 | Loss: 0.00001264
Iteration 29/1000 | Loss: 0.00001263
Iteration 30/1000 | Loss: 0.00001263
Iteration 31/1000 | Loss: 0.00001263
Iteration 32/1000 | Loss: 0.00001262
Iteration 33/1000 | Loss: 0.00001262
Iteration 34/1000 | Loss: 0.00001262
Iteration 35/1000 | Loss: 0.00001262
Iteration 36/1000 | Loss: 0.00001262
Iteration 37/1000 | Loss: 0.00001261
Iteration 38/1000 | Loss: 0.00001261
Iteration 39/1000 | Loss: 0.00001260
Iteration 40/1000 | Loss: 0.00001260
Iteration 41/1000 | Loss: 0.00001260
Iteration 42/1000 | Loss: 0.00001259
Iteration 43/1000 | Loss: 0.00001259
Iteration 44/1000 | Loss: 0.00001259
Iteration 45/1000 | Loss: 0.00001259
Iteration 46/1000 | Loss: 0.00001259
Iteration 47/1000 | Loss: 0.00001258
Iteration 48/1000 | Loss: 0.00001258
Iteration 49/1000 | Loss: 0.00001258
Iteration 50/1000 | Loss: 0.00001257
Iteration 51/1000 | Loss: 0.00001257
Iteration 52/1000 | Loss: 0.00001257
Iteration 53/1000 | Loss: 0.00001257
Iteration 54/1000 | Loss: 0.00001257
Iteration 55/1000 | Loss: 0.00001257
Iteration 56/1000 | Loss: 0.00001257
Iteration 57/1000 | Loss: 0.00001256
Iteration 58/1000 | Loss: 0.00001256
Iteration 59/1000 | Loss: 0.00001256
Iteration 60/1000 | Loss: 0.00001256
Iteration 61/1000 | Loss: 0.00001255
Iteration 62/1000 | Loss: 0.00001255
Iteration 63/1000 | Loss: 0.00001255
Iteration 64/1000 | Loss: 0.00001254
Iteration 65/1000 | Loss: 0.00001254
Iteration 66/1000 | Loss: 0.00001254
Iteration 67/1000 | Loss: 0.00001254
Iteration 68/1000 | Loss: 0.00001254
Iteration 69/1000 | Loss: 0.00001254
Iteration 70/1000 | Loss: 0.00001254
Iteration 71/1000 | Loss: 0.00001254
Iteration 72/1000 | Loss: 0.00001254
Iteration 73/1000 | Loss: 0.00001254
Iteration 74/1000 | Loss: 0.00001253
Iteration 75/1000 | Loss: 0.00001253
Iteration 76/1000 | Loss: 0.00001253
Iteration 77/1000 | Loss: 0.00001253
Iteration 78/1000 | Loss: 0.00001253
Iteration 79/1000 | Loss: 0.00001253
Iteration 80/1000 | Loss: 0.00001253
Iteration 81/1000 | Loss: 0.00001253
Iteration 82/1000 | Loss: 0.00001253
Iteration 83/1000 | Loss: 0.00001252
Iteration 84/1000 | Loss: 0.00001252
Iteration 85/1000 | Loss: 0.00001252
Iteration 86/1000 | Loss: 0.00001252
Iteration 87/1000 | Loss: 0.00001252
Iteration 88/1000 | Loss: 0.00001252
Iteration 89/1000 | Loss: 0.00001252
Iteration 90/1000 | Loss: 0.00001252
Iteration 91/1000 | Loss: 0.00001252
Iteration 92/1000 | Loss: 0.00001252
Iteration 93/1000 | Loss: 0.00001252
Iteration 94/1000 | Loss: 0.00001252
Iteration 95/1000 | Loss: 0.00001251
Iteration 96/1000 | Loss: 0.00001251
Iteration 97/1000 | Loss: 0.00001251
Iteration 98/1000 | Loss: 0.00001251
Iteration 99/1000 | Loss: 0.00001251
Iteration 100/1000 | Loss: 0.00001251
Iteration 101/1000 | Loss: 0.00001251
Iteration 102/1000 | Loss: 0.00001251
Iteration 103/1000 | Loss: 0.00001251
Iteration 104/1000 | Loss: 0.00001251
Iteration 105/1000 | Loss: 0.00001251
Iteration 106/1000 | Loss: 0.00001250
Iteration 107/1000 | Loss: 0.00001250
Iteration 108/1000 | Loss: 0.00001250
Iteration 109/1000 | Loss: 0.00001249
Iteration 110/1000 | Loss: 0.00001249
Iteration 111/1000 | Loss: 0.00001249
Iteration 112/1000 | Loss: 0.00001249
Iteration 113/1000 | Loss: 0.00001248
Iteration 114/1000 | Loss: 0.00001248
Iteration 115/1000 | Loss: 0.00001248
Iteration 116/1000 | Loss: 0.00001248
Iteration 117/1000 | Loss: 0.00001248
Iteration 118/1000 | Loss: 0.00001248
Iteration 119/1000 | Loss: 0.00001248
Iteration 120/1000 | Loss: 0.00001248
Iteration 121/1000 | Loss: 0.00001248
Iteration 122/1000 | Loss: 0.00001248
Iteration 123/1000 | Loss: 0.00001248
Iteration 124/1000 | Loss: 0.00001248
Iteration 125/1000 | Loss: 0.00001248
Iteration 126/1000 | Loss: 0.00001248
Iteration 127/1000 | Loss: 0.00001248
Iteration 128/1000 | Loss: 0.00001248
Iteration 129/1000 | Loss: 0.00001248
Iteration 130/1000 | Loss: 0.00001248
Iteration 131/1000 | Loss: 0.00001248
Iteration 132/1000 | Loss: 0.00001248
Iteration 133/1000 | Loss: 0.00001248
Iteration 134/1000 | Loss: 0.00001248
Iteration 135/1000 | Loss: 0.00001248
Iteration 136/1000 | Loss: 0.00001248
Iteration 137/1000 | Loss: 0.00001248
Iteration 138/1000 | Loss: 0.00001248
Iteration 139/1000 | Loss: 0.00001248
Iteration 140/1000 | Loss: 0.00001248
Iteration 141/1000 | Loss: 0.00001248
Iteration 142/1000 | Loss: 0.00001248
Iteration 143/1000 | Loss: 0.00001248
Iteration 144/1000 | Loss: 0.00001248
Iteration 145/1000 | Loss: 0.00001248
Iteration 146/1000 | Loss: 0.00001248
Iteration 147/1000 | Loss: 0.00001248
Iteration 148/1000 | Loss: 0.00001248
Iteration 149/1000 | Loss: 0.00001248
Iteration 150/1000 | Loss: 0.00001248
Iteration 151/1000 | Loss: 0.00001248
Iteration 152/1000 | Loss: 0.00001248
Iteration 153/1000 | Loss: 0.00001248
Iteration 154/1000 | Loss: 0.00001248
Iteration 155/1000 | Loss: 0.00001248
Iteration 156/1000 | Loss: 0.00001248
Iteration 157/1000 | Loss: 0.00001248
Iteration 158/1000 | Loss: 0.00001248
Iteration 159/1000 | Loss: 0.00001248
Iteration 160/1000 | Loss: 0.00001248
Iteration 161/1000 | Loss: 0.00001248
Iteration 162/1000 | Loss: 0.00001248
Iteration 163/1000 | Loss: 0.00001248
Iteration 164/1000 | Loss: 0.00001248
Iteration 165/1000 | Loss: 0.00001248
Iteration 166/1000 | Loss: 0.00001248
Iteration 167/1000 | Loss: 0.00001248
Iteration 168/1000 | Loss: 0.00001248
Iteration 169/1000 | Loss: 0.00001248
Iteration 170/1000 | Loss: 0.00001248
Iteration 171/1000 | Loss: 0.00001248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.2481989870138932e-05, 1.2481989870138932e-05, 1.2481989870138932e-05, 1.2481989870138932e-05, 1.2481989870138932e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2481989870138932e-05

Optimization complete. Final v2v error: 2.925809144973755 mm

Highest mean error: 3.222043752670288 mm for frame 141

Lowest mean error: 2.595355749130249 mm for frame 45

Saving results

Total time: 30.17059087753296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_it_4311/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_it_4311/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374856
Iteration 2/25 | Loss: 0.00070128
Iteration 3/25 | Loss: 0.00060634
Iteration 4/25 | Loss: 0.00059092
Iteration 5/25 | Loss: 0.00058507
Iteration 6/25 | Loss: 0.00058391
Iteration 7/25 | Loss: 0.00058391
Iteration 8/25 | Loss: 0.00058391
Iteration 9/25 | Loss: 0.00058391
Iteration 10/25 | Loss: 0.00058391
Iteration 11/25 | Loss: 0.00058391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0005839122459292412, 0.0005839122459292412, 0.0005839122459292412, 0.0005839122459292412, 0.0005839122459292412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005839122459292412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39625132
Iteration 2/25 | Loss: 0.00011608
Iteration 3/25 | Loss: 0.00011608
Iteration 4/25 | Loss: 0.00011608
Iteration 5/25 | Loss: 0.00011608
Iteration 6/25 | Loss: 0.00011607
Iteration 7/25 | Loss: 0.00011607
Iteration 8/25 | Loss: 0.00011607
Iteration 9/25 | Loss: 0.00011607
Iteration 10/25 | Loss: 0.00011607
Iteration 11/25 | Loss: 0.00011607
Iteration 12/25 | Loss: 0.00011607
Iteration 13/25 | Loss: 0.00011607
Iteration 14/25 | Loss: 0.00011607
Iteration 15/25 | Loss: 0.00011607
Iteration 16/25 | Loss: 0.00011607
Iteration 17/25 | Loss: 0.00011607
Iteration 18/25 | Loss: 0.00011607
Iteration 19/25 | Loss: 0.00011607
Iteration 20/25 | Loss: 0.00011607
Iteration 21/25 | Loss: 0.00011607
Iteration 22/25 | Loss: 0.00011607
Iteration 23/25 | Loss: 0.00011607
Iteration 24/25 | Loss: 0.00011607
Iteration 25/25 | Loss: 0.00011607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00011607
Iteration 2/1000 | Loss: 0.00001700
Iteration 3/1000 | Loss: 0.00001498
Iteration 4/1000 | Loss: 0.00001397
Iteration 5/1000 | Loss: 0.00001339
Iteration 6/1000 | Loss: 0.00001302
Iteration 7/1000 | Loss: 0.00001302
Iteration 8/1000 | Loss: 0.00001284
Iteration 9/1000 | Loss: 0.00001268
Iteration 10/1000 | Loss: 0.00001268
Iteration 11/1000 | Loss: 0.00001265
Iteration 12/1000 | Loss: 0.00001259
Iteration 13/1000 | Loss: 0.00001256
Iteration 14/1000 | Loss: 0.00001256
Iteration 15/1000 | Loss: 0.00001255
Iteration 16/1000 | Loss: 0.00001253
Iteration 17/1000 | Loss: 0.00001243
Iteration 18/1000 | Loss: 0.00001242
Iteration 19/1000 | Loss: 0.00001242
Iteration 20/1000 | Loss: 0.00001241
Iteration 21/1000 | Loss: 0.00001241
Iteration 22/1000 | Loss: 0.00001241
Iteration 23/1000 | Loss: 0.00001240
Iteration 24/1000 | Loss: 0.00001240
Iteration 25/1000 | Loss: 0.00001238
Iteration 26/1000 | Loss: 0.00001238
Iteration 27/1000 | Loss: 0.00001237
Iteration 28/1000 | Loss: 0.00001237
Iteration 29/1000 | Loss: 0.00001233
Iteration 30/1000 | Loss: 0.00001232
Iteration 31/1000 | Loss: 0.00001228
Iteration 32/1000 | Loss: 0.00001227
Iteration 33/1000 | Loss: 0.00001227
Iteration 34/1000 | Loss: 0.00001227
Iteration 35/1000 | Loss: 0.00001227
Iteration 36/1000 | Loss: 0.00001227
Iteration 37/1000 | Loss: 0.00001227
Iteration 38/1000 | Loss: 0.00001227
Iteration 39/1000 | Loss: 0.00001227
Iteration 40/1000 | Loss: 0.00001227
Iteration 41/1000 | Loss: 0.00001227
Iteration 42/1000 | Loss: 0.00001227
Iteration 43/1000 | Loss: 0.00001226
Iteration 44/1000 | Loss: 0.00001226
Iteration 45/1000 | Loss: 0.00001225
Iteration 46/1000 | Loss: 0.00001225
Iteration 47/1000 | Loss: 0.00001224
Iteration 48/1000 | Loss: 0.00001224
Iteration 49/1000 | Loss: 0.00001224
Iteration 50/1000 | Loss: 0.00001224
Iteration 51/1000 | Loss: 0.00001224
Iteration 52/1000 | Loss: 0.00001223
Iteration 53/1000 | Loss: 0.00001223
Iteration 54/1000 | Loss: 0.00001223
Iteration 55/1000 | Loss: 0.00001222
Iteration 56/1000 | Loss: 0.00001222
Iteration 57/1000 | Loss: 0.00001222
Iteration 58/1000 | Loss: 0.00001221
Iteration 59/1000 | Loss: 0.00001221
Iteration 60/1000 | Loss: 0.00001221
Iteration 61/1000 | Loss: 0.00001221
Iteration 62/1000 | Loss: 0.00001220
Iteration 63/1000 | Loss: 0.00001220
Iteration 64/1000 | Loss: 0.00001219
Iteration 65/1000 | Loss: 0.00001218
Iteration 66/1000 | Loss: 0.00001218
Iteration 67/1000 | Loss: 0.00001217
Iteration 68/1000 | Loss: 0.00001217
Iteration 69/1000 | Loss: 0.00001217
Iteration 70/1000 | Loss: 0.00001217
Iteration 71/1000 | Loss: 0.00001217
Iteration 72/1000 | Loss: 0.00001216
Iteration 73/1000 | Loss: 0.00001216
Iteration 74/1000 | Loss: 0.00001216
Iteration 75/1000 | Loss: 0.00001216
Iteration 76/1000 | Loss: 0.00001216
Iteration 77/1000 | Loss: 0.00001216
Iteration 78/1000 | Loss: 0.00001216
Iteration 79/1000 | Loss: 0.00001215
Iteration 80/1000 | Loss: 0.00001215
Iteration 81/1000 | Loss: 0.00001215
Iteration 82/1000 | Loss: 0.00001215
Iteration 83/1000 | Loss: 0.00001215
Iteration 84/1000 | Loss: 0.00001215
Iteration 85/1000 | Loss: 0.00001215
Iteration 86/1000 | Loss: 0.00001215
Iteration 87/1000 | Loss: 0.00001215
Iteration 88/1000 | Loss: 0.00001215
Iteration 89/1000 | Loss: 0.00001215
Iteration 90/1000 | Loss: 0.00001215
Iteration 91/1000 | Loss: 0.00001214
Iteration 92/1000 | Loss: 0.00001214
Iteration 93/1000 | Loss: 0.00001214
Iteration 94/1000 | Loss: 0.00001214
Iteration 95/1000 | Loss: 0.00001214
Iteration 96/1000 | Loss: 0.00001214
Iteration 97/1000 | Loss: 0.00001214
Iteration 98/1000 | Loss: 0.00001213
Iteration 99/1000 | Loss: 0.00001212
Iteration 100/1000 | Loss: 0.00001212
Iteration 101/1000 | Loss: 0.00001212
Iteration 102/1000 | Loss: 0.00001212
Iteration 103/1000 | Loss: 0.00001212
Iteration 104/1000 | Loss: 0.00001212
Iteration 105/1000 | Loss: 0.00001212
Iteration 106/1000 | Loss: 0.00001212
Iteration 107/1000 | Loss: 0.00001212
Iteration 108/1000 | Loss: 0.00001212
Iteration 109/1000 | Loss: 0.00001211
Iteration 110/1000 | Loss: 0.00001211
Iteration 111/1000 | Loss: 0.00001211
Iteration 112/1000 | Loss: 0.00001211
Iteration 113/1000 | Loss: 0.00001211
Iteration 114/1000 | Loss: 0.00001211
Iteration 115/1000 | Loss: 0.00001211
Iteration 116/1000 | Loss: 0.00001211
Iteration 117/1000 | Loss: 0.00001211
Iteration 118/1000 | Loss: 0.00001211
Iteration 119/1000 | Loss: 0.00001211
Iteration 120/1000 | Loss: 0.00001211
Iteration 121/1000 | Loss: 0.00001211
Iteration 122/1000 | Loss: 0.00001211
Iteration 123/1000 | Loss: 0.00001211
Iteration 124/1000 | Loss: 0.00001211
Iteration 125/1000 | Loss: 0.00001211
Iteration 126/1000 | Loss: 0.00001211
Iteration 127/1000 | Loss: 0.00001211
Iteration 128/1000 | Loss: 0.00001211
Iteration 129/1000 | Loss: 0.00001211
Iteration 130/1000 | Loss: 0.00001211
Iteration 131/1000 | Loss: 0.00001211
Iteration 132/1000 | Loss: 0.00001211
Iteration 133/1000 | Loss: 0.00001211
Iteration 134/1000 | Loss: 0.00001211
Iteration 135/1000 | Loss: 0.00001211
Iteration 136/1000 | Loss: 0.00001211
Iteration 137/1000 | Loss: 0.00001211
Iteration 138/1000 | Loss: 0.00001211
Iteration 139/1000 | Loss: 0.00001211
Iteration 140/1000 | Loss: 0.00001211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.2114291166653857e-05, 1.2114291166653857e-05, 1.2114291166653857e-05, 1.2114291166653857e-05, 1.2114291166653857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2114291166653857e-05

Optimization complete. Final v2v error: 2.963529586791992 mm

Highest mean error: 3.1664302349090576 mm for frame 84

Lowest mean error: 2.6050450801849365 mm for frame 266

Saving results

Total time: 36.56340169906616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801710
Iteration 2/25 | Loss: 0.00115684
Iteration 3/25 | Loss: 0.00101148
Iteration 4/25 | Loss: 0.00099016
Iteration 5/25 | Loss: 0.00098421
Iteration 6/25 | Loss: 0.00098209
Iteration 7/25 | Loss: 0.00098167
Iteration 8/25 | Loss: 0.00098167
Iteration 9/25 | Loss: 0.00098167
Iteration 10/25 | Loss: 0.00098167
Iteration 11/25 | Loss: 0.00098167
Iteration 12/25 | Loss: 0.00098167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000981669407337904, 0.000981669407337904, 0.000981669407337904, 0.000981669407337904, 0.000981669407337904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000981669407337904

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23561454
Iteration 2/25 | Loss: 0.00185755
Iteration 3/25 | Loss: 0.00185755
Iteration 4/25 | Loss: 0.00185754
Iteration 5/25 | Loss: 0.00185754
Iteration 6/25 | Loss: 0.00185754
Iteration 7/25 | Loss: 0.00185754
Iteration 8/25 | Loss: 0.00185754
Iteration 9/25 | Loss: 0.00185754
Iteration 10/25 | Loss: 0.00185754
Iteration 11/25 | Loss: 0.00185754
Iteration 12/25 | Loss: 0.00185754
Iteration 13/25 | Loss: 0.00185754
Iteration 14/25 | Loss: 0.00185754
Iteration 15/25 | Loss: 0.00185754
Iteration 16/25 | Loss: 0.00185754
Iteration 17/25 | Loss: 0.00185754
Iteration 18/25 | Loss: 0.00185754
Iteration 19/25 | Loss: 0.00185754
Iteration 20/25 | Loss: 0.00185754
Iteration 21/25 | Loss: 0.00185754
Iteration 22/25 | Loss: 0.00185754
Iteration 23/25 | Loss: 0.00185754
Iteration 24/25 | Loss: 0.00185754
Iteration 25/25 | Loss: 0.00185754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185754
Iteration 2/1000 | Loss: 0.00004423
Iteration 3/1000 | Loss: 0.00003167
Iteration 4/1000 | Loss: 0.00002553
Iteration 5/1000 | Loss: 0.00002316
Iteration 6/1000 | Loss: 0.00002185
Iteration 7/1000 | Loss: 0.00002098
Iteration 8/1000 | Loss: 0.00002027
Iteration 9/1000 | Loss: 0.00001980
Iteration 10/1000 | Loss: 0.00001946
Iteration 11/1000 | Loss: 0.00001913
Iteration 12/1000 | Loss: 0.00001890
Iteration 13/1000 | Loss: 0.00001873
Iteration 14/1000 | Loss: 0.00001869
Iteration 15/1000 | Loss: 0.00001859
Iteration 16/1000 | Loss: 0.00001852
Iteration 17/1000 | Loss: 0.00001849
Iteration 18/1000 | Loss: 0.00001848
Iteration 19/1000 | Loss: 0.00001847
Iteration 20/1000 | Loss: 0.00001846
Iteration 21/1000 | Loss: 0.00001846
Iteration 22/1000 | Loss: 0.00001846
Iteration 23/1000 | Loss: 0.00001846
Iteration 24/1000 | Loss: 0.00001845
Iteration 25/1000 | Loss: 0.00001845
Iteration 26/1000 | Loss: 0.00001845
Iteration 27/1000 | Loss: 0.00001844
Iteration 28/1000 | Loss: 0.00001844
Iteration 29/1000 | Loss: 0.00001843
Iteration 30/1000 | Loss: 0.00001843
Iteration 31/1000 | Loss: 0.00001842
Iteration 32/1000 | Loss: 0.00001842
Iteration 33/1000 | Loss: 0.00001841
Iteration 34/1000 | Loss: 0.00001841
Iteration 35/1000 | Loss: 0.00001840
Iteration 36/1000 | Loss: 0.00001840
Iteration 37/1000 | Loss: 0.00001839
Iteration 38/1000 | Loss: 0.00001838
Iteration 39/1000 | Loss: 0.00001838
Iteration 40/1000 | Loss: 0.00001837
Iteration 41/1000 | Loss: 0.00001837
Iteration 42/1000 | Loss: 0.00001837
Iteration 43/1000 | Loss: 0.00001836
Iteration 44/1000 | Loss: 0.00001836
Iteration 45/1000 | Loss: 0.00001836
Iteration 46/1000 | Loss: 0.00001835
Iteration 47/1000 | Loss: 0.00001835
Iteration 48/1000 | Loss: 0.00001835
Iteration 49/1000 | Loss: 0.00001834
Iteration 50/1000 | Loss: 0.00001834
Iteration 51/1000 | Loss: 0.00001834
Iteration 52/1000 | Loss: 0.00001834
Iteration 53/1000 | Loss: 0.00001834
Iteration 54/1000 | Loss: 0.00001834
Iteration 55/1000 | Loss: 0.00001834
Iteration 56/1000 | Loss: 0.00001833
Iteration 57/1000 | Loss: 0.00001833
Iteration 58/1000 | Loss: 0.00001833
Iteration 59/1000 | Loss: 0.00001833
Iteration 60/1000 | Loss: 0.00001832
Iteration 61/1000 | Loss: 0.00001832
Iteration 62/1000 | Loss: 0.00001832
Iteration 63/1000 | Loss: 0.00001832
Iteration 64/1000 | Loss: 0.00001832
Iteration 65/1000 | Loss: 0.00001832
Iteration 66/1000 | Loss: 0.00001831
Iteration 67/1000 | Loss: 0.00001831
Iteration 68/1000 | Loss: 0.00001831
Iteration 69/1000 | Loss: 0.00001831
Iteration 70/1000 | Loss: 0.00001831
Iteration 71/1000 | Loss: 0.00001830
Iteration 72/1000 | Loss: 0.00001830
Iteration 73/1000 | Loss: 0.00001830
Iteration 74/1000 | Loss: 0.00001830
Iteration 75/1000 | Loss: 0.00001830
Iteration 76/1000 | Loss: 0.00001830
Iteration 77/1000 | Loss: 0.00001829
Iteration 78/1000 | Loss: 0.00001829
Iteration 79/1000 | Loss: 0.00001829
Iteration 80/1000 | Loss: 0.00001829
Iteration 81/1000 | Loss: 0.00001829
Iteration 82/1000 | Loss: 0.00001828
Iteration 83/1000 | Loss: 0.00001828
Iteration 84/1000 | Loss: 0.00001828
Iteration 85/1000 | Loss: 0.00001828
Iteration 86/1000 | Loss: 0.00001828
Iteration 87/1000 | Loss: 0.00001828
Iteration 88/1000 | Loss: 0.00001828
Iteration 89/1000 | Loss: 0.00001828
Iteration 90/1000 | Loss: 0.00001828
Iteration 91/1000 | Loss: 0.00001827
Iteration 92/1000 | Loss: 0.00001827
Iteration 93/1000 | Loss: 0.00001827
Iteration 94/1000 | Loss: 0.00001827
Iteration 95/1000 | Loss: 0.00001827
Iteration 96/1000 | Loss: 0.00001827
Iteration 97/1000 | Loss: 0.00001827
Iteration 98/1000 | Loss: 0.00001827
Iteration 99/1000 | Loss: 0.00001826
Iteration 100/1000 | Loss: 0.00001826
Iteration 101/1000 | Loss: 0.00001826
Iteration 102/1000 | Loss: 0.00001825
Iteration 103/1000 | Loss: 0.00001825
Iteration 104/1000 | Loss: 0.00001825
Iteration 105/1000 | Loss: 0.00001825
Iteration 106/1000 | Loss: 0.00001824
Iteration 107/1000 | Loss: 0.00001824
Iteration 108/1000 | Loss: 0.00001824
Iteration 109/1000 | Loss: 0.00001824
Iteration 110/1000 | Loss: 0.00001823
Iteration 111/1000 | Loss: 0.00001823
Iteration 112/1000 | Loss: 0.00001823
Iteration 113/1000 | Loss: 0.00001823
Iteration 114/1000 | Loss: 0.00001823
Iteration 115/1000 | Loss: 0.00001823
Iteration 116/1000 | Loss: 0.00001822
Iteration 117/1000 | Loss: 0.00001822
Iteration 118/1000 | Loss: 0.00001822
Iteration 119/1000 | Loss: 0.00001822
Iteration 120/1000 | Loss: 0.00001822
Iteration 121/1000 | Loss: 0.00001822
Iteration 122/1000 | Loss: 0.00001821
Iteration 123/1000 | Loss: 0.00001821
Iteration 124/1000 | Loss: 0.00001821
Iteration 125/1000 | Loss: 0.00001821
Iteration 126/1000 | Loss: 0.00001821
Iteration 127/1000 | Loss: 0.00001821
Iteration 128/1000 | Loss: 0.00001821
Iteration 129/1000 | Loss: 0.00001820
Iteration 130/1000 | Loss: 0.00001820
Iteration 131/1000 | Loss: 0.00001820
Iteration 132/1000 | Loss: 0.00001820
Iteration 133/1000 | Loss: 0.00001820
Iteration 134/1000 | Loss: 0.00001820
Iteration 135/1000 | Loss: 0.00001820
Iteration 136/1000 | Loss: 0.00001820
Iteration 137/1000 | Loss: 0.00001820
Iteration 138/1000 | Loss: 0.00001820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.820013130782172e-05, 1.820013130782172e-05, 1.820013130782172e-05, 1.820013130782172e-05, 1.820013130782172e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.820013130782172e-05

Optimization complete. Final v2v error: 3.5440099239349365 mm

Highest mean error: 4.414511203765869 mm for frame 155

Lowest mean error: 2.9874415397644043 mm for frame 4

Saving results

Total time: 40.744290590286255
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853673
Iteration 2/25 | Loss: 0.00162363
Iteration 3/25 | Loss: 0.00132174
Iteration 4/25 | Loss: 0.00126958
Iteration 5/25 | Loss: 0.00126109
Iteration 6/25 | Loss: 0.00130879
Iteration 7/25 | Loss: 0.00134708
Iteration 8/25 | Loss: 0.00128745
Iteration 9/25 | Loss: 0.00119785
Iteration 10/25 | Loss: 0.00115350
Iteration 11/25 | Loss: 0.00115978
Iteration 12/25 | Loss: 0.00116600
Iteration 13/25 | Loss: 0.00115442
Iteration 14/25 | Loss: 0.00111003
Iteration 15/25 | Loss: 0.00107072
Iteration 16/25 | Loss: 0.00106004
Iteration 17/25 | Loss: 0.00105066
Iteration 18/25 | Loss: 0.00104264
Iteration 19/25 | Loss: 0.00104033
Iteration 20/25 | Loss: 0.00103998
Iteration 21/25 | Loss: 0.00103981
Iteration 22/25 | Loss: 0.00103910
Iteration 23/25 | Loss: 0.00103864
Iteration 24/25 | Loss: 0.00103768
Iteration 25/25 | Loss: 0.00103645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21451139
Iteration 2/25 | Loss: 0.00153669
Iteration 3/25 | Loss: 0.00153666
Iteration 4/25 | Loss: 0.00153666
Iteration 5/25 | Loss: 0.00153666
Iteration 6/25 | Loss: 0.00153666
Iteration 7/25 | Loss: 0.00153666
Iteration 8/25 | Loss: 0.00153666
Iteration 9/25 | Loss: 0.00153666
Iteration 10/25 | Loss: 0.00153666
Iteration 11/25 | Loss: 0.00153666
Iteration 12/25 | Loss: 0.00153666
Iteration 13/25 | Loss: 0.00153666
Iteration 14/25 | Loss: 0.00153666
Iteration 15/25 | Loss: 0.00153666
Iteration 16/25 | Loss: 0.00153666
Iteration 17/25 | Loss: 0.00153666
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015366594307124615, 0.0015366594307124615, 0.0015366594307124615, 0.0015366594307124615, 0.0015366594307124615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015366594307124615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153666
Iteration 2/1000 | Loss: 0.00007774
Iteration 3/1000 | Loss: 0.00006372
Iteration 4/1000 | Loss: 0.00005741
Iteration 5/1000 | Loss: 0.00004378
Iteration 6/1000 | Loss: 0.00004294
Iteration 7/1000 | Loss: 0.00004964
Iteration 8/1000 | Loss: 0.00005488
Iteration 9/1000 | Loss: 0.00004900
Iteration 10/1000 | Loss: 0.00003700
Iteration 11/1000 | Loss: 0.00004116
Iteration 12/1000 | Loss: 0.00003938
Iteration 13/1000 | Loss: 0.00003941
Iteration 14/1000 | Loss: 0.00004284
Iteration 15/1000 | Loss: 0.00005352
Iteration 16/1000 | Loss: 0.00004927
Iteration 17/1000 | Loss: 0.00005306
Iteration 18/1000 | Loss: 0.00004998
Iteration 19/1000 | Loss: 0.00004711
Iteration 20/1000 | Loss: 0.00004901
Iteration 21/1000 | Loss: 0.00004854
Iteration 22/1000 | Loss: 0.00003590
Iteration 23/1000 | Loss: 0.00004657
Iteration 24/1000 | Loss: 0.00003397
Iteration 25/1000 | Loss: 0.00004467
Iteration 26/1000 | Loss: 0.00003340
Iteration 27/1000 | Loss: 0.00004375
Iteration 28/1000 | Loss: 0.00004184
Iteration 29/1000 | Loss: 0.00004994
Iteration 30/1000 | Loss: 0.00005337
Iteration 31/1000 | Loss: 0.00004945
Iteration 32/1000 | Loss: 0.00004209
Iteration 33/1000 | Loss: 0.00004436
Iteration 34/1000 | Loss: 0.00004458
Iteration 35/1000 | Loss: 0.00005082
Iteration 36/1000 | Loss: 0.00003029
Iteration 37/1000 | Loss: 0.00002489
Iteration 38/1000 | Loss: 0.00002283
Iteration 39/1000 | Loss: 0.00002164
Iteration 40/1000 | Loss: 0.00002113
Iteration 41/1000 | Loss: 0.00002069
Iteration 42/1000 | Loss: 0.00002030
Iteration 43/1000 | Loss: 0.00002000
Iteration 44/1000 | Loss: 0.00001995
Iteration 45/1000 | Loss: 0.00001989
Iteration 46/1000 | Loss: 0.00001988
Iteration 47/1000 | Loss: 0.00001982
Iteration 48/1000 | Loss: 0.00001978
Iteration 49/1000 | Loss: 0.00001975
Iteration 50/1000 | Loss: 0.00001974
Iteration 51/1000 | Loss: 0.00001973
Iteration 52/1000 | Loss: 0.00001964
Iteration 53/1000 | Loss: 0.00001961
Iteration 54/1000 | Loss: 0.00001957
Iteration 55/1000 | Loss: 0.00001941
Iteration 56/1000 | Loss: 0.00001940
Iteration 57/1000 | Loss: 0.00001939
Iteration 58/1000 | Loss: 0.00001939
Iteration 59/1000 | Loss: 0.00001939
Iteration 60/1000 | Loss: 0.00001939
Iteration 61/1000 | Loss: 0.00001939
Iteration 62/1000 | Loss: 0.00001938
Iteration 63/1000 | Loss: 0.00001938
Iteration 64/1000 | Loss: 0.00001937
Iteration 65/1000 | Loss: 0.00001937
Iteration 66/1000 | Loss: 0.00001936
Iteration 67/1000 | Loss: 0.00001936
Iteration 68/1000 | Loss: 0.00001935
Iteration 69/1000 | Loss: 0.00001935
Iteration 70/1000 | Loss: 0.00001935
Iteration 71/1000 | Loss: 0.00001934
Iteration 72/1000 | Loss: 0.00001934
Iteration 73/1000 | Loss: 0.00001934
Iteration 74/1000 | Loss: 0.00001934
Iteration 75/1000 | Loss: 0.00001933
Iteration 76/1000 | Loss: 0.00001933
Iteration 77/1000 | Loss: 0.00001932
Iteration 78/1000 | Loss: 0.00001932
Iteration 79/1000 | Loss: 0.00001932
Iteration 80/1000 | Loss: 0.00001932
Iteration 81/1000 | Loss: 0.00001932
Iteration 82/1000 | Loss: 0.00001931
Iteration 83/1000 | Loss: 0.00001931
Iteration 84/1000 | Loss: 0.00001931
Iteration 85/1000 | Loss: 0.00001931
Iteration 86/1000 | Loss: 0.00001931
Iteration 87/1000 | Loss: 0.00001931
Iteration 88/1000 | Loss: 0.00001930
Iteration 89/1000 | Loss: 0.00001930
Iteration 90/1000 | Loss: 0.00001930
Iteration 91/1000 | Loss: 0.00001930
Iteration 92/1000 | Loss: 0.00001929
Iteration 93/1000 | Loss: 0.00001929
Iteration 94/1000 | Loss: 0.00001929
Iteration 95/1000 | Loss: 0.00001928
Iteration 96/1000 | Loss: 0.00001928
Iteration 97/1000 | Loss: 0.00001928
Iteration 98/1000 | Loss: 0.00001928
Iteration 99/1000 | Loss: 0.00001928
Iteration 100/1000 | Loss: 0.00001928
Iteration 101/1000 | Loss: 0.00001928
Iteration 102/1000 | Loss: 0.00001927
Iteration 103/1000 | Loss: 0.00001927
Iteration 104/1000 | Loss: 0.00001927
Iteration 105/1000 | Loss: 0.00001927
Iteration 106/1000 | Loss: 0.00001927
Iteration 107/1000 | Loss: 0.00001927
Iteration 108/1000 | Loss: 0.00001927
Iteration 109/1000 | Loss: 0.00001927
Iteration 110/1000 | Loss: 0.00001927
Iteration 111/1000 | Loss: 0.00001927
Iteration 112/1000 | Loss: 0.00001927
Iteration 113/1000 | Loss: 0.00001927
Iteration 114/1000 | Loss: 0.00001927
Iteration 115/1000 | Loss: 0.00001927
Iteration 116/1000 | Loss: 0.00001927
Iteration 117/1000 | Loss: 0.00001926
Iteration 118/1000 | Loss: 0.00001926
Iteration 119/1000 | Loss: 0.00001926
Iteration 120/1000 | Loss: 0.00001926
Iteration 121/1000 | Loss: 0.00001925
Iteration 122/1000 | Loss: 0.00001925
Iteration 123/1000 | Loss: 0.00001925
Iteration 124/1000 | Loss: 0.00001925
Iteration 125/1000 | Loss: 0.00001925
Iteration 126/1000 | Loss: 0.00001925
Iteration 127/1000 | Loss: 0.00001925
Iteration 128/1000 | Loss: 0.00001925
Iteration 129/1000 | Loss: 0.00001925
Iteration 130/1000 | Loss: 0.00001925
Iteration 131/1000 | Loss: 0.00001925
Iteration 132/1000 | Loss: 0.00001925
Iteration 133/1000 | Loss: 0.00001924
Iteration 134/1000 | Loss: 0.00001924
Iteration 135/1000 | Loss: 0.00001924
Iteration 136/1000 | Loss: 0.00001924
Iteration 137/1000 | Loss: 0.00001924
Iteration 138/1000 | Loss: 0.00001924
Iteration 139/1000 | Loss: 0.00001924
Iteration 140/1000 | Loss: 0.00001924
Iteration 141/1000 | Loss: 0.00001924
Iteration 142/1000 | Loss: 0.00001924
Iteration 143/1000 | Loss: 0.00001924
Iteration 144/1000 | Loss: 0.00001923
Iteration 145/1000 | Loss: 0.00001923
Iteration 146/1000 | Loss: 0.00001923
Iteration 147/1000 | Loss: 0.00001923
Iteration 148/1000 | Loss: 0.00001923
Iteration 149/1000 | Loss: 0.00001923
Iteration 150/1000 | Loss: 0.00001923
Iteration 151/1000 | Loss: 0.00001923
Iteration 152/1000 | Loss: 0.00001923
Iteration 153/1000 | Loss: 0.00001923
Iteration 154/1000 | Loss: 0.00001923
Iteration 155/1000 | Loss: 0.00001923
Iteration 156/1000 | Loss: 0.00001923
Iteration 157/1000 | Loss: 0.00001923
Iteration 158/1000 | Loss: 0.00001922
Iteration 159/1000 | Loss: 0.00001922
Iteration 160/1000 | Loss: 0.00001922
Iteration 161/1000 | Loss: 0.00001922
Iteration 162/1000 | Loss: 0.00001922
Iteration 163/1000 | Loss: 0.00001922
Iteration 164/1000 | Loss: 0.00001921
Iteration 165/1000 | Loss: 0.00001921
Iteration 166/1000 | Loss: 0.00001921
Iteration 167/1000 | Loss: 0.00001921
Iteration 168/1000 | Loss: 0.00001921
Iteration 169/1000 | Loss: 0.00001921
Iteration 170/1000 | Loss: 0.00001921
Iteration 171/1000 | Loss: 0.00001921
Iteration 172/1000 | Loss: 0.00001921
Iteration 173/1000 | Loss: 0.00001921
Iteration 174/1000 | Loss: 0.00001921
Iteration 175/1000 | Loss: 0.00001921
Iteration 176/1000 | Loss: 0.00001920
Iteration 177/1000 | Loss: 0.00001920
Iteration 178/1000 | Loss: 0.00001920
Iteration 179/1000 | Loss: 0.00001920
Iteration 180/1000 | Loss: 0.00001920
Iteration 181/1000 | Loss: 0.00001920
Iteration 182/1000 | Loss: 0.00001920
Iteration 183/1000 | Loss: 0.00001920
Iteration 184/1000 | Loss: 0.00001920
Iteration 185/1000 | Loss: 0.00001920
Iteration 186/1000 | Loss: 0.00001920
Iteration 187/1000 | Loss: 0.00001920
Iteration 188/1000 | Loss: 0.00001920
Iteration 189/1000 | Loss: 0.00001920
Iteration 190/1000 | Loss: 0.00001920
Iteration 191/1000 | Loss: 0.00001919
Iteration 192/1000 | Loss: 0.00001919
Iteration 193/1000 | Loss: 0.00001919
Iteration 194/1000 | Loss: 0.00001919
Iteration 195/1000 | Loss: 0.00001919
Iteration 196/1000 | Loss: 0.00001919
Iteration 197/1000 | Loss: 0.00001919
Iteration 198/1000 | Loss: 0.00001919
Iteration 199/1000 | Loss: 0.00001919
Iteration 200/1000 | Loss: 0.00001919
Iteration 201/1000 | Loss: 0.00001919
Iteration 202/1000 | Loss: 0.00001919
Iteration 203/1000 | Loss: 0.00001919
Iteration 204/1000 | Loss: 0.00001918
Iteration 205/1000 | Loss: 0.00001918
Iteration 206/1000 | Loss: 0.00001918
Iteration 207/1000 | Loss: 0.00001918
Iteration 208/1000 | Loss: 0.00001918
Iteration 209/1000 | Loss: 0.00001918
Iteration 210/1000 | Loss: 0.00001918
Iteration 211/1000 | Loss: 0.00001918
Iteration 212/1000 | Loss: 0.00001918
Iteration 213/1000 | Loss: 0.00001918
Iteration 214/1000 | Loss: 0.00001918
Iteration 215/1000 | Loss: 0.00001918
Iteration 216/1000 | Loss: 0.00001918
Iteration 217/1000 | Loss: 0.00001918
Iteration 218/1000 | Loss: 0.00001918
Iteration 219/1000 | Loss: 0.00001918
Iteration 220/1000 | Loss: 0.00001918
Iteration 221/1000 | Loss: 0.00001918
Iteration 222/1000 | Loss: 0.00001918
Iteration 223/1000 | Loss: 0.00001918
Iteration 224/1000 | Loss: 0.00001917
Iteration 225/1000 | Loss: 0.00001917
Iteration 226/1000 | Loss: 0.00001917
Iteration 227/1000 | Loss: 0.00001917
Iteration 228/1000 | Loss: 0.00001917
Iteration 229/1000 | Loss: 0.00001917
Iteration 230/1000 | Loss: 0.00001917
Iteration 231/1000 | Loss: 0.00001917
Iteration 232/1000 | Loss: 0.00001917
Iteration 233/1000 | Loss: 0.00001917
Iteration 234/1000 | Loss: 0.00001917
Iteration 235/1000 | Loss: 0.00001917
Iteration 236/1000 | Loss: 0.00001917
Iteration 237/1000 | Loss: 0.00001917
Iteration 238/1000 | Loss: 0.00001917
Iteration 239/1000 | Loss: 0.00001917
Iteration 240/1000 | Loss: 0.00001917
Iteration 241/1000 | Loss: 0.00001917
Iteration 242/1000 | Loss: 0.00001916
Iteration 243/1000 | Loss: 0.00001916
Iteration 244/1000 | Loss: 0.00001916
Iteration 245/1000 | Loss: 0.00001916
Iteration 246/1000 | Loss: 0.00001916
Iteration 247/1000 | Loss: 0.00001916
Iteration 248/1000 | Loss: 0.00001916
Iteration 249/1000 | Loss: 0.00001916
Iteration 250/1000 | Loss: 0.00001916
Iteration 251/1000 | Loss: 0.00001916
Iteration 252/1000 | Loss: 0.00001916
Iteration 253/1000 | Loss: 0.00001916
Iteration 254/1000 | Loss: 0.00001916
Iteration 255/1000 | Loss: 0.00001916
Iteration 256/1000 | Loss: 0.00001916
Iteration 257/1000 | Loss: 0.00001916
Iteration 258/1000 | Loss: 0.00001916
Iteration 259/1000 | Loss: 0.00001916
Iteration 260/1000 | Loss: 0.00001916
Iteration 261/1000 | Loss: 0.00001916
Iteration 262/1000 | Loss: 0.00001916
Iteration 263/1000 | Loss: 0.00001916
Iteration 264/1000 | Loss: 0.00001916
Iteration 265/1000 | Loss: 0.00001915
Iteration 266/1000 | Loss: 0.00001915
Iteration 267/1000 | Loss: 0.00001915
Iteration 268/1000 | Loss: 0.00001915
Iteration 269/1000 | Loss: 0.00001915
Iteration 270/1000 | Loss: 0.00001915
Iteration 271/1000 | Loss: 0.00001915
Iteration 272/1000 | Loss: 0.00001915
Iteration 273/1000 | Loss: 0.00001915
Iteration 274/1000 | Loss: 0.00001915
Iteration 275/1000 | Loss: 0.00001915
Iteration 276/1000 | Loss: 0.00001915
Iteration 277/1000 | Loss: 0.00001915
Iteration 278/1000 | Loss: 0.00001915
Iteration 279/1000 | Loss: 0.00001915
Iteration 280/1000 | Loss: 0.00001915
Iteration 281/1000 | Loss: 0.00001915
Iteration 282/1000 | Loss: 0.00001915
Iteration 283/1000 | Loss: 0.00001915
Iteration 284/1000 | Loss: 0.00001915
Iteration 285/1000 | Loss: 0.00001915
Iteration 286/1000 | Loss: 0.00001915
Iteration 287/1000 | Loss: 0.00001914
Iteration 288/1000 | Loss: 0.00001914
Iteration 289/1000 | Loss: 0.00001914
Iteration 290/1000 | Loss: 0.00001914
Iteration 291/1000 | Loss: 0.00001914
Iteration 292/1000 | Loss: 0.00001914
Iteration 293/1000 | Loss: 0.00001914
Iteration 294/1000 | Loss: 0.00001914
Iteration 295/1000 | Loss: 0.00001914
Iteration 296/1000 | Loss: 0.00001914
Iteration 297/1000 | Loss: 0.00001914
Iteration 298/1000 | Loss: 0.00001914
Iteration 299/1000 | Loss: 0.00001914
Iteration 300/1000 | Loss: 0.00001914
Iteration 301/1000 | Loss: 0.00001914
Iteration 302/1000 | Loss: 0.00001914
Iteration 303/1000 | Loss: 0.00001914
Iteration 304/1000 | Loss: 0.00001914
Iteration 305/1000 | Loss: 0.00001914
Iteration 306/1000 | Loss: 0.00001914
Iteration 307/1000 | Loss: 0.00001914
Iteration 308/1000 | Loss: 0.00001914
Iteration 309/1000 | Loss: 0.00001914
Iteration 310/1000 | Loss: 0.00001914
Iteration 311/1000 | Loss: 0.00001914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 311. Stopping optimization.
Last 5 losses: [1.914127824420575e-05, 1.914127824420575e-05, 1.914127824420575e-05, 1.914127824420575e-05, 1.914127824420575e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.914127824420575e-05

Optimization complete. Final v2v error: 3.6441195011138916 mm

Highest mean error: 4.497915744781494 mm for frame 90

Lowest mean error: 3.1299405097961426 mm for frame 16

Saving results

Total time: 145.03195762634277
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968991
Iteration 2/25 | Loss: 0.00250684
Iteration 3/25 | Loss: 0.00183775
Iteration 4/25 | Loss: 0.00158966
Iteration 5/25 | Loss: 0.00137747
Iteration 6/25 | Loss: 0.00128305
Iteration 7/25 | Loss: 0.00123276
Iteration 8/25 | Loss: 0.00118322
Iteration 9/25 | Loss: 0.00119109
Iteration 10/25 | Loss: 0.00118063
Iteration 11/25 | Loss: 0.00115812
Iteration 12/25 | Loss: 0.00115406
Iteration 13/25 | Loss: 0.00114540
Iteration 14/25 | Loss: 0.00114668
Iteration 15/25 | Loss: 0.00113956
Iteration 16/25 | Loss: 0.00113881
Iteration 17/25 | Loss: 0.00113817
Iteration 18/25 | Loss: 0.00113689
Iteration 19/25 | Loss: 0.00113666
Iteration 20/25 | Loss: 0.00113622
Iteration 21/25 | Loss: 0.00113799
Iteration 22/25 | Loss: 0.00113860
Iteration 23/25 | Loss: 0.00113862
Iteration 24/25 | Loss: 0.00113625
Iteration 25/25 | Loss: 0.00113555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22206187
Iteration 2/25 | Loss: 0.00385422
Iteration 3/25 | Loss: 0.00288159
Iteration 4/25 | Loss: 0.00288138
Iteration 5/25 | Loss: 0.00288137
Iteration 6/25 | Loss: 0.00288137
Iteration 7/25 | Loss: 0.00288137
Iteration 8/25 | Loss: 0.00288137
Iteration 9/25 | Loss: 0.00288137
Iteration 10/25 | Loss: 0.00288137
Iteration 11/25 | Loss: 0.00288137
Iteration 12/25 | Loss: 0.00288137
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0028813746757805347, 0.0028813746757805347, 0.0028813746757805347, 0.0028813746757805347, 0.0028813746757805347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028813746757805347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288137
Iteration 2/1000 | Loss: 0.00077105
Iteration 3/1000 | Loss: 0.00039843
Iteration 4/1000 | Loss: 0.00079694
Iteration 5/1000 | Loss: 0.00029439
Iteration 6/1000 | Loss: 0.00017759
Iteration 7/1000 | Loss: 0.00039504
Iteration 8/1000 | Loss: 0.00077612
Iteration 9/1000 | Loss: 0.00042003
Iteration 10/1000 | Loss: 0.00066303
Iteration 11/1000 | Loss: 0.00049524
Iteration 12/1000 | Loss: 0.00017448
Iteration 13/1000 | Loss: 0.00010579
Iteration 14/1000 | Loss: 0.00008151
Iteration 15/1000 | Loss: 0.00046348
Iteration 16/1000 | Loss: 0.00007099
Iteration 17/1000 | Loss: 0.00019702
Iteration 18/1000 | Loss: 0.00006365
Iteration 19/1000 | Loss: 0.00126830
Iteration 20/1000 | Loss: 0.00009022
Iteration 21/1000 | Loss: 0.00010426
Iteration 22/1000 | Loss: 0.00005870
Iteration 23/1000 | Loss: 0.00048346
Iteration 24/1000 | Loss: 0.00005495
Iteration 25/1000 | Loss: 0.00005371
Iteration 26/1000 | Loss: 0.00005256
Iteration 27/1000 | Loss: 0.00100468
Iteration 28/1000 | Loss: 0.00015964
Iteration 29/1000 | Loss: 0.00019168
Iteration 30/1000 | Loss: 0.00100636
Iteration 31/1000 | Loss: 0.00045435
Iteration 32/1000 | Loss: 0.00011079
Iteration 33/1000 | Loss: 0.00007410
Iteration 34/1000 | Loss: 0.00037366
Iteration 35/1000 | Loss: 0.00051151
Iteration 36/1000 | Loss: 0.00014034
Iteration 37/1000 | Loss: 0.00006482
Iteration 38/1000 | Loss: 0.00018673
Iteration 39/1000 | Loss: 0.00021505
Iteration 40/1000 | Loss: 0.00005376
Iteration 41/1000 | Loss: 0.00016862
Iteration 42/1000 | Loss: 0.00099256
Iteration 43/1000 | Loss: 0.00099815
Iteration 44/1000 | Loss: 0.00013466
Iteration 45/1000 | Loss: 0.00008441
Iteration 46/1000 | Loss: 0.00030974
Iteration 47/1000 | Loss: 0.00005058
Iteration 48/1000 | Loss: 0.00005298
Iteration 49/1000 | Loss: 0.00004373
Iteration 50/1000 | Loss: 0.00021553
Iteration 51/1000 | Loss: 0.00019961
Iteration 52/1000 | Loss: 0.00005366
Iteration 53/1000 | Loss: 0.00005019
Iteration 54/1000 | Loss: 0.00005134
Iteration 55/1000 | Loss: 0.00004845
Iteration 56/1000 | Loss: 0.00024957
Iteration 57/1000 | Loss: 0.00004239
Iteration 58/1000 | Loss: 0.00003971
Iteration 59/1000 | Loss: 0.00003860
Iteration 60/1000 | Loss: 0.00003781
Iteration 61/1000 | Loss: 0.00049456
Iteration 62/1000 | Loss: 0.00037203
Iteration 63/1000 | Loss: 0.00023409
Iteration 64/1000 | Loss: 0.00006631
Iteration 65/1000 | Loss: 0.00003965
Iteration 66/1000 | Loss: 0.00017768
Iteration 67/1000 | Loss: 0.00005124
Iteration 68/1000 | Loss: 0.00004770
Iteration 69/1000 | Loss: 0.00003555
Iteration 70/1000 | Loss: 0.00003484
Iteration 71/1000 | Loss: 0.00003444
Iteration 72/1000 | Loss: 0.00003407
Iteration 73/1000 | Loss: 0.00003379
Iteration 74/1000 | Loss: 0.00003362
Iteration 75/1000 | Loss: 0.00003356
Iteration 76/1000 | Loss: 0.00003351
Iteration 77/1000 | Loss: 0.00003349
Iteration 78/1000 | Loss: 0.00003348
Iteration 79/1000 | Loss: 0.00003345
Iteration 80/1000 | Loss: 0.00003343
Iteration 81/1000 | Loss: 0.00003343
Iteration 82/1000 | Loss: 0.00003340
Iteration 83/1000 | Loss: 0.00003340
Iteration 84/1000 | Loss: 0.00003340
Iteration 85/1000 | Loss: 0.00003339
Iteration 86/1000 | Loss: 0.00003339
Iteration 87/1000 | Loss: 0.00003338
Iteration 88/1000 | Loss: 0.00003338
Iteration 89/1000 | Loss: 0.00003337
Iteration 90/1000 | Loss: 0.00003337
Iteration 91/1000 | Loss: 0.00003337
Iteration 92/1000 | Loss: 0.00003336
Iteration 93/1000 | Loss: 0.00003336
Iteration 94/1000 | Loss: 0.00003336
Iteration 95/1000 | Loss: 0.00003336
Iteration 96/1000 | Loss: 0.00003336
Iteration 97/1000 | Loss: 0.00003336
Iteration 98/1000 | Loss: 0.00003336
Iteration 99/1000 | Loss: 0.00003336
Iteration 100/1000 | Loss: 0.00003336
Iteration 101/1000 | Loss: 0.00003336
Iteration 102/1000 | Loss: 0.00003335
Iteration 103/1000 | Loss: 0.00003335
Iteration 104/1000 | Loss: 0.00003335
Iteration 105/1000 | Loss: 0.00003335
Iteration 106/1000 | Loss: 0.00003334
Iteration 107/1000 | Loss: 0.00003333
Iteration 108/1000 | Loss: 0.00046908
Iteration 109/1000 | Loss: 0.00015288
Iteration 110/1000 | Loss: 0.00003359
Iteration 111/1000 | Loss: 0.00008649
Iteration 112/1000 | Loss: 0.00004410
Iteration 113/1000 | Loss: 0.00004800
Iteration 114/1000 | Loss: 0.00003325
Iteration 115/1000 | Loss: 0.00003311
Iteration 116/1000 | Loss: 0.00003310
Iteration 117/1000 | Loss: 0.00003306
Iteration 118/1000 | Loss: 0.00003306
Iteration 119/1000 | Loss: 0.00003306
Iteration 120/1000 | Loss: 0.00003302
Iteration 121/1000 | Loss: 0.00003302
Iteration 122/1000 | Loss: 0.00003300
Iteration 123/1000 | Loss: 0.00003299
Iteration 124/1000 | Loss: 0.00003299
Iteration 125/1000 | Loss: 0.00003298
Iteration 126/1000 | Loss: 0.00003298
Iteration 127/1000 | Loss: 0.00003297
Iteration 128/1000 | Loss: 0.00003297
Iteration 129/1000 | Loss: 0.00003297
Iteration 130/1000 | Loss: 0.00003297
Iteration 131/1000 | Loss: 0.00003297
Iteration 132/1000 | Loss: 0.00003297
Iteration 133/1000 | Loss: 0.00003297
Iteration 134/1000 | Loss: 0.00003296
Iteration 135/1000 | Loss: 0.00003296
Iteration 136/1000 | Loss: 0.00003296
Iteration 137/1000 | Loss: 0.00003295
Iteration 138/1000 | Loss: 0.00003293
Iteration 139/1000 | Loss: 0.00003293
Iteration 140/1000 | Loss: 0.00003290
Iteration 141/1000 | Loss: 0.00026431
Iteration 142/1000 | Loss: 0.00003650
Iteration 143/1000 | Loss: 0.00003307
Iteration 144/1000 | Loss: 0.00003231
Iteration 145/1000 | Loss: 0.00057456
Iteration 146/1000 | Loss: 0.00011152
Iteration 147/1000 | Loss: 0.00017000
Iteration 148/1000 | Loss: 0.00004220
Iteration 149/1000 | Loss: 0.00003493
Iteration 150/1000 | Loss: 0.00003580
Iteration 151/1000 | Loss: 0.00004350
Iteration 152/1000 | Loss: 0.00002992
Iteration 153/1000 | Loss: 0.00007311
Iteration 154/1000 | Loss: 0.00002934
Iteration 155/1000 | Loss: 0.00002909
Iteration 156/1000 | Loss: 0.00002888
Iteration 157/1000 | Loss: 0.00002886
Iteration 158/1000 | Loss: 0.00002885
Iteration 159/1000 | Loss: 0.00002883
Iteration 160/1000 | Loss: 0.00002882
Iteration 161/1000 | Loss: 0.00002882
Iteration 162/1000 | Loss: 0.00002881
Iteration 163/1000 | Loss: 0.00002881
Iteration 164/1000 | Loss: 0.00002876
Iteration 165/1000 | Loss: 0.00002873
Iteration 166/1000 | Loss: 0.00002869
Iteration 167/1000 | Loss: 0.00002854
Iteration 168/1000 | Loss: 0.00002851
Iteration 169/1000 | Loss: 0.00002844
Iteration 170/1000 | Loss: 0.00030564
Iteration 171/1000 | Loss: 0.00002880
Iteration 172/1000 | Loss: 0.00002842
Iteration 173/1000 | Loss: 0.00002831
Iteration 174/1000 | Loss: 0.00002830
Iteration 175/1000 | Loss: 0.00002829
Iteration 176/1000 | Loss: 0.00002829
Iteration 177/1000 | Loss: 0.00002828
Iteration 178/1000 | Loss: 0.00002828
Iteration 179/1000 | Loss: 0.00002827
Iteration 180/1000 | Loss: 0.00002827
Iteration 181/1000 | Loss: 0.00002826
Iteration 182/1000 | Loss: 0.00002826
Iteration 183/1000 | Loss: 0.00002826
Iteration 184/1000 | Loss: 0.00002823
Iteration 185/1000 | Loss: 0.00002823
Iteration 186/1000 | Loss: 0.00002822
Iteration 187/1000 | Loss: 0.00002821
Iteration 188/1000 | Loss: 0.00002821
Iteration 189/1000 | Loss: 0.00002821
Iteration 190/1000 | Loss: 0.00002820
Iteration 191/1000 | Loss: 0.00002819
Iteration 192/1000 | Loss: 0.00002819
Iteration 193/1000 | Loss: 0.00002818
Iteration 194/1000 | Loss: 0.00002818
Iteration 195/1000 | Loss: 0.00002817
Iteration 196/1000 | Loss: 0.00002817
Iteration 197/1000 | Loss: 0.00002817
Iteration 198/1000 | Loss: 0.00002817
Iteration 199/1000 | Loss: 0.00002817
Iteration 200/1000 | Loss: 0.00002817
Iteration 201/1000 | Loss: 0.00002815
Iteration 202/1000 | Loss: 0.00002812
Iteration 203/1000 | Loss: 0.00002812
Iteration 204/1000 | Loss: 0.00002812
Iteration 205/1000 | Loss: 0.00002811
Iteration 206/1000 | Loss: 0.00002811
Iteration 207/1000 | Loss: 0.00002811
Iteration 208/1000 | Loss: 0.00002810
Iteration 209/1000 | Loss: 0.00002810
Iteration 210/1000 | Loss: 0.00002810
Iteration 211/1000 | Loss: 0.00002809
Iteration 212/1000 | Loss: 0.00002809
Iteration 213/1000 | Loss: 0.00002808
Iteration 214/1000 | Loss: 0.00002808
Iteration 215/1000 | Loss: 0.00002808
Iteration 216/1000 | Loss: 0.00002808
Iteration 217/1000 | Loss: 0.00002808
Iteration 218/1000 | Loss: 0.00002808
Iteration 219/1000 | Loss: 0.00002808
Iteration 220/1000 | Loss: 0.00002807
Iteration 221/1000 | Loss: 0.00002807
Iteration 222/1000 | Loss: 0.00002807
Iteration 223/1000 | Loss: 0.00002806
Iteration 224/1000 | Loss: 0.00002806
Iteration 225/1000 | Loss: 0.00002805
Iteration 226/1000 | Loss: 0.00002805
Iteration 227/1000 | Loss: 0.00002805
Iteration 228/1000 | Loss: 0.00002804
Iteration 229/1000 | Loss: 0.00002800
Iteration 230/1000 | Loss: 0.00002799
Iteration 231/1000 | Loss: 0.00002798
Iteration 232/1000 | Loss: 0.00002798
Iteration 233/1000 | Loss: 0.00002798
Iteration 234/1000 | Loss: 0.00002797
Iteration 235/1000 | Loss: 0.00002797
Iteration 236/1000 | Loss: 0.00002797
Iteration 237/1000 | Loss: 0.00002796
Iteration 238/1000 | Loss: 0.00002796
Iteration 239/1000 | Loss: 0.00002796
Iteration 240/1000 | Loss: 0.00002796
Iteration 241/1000 | Loss: 0.00002796
Iteration 242/1000 | Loss: 0.00002796
Iteration 243/1000 | Loss: 0.00002796
Iteration 244/1000 | Loss: 0.00002796
Iteration 245/1000 | Loss: 0.00002796
Iteration 246/1000 | Loss: 0.00002796
Iteration 247/1000 | Loss: 0.00002796
Iteration 248/1000 | Loss: 0.00002795
Iteration 249/1000 | Loss: 0.00002795
Iteration 250/1000 | Loss: 0.00002795
Iteration 251/1000 | Loss: 0.00002794
Iteration 252/1000 | Loss: 0.00002794
Iteration 253/1000 | Loss: 0.00002794
Iteration 254/1000 | Loss: 0.00002793
Iteration 255/1000 | Loss: 0.00002793
Iteration 256/1000 | Loss: 0.00002793
Iteration 257/1000 | Loss: 0.00002793
Iteration 258/1000 | Loss: 0.00002792
Iteration 259/1000 | Loss: 0.00002792
Iteration 260/1000 | Loss: 0.00002792
Iteration 261/1000 | Loss: 0.00002792
Iteration 262/1000 | Loss: 0.00002792
Iteration 263/1000 | Loss: 0.00002792
Iteration 264/1000 | Loss: 0.00002791
Iteration 265/1000 | Loss: 0.00002791
Iteration 266/1000 | Loss: 0.00002791
Iteration 267/1000 | Loss: 0.00002791
Iteration 268/1000 | Loss: 0.00002791
Iteration 269/1000 | Loss: 0.00002791
Iteration 270/1000 | Loss: 0.00002791
Iteration 271/1000 | Loss: 0.00002791
Iteration 272/1000 | Loss: 0.00002791
Iteration 273/1000 | Loss: 0.00002791
Iteration 274/1000 | Loss: 0.00002791
Iteration 275/1000 | Loss: 0.00002790
Iteration 276/1000 | Loss: 0.00002790
Iteration 277/1000 | Loss: 0.00002790
Iteration 278/1000 | Loss: 0.00002789
Iteration 279/1000 | Loss: 0.00002789
Iteration 280/1000 | Loss: 0.00002789
Iteration 281/1000 | Loss: 0.00002788
Iteration 282/1000 | Loss: 0.00002788
Iteration 283/1000 | Loss: 0.00002788
Iteration 284/1000 | Loss: 0.00002788
Iteration 285/1000 | Loss: 0.00002788
Iteration 286/1000 | Loss: 0.00002788
Iteration 287/1000 | Loss: 0.00002788
Iteration 288/1000 | Loss: 0.00002787
Iteration 289/1000 | Loss: 0.00002787
Iteration 290/1000 | Loss: 0.00002787
Iteration 291/1000 | Loss: 0.00002787
Iteration 292/1000 | Loss: 0.00002787
Iteration 293/1000 | Loss: 0.00002787
Iteration 294/1000 | Loss: 0.00002787
Iteration 295/1000 | Loss: 0.00002787
Iteration 296/1000 | Loss: 0.00002787
Iteration 297/1000 | Loss: 0.00002787
Iteration 298/1000 | Loss: 0.00002787
Iteration 299/1000 | Loss: 0.00002787
Iteration 300/1000 | Loss: 0.00002786
Iteration 301/1000 | Loss: 0.00002786
Iteration 302/1000 | Loss: 0.00002786
Iteration 303/1000 | Loss: 0.00002786
Iteration 304/1000 | Loss: 0.00002786
Iteration 305/1000 | Loss: 0.00002786
Iteration 306/1000 | Loss: 0.00002786
Iteration 307/1000 | Loss: 0.00002785
Iteration 308/1000 | Loss: 0.00002785
Iteration 309/1000 | Loss: 0.00002785
Iteration 310/1000 | Loss: 0.00002785
Iteration 311/1000 | Loss: 0.00002785
Iteration 312/1000 | Loss: 0.00002785
Iteration 313/1000 | Loss: 0.00002785
Iteration 314/1000 | Loss: 0.00002785
Iteration 315/1000 | Loss: 0.00002785
Iteration 316/1000 | Loss: 0.00002785
Iteration 317/1000 | Loss: 0.00002785
Iteration 318/1000 | Loss: 0.00002785
Iteration 319/1000 | Loss: 0.00002785
Iteration 320/1000 | Loss: 0.00002785
Iteration 321/1000 | Loss: 0.00002785
Iteration 322/1000 | Loss: 0.00002785
Iteration 323/1000 | Loss: 0.00002785
Iteration 324/1000 | Loss: 0.00002784
Iteration 325/1000 | Loss: 0.00002784
Iteration 326/1000 | Loss: 0.00002784
Iteration 327/1000 | Loss: 0.00002784
Iteration 328/1000 | Loss: 0.00002784
Iteration 329/1000 | Loss: 0.00002784
Iteration 330/1000 | Loss: 0.00002784
Iteration 331/1000 | Loss: 0.00002784
Iteration 332/1000 | Loss: 0.00002784
Iteration 333/1000 | Loss: 0.00002784
Iteration 334/1000 | Loss: 0.00002784
Iteration 335/1000 | Loss: 0.00002783
Iteration 336/1000 | Loss: 0.00002783
Iteration 337/1000 | Loss: 0.00002783
Iteration 338/1000 | Loss: 0.00002783
Iteration 339/1000 | Loss: 0.00002783
Iteration 340/1000 | Loss: 0.00002783
Iteration 341/1000 | Loss: 0.00002783
Iteration 342/1000 | Loss: 0.00002783
Iteration 343/1000 | Loss: 0.00002783
Iteration 344/1000 | Loss: 0.00002783
Iteration 345/1000 | Loss: 0.00002783
Iteration 346/1000 | Loss: 0.00002783
Iteration 347/1000 | Loss: 0.00002783
Iteration 348/1000 | Loss: 0.00002783
Iteration 349/1000 | Loss: 0.00002783
Iteration 350/1000 | Loss: 0.00002783
Iteration 351/1000 | Loss: 0.00002783
Iteration 352/1000 | Loss: 0.00002783
Iteration 353/1000 | Loss: 0.00002783
Iteration 354/1000 | Loss: 0.00002783
Iteration 355/1000 | Loss: 0.00002783
Iteration 356/1000 | Loss: 0.00002783
Iteration 357/1000 | Loss: 0.00002783
Iteration 358/1000 | Loss: 0.00002783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 358. Stopping optimization.
Last 5 losses: [2.7825633878819644e-05, 2.7825633878819644e-05, 2.7825633878819644e-05, 2.7825633878819644e-05, 2.7825633878819644e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7825633878819644e-05

Optimization complete. Final v2v error: 3.3139472007751465 mm

Highest mean error: 12.669644355773926 mm for frame 131

Lowest mean error: 2.631042718887329 mm for frame 55

Saving results

Total time: 244.11140370368958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00649094
Iteration 2/25 | Loss: 0.00115672
Iteration 3/25 | Loss: 0.00098861
Iteration 4/25 | Loss: 0.00095443
Iteration 5/25 | Loss: 0.00094572
Iteration 6/25 | Loss: 0.00094263
Iteration 7/25 | Loss: 0.00094216
Iteration 8/25 | Loss: 0.00094216
Iteration 9/25 | Loss: 0.00094216
Iteration 10/25 | Loss: 0.00094216
Iteration 11/25 | Loss: 0.00094216
Iteration 12/25 | Loss: 0.00094216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009421593858860433, 0.0009421593858860433, 0.0009421593858860433, 0.0009421593858860433, 0.0009421593858860433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009421593858860433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25736964
Iteration 2/25 | Loss: 0.00196257
Iteration 3/25 | Loss: 0.00196257
Iteration 4/25 | Loss: 0.00196257
Iteration 5/25 | Loss: 0.00196257
Iteration 6/25 | Loss: 0.00196257
Iteration 7/25 | Loss: 0.00196257
Iteration 8/25 | Loss: 0.00196257
Iteration 9/25 | Loss: 0.00196257
Iteration 10/25 | Loss: 0.00196257
Iteration 11/25 | Loss: 0.00196257
Iteration 12/25 | Loss: 0.00196257
Iteration 13/25 | Loss: 0.00196257
Iteration 14/25 | Loss: 0.00196257
Iteration 15/25 | Loss: 0.00196257
Iteration 16/25 | Loss: 0.00196257
Iteration 17/25 | Loss: 0.00196257
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001962570007890463, 0.001962570007890463, 0.001962570007890463, 0.001962570007890463, 0.001962570007890463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001962570007890463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196257
Iteration 2/1000 | Loss: 0.00004031
Iteration 3/1000 | Loss: 0.00002432
Iteration 4/1000 | Loss: 0.00001575
Iteration 5/1000 | Loss: 0.00001382
Iteration 6/1000 | Loss: 0.00001259
Iteration 7/1000 | Loss: 0.00001163
Iteration 8/1000 | Loss: 0.00001119
Iteration 9/1000 | Loss: 0.00001078
Iteration 10/1000 | Loss: 0.00001055
Iteration 11/1000 | Loss: 0.00001054
Iteration 12/1000 | Loss: 0.00001035
Iteration 13/1000 | Loss: 0.00001026
Iteration 14/1000 | Loss: 0.00001015
Iteration 15/1000 | Loss: 0.00001013
Iteration 16/1000 | Loss: 0.00001012
Iteration 17/1000 | Loss: 0.00001011
Iteration 18/1000 | Loss: 0.00001009
Iteration 19/1000 | Loss: 0.00001009
Iteration 20/1000 | Loss: 0.00001008
Iteration 21/1000 | Loss: 0.00001008
Iteration 22/1000 | Loss: 0.00001006
Iteration 23/1000 | Loss: 0.00001005
Iteration 24/1000 | Loss: 0.00001005
Iteration 25/1000 | Loss: 0.00001002
Iteration 26/1000 | Loss: 0.00001001
Iteration 27/1000 | Loss: 0.00001000
Iteration 28/1000 | Loss: 0.00001000
Iteration 29/1000 | Loss: 0.00000999
Iteration 30/1000 | Loss: 0.00000999
Iteration 31/1000 | Loss: 0.00000998
Iteration 32/1000 | Loss: 0.00000997
Iteration 33/1000 | Loss: 0.00000997
Iteration 34/1000 | Loss: 0.00000996
Iteration 35/1000 | Loss: 0.00000995
Iteration 36/1000 | Loss: 0.00000994
Iteration 37/1000 | Loss: 0.00000992
Iteration 38/1000 | Loss: 0.00000991
Iteration 39/1000 | Loss: 0.00000991
Iteration 40/1000 | Loss: 0.00000990
Iteration 41/1000 | Loss: 0.00000988
Iteration 42/1000 | Loss: 0.00000988
Iteration 43/1000 | Loss: 0.00000988
Iteration 44/1000 | Loss: 0.00000987
Iteration 45/1000 | Loss: 0.00000986
Iteration 46/1000 | Loss: 0.00000985
Iteration 47/1000 | Loss: 0.00000985
Iteration 48/1000 | Loss: 0.00000985
Iteration 49/1000 | Loss: 0.00000984
Iteration 50/1000 | Loss: 0.00000984
Iteration 51/1000 | Loss: 0.00000983
Iteration 52/1000 | Loss: 0.00000983
Iteration 53/1000 | Loss: 0.00000982
Iteration 54/1000 | Loss: 0.00000982
Iteration 55/1000 | Loss: 0.00000982
Iteration 56/1000 | Loss: 0.00000981
Iteration 57/1000 | Loss: 0.00000981
Iteration 58/1000 | Loss: 0.00000980
Iteration 59/1000 | Loss: 0.00000980
Iteration 60/1000 | Loss: 0.00000980
Iteration 61/1000 | Loss: 0.00000979
Iteration 62/1000 | Loss: 0.00000979
Iteration 63/1000 | Loss: 0.00000979
Iteration 64/1000 | Loss: 0.00000979
Iteration 65/1000 | Loss: 0.00000979
Iteration 66/1000 | Loss: 0.00000979
Iteration 67/1000 | Loss: 0.00000979
Iteration 68/1000 | Loss: 0.00000978
Iteration 69/1000 | Loss: 0.00000978
Iteration 70/1000 | Loss: 0.00000978
Iteration 71/1000 | Loss: 0.00000978
Iteration 72/1000 | Loss: 0.00000978
Iteration 73/1000 | Loss: 0.00000978
Iteration 74/1000 | Loss: 0.00000977
Iteration 75/1000 | Loss: 0.00000977
Iteration 76/1000 | Loss: 0.00000977
Iteration 77/1000 | Loss: 0.00000977
Iteration 78/1000 | Loss: 0.00000977
Iteration 79/1000 | Loss: 0.00000976
Iteration 80/1000 | Loss: 0.00000976
Iteration 81/1000 | Loss: 0.00000976
Iteration 82/1000 | Loss: 0.00000976
Iteration 83/1000 | Loss: 0.00000976
Iteration 84/1000 | Loss: 0.00000976
Iteration 85/1000 | Loss: 0.00000976
Iteration 86/1000 | Loss: 0.00000976
Iteration 87/1000 | Loss: 0.00000975
Iteration 88/1000 | Loss: 0.00000975
Iteration 89/1000 | Loss: 0.00000975
Iteration 90/1000 | Loss: 0.00000975
Iteration 91/1000 | Loss: 0.00000975
Iteration 92/1000 | Loss: 0.00000974
Iteration 93/1000 | Loss: 0.00000974
Iteration 94/1000 | Loss: 0.00000974
Iteration 95/1000 | Loss: 0.00000974
Iteration 96/1000 | Loss: 0.00000974
Iteration 97/1000 | Loss: 0.00000974
Iteration 98/1000 | Loss: 0.00000974
Iteration 99/1000 | Loss: 0.00000973
Iteration 100/1000 | Loss: 0.00000973
Iteration 101/1000 | Loss: 0.00000973
Iteration 102/1000 | Loss: 0.00000973
Iteration 103/1000 | Loss: 0.00000973
Iteration 104/1000 | Loss: 0.00000973
Iteration 105/1000 | Loss: 0.00000973
Iteration 106/1000 | Loss: 0.00000973
Iteration 107/1000 | Loss: 0.00000973
Iteration 108/1000 | Loss: 0.00000973
Iteration 109/1000 | Loss: 0.00000972
Iteration 110/1000 | Loss: 0.00000972
Iteration 111/1000 | Loss: 0.00000972
Iteration 112/1000 | Loss: 0.00000972
Iteration 113/1000 | Loss: 0.00000972
Iteration 114/1000 | Loss: 0.00000972
Iteration 115/1000 | Loss: 0.00000972
Iteration 116/1000 | Loss: 0.00000972
Iteration 117/1000 | Loss: 0.00000972
Iteration 118/1000 | Loss: 0.00000972
Iteration 119/1000 | Loss: 0.00000972
Iteration 120/1000 | Loss: 0.00000972
Iteration 121/1000 | Loss: 0.00000971
Iteration 122/1000 | Loss: 0.00000971
Iteration 123/1000 | Loss: 0.00000971
Iteration 124/1000 | Loss: 0.00000971
Iteration 125/1000 | Loss: 0.00000970
Iteration 126/1000 | Loss: 0.00000970
Iteration 127/1000 | Loss: 0.00000970
Iteration 128/1000 | Loss: 0.00000970
Iteration 129/1000 | Loss: 0.00000970
Iteration 130/1000 | Loss: 0.00000970
Iteration 131/1000 | Loss: 0.00000970
Iteration 132/1000 | Loss: 0.00000970
Iteration 133/1000 | Loss: 0.00000970
Iteration 134/1000 | Loss: 0.00000970
Iteration 135/1000 | Loss: 0.00000970
Iteration 136/1000 | Loss: 0.00000970
Iteration 137/1000 | Loss: 0.00000969
Iteration 138/1000 | Loss: 0.00000969
Iteration 139/1000 | Loss: 0.00000969
Iteration 140/1000 | Loss: 0.00000969
Iteration 141/1000 | Loss: 0.00000969
Iteration 142/1000 | Loss: 0.00000969
Iteration 143/1000 | Loss: 0.00000969
Iteration 144/1000 | Loss: 0.00000969
Iteration 145/1000 | Loss: 0.00000969
Iteration 146/1000 | Loss: 0.00000969
Iteration 147/1000 | Loss: 0.00000969
Iteration 148/1000 | Loss: 0.00000968
Iteration 149/1000 | Loss: 0.00000968
Iteration 150/1000 | Loss: 0.00000968
Iteration 151/1000 | Loss: 0.00000968
Iteration 152/1000 | Loss: 0.00000968
Iteration 153/1000 | Loss: 0.00000968
Iteration 154/1000 | Loss: 0.00000968
Iteration 155/1000 | Loss: 0.00000968
Iteration 156/1000 | Loss: 0.00000968
Iteration 157/1000 | Loss: 0.00000967
Iteration 158/1000 | Loss: 0.00000967
Iteration 159/1000 | Loss: 0.00000967
Iteration 160/1000 | Loss: 0.00000967
Iteration 161/1000 | Loss: 0.00000967
Iteration 162/1000 | Loss: 0.00000967
Iteration 163/1000 | Loss: 0.00000967
Iteration 164/1000 | Loss: 0.00000967
Iteration 165/1000 | Loss: 0.00000967
Iteration 166/1000 | Loss: 0.00000967
Iteration 167/1000 | Loss: 0.00000967
Iteration 168/1000 | Loss: 0.00000967
Iteration 169/1000 | Loss: 0.00000967
Iteration 170/1000 | Loss: 0.00000967
Iteration 171/1000 | Loss: 0.00000967
Iteration 172/1000 | Loss: 0.00000967
Iteration 173/1000 | Loss: 0.00000967
Iteration 174/1000 | Loss: 0.00000967
Iteration 175/1000 | Loss: 0.00000967
Iteration 176/1000 | Loss: 0.00000967
Iteration 177/1000 | Loss: 0.00000967
Iteration 178/1000 | Loss: 0.00000967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [9.670636245573405e-06, 9.670636245573405e-06, 9.670636245573405e-06, 9.670636245573405e-06, 9.670636245573405e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.670636245573405e-06

Optimization complete. Final v2v error: 2.6287381649017334 mm

Highest mean error: 3.0987610816955566 mm for frame 77

Lowest mean error: 2.149203300476074 mm for frame 137

Saving results

Total time: 40.001136779785156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861211
Iteration 2/25 | Loss: 0.00131594
Iteration 3/25 | Loss: 0.00102284
Iteration 4/25 | Loss: 0.00098984
Iteration 5/25 | Loss: 0.00098487
Iteration 6/25 | Loss: 0.00098417
Iteration 7/25 | Loss: 0.00098417
Iteration 8/25 | Loss: 0.00098417
Iteration 9/25 | Loss: 0.00098417
Iteration 10/25 | Loss: 0.00098417
Iteration 11/25 | Loss: 0.00098417
Iteration 12/25 | Loss: 0.00098417
Iteration 13/25 | Loss: 0.00098417
Iteration 14/25 | Loss: 0.00098417
Iteration 15/25 | Loss: 0.00098417
Iteration 16/25 | Loss: 0.00098417
Iteration 17/25 | Loss: 0.00098417
Iteration 18/25 | Loss: 0.00098417
Iteration 19/25 | Loss: 0.00098417
Iteration 20/25 | Loss: 0.00098417
Iteration 21/25 | Loss: 0.00098417
Iteration 22/25 | Loss: 0.00098417
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009841705905273557, 0.0009841705905273557, 0.0009841705905273557, 0.0009841705905273557, 0.0009841705905273557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009841705905273557

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82744837
Iteration 2/25 | Loss: 0.00076318
Iteration 3/25 | Loss: 0.00076317
Iteration 4/25 | Loss: 0.00076317
Iteration 5/25 | Loss: 0.00076317
Iteration 6/25 | Loss: 0.00076317
Iteration 7/25 | Loss: 0.00076317
Iteration 8/25 | Loss: 0.00076317
Iteration 9/25 | Loss: 0.00076317
Iteration 10/25 | Loss: 0.00076317
Iteration 11/25 | Loss: 0.00076317
Iteration 12/25 | Loss: 0.00076317
Iteration 13/25 | Loss: 0.00076317
Iteration 14/25 | Loss: 0.00076317
Iteration 15/25 | Loss: 0.00076317
Iteration 16/25 | Loss: 0.00076317
Iteration 17/25 | Loss: 0.00076317
Iteration 18/25 | Loss: 0.00076317
Iteration 19/25 | Loss: 0.00076317
Iteration 20/25 | Loss: 0.00076317
Iteration 21/25 | Loss: 0.00076317
Iteration 22/25 | Loss: 0.00076317
Iteration 23/25 | Loss: 0.00076317
Iteration 24/25 | Loss: 0.00076317
Iteration 25/25 | Loss: 0.00076317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076317
Iteration 2/1000 | Loss: 0.00003326
Iteration 3/1000 | Loss: 0.00002403
Iteration 4/1000 | Loss: 0.00002227
Iteration 5/1000 | Loss: 0.00002054
Iteration 6/1000 | Loss: 0.00001981
Iteration 7/1000 | Loss: 0.00001921
Iteration 8/1000 | Loss: 0.00001873
Iteration 9/1000 | Loss: 0.00001835
Iteration 10/1000 | Loss: 0.00001815
Iteration 11/1000 | Loss: 0.00001794
Iteration 12/1000 | Loss: 0.00001784
Iteration 13/1000 | Loss: 0.00001777
Iteration 14/1000 | Loss: 0.00001773
Iteration 15/1000 | Loss: 0.00001772
Iteration 16/1000 | Loss: 0.00001768
Iteration 17/1000 | Loss: 0.00001767
Iteration 18/1000 | Loss: 0.00001767
Iteration 19/1000 | Loss: 0.00001766
Iteration 20/1000 | Loss: 0.00001766
Iteration 21/1000 | Loss: 0.00001766
Iteration 22/1000 | Loss: 0.00001765
Iteration 23/1000 | Loss: 0.00001765
Iteration 24/1000 | Loss: 0.00001764
Iteration 25/1000 | Loss: 0.00001764
Iteration 26/1000 | Loss: 0.00001764
Iteration 27/1000 | Loss: 0.00001764
Iteration 28/1000 | Loss: 0.00001764
Iteration 29/1000 | Loss: 0.00001763
Iteration 30/1000 | Loss: 0.00001763
Iteration 31/1000 | Loss: 0.00001763
Iteration 32/1000 | Loss: 0.00001763
Iteration 33/1000 | Loss: 0.00001763
Iteration 34/1000 | Loss: 0.00001760
Iteration 35/1000 | Loss: 0.00001760
Iteration 36/1000 | Loss: 0.00001760
Iteration 37/1000 | Loss: 0.00001760
Iteration 38/1000 | Loss: 0.00001760
Iteration 39/1000 | Loss: 0.00001759
Iteration 40/1000 | Loss: 0.00001759
Iteration 41/1000 | Loss: 0.00001759
Iteration 42/1000 | Loss: 0.00001759
Iteration 43/1000 | Loss: 0.00001759
Iteration 44/1000 | Loss: 0.00001759
Iteration 45/1000 | Loss: 0.00001759
Iteration 46/1000 | Loss: 0.00001759
Iteration 47/1000 | Loss: 0.00001759
Iteration 48/1000 | Loss: 0.00001759
Iteration 49/1000 | Loss: 0.00001759
Iteration 50/1000 | Loss: 0.00001759
Iteration 51/1000 | Loss: 0.00001759
Iteration 52/1000 | Loss: 0.00001759
Iteration 53/1000 | Loss: 0.00001759
Iteration 54/1000 | Loss: 0.00001759
Iteration 55/1000 | Loss: 0.00001758
Iteration 56/1000 | Loss: 0.00001758
Iteration 57/1000 | Loss: 0.00001758
Iteration 58/1000 | Loss: 0.00001758
Iteration 59/1000 | Loss: 0.00001758
Iteration 60/1000 | Loss: 0.00001758
Iteration 61/1000 | Loss: 0.00001758
Iteration 62/1000 | Loss: 0.00001758
Iteration 63/1000 | Loss: 0.00001758
Iteration 64/1000 | Loss: 0.00001758
Iteration 65/1000 | Loss: 0.00001758
Iteration 66/1000 | Loss: 0.00001758
Iteration 67/1000 | Loss: 0.00001758
Iteration 68/1000 | Loss: 0.00001758
Iteration 69/1000 | Loss: 0.00001758
Iteration 70/1000 | Loss: 0.00001758
Iteration 71/1000 | Loss: 0.00001758
Iteration 72/1000 | Loss: 0.00001758
Iteration 73/1000 | Loss: 0.00001758
Iteration 74/1000 | Loss: 0.00001758
Iteration 75/1000 | Loss: 0.00001758
Iteration 76/1000 | Loss: 0.00001758
Iteration 77/1000 | Loss: 0.00001758
Iteration 78/1000 | Loss: 0.00001758
Iteration 79/1000 | Loss: 0.00001758
Iteration 80/1000 | Loss: 0.00001758
Iteration 81/1000 | Loss: 0.00001758
Iteration 82/1000 | Loss: 0.00001758
Iteration 83/1000 | Loss: 0.00001758
Iteration 84/1000 | Loss: 0.00001758
Iteration 85/1000 | Loss: 0.00001758
Iteration 86/1000 | Loss: 0.00001758
Iteration 87/1000 | Loss: 0.00001758
Iteration 88/1000 | Loss: 0.00001758
Iteration 89/1000 | Loss: 0.00001758
Iteration 90/1000 | Loss: 0.00001758
Iteration 91/1000 | Loss: 0.00001758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.757964855642058e-05, 1.757964855642058e-05, 1.757964855642058e-05, 1.757964855642058e-05, 1.757964855642058e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.757964855642058e-05

Optimization complete. Final v2v error: 3.530550956726074 mm

Highest mean error: 3.761056661605835 mm for frame 66

Lowest mean error: 3.446739435195923 mm for frame 22

Saving results

Total time: 34.940574169158936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00372842
Iteration 2/25 | Loss: 0.00108433
Iteration 3/25 | Loss: 0.00097960
Iteration 4/25 | Loss: 0.00097369
Iteration 5/25 | Loss: 0.00097335
Iteration 6/25 | Loss: 0.00097335
Iteration 7/25 | Loss: 0.00097335
Iteration 8/25 | Loss: 0.00097335
Iteration 9/25 | Loss: 0.00097335
Iteration 10/25 | Loss: 0.00097335
Iteration 11/25 | Loss: 0.00097335
Iteration 12/25 | Loss: 0.00097335
Iteration 13/25 | Loss: 0.00097335
Iteration 14/25 | Loss: 0.00097335
Iteration 15/25 | Loss: 0.00097335
Iteration 16/25 | Loss: 0.00097335
Iteration 17/25 | Loss: 0.00097335
Iteration 18/25 | Loss: 0.00097335
Iteration 19/25 | Loss: 0.00097335
Iteration 20/25 | Loss: 0.00097335
Iteration 21/25 | Loss: 0.00097335
Iteration 22/25 | Loss: 0.00097335
Iteration 23/25 | Loss: 0.00097335
Iteration 24/25 | Loss: 0.00097335
Iteration 25/25 | Loss: 0.00097335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51216364
Iteration 2/25 | Loss: 0.00142403
Iteration 3/25 | Loss: 0.00142403
Iteration 4/25 | Loss: 0.00142403
Iteration 5/25 | Loss: 0.00142403
Iteration 6/25 | Loss: 0.00142402
Iteration 7/25 | Loss: 0.00142402
Iteration 8/25 | Loss: 0.00142402
Iteration 9/25 | Loss: 0.00142402
Iteration 10/25 | Loss: 0.00142402
Iteration 11/25 | Loss: 0.00142402
Iteration 12/25 | Loss: 0.00142402
Iteration 13/25 | Loss: 0.00142402
Iteration 14/25 | Loss: 0.00142402
Iteration 15/25 | Loss: 0.00142402
Iteration 16/25 | Loss: 0.00142402
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014240237651392817, 0.0014240237651392817, 0.0014240237651392817, 0.0014240237651392817, 0.0014240237651392817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014240237651392817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142402
Iteration 2/1000 | Loss: 0.00003725
Iteration 3/1000 | Loss: 0.00002198
Iteration 4/1000 | Loss: 0.00001675
Iteration 5/1000 | Loss: 0.00001494
Iteration 6/1000 | Loss: 0.00001390
Iteration 7/1000 | Loss: 0.00001314
Iteration 8/1000 | Loss: 0.00001268
Iteration 9/1000 | Loss: 0.00001225
Iteration 10/1000 | Loss: 0.00001180
Iteration 11/1000 | Loss: 0.00001138
Iteration 12/1000 | Loss: 0.00001129
Iteration 13/1000 | Loss: 0.00001105
Iteration 14/1000 | Loss: 0.00001089
Iteration 15/1000 | Loss: 0.00001080
Iteration 16/1000 | Loss: 0.00001078
Iteration 17/1000 | Loss: 0.00001069
Iteration 18/1000 | Loss: 0.00001068
Iteration 19/1000 | Loss: 0.00001066
Iteration 20/1000 | Loss: 0.00001066
Iteration 21/1000 | Loss: 0.00001065
Iteration 22/1000 | Loss: 0.00001065
Iteration 23/1000 | Loss: 0.00001061
Iteration 24/1000 | Loss: 0.00001061
Iteration 25/1000 | Loss: 0.00001060
Iteration 26/1000 | Loss: 0.00001056
Iteration 27/1000 | Loss: 0.00001056
Iteration 28/1000 | Loss: 0.00001056
Iteration 29/1000 | Loss: 0.00001055
Iteration 30/1000 | Loss: 0.00001051
Iteration 31/1000 | Loss: 0.00001051
Iteration 32/1000 | Loss: 0.00001048
Iteration 33/1000 | Loss: 0.00001046
Iteration 34/1000 | Loss: 0.00001046
Iteration 35/1000 | Loss: 0.00001046
Iteration 36/1000 | Loss: 0.00001045
Iteration 37/1000 | Loss: 0.00001045
Iteration 38/1000 | Loss: 0.00001045
Iteration 39/1000 | Loss: 0.00001044
Iteration 40/1000 | Loss: 0.00001044
Iteration 41/1000 | Loss: 0.00001043
Iteration 42/1000 | Loss: 0.00001042
Iteration 43/1000 | Loss: 0.00001042
Iteration 44/1000 | Loss: 0.00001042
Iteration 45/1000 | Loss: 0.00001041
Iteration 46/1000 | Loss: 0.00001039
Iteration 47/1000 | Loss: 0.00001039
Iteration 48/1000 | Loss: 0.00001039
Iteration 49/1000 | Loss: 0.00001038
Iteration 50/1000 | Loss: 0.00001038
Iteration 51/1000 | Loss: 0.00001037
Iteration 52/1000 | Loss: 0.00001033
Iteration 53/1000 | Loss: 0.00001033
Iteration 54/1000 | Loss: 0.00001032
Iteration 55/1000 | Loss: 0.00001031
Iteration 56/1000 | Loss: 0.00001031
Iteration 57/1000 | Loss: 0.00001030
Iteration 58/1000 | Loss: 0.00001030
Iteration 59/1000 | Loss: 0.00001029
Iteration 60/1000 | Loss: 0.00001029
Iteration 61/1000 | Loss: 0.00001029
Iteration 62/1000 | Loss: 0.00001029
Iteration 63/1000 | Loss: 0.00001028
Iteration 64/1000 | Loss: 0.00001028
Iteration 65/1000 | Loss: 0.00001027
Iteration 66/1000 | Loss: 0.00001027
Iteration 67/1000 | Loss: 0.00001027
Iteration 68/1000 | Loss: 0.00001027
Iteration 69/1000 | Loss: 0.00001026
Iteration 70/1000 | Loss: 0.00001025
Iteration 71/1000 | Loss: 0.00001025
Iteration 72/1000 | Loss: 0.00001025
Iteration 73/1000 | Loss: 0.00001024
Iteration 74/1000 | Loss: 0.00001024
Iteration 75/1000 | Loss: 0.00001024
Iteration 76/1000 | Loss: 0.00001023
Iteration 77/1000 | Loss: 0.00001023
Iteration 78/1000 | Loss: 0.00001023
Iteration 79/1000 | Loss: 0.00001023
Iteration 80/1000 | Loss: 0.00001022
Iteration 81/1000 | Loss: 0.00001022
Iteration 82/1000 | Loss: 0.00001021
Iteration 83/1000 | Loss: 0.00001021
Iteration 84/1000 | Loss: 0.00001020
Iteration 85/1000 | Loss: 0.00001020
Iteration 86/1000 | Loss: 0.00001019
Iteration 87/1000 | Loss: 0.00001019
Iteration 88/1000 | Loss: 0.00001018
Iteration 89/1000 | Loss: 0.00001018
Iteration 90/1000 | Loss: 0.00001017
Iteration 91/1000 | Loss: 0.00001017
Iteration 92/1000 | Loss: 0.00001017
Iteration 93/1000 | Loss: 0.00001017
Iteration 94/1000 | Loss: 0.00001017
Iteration 95/1000 | Loss: 0.00001017
Iteration 96/1000 | Loss: 0.00001017
Iteration 97/1000 | Loss: 0.00001017
Iteration 98/1000 | Loss: 0.00001016
Iteration 99/1000 | Loss: 0.00001016
Iteration 100/1000 | Loss: 0.00001016
Iteration 101/1000 | Loss: 0.00001016
Iteration 102/1000 | Loss: 0.00001016
Iteration 103/1000 | Loss: 0.00001016
Iteration 104/1000 | Loss: 0.00001016
Iteration 105/1000 | Loss: 0.00001016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.0162971193494741e-05, 1.0162971193494741e-05, 1.0162971193494741e-05, 1.0162971193494741e-05, 1.0162971193494741e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0162971193494741e-05

Optimization complete. Final v2v error: 2.8233654499053955 mm

Highest mean error: 3.1411643028259277 mm for frame 149

Lowest mean error: 2.621058702468872 mm for frame 82

Saving results

Total time: 42.4833607673645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819138
Iteration 2/25 | Loss: 0.00125003
Iteration 3/25 | Loss: 0.00093410
Iteration 4/25 | Loss: 0.00091492
Iteration 5/25 | Loss: 0.00091051
Iteration 6/25 | Loss: 0.00090917
Iteration 7/25 | Loss: 0.00090917
Iteration 8/25 | Loss: 0.00090917
Iteration 9/25 | Loss: 0.00090917
Iteration 10/25 | Loss: 0.00090917
Iteration 11/25 | Loss: 0.00090917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009091737447306514, 0.0009091737447306514, 0.0009091737447306514, 0.0009091737447306514, 0.0009091737447306514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009091737447306514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06609309
Iteration 2/25 | Loss: 0.00131126
Iteration 3/25 | Loss: 0.00131126
Iteration 4/25 | Loss: 0.00131126
Iteration 5/25 | Loss: 0.00131126
Iteration 6/25 | Loss: 0.00131126
Iteration 7/25 | Loss: 0.00131125
Iteration 8/25 | Loss: 0.00131125
Iteration 9/25 | Loss: 0.00131125
Iteration 10/25 | Loss: 0.00131125
Iteration 11/25 | Loss: 0.00131125
Iteration 12/25 | Loss: 0.00131125
Iteration 13/25 | Loss: 0.00131125
Iteration 14/25 | Loss: 0.00131125
Iteration 15/25 | Loss: 0.00131125
Iteration 16/25 | Loss: 0.00131125
Iteration 17/25 | Loss: 0.00131125
Iteration 18/25 | Loss: 0.00131125
Iteration 19/25 | Loss: 0.00131125
Iteration 20/25 | Loss: 0.00131125
Iteration 21/25 | Loss: 0.00131125
Iteration 22/25 | Loss: 0.00131125
Iteration 23/25 | Loss: 0.00131125
Iteration 24/25 | Loss: 0.00131125
Iteration 25/25 | Loss: 0.00131125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131125
Iteration 2/1000 | Loss: 0.00004094
Iteration 3/1000 | Loss: 0.00002673
Iteration 4/1000 | Loss: 0.00002136
Iteration 5/1000 | Loss: 0.00001884
Iteration 6/1000 | Loss: 0.00001760
Iteration 7/1000 | Loss: 0.00001664
Iteration 8/1000 | Loss: 0.00001592
Iteration 9/1000 | Loss: 0.00001546
Iteration 10/1000 | Loss: 0.00001508
Iteration 11/1000 | Loss: 0.00001473
Iteration 12/1000 | Loss: 0.00001450
Iteration 13/1000 | Loss: 0.00001434
Iteration 14/1000 | Loss: 0.00001417
Iteration 15/1000 | Loss: 0.00001415
Iteration 16/1000 | Loss: 0.00001398
Iteration 17/1000 | Loss: 0.00001397
Iteration 18/1000 | Loss: 0.00001396
Iteration 19/1000 | Loss: 0.00001395
Iteration 20/1000 | Loss: 0.00001394
Iteration 21/1000 | Loss: 0.00001388
Iteration 22/1000 | Loss: 0.00001388
Iteration 23/1000 | Loss: 0.00001384
Iteration 24/1000 | Loss: 0.00001383
Iteration 25/1000 | Loss: 0.00001380
Iteration 26/1000 | Loss: 0.00001380
Iteration 27/1000 | Loss: 0.00001377
Iteration 28/1000 | Loss: 0.00001375
Iteration 29/1000 | Loss: 0.00001375
Iteration 30/1000 | Loss: 0.00001375
Iteration 31/1000 | Loss: 0.00001375
Iteration 32/1000 | Loss: 0.00001374
Iteration 33/1000 | Loss: 0.00001374
Iteration 34/1000 | Loss: 0.00001373
Iteration 35/1000 | Loss: 0.00001373
Iteration 36/1000 | Loss: 0.00001372
Iteration 37/1000 | Loss: 0.00001371
Iteration 38/1000 | Loss: 0.00001371
Iteration 39/1000 | Loss: 0.00001370
Iteration 40/1000 | Loss: 0.00001370
Iteration 41/1000 | Loss: 0.00001369
Iteration 42/1000 | Loss: 0.00001368
Iteration 43/1000 | Loss: 0.00001367
Iteration 44/1000 | Loss: 0.00001367
Iteration 45/1000 | Loss: 0.00001366
Iteration 46/1000 | Loss: 0.00001366
Iteration 47/1000 | Loss: 0.00001366
Iteration 48/1000 | Loss: 0.00001366
Iteration 49/1000 | Loss: 0.00001365
Iteration 50/1000 | Loss: 0.00001365
Iteration 51/1000 | Loss: 0.00001365
Iteration 52/1000 | Loss: 0.00001365
Iteration 53/1000 | Loss: 0.00001364
Iteration 54/1000 | Loss: 0.00001364
Iteration 55/1000 | Loss: 0.00001364
Iteration 56/1000 | Loss: 0.00001364
Iteration 57/1000 | Loss: 0.00001364
Iteration 58/1000 | Loss: 0.00001364
Iteration 59/1000 | Loss: 0.00001363
Iteration 60/1000 | Loss: 0.00001363
Iteration 61/1000 | Loss: 0.00001363
Iteration 62/1000 | Loss: 0.00001362
Iteration 63/1000 | Loss: 0.00001362
Iteration 64/1000 | Loss: 0.00001362
Iteration 65/1000 | Loss: 0.00001362
Iteration 66/1000 | Loss: 0.00001362
Iteration 67/1000 | Loss: 0.00001362
Iteration 68/1000 | Loss: 0.00001361
Iteration 69/1000 | Loss: 0.00001361
Iteration 70/1000 | Loss: 0.00001361
Iteration 71/1000 | Loss: 0.00001361
Iteration 72/1000 | Loss: 0.00001361
Iteration 73/1000 | Loss: 0.00001361
Iteration 74/1000 | Loss: 0.00001361
Iteration 75/1000 | Loss: 0.00001361
Iteration 76/1000 | Loss: 0.00001361
Iteration 77/1000 | Loss: 0.00001361
Iteration 78/1000 | Loss: 0.00001361
Iteration 79/1000 | Loss: 0.00001361
Iteration 80/1000 | Loss: 0.00001361
Iteration 81/1000 | Loss: 0.00001361
Iteration 82/1000 | Loss: 0.00001361
Iteration 83/1000 | Loss: 0.00001361
Iteration 84/1000 | Loss: 0.00001361
Iteration 85/1000 | Loss: 0.00001361
Iteration 86/1000 | Loss: 0.00001361
Iteration 87/1000 | Loss: 0.00001361
Iteration 88/1000 | Loss: 0.00001361
Iteration 89/1000 | Loss: 0.00001361
Iteration 90/1000 | Loss: 0.00001361
Iteration 91/1000 | Loss: 0.00001361
Iteration 92/1000 | Loss: 0.00001361
Iteration 93/1000 | Loss: 0.00001361
Iteration 94/1000 | Loss: 0.00001361
Iteration 95/1000 | Loss: 0.00001361
Iteration 96/1000 | Loss: 0.00001361
Iteration 97/1000 | Loss: 0.00001361
Iteration 98/1000 | Loss: 0.00001361
Iteration 99/1000 | Loss: 0.00001361
Iteration 100/1000 | Loss: 0.00001361
Iteration 101/1000 | Loss: 0.00001361
Iteration 102/1000 | Loss: 0.00001361
Iteration 103/1000 | Loss: 0.00001361
Iteration 104/1000 | Loss: 0.00001361
Iteration 105/1000 | Loss: 0.00001361
Iteration 106/1000 | Loss: 0.00001361
Iteration 107/1000 | Loss: 0.00001361
Iteration 108/1000 | Loss: 0.00001361
Iteration 109/1000 | Loss: 0.00001361
Iteration 110/1000 | Loss: 0.00001361
Iteration 111/1000 | Loss: 0.00001361
Iteration 112/1000 | Loss: 0.00001361
Iteration 113/1000 | Loss: 0.00001361
Iteration 114/1000 | Loss: 0.00001361
Iteration 115/1000 | Loss: 0.00001361
Iteration 116/1000 | Loss: 0.00001361
Iteration 117/1000 | Loss: 0.00001361
Iteration 118/1000 | Loss: 0.00001361
Iteration 119/1000 | Loss: 0.00001361
Iteration 120/1000 | Loss: 0.00001361
Iteration 121/1000 | Loss: 0.00001361
Iteration 122/1000 | Loss: 0.00001361
Iteration 123/1000 | Loss: 0.00001361
Iteration 124/1000 | Loss: 0.00001361
Iteration 125/1000 | Loss: 0.00001361
Iteration 126/1000 | Loss: 0.00001361
Iteration 127/1000 | Loss: 0.00001361
Iteration 128/1000 | Loss: 0.00001361
Iteration 129/1000 | Loss: 0.00001361
Iteration 130/1000 | Loss: 0.00001361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.3614136150863487e-05, 1.3614136150863487e-05, 1.3614136150863487e-05, 1.3614136150863487e-05, 1.3614136150863487e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3614136150863487e-05

Optimization complete. Final v2v error: 2.963975667953491 mm

Highest mean error: 4.233434200286865 mm for frame 82

Lowest mean error: 2.2471923828125 mm for frame 197

Saving results

Total time: 43.13336801528931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01145198
Iteration 2/25 | Loss: 0.00187285
Iteration 3/25 | Loss: 0.00137570
Iteration 4/25 | Loss: 0.00117055
Iteration 5/25 | Loss: 0.00112723
Iteration 6/25 | Loss: 0.00111482
Iteration 7/25 | Loss: 0.00111301
Iteration 8/25 | Loss: 0.00110763
Iteration 9/25 | Loss: 0.00110886
Iteration 10/25 | Loss: 0.00110602
Iteration 11/25 | Loss: 0.00110795
Iteration 12/25 | Loss: 0.00110415
Iteration 13/25 | Loss: 0.00110378
Iteration 14/25 | Loss: 0.00110052
Iteration 15/25 | Loss: 0.00110074
Iteration 16/25 | Loss: 0.00109973
Iteration 17/25 | Loss: 0.00109845
Iteration 18/25 | Loss: 0.00109760
Iteration 19/25 | Loss: 0.00109747
Iteration 20/25 | Loss: 0.00109745
Iteration 21/25 | Loss: 0.00109745
Iteration 22/25 | Loss: 0.00109744
Iteration 23/25 | Loss: 0.00109744
Iteration 24/25 | Loss: 0.00109744
Iteration 25/25 | Loss: 0.00109744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81167650
Iteration 2/25 | Loss: 0.00113895
Iteration 3/25 | Loss: 0.00113894
Iteration 4/25 | Loss: 0.00113894
Iteration 5/25 | Loss: 0.00113894
Iteration 6/25 | Loss: 0.00113894
Iteration 7/25 | Loss: 0.00113894
Iteration 8/25 | Loss: 0.00113894
Iteration 9/25 | Loss: 0.00113894
Iteration 10/25 | Loss: 0.00113894
Iteration 11/25 | Loss: 0.00113894
Iteration 12/25 | Loss: 0.00113894
Iteration 13/25 | Loss: 0.00113894
Iteration 14/25 | Loss: 0.00113894
Iteration 15/25 | Loss: 0.00113894
Iteration 16/25 | Loss: 0.00113894
Iteration 17/25 | Loss: 0.00113894
Iteration 18/25 | Loss: 0.00113894
Iteration 19/25 | Loss: 0.00113894
Iteration 20/25 | Loss: 0.00113894
Iteration 21/25 | Loss: 0.00113894
Iteration 22/25 | Loss: 0.00113894
Iteration 23/25 | Loss: 0.00113894
Iteration 24/25 | Loss: 0.00113894
Iteration 25/25 | Loss: 0.00113894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113894
Iteration 2/1000 | Loss: 0.00007975
Iteration 3/1000 | Loss: 0.00006003
Iteration 4/1000 | Loss: 0.00005281
Iteration 5/1000 | Loss: 0.00005033
Iteration 6/1000 | Loss: 0.00004849
Iteration 7/1000 | Loss: 0.00026469
Iteration 8/1000 | Loss: 0.00033825
Iteration 9/1000 | Loss: 0.00005306
Iteration 10/1000 | Loss: 0.00004539
Iteration 11/1000 | Loss: 0.00004269
Iteration 12/1000 | Loss: 0.00004122
Iteration 13/1000 | Loss: 0.00004052
Iteration 14/1000 | Loss: 0.00004005
Iteration 15/1000 | Loss: 0.00003979
Iteration 16/1000 | Loss: 0.00003948
Iteration 17/1000 | Loss: 0.00003922
Iteration 18/1000 | Loss: 0.00003899
Iteration 19/1000 | Loss: 0.00003880
Iteration 20/1000 | Loss: 0.00003864
Iteration 21/1000 | Loss: 0.00003860
Iteration 22/1000 | Loss: 0.00003844
Iteration 23/1000 | Loss: 0.00003834
Iteration 24/1000 | Loss: 0.00003833
Iteration 25/1000 | Loss: 0.00003832
Iteration 26/1000 | Loss: 0.00003823
Iteration 27/1000 | Loss: 0.00003810
Iteration 28/1000 | Loss: 0.00003808
Iteration 29/1000 | Loss: 0.00003808
Iteration 30/1000 | Loss: 0.00003800
Iteration 31/1000 | Loss: 0.00003800
Iteration 32/1000 | Loss: 0.00003793
Iteration 33/1000 | Loss: 0.00003790
Iteration 34/1000 | Loss: 0.00003782
Iteration 35/1000 | Loss: 0.00003778
Iteration 36/1000 | Loss: 0.00003776
Iteration 37/1000 | Loss: 0.00003770
Iteration 38/1000 | Loss: 0.00003769
Iteration 39/1000 | Loss: 0.00003768
Iteration 40/1000 | Loss: 0.00003767
Iteration 41/1000 | Loss: 0.00003766
Iteration 42/1000 | Loss: 0.00003766
Iteration 43/1000 | Loss: 0.00003763
Iteration 44/1000 | Loss: 0.00003762
Iteration 45/1000 | Loss: 0.00003761
Iteration 46/1000 | Loss: 0.00003761
Iteration 47/1000 | Loss: 0.00003760
Iteration 48/1000 | Loss: 0.00003759
Iteration 49/1000 | Loss: 0.00003755
Iteration 50/1000 | Loss: 0.00003755
Iteration 51/1000 | Loss: 0.00003755
Iteration 52/1000 | Loss: 0.00003755
Iteration 53/1000 | Loss: 0.00003755
Iteration 54/1000 | Loss: 0.00003755
Iteration 55/1000 | Loss: 0.00003755
Iteration 56/1000 | Loss: 0.00003755
Iteration 57/1000 | Loss: 0.00003755
Iteration 58/1000 | Loss: 0.00003754
Iteration 59/1000 | Loss: 0.00003754
Iteration 60/1000 | Loss: 0.00003753
Iteration 61/1000 | Loss: 0.00003753
Iteration 62/1000 | Loss: 0.00003753
Iteration 63/1000 | Loss: 0.00003753
Iteration 64/1000 | Loss: 0.00003753
Iteration 65/1000 | Loss: 0.00003752
Iteration 66/1000 | Loss: 0.00003752
Iteration 67/1000 | Loss: 0.00003752
Iteration 68/1000 | Loss: 0.00003752
Iteration 69/1000 | Loss: 0.00003752
Iteration 70/1000 | Loss: 0.00003752
Iteration 71/1000 | Loss: 0.00003752
Iteration 72/1000 | Loss: 0.00003752
Iteration 73/1000 | Loss: 0.00003752
Iteration 74/1000 | Loss: 0.00003752
Iteration 75/1000 | Loss: 0.00003752
Iteration 76/1000 | Loss: 0.00003752
Iteration 77/1000 | Loss: 0.00003751
Iteration 78/1000 | Loss: 0.00003751
Iteration 79/1000 | Loss: 0.00003751
Iteration 80/1000 | Loss: 0.00003751
Iteration 81/1000 | Loss: 0.00003751
Iteration 82/1000 | Loss: 0.00003750
Iteration 83/1000 | Loss: 0.00003750
Iteration 84/1000 | Loss: 0.00003750
Iteration 85/1000 | Loss: 0.00003750
Iteration 86/1000 | Loss: 0.00003750
Iteration 87/1000 | Loss: 0.00003750
Iteration 88/1000 | Loss: 0.00003750
Iteration 89/1000 | Loss: 0.00003750
Iteration 90/1000 | Loss: 0.00003749
Iteration 91/1000 | Loss: 0.00003749
Iteration 92/1000 | Loss: 0.00003749
Iteration 93/1000 | Loss: 0.00003749
Iteration 94/1000 | Loss: 0.00003749
Iteration 95/1000 | Loss: 0.00003749
Iteration 96/1000 | Loss: 0.00003749
Iteration 97/1000 | Loss: 0.00003749
Iteration 98/1000 | Loss: 0.00003749
Iteration 99/1000 | Loss: 0.00003749
Iteration 100/1000 | Loss: 0.00003748
Iteration 101/1000 | Loss: 0.00003748
Iteration 102/1000 | Loss: 0.00003748
Iteration 103/1000 | Loss: 0.00003748
Iteration 104/1000 | Loss: 0.00003748
Iteration 105/1000 | Loss: 0.00003748
Iteration 106/1000 | Loss: 0.00003748
Iteration 107/1000 | Loss: 0.00003748
Iteration 108/1000 | Loss: 0.00003748
Iteration 109/1000 | Loss: 0.00003748
Iteration 110/1000 | Loss: 0.00003748
Iteration 111/1000 | Loss: 0.00003748
Iteration 112/1000 | Loss: 0.00003748
Iteration 113/1000 | Loss: 0.00003748
Iteration 114/1000 | Loss: 0.00003748
Iteration 115/1000 | Loss: 0.00003748
Iteration 116/1000 | Loss: 0.00003748
Iteration 117/1000 | Loss: 0.00003748
Iteration 118/1000 | Loss: 0.00003748
Iteration 119/1000 | Loss: 0.00003748
Iteration 120/1000 | Loss: 0.00003748
Iteration 121/1000 | Loss: 0.00003748
Iteration 122/1000 | Loss: 0.00003748
Iteration 123/1000 | Loss: 0.00003748
Iteration 124/1000 | Loss: 0.00003748
Iteration 125/1000 | Loss: 0.00003748
Iteration 126/1000 | Loss: 0.00003748
Iteration 127/1000 | Loss: 0.00003748
Iteration 128/1000 | Loss: 0.00003748
Iteration 129/1000 | Loss: 0.00003748
Iteration 130/1000 | Loss: 0.00003748
Iteration 131/1000 | Loss: 0.00003748
Iteration 132/1000 | Loss: 0.00003748
Iteration 133/1000 | Loss: 0.00003748
Iteration 134/1000 | Loss: 0.00003748
Iteration 135/1000 | Loss: 0.00003748
Iteration 136/1000 | Loss: 0.00003748
Iteration 137/1000 | Loss: 0.00003748
Iteration 138/1000 | Loss: 0.00003748
Iteration 139/1000 | Loss: 0.00003748
Iteration 140/1000 | Loss: 0.00003748
Iteration 141/1000 | Loss: 0.00003748
Iteration 142/1000 | Loss: 0.00003748
Iteration 143/1000 | Loss: 0.00003748
Iteration 144/1000 | Loss: 0.00003748
Iteration 145/1000 | Loss: 0.00003748
Iteration 146/1000 | Loss: 0.00003748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [3.748334347619675e-05, 3.748334347619675e-05, 3.748334347619675e-05, 3.748334347619675e-05, 3.748334347619675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.748334347619675e-05

Optimization complete. Final v2v error: 4.809664249420166 mm

Highest mean error: 5.375049114227295 mm for frame 74

Lowest mean error: 4.162647247314453 mm for frame 144

Saving results

Total time: 91.87506675720215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393031
Iteration 2/25 | Loss: 0.00101615
Iteration 3/25 | Loss: 0.00094385
Iteration 4/25 | Loss: 0.00093462
Iteration 5/25 | Loss: 0.00093242
Iteration 6/25 | Loss: 0.00093179
Iteration 7/25 | Loss: 0.00093179
Iteration 8/25 | Loss: 0.00093179
Iteration 9/25 | Loss: 0.00093179
Iteration 10/25 | Loss: 0.00093179
Iteration 11/25 | Loss: 0.00093179
Iteration 12/25 | Loss: 0.00093179
Iteration 13/25 | Loss: 0.00093179
Iteration 14/25 | Loss: 0.00093179
Iteration 15/25 | Loss: 0.00093179
Iteration 16/25 | Loss: 0.00093179
Iteration 17/25 | Loss: 0.00093179
Iteration 18/25 | Loss: 0.00093179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009317884687334299, 0.0009317884687334299, 0.0009317884687334299, 0.0009317884687334299, 0.0009317884687334299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009317884687334299

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22325504
Iteration 2/25 | Loss: 0.00135222
Iteration 3/25 | Loss: 0.00135222
Iteration 4/25 | Loss: 0.00135221
Iteration 5/25 | Loss: 0.00135221
Iteration 6/25 | Loss: 0.00135221
Iteration 7/25 | Loss: 0.00135221
Iteration 8/25 | Loss: 0.00135221
Iteration 9/25 | Loss: 0.00135221
Iteration 10/25 | Loss: 0.00135221
Iteration 11/25 | Loss: 0.00135221
Iteration 12/25 | Loss: 0.00135221
Iteration 13/25 | Loss: 0.00135221
Iteration 14/25 | Loss: 0.00135221
Iteration 15/25 | Loss: 0.00135221
Iteration 16/25 | Loss: 0.00135221
Iteration 17/25 | Loss: 0.00135221
Iteration 18/25 | Loss: 0.00135221
Iteration 19/25 | Loss: 0.00135221
Iteration 20/25 | Loss: 0.00135221
Iteration 21/25 | Loss: 0.00135221
Iteration 22/25 | Loss: 0.00135221
Iteration 23/25 | Loss: 0.00135221
Iteration 24/25 | Loss: 0.00135221
Iteration 25/25 | Loss: 0.00135221

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135221
Iteration 2/1000 | Loss: 0.00002037
Iteration 3/1000 | Loss: 0.00001332
Iteration 4/1000 | Loss: 0.00001199
Iteration 5/1000 | Loss: 0.00001099
Iteration 6/1000 | Loss: 0.00001050
Iteration 7/1000 | Loss: 0.00000997
Iteration 8/1000 | Loss: 0.00000969
Iteration 9/1000 | Loss: 0.00000936
Iteration 10/1000 | Loss: 0.00000933
Iteration 11/1000 | Loss: 0.00000928
Iteration 12/1000 | Loss: 0.00000925
Iteration 13/1000 | Loss: 0.00000921
Iteration 14/1000 | Loss: 0.00000920
Iteration 15/1000 | Loss: 0.00000919
Iteration 16/1000 | Loss: 0.00000918
Iteration 17/1000 | Loss: 0.00000917
Iteration 18/1000 | Loss: 0.00000905
Iteration 19/1000 | Loss: 0.00000903
Iteration 20/1000 | Loss: 0.00000892
Iteration 21/1000 | Loss: 0.00000888
Iteration 22/1000 | Loss: 0.00000888
Iteration 23/1000 | Loss: 0.00000885
Iteration 24/1000 | Loss: 0.00000884
Iteration 25/1000 | Loss: 0.00000883
Iteration 26/1000 | Loss: 0.00000883
Iteration 27/1000 | Loss: 0.00000882
Iteration 28/1000 | Loss: 0.00000882
Iteration 29/1000 | Loss: 0.00000881
Iteration 30/1000 | Loss: 0.00000881
Iteration 31/1000 | Loss: 0.00000880
Iteration 32/1000 | Loss: 0.00000880
Iteration 33/1000 | Loss: 0.00000879
Iteration 34/1000 | Loss: 0.00000879
Iteration 35/1000 | Loss: 0.00000878
Iteration 36/1000 | Loss: 0.00000877
Iteration 37/1000 | Loss: 0.00000876
Iteration 38/1000 | Loss: 0.00000875
Iteration 39/1000 | Loss: 0.00000874
Iteration 40/1000 | Loss: 0.00000873
Iteration 41/1000 | Loss: 0.00000873
Iteration 42/1000 | Loss: 0.00000871
Iteration 43/1000 | Loss: 0.00000870
Iteration 44/1000 | Loss: 0.00000870
Iteration 45/1000 | Loss: 0.00000870
Iteration 46/1000 | Loss: 0.00000869
Iteration 47/1000 | Loss: 0.00000869
Iteration 48/1000 | Loss: 0.00000869
Iteration 49/1000 | Loss: 0.00000868
Iteration 50/1000 | Loss: 0.00000868
Iteration 51/1000 | Loss: 0.00000866
Iteration 52/1000 | Loss: 0.00000866
Iteration 53/1000 | Loss: 0.00000866
Iteration 54/1000 | Loss: 0.00000866
Iteration 55/1000 | Loss: 0.00000865
Iteration 56/1000 | Loss: 0.00000865
Iteration 57/1000 | Loss: 0.00000865
Iteration 58/1000 | Loss: 0.00000865
Iteration 59/1000 | Loss: 0.00000864
Iteration 60/1000 | Loss: 0.00000864
Iteration 61/1000 | Loss: 0.00000864
Iteration 62/1000 | Loss: 0.00000864
Iteration 63/1000 | Loss: 0.00000863
Iteration 64/1000 | Loss: 0.00000863
Iteration 65/1000 | Loss: 0.00000863
Iteration 66/1000 | Loss: 0.00000863
Iteration 67/1000 | Loss: 0.00000863
Iteration 68/1000 | Loss: 0.00000863
Iteration 69/1000 | Loss: 0.00000863
Iteration 70/1000 | Loss: 0.00000863
Iteration 71/1000 | Loss: 0.00000863
Iteration 72/1000 | Loss: 0.00000863
Iteration 73/1000 | Loss: 0.00000863
Iteration 74/1000 | Loss: 0.00000863
Iteration 75/1000 | Loss: 0.00000863
Iteration 76/1000 | Loss: 0.00000863
Iteration 77/1000 | Loss: 0.00000863
Iteration 78/1000 | Loss: 0.00000863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [8.628134310129099e-06, 8.628134310129099e-06, 8.628134310129099e-06, 8.628134310129099e-06, 8.628134310129099e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.628134310129099e-06

Optimization complete. Final v2v error: 2.559173583984375 mm

Highest mean error: 2.7501585483551025 mm for frame 18

Lowest mean error: 2.3525123596191406 mm for frame 47

Saving results

Total time: 28.6782705783844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075155
Iteration 2/25 | Loss: 0.01075155
Iteration 3/25 | Loss: 0.01075155
Iteration 4/25 | Loss: 0.01075154
Iteration 5/25 | Loss: 0.01075154
Iteration 6/25 | Loss: 0.01075154
Iteration 7/25 | Loss: 0.01075154
Iteration 8/25 | Loss: 0.01075154
Iteration 9/25 | Loss: 0.01075154
Iteration 10/25 | Loss: 0.01075154
Iteration 11/25 | Loss: 0.01075154
Iteration 12/25 | Loss: 0.01075154
Iteration 13/25 | Loss: 0.01075153
Iteration 14/25 | Loss: 0.01075153
Iteration 15/25 | Loss: 0.01075153
Iteration 16/25 | Loss: 0.01075153
Iteration 17/25 | Loss: 0.01075153
Iteration 18/25 | Loss: 0.01075153
Iteration 19/25 | Loss: 0.01075153
Iteration 20/25 | Loss: 0.01075153
Iteration 21/25 | Loss: 0.01075153
Iteration 22/25 | Loss: 0.01075152
Iteration 23/25 | Loss: 0.01075152
Iteration 24/25 | Loss: 0.01075152
Iteration 25/25 | Loss: 0.01075152

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37902069
Iteration 2/25 | Loss: 0.12200222
Iteration 3/25 | Loss: 0.12169115
Iteration 4/25 | Loss: 0.11740652
Iteration 5/25 | Loss: 0.11740651
Iteration 6/25 | Loss: 0.11740650
Iteration 7/25 | Loss: 0.11740649
Iteration 8/25 | Loss: 0.11740649
Iteration 9/25 | Loss: 0.11740649
Iteration 10/25 | Loss: 0.11740649
Iteration 11/25 | Loss: 0.11740649
Iteration 12/25 | Loss: 0.11740648
Iteration 13/25 | Loss: 0.11740648
Iteration 14/25 | Loss: 0.11740648
Iteration 15/25 | Loss: 0.11740648
Iteration 16/25 | Loss: 0.11740648
Iteration 17/25 | Loss: 0.11740648
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.11740648001432419, 0.11740648001432419, 0.11740648001432419, 0.11740648001432419, 0.11740648001432419]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11740648001432419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11740648
Iteration 2/1000 | Loss: 0.00108604
Iteration 3/1000 | Loss: 0.00085434
Iteration 4/1000 | Loss: 0.00022700
Iteration 5/1000 | Loss: 0.00006341
Iteration 6/1000 | Loss: 0.00003826
Iteration 7/1000 | Loss: 0.00002692
Iteration 8/1000 | Loss: 0.00002038
Iteration 9/1000 | Loss: 0.00001647
Iteration 10/1000 | Loss: 0.00001389
Iteration 11/1000 | Loss: 0.00001181
Iteration 12/1000 | Loss: 0.00001061
Iteration 13/1000 | Loss: 0.00000975
Iteration 14/1000 | Loss: 0.00000910
Iteration 15/1000 | Loss: 0.00000851
Iteration 16/1000 | Loss: 0.00000800
Iteration 17/1000 | Loss: 0.00000766
Iteration 18/1000 | Loss: 0.00000748
Iteration 19/1000 | Loss: 0.00000745
Iteration 20/1000 | Loss: 0.00000728
Iteration 21/1000 | Loss: 0.00000712
Iteration 22/1000 | Loss: 0.00000705
Iteration 23/1000 | Loss: 0.00000696
Iteration 24/1000 | Loss: 0.00000692
Iteration 25/1000 | Loss: 0.00000690
Iteration 26/1000 | Loss: 0.00000687
Iteration 27/1000 | Loss: 0.00000686
Iteration 28/1000 | Loss: 0.00000684
Iteration 29/1000 | Loss: 0.00000680
Iteration 30/1000 | Loss: 0.00000679
Iteration 31/1000 | Loss: 0.00000674
Iteration 32/1000 | Loss: 0.00000671
Iteration 33/1000 | Loss: 0.00000668
Iteration 34/1000 | Loss: 0.00000667
Iteration 35/1000 | Loss: 0.00000665
Iteration 36/1000 | Loss: 0.00000657
Iteration 37/1000 | Loss: 0.00000656
Iteration 38/1000 | Loss: 0.00000655
Iteration 39/1000 | Loss: 0.00000655
Iteration 40/1000 | Loss: 0.00000655
Iteration 41/1000 | Loss: 0.00000654
Iteration 42/1000 | Loss: 0.00000653
Iteration 43/1000 | Loss: 0.00000653
Iteration 44/1000 | Loss: 0.00000653
Iteration 45/1000 | Loss: 0.00000653
Iteration 46/1000 | Loss: 0.00000652
Iteration 47/1000 | Loss: 0.00000652
Iteration 48/1000 | Loss: 0.00000651
Iteration 49/1000 | Loss: 0.00000651
Iteration 50/1000 | Loss: 0.00000650
Iteration 51/1000 | Loss: 0.00000650
Iteration 52/1000 | Loss: 0.00000650
Iteration 53/1000 | Loss: 0.00000649
Iteration 54/1000 | Loss: 0.00000649
Iteration 55/1000 | Loss: 0.00000649
Iteration 56/1000 | Loss: 0.00000648
Iteration 57/1000 | Loss: 0.00000648
Iteration 58/1000 | Loss: 0.00000648
Iteration 59/1000 | Loss: 0.00000648
Iteration 60/1000 | Loss: 0.00000647
Iteration 61/1000 | Loss: 0.00000647
Iteration 62/1000 | Loss: 0.00000646
Iteration 63/1000 | Loss: 0.00000646
Iteration 64/1000 | Loss: 0.00000645
Iteration 65/1000 | Loss: 0.00000645
Iteration 66/1000 | Loss: 0.00000645
Iteration 67/1000 | Loss: 0.00000644
Iteration 68/1000 | Loss: 0.00000644
Iteration 69/1000 | Loss: 0.00000643
Iteration 70/1000 | Loss: 0.00000643
Iteration 71/1000 | Loss: 0.00000643
Iteration 72/1000 | Loss: 0.00000643
Iteration 73/1000 | Loss: 0.00000643
Iteration 74/1000 | Loss: 0.00000643
Iteration 75/1000 | Loss: 0.00000643
Iteration 76/1000 | Loss: 0.00000642
Iteration 77/1000 | Loss: 0.00000641
Iteration 78/1000 | Loss: 0.00000641
Iteration 79/1000 | Loss: 0.00000641
Iteration 80/1000 | Loss: 0.00000639
Iteration 81/1000 | Loss: 0.00000639
Iteration 82/1000 | Loss: 0.00000639
Iteration 83/1000 | Loss: 0.00000639
Iteration 84/1000 | Loss: 0.00000639
Iteration 85/1000 | Loss: 0.00000638
Iteration 86/1000 | Loss: 0.00000638
Iteration 87/1000 | Loss: 0.00000638
Iteration 88/1000 | Loss: 0.00000638
Iteration 89/1000 | Loss: 0.00000637
Iteration 90/1000 | Loss: 0.00000637
Iteration 91/1000 | Loss: 0.00000637
Iteration 92/1000 | Loss: 0.00000636
Iteration 93/1000 | Loss: 0.00000636
Iteration 94/1000 | Loss: 0.00000636
Iteration 95/1000 | Loss: 0.00000635
Iteration 96/1000 | Loss: 0.00000635
Iteration 97/1000 | Loss: 0.00000635
Iteration 98/1000 | Loss: 0.00000634
Iteration 99/1000 | Loss: 0.00000634
Iteration 100/1000 | Loss: 0.00000634
Iteration 101/1000 | Loss: 0.00000634
Iteration 102/1000 | Loss: 0.00000633
Iteration 103/1000 | Loss: 0.00000633
Iteration 104/1000 | Loss: 0.00000632
Iteration 105/1000 | Loss: 0.00000632
Iteration 106/1000 | Loss: 0.00000632
Iteration 107/1000 | Loss: 0.00000631
Iteration 108/1000 | Loss: 0.00000631
Iteration 109/1000 | Loss: 0.00000631
Iteration 110/1000 | Loss: 0.00000631
Iteration 111/1000 | Loss: 0.00000631
Iteration 112/1000 | Loss: 0.00000631
Iteration 113/1000 | Loss: 0.00000631
Iteration 114/1000 | Loss: 0.00000631
Iteration 115/1000 | Loss: 0.00000631
Iteration 116/1000 | Loss: 0.00000631
Iteration 117/1000 | Loss: 0.00000631
Iteration 118/1000 | Loss: 0.00000631
Iteration 119/1000 | Loss: 0.00000631
Iteration 120/1000 | Loss: 0.00000631
Iteration 121/1000 | Loss: 0.00000631
Iteration 122/1000 | Loss: 0.00000631
Iteration 123/1000 | Loss: 0.00000631
Iteration 124/1000 | Loss: 0.00000631
Iteration 125/1000 | Loss: 0.00000631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [6.308918727881974e-06, 6.308918727881974e-06, 6.308918727881974e-06, 6.308918727881974e-06, 6.308918727881974e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.308918727881974e-06

Optimization complete. Final v2v error: 2.150641918182373 mm

Highest mean error: 8.820178985595703 mm for frame 144

Lowest mean error: 1.9604915380477905 mm for frame 239

Saving results

Total time: 55.06765270233154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061354
Iteration 2/25 | Loss: 0.00413437
Iteration 3/25 | Loss: 0.00298070
Iteration 4/25 | Loss: 0.00249809
Iteration 5/25 | Loss: 0.00204381
Iteration 6/25 | Loss: 0.00188043
Iteration 7/25 | Loss: 0.00182787
Iteration 8/25 | Loss: 0.00181335
Iteration 9/25 | Loss: 0.00180384
Iteration 10/25 | Loss: 0.00178103
Iteration 11/25 | Loss: 0.00177157
Iteration 12/25 | Loss: 0.00174724
Iteration 13/25 | Loss: 0.00173695
Iteration 14/25 | Loss: 0.00172683
Iteration 15/25 | Loss: 0.00172504
Iteration 16/25 | Loss: 0.00172499
Iteration 17/25 | Loss: 0.00171311
Iteration 18/25 | Loss: 0.00170929
Iteration 19/25 | Loss: 0.00170892
Iteration 20/25 | Loss: 0.00170804
Iteration 21/25 | Loss: 0.00170729
Iteration 22/25 | Loss: 0.00170660
Iteration 23/25 | Loss: 0.00170648
Iteration 24/25 | Loss: 0.00170906
Iteration 25/25 | Loss: 0.00170868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22687721
Iteration 2/25 | Loss: 0.01028686
Iteration 3/25 | Loss: 0.01028686
Iteration 4/25 | Loss: 0.01231599
Iteration 5/25 | Loss: 0.01152309
Iteration 6/25 | Loss: 0.00878553
Iteration 7/25 | Loss: 0.00878478
Iteration 8/25 | Loss: 0.00878477
Iteration 9/25 | Loss: 0.00878477
Iteration 10/25 | Loss: 0.00878477
Iteration 11/25 | Loss: 0.00878477
Iteration 12/25 | Loss: 0.00878477
Iteration 13/25 | Loss: 0.00878477
Iteration 14/25 | Loss: 0.00878477
Iteration 15/25 | Loss: 0.00878477
Iteration 16/25 | Loss: 0.00878477
Iteration 17/25 | Loss: 0.00878477
Iteration 18/25 | Loss: 0.00878477
Iteration 19/25 | Loss: 0.00878477
Iteration 20/25 | Loss: 0.00878477
Iteration 21/25 | Loss: 0.00878477
Iteration 22/25 | Loss: 0.00878477
Iteration 23/25 | Loss: 0.00878477
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.008784772828221321, 0.008784772828221321, 0.008784772828221321, 0.008784772828221321, 0.008784772828221321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008784772828221321

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00878477
Iteration 2/1000 | Loss: 0.00200228
Iteration 3/1000 | Loss: 0.00124289
Iteration 4/1000 | Loss: 0.00070747
Iteration 5/1000 | Loss: 0.00070055
Iteration 6/1000 | Loss: 0.00103066
Iteration 7/1000 | Loss: 0.00536570
Iteration 8/1000 | Loss: 0.00071837
Iteration 9/1000 | Loss: 0.00060761
Iteration 10/1000 | Loss: 0.00070585
Iteration 11/1000 | Loss: 0.00068841
Iteration 12/1000 | Loss: 0.00061190
Iteration 13/1000 | Loss: 0.00078565
Iteration 14/1000 | Loss: 0.00059698
Iteration 15/1000 | Loss: 0.00083525
Iteration 16/1000 | Loss: 0.00053662
Iteration 17/1000 | Loss: 0.00068708
Iteration 18/1000 | Loss: 0.00050904
Iteration 19/1000 | Loss: 0.00055664
Iteration 20/1000 | Loss: 0.00053302
Iteration 21/1000 | Loss: 0.00064040
Iteration 22/1000 | Loss: 0.00046624
Iteration 23/1000 | Loss: 0.00144973
Iteration 24/1000 | Loss: 0.00052262
Iteration 25/1000 | Loss: 0.00046520
Iteration 26/1000 | Loss: 0.00049330
Iteration 27/1000 | Loss: 0.00044883
Iteration 28/1000 | Loss: 0.00050234
Iteration 29/1000 | Loss: 0.00256051
Iteration 30/1000 | Loss: 0.00219694
Iteration 31/1000 | Loss: 0.00043384
Iteration 32/1000 | Loss: 0.00050207
Iteration 33/1000 | Loss: 0.00225533
Iteration 34/1000 | Loss: 0.00161993
Iteration 35/1000 | Loss: 0.00262765
Iteration 36/1000 | Loss: 0.00080747
Iteration 37/1000 | Loss: 0.00368508
Iteration 38/1000 | Loss: 0.00220491
Iteration 39/1000 | Loss: 0.00358283
Iteration 40/1000 | Loss: 0.00090294
Iteration 41/1000 | Loss: 0.00062767
Iteration 42/1000 | Loss: 0.00051915
Iteration 43/1000 | Loss: 0.00040770
Iteration 44/1000 | Loss: 0.00047472
Iteration 45/1000 | Loss: 0.00054086
Iteration 46/1000 | Loss: 0.00045160
Iteration 47/1000 | Loss: 0.00056262
Iteration 48/1000 | Loss: 0.00070693
Iteration 49/1000 | Loss: 0.00045316
Iteration 50/1000 | Loss: 0.00039599
Iteration 51/1000 | Loss: 0.00037512
Iteration 52/1000 | Loss: 0.00054889
Iteration 53/1000 | Loss: 0.00081507
Iteration 54/1000 | Loss: 0.00040139
Iteration 55/1000 | Loss: 0.00038162
Iteration 56/1000 | Loss: 0.00040228
Iteration 57/1000 | Loss: 0.00038419
Iteration 58/1000 | Loss: 0.00037074
Iteration 59/1000 | Loss: 0.00051668
Iteration 60/1000 | Loss: 0.00037381
Iteration 61/1000 | Loss: 0.00041958
Iteration 62/1000 | Loss: 0.00053401
Iteration 63/1000 | Loss: 0.00036860
Iteration 64/1000 | Loss: 0.00040852
Iteration 65/1000 | Loss: 0.00036017
Iteration 66/1000 | Loss: 0.00042070
Iteration 67/1000 | Loss: 0.00066351
Iteration 68/1000 | Loss: 0.00035186
Iteration 69/1000 | Loss: 0.00057666
Iteration 70/1000 | Loss: 0.00053036
Iteration 71/1000 | Loss: 0.00036379
Iteration 72/1000 | Loss: 0.00038456
Iteration 73/1000 | Loss: 0.00057149
Iteration 74/1000 | Loss: 0.00042141
Iteration 75/1000 | Loss: 0.00046424
Iteration 76/1000 | Loss: 0.00032634
Iteration 77/1000 | Loss: 0.00036638
Iteration 78/1000 | Loss: 0.00045507
Iteration 79/1000 | Loss: 0.00031794
Iteration 80/1000 | Loss: 0.00049261
Iteration 81/1000 | Loss: 0.00031357
Iteration 82/1000 | Loss: 0.00054061
Iteration 83/1000 | Loss: 0.00031310
Iteration 84/1000 | Loss: 0.00049429
Iteration 85/1000 | Loss: 0.00035504
Iteration 86/1000 | Loss: 0.00030806
Iteration 87/1000 | Loss: 0.00030699
Iteration 88/1000 | Loss: 0.00030627
Iteration 89/1000 | Loss: 0.00032874
Iteration 90/1000 | Loss: 0.00038432
Iteration 91/1000 | Loss: 0.00033949
Iteration 92/1000 | Loss: 0.00030532
Iteration 93/1000 | Loss: 0.00030436
Iteration 94/1000 | Loss: 0.00030406
Iteration 95/1000 | Loss: 0.00032254
Iteration 96/1000 | Loss: 0.00030367
Iteration 97/1000 | Loss: 0.00030348
Iteration 98/1000 | Loss: 0.00030328
Iteration 99/1000 | Loss: 0.00030323
Iteration 100/1000 | Loss: 0.00030316
Iteration 101/1000 | Loss: 0.00032631
Iteration 102/1000 | Loss: 0.00032631
Iteration 103/1000 | Loss: 0.00053306
Iteration 104/1000 | Loss: 0.00039598
Iteration 105/1000 | Loss: 0.00030579
Iteration 106/1000 | Loss: 0.00037374
Iteration 107/1000 | Loss: 0.00030373
Iteration 108/1000 | Loss: 0.00030315
Iteration 109/1000 | Loss: 0.00030293
Iteration 110/1000 | Loss: 0.00030289
Iteration 111/1000 | Loss: 0.00030289
Iteration 112/1000 | Loss: 0.00030288
Iteration 113/1000 | Loss: 0.00030287
Iteration 114/1000 | Loss: 0.00030287
Iteration 115/1000 | Loss: 0.00030287
Iteration 116/1000 | Loss: 0.00030287
Iteration 117/1000 | Loss: 0.00030287
Iteration 118/1000 | Loss: 0.00032853
Iteration 119/1000 | Loss: 0.00030289
Iteration 120/1000 | Loss: 0.00030283
Iteration 121/1000 | Loss: 0.00030283
Iteration 122/1000 | Loss: 0.00030283
Iteration 123/1000 | Loss: 0.00030283
Iteration 124/1000 | Loss: 0.00030283
Iteration 125/1000 | Loss: 0.00030283
Iteration 126/1000 | Loss: 0.00030282
Iteration 127/1000 | Loss: 0.00030282
Iteration 128/1000 | Loss: 0.00030282
Iteration 129/1000 | Loss: 0.00030282
Iteration 130/1000 | Loss: 0.00030282
Iteration 131/1000 | Loss: 0.00030282
Iteration 132/1000 | Loss: 0.00030282
Iteration 133/1000 | Loss: 0.00030282
Iteration 134/1000 | Loss: 0.00030282
Iteration 135/1000 | Loss: 0.00030281
Iteration 136/1000 | Loss: 0.00030281
Iteration 137/1000 | Loss: 0.00030281
Iteration 138/1000 | Loss: 0.00030280
Iteration 139/1000 | Loss: 0.00030280
Iteration 140/1000 | Loss: 0.00030280
Iteration 141/1000 | Loss: 0.00030280
Iteration 142/1000 | Loss: 0.00030280
Iteration 143/1000 | Loss: 0.00030279
Iteration 144/1000 | Loss: 0.00030279
Iteration 145/1000 | Loss: 0.00030279
Iteration 146/1000 | Loss: 0.00030279
Iteration 147/1000 | Loss: 0.00030279
Iteration 148/1000 | Loss: 0.00030279
Iteration 149/1000 | Loss: 0.00030279
Iteration 150/1000 | Loss: 0.00030279
Iteration 151/1000 | Loss: 0.00030279
Iteration 152/1000 | Loss: 0.00030279
Iteration 153/1000 | Loss: 0.00030279
Iteration 154/1000 | Loss: 0.00030279
Iteration 155/1000 | Loss: 0.00030279
Iteration 156/1000 | Loss: 0.00030279
Iteration 157/1000 | Loss: 0.00030278
Iteration 158/1000 | Loss: 0.00030278
Iteration 159/1000 | Loss: 0.00030278
Iteration 160/1000 | Loss: 0.00030278
Iteration 161/1000 | Loss: 0.00030278
Iteration 162/1000 | Loss: 0.00030278
Iteration 163/1000 | Loss: 0.00030278
Iteration 164/1000 | Loss: 0.00030278
Iteration 165/1000 | Loss: 0.00030278
Iteration 166/1000 | Loss: 0.00030278
Iteration 167/1000 | Loss: 0.00030277
Iteration 168/1000 | Loss: 0.00030277
Iteration 169/1000 | Loss: 0.00030277
Iteration 170/1000 | Loss: 0.00030277
Iteration 171/1000 | Loss: 0.00030277
Iteration 172/1000 | Loss: 0.00030277
Iteration 173/1000 | Loss: 0.00030277
Iteration 174/1000 | Loss: 0.00030276
Iteration 175/1000 | Loss: 0.00030276
Iteration 176/1000 | Loss: 0.00030276
Iteration 177/1000 | Loss: 0.00030276
Iteration 178/1000 | Loss: 0.00030275
Iteration 179/1000 | Loss: 0.00030275
Iteration 180/1000 | Loss: 0.00030275
Iteration 181/1000 | Loss: 0.00030275
Iteration 182/1000 | Loss: 0.00030274
Iteration 183/1000 | Loss: 0.00030274
Iteration 184/1000 | Loss: 0.00030274
Iteration 185/1000 | Loss: 0.00030274
Iteration 186/1000 | Loss: 0.00030274
Iteration 187/1000 | Loss: 0.00030274
Iteration 188/1000 | Loss: 0.00030274
Iteration 189/1000 | Loss: 0.00030274
Iteration 190/1000 | Loss: 0.00030274
Iteration 191/1000 | Loss: 0.00030274
Iteration 192/1000 | Loss: 0.00030274
Iteration 193/1000 | Loss: 0.00030274
Iteration 194/1000 | Loss: 0.00030274
Iteration 195/1000 | Loss: 0.00030274
Iteration 196/1000 | Loss: 0.00030274
Iteration 197/1000 | Loss: 0.00030273
Iteration 198/1000 | Loss: 0.00030273
Iteration 199/1000 | Loss: 0.00030273
Iteration 200/1000 | Loss: 0.00030273
Iteration 201/1000 | Loss: 0.00030273
Iteration 202/1000 | Loss: 0.00030273
Iteration 203/1000 | Loss: 0.00030273
Iteration 204/1000 | Loss: 0.00030273
Iteration 205/1000 | Loss: 0.00030273
Iteration 206/1000 | Loss: 0.00030273
Iteration 207/1000 | Loss: 0.00030273
Iteration 208/1000 | Loss: 0.00030273
Iteration 209/1000 | Loss: 0.00030273
Iteration 210/1000 | Loss: 0.00030273
Iteration 211/1000 | Loss: 0.00030273
Iteration 212/1000 | Loss: 0.00030273
Iteration 213/1000 | Loss: 0.00030273
Iteration 214/1000 | Loss: 0.00030273
Iteration 215/1000 | Loss: 0.00030273
Iteration 216/1000 | Loss: 0.00030273
Iteration 217/1000 | Loss: 0.00030273
Iteration 218/1000 | Loss: 0.00030273
Iteration 219/1000 | Loss: 0.00030273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [0.00030272884760051966, 0.00030272884760051966, 0.00030272884760051966, 0.00030272884760051966, 0.00030272884760051966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030272884760051966

Optimization complete. Final v2v error: 9.807333946228027 mm

Highest mean error: 11.49808120727539 mm for frame 137

Lowest mean error: 5.468501091003418 mm for frame 227

Saving results

Total time: 235.78314924240112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970204
Iteration 2/25 | Loss: 0.00318050
Iteration 3/25 | Loss: 0.00235071
Iteration 4/25 | Loss: 0.00220867
Iteration 5/25 | Loss: 0.00192734
Iteration 6/25 | Loss: 0.00169131
Iteration 7/25 | Loss: 0.00158841
Iteration 8/25 | Loss: 0.00132416
Iteration 9/25 | Loss: 0.00123804
Iteration 10/25 | Loss: 0.00121832
Iteration 11/25 | Loss: 0.00120571
Iteration 12/25 | Loss: 0.00118273
Iteration 13/25 | Loss: 0.00117196
Iteration 14/25 | Loss: 0.00117201
Iteration 15/25 | Loss: 0.00117047
Iteration 16/25 | Loss: 0.00116870
Iteration 17/25 | Loss: 0.00116698
Iteration 18/25 | Loss: 0.00116647
Iteration 19/25 | Loss: 0.00116619
Iteration 20/25 | Loss: 0.00116607
Iteration 21/25 | Loss: 0.00116607
Iteration 22/25 | Loss: 0.00116606
Iteration 23/25 | Loss: 0.00116606
Iteration 24/25 | Loss: 0.00116606
Iteration 25/25 | Loss: 0.00116606

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22229505
Iteration 2/25 | Loss: 0.00252039
Iteration 3/25 | Loss: 0.00252039
Iteration 4/25 | Loss: 0.00252039
Iteration 5/25 | Loss: 0.00252039
Iteration 6/25 | Loss: 0.00252039
Iteration 7/25 | Loss: 0.00252039
Iteration 8/25 | Loss: 0.00252039
Iteration 9/25 | Loss: 0.00252039
Iteration 10/25 | Loss: 0.00252039
Iteration 11/25 | Loss: 0.00252038
Iteration 12/25 | Loss: 0.00252038
Iteration 13/25 | Loss: 0.00252038
Iteration 14/25 | Loss: 0.00252038
Iteration 15/25 | Loss: 0.00252038
Iteration 16/25 | Loss: 0.00252038
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0025203849654644728, 0.0025203849654644728, 0.0025203849654644728, 0.0025203849654644728, 0.0025203849654644728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025203849654644728

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00252038
Iteration 2/1000 | Loss: 0.00020702
Iteration 3/1000 | Loss: 0.00014979
Iteration 4/1000 | Loss: 0.00012619
Iteration 5/1000 | Loss: 0.00011208
Iteration 6/1000 | Loss: 0.00010450
Iteration 7/1000 | Loss: 0.00009414
Iteration 8/1000 | Loss: 0.00009049
Iteration 9/1000 | Loss: 0.00008834
Iteration 10/1000 | Loss: 0.00032074
Iteration 11/1000 | Loss: 0.00054296
Iteration 12/1000 | Loss: 0.00009817
Iteration 13/1000 | Loss: 0.00008674
Iteration 14/1000 | Loss: 0.00008253
Iteration 15/1000 | Loss: 0.00007852
Iteration 16/1000 | Loss: 0.00043850
Iteration 17/1000 | Loss: 0.00023846
Iteration 18/1000 | Loss: 0.00008259
Iteration 19/1000 | Loss: 0.00007608
Iteration 20/1000 | Loss: 0.00007230
Iteration 21/1000 | Loss: 0.00006962
Iteration 22/1000 | Loss: 0.00006791
Iteration 23/1000 | Loss: 0.00006667
Iteration 24/1000 | Loss: 0.00020725
Iteration 25/1000 | Loss: 0.00143747
Iteration 26/1000 | Loss: 0.00394520
Iteration 27/1000 | Loss: 0.00167893
Iteration 28/1000 | Loss: 0.00030322
Iteration 29/1000 | Loss: 0.00009977
Iteration 30/1000 | Loss: 0.00007355
Iteration 31/1000 | Loss: 0.00004957
Iteration 32/1000 | Loss: 0.00003545
Iteration 33/1000 | Loss: 0.00002871
Iteration 34/1000 | Loss: 0.00002457
Iteration 35/1000 | Loss: 0.00002174
Iteration 36/1000 | Loss: 0.00001864
Iteration 37/1000 | Loss: 0.00001730
Iteration 38/1000 | Loss: 0.00001627
Iteration 39/1000 | Loss: 0.00001575
Iteration 40/1000 | Loss: 0.00001541
Iteration 41/1000 | Loss: 0.00001517
Iteration 42/1000 | Loss: 0.00001506
Iteration 43/1000 | Loss: 0.00001499
Iteration 44/1000 | Loss: 0.00001498
Iteration 45/1000 | Loss: 0.00001490
Iteration 46/1000 | Loss: 0.00001489
Iteration 47/1000 | Loss: 0.00001486
Iteration 48/1000 | Loss: 0.00001486
Iteration 49/1000 | Loss: 0.00001485
Iteration 50/1000 | Loss: 0.00001485
Iteration 51/1000 | Loss: 0.00001483
Iteration 52/1000 | Loss: 0.00001482
Iteration 53/1000 | Loss: 0.00001481
Iteration 54/1000 | Loss: 0.00001481
Iteration 55/1000 | Loss: 0.00001480
Iteration 56/1000 | Loss: 0.00001480
Iteration 57/1000 | Loss: 0.00001479
Iteration 58/1000 | Loss: 0.00001479
Iteration 59/1000 | Loss: 0.00001478
Iteration 60/1000 | Loss: 0.00001478
Iteration 61/1000 | Loss: 0.00001478
Iteration 62/1000 | Loss: 0.00001478
Iteration 63/1000 | Loss: 0.00001478
Iteration 64/1000 | Loss: 0.00001477
Iteration 65/1000 | Loss: 0.00001477
Iteration 66/1000 | Loss: 0.00001477
Iteration 67/1000 | Loss: 0.00001476
Iteration 68/1000 | Loss: 0.00001476
Iteration 69/1000 | Loss: 0.00001476
Iteration 70/1000 | Loss: 0.00001476
Iteration 71/1000 | Loss: 0.00001475
Iteration 72/1000 | Loss: 0.00001475
Iteration 73/1000 | Loss: 0.00001475
Iteration 74/1000 | Loss: 0.00001475
Iteration 75/1000 | Loss: 0.00001475
Iteration 76/1000 | Loss: 0.00001474
Iteration 77/1000 | Loss: 0.00001474
Iteration 78/1000 | Loss: 0.00001473
Iteration 79/1000 | Loss: 0.00001473
Iteration 80/1000 | Loss: 0.00001472
Iteration 81/1000 | Loss: 0.00001472
Iteration 82/1000 | Loss: 0.00001472
Iteration 83/1000 | Loss: 0.00001472
Iteration 84/1000 | Loss: 0.00001472
Iteration 85/1000 | Loss: 0.00001472
Iteration 86/1000 | Loss: 0.00001472
Iteration 87/1000 | Loss: 0.00001472
Iteration 88/1000 | Loss: 0.00001472
Iteration 89/1000 | Loss: 0.00001472
Iteration 90/1000 | Loss: 0.00001472
Iteration 91/1000 | Loss: 0.00001472
Iteration 92/1000 | Loss: 0.00001471
Iteration 93/1000 | Loss: 0.00001471
Iteration 94/1000 | Loss: 0.00001471
Iteration 95/1000 | Loss: 0.00001471
Iteration 96/1000 | Loss: 0.00001471
Iteration 97/1000 | Loss: 0.00001471
Iteration 98/1000 | Loss: 0.00001471
Iteration 99/1000 | Loss: 0.00001471
Iteration 100/1000 | Loss: 0.00001471
Iteration 101/1000 | Loss: 0.00001470
Iteration 102/1000 | Loss: 0.00001470
Iteration 103/1000 | Loss: 0.00001470
Iteration 104/1000 | Loss: 0.00001470
Iteration 105/1000 | Loss: 0.00001470
Iteration 106/1000 | Loss: 0.00001470
Iteration 107/1000 | Loss: 0.00001470
Iteration 108/1000 | Loss: 0.00001470
Iteration 109/1000 | Loss: 0.00001470
Iteration 110/1000 | Loss: 0.00001470
Iteration 111/1000 | Loss: 0.00001470
Iteration 112/1000 | Loss: 0.00001470
Iteration 113/1000 | Loss: 0.00001470
Iteration 114/1000 | Loss: 0.00001470
Iteration 115/1000 | Loss: 0.00001470
Iteration 116/1000 | Loss: 0.00001470
Iteration 117/1000 | Loss: 0.00001470
Iteration 118/1000 | Loss: 0.00001470
Iteration 119/1000 | Loss: 0.00001470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.4699365237902384e-05, 1.4699365237902384e-05, 1.4699365237902384e-05, 1.4699365237902384e-05, 1.4699365237902384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4699365237902384e-05

Optimization complete. Final v2v error: 3.284040927886963 mm

Highest mean error: 3.7316434383392334 mm for frame 15

Lowest mean error: 2.716045618057251 mm for frame 155

Saving results

Total time: 103.74594593048096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013156
Iteration 2/25 | Loss: 0.00167375
Iteration 3/25 | Loss: 0.00117339
Iteration 4/25 | Loss: 0.00102100
Iteration 5/25 | Loss: 0.00099717
Iteration 6/25 | Loss: 0.00099448
Iteration 7/25 | Loss: 0.00099444
Iteration 8/25 | Loss: 0.00099444
Iteration 9/25 | Loss: 0.00099444
Iteration 10/25 | Loss: 0.00099444
Iteration 11/25 | Loss: 0.00099444
Iteration 12/25 | Loss: 0.00099444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009944414487108588, 0.0009944414487108588, 0.0009944414487108588, 0.0009944414487108588, 0.0009944414487108588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009944414487108588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22313094
Iteration 2/25 | Loss: 0.00140676
Iteration 3/25 | Loss: 0.00140675
Iteration 4/25 | Loss: 0.00140675
Iteration 5/25 | Loss: 0.00140675
Iteration 6/25 | Loss: 0.00140675
Iteration 7/25 | Loss: 0.00140675
Iteration 8/25 | Loss: 0.00140675
Iteration 9/25 | Loss: 0.00140675
Iteration 10/25 | Loss: 0.00140675
Iteration 11/25 | Loss: 0.00140675
Iteration 12/25 | Loss: 0.00140675
Iteration 13/25 | Loss: 0.00140675
Iteration 14/25 | Loss: 0.00140675
Iteration 15/25 | Loss: 0.00140675
Iteration 16/25 | Loss: 0.00140675
Iteration 17/25 | Loss: 0.00140675
Iteration 18/25 | Loss: 0.00140675
Iteration 19/25 | Loss: 0.00140675
Iteration 20/25 | Loss: 0.00140675
Iteration 21/25 | Loss: 0.00140675
Iteration 22/25 | Loss: 0.00140675
Iteration 23/25 | Loss: 0.00140675
Iteration 24/25 | Loss: 0.00140675
Iteration 25/25 | Loss: 0.00140675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140675
Iteration 2/1000 | Loss: 0.00003232
Iteration 3/1000 | Loss: 0.00002173
Iteration 4/1000 | Loss: 0.00002042
Iteration 5/1000 | Loss: 0.00001917
Iteration 6/1000 | Loss: 0.00001828
Iteration 7/1000 | Loss: 0.00001770
Iteration 8/1000 | Loss: 0.00001719
Iteration 9/1000 | Loss: 0.00001684
Iteration 10/1000 | Loss: 0.00001659
Iteration 11/1000 | Loss: 0.00001636
Iteration 12/1000 | Loss: 0.00001633
Iteration 13/1000 | Loss: 0.00001632
Iteration 14/1000 | Loss: 0.00001622
Iteration 15/1000 | Loss: 0.00001620
Iteration 16/1000 | Loss: 0.00001617
Iteration 17/1000 | Loss: 0.00001616
Iteration 18/1000 | Loss: 0.00001614
Iteration 19/1000 | Loss: 0.00001612
Iteration 20/1000 | Loss: 0.00001612
Iteration 21/1000 | Loss: 0.00001612
Iteration 22/1000 | Loss: 0.00001610
Iteration 23/1000 | Loss: 0.00001605
Iteration 24/1000 | Loss: 0.00001605
Iteration 25/1000 | Loss: 0.00001604
Iteration 26/1000 | Loss: 0.00001604
Iteration 27/1000 | Loss: 0.00001603
Iteration 28/1000 | Loss: 0.00001603
Iteration 29/1000 | Loss: 0.00001602
Iteration 30/1000 | Loss: 0.00001602
Iteration 31/1000 | Loss: 0.00001601
Iteration 32/1000 | Loss: 0.00001601
Iteration 33/1000 | Loss: 0.00001600
Iteration 34/1000 | Loss: 0.00001600
Iteration 35/1000 | Loss: 0.00001600
Iteration 36/1000 | Loss: 0.00001599
Iteration 37/1000 | Loss: 0.00001599
Iteration 38/1000 | Loss: 0.00001597
Iteration 39/1000 | Loss: 0.00001597
Iteration 40/1000 | Loss: 0.00001596
Iteration 41/1000 | Loss: 0.00001596
Iteration 42/1000 | Loss: 0.00001596
Iteration 43/1000 | Loss: 0.00001596
Iteration 44/1000 | Loss: 0.00001596
Iteration 45/1000 | Loss: 0.00001596
Iteration 46/1000 | Loss: 0.00001595
Iteration 47/1000 | Loss: 0.00001595
Iteration 48/1000 | Loss: 0.00001595
Iteration 49/1000 | Loss: 0.00001595
Iteration 50/1000 | Loss: 0.00001595
Iteration 51/1000 | Loss: 0.00001594
Iteration 52/1000 | Loss: 0.00001594
Iteration 53/1000 | Loss: 0.00001594
Iteration 54/1000 | Loss: 0.00001594
Iteration 55/1000 | Loss: 0.00001594
Iteration 56/1000 | Loss: 0.00001594
Iteration 57/1000 | Loss: 0.00001594
Iteration 58/1000 | Loss: 0.00001594
Iteration 59/1000 | Loss: 0.00001594
Iteration 60/1000 | Loss: 0.00001593
Iteration 61/1000 | Loss: 0.00001593
Iteration 62/1000 | Loss: 0.00001593
Iteration 63/1000 | Loss: 0.00001593
Iteration 64/1000 | Loss: 0.00001593
Iteration 65/1000 | Loss: 0.00001593
Iteration 66/1000 | Loss: 0.00001592
Iteration 67/1000 | Loss: 0.00001592
Iteration 68/1000 | Loss: 0.00001592
Iteration 69/1000 | Loss: 0.00001592
Iteration 70/1000 | Loss: 0.00001592
Iteration 71/1000 | Loss: 0.00001592
Iteration 72/1000 | Loss: 0.00001592
Iteration 73/1000 | Loss: 0.00001592
Iteration 74/1000 | Loss: 0.00001592
Iteration 75/1000 | Loss: 0.00001592
Iteration 76/1000 | Loss: 0.00001592
Iteration 77/1000 | Loss: 0.00001592
Iteration 78/1000 | Loss: 0.00001591
Iteration 79/1000 | Loss: 0.00001591
Iteration 80/1000 | Loss: 0.00001591
Iteration 81/1000 | Loss: 0.00001591
Iteration 82/1000 | Loss: 0.00001591
Iteration 83/1000 | Loss: 0.00001591
Iteration 84/1000 | Loss: 0.00001591
Iteration 85/1000 | Loss: 0.00001591
Iteration 86/1000 | Loss: 0.00001591
Iteration 87/1000 | Loss: 0.00001591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.5913459719740786e-05, 1.5913459719740786e-05, 1.5913459719740786e-05, 1.5913459719740786e-05, 1.5913459719740786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5913459719740786e-05

Optimization complete. Final v2v error: 3.2904422283172607 mm

Highest mean error: 3.4707229137420654 mm for frame 48

Lowest mean error: 2.8161473274230957 mm for frame 0

Saving results

Total time: 32.988749265670776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00355941
Iteration 2/25 | Loss: 0.00109687
Iteration 3/25 | Loss: 0.00098621
Iteration 4/25 | Loss: 0.00096902
Iteration 5/25 | Loss: 0.00096224
Iteration 6/25 | Loss: 0.00096152
Iteration 7/25 | Loss: 0.00096152
Iteration 8/25 | Loss: 0.00096152
Iteration 9/25 | Loss: 0.00096152
Iteration 10/25 | Loss: 0.00096152
Iteration 11/25 | Loss: 0.00096152
Iteration 12/25 | Loss: 0.00096152
Iteration 13/25 | Loss: 0.00096152
Iteration 14/25 | Loss: 0.00096152
Iteration 15/25 | Loss: 0.00096152
Iteration 16/25 | Loss: 0.00096152
Iteration 17/25 | Loss: 0.00096152
Iteration 18/25 | Loss: 0.00096152
Iteration 19/25 | Loss: 0.00096152
Iteration 20/25 | Loss: 0.00096152
Iteration 21/25 | Loss: 0.00096152
Iteration 22/25 | Loss: 0.00096152
Iteration 23/25 | Loss: 0.00096152
Iteration 24/25 | Loss: 0.00096152
Iteration 25/25 | Loss: 0.00096152

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.77521551
Iteration 2/25 | Loss: 0.00154032
Iteration 3/25 | Loss: 0.00154032
Iteration 4/25 | Loss: 0.00154032
Iteration 5/25 | Loss: 0.00154032
Iteration 6/25 | Loss: 0.00154032
Iteration 7/25 | Loss: 0.00154032
Iteration 8/25 | Loss: 0.00154032
Iteration 9/25 | Loss: 0.00154032
Iteration 10/25 | Loss: 0.00154032
Iteration 11/25 | Loss: 0.00154032
Iteration 12/25 | Loss: 0.00154032
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015403202269226313, 0.0015403202269226313, 0.0015403202269226313, 0.0015403202269226313, 0.0015403202269226313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015403202269226313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154032
Iteration 2/1000 | Loss: 0.00004621
Iteration 3/1000 | Loss: 0.00002858
Iteration 4/1000 | Loss: 0.00002167
Iteration 5/1000 | Loss: 0.00001984
Iteration 6/1000 | Loss: 0.00001885
Iteration 7/1000 | Loss: 0.00001801
Iteration 8/1000 | Loss: 0.00001764
Iteration 9/1000 | Loss: 0.00001715
Iteration 10/1000 | Loss: 0.00001674
Iteration 11/1000 | Loss: 0.00001645
Iteration 12/1000 | Loss: 0.00001621
Iteration 13/1000 | Loss: 0.00001599
Iteration 14/1000 | Loss: 0.00001592
Iteration 15/1000 | Loss: 0.00001584
Iteration 16/1000 | Loss: 0.00001567
Iteration 17/1000 | Loss: 0.00001553
Iteration 18/1000 | Loss: 0.00001537
Iteration 19/1000 | Loss: 0.00001530
Iteration 20/1000 | Loss: 0.00001516
Iteration 21/1000 | Loss: 0.00001504
Iteration 22/1000 | Loss: 0.00001493
Iteration 23/1000 | Loss: 0.00001489
Iteration 24/1000 | Loss: 0.00001487
Iteration 25/1000 | Loss: 0.00001487
Iteration 26/1000 | Loss: 0.00001482
Iteration 27/1000 | Loss: 0.00001477
Iteration 28/1000 | Loss: 0.00001477
Iteration 29/1000 | Loss: 0.00001471
Iteration 30/1000 | Loss: 0.00001464
Iteration 31/1000 | Loss: 0.00001456
Iteration 32/1000 | Loss: 0.00001456
Iteration 33/1000 | Loss: 0.00001452
Iteration 34/1000 | Loss: 0.00001451
Iteration 35/1000 | Loss: 0.00001451
Iteration 36/1000 | Loss: 0.00001451
Iteration 37/1000 | Loss: 0.00001451
Iteration 38/1000 | Loss: 0.00001450
Iteration 39/1000 | Loss: 0.00001450
Iteration 40/1000 | Loss: 0.00001448
Iteration 41/1000 | Loss: 0.00001448
Iteration 42/1000 | Loss: 0.00001447
Iteration 43/1000 | Loss: 0.00001447
Iteration 44/1000 | Loss: 0.00001447
Iteration 45/1000 | Loss: 0.00001446
Iteration 46/1000 | Loss: 0.00001445
Iteration 47/1000 | Loss: 0.00001445
Iteration 48/1000 | Loss: 0.00001444
Iteration 49/1000 | Loss: 0.00001443
Iteration 50/1000 | Loss: 0.00001443
Iteration 51/1000 | Loss: 0.00001443
Iteration 52/1000 | Loss: 0.00001443
Iteration 53/1000 | Loss: 0.00001443
Iteration 54/1000 | Loss: 0.00001443
Iteration 55/1000 | Loss: 0.00001443
Iteration 56/1000 | Loss: 0.00001443
Iteration 57/1000 | Loss: 0.00001443
Iteration 58/1000 | Loss: 0.00001443
Iteration 59/1000 | Loss: 0.00001442
Iteration 60/1000 | Loss: 0.00001442
Iteration 61/1000 | Loss: 0.00001442
Iteration 62/1000 | Loss: 0.00001442
Iteration 63/1000 | Loss: 0.00001441
Iteration 64/1000 | Loss: 0.00001441
Iteration 65/1000 | Loss: 0.00001441
Iteration 66/1000 | Loss: 0.00001441
Iteration 67/1000 | Loss: 0.00001441
Iteration 68/1000 | Loss: 0.00001441
Iteration 69/1000 | Loss: 0.00001440
Iteration 70/1000 | Loss: 0.00001440
Iteration 71/1000 | Loss: 0.00001440
Iteration 72/1000 | Loss: 0.00001440
Iteration 73/1000 | Loss: 0.00001440
Iteration 74/1000 | Loss: 0.00001440
Iteration 75/1000 | Loss: 0.00001440
Iteration 76/1000 | Loss: 0.00001440
Iteration 77/1000 | Loss: 0.00001440
Iteration 78/1000 | Loss: 0.00001440
Iteration 79/1000 | Loss: 0.00001440
Iteration 80/1000 | Loss: 0.00001440
Iteration 81/1000 | Loss: 0.00001439
Iteration 82/1000 | Loss: 0.00001439
Iteration 83/1000 | Loss: 0.00001439
Iteration 84/1000 | Loss: 0.00001439
Iteration 85/1000 | Loss: 0.00001439
Iteration 86/1000 | Loss: 0.00001439
Iteration 87/1000 | Loss: 0.00001439
Iteration 88/1000 | Loss: 0.00001438
Iteration 89/1000 | Loss: 0.00001438
Iteration 90/1000 | Loss: 0.00001438
Iteration 91/1000 | Loss: 0.00001438
Iteration 92/1000 | Loss: 0.00001438
Iteration 93/1000 | Loss: 0.00001437
Iteration 94/1000 | Loss: 0.00001437
Iteration 95/1000 | Loss: 0.00001437
Iteration 96/1000 | Loss: 0.00001437
Iteration 97/1000 | Loss: 0.00001437
Iteration 98/1000 | Loss: 0.00001436
Iteration 99/1000 | Loss: 0.00001436
Iteration 100/1000 | Loss: 0.00001436
Iteration 101/1000 | Loss: 0.00001436
Iteration 102/1000 | Loss: 0.00001436
Iteration 103/1000 | Loss: 0.00001436
Iteration 104/1000 | Loss: 0.00001436
Iteration 105/1000 | Loss: 0.00001436
Iteration 106/1000 | Loss: 0.00001435
Iteration 107/1000 | Loss: 0.00001435
Iteration 108/1000 | Loss: 0.00001435
Iteration 109/1000 | Loss: 0.00001435
Iteration 110/1000 | Loss: 0.00001435
Iteration 111/1000 | Loss: 0.00001434
Iteration 112/1000 | Loss: 0.00001434
Iteration 113/1000 | Loss: 0.00001434
Iteration 114/1000 | Loss: 0.00001434
Iteration 115/1000 | Loss: 0.00001434
Iteration 116/1000 | Loss: 0.00001434
Iteration 117/1000 | Loss: 0.00001434
Iteration 118/1000 | Loss: 0.00001434
Iteration 119/1000 | Loss: 0.00001434
Iteration 120/1000 | Loss: 0.00001434
Iteration 121/1000 | Loss: 0.00001434
Iteration 122/1000 | Loss: 0.00001434
Iteration 123/1000 | Loss: 0.00001434
Iteration 124/1000 | Loss: 0.00001434
Iteration 125/1000 | Loss: 0.00001434
Iteration 126/1000 | Loss: 0.00001433
Iteration 127/1000 | Loss: 0.00001433
Iteration 128/1000 | Loss: 0.00001433
Iteration 129/1000 | Loss: 0.00001433
Iteration 130/1000 | Loss: 0.00001433
Iteration 131/1000 | Loss: 0.00001433
Iteration 132/1000 | Loss: 0.00001433
Iteration 133/1000 | Loss: 0.00001433
Iteration 134/1000 | Loss: 0.00001433
Iteration 135/1000 | Loss: 0.00001433
Iteration 136/1000 | Loss: 0.00001433
Iteration 137/1000 | Loss: 0.00001433
Iteration 138/1000 | Loss: 0.00001433
Iteration 139/1000 | Loss: 0.00001432
Iteration 140/1000 | Loss: 0.00001432
Iteration 141/1000 | Loss: 0.00001432
Iteration 142/1000 | Loss: 0.00001432
Iteration 143/1000 | Loss: 0.00001432
Iteration 144/1000 | Loss: 0.00001432
Iteration 145/1000 | Loss: 0.00001432
Iteration 146/1000 | Loss: 0.00001432
Iteration 147/1000 | Loss: 0.00001432
Iteration 148/1000 | Loss: 0.00001431
Iteration 149/1000 | Loss: 0.00001431
Iteration 150/1000 | Loss: 0.00001431
Iteration 151/1000 | Loss: 0.00001431
Iteration 152/1000 | Loss: 0.00001431
Iteration 153/1000 | Loss: 0.00001431
Iteration 154/1000 | Loss: 0.00001431
Iteration 155/1000 | Loss: 0.00001431
Iteration 156/1000 | Loss: 0.00001431
Iteration 157/1000 | Loss: 0.00001431
Iteration 158/1000 | Loss: 0.00001431
Iteration 159/1000 | Loss: 0.00001431
Iteration 160/1000 | Loss: 0.00001431
Iteration 161/1000 | Loss: 0.00001431
Iteration 162/1000 | Loss: 0.00001431
Iteration 163/1000 | Loss: 0.00001431
Iteration 164/1000 | Loss: 0.00001431
Iteration 165/1000 | Loss: 0.00001431
Iteration 166/1000 | Loss: 0.00001431
Iteration 167/1000 | Loss: 0.00001431
Iteration 168/1000 | Loss: 0.00001430
Iteration 169/1000 | Loss: 0.00001430
Iteration 170/1000 | Loss: 0.00001430
Iteration 171/1000 | Loss: 0.00001430
Iteration 172/1000 | Loss: 0.00001430
Iteration 173/1000 | Loss: 0.00001430
Iteration 174/1000 | Loss: 0.00001430
Iteration 175/1000 | Loss: 0.00001430
Iteration 176/1000 | Loss: 0.00001430
Iteration 177/1000 | Loss: 0.00001430
Iteration 178/1000 | Loss: 0.00001430
Iteration 179/1000 | Loss: 0.00001430
Iteration 180/1000 | Loss: 0.00001430
Iteration 181/1000 | Loss: 0.00001430
Iteration 182/1000 | Loss: 0.00001430
Iteration 183/1000 | Loss: 0.00001430
Iteration 184/1000 | Loss: 0.00001430
Iteration 185/1000 | Loss: 0.00001430
Iteration 186/1000 | Loss: 0.00001430
Iteration 187/1000 | Loss: 0.00001430
Iteration 188/1000 | Loss: 0.00001430
Iteration 189/1000 | Loss: 0.00001430
Iteration 190/1000 | Loss: 0.00001430
Iteration 191/1000 | Loss: 0.00001430
Iteration 192/1000 | Loss: 0.00001430
Iteration 193/1000 | Loss: 0.00001430
Iteration 194/1000 | Loss: 0.00001430
Iteration 195/1000 | Loss: 0.00001430
Iteration 196/1000 | Loss: 0.00001430
Iteration 197/1000 | Loss: 0.00001430
Iteration 198/1000 | Loss: 0.00001430
Iteration 199/1000 | Loss: 0.00001430
Iteration 200/1000 | Loss: 0.00001430
Iteration 201/1000 | Loss: 0.00001430
Iteration 202/1000 | Loss: 0.00001430
Iteration 203/1000 | Loss: 0.00001430
Iteration 204/1000 | Loss: 0.00001430
Iteration 205/1000 | Loss: 0.00001430
Iteration 206/1000 | Loss: 0.00001430
Iteration 207/1000 | Loss: 0.00001430
Iteration 208/1000 | Loss: 0.00001430
Iteration 209/1000 | Loss: 0.00001430
Iteration 210/1000 | Loss: 0.00001430
Iteration 211/1000 | Loss: 0.00001430
Iteration 212/1000 | Loss: 0.00001430
Iteration 213/1000 | Loss: 0.00001430
Iteration 214/1000 | Loss: 0.00001430
Iteration 215/1000 | Loss: 0.00001430
Iteration 216/1000 | Loss: 0.00001430
Iteration 217/1000 | Loss: 0.00001430
Iteration 218/1000 | Loss: 0.00001430
Iteration 219/1000 | Loss: 0.00001430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.429946132702753e-05, 1.429946132702753e-05, 1.429946132702753e-05, 1.429946132702753e-05, 1.429946132702753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.429946132702753e-05

Optimization complete. Final v2v error: 3.2422242164611816 mm

Highest mean error: 3.6075000762939453 mm for frame 143

Lowest mean error: 3.0896308422088623 mm for frame 85

Saving results

Total time: 61.263373136520386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834529
Iteration 2/25 | Loss: 0.00125791
Iteration 3/25 | Loss: 0.00093868
Iteration 4/25 | Loss: 0.00090791
Iteration 5/25 | Loss: 0.00090148
Iteration 6/25 | Loss: 0.00089918
Iteration 7/25 | Loss: 0.00089882
Iteration 8/25 | Loss: 0.00089882
Iteration 9/25 | Loss: 0.00089882
Iteration 10/25 | Loss: 0.00089882
Iteration 11/25 | Loss: 0.00089882
Iteration 12/25 | Loss: 0.00089882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008988185436464846, 0.0008988185436464846, 0.0008988185436464846, 0.0008988185436464846, 0.0008988185436464846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008988185436464846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61324692
Iteration 2/25 | Loss: 0.00139361
Iteration 3/25 | Loss: 0.00139361
Iteration 4/25 | Loss: 0.00139361
Iteration 5/25 | Loss: 0.00139361
Iteration 6/25 | Loss: 0.00139361
Iteration 7/25 | Loss: 0.00139361
Iteration 8/25 | Loss: 0.00139361
Iteration 9/25 | Loss: 0.00139361
Iteration 10/25 | Loss: 0.00139361
Iteration 11/25 | Loss: 0.00139361
Iteration 12/25 | Loss: 0.00139361
Iteration 13/25 | Loss: 0.00139361
Iteration 14/25 | Loss: 0.00139361
Iteration 15/25 | Loss: 0.00139361
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013936115428805351, 0.0013936115428805351, 0.0013936115428805351, 0.0013936115428805351, 0.0013936115428805351]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013936115428805351

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139361
Iteration 2/1000 | Loss: 0.00001813
Iteration 3/1000 | Loss: 0.00001257
Iteration 4/1000 | Loss: 0.00001143
Iteration 5/1000 | Loss: 0.00001080
Iteration 6/1000 | Loss: 0.00001008
Iteration 7/1000 | Loss: 0.00000980
Iteration 8/1000 | Loss: 0.00000941
Iteration 9/1000 | Loss: 0.00000923
Iteration 10/1000 | Loss: 0.00000914
Iteration 11/1000 | Loss: 0.00000913
Iteration 12/1000 | Loss: 0.00000912
Iteration 13/1000 | Loss: 0.00000909
Iteration 14/1000 | Loss: 0.00000908
Iteration 15/1000 | Loss: 0.00000903
Iteration 16/1000 | Loss: 0.00000902
Iteration 17/1000 | Loss: 0.00000902
Iteration 18/1000 | Loss: 0.00000901
Iteration 19/1000 | Loss: 0.00000900
Iteration 20/1000 | Loss: 0.00000899
Iteration 21/1000 | Loss: 0.00000898
Iteration 22/1000 | Loss: 0.00000896
Iteration 23/1000 | Loss: 0.00000895
Iteration 24/1000 | Loss: 0.00000895
Iteration 25/1000 | Loss: 0.00000894
Iteration 26/1000 | Loss: 0.00000894
Iteration 27/1000 | Loss: 0.00000892
Iteration 28/1000 | Loss: 0.00000892
Iteration 29/1000 | Loss: 0.00000892
Iteration 30/1000 | Loss: 0.00000888
Iteration 31/1000 | Loss: 0.00000887
Iteration 32/1000 | Loss: 0.00000887
Iteration 33/1000 | Loss: 0.00000886
Iteration 34/1000 | Loss: 0.00000886
Iteration 35/1000 | Loss: 0.00000886
Iteration 36/1000 | Loss: 0.00000886
Iteration 37/1000 | Loss: 0.00000885
Iteration 38/1000 | Loss: 0.00000885
Iteration 39/1000 | Loss: 0.00000884
Iteration 40/1000 | Loss: 0.00000884
Iteration 41/1000 | Loss: 0.00000883
Iteration 42/1000 | Loss: 0.00000883
Iteration 43/1000 | Loss: 0.00000882
Iteration 44/1000 | Loss: 0.00000882
Iteration 45/1000 | Loss: 0.00000881
Iteration 46/1000 | Loss: 0.00000880
Iteration 47/1000 | Loss: 0.00000879
Iteration 48/1000 | Loss: 0.00000879
Iteration 49/1000 | Loss: 0.00000878
Iteration 50/1000 | Loss: 0.00000877
Iteration 51/1000 | Loss: 0.00000877
Iteration 52/1000 | Loss: 0.00000877
Iteration 53/1000 | Loss: 0.00000876
Iteration 54/1000 | Loss: 0.00000875
Iteration 55/1000 | Loss: 0.00000874
Iteration 56/1000 | Loss: 0.00000874
Iteration 57/1000 | Loss: 0.00000873
Iteration 58/1000 | Loss: 0.00000873
Iteration 59/1000 | Loss: 0.00000873
Iteration 60/1000 | Loss: 0.00000873
Iteration 61/1000 | Loss: 0.00000873
Iteration 62/1000 | Loss: 0.00000873
Iteration 63/1000 | Loss: 0.00000873
Iteration 64/1000 | Loss: 0.00000873
Iteration 65/1000 | Loss: 0.00000873
Iteration 66/1000 | Loss: 0.00000872
Iteration 67/1000 | Loss: 0.00000872
Iteration 68/1000 | Loss: 0.00000872
Iteration 69/1000 | Loss: 0.00000871
Iteration 70/1000 | Loss: 0.00000871
Iteration 71/1000 | Loss: 0.00000871
Iteration 72/1000 | Loss: 0.00000871
Iteration 73/1000 | Loss: 0.00000871
Iteration 74/1000 | Loss: 0.00000871
Iteration 75/1000 | Loss: 0.00000871
Iteration 76/1000 | Loss: 0.00000871
Iteration 77/1000 | Loss: 0.00000871
Iteration 78/1000 | Loss: 0.00000870
Iteration 79/1000 | Loss: 0.00000870
Iteration 80/1000 | Loss: 0.00000870
Iteration 81/1000 | Loss: 0.00000870
Iteration 82/1000 | Loss: 0.00000870
Iteration 83/1000 | Loss: 0.00000870
Iteration 84/1000 | Loss: 0.00000869
Iteration 85/1000 | Loss: 0.00000869
Iteration 86/1000 | Loss: 0.00000868
Iteration 87/1000 | Loss: 0.00000868
Iteration 88/1000 | Loss: 0.00000868
Iteration 89/1000 | Loss: 0.00000868
Iteration 90/1000 | Loss: 0.00000868
Iteration 91/1000 | Loss: 0.00000868
Iteration 92/1000 | Loss: 0.00000868
Iteration 93/1000 | Loss: 0.00000868
Iteration 94/1000 | Loss: 0.00000868
Iteration 95/1000 | Loss: 0.00000868
Iteration 96/1000 | Loss: 0.00000868
Iteration 97/1000 | Loss: 0.00000868
Iteration 98/1000 | Loss: 0.00000868
Iteration 99/1000 | Loss: 0.00000868
Iteration 100/1000 | Loss: 0.00000868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [8.675189747009426e-06, 8.675189747009426e-06, 8.675189747009426e-06, 8.675189747009426e-06, 8.675189747009426e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.675189747009426e-06

Optimization complete. Final v2v error: 2.5240530967712402 mm

Highest mean error: 2.829709768295288 mm for frame 91

Lowest mean error: 1.9320619106292725 mm for frame 219

Saving results

Total time: 34.617810010910034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544291
Iteration 2/25 | Loss: 0.00127000
Iteration 3/25 | Loss: 0.00100259
Iteration 4/25 | Loss: 0.00091165
Iteration 5/25 | Loss: 0.00090758
Iteration 6/25 | Loss: 0.00090666
Iteration 7/25 | Loss: 0.00090755
Iteration 8/25 | Loss: 0.00090637
Iteration 9/25 | Loss: 0.00090611
Iteration 10/25 | Loss: 0.00090478
Iteration 11/25 | Loss: 0.00090477
Iteration 12/25 | Loss: 0.00090477
Iteration 13/25 | Loss: 0.00090476
Iteration 14/25 | Loss: 0.00090476
Iteration 15/25 | Loss: 0.00090476
Iteration 16/25 | Loss: 0.00090476
Iteration 17/25 | Loss: 0.00090476
Iteration 18/25 | Loss: 0.00090476
Iteration 19/25 | Loss: 0.00090476
Iteration 20/25 | Loss: 0.00090476
Iteration 21/25 | Loss: 0.00090476
Iteration 22/25 | Loss: 0.00090476
Iteration 23/25 | Loss: 0.00090475
Iteration 24/25 | Loss: 0.00090475
Iteration 25/25 | Loss: 0.00090475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.12653351
Iteration 2/25 | Loss: 0.00140112
Iteration 3/25 | Loss: 0.00140110
Iteration 4/25 | Loss: 0.00140110
Iteration 5/25 | Loss: 0.00140110
Iteration 6/25 | Loss: 0.00140109
Iteration 7/25 | Loss: 0.00140109
Iteration 8/25 | Loss: 0.00140109
Iteration 9/25 | Loss: 0.00140109
Iteration 10/25 | Loss: 0.00140109
Iteration 11/25 | Loss: 0.00140109
Iteration 12/25 | Loss: 0.00140109
Iteration 13/25 | Loss: 0.00140109
Iteration 14/25 | Loss: 0.00140109
Iteration 15/25 | Loss: 0.00140109
Iteration 16/25 | Loss: 0.00140109
Iteration 17/25 | Loss: 0.00140109
Iteration 18/25 | Loss: 0.00140109
Iteration 19/25 | Loss: 0.00140109
Iteration 20/25 | Loss: 0.00140109
Iteration 21/25 | Loss: 0.00140109
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014010932063683867, 0.0014010932063683867, 0.0014010932063683867, 0.0014010932063683867, 0.0014010932063683867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014010932063683867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140109
Iteration 2/1000 | Loss: 0.00002055
Iteration 3/1000 | Loss: 0.00001200
Iteration 4/1000 | Loss: 0.00001063
Iteration 5/1000 | Loss: 0.00001002
Iteration 6/1000 | Loss: 0.00000948
Iteration 7/1000 | Loss: 0.00000918
Iteration 8/1000 | Loss: 0.00000904
Iteration 9/1000 | Loss: 0.00000878
Iteration 10/1000 | Loss: 0.00000873
Iteration 11/1000 | Loss: 0.00000873
Iteration 12/1000 | Loss: 0.00000870
Iteration 13/1000 | Loss: 0.00000867
Iteration 14/1000 | Loss: 0.00000865
Iteration 15/1000 | Loss: 0.00000863
Iteration 16/1000 | Loss: 0.00000861
Iteration 17/1000 | Loss: 0.00000860
Iteration 18/1000 | Loss: 0.00000860
Iteration 19/1000 | Loss: 0.00000859
Iteration 20/1000 | Loss: 0.00000858
Iteration 21/1000 | Loss: 0.00000855
Iteration 22/1000 | Loss: 0.00000855
Iteration 23/1000 | Loss: 0.00000854
Iteration 24/1000 | Loss: 0.00000854
Iteration 25/1000 | Loss: 0.00000854
Iteration 26/1000 | Loss: 0.00000854
Iteration 27/1000 | Loss: 0.00000853
Iteration 28/1000 | Loss: 0.00000853
Iteration 29/1000 | Loss: 0.00000852
Iteration 30/1000 | Loss: 0.00000852
Iteration 31/1000 | Loss: 0.00000852
Iteration 32/1000 | Loss: 0.00000851
Iteration 33/1000 | Loss: 0.00000850
Iteration 34/1000 | Loss: 0.00000849
Iteration 35/1000 | Loss: 0.00000849
Iteration 36/1000 | Loss: 0.00000849
Iteration 37/1000 | Loss: 0.00000849
Iteration 38/1000 | Loss: 0.00000849
Iteration 39/1000 | Loss: 0.00000848
Iteration 40/1000 | Loss: 0.00000848
Iteration 41/1000 | Loss: 0.00000847
Iteration 42/1000 | Loss: 0.00000847
Iteration 43/1000 | Loss: 0.00000847
Iteration 44/1000 | Loss: 0.00000846
Iteration 45/1000 | Loss: 0.00000846
Iteration 46/1000 | Loss: 0.00000845
Iteration 47/1000 | Loss: 0.00000845
Iteration 48/1000 | Loss: 0.00000844
Iteration 49/1000 | Loss: 0.00000844
Iteration 50/1000 | Loss: 0.00000844
Iteration 51/1000 | Loss: 0.00000844
Iteration 52/1000 | Loss: 0.00000844
Iteration 53/1000 | Loss: 0.00000844
Iteration 54/1000 | Loss: 0.00000844
Iteration 55/1000 | Loss: 0.00000844
Iteration 56/1000 | Loss: 0.00000844
Iteration 57/1000 | Loss: 0.00000844
Iteration 58/1000 | Loss: 0.00000843
Iteration 59/1000 | Loss: 0.00000842
Iteration 60/1000 | Loss: 0.00000842
Iteration 61/1000 | Loss: 0.00000842
Iteration 62/1000 | Loss: 0.00000842
Iteration 63/1000 | Loss: 0.00000841
Iteration 64/1000 | Loss: 0.00000841
Iteration 65/1000 | Loss: 0.00000841
Iteration 66/1000 | Loss: 0.00000841
Iteration 67/1000 | Loss: 0.00000841
Iteration 68/1000 | Loss: 0.00000841
Iteration 69/1000 | Loss: 0.00000841
Iteration 70/1000 | Loss: 0.00000841
Iteration 71/1000 | Loss: 0.00000841
Iteration 72/1000 | Loss: 0.00000841
Iteration 73/1000 | Loss: 0.00000841
Iteration 74/1000 | Loss: 0.00000840
Iteration 75/1000 | Loss: 0.00000840
Iteration 76/1000 | Loss: 0.00000840
Iteration 77/1000 | Loss: 0.00000840
Iteration 78/1000 | Loss: 0.00000840
Iteration 79/1000 | Loss: 0.00000839
Iteration 80/1000 | Loss: 0.00000839
Iteration 81/1000 | Loss: 0.00000838
Iteration 82/1000 | Loss: 0.00000838
Iteration 83/1000 | Loss: 0.00000838
Iteration 84/1000 | Loss: 0.00000838
Iteration 85/1000 | Loss: 0.00000838
Iteration 86/1000 | Loss: 0.00000838
Iteration 87/1000 | Loss: 0.00000838
Iteration 88/1000 | Loss: 0.00000838
Iteration 89/1000 | Loss: 0.00000838
Iteration 90/1000 | Loss: 0.00000838
Iteration 91/1000 | Loss: 0.00000838
Iteration 92/1000 | Loss: 0.00000837
Iteration 93/1000 | Loss: 0.00000837
Iteration 94/1000 | Loss: 0.00000837
Iteration 95/1000 | Loss: 0.00000837
Iteration 96/1000 | Loss: 0.00000837
Iteration 97/1000 | Loss: 0.00000837
Iteration 98/1000 | Loss: 0.00000836
Iteration 99/1000 | Loss: 0.00000836
Iteration 100/1000 | Loss: 0.00000836
Iteration 101/1000 | Loss: 0.00000836
Iteration 102/1000 | Loss: 0.00000836
Iteration 103/1000 | Loss: 0.00000836
Iteration 104/1000 | Loss: 0.00000836
Iteration 105/1000 | Loss: 0.00000836
Iteration 106/1000 | Loss: 0.00000836
Iteration 107/1000 | Loss: 0.00000835
Iteration 108/1000 | Loss: 0.00000835
Iteration 109/1000 | Loss: 0.00000835
Iteration 110/1000 | Loss: 0.00000835
Iteration 111/1000 | Loss: 0.00000835
Iteration 112/1000 | Loss: 0.00000835
Iteration 113/1000 | Loss: 0.00000835
Iteration 114/1000 | Loss: 0.00000835
Iteration 115/1000 | Loss: 0.00000835
Iteration 116/1000 | Loss: 0.00000835
Iteration 117/1000 | Loss: 0.00000835
Iteration 118/1000 | Loss: 0.00000834
Iteration 119/1000 | Loss: 0.00000834
Iteration 120/1000 | Loss: 0.00000834
Iteration 121/1000 | Loss: 0.00000834
Iteration 122/1000 | Loss: 0.00000834
Iteration 123/1000 | Loss: 0.00000834
Iteration 124/1000 | Loss: 0.00000834
Iteration 125/1000 | Loss: 0.00000834
Iteration 126/1000 | Loss: 0.00000834
Iteration 127/1000 | Loss: 0.00000834
Iteration 128/1000 | Loss: 0.00000834
Iteration 129/1000 | Loss: 0.00000834
Iteration 130/1000 | Loss: 0.00000833
Iteration 131/1000 | Loss: 0.00000833
Iteration 132/1000 | Loss: 0.00000833
Iteration 133/1000 | Loss: 0.00000833
Iteration 134/1000 | Loss: 0.00000833
Iteration 135/1000 | Loss: 0.00000833
Iteration 136/1000 | Loss: 0.00000833
Iteration 137/1000 | Loss: 0.00000832
Iteration 138/1000 | Loss: 0.00000832
Iteration 139/1000 | Loss: 0.00000832
Iteration 140/1000 | Loss: 0.00000832
Iteration 141/1000 | Loss: 0.00000832
Iteration 142/1000 | Loss: 0.00000831
Iteration 143/1000 | Loss: 0.00000831
Iteration 144/1000 | Loss: 0.00000831
Iteration 145/1000 | Loss: 0.00000831
Iteration 146/1000 | Loss: 0.00000831
Iteration 147/1000 | Loss: 0.00000831
Iteration 148/1000 | Loss: 0.00000831
Iteration 149/1000 | Loss: 0.00000831
Iteration 150/1000 | Loss: 0.00000831
Iteration 151/1000 | Loss: 0.00000831
Iteration 152/1000 | Loss: 0.00000831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [8.307868483825587e-06, 8.307868483825587e-06, 8.307868483825587e-06, 8.307868483825587e-06, 8.307868483825587e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.307868483825587e-06

Optimization complete. Final v2v error: 2.4014973640441895 mm

Highest mean error: 2.8678555488586426 mm for frame 165

Lowest mean error: 2.0478036403656006 mm for frame 142

Saving results

Total time: 46.958045959472656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383416
Iteration 2/25 | Loss: 0.00098524
Iteration 3/25 | Loss: 0.00089086
Iteration 4/25 | Loss: 0.00087978
Iteration 5/25 | Loss: 0.00087692
Iteration 6/25 | Loss: 0.00087692
Iteration 7/25 | Loss: 0.00087692
Iteration 8/25 | Loss: 0.00087692
Iteration 9/25 | Loss: 0.00087692
Iteration 10/25 | Loss: 0.00087692
Iteration 11/25 | Loss: 0.00087692
Iteration 12/25 | Loss: 0.00087692
Iteration 13/25 | Loss: 0.00087692
Iteration 14/25 | Loss: 0.00087692
Iteration 15/25 | Loss: 0.00087692
Iteration 16/25 | Loss: 0.00087692
Iteration 17/25 | Loss: 0.00087692
Iteration 18/25 | Loss: 0.00087692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000876922276802361, 0.000876922276802361, 0.000876922276802361, 0.000876922276802361, 0.000876922276802361]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000876922276802361

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02888083
Iteration 2/25 | Loss: 0.00132229
Iteration 3/25 | Loss: 0.00132229
Iteration 4/25 | Loss: 0.00132228
Iteration 5/25 | Loss: 0.00132228
Iteration 6/25 | Loss: 0.00132228
Iteration 7/25 | Loss: 0.00132228
Iteration 8/25 | Loss: 0.00132228
Iteration 9/25 | Loss: 0.00132228
Iteration 10/25 | Loss: 0.00132228
Iteration 11/25 | Loss: 0.00132228
Iteration 12/25 | Loss: 0.00132228
Iteration 13/25 | Loss: 0.00132228
Iteration 14/25 | Loss: 0.00132228
Iteration 15/25 | Loss: 0.00132228
Iteration 16/25 | Loss: 0.00132228
Iteration 17/25 | Loss: 0.00132228
Iteration 18/25 | Loss: 0.00132228
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013222821289673448, 0.0013222821289673448, 0.0013222821289673448, 0.0013222821289673448, 0.0013222821289673448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013222821289673448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132228
Iteration 2/1000 | Loss: 0.00001667
Iteration 3/1000 | Loss: 0.00001198
Iteration 4/1000 | Loss: 0.00001060
Iteration 5/1000 | Loss: 0.00000987
Iteration 6/1000 | Loss: 0.00000929
Iteration 7/1000 | Loss: 0.00000898
Iteration 8/1000 | Loss: 0.00000857
Iteration 9/1000 | Loss: 0.00000841
Iteration 10/1000 | Loss: 0.00000839
Iteration 11/1000 | Loss: 0.00000834
Iteration 12/1000 | Loss: 0.00000825
Iteration 13/1000 | Loss: 0.00000825
Iteration 14/1000 | Loss: 0.00000820
Iteration 15/1000 | Loss: 0.00000815
Iteration 16/1000 | Loss: 0.00000814
Iteration 17/1000 | Loss: 0.00000813
Iteration 18/1000 | Loss: 0.00000813
Iteration 19/1000 | Loss: 0.00000813
Iteration 20/1000 | Loss: 0.00000813
Iteration 21/1000 | Loss: 0.00000813
Iteration 22/1000 | Loss: 0.00000813
Iteration 23/1000 | Loss: 0.00000813
Iteration 24/1000 | Loss: 0.00000813
Iteration 25/1000 | Loss: 0.00000806
Iteration 26/1000 | Loss: 0.00000806
Iteration 27/1000 | Loss: 0.00000801
Iteration 28/1000 | Loss: 0.00000799
Iteration 29/1000 | Loss: 0.00000798
Iteration 30/1000 | Loss: 0.00000798
Iteration 31/1000 | Loss: 0.00000798
Iteration 32/1000 | Loss: 0.00000797
Iteration 33/1000 | Loss: 0.00000797
Iteration 34/1000 | Loss: 0.00000796
Iteration 35/1000 | Loss: 0.00000795
Iteration 36/1000 | Loss: 0.00000794
Iteration 37/1000 | Loss: 0.00000793
Iteration 38/1000 | Loss: 0.00000793
Iteration 39/1000 | Loss: 0.00000793
Iteration 40/1000 | Loss: 0.00000793
Iteration 41/1000 | Loss: 0.00000793
Iteration 42/1000 | Loss: 0.00000793
Iteration 43/1000 | Loss: 0.00000792
Iteration 44/1000 | Loss: 0.00000792
Iteration 45/1000 | Loss: 0.00000792
Iteration 46/1000 | Loss: 0.00000792
Iteration 47/1000 | Loss: 0.00000791
Iteration 48/1000 | Loss: 0.00000791
Iteration 49/1000 | Loss: 0.00000790
Iteration 50/1000 | Loss: 0.00000790
Iteration 51/1000 | Loss: 0.00000789
Iteration 52/1000 | Loss: 0.00000789
Iteration 53/1000 | Loss: 0.00000789
Iteration 54/1000 | Loss: 0.00000789
Iteration 55/1000 | Loss: 0.00000788
Iteration 56/1000 | Loss: 0.00000788
Iteration 57/1000 | Loss: 0.00000788
Iteration 58/1000 | Loss: 0.00000788
Iteration 59/1000 | Loss: 0.00000788
Iteration 60/1000 | Loss: 0.00000787
Iteration 61/1000 | Loss: 0.00000787
Iteration 62/1000 | Loss: 0.00000787
Iteration 63/1000 | Loss: 0.00000787
Iteration 64/1000 | Loss: 0.00000787
Iteration 65/1000 | Loss: 0.00000786
Iteration 66/1000 | Loss: 0.00000786
Iteration 67/1000 | Loss: 0.00000786
Iteration 68/1000 | Loss: 0.00000786
Iteration 69/1000 | Loss: 0.00000786
Iteration 70/1000 | Loss: 0.00000785
Iteration 71/1000 | Loss: 0.00000785
Iteration 72/1000 | Loss: 0.00000785
Iteration 73/1000 | Loss: 0.00000785
Iteration 74/1000 | Loss: 0.00000785
Iteration 75/1000 | Loss: 0.00000785
Iteration 76/1000 | Loss: 0.00000785
Iteration 77/1000 | Loss: 0.00000784
Iteration 78/1000 | Loss: 0.00000784
Iteration 79/1000 | Loss: 0.00000784
Iteration 80/1000 | Loss: 0.00000784
Iteration 81/1000 | Loss: 0.00000783
Iteration 82/1000 | Loss: 0.00000783
Iteration 83/1000 | Loss: 0.00000783
Iteration 84/1000 | Loss: 0.00000783
Iteration 85/1000 | Loss: 0.00000783
Iteration 86/1000 | Loss: 0.00000783
Iteration 87/1000 | Loss: 0.00000782
Iteration 88/1000 | Loss: 0.00000782
Iteration 89/1000 | Loss: 0.00000782
Iteration 90/1000 | Loss: 0.00000782
Iteration 91/1000 | Loss: 0.00000781
Iteration 92/1000 | Loss: 0.00000781
Iteration 93/1000 | Loss: 0.00000781
Iteration 94/1000 | Loss: 0.00000781
Iteration 95/1000 | Loss: 0.00000781
Iteration 96/1000 | Loss: 0.00000781
Iteration 97/1000 | Loss: 0.00000780
Iteration 98/1000 | Loss: 0.00000780
Iteration 99/1000 | Loss: 0.00000780
Iteration 100/1000 | Loss: 0.00000780
Iteration 101/1000 | Loss: 0.00000779
Iteration 102/1000 | Loss: 0.00000779
Iteration 103/1000 | Loss: 0.00000779
Iteration 104/1000 | Loss: 0.00000779
Iteration 105/1000 | Loss: 0.00000779
Iteration 106/1000 | Loss: 0.00000779
Iteration 107/1000 | Loss: 0.00000779
Iteration 108/1000 | Loss: 0.00000779
Iteration 109/1000 | Loss: 0.00000779
Iteration 110/1000 | Loss: 0.00000779
Iteration 111/1000 | Loss: 0.00000779
Iteration 112/1000 | Loss: 0.00000778
Iteration 113/1000 | Loss: 0.00000778
Iteration 114/1000 | Loss: 0.00000778
Iteration 115/1000 | Loss: 0.00000778
Iteration 116/1000 | Loss: 0.00000778
Iteration 117/1000 | Loss: 0.00000778
Iteration 118/1000 | Loss: 0.00000778
Iteration 119/1000 | Loss: 0.00000778
Iteration 120/1000 | Loss: 0.00000778
Iteration 121/1000 | Loss: 0.00000778
Iteration 122/1000 | Loss: 0.00000778
Iteration 123/1000 | Loss: 0.00000778
Iteration 124/1000 | Loss: 0.00000778
Iteration 125/1000 | Loss: 0.00000778
Iteration 126/1000 | Loss: 0.00000777
Iteration 127/1000 | Loss: 0.00000777
Iteration 128/1000 | Loss: 0.00000777
Iteration 129/1000 | Loss: 0.00000777
Iteration 130/1000 | Loss: 0.00000777
Iteration 131/1000 | Loss: 0.00000777
Iteration 132/1000 | Loss: 0.00000777
Iteration 133/1000 | Loss: 0.00000777
Iteration 134/1000 | Loss: 0.00000777
Iteration 135/1000 | Loss: 0.00000777
Iteration 136/1000 | Loss: 0.00000776
Iteration 137/1000 | Loss: 0.00000776
Iteration 138/1000 | Loss: 0.00000776
Iteration 139/1000 | Loss: 0.00000776
Iteration 140/1000 | Loss: 0.00000776
Iteration 141/1000 | Loss: 0.00000776
Iteration 142/1000 | Loss: 0.00000776
Iteration 143/1000 | Loss: 0.00000776
Iteration 144/1000 | Loss: 0.00000776
Iteration 145/1000 | Loss: 0.00000776
Iteration 146/1000 | Loss: 0.00000776
Iteration 147/1000 | Loss: 0.00000776
Iteration 148/1000 | Loss: 0.00000776
Iteration 149/1000 | Loss: 0.00000776
Iteration 150/1000 | Loss: 0.00000776
Iteration 151/1000 | Loss: 0.00000776
Iteration 152/1000 | Loss: 0.00000776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [7.763430403429084e-06, 7.763430403429084e-06, 7.763430403429084e-06, 7.763430403429084e-06, 7.763430403429084e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.763430403429084e-06

Optimization complete. Final v2v error: 2.4107956886291504 mm

Highest mean error: 2.9138431549072266 mm for frame 209

Lowest mean error: 2.2241427898406982 mm for frame 79

Saving results

Total time: 39.48926854133606
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00648966
Iteration 2/25 | Loss: 0.00121380
Iteration 3/25 | Loss: 0.00099995
Iteration 4/25 | Loss: 0.00098210
Iteration 5/25 | Loss: 0.00097893
Iteration 6/25 | Loss: 0.00097858
Iteration 7/25 | Loss: 0.00097858
Iteration 8/25 | Loss: 0.00097858
Iteration 9/25 | Loss: 0.00097858
Iteration 10/25 | Loss: 0.00097858
Iteration 11/25 | Loss: 0.00097858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009785795118659735, 0.0009785795118659735, 0.0009785795118659735, 0.0009785795118659735, 0.0009785795118659735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009785795118659735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48728156
Iteration 2/25 | Loss: 0.00126141
Iteration 3/25 | Loss: 0.00126141
Iteration 4/25 | Loss: 0.00126141
Iteration 5/25 | Loss: 0.00126141
Iteration 6/25 | Loss: 0.00126141
Iteration 7/25 | Loss: 0.00126141
Iteration 8/25 | Loss: 0.00126141
Iteration 9/25 | Loss: 0.00126141
Iteration 10/25 | Loss: 0.00126141
Iteration 11/25 | Loss: 0.00126141
Iteration 12/25 | Loss: 0.00126141
Iteration 13/25 | Loss: 0.00126141
Iteration 14/25 | Loss: 0.00126141
Iteration 15/25 | Loss: 0.00126141
Iteration 16/25 | Loss: 0.00126141
Iteration 17/25 | Loss: 0.00126141
Iteration 18/25 | Loss: 0.00126141
Iteration 19/25 | Loss: 0.00126141
Iteration 20/25 | Loss: 0.00126141
Iteration 21/25 | Loss: 0.00126141
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001261408324353397, 0.001261408324353397, 0.001261408324353397, 0.001261408324353397, 0.001261408324353397]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001261408324353397

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126141
Iteration 2/1000 | Loss: 0.00005448
Iteration 3/1000 | Loss: 0.00002960
Iteration 4/1000 | Loss: 0.00002329
Iteration 5/1000 | Loss: 0.00002158
Iteration 6/1000 | Loss: 0.00002059
Iteration 7/1000 | Loss: 0.00001998
Iteration 8/1000 | Loss: 0.00001935
Iteration 9/1000 | Loss: 0.00001901
Iteration 10/1000 | Loss: 0.00001869
Iteration 11/1000 | Loss: 0.00001847
Iteration 12/1000 | Loss: 0.00001826
Iteration 13/1000 | Loss: 0.00001814
Iteration 14/1000 | Loss: 0.00001811
Iteration 15/1000 | Loss: 0.00001807
Iteration 16/1000 | Loss: 0.00001799
Iteration 17/1000 | Loss: 0.00001799
Iteration 18/1000 | Loss: 0.00001797
Iteration 19/1000 | Loss: 0.00001796
Iteration 20/1000 | Loss: 0.00001795
Iteration 21/1000 | Loss: 0.00001795
Iteration 22/1000 | Loss: 0.00001794
Iteration 23/1000 | Loss: 0.00001794
Iteration 24/1000 | Loss: 0.00001794
Iteration 25/1000 | Loss: 0.00001793
Iteration 26/1000 | Loss: 0.00001791
Iteration 27/1000 | Loss: 0.00001791
Iteration 28/1000 | Loss: 0.00001790
Iteration 29/1000 | Loss: 0.00001789
Iteration 30/1000 | Loss: 0.00001786
Iteration 31/1000 | Loss: 0.00001783
Iteration 32/1000 | Loss: 0.00001782
Iteration 33/1000 | Loss: 0.00001781
Iteration 34/1000 | Loss: 0.00001781
Iteration 35/1000 | Loss: 0.00001780
Iteration 36/1000 | Loss: 0.00001779
Iteration 37/1000 | Loss: 0.00001779
Iteration 38/1000 | Loss: 0.00001778
Iteration 39/1000 | Loss: 0.00001775
Iteration 40/1000 | Loss: 0.00001775
Iteration 41/1000 | Loss: 0.00001772
Iteration 42/1000 | Loss: 0.00001771
Iteration 43/1000 | Loss: 0.00001771
Iteration 44/1000 | Loss: 0.00001770
Iteration 45/1000 | Loss: 0.00001770
Iteration 46/1000 | Loss: 0.00001770
Iteration 47/1000 | Loss: 0.00001769
Iteration 48/1000 | Loss: 0.00001769
Iteration 49/1000 | Loss: 0.00001768
Iteration 50/1000 | Loss: 0.00001768
Iteration 51/1000 | Loss: 0.00001768
Iteration 52/1000 | Loss: 0.00001767
Iteration 53/1000 | Loss: 0.00001767
Iteration 54/1000 | Loss: 0.00001767
Iteration 55/1000 | Loss: 0.00001767
Iteration 56/1000 | Loss: 0.00001767
Iteration 57/1000 | Loss: 0.00001766
Iteration 58/1000 | Loss: 0.00001766
Iteration 59/1000 | Loss: 0.00001766
Iteration 60/1000 | Loss: 0.00001766
Iteration 61/1000 | Loss: 0.00001765
Iteration 62/1000 | Loss: 0.00001765
Iteration 63/1000 | Loss: 0.00001764
Iteration 64/1000 | Loss: 0.00001764
Iteration 65/1000 | Loss: 0.00001764
Iteration 66/1000 | Loss: 0.00001764
Iteration 67/1000 | Loss: 0.00001764
Iteration 68/1000 | Loss: 0.00001764
Iteration 69/1000 | Loss: 0.00001763
Iteration 70/1000 | Loss: 0.00001763
Iteration 71/1000 | Loss: 0.00001762
Iteration 72/1000 | Loss: 0.00001762
Iteration 73/1000 | Loss: 0.00001762
Iteration 74/1000 | Loss: 0.00001761
Iteration 75/1000 | Loss: 0.00001761
Iteration 76/1000 | Loss: 0.00001761
Iteration 77/1000 | Loss: 0.00001760
Iteration 78/1000 | Loss: 0.00001760
Iteration 79/1000 | Loss: 0.00001760
Iteration 80/1000 | Loss: 0.00001760
Iteration 81/1000 | Loss: 0.00001759
Iteration 82/1000 | Loss: 0.00001759
Iteration 83/1000 | Loss: 0.00001758
Iteration 84/1000 | Loss: 0.00001758
Iteration 85/1000 | Loss: 0.00001758
Iteration 86/1000 | Loss: 0.00001757
Iteration 87/1000 | Loss: 0.00001757
Iteration 88/1000 | Loss: 0.00001757
Iteration 89/1000 | Loss: 0.00001756
Iteration 90/1000 | Loss: 0.00001756
Iteration 91/1000 | Loss: 0.00001756
Iteration 92/1000 | Loss: 0.00001756
Iteration 93/1000 | Loss: 0.00001756
Iteration 94/1000 | Loss: 0.00001755
Iteration 95/1000 | Loss: 0.00001755
Iteration 96/1000 | Loss: 0.00001755
Iteration 97/1000 | Loss: 0.00001755
Iteration 98/1000 | Loss: 0.00001755
Iteration 99/1000 | Loss: 0.00001755
Iteration 100/1000 | Loss: 0.00001755
Iteration 101/1000 | Loss: 0.00001755
Iteration 102/1000 | Loss: 0.00001755
Iteration 103/1000 | Loss: 0.00001755
Iteration 104/1000 | Loss: 0.00001754
Iteration 105/1000 | Loss: 0.00001753
Iteration 106/1000 | Loss: 0.00001753
Iteration 107/1000 | Loss: 0.00001752
Iteration 108/1000 | Loss: 0.00001752
Iteration 109/1000 | Loss: 0.00001752
Iteration 110/1000 | Loss: 0.00001752
Iteration 111/1000 | Loss: 0.00001751
Iteration 112/1000 | Loss: 0.00001751
Iteration 113/1000 | Loss: 0.00001751
Iteration 114/1000 | Loss: 0.00001750
Iteration 115/1000 | Loss: 0.00001750
Iteration 116/1000 | Loss: 0.00001750
Iteration 117/1000 | Loss: 0.00001750
Iteration 118/1000 | Loss: 0.00001750
Iteration 119/1000 | Loss: 0.00001750
Iteration 120/1000 | Loss: 0.00001749
Iteration 121/1000 | Loss: 0.00001749
Iteration 122/1000 | Loss: 0.00001749
Iteration 123/1000 | Loss: 0.00001749
Iteration 124/1000 | Loss: 0.00001748
Iteration 125/1000 | Loss: 0.00001748
Iteration 126/1000 | Loss: 0.00001748
Iteration 127/1000 | Loss: 0.00001748
Iteration 128/1000 | Loss: 0.00001747
Iteration 129/1000 | Loss: 0.00001747
Iteration 130/1000 | Loss: 0.00001747
Iteration 131/1000 | Loss: 0.00001747
Iteration 132/1000 | Loss: 0.00001747
Iteration 133/1000 | Loss: 0.00001747
Iteration 134/1000 | Loss: 0.00001747
Iteration 135/1000 | Loss: 0.00001747
Iteration 136/1000 | Loss: 0.00001747
Iteration 137/1000 | Loss: 0.00001746
Iteration 138/1000 | Loss: 0.00001746
Iteration 139/1000 | Loss: 0.00001746
Iteration 140/1000 | Loss: 0.00001746
Iteration 141/1000 | Loss: 0.00001746
Iteration 142/1000 | Loss: 0.00001746
Iteration 143/1000 | Loss: 0.00001746
Iteration 144/1000 | Loss: 0.00001746
Iteration 145/1000 | Loss: 0.00001746
Iteration 146/1000 | Loss: 0.00001746
Iteration 147/1000 | Loss: 0.00001746
Iteration 148/1000 | Loss: 0.00001746
Iteration 149/1000 | Loss: 0.00001746
Iteration 150/1000 | Loss: 0.00001745
Iteration 151/1000 | Loss: 0.00001745
Iteration 152/1000 | Loss: 0.00001745
Iteration 153/1000 | Loss: 0.00001745
Iteration 154/1000 | Loss: 0.00001745
Iteration 155/1000 | Loss: 0.00001745
Iteration 156/1000 | Loss: 0.00001745
Iteration 157/1000 | Loss: 0.00001745
Iteration 158/1000 | Loss: 0.00001744
Iteration 159/1000 | Loss: 0.00001744
Iteration 160/1000 | Loss: 0.00001744
Iteration 161/1000 | Loss: 0.00001744
Iteration 162/1000 | Loss: 0.00001744
Iteration 163/1000 | Loss: 0.00001744
Iteration 164/1000 | Loss: 0.00001744
Iteration 165/1000 | Loss: 0.00001743
Iteration 166/1000 | Loss: 0.00001743
Iteration 167/1000 | Loss: 0.00001743
Iteration 168/1000 | Loss: 0.00001743
Iteration 169/1000 | Loss: 0.00001743
Iteration 170/1000 | Loss: 0.00001743
Iteration 171/1000 | Loss: 0.00001742
Iteration 172/1000 | Loss: 0.00001742
Iteration 173/1000 | Loss: 0.00001742
Iteration 174/1000 | Loss: 0.00001742
Iteration 175/1000 | Loss: 0.00001742
Iteration 176/1000 | Loss: 0.00001741
Iteration 177/1000 | Loss: 0.00001741
Iteration 178/1000 | Loss: 0.00001741
Iteration 179/1000 | Loss: 0.00001740
Iteration 180/1000 | Loss: 0.00001740
Iteration 181/1000 | Loss: 0.00001740
Iteration 182/1000 | Loss: 0.00001739
Iteration 183/1000 | Loss: 0.00001739
Iteration 184/1000 | Loss: 0.00001739
Iteration 185/1000 | Loss: 0.00001739
Iteration 186/1000 | Loss: 0.00001739
Iteration 187/1000 | Loss: 0.00001739
Iteration 188/1000 | Loss: 0.00001739
Iteration 189/1000 | Loss: 0.00001739
Iteration 190/1000 | Loss: 0.00001739
Iteration 191/1000 | Loss: 0.00001738
Iteration 192/1000 | Loss: 0.00001738
Iteration 193/1000 | Loss: 0.00001738
Iteration 194/1000 | Loss: 0.00001738
Iteration 195/1000 | Loss: 0.00001738
Iteration 196/1000 | Loss: 0.00001738
Iteration 197/1000 | Loss: 0.00001737
Iteration 198/1000 | Loss: 0.00001737
Iteration 199/1000 | Loss: 0.00001737
Iteration 200/1000 | Loss: 0.00001736
Iteration 201/1000 | Loss: 0.00001736
Iteration 202/1000 | Loss: 0.00001736
Iteration 203/1000 | Loss: 0.00001735
Iteration 204/1000 | Loss: 0.00001735
Iteration 205/1000 | Loss: 0.00001735
Iteration 206/1000 | Loss: 0.00001735
Iteration 207/1000 | Loss: 0.00001735
Iteration 208/1000 | Loss: 0.00001735
Iteration 209/1000 | Loss: 0.00001735
Iteration 210/1000 | Loss: 0.00001735
Iteration 211/1000 | Loss: 0.00001735
Iteration 212/1000 | Loss: 0.00001735
Iteration 213/1000 | Loss: 0.00001735
Iteration 214/1000 | Loss: 0.00001734
Iteration 215/1000 | Loss: 0.00001734
Iteration 216/1000 | Loss: 0.00001734
Iteration 217/1000 | Loss: 0.00001734
Iteration 218/1000 | Loss: 0.00001734
Iteration 219/1000 | Loss: 0.00001734
Iteration 220/1000 | Loss: 0.00001734
Iteration 221/1000 | Loss: 0.00001734
Iteration 222/1000 | Loss: 0.00001734
Iteration 223/1000 | Loss: 0.00001734
Iteration 224/1000 | Loss: 0.00001734
Iteration 225/1000 | Loss: 0.00001733
Iteration 226/1000 | Loss: 0.00001733
Iteration 227/1000 | Loss: 0.00001733
Iteration 228/1000 | Loss: 0.00001733
Iteration 229/1000 | Loss: 0.00001733
Iteration 230/1000 | Loss: 0.00001733
Iteration 231/1000 | Loss: 0.00001733
Iteration 232/1000 | Loss: 0.00001733
Iteration 233/1000 | Loss: 0.00001733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.7332014977000654e-05, 1.7332014977000654e-05, 1.7332014977000654e-05, 1.7332014977000654e-05, 1.7332014977000654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7332014977000654e-05

Optimization complete. Final v2v error: 3.3290343284606934 mm

Highest mean error: 4.425437927246094 mm for frame 14

Lowest mean error: 2.5975496768951416 mm for frame 37

Saving results

Total time: 51.62121891975403
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00367928
Iteration 2/25 | Loss: 0.00099190
Iteration 3/25 | Loss: 0.00091229
Iteration 4/25 | Loss: 0.00090623
Iteration 5/25 | Loss: 0.00090488
Iteration 6/25 | Loss: 0.00090485
Iteration 7/25 | Loss: 0.00090485
Iteration 8/25 | Loss: 0.00090485
Iteration 9/25 | Loss: 0.00090485
Iteration 10/25 | Loss: 0.00090485
Iteration 11/25 | Loss: 0.00090485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009048506617546082, 0.0009048506617546082, 0.0009048506617546082, 0.0009048506617546082, 0.0009048506617546082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009048506617546082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26539755
Iteration 2/25 | Loss: 0.00138873
Iteration 3/25 | Loss: 0.00138873
Iteration 4/25 | Loss: 0.00138873
Iteration 5/25 | Loss: 0.00138873
Iteration 6/25 | Loss: 0.00138873
Iteration 7/25 | Loss: 0.00138873
Iteration 8/25 | Loss: 0.00138873
Iteration 9/25 | Loss: 0.00138873
Iteration 10/25 | Loss: 0.00138873
Iteration 11/25 | Loss: 0.00138873
Iteration 12/25 | Loss: 0.00138873
Iteration 13/25 | Loss: 0.00138873
Iteration 14/25 | Loss: 0.00138873
Iteration 15/25 | Loss: 0.00138873
Iteration 16/25 | Loss: 0.00138873
Iteration 17/25 | Loss: 0.00138873
Iteration 18/25 | Loss: 0.00138873
Iteration 19/25 | Loss: 0.00138873
Iteration 20/25 | Loss: 0.00138873
Iteration 21/25 | Loss: 0.00138873
Iteration 22/25 | Loss: 0.00138873
Iteration 23/25 | Loss: 0.00138873
Iteration 24/25 | Loss: 0.00138873
Iteration 25/25 | Loss: 0.00138873

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138873
Iteration 2/1000 | Loss: 0.00002423
Iteration 3/1000 | Loss: 0.00001255
Iteration 4/1000 | Loss: 0.00001088
Iteration 5/1000 | Loss: 0.00000975
Iteration 6/1000 | Loss: 0.00000919
Iteration 7/1000 | Loss: 0.00000873
Iteration 8/1000 | Loss: 0.00000856
Iteration 9/1000 | Loss: 0.00000828
Iteration 10/1000 | Loss: 0.00000809
Iteration 11/1000 | Loss: 0.00000803
Iteration 12/1000 | Loss: 0.00000799
Iteration 13/1000 | Loss: 0.00000798
Iteration 14/1000 | Loss: 0.00000798
Iteration 15/1000 | Loss: 0.00000797
Iteration 16/1000 | Loss: 0.00000797
Iteration 17/1000 | Loss: 0.00000796
Iteration 18/1000 | Loss: 0.00000796
Iteration 19/1000 | Loss: 0.00000794
Iteration 20/1000 | Loss: 0.00000792
Iteration 21/1000 | Loss: 0.00000792
Iteration 22/1000 | Loss: 0.00000792
Iteration 23/1000 | Loss: 0.00000792
Iteration 24/1000 | Loss: 0.00000792
Iteration 25/1000 | Loss: 0.00000792
Iteration 26/1000 | Loss: 0.00000792
Iteration 27/1000 | Loss: 0.00000791
Iteration 28/1000 | Loss: 0.00000791
Iteration 29/1000 | Loss: 0.00000791
Iteration 30/1000 | Loss: 0.00000791
Iteration 31/1000 | Loss: 0.00000789
Iteration 32/1000 | Loss: 0.00000788
Iteration 33/1000 | Loss: 0.00000787
Iteration 34/1000 | Loss: 0.00000787
Iteration 35/1000 | Loss: 0.00000787
Iteration 36/1000 | Loss: 0.00000786
Iteration 37/1000 | Loss: 0.00000786
Iteration 38/1000 | Loss: 0.00000785
Iteration 39/1000 | Loss: 0.00000785
Iteration 40/1000 | Loss: 0.00000784
Iteration 41/1000 | Loss: 0.00000784
Iteration 42/1000 | Loss: 0.00000778
Iteration 43/1000 | Loss: 0.00000777
Iteration 44/1000 | Loss: 0.00000777
Iteration 45/1000 | Loss: 0.00000775
Iteration 46/1000 | Loss: 0.00000774
Iteration 47/1000 | Loss: 0.00000774
Iteration 48/1000 | Loss: 0.00000773
Iteration 49/1000 | Loss: 0.00000773
Iteration 50/1000 | Loss: 0.00000773
Iteration 51/1000 | Loss: 0.00000772
Iteration 52/1000 | Loss: 0.00000772
Iteration 53/1000 | Loss: 0.00000772
Iteration 54/1000 | Loss: 0.00000771
Iteration 55/1000 | Loss: 0.00000771
Iteration 56/1000 | Loss: 0.00000771
Iteration 57/1000 | Loss: 0.00000769
Iteration 58/1000 | Loss: 0.00000768
Iteration 59/1000 | Loss: 0.00000768
Iteration 60/1000 | Loss: 0.00000768
Iteration 61/1000 | Loss: 0.00000768
Iteration 62/1000 | Loss: 0.00000768
Iteration 63/1000 | Loss: 0.00000767
Iteration 64/1000 | Loss: 0.00000767
Iteration 65/1000 | Loss: 0.00000767
Iteration 66/1000 | Loss: 0.00000767
Iteration 67/1000 | Loss: 0.00000767
Iteration 68/1000 | Loss: 0.00000767
Iteration 69/1000 | Loss: 0.00000767
Iteration 70/1000 | Loss: 0.00000767
Iteration 71/1000 | Loss: 0.00000767
Iteration 72/1000 | Loss: 0.00000767
Iteration 73/1000 | Loss: 0.00000766
Iteration 74/1000 | Loss: 0.00000766
Iteration 75/1000 | Loss: 0.00000766
Iteration 76/1000 | Loss: 0.00000766
Iteration 77/1000 | Loss: 0.00000766
Iteration 78/1000 | Loss: 0.00000765
Iteration 79/1000 | Loss: 0.00000765
Iteration 80/1000 | Loss: 0.00000765
Iteration 81/1000 | Loss: 0.00000764
Iteration 82/1000 | Loss: 0.00000764
Iteration 83/1000 | Loss: 0.00000764
Iteration 84/1000 | Loss: 0.00000764
Iteration 85/1000 | Loss: 0.00000764
Iteration 86/1000 | Loss: 0.00000764
Iteration 87/1000 | Loss: 0.00000764
Iteration 88/1000 | Loss: 0.00000764
Iteration 89/1000 | Loss: 0.00000764
Iteration 90/1000 | Loss: 0.00000763
Iteration 91/1000 | Loss: 0.00000763
Iteration 92/1000 | Loss: 0.00000763
Iteration 93/1000 | Loss: 0.00000763
Iteration 94/1000 | Loss: 0.00000763
Iteration 95/1000 | Loss: 0.00000762
Iteration 96/1000 | Loss: 0.00000762
Iteration 97/1000 | Loss: 0.00000762
Iteration 98/1000 | Loss: 0.00000762
Iteration 99/1000 | Loss: 0.00000762
Iteration 100/1000 | Loss: 0.00000762
Iteration 101/1000 | Loss: 0.00000762
Iteration 102/1000 | Loss: 0.00000762
Iteration 103/1000 | Loss: 0.00000762
Iteration 104/1000 | Loss: 0.00000761
Iteration 105/1000 | Loss: 0.00000761
Iteration 106/1000 | Loss: 0.00000761
Iteration 107/1000 | Loss: 0.00000761
Iteration 108/1000 | Loss: 0.00000761
Iteration 109/1000 | Loss: 0.00000760
Iteration 110/1000 | Loss: 0.00000760
Iteration 111/1000 | Loss: 0.00000759
Iteration 112/1000 | Loss: 0.00000759
Iteration 113/1000 | Loss: 0.00000759
Iteration 114/1000 | Loss: 0.00000758
Iteration 115/1000 | Loss: 0.00000758
Iteration 116/1000 | Loss: 0.00000758
Iteration 117/1000 | Loss: 0.00000757
Iteration 118/1000 | Loss: 0.00000757
Iteration 119/1000 | Loss: 0.00000757
Iteration 120/1000 | Loss: 0.00000757
Iteration 121/1000 | Loss: 0.00000757
Iteration 122/1000 | Loss: 0.00000757
Iteration 123/1000 | Loss: 0.00000757
Iteration 124/1000 | Loss: 0.00000757
Iteration 125/1000 | Loss: 0.00000757
Iteration 126/1000 | Loss: 0.00000757
Iteration 127/1000 | Loss: 0.00000757
Iteration 128/1000 | Loss: 0.00000757
Iteration 129/1000 | Loss: 0.00000757
Iteration 130/1000 | Loss: 0.00000757
Iteration 131/1000 | Loss: 0.00000757
Iteration 132/1000 | Loss: 0.00000757
Iteration 133/1000 | Loss: 0.00000757
Iteration 134/1000 | Loss: 0.00000756
Iteration 135/1000 | Loss: 0.00000756
Iteration 136/1000 | Loss: 0.00000756
Iteration 137/1000 | Loss: 0.00000756
Iteration 138/1000 | Loss: 0.00000756
Iteration 139/1000 | Loss: 0.00000756
Iteration 140/1000 | Loss: 0.00000756
Iteration 141/1000 | Loss: 0.00000756
Iteration 142/1000 | Loss: 0.00000755
Iteration 143/1000 | Loss: 0.00000755
Iteration 144/1000 | Loss: 0.00000755
Iteration 145/1000 | Loss: 0.00000755
Iteration 146/1000 | Loss: 0.00000755
Iteration 147/1000 | Loss: 0.00000755
Iteration 148/1000 | Loss: 0.00000755
Iteration 149/1000 | Loss: 0.00000755
Iteration 150/1000 | Loss: 0.00000755
Iteration 151/1000 | Loss: 0.00000755
Iteration 152/1000 | Loss: 0.00000755
Iteration 153/1000 | Loss: 0.00000755
Iteration 154/1000 | Loss: 0.00000755
Iteration 155/1000 | Loss: 0.00000755
Iteration 156/1000 | Loss: 0.00000755
Iteration 157/1000 | Loss: 0.00000755
Iteration 158/1000 | Loss: 0.00000755
Iteration 159/1000 | Loss: 0.00000755
Iteration 160/1000 | Loss: 0.00000755
Iteration 161/1000 | Loss: 0.00000755
Iteration 162/1000 | Loss: 0.00000755
Iteration 163/1000 | Loss: 0.00000755
Iteration 164/1000 | Loss: 0.00000754
Iteration 165/1000 | Loss: 0.00000754
Iteration 166/1000 | Loss: 0.00000754
Iteration 167/1000 | Loss: 0.00000754
Iteration 168/1000 | Loss: 0.00000754
Iteration 169/1000 | Loss: 0.00000754
Iteration 170/1000 | Loss: 0.00000754
Iteration 171/1000 | Loss: 0.00000754
Iteration 172/1000 | Loss: 0.00000754
Iteration 173/1000 | Loss: 0.00000754
Iteration 174/1000 | Loss: 0.00000754
Iteration 175/1000 | Loss: 0.00000754
Iteration 176/1000 | Loss: 0.00000754
Iteration 177/1000 | Loss: 0.00000754
Iteration 178/1000 | Loss: 0.00000754
Iteration 179/1000 | Loss: 0.00000754
Iteration 180/1000 | Loss: 0.00000754
Iteration 181/1000 | Loss: 0.00000754
Iteration 182/1000 | Loss: 0.00000754
Iteration 183/1000 | Loss: 0.00000754
Iteration 184/1000 | Loss: 0.00000754
Iteration 185/1000 | Loss: 0.00000754
Iteration 186/1000 | Loss: 0.00000754
Iteration 187/1000 | Loss: 0.00000754
Iteration 188/1000 | Loss: 0.00000754
Iteration 189/1000 | Loss: 0.00000754
Iteration 190/1000 | Loss: 0.00000754
Iteration 191/1000 | Loss: 0.00000754
Iteration 192/1000 | Loss: 0.00000754
Iteration 193/1000 | Loss: 0.00000754
Iteration 194/1000 | Loss: 0.00000754
Iteration 195/1000 | Loss: 0.00000754
Iteration 196/1000 | Loss: 0.00000754
Iteration 197/1000 | Loss: 0.00000754
Iteration 198/1000 | Loss: 0.00000754
Iteration 199/1000 | Loss: 0.00000754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [7.539448688476114e-06, 7.539448688476114e-06, 7.539448688476114e-06, 7.539448688476114e-06, 7.539448688476114e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.539448688476114e-06

Optimization complete. Final v2v error: 2.3446335792541504 mm

Highest mean error: 2.758892059326172 mm for frame 143

Lowest mean error: 2.1551146507263184 mm for frame 72

Saving results

Total time: 34.40654897689819
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103792
Iteration 2/25 | Loss: 0.01103792
Iteration 3/25 | Loss: 0.01103792
Iteration 4/25 | Loss: 0.01103792
Iteration 5/25 | Loss: 0.01103792
Iteration 6/25 | Loss: 0.01103792
Iteration 7/25 | Loss: 0.01103792
Iteration 8/25 | Loss: 0.01103792
Iteration 9/25 | Loss: 0.01103792
Iteration 10/25 | Loss: 0.01103791
Iteration 11/25 | Loss: 0.01103791
Iteration 12/25 | Loss: 0.01103791
Iteration 13/25 | Loss: 0.01103791
Iteration 14/25 | Loss: 0.01103791
Iteration 15/25 | Loss: 0.01103791
Iteration 16/25 | Loss: 0.01103791
Iteration 17/25 | Loss: 0.01103791
Iteration 18/25 | Loss: 0.01103791
Iteration 19/25 | Loss: 0.01103791
Iteration 20/25 | Loss: 0.01103790
Iteration 21/25 | Loss: 0.01103790
Iteration 22/25 | Loss: 0.01103790
Iteration 23/25 | Loss: 0.01103790
Iteration 24/25 | Loss: 0.01103790
Iteration 25/25 | Loss: 0.01103790

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61477900
Iteration 2/25 | Loss: 0.15530731
Iteration 3/25 | Loss: 0.15508658
Iteration 4/25 | Loss: 0.15496403
Iteration 5/25 | Loss: 0.15489884
Iteration 6/25 | Loss: 0.15489876
Iteration 7/25 | Loss: 0.15489872
Iteration 8/25 | Loss: 0.15489872
Iteration 9/25 | Loss: 0.15489869
Iteration 10/25 | Loss: 0.15489869
Iteration 11/25 | Loss: 0.15489869
Iteration 12/25 | Loss: 0.15489867
Iteration 13/25 | Loss: 0.15489869
Iteration 14/25 | Loss: 0.15489869
Iteration 15/25 | Loss: 0.15489866
Iteration 16/25 | Loss: 0.15489866
Iteration 17/25 | Loss: 0.15489866
Iteration 18/25 | Loss: 0.15489866
Iteration 19/25 | Loss: 0.15489866
Iteration 20/25 | Loss: 0.15489866
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.15489865839481354, 0.15489865839481354, 0.15489865839481354, 0.15489865839481354, 0.15489865839481354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.15489865839481354

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.15489866
Iteration 2/1000 | Loss: 0.00281683
Iteration 3/1000 | Loss: 0.00074374
Iteration 4/1000 | Loss: 0.00033341
Iteration 5/1000 | Loss: 0.00018250
Iteration 6/1000 | Loss: 0.00017248
Iteration 7/1000 | Loss: 0.00010340
Iteration 8/1000 | Loss: 0.00007727
Iteration 9/1000 | Loss: 0.00011203
Iteration 10/1000 | Loss: 0.00019029
Iteration 11/1000 | Loss: 0.00006290
Iteration 12/1000 | Loss: 0.00008327
Iteration 13/1000 | Loss: 0.00020836
Iteration 14/1000 | Loss: 0.00009574
Iteration 15/1000 | Loss: 0.00004468
Iteration 16/1000 | Loss: 0.00009278
Iteration 17/1000 | Loss: 0.00004579
Iteration 18/1000 | Loss: 0.00004365
Iteration 19/1000 | Loss: 0.00004290
Iteration 20/1000 | Loss: 0.00004147
Iteration 21/1000 | Loss: 0.00007164
Iteration 22/1000 | Loss: 0.00004179
Iteration 23/1000 | Loss: 0.00004714
Iteration 24/1000 | Loss: 0.00003910
Iteration 25/1000 | Loss: 0.00007420
Iteration 26/1000 | Loss: 0.00005773
Iteration 27/1000 | Loss: 0.00003346
Iteration 28/1000 | Loss: 0.00003192
Iteration 29/1000 | Loss: 0.00003636
Iteration 30/1000 | Loss: 0.00003092
Iteration 31/1000 | Loss: 0.00005149
Iteration 32/1000 | Loss: 0.00010020
Iteration 33/1000 | Loss: 0.00004711
Iteration 34/1000 | Loss: 0.00006986
Iteration 35/1000 | Loss: 0.00002874
Iteration 36/1000 | Loss: 0.00003281
Iteration 37/1000 | Loss: 0.00006457
Iteration 38/1000 | Loss: 0.00005515
Iteration 39/1000 | Loss: 0.00003109
Iteration 40/1000 | Loss: 0.00003635
Iteration 41/1000 | Loss: 0.00003100
Iteration 42/1000 | Loss: 0.00002823
Iteration 43/1000 | Loss: 0.00005427
Iteration 44/1000 | Loss: 0.00002742
Iteration 45/1000 | Loss: 0.00003728
Iteration 46/1000 | Loss: 0.00006107
Iteration 47/1000 | Loss: 0.00003244
Iteration 48/1000 | Loss: 0.00007933
Iteration 49/1000 | Loss: 0.00003610
Iteration 50/1000 | Loss: 0.00002717
Iteration 51/1000 | Loss: 0.00002714
Iteration 52/1000 | Loss: 0.00004216
Iteration 53/1000 | Loss: 0.00002707
Iteration 54/1000 | Loss: 0.00002701
Iteration 55/1000 | Loss: 0.00002701
Iteration 56/1000 | Loss: 0.00005532
Iteration 57/1000 | Loss: 0.00002723
Iteration 58/1000 | Loss: 0.00002697
Iteration 59/1000 | Loss: 0.00007264
Iteration 60/1000 | Loss: 0.00008506
Iteration 61/1000 | Loss: 0.00003205
Iteration 62/1000 | Loss: 0.00006633
Iteration 63/1000 | Loss: 0.00003220
Iteration 64/1000 | Loss: 0.00002694
Iteration 65/1000 | Loss: 0.00002691
Iteration 66/1000 | Loss: 0.00002690
Iteration 67/1000 | Loss: 0.00002689
Iteration 68/1000 | Loss: 0.00002688
Iteration 69/1000 | Loss: 0.00002687
Iteration 70/1000 | Loss: 0.00002686
Iteration 71/1000 | Loss: 0.00005904
Iteration 72/1000 | Loss: 0.00006799
Iteration 73/1000 | Loss: 0.00002691
Iteration 74/1000 | Loss: 0.00002682
Iteration 75/1000 | Loss: 0.00002682
Iteration 76/1000 | Loss: 0.00002682
Iteration 77/1000 | Loss: 0.00002682
Iteration 78/1000 | Loss: 0.00002682
Iteration 79/1000 | Loss: 0.00002682
Iteration 80/1000 | Loss: 0.00002682
Iteration 81/1000 | Loss: 0.00002682
Iteration 82/1000 | Loss: 0.00002682
Iteration 83/1000 | Loss: 0.00002682
Iteration 84/1000 | Loss: 0.00002681
Iteration 85/1000 | Loss: 0.00002681
Iteration 86/1000 | Loss: 0.00002681
Iteration 87/1000 | Loss: 0.00002680
Iteration 88/1000 | Loss: 0.00002680
Iteration 89/1000 | Loss: 0.00002680
Iteration 90/1000 | Loss: 0.00002680
Iteration 91/1000 | Loss: 0.00002679
Iteration 92/1000 | Loss: 0.00002679
Iteration 93/1000 | Loss: 0.00002679
Iteration 94/1000 | Loss: 0.00002679
Iteration 95/1000 | Loss: 0.00002679
Iteration 96/1000 | Loss: 0.00002678
Iteration 97/1000 | Loss: 0.00002678
Iteration 98/1000 | Loss: 0.00002678
Iteration 99/1000 | Loss: 0.00002678
Iteration 100/1000 | Loss: 0.00002678
Iteration 101/1000 | Loss: 0.00002678
Iteration 102/1000 | Loss: 0.00006979
Iteration 103/1000 | Loss: 0.00007635
Iteration 104/1000 | Loss: 0.00005249
Iteration 105/1000 | Loss: 0.00002706
Iteration 106/1000 | Loss: 0.00002683
Iteration 107/1000 | Loss: 0.00002678
Iteration 108/1000 | Loss: 0.00002667
Iteration 109/1000 | Loss: 0.00002663
Iteration 110/1000 | Loss: 0.00002663
Iteration 111/1000 | Loss: 0.00002663
Iteration 112/1000 | Loss: 0.00002662
Iteration 113/1000 | Loss: 0.00002661
Iteration 114/1000 | Loss: 0.00002661
Iteration 115/1000 | Loss: 0.00002660
Iteration 116/1000 | Loss: 0.00002660
Iteration 117/1000 | Loss: 0.00002660
Iteration 118/1000 | Loss: 0.00002660
Iteration 119/1000 | Loss: 0.00002659
Iteration 120/1000 | Loss: 0.00002659
Iteration 121/1000 | Loss: 0.00002659
Iteration 122/1000 | Loss: 0.00002659
Iteration 123/1000 | Loss: 0.00002659
Iteration 124/1000 | Loss: 0.00002659
Iteration 125/1000 | Loss: 0.00002659
Iteration 126/1000 | Loss: 0.00002659
Iteration 127/1000 | Loss: 0.00002659
Iteration 128/1000 | Loss: 0.00002659
Iteration 129/1000 | Loss: 0.00002658
Iteration 130/1000 | Loss: 0.00002658
Iteration 131/1000 | Loss: 0.00002658
Iteration 132/1000 | Loss: 0.00002658
Iteration 133/1000 | Loss: 0.00002658
Iteration 134/1000 | Loss: 0.00002658
Iteration 135/1000 | Loss: 0.00002658
Iteration 136/1000 | Loss: 0.00002658
Iteration 137/1000 | Loss: 0.00002658
Iteration 138/1000 | Loss: 0.00002658
Iteration 139/1000 | Loss: 0.00002658
Iteration 140/1000 | Loss: 0.00002658
Iteration 141/1000 | Loss: 0.00002658
Iteration 142/1000 | Loss: 0.00002657
Iteration 143/1000 | Loss: 0.00002657
Iteration 144/1000 | Loss: 0.00002657
Iteration 145/1000 | Loss: 0.00002657
Iteration 146/1000 | Loss: 0.00002657
Iteration 147/1000 | Loss: 0.00002657
Iteration 148/1000 | Loss: 0.00002657
Iteration 149/1000 | Loss: 0.00002657
Iteration 150/1000 | Loss: 0.00002657
Iteration 151/1000 | Loss: 0.00002657
Iteration 152/1000 | Loss: 0.00002657
Iteration 153/1000 | Loss: 0.00002657
Iteration 154/1000 | Loss: 0.00002657
Iteration 155/1000 | Loss: 0.00002657
Iteration 156/1000 | Loss: 0.00002657
Iteration 157/1000 | Loss: 0.00002656
Iteration 158/1000 | Loss: 0.00002656
Iteration 159/1000 | Loss: 0.00002656
Iteration 160/1000 | Loss: 0.00004000
Iteration 161/1000 | Loss: 0.00002909
Iteration 162/1000 | Loss: 0.00002655
Iteration 163/1000 | Loss: 0.00002655
Iteration 164/1000 | Loss: 0.00002655
Iteration 165/1000 | Loss: 0.00002655
Iteration 166/1000 | Loss: 0.00002654
Iteration 167/1000 | Loss: 0.00002654
Iteration 168/1000 | Loss: 0.00002654
Iteration 169/1000 | Loss: 0.00002655
Iteration 170/1000 | Loss: 0.00002655
Iteration 171/1000 | Loss: 0.00002655
Iteration 172/1000 | Loss: 0.00002655
Iteration 173/1000 | Loss: 0.00002655
Iteration 174/1000 | Loss: 0.00002655
Iteration 175/1000 | Loss: 0.00002655
Iteration 176/1000 | Loss: 0.00002655
Iteration 177/1000 | Loss: 0.00002655
Iteration 178/1000 | Loss: 0.00002655
Iteration 179/1000 | Loss: 0.00002654
Iteration 180/1000 | Loss: 0.00002654
Iteration 181/1000 | Loss: 0.00002654
Iteration 182/1000 | Loss: 0.00002654
Iteration 183/1000 | Loss: 0.00002654
Iteration 184/1000 | Loss: 0.00002654
Iteration 185/1000 | Loss: 0.00002654
Iteration 186/1000 | Loss: 0.00002654
Iteration 187/1000 | Loss: 0.00002654
Iteration 188/1000 | Loss: 0.00002654
Iteration 189/1000 | Loss: 0.00002654
Iteration 190/1000 | Loss: 0.00002654
Iteration 191/1000 | Loss: 0.00002654
Iteration 192/1000 | Loss: 0.00002653
Iteration 193/1000 | Loss: 0.00002653
Iteration 194/1000 | Loss: 0.00002653
Iteration 195/1000 | Loss: 0.00002653
Iteration 196/1000 | Loss: 0.00002652
Iteration 197/1000 | Loss: 0.00002652
Iteration 198/1000 | Loss: 0.00002652
Iteration 199/1000 | Loss: 0.00002652
Iteration 200/1000 | Loss: 0.00002652
Iteration 201/1000 | Loss: 0.00002652
Iteration 202/1000 | Loss: 0.00003565
Iteration 203/1000 | Loss: 0.00006076
Iteration 204/1000 | Loss: 0.00021513
Iteration 205/1000 | Loss: 0.00014501
Iteration 206/1000 | Loss: 0.00017518
Iteration 207/1000 | Loss: 0.00005984
Iteration 208/1000 | Loss: 0.00004376
Iteration 209/1000 | Loss: 0.00003067
Iteration 210/1000 | Loss: 0.00007502
Iteration 211/1000 | Loss: 0.00007073
Iteration 212/1000 | Loss: 0.00005742
Iteration 213/1000 | Loss: 0.00002897
Iteration 214/1000 | Loss: 0.00002772
Iteration 215/1000 | Loss: 0.00003680
Iteration 216/1000 | Loss: 0.00011544
Iteration 217/1000 | Loss: 0.00004870
Iteration 218/1000 | Loss: 0.00003528
Iteration 219/1000 | Loss: 0.00003273
Iteration 220/1000 | Loss: 0.00002727
Iteration 221/1000 | Loss: 0.00002725
Iteration 222/1000 | Loss: 0.00002725
Iteration 223/1000 | Loss: 0.00002725
Iteration 224/1000 | Loss: 0.00002725
Iteration 225/1000 | Loss: 0.00002725
Iteration 226/1000 | Loss: 0.00002724
Iteration 227/1000 | Loss: 0.00002724
Iteration 228/1000 | Loss: 0.00002724
Iteration 229/1000 | Loss: 0.00002724
Iteration 230/1000 | Loss: 0.00002724
Iteration 231/1000 | Loss: 0.00002724
Iteration 232/1000 | Loss: 0.00002724
Iteration 233/1000 | Loss: 0.00002724
Iteration 234/1000 | Loss: 0.00002724
Iteration 235/1000 | Loss: 0.00002724
Iteration 236/1000 | Loss: 0.00002724
Iteration 237/1000 | Loss: 0.00002723
Iteration 238/1000 | Loss: 0.00004297
Iteration 239/1000 | Loss: 0.00004297
Iteration 240/1000 | Loss: 0.00009010
Iteration 241/1000 | Loss: 0.00006826
Iteration 242/1000 | Loss: 0.00003926
Iteration 243/1000 | Loss: 0.00002728
Iteration 244/1000 | Loss: 0.00002725
Iteration 245/1000 | Loss: 0.00002725
Iteration 246/1000 | Loss: 0.00002724
Iteration 247/1000 | Loss: 0.00002724
Iteration 248/1000 | Loss: 0.00002724
Iteration 249/1000 | Loss: 0.00002724
Iteration 250/1000 | Loss: 0.00002724
Iteration 251/1000 | Loss: 0.00002723
Iteration 252/1000 | Loss: 0.00002722
Iteration 253/1000 | Loss: 0.00002722
Iteration 254/1000 | Loss: 0.00002724
Iteration 255/1000 | Loss: 0.00002724
Iteration 256/1000 | Loss: 0.00002723
Iteration 257/1000 | Loss: 0.00002723
Iteration 258/1000 | Loss: 0.00002723
Iteration 259/1000 | Loss: 0.00002722
Iteration 260/1000 | Loss: 0.00002722
Iteration 261/1000 | Loss: 0.00002722
Iteration 262/1000 | Loss: 0.00002721
Iteration 263/1000 | Loss: 0.00002721
Iteration 264/1000 | Loss: 0.00002721
Iteration 265/1000 | Loss: 0.00002721
Iteration 266/1000 | Loss: 0.00002719
Iteration 267/1000 | Loss: 0.00002719
Iteration 268/1000 | Loss: 0.00002719
Iteration 269/1000 | Loss: 0.00002719
Iteration 270/1000 | Loss: 0.00002719
Iteration 271/1000 | Loss: 0.00002718
Iteration 272/1000 | Loss: 0.00002718
Iteration 273/1000 | Loss: 0.00002718
Iteration 274/1000 | Loss: 0.00002718
Iteration 275/1000 | Loss: 0.00002718
Iteration 276/1000 | Loss: 0.00002718
Iteration 277/1000 | Loss: 0.00002718
Iteration 278/1000 | Loss: 0.00002718
Iteration 279/1000 | Loss: 0.00002718
Iteration 280/1000 | Loss: 0.00002718
Iteration 281/1000 | Loss: 0.00002718
Iteration 282/1000 | Loss: 0.00002718
Iteration 283/1000 | Loss: 0.00002718
Iteration 284/1000 | Loss: 0.00002718
Iteration 285/1000 | Loss: 0.00002718
Iteration 286/1000 | Loss: 0.00002718
Iteration 287/1000 | Loss: 0.00002718
Iteration 288/1000 | Loss: 0.00002718
Iteration 289/1000 | Loss: 0.00002718
Iteration 290/1000 | Loss: 0.00002718
Iteration 291/1000 | Loss: 0.00002718
Iteration 292/1000 | Loss: 0.00002718
Iteration 293/1000 | Loss: 0.00002718
Iteration 294/1000 | Loss: 0.00002718
Iteration 295/1000 | Loss: 0.00002718
Iteration 296/1000 | Loss: 0.00002718
Iteration 297/1000 | Loss: 0.00002718
Iteration 298/1000 | Loss: 0.00002718
Iteration 299/1000 | Loss: 0.00002718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 299. Stopping optimization.
Last 5 losses: [2.7175468858331442e-05, 2.7175468858331442e-05, 2.7175468858331442e-05, 2.7175468858331442e-05, 2.7175468858331442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7175468858331442e-05

Optimization complete. Final v2v error: 3.7593822479248047 mm

Highest mean error: 22.011579513549805 mm for frame 23

Lowest mean error: 3.0162510871887207 mm for frame 1

Saving results

Total time: 168.53353810310364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794882
Iteration 2/25 | Loss: 0.00142972
Iteration 3/25 | Loss: 0.00102008
Iteration 4/25 | Loss: 0.00098962
Iteration 5/25 | Loss: 0.00098585
Iteration 6/25 | Loss: 0.00098558
Iteration 7/25 | Loss: 0.00098558
Iteration 8/25 | Loss: 0.00098558
Iteration 9/25 | Loss: 0.00098558
Iteration 10/25 | Loss: 0.00098558
Iteration 11/25 | Loss: 0.00098558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009855757234618068, 0.0009855757234618068, 0.0009855757234618068, 0.0009855757234618068, 0.0009855757234618068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009855757234618068

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22704744
Iteration 2/25 | Loss: 0.00126187
Iteration 3/25 | Loss: 0.00126186
Iteration 4/25 | Loss: 0.00126186
Iteration 5/25 | Loss: 0.00126186
Iteration 6/25 | Loss: 0.00126186
Iteration 7/25 | Loss: 0.00126186
Iteration 8/25 | Loss: 0.00126186
Iteration 9/25 | Loss: 0.00126186
Iteration 10/25 | Loss: 0.00126186
Iteration 11/25 | Loss: 0.00126186
Iteration 12/25 | Loss: 0.00126186
Iteration 13/25 | Loss: 0.00126186
Iteration 14/25 | Loss: 0.00126186
Iteration 15/25 | Loss: 0.00126186
Iteration 16/25 | Loss: 0.00126186
Iteration 17/25 | Loss: 0.00126186
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012618564069271088, 0.0012618564069271088, 0.0012618564069271088, 0.0012618564069271088, 0.0012618564069271088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012618564069271088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126186
Iteration 2/1000 | Loss: 0.00003333
Iteration 3/1000 | Loss: 0.00002114
Iteration 4/1000 | Loss: 0.00001916
Iteration 5/1000 | Loss: 0.00001788
Iteration 6/1000 | Loss: 0.00001720
Iteration 7/1000 | Loss: 0.00001684
Iteration 8/1000 | Loss: 0.00001653
Iteration 9/1000 | Loss: 0.00001609
Iteration 10/1000 | Loss: 0.00001589
Iteration 11/1000 | Loss: 0.00001571
Iteration 12/1000 | Loss: 0.00001544
Iteration 13/1000 | Loss: 0.00001542
Iteration 14/1000 | Loss: 0.00001532
Iteration 15/1000 | Loss: 0.00001528
Iteration 16/1000 | Loss: 0.00001527
Iteration 17/1000 | Loss: 0.00001522
Iteration 18/1000 | Loss: 0.00001519
Iteration 19/1000 | Loss: 0.00001515
Iteration 20/1000 | Loss: 0.00001515
Iteration 21/1000 | Loss: 0.00001515
Iteration 22/1000 | Loss: 0.00001515
Iteration 23/1000 | Loss: 0.00001515
Iteration 24/1000 | Loss: 0.00001515
Iteration 25/1000 | Loss: 0.00001515
Iteration 26/1000 | Loss: 0.00001515
Iteration 27/1000 | Loss: 0.00001515
Iteration 28/1000 | Loss: 0.00001514
Iteration 29/1000 | Loss: 0.00001514
Iteration 30/1000 | Loss: 0.00001513
Iteration 31/1000 | Loss: 0.00001512
Iteration 32/1000 | Loss: 0.00001511
Iteration 33/1000 | Loss: 0.00001511
Iteration 34/1000 | Loss: 0.00001511
Iteration 35/1000 | Loss: 0.00001510
Iteration 36/1000 | Loss: 0.00001509
Iteration 37/1000 | Loss: 0.00001508
Iteration 38/1000 | Loss: 0.00001508
Iteration 39/1000 | Loss: 0.00001507
Iteration 40/1000 | Loss: 0.00001507
Iteration 41/1000 | Loss: 0.00001507
Iteration 42/1000 | Loss: 0.00001506
Iteration 43/1000 | Loss: 0.00001506
Iteration 44/1000 | Loss: 0.00001505
Iteration 45/1000 | Loss: 0.00001505
Iteration 46/1000 | Loss: 0.00001505
Iteration 47/1000 | Loss: 0.00001504
Iteration 48/1000 | Loss: 0.00001504
Iteration 49/1000 | Loss: 0.00001504
Iteration 50/1000 | Loss: 0.00001504
Iteration 51/1000 | Loss: 0.00001504
Iteration 52/1000 | Loss: 0.00001504
Iteration 53/1000 | Loss: 0.00001504
Iteration 54/1000 | Loss: 0.00001504
Iteration 55/1000 | Loss: 0.00001504
Iteration 56/1000 | Loss: 0.00001504
Iteration 57/1000 | Loss: 0.00001504
Iteration 58/1000 | Loss: 0.00001504
Iteration 59/1000 | Loss: 0.00001504
Iteration 60/1000 | Loss: 0.00001504
Iteration 61/1000 | Loss: 0.00001504
Iteration 62/1000 | Loss: 0.00001504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [1.5037814591778442e-05, 1.5037814591778442e-05, 1.5037814591778442e-05, 1.5037814591778442e-05, 1.5037814591778442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5037814591778442e-05

Optimization complete. Final v2v error: 3.27260684967041 mm

Highest mean error: 3.5707461833953857 mm for frame 68

Lowest mean error: 2.8220503330230713 mm for frame 0

Saving results

Total time: 34.64638876914978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790621
Iteration 2/25 | Loss: 0.00104952
Iteration 3/25 | Loss: 0.00096702
Iteration 4/25 | Loss: 0.00095485
Iteration 5/25 | Loss: 0.00095168
Iteration 6/25 | Loss: 0.00095051
Iteration 7/25 | Loss: 0.00095044
Iteration 8/25 | Loss: 0.00095044
Iteration 9/25 | Loss: 0.00095044
Iteration 10/25 | Loss: 0.00095044
Iteration 11/25 | Loss: 0.00095044
Iteration 12/25 | Loss: 0.00095044
Iteration 13/25 | Loss: 0.00095044
Iteration 14/25 | Loss: 0.00095044
Iteration 15/25 | Loss: 0.00095044
Iteration 16/25 | Loss: 0.00095044
Iteration 17/25 | Loss: 0.00095044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009504427434876561, 0.0009504427434876561, 0.0009504427434876561, 0.0009504427434876561, 0.0009504427434876561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009504427434876561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24738574
Iteration 2/25 | Loss: 0.00169620
Iteration 3/25 | Loss: 0.00169620
Iteration 4/25 | Loss: 0.00169620
Iteration 5/25 | Loss: 0.00169620
Iteration 6/25 | Loss: 0.00169620
Iteration 7/25 | Loss: 0.00169620
Iteration 8/25 | Loss: 0.00169620
Iteration 9/25 | Loss: 0.00169620
Iteration 10/25 | Loss: 0.00169620
Iteration 11/25 | Loss: 0.00169620
Iteration 12/25 | Loss: 0.00169620
Iteration 13/25 | Loss: 0.00169620
Iteration 14/25 | Loss: 0.00169620
Iteration 15/25 | Loss: 0.00169620
Iteration 16/25 | Loss: 0.00169620
Iteration 17/25 | Loss: 0.00169620
Iteration 18/25 | Loss: 0.00169620
Iteration 19/25 | Loss: 0.00169620
Iteration 20/25 | Loss: 0.00169620
Iteration 21/25 | Loss: 0.00169620
Iteration 22/25 | Loss: 0.00169620
Iteration 23/25 | Loss: 0.00169620
Iteration 24/25 | Loss: 0.00169620
Iteration 25/25 | Loss: 0.00169619

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169620
Iteration 2/1000 | Loss: 0.00003608
Iteration 3/1000 | Loss: 0.00001934
Iteration 4/1000 | Loss: 0.00001599
Iteration 5/1000 | Loss: 0.00001476
Iteration 6/1000 | Loss: 0.00001378
Iteration 7/1000 | Loss: 0.00001328
Iteration 8/1000 | Loss: 0.00001271
Iteration 9/1000 | Loss: 0.00001234
Iteration 10/1000 | Loss: 0.00001204
Iteration 11/1000 | Loss: 0.00001181
Iteration 12/1000 | Loss: 0.00001167
Iteration 13/1000 | Loss: 0.00001161
Iteration 14/1000 | Loss: 0.00001148
Iteration 15/1000 | Loss: 0.00001142
Iteration 16/1000 | Loss: 0.00001138
Iteration 17/1000 | Loss: 0.00001137
Iteration 18/1000 | Loss: 0.00001133
Iteration 19/1000 | Loss: 0.00001128
Iteration 20/1000 | Loss: 0.00001128
Iteration 21/1000 | Loss: 0.00001127
Iteration 22/1000 | Loss: 0.00001127
Iteration 23/1000 | Loss: 0.00001126
Iteration 24/1000 | Loss: 0.00001125
Iteration 25/1000 | Loss: 0.00001125
Iteration 26/1000 | Loss: 0.00001124
Iteration 27/1000 | Loss: 0.00001122
Iteration 28/1000 | Loss: 0.00001122
Iteration 29/1000 | Loss: 0.00001119
Iteration 30/1000 | Loss: 0.00001119
Iteration 31/1000 | Loss: 0.00001119
Iteration 32/1000 | Loss: 0.00001119
Iteration 33/1000 | Loss: 0.00001116
Iteration 34/1000 | Loss: 0.00001116
Iteration 35/1000 | Loss: 0.00001115
Iteration 36/1000 | Loss: 0.00001115
Iteration 37/1000 | Loss: 0.00001115
Iteration 38/1000 | Loss: 0.00001114
Iteration 39/1000 | Loss: 0.00001114
Iteration 40/1000 | Loss: 0.00001113
Iteration 41/1000 | Loss: 0.00001113
Iteration 42/1000 | Loss: 0.00001113
Iteration 43/1000 | Loss: 0.00001113
Iteration 44/1000 | Loss: 0.00001112
Iteration 45/1000 | Loss: 0.00001112
Iteration 46/1000 | Loss: 0.00001112
Iteration 47/1000 | Loss: 0.00001112
Iteration 48/1000 | Loss: 0.00001112
Iteration 49/1000 | Loss: 0.00001111
Iteration 50/1000 | Loss: 0.00001111
Iteration 51/1000 | Loss: 0.00001111
Iteration 52/1000 | Loss: 0.00001111
Iteration 53/1000 | Loss: 0.00001110
Iteration 54/1000 | Loss: 0.00001110
Iteration 55/1000 | Loss: 0.00001110
Iteration 56/1000 | Loss: 0.00001110
Iteration 57/1000 | Loss: 0.00001110
Iteration 58/1000 | Loss: 0.00001110
Iteration 59/1000 | Loss: 0.00001110
Iteration 60/1000 | Loss: 0.00001110
Iteration 61/1000 | Loss: 0.00001110
Iteration 62/1000 | Loss: 0.00001110
Iteration 63/1000 | Loss: 0.00001109
Iteration 64/1000 | Loss: 0.00001109
Iteration 65/1000 | Loss: 0.00001109
Iteration 66/1000 | Loss: 0.00001109
Iteration 67/1000 | Loss: 0.00001108
Iteration 68/1000 | Loss: 0.00001108
Iteration 69/1000 | Loss: 0.00001108
Iteration 70/1000 | Loss: 0.00001107
Iteration 71/1000 | Loss: 0.00001107
Iteration 72/1000 | Loss: 0.00001107
Iteration 73/1000 | Loss: 0.00001107
Iteration 74/1000 | Loss: 0.00001106
Iteration 75/1000 | Loss: 0.00001106
Iteration 76/1000 | Loss: 0.00001106
Iteration 77/1000 | Loss: 0.00001106
Iteration 78/1000 | Loss: 0.00001106
Iteration 79/1000 | Loss: 0.00001106
Iteration 80/1000 | Loss: 0.00001105
Iteration 81/1000 | Loss: 0.00001105
Iteration 82/1000 | Loss: 0.00001105
Iteration 83/1000 | Loss: 0.00001105
Iteration 84/1000 | Loss: 0.00001104
Iteration 85/1000 | Loss: 0.00001104
Iteration 86/1000 | Loss: 0.00001104
Iteration 87/1000 | Loss: 0.00001104
Iteration 88/1000 | Loss: 0.00001104
Iteration 89/1000 | Loss: 0.00001104
Iteration 90/1000 | Loss: 0.00001104
Iteration 91/1000 | Loss: 0.00001104
Iteration 92/1000 | Loss: 0.00001104
Iteration 93/1000 | Loss: 0.00001104
Iteration 94/1000 | Loss: 0.00001103
Iteration 95/1000 | Loss: 0.00001103
Iteration 96/1000 | Loss: 0.00001103
Iteration 97/1000 | Loss: 0.00001103
Iteration 98/1000 | Loss: 0.00001103
Iteration 99/1000 | Loss: 0.00001103
Iteration 100/1000 | Loss: 0.00001103
Iteration 101/1000 | Loss: 0.00001102
Iteration 102/1000 | Loss: 0.00001102
Iteration 103/1000 | Loss: 0.00001102
Iteration 104/1000 | Loss: 0.00001102
Iteration 105/1000 | Loss: 0.00001102
Iteration 106/1000 | Loss: 0.00001102
Iteration 107/1000 | Loss: 0.00001102
Iteration 108/1000 | Loss: 0.00001102
Iteration 109/1000 | Loss: 0.00001101
Iteration 110/1000 | Loss: 0.00001101
Iteration 111/1000 | Loss: 0.00001101
Iteration 112/1000 | Loss: 0.00001101
Iteration 113/1000 | Loss: 0.00001101
Iteration 114/1000 | Loss: 0.00001101
Iteration 115/1000 | Loss: 0.00001101
Iteration 116/1000 | Loss: 0.00001101
Iteration 117/1000 | Loss: 0.00001101
Iteration 118/1000 | Loss: 0.00001101
Iteration 119/1000 | Loss: 0.00001101
Iteration 120/1000 | Loss: 0.00001101
Iteration 121/1000 | Loss: 0.00001101
Iteration 122/1000 | Loss: 0.00001101
Iteration 123/1000 | Loss: 0.00001101
Iteration 124/1000 | Loss: 0.00001100
Iteration 125/1000 | Loss: 0.00001100
Iteration 126/1000 | Loss: 0.00001100
Iteration 127/1000 | Loss: 0.00001100
Iteration 128/1000 | Loss: 0.00001100
Iteration 129/1000 | Loss: 0.00001100
Iteration 130/1000 | Loss: 0.00001100
Iteration 131/1000 | Loss: 0.00001100
Iteration 132/1000 | Loss: 0.00001100
Iteration 133/1000 | Loss: 0.00001100
Iteration 134/1000 | Loss: 0.00001100
Iteration 135/1000 | Loss: 0.00001100
Iteration 136/1000 | Loss: 0.00001100
Iteration 137/1000 | Loss: 0.00001100
Iteration 138/1000 | Loss: 0.00001100
Iteration 139/1000 | Loss: 0.00001100
Iteration 140/1000 | Loss: 0.00001099
Iteration 141/1000 | Loss: 0.00001099
Iteration 142/1000 | Loss: 0.00001099
Iteration 143/1000 | Loss: 0.00001099
Iteration 144/1000 | Loss: 0.00001099
Iteration 145/1000 | Loss: 0.00001099
Iteration 146/1000 | Loss: 0.00001099
Iteration 147/1000 | Loss: 0.00001099
Iteration 148/1000 | Loss: 0.00001098
Iteration 149/1000 | Loss: 0.00001098
Iteration 150/1000 | Loss: 0.00001098
Iteration 151/1000 | Loss: 0.00001098
Iteration 152/1000 | Loss: 0.00001098
Iteration 153/1000 | Loss: 0.00001098
Iteration 154/1000 | Loss: 0.00001097
Iteration 155/1000 | Loss: 0.00001097
Iteration 156/1000 | Loss: 0.00001097
Iteration 157/1000 | Loss: 0.00001097
Iteration 158/1000 | Loss: 0.00001097
Iteration 159/1000 | Loss: 0.00001096
Iteration 160/1000 | Loss: 0.00001096
Iteration 161/1000 | Loss: 0.00001096
Iteration 162/1000 | Loss: 0.00001096
Iteration 163/1000 | Loss: 0.00001096
Iteration 164/1000 | Loss: 0.00001096
Iteration 165/1000 | Loss: 0.00001095
Iteration 166/1000 | Loss: 0.00001095
Iteration 167/1000 | Loss: 0.00001095
Iteration 168/1000 | Loss: 0.00001095
Iteration 169/1000 | Loss: 0.00001095
Iteration 170/1000 | Loss: 0.00001095
Iteration 171/1000 | Loss: 0.00001095
Iteration 172/1000 | Loss: 0.00001094
Iteration 173/1000 | Loss: 0.00001094
Iteration 174/1000 | Loss: 0.00001094
Iteration 175/1000 | Loss: 0.00001094
Iteration 176/1000 | Loss: 0.00001094
Iteration 177/1000 | Loss: 0.00001094
Iteration 178/1000 | Loss: 0.00001094
Iteration 179/1000 | Loss: 0.00001094
Iteration 180/1000 | Loss: 0.00001094
Iteration 181/1000 | Loss: 0.00001094
Iteration 182/1000 | Loss: 0.00001093
Iteration 183/1000 | Loss: 0.00001093
Iteration 184/1000 | Loss: 0.00001093
Iteration 185/1000 | Loss: 0.00001093
Iteration 186/1000 | Loss: 0.00001093
Iteration 187/1000 | Loss: 0.00001093
Iteration 188/1000 | Loss: 0.00001093
Iteration 189/1000 | Loss: 0.00001093
Iteration 190/1000 | Loss: 0.00001093
Iteration 191/1000 | Loss: 0.00001093
Iteration 192/1000 | Loss: 0.00001093
Iteration 193/1000 | Loss: 0.00001093
Iteration 194/1000 | Loss: 0.00001093
Iteration 195/1000 | Loss: 0.00001093
Iteration 196/1000 | Loss: 0.00001093
Iteration 197/1000 | Loss: 0.00001092
Iteration 198/1000 | Loss: 0.00001092
Iteration 199/1000 | Loss: 0.00001092
Iteration 200/1000 | Loss: 0.00001092
Iteration 201/1000 | Loss: 0.00001092
Iteration 202/1000 | Loss: 0.00001092
Iteration 203/1000 | Loss: 0.00001092
Iteration 204/1000 | Loss: 0.00001092
Iteration 205/1000 | Loss: 0.00001092
Iteration 206/1000 | Loss: 0.00001092
Iteration 207/1000 | Loss: 0.00001092
Iteration 208/1000 | Loss: 0.00001092
Iteration 209/1000 | Loss: 0.00001092
Iteration 210/1000 | Loss: 0.00001092
Iteration 211/1000 | Loss: 0.00001092
Iteration 212/1000 | Loss: 0.00001092
Iteration 213/1000 | Loss: 0.00001092
Iteration 214/1000 | Loss: 0.00001092
Iteration 215/1000 | Loss: 0.00001092
Iteration 216/1000 | Loss: 0.00001092
Iteration 217/1000 | Loss: 0.00001092
Iteration 218/1000 | Loss: 0.00001092
Iteration 219/1000 | Loss: 0.00001092
Iteration 220/1000 | Loss: 0.00001092
Iteration 221/1000 | Loss: 0.00001092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.0917490726569667e-05, 1.0917490726569667e-05, 1.0917490726569667e-05, 1.0917490726569667e-05, 1.0917490726569667e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0917490726569667e-05

Optimization complete. Final v2v error: 2.8517704010009766 mm

Highest mean error: 3.5446619987487793 mm for frame 100

Lowest mean error: 2.4715750217437744 mm for frame 71

Saving results

Total time: 42.5798761844635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066818
Iteration 2/25 | Loss: 0.01066817
Iteration 3/25 | Loss: 0.01066817
Iteration 4/25 | Loss: 0.01066817
Iteration 5/25 | Loss: 0.01066817
Iteration 6/25 | Loss: 0.01066817
Iteration 7/25 | Loss: 0.01066817
Iteration 8/25 | Loss: 0.01066817
Iteration 9/25 | Loss: 0.01066816
Iteration 10/25 | Loss: 0.01066816
Iteration 11/25 | Loss: 0.01066816
Iteration 12/25 | Loss: 0.01066816
Iteration 13/25 | Loss: 0.01066816
Iteration 14/25 | Loss: 0.01066816
Iteration 15/25 | Loss: 0.01066816
Iteration 16/25 | Loss: 0.01066815
Iteration 17/25 | Loss: 0.01066815
Iteration 18/25 | Loss: 0.01066815
Iteration 19/25 | Loss: 0.01066815
Iteration 20/25 | Loss: 0.01066815
Iteration 21/25 | Loss: 0.01066815
Iteration 22/25 | Loss: 0.01066815
Iteration 23/25 | Loss: 0.01066815
Iteration 24/25 | Loss: 0.01066814
Iteration 25/25 | Loss: 0.01066814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59229910
Iteration 2/25 | Loss: 0.07106600
Iteration 3/25 | Loss: 0.06895787
Iteration 4/25 | Loss: 0.06895782
Iteration 5/25 | Loss: 0.06895781
Iteration 6/25 | Loss: 0.06895780
Iteration 7/25 | Loss: 0.06895780
Iteration 8/25 | Loss: 0.06895780
Iteration 9/25 | Loss: 0.06895780
Iteration 10/25 | Loss: 0.06895780
Iteration 11/25 | Loss: 0.06895780
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.06895779818296432, 0.06895779818296432, 0.06895779818296432, 0.06895779818296432, 0.06895779818296432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06895779818296432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06895780
Iteration 2/1000 | Loss: 0.01878915
Iteration 3/1000 | Loss: 0.01203388
Iteration 4/1000 | Loss: 0.00436848
Iteration 5/1000 | Loss: 0.00223611
Iteration 6/1000 | Loss: 0.00114189
Iteration 7/1000 | Loss: 0.00133331
Iteration 8/1000 | Loss: 0.00065071
Iteration 9/1000 | Loss: 0.00068844
Iteration 10/1000 | Loss: 0.00087602
Iteration 11/1000 | Loss: 0.00059859
Iteration 12/1000 | Loss: 0.00029811
Iteration 13/1000 | Loss: 0.00115307
Iteration 14/1000 | Loss: 0.00039844
Iteration 15/1000 | Loss: 0.00106590
Iteration 16/1000 | Loss: 0.00086729
Iteration 17/1000 | Loss: 0.00041562
Iteration 18/1000 | Loss: 0.00016728
Iteration 19/1000 | Loss: 0.00034004
Iteration 20/1000 | Loss: 0.00025280
Iteration 21/1000 | Loss: 0.00034574
Iteration 22/1000 | Loss: 0.00094336
Iteration 23/1000 | Loss: 0.00064429
Iteration 24/1000 | Loss: 0.00018145
Iteration 25/1000 | Loss: 0.00022999
Iteration 26/1000 | Loss: 0.00015315
Iteration 27/1000 | Loss: 0.00015300
Iteration 28/1000 | Loss: 0.00037855
Iteration 29/1000 | Loss: 0.00045109
Iteration 30/1000 | Loss: 0.00039577
Iteration 31/1000 | Loss: 0.00020233
Iteration 32/1000 | Loss: 0.00011059
Iteration 33/1000 | Loss: 0.00046674
Iteration 34/1000 | Loss: 0.00048112
Iteration 35/1000 | Loss: 0.00036591
Iteration 36/1000 | Loss: 0.00024488
Iteration 37/1000 | Loss: 0.00015330
Iteration 38/1000 | Loss: 0.00018993
Iteration 39/1000 | Loss: 0.00030008
Iteration 40/1000 | Loss: 0.00017830
Iteration 41/1000 | Loss: 0.00009255
Iteration 42/1000 | Loss: 0.00021094
Iteration 43/1000 | Loss: 0.00021559
Iteration 44/1000 | Loss: 0.00042711
Iteration 45/1000 | Loss: 0.00051879
Iteration 46/1000 | Loss: 0.00092409
Iteration 47/1000 | Loss: 0.00052195
Iteration 48/1000 | Loss: 0.00039463
Iteration 49/1000 | Loss: 0.00052647
Iteration 50/1000 | Loss: 0.00036887
Iteration 51/1000 | Loss: 0.00031446
Iteration 52/1000 | Loss: 0.00041301
Iteration 53/1000 | Loss: 0.00043067
Iteration 54/1000 | Loss: 0.00036609
Iteration 55/1000 | Loss: 0.00031867
Iteration 56/1000 | Loss: 0.00042343
Iteration 57/1000 | Loss: 0.00052024
Iteration 58/1000 | Loss: 0.00045011
Iteration 59/1000 | Loss: 0.00013397
Iteration 60/1000 | Loss: 0.00007884
Iteration 61/1000 | Loss: 0.00007076
Iteration 62/1000 | Loss: 0.00006424
Iteration 63/1000 | Loss: 0.00018912
Iteration 64/1000 | Loss: 0.00029038
Iteration 65/1000 | Loss: 0.00006622
Iteration 66/1000 | Loss: 0.00023078
Iteration 67/1000 | Loss: 0.00009059
Iteration 68/1000 | Loss: 0.00049868
Iteration 69/1000 | Loss: 0.00066923
Iteration 70/1000 | Loss: 0.00010863
Iteration 71/1000 | Loss: 0.00018385
Iteration 72/1000 | Loss: 0.00008183
Iteration 73/1000 | Loss: 0.00022655
Iteration 74/1000 | Loss: 0.00008975
Iteration 75/1000 | Loss: 0.00014272
Iteration 76/1000 | Loss: 0.00007256
Iteration 77/1000 | Loss: 0.00015099
Iteration 78/1000 | Loss: 0.00059740
Iteration 79/1000 | Loss: 0.00017670
Iteration 80/1000 | Loss: 0.00007214
Iteration 81/1000 | Loss: 0.00005788
Iteration 82/1000 | Loss: 0.00032309
Iteration 83/1000 | Loss: 0.00007098
Iteration 84/1000 | Loss: 0.00012132
Iteration 85/1000 | Loss: 0.00007656
Iteration 86/1000 | Loss: 0.00034335
Iteration 87/1000 | Loss: 0.00030849
Iteration 88/1000 | Loss: 0.00013815
Iteration 89/1000 | Loss: 0.00033560
Iteration 90/1000 | Loss: 0.00018034
Iteration 91/1000 | Loss: 0.00033752
Iteration 92/1000 | Loss: 0.00038700
Iteration 93/1000 | Loss: 0.00042556
Iteration 94/1000 | Loss: 0.00015988
Iteration 95/1000 | Loss: 0.00005239
Iteration 96/1000 | Loss: 0.00005196
Iteration 97/1000 | Loss: 0.00005593
Iteration 98/1000 | Loss: 0.00006740
Iteration 99/1000 | Loss: 0.00018067
Iteration 100/1000 | Loss: 0.00021907
Iteration 101/1000 | Loss: 0.00006841
Iteration 102/1000 | Loss: 0.00011672
Iteration 103/1000 | Loss: 0.00013080
Iteration 104/1000 | Loss: 0.00007007
Iteration 105/1000 | Loss: 0.00005322
Iteration 106/1000 | Loss: 0.00006377
Iteration 107/1000 | Loss: 0.00010954
Iteration 108/1000 | Loss: 0.00004831
Iteration 109/1000 | Loss: 0.00032545
Iteration 110/1000 | Loss: 0.00103319
Iteration 111/1000 | Loss: 0.00034307
Iteration 112/1000 | Loss: 0.00009152
Iteration 113/1000 | Loss: 0.00010311
Iteration 114/1000 | Loss: 0.00005678
Iteration 115/1000 | Loss: 0.00010035
Iteration 116/1000 | Loss: 0.00005106
Iteration 117/1000 | Loss: 0.00004717
Iteration 118/1000 | Loss: 0.00009360
Iteration 119/1000 | Loss: 0.00020839
Iteration 120/1000 | Loss: 0.00032653
Iteration 121/1000 | Loss: 0.00016338
Iteration 122/1000 | Loss: 0.00009550
Iteration 123/1000 | Loss: 0.00006218
Iteration 124/1000 | Loss: 0.00005470
Iteration 125/1000 | Loss: 0.00020768
Iteration 126/1000 | Loss: 0.00016218
Iteration 127/1000 | Loss: 0.00005199
Iteration 128/1000 | Loss: 0.00005735
Iteration 129/1000 | Loss: 0.00012635
Iteration 130/1000 | Loss: 0.00004557
Iteration 131/1000 | Loss: 0.00021764
Iteration 132/1000 | Loss: 0.00048169
Iteration 133/1000 | Loss: 0.00016675
Iteration 134/1000 | Loss: 0.00018209
Iteration 135/1000 | Loss: 0.00019862
Iteration 136/1000 | Loss: 0.00006220
Iteration 137/1000 | Loss: 0.00007132
Iteration 138/1000 | Loss: 0.00006283
Iteration 139/1000 | Loss: 0.00007046
Iteration 140/1000 | Loss: 0.00004772
Iteration 141/1000 | Loss: 0.00004504
Iteration 142/1000 | Loss: 0.00013214
Iteration 143/1000 | Loss: 0.00032253
Iteration 144/1000 | Loss: 0.00021041
Iteration 145/1000 | Loss: 0.00006167
Iteration 146/1000 | Loss: 0.00009147
Iteration 147/1000 | Loss: 0.00014217
Iteration 148/1000 | Loss: 0.00010829
Iteration 149/1000 | Loss: 0.00006438
Iteration 150/1000 | Loss: 0.00010824
Iteration 151/1000 | Loss: 0.00023837
Iteration 152/1000 | Loss: 0.00011226
Iteration 153/1000 | Loss: 0.00005657
Iteration 154/1000 | Loss: 0.00013670
Iteration 155/1000 | Loss: 0.00011656
Iteration 156/1000 | Loss: 0.00004686
Iteration 157/1000 | Loss: 0.00004868
Iteration 158/1000 | Loss: 0.00004468
Iteration 159/1000 | Loss: 0.00004866
Iteration 160/1000 | Loss: 0.00019383
Iteration 161/1000 | Loss: 0.00020109
Iteration 162/1000 | Loss: 0.00005484
Iteration 163/1000 | Loss: 0.00004854
Iteration 164/1000 | Loss: 0.00004399
Iteration 165/1000 | Loss: 0.00004262
Iteration 166/1000 | Loss: 0.00007650
Iteration 167/1000 | Loss: 0.00030341
Iteration 168/1000 | Loss: 0.00019796
Iteration 169/1000 | Loss: 0.00017874
Iteration 170/1000 | Loss: 0.00019358
Iteration 171/1000 | Loss: 0.00030491
Iteration 172/1000 | Loss: 0.00024129
Iteration 173/1000 | Loss: 0.00017812
Iteration 174/1000 | Loss: 0.00011972
Iteration 175/1000 | Loss: 0.00014233
Iteration 176/1000 | Loss: 0.00010802
Iteration 177/1000 | Loss: 0.00005187
Iteration 178/1000 | Loss: 0.00004658
Iteration 179/1000 | Loss: 0.00015143
Iteration 180/1000 | Loss: 0.00042172
Iteration 181/1000 | Loss: 0.00019690
Iteration 182/1000 | Loss: 0.00007665
Iteration 183/1000 | Loss: 0.00007934
Iteration 184/1000 | Loss: 0.00013687
Iteration 185/1000 | Loss: 0.00016492
Iteration 186/1000 | Loss: 0.00017314
Iteration 187/1000 | Loss: 0.00005591
Iteration 188/1000 | Loss: 0.00008005
Iteration 189/1000 | Loss: 0.00005002
Iteration 190/1000 | Loss: 0.00005111
Iteration 191/1000 | Loss: 0.00004680
Iteration 192/1000 | Loss: 0.00018570
Iteration 193/1000 | Loss: 0.00006382
Iteration 194/1000 | Loss: 0.00006235
Iteration 195/1000 | Loss: 0.00006168
Iteration 196/1000 | Loss: 0.00005619
Iteration 197/1000 | Loss: 0.00006222
Iteration 198/1000 | Loss: 0.00004576
Iteration 199/1000 | Loss: 0.00004360
Iteration 200/1000 | Loss: 0.00006965
Iteration 201/1000 | Loss: 0.00014934
Iteration 202/1000 | Loss: 0.00105882
Iteration 203/1000 | Loss: 0.00016553
Iteration 204/1000 | Loss: 0.00005102
Iteration 205/1000 | Loss: 0.00006320
Iteration 206/1000 | Loss: 0.00004643
Iteration 207/1000 | Loss: 0.00004222
Iteration 208/1000 | Loss: 0.00005531
Iteration 209/1000 | Loss: 0.00012518
Iteration 210/1000 | Loss: 0.00006070
Iteration 211/1000 | Loss: 0.00004362
Iteration 212/1000 | Loss: 0.00005511
Iteration 213/1000 | Loss: 0.00004279
Iteration 214/1000 | Loss: 0.00004656
Iteration 215/1000 | Loss: 0.00004086
Iteration 216/1000 | Loss: 0.00004076
Iteration 217/1000 | Loss: 0.00018858
Iteration 218/1000 | Loss: 0.00014177
Iteration 219/1000 | Loss: 0.00023353
Iteration 220/1000 | Loss: 0.00005891
Iteration 221/1000 | Loss: 0.00005921
Iteration 222/1000 | Loss: 0.00006934
Iteration 223/1000 | Loss: 0.00004786
Iteration 224/1000 | Loss: 0.00004374
Iteration 225/1000 | Loss: 0.00004291
Iteration 226/1000 | Loss: 0.00005883
Iteration 227/1000 | Loss: 0.00008756
Iteration 228/1000 | Loss: 0.00008621
Iteration 229/1000 | Loss: 0.00004576
Iteration 230/1000 | Loss: 0.00007692
Iteration 231/1000 | Loss: 0.00005649
Iteration 232/1000 | Loss: 0.00004164
Iteration 233/1000 | Loss: 0.00004972
Iteration 234/1000 | Loss: 0.00004972
Iteration 235/1000 | Loss: 0.00006450
Iteration 236/1000 | Loss: 0.00005975
Iteration 237/1000 | Loss: 0.00007068
Iteration 238/1000 | Loss: 0.00006255
Iteration 239/1000 | Loss: 0.00004270
Iteration 240/1000 | Loss: 0.00004141
Iteration 241/1000 | Loss: 0.00005739
Iteration 242/1000 | Loss: 0.00005739
Iteration 243/1000 | Loss: 0.00008530
Iteration 244/1000 | Loss: 0.00007444
Iteration 245/1000 | Loss: 0.00006169
Iteration 246/1000 | Loss: 0.00005599
Iteration 247/1000 | Loss: 0.00005329
Iteration 248/1000 | Loss: 0.00008612
Iteration 249/1000 | Loss: 0.00061850
Iteration 250/1000 | Loss: 0.00007465
Iteration 251/1000 | Loss: 0.00006069
Iteration 252/1000 | Loss: 0.00011298
Iteration 253/1000 | Loss: 0.00004263
Iteration 254/1000 | Loss: 0.00004622
Iteration 255/1000 | Loss: 0.00004118
Iteration 256/1000 | Loss: 0.00008071
Iteration 257/1000 | Loss: 0.00005885
Iteration 258/1000 | Loss: 0.00007947
Iteration 259/1000 | Loss: 0.00006138
Iteration 260/1000 | Loss: 0.00020706
Iteration 261/1000 | Loss: 0.00004657
Iteration 262/1000 | Loss: 0.00008181
Iteration 263/1000 | Loss: 0.00025338
Iteration 264/1000 | Loss: 0.00011831
Iteration 265/1000 | Loss: 0.00004684
Iteration 266/1000 | Loss: 0.00008439
Iteration 267/1000 | Loss: 0.00005154
Iteration 268/1000 | Loss: 0.00004775
Iteration 269/1000 | Loss: 0.00004081
Iteration 270/1000 | Loss: 0.00006286
Iteration 271/1000 | Loss: 0.00004067
Iteration 272/1000 | Loss: 0.00004066
Iteration 273/1000 | Loss: 0.00004066
Iteration 274/1000 | Loss: 0.00004076
Iteration 275/1000 | Loss: 0.00004444
Iteration 276/1000 | Loss: 0.00004079
Iteration 277/1000 | Loss: 0.00004350
Iteration 278/1000 | Loss: 0.00004827
Iteration 279/1000 | Loss: 0.00004047
Iteration 280/1000 | Loss: 0.00004040
Iteration 281/1000 | Loss: 0.00004040
Iteration 282/1000 | Loss: 0.00004040
Iteration 283/1000 | Loss: 0.00004040
Iteration 284/1000 | Loss: 0.00004040
Iteration 285/1000 | Loss: 0.00004040
Iteration 286/1000 | Loss: 0.00004040
Iteration 287/1000 | Loss: 0.00004040
Iteration 288/1000 | Loss: 0.00004039
Iteration 289/1000 | Loss: 0.00004039
Iteration 290/1000 | Loss: 0.00004039
Iteration 291/1000 | Loss: 0.00004039
Iteration 292/1000 | Loss: 0.00004037
Iteration 293/1000 | Loss: 0.00004037
Iteration 294/1000 | Loss: 0.00004037
Iteration 295/1000 | Loss: 0.00004036
Iteration 296/1000 | Loss: 0.00004036
Iteration 297/1000 | Loss: 0.00004036
Iteration 298/1000 | Loss: 0.00004036
Iteration 299/1000 | Loss: 0.00004036
Iteration 300/1000 | Loss: 0.00004035
Iteration 301/1000 | Loss: 0.00004035
Iteration 302/1000 | Loss: 0.00004035
Iteration 303/1000 | Loss: 0.00004035
Iteration 304/1000 | Loss: 0.00004035
Iteration 305/1000 | Loss: 0.00004035
Iteration 306/1000 | Loss: 0.00004035
Iteration 307/1000 | Loss: 0.00004035
Iteration 308/1000 | Loss: 0.00004035
Iteration 309/1000 | Loss: 0.00004034
Iteration 310/1000 | Loss: 0.00004034
Iteration 311/1000 | Loss: 0.00004034
Iteration 312/1000 | Loss: 0.00004034
Iteration 313/1000 | Loss: 0.00004034
Iteration 314/1000 | Loss: 0.00004034
Iteration 315/1000 | Loss: 0.00004034
Iteration 316/1000 | Loss: 0.00004034
Iteration 317/1000 | Loss: 0.00004034
Iteration 318/1000 | Loss: 0.00004034
Iteration 319/1000 | Loss: 0.00004034
Iteration 320/1000 | Loss: 0.00004034
Iteration 321/1000 | Loss: 0.00004033
Iteration 322/1000 | Loss: 0.00004033
Iteration 323/1000 | Loss: 0.00004033
Iteration 324/1000 | Loss: 0.00004033
Iteration 325/1000 | Loss: 0.00004033
Iteration 326/1000 | Loss: 0.00004033
Iteration 327/1000 | Loss: 0.00004033
Iteration 328/1000 | Loss: 0.00004033
Iteration 329/1000 | Loss: 0.00004032
Iteration 330/1000 | Loss: 0.00004032
Iteration 331/1000 | Loss: 0.00004032
Iteration 332/1000 | Loss: 0.00004032
Iteration 333/1000 | Loss: 0.00004032
Iteration 334/1000 | Loss: 0.00004032
Iteration 335/1000 | Loss: 0.00004031
Iteration 336/1000 | Loss: 0.00004031
Iteration 337/1000 | Loss: 0.00004031
Iteration 338/1000 | Loss: 0.00004031
Iteration 339/1000 | Loss: 0.00004031
Iteration 340/1000 | Loss: 0.00004031
Iteration 341/1000 | Loss: 0.00004031
Iteration 342/1000 | Loss: 0.00004031
Iteration 343/1000 | Loss: 0.00004031
Iteration 344/1000 | Loss: 0.00004031
Iteration 345/1000 | Loss: 0.00004031
Iteration 346/1000 | Loss: 0.00004031
Iteration 347/1000 | Loss: 0.00004031
Iteration 348/1000 | Loss: 0.00004031
Iteration 349/1000 | Loss: 0.00004031
Iteration 350/1000 | Loss: 0.00004031
Iteration 351/1000 | Loss: 0.00004031
Iteration 352/1000 | Loss: 0.00004031
Iteration 353/1000 | Loss: 0.00004031
Iteration 354/1000 | Loss: 0.00004031
Iteration 355/1000 | Loss: 0.00004031
Iteration 356/1000 | Loss: 0.00004031
Iteration 357/1000 | Loss: 0.00004031
Iteration 358/1000 | Loss: 0.00004031
Iteration 359/1000 | Loss: 0.00004031
Iteration 360/1000 | Loss: 0.00004031
Iteration 361/1000 | Loss: 0.00004031
Iteration 362/1000 | Loss: 0.00004031
Iteration 363/1000 | Loss: 0.00004031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 363. Stopping optimization.
Last 5 losses: [4.031354183098301e-05, 4.031354183098301e-05, 4.031354183098301e-05, 4.031354183098301e-05, 4.031354183098301e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.031354183098301e-05

Optimization complete. Final v2v error: 3.7230162620544434 mm

Highest mean error: 18.45174789428711 mm for frame 78

Lowest mean error: 2.2337088584899902 mm for frame 221

Saving results

Total time: 444.59592962265015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844906
Iteration 2/25 | Loss: 0.00101681
Iteration 3/25 | Loss: 0.00091452
Iteration 4/25 | Loss: 0.00088886
Iteration 5/25 | Loss: 0.00088278
Iteration 6/25 | Loss: 0.00088065
Iteration 7/25 | Loss: 0.00088057
Iteration 8/25 | Loss: 0.00088057
Iteration 9/25 | Loss: 0.00088057
Iteration 10/25 | Loss: 0.00088057
Iteration 11/25 | Loss: 0.00088057
Iteration 12/25 | Loss: 0.00088057
Iteration 13/25 | Loss: 0.00088057
Iteration 14/25 | Loss: 0.00088057
Iteration 15/25 | Loss: 0.00088057
Iteration 16/25 | Loss: 0.00088057
Iteration 17/25 | Loss: 0.00088057
Iteration 18/25 | Loss: 0.00088057
Iteration 19/25 | Loss: 0.00088057
Iteration 20/25 | Loss: 0.00088057
Iteration 21/25 | Loss: 0.00088057
Iteration 22/25 | Loss: 0.00088057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008805678808130324, 0.0008805678808130324, 0.0008805678808130324, 0.0008805678808130324, 0.0008805678808130324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008805678808130324

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27395260
Iteration 2/25 | Loss: 0.00142896
Iteration 3/25 | Loss: 0.00142896
Iteration 4/25 | Loss: 0.00142896
Iteration 5/25 | Loss: 0.00142896
Iteration 6/25 | Loss: 0.00142896
Iteration 7/25 | Loss: 0.00142896
Iteration 8/25 | Loss: 0.00142896
Iteration 9/25 | Loss: 0.00142896
Iteration 10/25 | Loss: 0.00142896
Iteration 11/25 | Loss: 0.00142896
Iteration 12/25 | Loss: 0.00142896
Iteration 13/25 | Loss: 0.00142896
Iteration 14/25 | Loss: 0.00142896
Iteration 15/25 | Loss: 0.00142896
Iteration 16/25 | Loss: 0.00142896
Iteration 17/25 | Loss: 0.00142896
Iteration 18/25 | Loss: 0.00142896
Iteration 19/25 | Loss: 0.00142896
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0014289584942162037, 0.0014289584942162037, 0.0014289584942162037, 0.0014289584942162037, 0.0014289584942162037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014289584942162037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142896
Iteration 2/1000 | Loss: 0.00002417
Iteration 3/1000 | Loss: 0.00001361
Iteration 4/1000 | Loss: 0.00001227
Iteration 5/1000 | Loss: 0.00001129
Iteration 6/1000 | Loss: 0.00001064
Iteration 7/1000 | Loss: 0.00001009
Iteration 8/1000 | Loss: 0.00000979
Iteration 9/1000 | Loss: 0.00000952
Iteration 10/1000 | Loss: 0.00000941
Iteration 11/1000 | Loss: 0.00000938
Iteration 12/1000 | Loss: 0.00000937
Iteration 13/1000 | Loss: 0.00000936
Iteration 14/1000 | Loss: 0.00000929
Iteration 15/1000 | Loss: 0.00000923
Iteration 16/1000 | Loss: 0.00000923
Iteration 17/1000 | Loss: 0.00000922
Iteration 18/1000 | Loss: 0.00000921
Iteration 19/1000 | Loss: 0.00000921
Iteration 20/1000 | Loss: 0.00000917
Iteration 21/1000 | Loss: 0.00000917
Iteration 22/1000 | Loss: 0.00000915
Iteration 23/1000 | Loss: 0.00000915
Iteration 24/1000 | Loss: 0.00000915
Iteration 25/1000 | Loss: 0.00000914
Iteration 26/1000 | Loss: 0.00000909
Iteration 27/1000 | Loss: 0.00000908
Iteration 28/1000 | Loss: 0.00000904
Iteration 29/1000 | Loss: 0.00000903
Iteration 30/1000 | Loss: 0.00000903
Iteration 31/1000 | Loss: 0.00000902
Iteration 32/1000 | Loss: 0.00000902
Iteration 33/1000 | Loss: 0.00000902
Iteration 34/1000 | Loss: 0.00000901
Iteration 35/1000 | Loss: 0.00000901
Iteration 36/1000 | Loss: 0.00000901
Iteration 37/1000 | Loss: 0.00000901
Iteration 38/1000 | Loss: 0.00000901
Iteration 39/1000 | Loss: 0.00000900
Iteration 40/1000 | Loss: 0.00000900
Iteration 41/1000 | Loss: 0.00000900
Iteration 42/1000 | Loss: 0.00000900
Iteration 43/1000 | Loss: 0.00000900
Iteration 44/1000 | Loss: 0.00000899
Iteration 45/1000 | Loss: 0.00000899
Iteration 46/1000 | Loss: 0.00000899
Iteration 47/1000 | Loss: 0.00000899
Iteration 48/1000 | Loss: 0.00000899
Iteration 49/1000 | Loss: 0.00000899
Iteration 50/1000 | Loss: 0.00000899
Iteration 51/1000 | Loss: 0.00000899
Iteration 52/1000 | Loss: 0.00000899
Iteration 53/1000 | Loss: 0.00000899
Iteration 54/1000 | Loss: 0.00000898
Iteration 55/1000 | Loss: 0.00000898
Iteration 56/1000 | Loss: 0.00000897
Iteration 57/1000 | Loss: 0.00000897
Iteration 58/1000 | Loss: 0.00000897
Iteration 59/1000 | Loss: 0.00000897
Iteration 60/1000 | Loss: 0.00000897
Iteration 61/1000 | Loss: 0.00000897
Iteration 62/1000 | Loss: 0.00000897
Iteration 63/1000 | Loss: 0.00000896
Iteration 64/1000 | Loss: 0.00000896
Iteration 65/1000 | Loss: 0.00000896
Iteration 66/1000 | Loss: 0.00000896
Iteration 67/1000 | Loss: 0.00000895
Iteration 68/1000 | Loss: 0.00000895
Iteration 69/1000 | Loss: 0.00000895
Iteration 70/1000 | Loss: 0.00000895
Iteration 71/1000 | Loss: 0.00000895
Iteration 72/1000 | Loss: 0.00000895
Iteration 73/1000 | Loss: 0.00000895
Iteration 74/1000 | Loss: 0.00000895
Iteration 75/1000 | Loss: 0.00000895
Iteration 76/1000 | Loss: 0.00000894
Iteration 77/1000 | Loss: 0.00000894
Iteration 78/1000 | Loss: 0.00000894
Iteration 79/1000 | Loss: 0.00000894
Iteration 80/1000 | Loss: 0.00000894
Iteration 81/1000 | Loss: 0.00000894
Iteration 82/1000 | Loss: 0.00000894
Iteration 83/1000 | Loss: 0.00000894
Iteration 84/1000 | Loss: 0.00000894
Iteration 85/1000 | Loss: 0.00000894
Iteration 86/1000 | Loss: 0.00000893
Iteration 87/1000 | Loss: 0.00000893
Iteration 88/1000 | Loss: 0.00000893
Iteration 89/1000 | Loss: 0.00000893
Iteration 90/1000 | Loss: 0.00000893
Iteration 91/1000 | Loss: 0.00000893
Iteration 92/1000 | Loss: 0.00000893
Iteration 93/1000 | Loss: 0.00000893
Iteration 94/1000 | Loss: 0.00000893
Iteration 95/1000 | Loss: 0.00000892
Iteration 96/1000 | Loss: 0.00000892
Iteration 97/1000 | Loss: 0.00000892
Iteration 98/1000 | Loss: 0.00000892
Iteration 99/1000 | Loss: 0.00000892
Iteration 100/1000 | Loss: 0.00000892
Iteration 101/1000 | Loss: 0.00000892
Iteration 102/1000 | Loss: 0.00000892
Iteration 103/1000 | Loss: 0.00000892
Iteration 104/1000 | Loss: 0.00000892
Iteration 105/1000 | Loss: 0.00000892
Iteration 106/1000 | Loss: 0.00000892
Iteration 107/1000 | Loss: 0.00000892
Iteration 108/1000 | Loss: 0.00000892
Iteration 109/1000 | Loss: 0.00000891
Iteration 110/1000 | Loss: 0.00000891
Iteration 111/1000 | Loss: 0.00000891
Iteration 112/1000 | Loss: 0.00000891
Iteration 113/1000 | Loss: 0.00000891
Iteration 114/1000 | Loss: 0.00000891
Iteration 115/1000 | Loss: 0.00000891
Iteration 116/1000 | Loss: 0.00000891
Iteration 117/1000 | Loss: 0.00000891
Iteration 118/1000 | Loss: 0.00000891
Iteration 119/1000 | Loss: 0.00000890
Iteration 120/1000 | Loss: 0.00000890
Iteration 121/1000 | Loss: 0.00000890
Iteration 122/1000 | Loss: 0.00000890
Iteration 123/1000 | Loss: 0.00000890
Iteration 124/1000 | Loss: 0.00000890
Iteration 125/1000 | Loss: 0.00000890
Iteration 126/1000 | Loss: 0.00000890
Iteration 127/1000 | Loss: 0.00000890
Iteration 128/1000 | Loss: 0.00000890
Iteration 129/1000 | Loss: 0.00000890
Iteration 130/1000 | Loss: 0.00000890
Iteration 131/1000 | Loss: 0.00000890
Iteration 132/1000 | Loss: 0.00000890
Iteration 133/1000 | Loss: 0.00000890
Iteration 134/1000 | Loss: 0.00000890
Iteration 135/1000 | Loss: 0.00000890
Iteration 136/1000 | Loss: 0.00000890
Iteration 137/1000 | Loss: 0.00000890
Iteration 138/1000 | Loss: 0.00000889
Iteration 139/1000 | Loss: 0.00000889
Iteration 140/1000 | Loss: 0.00000889
Iteration 141/1000 | Loss: 0.00000889
Iteration 142/1000 | Loss: 0.00000889
Iteration 143/1000 | Loss: 0.00000888
Iteration 144/1000 | Loss: 0.00000888
Iteration 145/1000 | Loss: 0.00000888
Iteration 146/1000 | Loss: 0.00000888
Iteration 147/1000 | Loss: 0.00000888
Iteration 148/1000 | Loss: 0.00000888
Iteration 149/1000 | Loss: 0.00000888
Iteration 150/1000 | Loss: 0.00000888
Iteration 151/1000 | Loss: 0.00000888
Iteration 152/1000 | Loss: 0.00000888
Iteration 153/1000 | Loss: 0.00000888
Iteration 154/1000 | Loss: 0.00000888
Iteration 155/1000 | Loss: 0.00000888
Iteration 156/1000 | Loss: 0.00000888
Iteration 157/1000 | Loss: 0.00000888
Iteration 158/1000 | Loss: 0.00000888
Iteration 159/1000 | Loss: 0.00000888
Iteration 160/1000 | Loss: 0.00000888
Iteration 161/1000 | Loss: 0.00000887
Iteration 162/1000 | Loss: 0.00000887
Iteration 163/1000 | Loss: 0.00000887
Iteration 164/1000 | Loss: 0.00000887
Iteration 165/1000 | Loss: 0.00000887
Iteration 166/1000 | Loss: 0.00000887
Iteration 167/1000 | Loss: 0.00000887
Iteration 168/1000 | Loss: 0.00000887
Iteration 169/1000 | Loss: 0.00000887
Iteration 170/1000 | Loss: 0.00000887
Iteration 171/1000 | Loss: 0.00000887
Iteration 172/1000 | Loss: 0.00000887
Iteration 173/1000 | Loss: 0.00000887
Iteration 174/1000 | Loss: 0.00000887
Iteration 175/1000 | Loss: 0.00000887
Iteration 176/1000 | Loss: 0.00000887
Iteration 177/1000 | Loss: 0.00000887
Iteration 178/1000 | Loss: 0.00000887
Iteration 179/1000 | Loss: 0.00000887
Iteration 180/1000 | Loss: 0.00000887
Iteration 181/1000 | Loss: 0.00000887
Iteration 182/1000 | Loss: 0.00000887
Iteration 183/1000 | Loss: 0.00000887
Iteration 184/1000 | Loss: 0.00000887
Iteration 185/1000 | Loss: 0.00000887
Iteration 186/1000 | Loss: 0.00000887
Iteration 187/1000 | Loss: 0.00000887
Iteration 188/1000 | Loss: 0.00000887
Iteration 189/1000 | Loss: 0.00000887
Iteration 190/1000 | Loss: 0.00000887
Iteration 191/1000 | Loss: 0.00000887
Iteration 192/1000 | Loss: 0.00000887
Iteration 193/1000 | Loss: 0.00000887
Iteration 194/1000 | Loss: 0.00000887
Iteration 195/1000 | Loss: 0.00000887
Iteration 196/1000 | Loss: 0.00000887
Iteration 197/1000 | Loss: 0.00000887
Iteration 198/1000 | Loss: 0.00000887
Iteration 199/1000 | Loss: 0.00000887
Iteration 200/1000 | Loss: 0.00000887
Iteration 201/1000 | Loss: 0.00000887
Iteration 202/1000 | Loss: 0.00000887
Iteration 203/1000 | Loss: 0.00000887
Iteration 204/1000 | Loss: 0.00000887
Iteration 205/1000 | Loss: 0.00000887
Iteration 206/1000 | Loss: 0.00000887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [8.873929800756741e-06, 8.873929800756741e-06, 8.873929800756741e-06, 8.873929800756741e-06, 8.873929800756741e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.873929800756741e-06

Optimization complete. Final v2v error: 2.5556640625 mm

Highest mean error: 2.904256582260132 mm for frame 4

Lowest mean error: 2.3873038291931152 mm for frame 55

Saving results

Total time: 36.235398292541504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_nl_6223/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_nl_6223/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013543
Iteration 2/25 | Loss: 0.00205821
Iteration 3/25 | Loss: 0.00168488
Iteration 4/25 | Loss: 0.00150725
Iteration 5/25 | Loss: 0.00146976
Iteration 6/25 | Loss: 0.00133749
Iteration 7/25 | Loss: 0.00130627
Iteration 8/25 | Loss: 0.00135308
Iteration 9/25 | Loss: 0.00126345
Iteration 10/25 | Loss: 0.00123836
Iteration 11/25 | Loss: 0.00123146
Iteration 12/25 | Loss: 0.00121396
Iteration 13/25 | Loss: 0.00121472
Iteration 14/25 | Loss: 0.00120110
Iteration 15/25 | Loss: 0.00119502
Iteration 16/25 | Loss: 0.00119403
Iteration 17/25 | Loss: 0.00119368
Iteration 18/25 | Loss: 0.00119346
Iteration 19/25 | Loss: 0.00119335
Iteration 20/25 | Loss: 0.00119334
Iteration 21/25 | Loss: 0.00119334
Iteration 22/25 | Loss: 0.00119334
Iteration 23/25 | Loss: 0.00119334
Iteration 24/25 | Loss: 0.00119333
Iteration 25/25 | Loss: 0.00119333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45738792
Iteration 2/25 | Loss: 0.00196481
Iteration 3/25 | Loss: 0.00196481
Iteration 4/25 | Loss: 0.00196481
Iteration 5/25 | Loss: 0.00196481
Iteration 6/25 | Loss: 0.00196481
Iteration 7/25 | Loss: 0.00196481
Iteration 8/25 | Loss: 0.00196481
Iteration 9/25 | Loss: 0.00196481
Iteration 10/25 | Loss: 0.00196481
Iteration 11/25 | Loss: 0.00196481
Iteration 12/25 | Loss: 0.00196480
Iteration 13/25 | Loss: 0.00196480
Iteration 14/25 | Loss: 0.00196480
Iteration 15/25 | Loss: 0.00196480
Iteration 16/25 | Loss: 0.00196480
Iteration 17/25 | Loss: 0.00196480
Iteration 18/25 | Loss: 0.00196480
Iteration 19/25 | Loss: 0.00196480
Iteration 20/25 | Loss: 0.00196480
Iteration 21/25 | Loss: 0.00196480
Iteration 22/25 | Loss: 0.00196480
Iteration 23/25 | Loss: 0.00196480
Iteration 24/25 | Loss: 0.00196480
Iteration 25/25 | Loss: 0.00196480

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196480
Iteration 2/1000 | Loss: 0.00006261
Iteration 3/1000 | Loss: 0.00004238
Iteration 4/1000 | Loss: 0.00003676
Iteration 5/1000 | Loss: 0.00019972
Iteration 6/1000 | Loss: 0.00004867
Iteration 7/1000 | Loss: 0.00003520
Iteration 8/1000 | Loss: 0.00003134
Iteration 9/1000 | Loss: 0.00003013
Iteration 10/1000 | Loss: 0.00002909
Iteration 11/1000 | Loss: 0.00019547
Iteration 12/1000 | Loss: 0.00002869
Iteration 13/1000 | Loss: 0.00002784
Iteration 14/1000 | Loss: 0.00002708
Iteration 15/1000 | Loss: 0.00081989
Iteration 16/1000 | Loss: 0.00004232
Iteration 17/1000 | Loss: 0.00002985
Iteration 18/1000 | Loss: 0.00003067
Iteration 19/1000 | Loss: 0.00002363
Iteration 20/1000 | Loss: 0.00002242
Iteration 21/1000 | Loss: 0.00002128
Iteration 22/1000 | Loss: 0.00015187
Iteration 23/1000 | Loss: 0.00002085
Iteration 24/1000 | Loss: 0.00002010
Iteration 25/1000 | Loss: 0.00001966
Iteration 26/1000 | Loss: 0.00002091
Iteration 27/1000 | Loss: 0.00001941
Iteration 28/1000 | Loss: 0.00002194
Iteration 29/1000 | Loss: 0.00002157
Iteration 30/1000 | Loss: 0.00002082
Iteration 31/1000 | Loss: 0.00001956
Iteration 32/1000 | Loss: 0.00001898
Iteration 33/1000 | Loss: 0.00001875
Iteration 34/1000 | Loss: 0.00001872
Iteration 35/1000 | Loss: 0.00001862
Iteration 36/1000 | Loss: 0.00001859
Iteration 37/1000 | Loss: 0.00001846
Iteration 38/1000 | Loss: 0.00001845
Iteration 39/1000 | Loss: 0.00001845
Iteration 40/1000 | Loss: 0.00001845
Iteration 41/1000 | Loss: 0.00001843
Iteration 42/1000 | Loss: 0.00001843
Iteration 43/1000 | Loss: 0.00001843
Iteration 44/1000 | Loss: 0.00001843
Iteration 45/1000 | Loss: 0.00001840
Iteration 46/1000 | Loss: 0.00001839
Iteration 47/1000 | Loss: 0.00001839
Iteration 48/1000 | Loss: 0.00001838
Iteration 49/1000 | Loss: 0.00001837
Iteration 50/1000 | Loss: 0.00001836
Iteration 51/1000 | Loss: 0.00002900
Iteration 52/1000 | Loss: 0.00020501
Iteration 53/1000 | Loss: 0.00001991
Iteration 54/1000 | Loss: 0.00001878
Iteration 55/1000 | Loss: 0.00011833
Iteration 56/1000 | Loss: 0.00003226
Iteration 57/1000 | Loss: 0.00006003
Iteration 58/1000 | Loss: 0.00001850
Iteration 59/1000 | Loss: 0.00001836
Iteration 60/1000 | Loss: 0.00001831
Iteration 61/1000 | Loss: 0.00001831
Iteration 62/1000 | Loss: 0.00001829
Iteration 63/1000 | Loss: 0.00001829
Iteration 64/1000 | Loss: 0.00001829
Iteration 65/1000 | Loss: 0.00001829
Iteration 66/1000 | Loss: 0.00001829
Iteration 67/1000 | Loss: 0.00001829
Iteration 68/1000 | Loss: 0.00001828
Iteration 69/1000 | Loss: 0.00001828
Iteration 70/1000 | Loss: 0.00001828
Iteration 71/1000 | Loss: 0.00001827
Iteration 72/1000 | Loss: 0.00001827
Iteration 73/1000 | Loss: 0.00001827
Iteration 74/1000 | Loss: 0.00001827
Iteration 75/1000 | Loss: 0.00001826
Iteration 76/1000 | Loss: 0.00001826
Iteration 77/1000 | Loss: 0.00001826
Iteration 78/1000 | Loss: 0.00001826
Iteration 79/1000 | Loss: 0.00001826
Iteration 80/1000 | Loss: 0.00001826
Iteration 81/1000 | Loss: 0.00001826
Iteration 82/1000 | Loss: 0.00001826
Iteration 83/1000 | Loss: 0.00001825
Iteration 84/1000 | Loss: 0.00001825
Iteration 85/1000 | Loss: 0.00001825
Iteration 86/1000 | Loss: 0.00001825
Iteration 87/1000 | Loss: 0.00001824
Iteration 88/1000 | Loss: 0.00001824
Iteration 89/1000 | Loss: 0.00001824
Iteration 90/1000 | Loss: 0.00001823
Iteration 91/1000 | Loss: 0.00001823
Iteration 92/1000 | Loss: 0.00001823
Iteration 93/1000 | Loss: 0.00001822
Iteration 94/1000 | Loss: 0.00001822
Iteration 95/1000 | Loss: 0.00001822
Iteration 96/1000 | Loss: 0.00001822
Iteration 97/1000 | Loss: 0.00001822
Iteration 98/1000 | Loss: 0.00001822
Iteration 99/1000 | Loss: 0.00001822
Iteration 100/1000 | Loss: 0.00001822
Iteration 101/1000 | Loss: 0.00001822
Iteration 102/1000 | Loss: 0.00001822
Iteration 103/1000 | Loss: 0.00001822
Iteration 104/1000 | Loss: 0.00001822
Iteration 105/1000 | Loss: 0.00001822
Iteration 106/1000 | Loss: 0.00001822
Iteration 107/1000 | Loss: 0.00001822
Iteration 108/1000 | Loss: 0.00001822
Iteration 109/1000 | Loss: 0.00001822
Iteration 110/1000 | Loss: 0.00001822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.8219376215711236e-05, 1.8219376215711236e-05, 1.8219376215711236e-05, 1.8219376215711236e-05, 1.8219376215711236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8219376215711236e-05

Optimization complete. Final v2v error: 3.3723390102386475 mm

Highest mean error: 9.309218406677246 mm for frame 36

Lowest mean error: 2.6995136737823486 mm for frame 29

Saving results

Total time: 96.87365388870239
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467751
Iteration 2/25 | Loss: 0.00158144
Iteration 3/25 | Loss: 0.00144026
Iteration 4/25 | Loss: 0.00143067
Iteration 5/25 | Loss: 0.00143024
Iteration 6/25 | Loss: 0.00143024
Iteration 7/25 | Loss: 0.00143024
Iteration 8/25 | Loss: 0.00143024
Iteration 9/25 | Loss: 0.00143024
Iteration 10/25 | Loss: 0.00143024
Iteration 11/25 | Loss: 0.00143024
Iteration 12/25 | Loss: 0.00143024
Iteration 13/25 | Loss: 0.00143024
Iteration 14/25 | Loss: 0.00143024
Iteration 15/25 | Loss: 0.00143024
Iteration 16/25 | Loss: 0.00143024
Iteration 17/25 | Loss: 0.00143024
Iteration 18/25 | Loss: 0.00143024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014302434865385294, 0.0014302434865385294, 0.0014302434865385294, 0.0014302434865385294, 0.0014302434865385294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014302434865385294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26350856
Iteration 2/25 | Loss: 0.00192935
Iteration 3/25 | Loss: 0.00192934
Iteration 4/25 | Loss: 0.00192934
Iteration 5/25 | Loss: 0.00192933
Iteration 6/25 | Loss: 0.00192933
Iteration 7/25 | Loss: 0.00192933
Iteration 8/25 | Loss: 0.00192933
Iteration 9/25 | Loss: 0.00192933
Iteration 10/25 | Loss: 0.00192933
Iteration 11/25 | Loss: 0.00192933
Iteration 12/25 | Loss: 0.00192933
Iteration 13/25 | Loss: 0.00192933
Iteration 14/25 | Loss: 0.00192933
Iteration 15/25 | Loss: 0.00192933
Iteration 16/25 | Loss: 0.00192933
Iteration 17/25 | Loss: 0.00192933
Iteration 18/25 | Loss: 0.00192933
Iteration 19/25 | Loss: 0.00192933
Iteration 20/25 | Loss: 0.00192933
Iteration 21/25 | Loss: 0.00192933
Iteration 22/25 | Loss: 0.00192933
Iteration 23/25 | Loss: 0.00192933
Iteration 24/25 | Loss: 0.00192933
Iteration 25/25 | Loss: 0.00192933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192933
Iteration 2/1000 | Loss: 0.00004771
Iteration 3/1000 | Loss: 0.00003180
Iteration 4/1000 | Loss: 0.00002747
Iteration 5/1000 | Loss: 0.00002583
Iteration 6/1000 | Loss: 0.00002468
Iteration 7/1000 | Loss: 0.00002391
Iteration 8/1000 | Loss: 0.00002325
Iteration 9/1000 | Loss: 0.00002263
Iteration 10/1000 | Loss: 0.00002209
Iteration 11/1000 | Loss: 0.00002162
Iteration 12/1000 | Loss: 0.00002135
Iteration 13/1000 | Loss: 0.00002116
Iteration 14/1000 | Loss: 0.00002097
Iteration 15/1000 | Loss: 0.00002074
Iteration 16/1000 | Loss: 0.00002058
Iteration 17/1000 | Loss: 0.00002050
Iteration 18/1000 | Loss: 0.00002049
Iteration 19/1000 | Loss: 0.00002038
Iteration 20/1000 | Loss: 0.00002037
Iteration 21/1000 | Loss: 0.00002036
Iteration 22/1000 | Loss: 0.00002035
Iteration 23/1000 | Loss: 0.00002034
Iteration 24/1000 | Loss: 0.00002034
Iteration 25/1000 | Loss: 0.00002034
Iteration 26/1000 | Loss: 0.00002033
Iteration 27/1000 | Loss: 0.00002033
Iteration 28/1000 | Loss: 0.00002031
Iteration 29/1000 | Loss: 0.00002028
Iteration 30/1000 | Loss: 0.00002026
Iteration 31/1000 | Loss: 0.00002020
Iteration 32/1000 | Loss: 0.00002020
Iteration 33/1000 | Loss: 0.00002014
Iteration 34/1000 | Loss: 0.00002014
Iteration 35/1000 | Loss: 0.00002012
Iteration 36/1000 | Loss: 0.00002012
Iteration 37/1000 | Loss: 0.00002011
Iteration 38/1000 | Loss: 0.00002011
Iteration 39/1000 | Loss: 0.00002009
Iteration 40/1000 | Loss: 0.00002007
Iteration 41/1000 | Loss: 0.00002006
Iteration 42/1000 | Loss: 0.00002006
Iteration 43/1000 | Loss: 0.00002005
Iteration 44/1000 | Loss: 0.00002005
Iteration 45/1000 | Loss: 0.00002004
Iteration 46/1000 | Loss: 0.00002004
Iteration 47/1000 | Loss: 0.00002003
Iteration 48/1000 | Loss: 0.00002003
Iteration 49/1000 | Loss: 0.00002003
Iteration 50/1000 | Loss: 0.00002002
Iteration 51/1000 | Loss: 0.00002002
Iteration 52/1000 | Loss: 0.00002002
Iteration 53/1000 | Loss: 0.00002002
Iteration 54/1000 | Loss: 0.00002001
Iteration 55/1000 | Loss: 0.00002001
Iteration 56/1000 | Loss: 0.00002001
Iteration 57/1000 | Loss: 0.00002001
Iteration 58/1000 | Loss: 0.00002001
Iteration 59/1000 | Loss: 0.00002001
Iteration 60/1000 | Loss: 0.00002001
Iteration 61/1000 | Loss: 0.00002001
Iteration 62/1000 | Loss: 0.00002001
Iteration 63/1000 | Loss: 0.00002001
Iteration 64/1000 | Loss: 0.00002001
Iteration 65/1000 | Loss: 0.00002000
Iteration 66/1000 | Loss: 0.00002000
Iteration 67/1000 | Loss: 0.00001999
Iteration 68/1000 | Loss: 0.00001999
Iteration 69/1000 | Loss: 0.00001999
Iteration 70/1000 | Loss: 0.00001999
Iteration 71/1000 | Loss: 0.00001998
Iteration 72/1000 | Loss: 0.00001998
Iteration 73/1000 | Loss: 0.00001998
Iteration 74/1000 | Loss: 0.00001998
Iteration 75/1000 | Loss: 0.00001997
Iteration 76/1000 | Loss: 0.00001997
Iteration 77/1000 | Loss: 0.00001997
Iteration 78/1000 | Loss: 0.00001997
Iteration 79/1000 | Loss: 0.00001997
Iteration 80/1000 | Loss: 0.00001997
Iteration 81/1000 | Loss: 0.00001996
Iteration 82/1000 | Loss: 0.00001996
Iteration 83/1000 | Loss: 0.00001996
Iteration 84/1000 | Loss: 0.00001996
Iteration 85/1000 | Loss: 0.00001996
Iteration 86/1000 | Loss: 0.00001996
Iteration 87/1000 | Loss: 0.00001996
Iteration 88/1000 | Loss: 0.00001996
Iteration 89/1000 | Loss: 0.00001996
Iteration 90/1000 | Loss: 0.00001996
Iteration 91/1000 | Loss: 0.00001996
Iteration 92/1000 | Loss: 0.00001995
Iteration 93/1000 | Loss: 0.00001995
Iteration 94/1000 | Loss: 0.00001995
Iteration 95/1000 | Loss: 0.00001995
Iteration 96/1000 | Loss: 0.00001995
Iteration 97/1000 | Loss: 0.00001995
Iteration 98/1000 | Loss: 0.00001995
Iteration 99/1000 | Loss: 0.00001995
Iteration 100/1000 | Loss: 0.00001994
Iteration 101/1000 | Loss: 0.00001994
Iteration 102/1000 | Loss: 0.00001994
Iteration 103/1000 | Loss: 0.00001994
Iteration 104/1000 | Loss: 0.00001994
Iteration 105/1000 | Loss: 0.00001994
Iteration 106/1000 | Loss: 0.00001994
Iteration 107/1000 | Loss: 0.00001994
Iteration 108/1000 | Loss: 0.00001994
Iteration 109/1000 | Loss: 0.00001994
Iteration 110/1000 | Loss: 0.00001994
Iteration 111/1000 | Loss: 0.00001994
Iteration 112/1000 | Loss: 0.00001994
Iteration 113/1000 | Loss: 0.00001994
Iteration 114/1000 | Loss: 0.00001994
Iteration 115/1000 | Loss: 0.00001994
Iteration 116/1000 | Loss: 0.00001993
Iteration 117/1000 | Loss: 0.00001993
Iteration 118/1000 | Loss: 0.00001993
Iteration 119/1000 | Loss: 0.00001993
Iteration 120/1000 | Loss: 0.00001993
Iteration 121/1000 | Loss: 0.00001993
Iteration 122/1000 | Loss: 0.00001993
Iteration 123/1000 | Loss: 0.00001993
Iteration 124/1000 | Loss: 0.00001993
Iteration 125/1000 | Loss: 0.00001992
Iteration 126/1000 | Loss: 0.00001992
Iteration 127/1000 | Loss: 0.00001992
Iteration 128/1000 | Loss: 0.00001992
Iteration 129/1000 | Loss: 0.00001992
Iteration 130/1000 | Loss: 0.00001992
Iteration 131/1000 | Loss: 0.00001992
Iteration 132/1000 | Loss: 0.00001992
Iteration 133/1000 | Loss: 0.00001992
Iteration 134/1000 | Loss: 0.00001992
Iteration 135/1000 | Loss: 0.00001992
Iteration 136/1000 | Loss: 0.00001992
Iteration 137/1000 | Loss: 0.00001991
Iteration 138/1000 | Loss: 0.00001991
Iteration 139/1000 | Loss: 0.00001991
Iteration 140/1000 | Loss: 0.00001991
Iteration 141/1000 | Loss: 0.00001991
Iteration 142/1000 | Loss: 0.00001991
Iteration 143/1000 | Loss: 0.00001991
Iteration 144/1000 | Loss: 0.00001991
Iteration 145/1000 | Loss: 0.00001991
Iteration 146/1000 | Loss: 0.00001991
Iteration 147/1000 | Loss: 0.00001991
Iteration 148/1000 | Loss: 0.00001991
Iteration 149/1000 | Loss: 0.00001991
Iteration 150/1000 | Loss: 0.00001991
Iteration 151/1000 | Loss: 0.00001991
Iteration 152/1000 | Loss: 0.00001991
Iteration 153/1000 | Loss: 0.00001991
Iteration 154/1000 | Loss: 0.00001991
Iteration 155/1000 | Loss: 0.00001991
Iteration 156/1000 | Loss: 0.00001991
Iteration 157/1000 | Loss: 0.00001991
Iteration 158/1000 | Loss: 0.00001991
Iteration 159/1000 | Loss: 0.00001991
Iteration 160/1000 | Loss: 0.00001991
Iteration 161/1000 | Loss: 0.00001991
Iteration 162/1000 | Loss: 0.00001991
Iteration 163/1000 | Loss: 0.00001991
Iteration 164/1000 | Loss: 0.00001991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.9908529793610796e-05, 1.9908529793610796e-05, 1.9908529793610796e-05, 1.9908529793610796e-05, 1.9908529793610796e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9908529793610796e-05

Optimization complete. Final v2v error: 3.7406485080718994 mm

Highest mean error: 4.043157577514648 mm for frame 166

Lowest mean error: 3.319504976272583 mm for frame 150

Saving results

Total time: 49.63378143310547
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00609506
Iteration 2/25 | Loss: 0.00181360
Iteration 3/25 | Loss: 0.00160022
Iteration 4/25 | Loss: 0.00157277
Iteration 5/25 | Loss: 0.00156215
Iteration 6/25 | Loss: 0.00155867
Iteration 7/25 | Loss: 0.00155142
Iteration 8/25 | Loss: 0.00154618
Iteration 9/25 | Loss: 0.00154058
Iteration 10/25 | Loss: 0.00154097
Iteration 11/25 | Loss: 0.00153688
Iteration 12/25 | Loss: 0.00153722
Iteration 13/25 | Loss: 0.00153496
Iteration 14/25 | Loss: 0.00153318
Iteration 15/25 | Loss: 0.00153192
Iteration 16/25 | Loss: 0.00153304
Iteration 17/25 | Loss: 0.00153067
Iteration 18/25 | Loss: 0.00152997
Iteration 19/25 | Loss: 0.00152708
Iteration 20/25 | Loss: 0.00152523
Iteration 21/25 | Loss: 0.00152467
Iteration 22/25 | Loss: 0.00152892
Iteration 23/25 | Loss: 0.00152395
Iteration 24/25 | Loss: 0.00152199
Iteration 25/25 | Loss: 0.00152172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22649264
Iteration 2/25 | Loss: 0.00264284
Iteration 3/25 | Loss: 0.00264284
Iteration 4/25 | Loss: 0.00264284
Iteration 5/25 | Loss: 0.00264284
Iteration 6/25 | Loss: 0.00264284
Iteration 7/25 | Loss: 0.00264284
Iteration 8/25 | Loss: 0.00264284
Iteration 9/25 | Loss: 0.00264284
Iteration 10/25 | Loss: 0.00264284
Iteration 11/25 | Loss: 0.00264284
Iteration 12/25 | Loss: 0.00264284
Iteration 13/25 | Loss: 0.00264284
Iteration 14/25 | Loss: 0.00264284
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002642836421728134, 0.002642836421728134, 0.002642836421728134, 0.002642836421728134, 0.002642836421728134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002642836421728134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00264284
Iteration 2/1000 | Loss: 0.00017787
Iteration 3/1000 | Loss: 0.00013122
Iteration 4/1000 | Loss: 0.00048608
Iteration 5/1000 | Loss: 0.00011685
Iteration 6/1000 | Loss: 0.00010842
Iteration 7/1000 | Loss: 0.00010244
Iteration 8/1000 | Loss: 0.00110448
Iteration 9/1000 | Loss: 0.00071291
Iteration 10/1000 | Loss: 0.00060951
Iteration 11/1000 | Loss: 0.00038726
Iteration 12/1000 | Loss: 0.00127693
Iteration 13/1000 | Loss: 0.00030788
Iteration 14/1000 | Loss: 0.00022212
Iteration 15/1000 | Loss: 0.00065916
Iteration 16/1000 | Loss: 0.00059494
Iteration 17/1000 | Loss: 0.00060274
Iteration 18/1000 | Loss: 0.00027969
Iteration 19/1000 | Loss: 0.00028001
Iteration 20/1000 | Loss: 0.00026026
Iteration 21/1000 | Loss: 0.00017785
Iteration 22/1000 | Loss: 0.00007727
Iteration 23/1000 | Loss: 0.00007206
Iteration 24/1000 | Loss: 0.00006799
Iteration 25/1000 | Loss: 0.00019752
Iteration 26/1000 | Loss: 0.00007460
Iteration 27/1000 | Loss: 0.00031419
Iteration 28/1000 | Loss: 0.00006351
Iteration 29/1000 | Loss: 0.00053869
Iteration 30/1000 | Loss: 0.00006772
Iteration 31/1000 | Loss: 0.00005931
Iteration 32/1000 | Loss: 0.00005659
Iteration 33/1000 | Loss: 0.00011207
Iteration 34/1000 | Loss: 0.00006101
Iteration 35/1000 | Loss: 0.00006508
Iteration 36/1000 | Loss: 0.00005418
Iteration 37/1000 | Loss: 0.00005316
Iteration 38/1000 | Loss: 0.00010950
Iteration 39/1000 | Loss: 0.00006202
Iteration 40/1000 | Loss: 0.00009829
Iteration 41/1000 | Loss: 0.00007790
Iteration 42/1000 | Loss: 0.00008884
Iteration 43/1000 | Loss: 0.00005281
Iteration 44/1000 | Loss: 0.00005115
Iteration 45/1000 | Loss: 0.00005057
Iteration 46/1000 | Loss: 0.00005002
Iteration 47/1000 | Loss: 0.00004952
Iteration 48/1000 | Loss: 0.00004912
Iteration 49/1000 | Loss: 0.00004872
Iteration 50/1000 | Loss: 0.00004843
Iteration 51/1000 | Loss: 0.00004826
Iteration 52/1000 | Loss: 0.00004826
Iteration 53/1000 | Loss: 0.00004824
Iteration 54/1000 | Loss: 0.00004807
Iteration 55/1000 | Loss: 0.00004796
Iteration 56/1000 | Loss: 0.00004790
Iteration 57/1000 | Loss: 0.00004782
Iteration 58/1000 | Loss: 0.00004780
Iteration 59/1000 | Loss: 0.00004779
Iteration 60/1000 | Loss: 0.00004778
Iteration 61/1000 | Loss: 0.00004777
Iteration 62/1000 | Loss: 0.00004777
Iteration 63/1000 | Loss: 0.00004776
Iteration 64/1000 | Loss: 0.00004776
Iteration 65/1000 | Loss: 0.00004775
Iteration 66/1000 | Loss: 0.00004774
Iteration 67/1000 | Loss: 0.00004774
Iteration 68/1000 | Loss: 0.00004773
Iteration 69/1000 | Loss: 0.00004771
Iteration 70/1000 | Loss: 0.00004771
Iteration 71/1000 | Loss: 0.00004771
Iteration 72/1000 | Loss: 0.00004771
Iteration 73/1000 | Loss: 0.00004771
Iteration 74/1000 | Loss: 0.00004770
Iteration 75/1000 | Loss: 0.00004770
Iteration 76/1000 | Loss: 0.00004770
Iteration 77/1000 | Loss: 0.00004769
Iteration 78/1000 | Loss: 0.00004769
Iteration 79/1000 | Loss: 0.00004768
Iteration 80/1000 | Loss: 0.00004767
Iteration 81/1000 | Loss: 0.00004767
Iteration 82/1000 | Loss: 0.00004766
Iteration 83/1000 | Loss: 0.00004766
Iteration 84/1000 | Loss: 0.00004765
Iteration 85/1000 | Loss: 0.00004765
Iteration 86/1000 | Loss: 0.00004765
Iteration 87/1000 | Loss: 0.00004764
Iteration 88/1000 | Loss: 0.00004764
Iteration 89/1000 | Loss: 0.00004763
Iteration 90/1000 | Loss: 0.00004763
Iteration 91/1000 | Loss: 0.00004762
Iteration 92/1000 | Loss: 0.00004762
Iteration 93/1000 | Loss: 0.00004761
Iteration 94/1000 | Loss: 0.00004761
Iteration 95/1000 | Loss: 0.00004761
Iteration 96/1000 | Loss: 0.00004760
Iteration 97/1000 | Loss: 0.00004760
Iteration 98/1000 | Loss: 0.00004760
Iteration 99/1000 | Loss: 0.00004759
Iteration 100/1000 | Loss: 0.00004759
Iteration 101/1000 | Loss: 0.00004758
Iteration 102/1000 | Loss: 0.00004758
Iteration 103/1000 | Loss: 0.00004757
Iteration 104/1000 | Loss: 0.00004757
Iteration 105/1000 | Loss: 0.00004757
Iteration 106/1000 | Loss: 0.00004756
Iteration 107/1000 | Loss: 0.00004756
Iteration 108/1000 | Loss: 0.00004755
Iteration 109/1000 | Loss: 0.00004755
Iteration 110/1000 | Loss: 0.00004755
Iteration 111/1000 | Loss: 0.00004754
Iteration 112/1000 | Loss: 0.00004754
Iteration 113/1000 | Loss: 0.00004754
Iteration 114/1000 | Loss: 0.00004753
Iteration 115/1000 | Loss: 0.00004753
Iteration 116/1000 | Loss: 0.00004752
Iteration 117/1000 | Loss: 0.00004752
Iteration 118/1000 | Loss: 0.00004752
Iteration 119/1000 | Loss: 0.00004751
Iteration 120/1000 | Loss: 0.00004751
Iteration 121/1000 | Loss: 0.00004751
Iteration 122/1000 | Loss: 0.00004750
Iteration 123/1000 | Loss: 0.00004750
Iteration 124/1000 | Loss: 0.00004749
Iteration 125/1000 | Loss: 0.00004749
Iteration 126/1000 | Loss: 0.00004749
Iteration 127/1000 | Loss: 0.00004748
Iteration 128/1000 | Loss: 0.00004748
Iteration 129/1000 | Loss: 0.00004747
Iteration 130/1000 | Loss: 0.00004747
Iteration 131/1000 | Loss: 0.00004746
Iteration 132/1000 | Loss: 0.00004746
Iteration 133/1000 | Loss: 0.00004745
Iteration 134/1000 | Loss: 0.00004745
Iteration 135/1000 | Loss: 0.00004745
Iteration 136/1000 | Loss: 0.00004744
Iteration 137/1000 | Loss: 0.00004744
Iteration 138/1000 | Loss: 0.00004743
Iteration 139/1000 | Loss: 0.00004743
Iteration 140/1000 | Loss: 0.00004743
Iteration 141/1000 | Loss: 0.00004742
Iteration 142/1000 | Loss: 0.00004742
Iteration 143/1000 | Loss: 0.00004742
Iteration 144/1000 | Loss: 0.00004741
Iteration 145/1000 | Loss: 0.00004741
Iteration 146/1000 | Loss: 0.00004741
Iteration 147/1000 | Loss: 0.00004740
Iteration 148/1000 | Loss: 0.00004740
Iteration 149/1000 | Loss: 0.00004740
Iteration 150/1000 | Loss: 0.00004739
Iteration 151/1000 | Loss: 0.00004739
Iteration 152/1000 | Loss: 0.00004739
Iteration 153/1000 | Loss: 0.00004738
Iteration 154/1000 | Loss: 0.00004738
Iteration 155/1000 | Loss: 0.00004738
Iteration 156/1000 | Loss: 0.00004738
Iteration 157/1000 | Loss: 0.00004737
Iteration 158/1000 | Loss: 0.00004737
Iteration 159/1000 | Loss: 0.00004737
Iteration 160/1000 | Loss: 0.00004737
Iteration 161/1000 | Loss: 0.00004737
Iteration 162/1000 | Loss: 0.00004736
Iteration 163/1000 | Loss: 0.00004736
Iteration 164/1000 | Loss: 0.00004736
Iteration 165/1000 | Loss: 0.00004735
Iteration 166/1000 | Loss: 0.00004735
Iteration 167/1000 | Loss: 0.00004735
Iteration 168/1000 | Loss: 0.00004735
Iteration 169/1000 | Loss: 0.00004735
Iteration 170/1000 | Loss: 0.00004735
Iteration 171/1000 | Loss: 0.00004735
Iteration 172/1000 | Loss: 0.00004735
Iteration 173/1000 | Loss: 0.00004734
Iteration 174/1000 | Loss: 0.00004734
Iteration 175/1000 | Loss: 0.00004734
Iteration 176/1000 | Loss: 0.00004734
Iteration 177/1000 | Loss: 0.00004734
Iteration 178/1000 | Loss: 0.00004734
Iteration 179/1000 | Loss: 0.00004734
Iteration 180/1000 | Loss: 0.00004734
Iteration 181/1000 | Loss: 0.00004734
Iteration 182/1000 | Loss: 0.00004734
Iteration 183/1000 | Loss: 0.00004734
Iteration 184/1000 | Loss: 0.00004734
Iteration 185/1000 | Loss: 0.00004733
Iteration 186/1000 | Loss: 0.00004733
Iteration 187/1000 | Loss: 0.00004733
Iteration 188/1000 | Loss: 0.00004733
Iteration 189/1000 | Loss: 0.00004733
Iteration 190/1000 | Loss: 0.00004733
Iteration 191/1000 | Loss: 0.00004733
Iteration 192/1000 | Loss: 0.00004733
Iteration 193/1000 | Loss: 0.00004733
Iteration 194/1000 | Loss: 0.00004733
Iteration 195/1000 | Loss: 0.00004733
Iteration 196/1000 | Loss: 0.00004733
Iteration 197/1000 | Loss: 0.00004733
Iteration 198/1000 | Loss: 0.00004732
Iteration 199/1000 | Loss: 0.00004732
Iteration 200/1000 | Loss: 0.00004732
Iteration 201/1000 | Loss: 0.00004732
Iteration 202/1000 | Loss: 0.00004732
Iteration 203/1000 | Loss: 0.00004732
Iteration 204/1000 | Loss: 0.00004732
Iteration 205/1000 | Loss: 0.00004732
Iteration 206/1000 | Loss: 0.00004732
Iteration 207/1000 | Loss: 0.00004732
Iteration 208/1000 | Loss: 0.00004731
Iteration 209/1000 | Loss: 0.00004731
Iteration 210/1000 | Loss: 0.00004731
Iteration 211/1000 | Loss: 0.00004731
Iteration 212/1000 | Loss: 0.00004731
Iteration 213/1000 | Loss: 0.00004731
Iteration 214/1000 | Loss: 0.00004731
Iteration 215/1000 | Loss: 0.00004731
Iteration 216/1000 | Loss: 0.00004731
Iteration 217/1000 | Loss: 0.00004731
Iteration 218/1000 | Loss: 0.00004730
Iteration 219/1000 | Loss: 0.00004730
Iteration 220/1000 | Loss: 0.00004730
Iteration 221/1000 | Loss: 0.00004730
Iteration 222/1000 | Loss: 0.00004730
Iteration 223/1000 | Loss: 0.00004730
Iteration 224/1000 | Loss: 0.00004730
Iteration 225/1000 | Loss: 0.00004730
Iteration 226/1000 | Loss: 0.00004729
Iteration 227/1000 | Loss: 0.00004729
Iteration 228/1000 | Loss: 0.00004729
Iteration 229/1000 | Loss: 0.00004729
Iteration 230/1000 | Loss: 0.00004729
Iteration 231/1000 | Loss: 0.00004729
Iteration 232/1000 | Loss: 0.00004728
Iteration 233/1000 | Loss: 0.00004728
Iteration 234/1000 | Loss: 0.00004728
Iteration 235/1000 | Loss: 0.00004728
Iteration 236/1000 | Loss: 0.00004728
Iteration 237/1000 | Loss: 0.00004728
Iteration 238/1000 | Loss: 0.00004728
Iteration 239/1000 | Loss: 0.00004728
Iteration 240/1000 | Loss: 0.00004728
Iteration 241/1000 | Loss: 0.00004728
Iteration 242/1000 | Loss: 0.00004728
Iteration 243/1000 | Loss: 0.00035154
Iteration 244/1000 | Loss: 0.00004996
Iteration 245/1000 | Loss: 0.00004839
Iteration 246/1000 | Loss: 0.00004756
Iteration 247/1000 | Loss: 0.00004668
Iteration 248/1000 | Loss: 0.00004630
Iteration 249/1000 | Loss: 0.00004621
Iteration 250/1000 | Loss: 0.00004615
Iteration 251/1000 | Loss: 0.00004614
Iteration 252/1000 | Loss: 0.00004614
Iteration 253/1000 | Loss: 0.00004613
Iteration 254/1000 | Loss: 0.00004612
Iteration 255/1000 | Loss: 0.00004612
Iteration 256/1000 | Loss: 0.00004612
Iteration 257/1000 | Loss: 0.00004612
Iteration 258/1000 | Loss: 0.00004612
Iteration 259/1000 | Loss: 0.00004611
Iteration 260/1000 | Loss: 0.00004610
Iteration 261/1000 | Loss: 0.00004610
Iteration 262/1000 | Loss: 0.00004609
Iteration 263/1000 | Loss: 0.00004609
Iteration 264/1000 | Loss: 0.00004608
Iteration 265/1000 | Loss: 0.00004608
Iteration 266/1000 | Loss: 0.00004608
Iteration 267/1000 | Loss: 0.00004607
Iteration 268/1000 | Loss: 0.00004607
Iteration 269/1000 | Loss: 0.00004607
Iteration 270/1000 | Loss: 0.00004606
Iteration 271/1000 | Loss: 0.00004606
Iteration 272/1000 | Loss: 0.00004606
Iteration 273/1000 | Loss: 0.00004606
Iteration 274/1000 | Loss: 0.00004606
Iteration 275/1000 | Loss: 0.00004606
Iteration 276/1000 | Loss: 0.00004605
Iteration 277/1000 | Loss: 0.00004605
Iteration 278/1000 | Loss: 0.00004605
Iteration 279/1000 | Loss: 0.00004605
Iteration 280/1000 | Loss: 0.00004605
Iteration 281/1000 | Loss: 0.00004605
Iteration 282/1000 | Loss: 0.00004605
Iteration 283/1000 | Loss: 0.00004604
Iteration 284/1000 | Loss: 0.00004604
Iteration 285/1000 | Loss: 0.00004603
Iteration 286/1000 | Loss: 0.00004603
Iteration 287/1000 | Loss: 0.00004603
Iteration 288/1000 | Loss: 0.00004603
Iteration 289/1000 | Loss: 0.00004603
Iteration 290/1000 | Loss: 0.00004602
Iteration 291/1000 | Loss: 0.00004602
Iteration 292/1000 | Loss: 0.00004602
Iteration 293/1000 | Loss: 0.00004602
Iteration 294/1000 | Loss: 0.00004601
Iteration 295/1000 | Loss: 0.00004601
Iteration 296/1000 | Loss: 0.00004601
Iteration 297/1000 | Loss: 0.00004601
Iteration 298/1000 | Loss: 0.00004601
Iteration 299/1000 | Loss: 0.00004600
Iteration 300/1000 | Loss: 0.00004600
Iteration 301/1000 | Loss: 0.00004600
Iteration 302/1000 | Loss: 0.00004600
Iteration 303/1000 | Loss: 0.00004600
Iteration 304/1000 | Loss: 0.00004599
Iteration 305/1000 | Loss: 0.00004599
Iteration 306/1000 | Loss: 0.00004599
Iteration 307/1000 | Loss: 0.00004599
Iteration 308/1000 | Loss: 0.00004599
Iteration 309/1000 | Loss: 0.00004598
Iteration 310/1000 | Loss: 0.00004598
Iteration 311/1000 | Loss: 0.00004598
Iteration 312/1000 | Loss: 0.00004598
Iteration 313/1000 | Loss: 0.00004598
Iteration 314/1000 | Loss: 0.00004598
Iteration 315/1000 | Loss: 0.00004597
Iteration 316/1000 | Loss: 0.00004597
Iteration 317/1000 | Loss: 0.00004597
Iteration 318/1000 | Loss: 0.00004597
Iteration 319/1000 | Loss: 0.00004596
Iteration 320/1000 | Loss: 0.00004596
Iteration 321/1000 | Loss: 0.00004596
Iteration 322/1000 | Loss: 0.00004596
Iteration 323/1000 | Loss: 0.00004595
Iteration 324/1000 | Loss: 0.00004595
Iteration 325/1000 | Loss: 0.00004595
Iteration 326/1000 | Loss: 0.00004594
Iteration 327/1000 | Loss: 0.00004594
Iteration 328/1000 | Loss: 0.00004594
Iteration 329/1000 | Loss: 0.00004594
Iteration 330/1000 | Loss: 0.00004593
Iteration 331/1000 | Loss: 0.00004593
Iteration 332/1000 | Loss: 0.00004593
Iteration 333/1000 | Loss: 0.00004592
Iteration 334/1000 | Loss: 0.00004592
Iteration 335/1000 | Loss: 0.00004592
Iteration 336/1000 | Loss: 0.00004592
Iteration 337/1000 | Loss: 0.00004592
Iteration 338/1000 | Loss: 0.00004591
Iteration 339/1000 | Loss: 0.00004591
Iteration 340/1000 | Loss: 0.00004591
Iteration 341/1000 | Loss: 0.00004591
Iteration 342/1000 | Loss: 0.00004591
Iteration 343/1000 | Loss: 0.00004590
Iteration 344/1000 | Loss: 0.00004590
Iteration 345/1000 | Loss: 0.00004590
Iteration 346/1000 | Loss: 0.00004590
Iteration 347/1000 | Loss: 0.00004590
Iteration 348/1000 | Loss: 0.00004590
Iteration 349/1000 | Loss: 0.00004590
Iteration 350/1000 | Loss: 0.00004589
Iteration 351/1000 | Loss: 0.00004589
Iteration 352/1000 | Loss: 0.00004589
Iteration 353/1000 | Loss: 0.00004589
Iteration 354/1000 | Loss: 0.00004589
Iteration 355/1000 | Loss: 0.00004589
Iteration 356/1000 | Loss: 0.00004589
Iteration 357/1000 | Loss: 0.00004589
Iteration 358/1000 | Loss: 0.00004588
Iteration 359/1000 | Loss: 0.00004588
Iteration 360/1000 | Loss: 0.00004588
Iteration 361/1000 | Loss: 0.00004588
Iteration 362/1000 | Loss: 0.00004588
Iteration 363/1000 | Loss: 0.00004588
Iteration 364/1000 | Loss: 0.00004588
Iteration 365/1000 | Loss: 0.00004588
Iteration 366/1000 | Loss: 0.00004588
Iteration 367/1000 | Loss: 0.00004588
Iteration 368/1000 | Loss: 0.00004588
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 368. Stopping optimization.
Last 5 losses: [4.5877557568019256e-05, 4.5877557568019256e-05, 4.5877557568019256e-05, 4.5877557568019256e-05, 4.5877557568019256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.5877557568019256e-05

Optimization complete. Final v2v error: 4.062374591827393 mm

Highest mean error: 11.245248794555664 mm for frame 20

Lowest mean error: 2.7070300579071045 mm for frame 228

Saving results

Total time: 172.97807431221008
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422211
Iteration 2/25 | Loss: 0.00140473
Iteration 3/25 | Loss: 0.00134597
Iteration 4/25 | Loss: 0.00133908
Iteration 5/25 | Loss: 0.00133908
Iteration 6/25 | Loss: 0.00133908
Iteration 7/25 | Loss: 0.00133908
Iteration 8/25 | Loss: 0.00133908
Iteration 9/25 | Loss: 0.00133908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.00133907503914088, 0.00133907503914088, 0.00133907503914088, 0.00133907503914088, 0.00133907503914088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00133907503914088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.37384868
Iteration 2/25 | Loss: 0.00157883
Iteration 3/25 | Loss: 0.00157883
Iteration 4/25 | Loss: 0.00157883
Iteration 5/25 | Loss: 0.00157882
Iteration 6/25 | Loss: 0.00157882
Iteration 7/25 | Loss: 0.00157882
Iteration 8/25 | Loss: 0.00157882
Iteration 9/25 | Loss: 0.00157882
Iteration 10/25 | Loss: 0.00157882
Iteration 11/25 | Loss: 0.00157882
Iteration 12/25 | Loss: 0.00157882
Iteration 13/25 | Loss: 0.00157882
Iteration 14/25 | Loss: 0.00157882
Iteration 15/25 | Loss: 0.00157882
Iteration 16/25 | Loss: 0.00157882
Iteration 17/25 | Loss: 0.00157882
Iteration 18/25 | Loss: 0.00157882
Iteration 19/25 | Loss: 0.00157882
Iteration 20/25 | Loss: 0.00157882
Iteration 21/25 | Loss: 0.00157882
Iteration 22/25 | Loss: 0.00157882
Iteration 23/25 | Loss: 0.00157882
Iteration 24/25 | Loss: 0.00157882
Iteration 25/25 | Loss: 0.00157882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157882
Iteration 2/1000 | Loss: 0.00002419
Iteration 3/1000 | Loss: 0.00002055
Iteration 4/1000 | Loss: 0.00001906
Iteration 5/1000 | Loss: 0.00001776
Iteration 6/1000 | Loss: 0.00001696
Iteration 7/1000 | Loss: 0.00001625
Iteration 8/1000 | Loss: 0.00001585
Iteration 9/1000 | Loss: 0.00001553
Iteration 10/1000 | Loss: 0.00001516
Iteration 11/1000 | Loss: 0.00001481
Iteration 12/1000 | Loss: 0.00001454
Iteration 13/1000 | Loss: 0.00001430
Iteration 14/1000 | Loss: 0.00001410
Iteration 15/1000 | Loss: 0.00001394
Iteration 16/1000 | Loss: 0.00001386
Iteration 17/1000 | Loss: 0.00001380
Iteration 18/1000 | Loss: 0.00001380
Iteration 19/1000 | Loss: 0.00001379
Iteration 20/1000 | Loss: 0.00001376
Iteration 21/1000 | Loss: 0.00001374
Iteration 22/1000 | Loss: 0.00001374
Iteration 23/1000 | Loss: 0.00001366
Iteration 24/1000 | Loss: 0.00001363
Iteration 25/1000 | Loss: 0.00001362
Iteration 26/1000 | Loss: 0.00001362
Iteration 27/1000 | Loss: 0.00001356
Iteration 28/1000 | Loss: 0.00001355
Iteration 29/1000 | Loss: 0.00001355
Iteration 30/1000 | Loss: 0.00001355
Iteration 31/1000 | Loss: 0.00001355
Iteration 32/1000 | Loss: 0.00001353
Iteration 33/1000 | Loss: 0.00001353
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001352
Iteration 36/1000 | Loss: 0.00001352
Iteration 37/1000 | Loss: 0.00001352
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001352
Iteration 40/1000 | Loss: 0.00001352
Iteration 41/1000 | Loss: 0.00001352
Iteration 42/1000 | Loss: 0.00001352
Iteration 43/1000 | Loss: 0.00001352
Iteration 44/1000 | Loss: 0.00001352
Iteration 45/1000 | Loss: 0.00001352
Iteration 46/1000 | Loss: 0.00001352
Iteration 47/1000 | Loss: 0.00001352
Iteration 48/1000 | Loss: 0.00001352
Iteration 49/1000 | Loss: 0.00001352
Iteration 50/1000 | Loss: 0.00001352
Iteration 51/1000 | Loss: 0.00001352
Iteration 52/1000 | Loss: 0.00001352
Iteration 53/1000 | Loss: 0.00001352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 53. Stopping optimization.
Last 5 losses: [1.3520165339286905e-05, 1.3520165339286905e-05, 1.3520165339286905e-05, 1.3520165339286905e-05, 1.3520165339286905e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3520165339286905e-05

Optimization complete. Final v2v error: 3.207550525665283 mm

Highest mean error: 3.3548855781555176 mm for frame 66

Lowest mean error: 3.0550968647003174 mm for frame 11

Saving results

Total time: 36.496859312057495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032395
Iteration 2/25 | Loss: 0.00173030
Iteration 3/25 | Loss: 0.00147041
Iteration 4/25 | Loss: 0.00144720
Iteration 5/25 | Loss: 0.00144071
Iteration 6/25 | Loss: 0.00143922
Iteration 7/25 | Loss: 0.00143922
Iteration 8/25 | Loss: 0.00143922
Iteration 9/25 | Loss: 0.00143922
Iteration 10/25 | Loss: 0.00143922
Iteration 11/25 | Loss: 0.00143922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014392196899279952, 0.0014392196899279952, 0.0014392196899279952, 0.0014392196899279952, 0.0014392196899279952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014392196899279952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06577420
Iteration 2/25 | Loss: 0.00203811
Iteration 3/25 | Loss: 0.00203810
Iteration 4/25 | Loss: 0.00203810
Iteration 5/25 | Loss: 0.00203810
Iteration 6/25 | Loss: 0.00203810
Iteration 7/25 | Loss: 0.00203810
Iteration 8/25 | Loss: 0.00203810
Iteration 9/25 | Loss: 0.00203810
Iteration 10/25 | Loss: 0.00203810
Iteration 11/25 | Loss: 0.00203810
Iteration 12/25 | Loss: 0.00203810
Iteration 13/25 | Loss: 0.00203810
Iteration 14/25 | Loss: 0.00203810
Iteration 15/25 | Loss: 0.00203810
Iteration 16/25 | Loss: 0.00203810
Iteration 17/25 | Loss: 0.00203810
Iteration 18/25 | Loss: 0.00203810
Iteration 19/25 | Loss: 0.00203810
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0020380986388772726, 0.0020380986388772726, 0.0020380986388772726, 0.0020380986388772726, 0.0020380986388772726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020380986388772726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00203810
Iteration 2/1000 | Loss: 0.00006517
Iteration 3/1000 | Loss: 0.00004134
Iteration 4/1000 | Loss: 0.00003138
Iteration 5/1000 | Loss: 0.00002876
Iteration 6/1000 | Loss: 0.00002719
Iteration 7/1000 | Loss: 0.00002640
Iteration 8/1000 | Loss: 0.00002554
Iteration 9/1000 | Loss: 0.00002504
Iteration 10/1000 | Loss: 0.00002465
Iteration 11/1000 | Loss: 0.00002427
Iteration 12/1000 | Loss: 0.00002403
Iteration 13/1000 | Loss: 0.00002383
Iteration 14/1000 | Loss: 0.00002363
Iteration 15/1000 | Loss: 0.00002344
Iteration 16/1000 | Loss: 0.00002341
Iteration 17/1000 | Loss: 0.00002336
Iteration 18/1000 | Loss: 0.00002336
Iteration 19/1000 | Loss: 0.00002331
Iteration 20/1000 | Loss: 0.00002325
Iteration 21/1000 | Loss: 0.00002310
Iteration 22/1000 | Loss: 0.00002307
Iteration 23/1000 | Loss: 0.00002307
Iteration 24/1000 | Loss: 0.00002300
Iteration 25/1000 | Loss: 0.00002297
Iteration 26/1000 | Loss: 0.00002297
Iteration 27/1000 | Loss: 0.00002296
Iteration 28/1000 | Loss: 0.00002295
Iteration 29/1000 | Loss: 0.00002295
Iteration 30/1000 | Loss: 0.00002294
Iteration 31/1000 | Loss: 0.00002293
Iteration 32/1000 | Loss: 0.00002293
Iteration 33/1000 | Loss: 0.00002292
Iteration 34/1000 | Loss: 0.00002292
Iteration 35/1000 | Loss: 0.00002291
Iteration 36/1000 | Loss: 0.00002289
Iteration 37/1000 | Loss: 0.00002289
Iteration 38/1000 | Loss: 0.00002289
Iteration 39/1000 | Loss: 0.00002289
Iteration 40/1000 | Loss: 0.00002288
Iteration 41/1000 | Loss: 0.00002288
Iteration 42/1000 | Loss: 0.00002288
Iteration 43/1000 | Loss: 0.00002288
Iteration 44/1000 | Loss: 0.00002288
Iteration 45/1000 | Loss: 0.00002288
Iteration 46/1000 | Loss: 0.00002288
Iteration 47/1000 | Loss: 0.00002288
Iteration 48/1000 | Loss: 0.00002285
Iteration 49/1000 | Loss: 0.00002284
Iteration 50/1000 | Loss: 0.00002284
Iteration 51/1000 | Loss: 0.00002283
Iteration 52/1000 | Loss: 0.00002283
Iteration 53/1000 | Loss: 0.00002282
Iteration 54/1000 | Loss: 0.00002282
Iteration 55/1000 | Loss: 0.00002282
Iteration 56/1000 | Loss: 0.00002281
Iteration 57/1000 | Loss: 0.00002280
Iteration 58/1000 | Loss: 0.00002280
Iteration 59/1000 | Loss: 0.00002280
Iteration 60/1000 | Loss: 0.00002280
Iteration 61/1000 | Loss: 0.00002280
Iteration 62/1000 | Loss: 0.00002280
Iteration 63/1000 | Loss: 0.00002279
Iteration 64/1000 | Loss: 0.00002279
Iteration 65/1000 | Loss: 0.00002279
Iteration 66/1000 | Loss: 0.00002278
Iteration 67/1000 | Loss: 0.00002278
Iteration 68/1000 | Loss: 0.00002278
Iteration 69/1000 | Loss: 0.00002277
Iteration 70/1000 | Loss: 0.00002277
Iteration 71/1000 | Loss: 0.00002277
Iteration 72/1000 | Loss: 0.00002276
Iteration 73/1000 | Loss: 0.00002276
Iteration 74/1000 | Loss: 0.00002276
Iteration 75/1000 | Loss: 0.00002275
Iteration 76/1000 | Loss: 0.00002275
Iteration 77/1000 | Loss: 0.00002275
Iteration 78/1000 | Loss: 0.00002275
Iteration 79/1000 | Loss: 0.00002275
Iteration 80/1000 | Loss: 0.00002275
Iteration 81/1000 | Loss: 0.00002275
Iteration 82/1000 | Loss: 0.00002275
Iteration 83/1000 | Loss: 0.00002275
Iteration 84/1000 | Loss: 0.00002275
Iteration 85/1000 | Loss: 0.00002275
Iteration 86/1000 | Loss: 0.00002274
Iteration 87/1000 | Loss: 0.00002274
Iteration 88/1000 | Loss: 0.00002274
Iteration 89/1000 | Loss: 0.00002273
Iteration 90/1000 | Loss: 0.00002273
Iteration 91/1000 | Loss: 0.00002273
Iteration 92/1000 | Loss: 0.00002272
Iteration 93/1000 | Loss: 0.00002272
Iteration 94/1000 | Loss: 0.00002272
Iteration 95/1000 | Loss: 0.00002272
Iteration 96/1000 | Loss: 0.00002272
Iteration 97/1000 | Loss: 0.00002272
Iteration 98/1000 | Loss: 0.00002272
Iteration 99/1000 | Loss: 0.00002272
Iteration 100/1000 | Loss: 0.00002272
Iteration 101/1000 | Loss: 0.00002271
Iteration 102/1000 | Loss: 0.00002271
Iteration 103/1000 | Loss: 0.00002271
Iteration 104/1000 | Loss: 0.00002271
Iteration 105/1000 | Loss: 0.00002271
Iteration 106/1000 | Loss: 0.00002270
Iteration 107/1000 | Loss: 0.00002270
Iteration 108/1000 | Loss: 0.00002270
Iteration 109/1000 | Loss: 0.00002270
Iteration 110/1000 | Loss: 0.00002269
Iteration 111/1000 | Loss: 0.00002269
Iteration 112/1000 | Loss: 0.00002269
Iteration 113/1000 | Loss: 0.00002269
Iteration 114/1000 | Loss: 0.00002269
Iteration 115/1000 | Loss: 0.00002269
Iteration 116/1000 | Loss: 0.00002269
Iteration 117/1000 | Loss: 0.00002269
Iteration 118/1000 | Loss: 0.00002269
Iteration 119/1000 | Loss: 0.00002268
Iteration 120/1000 | Loss: 0.00002268
Iteration 121/1000 | Loss: 0.00002268
Iteration 122/1000 | Loss: 0.00002268
Iteration 123/1000 | Loss: 0.00002268
Iteration 124/1000 | Loss: 0.00002268
Iteration 125/1000 | Loss: 0.00002268
Iteration 126/1000 | Loss: 0.00002268
Iteration 127/1000 | Loss: 0.00002268
Iteration 128/1000 | Loss: 0.00002267
Iteration 129/1000 | Loss: 0.00002267
Iteration 130/1000 | Loss: 0.00002267
Iteration 131/1000 | Loss: 0.00002267
Iteration 132/1000 | Loss: 0.00002267
Iteration 133/1000 | Loss: 0.00002267
Iteration 134/1000 | Loss: 0.00002267
Iteration 135/1000 | Loss: 0.00002267
Iteration 136/1000 | Loss: 0.00002267
Iteration 137/1000 | Loss: 0.00002267
Iteration 138/1000 | Loss: 0.00002266
Iteration 139/1000 | Loss: 0.00002266
Iteration 140/1000 | Loss: 0.00002266
Iteration 141/1000 | Loss: 0.00002266
Iteration 142/1000 | Loss: 0.00002266
Iteration 143/1000 | Loss: 0.00002266
Iteration 144/1000 | Loss: 0.00002266
Iteration 145/1000 | Loss: 0.00002266
Iteration 146/1000 | Loss: 0.00002266
Iteration 147/1000 | Loss: 0.00002266
Iteration 148/1000 | Loss: 0.00002265
Iteration 149/1000 | Loss: 0.00002265
Iteration 150/1000 | Loss: 0.00002265
Iteration 151/1000 | Loss: 0.00002265
Iteration 152/1000 | Loss: 0.00002265
Iteration 153/1000 | Loss: 0.00002265
Iteration 154/1000 | Loss: 0.00002265
Iteration 155/1000 | Loss: 0.00002265
Iteration 156/1000 | Loss: 0.00002265
Iteration 157/1000 | Loss: 0.00002265
Iteration 158/1000 | Loss: 0.00002264
Iteration 159/1000 | Loss: 0.00002264
Iteration 160/1000 | Loss: 0.00002264
Iteration 161/1000 | Loss: 0.00002264
Iteration 162/1000 | Loss: 0.00002264
Iteration 163/1000 | Loss: 0.00002264
Iteration 164/1000 | Loss: 0.00002264
Iteration 165/1000 | Loss: 0.00002264
Iteration 166/1000 | Loss: 0.00002264
Iteration 167/1000 | Loss: 0.00002264
Iteration 168/1000 | Loss: 0.00002264
Iteration 169/1000 | Loss: 0.00002264
Iteration 170/1000 | Loss: 0.00002264
Iteration 171/1000 | Loss: 0.00002264
Iteration 172/1000 | Loss: 0.00002264
Iteration 173/1000 | Loss: 0.00002264
Iteration 174/1000 | Loss: 0.00002264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.2640206225332804e-05, 2.2640206225332804e-05, 2.2640206225332804e-05, 2.2640206225332804e-05, 2.2640206225332804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2640206225332804e-05

Optimization complete. Final v2v error: 3.9678378105163574 mm

Highest mean error: 5.320773601531982 mm for frame 47

Lowest mean error: 3.528102397918701 mm for frame 150

Saving results

Total time: 47.27695345878601
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020320
Iteration 2/25 | Loss: 0.00184121
Iteration 3/25 | Loss: 0.00175275
Iteration 4/25 | Loss: 0.00159110
Iteration 5/25 | Loss: 0.00147443
Iteration 6/25 | Loss: 0.00146768
Iteration 7/25 | Loss: 0.00141140
Iteration 8/25 | Loss: 0.00140519
Iteration 9/25 | Loss: 0.00138702
Iteration 10/25 | Loss: 0.00137526
Iteration 11/25 | Loss: 0.00136787
Iteration 12/25 | Loss: 0.00136675
Iteration 13/25 | Loss: 0.00136475
Iteration 14/25 | Loss: 0.00136895
Iteration 15/25 | Loss: 0.00136616
Iteration 16/25 | Loss: 0.00136597
Iteration 17/25 | Loss: 0.00136753
Iteration 18/25 | Loss: 0.00136600
Iteration 19/25 | Loss: 0.00136552
Iteration 20/25 | Loss: 0.00136673
Iteration 21/25 | Loss: 0.00136947
Iteration 22/25 | Loss: 0.00136560
Iteration 23/25 | Loss: 0.00136727
Iteration 24/25 | Loss: 0.00136843
Iteration 25/25 | Loss: 0.00136529

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28188014
Iteration 2/25 | Loss: 0.00225230
Iteration 3/25 | Loss: 0.00225230
Iteration 4/25 | Loss: 0.00225230
Iteration 5/25 | Loss: 0.00225230
Iteration 6/25 | Loss: 0.00225230
Iteration 7/25 | Loss: 0.00225230
Iteration 8/25 | Loss: 0.00225230
Iteration 9/25 | Loss: 0.00225230
Iteration 10/25 | Loss: 0.00225230
Iteration 11/25 | Loss: 0.00225230
Iteration 12/25 | Loss: 0.00225230
Iteration 13/25 | Loss: 0.00225230
Iteration 14/25 | Loss: 0.00225230
Iteration 15/25 | Loss: 0.00225230
Iteration 16/25 | Loss: 0.00225230
Iteration 17/25 | Loss: 0.00225230
Iteration 18/25 | Loss: 0.00225230
Iteration 19/25 | Loss: 0.00225230
Iteration 20/25 | Loss: 0.00225230
Iteration 21/25 | Loss: 0.00225230
Iteration 22/25 | Loss: 0.00225230
Iteration 23/25 | Loss: 0.00225230
Iteration 24/25 | Loss: 0.00225230
Iteration 25/25 | Loss: 0.00225229

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225229
Iteration 2/1000 | Loss: 0.00015839
Iteration 3/1000 | Loss: 0.00027441
Iteration 4/1000 | Loss: 0.00034132
Iteration 5/1000 | Loss: 0.00002892
Iteration 6/1000 | Loss: 0.00002927
Iteration 7/1000 | Loss: 0.00002866
Iteration 8/1000 | Loss: 0.00002046
Iteration 9/1000 | Loss: 0.00002700
Iteration 10/1000 | Loss: 0.00001693
Iteration 11/1000 | Loss: 0.00001641
Iteration 12/1000 | Loss: 0.00003227
Iteration 13/1000 | Loss: 0.00001580
Iteration 14/1000 | Loss: 0.00001535
Iteration 15/1000 | Loss: 0.00005203
Iteration 16/1000 | Loss: 0.00001485
Iteration 17/1000 | Loss: 0.00001480
Iteration 18/1000 | Loss: 0.00004714
Iteration 19/1000 | Loss: 0.00004356
Iteration 20/1000 | Loss: 0.00013559
Iteration 21/1000 | Loss: 0.00093787
Iteration 22/1000 | Loss: 0.00013369
Iteration 23/1000 | Loss: 0.00007400
Iteration 24/1000 | Loss: 0.00001456
Iteration 25/1000 | Loss: 0.00001454
Iteration 26/1000 | Loss: 0.00001448
Iteration 27/1000 | Loss: 0.00001448
Iteration 28/1000 | Loss: 0.00001448
Iteration 29/1000 | Loss: 0.00001448
Iteration 30/1000 | Loss: 0.00001448
Iteration 31/1000 | Loss: 0.00001448
Iteration 32/1000 | Loss: 0.00001448
Iteration 33/1000 | Loss: 0.00001448
Iteration 34/1000 | Loss: 0.00001448
Iteration 35/1000 | Loss: 0.00001448
Iteration 36/1000 | Loss: 0.00001448
Iteration 37/1000 | Loss: 0.00001447
Iteration 38/1000 | Loss: 0.00001447
Iteration 39/1000 | Loss: 0.00001447
Iteration 40/1000 | Loss: 0.00001445
Iteration 41/1000 | Loss: 0.00001445
Iteration 42/1000 | Loss: 0.00001442
Iteration 43/1000 | Loss: 0.00001441
Iteration 44/1000 | Loss: 0.00001438
Iteration 45/1000 | Loss: 0.00001437
Iteration 46/1000 | Loss: 0.00001436
Iteration 47/1000 | Loss: 0.00001436
Iteration 48/1000 | Loss: 0.00001436
Iteration 49/1000 | Loss: 0.00001435
Iteration 50/1000 | Loss: 0.00001435
Iteration 51/1000 | Loss: 0.00001435
Iteration 52/1000 | Loss: 0.00001434
Iteration 53/1000 | Loss: 0.00001434
Iteration 54/1000 | Loss: 0.00001433
Iteration 55/1000 | Loss: 0.00001432
Iteration 56/1000 | Loss: 0.00001431
Iteration 57/1000 | Loss: 0.00001430
Iteration 58/1000 | Loss: 0.00004066
Iteration 59/1000 | Loss: 0.00001638
Iteration 60/1000 | Loss: 0.00001480
Iteration 61/1000 | Loss: 0.00001426
Iteration 62/1000 | Loss: 0.00006421
Iteration 63/1000 | Loss: 0.00001714
Iteration 64/1000 | Loss: 0.00002085
Iteration 65/1000 | Loss: 0.00001422
Iteration 66/1000 | Loss: 0.00001422
Iteration 67/1000 | Loss: 0.00001422
Iteration 68/1000 | Loss: 0.00001422
Iteration 69/1000 | Loss: 0.00001422
Iteration 70/1000 | Loss: 0.00001422
Iteration 71/1000 | Loss: 0.00001422
Iteration 72/1000 | Loss: 0.00001421
Iteration 73/1000 | Loss: 0.00001421
Iteration 74/1000 | Loss: 0.00001421
Iteration 75/1000 | Loss: 0.00001420
Iteration 76/1000 | Loss: 0.00001420
Iteration 77/1000 | Loss: 0.00001419
Iteration 78/1000 | Loss: 0.00001633
Iteration 79/1000 | Loss: 0.00002487
Iteration 80/1000 | Loss: 0.00007010
Iteration 81/1000 | Loss: 0.00001816
Iteration 82/1000 | Loss: 0.00001416
Iteration 83/1000 | Loss: 0.00001416
Iteration 84/1000 | Loss: 0.00001415
Iteration 85/1000 | Loss: 0.00001415
Iteration 86/1000 | Loss: 0.00001415
Iteration 87/1000 | Loss: 0.00001415
Iteration 88/1000 | Loss: 0.00001415
Iteration 89/1000 | Loss: 0.00001415
Iteration 90/1000 | Loss: 0.00001415
Iteration 91/1000 | Loss: 0.00001415
Iteration 92/1000 | Loss: 0.00001415
Iteration 93/1000 | Loss: 0.00001415
Iteration 94/1000 | Loss: 0.00001414
Iteration 95/1000 | Loss: 0.00001414
Iteration 96/1000 | Loss: 0.00001414
Iteration 97/1000 | Loss: 0.00001414
Iteration 98/1000 | Loss: 0.00001414
Iteration 99/1000 | Loss: 0.00001413
Iteration 100/1000 | Loss: 0.00001413
Iteration 101/1000 | Loss: 0.00001413
Iteration 102/1000 | Loss: 0.00002498
Iteration 103/1000 | Loss: 0.00001753
Iteration 104/1000 | Loss: 0.00001640
Iteration 105/1000 | Loss: 0.00001715
Iteration 106/1000 | Loss: 0.00005045
Iteration 107/1000 | Loss: 0.00001686
Iteration 108/1000 | Loss: 0.00002272
Iteration 109/1000 | Loss: 0.00001406
Iteration 110/1000 | Loss: 0.00001406
Iteration 111/1000 | Loss: 0.00001406
Iteration 112/1000 | Loss: 0.00001405
Iteration 113/1000 | Loss: 0.00001405
Iteration 114/1000 | Loss: 0.00001405
Iteration 115/1000 | Loss: 0.00001405
Iteration 116/1000 | Loss: 0.00001405
Iteration 117/1000 | Loss: 0.00001405
Iteration 118/1000 | Loss: 0.00001405
Iteration 119/1000 | Loss: 0.00001405
Iteration 120/1000 | Loss: 0.00001405
Iteration 121/1000 | Loss: 0.00001404
Iteration 122/1000 | Loss: 0.00001404
Iteration 123/1000 | Loss: 0.00001404
Iteration 124/1000 | Loss: 0.00001404
Iteration 125/1000 | Loss: 0.00001404
Iteration 126/1000 | Loss: 0.00001404
Iteration 127/1000 | Loss: 0.00001403
Iteration 128/1000 | Loss: 0.00001402
Iteration 129/1000 | Loss: 0.00001437
Iteration 130/1000 | Loss: 0.00001400
Iteration 131/1000 | Loss: 0.00001400
Iteration 132/1000 | Loss: 0.00001400
Iteration 133/1000 | Loss: 0.00001400
Iteration 134/1000 | Loss: 0.00001400
Iteration 135/1000 | Loss: 0.00001400
Iteration 136/1000 | Loss: 0.00001400
Iteration 137/1000 | Loss: 0.00001400
Iteration 138/1000 | Loss: 0.00001400
Iteration 139/1000 | Loss: 0.00001400
Iteration 140/1000 | Loss: 0.00001400
Iteration 141/1000 | Loss: 0.00001400
Iteration 142/1000 | Loss: 0.00001400
Iteration 143/1000 | Loss: 0.00001400
Iteration 144/1000 | Loss: 0.00001400
Iteration 145/1000 | Loss: 0.00001400
Iteration 146/1000 | Loss: 0.00001400
Iteration 147/1000 | Loss: 0.00001400
Iteration 148/1000 | Loss: 0.00001400
Iteration 149/1000 | Loss: 0.00001400
Iteration 150/1000 | Loss: 0.00001400
Iteration 151/1000 | Loss: 0.00001400
Iteration 152/1000 | Loss: 0.00001400
Iteration 153/1000 | Loss: 0.00001400
Iteration 154/1000 | Loss: 0.00001400
Iteration 155/1000 | Loss: 0.00001400
Iteration 156/1000 | Loss: 0.00001400
Iteration 157/1000 | Loss: 0.00001400
Iteration 158/1000 | Loss: 0.00001400
Iteration 159/1000 | Loss: 0.00001400
Iteration 160/1000 | Loss: 0.00001400
Iteration 161/1000 | Loss: 0.00001400
Iteration 162/1000 | Loss: 0.00001400
Iteration 163/1000 | Loss: 0.00001400
Iteration 164/1000 | Loss: 0.00001400
Iteration 165/1000 | Loss: 0.00001400
Iteration 166/1000 | Loss: 0.00001400
Iteration 167/1000 | Loss: 0.00001400
Iteration 168/1000 | Loss: 0.00001400
Iteration 169/1000 | Loss: 0.00001400
Iteration 170/1000 | Loss: 0.00001400
Iteration 171/1000 | Loss: 0.00001400
Iteration 172/1000 | Loss: 0.00001400
Iteration 173/1000 | Loss: 0.00001400
Iteration 174/1000 | Loss: 0.00001400
Iteration 175/1000 | Loss: 0.00001400
Iteration 176/1000 | Loss: 0.00001400
Iteration 177/1000 | Loss: 0.00001400
Iteration 178/1000 | Loss: 0.00001400
Iteration 179/1000 | Loss: 0.00001400
Iteration 180/1000 | Loss: 0.00001400
Iteration 181/1000 | Loss: 0.00001400
Iteration 182/1000 | Loss: 0.00001400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.3997141650179401e-05, 1.3997141650179401e-05, 1.3997141650179401e-05, 1.3997141650179401e-05, 1.3997141650179401e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3997141650179401e-05

Optimization complete. Final v2v error: 3.236746072769165 mm

Highest mean error: 4.17673921585083 mm for frame 83

Lowest mean error: 3.0011801719665527 mm for frame 134

Saving results

Total time: 108.01226544380188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00671089
Iteration 2/25 | Loss: 0.00140483
Iteration 3/25 | Loss: 0.00134730
Iteration 4/25 | Loss: 0.00133890
Iteration 5/25 | Loss: 0.00133597
Iteration 6/25 | Loss: 0.00133524
Iteration 7/25 | Loss: 0.00133523
Iteration 8/25 | Loss: 0.00133523
Iteration 9/25 | Loss: 0.00133523
Iteration 10/25 | Loss: 0.00133523
Iteration 11/25 | Loss: 0.00133523
Iteration 12/25 | Loss: 0.00133523
Iteration 13/25 | Loss: 0.00133523
Iteration 14/25 | Loss: 0.00133523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013352320529520512, 0.0013352320529520512, 0.0013352320529520512, 0.0013352320529520512, 0.0013352320529520512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013352320529520512

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34279406
Iteration 2/25 | Loss: 0.00181901
Iteration 3/25 | Loss: 0.00181901
Iteration 4/25 | Loss: 0.00181901
Iteration 5/25 | Loss: 0.00181901
Iteration 6/25 | Loss: 0.00181901
Iteration 7/25 | Loss: 0.00181901
Iteration 8/25 | Loss: 0.00181901
Iteration 9/25 | Loss: 0.00181901
Iteration 10/25 | Loss: 0.00181901
Iteration 11/25 | Loss: 0.00181901
Iteration 12/25 | Loss: 0.00181901
Iteration 13/25 | Loss: 0.00181901
Iteration 14/25 | Loss: 0.00181901
Iteration 15/25 | Loss: 0.00181901
Iteration 16/25 | Loss: 0.00181901
Iteration 17/25 | Loss: 0.00181901
Iteration 18/25 | Loss: 0.00181901
Iteration 19/25 | Loss: 0.00181901
Iteration 20/25 | Loss: 0.00181901
Iteration 21/25 | Loss: 0.00181901
Iteration 22/25 | Loss: 0.00181901
Iteration 23/25 | Loss: 0.00181901
Iteration 24/25 | Loss: 0.00181901
Iteration 25/25 | Loss: 0.00181901

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181901
Iteration 2/1000 | Loss: 0.00002358
Iteration 3/1000 | Loss: 0.00001673
Iteration 4/1000 | Loss: 0.00001510
Iteration 5/1000 | Loss: 0.00001423
Iteration 6/1000 | Loss: 0.00001381
Iteration 7/1000 | Loss: 0.00001321
Iteration 8/1000 | Loss: 0.00001285
Iteration 9/1000 | Loss: 0.00001256
Iteration 10/1000 | Loss: 0.00001227
Iteration 11/1000 | Loss: 0.00001209
Iteration 12/1000 | Loss: 0.00001189
Iteration 13/1000 | Loss: 0.00001187
Iteration 14/1000 | Loss: 0.00001186
Iteration 15/1000 | Loss: 0.00001185
Iteration 16/1000 | Loss: 0.00001185
Iteration 17/1000 | Loss: 0.00001173
Iteration 18/1000 | Loss: 0.00001166
Iteration 19/1000 | Loss: 0.00001161
Iteration 20/1000 | Loss: 0.00001157
Iteration 21/1000 | Loss: 0.00001151
Iteration 22/1000 | Loss: 0.00001151
Iteration 23/1000 | Loss: 0.00001145
Iteration 24/1000 | Loss: 0.00001145
Iteration 25/1000 | Loss: 0.00001143
Iteration 26/1000 | Loss: 0.00001140
Iteration 27/1000 | Loss: 0.00001140
Iteration 28/1000 | Loss: 0.00001139
Iteration 29/1000 | Loss: 0.00001139
Iteration 30/1000 | Loss: 0.00001139
Iteration 31/1000 | Loss: 0.00001139
Iteration 32/1000 | Loss: 0.00001139
Iteration 33/1000 | Loss: 0.00001139
Iteration 34/1000 | Loss: 0.00001139
Iteration 35/1000 | Loss: 0.00001139
Iteration 36/1000 | Loss: 0.00001139
Iteration 37/1000 | Loss: 0.00001139
Iteration 38/1000 | Loss: 0.00001139
Iteration 39/1000 | Loss: 0.00001138
Iteration 40/1000 | Loss: 0.00001138
Iteration 41/1000 | Loss: 0.00001138
Iteration 42/1000 | Loss: 0.00001137
Iteration 43/1000 | Loss: 0.00001137
Iteration 44/1000 | Loss: 0.00001135
Iteration 45/1000 | Loss: 0.00001135
Iteration 46/1000 | Loss: 0.00001135
Iteration 47/1000 | Loss: 0.00001134
Iteration 48/1000 | Loss: 0.00001134
Iteration 49/1000 | Loss: 0.00001134
Iteration 50/1000 | Loss: 0.00001134
Iteration 51/1000 | Loss: 0.00001133
Iteration 52/1000 | Loss: 0.00001132
Iteration 53/1000 | Loss: 0.00001131
Iteration 54/1000 | Loss: 0.00001131
Iteration 55/1000 | Loss: 0.00001130
Iteration 56/1000 | Loss: 0.00001130
Iteration 57/1000 | Loss: 0.00001126
Iteration 58/1000 | Loss: 0.00001125
Iteration 59/1000 | Loss: 0.00001124
Iteration 60/1000 | Loss: 0.00001124
Iteration 61/1000 | Loss: 0.00001123
Iteration 62/1000 | Loss: 0.00001122
Iteration 63/1000 | Loss: 0.00001122
Iteration 64/1000 | Loss: 0.00001122
Iteration 65/1000 | Loss: 0.00001122
Iteration 66/1000 | Loss: 0.00001122
Iteration 67/1000 | Loss: 0.00001122
Iteration 68/1000 | Loss: 0.00001121
Iteration 69/1000 | Loss: 0.00001121
Iteration 70/1000 | Loss: 0.00001120
Iteration 71/1000 | Loss: 0.00001120
Iteration 72/1000 | Loss: 0.00001120
Iteration 73/1000 | Loss: 0.00001120
Iteration 74/1000 | Loss: 0.00001120
Iteration 75/1000 | Loss: 0.00001120
Iteration 76/1000 | Loss: 0.00001119
Iteration 77/1000 | Loss: 0.00001119
Iteration 78/1000 | Loss: 0.00001119
Iteration 79/1000 | Loss: 0.00001119
Iteration 80/1000 | Loss: 0.00001119
Iteration 81/1000 | Loss: 0.00001119
Iteration 82/1000 | Loss: 0.00001118
Iteration 83/1000 | Loss: 0.00001118
Iteration 84/1000 | Loss: 0.00001118
Iteration 85/1000 | Loss: 0.00001117
Iteration 86/1000 | Loss: 0.00001117
Iteration 87/1000 | Loss: 0.00001117
Iteration 88/1000 | Loss: 0.00001117
Iteration 89/1000 | Loss: 0.00001117
Iteration 90/1000 | Loss: 0.00001117
Iteration 91/1000 | Loss: 0.00001116
Iteration 92/1000 | Loss: 0.00001116
Iteration 93/1000 | Loss: 0.00001116
Iteration 94/1000 | Loss: 0.00001116
Iteration 95/1000 | Loss: 0.00001116
Iteration 96/1000 | Loss: 0.00001115
Iteration 97/1000 | Loss: 0.00001115
Iteration 98/1000 | Loss: 0.00001115
Iteration 99/1000 | Loss: 0.00001115
Iteration 100/1000 | Loss: 0.00001115
Iteration 101/1000 | Loss: 0.00001115
Iteration 102/1000 | Loss: 0.00001115
Iteration 103/1000 | Loss: 0.00001115
Iteration 104/1000 | Loss: 0.00001115
Iteration 105/1000 | Loss: 0.00001115
Iteration 106/1000 | Loss: 0.00001115
Iteration 107/1000 | Loss: 0.00001115
Iteration 108/1000 | Loss: 0.00001115
Iteration 109/1000 | Loss: 0.00001115
Iteration 110/1000 | Loss: 0.00001115
Iteration 111/1000 | Loss: 0.00001115
Iteration 112/1000 | Loss: 0.00001115
Iteration 113/1000 | Loss: 0.00001115
Iteration 114/1000 | Loss: 0.00001115
Iteration 115/1000 | Loss: 0.00001115
Iteration 116/1000 | Loss: 0.00001115
Iteration 117/1000 | Loss: 0.00001115
Iteration 118/1000 | Loss: 0.00001115
Iteration 119/1000 | Loss: 0.00001115
Iteration 120/1000 | Loss: 0.00001115
Iteration 121/1000 | Loss: 0.00001115
Iteration 122/1000 | Loss: 0.00001115
Iteration 123/1000 | Loss: 0.00001115
Iteration 124/1000 | Loss: 0.00001115
Iteration 125/1000 | Loss: 0.00001115
Iteration 126/1000 | Loss: 0.00001115
Iteration 127/1000 | Loss: 0.00001115
Iteration 128/1000 | Loss: 0.00001115
Iteration 129/1000 | Loss: 0.00001115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.1154466847074218e-05, 1.1154466847074218e-05, 1.1154466847074218e-05, 1.1154466847074218e-05, 1.1154466847074218e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1154466847074218e-05

Optimization complete. Final v2v error: 2.8665108680725098 mm

Highest mean error: 3.5516867637634277 mm for frame 72

Lowest mean error: 2.6310086250305176 mm for frame 0

Saving results

Total time: 36.952943325042725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035275
Iteration 2/25 | Loss: 0.01035274
Iteration 3/25 | Loss: 0.01035274
Iteration 4/25 | Loss: 0.01035274
Iteration 5/25 | Loss: 0.01035273
Iteration 6/25 | Loss: 0.01035273
Iteration 7/25 | Loss: 0.01035273
Iteration 8/25 | Loss: 0.01035273
Iteration 9/25 | Loss: 0.01035273
Iteration 10/25 | Loss: 0.01035272
Iteration 11/25 | Loss: 0.01035272
Iteration 12/25 | Loss: 0.01035272
Iteration 13/25 | Loss: 0.01035272
Iteration 14/25 | Loss: 0.01035272
Iteration 15/25 | Loss: 0.01035272
Iteration 16/25 | Loss: 0.01035271
Iteration 17/25 | Loss: 0.01035271
Iteration 18/25 | Loss: 0.01035271
Iteration 19/25 | Loss: 0.01035270
Iteration 20/25 | Loss: 0.01035270
Iteration 21/25 | Loss: 0.01035270
Iteration 22/25 | Loss: 0.01035270
Iteration 23/25 | Loss: 0.01035269
Iteration 24/25 | Loss: 0.01035269
Iteration 25/25 | Loss: 0.01035269

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62982988
Iteration 2/25 | Loss: 0.08389042
Iteration 3/25 | Loss: 0.08339074
Iteration 4/25 | Loss: 0.08343790
Iteration 5/25 | Loss: 0.08332876
Iteration 6/25 | Loss: 0.08332876
Iteration 7/25 | Loss: 0.08332875
Iteration 8/25 | Loss: 0.08332875
Iteration 9/25 | Loss: 0.08332874
Iteration 10/25 | Loss: 0.08332875
Iteration 11/25 | Loss: 0.08332875
Iteration 12/25 | Loss: 0.08332874
Iteration 13/25 | Loss: 0.08332874
Iteration 14/25 | Loss: 0.08332874
Iteration 15/25 | Loss: 0.08332874
Iteration 16/25 | Loss: 0.08332874
Iteration 17/25 | Loss: 0.08332874
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0833287388086319, 0.0833287388086319, 0.0833287388086319, 0.0833287388086319, 0.0833287388086319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0833287388086319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08332874
Iteration 2/1000 | Loss: 0.00264430
Iteration 3/1000 | Loss: 0.00122153
Iteration 4/1000 | Loss: 0.00039012
Iteration 5/1000 | Loss: 0.00118768
Iteration 6/1000 | Loss: 0.00051624
Iteration 7/1000 | Loss: 0.00039942
Iteration 8/1000 | Loss: 0.00023001
Iteration 9/1000 | Loss: 0.00046663
Iteration 10/1000 | Loss: 0.00007490
Iteration 11/1000 | Loss: 0.00005008
Iteration 12/1000 | Loss: 0.00004528
Iteration 13/1000 | Loss: 0.00007747
Iteration 14/1000 | Loss: 0.00003629
Iteration 15/1000 | Loss: 0.00012999
Iteration 16/1000 | Loss: 0.00003846
Iteration 17/1000 | Loss: 0.00003020
Iteration 18/1000 | Loss: 0.00002487
Iteration 19/1000 | Loss: 0.00003884
Iteration 20/1000 | Loss: 0.00002105
Iteration 21/1000 | Loss: 0.00002843
Iteration 22/1000 | Loss: 0.00002123
Iteration 23/1000 | Loss: 0.00002214
Iteration 24/1000 | Loss: 0.00001887
Iteration 25/1000 | Loss: 0.00002309
Iteration 26/1000 | Loss: 0.00001603
Iteration 27/1000 | Loss: 0.00003786
Iteration 28/1000 | Loss: 0.00001518
Iteration 29/1000 | Loss: 0.00001512
Iteration 30/1000 | Loss: 0.00002902
Iteration 31/1000 | Loss: 0.00001452
Iteration 32/1000 | Loss: 0.00001452
Iteration 33/1000 | Loss: 0.00001430
Iteration 34/1000 | Loss: 0.00002828
Iteration 35/1000 | Loss: 0.00001391
Iteration 36/1000 | Loss: 0.00003494
Iteration 37/1000 | Loss: 0.00006170
Iteration 38/1000 | Loss: 0.00003529
Iteration 39/1000 | Loss: 0.00006118
Iteration 40/1000 | Loss: 0.00003273
Iteration 41/1000 | Loss: 0.00001357
Iteration 42/1000 | Loss: 0.00001350
Iteration 43/1000 | Loss: 0.00002066
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001344
Iteration 46/1000 | Loss: 0.00001344
Iteration 47/1000 | Loss: 0.00001343
Iteration 48/1000 | Loss: 0.00001343
Iteration 49/1000 | Loss: 0.00001343
Iteration 50/1000 | Loss: 0.00001343
Iteration 51/1000 | Loss: 0.00001343
Iteration 52/1000 | Loss: 0.00001343
Iteration 53/1000 | Loss: 0.00001343
Iteration 54/1000 | Loss: 0.00001342
Iteration 55/1000 | Loss: 0.00001342
Iteration 56/1000 | Loss: 0.00001342
Iteration 57/1000 | Loss: 0.00001342
Iteration 58/1000 | Loss: 0.00001342
Iteration 59/1000 | Loss: 0.00001342
Iteration 60/1000 | Loss: 0.00001342
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001341
Iteration 64/1000 | Loss: 0.00001340
Iteration 65/1000 | Loss: 0.00001339
Iteration 66/1000 | Loss: 0.00001338
Iteration 67/1000 | Loss: 0.00001338
Iteration 68/1000 | Loss: 0.00001338
Iteration 69/1000 | Loss: 0.00001337
Iteration 70/1000 | Loss: 0.00001336
Iteration 71/1000 | Loss: 0.00001336
Iteration 72/1000 | Loss: 0.00001336
Iteration 73/1000 | Loss: 0.00001335
Iteration 74/1000 | Loss: 0.00001335
Iteration 75/1000 | Loss: 0.00001333
Iteration 76/1000 | Loss: 0.00001333
Iteration 77/1000 | Loss: 0.00001330
Iteration 78/1000 | Loss: 0.00001330
Iteration 79/1000 | Loss: 0.00001330
Iteration 80/1000 | Loss: 0.00004256
Iteration 81/1000 | Loss: 0.00001926
Iteration 82/1000 | Loss: 0.00001326
Iteration 83/1000 | Loss: 0.00001325
Iteration 84/1000 | Loss: 0.00001325
Iteration 85/1000 | Loss: 0.00001325
Iteration 86/1000 | Loss: 0.00001324
Iteration 87/1000 | Loss: 0.00001324
Iteration 88/1000 | Loss: 0.00001324
Iteration 89/1000 | Loss: 0.00001324
Iteration 90/1000 | Loss: 0.00001324
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001324
Iteration 94/1000 | Loss: 0.00001324
Iteration 95/1000 | Loss: 0.00001324
Iteration 96/1000 | Loss: 0.00001324
Iteration 97/1000 | Loss: 0.00001324
Iteration 98/1000 | Loss: 0.00001323
Iteration 99/1000 | Loss: 0.00001323
Iteration 100/1000 | Loss: 0.00001323
Iteration 101/1000 | Loss: 0.00001323
Iteration 102/1000 | Loss: 0.00001323
Iteration 103/1000 | Loss: 0.00001323
Iteration 104/1000 | Loss: 0.00001322
Iteration 105/1000 | Loss: 0.00001322
Iteration 106/1000 | Loss: 0.00001322
Iteration 107/1000 | Loss: 0.00001321
Iteration 108/1000 | Loss: 0.00001321
Iteration 109/1000 | Loss: 0.00001320
Iteration 110/1000 | Loss: 0.00001320
Iteration 111/1000 | Loss: 0.00001320
Iteration 112/1000 | Loss: 0.00001320
Iteration 113/1000 | Loss: 0.00001320
Iteration 114/1000 | Loss: 0.00001320
Iteration 115/1000 | Loss: 0.00001320
Iteration 116/1000 | Loss: 0.00001320
Iteration 117/1000 | Loss: 0.00003852
Iteration 118/1000 | Loss: 0.00001434
Iteration 119/1000 | Loss: 0.00001354
Iteration 120/1000 | Loss: 0.00001318
Iteration 121/1000 | Loss: 0.00001318
Iteration 122/1000 | Loss: 0.00001318
Iteration 123/1000 | Loss: 0.00001317
Iteration 124/1000 | Loss: 0.00001317
Iteration 125/1000 | Loss: 0.00001317
Iteration 126/1000 | Loss: 0.00001316
Iteration 127/1000 | Loss: 0.00001316
Iteration 128/1000 | Loss: 0.00001315
Iteration 129/1000 | Loss: 0.00001315
Iteration 130/1000 | Loss: 0.00001315
Iteration 131/1000 | Loss: 0.00001314
Iteration 132/1000 | Loss: 0.00001314
Iteration 133/1000 | Loss: 0.00001314
Iteration 134/1000 | Loss: 0.00001314
Iteration 135/1000 | Loss: 0.00001314
Iteration 136/1000 | Loss: 0.00001313
Iteration 137/1000 | Loss: 0.00001313
Iteration 138/1000 | Loss: 0.00001313
Iteration 139/1000 | Loss: 0.00001312
Iteration 140/1000 | Loss: 0.00001312
Iteration 141/1000 | Loss: 0.00001312
Iteration 142/1000 | Loss: 0.00001312
Iteration 143/1000 | Loss: 0.00001312
Iteration 144/1000 | Loss: 0.00001312
Iteration 145/1000 | Loss: 0.00001311
Iteration 146/1000 | Loss: 0.00001311
Iteration 147/1000 | Loss: 0.00001311
Iteration 148/1000 | Loss: 0.00001311
Iteration 149/1000 | Loss: 0.00001311
Iteration 150/1000 | Loss: 0.00001311
Iteration 151/1000 | Loss: 0.00001311
Iteration 152/1000 | Loss: 0.00001309
Iteration 153/1000 | Loss: 0.00001309
Iteration 154/1000 | Loss: 0.00001309
Iteration 155/1000 | Loss: 0.00001309
Iteration 156/1000 | Loss: 0.00001309
Iteration 157/1000 | Loss: 0.00001309
Iteration 158/1000 | Loss: 0.00001309
Iteration 159/1000 | Loss: 0.00001309
Iteration 160/1000 | Loss: 0.00001309
Iteration 161/1000 | Loss: 0.00001308
Iteration 162/1000 | Loss: 0.00001308
Iteration 163/1000 | Loss: 0.00001308
Iteration 164/1000 | Loss: 0.00001308
Iteration 165/1000 | Loss: 0.00003636
Iteration 166/1000 | Loss: 0.00001430
Iteration 167/1000 | Loss: 0.00001404
Iteration 168/1000 | Loss: 0.00001306
Iteration 169/1000 | Loss: 0.00001306
Iteration 170/1000 | Loss: 0.00001306
Iteration 171/1000 | Loss: 0.00001306
Iteration 172/1000 | Loss: 0.00001306
Iteration 173/1000 | Loss: 0.00001306
Iteration 174/1000 | Loss: 0.00001306
Iteration 175/1000 | Loss: 0.00001306
Iteration 176/1000 | Loss: 0.00001305
Iteration 177/1000 | Loss: 0.00001305
Iteration 178/1000 | Loss: 0.00001305
Iteration 179/1000 | Loss: 0.00001305
Iteration 180/1000 | Loss: 0.00001305
Iteration 181/1000 | Loss: 0.00001305
Iteration 182/1000 | Loss: 0.00001305
Iteration 183/1000 | Loss: 0.00001305
Iteration 184/1000 | Loss: 0.00001305
Iteration 185/1000 | Loss: 0.00001305
Iteration 186/1000 | Loss: 0.00001305
Iteration 187/1000 | Loss: 0.00001305
Iteration 188/1000 | Loss: 0.00001305
Iteration 189/1000 | Loss: 0.00001305
Iteration 190/1000 | Loss: 0.00001304
Iteration 191/1000 | Loss: 0.00001304
Iteration 192/1000 | Loss: 0.00001304
Iteration 193/1000 | Loss: 0.00001304
Iteration 194/1000 | Loss: 0.00001304
Iteration 195/1000 | Loss: 0.00001304
Iteration 196/1000 | Loss: 0.00001304
Iteration 197/1000 | Loss: 0.00001304
Iteration 198/1000 | Loss: 0.00001303
Iteration 199/1000 | Loss: 0.00001303
Iteration 200/1000 | Loss: 0.00001303
Iteration 201/1000 | Loss: 0.00001303
Iteration 202/1000 | Loss: 0.00001303
Iteration 203/1000 | Loss: 0.00001303
Iteration 204/1000 | Loss: 0.00001302
Iteration 205/1000 | Loss: 0.00001302
Iteration 206/1000 | Loss: 0.00001302
Iteration 207/1000 | Loss: 0.00001302
Iteration 208/1000 | Loss: 0.00001302
Iteration 209/1000 | Loss: 0.00001302
Iteration 210/1000 | Loss: 0.00001302
Iteration 211/1000 | Loss: 0.00001302
Iteration 212/1000 | Loss: 0.00001302
Iteration 213/1000 | Loss: 0.00001302
Iteration 214/1000 | Loss: 0.00001301
Iteration 215/1000 | Loss: 0.00001301
Iteration 216/1000 | Loss: 0.00001301
Iteration 217/1000 | Loss: 0.00001301
Iteration 218/1000 | Loss: 0.00001301
Iteration 219/1000 | Loss: 0.00001301
Iteration 220/1000 | Loss: 0.00001301
Iteration 221/1000 | Loss: 0.00001301
Iteration 222/1000 | Loss: 0.00001301
Iteration 223/1000 | Loss: 0.00001301
Iteration 224/1000 | Loss: 0.00001301
Iteration 225/1000 | Loss: 0.00001301
Iteration 226/1000 | Loss: 0.00001300
Iteration 227/1000 | Loss: 0.00001300
Iteration 228/1000 | Loss: 0.00001300
Iteration 229/1000 | Loss: 0.00001300
Iteration 230/1000 | Loss: 0.00001300
Iteration 231/1000 | Loss: 0.00001300
Iteration 232/1000 | Loss: 0.00001300
Iteration 233/1000 | Loss: 0.00001300
Iteration 234/1000 | Loss: 0.00001300
Iteration 235/1000 | Loss: 0.00001300
Iteration 236/1000 | Loss: 0.00001300
Iteration 237/1000 | Loss: 0.00001300
Iteration 238/1000 | Loss: 0.00001300
Iteration 239/1000 | Loss: 0.00001300
Iteration 240/1000 | Loss: 0.00001300
Iteration 241/1000 | Loss: 0.00001300
Iteration 242/1000 | Loss: 0.00001300
Iteration 243/1000 | Loss: 0.00001299
Iteration 244/1000 | Loss: 0.00001299
Iteration 245/1000 | Loss: 0.00001299
Iteration 246/1000 | Loss: 0.00001299
Iteration 247/1000 | Loss: 0.00001299
Iteration 248/1000 | Loss: 0.00001299
Iteration 249/1000 | Loss: 0.00001299
Iteration 250/1000 | Loss: 0.00001299
Iteration 251/1000 | Loss: 0.00001299
Iteration 252/1000 | Loss: 0.00001299
Iteration 253/1000 | Loss: 0.00001298
Iteration 254/1000 | Loss: 0.00001298
Iteration 255/1000 | Loss: 0.00001298
Iteration 256/1000 | Loss: 0.00001298
Iteration 257/1000 | Loss: 0.00001298
Iteration 258/1000 | Loss: 0.00001298
Iteration 259/1000 | Loss: 0.00001298
Iteration 260/1000 | Loss: 0.00001298
Iteration 261/1000 | Loss: 0.00001298
Iteration 262/1000 | Loss: 0.00001298
Iteration 263/1000 | Loss: 0.00001298
Iteration 264/1000 | Loss: 0.00001298
Iteration 265/1000 | Loss: 0.00001298
Iteration 266/1000 | Loss: 0.00001298
Iteration 267/1000 | Loss: 0.00001298
Iteration 268/1000 | Loss: 0.00001298
Iteration 269/1000 | Loss: 0.00001298
Iteration 270/1000 | Loss: 0.00001298
Iteration 271/1000 | Loss: 0.00001298
Iteration 272/1000 | Loss: 0.00001298
Iteration 273/1000 | Loss: 0.00001298
Iteration 274/1000 | Loss: 0.00001298
Iteration 275/1000 | Loss: 0.00001298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [1.2977261576452293e-05, 1.2977261576452293e-05, 1.2977261576452293e-05, 1.2977261576452293e-05, 1.2977261576452293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2977261576452293e-05

Optimization complete. Final v2v error: 3.094238758087158 mm

Highest mean error: 3.666679859161377 mm for frame 29

Lowest mean error: 2.735389232635498 mm for frame 114

Saving results

Total time: 105.17445492744446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01095030
Iteration 2/25 | Loss: 0.00213278
Iteration 3/25 | Loss: 0.00177274
Iteration 4/25 | Loss: 0.00160209
Iteration 5/25 | Loss: 0.00154134
Iteration 6/25 | Loss: 0.00153413
Iteration 7/25 | Loss: 0.00152592
Iteration 8/25 | Loss: 0.00152014
Iteration 9/25 | Loss: 0.00151651
Iteration 10/25 | Loss: 0.00151543
Iteration 11/25 | Loss: 0.00151381
Iteration 12/25 | Loss: 0.00151468
Iteration 13/25 | Loss: 0.00151288
Iteration 14/25 | Loss: 0.00151222
Iteration 15/25 | Loss: 0.00151211
Iteration 16/25 | Loss: 0.00151208
Iteration 17/25 | Loss: 0.00151208
Iteration 18/25 | Loss: 0.00151208
Iteration 19/25 | Loss: 0.00151208
Iteration 20/25 | Loss: 0.00151208
Iteration 21/25 | Loss: 0.00151208
Iteration 22/25 | Loss: 0.00151208
Iteration 23/25 | Loss: 0.00151208
Iteration 24/25 | Loss: 0.00151208
Iteration 25/25 | Loss: 0.00151207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79664677
Iteration 2/25 | Loss: 0.00167032
Iteration 3/25 | Loss: 0.00167032
Iteration 4/25 | Loss: 0.00167032
Iteration 5/25 | Loss: 0.00167032
Iteration 6/25 | Loss: 0.00167031
Iteration 7/25 | Loss: 0.00167031
Iteration 8/25 | Loss: 0.00167031
Iteration 9/25 | Loss: 0.00167031
Iteration 10/25 | Loss: 0.00167031
Iteration 11/25 | Loss: 0.00167031
Iteration 12/25 | Loss: 0.00167031
Iteration 13/25 | Loss: 0.00167031
Iteration 14/25 | Loss: 0.00167031
Iteration 15/25 | Loss: 0.00167031
Iteration 16/25 | Loss: 0.00167031
Iteration 17/25 | Loss: 0.00167031
Iteration 18/25 | Loss: 0.00167031
Iteration 19/25 | Loss: 0.00167031
Iteration 20/25 | Loss: 0.00167031
Iteration 21/25 | Loss: 0.00167031
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0016703124856576324, 0.0016703124856576324, 0.0016703124856576324, 0.0016703124856576324, 0.0016703124856576324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016703124856576324

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167031
Iteration 2/1000 | Loss: 0.00007340
Iteration 3/1000 | Loss: 0.00005644
Iteration 4/1000 | Loss: 0.00023486
Iteration 5/1000 | Loss: 0.00009783
Iteration 6/1000 | Loss: 0.00012008
Iteration 7/1000 | Loss: 0.00005430
Iteration 8/1000 | Loss: 0.00010404
Iteration 9/1000 | Loss: 0.00004528
Iteration 10/1000 | Loss: 0.00046464
Iteration 11/1000 | Loss: 0.00004989
Iteration 12/1000 | Loss: 0.00004358
Iteration 13/1000 | Loss: 0.00004159
Iteration 14/1000 | Loss: 0.00003973
Iteration 15/1000 | Loss: 0.00003893
Iteration 16/1000 | Loss: 0.00003835
Iteration 17/1000 | Loss: 0.00003800
Iteration 18/1000 | Loss: 0.00003761
Iteration 19/1000 | Loss: 0.00003725
Iteration 20/1000 | Loss: 0.00003697
Iteration 21/1000 | Loss: 0.00003677
Iteration 22/1000 | Loss: 0.00003657
Iteration 23/1000 | Loss: 0.00003637
Iteration 24/1000 | Loss: 0.00003621
Iteration 25/1000 | Loss: 0.00003616
Iteration 26/1000 | Loss: 0.00003615
Iteration 27/1000 | Loss: 0.00003602
Iteration 28/1000 | Loss: 0.00003599
Iteration 29/1000 | Loss: 0.00003589
Iteration 30/1000 | Loss: 0.00003581
Iteration 31/1000 | Loss: 0.00003574
Iteration 32/1000 | Loss: 0.00003574
Iteration 33/1000 | Loss: 0.00003571
Iteration 34/1000 | Loss: 0.00003568
Iteration 35/1000 | Loss: 0.00003567
Iteration 36/1000 | Loss: 0.00003563
Iteration 37/1000 | Loss: 0.00003563
Iteration 38/1000 | Loss: 0.00003551
Iteration 39/1000 | Loss: 0.00003541
Iteration 40/1000 | Loss: 0.00003538
Iteration 41/1000 | Loss: 0.00003538
Iteration 42/1000 | Loss: 0.00003536
Iteration 43/1000 | Loss: 0.00003535
Iteration 44/1000 | Loss: 0.00003532
Iteration 45/1000 | Loss: 0.00003531
Iteration 46/1000 | Loss: 0.00003531
Iteration 47/1000 | Loss: 0.00003531
Iteration 48/1000 | Loss: 0.00003531
Iteration 49/1000 | Loss: 0.00003531
Iteration 50/1000 | Loss: 0.00003531
Iteration 51/1000 | Loss: 0.00003531
Iteration 52/1000 | Loss: 0.00003531
Iteration 53/1000 | Loss: 0.00003531
Iteration 54/1000 | Loss: 0.00003531
Iteration 55/1000 | Loss: 0.00003531
Iteration 56/1000 | Loss: 0.00003531
Iteration 57/1000 | Loss: 0.00003531
Iteration 58/1000 | Loss: 0.00003531
Iteration 59/1000 | Loss: 0.00003531
Iteration 60/1000 | Loss: 0.00003530
Iteration 61/1000 | Loss: 0.00003529
Iteration 62/1000 | Loss: 0.00003529
Iteration 63/1000 | Loss: 0.00003529
Iteration 64/1000 | Loss: 0.00003529
Iteration 65/1000 | Loss: 0.00003528
Iteration 66/1000 | Loss: 0.00003528
Iteration 67/1000 | Loss: 0.00003528
Iteration 68/1000 | Loss: 0.00003527
Iteration 69/1000 | Loss: 0.00003526
Iteration 70/1000 | Loss: 0.00003526
Iteration 71/1000 | Loss: 0.00003525
Iteration 72/1000 | Loss: 0.00003525
Iteration 73/1000 | Loss: 0.00003525
Iteration 74/1000 | Loss: 0.00003525
Iteration 75/1000 | Loss: 0.00003525
Iteration 76/1000 | Loss: 0.00003525
Iteration 77/1000 | Loss: 0.00003525
Iteration 78/1000 | Loss: 0.00003524
Iteration 79/1000 | Loss: 0.00003524
Iteration 80/1000 | Loss: 0.00003524
Iteration 81/1000 | Loss: 0.00003524
Iteration 82/1000 | Loss: 0.00003524
Iteration 83/1000 | Loss: 0.00003524
Iteration 84/1000 | Loss: 0.00003524
Iteration 85/1000 | Loss: 0.00003523
Iteration 86/1000 | Loss: 0.00003523
Iteration 87/1000 | Loss: 0.00003523
Iteration 88/1000 | Loss: 0.00003523
Iteration 89/1000 | Loss: 0.00003523
Iteration 90/1000 | Loss: 0.00003523
Iteration 91/1000 | Loss: 0.00003522
Iteration 92/1000 | Loss: 0.00003522
Iteration 93/1000 | Loss: 0.00003522
Iteration 94/1000 | Loss: 0.00003522
Iteration 95/1000 | Loss: 0.00003522
Iteration 96/1000 | Loss: 0.00003521
Iteration 97/1000 | Loss: 0.00003521
Iteration 98/1000 | Loss: 0.00003521
Iteration 99/1000 | Loss: 0.00003521
Iteration 100/1000 | Loss: 0.00003521
Iteration 101/1000 | Loss: 0.00003521
Iteration 102/1000 | Loss: 0.00003520
Iteration 103/1000 | Loss: 0.00003520
Iteration 104/1000 | Loss: 0.00003520
Iteration 105/1000 | Loss: 0.00003520
Iteration 106/1000 | Loss: 0.00003520
Iteration 107/1000 | Loss: 0.00003520
Iteration 108/1000 | Loss: 0.00003520
Iteration 109/1000 | Loss: 0.00003520
Iteration 110/1000 | Loss: 0.00003520
Iteration 111/1000 | Loss: 0.00003520
Iteration 112/1000 | Loss: 0.00003519
Iteration 113/1000 | Loss: 0.00003519
Iteration 114/1000 | Loss: 0.00003519
Iteration 115/1000 | Loss: 0.00003519
Iteration 116/1000 | Loss: 0.00003519
Iteration 117/1000 | Loss: 0.00003519
Iteration 118/1000 | Loss: 0.00003518
Iteration 119/1000 | Loss: 0.00003518
Iteration 120/1000 | Loss: 0.00003518
Iteration 121/1000 | Loss: 0.00003518
Iteration 122/1000 | Loss: 0.00003518
Iteration 123/1000 | Loss: 0.00003518
Iteration 124/1000 | Loss: 0.00003518
Iteration 125/1000 | Loss: 0.00003518
Iteration 126/1000 | Loss: 0.00003518
Iteration 127/1000 | Loss: 0.00003518
Iteration 128/1000 | Loss: 0.00003518
Iteration 129/1000 | Loss: 0.00003517
Iteration 130/1000 | Loss: 0.00003517
Iteration 131/1000 | Loss: 0.00003517
Iteration 132/1000 | Loss: 0.00003517
Iteration 133/1000 | Loss: 0.00003517
Iteration 134/1000 | Loss: 0.00003517
Iteration 135/1000 | Loss: 0.00003517
Iteration 136/1000 | Loss: 0.00003517
Iteration 137/1000 | Loss: 0.00003517
Iteration 138/1000 | Loss: 0.00003517
Iteration 139/1000 | Loss: 0.00003517
Iteration 140/1000 | Loss: 0.00003517
Iteration 141/1000 | Loss: 0.00003517
Iteration 142/1000 | Loss: 0.00003516
Iteration 143/1000 | Loss: 0.00003516
Iteration 144/1000 | Loss: 0.00003516
Iteration 145/1000 | Loss: 0.00003516
Iteration 146/1000 | Loss: 0.00003516
Iteration 147/1000 | Loss: 0.00003516
Iteration 148/1000 | Loss: 0.00003515
Iteration 149/1000 | Loss: 0.00003515
Iteration 150/1000 | Loss: 0.00003515
Iteration 151/1000 | Loss: 0.00003515
Iteration 152/1000 | Loss: 0.00003514
Iteration 153/1000 | Loss: 0.00003514
Iteration 154/1000 | Loss: 0.00003514
Iteration 155/1000 | Loss: 0.00003514
Iteration 156/1000 | Loss: 0.00003514
Iteration 157/1000 | Loss: 0.00003514
Iteration 158/1000 | Loss: 0.00003514
Iteration 159/1000 | Loss: 0.00003514
Iteration 160/1000 | Loss: 0.00003514
Iteration 161/1000 | Loss: 0.00003514
Iteration 162/1000 | Loss: 0.00003514
Iteration 163/1000 | Loss: 0.00003514
Iteration 164/1000 | Loss: 0.00003514
Iteration 165/1000 | Loss: 0.00003514
Iteration 166/1000 | Loss: 0.00003513
Iteration 167/1000 | Loss: 0.00003513
Iteration 168/1000 | Loss: 0.00003513
Iteration 169/1000 | Loss: 0.00003513
Iteration 170/1000 | Loss: 0.00003513
Iteration 171/1000 | Loss: 0.00003513
Iteration 172/1000 | Loss: 0.00003513
Iteration 173/1000 | Loss: 0.00003513
Iteration 174/1000 | Loss: 0.00003513
Iteration 175/1000 | Loss: 0.00003513
Iteration 176/1000 | Loss: 0.00003513
Iteration 177/1000 | Loss: 0.00003513
Iteration 178/1000 | Loss: 0.00003513
Iteration 179/1000 | Loss: 0.00003513
Iteration 180/1000 | Loss: 0.00003513
Iteration 181/1000 | Loss: 0.00003513
Iteration 182/1000 | Loss: 0.00003512
Iteration 183/1000 | Loss: 0.00003512
Iteration 184/1000 | Loss: 0.00003512
Iteration 185/1000 | Loss: 0.00003512
Iteration 186/1000 | Loss: 0.00003512
Iteration 187/1000 | Loss: 0.00003512
Iteration 188/1000 | Loss: 0.00003512
Iteration 189/1000 | Loss: 0.00003512
Iteration 190/1000 | Loss: 0.00003512
Iteration 191/1000 | Loss: 0.00003512
Iteration 192/1000 | Loss: 0.00003512
Iteration 193/1000 | Loss: 0.00003511
Iteration 194/1000 | Loss: 0.00003511
Iteration 195/1000 | Loss: 0.00003511
Iteration 196/1000 | Loss: 0.00003511
Iteration 197/1000 | Loss: 0.00003511
Iteration 198/1000 | Loss: 0.00003511
Iteration 199/1000 | Loss: 0.00003511
Iteration 200/1000 | Loss: 0.00003511
Iteration 201/1000 | Loss: 0.00003511
Iteration 202/1000 | Loss: 0.00003511
Iteration 203/1000 | Loss: 0.00003511
Iteration 204/1000 | Loss: 0.00003511
Iteration 205/1000 | Loss: 0.00003511
Iteration 206/1000 | Loss: 0.00003511
Iteration 207/1000 | Loss: 0.00003511
Iteration 208/1000 | Loss: 0.00003511
Iteration 209/1000 | Loss: 0.00003511
Iteration 210/1000 | Loss: 0.00003511
Iteration 211/1000 | Loss: 0.00003510
Iteration 212/1000 | Loss: 0.00003510
Iteration 213/1000 | Loss: 0.00003510
Iteration 214/1000 | Loss: 0.00003510
Iteration 215/1000 | Loss: 0.00003510
Iteration 216/1000 | Loss: 0.00003510
Iteration 217/1000 | Loss: 0.00003510
Iteration 218/1000 | Loss: 0.00003510
Iteration 219/1000 | Loss: 0.00003510
Iteration 220/1000 | Loss: 0.00003510
Iteration 221/1000 | Loss: 0.00003510
Iteration 222/1000 | Loss: 0.00003510
Iteration 223/1000 | Loss: 0.00003510
Iteration 224/1000 | Loss: 0.00003510
Iteration 225/1000 | Loss: 0.00003510
Iteration 226/1000 | Loss: 0.00003510
Iteration 227/1000 | Loss: 0.00003510
Iteration 228/1000 | Loss: 0.00003510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [3.509697489789687e-05, 3.509697489789687e-05, 3.509697489789687e-05, 3.509697489789687e-05, 3.509697489789687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.509697489789687e-05

Optimization complete. Final v2v error: 4.618464469909668 mm

Highest mean error: 6.827462196350098 mm for frame 133

Lowest mean error: 3.9317378997802734 mm for frame 138

Saving results

Total time: 96.04315090179443
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00580813
Iteration 2/25 | Loss: 0.00160490
Iteration 3/25 | Loss: 0.00143433
Iteration 4/25 | Loss: 0.00135713
Iteration 5/25 | Loss: 0.00135192
Iteration 6/25 | Loss: 0.00135423
Iteration 7/25 | Loss: 0.00135301
Iteration 8/25 | Loss: 0.00135129
Iteration 9/25 | Loss: 0.00135038
Iteration 10/25 | Loss: 0.00134969
Iteration 11/25 | Loss: 0.00134926
Iteration 12/25 | Loss: 0.00134906
Iteration 13/25 | Loss: 0.00134903
Iteration 14/25 | Loss: 0.00134903
Iteration 15/25 | Loss: 0.00134902
Iteration 16/25 | Loss: 0.00134902
Iteration 17/25 | Loss: 0.00134902
Iteration 18/25 | Loss: 0.00134902
Iteration 19/25 | Loss: 0.00134902
Iteration 20/25 | Loss: 0.00134902
Iteration 21/25 | Loss: 0.00134902
Iteration 22/25 | Loss: 0.00134902
Iteration 23/25 | Loss: 0.00134902
Iteration 24/25 | Loss: 0.00134902
Iteration 25/25 | Loss: 0.00134902

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.43311596
Iteration 2/25 | Loss: 0.00174985
Iteration 3/25 | Loss: 0.00174985
Iteration 4/25 | Loss: 0.00172851
Iteration 5/25 | Loss: 0.00172850
Iteration 6/25 | Loss: 0.00172850
Iteration 7/25 | Loss: 0.00172850
Iteration 8/25 | Loss: 0.00172850
Iteration 9/25 | Loss: 0.00172850
Iteration 10/25 | Loss: 0.00172850
Iteration 11/25 | Loss: 0.00172850
Iteration 12/25 | Loss: 0.00172850
Iteration 13/25 | Loss: 0.00172850
Iteration 14/25 | Loss: 0.00172850
Iteration 15/25 | Loss: 0.00172850
Iteration 16/25 | Loss: 0.00172850
Iteration 17/25 | Loss: 0.00172850
Iteration 18/25 | Loss: 0.00172850
Iteration 19/25 | Loss: 0.00172850
Iteration 20/25 | Loss: 0.00172850
Iteration 21/25 | Loss: 0.00172850
Iteration 22/25 | Loss: 0.00172850
Iteration 23/25 | Loss: 0.00172850
Iteration 24/25 | Loss: 0.00172850
Iteration 25/25 | Loss: 0.00172850

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172850
Iteration 2/1000 | Loss: 0.00004801
Iteration 3/1000 | Loss: 0.00001787
Iteration 4/1000 | Loss: 0.00001569
Iteration 5/1000 | Loss: 0.00005022
Iteration 6/1000 | Loss: 0.00001432
Iteration 7/1000 | Loss: 0.00004216
Iteration 8/1000 | Loss: 0.00046824
Iteration 9/1000 | Loss: 0.00004653
Iteration 10/1000 | Loss: 0.00001375
Iteration 11/1000 | Loss: 0.00002612
Iteration 12/1000 | Loss: 0.00002101
Iteration 13/1000 | Loss: 0.00001309
Iteration 14/1000 | Loss: 0.00001276
Iteration 15/1000 | Loss: 0.00001271
Iteration 16/1000 | Loss: 0.00001252
Iteration 17/1000 | Loss: 0.00001252
Iteration 18/1000 | Loss: 0.00001250
Iteration 19/1000 | Loss: 0.00001237
Iteration 20/1000 | Loss: 0.00001232
Iteration 21/1000 | Loss: 0.00001230
Iteration 22/1000 | Loss: 0.00001230
Iteration 23/1000 | Loss: 0.00001230
Iteration 24/1000 | Loss: 0.00001228
Iteration 25/1000 | Loss: 0.00001221
Iteration 26/1000 | Loss: 0.00001221
Iteration 27/1000 | Loss: 0.00001218
Iteration 28/1000 | Loss: 0.00001217
Iteration 29/1000 | Loss: 0.00001216
Iteration 30/1000 | Loss: 0.00001216
Iteration 31/1000 | Loss: 0.00001216
Iteration 32/1000 | Loss: 0.00001216
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001216
Iteration 35/1000 | Loss: 0.00001216
Iteration 36/1000 | Loss: 0.00001216
Iteration 37/1000 | Loss: 0.00001216
Iteration 38/1000 | Loss: 0.00001215
Iteration 39/1000 | Loss: 0.00001213
Iteration 40/1000 | Loss: 0.00001212
Iteration 41/1000 | Loss: 0.00001212
Iteration 42/1000 | Loss: 0.00004203
Iteration 43/1000 | Loss: 0.00001650
Iteration 44/1000 | Loss: 0.00001415
Iteration 45/1000 | Loss: 0.00001229
Iteration 46/1000 | Loss: 0.00001210
Iteration 47/1000 | Loss: 0.00001205
Iteration 48/1000 | Loss: 0.00003350
Iteration 49/1000 | Loss: 0.00001206
Iteration 50/1000 | Loss: 0.00001200
Iteration 51/1000 | Loss: 0.00001199
Iteration 52/1000 | Loss: 0.00001199
Iteration 53/1000 | Loss: 0.00001199
Iteration 54/1000 | Loss: 0.00001199
Iteration 55/1000 | Loss: 0.00001199
Iteration 56/1000 | Loss: 0.00001199
Iteration 57/1000 | Loss: 0.00001199
Iteration 58/1000 | Loss: 0.00001198
Iteration 59/1000 | Loss: 0.00001198
Iteration 60/1000 | Loss: 0.00001198
Iteration 61/1000 | Loss: 0.00001197
Iteration 62/1000 | Loss: 0.00001196
Iteration 63/1000 | Loss: 0.00001195
Iteration 64/1000 | Loss: 0.00001194
Iteration 65/1000 | Loss: 0.00001194
Iteration 66/1000 | Loss: 0.00001194
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001193
Iteration 69/1000 | Loss: 0.00001193
Iteration 70/1000 | Loss: 0.00001193
Iteration 71/1000 | Loss: 0.00001193
Iteration 72/1000 | Loss: 0.00001192
Iteration 73/1000 | Loss: 0.00001192
Iteration 74/1000 | Loss: 0.00001192
Iteration 75/1000 | Loss: 0.00001192
Iteration 76/1000 | Loss: 0.00001191
Iteration 77/1000 | Loss: 0.00001190
Iteration 78/1000 | Loss: 0.00001189
Iteration 79/1000 | Loss: 0.00001189
Iteration 80/1000 | Loss: 0.00001189
Iteration 81/1000 | Loss: 0.00001188
Iteration 82/1000 | Loss: 0.00001188
Iteration 83/1000 | Loss: 0.00001188
Iteration 84/1000 | Loss: 0.00001188
Iteration 85/1000 | Loss: 0.00001188
Iteration 86/1000 | Loss: 0.00001188
Iteration 87/1000 | Loss: 0.00001188
Iteration 88/1000 | Loss: 0.00001188
Iteration 89/1000 | Loss: 0.00001188
Iteration 90/1000 | Loss: 0.00001188
Iteration 91/1000 | Loss: 0.00001188
Iteration 92/1000 | Loss: 0.00001188
Iteration 93/1000 | Loss: 0.00001187
Iteration 94/1000 | Loss: 0.00001187
Iteration 95/1000 | Loss: 0.00001187
Iteration 96/1000 | Loss: 0.00001187
Iteration 97/1000 | Loss: 0.00001187
Iteration 98/1000 | Loss: 0.00001187
Iteration 99/1000 | Loss: 0.00001187
Iteration 100/1000 | Loss: 0.00001186
Iteration 101/1000 | Loss: 0.00001186
Iteration 102/1000 | Loss: 0.00001186
Iteration 103/1000 | Loss: 0.00001186
Iteration 104/1000 | Loss: 0.00001186
Iteration 105/1000 | Loss: 0.00001186
Iteration 106/1000 | Loss: 0.00001186
Iteration 107/1000 | Loss: 0.00001186
Iteration 108/1000 | Loss: 0.00001186
Iteration 109/1000 | Loss: 0.00001186
Iteration 110/1000 | Loss: 0.00001186
Iteration 111/1000 | Loss: 0.00001186
Iteration 112/1000 | Loss: 0.00001186
Iteration 113/1000 | Loss: 0.00001186
Iteration 114/1000 | Loss: 0.00001186
Iteration 115/1000 | Loss: 0.00001186
Iteration 116/1000 | Loss: 0.00001186
Iteration 117/1000 | Loss: 0.00001186
Iteration 118/1000 | Loss: 0.00001186
Iteration 119/1000 | Loss: 0.00001186
Iteration 120/1000 | Loss: 0.00001186
Iteration 121/1000 | Loss: 0.00001186
Iteration 122/1000 | Loss: 0.00001186
Iteration 123/1000 | Loss: 0.00001186
Iteration 124/1000 | Loss: 0.00001186
Iteration 125/1000 | Loss: 0.00001186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.1855385309900157e-05, 1.1855385309900157e-05, 1.1855385309900157e-05, 1.1855385309900157e-05, 1.1855385309900157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1855385309900157e-05

Optimization complete. Final v2v error: 2.939786434173584 mm

Highest mean error: 3.393754482269287 mm for frame 205

Lowest mean error: 2.6879634857177734 mm for frame 140

Saving results

Total time: 68.56740832328796
