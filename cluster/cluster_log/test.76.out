Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=76, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 4256-4311
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00762838
Iteration 2/25 | Loss: 0.00163406
Iteration 3/25 | Loss: 0.00122519
Iteration 4/25 | Loss: 0.00119480
Iteration 5/25 | Loss: 0.00118846
Iteration 6/25 | Loss: 0.00118417
Iteration 7/25 | Loss: 0.00118379
Iteration 8/25 | Loss: 0.00118374
Iteration 9/25 | Loss: 0.00118374
Iteration 10/25 | Loss: 0.00118374
Iteration 11/25 | Loss: 0.00118374
Iteration 12/25 | Loss: 0.00118374
Iteration 13/25 | Loss: 0.00118374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011837364872917533, 0.0011837364872917533, 0.0011837364872917533, 0.0011837364872917533, 0.0011837364872917533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011837364872917533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61505628
Iteration 2/25 | Loss: 0.00129333
Iteration 3/25 | Loss: 0.00129331
Iteration 4/25 | Loss: 0.00129331
Iteration 5/25 | Loss: 0.00129331
Iteration 6/25 | Loss: 0.00129331
Iteration 7/25 | Loss: 0.00129331
Iteration 8/25 | Loss: 0.00129331
Iteration 9/25 | Loss: 0.00129331
Iteration 10/25 | Loss: 0.00129331
Iteration 11/25 | Loss: 0.00129331
Iteration 12/25 | Loss: 0.00129331
Iteration 13/25 | Loss: 0.00129331
Iteration 14/25 | Loss: 0.00129331
Iteration 15/25 | Loss: 0.00129331
Iteration 16/25 | Loss: 0.00129331
Iteration 17/25 | Loss: 0.00129331
Iteration 18/25 | Loss: 0.00129331
Iteration 19/25 | Loss: 0.00129331
Iteration 20/25 | Loss: 0.00129331
Iteration 21/25 | Loss: 0.00129331
Iteration 22/25 | Loss: 0.00129331
Iteration 23/25 | Loss: 0.00129331
Iteration 24/25 | Loss: 0.00129331
Iteration 25/25 | Loss: 0.00129331

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129331
Iteration 2/1000 | Loss: 0.00002574
Iteration 3/1000 | Loss: 0.00001845
Iteration 4/1000 | Loss: 0.00001593
Iteration 5/1000 | Loss: 0.00001524
Iteration 6/1000 | Loss: 0.00001433
Iteration 7/1000 | Loss: 0.00001366
Iteration 8/1000 | Loss: 0.00001330
Iteration 9/1000 | Loss: 0.00001296
Iteration 10/1000 | Loss: 0.00001267
Iteration 11/1000 | Loss: 0.00001261
Iteration 12/1000 | Loss: 0.00001249
Iteration 13/1000 | Loss: 0.00001245
Iteration 14/1000 | Loss: 0.00001241
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001258
Iteration 17/1000 | Loss: 0.00001225
Iteration 18/1000 | Loss: 0.00001212
Iteration 19/1000 | Loss: 0.00001212
Iteration 20/1000 | Loss: 0.00001210
Iteration 21/1000 | Loss: 0.00001207
Iteration 22/1000 | Loss: 0.00001207
Iteration 23/1000 | Loss: 0.00001206
Iteration 24/1000 | Loss: 0.00001206
Iteration 25/1000 | Loss: 0.00001205
Iteration 26/1000 | Loss: 0.00001301
Iteration 27/1000 | Loss: 0.00001404
Iteration 28/1000 | Loss: 0.00001197
Iteration 29/1000 | Loss: 0.00001195
Iteration 30/1000 | Loss: 0.00001194
Iteration 31/1000 | Loss: 0.00001194
Iteration 32/1000 | Loss: 0.00001194
Iteration 33/1000 | Loss: 0.00001194
Iteration 34/1000 | Loss: 0.00001195
Iteration 35/1000 | Loss: 0.00001194
Iteration 36/1000 | Loss: 0.00001194
Iteration 37/1000 | Loss: 0.00001194
Iteration 38/1000 | Loss: 0.00001194
Iteration 39/1000 | Loss: 0.00001194
Iteration 40/1000 | Loss: 0.00001194
Iteration 41/1000 | Loss: 0.00001193
Iteration 42/1000 | Loss: 0.00001218
Iteration 43/1000 | Loss: 0.00001191
Iteration 44/1000 | Loss: 0.00001191
Iteration 45/1000 | Loss: 0.00001191
Iteration 46/1000 | Loss: 0.00001191
Iteration 47/1000 | Loss: 0.00001191
Iteration 48/1000 | Loss: 0.00001190
Iteration 49/1000 | Loss: 0.00001190
Iteration 50/1000 | Loss: 0.00001190
Iteration 51/1000 | Loss: 0.00001190
Iteration 52/1000 | Loss: 0.00001190
Iteration 53/1000 | Loss: 0.00001190
Iteration 54/1000 | Loss: 0.00001190
Iteration 55/1000 | Loss: 0.00001189
Iteration 56/1000 | Loss: 0.00001189
Iteration 57/1000 | Loss: 0.00001189
Iteration 58/1000 | Loss: 0.00001189
Iteration 59/1000 | Loss: 0.00001189
Iteration 60/1000 | Loss: 0.00001189
Iteration 61/1000 | Loss: 0.00001189
Iteration 62/1000 | Loss: 0.00001188
Iteration 63/1000 | Loss: 0.00001188
Iteration 64/1000 | Loss: 0.00001188
Iteration 65/1000 | Loss: 0.00001188
Iteration 66/1000 | Loss: 0.00001188
Iteration 67/1000 | Loss: 0.00001188
Iteration 68/1000 | Loss: 0.00001188
Iteration 69/1000 | Loss: 0.00001188
Iteration 70/1000 | Loss: 0.00001188
Iteration 71/1000 | Loss: 0.00001188
Iteration 72/1000 | Loss: 0.00001187
Iteration 73/1000 | Loss: 0.00001187
Iteration 74/1000 | Loss: 0.00001229
Iteration 75/1000 | Loss: 0.00001187
Iteration 76/1000 | Loss: 0.00001186
Iteration 77/1000 | Loss: 0.00001186
Iteration 78/1000 | Loss: 0.00001186
Iteration 79/1000 | Loss: 0.00001186
Iteration 80/1000 | Loss: 0.00001186
Iteration 81/1000 | Loss: 0.00001186
Iteration 82/1000 | Loss: 0.00001186
Iteration 83/1000 | Loss: 0.00001185
Iteration 84/1000 | Loss: 0.00001185
Iteration 85/1000 | Loss: 0.00001185
Iteration 86/1000 | Loss: 0.00001189
Iteration 87/1000 | Loss: 0.00001183
Iteration 88/1000 | Loss: 0.00001183
Iteration 89/1000 | Loss: 0.00001183
Iteration 90/1000 | Loss: 0.00001182
Iteration 91/1000 | Loss: 0.00001182
Iteration 92/1000 | Loss: 0.00001182
Iteration 93/1000 | Loss: 0.00001182
Iteration 94/1000 | Loss: 0.00001182
Iteration 95/1000 | Loss: 0.00001182
Iteration 96/1000 | Loss: 0.00001182
Iteration 97/1000 | Loss: 0.00001182
Iteration 98/1000 | Loss: 0.00001182
Iteration 99/1000 | Loss: 0.00001182
Iteration 100/1000 | Loss: 0.00001182
Iteration 101/1000 | Loss: 0.00001182
Iteration 102/1000 | Loss: 0.00001182
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001182
Iteration 105/1000 | Loss: 0.00001182
Iteration 106/1000 | Loss: 0.00001182
Iteration 107/1000 | Loss: 0.00001182
Iteration 108/1000 | Loss: 0.00001182
Iteration 109/1000 | Loss: 0.00001182
Iteration 110/1000 | Loss: 0.00001182
Iteration 111/1000 | Loss: 0.00001182
Iteration 112/1000 | Loss: 0.00001182
Iteration 113/1000 | Loss: 0.00001182
Iteration 114/1000 | Loss: 0.00001182
Iteration 115/1000 | Loss: 0.00001182
Iteration 116/1000 | Loss: 0.00001182
Iteration 117/1000 | Loss: 0.00001182
Iteration 118/1000 | Loss: 0.00001182
Iteration 119/1000 | Loss: 0.00001182
Iteration 120/1000 | Loss: 0.00001182
Iteration 121/1000 | Loss: 0.00001182
Iteration 122/1000 | Loss: 0.00001182
Iteration 123/1000 | Loss: 0.00001182
Iteration 124/1000 | Loss: 0.00001182
Iteration 125/1000 | Loss: 0.00001182
Iteration 126/1000 | Loss: 0.00001182
Iteration 127/1000 | Loss: 0.00001182
Iteration 128/1000 | Loss: 0.00001182
Iteration 129/1000 | Loss: 0.00001182
Iteration 130/1000 | Loss: 0.00001182
Iteration 131/1000 | Loss: 0.00001182
Iteration 132/1000 | Loss: 0.00001182
Iteration 133/1000 | Loss: 0.00001182
Iteration 134/1000 | Loss: 0.00001182
Iteration 135/1000 | Loss: 0.00001182
Iteration 136/1000 | Loss: 0.00001182
Iteration 137/1000 | Loss: 0.00001182
Iteration 138/1000 | Loss: 0.00001182
Iteration 139/1000 | Loss: 0.00001182
Iteration 140/1000 | Loss: 0.00001182
Iteration 141/1000 | Loss: 0.00001182
Iteration 142/1000 | Loss: 0.00001182
Iteration 143/1000 | Loss: 0.00001182
Iteration 144/1000 | Loss: 0.00001182
Iteration 145/1000 | Loss: 0.00001182
Iteration 146/1000 | Loss: 0.00001182
Iteration 147/1000 | Loss: 0.00001182
Iteration 148/1000 | Loss: 0.00001182
Iteration 149/1000 | Loss: 0.00001182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.1817365702881943e-05, 1.1817365702881943e-05, 1.1817365702881943e-05, 1.1817365702881943e-05, 1.1817365702881943e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1817365702881943e-05

Optimization complete. Final v2v error: 2.938962459564209 mm

Highest mean error: 3.2642884254455566 mm for frame 138

Lowest mean error: 2.7214698791503906 mm for frame 57

Saving results

Total time: 50.43925094604492
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019007
Iteration 2/25 | Loss: 0.00260358
Iteration 3/25 | Loss: 0.00193501
Iteration 4/25 | Loss: 0.00193124
Iteration 5/25 | Loss: 0.00170224
Iteration 6/25 | Loss: 0.00161877
Iteration 7/25 | Loss: 0.00154224
Iteration 8/25 | Loss: 0.00154342
Iteration 9/25 | Loss: 0.00152629
Iteration 10/25 | Loss: 0.00149815
Iteration 11/25 | Loss: 0.00148949
Iteration 12/25 | Loss: 0.00148719
Iteration 13/25 | Loss: 0.00148795
Iteration 14/25 | Loss: 0.00148054
Iteration 15/25 | Loss: 0.00147692
Iteration 16/25 | Loss: 0.00147437
Iteration 17/25 | Loss: 0.00146404
Iteration 18/25 | Loss: 0.00146295
Iteration 19/25 | Loss: 0.00146243
Iteration 20/25 | Loss: 0.00146149
Iteration 21/25 | Loss: 0.00146113
Iteration 22/25 | Loss: 0.00146086
Iteration 23/25 | Loss: 0.00146066
Iteration 24/25 | Loss: 0.00146049
Iteration 25/25 | Loss: 0.00146007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.76696873
Iteration 2/25 | Loss: 0.00285720
Iteration 3/25 | Loss: 0.00285719
Iteration 4/25 | Loss: 0.00285719
Iteration 5/25 | Loss: 0.00285719
Iteration 6/25 | Loss: 0.00285719
Iteration 7/25 | Loss: 0.00285719
Iteration 8/25 | Loss: 0.00285719
Iteration 9/25 | Loss: 0.00285719
Iteration 10/25 | Loss: 0.00285719
Iteration 11/25 | Loss: 0.00285719
Iteration 12/25 | Loss: 0.00285719
Iteration 13/25 | Loss: 0.00285719
Iteration 14/25 | Loss: 0.00285719
Iteration 15/25 | Loss: 0.00285719
Iteration 16/25 | Loss: 0.00285719
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0028571926523, 0.0028571926523, 0.0028571926523, 0.0028571926523, 0.0028571926523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028571926523

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00285719
Iteration 2/1000 | Loss: 0.00028448
Iteration 3/1000 | Loss: 0.00020473
Iteration 4/1000 | Loss: 0.00143066
Iteration 5/1000 | Loss: 0.00050557
Iteration 6/1000 | Loss: 0.00016806
Iteration 7/1000 | Loss: 0.00051969
Iteration 8/1000 | Loss: 0.00014848
Iteration 9/1000 | Loss: 0.00013469
Iteration 10/1000 | Loss: 0.00012432
Iteration 11/1000 | Loss: 0.00041419
Iteration 12/1000 | Loss: 0.00013468
Iteration 13/1000 | Loss: 0.00012249
Iteration 14/1000 | Loss: 0.00011717
Iteration 15/1000 | Loss: 0.00011322
Iteration 16/1000 | Loss: 0.00035290
Iteration 17/1000 | Loss: 0.00045733
Iteration 18/1000 | Loss: 0.00027582
Iteration 19/1000 | Loss: 0.00011115
Iteration 20/1000 | Loss: 0.00010585
Iteration 21/1000 | Loss: 0.00041405
Iteration 22/1000 | Loss: 0.00013416
Iteration 23/1000 | Loss: 0.00011370
Iteration 24/1000 | Loss: 0.00010372
Iteration 25/1000 | Loss: 0.00010030
Iteration 26/1000 | Loss: 0.00009782
Iteration 27/1000 | Loss: 0.00009664
Iteration 28/1000 | Loss: 0.00009556
Iteration 29/1000 | Loss: 0.00009427
Iteration 30/1000 | Loss: 0.00009315
Iteration 31/1000 | Loss: 0.00037701
Iteration 32/1000 | Loss: 0.00011141
Iteration 33/1000 | Loss: 0.00009692
Iteration 34/1000 | Loss: 0.00009401
Iteration 35/1000 | Loss: 0.00009229
Iteration 36/1000 | Loss: 0.00009014
Iteration 37/1000 | Loss: 0.00008853
Iteration 38/1000 | Loss: 0.00008748
Iteration 39/1000 | Loss: 0.00008703
Iteration 40/1000 | Loss: 0.00008652
Iteration 41/1000 | Loss: 0.00016925
Iteration 42/1000 | Loss: 0.00069952
Iteration 43/1000 | Loss: 0.00028945
Iteration 44/1000 | Loss: 0.00114479
Iteration 45/1000 | Loss: 0.00017700
Iteration 46/1000 | Loss: 0.00082550
Iteration 47/1000 | Loss: 0.00012572
Iteration 48/1000 | Loss: 0.00009368
Iteration 49/1000 | Loss: 0.00010778
Iteration 50/1000 | Loss: 0.00008127
Iteration 51/1000 | Loss: 0.00007544
Iteration 52/1000 | Loss: 0.00007240
Iteration 53/1000 | Loss: 0.00007056
Iteration 54/1000 | Loss: 0.00006965
Iteration 55/1000 | Loss: 0.00007887
Iteration 56/1000 | Loss: 0.00007419
Iteration 57/1000 | Loss: 0.00006804
Iteration 58/1000 | Loss: 0.00006754
Iteration 59/1000 | Loss: 0.00006725
Iteration 60/1000 | Loss: 0.00006693
Iteration 61/1000 | Loss: 0.00006667
Iteration 62/1000 | Loss: 0.00006650
Iteration 63/1000 | Loss: 0.00006643
Iteration 64/1000 | Loss: 0.00006642
Iteration 65/1000 | Loss: 0.00006641
Iteration 66/1000 | Loss: 0.00006640
Iteration 67/1000 | Loss: 0.00006636
Iteration 68/1000 | Loss: 0.00006629
Iteration 69/1000 | Loss: 0.00006621
Iteration 70/1000 | Loss: 0.00006619
Iteration 71/1000 | Loss: 0.00006619
Iteration 72/1000 | Loss: 0.00006619
Iteration 73/1000 | Loss: 0.00006617
Iteration 74/1000 | Loss: 0.00006617
Iteration 75/1000 | Loss: 0.00006616
Iteration 76/1000 | Loss: 0.00006616
Iteration 77/1000 | Loss: 0.00006616
Iteration 78/1000 | Loss: 0.00006616
Iteration 79/1000 | Loss: 0.00006616
Iteration 80/1000 | Loss: 0.00006616
Iteration 81/1000 | Loss: 0.00006616
Iteration 82/1000 | Loss: 0.00006615
Iteration 83/1000 | Loss: 0.00006615
Iteration 84/1000 | Loss: 0.00006614
Iteration 85/1000 | Loss: 0.00006614
Iteration 86/1000 | Loss: 0.00006614
Iteration 87/1000 | Loss: 0.00006614
Iteration 88/1000 | Loss: 0.00006614
Iteration 89/1000 | Loss: 0.00006613
Iteration 90/1000 | Loss: 0.00006613
Iteration 91/1000 | Loss: 0.00006613
Iteration 92/1000 | Loss: 0.00006613
Iteration 93/1000 | Loss: 0.00006613
Iteration 94/1000 | Loss: 0.00006613
Iteration 95/1000 | Loss: 0.00006612
Iteration 96/1000 | Loss: 0.00006612
Iteration 97/1000 | Loss: 0.00006611
Iteration 98/1000 | Loss: 0.00006611
Iteration 99/1000 | Loss: 0.00006611
Iteration 100/1000 | Loss: 0.00006611
Iteration 101/1000 | Loss: 0.00006611
Iteration 102/1000 | Loss: 0.00006611
Iteration 103/1000 | Loss: 0.00006611
Iteration 104/1000 | Loss: 0.00006611
Iteration 105/1000 | Loss: 0.00006611
Iteration 106/1000 | Loss: 0.00006610
Iteration 107/1000 | Loss: 0.00006610
Iteration 108/1000 | Loss: 0.00006610
Iteration 109/1000 | Loss: 0.00006610
Iteration 110/1000 | Loss: 0.00006610
Iteration 111/1000 | Loss: 0.00006610
Iteration 112/1000 | Loss: 0.00006610
Iteration 113/1000 | Loss: 0.00006610
Iteration 114/1000 | Loss: 0.00006609
Iteration 115/1000 | Loss: 0.00006609
Iteration 116/1000 | Loss: 0.00006609
Iteration 117/1000 | Loss: 0.00006609
Iteration 118/1000 | Loss: 0.00006609
Iteration 119/1000 | Loss: 0.00006609
Iteration 120/1000 | Loss: 0.00006609
Iteration 121/1000 | Loss: 0.00006608
Iteration 122/1000 | Loss: 0.00006608
Iteration 123/1000 | Loss: 0.00006608
Iteration 124/1000 | Loss: 0.00006608
Iteration 125/1000 | Loss: 0.00006608
Iteration 126/1000 | Loss: 0.00006608
Iteration 127/1000 | Loss: 0.00006608
Iteration 128/1000 | Loss: 0.00006608
Iteration 129/1000 | Loss: 0.00006608
Iteration 130/1000 | Loss: 0.00006608
Iteration 131/1000 | Loss: 0.00006608
Iteration 132/1000 | Loss: 0.00006607
Iteration 133/1000 | Loss: 0.00006607
Iteration 134/1000 | Loss: 0.00006607
Iteration 135/1000 | Loss: 0.00006607
Iteration 136/1000 | Loss: 0.00006607
Iteration 137/1000 | Loss: 0.00006607
Iteration 138/1000 | Loss: 0.00006607
Iteration 139/1000 | Loss: 0.00006607
Iteration 140/1000 | Loss: 0.00006606
Iteration 141/1000 | Loss: 0.00006606
Iteration 142/1000 | Loss: 0.00006606
Iteration 143/1000 | Loss: 0.00006606
Iteration 144/1000 | Loss: 0.00006606
Iteration 145/1000 | Loss: 0.00006606
Iteration 146/1000 | Loss: 0.00006606
Iteration 147/1000 | Loss: 0.00006605
Iteration 148/1000 | Loss: 0.00006605
Iteration 149/1000 | Loss: 0.00006605
Iteration 150/1000 | Loss: 0.00006605
Iteration 151/1000 | Loss: 0.00006604
Iteration 152/1000 | Loss: 0.00006604
Iteration 153/1000 | Loss: 0.00006604
Iteration 154/1000 | Loss: 0.00006604
Iteration 155/1000 | Loss: 0.00006604
Iteration 156/1000 | Loss: 0.00006604
Iteration 157/1000 | Loss: 0.00006604
Iteration 158/1000 | Loss: 0.00006604
Iteration 159/1000 | Loss: 0.00006603
Iteration 160/1000 | Loss: 0.00006603
Iteration 161/1000 | Loss: 0.00006603
Iteration 162/1000 | Loss: 0.00006603
Iteration 163/1000 | Loss: 0.00006603
Iteration 164/1000 | Loss: 0.00006603
Iteration 165/1000 | Loss: 0.00006603
Iteration 166/1000 | Loss: 0.00006603
Iteration 167/1000 | Loss: 0.00006603
Iteration 168/1000 | Loss: 0.00006603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [6.603118526982144e-05, 6.603118526982144e-05, 6.603118526982144e-05, 6.603118526982144e-05, 6.603118526982144e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.603118526982144e-05

Optimization complete. Final v2v error: 4.536412715911865 mm

Highest mean error: 11.633264541625977 mm for frame 51

Lowest mean error: 2.9205117225646973 mm for frame 6

Saving results

Total time: 140.00231313705444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460276
Iteration 2/25 | Loss: 0.00140742
Iteration 3/25 | Loss: 0.00126247
Iteration 4/25 | Loss: 0.00125559
Iteration 5/25 | Loss: 0.00125473
Iteration 6/25 | Loss: 0.00125473
Iteration 7/25 | Loss: 0.00125473
Iteration 8/25 | Loss: 0.00125473
Iteration 9/25 | Loss: 0.00125473
Iteration 10/25 | Loss: 0.00125473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012547316728159785, 0.0012547316728159785, 0.0012547316728159785, 0.0012547316728159785, 0.0012547316728159785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012547316728159785

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29896724
Iteration 2/25 | Loss: 0.00128948
Iteration 3/25 | Loss: 0.00128946
Iteration 4/25 | Loss: 0.00128945
Iteration 5/25 | Loss: 0.00128945
Iteration 6/25 | Loss: 0.00128945
Iteration 7/25 | Loss: 0.00128945
Iteration 8/25 | Loss: 0.00128945
Iteration 9/25 | Loss: 0.00128945
Iteration 10/25 | Loss: 0.00128945
Iteration 11/25 | Loss: 0.00128945
Iteration 12/25 | Loss: 0.00128945
Iteration 13/25 | Loss: 0.00128945
Iteration 14/25 | Loss: 0.00128945
Iteration 15/25 | Loss: 0.00128945
Iteration 16/25 | Loss: 0.00128945
Iteration 17/25 | Loss: 0.00128945
Iteration 18/25 | Loss: 0.00128945
Iteration 19/25 | Loss: 0.00128945
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012894525425508618, 0.0012894525425508618, 0.0012894525425508618, 0.0012894525425508618, 0.0012894525425508618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012894525425508618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128945
Iteration 2/1000 | Loss: 0.00002553
Iteration 3/1000 | Loss: 0.00001701
Iteration 4/1000 | Loss: 0.00001567
Iteration 5/1000 | Loss: 0.00001491
Iteration 6/1000 | Loss: 0.00001452
Iteration 7/1000 | Loss: 0.00001413
Iteration 8/1000 | Loss: 0.00001383
Iteration 9/1000 | Loss: 0.00001380
Iteration 10/1000 | Loss: 0.00001351
Iteration 11/1000 | Loss: 0.00001336
Iteration 12/1000 | Loss: 0.00001334
Iteration 13/1000 | Loss: 0.00001332
Iteration 14/1000 | Loss: 0.00001331
Iteration 15/1000 | Loss: 0.00001329
Iteration 16/1000 | Loss: 0.00001329
Iteration 17/1000 | Loss: 0.00001328
Iteration 18/1000 | Loss: 0.00001325
Iteration 19/1000 | Loss: 0.00001324
Iteration 20/1000 | Loss: 0.00001323
Iteration 21/1000 | Loss: 0.00001322
Iteration 22/1000 | Loss: 0.00001318
Iteration 23/1000 | Loss: 0.00001318
Iteration 24/1000 | Loss: 0.00001318
Iteration 25/1000 | Loss: 0.00001318
Iteration 26/1000 | Loss: 0.00001315
Iteration 27/1000 | Loss: 0.00001312
Iteration 28/1000 | Loss: 0.00001312
Iteration 29/1000 | Loss: 0.00001310
Iteration 30/1000 | Loss: 0.00001309
Iteration 31/1000 | Loss: 0.00001308
Iteration 32/1000 | Loss: 0.00001307
Iteration 33/1000 | Loss: 0.00001306
Iteration 34/1000 | Loss: 0.00001306
Iteration 35/1000 | Loss: 0.00001305
Iteration 36/1000 | Loss: 0.00001304
Iteration 37/1000 | Loss: 0.00001304
Iteration 38/1000 | Loss: 0.00001302
Iteration 39/1000 | Loss: 0.00001301
Iteration 40/1000 | Loss: 0.00001299
Iteration 41/1000 | Loss: 0.00001298
Iteration 42/1000 | Loss: 0.00001296
Iteration 43/1000 | Loss: 0.00001295
Iteration 44/1000 | Loss: 0.00001295
Iteration 45/1000 | Loss: 0.00001294
Iteration 46/1000 | Loss: 0.00001294
Iteration 47/1000 | Loss: 0.00001288
Iteration 48/1000 | Loss: 0.00001288
Iteration 49/1000 | Loss: 0.00001287
Iteration 50/1000 | Loss: 0.00001287
Iteration 51/1000 | Loss: 0.00001286
Iteration 52/1000 | Loss: 0.00001286
Iteration 53/1000 | Loss: 0.00001285
Iteration 54/1000 | Loss: 0.00001285
Iteration 55/1000 | Loss: 0.00001285
Iteration 56/1000 | Loss: 0.00001285
Iteration 57/1000 | Loss: 0.00001285
Iteration 58/1000 | Loss: 0.00001285
Iteration 59/1000 | Loss: 0.00001285
Iteration 60/1000 | Loss: 0.00001285
Iteration 61/1000 | Loss: 0.00001285
Iteration 62/1000 | Loss: 0.00001285
Iteration 63/1000 | Loss: 0.00001284
Iteration 64/1000 | Loss: 0.00001284
Iteration 65/1000 | Loss: 0.00001284
Iteration 66/1000 | Loss: 0.00001283
Iteration 67/1000 | Loss: 0.00001282
Iteration 68/1000 | Loss: 0.00001281
Iteration 69/1000 | Loss: 0.00001281
Iteration 70/1000 | Loss: 0.00001281
Iteration 71/1000 | Loss: 0.00001281
Iteration 72/1000 | Loss: 0.00001280
Iteration 73/1000 | Loss: 0.00001280
Iteration 74/1000 | Loss: 0.00001279
Iteration 75/1000 | Loss: 0.00001279
Iteration 76/1000 | Loss: 0.00001278
Iteration 77/1000 | Loss: 0.00001276
Iteration 78/1000 | Loss: 0.00001276
Iteration 79/1000 | Loss: 0.00001276
Iteration 80/1000 | Loss: 0.00001276
Iteration 81/1000 | Loss: 0.00001276
Iteration 82/1000 | Loss: 0.00001276
Iteration 83/1000 | Loss: 0.00001276
Iteration 84/1000 | Loss: 0.00001276
Iteration 85/1000 | Loss: 0.00001276
Iteration 86/1000 | Loss: 0.00001275
Iteration 87/1000 | Loss: 0.00001275
Iteration 88/1000 | Loss: 0.00001274
Iteration 89/1000 | Loss: 0.00001274
Iteration 90/1000 | Loss: 0.00001274
Iteration 91/1000 | Loss: 0.00001273
Iteration 92/1000 | Loss: 0.00001273
Iteration 93/1000 | Loss: 0.00001273
Iteration 94/1000 | Loss: 0.00001273
Iteration 95/1000 | Loss: 0.00001273
Iteration 96/1000 | Loss: 0.00001273
Iteration 97/1000 | Loss: 0.00001273
Iteration 98/1000 | Loss: 0.00001273
Iteration 99/1000 | Loss: 0.00001273
Iteration 100/1000 | Loss: 0.00001273
Iteration 101/1000 | Loss: 0.00001272
Iteration 102/1000 | Loss: 0.00001272
Iteration 103/1000 | Loss: 0.00001272
Iteration 104/1000 | Loss: 0.00001272
Iteration 105/1000 | Loss: 0.00001271
Iteration 106/1000 | Loss: 0.00001271
Iteration 107/1000 | Loss: 0.00001271
Iteration 108/1000 | Loss: 0.00001271
Iteration 109/1000 | Loss: 0.00001271
Iteration 110/1000 | Loss: 0.00001270
Iteration 111/1000 | Loss: 0.00001270
Iteration 112/1000 | Loss: 0.00001270
Iteration 113/1000 | Loss: 0.00001270
Iteration 114/1000 | Loss: 0.00001270
Iteration 115/1000 | Loss: 0.00001269
Iteration 116/1000 | Loss: 0.00001269
Iteration 117/1000 | Loss: 0.00001269
Iteration 118/1000 | Loss: 0.00001269
Iteration 119/1000 | Loss: 0.00001269
Iteration 120/1000 | Loss: 0.00001269
Iteration 121/1000 | Loss: 0.00001268
Iteration 122/1000 | Loss: 0.00001268
Iteration 123/1000 | Loss: 0.00001268
Iteration 124/1000 | Loss: 0.00001268
Iteration 125/1000 | Loss: 0.00001268
Iteration 126/1000 | Loss: 0.00001268
Iteration 127/1000 | Loss: 0.00001268
Iteration 128/1000 | Loss: 0.00001268
Iteration 129/1000 | Loss: 0.00001268
Iteration 130/1000 | Loss: 0.00001267
Iteration 131/1000 | Loss: 0.00001267
Iteration 132/1000 | Loss: 0.00001267
Iteration 133/1000 | Loss: 0.00001267
Iteration 134/1000 | Loss: 0.00001267
Iteration 135/1000 | Loss: 0.00001267
Iteration 136/1000 | Loss: 0.00001266
Iteration 137/1000 | Loss: 0.00001266
Iteration 138/1000 | Loss: 0.00001266
Iteration 139/1000 | Loss: 0.00001266
Iteration 140/1000 | Loss: 0.00001266
Iteration 141/1000 | Loss: 0.00001266
Iteration 142/1000 | Loss: 0.00001265
Iteration 143/1000 | Loss: 0.00001265
Iteration 144/1000 | Loss: 0.00001265
Iteration 145/1000 | Loss: 0.00001265
Iteration 146/1000 | Loss: 0.00001265
Iteration 147/1000 | Loss: 0.00001264
Iteration 148/1000 | Loss: 0.00001264
Iteration 149/1000 | Loss: 0.00001264
Iteration 150/1000 | Loss: 0.00001264
Iteration 151/1000 | Loss: 0.00001264
Iteration 152/1000 | Loss: 0.00001263
Iteration 153/1000 | Loss: 0.00001263
Iteration 154/1000 | Loss: 0.00001263
Iteration 155/1000 | Loss: 0.00001263
Iteration 156/1000 | Loss: 0.00001263
Iteration 157/1000 | Loss: 0.00001263
Iteration 158/1000 | Loss: 0.00001263
Iteration 159/1000 | Loss: 0.00001263
Iteration 160/1000 | Loss: 0.00001263
Iteration 161/1000 | Loss: 0.00001263
Iteration 162/1000 | Loss: 0.00001263
Iteration 163/1000 | Loss: 0.00001263
Iteration 164/1000 | Loss: 0.00001263
Iteration 165/1000 | Loss: 0.00001263
Iteration 166/1000 | Loss: 0.00001263
Iteration 167/1000 | Loss: 0.00001263
Iteration 168/1000 | Loss: 0.00001263
Iteration 169/1000 | Loss: 0.00001263
Iteration 170/1000 | Loss: 0.00001263
Iteration 171/1000 | Loss: 0.00001263
Iteration 172/1000 | Loss: 0.00001263
Iteration 173/1000 | Loss: 0.00001263
Iteration 174/1000 | Loss: 0.00001263
Iteration 175/1000 | Loss: 0.00001263
Iteration 176/1000 | Loss: 0.00001263
Iteration 177/1000 | Loss: 0.00001263
Iteration 178/1000 | Loss: 0.00001263
Iteration 179/1000 | Loss: 0.00001263
Iteration 180/1000 | Loss: 0.00001263
Iteration 181/1000 | Loss: 0.00001263
Iteration 182/1000 | Loss: 0.00001263
Iteration 183/1000 | Loss: 0.00001263
Iteration 184/1000 | Loss: 0.00001263
Iteration 185/1000 | Loss: 0.00001263
Iteration 186/1000 | Loss: 0.00001263
Iteration 187/1000 | Loss: 0.00001263
Iteration 188/1000 | Loss: 0.00001263
Iteration 189/1000 | Loss: 0.00001263
Iteration 190/1000 | Loss: 0.00001263
Iteration 191/1000 | Loss: 0.00001263
Iteration 192/1000 | Loss: 0.00001263
Iteration 193/1000 | Loss: 0.00001263
Iteration 194/1000 | Loss: 0.00001263
Iteration 195/1000 | Loss: 0.00001263
Iteration 196/1000 | Loss: 0.00001263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.2632327525352594e-05, 1.2632327525352594e-05, 1.2632327525352594e-05, 1.2632327525352594e-05, 1.2632327525352594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2632327525352594e-05

Optimization complete. Final v2v error: 2.9928536415100098 mm

Highest mean error: 3.233621120452881 mm for frame 92

Lowest mean error: 2.747041940689087 mm for frame 33

Saving results

Total time: 37.16782855987549
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00737377
Iteration 2/25 | Loss: 0.00178321
Iteration 3/25 | Loss: 0.00134500
Iteration 4/25 | Loss: 0.00126966
Iteration 5/25 | Loss: 0.00125881
Iteration 6/25 | Loss: 0.00125811
Iteration 7/25 | Loss: 0.00125811
Iteration 8/25 | Loss: 0.00125811
Iteration 9/25 | Loss: 0.00125811
Iteration 10/25 | Loss: 0.00125811
Iteration 11/25 | Loss: 0.00125811
Iteration 12/25 | Loss: 0.00125811
Iteration 13/25 | Loss: 0.00125811
Iteration 14/25 | Loss: 0.00125811
Iteration 15/25 | Loss: 0.00125811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012581140035763383, 0.0012581140035763383, 0.0012581140035763383, 0.0012581140035763383, 0.0012581140035763383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012581140035763383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.63674212
Iteration 2/25 | Loss: 0.00116029
Iteration 3/25 | Loss: 0.00116024
Iteration 4/25 | Loss: 0.00116024
Iteration 5/25 | Loss: 0.00116024
Iteration 6/25 | Loss: 0.00116024
Iteration 7/25 | Loss: 0.00116024
Iteration 8/25 | Loss: 0.00116024
Iteration 9/25 | Loss: 0.00116024
Iteration 10/25 | Loss: 0.00116024
Iteration 11/25 | Loss: 0.00116024
Iteration 12/25 | Loss: 0.00116024
Iteration 13/25 | Loss: 0.00116024
Iteration 14/25 | Loss: 0.00116024
Iteration 15/25 | Loss: 0.00116024
Iteration 16/25 | Loss: 0.00116024
Iteration 17/25 | Loss: 0.00116024
Iteration 18/25 | Loss: 0.00116024
Iteration 19/25 | Loss: 0.00116024
Iteration 20/25 | Loss: 0.00116024
Iteration 21/25 | Loss: 0.00116024
Iteration 22/25 | Loss: 0.00116024
Iteration 23/25 | Loss: 0.00116024
Iteration 24/25 | Loss: 0.00116024
Iteration 25/25 | Loss: 0.00116024

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116024
Iteration 2/1000 | Loss: 0.00003495
Iteration 3/1000 | Loss: 0.00002491
Iteration 4/1000 | Loss: 0.00002331
Iteration 5/1000 | Loss: 0.00002233
Iteration 6/1000 | Loss: 0.00002163
Iteration 7/1000 | Loss: 0.00002122
Iteration 8/1000 | Loss: 0.00002084
Iteration 9/1000 | Loss: 0.00002054
Iteration 10/1000 | Loss: 0.00002026
Iteration 11/1000 | Loss: 0.00001998
Iteration 12/1000 | Loss: 0.00001983
Iteration 13/1000 | Loss: 0.00001978
Iteration 14/1000 | Loss: 0.00001977
Iteration 15/1000 | Loss: 0.00001977
Iteration 16/1000 | Loss: 0.00001969
Iteration 17/1000 | Loss: 0.00001969
Iteration 18/1000 | Loss: 0.00001967
Iteration 19/1000 | Loss: 0.00001964
Iteration 20/1000 | Loss: 0.00001961
Iteration 21/1000 | Loss: 0.00001960
Iteration 22/1000 | Loss: 0.00001959
Iteration 23/1000 | Loss: 0.00001959
Iteration 24/1000 | Loss: 0.00001958
Iteration 25/1000 | Loss: 0.00001957
Iteration 26/1000 | Loss: 0.00001957
Iteration 27/1000 | Loss: 0.00001957
Iteration 28/1000 | Loss: 0.00001957
Iteration 29/1000 | Loss: 0.00001957
Iteration 30/1000 | Loss: 0.00001956
Iteration 31/1000 | Loss: 0.00001956
Iteration 32/1000 | Loss: 0.00001956
Iteration 33/1000 | Loss: 0.00001956
Iteration 34/1000 | Loss: 0.00001956
Iteration 35/1000 | Loss: 0.00001956
Iteration 36/1000 | Loss: 0.00001956
Iteration 37/1000 | Loss: 0.00001956
Iteration 38/1000 | Loss: 0.00001956
Iteration 39/1000 | Loss: 0.00001955
Iteration 40/1000 | Loss: 0.00001955
Iteration 41/1000 | Loss: 0.00001955
Iteration 42/1000 | Loss: 0.00001955
Iteration 43/1000 | Loss: 0.00001955
Iteration 44/1000 | Loss: 0.00001954
Iteration 45/1000 | Loss: 0.00001954
Iteration 46/1000 | Loss: 0.00001953
Iteration 47/1000 | Loss: 0.00001953
Iteration 48/1000 | Loss: 0.00001953
Iteration 49/1000 | Loss: 0.00001953
Iteration 50/1000 | Loss: 0.00001952
Iteration 51/1000 | Loss: 0.00001952
Iteration 52/1000 | Loss: 0.00001952
Iteration 53/1000 | Loss: 0.00001952
Iteration 54/1000 | Loss: 0.00001952
Iteration 55/1000 | Loss: 0.00001952
Iteration 56/1000 | Loss: 0.00001952
Iteration 57/1000 | Loss: 0.00001952
Iteration 58/1000 | Loss: 0.00001952
Iteration 59/1000 | Loss: 0.00001952
Iteration 60/1000 | Loss: 0.00001952
Iteration 61/1000 | Loss: 0.00001952
Iteration 62/1000 | Loss: 0.00001952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [1.9519291527103633e-05, 1.9519291527103633e-05, 1.9519291527103633e-05, 1.9519291527103633e-05, 1.9519291527103633e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9519291527103633e-05

Optimization complete. Final v2v error: 3.682175874710083 mm

Highest mean error: 3.9823060035705566 mm for frame 235

Lowest mean error: 3.4503769874572754 mm for frame 125

Saving results

Total time: 34.3761682510376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00567202
Iteration 2/25 | Loss: 0.00132980
Iteration 3/25 | Loss: 0.00122901
Iteration 4/25 | Loss: 0.00121758
Iteration 5/25 | Loss: 0.00121447
Iteration 6/25 | Loss: 0.00121384
Iteration 7/25 | Loss: 0.00121384
Iteration 8/25 | Loss: 0.00121384
Iteration 9/25 | Loss: 0.00121384
Iteration 10/25 | Loss: 0.00121384
Iteration 11/25 | Loss: 0.00121384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012138362508267164, 0.0012138362508267164, 0.0012138362508267164, 0.0012138362508267164, 0.0012138362508267164]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012138362508267164

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.87180471
Iteration 2/25 | Loss: 0.00116050
Iteration 3/25 | Loss: 0.00116049
Iteration 4/25 | Loss: 0.00116049
Iteration 5/25 | Loss: 0.00116049
Iteration 6/25 | Loss: 0.00116049
Iteration 7/25 | Loss: 0.00116049
Iteration 8/25 | Loss: 0.00116049
Iteration 9/25 | Loss: 0.00116049
Iteration 10/25 | Loss: 0.00116049
Iteration 11/25 | Loss: 0.00116049
Iteration 12/25 | Loss: 0.00116049
Iteration 13/25 | Loss: 0.00116049
Iteration 14/25 | Loss: 0.00116049
Iteration 15/25 | Loss: 0.00116049
Iteration 16/25 | Loss: 0.00116049
Iteration 17/25 | Loss: 0.00116049
Iteration 18/25 | Loss: 0.00116049
Iteration 19/25 | Loss: 0.00116049
Iteration 20/25 | Loss: 0.00116049
Iteration 21/25 | Loss: 0.00116049
Iteration 22/25 | Loss: 0.00116049
Iteration 23/25 | Loss: 0.00116049
Iteration 24/25 | Loss: 0.00116049
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011604888131842017, 0.0011604888131842017, 0.0011604888131842017, 0.0011604888131842017, 0.0011604888131842017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011604888131842017

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116049
Iteration 2/1000 | Loss: 0.00002923
Iteration 3/1000 | Loss: 0.00001700
Iteration 4/1000 | Loss: 0.00001526
Iteration 5/1000 | Loss: 0.00001460
Iteration 6/1000 | Loss: 0.00001421
Iteration 7/1000 | Loss: 0.00001382
Iteration 8/1000 | Loss: 0.00001349
Iteration 9/1000 | Loss: 0.00001345
Iteration 10/1000 | Loss: 0.00001344
Iteration 11/1000 | Loss: 0.00001321
Iteration 12/1000 | Loss: 0.00001293
Iteration 13/1000 | Loss: 0.00001274
Iteration 14/1000 | Loss: 0.00001271
Iteration 15/1000 | Loss: 0.00001263
Iteration 16/1000 | Loss: 0.00001263
Iteration 17/1000 | Loss: 0.00001260
Iteration 18/1000 | Loss: 0.00001258
Iteration 19/1000 | Loss: 0.00001258
Iteration 20/1000 | Loss: 0.00001255
Iteration 21/1000 | Loss: 0.00001252
Iteration 22/1000 | Loss: 0.00001252
Iteration 23/1000 | Loss: 0.00001251
Iteration 24/1000 | Loss: 0.00001250
Iteration 25/1000 | Loss: 0.00001250
Iteration 26/1000 | Loss: 0.00001249
Iteration 27/1000 | Loss: 0.00001248
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001248
Iteration 30/1000 | Loss: 0.00001248
Iteration 31/1000 | Loss: 0.00001248
Iteration 32/1000 | Loss: 0.00001248
Iteration 33/1000 | Loss: 0.00001248
Iteration 34/1000 | Loss: 0.00001247
Iteration 35/1000 | Loss: 0.00001247
Iteration 36/1000 | Loss: 0.00001246
Iteration 37/1000 | Loss: 0.00001246
Iteration 38/1000 | Loss: 0.00001245
Iteration 39/1000 | Loss: 0.00001245
Iteration 40/1000 | Loss: 0.00001244
Iteration 41/1000 | Loss: 0.00001244
Iteration 42/1000 | Loss: 0.00001244
Iteration 43/1000 | Loss: 0.00001241
Iteration 44/1000 | Loss: 0.00001241
Iteration 45/1000 | Loss: 0.00001241
Iteration 46/1000 | Loss: 0.00001240
Iteration 47/1000 | Loss: 0.00001240
Iteration 48/1000 | Loss: 0.00001240
Iteration 49/1000 | Loss: 0.00001240
Iteration 50/1000 | Loss: 0.00001240
Iteration 51/1000 | Loss: 0.00001240
Iteration 52/1000 | Loss: 0.00001240
Iteration 53/1000 | Loss: 0.00001240
Iteration 54/1000 | Loss: 0.00001240
Iteration 55/1000 | Loss: 0.00001239
Iteration 56/1000 | Loss: 0.00001239
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001237
Iteration 61/1000 | Loss: 0.00001237
Iteration 62/1000 | Loss: 0.00001237
Iteration 63/1000 | Loss: 0.00001237
Iteration 64/1000 | Loss: 0.00001237
Iteration 65/1000 | Loss: 0.00001237
Iteration 66/1000 | Loss: 0.00001237
Iteration 67/1000 | Loss: 0.00001236
Iteration 68/1000 | Loss: 0.00001236
Iteration 69/1000 | Loss: 0.00001235
Iteration 70/1000 | Loss: 0.00001235
Iteration 71/1000 | Loss: 0.00001234
Iteration 72/1000 | Loss: 0.00001234
Iteration 73/1000 | Loss: 0.00001234
Iteration 74/1000 | Loss: 0.00001234
Iteration 75/1000 | Loss: 0.00001233
Iteration 76/1000 | Loss: 0.00001233
Iteration 77/1000 | Loss: 0.00001233
Iteration 78/1000 | Loss: 0.00001233
Iteration 79/1000 | Loss: 0.00001232
Iteration 80/1000 | Loss: 0.00001232
Iteration 81/1000 | Loss: 0.00001232
Iteration 82/1000 | Loss: 0.00001232
Iteration 83/1000 | Loss: 0.00001232
Iteration 84/1000 | Loss: 0.00001232
Iteration 85/1000 | Loss: 0.00001232
Iteration 86/1000 | Loss: 0.00001232
Iteration 87/1000 | Loss: 0.00001232
Iteration 88/1000 | Loss: 0.00001232
Iteration 89/1000 | Loss: 0.00001231
Iteration 90/1000 | Loss: 0.00001231
Iteration 91/1000 | Loss: 0.00001231
Iteration 92/1000 | Loss: 0.00001231
Iteration 93/1000 | Loss: 0.00001231
Iteration 94/1000 | Loss: 0.00001231
Iteration 95/1000 | Loss: 0.00001231
Iteration 96/1000 | Loss: 0.00001231
Iteration 97/1000 | Loss: 0.00001231
Iteration 98/1000 | Loss: 0.00001231
Iteration 99/1000 | Loss: 0.00001231
Iteration 100/1000 | Loss: 0.00001230
Iteration 101/1000 | Loss: 0.00001230
Iteration 102/1000 | Loss: 0.00001230
Iteration 103/1000 | Loss: 0.00001230
Iteration 104/1000 | Loss: 0.00001230
Iteration 105/1000 | Loss: 0.00001230
Iteration 106/1000 | Loss: 0.00001230
Iteration 107/1000 | Loss: 0.00001230
Iteration 108/1000 | Loss: 0.00001230
Iteration 109/1000 | Loss: 0.00001230
Iteration 110/1000 | Loss: 0.00001230
Iteration 111/1000 | Loss: 0.00001230
Iteration 112/1000 | Loss: 0.00001230
Iteration 113/1000 | Loss: 0.00001230
Iteration 114/1000 | Loss: 0.00001230
Iteration 115/1000 | Loss: 0.00001230
Iteration 116/1000 | Loss: 0.00001230
Iteration 117/1000 | Loss: 0.00001230
Iteration 118/1000 | Loss: 0.00001230
Iteration 119/1000 | Loss: 0.00001230
Iteration 120/1000 | Loss: 0.00001230
Iteration 121/1000 | Loss: 0.00001230
Iteration 122/1000 | Loss: 0.00001230
Iteration 123/1000 | Loss: 0.00001230
Iteration 124/1000 | Loss: 0.00001230
Iteration 125/1000 | Loss: 0.00001230
Iteration 126/1000 | Loss: 0.00001230
Iteration 127/1000 | Loss: 0.00001230
Iteration 128/1000 | Loss: 0.00001230
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001230
Iteration 131/1000 | Loss: 0.00001230
Iteration 132/1000 | Loss: 0.00001230
Iteration 133/1000 | Loss: 0.00001230
Iteration 134/1000 | Loss: 0.00001230
Iteration 135/1000 | Loss: 0.00001230
Iteration 136/1000 | Loss: 0.00001230
Iteration 137/1000 | Loss: 0.00001230
Iteration 138/1000 | Loss: 0.00001230
Iteration 139/1000 | Loss: 0.00001230
Iteration 140/1000 | Loss: 0.00001230
Iteration 141/1000 | Loss: 0.00001230
Iteration 142/1000 | Loss: 0.00001230
Iteration 143/1000 | Loss: 0.00001230
Iteration 144/1000 | Loss: 0.00001230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.2298703950364143e-05, 1.2298703950364143e-05, 1.2298703950364143e-05, 1.2298703950364143e-05, 1.2298703950364143e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2298703950364143e-05

Optimization complete. Final v2v error: 2.995842933654785 mm

Highest mean error: 3.2475428581237793 mm for frame 98

Lowest mean error: 2.7670228481292725 mm for frame 41

Saving results

Total time: 33.37777400016785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00942455
Iteration 2/25 | Loss: 0.00304161
Iteration 3/25 | Loss: 0.00229092
Iteration 4/25 | Loss: 0.00206617
Iteration 5/25 | Loss: 0.00197806
Iteration 6/25 | Loss: 0.00186889
Iteration 7/25 | Loss: 0.00183383
Iteration 8/25 | Loss: 0.00178797
Iteration 9/25 | Loss: 0.00172415
Iteration 10/25 | Loss: 0.00173436
Iteration 11/25 | Loss: 0.00164569
Iteration 12/25 | Loss: 0.00160649
Iteration 13/25 | Loss: 0.00161253
Iteration 14/25 | Loss: 0.00160352
Iteration 15/25 | Loss: 0.00160416
Iteration 16/25 | Loss: 0.00157216
Iteration 17/25 | Loss: 0.00157287
Iteration 18/25 | Loss: 0.00157235
Iteration 19/25 | Loss: 0.00156209
Iteration 20/25 | Loss: 0.00155792
Iteration 21/25 | Loss: 0.00155542
Iteration 22/25 | Loss: 0.00155497
Iteration 23/25 | Loss: 0.00154932
Iteration 24/25 | Loss: 0.00154888
Iteration 25/25 | Loss: 0.00155802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25900304
Iteration 2/25 | Loss: 0.00399026
Iteration 3/25 | Loss: 0.00365319
Iteration 4/25 | Loss: 0.00365319
Iteration 5/25 | Loss: 0.00365319
Iteration 6/25 | Loss: 0.00365319
Iteration 7/25 | Loss: 0.00365319
Iteration 8/25 | Loss: 0.00365319
Iteration 9/25 | Loss: 0.00365319
Iteration 10/25 | Loss: 0.00365318
Iteration 11/25 | Loss: 0.00365318
Iteration 12/25 | Loss: 0.00365318
Iteration 13/25 | Loss: 0.00365318
Iteration 14/25 | Loss: 0.00365318
Iteration 15/25 | Loss: 0.00365318
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003653184976428747, 0.003653184976428747, 0.003653184976428747, 0.003653184976428747, 0.003653184976428747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003653184976428747

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00365318
Iteration 2/1000 | Loss: 0.00104180
Iteration 3/1000 | Loss: 0.00093107
Iteration 4/1000 | Loss: 0.00155217
Iteration 5/1000 | Loss: 0.00196379
Iteration 6/1000 | Loss: 0.00054428
Iteration 7/1000 | Loss: 0.00052080
Iteration 8/1000 | Loss: 0.00036658
Iteration 9/1000 | Loss: 0.00025405
Iteration 10/1000 | Loss: 0.00107116
Iteration 11/1000 | Loss: 0.00034878
Iteration 12/1000 | Loss: 0.00029713
Iteration 13/1000 | Loss: 0.00027205
Iteration 14/1000 | Loss: 0.00044003
Iteration 15/1000 | Loss: 0.00116388
Iteration 16/1000 | Loss: 0.00060237
Iteration 17/1000 | Loss: 0.00083847
Iteration 18/1000 | Loss: 0.00372764
Iteration 19/1000 | Loss: 0.00285930
Iteration 20/1000 | Loss: 0.00043881
Iteration 21/1000 | Loss: 0.00023487
Iteration 22/1000 | Loss: 0.00031103
Iteration 23/1000 | Loss: 0.00023956
Iteration 24/1000 | Loss: 0.00041287
Iteration 25/1000 | Loss: 0.00037443
Iteration 26/1000 | Loss: 0.00041215
Iteration 27/1000 | Loss: 0.00054517
Iteration 28/1000 | Loss: 0.00057177
Iteration 29/1000 | Loss: 0.00020314
Iteration 30/1000 | Loss: 0.00016540
Iteration 31/1000 | Loss: 0.00023862
Iteration 32/1000 | Loss: 0.00086608
Iteration 33/1000 | Loss: 0.00132195
Iteration 34/1000 | Loss: 0.00217746
Iteration 35/1000 | Loss: 0.00065346
Iteration 36/1000 | Loss: 0.00015906
Iteration 37/1000 | Loss: 0.00017524
Iteration 38/1000 | Loss: 0.00013969
Iteration 39/1000 | Loss: 0.00023999
Iteration 40/1000 | Loss: 0.00013770
Iteration 41/1000 | Loss: 0.00020978
Iteration 42/1000 | Loss: 0.00172913
Iteration 43/1000 | Loss: 0.00047136
Iteration 44/1000 | Loss: 0.00032107
Iteration 45/1000 | Loss: 0.00034956
Iteration 46/1000 | Loss: 0.00015269
Iteration 47/1000 | Loss: 0.00013794
Iteration 48/1000 | Loss: 0.00022695
Iteration 49/1000 | Loss: 0.00039727
Iteration 50/1000 | Loss: 0.00099235
Iteration 51/1000 | Loss: 0.00078617
Iteration 52/1000 | Loss: 0.00032733
Iteration 53/1000 | Loss: 0.00015546
Iteration 54/1000 | Loss: 0.00013454
Iteration 55/1000 | Loss: 0.00013284
Iteration 56/1000 | Loss: 0.00013135
Iteration 57/1000 | Loss: 0.00019809
Iteration 58/1000 | Loss: 0.00020889
Iteration 59/1000 | Loss: 0.00084953
Iteration 60/1000 | Loss: 0.00032152
Iteration 61/1000 | Loss: 0.00026896
Iteration 62/1000 | Loss: 0.00012998
Iteration 63/1000 | Loss: 0.00012821
Iteration 64/1000 | Loss: 0.00015069
Iteration 65/1000 | Loss: 0.00037468
Iteration 66/1000 | Loss: 0.00018705
Iteration 67/1000 | Loss: 0.00015563
Iteration 68/1000 | Loss: 0.00013499
Iteration 69/1000 | Loss: 0.00012674
Iteration 70/1000 | Loss: 0.00013456
Iteration 71/1000 | Loss: 0.00013266
Iteration 72/1000 | Loss: 0.00024813
Iteration 73/1000 | Loss: 0.00013054
Iteration 74/1000 | Loss: 0.00013876
Iteration 75/1000 | Loss: 0.00014009
Iteration 76/1000 | Loss: 0.00012439
Iteration 77/1000 | Loss: 0.00013015
Iteration 78/1000 | Loss: 0.00037178
Iteration 79/1000 | Loss: 0.00048613
Iteration 80/1000 | Loss: 0.00012546
Iteration 81/1000 | Loss: 0.00068562
Iteration 82/1000 | Loss: 0.00235525
Iteration 83/1000 | Loss: 0.00249893
Iteration 84/1000 | Loss: 0.00027280
Iteration 85/1000 | Loss: 0.00019291
Iteration 86/1000 | Loss: 0.00020169
Iteration 87/1000 | Loss: 0.00015278
Iteration 88/1000 | Loss: 0.00010311
Iteration 89/1000 | Loss: 0.00048834
Iteration 90/1000 | Loss: 0.00089168
Iteration 91/1000 | Loss: 0.00008597
Iteration 92/1000 | Loss: 0.00019163
Iteration 93/1000 | Loss: 0.00031677
Iteration 94/1000 | Loss: 0.00007582
Iteration 95/1000 | Loss: 0.00007253
Iteration 96/1000 | Loss: 0.00007053
Iteration 97/1000 | Loss: 0.00006915
Iteration 98/1000 | Loss: 0.00006775
Iteration 99/1000 | Loss: 0.00010614
Iteration 100/1000 | Loss: 0.00006629
Iteration 101/1000 | Loss: 0.00006556
Iteration 102/1000 | Loss: 0.00006516
Iteration 103/1000 | Loss: 0.00006481
Iteration 104/1000 | Loss: 0.00006453
Iteration 105/1000 | Loss: 0.00006433
Iteration 106/1000 | Loss: 0.00006426
Iteration 107/1000 | Loss: 0.00015740
Iteration 108/1000 | Loss: 0.00024406
Iteration 109/1000 | Loss: 0.00006422
Iteration 110/1000 | Loss: 0.00006413
Iteration 111/1000 | Loss: 0.00006411
Iteration 112/1000 | Loss: 0.00006411
Iteration 113/1000 | Loss: 0.00006411
Iteration 114/1000 | Loss: 0.00006411
Iteration 115/1000 | Loss: 0.00006411
Iteration 116/1000 | Loss: 0.00006411
Iteration 117/1000 | Loss: 0.00006411
Iteration 118/1000 | Loss: 0.00006411
Iteration 119/1000 | Loss: 0.00006411
Iteration 120/1000 | Loss: 0.00006411
Iteration 121/1000 | Loss: 0.00006411
Iteration 122/1000 | Loss: 0.00006410
Iteration 123/1000 | Loss: 0.00006410
Iteration 124/1000 | Loss: 0.00006410
Iteration 125/1000 | Loss: 0.00006409
Iteration 126/1000 | Loss: 0.00006409
Iteration 127/1000 | Loss: 0.00006409
Iteration 128/1000 | Loss: 0.00006409
Iteration 129/1000 | Loss: 0.00006409
Iteration 130/1000 | Loss: 0.00006408
Iteration 131/1000 | Loss: 0.00006408
Iteration 132/1000 | Loss: 0.00006408
Iteration 133/1000 | Loss: 0.00006408
Iteration 134/1000 | Loss: 0.00006408
Iteration 135/1000 | Loss: 0.00006408
Iteration 136/1000 | Loss: 0.00006408
Iteration 137/1000 | Loss: 0.00006407
Iteration 138/1000 | Loss: 0.00006407
Iteration 139/1000 | Loss: 0.00006407
Iteration 140/1000 | Loss: 0.00006407
Iteration 141/1000 | Loss: 0.00015610
Iteration 142/1000 | Loss: 0.00027773
Iteration 143/1000 | Loss: 0.00006428
Iteration 144/1000 | Loss: 0.00006409
Iteration 145/1000 | Loss: 0.00006409
Iteration 146/1000 | Loss: 0.00006405
Iteration 147/1000 | Loss: 0.00006405
Iteration 148/1000 | Loss: 0.00006405
Iteration 149/1000 | Loss: 0.00006405
Iteration 150/1000 | Loss: 0.00006405
Iteration 151/1000 | Loss: 0.00006405
Iteration 152/1000 | Loss: 0.00006405
Iteration 153/1000 | Loss: 0.00006404
Iteration 154/1000 | Loss: 0.00006404
Iteration 155/1000 | Loss: 0.00015338
Iteration 156/1000 | Loss: 0.00007281
Iteration 157/1000 | Loss: 0.00006447
Iteration 158/1000 | Loss: 0.00006416
Iteration 159/1000 | Loss: 0.00006413
Iteration 160/1000 | Loss: 0.00006413
Iteration 161/1000 | Loss: 0.00006413
Iteration 162/1000 | Loss: 0.00006413
Iteration 163/1000 | Loss: 0.00006413
Iteration 164/1000 | Loss: 0.00006413
Iteration 165/1000 | Loss: 0.00006412
Iteration 166/1000 | Loss: 0.00006412
Iteration 167/1000 | Loss: 0.00006412
Iteration 168/1000 | Loss: 0.00006412
Iteration 169/1000 | Loss: 0.00006412
Iteration 170/1000 | Loss: 0.00006801
Iteration 171/1000 | Loss: 0.00006414
Iteration 172/1000 | Loss: 0.00006414
Iteration 173/1000 | Loss: 0.00006414
Iteration 174/1000 | Loss: 0.00006414
Iteration 175/1000 | Loss: 0.00006414
Iteration 176/1000 | Loss: 0.00006414
Iteration 177/1000 | Loss: 0.00006414
Iteration 178/1000 | Loss: 0.00006414
Iteration 179/1000 | Loss: 0.00006414
Iteration 180/1000 | Loss: 0.00006414
Iteration 181/1000 | Loss: 0.00006414
Iteration 182/1000 | Loss: 0.00006413
Iteration 183/1000 | Loss: 0.00006413
Iteration 184/1000 | Loss: 0.00006412
Iteration 185/1000 | Loss: 0.00006410
Iteration 186/1000 | Loss: 0.00006409
Iteration 187/1000 | Loss: 0.00006408
Iteration 188/1000 | Loss: 0.00006408
Iteration 189/1000 | Loss: 0.00006408
Iteration 190/1000 | Loss: 0.00006408
Iteration 191/1000 | Loss: 0.00006408
Iteration 192/1000 | Loss: 0.00006408
Iteration 193/1000 | Loss: 0.00006407
Iteration 194/1000 | Loss: 0.00006407
Iteration 195/1000 | Loss: 0.00006407
Iteration 196/1000 | Loss: 0.00006407
Iteration 197/1000 | Loss: 0.00006407
Iteration 198/1000 | Loss: 0.00006406
Iteration 199/1000 | Loss: 0.00006406
Iteration 200/1000 | Loss: 0.00006406
Iteration 201/1000 | Loss: 0.00006405
Iteration 202/1000 | Loss: 0.00006405
Iteration 203/1000 | Loss: 0.00006405
Iteration 204/1000 | Loss: 0.00006405
Iteration 205/1000 | Loss: 0.00006405
Iteration 206/1000 | Loss: 0.00006405
Iteration 207/1000 | Loss: 0.00006405
Iteration 208/1000 | Loss: 0.00006405
Iteration 209/1000 | Loss: 0.00006405
Iteration 210/1000 | Loss: 0.00006405
Iteration 211/1000 | Loss: 0.00006404
Iteration 212/1000 | Loss: 0.00006404
Iteration 213/1000 | Loss: 0.00006404
Iteration 214/1000 | Loss: 0.00006403
Iteration 215/1000 | Loss: 0.00006403
Iteration 216/1000 | Loss: 0.00006403
Iteration 217/1000 | Loss: 0.00006403
Iteration 218/1000 | Loss: 0.00006402
Iteration 219/1000 | Loss: 0.00006402
Iteration 220/1000 | Loss: 0.00006402
Iteration 221/1000 | Loss: 0.00006401
Iteration 222/1000 | Loss: 0.00006401
Iteration 223/1000 | Loss: 0.00006401
Iteration 224/1000 | Loss: 0.00006401
Iteration 225/1000 | Loss: 0.00006401
Iteration 226/1000 | Loss: 0.00006400
Iteration 227/1000 | Loss: 0.00006400
Iteration 228/1000 | Loss: 0.00006400
Iteration 229/1000 | Loss: 0.00006400
Iteration 230/1000 | Loss: 0.00006400
Iteration 231/1000 | Loss: 0.00006400
Iteration 232/1000 | Loss: 0.00006400
Iteration 233/1000 | Loss: 0.00006400
Iteration 234/1000 | Loss: 0.00006400
Iteration 235/1000 | Loss: 0.00006400
Iteration 236/1000 | Loss: 0.00006400
Iteration 237/1000 | Loss: 0.00006400
Iteration 238/1000 | Loss: 0.00006400
Iteration 239/1000 | Loss: 0.00006400
Iteration 240/1000 | Loss: 0.00006400
Iteration 241/1000 | Loss: 0.00006400
Iteration 242/1000 | Loss: 0.00006400
Iteration 243/1000 | Loss: 0.00006400
Iteration 244/1000 | Loss: 0.00006399
Iteration 245/1000 | Loss: 0.00006399
Iteration 246/1000 | Loss: 0.00006399
Iteration 247/1000 | Loss: 0.00006399
Iteration 248/1000 | Loss: 0.00006399
Iteration 249/1000 | Loss: 0.00006399
Iteration 250/1000 | Loss: 0.00006399
Iteration 251/1000 | Loss: 0.00006399
Iteration 252/1000 | Loss: 0.00006399
Iteration 253/1000 | Loss: 0.00006399
Iteration 254/1000 | Loss: 0.00006399
Iteration 255/1000 | Loss: 0.00006399
Iteration 256/1000 | Loss: 0.00006399
Iteration 257/1000 | Loss: 0.00006399
Iteration 258/1000 | Loss: 0.00006399
Iteration 259/1000 | Loss: 0.00006399
Iteration 260/1000 | Loss: 0.00006399
Iteration 261/1000 | Loss: 0.00006399
Iteration 262/1000 | Loss: 0.00006398
Iteration 263/1000 | Loss: 0.00006398
Iteration 264/1000 | Loss: 0.00006398
Iteration 265/1000 | Loss: 0.00006398
Iteration 266/1000 | Loss: 0.00006398
Iteration 267/1000 | Loss: 0.00006398
Iteration 268/1000 | Loss: 0.00006398
Iteration 269/1000 | Loss: 0.00006398
Iteration 270/1000 | Loss: 0.00006398
Iteration 271/1000 | Loss: 0.00006398
Iteration 272/1000 | Loss: 0.00006398
Iteration 273/1000 | Loss: 0.00006398
Iteration 274/1000 | Loss: 0.00006398
Iteration 275/1000 | Loss: 0.00006398
Iteration 276/1000 | Loss: 0.00006398
Iteration 277/1000 | Loss: 0.00006398
Iteration 278/1000 | Loss: 0.00006398
Iteration 279/1000 | Loss: 0.00006398
Iteration 280/1000 | Loss: 0.00006398
Iteration 281/1000 | Loss: 0.00006398
Iteration 282/1000 | Loss: 0.00006397
Iteration 283/1000 | Loss: 0.00006397
Iteration 284/1000 | Loss: 0.00006397
Iteration 285/1000 | Loss: 0.00006397
Iteration 286/1000 | Loss: 0.00006397
Iteration 287/1000 | Loss: 0.00006397
Iteration 288/1000 | Loss: 0.00006397
Iteration 289/1000 | Loss: 0.00006397
Iteration 290/1000 | Loss: 0.00006397
Iteration 291/1000 | Loss: 0.00006397
Iteration 292/1000 | Loss: 0.00006397
Iteration 293/1000 | Loss: 0.00006397
Iteration 294/1000 | Loss: 0.00006397
Iteration 295/1000 | Loss: 0.00006397
Iteration 296/1000 | Loss: 0.00006396
Iteration 297/1000 | Loss: 0.00006396
Iteration 298/1000 | Loss: 0.00006396
Iteration 299/1000 | Loss: 0.00006396
Iteration 300/1000 | Loss: 0.00006396
Iteration 301/1000 | Loss: 0.00006396
Iteration 302/1000 | Loss: 0.00006396
Iteration 303/1000 | Loss: 0.00006396
Iteration 304/1000 | Loss: 0.00006396
Iteration 305/1000 | Loss: 0.00006396
Iteration 306/1000 | Loss: 0.00006396
Iteration 307/1000 | Loss: 0.00006396
Iteration 308/1000 | Loss: 0.00006396
Iteration 309/1000 | Loss: 0.00006396
Iteration 310/1000 | Loss: 0.00006396
Iteration 311/1000 | Loss: 0.00006396
Iteration 312/1000 | Loss: 0.00006396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 312. Stopping optimization.
Last 5 losses: [6.39604331809096e-05, 6.39604331809096e-05, 6.39604331809096e-05, 6.39604331809096e-05, 6.39604331809096e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.39604331809096e-05

Optimization complete. Final v2v error: 4.345877170562744 mm

Highest mean error: 10.30590534210205 mm for frame 65

Lowest mean error: 2.8406074047088623 mm for frame 16

Saving results

Total time: 213.84409308433533
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416462
Iteration 2/25 | Loss: 0.00134142
Iteration 3/25 | Loss: 0.00122643
Iteration 4/25 | Loss: 0.00121299
Iteration 5/25 | Loss: 0.00120892
Iteration 6/25 | Loss: 0.00120776
Iteration 7/25 | Loss: 0.00120744
Iteration 8/25 | Loss: 0.00120744
Iteration 9/25 | Loss: 0.00120744
Iteration 10/25 | Loss: 0.00120744
Iteration 11/25 | Loss: 0.00120744
Iteration 12/25 | Loss: 0.00120744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012074434198439121, 0.0012074434198439121, 0.0012074434198439121, 0.0012074434198439121, 0.0012074434198439121]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012074434198439121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44217396
Iteration 2/25 | Loss: 0.00133026
Iteration 3/25 | Loss: 0.00133026
Iteration 4/25 | Loss: 0.00133026
Iteration 5/25 | Loss: 0.00133026
Iteration 6/25 | Loss: 0.00133026
Iteration 7/25 | Loss: 0.00133026
Iteration 8/25 | Loss: 0.00133026
Iteration 9/25 | Loss: 0.00133026
Iteration 10/25 | Loss: 0.00133026
Iteration 11/25 | Loss: 0.00133026
Iteration 12/25 | Loss: 0.00133026
Iteration 13/25 | Loss: 0.00133026
Iteration 14/25 | Loss: 0.00133026
Iteration 15/25 | Loss: 0.00133026
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013302561128512025, 0.0013302561128512025, 0.0013302561128512025, 0.0013302561128512025, 0.0013302561128512025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013302561128512025

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133026
Iteration 2/1000 | Loss: 0.00003014
Iteration 3/1000 | Loss: 0.00001952
Iteration 4/1000 | Loss: 0.00001656
Iteration 5/1000 | Loss: 0.00001535
Iteration 6/1000 | Loss: 0.00001463
Iteration 7/1000 | Loss: 0.00001419
Iteration 8/1000 | Loss: 0.00001381
Iteration 9/1000 | Loss: 0.00001349
Iteration 10/1000 | Loss: 0.00001327
Iteration 11/1000 | Loss: 0.00001307
Iteration 12/1000 | Loss: 0.00001292
Iteration 13/1000 | Loss: 0.00001283
Iteration 14/1000 | Loss: 0.00001275
Iteration 15/1000 | Loss: 0.00001272
Iteration 16/1000 | Loss: 0.00001270
Iteration 17/1000 | Loss: 0.00001269
Iteration 18/1000 | Loss: 0.00001269
Iteration 19/1000 | Loss: 0.00001268
Iteration 20/1000 | Loss: 0.00001267
Iteration 21/1000 | Loss: 0.00001262
Iteration 22/1000 | Loss: 0.00001259
Iteration 23/1000 | Loss: 0.00001259
Iteration 24/1000 | Loss: 0.00001259
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001257
Iteration 27/1000 | Loss: 0.00001256
Iteration 28/1000 | Loss: 0.00001255
Iteration 29/1000 | Loss: 0.00001254
Iteration 30/1000 | Loss: 0.00001254
Iteration 31/1000 | Loss: 0.00001252
Iteration 32/1000 | Loss: 0.00001251
Iteration 33/1000 | Loss: 0.00001251
Iteration 34/1000 | Loss: 0.00001250
Iteration 35/1000 | Loss: 0.00001249
Iteration 36/1000 | Loss: 0.00001249
Iteration 37/1000 | Loss: 0.00001248
Iteration 38/1000 | Loss: 0.00001247
Iteration 39/1000 | Loss: 0.00001247
Iteration 40/1000 | Loss: 0.00001247
Iteration 41/1000 | Loss: 0.00001246
Iteration 42/1000 | Loss: 0.00001246
Iteration 43/1000 | Loss: 0.00001245
Iteration 44/1000 | Loss: 0.00001244
Iteration 45/1000 | Loss: 0.00001244
Iteration 46/1000 | Loss: 0.00001244
Iteration 47/1000 | Loss: 0.00001243
Iteration 48/1000 | Loss: 0.00001243
Iteration 49/1000 | Loss: 0.00001243
Iteration 50/1000 | Loss: 0.00001242
Iteration 51/1000 | Loss: 0.00001242
Iteration 52/1000 | Loss: 0.00001242
Iteration 53/1000 | Loss: 0.00001241
Iteration 54/1000 | Loss: 0.00001241
Iteration 55/1000 | Loss: 0.00001241
Iteration 56/1000 | Loss: 0.00001241
Iteration 57/1000 | Loss: 0.00001240
Iteration 58/1000 | Loss: 0.00001240
Iteration 59/1000 | Loss: 0.00001239
Iteration 60/1000 | Loss: 0.00001239
Iteration 61/1000 | Loss: 0.00001239
Iteration 62/1000 | Loss: 0.00001238
Iteration 63/1000 | Loss: 0.00001238
Iteration 64/1000 | Loss: 0.00001238
Iteration 65/1000 | Loss: 0.00001238
Iteration 66/1000 | Loss: 0.00001238
Iteration 67/1000 | Loss: 0.00001238
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001237
Iteration 70/1000 | Loss: 0.00001237
Iteration 71/1000 | Loss: 0.00001236
Iteration 72/1000 | Loss: 0.00001236
Iteration 73/1000 | Loss: 0.00001236
Iteration 74/1000 | Loss: 0.00001235
Iteration 75/1000 | Loss: 0.00001235
Iteration 76/1000 | Loss: 0.00001235
Iteration 77/1000 | Loss: 0.00001234
Iteration 78/1000 | Loss: 0.00001234
Iteration 79/1000 | Loss: 0.00001234
Iteration 80/1000 | Loss: 0.00001233
Iteration 81/1000 | Loss: 0.00001233
Iteration 82/1000 | Loss: 0.00001233
Iteration 83/1000 | Loss: 0.00001233
Iteration 84/1000 | Loss: 0.00001233
Iteration 85/1000 | Loss: 0.00001233
Iteration 86/1000 | Loss: 0.00001233
Iteration 87/1000 | Loss: 0.00001232
Iteration 88/1000 | Loss: 0.00001232
Iteration 89/1000 | Loss: 0.00001232
Iteration 90/1000 | Loss: 0.00001232
Iteration 91/1000 | Loss: 0.00001232
Iteration 92/1000 | Loss: 0.00001232
Iteration 93/1000 | Loss: 0.00001232
Iteration 94/1000 | Loss: 0.00001231
Iteration 95/1000 | Loss: 0.00001231
Iteration 96/1000 | Loss: 0.00001230
Iteration 97/1000 | Loss: 0.00001230
Iteration 98/1000 | Loss: 0.00001230
Iteration 99/1000 | Loss: 0.00001229
Iteration 100/1000 | Loss: 0.00001229
Iteration 101/1000 | Loss: 0.00001229
Iteration 102/1000 | Loss: 0.00001229
Iteration 103/1000 | Loss: 0.00001229
Iteration 104/1000 | Loss: 0.00001229
Iteration 105/1000 | Loss: 0.00001229
Iteration 106/1000 | Loss: 0.00001229
Iteration 107/1000 | Loss: 0.00001229
Iteration 108/1000 | Loss: 0.00001229
Iteration 109/1000 | Loss: 0.00001229
Iteration 110/1000 | Loss: 0.00001228
Iteration 111/1000 | Loss: 0.00001228
Iteration 112/1000 | Loss: 0.00001228
Iteration 113/1000 | Loss: 0.00001228
Iteration 114/1000 | Loss: 0.00001228
Iteration 115/1000 | Loss: 0.00001228
Iteration 116/1000 | Loss: 0.00001228
Iteration 117/1000 | Loss: 0.00001228
Iteration 118/1000 | Loss: 0.00001228
Iteration 119/1000 | Loss: 0.00001228
Iteration 120/1000 | Loss: 0.00001228
Iteration 121/1000 | Loss: 0.00001227
Iteration 122/1000 | Loss: 0.00001227
Iteration 123/1000 | Loss: 0.00001227
Iteration 124/1000 | Loss: 0.00001227
Iteration 125/1000 | Loss: 0.00001227
Iteration 126/1000 | Loss: 0.00001227
Iteration 127/1000 | Loss: 0.00001227
Iteration 128/1000 | Loss: 0.00001227
Iteration 129/1000 | Loss: 0.00001227
Iteration 130/1000 | Loss: 0.00001227
Iteration 131/1000 | Loss: 0.00001226
Iteration 132/1000 | Loss: 0.00001226
Iteration 133/1000 | Loss: 0.00001226
Iteration 134/1000 | Loss: 0.00001226
Iteration 135/1000 | Loss: 0.00001226
Iteration 136/1000 | Loss: 0.00001226
Iteration 137/1000 | Loss: 0.00001225
Iteration 138/1000 | Loss: 0.00001225
Iteration 139/1000 | Loss: 0.00001225
Iteration 140/1000 | Loss: 0.00001225
Iteration 141/1000 | Loss: 0.00001225
Iteration 142/1000 | Loss: 0.00001225
Iteration 143/1000 | Loss: 0.00001225
Iteration 144/1000 | Loss: 0.00001225
Iteration 145/1000 | Loss: 0.00001225
Iteration 146/1000 | Loss: 0.00001224
Iteration 147/1000 | Loss: 0.00001224
Iteration 148/1000 | Loss: 0.00001224
Iteration 149/1000 | Loss: 0.00001224
Iteration 150/1000 | Loss: 0.00001224
Iteration 151/1000 | Loss: 0.00001224
Iteration 152/1000 | Loss: 0.00001224
Iteration 153/1000 | Loss: 0.00001224
Iteration 154/1000 | Loss: 0.00001224
Iteration 155/1000 | Loss: 0.00001224
Iteration 156/1000 | Loss: 0.00001224
Iteration 157/1000 | Loss: 0.00001224
Iteration 158/1000 | Loss: 0.00001224
Iteration 159/1000 | Loss: 0.00001224
Iteration 160/1000 | Loss: 0.00001224
Iteration 161/1000 | Loss: 0.00001224
Iteration 162/1000 | Loss: 0.00001223
Iteration 163/1000 | Loss: 0.00001223
Iteration 164/1000 | Loss: 0.00001223
Iteration 165/1000 | Loss: 0.00001223
Iteration 166/1000 | Loss: 0.00001223
Iteration 167/1000 | Loss: 0.00001223
Iteration 168/1000 | Loss: 0.00001223
Iteration 169/1000 | Loss: 0.00001223
Iteration 170/1000 | Loss: 0.00001223
Iteration 171/1000 | Loss: 0.00001223
Iteration 172/1000 | Loss: 0.00001223
Iteration 173/1000 | Loss: 0.00001223
Iteration 174/1000 | Loss: 0.00001222
Iteration 175/1000 | Loss: 0.00001222
Iteration 176/1000 | Loss: 0.00001222
Iteration 177/1000 | Loss: 0.00001222
Iteration 178/1000 | Loss: 0.00001222
Iteration 179/1000 | Loss: 0.00001222
Iteration 180/1000 | Loss: 0.00001222
Iteration 181/1000 | Loss: 0.00001222
Iteration 182/1000 | Loss: 0.00001222
Iteration 183/1000 | Loss: 0.00001222
Iteration 184/1000 | Loss: 0.00001222
Iteration 185/1000 | Loss: 0.00001222
Iteration 186/1000 | Loss: 0.00001221
Iteration 187/1000 | Loss: 0.00001221
Iteration 188/1000 | Loss: 0.00001221
Iteration 189/1000 | Loss: 0.00001221
Iteration 190/1000 | Loss: 0.00001221
Iteration 191/1000 | Loss: 0.00001221
Iteration 192/1000 | Loss: 0.00001221
Iteration 193/1000 | Loss: 0.00001221
Iteration 194/1000 | Loss: 0.00001221
Iteration 195/1000 | Loss: 0.00001221
Iteration 196/1000 | Loss: 0.00001221
Iteration 197/1000 | Loss: 0.00001221
Iteration 198/1000 | Loss: 0.00001221
Iteration 199/1000 | Loss: 0.00001220
Iteration 200/1000 | Loss: 0.00001220
Iteration 201/1000 | Loss: 0.00001220
Iteration 202/1000 | Loss: 0.00001220
Iteration 203/1000 | Loss: 0.00001220
Iteration 204/1000 | Loss: 0.00001220
Iteration 205/1000 | Loss: 0.00001220
Iteration 206/1000 | Loss: 0.00001220
Iteration 207/1000 | Loss: 0.00001219
Iteration 208/1000 | Loss: 0.00001219
Iteration 209/1000 | Loss: 0.00001219
Iteration 210/1000 | Loss: 0.00001219
Iteration 211/1000 | Loss: 0.00001219
Iteration 212/1000 | Loss: 0.00001219
Iteration 213/1000 | Loss: 0.00001218
Iteration 214/1000 | Loss: 0.00001218
Iteration 215/1000 | Loss: 0.00001218
Iteration 216/1000 | Loss: 0.00001218
Iteration 217/1000 | Loss: 0.00001218
Iteration 218/1000 | Loss: 0.00001218
Iteration 219/1000 | Loss: 0.00001218
Iteration 220/1000 | Loss: 0.00001218
Iteration 221/1000 | Loss: 0.00001218
Iteration 222/1000 | Loss: 0.00001218
Iteration 223/1000 | Loss: 0.00001218
Iteration 224/1000 | Loss: 0.00001218
Iteration 225/1000 | Loss: 0.00001218
Iteration 226/1000 | Loss: 0.00001218
Iteration 227/1000 | Loss: 0.00001218
Iteration 228/1000 | Loss: 0.00001218
Iteration 229/1000 | Loss: 0.00001218
Iteration 230/1000 | Loss: 0.00001218
Iteration 231/1000 | Loss: 0.00001218
Iteration 232/1000 | Loss: 0.00001218
Iteration 233/1000 | Loss: 0.00001218
Iteration 234/1000 | Loss: 0.00001218
Iteration 235/1000 | Loss: 0.00001218
Iteration 236/1000 | Loss: 0.00001218
Iteration 237/1000 | Loss: 0.00001218
Iteration 238/1000 | Loss: 0.00001218
Iteration 239/1000 | Loss: 0.00001218
Iteration 240/1000 | Loss: 0.00001218
Iteration 241/1000 | Loss: 0.00001218
Iteration 242/1000 | Loss: 0.00001218
Iteration 243/1000 | Loss: 0.00001218
Iteration 244/1000 | Loss: 0.00001218
Iteration 245/1000 | Loss: 0.00001218
Iteration 246/1000 | Loss: 0.00001218
Iteration 247/1000 | Loss: 0.00001218
Iteration 248/1000 | Loss: 0.00001218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.2176250493212137e-05, 1.2176250493212137e-05, 1.2176250493212137e-05, 1.2176250493212137e-05, 1.2176250493212137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2176250493212137e-05

Optimization complete. Final v2v error: 2.9590184688568115 mm

Highest mean error: 4.086379051208496 mm for frame 39

Lowest mean error: 2.6298234462738037 mm for frame 17

Saving results

Total time: 43.517529249191284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789932
Iteration 2/25 | Loss: 0.00159421
Iteration 3/25 | Loss: 0.00134237
Iteration 4/25 | Loss: 0.00130981
Iteration 5/25 | Loss: 0.00130582
Iteration 6/25 | Loss: 0.00130508
Iteration 7/25 | Loss: 0.00130508
Iteration 8/25 | Loss: 0.00130508
Iteration 9/25 | Loss: 0.00130508
Iteration 10/25 | Loss: 0.00130508
Iteration 11/25 | Loss: 0.00130508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013050839770585299, 0.0013050839770585299, 0.0013050839770585299, 0.0013050839770585299, 0.0013050839770585299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013050839770585299

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27407181
Iteration 2/25 | Loss: 0.00182936
Iteration 3/25 | Loss: 0.00182931
Iteration 4/25 | Loss: 0.00182931
Iteration 5/25 | Loss: 0.00182931
Iteration 6/25 | Loss: 0.00182931
Iteration 7/25 | Loss: 0.00182931
Iteration 8/25 | Loss: 0.00182931
Iteration 9/25 | Loss: 0.00182931
Iteration 10/25 | Loss: 0.00182931
Iteration 11/25 | Loss: 0.00182931
Iteration 12/25 | Loss: 0.00182930
Iteration 13/25 | Loss: 0.00182930
Iteration 14/25 | Loss: 0.00182930
Iteration 15/25 | Loss: 0.00182930
Iteration 16/25 | Loss: 0.00182930
Iteration 17/25 | Loss: 0.00182930
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0018293048487976193, 0.0018293048487976193, 0.0018293048487976193, 0.0018293048487976193, 0.0018293048487976193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018293048487976193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182930
Iteration 2/1000 | Loss: 0.00005510
Iteration 3/1000 | Loss: 0.00003685
Iteration 4/1000 | Loss: 0.00002563
Iteration 5/1000 | Loss: 0.00002285
Iteration 6/1000 | Loss: 0.00002162
Iteration 7/1000 | Loss: 0.00002070
Iteration 8/1000 | Loss: 0.00002004
Iteration 9/1000 | Loss: 0.00001962
Iteration 10/1000 | Loss: 0.00001928
Iteration 11/1000 | Loss: 0.00001908
Iteration 12/1000 | Loss: 0.00001900
Iteration 13/1000 | Loss: 0.00001892
Iteration 14/1000 | Loss: 0.00001883
Iteration 15/1000 | Loss: 0.00001881
Iteration 16/1000 | Loss: 0.00001857
Iteration 17/1000 | Loss: 0.00001845
Iteration 18/1000 | Loss: 0.00001843
Iteration 19/1000 | Loss: 0.00001841
Iteration 20/1000 | Loss: 0.00001841
Iteration 21/1000 | Loss: 0.00001841
Iteration 22/1000 | Loss: 0.00001836
Iteration 23/1000 | Loss: 0.00001828
Iteration 24/1000 | Loss: 0.00001825
Iteration 25/1000 | Loss: 0.00001825
Iteration 26/1000 | Loss: 0.00001825
Iteration 27/1000 | Loss: 0.00001825
Iteration 28/1000 | Loss: 0.00001824
Iteration 29/1000 | Loss: 0.00001824
Iteration 30/1000 | Loss: 0.00001824
Iteration 31/1000 | Loss: 0.00001824
Iteration 32/1000 | Loss: 0.00001824
Iteration 33/1000 | Loss: 0.00001824
Iteration 34/1000 | Loss: 0.00001824
Iteration 35/1000 | Loss: 0.00001824
Iteration 36/1000 | Loss: 0.00001824
Iteration 37/1000 | Loss: 0.00001824
Iteration 38/1000 | Loss: 0.00001823
Iteration 39/1000 | Loss: 0.00001823
Iteration 40/1000 | Loss: 0.00001822
Iteration 41/1000 | Loss: 0.00001822
Iteration 42/1000 | Loss: 0.00001822
Iteration 43/1000 | Loss: 0.00001821
Iteration 44/1000 | Loss: 0.00001821
Iteration 45/1000 | Loss: 0.00001821
Iteration 46/1000 | Loss: 0.00001820
Iteration 47/1000 | Loss: 0.00001819
Iteration 48/1000 | Loss: 0.00001819
Iteration 49/1000 | Loss: 0.00001818
Iteration 50/1000 | Loss: 0.00001818
Iteration 51/1000 | Loss: 0.00001817
Iteration 52/1000 | Loss: 0.00001815
Iteration 53/1000 | Loss: 0.00001815
Iteration 54/1000 | Loss: 0.00001815
Iteration 55/1000 | Loss: 0.00001814
Iteration 56/1000 | Loss: 0.00001814
Iteration 57/1000 | Loss: 0.00001814
Iteration 58/1000 | Loss: 0.00001814
Iteration 59/1000 | Loss: 0.00001813
Iteration 60/1000 | Loss: 0.00001813
Iteration 61/1000 | Loss: 0.00001813
Iteration 62/1000 | Loss: 0.00001813
Iteration 63/1000 | Loss: 0.00001813
Iteration 64/1000 | Loss: 0.00001813
Iteration 65/1000 | Loss: 0.00001813
Iteration 66/1000 | Loss: 0.00001813
Iteration 67/1000 | Loss: 0.00001812
Iteration 68/1000 | Loss: 0.00001812
Iteration 69/1000 | Loss: 0.00001812
Iteration 70/1000 | Loss: 0.00001812
Iteration 71/1000 | Loss: 0.00001812
Iteration 72/1000 | Loss: 0.00001811
Iteration 73/1000 | Loss: 0.00001811
Iteration 74/1000 | Loss: 0.00001811
Iteration 75/1000 | Loss: 0.00001811
Iteration 76/1000 | Loss: 0.00001811
Iteration 77/1000 | Loss: 0.00001811
Iteration 78/1000 | Loss: 0.00001810
Iteration 79/1000 | Loss: 0.00001810
Iteration 80/1000 | Loss: 0.00001810
Iteration 81/1000 | Loss: 0.00001809
Iteration 82/1000 | Loss: 0.00001809
Iteration 83/1000 | Loss: 0.00001809
Iteration 84/1000 | Loss: 0.00001809
Iteration 85/1000 | Loss: 0.00001808
Iteration 86/1000 | Loss: 0.00001808
Iteration 87/1000 | Loss: 0.00001808
Iteration 88/1000 | Loss: 0.00001808
Iteration 89/1000 | Loss: 0.00001808
Iteration 90/1000 | Loss: 0.00001808
Iteration 91/1000 | Loss: 0.00001808
Iteration 92/1000 | Loss: 0.00001808
Iteration 93/1000 | Loss: 0.00001808
Iteration 94/1000 | Loss: 0.00001807
Iteration 95/1000 | Loss: 0.00001807
Iteration 96/1000 | Loss: 0.00001807
Iteration 97/1000 | Loss: 0.00001807
Iteration 98/1000 | Loss: 0.00001807
Iteration 99/1000 | Loss: 0.00001807
Iteration 100/1000 | Loss: 0.00001807
Iteration 101/1000 | Loss: 0.00001807
Iteration 102/1000 | Loss: 0.00001807
Iteration 103/1000 | Loss: 0.00001806
Iteration 104/1000 | Loss: 0.00001806
Iteration 105/1000 | Loss: 0.00001806
Iteration 106/1000 | Loss: 0.00001806
Iteration 107/1000 | Loss: 0.00001806
Iteration 108/1000 | Loss: 0.00001806
Iteration 109/1000 | Loss: 0.00001806
Iteration 110/1000 | Loss: 0.00001806
Iteration 111/1000 | Loss: 0.00001805
Iteration 112/1000 | Loss: 0.00001805
Iteration 113/1000 | Loss: 0.00001805
Iteration 114/1000 | Loss: 0.00001805
Iteration 115/1000 | Loss: 0.00001805
Iteration 116/1000 | Loss: 0.00001805
Iteration 117/1000 | Loss: 0.00001805
Iteration 118/1000 | Loss: 0.00001804
Iteration 119/1000 | Loss: 0.00001804
Iteration 120/1000 | Loss: 0.00001804
Iteration 121/1000 | Loss: 0.00001804
Iteration 122/1000 | Loss: 0.00001804
Iteration 123/1000 | Loss: 0.00001804
Iteration 124/1000 | Loss: 0.00001804
Iteration 125/1000 | Loss: 0.00001804
Iteration 126/1000 | Loss: 0.00001804
Iteration 127/1000 | Loss: 0.00001804
Iteration 128/1000 | Loss: 0.00001804
Iteration 129/1000 | Loss: 0.00001804
Iteration 130/1000 | Loss: 0.00001804
Iteration 131/1000 | Loss: 0.00001804
Iteration 132/1000 | Loss: 0.00001804
Iteration 133/1000 | Loss: 0.00001804
Iteration 134/1000 | Loss: 0.00001804
Iteration 135/1000 | Loss: 0.00001803
Iteration 136/1000 | Loss: 0.00001803
Iteration 137/1000 | Loss: 0.00001803
Iteration 138/1000 | Loss: 0.00001803
Iteration 139/1000 | Loss: 0.00001803
Iteration 140/1000 | Loss: 0.00001803
Iteration 141/1000 | Loss: 0.00001803
Iteration 142/1000 | Loss: 0.00001803
Iteration 143/1000 | Loss: 0.00001803
Iteration 144/1000 | Loss: 0.00001803
Iteration 145/1000 | Loss: 0.00001803
Iteration 146/1000 | Loss: 0.00001803
Iteration 147/1000 | Loss: 0.00001803
Iteration 148/1000 | Loss: 0.00001803
Iteration 149/1000 | Loss: 0.00001803
Iteration 150/1000 | Loss: 0.00001803
Iteration 151/1000 | Loss: 0.00001803
Iteration 152/1000 | Loss: 0.00001803
Iteration 153/1000 | Loss: 0.00001803
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.803075610951055e-05, 1.803075610951055e-05, 1.803075610951055e-05, 1.803075610951055e-05, 1.803075610951055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.803075610951055e-05

Optimization complete. Final v2v error: 3.5951499938964844 mm

Highest mean error: 3.930776834487915 mm for frame 30

Lowest mean error: 3.3297786712646484 mm for frame 1

Saving results

Total time: 39.48432159423828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035909
Iteration 2/25 | Loss: 0.01035909
Iteration 3/25 | Loss: 0.01035909
Iteration 4/25 | Loss: 0.01035909
Iteration 5/25 | Loss: 0.01035909
Iteration 6/25 | Loss: 0.01035909
Iteration 7/25 | Loss: 0.01035908
Iteration 8/25 | Loss: 0.01035908
Iteration 9/25 | Loss: 0.01035908
Iteration 10/25 | Loss: 0.01035908
Iteration 11/25 | Loss: 0.01035908
Iteration 12/25 | Loss: 0.01035908
Iteration 13/25 | Loss: 0.01035908
Iteration 14/25 | Loss: 0.01035908
Iteration 15/25 | Loss: 0.01035908
Iteration 16/25 | Loss: 0.01035907
Iteration 17/25 | Loss: 0.01035907
Iteration 18/25 | Loss: 0.01035907
Iteration 19/25 | Loss: 0.01035907
Iteration 20/25 | Loss: 0.01035907
Iteration 21/25 | Loss: 0.01035907
Iteration 22/25 | Loss: 0.01035907
Iteration 23/25 | Loss: 0.01035907
Iteration 24/25 | Loss: 0.01035907
Iteration 25/25 | Loss: 0.01035907

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64432013
Iteration 2/25 | Loss: 0.12114453
Iteration 3/25 | Loss: 0.09500501
Iteration 4/25 | Loss: 0.08898059
Iteration 5/25 | Loss: 0.08687557
Iteration 6/25 | Loss: 0.08667638
Iteration 7/25 | Loss: 0.08666053
Iteration 8/25 | Loss: 0.08666050
Iteration 9/25 | Loss: 0.08666048
Iteration 10/25 | Loss: 0.08666048
Iteration 11/25 | Loss: 0.08666048
Iteration 12/25 | Loss: 0.08666048
Iteration 13/25 | Loss: 0.08666048
Iteration 14/25 | Loss: 0.08666048
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0866604819893837, 0.0866604819893837, 0.0866604819893837, 0.0866604819893837, 0.0866604819893837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0866604819893837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08666048
Iteration 2/1000 | Loss: 0.00715582
Iteration 3/1000 | Loss: 0.00333832
Iteration 4/1000 | Loss: 0.00162956
Iteration 5/1000 | Loss: 0.00078102
Iteration 6/1000 | Loss: 0.00250547
Iteration 7/1000 | Loss: 0.00139777
Iteration 8/1000 | Loss: 0.00163905
Iteration 9/1000 | Loss: 0.00042879
Iteration 10/1000 | Loss: 0.00021753
Iteration 11/1000 | Loss: 0.00019290
Iteration 12/1000 | Loss: 0.00041793
Iteration 13/1000 | Loss: 0.00038435
Iteration 14/1000 | Loss: 0.00014054
Iteration 15/1000 | Loss: 0.00023957
Iteration 16/1000 | Loss: 0.00119953
Iteration 17/1000 | Loss: 0.00408484
Iteration 18/1000 | Loss: 0.00093289
Iteration 19/1000 | Loss: 0.00179562
Iteration 20/1000 | Loss: 0.00030428
Iteration 21/1000 | Loss: 0.00087724
Iteration 22/1000 | Loss: 0.00192925
Iteration 23/1000 | Loss: 0.00097444
Iteration 24/1000 | Loss: 0.00035613
Iteration 25/1000 | Loss: 0.00005535
Iteration 26/1000 | Loss: 0.00007239
Iteration 27/1000 | Loss: 0.00004778
Iteration 28/1000 | Loss: 0.00044489
Iteration 29/1000 | Loss: 0.00004696
Iteration 30/1000 | Loss: 0.00004291
Iteration 31/1000 | Loss: 0.00007863
Iteration 32/1000 | Loss: 0.00014542
Iteration 33/1000 | Loss: 0.00018129
Iteration 34/1000 | Loss: 0.00003937
Iteration 35/1000 | Loss: 0.00003827
Iteration 36/1000 | Loss: 0.00003686
Iteration 37/1000 | Loss: 0.00032481
Iteration 38/1000 | Loss: 0.00003608
Iteration 39/1000 | Loss: 0.00003497
Iteration 40/1000 | Loss: 0.00006945
Iteration 41/1000 | Loss: 0.00004550
Iteration 42/1000 | Loss: 0.00003990
Iteration 43/1000 | Loss: 0.00003879
Iteration 44/1000 | Loss: 0.00003750
Iteration 45/1000 | Loss: 0.00004387
Iteration 46/1000 | Loss: 0.00017074
Iteration 47/1000 | Loss: 0.00006241
Iteration 48/1000 | Loss: 0.00006224
Iteration 49/1000 | Loss: 0.00004297
Iteration 50/1000 | Loss: 0.00004281
Iteration 51/1000 | Loss: 0.00009296
Iteration 52/1000 | Loss: 0.00003728
Iteration 53/1000 | Loss: 0.00003328
Iteration 54/1000 | Loss: 0.00019246
Iteration 55/1000 | Loss: 0.00003165
Iteration 56/1000 | Loss: 0.00003104
Iteration 57/1000 | Loss: 0.00003078
Iteration 58/1000 | Loss: 0.00012896
Iteration 59/1000 | Loss: 0.00003061
Iteration 60/1000 | Loss: 0.00003031
Iteration 61/1000 | Loss: 0.00008794
Iteration 62/1000 | Loss: 0.00003108
Iteration 63/1000 | Loss: 0.00003008
Iteration 64/1000 | Loss: 0.00003028
Iteration 65/1000 | Loss: 0.00013992
Iteration 66/1000 | Loss: 0.00002993
Iteration 67/1000 | Loss: 0.00002981
Iteration 68/1000 | Loss: 0.00002980
Iteration 69/1000 | Loss: 0.00002978
Iteration 70/1000 | Loss: 0.00017142
Iteration 71/1000 | Loss: 0.00003021
Iteration 72/1000 | Loss: 0.00002992
Iteration 73/1000 | Loss: 0.00002969
Iteration 74/1000 | Loss: 0.00002937
Iteration 75/1000 | Loss: 0.00002917
Iteration 76/1000 | Loss: 0.00002896
Iteration 77/1000 | Loss: 0.00002882
Iteration 78/1000 | Loss: 0.00002875
Iteration 79/1000 | Loss: 0.00002869
Iteration 80/1000 | Loss: 0.00002857
Iteration 81/1000 | Loss: 0.00002855
Iteration 82/1000 | Loss: 0.00002854
Iteration 83/1000 | Loss: 0.00002852
Iteration 84/1000 | Loss: 0.00002852
Iteration 85/1000 | Loss: 0.00002851
Iteration 86/1000 | Loss: 0.00002851
Iteration 87/1000 | Loss: 0.00002850
Iteration 88/1000 | Loss: 0.00002850
Iteration 89/1000 | Loss: 0.00002850
Iteration 90/1000 | Loss: 0.00002849
Iteration 91/1000 | Loss: 0.00002849
Iteration 92/1000 | Loss: 0.00002849
Iteration 93/1000 | Loss: 0.00002846
Iteration 94/1000 | Loss: 0.00002840
Iteration 95/1000 | Loss: 0.00002840
Iteration 96/1000 | Loss: 0.00002839
Iteration 97/1000 | Loss: 0.00002839
Iteration 98/1000 | Loss: 0.00002839
Iteration 99/1000 | Loss: 0.00002839
Iteration 100/1000 | Loss: 0.00002839
Iteration 101/1000 | Loss: 0.00002839
Iteration 102/1000 | Loss: 0.00002839
Iteration 103/1000 | Loss: 0.00002839
Iteration 104/1000 | Loss: 0.00002838
Iteration 105/1000 | Loss: 0.00002837
Iteration 106/1000 | Loss: 0.00002836
Iteration 107/1000 | Loss: 0.00002836
Iteration 108/1000 | Loss: 0.00002836
Iteration 109/1000 | Loss: 0.00002836
Iteration 110/1000 | Loss: 0.00002836
Iteration 111/1000 | Loss: 0.00002835
Iteration 112/1000 | Loss: 0.00002835
Iteration 113/1000 | Loss: 0.00002834
Iteration 114/1000 | Loss: 0.00002834
Iteration 115/1000 | Loss: 0.00002833
Iteration 116/1000 | Loss: 0.00002833
Iteration 117/1000 | Loss: 0.00002833
Iteration 118/1000 | Loss: 0.00002833
Iteration 119/1000 | Loss: 0.00002833
Iteration 120/1000 | Loss: 0.00002832
Iteration 121/1000 | Loss: 0.00002832
Iteration 122/1000 | Loss: 0.00002832
Iteration 123/1000 | Loss: 0.00002832
Iteration 124/1000 | Loss: 0.00002832
Iteration 125/1000 | Loss: 0.00002832
Iteration 126/1000 | Loss: 0.00002831
Iteration 127/1000 | Loss: 0.00002831
Iteration 128/1000 | Loss: 0.00002831
Iteration 129/1000 | Loss: 0.00002831
Iteration 130/1000 | Loss: 0.00002831
Iteration 131/1000 | Loss: 0.00002831
Iteration 132/1000 | Loss: 0.00002831
Iteration 133/1000 | Loss: 0.00002831
Iteration 134/1000 | Loss: 0.00002831
Iteration 135/1000 | Loss: 0.00002830
Iteration 136/1000 | Loss: 0.00002830
Iteration 137/1000 | Loss: 0.00002830
Iteration 138/1000 | Loss: 0.00002830
Iteration 139/1000 | Loss: 0.00002830
Iteration 140/1000 | Loss: 0.00002830
Iteration 141/1000 | Loss: 0.00002830
Iteration 142/1000 | Loss: 0.00002830
Iteration 143/1000 | Loss: 0.00002830
Iteration 144/1000 | Loss: 0.00002830
Iteration 145/1000 | Loss: 0.00002830
Iteration 146/1000 | Loss: 0.00002830
Iteration 147/1000 | Loss: 0.00002830
Iteration 148/1000 | Loss: 0.00002830
Iteration 149/1000 | Loss: 0.00002830
Iteration 150/1000 | Loss: 0.00002830
Iteration 151/1000 | Loss: 0.00002830
Iteration 152/1000 | Loss: 0.00002829
Iteration 153/1000 | Loss: 0.00002829
Iteration 154/1000 | Loss: 0.00002829
Iteration 155/1000 | Loss: 0.00002829
Iteration 156/1000 | Loss: 0.00002829
Iteration 157/1000 | Loss: 0.00002829
Iteration 158/1000 | Loss: 0.00002829
Iteration 159/1000 | Loss: 0.00002829
Iteration 160/1000 | Loss: 0.00002829
Iteration 161/1000 | Loss: 0.00002828
Iteration 162/1000 | Loss: 0.00002828
Iteration 163/1000 | Loss: 0.00002828
Iteration 164/1000 | Loss: 0.00002828
Iteration 165/1000 | Loss: 0.00002828
Iteration 166/1000 | Loss: 0.00002828
Iteration 167/1000 | Loss: 0.00002828
Iteration 168/1000 | Loss: 0.00002828
Iteration 169/1000 | Loss: 0.00002828
Iteration 170/1000 | Loss: 0.00002828
Iteration 171/1000 | Loss: 0.00002828
Iteration 172/1000 | Loss: 0.00002827
Iteration 173/1000 | Loss: 0.00002827
Iteration 174/1000 | Loss: 0.00002827
Iteration 175/1000 | Loss: 0.00002827
Iteration 176/1000 | Loss: 0.00002827
Iteration 177/1000 | Loss: 0.00002827
Iteration 178/1000 | Loss: 0.00002827
Iteration 179/1000 | Loss: 0.00002827
Iteration 180/1000 | Loss: 0.00002827
Iteration 181/1000 | Loss: 0.00002827
Iteration 182/1000 | Loss: 0.00002827
Iteration 183/1000 | Loss: 0.00002827
Iteration 184/1000 | Loss: 0.00002827
Iteration 185/1000 | Loss: 0.00002827
Iteration 186/1000 | Loss: 0.00002827
Iteration 187/1000 | Loss: 0.00002827
Iteration 188/1000 | Loss: 0.00002827
Iteration 189/1000 | Loss: 0.00002827
Iteration 190/1000 | Loss: 0.00002827
Iteration 191/1000 | Loss: 0.00002827
Iteration 192/1000 | Loss: 0.00002827
Iteration 193/1000 | Loss: 0.00002827
Iteration 194/1000 | Loss: 0.00002827
Iteration 195/1000 | Loss: 0.00002826
Iteration 196/1000 | Loss: 0.00002826
Iteration 197/1000 | Loss: 0.00002826
Iteration 198/1000 | Loss: 0.00002826
Iteration 199/1000 | Loss: 0.00002826
Iteration 200/1000 | Loss: 0.00002826
Iteration 201/1000 | Loss: 0.00002826
Iteration 202/1000 | Loss: 0.00002826
Iteration 203/1000 | Loss: 0.00002826
Iteration 204/1000 | Loss: 0.00002826
Iteration 205/1000 | Loss: 0.00002826
Iteration 206/1000 | Loss: 0.00002825
Iteration 207/1000 | Loss: 0.00002825
Iteration 208/1000 | Loss: 0.00002825
Iteration 209/1000 | Loss: 0.00002825
Iteration 210/1000 | Loss: 0.00002825
Iteration 211/1000 | Loss: 0.00002825
Iteration 212/1000 | Loss: 0.00002825
Iteration 213/1000 | Loss: 0.00002825
Iteration 214/1000 | Loss: 0.00002824
Iteration 215/1000 | Loss: 0.00002824
Iteration 216/1000 | Loss: 0.00002824
Iteration 217/1000 | Loss: 0.00002824
Iteration 218/1000 | Loss: 0.00002824
Iteration 219/1000 | Loss: 0.00002824
Iteration 220/1000 | Loss: 0.00002824
Iteration 221/1000 | Loss: 0.00002824
Iteration 222/1000 | Loss: 0.00002824
Iteration 223/1000 | Loss: 0.00002824
Iteration 224/1000 | Loss: 0.00002824
Iteration 225/1000 | Loss: 0.00002824
Iteration 226/1000 | Loss: 0.00002824
Iteration 227/1000 | Loss: 0.00002823
Iteration 228/1000 | Loss: 0.00002823
Iteration 229/1000 | Loss: 0.00002823
Iteration 230/1000 | Loss: 0.00002823
Iteration 231/1000 | Loss: 0.00002823
Iteration 232/1000 | Loss: 0.00002823
Iteration 233/1000 | Loss: 0.00002823
Iteration 234/1000 | Loss: 0.00002823
Iteration 235/1000 | Loss: 0.00002823
Iteration 236/1000 | Loss: 0.00002823
Iteration 237/1000 | Loss: 0.00002823
Iteration 238/1000 | Loss: 0.00002823
Iteration 239/1000 | Loss: 0.00002823
Iteration 240/1000 | Loss: 0.00002823
Iteration 241/1000 | Loss: 0.00002823
Iteration 242/1000 | Loss: 0.00002823
Iteration 243/1000 | Loss: 0.00002823
Iteration 244/1000 | Loss: 0.00002823
Iteration 245/1000 | Loss: 0.00002822
Iteration 246/1000 | Loss: 0.00002822
Iteration 247/1000 | Loss: 0.00002822
Iteration 248/1000 | Loss: 0.00002822
Iteration 249/1000 | Loss: 0.00002822
Iteration 250/1000 | Loss: 0.00002822
Iteration 251/1000 | Loss: 0.00002822
Iteration 252/1000 | Loss: 0.00002822
Iteration 253/1000 | Loss: 0.00002822
Iteration 254/1000 | Loss: 0.00002822
Iteration 255/1000 | Loss: 0.00002822
Iteration 256/1000 | Loss: 0.00002822
Iteration 257/1000 | Loss: 0.00002822
Iteration 258/1000 | Loss: 0.00002822
Iteration 259/1000 | Loss: 0.00002822
Iteration 260/1000 | Loss: 0.00002822
Iteration 261/1000 | Loss: 0.00002822
Iteration 262/1000 | Loss: 0.00002822
Iteration 263/1000 | Loss: 0.00002822
Iteration 264/1000 | Loss: 0.00002822
Iteration 265/1000 | Loss: 0.00002822
Iteration 266/1000 | Loss: 0.00002822
Iteration 267/1000 | Loss: 0.00002822
Iteration 268/1000 | Loss: 0.00002821
Iteration 269/1000 | Loss: 0.00002821
Iteration 270/1000 | Loss: 0.00002821
Iteration 271/1000 | Loss: 0.00002821
Iteration 272/1000 | Loss: 0.00002821
Iteration 273/1000 | Loss: 0.00002821
Iteration 274/1000 | Loss: 0.00002821
Iteration 275/1000 | Loss: 0.00002821
Iteration 276/1000 | Loss: 0.00002821
Iteration 277/1000 | Loss: 0.00002821
Iteration 278/1000 | Loss: 0.00002821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 278. Stopping optimization.
Last 5 losses: [2.8214830308570527e-05, 2.8214830308570527e-05, 2.8214830308570527e-05, 2.8214830308570527e-05, 2.8214830308570527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8214830308570527e-05

Optimization complete. Final v2v error: 3.984994649887085 mm

Highest mean error: 6.424808979034424 mm for frame 39

Lowest mean error: 3.032489061355591 mm for frame 91

Saving results

Total time: 147.73784828186035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00707769
Iteration 2/25 | Loss: 0.00154950
Iteration 3/25 | Loss: 0.00135143
Iteration 4/25 | Loss: 0.00133190
Iteration 5/25 | Loss: 0.00132634
Iteration 6/25 | Loss: 0.00132147
Iteration 7/25 | Loss: 0.00131453
Iteration 8/25 | Loss: 0.00131578
Iteration 9/25 | Loss: 0.00131048
Iteration 10/25 | Loss: 0.00130721
Iteration 11/25 | Loss: 0.00130149
Iteration 12/25 | Loss: 0.00129971
Iteration 13/25 | Loss: 0.00130248
Iteration 14/25 | Loss: 0.00129843
Iteration 15/25 | Loss: 0.00129762
Iteration 16/25 | Loss: 0.00129751
Iteration 17/25 | Loss: 0.00129751
Iteration 18/25 | Loss: 0.00129751
Iteration 19/25 | Loss: 0.00129751
Iteration 20/25 | Loss: 0.00129751
Iteration 21/25 | Loss: 0.00129750
Iteration 22/25 | Loss: 0.00129750
Iteration 23/25 | Loss: 0.00129750
Iteration 24/25 | Loss: 0.00129750
Iteration 25/25 | Loss: 0.00129750

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30921960
Iteration 2/25 | Loss: 0.00139755
Iteration 3/25 | Loss: 0.00139754
Iteration 4/25 | Loss: 0.00139754
Iteration 5/25 | Loss: 0.00139754
Iteration 6/25 | Loss: 0.00139754
Iteration 7/25 | Loss: 0.00139754
Iteration 8/25 | Loss: 0.00139754
Iteration 9/25 | Loss: 0.00139754
Iteration 10/25 | Loss: 0.00139754
Iteration 11/25 | Loss: 0.00139754
Iteration 12/25 | Loss: 0.00139754
Iteration 13/25 | Loss: 0.00139754
Iteration 14/25 | Loss: 0.00139754
Iteration 15/25 | Loss: 0.00139754
Iteration 16/25 | Loss: 0.00139754
Iteration 17/25 | Loss: 0.00139754
Iteration 18/25 | Loss: 0.00139754
Iteration 19/25 | Loss: 0.00139754
Iteration 20/25 | Loss: 0.00139754
Iteration 21/25 | Loss: 0.00139754
Iteration 22/25 | Loss: 0.00139754
Iteration 23/25 | Loss: 0.00139754
Iteration 24/25 | Loss: 0.00139754
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013975398615002632, 0.0013975398615002632, 0.0013975398615002632, 0.0013975398615002632, 0.0013975398615002632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013975398615002632

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139754
Iteration 2/1000 | Loss: 0.00005257
Iteration 3/1000 | Loss: 0.00003697
Iteration 4/1000 | Loss: 0.00003307
Iteration 5/1000 | Loss: 0.00003102
Iteration 6/1000 | Loss: 0.00002975
Iteration 7/1000 | Loss: 0.00002886
Iteration 8/1000 | Loss: 0.00002830
Iteration 9/1000 | Loss: 0.00002785
Iteration 10/1000 | Loss: 0.00002741
Iteration 11/1000 | Loss: 0.00002712
Iteration 12/1000 | Loss: 0.00002688
Iteration 13/1000 | Loss: 0.00002667
Iteration 14/1000 | Loss: 0.00002645
Iteration 15/1000 | Loss: 0.00002627
Iteration 16/1000 | Loss: 0.00002619
Iteration 17/1000 | Loss: 0.00002613
Iteration 18/1000 | Loss: 0.00002608
Iteration 19/1000 | Loss: 0.00002606
Iteration 20/1000 | Loss: 0.00002601
Iteration 21/1000 | Loss: 0.00002587
Iteration 22/1000 | Loss: 0.00002579
Iteration 23/1000 | Loss: 0.00002574
Iteration 24/1000 | Loss: 0.00002571
Iteration 25/1000 | Loss: 0.00002571
Iteration 26/1000 | Loss: 0.00002571
Iteration 27/1000 | Loss: 0.00002570
Iteration 28/1000 | Loss: 0.00002569
Iteration 29/1000 | Loss: 0.00002568
Iteration 30/1000 | Loss: 0.00002568
Iteration 31/1000 | Loss: 0.00002567
Iteration 32/1000 | Loss: 0.00002567
Iteration 33/1000 | Loss: 0.00002567
Iteration 34/1000 | Loss: 0.00002567
Iteration 35/1000 | Loss: 0.00002566
Iteration 36/1000 | Loss: 0.00002565
Iteration 37/1000 | Loss: 0.00002565
Iteration 38/1000 | Loss: 0.00002564
Iteration 39/1000 | Loss: 0.00002564
Iteration 40/1000 | Loss: 0.00002560
Iteration 41/1000 | Loss: 0.00002555
Iteration 42/1000 | Loss: 0.00002553
Iteration 43/1000 | Loss: 0.00002552
Iteration 44/1000 | Loss: 0.00002552
Iteration 45/1000 | Loss: 0.00002552
Iteration 46/1000 | Loss: 0.00002551
Iteration 47/1000 | Loss: 0.00002551
Iteration 48/1000 | Loss: 0.00002551
Iteration 49/1000 | Loss: 0.00002550
Iteration 50/1000 | Loss: 0.00002549
Iteration 51/1000 | Loss: 0.00002549
Iteration 52/1000 | Loss: 0.00002548
Iteration 53/1000 | Loss: 0.00002548
Iteration 54/1000 | Loss: 0.00002548
Iteration 55/1000 | Loss: 0.00002548
Iteration 56/1000 | Loss: 0.00002548
Iteration 57/1000 | Loss: 0.00002547
Iteration 58/1000 | Loss: 0.00002547
Iteration 59/1000 | Loss: 0.00002547
Iteration 60/1000 | Loss: 0.00002546
Iteration 61/1000 | Loss: 0.00002546
Iteration 62/1000 | Loss: 0.00002546
Iteration 63/1000 | Loss: 0.00002546
Iteration 64/1000 | Loss: 0.00002545
Iteration 65/1000 | Loss: 0.00002545
Iteration 66/1000 | Loss: 0.00002545
Iteration 67/1000 | Loss: 0.00002544
Iteration 68/1000 | Loss: 0.00002544
Iteration 69/1000 | Loss: 0.00002543
Iteration 70/1000 | Loss: 0.00002543
Iteration 71/1000 | Loss: 0.00002542
Iteration 72/1000 | Loss: 0.00002542
Iteration 73/1000 | Loss: 0.00002541
Iteration 74/1000 | Loss: 0.00002541
Iteration 75/1000 | Loss: 0.00002540
Iteration 76/1000 | Loss: 0.00002540
Iteration 77/1000 | Loss: 0.00002540
Iteration 78/1000 | Loss: 0.00002540
Iteration 79/1000 | Loss: 0.00002540
Iteration 80/1000 | Loss: 0.00002540
Iteration 81/1000 | Loss: 0.00002540
Iteration 82/1000 | Loss: 0.00002539
Iteration 83/1000 | Loss: 0.00002539
Iteration 84/1000 | Loss: 0.00002539
Iteration 85/1000 | Loss: 0.00002538
Iteration 86/1000 | Loss: 0.00002538
Iteration 87/1000 | Loss: 0.00002537
Iteration 88/1000 | Loss: 0.00002537
Iteration 89/1000 | Loss: 0.00002537
Iteration 90/1000 | Loss: 0.00002536
Iteration 91/1000 | Loss: 0.00002536
Iteration 92/1000 | Loss: 0.00002536
Iteration 93/1000 | Loss: 0.00002536
Iteration 94/1000 | Loss: 0.00002535
Iteration 95/1000 | Loss: 0.00002535
Iteration 96/1000 | Loss: 0.00002535
Iteration 97/1000 | Loss: 0.00002535
Iteration 98/1000 | Loss: 0.00002535
Iteration 99/1000 | Loss: 0.00002535
Iteration 100/1000 | Loss: 0.00002534
Iteration 101/1000 | Loss: 0.00002534
Iteration 102/1000 | Loss: 0.00002534
Iteration 103/1000 | Loss: 0.00002534
Iteration 104/1000 | Loss: 0.00002534
Iteration 105/1000 | Loss: 0.00002534
Iteration 106/1000 | Loss: 0.00002533
Iteration 107/1000 | Loss: 0.00002533
Iteration 108/1000 | Loss: 0.00002533
Iteration 109/1000 | Loss: 0.00002532
Iteration 110/1000 | Loss: 0.00002532
Iteration 111/1000 | Loss: 0.00002532
Iteration 112/1000 | Loss: 0.00002532
Iteration 113/1000 | Loss: 0.00002532
Iteration 114/1000 | Loss: 0.00002532
Iteration 115/1000 | Loss: 0.00002532
Iteration 116/1000 | Loss: 0.00002532
Iteration 117/1000 | Loss: 0.00002531
Iteration 118/1000 | Loss: 0.00002531
Iteration 119/1000 | Loss: 0.00002531
Iteration 120/1000 | Loss: 0.00002531
Iteration 121/1000 | Loss: 0.00002531
Iteration 122/1000 | Loss: 0.00002530
Iteration 123/1000 | Loss: 0.00002530
Iteration 124/1000 | Loss: 0.00002530
Iteration 125/1000 | Loss: 0.00002530
Iteration 126/1000 | Loss: 0.00002530
Iteration 127/1000 | Loss: 0.00002530
Iteration 128/1000 | Loss: 0.00002530
Iteration 129/1000 | Loss: 0.00002530
Iteration 130/1000 | Loss: 0.00002529
Iteration 131/1000 | Loss: 0.00002529
Iteration 132/1000 | Loss: 0.00002529
Iteration 133/1000 | Loss: 0.00002529
Iteration 134/1000 | Loss: 0.00002529
Iteration 135/1000 | Loss: 0.00002528
Iteration 136/1000 | Loss: 0.00002528
Iteration 137/1000 | Loss: 0.00002528
Iteration 138/1000 | Loss: 0.00002528
Iteration 139/1000 | Loss: 0.00002528
Iteration 140/1000 | Loss: 0.00002528
Iteration 141/1000 | Loss: 0.00002527
Iteration 142/1000 | Loss: 0.00002527
Iteration 143/1000 | Loss: 0.00002527
Iteration 144/1000 | Loss: 0.00002527
Iteration 145/1000 | Loss: 0.00002527
Iteration 146/1000 | Loss: 0.00002527
Iteration 147/1000 | Loss: 0.00002527
Iteration 148/1000 | Loss: 0.00002527
Iteration 149/1000 | Loss: 0.00002527
Iteration 150/1000 | Loss: 0.00002526
Iteration 151/1000 | Loss: 0.00002526
Iteration 152/1000 | Loss: 0.00002526
Iteration 153/1000 | Loss: 0.00002526
Iteration 154/1000 | Loss: 0.00002526
Iteration 155/1000 | Loss: 0.00002526
Iteration 156/1000 | Loss: 0.00002526
Iteration 157/1000 | Loss: 0.00002525
Iteration 158/1000 | Loss: 0.00002525
Iteration 159/1000 | Loss: 0.00002525
Iteration 160/1000 | Loss: 0.00002525
Iteration 161/1000 | Loss: 0.00002524
Iteration 162/1000 | Loss: 0.00002524
Iteration 163/1000 | Loss: 0.00002524
Iteration 164/1000 | Loss: 0.00002524
Iteration 165/1000 | Loss: 0.00002524
Iteration 166/1000 | Loss: 0.00002524
Iteration 167/1000 | Loss: 0.00002524
Iteration 168/1000 | Loss: 0.00002523
Iteration 169/1000 | Loss: 0.00002523
Iteration 170/1000 | Loss: 0.00002523
Iteration 171/1000 | Loss: 0.00002522
Iteration 172/1000 | Loss: 0.00002522
Iteration 173/1000 | Loss: 0.00002522
Iteration 174/1000 | Loss: 0.00002521
Iteration 175/1000 | Loss: 0.00002521
Iteration 176/1000 | Loss: 0.00002521
Iteration 177/1000 | Loss: 0.00002521
Iteration 178/1000 | Loss: 0.00002520
Iteration 179/1000 | Loss: 0.00002520
Iteration 180/1000 | Loss: 0.00002520
Iteration 181/1000 | Loss: 0.00002520
Iteration 182/1000 | Loss: 0.00002519
Iteration 183/1000 | Loss: 0.00002519
Iteration 184/1000 | Loss: 0.00002519
Iteration 185/1000 | Loss: 0.00002518
Iteration 186/1000 | Loss: 0.00002518
Iteration 187/1000 | Loss: 0.00002518
Iteration 188/1000 | Loss: 0.00002518
Iteration 189/1000 | Loss: 0.00002518
Iteration 190/1000 | Loss: 0.00002518
Iteration 191/1000 | Loss: 0.00002518
Iteration 192/1000 | Loss: 0.00002518
Iteration 193/1000 | Loss: 0.00002517
Iteration 194/1000 | Loss: 0.00002517
Iteration 195/1000 | Loss: 0.00002517
Iteration 196/1000 | Loss: 0.00002517
Iteration 197/1000 | Loss: 0.00002517
Iteration 198/1000 | Loss: 0.00002517
Iteration 199/1000 | Loss: 0.00002517
Iteration 200/1000 | Loss: 0.00002516
Iteration 201/1000 | Loss: 0.00002516
Iteration 202/1000 | Loss: 0.00002516
Iteration 203/1000 | Loss: 0.00002516
Iteration 204/1000 | Loss: 0.00002516
Iteration 205/1000 | Loss: 0.00002516
Iteration 206/1000 | Loss: 0.00002516
Iteration 207/1000 | Loss: 0.00002516
Iteration 208/1000 | Loss: 0.00002516
Iteration 209/1000 | Loss: 0.00002516
Iteration 210/1000 | Loss: 0.00002516
Iteration 211/1000 | Loss: 0.00002516
Iteration 212/1000 | Loss: 0.00002516
Iteration 213/1000 | Loss: 0.00002516
Iteration 214/1000 | Loss: 0.00002516
Iteration 215/1000 | Loss: 0.00002515
Iteration 216/1000 | Loss: 0.00002515
Iteration 217/1000 | Loss: 0.00002515
Iteration 218/1000 | Loss: 0.00002515
Iteration 219/1000 | Loss: 0.00002515
Iteration 220/1000 | Loss: 0.00002515
Iteration 221/1000 | Loss: 0.00002515
Iteration 222/1000 | Loss: 0.00002515
Iteration 223/1000 | Loss: 0.00002515
Iteration 224/1000 | Loss: 0.00002515
Iteration 225/1000 | Loss: 0.00002515
Iteration 226/1000 | Loss: 0.00002515
Iteration 227/1000 | Loss: 0.00002515
Iteration 228/1000 | Loss: 0.00002514
Iteration 229/1000 | Loss: 0.00002514
Iteration 230/1000 | Loss: 0.00002514
Iteration 231/1000 | Loss: 0.00002514
Iteration 232/1000 | Loss: 0.00002514
Iteration 233/1000 | Loss: 0.00002514
Iteration 234/1000 | Loss: 0.00002514
Iteration 235/1000 | Loss: 0.00002514
Iteration 236/1000 | Loss: 0.00002514
Iteration 237/1000 | Loss: 0.00002514
Iteration 238/1000 | Loss: 0.00002514
Iteration 239/1000 | Loss: 0.00002514
Iteration 240/1000 | Loss: 0.00002514
Iteration 241/1000 | Loss: 0.00002514
Iteration 242/1000 | Loss: 0.00002514
Iteration 243/1000 | Loss: 0.00002514
Iteration 244/1000 | Loss: 0.00002513
Iteration 245/1000 | Loss: 0.00002513
Iteration 246/1000 | Loss: 0.00002513
Iteration 247/1000 | Loss: 0.00002513
Iteration 248/1000 | Loss: 0.00002513
Iteration 249/1000 | Loss: 0.00002513
Iteration 250/1000 | Loss: 0.00002513
Iteration 251/1000 | Loss: 0.00002513
Iteration 252/1000 | Loss: 0.00002512
Iteration 253/1000 | Loss: 0.00002512
Iteration 254/1000 | Loss: 0.00002512
Iteration 255/1000 | Loss: 0.00002512
Iteration 256/1000 | Loss: 0.00002512
Iteration 257/1000 | Loss: 0.00002512
Iteration 258/1000 | Loss: 0.00002512
Iteration 259/1000 | Loss: 0.00002512
Iteration 260/1000 | Loss: 0.00002512
Iteration 261/1000 | Loss: 0.00002512
Iteration 262/1000 | Loss: 0.00002512
Iteration 263/1000 | Loss: 0.00002512
Iteration 264/1000 | Loss: 0.00002512
Iteration 265/1000 | Loss: 0.00002512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [2.5118806661339477e-05, 2.5118806661339477e-05, 2.5118806661339477e-05, 2.5118806661339477e-05, 2.5118806661339477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5118806661339477e-05

Optimization complete. Final v2v error: 3.5857913494110107 mm

Highest mean error: 11.091205596923828 mm for frame 178

Lowest mean error: 2.6590421199798584 mm for frame 188

Saving results

Total time: 83.47633099555969
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813078
Iteration 2/25 | Loss: 0.00195270
Iteration 3/25 | Loss: 0.00140928
Iteration 4/25 | Loss: 0.00136394
Iteration 5/25 | Loss: 0.00135990
Iteration 6/25 | Loss: 0.00135980
Iteration 7/25 | Loss: 0.00135980
Iteration 8/25 | Loss: 0.00135980
Iteration 9/25 | Loss: 0.00135980
Iteration 10/25 | Loss: 0.00135980
Iteration 11/25 | Loss: 0.00135980
Iteration 12/25 | Loss: 0.00135980
Iteration 13/25 | Loss: 0.00135980
Iteration 14/25 | Loss: 0.00135980
Iteration 15/25 | Loss: 0.00135980
Iteration 16/25 | Loss: 0.00135980
Iteration 17/25 | Loss: 0.00135980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013597962679341435, 0.0013597962679341435, 0.0013597962679341435, 0.0013597962679341435, 0.0013597962679341435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013597962679341435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69381094
Iteration 2/25 | Loss: 0.00123650
Iteration 3/25 | Loss: 0.00123650
Iteration 4/25 | Loss: 0.00123650
Iteration 5/25 | Loss: 0.00123650
Iteration 6/25 | Loss: 0.00123650
Iteration 7/25 | Loss: 0.00123650
Iteration 8/25 | Loss: 0.00123650
Iteration 9/25 | Loss: 0.00123650
Iteration 10/25 | Loss: 0.00123650
Iteration 11/25 | Loss: 0.00123650
Iteration 12/25 | Loss: 0.00123650
Iteration 13/25 | Loss: 0.00123650
Iteration 14/25 | Loss: 0.00123650
Iteration 15/25 | Loss: 0.00123650
Iteration 16/25 | Loss: 0.00123650
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012365002185106277, 0.0012365002185106277, 0.0012365002185106277, 0.0012365002185106277, 0.0012365002185106277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012365002185106277

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123650
Iteration 2/1000 | Loss: 0.00003455
Iteration 3/1000 | Loss: 0.00002293
Iteration 4/1000 | Loss: 0.00002076
Iteration 5/1000 | Loss: 0.00001988
Iteration 6/1000 | Loss: 0.00001931
Iteration 7/1000 | Loss: 0.00001890
Iteration 8/1000 | Loss: 0.00001858
Iteration 9/1000 | Loss: 0.00001845
Iteration 10/1000 | Loss: 0.00001841
Iteration 11/1000 | Loss: 0.00001824
Iteration 12/1000 | Loss: 0.00001810
Iteration 13/1000 | Loss: 0.00001808
Iteration 14/1000 | Loss: 0.00001800
Iteration 15/1000 | Loss: 0.00001790
Iteration 16/1000 | Loss: 0.00001790
Iteration 17/1000 | Loss: 0.00001784
Iteration 18/1000 | Loss: 0.00001783
Iteration 19/1000 | Loss: 0.00001781
Iteration 20/1000 | Loss: 0.00001781
Iteration 21/1000 | Loss: 0.00001781
Iteration 22/1000 | Loss: 0.00001780
Iteration 23/1000 | Loss: 0.00001780
Iteration 24/1000 | Loss: 0.00001780
Iteration 25/1000 | Loss: 0.00001779
Iteration 26/1000 | Loss: 0.00001779
Iteration 27/1000 | Loss: 0.00001778
Iteration 28/1000 | Loss: 0.00001777
Iteration 29/1000 | Loss: 0.00001777
Iteration 30/1000 | Loss: 0.00001776
Iteration 31/1000 | Loss: 0.00001776
Iteration 32/1000 | Loss: 0.00001776
Iteration 33/1000 | Loss: 0.00001776
Iteration 34/1000 | Loss: 0.00001776
Iteration 35/1000 | Loss: 0.00001776
Iteration 36/1000 | Loss: 0.00001776
Iteration 37/1000 | Loss: 0.00001775
Iteration 38/1000 | Loss: 0.00001775
Iteration 39/1000 | Loss: 0.00001775
Iteration 40/1000 | Loss: 0.00001775
Iteration 41/1000 | Loss: 0.00001775
Iteration 42/1000 | Loss: 0.00001775
Iteration 43/1000 | Loss: 0.00001775
Iteration 44/1000 | Loss: 0.00001775
Iteration 45/1000 | Loss: 0.00001774
Iteration 46/1000 | Loss: 0.00001774
Iteration 47/1000 | Loss: 0.00001773
Iteration 48/1000 | Loss: 0.00001773
Iteration 49/1000 | Loss: 0.00001772
Iteration 50/1000 | Loss: 0.00001772
Iteration 51/1000 | Loss: 0.00001772
Iteration 52/1000 | Loss: 0.00001772
Iteration 53/1000 | Loss: 0.00001772
Iteration 54/1000 | Loss: 0.00001772
Iteration 55/1000 | Loss: 0.00001771
Iteration 56/1000 | Loss: 0.00001769
Iteration 57/1000 | Loss: 0.00001769
Iteration 58/1000 | Loss: 0.00001768
Iteration 59/1000 | Loss: 0.00001768
Iteration 60/1000 | Loss: 0.00001768
Iteration 61/1000 | Loss: 0.00001768
Iteration 62/1000 | Loss: 0.00001768
Iteration 63/1000 | Loss: 0.00001767
Iteration 64/1000 | Loss: 0.00001767
Iteration 65/1000 | Loss: 0.00001767
Iteration 66/1000 | Loss: 0.00001767
Iteration 67/1000 | Loss: 0.00001767
Iteration 68/1000 | Loss: 0.00001766
Iteration 69/1000 | Loss: 0.00001766
Iteration 70/1000 | Loss: 0.00001766
Iteration 71/1000 | Loss: 0.00001765
Iteration 72/1000 | Loss: 0.00001765
Iteration 73/1000 | Loss: 0.00001765
Iteration 74/1000 | Loss: 0.00001765
Iteration 75/1000 | Loss: 0.00001764
Iteration 76/1000 | Loss: 0.00001764
Iteration 77/1000 | Loss: 0.00001764
Iteration 78/1000 | Loss: 0.00001764
Iteration 79/1000 | Loss: 0.00001764
Iteration 80/1000 | Loss: 0.00001764
Iteration 81/1000 | Loss: 0.00001764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.7643365936237387e-05, 1.7643365936237387e-05, 1.7643365936237387e-05, 1.7643365936237387e-05, 1.7643365936237387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7643365936237387e-05

Optimization complete. Final v2v error: 3.5707902908325195 mm

Highest mean error: 3.8138203620910645 mm for frame 169

Lowest mean error: 3.3259119987487793 mm for frame 234

Saving results

Total time: 34.93654203414917
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00558407
Iteration 2/25 | Loss: 0.00133728
Iteration 3/25 | Loss: 0.00125332
Iteration 4/25 | Loss: 0.00124744
Iteration 5/25 | Loss: 0.00124513
Iteration 6/25 | Loss: 0.00124513
Iteration 7/25 | Loss: 0.00124513
Iteration 8/25 | Loss: 0.00124513
Iteration 9/25 | Loss: 0.00124513
Iteration 10/25 | Loss: 0.00124513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012451250804588199, 0.0012451250804588199, 0.0012451250804588199, 0.0012451250804588199, 0.0012451250804588199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012451250804588199

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.76687241
Iteration 2/25 | Loss: 0.00118034
Iteration 3/25 | Loss: 0.00117748
Iteration 4/25 | Loss: 0.00117748
Iteration 5/25 | Loss: 0.00117748
Iteration 6/25 | Loss: 0.00117748
Iteration 7/25 | Loss: 0.00117748
Iteration 8/25 | Loss: 0.00117748
Iteration 9/25 | Loss: 0.00117748
Iteration 10/25 | Loss: 0.00117748
Iteration 11/25 | Loss: 0.00117748
Iteration 12/25 | Loss: 0.00117748
Iteration 13/25 | Loss: 0.00117748
Iteration 14/25 | Loss: 0.00117748
Iteration 15/25 | Loss: 0.00117748
Iteration 16/25 | Loss: 0.00117748
Iteration 17/25 | Loss: 0.00117748
Iteration 18/25 | Loss: 0.00117748
Iteration 19/25 | Loss: 0.00117748
Iteration 20/25 | Loss: 0.00117748
Iteration 21/25 | Loss: 0.00117748
Iteration 22/25 | Loss: 0.00117748
Iteration 23/25 | Loss: 0.00117748
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011774764861911535, 0.0011774764861911535, 0.0011774764861911535, 0.0011774764861911535, 0.0011774764861911535]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011774764861911535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117748
Iteration 2/1000 | Loss: 0.00002573
Iteration 3/1000 | Loss: 0.00001820
Iteration 4/1000 | Loss: 0.00001669
Iteration 5/1000 | Loss: 0.00001602
Iteration 6/1000 | Loss: 0.00001569
Iteration 7/1000 | Loss: 0.00001542
Iteration 8/1000 | Loss: 0.00001541
Iteration 9/1000 | Loss: 0.00001524
Iteration 10/1000 | Loss: 0.00001498
Iteration 11/1000 | Loss: 0.00001473
Iteration 12/1000 | Loss: 0.00001460
Iteration 13/1000 | Loss: 0.00001443
Iteration 14/1000 | Loss: 0.00001435
Iteration 15/1000 | Loss: 0.00001431
Iteration 16/1000 | Loss: 0.00001430
Iteration 17/1000 | Loss: 0.00001429
Iteration 18/1000 | Loss: 0.00001428
Iteration 19/1000 | Loss: 0.00001428
Iteration 20/1000 | Loss: 0.00001427
Iteration 21/1000 | Loss: 0.00001427
Iteration 22/1000 | Loss: 0.00001420
Iteration 23/1000 | Loss: 0.00001416
Iteration 24/1000 | Loss: 0.00001407
Iteration 25/1000 | Loss: 0.00001407
Iteration 26/1000 | Loss: 0.00001404
Iteration 27/1000 | Loss: 0.00001403
Iteration 28/1000 | Loss: 0.00001403
Iteration 29/1000 | Loss: 0.00001401
Iteration 30/1000 | Loss: 0.00001401
Iteration 31/1000 | Loss: 0.00001395
Iteration 32/1000 | Loss: 0.00001395
Iteration 33/1000 | Loss: 0.00001393
Iteration 34/1000 | Loss: 0.00001390
Iteration 35/1000 | Loss: 0.00001389
Iteration 36/1000 | Loss: 0.00001389
Iteration 37/1000 | Loss: 0.00001389
Iteration 38/1000 | Loss: 0.00001388
Iteration 39/1000 | Loss: 0.00001388
Iteration 40/1000 | Loss: 0.00001388
Iteration 41/1000 | Loss: 0.00001387
Iteration 42/1000 | Loss: 0.00001386
Iteration 43/1000 | Loss: 0.00001386
Iteration 44/1000 | Loss: 0.00001386
Iteration 45/1000 | Loss: 0.00001385
Iteration 46/1000 | Loss: 0.00001384
Iteration 47/1000 | Loss: 0.00001381
Iteration 48/1000 | Loss: 0.00001380
Iteration 49/1000 | Loss: 0.00001380
Iteration 50/1000 | Loss: 0.00001374
Iteration 51/1000 | Loss: 0.00001374
Iteration 52/1000 | Loss: 0.00001374
Iteration 53/1000 | Loss: 0.00001373
Iteration 54/1000 | Loss: 0.00001373
Iteration 55/1000 | Loss: 0.00001372
Iteration 56/1000 | Loss: 0.00001372
Iteration 57/1000 | Loss: 0.00001372
Iteration 58/1000 | Loss: 0.00001371
Iteration 59/1000 | Loss: 0.00001371
Iteration 60/1000 | Loss: 0.00001371
Iteration 61/1000 | Loss: 0.00001371
Iteration 62/1000 | Loss: 0.00001371
Iteration 63/1000 | Loss: 0.00001370
Iteration 64/1000 | Loss: 0.00001370
Iteration 65/1000 | Loss: 0.00001370
Iteration 66/1000 | Loss: 0.00001370
Iteration 67/1000 | Loss: 0.00001370
Iteration 68/1000 | Loss: 0.00001370
Iteration 69/1000 | Loss: 0.00001370
Iteration 70/1000 | Loss: 0.00001370
Iteration 71/1000 | Loss: 0.00001370
Iteration 72/1000 | Loss: 0.00001369
Iteration 73/1000 | Loss: 0.00001369
Iteration 74/1000 | Loss: 0.00001369
Iteration 75/1000 | Loss: 0.00001369
Iteration 76/1000 | Loss: 0.00001369
Iteration 77/1000 | Loss: 0.00001369
Iteration 78/1000 | Loss: 0.00001369
Iteration 79/1000 | Loss: 0.00001369
Iteration 80/1000 | Loss: 0.00001369
Iteration 81/1000 | Loss: 0.00001369
Iteration 82/1000 | Loss: 0.00001369
Iteration 83/1000 | Loss: 0.00001369
Iteration 84/1000 | Loss: 0.00001369
Iteration 85/1000 | Loss: 0.00001369
Iteration 86/1000 | Loss: 0.00001369
Iteration 87/1000 | Loss: 0.00001369
Iteration 88/1000 | Loss: 0.00001369
Iteration 89/1000 | Loss: 0.00001369
Iteration 90/1000 | Loss: 0.00001369
Iteration 91/1000 | Loss: 0.00001369
Iteration 92/1000 | Loss: 0.00001369
Iteration 93/1000 | Loss: 0.00001369
Iteration 94/1000 | Loss: 0.00001369
Iteration 95/1000 | Loss: 0.00001369
Iteration 96/1000 | Loss: 0.00001369
Iteration 97/1000 | Loss: 0.00001369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.3691063031728845e-05, 1.3691063031728845e-05, 1.3691063031728845e-05, 1.3691063031728845e-05, 1.3691063031728845e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3691063031728845e-05

Optimization complete. Final v2v error: 3.1176323890686035 mm

Highest mean error: 3.3146467208862305 mm for frame 232

Lowest mean error: 2.943035840988159 mm for frame 166

Saving results

Total time: 41.54319477081299
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00290295
Iteration 2/25 | Loss: 0.00132323
Iteration 3/25 | Loss: 0.00118200
Iteration 4/25 | Loss: 0.00116286
Iteration 5/25 | Loss: 0.00115727
Iteration 6/25 | Loss: 0.00115544
Iteration 7/25 | Loss: 0.00115526
Iteration 8/25 | Loss: 0.00115526
Iteration 9/25 | Loss: 0.00115526
Iteration 10/25 | Loss: 0.00115526
Iteration 11/25 | Loss: 0.00115526
Iteration 12/25 | Loss: 0.00115526
Iteration 13/25 | Loss: 0.00115526
Iteration 14/25 | Loss: 0.00115526
Iteration 15/25 | Loss: 0.00115526
Iteration 16/25 | Loss: 0.00115526
Iteration 17/25 | Loss: 0.00115526
Iteration 18/25 | Loss: 0.00115526
Iteration 19/25 | Loss: 0.00115526
Iteration 20/25 | Loss: 0.00115526
Iteration 21/25 | Loss: 0.00115526
Iteration 22/25 | Loss: 0.00115526
Iteration 23/25 | Loss: 0.00115526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011552644427865744, 0.0011552644427865744, 0.0011552644427865744, 0.0011552644427865744, 0.0011552644427865744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011552644427865744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27381599
Iteration 2/25 | Loss: 0.00184502
Iteration 3/25 | Loss: 0.00184502
Iteration 4/25 | Loss: 0.00184502
Iteration 5/25 | Loss: 0.00184501
Iteration 6/25 | Loss: 0.00184501
Iteration 7/25 | Loss: 0.00184501
Iteration 8/25 | Loss: 0.00184501
Iteration 9/25 | Loss: 0.00184501
Iteration 10/25 | Loss: 0.00184501
Iteration 11/25 | Loss: 0.00184501
Iteration 12/25 | Loss: 0.00184501
Iteration 13/25 | Loss: 0.00184501
Iteration 14/25 | Loss: 0.00184501
Iteration 15/25 | Loss: 0.00184501
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0018450135830789804, 0.0018450135830789804, 0.0018450135830789804, 0.0018450135830789804, 0.0018450135830789804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018450135830789804

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184501
Iteration 2/1000 | Loss: 0.00003942
Iteration 3/1000 | Loss: 0.00002585
Iteration 4/1000 | Loss: 0.00002012
Iteration 5/1000 | Loss: 0.00001867
Iteration 6/1000 | Loss: 0.00001760
Iteration 7/1000 | Loss: 0.00001683
Iteration 8/1000 | Loss: 0.00001641
Iteration 9/1000 | Loss: 0.00001600
Iteration 10/1000 | Loss: 0.00001561
Iteration 11/1000 | Loss: 0.00001520
Iteration 12/1000 | Loss: 0.00001506
Iteration 13/1000 | Loss: 0.00001494
Iteration 14/1000 | Loss: 0.00001491
Iteration 15/1000 | Loss: 0.00001491
Iteration 16/1000 | Loss: 0.00001490
Iteration 17/1000 | Loss: 0.00001488
Iteration 18/1000 | Loss: 0.00001476
Iteration 19/1000 | Loss: 0.00001470
Iteration 20/1000 | Loss: 0.00001468
Iteration 21/1000 | Loss: 0.00001467
Iteration 22/1000 | Loss: 0.00001466
Iteration 23/1000 | Loss: 0.00001466
Iteration 24/1000 | Loss: 0.00001466
Iteration 25/1000 | Loss: 0.00001465
Iteration 26/1000 | Loss: 0.00001464
Iteration 27/1000 | Loss: 0.00001464
Iteration 28/1000 | Loss: 0.00001462
Iteration 29/1000 | Loss: 0.00001461
Iteration 30/1000 | Loss: 0.00001460
Iteration 31/1000 | Loss: 0.00001460
Iteration 32/1000 | Loss: 0.00001460
Iteration 33/1000 | Loss: 0.00001460
Iteration 34/1000 | Loss: 0.00001459
Iteration 35/1000 | Loss: 0.00001459
Iteration 36/1000 | Loss: 0.00001459
Iteration 37/1000 | Loss: 0.00001459
Iteration 38/1000 | Loss: 0.00001459
Iteration 39/1000 | Loss: 0.00001458
Iteration 40/1000 | Loss: 0.00001458
Iteration 41/1000 | Loss: 0.00001456
Iteration 42/1000 | Loss: 0.00001456
Iteration 43/1000 | Loss: 0.00001455
Iteration 44/1000 | Loss: 0.00001455
Iteration 45/1000 | Loss: 0.00001454
Iteration 46/1000 | Loss: 0.00001454
Iteration 47/1000 | Loss: 0.00001454
Iteration 48/1000 | Loss: 0.00001453
Iteration 49/1000 | Loss: 0.00001452
Iteration 50/1000 | Loss: 0.00001452
Iteration 51/1000 | Loss: 0.00001451
Iteration 52/1000 | Loss: 0.00001451
Iteration 53/1000 | Loss: 0.00001450
Iteration 54/1000 | Loss: 0.00001450
Iteration 55/1000 | Loss: 0.00001450
Iteration 56/1000 | Loss: 0.00001449
Iteration 57/1000 | Loss: 0.00001449
Iteration 58/1000 | Loss: 0.00001448
Iteration 59/1000 | Loss: 0.00001448
Iteration 60/1000 | Loss: 0.00001448
Iteration 61/1000 | Loss: 0.00001447
Iteration 62/1000 | Loss: 0.00001447
Iteration 63/1000 | Loss: 0.00001447
Iteration 64/1000 | Loss: 0.00001446
Iteration 65/1000 | Loss: 0.00001446
Iteration 66/1000 | Loss: 0.00001446
Iteration 67/1000 | Loss: 0.00001445
Iteration 68/1000 | Loss: 0.00001445
Iteration 69/1000 | Loss: 0.00001444
Iteration 70/1000 | Loss: 0.00001444
Iteration 71/1000 | Loss: 0.00001444
Iteration 72/1000 | Loss: 0.00001443
Iteration 73/1000 | Loss: 0.00001443
Iteration 74/1000 | Loss: 0.00001443
Iteration 75/1000 | Loss: 0.00001442
Iteration 76/1000 | Loss: 0.00001442
Iteration 77/1000 | Loss: 0.00001442
Iteration 78/1000 | Loss: 0.00001441
Iteration 79/1000 | Loss: 0.00001441
Iteration 80/1000 | Loss: 0.00001441
Iteration 81/1000 | Loss: 0.00001441
Iteration 82/1000 | Loss: 0.00001441
Iteration 83/1000 | Loss: 0.00001440
Iteration 84/1000 | Loss: 0.00001440
Iteration 85/1000 | Loss: 0.00001440
Iteration 86/1000 | Loss: 0.00001440
Iteration 87/1000 | Loss: 0.00001440
Iteration 88/1000 | Loss: 0.00001439
Iteration 89/1000 | Loss: 0.00001439
Iteration 90/1000 | Loss: 0.00001439
Iteration 91/1000 | Loss: 0.00001438
Iteration 92/1000 | Loss: 0.00001438
Iteration 93/1000 | Loss: 0.00001438
Iteration 94/1000 | Loss: 0.00001437
Iteration 95/1000 | Loss: 0.00001437
Iteration 96/1000 | Loss: 0.00001437
Iteration 97/1000 | Loss: 0.00001437
Iteration 98/1000 | Loss: 0.00001437
Iteration 99/1000 | Loss: 0.00001437
Iteration 100/1000 | Loss: 0.00001436
Iteration 101/1000 | Loss: 0.00001436
Iteration 102/1000 | Loss: 0.00001436
Iteration 103/1000 | Loss: 0.00001436
Iteration 104/1000 | Loss: 0.00001435
Iteration 105/1000 | Loss: 0.00001435
Iteration 106/1000 | Loss: 0.00001435
Iteration 107/1000 | Loss: 0.00001435
Iteration 108/1000 | Loss: 0.00001435
Iteration 109/1000 | Loss: 0.00001434
Iteration 110/1000 | Loss: 0.00001434
Iteration 111/1000 | Loss: 0.00001434
Iteration 112/1000 | Loss: 0.00001434
Iteration 113/1000 | Loss: 0.00001434
Iteration 114/1000 | Loss: 0.00001434
Iteration 115/1000 | Loss: 0.00001434
Iteration 116/1000 | Loss: 0.00001434
Iteration 117/1000 | Loss: 0.00001434
Iteration 118/1000 | Loss: 0.00001434
Iteration 119/1000 | Loss: 0.00001434
Iteration 120/1000 | Loss: 0.00001434
Iteration 121/1000 | Loss: 0.00001434
Iteration 122/1000 | Loss: 0.00001433
Iteration 123/1000 | Loss: 0.00001433
Iteration 124/1000 | Loss: 0.00001433
Iteration 125/1000 | Loss: 0.00001433
Iteration 126/1000 | Loss: 0.00001433
Iteration 127/1000 | Loss: 0.00001432
Iteration 128/1000 | Loss: 0.00001432
Iteration 129/1000 | Loss: 0.00001432
Iteration 130/1000 | Loss: 0.00001432
Iteration 131/1000 | Loss: 0.00001432
Iteration 132/1000 | Loss: 0.00001432
Iteration 133/1000 | Loss: 0.00001432
Iteration 134/1000 | Loss: 0.00001432
Iteration 135/1000 | Loss: 0.00001431
Iteration 136/1000 | Loss: 0.00001431
Iteration 137/1000 | Loss: 0.00001431
Iteration 138/1000 | Loss: 0.00001431
Iteration 139/1000 | Loss: 0.00001431
Iteration 140/1000 | Loss: 0.00001431
Iteration 141/1000 | Loss: 0.00001431
Iteration 142/1000 | Loss: 0.00001430
Iteration 143/1000 | Loss: 0.00001430
Iteration 144/1000 | Loss: 0.00001430
Iteration 145/1000 | Loss: 0.00001430
Iteration 146/1000 | Loss: 0.00001430
Iteration 147/1000 | Loss: 0.00001430
Iteration 148/1000 | Loss: 0.00001430
Iteration 149/1000 | Loss: 0.00001430
Iteration 150/1000 | Loss: 0.00001430
Iteration 151/1000 | Loss: 0.00001430
Iteration 152/1000 | Loss: 0.00001430
Iteration 153/1000 | Loss: 0.00001430
Iteration 154/1000 | Loss: 0.00001430
Iteration 155/1000 | Loss: 0.00001430
Iteration 156/1000 | Loss: 0.00001430
Iteration 157/1000 | Loss: 0.00001430
Iteration 158/1000 | Loss: 0.00001430
Iteration 159/1000 | Loss: 0.00001430
Iteration 160/1000 | Loss: 0.00001430
Iteration 161/1000 | Loss: 0.00001430
Iteration 162/1000 | Loss: 0.00001430
Iteration 163/1000 | Loss: 0.00001430
Iteration 164/1000 | Loss: 0.00001430
Iteration 165/1000 | Loss: 0.00001430
Iteration 166/1000 | Loss: 0.00001430
Iteration 167/1000 | Loss: 0.00001430
Iteration 168/1000 | Loss: 0.00001430
Iteration 169/1000 | Loss: 0.00001430
Iteration 170/1000 | Loss: 0.00001430
Iteration 171/1000 | Loss: 0.00001430
Iteration 172/1000 | Loss: 0.00001430
Iteration 173/1000 | Loss: 0.00001430
Iteration 174/1000 | Loss: 0.00001430
Iteration 175/1000 | Loss: 0.00001430
Iteration 176/1000 | Loss: 0.00001430
Iteration 177/1000 | Loss: 0.00001430
Iteration 178/1000 | Loss: 0.00001430
Iteration 179/1000 | Loss: 0.00001430
Iteration 180/1000 | Loss: 0.00001430
Iteration 181/1000 | Loss: 0.00001430
Iteration 182/1000 | Loss: 0.00001430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.430047723260941e-05, 1.430047723260941e-05, 1.430047723260941e-05, 1.430047723260941e-05, 1.430047723260941e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.430047723260941e-05

Optimization complete. Final v2v error: 3.1998159885406494 mm

Highest mean error: 3.591700792312622 mm for frame 209

Lowest mean error: 2.7888739109039307 mm for frame 5

Saving results

Total time: 45.11857771873474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00673952
Iteration 2/25 | Loss: 0.00128157
Iteration 3/25 | Loss: 0.00118812
Iteration 4/25 | Loss: 0.00117686
Iteration 5/25 | Loss: 0.00117331
Iteration 6/25 | Loss: 0.00117233
Iteration 7/25 | Loss: 0.00117232
Iteration 8/25 | Loss: 0.00117232
Iteration 9/25 | Loss: 0.00117232
Iteration 10/25 | Loss: 0.00117232
Iteration 11/25 | Loss: 0.00117232
Iteration 12/25 | Loss: 0.00117232
Iteration 13/25 | Loss: 0.00117232
Iteration 14/25 | Loss: 0.00117232
Iteration 15/25 | Loss: 0.00117232
Iteration 16/25 | Loss: 0.00117232
Iteration 17/25 | Loss: 0.00117232
Iteration 18/25 | Loss: 0.00117232
Iteration 19/25 | Loss: 0.00117232
Iteration 20/25 | Loss: 0.00117232
Iteration 21/25 | Loss: 0.00117232
Iteration 22/25 | Loss: 0.00117232
Iteration 23/25 | Loss: 0.00117232
Iteration 24/25 | Loss: 0.00117232
Iteration 25/25 | Loss: 0.00117232

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35819745
Iteration 2/25 | Loss: 0.00135725
Iteration 3/25 | Loss: 0.00135725
Iteration 4/25 | Loss: 0.00135725
Iteration 5/25 | Loss: 0.00135725
Iteration 6/25 | Loss: 0.00135725
Iteration 7/25 | Loss: 0.00135725
Iteration 8/25 | Loss: 0.00135725
Iteration 9/25 | Loss: 0.00135725
Iteration 10/25 | Loss: 0.00135725
Iteration 11/25 | Loss: 0.00135725
Iteration 12/25 | Loss: 0.00135725
Iteration 13/25 | Loss: 0.00135725
Iteration 14/25 | Loss: 0.00135725
Iteration 15/25 | Loss: 0.00135725
Iteration 16/25 | Loss: 0.00135725
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013572456082329154, 0.0013572456082329154, 0.0013572456082329154, 0.0013572456082329154, 0.0013572456082329154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013572456082329154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135725
Iteration 2/1000 | Loss: 0.00002289
Iteration 3/1000 | Loss: 0.00001601
Iteration 4/1000 | Loss: 0.00001266
Iteration 5/1000 | Loss: 0.00001169
Iteration 6/1000 | Loss: 0.00001107
Iteration 7/1000 | Loss: 0.00001058
Iteration 8/1000 | Loss: 0.00001022
Iteration 9/1000 | Loss: 0.00001012
Iteration 10/1000 | Loss: 0.00000998
Iteration 11/1000 | Loss: 0.00000977
Iteration 12/1000 | Loss: 0.00000966
Iteration 13/1000 | Loss: 0.00000961
Iteration 14/1000 | Loss: 0.00000959
Iteration 15/1000 | Loss: 0.00000948
Iteration 16/1000 | Loss: 0.00000946
Iteration 17/1000 | Loss: 0.00000946
Iteration 18/1000 | Loss: 0.00000939
Iteration 19/1000 | Loss: 0.00000936
Iteration 20/1000 | Loss: 0.00000935
Iteration 21/1000 | Loss: 0.00000931
Iteration 22/1000 | Loss: 0.00000929
Iteration 23/1000 | Loss: 0.00000928
Iteration 24/1000 | Loss: 0.00000927
Iteration 25/1000 | Loss: 0.00000927
Iteration 26/1000 | Loss: 0.00000926
Iteration 27/1000 | Loss: 0.00000925
Iteration 28/1000 | Loss: 0.00000925
Iteration 29/1000 | Loss: 0.00000924
Iteration 30/1000 | Loss: 0.00000924
Iteration 31/1000 | Loss: 0.00000923
Iteration 32/1000 | Loss: 0.00000923
Iteration 33/1000 | Loss: 0.00000922
Iteration 34/1000 | Loss: 0.00000921
Iteration 35/1000 | Loss: 0.00000919
Iteration 36/1000 | Loss: 0.00000919
Iteration 37/1000 | Loss: 0.00000919
Iteration 38/1000 | Loss: 0.00000919
Iteration 39/1000 | Loss: 0.00000918
Iteration 40/1000 | Loss: 0.00000918
Iteration 41/1000 | Loss: 0.00000918
Iteration 42/1000 | Loss: 0.00000917
Iteration 43/1000 | Loss: 0.00000917
Iteration 44/1000 | Loss: 0.00000915
Iteration 45/1000 | Loss: 0.00000915
Iteration 46/1000 | Loss: 0.00000915
Iteration 47/1000 | Loss: 0.00000915
Iteration 48/1000 | Loss: 0.00000914
Iteration 49/1000 | Loss: 0.00000914
Iteration 50/1000 | Loss: 0.00000914
Iteration 51/1000 | Loss: 0.00000914
Iteration 52/1000 | Loss: 0.00000914
Iteration 53/1000 | Loss: 0.00000913
Iteration 54/1000 | Loss: 0.00000913
Iteration 55/1000 | Loss: 0.00000913
Iteration 56/1000 | Loss: 0.00000913
Iteration 57/1000 | Loss: 0.00000913
Iteration 58/1000 | Loss: 0.00000912
Iteration 59/1000 | Loss: 0.00000912
Iteration 60/1000 | Loss: 0.00000912
Iteration 61/1000 | Loss: 0.00000912
Iteration 62/1000 | Loss: 0.00000912
Iteration 63/1000 | Loss: 0.00000911
Iteration 64/1000 | Loss: 0.00000911
Iteration 65/1000 | Loss: 0.00000911
Iteration 66/1000 | Loss: 0.00000911
Iteration 67/1000 | Loss: 0.00000910
Iteration 68/1000 | Loss: 0.00000910
Iteration 69/1000 | Loss: 0.00000909
Iteration 70/1000 | Loss: 0.00000909
Iteration 71/1000 | Loss: 0.00000908
Iteration 72/1000 | Loss: 0.00000908
Iteration 73/1000 | Loss: 0.00000908
Iteration 74/1000 | Loss: 0.00000908
Iteration 75/1000 | Loss: 0.00000908
Iteration 76/1000 | Loss: 0.00000908
Iteration 77/1000 | Loss: 0.00000907
Iteration 78/1000 | Loss: 0.00000907
Iteration 79/1000 | Loss: 0.00000906
Iteration 80/1000 | Loss: 0.00000906
Iteration 81/1000 | Loss: 0.00000905
Iteration 82/1000 | Loss: 0.00000905
Iteration 83/1000 | Loss: 0.00000905
Iteration 84/1000 | Loss: 0.00000905
Iteration 85/1000 | Loss: 0.00000905
Iteration 86/1000 | Loss: 0.00000905
Iteration 87/1000 | Loss: 0.00000904
Iteration 88/1000 | Loss: 0.00000904
Iteration 89/1000 | Loss: 0.00000904
Iteration 90/1000 | Loss: 0.00000904
Iteration 91/1000 | Loss: 0.00000904
Iteration 92/1000 | Loss: 0.00000904
Iteration 93/1000 | Loss: 0.00000904
Iteration 94/1000 | Loss: 0.00000903
Iteration 95/1000 | Loss: 0.00000903
Iteration 96/1000 | Loss: 0.00000903
Iteration 97/1000 | Loss: 0.00000903
Iteration 98/1000 | Loss: 0.00000903
Iteration 99/1000 | Loss: 0.00000902
Iteration 100/1000 | Loss: 0.00000902
Iteration 101/1000 | Loss: 0.00000902
Iteration 102/1000 | Loss: 0.00000902
Iteration 103/1000 | Loss: 0.00000902
Iteration 104/1000 | Loss: 0.00000901
Iteration 105/1000 | Loss: 0.00000901
Iteration 106/1000 | Loss: 0.00000901
Iteration 107/1000 | Loss: 0.00000901
Iteration 108/1000 | Loss: 0.00000901
Iteration 109/1000 | Loss: 0.00000901
Iteration 110/1000 | Loss: 0.00000900
Iteration 111/1000 | Loss: 0.00000900
Iteration 112/1000 | Loss: 0.00000900
Iteration 113/1000 | Loss: 0.00000900
Iteration 114/1000 | Loss: 0.00000900
Iteration 115/1000 | Loss: 0.00000899
Iteration 116/1000 | Loss: 0.00000899
Iteration 117/1000 | Loss: 0.00000899
Iteration 118/1000 | Loss: 0.00000899
Iteration 119/1000 | Loss: 0.00000899
Iteration 120/1000 | Loss: 0.00000899
Iteration 121/1000 | Loss: 0.00000899
Iteration 122/1000 | Loss: 0.00000899
Iteration 123/1000 | Loss: 0.00000899
Iteration 124/1000 | Loss: 0.00000899
Iteration 125/1000 | Loss: 0.00000899
Iteration 126/1000 | Loss: 0.00000898
Iteration 127/1000 | Loss: 0.00000898
Iteration 128/1000 | Loss: 0.00000898
Iteration 129/1000 | Loss: 0.00000898
Iteration 130/1000 | Loss: 0.00000898
Iteration 131/1000 | Loss: 0.00000898
Iteration 132/1000 | Loss: 0.00000898
Iteration 133/1000 | Loss: 0.00000897
Iteration 134/1000 | Loss: 0.00000897
Iteration 135/1000 | Loss: 0.00000897
Iteration 136/1000 | Loss: 0.00000897
Iteration 137/1000 | Loss: 0.00000897
Iteration 138/1000 | Loss: 0.00000897
Iteration 139/1000 | Loss: 0.00000897
Iteration 140/1000 | Loss: 0.00000897
Iteration 141/1000 | Loss: 0.00000897
Iteration 142/1000 | Loss: 0.00000897
Iteration 143/1000 | Loss: 0.00000897
Iteration 144/1000 | Loss: 0.00000896
Iteration 145/1000 | Loss: 0.00000896
Iteration 146/1000 | Loss: 0.00000896
Iteration 147/1000 | Loss: 0.00000896
Iteration 148/1000 | Loss: 0.00000896
Iteration 149/1000 | Loss: 0.00000896
Iteration 150/1000 | Loss: 0.00000896
Iteration 151/1000 | Loss: 0.00000896
Iteration 152/1000 | Loss: 0.00000896
Iteration 153/1000 | Loss: 0.00000896
Iteration 154/1000 | Loss: 0.00000896
Iteration 155/1000 | Loss: 0.00000896
Iteration 156/1000 | Loss: 0.00000896
Iteration 157/1000 | Loss: 0.00000896
Iteration 158/1000 | Loss: 0.00000896
Iteration 159/1000 | Loss: 0.00000896
Iteration 160/1000 | Loss: 0.00000896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [8.96079927770188e-06, 8.96079927770188e-06, 8.96079927770188e-06, 8.96079927770188e-06, 8.96079927770188e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.96079927770188e-06

Optimization complete. Final v2v error: 2.5650417804718018 mm

Highest mean error: 3.328317403793335 mm for frame 73

Lowest mean error: 2.3324005603790283 mm for frame 16

Saving results

Total time: 36.79122304916382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022691
Iteration 2/25 | Loss: 0.00165703
Iteration 3/25 | Loss: 0.00142151
Iteration 4/25 | Loss: 0.00138104
Iteration 5/25 | Loss: 0.00135845
Iteration 6/25 | Loss: 0.00128568
Iteration 7/25 | Loss: 0.00129957
Iteration 8/25 | Loss: 0.00126127
Iteration 9/25 | Loss: 0.00129208
Iteration 10/25 | Loss: 0.00123908
Iteration 11/25 | Loss: 0.00123615
Iteration 12/25 | Loss: 0.00123108
Iteration 13/25 | Loss: 0.00122654
Iteration 14/25 | Loss: 0.00123549
Iteration 15/25 | Loss: 0.00123569
Iteration 16/25 | Loss: 0.00122995
Iteration 17/25 | Loss: 0.00122879
Iteration 18/25 | Loss: 0.00122647
Iteration 19/25 | Loss: 0.00123188
Iteration 20/25 | Loss: 0.00122805
Iteration 21/25 | Loss: 0.00122693
Iteration 22/25 | Loss: 0.00122761
Iteration 23/25 | Loss: 0.00122137
Iteration 24/25 | Loss: 0.00121807
Iteration 25/25 | Loss: 0.00121783

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47290683
Iteration 2/25 | Loss: 0.00151366
Iteration 3/25 | Loss: 0.00151366
Iteration 4/25 | Loss: 0.00151366
Iteration 5/25 | Loss: 0.00151366
Iteration 6/25 | Loss: 0.00151365
Iteration 7/25 | Loss: 0.00151365
Iteration 8/25 | Loss: 0.00151365
Iteration 9/25 | Loss: 0.00151365
Iteration 10/25 | Loss: 0.00151365
Iteration 11/25 | Loss: 0.00151365
Iteration 12/25 | Loss: 0.00151365
Iteration 13/25 | Loss: 0.00151365
Iteration 14/25 | Loss: 0.00151365
Iteration 15/25 | Loss: 0.00151365
Iteration 16/25 | Loss: 0.00151365
Iteration 17/25 | Loss: 0.00151365
Iteration 18/25 | Loss: 0.00151365
Iteration 19/25 | Loss: 0.00151365
Iteration 20/25 | Loss: 0.00151365
Iteration 21/25 | Loss: 0.00151365
Iteration 22/25 | Loss: 0.00151365
Iteration 23/25 | Loss: 0.00151365
Iteration 24/25 | Loss: 0.00151365
Iteration 25/25 | Loss: 0.00151365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151365
Iteration 2/1000 | Loss: 0.00003369
Iteration 3/1000 | Loss: 0.00002438
Iteration 4/1000 | Loss: 0.00002126
Iteration 5/1000 | Loss: 0.00030004
Iteration 6/1000 | Loss: 0.00002033
Iteration 7/1000 | Loss: 0.00003151
Iteration 8/1000 | Loss: 0.00002848
Iteration 9/1000 | Loss: 0.00002068
Iteration 10/1000 | Loss: 0.00001885
Iteration 11/1000 | Loss: 0.00004039
Iteration 12/1000 | Loss: 0.00001845
Iteration 13/1000 | Loss: 0.00001831
Iteration 14/1000 | Loss: 0.00001809
Iteration 15/1000 | Loss: 0.00001797
Iteration 16/1000 | Loss: 0.00001795
Iteration 17/1000 | Loss: 0.00001795
Iteration 18/1000 | Loss: 0.00001792
Iteration 19/1000 | Loss: 0.00001785
Iteration 20/1000 | Loss: 0.00001781
Iteration 21/1000 | Loss: 0.00001777
Iteration 22/1000 | Loss: 0.00001775
Iteration 23/1000 | Loss: 0.00001774
Iteration 24/1000 | Loss: 0.00001772
Iteration 25/1000 | Loss: 0.00001767
Iteration 26/1000 | Loss: 0.00001767
Iteration 27/1000 | Loss: 0.00001763
Iteration 28/1000 | Loss: 0.00001762
Iteration 29/1000 | Loss: 0.00001762
Iteration 30/1000 | Loss: 0.00001762
Iteration 31/1000 | Loss: 0.00001762
Iteration 32/1000 | Loss: 0.00001762
Iteration 33/1000 | Loss: 0.00001761
Iteration 34/1000 | Loss: 0.00001761
Iteration 35/1000 | Loss: 0.00001760
Iteration 36/1000 | Loss: 0.00001758
Iteration 37/1000 | Loss: 0.00001758
Iteration 38/1000 | Loss: 0.00001758
Iteration 39/1000 | Loss: 0.00001758
Iteration 40/1000 | Loss: 0.00001758
Iteration 41/1000 | Loss: 0.00001758
Iteration 42/1000 | Loss: 0.00001758
Iteration 43/1000 | Loss: 0.00001758
Iteration 44/1000 | Loss: 0.00001758
Iteration 45/1000 | Loss: 0.00001758
Iteration 46/1000 | Loss: 0.00001758
Iteration 47/1000 | Loss: 0.00001758
Iteration 48/1000 | Loss: 0.00001757
Iteration 49/1000 | Loss: 0.00001757
Iteration 50/1000 | Loss: 0.00001755
Iteration 51/1000 | Loss: 0.00001755
Iteration 52/1000 | Loss: 0.00001755
Iteration 53/1000 | Loss: 0.00001755
Iteration 54/1000 | Loss: 0.00001755
Iteration 55/1000 | Loss: 0.00001755
Iteration 56/1000 | Loss: 0.00001755
Iteration 57/1000 | Loss: 0.00001755
Iteration 58/1000 | Loss: 0.00001754
Iteration 59/1000 | Loss: 0.00001754
Iteration 60/1000 | Loss: 0.00001753
Iteration 61/1000 | Loss: 0.00001752
Iteration 62/1000 | Loss: 0.00001752
Iteration 63/1000 | Loss: 0.00001752
Iteration 64/1000 | Loss: 0.00001751
Iteration 65/1000 | Loss: 0.00001751
Iteration 66/1000 | Loss: 0.00001751
Iteration 67/1000 | Loss: 0.00001751
Iteration 68/1000 | Loss: 0.00001750
Iteration 69/1000 | Loss: 0.00001750
Iteration 70/1000 | Loss: 0.00001750
Iteration 71/1000 | Loss: 0.00001749
Iteration 72/1000 | Loss: 0.00001749
Iteration 73/1000 | Loss: 0.00001748
Iteration 74/1000 | Loss: 0.00001748
Iteration 75/1000 | Loss: 0.00001748
Iteration 76/1000 | Loss: 0.00001747
Iteration 77/1000 | Loss: 0.00001747
Iteration 78/1000 | Loss: 0.00001747
Iteration 79/1000 | Loss: 0.00001747
Iteration 80/1000 | Loss: 0.00001746
Iteration 81/1000 | Loss: 0.00001746
Iteration 82/1000 | Loss: 0.00001746
Iteration 83/1000 | Loss: 0.00001745
Iteration 84/1000 | Loss: 0.00001745
Iteration 85/1000 | Loss: 0.00001745
Iteration 86/1000 | Loss: 0.00001745
Iteration 87/1000 | Loss: 0.00001744
Iteration 88/1000 | Loss: 0.00001744
Iteration 89/1000 | Loss: 0.00001744
Iteration 90/1000 | Loss: 0.00001744
Iteration 91/1000 | Loss: 0.00001743
Iteration 92/1000 | Loss: 0.00001743
Iteration 93/1000 | Loss: 0.00001743
Iteration 94/1000 | Loss: 0.00001742
Iteration 95/1000 | Loss: 0.00001742
Iteration 96/1000 | Loss: 0.00001742
Iteration 97/1000 | Loss: 0.00001742
Iteration 98/1000 | Loss: 0.00001741
Iteration 99/1000 | Loss: 0.00001741
Iteration 100/1000 | Loss: 0.00001741
Iteration 101/1000 | Loss: 0.00001967
Iteration 102/1000 | Loss: 0.00001737
Iteration 103/1000 | Loss: 0.00001721
Iteration 104/1000 | Loss: 0.00001719
Iteration 105/1000 | Loss: 0.00001715
Iteration 106/1000 | Loss: 0.00001715
Iteration 107/1000 | Loss: 0.00001715
Iteration 108/1000 | Loss: 0.00001714
Iteration 109/1000 | Loss: 0.00001713
Iteration 110/1000 | Loss: 0.00001713
Iteration 111/1000 | Loss: 0.00002655
Iteration 112/1000 | Loss: 0.00001711
Iteration 113/1000 | Loss: 0.00001709
Iteration 114/1000 | Loss: 0.00001708
Iteration 115/1000 | Loss: 0.00001708
Iteration 116/1000 | Loss: 0.00001708
Iteration 117/1000 | Loss: 0.00001707
Iteration 118/1000 | Loss: 0.00001707
Iteration 119/1000 | Loss: 0.00001707
Iteration 120/1000 | Loss: 0.00001707
Iteration 121/1000 | Loss: 0.00001707
Iteration 122/1000 | Loss: 0.00001707
Iteration 123/1000 | Loss: 0.00001707
Iteration 124/1000 | Loss: 0.00001707
Iteration 125/1000 | Loss: 0.00001707
Iteration 126/1000 | Loss: 0.00001707
Iteration 127/1000 | Loss: 0.00001706
Iteration 128/1000 | Loss: 0.00001706
Iteration 129/1000 | Loss: 0.00001705
Iteration 130/1000 | Loss: 0.00001705
Iteration 131/1000 | Loss: 0.00001704
Iteration 132/1000 | Loss: 0.00001704
Iteration 133/1000 | Loss: 0.00001704
Iteration 134/1000 | Loss: 0.00001704
Iteration 135/1000 | Loss: 0.00001703
Iteration 136/1000 | Loss: 0.00001703
Iteration 137/1000 | Loss: 0.00001702
Iteration 138/1000 | Loss: 0.00001702
Iteration 139/1000 | Loss: 0.00001701
Iteration 140/1000 | Loss: 0.00001701
Iteration 141/1000 | Loss: 0.00002763
Iteration 142/1000 | Loss: 0.00001698
Iteration 143/1000 | Loss: 0.00001697
Iteration 144/1000 | Loss: 0.00001696
Iteration 145/1000 | Loss: 0.00001696
Iteration 146/1000 | Loss: 0.00001925
Iteration 147/1000 | Loss: 0.00001694
Iteration 148/1000 | Loss: 0.00001693
Iteration 149/1000 | Loss: 0.00001693
Iteration 150/1000 | Loss: 0.00001693
Iteration 151/1000 | Loss: 0.00001693
Iteration 152/1000 | Loss: 0.00001693
Iteration 153/1000 | Loss: 0.00001693
Iteration 154/1000 | Loss: 0.00001692
Iteration 155/1000 | Loss: 0.00001692
Iteration 156/1000 | Loss: 0.00001692
Iteration 157/1000 | Loss: 0.00001691
Iteration 158/1000 | Loss: 0.00001690
Iteration 159/1000 | Loss: 0.00001689
Iteration 160/1000 | Loss: 0.00001688
Iteration 161/1000 | Loss: 0.00001688
Iteration 162/1000 | Loss: 0.00001687
Iteration 163/1000 | Loss: 0.00001687
Iteration 164/1000 | Loss: 0.00001687
Iteration 165/1000 | Loss: 0.00001686
Iteration 166/1000 | Loss: 0.00001685
Iteration 167/1000 | Loss: 0.00001684
Iteration 168/1000 | Loss: 0.00001684
Iteration 169/1000 | Loss: 0.00001684
Iteration 170/1000 | Loss: 0.00001684
Iteration 171/1000 | Loss: 0.00001683
Iteration 172/1000 | Loss: 0.00001683
Iteration 173/1000 | Loss: 0.00001683
Iteration 174/1000 | Loss: 0.00001681
Iteration 175/1000 | Loss: 0.00001679
Iteration 176/1000 | Loss: 0.00001679
Iteration 177/1000 | Loss: 0.00001679
Iteration 178/1000 | Loss: 0.00001679
Iteration 179/1000 | Loss: 0.00001679
Iteration 180/1000 | Loss: 0.00001679
Iteration 181/1000 | Loss: 0.00001679
Iteration 182/1000 | Loss: 0.00001679
Iteration 183/1000 | Loss: 0.00001679
Iteration 184/1000 | Loss: 0.00001679
Iteration 185/1000 | Loss: 0.00001678
Iteration 186/1000 | Loss: 0.00001678
Iteration 187/1000 | Loss: 0.00001677
Iteration 188/1000 | Loss: 0.00001676
Iteration 189/1000 | Loss: 0.00001676
Iteration 190/1000 | Loss: 0.00001676
Iteration 191/1000 | Loss: 0.00001676
Iteration 192/1000 | Loss: 0.00001676
Iteration 193/1000 | Loss: 0.00001676
Iteration 194/1000 | Loss: 0.00001676
Iteration 195/1000 | Loss: 0.00001676
Iteration 196/1000 | Loss: 0.00001676
Iteration 197/1000 | Loss: 0.00001676
Iteration 198/1000 | Loss: 0.00001675
Iteration 199/1000 | Loss: 0.00001675
Iteration 200/1000 | Loss: 0.00001675
Iteration 201/1000 | Loss: 0.00001675
Iteration 202/1000 | Loss: 0.00001675
Iteration 203/1000 | Loss: 0.00001675
Iteration 204/1000 | Loss: 0.00001674
Iteration 205/1000 | Loss: 0.00001674
Iteration 206/1000 | Loss: 0.00001674
Iteration 207/1000 | Loss: 0.00001674
Iteration 208/1000 | Loss: 0.00001674
Iteration 209/1000 | Loss: 0.00001673
Iteration 210/1000 | Loss: 0.00001673
Iteration 211/1000 | Loss: 0.00001673
Iteration 212/1000 | Loss: 0.00001672
Iteration 213/1000 | Loss: 0.00001672
Iteration 214/1000 | Loss: 0.00001672
Iteration 215/1000 | Loss: 0.00001672
Iteration 216/1000 | Loss: 0.00001672
Iteration 217/1000 | Loss: 0.00001672
Iteration 218/1000 | Loss: 0.00001672
Iteration 219/1000 | Loss: 0.00001672
Iteration 220/1000 | Loss: 0.00001672
Iteration 221/1000 | Loss: 0.00001671
Iteration 222/1000 | Loss: 0.00001671
Iteration 223/1000 | Loss: 0.00001671
Iteration 224/1000 | Loss: 0.00001671
Iteration 225/1000 | Loss: 0.00001671
Iteration 226/1000 | Loss: 0.00001671
Iteration 227/1000 | Loss: 0.00001671
Iteration 228/1000 | Loss: 0.00001671
Iteration 229/1000 | Loss: 0.00001671
Iteration 230/1000 | Loss: 0.00001671
Iteration 231/1000 | Loss: 0.00001671
Iteration 232/1000 | Loss: 0.00001671
Iteration 233/1000 | Loss: 0.00001671
Iteration 234/1000 | Loss: 0.00001671
Iteration 235/1000 | Loss: 0.00001670
Iteration 236/1000 | Loss: 0.00001670
Iteration 237/1000 | Loss: 0.00001670
Iteration 238/1000 | Loss: 0.00001670
Iteration 239/1000 | Loss: 0.00001670
Iteration 240/1000 | Loss: 0.00001670
Iteration 241/1000 | Loss: 0.00001670
Iteration 242/1000 | Loss: 0.00001670
Iteration 243/1000 | Loss: 0.00001670
Iteration 244/1000 | Loss: 0.00001670
Iteration 245/1000 | Loss: 0.00001669
Iteration 246/1000 | Loss: 0.00001669
Iteration 247/1000 | Loss: 0.00001669
Iteration 248/1000 | Loss: 0.00001669
Iteration 249/1000 | Loss: 0.00001669
Iteration 250/1000 | Loss: 0.00001669
Iteration 251/1000 | Loss: 0.00001669
Iteration 252/1000 | Loss: 0.00001669
Iteration 253/1000 | Loss: 0.00001669
Iteration 254/1000 | Loss: 0.00001669
Iteration 255/1000 | Loss: 0.00001668
Iteration 256/1000 | Loss: 0.00001668
Iteration 257/1000 | Loss: 0.00001668
Iteration 258/1000 | Loss: 0.00001668
Iteration 259/1000 | Loss: 0.00001668
Iteration 260/1000 | Loss: 0.00001668
Iteration 261/1000 | Loss: 0.00001668
Iteration 262/1000 | Loss: 0.00001668
Iteration 263/1000 | Loss: 0.00001668
Iteration 264/1000 | Loss: 0.00001668
Iteration 265/1000 | Loss: 0.00001668
Iteration 266/1000 | Loss: 0.00001668
Iteration 267/1000 | Loss: 0.00001667
Iteration 268/1000 | Loss: 0.00001667
Iteration 269/1000 | Loss: 0.00001667
Iteration 270/1000 | Loss: 0.00001667
Iteration 271/1000 | Loss: 0.00001667
Iteration 272/1000 | Loss: 0.00001667
Iteration 273/1000 | Loss: 0.00001667
Iteration 274/1000 | Loss: 0.00001667
Iteration 275/1000 | Loss: 0.00001667
Iteration 276/1000 | Loss: 0.00001667
Iteration 277/1000 | Loss: 0.00001667
Iteration 278/1000 | Loss: 0.00001667
Iteration 279/1000 | Loss: 0.00001666
Iteration 280/1000 | Loss: 0.00001666
Iteration 281/1000 | Loss: 0.00001666
Iteration 282/1000 | Loss: 0.00001666
Iteration 283/1000 | Loss: 0.00001666
Iteration 284/1000 | Loss: 0.00001666
Iteration 285/1000 | Loss: 0.00001666
Iteration 286/1000 | Loss: 0.00001666
Iteration 287/1000 | Loss: 0.00001666
Iteration 288/1000 | Loss: 0.00001666
Iteration 289/1000 | Loss: 0.00001666
Iteration 290/1000 | Loss: 0.00001665
Iteration 291/1000 | Loss: 0.00001665
Iteration 292/1000 | Loss: 0.00001665
Iteration 293/1000 | Loss: 0.00001665
Iteration 294/1000 | Loss: 0.00001665
Iteration 295/1000 | Loss: 0.00001665
Iteration 296/1000 | Loss: 0.00001665
Iteration 297/1000 | Loss: 0.00001664
Iteration 298/1000 | Loss: 0.00001664
Iteration 299/1000 | Loss: 0.00001664
Iteration 300/1000 | Loss: 0.00001664
Iteration 301/1000 | Loss: 0.00001664
Iteration 302/1000 | Loss: 0.00001664
Iteration 303/1000 | Loss: 0.00001664
Iteration 304/1000 | Loss: 0.00001664
Iteration 305/1000 | Loss: 0.00001664
Iteration 306/1000 | Loss: 0.00001664
Iteration 307/1000 | Loss: 0.00001663
Iteration 308/1000 | Loss: 0.00001663
Iteration 309/1000 | Loss: 0.00001663
Iteration 310/1000 | Loss: 0.00001663
Iteration 311/1000 | Loss: 0.00001663
Iteration 312/1000 | Loss: 0.00001663
Iteration 313/1000 | Loss: 0.00001663
Iteration 314/1000 | Loss: 0.00001662
Iteration 315/1000 | Loss: 0.00001662
Iteration 316/1000 | Loss: 0.00001662
Iteration 317/1000 | Loss: 0.00001662
Iteration 318/1000 | Loss: 0.00001662
Iteration 319/1000 | Loss: 0.00001662
Iteration 320/1000 | Loss: 0.00001662
Iteration 321/1000 | Loss: 0.00001662
Iteration 322/1000 | Loss: 0.00001661
Iteration 323/1000 | Loss: 0.00001661
Iteration 324/1000 | Loss: 0.00001661
Iteration 325/1000 | Loss: 0.00001661
Iteration 326/1000 | Loss: 0.00001661
Iteration 327/1000 | Loss: 0.00001661
Iteration 328/1000 | Loss: 0.00001661
Iteration 329/1000 | Loss: 0.00001661
Iteration 330/1000 | Loss: 0.00001661
Iteration 331/1000 | Loss: 0.00001660
Iteration 332/1000 | Loss: 0.00001660
Iteration 333/1000 | Loss: 0.00001660
Iteration 334/1000 | Loss: 0.00001660
Iteration 335/1000 | Loss: 0.00001660
Iteration 336/1000 | Loss: 0.00001659
Iteration 337/1000 | Loss: 0.00001659
Iteration 338/1000 | Loss: 0.00001659
Iteration 339/1000 | Loss: 0.00003368
Iteration 340/1000 | Loss: 0.00001664
Iteration 341/1000 | Loss: 0.00001658
Iteration 342/1000 | Loss: 0.00001658
Iteration 343/1000 | Loss: 0.00001658
Iteration 344/1000 | Loss: 0.00001658
Iteration 345/1000 | Loss: 0.00001658
Iteration 346/1000 | Loss: 0.00001658
Iteration 347/1000 | Loss: 0.00001658
Iteration 348/1000 | Loss: 0.00001658
Iteration 349/1000 | Loss: 0.00001657
Iteration 350/1000 | Loss: 0.00001657
Iteration 351/1000 | Loss: 0.00001657
Iteration 352/1000 | Loss: 0.00001657
Iteration 353/1000 | Loss: 0.00001657
Iteration 354/1000 | Loss: 0.00001657
Iteration 355/1000 | Loss: 0.00001657
Iteration 356/1000 | Loss: 0.00001657
Iteration 357/1000 | Loss: 0.00001657
Iteration 358/1000 | Loss: 0.00001657
Iteration 359/1000 | Loss: 0.00001657
Iteration 360/1000 | Loss: 0.00001657
Iteration 361/1000 | Loss: 0.00001657
Iteration 362/1000 | Loss: 0.00001657
Iteration 363/1000 | Loss: 0.00001657
Iteration 364/1000 | Loss: 0.00001657
Iteration 365/1000 | Loss: 0.00001657
Iteration 366/1000 | Loss: 0.00001657
Iteration 367/1000 | Loss: 0.00001657
Iteration 368/1000 | Loss: 0.00001657
Iteration 369/1000 | Loss: 0.00001657
Iteration 370/1000 | Loss: 0.00001657
Iteration 371/1000 | Loss: 0.00001657
Iteration 372/1000 | Loss: 0.00001657
Iteration 373/1000 | Loss: 0.00001657
Iteration 374/1000 | Loss: 0.00001657
Iteration 375/1000 | Loss: 0.00001657
Iteration 376/1000 | Loss: 0.00001657
Iteration 377/1000 | Loss: 0.00001657
Iteration 378/1000 | Loss: 0.00001657
Iteration 379/1000 | Loss: 0.00001657
Iteration 380/1000 | Loss: 0.00001657
Iteration 381/1000 | Loss: 0.00001657
Iteration 382/1000 | Loss: 0.00001657
Iteration 383/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 383. Stopping optimization.
Last 5 losses: [1.656647327763494e-05, 1.656647327763494e-05, 1.656647327763494e-05, 1.656647327763494e-05, 1.656647327763494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.656647327763494e-05

Optimization complete. Final v2v error: 2.8865301609039307 mm

Highest mean error: 20.66287612915039 mm for frame 67

Lowest mean error: 2.5158133506774902 mm for frame 136

Saving results

Total time: 102.46797442436218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429924
Iteration 2/25 | Loss: 0.00131008
Iteration 3/25 | Loss: 0.00121936
Iteration 4/25 | Loss: 0.00121676
Iteration 5/25 | Loss: 0.00121643
Iteration 6/25 | Loss: 0.00121643
Iteration 7/25 | Loss: 0.00121643
Iteration 8/25 | Loss: 0.00121643
Iteration 9/25 | Loss: 0.00121643
Iteration 10/25 | Loss: 0.00121643
Iteration 11/25 | Loss: 0.00121643
Iteration 12/25 | Loss: 0.00121643
Iteration 13/25 | Loss: 0.00121643
Iteration 14/25 | Loss: 0.00121643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012164267245680094, 0.0012164267245680094, 0.0012164267245680094, 0.0012164267245680094, 0.0012164267245680094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012164267245680094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.24310446
Iteration 2/25 | Loss: 0.00094125
Iteration 3/25 | Loss: 0.00094123
Iteration 4/25 | Loss: 0.00094123
Iteration 5/25 | Loss: 0.00094123
Iteration 6/25 | Loss: 0.00094123
Iteration 7/25 | Loss: 0.00094123
Iteration 8/25 | Loss: 0.00094123
Iteration 9/25 | Loss: 0.00094123
Iteration 10/25 | Loss: 0.00094123
Iteration 11/25 | Loss: 0.00094123
Iteration 12/25 | Loss: 0.00094123
Iteration 13/25 | Loss: 0.00094123
Iteration 14/25 | Loss: 0.00094123
Iteration 15/25 | Loss: 0.00094123
Iteration 16/25 | Loss: 0.00094123
Iteration 17/25 | Loss: 0.00094123
Iteration 18/25 | Loss: 0.00094123
Iteration 19/25 | Loss: 0.00094123
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000941228645388037, 0.000941228645388037, 0.000941228645388037, 0.000941228645388037, 0.000941228645388037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000941228645388037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094123
Iteration 2/1000 | Loss: 0.00002405
Iteration 3/1000 | Loss: 0.00001953
Iteration 4/1000 | Loss: 0.00001808
Iteration 5/1000 | Loss: 0.00001731
Iteration 6/1000 | Loss: 0.00001657
Iteration 7/1000 | Loss: 0.00001606
Iteration 8/1000 | Loss: 0.00001562
Iteration 9/1000 | Loss: 0.00001534
Iteration 10/1000 | Loss: 0.00001523
Iteration 11/1000 | Loss: 0.00001503
Iteration 12/1000 | Loss: 0.00001491
Iteration 13/1000 | Loss: 0.00001487
Iteration 14/1000 | Loss: 0.00001486
Iteration 15/1000 | Loss: 0.00001486
Iteration 16/1000 | Loss: 0.00001485
Iteration 17/1000 | Loss: 0.00001485
Iteration 18/1000 | Loss: 0.00001484
Iteration 19/1000 | Loss: 0.00001484
Iteration 20/1000 | Loss: 0.00001483
Iteration 21/1000 | Loss: 0.00001480
Iteration 22/1000 | Loss: 0.00001476
Iteration 23/1000 | Loss: 0.00001470
Iteration 24/1000 | Loss: 0.00001469
Iteration 25/1000 | Loss: 0.00001467
Iteration 26/1000 | Loss: 0.00001466
Iteration 27/1000 | Loss: 0.00001466
Iteration 28/1000 | Loss: 0.00001461
Iteration 29/1000 | Loss: 0.00001456
Iteration 30/1000 | Loss: 0.00001454
Iteration 31/1000 | Loss: 0.00001450
Iteration 32/1000 | Loss: 0.00001447
Iteration 33/1000 | Loss: 0.00001447
Iteration 34/1000 | Loss: 0.00001446
Iteration 35/1000 | Loss: 0.00001446
Iteration 36/1000 | Loss: 0.00001445
Iteration 37/1000 | Loss: 0.00001445
Iteration 38/1000 | Loss: 0.00001445
Iteration 39/1000 | Loss: 0.00001445
Iteration 40/1000 | Loss: 0.00001445
Iteration 41/1000 | Loss: 0.00001445
Iteration 42/1000 | Loss: 0.00001445
Iteration 43/1000 | Loss: 0.00001444
Iteration 44/1000 | Loss: 0.00001444
Iteration 45/1000 | Loss: 0.00001444
Iteration 46/1000 | Loss: 0.00001444
Iteration 47/1000 | Loss: 0.00001444
Iteration 48/1000 | Loss: 0.00001443
Iteration 49/1000 | Loss: 0.00001442
Iteration 50/1000 | Loss: 0.00001442
Iteration 51/1000 | Loss: 0.00001442
Iteration 52/1000 | Loss: 0.00001441
Iteration 53/1000 | Loss: 0.00001441
Iteration 54/1000 | Loss: 0.00001440
Iteration 55/1000 | Loss: 0.00001440
Iteration 56/1000 | Loss: 0.00001440
Iteration 57/1000 | Loss: 0.00001440
Iteration 58/1000 | Loss: 0.00001440
Iteration 59/1000 | Loss: 0.00001440
Iteration 60/1000 | Loss: 0.00001440
Iteration 61/1000 | Loss: 0.00001440
Iteration 62/1000 | Loss: 0.00001440
Iteration 63/1000 | Loss: 0.00001439
Iteration 64/1000 | Loss: 0.00001439
Iteration 65/1000 | Loss: 0.00001439
Iteration 66/1000 | Loss: 0.00001439
Iteration 67/1000 | Loss: 0.00001439
Iteration 68/1000 | Loss: 0.00001439
Iteration 69/1000 | Loss: 0.00001439
Iteration 70/1000 | Loss: 0.00001439
Iteration 71/1000 | Loss: 0.00001439
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001439
Iteration 74/1000 | Loss: 0.00001439
Iteration 75/1000 | Loss: 0.00001439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.438713661627844e-05, 1.438713661627844e-05, 1.438713661627844e-05, 1.438713661627844e-05, 1.438713661627844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.438713661627844e-05

Optimization complete. Final v2v error: 3.2527639865875244 mm

Highest mean error: 3.648855209350586 mm for frame 101

Lowest mean error: 2.960175037384033 mm for frame 130

Saving results

Total time: 29.75174331665039
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500497
Iteration 2/25 | Loss: 0.00136734
Iteration 3/25 | Loss: 0.00126242
Iteration 4/25 | Loss: 0.00125134
Iteration 5/25 | Loss: 0.00124885
Iteration 6/25 | Loss: 0.00124812
Iteration 7/25 | Loss: 0.00124812
Iteration 8/25 | Loss: 0.00124812
Iteration 9/25 | Loss: 0.00124812
Iteration 10/25 | Loss: 0.00124812
Iteration 11/25 | Loss: 0.00124812
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012481193989515305, 0.0012481193989515305, 0.0012481193989515305, 0.0012481193989515305, 0.0012481193989515305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012481193989515305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35502374
Iteration 2/25 | Loss: 0.00129408
Iteration 3/25 | Loss: 0.00129403
Iteration 4/25 | Loss: 0.00129403
Iteration 5/25 | Loss: 0.00129403
Iteration 6/25 | Loss: 0.00129403
Iteration 7/25 | Loss: 0.00129403
Iteration 8/25 | Loss: 0.00129403
Iteration 9/25 | Loss: 0.00129403
Iteration 10/25 | Loss: 0.00129403
Iteration 11/25 | Loss: 0.00129403
Iteration 12/25 | Loss: 0.00129403
Iteration 13/25 | Loss: 0.00129403
Iteration 14/25 | Loss: 0.00129403
Iteration 15/25 | Loss: 0.00129403
Iteration 16/25 | Loss: 0.00129403
Iteration 17/25 | Loss: 0.00129403
Iteration 18/25 | Loss: 0.00129403
Iteration 19/25 | Loss: 0.00129403
Iteration 20/25 | Loss: 0.00129403
Iteration 21/25 | Loss: 0.00129403
Iteration 22/25 | Loss: 0.00129403
Iteration 23/25 | Loss: 0.00129403
Iteration 24/25 | Loss: 0.00129403
Iteration 25/25 | Loss: 0.00129403

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129403
Iteration 2/1000 | Loss: 0.00004935
Iteration 3/1000 | Loss: 0.00002849
Iteration 4/1000 | Loss: 0.00002195
Iteration 5/1000 | Loss: 0.00002010
Iteration 6/1000 | Loss: 0.00001890
Iteration 7/1000 | Loss: 0.00001845
Iteration 8/1000 | Loss: 0.00001793
Iteration 9/1000 | Loss: 0.00001747
Iteration 10/1000 | Loss: 0.00001707
Iteration 11/1000 | Loss: 0.00001685
Iteration 12/1000 | Loss: 0.00001683
Iteration 13/1000 | Loss: 0.00001668
Iteration 14/1000 | Loss: 0.00001649
Iteration 15/1000 | Loss: 0.00001644
Iteration 16/1000 | Loss: 0.00001642
Iteration 17/1000 | Loss: 0.00001641
Iteration 18/1000 | Loss: 0.00001640
Iteration 19/1000 | Loss: 0.00001636
Iteration 20/1000 | Loss: 0.00001629
Iteration 21/1000 | Loss: 0.00001621
Iteration 22/1000 | Loss: 0.00001620
Iteration 23/1000 | Loss: 0.00001615
Iteration 24/1000 | Loss: 0.00001613
Iteration 25/1000 | Loss: 0.00001612
Iteration 26/1000 | Loss: 0.00001612
Iteration 27/1000 | Loss: 0.00001611
Iteration 28/1000 | Loss: 0.00001610
Iteration 29/1000 | Loss: 0.00001609
Iteration 30/1000 | Loss: 0.00001608
Iteration 31/1000 | Loss: 0.00001608
Iteration 32/1000 | Loss: 0.00001607
Iteration 33/1000 | Loss: 0.00001606
Iteration 34/1000 | Loss: 0.00001605
Iteration 35/1000 | Loss: 0.00001605
Iteration 36/1000 | Loss: 0.00001604
Iteration 37/1000 | Loss: 0.00001604
Iteration 38/1000 | Loss: 0.00001603
Iteration 39/1000 | Loss: 0.00001602
Iteration 40/1000 | Loss: 0.00001600
Iteration 41/1000 | Loss: 0.00001600
Iteration 42/1000 | Loss: 0.00001600
Iteration 43/1000 | Loss: 0.00001600
Iteration 44/1000 | Loss: 0.00001600
Iteration 45/1000 | Loss: 0.00001600
Iteration 46/1000 | Loss: 0.00001600
Iteration 47/1000 | Loss: 0.00001600
Iteration 48/1000 | Loss: 0.00001599
Iteration 49/1000 | Loss: 0.00001599
Iteration 50/1000 | Loss: 0.00001599
Iteration 51/1000 | Loss: 0.00001599
Iteration 52/1000 | Loss: 0.00001599
Iteration 53/1000 | Loss: 0.00001599
Iteration 54/1000 | Loss: 0.00001597
Iteration 55/1000 | Loss: 0.00001597
Iteration 56/1000 | Loss: 0.00001597
Iteration 57/1000 | Loss: 0.00001596
Iteration 58/1000 | Loss: 0.00001596
Iteration 59/1000 | Loss: 0.00001596
Iteration 60/1000 | Loss: 0.00001596
Iteration 61/1000 | Loss: 0.00001596
Iteration 62/1000 | Loss: 0.00001596
Iteration 63/1000 | Loss: 0.00001596
Iteration 64/1000 | Loss: 0.00001596
Iteration 65/1000 | Loss: 0.00001595
Iteration 66/1000 | Loss: 0.00001594
Iteration 67/1000 | Loss: 0.00001594
Iteration 68/1000 | Loss: 0.00001594
Iteration 69/1000 | Loss: 0.00001593
Iteration 70/1000 | Loss: 0.00001593
Iteration 71/1000 | Loss: 0.00001593
Iteration 72/1000 | Loss: 0.00001593
Iteration 73/1000 | Loss: 0.00001592
Iteration 74/1000 | Loss: 0.00001592
Iteration 75/1000 | Loss: 0.00001592
Iteration 76/1000 | Loss: 0.00001591
Iteration 77/1000 | Loss: 0.00001591
Iteration 78/1000 | Loss: 0.00001591
Iteration 79/1000 | Loss: 0.00001590
Iteration 80/1000 | Loss: 0.00001590
Iteration 81/1000 | Loss: 0.00001589
Iteration 82/1000 | Loss: 0.00001589
Iteration 83/1000 | Loss: 0.00001589
Iteration 84/1000 | Loss: 0.00001589
Iteration 85/1000 | Loss: 0.00001588
Iteration 86/1000 | Loss: 0.00001588
Iteration 87/1000 | Loss: 0.00001588
Iteration 88/1000 | Loss: 0.00001588
Iteration 89/1000 | Loss: 0.00001588
Iteration 90/1000 | Loss: 0.00001587
Iteration 91/1000 | Loss: 0.00001587
Iteration 92/1000 | Loss: 0.00001587
Iteration 93/1000 | Loss: 0.00001587
Iteration 94/1000 | Loss: 0.00001586
Iteration 95/1000 | Loss: 0.00001586
Iteration 96/1000 | Loss: 0.00001586
Iteration 97/1000 | Loss: 0.00001586
Iteration 98/1000 | Loss: 0.00001586
Iteration 99/1000 | Loss: 0.00001586
Iteration 100/1000 | Loss: 0.00001586
Iteration 101/1000 | Loss: 0.00001586
Iteration 102/1000 | Loss: 0.00001586
Iteration 103/1000 | Loss: 0.00001585
Iteration 104/1000 | Loss: 0.00001585
Iteration 105/1000 | Loss: 0.00001585
Iteration 106/1000 | Loss: 0.00001585
Iteration 107/1000 | Loss: 0.00001585
Iteration 108/1000 | Loss: 0.00001585
Iteration 109/1000 | Loss: 0.00001585
Iteration 110/1000 | Loss: 0.00001585
Iteration 111/1000 | Loss: 0.00001585
Iteration 112/1000 | Loss: 0.00001585
Iteration 113/1000 | Loss: 0.00001584
Iteration 114/1000 | Loss: 0.00001584
Iteration 115/1000 | Loss: 0.00001584
Iteration 116/1000 | Loss: 0.00001584
Iteration 117/1000 | Loss: 0.00001584
Iteration 118/1000 | Loss: 0.00001583
Iteration 119/1000 | Loss: 0.00001583
Iteration 120/1000 | Loss: 0.00001583
Iteration 121/1000 | Loss: 0.00001583
Iteration 122/1000 | Loss: 0.00001582
Iteration 123/1000 | Loss: 0.00001582
Iteration 124/1000 | Loss: 0.00001582
Iteration 125/1000 | Loss: 0.00001582
Iteration 126/1000 | Loss: 0.00001582
Iteration 127/1000 | Loss: 0.00001582
Iteration 128/1000 | Loss: 0.00001582
Iteration 129/1000 | Loss: 0.00001582
Iteration 130/1000 | Loss: 0.00001582
Iteration 131/1000 | Loss: 0.00001581
Iteration 132/1000 | Loss: 0.00001581
Iteration 133/1000 | Loss: 0.00001581
Iteration 134/1000 | Loss: 0.00001581
Iteration 135/1000 | Loss: 0.00001581
Iteration 136/1000 | Loss: 0.00001581
Iteration 137/1000 | Loss: 0.00001580
Iteration 138/1000 | Loss: 0.00001580
Iteration 139/1000 | Loss: 0.00001580
Iteration 140/1000 | Loss: 0.00001580
Iteration 141/1000 | Loss: 0.00001579
Iteration 142/1000 | Loss: 0.00001579
Iteration 143/1000 | Loss: 0.00001579
Iteration 144/1000 | Loss: 0.00001579
Iteration 145/1000 | Loss: 0.00001579
Iteration 146/1000 | Loss: 0.00001578
Iteration 147/1000 | Loss: 0.00001578
Iteration 148/1000 | Loss: 0.00001578
Iteration 149/1000 | Loss: 0.00001578
Iteration 150/1000 | Loss: 0.00001578
Iteration 151/1000 | Loss: 0.00001578
Iteration 152/1000 | Loss: 0.00001577
Iteration 153/1000 | Loss: 0.00001577
Iteration 154/1000 | Loss: 0.00001577
Iteration 155/1000 | Loss: 0.00001577
Iteration 156/1000 | Loss: 0.00001577
Iteration 157/1000 | Loss: 0.00001576
Iteration 158/1000 | Loss: 0.00001576
Iteration 159/1000 | Loss: 0.00001576
Iteration 160/1000 | Loss: 0.00001576
Iteration 161/1000 | Loss: 0.00001576
Iteration 162/1000 | Loss: 0.00001576
Iteration 163/1000 | Loss: 0.00001575
Iteration 164/1000 | Loss: 0.00001575
Iteration 165/1000 | Loss: 0.00001575
Iteration 166/1000 | Loss: 0.00001575
Iteration 167/1000 | Loss: 0.00001575
Iteration 168/1000 | Loss: 0.00001575
Iteration 169/1000 | Loss: 0.00001575
Iteration 170/1000 | Loss: 0.00001574
Iteration 171/1000 | Loss: 0.00001574
Iteration 172/1000 | Loss: 0.00001574
Iteration 173/1000 | Loss: 0.00001574
Iteration 174/1000 | Loss: 0.00001574
Iteration 175/1000 | Loss: 0.00001574
Iteration 176/1000 | Loss: 0.00001574
Iteration 177/1000 | Loss: 0.00001574
Iteration 178/1000 | Loss: 0.00001574
Iteration 179/1000 | Loss: 0.00001574
Iteration 180/1000 | Loss: 0.00001573
Iteration 181/1000 | Loss: 0.00001573
Iteration 182/1000 | Loss: 0.00001573
Iteration 183/1000 | Loss: 0.00001573
Iteration 184/1000 | Loss: 0.00001573
Iteration 185/1000 | Loss: 0.00001573
Iteration 186/1000 | Loss: 0.00001573
Iteration 187/1000 | Loss: 0.00001573
Iteration 188/1000 | Loss: 0.00001573
Iteration 189/1000 | Loss: 0.00001573
Iteration 190/1000 | Loss: 0.00001573
Iteration 191/1000 | Loss: 0.00001573
Iteration 192/1000 | Loss: 0.00001573
Iteration 193/1000 | Loss: 0.00001573
Iteration 194/1000 | Loss: 0.00001573
Iteration 195/1000 | Loss: 0.00001573
Iteration 196/1000 | Loss: 0.00001573
Iteration 197/1000 | Loss: 0.00001573
Iteration 198/1000 | Loss: 0.00001573
Iteration 199/1000 | Loss: 0.00001573
Iteration 200/1000 | Loss: 0.00001573
Iteration 201/1000 | Loss: 0.00001573
Iteration 202/1000 | Loss: 0.00001572
Iteration 203/1000 | Loss: 0.00001572
Iteration 204/1000 | Loss: 0.00001572
Iteration 205/1000 | Loss: 0.00001572
Iteration 206/1000 | Loss: 0.00001572
Iteration 207/1000 | Loss: 0.00001572
Iteration 208/1000 | Loss: 0.00001572
Iteration 209/1000 | Loss: 0.00001572
Iteration 210/1000 | Loss: 0.00001572
Iteration 211/1000 | Loss: 0.00001572
Iteration 212/1000 | Loss: 0.00001572
Iteration 213/1000 | Loss: 0.00001572
Iteration 214/1000 | Loss: 0.00001572
Iteration 215/1000 | Loss: 0.00001571
Iteration 216/1000 | Loss: 0.00001571
Iteration 217/1000 | Loss: 0.00001571
Iteration 218/1000 | Loss: 0.00001571
Iteration 219/1000 | Loss: 0.00001571
Iteration 220/1000 | Loss: 0.00001571
Iteration 221/1000 | Loss: 0.00001571
Iteration 222/1000 | Loss: 0.00001571
Iteration 223/1000 | Loss: 0.00001571
Iteration 224/1000 | Loss: 0.00001571
Iteration 225/1000 | Loss: 0.00001571
Iteration 226/1000 | Loss: 0.00001571
Iteration 227/1000 | Loss: 0.00001571
Iteration 228/1000 | Loss: 0.00001571
Iteration 229/1000 | Loss: 0.00001571
Iteration 230/1000 | Loss: 0.00001571
Iteration 231/1000 | Loss: 0.00001571
Iteration 232/1000 | Loss: 0.00001571
Iteration 233/1000 | Loss: 0.00001571
Iteration 234/1000 | Loss: 0.00001570
Iteration 235/1000 | Loss: 0.00001570
Iteration 236/1000 | Loss: 0.00001570
Iteration 237/1000 | Loss: 0.00001570
Iteration 238/1000 | Loss: 0.00001570
Iteration 239/1000 | Loss: 0.00001570
Iteration 240/1000 | Loss: 0.00001570
Iteration 241/1000 | Loss: 0.00001570
Iteration 242/1000 | Loss: 0.00001570
Iteration 243/1000 | Loss: 0.00001570
Iteration 244/1000 | Loss: 0.00001570
Iteration 245/1000 | Loss: 0.00001570
Iteration 246/1000 | Loss: 0.00001570
Iteration 247/1000 | Loss: 0.00001570
Iteration 248/1000 | Loss: 0.00001570
Iteration 249/1000 | Loss: 0.00001570
Iteration 250/1000 | Loss: 0.00001570
Iteration 251/1000 | Loss: 0.00001569
Iteration 252/1000 | Loss: 0.00001569
Iteration 253/1000 | Loss: 0.00001569
Iteration 254/1000 | Loss: 0.00001569
Iteration 255/1000 | Loss: 0.00001569
Iteration 256/1000 | Loss: 0.00001569
Iteration 257/1000 | Loss: 0.00001569
Iteration 258/1000 | Loss: 0.00001569
Iteration 259/1000 | Loss: 0.00001569
Iteration 260/1000 | Loss: 0.00001569
Iteration 261/1000 | Loss: 0.00001569
Iteration 262/1000 | Loss: 0.00001569
Iteration 263/1000 | Loss: 0.00001569
Iteration 264/1000 | Loss: 0.00001569
Iteration 265/1000 | Loss: 0.00001569
Iteration 266/1000 | Loss: 0.00001569
Iteration 267/1000 | Loss: 0.00001569
Iteration 268/1000 | Loss: 0.00001569
Iteration 269/1000 | Loss: 0.00001569
Iteration 270/1000 | Loss: 0.00001569
Iteration 271/1000 | Loss: 0.00001569
Iteration 272/1000 | Loss: 0.00001569
Iteration 273/1000 | Loss: 0.00001569
Iteration 274/1000 | Loss: 0.00001569
Iteration 275/1000 | Loss: 0.00001569
Iteration 276/1000 | Loss: 0.00001569
Iteration 277/1000 | Loss: 0.00001569
Iteration 278/1000 | Loss: 0.00001569
Iteration 279/1000 | Loss: 0.00001569
Iteration 280/1000 | Loss: 0.00001569
Iteration 281/1000 | Loss: 0.00001569
Iteration 282/1000 | Loss: 0.00001569
Iteration 283/1000 | Loss: 0.00001569
Iteration 284/1000 | Loss: 0.00001569
Iteration 285/1000 | Loss: 0.00001569
Iteration 286/1000 | Loss: 0.00001569
Iteration 287/1000 | Loss: 0.00001569
Iteration 288/1000 | Loss: 0.00001569
Iteration 289/1000 | Loss: 0.00001569
Iteration 290/1000 | Loss: 0.00001569
Iteration 291/1000 | Loss: 0.00001569
Iteration 292/1000 | Loss: 0.00001569
Iteration 293/1000 | Loss: 0.00001569
Iteration 294/1000 | Loss: 0.00001569
Iteration 295/1000 | Loss: 0.00001569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 295. Stopping optimization.
Last 5 losses: [1.5688672647229396e-05, 1.5688672647229396e-05, 1.5688672647229396e-05, 1.5688672647229396e-05, 1.5688672647229396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5688672647229396e-05

Optimization complete. Final v2v error: 3.1190478801727295 mm

Highest mean error: 5.069558620452881 mm for frame 59

Lowest mean error: 2.4646477699279785 mm for frame 87

Saving results

Total time: 46.60829019546509
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442009
Iteration 2/25 | Loss: 0.00129405
Iteration 3/25 | Loss: 0.00120888
Iteration 4/25 | Loss: 0.00119693
Iteration 5/25 | Loss: 0.00119376
Iteration 6/25 | Loss: 0.00119323
Iteration 7/25 | Loss: 0.00119323
Iteration 8/25 | Loss: 0.00119323
Iteration 9/25 | Loss: 0.00119323
Iteration 10/25 | Loss: 0.00119323
Iteration 11/25 | Loss: 0.00119323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001193233416415751, 0.001193233416415751, 0.001193233416415751, 0.001193233416415751, 0.001193233416415751]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001193233416415751

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.50751257
Iteration 2/25 | Loss: 0.00132822
Iteration 3/25 | Loss: 0.00132821
Iteration 4/25 | Loss: 0.00132820
Iteration 5/25 | Loss: 0.00132820
Iteration 6/25 | Loss: 0.00132820
Iteration 7/25 | Loss: 0.00132820
Iteration 8/25 | Loss: 0.00132820
Iteration 9/25 | Loss: 0.00132820
Iteration 10/25 | Loss: 0.00132820
Iteration 11/25 | Loss: 0.00132820
Iteration 12/25 | Loss: 0.00132820
Iteration 13/25 | Loss: 0.00132820
Iteration 14/25 | Loss: 0.00132820
Iteration 15/25 | Loss: 0.00132820
Iteration 16/25 | Loss: 0.00132820
Iteration 17/25 | Loss: 0.00132820
Iteration 18/25 | Loss: 0.00132820
Iteration 19/25 | Loss: 0.00132820
Iteration 20/25 | Loss: 0.00132820
Iteration 21/25 | Loss: 0.00132820
Iteration 22/25 | Loss: 0.00132820
Iteration 23/25 | Loss: 0.00132820
Iteration 24/25 | Loss: 0.00132820
Iteration 25/25 | Loss: 0.00132820

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132820
Iteration 2/1000 | Loss: 0.00002288
Iteration 3/1000 | Loss: 0.00001676
Iteration 4/1000 | Loss: 0.00001472
Iteration 5/1000 | Loss: 0.00001392
Iteration 6/1000 | Loss: 0.00001339
Iteration 7/1000 | Loss: 0.00001291
Iteration 8/1000 | Loss: 0.00001266
Iteration 9/1000 | Loss: 0.00001238
Iteration 10/1000 | Loss: 0.00001211
Iteration 11/1000 | Loss: 0.00001207
Iteration 12/1000 | Loss: 0.00001186
Iteration 13/1000 | Loss: 0.00001180
Iteration 14/1000 | Loss: 0.00001173
Iteration 15/1000 | Loss: 0.00001172
Iteration 16/1000 | Loss: 0.00001171
Iteration 17/1000 | Loss: 0.00001169
Iteration 18/1000 | Loss: 0.00001168
Iteration 19/1000 | Loss: 0.00001166
Iteration 20/1000 | Loss: 0.00001164
Iteration 21/1000 | Loss: 0.00001163
Iteration 22/1000 | Loss: 0.00001162
Iteration 23/1000 | Loss: 0.00001162
Iteration 24/1000 | Loss: 0.00001155
Iteration 25/1000 | Loss: 0.00001154
Iteration 26/1000 | Loss: 0.00001152
Iteration 27/1000 | Loss: 0.00001151
Iteration 28/1000 | Loss: 0.00001150
Iteration 29/1000 | Loss: 0.00001150
Iteration 30/1000 | Loss: 0.00001150
Iteration 31/1000 | Loss: 0.00001147
Iteration 32/1000 | Loss: 0.00001147
Iteration 33/1000 | Loss: 0.00001147
Iteration 34/1000 | Loss: 0.00001146
Iteration 35/1000 | Loss: 0.00001146
Iteration 36/1000 | Loss: 0.00001145
Iteration 37/1000 | Loss: 0.00001143
Iteration 38/1000 | Loss: 0.00001142
Iteration 39/1000 | Loss: 0.00001141
Iteration 40/1000 | Loss: 0.00001141
Iteration 41/1000 | Loss: 0.00001140
Iteration 42/1000 | Loss: 0.00001140
Iteration 43/1000 | Loss: 0.00001139
Iteration 44/1000 | Loss: 0.00001139
Iteration 45/1000 | Loss: 0.00001138
Iteration 46/1000 | Loss: 0.00001138
Iteration 47/1000 | Loss: 0.00001138
Iteration 48/1000 | Loss: 0.00001137
Iteration 49/1000 | Loss: 0.00001137
Iteration 50/1000 | Loss: 0.00001137
Iteration 51/1000 | Loss: 0.00001137
Iteration 52/1000 | Loss: 0.00001136
Iteration 53/1000 | Loss: 0.00001136
Iteration 54/1000 | Loss: 0.00001136
Iteration 55/1000 | Loss: 0.00001136
Iteration 56/1000 | Loss: 0.00001136
Iteration 57/1000 | Loss: 0.00001136
Iteration 58/1000 | Loss: 0.00001135
Iteration 59/1000 | Loss: 0.00001135
Iteration 60/1000 | Loss: 0.00001135
Iteration 61/1000 | Loss: 0.00001135
Iteration 62/1000 | Loss: 0.00001135
Iteration 63/1000 | Loss: 0.00001135
Iteration 64/1000 | Loss: 0.00001135
Iteration 65/1000 | Loss: 0.00001134
Iteration 66/1000 | Loss: 0.00001134
Iteration 67/1000 | Loss: 0.00001134
Iteration 68/1000 | Loss: 0.00001134
Iteration 69/1000 | Loss: 0.00001134
Iteration 70/1000 | Loss: 0.00001133
Iteration 71/1000 | Loss: 0.00001132
Iteration 72/1000 | Loss: 0.00001132
Iteration 73/1000 | Loss: 0.00001131
Iteration 74/1000 | Loss: 0.00001131
Iteration 75/1000 | Loss: 0.00001131
Iteration 76/1000 | Loss: 0.00001131
Iteration 77/1000 | Loss: 0.00001131
Iteration 78/1000 | Loss: 0.00001131
Iteration 79/1000 | Loss: 0.00001130
Iteration 80/1000 | Loss: 0.00001130
Iteration 81/1000 | Loss: 0.00001130
Iteration 82/1000 | Loss: 0.00001129
Iteration 83/1000 | Loss: 0.00001129
Iteration 84/1000 | Loss: 0.00001128
Iteration 85/1000 | Loss: 0.00001128
Iteration 86/1000 | Loss: 0.00001128
Iteration 87/1000 | Loss: 0.00001128
Iteration 88/1000 | Loss: 0.00001127
Iteration 89/1000 | Loss: 0.00001127
Iteration 90/1000 | Loss: 0.00001127
Iteration 91/1000 | Loss: 0.00001127
Iteration 92/1000 | Loss: 0.00001126
Iteration 93/1000 | Loss: 0.00001126
Iteration 94/1000 | Loss: 0.00001126
Iteration 95/1000 | Loss: 0.00001125
Iteration 96/1000 | Loss: 0.00001125
Iteration 97/1000 | Loss: 0.00001125
Iteration 98/1000 | Loss: 0.00001125
Iteration 99/1000 | Loss: 0.00001124
Iteration 100/1000 | Loss: 0.00001124
Iteration 101/1000 | Loss: 0.00001124
Iteration 102/1000 | Loss: 0.00001124
Iteration 103/1000 | Loss: 0.00001123
Iteration 104/1000 | Loss: 0.00001123
Iteration 105/1000 | Loss: 0.00001123
Iteration 106/1000 | Loss: 0.00001123
Iteration 107/1000 | Loss: 0.00001122
Iteration 108/1000 | Loss: 0.00001122
Iteration 109/1000 | Loss: 0.00001122
Iteration 110/1000 | Loss: 0.00001122
Iteration 111/1000 | Loss: 0.00001122
Iteration 112/1000 | Loss: 0.00001122
Iteration 113/1000 | Loss: 0.00001121
Iteration 114/1000 | Loss: 0.00001121
Iteration 115/1000 | Loss: 0.00001121
Iteration 116/1000 | Loss: 0.00001121
Iteration 117/1000 | Loss: 0.00001121
Iteration 118/1000 | Loss: 0.00001121
Iteration 119/1000 | Loss: 0.00001121
Iteration 120/1000 | Loss: 0.00001121
Iteration 121/1000 | Loss: 0.00001121
Iteration 122/1000 | Loss: 0.00001121
Iteration 123/1000 | Loss: 0.00001121
Iteration 124/1000 | Loss: 0.00001121
Iteration 125/1000 | Loss: 0.00001121
Iteration 126/1000 | Loss: 0.00001120
Iteration 127/1000 | Loss: 0.00001120
Iteration 128/1000 | Loss: 0.00001120
Iteration 129/1000 | Loss: 0.00001120
Iteration 130/1000 | Loss: 0.00001120
Iteration 131/1000 | Loss: 0.00001120
Iteration 132/1000 | Loss: 0.00001120
Iteration 133/1000 | Loss: 0.00001120
Iteration 134/1000 | Loss: 0.00001120
Iteration 135/1000 | Loss: 0.00001120
Iteration 136/1000 | Loss: 0.00001120
Iteration 137/1000 | Loss: 0.00001120
Iteration 138/1000 | Loss: 0.00001120
Iteration 139/1000 | Loss: 0.00001120
Iteration 140/1000 | Loss: 0.00001120
Iteration 141/1000 | Loss: 0.00001120
Iteration 142/1000 | Loss: 0.00001120
Iteration 143/1000 | Loss: 0.00001120
Iteration 144/1000 | Loss: 0.00001120
Iteration 145/1000 | Loss: 0.00001120
Iteration 146/1000 | Loss: 0.00001120
Iteration 147/1000 | Loss: 0.00001120
Iteration 148/1000 | Loss: 0.00001120
Iteration 149/1000 | Loss: 0.00001120
Iteration 150/1000 | Loss: 0.00001120
Iteration 151/1000 | Loss: 0.00001120
Iteration 152/1000 | Loss: 0.00001120
Iteration 153/1000 | Loss: 0.00001120
Iteration 154/1000 | Loss: 0.00001120
Iteration 155/1000 | Loss: 0.00001120
Iteration 156/1000 | Loss: 0.00001120
Iteration 157/1000 | Loss: 0.00001120
Iteration 158/1000 | Loss: 0.00001120
Iteration 159/1000 | Loss: 0.00001120
Iteration 160/1000 | Loss: 0.00001120
Iteration 161/1000 | Loss: 0.00001120
Iteration 162/1000 | Loss: 0.00001120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.1199766959180124e-05, 1.1199766959180124e-05, 1.1199766959180124e-05, 1.1199766959180124e-05, 1.1199766959180124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1199766959180124e-05

Optimization complete. Final v2v error: 2.852126359939575 mm

Highest mean error: 3.5286355018615723 mm for frame 97

Lowest mean error: 2.577256202697754 mm for frame 154

Saving results

Total time: 38.337727069854736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794807
Iteration 2/25 | Loss: 0.00131854
Iteration 3/25 | Loss: 0.00122012
Iteration 4/25 | Loss: 0.00120786
Iteration 5/25 | Loss: 0.00120481
Iteration 6/25 | Loss: 0.00120430
Iteration 7/25 | Loss: 0.00120430
Iteration 8/25 | Loss: 0.00120430
Iteration 9/25 | Loss: 0.00120430
Iteration 10/25 | Loss: 0.00120430
Iteration 11/25 | Loss: 0.00120430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012042978778481483, 0.0012042978778481483, 0.0012042978778481483, 0.0012042978778481483, 0.0012042978778481483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012042978778481483

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37318563
Iteration 2/25 | Loss: 0.00132872
Iteration 3/25 | Loss: 0.00132871
Iteration 4/25 | Loss: 0.00132871
Iteration 5/25 | Loss: 0.00132871
Iteration 6/25 | Loss: 0.00132871
Iteration 7/25 | Loss: 0.00132871
Iteration 8/25 | Loss: 0.00132871
Iteration 9/25 | Loss: 0.00132871
Iteration 10/25 | Loss: 0.00132871
Iteration 11/25 | Loss: 0.00132871
Iteration 12/25 | Loss: 0.00132871
Iteration 13/25 | Loss: 0.00132871
Iteration 14/25 | Loss: 0.00132871
Iteration 15/25 | Loss: 0.00132871
Iteration 16/25 | Loss: 0.00132871
Iteration 17/25 | Loss: 0.00132871
Iteration 18/25 | Loss: 0.00132871
Iteration 19/25 | Loss: 0.00132871
Iteration 20/25 | Loss: 0.00132871
Iteration 21/25 | Loss: 0.00132871
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001328709302470088, 0.001328709302470088, 0.001328709302470088, 0.001328709302470088, 0.001328709302470088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001328709302470088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132871
Iteration 2/1000 | Loss: 0.00003101
Iteration 3/1000 | Loss: 0.00002110
Iteration 4/1000 | Loss: 0.00001832
Iteration 5/1000 | Loss: 0.00001692
Iteration 6/1000 | Loss: 0.00001618
Iteration 7/1000 | Loss: 0.00001559
Iteration 8/1000 | Loss: 0.00001521
Iteration 9/1000 | Loss: 0.00001494
Iteration 10/1000 | Loss: 0.00001464
Iteration 11/1000 | Loss: 0.00001440
Iteration 12/1000 | Loss: 0.00001422
Iteration 13/1000 | Loss: 0.00001421
Iteration 14/1000 | Loss: 0.00001414
Iteration 15/1000 | Loss: 0.00001413
Iteration 16/1000 | Loss: 0.00001411
Iteration 17/1000 | Loss: 0.00001410
Iteration 18/1000 | Loss: 0.00001409
Iteration 19/1000 | Loss: 0.00001408
Iteration 20/1000 | Loss: 0.00001407
Iteration 21/1000 | Loss: 0.00001407
Iteration 22/1000 | Loss: 0.00001406
Iteration 23/1000 | Loss: 0.00001405
Iteration 24/1000 | Loss: 0.00001401
Iteration 25/1000 | Loss: 0.00001393
Iteration 26/1000 | Loss: 0.00001391
Iteration 27/1000 | Loss: 0.00001390
Iteration 28/1000 | Loss: 0.00001389
Iteration 29/1000 | Loss: 0.00001389
Iteration 30/1000 | Loss: 0.00001388
Iteration 31/1000 | Loss: 0.00001387
Iteration 32/1000 | Loss: 0.00001386
Iteration 33/1000 | Loss: 0.00001385
Iteration 34/1000 | Loss: 0.00001384
Iteration 35/1000 | Loss: 0.00001384
Iteration 36/1000 | Loss: 0.00001384
Iteration 37/1000 | Loss: 0.00001383
Iteration 38/1000 | Loss: 0.00001382
Iteration 39/1000 | Loss: 0.00001382
Iteration 40/1000 | Loss: 0.00001382
Iteration 41/1000 | Loss: 0.00001378
Iteration 42/1000 | Loss: 0.00001378
Iteration 43/1000 | Loss: 0.00001377
Iteration 44/1000 | Loss: 0.00001377
Iteration 45/1000 | Loss: 0.00001376
Iteration 46/1000 | Loss: 0.00001375
Iteration 47/1000 | Loss: 0.00001375
Iteration 48/1000 | Loss: 0.00001374
Iteration 49/1000 | Loss: 0.00001374
Iteration 50/1000 | Loss: 0.00001373
Iteration 51/1000 | Loss: 0.00001373
Iteration 52/1000 | Loss: 0.00001372
Iteration 53/1000 | Loss: 0.00001372
Iteration 54/1000 | Loss: 0.00001371
Iteration 55/1000 | Loss: 0.00001371
Iteration 56/1000 | Loss: 0.00001371
Iteration 57/1000 | Loss: 0.00001371
Iteration 58/1000 | Loss: 0.00001371
Iteration 59/1000 | Loss: 0.00001370
Iteration 60/1000 | Loss: 0.00001370
Iteration 61/1000 | Loss: 0.00001369
Iteration 62/1000 | Loss: 0.00001369
Iteration 63/1000 | Loss: 0.00001369
Iteration 64/1000 | Loss: 0.00001369
Iteration 65/1000 | Loss: 0.00001369
Iteration 66/1000 | Loss: 0.00001369
Iteration 67/1000 | Loss: 0.00001369
Iteration 68/1000 | Loss: 0.00001369
Iteration 69/1000 | Loss: 0.00001369
Iteration 70/1000 | Loss: 0.00001368
Iteration 71/1000 | Loss: 0.00001368
Iteration 72/1000 | Loss: 0.00001368
Iteration 73/1000 | Loss: 0.00001368
Iteration 74/1000 | Loss: 0.00001367
Iteration 75/1000 | Loss: 0.00001367
Iteration 76/1000 | Loss: 0.00001367
Iteration 77/1000 | Loss: 0.00001366
Iteration 78/1000 | Loss: 0.00001366
Iteration 79/1000 | Loss: 0.00001366
Iteration 80/1000 | Loss: 0.00001365
Iteration 81/1000 | Loss: 0.00001365
Iteration 82/1000 | Loss: 0.00001365
Iteration 83/1000 | Loss: 0.00001365
Iteration 84/1000 | Loss: 0.00001365
Iteration 85/1000 | Loss: 0.00001365
Iteration 86/1000 | Loss: 0.00001365
Iteration 87/1000 | Loss: 0.00001365
Iteration 88/1000 | Loss: 0.00001365
Iteration 89/1000 | Loss: 0.00001364
Iteration 90/1000 | Loss: 0.00001364
Iteration 91/1000 | Loss: 0.00001363
Iteration 92/1000 | Loss: 0.00001363
Iteration 93/1000 | Loss: 0.00001362
Iteration 94/1000 | Loss: 0.00001362
Iteration 95/1000 | Loss: 0.00001362
Iteration 96/1000 | Loss: 0.00001362
Iteration 97/1000 | Loss: 0.00001362
Iteration 98/1000 | Loss: 0.00001361
Iteration 99/1000 | Loss: 0.00001361
Iteration 100/1000 | Loss: 0.00001361
Iteration 101/1000 | Loss: 0.00001361
Iteration 102/1000 | Loss: 0.00001361
Iteration 103/1000 | Loss: 0.00001361
Iteration 104/1000 | Loss: 0.00001361
Iteration 105/1000 | Loss: 0.00001361
Iteration 106/1000 | Loss: 0.00001360
Iteration 107/1000 | Loss: 0.00001360
Iteration 108/1000 | Loss: 0.00001360
Iteration 109/1000 | Loss: 0.00001359
Iteration 110/1000 | Loss: 0.00001359
Iteration 111/1000 | Loss: 0.00001358
Iteration 112/1000 | Loss: 0.00001358
Iteration 113/1000 | Loss: 0.00001358
Iteration 114/1000 | Loss: 0.00001357
Iteration 115/1000 | Loss: 0.00001357
Iteration 116/1000 | Loss: 0.00001357
Iteration 117/1000 | Loss: 0.00001357
Iteration 118/1000 | Loss: 0.00001356
Iteration 119/1000 | Loss: 0.00001356
Iteration 120/1000 | Loss: 0.00001356
Iteration 121/1000 | Loss: 0.00001355
Iteration 122/1000 | Loss: 0.00001355
Iteration 123/1000 | Loss: 0.00001355
Iteration 124/1000 | Loss: 0.00001354
Iteration 125/1000 | Loss: 0.00001354
Iteration 126/1000 | Loss: 0.00001354
Iteration 127/1000 | Loss: 0.00001353
Iteration 128/1000 | Loss: 0.00001353
Iteration 129/1000 | Loss: 0.00001353
Iteration 130/1000 | Loss: 0.00001352
Iteration 131/1000 | Loss: 0.00001352
Iteration 132/1000 | Loss: 0.00001352
Iteration 133/1000 | Loss: 0.00001351
Iteration 134/1000 | Loss: 0.00001351
Iteration 135/1000 | Loss: 0.00001351
Iteration 136/1000 | Loss: 0.00001350
Iteration 137/1000 | Loss: 0.00001350
Iteration 138/1000 | Loss: 0.00001350
Iteration 139/1000 | Loss: 0.00001350
Iteration 140/1000 | Loss: 0.00001350
Iteration 141/1000 | Loss: 0.00001349
Iteration 142/1000 | Loss: 0.00001349
Iteration 143/1000 | Loss: 0.00001349
Iteration 144/1000 | Loss: 0.00001348
Iteration 145/1000 | Loss: 0.00001348
Iteration 146/1000 | Loss: 0.00001347
Iteration 147/1000 | Loss: 0.00001347
Iteration 148/1000 | Loss: 0.00001345
Iteration 149/1000 | Loss: 0.00001345
Iteration 150/1000 | Loss: 0.00001345
Iteration 151/1000 | Loss: 0.00001345
Iteration 152/1000 | Loss: 0.00001345
Iteration 153/1000 | Loss: 0.00001345
Iteration 154/1000 | Loss: 0.00001345
Iteration 155/1000 | Loss: 0.00001345
Iteration 156/1000 | Loss: 0.00001345
Iteration 157/1000 | Loss: 0.00001344
Iteration 158/1000 | Loss: 0.00001344
Iteration 159/1000 | Loss: 0.00001344
Iteration 160/1000 | Loss: 0.00001344
Iteration 161/1000 | Loss: 0.00001343
Iteration 162/1000 | Loss: 0.00001343
Iteration 163/1000 | Loss: 0.00001343
Iteration 164/1000 | Loss: 0.00001342
Iteration 165/1000 | Loss: 0.00001342
Iteration 166/1000 | Loss: 0.00001342
Iteration 167/1000 | Loss: 0.00001342
Iteration 168/1000 | Loss: 0.00001342
Iteration 169/1000 | Loss: 0.00001342
Iteration 170/1000 | Loss: 0.00001342
Iteration 171/1000 | Loss: 0.00001342
Iteration 172/1000 | Loss: 0.00001342
Iteration 173/1000 | Loss: 0.00001342
Iteration 174/1000 | Loss: 0.00001341
Iteration 175/1000 | Loss: 0.00001341
Iteration 176/1000 | Loss: 0.00001341
Iteration 177/1000 | Loss: 0.00001341
Iteration 178/1000 | Loss: 0.00001341
Iteration 179/1000 | Loss: 0.00001341
Iteration 180/1000 | Loss: 0.00001340
Iteration 181/1000 | Loss: 0.00001340
Iteration 182/1000 | Loss: 0.00001340
Iteration 183/1000 | Loss: 0.00001340
Iteration 184/1000 | Loss: 0.00001340
Iteration 185/1000 | Loss: 0.00001340
Iteration 186/1000 | Loss: 0.00001340
Iteration 187/1000 | Loss: 0.00001339
Iteration 188/1000 | Loss: 0.00001339
Iteration 189/1000 | Loss: 0.00001339
Iteration 190/1000 | Loss: 0.00001339
Iteration 191/1000 | Loss: 0.00001339
Iteration 192/1000 | Loss: 0.00001339
Iteration 193/1000 | Loss: 0.00001339
Iteration 194/1000 | Loss: 0.00001339
Iteration 195/1000 | Loss: 0.00001339
Iteration 196/1000 | Loss: 0.00001339
Iteration 197/1000 | Loss: 0.00001339
Iteration 198/1000 | Loss: 0.00001339
Iteration 199/1000 | Loss: 0.00001339
Iteration 200/1000 | Loss: 0.00001339
Iteration 201/1000 | Loss: 0.00001339
Iteration 202/1000 | Loss: 0.00001339
Iteration 203/1000 | Loss: 0.00001338
Iteration 204/1000 | Loss: 0.00001338
Iteration 205/1000 | Loss: 0.00001338
Iteration 206/1000 | Loss: 0.00001338
Iteration 207/1000 | Loss: 0.00001338
Iteration 208/1000 | Loss: 0.00001338
Iteration 209/1000 | Loss: 0.00001338
Iteration 210/1000 | Loss: 0.00001338
Iteration 211/1000 | Loss: 0.00001338
Iteration 212/1000 | Loss: 0.00001338
Iteration 213/1000 | Loss: 0.00001338
Iteration 214/1000 | Loss: 0.00001338
Iteration 215/1000 | Loss: 0.00001338
Iteration 216/1000 | Loss: 0.00001338
Iteration 217/1000 | Loss: 0.00001338
Iteration 218/1000 | Loss: 0.00001337
Iteration 219/1000 | Loss: 0.00001337
Iteration 220/1000 | Loss: 0.00001337
Iteration 221/1000 | Loss: 0.00001337
Iteration 222/1000 | Loss: 0.00001337
Iteration 223/1000 | Loss: 0.00001337
Iteration 224/1000 | Loss: 0.00001337
Iteration 225/1000 | Loss: 0.00001337
Iteration 226/1000 | Loss: 0.00001337
Iteration 227/1000 | Loss: 0.00001337
Iteration 228/1000 | Loss: 0.00001337
Iteration 229/1000 | Loss: 0.00001337
Iteration 230/1000 | Loss: 0.00001337
Iteration 231/1000 | Loss: 0.00001337
Iteration 232/1000 | Loss: 0.00001337
Iteration 233/1000 | Loss: 0.00001337
Iteration 234/1000 | Loss: 0.00001337
Iteration 235/1000 | Loss: 0.00001337
Iteration 236/1000 | Loss: 0.00001337
Iteration 237/1000 | Loss: 0.00001337
Iteration 238/1000 | Loss: 0.00001336
Iteration 239/1000 | Loss: 0.00001336
Iteration 240/1000 | Loss: 0.00001336
Iteration 241/1000 | Loss: 0.00001336
Iteration 242/1000 | Loss: 0.00001336
Iteration 243/1000 | Loss: 0.00001336
Iteration 244/1000 | Loss: 0.00001336
Iteration 245/1000 | Loss: 0.00001336
Iteration 246/1000 | Loss: 0.00001336
Iteration 247/1000 | Loss: 0.00001336
Iteration 248/1000 | Loss: 0.00001336
Iteration 249/1000 | Loss: 0.00001336
Iteration 250/1000 | Loss: 0.00001336
Iteration 251/1000 | Loss: 0.00001336
Iteration 252/1000 | Loss: 0.00001336
Iteration 253/1000 | Loss: 0.00001336
Iteration 254/1000 | Loss: 0.00001336
Iteration 255/1000 | Loss: 0.00001336
Iteration 256/1000 | Loss: 0.00001336
Iteration 257/1000 | Loss: 0.00001336
Iteration 258/1000 | Loss: 0.00001336
Iteration 259/1000 | Loss: 0.00001336
Iteration 260/1000 | Loss: 0.00001336
Iteration 261/1000 | Loss: 0.00001336
Iteration 262/1000 | Loss: 0.00001336
Iteration 263/1000 | Loss: 0.00001336
Iteration 264/1000 | Loss: 0.00001336
Iteration 265/1000 | Loss: 0.00001336
Iteration 266/1000 | Loss: 0.00001336
Iteration 267/1000 | Loss: 0.00001336
Iteration 268/1000 | Loss: 0.00001336
Iteration 269/1000 | Loss: 0.00001336
Iteration 270/1000 | Loss: 0.00001336
Iteration 271/1000 | Loss: 0.00001336
Iteration 272/1000 | Loss: 0.00001336
Iteration 273/1000 | Loss: 0.00001336
Iteration 274/1000 | Loss: 0.00001336
Iteration 275/1000 | Loss: 0.00001336
Iteration 276/1000 | Loss: 0.00001336
Iteration 277/1000 | Loss: 0.00001336
Iteration 278/1000 | Loss: 0.00001336
Iteration 279/1000 | Loss: 0.00001336
Iteration 280/1000 | Loss: 0.00001336
Iteration 281/1000 | Loss: 0.00001336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [1.3357104762690142e-05, 1.3357104762690142e-05, 1.3357104762690142e-05, 1.3357104762690142e-05, 1.3357104762690142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3357104762690142e-05

Optimization complete. Final v2v error: 3.049745798110962 mm

Highest mean error: 4.195004463195801 mm for frame 102

Lowest mean error: 2.459749460220337 mm for frame 201

Saving results

Total time: 53.26971411705017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050716
Iteration 2/25 | Loss: 0.01050716
Iteration 3/25 | Loss: 0.01050715
Iteration 4/25 | Loss: 0.01050715
Iteration 5/25 | Loss: 0.01050715
Iteration 6/25 | Loss: 0.01050715
Iteration 7/25 | Loss: 0.01050715
Iteration 8/25 | Loss: 0.01050715
Iteration 9/25 | Loss: 0.01050714
Iteration 10/25 | Loss: 0.01050714
Iteration 11/25 | Loss: 0.01050714
Iteration 12/25 | Loss: 0.01050714
Iteration 13/25 | Loss: 0.01050713
Iteration 14/25 | Loss: 0.01050713
Iteration 15/25 | Loss: 0.01050713
Iteration 16/25 | Loss: 0.01050713
Iteration 17/25 | Loss: 0.01050713
Iteration 18/25 | Loss: 0.01050713
Iteration 19/25 | Loss: 0.01050712
Iteration 20/25 | Loss: 0.01050712
Iteration 21/25 | Loss: 0.01050712
Iteration 22/25 | Loss: 0.01050712
Iteration 23/25 | Loss: 0.01050711
Iteration 24/25 | Loss: 0.01050711
Iteration 25/25 | Loss: 0.01050711

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63500381
Iteration 2/25 | Loss: 0.08073003
Iteration 3/25 | Loss: 0.08072008
Iteration 4/25 | Loss: 0.08068097
Iteration 5/25 | Loss: 0.08068097
Iteration 6/25 | Loss: 0.08068096
Iteration 7/25 | Loss: 0.08068095
Iteration 8/25 | Loss: 0.08068094
Iteration 9/25 | Loss: 0.08068094
Iteration 10/25 | Loss: 0.08068094
Iteration 11/25 | Loss: 0.08068093
Iteration 12/25 | Loss: 0.08068093
Iteration 13/25 | Loss: 0.08068091
Iteration 14/25 | Loss: 0.08068091
Iteration 15/25 | Loss: 0.08068091
Iteration 16/25 | Loss: 0.08068091
Iteration 17/25 | Loss: 0.08068091
Iteration 18/25 | Loss: 0.08068091
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.08068091422319412, 0.08068091422319412, 0.08068091422319412, 0.08068091422319412, 0.08068091422319412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08068091422319412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08068091
Iteration 2/1000 | Loss: 0.00058637
Iteration 3/1000 | Loss: 0.00013062
Iteration 4/1000 | Loss: 0.00016647
Iteration 5/1000 | Loss: 0.00019409
Iteration 6/1000 | Loss: 0.00004447
Iteration 7/1000 | Loss: 0.00033241
Iteration 8/1000 | Loss: 0.00011132
Iteration 9/1000 | Loss: 0.00012905
Iteration 10/1000 | Loss: 0.00003667
Iteration 11/1000 | Loss: 0.00015166
Iteration 12/1000 | Loss: 0.00006568
Iteration 13/1000 | Loss: 0.00002056
Iteration 14/1000 | Loss: 0.00002320
Iteration 15/1000 | Loss: 0.00002306
Iteration 16/1000 | Loss: 0.00013957
Iteration 17/1000 | Loss: 0.00020408
Iteration 18/1000 | Loss: 0.00001882
Iteration 19/1000 | Loss: 0.00005409
Iteration 20/1000 | Loss: 0.00015290
Iteration 21/1000 | Loss: 0.00069512
Iteration 22/1000 | Loss: 0.00003640
Iteration 23/1000 | Loss: 0.00003121
Iteration 24/1000 | Loss: 0.00001647
Iteration 25/1000 | Loss: 0.00001580
Iteration 26/1000 | Loss: 0.00006098
Iteration 27/1000 | Loss: 0.00001527
Iteration 28/1000 | Loss: 0.00001469
Iteration 29/1000 | Loss: 0.00001435
Iteration 30/1000 | Loss: 0.00001398
Iteration 31/1000 | Loss: 0.00011225
Iteration 32/1000 | Loss: 0.00002021
Iteration 33/1000 | Loss: 0.00002444
Iteration 34/1000 | Loss: 0.00001331
Iteration 35/1000 | Loss: 0.00004399
Iteration 36/1000 | Loss: 0.00015027
Iteration 37/1000 | Loss: 0.00001320
Iteration 38/1000 | Loss: 0.00002820
Iteration 39/1000 | Loss: 0.00008414
Iteration 40/1000 | Loss: 0.00001254
Iteration 41/1000 | Loss: 0.00001247
Iteration 42/1000 | Loss: 0.00001246
Iteration 43/1000 | Loss: 0.00001242
Iteration 44/1000 | Loss: 0.00001242
Iteration 45/1000 | Loss: 0.00001241
Iteration 46/1000 | Loss: 0.00001241
Iteration 47/1000 | Loss: 0.00001241
Iteration 48/1000 | Loss: 0.00001240
Iteration 49/1000 | Loss: 0.00001240
Iteration 50/1000 | Loss: 0.00001230
Iteration 51/1000 | Loss: 0.00002779
Iteration 52/1000 | Loss: 0.00002235
Iteration 53/1000 | Loss: 0.00009234
Iteration 54/1000 | Loss: 0.00010594
Iteration 55/1000 | Loss: 0.00001220
Iteration 56/1000 | Loss: 0.00001206
Iteration 57/1000 | Loss: 0.00001205
Iteration 58/1000 | Loss: 0.00001204
Iteration 59/1000 | Loss: 0.00001204
Iteration 60/1000 | Loss: 0.00001203
Iteration 61/1000 | Loss: 0.00001469
Iteration 62/1000 | Loss: 0.00001209
Iteration 63/1000 | Loss: 0.00001202
Iteration 64/1000 | Loss: 0.00001202
Iteration 65/1000 | Loss: 0.00001202
Iteration 66/1000 | Loss: 0.00001201
Iteration 67/1000 | Loss: 0.00001201
Iteration 68/1000 | Loss: 0.00001201
Iteration 69/1000 | Loss: 0.00001201
Iteration 70/1000 | Loss: 0.00001201
Iteration 71/1000 | Loss: 0.00001201
Iteration 72/1000 | Loss: 0.00001201
Iteration 73/1000 | Loss: 0.00001201
Iteration 74/1000 | Loss: 0.00001201
Iteration 75/1000 | Loss: 0.00001201
Iteration 76/1000 | Loss: 0.00001201
Iteration 77/1000 | Loss: 0.00001201
Iteration 78/1000 | Loss: 0.00001201
Iteration 79/1000 | Loss: 0.00001201
Iteration 80/1000 | Loss: 0.00001201
Iteration 81/1000 | Loss: 0.00001201
Iteration 82/1000 | Loss: 0.00001201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.2012528713967185e-05, 1.2012528713967185e-05, 1.2012528713967185e-05, 1.2012528713967185e-05, 1.2012528713967185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2012528713967185e-05

Optimization complete. Final v2v error: 2.9625942707061768 mm

Highest mean error: 3.4793155193328857 mm for frame 59

Lowest mean error: 2.5757641792297363 mm for frame 21

Saving results

Total time: 75.6243314743042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408715
Iteration 2/25 | Loss: 0.00124240
Iteration 3/25 | Loss: 0.00118233
Iteration 4/25 | Loss: 0.00117258
Iteration 5/25 | Loss: 0.00116922
Iteration 6/25 | Loss: 0.00116840
Iteration 7/25 | Loss: 0.00116840
Iteration 8/25 | Loss: 0.00116840
Iteration 9/25 | Loss: 0.00116840
Iteration 10/25 | Loss: 0.00116840
Iteration 11/25 | Loss: 0.00116840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011683996999636292, 0.0011683996999636292, 0.0011683996999636292, 0.0011683996999636292, 0.0011683996999636292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011683996999636292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85549545
Iteration 2/25 | Loss: 0.00142505
Iteration 3/25 | Loss: 0.00142505
Iteration 4/25 | Loss: 0.00142505
Iteration 5/25 | Loss: 0.00142505
Iteration 6/25 | Loss: 0.00142505
Iteration 7/25 | Loss: 0.00142505
Iteration 8/25 | Loss: 0.00142505
Iteration 9/25 | Loss: 0.00142505
Iteration 10/25 | Loss: 0.00142505
Iteration 11/25 | Loss: 0.00142505
Iteration 12/25 | Loss: 0.00142505
Iteration 13/25 | Loss: 0.00142505
Iteration 14/25 | Loss: 0.00142505
Iteration 15/25 | Loss: 0.00142505
Iteration 16/25 | Loss: 0.00142505
Iteration 17/25 | Loss: 0.00142505
Iteration 18/25 | Loss: 0.00142505
Iteration 19/25 | Loss: 0.00142505
Iteration 20/25 | Loss: 0.00142505
Iteration 21/25 | Loss: 0.00142505
Iteration 22/25 | Loss: 0.00142505
Iteration 23/25 | Loss: 0.00142505
Iteration 24/25 | Loss: 0.00142505
Iteration 25/25 | Loss: 0.00142505

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142505
Iteration 2/1000 | Loss: 0.00002572
Iteration 3/1000 | Loss: 0.00001845
Iteration 4/1000 | Loss: 0.00001480
Iteration 5/1000 | Loss: 0.00001370
Iteration 6/1000 | Loss: 0.00001290
Iteration 7/1000 | Loss: 0.00001243
Iteration 8/1000 | Loss: 0.00001202
Iteration 9/1000 | Loss: 0.00001157
Iteration 10/1000 | Loss: 0.00001146
Iteration 11/1000 | Loss: 0.00001144
Iteration 12/1000 | Loss: 0.00001121
Iteration 13/1000 | Loss: 0.00001103
Iteration 14/1000 | Loss: 0.00001101
Iteration 15/1000 | Loss: 0.00001100
Iteration 16/1000 | Loss: 0.00001097
Iteration 17/1000 | Loss: 0.00001093
Iteration 18/1000 | Loss: 0.00001078
Iteration 19/1000 | Loss: 0.00001066
Iteration 20/1000 | Loss: 0.00001060
Iteration 21/1000 | Loss: 0.00001060
Iteration 22/1000 | Loss: 0.00001060
Iteration 23/1000 | Loss: 0.00001058
Iteration 24/1000 | Loss: 0.00001057
Iteration 25/1000 | Loss: 0.00001055
Iteration 26/1000 | Loss: 0.00001054
Iteration 27/1000 | Loss: 0.00001054
Iteration 28/1000 | Loss: 0.00001053
Iteration 29/1000 | Loss: 0.00001052
Iteration 30/1000 | Loss: 0.00001052
Iteration 31/1000 | Loss: 0.00001052
Iteration 32/1000 | Loss: 0.00001051
Iteration 33/1000 | Loss: 0.00001051
Iteration 34/1000 | Loss: 0.00001050
Iteration 35/1000 | Loss: 0.00001050
Iteration 36/1000 | Loss: 0.00001049
Iteration 37/1000 | Loss: 0.00001048
Iteration 38/1000 | Loss: 0.00001048
Iteration 39/1000 | Loss: 0.00001047
Iteration 40/1000 | Loss: 0.00001046
Iteration 41/1000 | Loss: 0.00001046
Iteration 42/1000 | Loss: 0.00001046
Iteration 43/1000 | Loss: 0.00001045
Iteration 44/1000 | Loss: 0.00001045
Iteration 45/1000 | Loss: 0.00001044
Iteration 46/1000 | Loss: 0.00001043
Iteration 47/1000 | Loss: 0.00001042
Iteration 48/1000 | Loss: 0.00001042
Iteration 49/1000 | Loss: 0.00001042
Iteration 50/1000 | Loss: 0.00001042
Iteration 51/1000 | Loss: 0.00001041
Iteration 52/1000 | Loss: 0.00001041
Iteration 53/1000 | Loss: 0.00001041
Iteration 54/1000 | Loss: 0.00001041
Iteration 55/1000 | Loss: 0.00001041
Iteration 56/1000 | Loss: 0.00001041
Iteration 57/1000 | Loss: 0.00001040
Iteration 58/1000 | Loss: 0.00001040
Iteration 59/1000 | Loss: 0.00001039
Iteration 60/1000 | Loss: 0.00001039
Iteration 61/1000 | Loss: 0.00001038
Iteration 62/1000 | Loss: 0.00001038
Iteration 63/1000 | Loss: 0.00001038
Iteration 64/1000 | Loss: 0.00001037
Iteration 65/1000 | Loss: 0.00001037
Iteration 66/1000 | Loss: 0.00001037
Iteration 67/1000 | Loss: 0.00001036
Iteration 68/1000 | Loss: 0.00001035
Iteration 69/1000 | Loss: 0.00001035
Iteration 70/1000 | Loss: 0.00001035
Iteration 71/1000 | Loss: 0.00001034
Iteration 72/1000 | Loss: 0.00001033
Iteration 73/1000 | Loss: 0.00001033
Iteration 74/1000 | Loss: 0.00001033
Iteration 75/1000 | Loss: 0.00001033
Iteration 76/1000 | Loss: 0.00001032
Iteration 77/1000 | Loss: 0.00001032
Iteration 78/1000 | Loss: 0.00001032
Iteration 79/1000 | Loss: 0.00001031
Iteration 80/1000 | Loss: 0.00001031
Iteration 81/1000 | Loss: 0.00001030
Iteration 82/1000 | Loss: 0.00001030
Iteration 83/1000 | Loss: 0.00001030
Iteration 84/1000 | Loss: 0.00001030
Iteration 85/1000 | Loss: 0.00001029
Iteration 86/1000 | Loss: 0.00001029
Iteration 87/1000 | Loss: 0.00001028
Iteration 88/1000 | Loss: 0.00001028
Iteration 89/1000 | Loss: 0.00001028
Iteration 90/1000 | Loss: 0.00001028
Iteration 91/1000 | Loss: 0.00001028
Iteration 92/1000 | Loss: 0.00001027
Iteration 93/1000 | Loss: 0.00001027
Iteration 94/1000 | Loss: 0.00001027
Iteration 95/1000 | Loss: 0.00001026
Iteration 96/1000 | Loss: 0.00001026
Iteration 97/1000 | Loss: 0.00001026
Iteration 98/1000 | Loss: 0.00001026
Iteration 99/1000 | Loss: 0.00001026
Iteration 100/1000 | Loss: 0.00001026
Iteration 101/1000 | Loss: 0.00001026
Iteration 102/1000 | Loss: 0.00001025
Iteration 103/1000 | Loss: 0.00001025
Iteration 104/1000 | Loss: 0.00001025
Iteration 105/1000 | Loss: 0.00001025
Iteration 106/1000 | Loss: 0.00001025
Iteration 107/1000 | Loss: 0.00001025
Iteration 108/1000 | Loss: 0.00001024
Iteration 109/1000 | Loss: 0.00001024
Iteration 110/1000 | Loss: 0.00001024
Iteration 111/1000 | Loss: 0.00001024
Iteration 112/1000 | Loss: 0.00001024
Iteration 113/1000 | Loss: 0.00001023
Iteration 114/1000 | Loss: 0.00001023
Iteration 115/1000 | Loss: 0.00001023
Iteration 116/1000 | Loss: 0.00001023
Iteration 117/1000 | Loss: 0.00001023
Iteration 118/1000 | Loss: 0.00001023
Iteration 119/1000 | Loss: 0.00001023
Iteration 120/1000 | Loss: 0.00001023
Iteration 121/1000 | Loss: 0.00001022
Iteration 122/1000 | Loss: 0.00001022
Iteration 123/1000 | Loss: 0.00001022
Iteration 124/1000 | Loss: 0.00001022
Iteration 125/1000 | Loss: 0.00001022
Iteration 126/1000 | Loss: 0.00001021
Iteration 127/1000 | Loss: 0.00001021
Iteration 128/1000 | Loss: 0.00001021
Iteration 129/1000 | Loss: 0.00001021
Iteration 130/1000 | Loss: 0.00001021
Iteration 131/1000 | Loss: 0.00001021
Iteration 132/1000 | Loss: 0.00001021
Iteration 133/1000 | Loss: 0.00001021
Iteration 134/1000 | Loss: 0.00001021
Iteration 135/1000 | Loss: 0.00001021
Iteration 136/1000 | Loss: 0.00001021
Iteration 137/1000 | Loss: 0.00001021
Iteration 138/1000 | Loss: 0.00001021
Iteration 139/1000 | Loss: 0.00001020
Iteration 140/1000 | Loss: 0.00001020
Iteration 141/1000 | Loss: 0.00001020
Iteration 142/1000 | Loss: 0.00001020
Iteration 143/1000 | Loss: 0.00001019
Iteration 144/1000 | Loss: 0.00001019
Iteration 145/1000 | Loss: 0.00001019
Iteration 146/1000 | Loss: 0.00001019
Iteration 147/1000 | Loss: 0.00001019
Iteration 148/1000 | Loss: 0.00001019
Iteration 149/1000 | Loss: 0.00001019
Iteration 150/1000 | Loss: 0.00001019
Iteration 151/1000 | Loss: 0.00001018
Iteration 152/1000 | Loss: 0.00001018
Iteration 153/1000 | Loss: 0.00001018
Iteration 154/1000 | Loss: 0.00001018
Iteration 155/1000 | Loss: 0.00001018
Iteration 156/1000 | Loss: 0.00001018
Iteration 157/1000 | Loss: 0.00001018
Iteration 158/1000 | Loss: 0.00001018
Iteration 159/1000 | Loss: 0.00001018
Iteration 160/1000 | Loss: 0.00001018
Iteration 161/1000 | Loss: 0.00001018
Iteration 162/1000 | Loss: 0.00001018
Iteration 163/1000 | Loss: 0.00001018
Iteration 164/1000 | Loss: 0.00001018
Iteration 165/1000 | Loss: 0.00001017
Iteration 166/1000 | Loss: 0.00001017
Iteration 167/1000 | Loss: 0.00001017
Iteration 168/1000 | Loss: 0.00001017
Iteration 169/1000 | Loss: 0.00001017
Iteration 170/1000 | Loss: 0.00001017
Iteration 171/1000 | Loss: 0.00001017
Iteration 172/1000 | Loss: 0.00001017
Iteration 173/1000 | Loss: 0.00001017
Iteration 174/1000 | Loss: 0.00001017
Iteration 175/1000 | Loss: 0.00001017
Iteration 176/1000 | Loss: 0.00001017
Iteration 177/1000 | Loss: 0.00001017
Iteration 178/1000 | Loss: 0.00001016
Iteration 179/1000 | Loss: 0.00001016
Iteration 180/1000 | Loss: 0.00001016
Iteration 181/1000 | Loss: 0.00001016
Iteration 182/1000 | Loss: 0.00001016
Iteration 183/1000 | Loss: 0.00001016
Iteration 184/1000 | Loss: 0.00001016
Iteration 185/1000 | Loss: 0.00001016
Iteration 186/1000 | Loss: 0.00001016
Iteration 187/1000 | Loss: 0.00001016
Iteration 188/1000 | Loss: 0.00001016
Iteration 189/1000 | Loss: 0.00001016
Iteration 190/1000 | Loss: 0.00001015
Iteration 191/1000 | Loss: 0.00001015
Iteration 192/1000 | Loss: 0.00001015
Iteration 193/1000 | Loss: 0.00001015
Iteration 194/1000 | Loss: 0.00001015
Iteration 195/1000 | Loss: 0.00001015
Iteration 196/1000 | Loss: 0.00001015
Iteration 197/1000 | Loss: 0.00001015
Iteration 198/1000 | Loss: 0.00001015
Iteration 199/1000 | Loss: 0.00001015
Iteration 200/1000 | Loss: 0.00001015
Iteration 201/1000 | Loss: 0.00001015
Iteration 202/1000 | Loss: 0.00001015
Iteration 203/1000 | Loss: 0.00001015
Iteration 204/1000 | Loss: 0.00001015
Iteration 205/1000 | Loss: 0.00001015
Iteration 206/1000 | Loss: 0.00001015
Iteration 207/1000 | Loss: 0.00001015
Iteration 208/1000 | Loss: 0.00001015
Iteration 209/1000 | Loss: 0.00001014
Iteration 210/1000 | Loss: 0.00001014
Iteration 211/1000 | Loss: 0.00001014
Iteration 212/1000 | Loss: 0.00001014
Iteration 213/1000 | Loss: 0.00001014
Iteration 214/1000 | Loss: 0.00001014
Iteration 215/1000 | Loss: 0.00001014
Iteration 216/1000 | Loss: 0.00001014
Iteration 217/1000 | Loss: 0.00001014
Iteration 218/1000 | Loss: 0.00001014
Iteration 219/1000 | Loss: 0.00001014
Iteration 220/1000 | Loss: 0.00001014
Iteration 221/1000 | Loss: 0.00001014
Iteration 222/1000 | Loss: 0.00001014
Iteration 223/1000 | Loss: 0.00001014
Iteration 224/1000 | Loss: 0.00001014
Iteration 225/1000 | Loss: 0.00001013
Iteration 226/1000 | Loss: 0.00001013
Iteration 227/1000 | Loss: 0.00001013
Iteration 228/1000 | Loss: 0.00001013
Iteration 229/1000 | Loss: 0.00001013
Iteration 230/1000 | Loss: 0.00001013
Iteration 231/1000 | Loss: 0.00001013
Iteration 232/1000 | Loss: 0.00001013
Iteration 233/1000 | Loss: 0.00001013
Iteration 234/1000 | Loss: 0.00001013
Iteration 235/1000 | Loss: 0.00001013
Iteration 236/1000 | Loss: 0.00001013
Iteration 237/1000 | Loss: 0.00001013
Iteration 238/1000 | Loss: 0.00001013
Iteration 239/1000 | Loss: 0.00001013
Iteration 240/1000 | Loss: 0.00001013
Iteration 241/1000 | Loss: 0.00001013
Iteration 242/1000 | Loss: 0.00001013
Iteration 243/1000 | Loss: 0.00001013
Iteration 244/1000 | Loss: 0.00001013
Iteration 245/1000 | Loss: 0.00001013
Iteration 246/1000 | Loss: 0.00001013
Iteration 247/1000 | Loss: 0.00001013
Iteration 248/1000 | Loss: 0.00001013
Iteration 249/1000 | Loss: 0.00001013
Iteration 250/1000 | Loss: 0.00001013
Iteration 251/1000 | Loss: 0.00001012
Iteration 252/1000 | Loss: 0.00001012
Iteration 253/1000 | Loss: 0.00001012
Iteration 254/1000 | Loss: 0.00001012
Iteration 255/1000 | Loss: 0.00001012
Iteration 256/1000 | Loss: 0.00001012
Iteration 257/1000 | Loss: 0.00001012
Iteration 258/1000 | Loss: 0.00001012
Iteration 259/1000 | Loss: 0.00001012
Iteration 260/1000 | Loss: 0.00001012
Iteration 261/1000 | Loss: 0.00001012
Iteration 262/1000 | Loss: 0.00001012
Iteration 263/1000 | Loss: 0.00001012
Iteration 264/1000 | Loss: 0.00001012
Iteration 265/1000 | Loss: 0.00001012
Iteration 266/1000 | Loss: 0.00001012
Iteration 267/1000 | Loss: 0.00001012
Iteration 268/1000 | Loss: 0.00001012
Iteration 269/1000 | Loss: 0.00001012
Iteration 270/1000 | Loss: 0.00001012
Iteration 271/1000 | Loss: 0.00001012
Iteration 272/1000 | Loss: 0.00001012
Iteration 273/1000 | Loss: 0.00001012
Iteration 274/1000 | Loss: 0.00001012
Iteration 275/1000 | Loss: 0.00001012
Iteration 276/1000 | Loss: 0.00001012
Iteration 277/1000 | Loss: 0.00001012
Iteration 278/1000 | Loss: 0.00001012
Iteration 279/1000 | Loss: 0.00001012
Iteration 280/1000 | Loss: 0.00001012
Iteration 281/1000 | Loss: 0.00001012
Iteration 282/1000 | Loss: 0.00001012
Iteration 283/1000 | Loss: 0.00001012
Iteration 284/1000 | Loss: 0.00001012
Iteration 285/1000 | Loss: 0.00001012
Iteration 286/1000 | Loss: 0.00001012
Iteration 287/1000 | Loss: 0.00001012
Iteration 288/1000 | Loss: 0.00001012
Iteration 289/1000 | Loss: 0.00001012
Iteration 290/1000 | Loss: 0.00001012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [1.0124641448783223e-05, 1.0124641448783223e-05, 1.0124641448783223e-05, 1.0124641448783223e-05, 1.0124641448783223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0124641448783223e-05

Optimization complete. Final v2v error: 2.706368923187256 mm

Highest mean error: 4.299403190612793 mm for frame 61

Lowest mean error: 2.4715209007263184 mm for frame 83

Saving results

Total time: 44.872337341308594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817813
Iteration 2/25 | Loss: 0.00127820
Iteration 3/25 | Loss: 0.00118270
Iteration 4/25 | Loss: 0.00117253
Iteration 5/25 | Loss: 0.00117049
Iteration 6/25 | Loss: 0.00117027
Iteration 7/25 | Loss: 0.00117027
Iteration 8/25 | Loss: 0.00117027
Iteration 9/25 | Loss: 0.00117027
Iteration 10/25 | Loss: 0.00117027
Iteration 11/25 | Loss: 0.00117027
Iteration 12/25 | Loss: 0.00117027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001170273986645043, 0.001170273986645043, 0.001170273986645043, 0.001170273986645043, 0.001170273986645043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001170273986645043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29052484
Iteration 2/25 | Loss: 0.00119692
Iteration 3/25 | Loss: 0.00119690
Iteration 4/25 | Loss: 0.00119690
Iteration 5/25 | Loss: 0.00119690
Iteration 6/25 | Loss: 0.00119690
Iteration 7/25 | Loss: 0.00119690
Iteration 8/25 | Loss: 0.00119690
Iteration 9/25 | Loss: 0.00119690
Iteration 10/25 | Loss: 0.00119690
Iteration 11/25 | Loss: 0.00119690
Iteration 12/25 | Loss: 0.00119690
Iteration 13/25 | Loss: 0.00119690
Iteration 14/25 | Loss: 0.00119690
Iteration 15/25 | Loss: 0.00119690
Iteration 16/25 | Loss: 0.00119690
Iteration 17/25 | Loss: 0.00119690
Iteration 18/25 | Loss: 0.00119690
Iteration 19/25 | Loss: 0.00119690
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011968999169766903, 0.0011968999169766903, 0.0011968999169766903, 0.0011968999169766903, 0.0011968999169766903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011968999169766903

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119690
Iteration 2/1000 | Loss: 0.00001882
Iteration 3/1000 | Loss: 0.00001266
Iteration 4/1000 | Loss: 0.00001117
Iteration 5/1000 | Loss: 0.00001031
Iteration 6/1000 | Loss: 0.00000976
Iteration 7/1000 | Loss: 0.00000932
Iteration 8/1000 | Loss: 0.00000913
Iteration 9/1000 | Loss: 0.00000908
Iteration 10/1000 | Loss: 0.00000891
Iteration 11/1000 | Loss: 0.00000872
Iteration 12/1000 | Loss: 0.00000861
Iteration 13/1000 | Loss: 0.00000860
Iteration 14/1000 | Loss: 0.00000859
Iteration 15/1000 | Loss: 0.00000858
Iteration 16/1000 | Loss: 0.00000857
Iteration 17/1000 | Loss: 0.00000854
Iteration 18/1000 | Loss: 0.00000853
Iteration 19/1000 | Loss: 0.00000853
Iteration 20/1000 | Loss: 0.00000852
Iteration 21/1000 | Loss: 0.00000851
Iteration 22/1000 | Loss: 0.00000851
Iteration 23/1000 | Loss: 0.00000850
Iteration 24/1000 | Loss: 0.00000849
Iteration 25/1000 | Loss: 0.00000846
Iteration 26/1000 | Loss: 0.00000846
Iteration 27/1000 | Loss: 0.00000845
Iteration 28/1000 | Loss: 0.00000845
Iteration 29/1000 | Loss: 0.00000844
Iteration 30/1000 | Loss: 0.00000844
Iteration 31/1000 | Loss: 0.00000835
Iteration 32/1000 | Loss: 0.00000835
Iteration 33/1000 | Loss: 0.00000834
Iteration 34/1000 | Loss: 0.00000833
Iteration 35/1000 | Loss: 0.00000833
Iteration 36/1000 | Loss: 0.00000832
Iteration 37/1000 | Loss: 0.00000830
Iteration 38/1000 | Loss: 0.00000829
Iteration 39/1000 | Loss: 0.00000829
Iteration 40/1000 | Loss: 0.00000829
Iteration 41/1000 | Loss: 0.00000828
Iteration 42/1000 | Loss: 0.00000828
Iteration 43/1000 | Loss: 0.00000828
Iteration 44/1000 | Loss: 0.00000828
Iteration 45/1000 | Loss: 0.00000827
Iteration 46/1000 | Loss: 0.00000827
Iteration 47/1000 | Loss: 0.00000826
Iteration 48/1000 | Loss: 0.00000826
Iteration 49/1000 | Loss: 0.00000825
Iteration 50/1000 | Loss: 0.00000825
Iteration 51/1000 | Loss: 0.00000825
Iteration 52/1000 | Loss: 0.00000825
Iteration 53/1000 | Loss: 0.00000825
Iteration 54/1000 | Loss: 0.00000824
Iteration 55/1000 | Loss: 0.00000824
Iteration 56/1000 | Loss: 0.00000823
Iteration 57/1000 | Loss: 0.00000823
Iteration 58/1000 | Loss: 0.00000822
Iteration 59/1000 | Loss: 0.00000822
Iteration 60/1000 | Loss: 0.00000822
Iteration 61/1000 | Loss: 0.00000822
Iteration 62/1000 | Loss: 0.00000822
Iteration 63/1000 | Loss: 0.00000821
Iteration 64/1000 | Loss: 0.00000821
Iteration 65/1000 | Loss: 0.00000821
Iteration 66/1000 | Loss: 0.00000821
Iteration 67/1000 | Loss: 0.00000821
Iteration 68/1000 | Loss: 0.00000820
Iteration 69/1000 | Loss: 0.00000820
Iteration 70/1000 | Loss: 0.00000819
Iteration 71/1000 | Loss: 0.00000819
Iteration 72/1000 | Loss: 0.00000819
Iteration 73/1000 | Loss: 0.00000818
Iteration 74/1000 | Loss: 0.00000818
Iteration 75/1000 | Loss: 0.00000818
Iteration 76/1000 | Loss: 0.00000817
Iteration 77/1000 | Loss: 0.00000817
Iteration 78/1000 | Loss: 0.00000817
Iteration 79/1000 | Loss: 0.00000817
Iteration 80/1000 | Loss: 0.00000817
Iteration 81/1000 | Loss: 0.00000817
Iteration 82/1000 | Loss: 0.00000817
Iteration 83/1000 | Loss: 0.00000816
Iteration 84/1000 | Loss: 0.00000816
Iteration 85/1000 | Loss: 0.00000816
Iteration 86/1000 | Loss: 0.00000815
Iteration 87/1000 | Loss: 0.00000815
Iteration 88/1000 | Loss: 0.00000815
Iteration 89/1000 | Loss: 0.00000815
Iteration 90/1000 | Loss: 0.00000815
Iteration 91/1000 | Loss: 0.00000814
Iteration 92/1000 | Loss: 0.00000814
Iteration 93/1000 | Loss: 0.00000814
Iteration 94/1000 | Loss: 0.00000814
Iteration 95/1000 | Loss: 0.00000814
Iteration 96/1000 | Loss: 0.00000813
Iteration 97/1000 | Loss: 0.00000813
Iteration 98/1000 | Loss: 0.00000812
Iteration 99/1000 | Loss: 0.00000812
Iteration 100/1000 | Loss: 0.00000811
Iteration 101/1000 | Loss: 0.00000811
Iteration 102/1000 | Loss: 0.00000811
Iteration 103/1000 | Loss: 0.00000811
Iteration 104/1000 | Loss: 0.00000810
Iteration 105/1000 | Loss: 0.00000810
Iteration 106/1000 | Loss: 0.00000810
Iteration 107/1000 | Loss: 0.00000810
Iteration 108/1000 | Loss: 0.00000810
Iteration 109/1000 | Loss: 0.00000810
Iteration 110/1000 | Loss: 0.00000809
Iteration 111/1000 | Loss: 0.00000809
Iteration 112/1000 | Loss: 0.00000808
Iteration 113/1000 | Loss: 0.00000808
Iteration 114/1000 | Loss: 0.00000808
Iteration 115/1000 | Loss: 0.00000807
Iteration 116/1000 | Loss: 0.00000807
Iteration 117/1000 | Loss: 0.00000807
Iteration 118/1000 | Loss: 0.00000806
Iteration 119/1000 | Loss: 0.00000806
Iteration 120/1000 | Loss: 0.00000806
Iteration 121/1000 | Loss: 0.00000806
Iteration 122/1000 | Loss: 0.00000806
Iteration 123/1000 | Loss: 0.00000806
Iteration 124/1000 | Loss: 0.00000806
Iteration 125/1000 | Loss: 0.00000806
Iteration 126/1000 | Loss: 0.00000805
Iteration 127/1000 | Loss: 0.00000805
Iteration 128/1000 | Loss: 0.00000805
Iteration 129/1000 | Loss: 0.00000805
Iteration 130/1000 | Loss: 0.00000805
Iteration 131/1000 | Loss: 0.00000805
Iteration 132/1000 | Loss: 0.00000805
Iteration 133/1000 | Loss: 0.00000805
Iteration 134/1000 | Loss: 0.00000805
Iteration 135/1000 | Loss: 0.00000805
Iteration 136/1000 | Loss: 0.00000805
Iteration 137/1000 | Loss: 0.00000805
Iteration 138/1000 | Loss: 0.00000805
Iteration 139/1000 | Loss: 0.00000805
Iteration 140/1000 | Loss: 0.00000804
Iteration 141/1000 | Loss: 0.00000804
Iteration 142/1000 | Loss: 0.00000804
Iteration 143/1000 | Loss: 0.00000804
Iteration 144/1000 | Loss: 0.00000804
Iteration 145/1000 | Loss: 0.00000804
Iteration 146/1000 | Loss: 0.00000803
Iteration 147/1000 | Loss: 0.00000803
Iteration 148/1000 | Loss: 0.00000803
Iteration 149/1000 | Loss: 0.00000802
Iteration 150/1000 | Loss: 0.00000802
Iteration 151/1000 | Loss: 0.00000802
Iteration 152/1000 | Loss: 0.00000802
Iteration 153/1000 | Loss: 0.00000802
Iteration 154/1000 | Loss: 0.00000802
Iteration 155/1000 | Loss: 0.00000802
Iteration 156/1000 | Loss: 0.00000802
Iteration 157/1000 | Loss: 0.00000802
Iteration 158/1000 | Loss: 0.00000802
Iteration 159/1000 | Loss: 0.00000801
Iteration 160/1000 | Loss: 0.00000801
Iteration 161/1000 | Loss: 0.00000801
Iteration 162/1000 | Loss: 0.00000801
Iteration 163/1000 | Loss: 0.00000801
Iteration 164/1000 | Loss: 0.00000801
Iteration 165/1000 | Loss: 0.00000800
Iteration 166/1000 | Loss: 0.00000800
Iteration 167/1000 | Loss: 0.00000800
Iteration 168/1000 | Loss: 0.00000800
Iteration 169/1000 | Loss: 0.00000800
Iteration 170/1000 | Loss: 0.00000800
Iteration 171/1000 | Loss: 0.00000800
Iteration 172/1000 | Loss: 0.00000800
Iteration 173/1000 | Loss: 0.00000800
Iteration 174/1000 | Loss: 0.00000800
Iteration 175/1000 | Loss: 0.00000800
Iteration 176/1000 | Loss: 0.00000800
Iteration 177/1000 | Loss: 0.00000800
Iteration 178/1000 | Loss: 0.00000800
Iteration 179/1000 | Loss: 0.00000800
Iteration 180/1000 | Loss: 0.00000799
Iteration 181/1000 | Loss: 0.00000799
Iteration 182/1000 | Loss: 0.00000799
Iteration 183/1000 | Loss: 0.00000799
Iteration 184/1000 | Loss: 0.00000799
Iteration 185/1000 | Loss: 0.00000799
Iteration 186/1000 | Loss: 0.00000799
Iteration 187/1000 | Loss: 0.00000799
Iteration 188/1000 | Loss: 0.00000799
Iteration 189/1000 | Loss: 0.00000799
Iteration 190/1000 | Loss: 0.00000799
Iteration 191/1000 | Loss: 0.00000798
Iteration 192/1000 | Loss: 0.00000798
Iteration 193/1000 | Loss: 0.00000798
Iteration 194/1000 | Loss: 0.00000798
Iteration 195/1000 | Loss: 0.00000798
Iteration 196/1000 | Loss: 0.00000798
Iteration 197/1000 | Loss: 0.00000798
Iteration 198/1000 | Loss: 0.00000798
Iteration 199/1000 | Loss: 0.00000798
Iteration 200/1000 | Loss: 0.00000798
Iteration 201/1000 | Loss: 0.00000798
Iteration 202/1000 | Loss: 0.00000798
Iteration 203/1000 | Loss: 0.00000798
Iteration 204/1000 | Loss: 0.00000798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [7.981643648236059e-06, 7.981643648236059e-06, 7.981643648236059e-06, 7.981643648236059e-06, 7.981643648236059e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.981643648236059e-06

Optimization complete. Final v2v error: 2.4519426822662354 mm

Highest mean error: 2.7700653076171875 mm for frame 0

Lowest mean error: 2.340557336807251 mm for frame 96

Saving results

Total time: 37.56183862686157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775198
Iteration 2/25 | Loss: 0.00146563
Iteration 3/25 | Loss: 0.00131068
Iteration 4/25 | Loss: 0.00127886
Iteration 5/25 | Loss: 0.00126893
Iteration 6/25 | Loss: 0.00126646
Iteration 7/25 | Loss: 0.00126646
Iteration 8/25 | Loss: 0.00126646
Iteration 9/25 | Loss: 0.00126646
Iteration 10/25 | Loss: 0.00126646
Iteration 11/25 | Loss: 0.00126646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012664604000747204, 0.0012664604000747204, 0.0012664604000747204, 0.0012664604000747204, 0.0012664604000747204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012664604000747204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46367955
Iteration 2/25 | Loss: 0.00204405
Iteration 3/25 | Loss: 0.00204405
Iteration 4/25 | Loss: 0.00204405
Iteration 5/25 | Loss: 0.00204405
Iteration 6/25 | Loss: 0.00204404
Iteration 7/25 | Loss: 0.00204404
Iteration 8/25 | Loss: 0.00204404
Iteration 9/25 | Loss: 0.00204404
Iteration 10/25 | Loss: 0.00204404
Iteration 11/25 | Loss: 0.00204404
Iteration 12/25 | Loss: 0.00204404
Iteration 13/25 | Loss: 0.00204404
Iteration 14/25 | Loss: 0.00204404
Iteration 15/25 | Loss: 0.00204404
Iteration 16/25 | Loss: 0.00204404
Iteration 17/25 | Loss: 0.00204404
Iteration 18/25 | Loss: 0.00204404
Iteration 19/25 | Loss: 0.00204404
Iteration 20/25 | Loss: 0.00204404
Iteration 21/25 | Loss: 0.00204404
Iteration 22/25 | Loss: 0.00204404
Iteration 23/25 | Loss: 0.00204404
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002044043969362974, 0.002044043969362974, 0.002044043969362974, 0.002044043969362974, 0.002044043969362974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002044043969362974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204404
Iteration 2/1000 | Loss: 0.00006891
Iteration 3/1000 | Loss: 0.00004413
Iteration 4/1000 | Loss: 0.00003247
Iteration 5/1000 | Loss: 0.00002922
Iteration 6/1000 | Loss: 0.00002658
Iteration 7/1000 | Loss: 0.00002534
Iteration 8/1000 | Loss: 0.00002448
Iteration 9/1000 | Loss: 0.00002376
Iteration 10/1000 | Loss: 0.00002321
Iteration 11/1000 | Loss: 0.00002283
Iteration 12/1000 | Loss: 0.00002251
Iteration 13/1000 | Loss: 0.00002218
Iteration 14/1000 | Loss: 0.00002194
Iteration 15/1000 | Loss: 0.00002176
Iteration 16/1000 | Loss: 0.00002159
Iteration 17/1000 | Loss: 0.00002157
Iteration 18/1000 | Loss: 0.00002155
Iteration 19/1000 | Loss: 0.00002154
Iteration 20/1000 | Loss: 0.00002151
Iteration 21/1000 | Loss: 0.00002147
Iteration 22/1000 | Loss: 0.00002142
Iteration 23/1000 | Loss: 0.00002139
Iteration 24/1000 | Loss: 0.00002138
Iteration 25/1000 | Loss: 0.00002135
Iteration 26/1000 | Loss: 0.00002134
Iteration 27/1000 | Loss: 0.00002133
Iteration 28/1000 | Loss: 0.00002125
Iteration 29/1000 | Loss: 0.00002119
Iteration 30/1000 | Loss: 0.00002118
Iteration 31/1000 | Loss: 0.00002118
Iteration 32/1000 | Loss: 0.00002118
Iteration 33/1000 | Loss: 0.00002118
Iteration 34/1000 | Loss: 0.00002117
Iteration 35/1000 | Loss: 0.00002117
Iteration 36/1000 | Loss: 0.00002117
Iteration 37/1000 | Loss: 0.00002117
Iteration 38/1000 | Loss: 0.00002117
Iteration 39/1000 | Loss: 0.00002116
Iteration 40/1000 | Loss: 0.00002116
Iteration 41/1000 | Loss: 0.00002116
Iteration 42/1000 | Loss: 0.00002115
Iteration 43/1000 | Loss: 0.00002115
Iteration 44/1000 | Loss: 0.00002114
Iteration 45/1000 | Loss: 0.00002114
Iteration 46/1000 | Loss: 0.00002113
Iteration 47/1000 | Loss: 0.00002113
Iteration 48/1000 | Loss: 0.00002112
Iteration 49/1000 | Loss: 0.00002112
Iteration 50/1000 | Loss: 0.00002112
Iteration 51/1000 | Loss: 0.00002112
Iteration 52/1000 | Loss: 0.00002112
Iteration 53/1000 | Loss: 0.00002112
Iteration 54/1000 | Loss: 0.00002112
Iteration 55/1000 | Loss: 0.00002110
Iteration 56/1000 | Loss: 0.00002110
Iteration 57/1000 | Loss: 0.00002110
Iteration 58/1000 | Loss: 0.00002109
Iteration 59/1000 | Loss: 0.00002109
Iteration 60/1000 | Loss: 0.00002109
Iteration 61/1000 | Loss: 0.00002109
Iteration 62/1000 | Loss: 0.00002108
Iteration 63/1000 | Loss: 0.00002108
Iteration 64/1000 | Loss: 0.00002108
Iteration 65/1000 | Loss: 0.00002108
Iteration 66/1000 | Loss: 0.00002108
Iteration 67/1000 | Loss: 0.00002108
Iteration 68/1000 | Loss: 0.00002107
Iteration 69/1000 | Loss: 0.00002107
Iteration 70/1000 | Loss: 0.00002107
Iteration 71/1000 | Loss: 0.00002106
Iteration 72/1000 | Loss: 0.00002106
Iteration 73/1000 | Loss: 0.00002106
Iteration 74/1000 | Loss: 0.00002105
Iteration 75/1000 | Loss: 0.00002105
Iteration 76/1000 | Loss: 0.00002105
Iteration 77/1000 | Loss: 0.00002105
Iteration 78/1000 | Loss: 0.00002104
Iteration 79/1000 | Loss: 0.00002104
Iteration 80/1000 | Loss: 0.00002104
Iteration 81/1000 | Loss: 0.00002104
Iteration 82/1000 | Loss: 0.00002103
Iteration 83/1000 | Loss: 0.00002103
Iteration 84/1000 | Loss: 0.00002103
Iteration 85/1000 | Loss: 0.00002102
Iteration 86/1000 | Loss: 0.00002102
Iteration 87/1000 | Loss: 0.00002102
Iteration 88/1000 | Loss: 0.00002102
Iteration 89/1000 | Loss: 0.00002101
Iteration 90/1000 | Loss: 0.00002101
Iteration 91/1000 | Loss: 0.00002101
Iteration 92/1000 | Loss: 0.00002101
Iteration 93/1000 | Loss: 0.00002101
Iteration 94/1000 | Loss: 0.00002101
Iteration 95/1000 | Loss: 0.00002101
Iteration 96/1000 | Loss: 0.00002100
Iteration 97/1000 | Loss: 0.00002100
Iteration 98/1000 | Loss: 0.00002100
Iteration 99/1000 | Loss: 0.00002100
Iteration 100/1000 | Loss: 0.00002099
Iteration 101/1000 | Loss: 0.00002099
Iteration 102/1000 | Loss: 0.00002099
Iteration 103/1000 | Loss: 0.00002099
Iteration 104/1000 | Loss: 0.00002098
Iteration 105/1000 | Loss: 0.00002098
Iteration 106/1000 | Loss: 0.00002098
Iteration 107/1000 | Loss: 0.00002097
Iteration 108/1000 | Loss: 0.00002097
Iteration 109/1000 | Loss: 0.00002097
Iteration 110/1000 | Loss: 0.00002097
Iteration 111/1000 | Loss: 0.00002097
Iteration 112/1000 | Loss: 0.00002096
Iteration 113/1000 | Loss: 0.00002096
Iteration 114/1000 | Loss: 0.00002096
Iteration 115/1000 | Loss: 0.00002096
Iteration 116/1000 | Loss: 0.00002096
Iteration 117/1000 | Loss: 0.00002095
Iteration 118/1000 | Loss: 0.00002095
Iteration 119/1000 | Loss: 0.00002095
Iteration 120/1000 | Loss: 0.00002095
Iteration 121/1000 | Loss: 0.00002095
Iteration 122/1000 | Loss: 0.00002095
Iteration 123/1000 | Loss: 0.00002094
Iteration 124/1000 | Loss: 0.00002094
Iteration 125/1000 | Loss: 0.00002094
Iteration 126/1000 | Loss: 0.00002094
Iteration 127/1000 | Loss: 0.00002093
Iteration 128/1000 | Loss: 0.00002093
Iteration 129/1000 | Loss: 0.00002093
Iteration 130/1000 | Loss: 0.00002093
Iteration 131/1000 | Loss: 0.00002092
Iteration 132/1000 | Loss: 0.00002092
Iteration 133/1000 | Loss: 0.00002092
Iteration 134/1000 | Loss: 0.00002092
Iteration 135/1000 | Loss: 0.00002091
Iteration 136/1000 | Loss: 0.00002091
Iteration 137/1000 | Loss: 0.00002091
Iteration 138/1000 | Loss: 0.00002091
Iteration 139/1000 | Loss: 0.00002091
Iteration 140/1000 | Loss: 0.00002091
Iteration 141/1000 | Loss: 0.00002090
Iteration 142/1000 | Loss: 0.00002090
Iteration 143/1000 | Loss: 0.00002090
Iteration 144/1000 | Loss: 0.00002090
Iteration 145/1000 | Loss: 0.00002090
Iteration 146/1000 | Loss: 0.00002090
Iteration 147/1000 | Loss: 0.00002090
Iteration 148/1000 | Loss: 0.00002089
Iteration 149/1000 | Loss: 0.00002089
Iteration 150/1000 | Loss: 0.00002089
Iteration 151/1000 | Loss: 0.00002089
Iteration 152/1000 | Loss: 0.00002089
Iteration 153/1000 | Loss: 0.00002089
Iteration 154/1000 | Loss: 0.00002089
Iteration 155/1000 | Loss: 0.00002089
Iteration 156/1000 | Loss: 0.00002089
Iteration 157/1000 | Loss: 0.00002089
Iteration 158/1000 | Loss: 0.00002089
Iteration 159/1000 | Loss: 0.00002089
Iteration 160/1000 | Loss: 0.00002088
Iteration 161/1000 | Loss: 0.00002088
Iteration 162/1000 | Loss: 0.00002088
Iteration 163/1000 | Loss: 0.00002088
Iteration 164/1000 | Loss: 0.00002088
Iteration 165/1000 | Loss: 0.00002088
Iteration 166/1000 | Loss: 0.00002088
Iteration 167/1000 | Loss: 0.00002088
Iteration 168/1000 | Loss: 0.00002088
Iteration 169/1000 | Loss: 0.00002087
Iteration 170/1000 | Loss: 0.00002087
Iteration 171/1000 | Loss: 0.00002087
Iteration 172/1000 | Loss: 0.00002086
Iteration 173/1000 | Loss: 0.00002086
Iteration 174/1000 | Loss: 0.00002086
Iteration 175/1000 | Loss: 0.00002086
Iteration 176/1000 | Loss: 0.00002086
Iteration 177/1000 | Loss: 0.00002086
Iteration 178/1000 | Loss: 0.00002086
Iteration 179/1000 | Loss: 0.00002085
Iteration 180/1000 | Loss: 0.00002085
Iteration 181/1000 | Loss: 0.00002085
Iteration 182/1000 | Loss: 0.00002085
Iteration 183/1000 | Loss: 0.00002085
Iteration 184/1000 | Loss: 0.00002085
Iteration 185/1000 | Loss: 0.00002085
Iteration 186/1000 | Loss: 0.00002085
Iteration 187/1000 | Loss: 0.00002085
Iteration 188/1000 | Loss: 0.00002085
Iteration 189/1000 | Loss: 0.00002085
Iteration 190/1000 | Loss: 0.00002085
Iteration 191/1000 | Loss: 0.00002085
Iteration 192/1000 | Loss: 0.00002084
Iteration 193/1000 | Loss: 0.00002084
Iteration 194/1000 | Loss: 0.00002084
Iteration 195/1000 | Loss: 0.00002084
Iteration 196/1000 | Loss: 0.00002084
Iteration 197/1000 | Loss: 0.00002084
Iteration 198/1000 | Loss: 0.00002084
Iteration 199/1000 | Loss: 0.00002084
Iteration 200/1000 | Loss: 0.00002084
Iteration 201/1000 | Loss: 0.00002083
Iteration 202/1000 | Loss: 0.00002083
Iteration 203/1000 | Loss: 0.00002083
Iteration 204/1000 | Loss: 0.00002083
Iteration 205/1000 | Loss: 0.00002083
Iteration 206/1000 | Loss: 0.00002083
Iteration 207/1000 | Loss: 0.00002083
Iteration 208/1000 | Loss: 0.00002083
Iteration 209/1000 | Loss: 0.00002082
Iteration 210/1000 | Loss: 0.00002082
Iteration 211/1000 | Loss: 0.00002082
Iteration 212/1000 | Loss: 0.00002082
Iteration 213/1000 | Loss: 0.00002082
Iteration 214/1000 | Loss: 0.00002082
Iteration 215/1000 | Loss: 0.00002082
Iteration 216/1000 | Loss: 0.00002082
Iteration 217/1000 | Loss: 0.00002082
Iteration 218/1000 | Loss: 0.00002082
Iteration 219/1000 | Loss: 0.00002082
Iteration 220/1000 | Loss: 0.00002082
Iteration 221/1000 | Loss: 0.00002082
Iteration 222/1000 | Loss: 0.00002082
Iteration 223/1000 | Loss: 0.00002082
Iteration 224/1000 | Loss: 0.00002082
Iteration 225/1000 | Loss: 0.00002082
Iteration 226/1000 | Loss: 0.00002082
Iteration 227/1000 | Loss: 0.00002082
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [2.082132050418295e-05, 2.082132050418295e-05, 2.082132050418295e-05, 2.082132050418295e-05, 2.082132050418295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.082132050418295e-05

Optimization complete. Final v2v error: 3.8506267070770264 mm

Highest mean error: 5.374300479888916 mm for frame 176

Lowest mean error: 3.011554718017578 mm for frame 130

Saving results

Total time: 56.396795988082886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755283
Iteration 2/25 | Loss: 0.00195450
Iteration 3/25 | Loss: 0.00154261
Iteration 4/25 | Loss: 0.00157503
Iteration 5/25 | Loss: 0.00147076
Iteration 6/25 | Loss: 0.00143014
Iteration 7/25 | Loss: 0.00138246
Iteration 8/25 | Loss: 0.00136305
Iteration 9/25 | Loss: 0.00133625
Iteration 10/25 | Loss: 0.00133015
Iteration 11/25 | Loss: 0.00133186
Iteration 12/25 | Loss: 0.00132359
Iteration 13/25 | Loss: 0.00131946
Iteration 14/25 | Loss: 0.00131346
Iteration 15/25 | Loss: 0.00131514
Iteration 16/25 | Loss: 0.00131278
Iteration 17/25 | Loss: 0.00131104
Iteration 18/25 | Loss: 0.00130925
Iteration 19/25 | Loss: 0.00130376
Iteration 20/25 | Loss: 0.00130312
Iteration 21/25 | Loss: 0.00130308
Iteration 22/25 | Loss: 0.00130308
Iteration 23/25 | Loss: 0.00130308
Iteration 24/25 | Loss: 0.00130307
Iteration 25/25 | Loss: 0.00130307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19126630
Iteration 2/25 | Loss: 0.00127027
Iteration 3/25 | Loss: 0.00127025
Iteration 4/25 | Loss: 0.00127025
Iteration 5/25 | Loss: 0.00127025
Iteration 6/25 | Loss: 0.00127025
Iteration 7/25 | Loss: 0.00127025
Iteration 8/25 | Loss: 0.00127025
Iteration 9/25 | Loss: 0.00127025
Iteration 10/25 | Loss: 0.00127025
Iteration 11/25 | Loss: 0.00127025
Iteration 12/25 | Loss: 0.00127025
Iteration 13/25 | Loss: 0.00127025
Iteration 14/25 | Loss: 0.00127025
Iteration 15/25 | Loss: 0.00127025
Iteration 16/25 | Loss: 0.00127025
Iteration 17/25 | Loss: 0.00127025
Iteration 18/25 | Loss: 0.00127025
Iteration 19/25 | Loss: 0.00127025
Iteration 20/25 | Loss: 0.00127025
Iteration 21/25 | Loss: 0.00127025
Iteration 22/25 | Loss: 0.00127025
Iteration 23/25 | Loss: 0.00127025
Iteration 24/25 | Loss: 0.00127025
Iteration 25/25 | Loss: 0.00127025

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127025
Iteration 2/1000 | Loss: 0.00100337
Iteration 3/1000 | Loss: 0.00013348
Iteration 4/1000 | Loss: 0.00006976
Iteration 5/1000 | Loss: 0.00005102
Iteration 6/1000 | Loss: 0.00003834
Iteration 7/1000 | Loss: 0.00003301
Iteration 8/1000 | Loss: 0.00021304
Iteration 9/1000 | Loss: 0.00003052
Iteration 10/1000 | Loss: 0.00011969
Iteration 11/1000 | Loss: 0.00002977
Iteration 12/1000 | Loss: 0.00002708
Iteration 13/1000 | Loss: 0.00002556
Iteration 14/1000 | Loss: 0.00010186
Iteration 15/1000 | Loss: 0.00017787
Iteration 16/1000 | Loss: 0.00015909
Iteration 17/1000 | Loss: 0.00016636
Iteration 18/1000 | Loss: 0.00031728
Iteration 19/1000 | Loss: 0.00009089
Iteration 20/1000 | Loss: 0.00005919
Iteration 21/1000 | Loss: 0.00002557
Iteration 22/1000 | Loss: 0.00002268
Iteration 23/1000 | Loss: 0.00002065
Iteration 24/1000 | Loss: 0.00001970
Iteration 25/1000 | Loss: 0.00001916
Iteration 26/1000 | Loss: 0.00001860
Iteration 27/1000 | Loss: 0.00001812
Iteration 28/1000 | Loss: 0.00001776
Iteration 29/1000 | Loss: 0.00001741
Iteration 30/1000 | Loss: 0.00001720
Iteration 31/1000 | Loss: 0.00001706
Iteration 32/1000 | Loss: 0.00001686
Iteration 33/1000 | Loss: 0.00001686
Iteration 34/1000 | Loss: 0.00001679
Iteration 35/1000 | Loss: 0.00001674
Iteration 36/1000 | Loss: 0.00001673
Iteration 37/1000 | Loss: 0.00001672
Iteration 38/1000 | Loss: 0.00001671
Iteration 39/1000 | Loss: 0.00001671
Iteration 40/1000 | Loss: 0.00001670
Iteration 41/1000 | Loss: 0.00001670
Iteration 42/1000 | Loss: 0.00001669
Iteration 43/1000 | Loss: 0.00001669
Iteration 44/1000 | Loss: 0.00001668
Iteration 45/1000 | Loss: 0.00001668
Iteration 46/1000 | Loss: 0.00001667
Iteration 47/1000 | Loss: 0.00001667
Iteration 48/1000 | Loss: 0.00001667
Iteration 49/1000 | Loss: 0.00001666
Iteration 50/1000 | Loss: 0.00001666
Iteration 51/1000 | Loss: 0.00001665
Iteration 52/1000 | Loss: 0.00001665
Iteration 53/1000 | Loss: 0.00001664
Iteration 54/1000 | Loss: 0.00001664
Iteration 55/1000 | Loss: 0.00001663
Iteration 56/1000 | Loss: 0.00001663
Iteration 57/1000 | Loss: 0.00001662
Iteration 58/1000 | Loss: 0.00001662
Iteration 59/1000 | Loss: 0.00001661
Iteration 60/1000 | Loss: 0.00001661
Iteration 61/1000 | Loss: 0.00001661
Iteration 62/1000 | Loss: 0.00001660
Iteration 63/1000 | Loss: 0.00001660
Iteration 64/1000 | Loss: 0.00001660
Iteration 65/1000 | Loss: 0.00001660
Iteration 66/1000 | Loss: 0.00001660
Iteration 67/1000 | Loss: 0.00001660
Iteration 68/1000 | Loss: 0.00001659
Iteration 69/1000 | Loss: 0.00001659
Iteration 70/1000 | Loss: 0.00001659
Iteration 71/1000 | Loss: 0.00001659
Iteration 72/1000 | Loss: 0.00001659
Iteration 73/1000 | Loss: 0.00001659
Iteration 74/1000 | Loss: 0.00001659
Iteration 75/1000 | Loss: 0.00001659
Iteration 76/1000 | Loss: 0.00001659
Iteration 77/1000 | Loss: 0.00001658
Iteration 78/1000 | Loss: 0.00001658
Iteration 79/1000 | Loss: 0.00001658
Iteration 80/1000 | Loss: 0.00001658
Iteration 81/1000 | Loss: 0.00001658
Iteration 82/1000 | Loss: 0.00001658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.6584024706389755e-05, 1.6584024706389755e-05, 1.6584024706389755e-05, 1.6584024706389755e-05, 1.6584024706389755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6584024706389755e-05

Optimization complete. Final v2v error: 3.4278886318206787 mm

Highest mean error: 4.181857585906982 mm for frame 4

Lowest mean error: 3.186767101287842 mm for frame 151

Saving results

Total time: 96.91740298271179
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792680
Iteration 2/25 | Loss: 0.00128362
Iteration 3/25 | Loss: 0.00117825
Iteration 4/25 | Loss: 0.00116481
Iteration 5/25 | Loss: 0.00116283
Iteration 6/25 | Loss: 0.00116283
Iteration 7/25 | Loss: 0.00116283
Iteration 8/25 | Loss: 0.00116283
Iteration 9/25 | Loss: 0.00116283
Iteration 10/25 | Loss: 0.00116283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001162830158136785, 0.001162830158136785, 0.001162830158136785, 0.001162830158136785, 0.001162830158136785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001162830158136785

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28686404
Iteration 2/25 | Loss: 0.00115261
Iteration 3/25 | Loss: 0.00115260
Iteration 4/25 | Loss: 0.00115260
Iteration 5/25 | Loss: 0.00115260
Iteration 6/25 | Loss: 0.00115260
Iteration 7/25 | Loss: 0.00115260
Iteration 8/25 | Loss: 0.00115260
Iteration 9/25 | Loss: 0.00115260
Iteration 10/25 | Loss: 0.00115260
Iteration 11/25 | Loss: 0.00115260
Iteration 12/25 | Loss: 0.00115260
Iteration 13/25 | Loss: 0.00115260
Iteration 14/25 | Loss: 0.00115260
Iteration 15/25 | Loss: 0.00115260
Iteration 16/25 | Loss: 0.00115260
Iteration 17/25 | Loss: 0.00115260
Iteration 18/25 | Loss: 0.00115260
Iteration 19/25 | Loss: 0.00115260
Iteration 20/25 | Loss: 0.00115260
Iteration 21/25 | Loss: 0.00115260
Iteration 22/25 | Loss: 0.00115260
Iteration 23/25 | Loss: 0.00115260
Iteration 24/25 | Loss: 0.00115260
Iteration 25/25 | Loss: 0.00115260

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115260
Iteration 2/1000 | Loss: 0.00001844
Iteration 3/1000 | Loss: 0.00001458
Iteration 4/1000 | Loss: 0.00001302
Iteration 5/1000 | Loss: 0.00001214
Iteration 6/1000 | Loss: 0.00001172
Iteration 7/1000 | Loss: 0.00001126
Iteration 8/1000 | Loss: 0.00001086
Iteration 9/1000 | Loss: 0.00001076
Iteration 10/1000 | Loss: 0.00001060
Iteration 11/1000 | Loss: 0.00001059
Iteration 12/1000 | Loss: 0.00001053
Iteration 13/1000 | Loss: 0.00001049
Iteration 14/1000 | Loss: 0.00001037
Iteration 15/1000 | Loss: 0.00001030
Iteration 16/1000 | Loss: 0.00001028
Iteration 17/1000 | Loss: 0.00001027
Iteration 18/1000 | Loss: 0.00001027
Iteration 19/1000 | Loss: 0.00001027
Iteration 20/1000 | Loss: 0.00001026
Iteration 21/1000 | Loss: 0.00001026
Iteration 22/1000 | Loss: 0.00001025
Iteration 23/1000 | Loss: 0.00001025
Iteration 24/1000 | Loss: 0.00001024
Iteration 25/1000 | Loss: 0.00001024
Iteration 26/1000 | Loss: 0.00001024
Iteration 27/1000 | Loss: 0.00001022
Iteration 28/1000 | Loss: 0.00001020
Iteration 29/1000 | Loss: 0.00001020
Iteration 30/1000 | Loss: 0.00001019
Iteration 31/1000 | Loss: 0.00001018
Iteration 32/1000 | Loss: 0.00001018
Iteration 33/1000 | Loss: 0.00001017
Iteration 34/1000 | Loss: 0.00001017
Iteration 35/1000 | Loss: 0.00001016
Iteration 36/1000 | Loss: 0.00001015
Iteration 37/1000 | Loss: 0.00001014
Iteration 38/1000 | Loss: 0.00001013
Iteration 39/1000 | Loss: 0.00001012
Iteration 40/1000 | Loss: 0.00001011
Iteration 41/1000 | Loss: 0.00001011
Iteration 42/1000 | Loss: 0.00001011
Iteration 43/1000 | Loss: 0.00001010
Iteration 44/1000 | Loss: 0.00001010
Iteration 45/1000 | Loss: 0.00001009
Iteration 46/1000 | Loss: 0.00001009
Iteration 47/1000 | Loss: 0.00001008
Iteration 48/1000 | Loss: 0.00001008
Iteration 49/1000 | Loss: 0.00001007
Iteration 50/1000 | Loss: 0.00001007
Iteration 51/1000 | Loss: 0.00001006
Iteration 52/1000 | Loss: 0.00001004
Iteration 53/1000 | Loss: 0.00001004
Iteration 54/1000 | Loss: 0.00001003
Iteration 55/1000 | Loss: 0.00001003
Iteration 56/1000 | Loss: 0.00001003
Iteration 57/1000 | Loss: 0.00001002
Iteration 58/1000 | Loss: 0.00001002
Iteration 59/1000 | Loss: 0.00001002
Iteration 60/1000 | Loss: 0.00001002
Iteration 61/1000 | Loss: 0.00001002
Iteration 62/1000 | Loss: 0.00001001
Iteration 63/1000 | Loss: 0.00001001
Iteration 64/1000 | Loss: 0.00001000
Iteration 65/1000 | Loss: 0.00000999
Iteration 66/1000 | Loss: 0.00000998
Iteration 67/1000 | Loss: 0.00000997
Iteration 68/1000 | Loss: 0.00000996
Iteration 69/1000 | Loss: 0.00000995
Iteration 70/1000 | Loss: 0.00000993
Iteration 71/1000 | Loss: 0.00000992
Iteration 72/1000 | Loss: 0.00000991
Iteration 73/1000 | Loss: 0.00000991
Iteration 74/1000 | Loss: 0.00000990
Iteration 75/1000 | Loss: 0.00000990
Iteration 76/1000 | Loss: 0.00000990
Iteration 77/1000 | Loss: 0.00000989
Iteration 78/1000 | Loss: 0.00000989
Iteration 79/1000 | Loss: 0.00000989
Iteration 80/1000 | Loss: 0.00000987
Iteration 81/1000 | Loss: 0.00000987
Iteration 82/1000 | Loss: 0.00000986
Iteration 83/1000 | Loss: 0.00000986
Iteration 84/1000 | Loss: 0.00000986
Iteration 85/1000 | Loss: 0.00000985
Iteration 86/1000 | Loss: 0.00000984
Iteration 87/1000 | Loss: 0.00000984
Iteration 88/1000 | Loss: 0.00000984
Iteration 89/1000 | Loss: 0.00000984
Iteration 90/1000 | Loss: 0.00000983
Iteration 91/1000 | Loss: 0.00000982
Iteration 92/1000 | Loss: 0.00000982
Iteration 93/1000 | Loss: 0.00000981
Iteration 94/1000 | Loss: 0.00000981
Iteration 95/1000 | Loss: 0.00000981
Iteration 96/1000 | Loss: 0.00000980
Iteration 97/1000 | Loss: 0.00000980
Iteration 98/1000 | Loss: 0.00000980
Iteration 99/1000 | Loss: 0.00000980
Iteration 100/1000 | Loss: 0.00000980
Iteration 101/1000 | Loss: 0.00000980
Iteration 102/1000 | Loss: 0.00000979
Iteration 103/1000 | Loss: 0.00000979
Iteration 104/1000 | Loss: 0.00000979
Iteration 105/1000 | Loss: 0.00000979
Iteration 106/1000 | Loss: 0.00000977
Iteration 107/1000 | Loss: 0.00000977
Iteration 108/1000 | Loss: 0.00000976
Iteration 109/1000 | Loss: 0.00000976
Iteration 110/1000 | Loss: 0.00000976
Iteration 111/1000 | Loss: 0.00000975
Iteration 112/1000 | Loss: 0.00000975
Iteration 113/1000 | Loss: 0.00000975
Iteration 114/1000 | Loss: 0.00000974
Iteration 115/1000 | Loss: 0.00000974
Iteration 116/1000 | Loss: 0.00000973
Iteration 117/1000 | Loss: 0.00000973
Iteration 118/1000 | Loss: 0.00000973
Iteration 119/1000 | Loss: 0.00000973
Iteration 120/1000 | Loss: 0.00000973
Iteration 121/1000 | Loss: 0.00000973
Iteration 122/1000 | Loss: 0.00000973
Iteration 123/1000 | Loss: 0.00000973
Iteration 124/1000 | Loss: 0.00000972
Iteration 125/1000 | Loss: 0.00000972
Iteration 126/1000 | Loss: 0.00000972
Iteration 127/1000 | Loss: 0.00000971
Iteration 128/1000 | Loss: 0.00000971
Iteration 129/1000 | Loss: 0.00000971
Iteration 130/1000 | Loss: 0.00000970
Iteration 131/1000 | Loss: 0.00000970
Iteration 132/1000 | Loss: 0.00000970
Iteration 133/1000 | Loss: 0.00000970
Iteration 134/1000 | Loss: 0.00000969
Iteration 135/1000 | Loss: 0.00000969
Iteration 136/1000 | Loss: 0.00000969
Iteration 137/1000 | Loss: 0.00000969
Iteration 138/1000 | Loss: 0.00000969
Iteration 139/1000 | Loss: 0.00000969
Iteration 140/1000 | Loss: 0.00000969
Iteration 141/1000 | Loss: 0.00000969
Iteration 142/1000 | Loss: 0.00000969
Iteration 143/1000 | Loss: 0.00000969
Iteration 144/1000 | Loss: 0.00000969
Iteration 145/1000 | Loss: 0.00000969
Iteration 146/1000 | Loss: 0.00000968
Iteration 147/1000 | Loss: 0.00000967
Iteration 148/1000 | Loss: 0.00000967
Iteration 149/1000 | Loss: 0.00000967
Iteration 150/1000 | Loss: 0.00000967
Iteration 151/1000 | Loss: 0.00000967
Iteration 152/1000 | Loss: 0.00000967
Iteration 153/1000 | Loss: 0.00000967
Iteration 154/1000 | Loss: 0.00000967
Iteration 155/1000 | Loss: 0.00000967
Iteration 156/1000 | Loss: 0.00000967
Iteration 157/1000 | Loss: 0.00000967
Iteration 158/1000 | Loss: 0.00000967
Iteration 159/1000 | Loss: 0.00000967
Iteration 160/1000 | Loss: 0.00000967
Iteration 161/1000 | Loss: 0.00000966
Iteration 162/1000 | Loss: 0.00000966
Iteration 163/1000 | Loss: 0.00000966
Iteration 164/1000 | Loss: 0.00000966
Iteration 165/1000 | Loss: 0.00000966
Iteration 166/1000 | Loss: 0.00000966
Iteration 167/1000 | Loss: 0.00000966
Iteration 168/1000 | Loss: 0.00000966
Iteration 169/1000 | Loss: 0.00000965
Iteration 170/1000 | Loss: 0.00000965
Iteration 171/1000 | Loss: 0.00000965
Iteration 172/1000 | Loss: 0.00000965
Iteration 173/1000 | Loss: 0.00000965
Iteration 174/1000 | Loss: 0.00000965
Iteration 175/1000 | Loss: 0.00000965
Iteration 176/1000 | Loss: 0.00000965
Iteration 177/1000 | Loss: 0.00000965
Iteration 178/1000 | Loss: 0.00000965
Iteration 179/1000 | Loss: 0.00000965
Iteration 180/1000 | Loss: 0.00000965
Iteration 181/1000 | Loss: 0.00000965
Iteration 182/1000 | Loss: 0.00000965
Iteration 183/1000 | Loss: 0.00000964
Iteration 184/1000 | Loss: 0.00000964
Iteration 185/1000 | Loss: 0.00000964
Iteration 186/1000 | Loss: 0.00000964
Iteration 187/1000 | Loss: 0.00000964
Iteration 188/1000 | Loss: 0.00000964
Iteration 189/1000 | Loss: 0.00000964
Iteration 190/1000 | Loss: 0.00000964
Iteration 191/1000 | Loss: 0.00000963
Iteration 192/1000 | Loss: 0.00000963
Iteration 193/1000 | Loss: 0.00000963
Iteration 194/1000 | Loss: 0.00000963
Iteration 195/1000 | Loss: 0.00000963
Iteration 196/1000 | Loss: 0.00000963
Iteration 197/1000 | Loss: 0.00000963
Iteration 198/1000 | Loss: 0.00000963
Iteration 199/1000 | Loss: 0.00000963
Iteration 200/1000 | Loss: 0.00000963
Iteration 201/1000 | Loss: 0.00000963
Iteration 202/1000 | Loss: 0.00000963
Iteration 203/1000 | Loss: 0.00000963
Iteration 204/1000 | Loss: 0.00000963
Iteration 205/1000 | Loss: 0.00000963
Iteration 206/1000 | Loss: 0.00000963
Iteration 207/1000 | Loss: 0.00000963
Iteration 208/1000 | Loss: 0.00000963
Iteration 209/1000 | Loss: 0.00000963
Iteration 210/1000 | Loss: 0.00000963
Iteration 211/1000 | Loss: 0.00000963
Iteration 212/1000 | Loss: 0.00000963
Iteration 213/1000 | Loss: 0.00000963
Iteration 214/1000 | Loss: 0.00000963
Iteration 215/1000 | Loss: 0.00000963
Iteration 216/1000 | Loss: 0.00000963
Iteration 217/1000 | Loss: 0.00000963
Iteration 218/1000 | Loss: 0.00000963
Iteration 219/1000 | Loss: 0.00000963
Iteration 220/1000 | Loss: 0.00000963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [9.627941835788079e-06, 9.627941835788079e-06, 9.627941835788079e-06, 9.627941835788079e-06, 9.627941835788079e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.627941835788079e-06

Optimization complete. Final v2v error: 2.6865224838256836 mm

Highest mean error: 2.7690181732177734 mm for frame 120

Lowest mean error: 2.5700838565826416 mm for frame 223

Saving results

Total time: 45.36832046508789
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390620
Iteration 2/25 | Loss: 0.00143983
Iteration 3/25 | Loss: 0.00122424
Iteration 4/25 | Loss: 0.00119798
Iteration 5/25 | Loss: 0.00119464
Iteration 6/25 | Loss: 0.00119386
Iteration 7/25 | Loss: 0.00119386
Iteration 8/25 | Loss: 0.00119386
Iteration 9/25 | Loss: 0.00119386
Iteration 10/25 | Loss: 0.00119386
Iteration 11/25 | Loss: 0.00119386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011938597308471799, 0.0011938597308471799, 0.0011938597308471799, 0.0011938597308471799, 0.0011938597308471799]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011938597308471799

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29048932
Iteration 2/25 | Loss: 0.00145518
Iteration 3/25 | Loss: 0.00145517
Iteration 4/25 | Loss: 0.00145517
Iteration 5/25 | Loss: 0.00145517
Iteration 6/25 | Loss: 0.00145517
Iteration 7/25 | Loss: 0.00145517
Iteration 8/25 | Loss: 0.00145517
Iteration 9/25 | Loss: 0.00145517
Iteration 10/25 | Loss: 0.00145517
Iteration 11/25 | Loss: 0.00145517
Iteration 12/25 | Loss: 0.00145517
Iteration 13/25 | Loss: 0.00145517
Iteration 14/25 | Loss: 0.00145517
Iteration 15/25 | Loss: 0.00145517
Iteration 16/25 | Loss: 0.00145517
Iteration 17/25 | Loss: 0.00145517
Iteration 18/25 | Loss: 0.00145517
Iteration 19/25 | Loss: 0.00145517
Iteration 20/25 | Loss: 0.00145517
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014551735948771238, 0.0014551735948771238, 0.0014551735948771238, 0.0014551735948771238, 0.0014551735948771238]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014551735948771238

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145517
Iteration 2/1000 | Loss: 0.00002664
Iteration 3/1000 | Loss: 0.00001681
Iteration 4/1000 | Loss: 0.00001533
Iteration 5/1000 | Loss: 0.00001384
Iteration 6/1000 | Loss: 0.00001293
Iteration 7/1000 | Loss: 0.00001224
Iteration 8/1000 | Loss: 0.00001188
Iteration 9/1000 | Loss: 0.00001153
Iteration 10/1000 | Loss: 0.00001123
Iteration 11/1000 | Loss: 0.00001100
Iteration 12/1000 | Loss: 0.00001095
Iteration 13/1000 | Loss: 0.00001093
Iteration 14/1000 | Loss: 0.00001088
Iteration 15/1000 | Loss: 0.00001073
Iteration 16/1000 | Loss: 0.00001066
Iteration 17/1000 | Loss: 0.00001066
Iteration 18/1000 | Loss: 0.00001065
Iteration 19/1000 | Loss: 0.00001064
Iteration 20/1000 | Loss: 0.00001064
Iteration 21/1000 | Loss: 0.00001063
Iteration 22/1000 | Loss: 0.00001062
Iteration 23/1000 | Loss: 0.00001061
Iteration 24/1000 | Loss: 0.00001061
Iteration 25/1000 | Loss: 0.00001061
Iteration 26/1000 | Loss: 0.00001060
Iteration 27/1000 | Loss: 0.00001060
Iteration 28/1000 | Loss: 0.00001059
Iteration 29/1000 | Loss: 0.00001057
Iteration 30/1000 | Loss: 0.00001057
Iteration 31/1000 | Loss: 0.00001056
Iteration 32/1000 | Loss: 0.00001055
Iteration 33/1000 | Loss: 0.00001055
Iteration 34/1000 | Loss: 0.00001054
Iteration 35/1000 | Loss: 0.00001054
Iteration 36/1000 | Loss: 0.00001051
Iteration 37/1000 | Loss: 0.00001046
Iteration 38/1000 | Loss: 0.00001043
Iteration 39/1000 | Loss: 0.00001043
Iteration 40/1000 | Loss: 0.00001042
Iteration 41/1000 | Loss: 0.00001042
Iteration 42/1000 | Loss: 0.00001041
Iteration 43/1000 | Loss: 0.00001041
Iteration 44/1000 | Loss: 0.00001041
Iteration 45/1000 | Loss: 0.00001038
Iteration 46/1000 | Loss: 0.00001038
Iteration 47/1000 | Loss: 0.00001035
Iteration 48/1000 | Loss: 0.00001034
Iteration 49/1000 | Loss: 0.00001034
Iteration 50/1000 | Loss: 0.00001034
Iteration 51/1000 | Loss: 0.00001033
Iteration 52/1000 | Loss: 0.00001032
Iteration 53/1000 | Loss: 0.00001032
Iteration 54/1000 | Loss: 0.00001032
Iteration 55/1000 | Loss: 0.00001032
Iteration 56/1000 | Loss: 0.00001032
Iteration 57/1000 | Loss: 0.00001032
Iteration 58/1000 | Loss: 0.00001032
Iteration 59/1000 | Loss: 0.00001032
Iteration 60/1000 | Loss: 0.00001032
Iteration 61/1000 | Loss: 0.00001031
Iteration 62/1000 | Loss: 0.00001031
Iteration 63/1000 | Loss: 0.00001031
Iteration 64/1000 | Loss: 0.00001031
Iteration 65/1000 | Loss: 0.00001030
Iteration 66/1000 | Loss: 0.00001030
Iteration 67/1000 | Loss: 0.00001029
Iteration 68/1000 | Loss: 0.00001029
Iteration 69/1000 | Loss: 0.00001029
Iteration 70/1000 | Loss: 0.00001029
Iteration 71/1000 | Loss: 0.00001028
Iteration 72/1000 | Loss: 0.00001028
Iteration 73/1000 | Loss: 0.00001028
Iteration 74/1000 | Loss: 0.00001028
Iteration 75/1000 | Loss: 0.00001028
Iteration 76/1000 | Loss: 0.00001028
Iteration 77/1000 | Loss: 0.00001027
Iteration 78/1000 | Loss: 0.00001027
Iteration 79/1000 | Loss: 0.00001027
Iteration 80/1000 | Loss: 0.00001026
Iteration 81/1000 | Loss: 0.00001026
Iteration 82/1000 | Loss: 0.00001026
Iteration 83/1000 | Loss: 0.00001026
Iteration 84/1000 | Loss: 0.00001025
Iteration 85/1000 | Loss: 0.00001025
Iteration 86/1000 | Loss: 0.00001025
Iteration 87/1000 | Loss: 0.00001025
Iteration 88/1000 | Loss: 0.00001024
Iteration 89/1000 | Loss: 0.00001024
Iteration 90/1000 | Loss: 0.00001024
Iteration 91/1000 | Loss: 0.00001023
Iteration 92/1000 | Loss: 0.00001023
Iteration 93/1000 | Loss: 0.00001023
Iteration 94/1000 | Loss: 0.00001023
Iteration 95/1000 | Loss: 0.00001023
Iteration 96/1000 | Loss: 0.00001023
Iteration 97/1000 | Loss: 0.00001023
Iteration 98/1000 | Loss: 0.00001022
Iteration 99/1000 | Loss: 0.00001022
Iteration 100/1000 | Loss: 0.00001022
Iteration 101/1000 | Loss: 0.00001022
Iteration 102/1000 | Loss: 0.00001022
Iteration 103/1000 | Loss: 0.00001022
Iteration 104/1000 | Loss: 0.00001022
Iteration 105/1000 | Loss: 0.00001021
Iteration 106/1000 | Loss: 0.00001021
Iteration 107/1000 | Loss: 0.00001021
Iteration 108/1000 | Loss: 0.00001021
Iteration 109/1000 | Loss: 0.00001021
Iteration 110/1000 | Loss: 0.00001021
Iteration 111/1000 | Loss: 0.00001021
Iteration 112/1000 | Loss: 0.00001021
Iteration 113/1000 | Loss: 0.00001020
Iteration 114/1000 | Loss: 0.00001020
Iteration 115/1000 | Loss: 0.00001020
Iteration 116/1000 | Loss: 0.00001019
Iteration 117/1000 | Loss: 0.00001019
Iteration 118/1000 | Loss: 0.00001019
Iteration 119/1000 | Loss: 0.00001019
Iteration 120/1000 | Loss: 0.00001019
Iteration 121/1000 | Loss: 0.00001019
Iteration 122/1000 | Loss: 0.00001018
Iteration 123/1000 | Loss: 0.00001018
Iteration 124/1000 | Loss: 0.00001018
Iteration 125/1000 | Loss: 0.00001017
Iteration 126/1000 | Loss: 0.00001017
Iteration 127/1000 | Loss: 0.00001017
Iteration 128/1000 | Loss: 0.00001017
Iteration 129/1000 | Loss: 0.00001017
Iteration 130/1000 | Loss: 0.00001016
Iteration 131/1000 | Loss: 0.00001016
Iteration 132/1000 | Loss: 0.00001016
Iteration 133/1000 | Loss: 0.00001016
Iteration 134/1000 | Loss: 0.00001015
Iteration 135/1000 | Loss: 0.00001015
Iteration 136/1000 | Loss: 0.00001015
Iteration 137/1000 | Loss: 0.00001014
Iteration 138/1000 | Loss: 0.00001014
Iteration 139/1000 | Loss: 0.00001013
Iteration 140/1000 | Loss: 0.00001013
Iteration 141/1000 | Loss: 0.00001013
Iteration 142/1000 | Loss: 0.00001012
Iteration 143/1000 | Loss: 0.00001012
Iteration 144/1000 | Loss: 0.00001012
Iteration 145/1000 | Loss: 0.00001011
Iteration 146/1000 | Loss: 0.00001011
Iteration 147/1000 | Loss: 0.00001011
Iteration 148/1000 | Loss: 0.00001011
Iteration 149/1000 | Loss: 0.00001011
Iteration 150/1000 | Loss: 0.00001010
Iteration 151/1000 | Loss: 0.00001010
Iteration 152/1000 | Loss: 0.00001010
Iteration 153/1000 | Loss: 0.00001010
Iteration 154/1000 | Loss: 0.00001010
Iteration 155/1000 | Loss: 0.00001009
Iteration 156/1000 | Loss: 0.00001009
Iteration 157/1000 | Loss: 0.00001009
Iteration 158/1000 | Loss: 0.00001009
Iteration 159/1000 | Loss: 0.00001009
Iteration 160/1000 | Loss: 0.00001009
Iteration 161/1000 | Loss: 0.00001009
Iteration 162/1000 | Loss: 0.00001009
Iteration 163/1000 | Loss: 0.00001009
Iteration 164/1000 | Loss: 0.00001009
Iteration 165/1000 | Loss: 0.00001009
Iteration 166/1000 | Loss: 0.00001008
Iteration 167/1000 | Loss: 0.00001008
Iteration 168/1000 | Loss: 0.00001008
Iteration 169/1000 | Loss: 0.00001008
Iteration 170/1000 | Loss: 0.00001008
Iteration 171/1000 | Loss: 0.00001008
Iteration 172/1000 | Loss: 0.00001008
Iteration 173/1000 | Loss: 0.00001007
Iteration 174/1000 | Loss: 0.00001007
Iteration 175/1000 | Loss: 0.00001007
Iteration 176/1000 | Loss: 0.00001007
Iteration 177/1000 | Loss: 0.00001007
Iteration 178/1000 | Loss: 0.00001007
Iteration 179/1000 | Loss: 0.00001007
Iteration 180/1000 | Loss: 0.00001007
Iteration 181/1000 | Loss: 0.00001007
Iteration 182/1000 | Loss: 0.00001007
Iteration 183/1000 | Loss: 0.00001007
Iteration 184/1000 | Loss: 0.00001006
Iteration 185/1000 | Loss: 0.00001006
Iteration 186/1000 | Loss: 0.00001006
Iteration 187/1000 | Loss: 0.00001006
Iteration 188/1000 | Loss: 0.00001006
Iteration 189/1000 | Loss: 0.00001006
Iteration 190/1000 | Loss: 0.00001006
Iteration 191/1000 | Loss: 0.00001006
Iteration 192/1000 | Loss: 0.00001006
Iteration 193/1000 | Loss: 0.00001006
Iteration 194/1000 | Loss: 0.00001006
Iteration 195/1000 | Loss: 0.00001006
Iteration 196/1000 | Loss: 0.00001006
Iteration 197/1000 | Loss: 0.00001006
Iteration 198/1000 | Loss: 0.00001005
Iteration 199/1000 | Loss: 0.00001005
Iteration 200/1000 | Loss: 0.00001005
Iteration 201/1000 | Loss: 0.00001005
Iteration 202/1000 | Loss: 0.00001005
Iteration 203/1000 | Loss: 0.00001005
Iteration 204/1000 | Loss: 0.00001004
Iteration 205/1000 | Loss: 0.00001004
Iteration 206/1000 | Loss: 0.00001004
Iteration 207/1000 | Loss: 0.00001004
Iteration 208/1000 | Loss: 0.00001004
Iteration 209/1000 | Loss: 0.00001004
Iteration 210/1000 | Loss: 0.00001004
Iteration 211/1000 | Loss: 0.00001004
Iteration 212/1000 | Loss: 0.00001004
Iteration 213/1000 | Loss: 0.00001004
Iteration 214/1000 | Loss: 0.00001004
Iteration 215/1000 | Loss: 0.00001004
Iteration 216/1000 | Loss: 0.00001004
Iteration 217/1000 | Loss: 0.00001004
Iteration 218/1000 | Loss: 0.00001004
Iteration 219/1000 | Loss: 0.00001004
Iteration 220/1000 | Loss: 0.00001004
Iteration 221/1000 | Loss: 0.00001004
Iteration 222/1000 | Loss: 0.00001004
Iteration 223/1000 | Loss: 0.00001004
Iteration 224/1000 | Loss: 0.00001004
Iteration 225/1000 | Loss: 0.00001004
Iteration 226/1000 | Loss: 0.00001003
Iteration 227/1000 | Loss: 0.00001003
Iteration 228/1000 | Loss: 0.00001003
Iteration 229/1000 | Loss: 0.00001003
Iteration 230/1000 | Loss: 0.00001003
Iteration 231/1000 | Loss: 0.00001003
Iteration 232/1000 | Loss: 0.00001003
Iteration 233/1000 | Loss: 0.00001003
Iteration 234/1000 | Loss: 0.00001003
Iteration 235/1000 | Loss: 0.00001003
Iteration 236/1000 | Loss: 0.00001003
Iteration 237/1000 | Loss: 0.00001003
Iteration 238/1000 | Loss: 0.00001003
Iteration 239/1000 | Loss: 0.00001003
Iteration 240/1000 | Loss: 0.00001003
Iteration 241/1000 | Loss: 0.00001003
Iteration 242/1000 | Loss: 0.00001003
Iteration 243/1000 | Loss: 0.00001003
Iteration 244/1000 | Loss: 0.00001003
Iteration 245/1000 | Loss: 0.00001003
Iteration 246/1000 | Loss: 0.00001003
Iteration 247/1000 | Loss: 0.00001003
Iteration 248/1000 | Loss: 0.00001003
Iteration 249/1000 | Loss: 0.00001003
Iteration 250/1000 | Loss: 0.00001003
Iteration 251/1000 | Loss: 0.00001003
Iteration 252/1000 | Loss: 0.00001003
Iteration 253/1000 | Loss: 0.00001003
Iteration 254/1000 | Loss: 0.00001003
Iteration 255/1000 | Loss: 0.00001003
Iteration 256/1000 | Loss: 0.00001003
Iteration 257/1000 | Loss: 0.00001003
Iteration 258/1000 | Loss: 0.00001003
Iteration 259/1000 | Loss: 0.00001003
Iteration 260/1000 | Loss: 0.00001003
Iteration 261/1000 | Loss: 0.00001003
Iteration 262/1000 | Loss: 0.00001003
Iteration 263/1000 | Loss: 0.00001003
Iteration 264/1000 | Loss: 0.00001003
Iteration 265/1000 | Loss: 0.00001003
Iteration 266/1000 | Loss: 0.00001003
Iteration 267/1000 | Loss: 0.00001003
Iteration 268/1000 | Loss: 0.00001003
Iteration 269/1000 | Loss: 0.00001003
Iteration 270/1000 | Loss: 0.00001003
Iteration 271/1000 | Loss: 0.00001003
Iteration 272/1000 | Loss: 0.00001003
Iteration 273/1000 | Loss: 0.00001003
Iteration 274/1000 | Loss: 0.00001003
Iteration 275/1000 | Loss: 0.00001003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [1.0028604265244212e-05, 1.0028604265244212e-05, 1.0028604265244212e-05, 1.0028604265244212e-05, 1.0028604265244212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0028604265244212e-05

Optimization complete. Final v2v error: 2.712631940841675 mm

Highest mean error: 3.3206405639648438 mm for frame 80

Lowest mean error: 2.4771833419799805 mm for frame 202

Saving results

Total time: 51.16285419464111
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825600
Iteration 2/25 | Loss: 0.00141697
Iteration 3/25 | Loss: 0.00122454
Iteration 4/25 | Loss: 0.00119407
Iteration 5/25 | Loss: 0.00118825
Iteration 6/25 | Loss: 0.00118775
Iteration 7/25 | Loss: 0.00118775
Iteration 8/25 | Loss: 0.00118775
Iteration 9/25 | Loss: 0.00118775
Iteration 10/25 | Loss: 0.00118775
Iteration 11/25 | Loss: 0.00118775
Iteration 12/25 | Loss: 0.00118775
Iteration 13/25 | Loss: 0.00118775
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011877494398504496, 0.0011877494398504496, 0.0011877494398504496, 0.0011877494398504496, 0.0011877494398504496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011877494398504496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92136955
Iteration 2/25 | Loss: 0.00083759
Iteration 3/25 | Loss: 0.00083759
Iteration 4/25 | Loss: 0.00083758
Iteration 5/25 | Loss: 0.00083758
Iteration 6/25 | Loss: 0.00083758
Iteration 7/25 | Loss: 0.00083758
Iteration 8/25 | Loss: 0.00083758
Iteration 9/25 | Loss: 0.00083758
Iteration 10/25 | Loss: 0.00083758
Iteration 11/25 | Loss: 0.00083758
Iteration 12/25 | Loss: 0.00083758
Iteration 13/25 | Loss: 0.00083758
Iteration 14/25 | Loss: 0.00083758
Iteration 15/25 | Loss: 0.00083758
Iteration 16/25 | Loss: 0.00083758
Iteration 17/25 | Loss: 0.00083758
Iteration 18/25 | Loss: 0.00083758
Iteration 19/25 | Loss: 0.00083758
Iteration 20/25 | Loss: 0.00083758
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008375820470973849, 0.0008375820470973849, 0.0008375820470973849, 0.0008375820470973849, 0.0008375820470973849]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008375820470973849

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083758
Iteration 2/1000 | Loss: 0.00003647
Iteration 3/1000 | Loss: 0.00002859
Iteration 4/1000 | Loss: 0.00002585
Iteration 5/1000 | Loss: 0.00002433
Iteration 6/1000 | Loss: 0.00002320
Iteration 7/1000 | Loss: 0.00002247
Iteration 8/1000 | Loss: 0.00002180
Iteration 9/1000 | Loss: 0.00002128
Iteration 10/1000 | Loss: 0.00002094
Iteration 11/1000 | Loss: 0.00002065
Iteration 12/1000 | Loss: 0.00002046
Iteration 13/1000 | Loss: 0.00002028
Iteration 14/1000 | Loss: 0.00002013
Iteration 15/1000 | Loss: 0.00002012
Iteration 16/1000 | Loss: 0.00002012
Iteration 17/1000 | Loss: 0.00002011
Iteration 18/1000 | Loss: 0.00002010
Iteration 19/1000 | Loss: 0.00002008
Iteration 20/1000 | Loss: 0.00002006
Iteration 21/1000 | Loss: 0.00002003
Iteration 22/1000 | Loss: 0.00002002
Iteration 23/1000 | Loss: 0.00002002
Iteration 24/1000 | Loss: 0.00002002
Iteration 25/1000 | Loss: 0.00001998
Iteration 26/1000 | Loss: 0.00001998
Iteration 27/1000 | Loss: 0.00001997
Iteration 28/1000 | Loss: 0.00001996
Iteration 29/1000 | Loss: 0.00001994
Iteration 30/1000 | Loss: 0.00001991
Iteration 31/1000 | Loss: 0.00001991
Iteration 32/1000 | Loss: 0.00001990
Iteration 33/1000 | Loss: 0.00001989
Iteration 34/1000 | Loss: 0.00001989
Iteration 35/1000 | Loss: 0.00001989
Iteration 36/1000 | Loss: 0.00001989
Iteration 37/1000 | Loss: 0.00001989
Iteration 38/1000 | Loss: 0.00001989
Iteration 39/1000 | Loss: 0.00001989
Iteration 40/1000 | Loss: 0.00001989
Iteration 41/1000 | Loss: 0.00001989
Iteration 42/1000 | Loss: 0.00001988
Iteration 43/1000 | Loss: 0.00001988
Iteration 44/1000 | Loss: 0.00001988
Iteration 45/1000 | Loss: 0.00001988
Iteration 46/1000 | Loss: 0.00001988
Iteration 47/1000 | Loss: 0.00001987
Iteration 48/1000 | Loss: 0.00001987
Iteration 49/1000 | Loss: 0.00001987
Iteration 50/1000 | Loss: 0.00001987
Iteration 51/1000 | Loss: 0.00001987
Iteration 52/1000 | Loss: 0.00001986
Iteration 53/1000 | Loss: 0.00001986
Iteration 54/1000 | Loss: 0.00001986
Iteration 55/1000 | Loss: 0.00001986
Iteration 56/1000 | Loss: 0.00001986
Iteration 57/1000 | Loss: 0.00001986
Iteration 58/1000 | Loss: 0.00001986
Iteration 59/1000 | Loss: 0.00001985
Iteration 60/1000 | Loss: 0.00001985
Iteration 61/1000 | Loss: 0.00001985
Iteration 62/1000 | Loss: 0.00001985
Iteration 63/1000 | Loss: 0.00001985
Iteration 64/1000 | Loss: 0.00001984
Iteration 65/1000 | Loss: 0.00001984
Iteration 66/1000 | Loss: 0.00001983
Iteration 67/1000 | Loss: 0.00001983
Iteration 68/1000 | Loss: 0.00001983
Iteration 69/1000 | Loss: 0.00001982
Iteration 70/1000 | Loss: 0.00001982
Iteration 71/1000 | Loss: 0.00001982
Iteration 72/1000 | Loss: 0.00001982
Iteration 73/1000 | Loss: 0.00001981
Iteration 74/1000 | Loss: 0.00001981
Iteration 75/1000 | Loss: 0.00001981
Iteration 76/1000 | Loss: 0.00001981
Iteration 77/1000 | Loss: 0.00001980
Iteration 78/1000 | Loss: 0.00001980
Iteration 79/1000 | Loss: 0.00001979
Iteration 80/1000 | Loss: 0.00001978
Iteration 81/1000 | Loss: 0.00001978
Iteration 82/1000 | Loss: 0.00001977
Iteration 83/1000 | Loss: 0.00001975
Iteration 84/1000 | Loss: 0.00001974
Iteration 85/1000 | Loss: 0.00001972
Iteration 86/1000 | Loss: 0.00001972
Iteration 87/1000 | Loss: 0.00001972
Iteration 88/1000 | Loss: 0.00001972
Iteration 89/1000 | Loss: 0.00001972
Iteration 90/1000 | Loss: 0.00001972
Iteration 91/1000 | Loss: 0.00001972
Iteration 92/1000 | Loss: 0.00001971
Iteration 93/1000 | Loss: 0.00001971
Iteration 94/1000 | Loss: 0.00001971
Iteration 95/1000 | Loss: 0.00001971
Iteration 96/1000 | Loss: 0.00001971
Iteration 97/1000 | Loss: 0.00001970
Iteration 98/1000 | Loss: 0.00001970
Iteration 99/1000 | Loss: 0.00001970
Iteration 100/1000 | Loss: 0.00001970
Iteration 101/1000 | Loss: 0.00001970
Iteration 102/1000 | Loss: 0.00001970
Iteration 103/1000 | Loss: 0.00001970
Iteration 104/1000 | Loss: 0.00001970
Iteration 105/1000 | Loss: 0.00001970
Iteration 106/1000 | Loss: 0.00001970
Iteration 107/1000 | Loss: 0.00001970
Iteration 108/1000 | Loss: 0.00001970
Iteration 109/1000 | Loss: 0.00001969
Iteration 110/1000 | Loss: 0.00001969
Iteration 111/1000 | Loss: 0.00001969
Iteration 112/1000 | Loss: 0.00001968
Iteration 113/1000 | Loss: 0.00001968
Iteration 114/1000 | Loss: 0.00001968
Iteration 115/1000 | Loss: 0.00001968
Iteration 116/1000 | Loss: 0.00001968
Iteration 117/1000 | Loss: 0.00001967
Iteration 118/1000 | Loss: 0.00001967
Iteration 119/1000 | Loss: 0.00001967
Iteration 120/1000 | Loss: 0.00001966
Iteration 121/1000 | Loss: 0.00001966
Iteration 122/1000 | Loss: 0.00001965
Iteration 123/1000 | Loss: 0.00001965
Iteration 124/1000 | Loss: 0.00001965
Iteration 125/1000 | Loss: 0.00001965
Iteration 126/1000 | Loss: 0.00001965
Iteration 127/1000 | Loss: 0.00001965
Iteration 128/1000 | Loss: 0.00001964
Iteration 129/1000 | Loss: 0.00001964
Iteration 130/1000 | Loss: 0.00001964
Iteration 131/1000 | Loss: 0.00001964
Iteration 132/1000 | Loss: 0.00001964
Iteration 133/1000 | Loss: 0.00001964
Iteration 134/1000 | Loss: 0.00001963
Iteration 135/1000 | Loss: 0.00001963
Iteration 136/1000 | Loss: 0.00001963
Iteration 137/1000 | Loss: 0.00001963
Iteration 138/1000 | Loss: 0.00001963
Iteration 139/1000 | Loss: 0.00001963
Iteration 140/1000 | Loss: 0.00001963
Iteration 141/1000 | Loss: 0.00001963
Iteration 142/1000 | Loss: 0.00001963
Iteration 143/1000 | Loss: 0.00001963
Iteration 144/1000 | Loss: 0.00001963
Iteration 145/1000 | Loss: 0.00001963
Iteration 146/1000 | Loss: 0.00001963
Iteration 147/1000 | Loss: 0.00001963
Iteration 148/1000 | Loss: 0.00001963
Iteration 149/1000 | Loss: 0.00001963
Iteration 150/1000 | Loss: 0.00001963
Iteration 151/1000 | Loss: 0.00001963
Iteration 152/1000 | Loss: 0.00001963
Iteration 153/1000 | Loss: 0.00001963
Iteration 154/1000 | Loss: 0.00001963
Iteration 155/1000 | Loss: 0.00001963
Iteration 156/1000 | Loss: 0.00001963
Iteration 157/1000 | Loss: 0.00001963
Iteration 158/1000 | Loss: 0.00001963
Iteration 159/1000 | Loss: 0.00001962
Iteration 160/1000 | Loss: 0.00001962
Iteration 161/1000 | Loss: 0.00001962
Iteration 162/1000 | Loss: 0.00001962
Iteration 163/1000 | Loss: 0.00001962
Iteration 164/1000 | Loss: 0.00001962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.9624994820333086e-05, 1.9624994820333086e-05, 1.9624994820333086e-05, 1.9624994820333086e-05, 1.9624994820333086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9624994820333086e-05

Optimization complete. Final v2v error: 3.816192388534546 mm

Highest mean error: 4.054416656494141 mm for frame 149

Lowest mean error: 3.655397415161133 mm for frame 26

Saving results

Total time: 40.00363230705261
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484151
Iteration 2/25 | Loss: 0.00124492
Iteration 3/25 | Loss: 0.00118652
Iteration 4/25 | Loss: 0.00117465
Iteration 5/25 | Loss: 0.00117096
Iteration 6/25 | Loss: 0.00117096
Iteration 7/25 | Loss: 0.00117096
Iteration 8/25 | Loss: 0.00117096
Iteration 9/25 | Loss: 0.00117096
Iteration 10/25 | Loss: 0.00117096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011709571117535233, 0.0011709571117535233, 0.0011709571117535233, 0.0011709571117535233, 0.0011709571117535233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011709571117535233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.40672612
Iteration 2/25 | Loss: 0.00129466
Iteration 3/25 | Loss: 0.00129466
Iteration 4/25 | Loss: 0.00129466
Iteration 5/25 | Loss: 0.00129466
Iteration 6/25 | Loss: 0.00129466
Iteration 7/25 | Loss: 0.00129466
Iteration 8/25 | Loss: 0.00129466
Iteration 9/25 | Loss: 0.00129466
Iteration 10/25 | Loss: 0.00129466
Iteration 11/25 | Loss: 0.00129466
Iteration 12/25 | Loss: 0.00129466
Iteration 13/25 | Loss: 0.00129466
Iteration 14/25 | Loss: 0.00129466
Iteration 15/25 | Loss: 0.00129466
Iteration 16/25 | Loss: 0.00129466
Iteration 17/25 | Loss: 0.00129466
Iteration 18/25 | Loss: 0.00129466
Iteration 19/25 | Loss: 0.00129466
Iteration 20/25 | Loss: 0.00129466
Iteration 21/25 | Loss: 0.00129466
Iteration 22/25 | Loss: 0.00129466
Iteration 23/25 | Loss: 0.00129466
Iteration 24/25 | Loss: 0.00129466
Iteration 25/25 | Loss: 0.00129466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129466
Iteration 2/1000 | Loss: 0.00002700
Iteration 3/1000 | Loss: 0.00001770
Iteration 4/1000 | Loss: 0.00001562
Iteration 5/1000 | Loss: 0.00001434
Iteration 6/1000 | Loss: 0.00001361
Iteration 7/1000 | Loss: 0.00001298
Iteration 8/1000 | Loss: 0.00001254
Iteration 9/1000 | Loss: 0.00001206
Iteration 10/1000 | Loss: 0.00001177
Iteration 11/1000 | Loss: 0.00001154
Iteration 12/1000 | Loss: 0.00001147
Iteration 13/1000 | Loss: 0.00001146
Iteration 14/1000 | Loss: 0.00001144
Iteration 15/1000 | Loss: 0.00001143
Iteration 16/1000 | Loss: 0.00001142
Iteration 17/1000 | Loss: 0.00001123
Iteration 18/1000 | Loss: 0.00001119
Iteration 19/1000 | Loss: 0.00001118
Iteration 20/1000 | Loss: 0.00001105
Iteration 21/1000 | Loss: 0.00001101
Iteration 22/1000 | Loss: 0.00001096
Iteration 23/1000 | Loss: 0.00001094
Iteration 24/1000 | Loss: 0.00001083
Iteration 25/1000 | Loss: 0.00001078
Iteration 26/1000 | Loss: 0.00001078
Iteration 27/1000 | Loss: 0.00001077
Iteration 28/1000 | Loss: 0.00001077
Iteration 29/1000 | Loss: 0.00001076
Iteration 30/1000 | Loss: 0.00001075
Iteration 31/1000 | Loss: 0.00001075
Iteration 32/1000 | Loss: 0.00001075
Iteration 33/1000 | Loss: 0.00001074
Iteration 34/1000 | Loss: 0.00001074
Iteration 35/1000 | Loss: 0.00001074
Iteration 36/1000 | Loss: 0.00001073
Iteration 37/1000 | Loss: 0.00001073
Iteration 38/1000 | Loss: 0.00001073
Iteration 39/1000 | Loss: 0.00001073
Iteration 40/1000 | Loss: 0.00001073
Iteration 41/1000 | Loss: 0.00001072
Iteration 42/1000 | Loss: 0.00001072
Iteration 43/1000 | Loss: 0.00001072
Iteration 44/1000 | Loss: 0.00001071
Iteration 45/1000 | Loss: 0.00001071
Iteration 46/1000 | Loss: 0.00001069
Iteration 47/1000 | Loss: 0.00001069
Iteration 48/1000 | Loss: 0.00001068
Iteration 49/1000 | Loss: 0.00001068
Iteration 50/1000 | Loss: 0.00001067
Iteration 51/1000 | Loss: 0.00001067
Iteration 52/1000 | Loss: 0.00001067
Iteration 53/1000 | Loss: 0.00001067
Iteration 54/1000 | Loss: 0.00001066
Iteration 55/1000 | Loss: 0.00001066
Iteration 56/1000 | Loss: 0.00001066
Iteration 57/1000 | Loss: 0.00001066
Iteration 58/1000 | Loss: 0.00001066
Iteration 59/1000 | Loss: 0.00001065
Iteration 60/1000 | Loss: 0.00001065
Iteration 61/1000 | Loss: 0.00001064
Iteration 62/1000 | Loss: 0.00001064
Iteration 63/1000 | Loss: 0.00001064
Iteration 64/1000 | Loss: 0.00001064
Iteration 65/1000 | Loss: 0.00001063
Iteration 66/1000 | Loss: 0.00001063
Iteration 67/1000 | Loss: 0.00001062
Iteration 68/1000 | Loss: 0.00001061
Iteration 69/1000 | Loss: 0.00001061
Iteration 70/1000 | Loss: 0.00001061
Iteration 71/1000 | Loss: 0.00001060
Iteration 72/1000 | Loss: 0.00001060
Iteration 73/1000 | Loss: 0.00001060
Iteration 74/1000 | Loss: 0.00001059
Iteration 75/1000 | Loss: 0.00001059
Iteration 76/1000 | Loss: 0.00001059
Iteration 77/1000 | Loss: 0.00001058
Iteration 78/1000 | Loss: 0.00001058
Iteration 79/1000 | Loss: 0.00001058
Iteration 80/1000 | Loss: 0.00001057
Iteration 81/1000 | Loss: 0.00001057
Iteration 82/1000 | Loss: 0.00001057
Iteration 83/1000 | Loss: 0.00001057
Iteration 84/1000 | Loss: 0.00001057
Iteration 85/1000 | Loss: 0.00001056
Iteration 86/1000 | Loss: 0.00001056
Iteration 87/1000 | Loss: 0.00001056
Iteration 88/1000 | Loss: 0.00001056
Iteration 89/1000 | Loss: 0.00001056
Iteration 90/1000 | Loss: 0.00001056
Iteration 91/1000 | Loss: 0.00001056
Iteration 92/1000 | Loss: 0.00001056
Iteration 93/1000 | Loss: 0.00001055
Iteration 94/1000 | Loss: 0.00001055
Iteration 95/1000 | Loss: 0.00001055
Iteration 96/1000 | Loss: 0.00001055
Iteration 97/1000 | Loss: 0.00001055
Iteration 98/1000 | Loss: 0.00001055
Iteration 99/1000 | Loss: 0.00001055
Iteration 100/1000 | Loss: 0.00001055
Iteration 101/1000 | Loss: 0.00001054
Iteration 102/1000 | Loss: 0.00001054
Iteration 103/1000 | Loss: 0.00001054
Iteration 104/1000 | Loss: 0.00001054
Iteration 105/1000 | Loss: 0.00001054
Iteration 106/1000 | Loss: 0.00001054
Iteration 107/1000 | Loss: 0.00001054
Iteration 108/1000 | Loss: 0.00001054
Iteration 109/1000 | Loss: 0.00001054
Iteration 110/1000 | Loss: 0.00001054
Iteration 111/1000 | Loss: 0.00001054
Iteration 112/1000 | Loss: 0.00001054
Iteration 113/1000 | Loss: 0.00001054
Iteration 114/1000 | Loss: 0.00001054
Iteration 115/1000 | Loss: 0.00001054
Iteration 116/1000 | Loss: 0.00001054
Iteration 117/1000 | Loss: 0.00001054
Iteration 118/1000 | Loss: 0.00001054
Iteration 119/1000 | Loss: 0.00001054
Iteration 120/1000 | Loss: 0.00001054
Iteration 121/1000 | Loss: 0.00001054
Iteration 122/1000 | Loss: 0.00001054
Iteration 123/1000 | Loss: 0.00001054
Iteration 124/1000 | Loss: 0.00001054
Iteration 125/1000 | Loss: 0.00001054
Iteration 126/1000 | Loss: 0.00001054
Iteration 127/1000 | Loss: 0.00001054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.0541269148234278e-05, 1.0541269148234278e-05, 1.0541269148234278e-05, 1.0541269148234278e-05, 1.0541269148234278e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0541269148234278e-05

Optimization complete. Final v2v error: 2.8421757221221924 mm

Highest mean error: 3.0454695224761963 mm for frame 170

Lowest mean error: 2.7660176753997803 mm for frame 96

Saving results

Total time: 39.38094973564148
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960459
Iteration 2/25 | Loss: 0.00432242
Iteration 3/25 | Loss: 0.00286323
Iteration 4/25 | Loss: 0.00242877
Iteration 5/25 | Loss: 0.00227736
Iteration 6/25 | Loss: 0.00230855
Iteration 7/25 | Loss: 0.00209996
Iteration 8/25 | Loss: 0.00195457
Iteration 9/25 | Loss: 0.00189783
Iteration 10/25 | Loss: 0.00185848
Iteration 11/25 | Loss: 0.00179658
Iteration 12/25 | Loss: 0.00178685
Iteration 13/25 | Loss: 0.00176606
Iteration 14/25 | Loss: 0.00174800
Iteration 15/25 | Loss: 0.00174245
Iteration 16/25 | Loss: 0.00171947
Iteration 17/25 | Loss: 0.00169857
Iteration 18/25 | Loss: 0.00168900
Iteration 19/25 | Loss: 0.00167991
Iteration 20/25 | Loss: 0.00166980
Iteration 21/25 | Loss: 0.00166981
Iteration 22/25 | Loss: 0.00166202
Iteration 23/25 | Loss: 0.00166034
Iteration 24/25 | Loss: 0.00166140
Iteration 25/25 | Loss: 0.00165447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35959995
Iteration 2/25 | Loss: 0.00915650
Iteration 3/25 | Loss: 0.00531595
Iteration 4/25 | Loss: 0.00531593
Iteration 5/25 | Loss: 0.00531593
Iteration 6/25 | Loss: 0.00531592
Iteration 7/25 | Loss: 0.00531592
Iteration 8/25 | Loss: 0.00531592
Iteration 9/25 | Loss: 0.00531592
Iteration 10/25 | Loss: 0.00531592
Iteration 11/25 | Loss: 0.00531592
Iteration 12/25 | Loss: 0.00531592
Iteration 13/25 | Loss: 0.00531592
Iteration 14/25 | Loss: 0.00531592
Iteration 15/25 | Loss: 0.00531592
Iteration 16/25 | Loss: 0.00531592
Iteration 17/25 | Loss: 0.00531592
Iteration 18/25 | Loss: 0.00531592
Iteration 19/25 | Loss: 0.00531592
Iteration 20/25 | Loss: 0.00531592
Iteration 21/25 | Loss: 0.00531592
Iteration 22/25 | Loss: 0.00531592
Iteration 23/25 | Loss: 0.00531592
Iteration 24/25 | Loss: 0.00531592
Iteration 25/25 | Loss: 0.00531592

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00531592
Iteration 2/1000 | Loss: 0.00385127
Iteration 3/1000 | Loss: 0.00409648
Iteration 4/1000 | Loss: 0.00162448
Iteration 5/1000 | Loss: 0.00076800
Iteration 6/1000 | Loss: 0.00135888
Iteration 7/1000 | Loss: 0.00186330
Iteration 8/1000 | Loss: 0.00080656
Iteration 9/1000 | Loss: 0.00152161
Iteration 10/1000 | Loss: 0.00328694
Iteration 11/1000 | Loss: 0.00026279
Iteration 12/1000 | Loss: 0.00109651
Iteration 13/1000 | Loss: 0.00060887
Iteration 14/1000 | Loss: 0.00052128
Iteration 15/1000 | Loss: 0.00051722
Iteration 16/1000 | Loss: 0.00025036
Iteration 17/1000 | Loss: 0.00040181
Iteration 18/1000 | Loss: 0.00030761
Iteration 19/1000 | Loss: 0.00020307
Iteration 20/1000 | Loss: 0.00015722
Iteration 21/1000 | Loss: 0.00053673
Iteration 22/1000 | Loss: 0.00049424
Iteration 23/1000 | Loss: 0.00316784
Iteration 24/1000 | Loss: 0.00020449
Iteration 25/1000 | Loss: 0.00052369
Iteration 26/1000 | Loss: 0.00010903
Iteration 27/1000 | Loss: 0.00019397
Iteration 28/1000 | Loss: 0.00011510
Iteration 29/1000 | Loss: 0.00025226
Iteration 30/1000 | Loss: 0.00021875
Iteration 31/1000 | Loss: 0.00010538
Iteration 32/1000 | Loss: 0.00038848
Iteration 33/1000 | Loss: 0.00340180
Iteration 34/1000 | Loss: 0.00088562
Iteration 35/1000 | Loss: 0.00227725
Iteration 36/1000 | Loss: 0.00063345
Iteration 37/1000 | Loss: 0.00036149
Iteration 38/1000 | Loss: 0.00216392
Iteration 39/1000 | Loss: 0.00010675
Iteration 40/1000 | Loss: 0.00016428
Iteration 41/1000 | Loss: 0.00008597
Iteration 42/1000 | Loss: 0.00039587
Iteration 43/1000 | Loss: 0.00027763
Iteration 44/1000 | Loss: 0.00007585
Iteration 45/1000 | Loss: 0.00017281
Iteration 46/1000 | Loss: 0.00007552
Iteration 47/1000 | Loss: 0.00019660
Iteration 48/1000 | Loss: 0.00007205
Iteration 49/1000 | Loss: 0.00007085
Iteration 50/1000 | Loss: 0.00007003
Iteration 51/1000 | Loss: 0.00039936
Iteration 52/1000 | Loss: 0.00025284
Iteration 53/1000 | Loss: 0.00006911
Iteration 54/1000 | Loss: 0.00006849
Iteration 55/1000 | Loss: 0.00044407
Iteration 56/1000 | Loss: 0.00022005
Iteration 57/1000 | Loss: 0.00006762
Iteration 58/1000 | Loss: 0.00032962
Iteration 59/1000 | Loss: 0.00019242
Iteration 60/1000 | Loss: 0.00038238
Iteration 61/1000 | Loss: 0.00022322
Iteration 62/1000 | Loss: 0.00007144
Iteration 63/1000 | Loss: 0.00011391
Iteration 64/1000 | Loss: 0.00009814
Iteration 65/1000 | Loss: 0.00010380
Iteration 66/1000 | Loss: 0.00050796
Iteration 67/1000 | Loss: 0.00007283
Iteration 68/1000 | Loss: 0.00006855
Iteration 69/1000 | Loss: 0.00006609
Iteration 70/1000 | Loss: 0.00006399
Iteration 71/1000 | Loss: 0.00006285
Iteration 72/1000 | Loss: 0.00006218
Iteration 73/1000 | Loss: 0.00020391
Iteration 74/1000 | Loss: 0.00006150
Iteration 75/1000 | Loss: 0.00006101
Iteration 76/1000 | Loss: 0.00006067
Iteration 77/1000 | Loss: 0.00074007
Iteration 78/1000 | Loss: 0.00147278
Iteration 79/1000 | Loss: 0.00059550
Iteration 80/1000 | Loss: 0.00022784
Iteration 81/1000 | Loss: 0.00052757
Iteration 82/1000 | Loss: 0.00047280
Iteration 83/1000 | Loss: 0.00050895
Iteration 84/1000 | Loss: 0.00041033
Iteration 85/1000 | Loss: 0.00037754
Iteration 86/1000 | Loss: 0.00007007
Iteration 87/1000 | Loss: 0.00007054
Iteration 88/1000 | Loss: 0.00006549
Iteration 89/1000 | Loss: 0.00005455
Iteration 90/1000 | Loss: 0.00006324
Iteration 91/1000 | Loss: 0.00005016
Iteration 92/1000 | Loss: 0.00004884
Iteration 93/1000 | Loss: 0.00004812
Iteration 94/1000 | Loss: 0.00004750
Iteration 95/1000 | Loss: 0.00049612
Iteration 96/1000 | Loss: 0.00021780
Iteration 97/1000 | Loss: 0.00038891
Iteration 98/1000 | Loss: 0.00021848
Iteration 99/1000 | Loss: 0.00038529
Iteration 100/1000 | Loss: 0.00033348
Iteration 101/1000 | Loss: 0.00005990
Iteration 102/1000 | Loss: 0.00005968
Iteration 103/1000 | Loss: 0.00004987
Iteration 104/1000 | Loss: 0.00004842
Iteration 105/1000 | Loss: 0.00004781
Iteration 106/1000 | Loss: 0.00043401
Iteration 107/1000 | Loss: 0.00030468
Iteration 108/1000 | Loss: 0.00005391
Iteration 109/1000 | Loss: 0.00004834
Iteration 110/1000 | Loss: 0.00004581
Iteration 111/1000 | Loss: 0.00040146
Iteration 112/1000 | Loss: 0.00060540
Iteration 113/1000 | Loss: 0.00028580
Iteration 114/1000 | Loss: 0.00008419
Iteration 115/1000 | Loss: 0.00062164
Iteration 116/1000 | Loss: 0.00025167
Iteration 117/1000 | Loss: 0.00008759
Iteration 118/1000 | Loss: 0.00046791
Iteration 119/1000 | Loss: 0.00043588
Iteration 120/1000 | Loss: 0.00051822
Iteration 121/1000 | Loss: 0.00006250
Iteration 122/1000 | Loss: 0.00005306
Iteration 123/1000 | Loss: 0.00015336
Iteration 124/1000 | Loss: 0.00004714
Iteration 125/1000 | Loss: 0.00005511
Iteration 126/1000 | Loss: 0.00004520
Iteration 127/1000 | Loss: 0.00004379
Iteration 128/1000 | Loss: 0.00004343
Iteration 129/1000 | Loss: 0.00004340
Iteration 130/1000 | Loss: 0.00004338
Iteration 131/1000 | Loss: 0.00004325
Iteration 132/1000 | Loss: 0.00004314
Iteration 133/1000 | Loss: 0.00004310
Iteration 134/1000 | Loss: 0.00004298
Iteration 135/1000 | Loss: 0.00004298
Iteration 136/1000 | Loss: 0.00004297
Iteration 137/1000 | Loss: 0.00004294
Iteration 138/1000 | Loss: 0.00014516
Iteration 139/1000 | Loss: 0.00004655
Iteration 140/1000 | Loss: 0.00004329
Iteration 141/1000 | Loss: 0.00004265
Iteration 142/1000 | Loss: 0.00004181
Iteration 143/1000 | Loss: 0.00004142
Iteration 144/1000 | Loss: 0.00026683
Iteration 145/1000 | Loss: 0.00004966
Iteration 146/1000 | Loss: 0.00004302
Iteration 147/1000 | Loss: 0.00004215
Iteration 148/1000 | Loss: 0.00004143
Iteration 149/1000 | Loss: 0.00004096
Iteration 150/1000 | Loss: 0.00004063
Iteration 151/1000 | Loss: 0.00004047
Iteration 152/1000 | Loss: 0.00004040
Iteration 153/1000 | Loss: 0.00004030
Iteration 154/1000 | Loss: 0.00004025
Iteration 155/1000 | Loss: 0.00004017
Iteration 156/1000 | Loss: 0.00004017
Iteration 157/1000 | Loss: 0.00004016
Iteration 158/1000 | Loss: 0.00004015
Iteration 159/1000 | Loss: 0.00004015
Iteration 160/1000 | Loss: 0.00004015
Iteration 161/1000 | Loss: 0.00004014
Iteration 162/1000 | Loss: 0.00004014
Iteration 163/1000 | Loss: 0.00004014
Iteration 164/1000 | Loss: 0.00004014
Iteration 165/1000 | Loss: 0.00004013
Iteration 166/1000 | Loss: 0.00004013
Iteration 167/1000 | Loss: 0.00004012
Iteration 168/1000 | Loss: 0.00004012
Iteration 169/1000 | Loss: 0.00004012
Iteration 170/1000 | Loss: 0.00004012
Iteration 171/1000 | Loss: 0.00004012
Iteration 172/1000 | Loss: 0.00004012
Iteration 173/1000 | Loss: 0.00004012
Iteration 174/1000 | Loss: 0.00004012
Iteration 175/1000 | Loss: 0.00004012
Iteration 176/1000 | Loss: 0.00004011
Iteration 177/1000 | Loss: 0.00004011
Iteration 178/1000 | Loss: 0.00004011
Iteration 179/1000 | Loss: 0.00004011
Iteration 180/1000 | Loss: 0.00004011
Iteration 181/1000 | Loss: 0.00004011
Iteration 182/1000 | Loss: 0.00004011
Iteration 183/1000 | Loss: 0.00004011
Iteration 184/1000 | Loss: 0.00004010
Iteration 185/1000 | Loss: 0.00004010
Iteration 186/1000 | Loss: 0.00004010
Iteration 187/1000 | Loss: 0.00004010
Iteration 188/1000 | Loss: 0.00004010
Iteration 189/1000 | Loss: 0.00004010
Iteration 190/1000 | Loss: 0.00004010
Iteration 191/1000 | Loss: 0.00004010
Iteration 192/1000 | Loss: 0.00004010
Iteration 193/1000 | Loss: 0.00004010
Iteration 194/1000 | Loss: 0.00004010
Iteration 195/1000 | Loss: 0.00004010
Iteration 196/1000 | Loss: 0.00004009
Iteration 197/1000 | Loss: 0.00004009
Iteration 198/1000 | Loss: 0.00004009
Iteration 199/1000 | Loss: 0.00004009
Iteration 200/1000 | Loss: 0.00004009
Iteration 201/1000 | Loss: 0.00004009
Iteration 202/1000 | Loss: 0.00004008
Iteration 203/1000 | Loss: 0.00004008
Iteration 204/1000 | Loss: 0.00004008
Iteration 205/1000 | Loss: 0.00004008
Iteration 206/1000 | Loss: 0.00004008
Iteration 207/1000 | Loss: 0.00004008
Iteration 208/1000 | Loss: 0.00004008
Iteration 209/1000 | Loss: 0.00004008
Iteration 210/1000 | Loss: 0.00004008
Iteration 211/1000 | Loss: 0.00004008
Iteration 212/1000 | Loss: 0.00004008
Iteration 213/1000 | Loss: 0.00004008
Iteration 214/1000 | Loss: 0.00004008
Iteration 215/1000 | Loss: 0.00004008
Iteration 216/1000 | Loss: 0.00004008
Iteration 217/1000 | Loss: 0.00004008
Iteration 218/1000 | Loss: 0.00004007
Iteration 219/1000 | Loss: 0.00004007
Iteration 220/1000 | Loss: 0.00004007
Iteration 221/1000 | Loss: 0.00004007
Iteration 222/1000 | Loss: 0.00004007
Iteration 223/1000 | Loss: 0.00004007
Iteration 224/1000 | Loss: 0.00004007
Iteration 225/1000 | Loss: 0.00004007
Iteration 226/1000 | Loss: 0.00004007
Iteration 227/1000 | Loss: 0.00004007
Iteration 228/1000 | Loss: 0.00004007
Iteration 229/1000 | Loss: 0.00004007
Iteration 230/1000 | Loss: 0.00004007
Iteration 231/1000 | Loss: 0.00004007
Iteration 232/1000 | Loss: 0.00004007
Iteration 233/1000 | Loss: 0.00004007
Iteration 234/1000 | Loss: 0.00004007
Iteration 235/1000 | Loss: 0.00004007
Iteration 236/1000 | Loss: 0.00004007
Iteration 237/1000 | Loss: 0.00004007
Iteration 238/1000 | Loss: 0.00004007
Iteration 239/1000 | Loss: 0.00004007
Iteration 240/1000 | Loss: 0.00004007
Iteration 241/1000 | Loss: 0.00004007
Iteration 242/1000 | Loss: 0.00004007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [4.006756716989912e-05, 4.006756716989912e-05, 4.006756716989912e-05, 4.006756716989912e-05, 4.006756716989912e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.006756716989912e-05

Optimization complete. Final v2v error: 3.8621182441711426 mm

Highest mean error: 10.669248580932617 mm for frame 203

Lowest mean error: 2.9484896659851074 mm for frame 83

Saving results

Total time: 290.0139181613922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789770
Iteration 2/25 | Loss: 0.00145028
Iteration 3/25 | Loss: 0.00120518
Iteration 4/25 | Loss: 0.00118962
Iteration 5/25 | Loss: 0.00118446
Iteration 6/25 | Loss: 0.00118346
Iteration 7/25 | Loss: 0.00118346
Iteration 8/25 | Loss: 0.00118346
Iteration 9/25 | Loss: 0.00118346
Iteration 10/25 | Loss: 0.00118346
Iteration 11/25 | Loss: 0.00118346
Iteration 12/25 | Loss: 0.00118346
Iteration 13/25 | Loss: 0.00118346
Iteration 14/25 | Loss: 0.00118346
Iteration 15/25 | Loss: 0.00118346
Iteration 16/25 | Loss: 0.00118346
Iteration 17/25 | Loss: 0.00118346
Iteration 18/25 | Loss: 0.00118346
Iteration 19/25 | Loss: 0.00118346
Iteration 20/25 | Loss: 0.00118346
Iteration 21/25 | Loss: 0.00118346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011834576725959778, 0.0011834576725959778, 0.0011834576725959778, 0.0011834576725959778, 0.0011834576725959778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011834576725959778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06754506
Iteration 2/25 | Loss: 0.00138175
Iteration 3/25 | Loss: 0.00138175
Iteration 4/25 | Loss: 0.00138175
Iteration 5/25 | Loss: 0.00138175
Iteration 6/25 | Loss: 0.00138175
Iteration 7/25 | Loss: 0.00138175
Iteration 8/25 | Loss: 0.00138175
Iteration 9/25 | Loss: 0.00138175
Iteration 10/25 | Loss: 0.00138175
Iteration 11/25 | Loss: 0.00138175
Iteration 12/25 | Loss: 0.00138175
Iteration 13/25 | Loss: 0.00138175
Iteration 14/25 | Loss: 0.00138175
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013817482395097613, 0.0013817482395097613, 0.0013817482395097613, 0.0013817482395097613, 0.0013817482395097613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013817482395097613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138175
Iteration 2/1000 | Loss: 0.00004154
Iteration 3/1000 | Loss: 0.00002847
Iteration 4/1000 | Loss: 0.00002339
Iteration 5/1000 | Loss: 0.00002052
Iteration 6/1000 | Loss: 0.00001866
Iteration 7/1000 | Loss: 0.00001747
Iteration 8/1000 | Loss: 0.00001655
Iteration 9/1000 | Loss: 0.00001602
Iteration 10/1000 | Loss: 0.00001555
Iteration 11/1000 | Loss: 0.00001510
Iteration 12/1000 | Loss: 0.00001481
Iteration 13/1000 | Loss: 0.00001461
Iteration 14/1000 | Loss: 0.00001449
Iteration 15/1000 | Loss: 0.00001443
Iteration 16/1000 | Loss: 0.00001437
Iteration 17/1000 | Loss: 0.00001419
Iteration 18/1000 | Loss: 0.00001405
Iteration 19/1000 | Loss: 0.00001404
Iteration 20/1000 | Loss: 0.00001399
Iteration 21/1000 | Loss: 0.00001398
Iteration 22/1000 | Loss: 0.00001398
Iteration 23/1000 | Loss: 0.00001397
Iteration 24/1000 | Loss: 0.00001394
Iteration 25/1000 | Loss: 0.00001393
Iteration 26/1000 | Loss: 0.00001393
Iteration 27/1000 | Loss: 0.00001392
Iteration 28/1000 | Loss: 0.00001392
Iteration 29/1000 | Loss: 0.00001392
Iteration 30/1000 | Loss: 0.00001391
Iteration 31/1000 | Loss: 0.00001385
Iteration 32/1000 | Loss: 0.00001384
Iteration 33/1000 | Loss: 0.00001383
Iteration 34/1000 | Loss: 0.00001382
Iteration 35/1000 | Loss: 0.00001380
Iteration 36/1000 | Loss: 0.00001379
Iteration 37/1000 | Loss: 0.00001379
Iteration 38/1000 | Loss: 0.00001379
Iteration 39/1000 | Loss: 0.00001379
Iteration 40/1000 | Loss: 0.00001379
Iteration 41/1000 | Loss: 0.00001378
Iteration 42/1000 | Loss: 0.00001378
Iteration 43/1000 | Loss: 0.00001378
Iteration 44/1000 | Loss: 0.00001378
Iteration 45/1000 | Loss: 0.00001378
Iteration 46/1000 | Loss: 0.00001378
Iteration 47/1000 | Loss: 0.00001378
Iteration 48/1000 | Loss: 0.00001378
Iteration 49/1000 | Loss: 0.00001378
Iteration 50/1000 | Loss: 0.00001377
Iteration 51/1000 | Loss: 0.00001376
Iteration 52/1000 | Loss: 0.00001375
Iteration 53/1000 | Loss: 0.00001375
Iteration 54/1000 | Loss: 0.00001374
Iteration 55/1000 | Loss: 0.00001374
Iteration 56/1000 | Loss: 0.00001373
Iteration 57/1000 | Loss: 0.00001373
Iteration 58/1000 | Loss: 0.00001373
Iteration 59/1000 | Loss: 0.00001373
Iteration 60/1000 | Loss: 0.00001372
Iteration 61/1000 | Loss: 0.00001372
Iteration 62/1000 | Loss: 0.00001371
Iteration 63/1000 | Loss: 0.00001371
Iteration 64/1000 | Loss: 0.00001371
Iteration 65/1000 | Loss: 0.00001371
Iteration 66/1000 | Loss: 0.00001371
Iteration 67/1000 | Loss: 0.00001370
Iteration 68/1000 | Loss: 0.00001370
Iteration 69/1000 | Loss: 0.00001370
Iteration 70/1000 | Loss: 0.00001370
Iteration 71/1000 | Loss: 0.00001370
Iteration 72/1000 | Loss: 0.00001370
Iteration 73/1000 | Loss: 0.00001370
Iteration 74/1000 | Loss: 0.00001369
Iteration 75/1000 | Loss: 0.00001369
Iteration 76/1000 | Loss: 0.00001369
Iteration 77/1000 | Loss: 0.00001368
Iteration 78/1000 | Loss: 0.00001368
Iteration 79/1000 | Loss: 0.00001367
Iteration 80/1000 | Loss: 0.00001367
Iteration 81/1000 | Loss: 0.00001367
Iteration 82/1000 | Loss: 0.00001366
Iteration 83/1000 | Loss: 0.00001366
Iteration 84/1000 | Loss: 0.00001366
Iteration 85/1000 | Loss: 0.00001366
Iteration 86/1000 | Loss: 0.00001365
Iteration 87/1000 | Loss: 0.00001365
Iteration 88/1000 | Loss: 0.00001365
Iteration 89/1000 | Loss: 0.00001364
Iteration 90/1000 | Loss: 0.00001364
Iteration 91/1000 | Loss: 0.00001363
Iteration 92/1000 | Loss: 0.00001363
Iteration 93/1000 | Loss: 0.00001362
Iteration 94/1000 | Loss: 0.00001362
Iteration 95/1000 | Loss: 0.00001362
Iteration 96/1000 | Loss: 0.00001362
Iteration 97/1000 | Loss: 0.00001362
Iteration 98/1000 | Loss: 0.00001361
Iteration 99/1000 | Loss: 0.00001361
Iteration 100/1000 | Loss: 0.00001361
Iteration 101/1000 | Loss: 0.00001361
Iteration 102/1000 | Loss: 0.00001361
Iteration 103/1000 | Loss: 0.00001361
Iteration 104/1000 | Loss: 0.00001361
Iteration 105/1000 | Loss: 0.00001360
Iteration 106/1000 | Loss: 0.00001360
Iteration 107/1000 | Loss: 0.00001360
Iteration 108/1000 | Loss: 0.00001360
Iteration 109/1000 | Loss: 0.00001360
Iteration 110/1000 | Loss: 0.00001360
Iteration 111/1000 | Loss: 0.00001359
Iteration 112/1000 | Loss: 0.00001359
Iteration 113/1000 | Loss: 0.00001359
Iteration 114/1000 | Loss: 0.00001359
Iteration 115/1000 | Loss: 0.00001359
Iteration 116/1000 | Loss: 0.00001359
Iteration 117/1000 | Loss: 0.00001359
Iteration 118/1000 | Loss: 0.00001358
Iteration 119/1000 | Loss: 0.00001358
Iteration 120/1000 | Loss: 0.00001358
Iteration 121/1000 | Loss: 0.00001358
Iteration 122/1000 | Loss: 0.00001358
Iteration 123/1000 | Loss: 0.00001358
Iteration 124/1000 | Loss: 0.00001358
Iteration 125/1000 | Loss: 0.00001358
Iteration 126/1000 | Loss: 0.00001358
Iteration 127/1000 | Loss: 0.00001358
Iteration 128/1000 | Loss: 0.00001358
Iteration 129/1000 | Loss: 0.00001358
Iteration 130/1000 | Loss: 0.00001357
Iteration 131/1000 | Loss: 0.00001357
Iteration 132/1000 | Loss: 0.00001357
Iteration 133/1000 | Loss: 0.00001357
Iteration 134/1000 | Loss: 0.00001357
Iteration 135/1000 | Loss: 0.00001357
Iteration 136/1000 | Loss: 0.00001357
Iteration 137/1000 | Loss: 0.00001357
Iteration 138/1000 | Loss: 0.00001357
Iteration 139/1000 | Loss: 0.00001357
Iteration 140/1000 | Loss: 0.00001357
Iteration 141/1000 | Loss: 0.00001357
Iteration 142/1000 | Loss: 0.00001357
Iteration 143/1000 | Loss: 0.00001357
Iteration 144/1000 | Loss: 0.00001357
Iteration 145/1000 | Loss: 0.00001357
Iteration 146/1000 | Loss: 0.00001357
Iteration 147/1000 | Loss: 0.00001357
Iteration 148/1000 | Loss: 0.00001357
Iteration 149/1000 | Loss: 0.00001357
Iteration 150/1000 | Loss: 0.00001357
Iteration 151/1000 | Loss: 0.00001357
Iteration 152/1000 | Loss: 0.00001357
Iteration 153/1000 | Loss: 0.00001357
Iteration 154/1000 | Loss: 0.00001357
Iteration 155/1000 | Loss: 0.00001357
Iteration 156/1000 | Loss: 0.00001357
Iteration 157/1000 | Loss: 0.00001357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.3572945135820191e-05, 1.3572945135820191e-05, 1.3572945135820191e-05, 1.3572945135820191e-05, 1.3572945135820191e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3572945135820191e-05

Optimization complete. Final v2v error: 3.0422728061676025 mm

Highest mean error: 4.1435112953186035 mm for frame 83

Lowest mean error: 2.3810698986053467 mm for frame 196

Saving results

Total time: 48.392624378204346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847178
Iteration 2/25 | Loss: 0.00183837
Iteration 3/25 | Loss: 0.00140734
Iteration 4/25 | Loss: 0.00135218
Iteration 5/25 | Loss: 0.00134825
Iteration 6/25 | Loss: 0.00134805
Iteration 7/25 | Loss: 0.00134799
Iteration 8/25 | Loss: 0.00134799
Iteration 9/25 | Loss: 0.00134799
Iteration 10/25 | Loss: 0.00134799
Iteration 11/25 | Loss: 0.00134799
Iteration 12/25 | Loss: 0.00134799
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013479916378855705, 0.0013479916378855705, 0.0013479916378855705, 0.0013479916378855705, 0.0013479916378855705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013479916378855705

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12195444
Iteration 2/25 | Loss: 0.00100215
Iteration 3/25 | Loss: 0.00100215
Iteration 4/25 | Loss: 0.00100215
Iteration 5/25 | Loss: 0.00100215
Iteration 6/25 | Loss: 0.00100215
Iteration 7/25 | Loss: 0.00100215
Iteration 8/25 | Loss: 0.00100215
Iteration 9/25 | Loss: 0.00100215
Iteration 10/25 | Loss: 0.00100215
Iteration 11/25 | Loss: 0.00100215
Iteration 12/25 | Loss: 0.00100215
Iteration 13/25 | Loss: 0.00100215
Iteration 14/25 | Loss: 0.00100215
Iteration 15/25 | Loss: 0.00100215
Iteration 16/25 | Loss: 0.00100215
Iteration 17/25 | Loss: 0.00100215
Iteration 18/25 | Loss: 0.00100215
Iteration 19/25 | Loss: 0.00100215
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010021469788625836, 0.0010021469788625836, 0.0010021469788625836, 0.0010021469788625836, 0.0010021469788625836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010021469788625836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100215
Iteration 2/1000 | Loss: 0.00004975
Iteration 3/1000 | Loss: 0.00003016
Iteration 4/1000 | Loss: 0.00002533
Iteration 5/1000 | Loss: 0.00002299
Iteration 6/1000 | Loss: 0.00002207
Iteration 7/1000 | Loss: 0.00002140
Iteration 8/1000 | Loss: 0.00002106
Iteration 9/1000 | Loss: 0.00002062
Iteration 10/1000 | Loss: 0.00002040
Iteration 11/1000 | Loss: 0.00002031
Iteration 12/1000 | Loss: 0.00002006
Iteration 13/1000 | Loss: 0.00001987
Iteration 14/1000 | Loss: 0.00001970
Iteration 15/1000 | Loss: 0.00001967
Iteration 16/1000 | Loss: 0.00001965
Iteration 17/1000 | Loss: 0.00001964
Iteration 18/1000 | Loss: 0.00001964
Iteration 19/1000 | Loss: 0.00001963
Iteration 20/1000 | Loss: 0.00001963
Iteration 21/1000 | Loss: 0.00001961
Iteration 22/1000 | Loss: 0.00001957
Iteration 23/1000 | Loss: 0.00001956
Iteration 24/1000 | Loss: 0.00001953
Iteration 25/1000 | Loss: 0.00001947
Iteration 26/1000 | Loss: 0.00001947
Iteration 27/1000 | Loss: 0.00001946
Iteration 28/1000 | Loss: 0.00001945
Iteration 29/1000 | Loss: 0.00001944
Iteration 30/1000 | Loss: 0.00001942
Iteration 31/1000 | Loss: 0.00001942
Iteration 32/1000 | Loss: 0.00001941
Iteration 33/1000 | Loss: 0.00001941
Iteration 34/1000 | Loss: 0.00001941
Iteration 35/1000 | Loss: 0.00001940
Iteration 36/1000 | Loss: 0.00001940
Iteration 37/1000 | Loss: 0.00001940
Iteration 38/1000 | Loss: 0.00001940
Iteration 39/1000 | Loss: 0.00001940
Iteration 40/1000 | Loss: 0.00001940
Iteration 41/1000 | Loss: 0.00001940
Iteration 42/1000 | Loss: 0.00001940
Iteration 43/1000 | Loss: 0.00001939
Iteration 44/1000 | Loss: 0.00001939
Iteration 45/1000 | Loss: 0.00001939
Iteration 46/1000 | Loss: 0.00001936
Iteration 47/1000 | Loss: 0.00001936
Iteration 48/1000 | Loss: 0.00001936
Iteration 49/1000 | Loss: 0.00001936
Iteration 50/1000 | Loss: 0.00001936
Iteration 51/1000 | Loss: 0.00001936
Iteration 52/1000 | Loss: 0.00001936
Iteration 53/1000 | Loss: 0.00001935
Iteration 54/1000 | Loss: 0.00001932
Iteration 55/1000 | Loss: 0.00001931
Iteration 56/1000 | Loss: 0.00001929
Iteration 57/1000 | Loss: 0.00001929
Iteration 58/1000 | Loss: 0.00001928
Iteration 59/1000 | Loss: 0.00001928
Iteration 60/1000 | Loss: 0.00001927
Iteration 61/1000 | Loss: 0.00001927
Iteration 62/1000 | Loss: 0.00001927
Iteration 63/1000 | Loss: 0.00001927
Iteration 64/1000 | Loss: 0.00001926
Iteration 65/1000 | Loss: 0.00001926
Iteration 66/1000 | Loss: 0.00001926
Iteration 67/1000 | Loss: 0.00001926
Iteration 68/1000 | Loss: 0.00001926
Iteration 69/1000 | Loss: 0.00001926
Iteration 70/1000 | Loss: 0.00001926
Iteration 71/1000 | Loss: 0.00001926
Iteration 72/1000 | Loss: 0.00001926
Iteration 73/1000 | Loss: 0.00001925
Iteration 74/1000 | Loss: 0.00001925
Iteration 75/1000 | Loss: 0.00001925
Iteration 76/1000 | Loss: 0.00001925
Iteration 77/1000 | Loss: 0.00001924
Iteration 78/1000 | Loss: 0.00001924
Iteration 79/1000 | Loss: 0.00001923
Iteration 80/1000 | Loss: 0.00001923
Iteration 81/1000 | Loss: 0.00001923
Iteration 82/1000 | Loss: 0.00001922
Iteration 83/1000 | Loss: 0.00001922
Iteration 84/1000 | Loss: 0.00001922
Iteration 85/1000 | Loss: 0.00001922
Iteration 86/1000 | Loss: 0.00001922
Iteration 87/1000 | Loss: 0.00001922
Iteration 88/1000 | Loss: 0.00001921
Iteration 89/1000 | Loss: 0.00001921
Iteration 90/1000 | Loss: 0.00001921
Iteration 91/1000 | Loss: 0.00001921
Iteration 92/1000 | Loss: 0.00001921
Iteration 93/1000 | Loss: 0.00001921
Iteration 94/1000 | Loss: 0.00001920
Iteration 95/1000 | Loss: 0.00001920
Iteration 96/1000 | Loss: 0.00001920
Iteration 97/1000 | Loss: 0.00001920
Iteration 98/1000 | Loss: 0.00001920
Iteration 99/1000 | Loss: 0.00001920
Iteration 100/1000 | Loss: 0.00001920
Iteration 101/1000 | Loss: 0.00001919
Iteration 102/1000 | Loss: 0.00001919
Iteration 103/1000 | Loss: 0.00001919
Iteration 104/1000 | Loss: 0.00001918
Iteration 105/1000 | Loss: 0.00001918
Iteration 106/1000 | Loss: 0.00001918
Iteration 107/1000 | Loss: 0.00001918
Iteration 108/1000 | Loss: 0.00001918
Iteration 109/1000 | Loss: 0.00001918
Iteration 110/1000 | Loss: 0.00001918
Iteration 111/1000 | Loss: 0.00001918
Iteration 112/1000 | Loss: 0.00001917
Iteration 113/1000 | Loss: 0.00001917
Iteration 114/1000 | Loss: 0.00001917
Iteration 115/1000 | Loss: 0.00001917
Iteration 116/1000 | Loss: 0.00001917
Iteration 117/1000 | Loss: 0.00001917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.9174984117853455e-05, 1.9174984117853455e-05, 1.9174984117853455e-05, 1.9174984117853455e-05, 1.9174984117853455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9174984117853455e-05

Optimization complete. Final v2v error: 3.6942331790924072 mm

Highest mean error: 3.87733793258667 mm for frame 66

Lowest mean error: 3.446282148361206 mm for frame 34

Saving results

Total time: 34.08566665649414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444810
Iteration 2/25 | Loss: 0.00133076
Iteration 3/25 | Loss: 0.00123197
Iteration 4/25 | Loss: 0.00121605
Iteration 5/25 | Loss: 0.00121130
Iteration 6/25 | Loss: 0.00121041
Iteration 7/25 | Loss: 0.00121027
Iteration 8/25 | Loss: 0.00121027
Iteration 9/25 | Loss: 0.00121027
Iteration 10/25 | Loss: 0.00121027
Iteration 11/25 | Loss: 0.00121027
Iteration 12/25 | Loss: 0.00121027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012102730106562376, 0.0012102730106562376, 0.0012102730106562376, 0.0012102730106562376, 0.0012102730106562376]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012102730106562376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28840053
Iteration 2/25 | Loss: 0.00135405
Iteration 3/25 | Loss: 0.00135405
Iteration 4/25 | Loss: 0.00135405
Iteration 5/25 | Loss: 0.00135405
Iteration 6/25 | Loss: 0.00135405
Iteration 7/25 | Loss: 0.00135404
Iteration 8/25 | Loss: 0.00135404
Iteration 9/25 | Loss: 0.00135404
Iteration 10/25 | Loss: 0.00135404
Iteration 11/25 | Loss: 0.00135404
Iteration 12/25 | Loss: 0.00135404
Iteration 13/25 | Loss: 0.00135404
Iteration 14/25 | Loss: 0.00135404
Iteration 15/25 | Loss: 0.00135404
Iteration 16/25 | Loss: 0.00135404
Iteration 17/25 | Loss: 0.00135404
Iteration 18/25 | Loss: 0.00135404
Iteration 19/25 | Loss: 0.00135404
Iteration 20/25 | Loss: 0.00135404
Iteration 21/25 | Loss: 0.00135404
Iteration 22/25 | Loss: 0.00135404
Iteration 23/25 | Loss: 0.00135404
Iteration 24/25 | Loss: 0.00135404
Iteration 25/25 | Loss: 0.00135404

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135404
Iteration 2/1000 | Loss: 0.00003202
Iteration 3/1000 | Loss: 0.00002458
Iteration 4/1000 | Loss: 0.00002167
Iteration 5/1000 | Loss: 0.00002048
Iteration 6/1000 | Loss: 0.00001989
Iteration 7/1000 | Loss: 0.00001931
Iteration 8/1000 | Loss: 0.00001886
Iteration 9/1000 | Loss: 0.00001848
Iteration 10/1000 | Loss: 0.00001822
Iteration 11/1000 | Loss: 0.00001791
Iteration 12/1000 | Loss: 0.00001781
Iteration 13/1000 | Loss: 0.00001780
Iteration 14/1000 | Loss: 0.00001779
Iteration 15/1000 | Loss: 0.00001773
Iteration 16/1000 | Loss: 0.00001752
Iteration 17/1000 | Loss: 0.00001745
Iteration 18/1000 | Loss: 0.00001745
Iteration 19/1000 | Loss: 0.00001744
Iteration 20/1000 | Loss: 0.00001742
Iteration 21/1000 | Loss: 0.00001741
Iteration 22/1000 | Loss: 0.00001740
Iteration 23/1000 | Loss: 0.00001736
Iteration 24/1000 | Loss: 0.00001727
Iteration 25/1000 | Loss: 0.00001727
Iteration 26/1000 | Loss: 0.00001713
Iteration 27/1000 | Loss: 0.00001711
Iteration 28/1000 | Loss: 0.00001710
Iteration 29/1000 | Loss: 0.00001710
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001709
Iteration 32/1000 | Loss: 0.00001707
Iteration 33/1000 | Loss: 0.00001707
Iteration 34/1000 | Loss: 0.00001707
Iteration 35/1000 | Loss: 0.00001706
Iteration 36/1000 | Loss: 0.00001706
Iteration 37/1000 | Loss: 0.00001706
Iteration 38/1000 | Loss: 0.00001705
Iteration 39/1000 | Loss: 0.00001705
Iteration 40/1000 | Loss: 0.00001704
Iteration 41/1000 | Loss: 0.00001702
Iteration 42/1000 | Loss: 0.00001702
Iteration 43/1000 | Loss: 0.00001702
Iteration 44/1000 | Loss: 0.00001702
Iteration 45/1000 | Loss: 0.00001699
Iteration 46/1000 | Loss: 0.00001698
Iteration 47/1000 | Loss: 0.00001697
Iteration 48/1000 | Loss: 0.00001697
Iteration 49/1000 | Loss: 0.00001697
Iteration 50/1000 | Loss: 0.00001696
Iteration 51/1000 | Loss: 0.00001696
Iteration 52/1000 | Loss: 0.00001696
Iteration 53/1000 | Loss: 0.00001696
Iteration 54/1000 | Loss: 0.00001695
Iteration 55/1000 | Loss: 0.00001695
Iteration 56/1000 | Loss: 0.00001695
Iteration 57/1000 | Loss: 0.00001695
Iteration 58/1000 | Loss: 0.00001694
Iteration 59/1000 | Loss: 0.00001694
Iteration 60/1000 | Loss: 0.00001694
Iteration 61/1000 | Loss: 0.00001693
Iteration 62/1000 | Loss: 0.00001693
Iteration 63/1000 | Loss: 0.00001693
Iteration 64/1000 | Loss: 0.00001693
Iteration 65/1000 | Loss: 0.00001692
Iteration 66/1000 | Loss: 0.00001692
Iteration 67/1000 | Loss: 0.00001692
Iteration 68/1000 | Loss: 0.00001692
Iteration 69/1000 | Loss: 0.00001692
Iteration 70/1000 | Loss: 0.00001692
Iteration 71/1000 | Loss: 0.00001692
Iteration 72/1000 | Loss: 0.00001692
Iteration 73/1000 | Loss: 0.00001691
Iteration 74/1000 | Loss: 0.00001691
Iteration 75/1000 | Loss: 0.00001691
Iteration 76/1000 | Loss: 0.00001691
Iteration 77/1000 | Loss: 0.00001691
Iteration 78/1000 | Loss: 0.00001691
Iteration 79/1000 | Loss: 0.00001691
Iteration 80/1000 | Loss: 0.00001691
Iteration 81/1000 | Loss: 0.00001691
Iteration 82/1000 | Loss: 0.00001690
Iteration 83/1000 | Loss: 0.00001690
Iteration 84/1000 | Loss: 0.00001690
Iteration 85/1000 | Loss: 0.00001690
Iteration 86/1000 | Loss: 0.00001690
Iteration 87/1000 | Loss: 0.00001689
Iteration 88/1000 | Loss: 0.00001689
Iteration 89/1000 | Loss: 0.00001689
Iteration 90/1000 | Loss: 0.00001689
Iteration 91/1000 | Loss: 0.00001689
Iteration 92/1000 | Loss: 0.00001689
Iteration 93/1000 | Loss: 0.00001689
Iteration 94/1000 | Loss: 0.00001689
Iteration 95/1000 | Loss: 0.00001689
Iteration 96/1000 | Loss: 0.00001689
Iteration 97/1000 | Loss: 0.00001689
Iteration 98/1000 | Loss: 0.00001688
Iteration 99/1000 | Loss: 0.00001688
Iteration 100/1000 | Loss: 0.00001688
Iteration 101/1000 | Loss: 0.00001688
Iteration 102/1000 | Loss: 0.00001687
Iteration 103/1000 | Loss: 0.00001687
Iteration 104/1000 | Loss: 0.00001687
Iteration 105/1000 | Loss: 0.00001687
Iteration 106/1000 | Loss: 0.00001687
Iteration 107/1000 | Loss: 0.00001687
Iteration 108/1000 | Loss: 0.00001687
Iteration 109/1000 | Loss: 0.00001687
Iteration 110/1000 | Loss: 0.00001686
Iteration 111/1000 | Loss: 0.00001686
Iteration 112/1000 | Loss: 0.00001686
Iteration 113/1000 | Loss: 0.00001686
Iteration 114/1000 | Loss: 0.00001685
Iteration 115/1000 | Loss: 0.00001685
Iteration 116/1000 | Loss: 0.00001685
Iteration 117/1000 | Loss: 0.00001685
Iteration 118/1000 | Loss: 0.00001685
Iteration 119/1000 | Loss: 0.00001685
Iteration 120/1000 | Loss: 0.00001685
Iteration 121/1000 | Loss: 0.00001685
Iteration 122/1000 | Loss: 0.00001685
Iteration 123/1000 | Loss: 0.00001685
Iteration 124/1000 | Loss: 0.00001684
Iteration 125/1000 | Loss: 0.00001684
Iteration 126/1000 | Loss: 0.00001684
Iteration 127/1000 | Loss: 0.00001684
Iteration 128/1000 | Loss: 0.00001684
Iteration 129/1000 | Loss: 0.00001684
Iteration 130/1000 | Loss: 0.00001684
Iteration 131/1000 | Loss: 0.00001684
Iteration 132/1000 | Loss: 0.00001684
Iteration 133/1000 | Loss: 0.00001684
Iteration 134/1000 | Loss: 0.00001684
Iteration 135/1000 | Loss: 0.00001684
Iteration 136/1000 | Loss: 0.00001684
Iteration 137/1000 | Loss: 0.00001684
Iteration 138/1000 | Loss: 0.00001684
Iteration 139/1000 | Loss: 0.00001684
Iteration 140/1000 | Loss: 0.00001684
Iteration 141/1000 | Loss: 0.00001684
Iteration 142/1000 | Loss: 0.00001684
Iteration 143/1000 | Loss: 0.00001684
Iteration 144/1000 | Loss: 0.00001684
Iteration 145/1000 | Loss: 0.00001684
Iteration 146/1000 | Loss: 0.00001684
Iteration 147/1000 | Loss: 0.00001684
Iteration 148/1000 | Loss: 0.00001684
Iteration 149/1000 | Loss: 0.00001684
Iteration 150/1000 | Loss: 0.00001684
Iteration 151/1000 | Loss: 0.00001684
Iteration 152/1000 | Loss: 0.00001684
Iteration 153/1000 | Loss: 0.00001684
Iteration 154/1000 | Loss: 0.00001684
Iteration 155/1000 | Loss: 0.00001684
Iteration 156/1000 | Loss: 0.00001684
Iteration 157/1000 | Loss: 0.00001684
Iteration 158/1000 | Loss: 0.00001684
Iteration 159/1000 | Loss: 0.00001684
Iteration 160/1000 | Loss: 0.00001684
Iteration 161/1000 | Loss: 0.00001684
Iteration 162/1000 | Loss: 0.00001684
Iteration 163/1000 | Loss: 0.00001684
Iteration 164/1000 | Loss: 0.00001684
Iteration 165/1000 | Loss: 0.00001684
Iteration 166/1000 | Loss: 0.00001684
Iteration 167/1000 | Loss: 0.00001684
Iteration 168/1000 | Loss: 0.00001684
Iteration 169/1000 | Loss: 0.00001684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.683865957602393e-05, 1.683865957602393e-05, 1.683865957602393e-05, 1.683865957602393e-05, 1.683865957602393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.683865957602393e-05

Optimization complete. Final v2v error: 3.511139392852783 mm

Highest mean error: 4.222672939300537 mm for frame 30

Lowest mean error: 3.3559131622314453 mm for frame 2

Saving results

Total time: 39.92774486541748
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974959
Iteration 2/25 | Loss: 0.00362997
Iteration 3/25 | Loss: 0.00251088
Iteration 4/25 | Loss: 0.00198756
Iteration 5/25 | Loss: 0.00199657
Iteration 6/25 | Loss: 0.00180268
Iteration 7/25 | Loss: 0.00171546
Iteration 8/25 | Loss: 0.00170407
Iteration 9/25 | Loss: 0.00161777
Iteration 10/25 | Loss: 0.00155907
Iteration 11/25 | Loss: 0.00154741
Iteration 12/25 | Loss: 0.00154201
Iteration 13/25 | Loss: 0.00153858
Iteration 14/25 | Loss: 0.00153720
Iteration 15/25 | Loss: 0.00153669
Iteration 16/25 | Loss: 0.00153656
Iteration 17/25 | Loss: 0.00153655
Iteration 18/25 | Loss: 0.00153655
Iteration 19/25 | Loss: 0.00153655
Iteration 20/25 | Loss: 0.00153654
Iteration 21/25 | Loss: 0.00153654
Iteration 22/25 | Loss: 0.00153654
Iteration 23/25 | Loss: 0.00153654
Iteration 24/25 | Loss: 0.00153654
Iteration 25/25 | Loss: 0.00153654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24912059
Iteration 2/25 | Loss: 0.00338948
Iteration 3/25 | Loss: 0.00338948
Iteration 4/25 | Loss: 0.00338948
Iteration 5/25 | Loss: 0.00338948
Iteration 6/25 | Loss: 0.00338948
Iteration 7/25 | Loss: 0.00338948
Iteration 8/25 | Loss: 0.00338948
Iteration 9/25 | Loss: 0.00338948
Iteration 10/25 | Loss: 0.00338948
Iteration 11/25 | Loss: 0.00338948
Iteration 12/25 | Loss: 0.00338948
Iteration 13/25 | Loss: 0.00338948
Iteration 14/25 | Loss: 0.00338948
Iteration 15/25 | Loss: 0.00338948
Iteration 16/25 | Loss: 0.00338948
Iteration 17/25 | Loss: 0.00338948
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0033894770313054323, 0.0033894770313054323, 0.0033894770313054323, 0.0033894770313054323, 0.0033894770313054323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033894770313054323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00338948
Iteration 2/1000 | Loss: 0.00042394
Iteration 3/1000 | Loss: 0.00031681
Iteration 4/1000 | Loss: 0.00045729
Iteration 5/1000 | Loss: 0.00027931
Iteration 6/1000 | Loss: 0.00026173
Iteration 7/1000 | Loss: 0.00022572
Iteration 8/1000 | Loss: 0.00016804
Iteration 9/1000 | Loss: 0.00015937
Iteration 10/1000 | Loss: 0.00280343
Iteration 11/1000 | Loss: 0.00078959
Iteration 12/1000 | Loss: 0.00183281
Iteration 13/1000 | Loss: 0.00733695
Iteration 14/1000 | Loss: 0.00609728
Iteration 15/1000 | Loss: 0.00579658
Iteration 16/1000 | Loss: 0.00291970
Iteration 17/1000 | Loss: 0.00192093
Iteration 18/1000 | Loss: 0.00109655
Iteration 19/1000 | Loss: 0.00022211
Iteration 20/1000 | Loss: 0.00077835
Iteration 21/1000 | Loss: 0.00014789
Iteration 22/1000 | Loss: 0.00071267
Iteration 23/1000 | Loss: 0.00015022
Iteration 24/1000 | Loss: 0.00010654
Iteration 25/1000 | Loss: 0.00073344
Iteration 26/1000 | Loss: 0.00013073
Iteration 27/1000 | Loss: 0.00024482
Iteration 28/1000 | Loss: 0.00017740
Iteration 29/1000 | Loss: 0.00017781
Iteration 30/1000 | Loss: 0.00019686
Iteration 31/1000 | Loss: 0.00010911
Iteration 32/1000 | Loss: 0.00008407
Iteration 33/1000 | Loss: 0.00024273
Iteration 34/1000 | Loss: 0.00011925
Iteration 35/1000 | Loss: 0.00007937
Iteration 36/1000 | Loss: 0.00007616
Iteration 37/1000 | Loss: 0.00007261
Iteration 38/1000 | Loss: 0.00020272
Iteration 39/1000 | Loss: 0.00007031
Iteration 40/1000 | Loss: 0.00009013
Iteration 41/1000 | Loss: 0.00006653
Iteration 42/1000 | Loss: 0.00006499
Iteration 43/1000 | Loss: 0.00006388
Iteration 44/1000 | Loss: 0.00006321
Iteration 45/1000 | Loss: 0.00006271
Iteration 46/1000 | Loss: 0.00006231
Iteration 47/1000 | Loss: 0.00006186
Iteration 48/1000 | Loss: 0.00006156
Iteration 49/1000 | Loss: 0.00006125
Iteration 50/1000 | Loss: 0.00006108
Iteration 51/1000 | Loss: 0.00006096
Iteration 52/1000 | Loss: 0.00008404
Iteration 53/1000 | Loss: 0.00006466
Iteration 54/1000 | Loss: 0.00006082
Iteration 55/1000 | Loss: 0.00006076
Iteration 56/1000 | Loss: 0.00006076
Iteration 57/1000 | Loss: 0.00006076
Iteration 58/1000 | Loss: 0.00006076
Iteration 59/1000 | Loss: 0.00006072
Iteration 60/1000 | Loss: 0.00006068
Iteration 61/1000 | Loss: 0.00006062
Iteration 62/1000 | Loss: 0.00006062
Iteration 63/1000 | Loss: 0.00006062
Iteration 64/1000 | Loss: 0.00006061
Iteration 65/1000 | Loss: 0.00006061
Iteration 66/1000 | Loss: 0.00006061
Iteration 67/1000 | Loss: 0.00006060
Iteration 68/1000 | Loss: 0.00006060
Iteration 69/1000 | Loss: 0.00006059
Iteration 70/1000 | Loss: 0.00006058
Iteration 71/1000 | Loss: 0.00006058
Iteration 72/1000 | Loss: 0.00006057
Iteration 73/1000 | Loss: 0.00006057
Iteration 74/1000 | Loss: 0.00006056
Iteration 75/1000 | Loss: 0.00006056
Iteration 76/1000 | Loss: 0.00006056
Iteration 77/1000 | Loss: 0.00006055
Iteration 78/1000 | Loss: 0.00006055
Iteration 79/1000 | Loss: 0.00006055
Iteration 80/1000 | Loss: 0.00006055
Iteration 81/1000 | Loss: 0.00006055
Iteration 82/1000 | Loss: 0.00006055
Iteration 83/1000 | Loss: 0.00006055
Iteration 84/1000 | Loss: 0.00006055
Iteration 85/1000 | Loss: 0.00006055
Iteration 86/1000 | Loss: 0.00006055
Iteration 87/1000 | Loss: 0.00006055
Iteration 88/1000 | Loss: 0.00006054
Iteration 89/1000 | Loss: 0.00006054
Iteration 90/1000 | Loss: 0.00006054
Iteration 91/1000 | Loss: 0.00006053
Iteration 92/1000 | Loss: 0.00006053
Iteration 93/1000 | Loss: 0.00006053
Iteration 94/1000 | Loss: 0.00006053
Iteration 95/1000 | Loss: 0.00006053
Iteration 96/1000 | Loss: 0.00006052
Iteration 97/1000 | Loss: 0.00006052
Iteration 98/1000 | Loss: 0.00006052
Iteration 99/1000 | Loss: 0.00006051
Iteration 100/1000 | Loss: 0.00006051
Iteration 101/1000 | Loss: 0.00006051
Iteration 102/1000 | Loss: 0.00006051
Iteration 103/1000 | Loss: 0.00006051
Iteration 104/1000 | Loss: 0.00006051
Iteration 105/1000 | Loss: 0.00006050
Iteration 106/1000 | Loss: 0.00006050
Iteration 107/1000 | Loss: 0.00006050
Iteration 108/1000 | Loss: 0.00006050
Iteration 109/1000 | Loss: 0.00006050
Iteration 110/1000 | Loss: 0.00006050
Iteration 111/1000 | Loss: 0.00006049
Iteration 112/1000 | Loss: 0.00006049
Iteration 113/1000 | Loss: 0.00006049
Iteration 114/1000 | Loss: 0.00006049
Iteration 115/1000 | Loss: 0.00006049
Iteration 116/1000 | Loss: 0.00006049
Iteration 117/1000 | Loss: 0.00006049
Iteration 118/1000 | Loss: 0.00006049
Iteration 119/1000 | Loss: 0.00006049
Iteration 120/1000 | Loss: 0.00006049
Iteration 121/1000 | Loss: 0.00006049
Iteration 122/1000 | Loss: 0.00006049
Iteration 123/1000 | Loss: 0.00006049
Iteration 124/1000 | Loss: 0.00006048
Iteration 125/1000 | Loss: 0.00006048
Iteration 126/1000 | Loss: 0.00006048
Iteration 127/1000 | Loss: 0.00006048
Iteration 128/1000 | Loss: 0.00006048
Iteration 129/1000 | Loss: 0.00006048
Iteration 130/1000 | Loss: 0.00006048
Iteration 131/1000 | Loss: 0.00006048
Iteration 132/1000 | Loss: 0.00006048
Iteration 133/1000 | Loss: 0.00006048
Iteration 134/1000 | Loss: 0.00006048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [6.048108116374351e-05, 6.048108116374351e-05, 6.048108116374351e-05, 6.048108116374351e-05, 6.048108116374351e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.048108116374351e-05

Optimization complete. Final v2v error: 4.565659999847412 mm

Highest mean error: 10.961795806884766 mm for frame 110

Lowest mean error: 3.36963152885437 mm for frame 8

Saving results

Total time: 121.45063304901123
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400344
Iteration 2/25 | Loss: 0.00123104
Iteration 3/25 | Loss: 0.00116752
Iteration 4/25 | Loss: 0.00115646
Iteration 5/25 | Loss: 0.00115280
Iteration 6/25 | Loss: 0.00115207
Iteration 7/25 | Loss: 0.00115207
Iteration 8/25 | Loss: 0.00115207
Iteration 9/25 | Loss: 0.00115205
Iteration 10/25 | Loss: 0.00115205
Iteration 11/25 | Loss: 0.00115205
Iteration 12/25 | Loss: 0.00115205
Iteration 13/25 | Loss: 0.00115205
Iteration 14/25 | Loss: 0.00115205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011520528933033347, 0.0011520528933033347, 0.0011520528933033347, 0.0011520528933033347, 0.0011520528933033347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011520528933033347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70696354
Iteration 2/25 | Loss: 0.00126550
Iteration 3/25 | Loss: 0.00126550
Iteration 4/25 | Loss: 0.00126550
Iteration 5/25 | Loss: 0.00126550
Iteration 6/25 | Loss: 0.00126550
Iteration 7/25 | Loss: 0.00126550
Iteration 8/25 | Loss: 0.00126550
Iteration 9/25 | Loss: 0.00126550
Iteration 10/25 | Loss: 0.00126550
Iteration 11/25 | Loss: 0.00126550
Iteration 12/25 | Loss: 0.00126550
Iteration 13/25 | Loss: 0.00126550
Iteration 14/25 | Loss: 0.00126550
Iteration 15/25 | Loss: 0.00126550
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012654978781938553, 0.0012654978781938553, 0.0012654978781938553, 0.0012654978781938553, 0.0012654978781938553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012654978781938553

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126550
Iteration 2/1000 | Loss: 0.00001766
Iteration 3/1000 | Loss: 0.00001455
Iteration 4/1000 | Loss: 0.00001362
Iteration 5/1000 | Loss: 0.00001302
Iteration 6/1000 | Loss: 0.00001245
Iteration 7/1000 | Loss: 0.00001216
Iteration 8/1000 | Loss: 0.00001192
Iteration 9/1000 | Loss: 0.00001167
Iteration 10/1000 | Loss: 0.00001159
Iteration 11/1000 | Loss: 0.00001145
Iteration 12/1000 | Loss: 0.00001144
Iteration 13/1000 | Loss: 0.00001136
Iteration 14/1000 | Loss: 0.00001129
Iteration 15/1000 | Loss: 0.00001128
Iteration 16/1000 | Loss: 0.00001128
Iteration 17/1000 | Loss: 0.00001123
Iteration 18/1000 | Loss: 0.00001117
Iteration 19/1000 | Loss: 0.00001116
Iteration 20/1000 | Loss: 0.00001115
Iteration 21/1000 | Loss: 0.00001114
Iteration 22/1000 | Loss: 0.00001114
Iteration 23/1000 | Loss: 0.00001114
Iteration 24/1000 | Loss: 0.00001105
Iteration 25/1000 | Loss: 0.00001101
Iteration 26/1000 | Loss: 0.00001099
Iteration 27/1000 | Loss: 0.00001099
Iteration 28/1000 | Loss: 0.00001099
Iteration 29/1000 | Loss: 0.00001099
Iteration 30/1000 | Loss: 0.00001098
Iteration 31/1000 | Loss: 0.00001095
Iteration 32/1000 | Loss: 0.00001094
Iteration 33/1000 | Loss: 0.00001094
Iteration 34/1000 | Loss: 0.00001094
Iteration 35/1000 | Loss: 0.00001093
Iteration 36/1000 | Loss: 0.00001093
Iteration 37/1000 | Loss: 0.00001093
Iteration 38/1000 | Loss: 0.00001093
Iteration 39/1000 | Loss: 0.00001092
Iteration 40/1000 | Loss: 0.00001092
Iteration 41/1000 | Loss: 0.00001091
Iteration 42/1000 | Loss: 0.00001091
Iteration 43/1000 | Loss: 0.00001090
Iteration 44/1000 | Loss: 0.00001090
Iteration 45/1000 | Loss: 0.00001087
Iteration 46/1000 | Loss: 0.00001086
Iteration 47/1000 | Loss: 0.00001086
Iteration 48/1000 | Loss: 0.00001086
Iteration 49/1000 | Loss: 0.00001084
Iteration 50/1000 | Loss: 0.00001083
Iteration 51/1000 | Loss: 0.00001082
Iteration 52/1000 | Loss: 0.00001081
Iteration 53/1000 | Loss: 0.00001080
Iteration 54/1000 | Loss: 0.00001080
Iteration 55/1000 | Loss: 0.00001080
Iteration 56/1000 | Loss: 0.00001080
Iteration 57/1000 | Loss: 0.00001080
Iteration 58/1000 | Loss: 0.00001079
Iteration 59/1000 | Loss: 0.00001079
Iteration 60/1000 | Loss: 0.00001079
Iteration 61/1000 | Loss: 0.00001079
Iteration 62/1000 | Loss: 0.00001079
Iteration 63/1000 | Loss: 0.00001079
Iteration 64/1000 | Loss: 0.00001079
Iteration 65/1000 | Loss: 0.00001079
Iteration 66/1000 | Loss: 0.00001079
Iteration 67/1000 | Loss: 0.00001079
Iteration 68/1000 | Loss: 0.00001078
Iteration 69/1000 | Loss: 0.00001078
Iteration 70/1000 | Loss: 0.00001078
Iteration 71/1000 | Loss: 0.00001078
Iteration 72/1000 | Loss: 0.00001078
Iteration 73/1000 | Loss: 0.00001077
Iteration 74/1000 | Loss: 0.00001077
Iteration 75/1000 | Loss: 0.00001077
Iteration 76/1000 | Loss: 0.00001076
Iteration 77/1000 | Loss: 0.00001076
Iteration 78/1000 | Loss: 0.00001076
Iteration 79/1000 | Loss: 0.00001075
Iteration 80/1000 | Loss: 0.00001075
Iteration 81/1000 | Loss: 0.00001073
Iteration 82/1000 | Loss: 0.00001073
Iteration 83/1000 | Loss: 0.00001073
Iteration 84/1000 | Loss: 0.00001073
Iteration 85/1000 | Loss: 0.00001073
Iteration 86/1000 | Loss: 0.00001073
Iteration 87/1000 | Loss: 0.00001073
Iteration 88/1000 | Loss: 0.00001073
Iteration 89/1000 | Loss: 0.00001072
Iteration 90/1000 | Loss: 0.00001072
Iteration 91/1000 | Loss: 0.00001072
Iteration 92/1000 | Loss: 0.00001072
Iteration 93/1000 | Loss: 0.00001072
Iteration 94/1000 | Loss: 0.00001072
Iteration 95/1000 | Loss: 0.00001071
Iteration 96/1000 | Loss: 0.00001071
Iteration 97/1000 | Loss: 0.00001070
Iteration 98/1000 | Loss: 0.00001070
Iteration 99/1000 | Loss: 0.00001070
Iteration 100/1000 | Loss: 0.00001070
Iteration 101/1000 | Loss: 0.00001069
Iteration 102/1000 | Loss: 0.00001069
Iteration 103/1000 | Loss: 0.00001069
Iteration 104/1000 | Loss: 0.00001069
Iteration 105/1000 | Loss: 0.00001068
Iteration 106/1000 | Loss: 0.00001068
Iteration 107/1000 | Loss: 0.00001068
Iteration 108/1000 | Loss: 0.00001068
Iteration 109/1000 | Loss: 0.00001068
Iteration 110/1000 | Loss: 0.00001068
Iteration 111/1000 | Loss: 0.00001068
Iteration 112/1000 | Loss: 0.00001068
Iteration 113/1000 | Loss: 0.00001067
Iteration 114/1000 | Loss: 0.00001067
Iteration 115/1000 | Loss: 0.00001067
Iteration 116/1000 | Loss: 0.00001066
Iteration 117/1000 | Loss: 0.00001066
Iteration 118/1000 | Loss: 0.00001066
Iteration 119/1000 | Loss: 0.00001066
Iteration 120/1000 | Loss: 0.00001066
Iteration 121/1000 | Loss: 0.00001066
Iteration 122/1000 | Loss: 0.00001066
Iteration 123/1000 | Loss: 0.00001066
Iteration 124/1000 | Loss: 0.00001066
Iteration 125/1000 | Loss: 0.00001065
Iteration 126/1000 | Loss: 0.00001065
Iteration 127/1000 | Loss: 0.00001065
Iteration 128/1000 | Loss: 0.00001065
Iteration 129/1000 | Loss: 0.00001065
Iteration 130/1000 | Loss: 0.00001065
Iteration 131/1000 | Loss: 0.00001065
Iteration 132/1000 | Loss: 0.00001065
Iteration 133/1000 | Loss: 0.00001064
Iteration 134/1000 | Loss: 0.00001064
Iteration 135/1000 | Loss: 0.00001064
Iteration 136/1000 | Loss: 0.00001064
Iteration 137/1000 | Loss: 0.00001064
Iteration 138/1000 | Loss: 0.00001064
Iteration 139/1000 | Loss: 0.00001064
Iteration 140/1000 | Loss: 0.00001063
Iteration 141/1000 | Loss: 0.00001063
Iteration 142/1000 | Loss: 0.00001063
Iteration 143/1000 | Loss: 0.00001063
Iteration 144/1000 | Loss: 0.00001063
Iteration 145/1000 | Loss: 0.00001063
Iteration 146/1000 | Loss: 0.00001063
Iteration 147/1000 | Loss: 0.00001063
Iteration 148/1000 | Loss: 0.00001062
Iteration 149/1000 | Loss: 0.00001062
Iteration 150/1000 | Loss: 0.00001062
Iteration 151/1000 | Loss: 0.00001062
Iteration 152/1000 | Loss: 0.00001062
Iteration 153/1000 | Loss: 0.00001062
Iteration 154/1000 | Loss: 0.00001062
Iteration 155/1000 | Loss: 0.00001061
Iteration 156/1000 | Loss: 0.00001061
Iteration 157/1000 | Loss: 0.00001061
Iteration 158/1000 | Loss: 0.00001060
Iteration 159/1000 | Loss: 0.00001060
Iteration 160/1000 | Loss: 0.00001060
Iteration 161/1000 | Loss: 0.00001060
Iteration 162/1000 | Loss: 0.00001060
Iteration 163/1000 | Loss: 0.00001060
Iteration 164/1000 | Loss: 0.00001060
Iteration 165/1000 | Loss: 0.00001060
Iteration 166/1000 | Loss: 0.00001060
Iteration 167/1000 | Loss: 0.00001059
Iteration 168/1000 | Loss: 0.00001059
Iteration 169/1000 | Loss: 0.00001059
Iteration 170/1000 | Loss: 0.00001059
Iteration 171/1000 | Loss: 0.00001059
Iteration 172/1000 | Loss: 0.00001059
Iteration 173/1000 | Loss: 0.00001059
Iteration 174/1000 | Loss: 0.00001059
Iteration 175/1000 | Loss: 0.00001059
Iteration 176/1000 | Loss: 0.00001059
Iteration 177/1000 | Loss: 0.00001059
Iteration 178/1000 | Loss: 0.00001059
Iteration 179/1000 | Loss: 0.00001059
Iteration 180/1000 | Loss: 0.00001059
Iteration 181/1000 | Loss: 0.00001058
Iteration 182/1000 | Loss: 0.00001058
Iteration 183/1000 | Loss: 0.00001058
Iteration 184/1000 | Loss: 0.00001057
Iteration 185/1000 | Loss: 0.00001057
Iteration 186/1000 | Loss: 0.00001057
Iteration 187/1000 | Loss: 0.00001057
Iteration 188/1000 | Loss: 0.00001056
Iteration 189/1000 | Loss: 0.00001056
Iteration 190/1000 | Loss: 0.00001056
Iteration 191/1000 | Loss: 0.00001056
Iteration 192/1000 | Loss: 0.00001056
Iteration 193/1000 | Loss: 0.00001055
Iteration 194/1000 | Loss: 0.00001055
Iteration 195/1000 | Loss: 0.00001055
Iteration 196/1000 | Loss: 0.00001055
Iteration 197/1000 | Loss: 0.00001055
Iteration 198/1000 | Loss: 0.00001055
Iteration 199/1000 | Loss: 0.00001055
Iteration 200/1000 | Loss: 0.00001055
Iteration 201/1000 | Loss: 0.00001055
Iteration 202/1000 | Loss: 0.00001055
Iteration 203/1000 | Loss: 0.00001055
Iteration 204/1000 | Loss: 0.00001055
Iteration 205/1000 | Loss: 0.00001055
Iteration 206/1000 | Loss: 0.00001055
Iteration 207/1000 | Loss: 0.00001055
Iteration 208/1000 | Loss: 0.00001055
Iteration 209/1000 | Loss: 0.00001055
Iteration 210/1000 | Loss: 0.00001055
Iteration 211/1000 | Loss: 0.00001055
Iteration 212/1000 | Loss: 0.00001055
Iteration 213/1000 | Loss: 0.00001055
Iteration 214/1000 | Loss: 0.00001055
Iteration 215/1000 | Loss: 0.00001055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.0553921129030641e-05, 1.0553921129030641e-05, 1.0553921129030641e-05, 1.0553921129030641e-05, 1.0553921129030641e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0553921129030641e-05

Optimization complete. Final v2v error: 2.81473445892334 mm

Highest mean error: 3.0783934593200684 mm for frame 130

Lowest mean error: 2.7008514404296875 mm for frame 171

Saving results

Total time: 42.931206703186035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787442
Iteration 2/25 | Loss: 0.00153538
Iteration 3/25 | Loss: 0.00133034
Iteration 4/25 | Loss: 0.00131558
Iteration 5/25 | Loss: 0.00129602
Iteration 6/25 | Loss: 0.00128067
Iteration 7/25 | Loss: 0.00128095
Iteration 8/25 | Loss: 0.00127188
Iteration 9/25 | Loss: 0.00127036
Iteration 10/25 | Loss: 0.00126957
Iteration 11/25 | Loss: 0.00126927
Iteration 12/25 | Loss: 0.00126918
Iteration 13/25 | Loss: 0.00126918
Iteration 14/25 | Loss: 0.00126917
Iteration 15/25 | Loss: 0.00126917
Iteration 16/25 | Loss: 0.00126917
Iteration 17/25 | Loss: 0.00126917
Iteration 18/25 | Loss: 0.00126917
Iteration 19/25 | Loss: 0.00126917
Iteration 20/25 | Loss: 0.00126917
Iteration 21/25 | Loss: 0.00126917
Iteration 22/25 | Loss: 0.00126917
Iteration 23/25 | Loss: 0.00126917
Iteration 24/25 | Loss: 0.00126917
Iteration 25/25 | Loss: 0.00126916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.53573012
Iteration 2/25 | Loss: 0.00114331
Iteration 3/25 | Loss: 0.00114331
Iteration 4/25 | Loss: 0.00114331
Iteration 5/25 | Loss: 0.00114331
Iteration 6/25 | Loss: 0.00114331
Iteration 7/25 | Loss: 0.00114331
Iteration 8/25 | Loss: 0.00114331
Iteration 9/25 | Loss: 0.00114331
Iteration 10/25 | Loss: 0.00114331
Iteration 11/25 | Loss: 0.00114331
Iteration 12/25 | Loss: 0.00114331
Iteration 13/25 | Loss: 0.00114331
Iteration 14/25 | Loss: 0.00114331
Iteration 15/25 | Loss: 0.00114331
Iteration 16/25 | Loss: 0.00114331
Iteration 17/25 | Loss: 0.00114331
Iteration 18/25 | Loss: 0.00114331
Iteration 19/25 | Loss: 0.00114331
Iteration 20/25 | Loss: 0.00114331
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011433063773438334, 0.0011433063773438334, 0.0011433063773438334, 0.0011433063773438334, 0.0011433063773438334]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011433063773438334

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114331
Iteration 2/1000 | Loss: 0.00004457
Iteration 3/1000 | Loss: 0.00002614
Iteration 4/1000 | Loss: 0.00002167
Iteration 5/1000 | Loss: 0.00002056
Iteration 6/1000 | Loss: 0.00001977
Iteration 7/1000 | Loss: 0.00001918
Iteration 8/1000 | Loss: 0.00001880
Iteration 9/1000 | Loss: 0.00001844
Iteration 10/1000 | Loss: 0.00001818
Iteration 11/1000 | Loss: 0.00001795
Iteration 12/1000 | Loss: 0.00001793
Iteration 13/1000 | Loss: 0.00001779
Iteration 14/1000 | Loss: 0.00001778
Iteration 15/1000 | Loss: 0.00001766
Iteration 16/1000 | Loss: 0.00001765
Iteration 17/1000 | Loss: 0.00001750
Iteration 18/1000 | Loss: 0.00001747
Iteration 19/1000 | Loss: 0.00001745
Iteration 20/1000 | Loss: 0.00001745
Iteration 21/1000 | Loss: 0.00001745
Iteration 22/1000 | Loss: 0.00001744
Iteration 23/1000 | Loss: 0.00001744
Iteration 24/1000 | Loss: 0.00001744
Iteration 25/1000 | Loss: 0.00001744
Iteration 26/1000 | Loss: 0.00001743
Iteration 27/1000 | Loss: 0.00001743
Iteration 28/1000 | Loss: 0.00001743
Iteration 29/1000 | Loss: 0.00001743
Iteration 30/1000 | Loss: 0.00001740
Iteration 31/1000 | Loss: 0.00001740
Iteration 32/1000 | Loss: 0.00001740
Iteration 33/1000 | Loss: 0.00001739
Iteration 34/1000 | Loss: 0.00001739
Iteration 35/1000 | Loss: 0.00001739
Iteration 36/1000 | Loss: 0.00001738
Iteration 37/1000 | Loss: 0.00001738
Iteration 38/1000 | Loss: 0.00001737
Iteration 39/1000 | Loss: 0.00001737
Iteration 40/1000 | Loss: 0.00001736
Iteration 41/1000 | Loss: 0.00001736
Iteration 42/1000 | Loss: 0.00001735
Iteration 43/1000 | Loss: 0.00001735
Iteration 44/1000 | Loss: 0.00001735
Iteration 45/1000 | Loss: 0.00001735
Iteration 46/1000 | Loss: 0.00001734
Iteration 47/1000 | Loss: 0.00001734
Iteration 48/1000 | Loss: 0.00001734
Iteration 49/1000 | Loss: 0.00001734
Iteration 50/1000 | Loss: 0.00001734
Iteration 51/1000 | Loss: 0.00001733
Iteration 52/1000 | Loss: 0.00001733
Iteration 53/1000 | Loss: 0.00001733
Iteration 54/1000 | Loss: 0.00001733
Iteration 55/1000 | Loss: 0.00001733
Iteration 56/1000 | Loss: 0.00001733
Iteration 57/1000 | Loss: 0.00001733
Iteration 58/1000 | Loss: 0.00001733
Iteration 59/1000 | Loss: 0.00001733
Iteration 60/1000 | Loss: 0.00001733
Iteration 61/1000 | Loss: 0.00001732
Iteration 62/1000 | Loss: 0.00001732
Iteration 63/1000 | Loss: 0.00001732
Iteration 64/1000 | Loss: 0.00001732
Iteration 65/1000 | Loss: 0.00001732
Iteration 66/1000 | Loss: 0.00001732
Iteration 67/1000 | Loss: 0.00001732
Iteration 68/1000 | Loss: 0.00001732
Iteration 69/1000 | Loss: 0.00001732
Iteration 70/1000 | Loss: 0.00001732
Iteration 71/1000 | Loss: 0.00001732
Iteration 72/1000 | Loss: 0.00001732
Iteration 73/1000 | Loss: 0.00001731
Iteration 74/1000 | Loss: 0.00001731
Iteration 75/1000 | Loss: 0.00001731
Iteration 76/1000 | Loss: 0.00001731
Iteration 77/1000 | Loss: 0.00001731
Iteration 78/1000 | Loss: 0.00001731
Iteration 79/1000 | Loss: 0.00001731
Iteration 80/1000 | Loss: 0.00001731
Iteration 81/1000 | Loss: 0.00001731
Iteration 82/1000 | Loss: 0.00001731
Iteration 83/1000 | Loss: 0.00001730
Iteration 84/1000 | Loss: 0.00001730
Iteration 85/1000 | Loss: 0.00001730
Iteration 86/1000 | Loss: 0.00001730
Iteration 87/1000 | Loss: 0.00001730
Iteration 88/1000 | Loss: 0.00001730
Iteration 89/1000 | Loss: 0.00001730
Iteration 90/1000 | Loss: 0.00001730
Iteration 91/1000 | Loss: 0.00001729
Iteration 92/1000 | Loss: 0.00001729
Iteration 93/1000 | Loss: 0.00001729
Iteration 94/1000 | Loss: 0.00001729
Iteration 95/1000 | Loss: 0.00001729
Iteration 96/1000 | Loss: 0.00001729
Iteration 97/1000 | Loss: 0.00001729
Iteration 98/1000 | Loss: 0.00001729
Iteration 99/1000 | Loss: 0.00001729
Iteration 100/1000 | Loss: 0.00001728
Iteration 101/1000 | Loss: 0.00001728
Iteration 102/1000 | Loss: 0.00001728
Iteration 103/1000 | Loss: 0.00001728
Iteration 104/1000 | Loss: 0.00001728
Iteration 105/1000 | Loss: 0.00001728
Iteration 106/1000 | Loss: 0.00001728
Iteration 107/1000 | Loss: 0.00001728
Iteration 108/1000 | Loss: 0.00001728
Iteration 109/1000 | Loss: 0.00001728
Iteration 110/1000 | Loss: 0.00001728
Iteration 111/1000 | Loss: 0.00001728
Iteration 112/1000 | Loss: 0.00001728
Iteration 113/1000 | Loss: 0.00001728
Iteration 114/1000 | Loss: 0.00001727
Iteration 115/1000 | Loss: 0.00001727
Iteration 116/1000 | Loss: 0.00001727
Iteration 117/1000 | Loss: 0.00001727
Iteration 118/1000 | Loss: 0.00001727
Iteration 119/1000 | Loss: 0.00001727
Iteration 120/1000 | Loss: 0.00001727
Iteration 121/1000 | Loss: 0.00001727
Iteration 122/1000 | Loss: 0.00001726
Iteration 123/1000 | Loss: 0.00001726
Iteration 124/1000 | Loss: 0.00001726
Iteration 125/1000 | Loss: 0.00001726
Iteration 126/1000 | Loss: 0.00001726
Iteration 127/1000 | Loss: 0.00001726
Iteration 128/1000 | Loss: 0.00001726
Iteration 129/1000 | Loss: 0.00001726
Iteration 130/1000 | Loss: 0.00001726
Iteration 131/1000 | Loss: 0.00001726
Iteration 132/1000 | Loss: 0.00001726
Iteration 133/1000 | Loss: 0.00001726
Iteration 134/1000 | Loss: 0.00001725
Iteration 135/1000 | Loss: 0.00001725
Iteration 136/1000 | Loss: 0.00001725
Iteration 137/1000 | Loss: 0.00001725
Iteration 138/1000 | Loss: 0.00001725
Iteration 139/1000 | Loss: 0.00001725
Iteration 140/1000 | Loss: 0.00001725
Iteration 141/1000 | Loss: 0.00001725
Iteration 142/1000 | Loss: 0.00001725
Iteration 143/1000 | Loss: 0.00001725
Iteration 144/1000 | Loss: 0.00001725
Iteration 145/1000 | Loss: 0.00001725
Iteration 146/1000 | Loss: 0.00001724
Iteration 147/1000 | Loss: 0.00001724
Iteration 148/1000 | Loss: 0.00001724
Iteration 149/1000 | Loss: 0.00001724
Iteration 150/1000 | Loss: 0.00001724
Iteration 151/1000 | Loss: 0.00001724
Iteration 152/1000 | Loss: 0.00001724
Iteration 153/1000 | Loss: 0.00001724
Iteration 154/1000 | Loss: 0.00001724
Iteration 155/1000 | Loss: 0.00001723
Iteration 156/1000 | Loss: 0.00001723
Iteration 157/1000 | Loss: 0.00001723
Iteration 158/1000 | Loss: 0.00001723
Iteration 159/1000 | Loss: 0.00001723
Iteration 160/1000 | Loss: 0.00001723
Iteration 161/1000 | Loss: 0.00001723
Iteration 162/1000 | Loss: 0.00001723
Iteration 163/1000 | Loss: 0.00001723
Iteration 164/1000 | Loss: 0.00001723
Iteration 165/1000 | Loss: 0.00001723
Iteration 166/1000 | Loss: 0.00001723
Iteration 167/1000 | Loss: 0.00001723
Iteration 168/1000 | Loss: 0.00001723
Iteration 169/1000 | Loss: 0.00001723
Iteration 170/1000 | Loss: 0.00001723
Iteration 171/1000 | Loss: 0.00001723
Iteration 172/1000 | Loss: 0.00001723
Iteration 173/1000 | Loss: 0.00001723
Iteration 174/1000 | Loss: 0.00001723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.722956949379295e-05, 1.722956949379295e-05, 1.722956949379295e-05, 1.722956949379295e-05, 1.722956949379295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.722956949379295e-05

Optimization complete. Final v2v error: 3.5701992511749268 mm

Highest mean error: 4.201905727386475 mm for frame 41

Lowest mean error: 3.166691780090332 mm for frame 182

Saving results

Total time: 56.944496393203735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435644
Iteration 2/25 | Loss: 0.00132678
Iteration 3/25 | Loss: 0.00123160
Iteration 4/25 | Loss: 0.00121854
Iteration 5/25 | Loss: 0.00121311
Iteration 6/25 | Loss: 0.00121234
Iteration 7/25 | Loss: 0.00121234
Iteration 8/25 | Loss: 0.00121234
Iteration 9/25 | Loss: 0.00121234
Iteration 10/25 | Loss: 0.00121234
Iteration 11/25 | Loss: 0.00121234
Iteration 12/25 | Loss: 0.00121234
Iteration 13/25 | Loss: 0.00121234
Iteration 14/25 | Loss: 0.00121234
Iteration 15/25 | Loss: 0.00121234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012123442720621824, 0.0012123442720621824, 0.0012123442720621824, 0.0012123442720621824, 0.0012123442720621824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012123442720621824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27552021
Iteration 2/25 | Loss: 0.00173808
Iteration 3/25 | Loss: 0.00173808
Iteration 4/25 | Loss: 0.00173808
Iteration 5/25 | Loss: 0.00173808
Iteration 6/25 | Loss: 0.00173808
Iteration 7/25 | Loss: 0.00173808
Iteration 8/25 | Loss: 0.00173808
Iteration 9/25 | Loss: 0.00173808
Iteration 10/25 | Loss: 0.00173808
Iteration 11/25 | Loss: 0.00173808
Iteration 12/25 | Loss: 0.00173808
Iteration 13/25 | Loss: 0.00173808
Iteration 14/25 | Loss: 0.00173808
Iteration 15/25 | Loss: 0.00173808
Iteration 16/25 | Loss: 0.00173808
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0017380783101543784, 0.0017380783101543784, 0.0017380783101543784, 0.0017380783101543784, 0.0017380783101543784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017380783101543784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173808
Iteration 2/1000 | Loss: 0.00004376
Iteration 3/1000 | Loss: 0.00002572
Iteration 4/1000 | Loss: 0.00002049
Iteration 5/1000 | Loss: 0.00001897
Iteration 6/1000 | Loss: 0.00001774
Iteration 7/1000 | Loss: 0.00001693
Iteration 8/1000 | Loss: 0.00001638
Iteration 9/1000 | Loss: 0.00001599
Iteration 10/1000 | Loss: 0.00001564
Iteration 11/1000 | Loss: 0.00001529
Iteration 12/1000 | Loss: 0.00001508
Iteration 13/1000 | Loss: 0.00001497
Iteration 14/1000 | Loss: 0.00001494
Iteration 15/1000 | Loss: 0.00001482
Iteration 16/1000 | Loss: 0.00001475
Iteration 17/1000 | Loss: 0.00001473
Iteration 18/1000 | Loss: 0.00001471
Iteration 19/1000 | Loss: 0.00001470
Iteration 20/1000 | Loss: 0.00001466
Iteration 21/1000 | Loss: 0.00001465
Iteration 22/1000 | Loss: 0.00001459
Iteration 23/1000 | Loss: 0.00001452
Iteration 24/1000 | Loss: 0.00001447
Iteration 25/1000 | Loss: 0.00001445
Iteration 26/1000 | Loss: 0.00001444
Iteration 27/1000 | Loss: 0.00001444
Iteration 28/1000 | Loss: 0.00001444
Iteration 29/1000 | Loss: 0.00001441
Iteration 30/1000 | Loss: 0.00001441
Iteration 31/1000 | Loss: 0.00001439
Iteration 32/1000 | Loss: 0.00001439
Iteration 33/1000 | Loss: 0.00001438
Iteration 34/1000 | Loss: 0.00001438
Iteration 35/1000 | Loss: 0.00001438
Iteration 36/1000 | Loss: 0.00001438
Iteration 37/1000 | Loss: 0.00001437
Iteration 38/1000 | Loss: 0.00001437
Iteration 39/1000 | Loss: 0.00001436
Iteration 40/1000 | Loss: 0.00001433
Iteration 41/1000 | Loss: 0.00001432
Iteration 42/1000 | Loss: 0.00001432
Iteration 43/1000 | Loss: 0.00001432
Iteration 44/1000 | Loss: 0.00001431
Iteration 45/1000 | Loss: 0.00001431
Iteration 46/1000 | Loss: 0.00001430
Iteration 47/1000 | Loss: 0.00001430
Iteration 48/1000 | Loss: 0.00001429
Iteration 49/1000 | Loss: 0.00001429
Iteration 50/1000 | Loss: 0.00001429
Iteration 51/1000 | Loss: 0.00001429
Iteration 52/1000 | Loss: 0.00001428
Iteration 53/1000 | Loss: 0.00001428
Iteration 54/1000 | Loss: 0.00001428
Iteration 55/1000 | Loss: 0.00001428
Iteration 56/1000 | Loss: 0.00001428
Iteration 57/1000 | Loss: 0.00001427
Iteration 58/1000 | Loss: 0.00001427
Iteration 59/1000 | Loss: 0.00001427
Iteration 60/1000 | Loss: 0.00001426
Iteration 61/1000 | Loss: 0.00001426
Iteration 62/1000 | Loss: 0.00001426
Iteration 63/1000 | Loss: 0.00001425
Iteration 64/1000 | Loss: 0.00001425
Iteration 65/1000 | Loss: 0.00001425
Iteration 66/1000 | Loss: 0.00001424
Iteration 67/1000 | Loss: 0.00001424
Iteration 68/1000 | Loss: 0.00001424
Iteration 69/1000 | Loss: 0.00001424
Iteration 70/1000 | Loss: 0.00001423
Iteration 71/1000 | Loss: 0.00001423
Iteration 72/1000 | Loss: 0.00001423
Iteration 73/1000 | Loss: 0.00001423
Iteration 74/1000 | Loss: 0.00001423
Iteration 75/1000 | Loss: 0.00001423
Iteration 76/1000 | Loss: 0.00001422
Iteration 77/1000 | Loss: 0.00001422
Iteration 78/1000 | Loss: 0.00001422
Iteration 79/1000 | Loss: 0.00001422
Iteration 80/1000 | Loss: 0.00001422
Iteration 81/1000 | Loss: 0.00001421
Iteration 82/1000 | Loss: 0.00001421
Iteration 83/1000 | Loss: 0.00001421
Iteration 84/1000 | Loss: 0.00001421
Iteration 85/1000 | Loss: 0.00001421
Iteration 86/1000 | Loss: 0.00001421
Iteration 87/1000 | Loss: 0.00001421
Iteration 88/1000 | Loss: 0.00001421
Iteration 89/1000 | Loss: 0.00001421
Iteration 90/1000 | Loss: 0.00001420
Iteration 91/1000 | Loss: 0.00001420
Iteration 92/1000 | Loss: 0.00001420
Iteration 93/1000 | Loss: 0.00001420
Iteration 94/1000 | Loss: 0.00001419
Iteration 95/1000 | Loss: 0.00001419
Iteration 96/1000 | Loss: 0.00001419
Iteration 97/1000 | Loss: 0.00001419
Iteration 98/1000 | Loss: 0.00001419
Iteration 99/1000 | Loss: 0.00001419
Iteration 100/1000 | Loss: 0.00001419
Iteration 101/1000 | Loss: 0.00001419
Iteration 102/1000 | Loss: 0.00001418
Iteration 103/1000 | Loss: 0.00001418
Iteration 104/1000 | Loss: 0.00001418
Iteration 105/1000 | Loss: 0.00001418
Iteration 106/1000 | Loss: 0.00001418
Iteration 107/1000 | Loss: 0.00001418
Iteration 108/1000 | Loss: 0.00001417
Iteration 109/1000 | Loss: 0.00001417
Iteration 110/1000 | Loss: 0.00001417
Iteration 111/1000 | Loss: 0.00001417
Iteration 112/1000 | Loss: 0.00001416
Iteration 113/1000 | Loss: 0.00001416
Iteration 114/1000 | Loss: 0.00001416
Iteration 115/1000 | Loss: 0.00001415
Iteration 116/1000 | Loss: 0.00001415
Iteration 117/1000 | Loss: 0.00001415
Iteration 118/1000 | Loss: 0.00001415
Iteration 119/1000 | Loss: 0.00001414
Iteration 120/1000 | Loss: 0.00001414
Iteration 121/1000 | Loss: 0.00001414
Iteration 122/1000 | Loss: 0.00001413
Iteration 123/1000 | Loss: 0.00001413
Iteration 124/1000 | Loss: 0.00001413
Iteration 125/1000 | Loss: 0.00001412
Iteration 126/1000 | Loss: 0.00001412
Iteration 127/1000 | Loss: 0.00001411
Iteration 128/1000 | Loss: 0.00001411
Iteration 129/1000 | Loss: 0.00001411
Iteration 130/1000 | Loss: 0.00001411
Iteration 131/1000 | Loss: 0.00001411
Iteration 132/1000 | Loss: 0.00001411
Iteration 133/1000 | Loss: 0.00001411
Iteration 134/1000 | Loss: 0.00001411
Iteration 135/1000 | Loss: 0.00001411
Iteration 136/1000 | Loss: 0.00001411
Iteration 137/1000 | Loss: 0.00001411
Iteration 138/1000 | Loss: 0.00001411
Iteration 139/1000 | Loss: 0.00001411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.4107283277553506e-05, 1.4107283277553506e-05, 1.4107283277553506e-05, 1.4107283277553506e-05, 1.4107283277553506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4107283277553506e-05

Optimization complete. Final v2v error: 3.162137508392334 mm

Highest mean error: 3.957294464111328 mm for frame 12

Lowest mean error: 2.5396060943603516 mm for frame 186

Saving results

Total time: 45.959654331207275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433110
Iteration 2/25 | Loss: 0.00141561
Iteration 3/25 | Loss: 0.00127764
Iteration 4/25 | Loss: 0.00125946
Iteration 5/25 | Loss: 0.00125628
Iteration 6/25 | Loss: 0.00125533
Iteration 7/25 | Loss: 0.00125518
Iteration 8/25 | Loss: 0.00125518
Iteration 9/25 | Loss: 0.00125518
Iteration 10/25 | Loss: 0.00125518
Iteration 11/25 | Loss: 0.00125518
Iteration 12/25 | Loss: 0.00125518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001255178707651794, 0.001255178707651794, 0.001255178707651794, 0.001255178707651794, 0.001255178707651794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001255178707651794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.91179895
Iteration 2/25 | Loss: 0.00113336
Iteration 3/25 | Loss: 0.00113333
Iteration 4/25 | Loss: 0.00113333
Iteration 5/25 | Loss: 0.00113333
Iteration 6/25 | Loss: 0.00113333
Iteration 7/25 | Loss: 0.00113333
Iteration 8/25 | Loss: 0.00113333
Iteration 9/25 | Loss: 0.00113333
Iteration 10/25 | Loss: 0.00113333
Iteration 11/25 | Loss: 0.00113333
Iteration 12/25 | Loss: 0.00113333
Iteration 13/25 | Loss: 0.00113333
Iteration 14/25 | Loss: 0.00113333
Iteration 15/25 | Loss: 0.00113333
Iteration 16/25 | Loss: 0.00113333
Iteration 17/25 | Loss: 0.00113333
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011333279544487596, 0.0011333279544487596, 0.0011333279544487596, 0.0011333279544487596, 0.0011333279544487596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011333279544487596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113333
Iteration 2/1000 | Loss: 0.00004148
Iteration 3/1000 | Loss: 0.00002969
Iteration 4/1000 | Loss: 0.00002495
Iteration 5/1000 | Loss: 0.00002351
Iteration 6/1000 | Loss: 0.00002267
Iteration 7/1000 | Loss: 0.00002204
Iteration 8/1000 | Loss: 0.00002139
Iteration 9/1000 | Loss: 0.00002099
Iteration 10/1000 | Loss: 0.00002058
Iteration 11/1000 | Loss: 0.00002030
Iteration 12/1000 | Loss: 0.00002009
Iteration 13/1000 | Loss: 0.00001992
Iteration 14/1000 | Loss: 0.00001985
Iteration 15/1000 | Loss: 0.00001980
Iteration 16/1000 | Loss: 0.00001977
Iteration 17/1000 | Loss: 0.00001973
Iteration 18/1000 | Loss: 0.00001973
Iteration 19/1000 | Loss: 0.00001972
Iteration 20/1000 | Loss: 0.00001971
Iteration 21/1000 | Loss: 0.00001971
Iteration 22/1000 | Loss: 0.00001967
Iteration 23/1000 | Loss: 0.00001967
Iteration 24/1000 | Loss: 0.00001964
Iteration 25/1000 | Loss: 0.00001963
Iteration 26/1000 | Loss: 0.00001962
Iteration 27/1000 | Loss: 0.00001961
Iteration 28/1000 | Loss: 0.00001960
Iteration 29/1000 | Loss: 0.00001957
Iteration 30/1000 | Loss: 0.00001957
Iteration 31/1000 | Loss: 0.00001956
Iteration 32/1000 | Loss: 0.00001956
Iteration 33/1000 | Loss: 0.00001955
Iteration 34/1000 | Loss: 0.00001955
Iteration 35/1000 | Loss: 0.00001954
Iteration 36/1000 | Loss: 0.00001954
Iteration 37/1000 | Loss: 0.00001954
Iteration 38/1000 | Loss: 0.00001953
Iteration 39/1000 | Loss: 0.00001953
Iteration 40/1000 | Loss: 0.00001953
Iteration 41/1000 | Loss: 0.00001953
Iteration 42/1000 | Loss: 0.00001953
Iteration 43/1000 | Loss: 0.00001952
Iteration 44/1000 | Loss: 0.00001952
Iteration 45/1000 | Loss: 0.00001952
Iteration 46/1000 | Loss: 0.00001952
Iteration 47/1000 | Loss: 0.00001952
Iteration 48/1000 | Loss: 0.00001952
Iteration 49/1000 | Loss: 0.00001951
Iteration 50/1000 | Loss: 0.00001951
Iteration 51/1000 | Loss: 0.00001951
Iteration 52/1000 | Loss: 0.00001950
Iteration 53/1000 | Loss: 0.00001950
Iteration 54/1000 | Loss: 0.00001950
Iteration 55/1000 | Loss: 0.00001950
Iteration 56/1000 | Loss: 0.00001950
Iteration 57/1000 | Loss: 0.00001950
Iteration 58/1000 | Loss: 0.00001950
Iteration 59/1000 | Loss: 0.00001950
Iteration 60/1000 | Loss: 0.00001950
Iteration 61/1000 | Loss: 0.00001950
Iteration 62/1000 | Loss: 0.00001950
Iteration 63/1000 | Loss: 0.00001949
Iteration 64/1000 | Loss: 0.00001949
Iteration 65/1000 | Loss: 0.00001949
Iteration 66/1000 | Loss: 0.00001948
Iteration 67/1000 | Loss: 0.00001948
Iteration 68/1000 | Loss: 0.00001948
Iteration 69/1000 | Loss: 0.00001948
Iteration 70/1000 | Loss: 0.00001947
Iteration 71/1000 | Loss: 0.00001947
Iteration 72/1000 | Loss: 0.00001947
Iteration 73/1000 | Loss: 0.00001947
Iteration 74/1000 | Loss: 0.00001947
Iteration 75/1000 | Loss: 0.00001947
Iteration 76/1000 | Loss: 0.00001946
Iteration 77/1000 | Loss: 0.00001946
Iteration 78/1000 | Loss: 0.00001946
Iteration 79/1000 | Loss: 0.00001946
Iteration 80/1000 | Loss: 0.00001946
Iteration 81/1000 | Loss: 0.00001946
Iteration 82/1000 | Loss: 0.00001946
Iteration 83/1000 | Loss: 0.00001946
Iteration 84/1000 | Loss: 0.00001946
Iteration 85/1000 | Loss: 0.00001945
Iteration 86/1000 | Loss: 0.00001945
Iteration 87/1000 | Loss: 0.00001945
Iteration 88/1000 | Loss: 0.00001945
Iteration 89/1000 | Loss: 0.00001945
Iteration 90/1000 | Loss: 0.00001945
Iteration 91/1000 | Loss: 0.00001945
Iteration 92/1000 | Loss: 0.00001944
Iteration 93/1000 | Loss: 0.00001944
Iteration 94/1000 | Loss: 0.00001944
Iteration 95/1000 | Loss: 0.00001943
Iteration 96/1000 | Loss: 0.00001943
Iteration 97/1000 | Loss: 0.00001943
Iteration 98/1000 | Loss: 0.00001943
Iteration 99/1000 | Loss: 0.00001942
Iteration 100/1000 | Loss: 0.00001942
Iteration 101/1000 | Loss: 0.00001942
Iteration 102/1000 | Loss: 0.00001942
Iteration 103/1000 | Loss: 0.00001942
Iteration 104/1000 | Loss: 0.00001942
Iteration 105/1000 | Loss: 0.00001942
Iteration 106/1000 | Loss: 0.00001941
Iteration 107/1000 | Loss: 0.00001941
Iteration 108/1000 | Loss: 0.00001941
Iteration 109/1000 | Loss: 0.00001941
Iteration 110/1000 | Loss: 0.00001941
Iteration 111/1000 | Loss: 0.00001941
Iteration 112/1000 | Loss: 0.00001941
Iteration 113/1000 | Loss: 0.00001940
Iteration 114/1000 | Loss: 0.00001940
Iteration 115/1000 | Loss: 0.00001940
Iteration 116/1000 | Loss: 0.00001940
Iteration 117/1000 | Loss: 0.00001940
Iteration 118/1000 | Loss: 0.00001940
Iteration 119/1000 | Loss: 0.00001940
Iteration 120/1000 | Loss: 0.00001940
Iteration 121/1000 | Loss: 0.00001940
Iteration 122/1000 | Loss: 0.00001940
Iteration 123/1000 | Loss: 0.00001940
Iteration 124/1000 | Loss: 0.00001939
Iteration 125/1000 | Loss: 0.00001939
Iteration 126/1000 | Loss: 0.00001939
Iteration 127/1000 | Loss: 0.00001939
Iteration 128/1000 | Loss: 0.00001939
Iteration 129/1000 | Loss: 0.00001939
Iteration 130/1000 | Loss: 0.00001939
Iteration 131/1000 | Loss: 0.00001938
Iteration 132/1000 | Loss: 0.00001938
Iteration 133/1000 | Loss: 0.00001938
Iteration 134/1000 | Loss: 0.00001938
Iteration 135/1000 | Loss: 0.00001938
Iteration 136/1000 | Loss: 0.00001938
Iteration 137/1000 | Loss: 0.00001938
Iteration 138/1000 | Loss: 0.00001938
Iteration 139/1000 | Loss: 0.00001938
Iteration 140/1000 | Loss: 0.00001938
Iteration 141/1000 | Loss: 0.00001938
Iteration 142/1000 | Loss: 0.00001938
Iteration 143/1000 | Loss: 0.00001938
Iteration 144/1000 | Loss: 0.00001938
Iteration 145/1000 | Loss: 0.00001938
Iteration 146/1000 | Loss: 0.00001938
Iteration 147/1000 | Loss: 0.00001938
Iteration 148/1000 | Loss: 0.00001938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.9376244381419383e-05, 1.9376244381419383e-05, 1.9376244381419383e-05, 1.9376244381419383e-05, 1.9376244381419383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9376244381419383e-05

Optimization complete. Final v2v error: 3.7402968406677246 mm

Highest mean error: 4.225663185119629 mm for frame 59

Lowest mean error: 3.320650100708008 mm for frame 7

Saving results

Total time: 38.60242557525635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00922430
Iteration 2/25 | Loss: 0.00217285
Iteration 3/25 | Loss: 0.00153797
Iteration 4/25 | Loss: 0.00143310
Iteration 5/25 | Loss: 0.00140756
Iteration 6/25 | Loss: 0.00140386
Iteration 7/25 | Loss: 0.00139856
Iteration 8/25 | Loss: 0.00136791
Iteration 9/25 | Loss: 0.00136360
Iteration 10/25 | Loss: 0.00136018
Iteration 11/25 | Loss: 0.00135879
Iteration 12/25 | Loss: 0.00135603
Iteration 13/25 | Loss: 0.00135542
Iteration 14/25 | Loss: 0.00135833
Iteration 15/25 | Loss: 0.00135354
Iteration 16/25 | Loss: 0.00135272
Iteration 17/25 | Loss: 0.00135251
Iteration 18/25 | Loss: 0.00135244
Iteration 19/25 | Loss: 0.00135244
Iteration 20/25 | Loss: 0.00135244
Iteration 21/25 | Loss: 0.00135244
Iteration 22/25 | Loss: 0.00135244
Iteration 23/25 | Loss: 0.00135243
Iteration 24/25 | Loss: 0.00135243
Iteration 25/25 | Loss: 0.00135243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80899704
Iteration 2/25 | Loss: 0.00113892
Iteration 3/25 | Loss: 0.00113888
Iteration 4/25 | Loss: 0.00113888
Iteration 5/25 | Loss: 0.00113888
Iteration 6/25 | Loss: 0.00113888
Iteration 7/25 | Loss: 0.00113888
Iteration 8/25 | Loss: 0.00113888
Iteration 9/25 | Loss: 0.00113888
Iteration 10/25 | Loss: 0.00113888
Iteration 11/25 | Loss: 0.00113888
Iteration 12/25 | Loss: 0.00113888
Iteration 13/25 | Loss: 0.00113888
Iteration 14/25 | Loss: 0.00113888
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011388807324692607, 0.0011388807324692607, 0.0011388807324692607, 0.0011388807324692607, 0.0011388807324692607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011388807324692607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113888
Iteration 2/1000 | Loss: 0.00003535
Iteration 3/1000 | Loss: 0.00002580
Iteration 4/1000 | Loss: 0.00002408
Iteration 5/1000 | Loss: 0.00002290
Iteration 6/1000 | Loss: 0.00002227
Iteration 7/1000 | Loss: 0.00013810
Iteration 8/1000 | Loss: 0.00002194
Iteration 9/1000 | Loss: 0.00002153
Iteration 10/1000 | Loss: 0.00002116
Iteration 11/1000 | Loss: 0.00002101
Iteration 12/1000 | Loss: 0.00002091
Iteration 13/1000 | Loss: 0.00002089
Iteration 14/1000 | Loss: 0.00002088
Iteration 15/1000 | Loss: 0.00002075
Iteration 16/1000 | Loss: 0.00002075
Iteration 17/1000 | Loss: 0.00002073
Iteration 18/1000 | Loss: 0.00002070
Iteration 19/1000 | Loss: 0.00002069
Iteration 20/1000 | Loss: 0.00002064
Iteration 21/1000 | Loss: 0.00002059
Iteration 22/1000 | Loss: 0.00002058
Iteration 23/1000 | Loss: 0.00002058
Iteration 24/1000 | Loss: 0.00002058
Iteration 25/1000 | Loss: 0.00002053
Iteration 26/1000 | Loss: 0.00002053
Iteration 27/1000 | Loss: 0.00002052
Iteration 28/1000 | Loss: 0.00002049
Iteration 29/1000 | Loss: 0.00002048
Iteration 30/1000 | Loss: 0.00002045
Iteration 31/1000 | Loss: 0.00002045
Iteration 32/1000 | Loss: 0.00002045
Iteration 33/1000 | Loss: 0.00002045
Iteration 34/1000 | Loss: 0.00002045
Iteration 35/1000 | Loss: 0.00002044
Iteration 36/1000 | Loss: 0.00002044
Iteration 37/1000 | Loss: 0.00002043
Iteration 38/1000 | Loss: 0.00002042
Iteration 39/1000 | Loss: 0.00002042
Iteration 40/1000 | Loss: 0.00002041
Iteration 41/1000 | Loss: 0.00002041
Iteration 42/1000 | Loss: 0.00002041
Iteration 43/1000 | Loss: 0.00002040
Iteration 44/1000 | Loss: 0.00002040
Iteration 45/1000 | Loss: 0.00002040
Iteration 46/1000 | Loss: 0.00002039
Iteration 47/1000 | Loss: 0.00002039
Iteration 48/1000 | Loss: 0.00002038
Iteration 49/1000 | Loss: 0.00002037
Iteration 50/1000 | Loss: 0.00002037
Iteration 51/1000 | Loss: 0.00002037
Iteration 52/1000 | Loss: 0.00002037
Iteration 53/1000 | Loss: 0.00002037
Iteration 54/1000 | Loss: 0.00002037
Iteration 55/1000 | Loss: 0.00002036
Iteration 56/1000 | Loss: 0.00002036
Iteration 57/1000 | Loss: 0.00002036
Iteration 58/1000 | Loss: 0.00002036
Iteration 59/1000 | Loss: 0.00002036
Iteration 60/1000 | Loss: 0.00002036
Iteration 61/1000 | Loss: 0.00002035
Iteration 62/1000 | Loss: 0.00002035
Iteration 63/1000 | Loss: 0.00002035
Iteration 64/1000 | Loss: 0.00002035
Iteration 65/1000 | Loss: 0.00002035
Iteration 66/1000 | Loss: 0.00002035
Iteration 67/1000 | Loss: 0.00002035
Iteration 68/1000 | Loss: 0.00002035
Iteration 69/1000 | Loss: 0.00002035
Iteration 70/1000 | Loss: 0.00002035
Iteration 71/1000 | Loss: 0.00002034
Iteration 72/1000 | Loss: 0.00002034
Iteration 73/1000 | Loss: 0.00002034
Iteration 74/1000 | Loss: 0.00002034
Iteration 75/1000 | Loss: 0.00002034
Iteration 76/1000 | Loss: 0.00002034
Iteration 77/1000 | Loss: 0.00002034
Iteration 78/1000 | Loss: 0.00002034
Iteration 79/1000 | Loss: 0.00002033
Iteration 80/1000 | Loss: 0.00002033
Iteration 81/1000 | Loss: 0.00002033
Iteration 82/1000 | Loss: 0.00002033
Iteration 83/1000 | Loss: 0.00002032
Iteration 84/1000 | Loss: 0.00002032
Iteration 85/1000 | Loss: 0.00002032
Iteration 86/1000 | Loss: 0.00002032
Iteration 87/1000 | Loss: 0.00002032
Iteration 88/1000 | Loss: 0.00002032
Iteration 89/1000 | Loss: 0.00002032
Iteration 90/1000 | Loss: 0.00002031
Iteration 91/1000 | Loss: 0.00002031
Iteration 92/1000 | Loss: 0.00002031
Iteration 93/1000 | Loss: 0.00002031
Iteration 94/1000 | Loss: 0.00002031
Iteration 95/1000 | Loss: 0.00002031
Iteration 96/1000 | Loss: 0.00002031
Iteration 97/1000 | Loss: 0.00002031
Iteration 98/1000 | Loss: 0.00002031
Iteration 99/1000 | Loss: 0.00002031
Iteration 100/1000 | Loss: 0.00002031
Iteration 101/1000 | Loss: 0.00002030
Iteration 102/1000 | Loss: 0.00002030
Iteration 103/1000 | Loss: 0.00002030
Iteration 104/1000 | Loss: 0.00002030
Iteration 105/1000 | Loss: 0.00002030
Iteration 106/1000 | Loss: 0.00002030
Iteration 107/1000 | Loss: 0.00002030
Iteration 108/1000 | Loss: 0.00002030
Iteration 109/1000 | Loss: 0.00002030
Iteration 110/1000 | Loss: 0.00002030
Iteration 111/1000 | Loss: 0.00002030
Iteration 112/1000 | Loss: 0.00002030
Iteration 113/1000 | Loss: 0.00002030
Iteration 114/1000 | Loss: 0.00002030
Iteration 115/1000 | Loss: 0.00002030
Iteration 116/1000 | Loss: 0.00002030
Iteration 117/1000 | Loss: 0.00002030
Iteration 118/1000 | Loss: 0.00002030
Iteration 119/1000 | Loss: 0.00002030
Iteration 120/1000 | Loss: 0.00002030
Iteration 121/1000 | Loss: 0.00002030
Iteration 122/1000 | Loss: 0.00002030
Iteration 123/1000 | Loss: 0.00002030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.029813185799867e-05, 2.029813185799867e-05, 2.029813185799867e-05, 2.029813185799867e-05, 2.029813185799867e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.029813185799867e-05

Optimization complete. Final v2v error: 3.6707427501678467 mm

Highest mean error: 4.851318836212158 mm for frame 14

Lowest mean error: 3.396491765975952 mm for frame 151

Saving results

Total time: 66.59931564331055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959992
Iteration 2/25 | Loss: 0.00306973
Iteration 3/25 | Loss: 0.00213897
Iteration 4/25 | Loss: 0.00197026
Iteration 5/25 | Loss: 0.00183136
Iteration 6/25 | Loss: 0.00177421
Iteration 7/25 | Loss: 0.00188771
Iteration 8/25 | Loss: 0.00193660
Iteration 9/25 | Loss: 0.00179862
Iteration 10/25 | Loss: 0.00174413
Iteration 11/25 | Loss: 0.00164048
Iteration 12/25 | Loss: 0.00155804
Iteration 13/25 | Loss: 0.00150585
Iteration 14/25 | Loss: 0.00144417
Iteration 15/25 | Loss: 0.00143013
Iteration 16/25 | Loss: 0.00142332
Iteration 17/25 | Loss: 0.00141544
Iteration 18/25 | Loss: 0.00140496
Iteration 19/25 | Loss: 0.00141418
Iteration 20/25 | Loss: 0.00141581
Iteration 21/25 | Loss: 0.00140729
Iteration 22/25 | Loss: 0.00141375
Iteration 23/25 | Loss: 0.00140570
Iteration 24/25 | Loss: 0.00139634
Iteration 25/25 | Loss: 0.00139468

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26275146
Iteration 2/25 | Loss: 0.00278752
Iteration 3/25 | Loss: 0.00253791
Iteration 4/25 | Loss: 0.00253787
Iteration 5/25 | Loss: 0.00253787
Iteration 6/25 | Loss: 0.00253786
Iteration 7/25 | Loss: 0.00253786
Iteration 8/25 | Loss: 0.00253786
Iteration 9/25 | Loss: 0.00253786
Iteration 10/25 | Loss: 0.00253786
Iteration 11/25 | Loss: 0.00253786
Iteration 12/25 | Loss: 0.00253786
Iteration 13/25 | Loss: 0.00253786
Iteration 14/25 | Loss: 0.00253786
Iteration 15/25 | Loss: 0.00253786
Iteration 16/25 | Loss: 0.00253786
Iteration 17/25 | Loss: 0.00253786
Iteration 18/25 | Loss: 0.00253786
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002537861466407776, 0.002537861466407776, 0.002537861466407776, 0.002537861466407776, 0.002537861466407776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002537861466407776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00253786
Iteration 2/1000 | Loss: 0.00045784
Iteration 3/1000 | Loss: 0.00068843
Iteration 4/1000 | Loss: 0.00015279
Iteration 5/1000 | Loss: 0.00026580
Iteration 6/1000 | Loss: 0.00012526
Iteration 7/1000 | Loss: 0.00025556
Iteration 8/1000 | Loss: 0.00023292
Iteration 9/1000 | Loss: 0.00015122
Iteration 10/1000 | Loss: 0.00010499
Iteration 11/1000 | Loss: 0.00028843
Iteration 12/1000 | Loss: 0.00013057
Iteration 13/1000 | Loss: 0.00010005
Iteration 14/1000 | Loss: 0.00009762
Iteration 15/1000 | Loss: 0.00033911
Iteration 16/1000 | Loss: 0.00131379
Iteration 17/1000 | Loss: 0.00472290
Iteration 18/1000 | Loss: 0.00362025
Iteration 19/1000 | Loss: 0.00218977
Iteration 20/1000 | Loss: 0.00112281
Iteration 21/1000 | Loss: 0.00085368
Iteration 22/1000 | Loss: 0.00028112
Iteration 23/1000 | Loss: 0.00055804
Iteration 24/1000 | Loss: 0.00029692
Iteration 25/1000 | Loss: 0.00032427
Iteration 26/1000 | Loss: 0.00022616
Iteration 27/1000 | Loss: 0.00020129
Iteration 28/1000 | Loss: 0.00020097
Iteration 29/1000 | Loss: 0.00007120
Iteration 30/1000 | Loss: 0.00023051
Iteration 31/1000 | Loss: 0.00009743
Iteration 32/1000 | Loss: 0.00006500
Iteration 33/1000 | Loss: 0.00005360
Iteration 34/1000 | Loss: 0.00056797
Iteration 35/1000 | Loss: 0.00019634
Iteration 36/1000 | Loss: 0.00006617
Iteration 37/1000 | Loss: 0.00008433
Iteration 38/1000 | Loss: 0.00009514
Iteration 39/1000 | Loss: 0.00005643
Iteration 40/1000 | Loss: 0.00011045
Iteration 41/1000 | Loss: 0.00004945
Iteration 42/1000 | Loss: 0.00079101
Iteration 43/1000 | Loss: 0.00040908
Iteration 44/1000 | Loss: 0.00006795
Iteration 45/1000 | Loss: 0.00005327
Iteration 46/1000 | Loss: 0.00004220
Iteration 47/1000 | Loss: 0.00004069
Iteration 48/1000 | Loss: 0.00009361
Iteration 49/1000 | Loss: 0.00009841
Iteration 50/1000 | Loss: 0.00004083
Iteration 51/1000 | Loss: 0.00003851
Iteration 52/1000 | Loss: 0.00003794
Iteration 53/1000 | Loss: 0.00028339
Iteration 54/1000 | Loss: 0.00014080
Iteration 55/1000 | Loss: 0.00004240
Iteration 56/1000 | Loss: 0.00003803
Iteration 57/1000 | Loss: 0.00012176
Iteration 58/1000 | Loss: 0.00004983
Iteration 59/1000 | Loss: 0.00003514
Iteration 60/1000 | Loss: 0.00005044
Iteration 61/1000 | Loss: 0.00004279
Iteration 62/1000 | Loss: 0.00003433
Iteration 63/1000 | Loss: 0.00003403
Iteration 64/1000 | Loss: 0.00003389
Iteration 65/1000 | Loss: 0.00003366
Iteration 66/1000 | Loss: 0.00003347
Iteration 67/1000 | Loss: 0.00003341
Iteration 68/1000 | Loss: 0.00011403
Iteration 69/1000 | Loss: 0.00003319
Iteration 70/1000 | Loss: 0.00003306
Iteration 71/1000 | Loss: 0.00003286
Iteration 72/1000 | Loss: 0.00003265
Iteration 73/1000 | Loss: 0.00003235
Iteration 74/1000 | Loss: 0.00010919
Iteration 75/1000 | Loss: 0.00017384
Iteration 76/1000 | Loss: 0.00005181
Iteration 77/1000 | Loss: 0.00004730
Iteration 78/1000 | Loss: 0.00003193
Iteration 79/1000 | Loss: 0.00003174
Iteration 80/1000 | Loss: 0.00037289
Iteration 81/1000 | Loss: 0.00022148
Iteration 82/1000 | Loss: 0.00003229
Iteration 83/1000 | Loss: 0.00003167
Iteration 84/1000 | Loss: 0.00037033
Iteration 85/1000 | Loss: 0.00003668
Iteration 86/1000 | Loss: 0.00019133
Iteration 87/1000 | Loss: 0.00003387
Iteration 88/1000 | Loss: 0.00010114
Iteration 89/1000 | Loss: 0.00003247
Iteration 90/1000 | Loss: 0.00008566
Iteration 91/1000 | Loss: 0.00007100
Iteration 92/1000 | Loss: 0.00063864
Iteration 93/1000 | Loss: 0.00008494
Iteration 94/1000 | Loss: 0.00003098
Iteration 95/1000 | Loss: 0.00004923
Iteration 96/1000 | Loss: 0.00003058
Iteration 97/1000 | Loss: 0.00003057
Iteration 98/1000 | Loss: 0.00003048
Iteration 99/1000 | Loss: 0.00003047
Iteration 100/1000 | Loss: 0.00003044
Iteration 101/1000 | Loss: 0.00003044
Iteration 102/1000 | Loss: 0.00003044
Iteration 103/1000 | Loss: 0.00003043
Iteration 104/1000 | Loss: 0.00003042
Iteration 105/1000 | Loss: 0.00003042
Iteration 106/1000 | Loss: 0.00003041
Iteration 107/1000 | Loss: 0.00003041
Iteration 108/1000 | Loss: 0.00003039
Iteration 109/1000 | Loss: 0.00003039
Iteration 110/1000 | Loss: 0.00003037
Iteration 111/1000 | Loss: 0.00003037
Iteration 112/1000 | Loss: 0.00003036
Iteration 113/1000 | Loss: 0.00003036
Iteration 114/1000 | Loss: 0.00003036
Iteration 115/1000 | Loss: 0.00003036
Iteration 116/1000 | Loss: 0.00003036
Iteration 117/1000 | Loss: 0.00003035
Iteration 118/1000 | Loss: 0.00003035
Iteration 119/1000 | Loss: 0.00003035
Iteration 120/1000 | Loss: 0.00003035
Iteration 121/1000 | Loss: 0.00003035
Iteration 122/1000 | Loss: 0.00003035
Iteration 123/1000 | Loss: 0.00003034
Iteration 124/1000 | Loss: 0.00003034
Iteration 125/1000 | Loss: 0.00003033
Iteration 126/1000 | Loss: 0.00003033
Iteration 127/1000 | Loss: 0.00003032
Iteration 128/1000 | Loss: 0.00003032
Iteration 129/1000 | Loss: 0.00003032
Iteration 130/1000 | Loss: 0.00003032
Iteration 131/1000 | Loss: 0.00003032
Iteration 132/1000 | Loss: 0.00003032
Iteration 133/1000 | Loss: 0.00003032
Iteration 134/1000 | Loss: 0.00003032
Iteration 135/1000 | Loss: 0.00003032
Iteration 136/1000 | Loss: 0.00003031
Iteration 137/1000 | Loss: 0.00003031
Iteration 138/1000 | Loss: 0.00003031
Iteration 139/1000 | Loss: 0.00003031
Iteration 140/1000 | Loss: 0.00003031
Iteration 141/1000 | Loss: 0.00003031
Iteration 142/1000 | Loss: 0.00003031
Iteration 143/1000 | Loss: 0.00003031
Iteration 144/1000 | Loss: 0.00003031
Iteration 145/1000 | Loss: 0.00003031
Iteration 146/1000 | Loss: 0.00003031
Iteration 147/1000 | Loss: 0.00003031
Iteration 148/1000 | Loss: 0.00003031
Iteration 149/1000 | Loss: 0.00003031
Iteration 150/1000 | Loss: 0.00003031
Iteration 151/1000 | Loss: 0.00003031
Iteration 152/1000 | Loss: 0.00003031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [3.031319283763878e-05, 3.031319283763878e-05, 3.031319283763878e-05, 3.031319283763878e-05, 3.031319283763878e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.031319283763878e-05

Optimization complete. Final v2v error: 3.6566622257232666 mm

Highest mean error: 11.058642387390137 mm for frame 45

Lowest mean error: 3.057248830795288 mm for frame 137

Saving results

Total time: 180.04313158988953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419528
Iteration 2/25 | Loss: 0.00129070
Iteration 3/25 | Loss: 0.00121340
Iteration 4/25 | Loss: 0.00119366
Iteration 5/25 | Loss: 0.00118698
Iteration 6/25 | Loss: 0.00118539
Iteration 7/25 | Loss: 0.00118537
Iteration 8/25 | Loss: 0.00118537
Iteration 9/25 | Loss: 0.00118537
Iteration 10/25 | Loss: 0.00118537
Iteration 11/25 | Loss: 0.00118537
Iteration 12/25 | Loss: 0.00118537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011853661853820086, 0.0011853661853820086, 0.0011853661853820086, 0.0011853661853820086, 0.0011853661853820086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011853661853820086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76464438
Iteration 2/25 | Loss: 0.00132890
Iteration 3/25 | Loss: 0.00132889
Iteration 4/25 | Loss: 0.00132889
Iteration 5/25 | Loss: 0.00132889
Iteration 6/25 | Loss: 0.00132889
Iteration 7/25 | Loss: 0.00132889
Iteration 8/25 | Loss: 0.00132889
Iteration 9/25 | Loss: 0.00132889
Iteration 10/25 | Loss: 0.00132889
Iteration 11/25 | Loss: 0.00132889
Iteration 12/25 | Loss: 0.00132889
Iteration 13/25 | Loss: 0.00132889
Iteration 14/25 | Loss: 0.00132889
Iteration 15/25 | Loss: 0.00132889
Iteration 16/25 | Loss: 0.00132889
Iteration 17/25 | Loss: 0.00132889
Iteration 18/25 | Loss: 0.00132889
Iteration 19/25 | Loss: 0.00132889
Iteration 20/25 | Loss: 0.00132889
Iteration 21/25 | Loss: 0.00132889
Iteration 22/25 | Loss: 0.00132889
Iteration 23/25 | Loss: 0.00132889
Iteration 24/25 | Loss: 0.00132889
Iteration 25/25 | Loss: 0.00132889

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132889
Iteration 2/1000 | Loss: 0.00002391
Iteration 3/1000 | Loss: 0.00001693
Iteration 4/1000 | Loss: 0.00001566
Iteration 5/1000 | Loss: 0.00001488
Iteration 6/1000 | Loss: 0.00001450
Iteration 7/1000 | Loss: 0.00001403
Iteration 8/1000 | Loss: 0.00001373
Iteration 9/1000 | Loss: 0.00001370
Iteration 10/1000 | Loss: 0.00001343
Iteration 11/1000 | Loss: 0.00001317
Iteration 12/1000 | Loss: 0.00001310
Iteration 13/1000 | Loss: 0.00001306
Iteration 14/1000 | Loss: 0.00001292
Iteration 15/1000 | Loss: 0.00001287
Iteration 16/1000 | Loss: 0.00001286
Iteration 17/1000 | Loss: 0.00001285
Iteration 18/1000 | Loss: 0.00001276
Iteration 19/1000 | Loss: 0.00001271
Iteration 20/1000 | Loss: 0.00001265
Iteration 21/1000 | Loss: 0.00001262
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001259
Iteration 24/1000 | Loss: 0.00001258
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001256
Iteration 27/1000 | Loss: 0.00001256
Iteration 28/1000 | Loss: 0.00001255
Iteration 29/1000 | Loss: 0.00001255
Iteration 30/1000 | Loss: 0.00001255
Iteration 31/1000 | Loss: 0.00001254
Iteration 32/1000 | Loss: 0.00001254
Iteration 33/1000 | Loss: 0.00001254
Iteration 34/1000 | Loss: 0.00001254
Iteration 35/1000 | Loss: 0.00001254
Iteration 36/1000 | Loss: 0.00001253
Iteration 37/1000 | Loss: 0.00001253
Iteration 38/1000 | Loss: 0.00001252
Iteration 39/1000 | Loss: 0.00001252
Iteration 40/1000 | Loss: 0.00001252
Iteration 41/1000 | Loss: 0.00001251
Iteration 42/1000 | Loss: 0.00001251
Iteration 43/1000 | Loss: 0.00001251
Iteration 44/1000 | Loss: 0.00001250
Iteration 45/1000 | Loss: 0.00001250
Iteration 46/1000 | Loss: 0.00001250
Iteration 47/1000 | Loss: 0.00001249
Iteration 48/1000 | Loss: 0.00001249
Iteration 49/1000 | Loss: 0.00001245
Iteration 50/1000 | Loss: 0.00001244
Iteration 51/1000 | Loss: 0.00001244
Iteration 52/1000 | Loss: 0.00001243
Iteration 53/1000 | Loss: 0.00001243
Iteration 54/1000 | Loss: 0.00001242
Iteration 55/1000 | Loss: 0.00001240
Iteration 56/1000 | Loss: 0.00001240
Iteration 57/1000 | Loss: 0.00001240
Iteration 58/1000 | Loss: 0.00001240
Iteration 59/1000 | Loss: 0.00001240
Iteration 60/1000 | Loss: 0.00001239
Iteration 61/1000 | Loss: 0.00001239
Iteration 62/1000 | Loss: 0.00001239
Iteration 63/1000 | Loss: 0.00001239
Iteration 64/1000 | Loss: 0.00001239
Iteration 65/1000 | Loss: 0.00001237
Iteration 66/1000 | Loss: 0.00001237
Iteration 67/1000 | Loss: 0.00001237
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001236
Iteration 70/1000 | Loss: 0.00001236
Iteration 71/1000 | Loss: 0.00001236
Iteration 72/1000 | Loss: 0.00001236
Iteration 73/1000 | Loss: 0.00001236
Iteration 74/1000 | Loss: 0.00001236
Iteration 75/1000 | Loss: 0.00001236
Iteration 76/1000 | Loss: 0.00001236
Iteration 77/1000 | Loss: 0.00001235
Iteration 78/1000 | Loss: 0.00001235
Iteration 79/1000 | Loss: 0.00001235
Iteration 80/1000 | Loss: 0.00001235
Iteration 81/1000 | Loss: 0.00001234
Iteration 82/1000 | Loss: 0.00001234
Iteration 83/1000 | Loss: 0.00001233
Iteration 84/1000 | Loss: 0.00001233
Iteration 85/1000 | Loss: 0.00001233
Iteration 86/1000 | Loss: 0.00001233
Iteration 87/1000 | Loss: 0.00001233
Iteration 88/1000 | Loss: 0.00001233
Iteration 89/1000 | Loss: 0.00001233
Iteration 90/1000 | Loss: 0.00001232
Iteration 91/1000 | Loss: 0.00001232
Iteration 92/1000 | Loss: 0.00001232
Iteration 93/1000 | Loss: 0.00001232
Iteration 94/1000 | Loss: 0.00001231
Iteration 95/1000 | Loss: 0.00001231
Iteration 96/1000 | Loss: 0.00001231
Iteration 97/1000 | Loss: 0.00001230
Iteration 98/1000 | Loss: 0.00001230
Iteration 99/1000 | Loss: 0.00001230
Iteration 100/1000 | Loss: 0.00001230
Iteration 101/1000 | Loss: 0.00001230
Iteration 102/1000 | Loss: 0.00001230
Iteration 103/1000 | Loss: 0.00001229
Iteration 104/1000 | Loss: 0.00001229
Iteration 105/1000 | Loss: 0.00001229
Iteration 106/1000 | Loss: 0.00001228
Iteration 107/1000 | Loss: 0.00001228
Iteration 108/1000 | Loss: 0.00001228
Iteration 109/1000 | Loss: 0.00001228
Iteration 110/1000 | Loss: 0.00001227
Iteration 111/1000 | Loss: 0.00001227
Iteration 112/1000 | Loss: 0.00001227
Iteration 113/1000 | Loss: 0.00001227
Iteration 114/1000 | Loss: 0.00001227
Iteration 115/1000 | Loss: 0.00001227
Iteration 116/1000 | Loss: 0.00001227
Iteration 117/1000 | Loss: 0.00001227
Iteration 118/1000 | Loss: 0.00001226
Iteration 119/1000 | Loss: 0.00001226
Iteration 120/1000 | Loss: 0.00001226
Iteration 121/1000 | Loss: 0.00001226
Iteration 122/1000 | Loss: 0.00001226
Iteration 123/1000 | Loss: 0.00001226
Iteration 124/1000 | Loss: 0.00001226
Iteration 125/1000 | Loss: 0.00001226
Iteration 126/1000 | Loss: 0.00001226
Iteration 127/1000 | Loss: 0.00001226
Iteration 128/1000 | Loss: 0.00001226
Iteration 129/1000 | Loss: 0.00001226
Iteration 130/1000 | Loss: 0.00001225
Iteration 131/1000 | Loss: 0.00001225
Iteration 132/1000 | Loss: 0.00001225
Iteration 133/1000 | Loss: 0.00001225
Iteration 134/1000 | Loss: 0.00001225
Iteration 135/1000 | Loss: 0.00001225
Iteration 136/1000 | Loss: 0.00001225
Iteration 137/1000 | Loss: 0.00001225
Iteration 138/1000 | Loss: 0.00001225
Iteration 139/1000 | Loss: 0.00001225
Iteration 140/1000 | Loss: 0.00001225
Iteration 141/1000 | Loss: 0.00001225
Iteration 142/1000 | Loss: 0.00001225
Iteration 143/1000 | Loss: 0.00001225
Iteration 144/1000 | Loss: 0.00001225
Iteration 145/1000 | Loss: 0.00001225
Iteration 146/1000 | Loss: 0.00001225
Iteration 147/1000 | Loss: 0.00001225
Iteration 148/1000 | Loss: 0.00001225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.2251141015440226e-05, 1.2251141015440226e-05, 1.2251141015440226e-05, 1.2251141015440226e-05, 1.2251141015440226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2251141015440226e-05

Optimization complete. Final v2v error: 3.02056622505188 mm

Highest mean error: 3.5483133792877197 mm for frame 108

Lowest mean error: 2.7941510677337646 mm for frame 129

Saving results

Total time: 38.18453812599182
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470330
Iteration 2/25 | Loss: 0.00141166
Iteration 3/25 | Loss: 0.00126064
Iteration 4/25 | Loss: 0.00123525
Iteration 5/25 | Loss: 0.00122776
Iteration 6/25 | Loss: 0.00122603
Iteration 7/25 | Loss: 0.00122603
Iteration 8/25 | Loss: 0.00122603
Iteration 9/25 | Loss: 0.00122603
Iteration 10/25 | Loss: 0.00122603
Iteration 11/25 | Loss: 0.00122603
Iteration 12/25 | Loss: 0.00122603
Iteration 13/25 | Loss: 0.00122603
Iteration 14/25 | Loss: 0.00122603
Iteration 15/25 | Loss: 0.00122603
Iteration 16/25 | Loss: 0.00122603
Iteration 17/25 | Loss: 0.00122603
Iteration 18/25 | Loss: 0.00122603
Iteration 19/25 | Loss: 0.00122603
Iteration 20/25 | Loss: 0.00122603
Iteration 21/25 | Loss: 0.00122603
Iteration 22/25 | Loss: 0.00122603
Iteration 23/25 | Loss: 0.00122603
Iteration 24/25 | Loss: 0.00122603
Iteration 25/25 | Loss: 0.00122603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15487456
Iteration 2/25 | Loss: 0.00167554
Iteration 3/25 | Loss: 0.00167553
Iteration 4/25 | Loss: 0.00167553
Iteration 5/25 | Loss: 0.00167553
Iteration 6/25 | Loss: 0.00167553
Iteration 7/25 | Loss: 0.00167553
Iteration 8/25 | Loss: 0.00167553
Iteration 9/25 | Loss: 0.00167553
Iteration 10/25 | Loss: 0.00167553
Iteration 11/25 | Loss: 0.00167553
Iteration 12/25 | Loss: 0.00167553
Iteration 13/25 | Loss: 0.00167553
Iteration 14/25 | Loss: 0.00167553
Iteration 15/25 | Loss: 0.00167553
Iteration 16/25 | Loss: 0.00167553
Iteration 17/25 | Loss: 0.00167553
Iteration 18/25 | Loss: 0.00167553
Iteration 19/25 | Loss: 0.00167553
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001675529289059341, 0.001675529289059341, 0.001675529289059341, 0.001675529289059341, 0.001675529289059341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001675529289059341

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167553
Iteration 2/1000 | Loss: 0.00004317
Iteration 3/1000 | Loss: 0.00002734
Iteration 4/1000 | Loss: 0.00002244
Iteration 5/1000 | Loss: 0.00002085
Iteration 6/1000 | Loss: 0.00001958
Iteration 7/1000 | Loss: 0.00001882
Iteration 8/1000 | Loss: 0.00001830
Iteration 9/1000 | Loss: 0.00001793
Iteration 10/1000 | Loss: 0.00001765
Iteration 11/1000 | Loss: 0.00001739
Iteration 12/1000 | Loss: 0.00001729
Iteration 13/1000 | Loss: 0.00001711
Iteration 14/1000 | Loss: 0.00001706
Iteration 15/1000 | Loss: 0.00001703
Iteration 16/1000 | Loss: 0.00001701
Iteration 17/1000 | Loss: 0.00001698
Iteration 18/1000 | Loss: 0.00001697
Iteration 19/1000 | Loss: 0.00001695
Iteration 20/1000 | Loss: 0.00001692
Iteration 21/1000 | Loss: 0.00001690
Iteration 22/1000 | Loss: 0.00001686
Iteration 23/1000 | Loss: 0.00001685
Iteration 24/1000 | Loss: 0.00001684
Iteration 25/1000 | Loss: 0.00001683
Iteration 26/1000 | Loss: 0.00001682
Iteration 27/1000 | Loss: 0.00001682
Iteration 28/1000 | Loss: 0.00001681
Iteration 29/1000 | Loss: 0.00001681
Iteration 30/1000 | Loss: 0.00001680
Iteration 31/1000 | Loss: 0.00001679
Iteration 32/1000 | Loss: 0.00001675
Iteration 33/1000 | Loss: 0.00001674
Iteration 34/1000 | Loss: 0.00001671
Iteration 35/1000 | Loss: 0.00001671
Iteration 36/1000 | Loss: 0.00001669
Iteration 37/1000 | Loss: 0.00001669
Iteration 38/1000 | Loss: 0.00001668
Iteration 39/1000 | Loss: 0.00001667
Iteration 40/1000 | Loss: 0.00001665
Iteration 41/1000 | Loss: 0.00001665
Iteration 42/1000 | Loss: 0.00001664
Iteration 43/1000 | Loss: 0.00001664
Iteration 44/1000 | Loss: 0.00001663
Iteration 45/1000 | Loss: 0.00001663
Iteration 46/1000 | Loss: 0.00001663
Iteration 47/1000 | Loss: 0.00001662
Iteration 48/1000 | Loss: 0.00001662
Iteration 49/1000 | Loss: 0.00001662
Iteration 50/1000 | Loss: 0.00001661
Iteration 51/1000 | Loss: 0.00001660
Iteration 52/1000 | Loss: 0.00001660
Iteration 53/1000 | Loss: 0.00001656
Iteration 54/1000 | Loss: 0.00001656
Iteration 55/1000 | Loss: 0.00001655
Iteration 56/1000 | Loss: 0.00001654
Iteration 57/1000 | Loss: 0.00001654
Iteration 58/1000 | Loss: 0.00001654
Iteration 59/1000 | Loss: 0.00001653
Iteration 60/1000 | Loss: 0.00001653
Iteration 61/1000 | Loss: 0.00001653
Iteration 62/1000 | Loss: 0.00001652
Iteration 63/1000 | Loss: 0.00001652
Iteration 64/1000 | Loss: 0.00001651
Iteration 65/1000 | Loss: 0.00001651
Iteration 66/1000 | Loss: 0.00001650
Iteration 67/1000 | Loss: 0.00001650
Iteration 68/1000 | Loss: 0.00001649
Iteration 69/1000 | Loss: 0.00001649
Iteration 70/1000 | Loss: 0.00001649
Iteration 71/1000 | Loss: 0.00001648
Iteration 72/1000 | Loss: 0.00001647
Iteration 73/1000 | Loss: 0.00001647
Iteration 74/1000 | Loss: 0.00001647
Iteration 75/1000 | Loss: 0.00001647
Iteration 76/1000 | Loss: 0.00001647
Iteration 77/1000 | Loss: 0.00001646
Iteration 78/1000 | Loss: 0.00001646
Iteration 79/1000 | Loss: 0.00001645
Iteration 80/1000 | Loss: 0.00001645
Iteration 81/1000 | Loss: 0.00001645
Iteration 82/1000 | Loss: 0.00001645
Iteration 83/1000 | Loss: 0.00001645
Iteration 84/1000 | Loss: 0.00001644
Iteration 85/1000 | Loss: 0.00001644
Iteration 86/1000 | Loss: 0.00001644
Iteration 87/1000 | Loss: 0.00001644
Iteration 88/1000 | Loss: 0.00001644
Iteration 89/1000 | Loss: 0.00001644
Iteration 90/1000 | Loss: 0.00001644
Iteration 91/1000 | Loss: 0.00001643
Iteration 92/1000 | Loss: 0.00001643
Iteration 93/1000 | Loss: 0.00001643
Iteration 94/1000 | Loss: 0.00001642
Iteration 95/1000 | Loss: 0.00001642
Iteration 96/1000 | Loss: 0.00001642
Iteration 97/1000 | Loss: 0.00001641
Iteration 98/1000 | Loss: 0.00001641
Iteration 99/1000 | Loss: 0.00001641
Iteration 100/1000 | Loss: 0.00001641
Iteration 101/1000 | Loss: 0.00001640
Iteration 102/1000 | Loss: 0.00001640
Iteration 103/1000 | Loss: 0.00001640
Iteration 104/1000 | Loss: 0.00001640
Iteration 105/1000 | Loss: 0.00001639
Iteration 106/1000 | Loss: 0.00001639
Iteration 107/1000 | Loss: 0.00001639
Iteration 108/1000 | Loss: 0.00001638
Iteration 109/1000 | Loss: 0.00001638
Iteration 110/1000 | Loss: 0.00001638
Iteration 111/1000 | Loss: 0.00001637
Iteration 112/1000 | Loss: 0.00001637
Iteration 113/1000 | Loss: 0.00001637
Iteration 114/1000 | Loss: 0.00001636
Iteration 115/1000 | Loss: 0.00001636
Iteration 116/1000 | Loss: 0.00001636
Iteration 117/1000 | Loss: 0.00001635
Iteration 118/1000 | Loss: 0.00001635
Iteration 119/1000 | Loss: 0.00001635
Iteration 120/1000 | Loss: 0.00001635
Iteration 121/1000 | Loss: 0.00001634
Iteration 122/1000 | Loss: 0.00001634
Iteration 123/1000 | Loss: 0.00001634
Iteration 124/1000 | Loss: 0.00001633
Iteration 125/1000 | Loss: 0.00001633
Iteration 126/1000 | Loss: 0.00001633
Iteration 127/1000 | Loss: 0.00001633
Iteration 128/1000 | Loss: 0.00001633
Iteration 129/1000 | Loss: 0.00001633
Iteration 130/1000 | Loss: 0.00001633
Iteration 131/1000 | Loss: 0.00001633
Iteration 132/1000 | Loss: 0.00001633
Iteration 133/1000 | Loss: 0.00001633
Iteration 134/1000 | Loss: 0.00001633
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.6331468941643834e-05, 1.6331468941643834e-05, 1.6331468941643834e-05, 1.6331468941643834e-05, 1.6331468941643834e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6331468941643834e-05

Optimization complete. Final v2v error: 3.375779390335083 mm

Highest mean error: 4.746103286743164 mm for frame 66

Lowest mean error: 2.6728086471557617 mm for frame 78

Saving results

Total time: 45.56080508232117
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027002
Iteration 2/25 | Loss: 0.01027002
Iteration 3/25 | Loss: 0.01027002
Iteration 4/25 | Loss: 0.01027002
Iteration 5/25 | Loss: 0.01027002
Iteration 6/25 | Loss: 0.01027002
Iteration 7/25 | Loss: 0.01027002
Iteration 8/25 | Loss: 0.01027002
Iteration 9/25 | Loss: 0.01027001
Iteration 10/25 | Loss: 0.01027001
Iteration 11/25 | Loss: 0.01027001
Iteration 12/25 | Loss: 0.01027001
Iteration 13/25 | Loss: 0.01027001
Iteration 14/25 | Loss: 0.01027001
Iteration 15/25 | Loss: 0.01027001
Iteration 16/25 | Loss: 0.01027001
Iteration 17/25 | Loss: 0.01027001
Iteration 18/25 | Loss: 0.01027000
Iteration 19/25 | Loss: 0.01027000
Iteration 20/25 | Loss: 0.01027000
Iteration 21/25 | Loss: 0.01027000
Iteration 22/25 | Loss: 0.01027000
Iteration 23/25 | Loss: 0.01027000
Iteration 24/25 | Loss: 0.01027000
Iteration 25/25 | Loss: 0.01027000

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76988828
Iteration 2/25 | Loss: 0.09907372
Iteration 3/25 | Loss: 0.09062993
Iteration 4/25 | Loss: 0.08910117
Iteration 5/25 | Loss: 0.08900300
Iteration 6/25 | Loss: 0.08900297
Iteration 7/25 | Loss: 0.08900296
Iteration 8/25 | Loss: 0.08900295
Iteration 9/25 | Loss: 0.08900295
Iteration 10/25 | Loss: 0.08900295
Iteration 11/25 | Loss: 0.08900295
Iteration 12/25 | Loss: 0.08900294
Iteration 13/25 | Loss: 0.08900295
Iteration 14/25 | Loss: 0.08900295
Iteration 15/25 | Loss: 0.08900295
Iteration 16/25 | Loss: 0.08900294
Iteration 17/25 | Loss: 0.08900294
Iteration 18/25 | Loss: 0.08900294
Iteration 19/25 | Loss: 0.08900294
Iteration 20/25 | Loss: 0.08900294
Iteration 21/25 | Loss: 0.08900294
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.08900294452905655, 0.08900294452905655, 0.08900294452905655, 0.08900294452905655, 0.08900294452905655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08900294452905655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08900294
Iteration 2/1000 | Loss: 0.00313185
Iteration 3/1000 | Loss: 0.00217294
Iteration 4/1000 | Loss: 0.00036940
Iteration 5/1000 | Loss: 0.00097484
Iteration 6/1000 | Loss: 0.00023030
Iteration 7/1000 | Loss: 0.00026444
Iteration 8/1000 | Loss: 0.00007655
Iteration 9/1000 | Loss: 0.00005948
Iteration 10/1000 | Loss: 0.00004052
Iteration 11/1000 | Loss: 0.00003455
Iteration 12/1000 | Loss: 0.00017636
Iteration 13/1000 | Loss: 0.00002935
Iteration 14/1000 | Loss: 0.00003524
Iteration 15/1000 | Loss: 0.00003511
Iteration 16/1000 | Loss: 0.00006515
Iteration 17/1000 | Loss: 0.00004567
Iteration 18/1000 | Loss: 0.00010061
Iteration 19/1000 | Loss: 0.00002323
Iteration 20/1000 | Loss: 0.00002248
Iteration 21/1000 | Loss: 0.00003193
Iteration 22/1000 | Loss: 0.00028261
Iteration 23/1000 | Loss: 0.00003636
Iteration 24/1000 | Loss: 0.00005635
Iteration 25/1000 | Loss: 0.00007741
Iteration 26/1000 | Loss: 0.00022699
Iteration 27/1000 | Loss: 0.00002901
Iteration 28/1000 | Loss: 0.00001988
Iteration 29/1000 | Loss: 0.00001937
Iteration 30/1000 | Loss: 0.00015235
Iteration 31/1000 | Loss: 0.00004529
Iteration 32/1000 | Loss: 0.00008480
Iteration 33/1000 | Loss: 0.00001865
Iteration 34/1000 | Loss: 0.00001802
Iteration 35/1000 | Loss: 0.00011560
Iteration 36/1000 | Loss: 0.00002956
Iteration 37/1000 | Loss: 0.00004287
Iteration 38/1000 | Loss: 0.00001771
Iteration 39/1000 | Loss: 0.00001721
Iteration 40/1000 | Loss: 0.00003240
Iteration 41/1000 | Loss: 0.00001881
Iteration 42/1000 | Loss: 0.00001736
Iteration 43/1000 | Loss: 0.00001637
Iteration 44/1000 | Loss: 0.00001897
Iteration 45/1000 | Loss: 0.00001602
Iteration 46/1000 | Loss: 0.00001594
Iteration 47/1000 | Loss: 0.00001594
Iteration 48/1000 | Loss: 0.00001586
Iteration 49/1000 | Loss: 0.00001584
Iteration 50/1000 | Loss: 0.00001583
Iteration 51/1000 | Loss: 0.00001578
Iteration 52/1000 | Loss: 0.00001575
Iteration 53/1000 | Loss: 0.00001572
Iteration 54/1000 | Loss: 0.00001572
Iteration 55/1000 | Loss: 0.00001564
Iteration 56/1000 | Loss: 0.00001564
Iteration 57/1000 | Loss: 0.00001562
Iteration 58/1000 | Loss: 0.00001562
Iteration 59/1000 | Loss: 0.00001561
Iteration 60/1000 | Loss: 0.00001561
Iteration 61/1000 | Loss: 0.00001560
Iteration 62/1000 | Loss: 0.00001560
Iteration 63/1000 | Loss: 0.00001560
Iteration 64/1000 | Loss: 0.00001560
Iteration 65/1000 | Loss: 0.00001559
Iteration 66/1000 | Loss: 0.00001559
Iteration 67/1000 | Loss: 0.00001558
Iteration 68/1000 | Loss: 0.00001558
Iteration 69/1000 | Loss: 0.00001558
Iteration 70/1000 | Loss: 0.00001557
Iteration 71/1000 | Loss: 0.00001557
Iteration 72/1000 | Loss: 0.00001556
Iteration 73/1000 | Loss: 0.00001556
Iteration 74/1000 | Loss: 0.00001556
Iteration 75/1000 | Loss: 0.00001555
Iteration 76/1000 | Loss: 0.00001555
Iteration 77/1000 | Loss: 0.00001555
Iteration 78/1000 | Loss: 0.00001555
Iteration 79/1000 | Loss: 0.00001554
Iteration 80/1000 | Loss: 0.00001554
Iteration 81/1000 | Loss: 0.00001554
Iteration 82/1000 | Loss: 0.00001554
Iteration 83/1000 | Loss: 0.00001554
Iteration 84/1000 | Loss: 0.00001554
Iteration 85/1000 | Loss: 0.00001554
Iteration 86/1000 | Loss: 0.00001554
Iteration 87/1000 | Loss: 0.00001554
Iteration 88/1000 | Loss: 0.00001554
Iteration 89/1000 | Loss: 0.00001554
Iteration 90/1000 | Loss: 0.00001554
Iteration 91/1000 | Loss: 0.00001553
Iteration 92/1000 | Loss: 0.00001553
Iteration 93/1000 | Loss: 0.00001553
Iteration 94/1000 | Loss: 0.00001553
Iteration 95/1000 | Loss: 0.00001553
Iteration 96/1000 | Loss: 0.00001553
Iteration 97/1000 | Loss: 0.00001552
Iteration 98/1000 | Loss: 0.00001552
Iteration 99/1000 | Loss: 0.00001552
Iteration 100/1000 | Loss: 0.00001551
Iteration 101/1000 | Loss: 0.00001551
Iteration 102/1000 | Loss: 0.00001551
Iteration 103/1000 | Loss: 0.00001550
Iteration 104/1000 | Loss: 0.00001550
Iteration 105/1000 | Loss: 0.00001550
Iteration 106/1000 | Loss: 0.00001550
Iteration 107/1000 | Loss: 0.00001550
Iteration 108/1000 | Loss: 0.00001550
Iteration 109/1000 | Loss: 0.00001549
Iteration 110/1000 | Loss: 0.00001549
Iteration 111/1000 | Loss: 0.00004858
Iteration 112/1000 | Loss: 0.00001553
Iteration 113/1000 | Loss: 0.00001547
Iteration 114/1000 | Loss: 0.00001546
Iteration 115/1000 | Loss: 0.00001546
Iteration 116/1000 | Loss: 0.00001546
Iteration 117/1000 | Loss: 0.00001545
Iteration 118/1000 | Loss: 0.00001545
Iteration 119/1000 | Loss: 0.00001545
Iteration 120/1000 | Loss: 0.00001545
Iteration 121/1000 | Loss: 0.00001544
Iteration 122/1000 | Loss: 0.00001544
Iteration 123/1000 | Loss: 0.00001544
Iteration 124/1000 | Loss: 0.00001544
Iteration 125/1000 | Loss: 0.00001544
Iteration 126/1000 | Loss: 0.00001544
Iteration 127/1000 | Loss: 0.00001544
Iteration 128/1000 | Loss: 0.00001544
Iteration 129/1000 | Loss: 0.00001544
Iteration 130/1000 | Loss: 0.00001544
Iteration 131/1000 | Loss: 0.00001544
Iteration 132/1000 | Loss: 0.00001544
Iteration 133/1000 | Loss: 0.00001543
Iteration 134/1000 | Loss: 0.00001543
Iteration 135/1000 | Loss: 0.00001543
Iteration 136/1000 | Loss: 0.00001543
Iteration 137/1000 | Loss: 0.00001543
Iteration 138/1000 | Loss: 0.00001543
Iteration 139/1000 | Loss: 0.00001543
Iteration 140/1000 | Loss: 0.00001543
Iteration 141/1000 | Loss: 0.00001543
Iteration 142/1000 | Loss: 0.00001543
Iteration 143/1000 | Loss: 0.00001543
Iteration 144/1000 | Loss: 0.00001543
Iteration 145/1000 | Loss: 0.00001543
Iteration 146/1000 | Loss: 0.00001543
Iteration 147/1000 | Loss: 0.00001543
Iteration 148/1000 | Loss: 0.00001543
Iteration 149/1000 | Loss: 0.00001543
Iteration 150/1000 | Loss: 0.00001543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.5430283383466303e-05, 1.5430283383466303e-05, 1.5430283383466303e-05, 1.5430283383466303e-05, 1.5430283383466303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5430283383466303e-05

Optimization complete. Final v2v error: 3.3921709060668945 mm

Highest mean error: 3.6754038333892822 mm for frame 210

Lowest mean error: 3.015810012817383 mm for frame 105

Saving results

Total time: 94.4000232219696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772348
Iteration 2/25 | Loss: 0.00144280
Iteration 3/25 | Loss: 0.00125570
Iteration 4/25 | Loss: 0.00123348
Iteration 5/25 | Loss: 0.00124003
Iteration 6/25 | Loss: 0.00123233
Iteration 7/25 | Loss: 0.00122588
Iteration 8/25 | Loss: 0.00122445
Iteration 9/25 | Loss: 0.00122027
Iteration 10/25 | Loss: 0.00122144
Iteration 11/25 | Loss: 0.00121973
Iteration 12/25 | Loss: 0.00122007
Iteration 13/25 | Loss: 0.00121978
Iteration 14/25 | Loss: 0.00121923
Iteration 15/25 | Loss: 0.00121916
Iteration 16/25 | Loss: 0.00121920
Iteration 17/25 | Loss: 0.00121959
Iteration 18/25 | Loss: 0.00121905
Iteration 19/25 | Loss: 0.00121904
Iteration 20/25 | Loss: 0.00121869
Iteration 21/25 | Loss: 0.00121942
Iteration 22/25 | Loss: 0.00121884
Iteration 23/25 | Loss: 0.00121884
Iteration 24/25 | Loss: 0.00121716
Iteration 25/25 | Loss: 0.00121826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81625104
Iteration 2/25 | Loss: 0.00156777
Iteration 3/25 | Loss: 0.00156776
Iteration 4/25 | Loss: 0.00156776
Iteration 5/25 | Loss: 0.00156776
Iteration 6/25 | Loss: 0.00156776
Iteration 7/25 | Loss: 0.00156776
Iteration 8/25 | Loss: 0.00156776
Iteration 9/25 | Loss: 0.00156776
Iteration 10/25 | Loss: 0.00156776
Iteration 11/25 | Loss: 0.00156776
Iteration 12/25 | Loss: 0.00156776
Iteration 13/25 | Loss: 0.00156776
Iteration 14/25 | Loss: 0.00156776
Iteration 15/25 | Loss: 0.00156776
Iteration 16/25 | Loss: 0.00156776
Iteration 17/25 | Loss: 0.00156776
Iteration 18/25 | Loss: 0.00156776
Iteration 19/25 | Loss: 0.00156776
Iteration 20/25 | Loss: 0.00156776
Iteration 21/25 | Loss: 0.00156776
Iteration 22/25 | Loss: 0.00156776
Iteration 23/25 | Loss: 0.00156776
Iteration 24/25 | Loss: 0.00156776
Iteration 25/25 | Loss: 0.00156776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156776
Iteration 2/1000 | Loss: 0.00003037
Iteration 3/1000 | Loss: 0.00002240
Iteration 4/1000 | Loss: 0.00002011
Iteration 5/1000 | Loss: 0.00001894
Iteration 6/1000 | Loss: 0.00011030
Iteration 7/1000 | Loss: 0.00002202
Iteration 8/1000 | Loss: 0.00007303
Iteration 9/1000 | Loss: 0.00008674
Iteration 10/1000 | Loss: 0.00001821
Iteration 11/1000 | Loss: 0.00001725
Iteration 12/1000 | Loss: 0.00001655
Iteration 13/1000 | Loss: 0.00001602
Iteration 14/1000 | Loss: 0.00012558
Iteration 15/1000 | Loss: 0.00003817
Iteration 16/1000 | Loss: 0.00001589
Iteration 17/1000 | Loss: 0.00001586
Iteration 18/1000 | Loss: 0.00001575
Iteration 19/1000 | Loss: 0.00001552
Iteration 20/1000 | Loss: 0.00001538
Iteration 21/1000 | Loss: 0.00001518
Iteration 22/1000 | Loss: 0.00001518
Iteration 23/1000 | Loss: 0.00001493
Iteration 24/1000 | Loss: 0.00001471
Iteration 25/1000 | Loss: 0.00001466
Iteration 26/1000 | Loss: 0.00001465
Iteration 27/1000 | Loss: 0.00001463
Iteration 28/1000 | Loss: 0.00001460
Iteration 29/1000 | Loss: 0.00001439
Iteration 30/1000 | Loss: 0.00001412
Iteration 31/1000 | Loss: 0.00001394
Iteration 32/1000 | Loss: 0.00001390
Iteration 33/1000 | Loss: 0.00001378
Iteration 34/1000 | Loss: 0.00001370
Iteration 35/1000 | Loss: 0.00001364
Iteration 36/1000 | Loss: 0.00001363
Iteration 37/1000 | Loss: 0.00001362
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001361
Iteration 40/1000 | Loss: 0.00001360
Iteration 41/1000 | Loss: 0.00001360
Iteration 42/1000 | Loss: 0.00001360
Iteration 43/1000 | Loss: 0.00001358
Iteration 44/1000 | Loss: 0.00001357
Iteration 45/1000 | Loss: 0.00001357
Iteration 46/1000 | Loss: 0.00001490
Iteration 47/1000 | Loss: 0.00001355
Iteration 48/1000 | Loss: 0.00001341
Iteration 49/1000 | Loss: 0.00001338
Iteration 50/1000 | Loss: 0.00001324
Iteration 51/1000 | Loss: 0.00001323
Iteration 52/1000 | Loss: 0.00001322
Iteration 53/1000 | Loss: 0.00001322
Iteration 54/1000 | Loss: 0.00001322
Iteration 55/1000 | Loss: 0.00001322
Iteration 56/1000 | Loss: 0.00001322
Iteration 57/1000 | Loss: 0.00001322
Iteration 58/1000 | Loss: 0.00001322
Iteration 59/1000 | Loss: 0.00001321
Iteration 60/1000 | Loss: 0.00001321
Iteration 61/1000 | Loss: 0.00001321
Iteration 62/1000 | Loss: 0.00001321
Iteration 63/1000 | Loss: 0.00001321
Iteration 64/1000 | Loss: 0.00001320
Iteration 65/1000 | Loss: 0.00001320
Iteration 66/1000 | Loss: 0.00001320
Iteration 67/1000 | Loss: 0.00001319
Iteration 68/1000 | Loss: 0.00001319
Iteration 69/1000 | Loss: 0.00001319
Iteration 70/1000 | Loss: 0.00001318
Iteration 71/1000 | Loss: 0.00001318
Iteration 72/1000 | Loss: 0.00001318
Iteration 73/1000 | Loss: 0.00001318
Iteration 74/1000 | Loss: 0.00001317
Iteration 75/1000 | Loss: 0.00001317
Iteration 76/1000 | Loss: 0.00001317
Iteration 77/1000 | Loss: 0.00001317
Iteration 78/1000 | Loss: 0.00001317
Iteration 79/1000 | Loss: 0.00001317
Iteration 80/1000 | Loss: 0.00001317
Iteration 81/1000 | Loss: 0.00001317
Iteration 82/1000 | Loss: 0.00001317
Iteration 83/1000 | Loss: 0.00001317
Iteration 84/1000 | Loss: 0.00001317
Iteration 85/1000 | Loss: 0.00001317
Iteration 86/1000 | Loss: 0.00001316
Iteration 87/1000 | Loss: 0.00001316
Iteration 88/1000 | Loss: 0.00001316
Iteration 89/1000 | Loss: 0.00001316
Iteration 90/1000 | Loss: 0.00001316
Iteration 91/1000 | Loss: 0.00001316
Iteration 92/1000 | Loss: 0.00001316
Iteration 93/1000 | Loss: 0.00001316
Iteration 94/1000 | Loss: 0.00001315
Iteration 95/1000 | Loss: 0.00001315
Iteration 96/1000 | Loss: 0.00001315
Iteration 97/1000 | Loss: 0.00001315
Iteration 98/1000 | Loss: 0.00001315
Iteration 99/1000 | Loss: 0.00001315
Iteration 100/1000 | Loss: 0.00001315
Iteration 101/1000 | Loss: 0.00001315
Iteration 102/1000 | Loss: 0.00001314
Iteration 103/1000 | Loss: 0.00001314
Iteration 104/1000 | Loss: 0.00001314
Iteration 105/1000 | Loss: 0.00001314
Iteration 106/1000 | Loss: 0.00001314
Iteration 107/1000 | Loss: 0.00001314
Iteration 108/1000 | Loss: 0.00001314
Iteration 109/1000 | Loss: 0.00001314
Iteration 110/1000 | Loss: 0.00001314
Iteration 111/1000 | Loss: 0.00001314
Iteration 112/1000 | Loss: 0.00001314
Iteration 113/1000 | Loss: 0.00001314
Iteration 114/1000 | Loss: 0.00001314
Iteration 115/1000 | Loss: 0.00001314
Iteration 116/1000 | Loss: 0.00001314
Iteration 117/1000 | Loss: 0.00001314
Iteration 118/1000 | Loss: 0.00001314
Iteration 119/1000 | Loss: 0.00001314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.31437818708946e-05, 1.31437818708946e-05, 1.31437818708946e-05, 1.31437818708946e-05, 1.31437818708946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.31437818708946e-05

Optimization complete. Final v2v error: 3.0465638637542725 mm

Highest mean error: 4.366865158081055 mm for frame 38

Lowest mean error: 2.6719655990600586 mm for frame 188

Saving results

Total time: 106.47309374809265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468769
Iteration 2/25 | Loss: 0.00127962
Iteration 3/25 | Loss: 0.00120857
Iteration 4/25 | Loss: 0.00119691
Iteration 5/25 | Loss: 0.00119291
Iteration 6/25 | Loss: 0.00119204
Iteration 7/25 | Loss: 0.00119204
Iteration 8/25 | Loss: 0.00119204
Iteration 9/25 | Loss: 0.00119204
Iteration 10/25 | Loss: 0.00119204
Iteration 11/25 | Loss: 0.00119204
Iteration 12/25 | Loss: 0.00119204
Iteration 13/25 | Loss: 0.00119204
Iteration 14/25 | Loss: 0.00119204
Iteration 15/25 | Loss: 0.00119204
Iteration 16/25 | Loss: 0.00119204
Iteration 17/25 | Loss: 0.00119204
Iteration 18/25 | Loss: 0.00119204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011920352699235082, 0.0011920352699235082, 0.0011920352699235082, 0.0011920352699235082, 0.0011920352699235082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011920352699235082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.44821501
Iteration 2/25 | Loss: 0.00142866
Iteration 3/25 | Loss: 0.00142866
Iteration 4/25 | Loss: 0.00142866
Iteration 5/25 | Loss: 0.00142866
Iteration 6/25 | Loss: 0.00142866
Iteration 7/25 | Loss: 0.00142866
Iteration 8/25 | Loss: 0.00142866
Iteration 9/25 | Loss: 0.00142866
Iteration 10/25 | Loss: 0.00142866
Iteration 11/25 | Loss: 0.00142866
Iteration 12/25 | Loss: 0.00142866
Iteration 13/25 | Loss: 0.00142866
Iteration 14/25 | Loss: 0.00142866
Iteration 15/25 | Loss: 0.00142866
Iteration 16/25 | Loss: 0.00142866
Iteration 17/25 | Loss: 0.00142866
Iteration 18/25 | Loss: 0.00142866
Iteration 19/25 | Loss: 0.00142866
Iteration 20/25 | Loss: 0.00142866
Iteration 21/25 | Loss: 0.00142866
Iteration 22/25 | Loss: 0.00142866
Iteration 23/25 | Loss: 0.00142866
Iteration 24/25 | Loss: 0.00142866
Iteration 25/25 | Loss: 0.00142866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142866
Iteration 2/1000 | Loss: 0.00002388
Iteration 3/1000 | Loss: 0.00001575
Iteration 4/1000 | Loss: 0.00001427
Iteration 5/1000 | Loss: 0.00001330
Iteration 6/1000 | Loss: 0.00001290
Iteration 7/1000 | Loss: 0.00001252
Iteration 8/1000 | Loss: 0.00001221
Iteration 9/1000 | Loss: 0.00001209
Iteration 10/1000 | Loss: 0.00001209
Iteration 11/1000 | Loss: 0.00001191
Iteration 12/1000 | Loss: 0.00001189
Iteration 13/1000 | Loss: 0.00001180
Iteration 14/1000 | Loss: 0.00001174
Iteration 15/1000 | Loss: 0.00001170
Iteration 16/1000 | Loss: 0.00001168
Iteration 17/1000 | Loss: 0.00001166
Iteration 18/1000 | Loss: 0.00001162
Iteration 19/1000 | Loss: 0.00001154
Iteration 20/1000 | Loss: 0.00001151
Iteration 21/1000 | Loss: 0.00001144
Iteration 22/1000 | Loss: 0.00001136
Iteration 23/1000 | Loss: 0.00001136
Iteration 24/1000 | Loss: 0.00001136
Iteration 25/1000 | Loss: 0.00001135
Iteration 26/1000 | Loss: 0.00001134
Iteration 27/1000 | Loss: 0.00001133
Iteration 28/1000 | Loss: 0.00001133
Iteration 29/1000 | Loss: 0.00001132
Iteration 30/1000 | Loss: 0.00001132
Iteration 31/1000 | Loss: 0.00001131
Iteration 32/1000 | Loss: 0.00001131
Iteration 33/1000 | Loss: 0.00001130
Iteration 34/1000 | Loss: 0.00001130
Iteration 35/1000 | Loss: 0.00001129
Iteration 36/1000 | Loss: 0.00001128
Iteration 37/1000 | Loss: 0.00001128
Iteration 38/1000 | Loss: 0.00001128
Iteration 39/1000 | Loss: 0.00001128
Iteration 40/1000 | Loss: 0.00001128
Iteration 41/1000 | Loss: 0.00001127
Iteration 42/1000 | Loss: 0.00001127
Iteration 43/1000 | Loss: 0.00001126
Iteration 44/1000 | Loss: 0.00001126
Iteration 45/1000 | Loss: 0.00001126
Iteration 46/1000 | Loss: 0.00001126
Iteration 47/1000 | Loss: 0.00001125
Iteration 48/1000 | Loss: 0.00001125
Iteration 49/1000 | Loss: 0.00001125
Iteration 50/1000 | Loss: 0.00001124
Iteration 51/1000 | Loss: 0.00001124
Iteration 52/1000 | Loss: 0.00001123
Iteration 53/1000 | Loss: 0.00001123
Iteration 54/1000 | Loss: 0.00001123
Iteration 55/1000 | Loss: 0.00001123
Iteration 56/1000 | Loss: 0.00001122
Iteration 57/1000 | Loss: 0.00001122
Iteration 58/1000 | Loss: 0.00001121
Iteration 59/1000 | Loss: 0.00001121
Iteration 60/1000 | Loss: 0.00001120
Iteration 61/1000 | Loss: 0.00001120
Iteration 62/1000 | Loss: 0.00001120
Iteration 63/1000 | Loss: 0.00001119
Iteration 64/1000 | Loss: 0.00001119
Iteration 65/1000 | Loss: 0.00001119
Iteration 66/1000 | Loss: 0.00001118
Iteration 67/1000 | Loss: 0.00001118
Iteration 68/1000 | Loss: 0.00001118
Iteration 69/1000 | Loss: 0.00001118
Iteration 70/1000 | Loss: 0.00001117
Iteration 71/1000 | Loss: 0.00001117
Iteration 72/1000 | Loss: 0.00001117
Iteration 73/1000 | Loss: 0.00001116
Iteration 74/1000 | Loss: 0.00001116
Iteration 75/1000 | Loss: 0.00001115
Iteration 76/1000 | Loss: 0.00001115
Iteration 77/1000 | Loss: 0.00001115
Iteration 78/1000 | Loss: 0.00001115
Iteration 79/1000 | Loss: 0.00001115
Iteration 80/1000 | Loss: 0.00001114
Iteration 81/1000 | Loss: 0.00001114
Iteration 82/1000 | Loss: 0.00001113
Iteration 83/1000 | Loss: 0.00001113
Iteration 84/1000 | Loss: 0.00001113
Iteration 85/1000 | Loss: 0.00001112
Iteration 86/1000 | Loss: 0.00001112
Iteration 87/1000 | Loss: 0.00001112
Iteration 88/1000 | Loss: 0.00001112
Iteration 89/1000 | Loss: 0.00001112
Iteration 90/1000 | Loss: 0.00001112
Iteration 91/1000 | Loss: 0.00001112
Iteration 92/1000 | Loss: 0.00001112
Iteration 93/1000 | Loss: 0.00001111
Iteration 94/1000 | Loss: 0.00001111
Iteration 95/1000 | Loss: 0.00001111
Iteration 96/1000 | Loss: 0.00001111
Iteration 97/1000 | Loss: 0.00001111
Iteration 98/1000 | Loss: 0.00001111
Iteration 99/1000 | Loss: 0.00001111
Iteration 100/1000 | Loss: 0.00001111
Iteration 101/1000 | Loss: 0.00001111
Iteration 102/1000 | Loss: 0.00001111
Iteration 103/1000 | Loss: 0.00001111
Iteration 104/1000 | Loss: 0.00001111
Iteration 105/1000 | Loss: 0.00001111
Iteration 106/1000 | Loss: 0.00001110
Iteration 107/1000 | Loss: 0.00001110
Iteration 108/1000 | Loss: 0.00001110
Iteration 109/1000 | Loss: 0.00001109
Iteration 110/1000 | Loss: 0.00001109
Iteration 111/1000 | Loss: 0.00001109
Iteration 112/1000 | Loss: 0.00001108
Iteration 113/1000 | Loss: 0.00001108
Iteration 114/1000 | Loss: 0.00001107
Iteration 115/1000 | Loss: 0.00001107
Iteration 116/1000 | Loss: 0.00001107
Iteration 117/1000 | Loss: 0.00001107
Iteration 118/1000 | Loss: 0.00001107
Iteration 119/1000 | Loss: 0.00001107
Iteration 120/1000 | Loss: 0.00001107
Iteration 121/1000 | Loss: 0.00001107
Iteration 122/1000 | Loss: 0.00001107
Iteration 123/1000 | Loss: 0.00001107
Iteration 124/1000 | Loss: 0.00001107
Iteration 125/1000 | Loss: 0.00001107
Iteration 126/1000 | Loss: 0.00001106
Iteration 127/1000 | Loss: 0.00001106
Iteration 128/1000 | Loss: 0.00001106
Iteration 129/1000 | Loss: 0.00001106
Iteration 130/1000 | Loss: 0.00001106
Iteration 131/1000 | Loss: 0.00001106
Iteration 132/1000 | Loss: 0.00001106
Iteration 133/1000 | Loss: 0.00001106
Iteration 134/1000 | Loss: 0.00001106
Iteration 135/1000 | Loss: 0.00001105
Iteration 136/1000 | Loss: 0.00001105
Iteration 137/1000 | Loss: 0.00001105
Iteration 138/1000 | Loss: 0.00001105
Iteration 139/1000 | Loss: 0.00001105
Iteration 140/1000 | Loss: 0.00001105
Iteration 141/1000 | Loss: 0.00001105
Iteration 142/1000 | Loss: 0.00001105
Iteration 143/1000 | Loss: 0.00001105
Iteration 144/1000 | Loss: 0.00001104
Iteration 145/1000 | Loss: 0.00001104
Iteration 146/1000 | Loss: 0.00001104
Iteration 147/1000 | Loss: 0.00001104
Iteration 148/1000 | Loss: 0.00001104
Iteration 149/1000 | Loss: 0.00001104
Iteration 150/1000 | Loss: 0.00001104
Iteration 151/1000 | Loss: 0.00001103
Iteration 152/1000 | Loss: 0.00001103
Iteration 153/1000 | Loss: 0.00001103
Iteration 154/1000 | Loss: 0.00001103
Iteration 155/1000 | Loss: 0.00001103
Iteration 156/1000 | Loss: 0.00001103
Iteration 157/1000 | Loss: 0.00001103
Iteration 158/1000 | Loss: 0.00001103
Iteration 159/1000 | Loss: 0.00001102
Iteration 160/1000 | Loss: 0.00001102
Iteration 161/1000 | Loss: 0.00001102
Iteration 162/1000 | Loss: 0.00001102
Iteration 163/1000 | Loss: 0.00001102
Iteration 164/1000 | Loss: 0.00001102
Iteration 165/1000 | Loss: 0.00001102
Iteration 166/1000 | Loss: 0.00001102
Iteration 167/1000 | Loss: 0.00001102
Iteration 168/1000 | Loss: 0.00001102
Iteration 169/1000 | Loss: 0.00001101
Iteration 170/1000 | Loss: 0.00001101
Iteration 171/1000 | Loss: 0.00001101
Iteration 172/1000 | Loss: 0.00001101
Iteration 173/1000 | Loss: 0.00001101
Iteration 174/1000 | Loss: 0.00001101
Iteration 175/1000 | Loss: 0.00001101
Iteration 176/1000 | Loss: 0.00001100
Iteration 177/1000 | Loss: 0.00001100
Iteration 178/1000 | Loss: 0.00001100
Iteration 179/1000 | Loss: 0.00001100
Iteration 180/1000 | Loss: 0.00001100
Iteration 181/1000 | Loss: 0.00001100
Iteration 182/1000 | Loss: 0.00001100
Iteration 183/1000 | Loss: 0.00001100
Iteration 184/1000 | Loss: 0.00001100
Iteration 185/1000 | Loss: 0.00001100
Iteration 186/1000 | Loss: 0.00001100
Iteration 187/1000 | Loss: 0.00001100
Iteration 188/1000 | Loss: 0.00001100
Iteration 189/1000 | Loss: 0.00001099
Iteration 190/1000 | Loss: 0.00001099
Iteration 191/1000 | Loss: 0.00001099
Iteration 192/1000 | Loss: 0.00001099
Iteration 193/1000 | Loss: 0.00001099
Iteration 194/1000 | Loss: 0.00001099
Iteration 195/1000 | Loss: 0.00001099
Iteration 196/1000 | Loss: 0.00001099
Iteration 197/1000 | Loss: 0.00001099
Iteration 198/1000 | Loss: 0.00001099
Iteration 199/1000 | Loss: 0.00001099
Iteration 200/1000 | Loss: 0.00001099
Iteration 201/1000 | Loss: 0.00001099
Iteration 202/1000 | Loss: 0.00001099
Iteration 203/1000 | Loss: 0.00001098
Iteration 204/1000 | Loss: 0.00001098
Iteration 205/1000 | Loss: 0.00001098
Iteration 206/1000 | Loss: 0.00001098
Iteration 207/1000 | Loss: 0.00001098
Iteration 208/1000 | Loss: 0.00001098
Iteration 209/1000 | Loss: 0.00001098
Iteration 210/1000 | Loss: 0.00001098
Iteration 211/1000 | Loss: 0.00001098
Iteration 212/1000 | Loss: 0.00001098
Iteration 213/1000 | Loss: 0.00001098
Iteration 214/1000 | Loss: 0.00001098
Iteration 215/1000 | Loss: 0.00001098
Iteration 216/1000 | Loss: 0.00001098
Iteration 217/1000 | Loss: 0.00001098
Iteration 218/1000 | Loss: 0.00001098
Iteration 219/1000 | Loss: 0.00001098
Iteration 220/1000 | Loss: 0.00001097
Iteration 221/1000 | Loss: 0.00001097
Iteration 222/1000 | Loss: 0.00001097
Iteration 223/1000 | Loss: 0.00001097
Iteration 224/1000 | Loss: 0.00001097
Iteration 225/1000 | Loss: 0.00001097
Iteration 226/1000 | Loss: 0.00001097
Iteration 227/1000 | Loss: 0.00001097
Iteration 228/1000 | Loss: 0.00001097
Iteration 229/1000 | Loss: 0.00001096
Iteration 230/1000 | Loss: 0.00001096
Iteration 231/1000 | Loss: 0.00001096
Iteration 232/1000 | Loss: 0.00001096
Iteration 233/1000 | Loss: 0.00001096
Iteration 234/1000 | Loss: 0.00001096
Iteration 235/1000 | Loss: 0.00001096
Iteration 236/1000 | Loss: 0.00001096
Iteration 237/1000 | Loss: 0.00001096
Iteration 238/1000 | Loss: 0.00001096
Iteration 239/1000 | Loss: 0.00001096
Iteration 240/1000 | Loss: 0.00001096
Iteration 241/1000 | Loss: 0.00001095
Iteration 242/1000 | Loss: 0.00001095
Iteration 243/1000 | Loss: 0.00001095
Iteration 244/1000 | Loss: 0.00001095
Iteration 245/1000 | Loss: 0.00001095
Iteration 246/1000 | Loss: 0.00001095
Iteration 247/1000 | Loss: 0.00001095
Iteration 248/1000 | Loss: 0.00001095
Iteration 249/1000 | Loss: 0.00001095
Iteration 250/1000 | Loss: 0.00001095
Iteration 251/1000 | Loss: 0.00001095
Iteration 252/1000 | Loss: 0.00001095
Iteration 253/1000 | Loss: 0.00001095
Iteration 254/1000 | Loss: 0.00001095
Iteration 255/1000 | Loss: 0.00001095
Iteration 256/1000 | Loss: 0.00001095
Iteration 257/1000 | Loss: 0.00001095
Iteration 258/1000 | Loss: 0.00001095
Iteration 259/1000 | Loss: 0.00001095
Iteration 260/1000 | Loss: 0.00001095
Iteration 261/1000 | Loss: 0.00001095
Iteration 262/1000 | Loss: 0.00001095
Iteration 263/1000 | Loss: 0.00001095
Iteration 264/1000 | Loss: 0.00001095
Iteration 265/1000 | Loss: 0.00001095
Iteration 266/1000 | Loss: 0.00001095
Iteration 267/1000 | Loss: 0.00001095
Iteration 268/1000 | Loss: 0.00001095
Iteration 269/1000 | Loss: 0.00001095
Iteration 270/1000 | Loss: 0.00001095
Iteration 271/1000 | Loss: 0.00001095
Iteration 272/1000 | Loss: 0.00001095
Iteration 273/1000 | Loss: 0.00001095
Iteration 274/1000 | Loss: 0.00001095
Iteration 275/1000 | Loss: 0.00001095
Iteration 276/1000 | Loss: 0.00001095
Iteration 277/1000 | Loss: 0.00001095
Iteration 278/1000 | Loss: 0.00001095
Iteration 279/1000 | Loss: 0.00001095
Iteration 280/1000 | Loss: 0.00001095
Iteration 281/1000 | Loss: 0.00001095
Iteration 282/1000 | Loss: 0.00001095
Iteration 283/1000 | Loss: 0.00001095
Iteration 284/1000 | Loss: 0.00001095
Iteration 285/1000 | Loss: 0.00001095
Iteration 286/1000 | Loss: 0.00001095
Iteration 287/1000 | Loss: 0.00001095
Iteration 288/1000 | Loss: 0.00001095
Iteration 289/1000 | Loss: 0.00001095
Iteration 290/1000 | Loss: 0.00001095
Iteration 291/1000 | Loss: 0.00001095
Iteration 292/1000 | Loss: 0.00001095
Iteration 293/1000 | Loss: 0.00001095
Iteration 294/1000 | Loss: 0.00001095
Iteration 295/1000 | Loss: 0.00001095
Iteration 296/1000 | Loss: 0.00001095
Iteration 297/1000 | Loss: 0.00001095
Iteration 298/1000 | Loss: 0.00001095
Iteration 299/1000 | Loss: 0.00001095
Iteration 300/1000 | Loss: 0.00001095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [1.0948864655802026e-05, 1.0948864655802026e-05, 1.0948864655802026e-05, 1.0948864655802026e-05, 1.0948864655802026e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0948864655802026e-05

Optimization complete. Final v2v error: 2.781259536743164 mm

Highest mean error: 3.3472561836242676 mm for frame 39

Lowest mean error: 2.5258092880249023 mm for frame 104

Saving results

Total time: 43.53088855743408
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494975
Iteration 2/25 | Loss: 0.00154865
Iteration 3/25 | Loss: 0.00140267
Iteration 4/25 | Loss: 0.00138452
Iteration 5/25 | Loss: 0.00137225
Iteration 6/25 | Loss: 0.00136487
Iteration 7/25 | Loss: 0.00135286
Iteration 8/25 | Loss: 0.00134374
Iteration 9/25 | Loss: 0.00133957
Iteration 10/25 | Loss: 0.00132806
Iteration 11/25 | Loss: 0.00131654
Iteration 12/25 | Loss: 0.00131651
Iteration 13/25 | Loss: 0.00131223
Iteration 14/25 | Loss: 0.00131335
Iteration 15/25 | Loss: 0.00130948
Iteration 16/25 | Loss: 0.00130921
Iteration 17/25 | Loss: 0.00130485
Iteration 18/25 | Loss: 0.00130798
Iteration 19/25 | Loss: 0.00130622
Iteration 20/25 | Loss: 0.00130250
Iteration 21/25 | Loss: 0.00130159
Iteration 22/25 | Loss: 0.00130128
Iteration 23/25 | Loss: 0.00130064
Iteration 24/25 | Loss: 0.00130043
Iteration 25/25 | Loss: 0.00130027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28422105
Iteration 2/25 | Loss: 0.00351662
Iteration 3/25 | Loss: 0.00351662
Iteration 4/25 | Loss: 0.00351662
Iteration 5/25 | Loss: 0.00351662
Iteration 6/25 | Loss: 0.00351662
Iteration 7/25 | Loss: 0.00351662
Iteration 8/25 | Loss: 0.00351661
Iteration 9/25 | Loss: 0.00351661
Iteration 10/25 | Loss: 0.00351661
Iteration 11/25 | Loss: 0.00351661
Iteration 12/25 | Loss: 0.00351661
Iteration 13/25 | Loss: 0.00351661
Iteration 14/25 | Loss: 0.00351661
Iteration 15/25 | Loss: 0.00351661
Iteration 16/25 | Loss: 0.00351661
Iteration 17/25 | Loss: 0.00351661
Iteration 18/25 | Loss: 0.00351661
Iteration 19/25 | Loss: 0.00351661
Iteration 20/25 | Loss: 0.00351661
Iteration 21/25 | Loss: 0.00351661
Iteration 22/25 | Loss: 0.00351661
Iteration 23/25 | Loss: 0.00351661
Iteration 24/25 | Loss: 0.00351661
Iteration 25/25 | Loss: 0.00351661

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00351661
Iteration 2/1000 | Loss: 0.00017431
Iteration 3/1000 | Loss: 0.00011275
Iteration 4/1000 | Loss: 0.00009339
Iteration 5/1000 | Loss: 0.00008215
Iteration 6/1000 | Loss: 0.00007546
Iteration 7/1000 | Loss: 0.00007175
Iteration 8/1000 | Loss: 0.00006951
Iteration 9/1000 | Loss: 0.00006772
Iteration 10/1000 | Loss: 0.00006642
Iteration 11/1000 | Loss: 0.00006519
Iteration 12/1000 | Loss: 0.00006414
Iteration 13/1000 | Loss: 0.00006301
Iteration 14/1000 | Loss: 0.00006179
Iteration 15/1000 | Loss: 0.00006052
Iteration 16/1000 | Loss: 0.00078268
Iteration 17/1000 | Loss: 0.00046270
Iteration 18/1000 | Loss: 0.00023086
Iteration 19/1000 | Loss: 0.00007214
Iteration 20/1000 | Loss: 0.00006579
Iteration 21/1000 | Loss: 0.00023314
Iteration 22/1000 | Loss: 0.00007154
Iteration 23/1000 | Loss: 0.00006368
Iteration 24/1000 | Loss: 0.00006202
Iteration 25/1000 | Loss: 0.00006046
Iteration 26/1000 | Loss: 0.00029104
Iteration 27/1000 | Loss: 0.00026462
Iteration 28/1000 | Loss: 0.00006027
Iteration 29/1000 | Loss: 0.00005861
Iteration 30/1000 | Loss: 0.00005788
Iteration 31/1000 | Loss: 0.00044830
Iteration 32/1000 | Loss: 0.00030615
Iteration 33/1000 | Loss: 0.00007141
Iteration 34/1000 | Loss: 0.00006413
Iteration 35/1000 | Loss: 0.00006122
Iteration 36/1000 | Loss: 0.00005901
Iteration 37/1000 | Loss: 0.00005654
Iteration 38/1000 | Loss: 0.00005535
Iteration 39/1000 | Loss: 0.00005437
Iteration 40/1000 | Loss: 0.00005319
Iteration 41/1000 | Loss: 0.00005205
Iteration 42/1000 | Loss: 0.00005150
Iteration 43/1000 | Loss: 0.00005108
Iteration 44/1000 | Loss: 0.00005072
Iteration 45/1000 | Loss: 0.00005033
Iteration 46/1000 | Loss: 0.00005001
Iteration 47/1000 | Loss: 0.00004972
Iteration 48/1000 | Loss: 0.00004954
Iteration 49/1000 | Loss: 0.00004953
Iteration 50/1000 | Loss: 0.00004946
Iteration 51/1000 | Loss: 0.00004939
Iteration 52/1000 | Loss: 0.00004936
Iteration 53/1000 | Loss: 0.00004930
Iteration 54/1000 | Loss: 0.00004929
Iteration 55/1000 | Loss: 0.00004928
Iteration 56/1000 | Loss: 0.00004928
Iteration 57/1000 | Loss: 0.00004927
Iteration 58/1000 | Loss: 0.00004927
Iteration 59/1000 | Loss: 0.00004926
Iteration 60/1000 | Loss: 0.00004923
Iteration 61/1000 | Loss: 0.00005421
Iteration 62/1000 | Loss: 0.00005421
Iteration 63/1000 | Loss: 0.00005105
Iteration 64/1000 | Loss: 0.00004926
Iteration 65/1000 | Loss: 0.00005457
Iteration 66/1000 | Loss: 0.00005073
Iteration 67/1000 | Loss: 0.00004939
Iteration 68/1000 | Loss: 0.00004903
Iteration 69/1000 | Loss: 0.00004879
Iteration 70/1000 | Loss: 0.00004877
Iteration 71/1000 | Loss: 0.00004870
Iteration 72/1000 | Loss: 0.00004851
Iteration 73/1000 | Loss: 0.00004847
Iteration 74/1000 | Loss: 0.00004827
Iteration 75/1000 | Loss: 0.00004825
Iteration 76/1000 | Loss: 0.00004824
Iteration 77/1000 | Loss: 0.00004824
Iteration 78/1000 | Loss: 0.00004821
Iteration 79/1000 | Loss: 0.00004820
Iteration 80/1000 | Loss: 0.00004819
Iteration 81/1000 | Loss: 0.00004811
Iteration 82/1000 | Loss: 0.00004811
Iteration 83/1000 | Loss: 0.00004811
Iteration 84/1000 | Loss: 0.00004811
Iteration 85/1000 | Loss: 0.00004810
Iteration 86/1000 | Loss: 0.00004810
Iteration 87/1000 | Loss: 0.00004808
Iteration 88/1000 | Loss: 0.00004808
Iteration 89/1000 | Loss: 0.00004808
Iteration 90/1000 | Loss: 0.00004808
Iteration 91/1000 | Loss: 0.00004808
Iteration 92/1000 | Loss: 0.00004808
Iteration 93/1000 | Loss: 0.00004808
Iteration 94/1000 | Loss: 0.00004808
Iteration 95/1000 | Loss: 0.00004808
Iteration 96/1000 | Loss: 0.00004807
Iteration 97/1000 | Loss: 0.00004807
Iteration 98/1000 | Loss: 0.00004807
Iteration 99/1000 | Loss: 0.00004807
Iteration 100/1000 | Loss: 0.00004807
Iteration 101/1000 | Loss: 0.00004807
Iteration 102/1000 | Loss: 0.00004807
Iteration 103/1000 | Loss: 0.00004807
Iteration 104/1000 | Loss: 0.00004807
Iteration 105/1000 | Loss: 0.00004807
Iteration 106/1000 | Loss: 0.00004807
Iteration 107/1000 | Loss: 0.00004806
Iteration 108/1000 | Loss: 0.00004806
Iteration 109/1000 | Loss: 0.00004806
Iteration 110/1000 | Loss: 0.00004806
Iteration 111/1000 | Loss: 0.00004805
Iteration 112/1000 | Loss: 0.00004805
Iteration 113/1000 | Loss: 0.00004805
Iteration 114/1000 | Loss: 0.00004805
Iteration 115/1000 | Loss: 0.00004805
Iteration 116/1000 | Loss: 0.00004805
Iteration 117/1000 | Loss: 0.00004805
Iteration 118/1000 | Loss: 0.00004805
Iteration 119/1000 | Loss: 0.00004804
Iteration 120/1000 | Loss: 0.00004802
Iteration 121/1000 | Loss: 0.00004801
Iteration 122/1000 | Loss: 0.00004801
Iteration 123/1000 | Loss: 0.00004801
Iteration 124/1000 | Loss: 0.00004801
Iteration 125/1000 | Loss: 0.00004800
Iteration 126/1000 | Loss: 0.00004800
Iteration 127/1000 | Loss: 0.00004800
Iteration 128/1000 | Loss: 0.00004799
Iteration 129/1000 | Loss: 0.00004799
Iteration 130/1000 | Loss: 0.00004799
Iteration 131/1000 | Loss: 0.00004799
Iteration 132/1000 | Loss: 0.00004799
Iteration 133/1000 | Loss: 0.00004798
Iteration 134/1000 | Loss: 0.00004798
Iteration 135/1000 | Loss: 0.00004798
Iteration 136/1000 | Loss: 0.00004798
Iteration 137/1000 | Loss: 0.00004798
Iteration 138/1000 | Loss: 0.00004797
Iteration 139/1000 | Loss: 0.00004797
Iteration 140/1000 | Loss: 0.00004797
Iteration 141/1000 | Loss: 0.00004796
Iteration 142/1000 | Loss: 0.00004796
Iteration 143/1000 | Loss: 0.00004796
Iteration 144/1000 | Loss: 0.00004796
Iteration 145/1000 | Loss: 0.00004796
Iteration 146/1000 | Loss: 0.00004796
Iteration 147/1000 | Loss: 0.00004796
Iteration 148/1000 | Loss: 0.00004795
Iteration 149/1000 | Loss: 0.00004795
Iteration 150/1000 | Loss: 0.00004795
Iteration 151/1000 | Loss: 0.00004795
Iteration 152/1000 | Loss: 0.00004795
Iteration 153/1000 | Loss: 0.00004795
Iteration 154/1000 | Loss: 0.00004795
Iteration 155/1000 | Loss: 0.00004794
Iteration 156/1000 | Loss: 0.00004794
Iteration 157/1000 | Loss: 0.00004794
Iteration 158/1000 | Loss: 0.00004794
Iteration 159/1000 | Loss: 0.00004794
Iteration 160/1000 | Loss: 0.00004794
Iteration 161/1000 | Loss: 0.00004793
Iteration 162/1000 | Loss: 0.00004793
Iteration 163/1000 | Loss: 0.00004793
Iteration 164/1000 | Loss: 0.00004793
Iteration 165/1000 | Loss: 0.00004793
Iteration 166/1000 | Loss: 0.00004793
Iteration 167/1000 | Loss: 0.00004792
Iteration 168/1000 | Loss: 0.00004792
Iteration 169/1000 | Loss: 0.00004792
Iteration 170/1000 | Loss: 0.00004792
Iteration 171/1000 | Loss: 0.00004792
Iteration 172/1000 | Loss: 0.00004792
Iteration 173/1000 | Loss: 0.00004791
Iteration 174/1000 | Loss: 0.00004791
Iteration 175/1000 | Loss: 0.00004791
Iteration 176/1000 | Loss: 0.00004791
Iteration 177/1000 | Loss: 0.00004791
Iteration 178/1000 | Loss: 0.00004791
Iteration 179/1000 | Loss: 0.00004791
Iteration 180/1000 | Loss: 0.00004791
Iteration 181/1000 | Loss: 0.00004791
Iteration 182/1000 | Loss: 0.00004791
Iteration 183/1000 | Loss: 0.00004790
Iteration 184/1000 | Loss: 0.00004790
Iteration 185/1000 | Loss: 0.00004790
Iteration 186/1000 | Loss: 0.00004790
Iteration 187/1000 | Loss: 0.00004789
Iteration 188/1000 | Loss: 0.00004789
Iteration 189/1000 | Loss: 0.00004789
Iteration 190/1000 | Loss: 0.00004789
Iteration 191/1000 | Loss: 0.00004789
Iteration 192/1000 | Loss: 0.00004789
Iteration 193/1000 | Loss: 0.00004789
Iteration 194/1000 | Loss: 0.00004789
Iteration 195/1000 | Loss: 0.00004788
Iteration 196/1000 | Loss: 0.00004788
Iteration 197/1000 | Loss: 0.00004788
Iteration 198/1000 | Loss: 0.00004788
Iteration 199/1000 | Loss: 0.00004788
Iteration 200/1000 | Loss: 0.00004788
Iteration 201/1000 | Loss: 0.00004787
Iteration 202/1000 | Loss: 0.00004787
Iteration 203/1000 | Loss: 0.00004787
Iteration 204/1000 | Loss: 0.00004787
Iteration 205/1000 | Loss: 0.00004786
Iteration 206/1000 | Loss: 0.00004786
Iteration 207/1000 | Loss: 0.00004786
Iteration 208/1000 | Loss: 0.00004786
Iteration 209/1000 | Loss: 0.00004786
Iteration 210/1000 | Loss: 0.00004786
Iteration 211/1000 | Loss: 0.00004786
Iteration 212/1000 | Loss: 0.00004786
Iteration 213/1000 | Loss: 0.00004785
Iteration 214/1000 | Loss: 0.00004785
Iteration 215/1000 | Loss: 0.00004785
Iteration 216/1000 | Loss: 0.00004785
Iteration 217/1000 | Loss: 0.00004785
Iteration 218/1000 | Loss: 0.00004785
Iteration 219/1000 | Loss: 0.00004785
Iteration 220/1000 | Loss: 0.00004785
Iteration 221/1000 | Loss: 0.00004784
Iteration 222/1000 | Loss: 0.00004784
Iteration 223/1000 | Loss: 0.00004784
Iteration 224/1000 | Loss: 0.00004784
Iteration 225/1000 | Loss: 0.00004784
Iteration 226/1000 | Loss: 0.00004784
Iteration 227/1000 | Loss: 0.00004784
Iteration 228/1000 | Loss: 0.00004784
Iteration 229/1000 | Loss: 0.00004784
Iteration 230/1000 | Loss: 0.00004784
Iteration 231/1000 | Loss: 0.00004784
Iteration 232/1000 | Loss: 0.00004784
Iteration 233/1000 | Loss: 0.00004784
Iteration 234/1000 | Loss: 0.00004784
Iteration 235/1000 | Loss: 0.00004784
Iteration 236/1000 | Loss: 0.00004784
Iteration 237/1000 | Loss: 0.00004784
Iteration 238/1000 | Loss: 0.00004784
Iteration 239/1000 | Loss: 0.00004784
Iteration 240/1000 | Loss: 0.00004784
Iteration 241/1000 | Loss: 0.00004784
Iteration 242/1000 | Loss: 0.00004784
Iteration 243/1000 | Loss: 0.00004784
Iteration 244/1000 | Loss: 0.00004784
Iteration 245/1000 | Loss: 0.00004784
Iteration 246/1000 | Loss: 0.00004784
Iteration 247/1000 | Loss: 0.00004784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [4.7836056182859465e-05, 4.7836056182859465e-05, 4.7836056182859465e-05, 4.7836056182859465e-05, 4.7836056182859465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.7836056182859465e-05

Optimization complete. Final v2v error: 4.199017524719238 mm

Highest mean error: 11.51775074005127 mm for frame 107

Lowest mean error: 2.538540840148926 mm for frame 170

Saving results

Total time: 148.3722083568573
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784692
Iteration 2/25 | Loss: 0.00138703
Iteration 3/25 | Loss: 0.00124193
Iteration 4/25 | Loss: 0.00122393
Iteration 5/25 | Loss: 0.00121349
Iteration 6/25 | Loss: 0.00121120
Iteration 7/25 | Loss: 0.00121269
Iteration 8/25 | Loss: 0.00121137
Iteration 9/25 | Loss: 0.00120897
Iteration 10/25 | Loss: 0.00120793
Iteration 11/25 | Loss: 0.00120770
Iteration 12/25 | Loss: 0.00120763
Iteration 13/25 | Loss: 0.00120762
Iteration 14/25 | Loss: 0.00120762
Iteration 15/25 | Loss: 0.00120762
Iteration 16/25 | Loss: 0.00120762
Iteration 17/25 | Loss: 0.00120762
Iteration 18/25 | Loss: 0.00120762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012076233979314566, 0.0012076233979314566, 0.0012076233979314566, 0.0012076233979314566, 0.0012076233979314566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012076233979314566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.53276896
Iteration 2/25 | Loss: 0.00118032
Iteration 3/25 | Loss: 0.00118032
Iteration 4/25 | Loss: 0.00118032
Iteration 5/25 | Loss: 0.00118032
Iteration 6/25 | Loss: 0.00118032
Iteration 7/25 | Loss: 0.00118032
Iteration 8/25 | Loss: 0.00118032
Iteration 9/25 | Loss: 0.00118032
Iteration 10/25 | Loss: 0.00118032
Iteration 11/25 | Loss: 0.00118032
Iteration 12/25 | Loss: 0.00118032
Iteration 13/25 | Loss: 0.00118032
Iteration 14/25 | Loss: 0.00118032
Iteration 15/25 | Loss: 0.00118032
Iteration 16/25 | Loss: 0.00118032
Iteration 17/25 | Loss: 0.00118032
Iteration 18/25 | Loss: 0.00118032
Iteration 19/25 | Loss: 0.00118032
Iteration 20/25 | Loss: 0.00118032
Iteration 21/25 | Loss: 0.00118032
Iteration 22/25 | Loss: 0.00118032
Iteration 23/25 | Loss: 0.00118032
Iteration 24/25 | Loss: 0.00118032
Iteration 25/25 | Loss: 0.00118032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118032
Iteration 2/1000 | Loss: 0.00002510
Iteration 3/1000 | Loss: 0.00001838
Iteration 4/1000 | Loss: 0.00001696
Iteration 5/1000 | Loss: 0.00001600
Iteration 6/1000 | Loss: 0.00001551
Iteration 7/1000 | Loss: 0.00001512
Iteration 8/1000 | Loss: 0.00001482
Iteration 9/1000 | Loss: 0.00001454
Iteration 10/1000 | Loss: 0.00001442
Iteration 11/1000 | Loss: 0.00001436
Iteration 12/1000 | Loss: 0.00001425
Iteration 13/1000 | Loss: 0.00001424
Iteration 14/1000 | Loss: 0.00001423
Iteration 15/1000 | Loss: 0.00001419
Iteration 16/1000 | Loss: 0.00001411
Iteration 17/1000 | Loss: 0.00001408
Iteration 18/1000 | Loss: 0.00001407
Iteration 19/1000 | Loss: 0.00001407
Iteration 20/1000 | Loss: 0.00001401
Iteration 21/1000 | Loss: 0.00001401
Iteration 22/1000 | Loss: 0.00001401
Iteration 23/1000 | Loss: 0.00001401
Iteration 24/1000 | Loss: 0.00001400
Iteration 25/1000 | Loss: 0.00001395
Iteration 26/1000 | Loss: 0.00001395
Iteration 27/1000 | Loss: 0.00001395
Iteration 28/1000 | Loss: 0.00001395
Iteration 29/1000 | Loss: 0.00001395
Iteration 30/1000 | Loss: 0.00001395
Iteration 31/1000 | Loss: 0.00001395
Iteration 32/1000 | Loss: 0.00001394
Iteration 33/1000 | Loss: 0.00001394
Iteration 34/1000 | Loss: 0.00001388
Iteration 35/1000 | Loss: 0.00001388
Iteration 36/1000 | Loss: 0.00001388
Iteration 37/1000 | Loss: 0.00001388
Iteration 38/1000 | Loss: 0.00001388
Iteration 39/1000 | Loss: 0.00001387
Iteration 40/1000 | Loss: 0.00001387
Iteration 41/1000 | Loss: 0.00001386
Iteration 42/1000 | Loss: 0.00001385
Iteration 43/1000 | Loss: 0.00001385
Iteration 44/1000 | Loss: 0.00001384
Iteration 45/1000 | Loss: 0.00001384
Iteration 46/1000 | Loss: 0.00001384
Iteration 47/1000 | Loss: 0.00001384
Iteration 48/1000 | Loss: 0.00001383
Iteration 49/1000 | Loss: 0.00001383
Iteration 50/1000 | Loss: 0.00001382
Iteration 51/1000 | Loss: 0.00001382
Iteration 52/1000 | Loss: 0.00001382
Iteration 53/1000 | Loss: 0.00001381
Iteration 54/1000 | Loss: 0.00001381
Iteration 55/1000 | Loss: 0.00001380
Iteration 56/1000 | Loss: 0.00001380
Iteration 57/1000 | Loss: 0.00001380
Iteration 58/1000 | Loss: 0.00001380
Iteration 59/1000 | Loss: 0.00001380
Iteration 60/1000 | Loss: 0.00001380
Iteration 61/1000 | Loss: 0.00001380
Iteration 62/1000 | Loss: 0.00001379
Iteration 63/1000 | Loss: 0.00001379
Iteration 64/1000 | Loss: 0.00001379
Iteration 65/1000 | Loss: 0.00001379
Iteration 66/1000 | Loss: 0.00001379
Iteration 67/1000 | Loss: 0.00001379
Iteration 68/1000 | Loss: 0.00001379
Iteration 69/1000 | Loss: 0.00001378
Iteration 70/1000 | Loss: 0.00001378
Iteration 71/1000 | Loss: 0.00001378
Iteration 72/1000 | Loss: 0.00001378
Iteration 73/1000 | Loss: 0.00001378
Iteration 74/1000 | Loss: 0.00001378
Iteration 75/1000 | Loss: 0.00001378
Iteration 76/1000 | Loss: 0.00001378
Iteration 77/1000 | Loss: 0.00001378
Iteration 78/1000 | Loss: 0.00001377
Iteration 79/1000 | Loss: 0.00001377
Iteration 80/1000 | Loss: 0.00001377
Iteration 81/1000 | Loss: 0.00001377
Iteration 82/1000 | Loss: 0.00001377
Iteration 83/1000 | Loss: 0.00001377
Iteration 84/1000 | Loss: 0.00001377
Iteration 85/1000 | Loss: 0.00001377
Iteration 86/1000 | Loss: 0.00001377
Iteration 87/1000 | Loss: 0.00001377
Iteration 88/1000 | Loss: 0.00001377
Iteration 89/1000 | Loss: 0.00001377
Iteration 90/1000 | Loss: 0.00001377
Iteration 91/1000 | Loss: 0.00001377
Iteration 92/1000 | Loss: 0.00001377
Iteration 93/1000 | Loss: 0.00001376
Iteration 94/1000 | Loss: 0.00001376
Iteration 95/1000 | Loss: 0.00001376
Iteration 96/1000 | Loss: 0.00001376
Iteration 97/1000 | Loss: 0.00001376
Iteration 98/1000 | Loss: 0.00001376
Iteration 99/1000 | Loss: 0.00001376
Iteration 100/1000 | Loss: 0.00001375
Iteration 101/1000 | Loss: 0.00001375
Iteration 102/1000 | Loss: 0.00001375
Iteration 103/1000 | Loss: 0.00001375
Iteration 104/1000 | Loss: 0.00001375
Iteration 105/1000 | Loss: 0.00001375
Iteration 106/1000 | Loss: 0.00001375
Iteration 107/1000 | Loss: 0.00001375
Iteration 108/1000 | Loss: 0.00001375
Iteration 109/1000 | Loss: 0.00001375
Iteration 110/1000 | Loss: 0.00001375
Iteration 111/1000 | Loss: 0.00001375
Iteration 112/1000 | Loss: 0.00001374
Iteration 113/1000 | Loss: 0.00001374
Iteration 114/1000 | Loss: 0.00001374
Iteration 115/1000 | Loss: 0.00001374
Iteration 116/1000 | Loss: 0.00001374
Iteration 117/1000 | Loss: 0.00001374
Iteration 118/1000 | Loss: 0.00001373
Iteration 119/1000 | Loss: 0.00001373
Iteration 120/1000 | Loss: 0.00001373
Iteration 121/1000 | Loss: 0.00001373
Iteration 122/1000 | Loss: 0.00001373
Iteration 123/1000 | Loss: 0.00001373
Iteration 124/1000 | Loss: 0.00001373
Iteration 125/1000 | Loss: 0.00001373
Iteration 126/1000 | Loss: 0.00001373
Iteration 127/1000 | Loss: 0.00001373
Iteration 128/1000 | Loss: 0.00001372
Iteration 129/1000 | Loss: 0.00001372
Iteration 130/1000 | Loss: 0.00001372
Iteration 131/1000 | Loss: 0.00001372
Iteration 132/1000 | Loss: 0.00001372
Iteration 133/1000 | Loss: 0.00001372
Iteration 134/1000 | Loss: 0.00001372
Iteration 135/1000 | Loss: 0.00001372
Iteration 136/1000 | Loss: 0.00001372
Iteration 137/1000 | Loss: 0.00001372
Iteration 138/1000 | Loss: 0.00001372
Iteration 139/1000 | Loss: 0.00001372
Iteration 140/1000 | Loss: 0.00001372
Iteration 141/1000 | Loss: 0.00001372
Iteration 142/1000 | Loss: 0.00001372
Iteration 143/1000 | Loss: 0.00001372
Iteration 144/1000 | Loss: 0.00001372
Iteration 145/1000 | Loss: 0.00001372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.3722581570618786e-05, 1.3722581570618786e-05, 1.3722581570618786e-05, 1.3722581570618786e-05, 1.3722581570618786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3722581570618786e-05

Optimization complete. Final v2v error: 3.15494704246521 mm

Highest mean error: 3.7950806617736816 mm for frame 172

Lowest mean error: 2.8146870136260986 mm for frame 52

Saving results

Total time: 50.9822518825531
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00601649
Iteration 2/25 | Loss: 0.00170504
Iteration 3/25 | Loss: 0.00136350
Iteration 4/25 | Loss: 0.00134933
Iteration 5/25 | Loss: 0.00134487
Iteration 6/25 | Loss: 0.00134349
Iteration 7/25 | Loss: 0.00134349
Iteration 8/25 | Loss: 0.00134349
Iteration 9/25 | Loss: 0.00134349
Iteration 10/25 | Loss: 0.00134349
Iteration 11/25 | Loss: 0.00134349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013434858992695808, 0.0013434858992695808, 0.0013434858992695808, 0.0013434858992695808, 0.0013434858992695808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013434858992695808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87560254
Iteration 2/25 | Loss: 0.00144352
Iteration 3/25 | Loss: 0.00144351
Iteration 4/25 | Loss: 0.00144351
Iteration 5/25 | Loss: 0.00144351
Iteration 6/25 | Loss: 0.00144351
Iteration 7/25 | Loss: 0.00144351
Iteration 8/25 | Loss: 0.00144351
Iteration 9/25 | Loss: 0.00144351
Iteration 10/25 | Loss: 0.00144351
Iteration 11/25 | Loss: 0.00144351
Iteration 12/25 | Loss: 0.00144351
Iteration 13/25 | Loss: 0.00144351
Iteration 14/25 | Loss: 0.00144351
Iteration 15/25 | Loss: 0.00144351
Iteration 16/25 | Loss: 0.00144351
Iteration 17/25 | Loss: 0.00144351
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014435065677389503, 0.0014435065677389503, 0.0014435065677389503, 0.0014435065677389503, 0.0014435065677389503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014435065677389503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144351
Iteration 2/1000 | Loss: 0.00005701
Iteration 3/1000 | Loss: 0.00003965
Iteration 4/1000 | Loss: 0.00003224
Iteration 5/1000 | Loss: 0.00003055
Iteration 6/1000 | Loss: 0.00002987
Iteration 7/1000 | Loss: 0.00002934
Iteration 8/1000 | Loss: 0.00002870
Iteration 9/1000 | Loss: 0.00002834
Iteration 10/1000 | Loss: 0.00002796
Iteration 11/1000 | Loss: 0.00002768
Iteration 12/1000 | Loss: 0.00002740
Iteration 13/1000 | Loss: 0.00002714
Iteration 14/1000 | Loss: 0.00002685
Iteration 15/1000 | Loss: 0.00002661
Iteration 16/1000 | Loss: 0.00002641
Iteration 17/1000 | Loss: 0.00002625
Iteration 18/1000 | Loss: 0.00002610
Iteration 19/1000 | Loss: 0.00002600
Iteration 20/1000 | Loss: 0.00002597
Iteration 21/1000 | Loss: 0.00002595
Iteration 22/1000 | Loss: 0.00002595
Iteration 23/1000 | Loss: 0.00002595
Iteration 24/1000 | Loss: 0.00002595
Iteration 25/1000 | Loss: 0.00002595
Iteration 26/1000 | Loss: 0.00002594
Iteration 27/1000 | Loss: 0.00002594
Iteration 28/1000 | Loss: 0.00002594
Iteration 29/1000 | Loss: 0.00002592
Iteration 30/1000 | Loss: 0.00002592
Iteration 31/1000 | Loss: 0.00002592
Iteration 32/1000 | Loss: 0.00002592
Iteration 33/1000 | Loss: 0.00002592
Iteration 34/1000 | Loss: 0.00002592
Iteration 35/1000 | Loss: 0.00002592
Iteration 36/1000 | Loss: 0.00002592
Iteration 37/1000 | Loss: 0.00002592
Iteration 38/1000 | Loss: 0.00002592
Iteration 39/1000 | Loss: 0.00002591
Iteration 40/1000 | Loss: 0.00002591
Iteration 41/1000 | Loss: 0.00002591
Iteration 42/1000 | Loss: 0.00002590
Iteration 43/1000 | Loss: 0.00002590
Iteration 44/1000 | Loss: 0.00002590
Iteration 45/1000 | Loss: 0.00002590
Iteration 46/1000 | Loss: 0.00002590
Iteration 47/1000 | Loss: 0.00002589
Iteration 48/1000 | Loss: 0.00002589
Iteration 49/1000 | Loss: 0.00002589
Iteration 50/1000 | Loss: 0.00002589
Iteration 51/1000 | Loss: 0.00002589
Iteration 52/1000 | Loss: 0.00002589
Iteration 53/1000 | Loss: 0.00002589
Iteration 54/1000 | Loss: 0.00002589
Iteration 55/1000 | Loss: 0.00002589
Iteration 56/1000 | Loss: 0.00002589
Iteration 57/1000 | Loss: 0.00002589
Iteration 58/1000 | Loss: 0.00002588
Iteration 59/1000 | Loss: 0.00002588
Iteration 60/1000 | Loss: 0.00002588
Iteration 61/1000 | Loss: 0.00002588
Iteration 62/1000 | Loss: 0.00002588
Iteration 63/1000 | Loss: 0.00002588
Iteration 64/1000 | Loss: 0.00002588
Iteration 65/1000 | Loss: 0.00002588
Iteration 66/1000 | Loss: 0.00002588
Iteration 67/1000 | Loss: 0.00002587
Iteration 68/1000 | Loss: 0.00002587
Iteration 69/1000 | Loss: 0.00002587
Iteration 70/1000 | Loss: 0.00002587
Iteration 71/1000 | Loss: 0.00002587
Iteration 72/1000 | Loss: 0.00002587
Iteration 73/1000 | Loss: 0.00002587
Iteration 74/1000 | Loss: 0.00002587
Iteration 75/1000 | Loss: 0.00002586
Iteration 76/1000 | Loss: 0.00002586
Iteration 77/1000 | Loss: 0.00002586
Iteration 78/1000 | Loss: 0.00002586
Iteration 79/1000 | Loss: 0.00002586
Iteration 80/1000 | Loss: 0.00002586
Iteration 81/1000 | Loss: 0.00002586
Iteration 82/1000 | Loss: 0.00002586
Iteration 83/1000 | Loss: 0.00002586
Iteration 84/1000 | Loss: 0.00002586
Iteration 85/1000 | Loss: 0.00002586
Iteration 86/1000 | Loss: 0.00002586
Iteration 87/1000 | Loss: 0.00002586
Iteration 88/1000 | Loss: 0.00002585
Iteration 89/1000 | Loss: 0.00002585
Iteration 90/1000 | Loss: 0.00002585
Iteration 91/1000 | Loss: 0.00002585
Iteration 92/1000 | Loss: 0.00002585
Iteration 93/1000 | Loss: 0.00002585
Iteration 94/1000 | Loss: 0.00002585
Iteration 95/1000 | Loss: 0.00002585
Iteration 96/1000 | Loss: 0.00002585
Iteration 97/1000 | Loss: 0.00002585
Iteration 98/1000 | Loss: 0.00002585
Iteration 99/1000 | Loss: 0.00002584
Iteration 100/1000 | Loss: 0.00002584
Iteration 101/1000 | Loss: 0.00002584
Iteration 102/1000 | Loss: 0.00002584
Iteration 103/1000 | Loss: 0.00002584
Iteration 104/1000 | Loss: 0.00002584
Iteration 105/1000 | Loss: 0.00002584
Iteration 106/1000 | Loss: 0.00002584
Iteration 107/1000 | Loss: 0.00002584
Iteration 108/1000 | Loss: 0.00002584
Iteration 109/1000 | Loss: 0.00002584
Iteration 110/1000 | Loss: 0.00002584
Iteration 111/1000 | Loss: 0.00002584
Iteration 112/1000 | Loss: 0.00002584
Iteration 113/1000 | Loss: 0.00002584
Iteration 114/1000 | Loss: 0.00002584
Iteration 115/1000 | Loss: 0.00002584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.5835657652351074e-05, 2.5835657652351074e-05, 2.5835657652351074e-05, 2.5835657652351074e-05, 2.5835657652351074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5835657652351074e-05

Optimization complete. Final v2v error: 3.923099994659424 mm

Highest mean error: 4.779541492462158 mm for frame 97

Lowest mean error: 3.0618820190429688 mm for frame 36

Saving results

Total time: 40.246286392211914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540951
Iteration 2/25 | Loss: 0.00152658
Iteration 3/25 | Loss: 0.00129713
Iteration 4/25 | Loss: 0.00123400
Iteration 5/25 | Loss: 0.00123455
Iteration 6/25 | Loss: 0.00122494
Iteration 7/25 | Loss: 0.00123118
Iteration 8/25 | Loss: 0.00122660
Iteration 9/25 | Loss: 0.00122506
Iteration 10/25 | Loss: 0.00122285
Iteration 11/25 | Loss: 0.00122250
Iteration 12/25 | Loss: 0.00122271
Iteration 13/25 | Loss: 0.00122271
Iteration 14/25 | Loss: 0.00122271
Iteration 15/25 | Loss: 0.00122270
Iteration 16/25 | Loss: 0.00122258
Iteration 17/25 | Loss: 0.00122241
Iteration 18/25 | Loss: 0.00122236
Iteration 19/25 | Loss: 0.00122236
Iteration 20/25 | Loss: 0.00122236
Iteration 21/25 | Loss: 0.00122236
Iteration 22/25 | Loss: 0.00122236
Iteration 23/25 | Loss: 0.00122236
Iteration 24/25 | Loss: 0.00122236
Iteration 25/25 | Loss: 0.00122235

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.16488028
Iteration 2/25 | Loss: 0.00144871
Iteration 3/25 | Loss: 0.00144869
Iteration 4/25 | Loss: 0.00144869
Iteration 5/25 | Loss: 0.00144869
Iteration 6/25 | Loss: 0.00144869
Iteration 7/25 | Loss: 0.00144869
Iteration 8/25 | Loss: 0.00144869
Iteration 9/25 | Loss: 0.00144869
Iteration 10/25 | Loss: 0.00144869
Iteration 11/25 | Loss: 0.00144869
Iteration 12/25 | Loss: 0.00144869
Iteration 13/25 | Loss: 0.00144869
Iteration 14/25 | Loss: 0.00144869
Iteration 15/25 | Loss: 0.00144869
Iteration 16/25 | Loss: 0.00144869
Iteration 17/25 | Loss: 0.00144869
Iteration 18/25 | Loss: 0.00144869
Iteration 19/25 | Loss: 0.00144869
Iteration 20/25 | Loss: 0.00144869
Iteration 21/25 | Loss: 0.00144869
Iteration 22/25 | Loss: 0.00144869
Iteration 23/25 | Loss: 0.00144869
Iteration 24/25 | Loss: 0.00144869
Iteration 25/25 | Loss: 0.00144869

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144869
Iteration 2/1000 | Loss: 0.00002659
Iteration 3/1000 | Loss: 0.00001997
Iteration 4/1000 | Loss: 0.00001833
Iteration 5/1000 | Loss: 0.00002356
Iteration 6/1000 | Loss: 0.00001698
Iteration 7/1000 | Loss: 0.00001652
Iteration 8/1000 | Loss: 0.00001642
Iteration 9/1000 | Loss: 0.00001616
Iteration 10/1000 | Loss: 0.00001598
Iteration 11/1000 | Loss: 0.00001576
Iteration 12/1000 | Loss: 0.00001571
Iteration 13/1000 | Loss: 0.00001569
Iteration 14/1000 | Loss: 0.00001566
Iteration 15/1000 | Loss: 0.00001566
Iteration 16/1000 | Loss: 0.00001563
Iteration 17/1000 | Loss: 0.00001562
Iteration 18/1000 | Loss: 0.00001562
Iteration 19/1000 | Loss: 0.00001562
Iteration 20/1000 | Loss: 0.00001561
Iteration 21/1000 | Loss: 0.00001560
Iteration 22/1000 | Loss: 0.00001560
Iteration 23/1000 | Loss: 0.00001554
Iteration 24/1000 | Loss: 0.00001554
Iteration 25/1000 | Loss: 0.00001553
Iteration 26/1000 | Loss: 0.00001551
Iteration 27/1000 | Loss: 0.00001549
Iteration 28/1000 | Loss: 0.00001549
Iteration 29/1000 | Loss: 0.00001549
Iteration 30/1000 | Loss: 0.00001548
Iteration 31/1000 | Loss: 0.00001548
Iteration 32/1000 | Loss: 0.00001546
Iteration 33/1000 | Loss: 0.00001545
Iteration 34/1000 | Loss: 0.00001543
Iteration 35/1000 | Loss: 0.00001542
Iteration 36/1000 | Loss: 0.00001542
Iteration 37/1000 | Loss: 0.00001542
Iteration 38/1000 | Loss: 0.00001542
Iteration 39/1000 | Loss: 0.00001542
Iteration 40/1000 | Loss: 0.00001542
Iteration 41/1000 | Loss: 0.00001542
Iteration 42/1000 | Loss: 0.00001542
Iteration 43/1000 | Loss: 0.00001542
Iteration 44/1000 | Loss: 0.00001542
Iteration 45/1000 | Loss: 0.00001542
Iteration 46/1000 | Loss: 0.00001541
Iteration 47/1000 | Loss: 0.00001541
Iteration 48/1000 | Loss: 0.00001541
Iteration 49/1000 | Loss: 0.00001539
Iteration 50/1000 | Loss: 0.00001539
Iteration 51/1000 | Loss: 0.00001539
Iteration 52/1000 | Loss: 0.00001539
Iteration 53/1000 | Loss: 0.00001539
Iteration 54/1000 | Loss: 0.00001538
Iteration 55/1000 | Loss: 0.00001538
Iteration 56/1000 | Loss: 0.00001537
Iteration 57/1000 | Loss: 0.00001537
Iteration 58/1000 | Loss: 0.00001536
Iteration 59/1000 | Loss: 0.00001536
Iteration 60/1000 | Loss: 0.00001535
Iteration 61/1000 | Loss: 0.00001535
Iteration 62/1000 | Loss: 0.00001534
Iteration 63/1000 | Loss: 0.00001534
Iteration 64/1000 | Loss: 0.00001534
Iteration 65/1000 | Loss: 0.00001534
Iteration 66/1000 | Loss: 0.00001533
Iteration 67/1000 | Loss: 0.00001533
Iteration 68/1000 | Loss: 0.00001532
Iteration 69/1000 | Loss: 0.00001532
Iteration 70/1000 | Loss: 0.00001532
Iteration 71/1000 | Loss: 0.00001531
Iteration 72/1000 | Loss: 0.00001530
Iteration 73/1000 | Loss: 0.00001530
Iteration 74/1000 | Loss: 0.00001529
Iteration 75/1000 | Loss: 0.00001529
Iteration 76/1000 | Loss: 0.00001529
Iteration 77/1000 | Loss: 0.00001529
Iteration 78/1000 | Loss: 0.00001529
Iteration 79/1000 | Loss: 0.00001529
Iteration 80/1000 | Loss: 0.00001529
Iteration 81/1000 | Loss: 0.00001529
Iteration 82/1000 | Loss: 0.00001529
Iteration 83/1000 | Loss: 0.00001528
Iteration 84/1000 | Loss: 0.00001528
Iteration 85/1000 | Loss: 0.00001528
Iteration 86/1000 | Loss: 0.00001528
Iteration 87/1000 | Loss: 0.00001527
Iteration 88/1000 | Loss: 0.00001527
Iteration 89/1000 | Loss: 0.00001526
Iteration 90/1000 | Loss: 0.00001526
Iteration 91/1000 | Loss: 0.00001525
Iteration 92/1000 | Loss: 0.00001525
Iteration 93/1000 | Loss: 0.00001525
Iteration 94/1000 | Loss: 0.00001525
Iteration 95/1000 | Loss: 0.00001524
Iteration 96/1000 | Loss: 0.00001524
Iteration 97/1000 | Loss: 0.00001524
Iteration 98/1000 | Loss: 0.00001523
Iteration 99/1000 | Loss: 0.00001523
Iteration 100/1000 | Loss: 0.00001523
Iteration 101/1000 | Loss: 0.00001522
Iteration 102/1000 | Loss: 0.00001522
Iteration 103/1000 | Loss: 0.00001522
Iteration 104/1000 | Loss: 0.00001521
Iteration 105/1000 | Loss: 0.00001521
Iteration 106/1000 | Loss: 0.00001521
Iteration 107/1000 | Loss: 0.00001520
Iteration 108/1000 | Loss: 0.00001520
Iteration 109/1000 | Loss: 0.00001520
Iteration 110/1000 | Loss: 0.00001519
Iteration 111/1000 | Loss: 0.00001519
Iteration 112/1000 | Loss: 0.00001519
Iteration 113/1000 | Loss: 0.00001519
Iteration 114/1000 | Loss: 0.00001519
Iteration 115/1000 | Loss: 0.00001518
Iteration 116/1000 | Loss: 0.00001518
Iteration 117/1000 | Loss: 0.00001518
Iteration 118/1000 | Loss: 0.00001518
Iteration 119/1000 | Loss: 0.00001518
Iteration 120/1000 | Loss: 0.00001517
Iteration 121/1000 | Loss: 0.00001517
Iteration 122/1000 | Loss: 0.00001517
Iteration 123/1000 | Loss: 0.00001517
Iteration 124/1000 | Loss: 0.00001517
Iteration 125/1000 | Loss: 0.00001517
Iteration 126/1000 | Loss: 0.00001517
Iteration 127/1000 | Loss: 0.00001517
Iteration 128/1000 | Loss: 0.00001516
Iteration 129/1000 | Loss: 0.00001516
Iteration 130/1000 | Loss: 0.00001516
Iteration 131/1000 | Loss: 0.00001515
Iteration 132/1000 | Loss: 0.00001515
Iteration 133/1000 | Loss: 0.00001515
Iteration 134/1000 | Loss: 0.00001514
Iteration 135/1000 | Loss: 0.00001514
Iteration 136/1000 | Loss: 0.00001514
Iteration 137/1000 | Loss: 0.00001514
Iteration 138/1000 | Loss: 0.00001514
Iteration 139/1000 | Loss: 0.00001514
Iteration 140/1000 | Loss: 0.00001514
Iteration 141/1000 | Loss: 0.00001514
Iteration 142/1000 | Loss: 0.00001514
Iteration 143/1000 | Loss: 0.00001514
Iteration 144/1000 | Loss: 0.00001514
Iteration 145/1000 | Loss: 0.00001514
Iteration 146/1000 | Loss: 0.00001514
Iteration 147/1000 | Loss: 0.00001513
Iteration 148/1000 | Loss: 0.00001513
Iteration 149/1000 | Loss: 0.00001513
Iteration 150/1000 | Loss: 0.00001513
Iteration 151/1000 | Loss: 0.00001513
Iteration 152/1000 | Loss: 0.00001513
Iteration 153/1000 | Loss: 0.00001513
Iteration 154/1000 | Loss: 0.00001513
Iteration 155/1000 | Loss: 0.00001513
Iteration 156/1000 | Loss: 0.00001513
Iteration 157/1000 | Loss: 0.00001513
Iteration 158/1000 | Loss: 0.00001512
Iteration 159/1000 | Loss: 0.00001512
Iteration 160/1000 | Loss: 0.00001512
Iteration 161/1000 | Loss: 0.00001512
Iteration 162/1000 | Loss: 0.00001512
Iteration 163/1000 | Loss: 0.00001512
Iteration 164/1000 | Loss: 0.00001512
Iteration 165/1000 | Loss: 0.00001512
Iteration 166/1000 | Loss: 0.00001512
Iteration 167/1000 | Loss: 0.00001512
Iteration 168/1000 | Loss: 0.00001512
Iteration 169/1000 | Loss: 0.00001512
Iteration 170/1000 | Loss: 0.00001512
Iteration 171/1000 | Loss: 0.00001511
Iteration 172/1000 | Loss: 0.00001511
Iteration 173/1000 | Loss: 0.00001511
Iteration 174/1000 | Loss: 0.00001511
Iteration 175/1000 | Loss: 0.00001511
Iteration 176/1000 | Loss: 0.00001511
Iteration 177/1000 | Loss: 0.00001511
Iteration 178/1000 | Loss: 0.00001511
Iteration 179/1000 | Loss: 0.00001511
Iteration 180/1000 | Loss: 0.00001511
Iteration 181/1000 | Loss: 0.00001511
Iteration 182/1000 | Loss: 0.00001511
Iteration 183/1000 | Loss: 0.00001511
Iteration 184/1000 | Loss: 0.00001510
Iteration 185/1000 | Loss: 0.00001510
Iteration 186/1000 | Loss: 0.00001510
Iteration 187/1000 | Loss: 0.00001510
Iteration 188/1000 | Loss: 0.00001510
Iteration 189/1000 | Loss: 0.00001510
Iteration 190/1000 | Loss: 0.00001510
Iteration 191/1000 | Loss: 0.00001510
Iteration 192/1000 | Loss: 0.00002033
Iteration 193/1000 | Loss: 0.00001631
Iteration 194/1000 | Loss: 0.00001508
Iteration 195/1000 | Loss: 0.00001507
Iteration 196/1000 | Loss: 0.00001507
Iteration 197/1000 | Loss: 0.00001507
Iteration 198/1000 | Loss: 0.00001507
Iteration 199/1000 | Loss: 0.00001507
Iteration 200/1000 | Loss: 0.00001507
Iteration 201/1000 | Loss: 0.00001507
Iteration 202/1000 | Loss: 0.00001507
Iteration 203/1000 | Loss: 0.00001507
Iteration 204/1000 | Loss: 0.00001506
Iteration 205/1000 | Loss: 0.00001506
Iteration 206/1000 | Loss: 0.00001506
Iteration 207/1000 | Loss: 0.00001506
Iteration 208/1000 | Loss: 0.00001506
Iteration 209/1000 | Loss: 0.00001506
Iteration 210/1000 | Loss: 0.00001506
Iteration 211/1000 | Loss: 0.00001506
Iteration 212/1000 | Loss: 0.00001506
Iteration 213/1000 | Loss: 0.00001506
Iteration 214/1000 | Loss: 0.00001506
Iteration 215/1000 | Loss: 0.00001506
Iteration 216/1000 | Loss: 0.00001506
Iteration 217/1000 | Loss: 0.00001506
Iteration 218/1000 | Loss: 0.00001506
Iteration 219/1000 | Loss: 0.00001506
Iteration 220/1000 | Loss: 0.00001506
Iteration 221/1000 | Loss: 0.00001506
Iteration 222/1000 | Loss: 0.00001506
Iteration 223/1000 | Loss: 0.00001506
Iteration 224/1000 | Loss: 0.00001506
Iteration 225/1000 | Loss: 0.00001506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.5064833860378712e-05, 1.5064833860378712e-05, 1.5064833860378712e-05, 1.5064833860378712e-05, 1.5064833860378712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5064833860378712e-05

Optimization complete. Final v2v error: 2.8634908199310303 mm

Highest mean error: 19.59033203125 mm for frame 189

Lowest mean error: 2.470245599746704 mm for frame 89

Saving results

Total time: 64.21151232719421
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483132
Iteration 2/25 | Loss: 0.00132168
Iteration 3/25 | Loss: 0.00124215
Iteration 4/25 | Loss: 0.00123032
Iteration 5/25 | Loss: 0.00122602
Iteration 6/25 | Loss: 0.00122550
Iteration 7/25 | Loss: 0.00122550
Iteration 8/25 | Loss: 0.00122550
Iteration 9/25 | Loss: 0.00122550
Iteration 10/25 | Loss: 0.00122550
Iteration 11/25 | Loss: 0.00122550
Iteration 12/25 | Loss: 0.00122550
Iteration 13/25 | Loss: 0.00122550
Iteration 14/25 | Loss: 0.00122550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012255043257027864, 0.0012255043257027864, 0.0012255043257027864, 0.0012255043257027864, 0.0012255043257027864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012255043257027864

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37307346
Iteration 2/25 | Loss: 0.00125663
Iteration 3/25 | Loss: 0.00125661
Iteration 4/25 | Loss: 0.00125661
Iteration 5/25 | Loss: 0.00125661
Iteration 6/25 | Loss: 0.00125661
Iteration 7/25 | Loss: 0.00125661
Iteration 8/25 | Loss: 0.00125661
Iteration 9/25 | Loss: 0.00125661
Iteration 10/25 | Loss: 0.00125661
Iteration 11/25 | Loss: 0.00125661
Iteration 12/25 | Loss: 0.00125661
Iteration 13/25 | Loss: 0.00125661
Iteration 14/25 | Loss: 0.00125661
Iteration 15/25 | Loss: 0.00125661
Iteration 16/25 | Loss: 0.00125661
Iteration 17/25 | Loss: 0.00125661
Iteration 18/25 | Loss: 0.00125661
Iteration 19/25 | Loss: 0.00125661
Iteration 20/25 | Loss: 0.00125661
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012566072400659323, 0.0012566072400659323, 0.0012566072400659323, 0.0012566072400659323, 0.0012566072400659323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012566072400659323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125661
Iteration 2/1000 | Loss: 0.00002905
Iteration 3/1000 | Loss: 0.00001727
Iteration 4/1000 | Loss: 0.00001549
Iteration 5/1000 | Loss: 0.00001483
Iteration 6/1000 | Loss: 0.00001428
Iteration 7/1000 | Loss: 0.00001395
Iteration 8/1000 | Loss: 0.00001370
Iteration 9/1000 | Loss: 0.00001354
Iteration 10/1000 | Loss: 0.00001327
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001305
Iteration 13/1000 | Loss: 0.00001297
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001292
Iteration 16/1000 | Loss: 0.00001286
Iteration 17/1000 | Loss: 0.00001286
Iteration 18/1000 | Loss: 0.00001286
Iteration 19/1000 | Loss: 0.00001286
Iteration 20/1000 | Loss: 0.00001285
Iteration 21/1000 | Loss: 0.00001279
Iteration 22/1000 | Loss: 0.00001272
Iteration 23/1000 | Loss: 0.00001269
Iteration 24/1000 | Loss: 0.00001265
Iteration 25/1000 | Loss: 0.00001265
Iteration 26/1000 | Loss: 0.00001263
Iteration 27/1000 | Loss: 0.00001262
Iteration 28/1000 | Loss: 0.00001261
Iteration 29/1000 | Loss: 0.00001260
Iteration 30/1000 | Loss: 0.00001257
Iteration 31/1000 | Loss: 0.00001257
Iteration 32/1000 | Loss: 0.00001255
Iteration 33/1000 | Loss: 0.00001253
Iteration 34/1000 | Loss: 0.00001252
Iteration 35/1000 | Loss: 0.00001251
Iteration 36/1000 | Loss: 0.00001251
Iteration 37/1000 | Loss: 0.00001251
Iteration 38/1000 | Loss: 0.00001251
Iteration 39/1000 | Loss: 0.00001250
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00001250
Iteration 42/1000 | Loss: 0.00001249
Iteration 43/1000 | Loss: 0.00001249
Iteration 44/1000 | Loss: 0.00001249
Iteration 45/1000 | Loss: 0.00001248
Iteration 46/1000 | Loss: 0.00001247
Iteration 47/1000 | Loss: 0.00001246
Iteration 48/1000 | Loss: 0.00001246
Iteration 49/1000 | Loss: 0.00001245
Iteration 50/1000 | Loss: 0.00001244
Iteration 51/1000 | Loss: 0.00001244
Iteration 52/1000 | Loss: 0.00001241
Iteration 53/1000 | Loss: 0.00001240
Iteration 54/1000 | Loss: 0.00001240
Iteration 55/1000 | Loss: 0.00001240
Iteration 56/1000 | Loss: 0.00001240
Iteration 57/1000 | Loss: 0.00001239
Iteration 58/1000 | Loss: 0.00001239
Iteration 59/1000 | Loss: 0.00001239
Iteration 60/1000 | Loss: 0.00001239
Iteration 61/1000 | Loss: 0.00001238
Iteration 62/1000 | Loss: 0.00001238
Iteration 63/1000 | Loss: 0.00001237
Iteration 64/1000 | Loss: 0.00001237
Iteration 65/1000 | Loss: 0.00001237
Iteration 66/1000 | Loss: 0.00001237
Iteration 67/1000 | Loss: 0.00001237
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001236
Iteration 70/1000 | Loss: 0.00001236
Iteration 71/1000 | Loss: 0.00001236
Iteration 72/1000 | Loss: 0.00001236
Iteration 73/1000 | Loss: 0.00001236
Iteration 74/1000 | Loss: 0.00001236
Iteration 75/1000 | Loss: 0.00001236
Iteration 76/1000 | Loss: 0.00001235
Iteration 77/1000 | Loss: 0.00001235
Iteration 78/1000 | Loss: 0.00001235
Iteration 79/1000 | Loss: 0.00001235
Iteration 80/1000 | Loss: 0.00001235
Iteration 81/1000 | Loss: 0.00001234
Iteration 82/1000 | Loss: 0.00001234
Iteration 83/1000 | Loss: 0.00001234
Iteration 84/1000 | Loss: 0.00001233
Iteration 85/1000 | Loss: 0.00001233
Iteration 86/1000 | Loss: 0.00001233
Iteration 87/1000 | Loss: 0.00001233
Iteration 88/1000 | Loss: 0.00001233
Iteration 89/1000 | Loss: 0.00001233
Iteration 90/1000 | Loss: 0.00001233
Iteration 91/1000 | Loss: 0.00001232
Iteration 92/1000 | Loss: 0.00001232
Iteration 93/1000 | Loss: 0.00001232
Iteration 94/1000 | Loss: 0.00001232
Iteration 95/1000 | Loss: 0.00001231
Iteration 96/1000 | Loss: 0.00001231
Iteration 97/1000 | Loss: 0.00001231
Iteration 98/1000 | Loss: 0.00001231
Iteration 99/1000 | Loss: 0.00001231
Iteration 100/1000 | Loss: 0.00001231
Iteration 101/1000 | Loss: 0.00001230
Iteration 102/1000 | Loss: 0.00001230
Iteration 103/1000 | Loss: 0.00001230
Iteration 104/1000 | Loss: 0.00001229
Iteration 105/1000 | Loss: 0.00001229
Iteration 106/1000 | Loss: 0.00001229
Iteration 107/1000 | Loss: 0.00001228
Iteration 108/1000 | Loss: 0.00001228
Iteration 109/1000 | Loss: 0.00001228
Iteration 110/1000 | Loss: 0.00001228
Iteration 111/1000 | Loss: 0.00001228
Iteration 112/1000 | Loss: 0.00001227
Iteration 113/1000 | Loss: 0.00001227
Iteration 114/1000 | Loss: 0.00001227
Iteration 115/1000 | Loss: 0.00001227
Iteration 116/1000 | Loss: 0.00001227
Iteration 117/1000 | Loss: 0.00001226
Iteration 118/1000 | Loss: 0.00001226
Iteration 119/1000 | Loss: 0.00001226
Iteration 120/1000 | Loss: 0.00001226
Iteration 121/1000 | Loss: 0.00001226
Iteration 122/1000 | Loss: 0.00001226
Iteration 123/1000 | Loss: 0.00001226
Iteration 124/1000 | Loss: 0.00001226
Iteration 125/1000 | Loss: 0.00001225
Iteration 126/1000 | Loss: 0.00001225
Iteration 127/1000 | Loss: 0.00001225
Iteration 128/1000 | Loss: 0.00001225
Iteration 129/1000 | Loss: 0.00001224
Iteration 130/1000 | Loss: 0.00001224
Iteration 131/1000 | Loss: 0.00001224
Iteration 132/1000 | Loss: 0.00001224
Iteration 133/1000 | Loss: 0.00001224
Iteration 134/1000 | Loss: 0.00001224
Iteration 135/1000 | Loss: 0.00001223
Iteration 136/1000 | Loss: 0.00001223
Iteration 137/1000 | Loss: 0.00001223
Iteration 138/1000 | Loss: 0.00001223
Iteration 139/1000 | Loss: 0.00001223
Iteration 140/1000 | Loss: 0.00001222
Iteration 141/1000 | Loss: 0.00001222
Iteration 142/1000 | Loss: 0.00001222
Iteration 143/1000 | Loss: 0.00001222
Iteration 144/1000 | Loss: 0.00001222
Iteration 145/1000 | Loss: 0.00001222
Iteration 146/1000 | Loss: 0.00001222
Iteration 147/1000 | Loss: 0.00001222
Iteration 148/1000 | Loss: 0.00001222
Iteration 149/1000 | Loss: 0.00001222
Iteration 150/1000 | Loss: 0.00001221
Iteration 151/1000 | Loss: 0.00001221
Iteration 152/1000 | Loss: 0.00001221
Iteration 153/1000 | Loss: 0.00001221
Iteration 154/1000 | Loss: 0.00001221
Iteration 155/1000 | Loss: 0.00001221
Iteration 156/1000 | Loss: 0.00001221
Iteration 157/1000 | Loss: 0.00001221
Iteration 158/1000 | Loss: 0.00001221
Iteration 159/1000 | Loss: 0.00001221
Iteration 160/1000 | Loss: 0.00001221
Iteration 161/1000 | Loss: 0.00001221
Iteration 162/1000 | Loss: 0.00001221
Iteration 163/1000 | Loss: 0.00001221
Iteration 164/1000 | Loss: 0.00001221
Iteration 165/1000 | Loss: 0.00001221
Iteration 166/1000 | Loss: 0.00001221
Iteration 167/1000 | Loss: 0.00001221
Iteration 168/1000 | Loss: 0.00001221
Iteration 169/1000 | Loss: 0.00001221
Iteration 170/1000 | Loss: 0.00001221
Iteration 171/1000 | Loss: 0.00001221
Iteration 172/1000 | Loss: 0.00001221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.2211367902636994e-05, 1.2211367902636994e-05, 1.2211367902636994e-05, 1.2211367902636994e-05, 1.2211367902636994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2211367902636994e-05

Optimization complete. Final v2v error: 2.971639633178711 mm

Highest mean error: 3.4417636394500732 mm for frame 105

Lowest mean error: 2.7370054721832275 mm for frame 149

Saving results

Total time: 46.37028646469116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527572
Iteration 2/25 | Loss: 0.00150417
Iteration 3/25 | Loss: 0.00130598
Iteration 4/25 | Loss: 0.00128620
Iteration 5/25 | Loss: 0.00128311
Iteration 6/25 | Loss: 0.00128251
Iteration 7/25 | Loss: 0.00128251
Iteration 8/25 | Loss: 0.00128251
Iteration 9/25 | Loss: 0.00128251
Iteration 10/25 | Loss: 0.00128251
Iteration 11/25 | Loss: 0.00128251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012825144222006202, 0.0012825144222006202, 0.0012825144222006202, 0.0012825144222006202, 0.0012825144222006202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012825144222006202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40910900
Iteration 2/25 | Loss: 0.00100565
Iteration 3/25 | Loss: 0.00100564
Iteration 4/25 | Loss: 0.00100564
Iteration 5/25 | Loss: 0.00100564
Iteration 6/25 | Loss: 0.00100564
Iteration 7/25 | Loss: 0.00100564
Iteration 8/25 | Loss: 0.00100564
Iteration 9/25 | Loss: 0.00100564
Iteration 10/25 | Loss: 0.00100564
Iteration 11/25 | Loss: 0.00100564
Iteration 12/25 | Loss: 0.00100564
Iteration 13/25 | Loss: 0.00100564
Iteration 14/25 | Loss: 0.00100564
Iteration 15/25 | Loss: 0.00100564
Iteration 16/25 | Loss: 0.00100564
Iteration 17/25 | Loss: 0.00100564
Iteration 18/25 | Loss: 0.00100564
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010056390892714262, 0.0010056390892714262, 0.0010056390892714262, 0.0010056390892714262, 0.0010056390892714262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010056390892714262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100564
Iteration 2/1000 | Loss: 0.00004060
Iteration 3/1000 | Loss: 0.00003161
Iteration 4/1000 | Loss: 0.00002666
Iteration 5/1000 | Loss: 0.00002505
Iteration 6/1000 | Loss: 0.00002394
Iteration 7/1000 | Loss: 0.00002339
Iteration 8/1000 | Loss: 0.00002286
Iteration 9/1000 | Loss: 0.00002243
Iteration 10/1000 | Loss: 0.00002201
Iteration 11/1000 | Loss: 0.00002171
Iteration 12/1000 | Loss: 0.00002164
Iteration 13/1000 | Loss: 0.00002146
Iteration 14/1000 | Loss: 0.00002128
Iteration 15/1000 | Loss: 0.00002111
Iteration 16/1000 | Loss: 0.00002107
Iteration 17/1000 | Loss: 0.00002105
Iteration 18/1000 | Loss: 0.00002104
Iteration 19/1000 | Loss: 0.00002104
Iteration 20/1000 | Loss: 0.00002103
Iteration 21/1000 | Loss: 0.00002102
Iteration 22/1000 | Loss: 0.00002102
Iteration 23/1000 | Loss: 0.00002101
Iteration 24/1000 | Loss: 0.00002101
Iteration 25/1000 | Loss: 0.00002100
Iteration 26/1000 | Loss: 0.00002100
Iteration 27/1000 | Loss: 0.00002099
Iteration 28/1000 | Loss: 0.00002099
Iteration 29/1000 | Loss: 0.00002099
Iteration 30/1000 | Loss: 0.00002098
Iteration 31/1000 | Loss: 0.00002096
Iteration 32/1000 | Loss: 0.00002096
Iteration 33/1000 | Loss: 0.00002096
Iteration 34/1000 | Loss: 0.00002096
Iteration 35/1000 | Loss: 0.00002095
Iteration 36/1000 | Loss: 0.00002095
Iteration 37/1000 | Loss: 0.00002095
Iteration 38/1000 | Loss: 0.00002095
Iteration 39/1000 | Loss: 0.00002094
Iteration 40/1000 | Loss: 0.00002094
Iteration 41/1000 | Loss: 0.00002094
Iteration 42/1000 | Loss: 0.00002094
Iteration 43/1000 | Loss: 0.00002094
Iteration 44/1000 | Loss: 0.00002093
Iteration 45/1000 | Loss: 0.00002093
Iteration 46/1000 | Loss: 0.00002093
Iteration 47/1000 | Loss: 0.00002093
Iteration 48/1000 | Loss: 0.00002093
Iteration 49/1000 | Loss: 0.00002093
Iteration 50/1000 | Loss: 0.00002093
Iteration 51/1000 | Loss: 0.00002093
Iteration 52/1000 | Loss: 0.00002093
Iteration 53/1000 | Loss: 0.00002092
Iteration 54/1000 | Loss: 0.00002092
Iteration 55/1000 | Loss: 0.00002091
Iteration 56/1000 | Loss: 0.00002090
Iteration 57/1000 | Loss: 0.00002090
Iteration 58/1000 | Loss: 0.00002090
Iteration 59/1000 | Loss: 0.00002090
Iteration 60/1000 | Loss: 0.00002090
Iteration 61/1000 | Loss: 0.00002090
Iteration 62/1000 | Loss: 0.00002090
Iteration 63/1000 | Loss: 0.00002090
Iteration 64/1000 | Loss: 0.00002089
Iteration 65/1000 | Loss: 0.00002089
Iteration 66/1000 | Loss: 0.00002088
Iteration 67/1000 | Loss: 0.00002088
Iteration 68/1000 | Loss: 0.00002087
Iteration 69/1000 | Loss: 0.00002087
Iteration 70/1000 | Loss: 0.00002087
Iteration 71/1000 | Loss: 0.00002087
Iteration 72/1000 | Loss: 0.00002086
Iteration 73/1000 | Loss: 0.00002086
Iteration 74/1000 | Loss: 0.00002086
Iteration 75/1000 | Loss: 0.00002086
Iteration 76/1000 | Loss: 0.00002085
Iteration 77/1000 | Loss: 0.00002085
Iteration 78/1000 | Loss: 0.00002085
Iteration 79/1000 | Loss: 0.00002085
Iteration 80/1000 | Loss: 0.00002085
Iteration 81/1000 | Loss: 0.00002085
Iteration 82/1000 | Loss: 0.00002085
Iteration 83/1000 | Loss: 0.00002084
Iteration 84/1000 | Loss: 0.00002084
Iteration 85/1000 | Loss: 0.00002084
Iteration 86/1000 | Loss: 0.00002084
Iteration 87/1000 | Loss: 0.00002084
Iteration 88/1000 | Loss: 0.00002083
Iteration 89/1000 | Loss: 0.00002083
Iteration 90/1000 | Loss: 0.00002083
Iteration 91/1000 | Loss: 0.00002083
Iteration 92/1000 | Loss: 0.00002083
Iteration 93/1000 | Loss: 0.00002083
Iteration 94/1000 | Loss: 0.00002083
Iteration 95/1000 | Loss: 0.00002083
Iteration 96/1000 | Loss: 0.00002083
Iteration 97/1000 | Loss: 0.00002082
Iteration 98/1000 | Loss: 0.00002082
Iteration 99/1000 | Loss: 0.00002082
Iteration 100/1000 | Loss: 0.00002082
Iteration 101/1000 | Loss: 0.00002082
Iteration 102/1000 | Loss: 0.00002082
Iteration 103/1000 | Loss: 0.00002081
Iteration 104/1000 | Loss: 0.00002081
Iteration 105/1000 | Loss: 0.00002081
Iteration 106/1000 | Loss: 0.00002081
Iteration 107/1000 | Loss: 0.00002081
Iteration 108/1000 | Loss: 0.00002081
Iteration 109/1000 | Loss: 0.00002080
Iteration 110/1000 | Loss: 0.00002080
Iteration 111/1000 | Loss: 0.00002079
Iteration 112/1000 | Loss: 0.00002079
Iteration 113/1000 | Loss: 0.00002079
Iteration 114/1000 | Loss: 0.00002079
Iteration 115/1000 | Loss: 0.00002079
Iteration 116/1000 | Loss: 0.00002078
Iteration 117/1000 | Loss: 0.00002078
Iteration 118/1000 | Loss: 0.00002078
Iteration 119/1000 | Loss: 0.00002078
Iteration 120/1000 | Loss: 0.00002078
Iteration 121/1000 | Loss: 0.00002077
Iteration 122/1000 | Loss: 0.00002077
Iteration 123/1000 | Loss: 0.00002077
Iteration 124/1000 | Loss: 0.00002077
Iteration 125/1000 | Loss: 0.00002076
Iteration 126/1000 | Loss: 0.00002076
Iteration 127/1000 | Loss: 0.00002076
Iteration 128/1000 | Loss: 0.00002076
Iteration 129/1000 | Loss: 0.00002076
Iteration 130/1000 | Loss: 0.00002076
Iteration 131/1000 | Loss: 0.00002075
Iteration 132/1000 | Loss: 0.00002075
Iteration 133/1000 | Loss: 0.00002075
Iteration 134/1000 | Loss: 0.00002074
Iteration 135/1000 | Loss: 0.00002074
Iteration 136/1000 | Loss: 0.00002074
Iteration 137/1000 | Loss: 0.00002074
Iteration 138/1000 | Loss: 0.00002074
Iteration 139/1000 | Loss: 0.00002074
Iteration 140/1000 | Loss: 0.00002074
Iteration 141/1000 | Loss: 0.00002074
Iteration 142/1000 | Loss: 0.00002074
Iteration 143/1000 | Loss: 0.00002074
Iteration 144/1000 | Loss: 0.00002074
Iteration 145/1000 | Loss: 0.00002074
Iteration 146/1000 | Loss: 0.00002074
Iteration 147/1000 | Loss: 0.00002074
Iteration 148/1000 | Loss: 0.00002074
Iteration 149/1000 | Loss: 0.00002074
Iteration 150/1000 | Loss: 0.00002074
Iteration 151/1000 | Loss: 0.00002074
Iteration 152/1000 | Loss: 0.00002074
Iteration 153/1000 | Loss: 0.00002074
Iteration 154/1000 | Loss: 0.00002074
Iteration 155/1000 | Loss: 0.00002074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.0740126274176873e-05, 2.0740126274176873e-05, 2.0740126274176873e-05, 2.0740126274176873e-05, 2.0740126274176873e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0740126274176873e-05

Optimization complete. Final v2v error: 3.815016746520996 mm

Highest mean error: 4.302363872528076 mm for frame 78

Lowest mean error: 3.508882999420166 mm for frame 13

Saving results

Total time: 37.827985525131226
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00750665
Iteration 2/25 | Loss: 0.00134870
Iteration 3/25 | Loss: 0.00125573
Iteration 4/25 | Loss: 0.00124346
Iteration 5/25 | Loss: 0.00123831
Iteration 6/25 | Loss: 0.00123716
Iteration 7/25 | Loss: 0.00123716
Iteration 8/25 | Loss: 0.00123716
Iteration 9/25 | Loss: 0.00123716
Iteration 10/25 | Loss: 0.00123716
Iteration 11/25 | Loss: 0.00123716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012371608754619956, 0.0012371608754619956, 0.0012371608754619956, 0.0012371608754619956, 0.0012371608754619956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012371608754619956

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41026080
Iteration 2/25 | Loss: 0.00145564
Iteration 3/25 | Loss: 0.00145564
Iteration 4/25 | Loss: 0.00145564
Iteration 5/25 | Loss: 0.00145564
Iteration 6/25 | Loss: 0.00145564
Iteration 7/25 | Loss: 0.00145564
Iteration 8/25 | Loss: 0.00145564
Iteration 9/25 | Loss: 0.00145564
Iteration 10/25 | Loss: 0.00145563
Iteration 11/25 | Loss: 0.00145563
Iteration 12/25 | Loss: 0.00145563
Iteration 13/25 | Loss: 0.00145563
Iteration 14/25 | Loss: 0.00145563
Iteration 15/25 | Loss: 0.00145563
Iteration 16/25 | Loss: 0.00145563
Iteration 17/25 | Loss: 0.00145563
Iteration 18/25 | Loss: 0.00145563
Iteration 19/25 | Loss: 0.00145563
Iteration 20/25 | Loss: 0.00145563
Iteration 21/25 | Loss: 0.00145563
Iteration 22/25 | Loss: 0.00145563
Iteration 23/25 | Loss: 0.00145563
Iteration 24/25 | Loss: 0.00145563
Iteration 25/25 | Loss: 0.00145563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145563
Iteration 2/1000 | Loss: 0.00003002
Iteration 3/1000 | Loss: 0.00002049
Iteration 4/1000 | Loss: 0.00001886
Iteration 5/1000 | Loss: 0.00001802
Iteration 6/1000 | Loss: 0.00001725
Iteration 7/1000 | Loss: 0.00001684
Iteration 8/1000 | Loss: 0.00001641
Iteration 9/1000 | Loss: 0.00001613
Iteration 10/1000 | Loss: 0.00001596
Iteration 11/1000 | Loss: 0.00001583
Iteration 12/1000 | Loss: 0.00001565
Iteration 13/1000 | Loss: 0.00001552
Iteration 14/1000 | Loss: 0.00001540
Iteration 15/1000 | Loss: 0.00001531
Iteration 16/1000 | Loss: 0.00001523
Iteration 17/1000 | Loss: 0.00001518
Iteration 18/1000 | Loss: 0.00001517
Iteration 19/1000 | Loss: 0.00001517
Iteration 20/1000 | Loss: 0.00001517
Iteration 21/1000 | Loss: 0.00001513
Iteration 22/1000 | Loss: 0.00001512
Iteration 23/1000 | Loss: 0.00001511
Iteration 24/1000 | Loss: 0.00001510
Iteration 25/1000 | Loss: 0.00001509
Iteration 26/1000 | Loss: 0.00001509
Iteration 27/1000 | Loss: 0.00001509
Iteration 28/1000 | Loss: 0.00001509
Iteration 29/1000 | Loss: 0.00001508
Iteration 30/1000 | Loss: 0.00001508
Iteration 31/1000 | Loss: 0.00001507
Iteration 32/1000 | Loss: 0.00001507
Iteration 33/1000 | Loss: 0.00001507
Iteration 34/1000 | Loss: 0.00001507
Iteration 35/1000 | Loss: 0.00001507
Iteration 36/1000 | Loss: 0.00001506
Iteration 37/1000 | Loss: 0.00001506
Iteration 38/1000 | Loss: 0.00001506
Iteration 39/1000 | Loss: 0.00001505
Iteration 40/1000 | Loss: 0.00001504
Iteration 41/1000 | Loss: 0.00001504
Iteration 42/1000 | Loss: 0.00001504
Iteration 43/1000 | Loss: 0.00001504
Iteration 44/1000 | Loss: 0.00001503
Iteration 45/1000 | Loss: 0.00001503
Iteration 46/1000 | Loss: 0.00001503
Iteration 47/1000 | Loss: 0.00001503
Iteration 48/1000 | Loss: 0.00001502
Iteration 49/1000 | Loss: 0.00001502
Iteration 50/1000 | Loss: 0.00001502
Iteration 51/1000 | Loss: 0.00001502
Iteration 52/1000 | Loss: 0.00001502
Iteration 53/1000 | Loss: 0.00001501
Iteration 54/1000 | Loss: 0.00001501
Iteration 55/1000 | Loss: 0.00001500
Iteration 56/1000 | Loss: 0.00001500
Iteration 57/1000 | Loss: 0.00001500
Iteration 58/1000 | Loss: 0.00001499
Iteration 59/1000 | Loss: 0.00001499
Iteration 60/1000 | Loss: 0.00001499
Iteration 61/1000 | Loss: 0.00001498
Iteration 62/1000 | Loss: 0.00001498
Iteration 63/1000 | Loss: 0.00001498
Iteration 64/1000 | Loss: 0.00001497
Iteration 65/1000 | Loss: 0.00001497
Iteration 66/1000 | Loss: 0.00001497
Iteration 67/1000 | Loss: 0.00001497
Iteration 68/1000 | Loss: 0.00001497
Iteration 69/1000 | Loss: 0.00001496
Iteration 70/1000 | Loss: 0.00001496
Iteration 71/1000 | Loss: 0.00001496
Iteration 72/1000 | Loss: 0.00001495
Iteration 73/1000 | Loss: 0.00001495
Iteration 74/1000 | Loss: 0.00001494
Iteration 75/1000 | Loss: 0.00001494
Iteration 76/1000 | Loss: 0.00001494
Iteration 77/1000 | Loss: 0.00001493
Iteration 78/1000 | Loss: 0.00001493
Iteration 79/1000 | Loss: 0.00001492
Iteration 80/1000 | Loss: 0.00001491
Iteration 81/1000 | Loss: 0.00001491
Iteration 82/1000 | Loss: 0.00001491
Iteration 83/1000 | Loss: 0.00001491
Iteration 84/1000 | Loss: 0.00001491
Iteration 85/1000 | Loss: 0.00001491
Iteration 86/1000 | Loss: 0.00001490
Iteration 87/1000 | Loss: 0.00001490
Iteration 88/1000 | Loss: 0.00001490
Iteration 89/1000 | Loss: 0.00001490
Iteration 90/1000 | Loss: 0.00001490
Iteration 91/1000 | Loss: 0.00001490
Iteration 92/1000 | Loss: 0.00001490
Iteration 93/1000 | Loss: 0.00001489
Iteration 94/1000 | Loss: 0.00001489
Iteration 95/1000 | Loss: 0.00001489
Iteration 96/1000 | Loss: 0.00001489
Iteration 97/1000 | Loss: 0.00001489
Iteration 98/1000 | Loss: 0.00001489
Iteration 99/1000 | Loss: 0.00001489
Iteration 100/1000 | Loss: 0.00001489
Iteration 101/1000 | Loss: 0.00001489
Iteration 102/1000 | Loss: 0.00001489
Iteration 103/1000 | Loss: 0.00001489
Iteration 104/1000 | Loss: 0.00001489
Iteration 105/1000 | Loss: 0.00001489
Iteration 106/1000 | Loss: 0.00001489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.4893684237904381e-05, 1.4893684237904381e-05, 1.4893684237904381e-05, 1.4893684237904381e-05, 1.4893684237904381e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4893684237904381e-05

Optimization complete. Final v2v error: 3.294947624206543 mm

Highest mean error: 4.051083564758301 mm for frame 198

Lowest mean error: 2.8039042949676514 mm for frame 8

Saving results

Total time: 40.36892914772034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970833
Iteration 2/25 | Loss: 0.00162322
Iteration 3/25 | Loss: 0.00141276
Iteration 4/25 | Loss: 0.00145524
Iteration 5/25 | Loss: 0.00138598
Iteration 6/25 | Loss: 0.00137105
Iteration 7/25 | Loss: 0.00134249
Iteration 8/25 | Loss: 0.00133345
Iteration 9/25 | Loss: 0.00132202
Iteration 10/25 | Loss: 0.00132067
Iteration 11/25 | Loss: 0.00131861
Iteration 12/25 | Loss: 0.00131448
Iteration 13/25 | Loss: 0.00131104
Iteration 14/25 | Loss: 0.00130989
Iteration 15/25 | Loss: 0.00130954
Iteration 16/25 | Loss: 0.00130817
Iteration 17/25 | Loss: 0.00130822
Iteration 18/25 | Loss: 0.00130715
Iteration 19/25 | Loss: 0.00130388
Iteration 20/25 | Loss: 0.00130195
Iteration 21/25 | Loss: 0.00130153
Iteration 22/25 | Loss: 0.00130139
Iteration 23/25 | Loss: 0.00130138
Iteration 24/25 | Loss: 0.00130137
Iteration 25/25 | Loss: 0.00130135

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22167814
Iteration 2/25 | Loss: 0.00179484
Iteration 3/25 | Loss: 0.00179482
Iteration 4/25 | Loss: 0.00179481
Iteration 5/25 | Loss: 0.00179481
Iteration 6/25 | Loss: 0.00179481
Iteration 7/25 | Loss: 0.00179481
Iteration 8/25 | Loss: 0.00179481
Iteration 9/25 | Loss: 0.00179481
Iteration 10/25 | Loss: 0.00179481
Iteration 11/25 | Loss: 0.00179481
Iteration 12/25 | Loss: 0.00179481
Iteration 13/25 | Loss: 0.00179481
Iteration 14/25 | Loss: 0.00179481
Iteration 15/25 | Loss: 0.00179481
Iteration 16/25 | Loss: 0.00179481
Iteration 17/25 | Loss: 0.00179481
Iteration 18/25 | Loss: 0.00179481
Iteration 19/25 | Loss: 0.00179481
Iteration 20/25 | Loss: 0.00179481
Iteration 21/25 | Loss: 0.00179481
Iteration 22/25 | Loss: 0.00179481
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001794810057617724, 0.001794810057617724, 0.001794810057617724, 0.001794810057617724, 0.001794810057617724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001794810057617724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179481
Iteration 2/1000 | Loss: 0.00005457
Iteration 3/1000 | Loss: 0.00167458
Iteration 4/1000 | Loss: 0.00003565
Iteration 5/1000 | Loss: 0.00003083
Iteration 6/1000 | Loss: 0.00002889
Iteration 7/1000 | Loss: 0.00002745
Iteration 8/1000 | Loss: 0.00002648
Iteration 9/1000 | Loss: 0.00002577
Iteration 10/1000 | Loss: 0.00107902
Iteration 11/1000 | Loss: 0.00007434
Iteration 12/1000 | Loss: 0.00004709
Iteration 13/1000 | Loss: 0.00003641
Iteration 14/1000 | Loss: 0.00003342
Iteration 15/1000 | Loss: 0.00002832
Iteration 16/1000 | Loss: 0.00002567
Iteration 17/1000 | Loss: 0.00150766
Iteration 18/1000 | Loss: 0.00121177
Iteration 19/1000 | Loss: 0.00013850
Iteration 20/1000 | Loss: 0.00003517
Iteration 21/1000 | Loss: 0.00002919
Iteration 22/1000 | Loss: 0.00002407
Iteration 23/1000 | Loss: 0.00002201
Iteration 24/1000 | Loss: 0.00002132
Iteration 25/1000 | Loss: 0.00002104
Iteration 26/1000 | Loss: 0.00002078
Iteration 27/1000 | Loss: 0.00002073
Iteration 28/1000 | Loss: 0.00002067
Iteration 29/1000 | Loss: 0.00002056
Iteration 30/1000 | Loss: 0.00002053
Iteration 31/1000 | Loss: 0.00002052
Iteration 32/1000 | Loss: 0.00002052
Iteration 33/1000 | Loss: 0.00002052
Iteration 34/1000 | Loss: 0.00002049
Iteration 35/1000 | Loss: 0.00002049
Iteration 36/1000 | Loss: 0.00002047
Iteration 37/1000 | Loss: 0.00002046
Iteration 38/1000 | Loss: 0.00002046
Iteration 39/1000 | Loss: 0.00002045
Iteration 40/1000 | Loss: 0.00002044
Iteration 41/1000 | Loss: 0.00002043
Iteration 42/1000 | Loss: 0.00002043
Iteration 43/1000 | Loss: 0.00002043
Iteration 44/1000 | Loss: 0.00002042
Iteration 45/1000 | Loss: 0.00002038
Iteration 46/1000 | Loss: 0.00002037
Iteration 47/1000 | Loss: 0.00002036
Iteration 48/1000 | Loss: 0.00002035
Iteration 49/1000 | Loss: 0.00002035
Iteration 50/1000 | Loss: 0.00002035
Iteration 51/1000 | Loss: 0.00002034
Iteration 52/1000 | Loss: 0.00002034
Iteration 53/1000 | Loss: 0.00002034
Iteration 54/1000 | Loss: 0.00002033
Iteration 55/1000 | Loss: 0.00002033
Iteration 56/1000 | Loss: 0.00002033
Iteration 57/1000 | Loss: 0.00002033
Iteration 58/1000 | Loss: 0.00002032
Iteration 59/1000 | Loss: 0.00002032
Iteration 60/1000 | Loss: 0.00002031
Iteration 61/1000 | Loss: 0.00002031
Iteration 62/1000 | Loss: 0.00002031
Iteration 63/1000 | Loss: 0.00002031
Iteration 64/1000 | Loss: 0.00002031
Iteration 65/1000 | Loss: 0.00002031
Iteration 66/1000 | Loss: 0.00002031
Iteration 67/1000 | Loss: 0.00002031
Iteration 68/1000 | Loss: 0.00002031
Iteration 69/1000 | Loss: 0.00002031
Iteration 70/1000 | Loss: 0.00002030
Iteration 71/1000 | Loss: 0.00002030
Iteration 72/1000 | Loss: 0.00002030
Iteration 73/1000 | Loss: 0.00002029
Iteration 74/1000 | Loss: 0.00002029
Iteration 75/1000 | Loss: 0.00002029
Iteration 76/1000 | Loss: 0.00002028
Iteration 77/1000 | Loss: 0.00002028
Iteration 78/1000 | Loss: 0.00002028
Iteration 79/1000 | Loss: 0.00002027
Iteration 80/1000 | Loss: 0.00002027
Iteration 81/1000 | Loss: 0.00002027
Iteration 82/1000 | Loss: 0.00002026
Iteration 83/1000 | Loss: 0.00002025
Iteration 84/1000 | Loss: 0.00002023
Iteration 85/1000 | Loss: 0.00002022
Iteration 86/1000 | Loss: 0.00002022
Iteration 87/1000 | Loss: 0.00002019
Iteration 88/1000 | Loss: 0.00002019
Iteration 89/1000 | Loss: 0.00002019
Iteration 90/1000 | Loss: 0.00002017
Iteration 91/1000 | Loss: 0.00002017
Iteration 92/1000 | Loss: 0.00002016
Iteration 93/1000 | Loss: 0.00002016
Iteration 94/1000 | Loss: 0.00002014
Iteration 95/1000 | Loss: 0.00002014
Iteration 96/1000 | Loss: 0.00002014
Iteration 97/1000 | Loss: 0.00002014
Iteration 98/1000 | Loss: 0.00002013
Iteration 99/1000 | Loss: 0.00002012
Iteration 100/1000 | Loss: 0.00002012
Iteration 101/1000 | Loss: 0.00002010
Iteration 102/1000 | Loss: 0.00002010
Iteration 103/1000 | Loss: 0.00002010
Iteration 104/1000 | Loss: 0.00002010
Iteration 105/1000 | Loss: 0.00002010
Iteration 106/1000 | Loss: 0.00002010
Iteration 107/1000 | Loss: 0.00002009
Iteration 108/1000 | Loss: 0.00002006
Iteration 109/1000 | Loss: 0.00002006
Iteration 110/1000 | Loss: 0.00002006
Iteration 111/1000 | Loss: 0.00002002
Iteration 112/1000 | Loss: 0.00002002
Iteration 113/1000 | Loss: 0.00002001
Iteration 114/1000 | Loss: 0.00002000
Iteration 115/1000 | Loss: 0.00001999
Iteration 116/1000 | Loss: 0.00001998
Iteration 117/1000 | Loss: 0.00001998
Iteration 118/1000 | Loss: 0.00001998
Iteration 119/1000 | Loss: 0.00001998
Iteration 120/1000 | Loss: 0.00001997
Iteration 121/1000 | Loss: 0.00001997
Iteration 122/1000 | Loss: 0.00001996
Iteration 123/1000 | Loss: 0.00001996
Iteration 124/1000 | Loss: 0.00001996
Iteration 125/1000 | Loss: 0.00001996
Iteration 126/1000 | Loss: 0.00001995
Iteration 127/1000 | Loss: 0.00001995
Iteration 128/1000 | Loss: 0.00001995
Iteration 129/1000 | Loss: 0.00001994
Iteration 130/1000 | Loss: 0.00001994
Iteration 131/1000 | Loss: 0.00001994
Iteration 132/1000 | Loss: 0.00001994
Iteration 133/1000 | Loss: 0.00001994
Iteration 134/1000 | Loss: 0.00001994
Iteration 135/1000 | Loss: 0.00001994
Iteration 136/1000 | Loss: 0.00001993
Iteration 137/1000 | Loss: 0.00001993
Iteration 138/1000 | Loss: 0.00001993
Iteration 139/1000 | Loss: 0.00001993
Iteration 140/1000 | Loss: 0.00001993
Iteration 141/1000 | Loss: 0.00001993
Iteration 142/1000 | Loss: 0.00001993
Iteration 143/1000 | Loss: 0.00001992
Iteration 144/1000 | Loss: 0.00001992
Iteration 145/1000 | Loss: 0.00001992
Iteration 146/1000 | Loss: 0.00001991
Iteration 147/1000 | Loss: 0.00001991
Iteration 148/1000 | Loss: 0.00001991
Iteration 149/1000 | Loss: 0.00001990
Iteration 150/1000 | Loss: 0.00001990
Iteration 151/1000 | Loss: 0.00001990
Iteration 152/1000 | Loss: 0.00001990
Iteration 153/1000 | Loss: 0.00001989
Iteration 154/1000 | Loss: 0.00001989
Iteration 155/1000 | Loss: 0.00001989
Iteration 156/1000 | Loss: 0.00001988
Iteration 157/1000 | Loss: 0.00001988
Iteration 158/1000 | Loss: 0.00001988
Iteration 159/1000 | Loss: 0.00001988
Iteration 160/1000 | Loss: 0.00001988
Iteration 161/1000 | Loss: 0.00001987
Iteration 162/1000 | Loss: 0.00001987
Iteration 163/1000 | Loss: 0.00001987
Iteration 164/1000 | Loss: 0.00001987
Iteration 165/1000 | Loss: 0.00001986
Iteration 166/1000 | Loss: 0.00001986
Iteration 167/1000 | Loss: 0.00001986
Iteration 168/1000 | Loss: 0.00001986
Iteration 169/1000 | Loss: 0.00001985
Iteration 170/1000 | Loss: 0.00001985
Iteration 171/1000 | Loss: 0.00001985
Iteration 172/1000 | Loss: 0.00001985
Iteration 173/1000 | Loss: 0.00001985
Iteration 174/1000 | Loss: 0.00001985
Iteration 175/1000 | Loss: 0.00001985
Iteration 176/1000 | Loss: 0.00001985
Iteration 177/1000 | Loss: 0.00001985
Iteration 178/1000 | Loss: 0.00001984
Iteration 179/1000 | Loss: 0.00001984
Iteration 180/1000 | Loss: 0.00001984
Iteration 181/1000 | Loss: 0.00001984
Iteration 182/1000 | Loss: 0.00001984
Iteration 183/1000 | Loss: 0.00001984
Iteration 184/1000 | Loss: 0.00001984
Iteration 185/1000 | Loss: 0.00001983
Iteration 186/1000 | Loss: 0.00001983
Iteration 187/1000 | Loss: 0.00001983
Iteration 188/1000 | Loss: 0.00001983
Iteration 189/1000 | Loss: 0.00001983
Iteration 190/1000 | Loss: 0.00001983
Iteration 191/1000 | Loss: 0.00001983
Iteration 192/1000 | Loss: 0.00001983
Iteration 193/1000 | Loss: 0.00001982
Iteration 194/1000 | Loss: 0.00001982
Iteration 195/1000 | Loss: 0.00001982
Iteration 196/1000 | Loss: 0.00001982
Iteration 197/1000 | Loss: 0.00001982
Iteration 198/1000 | Loss: 0.00001982
Iteration 199/1000 | Loss: 0.00001982
Iteration 200/1000 | Loss: 0.00001982
Iteration 201/1000 | Loss: 0.00001982
Iteration 202/1000 | Loss: 0.00001981
Iteration 203/1000 | Loss: 0.00001981
Iteration 204/1000 | Loss: 0.00001981
Iteration 205/1000 | Loss: 0.00001981
Iteration 206/1000 | Loss: 0.00001981
Iteration 207/1000 | Loss: 0.00001981
Iteration 208/1000 | Loss: 0.00001981
Iteration 209/1000 | Loss: 0.00001981
Iteration 210/1000 | Loss: 0.00001981
Iteration 211/1000 | Loss: 0.00001981
Iteration 212/1000 | Loss: 0.00001981
Iteration 213/1000 | Loss: 0.00001981
Iteration 214/1000 | Loss: 0.00001981
Iteration 215/1000 | Loss: 0.00001981
Iteration 216/1000 | Loss: 0.00001981
Iteration 217/1000 | Loss: 0.00001981
Iteration 218/1000 | Loss: 0.00001980
Iteration 219/1000 | Loss: 0.00001980
Iteration 220/1000 | Loss: 0.00001980
Iteration 221/1000 | Loss: 0.00001980
Iteration 222/1000 | Loss: 0.00001980
Iteration 223/1000 | Loss: 0.00001980
Iteration 224/1000 | Loss: 0.00001980
Iteration 225/1000 | Loss: 0.00001980
Iteration 226/1000 | Loss: 0.00001980
Iteration 227/1000 | Loss: 0.00001980
Iteration 228/1000 | Loss: 0.00001980
Iteration 229/1000 | Loss: 0.00001980
Iteration 230/1000 | Loss: 0.00001980
Iteration 231/1000 | Loss: 0.00001980
Iteration 232/1000 | Loss: 0.00001980
Iteration 233/1000 | Loss: 0.00001980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.980287561309524e-05, 1.980287561309524e-05, 1.980287561309524e-05, 1.980287561309524e-05, 1.980287561309524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.980287561309524e-05

Optimization complete. Final v2v error: 3.63942813873291 mm

Highest mean error: 4.615810394287109 mm for frame 79

Lowest mean error: 2.712639808654785 mm for frame 168

Saving results

Total time: 102.88449692726135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071464
Iteration 2/25 | Loss: 0.00193096
Iteration 3/25 | Loss: 0.00150143
Iteration 4/25 | Loss: 0.00144528
Iteration 5/25 | Loss: 0.00172117
Iteration 6/25 | Loss: 0.00147560
Iteration 7/25 | Loss: 0.00140827
Iteration 8/25 | Loss: 0.00130351
Iteration 9/25 | Loss: 0.00129681
Iteration 10/25 | Loss: 0.00129618
Iteration 11/25 | Loss: 0.00129604
Iteration 12/25 | Loss: 0.00129603
Iteration 13/25 | Loss: 0.00129602
Iteration 14/25 | Loss: 0.00129602
Iteration 15/25 | Loss: 0.00129601
Iteration 16/25 | Loss: 0.00129601
Iteration 17/25 | Loss: 0.00129601
Iteration 18/25 | Loss: 0.00129601
Iteration 19/25 | Loss: 0.00129601
Iteration 20/25 | Loss: 0.00129601
Iteration 21/25 | Loss: 0.00129601
Iteration 22/25 | Loss: 0.00129601
Iteration 23/25 | Loss: 0.00129601
Iteration 24/25 | Loss: 0.00129601
Iteration 25/25 | Loss: 0.00129600

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05150199
Iteration 2/25 | Loss: 0.00161884
Iteration 3/25 | Loss: 0.00161884
Iteration 4/25 | Loss: 0.00161883
Iteration 5/25 | Loss: 0.00161883
Iteration 6/25 | Loss: 0.00161883
Iteration 7/25 | Loss: 0.00161883
Iteration 8/25 | Loss: 0.00161883
Iteration 9/25 | Loss: 0.00161883
Iteration 10/25 | Loss: 0.00161883
Iteration 11/25 | Loss: 0.00161883
Iteration 12/25 | Loss: 0.00161883
Iteration 13/25 | Loss: 0.00161883
Iteration 14/25 | Loss: 0.00161883
Iteration 15/25 | Loss: 0.00161883
Iteration 16/25 | Loss: 0.00161883
Iteration 17/25 | Loss: 0.00161883
Iteration 18/25 | Loss: 0.00161883
Iteration 19/25 | Loss: 0.00161883
Iteration 20/25 | Loss: 0.00161883
Iteration 21/25 | Loss: 0.00161883
Iteration 22/25 | Loss: 0.00161883
Iteration 23/25 | Loss: 0.00161883
Iteration 24/25 | Loss: 0.00161883
Iteration 25/25 | Loss: 0.00161883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161883
Iteration 2/1000 | Loss: 0.00006591
Iteration 3/1000 | Loss: 0.00004000
Iteration 4/1000 | Loss: 0.00002911
Iteration 5/1000 | Loss: 0.00002661
Iteration 6/1000 | Loss: 0.00002528
Iteration 7/1000 | Loss: 0.00002445
Iteration 8/1000 | Loss: 0.00002380
Iteration 9/1000 | Loss: 0.00002338
Iteration 10/1000 | Loss: 0.00002304
Iteration 11/1000 | Loss: 0.00002277
Iteration 12/1000 | Loss: 0.00002257
Iteration 13/1000 | Loss: 0.00002244
Iteration 14/1000 | Loss: 0.00002227
Iteration 15/1000 | Loss: 0.00002223
Iteration 16/1000 | Loss: 0.00002209
Iteration 17/1000 | Loss: 0.00002206
Iteration 18/1000 | Loss: 0.00002201
Iteration 19/1000 | Loss: 0.00002199
Iteration 20/1000 | Loss: 0.00002199
Iteration 21/1000 | Loss: 0.00002198
Iteration 22/1000 | Loss: 0.00002197
Iteration 23/1000 | Loss: 0.00002197
Iteration 24/1000 | Loss: 0.00002192
Iteration 25/1000 | Loss: 0.00002191
Iteration 26/1000 | Loss: 0.00002186
Iteration 27/1000 | Loss: 0.00002180
Iteration 28/1000 | Loss: 0.00002172
Iteration 29/1000 | Loss: 0.00002170
Iteration 30/1000 | Loss: 0.00002170
Iteration 31/1000 | Loss: 0.00002170
Iteration 32/1000 | Loss: 0.00002170
Iteration 33/1000 | Loss: 0.00002170
Iteration 34/1000 | Loss: 0.00002170
Iteration 35/1000 | Loss: 0.00002170
Iteration 36/1000 | Loss: 0.00002169
Iteration 37/1000 | Loss: 0.00002169
Iteration 38/1000 | Loss: 0.00002169
Iteration 39/1000 | Loss: 0.00002169
Iteration 40/1000 | Loss: 0.00002169
Iteration 41/1000 | Loss: 0.00002169
Iteration 42/1000 | Loss: 0.00002169
Iteration 43/1000 | Loss: 0.00002169
Iteration 44/1000 | Loss: 0.00002169
Iteration 45/1000 | Loss: 0.00002169
Iteration 46/1000 | Loss: 0.00002169
Iteration 47/1000 | Loss: 0.00002169
Iteration 48/1000 | Loss: 0.00002168
Iteration 49/1000 | Loss: 0.00002168
Iteration 50/1000 | Loss: 0.00002168
Iteration 51/1000 | Loss: 0.00002168
Iteration 52/1000 | Loss: 0.00002168
Iteration 53/1000 | Loss: 0.00002168
Iteration 54/1000 | Loss: 0.00002168
Iteration 55/1000 | Loss: 0.00002168
Iteration 56/1000 | Loss: 0.00002168
Iteration 57/1000 | Loss: 0.00002167
Iteration 58/1000 | Loss: 0.00002167
Iteration 59/1000 | Loss: 0.00002167
Iteration 60/1000 | Loss: 0.00002166
Iteration 61/1000 | Loss: 0.00002166
Iteration 62/1000 | Loss: 0.00002166
Iteration 63/1000 | Loss: 0.00002165
Iteration 64/1000 | Loss: 0.00002164
Iteration 65/1000 | Loss: 0.00002164
Iteration 66/1000 | Loss: 0.00002164
Iteration 67/1000 | Loss: 0.00002164
Iteration 68/1000 | Loss: 0.00002164
Iteration 69/1000 | Loss: 0.00002164
Iteration 70/1000 | Loss: 0.00002164
Iteration 71/1000 | Loss: 0.00002164
Iteration 72/1000 | Loss: 0.00002164
Iteration 73/1000 | Loss: 0.00002164
Iteration 74/1000 | Loss: 0.00002164
Iteration 75/1000 | Loss: 0.00002164
Iteration 76/1000 | Loss: 0.00002163
Iteration 77/1000 | Loss: 0.00002163
Iteration 78/1000 | Loss: 0.00002163
Iteration 79/1000 | Loss: 0.00002162
Iteration 80/1000 | Loss: 0.00002162
Iteration 81/1000 | Loss: 0.00002162
Iteration 82/1000 | Loss: 0.00002162
Iteration 83/1000 | Loss: 0.00002161
Iteration 84/1000 | Loss: 0.00002161
Iteration 85/1000 | Loss: 0.00002161
Iteration 86/1000 | Loss: 0.00002161
Iteration 87/1000 | Loss: 0.00002161
Iteration 88/1000 | Loss: 0.00002161
Iteration 89/1000 | Loss: 0.00002161
Iteration 90/1000 | Loss: 0.00002161
Iteration 91/1000 | Loss: 0.00002161
Iteration 92/1000 | Loss: 0.00002161
Iteration 93/1000 | Loss: 0.00002160
Iteration 94/1000 | Loss: 0.00002160
Iteration 95/1000 | Loss: 0.00002160
Iteration 96/1000 | Loss: 0.00002159
Iteration 97/1000 | Loss: 0.00002159
Iteration 98/1000 | Loss: 0.00002159
Iteration 99/1000 | Loss: 0.00002159
Iteration 100/1000 | Loss: 0.00002159
Iteration 101/1000 | Loss: 0.00002159
Iteration 102/1000 | Loss: 0.00002159
Iteration 103/1000 | Loss: 0.00002159
Iteration 104/1000 | Loss: 0.00002158
Iteration 105/1000 | Loss: 0.00002158
Iteration 106/1000 | Loss: 0.00002158
Iteration 107/1000 | Loss: 0.00002158
Iteration 108/1000 | Loss: 0.00002158
Iteration 109/1000 | Loss: 0.00002158
Iteration 110/1000 | Loss: 0.00002158
Iteration 111/1000 | Loss: 0.00002158
Iteration 112/1000 | Loss: 0.00002158
Iteration 113/1000 | Loss: 0.00002158
Iteration 114/1000 | Loss: 0.00002158
Iteration 115/1000 | Loss: 0.00002157
Iteration 116/1000 | Loss: 0.00002157
Iteration 117/1000 | Loss: 0.00002157
Iteration 118/1000 | Loss: 0.00002157
Iteration 119/1000 | Loss: 0.00002157
Iteration 120/1000 | Loss: 0.00002157
Iteration 121/1000 | Loss: 0.00002157
Iteration 122/1000 | Loss: 0.00002156
Iteration 123/1000 | Loss: 0.00002156
Iteration 124/1000 | Loss: 0.00002156
Iteration 125/1000 | Loss: 0.00002156
Iteration 126/1000 | Loss: 0.00002155
Iteration 127/1000 | Loss: 0.00002155
Iteration 128/1000 | Loss: 0.00002155
Iteration 129/1000 | Loss: 0.00002154
Iteration 130/1000 | Loss: 0.00002154
Iteration 131/1000 | Loss: 0.00002154
Iteration 132/1000 | Loss: 0.00002153
Iteration 133/1000 | Loss: 0.00002153
Iteration 134/1000 | Loss: 0.00002153
Iteration 135/1000 | Loss: 0.00002152
Iteration 136/1000 | Loss: 0.00002152
Iteration 137/1000 | Loss: 0.00002152
Iteration 138/1000 | Loss: 0.00002152
Iteration 139/1000 | Loss: 0.00002152
Iteration 140/1000 | Loss: 0.00002152
Iteration 141/1000 | Loss: 0.00002151
Iteration 142/1000 | Loss: 0.00002151
Iteration 143/1000 | Loss: 0.00002151
Iteration 144/1000 | Loss: 0.00002151
Iteration 145/1000 | Loss: 0.00002151
Iteration 146/1000 | Loss: 0.00002151
Iteration 147/1000 | Loss: 0.00002151
Iteration 148/1000 | Loss: 0.00002150
Iteration 149/1000 | Loss: 0.00002150
Iteration 150/1000 | Loss: 0.00002150
Iteration 151/1000 | Loss: 0.00002150
Iteration 152/1000 | Loss: 0.00002150
Iteration 153/1000 | Loss: 0.00002149
Iteration 154/1000 | Loss: 0.00002149
Iteration 155/1000 | Loss: 0.00002149
Iteration 156/1000 | Loss: 0.00002149
Iteration 157/1000 | Loss: 0.00002149
Iteration 158/1000 | Loss: 0.00002149
Iteration 159/1000 | Loss: 0.00002148
Iteration 160/1000 | Loss: 0.00002148
Iteration 161/1000 | Loss: 0.00002148
Iteration 162/1000 | Loss: 0.00002148
Iteration 163/1000 | Loss: 0.00002148
Iteration 164/1000 | Loss: 0.00002148
Iteration 165/1000 | Loss: 0.00002148
Iteration 166/1000 | Loss: 0.00002148
Iteration 167/1000 | Loss: 0.00002148
Iteration 168/1000 | Loss: 0.00002148
Iteration 169/1000 | Loss: 0.00002147
Iteration 170/1000 | Loss: 0.00002147
Iteration 171/1000 | Loss: 0.00002147
Iteration 172/1000 | Loss: 0.00002147
Iteration 173/1000 | Loss: 0.00002147
Iteration 174/1000 | Loss: 0.00002146
Iteration 175/1000 | Loss: 0.00002146
Iteration 176/1000 | Loss: 0.00002146
Iteration 177/1000 | Loss: 0.00002146
Iteration 178/1000 | Loss: 0.00002146
Iteration 179/1000 | Loss: 0.00002145
Iteration 180/1000 | Loss: 0.00002145
Iteration 181/1000 | Loss: 0.00002145
Iteration 182/1000 | Loss: 0.00002145
Iteration 183/1000 | Loss: 0.00002145
Iteration 184/1000 | Loss: 0.00002145
Iteration 185/1000 | Loss: 0.00002145
Iteration 186/1000 | Loss: 0.00002145
Iteration 187/1000 | Loss: 0.00002145
Iteration 188/1000 | Loss: 0.00002145
Iteration 189/1000 | Loss: 0.00002145
Iteration 190/1000 | Loss: 0.00002145
Iteration 191/1000 | Loss: 0.00002144
Iteration 192/1000 | Loss: 0.00002144
Iteration 193/1000 | Loss: 0.00002144
Iteration 194/1000 | Loss: 0.00002144
Iteration 195/1000 | Loss: 0.00002144
Iteration 196/1000 | Loss: 0.00002144
Iteration 197/1000 | Loss: 0.00002144
Iteration 198/1000 | Loss: 0.00002144
Iteration 199/1000 | Loss: 0.00002144
Iteration 200/1000 | Loss: 0.00002144
Iteration 201/1000 | Loss: 0.00002144
Iteration 202/1000 | Loss: 0.00002144
Iteration 203/1000 | Loss: 0.00002143
Iteration 204/1000 | Loss: 0.00002143
Iteration 205/1000 | Loss: 0.00002143
Iteration 206/1000 | Loss: 0.00002143
Iteration 207/1000 | Loss: 0.00002143
Iteration 208/1000 | Loss: 0.00002143
Iteration 209/1000 | Loss: 0.00002143
Iteration 210/1000 | Loss: 0.00002143
Iteration 211/1000 | Loss: 0.00002143
Iteration 212/1000 | Loss: 0.00002143
Iteration 213/1000 | Loss: 0.00002143
Iteration 214/1000 | Loss: 0.00002143
Iteration 215/1000 | Loss: 0.00002143
Iteration 216/1000 | Loss: 0.00002143
Iteration 217/1000 | Loss: 0.00002143
Iteration 218/1000 | Loss: 0.00002143
Iteration 219/1000 | Loss: 0.00002143
Iteration 220/1000 | Loss: 0.00002143
Iteration 221/1000 | Loss: 0.00002143
Iteration 222/1000 | Loss: 0.00002143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [2.143099845852703e-05, 2.143099845852703e-05, 2.143099845852703e-05, 2.143099845852703e-05, 2.143099845852703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.143099845852703e-05

Optimization complete. Final v2v error: 3.771261692047119 mm

Highest mean error: 5.000007629394531 mm for frame 51

Lowest mean error: 3.2017416954040527 mm for frame 126

Saving results

Total time: 57.07187056541443
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033042
Iteration 2/25 | Loss: 0.00179018
Iteration 3/25 | Loss: 0.00163885
Iteration 4/25 | Loss: 0.00154508
Iteration 5/25 | Loss: 0.00135599
Iteration 6/25 | Loss: 0.00126367
Iteration 7/25 | Loss: 0.00133841
Iteration 8/25 | Loss: 0.00121901
Iteration 9/25 | Loss: 0.00123872
Iteration 10/25 | Loss: 0.00121396
Iteration 11/25 | Loss: 0.00121110
Iteration 12/25 | Loss: 0.00121479
Iteration 13/25 | Loss: 0.00121031
Iteration 14/25 | Loss: 0.00120819
Iteration 15/25 | Loss: 0.00121432
Iteration 16/25 | Loss: 0.00121072
Iteration 17/25 | Loss: 0.00120684
Iteration 18/25 | Loss: 0.00121226
Iteration 19/25 | Loss: 0.00120566
Iteration 20/25 | Loss: 0.00121388
Iteration 21/25 | Loss: 0.00119995
Iteration 22/25 | Loss: 0.00119251
Iteration 23/25 | Loss: 0.00119059
Iteration 24/25 | Loss: 0.00119051
Iteration 25/25 | Loss: 0.00119050

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30753577
Iteration 2/25 | Loss: 0.00147921
Iteration 3/25 | Loss: 0.00147920
Iteration 4/25 | Loss: 0.00147920
Iteration 5/25 | Loss: 0.00147920
Iteration 6/25 | Loss: 0.00147920
Iteration 7/25 | Loss: 0.00147920
Iteration 8/25 | Loss: 0.00147920
Iteration 9/25 | Loss: 0.00147920
Iteration 10/25 | Loss: 0.00147920
Iteration 11/25 | Loss: 0.00147920
Iteration 12/25 | Loss: 0.00147920
Iteration 13/25 | Loss: 0.00147920
Iteration 14/25 | Loss: 0.00147920
Iteration 15/25 | Loss: 0.00147920
Iteration 16/25 | Loss: 0.00147920
Iteration 17/25 | Loss: 0.00147920
Iteration 18/25 | Loss: 0.00147920
Iteration 19/25 | Loss: 0.00147920
Iteration 20/25 | Loss: 0.00147920
Iteration 21/25 | Loss: 0.00147920
Iteration 22/25 | Loss: 0.00147920
Iteration 23/25 | Loss: 0.00147920
Iteration 24/25 | Loss: 0.00147920
Iteration 25/25 | Loss: 0.00147920

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147920
Iteration 2/1000 | Loss: 0.00002553
Iteration 3/1000 | Loss: 0.00003549
Iteration 4/1000 | Loss: 0.00135036
Iteration 5/1000 | Loss: 0.00004937
Iteration 6/1000 | Loss: 0.00003167
Iteration 7/1000 | Loss: 0.00002869
Iteration 8/1000 | Loss: 0.00013555
Iteration 9/1000 | Loss: 0.00001559
Iteration 10/1000 | Loss: 0.00001489
Iteration 11/1000 | Loss: 0.00001422
Iteration 12/1000 | Loss: 0.00002575
Iteration 13/1000 | Loss: 0.00001364
Iteration 14/1000 | Loss: 0.00002266
Iteration 15/1000 | Loss: 0.00001317
Iteration 16/1000 | Loss: 0.00001308
Iteration 17/1000 | Loss: 0.00149262
Iteration 18/1000 | Loss: 0.00020455
Iteration 19/1000 | Loss: 0.00025835
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00002026
Iteration 22/1000 | Loss: 0.00001293
Iteration 23/1000 | Loss: 0.00001506
Iteration 24/1000 | Loss: 0.00001246
Iteration 25/1000 | Loss: 0.00001244
Iteration 26/1000 | Loss: 0.00001243
Iteration 27/1000 | Loss: 0.00001242
Iteration 28/1000 | Loss: 0.00001242
Iteration 29/1000 | Loss: 0.00001242
Iteration 30/1000 | Loss: 0.00001241
Iteration 31/1000 | Loss: 0.00001241
Iteration 32/1000 | Loss: 0.00001241
Iteration 33/1000 | Loss: 0.00001240
Iteration 34/1000 | Loss: 0.00001240
Iteration 35/1000 | Loss: 0.00001240
Iteration 36/1000 | Loss: 0.00001239
Iteration 37/1000 | Loss: 0.00001236
Iteration 38/1000 | Loss: 0.00001235
Iteration 39/1000 | Loss: 0.00001235
Iteration 40/1000 | Loss: 0.00001234
Iteration 41/1000 | Loss: 0.00001234
Iteration 42/1000 | Loss: 0.00001233
Iteration 43/1000 | Loss: 0.00001233
Iteration 44/1000 | Loss: 0.00001232
Iteration 45/1000 | Loss: 0.00002475
Iteration 46/1000 | Loss: 0.00001230
Iteration 47/1000 | Loss: 0.00001226
Iteration 48/1000 | Loss: 0.00001226
Iteration 49/1000 | Loss: 0.00001224
Iteration 50/1000 | Loss: 0.00001224
Iteration 51/1000 | Loss: 0.00001224
Iteration 52/1000 | Loss: 0.00001223
Iteration 53/1000 | Loss: 0.00001223
Iteration 54/1000 | Loss: 0.00001223
Iteration 55/1000 | Loss: 0.00001223
Iteration 56/1000 | Loss: 0.00001222
Iteration 57/1000 | Loss: 0.00001222
Iteration 58/1000 | Loss: 0.00001222
Iteration 59/1000 | Loss: 0.00001222
Iteration 60/1000 | Loss: 0.00001221
Iteration 61/1000 | Loss: 0.00001221
Iteration 62/1000 | Loss: 0.00001220
Iteration 63/1000 | Loss: 0.00002515
Iteration 64/1000 | Loss: 0.00001217
Iteration 65/1000 | Loss: 0.00001216
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001216
Iteration 68/1000 | Loss: 0.00001216
Iteration 69/1000 | Loss: 0.00001216
Iteration 70/1000 | Loss: 0.00001216
Iteration 71/1000 | Loss: 0.00001216
Iteration 72/1000 | Loss: 0.00001216
Iteration 73/1000 | Loss: 0.00001216
Iteration 74/1000 | Loss: 0.00001216
Iteration 75/1000 | Loss: 0.00001216
Iteration 76/1000 | Loss: 0.00001216
Iteration 77/1000 | Loss: 0.00001215
Iteration 78/1000 | Loss: 0.00001215
Iteration 79/1000 | Loss: 0.00001214
Iteration 80/1000 | Loss: 0.00001214
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001213
Iteration 83/1000 | Loss: 0.00001213
Iteration 84/1000 | Loss: 0.00001212
Iteration 85/1000 | Loss: 0.00001212
Iteration 86/1000 | Loss: 0.00001212
Iteration 87/1000 | Loss: 0.00001211
Iteration 88/1000 | Loss: 0.00002480
Iteration 89/1000 | Loss: 0.00001212
Iteration 90/1000 | Loss: 0.00001211
Iteration 91/1000 | Loss: 0.00001211
Iteration 92/1000 | Loss: 0.00001209
Iteration 93/1000 | Loss: 0.00001207
Iteration 94/1000 | Loss: 0.00001206
Iteration 95/1000 | Loss: 0.00001206
Iteration 96/1000 | Loss: 0.00001205
Iteration 97/1000 | Loss: 0.00001205
Iteration 98/1000 | Loss: 0.00001205
Iteration 99/1000 | Loss: 0.00001204
Iteration 100/1000 | Loss: 0.00001204
Iteration 101/1000 | Loss: 0.00001204
Iteration 102/1000 | Loss: 0.00001203
Iteration 103/1000 | Loss: 0.00001203
Iteration 104/1000 | Loss: 0.00001203
Iteration 105/1000 | Loss: 0.00001203
Iteration 106/1000 | Loss: 0.00001203
Iteration 107/1000 | Loss: 0.00001203
Iteration 108/1000 | Loss: 0.00001203
Iteration 109/1000 | Loss: 0.00001202
Iteration 110/1000 | Loss: 0.00001202
Iteration 111/1000 | Loss: 0.00001202
Iteration 112/1000 | Loss: 0.00001202
Iteration 113/1000 | Loss: 0.00001202
Iteration 114/1000 | Loss: 0.00001202
Iteration 115/1000 | Loss: 0.00001202
Iteration 116/1000 | Loss: 0.00001202
Iteration 117/1000 | Loss: 0.00001201
Iteration 118/1000 | Loss: 0.00001201
Iteration 119/1000 | Loss: 0.00001201
Iteration 120/1000 | Loss: 0.00001201
Iteration 121/1000 | Loss: 0.00001201
Iteration 122/1000 | Loss: 0.00001201
Iteration 123/1000 | Loss: 0.00001201
Iteration 124/1000 | Loss: 0.00001201
Iteration 125/1000 | Loss: 0.00001201
Iteration 126/1000 | Loss: 0.00001200
Iteration 127/1000 | Loss: 0.00001200
Iteration 128/1000 | Loss: 0.00001200
Iteration 129/1000 | Loss: 0.00001200
Iteration 130/1000 | Loss: 0.00001200
Iteration 131/1000 | Loss: 0.00001200
Iteration 132/1000 | Loss: 0.00001200
Iteration 133/1000 | Loss: 0.00001200
Iteration 134/1000 | Loss: 0.00001200
Iteration 135/1000 | Loss: 0.00001199
Iteration 136/1000 | Loss: 0.00001199
Iteration 137/1000 | Loss: 0.00001199
Iteration 138/1000 | Loss: 0.00001199
Iteration 139/1000 | Loss: 0.00001199
Iteration 140/1000 | Loss: 0.00001199
Iteration 141/1000 | Loss: 0.00001199
Iteration 142/1000 | Loss: 0.00001199
Iteration 143/1000 | Loss: 0.00001199
Iteration 144/1000 | Loss: 0.00001199
Iteration 145/1000 | Loss: 0.00001199
Iteration 146/1000 | Loss: 0.00001199
Iteration 147/1000 | Loss: 0.00001199
Iteration 148/1000 | Loss: 0.00001199
Iteration 149/1000 | Loss: 0.00001199
Iteration 150/1000 | Loss: 0.00001198
Iteration 151/1000 | Loss: 0.00001198
Iteration 152/1000 | Loss: 0.00001198
Iteration 153/1000 | Loss: 0.00001198
Iteration 154/1000 | Loss: 0.00001198
Iteration 155/1000 | Loss: 0.00001198
Iteration 156/1000 | Loss: 0.00001198
Iteration 157/1000 | Loss: 0.00001198
Iteration 158/1000 | Loss: 0.00001198
Iteration 159/1000 | Loss: 0.00001198
Iteration 160/1000 | Loss: 0.00001198
Iteration 161/1000 | Loss: 0.00001198
Iteration 162/1000 | Loss: 0.00001198
Iteration 163/1000 | Loss: 0.00001198
Iteration 164/1000 | Loss: 0.00001198
Iteration 165/1000 | Loss: 0.00001198
Iteration 166/1000 | Loss: 0.00001198
Iteration 167/1000 | Loss: 0.00001198
Iteration 168/1000 | Loss: 0.00001198
Iteration 169/1000 | Loss: 0.00001198
Iteration 170/1000 | Loss: 0.00001198
Iteration 171/1000 | Loss: 0.00001198
Iteration 172/1000 | Loss: 0.00001198
Iteration 173/1000 | Loss: 0.00001198
Iteration 174/1000 | Loss: 0.00001198
Iteration 175/1000 | Loss: 0.00001198
Iteration 176/1000 | Loss: 0.00001198
Iteration 177/1000 | Loss: 0.00001198
Iteration 178/1000 | Loss: 0.00001198
Iteration 179/1000 | Loss: 0.00001198
Iteration 180/1000 | Loss: 0.00001198
Iteration 181/1000 | Loss: 0.00001198
Iteration 182/1000 | Loss: 0.00001198
Iteration 183/1000 | Loss: 0.00001198
Iteration 184/1000 | Loss: 0.00001198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.1975468623859342e-05, 1.1975468623859342e-05, 1.1975468623859342e-05, 1.1975468623859342e-05, 1.1975468623859342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1975468623859342e-05

Optimization complete. Final v2v error: 2.9814929962158203 mm

Highest mean error: 3.919973611831665 mm for frame 83

Lowest mean error: 2.7471203804016113 mm for frame 133

Saving results

Total time: 90.36340236663818
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422165
Iteration 2/25 | Loss: 0.00137249
Iteration 3/25 | Loss: 0.00120773
Iteration 4/25 | Loss: 0.00118699
Iteration 5/25 | Loss: 0.00118473
Iteration 6/25 | Loss: 0.00118467
Iteration 7/25 | Loss: 0.00118464
Iteration 8/25 | Loss: 0.00118464
Iteration 9/25 | Loss: 0.00118464
Iteration 10/25 | Loss: 0.00118464
Iteration 11/25 | Loss: 0.00118464
Iteration 12/25 | Loss: 0.00118464
Iteration 13/25 | Loss: 0.00118464
Iteration 14/25 | Loss: 0.00118464
Iteration 15/25 | Loss: 0.00118464
Iteration 16/25 | Loss: 0.00118464
Iteration 17/25 | Loss: 0.00118464
Iteration 18/25 | Loss: 0.00118464
Iteration 19/25 | Loss: 0.00118464
Iteration 20/25 | Loss: 0.00118464
Iteration 21/25 | Loss: 0.00118464
Iteration 22/25 | Loss: 0.00118464
Iteration 23/25 | Loss: 0.00118464
Iteration 24/25 | Loss: 0.00118464
Iteration 25/25 | Loss: 0.00118464

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29918993
Iteration 2/25 | Loss: 0.00137719
Iteration 3/25 | Loss: 0.00137719
Iteration 4/25 | Loss: 0.00137719
Iteration 5/25 | Loss: 0.00137719
Iteration 6/25 | Loss: 0.00137719
Iteration 7/25 | Loss: 0.00137719
Iteration 8/25 | Loss: 0.00137719
Iteration 9/25 | Loss: 0.00137719
Iteration 10/25 | Loss: 0.00137719
Iteration 11/25 | Loss: 0.00137718
Iteration 12/25 | Loss: 0.00137719
Iteration 13/25 | Loss: 0.00137718
Iteration 14/25 | Loss: 0.00137718
Iteration 15/25 | Loss: 0.00137718
Iteration 16/25 | Loss: 0.00137718
Iteration 17/25 | Loss: 0.00137719
Iteration 18/25 | Loss: 0.00137718
Iteration 19/25 | Loss: 0.00137719
Iteration 20/25 | Loss: 0.00137718
Iteration 21/25 | Loss: 0.00137719
Iteration 22/25 | Loss: 0.00137719
Iteration 23/25 | Loss: 0.00137719
Iteration 24/25 | Loss: 0.00137718
Iteration 25/25 | Loss: 0.00137718

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137718
Iteration 2/1000 | Loss: 0.00002718
Iteration 3/1000 | Loss: 0.00001654
Iteration 4/1000 | Loss: 0.00001442
Iteration 5/1000 | Loss: 0.00001324
Iteration 6/1000 | Loss: 0.00001251
Iteration 7/1000 | Loss: 0.00001193
Iteration 8/1000 | Loss: 0.00001147
Iteration 9/1000 | Loss: 0.00001118
Iteration 10/1000 | Loss: 0.00001085
Iteration 11/1000 | Loss: 0.00001068
Iteration 12/1000 | Loss: 0.00001059
Iteration 13/1000 | Loss: 0.00001059
Iteration 14/1000 | Loss: 0.00001054
Iteration 15/1000 | Loss: 0.00001051
Iteration 16/1000 | Loss: 0.00001050
Iteration 17/1000 | Loss: 0.00001050
Iteration 18/1000 | Loss: 0.00001049
Iteration 19/1000 | Loss: 0.00001048
Iteration 20/1000 | Loss: 0.00001047
Iteration 21/1000 | Loss: 0.00001045
Iteration 22/1000 | Loss: 0.00001045
Iteration 23/1000 | Loss: 0.00001043
Iteration 24/1000 | Loss: 0.00001042
Iteration 25/1000 | Loss: 0.00001041
Iteration 26/1000 | Loss: 0.00001040
Iteration 27/1000 | Loss: 0.00001038
Iteration 28/1000 | Loss: 0.00001037
Iteration 29/1000 | Loss: 0.00001037
Iteration 30/1000 | Loss: 0.00001033
Iteration 31/1000 | Loss: 0.00001033
Iteration 32/1000 | Loss: 0.00001027
Iteration 33/1000 | Loss: 0.00001024
Iteration 34/1000 | Loss: 0.00001024
Iteration 35/1000 | Loss: 0.00001023
Iteration 36/1000 | Loss: 0.00001023
Iteration 37/1000 | Loss: 0.00001022
Iteration 38/1000 | Loss: 0.00001014
Iteration 39/1000 | Loss: 0.00001013
Iteration 40/1000 | Loss: 0.00001012
Iteration 41/1000 | Loss: 0.00001010
Iteration 42/1000 | Loss: 0.00001007
Iteration 43/1000 | Loss: 0.00001007
Iteration 44/1000 | Loss: 0.00001006
Iteration 45/1000 | Loss: 0.00001006
Iteration 46/1000 | Loss: 0.00001006
Iteration 47/1000 | Loss: 0.00001003
Iteration 48/1000 | Loss: 0.00001000
Iteration 49/1000 | Loss: 0.00001000
Iteration 50/1000 | Loss: 0.00000999
Iteration 51/1000 | Loss: 0.00000998
Iteration 52/1000 | Loss: 0.00000997
Iteration 53/1000 | Loss: 0.00000996
Iteration 54/1000 | Loss: 0.00000996
Iteration 55/1000 | Loss: 0.00000995
Iteration 56/1000 | Loss: 0.00000995
Iteration 57/1000 | Loss: 0.00000994
Iteration 58/1000 | Loss: 0.00000994
Iteration 59/1000 | Loss: 0.00000993
Iteration 60/1000 | Loss: 0.00000992
Iteration 61/1000 | Loss: 0.00000991
Iteration 62/1000 | Loss: 0.00000990
Iteration 63/1000 | Loss: 0.00000990
Iteration 64/1000 | Loss: 0.00000990
Iteration 65/1000 | Loss: 0.00000990
Iteration 66/1000 | Loss: 0.00000990
Iteration 67/1000 | Loss: 0.00000990
Iteration 68/1000 | Loss: 0.00000990
Iteration 69/1000 | Loss: 0.00000989
Iteration 70/1000 | Loss: 0.00000989
Iteration 71/1000 | Loss: 0.00000988
Iteration 72/1000 | Loss: 0.00000988
Iteration 73/1000 | Loss: 0.00000987
Iteration 74/1000 | Loss: 0.00000986
Iteration 75/1000 | Loss: 0.00000986
Iteration 76/1000 | Loss: 0.00000986
Iteration 77/1000 | Loss: 0.00000986
Iteration 78/1000 | Loss: 0.00000986
Iteration 79/1000 | Loss: 0.00000985
Iteration 80/1000 | Loss: 0.00000985
Iteration 81/1000 | Loss: 0.00000984
Iteration 82/1000 | Loss: 0.00000983
Iteration 83/1000 | Loss: 0.00000982
Iteration 84/1000 | Loss: 0.00000982
Iteration 85/1000 | Loss: 0.00000982
Iteration 86/1000 | Loss: 0.00000982
Iteration 87/1000 | Loss: 0.00000982
Iteration 88/1000 | Loss: 0.00000981
Iteration 89/1000 | Loss: 0.00000981
Iteration 90/1000 | Loss: 0.00000981
Iteration 91/1000 | Loss: 0.00000981
Iteration 92/1000 | Loss: 0.00000981
Iteration 93/1000 | Loss: 0.00000980
Iteration 94/1000 | Loss: 0.00000980
Iteration 95/1000 | Loss: 0.00000979
Iteration 96/1000 | Loss: 0.00000979
Iteration 97/1000 | Loss: 0.00000979
Iteration 98/1000 | Loss: 0.00000979
Iteration 99/1000 | Loss: 0.00000979
Iteration 100/1000 | Loss: 0.00000979
Iteration 101/1000 | Loss: 0.00000978
Iteration 102/1000 | Loss: 0.00000978
Iteration 103/1000 | Loss: 0.00000978
Iteration 104/1000 | Loss: 0.00000978
Iteration 105/1000 | Loss: 0.00000978
Iteration 106/1000 | Loss: 0.00000978
Iteration 107/1000 | Loss: 0.00000978
Iteration 108/1000 | Loss: 0.00000978
Iteration 109/1000 | Loss: 0.00000978
Iteration 110/1000 | Loss: 0.00000977
Iteration 111/1000 | Loss: 0.00000977
Iteration 112/1000 | Loss: 0.00000977
Iteration 113/1000 | Loss: 0.00000977
Iteration 114/1000 | Loss: 0.00000977
Iteration 115/1000 | Loss: 0.00000976
Iteration 116/1000 | Loss: 0.00000976
Iteration 117/1000 | Loss: 0.00000976
Iteration 118/1000 | Loss: 0.00000976
Iteration 119/1000 | Loss: 0.00000976
Iteration 120/1000 | Loss: 0.00000975
Iteration 121/1000 | Loss: 0.00000975
Iteration 122/1000 | Loss: 0.00000975
Iteration 123/1000 | Loss: 0.00000975
Iteration 124/1000 | Loss: 0.00000974
Iteration 125/1000 | Loss: 0.00000974
Iteration 126/1000 | Loss: 0.00000974
Iteration 127/1000 | Loss: 0.00000974
Iteration 128/1000 | Loss: 0.00000974
Iteration 129/1000 | Loss: 0.00000974
Iteration 130/1000 | Loss: 0.00000973
Iteration 131/1000 | Loss: 0.00000973
Iteration 132/1000 | Loss: 0.00000972
Iteration 133/1000 | Loss: 0.00000971
Iteration 134/1000 | Loss: 0.00000971
Iteration 135/1000 | Loss: 0.00000971
Iteration 136/1000 | Loss: 0.00000971
Iteration 137/1000 | Loss: 0.00000971
Iteration 138/1000 | Loss: 0.00000970
Iteration 139/1000 | Loss: 0.00000970
Iteration 140/1000 | Loss: 0.00000970
Iteration 141/1000 | Loss: 0.00000969
Iteration 142/1000 | Loss: 0.00000969
Iteration 143/1000 | Loss: 0.00000969
Iteration 144/1000 | Loss: 0.00000969
Iteration 145/1000 | Loss: 0.00000969
Iteration 146/1000 | Loss: 0.00000968
Iteration 147/1000 | Loss: 0.00000968
Iteration 148/1000 | Loss: 0.00000968
Iteration 149/1000 | Loss: 0.00000968
Iteration 150/1000 | Loss: 0.00000968
Iteration 151/1000 | Loss: 0.00000967
Iteration 152/1000 | Loss: 0.00000967
Iteration 153/1000 | Loss: 0.00000967
Iteration 154/1000 | Loss: 0.00000967
Iteration 155/1000 | Loss: 0.00000967
Iteration 156/1000 | Loss: 0.00000967
Iteration 157/1000 | Loss: 0.00000967
Iteration 158/1000 | Loss: 0.00000967
Iteration 159/1000 | Loss: 0.00000967
Iteration 160/1000 | Loss: 0.00000967
Iteration 161/1000 | Loss: 0.00000967
Iteration 162/1000 | Loss: 0.00000967
Iteration 163/1000 | Loss: 0.00000966
Iteration 164/1000 | Loss: 0.00000966
Iteration 165/1000 | Loss: 0.00000966
Iteration 166/1000 | Loss: 0.00000966
Iteration 167/1000 | Loss: 0.00000966
Iteration 168/1000 | Loss: 0.00000966
Iteration 169/1000 | Loss: 0.00000966
Iteration 170/1000 | Loss: 0.00000966
Iteration 171/1000 | Loss: 0.00000966
Iteration 172/1000 | Loss: 0.00000966
Iteration 173/1000 | Loss: 0.00000966
Iteration 174/1000 | Loss: 0.00000966
Iteration 175/1000 | Loss: 0.00000966
Iteration 176/1000 | Loss: 0.00000965
Iteration 177/1000 | Loss: 0.00000965
Iteration 178/1000 | Loss: 0.00000965
Iteration 179/1000 | Loss: 0.00000965
Iteration 180/1000 | Loss: 0.00000965
Iteration 181/1000 | Loss: 0.00000965
Iteration 182/1000 | Loss: 0.00000965
Iteration 183/1000 | Loss: 0.00000965
Iteration 184/1000 | Loss: 0.00000964
Iteration 185/1000 | Loss: 0.00000964
Iteration 186/1000 | Loss: 0.00000964
Iteration 187/1000 | Loss: 0.00000964
Iteration 188/1000 | Loss: 0.00000964
Iteration 189/1000 | Loss: 0.00000964
Iteration 190/1000 | Loss: 0.00000964
Iteration 191/1000 | Loss: 0.00000964
Iteration 192/1000 | Loss: 0.00000964
Iteration 193/1000 | Loss: 0.00000964
Iteration 194/1000 | Loss: 0.00000964
Iteration 195/1000 | Loss: 0.00000964
Iteration 196/1000 | Loss: 0.00000964
Iteration 197/1000 | Loss: 0.00000964
Iteration 198/1000 | Loss: 0.00000963
Iteration 199/1000 | Loss: 0.00000963
Iteration 200/1000 | Loss: 0.00000963
Iteration 201/1000 | Loss: 0.00000963
Iteration 202/1000 | Loss: 0.00000963
Iteration 203/1000 | Loss: 0.00000963
Iteration 204/1000 | Loss: 0.00000963
Iteration 205/1000 | Loss: 0.00000963
Iteration 206/1000 | Loss: 0.00000963
Iteration 207/1000 | Loss: 0.00000963
Iteration 208/1000 | Loss: 0.00000963
Iteration 209/1000 | Loss: 0.00000963
Iteration 210/1000 | Loss: 0.00000963
Iteration 211/1000 | Loss: 0.00000963
Iteration 212/1000 | Loss: 0.00000963
Iteration 213/1000 | Loss: 0.00000963
Iteration 214/1000 | Loss: 0.00000963
Iteration 215/1000 | Loss: 0.00000963
Iteration 216/1000 | Loss: 0.00000963
Iteration 217/1000 | Loss: 0.00000963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [9.625440725358203e-06, 9.625440725358203e-06, 9.625440725358203e-06, 9.625440725358203e-06, 9.625440725358203e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.625440725358203e-06

Optimization complete. Final v2v error: 2.670581579208374 mm

Highest mean error: 3.352661371231079 mm for frame 90

Lowest mean error: 2.509887456893921 mm for frame 168

Saving results

Total time: 49.07760429382324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_024/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_024/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527286
Iteration 2/25 | Loss: 0.00138183
Iteration 3/25 | Loss: 0.00124704
Iteration 4/25 | Loss: 0.00123001
Iteration 5/25 | Loss: 0.00122256
Iteration 6/25 | Loss: 0.00122222
Iteration 7/25 | Loss: 0.00122222
Iteration 8/25 | Loss: 0.00122222
Iteration 9/25 | Loss: 0.00122222
Iteration 10/25 | Loss: 0.00122098
Iteration 11/25 | Loss: 0.00122098
Iteration 12/25 | Loss: 0.00122098
Iteration 13/25 | Loss: 0.00122098
Iteration 14/25 | Loss: 0.00122098
Iteration 15/25 | Loss: 0.00122098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012209824053570628, 0.0012209824053570628, 0.0012209824053570628, 0.0012209824053570628, 0.0012209824053570628]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012209824053570628

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.72157454
Iteration 2/25 | Loss: 0.00111038
Iteration 3/25 | Loss: 0.00111038
Iteration 4/25 | Loss: 0.00111038
Iteration 5/25 | Loss: 0.00111037
Iteration 6/25 | Loss: 0.00111037
Iteration 7/25 | Loss: 0.00111037
Iteration 8/25 | Loss: 0.00111037
Iteration 9/25 | Loss: 0.00111037
Iteration 10/25 | Loss: 0.00111037
Iteration 11/25 | Loss: 0.00111037
Iteration 12/25 | Loss: 0.00111037
Iteration 13/25 | Loss: 0.00111037
Iteration 14/25 | Loss: 0.00111037
Iteration 15/25 | Loss: 0.00111037
Iteration 16/25 | Loss: 0.00111037
Iteration 17/25 | Loss: 0.00111037
Iteration 18/25 | Loss: 0.00111037
Iteration 19/25 | Loss: 0.00111037
Iteration 20/25 | Loss: 0.00111037
Iteration 21/25 | Loss: 0.00111037
Iteration 22/25 | Loss: 0.00111037
Iteration 23/25 | Loss: 0.00111037
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001110372948460281, 0.001110372948460281, 0.001110372948460281, 0.001110372948460281, 0.001110372948460281]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001110372948460281

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111037
Iteration 2/1000 | Loss: 0.00003190
Iteration 3/1000 | Loss: 0.00002448
Iteration 4/1000 | Loss: 0.00002313
Iteration 5/1000 | Loss: 0.00002230
Iteration 6/1000 | Loss: 0.00002174
Iteration 7/1000 | Loss: 0.00002138
Iteration 8/1000 | Loss: 0.00002095
Iteration 9/1000 | Loss: 0.00002061
Iteration 10/1000 | Loss: 0.00002031
Iteration 11/1000 | Loss: 0.00001999
Iteration 12/1000 | Loss: 0.00001974
Iteration 13/1000 | Loss: 0.00001949
Iteration 14/1000 | Loss: 0.00001930
Iteration 15/1000 | Loss: 0.00001925
Iteration 16/1000 | Loss: 0.00001907
Iteration 17/1000 | Loss: 0.00001891
Iteration 18/1000 | Loss: 0.00001884
Iteration 19/1000 | Loss: 0.00001877
Iteration 20/1000 | Loss: 0.00001867
Iteration 21/1000 | Loss: 0.00001867
Iteration 22/1000 | Loss: 0.00001866
Iteration 23/1000 | Loss: 0.00001865
Iteration 24/1000 | Loss: 0.00001864
Iteration 25/1000 | Loss: 0.00001864
Iteration 26/1000 | Loss: 0.00001863
Iteration 27/1000 | Loss: 0.00001863
Iteration 28/1000 | Loss: 0.00001863
Iteration 29/1000 | Loss: 0.00001863
Iteration 30/1000 | Loss: 0.00001863
Iteration 31/1000 | Loss: 0.00001863
Iteration 32/1000 | Loss: 0.00001863
Iteration 33/1000 | Loss: 0.00001863
Iteration 34/1000 | Loss: 0.00001863
Iteration 35/1000 | Loss: 0.00001862
Iteration 36/1000 | Loss: 0.00001862
Iteration 37/1000 | Loss: 0.00001862
Iteration 38/1000 | Loss: 0.00001862
Iteration 39/1000 | Loss: 0.00001857
Iteration 40/1000 | Loss: 0.00001856
Iteration 41/1000 | Loss: 0.00001855
Iteration 42/1000 | Loss: 0.00001855
Iteration 43/1000 | Loss: 0.00001854
Iteration 44/1000 | Loss: 0.00001854
Iteration 45/1000 | Loss: 0.00001854
Iteration 46/1000 | Loss: 0.00001854
Iteration 47/1000 | Loss: 0.00001853
Iteration 48/1000 | Loss: 0.00001853
Iteration 49/1000 | Loss: 0.00001853
Iteration 50/1000 | Loss: 0.00001853
Iteration 51/1000 | Loss: 0.00001853
Iteration 52/1000 | Loss: 0.00001852
Iteration 53/1000 | Loss: 0.00001852
Iteration 54/1000 | Loss: 0.00001851
Iteration 55/1000 | Loss: 0.00001851
Iteration 56/1000 | Loss: 0.00001851
Iteration 57/1000 | Loss: 0.00001850
Iteration 58/1000 | Loss: 0.00001850
Iteration 59/1000 | Loss: 0.00001850
Iteration 60/1000 | Loss: 0.00001849
Iteration 61/1000 | Loss: 0.00001849
Iteration 62/1000 | Loss: 0.00001849
Iteration 63/1000 | Loss: 0.00001849
Iteration 64/1000 | Loss: 0.00001849
Iteration 65/1000 | Loss: 0.00001849
Iteration 66/1000 | Loss: 0.00001849
Iteration 67/1000 | Loss: 0.00001848
Iteration 68/1000 | Loss: 0.00001848
Iteration 69/1000 | Loss: 0.00001848
Iteration 70/1000 | Loss: 0.00001848
Iteration 71/1000 | Loss: 0.00001847
Iteration 72/1000 | Loss: 0.00001847
Iteration 73/1000 | Loss: 0.00001847
Iteration 74/1000 | Loss: 0.00001845
Iteration 75/1000 | Loss: 0.00001845
Iteration 76/1000 | Loss: 0.00001845
Iteration 77/1000 | Loss: 0.00001845
Iteration 78/1000 | Loss: 0.00001845
Iteration 79/1000 | Loss: 0.00001845
Iteration 80/1000 | Loss: 0.00001844
Iteration 81/1000 | Loss: 0.00001843
Iteration 82/1000 | Loss: 0.00001843
Iteration 83/1000 | Loss: 0.00001842
Iteration 84/1000 | Loss: 0.00001842
Iteration 85/1000 | Loss: 0.00001842
Iteration 86/1000 | Loss: 0.00001842
Iteration 87/1000 | Loss: 0.00001842
Iteration 88/1000 | Loss: 0.00001841
Iteration 89/1000 | Loss: 0.00001841
Iteration 90/1000 | Loss: 0.00001841
Iteration 91/1000 | Loss: 0.00001841
Iteration 92/1000 | Loss: 0.00001841
Iteration 93/1000 | Loss: 0.00001841
Iteration 94/1000 | Loss: 0.00001841
Iteration 95/1000 | Loss: 0.00001841
Iteration 96/1000 | Loss: 0.00001840
Iteration 97/1000 | Loss: 0.00001840
Iteration 98/1000 | Loss: 0.00001840
Iteration 99/1000 | Loss: 0.00001840
Iteration 100/1000 | Loss: 0.00001840
Iteration 101/1000 | Loss: 0.00001840
Iteration 102/1000 | Loss: 0.00001840
Iteration 103/1000 | Loss: 0.00001840
Iteration 104/1000 | Loss: 0.00001840
Iteration 105/1000 | Loss: 0.00001840
Iteration 106/1000 | Loss: 0.00001840
Iteration 107/1000 | Loss: 0.00001840
Iteration 108/1000 | Loss: 0.00001839
Iteration 109/1000 | Loss: 0.00001839
Iteration 110/1000 | Loss: 0.00001839
Iteration 111/1000 | Loss: 0.00001839
Iteration 112/1000 | Loss: 0.00001839
Iteration 113/1000 | Loss: 0.00001839
Iteration 114/1000 | Loss: 0.00001839
Iteration 115/1000 | Loss: 0.00001839
Iteration 116/1000 | Loss: 0.00001839
Iteration 117/1000 | Loss: 0.00001839
Iteration 118/1000 | Loss: 0.00001839
Iteration 119/1000 | Loss: 0.00001839
Iteration 120/1000 | Loss: 0.00001839
Iteration 121/1000 | Loss: 0.00001838
Iteration 122/1000 | Loss: 0.00001838
Iteration 123/1000 | Loss: 0.00001838
Iteration 124/1000 | Loss: 0.00001838
Iteration 125/1000 | Loss: 0.00001838
Iteration 126/1000 | Loss: 0.00001838
Iteration 127/1000 | Loss: 0.00001838
Iteration 128/1000 | Loss: 0.00001838
Iteration 129/1000 | Loss: 0.00001838
Iteration 130/1000 | Loss: 0.00001837
Iteration 131/1000 | Loss: 0.00001837
Iteration 132/1000 | Loss: 0.00001837
Iteration 133/1000 | Loss: 0.00001837
Iteration 134/1000 | Loss: 0.00001837
Iteration 135/1000 | Loss: 0.00001837
Iteration 136/1000 | Loss: 0.00001837
Iteration 137/1000 | Loss: 0.00001837
Iteration 138/1000 | Loss: 0.00001837
Iteration 139/1000 | Loss: 0.00001837
Iteration 140/1000 | Loss: 0.00001837
Iteration 141/1000 | Loss: 0.00001837
Iteration 142/1000 | Loss: 0.00001837
Iteration 143/1000 | Loss: 0.00001837
Iteration 144/1000 | Loss: 0.00001837
Iteration 145/1000 | Loss: 0.00001837
Iteration 146/1000 | Loss: 0.00001837
Iteration 147/1000 | Loss: 0.00001837
Iteration 148/1000 | Loss: 0.00001836
Iteration 149/1000 | Loss: 0.00001836
Iteration 150/1000 | Loss: 0.00001836
Iteration 151/1000 | Loss: 0.00001836
Iteration 152/1000 | Loss: 0.00001836
Iteration 153/1000 | Loss: 0.00001836
Iteration 154/1000 | Loss: 0.00001836
Iteration 155/1000 | Loss: 0.00001836
Iteration 156/1000 | Loss: 0.00001836
Iteration 157/1000 | Loss: 0.00001836
Iteration 158/1000 | Loss: 0.00001836
Iteration 159/1000 | Loss: 0.00001836
Iteration 160/1000 | Loss: 0.00001836
Iteration 161/1000 | Loss: 0.00001836
Iteration 162/1000 | Loss: 0.00001836
Iteration 163/1000 | Loss: 0.00001836
Iteration 164/1000 | Loss: 0.00001836
Iteration 165/1000 | Loss: 0.00001836
Iteration 166/1000 | Loss: 0.00001836
Iteration 167/1000 | Loss: 0.00001836
Iteration 168/1000 | Loss: 0.00001836
Iteration 169/1000 | Loss: 0.00001836
Iteration 170/1000 | Loss: 0.00001836
Iteration 171/1000 | Loss: 0.00001836
Iteration 172/1000 | Loss: 0.00001836
Iteration 173/1000 | Loss: 0.00001836
Iteration 174/1000 | Loss: 0.00001836
Iteration 175/1000 | Loss: 0.00001836
Iteration 176/1000 | Loss: 0.00001836
Iteration 177/1000 | Loss: 0.00001836
Iteration 178/1000 | Loss: 0.00001836
Iteration 179/1000 | Loss: 0.00001836
Iteration 180/1000 | Loss: 0.00001836
Iteration 181/1000 | Loss: 0.00001836
Iteration 182/1000 | Loss: 0.00001836
Iteration 183/1000 | Loss: 0.00001836
Iteration 184/1000 | Loss: 0.00001836
Iteration 185/1000 | Loss: 0.00001836
Iteration 186/1000 | Loss: 0.00001836
Iteration 187/1000 | Loss: 0.00001836
Iteration 188/1000 | Loss: 0.00001836
Iteration 189/1000 | Loss: 0.00001836
Iteration 190/1000 | Loss: 0.00001836
Iteration 191/1000 | Loss: 0.00001836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.836168667068705e-05, 1.836168667068705e-05, 1.836168667068705e-05, 1.836168667068705e-05, 1.836168667068705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.836168667068705e-05

Optimization complete. Final v2v error: 3.6506216526031494 mm

Highest mean error: 4.185521125793457 mm for frame 266

Lowest mean error: 3.586338996887207 mm for frame 11

Saving results

Total time: 52.292683362960815
