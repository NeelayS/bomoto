Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=197, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 11032-11087
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964906
Iteration 2/25 | Loss: 0.00964906
Iteration 3/25 | Loss: 0.00964905
Iteration 4/25 | Loss: 0.00366790
Iteration 5/25 | Loss: 0.00247113
Iteration 6/25 | Loss: 0.00232878
Iteration 7/25 | Loss: 0.00222404
Iteration 8/25 | Loss: 0.00221870
Iteration 9/25 | Loss: 0.00215645
Iteration 10/25 | Loss: 0.00212012
Iteration 11/25 | Loss: 0.00203731
Iteration 12/25 | Loss: 0.00198288
Iteration 13/25 | Loss: 0.00195399
Iteration 14/25 | Loss: 0.00195127
Iteration 15/25 | Loss: 0.00196380
Iteration 16/25 | Loss: 0.00194525
Iteration 17/25 | Loss: 0.00192390
Iteration 18/25 | Loss: 0.00192800
Iteration 19/25 | Loss: 0.00191813
Iteration 20/25 | Loss: 0.00191581
Iteration 21/25 | Loss: 0.00191886
Iteration 22/25 | Loss: 0.00191344
Iteration 23/25 | Loss: 0.00191281
Iteration 24/25 | Loss: 0.00191266
Iteration 25/25 | Loss: 0.00191265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32729375
Iteration 2/25 | Loss: 0.00389528
Iteration 3/25 | Loss: 0.00389528
Iteration 4/25 | Loss: 0.00389528
Iteration 5/25 | Loss: 0.00389528
Iteration 6/25 | Loss: 0.00389528
Iteration 7/25 | Loss: 0.00389528
Iteration 8/25 | Loss: 0.00389528
Iteration 9/25 | Loss: 0.00389528
Iteration 10/25 | Loss: 0.00389528
Iteration 11/25 | Loss: 0.00389528
Iteration 12/25 | Loss: 0.00389528
Iteration 13/25 | Loss: 0.00389528
Iteration 14/25 | Loss: 0.00389528
Iteration 15/25 | Loss: 0.00389528
Iteration 16/25 | Loss: 0.00389528
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0038952778559178114, 0.0038952778559178114, 0.0038952778559178114, 0.0038952778559178114, 0.0038952778559178114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038952778559178114

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00389528
Iteration 2/1000 | Loss: 0.00118799
Iteration 3/1000 | Loss: 0.00091018
Iteration 4/1000 | Loss: 0.00165009
Iteration 5/1000 | Loss: 0.00112430
Iteration 6/1000 | Loss: 0.00162322
Iteration 7/1000 | Loss: 0.00075839
Iteration 8/1000 | Loss: 0.00091979
Iteration 9/1000 | Loss: 0.00099472
Iteration 10/1000 | Loss: 0.00105269
Iteration 11/1000 | Loss: 0.00073191
Iteration 12/1000 | Loss: 0.00042904
Iteration 13/1000 | Loss: 0.00048037
Iteration 14/1000 | Loss: 0.00036043
Iteration 15/1000 | Loss: 0.00049190
Iteration 16/1000 | Loss: 0.00103830
Iteration 17/1000 | Loss: 0.00176354
Iteration 18/1000 | Loss: 0.00206225
Iteration 19/1000 | Loss: 0.00211475
Iteration 20/1000 | Loss: 0.00500334
Iteration 21/1000 | Loss: 0.00275419
Iteration 22/1000 | Loss: 0.00079754
Iteration 23/1000 | Loss: 0.00038872
Iteration 24/1000 | Loss: 0.00030530
Iteration 25/1000 | Loss: 0.00032745
Iteration 26/1000 | Loss: 0.00015323
Iteration 27/1000 | Loss: 0.00028261
Iteration 28/1000 | Loss: 0.00011844
Iteration 29/1000 | Loss: 0.00010093
Iteration 30/1000 | Loss: 0.00009769
Iteration 31/1000 | Loss: 0.00008511
Iteration 32/1000 | Loss: 0.00009595
Iteration 33/1000 | Loss: 0.00009318
Iteration 34/1000 | Loss: 0.00011400
Iteration 35/1000 | Loss: 0.00013825
Iteration 36/1000 | Loss: 0.00025447
Iteration 37/1000 | Loss: 0.00007943
Iteration 38/1000 | Loss: 0.00004626
Iteration 39/1000 | Loss: 0.00002738
Iteration 40/1000 | Loss: 0.00002467
Iteration 41/1000 | Loss: 0.00005656
Iteration 42/1000 | Loss: 0.00002282
Iteration 43/1000 | Loss: 0.00002105
Iteration 44/1000 | Loss: 0.00002004
Iteration 45/1000 | Loss: 0.00001890
Iteration 46/1000 | Loss: 0.00001813
Iteration 47/1000 | Loss: 0.00001762
Iteration 48/1000 | Loss: 0.00001718
Iteration 49/1000 | Loss: 0.00001702
Iteration 50/1000 | Loss: 0.00001679
Iteration 51/1000 | Loss: 0.00001666
Iteration 52/1000 | Loss: 0.00001662
Iteration 53/1000 | Loss: 0.00001657
Iteration 54/1000 | Loss: 0.00001655
Iteration 55/1000 | Loss: 0.00001654
Iteration 56/1000 | Loss: 0.00001654
Iteration 57/1000 | Loss: 0.00001651
Iteration 58/1000 | Loss: 0.00001651
Iteration 59/1000 | Loss: 0.00001651
Iteration 60/1000 | Loss: 0.00001650
Iteration 61/1000 | Loss: 0.00001650
Iteration 62/1000 | Loss: 0.00001649
Iteration 63/1000 | Loss: 0.00001649
Iteration 64/1000 | Loss: 0.00001649
Iteration 65/1000 | Loss: 0.00001649
Iteration 66/1000 | Loss: 0.00001648
Iteration 67/1000 | Loss: 0.00001646
Iteration 68/1000 | Loss: 0.00001646
Iteration 69/1000 | Loss: 0.00001646
Iteration 70/1000 | Loss: 0.00001645
Iteration 71/1000 | Loss: 0.00001645
Iteration 72/1000 | Loss: 0.00001645
Iteration 73/1000 | Loss: 0.00001645
Iteration 74/1000 | Loss: 0.00001645
Iteration 75/1000 | Loss: 0.00001645
Iteration 76/1000 | Loss: 0.00001645
Iteration 77/1000 | Loss: 0.00001645
Iteration 78/1000 | Loss: 0.00001645
Iteration 79/1000 | Loss: 0.00001645
Iteration 80/1000 | Loss: 0.00001644
Iteration 81/1000 | Loss: 0.00001644
Iteration 82/1000 | Loss: 0.00001642
Iteration 83/1000 | Loss: 0.00001642
Iteration 84/1000 | Loss: 0.00001642
Iteration 85/1000 | Loss: 0.00001642
Iteration 86/1000 | Loss: 0.00001641
Iteration 87/1000 | Loss: 0.00001641
Iteration 88/1000 | Loss: 0.00001640
Iteration 89/1000 | Loss: 0.00001640
Iteration 90/1000 | Loss: 0.00001639
Iteration 91/1000 | Loss: 0.00001639
Iteration 92/1000 | Loss: 0.00001639
Iteration 93/1000 | Loss: 0.00001639
Iteration 94/1000 | Loss: 0.00001639
Iteration 95/1000 | Loss: 0.00001639
Iteration 96/1000 | Loss: 0.00001639
Iteration 97/1000 | Loss: 0.00001638
Iteration 98/1000 | Loss: 0.00001638
Iteration 99/1000 | Loss: 0.00001638
Iteration 100/1000 | Loss: 0.00001638
Iteration 101/1000 | Loss: 0.00001637
Iteration 102/1000 | Loss: 0.00001637
Iteration 103/1000 | Loss: 0.00001637
Iteration 104/1000 | Loss: 0.00001637
Iteration 105/1000 | Loss: 0.00001637
Iteration 106/1000 | Loss: 0.00001636
Iteration 107/1000 | Loss: 0.00001636
Iteration 108/1000 | Loss: 0.00001636
Iteration 109/1000 | Loss: 0.00001635
Iteration 110/1000 | Loss: 0.00001635
Iteration 111/1000 | Loss: 0.00001635
Iteration 112/1000 | Loss: 0.00001635
Iteration 113/1000 | Loss: 0.00001635
Iteration 114/1000 | Loss: 0.00001635
Iteration 115/1000 | Loss: 0.00001634
Iteration 116/1000 | Loss: 0.00001634
Iteration 117/1000 | Loss: 0.00001634
Iteration 118/1000 | Loss: 0.00001634
Iteration 119/1000 | Loss: 0.00001633
Iteration 120/1000 | Loss: 0.00001633
Iteration 121/1000 | Loss: 0.00001633
Iteration 122/1000 | Loss: 0.00001633
Iteration 123/1000 | Loss: 0.00001633
Iteration 124/1000 | Loss: 0.00001633
Iteration 125/1000 | Loss: 0.00001633
Iteration 126/1000 | Loss: 0.00001633
Iteration 127/1000 | Loss: 0.00001632
Iteration 128/1000 | Loss: 0.00001632
Iteration 129/1000 | Loss: 0.00001632
Iteration 130/1000 | Loss: 0.00001631
Iteration 131/1000 | Loss: 0.00001631
Iteration 132/1000 | Loss: 0.00001631
Iteration 133/1000 | Loss: 0.00001630
Iteration 134/1000 | Loss: 0.00001630
Iteration 135/1000 | Loss: 0.00001629
Iteration 136/1000 | Loss: 0.00001629
Iteration 137/1000 | Loss: 0.00001629
Iteration 138/1000 | Loss: 0.00001629
Iteration 139/1000 | Loss: 0.00001629
Iteration 140/1000 | Loss: 0.00001629
Iteration 141/1000 | Loss: 0.00001629
Iteration 142/1000 | Loss: 0.00001629
Iteration 143/1000 | Loss: 0.00001629
Iteration 144/1000 | Loss: 0.00001629
Iteration 145/1000 | Loss: 0.00001629
Iteration 146/1000 | Loss: 0.00001629
Iteration 147/1000 | Loss: 0.00001629
Iteration 148/1000 | Loss: 0.00001629
Iteration 149/1000 | Loss: 0.00001629
Iteration 150/1000 | Loss: 0.00001628
Iteration 151/1000 | Loss: 0.00001628
Iteration 152/1000 | Loss: 0.00001628
Iteration 153/1000 | Loss: 0.00001628
Iteration 154/1000 | Loss: 0.00001628
Iteration 155/1000 | Loss: 0.00001628
Iteration 156/1000 | Loss: 0.00001627
Iteration 157/1000 | Loss: 0.00001627
Iteration 158/1000 | Loss: 0.00001627
Iteration 159/1000 | Loss: 0.00001627
Iteration 160/1000 | Loss: 0.00001627
Iteration 161/1000 | Loss: 0.00001627
Iteration 162/1000 | Loss: 0.00001627
Iteration 163/1000 | Loss: 0.00001627
Iteration 164/1000 | Loss: 0.00001627
Iteration 165/1000 | Loss: 0.00001627
Iteration 166/1000 | Loss: 0.00001627
Iteration 167/1000 | Loss: 0.00001627
Iteration 168/1000 | Loss: 0.00001626
Iteration 169/1000 | Loss: 0.00001626
Iteration 170/1000 | Loss: 0.00001626
Iteration 171/1000 | Loss: 0.00001626
Iteration 172/1000 | Loss: 0.00001626
Iteration 173/1000 | Loss: 0.00001626
Iteration 174/1000 | Loss: 0.00001626
Iteration 175/1000 | Loss: 0.00001626
Iteration 176/1000 | Loss: 0.00001625
Iteration 177/1000 | Loss: 0.00001625
Iteration 178/1000 | Loss: 0.00001625
Iteration 179/1000 | Loss: 0.00001625
Iteration 180/1000 | Loss: 0.00001625
Iteration 181/1000 | Loss: 0.00001625
Iteration 182/1000 | Loss: 0.00001625
Iteration 183/1000 | Loss: 0.00001625
Iteration 184/1000 | Loss: 0.00001625
Iteration 185/1000 | Loss: 0.00001625
Iteration 186/1000 | Loss: 0.00001625
Iteration 187/1000 | Loss: 0.00001625
Iteration 188/1000 | Loss: 0.00001625
Iteration 189/1000 | Loss: 0.00001625
Iteration 190/1000 | Loss: 0.00001625
Iteration 191/1000 | Loss: 0.00001625
Iteration 192/1000 | Loss: 0.00001625
Iteration 193/1000 | Loss: 0.00001625
Iteration 194/1000 | Loss: 0.00001625
Iteration 195/1000 | Loss: 0.00001625
Iteration 196/1000 | Loss: 0.00001625
Iteration 197/1000 | Loss: 0.00001625
Iteration 198/1000 | Loss: 0.00001625
Iteration 199/1000 | Loss: 0.00001625
Iteration 200/1000 | Loss: 0.00001625
Iteration 201/1000 | Loss: 0.00001625
Iteration 202/1000 | Loss: 0.00001625
Iteration 203/1000 | Loss: 0.00001625
Iteration 204/1000 | Loss: 0.00001625
Iteration 205/1000 | Loss: 0.00001625
Iteration 206/1000 | Loss: 0.00001625
Iteration 207/1000 | Loss: 0.00001625
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.625021468498744e-05, 1.625021468498744e-05, 1.625021468498744e-05, 1.625021468498744e-05, 1.625021468498744e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.625021468498744e-05

Optimization complete. Final v2v error: 3.4543588161468506 mm

Highest mean error: 4.056962013244629 mm for frame 99

Lowest mean error: 3.246391534805298 mm for frame 3

Saving results

Total time: 137.53751730918884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855694
Iteration 2/25 | Loss: 0.00212638
Iteration 3/25 | Loss: 0.00187465
Iteration 4/25 | Loss: 0.00167295
Iteration 5/25 | Loss: 0.00146926
Iteration 6/25 | Loss: 0.00142651
Iteration 7/25 | Loss: 0.00140611
Iteration 8/25 | Loss: 0.00140774
Iteration 9/25 | Loss: 0.00140257
Iteration 10/25 | Loss: 0.00139924
Iteration 11/25 | Loss: 0.00139949
Iteration 12/25 | Loss: 0.00139522
Iteration 13/25 | Loss: 0.00139358
Iteration 14/25 | Loss: 0.00139322
Iteration 15/25 | Loss: 0.00139209
Iteration 16/25 | Loss: 0.00139172
Iteration 17/25 | Loss: 0.00139228
Iteration 18/25 | Loss: 0.00139154
Iteration 19/25 | Loss: 0.00139070
Iteration 20/25 | Loss: 0.00139057
Iteration 21/25 | Loss: 0.00139055
Iteration 22/25 | Loss: 0.00139054
Iteration 23/25 | Loss: 0.00139054
Iteration 24/25 | Loss: 0.00139053
Iteration 25/25 | Loss: 0.00139053

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32991016
Iteration 2/25 | Loss: 0.00076974
Iteration 3/25 | Loss: 0.00076971
Iteration 4/25 | Loss: 0.00076971
Iteration 5/25 | Loss: 0.00076971
Iteration 6/25 | Loss: 0.00076971
Iteration 7/25 | Loss: 0.00076971
Iteration 8/25 | Loss: 0.00076971
Iteration 9/25 | Loss: 0.00076971
Iteration 10/25 | Loss: 0.00076971
Iteration 11/25 | Loss: 0.00076971
Iteration 12/25 | Loss: 0.00076971
Iteration 13/25 | Loss: 0.00076971
Iteration 14/25 | Loss: 0.00076971
Iteration 15/25 | Loss: 0.00076971
Iteration 16/25 | Loss: 0.00076971
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00076970987720415, 0.00076970987720415, 0.00076970987720415, 0.00076970987720415, 0.00076970987720415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00076970987720415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076971
Iteration 2/1000 | Loss: 0.00004455
Iteration 3/1000 | Loss: 0.00003513
Iteration 4/1000 | Loss: 0.00002992
Iteration 5/1000 | Loss: 0.00002669
Iteration 6/1000 | Loss: 0.00002456
Iteration 7/1000 | Loss: 0.00002334
Iteration 8/1000 | Loss: 0.00002279
Iteration 9/1000 | Loss: 0.00002238
Iteration 10/1000 | Loss: 0.00002216
Iteration 11/1000 | Loss: 0.00002193
Iteration 12/1000 | Loss: 0.00002177
Iteration 13/1000 | Loss: 0.00002173
Iteration 14/1000 | Loss: 0.00002171
Iteration 15/1000 | Loss: 0.00002170
Iteration 16/1000 | Loss: 0.00002168
Iteration 17/1000 | Loss: 0.00002168
Iteration 18/1000 | Loss: 0.00002160
Iteration 19/1000 | Loss: 0.00002160
Iteration 20/1000 | Loss: 0.00002160
Iteration 21/1000 | Loss: 0.00002159
Iteration 22/1000 | Loss: 0.00002159
Iteration 23/1000 | Loss: 0.00002158
Iteration 24/1000 | Loss: 0.00002157
Iteration 25/1000 | Loss: 0.00002156
Iteration 26/1000 | Loss: 0.00002155
Iteration 27/1000 | Loss: 0.00002155
Iteration 28/1000 | Loss: 0.00002155
Iteration 29/1000 | Loss: 0.00002155
Iteration 30/1000 | Loss: 0.00002154
Iteration 31/1000 | Loss: 0.00002153
Iteration 32/1000 | Loss: 0.00002152
Iteration 33/1000 | Loss: 0.00002152
Iteration 34/1000 | Loss: 0.00002147
Iteration 35/1000 | Loss: 0.00002145
Iteration 36/1000 | Loss: 0.00002144
Iteration 37/1000 | Loss: 0.00002144
Iteration 38/1000 | Loss: 0.00002143
Iteration 39/1000 | Loss: 0.00002142
Iteration 40/1000 | Loss: 0.00002142
Iteration 41/1000 | Loss: 0.00002142
Iteration 42/1000 | Loss: 0.00002141
Iteration 43/1000 | Loss: 0.00002141
Iteration 44/1000 | Loss: 0.00002141
Iteration 45/1000 | Loss: 0.00002141
Iteration 46/1000 | Loss: 0.00002141
Iteration 47/1000 | Loss: 0.00002141
Iteration 48/1000 | Loss: 0.00002141
Iteration 49/1000 | Loss: 0.00002141
Iteration 50/1000 | Loss: 0.00002140
Iteration 51/1000 | Loss: 0.00002140
Iteration 52/1000 | Loss: 0.00002140
Iteration 53/1000 | Loss: 0.00002140
Iteration 54/1000 | Loss: 0.00002139
Iteration 55/1000 | Loss: 0.00002139
Iteration 56/1000 | Loss: 0.00002139
Iteration 57/1000 | Loss: 0.00002139
Iteration 58/1000 | Loss: 0.00002139
Iteration 59/1000 | Loss: 0.00002139
Iteration 60/1000 | Loss: 0.00002139
Iteration 61/1000 | Loss: 0.00002138
Iteration 62/1000 | Loss: 0.00002138
Iteration 63/1000 | Loss: 0.00002138
Iteration 64/1000 | Loss: 0.00002138
Iteration 65/1000 | Loss: 0.00002138
Iteration 66/1000 | Loss: 0.00002138
Iteration 67/1000 | Loss: 0.00002138
Iteration 68/1000 | Loss: 0.00002138
Iteration 69/1000 | Loss: 0.00002138
Iteration 70/1000 | Loss: 0.00002138
Iteration 71/1000 | Loss: 0.00002138
Iteration 72/1000 | Loss: 0.00002138
Iteration 73/1000 | Loss: 0.00002137
Iteration 74/1000 | Loss: 0.00002137
Iteration 75/1000 | Loss: 0.00002137
Iteration 76/1000 | Loss: 0.00002137
Iteration 77/1000 | Loss: 0.00002137
Iteration 78/1000 | Loss: 0.00002137
Iteration 79/1000 | Loss: 0.00002137
Iteration 80/1000 | Loss: 0.00002137
Iteration 81/1000 | Loss: 0.00002137
Iteration 82/1000 | Loss: 0.00002136
Iteration 83/1000 | Loss: 0.00002136
Iteration 84/1000 | Loss: 0.00002136
Iteration 85/1000 | Loss: 0.00002136
Iteration 86/1000 | Loss: 0.00002136
Iteration 87/1000 | Loss: 0.00002136
Iteration 88/1000 | Loss: 0.00002136
Iteration 89/1000 | Loss: 0.00002136
Iteration 90/1000 | Loss: 0.00002136
Iteration 91/1000 | Loss: 0.00002135
Iteration 92/1000 | Loss: 0.00002135
Iteration 93/1000 | Loss: 0.00002135
Iteration 94/1000 | Loss: 0.00002135
Iteration 95/1000 | Loss: 0.00002135
Iteration 96/1000 | Loss: 0.00002135
Iteration 97/1000 | Loss: 0.00002135
Iteration 98/1000 | Loss: 0.00002135
Iteration 99/1000 | Loss: 0.00002135
Iteration 100/1000 | Loss: 0.00002135
Iteration 101/1000 | Loss: 0.00002135
Iteration 102/1000 | Loss: 0.00002135
Iteration 103/1000 | Loss: 0.00002135
Iteration 104/1000 | Loss: 0.00002134
Iteration 105/1000 | Loss: 0.00002134
Iteration 106/1000 | Loss: 0.00002134
Iteration 107/1000 | Loss: 0.00002134
Iteration 108/1000 | Loss: 0.00002134
Iteration 109/1000 | Loss: 0.00002134
Iteration 110/1000 | Loss: 0.00002134
Iteration 111/1000 | Loss: 0.00002133
Iteration 112/1000 | Loss: 0.00002133
Iteration 113/1000 | Loss: 0.00002133
Iteration 114/1000 | Loss: 0.00002133
Iteration 115/1000 | Loss: 0.00002133
Iteration 116/1000 | Loss: 0.00002133
Iteration 117/1000 | Loss: 0.00002133
Iteration 118/1000 | Loss: 0.00002133
Iteration 119/1000 | Loss: 0.00002133
Iteration 120/1000 | Loss: 0.00002132
Iteration 121/1000 | Loss: 0.00002132
Iteration 122/1000 | Loss: 0.00002132
Iteration 123/1000 | Loss: 0.00002132
Iteration 124/1000 | Loss: 0.00002132
Iteration 125/1000 | Loss: 0.00002132
Iteration 126/1000 | Loss: 0.00002132
Iteration 127/1000 | Loss: 0.00002132
Iteration 128/1000 | Loss: 0.00002132
Iteration 129/1000 | Loss: 0.00002132
Iteration 130/1000 | Loss: 0.00002132
Iteration 131/1000 | Loss: 0.00002132
Iteration 132/1000 | Loss: 0.00002132
Iteration 133/1000 | Loss: 0.00002132
Iteration 134/1000 | Loss: 0.00002132
Iteration 135/1000 | Loss: 0.00002132
Iteration 136/1000 | Loss: 0.00002132
Iteration 137/1000 | Loss: 0.00002132
Iteration 138/1000 | Loss: 0.00002132
Iteration 139/1000 | Loss: 0.00002132
Iteration 140/1000 | Loss: 0.00002132
Iteration 141/1000 | Loss: 0.00002132
Iteration 142/1000 | Loss: 0.00002132
Iteration 143/1000 | Loss: 0.00002132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [2.1320436644600704e-05, 2.1320436644600704e-05, 2.1320436644600704e-05, 2.1320436644600704e-05, 2.1320436644600704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1320436644600704e-05

Optimization complete. Final v2v error: 3.8871021270751953 mm

Highest mean error: 4.413629531860352 mm for frame 99

Lowest mean error: 3.7411813735961914 mm for frame 114

Saving results

Total time: 71.19626092910767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043195
Iteration 2/25 | Loss: 0.00292342
Iteration 3/25 | Loss: 0.00226924
Iteration 4/25 | Loss: 0.00198312
Iteration 5/25 | Loss: 0.00168778
Iteration 6/25 | Loss: 0.00157565
Iteration 7/25 | Loss: 0.00132581
Iteration 8/25 | Loss: 0.00130932
Iteration 9/25 | Loss: 0.00121664
Iteration 10/25 | Loss: 0.00120336
Iteration 11/25 | Loss: 0.00119553
Iteration 12/25 | Loss: 0.00119330
Iteration 13/25 | Loss: 0.00119244
Iteration 14/25 | Loss: 0.00119220
Iteration 15/25 | Loss: 0.00119209
Iteration 16/25 | Loss: 0.00119208
Iteration 17/25 | Loss: 0.00119208
Iteration 18/25 | Loss: 0.00119208
Iteration 19/25 | Loss: 0.00119208
Iteration 20/25 | Loss: 0.00119208
Iteration 21/25 | Loss: 0.00119208
Iteration 22/25 | Loss: 0.00119207
Iteration 23/25 | Loss: 0.00119207
Iteration 24/25 | Loss: 0.00119207
Iteration 25/25 | Loss: 0.00119207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30844522
Iteration 2/25 | Loss: 0.00101591
Iteration 3/25 | Loss: 0.00101591
Iteration 4/25 | Loss: 0.00101591
Iteration 5/25 | Loss: 0.00101591
Iteration 6/25 | Loss: 0.00101591
Iteration 7/25 | Loss: 0.00101591
Iteration 8/25 | Loss: 0.00101591
Iteration 9/25 | Loss: 0.00101591
Iteration 10/25 | Loss: 0.00101591
Iteration 11/25 | Loss: 0.00101591
Iteration 12/25 | Loss: 0.00101591
Iteration 13/25 | Loss: 0.00101591
Iteration 14/25 | Loss: 0.00101591
Iteration 15/25 | Loss: 0.00101591
Iteration 16/25 | Loss: 0.00101591
Iteration 17/25 | Loss: 0.00101591
Iteration 18/25 | Loss: 0.00101591
Iteration 19/25 | Loss: 0.00101591
Iteration 20/25 | Loss: 0.00101591
Iteration 21/25 | Loss: 0.00101591
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010159078519791365, 0.0010159078519791365, 0.0010159078519791365, 0.0010159078519791365, 0.0010159078519791365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010159078519791365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101591
Iteration 2/1000 | Loss: 0.00002522
Iteration 3/1000 | Loss: 0.00001982
Iteration 4/1000 | Loss: 0.00001804
Iteration 5/1000 | Loss: 0.00001702
Iteration 6/1000 | Loss: 0.00001653
Iteration 7/1000 | Loss: 0.00001595
Iteration 8/1000 | Loss: 0.00001567
Iteration 9/1000 | Loss: 0.00001529
Iteration 10/1000 | Loss: 0.00001504
Iteration 11/1000 | Loss: 0.00001487
Iteration 12/1000 | Loss: 0.00001469
Iteration 13/1000 | Loss: 0.00001458
Iteration 14/1000 | Loss: 0.00001455
Iteration 15/1000 | Loss: 0.00001450
Iteration 16/1000 | Loss: 0.00001450
Iteration 17/1000 | Loss: 0.00001449
Iteration 18/1000 | Loss: 0.00001449
Iteration 19/1000 | Loss: 0.00001447
Iteration 20/1000 | Loss: 0.00001439
Iteration 21/1000 | Loss: 0.00001426
Iteration 22/1000 | Loss: 0.00001425
Iteration 23/1000 | Loss: 0.00001422
Iteration 24/1000 | Loss: 0.00001422
Iteration 25/1000 | Loss: 0.00001418
Iteration 26/1000 | Loss: 0.00001418
Iteration 27/1000 | Loss: 0.00001417
Iteration 28/1000 | Loss: 0.00001417
Iteration 29/1000 | Loss: 0.00001416
Iteration 30/1000 | Loss: 0.00001416
Iteration 31/1000 | Loss: 0.00001415
Iteration 32/1000 | Loss: 0.00001412
Iteration 33/1000 | Loss: 0.00001411
Iteration 34/1000 | Loss: 0.00001410
Iteration 35/1000 | Loss: 0.00001410
Iteration 36/1000 | Loss: 0.00001409
Iteration 37/1000 | Loss: 0.00001409
Iteration 38/1000 | Loss: 0.00001408
Iteration 39/1000 | Loss: 0.00001407
Iteration 40/1000 | Loss: 0.00001405
Iteration 41/1000 | Loss: 0.00001405
Iteration 42/1000 | Loss: 0.00001404
Iteration 43/1000 | Loss: 0.00001401
Iteration 44/1000 | Loss: 0.00001401
Iteration 45/1000 | Loss: 0.00001400
Iteration 46/1000 | Loss: 0.00001400
Iteration 47/1000 | Loss: 0.00001400
Iteration 48/1000 | Loss: 0.00001398
Iteration 49/1000 | Loss: 0.00001397
Iteration 50/1000 | Loss: 0.00001397
Iteration 51/1000 | Loss: 0.00001396
Iteration 52/1000 | Loss: 0.00001396
Iteration 53/1000 | Loss: 0.00001396
Iteration 54/1000 | Loss: 0.00001395
Iteration 55/1000 | Loss: 0.00001392
Iteration 56/1000 | Loss: 0.00001390
Iteration 57/1000 | Loss: 0.00001390
Iteration 58/1000 | Loss: 0.00001389
Iteration 59/1000 | Loss: 0.00001389
Iteration 60/1000 | Loss: 0.00001389
Iteration 61/1000 | Loss: 0.00001388
Iteration 62/1000 | Loss: 0.00001388
Iteration 63/1000 | Loss: 0.00001388
Iteration 64/1000 | Loss: 0.00001387
Iteration 65/1000 | Loss: 0.00001386
Iteration 66/1000 | Loss: 0.00001386
Iteration 67/1000 | Loss: 0.00001386
Iteration 68/1000 | Loss: 0.00001386
Iteration 69/1000 | Loss: 0.00001386
Iteration 70/1000 | Loss: 0.00001385
Iteration 71/1000 | Loss: 0.00001385
Iteration 72/1000 | Loss: 0.00001385
Iteration 73/1000 | Loss: 0.00001385
Iteration 74/1000 | Loss: 0.00001385
Iteration 75/1000 | Loss: 0.00001384
Iteration 76/1000 | Loss: 0.00001384
Iteration 77/1000 | Loss: 0.00001384
Iteration 78/1000 | Loss: 0.00001384
Iteration 79/1000 | Loss: 0.00001383
Iteration 80/1000 | Loss: 0.00001383
Iteration 81/1000 | Loss: 0.00001383
Iteration 82/1000 | Loss: 0.00001383
Iteration 83/1000 | Loss: 0.00001383
Iteration 84/1000 | Loss: 0.00001382
Iteration 85/1000 | Loss: 0.00001382
Iteration 86/1000 | Loss: 0.00001382
Iteration 87/1000 | Loss: 0.00001382
Iteration 88/1000 | Loss: 0.00001382
Iteration 89/1000 | Loss: 0.00001382
Iteration 90/1000 | Loss: 0.00001381
Iteration 91/1000 | Loss: 0.00001381
Iteration 92/1000 | Loss: 0.00001381
Iteration 93/1000 | Loss: 0.00001381
Iteration 94/1000 | Loss: 0.00001381
Iteration 95/1000 | Loss: 0.00001381
Iteration 96/1000 | Loss: 0.00001381
Iteration 97/1000 | Loss: 0.00001381
Iteration 98/1000 | Loss: 0.00001381
Iteration 99/1000 | Loss: 0.00001381
Iteration 100/1000 | Loss: 0.00001380
Iteration 101/1000 | Loss: 0.00001380
Iteration 102/1000 | Loss: 0.00001380
Iteration 103/1000 | Loss: 0.00001380
Iteration 104/1000 | Loss: 0.00001380
Iteration 105/1000 | Loss: 0.00001380
Iteration 106/1000 | Loss: 0.00001380
Iteration 107/1000 | Loss: 0.00001380
Iteration 108/1000 | Loss: 0.00001380
Iteration 109/1000 | Loss: 0.00001380
Iteration 110/1000 | Loss: 0.00001380
Iteration 111/1000 | Loss: 0.00001380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.3803377441945486e-05, 1.3803377441945486e-05, 1.3803377441945486e-05, 1.3803377441945486e-05, 1.3803377441945486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3803377441945486e-05

Optimization complete. Final v2v error: 3.158729076385498 mm

Highest mean error: 3.291821241378784 mm for frame 136

Lowest mean error: 3.0124850273132324 mm for frame 84

Saving results

Total time: 56.35020685195923
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009583
Iteration 2/25 | Loss: 0.00154513
Iteration 3/25 | Loss: 0.00136546
Iteration 4/25 | Loss: 0.00133984
Iteration 5/25 | Loss: 0.00133188
Iteration 6/25 | Loss: 0.00132974
Iteration 7/25 | Loss: 0.00132972
Iteration 8/25 | Loss: 0.00132972
Iteration 9/25 | Loss: 0.00132972
Iteration 10/25 | Loss: 0.00132972
Iteration 11/25 | Loss: 0.00132972
Iteration 12/25 | Loss: 0.00132972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013297158293426037, 0.0013297158293426037, 0.0013297158293426037, 0.0013297158293426037, 0.0013297158293426037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013297158293426037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.70373839
Iteration 2/25 | Loss: 0.00128590
Iteration 3/25 | Loss: 0.00128586
Iteration 4/25 | Loss: 0.00128585
Iteration 5/25 | Loss: 0.00128585
Iteration 6/25 | Loss: 0.00128585
Iteration 7/25 | Loss: 0.00128585
Iteration 8/25 | Loss: 0.00128585
Iteration 9/25 | Loss: 0.00128585
Iteration 10/25 | Loss: 0.00128585
Iteration 11/25 | Loss: 0.00128585
Iteration 12/25 | Loss: 0.00128585
Iteration 13/25 | Loss: 0.00128585
Iteration 14/25 | Loss: 0.00128585
Iteration 15/25 | Loss: 0.00128585
Iteration 16/25 | Loss: 0.00128585
Iteration 17/25 | Loss: 0.00128585
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012858512345701456, 0.0012858512345701456, 0.0012858512345701456, 0.0012858512345701456, 0.0012858512345701456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012858512345701456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128585
Iteration 2/1000 | Loss: 0.00006047
Iteration 3/1000 | Loss: 0.00004008
Iteration 4/1000 | Loss: 0.00003529
Iteration 5/1000 | Loss: 0.00003363
Iteration 6/1000 | Loss: 0.00003231
Iteration 7/1000 | Loss: 0.00003124
Iteration 8/1000 | Loss: 0.00003053
Iteration 9/1000 | Loss: 0.00003002
Iteration 10/1000 | Loss: 0.00002964
Iteration 11/1000 | Loss: 0.00002927
Iteration 12/1000 | Loss: 0.00002891
Iteration 13/1000 | Loss: 0.00002866
Iteration 14/1000 | Loss: 0.00002843
Iteration 15/1000 | Loss: 0.00002841
Iteration 16/1000 | Loss: 0.00002831
Iteration 17/1000 | Loss: 0.00002820
Iteration 18/1000 | Loss: 0.00002817
Iteration 19/1000 | Loss: 0.00002815
Iteration 20/1000 | Loss: 0.00002814
Iteration 21/1000 | Loss: 0.00002812
Iteration 22/1000 | Loss: 0.00002812
Iteration 23/1000 | Loss: 0.00002811
Iteration 24/1000 | Loss: 0.00002811
Iteration 25/1000 | Loss: 0.00002809
Iteration 26/1000 | Loss: 0.00002808
Iteration 27/1000 | Loss: 0.00002808
Iteration 28/1000 | Loss: 0.00002806
Iteration 29/1000 | Loss: 0.00002806
Iteration 30/1000 | Loss: 0.00002806
Iteration 31/1000 | Loss: 0.00002806
Iteration 32/1000 | Loss: 0.00002806
Iteration 33/1000 | Loss: 0.00002806
Iteration 34/1000 | Loss: 0.00002806
Iteration 35/1000 | Loss: 0.00002805
Iteration 36/1000 | Loss: 0.00002803
Iteration 37/1000 | Loss: 0.00002803
Iteration 38/1000 | Loss: 0.00002803
Iteration 39/1000 | Loss: 0.00002803
Iteration 40/1000 | Loss: 0.00002803
Iteration 41/1000 | Loss: 0.00002803
Iteration 42/1000 | Loss: 0.00002803
Iteration 43/1000 | Loss: 0.00002803
Iteration 44/1000 | Loss: 0.00002803
Iteration 45/1000 | Loss: 0.00002803
Iteration 46/1000 | Loss: 0.00002803
Iteration 47/1000 | Loss: 0.00002802
Iteration 48/1000 | Loss: 0.00002802
Iteration 49/1000 | Loss: 0.00002802
Iteration 50/1000 | Loss: 0.00002802
Iteration 51/1000 | Loss: 0.00002802
Iteration 52/1000 | Loss: 0.00002802
Iteration 53/1000 | Loss: 0.00002802
Iteration 54/1000 | Loss: 0.00002802
Iteration 55/1000 | Loss: 0.00002801
Iteration 56/1000 | Loss: 0.00002800
Iteration 57/1000 | Loss: 0.00002800
Iteration 58/1000 | Loss: 0.00002800
Iteration 59/1000 | Loss: 0.00002800
Iteration 60/1000 | Loss: 0.00002800
Iteration 61/1000 | Loss: 0.00002800
Iteration 62/1000 | Loss: 0.00002800
Iteration 63/1000 | Loss: 0.00002800
Iteration 64/1000 | Loss: 0.00002800
Iteration 65/1000 | Loss: 0.00002800
Iteration 66/1000 | Loss: 0.00002800
Iteration 67/1000 | Loss: 0.00002800
Iteration 68/1000 | Loss: 0.00002800
Iteration 69/1000 | Loss: 0.00002800
Iteration 70/1000 | Loss: 0.00002800
Iteration 71/1000 | Loss: 0.00002800
Iteration 72/1000 | Loss: 0.00002800
Iteration 73/1000 | Loss: 0.00002800
Iteration 74/1000 | Loss: 0.00002800
Iteration 75/1000 | Loss: 0.00002800
Iteration 76/1000 | Loss: 0.00002800
Iteration 77/1000 | Loss: 0.00002800
Iteration 78/1000 | Loss: 0.00002800
Iteration 79/1000 | Loss: 0.00002800
Iteration 80/1000 | Loss: 0.00002800
Iteration 81/1000 | Loss: 0.00002800
Iteration 82/1000 | Loss: 0.00002800
Iteration 83/1000 | Loss: 0.00002800
Iteration 84/1000 | Loss: 0.00002800
Iteration 85/1000 | Loss: 0.00002800
Iteration 86/1000 | Loss: 0.00002800
Iteration 87/1000 | Loss: 0.00002800
Iteration 88/1000 | Loss: 0.00002800
Iteration 89/1000 | Loss: 0.00002800
Iteration 90/1000 | Loss: 0.00002800
Iteration 91/1000 | Loss: 0.00002800
Iteration 92/1000 | Loss: 0.00002800
Iteration 93/1000 | Loss: 0.00002800
Iteration 94/1000 | Loss: 0.00002800
Iteration 95/1000 | Loss: 0.00002800
Iteration 96/1000 | Loss: 0.00002800
Iteration 97/1000 | Loss: 0.00002800
Iteration 98/1000 | Loss: 0.00002800
Iteration 99/1000 | Loss: 0.00002800
Iteration 100/1000 | Loss: 0.00002800
Iteration 101/1000 | Loss: 0.00002800
Iteration 102/1000 | Loss: 0.00002800
Iteration 103/1000 | Loss: 0.00002800
Iteration 104/1000 | Loss: 0.00002800
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [2.800007314363029e-05, 2.800007314363029e-05, 2.800007314363029e-05, 2.800007314363029e-05, 2.800007314363029e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.800007314363029e-05

Optimization complete. Final v2v error: 4.473080635070801 mm

Highest mean error: 5.011393070220947 mm for frame 70

Lowest mean error: 4.1242594718933105 mm for frame 57

Saving results

Total time: 36.608318567276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00703041
Iteration 2/25 | Loss: 0.00155991
Iteration 3/25 | Loss: 0.00134897
Iteration 4/25 | Loss: 0.00130697
Iteration 5/25 | Loss: 0.00129655
Iteration 6/25 | Loss: 0.00129342
Iteration 7/25 | Loss: 0.00129095
Iteration 8/25 | Loss: 0.00128923
Iteration 9/25 | Loss: 0.00129032
Iteration 10/25 | Loss: 0.00128994
Iteration 11/25 | Loss: 0.00128807
Iteration 12/25 | Loss: 0.00128752
Iteration 13/25 | Loss: 0.00128748
Iteration 14/25 | Loss: 0.00128747
Iteration 15/25 | Loss: 0.00128747
Iteration 16/25 | Loss: 0.00128747
Iteration 17/25 | Loss: 0.00128747
Iteration 18/25 | Loss: 0.00128746
Iteration 19/25 | Loss: 0.00128746
Iteration 20/25 | Loss: 0.00128746
Iteration 21/25 | Loss: 0.00128745
Iteration 22/25 | Loss: 0.00128745
Iteration 23/25 | Loss: 0.00128745
Iteration 24/25 | Loss: 0.00128745
Iteration 25/25 | Loss: 0.00128745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.50250649
Iteration 2/25 | Loss: 0.00143595
Iteration 3/25 | Loss: 0.00143594
Iteration 4/25 | Loss: 0.00143594
Iteration 5/25 | Loss: 0.00143594
Iteration 6/25 | Loss: 0.00143594
Iteration 7/25 | Loss: 0.00143594
Iteration 8/25 | Loss: 0.00143594
Iteration 9/25 | Loss: 0.00143594
Iteration 10/25 | Loss: 0.00143594
Iteration 11/25 | Loss: 0.00143594
Iteration 12/25 | Loss: 0.00143594
Iteration 13/25 | Loss: 0.00143594
Iteration 14/25 | Loss: 0.00143593
Iteration 15/25 | Loss: 0.00143594
Iteration 16/25 | Loss: 0.00143594
Iteration 17/25 | Loss: 0.00143594
Iteration 18/25 | Loss: 0.00143594
Iteration 19/25 | Loss: 0.00143594
Iteration 20/25 | Loss: 0.00143594
Iteration 21/25 | Loss: 0.00143594
Iteration 22/25 | Loss: 0.00143594
Iteration 23/25 | Loss: 0.00143593
Iteration 24/25 | Loss: 0.00143594
Iteration 25/25 | Loss: 0.00143593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143594
Iteration 2/1000 | Loss: 0.00009455
Iteration 3/1000 | Loss: 0.00005375
Iteration 4/1000 | Loss: 0.00003687
Iteration 5/1000 | Loss: 0.00003284
Iteration 6/1000 | Loss: 0.00002978
Iteration 7/1000 | Loss: 0.00004411
Iteration 8/1000 | Loss: 0.00004639
Iteration 9/1000 | Loss: 0.00003711
Iteration 10/1000 | Loss: 0.00002954
Iteration 11/1000 | Loss: 0.00002742
Iteration 12/1000 | Loss: 0.00002637
Iteration 13/1000 | Loss: 0.00002565
Iteration 14/1000 | Loss: 0.00002528
Iteration 15/1000 | Loss: 0.00002489
Iteration 16/1000 | Loss: 0.00004387
Iteration 17/1000 | Loss: 0.00002693
Iteration 18/1000 | Loss: 0.00002596
Iteration 19/1000 | Loss: 0.00002544
Iteration 20/1000 | Loss: 0.00005808
Iteration 21/1000 | Loss: 0.00004249
Iteration 22/1000 | Loss: 0.00005830
Iteration 23/1000 | Loss: 0.00006317
Iteration 24/1000 | Loss: 0.00006252
Iteration 25/1000 | Loss: 0.00005728
Iteration 26/1000 | Loss: 0.00005779
Iteration 27/1000 | Loss: 0.00003700
Iteration 28/1000 | Loss: 0.00005291
Iteration 29/1000 | Loss: 0.00004109
Iteration 30/1000 | Loss: 0.00004307
Iteration 31/1000 | Loss: 0.00003485
Iteration 32/1000 | Loss: 0.00004180
Iteration 33/1000 | Loss: 0.00003334
Iteration 34/1000 | Loss: 0.00003148
Iteration 35/1000 | Loss: 0.00002798
Iteration 36/1000 | Loss: 0.00004114
Iteration 37/1000 | Loss: 0.00002690
Iteration 38/1000 | Loss: 0.00002382
Iteration 39/1000 | Loss: 0.00002368
Iteration 40/1000 | Loss: 0.00002357
Iteration 41/1000 | Loss: 0.00002356
Iteration 42/1000 | Loss: 0.00002355
Iteration 43/1000 | Loss: 0.00002355
Iteration 44/1000 | Loss: 0.00002352
Iteration 45/1000 | Loss: 0.00004602
Iteration 46/1000 | Loss: 0.00002735
Iteration 47/1000 | Loss: 0.00004442
Iteration 48/1000 | Loss: 0.00002594
Iteration 49/1000 | Loss: 0.00002367
Iteration 50/1000 | Loss: 0.00002330
Iteration 51/1000 | Loss: 0.00002311
Iteration 52/1000 | Loss: 0.00002297
Iteration 53/1000 | Loss: 0.00002289
Iteration 54/1000 | Loss: 0.00002287
Iteration 55/1000 | Loss: 0.00002286
Iteration 56/1000 | Loss: 0.00002285
Iteration 57/1000 | Loss: 0.00002285
Iteration 58/1000 | Loss: 0.00002284
Iteration 59/1000 | Loss: 0.00002284
Iteration 60/1000 | Loss: 0.00002278
Iteration 61/1000 | Loss: 0.00002265
Iteration 62/1000 | Loss: 0.00002264
Iteration 63/1000 | Loss: 0.00002253
Iteration 64/1000 | Loss: 0.00002246
Iteration 65/1000 | Loss: 0.00002246
Iteration 66/1000 | Loss: 0.00002245
Iteration 67/1000 | Loss: 0.00002245
Iteration 68/1000 | Loss: 0.00002241
Iteration 69/1000 | Loss: 0.00002240
Iteration 70/1000 | Loss: 0.00002240
Iteration 71/1000 | Loss: 0.00002240
Iteration 72/1000 | Loss: 0.00002239
Iteration 73/1000 | Loss: 0.00002239
Iteration 74/1000 | Loss: 0.00002232
Iteration 75/1000 | Loss: 0.00002231
Iteration 76/1000 | Loss: 0.00002231
Iteration 77/1000 | Loss: 0.00002230
Iteration 78/1000 | Loss: 0.00002224
Iteration 79/1000 | Loss: 0.00002223
Iteration 80/1000 | Loss: 0.00002223
Iteration 81/1000 | Loss: 0.00002223
Iteration 82/1000 | Loss: 0.00002222
Iteration 83/1000 | Loss: 0.00002222
Iteration 84/1000 | Loss: 0.00002222
Iteration 85/1000 | Loss: 0.00002222
Iteration 86/1000 | Loss: 0.00002221
Iteration 87/1000 | Loss: 0.00002221
Iteration 88/1000 | Loss: 0.00002221
Iteration 89/1000 | Loss: 0.00002220
Iteration 90/1000 | Loss: 0.00002220
Iteration 91/1000 | Loss: 0.00002220
Iteration 92/1000 | Loss: 0.00002218
Iteration 93/1000 | Loss: 0.00002218
Iteration 94/1000 | Loss: 0.00002216
Iteration 95/1000 | Loss: 0.00002216
Iteration 96/1000 | Loss: 0.00002215
Iteration 97/1000 | Loss: 0.00002215
Iteration 98/1000 | Loss: 0.00002214
Iteration 99/1000 | Loss: 0.00002214
Iteration 100/1000 | Loss: 0.00002214
Iteration 101/1000 | Loss: 0.00002214
Iteration 102/1000 | Loss: 0.00002213
Iteration 103/1000 | Loss: 0.00002213
Iteration 104/1000 | Loss: 0.00002213
Iteration 105/1000 | Loss: 0.00002213
Iteration 106/1000 | Loss: 0.00002213
Iteration 107/1000 | Loss: 0.00002213
Iteration 108/1000 | Loss: 0.00002212
Iteration 109/1000 | Loss: 0.00002212
Iteration 110/1000 | Loss: 0.00002212
Iteration 111/1000 | Loss: 0.00002211
Iteration 112/1000 | Loss: 0.00002211
Iteration 113/1000 | Loss: 0.00002211
Iteration 114/1000 | Loss: 0.00002211
Iteration 115/1000 | Loss: 0.00002211
Iteration 116/1000 | Loss: 0.00002210
Iteration 117/1000 | Loss: 0.00002210
Iteration 118/1000 | Loss: 0.00002210
Iteration 119/1000 | Loss: 0.00002210
Iteration 120/1000 | Loss: 0.00002210
Iteration 121/1000 | Loss: 0.00002210
Iteration 122/1000 | Loss: 0.00002210
Iteration 123/1000 | Loss: 0.00002210
Iteration 124/1000 | Loss: 0.00002210
Iteration 125/1000 | Loss: 0.00002210
Iteration 126/1000 | Loss: 0.00002210
Iteration 127/1000 | Loss: 0.00002210
Iteration 128/1000 | Loss: 0.00002210
Iteration 129/1000 | Loss: 0.00002210
Iteration 130/1000 | Loss: 0.00002210
Iteration 131/1000 | Loss: 0.00002210
Iteration 132/1000 | Loss: 0.00002209
Iteration 133/1000 | Loss: 0.00002209
Iteration 134/1000 | Loss: 0.00002209
Iteration 135/1000 | Loss: 0.00002209
Iteration 136/1000 | Loss: 0.00002209
Iteration 137/1000 | Loss: 0.00002209
Iteration 138/1000 | Loss: 0.00002209
Iteration 139/1000 | Loss: 0.00002209
Iteration 140/1000 | Loss: 0.00002209
Iteration 141/1000 | Loss: 0.00002209
Iteration 142/1000 | Loss: 0.00002209
Iteration 143/1000 | Loss: 0.00002209
Iteration 144/1000 | Loss: 0.00002209
Iteration 145/1000 | Loss: 0.00002209
Iteration 146/1000 | Loss: 0.00002209
Iteration 147/1000 | Loss: 0.00002208
Iteration 148/1000 | Loss: 0.00002208
Iteration 149/1000 | Loss: 0.00002208
Iteration 150/1000 | Loss: 0.00002208
Iteration 151/1000 | Loss: 0.00002208
Iteration 152/1000 | Loss: 0.00002208
Iteration 153/1000 | Loss: 0.00002208
Iteration 154/1000 | Loss: 0.00002208
Iteration 155/1000 | Loss: 0.00002208
Iteration 156/1000 | Loss: 0.00002208
Iteration 157/1000 | Loss: 0.00002208
Iteration 158/1000 | Loss: 0.00002208
Iteration 159/1000 | Loss: 0.00002208
Iteration 160/1000 | Loss: 0.00002208
Iteration 161/1000 | Loss: 0.00002208
Iteration 162/1000 | Loss: 0.00002208
Iteration 163/1000 | Loss: 0.00002208
Iteration 164/1000 | Loss: 0.00002208
Iteration 165/1000 | Loss: 0.00002208
Iteration 166/1000 | Loss: 0.00002208
Iteration 167/1000 | Loss: 0.00002208
Iteration 168/1000 | Loss: 0.00002208
Iteration 169/1000 | Loss: 0.00002208
Iteration 170/1000 | Loss: 0.00002208
Iteration 171/1000 | Loss: 0.00002208
Iteration 172/1000 | Loss: 0.00002208
Iteration 173/1000 | Loss: 0.00002208
Iteration 174/1000 | Loss: 0.00002208
Iteration 175/1000 | Loss: 0.00002208
Iteration 176/1000 | Loss: 0.00002208
Iteration 177/1000 | Loss: 0.00002208
Iteration 178/1000 | Loss: 0.00002208
Iteration 179/1000 | Loss: 0.00002208
Iteration 180/1000 | Loss: 0.00002208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.2083944713813253e-05, 2.2083944713813253e-05, 2.2083944713813253e-05, 2.2083944713813253e-05, 2.2083944713813253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2083944713813253e-05

Optimization complete. Final v2v error: 3.837477684020996 mm

Highest mean error: 5.617816925048828 mm for frame 39

Lowest mean error: 2.958789348602295 mm for frame 212

Saving results

Total time: 116.88647747039795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987754
Iteration 2/25 | Loss: 0.00987753
Iteration 3/25 | Loss: 0.00227147
Iteration 4/25 | Loss: 0.00184438
Iteration 5/25 | Loss: 0.00176918
Iteration 6/25 | Loss: 0.00174871
Iteration 7/25 | Loss: 0.00168234
Iteration 8/25 | Loss: 0.00165990
Iteration 9/25 | Loss: 0.00159355
Iteration 10/25 | Loss: 0.00159286
Iteration 11/25 | Loss: 0.00157152
Iteration 12/25 | Loss: 0.00153499
Iteration 13/25 | Loss: 0.00153954
Iteration 14/25 | Loss: 0.00152586
Iteration 15/25 | Loss: 0.00151958
Iteration 16/25 | Loss: 0.00151795
Iteration 17/25 | Loss: 0.00152415
Iteration 18/25 | Loss: 0.00151613
Iteration 19/25 | Loss: 0.00151520
Iteration 20/25 | Loss: 0.00151504
Iteration 21/25 | Loss: 0.00151504
Iteration 22/25 | Loss: 0.00151504
Iteration 23/25 | Loss: 0.00151504
Iteration 24/25 | Loss: 0.00151504
Iteration 25/25 | Loss: 0.00151504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31214571
Iteration 2/25 | Loss: 0.00257368
Iteration 3/25 | Loss: 0.00251919
Iteration 4/25 | Loss: 0.00251919
Iteration 5/25 | Loss: 0.00251919
Iteration 6/25 | Loss: 0.00251919
Iteration 7/25 | Loss: 0.00251919
Iteration 8/25 | Loss: 0.00251919
Iteration 9/25 | Loss: 0.00251919
Iteration 10/25 | Loss: 0.00251919
Iteration 11/25 | Loss: 0.00251919
Iteration 12/25 | Loss: 0.00251919
Iteration 13/25 | Loss: 0.00251919
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0025191905442625284, 0.0025191905442625284, 0.0025191905442625284, 0.0025191905442625284, 0.0025191905442625284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025191905442625284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00251919
Iteration 2/1000 | Loss: 0.00036253
Iteration 3/1000 | Loss: 0.00025286
Iteration 4/1000 | Loss: 0.00026081
Iteration 5/1000 | Loss: 0.00019129
Iteration 6/1000 | Loss: 0.00035344
Iteration 7/1000 | Loss: 0.00016693
Iteration 8/1000 | Loss: 0.00027052
Iteration 9/1000 | Loss: 0.00022071
Iteration 10/1000 | Loss: 0.00015023
Iteration 11/1000 | Loss: 0.00014471
Iteration 12/1000 | Loss: 0.00014053
Iteration 13/1000 | Loss: 0.00013643
Iteration 14/1000 | Loss: 0.00013348
Iteration 15/1000 | Loss: 0.00018519
Iteration 16/1000 | Loss: 0.00018971
Iteration 17/1000 | Loss: 0.00025180
Iteration 18/1000 | Loss: 0.00150008
Iteration 19/1000 | Loss: 0.00708636
Iteration 20/1000 | Loss: 0.00615891
Iteration 21/1000 | Loss: 0.00953305
Iteration 22/1000 | Loss: 0.00138771
Iteration 23/1000 | Loss: 0.00076047
Iteration 24/1000 | Loss: 0.00103469
Iteration 25/1000 | Loss: 0.00052403
Iteration 26/1000 | Loss: 0.00052261
Iteration 27/1000 | Loss: 0.00050135
Iteration 28/1000 | Loss: 0.00054081
Iteration 29/1000 | Loss: 0.00019492
Iteration 30/1000 | Loss: 0.00018011
Iteration 31/1000 | Loss: 0.00009044
Iteration 32/1000 | Loss: 0.00008659
Iteration 33/1000 | Loss: 0.00005603
Iteration 34/1000 | Loss: 0.00012442
Iteration 35/1000 | Loss: 0.00020575
Iteration 36/1000 | Loss: 0.00004266
Iteration 37/1000 | Loss: 0.00003565
Iteration 38/1000 | Loss: 0.00003120
Iteration 39/1000 | Loss: 0.00007897
Iteration 40/1000 | Loss: 0.00026119
Iteration 41/1000 | Loss: 0.00011862
Iteration 42/1000 | Loss: 0.00016385
Iteration 43/1000 | Loss: 0.00016253
Iteration 44/1000 | Loss: 0.00016490
Iteration 45/1000 | Loss: 0.00009583
Iteration 46/1000 | Loss: 0.00006726
Iteration 47/1000 | Loss: 0.00011027
Iteration 48/1000 | Loss: 0.00014435
Iteration 49/1000 | Loss: 0.00015525
Iteration 50/1000 | Loss: 0.00002964
Iteration 51/1000 | Loss: 0.00002244
Iteration 52/1000 | Loss: 0.00001986
Iteration 53/1000 | Loss: 0.00005184
Iteration 54/1000 | Loss: 0.00001658
Iteration 55/1000 | Loss: 0.00001554
Iteration 56/1000 | Loss: 0.00001482
Iteration 57/1000 | Loss: 0.00001396
Iteration 58/1000 | Loss: 0.00001351
Iteration 59/1000 | Loss: 0.00001317
Iteration 60/1000 | Loss: 0.00001292
Iteration 61/1000 | Loss: 0.00001276
Iteration 62/1000 | Loss: 0.00001268
Iteration 63/1000 | Loss: 0.00001265
Iteration 64/1000 | Loss: 0.00001264
Iteration 65/1000 | Loss: 0.00001262
Iteration 66/1000 | Loss: 0.00001261
Iteration 67/1000 | Loss: 0.00001259
Iteration 68/1000 | Loss: 0.00001247
Iteration 69/1000 | Loss: 0.00001244
Iteration 70/1000 | Loss: 0.00001242
Iteration 71/1000 | Loss: 0.00001242
Iteration 72/1000 | Loss: 0.00001242
Iteration 73/1000 | Loss: 0.00001242
Iteration 74/1000 | Loss: 0.00001241
Iteration 75/1000 | Loss: 0.00001241
Iteration 76/1000 | Loss: 0.00001241
Iteration 77/1000 | Loss: 0.00001241
Iteration 78/1000 | Loss: 0.00001240
Iteration 79/1000 | Loss: 0.00001240
Iteration 80/1000 | Loss: 0.00001240
Iteration 81/1000 | Loss: 0.00001240
Iteration 82/1000 | Loss: 0.00001240
Iteration 83/1000 | Loss: 0.00001239
Iteration 84/1000 | Loss: 0.00001236
Iteration 85/1000 | Loss: 0.00001236
Iteration 86/1000 | Loss: 0.00001235
Iteration 87/1000 | Loss: 0.00001235
Iteration 88/1000 | Loss: 0.00001235
Iteration 89/1000 | Loss: 0.00001235
Iteration 90/1000 | Loss: 0.00001235
Iteration 91/1000 | Loss: 0.00001235
Iteration 92/1000 | Loss: 0.00001235
Iteration 93/1000 | Loss: 0.00001232
Iteration 94/1000 | Loss: 0.00001232
Iteration 95/1000 | Loss: 0.00001232
Iteration 96/1000 | Loss: 0.00001232
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001232
Iteration 99/1000 | Loss: 0.00001232
Iteration 100/1000 | Loss: 0.00001231
Iteration 101/1000 | Loss: 0.00001231
Iteration 102/1000 | Loss: 0.00001231
Iteration 103/1000 | Loss: 0.00001230
Iteration 104/1000 | Loss: 0.00001230
Iteration 105/1000 | Loss: 0.00001230
Iteration 106/1000 | Loss: 0.00001230
Iteration 107/1000 | Loss: 0.00001229
Iteration 108/1000 | Loss: 0.00001229
Iteration 109/1000 | Loss: 0.00001229
Iteration 110/1000 | Loss: 0.00001229
Iteration 111/1000 | Loss: 0.00001229
Iteration 112/1000 | Loss: 0.00001229
Iteration 113/1000 | Loss: 0.00001229
Iteration 114/1000 | Loss: 0.00001229
Iteration 115/1000 | Loss: 0.00001229
Iteration 116/1000 | Loss: 0.00001229
Iteration 117/1000 | Loss: 0.00001228
Iteration 118/1000 | Loss: 0.00001228
Iteration 119/1000 | Loss: 0.00001228
Iteration 120/1000 | Loss: 0.00001228
Iteration 121/1000 | Loss: 0.00001228
Iteration 122/1000 | Loss: 0.00001228
Iteration 123/1000 | Loss: 0.00001228
Iteration 124/1000 | Loss: 0.00001227
Iteration 125/1000 | Loss: 0.00001227
Iteration 126/1000 | Loss: 0.00001227
Iteration 127/1000 | Loss: 0.00001227
Iteration 128/1000 | Loss: 0.00001227
Iteration 129/1000 | Loss: 0.00001227
Iteration 130/1000 | Loss: 0.00001227
Iteration 131/1000 | Loss: 0.00001226
Iteration 132/1000 | Loss: 0.00001226
Iteration 133/1000 | Loss: 0.00001226
Iteration 134/1000 | Loss: 0.00001226
Iteration 135/1000 | Loss: 0.00001226
Iteration 136/1000 | Loss: 0.00001226
Iteration 137/1000 | Loss: 0.00001226
Iteration 138/1000 | Loss: 0.00001226
Iteration 139/1000 | Loss: 0.00001226
Iteration 140/1000 | Loss: 0.00001225
Iteration 141/1000 | Loss: 0.00001225
Iteration 142/1000 | Loss: 0.00001225
Iteration 143/1000 | Loss: 0.00001225
Iteration 144/1000 | Loss: 0.00001225
Iteration 145/1000 | Loss: 0.00001225
Iteration 146/1000 | Loss: 0.00001225
Iteration 147/1000 | Loss: 0.00001225
Iteration 148/1000 | Loss: 0.00001224
Iteration 149/1000 | Loss: 0.00001224
Iteration 150/1000 | Loss: 0.00001224
Iteration 151/1000 | Loss: 0.00001224
Iteration 152/1000 | Loss: 0.00001224
Iteration 153/1000 | Loss: 0.00001224
Iteration 154/1000 | Loss: 0.00001224
Iteration 155/1000 | Loss: 0.00001224
Iteration 156/1000 | Loss: 0.00001224
Iteration 157/1000 | Loss: 0.00001224
Iteration 158/1000 | Loss: 0.00001224
Iteration 159/1000 | Loss: 0.00001224
Iteration 160/1000 | Loss: 0.00001224
Iteration 161/1000 | Loss: 0.00001224
Iteration 162/1000 | Loss: 0.00001224
Iteration 163/1000 | Loss: 0.00001224
Iteration 164/1000 | Loss: 0.00001223
Iteration 165/1000 | Loss: 0.00001223
Iteration 166/1000 | Loss: 0.00001223
Iteration 167/1000 | Loss: 0.00001223
Iteration 168/1000 | Loss: 0.00001223
Iteration 169/1000 | Loss: 0.00001223
Iteration 170/1000 | Loss: 0.00001223
Iteration 171/1000 | Loss: 0.00001223
Iteration 172/1000 | Loss: 0.00001223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.2233659617777448e-05, 1.2233659617777448e-05, 1.2233659617777448e-05, 1.2233659617777448e-05, 1.2233659617777448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2233659617777448e-05

Optimization complete. Final v2v error: 3.0337836742401123 mm

Highest mean error: 4.042691230773926 mm for frame 107

Lowest mean error: 2.8688175678253174 mm for frame 174

Saving results

Total time: 148.76983070373535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015770
Iteration 2/25 | Loss: 0.00206639
Iteration 3/25 | Loss: 0.00167915
Iteration 4/25 | Loss: 0.00195553
Iteration 5/25 | Loss: 0.00210195
Iteration 6/25 | Loss: 0.00179464
Iteration 7/25 | Loss: 0.00197519
Iteration 8/25 | Loss: 0.00207770
Iteration 9/25 | Loss: 0.00186800
Iteration 10/25 | Loss: 0.00148552
Iteration 11/25 | Loss: 0.00138193
Iteration 12/25 | Loss: 0.00133832
Iteration 13/25 | Loss: 0.00131491
Iteration 14/25 | Loss: 0.00130107
Iteration 15/25 | Loss: 0.00128729
Iteration 16/25 | Loss: 0.00127535
Iteration 17/25 | Loss: 0.00126905
Iteration 18/25 | Loss: 0.00127440
Iteration 19/25 | Loss: 0.00126911
Iteration 20/25 | Loss: 0.00124032
Iteration 21/25 | Loss: 0.00123604
Iteration 22/25 | Loss: 0.00124072
Iteration 23/25 | Loss: 0.00123055
Iteration 24/25 | Loss: 0.00122417
Iteration 25/25 | Loss: 0.00121549

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32398129
Iteration 2/25 | Loss: 0.00077527
Iteration 3/25 | Loss: 0.00077527
Iteration 4/25 | Loss: 0.00077527
Iteration 5/25 | Loss: 0.00077527
Iteration 6/25 | Loss: 0.00077527
Iteration 7/25 | Loss: 0.00077527
Iteration 8/25 | Loss: 0.00077527
Iteration 9/25 | Loss: 0.00077527
Iteration 10/25 | Loss: 0.00077527
Iteration 11/25 | Loss: 0.00077527
Iteration 12/25 | Loss: 0.00077527
Iteration 13/25 | Loss: 0.00077527
Iteration 14/25 | Loss: 0.00077527
Iteration 15/25 | Loss: 0.00077527
Iteration 16/25 | Loss: 0.00077527
Iteration 17/25 | Loss: 0.00077527
Iteration 18/25 | Loss: 0.00077527
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007752715027891099, 0.0007752715027891099, 0.0007752715027891099, 0.0007752715027891099, 0.0007752715027891099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007752715027891099

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077527
Iteration 2/1000 | Loss: 0.00002348
Iteration 3/1000 | Loss: 0.00001816
Iteration 4/1000 | Loss: 0.00001659
Iteration 5/1000 | Loss: 0.00001550
Iteration 6/1000 | Loss: 0.00001459
Iteration 7/1000 | Loss: 0.00001417
Iteration 8/1000 | Loss: 0.00001374
Iteration 9/1000 | Loss: 0.00001333
Iteration 10/1000 | Loss: 0.00001306
Iteration 11/1000 | Loss: 0.00001300
Iteration 12/1000 | Loss: 0.00001288
Iteration 13/1000 | Loss: 0.00001277
Iteration 14/1000 | Loss: 0.00001263
Iteration 15/1000 | Loss: 0.00001262
Iteration 16/1000 | Loss: 0.00001262
Iteration 17/1000 | Loss: 0.00001262
Iteration 18/1000 | Loss: 0.00001262
Iteration 19/1000 | Loss: 0.00001261
Iteration 20/1000 | Loss: 0.00001261
Iteration 21/1000 | Loss: 0.00001261
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001259
Iteration 24/1000 | Loss: 0.00001259
Iteration 25/1000 | Loss: 0.00001259
Iteration 26/1000 | Loss: 0.00001259
Iteration 27/1000 | Loss: 0.00001259
Iteration 28/1000 | Loss: 0.00001256
Iteration 29/1000 | Loss: 0.00001256
Iteration 30/1000 | Loss: 0.00001255
Iteration 31/1000 | Loss: 0.00001254
Iteration 32/1000 | Loss: 0.00001254
Iteration 33/1000 | Loss: 0.00001254
Iteration 34/1000 | Loss: 0.00001253
Iteration 35/1000 | Loss: 0.00001253
Iteration 36/1000 | Loss: 0.00001253
Iteration 37/1000 | Loss: 0.00001253
Iteration 38/1000 | Loss: 0.00001252
Iteration 39/1000 | Loss: 0.00001252
Iteration 40/1000 | Loss: 0.00001251
Iteration 41/1000 | Loss: 0.00001251
Iteration 42/1000 | Loss: 0.00001251
Iteration 43/1000 | Loss: 0.00001250
Iteration 44/1000 | Loss: 0.00001250
Iteration 45/1000 | Loss: 0.00001250
Iteration 46/1000 | Loss: 0.00001249
Iteration 47/1000 | Loss: 0.00001249
Iteration 48/1000 | Loss: 0.00001249
Iteration 49/1000 | Loss: 0.00001249
Iteration 50/1000 | Loss: 0.00001248
Iteration 51/1000 | Loss: 0.00001248
Iteration 52/1000 | Loss: 0.00001248
Iteration 53/1000 | Loss: 0.00001248
Iteration 54/1000 | Loss: 0.00001248
Iteration 55/1000 | Loss: 0.00001247
Iteration 56/1000 | Loss: 0.00001247
Iteration 57/1000 | Loss: 0.00001247
Iteration 58/1000 | Loss: 0.00001247
Iteration 59/1000 | Loss: 0.00001247
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001245
Iteration 62/1000 | Loss: 0.00001245
Iteration 63/1000 | Loss: 0.00001244
Iteration 64/1000 | Loss: 0.00001244
Iteration 65/1000 | Loss: 0.00001244
Iteration 66/1000 | Loss: 0.00001244
Iteration 67/1000 | Loss: 0.00001244
Iteration 68/1000 | Loss: 0.00001244
Iteration 69/1000 | Loss: 0.00001244
Iteration 70/1000 | Loss: 0.00001244
Iteration 71/1000 | Loss: 0.00001244
Iteration 72/1000 | Loss: 0.00001244
Iteration 73/1000 | Loss: 0.00001244
Iteration 74/1000 | Loss: 0.00001244
Iteration 75/1000 | Loss: 0.00001244
Iteration 76/1000 | Loss: 0.00001243
Iteration 77/1000 | Loss: 0.00001243
Iteration 78/1000 | Loss: 0.00001243
Iteration 79/1000 | Loss: 0.00001243
Iteration 80/1000 | Loss: 0.00001243
Iteration 81/1000 | Loss: 0.00001243
Iteration 82/1000 | Loss: 0.00001243
Iteration 83/1000 | Loss: 0.00001243
Iteration 84/1000 | Loss: 0.00001243
Iteration 85/1000 | Loss: 0.00001243
Iteration 86/1000 | Loss: 0.00001243
Iteration 87/1000 | Loss: 0.00001242
Iteration 88/1000 | Loss: 0.00001242
Iteration 89/1000 | Loss: 0.00001242
Iteration 90/1000 | Loss: 0.00001242
Iteration 91/1000 | Loss: 0.00001242
Iteration 92/1000 | Loss: 0.00001241
Iteration 93/1000 | Loss: 0.00001241
Iteration 94/1000 | Loss: 0.00001241
Iteration 95/1000 | Loss: 0.00001241
Iteration 96/1000 | Loss: 0.00001241
Iteration 97/1000 | Loss: 0.00001241
Iteration 98/1000 | Loss: 0.00001241
Iteration 99/1000 | Loss: 0.00001241
Iteration 100/1000 | Loss: 0.00001241
Iteration 101/1000 | Loss: 0.00001240
Iteration 102/1000 | Loss: 0.00001240
Iteration 103/1000 | Loss: 0.00001240
Iteration 104/1000 | Loss: 0.00001240
Iteration 105/1000 | Loss: 0.00001240
Iteration 106/1000 | Loss: 0.00001240
Iteration 107/1000 | Loss: 0.00001240
Iteration 108/1000 | Loss: 0.00001240
Iteration 109/1000 | Loss: 0.00001240
Iteration 110/1000 | Loss: 0.00001240
Iteration 111/1000 | Loss: 0.00001240
Iteration 112/1000 | Loss: 0.00001240
Iteration 113/1000 | Loss: 0.00001240
Iteration 114/1000 | Loss: 0.00001240
Iteration 115/1000 | Loss: 0.00001240
Iteration 116/1000 | Loss: 0.00001240
Iteration 117/1000 | Loss: 0.00001240
Iteration 118/1000 | Loss: 0.00001240
Iteration 119/1000 | Loss: 0.00001240
Iteration 120/1000 | Loss: 0.00001239
Iteration 121/1000 | Loss: 0.00001239
Iteration 122/1000 | Loss: 0.00001239
Iteration 123/1000 | Loss: 0.00001239
Iteration 124/1000 | Loss: 0.00001239
Iteration 125/1000 | Loss: 0.00001239
Iteration 126/1000 | Loss: 0.00001239
Iteration 127/1000 | Loss: 0.00001239
Iteration 128/1000 | Loss: 0.00001239
Iteration 129/1000 | Loss: 0.00001239
Iteration 130/1000 | Loss: 0.00001239
Iteration 131/1000 | Loss: 0.00001239
Iteration 132/1000 | Loss: 0.00001239
Iteration 133/1000 | Loss: 0.00001239
Iteration 134/1000 | Loss: 0.00001239
Iteration 135/1000 | Loss: 0.00001239
Iteration 136/1000 | Loss: 0.00001239
Iteration 137/1000 | Loss: 0.00001239
Iteration 138/1000 | Loss: 0.00001239
Iteration 139/1000 | Loss: 0.00001239
Iteration 140/1000 | Loss: 0.00001239
Iteration 141/1000 | Loss: 0.00001239
Iteration 142/1000 | Loss: 0.00001239
Iteration 143/1000 | Loss: 0.00001239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.2393240467645228e-05, 1.2393240467645228e-05, 1.2393240467645228e-05, 1.2393240467645228e-05, 1.2393240467645228e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2393240467645228e-05

Optimization complete. Final v2v error: 3.002195358276367 mm

Highest mean error: 5.380728244781494 mm for frame 179

Lowest mean error: 2.8899264335632324 mm for frame 31

Saving results

Total time: 75.24083924293518
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065583
Iteration 2/25 | Loss: 0.00298940
Iteration 3/25 | Loss: 0.00239467
Iteration 4/25 | Loss: 0.00221031
Iteration 5/25 | Loss: 0.00195804
Iteration 6/25 | Loss: 0.00177111
Iteration 7/25 | Loss: 0.00165080
Iteration 8/25 | Loss: 0.00161125
Iteration 9/25 | Loss: 0.00160353
Iteration 10/25 | Loss: 0.00159951
Iteration 11/25 | Loss: 0.00156279
Iteration 12/25 | Loss: 0.00152789
Iteration 13/25 | Loss: 0.00150530
Iteration 14/25 | Loss: 0.00149926
Iteration 15/25 | Loss: 0.00149367
Iteration 16/25 | Loss: 0.00148157
Iteration 17/25 | Loss: 0.00147548
Iteration 18/25 | Loss: 0.00147310
Iteration 19/25 | Loss: 0.00148169
Iteration 20/25 | Loss: 0.00146940
Iteration 21/25 | Loss: 0.00146760
Iteration 22/25 | Loss: 0.00146729
Iteration 23/25 | Loss: 0.00146717
Iteration 24/25 | Loss: 0.00146709
Iteration 25/25 | Loss: 0.00146707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.58983845
Iteration 2/25 | Loss: 0.00978977
Iteration 3/25 | Loss: 0.00195217
Iteration 4/25 | Loss: 0.00195217
Iteration 5/25 | Loss: 0.00195217
Iteration 6/25 | Loss: 0.00195217
Iteration 7/25 | Loss: 0.00195217
Iteration 8/25 | Loss: 0.00195217
Iteration 9/25 | Loss: 0.00195217
Iteration 10/25 | Loss: 0.00195216
Iteration 11/25 | Loss: 0.00195216
Iteration 12/25 | Loss: 0.00195216
Iteration 13/25 | Loss: 0.00195216
Iteration 14/25 | Loss: 0.00195216
Iteration 15/25 | Loss: 0.00195216
Iteration 16/25 | Loss: 0.00195216
Iteration 17/25 | Loss: 0.00195216
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019521643407642841, 0.0019521643407642841, 0.0019521643407642841, 0.0019521643407642841, 0.0019521643407642841]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019521643407642841

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195216
Iteration 2/1000 | Loss: 0.00020191
Iteration 3/1000 | Loss: 0.00014647
Iteration 4/1000 | Loss: 0.00406338
Iteration 5/1000 | Loss: 0.00061652
Iteration 6/1000 | Loss: 0.00433443
Iteration 7/1000 | Loss: 0.00123853
Iteration 8/1000 | Loss: 0.00207552
Iteration 9/1000 | Loss: 0.00020915
Iteration 10/1000 | Loss: 0.00039048
Iteration 11/1000 | Loss: 0.00186418
Iteration 12/1000 | Loss: 0.00303388
Iteration 13/1000 | Loss: 0.00165251
Iteration 14/1000 | Loss: 0.00347865
Iteration 15/1000 | Loss: 0.00162754
Iteration 16/1000 | Loss: 0.00113324
Iteration 17/1000 | Loss: 0.00037190
Iteration 18/1000 | Loss: 0.00037926
Iteration 19/1000 | Loss: 0.00032420
Iteration 20/1000 | Loss: 0.00013218
Iteration 21/1000 | Loss: 0.00101232
Iteration 22/1000 | Loss: 0.00015705
Iteration 23/1000 | Loss: 0.00061130
Iteration 24/1000 | Loss: 0.00023419
Iteration 25/1000 | Loss: 0.00012404
Iteration 26/1000 | Loss: 0.00011504
Iteration 27/1000 | Loss: 0.00102235
Iteration 28/1000 | Loss: 0.00011641
Iteration 29/1000 | Loss: 0.00010919
Iteration 30/1000 | Loss: 0.00010041
Iteration 31/1000 | Loss: 0.00009392
Iteration 32/1000 | Loss: 0.00009129
Iteration 33/1000 | Loss: 0.00066615
Iteration 34/1000 | Loss: 0.00078426
Iteration 35/1000 | Loss: 0.00039979
Iteration 36/1000 | Loss: 0.00010043
Iteration 37/1000 | Loss: 0.00009238
Iteration 38/1000 | Loss: 0.00009056
Iteration 39/1000 | Loss: 0.00008979
Iteration 40/1000 | Loss: 0.00008909
Iteration 41/1000 | Loss: 0.00008835
Iteration 42/1000 | Loss: 0.00008774
Iteration 43/1000 | Loss: 0.00037383
Iteration 44/1000 | Loss: 0.00121817
Iteration 45/1000 | Loss: 0.00027464
Iteration 46/1000 | Loss: 0.00021119
Iteration 47/1000 | Loss: 0.00029578
Iteration 48/1000 | Loss: 0.00008824
Iteration 49/1000 | Loss: 0.00008583
Iteration 50/1000 | Loss: 0.00008419
Iteration 51/1000 | Loss: 0.00008288
Iteration 52/1000 | Loss: 0.00008217
Iteration 53/1000 | Loss: 0.00008149
Iteration 54/1000 | Loss: 0.00008074
Iteration 55/1000 | Loss: 0.00007961
Iteration 56/1000 | Loss: 0.00007873
Iteration 57/1000 | Loss: 0.00007761
Iteration 58/1000 | Loss: 0.00007645
Iteration 59/1000 | Loss: 0.00024015
Iteration 60/1000 | Loss: 0.00042186
Iteration 61/1000 | Loss: 0.00027112
Iteration 62/1000 | Loss: 0.00008753
Iteration 63/1000 | Loss: 0.00007785
Iteration 64/1000 | Loss: 0.00007156
Iteration 65/1000 | Loss: 0.00006584
Iteration 66/1000 | Loss: 0.00006279
Iteration 67/1000 | Loss: 0.00006158
Iteration 68/1000 | Loss: 0.00006035
Iteration 69/1000 | Loss: 0.00005933
Iteration 70/1000 | Loss: 0.00005880
Iteration 71/1000 | Loss: 0.00005828
Iteration 72/1000 | Loss: 0.00005789
Iteration 73/1000 | Loss: 0.00005765
Iteration 74/1000 | Loss: 0.00005745
Iteration 75/1000 | Loss: 0.00005738
Iteration 76/1000 | Loss: 0.00005727
Iteration 77/1000 | Loss: 0.00005726
Iteration 78/1000 | Loss: 0.00005726
Iteration 79/1000 | Loss: 0.00005725
Iteration 80/1000 | Loss: 0.00005725
Iteration 81/1000 | Loss: 0.00005722
Iteration 82/1000 | Loss: 0.00005722
Iteration 83/1000 | Loss: 0.00005713
Iteration 84/1000 | Loss: 0.00005707
Iteration 85/1000 | Loss: 0.00005700
Iteration 86/1000 | Loss: 0.00005696
Iteration 87/1000 | Loss: 0.00005693
Iteration 88/1000 | Loss: 0.00005687
Iteration 89/1000 | Loss: 0.00005686
Iteration 90/1000 | Loss: 0.00005686
Iteration 91/1000 | Loss: 0.00005685
Iteration 92/1000 | Loss: 0.00005685
Iteration 93/1000 | Loss: 0.00005685
Iteration 94/1000 | Loss: 0.00005685
Iteration 95/1000 | Loss: 0.00005685
Iteration 96/1000 | Loss: 0.00005684
Iteration 97/1000 | Loss: 0.00005684
Iteration 98/1000 | Loss: 0.00005684
Iteration 99/1000 | Loss: 0.00005684
Iteration 100/1000 | Loss: 0.00005684
Iteration 101/1000 | Loss: 0.00005683
Iteration 102/1000 | Loss: 0.00005683
Iteration 103/1000 | Loss: 0.00005683
Iteration 104/1000 | Loss: 0.00005683
Iteration 105/1000 | Loss: 0.00005682
Iteration 106/1000 | Loss: 0.00005682
Iteration 107/1000 | Loss: 0.00005682
Iteration 108/1000 | Loss: 0.00005682
Iteration 109/1000 | Loss: 0.00005681
Iteration 110/1000 | Loss: 0.00005680
Iteration 111/1000 | Loss: 0.00005679
Iteration 112/1000 | Loss: 0.00005679
Iteration 113/1000 | Loss: 0.00005678
Iteration 114/1000 | Loss: 0.00005678
Iteration 115/1000 | Loss: 0.00005678
Iteration 116/1000 | Loss: 0.00005678
Iteration 117/1000 | Loss: 0.00005677
Iteration 118/1000 | Loss: 0.00005677
Iteration 119/1000 | Loss: 0.00005677
Iteration 120/1000 | Loss: 0.00005676
Iteration 121/1000 | Loss: 0.00005676
Iteration 122/1000 | Loss: 0.00005676
Iteration 123/1000 | Loss: 0.00005676
Iteration 124/1000 | Loss: 0.00005675
Iteration 125/1000 | Loss: 0.00005675
Iteration 126/1000 | Loss: 0.00005675
Iteration 127/1000 | Loss: 0.00005675
Iteration 128/1000 | Loss: 0.00005675
Iteration 129/1000 | Loss: 0.00005674
Iteration 130/1000 | Loss: 0.00005674
Iteration 131/1000 | Loss: 0.00005674
Iteration 132/1000 | Loss: 0.00005674
Iteration 133/1000 | Loss: 0.00005674
Iteration 134/1000 | Loss: 0.00005673
Iteration 135/1000 | Loss: 0.00005673
Iteration 136/1000 | Loss: 0.00005673
Iteration 137/1000 | Loss: 0.00005673
Iteration 138/1000 | Loss: 0.00005673
Iteration 139/1000 | Loss: 0.00005673
Iteration 140/1000 | Loss: 0.00005673
Iteration 141/1000 | Loss: 0.00005672
Iteration 142/1000 | Loss: 0.00005672
Iteration 143/1000 | Loss: 0.00005672
Iteration 144/1000 | Loss: 0.00005672
Iteration 145/1000 | Loss: 0.00005672
Iteration 146/1000 | Loss: 0.00005672
Iteration 147/1000 | Loss: 0.00005671
Iteration 148/1000 | Loss: 0.00005671
Iteration 149/1000 | Loss: 0.00005671
Iteration 150/1000 | Loss: 0.00005671
Iteration 151/1000 | Loss: 0.00005671
Iteration 152/1000 | Loss: 0.00005671
Iteration 153/1000 | Loss: 0.00005670
Iteration 154/1000 | Loss: 0.00005670
Iteration 155/1000 | Loss: 0.00005670
Iteration 156/1000 | Loss: 0.00005670
Iteration 157/1000 | Loss: 0.00005670
Iteration 158/1000 | Loss: 0.00005670
Iteration 159/1000 | Loss: 0.00005670
Iteration 160/1000 | Loss: 0.00005670
Iteration 161/1000 | Loss: 0.00005670
Iteration 162/1000 | Loss: 0.00005670
Iteration 163/1000 | Loss: 0.00005669
Iteration 164/1000 | Loss: 0.00005669
Iteration 165/1000 | Loss: 0.00005669
Iteration 166/1000 | Loss: 0.00005669
Iteration 167/1000 | Loss: 0.00005669
Iteration 168/1000 | Loss: 0.00005669
Iteration 169/1000 | Loss: 0.00005669
Iteration 170/1000 | Loss: 0.00005669
Iteration 171/1000 | Loss: 0.00005669
Iteration 172/1000 | Loss: 0.00005669
Iteration 173/1000 | Loss: 0.00005669
Iteration 174/1000 | Loss: 0.00005669
Iteration 175/1000 | Loss: 0.00005669
Iteration 176/1000 | Loss: 0.00005669
Iteration 177/1000 | Loss: 0.00005669
Iteration 178/1000 | Loss: 0.00005669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [5.669165693689138e-05, 5.669165693689138e-05, 5.669165693689138e-05, 5.669165693689138e-05, 5.669165693689138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.669165693689138e-05

Optimization complete. Final v2v error: 5.092772960662842 mm

Highest mean error: 11.506428718566895 mm for frame 79

Lowest mean error: 4.313449382781982 mm for frame 110

Saving results

Total time: 158.31037735939026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475788
Iteration 2/25 | Loss: 0.00128136
Iteration 3/25 | Loss: 0.00121261
Iteration 4/25 | Loss: 0.00119970
Iteration 5/25 | Loss: 0.00119521
Iteration 6/25 | Loss: 0.00119414
Iteration 7/25 | Loss: 0.00119414
Iteration 8/25 | Loss: 0.00119414
Iteration 9/25 | Loss: 0.00119414
Iteration 10/25 | Loss: 0.00119414
Iteration 11/25 | Loss: 0.00119414
Iteration 12/25 | Loss: 0.00119414
Iteration 13/25 | Loss: 0.00119414
Iteration 14/25 | Loss: 0.00119414
Iteration 15/25 | Loss: 0.00119414
Iteration 16/25 | Loss: 0.00119414
Iteration 17/25 | Loss: 0.00119414
Iteration 18/25 | Loss: 0.00119414
Iteration 19/25 | Loss: 0.00119414
Iteration 20/25 | Loss: 0.00119414
Iteration 21/25 | Loss: 0.00119414
Iteration 22/25 | Loss: 0.00119414
Iteration 23/25 | Loss: 0.00119414
Iteration 24/25 | Loss: 0.00119414
Iteration 25/25 | Loss: 0.00119414

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.29565430
Iteration 2/25 | Loss: 0.00093509
Iteration 3/25 | Loss: 0.00093509
Iteration 4/25 | Loss: 0.00093509
Iteration 5/25 | Loss: 0.00093508
Iteration 6/25 | Loss: 0.00093508
Iteration 7/25 | Loss: 0.00093508
Iteration 8/25 | Loss: 0.00093508
Iteration 9/25 | Loss: 0.00093508
Iteration 10/25 | Loss: 0.00093508
Iteration 11/25 | Loss: 0.00093508
Iteration 12/25 | Loss: 0.00093508
Iteration 13/25 | Loss: 0.00093508
Iteration 14/25 | Loss: 0.00093508
Iteration 15/25 | Loss: 0.00093508
Iteration 16/25 | Loss: 0.00093508
Iteration 17/25 | Loss: 0.00093508
Iteration 18/25 | Loss: 0.00093508
Iteration 19/25 | Loss: 0.00093508
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009350829641334713, 0.0009350829641334713, 0.0009350829641334713, 0.0009350829641334713, 0.0009350829641334713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009350829641334713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093508
Iteration 2/1000 | Loss: 0.00002792
Iteration 3/1000 | Loss: 0.00001879
Iteration 4/1000 | Loss: 0.00001643
Iteration 5/1000 | Loss: 0.00001541
Iteration 6/1000 | Loss: 0.00001467
Iteration 7/1000 | Loss: 0.00001419
Iteration 8/1000 | Loss: 0.00001377
Iteration 9/1000 | Loss: 0.00001347
Iteration 10/1000 | Loss: 0.00001325
Iteration 11/1000 | Loss: 0.00001300
Iteration 12/1000 | Loss: 0.00001282
Iteration 13/1000 | Loss: 0.00001280
Iteration 14/1000 | Loss: 0.00001279
Iteration 15/1000 | Loss: 0.00001269
Iteration 16/1000 | Loss: 0.00001265
Iteration 17/1000 | Loss: 0.00001261
Iteration 18/1000 | Loss: 0.00001256
Iteration 19/1000 | Loss: 0.00001249
Iteration 20/1000 | Loss: 0.00001249
Iteration 21/1000 | Loss: 0.00001247
Iteration 22/1000 | Loss: 0.00001247
Iteration 23/1000 | Loss: 0.00001246
Iteration 24/1000 | Loss: 0.00001240
Iteration 25/1000 | Loss: 0.00001240
Iteration 26/1000 | Loss: 0.00001239
Iteration 27/1000 | Loss: 0.00001239
Iteration 28/1000 | Loss: 0.00001238
Iteration 29/1000 | Loss: 0.00001237
Iteration 30/1000 | Loss: 0.00001237
Iteration 31/1000 | Loss: 0.00001236
Iteration 32/1000 | Loss: 0.00001236
Iteration 33/1000 | Loss: 0.00001235
Iteration 34/1000 | Loss: 0.00001235
Iteration 35/1000 | Loss: 0.00001235
Iteration 36/1000 | Loss: 0.00001235
Iteration 37/1000 | Loss: 0.00001235
Iteration 38/1000 | Loss: 0.00001235
Iteration 39/1000 | Loss: 0.00001231
Iteration 40/1000 | Loss: 0.00001231
Iteration 41/1000 | Loss: 0.00001230
Iteration 42/1000 | Loss: 0.00001230
Iteration 43/1000 | Loss: 0.00001230
Iteration 44/1000 | Loss: 0.00001229
Iteration 45/1000 | Loss: 0.00001228
Iteration 46/1000 | Loss: 0.00001228
Iteration 47/1000 | Loss: 0.00001228
Iteration 48/1000 | Loss: 0.00001227
Iteration 49/1000 | Loss: 0.00001227
Iteration 50/1000 | Loss: 0.00001227
Iteration 51/1000 | Loss: 0.00001226
Iteration 52/1000 | Loss: 0.00001226
Iteration 53/1000 | Loss: 0.00001226
Iteration 54/1000 | Loss: 0.00001225
Iteration 55/1000 | Loss: 0.00001225
Iteration 56/1000 | Loss: 0.00001225
Iteration 57/1000 | Loss: 0.00001225
Iteration 58/1000 | Loss: 0.00001224
Iteration 59/1000 | Loss: 0.00001224
Iteration 60/1000 | Loss: 0.00001224
Iteration 61/1000 | Loss: 0.00001223
Iteration 62/1000 | Loss: 0.00001223
Iteration 63/1000 | Loss: 0.00001222
Iteration 64/1000 | Loss: 0.00001222
Iteration 65/1000 | Loss: 0.00001222
Iteration 66/1000 | Loss: 0.00001222
Iteration 67/1000 | Loss: 0.00001222
Iteration 68/1000 | Loss: 0.00001222
Iteration 69/1000 | Loss: 0.00001222
Iteration 70/1000 | Loss: 0.00001222
Iteration 71/1000 | Loss: 0.00001222
Iteration 72/1000 | Loss: 0.00001221
Iteration 73/1000 | Loss: 0.00001221
Iteration 74/1000 | Loss: 0.00001221
Iteration 75/1000 | Loss: 0.00001221
Iteration 76/1000 | Loss: 0.00001221
Iteration 77/1000 | Loss: 0.00001221
Iteration 78/1000 | Loss: 0.00001220
Iteration 79/1000 | Loss: 0.00001220
Iteration 80/1000 | Loss: 0.00001219
Iteration 81/1000 | Loss: 0.00001219
Iteration 82/1000 | Loss: 0.00001219
Iteration 83/1000 | Loss: 0.00001218
Iteration 84/1000 | Loss: 0.00001218
Iteration 85/1000 | Loss: 0.00001218
Iteration 86/1000 | Loss: 0.00001217
Iteration 87/1000 | Loss: 0.00001217
Iteration 88/1000 | Loss: 0.00001217
Iteration 89/1000 | Loss: 0.00001217
Iteration 90/1000 | Loss: 0.00001217
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001217
Iteration 93/1000 | Loss: 0.00001217
Iteration 94/1000 | Loss: 0.00001216
Iteration 95/1000 | Loss: 0.00001216
Iteration 96/1000 | Loss: 0.00001216
Iteration 97/1000 | Loss: 0.00001215
Iteration 98/1000 | Loss: 0.00001215
Iteration 99/1000 | Loss: 0.00001215
Iteration 100/1000 | Loss: 0.00001215
Iteration 101/1000 | Loss: 0.00001215
Iteration 102/1000 | Loss: 0.00001215
Iteration 103/1000 | Loss: 0.00001215
Iteration 104/1000 | Loss: 0.00001214
Iteration 105/1000 | Loss: 0.00001214
Iteration 106/1000 | Loss: 0.00001214
Iteration 107/1000 | Loss: 0.00001214
Iteration 108/1000 | Loss: 0.00001214
Iteration 109/1000 | Loss: 0.00001213
Iteration 110/1000 | Loss: 0.00001213
Iteration 111/1000 | Loss: 0.00001213
Iteration 112/1000 | Loss: 0.00001213
Iteration 113/1000 | Loss: 0.00001213
Iteration 114/1000 | Loss: 0.00001213
Iteration 115/1000 | Loss: 0.00001213
Iteration 116/1000 | Loss: 0.00001213
Iteration 117/1000 | Loss: 0.00001212
Iteration 118/1000 | Loss: 0.00001212
Iteration 119/1000 | Loss: 0.00001212
Iteration 120/1000 | Loss: 0.00001212
Iteration 121/1000 | Loss: 0.00001212
Iteration 122/1000 | Loss: 0.00001212
Iteration 123/1000 | Loss: 0.00001212
Iteration 124/1000 | Loss: 0.00001212
Iteration 125/1000 | Loss: 0.00001212
Iteration 126/1000 | Loss: 0.00001212
Iteration 127/1000 | Loss: 0.00001212
Iteration 128/1000 | Loss: 0.00001212
Iteration 129/1000 | Loss: 0.00001212
Iteration 130/1000 | Loss: 0.00001212
Iteration 131/1000 | Loss: 0.00001212
Iteration 132/1000 | Loss: 0.00001212
Iteration 133/1000 | Loss: 0.00001212
Iteration 134/1000 | Loss: 0.00001212
Iteration 135/1000 | Loss: 0.00001212
Iteration 136/1000 | Loss: 0.00001212
Iteration 137/1000 | Loss: 0.00001212
Iteration 138/1000 | Loss: 0.00001212
Iteration 139/1000 | Loss: 0.00001212
Iteration 140/1000 | Loss: 0.00001212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.2123825399612542e-05, 1.2123825399612542e-05, 1.2123825399612542e-05, 1.2123825399612542e-05, 1.2123825399612542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2123825399612542e-05

Optimization complete. Final v2v error: 3.015333890914917 mm

Highest mean error: 3.300053834915161 mm for frame 109

Lowest mean error: 2.826169490814209 mm for frame 9

Saving results

Total time: 39.86461567878723
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374604
Iteration 2/25 | Loss: 0.00135298
Iteration 3/25 | Loss: 0.00121916
Iteration 4/25 | Loss: 0.00120341
Iteration 5/25 | Loss: 0.00119881
Iteration 6/25 | Loss: 0.00119821
Iteration 7/25 | Loss: 0.00119821
Iteration 8/25 | Loss: 0.00119821
Iteration 9/25 | Loss: 0.00119821
Iteration 10/25 | Loss: 0.00119821
Iteration 11/25 | Loss: 0.00119821
Iteration 12/25 | Loss: 0.00119821
Iteration 13/25 | Loss: 0.00119821
Iteration 14/25 | Loss: 0.00119821
Iteration 15/25 | Loss: 0.00119821
Iteration 16/25 | Loss: 0.00119821
Iteration 17/25 | Loss: 0.00119821
Iteration 18/25 | Loss: 0.00119821
Iteration 19/25 | Loss: 0.00119821
Iteration 20/25 | Loss: 0.00119821
Iteration 21/25 | Loss: 0.00119821
Iteration 22/25 | Loss: 0.00119821
Iteration 23/25 | Loss: 0.00119821
Iteration 24/25 | Loss: 0.00119821
Iteration 25/25 | Loss: 0.00119821

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77239895
Iteration 2/25 | Loss: 0.00108530
Iteration 3/25 | Loss: 0.00108530
Iteration 4/25 | Loss: 0.00108530
Iteration 5/25 | Loss: 0.00108530
Iteration 6/25 | Loss: 0.00108530
Iteration 7/25 | Loss: 0.00108530
Iteration 8/25 | Loss: 0.00108530
Iteration 9/25 | Loss: 0.00108530
Iteration 10/25 | Loss: 0.00108530
Iteration 11/25 | Loss: 0.00108530
Iteration 12/25 | Loss: 0.00108530
Iteration 13/25 | Loss: 0.00108530
Iteration 14/25 | Loss: 0.00108530
Iteration 15/25 | Loss: 0.00108530
Iteration 16/25 | Loss: 0.00108530
Iteration 17/25 | Loss: 0.00108530
Iteration 18/25 | Loss: 0.00108530
Iteration 19/25 | Loss: 0.00108530
Iteration 20/25 | Loss: 0.00108530
Iteration 21/25 | Loss: 0.00108530
Iteration 22/25 | Loss: 0.00108530
Iteration 23/25 | Loss: 0.00108530
Iteration 24/25 | Loss: 0.00108530
Iteration 25/25 | Loss: 0.00108530
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010852955747395754, 0.0010852955747395754, 0.0010852955747395754, 0.0010852955747395754, 0.0010852955747395754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010852955747395754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108530
Iteration 2/1000 | Loss: 0.00002575
Iteration 3/1000 | Loss: 0.00001773
Iteration 4/1000 | Loss: 0.00001617
Iteration 5/1000 | Loss: 0.00001493
Iteration 6/1000 | Loss: 0.00001414
Iteration 7/1000 | Loss: 0.00001355
Iteration 8/1000 | Loss: 0.00001323
Iteration 9/1000 | Loss: 0.00001291
Iteration 10/1000 | Loss: 0.00001258
Iteration 11/1000 | Loss: 0.00001237
Iteration 12/1000 | Loss: 0.00001217
Iteration 13/1000 | Loss: 0.00001204
Iteration 14/1000 | Loss: 0.00001192
Iteration 15/1000 | Loss: 0.00001187
Iteration 16/1000 | Loss: 0.00001180
Iteration 17/1000 | Loss: 0.00001180
Iteration 18/1000 | Loss: 0.00001179
Iteration 19/1000 | Loss: 0.00001175
Iteration 20/1000 | Loss: 0.00001172
Iteration 21/1000 | Loss: 0.00001168
Iteration 22/1000 | Loss: 0.00001168
Iteration 23/1000 | Loss: 0.00001166
Iteration 24/1000 | Loss: 0.00001166
Iteration 25/1000 | Loss: 0.00001165
Iteration 26/1000 | Loss: 0.00001165
Iteration 27/1000 | Loss: 0.00001165
Iteration 28/1000 | Loss: 0.00001164
Iteration 29/1000 | Loss: 0.00001164
Iteration 30/1000 | Loss: 0.00001164
Iteration 31/1000 | Loss: 0.00001164
Iteration 32/1000 | Loss: 0.00001164
Iteration 33/1000 | Loss: 0.00001164
Iteration 34/1000 | Loss: 0.00001164
Iteration 35/1000 | Loss: 0.00001164
Iteration 36/1000 | Loss: 0.00001164
Iteration 37/1000 | Loss: 0.00001164
Iteration 38/1000 | Loss: 0.00001163
Iteration 39/1000 | Loss: 0.00001163
Iteration 40/1000 | Loss: 0.00001163
Iteration 41/1000 | Loss: 0.00001163
Iteration 42/1000 | Loss: 0.00001163
Iteration 43/1000 | Loss: 0.00001163
Iteration 44/1000 | Loss: 0.00001163
Iteration 45/1000 | Loss: 0.00001162
Iteration 46/1000 | Loss: 0.00001162
Iteration 47/1000 | Loss: 0.00001161
Iteration 48/1000 | Loss: 0.00001161
Iteration 49/1000 | Loss: 0.00001161
Iteration 50/1000 | Loss: 0.00001160
Iteration 51/1000 | Loss: 0.00001160
Iteration 52/1000 | Loss: 0.00001160
Iteration 53/1000 | Loss: 0.00001159
Iteration 54/1000 | Loss: 0.00001159
Iteration 55/1000 | Loss: 0.00001159
Iteration 56/1000 | Loss: 0.00001159
Iteration 57/1000 | Loss: 0.00001159
Iteration 58/1000 | Loss: 0.00001159
Iteration 59/1000 | Loss: 0.00001159
Iteration 60/1000 | Loss: 0.00001158
Iteration 61/1000 | Loss: 0.00001158
Iteration 62/1000 | Loss: 0.00001158
Iteration 63/1000 | Loss: 0.00001158
Iteration 64/1000 | Loss: 0.00001158
Iteration 65/1000 | Loss: 0.00001158
Iteration 66/1000 | Loss: 0.00001158
Iteration 67/1000 | Loss: 0.00001157
Iteration 68/1000 | Loss: 0.00001157
Iteration 69/1000 | Loss: 0.00001157
Iteration 70/1000 | Loss: 0.00001157
Iteration 71/1000 | Loss: 0.00001157
Iteration 72/1000 | Loss: 0.00001157
Iteration 73/1000 | Loss: 0.00001157
Iteration 74/1000 | Loss: 0.00001157
Iteration 75/1000 | Loss: 0.00001157
Iteration 76/1000 | Loss: 0.00001156
Iteration 77/1000 | Loss: 0.00001156
Iteration 78/1000 | Loss: 0.00001156
Iteration 79/1000 | Loss: 0.00001156
Iteration 80/1000 | Loss: 0.00001156
Iteration 81/1000 | Loss: 0.00001156
Iteration 82/1000 | Loss: 0.00001156
Iteration 83/1000 | Loss: 0.00001156
Iteration 84/1000 | Loss: 0.00001155
Iteration 85/1000 | Loss: 0.00001155
Iteration 86/1000 | Loss: 0.00001155
Iteration 87/1000 | Loss: 0.00001155
Iteration 88/1000 | Loss: 0.00001155
Iteration 89/1000 | Loss: 0.00001155
Iteration 90/1000 | Loss: 0.00001154
Iteration 91/1000 | Loss: 0.00001154
Iteration 92/1000 | Loss: 0.00001154
Iteration 93/1000 | Loss: 0.00001154
Iteration 94/1000 | Loss: 0.00001153
Iteration 95/1000 | Loss: 0.00001153
Iteration 96/1000 | Loss: 0.00001153
Iteration 97/1000 | Loss: 0.00001153
Iteration 98/1000 | Loss: 0.00001153
Iteration 99/1000 | Loss: 0.00001153
Iteration 100/1000 | Loss: 0.00001153
Iteration 101/1000 | Loss: 0.00001153
Iteration 102/1000 | Loss: 0.00001153
Iteration 103/1000 | Loss: 0.00001153
Iteration 104/1000 | Loss: 0.00001153
Iteration 105/1000 | Loss: 0.00001152
Iteration 106/1000 | Loss: 0.00001152
Iteration 107/1000 | Loss: 0.00001152
Iteration 108/1000 | Loss: 0.00001152
Iteration 109/1000 | Loss: 0.00001152
Iteration 110/1000 | Loss: 0.00001152
Iteration 111/1000 | Loss: 0.00001152
Iteration 112/1000 | Loss: 0.00001152
Iteration 113/1000 | Loss: 0.00001152
Iteration 114/1000 | Loss: 0.00001151
Iteration 115/1000 | Loss: 0.00001151
Iteration 116/1000 | Loss: 0.00001151
Iteration 117/1000 | Loss: 0.00001151
Iteration 118/1000 | Loss: 0.00001151
Iteration 119/1000 | Loss: 0.00001151
Iteration 120/1000 | Loss: 0.00001150
Iteration 121/1000 | Loss: 0.00001150
Iteration 122/1000 | Loss: 0.00001150
Iteration 123/1000 | Loss: 0.00001150
Iteration 124/1000 | Loss: 0.00001150
Iteration 125/1000 | Loss: 0.00001150
Iteration 126/1000 | Loss: 0.00001150
Iteration 127/1000 | Loss: 0.00001149
Iteration 128/1000 | Loss: 0.00001149
Iteration 129/1000 | Loss: 0.00001149
Iteration 130/1000 | Loss: 0.00001149
Iteration 131/1000 | Loss: 0.00001149
Iteration 132/1000 | Loss: 0.00001148
Iteration 133/1000 | Loss: 0.00001148
Iteration 134/1000 | Loss: 0.00001148
Iteration 135/1000 | Loss: 0.00001148
Iteration 136/1000 | Loss: 0.00001147
Iteration 137/1000 | Loss: 0.00001147
Iteration 138/1000 | Loss: 0.00001147
Iteration 139/1000 | Loss: 0.00001147
Iteration 140/1000 | Loss: 0.00001147
Iteration 141/1000 | Loss: 0.00001146
Iteration 142/1000 | Loss: 0.00001146
Iteration 143/1000 | Loss: 0.00001146
Iteration 144/1000 | Loss: 0.00001145
Iteration 145/1000 | Loss: 0.00001145
Iteration 146/1000 | Loss: 0.00001145
Iteration 147/1000 | Loss: 0.00001145
Iteration 148/1000 | Loss: 0.00001145
Iteration 149/1000 | Loss: 0.00001145
Iteration 150/1000 | Loss: 0.00001144
Iteration 151/1000 | Loss: 0.00001144
Iteration 152/1000 | Loss: 0.00001144
Iteration 153/1000 | Loss: 0.00001144
Iteration 154/1000 | Loss: 0.00001143
Iteration 155/1000 | Loss: 0.00001143
Iteration 156/1000 | Loss: 0.00001143
Iteration 157/1000 | Loss: 0.00001143
Iteration 158/1000 | Loss: 0.00001143
Iteration 159/1000 | Loss: 0.00001142
Iteration 160/1000 | Loss: 0.00001142
Iteration 161/1000 | Loss: 0.00001142
Iteration 162/1000 | Loss: 0.00001142
Iteration 163/1000 | Loss: 0.00001142
Iteration 164/1000 | Loss: 0.00001142
Iteration 165/1000 | Loss: 0.00001142
Iteration 166/1000 | Loss: 0.00001142
Iteration 167/1000 | Loss: 0.00001142
Iteration 168/1000 | Loss: 0.00001142
Iteration 169/1000 | Loss: 0.00001142
Iteration 170/1000 | Loss: 0.00001142
Iteration 171/1000 | Loss: 0.00001142
Iteration 172/1000 | Loss: 0.00001142
Iteration 173/1000 | Loss: 0.00001142
Iteration 174/1000 | Loss: 0.00001141
Iteration 175/1000 | Loss: 0.00001141
Iteration 176/1000 | Loss: 0.00001141
Iteration 177/1000 | Loss: 0.00001141
Iteration 178/1000 | Loss: 0.00001141
Iteration 179/1000 | Loss: 0.00001141
Iteration 180/1000 | Loss: 0.00001141
Iteration 181/1000 | Loss: 0.00001141
Iteration 182/1000 | Loss: 0.00001141
Iteration 183/1000 | Loss: 0.00001141
Iteration 184/1000 | Loss: 0.00001141
Iteration 185/1000 | Loss: 0.00001141
Iteration 186/1000 | Loss: 0.00001141
Iteration 187/1000 | Loss: 0.00001141
Iteration 188/1000 | Loss: 0.00001141
Iteration 189/1000 | Loss: 0.00001141
Iteration 190/1000 | Loss: 0.00001141
Iteration 191/1000 | Loss: 0.00001141
Iteration 192/1000 | Loss: 0.00001140
Iteration 193/1000 | Loss: 0.00001140
Iteration 194/1000 | Loss: 0.00001140
Iteration 195/1000 | Loss: 0.00001140
Iteration 196/1000 | Loss: 0.00001140
Iteration 197/1000 | Loss: 0.00001140
Iteration 198/1000 | Loss: 0.00001140
Iteration 199/1000 | Loss: 0.00001140
Iteration 200/1000 | Loss: 0.00001140
Iteration 201/1000 | Loss: 0.00001140
Iteration 202/1000 | Loss: 0.00001140
Iteration 203/1000 | Loss: 0.00001140
Iteration 204/1000 | Loss: 0.00001140
Iteration 205/1000 | Loss: 0.00001140
Iteration 206/1000 | Loss: 0.00001140
Iteration 207/1000 | Loss: 0.00001140
Iteration 208/1000 | Loss: 0.00001140
Iteration 209/1000 | Loss: 0.00001140
Iteration 210/1000 | Loss: 0.00001140
Iteration 211/1000 | Loss: 0.00001140
Iteration 212/1000 | Loss: 0.00001140
Iteration 213/1000 | Loss: 0.00001140
Iteration 214/1000 | Loss: 0.00001140
Iteration 215/1000 | Loss: 0.00001140
Iteration 216/1000 | Loss: 0.00001140
Iteration 217/1000 | Loss: 0.00001140
Iteration 218/1000 | Loss: 0.00001140
Iteration 219/1000 | Loss: 0.00001140
Iteration 220/1000 | Loss: 0.00001140
Iteration 221/1000 | Loss: 0.00001140
Iteration 222/1000 | Loss: 0.00001140
Iteration 223/1000 | Loss: 0.00001140
Iteration 224/1000 | Loss: 0.00001140
Iteration 225/1000 | Loss: 0.00001140
Iteration 226/1000 | Loss: 0.00001140
Iteration 227/1000 | Loss: 0.00001140
Iteration 228/1000 | Loss: 0.00001140
Iteration 229/1000 | Loss: 0.00001140
Iteration 230/1000 | Loss: 0.00001140
Iteration 231/1000 | Loss: 0.00001140
Iteration 232/1000 | Loss: 0.00001140
Iteration 233/1000 | Loss: 0.00001140
Iteration 234/1000 | Loss: 0.00001140
Iteration 235/1000 | Loss: 0.00001140
Iteration 236/1000 | Loss: 0.00001140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.1399613867979497e-05, 1.1399613867979497e-05, 1.1399613867979497e-05, 1.1399613867979497e-05, 1.1399613867979497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1399613867979497e-05

Optimization complete. Final v2v error: 2.9197838306427 mm

Highest mean error: 3.3863654136657715 mm for frame 75

Lowest mean error: 2.4841136932373047 mm for frame 10

Saving results

Total time: 49.39226174354553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036871
Iteration 2/25 | Loss: 0.01036871
Iteration 3/25 | Loss: 0.01036871
Iteration 4/25 | Loss: 0.01036870
Iteration 5/25 | Loss: 0.01036870
Iteration 6/25 | Loss: 0.01036870
Iteration 7/25 | Loss: 0.01036870
Iteration 8/25 | Loss: 0.01036870
Iteration 9/25 | Loss: 0.01036870
Iteration 10/25 | Loss: 0.01036870
Iteration 11/25 | Loss: 0.01036870
Iteration 12/25 | Loss: 0.01036870
Iteration 13/25 | Loss: 0.01036870
Iteration 14/25 | Loss: 0.01036870
Iteration 15/25 | Loss: 0.01036869
Iteration 16/25 | Loss: 0.01036869
Iteration 17/25 | Loss: 0.01036869
Iteration 18/25 | Loss: 0.01036869
Iteration 19/25 | Loss: 0.01036869
Iteration 20/25 | Loss: 0.01036869
Iteration 21/25 | Loss: 0.01036869
Iteration 22/25 | Loss: 0.01036869
Iteration 23/25 | Loss: 0.01036869
Iteration 24/25 | Loss: 0.01036869
Iteration 25/25 | Loss: 0.01036869

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46923995
Iteration 2/25 | Loss: 0.12941238
Iteration 3/25 | Loss: 0.12490980
Iteration 4/25 | Loss: 0.12107366
Iteration 5/25 | Loss: 0.12140676
Iteration 6/25 | Loss: 0.12104557
Iteration 7/25 | Loss: 0.12081366
Iteration 8/25 | Loss: 0.12081366
Iteration 9/25 | Loss: 0.12081366
Iteration 10/25 | Loss: 0.12081366
Iteration 11/25 | Loss: 0.12081366
Iteration 12/25 | Loss: 0.12081366
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.12081366032361984, 0.12081366032361984, 0.12081366032361984, 0.12081366032361984, 0.12081366032361984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12081366032361984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12081366
Iteration 2/1000 | Loss: 0.00063505
Iteration 3/1000 | Loss: 0.00015703
Iteration 4/1000 | Loss: 0.00007399
Iteration 5/1000 | Loss: 0.00004268
Iteration 6/1000 | Loss: 0.00003011
Iteration 7/1000 | Loss: 0.00002462
Iteration 8/1000 | Loss: 0.00002082
Iteration 9/1000 | Loss: 0.00001778
Iteration 10/1000 | Loss: 0.00001634
Iteration 11/1000 | Loss: 0.00001489
Iteration 12/1000 | Loss: 0.00001404
Iteration 13/1000 | Loss: 0.00001341
Iteration 14/1000 | Loss: 0.00001283
Iteration 15/1000 | Loss: 0.00001215
Iteration 16/1000 | Loss: 0.00001172
Iteration 17/1000 | Loss: 0.00001141
Iteration 18/1000 | Loss: 0.00001116
Iteration 19/1000 | Loss: 0.00001090
Iteration 20/1000 | Loss: 0.00001066
Iteration 21/1000 | Loss: 0.00001050
Iteration 22/1000 | Loss: 0.00001031
Iteration 23/1000 | Loss: 0.00001020
Iteration 24/1000 | Loss: 0.00001017
Iteration 25/1000 | Loss: 0.00001012
Iteration 26/1000 | Loss: 0.00001010
Iteration 27/1000 | Loss: 0.00001006
Iteration 28/1000 | Loss: 0.00001005
Iteration 29/1000 | Loss: 0.00001000
Iteration 30/1000 | Loss: 0.00000997
Iteration 31/1000 | Loss: 0.00000996
Iteration 32/1000 | Loss: 0.00000996
Iteration 33/1000 | Loss: 0.00000995
Iteration 34/1000 | Loss: 0.00000994
Iteration 35/1000 | Loss: 0.00000991
Iteration 36/1000 | Loss: 0.00000980
Iteration 37/1000 | Loss: 0.00000978
Iteration 38/1000 | Loss: 0.00000972
Iteration 39/1000 | Loss: 0.00000972
Iteration 40/1000 | Loss: 0.00000972
Iteration 41/1000 | Loss: 0.00000971
Iteration 42/1000 | Loss: 0.00000970
Iteration 43/1000 | Loss: 0.00000970
Iteration 44/1000 | Loss: 0.00000969
Iteration 45/1000 | Loss: 0.00000968
Iteration 46/1000 | Loss: 0.00000967
Iteration 47/1000 | Loss: 0.00000964
Iteration 48/1000 | Loss: 0.00000961
Iteration 49/1000 | Loss: 0.00000960
Iteration 50/1000 | Loss: 0.00000960
Iteration 51/1000 | Loss: 0.00000959
Iteration 52/1000 | Loss: 0.00000959
Iteration 53/1000 | Loss: 0.00000959
Iteration 54/1000 | Loss: 0.00000958
Iteration 55/1000 | Loss: 0.00000958
Iteration 56/1000 | Loss: 0.00000953
Iteration 57/1000 | Loss: 0.00000952
Iteration 58/1000 | Loss: 0.00000951
Iteration 59/1000 | Loss: 0.00000951
Iteration 60/1000 | Loss: 0.00000950
Iteration 61/1000 | Loss: 0.00000950
Iteration 62/1000 | Loss: 0.00000949
Iteration 63/1000 | Loss: 0.00000949
Iteration 64/1000 | Loss: 0.00000948
Iteration 65/1000 | Loss: 0.00000948
Iteration 66/1000 | Loss: 0.00000947
Iteration 67/1000 | Loss: 0.00000947
Iteration 68/1000 | Loss: 0.00000947
Iteration 69/1000 | Loss: 0.00000947
Iteration 70/1000 | Loss: 0.00000947
Iteration 71/1000 | Loss: 0.00000946
Iteration 72/1000 | Loss: 0.00000946
Iteration 73/1000 | Loss: 0.00000946
Iteration 74/1000 | Loss: 0.00000945
Iteration 75/1000 | Loss: 0.00000945
Iteration 76/1000 | Loss: 0.00000944
Iteration 77/1000 | Loss: 0.00000944
Iteration 78/1000 | Loss: 0.00000943
Iteration 79/1000 | Loss: 0.00000943
Iteration 80/1000 | Loss: 0.00000943
Iteration 81/1000 | Loss: 0.00000943
Iteration 82/1000 | Loss: 0.00000943
Iteration 83/1000 | Loss: 0.00000942
Iteration 84/1000 | Loss: 0.00000942
Iteration 85/1000 | Loss: 0.00000941
Iteration 86/1000 | Loss: 0.00000941
Iteration 87/1000 | Loss: 0.00000941
Iteration 88/1000 | Loss: 0.00000940
Iteration 89/1000 | Loss: 0.00000940
Iteration 90/1000 | Loss: 0.00000940
Iteration 91/1000 | Loss: 0.00000940
Iteration 92/1000 | Loss: 0.00000940
Iteration 93/1000 | Loss: 0.00000939
Iteration 94/1000 | Loss: 0.00000939
Iteration 95/1000 | Loss: 0.00000939
Iteration 96/1000 | Loss: 0.00000939
Iteration 97/1000 | Loss: 0.00000938
Iteration 98/1000 | Loss: 0.00000938
Iteration 99/1000 | Loss: 0.00000938
Iteration 100/1000 | Loss: 0.00000937
Iteration 101/1000 | Loss: 0.00000937
Iteration 102/1000 | Loss: 0.00000937
Iteration 103/1000 | Loss: 0.00000937
Iteration 104/1000 | Loss: 0.00000936
Iteration 105/1000 | Loss: 0.00000936
Iteration 106/1000 | Loss: 0.00000936
Iteration 107/1000 | Loss: 0.00000936
Iteration 108/1000 | Loss: 0.00000936
Iteration 109/1000 | Loss: 0.00000935
Iteration 110/1000 | Loss: 0.00000935
Iteration 111/1000 | Loss: 0.00000935
Iteration 112/1000 | Loss: 0.00000934
Iteration 113/1000 | Loss: 0.00000934
Iteration 114/1000 | Loss: 0.00000934
Iteration 115/1000 | Loss: 0.00000933
Iteration 116/1000 | Loss: 0.00000933
Iteration 117/1000 | Loss: 0.00000933
Iteration 118/1000 | Loss: 0.00000933
Iteration 119/1000 | Loss: 0.00000933
Iteration 120/1000 | Loss: 0.00000932
Iteration 121/1000 | Loss: 0.00000932
Iteration 122/1000 | Loss: 0.00000932
Iteration 123/1000 | Loss: 0.00000932
Iteration 124/1000 | Loss: 0.00000932
Iteration 125/1000 | Loss: 0.00000932
Iteration 126/1000 | Loss: 0.00000932
Iteration 127/1000 | Loss: 0.00000932
Iteration 128/1000 | Loss: 0.00000932
Iteration 129/1000 | Loss: 0.00000931
Iteration 130/1000 | Loss: 0.00000931
Iteration 131/1000 | Loss: 0.00000931
Iteration 132/1000 | Loss: 0.00000931
Iteration 133/1000 | Loss: 0.00000931
Iteration 134/1000 | Loss: 0.00000931
Iteration 135/1000 | Loss: 0.00000931
Iteration 136/1000 | Loss: 0.00000931
Iteration 137/1000 | Loss: 0.00000931
Iteration 138/1000 | Loss: 0.00000931
Iteration 139/1000 | Loss: 0.00000930
Iteration 140/1000 | Loss: 0.00000930
Iteration 141/1000 | Loss: 0.00000930
Iteration 142/1000 | Loss: 0.00000930
Iteration 143/1000 | Loss: 0.00000929
Iteration 144/1000 | Loss: 0.00000929
Iteration 145/1000 | Loss: 0.00000929
Iteration 146/1000 | Loss: 0.00000929
Iteration 147/1000 | Loss: 0.00000928
Iteration 148/1000 | Loss: 0.00000928
Iteration 149/1000 | Loss: 0.00000928
Iteration 150/1000 | Loss: 0.00000928
Iteration 151/1000 | Loss: 0.00000928
Iteration 152/1000 | Loss: 0.00000928
Iteration 153/1000 | Loss: 0.00000928
Iteration 154/1000 | Loss: 0.00000928
Iteration 155/1000 | Loss: 0.00000928
Iteration 156/1000 | Loss: 0.00000928
Iteration 157/1000 | Loss: 0.00000927
Iteration 158/1000 | Loss: 0.00000927
Iteration 159/1000 | Loss: 0.00000927
Iteration 160/1000 | Loss: 0.00000927
Iteration 161/1000 | Loss: 0.00000927
Iteration 162/1000 | Loss: 0.00000927
Iteration 163/1000 | Loss: 0.00000927
Iteration 164/1000 | Loss: 0.00000927
Iteration 165/1000 | Loss: 0.00000927
Iteration 166/1000 | Loss: 0.00000927
Iteration 167/1000 | Loss: 0.00000927
Iteration 168/1000 | Loss: 0.00000927
Iteration 169/1000 | Loss: 0.00000927
Iteration 170/1000 | Loss: 0.00000927
Iteration 171/1000 | Loss: 0.00000927
Iteration 172/1000 | Loss: 0.00000927
Iteration 173/1000 | Loss: 0.00000927
Iteration 174/1000 | Loss: 0.00000927
Iteration 175/1000 | Loss: 0.00000927
Iteration 176/1000 | Loss: 0.00000926
Iteration 177/1000 | Loss: 0.00000926
Iteration 178/1000 | Loss: 0.00000926
Iteration 179/1000 | Loss: 0.00000926
Iteration 180/1000 | Loss: 0.00000926
Iteration 181/1000 | Loss: 0.00000926
Iteration 182/1000 | Loss: 0.00000926
Iteration 183/1000 | Loss: 0.00000926
Iteration 184/1000 | Loss: 0.00000926
Iteration 185/1000 | Loss: 0.00000926
Iteration 186/1000 | Loss: 0.00000926
Iteration 187/1000 | Loss: 0.00000926
Iteration 188/1000 | Loss: 0.00000926
Iteration 189/1000 | Loss: 0.00000926
Iteration 190/1000 | Loss: 0.00000926
Iteration 191/1000 | Loss: 0.00000926
Iteration 192/1000 | Loss: 0.00000926
Iteration 193/1000 | Loss: 0.00000926
Iteration 194/1000 | Loss: 0.00000926
Iteration 195/1000 | Loss: 0.00000926
Iteration 196/1000 | Loss: 0.00000925
Iteration 197/1000 | Loss: 0.00000925
Iteration 198/1000 | Loss: 0.00000925
Iteration 199/1000 | Loss: 0.00000925
Iteration 200/1000 | Loss: 0.00000925
Iteration 201/1000 | Loss: 0.00000925
Iteration 202/1000 | Loss: 0.00000925
Iteration 203/1000 | Loss: 0.00000925
Iteration 204/1000 | Loss: 0.00000925
Iteration 205/1000 | Loss: 0.00000925
Iteration 206/1000 | Loss: 0.00000925
Iteration 207/1000 | Loss: 0.00000925
Iteration 208/1000 | Loss: 0.00000925
Iteration 209/1000 | Loss: 0.00000925
Iteration 210/1000 | Loss: 0.00000925
Iteration 211/1000 | Loss: 0.00000925
Iteration 212/1000 | Loss: 0.00000925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [9.254164069716353e-06, 9.254164069716353e-06, 9.254164069716353e-06, 9.254164069716353e-06, 9.254164069716353e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.254164069716353e-06

Optimization complete. Final v2v error: 2.6283586025238037 mm

Highest mean error: 2.97286057472229 mm for frame 99

Lowest mean error: 2.463355779647827 mm for frame 180

Saving results

Total time: 69.45198678970337
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402562
Iteration 2/25 | Loss: 0.00129339
Iteration 3/25 | Loss: 0.00121484
Iteration 4/25 | Loss: 0.00120622
Iteration 5/25 | Loss: 0.00120404
Iteration 6/25 | Loss: 0.00120341
Iteration 7/25 | Loss: 0.00120329
Iteration 8/25 | Loss: 0.00120329
Iteration 9/25 | Loss: 0.00120329
Iteration 10/25 | Loss: 0.00120329
Iteration 11/25 | Loss: 0.00120329
Iteration 12/25 | Loss: 0.00120329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001203288440592587, 0.001203288440592587, 0.001203288440592587, 0.001203288440592587, 0.001203288440592587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001203288440592587

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44673896
Iteration 2/25 | Loss: 0.00095413
Iteration 3/25 | Loss: 0.00095412
Iteration 4/25 | Loss: 0.00095412
Iteration 5/25 | Loss: 0.00095412
Iteration 6/25 | Loss: 0.00095412
Iteration 7/25 | Loss: 0.00095412
Iteration 8/25 | Loss: 0.00095412
Iteration 9/25 | Loss: 0.00095412
Iteration 10/25 | Loss: 0.00095412
Iteration 11/25 | Loss: 0.00095412
Iteration 12/25 | Loss: 0.00095412
Iteration 13/25 | Loss: 0.00095412
Iteration 14/25 | Loss: 0.00095412
Iteration 15/25 | Loss: 0.00095412
Iteration 16/25 | Loss: 0.00095412
Iteration 17/25 | Loss: 0.00095412
Iteration 18/25 | Loss: 0.00095412
Iteration 19/25 | Loss: 0.00095412
Iteration 20/25 | Loss: 0.00095412
Iteration 21/25 | Loss: 0.00095412
Iteration 22/25 | Loss: 0.00095412
Iteration 23/25 | Loss: 0.00095412
Iteration 24/25 | Loss: 0.00095412
Iteration 25/25 | Loss: 0.00095412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095412
Iteration 2/1000 | Loss: 0.00003362
Iteration 3/1000 | Loss: 0.00002204
Iteration 4/1000 | Loss: 0.00001689
Iteration 5/1000 | Loss: 0.00001521
Iteration 6/1000 | Loss: 0.00001443
Iteration 7/1000 | Loss: 0.00001381
Iteration 8/1000 | Loss: 0.00001340
Iteration 9/1000 | Loss: 0.00001300
Iteration 10/1000 | Loss: 0.00001290
Iteration 11/1000 | Loss: 0.00001280
Iteration 12/1000 | Loss: 0.00001276
Iteration 13/1000 | Loss: 0.00001261
Iteration 14/1000 | Loss: 0.00001251
Iteration 15/1000 | Loss: 0.00001242
Iteration 16/1000 | Loss: 0.00001238
Iteration 17/1000 | Loss: 0.00001238
Iteration 18/1000 | Loss: 0.00001237
Iteration 19/1000 | Loss: 0.00001237
Iteration 20/1000 | Loss: 0.00001235
Iteration 21/1000 | Loss: 0.00001234
Iteration 22/1000 | Loss: 0.00001232
Iteration 23/1000 | Loss: 0.00001230
Iteration 24/1000 | Loss: 0.00001230
Iteration 25/1000 | Loss: 0.00001229
Iteration 26/1000 | Loss: 0.00001228
Iteration 27/1000 | Loss: 0.00001225
Iteration 28/1000 | Loss: 0.00001224
Iteration 29/1000 | Loss: 0.00001223
Iteration 30/1000 | Loss: 0.00001222
Iteration 31/1000 | Loss: 0.00001221
Iteration 32/1000 | Loss: 0.00001220
Iteration 33/1000 | Loss: 0.00001220
Iteration 34/1000 | Loss: 0.00001219
Iteration 35/1000 | Loss: 0.00001219
Iteration 36/1000 | Loss: 0.00001218
Iteration 37/1000 | Loss: 0.00001218
Iteration 38/1000 | Loss: 0.00001216
Iteration 39/1000 | Loss: 0.00001216
Iteration 40/1000 | Loss: 0.00001214
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00001214
Iteration 43/1000 | Loss: 0.00001213
Iteration 44/1000 | Loss: 0.00001213
Iteration 45/1000 | Loss: 0.00001213
Iteration 46/1000 | Loss: 0.00001212
Iteration 47/1000 | Loss: 0.00001212
Iteration 48/1000 | Loss: 0.00001212
Iteration 49/1000 | Loss: 0.00001211
Iteration 50/1000 | Loss: 0.00001211
Iteration 51/1000 | Loss: 0.00001211
Iteration 52/1000 | Loss: 0.00001211
Iteration 53/1000 | Loss: 0.00001211
Iteration 54/1000 | Loss: 0.00001211
Iteration 55/1000 | Loss: 0.00001211
Iteration 56/1000 | Loss: 0.00001211
Iteration 57/1000 | Loss: 0.00001210
Iteration 58/1000 | Loss: 0.00001210
Iteration 59/1000 | Loss: 0.00001210
Iteration 60/1000 | Loss: 0.00001210
Iteration 61/1000 | Loss: 0.00001210
Iteration 62/1000 | Loss: 0.00001209
Iteration 63/1000 | Loss: 0.00001209
Iteration 64/1000 | Loss: 0.00001209
Iteration 65/1000 | Loss: 0.00001208
Iteration 66/1000 | Loss: 0.00001208
Iteration 67/1000 | Loss: 0.00001207
Iteration 68/1000 | Loss: 0.00001207
Iteration 69/1000 | Loss: 0.00001207
Iteration 70/1000 | Loss: 0.00001206
Iteration 71/1000 | Loss: 0.00001206
Iteration 72/1000 | Loss: 0.00001206
Iteration 73/1000 | Loss: 0.00001205
Iteration 74/1000 | Loss: 0.00001205
Iteration 75/1000 | Loss: 0.00001205
Iteration 76/1000 | Loss: 0.00001205
Iteration 77/1000 | Loss: 0.00001204
Iteration 78/1000 | Loss: 0.00001204
Iteration 79/1000 | Loss: 0.00001204
Iteration 80/1000 | Loss: 0.00001204
Iteration 81/1000 | Loss: 0.00001204
Iteration 82/1000 | Loss: 0.00001203
Iteration 83/1000 | Loss: 0.00001203
Iteration 84/1000 | Loss: 0.00001203
Iteration 85/1000 | Loss: 0.00001203
Iteration 86/1000 | Loss: 0.00001202
Iteration 87/1000 | Loss: 0.00001201
Iteration 88/1000 | Loss: 0.00001201
Iteration 89/1000 | Loss: 0.00001201
Iteration 90/1000 | Loss: 0.00001201
Iteration 91/1000 | Loss: 0.00001200
Iteration 92/1000 | Loss: 0.00001200
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001200
Iteration 95/1000 | Loss: 0.00001199
Iteration 96/1000 | Loss: 0.00001199
Iteration 97/1000 | Loss: 0.00001199
Iteration 98/1000 | Loss: 0.00001199
Iteration 99/1000 | Loss: 0.00001199
Iteration 100/1000 | Loss: 0.00001198
Iteration 101/1000 | Loss: 0.00001198
Iteration 102/1000 | Loss: 0.00001198
Iteration 103/1000 | Loss: 0.00001198
Iteration 104/1000 | Loss: 0.00001198
Iteration 105/1000 | Loss: 0.00001197
Iteration 106/1000 | Loss: 0.00001197
Iteration 107/1000 | Loss: 0.00001197
Iteration 108/1000 | Loss: 0.00001197
Iteration 109/1000 | Loss: 0.00001196
Iteration 110/1000 | Loss: 0.00001196
Iteration 111/1000 | Loss: 0.00001196
Iteration 112/1000 | Loss: 0.00001195
Iteration 113/1000 | Loss: 0.00001195
Iteration 114/1000 | Loss: 0.00001195
Iteration 115/1000 | Loss: 0.00001195
Iteration 116/1000 | Loss: 0.00001194
Iteration 117/1000 | Loss: 0.00001194
Iteration 118/1000 | Loss: 0.00001193
Iteration 119/1000 | Loss: 0.00001193
Iteration 120/1000 | Loss: 0.00001193
Iteration 121/1000 | Loss: 0.00001192
Iteration 122/1000 | Loss: 0.00001192
Iteration 123/1000 | Loss: 0.00001192
Iteration 124/1000 | Loss: 0.00001191
Iteration 125/1000 | Loss: 0.00001191
Iteration 126/1000 | Loss: 0.00001191
Iteration 127/1000 | Loss: 0.00001191
Iteration 128/1000 | Loss: 0.00001190
Iteration 129/1000 | Loss: 0.00001190
Iteration 130/1000 | Loss: 0.00001190
Iteration 131/1000 | Loss: 0.00001189
Iteration 132/1000 | Loss: 0.00001189
Iteration 133/1000 | Loss: 0.00001189
Iteration 134/1000 | Loss: 0.00001189
Iteration 135/1000 | Loss: 0.00001189
Iteration 136/1000 | Loss: 0.00001188
Iteration 137/1000 | Loss: 0.00001187
Iteration 138/1000 | Loss: 0.00001187
Iteration 139/1000 | Loss: 0.00001187
Iteration 140/1000 | Loss: 0.00001187
Iteration 141/1000 | Loss: 0.00001187
Iteration 142/1000 | Loss: 0.00001187
Iteration 143/1000 | Loss: 0.00001187
Iteration 144/1000 | Loss: 0.00001187
Iteration 145/1000 | Loss: 0.00001187
Iteration 146/1000 | Loss: 0.00001186
Iteration 147/1000 | Loss: 0.00001186
Iteration 148/1000 | Loss: 0.00001186
Iteration 149/1000 | Loss: 0.00001186
Iteration 150/1000 | Loss: 0.00001186
Iteration 151/1000 | Loss: 0.00001186
Iteration 152/1000 | Loss: 0.00001185
Iteration 153/1000 | Loss: 0.00001185
Iteration 154/1000 | Loss: 0.00001184
Iteration 155/1000 | Loss: 0.00001184
Iteration 156/1000 | Loss: 0.00001184
Iteration 157/1000 | Loss: 0.00001184
Iteration 158/1000 | Loss: 0.00001184
Iteration 159/1000 | Loss: 0.00001184
Iteration 160/1000 | Loss: 0.00001184
Iteration 161/1000 | Loss: 0.00001184
Iteration 162/1000 | Loss: 0.00001184
Iteration 163/1000 | Loss: 0.00001184
Iteration 164/1000 | Loss: 0.00001184
Iteration 165/1000 | Loss: 0.00001184
Iteration 166/1000 | Loss: 0.00001184
Iteration 167/1000 | Loss: 0.00001184
Iteration 168/1000 | Loss: 0.00001184
Iteration 169/1000 | Loss: 0.00001184
Iteration 170/1000 | Loss: 0.00001183
Iteration 171/1000 | Loss: 0.00001183
Iteration 172/1000 | Loss: 0.00001183
Iteration 173/1000 | Loss: 0.00001183
Iteration 174/1000 | Loss: 0.00001183
Iteration 175/1000 | Loss: 0.00001183
Iteration 176/1000 | Loss: 0.00001183
Iteration 177/1000 | Loss: 0.00001183
Iteration 178/1000 | Loss: 0.00001183
Iteration 179/1000 | Loss: 0.00001183
Iteration 180/1000 | Loss: 0.00001182
Iteration 181/1000 | Loss: 0.00001182
Iteration 182/1000 | Loss: 0.00001182
Iteration 183/1000 | Loss: 0.00001182
Iteration 184/1000 | Loss: 0.00001182
Iteration 185/1000 | Loss: 0.00001182
Iteration 186/1000 | Loss: 0.00001182
Iteration 187/1000 | Loss: 0.00001182
Iteration 188/1000 | Loss: 0.00001182
Iteration 189/1000 | Loss: 0.00001182
Iteration 190/1000 | Loss: 0.00001182
Iteration 191/1000 | Loss: 0.00001182
Iteration 192/1000 | Loss: 0.00001182
Iteration 193/1000 | Loss: 0.00001182
Iteration 194/1000 | Loss: 0.00001182
Iteration 195/1000 | Loss: 0.00001182
Iteration 196/1000 | Loss: 0.00001182
Iteration 197/1000 | Loss: 0.00001182
Iteration 198/1000 | Loss: 0.00001182
Iteration 199/1000 | Loss: 0.00001182
Iteration 200/1000 | Loss: 0.00001182
Iteration 201/1000 | Loss: 0.00001182
Iteration 202/1000 | Loss: 0.00001182
Iteration 203/1000 | Loss: 0.00001182
Iteration 204/1000 | Loss: 0.00001182
Iteration 205/1000 | Loss: 0.00001182
Iteration 206/1000 | Loss: 0.00001182
Iteration 207/1000 | Loss: 0.00001182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.1818095117632765e-05, 1.1818095117632765e-05, 1.1818095117632765e-05, 1.1818095117632765e-05, 1.1818095117632765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1818095117632765e-05

Optimization complete. Final v2v error: 2.890718936920166 mm

Highest mean error: 4.305384159088135 mm for frame 60

Lowest mean error: 2.604273557662964 mm for frame 99

Saving results

Total time: 42.34799098968506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411965
Iteration 2/25 | Loss: 0.00134819
Iteration 3/25 | Loss: 0.00125234
Iteration 4/25 | Loss: 0.00123717
Iteration 5/25 | Loss: 0.00123253
Iteration 6/25 | Loss: 0.00123180
Iteration 7/25 | Loss: 0.00123180
Iteration 8/25 | Loss: 0.00123180
Iteration 9/25 | Loss: 0.00123180
Iteration 10/25 | Loss: 0.00123180
Iteration 11/25 | Loss: 0.00123180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012317993678152561, 0.0012317993678152561, 0.0012317993678152561, 0.0012317993678152561, 0.0012317993678152561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012317993678152561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99475527
Iteration 2/25 | Loss: 0.00094001
Iteration 3/25 | Loss: 0.00094001
Iteration 4/25 | Loss: 0.00094001
Iteration 5/25 | Loss: 0.00094001
Iteration 6/25 | Loss: 0.00094001
Iteration 7/25 | Loss: 0.00094000
Iteration 8/25 | Loss: 0.00094000
Iteration 9/25 | Loss: 0.00094000
Iteration 10/25 | Loss: 0.00094000
Iteration 11/25 | Loss: 0.00094000
Iteration 12/25 | Loss: 0.00094000
Iteration 13/25 | Loss: 0.00094000
Iteration 14/25 | Loss: 0.00094000
Iteration 15/25 | Loss: 0.00094000
Iteration 16/25 | Loss: 0.00094000
Iteration 17/25 | Loss: 0.00094000
Iteration 18/25 | Loss: 0.00094000
Iteration 19/25 | Loss: 0.00094000
Iteration 20/25 | Loss: 0.00094000
Iteration 21/25 | Loss: 0.00094000
Iteration 22/25 | Loss: 0.00094000
Iteration 23/25 | Loss: 0.00094000
Iteration 24/25 | Loss: 0.00094000
Iteration 25/25 | Loss: 0.00094000

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094000
Iteration 2/1000 | Loss: 0.00002730
Iteration 3/1000 | Loss: 0.00002277
Iteration 4/1000 | Loss: 0.00002132
Iteration 5/1000 | Loss: 0.00002037
Iteration 6/1000 | Loss: 0.00001974
Iteration 7/1000 | Loss: 0.00001918
Iteration 8/1000 | Loss: 0.00001884
Iteration 9/1000 | Loss: 0.00001842
Iteration 10/1000 | Loss: 0.00001817
Iteration 11/1000 | Loss: 0.00001800
Iteration 12/1000 | Loss: 0.00001799
Iteration 13/1000 | Loss: 0.00001787
Iteration 14/1000 | Loss: 0.00001785
Iteration 15/1000 | Loss: 0.00001784
Iteration 16/1000 | Loss: 0.00001782
Iteration 17/1000 | Loss: 0.00001780
Iteration 18/1000 | Loss: 0.00001779
Iteration 19/1000 | Loss: 0.00001778
Iteration 20/1000 | Loss: 0.00001777
Iteration 21/1000 | Loss: 0.00001775
Iteration 22/1000 | Loss: 0.00001775
Iteration 23/1000 | Loss: 0.00001774
Iteration 24/1000 | Loss: 0.00001774
Iteration 25/1000 | Loss: 0.00001773
Iteration 26/1000 | Loss: 0.00001773
Iteration 27/1000 | Loss: 0.00001772
Iteration 28/1000 | Loss: 0.00001772
Iteration 29/1000 | Loss: 0.00001771
Iteration 30/1000 | Loss: 0.00001770
Iteration 31/1000 | Loss: 0.00001766
Iteration 32/1000 | Loss: 0.00001766
Iteration 33/1000 | Loss: 0.00001762
Iteration 34/1000 | Loss: 0.00001762
Iteration 35/1000 | Loss: 0.00001762
Iteration 36/1000 | Loss: 0.00001761
Iteration 37/1000 | Loss: 0.00001758
Iteration 38/1000 | Loss: 0.00001757
Iteration 39/1000 | Loss: 0.00001756
Iteration 40/1000 | Loss: 0.00001754
Iteration 41/1000 | Loss: 0.00001754
Iteration 42/1000 | Loss: 0.00001754
Iteration 43/1000 | Loss: 0.00001750
Iteration 44/1000 | Loss: 0.00001750
Iteration 45/1000 | Loss: 0.00001749
Iteration 46/1000 | Loss: 0.00001748
Iteration 47/1000 | Loss: 0.00001748
Iteration 48/1000 | Loss: 0.00001747
Iteration 49/1000 | Loss: 0.00001746
Iteration 50/1000 | Loss: 0.00001746
Iteration 51/1000 | Loss: 0.00001745
Iteration 52/1000 | Loss: 0.00001743
Iteration 53/1000 | Loss: 0.00001743
Iteration 54/1000 | Loss: 0.00001743
Iteration 55/1000 | Loss: 0.00001742
Iteration 56/1000 | Loss: 0.00001742
Iteration 57/1000 | Loss: 0.00001741
Iteration 58/1000 | Loss: 0.00001741
Iteration 59/1000 | Loss: 0.00001740
Iteration 60/1000 | Loss: 0.00001740
Iteration 61/1000 | Loss: 0.00001739
Iteration 62/1000 | Loss: 0.00001739
Iteration 63/1000 | Loss: 0.00001739
Iteration 64/1000 | Loss: 0.00001739
Iteration 65/1000 | Loss: 0.00001738
Iteration 66/1000 | Loss: 0.00001738
Iteration 67/1000 | Loss: 0.00001738
Iteration 68/1000 | Loss: 0.00001738
Iteration 69/1000 | Loss: 0.00001737
Iteration 70/1000 | Loss: 0.00001737
Iteration 71/1000 | Loss: 0.00001737
Iteration 72/1000 | Loss: 0.00001736
Iteration 73/1000 | Loss: 0.00001736
Iteration 74/1000 | Loss: 0.00001736
Iteration 75/1000 | Loss: 0.00001736
Iteration 76/1000 | Loss: 0.00001735
Iteration 77/1000 | Loss: 0.00001735
Iteration 78/1000 | Loss: 0.00001735
Iteration 79/1000 | Loss: 0.00001735
Iteration 80/1000 | Loss: 0.00001735
Iteration 81/1000 | Loss: 0.00001735
Iteration 82/1000 | Loss: 0.00001734
Iteration 83/1000 | Loss: 0.00001734
Iteration 84/1000 | Loss: 0.00001733
Iteration 85/1000 | Loss: 0.00001733
Iteration 86/1000 | Loss: 0.00001733
Iteration 87/1000 | Loss: 0.00001733
Iteration 88/1000 | Loss: 0.00001732
Iteration 89/1000 | Loss: 0.00001732
Iteration 90/1000 | Loss: 0.00001732
Iteration 91/1000 | Loss: 0.00001732
Iteration 92/1000 | Loss: 0.00001732
Iteration 93/1000 | Loss: 0.00001732
Iteration 94/1000 | Loss: 0.00001731
Iteration 95/1000 | Loss: 0.00001730
Iteration 96/1000 | Loss: 0.00001730
Iteration 97/1000 | Loss: 0.00001730
Iteration 98/1000 | Loss: 0.00001730
Iteration 99/1000 | Loss: 0.00001730
Iteration 100/1000 | Loss: 0.00001730
Iteration 101/1000 | Loss: 0.00001730
Iteration 102/1000 | Loss: 0.00001729
Iteration 103/1000 | Loss: 0.00001729
Iteration 104/1000 | Loss: 0.00001729
Iteration 105/1000 | Loss: 0.00001729
Iteration 106/1000 | Loss: 0.00001729
Iteration 107/1000 | Loss: 0.00001728
Iteration 108/1000 | Loss: 0.00001728
Iteration 109/1000 | Loss: 0.00001728
Iteration 110/1000 | Loss: 0.00001728
Iteration 111/1000 | Loss: 0.00001728
Iteration 112/1000 | Loss: 0.00001727
Iteration 113/1000 | Loss: 0.00001727
Iteration 114/1000 | Loss: 0.00001727
Iteration 115/1000 | Loss: 0.00001727
Iteration 116/1000 | Loss: 0.00001727
Iteration 117/1000 | Loss: 0.00001727
Iteration 118/1000 | Loss: 0.00001727
Iteration 119/1000 | Loss: 0.00001726
Iteration 120/1000 | Loss: 0.00001726
Iteration 121/1000 | Loss: 0.00001726
Iteration 122/1000 | Loss: 0.00001726
Iteration 123/1000 | Loss: 0.00001726
Iteration 124/1000 | Loss: 0.00001726
Iteration 125/1000 | Loss: 0.00001726
Iteration 126/1000 | Loss: 0.00001726
Iteration 127/1000 | Loss: 0.00001726
Iteration 128/1000 | Loss: 0.00001726
Iteration 129/1000 | Loss: 0.00001726
Iteration 130/1000 | Loss: 0.00001725
Iteration 131/1000 | Loss: 0.00001725
Iteration 132/1000 | Loss: 0.00001725
Iteration 133/1000 | Loss: 0.00001725
Iteration 134/1000 | Loss: 0.00001725
Iteration 135/1000 | Loss: 0.00001725
Iteration 136/1000 | Loss: 0.00001725
Iteration 137/1000 | Loss: 0.00001725
Iteration 138/1000 | Loss: 0.00001725
Iteration 139/1000 | Loss: 0.00001725
Iteration 140/1000 | Loss: 0.00001725
Iteration 141/1000 | Loss: 0.00001724
Iteration 142/1000 | Loss: 0.00001724
Iteration 143/1000 | Loss: 0.00001724
Iteration 144/1000 | Loss: 0.00001724
Iteration 145/1000 | Loss: 0.00001724
Iteration 146/1000 | Loss: 0.00001724
Iteration 147/1000 | Loss: 0.00001724
Iteration 148/1000 | Loss: 0.00001724
Iteration 149/1000 | Loss: 0.00001724
Iteration 150/1000 | Loss: 0.00001724
Iteration 151/1000 | Loss: 0.00001724
Iteration 152/1000 | Loss: 0.00001724
Iteration 153/1000 | Loss: 0.00001724
Iteration 154/1000 | Loss: 0.00001724
Iteration 155/1000 | Loss: 0.00001723
Iteration 156/1000 | Loss: 0.00001723
Iteration 157/1000 | Loss: 0.00001723
Iteration 158/1000 | Loss: 0.00001723
Iteration 159/1000 | Loss: 0.00001723
Iteration 160/1000 | Loss: 0.00001723
Iteration 161/1000 | Loss: 0.00001723
Iteration 162/1000 | Loss: 0.00001723
Iteration 163/1000 | Loss: 0.00001723
Iteration 164/1000 | Loss: 0.00001723
Iteration 165/1000 | Loss: 0.00001723
Iteration 166/1000 | Loss: 0.00001723
Iteration 167/1000 | Loss: 0.00001723
Iteration 168/1000 | Loss: 0.00001723
Iteration 169/1000 | Loss: 0.00001723
Iteration 170/1000 | Loss: 0.00001723
Iteration 171/1000 | Loss: 0.00001723
Iteration 172/1000 | Loss: 0.00001723
Iteration 173/1000 | Loss: 0.00001723
Iteration 174/1000 | Loss: 0.00001723
Iteration 175/1000 | Loss: 0.00001723
Iteration 176/1000 | Loss: 0.00001723
Iteration 177/1000 | Loss: 0.00001723
Iteration 178/1000 | Loss: 0.00001723
Iteration 179/1000 | Loss: 0.00001723
Iteration 180/1000 | Loss: 0.00001722
Iteration 181/1000 | Loss: 0.00001722
Iteration 182/1000 | Loss: 0.00001722
Iteration 183/1000 | Loss: 0.00001722
Iteration 184/1000 | Loss: 0.00001722
Iteration 185/1000 | Loss: 0.00001722
Iteration 186/1000 | Loss: 0.00001722
Iteration 187/1000 | Loss: 0.00001722
Iteration 188/1000 | Loss: 0.00001722
Iteration 189/1000 | Loss: 0.00001722
Iteration 190/1000 | Loss: 0.00001722
Iteration 191/1000 | Loss: 0.00001722
Iteration 192/1000 | Loss: 0.00001722
Iteration 193/1000 | Loss: 0.00001722
Iteration 194/1000 | Loss: 0.00001722
Iteration 195/1000 | Loss: 0.00001722
Iteration 196/1000 | Loss: 0.00001722
Iteration 197/1000 | Loss: 0.00001722
Iteration 198/1000 | Loss: 0.00001722
Iteration 199/1000 | Loss: 0.00001722
Iteration 200/1000 | Loss: 0.00001722
Iteration 201/1000 | Loss: 0.00001722
Iteration 202/1000 | Loss: 0.00001722
Iteration 203/1000 | Loss: 0.00001722
Iteration 204/1000 | Loss: 0.00001722
Iteration 205/1000 | Loss: 0.00001722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.721908301988151e-05, 1.721908301988151e-05, 1.721908301988151e-05, 1.721908301988151e-05, 1.721908301988151e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.721908301988151e-05

Optimization complete. Final v2v error: 3.5153470039367676 mm

Highest mean error: 4.091434955596924 mm for frame 30

Lowest mean error: 3.2432096004486084 mm for frame 61

Saving results

Total time: 39.91833686828613
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00589587
Iteration 2/25 | Loss: 0.00148010
Iteration 3/25 | Loss: 0.00136030
Iteration 4/25 | Loss: 0.00123510
Iteration 5/25 | Loss: 0.00122676
Iteration 6/25 | Loss: 0.00122530
Iteration 7/25 | Loss: 0.00121731
Iteration 8/25 | Loss: 0.00121504
Iteration 9/25 | Loss: 0.00121319
Iteration 10/25 | Loss: 0.00121209
Iteration 11/25 | Loss: 0.00121408
Iteration 12/25 | Loss: 0.00121024
Iteration 13/25 | Loss: 0.00120924
Iteration 14/25 | Loss: 0.00120873
Iteration 15/25 | Loss: 0.00120862
Iteration 16/25 | Loss: 0.00120862
Iteration 17/25 | Loss: 0.00120862
Iteration 18/25 | Loss: 0.00120861
Iteration 19/25 | Loss: 0.00120861
Iteration 20/25 | Loss: 0.00120861
Iteration 21/25 | Loss: 0.00120861
Iteration 22/25 | Loss: 0.00120861
Iteration 23/25 | Loss: 0.00120861
Iteration 24/25 | Loss: 0.00120861
Iteration 25/25 | Loss: 0.00120861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.18961883
Iteration 2/25 | Loss: 0.00098431
Iteration 3/25 | Loss: 0.00098431
Iteration 4/25 | Loss: 0.00098431
Iteration 5/25 | Loss: 0.00098430
Iteration 6/25 | Loss: 0.00098430
Iteration 7/25 | Loss: 0.00098430
Iteration 8/25 | Loss: 0.00098430
Iteration 9/25 | Loss: 0.00098430
Iteration 10/25 | Loss: 0.00098430
Iteration 11/25 | Loss: 0.00098430
Iteration 12/25 | Loss: 0.00098430
Iteration 13/25 | Loss: 0.00098430
Iteration 14/25 | Loss: 0.00098430
Iteration 15/25 | Loss: 0.00098430
Iteration 16/25 | Loss: 0.00098430
Iteration 17/25 | Loss: 0.00098430
Iteration 18/25 | Loss: 0.00098430
Iteration 19/25 | Loss: 0.00098430
Iteration 20/25 | Loss: 0.00098430
Iteration 21/25 | Loss: 0.00098430
Iteration 22/25 | Loss: 0.00098430
Iteration 23/25 | Loss: 0.00098430
Iteration 24/25 | Loss: 0.00098430
Iteration 25/25 | Loss: 0.00098430

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098430
Iteration 2/1000 | Loss: 0.00002641
Iteration 3/1000 | Loss: 0.00001845
Iteration 4/1000 | Loss: 0.00001531
Iteration 5/1000 | Loss: 0.00001452
Iteration 6/1000 | Loss: 0.00001391
Iteration 7/1000 | Loss: 0.00001341
Iteration 8/1000 | Loss: 0.00001310
Iteration 9/1000 | Loss: 0.00001288
Iteration 10/1000 | Loss: 0.00001265
Iteration 11/1000 | Loss: 0.00001262
Iteration 12/1000 | Loss: 0.00001252
Iteration 13/1000 | Loss: 0.00001247
Iteration 14/1000 | Loss: 0.00001239
Iteration 15/1000 | Loss: 0.00001236
Iteration 16/1000 | Loss: 0.00001231
Iteration 17/1000 | Loss: 0.00001230
Iteration 18/1000 | Loss: 0.00001230
Iteration 19/1000 | Loss: 0.00001229
Iteration 20/1000 | Loss: 0.00001229
Iteration 21/1000 | Loss: 0.00001228
Iteration 22/1000 | Loss: 0.00001228
Iteration 23/1000 | Loss: 0.00001227
Iteration 24/1000 | Loss: 0.00001226
Iteration 25/1000 | Loss: 0.00001226
Iteration 26/1000 | Loss: 0.00001226
Iteration 27/1000 | Loss: 0.00001225
Iteration 28/1000 | Loss: 0.00001225
Iteration 29/1000 | Loss: 0.00001224
Iteration 30/1000 | Loss: 0.00001223
Iteration 31/1000 | Loss: 0.00001223
Iteration 32/1000 | Loss: 0.00001222
Iteration 33/1000 | Loss: 0.00001222
Iteration 34/1000 | Loss: 0.00001222
Iteration 35/1000 | Loss: 0.00001221
Iteration 36/1000 | Loss: 0.00001221
Iteration 37/1000 | Loss: 0.00001221
Iteration 38/1000 | Loss: 0.00001220
Iteration 39/1000 | Loss: 0.00001220
Iteration 40/1000 | Loss: 0.00001220
Iteration 41/1000 | Loss: 0.00001220
Iteration 42/1000 | Loss: 0.00001220
Iteration 43/1000 | Loss: 0.00001220
Iteration 44/1000 | Loss: 0.00001220
Iteration 45/1000 | Loss: 0.00001220
Iteration 46/1000 | Loss: 0.00001220
Iteration 47/1000 | Loss: 0.00001220
Iteration 48/1000 | Loss: 0.00001220
Iteration 49/1000 | Loss: 0.00001219
Iteration 50/1000 | Loss: 0.00001219
Iteration 51/1000 | Loss: 0.00001219
Iteration 52/1000 | Loss: 0.00001218
Iteration 53/1000 | Loss: 0.00001217
Iteration 54/1000 | Loss: 0.00001217
Iteration 55/1000 | Loss: 0.00001217
Iteration 56/1000 | Loss: 0.00001216
Iteration 57/1000 | Loss: 0.00001216
Iteration 58/1000 | Loss: 0.00001216
Iteration 59/1000 | Loss: 0.00001215
Iteration 60/1000 | Loss: 0.00001215
Iteration 61/1000 | Loss: 0.00001214
Iteration 62/1000 | Loss: 0.00001213
Iteration 63/1000 | Loss: 0.00001212
Iteration 64/1000 | Loss: 0.00001212
Iteration 65/1000 | Loss: 0.00001212
Iteration 66/1000 | Loss: 0.00001211
Iteration 67/1000 | Loss: 0.00001211
Iteration 68/1000 | Loss: 0.00001211
Iteration 69/1000 | Loss: 0.00001211
Iteration 70/1000 | Loss: 0.00001210
Iteration 71/1000 | Loss: 0.00001210
Iteration 72/1000 | Loss: 0.00001209
Iteration 73/1000 | Loss: 0.00001209
Iteration 74/1000 | Loss: 0.00001208
Iteration 75/1000 | Loss: 0.00001207
Iteration 76/1000 | Loss: 0.00001207
Iteration 77/1000 | Loss: 0.00001207
Iteration 78/1000 | Loss: 0.00001206
Iteration 79/1000 | Loss: 0.00001206
Iteration 80/1000 | Loss: 0.00001206
Iteration 81/1000 | Loss: 0.00001205
Iteration 82/1000 | Loss: 0.00001205
Iteration 83/1000 | Loss: 0.00001204
Iteration 84/1000 | Loss: 0.00001204
Iteration 85/1000 | Loss: 0.00001204
Iteration 86/1000 | Loss: 0.00001204
Iteration 87/1000 | Loss: 0.00001204
Iteration 88/1000 | Loss: 0.00001204
Iteration 89/1000 | Loss: 0.00001203
Iteration 90/1000 | Loss: 0.00001203
Iteration 91/1000 | Loss: 0.00001203
Iteration 92/1000 | Loss: 0.00001203
Iteration 93/1000 | Loss: 0.00001203
Iteration 94/1000 | Loss: 0.00001203
Iteration 95/1000 | Loss: 0.00001203
Iteration 96/1000 | Loss: 0.00001203
Iteration 97/1000 | Loss: 0.00001202
Iteration 98/1000 | Loss: 0.00001202
Iteration 99/1000 | Loss: 0.00001202
Iteration 100/1000 | Loss: 0.00001202
Iteration 101/1000 | Loss: 0.00001202
Iteration 102/1000 | Loss: 0.00001201
Iteration 103/1000 | Loss: 0.00001201
Iteration 104/1000 | Loss: 0.00001201
Iteration 105/1000 | Loss: 0.00001201
Iteration 106/1000 | Loss: 0.00001201
Iteration 107/1000 | Loss: 0.00001200
Iteration 108/1000 | Loss: 0.00001200
Iteration 109/1000 | Loss: 0.00001200
Iteration 110/1000 | Loss: 0.00001200
Iteration 111/1000 | Loss: 0.00001200
Iteration 112/1000 | Loss: 0.00001199
Iteration 113/1000 | Loss: 0.00001199
Iteration 114/1000 | Loss: 0.00001199
Iteration 115/1000 | Loss: 0.00001199
Iteration 116/1000 | Loss: 0.00001199
Iteration 117/1000 | Loss: 0.00001199
Iteration 118/1000 | Loss: 0.00001199
Iteration 119/1000 | Loss: 0.00001199
Iteration 120/1000 | Loss: 0.00001199
Iteration 121/1000 | Loss: 0.00001198
Iteration 122/1000 | Loss: 0.00001198
Iteration 123/1000 | Loss: 0.00001198
Iteration 124/1000 | Loss: 0.00001198
Iteration 125/1000 | Loss: 0.00001198
Iteration 126/1000 | Loss: 0.00001198
Iteration 127/1000 | Loss: 0.00001198
Iteration 128/1000 | Loss: 0.00001198
Iteration 129/1000 | Loss: 0.00001197
Iteration 130/1000 | Loss: 0.00001197
Iteration 131/1000 | Loss: 0.00001197
Iteration 132/1000 | Loss: 0.00001197
Iteration 133/1000 | Loss: 0.00001196
Iteration 134/1000 | Loss: 0.00001196
Iteration 135/1000 | Loss: 0.00001196
Iteration 136/1000 | Loss: 0.00001196
Iteration 137/1000 | Loss: 0.00001196
Iteration 138/1000 | Loss: 0.00001196
Iteration 139/1000 | Loss: 0.00001196
Iteration 140/1000 | Loss: 0.00001195
Iteration 141/1000 | Loss: 0.00001195
Iteration 142/1000 | Loss: 0.00001195
Iteration 143/1000 | Loss: 0.00001195
Iteration 144/1000 | Loss: 0.00001195
Iteration 145/1000 | Loss: 0.00001195
Iteration 146/1000 | Loss: 0.00001195
Iteration 147/1000 | Loss: 0.00001194
Iteration 148/1000 | Loss: 0.00001194
Iteration 149/1000 | Loss: 0.00001194
Iteration 150/1000 | Loss: 0.00001194
Iteration 151/1000 | Loss: 0.00001194
Iteration 152/1000 | Loss: 0.00001194
Iteration 153/1000 | Loss: 0.00001194
Iteration 154/1000 | Loss: 0.00001194
Iteration 155/1000 | Loss: 0.00001194
Iteration 156/1000 | Loss: 0.00001194
Iteration 157/1000 | Loss: 0.00001194
Iteration 158/1000 | Loss: 0.00001194
Iteration 159/1000 | Loss: 0.00001193
Iteration 160/1000 | Loss: 0.00001193
Iteration 161/1000 | Loss: 0.00001193
Iteration 162/1000 | Loss: 0.00001193
Iteration 163/1000 | Loss: 0.00001193
Iteration 164/1000 | Loss: 0.00001193
Iteration 165/1000 | Loss: 0.00001193
Iteration 166/1000 | Loss: 0.00001193
Iteration 167/1000 | Loss: 0.00001193
Iteration 168/1000 | Loss: 0.00001193
Iteration 169/1000 | Loss: 0.00001193
Iteration 170/1000 | Loss: 0.00001193
Iteration 171/1000 | Loss: 0.00001192
Iteration 172/1000 | Loss: 0.00001192
Iteration 173/1000 | Loss: 0.00001192
Iteration 174/1000 | Loss: 0.00001192
Iteration 175/1000 | Loss: 0.00001192
Iteration 176/1000 | Loss: 0.00001192
Iteration 177/1000 | Loss: 0.00001192
Iteration 178/1000 | Loss: 0.00001192
Iteration 179/1000 | Loss: 0.00001192
Iteration 180/1000 | Loss: 0.00001192
Iteration 181/1000 | Loss: 0.00001192
Iteration 182/1000 | Loss: 0.00001192
Iteration 183/1000 | Loss: 0.00001192
Iteration 184/1000 | Loss: 0.00001192
Iteration 185/1000 | Loss: 0.00001191
Iteration 186/1000 | Loss: 0.00001191
Iteration 187/1000 | Loss: 0.00001191
Iteration 188/1000 | Loss: 0.00001191
Iteration 189/1000 | Loss: 0.00001191
Iteration 190/1000 | Loss: 0.00001191
Iteration 191/1000 | Loss: 0.00001191
Iteration 192/1000 | Loss: 0.00001191
Iteration 193/1000 | Loss: 0.00001191
Iteration 194/1000 | Loss: 0.00001191
Iteration 195/1000 | Loss: 0.00001191
Iteration 196/1000 | Loss: 0.00001191
Iteration 197/1000 | Loss: 0.00001191
Iteration 198/1000 | Loss: 0.00001191
Iteration 199/1000 | Loss: 0.00001191
Iteration 200/1000 | Loss: 0.00001191
Iteration 201/1000 | Loss: 0.00001191
Iteration 202/1000 | Loss: 0.00001191
Iteration 203/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.191117462440161e-05, 1.191117462440161e-05, 1.191117462440161e-05, 1.191117462440161e-05, 1.191117462440161e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.191117462440161e-05

Optimization complete. Final v2v error: 2.92807674407959 mm

Highest mean error: 3.3100268840789795 mm for frame 66

Lowest mean error: 2.6460683345794678 mm for frame 130

Saving results

Total time: 62.52459168434143
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00725854
Iteration 2/25 | Loss: 0.00162636
Iteration 3/25 | Loss: 0.00144099
Iteration 4/25 | Loss: 0.00124504
Iteration 5/25 | Loss: 0.00123848
Iteration 6/25 | Loss: 0.00123347
Iteration 7/25 | Loss: 0.00122949
Iteration 8/25 | Loss: 0.00122845
Iteration 9/25 | Loss: 0.00122733
Iteration 10/25 | Loss: 0.00122572
Iteration 11/25 | Loss: 0.00122474
Iteration 12/25 | Loss: 0.00122448
Iteration 13/25 | Loss: 0.00122440
Iteration 14/25 | Loss: 0.00122438
Iteration 15/25 | Loss: 0.00122438
Iteration 16/25 | Loss: 0.00122438
Iteration 17/25 | Loss: 0.00122437
Iteration 18/25 | Loss: 0.00122437
Iteration 19/25 | Loss: 0.00122437
Iteration 20/25 | Loss: 0.00122437
Iteration 21/25 | Loss: 0.00122437
Iteration 22/25 | Loss: 0.00122437
Iteration 23/25 | Loss: 0.00122437
Iteration 24/25 | Loss: 0.00122437
Iteration 25/25 | Loss: 0.00122437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82584524
Iteration 2/25 | Loss: 0.00113644
Iteration 3/25 | Loss: 0.00100450
Iteration 4/25 | Loss: 0.00100450
Iteration 5/25 | Loss: 0.00100450
Iteration 6/25 | Loss: 0.00100450
Iteration 7/25 | Loss: 0.00100450
Iteration 8/25 | Loss: 0.00100449
Iteration 9/25 | Loss: 0.00100449
Iteration 10/25 | Loss: 0.00100449
Iteration 11/25 | Loss: 0.00100449
Iteration 12/25 | Loss: 0.00100449
Iteration 13/25 | Loss: 0.00100449
Iteration 14/25 | Loss: 0.00100449
Iteration 15/25 | Loss: 0.00100449
Iteration 16/25 | Loss: 0.00100449
Iteration 17/25 | Loss: 0.00100449
Iteration 18/25 | Loss: 0.00100449
Iteration 19/25 | Loss: 0.00100449
Iteration 20/25 | Loss: 0.00100449
Iteration 21/25 | Loss: 0.00100449
Iteration 22/25 | Loss: 0.00100449
Iteration 23/25 | Loss: 0.00100449
Iteration 24/25 | Loss: 0.00100449
Iteration 25/25 | Loss: 0.00100449

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100449
Iteration 2/1000 | Loss: 0.00018381
Iteration 3/1000 | Loss: 0.00017172
Iteration 4/1000 | Loss: 0.00002441
Iteration 5/1000 | Loss: 0.00002195
Iteration 6/1000 | Loss: 0.00002109
Iteration 7/1000 | Loss: 0.00002052
Iteration 8/1000 | Loss: 0.00015917
Iteration 9/1000 | Loss: 0.00002040
Iteration 10/1000 | Loss: 0.00006529
Iteration 11/1000 | Loss: 0.00001976
Iteration 12/1000 | Loss: 0.00001925
Iteration 13/1000 | Loss: 0.00001893
Iteration 14/1000 | Loss: 0.00001863
Iteration 15/1000 | Loss: 0.00001852
Iteration 16/1000 | Loss: 0.00001842
Iteration 17/1000 | Loss: 0.00001821
Iteration 18/1000 | Loss: 0.00001799
Iteration 19/1000 | Loss: 0.00001793
Iteration 20/1000 | Loss: 0.00001790
Iteration 21/1000 | Loss: 0.00001789
Iteration 22/1000 | Loss: 0.00001789
Iteration 23/1000 | Loss: 0.00001789
Iteration 24/1000 | Loss: 0.00001789
Iteration 25/1000 | Loss: 0.00001788
Iteration 26/1000 | Loss: 0.00001788
Iteration 27/1000 | Loss: 0.00001780
Iteration 28/1000 | Loss: 0.00001778
Iteration 29/1000 | Loss: 0.00001776
Iteration 30/1000 | Loss: 0.00001775
Iteration 31/1000 | Loss: 0.00001774
Iteration 32/1000 | Loss: 0.00001773
Iteration 33/1000 | Loss: 0.00001772
Iteration 34/1000 | Loss: 0.00001771
Iteration 35/1000 | Loss: 0.00001770
Iteration 36/1000 | Loss: 0.00001769
Iteration 37/1000 | Loss: 0.00001768
Iteration 38/1000 | Loss: 0.00001768
Iteration 39/1000 | Loss: 0.00001767
Iteration 40/1000 | Loss: 0.00001767
Iteration 41/1000 | Loss: 0.00001766
Iteration 42/1000 | Loss: 0.00001765
Iteration 43/1000 | Loss: 0.00001765
Iteration 44/1000 | Loss: 0.00001765
Iteration 45/1000 | Loss: 0.00001764
Iteration 46/1000 | Loss: 0.00001764
Iteration 47/1000 | Loss: 0.00001764
Iteration 48/1000 | Loss: 0.00001762
Iteration 49/1000 | Loss: 0.00001761
Iteration 50/1000 | Loss: 0.00001761
Iteration 51/1000 | Loss: 0.00001761
Iteration 52/1000 | Loss: 0.00001761
Iteration 53/1000 | Loss: 0.00001760
Iteration 54/1000 | Loss: 0.00001760
Iteration 55/1000 | Loss: 0.00001760
Iteration 56/1000 | Loss: 0.00001760
Iteration 57/1000 | Loss: 0.00001760
Iteration 58/1000 | Loss: 0.00001760
Iteration 59/1000 | Loss: 0.00001760
Iteration 60/1000 | Loss: 0.00001760
Iteration 61/1000 | Loss: 0.00001760
Iteration 62/1000 | Loss: 0.00001760
Iteration 63/1000 | Loss: 0.00001759
Iteration 64/1000 | Loss: 0.00001759
Iteration 65/1000 | Loss: 0.00001757
Iteration 66/1000 | Loss: 0.00001756
Iteration 67/1000 | Loss: 0.00001756
Iteration 68/1000 | Loss: 0.00001756
Iteration 69/1000 | Loss: 0.00001756
Iteration 70/1000 | Loss: 0.00001756
Iteration 71/1000 | Loss: 0.00001756
Iteration 72/1000 | Loss: 0.00001756
Iteration 73/1000 | Loss: 0.00001756
Iteration 74/1000 | Loss: 0.00001756
Iteration 75/1000 | Loss: 0.00001756
Iteration 76/1000 | Loss: 0.00001755
Iteration 77/1000 | Loss: 0.00001755
Iteration 78/1000 | Loss: 0.00001754
Iteration 79/1000 | Loss: 0.00001754
Iteration 80/1000 | Loss: 0.00001754
Iteration 81/1000 | Loss: 0.00001753
Iteration 82/1000 | Loss: 0.00001752
Iteration 83/1000 | Loss: 0.00001752
Iteration 84/1000 | Loss: 0.00001752
Iteration 85/1000 | Loss: 0.00001751
Iteration 86/1000 | Loss: 0.00001751
Iteration 87/1000 | Loss: 0.00001751
Iteration 88/1000 | Loss: 0.00001751
Iteration 89/1000 | Loss: 0.00001751
Iteration 90/1000 | Loss: 0.00001751
Iteration 91/1000 | Loss: 0.00001751
Iteration 92/1000 | Loss: 0.00001750
Iteration 93/1000 | Loss: 0.00001750
Iteration 94/1000 | Loss: 0.00001750
Iteration 95/1000 | Loss: 0.00001749
Iteration 96/1000 | Loss: 0.00001749
Iteration 97/1000 | Loss: 0.00001749
Iteration 98/1000 | Loss: 0.00001748
Iteration 99/1000 | Loss: 0.00001748
Iteration 100/1000 | Loss: 0.00001748
Iteration 101/1000 | Loss: 0.00001747
Iteration 102/1000 | Loss: 0.00001747
Iteration 103/1000 | Loss: 0.00001747
Iteration 104/1000 | Loss: 0.00001747
Iteration 105/1000 | Loss: 0.00001746
Iteration 106/1000 | Loss: 0.00001746
Iteration 107/1000 | Loss: 0.00001746
Iteration 108/1000 | Loss: 0.00001746
Iteration 109/1000 | Loss: 0.00001745
Iteration 110/1000 | Loss: 0.00001745
Iteration 111/1000 | Loss: 0.00001745
Iteration 112/1000 | Loss: 0.00001745
Iteration 113/1000 | Loss: 0.00001745
Iteration 114/1000 | Loss: 0.00001745
Iteration 115/1000 | Loss: 0.00001745
Iteration 116/1000 | Loss: 0.00001745
Iteration 117/1000 | Loss: 0.00001745
Iteration 118/1000 | Loss: 0.00001745
Iteration 119/1000 | Loss: 0.00001745
Iteration 120/1000 | Loss: 0.00001745
Iteration 121/1000 | Loss: 0.00001744
Iteration 122/1000 | Loss: 0.00001744
Iteration 123/1000 | Loss: 0.00001744
Iteration 124/1000 | Loss: 0.00001744
Iteration 125/1000 | Loss: 0.00001743
Iteration 126/1000 | Loss: 0.00001743
Iteration 127/1000 | Loss: 0.00001743
Iteration 128/1000 | Loss: 0.00001743
Iteration 129/1000 | Loss: 0.00001743
Iteration 130/1000 | Loss: 0.00001742
Iteration 131/1000 | Loss: 0.00001742
Iteration 132/1000 | Loss: 0.00001742
Iteration 133/1000 | Loss: 0.00001742
Iteration 134/1000 | Loss: 0.00001742
Iteration 135/1000 | Loss: 0.00001742
Iteration 136/1000 | Loss: 0.00001742
Iteration 137/1000 | Loss: 0.00001742
Iteration 138/1000 | Loss: 0.00001742
Iteration 139/1000 | Loss: 0.00001742
Iteration 140/1000 | Loss: 0.00001742
Iteration 141/1000 | Loss: 0.00001742
Iteration 142/1000 | Loss: 0.00001742
Iteration 143/1000 | Loss: 0.00001742
Iteration 144/1000 | Loss: 0.00001742
Iteration 145/1000 | Loss: 0.00001742
Iteration 146/1000 | Loss: 0.00001742
Iteration 147/1000 | Loss: 0.00001742
Iteration 148/1000 | Loss: 0.00001742
Iteration 149/1000 | Loss: 0.00001741
Iteration 150/1000 | Loss: 0.00001741
Iteration 151/1000 | Loss: 0.00001741
Iteration 152/1000 | Loss: 0.00001741
Iteration 153/1000 | Loss: 0.00001741
Iteration 154/1000 | Loss: 0.00001741
Iteration 155/1000 | Loss: 0.00001741
Iteration 156/1000 | Loss: 0.00001741
Iteration 157/1000 | Loss: 0.00001741
Iteration 158/1000 | Loss: 0.00001741
Iteration 159/1000 | Loss: 0.00001741
Iteration 160/1000 | Loss: 0.00001741
Iteration 161/1000 | Loss: 0.00001741
Iteration 162/1000 | Loss: 0.00001741
Iteration 163/1000 | Loss: 0.00001741
Iteration 164/1000 | Loss: 0.00001741
Iteration 165/1000 | Loss: 0.00001741
Iteration 166/1000 | Loss: 0.00001741
Iteration 167/1000 | Loss: 0.00001741
Iteration 168/1000 | Loss: 0.00001741
Iteration 169/1000 | Loss: 0.00001741
Iteration 170/1000 | Loss: 0.00001741
Iteration 171/1000 | Loss: 0.00001741
Iteration 172/1000 | Loss: 0.00001741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.7405209291609935e-05, 1.7405209291609935e-05, 1.7405209291609935e-05, 1.7405209291609935e-05, 1.7405209291609935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7405209291609935e-05

Optimization complete. Final v2v error: 3.5536270141601562 mm

Highest mean error: 3.9373362064361572 mm for frame 103

Lowest mean error: 3.236017942428589 mm for frame 127

Saving results

Total time: 58.070841550827026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493340
Iteration 2/25 | Loss: 0.00125933
Iteration 3/25 | Loss: 0.00120327
Iteration 4/25 | Loss: 0.00119674
Iteration 5/25 | Loss: 0.00119457
Iteration 6/25 | Loss: 0.00119445
Iteration 7/25 | Loss: 0.00119445
Iteration 8/25 | Loss: 0.00119445
Iteration 9/25 | Loss: 0.00119445
Iteration 10/25 | Loss: 0.00119445
Iteration 11/25 | Loss: 0.00119445
Iteration 12/25 | Loss: 0.00119445
Iteration 13/25 | Loss: 0.00119445
Iteration 14/25 | Loss: 0.00119445
Iteration 15/25 | Loss: 0.00119445
Iteration 16/25 | Loss: 0.00119445
Iteration 17/25 | Loss: 0.00119445
Iteration 18/25 | Loss: 0.00119445
Iteration 19/25 | Loss: 0.00119445
Iteration 20/25 | Loss: 0.00119445
Iteration 21/25 | Loss: 0.00119445
Iteration 22/25 | Loss: 0.00119445
Iteration 23/25 | Loss: 0.00119445
Iteration 24/25 | Loss: 0.00119445
Iteration 25/25 | Loss: 0.00119445

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.96615362
Iteration 2/25 | Loss: 0.00097929
Iteration 3/25 | Loss: 0.00097928
Iteration 4/25 | Loss: 0.00097928
Iteration 5/25 | Loss: 0.00097928
Iteration 6/25 | Loss: 0.00097928
Iteration 7/25 | Loss: 0.00097928
Iteration 8/25 | Loss: 0.00097928
Iteration 9/25 | Loss: 0.00097928
Iteration 10/25 | Loss: 0.00097928
Iteration 11/25 | Loss: 0.00097928
Iteration 12/25 | Loss: 0.00097928
Iteration 13/25 | Loss: 0.00097928
Iteration 14/25 | Loss: 0.00097928
Iteration 15/25 | Loss: 0.00097928
Iteration 16/25 | Loss: 0.00097928
Iteration 17/25 | Loss: 0.00097928
Iteration 18/25 | Loss: 0.00097928
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009792814962565899, 0.0009792814962565899, 0.0009792814962565899, 0.0009792814962565899, 0.0009792814962565899]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009792814962565899

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097928
Iteration 2/1000 | Loss: 0.00002278
Iteration 3/1000 | Loss: 0.00001817
Iteration 4/1000 | Loss: 0.00001566
Iteration 5/1000 | Loss: 0.00001476
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001332
Iteration 8/1000 | Loss: 0.00001294
Iteration 9/1000 | Loss: 0.00001272
Iteration 10/1000 | Loss: 0.00001248
Iteration 11/1000 | Loss: 0.00001228
Iteration 12/1000 | Loss: 0.00001226
Iteration 13/1000 | Loss: 0.00001226
Iteration 14/1000 | Loss: 0.00001225
Iteration 15/1000 | Loss: 0.00001225
Iteration 16/1000 | Loss: 0.00001215
Iteration 17/1000 | Loss: 0.00001206
Iteration 18/1000 | Loss: 0.00001205
Iteration 19/1000 | Loss: 0.00001198
Iteration 20/1000 | Loss: 0.00001197
Iteration 21/1000 | Loss: 0.00001197
Iteration 22/1000 | Loss: 0.00001195
Iteration 23/1000 | Loss: 0.00001195
Iteration 24/1000 | Loss: 0.00001194
Iteration 25/1000 | Loss: 0.00001192
Iteration 26/1000 | Loss: 0.00001191
Iteration 27/1000 | Loss: 0.00001189
Iteration 28/1000 | Loss: 0.00001189
Iteration 29/1000 | Loss: 0.00001189
Iteration 30/1000 | Loss: 0.00001189
Iteration 31/1000 | Loss: 0.00001189
Iteration 32/1000 | Loss: 0.00001188
Iteration 33/1000 | Loss: 0.00001188
Iteration 34/1000 | Loss: 0.00001188
Iteration 35/1000 | Loss: 0.00001187
Iteration 36/1000 | Loss: 0.00001186
Iteration 37/1000 | Loss: 0.00001185
Iteration 38/1000 | Loss: 0.00001185
Iteration 39/1000 | Loss: 0.00001184
Iteration 40/1000 | Loss: 0.00001183
Iteration 41/1000 | Loss: 0.00001182
Iteration 42/1000 | Loss: 0.00001181
Iteration 43/1000 | Loss: 0.00001181
Iteration 44/1000 | Loss: 0.00001180
Iteration 45/1000 | Loss: 0.00001180
Iteration 46/1000 | Loss: 0.00001175
Iteration 47/1000 | Loss: 0.00001175
Iteration 48/1000 | Loss: 0.00001173
Iteration 49/1000 | Loss: 0.00001173
Iteration 50/1000 | Loss: 0.00001172
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001170
Iteration 53/1000 | Loss: 0.00001169
Iteration 54/1000 | Loss: 0.00001169
Iteration 55/1000 | Loss: 0.00001169
Iteration 56/1000 | Loss: 0.00001168
Iteration 57/1000 | Loss: 0.00001168
Iteration 58/1000 | Loss: 0.00001167
Iteration 59/1000 | Loss: 0.00001167
Iteration 60/1000 | Loss: 0.00001166
Iteration 61/1000 | Loss: 0.00001166
Iteration 62/1000 | Loss: 0.00001165
Iteration 63/1000 | Loss: 0.00001165
Iteration 64/1000 | Loss: 0.00001165
Iteration 65/1000 | Loss: 0.00001164
Iteration 66/1000 | Loss: 0.00001164
Iteration 67/1000 | Loss: 0.00001163
Iteration 68/1000 | Loss: 0.00001163
Iteration 69/1000 | Loss: 0.00001163
Iteration 70/1000 | Loss: 0.00001162
Iteration 71/1000 | Loss: 0.00001161
Iteration 72/1000 | Loss: 0.00001161
Iteration 73/1000 | Loss: 0.00001160
Iteration 74/1000 | Loss: 0.00001159
Iteration 75/1000 | Loss: 0.00001159
Iteration 76/1000 | Loss: 0.00001159
Iteration 77/1000 | Loss: 0.00001159
Iteration 78/1000 | Loss: 0.00001158
Iteration 79/1000 | Loss: 0.00001158
Iteration 80/1000 | Loss: 0.00001157
Iteration 81/1000 | Loss: 0.00001156
Iteration 82/1000 | Loss: 0.00001155
Iteration 83/1000 | Loss: 0.00001154
Iteration 84/1000 | Loss: 0.00001152
Iteration 85/1000 | Loss: 0.00001152
Iteration 86/1000 | Loss: 0.00001152
Iteration 87/1000 | Loss: 0.00001151
Iteration 88/1000 | Loss: 0.00001151
Iteration 89/1000 | Loss: 0.00001151
Iteration 90/1000 | Loss: 0.00001151
Iteration 91/1000 | Loss: 0.00001151
Iteration 92/1000 | Loss: 0.00001150
Iteration 93/1000 | Loss: 0.00001150
Iteration 94/1000 | Loss: 0.00001149
Iteration 95/1000 | Loss: 0.00001149
Iteration 96/1000 | Loss: 0.00001149
Iteration 97/1000 | Loss: 0.00001148
Iteration 98/1000 | Loss: 0.00001148
Iteration 99/1000 | Loss: 0.00001148
Iteration 100/1000 | Loss: 0.00001148
Iteration 101/1000 | Loss: 0.00001147
Iteration 102/1000 | Loss: 0.00001147
Iteration 103/1000 | Loss: 0.00001147
Iteration 104/1000 | Loss: 0.00001147
Iteration 105/1000 | Loss: 0.00001147
Iteration 106/1000 | Loss: 0.00001146
Iteration 107/1000 | Loss: 0.00001146
Iteration 108/1000 | Loss: 0.00001146
Iteration 109/1000 | Loss: 0.00001145
Iteration 110/1000 | Loss: 0.00001145
Iteration 111/1000 | Loss: 0.00001145
Iteration 112/1000 | Loss: 0.00001145
Iteration 113/1000 | Loss: 0.00001145
Iteration 114/1000 | Loss: 0.00001145
Iteration 115/1000 | Loss: 0.00001144
Iteration 116/1000 | Loss: 0.00001144
Iteration 117/1000 | Loss: 0.00001144
Iteration 118/1000 | Loss: 0.00001144
Iteration 119/1000 | Loss: 0.00001144
Iteration 120/1000 | Loss: 0.00001144
Iteration 121/1000 | Loss: 0.00001144
Iteration 122/1000 | Loss: 0.00001143
Iteration 123/1000 | Loss: 0.00001143
Iteration 124/1000 | Loss: 0.00001143
Iteration 125/1000 | Loss: 0.00001143
Iteration 126/1000 | Loss: 0.00001143
Iteration 127/1000 | Loss: 0.00001143
Iteration 128/1000 | Loss: 0.00001143
Iteration 129/1000 | Loss: 0.00001143
Iteration 130/1000 | Loss: 0.00001142
Iteration 131/1000 | Loss: 0.00001142
Iteration 132/1000 | Loss: 0.00001142
Iteration 133/1000 | Loss: 0.00001142
Iteration 134/1000 | Loss: 0.00001142
Iteration 135/1000 | Loss: 0.00001142
Iteration 136/1000 | Loss: 0.00001142
Iteration 137/1000 | Loss: 0.00001142
Iteration 138/1000 | Loss: 0.00001142
Iteration 139/1000 | Loss: 0.00001142
Iteration 140/1000 | Loss: 0.00001141
Iteration 141/1000 | Loss: 0.00001141
Iteration 142/1000 | Loss: 0.00001141
Iteration 143/1000 | Loss: 0.00001141
Iteration 144/1000 | Loss: 0.00001141
Iteration 145/1000 | Loss: 0.00001141
Iteration 146/1000 | Loss: 0.00001141
Iteration 147/1000 | Loss: 0.00001141
Iteration 148/1000 | Loss: 0.00001141
Iteration 149/1000 | Loss: 0.00001141
Iteration 150/1000 | Loss: 0.00001141
Iteration 151/1000 | Loss: 0.00001140
Iteration 152/1000 | Loss: 0.00001140
Iteration 153/1000 | Loss: 0.00001139
Iteration 154/1000 | Loss: 0.00001139
Iteration 155/1000 | Loss: 0.00001139
Iteration 156/1000 | Loss: 0.00001139
Iteration 157/1000 | Loss: 0.00001139
Iteration 158/1000 | Loss: 0.00001139
Iteration 159/1000 | Loss: 0.00001139
Iteration 160/1000 | Loss: 0.00001139
Iteration 161/1000 | Loss: 0.00001138
Iteration 162/1000 | Loss: 0.00001138
Iteration 163/1000 | Loss: 0.00001138
Iteration 164/1000 | Loss: 0.00001138
Iteration 165/1000 | Loss: 0.00001138
Iteration 166/1000 | Loss: 0.00001138
Iteration 167/1000 | Loss: 0.00001138
Iteration 168/1000 | Loss: 0.00001138
Iteration 169/1000 | Loss: 0.00001137
Iteration 170/1000 | Loss: 0.00001137
Iteration 171/1000 | Loss: 0.00001137
Iteration 172/1000 | Loss: 0.00001137
Iteration 173/1000 | Loss: 0.00001137
Iteration 174/1000 | Loss: 0.00001137
Iteration 175/1000 | Loss: 0.00001137
Iteration 176/1000 | Loss: 0.00001137
Iteration 177/1000 | Loss: 0.00001137
Iteration 178/1000 | Loss: 0.00001137
Iteration 179/1000 | Loss: 0.00001137
Iteration 180/1000 | Loss: 0.00001137
Iteration 181/1000 | Loss: 0.00001137
Iteration 182/1000 | Loss: 0.00001136
Iteration 183/1000 | Loss: 0.00001136
Iteration 184/1000 | Loss: 0.00001136
Iteration 185/1000 | Loss: 0.00001136
Iteration 186/1000 | Loss: 0.00001136
Iteration 187/1000 | Loss: 0.00001136
Iteration 188/1000 | Loss: 0.00001135
Iteration 189/1000 | Loss: 0.00001135
Iteration 190/1000 | Loss: 0.00001135
Iteration 191/1000 | Loss: 0.00001135
Iteration 192/1000 | Loss: 0.00001135
Iteration 193/1000 | Loss: 0.00001135
Iteration 194/1000 | Loss: 0.00001134
Iteration 195/1000 | Loss: 0.00001134
Iteration 196/1000 | Loss: 0.00001134
Iteration 197/1000 | Loss: 0.00001134
Iteration 198/1000 | Loss: 0.00001134
Iteration 199/1000 | Loss: 0.00001134
Iteration 200/1000 | Loss: 0.00001134
Iteration 201/1000 | Loss: 0.00001134
Iteration 202/1000 | Loss: 0.00001134
Iteration 203/1000 | Loss: 0.00001134
Iteration 204/1000 | Loss: 0.00001134
Iteration 205/1000 | Loss: 0.00001134
Iteration 206/1000 | Loss: 0.00001134
Iteration 207/1000 | Loss: 0.00001134
Iteration 208/1000 | Loss: 0.00001134
Iteration 209/1000 | Loss: 0.00001133
Iteration 210/1000 | Loss: 0.00001133
Iteration 211/1000 | Loss: 0.00001133
Iteration 212/1000 | Loss: 0.00001133
Iteration 213/1000 | Loss: 0.00001133
Iteration 214/1000 | Loss: 0.00001133
Iteration 215/1000 | Loss: 0.00001133
Iteration 216/1000 | Loss: 0.00001133
Iteration 217/1000 | Loss: 0.00001132
Iteration 218/1000 | Loss: 0.00001132
Iteration 219/1000 | Loss: 0.00001132
Iteration 220/1000 | Loss: 0.00001132
Iteration 221/1000 | Loss: 0.00001132
Iteration 222/1000 | Loss: 0.00001132
Iteration 223/1000 | Loss: 0.00001132
Iteration 224/1000 | Loss: 0.00001132
Iteration 225/1000 | Loss: 0.00001132
Iteration 226/1000 | Loss: 0.00001132
Iteration 227/1000 | Loss: 0.00001132
Iteration 228/1000 | Loss: 0.00001132
Iteration 229/1000 | Loss: 0.00001132
Iteration 230/1000 | Loss: 0.00001132
Iteration 231/1000 | Loss: 0.00001132
Iteration 232/1000 | Loss: 0.00001132
Iteration 233/1000 | Loss: 0.00001132
Iteration 234/1000 | Loss: 0.00001132
Iteration 235/1000 | Loss: 0.00001132
Iteration 236/1000 | Loss: 0.00001132
Iteration 237/1000 | Loss: 0.00001132
Iteration 238/1000 | Loss: 0.00001132
Iteration 239/1000 | Loss: 0.00001132
Iteration 240/1000 | Loss: 0.00001132
Iteration 241/1000 | Loss: 0.00001132
Iteration 242/1000 | Loss: 0.00001132
Iteration 243/1000 | Loss: 0.00001132
Iteration 244/1000 | Loss: 0.00001132
Iteration 245/1000 | Loss: 0.00001132
Iteration 246/1000 | Loss: 0.00001132
Iteration 247/1000 | Loss: 0.00001132
Iteration 248/1000 | Loss: 0.00001132
Iteration 249/1000 | Loss: 0.00001132
Iteration 250/1000 | Loss: 0.00001132
Iteration 251/1000 | Loss: 0.00001132
Iteration 252/1000 | Loss: 0.00001132
Iteration 253/1000 | Loss: 0.00001132
Iteration 254/1000 | Loss: 0.00001132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [1.132352917920798e-05, 1.132352917920798e-05, 1.132352917920798e-05, 1.132352917920798e-05, 1.132352917920798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.132352917920798e-05

Optimization complete. Final v2v error: 2.882768154144287 mm

Highest mean error: 3.3058929443359375 mm for frame 69

Lowest mean error: 2.642551898956299 mm for frame 139

Saving results

Total time: 42.497483253479004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015509
Iteration 2/25 | Loss: 0.00197891
Iteration 3/25 | Loss: 0.00177047
Iteration 4/25 | Loss: 0.00128098
Iteration 5/25 | Loss: 0.00125253
Iteration 6/25 | Loss: 0.00125001
Iteration 7/25 | Loss: 0.00125245
Iteration 8/25 | Loss: 0.00123982
Iteration 9/25 | Loss: 0.00124789
Iteration 10/25 | Loss: 0.00124127
Iteration 11/25 | Loss: 0.00122896
Iteration 12/25 | Loss: 0.00122528
Iteration 13/25 | Loss: 0.00123049
Iteration 14/25 | Loss: 0.00122461
Iteration 15/25 | Loss: 0.00122455
Iteration 16/25 | Loss: 0.00122455
Iteration 17/25 | Loss: 0.00122455
Iteration 18/25 | Loss: 0.00122455
Iteration 19/25 | Loss: 0.00122454
Iteration 20/25 | Loss: 0.00122454
Iteration 21/25 | Loss: 0.00122454
Iteration 22/25 | Loss: 0.00122454
Iteration 23/25 | Loss: 0.00122454
Iteration 24/25 | Loss: 0.00122454
Iteration 25/25 | Loss: 0.00122454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31255913
Iteration 2/25 | Loss: 0.00097102
Iteration 3/25 | Loss: 0.00097102
Iteration 4/25 | Loss: 0.00097102
Iteration 5/25 | Loss: 0.00097102
Iteration 6/25 | Loss: 0.00097102
Iteration 7/25 | Loss: 0.00097102
Iteration 8/25 | Loss: 0.00097102
Iteration 9/25 | Loss: 0.00097102
Iteration 10/25 | Loss: 0.00097102
Iteration 11/25 | Loss: 0.00097102
Iteration 12/25 | Loss: 0.00097101
Iteration 13/25 | Loss: 0.00097101
Iteration 14/25 | Loss: 0.00097101
Iteration 15/25 | Loss: 0.00097101
Iteration 16/25 | Loss: 0.00097101
Iteration 17/25 | Loss: 0.00097101
Iteration 18/25 | Loss: 0.00097101
Iteration 19/25 | Loss: 0.00097101
Iteration 20/25 | Loss: 0.00097101
Iteration 21/25 | Loss: 0.00097101
Iteration 22/25 | Loss: 0.00097101
Iteration 23/25 | Loss: 0.00097101
Iteration 24/25 | Loss: 0.00097101
Iteration 25/25 | Loss: 0.00097101

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097101
Iteration 2/1000 | Loss: 0.00003850
Iteration 3/1000 | Loss: 0.00002193
Iteration 4/1000 | Loss: 0.00001911
Iteration 5/1000 | Loss: 0.00001769
Iteration 6/1000 | Loss: 0.00001665
Iteration 7/1000 | Loss: 0.00009010
Iteration 8/1000 | Loss: 0.00001568
Iteration 9/1000 | Loss: 0.00001508
Iteration 10/1000 | Loss: 0.00001464
Iteration 11/1000 | Loss: 0.00001432
Iteration 12/1000 | Loss: 0.00001415
Iteration 13/1000 | Loss: 0.00001400
Iteration 14/1000 | Loss: 0.00001399
Iteration 15/1000 | Loss: 0.00001390
Iteration 16/1000 | Loss: 0.00001386
Iteration 17/1000 | Loss: 0.00001386
Iteration 18/1000 | Loss: 0.00001386
Iteration 19/1000 | Loss: 0.00001386
Iteration 20/1000 | Loss: 0.00001386
Iteration 21/1000 | Loss: 0.00001386
Iteration 22/1000 | Loss: 0.00001386
Iteration 23/1000 | Loss: 0.00001386
Iteration 24/1000 | Loss: 0.00001386
Iteration 25/1000 | Loss: 0.00001386
Iteration 26/1000 | Loss: 0.00001385
Iteration 27/1000 | Loss: 0.00001385
Iteration 28/1000 | Loss: 0.00001385
Iteration 29/1000 | Loss: 0.00001385
Iteration 30/1000 | Loss: 0.00001385
Iteration 31/1000 | Loss: 0.00001385
Iteration 32/1000 | Loss: 0.00001385
Iteration 33/1000 | Loss: 0.00001384
Iteration 34/1000 | Loss: 0.00001383
Iteration 35/1000 | Loss: 0.00001383
Iteration 36/1000 | Loss: 0.00001383
Iteration 37/1000 | Loss: 0.00001383
Iteration 38/1000 | Loss: 0.00001383
Iteration 39/1000 | Loss: 0.00001382
Iteration 40/1000 | Loss: 0.00001381
Iteration 41/1000 | Loss: 0.00001381
Iteration 42/1000 | Loss: 0.00001381
Iteration 43/1000 | Loss: 0.00001381
Iteration 44/1000 | Loss: 0.00001381
Iteration 45/1000 | Loss: 0.00001381
Iteration 46/1000 | Loss: 0.00001381
Iteration 47/1000 | Loss: 0.00001380
Iteration 48/1000 | Loss: 0.00001380
Iteration 49/1000 | Loss: 0.00001379
Iteration 50/1000 | Loss: 0.00001379
Iteration 51/1000 | Loss: 0.00001379
Iteration 52/1000 | Loss: 0.00001378
Iteration 53/1000 | Loss: 0.00001378
Iteration 54/1000 | Loss: 0.00001378
Iteration 55/1000 | Loss: 0.00001377
Iteration 56/1000 | Loss: 0.00001377
Iteration 57/1000 | Loss: 0.00001377
Iteration 58/1000 | Loss: 0.00001377
Iteration 59/1000 | Loss: 0.00001377
Iteration 60/1000 | Loss: 0.00001377
Iteration 61/1000 | Loss: 0.00001377
Iteration 62/1000 | Loss: 0.00001377
Iteration 63/1000 | Loss: 0.00001376
Iteration 64/1000 | Loss: 0.00001376
Iteration 65/1000 | Loss: 0.00001376
Iteration 66/1000 | Loss: 0.00001376
Iteration 67/1000 | Loss: 0.00001376
Iteration 68/1000 | Loss: 0.00001375
Iteration 69/1000 | Loss: 0.00001375
Iteration 70/1000 | Loss: 0.00001375
Iteration 71/1000 | Loss: 0.00001375
Iteration 72/1000 | Loss: 0.00001375
Iteration 73/1000 | Loss: 0.00001374
Iteration 74/1000 | Loss: 0.00001374
Iteration 75/1000 | Loss: 0.00001374
Iteration 76/1000 | Loss: 0.00001373
Iteration 77/1000 | Loss: 0.00001372
Iteration 78/1000 | Loss: 0.00001371
Iteration 79/1000 | Loss: 0.00001371
Iteration 80/1000 | Loss: 0.00001371
Iteration 81/1000 | Loss: 0.00001370
Iteration 82/1000 | Loss: 0.00001370
Iteration 83/1000 | Loss: 0.00001370
Iteration 84/1000 | Loss: 0.00001370
Iteration 85/1000 | Loss: 0.00001369
Iteration 86/1000 | Loss: 0.00001369
Iteration 87/1000 | Loss: 0.00001369
Iteration 88/1000 | Loss: 0.00001369
Iteration 89/1000 | Loss: 0.00001369
Iteration 90/1000 | Loss: 0.00001369
Iteration 91/1000 | Loss: 0.00001369
Iteration 92/1000 | Loss: 0.00001369
Iteration 93/1000 | Loss: 0.00001369
Iteration 94/1000 | Loss: 0.00001369
Iteration 95/1000 | Loss: 0.00001369
Iteration 96/1000 | Loss: 0.00001369
Iteration 97/1000 | Loss: 0.00001369
Iteration 98/1000 | Loss: 0.00001368
Iteration 99/1000 | Loss: 0.00001368
Iteration 100/1000 | Loss: 0.00001368
Iteration 101/1000 | Loss: 0.00001368
Iteration 102/1000 | Loss: 0.00001368
Iteration 103/1000 | Loss: 0.00001368
Iteration 104/1000 | Loss: 0.00001368
Iteration 105/1000 | Loss: 0.00001368
Iteration 106/1000 | Loss: 0.00001367
Iteration 107/1000 | Loss: 0.00001367
Iteration 108/1000 | Loss: 0.00001367
Iteration 109/1000 | Loss: 0.00001367
Iteration 110/1000 | Loss: 0.00001367
Iteration 111/1000 | Loss: 0.00001367
Iteration 112/1000 | Loss: 0.00001367
Iteration 113/1000 | Loss: 0.00001367
Iteration 114/1000 | Loss: 0.00001366
Iteration 115/1000 | Loss: 0.00001366
Iteration 116/1000 | Loss: 0.00001366
Iteration 117/1000 | Loss: 0.00001366
Iteration 118/1000 | Loss: 0.00001366
Iteration 119/1000 | Loss: 0.00001366
Iteration 120/1000 | Loss: 0.00001366
Iteration 121/1000 | Loss: 0.00001366
Iteration 122/1000 | Loss: 0.00001366
Iteration 123/1000 | Loss: 0.00001366
Iteration 124/1000 | Loss: 0.00001365
Iteration 125/1000 | Loss: 0.00001365
Iteration 126/1000 | Loss: 0.00001365
Iteration 127/1000 | Loss: 0.00001365
Iteration 128/1000 | Loss: 0.00001365
Iteration 129/1000 | Loss: 0.00001365
Iteration 130/1000 | Loss: 0.00001365
Iteration 131/1000 | Loss: 0.00001365
Iteration 132/1000 | Loss: 0.00001365
Iteration 133/1000 | Loss: 0.00001365
Iteration 134/1000 | Loss: 0.00001365
Iteration 135/1000 | Loss: 0.00001365
Iteration 136/1000 | Loss: 0.00001365
Iteration 137/1000 | Loss: 0.00001365
Iteration 138/1000 | Loss: 0.00001365
Iteration 139/1000 | Loss: 0.00001365
Iteration 140/1000 | Loss: 0.00001365
Iteration 141/1000 | Loss: 0.00001365
Iteration 142/1000 | Loss: 0.00001365
Iteration 143/1000 | Loss: 0.00001365
Iteration 144/1000 | Loss: 0.00001365
Iteration 145/1000 | Loss: 0.00001365
Iteration 146/1000 | Loss: 0.00001365
Iteration 147/1000 | Loss: 0.00001365
Iteration 148/1000 | Loss: 0.00001365
Iteration 149/1000 | Loss: 0.00001365
Iteration 150/1000 | Loss: 0.00001365
Iteration 151/1000 | Loss: 0.00001365
Iteration 152/1000 | Loss: 0.00001365
Iteration 153/1000 | Loss: 0.00001365
Iteration 154/1000 | Loss: 0.00001365
Iteration 155/1000 | Loss: 0.00001365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.3647243576997425e-05, 1.3647243576997425e-05, 1.3647243576997425e-05, 1.3647243576997425e-05, 1.3647243576997425e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3647243576997425e-05

Optimization complete. Final v2v error: 3.140153169631958 mm

Highest mean error: 3.4601991176605225 mm for frame 110

Lowest mean error: 2.8769538402557373 mm for frame 1

Saving results

Total time: 51.61414813995361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_011/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_011/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015139
Iteration 2/25 | Loss: 0.00231253
Iteration 3/25 | Loss: 0.00170624
Iteration 4/25 | Loss: 0.00151697
Iteration 5/25 | Loss: 0.00145899
Iteration 6/25 | Loss: 0.00139829
Iteration 7/25 | Loss: 0.00139220
Iteration 8/25 | Loss: 0.00136638
Iteration 9/25 | Loss: 0.00134804
Iteration 10/25 | Loss: 0.00133774
Iteration 11/25 | Loss: 0.00133125
Iteration 12/25 | Loss: 0.00132905
Iteration 13/25 | Loss: 0.00132820
Iteration 14/25 | Loss: 0.00132769
Iteration 15/25 | Loss: 0.00132728
Iteration 16/25 | Loss: 0.00132596
Iteration 17/25 | Loss: 0.00132529
Iteration 18/25 | Loss: 0.00132514
Iteration 19/25 | Loss: 0.00132506
Iteration 20/25 | Loss: 0.00132506
Iteration 21/25 | Loss: 0.00132505
Iteration 22/25 | Loss: 0.00132505
Iteration 23/25 | Loss: 0.00132505
Iteration 24/25 | Loss: 0.00132505
Iteration 25/25 | Loss: 0.00132505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32895708
Iteration 2/25 | Loss: 0.00101024
Iteration 3/25 | Loss: 0.00100522
Iteration 4/25 | Loss: 0.00100522
Iteration 5/25 | Loss: 0.00100522
Iteration 6/25 | Loss: 0.00100522
Iteration 7/25 | Loss: 0.00100522
Iteration 8/25 | Loss: 0.00100522
Iteration 9/25 | Loss: 0.00100522
Iteration 10/25 | Loss: 0.00100522
Iteration 11/25 | Loss: 0.00100522
Iteration 12/25 | Loss: 0.00100522
Iteration 13/25 | Loss: 0.00100522
Iteration 14/25 | Loss: 0.00100522
Iteration 15/25 | Loss: 0.00100522
Iteration 16/25 | Loss: 0.00100522
Iteration 17/25 | Loss: 0.00100522
Iteration 18/25 | Loss: 0.00100522
Iteration 19/25 | Loss: 0.00100522
Iteration 20/25 | Loss: 0.00100522
Iteration 21/25 | Loss: 0.00100522
Iteration 22/25 | Loss: 0.00100522
Iteration 23/25 | Loss: 0.00100522
Iteration 24/25 | Loss: 0.00100522
Iteration 25/25 | Loss: 0.00100522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100522
Iteration 2/1000 | Loss: 0.00020762
Iteration 3/1000 | Loss: 0.00014865
Iteration 4/1000 | Loss: 0.00006327
Iteration 5/1000 | Loss: 0.00004932
Iteration 6/1000 | Loss: 0.00013207
Iteration 7/1000 | Loss: 0.00006350
Iteration 8/1000 | Loss: 0.00016889
Iteration 9/1000 | Loss: 0.00022364
Iteration 10/1000 | Loss: 0.00014694
Iteration 11/1000 | Loss: 0.00004363
Iteration 12/1000 | Loss: 0.00003860
Iteration 13/1000 | Loss: 0.00003667
Iteration 14/1000 | Loss: 0.00003505
Iteration 15/1000 | Loss: 0.00003407
Iteration 16/1000 | Loss: 0.00003349
Iteration 17/1000 | Loss: 0.00003272
Iteration 18/1000 | Loss: 0.00003234
Iteration 19/1000 | Loss: 0.00037608
Iteration 20/1000 | Loss: 0.00010586
Iteration 21/1000 | Loss: 0.00003759
Iteration 22/1000 | Loss: 0.00003096
Iteration 23/1000 | Loss: 0.00002815
Iteration 24/1000 | Loss: 0.00002559
Iteration 25/1000 | Loss: 0.00002449
Iteration 26/1000 | Loss: 0.00002360
Iteration 27/1000 | Loss: 0.00002354
Iteration 28/1000 | Loss: 0.00002265
Iteration 29/1000 | Loss: 0.00002221
Iteration 30/1000 | Loss: 0.00002194
Iteration 31/1000 | Loss: 0.00002170
Iteration 32/1000 | Loss: 0.00002151
Iteration 33/1000 | Loss: 0.00002325
Iteration 34/1000 | Loss: 0.00002126
Iteration 35/1000 | Loss: 0.00002125
Iteration 36/1000 | Loss: 0.00002125
Iteration 37/1000 | Loss: 0.00002125
Iteration 38/1000 | Loss: 0.00002125
Iteration 39/1000 | Loss: 0.00002124
Iteration 40/1000 | Loss: 0.00002124
Iteration 41/1000 | Loss: 0.00002124
Iteration 42/1000 | Loss: 0.00002124
Iteration 43/1000 | Loss: 0.00002124
Iteration 44/1000 | Loss: 0.00002123
Iteration 45/1000 | Loss: 0.00002123
Iteration 46/1000 | Loss: 0.00002122
Iteration 47/1000 | Loss: 0.00002122
Iteration 48/1000 | Loss: 0.00002119
Iteration 49/1000 | Loss: 0.00002115
Iteration 50/1000 | Loss: 0.00002115
Iteration 51/1000 | Loss: 0.00002114
Iteration 52/1000 | Loss: 0.00002175
Iteration 53/1000 | Loss: 0.00002111
Iteration 54/1000 | Loss: 0.00002111
Iteration 55/1000 | Loss: 0.00002111
Iteration 56/1000 | Loss: 0.00002111
Iteration 57/1000 | Loss: 0.00002110
Iteration 58/1000 | Loss: 0.00002110
Iteration 59/1000 | Loss: 0.00002110
Iteration 60/1000 | Loss: 0.00002110
Iteration 61/1000 | Loss: 0.00002110
Iteration 62/1000 | Loss: 0.00002110
Iteration 63/1000 | Loss: 0.00002110
Iteration 64/1000 | Loss: 0.00002109
Iteration 65/1000 | Loss: 0.00002109
Iteration 66/1000 | Loss: 0.00002108
Iteration 67/1000 | Loss: 0.00002108
Iteration 68/1000 | Loss: 0.00002108
Iteration 69/1000 | Loss: 0.00002108
Iteration 70/1000 | Loss: 0.00002108
Iteration 71/1000 | Loss: 0.00002107
Iteration 72/1000 | Loss: 0.00002107
Iteration 73/1000 | Loss: 0.00002107
Iteration 74/1000 | Loss: 0.00002106
Iteration 75/1000 | Loss: 0.00002106
Iteration 76/1000 | Loss: 0.00002106
Iteration 77/1000 | Loss: 0.00002106
Iteration 78/1000 | Loss: 0.00002105
Iteration 79/1000 | Loss: 0.00002105
Iteration 80/1000 | Loss: 0.00002104
Iteration 81/1000 | Loss: 0.00002104
Iteration 82/1000 | Loss: 0.00002104
Iteration 83/1000 | Loss: 0.00002103
Iteration 84/1000 | Loss: 0.00002103
Iteration 85/1000 | Loss: 0.00002103
Iteration 86/1000 | Loss: 0.00002103
Iteration 87/1000 | Loss: 0.00002102
Iteration 88/1000 | Loss: 0.00002102
Iteration 89/1000 | Loss: 0.00002102
Iteration 90/1000 | Loss: 0.00002102
Iteration 91/1000 | Loss: 0.00002102
Iteration 92/1000 | Loss: 0.00002101
Iteration 93/1000 | Loss: 0.00002101
Iteration 94/1000 | Loss: 0.00002101
Iteration 95/1000 | Loss: 0.00002101
Iteration 96/1000 | Loss: 0.00002101
Iteration 97/1000 | Loss: 0.00002101
Iteration 98/1000 | Loss: 0.00002101
Iteration 99/1000 | Loss: 0.00002101
Iteration 100/1000 | Loss: 0.00002101
Iteration 101/1000 | Loss: 0.00002101
Iteration 102/1000 | Loss: 0.00002101
Iteration 103/1000 | Loss: 0.00002101
Iteration 104/1000 | Loss: 0.00002100
Iteration 105/1000 | Loss: 0.00002100
Iteration 106/1000 | Loss: 0.00002100
Iteration 107/1000 | Loss: 0.00002100
Iteration 108/1000 | Loss: 0.00002100
Iteration 109/1000 | Loss: 0.00002100
Iteration 110/1000 | Loss: 0.00002100
Iteration 111/1000 | Loss: 0.00002100
Iteration 112/1000 | Loss: 0.00002100
Iteration 113/1000 | Loss: 0.00002100
Iteration 114/1000 | Loss: 0.00002100
Iteration 115/1000 | Loss: 0.00002100
Iteration 116/1000 | Loss: 0.00002100
Iteration 117/1000 | Loss: 0.00002100
Iteration 118/1000 | Loss: 0.00002100
Iteration 119/1000 | Loss: 0.00002100
Iteration 120/1000 | Loss: 0.00002100
Iteration 121/1000 | Loss: 0.00002100
Iteration 122/1000 | Loss: 0.00002100
Iteration 123/1000 | Loss: 0.00002100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.1003172150813043e-05, 2.1003172150813043e-05, 2.1003172150813043e-05, 2.1003172150813043e-05, 2.1003172150813043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1003172150813043e-05

Optimization complete. Final v2v error: 3.819176435470581 mm

Highest mean error: 4.027627468109131 mm for frame 118

Lowest mean error: 3.46612811088562 mm for frame 0

Saving results

Total time: 99.1217896938324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00350980
Iteration 2/25 | Loss: 0.00183420
Iteration 3/25 | Loss: 0.00164789
Iteration 4/25 | Loss: 0.00162221
Iteration 5/25 | Loss: 0.00161231
Iteration 6/25 | Loss: 0.00160909
Iteration 7/25 | Loss: 0.00160842
Iteration 8/25 | Loss: 0.00160842
Iteration 9/25 | Loss: 0.00160842
Iteration 10/25 | Loss: 0.00160842
Iteration 11/25 | Loss: 0.00160842
Iteration 12/25 | Loss: 0.00160842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0016084160888567567, 0.0016084160888567567, 0.0016084160888567567, 0.0016084160888567567, 0.0016084160888567567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016084160888567567

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60154533
Iteration 2/25 | Loss: 0.00267337
Iteration 3/25 | Loss: 0.00267336
Iteration 4/25 | Loss: 0.00267336
Iteration 5/25 | Loss: 0.00267336
Iteration 6/25 | Loss: 0.00267336
Iteration 7/25 | Loss: 0.00267336
Iteration 8/25 | Loss: 0.00267336
Iteration 9/25 | Loss: 0.00267336
Iteration 10/25 | Loss: 0.00267336
Iteration 11/25 | Loss: 0.00267336
Iteration 12/25 | Loss: 0.00267336
Iteration 13/25 | Loss: 0.00267336
Iteration 14/25 | Loss: 0.00267336
Iteration 15/25 | Loss: 0.00267336
Iteration 16/25 | Loss: 0.00267336
Iteration 17/25 | Loss: 0.00267336
Iteration 18/25 | Loss: 0.00267336
Iteration 19/25 | Loss: 0.00267336
Iteration 20/25 | Loss: 0.00267336
Iteration 21/25 | Loss: 0.00267336
Iteration 22/25 | Loss: 0.00267336
Iteration 23/25 | Loss: 0.00267336
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002673361450433731, 0.002673361450433731, 0.002673361450433731, 0.002673361450433731, 0.002673361450433731]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002673361450433731

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00267336
Iteration 2/1000 | Loss: 0.00006835
Iteration 3/1000 | Loss: 0.00004456
Iteration 4/1000 | Loss: 0.00003833
Iteration 5/1000 | Loss: 0.00003382
Iteration 6/1000 | Loss: 0.00003172
Iteration 7/1000 | Loss: 0.00002990
Iteration 8/1000 | Loss: 0.00002895
Iteration 9/1000 | Loss: 0.00002828
Iteration 10/1000 | Loss: 0.00002778
Iteration 11/1000 | Loss: 0.00002731
Iteration 12/1000 | Loss: 0.00002696
Iteration 13/1000 | Loss: 0.00002663
Iteration 14/1000 | Loss: 0.00002640
Iteration 15/1000 | Loss: 0.00002622
Iteration 16/1000 | Loss: 0.00002607
Iteration 17/1000 | Loss: 0.00002604
Iteration 18/1000 | Loss: 0.00002602
Iteration 19/1000 | Loss: 0.00002602
Iteration 20/1000 | Loss: 0.00002600
Iteration 21/1000 | Loss: 0.00002599
Iteration 22/1000 | Loss: 0.00002599
Iteration 23/1000 | Loss: 0.00002598
Iteration 24/1000 | Loss: 0.00002598
Iteration 25/1000 | Loss: 0.00002597
Iteration 26/1000 | Loss: 0.00002594
Iteration 27/1000 | Loss: 0.00002594
Iteration 28/1000 | Loss: 0.00002594
Iteration 29/1000 | Loss: 0.00002593
Iteration 30/1000 | Loss: 0.00002593
Iteration 31/1000 | Loss: 0.00002592
Iteration 32/1000 | Loss: 0.00002592
Iteration 33/1000 | Loss: 0.00002591
Iteration 34/1000 | Loss: 0.00002591
Iteration 35/1000 | Loss: 0.00002591
Iteration 36/1000 | Loss: 0.00002591
Iteration 37/1000 | Loss: 0.00002591
Iteration 38/1000 | Loss: 0.00002590
Iteration 39/1000 | Loss: 0.00002590
Iteration 40/1000 | Loss: 0.00002590
Iteration 41/1000 | Loss: 0.00002589
Iteration 42/1000 | Loss: 0.00002589
Iteration 43/1000 | Loss: 0.00002589
Iteration 44/1000 | Loss: 0.00002589
Iteration 45/1000 | Loss: 0.00002589
Iteration 46/1000 | Loss: 0.00002589
Iteration 47/1000 | Loss: 0.00002589
Iteration 48/1000 | Loss: 0.00002588
Iteration 49/1000 | Loss: 0.00002588
Iteration 50/1000 | Loss: 0.00002588
Iteration 51/1000 | Loss: 0.00002588
Iteration 52/1000 | Loss: 0.00002588
Iteration 53/1000 | Loss: 0.00002587
Iteration 54/1000 | Loss: 0.00002587
Iteration 55/1000 | Loss: 0.00002586
Iteration 56/1000 | Loss: 0.00002586
Iteration 57/1000 | Loss: 0.00002586
Iteration 58/1000 | Loss: 0.00002585
Iteration 59/1000 | Loss: 0.00002585
Iteration 60/1000 | Loss: 0.00002585
Iteration 61/1000 | Loss: 0.00002584
Iteration 62/1000 | Loss: 0.00002584
Iteration 63/1000 | Loss: 0.00002584
Iteration 64/1000 | Loss: 0.00002584
Iteration 65/1000 | Loss: 0.00002583
Iteration 66/1000 | Loss: 0.00002583
Iteration 67/1000 | Loss: 0.00002583
Iteration 68/1000 | Loss: 0.00002583
Iteration 69/1000 | Loss: 0.00002582
Iteration 70/1000 | Loss: 0.00002582
Iteration 71/1000 | Loss: 0.00002582
Iteration 72/1000 | Loss: 0.00002581
Iteration 73/1000 | Loss: 0.00002581
Iteration 74/1000 | Loss: 0.00002580
Iteration 75/1000 | Loss: 0.00002580
Iteration 76/1000 | Loss: 0.00002580
Iteration 77/1000 | Loss: 0.00002579
Iteration 78/1000 | Loss: 0.00002579
Iteration 79/1000 | Loss: 0.00002579
Iteration 80/1000 | Loss: 0.00002579
Iteration 81/1000 | Loss: 0.00002579
Iteration 82/1000 | Loss: 0.00002579
Iteration 83/1000 | Loss: 0.00002578
Iteration 84/1000 | Loss: 0.00002578
Iteration 85/1000 | Loss: 0.00002578
Iteration 86/1000 | Loss: 0.00002578
Iteration 87/1000 | Loss: 0.00002578
Iteration 88/1000 | Loss: 0.00002578
Iteration 89/1000 | Loss: 0.00002578
Iteration 90/1000 | Loss: 0.00002578
Iteration 91/1000 | Loss: 0.00002578
Iteration 92/1000 | Loss: 0.00002577
Iteration 93/1000 | Loss: 0.00002577
Iteration 94/1000 | Loss: 0.00002577
Iteration 95/1000 | Loss: 0.00002577
Iteration 96/1000 | Loss: 0.00002577
Iteration 97/1000 | Loss: 0.00002577
Iteration 98/1000 | Loss: 0.00002577
Iteration 99/1000 | Loss: 0.00002576
Iteration 100/1000 | Loss: 0.00002576
Iteration 101/1000 | Loss: 0.00002576
Iteration 102/1000 | Loss: 0.00002576
Iteration 103/1000 | Loss: 0.00002576
Iteration 104/1000 | Loss: 0.00002576
Iteration 105/1000 | Loss: 0.00002576
Iteration 106/1000 | Loss: 0.00002576
Iteration 107/1000 | Loss: 0.00002575
Iteration 108/1000 | Loss: 0.00002575
Iteration 109/1000 | Loss: 0.00002575
Iteration 110/1000 | Loss: 0.00002574
Iteration 111/1000 | Loss: 0.00002574
Iteration 112/1000 | Loss: 0.00002574
Iteration 113/1000 | Loss: 0.00002574
Iteration 114/1000 | Loss: 0.00002573
Iteration 115/1000 | Loss: 0.00002573
Iteration 116/1000 | Loss: 0.00002573
Iteration 117/1000 | Loss: 0.00002572
Iteration 118/1000 | Loss: 0.00002572
Iteration 119/1000 | Loss: 0.00002571
Iteration 120/1000 | Loss: 0.00002571
Iteration 121/1000 | Loss: 0.00002571
Iteration 122/1000 | Loss: 0.00002570
Iteration 123/1000 | Loss: 0.00002570
Iteration 124/1000 | Loss: 0.00002570
Iteration 125/1000 | Loss: 0.00002569
Iteration 126/1000 | Loss: 0.00002569
Iteration 127/1000 | Loss: 0.00002568
Iteration 128/1000 | Loss: 0.00002568
Iteration 129/1000 | Loss: 0.00002568
Iteration 130/1000 | Loss: 0.00002568
Iteration 131/1000 | Loss: 0.00002567
Iteration 132/1000 | Loss: 0.00002567
Iteration 133/1000 | Loss: 0.00002567
Iteration 134/1000 | Loss: 0.00002567
Iteration 135/1000 | Loss: 0.00002566
Iteration 136/1000 | Loss: 0.00002566
Iteration 137/1000 | Loss: 0.00002566
Iteration 138/1000 | Loss: 0.00002566
Iteration 139/1000 | Loss: 0.00002565
Iteration 140/1000 | Loss: 0.00002565
Iteration 141/1000 | Loss: 0.00002565
Iteration 142/1000 | Loss: 0.00002565
Iteration 143/1000 | Loss: 0.00002565
Iteration 144/1000 | Loss: 0.00002564
Iteration 145/1000 | Loss: 0.00002564
Iteration 146/1000 | Loss: 0.00002564
Iteration 147/1000 | Loss: 0.00002564
Iteration 148/1000 | Loss: 0.00002564
Iteration 149/1000 | Loss: 0.00002564
Iteration 150/1000 | Loss: 0.00002564
Iteration 151/1000 | Loss: 0.00002563
Iteration 152/1000 | Loss: 0.00002563
Iteration 153/1000 | Loss: 0.00002563
Iteration 154/1000 | Loss: 0.00002563
Iteration 155/1000 | Loss: 0.00002562
Iteration 156/1000 | Loss: 0.00002562
Iteration 157/1000 | Loss: 0.00002562
Iteration 158/1000 | Loss: 0.00002562
Iteration 159/1000 | Loss: 0.00002562
Iteration 160/1000 | Loss: 0.00002561
Iteration 161/1000 | Loss: 0.00002561
Iteration 162/1000 | Loss: 0.00002561
Iteration 163/1000 | Loss: 0.00002561
Iteration 164/1000 | Loss: 0.00002561
Iteration 165/1000 | Loss: 0.00002561
Iteration 166/1000 | Loss: 0.00002561
Iteration 167/1000 | Loss: 0.00002561
Iteration 168/1000 | Loss: 0.00002561
Iteration 169/1000 | Loss: 0.00002561
Iteration 170/1000 | Loss: 0.00002561
Iteration 171/1000 | Loss: 0.00002561
Iteration 172/1000 | Loss: 0.00002561
Iteration 173/1000 | Loss: 0.00002561
Iteration 174/1000 | Loss: 0.00002561
Iteration 175/1000 | Loss: 0.00002561
Iteration 176/1000 | Loss: 0.00002561
Iteration 177/1000 | Loss: 0.00002561
Iteration 178/1000 | Loss: 0.00002561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.5610799639252946e-05, 2.5610799639252946e-05, 2.5610799639252946e-05, 2.5610799639252946e-05, 2.5610799639252946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5610799639252946e-05

Optimization complete. Final v2v error: 4.506351470947266 mm

Highest mean error: 4.685682773590088 mm for frame 136

Lowest mean error: 4.250239372253418 mm for frame 57

Saving results

Total time: 51.0357620716095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00550391
Iteration 2/25 | Loss: 0.00167052
Iteration 3/25 | Loss: 0.00160391
Iteration 4/25 | Loss: 0.00159424
Iteration 5/25 | Loss: 0.00159096
Iteration 6/25 | Loss: 0.00158999
Iteration 7/25 | Loss: 0.00158999
Iteration 8/25 | Loss: 0.00158999
Iteration 9/25 | Loss: 0.00158999
Iteration 10/25 | Loss: 0.00158999
Iteration 11/25 | Loss: 0.00158999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001589993597008288, 0.001589993597008288, 0.001589993597008288, 0.001589993597008288, 0.001589993597008288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001589993597008288

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.27750587
Iteration 2/25 | Loss: 0.00212332
Iteration 3/25 | Loss: 0.00212332
Iteration 4/25 | Loss: 0.00212332
Iteration 5/25 | Loss: 0.00212331
Iteration 6/25 | Loss: 0.00212331
Iteration 7/25 | Loss: 0.00212331
Iteration 8/25 | Loss: 0.00212331
Iteration 9/25 | Loss: 0.00212331
Iteration 10/25 | Loss: 0.00212331
Iteration 11/25 | Loss: 0.00212331
Iteration 12/25 | Loss: 0.00212331
Iteration 13/25 | Loss: 0.00212331
Iteration 14/25 | Loss: 0.00212331
Iteration 15/25 | Loss: 0.00212331
Iteration 16/25 | Loss: 0.00212331
Iteration 17/25 | Loss: 0.00212331
Iteration 18/25 | Loss: 0.00212331
Iteration 19/25 | Loss: 0.00212331
Iteration 20/25 | Loss: 0.00212331
Iteration 21/25 | Loss: 0.00212331
Iteration 22/25 | Loss: 0.00212331
Iteration 23/25 | Loss: 0.00212331
Iteration 24/25 | Loss: 0.00212331
Iteration 25/25 | Loss: 0.00212331

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212331
Iteration 2/1000 | Loss: 0.00004790
Iteration 3/1000 | Loss: 0.00003221
Iteration 4/1000 | Loss: 0.00002801
Iteration 5/1000 | Loss: 0.00002581
Iteration 6/1000 | Loss: 0.00002467
Iteration 7/1000 | Loss: 0.00002403
Iteration 8/1000 | Loss: 0.00002369
Iteration 9/1000 | Loss: 0.00002344
Iteration 10/1000 | Loss: 0.00002322
Iteration 11/1000 | Loss: 0.00002312
Iteration 12/1000 | Loss: 0.00002311
Iteration 13/1000 | Loss: 0.00002297
Iteration 14/1000 | Loss: 0.00002281
Iteration 15/1000 | Loss: 0.00002275
Iteration 16/1000 | Loss: 0.00002275
Iteration 17/1000 | Loss: 0.00002273
Iteration 18/1000 | Loss: 0.00002272
Iteration 19/1000 | Loss: 0.00002272
Iteration 20/1000 | Loss: 0.00002271
Iteration 21/1000 | Loss: 0.00002271
Iteration 22/1000 | Loss: 0.00002271
Iteration 23/1000 | Loss: 0.00002271
Iteration 24/1000 | Loss: 0.00002271
Iteration 25/1000 | Loss: 0.00002271
Iteration 26/1000 | Loss: 0.00002271
Iteration 27/1000 | Loss: 0.00002270
Iteration 28/1000 | Loss: 0.00002270
Iteration 29/1000 | Loss: 0.00002270
Iteration 30/1000 | Loss: 0.00002270
Iteration 31/1000 | Loss: 0.00002269
Iteration 32/1000 | Loss: 0.00002269
Iteration 33/1000 | Loss: 0.00002269
Iteration 34/1000 | Loss: 0.00002269
Iteration 35/1000 | Loss: 0.00002269
Iteration 36/1000 | Loss: 0.00002269
Iteration 37/1000 | Loss: 0.00002268
Iteration 38/1000 | Loss: 0.00002268
Iteration 39/1000 | Loss: 0.00002268
Iteration 40/1000 | Loss: 0.00002267
Iteration 41/1000 | Loss: 0.00002266
Iteration 42/1000 | Loss: 0.00002265
Iteration 43/1000 | Loss: 0.00002265
Iteration 44/1000 | Loss: 0.00002264
Iteration 45/1000 | Loss: 0.00002264
Iteration 46/1000 | Loss: 0.00002264
Iteration 47/1000 | Loss: 0.00002263
Iteration 48/1000 | Loss: 0.00002263
Iteration 49/1000 | Loss: 0.00002262
Iteration 50/1000 | Loss: 0.00002262
Iteration 51/1000 | Loss: 0.00002262
Iteration 52/1000 | Loss: 0.00002262
Iteration 53/1000 | Loss: 0.00002261
Iteration 54/1000 | Loss: 0.00002261
Iteration 55/1000 | Loss: 0.00002260
Iteration 56/1000 | Loss: 0.00002260
Iteration 57/1000 | Loss: 0.00002260
Iteration 58/1000 | Loss: 0.00002260
Iteration 59/1000 | Loss: 0.00002260
Iteration 60/1000 | Loss: 0.00002259
Iteration 61/1000 | Loss: 0.00002259
Iteration 62/1000 | Loss: 0.00002259
Iteration 63/1000 | Loss: 0.00002258
Iteration 64/1000 | Loss: 0.00002258
Iteration 65/1000 | Loss: 0.00002258
Iteration 66/1000 | Loss: 0.00002258
Iteration 67/1000 | Loss: 0.00002257
Iteration 68/1000 | Loss: 0.00002257
Iteration 69/1000 | Loss: 0.00002257
Iteration 70/1000 | Loss: 0.00002257
Iteration 71/1000 | Loss: 0.00002256
Iteration 72/1000 | Loss: 0.00002256
Iteration 73/1000 | Loss: 0.00002256
Iteration 74/1000 | Loss: 0.00002256
Iteration 75/1000 | Loss: 0.00002256
Iteration 76/1000 | Loss: 0.00002256
Iteration 77/1000 | Loss: 0.00002256
Iteration 78/1000 | Loss: 0.00002256
Iteration 79/1000 | Loss: 0.00002256
Iteration 80/1000 | Loss: 0.00002256
Iteration 81/1000 | Loss: 0.00002256
Iteration 82/1000 | Loss: 0.00002255
Iteration 83/1000 | Loss: 0.00002255
Iteration 84/1000 | Loss: 0.00002255
Iteration 85/1000 | Loss: 0.00002255
Iteration 86/1000 | Loss: 0.00002255
Iteration 87/1000 | Loss: 0.00002255
Iteration 88/1000 | Loss: 0.00002255
Iteration 89/1000 | Loss: 0.00002255
Iteration 90/1000 | Loss: 0.00002254
Iteration 91/1000 | Loss: 0.00002254
Iteration 92/1000 | Loss: 0.00002254
Iteration 93/1000 | Loss: 0.00002254
Iteration 94/1000 | Loss: 0.00002254
Iteration 95/1000 | Loss: 0.00002254
Iteration 96/1000 | Loss: 0.00002254
Iteration 97/1000 | Loss: 0.00002254
Iteration 98/1000 | Loss: 0.00002254
Iteration 99/1000 | Loss: 0.00002254
Iteration 100/1000 | Loss: 0.00002254
Iteration 101/1000 | Loss: 0.00002253
Iteration 102/1000 | Loss: 0.00002253
Iteration 103/1000 | Loss: 0.00002253
Iteration 104/1000 | Loss: 0.00002253
Iteration 105/1000 | Loss: 0.00002253
Iteration 106/1000 | Loss: 0.00002252
Iteration 107/1000 | Loss: 0.00002252
Iteration 108/1000 | Loss: 0.00002252
Iteration 109/1000 | Loss: 0.00002252
Iteration 110/1000 | Loss: 0.00002252
Iteration 111/1000 | Loss: 0.00002252
Iteration 112/1000 | Loss: 0.00002252
Iteration 113/1000 | Loss: 0.00002252
Iteration 114/1000 | Loss: 0.00002252
Iteration 115/1000 | Loss: 0.00002251
Iteration 116/1000 | Loss: 0.00002251
Iteration 117/1000 | Loss: 0.00002251
Iteration 118/1000 | Loss: 0.00002251
Iteration 119/1000 | Loss: 0.00002251
Iteration 120/1000 | Loss: 0.00002251
Iteration 121/1000 | Loss: 0.00002251
Iteration 122/1000 | Loss: 0.00002251
Iteration 123/1000 | Loss: 0.00002251
Iteration 124/1000 | Loss: 0.00002251
Iteration 125/1000 | Loss: 0.00002251
Iteration 126/1000 | Loss: 0.00002251
Iteration 127/1000 | Loss: 0.00002251
Iteration 128/1000 | Loss: 0.00002251
Iteration 129/1000 | Loss: 0.00002251
Iteration 130/1000 | Loss: 0.00002251
Iteration 131/1000 | Loss: 0.00002251
Iteration 132/1000 | Loss: 0.00002251
Iteration 133/1000 | Loss: 0.00002251
Iteration 134/1000 | Loss: 0.00002251
Iteration 135/1000 | Loss: 0.00002251
Iteration 136/1000 | Loss: 0.00002251
Iteration 137/1000 | Loss: 0.00002251
Iteration 138/1000 | Loss: 0.00002251
Iteration 139/1000 | Loss: 0.00002251
Iteration 140/1000 | Loss: 0.00002251
Iteration 141/1000 | Loss: 0.00002251
Iteration 142/1000 | Loss: 0.00002251
Iteration 143/1000 | Loss: 0.00002251
Iteration 144/1000 | Loss: 0.00002251
Iteration 145/1000 | Loss: 0.00002251
Iteration 146/1000 | Loss: 0.00002251
Iteration 147/1000 | Loss: 0.00002251
Iteration 148/1000 | Loss: 0.00002251
Iteration 149/1000 | Loss: 0.00002251
Iteration 150/1000 | Loss: 0.00002251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.2508203983306885e-05, 2.2508203983306885e-05, 2.2508203983306885e-05, 2.2508203983306885e-05, 2.2508203983306885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2508203983306885e-05

Optimization complete. Final v2v error: 4.102692604064941 mm

Highest mean error: 4.5120062828063965 mm for frame 46

Lowest mean error: 3.896322727203369 mm for frame 137

Saving results

Total time: 35.53721284866333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032628
Iteration 2/25 | Loss: 0.00205231
Iteration 3/25 | Loss: 0.00170543
Iteration 4/25 | Loss: 0.00167247
Iteration 5/25 | Loss: 0.00166579
Iteration 6/25 | Loss: 0.00166417
Iteration 7/25 | Loss: 0.00166409
Iteration 8/25 | Loss: 0.00166409
Iteration 9/25 | Loss: 0.00166409
Iteration 10/25 | Loss: 0.00166409
Iteration 11/25 | Loss: 0.00166409
Iteration 12/25 | Loss: 0.00166409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0016640890389680862, 0.0016640890389680862, 0.0016640890389680862, 0.0016640890389680862, 0.0016640890389680862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016640890389680862

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.67418283
Iteration 2/25 | Loss: 0.00223295
Iteration 3/25 | Loss: 0.00223295
Iteration 4/25 | Loss: 0.00223295
Iteration 5/25 | Loss: 0.00223295
Iteration 6/25 | Loss: 0.00223295
Iteration 7/25 | Loss: 0.00223295
Iteration 8/25 | Loss: 0.00223295
Iteration 9/25 | Loss: 0.00223295
Iteration 10/25 | Loss: 0.00223295
Iteration 11/25 | Loss: 0.00223295
Iteration 12/25 | Loss: 0.00223295
Iteration 13/25 | Loss: 0.00223295
Iteration 14/25 | Loss: 0.00223295
Iteration 15/25 | Loss: 0.00223295
Iteration 16/25 | Loss: 0.00223295
Iteration 17/25 | Loss: 0.00223295
Iteration 18/25 | Loss: 0.00223295
Iteration 19/25 | Loss: 0.00223295
Iteration 20/25 | Loss: 0.00223295
Iteration 21/25 | Loss: 0.00223295
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002232946455478668, 0.002232946455478668, 0.002232946455478668, 0.002232946455478668, 0.002232946455478668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002232946455478668

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223295
Iteration 2/1000 | Loss: 0.00010267
Iteration 3/1000 | Loss: 0.00006519
Iteration 4/1000 | Loss: 0.00005468
Iteration 5/1000 | Loss: 0.00005047
Iteration 6/1000 | Loss: 0.00004747
Iteration 7/1000 | Loss: 0.00004642
Iteration 8/1000 | Loss: 0.00004569
Iteration 9/1000 | Loss: 0.00004534
Iteration 10/1000 | Loss: 0.00004498
Iteration 11/1000 | Loss: 0.00004467
Iteration 12/1000 | Loss: 0.00004444
Iteration 13/1000 | Loss: 0.00004423
Iteration 14/1000 | Loss: 0.00004404
Iteration 15/1000 | Loss: 0.00004396
Iteration 16/1000 | Loss: 0.00004388
Iteration 17/1000 | Loss: 0.00004385
Iteration 18/1000 | Loss: 0.00004384
Iteration 19/1000 | Loss: 0.00004384
Iteration 20/1000 | Loss: 0.00004383
Iteration 21/1000 | Loss: 0.00004380
Iteration 22/1000 | Loss: 0.00004379
Iteration 23/1000 | Loss: 0.00004379
Iteration 24/1000 | Loss: 0.00004376
Iteration 25/1000 | Loss: 0.00004375
Iteration 26/1000 | Loss: 0.00004375
Iteration 27/1000 | Loss: 0.00004375
Iteration 28/1000 | Loss: 0.00004375
Iteration 29/1000 | Loss: 0.00004375
Iteration 30/1000 | Loss: 0.00004374
Iteration 31/1000 | Loss: 0.00004373
Iteration 32/1000 | Loss: 0.00004373
Iteration 33/1000 | Loss: 0.00004373
Iteration 34/1000 | Loss: 0.00004372
Iteration 35/1000 | Loss: 0.00004372
Iteration 36/1000 | Loss: 0.00004372
Iteration 37/1000 | Loss: 0.00004372
Iteration 38/1000 | Loss: 0.00004372
Iteration 39/1000 | Loss: 0.00004372
Iteration 40/1000 | Loss: 0.00004372
Iteration 41/1000 | Loss: 0.00004372
Iteration 42/1000 | Loss: 0.00004371
Iteration 43/1000 | Loss: 0.00004370
Iteration 44/1000 | Loss: 0.00004370
Iteration 45/1000 | Loss: 0.00004369
Iteration 46/1000 | Loss: 0.00004369
Iteration 47/1000 | Loss: 0.00004368
Iteration 48/1000 | Loss: 0.00004368
Iteration 49/1000 | Loss: 0.00004368
Iteration 50/1000 | Loss: 0.00004368
Iteration 51/1000 | Loss: 0.00004368
Iteration 52/1000 | Loss: 0.00004368
Iteration 53/1000 | Loss: 0.00004367
Iteration 54/1000 | Loss: 0.00004367
Iteration 55/1000 | Loss: 0.00004367
Iteration 56/1000 | Loss: 0.00004366
Iteration 57/1000 | Loss: 0.00004366
Iteration 58/1000 | Loss: 0.00004366
Iteration 59/1000 | Loss: 0.00004366
Iteration 60/1000 | Loss: 0.00004366
Iteration 61/1000 | Loss: 0.00004366
Iteration 62/1000 | Loss: 0.00004366
Iteration 63/1000 | Loss: 0.00004365
Iteration 64/1000 | Loss: 0.00004365
Iteration 65/1000 | Loss: 0.00004365
Iteration 66/1000 | Loss: 0.00004365
Iteration 67/1000 | Loss: 0.00004365
Iteration 68/1000 | Loss: 0.00004365
Iteration 69/1000 | Loss: 0.00004365
Iteration 70/1000 | Loss: 0.00004365
Iteration 71/1000 | Loss: 0.00004364
Iteration 72/1000 | Loss: 0.00004364
Iteration 73/1000 | Loss: 0.00004364
Iteration 74/1000 | Loss: 0.00004364
Iteration 75/1000 | Loss: 0.00004364
Iteration 76/1000 | Loss: 0.00004364
Iteration 77/1000 | Loss: 0.00004364
Iteration 78/1000 | Loss: 0.00004363
Iteration 79/1000 | Loss: 0.00004363
Iteration 80/1000 | Loss: 0.00004363
Iteration 81/1000 | Loss: 0.00004363
Iteration 82/1000 | Loss: 0.00004363
Iteration 83/1000 | Loss: 0.00004363
Iteration 84/1000 | Loss: 0.00004363
Iteration 85/1000 | Loss: 0.00004363
Iteration 86/1000 | Loss: 0.00004362
Iteration 87/1000 | Loss: 0.00004362
Iteration 88/1000 | Loss: 0.00004362
Iteration 89/1000 | Loss: 0.00004362
Iteration 90/1000 | Loss: 0.00004362
Iteration 91/1000 | Loss: 0.00004362
Iteration 92/1000 | Loss: 0.00004362
Iteration 93/1000 | Loss: 0.00004362
Iteration 94/1000 | Loss: 0.00004362
Iteration 95/1000 | Loss: 0.00004362
Iteration 96/1000 | Loss: 0.00004362
Iteration 97/1000 | Loss: 0.00004362
Iteration 98/1000 | Loss: 0.00004362
Iteration 99/1000 | Loss: 0.00004362
Iteration 100/1000 | Loss: 0.00004362
Iteration 101/1000 | Loss: 0.00004362
Iteration 102/1000 | Loss: 0.00004362
Iteration 103/1000 | Loss: 0.00004362
Iteration 104/1000 | Loss: 0.00004362
Iteration 105/1000 | Loss: 0.00004362
Iteration 106/1000 | Loss: 0.00004362
Iteration 107/1000 | Loss: 0.00004362
Iteration 108/1000 | Loss: 0.00004362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [4.361683750175871e-05, 4.361683750175871e-05, 4.361683750175871e-05, 4.361683750175871e-05, 4.361683750175871e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.361683750175871e-05

Optimization complete. Final v2v error: 5.7161455154418945 mm

Highest mean error: 5.989701271057129 mm for frame 22

Lowest mean error: 5.438562870025635 mm for frame 53

Saving results

Total time: 36.71546173095703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412548
Iteration 2/25 | Loss: 0.00190320
Iteration 3/25 | Loss: 0.00167667
Iteration 4/25 | Loss: 0.00161388
Iteration 5/25 | Loss: 0.00160406
Iteration 6/25 | Loss: 0.00159984
Iteration 7/25 | Loss: 0.00159861
Iteration 8/25 | Loss: 0.00159821
Iteration 9/25 | Loss: 0.00159815
Iteration 10/25 | Loss: 0.00159815
Iteration 11/25 | Loss: 0.00159815
Iteration 12/25 | Loss: 0.00159815
Iteration 13/25 | Loss: 0.00159815
Iteration 14/25 | Loss: 0.00159815
Iteration 15/25 | Loss: 0.00159815
Iteration 16/25 | Loss: 0.00159815
Iteration 17/25 | Loss: 0.00159815
Iteration 18/25 | Loss: 0.00159815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015981508186087012, 0.0015981508186087012, 0.0015981508186087012, 0.0015981508186087012, 0.0015981508186087012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015981508186087012

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59973538
Iteration 2/25 | Loss: 0.00270416
Iteration 3/25 | Loss: 0.00270416
Iteration 4/25 | Loss: 0.00270416
Iteration 5/25 | Loss: 0.00270416
Iteration 6/25 | Loss: 0.00270416
Iteration 7/25 | Loss: 0.00270416
Iteration 8/25 | Loss: 0.00270416
Iteration 9/25 | Loss: 0.00270416
Iteration 10/25 | Loss: 0.00270416
Iteration 11/25 | Loss: 0.00270416
Iteration 12/25 | Loss: 0.00270416
Iteration 13/25 | Loss: 0.00270416
Iteration 14/25 | Loss: 0.00270416
Iteration 15/25 | Loss: 0.00270416
Iteration 16/25 | Loss: 0.00270416
Iteration 17/25 | Loss: 0.00270416
Iteration 18/25 | Loss: 0.00270416
Iteration 19/25 | Loss: 0.00270416
Iteration 20/25 | Loss: 0.00270416
Iteration 21/25 | Loss: 0.00270416
Iteration 22/25 | Loss: 0.00270416
Iteration 23/25 | Loss: 0.00270416
Iteration 24/25 | Loss: 0.00270416
Iteration 25/25 | Loss: 0.00270416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00270416
Iteration 2/1000 | Loss: 0.00007448
Iteration 3/1000 | Loss: 0.00004759
Iteration 4/1000 | Loss: 0.00003805
Iteration 5/1000 | Loss: 0.00003347
Iteration 6/1000 | Loss: 0.00003114
Iteration 7/1000 | Loss: 0.00002943
Iteration 8/1000 | Loss: 0.00002857
Iteration 9/1000 | Loss: 0.00002793
Iteration 10/1000 | Loss: 0.00002751
Iteration 11/1000 | Loss: 0.00002725
Iteration 12/1000 | Loss: 0.00002699
Iteration 13/1000 | Loss: 0.00002671
Iteration 14/1000 | Loss: 0.00002651
Iteration 15/1000 | Loss: 0.00002633
Iteration 16/1000 | Loss: 0.00002628
Iteration 17/1000 | Loss: 0.00002628
Iteration 18/1000 | Loss: 0.00002625
Iteration 19/1000 | Loss: 0.00002625
Iteration 20/1000 | Loss: 0.00002623
Iteration 21/1000 | Loss: 0.00002623
Iteration 22/1000 | Loss: 0.00002623
Iteration 23/1000 | Loss: 0.00002622
Iteration 24/1000 | Loss: 0.00002622
Iteration 25/1000 | Loss: 0.00002621
Iteration 26/1000 | Loss: 0.00002621
Iteration 27/1000 | Loss: 0.00002621
Iteration 28/1000 | Loss: 0.00002620
Iteration 29/1000 | Loss: 0.00002620
Iteration 30/1000 | Loss: 0.00002619
Iteration 31/1000 | Loss: 0.00002619
Iteration 32/1000 | Loss: 0.00002619
Iteration 33/1000 | Loss: 0.00002618
Iteration 34/1000 | Loss: 0.00002618
Iteration 35/1000 | Loss: 0.00002617
Iteration 36/1000 | Loss: 0.00002617
Iteration 37/1000 | Loss: 0.00002617
Iteration 38/1000 | Loss: 0.00002617
Iteration 39/1000 | Loss: 0.00002616
Iteration 40/1000 | Loss: 0.00002616
Iteration 41/1000 | Loss: 0.00002616
Iteration 42/1000 | Loss: 0.00002615
Iteration 43/1000 | Loss: 0.00002615
Iteration 44/1000 | Loss: 0.00002614
Iteration 45/1000 | Loss: 0.00002614
Iteration 46/1000 | Loss: 0.00002614
Iteration 47/1000 | Loss: 0.00002614
Iteration 48/1000 | Loss: 0.00002613
Iteration 49/1000 | Loss: 0.00002613
Iteration 50/1000 | Loss: 0.00002612
Iteration 51/1000 | Loss: 0.00002610
Iteration 52/1000 | Loss: 0.00002607
Iteration 53/1000 | Loss: 0.00002607
Iteration 54/1000 | Loss: 0.00002606
Iteration 55/1000 | Loss: 0.00002606
Iteration 56/1000 | Loss: 0.00002606
Iteration 57/1000 | Loss: 0.00002605
Iteration 58/1000 | Loss: 0.00002605
Iteration 59/1000 | Loss: 0.00002605
Iteration 60/1000 | Loss: 0.00002604
Iteration 61/1000 | Loss: 0.00002604
Iteration 62/1000 | Loss: 0.00002604
Iteration 63/1000 | Loss: 0.00002603
Iteration 64/1000 | Loss: 0.00002603
Iteration 65/1000 | Loss: 0.00002603
Iteration 66/1000 | Loss: 0.00002602
Iteration 67/1000 | Loss: 0.00002602
Iteration 68/1000 | Loss: 0.00002602
Iteration 69/1000 | Loss: 0.00002602
Iteration 70/1000 | Loss: 0.00002601
Iteration 71/1000 | Loss: 0.00002601
Iteration 72/1000 | Loss: 0.00002601
Iteration 73/1000 | Loss: 0.00002601
Iteration 74/1000 | Loss: 0.00002600
Iteration 75/1000 | Loss: 0.00002600
Iteration 76/1000 | Loss: 0.00002600
Iteration 77/1000 | Loss: 0.00002600
Iteration 78/1000 | Loss: 0.00002600
Iteration 79/1000 | Loss: 0.00002600
Iteration 80/1000 | Loss: 0.00002600
Iteration 81/1000 | Loss: 0.00002600
Iteration 82/1000 | Loss: 0.00002600
Iteration 83/1000 | Loss: 0.00002600
Iteration 84/1000 | Loss: 0.00002600
Iteration 85/1000 | Loss: 0.00002599
Iteration 86/1000 | Loss: 0.00002599
Iteration 87/1000 | Loss: 0.00002599
Iteration 88/1000 | Loss: 0.00002599
Iteration 89/1000 | Loss: 0.00002599
Iteration 90/1000 | Loss: 0.00002598
Iteration 91/1000 | Loss: 0.00002598
Iteration 92/1000 | Loss: 0.00002598
Iteration 93/1000 | Loss: 0.00002598
Iteration 94/1000 | Loss: 0.00002598
Iteration 95/1000 | Loss: 0.00002598
Iteration 96/1000 | Loss: 0.00002597
Iteration 97/1000 | Loss: 0.00002597
Iteration 98/1000 | Loss: 0.00002597
Iteration 99/1000 | Loss: 0.00002597
Iteration 100/1000 | Loss: 0.00002597
Iteration 101/1000 | Loss: 0.00002597
Iteration 102/1000 | Loss: 0.00002597
Iteration 103/1000 | Loss: 0.00002597
Iteration 104/1000 | Loss: 0.00002597
Iteration 105/1000 | Loss: 0.00002596
Iteration 106/1000 | Loss: 0.00002596
Iteration 107/1000 | Loss: 0.00002596
Iteration 108/1000 | Loss: 0.00002596
Iteration 109/1000 | Loss: 0.00002596
Iteration 110/1000 | Loss: 0.00002596
Iteration 111/1000 | Loss: 0.00002596
Iteration 112/1000 | Loss: 0.00002596
Iteration 113/1000 | Loss: 0.00002596
Iteration 114/1000 | Loss: 0.00002596
Iteration 115/1000 | Loss: 0.00002595
Iteration 116/1000 | Loss: 0.00002595
Iteration 117/1000 | Loss: 0.00002595
Iteration 118/1000 | Loss: 0.00002595
Iteration 119/1000 | Loss: 0.00002595
Iteration 120/1000 | Loss: 0.00002595
Iteration 121/1000 | Loss: 0.00002595
Iteration 122/1000 | Loss: 0.00002594
Iteration 123/1000 | Loss: 0.00002594
Iteration 124/1000 | Loss: 0.00002594
Iteration 125/1000 | Loss: 0.00002594
Iteration 126/1000 | Loss: 0.00002594
Iteration 127/1000 | Loss: 0.00002594
Iteration 128/1000 | Loss: 0.00002594
Iteration 129/1000 | Loss: 0.00002594
Iteration 130/1000 | Loss: 0.00002593
Iteration 131/1000 | Loss: 0.00002593
Iteration 132/1000 | Loss: 0.00002593
Iteration 133/1000 | Loss: 0.00002593
Iteration 134/1000 | Loss: 0.00002593
Iteration 135/1000 | Loss: 0.00002593
Iteration 136/1000 | Loss: 0.00002593
Iteration 137/1000 | Loss: 0.00002593
Iteration 138/1000 | Loss: 0.00002593
Iteration 139/1000 | Loss: 0.00002593
Iteration 140/1000 | Loss: 0.00002593
Iteration 141/1000 | Loss: 0.00002593
Iteration 142/1000 | Loss: 0.00002593
Iteration 143/1000 | Loss: 0.00002592
Iteration 144/1000 | Loss: 0.00002592
Iteration 145/1000 | Loss: 0.00002592
Iteration 146/1000 | Loss: 0.00002592
Iteration 147/1000 | Loss: 0.00002592
Iteration 148/1000 | Loss: 0.00002592
Iteration 149/1000 | Loss: 0.00002592
Iteration 150/1000 | Loss: 0.00002592
Iteration 151/1000 | Loss: 0.00002592
Iteration 152/1000 | Loss: 0.00002592
Iteration 153/1000 | Loss: 0.00002592
Iteration 154/1000 | Loss: 0.00002592
Iteration 155/1000 | Loss: 0.00002591
Iteration 156/1000 | Loss: 0.00002591
Iteration 157/1000 | Loss: 0.00002591
Iteration 158/1000 | Loss: 0.00002591
Iteration 159/1000 | Loss: 0.00002591
Iteration 160/1000 | Loss: 0.00002591
Iteration 161/1000 | Loss: 0.00002591
Iteration 162/1000 | Loss: 0.00002591
Iteration 163/1000 | Loss: 0.00002591
Iteration 164/1000 | Loss: 0.00002591
Iteration 165/1000 | Loss: 0.00002591
Iteration 166/1000 | Loss: 0.00002591
Iteration 167/1000 | Loss: 0.00002591
Iteration 168/1000 | Loss: 0.00002590
Iteration 169/1000 | Loss: 0.00002590
Iteration 170/1000 | Loss: 0.00002590
Iteration 171/1000 | Loss: 0.00002590
Iteration 172/1000 | Loss: 0.00002590
Iteration 173/1000 | Loss: 0.00002590
Iteration 174/1000 | Loss: 0.00002590
Iteration 175/1000 | Loss: 0.00002590
Iteration 176/1000 | Loss: 0.00002590
Iteration 177/1000 | Loss: 0.00002590
Iteration 178/1000 | Loss: 0.00002590
Iteration 179/1000 | Loss: 0.00002590
Iteration 180/1000 | Loss: 0.00002590
Iteration 181/1000 | Loss: 0.00002589
Iteration 182/1000 | Loss: 0.00002589
Iteration 183/1000 | Loss: 0.00002589
Iteration 184/1000 | Loss: 0.00002589
Iteration 185/1000 | Loss: 0.00002589
Iteration 186/1000 | Loss: 0.00002589
Iteration 187/1000 | Loss: 0.00002589
Iteration 188/1000 | Loss: 0.00002589
Iteration 189/1000 | Loss: 0.00002589
Iteration 190/1000 | Loss: 0.00002589
Iteration 191/1000 | Loss: 0.00002589
Iteration 192/1000 | Loss: 0.00002589
Iteration 193/1000 | Loss: 0.00002589
Iteration 194/1000 | Loss: 0.00002589
Iteration 195/1000 | Loss: 0.00002589
Iteration 196/1000 | Loss: 0.00002589
Iteration 197/1000 | Loss: 0.00002589
Iteration 198/1000 | Loss: 0.00002589
Iteration 199/1000 | Loss: 0.00002589
Iteration 200/1000 | Loss: 0.00002589
Iteration 201/1000 | Loss: 0.00002589
Iteration 202/1000 | Loss: 0.00002589
Iteration 203/1000 | Loss: 0.00002589
Iteration 204/1000 | Loss: 0.00002589
Iteration 205/1000 | Loss: 0.00002589
Iteration 206/1000 | Loss: 0.00002589
Iteration 207/1000 | Loss: 0.00002589
Iteration 208/1000 | Loss: 0.00002589
Iteration 209/1000 | Loss: 0.00002589
Iteration 210/1000 | Loss: 0.00002589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.588683673820924e-05, 2.588683673820924e-05, 2.588683673820924e-05, 2.588683673820924e-05, 2.588683673820924e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.588683673820924e-05

Optimization complete. Final v2v error: 4.451572418212891 mm

Highest mean error: 5.3124003410339355 mm for frame 32

Lowest mean error: 4.081234931945801 mm for frame 126

Saving results

Total time: 46.451566219329834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473609
Iteration 2/25 | Loss: 0.00168958
Iteration 3/25 | Loss: 0.00161630
Iteration 4/25 | Loss: 0.00159703
Iteration 5/25 | Loss: 0.00159336
Iteration 6/25 | Loss: 0.00159245
Iteration 7/25 | Loss: 0.00159245
Iteration 8/25 | Loss: 0.00159245
Iteration 9/25 | Loss: 0.00159245
Iteration 10/25 | Loss: 0.00159245
Iteration 11/25 | Loss: 0.00159245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015924529870972037, 0.0015924529870972037, 0.0015924529870972037, 0.0015924529870972037, 0.0015924529870972037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015924529870972037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16349602
Iteration 2/25 | Loss: 0.00216525
Iteration 3/25 | Loss: 0.00216525
Iteration 4/25 | Loss: 0.00216525
Iteration 5/25 | Loss: 0.00216525
Iteration 6/25 | Loss: 0.00216525
Iteration 7/25 | Loss: 0.00216525
Iteration 8/25 | Loss: 0.00216525
Iteration 9/25 | Loss: 0.00216525
Iteration 10/25 | Loss: 0.00216525
Iteration 11/25 | Loss: 0.00216525
Iteration 12/25 | Loss: 0.00216525
Iteration 13/25 | Loss: 0.00216525
Iteration 14/25 | Loss: 0.00216525
Iteration 15/25 | Loss: 0.00216525
Iteration 16/25 | Loss: 0.00216525
Iteration 17/25 | Loss: 0.00216525
Iteration 18/25 | Loss: 0.00216525
Iteration 19/25 | Loss: 0.00216525
Iteration 20/25 | Loss: 0.00216525
Iteration 21/25 | Loss: 0.00216525
Iteration 22/25 | Loss: 0.00216525
Iteration 23/25 | Loss: 0.00216525
Iteration 24/25 | Loss: 0.00216525
Iteration 25/25 | Loss: 0.00216525

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216525
Iteration 2/1000 | Loss: 0.00005736
Iteration 3/1000 | Loss: 0.00003963
Iteration 4/1000 | Loss: 0.00003315
Iteration 5/1000 | Loss: 0.00003046
Iteration 6/1000 | Loss: 0.00002924
Iteration 7/1000 | Loss: 0.00002864
Iteration 8/1000 | Loss: 0.00002822
Iteration 9/1000 | Loss: 0.00002789
Iteration 10/1000 | Loss: 0.00002761
Iteration 11/1000 | Loss: 0.00002742
Iteration 12/1000 | Loss: 0.00002729
Iteration 13/1000 | Loss: 0.00002726
Iteration 14/1000 | Loss: 0.00002726
Iteration 15/1000 | Loss: 0.00002724
Iteration 16/1000 | Loss: 0.00002721
Iteration 17/1000 | Loss: 0.00002720
Iteration 18/1000 | Loss: 0.00002719
Iteration 19/1000 | Loss: 0.00002719
Iteration 20/1000 | Loss: 0.00002718
Iteration 21/1000 | Loss: 0.00002718
Iteration 22/1000 | Loss: 0.00002717
Iteration 23/1000 | Loss: 0.00002717
Iteration 24/1000 | Loss: 0.00002717
Iteration 25/1000 | Loss: 0.00002716
Iteration 26/1000 | Loss: 0.00002716
Iteration 27/1000 | Loss: 0.00002715
Iteration 28/1000 | Loss: 0.00002715
Iteration 29/1000 | Loss: 0.00002714
Iteration 30/1000 | Loss: 0.00002714
Iteration 31/1000 | Loss: 0.00002714
Iteration 32/1000 | Loss: 0.00002714
Iteration 33/1000 | Loss: 0.00002714
Iteration 34/1000 | Loss: 0.00002713
Iteration 35/1000 | Loss: 0.00002713
Iteration 36/1000 | Loss: 0.00002712
Iteration 37/1000 | Loss: 0.00002712
Iteration 38/1000 | Loss: 0.00002711
Iteration 39/1000 | Loss: 0.00002711
Iteration 40/1000 | Loss: 0.00002710
Iteration 41/1000 | Loss: 0.00002710
Iteration 42/1000 | Loss: 0.00002710
Iteration 43/1000 | Loss: 0.00002710
Iteration 44/1000 | Loss: 0.00002710
Iteration 45/1000 | Loss: 0.00002710
Iteration 46/1000 | Loss: 0.00002710
Iteration 47/1000 | Loss: 0.00002710
Iteration 48/1000 | Loss: 0.00002710
Iteration 49/1000 | Loss: 0.00002710
Iteration 50/1000 | Loss: 0.00002710
Iteration 51/1000 | Loss: 0.00002709
Iteration 52/1000 | Loss: 0.00002709
Iteration 53/1000 | Loss: 0.00002709
Iteration 54/1000 | Loss: 0.00002708
Iteration 55/1000 | Loss: 0.00002708
Iteration 56/1000 | Loss: 0.00002707
Iteration 57/1000 | Loss: 0.00002707
Iteration 58/1000 | Loss: 0.00002707
Iteration 59/1000 | Loss: 0.00002706
Iteration 60/1000 | Loss: 0.00002705
Iteration 61/1000 | Loss: 0.00002705
Iteration 62/1000 | Loss: 0.00002705
Iteration 63/1000 | Loss: 0.00002704
Iteration 64/1000 | Loss: 0.00002704
Iteration 65/1000 | Loss: 0.00002704
Iteration 66/1000 | Loss: 0.00002704
Iteration 67/1000 | Loss: 0.00002704
Iteration 68/1000 | Loss: 0.00002703
Iteration 69/1000 | Loss: 0.00002703
Iteration 70/1000 | Loss: 0.00002702
Iteration 71/1000 | Loss: 0.00002702
Iteration 72/1000 | Loss: 0.00002702
Iteration 73/1000 | Loss: 0.00002702
Iteration 74/1000 | Loss: 0.00002702
Iteration 75/1000 | Loss: 0.00002701
Iteration 76/1000 | Loss: 0.00002701
Iteration 77/1000 | Loss: 0.00002701
Iteration 78/1000 | Loss: 0.00002700
Iteration 79/1000 | Loss: 0.00002700
Iteration 80/1000 | Loss: 0.00002699
Iteration 81/1000 | Loss: 0.00002699
Iteration 82/1000 | Loss: 0.00002698
Iteration 83/1000 | Loss: 0.00002698
Iteration 84/1000 | Loss: 0.00002698
Iteration 85/1000 | Loss: 0.00002697
Iteration 86/1000 | Loss: 0.00002697
Iteration 87/1000 | Loss: 0.00002697
Iteration 88/1000 | Loss: 0.00002696
Iteration 89/1000 | Loss: 0.00002694
Iteration 90/1000 | Loss: 0.00002694
Iteration 91/1000 | Loss: 0.00002694
Iteration 92/1000 | Loss: 0.00002694
Iteration 93/1000 | Loss: 0.00002694
Iteration 94/1000 | Loss: 0.00002694
Iteration 95/1000 | Loss: 0.00002694
Iteration 96/1000 | Loss: 0.00002694
Iteration 97/1000 | Loss: 0.00002694
Iteration 98/1000 | Loss: 0.00002694
Iteration 99/1000 | Loss: 0.00002694
Iteration 100/1000 | Loss: 0.00002694
Iteration 101/1000 | Loss: 0.00002693
Iteration 102/1000 | Loss: 0.00002693
Iteration 103/1000 | Loss: 0.00002693
Iteration 104/1000 | Loss: 0.00002693
Iteration 105/1000 | Loss: 0.00002693
Iteration 106/1000 | Loss: 0.00002693
Iteration 107/1000 | Loss: 0.00002692
Iteration 108/1000 | Loss: 0.00002692
Iteration 109/1000 | Loss: 0.00002692
Iteration 110/1000 | Loss: 0.00002692
Iteration 111/1000 | Loss: 0.00002691
Iteration 112/1000 | Loss: 0.00002691
Iteration 113/1000 | Loss: 0.00002691
Iteration 114/1000 | Loss: 0.00002691
Iteration 115/1000 | Loss: 0.00002691
Iteration 116/1000 | Loss: 0.00002691
Iteration 117/1000 | Loss: 0.00002691
Iteration 118/1000 | Loss: 0.00002691
Iteration 119/1000 | Loss: 0.00002690
Iteration 120/1000 | Loss: 0.00002690
Iteration 121/1000 | Loss: 0.00002690
Iteration 122/1000 | Loss: 0.00002690
Iteration 123/1000 | Loss: 0.00002690
Iteration 124/1000 | Loss: 0.00002690
Iteration 125/1000 | Loss: 0.00002690
Iteration 126/1000 | Loss: 0.00002690
Iteration 127/1000 | Loss: 0.00002690
Iteration 128/1000 | Loss: 0.00002690
Iteration 129/1000 | Loss: 0.00002690
Iteration 130/1000 | Loss: 0.00002689
Iteration 131/1000 | Loss: 0.00002689
Iteration 132/1000 | Loss: 0.00002689
Iteration 133/1000 | Loss: 0.00002689
Iteration 134/1000 | Loss: 0.00002689
Iteration 135/1000 | Loss: 0.00002689
Iteration 136/1000 | Loss: 0.00002689
Iteration 137/1000 | Loss: 0.00002689
Iteration 138/1000 | Loss: 0.00002689
Iteration 139/1000 | Loss: 0.00002689
Iteration 140/1000 | Loss: 0.00002689
Iteration 141/1000 | Loss: 0.00002689
Iteration 142/1000 | Loss: 0.00002689
Iteration 143/1000 | Loss: 0.00002689
Iteration 144/1000 | Loss: 0.00002689
Iteration 145/1000 | Loss: 0.00002689
Iteration 146/1000 | Loss: 0.00002688
Iteration 147/1000 | Loss: 0.00002688
Iteration 148/1000 | Loss: 0.00002688
Iteration 149/1000 | Loss: 0.00002688
Iteration 150/1000 | Loss: 0.00002688
Iteration 151/1000 | Loss: 0.00002688
Iteration 152/1000 | Loss: 0.00002688
Iteration 153/1000 | Loss: 0.00002688
Iteration 154/1000 | Loss: 0.00002688
Iteration 155/1000 | Loss: 0.00002688
Iteration 156/1000 | Loss: 0.00002688
Iteration 157/1000 | Loss: 0.00002688
Iteration 158/1000 | Loss: 0.00002688
Iteration 159/1000 | Loss: 0.00002687
Iteration 160/1000 | Loss: 0.00002687
Iteration 161/1000 | Loss: 0.00002687
Iteration 162/1000 | Loss: 0.00002687
Iteration 163/1000 | Loss: 0.00002687
Iteration 164/1000 | Loss: 0.00002687
Iteration 165/1000 | Loss: 0.00002687
Iteration 166/1000 | Loss: 0.00002687
Iteration 167/1000 | Loss: 0.00002687
Iteration 168/1000 | Loss: 0.00002687
Iteration 169/1000 | Loss: 0.00002687
Iteration 170/1000 | Loss: 0.00002687
Iteration 171/1000 | Loss: 0.00002687
Iteration 172/1000 | Loss: 0.00002687
Iteration 173/1000 | Loss: 0.00002687
Iteration 174/1000 | Loss: 0.00002687
Iteration 175/1000 | Loss: 0.00002687
Iteration 176/1000 | Loss: 0.00002687
Iteration 177/1000 | Loss: 0.00002687
Iteration 178/1000 | Loss: 0.00002687
Iteration 179/1000 | Loss: 0.00002687
Iteration 180/1000 | Loss: 0.00002687
Iteration 181/1000 | Loss: 0.00002687
Iteration 182/1000 | Loss: 0.00002687
Iteration 183/1000 | Loss: 0.00002687
Iteration 184/1000 | Loss: 0.00002687
Iteration 185/1000 | Loss: 0.00002687
Iteration 186/1000 | Loss: 0.00002687
Iteration 187/1000 | Loss: 0.00002687
Iteration 188/1000 | Loss: 0.00002687
Iteration 189/1000 | Loss: 0.00002687
Iteration 190/1000 | Loss: 0.00002687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.687145388335921e-05, 2.687145388335921e-05, 2.687145388335921e-05, 2.687145388335921e-05, 2.687145388335921e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.687145388335921e-05

Optimization complete. Final v2v error: 4.5383687019348145 mm

Highest mean error: 4.871191024780273 mm for frame 89

Lowest mean error: 4.412608623504639 mm for frame 97

Saving results

Total time: 36.23107099533081
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00345053
Iteration 2/25 | Loss: 0.00182753
Iteration 3/25 | Loss: 0.00166790
Iteration 4/25 | Loss: 0.00162890
Iteration 5/25 | Loss: 0.00162470
Iteration 6/25 | Loss: 0.00162330
Iteration 7/25 | Loss: 0.00162304
Iteration 8/25 | Loss: 0.00162304
Iteration 9/25 | Loss: 0.00162304
Iteration 10/25 | Loss: 0.00162304
Iteration 11/25 | Loss: 0.00162304
Iteration 12/25 | Loss: 0.00162304
Iteration 13/25 | Loss: 0.00162304
Iteration 14/25 | Loss: 0.00162304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0016230414621531963, 0.0016230414621531963, 0.0016230414621531963, 0.0016230414621531963, 0.0016230414621531963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016230414621531963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60643291
Iteration 2/25 | Loss: 0.00246205
Iteration 3/25 | Loss: 0.00246205
Iteration 4/25 | Loss: 0.00246205
Iteration 5/25 | Loss: 0.00246205
Iteration 6/25 | Loss: 0.00246205
Iteration 7/25 | Loss: 0.00246205
Iteration 8/25 | Loss: 0.00246205
Iteration 9/25 | Loss: 0.00246205
Iteration 10/25 | Loss: 0.00246205
Iteration 11/25 | Loss: 0.00246205
Iteration 12/25 | Loss: 0.00246205
Iteration 13/25 | Loss: 0.00246205
Iteration 14/25 | Loss: 0.00246205
Iteration 15/25 | Loss: 0.00246205
Iteration 16/25 | Loss: 0.00246205
Iteration 17/25 | Loss: 0.00246205
Iteration 18/25 | Loss: 0.00246205
Iteration 19/25 | Loss: 0.00246205
Iteration 20/25 | Loss: 0.00246205
Iteration 21/25 | Loss: 0.00246205
Iteration 22/25 | Loss: 0.00246205
Iteration 23/25 | Loss: 0.00246205
Iteration 24/25 | Loss: 0.00246205
Iteration 25/25 | Loss: 0.00246205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00246205
Iteration 2/1000 | Loss: 0.00007475
Iteration 3/1000 | Loss: 0.00005778
Iteration 4/1000 | Loss: 0.00004932
Iteration 5/1000 | Loss: 0.00004508
Iteration 6/1000 | Loss: 0.00004242
Iteration 7/1000 | Loss: 0.00004077
Iteration 8/1000 | Loss: 0.00003993
Iteration 9/1000 | Loss: 0.00003932
Iteration 10/1000 | Loss: 0.00003877
Iteration 11/1000 | Loss: 0.00003839
Iteration 12/1000 | Loss: 0.00003804
Iteration 13/1000 | Loss: 0.00003779
Iteration 14/1000 | Loss: 0.00003758
Iteration 15/1000 | Loss: 0.00003752
Iteration 16/1000 | Loss: 0.00003748
Iteration 17/1000 | Loss: 0.00003748
Iteration 18/1000 | Loss: 0.00003747
Iteration 19/1000 | Loss: 0.00003746
Iteration 20/1000 | Loss: 0.00003746
Iteration 21/1000 | Loss: 0.00003744
Iteration 22/1000 | Loss: 0.00003741
Iteration 23/1000 | Loss: 0.00003741
Iteration 24/1000 | Loss: 0.00003739
Iteration 25/1000 | Loss: 0.00003737
Iteration 26/1000 | Loss: 0.00003737
Iteration 27/1000 | Loss: 0.00003737
Iteration 28/1000 | Loss: 0.00003736
Iteration 29/1000 | Loss: 0.00003734
Iteration 30/1000 | Loss: 0.00003733
Iteration 31/1000 | Loss: 0.00003733
Iteration 32/1000 | Loss: 0.00003733
Iteration 33/1000 | Loss: 0.00003732
Iteration 34/1000 | Loss: 0.00003732
Iteration 35/1000 | Loss: 0.00003732
Iteration 36/1000 | Loss: 0.00003732
Iteration 37/1000 | Loss: 0.00003731
Iteration 38/1000 | Loss: 0.00003731
Iteration 39/1000 | Loss: 0.00003730
Iteration 40/1000 | Loss: 0.00003730
Iteration 41/1000 | Loss: 0.00003730
Iteration 42/1000 | Loss: 0.00003730
Iteration 43/1000 | Loss: 0.00003729
Iteration 44/1000 | Loss: 0.00003729
Iteration 45/1000 | Loss: 0.00003729
Iteration 46/1000 | Loss: 0.00003729
Iteration 47/1000 | Loss: 0.00003729
Iteration 48/1000 | Loss: 0.00003729
Iteration 49/1000 | Loss: 0.00003728
Iteration 50/1000 | Loss: 0.00003728
Iteration 51/1000 | Loss: 0.00003728
Iteration 52/1000 | Loss: 0.00003728
Iteration 53/1000 | Loss: 0.00003727
Iteration 54/1000 | Loss: 0.00003727
Iteration 55/1000 | Loss: 0.00003727
Iteration 56/1000 | Loss: 0.00003727
Iteration 57/1000 | Loss: 0.00003727
Iteration 58/1000 | Loss: 0.00003727
Iteration 59/1000 | Loss: 0.00003726
Iteration 60/1000 | Loss: 0.00003726
Iteration 61/1000 | Loss: 0.00003726
Iteration 62/1000 | Loss: 0.00003726
Iteration 63/1000 | Loss: 0.00003726
Iteration 64/1000 | Loss: 0.00003725
Iteration 65/1000 | Loss: 0.00003725
Iteration 66/1000 | Loss: 0.00003725
Iteration 67/1000 | Loss: 0.00003725
Iteration 68/1000 | Loss: 0.00003725
Iteration 69/1000 | Loss: 0.00003725
Iteration 70/1000 | Loss: 0.00003724
Iteration 71/1000 | Loss: 0.00003724
Iteration 72/1000 | Loss: 0.00003724
Iteration 73/1000 | Loss: 0.00003724
Iteration 74/1000 | Loss: 0.00003724
Iteration 75/1000 | Loss: 0.00003724
Iteration 76/1000 | Loss: 0.00003724
Iteration 77/1000 | Loss: 0.00003724
Iteration 78/1000 | Loss: 0.00003724
Iteration 79/1000 | Loss: 0.00003724
Iteration 80/1000 | Loss: 0.00003724
Iteration 81/1000 | Loss: 0.00003723
Iteration 82/1000 | Loss: 0.00003723
Iteration 83/1000 | Loss: 0.00003723
Iteration 84/1000 | Loss: 0.00003723
Iteration 85/1000 | Loss: 0.00003723
Iteration 86/1000 | Loss: 0.00003723
Iteration 87/1000 | Loss: 0.00003723
Iteration 88/1000 | Loss: 0.00003723
Iteration 89/1000 | Loss: 0.00003723
Iteration 90/1000 | Loss: 0.00003723
Iteration 91/1000 | Loss: 0.00003723
Iteration 92/1000 | Loss: 0.00003722
Iteration 93/1000 | Loss: 0.00003722
Iteration 94/1000 | Loss: 0.00003722
Iteration 95/1000 | Loss: 0.00003722
Iteration 96/1000 | Loss: 0.00003722
Iteration 97/1000 | Loss: 0.00003722
Iteration 98/1000 | Loss: 0.00003722
Iteration 99/1000 | Loss: 0.00003722
Iteration 100/1000 | Loss: 0.00003721
Iteration 101/1000 | Loss: 0.00003721
Iteration 102/1000 | Loss: 0.00003721
Iteration 103/1000 | Loss: 0.00003721
Iteration 104/1000 | Loss: 0.00003721
Iteration 105/1000 | Loss: 0.00003720
Iteration 106/1000 | Loss: 0.00003720
Iteration 107/1000 | Loss: 0.00003720
Iteration 108/1000 | Loss: 0.00003720
Iteration 109/1000 | Loss: 0.00003720
Iteration 110/1000 | Loss: 0.00003719
Iteration 111/1000 | Loss: 0.00003719
Iteration 112/1000 | Loss: 0.00003719
Iteration 113/1000 | Loss: 0.00003719
Iteration 114/1000 | Loss: 0.00003718
Iteration 115/1000 | Loss: 0.00003718
Iteration 116/1000 | Loss: 0.00003718
Iteration 117/1000 | Loss: 0.00003718
Iteration 118/1000 | Loss: 0.00003718
Iteration 119/1000 | Loss: 0.00003718
Iteration 120/1000 | Loss: 0.00003718
Iteration 121/1000 | Loss: 0.00003718
Iteration 122/1000 | Loss: 0.00003717
Iteration 123/1000 | Loss: 0.00003717
Iteration 124/1000 | Loss: 0.00003717
Iteration 125/1000 | Loss: 0.00003717
Iteration 126/1000 | Loss: 0.00003717
Iteration 127/1000 | Loss: 0.00003717
Iteration 128/1000 | Loss: 0.00003717
Iteration 129/1000 | Loss: 0.00003716
Iteration 130/1000 | Loss: 0.00003716
Iteration 131/1000 | Loss: 0.00003716
Iteration 132/1000 | Loss: 0.00003716
Iteration 133/1000 | Loss: 0.00003716
Iteration 134/1000 | Loss: 0.00003716
Iteration 135/1000 | Loss: 0.00003716
Iteration 136/1000 | Loss: 0.00003716
Iteration 137/1000 | Loss: 0.00003716
Iteration 138/1000 | Loss: 0.00003716
Iteration 139/1000 | Loss: 0.00003716
Iteration 140/1000 | Loss: 0.00003716
Iteration 141/1000 | Loss: 0.00003716
Iteration 142/1000 | Loss: 0.00003716
Iteration 143/1000 | Loss: 0.00003715
Iteration 144/1000 | Loss: 0.00003715
Iteration 145/1000 | Loss: 0.00003715
Iteration 146/1000 | Loss: 0.00003715
Iteration 147/1000 | Loss: 0.00003715
Iteration 148/1000 | Loss: 0.00003715
Iteration 149/1000 | Loss: 0.00003715
Iteration 150/1000 | Loss: 0.00003715
Iteration 151/1000 | Loss: 0.00003715
Iteration 152/1000 | Loss: 0.00003715
Iteration 153/1000 | Loss: 0.00003715
Iteration 154/1000 | Loss: 0.00003714
Iteration 155/1000 | Loss: 0.00003714
Iteration 156/1000 | Loss: 0.00003714
Iteration 157/1000 | Loss: 0.00003714
Iteration 158/1000 | Loss: 0.00003714
Iteration 159/1000 | Loss: 0.00003714
Iteration 160/1000 | Loss: 0.00003714
Iteration 161/1000 | Loss: 0.00003714
Iteration 162/1000 | Loss: 0.00003714
Iteration 163/1000 | Loss: 0.00003714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [3.714276317623444e-05, 3.714276317623444e-05, 3.714276317623444e-05, 3.714276317623444e-05, 3.714276317623444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.714276317623444e-05

Optimization complete. Final v2v error: 5.337779521942139 mm

Highest mean error: 5.727847576141357 mm for frame 109

Lowest mean error: 4.826410293579102 mm for frame 1

Saving results

Total time: 39.77809453010559
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959638
Iteration 2/25 | Loss: 0.00176553
Iteration 3/25 | Loss: 0.00165943
Iteration 4/25 | Loss: 0.00164059
Iteration 5/25 | Loss: 0.00163361
Iteration 6/25 | Loss: 0.00163200
Iteration 7/25 | Loss: 0.00163187
Iteration 8/25 | Loss: 0.00163187
Iteration 9/25 | Loss: 0.00163187
Iteration 10/25 | Loss: 0.00163187
Iteration 11/25 | Loss: 0.00163187
Iteration 12/25 | Loss: 0.00163187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0016318679554387927, 0.0016318679554387927, 0.0016318679554387927, 0.0016318679554387927, 0.0016318679554387927]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016318679554387927

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58333635
Iteration 2/25 | Loss: 0.00238058
Iteration 3/25 | Loss: 0.00238056
Iteration 4/25 | Loss: 0.00238056
Iteration 5/25 | Loss: 0.00238056
Iteration 6/25 | Loss: 0.00238056
Iteration 7/25 | Loss: 0.00238056
Iteration 8/25 | Loss: 0.00238056
Iteration 9/25 | Loss: 0.00238056
Iteration 10/25 | Loss: 0.00238055
Iteration 11/25 | Loss: 0.00238055
Iteration 12/25 | Loss: 0.00238055
Iteration 13/25 | Loss: 0.00238055
Iteration 14/25 | Loss: 0.00238055
Iteration 15/25 | Loss: 0.00238055
Iteration 16/25 | Loss: 0.00238055
Iteration 17/25 | Loss: 0.00238055
Iteration 18/25 | Loss: 0.00238055
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002380554797127843, 0.002380554797127843, 0.002380554797127843, 0.002380554797127843, 0.002380554797127843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002380554797127843

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00238055
Iteration 2/1000 | Loss: 0.00005206
Iteration 3/1000 | Loss: 0.00003824
Iteration 4/1000 | Loss: 0.00003175
Iteration 5/1000 | Loss: 0.00002960
Iteration 6/1000 | Loss: 0.00002870
Iteration 7/1000 | Loss: 0.00002804
Iteration 8/1000 | Loss: 0.00002741
Iteration 9/1000 | Loss: 0.00002711
Iteration 10/1000 | Loss: 0.00002684
Iteration 11/1000 | Loss: 0.00002675
Iteration 12/1000 | Loss: 0.00002674
Iteration 13/1000 | Loss: 0.00002669
Iteration 14/1000 | Loss: 0.00002654
Iteration 15/1000 | Loss: 0.00002652
Iteration 16/1000 | Loss: 0.00002640
Iteration 17/1000 | Loss: 0.00002639
Iteration 18/1000 | Loss: 0.00002637
Iteration 19/1000 | Loss: 0.00002637
Iteration 20/1000 | Loss: 0.00002636
Iteration 21/1000 | Loss: 0.00002636
Iteration 22/1000 | Loss: 0.00002635
Iteration 23/1000 | Loss: 0.00002634
Iteration 24/1000 | Loss: 0.00002633
Iteration 25/1000 | Loss: 0.00002633
Iteration 26/1000 | Loss: 0.00002633
Iteration 27/1000 | Loss: 0.00002633
Iteration 28/1000 | Loss: 0.00002633
Iteration 29/1000 | Loss: 0.00002633
Iteration 30/1000 | Loss: 0.00002632
Iteration 31/1000 | Loss: 0.00002632
Iteration 32/1000 | Loss: 0.00002632
Iteration 33/1000 | Loss: 0.00002632
Iteration 34/1000 | Loss: 0.00002632
Iteration 35/1000 | Loss: 0.00002632
Iteration 36/1000 | Loss: 0.00002632
Iteration 37/1000 | Loss: 0.00002632
Iteration 38/1000 | Loss: 0.00002632
Iteration 39/1000 | Loss: 0.00002631
Iteration 40/1000 | Loss: 0.00002630
Iteration 41/1000 | Loss: 0.00002629
Iteration 42/1000 | Loss: 0.00002629
Iteration 43/1000 | Loss: 0.00002629
Iteration 44/1000 | Loss: 0.00002629
Iteration 45/1000 | Loss: 0.00002629
Iteration 46/1000 | Loss: 0.00002629
Iteration 47/1000 | Loss: 0.00002628
Iteration 48/1000 | Loss: 0.00002628
Iteration 49/1000 | Loss: 0.00002628
Iteration 50/1000 | Loss: 0.00002627
Iteration 51/1000 | Loss: 0.00002627
Iteration 52/1000 | Loss: 0.00002627
Iteration 53/1000 | Loss: 0.00002626
Iteration 54/1000 | Loss: 0.00002626
Iteration 55/1000 | Loss: 0.00002626
Iteration 56/1000 | Loss: 0.00002625
Iteration 57/1000 | Loss: 0.00002625
Iteration 58/1000 | Loss: 0.00002625
Iteration 59/1000 | Loss: 0.00002624
Iteration 60/1000 | Loss: 0.00002624
Iteration 61/1000 | Loss: 0.00002624
Iteration 62/1000 | Loss: 0.00002623
Iteration 63/1000 | Loss: 0.00002623
Iteration 64/1000 | Loss: 0.00002623
Iteration 65/1000 | Loss: 0.00002623
Iteration 66/1000 | Loss: 0.00002623
Iteration 67/1000 | Loss: 0.00002622
Iteration 68/1000 | Loss: 0.00002622
Iteration 69/1000 | Loss: 0.00002622
Iteration 70/1000 | Loss: 0.00002622
Iteration 71/1000 | Loss: 0.00002622
Iteration 72/1000 | Loss: 0.00002621
Iteration 73/1000 | Loss: 0.00002621
Iteration 74/1000 | Loss: 0.00002621
Iteration 75/1000 | Loss: 0.00002621
Iteration 76/1000 | Loss: 0.00002620
Iteration 77/1000 | Loss: 0.00002620
Iteration 78/1000 | Loss: 0.00002620
Iteration 79/1000 | Loss: 0.00002620
Iteration 80/1000 | Loss: 0.00002619
Iteration 81/1000 | Loss: 0.00002619
Iteration 82/1000 | Loss: 0.00002619
Iteration 83/1000 | Loss: 0.00002619
Iteration 84/1000 | Loss: 0.00002619
Iteration 85/1000 | Loss: 0.00002619
Iteration 86/1000 | Loss: 0.00002619
Iteration 87/1000 | Loss: 0.00002619
Iteration 88/1000 | Loss: 0.00002619
Iteration 89/1000 | Loss: 0.00002618
Iteration 90/1000 | Loss: 0.00002618
Iteration 91/1000 | Loss: 0.00002618
Iteration 92/1000 | Loss: 0.00002618
Iteration 93/1000 | Loss: 0.00002618
Iteration 94/1000 | Loss: 0.00002618
Iteration 95/1000 | Loss: 0.00002618
Iteration 96/1000 | Loss: 0.00002618
Iteration 97/1000 | Loss: 0.00002618
Iteration 98/1000 | Loss: 0.00002618
Iteration 99/1000 | Loss: 0.00002618
Iteration 100/1000 | Loss: 0.00002617
Iteration 101/1000 | Loss: 0.00002617
Iteration 102/1000 | Loss: 0.00002617
Iteration 103/1000 | Loss: 0.00002617
Iteration 104/1000 | Loss: 0.00002617
Iteration 105/1000 | Loss: 0.00002617
Iteration 106/1000 | Loss: 0.00002617
Iteration 107/1000 | Loss: 0.00002617
Iteration 108/1000 | Loss: 0.00002617
Iteration 109/1000 | Loss: 0.00002617
Iteration 110/1000 | Loss: 0.00002617
Iteration 111/1000 | Loss: 0.00002617
Iteration 112/1000 | Loss: 0.00002617
Iteration 113/1000 | Loss: 0.00002617
Iteration 114/1000 | Loss: 0.00002616
Iteration 115/1000 | Loss: 0.00002616
Iteration 116/1000 | Loss: 0.00002616
Iteration 117/1000 | Loss: 0.00002616
Iteration 118/1000 | Loss: 0.00002616
Iteration 119/1000 | Loss: 0.00002616
Iteration 120/1000 | Loss: 0.00002616
Iteration 121/1000 | Loss: 0.00002616
Iteration 122/1000 | Loss: 0.00002616
Iteration 123/1000 | Loss: 0.00002616
Iteration 124/1000 | Loss: 0.00002616
Iteration 125/1000 | Loss: 0.00002616
Iteration 126/1000 | Loss: 0.00002616
Iteration 127/1000 | Loss: 0.00002616
Iteration 128/1000 | Loss: 0.00002616
Iteration 129/1000 | Loss: 0.00002616
Iteration 130/1000 | Loss: 0.00002616
Iteration 131/1000 | Loss: 0.00002616
Iteration 132/1000 | Loss: 0.00002616
Iteration 133/1000 | Loss: 0.00002616
Iteration 134/1000 | Loss: 0.00002616
Iteration 135/1000 | Loss: 0.00002616
Iteration 136/1000 | Loss: 0.00002616
Iteration 137/1000 | Loss: 0.00002615
Iteration 138/1000 | Loss: 0.00002615
Iteration 139/1000 | Loss: 0.00002615
Iteration 140/1000 | Loss: 0.00002615
Iteration 141/1000 | Loss: 0.00002615
Iteration 142/1000 | Loss: 0.00002615
Iteration 143/1000 | Loss: 0.00002615
Iteration 144/1000 | Loss: 0.00002615
Iteration 145/1000 | Loss: 0.00002615
Iteration 146/1000 | Loss: 0.00002615
Iteration 147/1000 | Loss: 0.00002615
Iteration 148/1000 | Loss: 0.00002615
Iteration 149/1000 | Loss: 0.00002615
Iteration 150/1000 | Loss: 0.00002615
Iteration 151/1000 | Loss: 0.00002615
Iteration 152/1000 | Loss: 0.00002615
Iteration 153/1000 | Loss: 0.00002615
Iteration 154/1000 | Loss: 0.00002615
Iteration 155/1000 | Loss: 0.00002615
Iteration 156/1000 | Loss: 0.00002615
Iteration 157/1000 | Loss: 0.00002615
Iteration 158/1000 | Loss: 0.00002615
Iteration 159/1000 | Loss: 0.00002615
Iteration 160/1000 | Loss: 0.00002615
Iteration 161/1000 | Loss: 0.00002615
Iteration 162/1000 | Loss: 0.00002615
Iteration 163/1000 | Loss: 0.00002615
Iteration 164/1000 | Loss: 0.00002615
Iteration 165/1000 | Loss: 0.00002615
Iteration 166/1000 | Loss: 0.00002615
Iteration 167/1000 | Loss: 0.00002615
Iteration 168/1000 | Loss: 0.00002615
Iteration 169/1000 | Loss: 0.00002615
Iteration 170/1000 | Loss: 0.00002615
Iteration 171/1000 | Loss: 0.00002615
Iteration 172/1000 | Loss: 0.00002615
Iteration 173/1000 | Loss: 0.00002615
Iteration 174/1000 | Loss: 0.00002615
Iteration 175/1000 | Loss: 0.00002615
Iteration 176/1000 | Loss: 0.00002615
Iteration 177/1000 | Loss: 0.00002615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.615243283798918e-05, 2.615243283798918e-05, 2.615243283798918e-05, 2.615243283798918e-05, 2.615243283798918e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.615243283798918e-05

Optimization complete. Final v2v error: 4.521416187286377 mm

Highest mean error: 4.8324875831604 mm for frame 74

Lowest mean error: 4.236581802368164 mm for frame 2

Saving results

Total time: 36.48400115966797
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01143930
Iteration 2/25 | Loss: 0.00340232
Iteration 3/25 | Loss: 0.00276528
Iteration 4/25 | Loss: 0.00293385
Iteration 5/25 | Loss: 0.00242350
Iteration 6/25 | Loss: 0.00239648
Iteration 7/25 | Loss: 0.00230026
Iteration 8/25 | Loss: 0.00219278
Iteration 9/25 | Loss: 0.00214986
Iteration 10/25 | Loss: 0.00210817
Iteration 11/25 | Loss: 0.00208082
Iteration 12/25 | Loss: 0.00205811
Iteration 13/25 | Loss: 0.00204265
Iteration 14/25 | Loss: 0.00203932
Iteration 15/25 | Loss: 0.00203769
Iteration 16/25 | Loss: 0.00203691
Iteration 17/25 | Loss: 0.00203661
Iteration 18/25 | Loss: 0.00203641
Iteration 19/25 | Loss: 0.00203641
Iteration 20/25 | Loss: 0.00203641
Iteration 21/25 | Loss: 0.00203641
Iteration 22/25 | Loss: 0.00203641
Iteration 23/25 | Loss: 0.00203641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002036411315202713, 0.002036411315202713, 0.002036411315202713, 0.002036411315202713, 0.002036411315202713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002036411315202713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54021156
Iteration 2/25 | Loss: 0.00598054
Iteration 3/25 | Loss: 0.00598054
Iteration 4/25 | Loss: 0.00598054
Iteration 5/25 | Loss: 0.00598054
Iteration 6/25 | Loss: 0.00598054
Iteration 7/25 | Loss: 0.00598053
Iteration 8/25 | Loss: 0.00598053
Iteration 9/25 | Loss: 0.00598053
Iteration 10/25 | Loss: 0.00598053
Iteration 11/25 | Loss: 0.00598053
Iteration 12/25 | Loss: 0.00598053
Iteration 13/25 | Loss: 0.00598053
Iteration 14/25 | Loss: 0.00598053
Iteration 15/25 | Loss: 0.00598053
Iteration 16/25 | Loss: 0.00598053
Iteration 17/25 | Loss: 0.00598053
Iteration 18/25 | Loss: 0.00598053
Iteration 19/25 | Loss: 0.00598053
Iteration 20/25 | Loss: 0.00598053
Iteration 21/25 | Loss: 0.00598053
Iteration 22/25 | Loss: 0.00598053
Iteration 23/25 | Loss: 0.00598053
Iteration 24/25 | Loss: 0.00598053
Iteration 25/25 | Loss: 0.00598053

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00598053
Iteration 2/1000 | Loss: 0.00433556
Iteration 3/1000 | Loss: 0.00191863
Iteration 4/1000 | Loss: 0.00078541
Iteration 5/1000 | Loss: 0.00069871
Iteration 6/1000 | Loss: 0.00040530
Iteration 7/1000 | Loss: 0.00041893
Iteration 8/1000 | Loss: 0.00047880
Iteration 9/1000 | Loss: 0.00041875
Iteration 10/1000 | Loss: 0.00032807
Iteration 11/1000 | Loss: 0.00072067
Iteration 12/1000 | Loss: 0.00038070
Iteration 13/1000 | Loss: 0.00042625
Iteration 14/1000 | Loss: 0.00027896
Iteration 15/1000 | Loss: 0.00022850
Iteration 16/1000 | Loss: 0.00031041
Iteration 17/1000 | Loss: 0.00033413
Iteration 18/1000 | Loss: 0.00022471
Iteration 19/1000 | Loss: 0.00020482
Iteration 20/1000 | Loss: 0.00100113
Iteration 21/1000 | Loss: 0.00122370
Iteration 22/1000 | Loss: 0.00209783
Iteration 23/1000 | Loss: 0.00207460
Iteration 24/1000 | Loss: 0.00083208
Iteration 25/1000 | Loss: 0.00103786
Iteration 26/1000 | Loss: 0.00074659
Iteration 27/1000 | Loss: 0.00046151
Iteration 28/1000 | Loss: 0.00037620
Iteration 29/1000 | Loss: 0.00035657
Iteration 30/1000 | Loss: 0.00027103
Iteration 31/1000 | Loss: 0.00034563
Iteration 32/1000 | Loss: 0.00019412
Iteration 33/1000 | Loss: 0.00018202
Iteration 34/1000 | Loss: 0.00018109
Iteration 35/1000 | Loss: 0.00022588
Iteration 36/1000 | Loss: 0.00017719
Iteration 37/1000 | Loss: 0.00014866
Iteration 38/1000 | Loss: 0.00015242
Iteration 39/1000 | Loss: 0.00014452
Iteration 40/1000 | Loss: 0.00013787
Iteration 41/1000 | Loss: 0.00014016
Iteration 42/1000 | Loss: 0.00013164
Iteration 43/1000 | Loss: 0.00013835
Iteration 44/1000 | Loss: 0.00013679
Iteration 45/1000 | Loss: 0.00012815
Iteration 46/1000 | Loss: 0.00012952
Iteration 47/1000 | Loss: 0.00012894
Iteration 48/1000 | Loss: 0.00012536
Iteration 49/1000 | Loss: 0.00013083
Iteration 50/1000 | Loss: 0.00012649
Iteration 51/1000 | Loss: 0.00012808
Iteration 52/1000 | Loss: 0.00012336
Iteration 53/1000 | Loss: 0.00012303
Iteration 54/1000 | Loss: 0.00018669
Iteration 55/1000 | Loss: 0.00018669
Iteration 56/1000 | Loss: 0.00023118
Iteration 57/1000 | Loss: 0.00014639
Iteration 58/1000 | Loss: 0.00013103
Iteration 59/1000 | Loss: 0.00012790
Iteration 60/1000 | Loss: 0.00012730
Iteration 61/1000 | Loss: 0.00013876
Iteration 62/1000 | Loss: 0.00013111
Iteration 63/1000 | Loss: 0.00012492
Iteration 64/1000 | Loss: 0.00012307
Iteration 65/1000 | Loss: 0.00012171
Iteration 66/1000 | Loss: 0.00012084
Iteration 67/1000 | Loss: 0.00012035
Iteration 68/1000 | Loss: 0.00012020
Iteration 69/1000 | Loss: 0.00012003
Iteration 70/1000 | Loss: 0.00012000
Iteration 71/1000 | Loss: 0.00012000
Iteration 72/1000 | Loss: 0.00012000
Iteration 73/1000 | Loss: 0.00011999
Iteration 74/1000 | Loss: 0.00011999
Iteration 75/1000 | Loss: 0.00011998
Iteration 76/1000 | Loss: 0.00011998
Iteration 77/1000 | Loss: 0.00011998
Iteration 78/1000 | Loss: 0.00011997
Iteration 79/1000 | Loss: 0.00011997
Iteration 80/1000 | Loss: 0.00011997
Iteration 81/1000 | Loss: 0.00011997
Iteration 82/1000 | Loss: 0.00011996
Iteration 83/1000 | Loss: 0.00011996
Iteration 84/1000 | Loss: 0.00011996
Iteration 85/1000 | Loss: 0.00011996
Iteration 86/1000 | Loss: 0.00011996
Iteration 87/1000 | Loss: 0.00011996
Iteration 88/1000 | Loss: 0.00011996
Iteration 89/1000 | Loss: 0.00011996
Iteration 90/1000 | Loss: 0.00011995
Iteration 91/1000 | Loss: 0.00011995
Iteration 92/1000 | Loss: 0.00011995
Iteration 93/1000 | Loss: 0.00011995
Iteration 94/1000 | Loss: 0.00011995
Iteration 95/1000 | Loss: 0.00011994
Iteration 96/1000 | Loss: 0.00011994
Iteration 97/1000 | Loss: 0.00011994
Iteration 98/1000 | Loss: 0.00011993
Iteration 99/1000 | Loss: 0.00011993
Iteration 100/1000 | Loss: 0.00011993
Iteration 101/1000 | Loss: 0.00011993
Iteration 102/1000 | Loss: 0.00011993
Iteration 103/1000 | Loss: 0.00011993
Iteration 104/1000 | Loss: 0.00011992
Iteration 105/1000 | Loss: 0.00011992
Iteration 106/1000 | Loss: 0.00011991
Iteration 107/1000 | Loss: 0.00011991
Iteration 108/1000 | Loss: 0.00011991
Iteration 109/1000 | Loss: 0.00011990
Iteration 110/1000 | Loss: 0.00011990
Iteration 111/1000 | Loss: 0.00011990
Iteration 112/1000 | Loss: 0.00011990
Iteration 113/1000 | Loss: 0.00011990
Iteration 114/1000 | Loss: 0.00011990
Iteration 115/1000 | Loss: 0.00011990
Iteration 116/1000 | Loss: 0.00011990
Iteration 117/1000 | Loss: 0.00011990
Iteration 118/1000 | Loss: 0.00011989
Iteration 119/1000 | Loss: 0.00011989
Iteration 120/1000 | Loss: 0.00011989
Iteration 121/1000 | Loss: 0.00011989
Iteration 122/1000 | Loss: 0.00011988
Iteration 123/1000 | Loss: 0.00011988
Iteration 124/1000 | Loss: 0.00011988
Iteration 125/1000 | Loss: 0.00011988
Iteration 126/1000 | Loss: 0.00011988
Iteration 127/1000 | Loss: 0.00011988
Iteration 128/1000 | Loss: 0.00011988
Iteration 129/1000 | Loss: 0.00011988
Iteration 130/1000 | Loss: 0.00011988
Iteration 131/1000 | Loss: 0.00011988
Iteration 132/1000 | Loss: 0.00011988
Iteration 133/1000 | Loss: 0.00011988
Iteration 134/1000 | Loss: 0.00011987
Iteration 135/1000 | Loss: 0.00011987
Iteration 136/1000 | Loss: 0.00011987
Iteration 137/1000 | Loss: 0.00011987
Iteration 138/1000 | Loss: 0.00011987
Iteration 139/1000 | Loss: 0.00011987
Iteration 140/1000 | Loss: 0.00011987
Iteration 141/1000 | Loss: 0.00011987
Iteration 142/1000 | Loss: 0.00011987
Iteration 143/1000 | Loss: 0.00011987
Iteration 144/1000 | Loss: 0.00011987
Iteration 145/1000 | Loss: 0.00011987
Iteration 146/1000 | Loss: 0.00011987
Iteration 147/1000 | Loss: 0.00011987
Iteration 148/1000 | Loss: 0.00011986
Iteration 149/1000 | Loss: 0.00011986
Iteration 150/1000 | Loss: 0.00011986
Iteration 151/1000 | Loss: 0.00011986
Iteration 152/1000 | Loss: 0.00011986
Iteration 153/1000 | Loss: 0.00011986
Iteration 154/1000 | Loss: 0.00011986
Iteration 155/1000 | Loss: 0.00011986
Iteration 156/1000 | Loss: 0.00011986
Iteration 157/1000 | Loss: 0.00011986
Iteration 158/1000 | Loss: 0.00011986
Iteration 159/1000 | Loss: 0.00011986
Iteration 160/1000 | Loss: 0.00011986
Iteration 161/1000 | Loss: 0.00011986
Iteration 162/1000 | Loss: 0.00011986
Iteration 163/1000 | Loss: 0.00011986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [0.00011986056779278442, 0.00011986056779278442, 0.00011986056779278442, 0.00011986056779278442, 0.00011986056779278442]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00011986056779278442

Optimization complete. Final v2v error: 6.408132076263428 mm

Highest mean error: 12.057561874389648 mm for frame 85

Lowest mean error: 4.842592239379883 mm for frame 78

Saving results

Total time: 152.80907154083252
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995414
Iteration 2/25 | Loss: 0.00216125
Iteration 3/25 | Loss: 0.00190664
Iteration 4/25 | Loss: 0.00185086
Iteration 5/25 | Loss: 0.00183249
Iteration 6/25 | Loss: 0.00182883
Iteration 7/25 | Loss: 0.00182782
Iteration 8/25 | Loss: 0.00182782
Iteration 9/25 | Loss: 0.00182782
Iteration 10/25 | Loss: 0.00182782
Iteration 11/25 | Loss: 0.00182782
Iteration 12/25 | Loss: 0.00182782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001827823929488659, 0.001827823929488659, 0.001827823929488659, 0.001827823929488659, 0.001827823929488659]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001827823929488659

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56602752
Iteration 2/25 | Loss: 0.00348944
Iteration 3/25 | Loss: 0.00348942
Iteration 4/25 | Loss: 0.00348942
Iteration 5/25 | Loss: 0.00348942
Iteration 6/25 | Loss: 0.00348942
Iteration 7/25 | Loss: 0.00348942
Iteration 8/25 | Loss: 0.00348942
Iteration 9/25 | Loss: 0.00348942
Iteration 10/25 | Loss: 0.00348942
Iteration 11/25 | Loss: 0.00348942
Iteration 12/25 | Loss: 0.00348942
Iteration 13/25 | Loss: 0.00348942
Iteration 14/25 | Loss: 0.00348942
Iteration 15/25 | Loss: 0.00348942
Iteration 16/25 | Loss: 0.00348942
Iteration 17/25 | Loss: 0.00348942
Iteration 18/25 | Loss: 0.00348942
Iteration 19/25 | Loss: 0.00348942
Iteration 20/25 | Loss: 0.00348942
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0034894226118922234, 0.0034894226118922234, 0.0034894226118922234, 0.0034894226118922234, 0.0034894226118922234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034894226118922234

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00348942
Iteration 2/1000 | Loss: 0.00026572
Iteration 3/1000 | Loss: 0.00019108
Iteration 4/1000 | Loss: 0.00015372
Iteration 5/1000 | Loss: 0.00013106
Iteration 6/1000 | Loss: 0.00129731
Iteration 7/1000 | Loss: 0.00105287
Iteration 8/1000 | Loss: 0.00021559
Iteration 9/1000 | Loss: 0.00020321
Iteration 10/1000 | Loss: 0.00013497
Iteration 11/1000 | Loss: 0.00040766
Iteration 12/1000 | Loss: 0.00047340
Iteration 13/1000 | Loss: 0.00011368
Iteration 14/1000 | Loss: 0.00043554
Iteration 15/1000 | Loss: 0.00026469
Iteration 16/1000 | Loss: 0.00025877
Iteration 17/1000 | Loss: 0.00009492
Iteration 18/1000 | Loss: 0.00008427
Iteration 19/1000 | Loss: 0.00045217
Iteration 20/1000 | Loss: 0.00036527
Iteration 21/1000 | Loss: 0.00049892
Iteration 22/1000 | Loss: 0.00047063
Iteration 23/1000 | Loss: 0.00045539
Iteration 24/1000 | Loss: 0.00076425
Iteration 25/1000 | Loss: 0.00034458
Iteration 26/1000 | Loss: 0.00060050
Iteration 27/1000 | Loss: 0.00068035
Iteration 28/1000 | Loss: 0.00012121
Iteration 29/1000 | Loss: 0.00008856
Iteration 30/1000 | Loss: 0.00007880
Iteration 31/1000 | Loss: 0.00100562
Iteration 32/1000 | Loss: 0.00031782
Iteration 33/1000 | Loss: 0.00042841
Iteration 34/1000 | Loss: 0.00045133
Iteration 35/1000 | Loss: 0.00007512
Iteration 36/1000 | Loss: 0.00006067
Iteration 37/1000 | Loss: 0.00005464
Iteration 38/1000 | Loss: 0.00005107
Iteration 39/1000 | Loss: 0.00030731
Iteration 40/1000 | Loss: 0.00005850
Iteration 41/1000 | Loss: 0.00067812
Iteration 42/1000 | Loss: 0.00033941
Iteration 43/1000 | Loss: 0.00053610
Iteration 44/1000 | Loss: 0.00005723
Iteration 45/1000 | Loss: 0.00005197
Iteration 46/1000 | Loss: 0.00004923
Iteration 47/1000 | Loss: 0.00025787
Iteration 48/1000 | Loss: 0.00021639
Iteration 49/1000 | Loss: 0.00067065
Iteration 50/1000 | Loss: 0.00018738
Iteration 51/1000 | Loss: 0.00007005
Iteration 52/1000 | Loss: 0.00005224
Iteration 53/1000 | Loss: 0.00004946
Iteration 54/1000 | Loss: 0.00004759
Iteration 55/1000 | Loss: 0.00004569
Iteration 56/1000 | Loss: 0.00004429
Iteration 57/1000 | Loss: 0.00004330
Iteration 58/1000 | Loss: 0.00004259
Iteration 59/1000 | Loss: 0.00004219
Iteration 60/1000 | Loss: 0.00004156
Iteration 61/1000 | Loss: 0.00004063
Iteration 62/1000 | Loss: 0.00003991
Iteration 63/1000 | Loss: 0.00003961
Iteration 64/1000 | Loss: 0.00003936
Iteration 65/1000 | Loss: 0.00003914
Iteration 66/1000 | Loss: 0.00003913
Iteration 67/1000 | Loss: 0.00003908
Iteration 68/1000 | Loss: 0.00003905
Iteration 69/1000 | Loss: 0.00003902
Iteration 70/1000 | Loss: 0.00003902
Iteration 71/1000 | Loss: 0.00003901
Iteration 72/1000 | Loss: 0.00003901
Iteration 73/1000 | Loss: 0.00003900
Iteration 74/1000 | Loss: 0.00003899
Iteration 75/1000 | Loss: 0.00003899
Iteration 76/1000 | Loss: 0.00003899
Iteration 77/1000 | Loss: 0.00003899
Iteration 78/1000 | Loss: 0.00003899
Iteration 79/1000 | Loss: 0.00003898
Iteration 80/1000 | Loss: 0.00003897
Iteration 81/1000 | Loss: 0.00003897
Iteration 82/1000 | Loss: 0.00003893
Iteration 83/1000 | Loss: 0.00003893
Iteration 84/1000 | Loss: 0.00003892
Iteration 85/1000 | Loss: 0.00003891
Iteration 86/1000 | Loss: 0.00003890
Iteration 87/1000 | Loss: 0.00003890
Iteration 88/1000 | Loss: 0.00003890
Iteration 89/1000 | Loss: 0.00003890
Iteration 90/1000 | Loss: 0.00003889
Iteration 91/1000 | Loss: 0.00003889
Iteration 92/1000 | Loss: 0.00003889
Iteration 93/1000 | Loss: 0.00003889
Iteration 94/1000 | Loss: 0.00003889
Iteration 95/1000 | Loss: 0.00003888
Iteration 96/1000 | Loss: 0.00003888
Iteration 97/1000 | Loss: 0.00003888
Iteration 98/1000 | Loss: 0.00003887
Iteration 99/1000 | Loss: 0.00003887
Iteration 100/1000 | Loss: 0.00003887
Iteration 101/1000 | Loss: 0.00003886
Iteration 102/1000 | Loss: 0.00003886
Iteration 103/1000 | Loss: 0.00003886
Iteration 104/1000 | Loss: 0.00003885
Iteration 105/1000 | Loss: 0.00003885
Iteration 106/1000 | Loss: 0.00003884
Iteration 107/1000 | Loss: 0.00003884
Iteration 108/1000 | Loss: 0.00003884
Iteration 109/1000 | Loss: 0.00003884
Iteration 110/1000 | Loss: 0.00003884
Iteration 111/1000 | Loss: 0.00003883
Iteration 112/1000 | Loss: 0.00003883
Iteration 113/1000 | Loss: 0.00003883
Iteration 114/1000 | Loss: 0.00003883
Iteration 115/1000 | Loss: 0.00003883
Iteration 116/1000 | Loss: 0.00003883
Iteration 117/1000 | Loss: 0.00003883
Iteration 118/1000 | Loss: 0.00003883
Iteration 119/1000 | Loss: 0.00003883
Iteration 120/1000 | Loss: 0.00003883
Iteration 121/1000 | Loss: 0.00003883
Iteration 122/1000 | Loss: 0.00003883
Iteration 123/1000 | Loss: 0.00003883
Iteration 124/1000 | Loss: 0.00003882
Iteration 125/1000 | Loss: 0.00003882
Iteration 126/1000 | Loss: 0.00003882
Iteration 127/1000 | Loss: 0.00003882
Iteration 128/1000 | Loss: 0.00003881
Iteration 129/1000 | Loss: 0.00003881
Iteration 130/1000 | Loss: 0.00003881
Iteration 131/1000 | Loss: 0.00003880
Iteration 132/1000 | Loss: 0.00003880
Iteration 133/1000 | Loss: 0.00003880
Iteration 134/1000 | Loss: 0.00003880
Iteration 135/1000 | Loss: 0.00003880
Iteration 136/1000 | Loss: 0.00003879
Iteration 137/1000 | Loss: 0.00003879
Iteration 138/1000 | Loss: 0.00003879
Iteration 139/1000 | Loss: 0.00003879
Iteration 140/1000 | Loss: 0.00003879
Iteration 141/1000 | Loss: 0.00003879
Iteration 142/1000 | Loss: 0.00003879
Iteration 143/1000 | Loss: 0.00003879
Iteration 144/1000 | Loss: 0.00003879
Iteration 145/1000 | Loss: 0.00003879
Iteration 146/1000 | Loss: 0.00003879
Iteration 147/1000 | Loss: 0.00003879
Iteration 148/1000 | Loss: 0.00003879
Iteration 149/1000 | Loss: 0.00003878
Iteration 150/1000 | Loss: 0.00003878
Iteration 151/1000 | Loss: 0.00003878
Iteration 152/1000 | Loss: 0.00003878
Iteration 153/1000 | Loss: 0.00003878
Iteration 154/1000 | Loss: 0.00003878
Iteration 155/1000 | Loss: 0.00003878
Iteration 156/1000 | Loss: 0.00003878
Iteration 157/1000 | Loss: 0.00003878
Iteration 158/1000 | Loss: 0.00003878
Iteration 159/1000 | Loss: 0.00003878
Iteration 160/1000 | Loss: 0.00003878
Iteration 161/1000 | Loss: 0.00003878
Iteration 162/1000 | Loss: 0.00003878
Iteration 163/1000 | Loss: 0.00003878
Iteration 164/1000 | Loss: 0.00003877
Iteration 165/1000 | Loss: 0.00003877
Iteration 166/1000 | Loss: 0.00003877
Iteration 167/1000 | Loss: 0.00003877
Iteration 168/1000 | Loss: 0.00003877
Iteration 169/1000 | Loss: 0.00003877
Iteration 170/1000 | Loss: 0.00003877
Iteration 171/1000 | Loss: 0.00003877
Iteration 172/1000 | Loss: 0.00003877
Iteration 173/1000 | Loss: 0.00003877
Iteration 174/1000 | Loss: 0.00003877
Iteration 175/1000 | Loss: 0.00003877
Iteration 176/1000 | Loss: 0.00003877
Iteration 177/1000 | Loss: 0.00003877
Iteration 178/1000 | Loss: 0.00003877
Iteration 179/1000 | Loss: 0.00003877
Iteration 180/1000 | Loss: 0.00003877
Iteration 181/1000 | Loss: 0.00003877
Iteration 182/1000 | Loss: 0.00003877
Iteration 183/1000 | Loss: 0.00003877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [3.877335984725505e-05, 3.877335984725505e-05, 3.877335984725505e-05, 3.877335984725505e-05, 3.877335984725505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.877335984725505e-05

Optimization complete. Final v2v error: 5.358160972595215 mm

Highest mean error: 6.517856121063232 mm for frame 239

Lowest mean error: 4.8209357261657715 mm for frame 18

Saving results

Total time: 127.94862985610962
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527981
Iteration 2/25 | Loss: 0.00183187
Iteration 3/25 | Loss: 0.00169429
Iteration 4/25 | Loss: 0.00166043
Iteration 5/25 | Loss: 0.00165049
Iteration 6/25 | Loss: 0.00164821
Iteration 7/25 | Loss: 0.00164765
Iteration 8/25 | Loss: 0.00164765
Iteration 9/25 | Loss: 0.00164765
Iteration 10/25 | Loss: 0.00164765
Iteration 11/25 | Loss: 0.00164765
Iteration 12/25 | Loss: 0.00164765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001647647120989859, 0.001647647120989859, 0.001647647120989859, 0.001647647120989859, 0.001647647120989859]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001647647120989859

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45470750
Iteration 2/25 | Loss: 0.00236794
Iteration 3/25 | Loss: 0.00236793
Iteration 4/25 | Loss: 0.00236793
Iteration 5/25 | Loss: 0.00236793
Iteration 6/25 | Loss: 0.00236793
Iteration 7/25 | Loss: 0.00236793
Iteration 8/25 | Loss: 0.00236793
Iteration 9/25 | Loss: 0.00236793
Iteration 10/25 | Loss: 0.00236793
Iteration 11/25 | Loss: 0.00236793
Iteration 12/25 | Loss: 0.00236793
Iteration 13/25 | Loss: 0.00236793
Iteration 14/25 | Loss: 0.00236793
Iteration 15/25 | Loss: 0.00236793
Iteration 16/25 | Loss: 0.00236793
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002367928856983781, 0.002367928856983781, 0.002367928856983781, 0.002367928856983781, 0.002367928856983781]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002367928856983781

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00236793
Iteration 2/1000 | Loss: 0.00007598
Iteration 3/1000 | Loss: 0.00005299
Iteration 4/1000 | Loss: 0.00004603
Iteration 5/1000 | Loss: 0.00004225
Iteration 6/1000 | Loss: 0.00004061
Iteration 7/1000 | Loss: 0.00003980
Iteration 8/1000 | Loss: 0.00003903
Iteration 9/1000 | Loss: 0.00003857
Iteration 10/1000 | Loss: 0.00003827
Iteration 11/1000 | Loss: 0.00003827
Iteration 12/1000 | Loss: 0.00003824
Iteration 13/1000 | Loss: 0.00003803
Iteration 14/1000 | Loss: 0.00003787
Iteration 15/1000 | Loss: 0.00003778
Iteration 16/1000 | Loss: 0.00003775
Iteration 17/1000 | Loss: 0.00003774
Iteration 18/1000 | Loss: 0.00003772
Iteration 19/1000 | Loss: 0.00003772
Iteration 20/1000 | Loss: 0.00003772
Iteration 21/1000 | Loss: 0.00003771
Iteration 22/1000 | Loss: 0.00003771
Iteration 23/1000 | Loss: 0.00003770
Iteration 24/1000 | Loss: 0.00003770
Iteration 25/1000 | Loss: 0.00003770
Iteration 26/1000 | Loss: 0.00003770
Iteration 27/1000 | Loss: 0.00003770
Iteration 28/1000 | Loss: 0.00003769
Iteration 29/1000 | Loss: 0.00003769
Iteration 30/1000 | Loss: 0.00003769
Iteration 31/1000 | Loss: 0.00003769
Iteration 32/1000 | Loss: 0.00003768
Iteration 33/1000 | Loss: 0.00003768
Iteration 34/1000 | Loss: 0.00003768
Iteration 35/1000 | Loss: 0.00003767
Iteration 36/1000 | Loss: 0.00003767
Iteration 37/1000 | Loss: 0.00003767
Iteration 38/1000 | Loss: 0.00003766
Iteration 39/1000 | Loss: 0.00003766
Iteration 40/1000 | Loss: 0.00003766
Iteration 41/1000 | Loss: 0.00003765
Iteration 42/1000 | Loss: 0.00003765
Iteration 43/1000 | Loss: 0.00003765
Iteration 44/1000 | Loss: 0.00003765
Iteration 45/1000 | Loss: 0.00003765
Iteration 46/1000 | Loss: 0.00003765
Iteration 47/1000 | Loss: 0.00003765
Iteration 48/1000 | Loss: 0.00003765
Iteration 49/1000 | Loss: 0.00003765
Iteration 50/1000 | Loss: 0.00003765
Iteration 51/1000 | Loss: 0.00003764
Iteration 52/1000 | Loss: 0.00003764
Iteration 53/1000 | Loss: 0.00003764
Iteration 54/1000 | Loss: 0.00003764
Iteration 55/1000 | Loss: 0.00003764
Iteration 56/1000 | Loss: 0.00003763
Iteration 57/1000 | Loss: 0.00003763
Iteration 58/1000 | Loss: 0.00003763
Iteration 59/1000 | Loss: 0.00003763
Iteration 60/1000 | Loss: 0.00003762
Iteration 61/1000 | Loss: 0.00003762
Iteration 62/1000 | Loss: 0.00003762
Iteration 63/1000 | Loss: 0.00003762
Iteration 64/1000 | Loss: 0.00003762
Iteration 65/1000 | Loss: 0.00003762
Iteration 66/1000 | Loss: 0.00003762
Iteration 67/1000 | Loss: 0.00003762
Iteration 68/1000 | Loss: 0.00003761
Iteration 69/1000 | Loss: 0.00003761
Iteration 70/1000 | Loss: 0.00003761
Iteration 71/1000 | Loss: 0.00003760
Iteration 72/1000 | Loss: 0.00003760
Iteration 73/1000 | Loss: 0.00003760
Iteration 74/1000 | Loss: 0.00003759
Iteration 75/1000 | Loss: 0.00003759
Iteration 76/1000 | Loss: 0.00003759
Iteration 77/1000 | Loss: 0.00003758
Iteration 78/1000 | Loss: 0.00003757
Iteration 79/1000 | Loss: 0.00003757
Iteration 80/1000 | Loss: 0.00003757
Iteration 81/1000 | Loss: 0.00003756
Iteration 82/1000 | Loss: 0.00003756
Iteration 83/1000 | Loss: 0.00003756
Iteration 84/1000 | Loss: 0.00003755
Iteration 85/1000 | Loss: 0.00003755
Iteration 86/1000 | Loss: 0.00003755
Iteration 87/1000 | Loss: 0.00003754
Iteration 88/1000 | Loss: 0.00003754
Iteration 89/1000 | Loss: 0.00003754
Iteration 90/1000 | Loss: 0.00003753
Iteration 91/1000 | Loss: 0.00003753
Iteration 92/1000 | Loss: 0.00003753
Iteration 93/1000 | Loss: 0.00003753
Iteration 94/1000 | Loss: 0.00003752
Iteration 95/1000 | Loss: 0.00003752
Iteration 96/1000 | Loss: 0.00003751
Iteration 97/1000 | Loss: 0.00003751
Iteration 98/1000 | Loss: 0.00003751
Iteration 99/1000 | Loss: 0.00003751
Iteration 100/1000 | Loss: 0.00003751
Iteration 101/1000 | Loss: 0.00003751
Iteration 102/1000 | Loss: 0.00003751
Iteration 103/1000 | Loss: 0.00003751
Iteration 104/1000 | Loss: 0.00003751
Iteration 105/1000 | Loss: 0.00003750
Iteration 106/1000 | Loss: 0.00003750
Iteration 107/1000 | Loss: 0.00003750
Iteration 108/1000 | Loss: 0.00003750
Iteration 109/1000 | Loss: 0.00003750
Iteration 110/1000 | Loss: 0.00003750
Iteration 111/1000 | Loss: 0.00003750
Iteration 112/1000 | Loss: 0.00003750
Iteration 113/1000 | Loss: 0.00003750
Iteration 114/1000 | Loss: 0.00003749
Iteration 115/1000 | Loss: 0.00003749
Iteration 116/1000 | Loss: 0.00003749
Iteration 117/1000 | Loss: 0.00003749
Iteration 118/1000 | Loss: 0.00003749
Iteration 119/1000 | Loss: 0.00003749
Iteration 120/1000 | Loss: 0.00003749
Iteration 121/1000 | Loss: 0.00003749
Iteration 122/1000 | Loss: 0.00003749
Iteration 123/1000 | Loss: 0.00003748
Iteration 124/1000 | Loss: 0.00003748
Iteration 125/1000 | Loss: 0.00003748
Iteration 126/1000 | Loss: 0.00003748
Iteration 127/1000 | Loss: 0.00003748
Iteration 128/1000 | Loss: 0.00003748
Iteration 129/1000 | Loss: 0.00003748
Iteration 130/1000 | Loss: 0.00003748
Iteration 131/1000 | Loss: 0.00003748
Iteration 132/1000 | Loss: 0.00003748
Iteration 133/1000 | Loss: 0.00003748
Iteration 134/1000 | Loss: 0.00003748
Iteration 135/1000 | Loss: 0.00003748
Iteration 136/1000 | Loss: 0.00003748
Iteration 137/1000 | Loss: 0.00003748
Iteration 138/1000 | Loss: 0.00003748
Iteration 139/1000 | Loss: 0.00003748
Iteration 140/1000 | Loss: 0.00003748
Iteration 141/1000 | Loss: 0.00003748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [3.74810078938026e-05, 3.74810078938026e-05, 3.74810078938026e-05, 3.74810078938026e-05, 3.74810078938026e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.74810078938026e-05

Optimization complete. Final v2v error: 5.350050926208496 mm

Highest mean error: 5.698886394500732 mm for frame 20

Lowest mean error: 5.138228416442871 mm for frame 85

Saving results

Total time: 36.04867959022522
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884387
Iteration 2/25 | Loss: 0.00181771
Iteration 3/25 | Loss: 0.00165938
Iteration 4/25 | Loss: 0.00163122
Iteration 5/25 | Loss: 0.00162130
Iteration 6/25 | Loss: 0.00161996
Iteration 7/25 | Loss: 0.00161996
Iteration 8/25 | Loss: 0.00161996
Iteration 9/25 | Loss: 0.00161996
Iteration 10/25 | Loss: 0.00161996
Iteration 11/25 | Loss: 0.00161996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0016199633246287704, 0.0016199633246287704, 0.0016199633246287704, 0.0016199633246287704, 0.0016199633246287704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016199633246287704

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64909899
Iteration 2/25 | Loss: 0.00240292
Iteration 3/25 | Loss: 0.00240292
Iteration 4/25 | Loss: 0.00240292
Iteration 5/25 | Loss: 0.00240291
Iteration 6/25 | Loss: 0.00240291
Iteration 7/25 | Loss: 0.00240291
Iteration 8/25 | Loss: 0.00240291
Iteration 9/25 | Loss: 0.00240291
Iteration 10/25 | Loss: 0.00240291
Iteration 11/25 | Loss: 0.00240291
Iteration 12/25 | Loss: 0.00240291
Iteration 13/25 | Loss: 0.00240291
Iteration 14/25 | Loss: 0.00240291
Iteration 15/25 | Loss: 0.00240291
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002402913523837924, 0.002402913523837924, 0.002402913523837924, 0.002402913523837924, 0.002402913523837924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002402913523837924

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00240291
Iteration 2/1000 | Loss: 0.00006410
Iteration 3/1000 | Loss: 0.00004947
Iteration 4/1000 | Loss: 0.00004209
Iteration 5/1000 | Loss: 0.00003900
Iteration 6/1000 | Loss: 0.00003718
Iteration 7/1000 | Loss: 0.00003602
Iteration 8/1000 | Loss: 0.00003515
Iteration 9/1000 | Loss: 0.00003457
Iteration 10/1000 | Loss: 0.00003403
Iteration 11/1000 | Loss: 0.00003351
Iteration 12/1000 | Loss: 0.00003322
Iteration 13/1000 | Loss: 0.00003298
Iteration 14/1000 | Loss: 0.00003281
Iteration 15/1000 | Loss: 0.00003278
Iteration 16/1000 | Loss: 0.00003278
Iteration 17/1000 | Loss: 0.00003277
Iteration 18/1000 | Loss: 0.00003276
Iteration 19/1000 | Loss: 0.00003275
Iteration 20/1000 | Loss: 0.00003275
Iteration 21/1000 | Loss: 0.00003274
Iteration 22/1000 | Loss: 0.00003274
Iteration 23/1000 | Loss: 0.00003274
Iteration 24/1000 | Loss: 0.00003273
Iteration 25/1000 | Loss: 0.00003273
Iteration 26/1000 | Loss: 0.00003271
Iteration 27/1000 | Loss: 0.00003271
Iteration 28/1000 | Loss: 0.00003270
Iteration 29/1000 | Loss: 0.00003270
Iteration 30/1000 | Loss: 0.00003270
Iteration 31/1000 | Loss: 0.00003269
Iteration 32/1000 | Loss: 0.00003269
Iteration 33/1000 | Loss: 0.00003267
Iteration 34/1000 | Loss: 0.00003267
Iteration 35/1000 | Loss: 0.00003265
Iteration 36/1000 | Loss: 0.00003265
Iteration 37/1000 | Loss: 0.00003265
Iteration 38/1000 | Loss: 0.00003263
Iteration 39/1000 | Loss: 0.00003263
Iteration 40/1000 | Loss: 0.00003263
Iteration 41/1000 | Loss: 0.00003263
Iteration 42/1000 | Loss: 0.00003262
Iteration 43/1000 | Loss: 0.00003262
Iteration 44/1000 | Loss: 0.00003261
Iteration 45/1000 | Loss: 0.00003261
Iteration 46/1000 | Loss: 0.00003261
Iteration 47/1000 | Loss: 0.00003260
Iteration 48/1000 | Loss: 0.00003260
Iteration 49/1000 | Loss: 0.00003260
Iteration 50/1000 | Loss: 0.00003259
Iteration 51/1000 | Loss: 0.00003259
Iteration 52/1000 | Loss: 0.00003258
Iteration 53/1000 | Loss: 0.00003258
Iteration 54/1000 | Loss: 0.00003257
Iteration 55/1000 | Loss: 0.00003257
Iteration 56/1000 | Loss: 0.00003257
Iteration 57/1000 | Loss: 0.00003257
Iteration 58/1000 | Loss: 0.00003257
Iteration 59/1000 | Loss: 0.00003257
Iteration 60/1000 | Loss: 0.00003257
Iteration 61/1000 | Loss: 0.00003257
Iteration 62/1000 | Loss: 0.00003257
Iteration 63/1000 | Loss: 0.00003257
Iteration 64/1000 | Loss: 0.00003256
Iteration 65/1000 | Loss: 0.00003256
Iteration 66/1000 | Loss: 0.00003256
Iteration 67/1000 | Loss: 0.00003256
Iteration 68/1000 | Loss: 0.00003256
Iteration 69/1000 | Loss: 0.00003256
Iteration 70/1000 | Loss: 0.00003256
Iteration 71/1000 | Loss: 0.00003255
Iteration 72/1000 | Loss: 0.00003255
Iteration 73/1000 | Loss: 0.00003254
Iteration 74/1000 | Loss: 0.00003254
Iteration 75/1000 | Loss: 0.00003254
Iteration 76/1000 | Loss: 0.00003254
Iteration 77/1000 | Loss: 0.00003254
Iteration 78/1000 | Loss: 0.00003254
Iteration 79/1000 | Loss: 0.00003254
Iteration 80/1000 | Loss: 0.00003253
Iteration 81/1000 | Loss: 0.00003253
Iteration 82/1000 | Loss: 0.00003253
Iteration 83/1000 | Loss: 0.00003253
Iteration 84/1000 | Loss: 0.00003252
Iteration 85/1000 | Loss: 0.00003252
Iteration 86/1000 | Loss: 0.00003252
Iteration 87/1000 | Loss: 0.00003252
Iteration 88/1000 | Loss: 0.00003252
Iteration 89/1000 | Loss: 0.00003251
Iteration 90/1000 | Loss: 0.00003251
Iteration 91/1000 | Loss: 0.00003251
Iteration 92/1000 | Loss: 0.00003250
Iteration 93/1000 | Loss: 0.00003250
Iteration 94/1000 | Loss: 0.00003250
Iteration 95/1000 | Loss: 0.00003249
Iteration 96/1000 | Loss: 0.00003249
Iteration 97/1000 | Loss: 0.00003249
Iteration 98/1000 | Loss: 0.00003249
Iteration 99/1000 | Loss: 0.00003248
Iteration 100/1000 | Loss: 0.00003248
Iteration 101/1000 | Loss: 0.00003248
Iteration 102/1000 | Loss: 0.00003248
Iteration 103/1000 | Loss: 0.00003248
Iteration 104/1000 | Loss: 0.00003248
Iteration 105/1000 | Loss: 0.00003248
Iteration 106/1000 | Loss: 0.00003247
Iteration 107/1000 | Loss: 0.00003247
Iteration 108/1000 | Loss: 0.00003247
Iteration 109/1000 | Loss: 0.00003247
Iteration 110/1000 | Loss: 0.00003247
Iteration 111/1000 | Loss: 0.00003247
Iteration 112/1000 | Loss: 0.00003246
Iteration 113/1000 | Loss: 0.00003246
Iteration 114/1000 | Loss: 0.00003246
Iteration 115/1000 | Loss: 0.00003246
Iteration 116/1000 | Loss: 0.00003246
Iteration 117/1000 | Loss: 0.00003246
Iteration 118/1000 | Loss: 0.00003246
Iteration 119/1000 | Loss: 0.00003246
Iteration 120/1000 | Loss: 0.00003246
Iteration 121/1000 | Loss: 0.00003246
Iteration 122/1000 | Loss: 0.00003245
Iteration 123/1000 | Loss: 0.00003245
Iteration 124/1000 | Loss: 0.00003245
Iteration 125/1000 | Loss: 0.00003245
Iteration 126/1000 | Loss: 0.00003245
Iteration 127/1000 | Loss: 0.00003245
Iteration 128/1000 | Loss: 0.00003245
Iteration 129/1000 | Loss: 0.00003245
Iteration 130/1000 | Loss: 0.00003245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [3.245407424401492e-05, 3.245407424401492e-05, 3.245407424401492e-05, 3.245407424401492e-05, 3.245407424401492e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.245407424401492e-05

Optimization complete. Final v2v error: 4.959117412567139 mm

Highest mean error: 5.296010971069336 mm for frame 21

Lowest mean error: 4.58139705657959 mm for frame 216

Saving results

Total time: 41.582436084747314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481097
Iteration 2/25 | Loss: 0.00180913
Iteration 3/25 | Loss: 0.00159304
Iteration 4/25 | Loss: 0.00156170
Iteration 5/25 | Loss: 0.00155499
Iteration 6/25 | Loss: 0.00155305
Iteration 7/25 | Loss: 0.00155263
Iteration 8/25 | Loss: 0.00155263
Iteration 9/25 | Loss: 0.00155263
Iteration 10/25 | Loss: 0.00155263
Iteration 11/25 | Loss: 0.00155263
Iteration 12/25 | Loss: 0.00155263
Iteration 13/25 | Loss: 0.00155263
Iteration 14/25 | Loss: 0.00155263
Iteration 15/25 | Loss: 0.00155263
Iteration 16/25 | Loss: 0.00155263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015526332426816225, 0.0015526332426816225, 0.0015526332426816225, 0.0015526332426816225, 0.0015526332426816225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015526332426816225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58396864
Iteration 2/25 | Loss: 0.00215881
Iteration 3/25 | Loss: 0.00215881
Iteration 4/25 | Loss: 0.00215881
Iteration 5/25 | Loss: 0.00215881
Iteration 6/25 | Loss: 0.00215881
Iteration 7/25 | Loss: 0.00215881
Iteration 8/25 | Loss: 0.00215881
Iteration 9/25 | Loss: 0.00215881
Iteration 10/25 | Loss: 0.00215881
Iteration 11/25 | Loss: 0.00215881
Iteration 12/25 | Loss: 0.00215881
Iteration 13/25 | Loss: 0.00215881
Iteration 14/25 | Loss: 0.00215881
Iteration 15/25 | Loss: 0.00215881
Iteration 16/25 | Loss: 0.00215881
Iteration 17/25 | Loss: 0.00215881
Iteration 18/25 | Loss: 0.00215881
Iteration 19/25 | Loss: 0.00215881
Iteration 20/25 | Loss: 0.00215881
Iteration 21/25 | Loss: 0.00215881
Iteration 22/25 | Loss: 0.00215881
Iteration 23/25 | Loss: 0.00215881
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002158805262297392, 0.002158805262297392, 0.002158805262297392, 0.002158805262297392, 0.002158805262297392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002158805262297392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215881
Iteration 2/1000 | Loss: 0.00004737
Iteration 3/1000 | Loss: 0.00003394
Iteration 4/1000 | Loss: 0.00002926
Iteration 5/1000 | Loss: 0.00002677
Iteration 6/1000 | Loss: 0.00002554
Iteration 7/1000 | Loss: 0.00002469
Iteration 8/1000 | Loss: 0.00002402
Iteration 9/1000 | Loss: 0.00002373
Iteration 10/1000 | Loss: 0.00002340
Iteration 11/1000 | Loss: 0.00002313
Iteration 12/1000 | Loss: 0.00002287
Iteration 13/1000 | Loss: 0.00002270
Iteration 14/1000 | Loss: 0.00002258
Iteration 15/1000 | Loss: 0.00002257
Iteration 16/1000 | Loss: 0.00002257
Iteration 17/1000 | Loss: 0.00002257
Iteration 18/1000 | Loss: 0.00002256
Iteration 19/1000 | Loss: 0.00002254
Iteration 20/1000 | Loss: 0.00002253
Iteration 21/1000 | Loss: 0.00002253
Iteration 22/1000 | Loss: 0.00002252
Iteration 23/1000 | Loss: 0.00002251
Iteration 24/1000 | Loss: 0.00002250
Iteration 25/1000 | Loss: 0.00002250
Iteration 26/1000 | Loss: 0.00002249
Iteration 27/1000 | Loss: 0.00002249
Iteration 28/1000 | Loss: 0.00002249
Iteration 29/1000 | Loss: 0.00002249
Iteration 30/1000 | Loss: 0.00002248
Iteration 31/1000 | Loss: 0.00002248
Iteration 32/1000 | Loss: 0.00002247
Iteration 33/1000 | Loss: 0.00002246
Iteration 34/1000 | Loss: 0.00002243
Iteration 35/1000 | Loss: 0.00002243
Iteration 36/1000 | Loss: 0.00002243
Iteration 37/1000 | Loss: 0.00002242
Iteration 38/1000 | Loss: 0.00002242
Iteration 39/1000 | Loss: 0.00002241
Iteration 40/1000 | Loss: 0.00002241
Iteration 41/1000 | Loss: 0.00002241
Iteration 42/1000 | Loss: 0.00002241
Iteration 43/1000 | Loss: 0.00002241
Iteration 44/1000 | Loss: 0.00002240
Iteration 45/1000 | Loss: 0.00002240
Iteration 46/1000 | Loss: 0.00002240
Iteration 47/1000 | Loss: 0.00002239
Iteration 48/1000 | Loss: 0.00002239
Iteration 49/1000 | Loss: 0.00002239
Iteration 50/1000 | Loss: 0.00002239
Iteration 51/1000 | Loss: 0.00002239
Iteration 52/1000 | Loss: 0.00002239
Iteration 53/1000 | Loss: 0.00002239
Iteration 54/1000 | Loss: 0.00002239
Iteration 55/1000 | Loss: 0.00002239
Iteration 56/1000 | Loss: 0.00002239
Iteration 57/1000 | Loss: 0.00002239
Iteration 58/1000 | Loss: 0.00002238
Iteration 59/1000 | Loss: 0.00002238
Iteration 60/1000 | Loss: 0.00002238
Iteration 61/1000 | Loss: 0.00002238
Iteration 62/1000 | Loss: 0.00002237
Iteration 63/1000 | Loss: 0.00002237
Iteration 64/1000 | Loss: 0.00002237
Iteration 65/1000 | Loss: 0.00002237
Iteration 66/1000 | Loss: 0.00002237
Iteration 67/1000 | Loss: 0.00002236
Iteration 68/1000 | Loss: 0.00002236
Iteration 69/1000 | Loss: 0.00002236
Iteration 70/1000 | Loss: 0.00002236
Iteration 71/1000 | Loss: 0.00002236
Iteration 72/1000 | Loss: 0.00002236
Iteration 73/1000 | Loss: 0.00002236
Iteration 74/1000 | Loss: 0.00002236
Iteration 75/1000 | Loss: 0.00002236
Iteration 76/1000 | Loss: 0.00002236
Iteration 77/1000 | Loss: 0.00002236
Iteration 78/1000 | Loss: 0.00002236
Iteration 79/1000 | Loss: 0.00002236
Iteration 80/1000 | Loss: 0.00002235
Iteration 81/1000 | Loss: 0.00002235
Iteration 82/1000 | Loss: 0.00002235
Iteration 83/1000 | Loss: 0.00002235
Iteration 84/1000 | Loss: 0.00002235
Iteration 85/1000 | Loss: 0.00002235
Iteration 86/1000 | Loss: 0.00002235
Iteration 87/1000 | Loss: 0.00002235
Iteration 88/1000 | Loss: 0.00002235
Iteration 89/1000 | Loss: 0.00002235
Iteration 90/1000 | Loss: 0.00002235
Iteration 91/1000 | Loss: 0.00002235
Iteration 92/1000 | Loss: 0.00002235
Iteration 93/1000 | Loss: 0.00002234
Iteration 94/1000 | Loss: 0.00002234
Iteration 95/1000 | Loss: 0.00002234
Iteration 96/1000 | Loss: 0.00002234
Iteration 97/1000 | Loss: 0.00002234
Iteration 98/1000 | Loss: 0.00002234
Iteration 99/1000 | Loss: 0.00002234
Iteration 100/1000 | Loss: 0.00002234
Iteration 101/1000 | Loss: 0.00002234
Iteration 102/1000 | Loss: 0.00002234
Iteration 103/1000 | Loss: 0.00002234
Iteration 104/1000 | Loss: 0.00002234
Iteration 105/1000 | Loss: 0.00002234
Iteration 106/1000 | Loss: 0.00002233
Iteration 107/1000 | Loss: 0.00002233
Iteration 108/1000 | Loss: 0.00002233
Iteration 109/1000 | Loss: 0.00002233
Iteration 110/1000 | Loss: 0.00002233
Iteration 111/1000 | Loss: 0.00002233
Iteration 112/1000 | Loss: 0.00002233
Iteration 113/1000 | Loss: 0.00002233
Iteration 114/1000 | Loss: 0.00002233
Iteration 115/1000 | Loss: 0.00002232
Iteration 116/1000 | Loss: 0.00002232
Iteration 117/1000 | Loss: 0.00002232
Iteration 118/1000 | Loss: 0.00002232
Iteration 119/1000 | Loss: 0.00002232
Iteration 120/1000 | Loss: 0.00002232
Iteration 121/1000 | Loss: 0.00002232
Iteration 122/1000 | Loss: 0.00002232
Iteration 123/1000 | Loss: 0.00002232
Iteration 124/1000 | Loss: 0.00002231
Iteration 125/1000 | Loss: 0.00002231
Iteration 126/1000 | Loss: 0.00002231
Iteration 127/1000 | Loss: 0.00002231
Iteration 128/1000 | Loss: 0.00002231
Iteration 129/1000 | Loss: 0.00002231
Iteration 130/1000 | Loss: 0.00002231
Iteration 131/1000 | Loss: 0.00002231
Iteration 132/1000 | Loss: 0.00002231
Iteration 133/1000 | Loss: 0.00002231
Iteration 134/1000 | Loss: 0.00002231
Iteration 135/1000 | Loss: 0.00002231
Iteration 136/1000 | Loss: 0.00002230
Iteration 137/1000 | Loss: 0.00002230
Iteration 138/1000 | Loss: 0.00002230
Iteration 139/1000 | Loss: 0.00002230
Iteration 140/1000 | Loss: 0.00002230
Iteration 141/1000 | Loss: 0.00002230
Iteration 142/1000 | Loss: 0.00002230
Iteration 143/1000 | Loss: 0.00002230
Iteration 144/1000 | Loss: 0.00002230
Iteration 145/1000 | Loss: 0.00002230
Iteration 146/1000 | Loss: 0.00002230
Iteration 147/1000 | Loss: 0.00002230
Iteration 148/1000 | Loss: 0.00002230
Iteration 149/1000 | Loss: 0.00002230
Iteration 150/1000 | Loss: 0.00002230
Iteration 151/1000 | Loss: 0.00002229
Iteration 152/1000 | Loss: 0.00002229
Iteration 153/1000 | Loss: 0.00002229
Iteration 154/1000 | Loss: 0.00002229
Iteration 155/1000 | Loss: 0.00002229
Iteration 156/1000 | Loss: 0.00002229
Iteration 157/1000 | Loss: 0.00002229
Iteration 158/1000 | Loss: 0.00002229
Iteration 159/1000 | Loss: 0.00002229
Iteration 160/1000 | Loss: 0.00002228
Iteration 161/1000 | Loss: 0.00002228
Iteration 162/1000 | Loss: 0.00002228
Iteration 163/1000 | Loss: 0.00002228
Iteration 164/1000 | Loss: 0.00002228
Iteration 165/1000 | Loss: 0.00002228
Iteration 166/1000 | Loss: 0.00002228
Iteration 167/1000 | Loss: 0.00002228
Iteration 168/1000 | Loss: 0.00002228
Iteration 169/1000 | Loss: 0.00002228
Iteration 170/1000 | Loss: 0.00002228
Iteration 171/1000 | Loss: 0.00002228
Iteration 172/1000 | Loss: 0.00002228
Iteration 173/1000 | Loss: 0.00002228
Iteration 174/1000 | Loss: 0.00002228
Iteration 175/1000 | Loss: 0.00002228
Iteration 176/1000 | Loss: 0.00002228
Iteration 177/1000 | Loss: 0.00002228
Iteration 178/1000 | Loss: 0.00002228
Iteration 179/1000 | Loss: 0.00002228
Iteration 180/1000 | Loss: 0.00002228
Iteration 181/1000 | Loss: 0.00002228
Iteration 182/1000 | Loss: 0.00002228
Iteration 183/1000 | Loss: 0.00002228
Iteration 184/1000 | Loss: 0.00002228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.2283082216745242e-05, 2.2283082216745242e-05, 2.2283082216745242e-05, 2.2283082216745242e-05, 2.2283082216745242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2283082216745242e-05

Optimization complete. Final v2v error: 4.115906238555908 mm

Highest mean error: 4.820662498474121 mm for frame 98

Lowest mean error: 3.9090943336486816 mm for frame 136

Saving results

Total time: 42.454230308532715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01133089
Iteration 2/25 | Loss: 0.00317865
Iteration 3/25 | Loss: 0.00272565
Iteration 4/25 | Loss: 0.00235486
Iteration 5/25 | Loss: 0.00236210
Iteration 6/25 | Loss: 0.00232704
Iteration 7/25 | Loss: 0.00230812
Iteration 8/25 | Loss: 0.00210942
Iteration 9/25 | Loss: 0.00207167
Iteration 10/25 | Loss: 0.00203161
Iteration 11/25 | Loss: 0.00199836
Iteration 12/25 | Loss: 0.00195836
Iteration 13/25 | Loss: 0.00195057
Iteration 14/25 | Loss: 0.00194481
Iteration 15/25 | Loss: 0.00194589
Iteration 16/25 | Loss: 0.00193596
Iteration 17/25 | Loss: 0.00192926
Iteration 18/25 | Loss: 0.00193954
Iteration 19/25 | Loss: 0.00192616
Iteration 20/25 | Loss: 0.00191589
Iteration 21/25 | Loss: 0.00190974
Iteration 22/25 | Loss: 0.00190262
Iteration 23/25 | Loss: 0.00190508
Iteration 24/25 | Loss: 0.00190146
Iteration 25/25 | Loss: 0.00189252

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64449167
Iteration 2/25 | Loss: 0.00707371
Iteration 3/25 | Loss: 0.00602394
Iteration 4/25 | Loss: 0.00602393
Iteration 5/25 | Loss: 0.00602393
Iteration 6/25 | Loss: 0.00602393
Iteration 7/25 | Loss: 0.00602393
Iteration 8/25 | Loss: 0.00602393
Iteration 9/25 | Loss: 0.00602393
Iteration 10/25 | Loss: 0.00602393
Iteration 11/25 | Loss: 0.00602393
Iteration 12/25 | Loss: 0.00602393
Iteration 13/25 | Loss: 0.00602393
Iteration 14/25 | Loss: 0.00602393
Iteration 15/25 | Loss: 0.00602393
Iteration 16/25 | Loss: 0.00602393
Iteration 17/25 | Loss: 0.00602393
Iteration 18/25 | Loss: 0.00602393
Iteration 19/25 | Loss: 0.00602393
Iteration 20/25 | Loss: 0.00602393
Iteration 21/25 | Loss: 0.00602393
Iteration 22/25 | Loss: 0.00602393
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.006023931782692671, 0.006023931782692671, 0.006023931782692671, 0.006023931782692671, 0.006023931782692671]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006023931782692671

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00602393
Iteration 2/1000 | Loss: 0.00884978
Iteration 3/1000 | Loss: 0.01028256
Iteration 4/1000 | Loss: 0.00494812
Iteration 5/1000 | Loss: 0.00817893
Iteration 6/1000 | Loss: 0.00202455
Iteration 7/1000 | Loss: 0.00320589
Iteration 8/1000 | Loss: 0.00058095
Iteration 9/1000 | Loss: 0.01078466
Iteration 10/1000 | Loss: 0.00255248
Iteration 11/1000 | Loss: 0.00317897
Iteration 12/1000 | Loss: 0.00118624
Iteration 13/1000 | Loss: 0.01446984
Iteration 14/1000 | Loss: 0.00509232
Iteration 15/1000 | Loss: 0.01183512
Iteration 16/1000 | Loss: 0.00494072
Iteration 17/1000 | Loss: 0.00706295
Iteration 18/1000 | Loss: 0.00725019
Iteration 19/1000 | Loss: 0.00271739
Iteration 20/1000 | Loss: 0.00304567
Iteration 21/1000 | Loss: 0.00341037
Iteration 22/1000 | Loss: 0.00139530
Iteration 23/1000 | Loss: 0.00367440
Iteration 24/1000 | Loss: 0.00264334
Iteration 25/1000 | Loss: 0.00174069
Iteration 26/1000 | Loss: 0.00030975
Iteration 27/1000 | Loss: 0.00049863
Iteration 28/1000 | Loss: 0.00060932
Iteration 29/1000 | Loss: 0.00023208
Iteration 30/1000 | Loss: 0.00262397
Iteration 31/1000 | Loss: 0.00275122
Iteration 32/1000 | Loss: 0.00052434
Iteration 33/1000 | Loss: 0.00021043
Iteration 34/1000 | Loss: 0.00019692
Iteration 35/1000 | Loss: 0.00516879
Iteration 36/1000 | Loss: 0.00646757
Iteration 37/1000 | Loss: 0.00241236
Iteration 38/1000 | Loss: 0.00160382
Iteration 39/1000 | Loss: 0.00535784
Iteration 40/1000 | Loss: 0.00145505
Iteration 41/1000 | Loss: 0.00109623
Iteration 42/1000 | Loss: 0.00144553
Iteration 43/1000 | Loss: 0.00154113
Iteration 44/1000 | Loss: 0.00085048
Iteration 45/1000 | Loss: 0.00121625
Iteration 46/1000 | Loss: 0.00030355
Iteration 47/1000 | Loss: 0.00201044
Iteration 48/1000 | Loss: 0.00018688
Iteration 49/1000 | Loss: 0.00105998
Iteration 50/1000 | Loss: 0.00027102
Iteration 51/1000 | Loss: 0.00023808
Iteration 52/1000 | Loss: 0.00117280
Iteration 53/1000 | Loss: 0.00015439
Iteration 54/1000 | Loss: 0.00205102
Iteration 55/1000 | Loss: 0.00214287
Iteration 56/1000 | Loss: 0.00220593
Iteration 57/1000 | Loss: 0.00232149
Iteration 58/1000 | Loss: 0.00036814
Iteration 59/1000 | Loss: 0.00033187
Iteration 60/1000 | Loss: 0.00226929
Iteration 61/1000 | Loss: 0.00212724
Iteration 62/1000 | Loss: 0.00154429
Iteration 63/1000 | Loss: 0.00017637
Iteration 64/1000 | Loss: 0.00050034
Iteration 65/1000 | Loss: 0.00106526
Iteration 66/1000 | Loss: 0.00009919
Iteration 67/1000 | Loss: 0.00155836
Iteration 68/1000 | Loss: 0.00010785
Iteration 69/1000 | Loss: 0.00159838
Iteration 70/1000 | Loss: 0.00014335
Iteration 71/1000 | Loss: 0.00019207
Iteration 72/1000 | Loss: 0.00008425
Iteration 73/1000 | Loss: 0.00005767
Iteration 74/1000 | Loss: 0.00004786
Iteration 75/1000 | Loss: 0.00120494
Iteration 76/1000 | Loss: 0.00043222
Iteration 77/1000 | Loss: 0.00005427
Iteration 78/1000 | Loss: 0.00004704
Iteration 79/1000 | Loss: 0.00007063
Iteration 80/1000 | Loss: 0.00040107
Iteration 81/1000 | Loss: 0.00005427
Iteration 82/1000 | Loss: 0.00004339
Iteration 83/1000 | Loss: 0.00043650
Iteration 84/1000 | Loss: 0.00045703
Iteration 85/1000 | Loss: 0.00031596
Iteration 86/1000 | Loss: 0.00014160
Iteration 87/1000 | Loss: 0.00007017
Iteration 88/1000 | Loss: 0.00042012
Iteration 89/1000 | Loss: 0.00017460
Iteration 90/1000 | Loss: 0.00029398
Iteration 91/1000 | Loss: 0.00034803
Iteration 92/1000 | Loss: 0.00016072
Iteration 93/1000 | Loss: 0.00004301
Iteration 94/1000 | Loss: 0.00038858
Iteration 95/1000 | Loss: 0.00101298
Iteration 96/1000 | Loss: 0.00133546
Iteration 97/1000 | Loss: 0.00241376
Iteration 98/1000 | Loss: 0.00006039
Iteration 99/1000 | Loss: 0.00003980
Iteration 100/1000 | Loss: 0.00003574
Iteration 101/1000 | Loss: 0.00018970
Iteration 102/1000 | Loss: 0.00003312
Iteration 103/1000 | Loss: 0.00003225
Iteration 104/1000 | Loss: 0.00003155
Iteration 105/1000 | Loss: 0.00003093
Iteration 106/1000 | Loss: 0.00018142
Iteration 107/1000 | Loss: 0.00003067
Iteration 108/1000 | Loss: 0.00002855
Iteration 109/1000 | Loss: 0.00002813
Iteration 110/1000 | Loss: 0.00002782
Iteration 111/1000 | Loss: 0.00002766
Iteration 112/1000 | Loss: 0.00002757
Iteration 113/1000 | Loss: 0.00002756
Iteration 114/1000 | Loss: 0.00002756
Iteration 115/1000 | Loss: 0.00002756
Iteration 116/1000 | Loss: 0.00002755
Iteration 117/1000 | Loss: 0.00002755
Iteration 118/1000 | Loss: 0.00002755
Iteration 119/1000 | Loss: 0.00002755
Iteration 120/1000 | Loss: 0.00002754
Iteration 121/1000 | Loss: 0.00002754
Iteration 122/1000 | Loss: 0.00002754
Iteration 123/1000 | Loss: 0.00002753
Iteration 124/1000 | Loss: 0.00002753
Iteration 125/1000 | Loss: 0.00002753
Iteration 126/1000 | Loss: 0.00002752
Iteration 127/1000 | Loss: 0.00002752
Iteration 128/1000 | Loss: 0.00002752
Iteration 129/1000 | Loss: 0.00002752
Iteration 130/1000 | Loss: 0.00002751
Iteration 131/1000 | Loss: 0.00002751
Iteration 132/1000 | Loss: 0.00002751
Iteration 133/1000 | Loss: 0.00002751
Iteration 134/1000 | Loss: 0.00002751
Iteration 135/1000 | Loss: 0.00002751
Iteration 136/1000 | Loss: 0.00002750
Iteration 137/1000 | Loss: 0.00002750
Iteration 138/1000 | Loss: 0.00002750
Iteration 139/1000 | Loss: 0.00002750
Iteration 140/1000 | Loss: 0.00002750
Iteration 141/1000 | Loss: 0.00002750
Iteration 142/1000 | Loss: 0.00002750
Iteration 143/1000 | Loss: 0.00002750
Iteration 144/1000 | Loss: 0.00002750
Iteration 145/1000 | Loss: 0.00002749
Iteration 146/1000 | Loss: 0.00002749
Iteration 147/1000 | Loss: 0.00002749
Iteration 148/1000 | Loss: 0.00002749
Iteration 149/1000 | Loss: 0.00002748
Iteration 150/1000 | Loss: 0.00002748
Iteration 151/1000 | Loss: 0.00002748
Iteration 152/1000 | Loss: 0.00002748
Iteration 153/1000 | Loss: 0.00002748
Iteration 154/1000 | Loss: 0.00002748
Iteration 155/1000 | Loss: 0.00002748
Iteration 156/1000 | Loss: 0.00002748
Iteration 157/1000 | Loss: 0.00002748
Iteration 158/1000 | Loss: 0.00002747
Iteration 159/1000 | Loss: 0.00002747
Iteration 160/1000 | Loss: 0.00002747
Iteration 161/1000 | Loss: 0.00002747
Iteration 162/1000 | Loss: 0.00002747
Iteration 163/1000 | Loss: 0.00002747
Iteration 164/1000 | Loss: 0.00002747
Iteration 165/1000 | Loss: 0.00002747
Iteration 166/1000 | Loss: 0.00002747
Iteration 167/1000 | Loss: 0.00002747
Iteration 168/1000 | Loss: 0.00002746
Iteration 169/1000 | Loss: 0.00002746
Iteration 170/1000 | Loss: 0.00002746
Iteration 171/1000 | Loss: 0.00002746
Iteration 172/1000 | Loss: 0.00002746
Iteration 173/1000 | Loss: 0.00002746
Iteration 174/1000 | Loss: 0.00002746
Iteration 175/1000 | Loss: 0.00002746
Iteration 176/1000 | Loss: 0.00002746
Iteration 177/1000 | Loss: 0.00002746
Iteration 178/1000 | Loss: 0.00002746
Iteration 179/1000 | Loss: 0.00002746
Iteration 180/1000 | Loss: 0.00002746
Iteration 181/1000 | Loss: 0.00002746
Iteration 182/1000 | Loss: 0.00002746
Iteration 183/1000 | Loss: 0.00002745
Iteration 184/1000 | Loss: 0.00002745
Iteration 185/1000 | Loss: 0.00002745
Iteration 186/1000 | Loss: 0.00002745
Iteration 187/1000 | Loss: 0.00002745
Iteration 188/1000 | Loss: 0.00002745
Iteration 189/1000 | Loss: 0.00002745
Iteration 190/1000 | Loss: 0.00002744
Iteration 191/1000 | Loss: 0.00002744
Iteration 192/1000 | Loss: 0.00002744
Iteration 193/1000 | Loss: 0.00002744
Iteration 194/1000 | Loss: 0.00002744
Iteration 195/1000 | Loss: 0.00002744
Iteration 196/1000 | Loss: 0.00002744
Iteration 197/1000 | Loss: 0.00002743
Iteration 198/1000 | Loss: 0.00002743
Iteration 199/1000 | Loss: 0.00002743
Iteration 200/1000 | Loss: 0.00002743
Iteration 201/1000 | Loss: 0.00002743
Iteration 202/1000 | Loss: 0.00002743
Iteration 203/1000 | Loss: 0.00002743
Iteration 204/1000 | Loss: 0.00002743
Iteration 205/1000 | Loss: 0.00002743
Iteration 206/1000 | Loss: 0.00002743
Iteration 207/1000 | Loss: 0.00002743
Iteration 208/1000 | Loss: 0.00002743
Iteration 209/1000 | Loss: 0.00002743
Iteration 210/1000 | Loss: 0.00002743
Iteration 211/1000 | Loss: 0.00002743
Iteration 212/1000 | Loss: 0.00002743
Iteration 213/1000 | Loss: 0.00002743
Iteration 214/1000 | Loss: 0.00002743
Iteration 215/1000 | Loss: 0.00002742
Iteration 216/1000 | Loss: 0.00002742
Iteration 217/1000 | Loss: 0.00002742
Iteration 218/1000 | Loss: 0.00002742
Iteration 219/1000 | Loss: 0.00002742
Iteration 220/1000 | Loss: 0.00002742
Iteration 221/1000 | Loss: 0.00002742
Iteration 222/1000 | Loss: 0.00002742
Iteration 223/1000 | Loss: 0.00002742
Iteration 224/1000 | Loss: 0.00002742
Iteration 225/1000 | Loss: 0.00002742
Iteration 226/1000 | Loss: 0.00002742
Iteration 227/1000 | Loss: 0.00002742
Iteration 228/1000 | Loss: 0.00002742
Iteration 229/1000 | Loss: 0.00002742
Iteration 230/1000 | Loss: 0.00002742
Iteration 231/1000 | Loss: 0.00002742
Iteration 232/1000 | Loss: 0.00002742
Iteration 233/1000 | Loss: 0.00002742
Iteration 234/1000 | Loss: 0.00002742
Iteration 235/1000 | Loss: 0.00002742
Iteration 236/1000 | Loss: 0.00002742
Iteration 237/1000 | Loss: 0.00002742
Iteration 238/1000 | Loss: 0.00002742
Iteration 239/1000 | Loss: 0.00002742
Iteration 240/1000 | Loss: 0.00002742
Iteration 241/1000 | Loss: 0.00002742
Iteration 242/1000 | Loss: 0.00002742
Iteration 243/1000 | Loss: 0.00002742
Iteration 244/1000 | Loss: 0.00002742
Iteration 245/1000 | Loss: 0.00002742
Iteration 246/1000 | Loss: 0.00002742
Iteration 247/1000 | Loss: 0.00002742
Iteration 248/1000 | Loss: 0.00002742
Iteration 249/1000 | Loss: 0.00002742
Iteration 250/1000 | Loss: 0.00002742
Iteration 251/1000 | Loss: 0.00002742
Iteration 252/1000 | Loss: 0.00002742
Iteration 253/1000 | Loss: 0.00002742
Iteration 254/1000 | Loss: 0.00002742
Iteration 255/1000 | Loss: 0.00002742
Iteration 256/1000 | Loss: 0.00002742
Iteration 257/1000 | Loss: 0.00002742
Iteration 258/1000 | Loss: 0.00002742
Iteration 259/1000 | Loss: 0.00002742
Iteration 260/1000 | Loss: 0.00002742
Iteration 261/1000 | Loss: 0.00002742
Iteration 262/1000 | Loss: 0.00002742
Iteration 263/1000 | Loss: 0.00002742
Iteration 264/1000 | Loss: 0.00002742
Iteration 265/1000 | Loss: 0.00002742
Iteration 266/1000 | Loss: 0.00002742
Iteration 267/1000 | Loss: 0.00002742
Iteration 268/1000 | Loss: 0.00002742
Iteration 269/1000 | Loss: 0.00002742
Iteration 270/1000 | Loss: 0.00002742
Iteration 271/1000 | Loss: 0.00002742
Iteration 272/1000 | Loss: 0.00002742
Iteration 273/1000 | Loss: 0.00002742
Iteration 274/1000 | Loss: 0.00002742
Iteration 275/1000 | Loss: 0.00002742
Iteration 276/1000 | Loss: 0.00002742
Iteration 277/1000 | Loss: 0.00002742
Iteration 278/1000 | Loss: 0.00002742
Iteration 279/1000 | Loss: 0.00002742
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [2.7422322091297247e-05, 2.7422322091297247e-05, 2.7422322091297247e-05, 2.7422322091297247e-05, 2.7422322091297247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7422322091297247e-05

Optimization complete. Final v2v error: 4.1621599197387695 mm

Highest mean error: 13.892644882202148 mm for frame 77

Lowest mean error: 3.6813619136810303 mm for frame 99

Saving results

Total time: 204.91295671463013
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01124532
Iteration 2/25 | Loss: 0.00238899
Iteration 3/25 | Loss: 0.00185577
Iteration 4/25 | Loss: 0.00182127
Iteration 5/25 | Loss: 0.00180706
Iteration 6/25 | Loss: 0.00180409
Iteration 7/25 | Loss: 0.00180380
Iteration 8/25 | Loss: 0.00180380
Iteration 9/25 | Loss: 0.00180380
Iteration 10/25 | Loss: 0.00180380
Iteration 11/25 | Loss: 0.00180380
Iteration 12/25 | Loss: 0.00180380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0018037994159385562, 0.0018037994159385562, 0.0018037994159385562, 0.0018037994159385562, 0.0018037994159385562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018037994159385562

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.65047431
Iteration 2/25 | Loss: 0.00194316
Iteration 3/25 | Loss: 0.00194315
Iteration 4/25 | Loss: 0.00194315
Iteration 5/25 | Loss: 0.00194315
Iteration 6/25 | Loss: 0.00194315
Iteration 7/25 | Loss: 0.00194315
Iteration 8/25 | Loss: 0.00194315
Iteration 9/25 | Loss: 0.00194315
Iteration 10/25 | Loss: 0.00194315
Iteration 11/25 | Loss: 0.00194315
Iteration 12/25 | Loss: 0.00194315
Iteration 13/25 | Loss: 0.00194315
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0019431516993790865, 0.0019431516993790865, 0.0019431516993790865, 0.0019431516993790865, 0.0019431516993790865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019431516993790865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194315
Iteration 2/1000 | Loss: 0.00011440
Iteration 3/1000 | Loss: 0.00008192
Iteration 4/1000 | Loss: 0.00007718
Iteration 5/1000 | Loss: 0.00007279
Iteration 6/1000 | Loss: 0.00007002
Iteration 7/1000 | Loss: 0.00006885
Iteration 8/1000 | Loss: 0.00006788
Iteration 9/1000 | Loss: 0.00006710
Iteration 10/1000 | Loss: 0.00006643
Iteration 11/1000 | Loss: 0.00006598
Iteration 12/1000 | Loss: 0.00006573
Iteration 13/1000 | Loss: 0.00006550
Iteration 14/1000 | Loss: 0.00006532
Iteration 15/1000 | Loss: 0.00006517
Iteration 16/1000 | Loss: 0.00006516
Iteration 17/1000 | Loss: 0.00006501
Iteration 18/1000 | Loss: 0.00006489
Iteration 19/1000 | Loss: 0.00006484
Iteration 20/1000 | Loss: 0.00006484
Iteration 21/1000 | Loss: 0.00006484
Iteration 22/1000 | Loss: 0.00006484
Iteration 23/1000 | Loss: 0.00006484
Iteration 24/1000 | Loss: 0.00006484
Iteration 25/1000 | Loss: 0.00006484
Iteration 26/1000 | Loss: 0.00006484
Iteration 27/1000 | Loss: 0.00006483
Iteration 28/1000 | Loss: 0.00006483
Iteration 29/1000 | Loss: 0.00006483
Iteration 30/1000 | Loss: 0.00006483
Iteration 31/1000 | Loss: 0.00006483
Iteration 32/1000 | Loss: 0.00006483
Iteration 33/1000 | Loss: 0.00006482
Iteration 34/1000 | Loss: 0.00006482
Iteration 35/1000 | Loss: 0.00006482
Iteration 36/1000 | Loss: 0.00006481
Iteration 37/1000 | Loss: 0.00006481
Iteration 38/1000 | Loss: 0.00006480
Iteration 39/1000 | Loss: 0.00006480
Iteration 40/1000 | Loss: 0.00006480
Iteration 41/1000 | Loss: 0.00006480
Iteration 42/1000 | Loss: 0.00006479
Iteration 43/1000 | Loss: 0.00006479
Iteration 44/1000 | Loss: 0.00006479
Iteration 45/1000 | Loss: 0.00006479
Iteration 46/1000 | Loss: 0.00006479
Iteration 47/1000 | Loss: 0.00006479
Iteration 48/1000 | Loss: 0.00006479
Iteration 49/1000 | Loss: 0.00006479
Iteration 50/1000 | Loss: 0.00006479
Iteration 51/1000 | Loss: 0.00006479
Iteration 52/1000 | Loss: 0.00006478
Iteration 53/1000 | Loss: 0.00006478
Iteration 54/1000 | Loss: 0.00006478
Iteration 55/1000 | Loss: 0.00006477
Iteration 56/1000 | Loss: 0.00006477
Iteration 57/1000 | Loss: 0.00006476
Iteration 58/1000 | Loss: 0.00006476
Iteration 59/1000 | Loss: 0.00006476
Iteration 60/1000 | Loss: 0.00006476
Iteration 61/1000 | Loss: 0.00006475
Iteration 62/1000 | Loss: 0.00006475
Iteration 63/1000 | Loss: 0.00006475
Iteration 64/1000 | Loss: 0.00006475
Iteration 65/1000 | Loss: 0.00006475
Iteration 66/1000 | Loss: 0.00006475
Iteration 67/1000 | Loss: 0.00006475
Iteration 68/1000 | Loss: 0.00006475
Iteration 69/1000 | Loss: 0.00006475
Iteration 70/1000 | Loss: 0.00006475
Iteration 71/1000 | Loss: 0.00006475
Iteration 72/1000 | Loss: 0.00006474
Iteration 73/1000 | Loss: 0.00006474
Iteration 74/1000 | Loss: 0.00006474
Iteration 75/1000 | Loss: 0.00006474
Iteration 76/1000 | Loss: 0.00006474
Iteration 77/1000 | Loss: 0.00006473
Iteration 78/1000 | Loss: 0.00006473
Iteration 79/1000 | Loss: 0.00006473
Iteration 80/1000 | Loss: 0.00006473
Iteration 81/1000 | Loss: 0.00006473
Iteration 82/1000 | Loss: 0.00006473
Iteration 83/1000 | Loss: 0.00006473
Iteration 84/1000 | Loss: 0.00006473
Iteration 85/1000 | Loss: 0.00006473
Iteration 86/1000 | Loss: 0.00006472
Iteration 87/1000 | Loss: 0.00006472
Iteration 88/1000 | Loss: 0.00006472
Iteration 89/1000 | Loss: 0.00006472
Iteration 90/1000 | Loss: 0.00006472
Iteration 91/1000 | Loss: 0.00006472
Iteration 92/1000 | Loss: 0.00006472
Iteration 93/1000 | Loss: 0.00006472
Iteration 94/1000 | Loss: 0.00006471
Iteration 95/1000 | Loss: 0.00006470
Iteration 96/1000 | Loss: 0.00006470
Iteration 97/1000 | Loss: 0.00006470
Iteration 98/1000 | Loss: 0.00006470
Iteration 99/1000 | Loss: 0.00006469
Iteration 100/1000 | Loss: 0.00006469
Iteration 101/1000 | Loss: 0.00006469
Iteration 102/1000 | Loss: 0.00006469
Iteration 103/1000 | Loss: 0.00006469
Iteration 104/1000 | Loss: 0.00006468
Iteration 105/1000 | Loss: 0.00006468
Iteration 106/1000 | Loss: 0.00006468
Iteration 107/1000 | Loss: 0.00006468
Iteration 108/1000 | Loss: 0.00006467
Iteration 109/1000 | Loss: 0.00006467
Iteration 110/1000 | Loss: 0.00006467
Iteration 111/1000 | Loss: 0.00006467
Iteration 112/1000 | Loss: 0.00006467
Iteration 113/1000 | Loss: 0.00006467
Iteration 114/1000 | Loss: 0.00006467
Iteration 115/1000 | Loss: 0.00006467
Iteration 116/1000 | Loss: 0.00006467
Iteration 117/1000 | Loss: 0.00006467
Iteration 118/1000 | Loss: 0.00006466
Iteration 119/1000 | Loss: 0.00006466
Iteration 120/1000 | Loss: 0.00006466
Iteration 121/1000 | Loss: 0.00006466
Iteration 122/1000 | Loss: 0.00006465
Iteration 123/1000 | Loss: 0.00006465
Iteration 124/1000 | Loss: 0.00006465
Iteration 125/1000 | Loss: 0.00006465
Iteration 126/1000 | Loss: 0.00006465
Iteration 127/1000 | Loss: 0.00006465
Iteration 128/1000 | Loss: 0.00006464
Iteration 129/1000 | Loss: 0.00006464
Iteration 130/1000 | Loss: 0.00006464
Iteration 131/1000 | Loss: 0.00006464
Iteration 132/1000 | Loss: 0.00006464
Iteration 133/1000 | Loss: 0.00006464
Iteration 134/1000 | Loss: 0.00006464
Iteration 135/1000 | Loss: 0.00006464
Iteration 136/1000 | Loss: 0.00006464
Iteration 137/1000 | Loss: 0.00006464
Iteration 138/1000 | Loss: 0.00006464
Iteration 139/1000 | Loss: 0.00006464
Iteration 140/1000 | Loss: 0.00006464
Iteration 141/1000 | Loss: 0.00006464
Iteration 142/1000 | Loss: 0.00006464
Iteration 143/1000 | Loss: 0.00006464
Iteration 144/1000 | Loss: 0.00006464
Iteration 145/1000 | Loss: 0.00006464
Iteration 146/1000 | Loss: 0.00006464
Iteration 147/1000 | Loss: 0.00006464
Iteration 148/1000 | Loss: 0.00006464
Iteration 149/1000 | Loss: 0.00006464
Iteration 150/1000 | Loss: 0.00006464
Iteration 151/1000 | Loss: 0.00006464
Iteration 152/1000 | Loss: 0.00006464
Iteration 153/1000 | Loss: 0.00006464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [6.463895260822028e-05, 6.463895260822028e-05, 6.463895260822028e-05, 6.463895260822028e-05, 6.463895260822028e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.463895260822028e-05

Optimization complete. Final v2v error: 6.676366806030273 mm

Highest mean error: 8.067005157470703 mm for frame 15

Lowest mean error: 6.200563430786133 mm for frame 192

Saving results

Total time: 43.593059062957764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854246
Iteration 2/25 | Loss: 0.00188303
Iteration 3/25 | Loss: 0.00178111
Iteration 4/25 | Loss: 0.00176073
Iteration 5/25 | Loss: 0.00175655
Iteration 6/25 | Loss: 0.00175642
Iteration 7/25 | Loss: 0.00175642
Iteration 8/25 | Loss: 0.00175642
Iteration 9/25 | Loss: 0.00175642
Iteration 10/25 | Loss: 0.00175642
Iteration 11/25 | Loss: 0.00175642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0017564158188179135, 0.0017564158188179135, 0.0017564158188179135, 0.0017564158188179135, 0.0017564158188179135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017564158188179135

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55039120
Iteration 2/25 | Loss: 0.00238603
Iteration 3/25 | Loss: 0.00238598
Iteration 4/25 | Loss: 0.00238598
Iteration 5/25 | Loss: 0.00238598
Iteration 6/25 | Loss: 0.00238598
Iteration 7/25 | Loss: 0.00238598
Iteration 8/25 | Loss: 0.00238598
Iteration 9/25 | Loss: 0.00238598
Iteration 10/25 | Loss: 0.00238598
Iteration 11/25 | Loss: 0.00238598
Iteration 12/25 | Loss: 0.00238598
Iteration 13/25 | Loss: 0.00238598
Iteration 14/25 | Loss: 0.00238598
Iteration 15/25 | Loss: 0.00238598
Iteration 16/25 | Loss: 0.00238598
Iteration 17/25 | Loss: 0.00238598
Iteration 18/25 | Loss: 0.00238598
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002385978354141116, 0.002385978354141116, 0.002385978354141116, 0.002385978354141116, 0.002385978354141116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002385978354141116

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00238598
Iteration 2/1000 | Loss: 0.00006133
Iteration 3/1000 | Loss: 0.00004577
Iteration 4/1000 | Loss: 0.00004063
Iteration 5/1000 | Loss: 0.00003819
Iteration 6/1000 | Loss: 0.00003639
Iteration 7/1000 | Loss: 0.00003507
Iteration 8/1000 | Loss: 0.00003411
Iteration 9/1000 | Loss: 0.00003348
Iteration 10/1000 | Loss: 0.00003315
Iteration 11/1000 | Loss: 0.00003284
Iteration 12/1000 | Loss: 0.00003270
Iteration 13/1000 | Loss: 0.00003268
Iteration 14/1000 | Loss: 0.00003264
Iteration 15/1000 | Loss: 0.00003258
Iteration 16/1000 | Loss: 0.00003255
Iteration 17/1000 | Loss: 0.00003252
Iteration 18/1000 | Loss: 0.00003252
Iteration 19/1000 | Loss: 0.00003249
Iteration 20/1000 | Loss: 0.00003248
Iteration 21/1000 | Loss: 0.00003247
Iteration 22/1000 | Loss: 0.00003246
Iteration 23/1000 | Loss: 0.00003246
Iteration 24/1000 | Loss: 0.00003245
Iteration 25/1000 | Loss: 0.00003245
Iteration 26/1000 | Loss: 0.00003245
Iteration 27/1000 | Loss: 0.00003244
Iteration 28/1000 | Loss: 0.00003244
Iteration 29/1000 | Loss: 0.00003244
Iteration 30/1000 | Loss: 0.00003243
Iteration 31/1000 | Loss: 0.00003243
Iteration 32/1000 | Loss: 0.00003242
Iteration 33/1000 | Loss: 0.00003242
Iteration 34/1000 | Loss: 0.00003242
Iteration 35/1000 | Loss: 0.00003241
Iteration 36/1000 | Loss: 0.00003241
Iteration 37/1000 | Loss: 0.00003241
Iteration 38/1000 | Loss: 0.00003241
Iteration 39/1000 | Loss: 0.00003241
Iteration 40/1000 | Loss: 0.00003241
Iteration 41/1000 | Loss: 0.00003241
Iteration 42/1000 | Loss: 0.00003241
Iteration 43/1000 | Loss: 0.00003241
Iteration 44/1000 | Loss: 0.00003241
Iteration 45/1000 | Loss: 0.00003240
Iteration 46/1000 | Loss: 0.00003239
Iteration 47/1000 | Loss: 0.00003239
Iteration 48/1000 | Loss: 0.00003239
Iteration 49/1000 | Loss: 0.00003238
Iteration 50/1000 | Loss: 0.00003238
Iteration 51/1000 | Loss: 0.00003237
Iteration 52/1000 | Loss: 0.00003236
Iteration 53/1000 | Loss: 0.00003235
Iteration 54/1000 | Loss: 0.00003234
Iteration 55/1000 | Loss: 0.00003234
Iteration 56/1000 | Loss: 0.00003234
Iteration 57/1000 | Loss: 0.00003233
Iteration 58/1000 | Loss: 0.00003233
Iteration 59/1000 | Loss: 0.00003233
Iteration 60/1000 | Loss: 0.00003232
Iteration 61/1000 | Loss: 0.00003231
Iteration 62/1000 | Loss: 0.00003231
Iteration 63/1000 | Loss: 0.00003231
Iteration 64/1000 | Loss: 0.00003231
Iteration 65/1000 | Loss: 0.00003231
Iteration 66/1000 | Loss: 0.00003231
Iteration 67/1000 | Loss: 0.00003231
Iteration 68/1000 | Loss: 0.00003231
Iteration 69/1000 | Loss: 0.00003231
Iteration 70/1000 | Loss: 0.00003231
Iteration 71/1000 | Loss: 0.00003230
Iteration 72/1000 | Loss: 0.00003229
Iteration 73/1000 | Loss: 0.00003229
Iteration 74/1000 | Loss: 0.00003229
Iteration 75/1000 | Loss: 0.00003229
Iteration 76/1000 | Loss: 0.00003229
Iteration 77/1000 | Loss: 0.00003229
Iteration 78/1000 | Loss: 0.00003228
Iteration 79/1000 | Loss: 0.00003228
Iteration 80/1000 | Loss: 0.00003228
Iteration 81/1000 | Loss: 0.00003228
Iteration 82/1000 | Loss: 0.00003228
Iteration 83/1000 | Loss: 0.00003228
Iteration 84/1000 | Loss: 0.00003228
Iteration 85/1000 | Loss: 0.00003228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [3.2283842301694676e-05, 3.2283842301694676e-05, 3.2283842301694676e-05, 3.2283842301694676e-05, 3.2283842301694676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2283842301694676e-05

Optimization complete. Final v2v error: 4.933316230773926 mm

Highest mean error: 5.211030006408691 mm for frame 56

Lowest mean error: 4.721478462219238 mm for frame 14

Saving results

Total time: 36.38761329650879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01122773
Iteration 2/25 | Loss: 0.00225983
Iteration 3/25 | Loss: 0.00158233
Iteration 4/25 | Loss: 0.00148788
Iteration 5/25 | Loss: 0.00160737
Iteration 6/25 | Loss: 0.00141796
Iteration 7/25 | Loss: 0.00135740
Iteration 8/25 | Loss: 0.00145521
Iteration 9/25 | Loss: 0.00129370
Iteration 10/25 | Loss: 0.00129340
Iteration 11/25 | Loss: 0.00124885
Iteration 12/25 | Loss: 0.00122478
Iteration 13/25 | Loss: 0.00122152
Iteration 14/25 | Loss: 0.00121932
Iteration 15/25 | Loss: 0.00123323
Iteration 16/25 | Loss: 0.00122225
Iteration 17/25 | Loss: 0.00125499
Iteration 18/25 | Loss: 0.00122151
Iteration 19/25 | Loss: 0.00120747
Iteration 20/25 | Loss: 0.00121293
Iteration 21/25 | Loss: 0.00120815
Iteration 22/25 | Loss: 0.00121183
Iteration 23/25 | Loss: 0.00121284
Iteration 24/25 | Loss: 0.00121032
Iteration 25/25 | Loss: 0.00121268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59666574
Iteration 2/25 | Loss: 0.00194982
Iteration 3/25 | Loss: 0.00194982
Iteration 4/25 | Loss: 0.00194982
Iteration 5/25 | Loss: 0.00194982
Iteration 6/25 | Loss: 0.00194982
Iteration 7/25 | Loss: 0.00194982
Iteration 8/25 | Loss: 0.00194982
Iteration 9/25 | Loss: 0.00194982
Iteration 10/25 | Loss: 0.00194982
Iteration 11/25 | Loss: 0.00194982
Iteration 12/25 | Loss: 0.00194982
Iteration 13/25 | Loss: 0.00194982
Iteration 14/25 | Loss: 0.00194982
Iteration 15/25 | Loss: 0.00194982
Iteration 16/25 | Loss: 0.00194982
Iteration 17/25 | Loss: 0.00194982
Iteration 18/25 | Loss: 0.00194982
Iteration 19/25 | Loss: 0.00194982
Iteration 20/25 | Loss: 0.00194982
Iteration 21/25 | Loss: 0.00194982
Iteration 22/25 | Loss: 0.00194982
Iteration 23/25 | Loss: 0.00194982
Iteration 24/25 | Loss: 0.00194982
Iteration 25/25 | Loss: 0.00194982

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194982
Iteration 2/1000 | Loss: 0.00070596
Iteration 3/1000 | Loss: 0.00056376
Iteration 4/1000 | Loss: 0.00080765
Iteration 5/1000 | Loss: 0.00068467
Iteration 6/1000 | Loss: 0.00077603
Iteration 7/1000 | Loss: 0.00050474
Iteration 8/1000 | Loss: 0.00041263
Iteration 9/1000 | Loss: 0.00072381
Iteration 10/1000 | Loss: 0.00072574
Iteration 11/1000 | Loss: 0.00058233
Iteration 12/1000 | Loss: 0.00058869
Iteration 13/1000 | Loss: 0.00044627
Iteration 14/1000 | Loss: 0.00032550
Iteration 15/1000 | Loss: 0.00045054
Iteration 16/1000 | Loss: 0.00026719
Iteration 17/1000 | Loss: 0.00035132
Iteration 18/1000 | Loss: 0.00023509
Iteration 19/1000 | Loss: 0.00024147
Iteration 20/1000 | Loss: 0.00031520
Iteration 21/1000 | Loss: 0.00091175
Iteration 22/1000 | Loss: 0.00016122
Iteration 23/1000 | Loss: 0.00022595
Iteration 24/1000 | Loss: 0.00014938
Iteration 25/1000 | Loss: 0.00025463
Iteration 26/1000 | Loss: 0.00041331
Iteration 27/1000 | Loss: 0.00072121
Iteration 28/1000 | Loss: 0.00060804
Iteration 29/1000 | Loss: 0.00048964
Iteration 30/1000 | Loss: 0.00060825
Iteration 31/1000 | Loss: 0.00057459
Iteration 32/1000 | Loss: 0.00032125
Iteration 33/1000 | Loss: 0.00041889
Iteration 34/1000 | Loss: 0.00032920
Iteration 35/1000 | Loss: 0.00047045
Iteration 36/1000 | Loss: 0.00054138
Iteration 37/1000 | Loss: 0.00048259
Iteration 38/1000 | Loss: 0.00103037
Iteration 39/1000 | Loss: 0.00041178
Iteration 40/1000 | Loss: 0.00034074
Iteration 41/1000 | Loss: 0.00022287
Iteration 42/1000 | Loss: 0.00016388
Iteration 43/1000 | Loss: 0.00024177
Iteration 44/1000 | Loss: 0.00016554
Iteration 45/1000 | Loss: 0.00010241
Iteration 46/1000 | Loss: 0.00012074
Iteration 47/1000 | Loss: 0.00027196
Iteration 48/1000 | Loss: 0.00019743
Iteration 49/1000 | Loss: 0.00026828
Iteration 50/1000 | Loss: 0.00006400
Iteration 51/1000 | Loss: 0.00004707
Iteration 52/1000 | Loss: 0.00017481
Iteration 53/1000 | Loss: 0.00006760
Iteration 54/1000 | Loss: 0.00028519
Iteration 55/1000 | Loss: 0.00029307
Iteration 56/1000 | Loss: 0.00034565
Iteration 57/1000 | Loss: 0.00023295
Iteration 58/1000 | Loss: 0.00024102
Iteration 59/1000 | Loss: 0.00038084
Iteration 60/1000 | Loss: 0.00021528
Iteration 61/1000 | Loss: 0.00024005
Iteration 62/1000 | Loss: 0.00048135
Iteration 63/1000 | Loss: 0.00005467
Iteration 64/1000 | Loss: 0.00014091
Iteration 65/1000 | Loss: 0.00011022
Iteration 66/1000 | Loss: 0.00012885
Iteration 67/1000 | Loss: 0.00004190
Iteration 68/1000 | Loss: 0.00085685
Iteration 69/1000 | Loss: 0.00128060
Iteration 70/1000 | Loss: 0.00096685
Iteration 71/1000 | Loss: 0.00093495
Iteration 72/1000 | Loss: 0.00081514
Iteration 73/1000 | Loss: 0.00063533
Iteration 74/1000 | Loss: 0.00052709
Iteration 75/1000 | Loss: 0.00047569
Iteration 76/1000 | Loss: 0.00055683
Iteration 77/1000 | Loss: 0.00026859
Iteration 78/1000 | Loss: 0.00031653
Iteration 79/1000 | Loss: 0.00106142
Iteration 80/1000 | Loss: 0.00111613
Iteration 81/1000 | Loss: 0.00018067
Iteration 82/1000 | Loss: 0.00021003
Iteration 83/1000 | Loss: 0.00196361
Iteration 84/1000 | Loss: 0.00065571
Iteration 85/1000 | Loss: 0.00007693
Iteration 86/1000 | Loss: 0.00092484
Iteration 87/1000 | Loss: 0.00046236
Iteration 88/1000 | Loss: 0.00005122
Iteration 89/1000 | Loss: 0.00004276
Iteration 90/1000 | Loss: 0.00003845
Iteration 91/1000 | Loss: 0.00003582
Iteration 92/1000 | Loss: 0.00003571
Iteration 93/1000 | Loss: 0.00003265
Iteration 94/1000 | Loss: 0.00003064
Iteration 95/1000 | Loss: 0.00028141
Iteration 96/1000 | Loss: 0.00004920
Iteration 97/1000 | Loss: 0.00003892
Iteration 98/1000 | Loss: 0.00003597
Iteration 99/1000 | Loss: 0.00021670
Iteration 100/1000 | Loss: 0.00010436
Iteration 101/1000 | Loss: 0.00011972
Iteration 102/1000 | Loss: 0.00011286
Iteration 103/1000 | Loss: 0.00015005
Iteration 104/1000 | Loss: 0.00018082
Iteration 105/1000 | Loss: 0.00003871
Iteration 106/1000 | Loss: 0.00003461
Iteration 107/1000 | Loss: 0.00003305
Iteration 108/1000 | Loss: 0.00003145
Iteration 109/1000 | Loss: 0.00003290
Iteration 110/1000 | Loss: 0.00003075
Iteration 111/1000 | Loss: 0.00039255
Iteration 112/1000 | Loss: 0.00007189
Iteration 113/1000 | Loss: 0.00003716
Iteration 114/1000 | Loss: 0.00002946
Iteration 115/1000 | Loss: 0.00003091
Iteration 116/1000 | Loss: 0.00002860
Iteration 117/1000 | Loss: 0.00002769
Iteration 118/1000 | Loss: 0.00002722
Iteration 119/1000 | Loss: 0.00002954
Iteration 120/1000 | Loss: 0.00002978
Iteration 121/1000 | Loss: 0.00002948
Iteration 122/1000 | Loss: 0.00002703
Iteration 123/1000 | Loss: 0.00002654
Iteration 124/1000 | Loss: 0.00002855
Iteration 125/1000 | Loss: 0.00002711
Iteration 126/1000 | Loss: 0.00002645
Iteration 127/1000 | Loss: 0.00002644
Iteration 128/1000 | Loss: 0.00002644
Iteration 129/1000 | Loss: 0.00002643
Iteration 130/1000 | Loss: 0.00002643
Iteration 131/1000 | Loss: 0.00002643
Iteration 132/1000 | Loss: 0.00002643
Iteration 133/1000 | Loss: 0.00002643
Iteration 134/1000 | Loss: 0.00002643
Iteration 135/1000 | Loss: 0.00002643
Iteration 136/1000 | Loss: 0.00002643
Iteration 137/1000 | Loss: 0.00002643
Iteration 138/1000 | Loss: 0.00002643
Iteration 139/1000 | Loss: 0.00002643
Iteration 140/1000 | Loss: 0.00002839
Iteration 141/1000 | Loss: 0.00002731
Iteration 142/1000 | Loss: 0.00002647
Iteration 143/1000 | Loss: 0.00002638
Iteration 144/1000 | Loss: 0.00002821
Iteration 145/1000 | Loss: 0.00002714
Iteration 146/1000 | Loss: 0.00002634
Iteration 147/1000 | Loss: 0.00002633
Iteration 148/1000 | Loss: 0.00002633
Iteration 149/1000 | Loss: 0.00002633
Iteration 150/1000 | Loss: 0.00002633
Iteration 151/1000 | Loss: 0.00002633
Iteration 152/1000 | Loss: 0.00002633
Iteration 153/1000 | Loss: 0.00002633
Iteration 154/1000 | Loss: 0.00002633
Iteration 155/1000 | Loss: 0.00002633
Iteration 156/1000 | Loss: 0.00002633
Iteration 157/1000 | Loss: 0.00002633
Iteration 158/1000 | Loss: 0.00002633
Iteration 159/1000 | Loss: 0.00002633
Iteration 160/1000 | Loss: 0.00002633
Iteration 161/1000 | Loss: 0.00002633
Iteration 162/1000 | Loss: 0.00002633
Iteration 163/1000 | Loss: 0.00002633
Iteration 164/1000 | Loss: 0.00002633
Iteration 165/1000 | Loss: 0.00002633
Iteration 166/1000 | Loss: 0.00002633
Iteration 167/1000 | Loss: 0.00002633
Iteration 168/1000 | Loss: 0.00002633
Iteration 169/1000 | Loss: 0.00002633
Iteration 170/1000 | Loss: 0.00002633
Iteration 171/1000 | Loss: 0.00002633
Iteration 172/1000 | Loss: 0.00002633
Iteration 173/1000 | Loss: 0.00002633
Iteration 174/1000 | Loss: 0.00002633
Iteration 175/1000 | Loss: 0.00002633
Iteration 176/1000 | Loss: 0.00002633
Iteration 177/1000 | Loss: 0.00002633
Iteration 178/1000 | Loss: 0.00002633
Iteration 179/1000 | Loss: 0.00002633
Iteration 180/1000 | Loss: 0.00002633
Iteration 181/1000 | Loss: 0.00002633
Iteration 182/1000 | Loss: 0.00002633
Iteration 183/1000 | Loss: 0.00002633
Iteration 184/1000 | Loss: 0.00002633
Iteration 185/1000 | Loss: 0.00002633
Iteration 186/1000 | Loss: 0.00002633
Iteration 187/1000 | Loss: 0.00002633
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [2.6328169042244554e-05, 2.6328169042244554e-05, 2.6328169042244554e-05, 2.6328169042244554e-05, 2.6328169042244554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6328169042244554e-05

Optimization complete. Final v2v error: 4.165402889251709 mm

Highest mean error: 9.926348686218262 mm for frame 122

Lowest mean error: 3.5580639839172363 mm for frame 97

Saving results

Total time: 222.80160665512085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836931
Iteration 2/25 | Loss: 0.00281578
Iteration 3/25 | Loss: 0.00213704
Iteration 4/25 | Loss: 0.00203188
Iteration 5/25 | Loss: 0.00191819
Iteration 6/25 | Loss: 0.00183925
Iteration 7/25 | Loss: 0.00180131
Iteration 8/25 | Loss: 0.00179660
Iteration 9/25 | Loss: 0.00179519
Iteration 10/25 | Loss: 0.00179386
Iteration 11/25 | Loss: 0.00179303
Iteration 12/25 | Loss: 0.00179290
Iteration 13/25 | Loss: 0.00179289
Iteration 14/25 | Loss: 0.00179289
Iteration 15/25 | Loss: 0.00179289
Iteration 16/25 | Loss: 0.00179289
Iteration 17/25 | Loss: 0.00179289
Iteration 18/25 | Loss: 0.00179289
Iteration 19/25 | Loss: 0.00179289
Iteration 20/25 | Loss: 0.00179289
Iteration 21/25 | Loss: 0.00179289
Iteration 22/25 | Loss: 0.00179289
Iteration 23/25 | Loss: 0.00179289
Iteration 24/25 | Loss: 0.00179289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0017928887391462922, 0.0017928887391462922, 0.0017928887391462922, 0.0017928887391462922, 0.0017928887391462922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017928887391462922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58647025
Iteration 2/25 | Loss: 0.00284263
Iteration 3/25 | Loss: 0.00284263
Iteration 4/25 | Loss: 0.00284263
Iteration 5/25 | Loss: 0.00284263
Iteration 6/25 | Loss: 0.00284263
Iteration 7/25 | Loss: 0.00284263
Iteration 8/25 | Loss: 0.00284263
Iteration 9/25 | Loss: 0.00284263
Iteration 10/25 | Loss: 0.00284263
Iteration 11/25 | Loss: 0.00284263
Iteration 12/25 | Loss: 0.00284263
Iteration 13/25 | Loss: 0.00284263
Iteration 14/25 | Loss: 0.00284263
Iteration 15/25 | Loss: 0.00284263
Iteration 16/25 | Loss: 0.00284263
Iteration 17/25 | Loss: 0.00284263
Iteration 18/25 | Loss: 0.00284263
Iteration 19/25 | Loss: 0.00284263
Iteration 20/25 | Loss: 0.00284263
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0028426276985555887, 0.0028426276985555887, 0.0028426276985555887, 0.0028426276985555887, 0.0028426276985555887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028426276985555887

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00284263
Iteration 2/1000 | Loss: 0.00015454
Iteration 3/1000 | Loss: 0.00009445
Iteration 4/1000 | Loss: 0.00007524
Iteration 5/1000 | Loss: 0.00006629
Iteration 6/1000 | Loss: 0.00006273
Iteration 7/1000 | Loss: 0.00006064
Iteration 8/1000 | Loss: 0.00005969
Iteration 9/1000 | Loss: 0.00005898
Iteration 10/1000 | Loss: 0.00005865
Iteration 11/1000 | Loss: 0.00005833
Iteration 12/1000 | Loss: 0.00005809
Iteration 13/1000 | Loss: 0.00005787
Iteration 14/1000 | Loss: 0.00005768
Iteration 15/1000 | Loss: 0.00005751
Iteration 16/1000 | Loss: 0.00005736
Iteration 17/1000 | Loss: 0.00005728
Iteration 18/1000 | Loss: 0.00005725
Iteration 19/1000 | Loss: 0.00005723
Iteration 20/1000 | Loss: 0.00005714
Iteration 21/1000 | Loss: 0.00005711
Iteration 22/1000 | Loss: 0.00005710
Iteration 23/1000 | Loss: 0.00005710
Iteration 24/1000 | Loss: 0.00005710
Iteration 25/1000 | Loss: 0.00005710
Iteration 26/1000 | Loss: 0.00005710
Iteration 27/1000 | Loss: 0.00005710
Iteration 28/1000 | Loss: 0.00005710
Iteration 29/1000 | Loss: 0.00005710
Iteration 30/1000 | Loss: 0.00005710
Iteration 31/1000 | Loss: 0.00005709
Iteration 32/1000 | Loss: 0.00005709
Iteration 33/1000 | Loss: 0.00005709
Iteration 34/1000 | Loss: 0.00005708
Iteration 35/1000 | Loss: 0.00005708
Iteration 36/1000 | Loss: 0.00005707
Iteration 37/1000 | Loss: 0.00005707
Iteration 38/1000 | Loss: 0.00005707
Iteration 39/1000 | Loss: 0.00005706
Iteration 40/1000 | Loss: 0.00005705
Iteration 41/1000 | Loss: 0.00005704
Iteration 42/1000 | Loss: 0.00005703
Iteration 43/1000 | Loss: 0.00005703
Iteration 44/1000 | Loss: 0.00005697
Iteration 45/1000 | Loss: 0.00005697
Iteration 46/1000 | Loss: 0.00005690
Iteration 47/1000 | Loss: 0.00005688
Iteration 48/1000 | Loss: 0.00005688
Iteration 49/1000 | Loss: 0.00005688
Iteration 50/1000 | Loss: 0.00005687
Iteration 51/1000 | Loss: 0.00005687
Iteration 52/1000 | Loss: 0.00005687
Iteration 53/1000 | Loss: 0.00005687
Iteration 54/1000 | Loss: 0.00005687
Iteration 55/1000 | Loss: 0.00005685
Iteration 56/1000 | Loss: 0.00005685
Iteration 57/1000 | Loss: 0.00005683
Iteration 58/1000 | Loss: 0.00005683
Iteration 59/1000 | Loss: 0.00005683
Iteration 60/1000 | Loss: 0.00005683
Iteration 61/1000 | Loss: 0.00005682
Iteration 62/1000 | Loss: 0.00005682
Iteration 63/1000 | Loss: 0.00005682
Iteration 64/1000 | Loss: 0.00005682
Iteration 65/1000 | Loss: 0.00005682
Iteration 66/1000 | Loss: 0.00005682
Iteration 67/1000 | Loss: 0.00005681
Iteration 68/1000 | Loss: 0.00005681
Iteration 69/1000 | Loss: 0.00005681
Iteration 70/1000 | Loss: 0.00005680
Iteration 71/1000 | Loss: 0.00005680
Iteration 72/1000 | Loss: 0.00005680
Iteration 73/1000 | Loss: 0.00005680
Iteration 74/1000 | Loss: 0.00005680
Iteration 75/1000 | Loss: 0.00005679
Iteration 76/1000 | Loss: 0.00005679
Iteration 77/1000 | Loss: 0.00005679
Iteration 78/1000 | Loss: 0.00005679
Iteration 79/1000 | Loss: 0.00005679
Iteration 80/1000 | Loss: 0.00005679
Iteration 81/1000 | Loss: 0.00005679
Iteration 82/1000 | Loss: 0.00005679
Iteration 83/1000 | Loss: 0.00005679
Iteration 84/1000 | Loss: 0.00005678
Iteration 85/1000 | Loss: 0.00005678
Iteration 86/1000 | Loss: 0.00005678
Iteration 87/1000 | Loss: 0.00005678
Iteration 88/1000 | Loss: 0.00005678
Iteration 89/1000 | Loss: 0.00005678
Iteration 90/1000 | Loss: 0.00005677
Iteration 91/1000 | Loss: 0.00005677
Iteration 92/1000 | Loss: 0.00005677
Iteration 93/1000 | Loss: 0.00005677
Iteration 94/1000 | Loss: 0.00005677
Iteration 95/1000 | Loss: 0.00005677
Iteration 96/1000 | Loss: 0.00005677
Iteration 97/1000 | Loss: 0.00005677
Iteration 98/1000 | Loss: 0.00005677
Iteration 99/1000 | Loss: 0.00005677
Iteration 100/1000 | Loss: 0.00005677
Iteration 101/1000 | Loss: 0.00005677
Iteration 102/1000 | Loss: 0.00005677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [5.676541695720516e-05, 5.676541695720516e-05, 5.676541695720516e-05, 5.676541695720516e-05, 5.676541695720516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.676541695720516e-05

Optimization complete. Final v2v error: 6.31390905380249 mm

Highest mean error: 6.953684329986572 mm for frame 33

Lowest mean error: 5.664814472198486 mm for frame 197

Saving results

Total time: 62.11152935028076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408331
Iteration 2/25 | Loss: 0.00176433
Iteration 3/25 | Loss: 0.00163933
Iteration 4/25 | Loss: 0.00161992
Iteration 5/25 | Loss: 0.00161300
Iteration 6/25 | Loss: 0.00161161
Iteration 7/25 | Loss: 0.00161161
Iteration 8/25 | Loss: 0.00161161
Iteration 9/25 | Loss: 0.00161161
Iteration 10/25 | Loss: 0.00161161
Iteration 11/25 | Loss: 0.00161161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0016116072656586766, 0.0016116072656586766, 0.0016116072656586766, 0.0016116072656586766, 0.0016116072656586766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016116072656586766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97573864
Iteration 2/25 | Loss: 0.00237290
Iteration 3/25 | Loss: 0.00237290
Iteration 4/25 | Loss: 0.00237290
Iteration 5/25 | Loss: 0.00237290
Iteration 6/25 | Loss: 0.00237290
Iteration 7/25 | Loss: 0.00237290
Iteration 8/25 | Loss: 0.00237290
Iteration 9/25 | Loss: 0.00237290
Iteration 10/25 | Loss: 0.00237290
Iteration 11/25 | Loss: 0.00237290
Iteration 12/25 | Loss: 0.00237290
Iteration 13/25 | Loss: 0.00237290
Iteration 14/25 | Loss: 0.00237290
Iteration 15/25 | Loss: 0.00237290
Iteration 16/25 | Loss: 0.00237290
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0023728993255645037, 0.0023728993255645037, 0.0023728993255645037, 0.0023728993255645037, 0.0023728993255645037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023728993255645037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237290
Iteration 2/1000 | Loss: 0.00006260
Iteration 3/1000 | Loss: 0.00004191
Iteration 4/1000 | Loss: 0.00003644
Iteration 5/1000 | Loss: 0.00003374
Iteration 6/1000 | Loss: 0.00003230
Iteration 7/1000 | Loss: 0.00003155
Iteration 8/1000 | Loss: 0.00003104
Iteration 9/1000 | Loss: 0.00003059
Iteration 10/1000 | Loss: 0.00003025
Iteration 11/1000 | Loss: 0.00003005
Iteration 12/1000 | Loss: 0.00002982
Iteration 13/1000 | Loss: 0.00002975
Iteration 14/1000 | Loss: 0.00002969
Iteration 15/1000 | Loss: 0.00002962
Iteration 16/1000 | Loss: 0.00002962
Iteration 17/1000 | Loss: 0.00002960
Iteration 18/1000 | Loss: 0.00002959
Iteration 19/1000 | Loss: 0.00002955
Iteration 20/1000 | Loss: 0.00002955
Iteration 21/1000 | Loss: 0.00002955
Iteration 22/1000 | Loss: 0.00002954
Iteration 23/1000 | Loss: 0.00002954
Iteration 24/1000 | Loss: 0.00002953
Iteration 25/1000 | Loss: 0.00002953
Iteration 26/1000 | Loss: 0.00002953
Iteration 27/1000 | Loss: 0.00002953
Iteration 28/1000 | Loss: 0.00002952
Iteration 29/1000 | Loss: 0.00002951
Iteration 30/1000 | Loss: 0.00002951
Iteration 31/1000 | Loss: 0.00002951
Iteration 32/1000 | Loss: 0.00002951
Iteration 33/1000 | Loss: 0.00002950
Iteration 34/1000 | Loss: 0.00002950
Iteration 35/1000 | Loss: 0.00002950
Iteration 36/1000 | Loss: 0.00002950
Iteration 37/1000 | Loss: 0.00002950
Iteration 38/1000 | Loss: 0.00002950
Iteration 39/1000 | Loss: 0.00002950
Iteration 40/1000 | Loss: 0.00002950
Iteration 41/1000 | Loss: 0.00002950
Iteration 42/1000 | Loss: 0.00002950
Iteration 43/1000 | Loss: 0.00002950
Iteration 44/1000 | Loss: 0.00002949
Iteration 45/1000 | Loss: 0.00002949
Iteration 46/1000 | Loss: 0.00002948
Iteration 47/1000 | Loss: 0.00002948
Iteration 48/1000 | Loss: 0.00002948
Iteration 49/1000 | Loss: 0.00002948
Iteration 50/1000 | Loss: 0.00002947
Iteration 51/1000 | Loss: 0.00002947
Iteration 52/1000 | Loss: 0.00002946
Iteration 53/1000 | Loss: 0.00002946
Iteration 54/1000 | Loss: 0.00002946
Iteration 55/1000 | Loss: 0.00002946
Iteration 56/1000 | Loss: 0.00002945
Iteration 57/1000 | Loss: 0.00002945
Iteration 58/1000 | Loss: 0.00002945
Iteration 59/1000 | Loss: 0.00002944
Iteration 60/1000 | Loss: 0.00002944
Iteration 61/1000 | Loss: 0.00002943
Iteration 62/1000 | Loss: 0.00002943
Iteration 63/1000 | Loss: 0.00002943
Iteration 64/1000 | Loss: 0.00002943
Iteration 65/1000 | Loss: 0.00002943
Iteration 66/1000 | Loss: 0.00002943
Iteration 67/1000 | Loss: 0.00002943
Iteration 68/1000 | Loss: 0.00002942
Iteration 69/1000 | Loss: 0.00002942
Iteration 70/1000 | Loss: 0.00002942
Iteration 71/1000 | Loss: 0.00002942
Iteration 72/1000 | Loss: 0.00002942
Iteration 73/1000 | Loss: 0.00002941
Iteration 74/1000 | Loss: 0.00002941
Iteration 75/1000 | Loss: 0.00002941
Iteration 76/1000 | Loss: 0.00002941
Iteration 77/1000 | Loss: 0.00002941
Iteration 78/1000 | Loss: 0.00002940
Iteration 79/1000 | Loss: 0.00002940
Iteration 80/1000 | Loss: 0.00002940
Iteration 81/1000 | Loss: 0.00002940
Iteration 82/1000 | Loss: 0.00002940
Iteration 83/1000 | Loss: 0.00002939
Iteration 84/1000 | Loss: 0.00002939
Iteration 85/1000 | Loss: 0.00002939
Iteration 86/1000 | Loss: 0.00002939
Iteration 87/1000 | Loss: 0.00002938
Iteration 88/1000 | Loss: 0.00002938
Iteration 89/1000 | Loss: 0.00002938
Iteration 90/1000 | Loss: 0.00002937
Iteration 91/1000 | Loss: 0.00002937
Iteration 92/1000 | Loss: 0.00002937
Iteration 93/1000 | Loss: 0.00002937
Iteration 94/1000 | Loss: 0.00002937
Iteration 95/1000 | Loss: 0.00002936
Iteration 96/1000 | Loss: 0.00002936
Iteration 97/1000 | Loss: 0.00002936
Iteration 98/1000 | Loss: 0.00002936
Iteration 99/1000 | Loss: 0.00002936
Iteration 100/1000 | Loss: 0.00002936
Iteration 101/1000 | Loss: 0.00002936
Iteration 102/1000 | Loss: 0.00002936
Iteration 103/1000 | Loss: 0.00002936
Iteration 104/1000 | Loss: 0.00002936
Iteration 105/1000 | Loss: 0.00002936
Iteration 106/1000 | Loss: 0.00002935
Iteration 107/1000 | Loss: 0.00002935
Iteration 108/1000 | Loss: 0.00002935
Iteration 109/1000 | Loss: 0.00002935
Iteration 110/1000 | Loss: 0.00002934
Iteration 111/1000 | Loss: 0.00002934
Iteration 112/1000 | Loss: 0.00002934
Iteration 113/1000 | Loss: 0.00002934
Iteration 114/1000 | Loss: 0.00002934
Iteration 115/1000 | Loss: 0.00002934
Iteration 116/1000 | Loss: 0.00002934
Iteration 117/1000 | Loss: 0.00002934
Iteration 118/1000 | Loss: 0.00002934
Iteration 119/1000 | Loss: 0.00002934
Iteration 120/1000 | Loss: 0.00002934
Iteration 121/1000 | Loss: 0.00002934
Iteration 122/1000 | Loss: 0.00002934
Iteration 123/1000 | Loss: 0.00002934
Iteration 124/1000 | Loss: 0.00002934
Iteration 125/1000 | Loss: 0.00002934
Iteration 126/1000 | Loss: 0.00002934
Iteration 127/1000 | Loss: 0.00002934
Iteration 128/1000 | Loss: 0.00002934
Iteration 129/1000 | Loss: 0.00002934
Iteration 130/1000 | Loss: 0.00002934
Iteration 131/1000 | Loss: 0.00002934
Iteration 132/1000 | Loss: 0.00002934
Iteration 133/1000 | Loss: 0.00002934
Iteration 134/1000 | Loss: 0.00002934
Iteration 135/1000 | Loss: 0.00002934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.9338600143091753e-05, 2.9338600143091753e-05, 2.9338600143091753e-05, 2.9338600143091753e-05, 2.9338600143091753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9338600143091753e-05

Optimization complete. Final v2v error: 4.755359649658203 mm

Highest mean error: 5.014173984527588 mm for frame 75

Lowest mean error: 4.484147071838379 mm for frame 227

Saving results

Total time: 40.86498308181763
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034301
Iteration 2/25 | Loss: 0.00195477
Iteration 3/25 | Loss: 0.00179166
Iteration 4/25 | Loss: 0.00176102
Iteration 5/25 | Loss: 0.00175310
Iteration 6/25 | Loss: 0.00174963
Iteration 7/25 | Loss: 0.00174898
Iteration 8/25 | Loss: 0.00174898
Iteration 9/25 | Loss: 0.00174898
Iteration 10/25 | Loss: 0.00174898
Iteration 11/25 | Loss: 0.00174898
Iteration 12/25 | Loss: 0.00174898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0017489823512732983, 0.0017489823512732983, 0.0017489823512732983, 0.0017489823512732983, 0.0017489823512732983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017489823512732983

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52768362
Iteration 2/25 | Loss: 0.00238474
Iteration 3/25 | Loss: 0.00238463
Iteration 4/25 | Loss: 0.00238463
Iteration 5/25 | Loss: 0.00238463
Iteration 6/25 | Loss: 0.00238463
Iteration 7/25 | Loss: 0.00238463
Iteration 8/25 | Loss: 0.00238463
Iteration 9/25 | Loss: 0.00238463
Iteration 10/25 | Loss: 0.00238463
Iteration 11/25 | Loss: 0.00238463
Iteration 12/25 | Loss: 0.00238463
Iteration 13/25 | Loss: 0.00238463
Iteration 14/25 | Loss: 0.00238463
Iteration 15/25 | Loss: 0.00238463
Iteration 16/25 | Loss: 0.00238463
Iteration 17/25 | Loss: 0.00238463
Iteration 18/25 | Loss: 0.00238463
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0023846302647143602, 0.0023846302647143602, 0.0023846302647143602, 0.0023846302647143602, 0.0023846302647143602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023846302647143602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00238463
Iteration 2/1000 | Loss: 0.00007560
Iteration 3/1000 | Loss: 0.00005409
Iteration 4/1000 | Loss: 0.00004814
Iteration 5/1000 | Loss: 0.00004587
Iteration 6/1000 | Loss: 0.00004406
Iteration 7/1000 | Loss: 0.00004277
Iteration 8/1000 | Loss: 0.00004135
Iteration 9/1000 | Loss: 0.00004039
Iteration 10/1000 | Loss: 0.00003946
Iteration 11/1000 | Loss: 0.00003867
Iteration 12/1000 | Loss: 0.00003802
Iteration 13/1000 | Loss: 0.00003749
Iteration 14/1000 | Loss: 0.00003709
Iteration 15/1000 | Loss: 0.00003687
Iteration 16/1000 | Loss: 0.00003657
Iteration 17/1000 | Loss: 0.00003650
Iteration 18/1000 | Loss: 0.00003650
Iteration 19/1000 | Loss: 0.00003649
Iteration 20/1000 | Loss: 0.00003649
Iteration 21/1000 | Loss: 0.00003649
Iteration 22/1000 | Loss: 0.00003648
Iteration 23/1000 | Loss: 0.00003643
Iteration 24/1000 | Loss: 0.00003643
Iteration 25/1000 | Loss: 0.00003642
Iteration 26/1000 | Loss: 0.00003641
Iteration 27/1000 | Loss: 0.00003636
Iteration 28/1000 | Loss: 0.00003636
Iteration 29/1000 | Loss: 0.00003626
Iteration 30/1000 | Loss: 0.00003619
Iteration 31/1000 | Loss: 0.00003619
Iteration 32/1000 | Loss: 0.00003619
Iteration 33/1000 | Loss: 0.00003619
Iteration 34/1000 | Loss: 0.00003619
Iteration 35/1000 | Loss: 0.00003619
Iteration 36/1000 | Loss: 0.00003619
Iteration 37/1000 | Loss: 0.00003619
Iteration 38/1000 | Loss: 0.00003618
Iteration 39/1000 | Loss: 0.00003618
Iteration 40/1000 | Loss: 0.00003618
Iteration 41/1000 | Loss: 0.00003618
Iteration 42/1000 | Loss: 0.00003618
Iteration 43/1000 | Loss: 0.00003617
Iteration 44/1000 | Loss: 0.00003616
Iteration 45/1000 | Loss: 0.00003616
Iteration 46/1000 | Loss: 0.00003615
Iteration 47/1000 | Loss: 0.00003615
Iteration 48/1000 | Loss: 0.00003614
Iteration 49/1000 | Loss: 0.00003614
Iteration 50/1000 | Loss: 0.00003613
Iteration 51/1000 | Loss: 0.00003613
Iteration 52/1000 | Loss: 0.00003610
Iteration 53/1000 | Loss: 0.00003610
Iteration 54/1000 | Loss: 0.00003610
Iteration 55/1000 | Loss: 0.00003609
Iteration 56/1000 | Loss: 0.00003609
Iteration 57/1000 | Loss: 0.00003609
Iteration 58/1000 | Loss: 0.00003608
Iteration 59/1000 | Loss: 0.00003608
Iteration 60/1000 | Loss: 0.00003608
Iteration 61/1000 | Loss: 0.00003608
Iteration 62/1000 | Loss: 0.00003607
Iteration 63/1000 | Loss: 0.00003607
Iteration 64/1000 | Loss: 0.00003607
Iteration 65/1000 | Loss: 0.00003607
Iteration 66/1000 | Loss: 0.00003607
Iteration 67/1000 | Loss: 0.00003606
Iteration 68/1000 | Loss: 0.00003606
Iteration 69/1000 | Loss: 0.00003606
Iteration 70/1000 | Loss: 0.00003606
Iteration 71/1000 | Loss: 0.00003606
Iteration 72/1000 | Loss: 0.00003606
Iteration 73/1000 | Loss: 0.00003606
Iteration 74/1000 | Loss: 0.00003606
Iteration 75/1000 | Loss: 0.00003605
Iteration 76/1000 | Loss: 0.00003605
Iteration 77/1000 | Loss: 0.00003605
Iteration 78/1000 | Loss: 0.00003605
Iteration 79/1000 | Loss: 0.00003605
Iteration 80/1000 | Loss: 0.00003605
Iteration 81/1000 | Loss: 0.00003605
Iteration 82/1000 | Loss: 0.00003604
Iteration 83/1000 | Loss: 0.00003604
Iteration 84/1000 | Loss: 0.00003604
Iteration 85/1000 | Loss: 0.00003604
Iteration 86/1000 | Loss: 0.00003604
Iteration 87/1000 | Loss: 0.00003604
Iteration 88/1000 | Loss: 0.00003604
Iteration 89/1000 | Loss: 0.00003604
Iteration 90/1000 | Loss: 0.00003604
Iteration 91/1000 | Loss: 0.00003604
Iteration 92/1000 | Loss: 0.00003603
Iteration 93/1000 | Loss: 0.00003603
Iteration 94/1000 | Loss: 0.00003603
Iteration 95/1000 | Loss: 0.00003603
Iteration 96/1000 | Loss: 0.00003603
Iteration 97/1000 | Loss: 0.00003603
Iteration 98/1000 | Loss: 0.00003603
Iteration 99/1000 | Loss: 0.00003603
Iteration 100/1000 | Loss: 0.00003603
Iteration 101/1000 | Loss: 0.00003603
Iteration 102/1000 | Loss: 0.00003603
Iteration 103/1000 | Loss: 0.00003603
Iteration 104/1000 | Loss: 0.00003603
Iteration 105/1000 | Loss: 0.00003602
Iteration 106/1000 | Loss: 0.00003602
Iteration 107/1000 | Loss: 0.00003602
Iteration 108/1000 | Loss: 0.00003602
Iteration 109/1000 | Loss: 0.00003602
Iteration 110/1000 | Loss: 0.00003602
Iteration 111/1000 | Loss: 0.00003602
Iteration 112/1000 | Loss: 0.00003602
Iteration 113/1000 | Loss: 0.00003602
Iteration 114/1000 | Loss: 0.00003602
Iteration 115/1000 | Loss: 0.00003602
Iteration 116/1000 | Loss: 0.00003602
Iteration 117/1000 | Loss: 0.00003602
Iteration 118/1000 | Loss: 0.00003601
Iteration 119/1000 | Loss: 0.00003601
Iteration 120/1000 | Loss: 0.00003601
Iteration 121/1000 | Loss: 0.00003601
Iteration 122/1000 | Loss: 0.00003601
Iteration 123/1000 | Loss: 0.00003601
Iteration 124/1000 | Loss: 0.00003601
Iteration 125/1000 | Loss: 0.00003601
Iteration 126/1000 | Loss: 0.00003601
Iteration 127/1000 | Loss: 0.00003601
Iteration 128/1000 | Loss: 0.00003601
Iteration 129/1000 | Loss: 0.00003601
Iteration 130/1000 | Loss: 0.00003601
Iteration 131/1000 | Loss: 0.00003601
Iteration 132/1000 | Loss: 0.00003600
Iteration 133/1000 | Loss: 0.00003600
Iteration 134/1000 | Loss: 0.00003600
Iteration 135/1000 | Loss: 0.00003600
Iteration 136/1000 | Loss: 0.00003599
Iteration 137/1000 | Loss: 0.00003599
Iteration 138/1000 | Loss: 0.00003599
Iteration 139/1000 | Loss: 0.00003599
Iteration 140/1000 | Loss: 0.00003599
Iteration 141/1000 | Loss: 0.00003599
Iteration 142/1000 | Loss: 0.00003599
Iteration 143/1000 | Loss: 0.00003599
Iteration 144/1000 | Loss: 0.00003599
Iteration 145/1000 | Loss: 0.00003599
Iteration 146/1000 | Loss: 0.00003599
Iteration 147/1000 | Loss: 0.00003599
Iteration 148/1000 | Loss: 0.00003599
Iteration 149/1000 | Loss: 0.00003599
Iteration 150/1000 | Loss: 0.00003599
Iteration 151/1000 | Loss: 0.00003599
Iteration 152/1000 | Loss: 0.00003599
Iteration 153/1000 | Loss: 0.00003599
Iteration 154/1000 | Loss: 0.00003599
Iteration 155/1000 | Loss: 0.00003599
Iteration 156/1000 | Loss: 0.00003599
Iteration 157/1000 | Loss: 0.00003599
Iteration 158/1000 | Loss: 0.00003599
Iteration 159/1000 | Loss: 0.00003599
Iteration 160/1000 | Loss: 0.00003599
Iteration 161/1000 | Loss: 0.00003599
Iteration 162/1000 | Loss: 0.00003599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [3.598893090384081e-05, 3.598893090384081e-05, 3.598893090384081e-05, 3.598893090384081e-05, 3.598893090384081e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.598893090384081e-05

Optimization complete. Final v2v error: 5.244650363922119 mm

Highest mean error: 5.648401737213135 mm for frame 200

Lowest mean error: 4.776716232299805 mm for frame 5

Saving results

Total time: 51.9157600402832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445802
Iteration 2/25 | Loss: 0.00166033
Iteration 3/25 | Loss: 0.00156272
Iteration 4/25 | Loss: 0.00155136
Iteration 5/25 | Loss: 0.00154815
Iteration 6/25 | Loss: 0.00154685
Iteration 7/25 | Loss: 0.00154677
Iteration 8/25 | Loss: 0.00154677
Iteration 9/25 | Loss: 0.00154677
Iteration 10/25 | Loss: 0.00154677
Iteration 11/25 | Loss: 0.00154677
Iteration 12/25 | Loss: 0.00154677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015467697521671653, 0.0015467697521671653, 0.0015467697521671653, 0.0015467697521671653, 0.0015467697521671653]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015467697521671653

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.00619483
Iteration 2/25 | Loss: 0.00215835
Iteration 3/25 | Loss: 0.00215835
Iteration 4/25 | Loss: 0.00215835
Iteration 5/25 | Loss: 0.00215835
Iteration 6/25 | Loss: 0.00215835
Iteration 7/25 | Loss: 0.00215835
Iteration 8/25 | Loss: 0.00215835
Iteration 9/25 | Loss: 0.00215835
Iteration 10/25 | Loss: 0.00215835
Iteration 11/25 | Loss: 0.00215835
Iteration 12/25 | Loss: 0.00215835
Iteration 13/25 | Loss: 0.00215835
Iteration 14/25 | Loss: 0.00215835
Iteration 15/25 | Loss: 0.00215835
Iteration 16/25 | Loss: 0.00215835
Iteration 17/25 | Loss: 0.00215835
Iteration 18/25 | Loss: 0.00215835
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002158345654606819, 0.002158345654606819, 0.002158345654606819, 0.002158345654606819, 0.002158345654606819]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002158345654606819

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215835
Iteration 2/1000 | Loss: 0.00004464
Iteration 3/1000 | Loss: 0.00003265
Iteration 4/1000 | Loss: 0.00002841
Iteration 5/1000 | Loss: 0.00002649
Iteration 6/1000 | Loss: 0.00002541
Iteration 7/1000 | Loss: 0.00002485
Iteration 8/1000 | Loss: 0.00002456
Iteration 9/1000 | Loss: 0.00002422
Iteration 10/1000 | Loss: 0.00002395
Iteration 11/1000 | Loss: 0.00002376
Iteration 12/1000 | Loss: 0.00002356
Iteration 13/1000 | Loss: 0.00002350
Iteration 14/1000 | Loss: 0.00002349
Iteration 15/1000 | Loss: 0.00002347
Iteration 16/1000 | Loss: 0.00002346
Iteration 17/1000 | Loss: 0.00002346
Iteration 18/1000 | Loss: 0.00002345
Iteration 19/1000 | Loss: 0.00002345
Iteration 20/1000 | Loss: 0.00002344
Iteration 21/1000 | Loss: 0.00002341
Iteration 22/1000 | Loss: 0.00002341
Iteration 23/1000 | Loss: 0.00002341
Iteration 24/1000 | Loss: 0.00002339
Iteration 25/1000 | Loss: 0.00002339
Iteration 26/1000 | Loss: 0.00002338
Iteration 27/1000 | Loss: 0.00002337
Iteration 28/1000 | Loss: 0.00002336
Iteration 29/1000 | Loss: 0.00002336
Iteration 30/1000 | Loss: 0.00002335
Iteration 31/1000 | Loss: 0.00002334
Iteration 32/1000 | Loss: 0.00002334
Iteration 33/1000 | Loss: 0.00002333
Iteration 34/1000 | Loss: 0.00002333
Iteration 35/1000 | Loss: 0.00002333
Iteration 36/1000 | Loss: 0.00002333
Iteration 37/1000 | Loss: 0.00002333
Iteration 38/1000 | Loss: 0.00002333
Iteration 39/1000 | Loss: 0.00002332
Iteration 40/1000 | Loss: 0.00002332
Iteration 41/1000 | Loss: 0.00002332
Iteration 42/1000 | Loss: 0.00002331
Iteration 43/1000 | Loss: 0.00002331
Iteration 44/1000 | Loss: 0.00002331
Iteration 45/1000 | Loss: 0.00002331
Iteration 46/1000 | Loss: 0.00002331
Iteration 47/1000 | Loss: 0.00002330
Iteration 48/1000 | Loss: 0.00002330
Iteration 49/1000 | Loss: 0.00002330
Iteration 50/1000 | Loss: 0.00002330
Iteration 51/1000 | Loss: 0.00002329
Iteration 52/1000 | Loss: 0.00002329
Iteration 53/1000 | Loss: 0.00002329
Iteration 54/1000 | Loss: 0.00002329
Iteration 55/1000 | Loss: 0.00002329
Iteration 56/1000 | Loss: 0.00002329
Iteration 57/1000 | Loss: 0.00002329
Iteration 58/1000 | Loss: 0.00002329
Iteration 59/1000 | Loss: 0.00002329
Iteration 60/1000 | Loss: 0.00002328
Iteration 61/1000 | Loss: 0.00002328
Iteration 62/1000 | Loss: 0.00002328
Iteration 63/1000 | Loss: 0.00002328
Iteration 64/1000 | Loss: 0.00002328
Iteration 65/1000 | Loss: 0.00002327
Iteration 66/1000 | Loss: 0.00002327
Iteration 67/1000 | Loss: 0.00002327
Iteration 68/1000 | Loss: 0.00002327
Iteration 69/1000 | Loss: 0.00002327
Iteration 70/1000 | Loss: 0.00002327
Iteration 71/1000 | Loss: 0.00002327
Iteration 72/1000 | Loss: 0.00002326
Iteration 73/1000 | Loss: 0.00002326
Iteration 74/1000 | Loss: 0.00002326
Iteration 75/1000 | Loss: 0.00002325
Iteration 76/1000 | Loss: 0.00002325
Iteration 77/1000 | Loss: 0.00002325
Iteration 78/1000 | Loss: 0.00002325
Iteration 79/1000 | Loss: 0.00002325
Iteration 80/1000 | Loss: 0.00002325
Iteration 81/1000 | Loss: 0.00002325
Iteration 82/1000 | Loss: 0.00002325
Iteration 83/1000 | Loss: 0.00002324
Iteration 84/1000 | Loss: 0.00002324
Iteration 85/1000 | Loss: 0.00002324
Iteration 86/1000 | Loss: 0.00002324
Iteration 87/1000 | Loss: 0.00002323
Iteration 88/1000 | Loss: 0.00002323
Iteration 89/1000 | Loss: 0.00002323
Iteration 90/1000 | Loss: 0.00002323
Iteration 91/1000 | Loss: 0.00002323
Iteration 92/1000 | Loss: 0.00002323
Iteration 93/1000 | Loss: 0.00002323
Iteration 94/1000 | Loss: 0.00002323
Iteration 95/1000 | Loss: 0.00002323
Iteration 96/1000 | Loss: 0.00002323
Iteration 97/1000 | Loss: 0.00002323
Iteration 98/1000 | Loss: 0.00002323
Iteration 99/1000 | Loss: 0.00002323
Iteration 100/1000 | Loss: 0.00002323
Iteration 101/1000 | Loss: 0.00002322
Iteration 102/1000 | Loss: 0.00002322
Iteration 103/1000 | Loss: 0.00002322
Iteration 104/1000 | Loss: 0.00002322
Iteration 105/1000 | Loss: 0.00002322
Iteration 106/1000 | Loss: 0.00002321
Iteration 107/1000 | Loss: 0.00002321
Iteration 108/1000 | Loss: 0.00002321
Iteration 109/1000 | Loss: 0.00002321
Iteration 110/1000 | Loss: 0.00002321
Iteration 111/1000 | Loss: 0.00002320
Iteration 112/1000 | Loss: 0.00002320
Iteration 113/1000 | Loss: 0.00002320
Iteration 114/1000 | Loss: 0.00002320
Iteration 115/1000 | Loss: 0.00002320
Iteration 116/1000 | Loss: 0.00002320
Iteration 117/1000 | Loss: 0.00002320
Iteration 118/1000 | Loss: 0.00002319
Iteration 119/1000 | Loss: 0.00002319
Iteration 120/1000 | Loss: 0.00002319
Iteration 121/1000 | Loss: 0.00002319
Iteration 122/1000 | Loss: 0.00002319
Iteration 123/1000 | Loss: 0.00002319
Iteration 124/1000 | Loss: 0.00002319
Iteration 125/1000 | Loss: 0.00002319
Iteration 126/1000 | Loss: 0.00002319
Iteration 127/1000 | Loss: 0.00002319
Iteration 128/1000 | Loss: 0.00002319
Iteration 129/1000 | Loss: 0.00002318
Iteration 130/1000 | Loss: 0.00002318
Iteration 131/1000 | Loss: 0.00002318
Iteration 132/1000 | Loss: 0.00002318
Iteration 133/1000 | Loss: 0.00002318
Iteration 134/1000 | Loss: 0.00002318
Iteration 135/1000 | Loss: 0.00002318
Iteration 136/1000 | Loss: 0.00002318
Iteration 137/1000 | Loss: 0.00002318
Iteration 138/1000 | Loss: 0.00002318
Iteration 139/1000 | Loss: 0.00002318
Iteration 140/1000 | Loss: 0.00002318
Iteration 141/1000 | Loss: 0.00002318
Iteration 142/1000 | Loss: 0.00002318
Iteration 143/1000 | Loss: 0.00002318
Iteration 144/1000 | Loss: 0.00002318
Iteration 145/1000 | Loss: 0.00002317
Iteration 146/1000 | Loss: 0.00002317
Iteration 147/1000 | Loss: 0.00002317
Iteration 148/1000 | Loss: 0.00002317
Iteration 149/1000 | Loss: 0.00002317
Iteration 150/1000 | Loss: 0.00002317
Iteration 151/1000 | Loss: 0.00002317
Iteration 152/1000 | Loss: 0.00002317
Iteration 153/1000 | Loss: 0.00002317
Iteration 154/1000 | Loss: 0.00002317
Iteration 155/1000 | Loss: 0.00002317
Iteration 156/1000 | Loss: 0.00002317
Iteration 157/1000 | Loss: 0.00002317
Iteration 158/1000 | Loss: 0.00002317
Iteration 159/1000 | Loss: 0.00002317
Iteration 160/1000 | Loss: 0.00002317
Iteration 161/1000 | Loss: 0.00002317
Iteration 162/1000 | Loss: 0.00002317
Iteration 163/1000 | Loss: 0.00002317
Iteration 164/1000 | Loss: 0.00002317
Iteration 165/1000 | Loss: 0.00002317
Iteration 166/1000 | Loss: 0.00002317
Iteration 167/1000 | Loss: 0.00002317
Iteration 168/1000 | Loss: 0.00002317
Iteration 169/1000 | Loss: 0.00002317
Iteration 170/1000 | Loss: 0.00002317
Iteration 171/1000 | Loss: 0.00002317
Iteration 172/1000 | Loss: 0.00002317
Iteration 173/1000 | Loss: 0.00002317
Iteration 174/1000 | Loss: 0.00002317
Iteration 175/1000 | Loss: 0.00002317
Iteration 176/1000 | Loss: 0.00002317
Iteration 177/1000 | Loss: 0.00002317
Iteration 178/1000 | Loss: 0.00002317
Iteration 179/1000 | Loss: 0.00002317
Iteration 180/1000 | Loss: 0.00002317
Iteration 181/1000 | Loss: 0.00002317
Iteration 182/1000 | Loss: 0.00002317
Iteration 183/1000 | Loss: 0.00002317
Iteration 184/1000 | Loss: 0.00002317
Iteration 185/1000 | Loss: 0.00002317
Iteration 186/1000 | Loss: 0.00002317
Iteration 187/1000 | Loss: 0.00002317
Iteration 188/1000 | Loss: 0.00002317
Iteration 189/1000 | Loss: 0.00002317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.3165792299550958e-05, 2.3165792299550958e-05, 2.3165792299550958e-05, 2.3165792299550958e-05, 2.3165792299550958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3165792299550958e-05

Optimization complete. Final v2v error: 4.189290523529053 mm

Highest mean error: 4.673869609832764 mm for frame 74

Lowest mean error: 3.986867666244507 mm for frame 82

Saving results

Total time: 38.94774055480957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809244
Iteration 2/25 | Loss: 0.00211351
Iteration 3/25 | Loss: 0.00169348
Iteration 4/25 | Loss: 0.00164607
Iteration 5/25 | Loss: 0.00165955
Iteration 6/25 | Loss: 0.00164966
Iteration 7/25 | Loss: 0.00163127
Iteration 8/25 | Loss: 0.00160256
Iteration 9/25 | Loss: 0.00159543
Iteration 10/25 | Loss: 0.00159460
Iteration 11/25 | Loss: 0.00159502
Iteration 12/25 | Loss: 0.00159553
Iteration 13/25 | Loss: 0.00159169
Iteration 14/25 | Loss: 0.00158841
Iteration 15/25 | Loss: 0.00158818
Iteration 16/25 | Loss: 0.00158814
Iteration 17/25 | Loss: 0.00158813
Iteration 18/25 | Loss: 0.00158813
Iteration 19/25 | Loss: 0.00158813
Iteration 20/25 | Loss: 0.00158813
Iteration 21/25 | Loss: 0.00158813
Iteration 22/25 | Loss: 0.00158813
Iteration 23/25 | Loss: 0.00158813
Iteration 24/25 | Loss: 0.00158813
Iteration 25/25 | Loss: 0.00158813

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.22429371
Iteration 2/25 | Loss: 0.00227233
Iteration 3/25 | Loss: 0.00227231
Iteration 4/25 | Loss: 0.00227231
Iteration 5/25 | Loss: 0.00227231
Iteration 6/25 | Loss: 0.00227231
Iteration 7/25 | Loss: 0.00227231
Iteration 8/25 | Loss: 0.00227231
Iteration 9/25 | Loss: 0.00227231
Iteration 10/25 | Loss: 0.00227231
Iteration 11/25 | Loss: 0.00227231
Iteration 12/25 | Loss: 0.00227231
Iteration 13/25 | Loss: 0.00227231
Iteration 14/25 | Loss: 0.00227231
Iteration 15/25 | Loss: 0.00227231
Iteration 16/25 | Loss: 0.00227231
Iteration 17/25 | Loss: 0.00227231
Iteration 18/25 | Loss: 0.00227231
Iteration 19/25 | Loss: 0.00227231
Iteration 20/25 | Loss: 0.00227231
Iteration 21/25 | Loss: 0.00227231
Iteration 22/25 | Loss: 0.00227231
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002272307174280286, 0.002272307174280286, 0.002272307174280286, 0.002272307174280286, 0.002272307174280286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002272307174280286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227231
Iteration 2/1000 | Loss: 0.00004686
Iteration 3/1000 | Loss: 0.00006047
Iteration 4/1000 | Loss: 0.00024964
Iteration 5/1000 | Loss: 0.00016398
Iteration 6/1000 | Loss: 0.00027988
Iteration 7/1000 | Loss: 0.00012323
Iteration 8/1000 | Loss: 0.00016212
Iteration 9/1000 | Loss: 0.00031025
Iteration 10/1000 | Loss: 0.00015749
Iteration 11/1000 | Loss: 0.00003514
Iteration 12/1000 | Loss: 0.00003263
Iteration 13/1000 | Loss: 0.00003157
Iteration 14/1000 | Loss: 0.00003023
Iteration 15/1000 | Loss: 0.00005019
Iteration 16/1000 | Loss: 0.00003631
Iteration 17/1000 | Loss: 0.00002697
Iteration 18/1000 | Loss: 0.00002644
Iteration 19/1000 | Loss: 0.00002619
Iteration 20/1000 | Loss: 0.00002616
Iteration 21/1000 | Loss: 0.00002615
Iteration 22/1000 | Loss: 0.00002610
Iteration 23/1000 | Loss: 0.00002609
Iteration 24/1000 | Loss: 0.00004536
Iteration 25/1000 | Loss: 0.00003153
Iteration 26/1000 | Loss: 0.00002594
Iteration 27/1000 | Loss: 0.00002592
Iteration 28/1000 | Loss: 0.00002590
Iteration 29/1000 | Loss: 0.00002589
Iteration 30/1000 | Loss: 0.00003386
Iteration 31/1000 | Loss: 0.00002587
Iteration 32/1000 | Loss: 0.00002581
Iteration 33/1000 | Loss: 0.00002581
Iteration 34/1000 | Loss: 0.00002581
Iteration 35/1000 | Loss: 0.00002580
Iteration 36/1000 | Loss: 0.00002577
Iteration 37/1000 | Loss: 0.00002572
Iteration 38/1000 | Loss: 0.00002572
Iteration 39/1000 | Loss: 0.00002571
Iteration 40/1000 | Loss: 0.00002571
Iteration 41/1000 | Loss: 0.00002571
Iteration 42/1000 | Loss: 0.00002571
Iteration 43/1000 | Loss: 0.00002571
Iteration 44/1000 | Loss: 0.00002571
Iteration 45/1000 | Loss: 0.00002571
Iteration 46/1000 | Loss: 0.00004172
Iteration 47/1000 | Loss: 0.00002669
Iteration 48/1000 | Loss: 0.00002568
Iteration 49/1000 | Loss: 0.00002568
Iteration 50/1000 | Loss: 0.00002568
Iteration 51/1000 | Loss: 0.00002568
Iteration 52/1000 | Loss: 0.00002568
Iteration 53/1000 | Loss: 0.00002568
Iteration 54/1000 | Loss: 0.00002568
Iteration 55/1000 | Loss: 0.00002568
Iteration 56/1000 | Loss: 0.00002567
Iteration 57/1000 | Loss: 0.00002567
Iteration 58/1000 | Loss: 0.00002567
Iteration 59/1000 | Loss: 0.00002567
Iteration 60/1000 | Loss: 0.00002567
Iteration 61/1000 | Loss: 0.00002567
Iteration 62/1000 | Loss: 0.00002567
Iteration 63/1000 | Loss: 0.00002567
Iteration 64/1000 | Loss: 0.00002567
Iteration 65/1000 | Loss: 0.00002567
Iteration 66/1000 | Loss: 0.00002567
Iteration 67/1000 | Loss: 0.00002567
Iteration 68/1000 | Loss: 0.00002567
Iteration 69/1000 | Loss: 0.00002567
Iteration 70/1000 | Loss: 0.00002567
Iteration 71/1000 | Loss: 0.00002567
Iteration 72/1000 | Loss: 0.00002567
Iteration 73/1000 | Loss: 0.00002566
Iteration 74/1000 | Loss: 0.00002566
Iteration 75/1000 | Loss: 0.00002566
Iteration 76/1000 | Loss: 0.00002566
Iteration 77/1000 | Loss: 0.00002566
Iteration 78/1000 | Loss: 0.00002566
Iteration 79/1000 | Loss: 0.00002566
Iteration 80/1000 | Loss: 0.00002565
Iteration 81/1000 | Loss: 0.00002565
Iteration 82/1000 | Loss: 0.00002565
Iteration 83/1000 | Loss: 0.00002565
Iteration 84/1000 | Loss: 0.00002565
Iteration 85/1000 | Loss: 0.00002565
Iteration 86/1000 | Loss: 0.00002565
Iteration 87/1000 | Loss: 0.00002564
Iteration 88/1000 | Loss: 0.00003231
Iteration 89/1000 | Loss: 0.00002829
Iteration 90/1000 | Loss: 0.00002565
Iteration 91/1000 | Loss: 0.00002565
Iteration 92/1000 | Loss: 0.00002565
Iteration 93/1000 | Loss: 0.00002565
Iteration 94/1000 | Loss: 0.00002565
Iteration 95/1000 | Loss: 0.00002565
Iteration 96/1000 | Loss: 0.00002564
Iteration 97/1000 | Loss: 0.00002564
Iteration 98/1000 | Loss: 0.00002564
Iteration 99/1000 | Loss: 0.00002564
Iteration 100/1000 | Loss: 0.00002564
Iteration 101/1000 | Loss: 0.00002564
Iteration 102/1000 | Loss: 0.00002564
Iteration 103/1000 | Loss: 0.00002564
Iteration 104/1000 | Loss: 0.00002564
Iteration 105/1000 | Loss: 0.00002564
Iteration 106/1000 | Loss: 0.00002564
Iteration 107/1000 | Loss: 0.00002563
Iteration 108/1000 | Loss: 0.00002563
Iteration 109/1000 | Loss: 0.00002563
Iteration 110/1000 | Loss: 0.00002563
Iteration 111/1000 | Loss: 0.00002563
Iteration 112/1000 | Loss: 0.00002563
Iteration 113/1000 | Loss: 0.00002562
Iteration 114/1000 | Loss: 0.00002562
Iteration 115/1000 | Loss: 0.00002562
Iteration 116/1000 | Loss: 0.00002562
Iteration 117/1000 | Loss: 0.00002562
Iteration 118/1000 | Loss: 0.00002562
Iteration 119/1000 | Loss: 0.00002562
Iteration 120/1000 | Loss: 0.00002562
Iteration 121/1000 | Loss: 0.00002562
Iteration 122/1000 | Loss: 0.00002562
Iteration 123/1000 | Loss: 0.00002562
Iteration 124/1000 | Loss: 0.00002562
Iteration 125/1000 | Loss: 0.00002561
Iteration 126/1000 | Loss: 0.00002738
Iteration 127/1000 | Loss: 0.00002738
Iteration 128/1000 | Loss: 0.00002698
Iteration 129/1000 | Loss: 0.00002560
Iteration 130/1000 | Loss: 0.00002560
Iteration 131/1000 | Loss: 0.00002560
Iteration 132/1000 | Loss: 0.00002560
Iteration 133/1000 | Loss: 0.00002560
Iteration 134/1000 | Loss: 0.00002560
Iteration 135/1000 | Loss: 0.00002560
Iteration 136/1000 | Loss: 0.00002560
Iteration 137/1000 | Loss: 0.00002560
Iteration 138/1000 | Loss: 0.00002560
Iteration 139/1000 | Loss: 0.00002560
Iteration 140/1000 | Loss: 0.00002560
Iteration 141/1000 | Loss: 0.00002560
Iteration 142/1000 | Loss: 0.00002560
Iteration 143/1000 | Loss: 0.00002560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [2.560042958066333e-05, 2.560042958066333e-05, 2.560042958066333e-05, 2.560042958066333e-05, 2.560042958066333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.560042958066333e-05

Optimization complete. Final v2v error: 4.467620372772217 mm

Highest mean error: 5.109899044036865 mm for frame 161

Lowest mean error: 4.076645851135254 mm for frame 108

Saving results

Total time: 85.57165288925171
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964682
Iteration 2/25 | Loss: 0.00232503
Iteration 3/25 | Loss: 0.00176344
Iteration 4/25 | Loss: 0.00170574
Iteration 5/25 | Loss: 0.00170037
Iteration 6/25 | Loss: 0.00169941
Iteration 7/25 | Loss: 0.00169927
Iteration 8/25 | Loss: 0.00169927
Iteration 9/25 | Loss: 0.00169927
Iteration 10/25 | Loss: 0.00169927
Iteration 11/25 | Loss: 0.00169927
Iteration 12/25 | Loss: 0.00169927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0016992740565910935, 0.0016992740565910935, 0.0016992740565910935, 0.0016992740565910935, 0.0016992740565910935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016992740565910935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41031623
Iteration 2/25 | Loss: 0.00228982
Iteration 3/25 | Loss: 0.00228981
Iteration 4/25 | Loss: 0.00228981
Iteration 5/25 | Loss: 0.00228981
Iteration 6/25 | Loss: 0.00228981
Iteration 7/25 | Loss: 0.00228981
Iteration 8/25 | Loss: 0.00228981
Iteration 9/25 | Loss: 0.00228981
Iteration 10/25 | Loss: 0.00228981
Iteration 11/25 | Loss: 0.00228981
Iteration 12/25 | Loss: 0.00228981
Iteration 13/25 | Loss: 0.00228981
Iteration 14/25 | Loss: 0.00228981
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002289813244715333, 0.002289813244715333, 0.002289813244715333, 0.002289813244715333, 0.002289813244715333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002289813244715333

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00228981
Iteration 2/1000 | Loss: 0.00006814
Iteration 3/1000 | Loss: 0.00004844
Iteration 4/1000 | Loss: 0.00004270
Iteration 5/1000 | Loss: 0.00004023
Iteration 6/1000 | Loss: 0.00003849
Iteration 7/1000 | Loss: 0.00003771
Iteration 8/1000 | Loss: 0.00003713
Iteration 9/1000 | Loss: 0.00003666
Iteration 10/1000 | Loss: 0.00003620
Iteration 11/1000 | Loss: 0.00003590
Iteration 12/1000 | Loss: 0.00003563
Iteration 13/1000 | Loss: 0.00003556
Iteration 14/1000 | Loss: 0.00003552
Iteration 15/1000 | Loss: 0.00003552
Iteration 16/1000 | Loss: 0.00003552
Iteration 17/1000 | Loss: 0.00003552
Iteration 18/1000 | Loss: 0.00003552
Iteration 19/1000 | Loss: 0.00003552
Iteration 20/1000 | Loss: 0.00003551
Iteration 21/1000 | Loss: 0.00003551
Iteration 22/1000 | Loss: 0.00003551
Iteration 23/1000 | Loss: 0.00003551
Iteration 24/1000 | Loss: 0.00003551
Iteration 25/1000 | Loss: 0.00003551
Iteration 26/1000 | Loss: 0.00003551
Iteration 27/1000 | Loss: 0.00003550
Iteration 28/1000 | Loss: 0.00003550
Iteration 29/1000 | Loss: 0.00003548
Iteration 30/1000 | Loss: 0.00003548
Iteration 31/1000 | Loss: 0.00003548
Iteration 32/1000 | Loss: 0.00003544
Iteration 33/1000 | Loss: 0.00003540
Iteration 34/1000 | Loss: 0.00003539
Iteration 35/1000 | Loss: 0.00003538
Iteration 36/1000 | Loss: 0.00003538
Iteration 37/1000 | Loss: 0.00003538
Iteration 38/1000 | Loss: 0.00003538
Iteration 39/1000 | Loss: 0.00003538
Iteration 40/1000 | Loss: 0.00003538
Iteration 41/1000 | Loss: 0.00003538
Iteration 42/1000 | Loss: 0.00003538
Iteration 43/1000 | Loss: 0.00003538
Iteration 44/1000 | Loss: 0.00003538
Iteration 45/1000 | Loss: 0.00003538
Iteration 46/1000 | Loss: 0.00003538
Iteration 47/1000 | Loss: 0.00003537
Iteration 48/1000 | Loss: 0.00003537
Iteration 49/1000 | Loss: 0.00003537
Iteration 50/1000 | Loss: 0.00003537
Iteration 51/1000 | Loss: 0.00003537
Iteration 52/1000 | Loss: 0.00003537
Iteration 53/1000 | Loss: 0.00003537
Iteration 54/1000 | Loss: 0.00003536
Iteration 55/1000 | Loss: 0.00003536
Iteration 56/1000 | Loss: 0.00003536
Iteration 57/1000 | Loss: 0.00003536
Iteration 58/1000 | Loss: 0.00003536
Iteration 59/1000 | Loss: 0.00003536
Iteration 60/1000 | Loss: 0.00003535
Iteration 61/1000 | Loss: 0.00003535
Iteration 62/1000 | Loss: 0.00003535
Iteration 63/1000 | Loss: 0.00003535
Iteration 64/1000 | Loss: 0.00003535
Iteration 65/1000 | Loss: 0.00003535
Iteration 66/1000 | Loss: 0.00003535
Iteration 67/1000 | Loss: 0.00003535
Iteration 68/1000 | Loss: 0.00003535
Iteration 69/1000 | Loss: 0.00003534
Iteration 70/1000 | Loss: 0.00003534
Iteration 71/1000 | Loss: 0.00003534
Iteration 72/1000 | Loss: 0.00003534
Iteration 73/1000 | Loss: 0.00003534
Iteration 74/1000 | Loss: 0.00003534
Iteration 75/1000 | Loss: 0.00003534
Iteration 76/1000 | Loss: 0.00003534
Iteration 77/1000 | Loss: 0.00003534
Iteration 78/1000 | Loss: 0.00003533
Iteration 79/1000 | Loss: 0.00003533
Iteration 80/1000 | Loss: 0.00003533
Iteration 81/1000 | Loss: 0.00003533
Iteration 82/1000 | Loss: 0.00003533
Iteration 83/1000 | Loss: 0.00003533
Iteration 84/1000 | Loss: 0.00003533
Iteration 85/1000 | Loss: 0.00003533
Iteration 86/1000 | Loss: 0.00003533
Iteration 87/1000 | Loss: 0.00003533
Iteration 88/1000 | Loss: 0.00003533
Iteration 89/1000 | Loss: 0.00003533
Iteration 90/1000 | Loss: 0.00003533
Iteration 91/1000 | Loss: 0.00003532
Iteration 92/1000 | Loss: 0.00003532
Iteration 93/1000 | Loss: 0.00003532
Iteration 94/1000 | Loss: 0.00003532
Iteration 95/1000 | Loss: 0.00003532
Iteration 96/1000 | Loss: 0.00003531
Iteration 97/1000 | Loss: 0.00003531
Iteration 98/1000 | Loss: 0.00003531
Iteration 99/1000 | Loss: 0.00003531
Iteration 100/1000 | Loss: 0.00003531
Iteration 101/1000 | Loss: 0.00003530
Iteration 102/1000 | Loss: 0.00003530
Iteration 103/1000 | Loss: 0.00003530
Iteration 104/1000 | Loss: 0.00003530
Iteration 105/1000 | Loss: 0.00003530
Iteration 106/1000 | Loss: 0.00003530
Iteration 107/1000 | Loss: 0.00003530
Iteration 108/1000 | Loss: 0.00003530
Iteration 109/1000 | Loss: 0.00003530
Iteration 110/1000 | Loss: 0.00003529
Iteration 111/1000 | Loss: 0.00003529
Iteration 112/1000 | Loss: 0.00003529
Iteration 113/1000 | Loss: 0.00003529
Iteration 114/1000 | Loss: 0.00003529
Iteration 115/1000 | Loss: 0.00003529
Iteration 116/1000 | Loss: 0.00003529
Iteration 117/1000 | Loss: 0.00003529
Iteration 118/1000 | Loss: 0.00003528
Iteration 119/1000 | Loss: 0.00003528
Iteration 120/1000 | Loss: 0.00003528
Iteration 121/1000 | Loss: 0.00003528
Iteration 122/1000 | Loss: 0.00003528
Iteration 123/1000 | Loss: 0.00003528
Iteration 124/1000 | Loss: 0.00003528
Iteration 125/1000 | Loss: 0.00003527
Iteration 126/1000 | Loss: 0.00003527
Iteration 127/1000 | Loss: 0.00003527
Iteration 128/1000 | Loss: 0.00003527
Iteration 129/1000 | Loss: 0.00003527
Iteration 130/1000 | Loss: 0.00003527
Iteration 131/1000 | Loss: 0.00003527
Iteration 132/1000 | Loss: 0.00003527
Iteration 133/1000 | Loss: 0.00003527
Iteration 134/1000 | Loss: 0.00003527
Iteration 135/1000 | Loss: 0.00003527
Iteration 136/1000 | Loss: 0.00003527
Iteration 137/1000 | Loss: 0.00003527
Iteration 138/1000 | Loss: 0.00003527
Iteration 139/1000 | Loss: 0.00003526
Iteration 140/1000 | Loss: 0.00003526
Iteration 141/1000 | Loss: 0.00003526
Iteration 142/1000 | Loss: 0.00003526
Iteration 143/1000 | Loss: 0.00003526
Iteration 144/1000 | Loss: 0.00003526
Iteration 145/1000 | Loss: 0.00003526
Iteration 146/1000 | Loss: 0.00003526
Iteration 147/1000 | Loss: 0.00003526
Iteration 148/1000 | Loss: 0.00003526
Iteration 149/1000 | Loss: 0.00003526
Iteration 150/1000 | Loss: 0.00003526
Iteration 151/1000 | Loss: 0.00003526
Iteration 152/1000 | Loss: 0.00003526
Iteration 153/1000 | Loss: 0.00003525
Iteration 154/1000 | Loss: 0.00003525
Iteration 155/1000 | Loss: 0.00003525
Iteration 156/1000 | Loss: 0.00003525
Iteration 157/1000 | Loss: 0.00003525
Iteration 158/1000 | Loss: 0.00003525
Iteration 159/1000 | Loss: 0.00003525
Iteration 160/1000 | Loss: 0.00003525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [3.525430292938836e-05, 3.525430292938836e-05, 3.525430292938836e-05, 3.525430292938836e-05, 3.525430292938836e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.525430292938836e-05

Optimization complete. Final v2v error: 5.164454936981201 mm

Highest mean error: 5.4528913497924805 mm for frame 13

Lowest mean error: 4.903777599334717 mm for frame 220

Saving results

Total time: 42.09771132469177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00961546
Iteration 2/25 | Loss: 0.00184442
Iteration 3/25 | Loss: 0.00170636
Iteration 4/25 | Loss: 0.00168288
Iteration 5/25 | Loss: 0.00167481
Iteration 6/25 | Loss: 0.00167314
Iteration 7/25 | Loss: 0.00167314
Iteration 8/25 | Loss: 0.00167314
Iteration 9/25 | Loss: 0.00167314
Iteration 10/25 | Loss: 0.00167314
Iteration 11/25 | Loss: 0.00167314
Iteration 12/25 | Loss: 0.00167314
Iteration 13/25 | Loss: 0.00167314
Iteration 14/25 | Loss: 0.00167314
Iteration 15/25 | Loss: 0.00167314
Iteration 16/25 | Loss: 0.00167314
Iteration 17/25 | Loss: 0.00167314
Iteration 18/25 | Loss: 0.00167314
Iteration 19/25 | Loss: 0.00167314
Iteration 20/25 | Loss: 0.00167314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001673140679486096, 0.001673140679486096, 0.001673140679486096, 0.001673140679486096, 0.001673140679486096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001673140679486096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56000221
Iteration 2/25 | Loss: 0.00242426
Iteration 3/25 | Loss: 0.00242417
Iteration 4/25 | Loss: 0.00242417
Iteration 5/25 | Loss: 0.00242417
Iteration 6/25 | Loss: 0.00242417
Iteration 7/25 | Loss: 0.00242417
Iteration 8/25 | Loss: 0.00242417
Iteration 9/25 | Loss: 0.00242417
Iteration 10/25 | Loss: 0.00242417
Iteration 11/25 | Loss: 0.00242417
Iteration 12/25 | Loss: 0.00242417
Iteration 13/25 | Loss: 0.00242417
Iteration 14/25 | Loss: 0.00242417
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0024241702631115913, 0.0024241702631115913, 0.0024241702631115913, 0.0024241702631115913, 0.0024241702631115913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024241702631115913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00242417
Iteration 2/1000 | Loss: 0.00005754
Iteration 3/1000 | Loss: 0.00004456
Iteration 4/1000 | Loss: 0.00003856
Iteration 5/1000 | Loss: 0.00003488
Iteration 6/1000 | Loss: 0.00003312
Iteration 7/1000 | Loss: 0.00003189
Iteration 8/1000 | Loss: 0.00003120
Iteration 9/1000 | Loss: 0.00003062
Iteration 10/1000 | Loss: 0.00003014
Iteration 11/1000 | Loss: 0.00002979
Iteration 12/1000 | Loss: 0.00002954
Iteration 13/1000 | Loss: 0.00002934
Iteration 14/1000 | Loss: 0.00002920
Iteration 15/1000 | Loss: 0.00002917
Iteration 16/1000 | Loss: 0.00002916
Iteration 17/1000 | Loss: 0.00002910
Iteration 18/1000 | Loss: 0.00002900
Iteration 19/1000 | Loss: 0.00002896
Iteration 20/1000 | Loss: 0.00002896
Iteration 21/1000 | Loss: 0.00002895
Iteration 22/1000 | Loss: 0.00002895
Iteration 23/1000 | Loss: 0.00002893
Iteration 24/1000 | Loss: 0.00002893
Iteration 25/1000 | Loss: 0.00002893
Iteration 26/1000 | Loss: 0.00002893
Iteration 27/1000 | Loss: 0.00002893
Iteration 28/1000 | Loss: 0.00002892
Iteration 29/1000 | Loss: 0.00002892
Iteration 30/1000 | Loss: 0.00002892
Iteration 31/1000 | Loss: 0.00002892
Iteration 32/1000 | Loss: 0.00002892
Iteration 33/1000 | Loss: 0.00002892
Iteration 34/1000 | Loss: 0.00002892
Iteration 35/1000 | Loss: 0.00002892
Iteration 36/1000 | Loss: 0.00002892
Iteration 37/1000 | Loss: 0.00002892
Iteration 38/1000 | Loss: 0.00002892
Iteration 39/1000 | Loss: 0.00002892
Iteration 40/1000 | Loss: 0.00002892
Iteration 41/1000 | Loss: 0.00002892
Iteration 42/1000 | Loss: 0.00002892
Iteration 43/1000 | Loss: 0.00002892
Iteration 44/1000 | Loss: 0.00002892
Iteration 45/1000 | Loss: 0.00002892
Iteration 46/1000 | Loss: 0.00002892
Iteration 47/1000 | Loss: 0.00002892
Iteration 48/1000 | Loss: 0.00002892
Iteration 49/1000 | Loss: 0.00002892
Iteration 50/1000 | Loss: 0.00002892
Iteration 51/1000 | Loss: 0.00002892
Iteration 52/1000 | Loss: 0.00002892
Iteration 53/1000 | Loss: 0.00002892
Iteration 54/1000 | Loss: 0.00002892
Iteration 55/1000 | Loss: 0.00002892
Iteration 56/1000 | Loss: 0.00002892
Iteration 57/1000 | Loss: 0.00002892
Iteration 58/1000 | Loss: 0.00002892
Iteration 59/1000 | Loss: 0.00002892
Iteration 60/1000 | Loss: 0.00002892
Iteration 61/1000 | Loss: 0.00002892
Iteration 62/1000 | Loss: 0.00002892
Iteration 63/1000 | Loss: 0.00002892
Iteration 64/1000 | Loss: 0.00002892
Iteration 65/1000 | Loss: 0.00002892
Iteration 66/1000 | Loss: 0.00002892
Iteration 67/1000 | Loss: 0.00002892
Iteration 68/1000 | Loss: 0.00002892
Iteration 69/1000 | Loss: 0.00002892
Iteration 70/1000 | Loss: 0.00002892
Iteration 71/1000 | Loss: 0.00002892
Iteration 72/1000 | Loss: 0.00002892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [2.891739313781727e-05, 2.891739313781727e-05, 2.891739313781727e-05, 2.891739313781727e-05, 2.891739313781727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.891739313781727e-05

Optimization complete. Final v2v error: 4.729708194732666 mm

Highest mean error: 5.2122802734375 mm for frame 222

Lowest mean error: 4.216282367706299 mm for frame 198

Saving results

Total time: 37.17640781402588
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01118739
Iteration 2/25 | Loss: 0.00224227
Iteration 3/25 | Loss: 0.00200530
Iteration 4/25 | Loss: 0.00195946
Iteration 5/25 | Loss: 0.00193224
Iteration 6/25 | Loss: 0.00191762
Iteration 7/25 | Loss: 0.00189698
Iteration 8/25 | Loss: 0.00188938
Iteration 9/25 | Loss: 0.00188861
Iteration 10/25 | Loss: 0.00188427
Iteration 11/25 | Loss: 0.00188320
Iteration 12/25 | Loss: 0.00188566
Iteration 13/25 | Loss: 0.00188556
Iteration 14/25 | Loss: 0.00188477
Iteration 15/25 | Loss: 0.00188534
Iteration 16/25 | Loss: 0.00188687
Iteration 17/25 | Loss: 0.00188491
Iteration 18/25 | Loss: 0.00188273
Iteration 19/25 | Loss: 0.00187924
Iteration 20/25 | Loss: 0.00188083
Iteration 21/25 | Loss: 0.00188212
Iteration 22/25 | Loss: 0.00188105
Iteration 23/25 | Loss: 0.00188111
Iteration 24/25 | Loss: 0.00188111
Iteration 25/25 | Loss: 0.00188042

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50954521
Iteration 2/25 | Loss: 0.00237787
Iteration 3/25 | Loss: 0.00237763
Iteration 4/25 | Loss: 0.00237763
Iteration 5/25 | Loss: 0.00237763
Iteration 6/25 | Loss: 0.00237763
Iteration 7/25 | Loss: 0.00237763
Iteration 8/25 | Loss: 0.00237762
Iteration 9/25 | Loss: 0.00237762
Iteration 10/25 | Loss: 0.00237762
Iteration 11/25 | Loss: 0.00237762
Iteration 12/25 | Loss: 0.00237762
Iteration 13/25 | Loss: 0.00237762
Iteration 14/25 | Loss: 0.00237762
Iteration 15/25 | Loss: 0.00237762
Iteration 16/25 | Loss: 0.00237762
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0023776243906468153, 0.0023776243906468153, 0.0023776243906468153, 0.0023776243906468153, 0.0023776243906468153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023776243906468153

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237762
Iteration 2/1000 | Loss: 0.00014464
Iteration 3/1000 | Loss: 0.00022979
Iteration 4/1000 | Loss: 0.00029679
Iteration 5/1000 | Loss: 0.00020879
Iteration 6/1000 | Loss: 0.00031299
Iteration 7/1000 | Loss: 0.00007940
Iteration 8/1000 | Loss: 0.00007005
Iteration 9/1000 | Loss: 0.00006701
Iteration 10/1000 | Loss: 0.00006496
Iteration 11/1000 | Loss: 0.00006343
Iteration 12/1000 | Loss: 0.00006217
Iteration 13/1000 | Loss: 0.00006121
Iteration 14/1000 | Loss: 0.00006039
Iteration 15/1000 | Loss: 0.00005988
Iteration 16/1000 | Loss: 0.00005937
Iteration 17/1000 | Loss: 0.00005898
Iteration 18/1000 | Loss: 0.00005863
Iteration 19/1000 | Loss: 0.00005840
Iteration 20/1000 | Loss: 0.00005822
Iteration 21/1000 | Loss: 0.00005805
Iteration 22/1000 | Loss: 0.00005805
Iteration 23/1000 | Loss: 0.00005801
Iteration 24/1000 | Loss: 0.00005790
Iteration 25/1000 | Loss: 0.00005786
Iteration 26/1000 | Loss: 0.00005785
Iteration 27/1000 | Loss: 0.00005785
Iteration 28/1000 | Loss: 0.00005784
Iteration 29/1000 | Loss: 0.00005784
Iteration 30/1000 | Loss: 0.00005783
Iteration 31/1000 | Loss: 0.00005783
Iteration 32/1000 | Loss: 0.00005782
Iteration 33/1000 | Loss: 0.00005782
Iteration 34/1000 | Loss: 0.00005782
Iteration 35/1000 | Loss: 0.00005781
Iteration 36/1000 | Loss: 0.00005781
Iteration 37/1000 | Loss: 0.00005781
Iteration 38/1000 | Loss: 0.00005781
Iteration 39/1000 | Loss: 0.00005781
Iteration 40/1000 | Loss: 0.00005781
Iteration 41/1000 | Loss: 0.00005780
Iteration 42/1000 | Loss: 0.00005780
Iteration 43/1000 | Loss: 0.00005780
Iteration 44/1000 | Loss: 0.00005780
Iteration 45/1000 | Loss: 0.00005779
Iteration 46/1000 | Loss: 0.00005778
Iteration 47/1000 | Loss: 0.00005778
Iteration 48/1000 | Loss: 0.00005778
Iteration 49/1000 | Loss: 0.00005777
Iteration 50/1000 | Loss: 0.00005777
Iteration 51/1000 | Loss: 0.00005776
Iteration 52/1000 | Loss: 0.00005776
Iteration 53/1000 | Loss: 0.00005775
Iteration 54/1000 | Loss: 0.00005775
Iteration 55/1000 | Loss: 0.00005775
Iteration 56/1000 | Loss: 0.00005775
Iteration 57/1000 | Loss: 0.00005775
Iteration 58/1000 | Loss: 0.00005774
Iteration 59/1000 | Loss: 0.00005774
Iteration 60/1000 | Loss: 0.00005774
Iteration 61/1000 | Loss: 0.00005773
Iteration 62/1000 | Loss: 0.00005773
Iteration 63/1000 | Loss: 0.00005773
Iteration 64/1000 | Loss: 0.00005772
Iteration 65/1000 | Loss: 0.00005772
Iteration 66/1000 | Loss: 0.00005772
Iteration 67/1000 | Loss: 0.00005771
Iteration 68/1000 | Loss: 0.00005771
Iteration 69/1000 | Loss: 0.00005770
Iteration 70/1000 | Loss: 0.00005770
Iteration 71/1000 | Loss: 0.00005770
Iteration 72/1000 | Loss: 0.00005770
Iteration 73/1000 | Loss: 0.00005770
Iteration 74/1000 | Loss: 0.00005770
Iteration 75/1000 | Loss: 0.00005770
Iteration 76/1000 | Loss: 0.00005769
Iteration 77/1000 | Loss: 0.00005769
Iteration 78/1000 | Loss: 0.00005769
Iteration 79/1000 | Loss: 0.00005769
Iteration 80/1000 | Loss: 0.00005769
Iteration 81/1000 | Loss: 0.00005769
Iteration 82/1000 | Loss: 0.00005769
Iteration 83/1000 | Loss: 0.00005769
Iteration 84/1000 | Loss: 0.00005769
Iteration 85/1000 | Loss: 0.00005769
Iteration 86/1000 | Loss: 0.00005768
Iteration 87/1000 | Loss: 0.00005768
Iteration 88/1000 | Loss: 0.00005768
Iteration 89/1000 | Loss: 0.00005767
Iteration 90/1000 | Loss: 0.00005767
Iteration 91/1000 | Loss: 0.00005767
Iteration 92/1000 | Loss: 0.00005766
Iteration 93/1000 | Loss: 0.00005766
Iteration 94/1000 | Loss: 0.00005766
Iteration 95/1000 | Loss: 0.00005766
Iteration 96/1000 | Loss: 0.00005765
Iteration 97/1000 | Loss: 0.00005765
Iteration 98/1000 | Loss: 0.00005765
Iteration 99/1000 | Loss: 0.00005765
Iteration 100/1000 | Loss: 0.00005765
Iteration 101/1000 | Loss: 0.00005764
Iteration 102/1000 | Loss: 0.00005764
Iteration 103/1000 | Loss: 0.00005764
Iteration 104/1000 | Loss: 0.00005764
Iteration 105/1000 | Loss: 0.00005764
Iteration 106/1000 | Loss: 0.00005764
Iteration 107/1000 | Loss: 0.00005764
Iteration 108/1000 | Loss: 0.00005763
Iteration 109/1000 | Loss: 0.00005763
Iteration 110/1000 | Loss: 0.00005763
Iteration 111/1000 | Loss: 0.00005763
Iteration 112/1000 | Loss: 0.00005763
Iteration 113/1000 | Loss: 0.00005763
Iteration 114/1000 | Loss: 0.00005763
Iteration 115/1000 | Loss: 0.00005763
Iteration 116/1000 | Loss: 0.00005763
Iteration 117/1000 | Loss: 0.00005763
Iteration 118/1000 | Loss: 0.00005763
Iteration 119/1000 | Loss: 0.00005763
Iteration 120/1000 | Loss: 0.00005763
Iteration 121/1000 | Loss: 0.00005763
Iteration 122/1000 | Loss: 0.00005763
Iteration 123/1000 | Loss: 0.00005763
Iteration 124/1000 | Loss: 0.00005763
Iteration 125/1000 | Loss: 0.00005763
Iteration 126/1000 | Loss: 0.00005763
Iteration 127/1000 | Loss: 0.00005763
Iteration 128/1000 | Loss: 0.00005763
Iteration 129/1000 | Loss: 0.00005763
Iteration 130/1000 | Loss: 0.00005763
Iteration 131/1000 | Loss: 0.00005763
Iteration 132/1000 | Loss: 0.00005763
Iteration 133/1000 | Loss: 0.00005763
Iteration 134/1000 | Loss: 0.00005763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [5.7628596550785005e-05, 5.7628596550785005e-05, 5.7628596550785005e-05, 5.7628596550785005e-05, 5.7628596550785005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.7628596550785005e-05

Optimization complete. Final v2v error: 6.199273109436035 mm

Highest mean error: 7.505272388458252 mm for frame 178

Lowest mean error: 4.609315395355225 mm for frame 35

Saving results

Total time: 95.69758749008179
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_44_us_0592/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_44_us_0592/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954519
Iteration 2/25 | Loss: 0.00169732
Iteration 3/25 | Loss: 0.00158885
Iteration 4/25 | Loss: 0.00157408
Iteration 5/25 | Loss: 0.00157317
Iteration 6/25 | Loss: 0.00157317
Iteration 7/25 | Loss: 0.00157317
Iteration 8/25 | Loss: 0.00157317
Iteration 9/25 | Loss: 0.00157317
Iteration 10/25 | Loss: 0.00157317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0015731662278994918, 0.0015731662278994918, 0.0015731662278994918, 0.0015731662278994918, 0.0015731662278994918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015731662278994918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57596457
Iteration 2/25 | Loss: 0.00225858
Iteration 3/25 | Loss: 0.00225858
Iteration 4/25 | Loss: 0.00225858
Iteration 5/25 | Loss: 0.00225858
Iteration 6/25 | Loss: 0.00225858
Iteration 7/25 | Loss: 0.00225858
Iteration 8/25 | Loss: 0.00225858
Iteration 9/25 | Loss: 0.00225858
Iteration 10/25 | Loss: 0.00225858
Iteration 11/25 | Loss: 0.00225858
Iteration 12/25 | Loss: 0.00225858
Iteration 13/25 | Loss: 0.00225858
Iteration 14/25 | Loss: 0.00225858
Iteration 15/25 | Loss: 0.00225858
Iteration 16/25 | Loss: 0.00225858
Iteration 17/25 | Loss: 0.00225858
Iteration 18/25 | Loss: 0.00225858
Iteration 19/25 | Loss: 0.00225858
Iteration 20/25 | Loss: 0.00225858
Iteration 21/25 | Loss: 0.00225858
Iteration 22/25 | Loss: 0.00225858
Iteration 23/25 | Loss: 0.00225858
Iteration 24/25 | Loss: 0.00225858
Iteration 25/25 | Loss: 0.00225858

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225858
Iteration 2/1000 | Loss: 0.00003897
Iteration 3/1000 | Loss: 0.00003201
Iteration 4/1000 | Loss: 0.00002905
Iteration 5/1000 | Loss: 0.00002778
Iteration 6/1000 | Loss: 0.00002687
Iteration 7/1000 | Loss: 0.00002647
Iteration 8/1000 | Loss: 0.00002606
Iteration 9/1000 | Loss: 0.00002582
Iteration 10/1000 | Loss: 0.00002562
Iteration 11/1000 | Loss: 0.00002556
Iteration 12/1000 | Loss: 0.00002556
Iteration 13/1000 | Loss: 0.00002554
Iteration 14/1000 | Loss: 0.00002554
Iteration 15/1000 | Loss: 0.00002553
Iteration 16/1000 | Loss: 0.00002549
Iteration 17/1000 | Loss: 0.00002549
Iteration 18/1000 | Loss: 0.00002549
Iteration 19/1000 | Loss: 0.00002549
Iteration 20/1000 | Loss: 0.00002549
Iteration 21/1000 | Loss: 0.00002548
Iteration 22/1000 | Loss: 0.00002547
Iteration 23/1000 | Loss: 0.00002543
Iteration 24/1000 | Loss: 0.00002542
Iteration 25/1000 | Loss: 0.00002534
Iteration 26/1000 | Loss: 0.00002534
Iteration 27/1000 | Loss: 0.00002532
Iteration 28/1000 | Loss: 0.00002532
Iteration 29/1000 | Loss: 0.00002531
Iteration 30/1000 | Loss: 0.00002531
Iteration 31/1000 | Loss: 0.00002530
Iteration 32/1000 | Loss: 0.00002527
Iteration 33/1000 | Loss: 0.00002526
Iteration 34/1000 | Loss: 0.00002525
Iteration 35/1000 | Loss: 0.00002525
Iteration 36/1000 | Loss: 0.00002524
Iteration 37/1000 | Loss: 0.00002524
Iteration 38/1000 | Loss: 0.00002523
Iteration 39/1000 | Loss: 0.00002523
Iteration 40/1000 | Loss: 0.00002521
Iteration 41/1000 | Loss: 0.00002521
Iteration 42/1000 | Loss: 0.00002521
Iteration 43/1000 | Loss: 0.00002521
Iteration 44/1000 | Loss: 0.00002521
Iteration 45/1000 | Loss: 0.00002521
Iteration 46/1000 | Loss: 0.00002521
Iteration 47/1000 | Loss: 0.00002520
Iteration 48/1000 | Loss: 0.00002520
Iteration 49/1000 | Loss: 0.00002518
Iteration 50/1000 | Loss: 0.00002518
Iteration 51/1000 | Loss: 0.00002517
Iteration 52/1000 | Loss: 0.00002517
Iteration 53/1000 | Loss: 0.00002517
Iteration 54/1000 | Loss: 0.00002516
Iteration 55/1000 | Loss: 0.00002516
Iteration 56/1000 | Loss: 0.00002516
Iteration 57/1000 | Loss: 0.00002515
Iteration 58/1000 | Loss: 0.00002515
Iteration 59/1000 | Loss: 0.00002515
Iteration 60/1000 | Loss: 0.00002515
Iteration 61/1000 | Loss: 0.00002514
Iteration 62/1000 | Loss: 0.00002514
Iteration 63/1000 | Loss: 0.00002514
Iteration 64/1000 | Loss: 0.00002514
Iteration 65/1000 | Loss: 0.00002513
Iteration 66/1000 | Loss: 0.00002513
Iteration 67/1000 | Loss: 0.00002513
Iteration 68/1000 | Loss: 0.00002513
Iteration 69/1000 | Loss: 0.00002513
Iteration 70/1000 | Loss: 0.00002513
Iteration 71/1000 | Loss: 0.00002512
Iteration 72/1000 | Loss: 0.00002512
Iteration 73/1000 | Loss: 0.00002512
Iteration 74/1000 | Loss: 0.00002512
Iteration 75/1000 | Loss: 0.00002512
Iteration 76/1000 | Loss: 0.00002512
Iteration 77/1000 | Loss: 0.00002511
Iteration 78/1000 | Loss: 0.00002511
Iteration 79/1000 | Loss: 0.00002510
Iteration 80/1000 | Loss: 0.00002510
Iteration 81/1000 | Loss: 0.00002510
Iteration 82/1000 | Loss: 0.00002510
Iteration 83/1000 | Loss: 0.00002510
Iteration 84/1000 | Loss: 0.00002510
Iteration 85/1000 | Loss: 0.00002510
Iteration 86/1000 | Loss: 0.00002510
Iteration 87/1000 | Loss: 0.00002510
Iteration 88/1000 | Loss: 0.00002510
Iteration 89/1000 | Loss: 0.00002510
Iteration 90/1000 | Loss: 0.00002509
Iteration 91/1000 | Loss: 0.00002509
Iteration 92/1000 | Loss: 0.00002509
Iteration 93/1000 | Loss: 0.00002509
Iteration 94/1000 | Loss: 0.00002509
Iteration 95/1000 | Loss: 0.00002509
Iteration 96/1000 | Loss: 0.00002509
Iteration 97/1000 | Loss: 0.00002509
Iteration 98/1000 | Loss: 0.00002509
Iteration 99/1000 | Loss: 0.00002509
Iteration 100/1000 | Loss: 0.00002509
Iteration 101/1000 | Loss: 0.00002509
Iteration 102/1000 | Loss: 0.00002509
Iteration 103/1000 | Loss: 0.00002509
Iteration 104/1000 | Loss: 0.00002509
Iteration 105/1000 | Loss: 0.00002509
Iteration 106/1000 | Loss: 0.00002509
Iteration 107/1000 | Loss: 0.00002509
Iteration 108/1000 | Loss: 0.00002509
Iteration 109/1000 | Loss: 0.00002509
Iteration 110/1000 | Loss: 0.00002509
Iteration 111/1000 | Loss: 0.00002509
Iteration 112/1000 | Loss: 0.00002509
Iteration 113/1000 | Loss: 0.00002509
Iteration 114/1000 | Loss: 0.00002509
Iteration 115/1000 | Loss: 0.00002509
Iteration 116/1000 | Loss: 0.00002509
Iteration 117/1000 | Loss: 0.00002509
Iteration 118/1000 | Loss: 0.00002509
Iteration 119/1000 | Loss: 0.00002509
Iteration 120/1000 | Loss: 0.00002509
Iteration 121/1000 | Loss: 0.00002509
Iteration 122/1000 | Loss: 0.00002509
Iteration 123/1000 | Loss: 0.00002509
Iteration 124/1000 | Loss: 0.00002509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.509011756046675e-05, 2.509011756046675e-05, 2.509011756046675e-05, 2.509011756046675e-05, 2.509011756046675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.509011756046675e-05

Optimization complete. Final v2v error: 4.424070358276367 mm

Highest mean error: 4.636332035064697 mm for frame 223

Lowest mean error: 4.159407615661621 mm for frame 178

Saving results

Total time: 35.54044556617737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1840/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410520
Iteration 2/25 | Loss: 0.00117117
Iteration 3/25 | Loss: 0.00106939
Iteration 4/25 | Loss: 0.00105447
Iteration 5/25 | Loss: 0.00104842
Iteration 6/25 | Loss: 0.00104677
Iteration 7/25 | Loss: 0.00104677
Iteration 8/25 | Loss: 0.00104677
Iteration 9/25 | Loss: 0.00104677
Iteration 10/25 | Loss: 0.00104677
Iteration 11/25 | Loss: 0.00104677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001046774792484939, 0.001046774792484939, 0.001046774792484939, 0.001046774792484939, 0.001046774792484939]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001046774792484939

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55612254
Iteration 2/25 | Loss: 0.00105062
Iteration 3/25 | Loss: 0.00105062
Iteration 4/25 | Loss: 0.00105062
Iteration 5/25 | Loss: 0.00105062
Iteration 6/25 | Loss: 0.00105062
Iteration 7/25 | Loss: 0.00105062
Iteration 8/25 | Loss: 0.00105062
Iteration 9/25 | Loss: 0.00105062
Iteration 10/25 | Loss: 0.00105062
Iteration 11/25 | Loss: 0.00105061
Iteration 12/25 | Loss: 0.00105061
Iteration 13/25 | Loss: 0.00105061
Iteration 14/25 | Loss: 0.00105061
Iteration 15/25 | Loss: 0.00105061
Iteration 16/25 | Loss: 0.00105061
Iteration 17/25 | Loss: 0.00105061
Iteration 18/25 | Loss: 0.00105061
Iteration 19/25 | Loss: 0.00105061
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010506148682907224, 0.0010506148682907224, 0.0010506148682907224, 0.0010506148682907224, 0.0010506148682907224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010506148682907224

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105061
Iteration 2/1000 | Loss: 0.00002950
Iteration 3/1000 | Loss: 0.00002358
Iteration 4/1000 | Loss: 0.00002260
Iteration 5/1000 | Loss: 0.00002186
Iteration 6/1000 | Loss: 0.00002143
Iteration 7/1000 | Loss: 0.00002120
Iteration 8/1000 | Loss: 0.00002108
Iteration 9/1000 | Loss: 0.00002104
Iteration 10/1000 | Loss: 0.00002104
Iteration 11/1000 | Loss: 0.00002103
Iteration 12/1000 | Loss: 0.00002103
Iteration 13/1000 | Loss: 0.00002102
Iteration 14/1000 | Loss: 0.00002092
Iteration 15/1000 | Loss: 0.00002092
Iteration 16/1000 | Loss: 0.00002092
Iteration 17/1000 | Loss: 0.00002092
Iteration 18/1000 | Loss: 0.00002092
Iteration 19/1000 | Loss: 0.00002090
Iteration 20/1000 | Loss: 0.00002089
Iteration 21/1000 | Loss: 0.00002089
Iteration 22/1000 | Loss: 0.00002089
Iteration 23/1000 | Loss: 0.00002088
Iteration 24/1000 | Loss: 0.00002088
Iteration 25/1000 | Loss: 0.00002088
Iteration 26/1000 | Loss: 0.00002087
Iteration 27/1000 | Loss: 0.00002087
Iteration 28/1000 | Loss: 0.00002085
Iteration 29/1000 | Loss: 0.00002085
Iteration 30/1000 | Loss: 0.00002085
Iteration 31/1000 | Loss: 0.00002085
Iteration 32/1000 | Loss: 0.00002085
Iteration 33/1000 | Loss: 0.00002085
Iteration 34/1000 | Loss: 0.00002084
Iteration 35/1000 | Loss: 0.00002084
Iteration 36/1000 | Loss: 0.00002084
Iteration 37/1000 | Loss: 0.00002084
Iteration 38/1000 | Loss: 0.00002084
Iteration 39/1000 | Loss: 0.00002084
Iteration 40/1000 | Loss: 0.00002084
Iteration 41/1000 | Loss: 0.00002084
Iteration 42/1000 | Loss: 0.00002084
Iteration 43/1000 | Loss: 0.00002083
Iteration 44/1000 | Loss: 0.00002083
Iteration 45/1000 | Loss: 0.00002083
Iteration 46/1000 | Loss: 0.00002083
Iteration 47/1000 | Loss: 0.00002083
Iteration 48/1000 | Loss: 0.00002083
Iteration 49/1000 | Loss: 0.00002083
Iteration 50/1000 | Loss: 0.00002083
Iteration 51/1000 | Loss: 0.00002083
Iteration 52/1000 | Loss: 0.00002083
Iteration 53/1000 | Loss: 0.00002083
Iteration 54/1000 | Loss: 0.00002083
Iteration 55/1000 | Loss: 0.00002083
Iteration 56/1000 | Loss: 0.00002082
Iteration 57/1000 | Loss: 0.00002082
Iteration 58/1000 | Loss: 0.00002082
Iteration 59/1000 | Loss: 0.00002082
Iteration 60/1000 | Loss: 0.00002082
Iteration 61/1000 | Loss: 0.00002081
Iteration 62/1000 | Loss: 0.00002081
Iteration 63/1000 | Loss: 0.00002081
Iteration 64/1000 | Loss: 0.00002081
Iteration 65/1000 | Loss: 0.00002081
Iteration 66/1000 | Loss: 0.00002081
Iteration 67/1000 | Loss: 0.00002081
Iteration 68/1000 | Loss: 0.00002081
Iteration 69/1000 | Loss: 0.00002081
Iteration 70/1000 | Loss: 0.00002080
Iteration 71/1000 | Loss: 0.00002080
Iteration 72/1000 | Loss: 0.00002080
Iteration 73/1000 | Loss: 0.00002080
Iteration 74/1000 | Loss: 0.00002080
Iteration 75/1000 | Loss: 0.00002080
Iteration 76/1000 | Loss: 0.00002080
Iteration 77/1000 | Loss: 0.00002080
Iteration 78/1000 | Loss: 0.00002080
Iteration 79/1000 | Loss: 0.00002080
Iteration 80/1000 | Loss: 0.00002080
Iteration 81/1000 | Loss: 0.00002080
Iteration 82/1000 | Loss: 0.00002079
Iteration 83/1000 | Loss: 0.00002079
Iteration 84/1000 | Loss: 0.00002079
Iteration 85/1000 | Loss: 0.00002079
Iteration 86/1000 | Loss: 0.00002079
Iteration 87/1000 | Loss: 0.00002079
Iteration 88/1000 | Loss: 0.00002079
Iteration 89/1000 | Loss: 0.00002079
Iteration 90/1000 | Loss: 0.00002079
Iteration 91/1000 | Loss: 0.00002079
Iteration 92/1000 | Loss: 0.00002079
Iteration 93/1000 | Loss: 0.00002079
Iteration 94/1000 | Loss: 0.00002079
Iteration 95/1000 | Loss: 0.00002079
Iteration 96/1000 | Loss: 0.00002079
Iteration 97/1000 | Loss: 0.00002079
Iteration 98/1000 | Loss: 0.00002079
Iteration 99/1000 | Loss: 0.00002079
Iteration 100/1000 | Loss: 0.00002079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.0791463612113148e-05, 2.0791463612113148e-05, 2.0791463612113148e-05, 2.0791463612113148e-05, 2.0791463612113148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0791463612113148e-05

Optimization complete. Final v2v error: 3.9795916080474854 mm

Highest mean error: 4.406605243682861 mm for frame 150

Lowest mean error: 3.7257139682769775 mm for frame 87

Saving results

Total time: 26.50722312927246
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1840/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01151785
Iteration 2/25 | Loss: 0.00312569
Iteration 3/25 | Loss: 0.00214563
Iteration 4/25 | Loss: 0.00303405
Iteration 5/25 | Loss: 0.00295244
Iteration 6/25 | Loss: 0.00251486
Iteration 7/25 | Loss: 0.00248316
Iteration 8/25 | Loss: 0.00239656
Iteration 9/25 | Loss: 0.00223527
Iteration 10/25 | Loss: 0.00208602
Iteration 11/25 | Loss: 0.00204142
Iteration 12/25 | Loss: 0.00205915
Iteration 13/25 | Loss: 0.00202896
Iteration 14/25 | Loss: 0.00195994
Iteration 15/25 | Loss: 0.00195674
Iteration 16/25 | Loss: 0.00194190
Iteration 17/25 | Loss: 0.00190333
Iteration 18/25 | Loss: 0.00189939
Iteration 19/25 | Loss: 0.00192277
Iteration 20/25 | Loss: 0.00187408
Iteration 21/25 | Loss: 0.00181442
Iteration 22/25 | Loss: 0.00181362
Iteration 23/25 | Loss: 0.00179079
Iteration 24/25 | Loss: 0.00178366
Iteration 25/25 | Loss: 0.00175170

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45416188
Iteration 2/25 | Loss: 0.00961943
Iteration 3/25 | Loss: 0.00934745
Iteration 4/25 | Loss: 0.00931833
Iteration 5/25 | Loss: 0.00931833
Iteration 6/25 | Loss: 0.00931833
Iteration 7/25 | Loss: 0.00931833
Iteration 8/25 | Loss: 0.00931833
Iteration 9/25 | Loss: 0.00931833
Iteration 10/25 | Loss: 0.00931833
Iteration 11/25 | Loss: 0.00931833
Iteration 12/25 | Loss: 0.00931833
Iteration 13/25 | Loss: 0.00931833
Iteration 14/25 | Loss: 0.00931832
Iteration 15/25 | Loss: 0.00931832
Iteration 16/25 | Loss: 0.00931832
Iteration 17/25 | Loss: 0.00931832
Iteration 18/25 | Loss: 0.00931833
Iteration 19/25 | Loss: 0.00931833
Iteration 20/25 | Loss: 0.00931833
Iteration 21/25 | Loss: 0.00931833
Iteration 22/25 | Loss: 0.00931833
Iteration 23/25 | Loss: 0.00931833
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00931832566857338, 0.00931832566857338, 0.00931832566857338, 0.00931832566857338, 0.00931832566857338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00931832566857338

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00931833
Iteration 2/1000 | Loss: 0.00730748
Iteration 3/1000 | Loss: 0.00464099
Iteration 4/1000 | Loss: 0.00547117
Iteration 5/1000 | Loss: 0.00602229
Iteration 6/1000 | Loss: 0.00562191
Iteration 7/1000 | Loss: 0.00505844
Iteration 8/1000 | Loss: 0.00591643
Iteration 9/1000 | Loss: 0.00581833
Iteration 10/1000 | Loss: 0.00661693
Iteration 11/1000 | Loss: 0.00390945
Iteration 12/1000 | Loss: 0.00392174
Iteration 13/1000 | Loss: 0.00383203
Iteration 14/1000 | Loss: 0.00403650
Iteration 15/1000 | Loss: 0.00435215
Iteration 16/1000 | Loss: 0.00475242
Iteration 17/1000 | Loss: 0.00542881
Iteration 18/1000 | Loss: 0.00364007
Iteration 19/1000 | Loss: 0.00345883
Iteration 20/1000 | Loss: 0.00422117
Iteration 21/1000 | Loss: 0.00477722
Iteration 22/1000 | Loss: 0.00419520
Iteration 23/1000 | Loss: 0.00391874
Iteration 24/1000 | Loss: 0.00424919
Iteration 25/1000 | Loss: 0.00410936
Iteration 26/1000 | Loss: 0.00379309
Iteration 27/1000 | Loss: 0.00430528
Iteration 28/1000 | Loss: 0.00452114
Iteration 29/1000 | Loss: 0.00470707
Iteration 30/1000 | Loss: 0.00464932
Iteration 31/1000 | Loss: 0.00374702
Iteration 32/1000 | Loss: 0.00361854
Iteration 33/1000 | Loss: 0.00226389
Iteration 34/1000 | Loss: 0.00299824
Iteration 35/1000 | Loss: 0.00251962
Iteration 36/1000 | Loss: 0.00347874
Iteration 37/1000 | Loss: 0.00305913
Iteration 38/1000 | Loss: 0.00396820
Iteration 39/1000 | Loss: 0.00455380
Iteration 40/1000 | Loss: 0.00358197
Iteration 41/1000 | Loss: 0.00224148
Iteration 42/1000 | Loss: 0.00202319
Iteration 43/1000 | Loss: 0.00212887
Iteration 44/1000 | Loss: 0.00219303
Iteration 45/1000 | Loss: 0.00187257
Iteration 46/1000 | Loss: 0.00191051
Iteration 47/1000 | Loss: 0.00154844
Iteration 48/1000 | Loss: 0.00184419
Iteration 49/1000 | Loss: 0.00243775
Iteration 50/1000 | Loss: 0.00202127
Iteration 51/1000 | Loss: 0.00273915
Iteration 52/1000 | Loss: 0.00205465
Iteration 53/1000 | Loss: 0.00171634
Iteration 54/1000 | Loss: 0.00093105
Iteration 55/1000 | Loss: 0.00106287
Iteration 56/1000 | Loss: 0.00126126
Iteration 57/1000 | Loss: 0.00199870
Iteration 58/1000 | Loss: 0.00203314
Iteration 59/1000 | Loss: 0.00135226
Iteration 60/1000 | Loss: 0.00155385
Iteration 61/1000 | Loss: 0.00195229
Iteration 62/1000 | Loss: 0.00092501
Iteration 63/1000 | Loss: 0.00064023
Iteration 64/1000 | Loss: 0.00152010
Iteration 65/1000 | Loss: 0.00113759
Iteration 66/1000 | Loss: 0.00096769
Iteration 67/1000 | Loss: 0.00140762
Iteration 68/1000 | Loss: 0.00099990
Iteration 69/1000 | Loss: 0.00098264
Iteration 70/1000 | Loss: 0.00083818
Iteration 71/1000 | Loss: 0.00088682
Iteration 72/1000 | Loss: 0.00184827
Iteration 73/1000 | Loss: 0.00178870
Iteration 74/1000 | Loss: 0.00065285
Iteration 75/1000 | Loss: 0.00060452
Iteration 76/1000 | Loss: 0.00124654
Iteration 77/1000 | Loss: 0.00164561
Iteration 78/1000 | Loss: 0.00172469
Iteration 79/1000 | Loss: 0.00239965
Iteration 80/1000 | Loss: 0.00164725
Iteration 81/1000 | Loss: 0.00192217
Iteration 82/1000 | Loss: 0.00091909
Iteration 83/1000 | Loss: 0.00086257
Iteration 84/1000 | Loss: 0.00059838
Iteration 85/1000 | Loss: 0.00046229
Iteration 86/1000 | Loss: 0.00064121
Iteration 87/1000 | Loss: 0.00054851
Iteration 88/1000 | Loss: 0.00087904
Iteration 89/1000 | Loss: 0.00122858
Iteration 90/1000 | Loss: 0.00064742
Iteration 91/1000 | Loss: 0.00099456
Iteration 92/1000 | Loss: 0.00055614
Iteration 93/1000 | Loss: 0.00065783
Iteration 94/1000 | Loss: 0.00088746
Iteration 95/1000 | Loss: 0.00054986
Iteration 96/1000 | Loss: 0.00051979
Iteration 97/1000 | Loss: 0.00062329
Iteration 98/1000 | Loss: 0.00041050
Iteration 99/1000 | Loss: 0.00053207
Iteration 100/1000 | Loss: 0.00089133
Iteration 101/1000 | Loss: 0.00179136
Iteration 102/1000 | Loss: 0.00092771
Iteration 103/1000 | Loss: 0.00072549
Iteration 104/1000 | Loss: 0.00126535
Iteration 105/1000 | Loss: 0.00081963
Iteration 106/1000 | Loss: 0.00084364
Iteration 107/1000 | Loss: 0.00032234
Iteration 108/1000 | Loss: 0.00070009
Iteration 109/1000 | Loss: 0.00177664
Iteration 110/1000 | Loss: 0.00104832
Iteration 111/1000 | Loss: 0.00084170
Iteration 112/1000 | Loss: 0.00076681
Iteration 113/1000 | Loss: 0.00029640
Iteration 114/1000 | Loss: 0.00056270
Iteration 115/1000 | Loss: 0.00036783
Iteration 116/1000 | Loss: 0.00066272
Iteration 117/1000 | Loss: 0.00044249
Iteration 118/1000 | Loss: 0.00024632
Iteration 119/1000 | Loss: 0.00072729
Iteration 120/1000 | Loss: 0.00061068
Iteration 121/1000 | Loss: 0.00043037
Iteration 122/1000 | Loss: 0.00050304
Iteration 123/1000 | Loss: 0.00057431
Iteration 124/1000 | Loss: 0.00083281
Iteration 125/1000 | Loss: 0.00063169
Iteration 126/1000 | Loss: 0.00064959
Iteration 127/1000 | Loss: 0.00036369
Iteration 128/1000 | Loss: 0.00071912
Iteration 129/1000 | Loss: 0.00055587
Iteration 130/1000 | Loss: 0.00052528
Iteration 131/1000 | Loss: 0.00044485
Iteration 132/1000 | Loss: 0.00043138
Iteration 133/1000 | Loss: 0.00110241
Iteration 134/1000 | Loss: 0.00048812
Iteration 135/1000 | Loss: 0.00051752
Iteration 136/1000 | Loss: 0.00054178
Iteration 137/1000 | Loss: 0.00053399
Iteration 138/1000 | Loss: 0.00050875
Iteration 139/1000 | Loss: 0.00041256
Iteration 140/1000 | Loss: 0.00070228
Iteration 141/1000 | Loss: 0.00031984
Iteration 142/1000 | Loss: 0.00027108
Iteration 143/1000 | Loss: 0.00039329
Iteration 144/1000 | Loss: 0.00029296
Iteration 145/1000 | Loss: 0.00049301
Iteration 146/1000 | Loss: 0.00061374
Iteration 147/1000 | Loss: 0.00027898
Iteration 148/1000 | Loss: 0.00028163
Iteration 149/1000 | Loss: 0.00050040
Iteration 150/1000 | Loss: 0.00030157
Iteration 151/1000 | Loss: 0.00068036
Iteration 152/1000 | Loss: 0.00058353
Iteration 153/1000 | Loss: 0.00030497
Iteration 154/1000 | Loss: 0.00072203
Iteration 155/1000 | Loss: 0.00045820
Iteration 156/1000 | Loss: 0.00057092
Iteration 157/1000 | Loss: 0.00062922
Iteration 158/1000 | Loss: 0.00037672
Iteration 159/1000 | Loss: 0.00039310
Iteration 160/1000 | Loss: 0.00040068
Iteration 161/1000 | Loss: 0.00030139
Iteration 162/1000 | Loss: 0.00032153
Iteration 163/1000 | Loss: 0.00035735
Iteration 164/1000 | Loss: 0.00012263
Iteration 165/1000 | Loss: 0.00030035
Iteration 166/1000 | Loss: 0.00057340
Iteration 167/1000 | Loss: 0.00058455
Iteration 168/1000 | Loss: 0.00039412
Iteration 169/1000 | Loss: 0.00059933
Iteration 170/1000 | Loss: 0.00063088
Iteration 171/1000 | Loss: 0.00056696
Iteration 172/1000 | Loss: 0.00058939
Iteration 173/1000 | Loss: 0.00052941
Iteration 174/1000 | Loss: 0.00050994
Iteration 175/1000 | Loss: 0.00033432
Iteration 176/1000 | Loss: 0.00024595
Iteration 177/1000 | Loss: 0.00031778
Iteration 178/1000 | Loss: 0.00055642
Iteration 179/1000 | Loss: 0.00068746
Iteration 180/1000 | Loss: 0.00047453
Iteration 181/1000 | Loss: 0.00040231
Iteration 182/1000 | Loss: 0.00035181
Iteration 183/1000 | Loss: 0.00025253
Iteration 184/1000 | Loss: 0.00040776
Iteration 185/1000 | Loss: 0.00024289
Iteration 186/1000 | Loss: 0.00043822
Iteration 187/1000 | Loss: 0.00038866
Iteration 188/1000 | Loss: 0.00024857
Iteration 189/1000 | Loss: 0.00031361
Iteration 190/1000 | Loss: 0.00054374
Iteration 191/1000 | Loss: 0.00047487
Iteration 192/1000 | Loss: 0.00042909
Iteration 193/1000 | Loss: 0.00025972
Iteration 194/1000 | Loss: 0.00045397
Iteration 195/1000 | Loss: 0.00025255
Iteration 196/1000 | Loss: 0.00025289
Iteration 197/1000 | Loss: 0.00039862
Iteration 198/1000 | Loss: 0.00026031
Iteration 199/1000 | Loss: 0.00049318
Iteration 200/1000 | Loss: 0.00044826
Iteration 201/1000 | Loss: 0.00048426
Iteration 202/1000 | Loss: 0.00044091
Iteration 203/1000 | Loss: 0.00048112
Iteration 204/1000 | Loss: 0.00024181
Iteration 205/1000 | Loss: 0.00020660
Iteration 206/1000 | Loss: 0.00023346
Iteration 207/1000 | Loss: 0.00025631
Iteration 208/1000 | Loss: 0.00022142
Iteration 209/1000 | Loss: 0.00024331
Iteration 210/1000 | Loss: 0.00023830
Iteration 211/1000 | Loss: 0.00024168
Iteration 212/1000 | Loss: 0.00024139
Iteration 213/1000 | Loss: 0.00028104
Iteration 214/1000 | Loss: 0.00023726
Iteration 215/1000 | Loss: 0.00027071
Iteration 216/1000 | Loss: 0.00047238
Iteration 217/1000 | Loss: 0.00044127
Iteration 218/1000 | Loss: 0.00044251
Iteration 219/1000 | Loss: 0.00025557
Iteration 220/1000 | Loss: 0.00027726
Iteration 221/1000 | Loss: 0.00024299
Iteration 222/1000 | Loss: 0.00022188
Iteration 223/1000 | Loss: 0.00024204
Iteration 224/1000 | Loss: 0.00021956
Iteration 225/1000 | Loss: 0.00023277
Iteration 226/1000 | Loss: 0.00024314
Iteration 227/1000 | Loss: 0.00023206
Iteration 228/1000 | Loss: 0.00023043
Iteration 229/1000 | Loss: 0.00020740
Iteration 230/1000 | Loss: 0.00018219
Iteration 231/1000 | Loss: 0.00014845
Iteration 232/1000 | Loss: 0.00020595
Iteration 233/1000 | Loss: 0.00023710
Iteration 234/1000 | Loss: 0.00023150
Iteration 235/1000 | Loss: 0.00021888
Iteration 236/1000 | Loss: 0.00039838
Iteration 237/1000 | Loss: 0.00022868
Iteration 238/1000 | Loss: 0.00024044
Iteration 239/1000 | Loss: 0.00021953
Iteration 240/1000 | Loss: 0.00023915
Iteration 241/1000 | Loss: 0.00021962
Iteration 242/1000 | Loss: 0.00023034
Iteration 243/1000 | Loss: 0.00021523
Iteration 244/1000 | Loss: 0.00023105
Iteration 245/1000 | Loss: 0.00029273
Iteration 246/1000 | Loss: 0.00019943
Iteration 247/1000 | Loss: 0.00024576
Iteration 248/1000 | Loss: 0.00022313
Iteration 249/1000 | Loss: 0.00022350
Iteration 250/1000 | Loss: 0.00026496
Iteration 251/1000 | Loss: 0.00023578
Iteration 252/1000 | Loss: 0.00023298
Iteration 253/1000 | Loss: 0.00023857
Iteration 254/1000 | Loss: 0.00023582
Iteration 255/1000 | Loss: 0.00025141
Iteration 256/1000 | Loss: 0.00028971
Iteration 257/1000 | Loss: 0.00025847
Iteration 258/1000 | Loss: 0.00017798
Iteration 259/1000 | Loss: 0.00015734
Iteration 260/1000 | Loss: 0.00017653
Iteration 261/1000 | Loss: 0.00041007
Iteration 262/1000 | Loss: 0.00034381
Iteration 263/1000 | Loss: 0.00026826
Iteration 264/1000 | Loss: 0.00021072
Iteration 265/1000 | Loss: 0.00021436
Iteration 266/1000 | Loss: 0.00016684
Iteration 267/1000 | Loss: 0.00021307
Iteration 268/1000 | Loss: 0.00020960
Iteration 269/1000 | Loss: 0.00022965
Iteration 270/1000 | Loss: 0.00021451
Iteration 271/1000 | Loss: 0.00024559
Iteration 272/1000 | Loss: 0.00022562
Iteration 273/1000 | Loss: 0.00023286
Iteration 274/1000 | Loss: 0.00020451
Iteration 275/1000 | Loss: 0.00022709
Iteration 276/1000 | Loss: 0.00021301
Iteration 277/1000 | Loss: 0.00023186
Iteration 278/1000 | Loss: 0.00024990
Iteration 279/1000 | Loss: 0.00046854
Iteration 280/1000 | Loss: 0.00047586
Iteration 281/1000 | Loss: 0.00046481
Iteration 282/1000 | Loss: 0.00043633
Iteration 283/1000 | Loss: 0.00023393
Iteration 284/1000 | Loss: 0.00025566
Iteration 285/1000 | Loss: 0.00018790
Iteration 286/1000 | Loss: 0.00021776
Iteration 287/1000 | Loss: 0.00020732
Iteration 288/1000 | Loss: 0.00023561
Iteration 289/1000 | Loss: 0.00021353
Iteration 290/1000 | Loss: 0.00021762
Iteration 291/1000 | Loss: 0.00041649
Iteration 292/1000 | Loss: 0.00034971
Iteration 293/1000 | Loss: 0.00036367
Iteration 294/1000 | Loss: 0.00023958
Iteration 295/1000 | Loss: 0.00022697
Iteration 296/1000 | Loss: 0.00021894
Iteration 297/1000 | Loss: 0.00023018
Iteration 298/1000 | Loss: 0.00020350
Iteration 299/1000 | Loss: 0.00058690
Iteration 300/1000 | Loss: 0.00029253
Iteration 301/1000 | Loss: 0.00024039
Iteration 302/1000 | Loss: 0.00022692
Iteration 303/1000 | Loss: 0.00065835
Iteration 304/1000 | Loss: 0.00055506
Iteration 305/1000 | Loss: 0.00024555
Iteration 306/1000 | Loss: 0.00024040
Iteration 307/1000 | Loss: 0.00021058
Iteration 308/1000 | Loss: 0.00021699
Iteration 309/1000 | Loss: 0.00022935
Iteration 310/1000 | Loss: 0.00022094
Iteration 311/1000 | Loss: 0.00033900
Iteration 312/1000 | Loss: 0.00025210
Iteration 313/1000 | Loss: 0.00028360
Iteration 314/1000 | Loss: 0.00024394
Iteration 315/1000 | Loss: 0.00024394
Iteration 316/1000 | Loss: 0.00023715
Iteration 317/1000 | Loss: 0.00022412
Iteration 318/1000 | Loss: 0.00028313
Iteration 319/1000 | Loss: 0.00020329
Iteration 320/1000 | Loss: 0.00017992
Iteration 321/1000 | Loss: 0.00020910
Iteration 322/1000 | Loss: 0.00019051
Iteration 323/1000 | Loss: 0.00025234
Iteration 324/1000 | Loss: 0.00023740
Iteration 325/1000 | Loss: 0.00025720
Iteration 326/1000 | Loss: 0.00024135
Iteration 327/1000 | Loss: 0.00029840
Iteration 328/1000 | Loss: 0.00033841
Iteration 329/1000 | Loss: 0.00023154
Iteration 330/1000 | Loss: 0.00023379
Iteration 331/1000 | Loss: 0.00023940
Iteration 332/1000 | Loss: 0.00022093
Iteration 333/1000 | Loss: 0.00027091
Iteration 334/1000 | Loss: 0.00026000
Iteration 335/1000 | Loss: 0.00025818
Iteration 336/1000 | Loss: 0.00027175
Iteration 337/1000 | Loss: 0.00024460
Iteration 338/1000 | Loss: 0.00023147
Iteration 339/1000 | Loss: 0.00024585
Iteration 340/1000 | Loss: 0.00025299
Iteration 341/1000 | Loss: 0.00023557
Iteration 342/1000 | Loss: 0.00041019
Iteration 343/1000 | Loss: 0.00020835
Iteration 344/1000 | Loss: 0.00025554
Iteration 345/1000 | Loss: 0.00025678
Iteration 346/1000 | Loss: 0.00023852
Iteration 347/1000 | Loss: 0.00024685
Iteration 348/1000 | Loss: 0.00022608
Iteration 349/1000 | Loss: 0.00024157
Iteration 350/1000 | Loss: 0.00024557
Iteration 351/1000 | Loss: 0.00024063
Iteration 352/1000 | Loss: 0.00023499
Iteration 353/1000 | Loss: 0.00022765
Iteration 354/1000 | Loss: 0.00022180
Iteration 355/1000 | Loss: 0.00021246
Iteration 356/1000 | Loss: 0.00022510
Iteration 357/1000 | Loss: 0.00018568
Iteration 358/1000 | Loss: 0.00015014
Iteration 359/1000 | Loss: 0.00023644
Iteration 360/1000 | Loss: 0.00021071
Iteration 361/1000 | Loss: 0.00023588
Iteration 362/1000 | Loss: 0.00023805
Iteration 363/1000 | Loss: 0.00023674
Iteration 364/1000 | Loss: 0.00021865
Iteration 365/1000 | Loss: 0.00022698
Iteration 366/1000 | Loss: 0.00021454
Iteration 367/1000 | Loss: 0.00022912
Iteration 368/1000 | Loss: 0.00020873
Iteration 369/1000 | Loss: 0.00023272
Iteration 370/1000 | Loss: 0.00021468
Iteration 371/1000 | Loss: 0.00022738
Iteration 372/1000 | Loss: 0.00021002
Iteration 373/1000 | Loss: 0.00021854
Iteration 374/1000 | Loss: 0.00020212
Iteration 375/1000 | Loss: 0.00021595
Iteration 376/1000 | Loss: 0.00035552
Iteration 377/1000 | Loss: 0.00020606
Iteration 378/1000 | Loss: 0.00025140
Iteration 379/1000 | Loss: 0.00022202
Iteration 380/1000 | Loss: 0.00023815
Iteration 381/1000 | Loss: 0.00023538
Iteration 382/1000 | Loss: 0.00021545
Iteration 383/1000 | Loss: 0.00022652
Iteration 384/1000 | Loss: 0.00021439
Iteration 385/1000 | Loss: 0.00021419
Iteration 386/1000 | Loss: 0.00021606
Iteration 387/1000 | Loss: 0.00021231
Iteration 388/1000 | Loss: 0.00021014
Iteration 389/1000 | Loss: 0.00021011
Iteration 390/1000 | Loss: 0.00019696
Iteration 391/1000 | Loss: 0.00015320
Iteration 392/1000 | Loss: 0.00019159
Iteration 393/1000 | Loss: 0.00009111
Iteration 394/1000 | Loss: 0.00010092
Iteration 395/1000 | Loss: 0.00004688
Iteration 396/1000 | Loss: 0.00007438
Iteration 397/1000 | Loss: 0.00006112
Iteration 398/1000 | Loss: 0.00005504
Iteration 399/1000 | Loss: 0.00005636
Iteration 400/1000 | Loss: 0.00008821
Iteration 401/1000 | Loss: 0.00006687
Iteration 402/1000 | Loss: 0.00007312
Iteration 403/1000 | Loss: 0.00005444
Iteration 404/1000 | Loss: 0.00005488
Iteration 405/1000 | Loss: 0.00005258
Iteration 406/1000 | Loss: 0.00004810
Iteration 407/1000 | Loss: 0.00004499
Iteration 408/1000 | Loss: 0.00006751
Iteration 409/1000 | Loss: 0.00006486
Iteration 410/1000 | Loss: 0.00006089
Iteration 411/1000 | Loss: 0.00006487
Iteration 412/1000 | Loss: 0.00005459
Iteration 413/1000 | Loss: 0.00006035
Iteration 414/1000 | Loss: 0.00006940
Iteration 415/1000 | Loss: 0.00006694
Iteration 416/1000 | Loss: 0.00005347
Iteration 417/1000 | Loss: 0.00004226
Iteration 418/1000 | Loss: 0.00005631
Iteration 419/1000 | Loss: 0.00006834
Iteration 420/1000 | Loss: 0.00005962
Iteration 421/1000 | Loss: 0.00006347
Iteration 422/1000 | Loss: 0.00009223
Iteration 423/1000 | Loss: 0.00006832
Iteration 424/1000 | Loss: 0.00005863
Iteration 425/1000 | Loss: 0.00004663
Iteration 426/1000 | Loss: 0.00005734
Iteration 427/1000 | Loss: 0.00007928
Iteration 428/1000 | Loss: 0.00006865
Iteration 429/1000 | Loss: 0.00004724
Iteration 430/1000 | Loss: 0.00006730
Iteration 431/1000 | Loss: 0.00006000
Iteration 432/1000 | Loss: 0.00007546
Iteration 433/1000 | Loss: 0.00006399
Iteration 434/1000 | Loss: 0.00003534
Iteration 435/1000 | Loss: 0.00003151
Iteration 436/1000 | Loss: 0.00003012
Iteration 437/1000 | Loss: 0.00003423
Iteration 438/1000 | Loss: 0.00003543
Iteration 439/1000 | Loss: 0.00003058
Iteration 440/1000 | Loss: 0.00002777
Iteration 441/1000 | Loss: 0.00002488
Iteration 442/1000 | Loss: 0.00002377
Iteration 443/1000 | Loss: 0.00002296
Iteration 444/1000 | Loss: 0.00002266
Iteration 445/1000 | Loss: 0.00002244
Iteration 446/1000 | Loss: 0.00002225
Iteration 447/1000 | Loss: 0.00002217
Iteration 448/1000 | Loss: 0.00002217
Iteration 449/1000 | Loss: 0.00002215
Iteration 450/1000 | Loss: 0.00002215
Iteration 451/1000 | Loss: 0.00002214
Iteration 452/1000 | Loss: 0.00002214
Iteration 453/1000 | Loss: 0.00002214
Iteration 454/1000 | Loss: 0.00002214
Iteration 455/1000 | Loss: 0.00002214
Iteration 456/1000 | Loss: 0.00002213
Iteration 457/1000 | Loss: 0.00002213
Iteration 458/1000 | Loss: 0.00002213
Iteration 459/1000 | Loss: 0.00002213
Iteration 460/1000 | Loss: 0.00002213
Iteration 461/1000 | Loss: 0.00002213
Iteration 462/1000 | Loss: 0.00002213
Iteration 463/1000 | Loss: 0.00002213
Iteration 464/1000 | Loss: 0.00002213
Iteration 465/1000 | Loss: 0.00002213
Iteration 466/1000 | Loss: 0.00002213
Iteration 467/1000 | Loss: 0.00002212
Iteration 468/1000 | Loss: 0.00002212
Iteration 469/1000 | Loss: 0.00002212
Iteration 470/1000 | Loss: 0.00002212
Iteration 471/1000 | Loss: 0.00002212
Iteration 472/1000 | Loss: 0.00002212
Iteration 473/1000 | Loss: 0.00002212
Iteration 474/1000 | Loss: 0.00002212
Iteration 475/1000 | Loss: 0.00002212
Iteration 476/1000 | Loss: 0.00002211
Iteration 477/1000 | Loss: 0.00002211
Iteration 478/1000 | Loss: 0.00002211
Iteration 479/1000 | Loss: 0.00002211
Iteration 480/1000 | Loss: 0.00002210
Iteration 481/1000 | Loss: 0.00002210
Iteration 482/1000 | Loss: 0.00002210
Iteration 483/1000 | Loss: 0.00002210
Iteration 484/1000 | Loss: 0.00002210
Iteration 485/1000 | Loss: 0.00002210
Iteration 486/1000 | Loss: 0.00002210
Iteration 487/1000 | Loss: 0.00002210
Iteration 488/1000 | Loss: 0.00002210
Iteration 489/1000 | Loss: 0.00002210
Iteration 490/1000 | Loss: 0.00002209
Iteration 491/1000 | Loss: 0.00002209
Iteration 492/1000 | Loss: 0.00002209
Iteration 493/1000 | Loss: 0.00002209
Iteration 494/1000 | Loss: 0.00002209
Iteration 495/1000 | Loss: 0.00002209
Iteration 496/1000 | Loss: 0.00002209
Iteration 497/1000 | Loss: 0.00002209
Iteration 498/1000 | Loss: 0.00002209
Iteration 499/1000 | Loss: 0.00002209
Iteration 500/1000 | Loss: 0.00002208
Iteration 501/1000 | Loss: 0.00002208
Iteration 502/1000 | Loss: 0.00002208
Iteration 503/1000 | Loss: 0.00002208
Iteration 504/1000 | Loss: 0.00002208
Iteration 505/1000 | Loss: 0.00002208
Iteration 506/1000 | Loss: 0.00002208
Iteration 507/1000 | Loss: 0.00002208
Iteration 508/1000 | Loss: 0.00002207
Iteration 509/1000 | Loss: 0.00002207
Iteration 510/1000 | Loss: 0.00002207
Iteration 511/1000 | Loss: 0.00002207
Iteration 512/1000 | Loss: 0.00002207
Iteration 513/1000 | Loss: 0.00002207
Iteration 514/1000 | Loss: 0.00002207
Iteration 515/1000 | Loss: 0.00002207
Iteration 516/1000 | Loss: 0.00002207
Iteration 517/1000 | Loss: 0.00002207
Iteration 518/1000 | Loss: 0.00002207
Iteration 519/1000 | Loss: 0.00002207
Iteration 520/1000 | Loss: 0.00002206
Iteration 521/1000 | Loss: 0.00002206
Iteration 522/1000 | Loss: 0.00002206
Iteration 523/1000 | Loss: 0.00002206
Iteration 524/1000 | Loss: 0.00002206
Iteration 525/1000 | Loss: 0.00002206
Iteration 526/1000 | Loss: 0.00002205
Iteration 527/1000 | Loss: 0.00002205
Iteration 528/1000 | Loss: 0.00002205
Iteration 529/1000 | Loss: 0.00002205
Iteration 530/1000 | Loss: 0.00002204
Iteration 531/1000 | Loss: 0.00002204
Iteration 532/1000 | Loss: 0.00002204
Iteration 533/1000 | Loss: 0.00002204
Iteration 534/1000 | Loss: 0.00002204
Iteration 535/1000 | Loss: 0.00002204
Iteration 536/1000 | Loss: 0.00002204
Iteration 537/1000 | Loss: 0.00002204
Iteration 538/1000 | Loss: 0.00002204
Iteration 539/1000 | Loss: 0.00002204
Iteration 540/1000 | Loss: 0.00002204
Iteration 541/1000 | Loss: 0.00002203
Iteration 542/1000 | Loss: 0.00002203
Iteration 543/1000 | Loss: 0.00002203
Iteration 544/1000 | Loss: 0.00002203
Iteration 545/1000 | Loss: 0.00002203
Iteration 546/1000 | Loss: 0.00002203
Iteration 547/1000 | Loss: 0.00002203
Iteration 548/1000 | Loss: 0.00002203
Iteration 549/1000 | Loss: 0.00002203
Iteration 550/1000 | Loss: 0.00002203
Iteration 551/1000 | Loss: 0.00002203
Iteration 552/1000 | Loss: 0.00002203
Iteration 553/1000 | Loss: 0.00002203
Iteration 554/1000 | Loss: 0.00002203
Iteration 555/1000 | Loss: 0.00002203
Iteration 556/1000 | Loss: 0.00002203
Iteration 557/1000 | Loss: 0.00002203
Iteration 558/1000 | Loss: 0.00002203
Iteration 559/1000 | Loss: 0.00002203
Iteration 560/1000 | Loss: 0.00002203
Iteration 561/1000 | Loss: 0.00002203
Iteration 562/1000 | Loss: 0.00002203
Iteration 563/1000 | Loss: 0.00002203
Iteration 564/1000 | Loss: 0.00002203
Iteration 565/1000 | Loss: 0.00002203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 565. Stopping optimization.
Last 5 losses: [2.2034197172615677e-05, 2.2034197172615677e-05, 2.2034197172615677e-05, 2.2034197172615677e-05, 2.2034197172615677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2034197172615677e-05

Optimization complete. Final v2v error: 4.003110408782959 mm

Highest mean error: 4.548006534576416 mm for frame 99

Lowest mean error: 3.6965420246124268 mm for frame 18

Saving results

Total time: 697.4869360923767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1840/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00649652
Iteration 2/25 | Loss: 0.00146464
Iteration 3/25 | Loss: 0.00126280
Iteration 4/25 | Loss: 0.00109534
Iteration 5/25 | Loss: 0.00107826
Iteration 6/25 | Loss: 0.00106644
Iteration 7/25 | Loss: 0.00105293
Iteration 8/25 | Loss: 0.00104455
Iteration 9/25 | Loss: 0.00103940
Iteration 10/25 | Loss: 0.00103708
Iteration 11/25 | Loss: 0.00103625
Iteration 12/25 | Loss: 0.00103587
Iteration 13/25 | Loss: 0.00103571
Iteration 14/25 | Loss: 0.00103571
Iteration 15/25 | Loss: 0.00103570
Iteration 16/25 | Loss: 0.00103570
Iteration 17/25 | Loss: 0.00103570
Iteration 18/25 | Loss: 0.00103570
Iteration 19/25 | Loss: 0.00103570
Iteration 20/25 | Loss: 0.00103570
Iteration 21/25 | Loss: 0.00103570
Iteration 22/25 | Loss: 0.00103570
Iteration 23/25 | Loss: 0.00103570
Iteration 24/25 | Loss: 0.00103570
Iteration 25/25 | Loss: 0.00103570

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.32104254
Iteration 2/25 | Loss: 0.00092712
Iteration 3/25 | Loss: 0.00092712
Iteration 4/25 | Loss: 0.00092712
Iteration 5/25 | Loss: 0.00092712
Iteration 6/25 | Loss: 0.00092712
Iteration 7/25 | Loss: 0.00092712
Iteration 8/25 | Loss: 0.00092712
Iteration 9/25 | Loss: 0.00092712
Iteration 10/25 | Loss: 0.00092712
Iteration 11/25 | Loss: 0.00092712
Iteration 12/25 | Loss: 0.00092712
Iteration 13/25 | Loss: 0.00092712
Iteration 14/25 | Loss: 0.00092712
Iteration 15/25 | Loss: 0.00092712
Iteration 16/25 | Loss: 0.00092712
Iteration 17/25 | Loss: 0.00092712
Iteration 18/25 | Loss: 0.00092712
Iteration 19/25 | Loss: 0.00092712
Iteration 20/25 | Loss: 0.00092712
Iteration 21/25 | Loss: 0.00092712
Iteration 22/25 | Loss: 0.00092712
Iteration 23/25 | Loss: 0.00092712
Iteration 24/25 | Loss: 0.00092712
Iteration 25/25 | Loss: 0.00092712

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092712
Iteration 2/1000 | Loss: 0.00003337
Iteration 3/1000 | Loss: 0.00002296
Iteration 4/1000 | Loss: 0.00002073
Iteration 5/1000 | Loss: 0.00001974
Iteration 6/1000 | Loss: 0.00001914
Iteration 7/1000 | Loss: 0.00001866
Iteration 8/1000 | Loss: 0.00001847
Iteration 9/1000 | Loss: 0.00001828
Iteration 10/1000 | Loss: 0.00001819
Iteration 11/1000 | Loss: 0.00001816
Iteration 12/1000 | Loss: 0.00001815
Iteration 13/1000 | Loss: 0.00001815
Iteration 14/1000 | Loss: 0.00001814
Iteration 15/1000 | Loss: 0.00001811
Iteration 16/1000 | Loss: 0.00001810
Iteration 17/1000 | Loss: 0.00001810
Iteration 18/1000 | Loss: 0.00001809
Iteration 19/1000 | Loss: 0.00001809
Iteration 20/1000 | Loss: 0.00001808
Iteration 21/1000 | Loss: 0.00001808
Iteration 22/1000 | Loss: 0.00001808
Iteration 23/1000 | Loss: 0.00001807
Iteration 24/1000 | Loss: 0.00001807
Iteration 25/1000 | Loss: 0.00001807
Iteration 26/1000 | Loss: 0.00001806
Iteration 27/1000 | Loss: 0.00001805
Iteration 28/1000 | Loss: 0.00001805
Iteration 29/1000 | Loss: 0.00001805
Iteration 30/1000 | Loss: 0.00001804
Iteration 31/1000 | Loss: 0.00001804
Iteration 32/1000 | Loss: 0.00001804
Iteration 33/1000 | Loss: 0.00001804
Iteration 34/1000 | Loss: 0.00001803
Iteration 35/1000 | Loss: 0.00001803
Iteration 36/1000 | Loss: 0.00001803
Iteration 37/1000 | Loss: 0.00001803
Iteration 38/1000 | Loss: 0.00001802
Iteration 39/1000 | Loss: 0.00001802
Iteration 40/1000 | Loss: 0.00001802
Iteration 41/1000 | Loss: 0.00001802
Iteration 42/1000 | Loss: 0.00001802
Iteration 43/1000 | Loss: 0.00001801
Iteration 44/1000 | Loss: 0.00001801
Iteration 45/1000 | Loss: 0.00001801
Iteration 46/1000 | Loss: 0.00001800
Iteration 47/1000 | Loss: 0.00001800
Iteration 48/1000 | Loss: 0.00001800
Iteration 49/1000 | Loss: 0.00001799
Iteration 50/1000 | Loss: 0.00001799
Iteration 51/1000 | Loss: 0.00001799
Iteration 52/1000 | Loss: 0.00001799
Iteration 53/1000 | Loss: 0.00001799
Iteration 54/1000 | Loss: 0.00001798
Iteration 55/1000 | Loss: 0.00001798
Iteration 56/1000 | Loss: 0.00001798
Iteration 57/1000 | Loss: 0.00001798
Iteration 58/1000 | Loss: 0.00001798
Iteration 59/1000 | Loss: 0.00001798
Iteration 60/1000 | Loss: 0.00001798
Iteration 61/1000 | Loss: 0.00001798
Iteration 62/1000 | Loss: 0.00001797
Iteration 63/1000 | Loss: 0.00001797
Iteration 64/1000 | Loss: 0.00001797
Iteration 65/1000 | Loss: 0.00001797
Iteration 66/1000 | Loss: 0.00001797
Iteration 67/1000 | Loss: 0.00001797
Iteration 68/1000 | Loss: 0.00001797
Iteration 69/1000 | Loss: 0.00001797
Iteration 70/1000 | Loss: 0.00001797
Iteration 71/1000 | Loss: 0.00001797
Iteration 72/1000 | Loss: 0.00001797
Iteration 73/1000 | Loss: 0.00001797
Iteration 74/1000 | Loss: 0.00001797
Iteration 75/1000 | Loss: 0.00001797
Iteration 76/1000 | Loss: 0.00001796
Iteration 77/1000 | Loss: 0.00001796
Iteration 78/1000 | Loss: 0.00001796
Iteration 79/1000 | Loss: 0.00001796
Iteration 80/1000 | Loss: 0.00001796
Iteration 81/1000 | Loss: 0.00001796
Iteration 82/1000 | Loss: 0.00001796
Iteration 83/1000 | Loss: 0.00001796
Iteration 84/1000 | Loss: 0.00001796
Iteration 85/1000 | Loss: 0.00001796
Iteration 86/1000 | Loss: 0.00001796
Iteration 87/1000 | Loss: 0.00001796
Iteration 88/1000 | Loss: 0.00001796
Iteration 89/1000 | Loss: 0.00001796
Iteration 90/1000 | Loss: 0.00001796
Iteration 91/1000 | Loss: 0.00001796
Iteration 92/1000 | Loss: 0.00001796
Iteration 93/1000 | Loss: 0.00001796
Iteration 94/1000 | Loss: 0.00001795
Iteration 95/1000 | Loss: 0.00001795
Iteration 96/1000 | Loss: 0.00001795
Iteration 97/1000 | Loss: 0.00001795
Iteration 98/1000 | Loss: 0.00001795
Iteration 99/1000 | Loss: 0.00001795
Iteration 100/1000 | Loss: 0.00001795
Iteration 101/1000 | Loss: 0.00001794
Iteration 102/1000 | Loss: 0.00001794
Iteration 103/1000 | Loss: 0.00001794
Iteration 104/1000 | Loss: 0.00001794
Iteration 105/1000 | Loss: 0.00001794
Iteration 106/1000 | Loss: 0.00001794
Iteration 107/1000 | Loss: 0.00001794
Iteration 108/1000 | Loss: 0.00001793
Iteration 109/1000 | Loss: 0.00001793
Iteration 110/1000 | Loss: 0.00001793
Iteration 111/1000 | Loss: 0.00001793
Iteration 112/1000 | Loss: 0.00001793
Iteration 113/1000 | Loss: 0.00001793
Iteration 114/1000 | Loss: 0.00001793
Iteration 115/1000 | Loss: 0.00001793
Iteration 116/1000 | Loss: 0.00001793
Iteration 117/1000 | Loss: 0.00001793
Iteration 118/1000 | Loss: 0.00001792
Iteration 119/1000 | Loss: 0.00001792
Iteration 120/1000 | Loss: 0.00001792
Iteration 121/1000 | Loss: 0.00001792
Iteration 122/1000 | Loss: 0.00001792
Iteration 123/1000 | Loss: 0.00001792
Iteration 124/1000 | Loss: 0.00001792
Iteration 125/1000 | Loss: 0.00001792
Iteration 126/1000 | Loss: 0.00001792
Iteration 127/1000 | Loss: 0.00001792
Iteration 128/1000 | Loss: 0.00001792
Iteration 129/1000 | Loss: 0.00001792
Iteration 130/1000 | Loss: 0.00001792
Iteration 131/1000 | Loss: 0.00001792
Iteration 132/1000 | Loss: 0.00001792
Iteration 133/1000 | Loss: 0.00001792
Iteration 134/1000 | Loss: 0.00001791
Iteration 135/1000 | Loss: 0.00001791
Iteration 136/1000 | Loss: 0.00001791
Iteration 137/1000 | Loss: 0.00001791
Iteration 138/1000 | Loss: 0.00001791
Iteration 139/1000 | Loss: 0.00001791
Iteration 140/1000 | Loss: 0.00001791
Iteration 141/1000 | Loss: 0.00001791
Iteration 142/1000 | Loss: 0.00001791
Iteration 143/1000 | Loss: 0.00001791
Iteration 144/1000 | Loss: 0.00001791
Iteration 145/1000 | Loss: 0.00001791
Iteration 146/1000 | Loss: 0.00001791
Iteration 147/1000 | Loss: 0.00001791
Iteration 148/1000 | Loss: 0.00001791
Iteration 149/1000 | Loss: 0.00001791
Iteration 150/1000 | Loss: 0.00001791
Iteration 151/1000 | Loss: 0.00001791
Iteration 152/1000 | Loss: 0.00001791
Iteration 153/1000 | Loss: 0.00001791
Iteration 154/1000 | Loss: 0.00001791
Iteration 155/1000 | Loss: 0.00001791
Iteration 156/1000 | Loss: 0.00001791
Iteration 157/1000 | Loss: 0.00001791
Iteration 158/1000 | Loss: 0.00001791
Iteration 159/1000 | Loss: 0.00001791
Iteration 160/1000 | Loss: 0.00001791
Iteration 161/1000 | Loss: 0.00001791
Iteration 162/1000 | Loss: 0.00001791
Iteration 163/1000 | Loss: 0.00001791
Iteration 164/1000 | Loss: 0.00001791
Iteration 165/1000 | Loss: 0.00001791
Iteration 166/1000 | Loss: 0.00001791
Iteration 167/1000 | Loss: 0.00001791
Iteration 168/1000 | Loss: 0.00001791
Iteration 169/1000 | Loss: 0.00001791
Iteration 170/1000 | Loss: 0.00001791
Iteration 171/1000 | Loss: 0.00001791
Iteration 172/1000 | Loss: 0.00001791
Iteration 173/1000 | Loss: 0.00001791
Iteration 174/1000 | Loss: 0.00001791
Iteration 175/1000 | Loss: 0.00001791
Iteration 176/1000 | Loss: 0.00001791
Iteration 177/1000 | Loss: 0.00001791
Iteration 178/1000 | Loss: 0.00001791
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.790542773960624e-05, 1.790542773960624e-05, 1.790542773960624e-05, 1.790542773960624e-05, 1.790542773960624e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.790542773960624e-05

Optimization complete. Final v2v error: 3.6669440269470215 mm

Highest mean error: 4.046061992645264 mm for frame 33

Lowest mean error: 3.2386558055877686 mm for frame 216

Saving results

Total time: 51.98130679130554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1840/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894033
Iteration 2/25 | Loss: 0.00126410
Iteration 3/25 | Loss: 0.00116469
Iteration 4/25 | Loss: 0.00113583
Iteration 5/25 | Loss: 0.00112595
Iteration 6/25 | Loss: 0.00112343
Iteration 7/25 | Loss: 0.00112304
Iteration 8/25 | Loss: 0.00112304
Iteration 9/25 | Loss: 0.00112304
Iteration 10/25 | Loss: 0.00112304
Iteration 11/25 | Loss: 0.00112304
Iteration 12/25 | Loss: 0.00112304
Iteration 13/25 | Loss: 0.00112304
Iteration 14/25 | Loss: 0.00112304
Iteration 15/25 | Loss: 0.00112304
Iteration 16/25 | Loss: 0.00112304
Iteration 17/25 | Loss: 0.00112304
Iteration 18/25 | Loss: 0.00112304
Iteration 19/25 | Loss: 0.00112304
Iteration 20/25 | Loss: 0.00112304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001123035908676684, 0.001123035908676684, 0.001123035908676684, 0.001123035908676684, 0.001123035908676684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001123035908676684

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52757514
Iteration 2/25 | Loss: 0.00112194
Iteration 3/25 | Loss: 0.00112193
Iteration 4/25 | Loss: 0.00112193
Iteration 5/25 | Loss: 0.00112193
Iteration 6/25 | Loss: 0.00112193
Iteration 7/25 | Loss: 0.00112193
Iteration 8/25 | Loss: 0.00112193
Iteration 9/25 | Loss: 0.00112193
Iteration 10/25 | Loss: 0.00112193
Iteration 11/25 | Loss: 0.00112193
Iteration 12/25 | Loss: 0.00112193
Iteration 13/25 | Loss: 0.00112193
Iteration 14/25 | Loss: 0.00112193
Iteration 15/25 | Loss: 0.00112193
Iteration 16/25 | Loss: 0.00112193
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011219301959499717, 0.0011219301959499717, 0.0011219301959499717, 0.0011219301959499717, 0.0011219301959499717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011219301959499717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112193
Iteration 2/1000 | Loss: 0.00003694
Iteration 3/1000 | Loss: 0.00003110
Iteration 4/1000 | Loss: 0.00002815
Iteration 5/1000 | Loss: 0.00002719
Iteration 6/1000 | Loss: 0.00002632
Iteration 7/1000 | Loss: 0.00002566
Iteration 8/1000 | Loss: 0.00002524
Iteration 9/1000 | Loss: 0.00002488
Iteration 10/1000 | Loss: 0.00002469
Iteration 11/1000 | Loss: 0.00002465
Iteration 12/1000 | Loss: 0.00002459
Iteration 13/1000 | Loss: 0.00002458
Iteration 14/1000 | Loss: 0.00002458
Iteration 15/1000 | Loss: 0.00002455
Iteration 16/1000 | Loss: 0.00002455
Iteration 17/1000 | Loss: 0.00002455
Iteration 18/1000 | Loss: 0.00002455
Iteration 19/1000 | Loss: 0.00002453
Iteration 20/1000 | Loss: 0.00002452
Iteration 21/1000 | Loss: 0.00002452
Iteration 22/1000 | Loss: 0.00002452
Iteration 23/1000 | Loss: 0.00002452
Iteration 24/1000 | Loss: 0.00002451
Iteration 25/1000 | Loss: 0.00002451
Iteration 26/1000 | Loss: 0.00002451
Iteration 27/1000 | Loss: 0.00002451
Iteration 28/1000 | Loss: 0.00002450
Iteration 29/1000 | Loss: 0.00002450
Iteration 30/1000 | Loss: 0.00002450
Iteration 31/1000 | Loss: 0.00002450
Iteration 32/1000 | Loss: 0.00002450
Iteration 33/1000 | Loss: 0.00002450
Iteration 34/1000 | Loss: 0.00002450
Iteration 35/1000 | Loss: 0.00002450
Iteration 36/1000 | Loss: 0.00002450
Iteration 37/1000 | Loss: 0.00002449
Iteration 38/1000 | Loss: 0.00002449
Iteration 39/1000 | Loss: 0.00002449
Iteration 40/1000 | Loss: 0.00002449
Iteration 41/1000 | Loss: 0.00002449
Iteration 42/1000 | Loss: 0.00002449
Iteration 43/1000 | Loss: 0.00002449
Iteration 44/1000 | Loss: 0.00002449
Iteration 45/1000 | Loss: 0.00002449
Iteration 46/1000 | Loss: 0.00002449
Iteration 47/1000 | Loss: 0.00002449
Iteration 48/1000 | Loss: 0.00002448
Iteration 49/1000 | Loss: 0.00002448
Iteration 50/1000 | Loss: 0.00002448
Iteration 51/1000 | Loss: 0.00002448
Iteration 52/1000 | Loss: 0.00002448
Iteration 53/1000 | Loss: 0.00002447
Iteration 54/1000 | Loss: 0.00002447
Iteration 55/1000 | Loss: 0.00002447
Iteration 56/1000 | Loss: 0.00002447
Iteration 57/1000 | Loss: 0.00002447
Iteration 58/1000 | Loss: 0.00002447
Iteration 59/1000 | Loss: 0.00002447
Iteration 60/1000 | Loss: 0.00002447
Iteration 61/1000 | Loss: 0.00002447
Iteration 62/1000 | Loss: 0.00002447
Iteration 63/1000 | Loss: 0.00002447
Iteration 64/1000 | Loss: 0.00002447
Iteration 65/1000 | Loss: 0.00002446
Iteration 66/1000 | Loss: 0.00002446
Iteration 67/1000 | Loss: 0.00002446
Iteration 68/1000 | Loss: 0.00002446
Iteration 69/1000 | Loss: 0.00002446
Iteration 70/1000 | Loss: 0.00002446
Iteration 71/1000 | Loss: 0.00002446
Iteration 72/1000 | Loss: 0.00002446
Iteration 73/1000 | Loss: 0.00002446
Iteration 74/1000 | Loss: 0.00002445
Iteration 75/1000 | Loss: 0.00002445
Iteration 76/1000 | Loss: 0.00002445
Iteration 77/1000 | Loss: 0.00002445
Iteration 78/1000 | Loss: 0.00002445
Iteration 79/1000 | Loss: 0.00002445
Iteration 80/1000 | Loss: 0.00002445
Iteration 81/1000 | Loss: 0.00002445
Iteration 82/1000 | Loss: 0.00002445
Iteration 83/1000 | Loss: 0.00002445
Iteration 84/1000 | Loss: 0.00002445
Iteration 85/1000 | Loss: 0.00002445
Iteration 86/1000 | Loss: 0.00002445
Iteration 87/1000 | Loss: 0.00002445
Iteration 88/1000 | Loss: 0.00002445
Iteration 89/1000 | Loss: 0.00002445
Iteration 90/1000 | Loss: 0.00002445
Iteration 91/1000 | Loss: 0.00002445
Iteration 92/1000 | Loss: 0.00002444
Iteration 93/1000 | Loss: 0.00002444
Iteration 94/1000 | Loss: 0.00002444
Iteration 95/1000 | Loss: 0.00002444
Iteration 96/1000 | Loss: 0.00002444
Iteration 97/1000 | Loss: 0.00002444
Iteration 98/1000 | Loss: 0.00002444
Iteration 99/1000 | Loss: 0.00002444
Iteration 100/1000 | Loss: 0.00002444
Iteration 101/1000 | Loss: 0.00002443
Iteration 102/1000 | Loss: 0.00002443
Iteration 103/1000 | Loss: 0.00002443
Iteration 104/1000 | Loss: 0.00002443
Iteration 105/1000 | Loss: 0.00002443
Iteration 106/1000 | Loss: 0.00002443
Iteration 107/1000 | Loss: 0.00002443
Iteration 108/1000 | Loss: 0.00002443
Iteration 109/1000 | Loss: 0.00002443
Iteration 110/1000 | Loss: 0.00002443
Iteration 111/1000 | Loss: 0.00002443
Iteration 112/1000 | Loss: 0.00002443
Iteration 113/1000 | Loss: 0.00002443
Iteration 114/1000 | Loss: 0.00002442
Iteration 115/1000 | Loss: 0.00002442
Iteration 116/1000 | Loss: 0.00002442
Iteration 117/1000 | Loss: 0.00002442
Iteration 118/1000 | Loss: 0.00002442
Iteration 119/1000 | Loss: 0.00002442
Iteration 120/1000 | Loss: 0.00002442
Iteration 121/1000 | Loss: 0.00002442
Iteration 122/1000 | Loss: 0.00002442
Iteration 123/1000 | Loss: 0.00002442
Iteration 124/1000 | Loss: 0.00002442
Iteration 125/1000 | Loss: 0.00002442
Iteration 126/1000 | Loss: 0.00002442
Iteration 127/1000 | Loss: 0.00002442
Iteration 128/1000 | Loss: 0.00002442
Iteration 129/1000 | Loss: 0.00002442
Iteration 130/1000 | Loss: 0.00002442
Iteration 131/1000 | Loss: 0.00002442
Iteration 132/1000 | Loss: 0.00002442
Iteration 133/1000 | Loss: 0.00002442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [2.4420061890850775e-05, 2.4420061890850775e-05, 2.4420061890850775e-05, 2.4420061890850775e-05, 2.4420061890850775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4420061890850775e-05

Optimization complete. Final v2v error: 4.214972496032715 mm

Highest mean error: 4.460938453674316 mm for frame 75

Lowest mean error: 3.970461845397949 mm for frame 172

Saving results

Total time: 36.44620680809021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1840/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00921417
Iteration 2/25 | Loss: 0.00117614
Iteration 3/25 | Loss: 0.00107737
Iteration 4/25 | Loss: 0.00106662
Iteration 5/25 | Loss: 0.00106311
Iteration 6/25 | Loss: 0.00106266
Iteration 7/25 | Loss: 0.00106266
Iteration 8/25 | Loss: 0.00106266
Iteration 9/25 | Loss: 0.00106266
Iteration 10/25 | Loss: 0.00106266
Iteration 11/25 | Loss: 0.00106266
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010626550065353513, 0.0010626550065353513, 0.0010626550065353513, 0.0010626550065353513, 0.0010626550065353513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010626550065353513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.57957840
Iteration 2/25 | Loss: 0.00104206
Iteration 3/25 | Loss: 0.00104206
Iteration 4/25 | Loss: 0.00104206
Iteration 5/25 | Loss: 0.00104206
Iteration 6/25 | Loss: 0.00104206
Iteration 7/25 | Loss: 0.00104206
Iteration 8/25 | Loss: 0.00104206
Iteration 9/25 | Loss: 0.00104206
Iteration 10/25 | Loss: 0.00104206
Iteration 11/25 | Loss: 0.00104206
Iteration 12/25 | Loss: 0.00104206
Iteration 13/25 | Loss: 0.00104206
Iteration 14/25 | Loss: 0.00104206
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00104205880779773, 0.00104205880779773, 0.00104205880779773, 0.00104205880779773, 0.00104205880779773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00104205880779773

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104206
Iteration 2/1000 | Loss: 0.00002993
Iteration 3/1000 | Loss: 0.00002453
Iteration 4/1000 | Loss: 0.00002263
Iteration 5/1000 | Loss: 0.00002173
Iteration 6/1000 | Loss: 0.00002093
Iteration 7/1000 | Loss: 0.00002061
Iteration 8/1000 | Loss: 0.00002061
Iteration 9/1000 | Loss: 0.00002045
Iteration 10/1000 | Loss: 0.00002040
Iteration 11/1000 | Loss: 0.00002036
Iteration 12/1000 | Loss: 0.00002034
Iteration 13/1000 | Loss: 0.00002033
Iteration 14/1000 | Loss: 0.00002033
Iteration 15/1000 | Loss: 0.00002032
Iteration 16/1000 | Loss: 0.00002032
Iteration 17/1000 | Loss: 0.00002031
Iteration 18/1000 | Loss: 0.00002030
Iteration 19/1000 | Loss: 0.00002029
Iteration 20/1000 | Loss: 0.00002029
Iteration 21/1000 | Loss: 0.00002029
Iteration 22/1000 | Loss: 0.00002028
Iteration 23/1000 | Loss: 0.00002028
Iteration 24/1000 | Loss: 0.00002027
Iteration 25/1000 | Loss: 0.00002026
Iteration 26/1000 | Loss: 0.00002026
Iteration 27/1000 | Loss: 0.00002025
Iteration 28/1000 | Loss: 0.00002024
Iteration 29/1000 | Loss: 0.00002024
Iteration 30/1000 | Loss: 0.00002024
Iteration 31/1000 | Loss: 0.00002023
Iteration 32/1000 | Loss: 0.00002023
Iteration 33/1000 | Loss: 0.00002022
Iteration 34/1000 | Loss: 0.00002022
Iteration 35/1000 | Loss: 0.00002022
Iteration 36/1000 | Loss: 0.00002021
Iteration 37/1000 | Loss: 0.00002021
Iteration 38/1000 | Loss: 0.00002021
Iteration 39/1000 | Loss: 0.00002021
Iteration 40/1000 | Loss: 0.00002021
Iteration 41/1000 | Loss: 0.00002021
Iteration 42/1000 | Loss: 0.00002021
Iteration 43/1000 | Loss: 0.00002021
Iteration 44/1000 | Loss: 0.00002020
Iteration 45/1000 | Loss: 0.00002020
Iteration 46/1000 | Loss: 0.00002020
Iteration 47/1000 | Loss: 0.00002019
Iteration 48/1000 | Loss: 0.00002019
Iteration 49/1000 | Loss: 0.00002019
Iteration 50/1000 | Loss: 0.00002019
Iteration 51/1000 | Loss: 0.00002019
Iteration 52/1000 | Loss: 0.00002018
Iteration 53/1000 | Loss: 0.00002018
Iteration 54/1000 | Loss: 0.00002018
Iteration 55/1000 | Loss: 0.00002017
Iteration 56/1000 | Loss: 0.00002017
Iteration 57/1000 | Loss: 0.00002016
Iteration 58/1000 | Loss: 0.00002016
Iteration 59/1000 | Loss: 0.00002016
Iteration 60/1000 | Loss: 0.00002015
Iteration 61/1000 | Loss: 0.00002015
Iteration 62/1000 | Loss: 0.00002015
Iteration 63/1000 | Loss: 0.00002014
Iteration 64/1000 | Loss: 0.00002014
Iteration 65/1000 | Loss: 0.00002014
Iteration 66/1000 | Loss: 0.00002013
Iteration 67/1000 | Loss: 0.00002013
Iteration 68/1000 | Loss: 0.00002012
Iteration 69/1000 | Loss: 0.00002012
Iteration 70/1000 | Loss: 0.00002012
Iteration 71/1000 | Loss: 0.00002012
Iteration 72/1000 | Loss: 0.00002012
Iteration 73/1000 | Loss: 0.00002012
Iteration 74/1000 | Loss: 0.00002012
Iteration 75/1000 | Loss: 0.00002012
Iteration 76/1000 | Loss: 0.00002012
Iteration 77/1000 | Loss: 0.00002011
Iteration 78/1000 | Loss: 0.00002011
Iteration 79/1000 | Loss: 0.00002011
Iteration 80/1000 | Loss: 0.00002011
Iteration 81/1000 | Loss: 0.00002011
Iteration 82/1000 | Loss: 0.00002010
Iteration 83/1000 | Loss: 0.00002010
Iteration 84/1000 | Loss: 0.00002010
Iteration 85/1000 | Loss: 0.00002010
Iteration 86/1000 | Loss: 0.00002010
Iteration 87/1000 | Loss: 0.00002009
Iteration 88/1000 | Loss: 0.00002009
Iteration 89/1000 | Loss: 0.00002009
Iteration 90/1000 | Loss: 0.00002008
Iteration 91/1000 | Loss: 0.00002008
Iteration 92/1000 | Loss: 0.00002008
Iteration 93/1000 | Loss: 0.00002008
Iteration 94/1000 | Loss: 0.00002008
Iteration 95/1000 | Loss: 0.00002008
Iteration 96/1000 | Loss: 0.00002008
Iteration 97/1000 | Loss: 0.00002007
Iteration 98/1000 | Loss: 0.00002007
Iteration 99/1000 | Loss: 0.00002007
Iteration 100/1000 | Loss: 0.00002007
Iteration 101/1000 | Loss: 0.00002007
Iteration 102/1000 | Loss: 0.00002007
Iteration 103/1000 | Loss: 0.00002006
Iteration 104/1000 | Loss: 0.00002006
Iteration 105/1000 | Loss: 0.00002006
Iteration 106/1000 | Loss: 0.00002006
Iteration 107/1000 | Loss: 0.00002006
Iteration 108/1000 | Loss: 0.00002006
Iteration 109/1000 | Loss: 0.00002006
Iteration 110/1000 | Loss: 0.00002006
Iteration 111/1000 | Loss: 0.00002006
Iteration 112/1000 | Loss: 0.00002006
Iteration 113/1000 | Loss: 0.00002006
Iteration 114/1000 | Loss: 0.00002006
Iteration 115/1000 | Loss: 0.00002005
Iteration 116/1000 | Loss: 0.00002005
Iteration 117/1000 | Loss: 0.00002005
Iteration 118/1000 | Loss: 0.00002005
Iteration 119/1000 | Loss: 0.00002005
Iteration 120/1000 | Loss: 0.00002005
Iteration 121/1000 | Loss: 0.00002005
Iteration 122/1000 | Loss: 0.00002005
Iteration 123/1000 | Loss: 0.00002004
Iteration 124/1000 | Loss: 0.00002004
Iteration 125/1000 | Loss: 0.00002004
Iteration 126/1000 | Loss: 0.00002004
Iteration 127/1000 | Loss: 0.00002003
Iteration 128/1000 | Loss: 0.00002003
Iteration 129/1000 | Loss: 0.00002003
Iteration 130/1000 | Loss: 0.00002003
Iteration 131/1000 | Loss: 0.00002003
Iteration 132/1000 | Loss: 0.00002003
Iteration 133/1000 | Loss: 0.00002003
Iteration 134/1000 | Loss: 0.00002003
Iteration 135/1000 | Loss: 0.00002003
Iteration 136/1000 | Loss: 0.00002002
Iteration 137/1000 | Loss: 0.00002002
Iteration 138/1000 | Loss: 0.00002002
Iteration 139/1000 | Loss: 0.00002002
Iteration 140/1000 | Loss: 0.00002002
Iteration 141/1000 | Loss: 0.00002002
Iteration 142/1000 | Loss: 0.00002001
Iteration 143/1000 | Loss: 0.00002001
Iteration 144/1000 | Loss: 0.00002001
Iteration 145/1000 | Loss: 0.00002001
Iteration 146/1000 | Loss: 0.00002001
Iteration 147/1000 | Loss: 0.00002001
Iteration 148/1000 | Loss: 0.00002001
Iteration 149/1000 | Loss: 0.00002001
Iteration 150/1000 | Loss: 0.00002001
Iteration 151/1000 | Loss: 0.00002001
Iteration 152/1000 | Loss: 0.00002001
Iteration 153/1000 | Loss: 0.00002001
Iteration 154/1000 | Loss: 0.00002001
Iteration 155/1000 | Loss: 0.00002001
Iteration 156/1000 | Loss: 0.00002001
Iteration 157/1000 | Loss: 0.00002001
Iteration 158/1000 | Loss: 0.00002001
Iteration 159/1000 | Loss: 0.00002001
Iteration 160/1000 | Loss: 0.00002001
Iteration 161/1000 | Loss: 0.00002001
Iteration 162/1000 | Loss: 0.00002001
Iteration 163/1000 | Loss: 0.00002001
Iteration 164/1000 | Loss: 0.00002001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.000930908252485e-05, 2.000930908252485e-05, 2.000930908252485e-05, 2.000930908252485e-05, 2.000930908252485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.000930908252485e-05

Optimization complete. Final v2v error: 3.8065404891967773 mm

Highest mean error: 4.114711761474609 mm for frame 198

Lowest mean error: 3.567164182662964 mm for frame 77

Saving results

Total time: 34.6266233921051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1840/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405591
Iteration 2/25 | Loss: 0.00112071
Iteration 3/25 | Loss: 0.00104041
Iteration 4/25 | Loss: 0.00103150
Iteration 5/25 | Loss: 0.00102902
Iteration 6/25 | Loss: 0.00102805
Iteration 7/25 | Loss: 0.00102802
Iteration 8/25 | Loss: 0.00102802
Iteration 9/25 | Loss: 0.00102802
Iteration 10/25 | Loss: 0.00102802
Iteration 11/25 | Loss: 0.00102802
Iteration 12/25 | Loss: 0.00102802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010280204005539417, 0.0010280204005539417, 0.0010280204005539417, 0.0010280204005539417, 0.0010280204005539417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010280204005539417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49809253
Iteration 2/25 | Loss: 0.00095812
Iteration 3/25 | Loss: 0.00095811
Iteration 4/25 | Loss: 0.00095811
Iteration 5/25 | Loss: 0.00095811
Iteration 6/25 | Loss: 0.00095811
Iteration 7/25 | Loss: 0.00095811
Iteration 8/25 | Loss: 0.00095811
Iteration 9/25 | Loss: 0.00095811
Iteration 10/25 | Loss: 0.00095811
Iteration 11/25 | Loss: 0.00095811
Iteration 12/25 | Loss: 0.00095811
Iteration 13/25 | Loss: 0.00095811
Iteration 14/25 | Loss: 0.00095811
Iteration 15/25 | Loss: 0.00095811
Iteration 16/25 | Loss: 0.00095811
Iteration 17/25 | Loss: 0.00095811
Iteration 18/25 | Loss: 0.00095811
Iteration 19/25 | Loss: 0.00095811
Iteration 20/25 | Loss: 0.00095811
Iteration 21/25 | Loss: 0.00095811
Iteration 22/25 | Loss: 0.00095811
Iteration 23/25 | Loss: 0.00095811
Iteration 24/25 | Loss: 0.00095811
Iteration 25/25 | Loss: 0.00095811

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095811
Iteration 2/1000 | Loss: 0.00003125
Iteration 3/1000 | Loss: 0.00001987
Iteration 4/1000 | Loss: 0.00001705
Iteration 5/1000 | Loss: 0.00001633
Iteration 6/1000 | Loss: 0.00001588
Iteration 7/1000 | Loss: 0.00001562
Iteration 8/1000 | Loss: 0.00001554
Iteration 9/1000 | Loss: 0.00001538
Iteration 10/1000 | Loss: 0.00001525
Iteration 11/1000 | Loss: 0.00001524
Iteration 12/1000 | Loss: 0.00001524
Iteration 13/1000 | Loss: 0.00001524
Iteration 14/1000 | Loss: 0.00001522
Iteration 15/1000 | Loss: 0.00001522
Iteration 16/1000 | Loss: 0.00001522
Iteration 17/1000 | Loss: 0.00001522
Iteration 18/1000 | Loss: 0.00001522
Iteration 19/1000 | Loss: 0.00001522
Iteration 20/1000 | Loss: 0.00001522
Iteration 21/1000 | Loss: 0.00001522
Iteration 22/1000 | Loss: 0.00001522
Iteration 23/1000 | Loss: 0.00001522
Iteration 24/1000 | Loss: 0.00001522
Iteration 25/1000 | Loss: 0.00001521
Iteration 26/1000 | Loss: 0.00001521
Iteration 27/1000 | Loss: 0.00001520
Iteration 28/1000 | Loss: 0.00001520
Iteration 29/1000 | Loss: 0.00001520
Iteration 30/1000 | Loss: 0.00001520
Iteration 31/1000 | Loss: 0.00001519
Iteration 32/1000 | Loss: 0.00001519
Iteration 33/1000 | Loss: 0.00001519
Iteration 34/1000 | Loss: 0.00001519
Iteration 35/1000 | Loss: 0.00001519
Iteration 36/1000 | Loss: 0.00001519
Iteration 37/1000 | Loss: 0.00001519
Iteration 38/1000 | Loss: 0.00001518
Iteration 39/1000 | Loss: 0.00001518
Iteration 40/1000 | Loss: 0.00001517
Iteration 41/1000 | Loss: 0.00001517
Iteration 42/1000 | Loss: 0.00001517
Iteration 43/1000 | Loss: 0.00001517
Iteration 44/1000 | Loss: 0.00001517
Iteration 45/1000 | Loss: 0.00001516
Iteration 46/1000 | Loss: 0.00001516
Iteration 47/1000 | Loss: 0.00001516
Iteration 48/1000 | Loss: 0.00001516
Iteration 49/1000 | Loss: 0.00001516
Iteration 50/1000 | Loss: 0.00001516
Iteration 51/1000 | Loss: 0.00001515
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001515
Iteration 54/1000 | Loss: 0.00001515
Iteration 55/1000 | Loss: 0.00001515
Iteration 56/1000 | Loss: 0.00001514
Iteration 57/1000 | Loss: 0.00001514
Iteration 58/1000 | Loss: 0.00001514
Iteration 59/1000 | Loss: 0.00001514
Iteration 60/1000 | Loss: 0.00001514
Iteration 61/1000 | Loss: 0.00001514
Iteration 62/1000 | Loss: 0.00001514
Iteration 63/1000 | Loss: 0.00001514
Iteration 64/1000 | Loss: 0.00001513
Iteration 65/1000 | Loss: 0.00001513
Iteration 66/1000 | Loss: 0.00001513
Iteration 67/1000 | Loss: 0.00001513
Iteration 68/1000 | Loss: 0.00001513
Iteration 69/1000 | Loss: 0.00001513
Iteration 70/1000 | Loss: 0.00001513
Iteration 71/1000 | Loss: 0.00001513
Iteration 72/1000 | Loss: 0.00001513
Iteration 73/1000 | Loss: 0.00001512
Iteration 74/1000 | Loss: 0.00001512
Iteration 75/1000 | Loss: 0.00001512
Iteration 76/1000 | Loss: 0.00001512
Iteration 77/1000 | Loss: 0.00001512
Iteration 78/1000 | Loss: 0.00001512
Iteration 79/1000 | Loss: 0.00001512
Iteration 80/1000 | Loss: 0.00001510
Iteration 81/1000 | Loss: 0.00001510
Iteration 82/1000 | Loss: 0.00001510
Iteration 83/1000 | Loss: 0.00001510
Iteration 84/1000 | Loss: 0.00001510
Iteration 85/1000 | Loss: 0.00001510
Iteration 86/1000 | Loss: 0.00001510
Iteration 87/1000 | Loss: 0.00001510
Iteration 88/1000 | Loss: 0.00001510
Iteration 89/1000 | Loss: 0.00001510
Iteration 90/1000 | Loss: 0.00001510
Iteration 91/1000 | Loss: 0.00001509
Iteration 92/1000 | Loss: 0.00001509
Iteration 93/1000 | Loss: 0.00001509
Iteration 94/1000 | Loss: 0.00001509
Iteration 95/1000 | Loss: 0.00001509
Iteration 96/1000 | Loss: 0.00001509
Iteration 97/1000 | Loss: 0.00001509
Iteration 98/1000 | Loss: 0.00001509
Iteration 99/1000 | Loss: 0.00001509
Iteration 100/1000 | Loss: 0.00001509
Iteration 101/1000 | Loss: 0.00001509
Iteration 102/1000 | Loss: 0.00001509
Iteration 103/1000 | Loss: 0.00001509
Iteration 104/1000 | Loss: 0.00001509
Iteration 105/1000 | Loss: 0.00001509
Iteration 106/1000 | Loss: 0.00001509
Iteration 107/1000 | Loss: 0.00001509
Iteration 108/1000 | Loss: 0.00001509
Iteration 109/1000 | Loss: 0.00001509
Iteration 110/1000 | Loss: 0.00001509
Iteration 111/1000 | Loss: 0.00001509
Iteration 112/1000 | Loss: 0.00001509
Iteration 113/1000 | Loss: 0.00001509
Iteration 114/1000 | Loss: 0.00001509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.5093229194462765e-05, 1.5093229194462765e-05, 1.5093229194462765e-05, 1.5093229194462765e-05, 1.5093229194462765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5093229194462765e-05

Optimization complete. Final v2v error: 3.326098918914795 mm

Highest mean error: 3.721867561340332 mm for frame 164

Lowest mean error: 2.962275743484497 mm for frame 98

Saving results

Total time: 28.121766567230225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1840/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427222
Iteration 2/25 | Loss: 0.00124843
Iteration 3/25 | Loss: 0.00110359
Iteration 4/25 | Loss: 0.00108943
Iteration 5/25 | Loss: 0.00108215
Iteration 6/25 | Loss: 0.00107940
Iteration 7/25 | Loss: 0.00107869
Iteration 8/25 | Loss: 0.00107869
Iteration 9/25 | Loss: 0.00107869
Iteration 10/25 | Loss: 0.00107869
Iteration 11/25 | Loss: 0.00107869
Iteration 12/25 | Loss: 0.00107869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010786898201331496, 0.0010786898201331496, 0.0010786898201331496, 0.0010786898201331496, 0.0010786898201331496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010786898201331496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79175735
Iteration 2/25 | Loss: 0.00111700
Iteration 3/25 | Loss: 0.00111699
Iteration 4/25 | Loss: 0.00111699
Iteration 5/25 | Loss: 0.00111699
Iteration 6/25 | Loss: 0.00111699
Iteration 7/25 | Loss: 0.00111699
Iteration 8/25 | Loss: 0.00111699
Iteration 9/25 | Loss: 0.00111699
Iteration 10/25 | Loss: 0.00111699
Iteration 11/25 | Loss: 0.00111699
Iteration 12/25 | Loss: 0.00111699
Iteration 13/25 | Loss: 0.00111699
Iteration 14/25 | Loss: 0.00111699
Iteration 15/25 | Loss: 0.00111699
Iteration 16/25 | Loss: 0.00111699
Iteration 17/25 | Loss: 0.00111699
Iteration 18/25 | Loss: 0.00111699
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011169904610142112, 0.0011169904610142112, 0.0011169904610142112, 0.0011169904610142112, 0.0011169904610142112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011169904610142112

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111699
Iteration 2/1000 | Loss: 0.00003303
Iteration 3/1000 | Loss: 0.00002417
Iteration 4/1000 | Loss: 0.00002177
Iteration 5/1000 | Loss: 0.00002093
Iteration 6/1000 | Loss: 0.00002010
Iteration 7/1000 | Loss: 0.00001962
Iteration 8/1000 | Loss: 0.00001951
Iteration 9/1000 | Loss: 0.00001935
Iteration 10/1000 | Loss: 0.00001914
Iteration 11/1000 | Loss: 0.00001906
Iteration 12/1000 | Loss: 0.00001900
Iteration 13/1000 | Loss: 0.00001898
Iteration 14/1000 | Loss: 0.00001891
Iteration 15/1000 | Loss: 0.00001889
Iteration 16/1000 | Loss: 0.00001883
Iteration 17/1000 | Loss: 0.00001878
Iteration 18/1000 | Loss: 0.00001878
Iteration 19/1000 | Loss: 0.00001875
Iteration 20/1000 | Loss: 0.00001875
Iteration 21/1000 | Loss: 0.00001874
Iteration 22/1000 | Loss: 0.00001874
Iteration 23/1000 | Loss: 0.00001874
Iteration 24/1000 | Loss: 0.00001874
Iteration 25/1000 | Loss: 0.00001874
Iteration 26/1000 | Loss: 0.00001874
Iteration 27/1000 | Loss: 0.00001872
Iteration 28/1000 | Loss: 0.00001869
Iteration 29/1000 | Loss: 0.00001869
Iteration 30/1000 | Loss: 0.00001868
Iteration 31/1000 | Loss: 0.00001867
Iteration 32/1000 | Loss: 0.00001866
Iteration 33/1000 | Loss: 0.00001866
Iteration 34/1000 | Loss: 0.00001865
Iteration 35/1000 | Loss: 0.00001864
Iteration 36/1000 | Loss: 0.00001863
Iteration 37/1000 | Loss: 0.00001863
Iteration 38/1000 | Loss: 0.00001863
Iteration 39/1000 | Loss: 0.00001862
Iteration 40/1000 | Loss: 0.00001862
Iteration 41/1000 | Loss: 0.00001862
Iteration 42/1000 | Loss: 0.00001861
Iteration 43/1000 | Loss: 0.00001861
Iteration 44/1000 | Loss: 0.00001861
Iteration 45/1000 | Loss: 0.00001861
Iteration 46/1000 | Loss: 0.00001860
Iteration 47/1000 | Loss: 0.00001859
Iteration 48/1000 | Loss: 0.00001859
Iteration 49/1000 | Loss: 0.00001859
Iteration 50/1000 | Loss: 0.00001859
Iteration 51/1000 | Loss: 0.00001859
Iteration 52/1000 | Loss: 0.00001858
Iteration 53/1000 | Loss: 0.00001858
Iteration 54/1000 | Loss: 0.00001858
Iteration 55/1000 | Loss: 0.00001858
Iteration 56/1000 | Loss: 0.00001858
Iteration 57/1000 | Loss: 0.00001858
Iteration 58/1000 | Loss: 0.00001858
Iteration 59/1000 | Loss: 0.00001858
Iteration 60/1000 | Loss: 0.00001858
Iteration 61/1000 | Loss: 0.00001857
Iteration 62/1000 | Loss: 0.00001857
Iteration 63/1000 | Loss: 0.00001857
Iteration 64/1000 | Loss: 0.00001857
Iteration 65/1000 | Loss: 0.00001857
Iteration 66/1000 | Loss: 0.00001857
Iteration 67/1000 | Loss: 0.00001857
Iteration 68/1000 | Loss: 0.00001856
Iteration 69/1000 | Loss: 0.00001856
Iteration 70/1000 | Loss: 0.00001856
Iteration 71/1000 | Loss: 0.00001856
Iteration 72/1000 | Loss: 0.00001856
Iteration 73/1000 | Loss: 0.00001856
Iteration 74/1000 | Loss: 0.00001856
Iteration 75/1000 | Loss: 0.00001856
Iteration 76/1000 | Loss: 0.00001856
Iteration 77/1000 | Loss: 0.00001856
Iteration 78/1000 | Loss: 0.00001856
Iteration 79/1000 | Loss: 0.00001856
Iteration 80/1000 | Loss: 0.00001856
Iteration 81/1000 | Loss: 0.00001856
Iteration 82/1000 | Loss: 0.00001856
Iteration 83/1000 | Loss: 0.00001856
Iteration 84/1000 | Loss: 0.00001856
Iteration 85/1000 | Loss: 0.00001856
Iteration 86/1000 | Loss: 0.00001856
Iteration 87/1000 | Loss: 0.00001856
Iteration 88/1000 | Loss: 0.00001856
Iteration 89/1000 | Loss: 0.00001856
Iteration 90/1000 | Loss: 0.00001856
Iteration 91/1000 | Loss: 0.00001856
Iteration 92/1000 | Loss: 0.00001856
Iteration 93/1000 | Loss: 0.00001856
Iteration 94/1000 | Loss: 0.00001856
Iteration 95/1000 | Loss: 0.00001856
Iteration 96/1000 | Loss: 0.00001856
Iteration 97/1000 | Loss: 0.00001856
Iteration 98/1000 | Loss: 0.00001856
Iteration 99/1000 | Loss: 0.00001856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.8556744180386886e-05, 1.8556744180386886e-05, 1.8556744180386886e-05, 1.8556744180386886e-05, 1.8556744180386886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8556744180386886e-05

Optimization complete. Final v2v error: 3.7647452354431152 mm

Highest mean error: 4.079611301422119 mm for frame 61

Lowest mean error: 3.344564914703369 mm for frame 6

Saving results

Total time: 36.3281524181366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1840/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847792
Iteration 2/25 | Loss: 0.00194067
Iteration 3/25 | Loss: 0.00127677
Iteration 4/25 | Loss: 0.00121162
Iteration 5/25 | Loss: 0.00120961
Iteration 6/25 | Loss: 0.00116968
Iteration 7/25 | Loss: 0.00115443
Iteration 8/25 | Loss: 0.00114011
Iteration 9/25 | Loss: 0.00113577
Iteration 10/25 | Loss: 0.00111299
Iteration 11/25 | Loss: 0.00110981
Iteration 12/25 | Loss: 0.00110941
Iteration 13/25 | Loss: 0.00110920
Iteration 14/25 | Loss: 0.00110903
Iteration 15/25 | Loss: 0.00110895
Iteration 16/25 | Loss: 0.00110895
Iteration 17/25 | Loss: 0.00110895
Iteration 18/25 | Loss: 0.00110895
Iteration 19/25 | Loss: 0.00110895
Iteration 20/25 | Loss: 0.00110894
Iteration 21/25 | Loss: 0.00110894
Iteration 22/25 | Loss: 0.00110894
Iteration 23/25 | Loss: 0.00110894
Iteration 24/25 | Loss: 0.00110894
Iteration 25/25 | Loss: 0.00110894

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 14.01777649
Iteration 2/25 | Loss: 0.00094575
Iteration 3/25 | Loss: 0.00085360
Iteration 4/25 | Loss: 0.00085357
Iteration 5/25 | Loss: 0.00085357
Iteration 6/25 | Loss: 0.00085357
Iteration 7/25 | Loss: 0.00085357
Iteration 8/25 | Loss: 0.00085357
Iteration 9/25 | Loss: 0.00085357
Iteration 10/25 | Loss: 0.00085357
Iteration 11/25 | Loss: 0.00085357
Iteration 12/25 | Loss: 0.00085357
Iteration 13/25 | Loss: 0.00085357
Iteration 14/25 | Loss: 0.00085357
Iteration 15/25 | Loss: 0.00085357
Iteration 16/25 | Loss: 0.00085357
Iteration 17/25 | Loss: 0.00085357
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008535703527741134, 0.0008535703527741134, 0.0008535703527741134, 0.0008535703527741134, 0.0008535703527741134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008535703527741134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085357
Iteration 2/1000 | Loss: 0.00012644
Iteration 3/1000 | Loss: 0.00007061
Iteration 4/1000 | Loss: 0.00004041
Iteration 5/1000 | Loss: 0.00002784
Iteration 6/1000 | Loss: 0.00002687
Iteration 7/1000 | Loss: 0.00002618
Iteration 8/1000 | Loss: 0.00022283
Iteration 9/1000 | Loss: 0.00003253
Iteration 10/1000 | Loss: 0.00002773
Iteration 11/1000 | Loss: 0.00010895
Iteration 12/1000 | Loss: 0.00002511
Iteration 13/1000 | Loss: 0.00011799
Iteration 14/1000 | Loss: 0.00003281
Iteration 15/1000 | Loss: 0.00002626
Iteration 16/1000 | Loss: 0.00002381
Iteration 17/1000 | Loss: 0.00010945
Iteration 18/1000 | Loss: 0.00002369
Iteration 19/1000 | Loss: 0.00002338
Iteration 20/1000 | Loss: 0.00002336
Iteration 21/1000 | Loss: 0.00002327
Iteration 22/1000 | Loss: 0.00002326
Iteration 23/1000 | Loss: 0.00002322
Iteration 24/1000 | Loss: 0.00002321
Iteration 25/1000 | Loss: 0.00002321
Iteration 26/1000 | Loss: 0.00002320
Iteration 27/1000 | Loss: 0.00002319
Iteration 28/1000 | Loss: 0.00002319
Iteration 29/1000 | Loss: 0.00002318
Iteration 30/1000 | Loss: 0.00002317
Iteration 31/1000 | Loss: 0.00002316
Iteration 32/1000 | Loss: 0.00002316
Iteration 33/1000 | Loss: 0.00002315
Iteration 34/1000 | Loss: 0.00002315
Iteration 35/1000 | Loss: 0.00002314
Iteration 36/1000 | Loss: 0.00002314
Iteration 37/1000 | Loss: 0.00002314
Iteration 38/1000 | Loss: 0.00002314
Iteration 39/1000 | Loss: 0.00002314
Iteration 40/1000 | Loss: 0.00002314
Iteration 41/1000 | Loss: 0.00002314
Iteration 42/1000 | Loss: 0.00002313
Iteration 43/1000 | Loss: 0.00002313
Iteration 44/1000 | Loss: 0.00002312
Iteration 45/1000 | Loss: 0.00002312
Iteration 46/1000 | Loss: 0.00002311
Iteration 47/1000 | Loss: 0.00002311
Iteration 48/1000 | Loss: 0.00002311
Iteration 49/1000 | Loss: 0.00002310
Iteration 50/1000 | Loss: 0.00002309
Iteration 51/1000 | Loss: 0.00002309
Iteration 52/1000 | Loss: 0.00002309
Iteration 53/1000 | Loss: 0.00002309
Iteration 54/1000 | Loss: 0.00002309
Iteration 55/1000 | Loss: 0.00002309
Iteration 56/1000 | Loss: 0.00002309
Iteration 57/1000 | Loss: 0.00002309
Iteration 58/1000 | Loss: 0.00002309
Iteration 59/1000 | Loss: 0.00002309
Iteration 60/1000 | Loss: 0.00002309
Iteration 61/1000 | Loss: 0.00002309
Iteration 62/1000 | Loss: 0.00002309
Iteration 63/1000 | Loss: 0.00002309
Iteration 64/1000 | Loss: 0.00002309
Iteration 65/1000 | Loss: 0.00002309
Iteration 66/1000 | Loss: 0.00002309
Iteration 67/1000 | Loss: 0.00002309
Iteration 68/1000 | Loss: 0.00002309
Iteration 69/1000 | Loss: 0.00002309
Iteration 70/1000 | Loss: 0.00002309
Iteration 71/1000 | Loss: 0.00002309
Iteration 72/1000 | Loss: 0.00002309
Iteration 73/1000 | Loss: 0.00002309
Iteration 74/1000 | Loss: 0.00002309
Iteration 75/1000 | Loss: 0.00002309
Iteration 76/1000 | Loss: 0.00002309
Iteration 77/1000 | Loss: 0.00002309
Iteration 78/1000 | Loss: 0.00002309
Iteration 79/1000 | Loss: 0.00002309
Iteration 80/1000 | Loss: 0.00002309
Iteration 81/1000 | Loss: 0.00002309
Iteration 82/1000 | Loss: 0.00002309
Iteration 83/1000 | Loss: 0.00002309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [2.3085465727490373e-05, 2.3085465727490373e-05, 2.3085465727490373e-05, 2.3085465727490373e-05, 2.3085465727490373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3085465727490373e-05

Optimization complete. Final v2v error: 4.122290134429932 mm

Highest mean error: 5.232690811157227 mm for frame 223

Lowest mean error: 3.759589672088623 mm for frame 4

Saving results

Total time: 67.90525984764099
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1840/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01211982
Iteration 2/25 | Loss: 0.00190038
Iteration 3/25 | Loss: 0.00117609
Iteration 4/25 | Loss: 0.00110277
Iteration 5/25 | Loss: 0.00109070
Iteration 6/25 | Loss: 0.00109251
Iteration 7/25 | Loss: 0.00108663
Iteration 8/25 | Loss: 0.00108223
Iteration 9/25 | Loss: 0.00108463
Iteration 10/25 | Loss: 0.00108115
Iteration 11/25 | Loss: 0.00107996
Iteration 12/25 | Loss: 0.00108276
Iteration 13/25 | Loss: 0.00108021
Iteration 14/25 | Loss: 0.00107856
Iteration 15/25 | Loss: 0.00107840
Iteration 16/25 | Loss: 0.00107840
Iteration 17/25 | Loss: 0.00107840
Iteration 18/25 | Loss: 0.00107840
Iteration 19/25 | Loss: 0.00107839
Iteration 20/25 | Loss: 0.00107830
Iteration 21/25 | Loss: 0.00107767
Iteration 22/25 | Loss: 0.00107643
Iteration 23/25 | Loss: 0.00107635
Iteration 24/25 | Loss: 0.00107635
Iteration 25/25 | Loss: 0.00107635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54936337
Iteration 2/25 | Loss: 0.00099867
Iteration 3/25 | Loss: 0.00099866
Iteration 4/25 | Loss: 0.00099866
Iteration 5/25 | Loss: 0.00099866
Iteration 6/25 | Loss: 0.00099866
Iteration 7/25 | Loss: 0.00099866
Iteration 8/25 | Loss: 0.00099866
Iteration 9/25 | Loss: 0.00099866
Iteration 10/25 | Loss: 0.00099866
Iteration 11/25 | Loss: 0.00099866
Iteration 12/25 | Loss: 0.00099866
Iteration 13/25 | Loss: 0.00099866
Iteration 14/25 | Loss: 0.00099866
Iteration 15/25 | Loss: 0.00099866
Iteration 16/25 | Loss: 0.00099866
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009986632503569126, 0.0009986632503569126, 0.0009986632503569126, 0.0009986632503569126, 0.0009986632503569126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009986632503569126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099866
Iteration 2/1000 | Loss: 0.00004195
Iteration 3/1000 | Loss: 0.00003136
Iteration 4/1000 | Loss: 0.00002937
Iteration 5/1000 | Loss: 0.00002828
Iteration 6/1000 | Loss: 0.00002760
Iteration 7/1000 | Loss: 0.00002723
Iteration 8/1000 | Loss: 0.00002700
Iteration 9/1000 | Loss: 0.00002699
Iteration 10/1000 | Loss: 0.00002696
Iteration 11/1000 | Loss: 0.00002695
Iteration 12/1000 | Loss: 0.00002683
Iteration 13/1000 | Loss: 0.00002671
Iteration 14/1000 | Loss: 0.00002660
Iteration 15/1000 | Loss: 0.00002659
Iteration 16/1000 | Loss: 0.00002657
Iteration 17/1000 | Loss: 0.00002656
Iteration 18/1000 | Loss: 0.00002656
Iteration 19/1000 | Loss: 0.00002656
Iteration 20/1000 | Loss: 0.00002655
Iteration 21/1000 | Loss: 0.00002655
Iteration 22/1000 | Loss: 0.00002654
Iteration 23/1000 | Loss: 0.00002654
Iteration 24/1000 | Loss: 0.00002654
Iteration 25/1000 | Loss: 0.00002654
Iteration 26/1000 | Loss: 0.00002654
Iteration 27/1000 | Loss: 0.00002653
Iteration 28/1000 | Loss: 0.00002653
Iteration 29/1000 | Loss: 0.00002653
Iteration 30/1000 | Loss: 0.00002653
Iteration 31/1000 | Loss: 0.00002653
Iteration 32/1000 | Loss: 0.00002652
Iteration 33/1000 | Loss: 0.00002652
Iteration 34/1000 | Loss: 0.00002652
Iteration 35/1000 | Loss: 0.00002652
Iteration 36/1000 | Loss: 0.00002652
Iteration 37/1000 | Loss: 0.00002651
Iteration 38/1000 | Loss: 0.00002651
Iteration 39/1000 | Loss: 0.00002651
Iteration 40/1000 | Loss: 0.00002651
Iteration 41/1000 | Loss: 0.00002650
Iteration 42/1000 | Loss: 0.00002650
Iteration 43/1000 | Loss: 0.00002650
Iteration 44/1000 | Loss: 0.00002650
Iteration 45/1000 | Loss: 0.00002650
Iteration 46/1000 | Loss: 0.00002649
Iteration 47/1000 | Loss: 0.00002649
Iteration 48/1000 | Loss: 0.00002649
Iteration 49/1000 | Loss: 0.00002649
Iteration 50/1000 | Loss: 0.00002649
Iteration 51/1000 | Loss: 0.00002649
Iteration 52/1000 | Loss: 0.00002649
Iteration 53/1000 | Loss: 0.00002649
Iteration 54/1000 | Loss: 0.00002649
Iteration 55/1000 | Loss: 0.00002649
Iteration 56/1000 | Loss: 0.00002649
Iteration 57/1000 | Loss: 0.00002648
Iteration 58/1000 | Loss: 0.00002648
Iteration 59/1000 | Loss: 0.00002647
Iteration 60/1000 | Loss: 0.00002647
Iteration 61/1000 | Loss: 0.00002646
Iteration 62/1000 | Loss: 0.00002646
Iteration 63/1000 | Loss: 0.00002646
Iteration 64/1000 | Loss: 0.00002646
Iteration 65/1000 | Loss: 0.00002646
Iteration 66/1000 | Loss: 0.00002646
Iteration 67/1000 | Loss: 0.00002646
Iteration 68/1000 | Loss: 0.00002646
Iteration 69/1000 | Loss: 0.00002646
Iteration 70/1000 | Loss: 0.00002646
Iteration 71/1000 | Loss: 0.00002646
Iteration 72/1000 | Loss: 0.00002646
Iteration 73/1000 | Loss: 0.00002646
Iteration 74/1000 | Loss: 0.00002646
Iteration 75/1000 | Loss: 0.00002645
Iteration 76/1000 | Loss: 0.00002645
Iteration 77/1000 | Loss: 0.00002645
Iteration 78/1000 | Loss: 0.00002644
Iteration 79/1000 | Loss: 0.00002644
Iteration 80/1000 | Loss: 0.00002644
Iteration 81/1000 | Loss: 0.00002644
Iteration 82/1000 | Loss: 0.00002644
Iteration 83/1000 | Loss: 0.00002644
Iteration 84/1000 | Loss: 0.00002644
Iteration 85/1000 | Loss: 0.00002644
Iteration 86/1000 | Loss: 0.00002643
Iteration 87/1000 | Loss: 0.00002643
Iteration 88/1000 | Loss: 0.00002643
Iteration 89/1000 | Loss: 0.00002642
Iteration 90/1000 | Loss: 0.00002642
Iteration 91/1000 | Loss: 0.00002642
Iteration 92/1000 | Loss: 0.00002642
Iteration 93/1000 | Loss: 0.00002642
Iteration 94/1000 | Loss: 0.00002642
Iteration 95/1000 | Loss: 0.00002642
Iteration 96/1000 | Loss: 0.00002642
Iteration 97/1000 | Loss: 0.00002642
Iteration 98/1000 | Loss: 0.00002642
Iteration 99/1000 | Loss: 0.00002642
Iteration 100/1000 | Loss: 0.00002641
Iteration 101/1000 | Loss: 0.00002641
Iteration 102/1000 | Loss: 0.00002641
Iteration 103/1000 | Loss: 0.00002641
Iteration 104/1000 | Loss: 0.00002641
Iteration 105/1000 | Loss: 0.00002641
Iteration 106/1000 | Loss: 0.00002641
Iteration 107/1000 | Loss: 0.00002640
Iteration 108/1000 | Loss: 0.00002640
Iteration 109/1000 | Loss: 0.00002640
Iteration 110/1000 | Loss: 0.00002640
Iteration 111/1000 | Loss: 0.00002640
Iteration 112/1000 | Loss: 0.00002640
Iteration 113/1000 | Loss: 0.00002640
Iteration 114/1000 | Loss: 0.00002640
Iteration 115/1000 | Loss: 0.00002640
Iteration 116/1000 | Loss: 0.00002640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.640436650835909e-05, 2.640436650835909e-05, 2.640436650835909e-05, 2.640436650835909e-05, 2.640436650835909e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.640436650835909e-05

Optimization complete. Final v2v error: 4.242675304412842 mm

Highest mean error: 10.444515228271484 mm for frame 67

Lowest mean error: 3.6886417865753174 mm for frame 155

Saving results

Total time: 58.76270771026611
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1840/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00604214
Iteration 2/25 | Loss: 0.00131524
Iteration 3/25 | Loss: 0.00116888
Iteration 4/25 | Loss: 0.00114681
Iteration 5/25 | Loss: 0.00113771
Iteration 6/25 | Loss: 0.00113425
Iteration 7/25 | Loss: 0.00113354
Iteration 8/25 | Loss: 0.00113354
Iteration 9/25 | Loss: 0.00113354
Iteration 10/25 | Loss: 0.00113354
Iteration 11/25 | Loss: 0.00113354
Iteration 12/25 | Loss: 0.00113354
Iteration 13/25 | Loss: 0.00113354
Iteration 14/25 | Loss: 0.00113354
Iteration 15/25 | Loss: 0.00113354
Iteration 16/25 | Loss: 0.00113354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011335409944877028, 0.0011335409944877028, 0.0011335409944877028, 0.0011335409944877028, 0.0011335409944877028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011335409944877028

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 14.34229565
Iteration 2/25 | Loss: 0.00111082
Iteration 3/25 | Loss: 0.00111011
Iteration 4/25 | Loss: 0.00111010
Iteration 5/25 | Loss: 0.00111010
Iteration 6/25 | Loss: 0.00111010
Iteration 7/25 | Loss: 0.00111010
Iteration 8/25 | Loss: 0.00111010
Iteration 9/25 | Loss: 0.00111010
Iteration 10/25 | Loss: 0.00111010
Iteration 11/25 | Loss: 0.00111010
Iteration 12/25 | Loss: 0.00111010
Iteration 13/25 | Loss: 0.00111010
Iteration 14/25 | Loss: 0.00111010
Iteration 15/25 | Loss: 0.00111010
Iteration 16/25 | Loss: 0.00111010
Iteration 17/25 | Loss: 0.00111010
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001110103097744286, 0.001110103097744286, 0.001110103097744286, 0.001110103097744286, 0.001110103097744286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001110103097744286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111010
Iteration 2/1000 | Loss: 0.00003273
Iteration 3/1000 | Loss: 0.00002621
Iteration 4/1000 | Loss: 0.00002496
Iteration 5/1000 | Loss: 0.00002444
Iteration 6/1000 | Loss: 0.00002418
Iteration 7/1000 | Loss: 0.00002394
Iteration 8/1000 | Loss: 0.00002371
Iteration 9/1000 | Loss: 0.00002369
Iteration 10/1000 | Loss: 0.00002361
Iteration 11/1000 | Loss: 0.00002356
Iteration 12/1000 | Loss: 0.00002355
Iteration 13/1000 | Loss: 0.00002355
Iteration 14/1000 | Loss: 0.00002355
Iteration 15/1000 | Loss: 0.00002354
Iteration 16/1000 | Loss: 0.00002354
Iteration 17/1000 | Loss: 0.00002354
Iteration 18/1000 | Loss: 0.00002353
Iteration 19/1000 | Loss: 0.00002353
Iteration 20/1000 | Loss: 0.00002353
Iteration 21/1000 | Loss: 0.00002352
Iteration 22/1000 | Loss: 0.00002352
Iteration 23/1000 | Loss: 0.00002352
Iteration 24/1000 | Loss: 0.00002351
Iteration 25/1000 | Loss: 0.00002351
Iteration 26/1000 | Loss: 0.00002350
Iteration 27/1000 | Loss: 0.00002350
Iteration 28/1000 | Loss: 0.00002350
Iteration 29/1000 | Loss: 0.00002349
Iteration 30/1000 | Loss: 0.00002348
Iteration 31/1000 | Loss: 0.00002348
Iteration 32/1000 | Loss: 0.00002347
Iteration 33/1000 | Loss: 0.00002347
Iteration 34/1000 | Loss: 0.00002347
Iteration 35/1000 | Loss: 0.00002347
Iteration 36/1000 | Loss: 0.00002347
Iteration 37/1000 | Loss: 0.00002346
Iteration 38/1000 | Loss: 0.00002346
Iteration 39/1000 | Loss: 0.00002346
Iteration 40/1000 | Loss: 0.00002346
Iteration 41/1000 | Loss: 0.00002346
Iteration 42/1000 | Loss: 0.00002346
Iteration 43/1000 | Loss: 0.00002346
Iteration 44/1000 | Loss: 0.00002346
Iteration 45/1000 | Loss: 0.00002346
Iteration 46/1000 | Loss: 0.00002346
Iteration 47/1000 | Loss: 0.00002346
Iteration 48/1000 | Loss: 0.00002346
Iteration 49/1000 | Loss: 0.00002346
Iteration 50/1000 | Loss: 0.00002346
Iteration 51/1000 | Loss: 0.00002346
Iteration 52/1000 | Loss: 0.00002346
Iteration 53/1000 | Loss: 0.00002346
Iteration 54/1000 | Loss: 0.00002346
Iteration 55/1000 | Loss: 0.00002346
Iteration 56/1000 | Loss: 0.00002346
Iteration 57/1000 | Loss: 0.00002346
Iteration 58/1000 | Loss: 0.00002346
Iteration 59/1000 | Loss: 0.00002346
Iteration 60/1000 | Loss: 0.00002346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [2.346282417420298e-05, 2.346282417420298e-05, 2.346282417420298e-05, 2.346282417420298e-05, 2.346282417420298e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.346282417420298e-05

Optimization complete. Final v2v error: 4.1839680671691895 mm

Highest mean error: 4.428375244140625 mm for frame 201

Lowest mean error: 4.02681303024292 mm for frame 154

Saving results

Total time: 28.903428077697754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1840/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002100
Iteration 2/25 | Loss: 0.00162337
Iteration 3/25 | Loss: 0.00126099
Iteration 4/25 | Loss: 0.00122210
Iteration 5/25 | Loss: 0.00120642
Iteration 6/25 | Loss: 0.00120300
Iteration 7/25 | Loss: 0.00120269
Iteration 8/25 | Loss: 0.00120269
Iteration 9/25 | Loss: 0.00120269
Iteration 10/25 | Loss: 0.00120269
Iteration 11/25 | Loss: 0.00120269
Iteration 12/25 | Loss: 0.00120269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001202691812068224, 0.001202691812068224, 0.001202691812068224, 0.001202691812068224, 0.001202691812068224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001202691812068224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19171607
Iteration 2/25 | Loss: 0.00114294
Iteration 3/25 | Loss: 0.00114292
Iteration 4/25 | Loss: 0.00114292
Iteration 5/25 | Loss: 0.00114292
Iteration 6/25 | Loss: 0.00114292
Iteration 7/25 | Loss: 0.00114292
Iteration 8/25 | Loss: 0.00114292
Iteration 9/25 | Loss: 0.00114292
Iteration 10/25 | Loss: 0.00114292
Iteration 11/25 | Loss: 0.00114292
Iteration 12/25 | Loss: 0.00114292
Iteration 13/25 | Loss: 0.00114292
Iteration 14/25 | Loss: 0.00114292
Iteration 15/25 | Loss: 0.00114292
Iteration 16/25 | Loss: 0.00114292
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001142918481491506, 0.001142918481491506, 0.001142918481491506, 0.001142918481491506, 0.001142918481491506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001142918481491506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114292
Iteration 2/1000 | Loss: 0.00006279
Iteration 3/1000 | Loss: 0.00004546
Iteration 4/1000 | Loss: 0.00004099
Iteration 5/1000 | Loss: 0.00003887
Iteration 6/1000 | Loss: 0.00003771
Iteration 7/1000 | Loss: 0.00003683
Iteration 8/1000 | Loss: 0.00003615
Iteration 9/1000 | Loss: 0.00003573
Iteration 10/1000 | Loss: 0.00003544
Iteration 11/1000 | Loss: 0.00003528
Iteration 12/1000 | Loss: 0.00003521
Iteration 13/1000 | Loss: 0.00003520
Iteration 14/1000 | Loss: 0.00003518
Iteration 15/1000 | Loss: 0.00003518
Iteration 16/1000 | Loss: 0.00003518
Iteration 17/1000 | Loss: 0.00003518
Iteration 18/1000 | Loss: 0.00003518
Iteration 19/1000 | Loss: 0.00003517
Iteration 20/1000 | Loss: 0.00003516
Iteration 21/1000 | Loss: 0.00003515
Iteration 22/1000 | Loss: 0.00003514
Iteration 23/1000 | Loss: 0.00003513
Iteration 24/1000 | Loss: 0.00003513
Iteration 25/1000 | Loss: 0.00003512
Iteration 26/1000 | Loss: 0.00003512
Iteration 27/1000 | Loss: 0.00003512
Iteration 28/1000 | Loss: 0.00003511
Iteration 29/1000 | Loss: 0.00003511
Iteration 30/1000 | Loss: 0.00003510
Iteration 31/1000 | Loss: 0.00003510
Iteration 32/1000 | Loss: 0.00003510
Iteration 33/1000 | Loss: 0.00003510
Iteration 34/1000 | Loss: 0.00003510
Iteration 35/1000 | Loss: 0.00003510
Iteration 36/1000 | Loss: 0.00003509
Iteration 37/1000 | Loss: 0.00003509
Iteration 38/1000 | Loss: 0.00003509
Iteration 39/1000 | Loss: 0.00003509
Iteration 40/1000 | Loss: 0.00003509
Iteration 41/1000 | Loss: 0.00003509
Iteration 42/1000 | Loss: 0.00003509
Iteration 43/1000 | Loss: 0.00003509
Iteration 44/1000 | Loss: 0.00003509
Iteration 45/1000 | Loss: 0.00003508
Iteration 46/1000 | Loss: 0.00003508
Iteration 47/1000 | Loss: 0.00003508
Iteration 48/1000 | Loss: 0.00003508
Iteration 49/1000 | Loss: 0.00003508
Iteration 50/1000 | Loss: 0.00003508
Iteration 51/1000 | Loss: 0.00003508
Iteration 52/1000 | Loss: 0.00003508
Iteration 53/1000 | Loss: 0.00003508
Iteration 54/1000 | Loss: 0.00003508
Iteration 55/1000 | Loss: 0.00003508
Iteration 56/1000 | Loss: 0.00003507
Iteration 57/1000 | Loss: 0.00003506
Iteration 58/1000 | Loss: 0.00003506
Iteration 59/1000 | Loss: 0.00003506
Iteration 60/1000 | Loss: 0.00003506
Iteration 61/1000 | Loss: 0.00003505
Iteration 62/1000 | Loss: 0.00003505
Iteration 63/1000 | Loss: 0.00003505
Iteration 64/1000 | Loss: 0.00003505
Iteration 65/1000 | Loss: 0.00003505
Iteration 66/1000 | Loss: 0.00003505
Iteration 67/1000 | Loss: 0.00003505
Iteration 68/1000 | Loss: 0.00003505
Iteration 69/1000 | Loss: 0.00003505
Iteration 70/1000 | Loss: 0.00003505
Iteration 71/1000 | Loss: 0.00003504
Iteration 72/1000 | Loss: 0.00003504
Iteration 73/1000 | Loss: 0.00003504
Iteration 74/1000 | Loss: 0.00003504
Iteration 75/1000 | Loss: 0.00003504
Iteration 76/1000 | Loss: 0.00003503
Iteration 77/1000 | Loss: 0.00003503
Iteration 78/1000 | Loss: 0.00003503
Iteration 79/1000 | Loss: 0.00003503
Iteration 80/1000 | Loss: 0.00003502
Iteration 81/1000 | Loss: 0.00003502
Iteration 82/1000 | Loss: 0.00003502
Iteration 83/1000 | Loss: 0.00003502
Iteration 84/1000 | Loss: 0.00003501
Iteration 85/1000 | Loss: 0.00003501
Iteration 86/1000 | Loss: 0.00003501
Iteration 87/1000 | Loss: 0.00003501
Iteration 88/1000 | Loss: 0.00003501
Iteration 89/1000 | Loss: 0.00003501
Iteration 90/1000 | Loss: 0.00003501
Iteration 91/1000 | Loss: 0.00003501
Iteration 92/1000 | Loss: 0.00003501
Iteration 93/1000 | Loss: 0.00003500
Iteration 94/1000 | Loss: 0.00003500
Iteration 95/1000 | Loss: 0.00003500
Iteration 96/1000 | Loss: 0.00003500
Iteration 97/1000 | Loss: 0.00003500
Iteration 98/1000 | Loss: 0.00003500
Iteration 99/1000 | Loss: 0.00003500
Iteration 100/1000 | Loss: 0.00003500
Iteration 101/1000 | Loss: 0.00003499
Iteration 102/1000 | Loss: 0.00003499
Iteration 103/1000 | Loss: 0.00003498
Iteration 104/1000 | Loss: 0.00003498
Iteration 105/1000 | Loss: 0.00003498
Iteration 106/1000 | Loss: 0.00003498
Iteration 107/1000 | Loss: 0.00003498
Iteration 108/1000 | Loss: 0.00003497
Iteration 109/1000 | Loss: 0.00003497
Iteration 110/1000 | Loss: 0.00003497
Iteration 111/1000 | Loss: 0.00003497
Iteration 112/1000 | Loss: 0.00003497
Iteration 113/1000 | Loss: 0.00003497
Iteration 114/1000 | Loss: 0.00003497
Iteration 115/1000 | Loss: 0.00003496
Iteration 116/1000 | Loss: 0.00003496
Iteration 117/1000 | Loss: 0.00003496
Iteration 118/1000 | Loss: 0.00003496
Iteration 119/1000 | Loss: 0.00003495
Iteration 120/1000 | Loss: 0.00003495
Iteration 121/1000 | Loss: 0.00003495
Iteration 122/1000 | Loss: 0.00003495
Iteration 123/1000 | Loss: 0.00003495
Iteration 124/1000 | Loss: 0.00003495
Iteration 125/1000 | Loss: 0.00003495
Iteration 126/1000 | Loss: 0.00003495
Iteration 127/1000 | Loss: 0.00003495
Iteration 128/1000 | Loss: 0.00003494
Iteration 129/1000 | Loss: 0.00003494
Iteration 130/1000 | Loss: 0.00003494
Iteration 131/1000 | Loss: 0.00003494
Iteration 132/1000 | Loss: 0.00003494
Iteration 133/1000 | Loss: 0.00003494
Iteration 134/1000 | Loss: 0.00003493
Iteration 135/1000 | Loss: 0.00003493
Iteration 136/1000 | Loss: 0.00003493
Iteration 137/1000 | Loss: 0.00003493
Iteration 138/1000 | Loss: 0.00003493
Iteration 139/1000 | Loss: 0.00003493
Iteration 140/1000 | Loss: 0.00003493
Iteration 141/1000 | Loss: 0.00003493
Iteration 142/1000 | Loss: 0.00003493
Iteration 143/1000 | Loss: 0.00003493
Iteration 144/1000 | Loss: 0.00003492
Iteration 145/1000 | Loss: 0.00003492
Iteration 146/1000 | Loss: 0.00003492
Iteration 147/1000 | Loss: 0.00003492
Iteration 148/1000 | Loss: 0.00003492
Iteration 149/1000 | Loss: 0.00003492
Iteration 150/1000 | Loss: 0.00003492
Iteration 151/1000 | Loss: 0.00003491
Iteration 152/1000 | Loss: 0.00003491
Iteration 153/1000 | Loss: 0.00003491
Iteration 154/1000 | Loss: 0.00003491
Iteration 155/1000 | Loss: 0.00003491
Iteration 156/1000 | Loss: 0.00003491
Iteration 157/1000 | Loss: 0.00003491
Iteration 158/1000 | Loss: 0.00003491
Iteration 159/1000 | Loss: 0.00003491
Iteration 160/1000 | Loss: 0.00003491
Iteration 161/1000 | Loss: 0.00003490
Iteration 162/1000 | Loss: 0.00003490
Iteration 163/1000 | Loss: 0.00003490
Iteration 164/1000 | Loss: 0.00003490
Iteration 165/1000 | Loss: 0.00003490
Iteration 166/1000 | Loss: 0.00003490
Iteration 167/1000 | Loss: 0.00003490
Iteration 168/1000 | Loss: 0.00003490
Iteration 169/1000 | Loss: 0.00003490
Iteration 170/1000 | Loss: 0.00003490
Iteration 171/1000 | Loss: 0.00003490
Iteration 172/1000 | Loss: 0.00003489
Iteration 173/1000 | Loss: 0.00003489
Iteration 174/1000 | Loss: 0.00003489
Iteration 175/1000 | Loss: 0.00003489
Iteration 176/1000 | Loss: 0.00003489
Iteration 177/1000 | Loss: 0.00003489
Iteration 178/1000 | Loss: 0.00003489
Iteration 179/1000 | Loss: 0.00003489
Iteration 180/1000 | Loss: 0.00003489
Iteration 181/1000 | Loss: 0.00003489
Iteration 182/1000 | Loss: 0.00003489
Iteration 183/1000 | Loss: 0.00003489
Iteration 184/1000 | Loss: 0.00003489
Iteration 185/1000 | Loss: 0.00003489
Iteration 186/1000 | Loss: 0.00003489
Iteration 187/1000 | Loss: 0.00003489
Iteration 188/1000 | Loss: 0.00003489
Iteration 189/1000 | Loss: 0.00003489
Iteration 190/1000 | Loss: 0.00003489
Iteration 191/1000 | Loss: 0.00003489
Iteration 192/1000 | Loss: 0.00003489
Iteration 193/1000 | Loss: 0.00003489
Iteration 194/1000 | Loss: 0.00003489
Iteration 195/1000 | Loss: 0.00003489
Iteration 196/1000 | Loss: 0.00003489
Iteration 197/1000 | Loss: 0.00003489
Iteration 198/1000 | Loss: 0.00003489
Iteration 199/1000 | Loss: 0.00003489
Iteration 200/1000 | Loss: 0.00003489
Iteration 201/1000 | Loss: 0.00003489
Iteration 202/1000 | Loss: 0.00003489
Iteration 203/1000 | Loss: 0.00003489
Iteration 204/1000 | Loss: 0.00003489
Iteration 205/1000 | Loss: 0.00003489
Iteration 206/1000 | Loss: 0.00003489
Iteration 207/1000 | Loss: 0.00003489
Iteration 208/1000 | Loss: 0.00003489
Iteration 209/1000 | Loss: 0.00003489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [3.4891032555606216e-05, 3.4891032555606216e-05, 3.4891032555606216e-05, 3.4891032555606216e-05, 3.4891032555606216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4891032555606216e-05

Optimization complete. Final v2v error: 4.89272403717041 mm

Highest mean error: 5.852115631103516 mm for frame 101

Lowest mean error: 4.058403015136719 mm for frame 124

Saving results

Total time: 37.32616925239563
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1840/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903699
Iteration 2/25 | Loss: 0.00141797
Iteration 3/25 | Loss: 0.00125264
Iteration 4/25 | Loss: 0.00122289
Iteration 5/25 | Loss: 0.00121451
Iteration 6/25 | Loss: 0.00121236
Iteration 7/25 | Loss: 0.00121194
Iteration 8/25 | Loss: 0.00121194
Iteration 9/25 | Loss: 0.00121194
Iteration 10/25 | Loss: 0.00121194
Iteration 11/25 | Loss: 0.00121194
Iteration 12/25 | Loss: 0.00121194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001211935537867248, 0.001211935537867248, 0.001211935537867248, 0.001211935537867248, 0.001211935537867248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001211935537867248

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47566903
Iteration 2/25 | Loss: 0.00146232
Iteration 3/25 | Loss: 0.00146231
Iteration 4/25 | Loss: 0.00146230
Iteration 5/25 | Loss: 0.00146230
Iteration 6/25 | Loss: 0.00146230
Iteration 7/25 | Loss: 0.00146230
Iteration 8/25 | Loss: 0.00146230
Iteration 9/25 | Loss: 0.00146230
Iteration 10/25 | Loss: 0.00146230
Iteration 11/25 | Loss: 0.00146230
Iteration 12/25 | Loss: 0.00146230
Iteration 13/25 | Loss: 0.00146230
Iteration 14/25 | Loss: 0.00146230
Iteration 15/25 | Loss: 0.00146230
Iteration 16/25 | Loss: 0.00146230
Iteration 17/25 | Loss: 0.00146230
Iteration 18/25 | Loss: 0.00146230
Iteration 19/25 | Loss: 0.00146230
Iteration 20/25 | Loss: 0.00146230
Iteration 21/25 | Loss: 0.00146230
Iteration 22/25 | Loss: 0.00146230
Iteration 23/25 | Loss: 0.00146230
Iteration 24/25 | Loss: 0.00146230
Iteration 25/25 | Loss: 0.00146230

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146230
Iteration 2/1000 | Loss: 0.00008370
Iteration 3/1000 | Loss: 0.00005188
Iteration 4/1000 | Loss: 0.00004530
Iteration 5/1000 | Loss: 0.00004081
Iteration 6/1000 | Loss: 0.00003878
Iteration 7/1000 | Loss: 0.00003726
Iteration 8/1000 | Loss: 0.00003622
Iteration 9/1000 | Loss: 0.00003526
Iteration 10/1000 | Loss: 0.00003483
Iteration 11/1000 | Loss: 0.00003437
Iteration 12/1000 | Loss: 0.00003408
Iteration 13/1000 | Loss: 0.00003390
Iteration 14/1000 | Loss: 0.00003382
Iteration 15/1000 | Loss: 0.00003380
Iteration 16/1000 | Loss: 0.00003379
Iteration 17/1000 | Loss: 0.00003378
Iteration 18/1000 | Loss: 0.00003376
Iteration 19/1000 | Loss: 0.00003375
Iteration 20/1000 | Loss: 0.00003374
Iteration 21/1000 | Loss: 0.00003374
Iteration 22/1000 | Loss: 0.00003374
Iteration 23/1000 | Loss: 0.00003373
Iteration 24/1000 | Loss: 0.00003373
Iteration 25/1000 | Loss: 0.00003373
Iteration 26/1000 | Loss: 0.00003373
Iteration 27/1000 | Loss: 0.00003372
Iteration 28/1000 | Loss: 0.00003371
Iteration 29/1000 | Loss: 0.00003371
Iteration 30/1000 | Loss: 0.00003371
Iteration 31/1000 | Loss: 0.00003370
Iteration 32/1000 | Loss: 0.00003370
Iteration 33/1000 | Loss: 0.00003370
Iteration 34/1000 | Loss: 0.00003369
Iteration 35/1000 | Loss: 0.00003369
Iteration 36/1000 | Loss: 0.00003369
Iteration 37/1000 | Loss: 0.00003368
Iteration 38/1000 | Loss: 0.00003368
Iteration 39/1000 | Loss: 0.00003368
Iteration 40/1000 | Loss: 0.00003368
Iteration 41/1000 | Loss: 0.00003368
Iteration 42/1000 | Loss: 0.00003368
Iteration 43/1000 | Loss: 0.00003368
Iteration 44/1000 | Loss: 0.00003368
Iteration 45/1000 | Loss: 0.00003367
Iteration 46/1000 | Loss: 0.00003367
Iteration 47/1000 | Loss: 0.00003366
Iteration 48/1000 | Loss: 0.00003366
Iteration 49/1000 | Loss: 0.00003366
Iteration 50/1000 | Loss: 0.00003366
Iteration 51/1000 | Loss: 0.00003365
Iteration 52/1000 | Loss: 0.00003365
Iteration 53/1000 | Loss: 0.00003365
Iteration 54/1000 | Loss: 0.00003365
Iteration 55/1000 | Loss: 0.00003365
Iteration 56/1000 | Loss: 0.00003365
Iteration 57/1000 | Loss: 0.00003365
Iteration 58/1000 | Loss: 0.00003365
Iteration 59/1000 | Loss: 0.00003365
Iteration 60/1000 | Loss: 0.00003365
Iteration 61/1000 | Loss: 0.00003364
Iteration 62/1000 | Loss: 0.00003364
Iteration 63/1000 | Loss: 0.00003364
Iteration 64/1000 | Loss: 0.00003364
Iteration 65/1000 | Loss: 0.00003364
Iteration 66/1000 | Loss: 0.00003364
Iteration 67/1000 | Loss: 0.00003364
Iteration 68/1000 | Loss: 0.00003363
Iteration 69/1000 | Loss: 0.00003363
Iteration 70/1000 | Loss: 0.00003363
Iteration 71/1000 | Loss: 0.00003363
Iteration 72/1000 | Loss: 0.00003362
Iteration 73/1000 | Loss: 0.00003362
Iteration 74/1000 | Loss: 0.00003362
Iteration 75/1000 | Loss: 0.00003361
Iteration 76/1000 | Loss: 0.00003361
Iteration 77/1000 | Loss: 0.00003361
Iteration 78/1000 | Loss: 0.00003361
Iteration 79/1000 | Loss: 0.00003360
Iteration 80/1000 | Loss: 0.00003360
Iteration 81/1000 | Loss: 0.00003360
Iteration 82/1000 | Loss: 0.00003360
Iteration 83/1000 | Loss: 0.00003360
Iteration 84/1000 | Loss: 0.00003360
Iteration 85/1000 | Loss: 0.00003360
Iteration 86/1000 | Loss: 0.00003359
Iteration 87/1000 | Loss: 0.00003359
Iteration 88/1000 | Loss: 0.00003359
Iteration 89/1000 | Loss: 0.00003359
Iteration 90/1000 | Loss: 0.00003359
Iteration 91/1000 | Loss: 0.00003359
Iteration 92/1000 | Loss: 0.00003359
Iteration 93/1000 | Loss: 0.00003359
Iteration 94/1000 | Loss: 0.00003359
Iteration 95/1000 | Loss: 0.00003359
Iteration 96/1000 | Loss: 0.00003359
Iteration 97/1000 | Loss: 0.00003359
Iteration 98/1000 | Loss: 0.00003359
Iteration 99/1000 | Loss: 0.00003359
Iteration 100/1000 | Loss: 0.00003359
Iteration 101/1000 | Loss: 0.00003359
Iteration 102/1000 | Loss: 0.00003359
Iteration 103/1000 | Loss: 0.00003359
Iteration 104/1000 | Loss: 0.00003359
Iteration 105/1000 | Loss: 0.00003358
Iteration 106/1000 | Loss: 0.00003358
Iteration 107/1000 | Loss: 0.00003358
Iteration 108/1000 | Loss: 0.00003358
Iteration 109/1000 | Loss: 0.00003358
Iteration 110/1000 | Loss: 0.00003358
Iteration 111/1000 | Loss: 0.00003358
Iteration 112/1000 | Loss: 0.00003358
Iteration 113/1000 | Loss: 0.00003358
Iteration 114/1000 | Loss: 0.00003358
Iteration 115/1000 | Loss: 0.00003358
Iteration 116/1000 | Loss: 0.00003358
Iteration 117/1000 | Loss: 0.00003358
Iteration 118/1000 | Loss: 0.00003358
Iteration 119/1000 | Loss: 0.00003358
Iteration 120/1000 | Loss: 0.00003358
Iteration 121/1000 | Loss: 0.00003358
Iteration 122/1000 | Loss: 0.00003358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [3.357774403411895e-05, 3.357774403411895e-05, 3.357774403411895e-05, 3.357774403411895e-05, 3.357774403411895e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.357774403411895e-05

Optimization complete. Final v2v error: 4.825051784515381 mm

Highest mean error: 5.4992876052856445 mm for frame 28

Lowest mean error: 4.2239789962768555 mm for frame 0

Saving results

Total time: 35.53021049499512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_us_1840/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_us_1840/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00921079
Iteration 2/25 | Loss: 0.00118255
Iteration 3/25 | Loss: 0.00106727
Iteration 4/25 | Loss: 0.00105530
Iteration 5/25 | Loss: 0.00105179
Iteration 6/25 | Loss: 0.00105134
Iteration 7/25 | Loss: 0.00105134
Iteration 8/25 | Loss: 0.00105134
Iteration 9/25 | Loss: 0.00105134
Iteration 10/25 | Loss: 0.00105134
Iteration 11/25 | Loss: 0.00105134
Iteration 12/25 | Loss: 0.00105134
Iteration 13/25 | Loss: 0.00105134
Iteration 14/25 | Loss: 0.00105134
Iteration 15/25 | Loss: 0.00105134
Iteration 16/25 | Loss: 0.00105134
Iteration 17/25 | Loss: 0.00105134
Iteration 18/25 | Loss: 0.00105134
Iteration 19/25 | Loss: 0.00105134
Iteration 20/25 | Loss: 0.00105134
Iteration 21/25 | Loss: 0.00105134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010513425804674625, 0.0010513425804674625, 0.0010513425804674625, 0.0010513425804674625, 0.0010513425804674625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010513425804674625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48761582
Iteration 2/25 | Loss: 0.00104904
Iteration 3/25 | Loss: 0.00104904
Iteration 4/25 | Loss: 0.00104903
Iteration 5/25 | Loss: 0.00104903
Iteration 6/25 | Loss: 0.00104903
Iteration 7/25 | Loss: 0.00104903
Iteration 8/25 | Loss: 0.00104903
Iteration 9/25 | Loss: 0.00104903
Iteration 10/25 | Loss: 0.00104903
Iteration 11/25 | Loss: 0.00104903
Iteration 12/25 | Loss: 0.00104903
Iteration 13/25 | Loss: 0.00104903
Iteration 14/25 | Loss: 0.00104903
Iteration 15/25 | Loss: 0.00104903
Iteration 16/25 | Loss: 0.00104903
Iteration 17/25 | Loss: 0.00104903
Iteration 18/25 | Loss: 0.00104903
Iteration 19/25 | Loss: 0.00104903
Iteration 20/25 | Loss: 0.00104903
Iteration 21/25 | Loss: 0.00104903
Iteration 22/25 | Loss: 0.00104903
Iteration 23/25 | Loss: 0.00104903
Iteration 24/25 | Loss: 0.00104903
Iteration 25/25 | Loss: 0.00104903

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104903
Iteration 2/1000 | Loss: 0.00002993
Iteration 3/1000 | Loss: 0.00002139
Iteration 4/1000 | Loss: 0.00001907
Iteration 5/1000 | Loss: 0.00001810
Iteration 6/1000 | Loss: 0.00001736
Iteration 7/1000 | Loss: 0.00001679
Iteration 8/1000 | Loss: 0.00001659
Iteration 9/1000 | Loss: 0.00001647
Iteration 10/1000 | Loss: 0.00001647
Iteration 11/1000 | Loss: 0.00001647
Iteration 12/1000 | Loss: 0.00001646
Iteration 13/1000 | Loss: 0.00001646
Iteration 14/1000 | Loss: 0.00001645
Iteration 15/1000 | Loss: 0.00001643
Iteration 16/1000 | Loss: 0.00001639
Iteration 17/1000 | Loss: 0.00001636
Iteration 18/1000 | Loss: 0.00001635
Iteration 19/1000 | Loss: 0.00001635
Iteration 20/1000 | Loss: 0.00001634
Iteration 21/1000 | Loss: 0.00001634
Iteration 22/1000 | Loss: 0.00001633
Iteration 23/1000 | Loss: 0.00001632
Iteration 24/1000 | Loss: 0.00001632
Iteration 25/1000 | Loss: 0.00001631
Iteration 26/1000 | Loss: 0.00001630
Iteration 27/1000 | Loss: 0.00001629
Iteration 28/1000 | Loss: 0.00001629
Iteration 29/1000 | Loss: 0.00001629
Iteration 30/1000 | Loss: 0.00001626
Iteration 31/1000 | Loss: 0.00001626
Iteration 32/1000 | Loss: 0.00001626
Iteration 33/1000 | Loss: 0.00001626
Iteration 34/1000 | Loss: 0.00001626
Iteration 35/1000 | Loss: 0.00001626
Iteration 36/1000 | Loss: 0.00001626
Iteration 37/1000 | Loss: 0.00001626
Iteration 38/1000 | Loss: 0.00001626
Iteration 39/1000 | Loss: 0.00001626
Iteration 40/1000 | Loss: 0.00001626
Iteration 41/1000 | Loss: 0.00001626
Iteration 42/1000 | Loss: 0.00001625
Iteration 43/1000 | Loss: 0.00001625
Iteration 44/1000 | Loss: 0.00001625
Iteration 45/1000 | Loss: 0.00001624
Iteration 46/1000 | Loss: 0.00001624
Iteration 47/1000 | Loss: 0.00001624
Iteration 48/1000 | Loss: 0.00001624
Iteration 49/1000 | Loss: 0.00001624
Iteration 50/1000 | Loss: 0.00001624
Iteration 51/1000 | Loss: 0.00001623
Iteration 52/1000 | Loss: 0.00001623
Iteration 53/1000 | Loss: 0.00001623
Iteration 54/1000 | Loss: 0.00001623
Iteration 55/1000 | Loss: 0.00001622
Iteration 56/1000 | Loss: 0.00001622
Iteration 57/1000 | Loss: 0.00001622
Iteration 58/1000 | Loss: 0.00001622
Iteration 59/1000 | Loss: 0.00001622
Iteration 60/1000 | Loss: 0.00001621
Iteration 61/1000 | Loss: 0.00001621
Iteration 62/1000 | Loss: 0.00001620
Iteration 63/1000 | Loss: 0.00001620
Iteration 64/1000 | Loss: 0.00001620
Iteration 65/1000 | Loss: 0.00001620
Iteration 66/1000 | Loss: 0.00001620
Iteration 67/1000 | Loss: 0.00001620
Iteration 68/1000 | Loss: 0.00001620
Iteration 69/1000 | Loss: 0.00001620
Iteration 70/1000 | Loss: 0.00001619
Iteration 71/1000 | Loss: 0.00001619
Iteration 72/1000 | Loss: 0.00001619
Iteration 73/1000 | Loss: 0.00001619
Iteration 74/1000 | Loss: 0.00001619
Iteration 75/1000 | Loss: 0.00001619
Iteration 76/1000 | Loss: 0.00001619
Iteration 77/1000 | Loss: 0.00001618
Iteration 78/1000 | Loss: 0.00001618
Iteration 79/1000 | Loss: 0.00001618
Iteration 80/1000 | Loss: 0.00001618
Iteration 81/1000 | Loss: 0.00001618
Iteration 82/1000 | Loss: 0.00001618
Iteration 83/1000 | Loss: 0.00001618
Iteration 84/1000 | Loss: 0.00001618
Iteration 85/1000 | Loss: 0.00001618
Iteration 86/1000 | Loss: 0.00001618
Iteration 87/1000 | Loss: 0.00001618
Iteration 88/1000 | Loss: 0.00001618
Iteration 89/1000 | Loss: 0.00001617
Iteration 90/1000 | Loss: 0.00001617
Iteration 91/1000 | Loss: 0.00001617
Iteration 92/1000 | Loss: 0.00001617
Iteration 93/1000 | Loss: 0.00001617
Iteration 94/1000 | Loss: 0.00001617
Iteration 95/1000 | Loss: 0.00001617
Iteration 96/1000 | Loss: 0.00001617
Iteration 97/1000 | Loss: 0.00001617
Iteration 98/1000 | Loss: 0.00001617
Iteration 99/1000 | Loss: 0.00001617
Iteration 100/1000 | Loss: 0.00001617
Iteration 101/1000 | Loss: 0.00001617
Iteration 102/1000 | Loss: 0.00001617
Iteration 103/1000 | Loss: 0.00001616
Iteration 104/1000 | Loss: 0.00001616
Iteration 105/1000 | Loss: 0.00001616
Iteration 106/1000 | Loss: 0.00001616
Iteration 107/1000 | Loss: 0.00001616
Iteration 108/1000 | Loss: 0.00001616
Iteration 109/1000 | Loss: 0.00001616
Iteration 110/1000 | Loss: 0.00001615
Iteration 111/1000 | Loss: 0.00001615
Iteration 112/1000 | Loss: 0.00001615
Iteration 113/1000 | Loss: 0.00001615
Iteration 114/1000 | Loss: 0.00001615
Iteration 115/1000 | Loss: 0.00001615
Iteration 116/1000 | Loss: 0.00001615
Iteration 117/1000 | Loss: 0.00001615
Iteration 118/1000 | Loss: 0.00001615
Iteration 119/1000 | Loss: 0.00001615
Iteration 120/1000 | Loss: 0.00001615
Iteration 121/1000 | Loss: 0.00001615
Iteration 122/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.6152116586454213e-05, 1.6152116586454213e-05, 1.6152116586454213e-05, 1.6152116586454213e-05, 1.6152116586454213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6152116586454213e-05

Optimization complete. Final v2v error: 3.430255651473999 mm

Highest mean error: 3.708885431289673 mm for frame 63

Lowest mean error: 3.257978677749634 mm for frame 80

Saving results

Total time: 33.86896562576294
